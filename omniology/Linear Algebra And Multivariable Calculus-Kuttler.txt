Math 302 Lecture Notes
Kenneth Kuttler
October 6, 2006

2

Contents
1
Introduction
11
I
Vectors, Vector Products, Lines
13
2
Vectors And Points In Rn 5 Sept.
19
2.1
Rn Ordered nâˆ’tuples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.2
Vectors And Algebra In Rn
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.3
Geometric Meaning Of Vectors . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.4
Geometric Meaning Of Vector Addition
. . . . . . . . . . . . . . . . . . . . .
22
2.5
Distance Between Points In Rn . . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.6
Geometric Meaning Of Scalar Multiplication
. . . . . . . . . . . . . . . . . .
26
2.7
Unit Vectors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.8
Lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.9
Vectors And Physics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
3
Vector Products
39
3.1
The Dot Product 6 Sept.
. . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3.1.1
Deï¬nition In terms Of Coordinates . . . . . . . . . . . . . . . . . . . .
39
3.1.2
The Geometric Meaning Of The Dot Product, The Included Angle . .
40
3.1.3
The Cauchy Schwarz Inequality . . . . . . . . . . . . . . . . . . . . . .
42
3.1.4
The Triangle Inequality . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.1.5
Direction Cosines Of A Line . . . . . . . . . . . . . . . . . . . . . . . .
44
3.1.6
Work And Projections . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.2
The Cross Product 7 Sept.
. . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.2.1
The Geometric Description Of The Cross Product In Terms Of The
Included Angle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.2.2
The Coordinate Description Of The Cross Product . . . . . . . . . . .
50
3.2.3
The Box Product, Triple Product . . . . . . . . . . . . . . . . . . . .
52
3.2.4
A Proof Of The Distributive Law For The Cross Productâˆ—. . . . . . .
53
3.2.5
Torque, Moment Of A Force
. . . . . . . . . . . . . . . . . . . . . . .
54
3.2.6
Angular Velocity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
3.2.7
Center Of Massâˆ—. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
3.3
Further Explanationsâˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3.3.1
The Distributive Law For The Cross Productâˆ—. . . . . . . . . . . . .
57
3.3.2
Vector Identities And Notationâˆ—
. . . . . . . . . . . . . . . . . . . . .
59
3.3.3
Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3

4
CONTENTS
II
Planes And Systems Of Equations
69
4
Planes 11 Sept.
73
4.1
Finding Planes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
4.1.1
Planes From A Normal And A Point . . . . . . . . . . . . . . . . . . .
73
4.1.2
The Angle Between Two Planes
. . . . . . . . . . . . . . . . . . . . .
74
4.1.3
The Plane Which Contains Three Points . . . . . . . . . . . . . . . . .
75
4.1.4
Intercepts Of A Plane . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.1.5
Distance Between A Point And A Plane Or A Point And A Lineâˆ—
. .
77
5
Systems Of Linear Equations 12,13 Sept.
79
5.1
Systems Of Equations, Geometric Interpretations
. . . . . . . . . . . . . . .
79
5.2
Systems Of Equations, Algebraic Procedures
. . . . . . . . . . . . . . . . . .
82
5.2.1
Elementary Operations
. . . . . . . . . . . . . . . . . . . . . . . . . .
82
5.2.2
Gauss Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
5.3
The Rank Of A Matrix 14 Sept. . . . . . . . . . . . . . . . . . . . . . . .
94
5.4
Theory Of Row Reduced Echelon Formâˆ—. . . . . . . . . . . . . . . . . . . . .
96
5.4.1
Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . .
99
III
Linear Independence And Matrices
107
6
Spanning Sets And Linear Independence 18,19 Sept.
111
6.0.2
Spanning Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.0.3
Linear Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
6.0.4
Recognizing Linear Dependence . . . . . . . . . . . . . . . . . . . . . . 118
6.0.5
Discovering Dependence Relations
. . . . . . . . . . . . . . . . . . . . 119
7
Matrices
121
7.1
Matrix Operations And Algebra 20,21 Sept.
. . . . . . . . . . . . . . 121
7.1.1
Addition And Scalar Multiplication Of Matrices
. . . . . . . . . . . . 121
7.1.2
Multiplication Of Matrices
. . . . . . . . . . . . . . . . . . . . . . . . 124
7.1.3
The ijth Entry Of A Product . . . . . . . . . . . . . . . . . . . . . . . 127
7.1.4
Properties Of Matrix Multiplication
. . . . . . . . . . . . . . . . . . . 129
7.1.5
The Transpose . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
7.1.6
The Identity And Inverses . . . . . . . . . . . . . . . . . . . . . . . . . 131
7.2
Finding The Inverse Of A Matrix, Gauss Jordan Method 21,22 Sept.133
7.3
Elementary Matrices 22 Sept.
. . . . . . . . . . . . . . . . . . . . . . . . 138
7.4
Block Multiplication Of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 145
7.4.1
Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 146
IV
LU Decomposition, Subspaces, Linear Transformations
151
8
The LU Factorization 25 Sept.
155
8.0.2
Deï¬nition Of An LU Decomposition . . . . . . . . . . . . . . . . . . . 155
8.0.3
Finding An LU Decomposition By Inspection . . . . . . . . . . . . . . 155
8.0.4
Using Multipliers To Find An LU Decomposition . . . . . . . . . . . . 156
8.0.5
Solving Systems Using The LU Decomposition . . . . . . . . . . . . . 157

CONTENTS
5
9
Rank Of A Matrix 26,27 Sept.
159
9.1
The Row Reduced Echelon Form Of A Matrix . . . . . . . . . . . . . . . . . . 159
9.2
The Rank Of A Matrix
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
9.2.1
The Deï¬nition Of Rank . . . . . . . . . . . . . . . . . . . . . . . . . . 163
9.2.2
Finding The Row And Column Space Of A Matrix . . . . . . . . . . . 164
9.3
Linear Independence And Bases . . . . . . . . . . . . . . . . . . . . . . . . . . 166
9.3.1
Linear Independence And Dependence . . . . . . . . . . . . . . . . . . 166
9.3.2
Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
9.3.3
The Basis Of A Subspace . . . . . . . . . . . . . . . . . . . . . . . . . 170
9.3.4
Finding The Null Space Or Kernel Of A Matrix
. . . . . . . . . . . . 172
9.3.5
Rank And Existence Of Solutions To Linear Systemsâˆ—. . . . . . . . . 174
9.3.6
Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 175
10 Linear Transformations 27 Sept.
181
10.1 Constructing The Matrix Of A Linear Transformation . . . . . . . . . . . . . 182
10.1.1 Rotations of R2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
10.1.2 Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
10.1.3 Matrices Which Are One To One Or Onto . . . . . . . . . . . . . . . . 186
10.1.4 The General Solution Of A Linear System . . . . . . . . . . . . . . . . 187
10.1.5 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 190
V
Eigenvalues, Eigenvectors, Determinants, Diagonalization
193
11 Determinants 2,3 Oct.
197
11.1 Basic Techniques And Properties . . . . . . . . . . . . . . . . . . . . . . . . . 197
11.1.1 Cofactors And 2 Ã— 2 Determinants . . . . . . . . . . . . . . . . . . . . 197
11.1.2 The Determinant Of A Triangular Matrix . . . . . . . . . . . . . . . . 200
11.1.3 Properties Of Determinants . . . . . . . . . . . . . . . . . . . . . . . . 201
11.1.4 Finding Determinants Using Row Operations . . . . . . . . . . . . . . 203
11.1.5 A Formula For The Inverse . . . . . . . . . . . . . . . . . . . . . . . . 204
12 Eigenvalues And Eigenvectors Of A Matrix 4-6 Oct.
209
12.0.6 Deï¬nition Of Eigenvectors And Eigenvalues . . . . . . . . . . . . . . . 209
12.0.7 Finding Eigenvectors And Eigenvalues . . . . . . . . . . . . . . . . . . 211
12.0.8 A Warning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
12.0.9 Defective And Nondefective Matrices . . . . . . . . . . . . . . . . . . . 215
12.0.10Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
12.0.11Migration Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
12.0.12Complex Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
12.0.13The Estimation Of Eigenvalues . . . . . . . . . . . . . . . . . . . . . . 228
12.1 The Mathematical Theory Of Determinantsâˆ—. . . . . . . . . . . . . . . 229
12.1.1
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
12.2 The Cayley Hamilton Theoremâˆ—. . . . . . . . . . . . . . . . . . . . . . . 241
12.2.1 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 242
VI
Curves, Curvilinear Motion, Surfaces
253
13 Quadric Surfaces 9 Oct.
257

6
CONTENTS
14 Curves In Space 10,11 Oct.
261
14.1 Limits Of A Vector Valued Function Of One Variable
. . . . . . . . . . . . . 261
14.2 The Derivative And Integral . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
14.2.1 Arc Length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
14.2.2 Geometric And Physical Signiï¬cance Of The Derivative . . . . . . . . 267
14.2.3 Diï¬€erentiation Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
14.2.4 Leibnizâ€™s Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
14.2.5 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 271
15 Newtonâ€™s Laws Of Motionâˆ—
273
15.0.6 Kinetic Energyâˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
15.0.7 Impulse And Momentumâˆ—. . . . . . . . . . . . . . . . . . . . . . . . . 278
15.0.8 Conservation Of Momentumâˆ—. . . . . . . . . . . . . . . . . . . . . . . 278
15.0.9 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 279
16 Physics Of Curvilinear Motion 12 Oct.
281
16.0.10The Acceleration In Terms Of The Unit Tangent And Normal . . . . . 281
16.0.11The Curvature Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
16.0.12The Circle Of Curvature* . . . . . . . . . . . . . . . . . . . . . . . . . 286
16.1 Geometry Of Space Curvesâˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . 288
16.2 Independence Of Parameterizationâˆ—
. . . . . . . . . . . . . . . . . . . . 291
16.2.1 Hard Calculusâˆ—. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
16.2.2 Independence Of Parameterizationâˆ—
. . . . . . . . . . . . . . . . . . . 295
16.3 Product Rule For Matricesâˆ—. . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
16.4 Moving Coordinate Systemsâˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . . . 298
VII
Functions Of Many Variables
301
17 Functions Of Many Variables 16 Oct.
305
17.1 The Graph Of A Function Of Two Variables . . . . . . . . . . . . . . . . . . . 305
17.2 The Domain Of A Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
17.3 Open And Closed Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
17.4 Continuous Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
17.5 Suï¬ƒcient Conditions For Continuity . . . . . . . . . . . . . . . . . . . . . . . 312
17.6 Properties Of Continuous Functions
. . . . . . . . . . . . . . . . . . . . . . . 313
18 Limits Of A Function 17-23 Oct.
315
18.1 The Directional Derivative And Partial Derivatives . . . . . . . . . . . . . . . 318
18.1.1 The Directional Derivative
. . . . . . . . . . . . . . . . . . . . . . . . 318
18.1.2 Partial Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
18.1.3 Mixed Partial Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . 323
18.2 Some Fundamentalsâˆ—. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
18.2.1 The Nested Interval Lemmaâˆ—. . . . . . . . . . . . . . . . . . . . . . . 328
18.2.2 The Extreme Value Theoremâˆ—
. . . . . . . . . . . . . . . . . . . . . . 329
18.2.3 Sequences And Completenessâˆ—
. . . . . . . . . . . . . . . . . . . . . . 330
18.2.4 Continuity And The Limit Of A Sequenceâˆ—
. . . . . . . . . . . . . . . 333

CONTENTS
7
VIII
Diï¬€erentiability
335
19 Diï¬€erentiability 24-26 Oct.
339
19.1 The Deï¬nition Of Diï¬€erentiability
. . . . . . . . . . . . . . . . . . . . . . . . 339
19.2 C1 Functions And Diï¬€erentiability . . . . . . . . . . . . . . . . . . . . . . . . 341
19.3 The Directional Derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
19.3.1 Separable Diï¬€erential Equationsâˆ—. . . . . . . . . . . . . . . . . . . . . 344
19.3.2 Exercises With Answersâˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . 347
19.3.3 A Heat Seaking Particle . . . . . . . . . . . . . . . . . . . . . . . . . . 348
19.4 The Chain Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
19.4.1 Related Rates Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 351
19.5 Normal Vectors And Tangent Planes 26 Oct. . . . . . . . . . . . . . . . 353
20 Extrema Of Functions Of Several Variables 30 Oct.
355
20.1 Local Extrema
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
20.2 The Second Derivative Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
20.2.1 Functions Of Two Variables . . . . . . . . . . . . . . . . . . . . . . . . 358
20.2.2 Functions Of Many Variablesâˆ—
. . . . . . . . . . . . . . . . . . . . . . 359
20.3 Lagrange Multipliers, Constrained Extrema 31 Oct. . . . . . . . . . . 362
20.3.1 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 367
21 The Derivative Of Vector Valued Functions, What Is The Derivative?âˆ—
371
21.1 C1 Functionsâˆ—. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
21.2 The Chain Ruleâˆ—. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
21.2.1 The Chain Rule For Functions Of One Variableâˆ—. . . . . . . . . . . . 377
21.2.2 The Chain Rule For Functions Of Many Variablesâˆ—. . . . . . . . . . . 377
21.2.3 The Derivative Of The Inverse Functionâˆ—
. . . . . . . . . . . . . . . . 381
21.2.4 Acceleration In Spherical Coordinatesâˆ—. . . . . . . . . . . . . . . . . . 381
21.3 Proof Of The Chain Ruleâˆ—. . . . . . . . . . . . . . . . . . . . . . . . . . . 384
21.4 Proof Of The Second Derivative Testâˆ—. . . . . . . . . . . . . . . . . . . 386
22 Implicit Function Theoremâˆ—
389
22.1 The Method Of Lagrange Multipliers . . . . . . . . . . . . . . . . . . . . . . . 393
22.2 The Local Structure Of C1 Mappings
. . . . . . . . . . . . . . . . . . . . . . 394
IX
Multiple Integrals
397
23 The Riemann Integral On Rn
403
23.1 Methods For Double Integrals 1 Nov.
. . . . . . . . . . . . . . . . . . . 403
23.1.1 Density Mass And Center Of Mass . . . . . . . . . . . . . . . . . . . . 410
23.2 Double Integrals In Polar Coordinates . . . . . . . . . . . . . . . . . . . . . . 411
23.3 Methods For Triple Integrals 2-7 Nov. . . . . . . . . . . . . . . . . . . . 416
23.3.1 Deï¬nition Of The Integral . . . . . . . . . . . . . . . . . . . . . . . . . 416
23.3.2 Iterated Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
23.3.3 Mass And Density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
23.3.4 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 423

8
CONTENTS
24 The Integral In Other Coordinates 8-10 Nov.
427
24.1 Diï¬€erent Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
24.1.1 Review Of Polar Coordinates . . . . . . . . . . . . . . . . . . . . . . . 428
24.1.2 General Two Dimensional Coordinates . . . . . . . . . . . . . . . . . . 429
24.1.3 Three Dimensions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
24.1.4 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 436
24.2 The Moment Of Inertia âˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . . . 442
24.2.1 The Spinning Topâˆ—. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
24.2.2 Kinetic Energyâˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
24.3 Finding The Moment Of Inertia And Center Of Mass 13 Nov.
. . . 447
24.4 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
X
Line Integrals
455
25 Line Integrals 14 Nov.
459
25.0.1
Orientations And Smooth Curves
. . . . . . . . . . . . . . . . . . . . 459
25.0.2 The Integral Of A Function Deï¬ned On A Smooth Curve . . . . . . . 461
25.0.3 Vector Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
25.0.4 Line Integrals And Work
. . . . . . . . . . . . . . . . . . . . . . . . . 464
25.0.5 Another Notation For Line Integrals . . . . . . . . . . . . . . . . . . . 466
25.0.6 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 467
25.1 Path Independent Line Integrals 15 Nov. . . . . . . . . . . . . . . . . . 468
25.1.1 Finding The Scalar Potential, (Recover The Function From Its Gradient)469
25.1.2 Terminology
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471
XI
Greenâ€™s Theorem, Integrals On Surfaces
473
26 Greenâ€™s Theorem 20 Nov.
477
26.1 An Alternative Explanation Of Greenâ€™s Theorem . . . . . . . . . . . . . . . . 479
26.2 Area And Greenâ€™s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
27 The Integral On Two Dimensional Surfaces In R3 27-28 Nov.
485
27.1 Parametrically Deï¬ned Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . 485
27.2 The Two Dimensional Area In R3 . . . . . . . . . . . . . . . . . . . . . . . . . 487
27.2.1 Surfaces Of The Form z = f (x, y)
. . . . . . . . . . . . . . . . . . . . 494
27.3 Flux . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496
27.3.1 Exercises With Answers . . . . . . . . . . . . . . . . . . . . . . . . . . 496
XII
Divergence Theorem
501
28 The Divergence Theorem 29-30 Nov.
505
28.1 Divergence Of A Vector Field
. . . . . . . . . . . . . . . . . . . . . . . . 505
28.2 The Divergence Theorem
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506
28.2.1 Coordinate Free Concept Of Divergence, Flux Density . . . . . . . . . 510
28.3 The Weak Maximum Principleâˆ—. . . . . . . . . . . . . . . . . . . . . . . 510
28.4 Some Applications Of The Divergence Theoremâˆ—. . . . . . . . . . . . 511
28.4.1 Hydrostatic Pressureâˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . . . 511
28.4.2 Archimedes Law Of Buoyancyâˆ—. . . . . . . . . . . . . . . . . . . . . . 512
28.4.3 Equations Of Heat And Diï¬€usionâˆ—. . . . . . . . . . . . . . . . . . . . 512

CONTENTS
9
28.4.4 Balance Of Massâˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
28.4.5 Balance Of Momentumâˆ—. . . . . . . . . . . . . . . . . . . . . . . . . . 514
28.4.6 Bernoulliâ€™s Principleâˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . . . 519
28.4.7 The Wave Equationâˆ—. . . . . . . . . . . . . . . . . . . . . . . . . . . . 520
28.4.8 A Negative Observationâˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . 521
28.4.9 Electrostaticsâˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521
XIII
Stokeâ€™s Theorem
523
29 Stokeâ€™s Theorem 4-5 Dec.
527
29.1 Curl Of A Vector Field . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527
29.2 Greenâ€™s Theorem, A Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
29.3 Stokeâ€™s Theorem From Greenâ€™s Theorem . . . . . . . . . . . . . . . . . . . . . 529
29.3.1 Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 532
29.3.2 Conservative Vector Fields And Stokeâ€™s Theorem . . . . . . . . . . . . 533
29.3.3 Some Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
29.3.4 Vector Identitiesâˆ—. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
29.3.5 Vector Potentialsâˆ—
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
29.3.6 Maxwellâ€™s Equations And The Wave Equationâˆ—. . . . . . . . . . . . . 536
XIV
Some Iterative Techniques For Linear Algebra
539
30 Iterative Methods For Linear Systems
541
30.1
Jacobi Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541
30.2 Gauss Seidel Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545
31 Iterative Methods For Finding Eigenvalues
551
31.1 The Power Method For Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . 551
31.1.1 Rayleigh Quotient
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555
31.2 The Shifted Inverse Power Method . . . . . . . . . . . . . . . . . . . . . . . . 556
XV
The Correct Version Of The Riemann Integral âˆ—
563
A The Theory Of The Riemann Integralâˆ—âˆ—
565
A.1 An Important Warning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565
A.2 The Deï¬nition Of The Riemann Integral . . . . . . . . . . . . . . . . . . . . . 565
A.3 Basic Properties
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568
A.4 Iterated Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581
A.5 The Change Of Variables Formula
. . . . . . . . . . . . . . . . . . . . . . . . 584
A.6
Some Observations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591
Copyright câƒ2005,

10
CONTENTS

Introduction
These are the lecture notes for my section of Math 302. They are pretty much in the order
of the syllabus for the course. You donâ€™t need to read the starred sections and chapters and
subsections. These are there to provide depth in the subject. To quote from the mission
statement of BYU, â€œ Depth comes when students realize the eï¬€ect of rigorous, coherent, and
progressively more sophisticated study. Depth helps students distinguish between what is
fundamental and what is only peripheral; it requires focus, provides intense concentration.
...â€ To see clearly what is peripheral you need to read the fundamental and diï¬ƒcult concepts,
most of which are presented in the starred sections. These are not always easy to read and I
have indicated the most diï¬ƒcult with a picture of a dragon. Some are not much harder than
what is presented in the course. A good example is the one which deï¬nes the derivative. If
you donâ€™t learn this material, you will have trouble understanding many fundamental topics.
Some which come to mind are basic continuum mechanics (The deformation gradient is a
derivative.) and Newtonâ€™s method for solving nonlinear systems of equations.(The entire
method involves looking at the derivative and its inverse.)
If you donâ€™t want to learn
anything more than what you will be tested on, then you can omit these sections. This is
up to you. It is your choice.
A word about notation might help. Most of the linear algebra works in any ï¬eld. Exam-
ples are the rational numbers, the integers modulo a prime number, the complex numbers,
or the real numbers. Therefore, I will often write F to denote this ï¬eld. If you donâ€™t like
this, just put in R and you will be ï¬ne. This is the main one of interest. However, I at least
want you to realize that everything holds for the complex numbers in addition to the reals.
In many applications this is essential so it does not hurt to begin to realize this. Also, I will
write vectors in terms of bold letters. Thus u will denote a vector. Sometimes people write
something like âƒ—u to indicate a vector. However, the bold face is the usual notation so I am
using this in these notes. On the board, I will likely write the other notation. The norm
or length of a vector is often written as ||u|| . I will usually write it as |u| . This is standard
notation also although most books use the double bar notation. The notation I am using
emphasizes that the norm is just like the absolute value which is an important connection
to make. It also seems less cluttered. You need to understand that either notation means
the same thing.
For a more substantial treatment of certain topics, there is a complete calculus book on
my web page. There are signiï¬cant generalizations which unify all the notions of volume
into one beautiful theory. I have not pursued this topic in these notes but it is in the calculus
book. There are other things also, especially all the one variable theory if you need a review.
11

12
INTRODUCTION

Part I
Vectors, Vector Products, Lines
13


15
Outcomes
Vectors in Two and Three Dimensions
A. Evaluate the distance between two points in 3-space.
B. Deï¬ne vector and identify examples of vectors.
C. Be able to represent a vector in each of the following ways for n = 2, 3:
(a) as a directed arrow in n-space
(b) as an ordered n-tuple
(c) as a linear combinations of unit coordinate vectors
D. Carry out the vector operations:
(a) addition
(b) scalar multiplication
(c) magnitude (or norm or length)
(d) normalize a vector (ï¬nd the vector of unit length in the direction of a given
vector)
E. Represent the operations of vector addition, scalar multiplication and norm geomet-
rically.
F. Recall, apply and verify the basic properties of vector addition, scalar multiplication
and norm.
G. Model and solve application problems using vectors.
Reading: Multivariable Calculus 1.1, Linear Algebra 1.1
Outcome Mapping:
A. 1,2,4
B. A1,A2
C. 8,9,11,13,14
D. 9,11,12,13
E. 8,10
F. 17,A3,A4
G. A5
Vector Products
A. Evaluate a dot product from the angle formula or the coordinate formula.
B. Interpret the dot product geometrically.
C. Evaluate the following using the dot product:
i. the angle between two vectors.

16
ii. the magnitude of a vector.
iii. the projection of a vector onto another vector.
iv. the component of a vector in the direction of another vector.
v. the work done by a constant force on an object.
D. Evaluate a cross product from the angle formula or the coordinate formula.
E. Interpret the cross product geometrically.
F. Evaluate the following using the cross product:
i. the area of a parallelogram.
ii. the area or a triangle.
iii. physical quantities such as moment of force and angular velocity.
G. Find the volume of a parallelepiped using the scalar triple product.
H. Recall, apply and derive the algebraic properties of the dot and cross products.
Reading: Multivariable Calculus 1.2-3, Linear Algebra 1.2
Outcome Mapping:
A. 1,2bd,3,7
B. 3
C. 2egi
D. 2kmp,7dgh
E. 4
F. 5,15,B5
G. 6,B6
H. 8,17,B1,B2,B3,B4
Lines in Space
A. Represent a line in 3-space by a vector parameterization, a set of scalar parametric
equations or using symmetric form.
B. Find a parameterization of a line given information about
(a) a point of the line and the direction of the line or
(b) two points contained in the line.
(c) the direction cosines of the line.
C. Determine the direction of a line given its parameterization.
D. Find the angle between two lines.
E. Determine a point of intersection between a line and a surface.

17
Reading: Multivariable Calculus 1.5, Linear Algebra 1.3
Outcome Mapping:
A. 3,4
B. 3,4
C. 1
D. 2
E. 11,14

18

Vectors And Points In Rn 5
Sept.
2.1
Rn Ordered nâˆ’tuples
The notation, Rn refers to the collection of ordered lists of n real numbers. More precisely,
consider the following deï¬nition.
Deï¬nition 2.1.1 Deï¬ne
Rn â‰¡{(x1, Â· Â· Â·, xn) : xj âˆˆR for j = 1, Â· Â· Â·, n} .
(x1, Â· Â· Â·, xn) = (y1, Â· Â· Â·, yn) if and only if for all j = 1, Â·Â·Â·, n, xj = yj. When (x1, Â· Â· Â·, xn) âˆˆRn,
it is conventional to denote (x1, Â· Â· Â·, xn) by the single bold face letter, x. The numbers, xj
are called the coordinates. The set
{(0, Â· Â· Â·, 0, t, 0, Â· Â· Â·, 0) : t âˆˆR }
for t in the ith slot is called the ith coordinate axis coordinate axis, the xi axis for short.
The point 0 â‰¡(0, Â· Â· Â·, 0) is called the origin.
Thus (1, 2, 4) âˆˆR3 and (2, 1, 4) âˆˆR3 but (1, 2, 4) Ì¸= (2, 1, 4) because, even though the
same numbers are involved, they donâ€™t match up. In particular, the ï¬rst entries are not
equal.
Why would anyone be interested in such a thing? First consider the case when n = 1.
Then from the deï¬nition, R1 = R. Recall that R is identiï¬ed with the points of
a line.
Look at the number line again. Observe that this amounts to identifying a point on this
line with a real number. In other words a real number determines where you are on this
line. Now suppose n = 2 and consider two lines which intersect each other at right angles
as shown in the following picture.
2
6
Â· (2, 6)
âˆ’8
3
Â·
(âˆ’8, 3)
19

20
VECTORS AND POINTS IN RN 5 SEPT.
Notice how you can identify a point shown in the plane with the ordered pair, (2, 6) .
You go to the right a distance of 2 and then up a distance of 6. Similarly, you can identify
another point in the plane with the ordered pair (âˆ’8, 3) . Go to the left a distance of 8 and
then up a distance of 3. The reason you go to the left is that there is a âˆ’sign on the eight.
From this reasoning, every ordered pair determines a unique point in the plane. Conversely,
taking a point in the plane, you could draw two lines through the point, one vertical and the
other horizontal and determine unique points, x1 on the horizontal line in the above picture
and x2 on the vertical line in the above picture, such that the point of interest is identiï¬ed
with the ordered pair, (x1, x2) . In short, points in the plane can be identiï¬ed with ordered
pairs similar to the way that points on the real line are identiï¬ed with real numbers. Now
suppose n = 3. As just explained, the ï¬rst two coordinates determine a point in a plane.
Letting the third component determine how far up or down you go, depending on whether
this number is positive or negative, this determines a point in space. Thus, (1, 4, âˆ’5) would
mean to determine the point in the plane that goes with (1, 4) and then to go below this
plane a distance of 5 to obtain a unique point in space. You see that the ordered triples
correspond to points in space just as the ordered pairs correspond to points in a plane and
single real numbers correspond to points on a line.
You canâ€™t stop here and say that you are only interested in n â‰¤3. What if you were
interested in the motion of two objects?
You would need three coordinates to describe
where the ï¬rst object is and you would need another three coordinates to describe where
the other object is located. Therefore, you would need to be considering R6. If the two
objects moved around, you would need a time coordinate as well. As another example,
consider a hot object which is cooling and suppose you want the temperature of this object.
How many coordinates would be needed? You would need one for the temperature, three
for the position of the point in the object and one more for the time. Thus you would need
to be considering R5. Many other examples can be given. Sometimes n is very large. This
is often the case in applications to business when they are trying to maximize proï¬t subject
to constraints. It also occurs in numerical analysis when people try to solve hard problems
on a computer.
There are other ways to identify points in space with three numbers but the one presented
is the most basic. In this case, the coordinates are known as Cartesian coordinates after
Descartes1 who invented this idea in the ï¬rst half of the seventeenth century. I will often
not bother to draw a distinction between the point in n dimensional space and its Cartesian
coordinates.
2.2
Vectors And Algebra In Rn
There are two algebraic operations done with points of Rn. One is addition and the other
is multiplication by numbers, called scalars.
Deï¬nition 2.2.1 If x âˆˆRn and a is a number, also called a scalar, then ax âˆˆRn
is deï¬ned by
ax = a (x1, Â· Â· Â·, xn) â‰¡(ax1, Â· Â· Â·, axn) .
(2.1)
This is known as scalar multiplication. If x, y âˆˆRn then x + y âˆˆRn and is deï¬ned by
x + y = (x1, Â· Â· Â·, xn) + (y1, Â· Â· Â·, yn)
â‰¡(x1 + y1, Â· Â· Â·, xn + yn)
(2.2)
1RenÂ´e Descartes 1596-1650 is often credited with inventing analytic geometry although it seems the ideas
were actually known much earlier. He was interested in many diï¬€erent subjects, physiology, chemistry, and
physics being some of them. He also wrote a large book in which he tried to explain the book of Genesis
scientiï¬cally. Descartes ended up dying in Sweden.

2.3.
GEOMETRIC MEANING OF VECTORS
21
An element of Rn, x â‰¡(x1, Â· Â· Â·, xn) is often called a vector. The above deï¬nition is known
as vector addition.
With this deï¬nition, the algebraic properties satisfy the conclusions of the following
theorem.
Theorem 2.2.2 For v, w vectors in Rn and Î±, Î² scalars, (real numbers), the fol-
lowing hold.
v + w = w + v,
(2.3)
the commutative law of addition,
(v + w) + z = v+ (w + z) ,
(2.4)
the associative law for addition,
v + 0 = v,
(2.5)
the existence of an additive identity,
v+ (âˆ’v) = 0,
(2.6)
the existence of an additive inverse, Also
Î± (v + w) = Î±v+Î±w,
(2.7)
(Î± + Î²) v =Î±v+Î²v,
(2.8)
Î± (Î²v) = Î±Î² (v) ,
(2.9)
1v = v.
(2.10)
In the above 0 = (0, Â· Â· Â·, 0).
You should verify these properties all hold. For example, consider 2.7
Î± (v + w) = Î± (v1 + w1, Â· Â· Â·, vn + wn)
= (Î± (v1 + w1) , Â· Â· Â·, Î± (vn + wn))
= (Î±v1 + Î±w1, Â· Â· Â·, Î±vn + Î±wn)
= (Î±v1, Â· Â· Â·, Î±vn) + (Î±w1, Â· Â· Â·, Î±wn)
= Î±v + Î±w.
As usual subtraction is deï¬ned as x âˆ’y â‰¡x+ (âˆ’y) .
2.3
Geometric Meaning Of Vectors
Deï¬nition 2.3.1 Let x = (x1, Â· Â· Â·, xn) be the coordinates of a point in Rn. Imagine
an arrow with its tail at 0 = (0, Â· Â· Â·, 0) and its point at x as shown in the following picture
in the case of R3.
Â¡
Â¡
Â¡

3r
(x1, x2, x3) = x
Then this arrow is called the position vector of the point, x.

22
VECTORS AND POINTS IN RN 5 SEPT.
Thus every point determines such a vector and conversely, every such vector (arrow)
which has its tail at 0 determines a point of Rn, namely the point of Rn which coincides
with the point of the vector.
Imagine taking the above position vector and moving it around, always keeping it point-
ing in the same direction as shown in the following picture.
Â¡
Â¡
Â¡

3r
(x1, x2, x3) = x

3

3

3
After moving it around, it is regarded as the same vector because it points in the same
direction and has the same length.2Thus each of the arrows in the above picture is regarded
as the same vector.
The components of this vector are the numbers, x1, Â· Â· Â·, xn. You
should think of these numbers as directions for obtainng an arrow. Starting at some point,
(a1, a2, Â· Â· Â·, an) in Rn, you move to the point (a1 + x1, Â· Â· Â·, an) and from there to the point
(a1 + x1, a2 + x2, a3 Â· Â·Â·, an) and then to (a1 + x1, a2 + x2, a3 + x3, Â· Â· Â·, an) and continue this
way until you obtain the point (a1 + x1, a2 + x2, Â· Â· Â·, an + xn) . The arrow having its tail
at (a1, a2, Â· Â· Â·, an) and its point at (a1 + x1, a2 + x2, Â· Â· Â·, an + xn) looks just like the arrow
which has its tail at 0 and its point at (x1, Â· Â· Â·, xn) so it is regarded as the same vector.
2.4
Geometric Meaning Of Vector Addition
It was explained earlier that an element of Rn is an n tuple of numbers and it was also
shown that this can be used to determine a point in three dimensional space in the case
where n = 3 and in two dimensional space, in the case where n = 2. This point was speciï¬ed
relative to some coordinate axes.
Consider the case where n = 3 for now. If you draw an arrow from the point in three
dimensional space determined by (0, 0, 0) to the point (a, b, c) with its tail sitting at the
point (0, 0, 0) and its point at the point (a, b, c) , this arrow is called the position vector
of the point determined by u â‰¡(a, b, c) . One way to get to this point is to start at (0, 0, 0)
and move in the direction of the x1 axis to (a, 0, 0) and then in the direction of the x2 axis
to (a, b, 0) and ï¬nally in the direction of the x3 axis to (a, b, c) . It is evident that the same
arrow (vector) would result if you began at the point, v â‰¡(d, e, f) , moved in the direction
of the x1 axis to (d + a, e, f) , then in the direction of the x2 axis to (d + a, e + b, f) , and
ï¬nally in the x3 direction to (d + a, e + b, f + c) only this time, the arrow would have its
tail sitting at the point determined by v â‰¡(d, e, f) and its point at (d + a, e + b, f + c) . It
is said to be the same arrow (vector) because it will point in the same direction and have
the same length. It is like you took an actual arrow, the sort of thing you shoot with a bow,
and moved it from one location to another keeping it pointing the same direction. This
is illustrated in the following picture in which v + u is illustrated. Note the parallelogram
determined in the picture by the vectors u and v.
2I will discuss how to deï¬ne length later. For now, it is only necessary to observe that the length should
be deï¬ned in such a way that it does not change when such motion takes place.

2.5.
DISTANCE BETWEEN POINTS IN RN
23
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡

u
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
v
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
u + v
@
@
I
Â¡
Â¡
Â¡
Â¡

u
x1
x3
x2
Thus the geometric signiï¬cance of (d, e, f) + (a, b, c) = (d + a, e + b, f + c) is this. You
start with the position vector of the point (d, e, f) and at its point, you place the vector
determined by (a, b, c) with its tail at (d, e, f) . Then the point of this last vector will be
(d + a, e + b, f + c) . This is the geometric signiï¬cance of vector addition. Also, as shown
in the picture, u + v is the directed diagonal of the parallelogram determined by the two
vectors u and v. A similar interpretation holds in Rn, n > 3 but I canâ€™t draw a picture in
this case.
Since the convention is that identical arrows pointing in the same direction represent
the same vector, the geometric signiï¬cance of vector addition is as follows in any number of
dimensions.
Procedure 2.4.1 Let u and v be two vectors. Slide v so that the tail of v is on the
point of u. Then draw the arrow which goes from the tail of u to the point of the slid vector,
v. This arrow represents the vector u + v.
-Â¡
Â¡
Â¡
Â¡

Â©Â©Â©Â©Â©Â©Â©Â©
*
u
u + v
v
2.5
Distance Between Points In Rn
How is distance between two points in Rn deï¬ned?
Deï¬nition 2.5.1 Let x = (x1, Â· Â· Â·, xn) and y = (y1, Â· Â· Â·, yn) be two points in Rn.
Then |x âˆ’y| to indicates the distance between these points and is deï¬ned as
distance between x and y â‰¡|x âˆ’y| â‰¡
Ãƒ n
X
k=1
|xk âˆ’yk|2
!1/2
.

24
VECTORS AND POINTS IN RN 5 SEPT.
This is called the distance formula. Thus |x| â‰¡|x âˆ’0| . The symbol, B (a, r) is deï¬ned
by
B (a, r) â‰¡{x âˆˆRn : |x âˆ’a| < r} .
This is called an open ball of radius r centered at a. It means all points in Rn which are
closer to a than r.
First of all note this is a generalization of the notion of distance in R. There the distance
between two points, x and y was given by the absolute value of their diï¬€erence. Thus |x âˆ’y|
is equal to the distance between these two points on R. Now |x âˆ’y| =
Â³
(x âˆ’y)2Â´1/2
where
the square root is always the positive square root. Thus it is the same formula as the above
deï¬nition except there is only one term in the sum. Geometrically, this is the right way to
deï¬ne distance which is seen from the Pythagorean theorem. Often people use two lines
to denote this distance, ||x âˆ’y||. However, I want to emphasize this is really just like the
absolute value. Also, the notation I am using is fairly standard.
Consider the following picture in the case that n = 2.
(x1, x2)
(y1, x2)
(y1, y2)
There are two points in the plane whose Cartesian coordinates are (x1, x2) and (y1, y2)
respectively. Then the solid line joining these two points is the hypotenuse of a right triangle
which is half of the rectangle shown in dotted lines. What is its length? Note the lengths
of the sides of this triangle are |y1 âˆ’x1| and |y2 âˆ’x2| . Therefore, the Pythagorean theorem
implies the length of the hypotenuse equals
Â³
|y1 âˆ’x1|2 + |y2 âˆ’x2|2Â´1/2
=
Â³
(y1 âˆ’x1)2 + (y2 âˆ’x2)2Â´1/2
which is just the formula for the distance given above. In other words, this distance deï¬ned
above is the same as the distance of plane geometry in which the Pythagorean theorem
holds.
Now suppose n = 3 and let (x1, x2, x3) and (y1, y2, y3) be two points in R3. Consider the
following picture in which one of the solid lines joins the two points and a dotted line joins

2.5.
DISTANCE BETWEEN POINTS IN RN
25
the points (x1, x2, x3) and (y1, y2, x3) .
(x1, x2, x3)
(y1, x2, x3)
(y1, y2, x3)
(y1, y2, y3)
By the Pythagorean theorem, the length of the dotted line joining (x1, x2, x3) and
(y1, y2, x3) equals
Â³
(y1 âˆ’x1)2 + (y2 âˆ’x2)2Â´1/2
while the length of the line joining (y1, y2, x3) to (y1, y2, y3) is just |y3 âˆ’x3| . Therefore, by
the Pythagorean theorem again, the length of the line joining the points (x1, x2, x3) and
(y1, y2, y3) equals
(Â·Â³
(y1 âˆ’x1)2 + (y2 âˆ’x2)2Â´1/2Â¸2
+ (y3 âˆ’x3)2
)1/2
=
Â³
(y1 âˆ’x1)2 + (y2 âˆ’x2)2 + (y3 âˆ’x3)2Â´1/2
,
which is again just the distance formula above.
This completes the argument that the above deï¬nition is reasonable. Of course you
cannot continue drawing pictures in ever higher dimensions but there is no problem with
the formula for distance in any number of dimensions. Here is an example.
Example 2.5.2 Find the distance between the points in R4, a = (1, 2, âˆ’4, 6) and b = (2, 3, âˆ’1, 0)
Use the distance formula and write
|a âˆ’b|2 = (1 âˆ’2)2 + (2 âˆ’3)2 + (âˆ’4 âˆ’(âˆ’1))2 + (6 âˆ’0)2 = 47
Therefore, |a âˆ’b| =
âˆš
47.
All this amounts to deï¬ning the distance between two points as the length of a straight
line joining these two points. However, there is nothing sacred about using straight lines.
One could deï¬ne the distance to be the length of some other sort of line joining these points.
It wonâ€™t be done in this book but sometimes this sort of thing is done.
Another convention which is usually followed, especially in R2 and R3 is to denote the
ï¬rst component of a point in R2 by x and the second component by y. In R3 it is customary
to denote the ï¬rst and second components as just described while the third component is
called z.
Example 2.5.3 Describe the points which are at the same distance between (1, 2, 3) and
(0, 1, 2) .

26
VECTORS AND POINTS IN RN 5 SEPT.
Let (x, y, z) be such a point. Then
q
(x âˆ’1)2 + (y âˆ’2)2 + (z âˆ’3)2 =
q
x2 + (y âˆ’1)2 + (z âˆ’2)2.
Squaring both sides
(x âˆ’1)2 + (y âˆ’2)2 + (z âˆ’3)2 = x2 + (y âˆ’1)2 + (z âˆ’2)2
and so
x2 âˆ’2x + 14 + y2 âˆ’4y + z2 âˆ’6z = x2 + y2 âˆ’2y + 5 + z2 âˆ’4z
which implies
âˆ’2x + 14 âˆ’4y âˆ’6z = âˆ’2y + 5 âˆ’4z
and so
2x + 2y + 2z = âˆ’9.
(2.11)
Since these steps are reversible, the set of points which is at the same distance from the two
given points consists of the points, (x, y, z) such that 2.11 holds.
There are certain properties of the distance which are obvious. Two of them which follow
directly from the deï¬nition are
|x âˆ’y| = |y âˆ’x| ,
|x âˆ’y| â‰¥0 and equals 0 only if y = x.
The third fundamental property of distance is known as the triangle inequality. Recall that
in any triangle the sum of the lengths of two sides is always at least as large as the third
side. I will show you a proof of this pretty soon. This is usually stated as
|x + y| â‰¤|x| + |y| .
Here is a picture which illustrates the statement of this inequality in terms of geometry.
-

3
Â¢
Â¢
Â¢
Â¢
x + y
x
y
2.6
Geometric Meaning Of Scalar Multiplication
As discussed earlier, x = (x1, x2, x3) determines a vector. You draw the line from 0 to
x placing the point of the vector on x. What is the length of this vector?
The length
of this vector is deï¬ned to equal |x| as in Deï¬nition 2.5.1. Thus the length of x equals
p
x2
1 + x2
2 + x2
3. When you multiply x by a scalar, Î±, you get (Î±x1, Î±x2, Î±x3) and the length
of this vector is deï¬ned as
rÂ³
(Î±x1)2 + (Î±x2)2 + (Î±x3)2Â´
= |Î±|
p
x2
1 + x2
2 + x2
3. Thus the
following holds.
|Î±x| = |Î±| |x| .
In other words, multiplication by a scalar magniï¬es the length of the vector. What about
the direction? You should convince yourself by drawing a picture that if Î± is negative, it
causes the resulting vector to point in the opposite direction while if Î± > 0 it preserves the
direction the vector points.

2.6.
GEOMETRIC MEANING OF SCALAR MULTIPLICATION
27
You can think of vectors as quantities which have direction and magnitude, little arrows.
Thus any two little arrows which have the same length and point in the same direction are
considered to be the same vector even if their tails are at diï¬€erent points.
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
You can always slide such an arrow and place its tail at the origin. If the resulting
point of the vector is (a, b, c) , it is clear the length of the little arrow is
âˆš
a2 + b2 + c2.
Geometrically, the way you add two geometric vectors is to place the tail of one on the
point of the other and then to form the vector which results by starting with the tail of the
ï¬rst and ending with this point as illustrated in the following picture. Also when (a, b, c)
is referred to as a vector, you mean any of the arrows which have the same direction and
magnitude as the position vector of this point. Geometrically, for u = (u1, u2, u3) , Î±u is any
of the little arrows which have the same direction and magnitude as (Î±u1, Î±u2, Î±u3) .
Â£
Â£
Â£
Â£
Â£
Â£

1

1
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡

u
v
u + v
The following example is art which illustrates these deï¬nitions and conventions.
Exercise 2.6.1 Here is a picture of two vectors, u and v.
u
Â¡
Â¡
Â¡
Â¡
Â¡

v
HHHHH
j
Sketch a picture of u + v, u âˆ’v, and u+2v.
First here is a picture of u + v. You ï¬rst draw u and then at the point of u you place the
tail of v as shown. Then u + v is the vector which results which is drawn in the following
pretty picture.

28
VECTORS AND POINTS IN RN 5 SEPT.
u
Â¡
Â¡
Â¡
Â¡
Â¡

v
HHHHH
j
u + v

:
Next consider u âˆ’v. This means u+ (âˆ’v) . From the above geometric description of
vector addition, âˆ’v is the vector which has the same length but which points in the opposite
direction to v. Here is a picture.
u
Â¡
Â¡
Â¡
Â¡
Â¡

âˆ’v
H
H
H
H
H
Y
u + (âˆ’v)
6
Finally consider the vector u+2v. Here is a picture of this one also.
u
Â¡
Â¡
Â¡
Â¡
Â¡

2v
HHHHHHHHHH
j
u + 2v
-
2.7
Unit Vectors
Let v be a vector,
v = (v1, Â· Â· Â·, vn) .
The direction vector for v is deï¬ned as v/ |v| . This vector points in the same direction
as v because it consists of the scalar, 1/ |v| times v. This vector is called a unit vector
because |v/ |v|| = |v| / |v| = 1. That is, it has length equal to 1. The process of dividing a
vector by its length is called normalizing. It provides you with a vector which has unit
length and the same direction as the original vector.
2.8
Lines
To begin with consider the case n = 1, 2. In the case where n = 1, the only line is just
R1 = R. Therefore, if x1 and x2 are two diï¬€erent points in R, consider
x = x1 + t (x2 âˆ’x1)

2.8.
LINES
29
where t âˆˆR and the totality of all such points will give R. You see that you can always
solve the above equation for t, showing that every point on R is of this form. Now consider
the plane. Does a similar formula hold? Let (x1, y1) and (x2, y2) be two diï¬€erent points
in R2 which are contained in a line, l. Suppose that x1 Ì¸= x2. Then if (x, y) is an arbitrary
point on l,
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
(x1, y1)
(x2, y2)
(x, y)
Now by similar triangles,
m â‰¡y2 âˆ’y1
x2 âˆ’x1
= y âˆ’y1
x âˆ’x1
and so the point slope form of the line, l, is given as
y âˆ’y1 = m (x âˆ’x1) = y2 âˆ’y1
x2 âˆ’x1
(x âˆ’x1) .
(2.12)
Now consider points of the form
(x, y) = (x1, y1) + t (x2 âˆ’x1, y2 âˆ’y1) .
(2.13)
Do these points satisfy the above equation of the line? Is
y1 + t (y2 âˆ’y1) âˆ’y1 =
Âµ y2 âˆ’y1
x2 âˆ’x1
Â¶
(x1 + t (x2 âˆ’x1) âˆ’x1)?
Yes, this is so. Both sides equal t (y2 âˆ’y1). Conversely, if (x, y) is a point which satisï¬es
the equation, 2.12 does there exist a value of t such that this point is of the form (x1, y1) +
t (x2 âˆ’x1, y2 âˆ’y1)? If the point satisï¬es 2.12, it is of the form
Âµ
x, y1 +
Âµ y2 âˆ’y1
x2 âˆ’x1
Â¶
(x âˆ’x1)
Â¶
.
Now let t =
xâˆ’x1
x2âˆ’x1 so
x = x1 + t (x2 âˆ’x1) .
Then in terms of t, the above reduces to
Âµ
x1 + t (x2 âˆ’x1) , y1 +
Âµ y2 âˆ’y1
x2 âˆ’x1
Â¶
t (x2 âˆ’x1)
Â¶
= (x1, y1) + t (x2 âˆ’x1, y2 âˆ’y1) .
It follows the set of points in R2 obtained from 2.12 and 2.13 are the same. The following
is the deï¬nition of a line in Rn.

30
VECTORS AND POINTS IN RN 5 SEPT.
Deï¬nition 2.8.1 A line in Rn containing the two diï¬€erent points, x1 and x2 is the
collection of points of the form
x = x1 + t
Â¡
x2 âˆ’x1Â¢
where t âˆˆR. This is known as a parametric equation and the variable t is called the
parameter.
Often t denotes time in applications to Physics. Note this deï¬nition agrees with the
usual notion of a line in two dimensions and so this is consistent with earlier concepts.
From now on, you should think of lines in this way. Forget about the stupid special case in
R2 which you had drilled in to your head in high school. The concept of a line is really very
simple and it holds in any number of dimensions, not just in two dimensions. It is given in
the above deï¬nition.
Lemma 2.8.2 Let a, b âˆˆRn with a Ì¸= 0. Then x = ta + b, t âˆˆR, is a line.
Proof: Let x1 = b and let x2 âˆ’x1 = a so that x2 Ì¸= x1. Then ta + b = x1 + t
Â¡
x2 âˆ’x1Â¢
and so x = ta + b is a line containing the two diï¬€erent points, x1 and x2. This proves the
lemma.
Deï¬nition 2.8.3 The vector a in the above lemma is called a direction vector for
the line.
Direction vectors are what it is all about, not slope. Slope is ï¬ne in two dimensions
but we live in three dimensions. Slope is a trivial and stupid concept designed mainly to
give children something to do in high school. The correct and worthwhile notion is that of
direction vector. This is a new concept. Do not try to ï¬t it in to the stuï¬€you saw earlier.
Do not try to put the new wine in the old bottles, to quote the scripture. It only creates
confusion and you do not need that.
Example 2.8.4 Find the line through (1, 2) and (4, 7) .
A vector equation of this line is (x, y) = (1, 2) + t (3, 5) . Now if you want to get the
equation in the form you are used to seeing in high school,
x = 1 + 3t, y = 2 + 5t
Solving the ï¬rst one for t, you get t = (x âˆ’1) /3 and now plugging this in to the second
yields,
y = 2 + 5
Âµx âˆ’1
3
Â¶
so y âˆ’2 = 5
3 (x âˆ’1) which is the usual point slope form for this line.
Now that you know about lines, it is possible to give a more analytical description of a
vector as a directed line segment.
Deï¬nition 2.8.5 Let p and q be two points in Rn, p Ì¸= q. The directed line seg-
ment from p to q, denoted by âˆ’â†’
pq, is deï¬ned to be the collection of points,
x = p + t (q âˆ’p) , t âˆˆ[0, 1]
with the direction corresponding to increasing t. In the deï¬nition, when t = 0, the point p is
obtained and as t increases other points on this line segment are obtained until when t = 1,
you get the point, q. This is what is meant by saying the direction corresponds to increasing
t.

2.8.
LINES
31
Think of âˆ’â†’
pq as an arrow whose point is on q and whose base is at p as shown in the
following picture.






q
p
This line segment is a part of a line from the above Deï¬nition.
Example 2.8.6 Find a parametric equation for the line through the points (1, 2, 0) and
(2, âˆ’4, 6) .
Use the deï¬nition of a line given above to write
(x, y, z) = (1, 2, 0) + t (1, âˆ’6, 6) , t âˆˆR.
The vector (1, âˆ’6, 6) is obtained by (2, âˆ’4, 6) âˆ’(1, 2, 0) as indicated above.
The reason for the word, â€œaâ€, rather than the word, â€œtheâ€ is there are inï¬nitely many
diï¬€erent parametric equations for the same line. To see this replace t with 3s. Then you
obtain a parametric equation for the same line because the same set of points is obtained.
The diï¬€erence is they are obtained from diï¬€erent values of the parameter. What happens
is this: The line is a set of points but the parametric description gives more information
than that. It tells how the set of points are obtained. Obviously, there are many ways to
trace out a given set of points and each of these ways corresponds to a diï¬€erent parametric
equation for the line.
Example 2.8.7 Find a parametric equation for the line which contains the point (1, 2, 0)
and has direction vector, (1, 2, 1) .
From the above this is just
(x, y, z) = (1, 2, 0) + t (1, 2, 1) , t âˆˆR.
(2.14)
Sometimes people elect to write a line like the above in the form
x = 1 + t, y = 2 + 2t, z = t, t âˆˆR.
(2.15)
This is a set of scalar parametric equations which amounts to the same thing as 2.14.
There is one other form for a line which is sometimes considered useful. It is the so called
symmetric form. Consider the line of 2.15. You can solve for the parameter, t to write
t = x âˆ’1, t = y âˆ’2
2
, t = z.
Therefore,
x âˆ’1 = y âˆ’2
2
= z.
This is the symmetric form of the line. Later, it will become clear that this expresses the
line as the intersection of two planes but this is not important at this time.

32
VECTORS AND POINTS IN RN 5 SEPT.
Example 2.8.8 Suppose the symmetric form of a line is
x âˆ’2
3
= y âˆ’1
2
= z + 3.
Find the line in parametric form.
Let t = xâˆ’2
3 , t = yâˆ’1
2
and t = z + 3. Then solving for x, y, z, you get
x = 3t + 2, y = 2t + 1, z = t âˆ’3, t âˆˆR.
Written in terms of vectors this is
(2, 1, âˆ’3) + t (3, 2, 1) = (x, y, z) , t âˆˆR.
Example 2.8.9 A relation such as x2 + y2/4 + z2/9 = 1 describes something called a level
surface. It consists of the points in Rn, (x, y, z) which satisfy the relation. Now here are
parametric equations for a line: x = t, y = 1 + 2t, z = 1 âˆ’t. Find where this line intersects
the above level surface.
This sort of problem is not hard if you donâ€™t panic. The points on the line are of the
form (t, 1 + 2t, 1 âˆ’t) where t âˆˆR. All you have to do is to ï¬nd values of t where this also
satisï¬es the condition for being on the level surface. Thus you need t such that
(t)2 + (1 + 2t)2 /4 + (1 âˆ’t)2 /9 = 1.
This is just a quadratic equation. Expanding the left side yields 19
9 t2 + 13
36 + 7
9t and so you
have to solve the quadratic equation,
19
9 t2 + 13
36 + 7
9t = 1
First simplify this to get the equation
76t2 + 28t âˆ’23 = 0.
Then the quadratic formula gives two solutions for t, t = âˆ’7
38 +
9
38
âˆš
6, âˆ’7
38 âˆ’
9
38
âˆš
6. Now
you can obtain two points of intersection by plugging these values of t into the equation for
the line. The two points are
Âµ
âˆ’7
38 + 9
38
âˆš
6, 12
19 + 9
19
âˆš
6, 45
38 âˆ’9
38
âˆš
6
Â¶
and
Âµ
âˆ’7
38 âˆ’9
38
âˆš
6, 12
19 âˆ’9
19
âˆš
6, 45
38 + 9
38
âˆš
6
Â¶
.
Possibly you would not have guessed these points. You likely would not have found them
by drawing a picture either.
2.9
Vectors And Physics
Suppose you push on something. What is important? There are really two things which
are important, how hard you push and the direction you push. This illustrates the concept
of force.
Also you can see that the concept of a geometric vector is useful for deï¬ning
something like force.

2.9.
VECTORS AND PHYSICS
33
Deï¬nition 2.9.1 Force is a vector. The magnitude of this vector is a measure of
how hard it is pushing. It is measured in units such as Newtons or pounds or tons. Its
direction is the direction in which the push is taking place.
Of course this is a little vague and will be left a little vague until the presentation of
Newtonâ€™s second law later.
Vectors are used to model force and other physical vectors like velocity. What was just
described would be called a force vector. It has two essential ingredients, its magnitude and
its direction. Geometrically think of vectors as directed line segments or arrows as shown in
the following picture in which all the directed line segments are considered to be the same
vector because they have the same direction, the direction in which the arrows point, and
the same magnitude (length).
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Because of this fact that only direction and magnitude are important, it is always possible
to put a vector in a certain particularly simple form. Let âˆ’â†’
pq be a directed line segment or
vector. Then from Deï¬nition 2.8.5 it follows that âˆ’â†’
pq consists of the points of the form
p + t (q âˆ’p)
where t âˆˆ[0, 1] . Subtract p from all these points to obtain the directed line segment con-
sisting of the points
0 + t (q âˆ’p) , t âˆˆ[0, 1] .
The point in Rn, q âˆ’p, will represent the vector.
Geometrically, the arrow, âˆ’â†’
pq, was slid so it points in the same direction and the base is
at the origin, 0. For example, see the following picture.
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
In this way vectors can be identiï¬ed with points of Rn.
Deï¬nition 2.9.2 Let x = (x1, Â· Â· Â·, xn) âˆˆRn.
The position vector of this point is
the vector whose point is at x and whose tail is at the origin, (0, Â· Â· Â·, 0). If x = (x1, Â· Â· Â·, xn)
is called a vector, the vector which is meant is this position vector just described. Another
term associated with this is standard position. A vector is in standard position if the tail
is placed at the origin.
It is customary to identify the point in Rn with its position vector.

34
VECTORS AND POINTS IN RN 5 SEPT.
The magnitude of a vector determined by a directed line segment âˆ’â†’
pq is just the distance
between the point p and the point q. By the distance formula this equals
Ãƒ n
X
k=1
(qk âˆ’pk)2
!1/2
= |p âˆ’q|
and for v any vector in Rn the magnitude of v equals
Â¡Pn
k=1 v2
k
Â¢1/2 = |v|.
Example 2.9.3 Consider the vector, v â‰¡(1, 2, 3) in Rn. Find |v| .
First, the vector is the directed line segment (arrow) which has its base at 0 â‰¡(0, 0, 0)
and its point at (1, 2, 3) . Therefore,
|v| =
p
12 + 22 + 32 =
âˆš
14.
What is the geometric signiï¬cance of scalar multiplication? As noted earlier, if a vector,
vIf a represents the vector, v in the sense that when it is slid to place its tail at the origin,
the element of Rn at its point is a, what is rv?
|rv| =
Ãƒ n
X
k=1
(rai)2
!1/2
=
Ãƒ n
X
k=1
r2 (ai)2
!1/2
=
Â¡
r2Â¢1/2
Ãƒ n
X
k=1
a2
i
!1/2
= |r| |v| .
Thus the magnitude of rv equals |r| times the magnitude of v. If r is positive, then the
vector represented by rv has the same direction as the vector, v because multiplying by the
scalar, r, only has the eï¬€ect of scaling all the distances. Thus the unit distance along any
coordinate axis now has length r and in this rescaled system the vector is represented by a.
If r < 0 similar considerations apply except in this case all the ai also change sign. From
now on, a will be referred to as a vector instead of an element of Rn representing a vector
as just described. The following picture illustrates the eï¬€ect of scalar multiplication.
Â£
Â£Â£
v
Â£
Â£
Â£
Â£Â£
2v Â£
Â£
Â£
Â£Â£
âˆ’2v
Note there are n special vectors which point along the coordinate axes. These are
ei â‰¡(0, Â· Â· Â·, 0, 1, 0, Â· Â· Â·, 0)
where the 1 is in the ith slot and there are zeros in all the other spaces. See the picture in
the case of R3.
-
y
e2
6
z
e3
Â¡
Â¡
Âª
x
e1
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡

2.9.
VECTORS AND PHYSICS
35
The direction of ei is referred to as the ith direction. Given a vector, v = (a1, Â· Â· Â·, an) ,
aiei is the ith component of the vector.
Thus aiei = (0, Â· Â· Â·, 0, ai, 0, Â· Â· Â·, 0) and so this
vector gives something possibly nonzero only in the ith direction. Also, knowledge of the ith
component of the vector is equivalent to knowledge of the vector because it gives the entry
in the ith slot and for v = (a1, Â· Â· Â·, an) ,
v =
n
X
k=1
aiei.
What does addition of vectors mean physically? Suppose two forces are applied to some
object. Each of these would be represented by a force vector and the two forces acting
together would yield an overall force acting on the object which would also be a force vector
known as the resultant. Suppose the two vectors are a = Pn
k=1 aiei and b = Pn
k=1 biei.
Then the vector, a involves a component in the ith direction, aiei while the component in
the ith direction of b is biei. Then it seems physically reasonable that the resultant vector
should have a component in the ith direction equal to (ai + bi) ei. This is exactly what is
obtained when the vectors, a and b are added.
a + b = (a1 + b1, Â· Â· Â·, an + bn) .
=
n
X
i=1
(ai + bi) ei.
Thus the addition of vectors according to the rules of addition in Rn which were presented
earlier, yields the appropriate vector which duplicates the cumulative eï¬€ect of all the vectors
in the sum.
What is the geometric signiï¬cance of vector addition? Suppose u, v are vectors,
u = (u1, Â· Â· Â·, un) , v = (v1, Â· Â· Â·, vn)
Then u + v = (u1 + v1, Â· Â· Â·, un + vn) . How can one obtain this geometrically? Consider the
directed line segment, âˆ’â†’
0u and then, starting at the end of this directed line segment, follow
the directed line segment âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’
u (u + v) to its end, u + v. In other words, place the vector u in
standard position with its base at the origin and then slide the vector v till its base coincides
with the point of u. The point of this slid vector, determines u + v. To illustrate, see the
following picture
Â£
Â£
Â£
Â£
Â£
Â£

1

1
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡

u
v
u + v
Note the vector u + v is the diagonal of a parallelogram determined from the two vec-
tors u and v and that identifying u + v with the directed diagonal of the parallelogram
determined by the vectors u and v amounts to the same thing as the above procedure.
An item of notation should be mentioned here. In the case of Rn where n â‰¤3, it is
standard notation to use i for e1, j for e2, and k for e3. Now here are some applications of
vector addition to some problems.
Example 2.9.4 There are three ropes attached to a car and three people pull on these ropes.
The ï¬rst exerts a force of 2i+3jâˆ’2k Newtons, the second exerts a force of 3i+5j + k Newtons

36
VECTORS AND POINTS IN RN 5 SEPT.
and the third exerts a force of 5i âˆ’j+2k. Newtons. Find the total force in the direction of
i.
To ï¬nd the total force add the vectors as described above.
This gives 10i+7j + k
Newtons. Therefore, the force in the i direction is 10 Newtons.
As mentioned earlier, the Newton is a unit of force like pounds.
Example 2.9.5 An airplane ï¬‚ies North East at 100 miles per hour. Write this as a vector.
A picture of this situation follows.
Â¡
Â¡
Â¡
Â¡
Â¡

The vector has length 100. Now using that vector as the hypotenuse of a right triangle
having equal sides, the sides should be each of length 100/
âˆš
2. Therefore, the vector would
be (100/
âˆš
2)i + (100/
âˆš
2)j.
This example also motivates the concept of velocity.
Deï¬nition 2.9.6 The speed of an object is a measure of how fast it is going. It
is measured in units of length per unit time. For example, miles per hour, kilometers per
minute, feet per second. The velocity is a vector having the speed as the magnitude but also
speciï¬ng the direction.
Thus the velocity vector in the above example is (100/
âˆš
2)i + (100/
âˆš
2)j.
Example 2.9.7 The velocity of an airplane is 100i+j+k measured in kilometers per hour
and at a certain instant of time its position is (1, 2, 1) . Here imagine a Cartesian coordinate
system in which the third component is altitude and the ï¬rst and second components are
measured on a line from West to East and a line from South to North. Find the position of
this airplane one minute later.
Consider the vector (1, 2, 1) , is the initial position vector of the airplane. As it moves,
the position vector changes. After one minute the airplane has moved in the i direction a
distance of 100 Ã— 1
60 = 5
3 kilometer. In the j direction it has moved
1
60 kilometer during this
same time, while it moves
1
60 kilometer in the k direction. Therefore, the new displacement
vector for the airplane is
(1, 2, 1) +
Âµ5
3, 1
60, 1
60
Â¶
=
Âµ8
3, 121
60 , 121
60
Â¶
Example 2.9.8 A certain river is one half mile wide with a current ï¬‚owing at 4 miles per
hour from East to West. A man swims directly toward the opposite shore from the South
bank of the river at a speed of 3 miles per hour. How far down the river does he ï¬nd himself
when he has swam across? How far does he end up swimming?
Consider the following picture.

2.9.
VECTORS AND PHYSICS
37

4
6
3
You should write these vectors in terms of components. The velocity of the swimmer in
still water would be 3j while the velocity of the river would be âˆ’4i. Therefore, the velocity
of the swimmer is âˆ’4i + 3j. Since the component of velocity in the direction across the river
is 3, it follows the trip takes 1/6 hour or 10 minutes. The speed at which he travels is
âˆš
42 + 32 = 5 miles per hour and so he travels 5 Ã— 1
6 = 5
6 miles. Now to ï¬nd the distance
downstream he ï¬nds himself, note that if x is this distance, x and 1/2 are two legs of a
right triangle whose hypotenuse equals 5/6 miles. Therefore, by the Pythagorean theorem
the distance downstream is
q
(5/6)2 âˆ’(1/2)2 = 2
3 miles.

38
VECTORS AND POINTS IN RN 5 SEPT.

Vector Products
3.1
The Dot Product 6 Sept.
Quiz
1. Given two points in R3, (x1, y1, z1) and (x2, y2, z2) , show the point
Âµx1 + x2
2
, y1 + y2
2
, z1 + z2
2
Â¶
is on the line between these two points and is the same distance from each of them.
2. Given the two points in R3, (x1, y1, z1) and (x2, y2, z2) , describe the set of all points
which are equidistant from these two points in terms of a simple equation.
3. An airplane heads due north at a speed of 120 miles per hour. The wind is blowing
north east at a speed of 30 miles per hour. Find the resulting speed of the airplane.
3.1.1
Deï¬nition In terms Of Coordinates
There are two ways of multiplying vectors which are of great importance in applications.
The ï¬rst of these is called the dot product, also called the scalar product and sometimes
the inner product.
Deï¬nition 3.1.1 Let a, b be two vectors in Rn deï¬ne a Â· b as
a Â· b â‰¡
n
X
k=1
akbk.
With this deï¬nition, there are several important properties satisï¬ed by the dot product.
In the statement of these properties, Î± and Î² will denote scalars and a, b, c will denote
vectors.
Proposition 3.1.2 The dot product satisï¬es the following properties.
a Â· b = b Â· a
(3.1)
a Â· a â‰¥0 and equals zero if and only if a = 0
(3.2)
(Î±a + Î²b) Â· c =Î± (a Â· c) + Î² (b Â· c)
(3.3)
c Â· (Î±a + Î²b) = Î± (c Â· a) + Î² (c Â· b)
(3.4)
|a|2 = a Â· a
(3.5)
39

40
VECTOR PRODUCTS
You should verify these properties. Also be sure you understand that 3.4 follows from
the ï¬rst three and is therefore redundant. It is listed here for the sake of convenience.
Example 3.1.3 Find (1, 2, 0, âˆ’1) Â· (0, 1, 2, 3) .
This equals 0 + 2 + 0 + âˆ’3 = âˆ’1.
Example 3.1.4 Find the magnitude of a = (2, 1, 4, 2) . That is, ï¬nd |a| .
This is
p
(2, 1, 4, 2) Â· (2, 1, 4, 2) = 5.
3.1.2
The Geometric Meaning Of The Dot Product, The Included
Angle
Given two vectors, a and b, the included angle is the angle between these two vectors which
is less than or equal to 180 degrees. The dot product can be used to determine the included
angle between two vectors. To see how to do this, consider the following picture.
Â©Â©Â©Â©Â©
*
PPPPPPPP
q
A
A
A
A
AU
A
A
A
A
AU
b
a
a âˆ’b
Î¸
By the law of cosines,
|a âˆ’b|2 = |a|2 + |b|2 âˆ’2 |a| |b| cos Î¸.
Also from the properties of the dot product,
|a âˆ’b|2 = (a âˆ’b) Â· (a âˆ’b)
= |a|2 + |b|2 âˆ’2a Â· b
and so comparing the above two formulas,
a Â· b = |a| |b| cos Î¸.
(3.6)
In words, the dot product of two vectors equals the product of the magnitude of the two
vectors multiplied by the cosine of the included angle. Note this gives a geometric de-
scription of the dot product which does not depend explicitly on the coordinates of the
vectors.
Example 3.1.5 Find the angle between the vectors 2i + j âˆ’k and 3i + 4j + k.
The dot product of these two vectors equals 6+4âˆ’1 = 9 and the norms are âˆš4 + 1 + 1 =
âˆš
6 and âˆš9 + 16 + 1 =
âˆš
26. Therefore, from 3.6 the cosine of the included angle equals
cos Î¸ =
9
âˆš
26
âˆš
6 = . 720 58
Now the cosine is known, the angle can be determined by solving the equation, cos Î¸ = .
720 58. This will involve using a calculator or a table of trigonometric functions. The answer

3.1.
THE DOT PRODUCT 6 SEPT.
41
is Î¸ = . 766 16 radians or in terms of degrees, Î¸ = . 766 16 Ã— 360
2Ï€ = 43. 898â—¦. Recall how this
last computation is done. Set up a proportion,
x
.76616 = 360
2Ï€ because 360â—¦corresponds to 2Ï€
radians. However, in calculus, you should get used to thinking in terms of radians and not
degrees. This is because all the important calculus formulas are deï¬ned in terms of radians.
Example 3.1.6 Find the magnitude of the vector 2i + 3j âˆ’k
As discussed above, this has magnitude equal to
p
(2i + 3j âˆ’k) Â· (2i + 3j âˆ’k) =
âˆš
4 + 9 + 1 =
âˆš
14.
Example 3.1.7 Let u, v be two vectors whose magnitudes are equal to 3 and 4 respectively
and such that if they are placed in standard position with their tails at the origin, the angle
between u and the positive x axis equals 30â—¦and the angle between v and the positive x axis
is -30â—¦. Find u Â· v.
From the geometric description of the dot product in 3.6
u Â· v = 3 Ã— 4 Ã— cos (60â—¦) = 3 Ã— 4 Ã— 1/2 = 6.
Observation 3.1.8 Two vectors are said to be perpendicular or orthogonal if the
included angle is Ï€/2 radians (90â—¦). You can tell if two nonzero vectors are perpendicular by
simply taking their dot product. If the answer is zero, this means they are are perpendicular
because cos Î¸ = 0.
Example 3.1.9 Determine whether the two vectors, 2i + j âˆ’k and 1i + 3j + 5k are perpen-
dicular.
When you take this dot product you get 2 + 3 âˆ’5 = 0 and so these two are indeed
perpendicular.
Deï¬nition 3.1.10 When two lines intersect, the angle between the two lines is the
smaller of the two angles determined.
Example 3.1.11 Find the angle between the two lines, (1, 2, 0) + t (1, 2, 3) and (0, 4, âˆ’3) +
t (âˆ’1, 2, âˆ’3) .
These two lines intersect, when t = 0 in the ï¬rst and t = âˆ’1 in the second. It is only a
matter of ï¬nding the angle between the direction vectors. One angle determined is given by
cos Î¸ = âˆ’6
14 = âˆ’3
7 .
(3.7)
We donâ€™t want this angle because it is obtuse. The angle desired is the acute angle given by
cos Î¸ = 3
7.
It is obtained by using replacing one of the direction vectors with âˆ’1 times it.

42
VECTOR PRODUCTS
3.1.3
The Cauchy Schwarz Inequality
The dot product satisï¬es a fundamental inequality known as the Cauchy Schwarz in-
equality.
Theorem 3.1.12 The dot product satisï¬es the inequality
|a Â· b| â‰¤|a| |b| .
(3.8)
Furthermore equality is obtained if and only if one of a or b is a scalar multiple of the other.
Geometric Proof: From the geometric description of the dot product,
|a Â· b| = ||a| |b| cos Î¸| â‰¤|a| |b|
because cos Î¸ is a number between âˆ’1 and 1. Equality occurs if and only if cos Î¸ = Â±1. This
corresponds to b being a scalar multiple of a. If cos Î¸ = âˆ’1, then b points in the opposite
direction to a and if cos Î¸ = 1 then b points in the same direction as a.
The Cauchy Schwarz inequality is important in many contexts other than vectors in
Rn. What follows is a vastly superior algebraic proof. In general it is this way. Algebraic
methods are nearly always to be preferred to geometric reasoning.
Algebraic Proof: First note that if b = 0 both sides of 3.8 equal zero and so the
inequality holds in this case. Therefore, it will be assumed in what follows that b Ì¸= 0.
Deï¬ne a function of t âˆˆR
f (t) = (a + tb) Â· (a + tb) .
Then by 3.2, f (t) â‰¥0 for all t âˆˆR. Also from 3.3,3.4,3.1, and 3.5
f (t) = a Â· (a + tb) + tb Â· (a + tb)
= a Â· a + t (a Â· b) + tb Â· a + t2b Â· b
= |a|2 + 2t (a Â· b) + |b|2 t2.
Now
f (t) = |b|2
Ãƒ
t2 + 2ta Â· b
|b|2 + |a|2
|b|2
!
= |b|2
ï£«
ï£­t2 + 2ta Â· b
|b|2 +
Ãƒ
a Â· b
|b|2
!2
âˆ’
Ãƒ
a Â· b
|b|2
!2
+ |a|2
|b|2
ï£¶
ï£¸
= |b|2
ï£«
ï£­
Ãƒ
t + a Â· b
|b|2
!2
+
ï£«
ï£­|a|2
|b|2 âˆ’
Ãƒ
a Â· b
|b|2
!2ï£¶
ï£¸
ï£¶
ï£¸â‰¥0
for all t âˆˆR. In particular f (t) â‰¥0 when t = âˆ’
Â³
a Â· b/ |b|2Â´
, the value of t which yields
the minimum value of f, which implies
|a|2
|b|2 âˆ’
Ãƒ
a Â· b
|b|2
!2
â‰¥0.
(3.9)
Multiplying both sides by |b|4,
|a|2 |b|2 â‰¥(a Â· b)2

3.1.
THE DOT PRODUCT 6 SEPT.
43
which yields 3.8. If equality in the Cauchy Schwarz inequality holds, then the minimum
value of f (t) is zero and so for some t, (a + tb) Â· (a + tb) = |a + tb| = 0 so that a = âˆ’tb.
This proves the theorem.
Another Algebraic Proof: Let f (t) be given as above. Thus as above f (t) â‰¥0 for
all t âˆˆR. Thus as above,
f (t) = |a|2 + 2t (a Â· b) + |b|2 t2 â‰¥0
The graph of f (t) is a parabola which must open up and cannot cross the t axis. Thus
f (t) = 0 has either one real root or no real roots. Now recall the quadratic formula. This
condition implies the stuï¬€under the square root sign in the quadratic formula must be
nonpositive. Applied to this function of t it says
4 (a Â· b)2 âˆ’4 |a|2 |b|2 â‰¤0
which is just the Cauchy Schwarz inequality. As before, equality in this inequality implies
f has one real zero. Thus the minimum value of f is 0. This means a + tb = 0 for some t
and so one vector is a multiple of the other. This proves the theorem.
You should note that the algebraic arguments were based only on the properties of
the dot product listed in 3.1 - 3.5. This means that whenever something satisï¬es these
properties, the Cauchy Schwartz inequality holds. There are many other instances of these
properties besides vectors in Rn.
3.1.4
The Triangle Inequality
The Cauchy Schwartz inequality allows a proof of the triangle inequality for distances in
Rn in much the same way as the triangle inequality for the absolute value.
Theorem 3.1.13 (Triangle inequality) For a, b âˆˆRn
|a + b| â‰¤|a| + |b|
(3.10)
and equality holds if and only if one of the vectors is a nonnegative scalar multiple of the
other. Also
||a| âˆ’|b|| â‰¤|a âˆ’b|
(3.11)
Proof: By properties of the dot product and the Cauchy Schwartz inequality,
|a + b|2 = (a + b) Â· (a + b)
= (a Â· a) + (a Â· b) + (b Â· a) + (b Â· b)
= |a|2 + 2 (a Â· b) + |b|2
â‰¤|a|2 + 2 |a Â· b| + |b|2
â‰¤|a|2 + 2 |a| |b| + |b|2
= (|a| + |b|)2 .
Taking square roots of both sides you obtain 3.10.
It remains to consider when equality occurs.
If either vector equals zero, then that
vector equals zero times the other vector and the claim about when equality occurs is
veriï¬ed. Therefore, it can be assumed both vectors are nonzero. To get equality in the
second inequality above, Theorem 3.1.12 implies one of the vectors must be a multiple of

44
VECTOR PRODUCTS
the other. Say b = Î±a. If Î± < 0 then equality cannot occur in the ï¬rst inequality because
in this case
(a Â· b) = Î± |a|2 < 0 < |Î±| |a|2 = |a Â· b|
Therefore, Î± â‰¥0.
To get the other form of the triangle inequality,
a = a âˆ’b + b
so
|a| = |a âˆ’b + b|
â‰¤|a âˆ’b| + |b| .
Therefore,
|a| âˆ’|b| â‰¤|a âˆ’b|
(3.12)
Similarly,
|b| âˆ’|a| â‰¤|b âˆ’a| = |a âˆ’b| .
(3.13)
It follows from 3.12 and 3.13 that 3.11 holds. This is because ||a| âˆ’|b|| equals the left side
of either 3.12 or 3.13 and either way, ||a| âˆ’|b|| â‰¤|a âˆ’b| . This proves the theorem.
3.1.5
Direction Cosines Of A Line
Now that the dot product and distance has been deï¬ned, it is possible to mention some
archaic terminology which is sometimes found.
Suppose x = a + tb is a vector equation for a line in Rn where, as explained before, the
vector, b is called a direction vector. When b is a unit vector (|b| = 1), the components
of b are called direction cosines. Say b = (b1, Â· Â· Â·, bn) . Thus, from the deï¬nition of the
dot product, bk = b Â· ek where ek is the unit vector for the kth coordinate axis consisting of
all zeros except for a 1 in the kth slot. So why in the world do people call these â€œdirection
cosinesâ€? It is because the cosine of the angle, Î¸k between the unit vector b and the vector,
ek is given by
cos Î¸k â‰¡b Â· ek
|b| |ek| = b Â· ek = bk.
There, isnâ€™t that interesting? Now you know why these are called direction cosines. So
what importance does it have? If someone gives you the â€œdirection cosinesâ€ of a line, they
are just using jargon to identify the components of a unit vector which serves as a direction
vector for the line.
Example 3.1.14 A line, l in R3 contains the point (1, 2, 3) and letting Î¸k be the angle
between a direction vector and ek,
cos (Î¸1) =
1
âˆš
5, cos (Î¸2) =
1
âˆš
5, cos (Î¸3) = âˆ’
âˆš
15
5 ,
ï¬nd a vector equation for the line.
The information in this example is nothing more than a jargon laden statement that a
direction vector for the line is
Ãƒ
1
âˆš
5, 1
âˆš
5, âˆ’
âˆš
15
5
!
.

3.1.
THE DOT PRODUCT 6 SEPT.
45
Therefore, a vector equation for the line is
(x, y, z) = (1, 2, 3) + t
Ãƒ
1
âˆš
5, 1
âˆš
5, âˆ’
âˆš
15
5
!
.
Of course if you like to wallow in terminology, you could also say parametric equations for
this line are
x = 1 + t 1
âˆš
5, y = 2 + t 1
âˆš
5, z = 3 + t
Ãƒ
âˆ’
âˆš
15
5
!
.
Symmetric equations for the line are obtained by solving for the parameter. Thus symmetric
equations for the line are
âˆš
5 (x âˆ’1) =
âˆš
5 (y âˆ’2) = âˆ’5
âˆš
15 (z âˆ’3) .
Isnâ€™t this exciting? No doubt there are other monumental trivialities and stupid observations
which could be drawn. The fundamental and signiï¬cant ingredients of a line are the direction
vector and a point on the line. These are the most important things to understand.
3.1.6
Work And Projections
An important application of the dot product is the concept of work. The physical concept
of work does not in any way correspond to the notion of work employed in ordinary con-
versation. For example, if you were to slide a 150 pound weight oï¬€a table which is three
feet high and shuï¬„e along the ï¬‚oor for 50 yards, sweating profusely and exerting all your
strength to keep the weight from falling on your feet, keeping the height always three feet
and then deposit this weight on another three foot high table, the physical concept of work
would indicate that the force exerted by your arms did no work during this project even
though the muscles in your hands and arms would likely be very tired. The reason for
such an unusual deï¬nition is that even though your arms exerted considerable force on the
weight, enough to keep it from falling, the direction of motion was at right angles to the
force they exerted. The only part of a force which does work in the sense of physics is the
component of the force in the direction of motion (This is made more precise below.). The
work is deï¬ned to be the magnitude of the component of this force times the distance over
which it acts in the case where this component of force points in the direction of motion and
(âˆ’1) times the magnitude of this component times the distance in case the force tends to
impede the motion. Thus the work done by a force on an object as the object moves from
one point to another is a measure of the extent to which the force contributes to the motion.
This is illustrated in the following picture in the case where the given force contributes to
the motion.

Â¡
Â¡
Â¡Â¡


:
C
C
CCO
F
F||
FâŠ¥
q
qp2
p1
Î¸
In this picture the force, F is applied to an object which moves on the straight line from
p1 to p2. There are two vectors shown, F|| and FâŠ¥and the picture is intended to indicate
that when you add these two vectors you get F while F|| acts in the direction of motion and
FâŠ¥acts perpendicular to the direction of motion. Only F|| contributes to the work done

46
VECTOR PRODUCTS
by F on the object as it moves from p1 to p2. F|| is called the projection of the force
in the direction of motion. From trigonometry, you see the magnitude of F|| should equal
|F| |cos Î¸| . Thus, since F|| points in the direction of the vector from p1 to p2, the total work
done should equal
|F|
Â¯Â¯âˆ’âˆ’âˆ’â†’
p1p2
Â¯Â¯ cos Î¸ = |F| |p2 âˆ’p1| cos Î¸
If the included angle had been obtuse, then the work done by the force, F on the object
would have been negative because in this case, the force tends to impede the motion from
p1 to p2 but in this case, cos Î¸ would also be negative and so it is still the case that the
work done would be given by the above formula. Thus from the geometric description of
the dot product given above, the work equals
|F| |p2 âˆ’p1| cos Î¸ = FÂ· (p2âˆ’p1) .
This explains the following deï¬nition.
Deï¬nition 3.1.15 Let F be a force acting on an object which moves from the point,
p1 to the point p2. Then the work done on the object by the given force equals FÂ· (p2 âˆ’p1) .
The concept of writing a given vector, F in terms of two vectors, one which is parallel
to a given vector, D and the other which is perpendicular can also be explained with no
reliance on trigonometry, completely in terms of the algebraic properties of the dot product.
As before, this is mathematically more signiï¬cant than any approach involving geometry or
trigonometry because it extends to more interesting situations. This is done next.
Theorem 3.1.16 Let F and D be nonzero vectors. Then there exist unique vectors
F|| and FâŠ¥such that
F = F|| + FâŠ¥
(3.14)
where F|| is a scalar multiple of D, also referred to as
projD (F) ,
and FâŠ¥Â· D = 0. The vector projD (F) is called the projection of F onto D.
Proof: Suppose 3.14 and F|| = Î±D. Taking the dot product of both sides with D and
using FâŠ¥Â· D = 0, this yields
F Â· D = Î± |D|2
which requires Î± = F Â· D/ |D|2 . Thus there can be no more than one vector, F||. It follows
FâŠ¥must equal F âˆ’F||. This veriï¬es there can be no more than one choice for both F|| and
FâŠ¥.
Now let
F|| â‰¡F Â· D
|D|2 D
and let
FâŠ¥= F âˆ’F|| = Fâˆ’F Â· D
|D|2 D
Then F|| = Î± D where Î± = FÂ·D
|D|2 . It only remains to verify FâŠ¥Â· D = 0. But
FâŠ¥Â· D = F Â· Dâˆ’F Â· D
|D|2 D Â· D
= F Â· D âˆ’F Â· D = 0.
This proves the theorem.

3.1.
THE DOT PRODUCT 6 SEPT.
47
Deï¬nition 3.1.17 The component of the vector F in the direction, D equals the
scalar
F Â· D
|D| .
Thus
projD (F) = F Â· D
|D|
D
|D|.
In words, the projection of F on D equals the component of F in the direction D times the
unit vector in the direction of D.
Example 3.1.18 Let F = (1, 2, 3) . Find the projection of F on D = (2, 1, 1) and also ï¬nd
the component of F in the direction, D.
projD (F)
=
F Â· D
|D|
D
|D|
=
2 + 2 + 3
âˆš4 + 1 + 1
(2, 1, 1)
âˆš4 + 1 + 1
=
7
6 (2, 1, 1) =
Âµ7
3, 7
6, 7
6
Â¶
and the component of F in the direction of D is
2 + 2 + 3
âˆš4 + 1 + 1 = 7
6
âˆš
6.
Example 3.1.19 Let F = 2i+7jâˆ’3k Newtons. Find the work done by this force in moving
from the point (1, 2, 3) to the point (âˆ’9, âˆ’3, 4) along the straight line segment joining these
points where distances are measured in meters.
According to the deï¬nition, this work is
(2i+7j âˆ’3k) Â· (âˆ’10i âˆ’5j + k) = âˆ’20 + (âˆ’35) + (âˆ’3)
= âˆ’58 Newton meters.
Note that if the force had been given in pounds and the distance had been given in feet,
the units on the work would have been foot pounds. In general, work has units equal to
units of a force times units of a length. Instead of writing Newton meter, people write joule
because a joule is by deï¬nition a Newton meter. That word is pronounced â€œjewelâ€ and it is
the unit of work in the metric system of units. Also be sure you observe that the work done
by the force can be negative as in the above example. In fact, work can be either positive,
negative, or zero. You just have to do the computations to ï¬nd out.
Example 3.1.20 Find proju (v) if u = 2i + 3j âˆ’4k and v = i âˆ’2j + k.
From the above discussion in Theorem 3.1.16, this is just
1
4 + 9 + 16 (i âˆ’2j + k) Â· (2i + 3j âˆ’4k) (2i + 3j âˆ’4k)
=
âˆ’8
29 (2i + 3j âˆ’4k) = âˆ’16
29i âˆ’24
29j + 32
29k.

48
VECTOR PRODUCTS
Example 3.1.21 Suppose a, and b are vectors and bâŠ¥= b âˆ’proja (b) . What is the mag-
nitude of bâŠ¥in terms of the included angle?
|bâŠ¥|2 = (b âˆ’proja (b)) Â· (b âˆ’proja (b))
=
Ãƒ
bâˆ’b Â· a
|a|2 a
!
Â·
Ãƒ
bâˆ’b Â· a
|a|2 a
!
= |b|2 âˆ’2(b Â· a)2
|a|2
+
Ãƒ
b Â· a
|a|2
!2
|a|2
= |b|2
Ãƒ
1 âˆ’(b Â· a)2
|a|2 |b|2
!
= |b|2 Â¡
1 âˆ’cos2 Î¸
Â¢
= |b|2 sin2 (Î¸)
where Î¸ is the included angle between a and b which is less than Ï€ radians. Therefore,
taking square roots,
|bâŠ¥| = |b| sin Î¸.
3.2
The Cross Product 7 Sept.
Quiz
1. Find the cosine of the angle between the two vectors (1, 2, 0) and (2, 0, 1) .
2. Suppose u, v are vectors. Show the parallelogram identity.
|u + v|2 + |u âˆ’v|2 = 2 |u|2 + 2 |v|2
You must show this in any dimension, not just in two or three dimensions.
3. Find the projection of the vector (1, 2, 3) onto the vector (2, 3, 1) .
4. Given two vectors, u, v in Rn, show using the properties of the dot product alone that
uâˆ’u Â· v
|v|2 v
is perpendicular to v.
5. x = u + tv for t âˆˆR is a line. Suppose z is a point in Rn. Find a formula for the
distance between z and this line.
3.2.1
The Geometric Description Of The Cross Product In Terms
Of The Included Angle
The cross product is the other way of multiplying two vectors in R3. It is very diï¬€erent
from the dot product in many ways. First the geometric meaning is discussed and then
a description in terms of coordinates is given. Both descriptions of the cross product are
important. The geometric description is essential in order to understand the applications
to physics and geometry while the coordinate description is the only way to practically
compute the cross product.

3.2.
THE CROSS PRODUCT 7 SEPT.
49
Deï¬nition 3.2.1 Three vectors, a, b, c form a right handed system if when you
extend the ï¬ngers of your right hand along the vector, a and close them in the direction of
b, the thumb points roughly in the direction of c.
For an example of a right handed system of vectors, see the following picture.
X
X
X
X
X
X
y
Â©
Â©
Â©
Â©

Â£
Â£
Â£
Â£
Â£
Â£
a
b
c
In this picture the vector c points upwards from the plane determined by the other two
vectors. You should consider how a right hand system would diï¬€er from a left hand system.
Try using your left hand and you will see that the vector, c would need to point in the
opposite direction as it would for a right hand system.
From now on, the vectors, i, j, k will always form a right handed system. To repeat,
if you extend the ï¬ngers of your right hand along i and close them in the direction j, the
thumb points in the direction of k.
The following is the geometric description of the cross product. It gives both the direction
and the magnitude and therefore speciï¬es the vector.
Deï¬nition 3.2.2 Let a and b be two vectors in R3. Then a Ã— b is deï¬ned by the
following two rules.
1. |a Ã— b| = |a| |b| sin Î¸ where Î¸ is the included angle.
2. a Ã— b Â· a = 0, a Ã— b Â· b = 0, and a, b, a Ã— b forms a right hand system.
Note that |a Ã— b| is the area of the parallelogram spanned by a and b.

3
-
b
a
Î¸
|b|sin(Î¸)
Â©
Â©

The cross product satisï¬es the following properties.
a Ã— b = âˆ’(b Ã— a) , a Ã— a = 0,
(3.15)

50
VECTOR PRODUCTS
For Î± a scalar,
(Î±a) Ã—b = Î± (a Ã— b) = aÃ— (Î±b) ,
(3.16)
For a, b, and c vectors, one obtains the distributive laws,
aÃ— (b + c) = a Ã— b + a Ã— c,
(3.17)
(b + c) Ã— a = b Ã— a + c Ã— a.
(3.18)
Formula 3.15 follows immediately from the deï¬nition. The vectors a Ã— b and b Ã— a have
the same magnitude, |a| |b| sin Î¸, and an application of the right hand rule shows they have
opposite direction. Formula 3.16 is also fairly clear. If Î± is a nonnegative scalar, the direction
of (Î±a) Ã—b is the same as the direction of a Ã— b,Î± (a Ã— b) and aÃ— (Î±b) while the magnitude
is just Î± times the magnitude of a Ã— b which is the same as the magnitude of Î± (a Ã— b)
and aÃ— (Î±b) . Using this yields equality in 3.16. In the case where Î± < 0, everything works
the same way except the vectors are all pointing in the opposite direction and you must
multiply by |Î±| when comparing their magnitudes. The distributive laws are much harder
to establish but the second follows from the ï¬rst quite easily. Thus, assuming the ï¬rst, and
using 3.15,
(b + c) Ã— a = âˆ’aÃ— (b + c)
= âˆ’(a Ã— b + a Ã— c)
= b Ã— a + c Ã— a.
A proof of the distributive law is given in a later section for those who are interested.
3.2.2
The Coordinate Description Of The Cross Product
Now from the deï¬nition of the cross product,
i Ã— j = k
j Ã— i = âˆ’k
k Ã— i = j
i Ã— k = âˆ’j
j Ã— k = i
k Ã— j = âˆ’i
With this information, the following gives the coordinate description of the cross product.
Proposition 3.2.3 Let a = a1i+a2j+a3k and b = b1i+b2j+b3k be two vectors. Then
a Ã— b = (a2b3 âˆ’a3b2) i+ (a3b1 âˆ’a1b3) j+
+ (a1b2 âˆ’a2b1) k.
(3.19)
Proof: From the above table and the properties of the cross product listed,
(a1i + a2j + a3k) Ã— (b1i + b2j + b3k) =
a1b2i Ã— j + a1b3i Ã— k + a2b1j Ã— i + a2b3j Ã— k+
+a3b1k Ã— i + a3b2k Ã— j
= a1b2k âˆ’a1b3j âˆ’a2b1k + a2b3i + a3b1j âˆ’a3b2i
= (a2b3 âˆ’a3b2) i+ (a3b1 âˆ’a1b3) j+ (a1b2 âˆ’a2b1) k
(3.20)
This proves the proposition.

3.2.
THE CROSS PRODUCT 7 SEPT.
51
It is probably impossible for most people to remember 3.19.
Fortunately, there is a
somewhat easier way to remember it.
a Ã— b =
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
a1
a2
a3
b1
b2
b3
Â¯Â¯Â¯Â¯Â¯Â¯
(3.21)
where you expand the determinant along the top row. This yields
(a2b3 âˆ’a3b2) iâˆ’(a1b3 âˆ’a3b1) j+ (a1b2 âˆ’a2b1) k
(3.22)
which is the same as 3.20.
You will see determinants later in the course but some of you have already seen them.
All you need here is how to evaluate 2 Ã— 2 and 3 Ã— 3 determinants.
Â¯Â¯Â¯Â¯
x
y
z
w
Â¯Â¯Â¯Â¯ = xw âˆ’yz
and
Â¯Â¯Â¯Â¯Â¯Â¯
a
b
c
x
y
z
u
v
w
Â¯Â¯Â¯Â¯Â¯Â¯
= a
Â¯Â¯Â¯Â¯
y
z
v
w
Â¯Â¯Â¯Â¯ âˆ’b
Â¯Â¯Â¯Â¯
x
z
u
w
Â¯Â¯Â¯Â¯ + c
Â¯Â¯Â¯Â¯
x
y
u
v
Â¯Â¯Â¯Â¯ .
Some of you are wondering what the rule is. You look at an entry in the top row and cross
out the row and column which contain that entry. If the entry is in the ith column, you
multiply (âˆ’1)1+i times the determinant of the 2 Ã— 2 which remains. This is the cofactor.
You take the element in the top row times this cofactor and add all such up.
Example 3.2.4 Find (i âˆ’j + 2k) Ã— (3i âˆ’2j + k) .
Use 3.21 to compute this.
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
1
âˆ’1
2
3
âˆ’2
1
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯
âˆ’1
2
âˆ’2
1
Â¯Â¯Â¯Â¯ iâˆ’
Â¯Â¯Â¯Â¯
1
2
3
1
Â¯Â¯Â¯Â¯ j+
Â¯Â¯Â¯Â¯
1
âˆ’1
3
âˆ’2
Â¯Â¯Â¯Â¯ k
= 3i + 5j + k.
Example 3.2.5 Find the area of the parallelogram determined by the vectors, (i âˆ’j + 2k)
and (3i âˆ’2j + k) . These are the same two vectors in Example 3.2.4.
From Example 3.2.4 and the geometric description of the cross product, the area is just
the norm of the vector obtained in Example 3.2.4. Thus the area is âˆš9 + 25 + 1 =
âˆš
35.
Example 3.2.6 Find the area of the triangle determined by (1, 2, 3) , (0, 2, 5) , and (5, 1, 2) .
This triangle is obtained by connecting the three points with lines. Picking (1, 2, 3) as a
starting point, there are two displacement vectors, (âˆ’1, 0, 2) and (4, âˆ’1, âˆ’1) such that the
given vector added to these displacement vectors gives the other two vectors. The area of
the triangle is half the area of the parallelogram determined by (âˆ’1, 0, 2) and (4, âˆ’1, âˆ’1) .
Thus (âˆ’1, 0, 2) Ã— (4, âˆ’1, âˆ’1) = (2, 7, 1) and so the area of the triangle is 1
2
âˆš4 + 49 + 1 =
3
2
âˆš
6.
Observation 3.2.7 In general, if you have three points (vectors) in R3, P, Q, R the
area of the triangle is given by
1
2 |(Q âˆ’P) Ã— (R âˆ’P)| .

52
VECTOR PRODUCTS
-






r
r
r
P
Q
R
3.2.3
The Box Product, Triple Product
Deï¬nition 3.2.8 A parallelepiped determined by the three vectors, a, b, and c con-
sists of
{ra+sb + tc : r, s, t âˆˆ[0, 1]} .
That is, if you pick three numbers, r, s, and t each in [0, 1] and form ra+sb + tc, then the
collection of all such points is what is meant by the parallelepiped determined by these three
vectors.
The following is a picture of such a thing.
-











3













a
b
c
6
a Ã— b
Î¸
You notice the area of the base of the parallelepiped, the parallelogram determined by
the vectors, a and b has area equal to |a Ã— b| while the altitude of the parallelepiped is
|c| cos Î¸ where Î¸ is the angle shown in the picture between c and a Ã— b. Therefore, the
volume of this parallelepiped is the area of the base times the altitude which is just
|a Ã— b| |c| cos Î¸ = a Ã— b Â· c.
This expression is known as the box product and is sometimes written as [a, b, c] . You
should consider what happens if you interchange the b with the c or the a with the c. You
can see geometrically from drawing pictures that this merely introduces a minus sign. In any
case the box product of three vectors always equals either the volume of the parallelepiped
determined by the three vectors or else minus this volume.
Example 3.2.9 Find the volume of the parallelepiped determined by the vectors, i + 2j âˆ’
5k, i + 3j âˆ’6k,3i + 2j + 3k.
According to the above discussion, pick any two of these, take the cross product and
then take the dot product of this with the third of these vectors. The result will be either

3.2.
THE CROSS PRODUCT 7 SEPT.
53
the desired volume or minus the desired volume.
(i + 2j âˆ’5k) Ã— (i + 3j âˆ’6k)
=
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
1
2
âˆ’5
1
3
âˆ’6
Â¯Â¯Â¯Â¯Â¯Â¯
=
3i + j + k
Now take the dot product of this vector with the third which yields
(3i + j + k) Â· (3i + 2j + 3k) = 9 + 2 + 3 = 14.
This shows the volume of this parallelepiped is 14 cubic units.
Observation 3.2.10 Suppose you have three vectors, u = (a, b, c) , v = (d, e, f) , and
w = (g, h, i) . Then u Â· v Ã— w is given by the following.
u Â· v Ã— w
=
(a, b, c) Â·
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
d
e
f
g
h
i
Â¯Â¯Â¯Â¯Â¯Â¯
=
a
Â¯Â¯Â¯Â¯
e
f
h
i
Â¯Â¯Â¯Â¯ âˆ’b
Â¯Â¯Â¯Â¯
d
f
g
i
Â¯Â¯Â¯Â¯ + c
Â¯Â¯Â¯Â¯
d
e
g
h
Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯Â¯
a
b
c
d
e
f
g
h
i
Â¯Â¯Â¯Â¯Â¯Â¯
.
The message is that to take the box product, you can simply take the determinant of the
3 Ã— 3 â€œmatrixâ€ as described above.
Example 3.2.11 Find the volume of the parallelepiped determined by the vectors, (1, 2, âˆ’1) , (2, 1, 5) ,
and (âˆ’3, 1, 2).
As just observed, it suï¬ƒces to take the absolute value of the following determinant.
Â¯Â¯Â¯Â¯Â¯Â¯
1
2
âˆ’1
2
1
5
âˆ’3
1
2
Â¯Â¯Â¯Â¯Â¯Â¯
= âˆ’46
Thus the volume of this parallelepiped is 46.
There is a fundamental observation which comes directly from the geometric deï¬nitions
of the cross product and the dot product.
Lemma 3.2.12 Let a, b, and c be vectors. Then (a Ã— b) Â·c = aÂ· (b Ã— c) .
Proof: This follows from observing that either (a Ã— b) Â·c and aÂ· (b Ã— c) both give the
volume of the parallelepiped or they both give âˆ’1 times the volume.
3.2.4
A Proof Of The Distributive Law For The Cross Productâˆ—
Here is a proof of the distributive law for the cross product. Let x be a vector. From the
above observation,
x Â· aÃ— (b + c) = (x Ã— a) Â· (b + c)
= (x Ã— a) Â· b+ (x Ã— a) Â· c
= x Â· a Ã— b + x Â· a Ã— c
= xÂ· (a Ã— b + a Ã— c) .

54
VECTOR PRODUCTS
Therefore,
xÂ· [aÃ— (b + c) âˆ’(a Ã— b + a Ã— c)] = 0
for all x.
In particular, this holds for x = aÃ— (b + c) âˆ’(a Ã— b + a Ã— c) showing that
aÃ— (b + c) = a Ã— b + a Ã— c and this proves the distributive law for the cross product.
Example 3.2.13 Find the volume of the parallelepiped determined by the vectors,
(âˆ’1, 2, 3) , (2, âˆ’1, 1) , (3, âˆ’2, 3)
As just explained you only have to ï¬nd the following 3 Ã— 3 determinants.
Â¯Â¯Â¯Â¯Â¯Â¯
âˆ’1
2
3
2
âˆ’1
1
3
âˆ’2
3
Â¯Â¯Â¯Â¯Â¯Â¯
= âˆ’1
Â¯Â¯Â¯Â¯
âˆ’1
1
âˆ’2
3
Â¯Â¯Â¯Â¯ âˆ’2
Â¯Â¯Â¯Â¯
2
1
3
3
Â¯Â¯Â¯Â¯ + 3
Â¯Â¯Â¯Â¯
2
âˆ’1
3
âˆ’2
Â¯Â¯Â¯Â¯ = âˆ’8
Now volume is always nonnegative so you take the absolute value of this number. The
volume of the parallelepiped is 8.
3.2.5
Torque, Moment Of A Force
Imagine you are using a wrench to loosen a nut. The idea is to turn the nut by applying a
force to the end of the wrench. If you push or pull the wrench directly toward or away from
the nut, it should be obvious from experience that no progress will be made in turning the
nut. The important thing is the component of force perpendicular to the wrench. It is this
component of force which will cause the nut to turn. For example see the following picture.
Â©Â©Â©Â©Â©Â©Â©Â©Â©
*Â£
Â£
Â£
Â£Â£
F
Â£
Â£
Â£
Â£Â£
Â©Â©Â©
F
R
A
A
AK
FâŠ¥
Î¸
Î¸
In the picture a force, F is applied at the end of a wrench represented by the position
vector, R and the angle between these two is Î¸. Then the tendency to turn will be |R| |FâŠ¥| =
|R| |F| sin Î¸, which you recognize as the magnitude of the cross product of R and F. If there
were just one force acting at one point whose position vector is R, perhaps this would be
suï¬ƒcient, but what if there are numerous forces acting at many diï¬€erent points with neither
the position vectors nor the force vectors in the same plane; what then? To keep track of
this sort of thing, deï¬ne for each R and F, the torque vector,
Ï„ â‰¡R Ã— F.
This is also called the moment of the force, F. That way, if there are several forces acting at
several points, the total torque or moment can be obtained by simply adding up the torques
associated with the diï¬€erent forces and positions.
Example 3.2.14 Suppose R1 = 2i âˆ’j+3k, R2 = i+2jâˆ’6k meters and at the points de-
termined by these vectors there are forces, F1 = i âˆ’j+2k and F2 = i âˆ’5j + k Newtons
respectively. Find the total torque about the origin produced by these forces acting at the
given points.

3.2.
THE CROSS PRODUCT 7 SEPT.
55
It is necessary to take R1 Ã— F1 + R2 Ã— F2. Thus the total torque equals
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
2
âˆ’1
3
1
âˆ’1
2
Â¯Â¯Â¯Â¯Â¯Â¯
+
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
1
2
âˆ’6
1
âˆ’5
1
Â¯Â¯Â¯Â¯Â¯Â¯
= âˆ’27i âˆ’8j âˆ’8k Newton meters
Example 3.2.15 Find if possible a single force vector, F which if applied at the point
i + j + k will produce the same torque as the above two forces acting at the given points.
This is fairly routine. The problem is to ï¬nd F = F1i + F2j + F3k which produces the
above torque vector. Therefore,
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
1
1
1
F1
F2
F3
Â¯Â¯Â¯Â¯Â¯Â¯
= âˆ’27i âˆ’8j âˆ’8k
which reduces to (F3 âˆ’F2) i+ (F1 âˆ’F3) j+ (F2 âˆ’F1) k = âˆ’27i âˆ’8j âˆ’8k. This amounts to
solving the system of three equations in three unknowns, F1, F2, and F3,
F3 âˆ’F2 = âˆ’27
F1 âˆ’F3 = âˆ’8
F2 âˆ’F1 = âˆ’8
However, there is no solution to these three equations. (Why?) Therefore no single force
acting at the point i + j + k will produce the given torque.
3.2.6
Angular Velocity
Deï¬nition 3.2.16 In a rotating body, a vector, â„¦is called an angular velocity
vector if the velocity of a point having position vector, u relative to the body is given by
â„¦Ã— u.
The existence of an angular velocity vector is the key to understanding motion in a
moving system of coordinates. It is used to explain the motion on the surface of the rotating
earth. For example, have you ever wondered why low pressure areas rotate counter clockwise
in the upper hemisphere but clockwise in the lower hemisphere? To quantify these things,
you will need the concept of an angular velocity vector.
Details are presented later for
interesting examples.
Here is a simple example.
Think of a coordinate system ï¬xed in
the rotating body. Thus if you were riding on the rotating body, you would observe this
coordinate system as ï¬xed but it is not ï¬xed.
Example 3.2.17 A wheel rotates counter clockwise about the vector i + j + k at 60 revo-
lutions per minute. This means that if the thumb of your right hand were to point in the
direction of i + j + k your ï¬ngers of this hand would wrap in the direction of rotation. Find
the angular velocity vector for this wheel. Assume the unit of distance is meters and the
unit of time is minutes.
Let Ï‰ = 60 Ã— 2Ï€ = 120Ï€. This is the number of radians per minute corresponding to
60 revolutions per minute. Then the angular velocity vector is 120Ï€
âˆš
3 (i + j + k) . Note this
gives what you would expect in the case the position vector to the point is perpendicular
to i + j + k and at a distance of r. This is because of the geometric description of the cross
product. The magnitude of the vector is r120Ï€ meters per minute and corresponds to the
speed and an exercise with the right hand shows the direction is correct also. However, if
this body is rigid, this will work for every other point in it, even those for which the position
vector is not perpendicular to the given vector. A complete analysis of this is given later.

56
VECTOR PRODUCTS
Example 3.2.18 A wheel rotates counter clockwise about the vector i + j + k at 60 revolu-
tions per minute exactly as in Example 3.2.17. Let {u1, u2, u3} denote an orthogonal right
handed system attached to the rotating wheel in which u3 =
1
âˆš
3 (i + j + k) . Thus u1 and u2
depend on time. Find the velocity of the point of the wheel located at the point 2u1+3u2âˆ’u3.
Note this point is not ï¬xed in space. It is moving.
Since {u1, u2, u3} is a right handed system like i, j, k, everything applies to this system
in the same way as with i, j, k. Thus the cross product is given by
(au1 + bu2 + cu3) Ã— (du1 + eu2 + fu3)
=
Â¯Â¯Â¯Â¯Â¯Â¯
u1
u2
u3
a
b
c
d
e
f
Â¯Â¯Â¯Â¯Â¯Â¯
Therefore, in terms of the given vectors ui, the angular velocity vector is
120Ï€u3
the velocity of the given point is
Â¯Â¯Â¯Â¯Â¯Â¯
u1
u2
u3
0
0
120Ï€
2
3
âˆ’1
Â¯Â¯Â¯Â¯Â¯Â¯
=
âˆ’360Ï€u1 + 240Ï€u2
in meters per minute. Note how this gives the answer in terms of these vectors which are
ï¬xed in the body, not in space. Since ui depends on t, this shows the answer in this case
does also. Of course this is right. Just think of what is going on with the wheel rotating.
Those vectors which are ï¬xed in the wheel are moving in space. The velocity of a point in
the wheel should be constantly changing. However, its speed will not change. The speed
will be the magnitude of the velocity and this is
p
(âˆ’360Ï€u1 + 240Ï€u2) Â· (âˆ’360Ï€u1 + 240Ï€u2)
which from the properties of the dot product equals
q
(âˆ’360Ï€)2 + (240Ï€)2 = 120
âˆš
13Ï€
because the ui are given to be orthogonal.
3.2.7
Center Of Massâˆ—
The mass of an object is a measure of how much stuï¬€there is in the object. An object has
mass equal to one kilogram, a unit of mass in the metric system, if it would exactly balance
a known one kilogram object when placed on a balance. The known object is one kilogram
by deï¬nition. The mass of an object does not depend on where the balance is used. It
would be one kilogram on the moon as well as on the earth. The weight of an object is
something else. It is the force exerted on the object by gravity and has magnitude gm where
g is a constant called the acceleration of gravity. Thus the weight of a one kilogram object
would be diï¬€erent on the moon which has much less gravity, smaller g, than on the earth.
An important idea is that of the center of mass. This is the point at which an object will
balance no matter how it is turned.

3.3.
FURTHER EXPLANATIONSâˆ—
57
Deï¬nition 3.2.19 Let an object consist of p point masses, m1, Â· Â· Â·Â·, mp with the
position of the kth of these at Rk. The center of mass of this object, R0 is the point satisfying
p
X
k=1
(Rk âˆ’R0) Ã— gmku = 0
for all unit vectors, u.
The above deï¬nition indicates that no matter how the object is suspended, the total
torque on it due to gravity is such that no rotation occurs. Using the properties of the cross
product,
Ãƒ p
X
k=1
Rkgmk âˆ’R0
p
X
k=1
gmk
!
Ã— u = 0
(3.23)
for any choice of unit vector, u. You should verify that if a Ã— u = 0 for all u, then it must
be the case that a = 0. Then the above formula requires that
p
X
k=1
Rkgmk âˆ’R0
p
X
k=1
gmk= 0.
dividing by g, and then by Pp
k=1 mk,
R0 =
Pp
k=1 Rkmk
Pp
k=1 mk
.
(3.24)
This is the formula for the center of mass of a collection of point masses. To consider the
center of mass of a solid consisting of continuously distributed masses, you need the methods
of calculus.
Example 3.2.20 Let m1 = 5, m2 = 6, and m3 = 3 where the masses are in kilograms.
Suppose m1 is located at 2i + 3j + k, m2 is located at i âˆ’3j + 2k and m3 is located at
2i âˆ’j + 3k. Find the center of mass of these three masses.
Using 3.24
R0 = 5 (2i + 3j + k) + 6 (i âˆ’3j + 2k) + 3 (2i âˆ’j + 3k)
5 + 6 + 3
= 11
7 i âˆ’3
7j + 13
7 k
3.3
Further Explanationsâˆ—
3.3.1
The Distributive Law For The Cross Productâˆ—
This section gives a proof for 3.17 which is independent of volume considerations. It is
included here for the interested student. If you are satisï¬ed with taking the distributive law
on faith or are happy with the other argument given, it is not necessary to read this section.
The proof given here is quite clever and follows the one given in [7]. The other approach
based on areas is found in [23] and is discussed brieï¬‚y earlier.
Lemma 3.3.1 Let b and c be two vectors. Then b Ã— c = b Ã— câŠ¥where c|| + câŠ¥= c and
câŠ¥Â· b = 0.

58
VECTOR PRODUCTS
Proof: Consider the following picture.
-
b
Â¡
Â¡Â¡

Î¸
c
6
câŠ¥
Now câŠ¥= c âˆ’cÂ· b
|b|
b
|b| and so câŠ¥is in the plane determined by c and b. Therefore, from
the geometric deï¬nition of the cross product, b Ã— c and b Ã— câŠ¥have the same direction.
Now, referring to the picture,
|b Ã— câŠ¥| = |b| |câŠ¥|
= |b| |c| sin Î¸
= |b Ã— c| .
Therefore, b Ã— c and b Ã— câŠ¥also have the same magnitude and so they are the same vector.
With this, the proof of the distributive law is in the following theorem.
Theorem 3.3.2 Let a, b, and c be vectors in R3. Then
aÃ— (b + c) = a Ã— b + a Ã— c
(3.25)
Proof: Suppose ï¬rst that a Â· b = a Â· c = 0. Now imagine a is a vector coming out of the
page and let b, c and b + c be as shown in the following picture.
-
b
Â¡
Â¡Â¡

c

1
b + c
H
H
6
a Ã— b
B
B
B
B
B
B
B
B
B
B
B
B
B
B
BM
a Ã— (b + c)
@
@
@
@
@
I
a Ã— c
Then a Ã— b, aÃ— (b + c) , and a Ã— c are each vectors in the same plane, perpendicular to a
as shown. Thus a Ã— c Â· c = 0, aÃ— (b + c) Â· (b + c) = 0, and a Ã— b Â· b = 0. This implies that
to get a Ã— b you move counterclockwise through an angle of Ï€/2 radians from the vector, b.
Similar relationships exist between the vectors aÃ— (b + c) and b + c and the vectors a Ã— c
and c. Thus the angle between a Ã— b and aÃ— (b + c) is the same as the angle between b + c
and b and the angle between a Ã— c and aÃ— (b + c) is the same as the angle between c and
b + c. In addition to this, since a is perpendicular to these vectors,
|a Ã— b| = |a| |b| , |aÃ— (b + c)| = |a| |b + c| , and
|a Ã— c| = |a| |c| .

3.3.
FURTHER EXPLANATIONSâˆ—
59
Therefore,
|aÃ— (b + c)|
|b + c|
= |a Ã— c|
|c|
= |a Ã— b|
|b|
= |a|
and so
|aÃ— (b + c)|
|a Ã— c|
= |b + c|
|c|
, |aÃ— (b + c)|
|a Ã— b|
= |b + c|
|b|
showing the triangles making up the parallelogram on the right and the four sided ï¬gure on
the left in the above picture are similar. It follows the four sided ï¬gure on the left is in fact
a parallelogram and this implies the diagonal is the vector sum of the vectors on the sides,
yielding 3.25.
Now suppose it is not necessarily the case that a Â· b = a Â· c = 0. Then write b = b|| +bâŠ¥
where bâŠ¥Â· a = 0. Similarly c = c|| + câŠ¥. By the above lemma and what was just shown,
aÃ— (b + c) = aÃ— (b + c)âŠ¥
= aÃ— (bâŠ¥+ câŠ¥)
= a Ã— bâŠ¥+ a Ã— câŠ¥
= a Ã— b + a Ã— c.
This proves the theorem.
3.3.2
Vector Identities And Notationâˆ—
To begin with consider u Ã— (v Ã— w) and it is desired to simplify this quantity. It turns out
this is an important quantity which comes up in many diï¬€erent contexts. Let u = (u1, u2, u3)
and let v and w be deï¬ned similarly.
v Ã— w
=
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
v1
v2
v3
w1
w2
w3
Â¯Â¯Â¯Â¯Â¯Â¯
=
(v2w3 âˆ’v3w2) i+ (w1v3 âˆ’v1w3) j+ (v1w2 âˆ’v2w1) k
Next consider uÃ— (v Ã— w) which is given by
uÃ— (v Ã— w) =
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
u1
u2
u3
(v2w3 âˆ’v3w2)
(w1v3 âˆ’v1w3)
(v1w2 âˆ’v2w1)
Â¯Â¯Â¯Â¯Â¯Â¯
.
When you multiply this out, you get
i (v1u2w2 + u3v1w3 âˆ’w1u2v2 âˆ’u3w1v3) + j (v2u1w1 + v2w3u3 âˆ’w2u1v1 âˆ’u3w2v3)
+k (u1w1v3 + v3w2u2 âˆ’u1v1w3 âˆ’v2w3u2)
and if you are clever, you see right away that
(iv1 + jv2 + kv3) (u1w1 + u2w2 + u3w3) âˆ’(iw1 + jw2 + kw3) (u1v1 + u2v2 + u3v3) .
Thus
uÃ— (v Ã— w) = v (u Â· w) âˆ’w (u Â· v) .
(3.26)
A related formula is
(u Ã— v) Ã— w
=
âˆ’[wÃ— (u Ã— v)]
=
âˆ’[u (w Â· v) âˆ’v (w Â· u)]
=
v (w Â· u) âˆ’u (w Â· v) .
(3.27)

60
VECTOR PRODUCTS
This derivation is simply wretched and it does nothing for other identities which may arise
in applications.
Actually, the above two formulas, 3.26 and 3.27 are suï¬ƒcient for most
applications if you are creative in using them, but there is another way. This other way
allows you to discover such vector identities as the above without any creativity or any
cleverness.
Therefore, it is far superior to the above nasty computation.
It is a vector
identity discovering machine and it is this which is the main topic in what follows.
There are two special symbols, Î´ij and Îµijk which are very useful in dealing with vector
identities. To begin with, here is the deï¬nition of these symbols.
Deï¬nition 3.3.3 The symbol, Î´ij, called the Kroneker delta symbol is deï¬ned as
follows.
Î´ij â‰¡
Â½
1 if i = j
0 if i Ì¸= j
.
With the Kroneker symbol, i and j can equal any integer in {1, 2, Â· Â· Â·, n} for any n âˆˆN.
Deï¬nition 3.3.4 For i, j, and k integers in the set, {1, 2, 3} , Îµijk is deï¬ned as
follows.
Îµijk â‰¡
ï£±
ï£²
ï£³
1 if (i, j, k) = (1, 2, 3) , (2, 3, 1) , or (3, 1, 2)
âˆ’1 if (i, j, k) = (2, 1, 3) , (1, 3, 2) , or (3, 2, 1)
0 if there are any repeated integers
.
The subscripts ijk and ij in the above are called indices. A single one is called an index.
This symbol, Îµijk is also called the permutation symbol.
The way to think of Îµijk is that Îµ123 = 1 and if you switch any two of the numbers in
the list i, j, k, it changes the sign. Thus Îµijk = âˆ’Îµjik and Îµijk = âˆ’Îµkji etc. You should
check that this rule reduces to the above deï¬nition. For example, it immediately implies
that if there is a repeated index, the answer is zero. This follows because Îµiij = âˆ’Îµiij and
so Îµiij = 0.
It is useful to use the Einstein summation convention when dealing with these symbols.
Simply stated, the convention is that you sum over the repeated index. Thus aibi means
P
i aibi. Also, Î´ijxj means P
j Î´ijxj = xi. When you use this convention, there is one very
important thing to never forget. It is this: Never have an index be repeated more than once.
Thus aibi is all right but aiibi is not. The reason for this is that you end up getting confused
about what is meant. If you want to write P
i aibici it is best to simply use the summation
notation. There is a very important reduction identity connecting these two symbols.
Lemma 3.3.5 The following holds.
ÎµijkÎµirs = (Î´jrÎ´ks âˆ’Î´krÎ´js) .
Proof: If {j, k} Ì¸= {r, s} then every term in the sum on the left must have either Îµijk
or Îµirs contains a repeated index. Therefore, the left side equals zero. The right side also
equals zero in this case. To see this, note that if the two sets are not equal, then there is
one of the indices in one of the sets which is not in the other set. For example, it could be
that j is not equal to either r or s. Then the right side equals zero.
Therefore, it can be assumed {j, k} = {r, s} . If i = r and j = s for s Ì¸= r, then there
is exactly one term in the sum on the left and it equals 1. The right also reduces to 1 in
this case. If i = s and j = r, there is exactly one term in the sum on the left which is
nonzero and it must equal -1. The right side also reduces to -1 in this case. If there is
a repeated index in {j, k} , then every term in the sum on the left equals zero. The right
also reduces to zero in this case because then j = k = r = s and so the right side becomes
(1) (1) âˆ’(âˆ’1) (âˆ’1) = 0.

3.3.
FURTHER EXPLANATIONSâˆ—
61
Proposition 3.3.6 Let u, v be vectors in Rn where the Cartesian coordinates of u are
(u1, Â· Â· Â·, un) and the Cartesian coordinates of v are (v1, Â· Â· Â·, vn). Then u Â· v = uivi. If u, v
are vectors in R3, then
(u Ã— v)i = Îµijkujvk.
Also, Î´ikak = ai.
Proof: The ï¬rst claim is obvious from the deï¬nition of the dot product. The second is
veriï¬ed by simply checking it works. For example,
u Ã— v â‰¡
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
u1
u2
u3
v1
v2
v3
Â¯Â¯Â¯Â¯Â¯Â¯
and so
(u Ã— v)1 = (u2v3 âˆ’u3v2) .
From the above formula in the proposition,
Îµ1jkujvk â‰¡u2v3 âˆ’u3v2,
the same thing. The cases for (u Ã— v)2 and (u Ã— v)3 are veriï¬ed similarly. The last claim
follows directly from the deï¬nition.
With this notation, you can easily discover vector identities and simplify expressions
which involve the cross product.
Example 3.3.7 Discover a formula which simpliï¬es (u Ã— v) Ã—w.
From the above reduction formula,
((u Ã— v) Ã—w)i
=
Îµijk (u Ã— v)j wk
=
ÎµijkÎµjrsurvswk
=
âˆ’ÎµjikÎµjrsurvswk
=
âˆ’(Î´irÎ´ks âˆ’Î´isÎ´kr) urvswk
=
âˆ’(uivkwk âˆ’ukviwk)
=
u Â· wvi âˆ’v Â· wui
=
((u Â· w) v âˆ’(v Â· w) u)i .
Since this holds for all i, it follows that
(u Ã— v) Ã—w = (u Â· w) v âˆ’(v Â· w) u.
3.3.3
Exercises With Answers
1. Draw the vector u = (1, âˆ’2) , the vector v = (2, 3) , and the vector (1, âˆ’2) + (2, 3) =
u + v.
A
A
A
A
A
AU











1
u
v
u + v

62
VECTOR PRODUCTS
2. Let u = (1, 2, âˆ’5) , v = (3, âˆ’1, 2) and w = (2, 0, 3) Find the following.
(a) (2u + v) Â· w
This is (2 (1, 2, âˆ’5) + (3, âˆ’1, 2)) Â· (2, 0, 3) = âˆ’14. Here is why.
2 (1, 2, âˆ’5) + (3, âˆ’1, 2) = (5, 3, âˆ’8)
and
(5, 3, âˆ’8) Â· (2, 0, 3) = âˆ’14
(b) (uâˆ’3v) Â· w
This is ((1, 2, âˆ’5) âˆ’3 (3, âˆ’1, 2)) Â· (2, 0, 3) = âˆ’49
3. Find the cosine of the angle between the two vectors, (1, 2, 5) and (3, âˆ’2, 1) .
cos Î¸ =
(1,2,5)Â·(3,âˆ’2,1)
âˆš1+4+25âˆš9+4+1 =
1
105
âˆš
30
âˆš
14 = . 195 18.
4. Here are two vectors. (1, 2, 3) and (3, 2, 1) . Find a vector which is perpendicular to
both of these vectors.
One way to do this is to take the cross product of the two vectors. (1, 2, 3)Ã—(3, 2, 1) =
(âˆ’4, 8, âˆ’4) . A vector perpendicular to both of these vectors is (âˆ’1, 2, 1) . Note nothing
is changed as far as being perpendicular is concerned by division by 4.
5. Given two vectors in Rn, u, v show that
u Â· v =1
4
Â³
|u + v|2 âˆ’|u âˆ’v|2Â´
.
This is really easy if you remember the axioms for the dot product. Otherwise it is
very troublesome. Start with the right side.
1
4
Â³
|u + v|2 âˆ’|u âˆ’v|2Â´
=
1
4 ((u + v) Â· (u + v) âˆ’(u âˆ’v) Â· (u âˆ’v))
=
1
4 [u Â· u+ + v Â· vâˆ’{u Â· u + v Â· vâˆ’2u Â· v}]
=
1
4 [2u Â· vâˆ’(âˆ’2u Â· v)] = u Â· v.
6. If |u| = 3, |v| = 4, and u Â· v = 5, ï¬nd |u + v| .
This is easy if you know the properties of the dot product. Otherwise it is trouble.
|u + v|2
=
|u|2 + |v|2 + 2u Â· v
=
9 + 16 + 50 = 75.
Therefore, |u + v| = 5
âˆš
5.
7. Find vectors in R3, u, v such that u Â· v =6 and |u| = 2 while |v| = 3. You see that
equality holds in the Cauchy Schwarz inequality and so one of these vectors must
be a multiple of the other. It must be a positive multiple of the other because the
dot product is positive which implies the angle between the vectors is 0 and not Ï€.
Let u = (0, 0, 2) , v = (0, 0, 3) . This appears to work.
You should ï¬nd some other
examples. What if u =
Â¡âˆš
2/2,
âˆš
2/2,
âˆš
3
Â¢
. In this case |u| = 2 also. Can you ï¬nd v
such that the above will hold?

3.3.
FURTHER EXPLANATIONSâˆ—
63
8. The projection of u onto v, denoted by projv (u) is given by the formula
projv (u) = u Â· v
|v|2 v
Show uâˆ’projv (u) is perpendicular to v. Also show projv (au + bw) = a (projv (u))+
b (projv (w)) .
This is another of those things which is very easy if you know the properties of the
dot product but lots of trouble if you donâ€™t. Of course you can persist in not learning
these things if you want. It is up to you.
vÂ· (uâˆ’projv (u)) = vÂ·
Ãƒ
uâˆ’u Â· v
|v|2 v
!
= v Â· uâˆ’u Â· v
|v|2 v Â· v = v Â· u âˆ’u Â· v = 0
This does it. The dot product equals zero and so the two vectors are perpendicular.
As to the other claim,
(au + bw) Â·v
|v|2
v
=
au Â· v
|v|2 v + bw Â· v
|v|2 v
â‰¡
a (projv (u)) + b (projv (w)) .
Now that was real easy wasnâ€™t it. Note I never said anything about u, v being lists of
numbers. I just used the properties of the dot product.
9. Find the angle between the vectors 3i âˆ’j âˆ’k and i + 4j + 2k.
cos Î¸ =
3âˆ’4âˆ’2
âˆš9+1+1âˆš1+16+4 = âˆ’. 197 39. Therefore, you have to solve the equation cos Î¸ =
âˆ’. 197 39, Solution is : Î¸ = 1. 769 5 radians. You need to use a calculator or table to
solve this.
10. Find proju (v) where v = (1, 3, âˆ’2) and u = (1, 2, 3) .
Remember to ï¬nd this you take vÂ·u
uÂ·uu. Thus the answer is
1
14 (1, 2, 3) .
11. If F is a force and D is a vector, show projD (F) = (|F| cos Î¸) u where u is the unit
vector in the direction of D, u = D/ |D| and Î¸ is the included angle between the two
vectors, F and D. |F| cos Î¸ is sometimes called the component of the force, F in the
direction, D.
projD (F) = FÂ·D
DÂ·DD = |F| |D| cos Î¸
1
|D|2 D = |F| cos Î¸ D
|D|.
12. A boy drags a sled for 100 feet along the ground by pulling on a rope which is 40
degrees from the horizontal with a force of 10 pounds. How much work does this force
do?
The component of force is 10 cos
Â¡ 40
180Ï€
Â¢
and it acts for 100 feet so the work done is
10 cos
Âµ 40
180Ï€
Â¶
Ã— 100 = 766. 04
13. If a, b, and c are vectors. Show that (b + c)âŠ¥= bâŠ¥+ câŠ¥where bâŠ¥= bâˆ’proja (b) .
14. Find (1, 0, 3, 4) Â· (2, 7, 1, 3) . (1, 0, 3, 4) Â· (2, 7, 1, 3) = 17.

64
VECTOR PRODUCTS
15. Prove from the axioms of the dot product the parallelogram identity, |a + b|2 +
|a âˆ’b|2 = 2 |a|2 + 2 |b|2 .
Use the properties of the dot product and the deï¬nition of the norm in terms of the
dot product. Thus the left side is
a Â· a + b Â· b + 2 (a Â· b) + a Â· a + b Â· b âˆ’2a Â· b =2 |a|2 + 2 |b|2 .
16. Find all vectors, (x, y) which are perpendicular to (1, 2) .
You need x + 2y = 0 so x = âˆ’2y and you can write all desired vectors in the form
(âˆ’2y, y) : y âˆˆR.
17. Find the line through (1, 2, 1) and (2, 0, 3) .
First get a direction vector which in this case is (1, âˆ’2, 2) . Then the equation of the
line is
(x, y, z) = (1, 2, 1) + t (1, âˆ’2, 2) = (1 + t, 2 âˆ’2t, 1 + 2t) .
Thus a parametric form for this line is x = 1 + t, y = 2 âˆ’2t, z = 1 + 2t and a vector
equation for this line is
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
1
2
1
ï£¶
ï£¸+ t
ï£«
ï£­
1
âˆ’2
2
ï£¶
ï£¸
if you want to write the vectors as column vectors.
18. In R2, the equation of a line is given as 2x + 3y = 6. Find a vector equation of this
line.
One way to do it is to get a couple of points on the line and then do as in the previous
problem. Two points on this line are (0, 2) and (3, 0) . Then a direction vector for the
line is (âˆ’3, 2) and so a vector equation of the line is
(x, y) = (0, 2) + t (âˆ’3, 2) .
Written parametrically, this would be x = âˆ’3t, y = 2 + 2t.
19. Suppose you have the vector equation for a line joining the two points, p, q. This is
x = p + t (q âˆ’p)
Note this works because when t = 0 the right side is p and when t = 1, the right side
is q. Now ï¬nd the point which is 1/3 of the way between p and q.
This point would be obtained by letting t = 1/3. Thus the point is
x1/3 = 2
3p+1
3q.
Does it work?
Â¯Â¯x1/3 âˆ’p
Â¯Â¯ =
Â¯Â¯Â¯Â¯âˆ’1
3p+1
3q
Â¯Â¯Â¯Â¯ = 1
3 |q âˆ’p| .
Seems to work just ï¬ne. I suppose you could also ï¬nd points which are 1/5 of the way
between p and q also.

3.3.
FURTHER EXPLANATIONSâˆ—
65
20. The wind blows from West to East at a speed of 30 kilometers per hour and an airplane
which travels at 300 Kilometers per hour in still air is heading North West. What is
the velocity of the airplane relative to the ground? What is the component of this
velocity in the direction North?
Let the positive y axis point in the direction North and let the positive x axis point in
the direction East. The velocity of the wind is 30i. The plane moves in the direction
i + j. A unit vector in this direction is
1
âˆš
2 (i + j) . Therefore, the velocity of the plane
relative to the ground is 30i+ 300
âˆš
2 (i + j) = 150
âˆš
2j +
Â¡
30 + 150
âˆš
2
Â¢
i. The component
of velocity in the direction North is 150
âˆš
2.
21. In the situation of Problem 20 how many degrees to the West of North should the
airplane head in order to ï¬‚y exactly North. What will be the speed of the airplane
relative to the ground?
In this case the unit vector will be âˆ’sin (Î¸) i + cos (Î¸) j. Therefore, the velocity of the
plane will be
300 (âˆ’sin (Î¸) i + cos (Î¸) j)
and this is supposed to satisfy
300 (âˆ’sin (Î¸) i + cos (Î¸) j) + 30i = 0i+?j.
Therefore, you need to have sin Î¸ = 1/10, which means Î¸ = . 100 17 radians. Therefore,
the degrees should be .1Ã—180
Ï€
= 5. 729 6 degrees. In this case the velocity vector of the
plane relative to the ground is 300
Â³ âˆš
99
10
Â´
j.
22. In the situation of 21 suppose the airplane uses 34 gallons of fuel every hour at that
air speed and that it needs to ï¬‚y North a distance of 600 miles. Will the airplane have
enough fuel to arrive at its destination given that it has 63 gallons of fuel?
The airplane needs to ï¬‚y 600 miles at a speed of 300
Â³ âˆš
99
10
Â´
. Therefore, it takes
600
Â³
300
Â³ âˆš
99
10
Â´Â´ = 2. 010 1 hours to get there. Therefore, the plane will need to use about
68 gallons of gas. It wonâ€™t make it.
23. A certain river is one half mile wide with a current ï¬‚owing at 3 miles per hour from
East to West. A man swims directly toward the opposite shore from the South bank
of the river at a speed of 2 miles per hour. How far down the river does he ï¬nd himself
when he has swam across? How far does he end up swimming?
The velocity of the man relative to the earth is then âˆ’3i + 2j. Since the component
of j equals 2 it follows he takes 1/8 of an hour to get across. Durring this time he
is swept downstream at the rate of 3 miles per hour and so he ends up 3/8 of a mile
down stream. He has gone
qÂ¡ 3
8
Â¢2 +
Â¡ 1
2
Â¢2 = . 625 miles in all.
24. Three forces are applied to a point which does not move.
Two of the forces are
2i âˆ’j + 3k Newtons and i âˆ’3j âˆ’2k Newtons.
Find the third force.
Call it ai + bj + ck Then you need a + 2 + 1 = 0, b âˆ’1 âˆ’3 = 0, and c + 3 âˆ’2 = 0.
Therefore, the force is âˆ’3i + 4j âˆ’k.
25. If you only assume 3.23 holds for u = i, j, k, show that this implies 3.23 holds for all
unit vectors, u.
Suppose than that (Pp
k=1 Rkgmk âˆ’R0
Pp
k=1 gmk)Ã—u = 0 for u = i, j, k. Then if u is
an arbitrary unit vector, u must be of the form ai+bj+ck. Now from the distributive

66
VECTOR PRODUCTS
property of the cross product and letting w = (Pp
k=1 Rkgmk âˆ’R0
Pp
k=1 gmk), this
says
(Pp
k=1 Rkgmk âˆ’R0
Pp
k=1 gmk) Ã— u
= w Ã— (ai + bj + ck)
= aw Ã— i + bw Ã— j + cw Ã— k
= 0 + 0 + 0 = 0.
26. Let m1 = 4, m2 = 3, and m3 = 1 where the masses are in kilograms and the distance
is in meters. Suppose m1 is located at 2i âˆ’j + k, m2 is located at 2i âˆ’3j + k and m3
is located at 2i + j + 3k. Find the center of mass of these three masses.
Let the center of mass be located at ai + bj + ck. Then (4 + 3 + 1) (ai + bj + ck) =
4 (2i âˆ’j + k)+3 (2i âˆ’3j + k)+1 (2i + j + 3k) = 16iâˆ’12j+10k. Therefore, a = 2, b =
âˆ’3
2 and c = 5
4. The center of mass is then 2i âˆ’3
2j + 5
4k.
27. Find the angular velocity vector of a rigid body which rotates counter clockwise about
the vector i âˆ’j + k at 20 revolutions per minute. Assume distance is measured in
meters.
The angular velocity is 20 Ã— 2Ï€ = 40Ï€. Then â„¦= 40Ï€ 1
âˆš
3 (i âˆ’j + k) .
28. Find the area of the triangle determined by the three points, (1, 2, 3) , (1, 2, 6) and
(âˆ’3, 2, 1) .
The three points determine two displacement vectors from the point (1, 2, 3) , (0, 0, 3)
and (âˆ’4, 0, âˆ’2) . To ï¬nd the area of the parallelogram determined by these two dis-
placement vectors, you simply take the norm of their cross product. To ï¬nd the area
of the triangle, you take one half of that. Thus the area is
(1/2) |(0, 0, 3) Ã— (âˆ’4, 0, âˆ’2)| = 1
2 |(0, âˆ’12, 0)| = 6.
29. Find the area of the parallelogram determined by the vectors, (1, 0, 3) and (4, âˆ’2, 1) .
|(1, 0, 3) Ã— (4, âˆ’2, 1)| = |(6, 11, âˆ’2)| = âˆš26 + 121 + 4 =
âˆš
151.
30. Find the volume of the parallelepiped determined by the vectors, i âˆ’7j âˆ’5k, i + 2j âˆ’
6k,3i âˆ’3j + k.
Remember you just need to take the absolute value of the determinant having the
given vectors as rows. Thus the volume is the absolute value of
Â¯Â¯Â¯Â¯Â¯Â¯
1
âˆ’7
âˆ’5
1
2
âˆ’6
3
âˆ’3
1
Â¯Â¯Â¯Â¯Â¯Â¯
= 162
31. Suppose a, b, and c are three vectors whose components are all integers. Can you
conclude the volume of the parallelepiped determined from these three vectors will
always be an integer?
Hint: Consider what happens when you take the determinant of a matrix which has
all integers.
32. Using the notion of the box product yielding either plus or minus the volume of the
parallelepiped determined by the given three vectors, show that
(a Ã— b) Â·c = aÂ· (b Ã— c)

3.3.
FURTHER EXPLANATIONSâˆ—
67
In other words, the dot and the cross can be switched as long as the order of the
vectors remains the same. Hint: There are two ways to do this, by the coordinate
description of the dot and cross product and by geometric reasoning. It is best if you
use the geometric reasoning. Here is a picture which might help.
-Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
a
-

b
c
Â¤Â¤
6
a Ã— b
XXXXXXXXXXX
z
b Ã— c
In this picture there is an angle between a Ã— b and c. Call it Î¸. Now if you take
|a Ã— b| |c| cos Î¸ this gives the area of the base of the parallelepiped determined by a
and b times the altitude of the parallelepiped, |c| cos Î¸. This is what is meant by the
volume of the parallelepiped. It also equals a Ã— b Â· c by the geometric description of
the dot product. Similarly, there is an angle between b Ã— c and a. Call it Î±. Then
if you take |b Ã— c| |a| cos Î± this would equal the area of the face determined by the
vectors b and c times the altitude measured from this face, |a| cos Î±. Thus this also is
the volume of the parallelepiped. and it equals a Â· b Ã— c. The picture is not completely
representative. If you switch the labels of two of these vectors, say b and c, explain
why it is still the case that a Â· b Ã— c = a Ã— b Â· c. You should draw a similar picture
and explain why in this case you get âˆ’1 times the volume of the parallelepiped.
33. Discover a vector identity for(u Ã— v) Ã—w.
((u Ã— v) Ã—w)i
=
Îµijk (u Ã— v)j wk = ÎµijkÎµjrsurvswk = (Î´isÎ´kr âˆ’Î´irÎ´ks) urvswk
=
ukwkvi âˆ’uivkwk = (u Â· w) vi âˆ’(v Â· w) ui.
Therefore, (u Ã— v) Ã—w = (u Â· w) v âˆ’(v Â· w) u.
34. Discover a vector identity for (u Ã— v) Â· (z Ã— w) .
Start with ÎµijkujvkÎµirszrws and then go to work on it using the reduction identities
for the permutation symbol.
35. Discover a vector identity for (u Ã— v) Ã— (z Ã— w) in terms of box products.
You will save time if you use the identity for (u Ã— v) Ã—w or uÃ— (v Ã— w) .

68
VECTOR PRODUCTS

Part II
Planes And Systems Of
Equations
69


71
Outcomes
Planes
A. Find the equation of a plane in 3-space given a point and a normal vector, three points,
a sketch of a plane or a geometric description of the plane.
B. Determine a normal vector and the intercepts of a given plane.
C. Sketch the graph of a plane given its equation.
D. Determine the angle between two planes.
E. Find the equation of a plane determined by lines.
Reading: Multivariable Calculus 1.3, Linear Algebra 1.3
Outcome Mapping:
A. 1,3
B. 2,4
C. 4
D. 2
E. Problem 9 in Section 1.5 of Multivariable Calculus
Systems of Linear Equations
A. Deï¬ne linear equation and system of linear equations. Deï¬ne solution and solution
set for both an linear equation and a system of linear equations.
B. Relate the following types of solution sets of a system of two or three variables to the
intersections of lines in a plane or the intersection of planes in three space:
(a) a unique solution.
(b) inï¬nitely many solutions.
(c) no solution.
C. Represent a linear system as an augmented matrix and vice versa.
D. Transform a system to a triangular pattern and then apply back substitution to solve
the linear system.
E. Represent the solution set to a linear system using parametric equations.
Reading: Linear Algebra 2.1
Outcome Mapping:
A. 1-6, 7-10
B. 15-18
C. 27-30, 31-32
D. 19-24, 25-26, 33-38

72
E. 11-14, 39-40
Direct Methods for Solving Linear Systems
A. Identify matrices that are in row echelon form and reduced row echelon form.
B. Determine whether a system of linear equations has no solution, a unique solution or
an inï¬nite number of solutions from its echelon form.
C. Apply elementary row operations to transform systems of linear equations.
D. Solve systems of linear equations using Gaussian elimination.
E. Solve systems of linear equations using Gauss-Jordan elimination.
F. Deï¬ne and evaluate the rank of a matrix.
G. Apply the Rank Theorem relate the rank of an augmented matrix to the solution set
of a system in the case of homogeneous and nonhomogeneous systems.
H. Model and solve application problems using linear systems.
Reading: Linear Algebra 2.2
Outcome Mapping:
A. 1-8,24
B. 39-44
C. 9-14,15-16,17-18,19-22
D. 25-34
E. 23
F. 35-38
G. 45-52, (2.4: 1-47)

Planes 11 Sept.
4.1
Finding Planes
Quiz
1. Let a = (1, 2, 3) , b = (2, âˆ’1, 1) . Find a vector which is perpendicular to both of these
vectors.
2. Find the area of the parallelogram determined by the above two vectors.
3. Find the cosine of the angle between the above two vectors.
4. Find the sine of the angle between the above two vectors.
5. Find the volume of the parallelepiped determined by the vectors, a = (1, 2, 3) , b = (2, âˆ’1, 1)
and c = (1, 1, 1).
4.1.1
Planes From A Normal And A Point
A plane is a long ï¬‚at thing. It can also be considered geometrically in terms of a dot product.
To ï¬nd the equation of a plane, you need two things, a point contained in the plane and a
vector normal to the plane. Let p0 = (x0, y0, z0) denote the position vector of a point in
the plane, let p = (x, y, z) be the position vector of an arbitrary point in the plane, and let
n denote a vector normal to the plane. This means that
nÂ· (p âˆ’p0) = 0
whenever p is the position vector of a point in the plane. The following picture illustrates
the geometry of this idea.
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â£
Â¡
Â¡
Â¡
Â¡
Â¡
 p0
Â¢
Â¢
Â¢
Â¢
Â¢

p i
Â¤
Â¤
Â¤Â¤n
P
Expressed equivalently, the plane is just the set of all points p such that the vector,
p âˆ’p0 is perpendicular to the given normal vector, n.
73

74
PLANES 11 SEPT.
Example 4.1.1 Find the equation of the plane with normal vector, n = (1, 2, 3) containing
the point (2, âˆ’1, 5) .
From the above, the equation of this plane is
(1, 2, 3) Â· (x âˆ’2, y + 1, z âˆ’5) = x âˆ’15 + 2y + 3z = 0
Example 4.1.2 2x + 4y âˆ’5z = 11 is the equation of a plane. Find the normal vector and
a point on this plane.
You can write this in the form 2
Â¡
x âˆ’11
2
Â¢
+ 4 (y âˆ’0) + (âˆ’5) (z âˆ’0) = 0. Therefore, a
normal vector to the plane is 2i + 4j âˆ’5k and a point in this plane is
Â¡ 11
2 , 0, 0
Â¢
. Of course
there are many other points in the plane.
Proposition 4.1.3 If (a, b, c) Ì¸= (0, 0, 0) , then ax + by + cz = d is the equation of a
plane with normal vector ai + bj + ck. Conversely, any plane can be written in this form.
Proof: One of a, b, c is nonzero. Suppose for example that c Ì¸= 0. Then the equation can
be written as
a (x âˆ’0) + b (y âˆ’0) + c
Âµ
z âˆ’d
c
Â¶
= 0
Therefore,
Â¡
0, 0, d
c
Â¢
is a point on the plane and a normal vector is ai + bj + ck. Suppose
a Ì¸= 0. Then the points which satisfy ax + by + cz = d are the same as the points which
satisfy
a
Âµ
x âˆ’d
a
Â¶
+ b (y âˆ’0) + c (z âˆ’0) = 0.
Thus a point on the plane is
Â¡ d
a, 0, 0
Â¢
and a normal vector is (a, b, c) as claimed. (You can
do something similar if b Ì¸= 0. Note there are many points on the plane. This just picks out
one.)
The converse follows from the above discussion involving the point and a normal vector.
This proves the proposition.
Example 4.1.4 Find a normal vector to the plane 2x + 5y âˆ’z = 12.3.
A normal vector is (2, 5, âˆ’1). A point on this plane is (0, 0, âˆ’12.3). Of course there are
many other points on this plane.
4.1.2
The Angle Between Two Planes
Deï¬nition 4.1.5 Suppose two planes intersect.
The angle between the planes is
deï¬ned to be the angle between their normal vectors.
Example 4.1.6 Find the angle between the two planes, x+2y âˆ’z = 6 and 3x+2y âˆ’z = 7.
The two normal vectors are (1, 2, âˆ’1) and (3, 2, âˆ’1) . Therefore, the cosine of the angle
desired is
cos Î¸ =
(1, 2, âˆ’1) Â· (3, 2, âˆ’1)
q
12 + 22 + (âˆ’1)2q
32 + 22 + (âˆ’1)2 = . 872 87
Now use a calculator or table to ï¬nd what the angle is.
cos Î¸ = . 872 87, Solution is :
{Î¸ = . 509 74} . This value is in radians.

4.1.
FINDING PLANES
75
4.1.3
The Plane Which Contains Three Points
Sometimes you need to ï¬nd the equation of a plane which contains three points. Consider
the following picture.

3
-
(a0, b0, c0)
s
s
s
(a1, b1, c1)
(a2, b2, c2)
a
b
You have plenty of points but you need a normal. This can be obtained by taking a Ã— b
where a = (a1 âˆ’a0, b1 âˆ’b0, c1 âˆ’c0) and b = (a2 âˆ’a0, b2 âˆ’b0, c2 âˆ’c0) .
Example 4.1.7 Find the equation of the plane which contains the three points, (1, 2, 1) , (3, âˆ’1, 2) ,
and (4, 2, 1) .
You just need to get a normal vector to this plane. This can be done by taking the cross
products of the two vectors,
(3, âˆ’1, 2) âˆ’(1, 2, 1) and (4, 2, 1) âˆ’(1, 2, 1)
Thus a normal vector is (2, âˆ’3, 1) Ã— (3, 0, 0) = (0, 3, 9) . Therefore, the equation of the plane
is
0 (x âˆ’1) + 3 (y âˆ’2) + 9 (z âˆ’1) = 0
or 3y + 9z = 15 which is the same as y + 3z = 5. When you have what you think is the
plane containing the three points, you ought to check it by seeing if it really does contain
the three points.
Example 4.1.8 Find the equation of the plane which contains the three points, (1, 2, 1) , (3, âˆ’1, 2) ,
and (4, 2, 1) another way.
Letting (x, y, z) be a point on the plane, the volume of the parallelepiped spanned by
(x, y, z) âˆ’(1, 2, 1) and the two vectors, (2, âˆ’3, 1) and (3, 0, 0) must be equal to zero. Thus
the equation of the plane is
Â¯Â¯Â¯Â¯Â¯Â¯
3
0
0
2
âˆ’3
1
x âˆ’1
y âˆ’2
z âˆ’1
Â¯Â¯Â¯Â¯Â¯Â¯
= 0.
Hence âˆ’9z + 15 âˆ’3y = 0 and dividing by 3 yields the same answer as the above.
Example 4.1.9 Find the equation of the plane containing the points (1, 2, 3) and the line
(0, 1, 1) + t (2, 1, 2) = (x, y, z).
There are several ways to do this. One is to ï¬nd three points and use any of the above
procedures.
Let t = 0 and then let t = 1 to get two points on the line.
This yields
(1, 2, 3) , (0, 1, 1) , and (2, 2, 3) . Then procede as above.

76
PLANES 11 SEPT.
Example 4.1.10 Find the equation of the plane which contains the two lines, given by the
following parametric expressions in which t âˆˆR.
(2t, 1 + t, 1 + 2t) = (x, y, z) , (2t + 2, 1, 3 + 2t) = (x, y, z)
Note ï¬rst that you donâ€™t know there even is such a plane. However, if there is, you could
ï¬nd it by obtaining three points, two on one line and one on another and then using any of
the above procedures for ï¬nding the plane. From the ï¬rst line, two points are (0, 1, 1) and
(2, 2, 3) while a third point can be obtained from second line, (2, 1, 3) . You need a normal
vector and then use any of these points. To get a normal vector, form (2, 0, 2) Ã— (2, 1, 2) =
(âˆ’2, 0, 2) . Therefore, the plane is âˆ’2x+0 (y âˆ’1)+2 (z âˆ’1) = 0. This reduces to z âˆ’x = 1.
If there is a plane, this is it. Now you can simply verify that both of the lines are really in
this plane. From the ï¬rst, (1 + 2t) âˆ’2t = 1 and the second, (3 + 2t) âˆ’(2t + 2) = 1 so both
lines lie in the plane.
4.1.4
Intercepts Of A Plane
One way to understand how a plane looks is to connect the points where it intercepts the
x, y, and z axes. This allows you to visualize the plane somewhat and is a good way to
sketch the plane. Not surprisingly these points are called intercepts.
Example 4.1.11 Sketch the plane which has intercepts (2, 0, 0) , (0, 3, 0) , and (0, 0, 4) .
Â¡
Â¡
x
y
z
You see how connecting the intercepts gives a fairly good geometric description of the
plane. These lines which connect the intercepts are also called the traces of the plane. Thus
the line which joins (0, 3, 0) to (0, 0, 4) is the intersection of the plane with the yz plane. It
is the trace on the yz plane.
Example 4.1.12 Identify the intercepts of the plane, 3x âˆ’4y + 5z = 11.
The easy way to do this is to divide both sides by 11.
x
(11/3) +
y
(âˆ’11/4) +
z
(11/5) = 1
The intercepts are (11/3, 0, 0) , (0, âˆ’11/4, 0) and (0, 0, 11/5) . You can see this by letting
both y and z equal to zero to ï¬nd the point on the x axis which is intersected by the plane.
The other axes are handled similarly.
In general, to ï¬nd the intercepts of a plane of the form ax + by + cz = d where d Ì¸= 0
and none of a, b, or c are equal to 0, divide by d. This gives
x
(d/a) +
y
(d/b) +
z
(d/c) = 1
the intercepts are
Â¡ d
a, 0, 0
Â¢
,
Â¡
0, d
b , 0
Â¢
,
Â¡
0, 0, d
c
Â¢
.

4.1.
FINDING PLANES
77
4.1.5
Distance Between A Point And A Plane Or A Point And A
Lineâˆ—
There exists a stupid formula for the distance between a point and a plane. I will ï¬rst
illustrate with an example.
Example 4.1.13 Find the distance from the point (1, 2, 3) to the plane x âˆ’y + z = 3.
The distance is the length of the line segment normal to the plane which goes from the
given point to the given plane. In this example, a direction vector for this line is (1, âˆ’1, 1) ,
a normal vector to the plane. Thus the equation for the desired line is
(x, y, z)
=
(1, 2, 3) + t (1, âˆ’1, 1)
=
(1 + t, 2 âˆ’t, 3 + t)
Lets ï¬nd the value of t at which the line intersects the plane. Thus
(1 + t) âˆ’(2 âˆ’t) + (3 + t) = 3
and so t = 1
3. Therefore, the line segment is the one which joins (1, 2, 3) to
Âµ
1 + 1
3, 2 âˆ’1
3, 3 + 1
3
Â¶
=
Âµ4
3, 5
3, 10
3
Â¶
.
Now it follows the desired distance is
sÂµ
1 âˆ’4
3
Â¶2
+
Âµ
2 âˆ’5
3
Â¶2
+
Âµ
3 âˆ’10
3
Â¶2
= 1
3
âˆš
3
In the general case there is a simple and interesting geometrical consideration which will
lead to a stupid formula which you can then use with no thought to do an uninteresting
task, ï¬nding the distance from a point to a plane.
Example 4.1.14 Find the distance from the point (x0, y0, z0) to the plane ax+by +cz = d.
Here (a, b, c) Ì¸= (0, 0, 0) .
Consider the following picture in which P0 is a point in the plane and X0 = (x0, y0, z0)
is the point whose distance to the plane is to be found. The normal to the plane is n.


C
C
C
CO
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡

n
Î¸
r (x0, y0, z0) = X0
P0
r
Then from the picture, what you want is to take the projection of the vector X0 âˆ’P0
onto the line determined by the point, P0 in the direction, n. That is, you need
|X0 âˆ’P0| |cos Î¸|
=
|X0 âˆ’P0|
Â¯Â¯Â¯Â¯
(X0 âˆ’P0) Â· n
|X0 âˆ’P0| |n|
Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯(X0 âˆ’P0) Â· n
|n|
Â¯Â¯Â¯Â¯

78
PLANES 11 SEPT.
As drawn in the picture, |X0 âˆ’P0| cos Î¸ will be positive but if you had n pointing the
opposite direction this would be negative. However, either way, itâ€™s absolute value would
give the right answer. This is why the absolute value is taken in the above. From this the
stupid formula will follow easily. Suppose a Ì¸= 0. Things work the same if b or c are not
zero. Then as explained above, you can take P0 =
Â¡ d
a, 0, 0
Â¢
and n = (a, b, c) . Therefore, the
above expression is
Â¯Â¯Â¯Â¯
Âµ
x0 âˆ’d
a, y0, z0
Â¶
Â·
(a, b, c)
âˆš
a2 + b2 + c2
Â¯Â¯Â¯Â¯ = |ax0 + by0 + cz0 âˆ’d|
âˆš
a2 + b2 + c2
and it is this last expression which is the stupid formula. Here is the same example done in
an ad hoc manner earlier but this time through the use of a stupid formula.
Example 4.1.15 Find the distance from the point (1, 2, 3) to the plane x âˆ’y + z = 3.
Lets apply the stupid formula. a = 1, b = âˆ’1, c = 1, d = 3, x0 = 1, y0 = 2, z0 = 3. Then
plugging in to the formula, you get
|1 Ã— 1 + (âˆ’1) Ã— 2 + 1 Ã— 3 âˆ’3|
âˆš1 + 1 + 1
= 1
3
âˆš
3
which gives the same answer much more easily. Those of you who expect to ï¬nd the distance
from a given point to a plane repeatedly, should certainly cherish and memorize this formula
because it will save you lots of time. The rest of you should try to understand its derivation
which is genuinely interesting and worth while. Unfortunately, ï¬nding the distance from a
point to a plane is an excellent test question.
A similar formula holds for the distance from a point to a line in R2. Recall from high
school algebra, a line can be written as
ax + by = c
Then if (x0, y0) is a point and you want the distance from this point to the given line, it
equals
|ax0 + by0 âˆ’c|
âˆš
a2 + b2
.
You should derive this stupid formula from the same geometric considerations used to get
the stupid formula for a point and a plane.
Of course it all generalizes. The same reasoning will yield a stupid formula for the dis-
tance between (y1, Â· Â· Â·, yn) and the level surface, called a hyper1 plane given by Pn
k=1 akxk =
d. You can probably guess what it is by analogy to the above but it is better to derive it
directly using the same sort of geometric reasoning just given.
1Words such as â€œhyperâ€ give an aura of signiï¬cance to things which are in reality trivial while obfus-
cating the real issues. They constitute an example of pretentious jargon which militates against correct
understanding.

Systems Of Linear Equations
12,13 Sept.
Quiz
1. The intercepts of a plane are (1, 0, 0) , (0, 2, 0) , and (0, 0, âˆ’1) . Find the equation of
the plane.
2. A plane has a normal vector (1, 2, âˆ’3) and contains the point (1, 1, 2) . Find the equa-
tion of the plane.
3. Find the equation of a plane which has the three points, (1, 2, 1) , (2, âˆ’2, 1) , (0, 3, 0) .
5.1
Systems Of Equations, Geometric Interpretations
As you know, equations like 2x + 3y = 6 can be graphed as straight lines. To ï¬nd the
solution to two such equations, you could graph the two straight lines and the ordered pairs
identifying the point (or points) of intersection would give the x and y values of the solution
to the two equations because such an ordered pair satisï¬es both equations. The following
picture illustrates what can occur with two equations involving two variables.
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
HHHHHHH
H
x
y
one solution
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡
x
y
two parallel lines
no solutions
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
x
y
inï¬nitely
many solutions
In the ï¬rst example of the above picture, there is a unique point of intersection. In the
second, there are no points of intersection. The other thing which can occur is that the
two lines are really the same line. For example, x + y = 1 and 2x + 2y = 2 are relations
which when graphed yield the same line. In this case there are inï¬nitely many points in the
simultaneous solution of these two equations, every ordered pair which is on the graph of
the line. It is always this way when considering linear systems of equations. There is either
no solution, exactly one or inï¬nitely many although the reasons for this are not completely
comprehended by considering a simple picture in two dimensions.
Example 5.1.1 Find the solution to the system x + y = 3, y âˆ’x = 5.
79

80
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
You can verify the solution is (x, y) = (âˆ’1, 4) . You can see this geometrically by graphing
the equations of the two lines. If you do so correctly, you should obtain a graph which looks
something like the following in which the point of intersection represents the solution of the
two equations.
Â¡
Â¡
Â¡
Â¡
@
@
@
x
(x, y) = (âˆ’1, 4) -
r
Example 5.1.2 You can also imagine other situations such as the case of three intersecting
lines having no common point of intersection or three intersecting lines which do intersect
at a single point as illustrated in the following picture.
Â¡
Â¡
Â¡
Â¡
@
@
@
x
y
Â¡
Â¡
Â¡
Â¡
Â¡


x
@
@
@
@
@@
y
In the case of the ï¬rst picture above, there would be no solution to the three equations
whose graphs are the given lines. In the case of the second picture there is a solution to the
three equations whose graphs are the given lines.
The points, (x, y, z) satisfying an equation in three variables like 2x + 4y âˆ’5z = 8
form a plane in three dimensions and geometrically, when you solve systems of equations
involving three variables, you are taking intersections of planes.
Consider the following
picture involving two planes.
Â¡
Â¡
Â¡Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡
@
@
@
@
@
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡@
@
@
@
@
@
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
@
@
@
@@
Notice how these two planes intersect in a line. It could also happen the two planes
could fail to intersect.

5.1.
SYSTEMS OF EQUATIONS, GEOMETRIC INTERPRETATIONS
81
Now imagine a third plane. One thing that could happen is this third plane could have
an intersection with one of the ï¬rst planes which results in a line which fails to intersect the
ï¬rst line as illustrated in the following picture.
Â¡
Â¡Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡
@
@
@
@
@
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡@
@@
@@
@
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
@
@
@
@@
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Âª
New Plane
Thus there is no point which lies in all three planes. The picture illustrates the situation
in which the line of intersection of the new plane with one of the original planes forms a line
parallel to the line of intersection of the ï¬rst two planes. However, in three dimensions, it
is possible for two lines to fail to intersect even though they are not parallel. Such lines are
called skew lines. You might consider whether there exist two skew lines, each of which
is the intersection of a pair of planes selected from a set of exactly three planes such that
there is no point of intersection between the three planes. You can also see that if you tilt
one of the planes you could obtain every pair of planes having a nonempty intersection in a
line and yet there may be no point in the intersection of all three.
It could happen also that the three planes could intersect in a single point as shown in
the following picture.
Â¡
Â¡
Â¡Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡
@
@
@
@
@
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡
@
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
@
@
@
@@
r
Â¡
Â¡
Â¡
Âª
New Plane
In this case, the three planes have a single point of intersection. The three planes could
also intersect in a line.

82
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
Â¡
Â¡
Â¡Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡
@
@
@
@
@
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡@
@
@
@
@
@
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
@
@
@
@@





Â¡Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡





Thus in the case of three equations having three variables, the planes determined by these
equations could intersect in a single point, a line, or even fail to intersect at all. You see
that in three dimensions there are many possibilities. If you want to waste some time, you
can try to imagine all the things which could happen but this will not help for dimensions
higher than 3 which is where many of the important applications lie.
In higher dimensions it is customary to refer to the set of points described by relations
like x + y âˆ’2z + 4w = 8 as hyper-planes.1 Such pictures as above are useful in two or
three dimensions for gaining insight into what can happen but they are not adequate for
obtaining the exact solution set of the linear system. The only rational and useful way to
deal with this subject is through the use of algebra. Indeed, a major reason for studying
mathematics is to obtain freedom from always having to draw a picture in order to do a
computation or ï¬nd out something important.
5.2
Systems Of Equations, Algebraic Procedures
5.2.1
Elementary Operations
Deï¬nition 5.2.1 A system of linear equations is a set of p equations for the n
variables, x1, Â· Â· Â·, xn which is of the form
n
X
k=1
amkxk = dm, m = 1, 2, Â· Â· Â·, p
Written less compactly it is a set of equations of the following form
a11x1 + a12x2 + Â· Â· Â· + a1nxn = d1
a21x1 + a22x2 + Â· Â· Â· + a2nxn = d2
...
ap1x1 + ap2x2 + Â· Â· Â· + apnxn = dp
1The evocative semi word, â€œhyperâ€ conveys absolutely no meaning but is traditional usage which makes
the terminology sound more impressive than something like long wide comparatively ï¬‚at thing which does
convey some meaning. However, in such cases as these pretentious jargon is nearly always preferred. Later
we will discuss some terms which are not just evocative but yield real understanding.

5.2.
SYSTEMS OF EQUATIONS, ALGEBRAIC PROCEDURES
83
The problem is to ï¬nd the values of x1, x2 Â· Â·Â·, xn which satisfy all p equations. This is called
the solution set of the system of equations. In other words, (a1, Â· Â· Â·, an) is in the solution
set of the system of equations if when you plug a1 in place of x1, a2 in place of x2 etc., each
equation in the system is satisï¬ed.
Consider the following example.
Example 5.2.2 Find x and y such that
x + y = 7 and 2x âˆ’y = 8.
(5.1)
The set of ordered pairs, (x, y) which solve both equations is called the solution set.
You can verify that (x, y) = (5, 2) is a solution to the above system. The interesting
question is this: If you were not given this information to verify, how could you determine
the solution? You can do this by using the following basic operations on the equations, none
of which change the set of solutions of the system of equations.
Deï¬nition 5.2.3 Elementary operations are those operations consisting of the
following.
1. Interchange the order in which the equations are listed.
2. Multiply any equation by a nonzero number.
3. Replace any equation with itself added to a multiple of another equation.
Example 5.2.4 To illustrate the third of these operations on this particular system, con-
sider the following.
x + y = 7
2x âˆ’y = 8
The system has the same solution set as the system
x + y = 7
âˆ’3y = âˆ’6 .
To obtain the second system, take the second equation of the ï¬rst system and add -2 times
the ï¬rst equation to obtain
âˆ’3y = âˆ’6.
Now, this clearly shows that y = 2 and so it follows from the other equation that x + 2 = 7
and so x = 5.
Of course a linear system may involve many equations and many variables. The solution
set is still the collection of solutions to the equations. In every case, the above operations
of Deï¬nition 5.2.3 do not change the set of solutions to the system of linear equations.
Theorem 5.2.5 Suppose you have two equations, involving the variables, (x1, Â· Â· Â·, xn)
E1 = f1, E2 = f2
(5.2)
where E1 and E2 are expressions involving the variables and f1 and f2 are constants. (In
the above example there are only two variables, x and y and E1 = x + y while E2 = 2x âˆ’y.)
Then the system E1 = f1, E2 = f2 has the same solution set as
E1 = f1, E2 + aE1 = f2 + af1.
(5.3)
Also the system E1 = f1, E2 = f2 has the same solutions as the system, E2 = f2, E1 = f1.
The system E1 = f1, E2 = f2 has the same solution as the system E1 = f1, aE2 = af2
provided a Ì¸= 0.

84
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
Proof: If (x1, Â· Â· Â·, xn) solves E1 = f1, E2 = f2 then it solves the ï¬rst equation in
E1 = f1, E2+aE1 = f2+af1. Also, it satisï¬es aE1 = af1 and so, since it also solves E2 = f2
it must solve E2 +aE1 = f2 +af1. Therefore, if (x1, Â· Â· Â·, xn) solves E1 = f1, E2 = f2 it must
also solve E2 + aE1 = f2 + af1. On the other hand, if it solves the system E1 = f1 and
E2 + aE1 = f2 + af1, then aE1 = af1 and so you can subtract these equal quantities from
both sides of E2+aE1 = f2+af1 to obtain E2 = f2 showing that it satisï¬es E1 = f1, E2 = f2.
The second assertion of the theorem which says that the system E1 = f1, E2 = f2 has the
same solution as the system, E2 = f2, E1 = f1 is seen to be true because it involves nothing
more than listing the two equations in a diï¬€erent order. They are the same equations.
The third assertion of the theorem which says E1 = f1, E2 = f2 has the same solution
as the system E1 = f1, aE2 = af2 provided a Ì¸= 0 is veriï¬ed as follows: If (x1, Â· Â· Â·, xn) is a
solution of E1 = f1, E2 = f2, then it is a solution to E1 = f1, aE2 = af2 because the second
system only involves multiplying the equation, E2 = f2 by a. If (x1, Â· Â· Â·, xn) is a solution of
E1 = f1, aE2 = af2, then upon multiplying aE2 = af2 by the number, 1/a, you ï¬nd that
E2 = f2.
Stated simply, the above theorem shows that the elementary operations do not change
the solution set of a system of equations.
Here is an example in which there are three equations and three variables. You want to
ï¬nd values for x, y, z such that each of the given equations are satisï¬ed when these values
are plugged in to the equations.
Example 5.2.6 Find the solutions to the system,
x + 3y + 6z = 25
2x + 7y + 14z = 58
2y + 5z = 19
(5.4)
To solve this system replace the second equation by (âˆ’2) times the ï¬rst equation added
to the second. This yields the system
x + 3y + 6z = 25
y + 2z = 8
2y + 5z = 19
(5.5)
Now take (âˆ’2) times the second and add to the third. More precisely, replace the third
equation with (âˆ’2) times the second added to the third. This yields the system
x + 3y + 6z = 25
y + 2z = 8
z = 3
(5.6)
At this point, you can tell what the solution is. This system has the same solution as the
original system and in the above, z = 3. Then using this in the second equation, it follows
y + 6 = 8 and so y = 2. Now using this in the top equation yields x + 6 + 18 = 25 and so
x = 1. This process is called back substitution.
Alternatively, in 5.6 you could have continued as follows. Add (âˆ’2) times the bottom
equation to the middle and then add (âˆ’6) times the bottom to the top. This yields
x + 3y = 7
y = 2
z = 3

5.2.
SYSTEMS OF EQUATIONS, ALGEBRAIC PROCEDURES
85
Now add (âˆ’3) times the second to the top. This yields
x = 1
y = 2
z = 3
,
a system which has the same solution set as the original system. This avoided back substi-
tution and led to the same solution set.
5.2.2
Gauss Elimination
A less cumbersome way to represent a linear system is to write it as an augmented matrix.
For example the linear system, 5.4 can be written as
ï£«
ï£­
1
3
6
| 25
2
7
14
| 58
0
2
5
| 19
ï£¶
ï£¸.
It has exactly the same information as the original system but here it is understood there is
an x column,
ï£«
ï£­
1
2
0
ï£¶
ï£¸, a y column,
ï£«
ï£­
3
7
2
ï£¶
ï£¸and a z column,
ï£«
ï£­
6
14
5
ï£¶
ï£¸. The rows correspond
to the equations in the system. Thus the top row in the augmented matrix corresponds to
the equation,
x + 3y + 6z = 25.
Now when you replace an equation with a multiple of another equation added to itself, you
are just taking a row of this augmented matrix and replacing it with a multiple of another
row added to it. Thus the ï¬rst step in solving 5.4 would be to take (âˆ’2) times the ï¬rst row
of the augmented matrix above and add it to the second row,
ï£«
ï£­
1
3
6
| 25
0
1
2
| 8
0
2
5
| 19
ï£¶
ï£¸.
Note how this corresponds to 5.5. Next take (âˆ’2) times the second row and add to the
third,
ï£«
ï£­
1
3
6
| 25
0
1
2
| 8
0
0
1
| 3
ï£¶
ï£¸
This augmented matrix corresponds to the system
x + 3y + 6z = 25
y + 2z = 8
z = 3
which is the same as 5.6. By back substitution you obtain the solution x = 1, y = 6, and
z = 3.
In general a linear system is of the form
a11x1 + Â· Â· Â· + a1nxn = b1
...
am1x1 + Â· Â· Â· + amnxn = bm
,
(5.7)

86
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
where the xi are variables and the aij and bi are constants. This system can be represented
by the augmented matrix,
ï£«
ï£¬
ï£­
a11
Â· Â· Â·
a1n
|
b1
...
...
|
...
am1
Â· Â· Â·
amn
|
bm
ï£¶
ï£·
ï£¸.
(5.8)
Changes to the system of equations in 5.7 as a result of an elementary operations translate
into changes of the augmented matrix resulting from a row operation. Note that Theorem
5.2.5 implies that the row operations deliver an augmented matrix for a system of equations
which has the same solution set as the original system.
Deï¬nition 5.2.7 The row operations consist of the following
1. Switch two rows.
2. Multiply a row by a nonzero number.
3. Replace a row by a multiple of another row added to it.
Gauss elimination is a systematic procedure to simplify an augmented matrix to a
reduced form.
In the following deï¬nition, the term â€œleading entryâ€ refers to the ï¬rst
nonzero entry of a row when scanning the row from left to right.
Deï¬nition 5.2.8 An augmented matrix is in echelon form also called row ech-
elon form if
1. All nonzero rows are above any rows of zeros.
2. Each leading entry of a row is in a column to the right of the leading entries of any
rows above it.
Deï¬nition 5.2.9 An augmented matrix is in row reduced echelon form if
1. All nonzero rows are above any rows of zeros.
2. Each leading entry of a row is in a column to the right of the leading entries of any
rows above it.
3. All entries in a column above and below a leading entry are zero.
4. Each leading entry is a 1, the only nonzero entry in its column.
The relation between these two deï¬nitions is as described in the following picture.
row echelon form
row reduced echelon form
Thus if the matrix is in row reduced echelon form, it is in row echelon form but not
necessarily the other way around. You can usually ï¬nd the solution to a system of equations
by row reducing to row echelon form. You typically donâ€™t have to go all the way to the row
reduced echelon form but the row reduced echelon form is very important because, unlike
a row echelon form, it is unique. It is also easier to use in the case where the system of
equations has an inï¬nite solution set.

5.2.
SYSTEMS OF EQUATIONS, ALGEBRAIC PROCEDURES
87
Example 5.2.10 Here are some augmented matrices which are in row reduced echelon form.
ï£«
ï£¬
ï£¬
ï£­
1
0
0
5
8
|
0
0
0
1
2
7
|
0
0
0
0
0
0
|
1
0
0
0
0
0
|
0
ï£¶
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
|
0
0
1
0
|
0
0
0
1
|
0
0
0
0
|
1
0
0
0
|
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Example 5.2.11 Here are augmented matrices in echelon form which are not in row re-
duced echelon form but which are in echelon form.
ï£«
ï£¬
ï£¬
ï£­
1
0
6
5
8
|
2
0
0
2
2
7
|
3
0
0
0
0
0
|
1
0
0
0
0
0
|
0
ï£¶
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
3
5
|
4
0
2
0
|
7
0
0
3
|
0
0
0
0
|
1
0
0
0
|
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
Example 5.2.12 Here are some augmented matrices which are not in echelon form.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
0
|
0
1
2
3
|
3
0
1
0
|
2
0
0
0
|
1
0
0
0
|
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
,
ï£«
ï£­
1
2
|
3
2
4
|
âˆ’6
4
0
|
7
ï£¶
ï£¸,
ï£«
ï£¬
ï£¬
ï£­
0
2
3
|
3
1
5
0
|
2
7
5
0
|
1
0
0
1
|
0
ï£¶
ï£·
ï£·
ï£¸.
Deï¬nition 5.2.13 A pivot position in a matrix is the location of a leading entry
in an echelon form resulting from the application of row operations to the matrix. A pivot
column is a column that contains a pivot position.
For example consider the following.
Example 5.2.14 Suppose
A =
ï£«
ï£­
1
2
3
|
4
3
2
1
|
6
4
4
4
|
10
ï£¶
ï£¸
Where are the pivot positions and pivot columns?
Replace the second row by âˆ’3 times the ï¬rst added to the second. This yields
ï£«
ï£­
1
2
3
|
4
0
âˆ’4
âˆ’8
|
âˆ’6
4
4
4
|
10
ï£¶
ï£¸.
This is not in reduced echelon form so replace the bottom row by âˆ’4 times the top row
added to the bottom. This yields
ï£«
ï£­
1
2
3
|
4
0
âˆ’4
âˆ’8
|
âˆ’6
0
âˆ’4
âˆ’8
|
âˆ’6
ï£¶
ï£¸.
This is still not in reduced echelon form. Replace the bottom row by âˆ’1 times the middle
row added to the bottom. This yields
ï£«
ï£­
1
2
3
|
4
0
âˆ’4
âˆ’8
|
âˆ’6
0
0
0
|
0
ï£¶
ï£¸

88
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
which is in echelon form, although not in reduced echelon form. Therefore, the pivot posi-
tions in the original matrix are the locations corresponding to the ï¬rst row and ï¬rst column
and the second row and second columns as shown in the following:
ï£«
ï£­
1
2
3
|
4
3
2
1
|
6
4
4
4
|
10
ï£¶
ï£¸
Thus the pivot columns in the matrix are the ï¬rst two columns.
The following is the algorithm for obtaining a matrix which is in row reduced echelon
form.
Algorithm 5.2.15
This algorithm tells how to start with a matrix and do row operations on it in such a
way as to end up with a matrix in row reduced echelon form.
1. Find the ï¬rst nonzero column from the left.
This is the ï¬rst pivot column.
The
position at the top of the ï¬rst pivot column is the ï¬rst pivot position. Switch rows if
necessary to place a nonzero number in the ï¬rst pivot position.
2. Use row operations to zero out the entries below the ï¬rst pivot position.
3. Ignore the row containing the most recent pivot position identiï¬ed and the rows above
it. Repeat steps 1 and 2 to the remaining submatrix, the rectangular array of numbers
obtained from the original matrix by deleting the rows you just ignored. Repeat the
process until there are no more rows to modify. The matrix will then be in echelon
form.
4. Moving from right to left, use the nonzero elements in the pivot positions to zero out
the elements in the pivot columns which are above the pivots.
5. Divide each nonzero row by the value of the leading entry. The result will be a matrix
in row reduced echelon form.
This row reduction procedure applies to both augmented matrices and non augmented
matrices. There is nothing special about the augmented column with respect to the row
reduction procedure.
Example 5.2.16 Here is a matrix.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
2
3
2
0
1
1
4
3
0
0
1
2
2
0
0
0
0
0
0
0
0
2
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
Do row reductions till you obtain a matrix in echelon form. Then complete the process by
producing one in reduced echelon form.
The pivot column is the second. Hence the pivot position is the one in the ï¬rst row and
second column. Switch the ï¬rst two rows to obtain a nonzero entry in this pivot position.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
1
4
3
0
0
2
3
2
0
0
1
2
2
0
0
0
0
0
0
0
0
2
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸

5.2.
SYSTEMS OF EQUATIONS, ALGEBRAIC PROCEDURES
89
Step two is not necessary because all the entries below the ï¬rst pivot position in the resulting
matrix are zero. Now ignore the top row and the columns to the left of this ï¬rst pivot
position. Thus you apply the same operations to the smaller matrix,
ï£«
ï£¬
ï£¬
ï£­
2
3
2
1
2
2
0
0
0
0
2
1
ï£¶
ï£·
ï£·
ï£¸.
The next pivot column is the third corresponding to the ï¬rst in this smaller matrix and the
second pivot position is therefore, the one which is in the second row and third column. In
this case it is not necessary to switch any rows to place a nonzero entry in this position
because there is already a nonzero entry there. Multiply the third row of the original matrix
by âˆ’2 and then add the second row to it. This yields
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
1
4
3
0
0
2
3
2
0
0
0
âˆ’1
âˆ’2
0
0
0
0
0
0
0
0
2
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
The next matrix the steps in the algorithm are applied to is
ï£«
ï£­
âˆ’1
âˆ’2
0
0
2
1
ï£¶
ï£¸.
The ï¬rst pivot column is the ï¬rst column in this case and no switching of rows is necessary
because there is a nonzero entry in the ï¬rst pivot position. Therefore, the algorithm yields
for the next step
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
1
4
3
0
0
2
3
2
0
0
0
âˆ’1
âˆ’2
0
0
0
0
0
0
0
0
0
âˆ’3
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Now the algorithm will be applied to the matrix,
Âµ
0
âˆ’3
Â¶
There is only one column and it is nonzero so this single column is the pivot column.
Therefore, the algorithm yields the following matrix for the echelon form.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
1
4
3
0
0
2
3
2
0
0
0
âˆ’1
âˆ’2
0
0
0
0
âˆ’3
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
To complete placing the matrix in reduced echelon form, multiply the third row by 3 and
add âˆ’2 times the fourth row to it. This yields
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
1
4
3
0
0
2
3
2
0
0
0
âˆ’3
0
0
0
0
0
âˆ’3
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸

90
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
Next multiply the second row by 3 and take 2 times the fourth row and add to it. Then
add the fourth row to the ï¬rst.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
1
4
0
0
0
6
9
0
0
0
0
âˆ’3
0
0
0
0
0
âˆ’3
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Next work on the fourth column in the same way.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
3
3
0
0
0
0
6
0
0
0
0
0
âˆ’3
0
0
0
0
0
âˆ’3
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
Take âˆ’1/2 times the second row and add to the ï¬rst.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
3
0
0
0
0
0
6
0
0
0
0
0
âˆ’3
0
0
0
0
0
âˆ’3
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Finally, divide by the value of the leading entries in the nonzero rows.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
The above algorithm is the way a computer would obtain a reduced echelon form for a
given matrix. It is not necessary for you to pretend you are a computer but if you like to do
so, the algorithm described above will work. The main idea is to do row operations in such
a way as to end up with a matrix in echelon form or row reduced echelon form because when
this has been done, the resulting augmented matrix will allow you to describe the solutions
to the linear system of equations in a meaningful way.
Example 5.2.17 Give the complete solution to the system of equations, 5x+10yâˆ’7z = âˆ’2,
2x + 4y âˆ’3z = âˆ’1, and 3x + 6y + 5z = 9.
The augmented matrix for this system is
ï£«
ï£­
2
4
âˆ’3
|
âˆ’1
5
10
âˆ’7
|
âˆ’2
3
6
5
|
9
ï£¶
ï£¸
Multiply the second row by 2, the ï¬rst row by 5, and then take (âˆ’1) times the ï¬rst row and
add to the second. Then multiply the ï¬rst row by 1/5. This yields
ï£«
ï£­
2
4
âˆ’3
|
âˆ’1
0
0
1
|
1
3
6
5
|
9
ï£¶
ï£¸

5.2.
SYSTEMS OF EQUATIONS, ALGEBRAIC PROCEDURES
91
Now, combining some row operations, take (âˆ’3) times the ï¬rst row and add this to 2 times
the last row and replace the last row with this. This yields.
ï£«
ï£­
2
4
âˆ’3
|
âˆ’1
0
0
1
|
1
0
0
1
|
21
ï£¶
ï£¸.
One more row operation, taking (âˆ’1) times the second row and adding to the bottom yields.
ï£«
ï£­
2
4
âˆ’3
|
âˆ’1
0
0
1
|
1
0
0
0
|
20
ï£¶
ï£¸.
This is impossible because the last row indicates the need for a solution to the equation
0x + 0y + 0z = 20
and there is no such thing because 0 Ì¸= 20. This shows there is no solution to the three given
equations. When this happens, the system is called inconsistent. In this case it is very
easy to describe the solution set. The system has no solution.
Here is another example based on the use of row operations.
Example 5.2.18 Give the complete solution to the system of equations, 3x âˆ’y âˆ’5z = 9,
y âˆ’10z = 0, and âˆ’2x + y = âˆ’6.
The augmented matrix of this system is
ï£«
ï£­
3
âˆ’1
âˆ’5
|
9
0
1
âˆ’10
|
0
âˆ’2
1
0
|
âˆ’6
ï£¶
ï£¸
Replace the last row with 2 times the top row added to 3 times the bottom row. This gives
ï£«
ï£­
3
âˆ’1
âˆ’5
|
9
0
1
âˆ’10
|
0
0
1
âˆ’10
|
0
ï£¶
ï£¸.
The entry, 3 in this sequence of row operations is called the pivot. It is used to create
zeros in the other places of the column. Next take âˆ’1 times the middle row and add to the
bottom. Here the 1 in the second row is the pivot.
ï£«
ï£­
3
âˆ’1
âˆ’5
|
9
0
1
âˆ’10
|
0
0
0
0
|
0
ï£¶
ï£¸
Take the middle row and add to the top and then divide the top row which results by 3.
ï£«
ï£­
1
0
âˆ’5
|
3
0
1
âˆ’10
|
0
0
0
0
|
0
ï£¶
ï£¸.
This is in reduced echelon form. The equations corresponding to this reduced echelon form
are y = 10z and x = 3 + 5z. Apparently z can equal any number. Lets call this number,
t. 2Therefore, the solution set of this system is x = 3 + 5t, y = 10t, and z = t where t
2In this context t is called a parameter.

92
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
is completely arbitrary. The system has an inï¬nite set of solutions which are given in the
above simple way. This is what it is all about, ï¬nding the solutions to the system.
There is some terminology connected to this which is useful. Recall how each column
corresponds to a variable in the original system of equations. The variables corresponding to
a pivot column are called basic variables . The other variables are called free variables.
In Example 5.2.18 there was one free variable, z, and two basic variables, x and y. In de-
scribing the solution to the system of equations, the free variables are assigned a parameter.
In Example 5.2.18 this parameter was t. Sometimes there are many free variables and in
these cases, you need to use many parameters. Here is another example.
Example 5.2.19 Find the solution to the system
x + 2y âˆ’z + w = 3
x + y âˆ’z + w = 1
x + 3y âˆ’z + w = 5
The augmented matrix is
ï£«
ï£­
1
2
âˆ’1
1
|
3
1
1
âˆ’1
1
|
1
1
3
âˆ’1
1
|
5
ï£¶
ï£¸.
Take âˆ’1 times the ï¬rst row and add to the second. Then take âˆ’1 times the ï¬rst row and
add to the third. This yields
ï£«
ï£­
1
2
âˆ’1
1
|
3
0
âˆ’1
0
0
|
âˆ’2
0
1
0
0
|
2
ï£¶
ï£¸
Now add the second row to the bottom row
ï£«
ï£­
1
2
âˆ’1
1
|
3
0
âˆ’1
0
0
|
âˆ’2
0
0
0
0
|
0
ï£¶
ï£¸
(5.9)
This matrix is in echelon form and you see the basic variables are x and y while the free
variables are z and w. Assign s to z and t to w. Then the second row yields the equation,
y = 2 while the top equation yields the equation, x + 2y âˆ’s + t = 3 and so since y = 2, this
gives x + 4 âˆ’s + t = 3 showing that x = âˆ’1 + s âˆ’t, y = 2, z = s, and w = t. It is customary
to write this in the form
ï£«
ï£¬
ï£¬
ï£­
x
y
z
w
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
âˆ’1 + s âˆ’t
2
s
t
ï£¶
ï£·
ï£·
ï£¸.
(5.10)
This is another example of a system which has an inï¬nite solution set but this time
the solution set depends on two parameters, not one. Most people ï¬nd it less confusing
in the case of an inï¬nite solution set to ï¬rst place the augmented matrix in row reduced
echelon form rather than just echelon form before seeking to write down the description of
the solution. In the above, this means we donâ€™t stop with the echelon form 5.9. Instead we
ï¬rst place it in reduced echelon form as follows.
ï£«
ï£­
1
0
âˆ’1
1
|
âˆ’1
0
1
0
0
|
2
0
0
0
0
|
0
ï£¶
ï£¸.

5.2.
SYSTEMS OF EQUATIONS, ALGEBRAIC PROCEDURES
93
Then the solution is y = 2 from the second row and x = âˆ’1 + z âˆ’w from the ï¬rst. Thus
letting z = s and w = t, the solution is given in 5.10.
The number of free variables is always equal to the number of diï¬€erent parameters
used to describe the solution. If there are no free variables, then either there is no solution
as in the case where row operations yield an echelon form like
ï£«
ï£­
1
2
|
3
0
4
|
âˆ’2
0
0
|
1
ï£¶
ï£¸
or there is a unique solution as in the case where row operations yield an echelon form like
ï£«
ï£­
1
2
2
|
3
0
4
3
|
âˆ’2
0
0
4
|
1
ï£¶
ï£¸.
Also, sometimes there are free variables and no solution as in the following:
ï£«
ï£­
1
2
2
|
3
0
4
3
|
âˆ’2
0
0
0
|
1
ï£¶
ï£¸.
There are a lot of cases to consider but it is not necessary to make a major production of
this. Do row operations till you obtain a matrix in echelon form or reduced echelon form
and determine whether there is a solution. If there is, see if there are free variables. In this
case, there will be inï¬nitely many solutions. Find them by assigning diï¬€erent parameters
to the free variables and obtain the solution. If there are no free variables, then there will
be a unique solution which is easily determined once the augmented matrix is in echelon
or row reduced echelon form. In every case, the process yields a straightforward way to
describe the solutions to the linear system. As indicated above, you are probably less likely
to become confused if you place the augmented matrix in row reduced echelon form rather
than just echelon form.
In summary,
Deï¬nition 5.2.20 A system of linear equations is a list of equations,
a11x1 + a12x2 + Â· Â· Â· + a1nxn = b1
a21x1 + a22x2 + Â· Â· Â· + a2nxn = b2
...
am1x1 + am2x2 + Â· Â· Â· + amnxn = bm
where aij are numbers, and bj is a number. The above is a system of m equations in the
n variables, x1, x2 Â· Â·Â·, xn. Nothing is said about the relative size of m and n. Written more
simply in terms of summation notation, the above can be written in the form
n
X
j=1
aijxj = fj, i = 1, 2, 3, Â· Â· Â·, m
It is desired to ï¬nd (x1, Â· Â· Â·, xn) solving each of the equations listed.
As illustrated above, such a system of linear equations may have a unique solution, no
solution, or inï¬nitely many solutions and these are the only three cases which can occur for
any linear system. Furthermore, you do exactly the same things to solve any linear system.

94
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
You write the augmented matrix and do row operations until you get a simpler system in
which it is possible to see the solution, usually obtaining a matrix in echelon or reduced
echelon form. All is based on the observation that the row operations do not change the
solution set. You can have more equations than variables, fewer equations than variables,
etc. It doesnâ€™t matter. You always set up the augmented matrix and go to work on it.
Deï¬nition 5.2.21 A system of linear equations is called consistent if there exists
a solution. It is called inconsistent if there is no solution.
These are reasonable words to describe the situations of having or not having a solu-
tion. If you think of each equation as a condition which must be satisï¬ed by the variables,
consistent would mean there is some choice of variables which can satisfy all the condi-
tions. Inconsistent means there is no choice of the variables which can satisfy each of the
conditions.
5.3
The Rank Of A Matrix 14 Sept.
The notion of an augmented matrix was used to solve systems of equations. In general, a
matrix is simply a rectangular array of numbers.
Deï¬nition 5.3.1 A matrix, A is called an m Ã— n matrix if it has m rows and n
columns.
Example 5.3.2 The matrix,
ï£«
ï£­
1
2
3
4
5
6
ï£¶
ï£¸
is a 3 Ã— 2 matrix because it has two columns, (These stand upright.) and three rows.
Corresponding to such a rectangular array of numbers, there is a row reduced echelon
form discussed above. The following theorem is of fundamental signiï¬cance.
Theorem 5.3.3 Given an m Ã— n matrix, the row reduced echelon form is unique.
This is a remarkable theorem because there are many ways to do row operations and
eventually end up with something in row reduced echelon form. It is remarkable that you
always get the same thing. Now it is easy to describe the rank of a matrix.
Deï¬nition 5.3.4 The rank of a matrix, A equals the number of nonzero rows in its
row reduced echelon form. This is the same as the number of pivot columns.
Example 5.3.5 Find the rank of the matrix,
A =
ï£«
ï£­
1
2
3
1
0
2
1
1
1
4
4
2
ï£¶
ï£¸
To ï¬nd the rank, you obtain the row reduced echelon form and count the number of
nonzero rows or equivalently the number of pivot columns. First take âˆ’1 times the top row
and add to the bottom row. This yields
ï£«
ï£­
1
2
3
1
0
2
1
1
0
2
1
1
ï£¶
ï£¸

5.3.
THE RANK OF A MATRIX 14 SEPT.
95
Now add âˆ’1 times the second row to the bottom. This yields
ï£«
ï£­
1
2
3
1
0
2
1
1
0
0
0
0
ï£¶
ï£¸
Now take âˆ’1 times the second row and add to the top.
ï£«
ï£­
1
0
2
0
0
2
1
1
0
0
0
0
ï£¶
ï£¸
Finally, multiply the second row by 1/2 to get
ï£«
ï£­
1
0
2
0
0
1
1/2
1/2
0
0
0
0
ï£¶
ï£¸
which is in row reduced echelon form. The rank of this matrix is therefore 2.
Note that from the process used to obtain the row reduced echelon form, once you have
obtained an echelon form, you know the correct number of rows in the ï¬nal result. Thus you
can simply take the number of nonzero rows in an echelon form and this will be the rank.
Note also that the rank is the number of pivot columns. In this case the pivot columns are
the ï¬rst two.
Deï¬nition 5.3.6 A homogeneous system of linear equations is one with aug-
mented matrix of the form
Â¡ A
|
0 Â¢
where 0 is a column of zeros and A is an m Ã— n matrix.
Example 5.3.7 An example of a homogeneous system of equations is x+y = 0, 3xâˆ’y = 0.
It has augmented matrix,
Âµ
1
1
|
0
3
âˆ’1
|
0
Â¶
.
The nice thing about homogeneous systems is that they are always consistent. Simply
let all the variables equal zero and you obtain a solution. However, there may be other
solutions besides this one. This is related to the concept of rank and free variables.
Theorem 5.3.8 Let A be an m Ã— n matrix. Form the augmented matrix,
Â¡
A
|
0
Â¢
where 0 is the column of zeros.
Thus A is the coeï¬ƒcient matrix of a system of linear
equations with n variables. Then the number of free variables = n âˆ’rank (A) .
Proof: The basic variables correspond to the pivot columns of A and the free variables
correspond to the other columns.
However, the rank of A equals the number of pivot
columns.
As a corollary here is a theorem which is called the Rank theorem.
Corollary 5.3.9 (Rank Theorem) Let A be an m Ã— n matrix.
Form the augmented
matrix,
Â¡
A
|
b
Â¢
where b is an mÃ—1 column. Thus A is the coeï¬ƒcient matrix of a system of linear equations
with n variables. Then if the system of equations represented by the above augmented matrix
is consistent, number of free variables = n âˆ’rank (A) .

96
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
Proof: Since the equations represented by the above augmented matrix are consistent,
the same argument as in Theorem 5.3.8 holds. The leading entry in the last nonzero row
cannot be in the last column because if it were, then the system would fail to be consistent.
5.4
Theory Of Row Reduced Echelon Formâˆ—
This material will be done much more easily later after the introduction of elementary
matrices. You can wait to read it till then. However, if you wish to understand what is
going on right now, I am giving an explanation. First recall the row operations.
Deï¬nition 5.4.1 The row operations consist of the following
1. Switch two rows.
2. Multiply a row by a nonzero number.
3. Replace a row by a multiple of another row added to itself.
In rough terms, the following lemma states that linear relationships between columns in
a matrix are preserved by row operations.
Deï¬nition 5.4.2 The vector, u is a linear combination of the vectors, v1, Â·Â·Â·, vm
if there exist scalars, c1, Â· Â· Â·, cm such that
u
=
c1v1 + c2v2 + Â· Â· Â· + cmvm
=
m
X
k=1
ckvk.
Example 5.4.3
3
ï£«
ï£­
2
3
1
ï£¶
ï£¸+ 5
ï£«
ï£­
1
âˆ’2
4
ï£¶
ï£¸+ (âˆ’2)
ï£«
ï£­
5
3
7
ï£¶
ï£¸=
ï£«
ï£­
1
âˆ’7
9
ï£¶
ï£¸
Thus
ï£«
ï£­
1
âˆ’7
9
ï£¶
ï£¸is a linear combination of the vectors,
ï£«
ï£­
2
3
1
ï£¶
ï£¸,
ï£«
ï£­
1
âˆ’2
4
ï£¶
ï£¸, and
ï£«
ï£­
5
3
7
ï£¶
ï£¸. In
this case the scalars are 3, 5, and âˆ’2.
Deï¬nition 5.4.4 When dealing with an mÃ—n matrix, A, the element in the ith row
and the jth column is denoted as Aij. Thus the jth column is
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
A1j
A2j
A3j
...
Amj
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
Lemma 5.4.5 Let B and A be two m Ã— n matrices and suppose B results from a row
operation applied to A. Then the kth column of B is a linear combination of the i1, Â· Â· Â·, ir
columns of B if and only if the kth column of A is a linear combination of the i1, Â· Â· Â·, ir
columns of A. Furthermore, the scalars in the linear combination are the same. (The linear
relationship between the kth column of A and the i1, Â· Â· Â·, ir columns of A is the same as the
linear relationship between the kth column of B and the i1, Â· Â· Â·, ir columns of B.)

5.4.
THEORY OF ROW REDUCED ECHELON FORMâˆ—
97
Proof: This is obvious in the case of the ï¬rst two row operations and a little less obvious
in the case of the third. Therefore, consider the third. Suppose the sth row of B equals the
sth row of A added to c times the qth row of A. Therefore,
Bij = Aij if i Ì¸= s, Bsj = Asj + cAqj.
The assumption about the kth column of B is equivalent to saying that for each p,
Bpk =
r
X
j=1
Î±jBpij.
(5.11)
For p Ì¸= s, this is equivalent to saying
Apk =
r
X
j=1
Î±jApij
(5.12)
because for these values of p, Bpj = Apj. For p = s, this is equivalent to saying
Ask + cAqk =
r
X
j=1
Î±j
Â¡
Asij + cAqij
Â¢
.
(5.13)
but from 5.12, applied to p = q,
cAqk = c
r
X
j=1
Î±jAqij
and so from 5.13, it follows 5.11 is equivalent to 5.12 for all p, including p = s. This proves
the lemma.
Now I will present a review of the row reduced echelon form. It is convenient to describe
it slightly diï¬€erently to use Lemma 5.4.5.
Deï¬nition 5.4.6 Let ei denote the column vector which has all zero entries except
for the ith slot which is one. An m Ã— n matrix is said to be in row reduced echelon form
if, in viewing succesive columns from left to right, the ï¬rst nonzero column encountered is
e1 and if you have encountered e1, e2, Â· Â· Â·, ek, the next column is either ek+1 or is a linear
combination of the vectors, e1, e2, Â· Â· Â·, ek.
Theorem 5.4.7 Let A be an m Ã— n matrix. Then A has a row reduced echelon
form determined by a simple process.
Proof: Viewing the columns of A from left to right take the ï¬rst nonzero column. Pick
a nonzero entry in this column and switch the row containing this entry with the top row of
A. Now divide this new top row by the value of this nonzero entry to get a 1 in this position
and then use row operations to make all entries below this element equal to zero. Thus the
ï¬rst nonzero column is now e1. Denote the resulting matrix by A1. Consider the submatrix
of A1 to the right of this column and below the ï¬rst row. Do exactly the same thing for it
that was done for A. This time the e1 will refer to Rmâˆ’1. Use this 1 and row operations to
zero out every element above it in the rows of A1. Call the resulting matrix, A2. Thus A2
satisï¬es the conditions of the above deï¬nition up to the column just encountered. Continue
this way till every column has been dealt with and the result must be in row reduced echelon
form.

98
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
The following diagram illustrates the above procedure. Say the matrix looked something
like the following.
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
...
...
...
...
...
...
...
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸
First step would yield something like
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
1
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
...
...
...
...
...
...
...
0
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸
For the second step you look at the lower right corner as described,
ï£«
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
...
...
...
...
...
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£¸
and if the ï¬rst column consists of all zeros but the next one is not all zeros, you would get
something like this.
ï£«
ï£¬
ï£­
0
1
âˆ—
âˆ—
âˆ—
...
...
...
...
...
0
0
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£¸
Thus, after zeroing out the term in the top row above the 1, you get the following for the
next step in the computation of the row reduced echelon form for the original matrix.
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
1
âˆ—
0
âˆ—
âˆ—
âˆ—
0
0
0
1
âˆ—
âˆ—
âˆ—
...
...
...
...
...
...
...
0
0
0
0
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸.
Next you look at the lower right matrix below the top two rows and to the right of the ï¬rst
four columns and repeat the process.
Deï¬nition 5.4.8 The ï¬rst pivot column of A is the ï¬rst nonzero column of A. The
next pivot column is the ï¬rst column after this which becomes e2 in the row reduced echelon
form. The third is the next column which becomes e3 in the row reduced echelon form and
so forth.
There are three choices for row operations at each step in the above theorem. A natural
question is whether the same row reduced echelon matrix always results in the end from
following the above algorithm applied in any way. The next corollary says this is the case.
Deï¬nition 5.4.9 Two matrices are said to be row equivalent if one can be ob-
tained from the other by a sequence of row operations.
It has been shown above that every matrix is row equivalent to one which is in row
reduced echelon form.

5.4.
THEORY OF ROW REDUCED ECHELON FORMâˆ—
99
Corollary 5.4.10 The row reduced echelon form is unique.
That is if B, C are two
matrices in row reduced echelon form and both are row equivalent to A, then B = C.
Proof: Suppose B and C are both row reduced echelon forms for the matrix, A. Then
they clearly have the same zero columns since row operations leave zero columns unchanged.
If B has the sequence e1, e2, Â· Â· Â·, er occuring for the ï¬rst time in the positions, i1, i2, Â· Â· Â·, ir
the description of the row reduced echelon form means that if bk is the kth column of B such
that ijâˆ’1 < k < ij then bk is a linear combination of the columns in positions i1, i2, Â· Â· Â·, ir.
By Lemma 5.4.5 the same is true for ck, the kth column of C. Therefore, ck is not equal
to ej for any j because ej is not obtained as a linear combinations of the ei for i < j. It
follows the ej for C can only occur in positions i1, i2, Â· Â· Â·, ir. Furthermore, position ij in C
must contain ej because if not, then cij would be a linear combination of e1, Â· Â· Â·, ejâˆ’1 in C
but not in B, thus contradicting Lemma 5.4.5. Therefore, both B and C have the sequence
e1, e2, Â· Â· Â·, er occuring for the ï¬rst time in the positions, i1, i2, Â· Â· Â·, ir. By Lemma 5.4.5, the
columns between the ik and ik+1 position are linear combinations involving the same scalars
of the columns in the i1, Â· Â· Â·, ik position. This is equivalent to the assertion that each of
these columns is identical and this proves the corollary.
This suggests that to ï¬nd the rank of a matrix, one should do row operations until a
matrix is obtained in which its rank is obvious.
5.4.1
Exercises With Answers
1. Find the distance from the point, (1, 2, 1) to the plane 3x + y âˆ’z = 7.
You can use the stupid formula for this.
|3 + 2 âˆ’1 âˆ’7|
âˆš9 + 1 + 1
= 3
11
âˆš
11
2. Find the cosine of the angle between the planes x âˆ’y + z = 7 and 2x + y âˆ’3z = 4.
You just need to consider the normal vectors which are (1, âˆ’1, 1) and (2, 1, âˆ’3) . Then
the cosine of the angle desired is
cos Î¸ =
Â¯Â¯Â¯Â¯
(2, 1, âˆ’3) Â· (1, âˆ’1, 1)
âˆš1 + 1 + 1âˆš4 + 1 + 9
Â¯Â¯Â¯Â¯ = 1
21
âˆš
3
âˆš
14
3. Here are vector equations for two lines. (x, y, z) = (1, 2, 0) + t (2, 1, 1) and (x, y, z) =
(3, 0, 1) + t (1, âˆ’2, 1) . The angle between the direction vectors is not 0 or Ï€ and so
the lines are not parallel. If they were two lines in R2, this means they would need to
intersect. However, these two lines do not intersect. If they did, there would exist s, t
such that
(1, 2, 0) + t (2, 1, 1) = (3, 0, 1) + s (1, âˆ’2, 1)
and this would require the following system of equations would need to hold.
1 + 2t = 3 + s
2 + t = âˆ’2s
t = 1 + s
The augmented matrix for this system is
ï£«
ï£­
2
âˆ’1
|
2
1
2
|
âˆ’2
1
âˆ’1
|
1
ï£¶
ï£¸

100
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
The row reduced echelon form is
ï£«
ï£­
1
0
|
0
0
1
|
0
0
0
|
1
ï£¶
ï£¸
and so there is no solution. These lines are called skew lines. Imagine two airplanes,
one going from South to North and the other going from East to West. The ï¬rst
travels at 40000 feet and the second at 35000 feet. Their paths never cross. Of course
the extra dimension is not present in two dimensions and so their paths would cross
if they were moving in a plane. Note also that to consider the question whether the
lines intersect, you must look at possibly diï¬€erent values for the parameters.
4. Let two skew lines be given in Problem 3. Find two parallel planes which contain the
two lines.
This is easy if you can ï¬nd the normal vector of the two planes. To say the planes
are parallel requires them to have the same normal vector.
The two lines were
(x, y, z) = (1, 2, 0) + t (2, 1, 1) and (x, y, z) = (3, 0, 1) + t (1, âˆ’2, 1) . Therefore, the
normal vector needs to be perpendicular to both direction vectors. You need n =
(2, 1, 1) Ã— (1, âˆ’2, 1) = (3, âˆ’1, âˆ’5) . Now the equation of the ï¬rst plane is
(3, âˆ’1, âˆ’5) Â· (x âˆ’1, y âˆ’2, z) = 0
and the equation of the second plane is
(3, âˆ’1, âˆ’5) Â· (x âˆ’3, y, z âˆ’1) = 0
The two planes are therefore, 3x âˆ’y âˆ’5z = 1 and 3x âˆ’y âˆ’5z = 4. You see these are
parallel planes because they have the same normal vector and the ï¬rst contains the
ï¬rst line while the second contains the second line.
5. Here is an augmented matrix in which âˆ—denotes an arbitrary number and â– denotes
a nonzero number. Determine whether the given augmented matrix is consistent. If
consistent, is the solution unique?
ï£«
ï£¬
ï£¬
ï£­
â– 
âˆ—
âˆ—
âˆ—
âˆ—
|
âˆ—
0
â– 
âˆ—
âˆ—
0
|
âˆ—
0
0
â– 
âˆ—
âˆ—
|
â– 
0
0
0
0
â– 
|
âˆ—
ï£¶
ï£·
ï£·
ï£¸
In this case the system is consistent and there is an inï¬nite set of solutions. To see
it is consistent, the bottom equation would yield a unique solution for x5.
Then
letting x4 = t, and substituting in to the other equations, beginning with the equation
determined by the third row and then proceding up to the next row followed by the
ï¬rst row, you get a solution for each value of t. There is a free variable which comes
from the fourth column which is why you can say x4 = t. Therefore, the solution is
inï¬nite.
6. Here is an augmented matrix in which âˆ—denotes an arbitrary number and â– denotes
a nonzero number. Determine whether the given augmented matrix is consistent. If
consistent, is the solution unique?
ï£«
ï£­
â– 
âˆ—
âˆ—
|
âˆ—
0
0
â– 
|
â– 
0
0
âˆ—
|
0
ï£¶
ï£¸

5.4.
THEORY OF ROW REDUCED ECHELON FORMâˆ—
101
In this case there is no solution because you could use a row operation to place a 0 in
the third row and third column position, like this:
ï£«
ï£­
â– 
âˆ—
âˆ—
|
âˆ—
0
0
â– 
|
â– 
0
0
0
|
â– 
ï£¶
ï£¸
This would give a row of zeros equal to something nonzero.
7. Find h such that
Âµ
1
h
|
4
3
7
|
7
Â¶
is the augmented matrix of an inconsistent matrix.
Doing a row operation by taking âˆ’3 times the top row and adding to the bottom, this
gives
Âµ
1
h
|
4
0
7 âˆ’3h
|
7 âˆ’12
Â¶
.
The system will be inconsistent if 7 âˆ’3h = 0 or in other words, h = 7/3.
8. Determine if the system is consistent.
x + 2y + 3z âˆ’w = 2
x âˆ’y + 2z + w = 1
2x + 3y âˆ’z = 1
4x + 2y + z = 5
The augmented matrix is
ï£«
ï£¬
ï£¬
ï£­
1
2
3
âˆ’1
|
2
1
âˆ’1
2
1
|
1
2
3
âˆ’1
0
|
1
4
2
1
0
|
5
ï£¶
ï£·
ï£·
ï£¸
A reduced echelon form for this is
ï£«
ï£¬
ï£¬
ï£­
9
0
0
0
|
14
0
9
0
0
|
âˆ’6
0
0
9
0
|
1
0
0
0
9
|
âˆ’13
ï£¶
ï£·
ï£·
ï£¸.
Therefore, there is a unique solution. In particular the system is consistent.
9. Find the point, (x1, y1) which lies on both lines, 5x + 3y = 1 and 4x âˆ’y = 3.
You solve the system of equations whose augmented matrix is
Âµ
5
3
|
1
4
âˆ’1
|
3
Â¶
A reduced echelon form is
Âµ
17
0
10
0
17
âˆ’11
Â¶
and so the solution is x = 17/10 and y = âˆ’11/17.

102
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
10. Do the three lines, 3x + 2y = 1, 2x âˆ’y = 1, and 4x + 3y = 3 have a common point of
intersection? If so, ï¬nd the point and if not, tell why they donâ€™t have such a common
point of intersection.
This is asking for the solution to the three equations shown. The augmented matrix
is
ï£«
ï£­
3
2
|
1
2
âˆ’1
|
1
4
3
|
3
ï£¶
ï£¸
A reduced echelon form is
ï£«
ï£­
1
0
|
0
0
1
|
0
0
0
|
1
ï£¶
ï£¸
and this would require 0x + 0y = 1 which is impossible so there is no solution to this
system of equations and hence no point on each of the three lines.
11. Find the general solution of the system whose augmented matrix is
ï£«
ï£­
1
2
0
|
2
1
1
4
|
2
2
3
4
|
4
ï£¶
ï£¸.
A reduced echelon form for the matrix is
ï£«
ï£­
1
0
8
2
0
1
âˆ’4
0
0
0
0
0
ï£¶
ï£¸.
Therefore, y = 4z and x = 2 âˆ’8z. Apparently z can equal anything so we let z = t
and then the solution is
x = 2 âˆ’8t, y = 4t, z = t.
12. Find the point, (x1, y1) which lies on both lines, x + 2y = 1 and 3x âˆ’y = 3.
The solution is y = 0 and x = 1.
13. Find the point of intersection of the two lines x + y = 3 and x + 2y = 1.
The solution is (5, âˆ’2) .
14. Do the three lines, x + 2y = 1, 2x âˆ’y = 1, and 4x + 3y = 3 have a common point of
intersection? If so, ï¬nd the point and if not, tell why they donâ€™t have such a common
point of intersection.
To solve this set up the augmented matrix and go to work on it. The augmented
matrix is
ï£«
ï£­
1
2
|
1
2
âˆ’1
|
1
4
3
|
3
ï£¶
ï£¸
A reduced echelon matrix for this is
ï£«
ï£­
1
0
|
3
5
0
1
|
1
5
0
0
|
0
ï£¶
ï£¸
Therefore, there is a point in the intersection of these and it is y = 1/5 and x = 3/5.
Thus the point is (3/5, 1/5) .

5.4.
THEORY OF ROW REDUCED ECHELON FORMâˆ—
103
15. Do the three planes, x + 2y âˆ’3z = 2, x + y + z = 1, and 3x + 2y + 2z = 0 have
a common point of intersection? If so, ï¬nd one and if not, tell why there is no such
point.
You need to ï¬nd (x, y, z) which solves each equation. The augmented matrix is
ï£«
ï£­
1
2
âˆ’3
|
2
1
1
1
|
1
3
2
2
|
0
ï£¶
ï£¸
A reduced echelon form for the matrix is
ï£«
ï£­
1
0
0
|
âˆ’2
0
1
0
|
13
5
0
0
1
|
2
5
ï£¶
ï£¸
and so you should let (x, y, z) = (âˆ’2, 13/5, 2/5) .
16. Here is an augmented matrix in which âˆ—denotes an arbitrary number and â– denotes
a nonzero number. Determine whether the given augmented matrix is consistent. If
consistent, is the solution unique?
ï£«
ï£¬
ï£¬
ï£­
â– 
âˆ—
âˆ—
âˆ—
âˆ—
|
âˆ—
0
â– 
âˆ—
âˆ—
0
|
âˆ—
0
0
â– 
âˆ—
âˆ—
|
âˆ—
0
0
0
0
â– 
|
âˆ—
ï£¶
ï£·
ï£·
ï£¸
You could do another set of row operations and reduce the matrix to one of the form
ï£«
ï£¬
ï£¬
ï£­
â– 
âˆ—
âˆ—
âˆ—
0
|
âˆ—
0
â– 
âˆ—
âˆ—
0
|
âˆ—
0
0
â– 
âˆ—
0
|
âˆ—
0
0
0
0
â– 
|
âˆ—
ï£¶
ï£·
ï£·
ï£¸
It follows there exists a solution but the solution is not unique because x4 is a free
variable. You can pick it to be anything you like and the system will yield values for
the other variables.
17. Here is an augmented matrix in which âˆ—denotes an arbitrary number and â– denotes
a nonzero number. Determine whether the given augmented matrix is consistent. If
consistent, is the solution unique?
ï£«
ï£­
â– 
âˆ—
âˆ—
|
âˆ—
0
â– 
âˆ—
|
âˆ—
0
0
â– 
|
âˆ—
ï£¶
ï£¸
In this case there is a unique solution to the system. To see this, you could do more
row operations and reduce this to something of the form
ï£«
ï£­
â– 
0
0
|
âˆ—
0
â– 
0
|
âˆ—
0
0
â– 
|
âˆ—
ï£¶
ï£¸.

104
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.
18. Here is an augmented matrix in which âˆ—denotes an arbitrary number and â– denotes
a nonzero number. Determine whether the given augmented matrix is consistent. If
consistent, is the solution unique?
ï£«
ï£¬
ï£¬
ï£­
â– 
âˆ—
âˆ—
âˆ—
âˆ—
|
âˆ—
0
â– 
0
âˆ—
0
|
âˆ—
0
0
0
â– 
âˆ—
|
âˆ—
0
0
0
0
â– 
|
âˆ—
ï£¶
ï£·
ï£·
ï£¸
In this case, you could do more row operations and get something of the form
ï£«
ï£¬
ï£¬
ï£­
â– 
0
âˆ—
0
0
|
âˆ—
0
â– 
0
0
0
|
âˆ—
0
0
0
â– 
0
|
âˆ—
0
0
0
0
â– 
|
âˆ—
ï£¶
ï£·
ï£·
ï£¸
Now you can determine the answer.
19. Find h such that
Âµ
2
h
|
4
3
6
|
7
Â¶
is the augmented matrix of an inconsistent matrix.
Take âˆ’3 times the top row and add to 2 times the bottom. This yields
Âµ
2
h
|
4
0
12 âˆ’3h
|
2
Â¶
Now if h = 4 the system is inconsistent because it would have the bottom row equal
to
Â¡ 0
0
|
2
Â¢
.
20. Choose h and k such that the augmented matrix shown has one solution. Then choose
h and k such that the system has no solutions. Finally, choose h and k such that the
system has inï¬nitely many solutions.
Âµ
1
h
|
2
2
4
|
k
Â¶
.
If h Ì¸= 2 then k can be anything and the system represented by the augmented matrix
will have a unique solution. Suppose then that h = 2. Then taking âˆ’2 times the top
row and adding to the bottom row gives
Âµ
1
2
|
2
0
0
|
k âˆ’4
Â¶
If k Ì¸= 4 there is no solution. However, if k = 4 you are left with the single equation,
x+2y = 2 and there are inï¬nitely many solutions to this. In fact anything of the form
(2 âˆ’2y, y) will work just ï¬ne.
21. Determine if the system is consistent.
x + 2y + z âˆ’w = 2
x âˆ’y + z + w = 1
2x + y âˆ’z = 1
4x + 2y + z = 5

5.4.
THEORY OF ROW REDUCED ECHELON FORMâˆ—
105
This system is inconsistent.
To see this, write the augmented matrix and do row
operations. The augmented matrix is
ï£«
ï£¬
ï£¬
ï£­
1
2
1
âˆ’1
|
2
1
âˆ’1
1
1
|
1
2
1
âˆ’1
0
|
1
4
2
1
0
|
5
ï£¶
ï£·
ï£·
ï£¸
A reduced echelon form for this matrix is
ï£«
ï£¬
ï£¬
ï£­
1
0
0
1
3
|
0
0
1
0
âˆ’2
3
|
0
0
0
1
0
|
0
0
0
0
0
|
1
ï£¶
ï£·
ï£·
ï£¸
and the bottom row shows there is no solution.
22. Find the general solution of the system whose augmented matrix is
ï£«
ï£­
1
2
0
|
2
1
3
4
|
2
1
0
2
|
1
ï£¶
ï£¸
A reduced echelon form for this matrix is
ï£«
ï£­
1
0
0
|
6
5
0
1
0
|
2
5
0
0
1
|
âˆ’1
10
ï£¶
ï£¸
and so the solution is unique and is z = âˆ’1/10, y = 2/5, and x = 6/5.
23. Find the general solution of the system whose augmented matrix is
Âµ
1
1
0
|
5
1
0
3
|
2
Â¶
.
A reduced echelon form for this matrix is
Âµ
1
0
3
|
2
0
1
âˆ’3
|
3
Â¶
and so the general solution is of the form y = 3 + 3z, x = 2 âˆ’3z with z arbitrary.
24. Find the general solution of the system whose augmented matrix is
ï£«
ï£¬
ï£¬
ï£­
1
0
2
1
1
|
3
0
1
0
4
2
|
1
2
2
0
0
1
|
3
1
0
1
0
2
|
2
ï£¶
ï£·
ï£·
ï£¸.
You do the usual thing, row operations on the matrix to obtain a reduced echelon
form. A reduced echelon form is
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
9
2
|
7
6
0
1
0
0
âˆ’4
|
1
3
0
0
1
0
âˆ’5
2
|
5
6
0
0
0
1
3
2
|
1
6
ï£¶
ï£·
ï£·
ï£¸
Therefore, the general solution is x4 = 1/6 âˆ’3/2x5, x3 = 5/6 + 5/2x5, x2 = 1/3 + 4x5,
and x1 = 7/6 âˆ’9/2x5 with x5 arbitrary.

106
SYSTEMS OF LINEAR EQUATIONS 12,13 SEPT.

Part III
Linear Independence And
Matrices
107


109
Outcomes
Spanning sets and Linear Independence
A. Explain what is meant by the span of a set of vectors both geometrically and alge-
braically.
B. Determine the span of a set of vectors. Determine if a given vector is in the span of a
set of vectors.
C. Deï¬ne linear independence.
D. Determine whether a set of vectors is linearly dependent or linearly independent. For
sets that are linearly dependent, determine a dependence relation.
E. Prove theorems about span and linear independence.
Reading: Linear Algebra 2.3
Supplemental Problems:
A1. Study the deï¬nition of linear independence. Write it from memory.
Outcome Mapping:
A. 13-16,17
B. 1-6,7-12
C. A1
D. 22-31
E. 18-21,42-48

110

Spanning Sets And Linear
Independence 18,19 Sept.
Quiz
1. Find a parametric equation for the line determined by the two points, (1, 2, 1) and
(2, âˆ’1, 3) .
2. Find an equation of the plane containing the point (0, 1, 0) and the line (1, 1, 1) +
t (2, âˆ’1, 1) .
3. An equation contains the point (0, 0, 0) and is perpendicular to the vector (1, 1, 1) .
Find an equation of this plane.
4. Here are some equations. Find the complete solution.
x + y + 4z = 1
âˆ’2x + y âˆ’2z = âˆ’2
x + 2z = 1
6.0.2
Spanning Sets
Deï¬nition 6.0.11 The vector, u is a linear combination of the vectors, v1, Â· Â·
Â·, vm if there exist scalars, c1, Â· Â· Â·, cm such that
u
=
c1v1 + c2v2 + Â· Â· Â· + cmvm
=
m
X
k=1
ckvk.
When u is a linear combination of {v1, Â· Â· Â·, vm} , we say u is in the span of v1, Â· Â· Â·, vm
written
u âˆˆspan (v1, Â· Â· Â·, vm) .
Equivalently, span (v1, Â· Â· Â·, vm) equals the set of all linear combinations of the vectors v1, Â· Â·
Â·, vm. If V = span (v1, Â· Â· Â·, vm) , then {v1, Â· Â· Â·, vm} is called a spanning set for V.
You can consider the geometric signiï¬cance of the span of a few vectors in three or two
dimensional space.
Example 6.0.12 Consider the span of one vector in R3.
111

112
SPANNING SETS AND LINEAR INDEPENDENCE 18,19 SEPT.
Â¡
Â¡
Â¡
Â¡
Â©Â©Â©
*v
Â©Â©Â©Â©Â©Â©Â©Â©Â©Â©
x
y
z
You see there is a vector, v and the span of this single vector, {tv such that t âˆˆR} gives
the indicated line which goes through the origin, (0, 0, 0) having v as a direction vector.
Example 6.0.13 You can get an idea of the appearance of the span of two vectors in R3.
These are just planes which pass through the origin. Here is a picture.
y
z
x
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
HHHHHHHHHHÂ¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
H
H
H
H
H
H
H
HHH
Â¡

1u
v
Â¤
Â¤
Â¤
Â¤
Lets consider why the displayed plane really is the span of the two vectors which lie in
this plane as shown.

113
y
z
x
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
HHHHHHHHHHHHHHHHHHHHÂ¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢Â¢
Â¢
Â¢
Â¢
Â¢
H
H
H
H
H
H
H
H
H
H
H
H
H
H
HHHHHH
Â¡
Â¡

1







)
su
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
Â¤
tv + 0u
tv + s1u
tv + s2u
As indicated in the above picture, a typical thing in the span of these two vectors is of the
form su + tv where s and t are real numbers. By specifying s, you determine a point on the
line through the origin, (0, 0, 0) having direction vector, u. Then through this point, there
is a line having direction vector, v. We have drawn three such lines in the above picture,
one for s = 0, s1, and s2. The totality of all such lines yields the span of the two vectors, u
and v and you see from geometric considerations it is just a plane.
Geometric considerations such as these donâ€™t take you anywhere because as soon as you
encounter more than three dimensions, you canâ€™t draw a meaningful picture. The notions of
span and spanning set and linear combination are best understood according to the above
deï¬nition and are algebraic in nature. Here is an example.
Example 6.0.14
3
ï£«
ï£­
2
3
1
ï£¶
ï£¸+ 5
ï£«
ï£­
1
âˆ’8
2
ï£¶
ï£¸+ (âˆ’2)
ï£«
ï£­
1
3
7
ï£¶
ï£¸=
ï£«
ï£­
9
âˆ’37
âˆ’1
ï£¶
ï£¸
Thus
ï£«
ï£­
9
âˆ’37
âˆ’1
ï£¶
ï£¸is a linear combination of the vectors,
ï£«
ï£­
2
3
1
ï£¶
ï£¸,
ï£«
ï£­
1
âˆ’8
2
ï£¶
ï£¸, and
ï£«
ï£­
1
3
7
ï£¶
ï£¸. In
this case the scalars are 3, 5, and âˆ’2.
The following theorem is nothing but a restatement of the deï¬nition of what it means
for a vector to be in the span of some other vectors.
Theorem 6.0.15 Let A be an m Ã— n matrix and let b be an m Ã— 1 vector. Then if
the columns of A are a1, Â· Â· Â·, an, b âˆˆspan (a1, Â· Â· Â·, an) if and only if the system of equations

114
SPANNING SETS AND LINEAR INDEPENDENCE 18,19 SEPT.
represented by the augmented matrix,
Â¡
A
|
b
Â¢
(6.1)
is consistent.
Proof: Suppose ï¬rst the system of equations just described is consistent. Let
ak =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
a1k
a2k
...
amk
ï£¶
ï£·
ï£·
ï£·
ï£¸, b =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
b1
b2
...
bm
ï£¶
ï£·
ï£·
ï£·
ï£¸
To say the system is consistent is to say there exist x1, Â· Â· Â·, xn solving the following system
of equations.
a11x1 + a12x2 + Â· Â· Â· + a1nxn = b1
a21x1 + a22x2 + Â· Â· Â· + a2nxn = b2
...
am1x1 + am2x2 + Â· Â· Â· + amnxn = bm
(6.2)
But from the way we add vectors, this can be written as
x1
ï£«
ï£¬
ï£¬
ï£¬
ï£­
a11
a21
...
am1
ï£¶
ï£·
ï£·
ï£·
ï£¸+ x2
ï£«
ï£¬
ï£¬
ï£¬
ï£­
a12
a22
...
am2
ï£¶
ï£·
ï£·
ï£·
ï£¸+ Â· Â· Â· + xn
ï£«
ï£¬
ï£¬
ï£¬
ï£­
a1n
a2n
...
amn
ï£¶
ï£·
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
b1
b2
...
bm
ï£¶
ï£·
ï£·
ï£·
ï£¸
(6.3)
which says the same thing as b âˆˆspan (a1, Â· Â· Â·, an).
Next suppose b âˆˆspan (a1, Â· Â· Â·, an) . This says there exist scalars, x1, Â· Â· Â·, xn such that
6.3 holds. This says the same thing as 6.2 and so the system of equations represented by
6.1 is consistent. This proves the theorem.
Example 6.0.16 Show that a spanning set for R3 is {e1, e2, e3} where
e1 =
ï£«
ï£­
1
0
0
ï£¶
ï£¸, e2 =
ï£«
ï£­
0
1
0
ï£¶
ï£¸, e3 =
ï£«
ï£­
0
0
1
ï£¶
ï£¸.
This is really easy. If
ï£«
ï£­
x
y
z
ï£¶
ï£¸âˆˆR3, you can write it as a linear combination of the above
three vectors as follows.
ï£«
ï£­
x
y
z
ï£¶
ï£¸= x
ï£«
ï£­
1
0
0
ï£¶
ï£¸+ y
ï£«
ï£­
0
1
0
ï£¶
ï£¸+ z
ï£«
ï£­
0
0
1
ï£¶
ï£¸.
Of course it isnâ€™t always so easy.
Example 6.0.17 Is
ï£«
ï£­
1
1
0
ï£¶
ï£¸,
ï£«
ï£­
1
0
1
ï£¶
ï£¸,
ï£«
ï£­
0
1
0
ï£¶
ï£¸
a spanning set for R3?

115
From Theorem 6.0.15 it is required to show the system of equations represented by the
augmented matrix,
ï£«
ï£­
1
1
0
|
a
1
0
1
|
b
0
1
0
|
c
ï£¶
ï£¸
has a solution for any choice of a, b, c. Take (âˆ’1) times the top row and add to the middle
row.
ï£«
ï£­
1
1
0
|
a
0
âˆ’1
1
|
b âˆ’a
0
1
0
|
c
ï£¶
ï£¸
Now take the second row and add to the bottom.
ï£«
ï£­
1
1
0
|
a
0
âˆ’1
1
|
b âˆ’a
0
0
1
|
c
ï£¶
ï£¸
You can see at this point that there will be a solution which you can obtain by back
substitution. Therefore, the vectors are a spanning set for R3.
Example 6.0.18 Is
ï£«
ï£­
1
1
0
ï£¶
ï£¸,
ï£«
ï£­
1
0
1
ï£¶
ï£¸,
ï£«
ï£­
2
1
1
ï£¶
ï£¸
a spanning set for R3?
By Theorem 6.0.15 you must consider the system of equations represented by
ï£«
ï£­
1
1
2
|
a
1
0
1
|
b
0
1
1
|
c
ï£¶
ï£¸
and see if there is a solution for any choice of a, b, c. Take (âˆ’1) times the top row and add
to the second.
ï£«
ï£­
1
1
2
|
a
0
âˆ’1
âˆ’1
|
b âˆ’a
0
1
1
|
c
ï£¶
ï£¸
Now add the second row to the bottom.
ï£«
ï£­
1
1
2
|
a
0
âˆ’1
âˆ’1
|
b âˆ’a
0
0
0
|
c + b âˆ’a
ï£¶
ï£¸
(6.4)
It follows that to obtain a solution to this system you must have c + b âˆ’a = 0. Therefore,
the vector,
ï£«
ï£­
0
1
0
ï£¶
ï£¸fails to be in
span
ï£«
ï£­
ï£«
ï£­
1
1
0
ï£¶
ï£¸,
ï£«
ï£­
1
0
1
ï£¶
ï£¸,
ï£«
ï£­
2
1
1
ï£¶
ï£¸
ï£¶
ï£¸
along with many others.

116
SPANNING SETS AND LINEAR INDEPENDENCE 18,19 SEPT.
Example 6.0.19 In the above example what is the span of the three given vectors?
Let a = b + c so 6.4 reduces to
ï£«
ï£­
1
1
2
|
a
0
âˆ’1
âˆ’1
|
b âˆ’a
0
0
0
|
0
ï£¶
ï£¸
and now you can add the middle row to the top row to obtain
ï£«
ï£­
1
0
1
|
b
0
âˆ’1
âˆ’1
|
b âˆ’a
0
0
0
|
0
ï£¶
ï£¸
Multiply the second row by (âˆ’1) to get the result in row reduced echelon form
ï£«
ï£­
1
0
1
|
b
0
1
1
|
a âˆ’b
0
0
0
|
0
ï£¶
ï£¸
Now there exists a solution to this. So what is the span of these vectors? It is
ï£«
ï£­
b + c
b
c
ï£¶
ï£¸: b, c âˆˆR.
That is, you can take either b or c to be anything you want and put it in the above formula
for a vector and it will be in the span of the three vectors. Thus
ï£«
ï£­
2
1
1
ï£¶
ï£¸,
ï£«
ï£­
0
1
âˆ’1
ï£¶
ï£¸,
ï£«
ï£­
3
2
1
ï£¶
ï£¸
are examples of vectors in the span of
ï£±
ï£²
ï£³
ï£«
ï£­
1
1
0
ï£¶
ï£¸,
ï£«
ï£­
1
0
1
ï£¶
ï£¸,
ï£«
ï£­
2
1
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾.
6.0.3
Linear Independence
When a vector is in the span of some other vectors you can say it is dependent on these
other vectors and that all the vectors involved are a dependent set of vectors. The precise
deï¬nition follows.
Deï¬nition 6.0.20 A set of vectors, {v1, Â· Â· Â·, vn} is dependent if there exist scalars,
c1, c2, Â· Â· Â·, cn not all zero such that
c1v1 + c2v2 + Â· Â· Â· + cnvn = 0.
People often refer to this as: There exists a nontrivial linear combination of the vectors which
equals zero. (It is nontrivial because some ck is nonzero.) The set of vectors, {v1, Â· Â· Â·, vn}
is independent if it is not dependent. Thus there is no nontrivial linear combination which
equals zero. Or equivalently, if
c1v1 + c2v2 + Â· Â· Â· + cnvn = 0

117
then each ci = 0. If you ï¬nd scalars, c1, c2, Â· Â· Â·, cn not all zero such that
c1v1 + c2v2 + Â· Â· Â· + cnvn = 0
this equation is called a dependence relation.
The following theorem is important.
Theorem 6.0.21 A set of vectors, {v1, Â· Â· Â·, vn} is dependent if and only if one of
the vectors is a linear combination of the others.
Proof: Suppose ï¬rst that {v1, Â· Â· Â·, vn} is dependent. Then there exist scalars, c1, Â·Â·Â·, cn
not all zero such that
c1v1 + c2v2 + Â· Â· Â· + cnvn = 0
Let ck be one of the scalars which is not zero. Then from the above equation,
ckvk = âˆ’c1v1 âˆ’c2v2 Â· Â· Â· âˆ’ckâˆ’1vkâˆ’1 âˆ’ck+1vk Â· Â· Â· âˆ’cnvn
Now divide both sides by ck to obtain
vk
=
(âˆ’c1/ck) v1 + (âˆ’c2/ck) v2 + Â· Â· Â· + (âˆ’ckâˆ’1/ck) vkâˆ’1
+ (âˆ’ck+1/ck) vk + Â· Â· Â· + (âˆ’cn/ck) vn.
and this shows vk is a linear combination of the other vectors.
Now suppose
vk = d1v1 + Â· Â· Â· + dkâˆ’1vkâˆ’1 + dk+1vk+1 + Â· Â· Â· + dnvn.
Then
0 = d1v1 + Â· Â· Â· + dkâˆ’1vkâˆ’1 + (âˆ’1) vk + dk+1vk+1 + Â· Â· Â· + dnvn
and so there is a nontrivial linear combination which equals zero. In fact (âˆ’1) Ì¸= 0. This
proves the theorem.
Observation 6.0.22 Any set of vectors containing the zero vector is dependent. To see
this, multiply the zero vector by 1 and all the other vectors by 0 and then add them together.
You have a nontrivial linear combination equal to zero.
How can you tell if {a1, Â· Â· Â·, an} is independent or dependent? It must be one or the
other. How can you determine which it is? Let
ak =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
a1k
a2k
...
amk
ï£¶
ï£·
ï£·
ï£·
ï£¸
Then a linear combination of the aj where aj is multiplied by the scalar, xj is of the form
x1
ï£«
ï£¬
ï£¬
ï£¬
ï£­
a11
a21
...
am1
ï£¶
ï£·
ï£·
ï£·
ï£¸+ x2
ï£«
ï£¬
ï£¬
ï£¬
ï£­
a12
a22
...
am2
ï£¶
ï£·
ï£·
ï£·
ï£¸+ Â· Â· Â· + xn
ï£«
ï£¬
ï£¬
ï£¬
ï£­
a1n
a2n
...
amn
ï£¶
ï£·
ï£·
ï£·
ï£¸

118
SPANNING SETS AND LINEAR INDEPENDENCE 18,19 SEPT.
which is the same as
ï£«
ï£¬
ï£¬
ï£¬
ï£­
x1a11 + x2a12 + Â· Â· Â· + xna1n
x1a21 + x2a22 + Â· Â· Â· + xna2n
...
x1am + x2a2m + Â· Â· Â· + xnamn
ï£¶
ï£·
ï£·
ï£·
ï£¸
Therefore,
x1
ï£«
ï£¬
ï£¬
ï£¬
ï£­
a11
a21
...
am1
ï£¶
ï£·
ï£·
ï£·
ï£¸+ x2
ï£«
ï£¬
ï£¬
ï£¬
ï£­
a12
a22
...
am2
ï£¶
ï£·
ï£·
ï£·
ï£¸+ Â· Â· Â· + xn
ï£«
ï£¬
ï£¬
ï£¬
ï£­
a1n
a2n
...
amn
ï£¶
ï£·
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£¸
if and only if
ï£«
ï£¬
ï£¬
ï£¬
ï£­
x1a11 + x2a12 + Â· Â· Â· + xna1n = 0
x1a21 + x2a22 + Â· Â· Â· + xna2n = 0
...
x1am + x2a2m + Â· Â· Â· + xnamn = 0
ï£¶
ï£·
ï£·
ï£·
ï£¸
Thus if A is an m Ã— n matrix, the columns of A are dependent if and only if there exists
a nonzero (nontrivial) solution to the system of equations represented by the augmented
matrix,
Â¡ A
|
0 Â¢
.
If {v1, Â· Â· Â·, vm} are dependent, it follows from Theorem 6.0.21 that one of the vectors is
a linear combination of the others. Say vk is a linear combination of the others. This means
a suitable linear combination of the other vectors added to vk yields 0.
This leads directly to the following theorem.
6.0.4
Recognizing Linear Dependence
Theorem 6.0.23 Let {v1, Â· Â· Â·, vm} be vectors in Rn. Make these vectors the rows
of a matrix, A. Thus A is of the form
ï£«
ï£¬
ï£­
âˆ’
v1
âˆ’
...
âˆ’
vm
âˆ’
ï£¶
ï£·
ï£¸.
Then the vectors are dependent if and only if rank (A) < m.
Proof: If the vectors are dependent, then a linear combination gives the zero vector.
Thus the row reduced echelon form has at least one row of zeros and so rank (A) < m.
If rank (A) < m, then the row reduced echelon form has at least one row of zeros. This
row of zeros was obtained from doing row operations and so the rows are dependent. This
proves the theorem.
Now recall Theorem 5.3.8 on Page 95 which is listed here for convenience.
Theorem 6.0.24 Let A be an m Ã— n matrix. Form the augmented matrix,
Â¡
A
|
0
Â¢
(6.5)
so that A is the coeï¬ƒcient matrix of a system of linear equations with n variables. Then the
number of free variables = n âˆ’rank (A) .

119
This theorem will be used to establish the following.
Theorem 6.0.25 Any set of n vectors in Rm is linearly dependent if n > m.
Proof: Let the set of n vectors be {a1, Â· Â· Â·, an} and make them the column vectors of a
matrix, A. Thus A is of the form
ï£«
ï£­
|
|
a1
Â· Â· Â·
an
|
|
ï£¶
ï£¸.
Consider the augmented matrix of Theorem 5.3.8 listed above. The number of free variables
equals n âˆ’rank (A) and rank (A) is no more than m because there are only m rows in the
matrix. Therefore, the number of free variables in the system of equations represented by
6.5 equals n âˆ’rank (A) > n âˆ’m > 0. Since there exist free variables, there exist non zero
solutions to the system represented by 6.5 which implies a nontrivial linear combination of
the vectors equals zero. Thus the set of vectors is dependent. This proves the theorem.
6.0.5
Discovering Dependence Relations
Suppose you have some vectors, {v1, Â· Â· Â·, vn} and you wonder whether they are independent
or dependent. If dependent, can you ï¬nd a dependence relation? How do you go about
answering this question? Recall the deï¬nition. You are looking for scalars, x1, Â· Â· Â·, xn such
that
x1v1 + Â· Â· Â· + xnvn = 0
and you are trying to ï¬nd whether there are any nonzero solutions to this vector equation.
If the vectors, vi are in Fm, then the above is really just a homogeneous system of equations
because there is an equation for each component. Thus you form the augmented matrix,
Â¡
v1
Â· Â· Â·
vn
|
0
Â¢
and row reduce to ï¬nd the solution. If the only solution is x1 = x2 = Â· Â· Â· = xn = 0, then
the vectors are linearly independent. If there exists something nonzero in the system of
equations, then you have produced a dependence relation.
Example 6.0.26 Here are some vectors in R3.
ï£«
ï£­
1
2
0
ï£¶
ï£¸,
ï£«
ï£­
0
1
1
ï£¶
ï£¸,
ï£«
ï£­
2
5
1
ï£¶
ï£¸,
ï£«
ï£­
1
1
1
ï£¶
ï£¸. I know
by Theorem 6.0.25 that these vectors are dependent. However, I would like to ï¬nd a depen-
dence relation.
To do this, I let them be the columns of an augmented matrix as shown.
ï£«
ï£­
1
0
2
1
|
0
2
1
5
1
|
0
0
1
1
1
|
0
ï£¶
ï£¸
Then I row reduce this in order to ï¬nd the solutions. The row reduced echelon form is
ï£«
ï£­
1
0
2
0
0
0
1
1
0
0
0
0
0
1
0
ï£¶
ï£¸.

120
SPANNING SETS AND LINEAR INDEPENDENCE 18,19 SEPT.
Therefore, The solutions are x4 = 0, x2 = âˆ’x3, and x1 = âˆ’2x3. Thus, letting x3 = t, all the
solutions are
(âˆ’2t, âˆ’t, t, 0) : t âˆˆR
and so if you let t = 1, you ï¬nd the dependence relation,
âˆ’2
ï£«
ï£­
1
2
0
ï£¶
ï£¸+ (âˆ’1)
ï£«
ï£­
0
1
1
ï£¶
ï£¸+ 1
ï£«
ï£­
2
5
1
ï£¶
ï£¸+ 0
ï£«
ï£­
1
1
1
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸.
There is nothing new here at all! It is just another way of asking for the solution to a
homogeneous system of linear equations.1
1This is the style in linear algebra books these days, ask the same question over and over again in
disguised form to give the illusion of learning something new. However, there is much more to linear algebra
than row reduction of augmented matrices.

Matrices
7.1
Matrix Operations And Algebra 20,21 Sept.
Quiz
1. Here are three points: (1, 1, 1) , (2, 0, 1) , (0, 1, 0) . Find an equation of a plane which
contains all three points.
2. Find the equation of a plane which is parallel to the plane whose equations is x+2y +
z = 7 which contains the point (1, 2, 1) .
3. Here are three vectors: (1, 2, 1) , (2, 1, 0) , (âˆ’2, 0, 1) . Find the volume of the paral-
lelepiped determined by these three vectors.
4. Here are three vectors.
ï£«
ï£­
1
2
2
ï£¶
ï£¸
ï£«
ï£­
1
3
1
ï£¶
ï£¸
ï£«
ï£­
1
1
3
ï£¶
ï£¸. Determine whether the vectors are
dependent. If they are dependent, ï¬nd a dependence relation.
5. Here is a system of equations.
3x + 4y + z = 4
x + 2y + z = 2
y + z = 1
Find the complete solution.
7.1.1
Addition And Scalar Multiplication Of Matrices
You have now solved systems of equations by writing them in terms of an augmented matrix
and then doing row operations on this augmented matrix. It turns out such rectangular
arrays of numbers are important from many other diï¬€erent points of view. Numbers are
also called scalars. In these notes numbers will always be either real or complex numbers.
A matrix is a rectangular array of numbers. Several of them are referred to as matrices.
For example, here is a matrix.
ï£«
ï£­
1
2
3
4
5
2
8
7
6
âˆ’9
1
2
ï£¶
ï£¸
The size or dimension of a matrix is deï¬ned as m Ã— n where m is the number of rows and n
is the number of columns. The above matrix is a 3 Ã— 4 matrix because there are three rows
and four columns. The ï¬rst row is (1 2 3 4) , the second row is (5 2 8 7) and so forth. The
121

122
MATRICES
ï¬rst column is
ï£«
ï£­
1
5
6
ï£¶
ï£¸. When specifying the size of a matrix, you always list the number of
rows before the number of columns. Also, you can remember the columns are like columns
in a Greek temple. They stand upright while the rows just lay there like rows made by
a tractor in a plowed ï¬eld. Elements of the matrix are identiï¬ed according to position in
the matrix. For example, 8 is in position 2, 3 because it is in the second row and the third
column. You might remember that you always list the rows before the columns by using
the phrase Rowman Catholic. The symbol, (aij) refersto a matrix. The entry in the ith
row and the jth column of this matrix is denoted by aij. Using this notation on the above
matrix, a23 = 8, a32 = âˆ’9, a12 = 2, etc.
There are various operations which are done on matrices. Matrices can be added mul-
tiplied by a scalar, and multiplied by other matrices. To illustrate scalar multiplication,
consider the following example in which a matrix is being multiplied by the scalar, 3.
3
ï£«
ï£­
1
2
3
4
5
2
8
7
6
âˆ’9
1
2
ï£¶
ï£¸=
ï£«
ï£­
3
6
9
12
15
6
24
21
18
âˆ’27
3
6
ï£¶
ï£¸.
The new matrix is obtained by multiplying every entry of the original matrix by the given
scalar. If A is an m Ã— n matrix, âˆ’A is deï¬ned to equal (âˆ’1) A.
Two matrices must be the same size to be added. The sum of two matrices is a matrix
which is obtained by adding the corresponding entries. Thus
ï£«
ï£­
1
2
3
4
5
2
ï£¶
ï£¸+
ï£«
ï£­
âˆ’1
4
2
8
6
âˆ’4
ï£¶
ï£¸=
ï£«
ï£­
0
6
5
12
11
âˆ’2
ï£¶
ï£¸.
Two matrices are equal exactly when they are the same size and the corresponding entries
are identical. Thus
ï£«
ï£­
0
0
0
0
0
0
ï£¶
ï£¸Ì¸=
Âµ
0
0
0
0
Â¶
because they are diï¬€erent sizes. As noted above, you write (cij) for the matrix C whose
ijth entry is cij. In doing arithmetic with matrices you must deï¬ne what happens in terms
of the cij sometimes called the entries of the matrix or the components of the matrix.
The above discussion stated for general matrices is given in the following deï¬nition.
Deï¬nition 7.1.1 (Scalar Multiplication) If A = (aij) and k is a scalar, then kA =
(kaij) .
Example 7.1.2 7
Âµ
2
0
1
âˆ’4
Â¶
=
Âµ
14
0
7
âˆ’28
Â¶
.
Deï¬nition 7.1.3 (Addition) If A = (aij) and B = (bij) are two m Ã— n matrices.
Then A + B = C where
C = (cij)
for cij = aij + bij.
Example 7.1.4
Âµ
1
2
3
1
0
4
Â¶
+
Âµ
5
2
3
âˆ’6
2
1
Â¶
=
Âµ
6
4
6
âˆ’5
2
5
Â¶

7.1.
MATRIX OPERATIONS AND ALGEBRA 20,21 SEPT.
123
To save on notation, we will often use Aij to refer to the ijth entry of the matrix, A.
Deï¬nition 7.1.5 (The zero matrix) The m Ã— n zero matrix is the m Ã— n matrix
having every entry equal to zero. It is denoted by 0.
Example 7.1.6 The 2 Ã— 3 zero matrix is
Âµ
0
0
0
0
0
0
Â¶
.
Note there are 2Ã—3 zero matrices, 3Ã—4 zero matrices, etc. In fact there is a zero matrix
for every size.
Deï¬nition 7.1.7 (Equality of matrices) Let A and B be two matrices. Then A = B
means that the two matrices are of the same size and for A = (aij) and B = (bij) , aij = bij
for all 1 â‰¤i â‰¤m and 1 â‰¤j â‰¤n.
The following properties of matrices can be easily veriï¬ed. You should do so.
â€¢ Commutative Law Of Addition.
A + B = B + A,
(7.1)
â€¢ Associative Law for Addition.
(A + B) + C = A + (B + C) ,
(7.2)
â€¢ Existence of an Additive Identity
A + 0 = A,
(7.3)
â€¢ Existence of an Additive Inverse
A + (âˆ’A) = 0,
(7.4)
Also for Î±, Î² scalars, the following additional properties hold.
â€¢ Distributive law over Matrix Addition.
Î± (A + B) = Î±A + Î±B,
(7.5)
â€¢ Distributive law over Scalar Addition
(Î± + Î²) A = Î±A + Î²A,
(7.6)
â€¢ Associative law for Scalar Multiplication
Î± (Î²A) = Î±Î² (A) ,
(7.7)
â€¢ Rule for Multiplication by 1.
1A = A.
(7.8)
As an example, consider the Commutative Law of Addition.
Let A + B = C and
B + A = D. Why is D = C?
Cij = Aij + Bij = Bij + Aij = Dij.
Therefore, C = D because the ijth entries are the same. Note that the conclusion follows
from the commutative law of addition of numbers.

124
MATRICES
7.1.2
Multiplication Of Matrices
Deï¬nition 7.1.8 Matrices which are nÃ—1 or 1Ã—n are called vectors and are often
denoted by a bold letter. Thus the n Ã— 1 matrix
x =
ï£«
ï£¬
ï£­
x1
...
xn
ï£¶
ï£·
ï£¸
is also called a column vector. The 1 Ã— n matrix
(x1 Â· Â· Â· xn)
is called a row vector.
Although the following description of matrix multiplication may seem strange, it is in
fact the most important and useful of the matrix operations. To begin with consider the case
where a matrix is multiplied by a column vector. We will illustrate the general deï¬nition
by ï¬rst considering a special case.
Âµ
1
2
3
4
5
6
Â¶ ï£«
ï£­
7
8
9
ï£¶
ï£¸=?
One way to remember this is as follows. Slide the vector, placing it on top the two rows as
shown and then do the indicated operation.
ï£«
ï£­
7
1
8
2
9
3
7
4
8
5
9
6
ï£¶
ï£¸â†’
Âµ 7 Ã— 1 + 8 Ã— 2 + 9 Ã— 3
7 Ã— 4 + 8 Ã— 5 + 9 Ã— 6
Â¶
=
Âµ 50
122
Â¶
.
multiply the numbers on the top by the numbers on the bottom and add them up to get a
single number for each row of the matrix as shown above.
In more general terms,
Âµ
a11
a12
a13
a21
a22
a23
Â¶ ï£«
ï£­
x1
x2
x3
ï£¶
ï£¸=
Âµ
a11x1 + a12x2 + a13x3
a21x1 + a22x2 + a23x3
Â¶
.
Another way to think of this is
x1
Âµ
a11
a21
Â¶
+ x2
Âµ
a12
a22
Â¶
+ x3
Âµ
a13
a23
Â¶
Thus you take x1 times the ï¬rst column, add to x2 times the second column, and ï¬nally
x3 times the third column. In general, here is the deï¬nition of how to multiply an (m Ã— n)
matrix times a (n Ã— 1) matrix.
Deï¬nition 7.1.9 Let A = Aij be an m Ã— n matrix and let v be an n Ã— 1 matrix,
v =
ï£«
ï£¬
ï£­
v1
...
vn
ï£¶
ï£·
ï£¸

7.1.
MATRIX OPERATIONS AND ALGEBRA 20,21 SEPT.
125
Then Av is an m Ã— 1 matrix and the ith component of this matrix is
(Av)i = Ai1v1 + Ai2v2 + Â· Â· Â· + Ainvn =
n
X
j=1
Aijvj.
Thus
Av =
ï£«
ï£¬
ï£­
Pn
j=1 A1jvj
...
Pn
j=1 Amjvj
ï£¶
ï£·
ï£¸.
(7.9)
In other words, if
A = (a1, Â· Â· Â·, an)
where the ak are the columns,
Av =
n
X
k=1
vkak
This follows from 7.9 and the observation that the jth column of A is
ï£«
ï£¬
ï£¬
ï£¬
ï£­
A1j
A2j
...
Amj
ï£¶
ï£·
ï£·
ï£·
ï£¸
so 7.9 reduces to
v1
ï£«
ï£¬
ï£¬
ï£¬
ï£­
A11
A21
...
Am1
ï£¶
ï£·
ï£·
ï£·
ï£¸+ v2
ï£«
ï£¬
ï£¬
ï£¬
ï£­
A12
A22
...
Am2
ï£¶
ï£·
ï£·
ï£·
ï£¸+ Â· Â· Â· + vn
ï£«
ï£¬
ï£¬
ï£¬
ï£­
A1n
A2n
...
Amn
ï£¶
ï£·
ï£·
ï£·
ï£¸
Note also that multiplication by an m Ã— n matrix takes an n Ã— 1 matrix, and produces an
m Ã— 1 matrix.
Here is another example.
Example 7.1.10 Compute
ï£«
ï£­
1
2
1
3
0
2
1
âˆ’2
2
1
4
1
ï£¶
ï£¸
ï£«
ï£¬
ï£¬
ï£­
1
2
0
1
ï£¶
ï£·
ï£·
ï£¸.
First of all this is of the form (3 Ã— 4) (4 Ã— 1) and so the result should be a (3 Ã— 1) . Note
how the inside numbers cancel. To get the element in the second row and ï¬rst and only
column, compute
4
X
k=1
a2kvk
=
a21v1 + a22v2 + a23v3 + a24v4
=
0 Ã— 1 + 2 Ã— 2 + 1 Ã— 0 + (âˆ’2) Ã— 1 = 2.

126
MATRICES
You should do the rest of the problem and verify
ï£«
ï£­
1
2
1
3
0
2
1
âˆ’2
2
1
4
1
ï£¶
ï£¸
ï£«
ï£¬
ï£¬
ï£­
1
2
0
1
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£­
8
2
5
ï£¶
ï£¸.
The next task is to multiply an m Ã— n matrix times an n Ã— p matrix. Before doing so,
the following may be helpful.
For A and B matrices, in order to form the product, AB the number of columns of A
must equal the number of rows of B.
(m Ã—
these must match!
[
n) (n Ã— p
) = m Ã— p
Note the two outside numbers give the size of the product. Remember:
If the two middle numbers donâ€™t match, you canâ€™t multiply the matrices!
Deï¬nition 7.1.11 When the number of columns of A equals the number of rows of
B the two matrices are said to be conformable and the product, AB is obtained as follows.
Let A be an m Ã— n matrix and let B be an n Ã— p matrix. Then B is of the form
B = (b1, Â· Â· Â·, bp)
where bk is an n Ã— 1 matrix or column vector. Then the m Ã— p matrix, AB is deï¬ned as
follows:
AB â‰¡(Ab1, Â· Â· Â·, Abp)
(7.10)
where Abk is an m Ã— 1 matrix or column vector which gives the kth column of AB.
Example 7.1.12 Multiply the following.
Âµ 1
2
1
0
2
1
Â¶ ï£«
ï£­
1
2
0
0
3
1
âˆ’2
1
1
ï£¶
ï£¸
The ï¬rst thing you need to check before doing anything else is whether it is possible to
do the multiplication. The ï¬rst matrix is a 2Ã—3 and the second matrix is a 3Ã—3. Therefore,
is it possible to multiply these matrices. According to the above discussion it should be a
2 Ã— 3 matrix of the form
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
First column
z
}|
{
Âµ
1
2
1
0
2
1
Â¶ ï£«
ï£­
1
0
âˆ’2
ï£¶
ï£¸,
Second column
z
}|
{
Âµ
1
2
1
0
2
1
Â¶ ï£«
ï£­
2
3
1
ï£¶
ï£¸,
Third column
z
}|
{
Âµ
1
2
1
0
2
1
Â¶ ï£«
ï£­
0
1
1
ï£¶
ï£¸
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
You know how to multiply a matrix times a vector and so you do so to obtain each of the
three columns. Thus
Âµ 1
2
1
0
2
1
Â¶ ï£«
ï£­
1
2
0
0
3
1
âˆ’2
1
1
ï£¶
ï£¸=
Âµ âˆ’1
9
3
âˆ’2
7
3
Â¶
.

7.1.
MATRIX OPERATIONS AND ALGEBRA 20,21 SEPT.
127
Example 7.1.13 Multiply the following.
ï£«
ï£­
1
2
0
0
3
1
âˆ’2
1
1
ï£¶
ï£¸
Âµ 1
2
1
0
2
1
Â¶
First check if it is possible. This is of the form (3 Ã— 3) (2 Ã— 3) . The inside numbers do not
match and so you canâ€™t do this multiplication. This means that anything you write will be
absolute nonsense because it is impossible to multiply these matrices in this order. Arenâ€™t
they the same two matrices considered in the previous example? Yes they are. It is just
that here they are in a diï¬€erent order. This shows something you must always remember
about matrix multiplication.
Order Matters!
Matrix Multiplication Is Not Commutative!
This is very diï¬€erent than multiplication of numbers!
7.1.3
The ijth Entry Of A Product
It is important to describe matrix multiplication in terms of entries of the matrices. What
is the ijth entry of AB? It would be the ith entry of the jth column of AB. Thus it would
be the ith entry of Abj. Now
bj =
ï£«
ï£¬
ï£­
B1j
...
Bnj
ï£¶
ï£·
ï£¸
and from the above deï¬nition, the ith entry is
n
X
k=1
AikBkj.
(7.11)
In terms of pictures of the matrix, you are doing
ï£«
ï£¬
ï£¬
ï£¬
ï£­
A11
A12
Â· Â· Â·
A1n
A21
A22
Â· Â· Â·
A2n
...
...
...
Am1
Am2
Â· Â· Â·
Amn
ï£¶
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£­
B11
B12
Â· Â· Â·
B1p
B21
B22
Â· Â· Â·
B2p
...
...
...
Bn1
Bn2
Â· Â· Â·
Bnp
ï£¶
ï£·
ï£·
ï£·
ï£¸
Then as explained above, the jth column is of the form
ï£«
ï£¬
ï£¬
ï£¬
ï£­
A11
A12
Â· Â· Â·
A1n
A21
A22
Â· Â· Â·
A2n
...
...
...
Am1
Am2
Â· Â· Â·
Amn
ï£¶
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£­
B1j
B2j
...
Bnj
ï£¶
ï£·
ï£·
ï£·
ï£¸
which is a m Ã— 1 matrix or column vector which equals
ï£«
ï£¬
ï£¬
ï£¬
ï£­
A11
A21
...
Am1
ï£¶
ï£·
ï£·
ï£·
ï£¸B1j +
ï£«
ï£¬
ï£¬
ï£¬
ï£­
A12
A22
...
Am2
ï£¶
ï£·
ï£·
ï£·
ï£¸B2j + Â· Â· Â· +
ï£«
ï£¬
ï£¬
ï£¬
ï£­
A1n
A2n
...
Amn
ï£¶
ï£·
ï£·
ï£·
ï£¸Bnj.

128
MATRICES
The second entry of this m Ã— 1 matrix is
A21B1j + A22B2j + Â· Â· Â· + A2nBnj =
m
X
k=1
A2kBkj.
Similarly, the ith entry of this m Ã— 1 matrix is
Ai1B1j + Ai2B2j + Â· Â· Â· + AinBnj =
m
X
k=1
AikBkj.
This shows the following deï¬nition for matrix multiplication in terms of the ijth entries of
the product coincides with Deï¬nition 7.1.11.
Deï¬nition 7.1.14 Let A = (Aij) be an mÃ—n matrix and let B = (Bij) be an nÃ—p
matrix. Then AB is an m Ã— p matrix and
(AB)ij =
n
X
k=1
AikBkj.
(7.12)
Example 7.1.15 Multiply if possible
ï£«
ï£­
1
2
3
1
2
6
ï£¶
ï£¸
Âµ
2
3
1
7
6
2
Â¶
.
First check to see if this is possible. It is of the form (3 Ã— 2) (2 Ã— 3) and since the inside
numbers match, the two matrices are conformable and it is possible to do the multiplication.
The result should be a 3 Ã— 3 matrix. The answer is of the form
ï£«
ï£­
ï£«
ï£­
1
2
3
1
2
6
ï£¶
ï£¸
Âµ
2
7
Â¶
,
ï£«
ï£­
1
2
3
1
2
6
ï£¶
ï£¸
Âµ
3
6
Â¶
,
ï£«
ï£­
1
2
3
1
2
6
ï£¶
ï£¸
Âµ
1
2
Â¶ï£¶
ï£¸
where the commas separate the columns in the resulting product. Thus the above product
equals
ï£«
ï£­
16
15
5
13
15
5
46
42
14
ï£¶
ï£¸,
a 3 Ã— 3 matrix as desired. In terms of the ijth entries and the above deï¬nition, the entry in
the third row and second column of the product should equal
X
j
a3kbk2
=
a31b12 + a32b22
=
2 Ã— 3 + 6 Ã— 6 = 42.
You should try a few more such examples to verify the above deï¬nition in terms of the ijth
entries works for other entries.
Example 7.1.16 Multiply if possible
ï£«
ï£­
1
2
3
1
2
6
ï£¶
ï£¸
ï£«
ï£­
2
3
1
7
6
2
0
0
0
ï£¶
ï£¸.
This is not possible because it is of the form (3 Ã— 2) (3 Ã— 3) and the middle numbers
donâ€™t match. In other words the two matrices are not conformable in the indicated order.

7.1.
MATRIX OPERATIONS AND ALGEBRA 20,21 SEPT.
129
Example 7.1.17 Multiply if possible
ï£«
ï£­
2
3
1
7
6
2
0
0
0
ï£¶
ï£¸
ï£«
ï£­
1
2
3
1
2
6
ï£¶
ï£¸.
This is possible because in this case it is of the form (3 Ã— 3) (3 Ã— 2) and the middle
numbers do match so the matrices are conformable. When the multiplication is done it
equals
ï£«
ï£­
13
13
29
32
0
0
ï£¶
ï£¸.
Check this and be sure you come up with the same answer.
Example 7.1.18 Multiply if possible
ï£«
ï£­
1
2
1
ï£¶
ï£¸Â¡
1
2
1
0
Â¢
.
In this case you are trying to do (3 Ã— 1) (1 Ã— 4) . The inside numbers match so you can
do it. Verify
ï£«
ï£­
1
2
1
ï£¶
ï£¸Â¡ 1
2
1
0 Â¢
=
ï£«
ï£­
1
2
1
0
2
4
2
0
1
2
1
0
ï£¶
ï£¸
7.1.4
Properties Of Matrix Multiplication
As pointed out above, sometimes it is possible to multiply matrices in one order but not
in the other order. What if it makes sense to multiply them in either order? Will the two
products be equal then?
Example 7.1.19 Compare
Âµ 1
2
3
4
Â¶ Âµ 0
1
1
0
Â¶
and
Âµ 0
1
1
0
Â¶ Âµ 1
2
3
4
Â¶
.
The ï¬rst product is
Âµ
1
2
3
4
Â¶ Âµ
0
1
1
0
Â¶
=
Âµ
2
1
4
3
Â¶
.
The second product is
Âµ 0
1
1
0
Â¶ Âµ 1
2
3
4
Â¶
=
Âµ 3
4
1
2
Â¶
.
You see these are not equal. Again you cannot conclude that AB = BA for matrix mul-
tiplication even when multiplication is deï¬ned in both orders. However, there are some
properties which do hold.
Proposition 7.1.20 If all multiplications and additions make sense, the following hold
for matrices, A, B, C and a, b scalars.
A (aB + bC) = a (AB) + b (AC)
(7.13)
(B + C) A = BA + CA
(7.14)
A (BC) = (AB) C
(7.15)

130
MATRICES
Proof: Using Deï¬nition 7.1.14,
(A (aB + bC))ij
=
X
k
Aik (aB + bC)kj
=
X
k
Aik (aBkj + bCkj)
=
a
X
k
AikBkj + b
X
k
AikCkj
=
a (AB)ij + b (AC)ij
=
(a (AB) + b (AC))ij .
Thus A (B + C) = AB + AC as claimed. Formula 7.14 is entirely similar.
Formula 7.15 is the associative law of multiplication. Using Deï¬nition 7.1.14,
(A (BC))ij
=
X
k
Aik (BC)kj
=
X
k
Aik
X
l
BklClj
=
X
l
(AB)il Clj
=
((AB) C)ij .
This proves 7.15.
7.1.5
The Transpose
Another important operation on matrices is that of taking the transpose. The following
example shows what is meant by this operation, denoted by placing a T as an exponent on
the matrix.
ï£«
ï£­
1
4
3
1
2
6
ï£¶
ï£¸
T
=
Âµ
1
3
2
4
1
6
Â¶
What happened? The ï¬rst column became the ï¬rst row and the second column became the
second row. Thus the 3 Ã— 2 matrix became a 2 Ã— 3 matrix. The number 3 was in the second
row and the ï¬rst column and it ended up in the ï¬rst row and second column. Here is the
deï¬nition.
Deï¬nition 7.1.21 Let A be an m Ã— n matrix. Then AT denotes the n Ã— m matrix
which is deï¬ned as follows.
Â¡
AT Â¢
ij = Aji
Example 7.1.22
Âµ
1
2
âˆ’6
3
5
4
Â¶T
=
ï£«
ï£­
1
3
2
5
âˆ’6
4
ï£¶
ï£¸.
The transpose of a matrix has the following important properties.
Lemma 7.1.23 Let A be an m Ã— n matrix and let B be a n Ã— p matrix. Then
(AB)T = BT AT
(7.16)

7.1.
MATRIX OPERATIONS AND ALGEBRA 20,21 SEPT.
131
and if Î± and Î² are scalars,
(Î±A + Î²B)T = Î±AT + Î²BT
(7.17)
Proof: From the deï¬nition,
Â³
(AB)T Â´
ij
=
(AB)ji
=
X
k
AjkBki
=
X
k
Â¡
BT Â¢
ik
Â¡
AT Â¢
kj
=
Â¡
BT AT Â¢
ij
The proof of Formula 7.17 is left as an exercise and this proves the lemma.
Deï¬nition 7.1.24 An n Ã— n matrix, A is said to be symmetric if A = AT . It is
said to be skew symmetric if A = âˆ’AT .
Example 7.1.25 Let
A =
ï£«
ï£­
2
1
3
1
5
âˆ’3
3
âˆ’3
7
ï£¶
ï£¸.
Then A is symmetric.
Example 7.1.26 Let
A =
ï£«
ï£­
0
1
3
âˆ’1
0
2
âˆ’3
âˆ’2
0
ï£¶
ï£¸
Then A is skew symmetric.
7.1.6
The Identity And Inverses
There is a special matrix called I and referred to as the identity matrix. It is always a
square matrix, meaning the number of rows equals the number of columns and it has the
property that there are ones down the main diagonal and zeroes elsewhere. Here are some
identity matrices of various sizes.
(1) ,
Âµ
1
0
0
1
Â¶
,
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸,
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
ï£¶
ï£·
ï£·
ï£¸.
The ï¬rst is the 1 Ã— 1 identity matrix, the second is the 2 Ã— 2 identity matrix, the third is
the 3 Ã— 3 identity matrix, and the fourth is the 4 Ã— 4 identity matrix. By extension, you can
likely see what the n Ã— n identity matrix would be. It is so important that there is a special
symbol to denote the ijth entry of the identity matrix
Iij = Î´ij
where Î´ij is the Kroneker symbol deï¬ned by
Î´ij =
Â½
1 if i = j
0 if i Ì¸= j
It is called the identity matrix because it is a multiplicative identity in the following
sense.

132
MATRICES
Lemma 7.1.27 Suppose A is an mÃ—n matrix and In is the nÃ—n identity matrix. Then
AIn = A. If Im is the m Ã— m identity matrix, it also follows that ImA = A.
Proof:
(AIn)ij
=
X
k
AikÎ´kj
=
Aij
and so AIn = A. The other case is left as an exercise for you.
Deï¬nition 7.1.28 An nÃ—n matrix, A has an inverse, Aâˆ’1 if and only if AAâˆ’1 =
Aâˆ’1A = I. Such a matrix is called invertible.
It is very important to observe that the inverse of a matrix, if it exists, is unique. Another
way to think of this is that if it acts like the inverse, then it is the inverse.
Theorem 7.1.29 Suppose Aâˆ’1 exists and AB = BA = I. Then B = Aâˆ’1.
Proof:
Aâˆ’1 = Aâˆ’1I = Aâˆ’1 (AB) =
Â¡
Aâˆ’1A
Â¢
B = IB = B.
Unlike ordinary multiplication of numbers, it can happen that A Ì¸= 0 but A may fail to
have an inverse. This is illustrated in the following example.
Example 7.1.30 Let A =
Âµ
1
1
1
1
Â¶
. Does A have an inverse?
One might think A would have an inverse because it does not equal zero. However,
Âµ 1
1
1
1
Â¶ Âµ âˆ’1
1
Â¶
=
Âµ 0
0
Â¶
and if Aâˆ’1 existed, this could not happen because you could write
Âµ
0
0
Â¶
= Aâˆ’1
ÂµÂµ
0
0
Â¶Â¶
= Aâˆ’1
Âµ
A
Âµ
âˆ’1
1
Â¶Â¶
=
=
Â¡
Aâˆ’1A
Â¢ Âµ
âˆ’1
1
Â¶
= I
Âµ
âˆ’1
1
Â¶
=
Âµ
âˆ’1
1
Â¶
,
a contradiction. Thus the answer is that A does not have an inverse.
Example 7.1.31 Let A =
Âµ
1
1
1
2
Â¶
. Show
Âµ
2
âˆ’1
âˆ’1
1
Â¶
is the inverse of A.
To check this, multiply
Âµ
1
1
1
2
Â¶ Âµ
2
âˆ’1
âˆ’1
1
Â¶
=
Âµ
1
0
0
1
Â¶
and
Âµ
2
âˆ’1
âˆ’1
1
Â¶ Âµ
1
1
1
2
Â¶
=
Âµ
1
0
0
1
Â¶
showing that this matrix is indeed the inverse of A.

7.2. FINDING THE INVERSE OF A MATRIX, GAUSS JORDAN METHOD 21,22 SEPT.133
7.2
Finding The Inverse Of A Matrix, Gauss Jordan
Method 21,22 Sept.
Quiz
1. Multiply the matrices if possible.
Âµ 1
1
2
0
1
1
Â¶ ï£«
ï£­
1
1
1
0
1
2
ï£¶
ï£¸
2. Multiply the matrices if possible.
Â¡
1
1
2
0
Â¢
ï£«
ï£¬
ï£¬
ï£­
1
0
1
1
ï£¶
ï£·
ï£·
ï£¸
3. Multiply the matrices if possible.
ï£«
ï£¬
ï£¬
ï£­
1
0
1
1
ï£¶
ï£·
ï£·
ï£¸
Â¡ 1
1
2
0 Â¢
4. True or False. In each case the capital letters are matrices of an appropriate size and
the lower case letters represent numbers.
(a) A2 âˆ’B2 = (A âˆ’B) (A + B)
(b) (AB)T = AT BT
(c) (aA + bB) C = aAC + bCB
(d) If AB = 0, then either A = 0 or B = 0.
(e) A/A = 1
(f) (AB) C = A (BC)
In the last example, how would you ï¬nd Aâˆ’1? You wish to ï¬nd a matrix,
Âµ
x
z
y
w
Â¶
such that
Âµ
1
1
1
2
Â¶ Âµ
x
z
y
w
Â¶
=
Âµ
1
0
0
1
Â¶
.
This requires the solution of the systems of equations,
x + y = 1, x + 2y = 0
and
z + w = 0, z + 2w = 1.
Writing the augmented matrix for these two systems gives
Âµ
1
1
|
1
1
2
|
0
Â¶
(7.18)

134
MATRICES
for the ï¬rst system and
Âµ
1
1
|
0
1
2
|
1
Â¶
(7.19)
for the second. Lets solve the ï¬rst system. Take (âˆ’1) times the ï¬rst row and add to the
second to get
Âµ
1
1
|
1
0
1
|
âˆ’1
Â¶
Now take (âˆ’1) times the second row and add to the ï¬rst to get
Âµ
1
0
|
2
0
1
|
âˆ’1
Â¶
.
Putting in the variables, this says x = 2 and y = âˆ’1.
Now solve the second system, 7.19 to ï¬nd z and w. Take (âˆ’1) times the ï¬rst row and
add to the second to get
Âµ
1
1
|
0
0
1
|
1
Â¶
.
Now take (âˆ’1) times the second row and add to the ï¬rst to get
Âµ
1
0
|
âˆ’1
0
1
|
1
Â¶
.
Putting in the variables, this says z = âˆ’1 and w = 1. Therefore, the inverse is
Âµ
2
âˆ’1
âˆ’1
1
Â¶
.
Didnâ€™t the above seem rather repetitive? Note that exactly the same row operations
were used in both systems. In each case, the end result was something of the form (I|v)
where I is the identity and v gave a column of the inverse. In the above,
Âµ
x
y
Â¶
, the ï¬rst
column of the inverse was obtained ï¬rst and then the second column
Âµ
z
w
Â¶
.
To simplify this procedure, you could have written
Âµ
1
1
|
1
0
1
2
|
0
1
Â¶
and row reduced till you obtained
Âµ
1
0
|
2
âˆ’1
0
1
|
âˆ’1
1
Â¶
and read oï¬€the inverse as the 2 Ã— 2 matrix on the right side.
This is the reason for the following simple procedure for ï¬nding the inverse of a matrix.
This procedure is called the Gauss-Jordan procedure.
Procedure 7.2.1 Suppose A is an n Ã— n matrix. To ï¬nd Aâˆ’1 if it exists, form the
augmented n Ã— 2n matrix,
(A|I)
and then, if possible do row operations until you obtain an n Ã— 2n matrix of the form
(I|B) .
(7.20)
When this has been done, B = Aâˆ’1. If it is impossible to row reduce to a matrix of the form
(I|B) , then A has no inverse.

7.2. FINDING THE INVERSE OF A MATRIX, GAUSS JORDAN METHOD 21,22 SEPT.135
Example 7.2.2 Let A =
ï£«
ï£­
1
2
2
1
0
2
3
1
âˆ’1
ï£¶
ï£¸. Find Aâˆ’1 if it exists.
Set up the augmented matrix, (A|I)
ï£«
ï£­
1
2
2
|
1
0
0
1
0
2
|
0
1
0
3
1
âˆ’1
|
0
0
1
ï£¶
ï£¸
Next take (âˆ’1) times the ï¬rst row and add to the second followed by (âˆ’3) times the ï¬rst
row added to the last. This yields
ï£«
ï£­
1
2
2
|
1
0
0
0
âˆ’2
0
|
âˆ’1
1
0
0
âˆ’5
âˆ’7
|
âˆ’3
0
1
ï£¶
ï£¸.
Then take 5 times the second row and add to -2 times the last row.
ï£«
ï£­
1
2
2
|
1
0
0
0
âˆ’10
0
|
âˆ’5
5
0
0
0
14
|
1
5
âˆ’2
ï£¶
ï£¸
Next take the last row and add to (âˆ’7) times the top row. This yields
ï£«
ï£­
âˆ’7
âˆ’14
0
|
âˆ’6
5
âˆ’2
0
âˆ’10
0
|
âˆ’5
5
0
0
0
14
|
1
5
âˆ’2
ï£¶
ï£¸.
Now take (âˆ’7/5) times the second row and add to the top.
ï£«
ï£­
âˆ’7
0
0
|
1
âˆ’2
âˆ’2
0
âˆ’10
0
|
âˆ’5
5
0
0
0
14
|
1
5
âˆ’2
ï£¶
ï£¸.
Finally divide the top row by -7, the second row by -10 and the bottom row by 14 which
yields
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
0
|
âˆ’1
7
2
7
2
7
0
1
0
|
1
2
âˆ’1
2
0
0
0
1
|
1
14
5
14
âˆ’1
7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Therefore, the inverse is
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
7
2
7
2
7
1
2
âˆ’1
2
0
1
14
5
14
âˆ’1
7
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
Example 7.2.3 Let A =
ï£«
ï£­
1
2
2
1
0
2
2
2
4
ï£¶
ï£¸. Find Aâˆ’1 if it exists.

136
MATRICES
Write the augmented matrix, (A|I)
ï£«
ï£­
1
2
2
|
1
0
0
1
0
2
|
0
1
0
2
2
4
|
0
0
1
ï£¶
ï£¸
and proceed to do row operations attempting to obtain
Â¡
I|Aâˆ’1Â¢
. Take (âˆ’1) times the top
row and add to the second. Then take (âˆ’2) times the top row and add to the bottom.
ï£«
ï£­
1
2
2
|
1
0
0
0
âˆ’2
0
|
âˆ’1
1
0
0
âˆ’2
0
|
âˆ’2
0
1
ï£¶
ï£¸
Next add (âˆ’1) times the second row to the bottom row.
ï£«
ï£­
1
2
2
|
1
0
0
0
âˆ’2
0
|
âˆ’1
1
0
0
0
0
|
âˆ’1
âˆ’1
1
ï£¶
ï£¸
At this point, you can see there will be no inverse because you have obtained a row of zeros
in the left half of the augmented matrix, (A|I) . Thus there will be no way to obtain I on
the left.
Example 7.2.4 Let A =
ï£«
ï£­
1
0
1
1
âˆ’1
1
1
1
âˆ’1
ï£¶
ï£¸. Find Aâˆ’1 if it exists.
Form the augmented matrix,
ï£«
ï£­
1
0
1
|
1
0
0
1
âˆ’1
1
|
0
1
0
1
1
âˆ’1
|
0
0
1
ï£¶
ï£¸.
Now do row operations until the n Ã— n matrix on the left becomes the identity matrix. This
yields after some computations,
ï£«
ï£­
1
0
0
|
0
1
2
1
2
0
1
0
|
1
âˆ’1
0
0
0
1
|
1
âˆ’1
2
âˆ’1
2
ï£¶
ï£¸
and so the inverse of A is the matrix on the right,
ï£«
ï£­
0
1
2
1
2
1
âˆ’1
0
1
âˆ’1
2
âˆ’1
2
ï£¶
ï£¸.
Checking the answer is easy. Just multiply the matrices and see if it works.
ï£«
ï£­
1
0
1
1
âˆ’1
1
1
1
âˆ’1
ï£¶
ï£¸
ï£«
ï£­
0
1
2
1
2
1
âˆ’1
0
1
âˆ’1
2
âˆ’1
2
ï£¶
ï£¸=
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸.
Always check your answer because if you are like some of us, you will usually have made a
mistake.

7.2. FINDING THE INVERSE OF A MATRIX, GAUSS JORDAN METHOD 21,22 SEPT.137
Example 7.2.5 In this example, it is shown how to use the inverse of a matrix to ï¬nd
the solution to a system of equations. Consider the following system of equations. Use the
inverse of a suitable matrix to give the solutions to this system.
ï£«
ï£­
x + z = 1
x âˆ’y + z = 3
x + y âˆ’z = 2
ï£¶
ï£¸.
The system of equations can be written in terms of matrices as
ï£«
ï£­
1
0
1
1
âˆ’1
1
1
1
âˆ’1
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
1
3
2
ï£¶
ï£¸.
(7.21)
More simply, this is of the form Ax = b. Suppose you ï¬nd the inverse of the matrix, Aâˆ’1.
Then you could multiply both sides of this equation by Aâˆ’1 to obtain
x =
Â¡
Aâˆ’1A
Â¢
x = Aâˆ’1 (Ax) = Aâˆ’1b.
This gives the solution as x = Aâˆ’1b. Note that once you have found the inverse, you can
easily get the solution for diï¬€erent right hand sides without any eï¬€ort. It is always just
Aâˆ’1b. In the given example, the inverse of the matrix is
ï£«
ï£­
0
1
2
1
2
1
âˆ’1
0
1
âˆ’1
2
âˆ’1
2
ï£¶
ï£¸
This was shown in Example 7.2.4. Therefore, from what was just explained the solution to
the given system is
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
1
2
1
2
1
âˆ’1
0
1
âˆ’1
2
âˆ’1
2
ï£¶
ï£¸
ï£«
ï£­
1
3
2
ï£¶
ï£¸=
ï£«
ï£­
5
2
âˆ’2
âˆ’3
2
ï£¶
ï£¸.
What if the right side of 7.21 had been
ï£«
ï£­
0
1
3
ï£¶
ï£¸?
What would be the solution to
ï£«
ï£­
1
0
1
1
âˆ’1
1
1
1
âˆ’1
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
1
3
ï£¶
ï£¸?
By the above discussion, it is just
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
1
2
1
2
1
âˆ’1
0
1
âˆ’1
2
âˆ’1
2
ï£¶
ï£¸
ï£«
ï£­
0
1
3
ï£¶
ï£¸=
ï£«
ï£­
2
âˆ’1
âˆ’2
ï£¶
ï£¸.
This illustrates why once you have found the inverse of a given matrix, you can use it to
solve many diï¬€erent systems easily.
Here is a formula for the inverse of a 2Ã—2 matrix.

138
MATRICES
Theorem 7.2.6 Let A =
Âµ a
b
c
d
Â¶
where ad âˆ’bc Ì¸= 0. Then
Aâˆ’1 =
1
ad âˆ’cb
Âµ
d
âˆ’b
âˆ’c
a
Â¶
.
Proof: Just multiply and verify it works.
1
ad âˆ’cb
Âµ
d
âˆ’b
âˆ’c
a
Â¶ Âµ
a
b
c
d
Â¶
=
Âµ
1
0
0
1
Â¶
.
Therefore, this is indeed the inverse.
The expression, adâˆ’cb is the determinant of the given matrix. Recall, this was discussed
in connection with the cross product. This will be discussed in more generality later.
7.3
Elementary Matrices 22 Sept.
Quiz
1. Here is a matrix.
ï£«
ï£­
1
âˆ’1
0
1
âˆ’1
2
âˆ’1
2
âˆ’1
3
2
1
2
ï£¶
ï£¸
Find its inverse.
2. The inverse of
ï£«
ï£­
1
1
2
1
2
0
1
2
1
2
1
âˆ’1
0
ï£¶
ï£¸is
ï£«
ï£­
1
âˆ’1
0
1
âˆ’1
âˆ’1
âˆ’1
3
1
ï£¶
ï£¸. Use this fact to write the so-
lution to the system
ï£«
ï£­
x + 1
2y + 1
2z = a
1
2y + 1
2z = b
x âˆ’y = c
ï£¶
ï£¸
in terms of a, b, c.
The elementary matrices result from doing a row operation to the identity matrix.
Deï¬nition 7.3.1 The row operations consist of the following
1. Switch two rows.
2. Multiply a row by a nonzero number.
3. Replace a row by a multiple of another row added to it.
The elementary matrices are given in the following deï¬nition.
Deï¬nition 7.3.2 The elementary matrices consist of those matrices which result by
applying a row operation to an identity matrix. Those which involve switching rows of the
identity are called permutation matrices1.
1More generally, a permutation matrix is a matrix which comes by permuting the rows of the identity
matrix, not just switching two rows.

7.3.
ELEMENTARY MATRICES 22 SEPT.
139
As an example of why these elementary matrices are interesting, consider the following.
ï£«
ï£­
0
1
0
1
0
0
0
0
1
ï£¶
ï£¸
ï£«
ï£­
a
b
c
d
x
y
z
w
f
g
h
i
ï£¶
ï£¸=
ï£«
ï£­
x
y
z
w
a
b
c
d
f
g
h
i
ï£¶
ï£¸
A 3Ã—4 matrix was multiplied on the left by an elementary matrix which was obtained from
row operation 1 applied to the identity matrix. This resulted in applying the operation 1
to the given matrix. This is what happens in general.
Now consider what these elementary matrices look like. First consider the one which
involves switching row i and row j where i < j. This matrix is of the form
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
0
...
...
...
1
...
0
Â· Â· Â·
0
0
Â· Â· Â·
Â· Â· Â·
0
1
Â· Â· Â·
Â· Â· Â·
0
...
...
1
0
Â· Â· Â·
0
...
...
...
...
...
...
0
Â· Â· Â·
Â· Â· Â·
0
Â· Â· Â·
0
1
0
Â· Â· Â·
Â· Â· Â·
0
0
Â· Â· Â·
0
1
0
Â· Â· Â·
Â· Â· Â·
0
Â· Â· Â·
Â· Â· Â·
0
...
1
...
...
...
0
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
The two exceptional rows are shown. The ith row was the jth and the jth row was the ith
in the identity matrix. Now consider what this does to a column vector.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
0
...
...
...
1
...
0
Â· Â· Â·
0
0
Â· Â· Â·
Â· Â· Â·
0
1
Â· Â· Â·
Â· Â· Â·
0
...
...
1
0
Â· Â· Â·
0
...
...
...
...
...
...
0
Â· Â· Â·
Â· Â· Â·
0
Â· Â· Â·
0
1
0
Â· Â· Â·
Â· Â· Â·
0
0
Â· Â· Â·
0
1
0
Â· Â· Â·
Â· Â· Â·
0
Â· Â· Â·
Â· Â· Â·
0
...
1
...
...
...
0
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
v1
...
...
vi
...
...
...
vj
...
...
vn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
v1
...
...
vj
...
...
...
vi
...
...
vn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
Now denote by P ij the elementary matrix which comes from the identity from switching
rows i and j. From what was just explained consider multiplication on the left by this

140
MATRICES
elementary matrix.
P ij
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a11
a12
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
a1p
...
...
...
ai1
ai2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
aip
...
...
...
aj1
aj2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
ajp
...
...
...
an1
an2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
anp
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
From the way you multiply matrices this is a matrix which has the indicated collumns.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
P ij
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a11
...
ai1
...
aj1
...
an1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
, P ij
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a12
...
ai2
...
aj2
...
an2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
, Â· Â· Â·, P ij
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a1p
...
aip
...
ajp
...
anp
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a11
...
aj1
...
ai1
...
an1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a12
...
aj2
...
ai2
...
an2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
, Â· Â· Â·,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a1p
...
ajp
...
aip
...
anp
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a11
a12
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
a1p
...
...
...
aj1
aj2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
ajp
...
...
...
ai1
ai2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
aip
...
...
...
an1
an2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
anp
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
This has established the following lemma.
Lemma 7.3.3 Let P ij denote the elementary matrix which involves switching the ith and
the jth rows. Then
P ijA = B
where B is obtained from A by switching the ith and the jth rows.
Next consider the row operation which involves multiplying the ith row by a nonzero
constant, c. The elementary matrix which results from applying this operation to the ith

7.3.
ELEMENTARY MATRICES 22 SEPT.
141
row of the identity matrix is of the form
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
0
...
...
...
1
...
...
c
...
...
1
...
...
...
0
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
Now consider what this does to a column vector.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
0
...
...
...
1
...
...
c
...
...
1
...
...
...
0
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
v1
...
viâˆ’1
vi
vi+1
...
vn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
v1
...
viâˆ’1
cvi
vi+1
...
vn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
Denote by E (c, i) this elementary matrix which multiplies the ith row of the identity by the
nonzero constant, c. Then from what was just discussed and the way matrices are multiplied,
E (c, i)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a11
a12
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
a1p
...
...
...
ai1
ai2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
aip
...
...
...
aj2
aj2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
ajp
...
...
...
an1
an2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
anp
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

142
MATRICES
equals a matrix having the columns indicated below.
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
E (c, i)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a11
...
ai1
...
aj1
...
an1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
, E (c, i)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a12
...
ai2
...
aj2
...
an2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
, Â· Â· Â·, E (c, i)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a1p
...
aip
...
ajp
...
anp
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a11
a12
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
a1p
...
...
...
cai1
cai2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
caip
...
...
...
aj2
aj2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
ajp
...
...
...
an1
an2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
anp
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
This proves the following lemma.
Lemma 7.3.4 Let E (c, i) denote the elementary matrix corresponding to the row oper-
ation in which the ith row is multiplied by the nonzero constant, c. Thus E (c, i) involves
multiplying the ith row of the identity matrix by c. Then
E (c, i) A = B
where B is obtained from A by multiplying the ith row of A by c.
Finally consider the third of these row operations. Denote by E (c Ã— i + j) the elementary
matrix which replaces the jth row with itself added to c times the ith row added to it. In
case i < j this will be of the form
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
0
0
...
...
...
1
...
...
...
...
...
...
c
Â· Â· Â·
1
...
...
...
0
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

7.3.
ELEMENTARY MATRICES 22 SEPT.
143
Now consider what this does to a column vector.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
0
0
...
...
...
1
...
...
...
...
...
...
c
Â· Â· Â·
1
...
...
...
0
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
v1
...
vi
...
vj
...
vn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
v1
...
vi
...
cvi + vj
...
vn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
Now from this and the way matrices are multiplied,
E (c Ã— i + j)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a11
a12
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
a1p
...
...
...
ai1
ai2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
aip
...
...
...
aj2
aj2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
ajp
...
...
...
an1
an2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
anp
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
equals a matrix of the following form having the indicated columns.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
E (c Ã— i + j)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a11
...
ai1
...
aj2
...
an1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
, E (c Ã— i + j)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a12
...
ai2
...
aj2
...
an2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
, Â· Â· Â·E (c Ã— i + j)
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a1p
...
aip
...
ajp
...
anp
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a11
a12
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
a1p
...
...
...
ai1
ai2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
aip
...
...
...
aj2 + cai1
aj2 + cai2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
ajp + caip
...
...
...
an1
an2
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
anp
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
The case where i > j is handled similarly. This proves the following lemma.
Lemma 7.3.5 Let E (c Ã— i + j) denote the elementary matrix obtained from I by replac-
ing the jth row with c times the ith row added to it. Then
E (c Ã— i + j) A = B
where B is obtained from A by replacing the jth row of A with itself added to c times the
ith row of A.

144
MATRICES
The next theorem is the main result.
Theorem 7.3.6 To perform any of the three row operations on a matrix, A it
suï¬ƒces to do the row operation on the identity matrix obtaining an elementary matrix, E
and then take the product, EA. Furthermore, each elementary matrix is invertible and its
inverse is an elementary matrix.
Proof: The ï¬rst part of this theorem has been proved in Lemmas 7.3.3 - 7.3.5.
It
only remains to verify the claim about the inverses. Consider ï¬rst the elementary matrices
corresponding to row operation of type three.
E (âˆ’c Ã— i + j) E (c Ã— i + j) = I
This follows because the ï¬rst matrix takes c times row i in the identity and adds it to row j.
When multiplied on the left by E (âˆ’c Ã— i + j) it follows from the ï¬rst part of this theorem
that you take the ith row of E (c Ã— i + j) which coincides with the ith row of I since that
row was not changed, multiply it by âˆ’c and add to the jth row of E (c Ã— i + j) which was
the jth row of I added to c times the ith row of I. Thus E (âˆ’c Ã— i + j) multiplied on the
left, undoes the row operation which resulted in E (c Ã— i + j). The same argument applied
to the product
E (c Ã— i + j) E (âˆ’c Ã— i + j)
replacing c with âˆ’c in the argument yields that this product is also equal to I. Therefore,
E (c Ã— i + j)âˆ’1 = E (âˆ’c Ã— i + j) .
Similar reasoning shows that for E (c, i) the elementary matrix which comes from mul-
tiplying the ith row by the nonzero constant, c,
E (c, i)âˆ’1 = E
Â¡
câˆ’1, i
Â¢
.
Finally, consider P ij which involves switching the ith and the jth rows.
P ijP ij = I
because by the ï¬rst part of this theorem, multiplying on the left by P ij switches the ith
and jth rows of P ij which was obtained from switching the ith and jth rows of the identity.
First you switch them to get P ij and then you multiply on the left by P ij which switches
these rows again and restores the identity matrix. Thus
Â¡
P ijÂ¢âˆ’1 = P ij.
Corollary 7.3.7 Let A be an invertible n Ã— n matrix. Then A equals a ï¬nite product of
elementary matrices.
Proof: Since Aâˆ’1 is given to exist, it follows A must have rank n and so the row reduced
echelon form of A is I. Therefore, by Theorem 7.3.6 there is a sequence of elementary
matrices, E1, Â· Â· Â·, Ep which accomplish successive row operations such that
(EpEpâˆ’1 Â· Â· Â· E1) A = I.
But now multiply on the left on both sides by Eâˆ’1
p
then by Eâˆ’1
pâˆ’1 and then by Eâˆ’1
pâˆ’2 etc.
until you get
A = Eâˆ’1
1 Eâˆ’1
2
Â· Â· Â· Eâˆ’1
pâˆ’1Eâˆ’1
p
and by Theorem 7.3.6 each of these in this product is an elementary matrix.

7.4.
BLOCK MULTIPLICATION OF MATRICES
145
7.4
Block Multiplication Of Matrices
Consider the following problem
Âµ
A
B
C
D
Â¶ Âµ
E
F
G
H
Â¶
You know how to do this. You get
Âµ
AE + BG
AF + BH
CE + DG
CF + DH
Â¶
.
Now what if instead of numbers, the entries, A, B, C, D, E, F, G are matrices of a size such
that the multiplications and additions needed in the above formula all make sense. Would
the formula be true in this case? I will show below that it is true.
Suppose A is a matrix of the form
ï£«
ï£¬
ï£­
A11
Â· Â· Â·
A1m
...
...
...
Ar1
Â· Â· Â·
Arm
ï£¶
ï£·
ï£¸
(7.22)
where Aij is a si Ã— pj matrix where si does not depend on j and pj does not depend on
i. Such a matrix is called a block matrix, also a partitioned matrix. Let n = P
j pj
and k = P
i si so A is an k Ã— n matrix. What is Ax where x âˆˆFn? From the process of
multiplying a matrix times a vector, the following lemma follows.
Lemma 7.4.1 Let A be an m Ã— n block matrix as in 7.22 and let x âˆˆFn. Then Ax is of
the form
Ax =
ï£«
ï£¬
ï£­
P
j A1jxj
...
P
j Arjxj
ï£¶
ï£·
ï£¸
where x = (x1, Â· Â· Â·, xm)T and xi âˆˆFpi.
Suppose also that B is a block matrix of the form
ï£«
ï£¬
ï£­
B11
Â· Â· Â·
B1p
...
...
...
Br1
Â· Â· Â·
Brp
ï£¶
ï£·
ï£¸
(7.23)
and A is a block matrix of the form
ï£«
ï£¬
ï£­
A11
Â· Â· Â·
A1m
...
...
...
Ap1
Â· Â· Â·
Apm
ï£¶
ï£·
ï£¸
(7.24)
and that for all i, j, it makes sense to multiply BisAsj for all s âˆˆ{1, Â· Â· Â·, m}. (That is the
two matrices, Bis and Asj are conformable.) and that for each s, BisAsj is the same size so
that it makes sense to write P
s BisAsj.
Theorem 7.4.2 Let B be a block matrix as in 7.23 and let A be a block matrix as
in 7.24 such that Bis is conformable with Asj and each product, BisAsj is of the same size
so they can be added. Then BA is a block matrix such that the ijth block is of the form
X
s
BisAsj.
(7.25)

146
MATRICES
Proof: Let Bis be a qi Ã— ps matrix and Asj be a ps Ã— rj matrix. Also let x âˆˆFn and
let x = (x1, Â· Â· Â·, xm)T and xi âˆˆFri so it makes sense to multiply Asjxj. Then from the
associative law of matrix multiplication and Lemma 7.4.1 applied twice,
ï£«
ï£¬
ï£­
ï£«
ï£¬
ï£­
B11
Â· Â· Â·
B1p
...
...
...
Br1
Â· Â· Â·
Brp
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
A11
Â· Â· Â·
A1m
...
...
...
Ap1
Â· Â· Â·
Apm
ï£¶
ï£·
ï£¸
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
x1
...
xm
ï£¶
ï£·
ï£¸
=
ï£«
ï£¬
ï£­
B11
Â· Â· Â·
B1p
...
...
...
Br1
Â· Â· Â·
Brp
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
P
j A1jxj
...
P
j Arjxj
ï£¶
ï£·
ï£¸
=
ï£«
ï£¬
ï£­
P
s
P
j B1sAsjxj
...
P
s
P
j BrsAsjxj
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
P
j (P
s B1sAsj) xj
...
P
j (P
s BrsAsj) xj
ï£¶
ï£·
ï£¸
=
ï£«
ï£¬
ï£­
P
s B1sAs1
Â· Â· Â·
P
s B1sAsm
...
...
...
P
s BrsAs1
Â· Â· Â·
P
s BrsAsm
ï£¶
ï£·
ï£¸
ï£«
ï£¬
ï£­
x1
...
xm
ï£¶
ï£·
ï£¸
By Lemma 7.4.1, this shows that (BA) x equals the block matrix whose ijth entry is given
by 7.25 times x. Since x is an arbitrary vector in Fn, this proves the theorem.
The message of this theorem is that you can formally multiply block matrices as though
the blocks were numbers. You just have to pay attention to the preservation of order.
7.4.1
Exercises With Answers
1. Here are some matrices:
A
=
ï£«
ï£­
1
2
3
2
3
7
1
0
1
ï£¶
ï£¸, B =
Âµ
3
âˆ’1
2
âˆ’3
2
1
Â¶
,
C
=
ï£«
ï£­
1
2
3
1
1
1
ï£¶
ï£¸, D =
Âµ
âˆ’1
2
2
âˆ’3
Â¶
, E =
Âµ
2
3
Â¶
.
Find if possible âˆ’3A, 3B âˆ’A, AC, CB, EA, DCT If it is not possible explain why.
âˆ’3A = âˆ’3
ï£«
ï£­
1
2
3
2
3
7
1
0
1
ï£¶
ï£¸=
ï£«
ï£­
âˆ’3
âˆ’6
âˆ’9
âˆ’6
âˆ’9
âˆ’21
âˆ’3
0
âˆ’3
ï£¶
ï£¸
3B âˆ’A is nonsense because the matrices B and A are not of the same size.
AC =
ï£«
ï£­
1
2
3
2
3
7
1
0
1
ï£¶
ï£¸
ï£«
ï£­
1
2
3
1
1
1
ï£¶
ï£¸=
ï£«
ï£­
10
7
18
14
2
3
ï£¶
ï£¸
There is no problem here because you are doing (3 Ã— 3) (3 Ã— 2) .
CB =
ï£«
ï£­
1
2
3
1
1
1
ï£¶
ï£¸
Âµ
3
âˆ’1
2
âˆ’3
2
1
Â¶
=
ï£«
ï£­
âˆ’3
3
4
6
âˆ’1
7
0
1
3
ï£¶
ï£¸

7.4.
BLOCK MULTIPLICATION OF MATRICES
147
There is no problem here because you are doing (3 Ã— 2) (2 Ã— 3) and the inside numbers
match. EA is nonsense because it is of the form (2 Ã— 1) (3 Ã— 3) so since the inside
numbers do not match the matrices are not conformable.
DCT =
Âµ âˆ’1
2
2
âˆ’3
Â¶ Âµ 1
3
1
2
1
1
Â¶
=
Âµ
3
âˆ’1
1
âˆ’4
3
âˆ’1
Â¶
.
2. Let A =
Âµ
0
2
3
4
Â¶
, B =
Âµ
1
2
1
k
Â¶
. Is it possible to choose k such that AB = BA?
If so, what should k equal?
We just multiply and see if it can happen.
AB =
Âµ
0
2
3
4
Â¶ Âµ
1
2
1
k
Â¶
=
Âµ
2
2k
7
6 + 4k
Â¶
.
On the other hand,
BA =
Âµ
1
2
1
k
Â¶ Âµ
0
2
3
4
Â¶
=
Âµ
6
10
3k
2 + 4k
Â¶
.
If these were equal you would need to have 6 = 2 which is not the case. Therefore,
there is no way to choose k such that these two matrices will commute.
3. Let x = (âˆ’1, 0, 3) and y = (3, 1, 2) . Find xT y.
xT y =
ï£«
ï£­
âˆ’1
0
3
ï£¶
ï£¸Â¡
3
1
2
Â¢
=
ï£«
ï£­
âˆ’3
âˆ’1
âˆ’2
0
0
0
9
3
6
ï£¶
ï£¸.
4. Write
ï£«
ï£¬
ï£¬
ï£­
4x1 âˆ’x2 + 2x3
2x3 + 7x1
2x3
3x3 + 3x2 + x1
ï£¶
ï£·
ï£·
ï£¸in the form A
ï£«
ï£¬
ï£¬
ï£­
x1
x2
x3
x4
ï£¶
ï£·
ï£·
ï£¸where A is an appropriate matrix.
ï£«
ï£¬
ï£¬
ï£­
4
âˆ’1
2
0
7
0
2
0
0
0
2
0
1
3
3
0
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
x1
x2
x3
x4
ï£¶
ï£·
ï£·
ï£¸.
5. Let
A =
ï£«
ï£­
1
2
5
2
1
4
1
0
2
ï£¶
ï£¸.
Find Aâˆ’1 if possible. If Aâˆ’1 does not exist, determine why.
ï£«
ï£­
1
2
5
2
1
4
1
0
2
ï£¶
ï£¸
âˆ’1
=
ï£«
ï£­
âˆ’2
3
4
3
âˆ’1
0
1
âˆ’2
1
3
âˆ’2
3
1
ï£¶
ï£¸.
6. Let
A =
ï£«
ï£¬
ï£¬
ï£­
1
2
0
2
1
5
2
0
2
1
âˆ’3
2
1
2
1
2
ï£¶
ï£·
ï£·
ï£¸

148
MATRICES
Find Aâˆ’1 if possible. If Aâˆ’1 does not exist, determine why.
ï£«
ï£¬
ï£¬
ï£­
1
2
0
2
1
5
2
0
2
1
âˆ’3
2
1
2
1
2
ï£¶
ï£·
ï£·
ï£¸
âˆ’1
=
ï£«
ï£¬
ï£¬
ï£­
âˆ’3
1
6
5
6
13
6
1
1
6
âˆ’1
6
âˆ’5
6
âˆ’1
0
0
1
1
âˆ’1
4
âˆ’1
4
âˆ’1
4
ï£¶
ï£·
ï£·
ï£¸.
7. Show that if Aâˆ’1 exists for an n Ã— n matrix, then it is unique. That is, if BA = I and
AB = I, then B = Aâˆ’1.
From AB = I, multiply both sides by Aâˆ’1. Thus Aâˆ’1 (AB) = Aâˆ’1. Then from the
associative property of matrix multiplication, Aâˆ’1 = Aâˆ’1 (AB) =
Â¡
Aâˆ’1A
Â¢
B = IB =
B.
8. Suppose A, B are two matrices. Show (AB)âˆ’1 = Bâˆ’1Aâˆ’1.
All you have to do is multiply it. If it acts like the inverse, it is the inverse.
Â¡
Bâˆ’1Aâˆ’1Â¢
(AB) = Bâˆ’1 Â¡
Aâˆ’1A
Â¢
B = Bâˆ’1IB = Bâˆ’1B = I.
Therefore, Bâˆ’1Aâˆ’1 = (AB)âˆ’1 .
9. Show
Â¡
Aâˆ’1Â¢T =
Â¡
AT Â¢âˆ’1.
AT Â¡
Aâˆ’1Â¢T =
Â¡
Aâˆ’1A
Â¢T = IT = I and so
Â¡
Aâˆ’1Â¢T =
Â¡
AT Â¢âˆ’1.
10. Here are elementary matrices. Find their inverses.
(a)
ï£«
ï£­
1
0
0
a
1
0
0
0
1
ï£¶
ï£¸
ï£«
ï£­
1
0
0
a
1
0
0
0
1
ï£¶
ï£¸
âˆ’1
=
ï£«
ï£­
1
0
0
âˆ’a
1
0
0
0
1
ï£¶
ï£¸
(b)
ï£«
ï£­
1
0
0
0
2
0
0
0
1
ï£¶
ï£¸
ï£«
ï£­
1
0
0
0
2
0
0
0
1
ï£¶
ï£¸
âˆ’1
=
ï£«
ï£­
1
0
0
0
1
2
0
0
0
1
ï£¶
ï£¸
(c)
ï£«
ï£­
0
1
0
1
0
0
0
0
1
ï£¶
ï£¸
ï£«
ï£­
0
1
0
1
0
0
0
0
1
ï£¶
ï£¸
âˆ’1
=
ï£«
ï£­
0
1
0
1
0
0
0
0
1
ï£¶
ï£¸
11. When you have an n Ã— n matrix, A, An =
n times
z
}|
{
A Ã— A Ã— Â· Â· Â· Ã— A. If Aâˆ’1 exists, show
Â¡
Aâˆ’1Â¢n = (An)âˆ’1.

7.4.
BLOCK MULTIPLICATION OF MATRICES
149
The equation is true if n = 1. Suppose it is true for n. Then by the induction hypoth-
esis,
Â¡
Aâˆ’1Â¢n+1
=
Â¡
Aâˆ’1Â¢n Aâˆ’1 = (An)âˆ’1 Aâˆ’1
=
(A (An))âˆ’1 =
Â¡
An+1Â¢âˆ’1 .

150
MATRICES

Part IV
LU Decomposition, Subspaces,
Linear Transformations
151


153
Outcomes
A. Find the LU factorization of a matrix.
B. Use the LU factorization of a matrix to solve a system of linear equations.
*C. Find the PT LU factorization of a matrix.
*D. Use that PT LU factorization of a matrix to solve a system of linear equations.
*E. Find the inverse of a matrix using the LU factorization.
Reading: Linear Algebra 3.4
Outcome Mapping:
A. 7-12,13-14
B. 1-6
*C. 19-22,23-25
*D. 27-28
*E. 15-18,30
A. Deï¬ne subspace of Rn.
Determine whether or not a given set of vectors forms a
subspace of Rn.
B. Deï¬ne row space, column space, and null space for a matrix. Determine whether or
not a given vector is in one of these spaces.
C. Deï¬ne basis and dimension. Given a subspace, determine its dimension and a basis.
Verify whether or not a given set of vectors is a basis for the subspace.
D. Deï¬ne rank and nullity. Determine the rank and nullity of a given matrix.
*E. Prove and recall theorems involving the rank, nullity, and invertibility of matrices.
*F. Find the coordinates of a vector with respect to a given basis.
Reading: Linear Algebra 3.5
Outcome Mapping:
A. 1-10
B. 11-16
C. 17-20,21-26,27-30,31-34,45-48
D. 35-42,43-44
*E. 55-64
*F. 49-50
A. Deï¬ne linear transformation.
Determine whether or not a given transformation is
linear.

154
B. Determine the matrix that represents a given linear transformation of vectors.
C. Prove and recall theorems involving linear transformations.
*D. Find compositions and inverses of linear transformations.
Reading: Linear Algebra 3.6
Outcome Mapping:
A. 1-10,46-51
B. 11-14,15-28
C. 29,40-45,52-55
*D. 30-35,36-39

The LU Factorization 25 Sept.
8.0.2
Deï¬nition Of An LU Decomposition
An LU decomposition of a matrix involves writing the given matrix as the product of a
lower triangular matrix which has the main diagonal consisting entirely of ones L, and an
upper triangular matrix U in the indicated order. This is the version discussed here but it
is sometimes the case that the L has numbers other than 1 down the main diagonal. It is
still a useful concept. The L goes with â€œlowerâ€ and the U with â€œupperâ€. It turns out many
matrices can be written in this way and when this is possible, people get excited about slick
ways of solving the system of equations, Ax = y. It is for this reason that you want to study
the LU decomposition. It allows you to work only with triangular matrices. It turns out
that it takes about 2n3/3 operations to use Gauss elimination but only n3/3 to obtain an
LU factorization.
First it should be noted not all matrices have an LU decomposition and so we will
emphasize the techniques for achieving it rather than formal proofs.
Example 8.0.3 Can you write
Âµ
0
1
1
0
Â¶
in the form LU as just described?
To do so you would need
Âµ
1
0
x
1
Â¶ Âµ
a
b
0
c
Â¶
=
Âµ
a
b
xa
xb + c
Â¶
=
Âµ
0
1
1
0
Â¶
.
Therefore, b = 1 and a = 0. Also, from the bottom rows, xa = 1 which canâ€™t happen
and have a = 0. Therefore, you canâ€™t write this matrix in the form LU. It has no LU
decomposition. This is what we mean above by saying the method lacks generality.
8.0.3
Finding An LU Decomposition By Inspection
Which matrices have an LU decomposition? It turns out it is those whose row reduced
echelon form can be achieved without switching rows and which only involve row operations
of type 3 in which row j is replaced with a multiple of row i added to row j for i < j.
Example 8.0.4 Find an LU decomposition of A =
ï£«
ï£­
1
2
0
2
1
3
2
1
2
3
4
0
ï£¶
ï£¸.
One way to ï¬nd the LU decomposition is to simply look for it directly. You need
ï£«
ï£­
1
2
0
2
1
3
2
1
2
3
4
0
ï£¶
ï£¸=
ï£«
ï£­
1
0
0
x
1
0
y
z
1
ï£¶
ï£¸
ï£«
ï£­
a
d
h
j
0
b
e
i
0
0
c
f
ï£¶
ï£¸.
155

156
THE LU FACTORIZATION 25 SEPT.
Then multiplying these you get
ï£«
ï£­
a
d
h
j
xa
xd + b
xh + e
xj + i
ya
yd + zb
yh + ze + c
yj + iz + f
ï£¶
ï£¸
and so you can now tell what the various quantities equal. From the ï¬rst column, you
need a = 1, x = 1, y = 2. Now go to the second column. You need d = 2, xd + b = 3 so
b = 1, yd + zb = 3 so z = âˆ’1. From the third column, h = 0, e = 2, c = 6. Now from the
fourth column, j = 2, i = âˆ’1, f = âˆ’5. Therefore, an LU decomposition is
ï£«
ï£­
1
0
0
1
1
0
2
âˆ’1
1
ï£¶
ï£¸
ï£«
ï£­
1
2
0
2
0
1
2
âˆ’1
0
0
6
âˆ’5
ï£¶
ï£¸.
You can check whether you got it right by simply multiplying these two.
8.0.4
Using Multipliers To Find An LU Decomposition
There is also a convenient procedure for ï¬nding an LU decomposition. It turns out that it
is only necessary to keep track of the multipliers which are used to row reduce to upper
triangular form. This procedure is described in the following examples.
Example 8.0.5 Find an LU decomposition for A =
ï£«
ï£­
1
2
3
2
1
âˆ’4
1
5
2
ï£¶
ï£¸
Write the matrix next to the identity matrix as shown.
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
ï£«
ï£­
1
2
3
2
1
âˆ’4
1
5
2
ï£¶
ï£¸.
The process involves doing row operations to the matrix on the right while simultaneously
updating successive columns of the matrix on the left. First take âˆ’2 times the ï¬rst row and
add to the second in the matrix on the right.
ï£«
ï£­
1
0
0
2
1
0
0
0
1
ï£¶
ï£¸
ï£«
ï£­
1
2
3
0
âˆ’3
âˆ’10
1
5
2
ï£¶
ï£¸
Note the way we updated the matrix on the left. We put a 2 in the second entry of the ï¬rst
column because we used âˆ’2 times the ï¬rst row added to the second row. Now replace the
third row in the matrix on the right by âˆ’1 times the ï¬rst row added to the third. Thus the
next step is
ï£«
ï£­
1
0
0
2
1
0
1
0
1
ï£¶
ï£¸
ï£«
ï£­
1
2
3
0
âˆ’3
âˆ’10
0
3
âˆ’1
ï£¶
ï£¸
Finally, we will add the second row to the bottom row and make the following changes
ï£«
ï£­
1
0
0
2
1
0
1
âˆ’1
1
ï£¶
ï£¸
ï£«
ï£­
1
2
3
0
âˆ’3
âˆ’10
0
0
âˆ’11
ï£¶
ï£¸.
At this point, we stop because the matrix on the right is upper triangular. An LU decom-
position is the above.
The justiï¬cation for this gimmick is in my linear algebra book on the web.

157
Example 8.0.6 Find an LU decomposition for A =
ï£«
ï£¬
ï£¬
ï£­
1
2
1
2
1
2
0
2
1
1
2
3
1
3
2
1
0
1
1
2
ï£¶
ï£·
ï£·
ï£¸.
We will use the same procedure as above. However, this time we will do everything for
one column at a time. First multiply the ï¬rst row by (âˆ’1) and then add to the last row.
Next take (âˆ’2) times the ï¬rst and add to the second and then (âˆ’2) times the ï¬rst and add
to the third.
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
2
1
0
0
2
0
1
0
1
0
0
1
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
1
2
1
2
1
0
âˆ’4
0
âˆ’3
âˆ’1
0
âˆ’1
âˆ’1
âˆ’1
0
0
âˆ’2
0
âˆ’1
1
ï£¶
ï£·
ï£·
ï£¸.
This ï¬nishes the ï¬rst column of L and the ï¬rst column of U. Now take âˆ’(1/4) times the
second row in the matrix on the right and add to the third followed by âˆ’(1/2) times the
second added to the last.
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
2
1
0
0
2
1/4
1
0
1
1/2
0
1
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
1
2
1
2
1
0
âˆ’4
0
âˆ’3
âˆ’1
0
0
âˆ’1
âˆ’1/4
1/4
0
0
0
1/2
3/2
ï£¶
ï£·
ï£·
ï£¸
This ï¬nishes the second column of L as well as the second column of U. Since the matrix
on the right is upper triangular, stop. The LU decomposition has now been obtained. This
technique is called Dolittleâ€™s method.
This process is entirely typical of the general case. The matrix U is just the ï¬rst upper
triangular matrix you come to in your quest for the row reduced echelon form using only
the row operation which involves replacing a row by itself added to a multiple of another
row. The matrix, L is what you get by updating the identity matrix as illustrated above.
You should note that for a square matrix, the number of row operations necessary to
reduce to LU form is about half the number needed to place the matrix in row reduced
echelon form. This is why an LU decomposition is of interest in solving systems of equations.
8.0.5
Solving Systems Using The LU Decomposition
The reason people care about the LU decomposition is it allows the quick solution of systems
of equations. Here is an example.
Example 8.0.7 Suppose you want to ï¬nd the solutions to
ï£«
ï£­
1
2
3
2
4
3
1
1
1
2
3
0
ï£¶
ï£¸
ï£«
ï£¬
ï£¬
ï£­
x
y
z
w
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£­
1
2
3
ï£¶
ï£¸.
Of course one way is to write the augmented matrix and grind away. However, this
involves more row operations than the computation of the LU decomposition and it turns
out that the LU decomposition can give the solution quickly. Here is how. The following is
an LU decomposition for the matrix.
ï£«
ï£­
1
2
3
2
4
3
1
1
1
2
3
0
ï£¶
ï£¸=
ï£«
ï£­
1
0
0
4
1
0
1
0
1
ï£¶
ï£¸
ï£«
ï£­
1
2
3
2
0
âˆ’5
âˆ’11
âˆ’7
0
0
0
âˆ’2
ï£¶
ï£¸.

158
THE LU FACTORIZATION 25 SEPT.
Let Ux = y and consider Ly = b where in this case, b = (1, 2, 3)T . Thus
ï£«
ï£­
1
0
0
4
1
0
1
0
1
ï£¶
ï£¸
ï£«
ï£­
y1
y2
y3
ï£¶
ï£¸=
ï£«
ï£­
1
2
3
ï£¶
ï£¸
which yields very quickly that y =
ï£«
ï£­
1
âˆ’2
2
ï£¶
ï£¸. Now you can ï¬nd x by solving Ux = y. Thus
in this case,
ï£«
ï£­
1
2
3
2
0
âˆ’5
âˆ’11
âˆ’7
0
0
0
âˆ’2
ï£¶
ï£¸
ï£«
ï£¬
ï£¬
ï£­
x
y
z
w
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£­
1
âˆ’2
2
ï£¶
ï£¸
which yields
x =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’3
5 + 7
5t
9
5 âˆ’11
5 t
t
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
, t âˆˆR.

Rank Of A Matrix 26,27 Sept.
Quiz
1. Here is a matrix. Find an LU factorization.
ï£«
ï£­
1
2
0
3
âˆ’2
âˆ’3
âˆ’4
1
âˆ’1
2
1
1
ï£¶
ï£¸
2. An LU factorization for
ï£«
ï£­
1
2
0
3
âˆ’1
âˆ’4
âˆ’4
1
âˆ’1
âˆ’5
1
6
ï£¶
ï£¸is
ï£«
ï£­
1
0
0
âˆ’1
1
0
âˆ’1
3
2
1
ï£¶
ï£¸
ï£«
ï£­
1
2
0
3
0
âˆ’2
âˆ’4
4
0
0
7
3
ï£¶
ï£¸
Use this to solve the system
ï£«
ï£­
1
2
0
3
âˆ’1
âˆ’4
âˆ’4
1
âˆ’1
âˆ’5
1
6
ï£¶
ï£¸
ï£«
ï£¬
ï£¬
ï£­
x
y
z
w
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸
Show your work. You must use the LU factorization to receive any credit.
9.1
The Row Reduced Echelon Form Of A Matrix
Recall the row operations used to solve a system of equations which were presented earlier.
Deï¬nition 9.1.1 The row operations consist of the following
1. Switch two rows.
2. Multiply a row by a nonzero number.
3. Replace a row by a multiple of another row added to itself.
Recall that putting a matrix in row reduced echelon form involves doing row operations
as described on Page 88. In this section we review the description of the row reduced echelon
form and prove the row reduced echelon form for a given matrix is unique. That is, every
matrix can be row reduced to a unique row reduced echelon form. Of course this is not true
159

160
RANK OF A MATRIX 26,27 SEPT.
of the echelon form. The signiï¬cance of this is that it becomes possible to use the deï¬nite
article in referring to the row reduced echelon form and hence important conclusions about
the original matrix may be logically deduced from an examination of its unique row reduced
echelon form. Also recall the deï¬nition of linear combination and span.
Deï¬nition 9.1.2 Let v1, Â· Â· Â·, vk, u be vectors. Then u is said to be a linear com-
bination of the vectors {v1, Â· Â· Â·, vk} if there exist scalars, c1, Â· Â· Â·, ck such that
u =
k
X
i=1
civi.
The collection of all linear combinations of the vectors, {v1, Â· Â· Â·, vk} is known as the span
of these vectors and is written as span (v1, Â· Â· Â·, vk).
Another way to say the same thing as expressed in the earlier deï¬nition of row reduced
echelon form found on Page 86 is the following which is a more useful description when
proving the major assertions about the row reduced echelon form.
Deï¬nition 9.1.3 Let ei denote the column vector which has all zero entries except
for the ith slot which is one. An m Ã— n matrix is said to be in row reduced echelon form
if, in viewing successive columns from left to right, the ï¬rst nonzero column encountered is
e1 and if you have encountered e1, e2, Â· Â· Â·, ek, the next column is either ek+1 or is a linear
combination of the vectors, e1, e2, Â· Â· Â·, ek.
Theorem 9.1.4 Let A be an m Ã— n matrix. Then A has a row reduced echelon
form determined by a simple process.
Proof: Viewing the columns of A from left to right take the ï¬rst nonzero column. Pick
a nonzero entry in this column and switch the row containing this entry with the top row of
A. Now divide this new top row by the value of this nonzero entry to get a 1 in this position
and then use row operations to make all entries below this entry equal to zero. Thus the
ï¬rst nonzero column is now e1. Denote the resulting matrix by A1. It has been obtained
from A through a sequence of row operations.
Consider the submatrix of A1 to the right of this column and below the ï¬rst row. Do
exactly the same thing for this submatrix that was done for A. This time the e1 will refer to
Fmâˆ’1. Use the ï¬rst 1 obtained by the above process which is in the top row of this submatrix
and row operations to zero out every entry above it in the rows of A1. Call the resulting
matrix, A2. Thus A2 satisï¬es the conditions of the above deï¬nition up to the column just
encountered. Continue this way till every column has been dealt with and the result must
be in row reduced echelon form. This proves the theorem.
The following diagram illustrates the above procedure. Say the matrix looked something
like the following.
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
...
...
...
...
...
...
...
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸
First step would yield something like
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
1
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
0
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
...
...
...
...
...
...
...
0
0
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸

9.1.
THE ROW REDUCED ECHELON FORM OF A MATRIX
161
For the second step you look at the lower right corner as described,
ï£«
ï£¬
ï£­
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
...
...
...
...
...
âˆ—
âˆ—
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£¸
and if the ï¬rst column consists of all zeros but the next one is not all zeros, you would get
something like this.
ï£«
ï£¬
ï£­
0
1
âˆ—
âˆ—
âˆ—
...
...
...
...
...
0
0
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£¸
Thus, after zeroing out the term in the top row above the 1, you get the following for the
next step in the computation of the row reduced echelon form for the original matrix.
ï£«
ï£¬
ï£¬
ï£¬
ï£­
0
1
âˆ—
0
âˆ—
âˆ—
âˆ—
0
0
0
1
âˆ—
âˆ—
âˆ—
...
...
...
...
...
...
...
0
0
0
0
âˆ—
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£¸.
Next you look at the lower right matrix below the top two rows and to the right of the ï¬rst
four columns and repeat the process.
Recall the following deï¬nition which was discussed earlier.
Deï¬nition 9.1.5 The ï¬rst pivot column of A is the ï¬rst nonzero column of A.
The next pivot column is the ï¬rst column after this which becomes e2 in the row reduced
echelon form. The third is the next column which becomes e3 in the row reduced echelon
form and so forth.
There are three choices for row operations at each step in the above theorem. A natural
question is whether the same row reduced echelon matrix always results in the end from
following the above algorithm applied in any way. The next corollary says this is the case.
To prove this corollary, the following fundamental lemma will be used.
In rough terms, this lemma states that linear relationships between columns in a
matrix are preserved by row operations.
Lemma 9.1.6 Let A and B be two m Ã— n matrices and suppose B results from a row
operation applied to A. Then the kth column of B is a linear combination of the i1, Â· Â· Â·, ir
columns of B if and only if the kth column of A is a linear combination of the i1, Â· Â· Â·, ir
columns of A. Furthermore, the scalars in the linear combination are the same. (The linear
relationship between the kth column of A and the i1, Â· Â· Â·, ir columns of A is the same as the
linear relationship between the kth column of B and the i1, Â· Â· Â·, ir columns of B.)
Proof: Let A equal the following matrix in which the ak are the columns
Â¡
a1
a2
Â· Â· Â·
an
Â¢
and let B equal the following matrix in which the columns are given by the bk
Â¡
b1
b2
Â· Â· Â·
bn
Â¢
Then by Theorem 7.3.6 on Page 144 bk = Eak where E is an elementary matrix. Suppose
then that one of the columns of A is a linear combination of some other columns of A. Say
ak =
X
râˆˆS
crar.

162
RANK OF A MATRIX 26,27 SEPT.
Then multiplying by E,
bk = Eak =
X
râˆˆS
crEar =
X
râˆˆS
crbr.
This proves the lemma.
Deï¬nition 9.1.7 Two matrices are said to be row equivalent if one can be ob-
tained from the other by a sequence of row operations.
It has been shown above in Theorem 9.1.4 that every matrix is row equivalent to one
which is in row reduced echelon form.
Corollary 9.1.8 The row reduced echelon form is unique. That is if B, C are two ma-
trices in row reduced echelon form and both are row equivalent to A, then B = C.
Proof: Suppose B and C are both row reduced echelon forms for the matrix, A. Then
they clearly have the same zero columns since row operations leave zero columns unchanged.
If B has the sequence e1, e2, Â· Â· Â·, er occuring for the ï¬rst time in the positions, i1, i2, Â· Â· Â·, ir
the description of the row reduced echelon form means that if bk is the kth column of B such
that ijâˆ’1 < k < ij then bk is a linear combination of the columns in positions i1, i2, Â·Â·Â·, ikâˆ’1.
By Lemma 9.1.6 the same is true for ck, the kth column of C. Therefore, ck is not equal
to ej for any j because ej is not obtained as a linear combinations of the ei for i < j. It
follows the ej for C can only occur in positions i1, i2, Â· Â· Â·, ir. Furthermore, position ij in C
must contain ej because if not, then cij would be a linear combination of e1, Â· Â· Â·, ejâˆ’1 in C
but not in B, thus contradicting Lemma 9.1.6. Therefore, both B and C have the sequence
e1, e2, Â· Â· Â·, er occuring for the ï¬rst time in the positions, i1, i2, Â· Â· Â·, ir. By Lemma 9.1.6, the
columns between the ik and ik+1 position are linear combinations involving the same scalars
of the columns in the i1, Â· Â· Â·, ik position. This is equivalent to the assertion that each of
these columns is identical and this proves the corollary.
Example 9.1.9 Find the row reduced echelon form of the matrix,
ï£«
ï£­
0
0
2
3
0
2
0
1
0
1
1
5
ï£¶
ï£¸
The ï¬rst nonzero column is the second in the matrix. Switch the third and ï¬rst rows to
obtain
ï£«
ï£­
0
1
1
5
0
2
0
1
0
0
2
3
ï£¶
ï£¸
Now we multiply the top row by âˆ’2 and add to the second.
ï£«
ï£­
0
1
1
5
0
0
âˆ’2
âˆ’9
0
0
2
3
ï£¶
ï£¸
Next, add the second row to the bottom and then divide the bottom row by âˆ’6
ï£«
ï£­
0
1
1
5
0
0
âˆ’2
âˆ’9
0
0
0
1
ï£¶
ï£¸

9.2.
THE RANK OF A MATRIX
163
Next use the bottom row to obtain zeros in the last column above the 1 and divide the
second row by âˆ’2
ï£«
ï£­
0
1
1
0
0
0
1
0
0
0
0
1
ï£¶
ï£¸
Finally, add âˆ’1 times the middle row to the top.
ï£«
ï£­
0
1
0
0
0
0
1
0
0
0
0
1
ï£¶
ï£¸.
This is in row reduced echelon form.
Example 9.1.10 Find the row reduced echelon form for the matrix,
ï£«
ï£­
1
2
0
2
âˆ’1
3
4
3
0
5
4
5
ï£¶
ï£¸
You should verify that the row reduced echelon form is
ï£«
ï£­
1
0
âˆ’8
5
0
0
1
4
5
1
0
0
0
0
ï£¶
ï£¸.
9.2
The Rank Of A Matrix
9.2.1
The Deï¬nition Of Rank
To begin, here is a deï¬nition to introduce some terminology.
Deï¬nition 9.2.1 Let A be an mÃ—n matrix. The column space of A is the subspace
of Fm spanned by the columns. The row space is the subspace of Fn spanned by the rows.
Earlier the rank was deï¬ned to be the number of nonzero rows in the row reduced echelon
form. This is ï¬ne. However, it is useful to tie this in to the notion of spans of columns and
rows.
Deï¬nition 9.2.2 The row space of a matrix, A is the span of the rows, denoted as
row (A) and the column space of a matrix is the span of the columns, denoted as col (A).
The row rank of a matrix is the number of nonzero rows in the row reduced echelon form
and the column rank is the number of columns in the row reduced echelon form which are
one of the ek. Thus the column rank equals the number of pivot columns. Thus the row rank
equals the column rank. This is also called the rank of the matrix. The rank of a matrix, A
is denoted by rank (A) .
Example 9.2.3 Consider the matrix,
Âµ 1
2
3
2
4
6
Â¶
What is its rank?

164
RANK OF A MATRIX 26,27 SEPT.
The row reduced echelon form is
Âµ 1
2
3
0
0
0
Â¶
. There is one pivot column so the column
rank is 1. There is one nonzero row so the row rank is 1.
Example 9.2.4 Find the rank of the matrix,
ï£«
ï£¬
ï£¬
ï£­
1
2
1
3
0
âˆ’4
3
2
1
2
3
2
1
6
5
4
âˆ’3
âˆ’2
1
7
ï£¶
ï£·
ï£·
ï£¸.
From the above deï¬nition, all you have to do is ï¬nd the row reduced echelon form and
then count up the number of nonzero rows.
But the row reduced echelon form of this
matrix is
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
âˆ’17
4
0
1
0
0
1
0
0
1
0
âˆ’45
4
0
0
0
1
9
2
ï£¶
ï£·
ï£·
ï£¸
and so the rank of this matrix is 4.
Example 9.2.5 Find the rank of the matrix
ï£«
ï£¬
ï£¬
ï£­
1
2
1
3
0
âˆ’4
3
2
1
2
3
2
1
6
5
0
7
4
10
7
ï£¶
ï£·
ï£·
ï£¸
The row reduced echelon form is
ï£«
ï£¬
ï£¬
ï£­
1
0
0
3
2
5
2
0
1
0
âˆ’4
âˆ’17
0
0
1
19
2
63
2
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£¸
and so this time the rank is 3.
9.2.2
Finding The Row And Column Space Of A Matrix
The row reduced echelon form also can be used to obtain an eï¬ƒcient description of the row
and column space of a matrix. Of course you can get the column space by simply saying
that it equals the span of all the columns but often you can get the column space as the
span of fewer columns than this. This is what we mean by an â€œeï¬ƒcient descriptionâ€. This
is illustrated in the next example.
Example 9.2.6 Find the rank of the following matrix and describe the column and row
spaces eï¬ƒciently.
ï£«
ï£­
1
2
1
3
2
1
3
6
0
2
3
7
8
6
6
ï£¶
ï£¸
(9.1)
The row reduced echelon form is
ï£«
ï£­
1
0
âˆ’9
9
2
0
1
5
âˆ’3
0
0
0
0
0
0
ï£¶
ï£¸.

9.2.
THE RANK OF A MATRIX
165
Therefore, the rank of this matrix equals 2. All columns of this row reduced echelon form
are in
span
ï£«
ï£­
ï£«
ï£­
1
0
0
ï£¶
ï£¸,
ï£«
ï£­
0
1
0
ï£¶
ï£¸
ï£¶
ï£¸.
For example,
ï£«
ï£­
âˆ’9
5
0
ï£¶
ï£¸= âˆ’9
ï£«
ï£­
1
0
0
ï£¶
ï£¸+ 5
ï£«
ï£­
0
1
0
ï£¶
ï£¸.
By Lemma 9.1.6, all columns of the original matrix, are similarly contained in the span
of the ï¬rst two columns of that matrix. For example, consider the third column of the
original matrix.
ï£«
ï£­
1
6
8
ï£¶
ï£¸= âˆ’9
ï£«
ï£­
1
1
3
ï£¶
ï£¸+ 5
ï£«
ï£­
2
3
7
ï£¶
ï£¸.
How did I know to use âˆ’9 and 5 for the coeï¬ƒcients? This is what Lemma 9.1.6 says! It says
linear relationships are all preserved. Therefore, the column space of the original matrix
equals the span of the ï¬rst two columns. This is the desired eï¬ƒcient description of the
column space.
What about an eï¬ƒcient description of the row space? When row operations are used, the
resulting vectors remain in the row space. Thus the rows in the row reduced echelon form
are in the row space of the original matrix. Furthermore, by reversing the row operations,
each row of the original matrix can be obtained as a linear combination of the rows in the
row reduced echelon form. It follows that the span of the nonzero rows in the row reduced
echelon equals the span of the original rows. In the above example, the row space equals
the span of the two vectors,
Â¡
1
0
âˆ’9
9
2
Â¢
and
Â¡
0
1
5
âˆ’3
0
Â¢
.
Example 9.2.7 Find the rank of the following matrix and describe the column and row
spaces eï¬ƒciently.
ï£«
ï£¬
ï£¬
ï£­
1
2
1
3
2
1
3
6
0
2
1
2
1
3
2
1
3
2
4
0
ï£¶
ï£·
ï£·
ï£¸
(9.2)
The row reduced echelon form is
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
13
2
0
1
0
2
âˆ’5
2
0
0
1
âˆ’1
1
2
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£¸.
and so the rank is 3, the row space is the span of the vectors,
Â¡
0
0
1
âˆ’1
1
2
Â¢
,
Â¡
0
1
0
2
âˆ’5
2
Â¢
,
and
Â¡
1
0
0
0
13
2
Â¢
and the column space is the span of the ï¬rst three columns in the original matrix,
span
ï£«
ï£¬
ï£¬
ï£­
ï£«
ï£¬
ï£¬
ï£­
1
1
1
1
ï£¶
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£­
2
3
2
3
ï£¶
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£­
1
6
1
2
ï£¶
ï£·
ï£·
ï£¸
ï£¶
ï£·
ï£·
ï£¸.

166
RANK OF A MATRIX 26,27 SEPT.
Example 9.2.8 Find the rank of the following matrix and describe the column and row
spaces eï¬ƒciently.
ï£«
ï£­
1
2
3
0
1
2
1
3
2
4
âˆ’1
2
1
3
1
ï£¶
ï£¸.
The row reduced echelon form is
ï£«
ï£­
1
0
1
0
21
17
0
1
1
0
âˆ’2
17
0
0
0
1
14
17
ï£¶
ï£¸.
It follows the rank is three and the column space is the span of the ï¬rst, second and fourth
columns of the original matrix.
span
ï£«
ï£­
ï£«
ï£­
1
2
âˆ’1
ï£¶
ï£¸,
ï£«
ï£­
2
1
2
ï£¶
ï£¸,
ï£«
ï£­
0
2
3
ï£¶
ï£¸
ï£¶
ï£¸
while the row space is the span of the vectors
Â¡
0
0
0
1
14
17
Â¢
,
Â¡
0
1
1
0
âˆ’2
17
Â¢
,
and
Â¡
1
0
1
0
21
17
Â¢
.
Procedure 9.2.9 To ï¬nd the rank of a matrix, obtain the row reduced echelon form
for the matrix. Then count the number of nonzero rows or equivalently the number of pivot
columns. This is the rank. The row space is the span of the nonzero rows in the row reduced
echelon form and the column space is the span of the pivot columns of the original matrix.
9.3
Linear Independence And Bases
Quiz
1. Let u = (1, 2, 1) and v = (1, 1, 2) . Find proju (v) .
2. Here are two vectors:
ï£«
ï£­
1
1
2
ï£¶
ï£¸,
ï£«
ï£­
0
1
1
ï£¶
ï£¸. Find an equation of the plane which equals
the span of these two vectors.
3. Here is a matrix:
ï£«
ï£­
1
2
1
1
0
1
2
1
3
ï£¶
ï£¸. Find an LU factorization of this matrix.
4. Here are three points: (1, 2, 1) , (2, 1, 0) , (0, 1, 1) . Find the area of the triangle deter-
mined by these three points.
9.3.1
Linear Independence And Dependence
First we review the concept of linear independence and dependence which was presented
earlier. We deï¬ne what it means for vectors in Fn to be linearly independent and then give
equivalent descriptions. In the following discussion, the symbol,
Â¡
v1
v2
Â· Â· Â·
vk
Â¢
denotes the matrix which has the vector, v1 as the ï¬rst column, v2 as the second column
and so forth until vk is the kth column.

9.3.
LINEAR INDEPENDENCE AND BASES
167
Deï¬nition 9.3.1 Let {v1, Â· Â· Â·, vk} be vectors in Fn. Then this collection of vectors
is said to be linearly independent if whenever
k
X
i=1
civi = 0
it follows each ci = 0. If this condition does not hold, then the set of vectors is said to be
dependent.
The following theorem is very important.
Theorem 9.3.2 A set of vectors, {v1, Â· Â· Â·, vk} is linearly independent if and only
if none of the vectors is a linear combination of the others.
Proof: Suppose {v1, Â· Â· Â·, vk} is linearly independent. If vk = Pkâˆ’1
i=1 civi, then 0 = Pkâˆ’1
i=1 civi+
(âˆ’1) vk and this would mean the vectors are not linearly independent after all. Therefore,
vk cannot be a linear combination of the other vectors. Similarly, none of the other vi can
be a linear combination of the other vectors.
Next suppose none of the vectors is a linear combination of the others. If Pk
i=1 civi = 0,
then if some cl Ì¸= 0, you could write
clvl = âˆ’
X
iÌ¸=l
civi
and then divide by cl to obtain
vl = âˆ’
X
iÌ¸=l
Âµci
cl
Â¶
vi
showing that one of the vectors is a linear combination of the others which, by assump-
tion, does not happen. Therefore, each of the ci = 0 which shows {v1, Â· Â· Â·, vk} is linearly
independent. This proves the theorem.
In words, this says a set of vectors is linearly independent if and only if none of the
vectors is â€œdependentâ€ on the other vectors.
Lemma 9.3.3 The set of vectors, {v1, Â· Â· Â·, vk} âŠ†Fm is linearly independent if and only if
whenever {w1, Â· Â· Â·, wr} is a set of vectors in Fm,each of the ï¬rst k columns of the nÃ—(k + r)
matrix
Â¡
v1
v2
Â· Â· Â·
vk
w1
Â· Â· Â·
wr
Â¢
is a pivot column.
Proof: Suppose ï¬rst {v1, Â· Â· Â·, vk} is linearly independent. Then by Theorem 9.3.2 none
of the vk is a linear combination of the others. It follows each must be a pivot column vk
becoming ek in the row reduced echelon form. If this didnâ€™t happen, then you could apply
Lemma 9.1.6 and conclude one of the vk is a combination of the others.
Next suppose each are pivot columns. Then the row reduced echelon form of the above
matrix is of the form
Â¡
e1
e2
Â· Â· Â·
ek
wâ€²
1
Â· Â· Â·
wâ€²
r
Â¢
.
None of the ek is a linear combination of the others and so by Lemma 9.1.6, the same is
true of the {v1, Â· Â· Â·, vk} . In other words {v1, Â· Â· Â·, vk} is independent.
Corollary 9.3.4 Let {v1, Â· Â· Â·, vk} be a set of vectors in Fn. Then if k > n, it must
be the case that {v1, Â· Â· Â·, vk} is not linearly independent. In other words, if k > n, then
{v1, Â· Â· Â·, vk} is dependent.

168
RANK OF A MATRIX 26,27 SEPT.
Proof: If k > n, then the columns of
Â¡
v1
v2
Â· Â· Â·
vk
Â¢
cannot each be a pivot
column because there are at most n pivot columns due to the fact the matrix has only n
rows.
Example 9.3.5 Determine whether the vectors,
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
ï£«
ï£¬
ï£¬
ï£­
1
2
3
0
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
2
1
0
1
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
0
1
1
2
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
3
2
2
âˆ’1
ï£¶
ï£·
ï£·
ï£¸
ï£¼
ï£´
ï£´
ï£½
ï£´
ï£´
ï£¾
are
linearly independent. If they are linearly dependent, exhibit one of the vectors as a linear
combination of the others.
Form the matrix mentioned above.
ï£«
ï£¬
ï£¬
ï£­
1
2
0
3
2
1
1
2
3
0
1
2
0
1
2
âˆ’1
ï£¶
ï£·
ï£·
ï£¸
Then the row reduced echelon form of this matrix is
ï£«
ï£¬
ï£¬
ï£­
1
0
0
1
0
1
0
1
0
0
1
âˆ’1
0
0
0
0
ï£¶
ï£·
ï£·
ï£¸.
Thus not all the columns are pivot columns and so the vectors are not linear independent.
Note the fourth column is of the form
1
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
ï£¶
ï£·
ï£·
ï£¸+ 1
ï£«
ï£¬
ï£¬
ï£­
0
1
0
0
ï£¶
ï£·
ï£·
ï£¸+ (âˆ’1)
ï£«
ï£¬
ï£¬
ï£­
1
1
âˆ’1
0
ï£¶
ï£·
ï£·
ï£¸
From Lemma 9.1.6, the same linear relationship exists between the columns of the original
matrix. Thus
1
ï£«
ï£¬
ï£¬
ï£­
1
2
3
0
ï£¶
ï£·
ï£·
ï£¸+ 1
ï£«
ï£¬
ï£¬
ï£­
2
1
0
1
ï£¶
ï£·
ï£·
ï£¸+ (âˆ’1)
ï£«
ï£¬
ï£¬
ï£­
0
1
1
2
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
3
2
2
âˆ’1
ï£¶
ï£·
ï£·
ï£¸.
Note the usefullness of the row reduced echelon form in discovering hidden linear rela-
tionships in collections of vectors.
Example 9.3.6 Determine whether the vectors,
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
ï£«
ï£¬
ï£¬
ï£­
1
2
3
0
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
2
1
0
1
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
0
1
1
2
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
3
2
2
0
ï£¶
ï£·
ï£·
ï£¸
ï£¼
ï£´
ï£´
ï£½
ï£´
ï£´
ï£¾
are lin-
early independent.
If they are linearly dependent, exhibit one of the vectors as a linear
combination of the others.
The matrix used to ï¬nd this is
ï£«
ï£¬
ï£¬
ï£­
1
2
0
3
2
1
1
2
3
0
1
2
0
1
2
0
ï£¶
ï£·
ï£·
ï£¸

9.3.
LINEAR INDEPENDENCE AND BASES
169
The row reduced echelon form is
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
ï£¶
ï£·
ï£·
ï£¸
and so every column is a pivot column. Therefore, these vectors are linearly independent
and there is no way to obtain one of the vectors as a linear combination of the others.
9.3.2
Subspaces
It turns out that the span of a set of vectors is something called a subspace. What follows is
an easier to remember description of subspaces. Furthermore, every such thing is the span
of a set of vectors.
Deï¬nition 9.3.7 Let V be a nonempty collection of vectors in Fn. Then V is called
a subspace if whenever Î±, Î² are scalars and u, v are vectors in V, the linear combination,
Î±u + Î²v is also in V .
Theorem 9.3.8 V is a subspace of Fn if and only if there exist vectors of V,
{u1, Â· Â· Â·, uk} such that V = span (u1, Â· Â· Â·, uk) .
Proof: Pick a vector of V, u1. If V = span {u1} , then stop. You have found your
list of vectors. If V Ì¸= span (u1) , then there exists u2 a vector of V which is not a vector
in span (u1) . Consider span (u1, u2) . If V = span (u1, u2) , stop.
Otherwise, pick u3 /âˆˆ
span (u1, u2) . Continue this way. Here is why {u1, Â· Â· Â·, uk} is a linearly independent set.
Suppose it is not so. Then you can let l be the largest index such that ul is a linear
combination of the other vectors. Then
ul =
lâˆ’1
X
i=1
ciui +
k
X
j=l+1
djuj.
By the construction, at least one of the dj must be nonzero since otherwise, ul would be a
linear combination of the preceding vectors which is not allowed by the construction. But
then you could solve for that uj in terms of the other vectors and contradict the choice of l.
Therefore, from Corollary 9.3.4 the process stops when k is no larger than n. This proves
one half of the theorem.
For the other half, suppose V = span (u1, Â· Â· Â·, uk) and let Pk
i=1 ciui and Pk
i=1 diui be
two vectors in V. Now let Î± and Î² be two scalars. Then
Î±
k
X
i=1
ciui + Î²
k
X
i=1
diui =
k
X
i=1
(Î±ci + Î²di) ui
which is one of the things in span (u1, Â· Â· Â·, uk) showing that span (u1, Â· Â· Â·, uk) has the prop-
erties of a subspace. This proves the theorem.
Contained within the proof is the following corollary.
Corollary 9.3.9 If V is a subspace of Fn, then there exist vectors of V, {u1, Â· Â· Â·, uk}
such that V = span (u1, Â· Â· Â·, uk) and {u1, Â· Â· Â·, uk} is linearly independent.
Proof: In the proof we eventually obtain {u1, Â· Â· Â·, uk} such that V = span (u1, Â· Â· Â·, uk)
and {u1, Â· Â· Â·, uk} is linearly independent.
The message is that subspaces of Fn consist of spans of ï¬nite, linearly independent
collections of vectors of Fn.

170
RANK OF A MATRIX 26,27 SEPT.
9.3.3
The Basis Of A Subspace
It was just shown in Corollary 9.3.9 that every subspace of Fn is equal to the span of a
linearly independent collection of vectors of Fn.
Such a collection of vectors is called a
basis.
Deï¬nition 9.3.10 Let V be a subspace of Fn. Then {u1, Â· Â· Â·, uk} is a basis for V
if the following two conditions hold.
1. span (u1, Â· Â· Â·, uk) = V.
2. {u1, Â· Â· Â·, uk} is linearly independent.
The plural of basis is bases.1
The main theorem about bases is the following.
Theorem 9.3.11 Let V be a subspace of Fn and suppose {u1, Â· Â· Â·, uk} and {v1, Â· Â· Â·, vm}
are two bases for V . Then k = m.
Proof: Suppose k < m. Then since {u1, Â· Â· Â·, uk} is a basis for V, each vi is a linear
combination of the vectors of {u1, Â· Â· Â·, uk} . Consider the matrix
Â¡
u1
Â· Â· Â·
uk
v1
Â· Â· Â·
vm
Â¢
in which each of the ui is a pivot column by Lemma 9.3.3 because the {u1, Â· Â· Â·, uk} are
linearly independent. Therefore, the row reduced echelon form of this matrix is
Â¡
e1
Â· Â· Â·
ek
w1
Â· Â· Â·
wm
Â¢
(9.3)
where each wj has zeroes below the kth row. This is because of Lemma 9.1.6 which implies
each wi is a linear combination of the e1, Â·Â·Â·, ek due to the fact each vk is a linear combination
of the uj vectors. Discarding the bottom nâˆ’k rows of zeroes in the above, yields the matrix,
Â¡
eâ€²
1
Â· Â· Â·
eâ€²
k
wâ€²
1
Â· Â· Â·
wâ€²
m
Â¢
in which all vectors are in Fk. Since m > k, it follows from Corollary 9.3.4 that the vectors,
{wâ€²
1, Â· Â· Â·, wâ€²
m} are dependent. Therefore, some wâ€²
j is a linear combination of the other wâ€²
i.
Therefore, wj is a linear combination of the other wi in 9.3. By Lemma 9.1.6 again, the
same linear relationship exists between the {v1, Â· Â· Â·, vm} showing that {v1, Â· Â· Â·, vm} is not
linearly independent and contradicting the assumption that {v1, Â· Â· Â·, vm} is a basis.
It
follows k â‰¤m. Similarly, m â‰¤k. This proves the theorem.
The following deï¬nition can now be stated.
Deï¬nition 9.3.12 Let V be a subspace of Fn. Then the dimension of V is deï¬ned
to be the number of vectors in a basis.
Corollary 9.3.13 The dimension of Fn is n.
Proof: You only need to exhibit a basis for Fn which has n vectors. Such a basis is
{e1, Â· Â· Â·, en}.
Corollary 9.3.14 Let A be an m Ã— n matrix. Then rank (A) equals the dimension of
col (A) and this equals the dimension of row (A).
1To see why the plural of basis is bases, try to say basiss. It involves much hissing.

9.3.
LINEAR INDEPENDENCE AND BASES
171
Proof: The rank of A equals the number of pivot columns. By Lemma 9.1.6 these pivot
columns are linearly independent and span col (A) . Therefore, the dimension of col (A)
equals the rank of A. The number of nonzero rows in the row reduced echelon form equals
the rank of A also. Furthermore, these rows are independent and span row (A) . Thus the
rank of A equals the dimension of the row space as claimed.
From this corollary, the following is obvious.
Corollary 9.3.15 rank(A) = rank
Â¡
AT Â¢
.
Corollary 9.3.16 Suppose {v1, Â· Â· Â·, vn} is linearly independent and each vi is a vector
in Fn. Then {v1, Â· Â· Â·, vn} is a basis for Fn. Suppose {v1, Â· Â· Â·, vm} spans Fn. Then m â‰¥n.
If {v1, Â· Â· Â·, vn} spans Fn, then {v1, Â· Â· Â·, vn} is linearly independent.
Proof: First suppose {v1, Â· Â· Â·, vn} is linearly independent. Let u be a vector of Fn and
consider the matrix,
Â¡
v1
Â· Â· Â·
vn
u
Â¢
.
By Lemma 9.3.3, on Page 167 each vi is a pivot column, the row reduced echelon form is
Â¡
e1
Â· Â· Â·
en
w
Â¢
and so, since w is in span (e1, Â· Â· Â·, en) , it follows from Lemma 9.1.6 that u is in span (v1, Â· Â· Â·, vn).
Therefore, {v1, Â· Â· Â·, vn} is a basis as claimed.
For the second claim, if any of the vi is a linear combination of the others, then delete
that vector from the list. This yields a shorter list of vectors which has the same span. Now
do the same with this shorter list eventually obtaining vectors {vâ€²
1, Â· Â· Â·, vâ€²
l} with l â‰¤m that
spans Fn and has the property that no vector is a linear combination of the others. Thus
{vâ€²
1, Â· Â· Â·, vâ€²
l} is a basis for Fn and so by Theorem 9.3.11, l = n. Therefore, m â‰¥l = n.
Finally consider the third claim. If {v1, Â· Â· Â·, vn} is not linearly independent, then some
vector is a linear combination of the others. Delete that vector from the list. The new list
of vectors still has the same span. If it is linearly independent, stop. If not, some vector
is a linear combination of the others. Delete that vector. This does not change the span.
Continue this way, ï¬nally obtaining a shorter list of vectors, {vâ€²
1, Â· Â· Â·, vâ€²
m} which spans Fn
and is also linearly independent. But then this contradicts Theorem 9.3.11 because this
would yield two bases having diï¬€erent sizes. This proves the corollary.
By way of review, here are a few more examples of the sort worked earlier on which you
can use the new terminology, row (A), col (A) and basis.
Example 9.3.17 Find the rank of the following matrix. If the rank is r, identify r columns
in the original matrix which have the property that every other column may be written
as a linear combination of these. Also ï¬nd a basis for the row and column spaces of the
matrices.
ï£«
ï£­
1
2
3
2
1
5
âˆ’4
âˆ’1
âˆ’2
3
1
0
ï£¶
ï£¸
The row reduced echelon form is
ï£«
ï£­
1
0
0
27
70
0
1
0
1
10
0
0
1
33
70
ï£¶
ï£¸
and so the rank of the matrix is 3. A basis for the column space is the ï¬rst three columns of
the original matrix. I know they span because the ï¬rst three columns of the row reduced

172
RANK OF A MATRIX 26,27 SEPT.
echelon form above span the column space of that matrix. They are linearly independent
because the ï¬rst three columns of the row reduced echelon form are linearly independent.
By Lemma 9.1.6 all linear relationships are preserved and so these ï¬rst three vectors form
a basis for the column space. The four rows of the row reduced echelon form form a basis
for the row space of the original matrix.
Example 9.3.18 Find the rank of the following matrix. If the rank is r, identify r columns
in the original matrix which have the property that every other column may be written
as a linear combination of these. Also ï¬nd a basis for the row and column spaces of the
matrices.
ï£«
ï£­
1
2
3
0
1
1
1
2
âˆ’6
2
âˆ’2
3
1
0
2
ï£¶
ï£¸
The row reduced echelon form is
ï£«
ï£­
1
0
1
0
âˆ’1
7
0
1
1
0
4
7
0
0
0
1
âˆ’11
42
ï£¶
ï£¸.
A basis for the column space of this row reduced echelon form is the ï¬rst second and fourth
columns. Therefore, a basis for the column space in the original matrix is the ï¬rst second
and fourth columns. The rank of the matrix is 3. A basis for the row space of the original
matrix is the columns of the row reduced echelon form.
9.3.4
Finding The Null Space Or Kernel Of A Matrix
Let A be an m Ã— n matrix.
Deï¬nition 9.3.19 ker (A), also referred to as the null space of A is deï¬ned as
follows.
ker (A) = {x : Ax = 0}
and to ï¬nd ker (A) one must solve the system of equations Ax = 0. This is also denoted as
null (A) .
That is, null (A) = ker (A) = {x : Ax = 0} which equals all the vectors which A sends to 0.
This is not new! There is just some new terminology being used. To repeat, ker (A) is
the solution to the system Ax = 0.
Example 9.3.20 Let
A =
ï£«
ï£­
1
2
1
0
âˆ’1
1
2
3
3
ï£¶
ï£¸.
Find ker (A).
You need to solve the equation Ax = 0. To do this you write the augmented matrix and
then obtain the row reduced echelon form and the solution. The augmented matrix is
ï£«
ï£­
1
2
1
|
0
0
âˆ’1
1
|
0
2
3
3
|
0
ï£¶
ï£¸

9.3.
LINEAR INDEPENDENCE AND BASES
173
Next place this matrix in row reduced echelon form,
ï£«
ï£­
1
0
3
|
0
0
1
âˆ’1
|
0
0
0
0
|
0
ï£¶
ï£¸
Note that x1 and x2 are basic variables while x3 is a free variable. Therefore, the solution
to this system of equations, Ax = 0 is given by
ï£«
ï£­
3t
t
t
ï£¶
ï£¸: t âˆˆR.
Example 9.3.21 Let
A =
ï£«
ï£¬
ï£¬
ï£­
1
2
1
0
1
2
âˆ’1
1
3
0
3
1
2
3
1
4
âˆ’2
2
6
0
ï£¶
ï£·
ï£·
ï£¸
Find the null space of A.
You need to solve the equation, Ax = 0. The augmented matrix is
ï£«
ï£¬
ï£¬
ï£­
1
2
1
0
1
|
0
2
âˆ’1
1
3
0
|
0
3
1
2
3
1
|
0
4
âˆ’2
2
6
0
|
0
ï£¶
ï£·
ï£·
ï£¸
Its row reduced echelon form is
ï£«
ï£¬
ï£¬
ï£­
1
0
3
5
6
5
1
5
|
0
0
1
1
5
âˆ’3
5
2
5
|
0
0
0
0
0
0
|
0
0
0
0
0
0
|
0
ï£¶
ï£·
ï£·
ï£¸
It follows x1 and x2 are basic variables and x3, x4, x5 are free variables. Therefore, ker (A)
is given by
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Â¡
âˆ’3
5
Â¢
s1 +
Â¡ âˆ’6
5
Â¢
s2 +
Â¡ 1
5
Â¢
s3
Â¡
âˆ’1
5
Â¢
s1 +
Â¡ 3
5
Â¢
s2 +
Â¡
âˆ’2
5
Â¢
s3
s1
s2
s3
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
: s1, s2, s3 âˆˆR.
We write this in the form
s1
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’3
5
âˆ’1
5
1
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
+ s2
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’6
53
5
0
1
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
+ s3
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
5
âˆ’2
5
0
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
: s1, s2, s3 âˆˆR.
In other words, the null space of this matrix equals the span of the three vectors above.
Thus
ker (A) = span
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’3
5
âˆ’1
5
1
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’6
53
5
0
1
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
5
âˆ’2
5
0
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.

174
RANK OF A MATRIX 26,27 SEPT.
This is the same as
ker (A) = span
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
3
51
5
âˆ’1
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
6
5
âˆ’3
5
0
âˆ’1
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
52
5
0
0
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Notice also that the three vectors above are linearly independent and so the dimension of
ker (A) is 3. This is generally the way it works. The number of free variables equals the
dimension of the null space while the number of basic variables equals the number of pivot
columns which equals the rank. We state this in the following theorem.
Deï¬nition 9.3.22 The dimension of the null space of a matrix is called the nullity2
and written as nullity(A) .
Theorem 9.3.23 Let A be an m Ã— n matrix. Then rank (A) +nullity(A) = n.
This implies the following corollary.
Corollary 9.3.24 Let A be an n Ã— n matrix. Then A is onto if and only if A is one to
one.
The following theorem is an interesting review of the transpose of a matrix.
Theorem 9.3.25 Let A be a real mÃ—n matrix. Then rank
Â¡
AT A
Â¢
= rank (A) and
AT A is invertible if and only if rank (A) = n.
Proof: There are various ways to show this. From Theorem 9.3.23
n = rank (A) + nullity (A) = rank
Â¡
AT A
Â¢
+ nullity
Â¡
AT A
Â¢
.
I will show null (A) = null
Â¡
AT A
Â¢
because this implies the two nullitys above are equal. Sup-
pose Ax = 0. Then AT Ax = 0 also. Hence null (A) âŠ†null
Â¡
AT A
Â¢
. Next suppose AT Ax = 0.
Then
xT AT Ax = (Ax)T Ax = AxÂ·Ax
and so Ax = 0. Hence null (A) âŠ‡null
Â¡
AT A
Â¢
.
If AT A is invertible, then this implies A is one to one and so the columns of A are
independent. Hence rank (A) = n. If rank (A) = n, then the dimension of the column space
is n and so since the column vectors span the column space, they are a basis for it. In
particular they are independent. Hence A is one to one. It follows as above that AT A is
also one to one mapping Fn to Fn. The n columns of AT A are linearly independent because
the matrix is one to one. Therefore, by Corollary 9.3.16 these columns are a basis for Fn
and so AT A is both onto and one to one. Hence it is invertible.
9.3.5
Rank And Existence Of Solutions To Linear Systemsâˆ—
Consider the linear system of equations,
Ax = b
(9.4)
2Isnâ€™t it amazing how many diï¬€erent words are available for use in linear algebra?

9.3.
LINEAR INDEPENDENCE AND BASES
175
where A is an m Ã— n matrix, x is a n Ã— 1 column vector, and b is an m Ã— 1 column vector.
Suppose
A =
Â¡
a1
Â· Â· Â·
an
Â¢
where the ak denote the columns of A. Then if x = (x1, Â· Â· Â·, xn)T is a solution of the system
9.4, it follows
x1a1 + Â· Â· Â· + xnan = b
which says that b is a vector in span (a1, Â· Â· Â·, an) . This shows that there exists a solution
to the system, 9.4 if and only if b is contained in span (a1, Â· Â· Â·, an) . In words, there is a
solution to 9.4 if and only if b is in the column space of A. In terms of rank, the following
proposition describes the situation.
Proposition 9.3.26 Let A be an m Ã— n matrix and let b be an m Ã— 1 column vector.
Then there exists a solution to 9.4 if and only if
rank
Â¡ A
|
b Â¢
= rank (A) .
(9.5)
Proof: Place
Â¡
A
|
b
Â¢
and A in row reduced echelon form, respectively B and C. If
the above condition on rank is true, then both B and C have the same number of nonzero
rows. In particular, you cannot have a row of the form
Â¡
0
Â· Â· Â·
0
â– 
Â¢
where â– Ì¸= 0 in B. Therefore, there will exist a solution to the system.
Conversely, suppose there exists a solution. This means there cannot be such a row in
B described above. Therefore, B and C must have the same number of zero rows and so
they have the same number of nonzero rows. Therefore, the rank of the two matrices in 9.5
is the same.
9.3.6
Exercises With Answers
1. Find the rank of the following matrices. If the rank is r, identify r columns in the
original matrix which have the property that every other column may be written
as a linear combination of these. Also ï¬nd a basis for the row and column spaces of
the matrices.
(a)
ï£«
ï£¬
ï£¬
ï£­
9
2
0
3
7
1
6
1
0
0
2
1
ï£¶
ï£·
ï£·
ï£¸
From using row operations we obtain the row reduced echelon form which is
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
1
0
0
0
1
0
0
0
ï£¶
ï£·
ï£·
ï£¸
Therefore, a basis for the column space of the original matrix is the ï¬rst three
columns of the original matrix. A basis for the row space is just
Â¡
1
0
0
Â¢
,
Â¡
0
1
0
Â¢
,
and
Â¡
0
0
1
Â¢
.

176
RANK OF A MATRIX 26,27 SEPT.
(b)
ï£«
ï£¬
ï£¬
ï£­
3
0
3
10
9
1
1
1
0
2
2
0
ï£¶
ï£·
ï£·
ï£¸
In this case the row reduced echelon form is
ï£«
ï£¬
ï£¬
ï£­
1
0
1
0
1
âˆ’1
0
0
0
0
0
0
ï£¶
ï£·
ï£·
ï£¸
and so a basis for the column space of the original matrix consists of the ï¬rst two
columns of the original matrix and a basis for the row space is
Â¡
1
0
1
Â¢
and
Â¡
0
1
âˆ’1
Â¢
.
(c)
ï£«
ï£¬
ï£¬
ï£­
0
1
7
8
1
9
2
0
3
2
5
1
6
8
0
1
1
2
0
2
3
0
2
1
3
0
3
4
ï£¶
ï£·
ï£·
ï£¸
The row reduced echelon form of this matrix is
ï£«
ï£¬
ï£¬
ï£­
0
1
0
1
0
1
0
0
0
1
1
0
1
0
0
0
0
0
1
1
0
0
0
0
0
0
0
1
ï£¶
ï£·
ï£·
ï£¸
and so a basis for the column space of the original matrix consists of the second,
third, ï¬fth, and seventh columns of the original matrix. A basis for the row space
consists of the rows of this last matrix in row reduced echelon form.
2. Let H denote span
ï£«
ï£­
ï£«
ï£­
1
1
0
ï£¶
ï£¸,
ï£«
ï£­
1
4
5
ï£¶
ï£¸,
ï£«
ï£­
1
3
1
ï£¶
ï£¸,
ï£«
ï£­
1
1
1
ï£¶
ï£¸
ï£¶
ï£¸. Find the dimension of H
and determine a basis.
Make these the columns of a matrix and ask for the rank of this matrix.
ï£«
ï£­
1
1
1
1
1
4
3
1
0
5
1
1
ï£¶
ï£¸
The row reduced echelon form is
ï£«
ï£­
1
0
0
8
7
0
1
0
2
7
0
0
1
âˆ’3
7
ï£¶
ï£¸
A basis for H is
ï£±
ï£²
ï£³
ï£«
ï£­
1
1
0
ï£¶
ï£¸,
ï£«
ï£­
1
4
5
ï£¶
ï£¸,
ï£«
ï£­
1
3
1
ï£¶
ï£¸
ï£¼
ï£½
ï£¾
and so H has dimension 3.
3. Here are three vectors. Determine whether they are linearly independent or linearly
dependent.
ï£«
ï£­
1
0
1
ï£¶
ï£¸,
ï£«
ï£­
2
0
1
ï£¶
ï£¸,
ï£«
ï£­
3
0
0
ï£¶
ï£¸

9.3.
LINEAR INDEPENDENCE AND BASES
177
You need to consider the solutions to the equation
c1
ï£«
ï£­
1
0
1
ï£¶
ï£¸+ c2
ï£«
ï£­
2
0
1
ï£¶
ï£¸+ c3
ï£«
ï£­
3
0
0
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸
and determine whether there is a solution other than the obvious one, c1 = c2 = c3 = 0.
The augmented matrix for the system of equations is
ï£«
ï£­
1
2
3
|
0
0
0
0
|
0
1
1
0
|
0
ï£¶
ï£¸
Taking âˆ’1 times the top row and adding to the bottom and then switching the two
bottom rows yields
ï£«
ï£­
1
2
3
|
0
0
âˆ’1
âˆ’3
|
0
0
0
0
|
0
ï£¶
ï£¸
Next take 2 times the second row and add to the top. This yields
ï£«
ï£­
1
0
âˆ’3
|
0
0
âˆ’1
âˆ’3
|
0
0
0
0
|
0
ï£¶
ï£¸
There are solutions other than the zero solution because c3 is a free variable. Therefore,
these vectors are not linearly independent.
4. Here are four vectors. Determine whether they span R3. Are these vectors linearly
independent?
ï£«
ï£­
1
2
3
ï£¶
ï£¸,
ï£«
ï£­
4
0
3
ï£¶
ï£¸,
ï£«
ï£­
3
2
0
ï£¶
ï£¸,
ï£«
ï£­
2
1
6
ï£¶
ï£¸
The vectors canâ€™t possibly be linearly independent. If they were, they would constitute
a linearly independent set consisting of four vectors even though there exists a spanning
set of only three,
ï£«
ï£­
1
0
0
ï£¶
ï£¸,
ï£«
ï£­
0
1
0
ï£¶
ï£¸,
ï£«
ï£­
0
0
1
ï£¶
ï£¸
However, the four given vectors might still span R3 even though they are not a basis.
What does it take to span R3? Given a vector (x, y, z)T âˆˆR3, do there exist scalars
c1, c2, c3, and c4 such that
c1
ï£«
ï£­
1
2
3
ï£¶
ï£¸+ c2
ï£«
ï£­
4
0
3
ï£¶
ï£¸+ c3
ï£«
ï£­
3
2
0
ï£¶
ï£¸+ c4
ï£«
ï£­
2
1
6
ï£¶
ï£¸=
ï£«
ï£­
x
y
z
ï£¶
ï£¸?
Consider the augmented matrix of the above,
ï£«
ï£­
1
4
3
2
|
x
2
0
2
1
|
y
3
3
0
6
|
z
ï£¶
ï£¸

178
RANK OF A MATRIX 26,27 SEPT.
Doing row operations till an echelon form is obtained leads to
ï£«
ï£­
1
0
0
5
4
|
1
4y + 2
9z âˆ’1
6x
0
1
0
3
4
|
âˆ’1
4y + 1
6x + 1
9z
0
0
1
âˆ’3
4
|
âˆ’2
9z + 1
6x + 1
4y
ï£¶
ï£¸
and you see there is a solution to the desired system of equations. In fact there are
inï¬nitely many because c4 is a free variable. Therefore, the four vectors do span R3.
5. Consider the vectors of the form
ï£±
ï£²
ï£³
ï£«
ï£­
2t + 6s
s âˆ’2t
3t + s
ï£¶
ï£¸: s, t âˆˆR
ï£¼
ï£½
ï£¾.
Is this set of vectors a subspace of R3? If so, explain why, give a basis for the subspace
and ï¬nd its dimension.
This is indeed a subspace. You only need to verify the set of vectors is closed with
respect to the vector space operations. Let
ï£«
ï£­
2t1 + 6s1
s1 âˆ’2t1
3t1 + s1
ï£¶
ï£¸and
ï£«
ï£­
2t + 6s
s âˆ’2t
3t + s
ï£¶
ï£¸be two
vectors in the given set of vectors.
Î±
ï£«
ï£­
2t + 6s
s âˆ’2t
3t + s
ï£¶
ï£¸+ Î²
ï£«
ï£­
2t1 + 6s1
s1 âˆ’2t1
3t1 + s1
ï£¶
ï£¸
=
ï£«
ï£­
2Î±t + 6Î±s + 2Î²t1 + 6Î²s1
Î±s âˆ’2Î±t + Î²s1 âˆ’2Î²t1
3Î±t + Î±s + 3Î²t1 + Î²s1
ï£¶
ï£¸
=
ï£«
ï£­
2 (Î±t + Î²t1) + 6 (Î±s + Î²s1)
Î±s + Î²s1 âˆ’2 (Î±t + Î²t1)
3 (Î±t + Î²t1) + Î±s + Î²s1
ï£¶
ï£¸
If we let T â‰¡Î±t + Î²t1 and S â‰¡Î±s + Î²s1, this is seen to be of the form
ï£«
ï£­
2T + 6S
S âˆ’2T
3T + S
ï£¶
ï£¸
which is the way the vectors in the given set are described. Another way to see this is
to notice that the vectors in the given set are of the form
t
ï£«
ï£­
2
âˆ’2
3
ï£¶
ï£¸+ s
ï£«
ï£­
6
1
1
ï£¶
ï£¸
so it consists of the span of the two vectors,
ï£«
ï£­
2
âˆ’2
3
ï£¶
ï£¸,
ï£«
ï£­
6
1
1
ï£¶
ï£¸.
(9.6)
Recall that the span of a set of vectors is always a subspace. You can also verify these
vectors in 9.6 form a linearly independent set and so they are a basis.

9.3.
LINEAR INDEPENDENCE AND BASES
179
6. Let M =
Â©
u = (u1, u2, u3, u4) âˆˆR4 : u3 â‰¥u2
Âª
. Is M a subspace? Explain.
This is not a subspace because if u âˆˆM, is such that u3 > u2, then consider (âˆ’1) u.
If this were in M you would need to have âˆ’u3 > âˆ’u2 and so u3 < u2 which cannot
be true if u3 > u2. Thus M is not closed under scalar multiplication so it is not a
subspace.
7. Let w, w1 be given vectors in R2 and deï¬ne
M =
Â©
u = (u1, u2) âˆˆR2 : w Â· u = 0 and w1 Â· u = 0
Âª
.
Is M a subspace? Explain.
Suppose uâ€² and u are both in M. What about Î±uâ€² + Î²u?
wÂ· (Î±uâ€² + Î²u) = Î±w Â· uâ€² + Î²w Â· u = Î±0 + Î²0 = 0
Similarly,
w1Â· (Î±uâ€² + Î²u) = Î±w1Â·uâ€² + Î²w1Â·u = Î±0 + Î²0 = 0
and so Î±uâ€² + Î²u âˆˆM. This has veriï¬ed that M is a subspace.

180
RANK OF A MATRIX 26,27 SEPT.

Linear Transformations 27 Sept.
Quiz
1. Find the rank of the matrix
ï£«
ï£¬
ï£¬
ï£­
1
2
1
1
2
1
1
1
2
7
3
3
1
8
3
3
ï£¶
ï£·
ï£·
ï£¸
2. For A the above matrix, ï¬nd
null (A) = ker (A) .
That is, ï¬nd its null space.
3. For A the matrix of Problem 1 ï¬nd a basis for the column space of this matrix.
4. For A the matrix of Problem 1 ï¬nd a basis for the row space of this matrix.
An m Ã— n matrix can be used to transform vectors in Fn to vectors in Fm through the
use of matrix multiplication.
Example 10.0.27 Consider the matrix,
Âµ
1
2
0
2
1
0
Â¶
. Think of it as a function which
takes vectors in F3 and makes them in to vectors in F2 as follows. For
ï£«
ï£­
x
y
z
ï£¶
ï£¸a vector
in F3, multiply on the left by the given matrix to obtain the vector in F2. Here are some
numerical examples.
Âµ
1
2
0
2
1
0
Â¶ ï£«
ï£­
1
2
3
ï£¶
ï£¸=
Âµ
5
4
Â¶
,
Âµ
1
2
0
2
1
0
Â¶ ï£«
ï£­
1
âˆ’2
3
ï£¶
ï£¸=
Âµ
âˆ’3
0
Â¶
,
Âµ
1
2
0
2
1
0
Â¶ ï£«
ï£­
10
5
âˆ’3
ï£¶
ï£¸=
Âµ
20
25
Â¶
,
Âµ
1
2
0
2
1
0
Â¶ ï£«
ï£­
0
7
3
ï£¶
ï£¸=
Âµ
14
7
Â¶
,
More generally,
Âµ
1
2
0
2
1
0
Â¶ ï£«
ï£­
x
y
z
ï£¶
ï£¸=
Âµ
x + 2y
2x + y
Â¶
The idea is to deï¬ne a function which takes vectors in F3 and delivers new vectors in F2.
181

182
LINEAR TRANSFORMATIONS 27 SEPT.
This is an example of something called a linear transformation.
Deï¬nition 10.0.28 Let X and Y be vector spaces and let T : X â†’Y be a function.
Thus for each x âˆˆX, Tx âˆˆY. Then T is a linear transformation if whenever Î±, Î² are
scalars and x1 and x2 are vectors in X,
T (Î±x1 + Î²x2) = Î±1Tx1 + Î²Tx2.
In words, linear transformations distribute across + and allow you to factor out scalars.
At this point, recall the properties of matrix multiplication. The pertinent property is 7.14
on Page 129. Recall it states that for a and b scalars,
A (aB + bC) = aAB + bAC
In particular, for A an m Ã— n matrix and B and C, n Ã— 1 matrices (column vectors) the
above formula holds which is nothing more than the statement that matrix multiplication
gives an example of a linear transformation.
Deï¬nition 10.0.29 A linear transformation is called one to one (often written
as 1 âˆ’1) if it never takes two diï¬€erent vectors to the same vector. Thus T is one to one if
whenever x Ì¸= y
Tx Ì¸= Ty.
Equivalently, if T (x) = T (y) , then x = y.
In the case that a linear transformation comes from matrix multiplication, it is common
usage to refer to the matrix as a one to one matrix when the linear transformation it
determines is one to one.
Deï¬nition 10.0.30 A linear transformation mapping X to Y is called onto if
whenever y âˆˆY there exists x âˆˆX such that T (x) = y.
Thus T is onto if everything in Y gets hit. In the case that a linear transformation
comes from matrix multiplication, it is common to refer to the matrix as onto when the
linear transformation it determines is onto. Also it is common usage to write TX, T (X) ,or
Im (T) as the set of vectors of Y which are of the form Tx for some x âˆˆX. In the case that
T is obtained from multiplication by an m Ã— n matrix, A, it is standard to simply write
A (Fn) AFn, or Im (A) to denote those vectors in Fm which are obtained in the form Ax for
some x âˆˆFn.
10.1
Constructing The Matrix Of A Linear Transfor-
mation
It turns out that if T is any linear transformation which maps Fn to Fm, there is always an
m Ã— n matrix, A with the property that
Ax = Tx
(10.1)
for all x âˆˆFn. Here is why. Suppose T : Fn â†’Fm is a linear transformation and you want
to ï¬nd the matrix deï¬ned by this linear transformation as described in 10.1. Then if x âˆˆFn
it follows
x =
n
X
i=1
xiei

10.1.
CONSTRUCTING THE MATRIX OF A LINEAR TRANSFORMATION
183
where ei is the vector which has zeros in every slot but the ith and a 1 in this slot. Then
since T is linear,
Tx
=
n
X
i=1
xiT (ei)
=
ï£«
ï£­
|
|
T (e1)
Â· Â· Â·
T (en)
|
|
ï£¶
ï£¸
ï£«
ï£¬
ï£­
x1
...
xn
ï£¶
ï£·
ï£¸
â‰¡
A
ï£«
ï£¬
ï£­
x1
...
xn
ï£¶
ï£·
ï£¸
and so you see that the matrix desired is obtained from letting the ith column equal T (ei) .
We state this as the following theorem.
Theorem 10.1.1 Let T be a linear transformation from Fn to Fm. Then the ma-
trix, A satisfying 10.1 is given by
ï£«
ï£­
|
|
T (e1)
Â· Â· Â·
T (en)
|
|
ï£¶
ï£¸
where Tei is the ith column of A.
10.1.1
Rotations of R2
Sometimes you need to ï¬nd a matrix which represents a given linear transformation which
is described in geometrical terms. The idea is to produce a matrix which you can multiply
a vector by to get the same thing as some geometrical description. A good example of this
is the problem of rotation of vectors.
Example 10.1.2 Determine the matrix which represents the linear transformation deï¬ned
by rotating every vector through an angle of Î¸.
Let e1 â‰¡
Âµ
1
0
Â¶
and e2 â‰¡
Âµ
0
1
Â¶
. These identify the geometric vectors which point
along the positive x axis and positive y axis as shown.
-
6
e1
e2

184
LINEAR TRANSFORMATIONS 27 SEPT.
From the above, you only need to ï¬nd Te1 and Te2, the ï¬rst being the ï¬rst column of
the desired matrix, A and the second being the second column. From drawing a picture and
doing a little geometry, you see that
Te1 =
Âµ cos Î¸
sin Î¸
Â¶
, Te2 =
Âµ âˆ’sin Î¸
cos Î¸
Â¶
.
Therefore, from Theorem 10.1.1,
A =
Âµ cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
Â¶
Example 10.1.3 Find the matrix of the linear transformation which is obtained by ï¬rst
rotating all vectors through an angle of Ï† and then through an angle Î¸. Thus you want the
linear transformation which rotates all angles through an angle of Î¸ + Ï†.
Let TÎ¸+Ï† denote the linear transformation which rotates every vector through an angle
of Î¸ + Ï†. Then to get TÎ¸+Ï†, you could ï¬rst do TÏ† and then do TÎ¸ where TÏ† is the linear
transformation which rotates through an angle of Ï† and TÎ¸ is the linear transformation
which rotates through an angle of Î¸. Denoting the corresponding matrices by AÎ¸+Ï†, AÏ†,
and AÎ¸, you must have for every x
AÎ¸+Ï†x = TÎ¸+Ï†x = TÎ¸TÏ†x = AÎ¸AÏ†x.
Consequently, you must have
AÎ¸+Ï†
=
Âµ
cos (Î¸ + Ï†)
âˆ’sin (Î¸ + Ï†)
sin (Î¸ + Ï†)
cos (Î¸ + Ï†)
Â¶
= AÎ¸AÏ†
=
Âµ cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
Â¶ Âµ cos Ï†
âˆ’sin Ï†
sin Ï†
cos Ï†
Â¶
.
You know how to multiply matrices. Do so to the pair on the right. This yields
Âµ cos (Î¸ + Ï†)
âˆ’sin (Î¸ + Ï†)
sin (Î¸ + Ï†)
cos (Î¸ + Ï†)
Â¶
=
Âµ cos Î¸ cos Ï† âˆ’sin Î¸ sin Ï†
âˆ’cos Î¸ sin Ï† âˆ’sin Î¸ cos Ï†
sin Î¸ cos Ï† + cos Î¸ sin Ï†
cos Î¸ cos Ï† âˆ’sin Î¸ sin Ï†
Â¶
.
Donâ€™t these look familiar? They are the usual trig. identities for the sum of two angles
derived here using linear algebra concepts.
You do not have to stop with two dimensions. You can consider rotations and other
geometric concepts in any number of dimensions.
This is one of the major advantages
of linear algebra. You can break down a diï¬ƒcult geometrical procedure into small steps,
each corresponding to multiplication by an appropriate matrix. Then by multiplying the
matrices, you can obtain a single matrix which can give you numerical information on the
results of applying the given sequence of simple procedures. That which you could never
visualize can still be understood to the extent of ï¬nding exact numerical answers. Another
example follows.
Example 10.1.4 Find the matrix of the linear transformation which is obtained by ï¬rst
rotating all vectors through an angle of Ï€/6 and then reï¬‚ecting through the x axis.
As shown in Example 10.1.3, the matrix of the transformation which involves rotating
through an angle of Ï€/6 is
Âµ
cos (Ï€/6)
âˆ’sin (Ï€/6)
sin (Ï€/6)
cos (Ï€/6)
Â¶
=
Âµ
1
2
âˆš
3
âˆ’1
2
1
2
1
2
âˆš
3
Â¶

10.1.
CONSTRUCTING THE MATRIX OF A LINEAR TRANSFORMATION
185
The matrix for the transformation which reï¬‚ects all vectors through the x axis is
Âµ
1
0
0
âˆ’1
Â¶
.
Therefore, the matrix of the linear transformation which ï¬rst rotates through Ï€/6 and then
reï¬‚ects through the x axis is
Âµ
1
0
0
âˆ’1
Â¶ Âµ
1
2
âˆš
3
âˆ’1
2
1
2
1
2
âˆš
3
Â¶
=
Âµ
1
2
âˆš
3
âˆ’1
2
âˆ’1
2
âˆ’1
2
âˆš
3
Â¶
.
10.1.2
Projections
In Physics it is important to consider the work done by a force ï¬eld on an object. This
involves the concept of projection onto a vector. Suppose you want to ï¬nd the projection
of a vector, v onto the given vector, u, denoted by proju (v) This is done using the dot
product as follows.
proju (v) =
Â³v Â· u
u Â· u
Â´
u
Because of properties of the dot product, the map v â†’proju (v) is linear,
proju (Î±v+Î²w)
=
ÂµÎ±v+Î²w Â· u
u Â· u
Â¶
u = Î±
Â³v Â· u
u Â· u
Â´
u + Î²
Â³w Â· u
u Â· u
Â´
u
=
Î± proju (v) + Î² proju (w) .
Example 10.1.5 Let the projection map be deï¬ned above and let u = (1, 2, 3)T . Does this
linear transformation come from multiplication by a matrix? If so, what is the matrix?
You can ï¬nd this matrix in the same way as in the previous example. Let ei denote the
vector in Rn which has a 1 in the ith position and a zero everywhere else. Thus a typical
vector, x = (x1, Â· Â· Â·, xn)T can be written in a unique way as
x =
n
X
j=1
xjej.
From the way you multiply a matrix by a vector, it follows that proju (ei) gives the ith
column of the desired matrix. Therefore, it is only necessary to ï¬nd
proju (ei) â‰¡
Â³ eiÂ·u
u Â· u
Â´
u
For the given vector in the example, this implies the columns of the desired matrix are
1
14
ï£«
ï£­
1
2
3
ï£¶
ï£¸, 2
14
ï£«
ï£­
1
2
3
ï£¶
ï£¸, 3
14
ï£«
ï£­
1
2
3
ï£¶
ï£¸.
Hence the matrix is
1
14
ï£«
ï£­
1
2
3
2
4
6
3
6
9
ï£¶
ï£¸.

186
LINEAR TRANSFORMATIONS 27 SEPT.
10.1.3
Matrices Which Are One To One Or Onto
Lemma 10.1.6 Let A be an m Ã— n matrix.
Then A (Fn) = span (a1, Â· Â· Â·, an) where
a1, Â· Â· Â·, an denote the columns of A. In fact, for x = (x1, Â· Â· Â·, xn)T ,
Ax =
n
X
k=1
xkak.
Proof: This follows from the deï¬nition of matrix multiplication in Deï¬nition 7.1.9 on
Page 124.
The following is a theorem of major signiï¬cance. First here is an interesting observation.
Observation 10.1.7 Let A be an m Ã— n matrix. Then A is one to one if and only if
Ax = 0 implies x = 0.
Here is why: A0 = A (0 + 0) = A0 + A0 and so A0 = 0.
Now suppose A is one to one and Ax = 0. Then since A0 = 0, it follows x = 0. Thus if
A is one to one and Ax = 0, then x = 0.
Next suppose the condition that Ax = 0 implies x = 0 is valid. Then if Ax = Ay, then
A (x âˆ’y) = 0 and so from the condition, x âˆ’y = 0 so that x = y. Thus A is one to one.
Theorem 10.1.8 Suppose A is an nÃ—n matrix. Then A is one to one if and only
if A is onto. Also, if B is an n Ã— n matrix and AB = I, then it follows BA = I.
Proof: First suppose A is one to one. Consider the vectors, {Ae1, Â· Â· Â·, Aen} where ek is
the column vector which is all zeros except for a 1 in the kth position. This set of vectors is
linearly independent because if
n
X
k=1
ckAek = 0,
then since A is linear,
A
Ãƒ n
X
k=1
ckek
!
= 0
and since A is one to one, it follows
n
X
k=1
ckek = 0
which implies each ck = 0. Therefore, {Ae1, Â· Â· Â·, Aen} must be a basis for Fn by Corollary
9.3.16. It follows that for y âˆˆFn there exist constants, ci such that
y =
n
X
k=1
ckAek = A
Ãƒ n
X
k=1
ckek
!
showing that, since y was arbitrary, A is onto.
Next suppose A is onto. This implies the span of the columns of A equals Fn and by
Corollary 9.3.16 this implies the columns of A are independent. If Ax = 0, then letting
x = (x1, Â· Â· Â·, xn)T , it follows
n
X
i=1
xiai = 0

10.1.
CONSTRUCTING THE MATRIX OF A LINEAR TRANSFORMATION
187
and so each xi = 0. If Ax = Ay, then A (x âˆ’y) = 0 and so x = y. This shows A is one to
one.
Now suppose AB = I. Why is BA = I? Since AB = I it follows B is one to one since
otherwise, there would exist, x Ì¸= 0 such that Bx = 0 and then ABx = A0 = 0 Ì¸= Ix.
Therefore, from what was just shown, B is also onto. In addition to this, A must be one
to one because if Ay = 0, then y = Bx for some x and then x = ABx = Ay = 0 showing
y = 0. Now from what is given to be so, it follows (AB) A = A and so using the associative
law for matrix multiplication,
A (BA) âˆ’A = A (BA âˆ’I) = 0.
But this means (BA âˆ’I) x = 0 for all x since otherwise, A would not be one to one. Hence
BA = I as claimed. This proves the theorem.
This theorem shows that if an n Ã— n matrix, B acts like an inverse when multiplied on
one side of A it follows that B = Aâˆ’1and it will act like an inverse on both sides of A.
The conclusion of this theorem pertains to square matrices only. For example, let
A =
ï£«
ï£­
1
0
0
1
1
0
ï£¶
ï£¸, B =
Âµ 1
0
0
1
1
âˆ’1
Â¶
(10.2)
Then
BA =
Âµ 1
0
0
1
Â¶
but
AB =
ï£«
ï£­
1
0
0
1
1
âˆ’1
1
0
0
ï£¶
ï£¸.
10.1.4
The General Solution Of A Linear System
Recall the following deï¬nition which was discussed above.
Deï¬nition 10.1.9 T is a linear transformation if whenever x, y are vectors and
a, b scalars,
T (ax + by) = aTx + bTy.
Thus linear transformations distribute across addition and pass scalars to the outside. A
linear system is one which is of the form
Tx = b.
If Txp = b, then xp is called a particular solution to the linear system.
For example, if A is an m Ã— n matrix and TA is determined by
TA (x) = Ax,
then from the properties of matrix multiplication, TA is a linear transformation. In this
setting, we will usually write A for the linear transformation as well as the matrix. There
are many other examples of linear transformations other than this. In diï¬€erential equations,
you will encounter linear transformations which act on functions to give new functions. In
this case, the functions are considered as vectors.

188
LINEAR TRANSFORMATIONS 27 SEPT.
Deï¬nition 10.1.10 Let T be a linear transformation. Deï¬ne
ker (T) â‰¡{x : Tx = 0} .
In words, ker (T) is called the kernel of T. As just described, ker (T) consists of the set of
all vectors which T sends to 0. This is also called the null space of T. It is also called the
solution space of the equation Tx = 0.
The above deï¬nition states that ker (T) is the set of solutions to the equation,
Tx = 0.
In the case where T is really a matrix, you have been solving such equations for quite some
time. However, sometimes linear transformations act on vectors which are not in Fn.
Example 10.1.11 Let
d
dx denote the linear transformation deï¬ned on X, the functions
which are deï¬ned on R and have a continuous derivative. Find ker
Â¡ d
dx
Â¢
.
The example asks for functions, f which the property that df
dx = 0. As you know from
calculus, these functions are the constant functions. Thus ker
Â¡ d
dx
Â¢
= constant functions.
When T is a linear transformation, systems of the form Tx = 0 are called homogeneous
systems. Thus the solution to the homogeneous system is known as ker (T) .
Systems of the form Tx = b where b Ì¸= 0 are called nonhomogeneous systems. It
turns out there is a very interesting and important relation between the solutions to the
homogeneous systems and the solutions to the nonhomogeneous systems.
Theorem 10.1.12 Suppose xp is a solution to the linear system,
Tx = b
Then if y is any other solution to the linear system, there exists x âˆˆker (T) such that
y = xp + x.
Proof: Consider y âˆ’xp â‰¡y+ (âˆ’1) xp. Then T
Â¡
y âˆ’xp
Â¢
= Ty âˆ’Txp = b âˆ’b = 0. Let
x â‰¡y âˆ’xp. This proves the theorem.
Sometimes people remember the above theorem in the following form. The solutions
to the nonhomogeneous system, Tx = b are given by xp + ker (T) where xp is a particular
solution to Tx = b.
We have been vague about what T is and what x is on purpose.
This theorem is
completely algebraic in nature and will work whenever you have linear transformations. In
particular, it will be important in diï¬€erential equations. For now, here is a familiar example.
Example 10.1.13 Let
A =
ï£«
ï£­
1
2
3
0
2
1
1
2
4
5
7
2
ï£¶
ï£¸
Find ker (A). Equivalently, ï¬nd the solution space to the system of equations Ax = 0.
This asks you to ï¬nd {x : Ax = 0} . In other words you are asked to solve the system,
Ax = 0. Let x = (x, y, z, w)T . Then this amounts to solving
ï£«
ï£­
1
2
3
0
2
1
1
2
4
5
7
2
ï£¶
ï£¸
ï£«
ï£¬
ï£¬
ï£­
x
y
z
w
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸

10.1.
CONSTRUCTING THE MATRIX OF A LINEAR TRANSFORMATION
189
This is the linear system
x + 2y + 3z = 0
2x + y + z + 2w = 0
4x + 5y + 7z + 2w = 0
and you know how to solve this using row operations, (Gauss Elimination). Set up the
augmented matrix,
ï£«
ï£­
1
2
3
0
|
0
2
1
1
2
|
0
4
5
7
2
|
0
ï£¶
ï£¸
Then row reduce to obtain the row reduced echelon form,
ï£«
ï£¬
ï£¬
ï£­
1
0
âˆ’1
3
4
3
|
0
0
1
5
3
âˆ’2
3
|
0
0
0
0
0
|
0
ï£¶
ï£·
ï£·
ï£¸.
This yields x = 1
3z âˆ’4
3w and y = 2
3w âˆ’5
3z. Thus ker (A) consists of vectors of the form,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
3z âˆ’4
3w
2
3w âˆ’5
3z
z
w
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
= z
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
3
âˆ’5
3
1
0
ï£¶
ï£·
ï£·
ï£·
ï£¸+ w
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’4
3
2
3
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸.
Example 10.1.14 The general solution of a linear system of equations is just the set of
all solutions. Find the general solution to the linear system,
ï£«
ï£­
1
2
3
0
2
1
1
2
4
5
7
2
ï£¶
ï£¸
ï£«
ï£¬
ï£¬
ï£­
x
y
z
w
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£­
9
7
25
ï£¶
ï£¸
given that
Â¡
1
1
2
1
Â¢T =
Â¡
x
y
z
w
Â¢T is one solution.
Note the matrix on the left is the same as the matrix in Example 10.1.13. Therefore,
from Theorem 10.1.12, you will obtain all solutions to the above linear system in the form
z
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
3
âˆ’5
3
1
0
ï£¶
ï£·
ï£·
ï£·
ï£¸+ w
ï£«
ï£¬
ï£¬
ï£¬
ï£­
âˆ’4
3
2
3
0
1
ï£¶
ï£·
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
1
2
1
ï£¶
ï£·
ï£·
ï£¸
because
ï£«
ï£¬
ï£¬
ï£­
x
y
z
w
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
1
1
2
1
ï£¶
ï£·
ï£·
ï£¸is a particular solution to the given system of equations.

190
LINEAR TRANSFORMATIONS 27 SEPT.
10.1.5
Exercises With Answers
1. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of 5Ï€/12.
You note that 5Ï€/12 = 2Ï€/3 âˆ’Ï€/4. Therefore, you can ï¬rst rotate through âˆ’Ï€/4
and then rotate through 2Ï€/3 to get the rotation through 5Ï€/12. The matrix of the
transformation with respect to the usual coordinates which rotates through âˆ’Ï€/4 is
Âµ
âˆš
2/2
âˆš
2/2
âˆ’
âˆš
2/2
âˆš
2/2
Â¶
and the matrix of the transformation which rotates through 2Ï€/3 is
Âµ
âˆ’1/2
âˆ’
âˆš
3/2
âˆš
3/2
âˆ’1/2
Â¶
.
Multiplying these gives
Âµ
âˆ’1/2
âˆ’
âˆš
3/2
âˆš
3/2
âˆ’1/2
Â¶ Âµ
âˆš
2/2
âˆš
2/2
âˆ’
âˆš
2/2
âˆš
2/2
Â¶
=
Âµ
âˆ’1
4
âˆš
2 + 1
4
âˆš
3
âˆš
2
âˆ’1
4
âˆš
2 âˆ’1
4
âˆš
3
âˆš
2
1
4
âˆš
3
âˆš
2 + 1
4
âˆš
2
âˆ’1
4
âˆš
2 + 1
4
âˆš
3
âˆš
2
Â¶
and this is the matrix of the desired transformation. Note this shows that
cos (5Ï€/12) = âˆ’1
4
âˆš
2 + 1
4
âˆš
3
âˆš
2 â‰ˆ. 258 819 05
sin (5Ï€/12) = 1
4
âˆš
3
âˆš
2 + 1
4
âˆš
2 â‰ˆ. 965 925 83.
2. Find the matrix for the linear transformation which rotates every vector in R2 through
an angle of 2Ï€/3 and then reï¬‚ects across the x axis.
What does it do to e1? First you rotate e1 through the given angle to obtain
Âµ âˆ’1/2
âˆš
3/2
Â¶
and then this becomes
Âµ
âˆ’1/2
âˆ’
âˆš
3/2
Â¶
.
This is the ï¬rst column of the desired matrix. Next e2 ï¬rst is rotated through the
given angle to give
Âµ
âˆ’
âˆš
3/2
âˆ’1/2
Â¶
and then it is reï¬‚ected across the x axis to give
Âµ
âˆ’
âˆš
3/2
1/2
Â¶
and this gives the second column of the desired matrix. Thus the matrix is
Âµ
âˆ’1/2
âˆ’
âˆš
3/2
âˆ’
âˆš
3/2
1/2
Â¶
.

10.1.
CONSTRUCTING THE MATRIX OF A LINEAR TRANSFORMATION
191
3. Find the matrix for proju (v) where u = (1, âˆ’2, 3)T .
Recall
proju (v) = v Â· u
||u||2 u
Therefore,
proju (e1)
=
1
14
ï£«
ï£­
1
âˆ’2
3
ï£¶
ï£¸, proju (e2) = âˆ’2
14
ï£«
ï£­
1
âˆ’2
3
ï£¶
ï£¸,
proju (e2)
=
3
14
ï£«
ï£­
1
âˆ’2
3
ï£¶
ï£¸.
Hence the desired matrix is
1
14
ï£«
ï£­
1
âˆ’2
3
âˆ’2
4
âˆ’6
3
âˆ’6
9
ï£¶
ï£¸.
4. Show that the function Tu deï¬ned by Tu (v) â‰¡v âˆ’proju (v) is also a linear transfor-
mation.
Tu (Î±v + Î²w) = Î±v + Î²wâˆ’proju (Î±v + Î²w)
which from 3 equals
Î± (v âˆ’proju (v)) + Î² (w âˆ’proju (w)) = Î±Tuv + Î²Tuw.
This is what it takes to be a linear transformation.
5. If A, B, and C are each n Ã— n matrices and ABC is invertible, why are each of A, B,
and C invertible.
0 Ì¸= det (ABC) = det (A) det (B) det (C) and so none of det (A) , det (B) , or det (C)
can equal zero. Therefore, each is invertible. You should do this another way, showing
that each of A, B, and C is one to one and then using a theorem presented earlier.
6. Give an example of a 3 Ã— 1 matrix with the property that the linear transformation
determined by this matrix is one to one but not onto.
Here is one.
ï£«
ï£­
1
0
0
ï£¶
ï£¸. If
ï£«
ï£­
1
0
0
ï£¶
ï£¸x =
ï£«
ï£­
0
0
0
ï£¶
ï£¸, then x = 0 but this is certainly not onto
as a map from R1 to R3 because it does not ever yield
ï£«
ï£­
1
1
0
ï£¶
ï£¸.
7. Find the matrix of the linear transformation from R3 to R3 which ï¬rst rotates every
vector through an angle of Ï€/4 about the z axis when viewed from the positive z axis
and then rotates every vector through an angle of Ï€/6 about the x axis when viewed
from the positive x axis.
The matrix of the linear transformation which accomplishes the ï¬rst rotation is
ï£«
ï£­
âˆš
2/2
âˆ’
âˆš
2/2
0
âˆš
2/2
âˆš
2/2
0
0
0
1
ï£¶
ï£¸

192
LINEAR TRANSFORMATIONS 27 SEPT.
and the matrix which accomplishes the second rotation is
ï£«
ï£­
1
0
0
0
âˆš
3/2
âˆ’1/2
0
1/2
âˆš
3/2
ï£¶
ï£¸
Therefore, the matrix of the desired linear transformation is
ï£«
ï£­
1
0
0
0
âˆš
3/2
âˆ’1/2
0
1/2
âˆš
3/2
ï£¶
ï£¸
ï£«
ï£­
âˆš
2/2
âˆ’
âˆš
2/2
0
âˆš
2/2
âˆš
2/2
0
0
0
1
ï£¶
ï£¸
=
ï£«
ï£­
1
2
âˆš
2
âˆ’1
2
âˆš
2
0
1
4
âˆš
3
âˆš
2
1
4
âˆš
3
âˆš
2
âˆ’1
2
1
4
âˆš
2
1
4
âˆš
2
1
2
âˆš
3
ï£¶
ï£¸
This might not be the ï¬rst thing you would think of.

Part V
Eigenvalues, Eigenvectors,
Determinants, Diagonalization
193


195
Outcomes
A. Interpret the eigenvalue problem algebraically.
i. Determine whether a given vector is an eigenvector.
ii. Verify that a given value is an eigenvalue.
B. Interpret the eigenvalue problem geometrically. Determine eigenvalues and eigenvec-
tors based on:
i. an understanding of the linear transformation determined by the matrix
ii. from the graph of the eigenspace.
C. Find the eigenvalues and eigenvectors of a general 2 Ã— 2 matrix.
Reading: Linear Algebra 4.1
Homework: 4.1:
Outcome Mapping:
A. 1-6,7-12
B. 13-18,19-22
C. 23-26,27-30,31-34,35-38
A. Apply the Laplace Expansion to evaluate determinants of n Ã— n matrices.
B. Recall and apply the properties of determinants to evaluate determinants, including:
i. det(AB) = det(A) det(B)
ii. det(kA) = kn det(A)
iii. det(Aâˆ’1) =
1
det(A)
iv. det(AT ) = det(A)
C. Recall the eï¬€ects that row operations have on the determinants of matrices. Relate
to the determinants of elementary matrices.
D. Prove theorems involving determinants.
E. Evaluate matrix inverses using the adjoint method. Determine whether or not a matrix
has an inverse based on its determinant.
F. Use Cramerâ€™s rule to solve a linear system.
Reading: Linear Algebra 4.2
Homework: 4.2:
Outcome Mapping:
A. 1-6,7-15,16-20

196
B. 35-38,47-52
C. 26-33,35-40
D. 21,41-44,53-56,66
E. 45-46,61-64,65
F. 57-60
A. Given an n Ã— n matrix, compute
i. the characteristic polynomial
ii. the eigenvalues
iii. a basis for each eigenspace
iv. the algebraic and geometric multiplicities of each eigenvalue
B. Solve application problems involving eigenvalues and eigenvectors.
C. Recall and prove theorems involving eigenvalues and eigenvectors.
Reading: Linear Algebra 4.3
Homework: 4.3:
Outcome Mapping:
A. 1-12
B. 15-22,26-31,33-38
C. 23-25,32,39-42
A. Deï¬ne similarity. Determine whether or not two matrices are similar.
B. Determine if a matrix is diagonalizable. Find the diagonalization of a matrix.
C. Find powers of a matrix using the diagonalization of a matrix.
D. Prove theorems involving the similarity and diagonalization of matrices.
Reading: Linear Algebra 4.4
Homework: 4.4:
Outcome Mapping:
A. 1-4,36-39
B. 5-7,8-15,24-29
C. 16-23
D. 30-35,40-50

Determinants 2,3 Oct.
Quiz
1. A linear transformation involves ï¬rst rotating the vectors in R2 counterclockwise
through an angle of 30 degrees and then reï¬‚ecting across the x axis. Find the matrix
of this linear transformation.
2. A linear transformation involves projecting all vectors on to the span of the vector
(1, 1, 1) . Find the matrix of this linear transformation.
11.1
Basic Techniques And Properties
11.1.1
Cofactors And 2 Ã— 2 Determinants
Let A be an n Ã— n matrix. The determinant of A, denoted as det (A) is a number. If the
matrix is a 2Ã—2 matrix, this number is very easy to ï¬nd.
Deï¬nition 11.1.1 Let A =
Âµ
a
b
c
d
Â¶
. Then
det (A) â‰¡ad âˆ’cb.
The determinant is also often denoted by enclosing the matrix with two vertical lines. Thus
det
Âµ
a
b
c
d
Â¶
=
Â¯Â¯Â¯Â¯
a
b
c
d
Â¯Â¯Â¯Â¯ .
Example 11.1.2 Find det
Âµ
2
4
âˆ’1
6
Â¶
.
From the deï¬nition this is just (2) (6) âˆ’(âˆ’1) (4) = 16.
Having deï¬ned what is meant by the determinant of a 2 Ã— 2 matrix, what about a 3 Ã— 3
matrix?
Deï¬nition 11.1.3 Suppose A is a 3 Ã— 3 matrix.
The ijth minor, denoted as
minor(A)ij , is the determinant of the 2 Ã— 2 matrix which results from deleting the ith row
and the jth column.
Example 11.1.4 Consider the matrix,
ï£«
ï£­
1
2
3
4
3
2
3
2
1
ï£¶
ï£¸.
197

198
DETERMINANTS 2,3 OCT.
The (1, 2) minor is the determinant of the 2 Ã— 2 matrix which results when you delete the
ï¬rst row and the second column. This minor is therefore
det
Âµ
4
2
3
1
Â¶
= âˆ’2.
The (2, 3) minor is the determinant of the 2 Ã— 2 matrix which results when you delete the
second row and the third column. This minor is therefore
det
Âµ 1
2
3
2
Â¶
= âˆ’4.
Deï¬nition 11.1.5 Suppose A is a 3 Ã— 3 matrix. The ijth cofactor is deï¬ned to be
(âˆ’1)i+j Ã—
Â¡
ijth minor
Â¢
. In words, you multiply (âˆ’1)i+j times the ijth minor to get the ijth
cofactor. The cofactors of a matrix are so important that special notation is appropriate
when referring to them. The ijth cofactor of a matrix, A will be denoted by cof (A)ij . It is
also convenient to refer to the cofactor of an entry of a matrix as follows. For aij an entry
of the matrix, its cofactor is just cof (A)ij . Thus the cofactor of the ijth entry is just the
ijth cofactor.
Example 11.1.6 Consider the matrix,
A =
ï£«
ï£­
1
2
3
4
3
2
3
2
1
ï£¶
ï£¸.
The (1, 2) minor is the determinant of the 2 Ã— 2 matrix which results when you delete the
ï¬rst row and the second column. This minor is therefore
det
Âµ
4
2
3
1
Â¶
= âˆ’2.
It follows
cof (A)12 = (âˆ’1)1+2 det
Âµ
4
2
3
1
Â¶
= (âˆ’1)1+2 (âˆ’2) = 2
The (2, 3) minor is the determinant of the 2 Ã— 2 matrix which results when you delete the
second row and the third column. This minor is therefore
det
Âµ
1
2
3
2
Â¶
= âˆ’4.
Therefore,
cof (A)23 = (âˆ’1)2+3 det
Âµ 1
2
3
2
Â¶
= (âˆ’1)2+3 (âˆ’4) = 4.
Similarly,
cof (A)22 = (âˆ’1)2+2 det
Âµ 1
3
3
1
Â¶
= âˆ’8.
Deï¬nition 11.1.7 The determinant of a 3 Ã— 3 matrix, A, is obtained by picking a
row (column) and taking the product of each entry in that row (column) with its cofactor and
adding these up. This process when applied to the ith row (column) is known as expanding
the determinant along the ith row (column).

11.1.
BASIC TECHNIQUES AND PROPERTIES
199
Example 11.1.8 Find the determinant of
A =
ï£«
ï£­
1
2
3
4
3
2
3
2
1
ï£¶
ï£¸.
Here is how it is done by â€œexpanding along the ï¬rst columnâ€.
1
cof(A)11
z
}|
{
(âˆ’1)1+1
Â¯Â¯Â¯Â¯
3
2
2
1
Â¯Â¯Â¯Â¯ + 4
cof(A)21
z
}|
{
(âˆ’1)2+1
Â¯Â¯Â¯Â¯
2
3
2
1
Â¯Â¯Â¯Â¯ + 3
cof(A)31
z
}|
{
(âˆ’1)3+1
Â¯Â¯Â¯Â¯
2
3
3
2
Â¯Â¯Â¯Â¯ = 0.
You see, we just followed the rule in the above deï¬nition. We took the 1 in the ï¬rst column
and multiplied it by its cofactor, the 4 in the ï¬rst column and multiplied it by its cofactor,
and the 3 in the ï¬rst column and multiplied it by its cofactor. Then we added these numbers
together.
You could also expand the determinant along the second row as follows.
4
cof(A)21
z
}|
{
(âˆ’1)2+1
Â¯Â¯Â¯Â¯
2
3
2
1
Â¯Â¯Â¯Â¯ + 3
cof(A)22
z
}|
{
(âˆ’1)2+2
Â¯Â¯Â¯Â¯
1
3
3
1
Â¯Â¯Â¯Â¯ + 2
cof(A)23
z
}|
{
(âˆ’1)2+3
Â¯Â¯Â¯Â¯
1
2
3
2
Â¯Â¯Â¯Â¯ = 0.
Observe this gives the same number.
You should try expanding along other rows and
columns. If you donâ€™t make any mistakes, you will always get the same answer.
What about a 4 Ã— 4 matrix? You know now how to ï¬nd the determinant of a 3 Ã— 3
matrix. The pattern is the same.
Deï¬nition 11.1.9 Suppose A is a 4Ã—4 matrix. The ijth minor is the determinant
of the 3 Ã— 3 matrix you obtain when you delete the ith row and the jth column. The ijth
cofactor, cof (A)ij is deï¬ned to be (âˆ’1)i+j Ã—
Â¡
ijth minor
Â¢
. In words, you multiply (âˆ’1)i+j
times the ijth minor to get the ijth cofactor.
Deï¬nition 11.1.10 The determinant of a 4Ã—4 matrix, A, is obtained by picking a
row (column) and taking the product of each entry in that row (column) with its cofactor and
adding these up. This process when applied to the ith row (column) is known as expanding
the determinant along the ith row (column).
Example 11.1.11 Find det (A) where
A =
ï£«
ï£¬
ï£¬
ï£­
1
2
3
4
5
4
2
3
1
3
4
5
3
4
3
2
ï£¶
ï£·
ï£·
ï£¸
As in the case of a 3 Ã— 3 matrix, you can expand this along any row or column. Lets
pick the third column. det (A) =
3 (âˆ’1)1+3
Â¯Â¯Â¯Â¯Â¯Â¯
5
4
3
1
3
5
3
4
2
Â¯Â¯Â¯Â¯Â¯Â¯
+ 2 (âˆ’1)2+3
Â¯Â¯Â¯Â¯Â¯Â¯
1
2
4
1
3
5
3
4
2
Â¯Â¯Â¯Â¯Â¯Â¯
+
4 (âˆ’1)3+3
Â¯Â¯Â¯Â¯Â¯Â¯
1
2
4
5
4
3
3
4
2
Â¯Â¯Â¯Â¯Â¯Â¯
+ 3 (âˆ’1)4+3
Â¯Â¯Â¯Â¯Â¯Â¯
1
2
4
5
4
3
1
3
5
Â¯Â¯Â¯Â¯Â¯Â¯
.

200
DETERMINANTS 2,3 OCT.
Now you know how to expand each of these 3Ã—3 matrices along a row or a column. If you do
so, you will get âˆ’12 assuming you make no mistakes. You could expand this matrix along
any row or any column and assuming you make no mistakes, you will always get the same
thing which is deï¬ned to be the determinant of the matrix, A. This method of evaluating
a determinant by expanding along a row or a column is called the method of Laplace
expansion.
Note that each of the four terms above involves three terms consisting of determinants
of 2Ã—2 matrices and each of these will need 2 terms. Therefore, there will be 4Ã—3Ã—2 = 24
terms to evaluate in order to ï¬nd the determinant using the method of Laplace expansion.
Suppose now you have a 10 Ã— 10 matrix and you follow the above pattern for evaluating
determinants. By analogy to the above, there will be 10! = 3, 628 , 800 terms involved in
the evaluation of such a determinant by Laplace expansion along a row or column. This is
a lot of terms.
In addition to the diï¬ƒculties just discussed, you should regard the above claim that you
always get the same answer by picking any row or column with considerable skepticism. It
is incredible and not at all obvious. However, it requires a little eï¬€ort to establish it. This
is done in the section on the theory of the determinant. The above examples motivate the
following deï¬nitions, the second of which is incredible.
Deï¬nition 11.1.12 Let A = (aij) be an nÃ—n matrix and suppose the determinant
of a (n âˆ’1) Ã— (n âˆ’1) matrix has been deï¬ned. Then a new matrix called the cofactor
matrix, cof (A) is deï¬ned by cof (A) = (cij) where to obtain cij delete the ith row and
the jth column of A, take the determinant of the (n âˆ’1) Ã— (n âˆ’1) matrix which results,
(This is called the ijth minor of A. ) and then multiply this number by (âˆ’1)i+j. Thus
(âˆ’1)i+j Ã—
Â¡
the ijth minor
Â¢
equals the ijth cofactor. To make the formulas easier to remem-
ber, cof (A)ij will denote the ijth entry of the cofactor matrix.
With this deï¬nition of the cofactor matrix, here is how to deï¬ne the determinant of an
n Ã— n matrix.
Deï¬nition 11.1.13 Let A be an n Ã— n matrix where n â‰¥2 and suppose the deter-
minant of an (n âˆ’1) Ã— (n âˆ’1) has been deï¬ned. Then
det (A) =
n
X
j=1
aij cof (A)ij =
n
X
i=1
aij cof (A)ij .
(11.1)
The ï¬rst formula consists of expanding the determinant along the ith row and the second
expands the determinant along the jth column.
Theorem 11.1.14 Expanding the n Ã— n matrix along any row or column always
gives the same answer so the above deï¬nition is a good deï¬nition.
11.1.2
The Determinant Of A Triangular Matrix
Notwithstanding the diï¬ƒculties involved in using the method of Laplace expansion, certain
types of matrices are very easy to deal with.
Deï¬nition 11.1.15 A matrix M, is upper triangular if Mij = 0 whenever i > j.
Thus such a matrix equals zero below the main diagonal, the entries of the form Mii, as
shown.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
Â· Â· Â·
âˆ—
0
âˆ—
...
...
...
...
...
âˆ—
0
Â· Â· Â·
0
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸

11.1.
BASIC TECHNIQUES AND PROPERTIES
201
A lower triangular matrix is deï¬ned similarly as a matrix for which all entries above the
main diagonal are equal to zero.
You should verify the following using the above theorem on Laplace expansion.
Corollary 11.1.16 Let M be an upper (lower) triangular matrix.
Then det (M) is
obtained by taking the product of the entries on the main diagonal.
Example 11.1.17 Let
A =
ï£«
ï£¬
ï£¬
ï£­
1
2
3
77
0
2
6
7
0
0
3
33.7
0
0
0
âˆ’1
ï£¶
ï£·
ï£·
ï£¸
Find det (A) .
From the above corollary, it suï¬ƒces to take the product of the diagonal elements. Thus
det (A) = 1 Ã— 2 Ã— 3 Ã— (âˆ’1) = âˆ’6. Without using the corollary, you could expand along the
ï¬rst column. This gives
1
Â¯Â¯Â¯Â¯Â¯Â¯
2
6
7
0
3
33.7
0
0
âˆ’1
Â¯Â¯Â¯Â¯Â¯Â¯
+ 0 (âˆ’1)2+1
Â¯Â¯Â¯Â¯Â¯Â¯
2
3
77
0
3
33.7
0
0
âˆ’1
Â¯Â¯Â¯Â¯Â¯Â¯
+
0 (âˆ’1)3+1
Â¯Â¯Â¯Â¯Â¯Â¯
2
3
77
2
6
7
0
0
âˆ’1
Â¯Â¯Â¯Â¯Â¯Â¯
+ 0 (âˆ’1)4+1
Â¯Â¯Â¯Â¯Â¯Â¯
2
3
77
2
6
7
0
3
33.7
Â¯Â¯Â¯Â¯Â¯Â¯
and the only nonzero term in the expansion is
1
Â¯Â¯Â¯Â¯Â¯Â¯
2
6
7
0
3
33.7
0
0
âˆ’1
Â¯Â¯Â¯Â¯Â¯Â¯
.
Now expand this along the ï¬rst column to obtain
1 Ã—
Âµ
2 Ã—
Â¯Â¯Â¯Â¯
3
33.7
0
âˆ’1
Â¯Â¯Â¯Â¯ + 0 (âˆ’1)2+1
Â¯Â¯Â¯Â¯
6
7
0
âˆ’1
Â¯Â¯Â¯Â¯ + 0 (âˆ’1)3+1
Â¯Â¯Â¯Â¯
6
7
3
33.7
Â¯Â¯Â¯Â¯
Â¶
= 1 Ã— 2 Ã—
Â¯Â¯Â¯Â¯
3
33.7
0
âˆ’1
Â¯Â¯Â¯Â¯
Next expand this last determinant along the ï¬rst column to obtain the above equals
1 Ã— 2 Ã— 3 Ã— (âˆ’1) = âˆ’6
which is just the product of the entries down the main diagonal of the original matrix.
11.1.3
Properties Of Determinants
There are many properties satisï¬ed by determinants. Some of these properties have to do
with row operations. Recall the row operations.
Deï¬nition 11.1.18 The row operations consist of the following
1. Switch two rows.

202
DETERMINANTS 2,3 OCT.
2. Multiply a row by a nonzero number.
3. Replace a row by a multiple of another row added to itself.
Theorem 11.1.19 Let A be an n Ã— n matrix and let A1 be a matrix which results
from multiplying some row of A by a scalar, c. Then c det (A) = det (A1).
Example 11.1.20 Let A =
Âµ
1
2
3
4
Â¶
, A1 =
Âµ
2
4
3
4
Â¶
. det (A) = âˆ’2, det (A1) = âˆ’4.
Theorem 11.1.21 Let A be an n Ã— n matrix and let A1 be a matrix which results
from switching two rows of A. Then det (A) = âˆ’det (A1) . Also, if one row of A is a multiple
of another row of A, then det (A) = 0.
Example 11.1.22 Let A =
Âµ
1
2
3
4
Â¶
and let A1 =
Âµ
3
4
1
2
Â¶
. det A = âˆ’2, det (A1) = 2.
Theorem 11.1.23 Let A be an n Ã— n matrix and let A1 be a matrix which results
from applying row operation 3. That is you replace some row by a multiple of another row
added to itself. Then det (A) = det (A1).
Example 11.1.24 Let A =
Âµ
1
2
3
4
Â¶
and let A1 =
Âµ
1
2
4
6
Â¶
. Thus the second row of A1
is one times the ï¬rst row added to the second row. det (A) = âˆ’2 and det (A1) = âˆ’2.
Theorem 11.1.25 In Theorems 11.1.19 - 11.1.23 you can replace the word, â€œrowâ€
with the word â€œcolumnâ€.
There are two other major properties of determinants which do not involve row opera-
tions.
Theorem 11.1.26 Let A and B be two n Ã— n matrices. Then
det (AB) = det (A) det (B).
Also,
det (A) = det
Â¡
AT Â¢
.
Example 11.1.27 Compare det (AB) and det (A) det (B) for
A =
Âµ
1
2
âˆ’3
2
Â¶
, B =
Âµ
3
2
4
1
Â¶
.
First
AB =
Âµ
1
2
âˆ’3
2
Â¶ Âµ
3
2
4
1
Â¶
=
Âµ
11
4
âˆ’1
âˆ’4
Â¶
and so
det (AB) = det
Âµ
11
4
âˆ’1
âˆ’4
Â¶
= âˆ’40.
Now
det (A) = det
Âµ
1
2
âˆ’3
2
Â¶
= 8
and
det (B) = det
Âµ
3
2
4
1
Â¶
= âˆ’5.
Thus det (A) det (B) = 8 Ã— (âˆ’5) = âˆ’40.

11.1.
BASIC TECHNIQUES AND PROPERTIES
203
11.1.4
Finding Determinants Using Row Operations
Theorems 11.1.23 - 11.1.25 can be used to ï¬nd determinants using row operations.
As pointed out above, the method of Laplace expansion will not be practical for any
matrix of large size. Here is an example in which all the row operations are used.
Example 11.1.28 Find the determinant of the matrix,
A =
ï£«
ï£¬
ï£¬
ï£­
1
2
3
4
5
1
2
3
4
5
4
3
2
2
âˆ’4
5
ï£¶
ï£·
ï£·
ï£¸
Replace the second row by (âˆ’5) times the ï¬rst row added to it. Then replace the third
row by (âˆ’4) times the ï¬rst row added to it. Finally, replace the fourth row by (âˆ’2) times
the ï¬rst row added to it. This yields the matrix,
B =
ï£«
ï£¬
ï£¬
ï£­
1
2
3
4
0
âˆ’9
âˆ’13
âˆ’17
0
âˆ’3
âˆ’8
âˆ’13
0
âˆ’2
âˆ’10
âˆ’3
ï£¶
ï£·
ï£·
ï£¸
and from Theorem 11.1.23, it has the same determinant as A. Now using other row opera-
tions, det (B) =
Â¡ âˆ’1
3
Â¢
det (C) where
C =
ï£«
ï£¬
ï£¬
ï£­
1
2
3
4
0
0
11
22
0
âˆ’3
âˆ’8
âˆ’13
0
6
30
9
ï£¶
ï£·
ï£·
ï£¸.
The second row was replaced by (âˆ’3) times the third row added to the second row. By
Theorem 11.1.23 this didnâ€™t change the value of the determinant. Then the last row was
multiplied by (âˆ’3) . By Theorem 11.1.19 the resulting matrix has a determinant which is
(âˆ’3) times the determinant of the unmultiplied matrix. Therefore, we multiplied by âˆ’1/3
to retain the correct value. Now replace the last row with 2 times the third added to it.
This does not change the value of the determinant by Theorem 11.1.23. Finally switch
the third and second rows. This causes the determinant to be multiplied by (âˆ’1) . Thus
det (C) = âˆ’det (D) where
D =
ï£«
ï£¬
ï£¬
ï£­
1
2
3
4
0
âˆ’3
âˆ’8
âˆ’13
0
0
11
22
0
0
14
âˆ’17
ï£¶
ï£·
ï£·
ï£¸
You could do more row operations or you could note that this can be easily expanded along
the ï¬rst column followed by expanding the 3Ã—3 matrix which results along its ï¬rst column.
Thus
det (D) = 1 (âˆ’3)
Â¯Â¯Â¯Â¯
11
22
14
âˆ’17
Â¯Â¯Â¯Â¯ = 1485
and so det (C) = âˆ’1485 and det (A) = det (B) =
Â¡ âˆ’1
3
Â¢
(âˆ’1485) = 495.
Example 11.1.29 Find the determinant of the matrix
ï£«
ï£¬
ï£¬
ï£­
1
2
3
2
1
âˆ’3
2
1
2
1
2
5
3
âˆ’4
1
2
ï£¶
ï£·
ï£·
ï£¸

204
DETERMINANTS 2,3 OCT.
Replace the second row by (âˆ’1) times the ï¬rst row added to it. Next take âˆ’2 times the
ï¬rst row and add to the third and ï¬nally take âˆ’3 times the ï¬rst row and add to the last
row. This yields
ï£«
ï£¬
ï£¬
ï£­
1
2
3
2
0
âˆ’5
âˆ’1
âˆ’1
0
âˆ’3
âˆ’4
1
0
âˆ’10
âˆ’8
âˆ’4
ï£¶
ï£·
ï£·
ï£¸.
By Theorem 11.1.23 this matrix has the same determinant as the original matrix. Remember
you can work with the columns also. Take âˆ’5 times the last column and add to the second
column. This yields
ï£«
ï£¬
ï£¬
ï£­
1
âˆ’8
3
2
0
0
âˆ’1
âˆ’1
0
âˆ’8
âˆ’4
1
0
10
âˆ’8
âˆ’4
ï£¶
ï£·
ï£·
ï£¸
By Theorem 11.1.25 this matrix has the same determinant as the original matrix. Now take
(âˆ’1) times the third row and add to the top row. This gives.
ï£«
ï£¬
ï£¬
ï£­
1
0
7
1
0
0
âˆ’1
âˆ’1
0
âˆ’8
âˆ’4
1
0
10
âˆ’8
âˆ’4
ï£¶
ï£·
ï£·
ï£¸
which by Theorem 11.1.23 has the same determinant as the original matrix. Lets expand
it now along the ï¬rst column. This yields the following for the determinant of the original
matrix.
det
ï£«
ï£­
0
âˆ’1
âˆ’1
âˆ’8
âˆ’4
1
10
âˆ’8
âˆ’4
ï£¶
ï£¸
which equals
8 det
Âµ
âˆ’1
âˆ’1
âˆ’8
âˆ’4
Â¶
+ 10 det
Âµ
âˆ’1
âˆ’1
âˆ’4
1
Â¶
= âˆ’82
We suggest you do not try to be fancy in using row operations. That is, stick mostly to
the one which replaces a row or column with a multiple of another row or column added to
it. Also note there is no way to check your answer other than working the problem more
than one way. To be sure you have gotten it right you must do this.
11.1.5
A Formula For The Inverse
The deï¬nition of the determinant in terms of Laplace expansion along a row or column
also provides a way to give a formula for the inverse of a matrix. Recall the deï¬nition of
the inverse of a matrix. Also recall the deï¬nition of the cofactor matrix given in Deï¬nition
11.1.12 on Page 200. This cofactor matrix was just the matrix which results from replacing
the ijth entry of the matrix with the ijth cofactor.
The following theorem says that to ï¬nd the inverse, take the transpose of the cofactor
matrix and divide by the determinant. The transpose of the cofactor matrix is called the
adjugate or sometimes the classical adjoint of the matrix A. In other words, Aâˆ’1 is
equal to one divided by the determinant of A times the adjugate matrix of A. This is what
the following theorem says with more precision.

11.1.
BASIC TECHNIQUES AND PROPERTIES
205
Theorem 11.1.30 Aâˆ’1 exists if and only if det(A) Ì¸= 0. If det(A) Ì¸= 0, then
Aâˆ’1 =
Â¡
aâˆ’1
ij
Â¢
where
aâˆ’1
ij = det(A)âˆ’1 cof (A)ji
for cof (A)ij the ijth cofactor of A.
Example 11.1.31 Find the inverse of the matrix,
A =
ï£«
ï£­
1
2
3
3
0
1
1
2
1
ï£¶
ï£¸
First ï¬nd the determinant of this matrix. Using Theorems 11.1.23 - 11.1.25 on Page 202,
the determinant of this matrix equals the determinant of the matrix,
ï£«
ï£­
1
2
3
0
âˆ’6
âˆ’8
0
0
âˆ’2
ï£¶
ï£¸
which equals 12. The cofactor matrix of A is
ï£«
ï£­
âˆ’2
âˆ’2
6
4
âˆ’2
0
2
8
âˆ’6
ï£¶
ï£¸.
Each entry of A was replaced by its cofactor. Therefore, from the above theorem, the inverse
of A should equal
1
12
ï£«
ï£­
âˆ’2
âˆ’2
6
4
âˆ’2
0
2
8
âˆ’6
ï£¶
ï£¸
T
=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
6
1
3
1
6
âˆ’1
6
âˆ’1
6
2
3
1
2
0
âˆ’1
2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Does it work? You should check to see if it does. When the matrices are multiplied
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
6
1
3
1
6
âˆ’1
6
âˆ’1
6
2
3
1
2
0
âˆ’1
2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£­
1
2
3
3
0
1
1
2
1
ï£¶
ï£¸=
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
and so it is correct.
Example 11.1.32 Find the inverse of the matrix,
A =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
2
0
1
2
âˆ’1
6
1
3
âˆ’1
2
âˆ’5
6
2
3
âˆ’1
2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸

206
DETERMINANTS 2,3 OCT.
First ï¬nd its determinant. This determinant is 1
6. The inverse is therefore equal to
6
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Â¯Â¯Â¯Â¯Â¯Â¯
1
3
âˆ’1
2
2
3
âˆ’1
2
Â¯Â¯Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯Â¯Â¯
âˆ’1
6
âˆ’1
2
âˆ’5
6
âˆ’1
2
Â¯Â¯Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯Â¯Â¯
âˆ’1
6
1
3
âˆ’5
6
2
3
Â¯Â¯Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯Â¯Â¯
0
1
2
2
3
âˆ’1
2
Â¯Â¯Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯Â¯Â¯
1
2
1
2
âˆ’5
6
âˆ’1
2
Â¯Â¯Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯Â¯Â¯
1
2
0
âˆ’5
6
2
3
Â¯Â¯Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯Â¯Â¯
0
1
2
1
3
âˆ’1
2
Â¯Â¯Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯Â¯Â¯
1
2
1
2
âˆ’1
6
âˆ’1
2
Â¯Â¯Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯Â¯Â¯
1
2
0
âˆ’1
6
1
3
Â¯Â¯Â¯Â¯Â¯Â¯
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
T
.
Expanding all the 2 Ã— 2 determinants this yields
6
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
6
1
3
1
6
1
3
1
6
âˆ’1
3
âˆ’1
6
1
6
1
6
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
T
=
ï£«
ï£­
1
2
âˆ’1
2
1
1
1
âˆ’2
1
ï£¶
ï£¸
Always check your work.
ï£«
ï£­
1
2
âˆ’1
2
1
1
1
âˆ’2
1
ï£¶
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
2
0
1
2
âˆ’1
6
1
3
âˆ’1
2
âˆ’5
6
2
3
âˆ’1
2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
and so we got it right. If the result of multiplying these matrices had been something other
than the identity matrix, you would know there was an error. When this happens, you
need to search for the mistake if you am interested in getting the right answer. A common
mistake is to forget to take the transpose of the cofactor matrix.
Proof of Theorem 11.1.30: From the deï¬nition of the determinant in terms of ex-
pansion along a column, and letting (air) = A, if det (A) Ì¸= 0,
n
X
i=1
air cof (A)ir det(A)âˆ’1 = det(A) det(A)âˆ’1 = 1.
Now consider
n
X
i=1
air cof (A)ik det(A)âˆ’1
when k Ì¸= r. Replace the kth column with the rth column to obtain a matrix, Bk whose
determinant equals zero by Theorem 11.1.21. However, expanding this matrix, Bk along
the kth column yields
0 = det (Bk) det (A)âˆ’1 =
n
X
i=1
air cof (A)ik det (A)âˆ’1
Summarizing,
n
X
i=1
air cof (A)ik det (A)âˆ’1 = Î´rk â‰¡
Â½ 1 if r = k
0 if r Ì¸= k
.

11.1.
BASIC TECHNIQUES AND PROPERTIES
207
Now
n
X
i=1
air cof (A)ik =
n
X
i=1
air cof (A)T
ki
which is the krth entry of cof (A)T A. Therefore,
cof (A)T
det (A) A = I.
(11.2)
Using the other formula in Deï¬nition 11.1.13, and similar reasoning,
n
X
j=1
arj cof (A)kj det (A)âˆ’1 = Î´rk
Now
n
X
j=1
arj cof (A)kj =
n
X
j=1
arj cof (A)T
jk
which is the rkth entry of A cof (A)T . Therefore,
Acof (A)T
det (A) = I,
(11.3)
and it follows from 11.2 and 11.3 that Aâˆ’1 =
Â¡
aâˆ’1
ij
Â¢
, where
aâˆ’1
ij = cof (A)ji det (A)âˆ’1 .
In other words,
Aâˆ’1 = cof (A)T
det (A) .
Now suppose Aâˆ’1 exists. Then by Theorem 11.1.26,
1 = det (I) = det
Â¡
AAâˆ’1Â¢
= det (A) det
Â¡
Aâˆ’1Â¢
so det (A) Ì¸= 0. This proves the theorem.
This way of ï¬nding inverses is especially useful in the case where it is desired to ï¬nd the
inverse of a matrix whose entries are functions.
Example 11.1.33 Suppose
A (t) =
ï£«
ï£­
et
0
0
0
cos t
sin t
0
âˆ’sin t
cos t
ï£¶
ï£¸
Show that A (t)âˆ’1 exists and then ï¬nd it.
First note det (A (t)) = et Ì¸= 0 so A (t)âˆ’1 exists. The cofactor matrix is
C (t) =
ï£«
ï£­
1
0
0
0
et cos t
et sin t
0
âˆ’et sin t
et cos t
ï£¶
ï£¸
and so the inverse is
1
et
ï£«
ï£­
1
0
0
0
et cos t
et sin t
0
âˆ’et sin t
et cos t
ï£¶
ï£¸
T
=
ï£«
ï£­
eâˆ’t
0
0
0
cos t
âˆ’sin t
0
sin t
cos t
ï£¶
ï£¸.

208
DETERMINANTS 2,3 OCT.

Eigenvalues And Eigenvectors
Of A Matrix 4-6 Oct.
Quiz
1. Here is a matrix.
ï£«
ï£­
1
0
2
2
1
3
3
2
1
ï£¶
ï£¸
Find its determinant.
2. Use the theory of determinants to ï¬nd the inverse of the matrix,
ï£«
ï£­
2
1
1
1
0
1
0
1
1
ï£¶
ï£¸
3. Let C = F T F where F is an n Ã— n real matrix. Show det (C) â‰¥0.
4. Show that if Aâˆ’1 exists, then det
Â¡
Aâˆ’1Â¢
= 1/ det (A).
Spectral Theory refers to the study of eigenvalues and eigenvectors of a matrix. It is of
fundamental importance in many areas. Row operations will no longer be such a useful tool
in this subject.
12.0.6
Deï¬nition Of Eigenvectors And Eigenvalues
In this section, F = C.
To illustrate the idea behind what will be discussed, consider the following example.
Example 12.0.34 Here is a matrix.
ï£«
ï£­
0
5
âˆ’10
0
22
16
0
âˆ’9
âˆ’2
ï£¶
ï£¸.
Multiply this matrix by the vector
ï£«
ï£­
âˆ’5
âˆ’4
3
ï£¶
ï£¸
209

210
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
and see what happens. Then multiply it by
ï£«
ï£­
1
0
0
ï£¶
ï£¸
and see what happens. Does this matrix act this way for some other vector?
First
ï£«
ï£­
0
5
âˆ’10
0
22
16
0
âˆ’9
âˆ’2
ï£¶
ï£¸
ï£«
ï£­
âˆ’5
âˆ’4
3
ï£¶
ï£¸=
ï£«
ï£­
âˆ’50
âˆ’40
30
ï£¶
ï£¸= 10
ï£«
ï£­
âˆ’5
âˆ’4
3
ï£¶
ï£¸.
Next
ï£«
ï£­
0
5
âˆ’10
0
22
16
0
âˆ’9
âˆ’2
ï£¶
ï£¸
ï£«
ï£­
1
0
0
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸= 0
ï£«
ï£­
1
0
0
ï£¶
ï£¸.
When you multiply the ï¬rst vector by the given matrix, it stretched the vector, multiplying
it by 10. When you multiplied the matrix by the second vector it sent it to the zero vector.
Now consider
ï£«
ï£­
0
5
âˆ’10
0
22
16
0
âˆ’9
âˆ’2
ï£¶
ï£¸
ï£«
ï£­
1
1
1
ï£¶
ï£¸=
ï£«
ï£­
âˆ’5
38
âˆ’11
ï£¶
ï£¸.
In this case, multiplication by the matrix did not result in merely multiplying the vector by
a number.
In the above example, the ï¬rst two vectors were called eigenvectors and the numbers, 10
and 0 are called eigenvalues. Not every number is an eigenvalue and not every vector is an
eigenvector.
Deï¬nition 12.0.35 Let M be an nÃ—n matrix and let x âˆˆCn be a nonzero vector
for which
Mx = Î»x
(12.1)
for some scalar, Î». Then x is called an eigenvector and Î» is called an eigenvalue (char-
acteristic value) of the matrix, M.
Note: Eigenvectors are never equal to zero!
The set of all eigenvalues of an n Ã— n matrix, M, is denoted by Ïƒ (M) and is referred to as
the spectrum of M.
The eigenvectors of a matrix M are those vectors, x for which multiplication by M
results in a scalar multiple of x. Since the zero vector, 0 has no direction this would make
no sense for the zero vector. As noted above, 0 is never allowed to be an eigenvector. How
can eigenvectors and eigenvalues be identiï¬ed?
There is an important characterization of when a matrix is invertible in terms of deter-
minants. This is proved completely in the section on the theory of determinants where a
formula is given for the inverse in terms of the determinant and cofactors.
Theorem 12.0.36 Let M be an n Ã— n matrix and let TM denote the linear trans-
formation determined by M. Thus TMx = Mx. Then the following are equivalent.
1. TM is one to one.

211
2. TM is onto.
3. det (M) Ì¸= 0.
Suppose x satisï¬es 12.1. Then
(M âˆ’Î»I) x = 0
for some x Ì¸= 0. (Equivalently, you could write (Î»I âˆ’M) x = 0.) Sometimes we will use
(Î»I âˆ’M) x = 0 and sometimes (M âˆ’Î»I) x = 0. It makes absolutely no diï¬€erence and you
should use whichever you like better. Therefore, the matrix M âˆ’Î»I cannot have an inverse
because if it did, the equation could be solved,
x =
Â³
(M âˆ’Î»I)âˆ’1 (M âˆ’Î»I)
Â´
x = (M âˆ’Î»I)âˆ’1 ((M âˆ’Î»I) x) = (M âˆ’Î»I)âˆ’1 0 = 0,
and this would require x = 0, contrary to the requirement that x Ì¸= 0. By Theorem 12.0.36,
det (M âˆ’Î»I) = 0.
(12.2)
(Equivalently you could write det (Î»I âˆ’M) = 0.) The expression, det (Î»I âˆ’M) or equiva-
lently, det (M âˆ’Î»I) is a polynomial called the characteristic polynomial and the above
equation is called the characteristic equation. For M an n Ã— n matrix, it follows from the
theorem on expanding a matrix by its cofactor that det (M âˆ’Î»I) is a polynomial of degree
n. As such, the equation, 12.2 has a solution, Î» âˆˆC by the fundamental theorem of algebra.
Is it actually an eigenvalue? The answer is yes by Theorem 12.0.36. Since Î»I âˆ’M has no
inverse due to its determinant equaling zero, it must fail to be one to one and so there must
exist a nonzero vector which it maps to zero. This proves the following corollary.
Corollary 12.0.37 Let M be an n Ã— n matrix and det (M âˆ’Î»I) = 0. Then there exists
a nonzero vector, x âˆˆCn such that (M âˆ’Î»I) x = 0.
12.0.7
Finding Eigenvectors And Eigenvalues
As an example, consider the following.
Example 12.0.38 Find the eigenvalues and eigenvectors for the matrix,
A =
ï£«
ï£­
5
âˆ’10
âˆ’5
2
14
2
âˆ’4
âˆ’8
6
ï£¶
ï£¸.
You ï¬rst need to identify the eigenvalues. Recall this requires the solution of the equation
det (A âˆ’Î»I) = 0.
In this case this equation is
det
ï£«
ï£­
ï£«
ï£­
5
âˆ’10
âˆ’5
2
14
2
âˆ’4
âˆ’8
6
ï£¶
ï£¸âˆ’Î»
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
ï£¶
ï£¸= 0
When you expand this determinant and simplify, you ï¬nd the equation you need to solve is
(Î» âˆ’5)
Â¡
Î»2 âˆ’20Î» + 100
Â¢
= 0

212
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
and so the eigenvalues are
5, 10, 10.
We have listed 10 twice because it is a zero of multiplicity two due to
Î»2 âˆ’20Î» + 100 = (Î» âˆ’10)2 .
Having found the eigenvalues, it only remains to ï¬nd the eigenvectors. First ï¬nd the
eigenvectors for Î» = 5. As explained above, this requires you to solve the equation,
ï£«
ï£­
ï£«
ï£­
5
âˆ’10
âˆ’5
2
14
2
âˆ’4
âˆ’8
6
ï£¶
ï£¸âˆ’5
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸.
That is you need to ï¬nd the solution to
ï£«
ï£­
0
âˆ’10
âˆ’5
2
9
2
âˆ’4
âˆ’8
1
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸
By now this is an old problem. You set up the augmented matrix and row reduce to get the
solution. Thus the matrix you must row reduce is
ï£«
ï£­
0
âˆ’10
âˆ’5
|
0
2
9
2
|
0
âˆ’4
âˆ’8
1
|
0
ï£¶
ï£¸.
(12.3)
The row reduced echelon form is
ï£«
ï£¬
ï£­
1
0
âˆ’5
4
|
0
0
1
1
2
|
0
0
0
0
|
0
ï£¶
ï£·
ï£¸
and so the solution is any vector of the form
ï£«
ï£¬
ï£­
5
4t
âˆ’1
2 t
t
ï£¶
ï£·
ï£¸= t
ï£«
ï£¬
ï£­
5
4
âˆ’1
2
1
ï£¶
ï£·
ï£¸
where t âˆˆF. You would obtain the same collection of vectors if you replaced t with 4t. Thus
a simpler description for the solutions to this system of equations whose augmented matrix
is in 12.3 is
t
ï£«
ï£­
5
âˆ’2
4
ï£¶
ï£¸
(12.4)
where t âˆˆF. Now you need to remember that you canâ€™t take t = 0 because this would result
in the zero vector and
Eigenvectors are never equal to zero!
Other than this value, every other choice of z in 12.4 results in an eigenvector. It is a good
idea to check your work! To do so, we will take the original matrix and multiply by this
vector and see if we get 5 times this vector.
ï£«
ï£­
5
âˆ’10
âˆ’5
2
14
2
âˆ’4
âˆ’8
6
ï£¶
ï£¸
ï£«
ï£­
5
âˆ’2
4
ï£¶
ï£¸=
ï£«
ï£­
25
âˆ’10
20
ï£¶
ï£¸= 5
ï£«
ï£­
5
âˆ’2
4
ï£¶
ï£¸

213
so it appears this is correct. Always check your work on these problems if you care about
getting the answer right.
The parameter, t is sometimes called a free variable. The set of vectors in 12.4 is
called the eigenspace and it equals ker (A âˆ’Î»I) . You should observe that in this case
the eigenspace has dimension 1 because the eigenspace is the span of a single vector. In
general, you obtain the solution from the row echelon form and the number of diï¬€erent free
variables gives you the dimension of the eigenspace. Just remember that not every vector
in the eigenspace is an eigenvector. The vector, 0 is not an eigenvector although it is in the
eigenspace because
Eigenvectors are never equal to zero!
Next consider the eigenvectors for Î» = 10. These vectors are solutions to the equation,
ï£«
ï£­
ï£«
ï£­
5
âˆ’10
âˆ’5
2
14
2
âˆ’4
âˆ’8
6
ï£¶
ï£¸âˆ’10
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸
That is you must ï¬nd the solutions to
ï£«
ï£­
âˆ’5
âˆ’10
âˆ’5
2
4
2
âˆ’4
âˆ’8
âˆ’4
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸
which reduces to consideration of the augmented matrix,
ï£«
ï£­
âˆ’5
âˆ’10
âˆ’5
|
0
2
4
2
|
0
âˆ’4
âˆ’8
âˆ’4
|
0
ï£¶
ï£¸
The row reduced echelon form for this matrix is
ï£«
ï£­
1
2
1
0
0
0
0
0
0
0
0
0
ï£¶
ï£¸
and so the eigenvectors are of the form
ï£«
ï£­
âˆ’2s âˆ’t
s
t
ï£¶
ï£¸= s
ï£«
ï£­
âˆ’2
1
0
ï£¶
ï£¸+ t
ï£«
ï£­
âˆ’1
0
1
ï£¶
ï£¸.
You canâ€™t pick t and s both equal to zero because this would result in the zero vector and
Eigenvectors are never equal to zero!
However, every other choice of t and s does result in an eigenvector for the eigenvalue Î» = 10.
As in the case for Î» = 5 you should check your work if you care about getting it right.
ï£«
ï£­
5
âˆ’10
âˆ’5
2
14
2
âˆ’4
âˆ’8
6
ï£¶
ï£¸
ï£«
ï£­
âˆ’1
0
1
ï£¶
ï£¸=
ï£«
ï£­
âˆ’10
0
10
ï£¶
ï£¸= 10
ï£«
ï£­
âˆ’1
0
1
ï£¶
ï£¸
so it worked. The other vector will also work. Check it.

214
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
12.0.8
A Warning
The above example shows how to ï¬nd eigenvectors and eigenvalues algebraically. You may
have noticed it is a bit long. Sometimes students try to ï¬rst row reduce the matrix be-
fore looking for eigenvalues. This is a terrible idea because row operations destroy the
eigenvalues. The eigenvalue problem is really not about row operations.
The general eigenvalue problem is the hardest problem in algebra and people still do
research on ways to ï¬nd eigenvalues and their eigenvectors. If you are doing anything which
would yield a way to ï¬nd eigenvalues and eigenvectors for general matrices without too
much trouble, the thing you are doing will certainly be wrong. The problems you will see
in these notes are not too hard because they are cooked up by us to be easy. Later we
will describe general methods to compute eigenvalues and eigenvectors numerically. These
methods work even when the problem is not cooked up to be easy.
If you are so fortunate as to ï¬nd the eigenvalues as in the above example, then ï¬nding the
eigenvectors does reduce to row operations and this part of the problem is easy. However,
ï¬nding the eigenvalues along with the eigenvectors is anything but easy because for an
n Ã— n matrix, it involves solving a polynomial equation of degree n. If you only ï¬nd a good
approximation to the eigenvalue, it wonâ€™t work. It either is or is not an eigenvalue and
if it is not, the only solution to the equation, (M âˆ’Î»I) x = 0 will be the zero solution as
explained above and
Eigenvectors are never equal to zero!
Here is another example.
Example 12.0.39 Let
A =
ï£«
ï£­
2
2
âˆ’2
1
3
âˆ’1
âˆ’1
1
1
ï£¶
ï£¸
First ï¬nd the eigenvalues.
det
ï£«
ï£­
ï£«
ï£­
2
2
âˆ’2
1
3
âˆ’1
âˆ’1
1
1
ï£¶
ï£¸âˆ’Î»
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
ï£¶
ï£¸= 0
This reduces to Î»3 âˆ’6Î»2 + 8Î» = 0 and the solutions are 0, 2, and 4.
0 Can be an Eigenvalue!
Now ï¬nd the eigenvectors. For Î» = 0 the augmented matrix for ï¬nding the solutions is
ï£«
ï£­
2
2
âˆ’2
|
0
1
3
âˆ’1
|
0
âˆ’1
1
1
|
0
ï£¶
ï£¸
and the row reduced echelon form is
ï£«
ï£­
1
0
âˆ’1
0
0
1
0
0
0
0
0
0
ï£¶
ï£¸

215
Therefore, the eigenvectors are of the form
t
ï£«
ï£­
1
0
1
ï£¶
ï£¸
where t Ì¸= 0.
Next ï¬nd the eigenvectors for Î» = 2. The augmented matrix for the system of equations
needed to ï¬nd these eigenvectors is
ï£«
ï£­
0
2
âˆ’2
|
0
1
1
âˆ’1
|
0
âˆ’1
1
âˆ’1
|
0
ï£¶
ï£¸
and the row reduced echelon form is
ï£«
ï£­
1
0
0
0
0
1
âˆ’1
0
0
0
0
0
ï£¶
ï£¸
and so the eigenvectors are of the form
t
ï£«
ï£­
0
1
1
ï£¶
ï£¸
where t Ì¸= 0.
Finally ï¬nd the eigenvectors for Î» = 4. The augmented matrix for the system of equations
needed to ï¬nd these eigenvectors is
ï£«
ï£­
âˆ’2
2
âˆ’2
|
0
1
âˆ’1
âˆ’1
|
0
âˆ’1
1
âˆ’3
|
0
ï£¶
ï£¸
and the row reduced echelon form is
ï£«
ï£­
1
âˆ’1
0
0
0
0
1
0
0
0
0
0
ï£¶
ï£¸.
Therefore, the eigenvectors are of the form
t
ï£«
ï£­
1
1
0
ï£¶
ï£¸
where t Ì¸= 0.
12.0.9
Defective And Nondefective Matrices
Deï¬nition 12.0.40 By the fundamental theorem of algebra, it is possible to write
the characteristic equation in the form
(Î» âˆ’Î»1)r1 (Î» âˆ’Î»2)r2 Â· Â· Â· (Î» âˆ’Î»m)rm = 0
where ri is some integer no smaller than 1. Thus the eigenvalues are Î»1, Î»2, Â· Â· Â·, Î»m. The
algebraic multiplicity of Î»j is deï¬ned to be rj.

216
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Example 12.0.41 Consider the matrix,
A =
ï£«
ï£­
1
1
0
0
1
1
0
0
1
ï£¶
ï£¸
(12.5)
What is the algebraic multiplicity of the eigenvalue Î» = 1?
In this case the characteristic equation is
det (A âˆ’Î»I) = (1 âˆ’Î»)3 = 0
or equivalently,
det (Î»I âˆ’A) = (Î» âˆ’1)3 = 0.
Therefore, Î» is of algebraic multiplicity 3.
Deï¬nition 12.0.42 The geometric multiplicity of an eigenvalue is the dimen-
sion of the eigenspace,
ker (A âˆ’Î»I) .
Example 12.0.43 Find the geometric multiplicity of Î» = 1 for the matrix in 12.5.
We need to solve
ï£«
ï£­
0
1
0
0
0
1
0
0
0
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸.
The augmented matrix which must be row reduced to get this solution is therefore,
ï£«
ï£­
0
1
0
|
0
0
0
1
|
0
0
0
0
|
0
ï£¶
ï£¸
This requires z = y = 0 and x is arbitrary. Thus the eigenspace is
t
ï£«
ï£­
1
0
0
ï£¶
ï£¸, t âˆˆF.
It follows the geometric multiplicity of Î» = 1 is 1.
Deï¬nition 12.0.44 An n Ã— n matrix is called defective if the geometric multi-
plicity is not equal to the algebraic multiplicity for some eigenvalue. Sometimes such an
eigenvalue for which the geometric multiplicity is not equal to the algebraic multiplicity is
called a defective eigenvalue. If the geometric multiplicity for an eigenvalue equals the alge-
braic multiplicity, the eigenvalue is sometimes referred to as nondefective.
Here is another more interesting example of a defective matrix.
Example 12.0.45 Let
A =
ï£«
ï£­
2
âˆ’2
âˆ’1
âˆ’2
âˆ’1
âˆ’2
14
25
14
ï£¶
ï£¸.
Find the eigenvectors and eigenvalues.

217
In this case the eigenvalues are 3, 6, 6 where we have listed 6 twice because it is a zero
of algebraic multiplicity two, the characteristic equation being
(Î» âˆ’3) (Î» âˆ’6)2 = 0.
It remains to ï¬nd the eigenvectors for these eigenvalues. First consider the eigenvectors for
Î» = 3. You must solve
ï£«
ï£­
ï£«
ï£­
2
âˆ’2
âˆ’1
âˆ’2
âˆ’1
âˆ’2
14
25
14
ï£¶
ï£¸âˆ’3
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸.
The augmented matrix is
ï£«
ï£­
âˆ’1
âˆ’2
âˆ’1
|
0
âˆ’2
âˆ’4
âˆ’2
|
0
14
25
11
|
0
ï£¶
ï£¸
and the row reduced echelon form is
ï£«
ï£­
1
0
âˆ’1
0
0
1
1
0
0
0
0
0
ï£¶
ï£¸
so the eigenvectors are nonzero vectors of the form
ï£«
ï£­
t
âˆ’t
t
ï£¶
ï£¸= t
ï£«
ï£­
1
âˆ’1
1
ï£¶
ï£¸
Next consider the eigenvectors for Î» = 6. This requires you to solve
ï£«
ï£­
ï£«
ï£­
2
âˆ’2
âˆ’1
âˆ’2
âˆ’1
âˆ’2
14
25
14
ï£¶
ï£¸âˆ’6
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸
and the augmented matrix for this system of equations is
ï£«
ï£­
âˆ’4
âˆ’2
âˆ’1
|
0
âˆ’2
âˆ’7
âˆ’2
|
0
14
25
8
|
0
ï£¶
ï£¸
The row reduced echelon form is
ï£«
ï£¬
ï£­
1
0
1
8
0
0
1
1
4
0
0
0
0
0
ï£¶
ï£·
ï£¸
and so the eigenvectors for Î» = 6 are of the form
t
ï£«
ï£¬
ï£­
âˆ’1
8
âˆ’1
4
1
ï£¶
ï£·
ï£¸
or written more simply,
t
ï£«
ï£­
âˆ’1
âˆ’2
8
ï£¶
ï£¸

218
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
where t âˆˆF.
Note that in this example the eigenspace for the eigenvalue, Î» = 6 is of dimension 1
because there is only one parameter. However, this eigenvalue is of multiplicity two as a
root to the characteristic equation. Thus this eigenvalue is a defective eigenvalue. However,
the eigenvalue 3 is nondefective. The matrix is defective because it has a defective eigenvalue.
The word, defective, seems to suggest there is something wrong with the matrix. This
is in fact the case. Defective matrices are a lot of trouble in applications and we may wish
they never occurred. However, they do occur as the above example shows. When you study
linear systems of diï¬€erential equations, you will have to deal with the case of defective
matrices and you will see how awful they are. The reason these matrices are so horrible
to work with is that it is impossible to obtain a basis of eigenvectors. When you study
diï¬€erential equations, solutions to ï¬rst order systems are expressed in terms of eigenvectors
of a certain matrix times eÎ»t where Î» is an eigenvalue. In order to obtain a general solution
of this sort, you must have a basis of eigenvectors. For a defective matrix, such a basis does
not exist and so you have to go to something called generalized eigenvectors. Unfortunately,
it is never explained in beginning diï¬€erential equations courses why there are enough
generalized eigenvectors and eigenvectors to represent the general solution. In fact, this
reduces to a diï¬ƒcult question in linear algebra equivalent to the existence of something
called the Jordan Canonical form which is much more diï¬ƒcult than everything discussed in
the entire diï¬€erential equations course. If you become interested in this, see a good book in
linear algebra. The good ones do discuss this topic. There is such a linear algebra book on
my web page.
Ultimately, the algebraic issues which will occur in diï¬€erential equations are a red herring
anyway. The real issues relative to existence of solutions to systems of ordinary diï¬€erential
equations are analytical, having much more to do with calculus than with linear algebra
although this will likely not be made clear when you take a beginning diï¬€erential equations
class.
In terms of algebra, this lack of a basis of eigenvectors says that it is impossible to obtain
a diagonal matrix which is similar to the given matrix.
Although there may be repeated roots to the characteristic equation, 12.2 and it is not
known whether the matrix is defective in this case, there is an important theorem which
holds when considering eigenvectors which correspond to distinct eigenvalues.
Theorem 12.0.46 Suppose Mvi = Î»ivi, i = 1, Â· Â· Â·, r , vi Ì¸= 0, and that if i Ì¸= j,
then Î»i Ì¸= Î»j. Then the set of eigenvectors, {v1, Â· Â· Â·, vr} is linearly independent.
Proof: If the conclusion of this theorem is not true, then there exist non zero scalars,
ckj such that
m
X
j=1
ckjvkj = 0.
(12.6)
Take m to be the smallest number possible for an expression of the form 12.6 to hold. Then
solving for vk1
vk1 =
X
kjÌ¸=k1
dkjvkj
(12.7)
where dkj = ckj/ck1 Ì¸= 0. Multiplying both sides by M,
Î»k1vk1 =
X
kjÌ¸=k1
dkjÎ»kjvkj,
which from 12.7 yields
X
kjÌ¸=k1
dkjÎ»k1vkj =
X
kjÌ¸=k1
dkjÎ»kjvkj

219
and therefore,
0 =
X
kjÌ¸=k1
dkj
Â¡
Î»k1 âˆ’Î»kj
Â¢
vkj,
a sum having fewer than m terms. However, from the assumption that m is as small as
possible for 12.6 to hold with all the scalars, ckj non zero, it follows that for some j Ì¸= 1,
dkj
Â¡
Î»k1 âˆ’Î»kj
Â¢
= 0
which implies Î»k1 = Î»kj, a contradiction.
12.0.10
Diagonalization
Deï¬nition 12.0.47 Let A be an n Ã— n matrix. Then A is diagonalizable if there
exists an invertible matrix, S such that
Sâˆ’1AS = D
where D is a diagonal matrix. This means D has a zero as every entry except for the main
diagonal.
Theorem 12.0.48 An n Ã— n matrix is diagonalizable if and only if Fn has a basis
of eigenvectors of A. Furthermore, you can take the matrix, S described above to be given
as
S =
Â¡
v1
v2
Â· Â· Â·
vn
Â¢
where here the vk are the eigenvectors in the basis for Fn.
If A is diagonalizable, the
eigenvalues of A are the diagonal entries of the diagonal matrix.
Proof: Suppose there exists a basis of eigenvectors, {vk} where Avk = Î»kvk. Then let
S be given as above. It follows Sâˆ’1 exists and is of the form
Sâˆ’1 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
wT
1
wT
2
...
wT
n
ï£¶
ï£·
ï£·
ï£·
ï£¸
where wT
k vj = Î´kj. Then
ï£«
ï£¬
ï£­
Î»1
0
...
0
Î»n
ï£¶
ï£·
ï£¸
=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
wT
1
wT
2
...
wT
n
ï£¶
ï£·
ï£·
ï£·
ï£¸
Â¡ Î»1v1
Î»2v2
Â· Â· Â·
Î»nvn
Â¢
=
ï£«
ï£¬
ï£¬
ï£¬
ï£­
wT
1
wT
2
...
wT
n
ï£¶
ï£·
ï£·
ï£·
ï£¸
Â¡
Av1
Av2
Â· Â· Â·
Avn
Â¢
=
Sâˆ’1AS
Next suppose A is diagonalizable so Sâˆ’1AS = D. Let S =
Â¡
v1
v2
Â· Â· Â·
vn
Â¢
where
the columns are the vk and
D =
ï£«
ï£¬
ï£­
Î»1
0
...
0
Î»n
ï£¶
ï£·
ï£¸

220
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Then
AS = SD =
Â¡
v1
v2
Â· Â· Â·
vn
Â¢
ï£«
ï£¬
ï£­
Î»1
0
...
0
Î»n
ï£¶
ï£·
ï£¸
and so
Â¡
Av1
Av2
Â· Â· Â·
Avn
Â¢
=
Â¡
Î»1v1
Î»2v2
Â· Â· Â·
Î»nvn
Â¢
showing the vi are eigenvectors of A and the Î»k are eigenvectors. Now the vk form a basis
for Fn because the matrix, S having these vectors as columns is given to be invertible. This
proves the theorem.
Deï¬nition 12.0.49 Let A, B be two diagonal matrices. Then A is said to be sim-
ilar to B if there exists an invertible matrix, S such that B = Sâˆ’1AS.
Example 12.0.50 Let A =
ï£«
ï£­
2
0
0
1
4
âˆ’1
âˆ’2
âˆ’4
4
ï£¶
ï£¸. Find a matrix, S such that Sâˆ’1AS = D,
a diagonal matrix.
Solving det (Î»I âˆ’A) = 0 yields the eigenvalues are 2 and 6 with 2 an eigenvalue of mul-
tiplicity two. Solving (2I âˆ’A) x = 0 to ï¬nd the eigenvectors, you ï¬nd that the eigenvectors
are
a
ï£«
ï£­
âˆ’2
1
0
ï£¶
ï£¸+ b
ï£«
ï£­
1
0
1
ï£¶
ï£¸
where a, b are scalars. An eigenvector for Î» = 6 is
ï£«
ï£­
0
1
âˆ’2
ï£¶
ï£¸. Let the matrix S be
S =
ï£«
ï£­
âˆ’2
1
0
1
0
1
0
1
âˆ’2
ï£¶
ï£¸
That is, the columns are the eigenvectors. Then
Sâˆ’1 =
ï£«
ï£­
âˆ’1
4
1
2
1
4
1
2
1
1
2
1
4
1
2
âˆ’1
4
ï£¶
ï£¸.
Sâˆ’1AS
=
ï£«
ï£­
âˆ’1
4
1
2
1
4
1
2
1
1
2
1
4
1
2
âˆ’1
4
ï£¶
ï£¸
ï£«
ï£­
2
0
0
1
4
âˆ’1
âˆ’2
âˆ’4
4
ï£¶
ï£¸
ï£«
ï£­
âˆ’2
1
0
1
0
1
0
1
âˆ’2
ï£¶
ï£¸
=
ï£«
ï£­
2
0
0
0
2
0
0
0
6
ï£¶
ï£¸.
Example 12.0.51 Here is a matrix. A =
ï£«
ï£­
2
1
0
0
1
0
âˆ’1
âˆ’1
1
ï£¶
ï£¸Find A50.

221
Sometimes this sort of problem can be made easy by using diagonalization. In this case
there are eigenvectors,
ï£«
ï£­
0
0
1
ï£¶
ï£¸,
ï£«
ï£­
âˆ’1
1
0
ï£¶
ï£¸,
ï£«
ï£­
âˆ’1
0
1
ï£¶
ï£¸,
the ï¬rst two corresponding to Î» = 1 and the last corresponding to Î» = 2. Then let the
eigenvectors be the columns of the matrix, S. Thus
S =
ï£«
ï£­
0
âˆ’1
âˆ’1
0
1
0
1
0
1
ï£¶
ï£¸
Then also
Sâˆ’1 =
ï£«
ï£­
1
1
1
0
1
0
âˆ’1
âˆ’1
0
ï£¶
ï£¸
and
Sâˆ’1AS
=
ï£«
ï£­
1
1
1
0
1
0
âˆ’1
âˆ’1
0
ï£¶
ï£¸
ï£«
ï£­
2
1
0
0
1
0
âˆ’1
âˆ’1
1
ï£¶
ï£¸
ï£«
ï£­
0
âˆ’1
âˆ’1
0
1
0
1
0
1
ï£¶
ï£¸
=
ï£«
ï£­
1
0
0
0
1
0
0
0
2
ï£¶
ï£¸= D
Now it follows
A = SDSâˆ’1 =
ï£«
ï£­
0
âˆ’1
âˆ’1
0
1
0
1
0
1
ï£¶
ï£¸
ï£«
ï£­
1
0
0
0
1
0
0
0
2
ï£¶
ï£¸
ï£«
ï£­
1
1
1
0
1
0
âˆ’1
âˆ’1
0
ï£¶
ï£¸.
Now note that
Â¡
SDSâˆ’1Â¢2 = SDSâˆ’1SDSâˆ’1 = SD2Sâˆ’1 and
Â¡
SDSâˆ’1Â¢3 = SDSâˆ’1SDSâˆ’1SDSâˆ’1 = SD3Sâˆ’1,
etc. In general, you can see that
Â¡
SDSâˆ’1Â¢n = SDnSâˆ’1
In other words, An = SDnSâˆ’1. Therefore,
A50
=
SD50Sâˆ’1
=
ï£«
ï£­
0
âˆ’1
âˆ’1
0
1
0
1
0
1
ï£¶
ï£¸
ï£«
ï£­
1
0
0
0
1
0
0
0
2
ï£¶
ï£¸
50 ï£«
ï£­
1
1
1
0
1
0
âˆ’1
âˆ’1
0
ï£¶
ï£¸.
Now
ï£«
ï£­
1
0
0
0
1
0
0
0
2
ï£¶
ï£¸
50
=
ï£«
ï£­
1
0
0
0
1
0
0
0
250
ï£¶
ï£¸.

222
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
It follows
A50
=
ï£«
ï£­
0
âˆ’1
âˆ’1
0
1
0
1
0
1
ï£¶
ï£¸
ï£«
ï£­
1
0
0
0
1
0
0
0
250
ï£¶
ï£¸
ï£«
ï£­
1
1
1
0
1
0
âˆ’1
âˆ’1
0
ï£¶
ï£¸
=
ï£«
ï£­
250
âˆ’1 + 250
0
0
1
0
1 âˆ’250
1 âˆ’250
1
ï£¶
ï£¸.
That isnâ€™t too hard. However, this would have been horrendous if you had tried to multiply
A50 by hand.
This technique of diagonalization is also important in solving the diï¬€erential equations
resulting from vibrations. Sometimes you have systems of diï¬€erential equation and when
you diagonalize an appropriate matrix, you â€œdecoupleâ€ the equations. This is very nice. It
makes hard problems trivial.
The above example is entirely typical. If A = SDSâˆ’1 then Am = SDmSâˆ’1 and it is easy
to compute Dm. More generally, you can deï¬ne functions of the matrix using power series
in this way. However, the real interesting case is when A is defective. This is much more
interesting. You can always speak of things like sin (A) for A an n Ã— n matrix. However,
more interesting functions have no power series and you have to work harder for these. This
is enough on this. One can go on and on.
12.0.11
Migration Matrices
There are applications of the eigenvalue problem which are of great importance and feature
only one eigenvalue.
Consider the following table.
A
B
A
1/4
2/3
B
3/4
1/3
In this table, 1/4 is the probability that someone in location A ends up in A after a single
unit of time. 2/3 is the probability that a person in location B ends up in location A after
a single unit of time. 3/4 is the probability that a person in location A ends up in location
B after a single unit of time and 1/3 is the probability that a person in location B ends
up in location B. Instead of the word probability, you could use the word â€œproportionâ€ and
the numbers would then represent the proportion of people in the various locations who end
up in the other location after one unit of time. Thue 1/4 is the proportion of people in A
who end up in A,etc. Then this matrix is called a stochastic matrix, a Markov matrix or
a Migration matrix. In the case the numbers are interpreted as probabilities, it is called a
Markov or Stochastic matrix. In the case where they are proportions it is called a migration
matrix.
Consider it as a migration matrix and suppose that initially there are 200 people in
location A and 120 in location B. You might wonder how many there would be in the two
locations after one unit of time. This is easy to ï¬gure out. Those in A after one unit of
time consist of those in A who were in A to begin with added to those in A who started oï¬€
in B. Thus
# in A = 1
4 (200) + 2
3 (120) = 130
# in B = 3
4 (200) + 1
3 (120) = 190.

223
You can see that this amounts to nothing more than matrix multiplication. Thus, letting
(a1, b1)T be deï¬ned by
Âµ
a1
b1
Â¶
=
Âµ
# in A after one unit of time
# in B after one unit of time
Â¶
It follows
Âµ a1
b1
Â¶
=
Âµ
1
4
2
3
3
4
1
3
Â¶ Âµ 200
120
Â¶
=
Âµ 130
190
Â¶
Now with this vector as new input, you can determine how many are in the two locations
after another unit of time using the same procedure. Thus letting an denote the numbers
in location A after n units of time and bn the number in B after n units of time,
Âµ
a2
b2
Â¶
=
Âµ
1
4
2
3
3
4
1
3
Â¶ Âµ
a1
b1
Â¶
=
Âµ
1
4
2
3
3
4
1
3
Â¶ Âµ
1
4
2
3
3
4
1
3
Â¶ Âµ
200
120
Â¶
=
Âµ
1
4
2
3
3
4
1
3
Â¶2 Âµ
200
120
Â¶
=
Âµ
159. 166 667
160. 833 333
Â¶
.
Obviously you need to round oï¬€if you are considering people doing the migrating. Then
by analogy,
Âµ
an
bn
Â¶
=
Âµ
1
4
2
3
3
4
1
3
Â¶ Âµ
anâˆ’1
bnâˆ’1
Â¶
=
Âµ
1
4
2
3
3
4
1
3
Â¶ Âµ
1
4
2
3
3
4
1
3
Â¶nâˆ’1 Âµ
a0
b0
Â¶
=
Âµ
1
4
2
3
3
4
1
3
Â¶n Âµ
a0
b0
Â¶
After 50 units of time you would have
Âµ
a50
b50
Â¶
=
Âµ
1
4
2
3
3
4
1
3
Â¶50 Âµ
a0
b0
Â¶
=
Âµ
. 470 588 235
. 470 588 235
. 529 411 765
. 529 411 765
Â¶ Âµ
a0
b0
Â¶
After 100 units of time, you would have
Âµ
a100
b100
Â¶
=
Âµ
1
4
2
3
3
4
1
3
Â¶100 Âµ
a0
b0
Â¶
=
Âµ . 470 588 235
. 470 588 235
. 529 411 765
. 529 411 765
Â¶ Âµ a0
b0
Â¶
You canâ€™t detect any diï¬€erence between these two answers. In general, if you wanted to know
about how many would be in the two locations, you would need to take a limit. However,
there is a better way.
More generally here is a deï¬nition.
Deï¬nition 12.0.52 Let n locations be denoted by the numbers 1, 2, Â· Â· Â·, n. Also
suppose it is the case that each year aij denotes the proportion of residents in location j

224
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
which move to location i. Also suppose no one escapes or emigrates from without these n
locations. This last assumption requires P
i aij = 1. Such matrices in which the columns
are nonnegative numbers which sum to one are called Markov matrices. In this context
describing migration, they are also called migration matrices.
Example 12.0.53 Here is an example of one of these matrices.
Âµ
.4
.2
.6
.8
Â¶
Thus if it is considered as a migration matrix, .4 is the proportion of residents in location
1 which stay in location one in a given time period while .6 is the proportion of residents in
location 1 which move to location 2 and .2 is the proportion of residents in location 2 which
move to location 1. Considered as a Markov matrix, these numbers are usually identiï¬ed
with probabilities.
If v = (x1, Â· Â· Â·, xn)T where xi is the population of location i at a given instant, you obtain
the population of location i one year later by computing P
j aijxj = (Av)i . Therefore, the
population of location i after k years is
Â¡
Akv
Â¢
i . An obvious application of this would be to
a situation in which you rent trailers which can go to various parts of a city and you observe
through experiments the proportion of trailers which go from point i to point j in a single
day. Then you might want to ï¬nd how many trailers would be in all the locations after 8
days.
Proposition 12.0.54 Let A = (aij) be a migration matrix. Then 1 is always an eigen-
value for A.
Proof: Remember that det
Â¡
BT Â¢
= det (B) . Therefore,
det (A âˆ’Î»I) = det
Â³
(A âˆ’Î»I)T Â´
= det
Â¡
AT âˆ’Î»I
Â¢
because IT = I. Thus the characteristic equation for A is the same as the characteristic
equation for AT and so A and AT have the same eigenvalues. We will show that 1 is an
eigenvalue for AT and then it will follow that 1 is an eigenvalue for A.
Remember that for a migration matrix, P
i aij = 1. Therefore, if AT = (bij) so bij = aji,
it follows that
X
j
bij =
X
j
aji = 1.
Therefore, from matrix multiplication,
AT
ï£«
ï£¬
ï£­
1
...
1
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
P
j bij
...
P
j bij
ï£¶
ï£·
ï£¸=
ï£«
ï£¬
ï£­
1
...
1
ï£¶
ï£·
ï£¸
which shows that
ï£«
ï£¬
ï£­
1
...
1
ï£¶
ï£·
ï£¸is an eigenvector for AT corresponding to the eigenvalue, Î» = 1.
As explained above, this shows that Î» = 1 is an eigenvalue for A because A and AT have
the same eigenvalues.

225
Example 12.0.55 Consider the migration matrix,
ï£«
ï£­
.6
0
.1
.2
.8
0
.2
.2
.9
ï£¶
ï£¸for locations 1, 2, and
3. Suppose initially there are 100 residents in location 1, 200 in location 2 and 400 in location
4. Find the population in the three locations after 10 units of time.
From the above, it suï¬ƒces to consider
ï£«
ï£­
.6
0
.1
.2
.8
0
.2
.2
.9
ï£¶
ï£¸
10 ï£«
ï£­
100
200
400
ï£¶
ï£¸=
ï£«
ï£­
115. 085 829 22
120. 130 672 44
464. 783 498 34
ï£¶
ï£¸
Of course you would need to round these numbers oï¬€.
A related problem asks for how many there will be in the various locations after a long
time. It turns out that if some power of the migration matrix has all positive entries, then
there is a limiting vector, x = limkâ†’âˆAkx0 where x0 is the initial vector describing the
number of inhabitants in the various locations initially. This vector will be an eigenvector
for the eigenvalue 1 because
x = lim
kâ†’âˆAkx0 = lim
kâ†’âˆAk+1x0 = A lim
kâ†’âˆAkx = Ax,
and the sum of its entries will equal the sum of the entries of the initial vector, x0 because
this sum is preserved for every multiplication by A since
X
i
X
j
aijxj =
X
j
xj
ÃƒX
i
aij
!
=
X
j
xj.
Here is an example. It is the same example as the one above but here it will involve the
long time limit.
Example 12.0.56 Consider the migration matrix,
ï£«
ï£­
.6
0
.1
.2
.8
0
.2
.2
.9
ï£¶
ï£¸for locations 1, 2, and
3. Suppose initially there are 100 residents in location 1, 200 in location 2 and 400 in location
4. Find the population in the three locations after a long time.
You just need to ï¬nd the eigenvector which goes with the eigenvalue 1 and then normalize
it so the sum of its entries equals the sum of the entries of the initial vector. Thus you need
to ï¬nd a solution to
ï£«
ï£­
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸âˆ’
ï£«
ï£­
.6
0
.1
.2
.8
0
.2
.2
.9
ï£¶
ï£¸
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸
The augmented matrix is
ï£«
ï£­
. 4
0
âˆ’. 1
|
0
âˆ’. 2
. 2
0
|
0
âˆ’. 2
âˆ’. 2
. 1
|
0
ï£¶
ï£¸
and its row reduced echelon form is
ï£«
ï£­
1
0
âˆ’. 25
0
0
1
âˆ’. 25
0
0
0
0
0
ï£¶
ï£¸

226
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Therefore, the eigenvectors are
s
ï£«
ï£­
(1/4)
(1/4)
1
ï£¶
ï£¸
and all that remains is to choose the value of s such that
1
4s + 1
4s + s = 100 + 200 + 400
This yields s = 1400
3
and so the long time limit would equal
1400
3
ï£«
ï£­
(1/4)
(1/4)
1
ï£¶
ï£¸=
ï£«
ï£­
116. 666 666 666 666 7
116. 666 666 666 666 7
466. 666 666 666 666 7
ï£¶
ï£¸.
You would of course need to round these numbers oï¬€. You see that you are not far oï¬€after
just 10 units of time. Therefore, you might consider this as a useful procedure because it is
probably easier to solve a simple system of equations than it is to raise a matrix to a large
power.
Example 12.0.57 Suppose a migration matrix is
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
5
1
2
1
5
1
4
1
4
1
2
11
20
1
4
3
10
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
. Find the comparison
between the populations in the three locations after a long time.
This amounts to nothing more than ï¬nding the eigenvector for Î» = 1. Solve
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸âˆ’
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
5
1
2
1
5
1
4
1
4
1
2
11
20
1
4
3
10
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸.
The augmented matrix is
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
4
5
âˆ’1
2
âˆ’1
5
|
0
âˆ’1
4
3
4
âˆ’1
2
|
0
âˆ’11
20
âˆ’1
4
7
10
|
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
The row echelon form is
ï£«
ï£¬
ï£­
1
0
âˆ’16
19
0
0
1
âˆ’18
19
0
0
0
0
0
ï£¶
ï£·
ï£¸
and so an eigenvector is
ï£«
ï£­
16
18
19
ï£¶
ï£¸.

227
Thus there will be 18
16
th more in location 2 than in location 1. There will be 19
18
th more in
location 3 than in location 2.
You see the eigenvalue problem makes these sorts of determinations fairly simple.
There are many other things which can be said about these sorts of migration prob-
lems. They include things like the gamblerâ€™s ruin problem which asks for the probability
that a compulsive gambler will eventually lose all his money. However those problems are
not so easy although they still involve eigenvalues and eigenvectors.
12.0.12
Complex Eigenvalues
Sometimes you have to consider eigenvalues which are complex numbers. This occurs in
diï¬€erential equations for example. You do these problems exactly the same way as you do
the ones in which the eigenvalues are real. Here is an example.
Example 12.0.58 Find the eigenvalues and eigenvectors of the matrix
A =
ï£«
ï£­
1
0
0
0
2
âˆ’1
0
1
2
ï£¶
ï£¸.
You need to ï¬nd the eigenvalues. Solve
det
ï£«
ï£­
ï£«
ï£­
1
0
0
0
2
âˆ’1
0
1
2
ï£¶
ï£¸âˆ’Î»
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
ï£¶
ï£¸= 0.
This reduces to (Î» âˆ’1)
Â¡
Î»2 âˆ’4Î» + 5
Â¢
= 0. The solutions are Î» = 1, Î» = 2 + i, Î» = 2 âˆ’i.
There is nothing new about ï¬nding the eigenvectors for Î» = 1 so consider the eigenvalue
Î» = 2 + i. You need to solve
ï£«
ï£­(2 + i)
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸âˆ’
ï£«
ï£­
1
0
0
0
2
âˆ’1
0
1
2
ï£¶
ï£¸
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸
In other words, you must consider the augmented matrix,
ï£«
ï£­
1 + i
0
0
|
0
0
i
1
|
0
0
âˆ’1
i
|
0
ï£¶
ï£¸
for the solution. Divide the top row by (1 + i) and then take âˆ’i times the second row and
add to the bottom. This yields
ï£«
ï£­
1
0
0
|
0
0
i
1
|
0
0
0
0
|
0
ï£¶
ï£¸
Now multiply the second row by âˆ’i to obtain
ï£«
ï£­
1
0
0
|
0
0
1
âˆ’i
|
0
0
0
0
|
0
ï£¶
ï£¸

228
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Therefore, the eigenvectors are of the form
t
ï£«
ï£­
0
i
1
ï£¶
ï£¸.
You should ï¬nd the eigenvectors for Î» = 2 âˆ’i. These are
t
ï£«
ï£­
0
âˆ’i
1
ï£¶
ï£¸.
As usual, if you want to get it right you had better check it.
ï£«
ï£­
1
0
0
0
2
âˆ’1
0
1
2
ï£¶
ï£¸
ï£«
ï£­
0
âˆ’i
1
ï£¶
ï£¸=
ï£«
ï£­
0
âˆ’1 âˆ’2i
2 âˆ’i
ï£¶
ï£¸= (2 âˆ’i)
ï£«
ï£­
0
âˆ’i
1
ï£¶
ï£¸
so it worked.
12.0.13
The Estimation Of Eigenvalues
There are ways to estimate the eigenvalues for matrices. The most famous is known as
Gerschgorinâ€™s theorem. This theorem gives a rough idea where the eigenvalues are just from
looking at the matrix.
Theorem 12.0.59 Let A be an n Ã— n matrix. Consider the n Gerschgorin discs
deï¬ned as
Di â‰¡
ï£±
ï£²
ï£³Î» âˆˆC : |Î» âˆ’aii| â‰¤
X
jÌ¸=i
|aij|
ï£¼
ï£½
ï£¾.
Then every eigenvalue is contained in some Gerschgorin disc.
This theorem says to add up the absolute values of the entries of the ith row which are
oï¬€the main diagonal and form the disc centered at aii having this radius. The union of
these discs contains Ïƒ (A) .
Proof: Suppose Ax = Î»x where x Ì¸= 0. Then for A = (aij)
X
jÌ¸=i
aijxj = (Î» âˆ’aii) xi.
Therefore, picking k such that |xk| â‰¥|xj| for all xj, it follows that |xk| Ì¸= 0 since |x| Ì¸= 0
and
|xk|
X
jÌ¸=i
|akj| â‰¥
X
jÌ¸=i
|akj| |xj| â‰¥|Î» âˆ’aii| |xk| .
Now dividing by |xk|, it follows Î» is contained in the kth Gerschgorin disc.
Example 12.0.60 Here is a matrix. Estimate its eigenvalues.
ï£«
ï£­
2
1
1
3
5
0
0
1
9
ï£¶
ï£¸

12.1.
THE MATHEMATICAL THEORY OF DETERMINANTSâˆ—
229
According to Gerschgorinâ€™s theorem the eigenvalues are contained in the disks
D1
=
{Î» âˆˆC : |Î» âˆ’2| â‰¤2} ,
D2
=
{Î» âˆˆC : |Î» âˆ’5| â‰¤3} ,
D3
=
{Î» âˆˆC : |Î» âˆ’9| â‰¤1}
It is important to observe that these disks are in the complex plane. In general this is the
case. If you want to ï¬nd eigenvalues they will be complex numbers.
x
iy
r
2
r
8
r
9
So what are the values of the eigenvalues? In this case they are real. You can compute
them by graphing the characteristic polynomial, Î»3 âˆ’16Î»2 + 70Î» âˆ’66 and then zoom-
ing in on the zeros. If you do this you ï¬nd the solution is {Î» = 1. 295 3} , {Î» = 5. 590 5} ,
{Î» = 9. 114 2} . Of course these are only approximations and so this information is useless
for ï¬nding eigenvectors. However, in many applications, it is the size of the eigenvalues
which is important and so these numerical values would be helpful for such applications.
Because of this example, you might think there is no real reason for Gerschgorinâ€™s theorem.
Why not just compute the characteristic equation and graph and zoom? This is ï¬ne up to
a point, but what if the matrix was huge? Then it might be hard to ï¬nd the characteristic
polynomial. Remember the diï¬ƒculties in expanding a big matrix along a row or column.
You would need a better way to come up with the characteristic polynomial. Also, what
if the eigenvalue were complex? You donâ€™t see these by following this procedure. However,
Gerschgorinâ€™s theorem will at least estimate them.
There are also more advanced versions of this theorem which depend on the theory of
functions of a complex variable covering the case where the Gerschgorin disks are disjoint.
In this case, you can assert each disk contains an eigenvalue. In fact, if k of the Gerschgorin
disks are disjoint from the other disks then they contain k eigenvalues. To see this proved,
see the linear algebra book on my web page. Donâ€™t bother to look at it if you have not had
a substantial course on complex analysis because it wonâ€™t make any sense. Math is not like
comparative literature, history, or humanities. You canâ€™t read the advanced topics until you
have mastered the basic topics even if you are real smart.
12.1
The Mathematical Theory Of Determinantsâˆ—
This material is deï¬nitely not for the faint of heart. It is only for people who want

230
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
to see everything proved. It is a fairly complete and unusually elementary treatment of
the subject. There will be some repetition between this section and the earlier section on
determinants. The main purpose is to give all the missing proofs. Two books which give
a good introduction to determinants are Apostol [2] and Rudin [21]. A recent book which
also has a good introduction is Baker [4]. Most linear algebra books do not do an honest
job presenting this topic.
It is easiest to give a diï¬€erent deï¬nition of the determinant which is clearly well deï¬ned
and then prove the earlier one in terms of Laplace expansion. Let (i1, Â· Â· Â·, in) be an ordered
list of numbers from {1, Â· Â· Â·, n} . This means the order is important so (1, 2, 3) and (2, 1, 3)
are diï¬€erent.
The following Lemma will be essential in the deï¬nition of the determinant.
Lemma 12.1.1 There exists a unique function, sgnn which maps each list of numbers
from {1, Â· Â· Â·, n} to one of the three numbers, 0, 1, or âˆ’1 which also has the following prop-
erties.
sgnn (1, Â· Â· Â·, n) = 1
(12.8)
sgnn (i1, Â· Â· Â·, p, Â· Â· Â·, q, Â· Â· Â·, in) = âˆ’sgnn (i1, Â· Â· Â·, q, Â· Â· Â·, p, Â· Â· Â·, in)
(12.9)
In words, the second property states that if two of the numbers are switched, the value of the
function is multiplied by âˆ’1. Also, in the case where n > 1 and {i1, Â· Â· Â·, in} = {1, Â· Â· Â·, n} so
that every number from {1, Â· Â· Â·, n} appears in the ordered list, (i1, Â· Â· Â·, in) ,
sgnn (i1, Â· Â· Â·, iÎ¸âˆ’1, n, iÎ¸+1, Â· Â· Â·, in) â‰¡
(âˆ’1)nâˆ’Î¸ sgnnâˆ’1 (i1, Â· Â· Â·, iÎ¸âˆ’1, iÎ¸+1, Â· Â· Â·, in)
(12.10)
where n = iÎ¸ in the ordered list, (i1, Â· Â· Â·, in) .
Proof: To begin with, it is necessary to show the existence of such a function. This is
clearly true if n = 1. Deï¬ne sgn1 (1) â‰¡1 and observe that it works. No switching is possible.
In the case where n = 2, it is also clearly true. Let sgn2 (1, 2) = 1 and sgn2 (2, 1) = 0 while
sgn2 (2, 2) = sgn2 (1, 1) = 0 and verify it works. Assuming such a function exists for n,
sgnn+1 will be deï¬ned in terms of sgnn . If there are any repeated numbers in (i1, Â· Â· Â·, in+1) ,
sgnn+1 (i1, Â· Â· Â·, in+1) â‰¡0. If there are no repeats, then n + 1 appears somewhere in the
ordered list. Let Î¸ be the position of the number n + 1 in the list. Thus, the list is of the
form (i1, Â· Â· Â·, iÎ¸âˆ’1, n + 1, iÎ¸+1, Â· Â· Â·, in+1) . From 12.10 it must be that
sgnn+1 (i1, Â· Â· Â·, iÎ¸âˆ’1, n + 1, iÎ¸+1, Â· Â· Â·, in+1) â‰¡
(âˆ’1)n+1âˆ’Î¸ sgnn (i1, Â· Â· Â·, iÎ¸âˆ’1, iÎ¸+1, Â· Â· Â·, in+1) .
It is necessary to verify this satisï¬es 12.8 and 12.9 with n replaced with n + 1. The ï¬rst of
these is obviously true because
sgnn+1 (1, Â· Â· Â·, n, n + 1) â‰¡(âˆ’1)n+1âˆ’(n+1) sgnn (1, Â· Â· Â·, n) = 1.
If there are repeated numbers in (i1, Â· Â· Â·, in+1) , then it is obvious 12.9 holds because both
sides would equal zero from the above deï¬nition. It remains to verify 12.9 in the case where
there are no numbers repeated in (i1, Â· Â· Â·, in+1) . Consider
sgnn+1
Â³
i1, Â· Â· Â·,
rp, Â· Â· Â·,
sq, Â· Â· Â·, in+1
Â´
,

12.1.
THE MATHEMATICAL THEORY OF DETERMINANTSâˆ—
231
where the r above the p indicates the number, p is in the rth position and the s above the
q indicates that the number, q is in the sth position. Suppose ï¬rst that r < Î¸ < s. Then
sgnn+1
Âµ
i1, Â· Â· Â·,
rp, Â· Â· Â·,
Î¸
n + 1, Â· Â· Â·,
sq, Â· Â· Â·, in+1
Â¶
â‰¡
(âˆ’1)n+1âˆ’Î¸ sgnn
Â³
i1, Â· Â· Â·,
rp, Â· Â· Â·,
sâˆ’1
q , Â· Â· Â·, in+1
Â´
while
sgnn+1
Âµ
i1, Â· Â· Â·,
rq, Â· Â· Â·,
Î¸
n + 1, Â· Â· Â·,
sp, Â· Â· Â·, in+1
Â¶
=
(âˆ’1)n+1âˆ’Î¸ sgnn
Â³
i1, Â· Â· Â·,
rq, Â· Â· Â·,
sâˆ’1
p , Â· Â· Â·, in+1
Â´
and so, by induction, a switch of p and q introduces a minus sign in the result. Similarly, if
Î¸ > s or if Î¸ < r it also follows that 12.9 holds. The interesting case is when Î¸ = r or Î¸ = s.
Consider the case where Î¸ = r and note the other case is entirely similar.
sgnn+1
Â³
i1, Â· Â· Â·,
r
n + 1, Â· Â· Â·,
sq, Â· Â· Â·, in+1
Â´
=
(âˆ’1)n+1âˆ’r sgnn
Â³
i1, Â· Â· Â·,
sâˆ’1
q , Â· Â· Â·, in+1
Â´
(12.11)
while
sgnn+1
Â³
i1, Â· Â· Â·,
rq, Â· Â· Â·,
s
n + 1, Â· Â· Â·, in+1
Â´
=
(âˆ’1)n+1âˆ’s sgnn
Â³
i1, Â· Â· Â·,
rq, Â· Â· Â·, in+1
Â´
.
(12.12)
By making s âˆ’1 âˆ’r switches, move the q which is in the s âˆ’1th position in 12.11 to the
rth position in 12.12. By induction, each of these switches introduces a factor of âˆ’1 and so
sgnn
Â³
i1, Â· Â· Â·,
sâˆ’1
q , Â· Â· Â·, in+1
Â´
= (âˆ’1)sâˆ’1âˆ’r sgnn
Â³
i1, Â· Â· Â·,
rq, Â· Â· Â·, in+1
Â´
.
Therefore,
sgnn+1
Â³
i1, Â· Â· Â·,
r
n + 1, Â· Â· Â·,
sq, Â· Â· Â·, in+1
Â´
= (âˆ’1)n+1âˆ’r sgnn
Â³
i1, Â· Â· Â·,
sâˆ’1
q , Â· Â· Â·, in+1
Â´
= (âˆ’1)n+1âˆ’r (âˆ’1)sâˆ’1âˆ’r sgnn
Â³
i1, Â· Â· Â·,
rq, Â· Â· Â·, in+1
Â´
= (âˆ’1)n+s sgnn
Â³
i1, Â· Â· Â·,
rq, Â· Â· Â·, in+1
Â´
= (âˆ’1)2sâˆ’1 (âˆ’1)n+1âˆ’s sgnn
Â³
i1, Â· Â· Â·,
rq, Â· Â· Â·, in+1
Â´
= âˆ’sgnn+1
Â³
i1, Â· Â· Â·,
rq, Â· Â· Â·,
s
n + 1, Â· Â· Â·, in+1
Â´
.
This proves the existence of the desired function.
To see this function is unique, note that you can obtain any ordered list of distinct
numbers from a sequence of switches. If there exist two functions, f and g both satisfying
12.8 and 12.9, you could start with f (1, Â· Â· Â·, n) = g (1, Â· Â· Â·, n) and applying the same sequence
of switches, eventually arrive at f (i1, Â· Â· Â·, in) = g (i1, Â· Â· Â·, in) . If any numbers are repeated,
then 12.9 gives both functions are equal to zero for that ordered list. This proves the lemma.
In what follows sgn will often be used rather than sgnn because the context supplies the
appropriate n.

232
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Deï¬nition 12.1.2 Let f be a real valued function which has the set of ordered lists
of numbers from {1, Â· Â· Â·, n} as its domain. Deï¬ne
X
(k1,Â·Â·Â·,kn)
f (k1 Â· Â· Â· kn)
to be the sum of all the f (k1 Â· Â· Â· kn) for all possible choices of ordered lists (k1, Â· Â· Â·, kn) of
numbers of {1, Â· Â· Â·, n} . For example,
X
(k1,k2)
f (k1, k2) = f (1, 2) + f (2, 1) + f (1, 1) + f (2, 2) .
Deï¬nition 12.1.3 Let (aij) = A denote an n Ã— n matrix. The determinant of A,
denoted by det (A) is deï¬ned by
det (A) â‰¡
X
(k1,Â·Â·Â·,kn)
sgn (k1, Â· Â· Â·, kn) a1k1 Â· Â· Â· ankn
where the sum is taken over all ordered lists of numbers from {1, Â· Â· Â·, n}. Note it suï¬ƒces to
take the sum over only those ordered lists in which there are no repeats because if there are,
sgn (k1, Â· Â· Â·, kn) = 0 and so that term contributes 0 to the sum.
Let A be an n Ã— n matrix, A = (aij) and let (r1, Â· Â· Â·, rn) denote an ordered list of n
numbers from {1, Â· Â· Â·, n}. Let A (r1, Â· Â· Â·, rn) denote the matrix whose kth row is the rk row
of the matrix, A. Thus
det (A (r1, Â· Â· Â·, rn)) =
X
(k1,Â·Â·Â·,kn)
sgn (k1, Â· Â· Â·, kn) ar1k1 Â· Â· Â· arnkn
(12.13)
and
A (1, Â· Â· Â·, n) = A.
Proposition 12.1.4 Let
(r1, Â· Â· Â·, rn)
be an ordered list of numbers from {1, Â· Â· Â·, n}. Then
sgn (r1, Â· Â· Â·, rn) det (A)
=
X
(k1,Â·Â·Â·,kn)
sgn (k1, Â· Â· Â·, kn) ar1k1 Â· Â· Â· arnkn
(12.14)
=
det (A (r1, Â· Â· Â·, rn)) .
(12.15)
Proof: Let (1, Â· Â· Â·, n) = (1, Â· Â· Â·, r, Â· Â· Â·s, Â· Â· Â·, n) so r < s.
det (A (1, Â· Â· Â·, r, Â· Â· Â·, s, Â· Â· Â·, n)) =
(12.16)
X
(k1,Â·Â·Â·,kn)
sgn (k1, Â· Â· Â·, kr, Â· Â· Â·, ks, Â· Â· Â·, kn) a1k1 Â· Â· Â· arkr Â· Â· Â· asks Â· Â· Â· ankn,
and renaming the variables, calling ks, kr and kr, ks, this equals
=
X
(k1,Â·Â·Â·,kn)
sgn (k1, Â· Â· Â·, ks, Â· Â· Â·, kr, Â· Â· Â·, kn) a1k1 Â· Â· Â· arks Â· Â· Â· askr Â· Â· Â· ankn

12.1.
THE MATHEMATICAL THEORY OF DETERMINANTSâˆ—
233
=
X
(k1,Â·Â·Â·,kn)
âˆ’sgn
ï£«
ï£­k1, Â· Â· Â·,
These got switched
z
}|
{
kr, Â· Â· Â·, ks
, Â· Â· Â·, kn
ï£¶
ï£¸a1k1 Â· Â· Â· askr Â· Â· Â· arks Â· Â· Â· ankn
= âˆ’det (A (1, Â· Â· Â·, s, Â· Â· Â·, r, Â· Â· Â·, n)) .
(12.17)
Consequently,
det (A (1, Â· Â· Â·, s, Â· Â· Â·, r, Â· Â· Â·, n)) =
âˆ’det (A (1, Â· Â· Â·, r, Â· Â· Â·, s, Â· Â· Â·, n)) = âˆ’det (A)
Now letting A (1, Â· Â· Â·, s, Â· Â· Â·, r, Â· Â· Â·, n) play the role of A, and continuing in this way, switching
pairs of numbers,
det (A (r1, Â· Â· Â·, rn)) = (âˆ’1)p det (A)
where it took p switches to obtain(r1, Â· Â· Â·, rn) from (1, Â· Â· Â·, n). By Lemma 12.1.1, this implies
det (A (r1, Â· Â· Â·, rn)) = (âˆ’1)p det (A) = sgn (r1, Â· Â· Â·, rn) det (A)
and proves the proposition in the case when there are no repeated numbers in the ordered
list, (r1, Â· Â· Â·, rn). However, if there is a repeat, say the rth row equals the sth row, then the
reasoning of 12.16 -12.17 shows that A (r1, Â· Â· Â·, rn) = 0 and also sgn (r1, Â· Â· Â·, rn) = 0 so the
formula holds in this case also.
Observation 12.1.5 There are n! ordered lists of distinct numbers from {1, Â· Â· Â·, n} .
To see this, consider n slots placed in order. There are n choices for the ï¬rst slot. For
each of these choices, there are n âˆ’1 choices for the second. Thus there are n (n âˆ’1) ways
to ï¬ll the ï¬rst two slots. Then for each of these ways there are nâˆ’2 choices left for the third
slot. Continuing this way, there are n! ordered lists of distinct numbers from {1, Â· Â· Â·, n} as
stated in the observation.
With the above, it is possible to give a more symmetric description of the determinant
from which it will follow that det (A) = det
Â¡
AT Â¢
.
Corollary 12.1.6 The following formula for det (A) is valid.
det (A) = 1
n!Â·
X
(r1,Â·Â·Â·,rn)
X
(k1,Â·Â·Â·,kn)
sgn (r1, Â· Â· Â·, rn) sgn (k1, Â· Â· Â·, kn) ar1k1 Â· Â· Â· arnkn.
(12.18)
And also det
Â¡
AT Â¢
= det (A) where AT is the transpose of A. (Recall that for AT =
Â¡
aT
ij
Â¢
,
aT
ij = aji.)
Proof: From Proposition 12.1.4, if the ri are distinct,
det (A) =
X
(k1,Â·Â·Â·,kn)
sgn (r1, Â· Â· Â·, rn) sgn (k1, Â· Â· Â·, kn) ar1k1 Â· Â· Â· arnkn.
Summing over all ordered lists, (r1, Â· Â· Â·, rn) where the ri are distinct, (If the ri are not
distinct, sgn (r1, Â· Â· Â·, rn) = 0 and so there is no contribution to the sum.)
n! det (A) =
X
(r1,Â·Â·Â·,rn)
X
(k1,Â·Â·Â·,kn)
sgn (r1, Â· Â· Â·, rn) sgn (k1, Â· Â· Â·, kn) ar1k1 Â· Â· Â· arnkn.
This proves the corollary since the formula gives the same number for A as it does for AT .

234
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Corollary 12.1.7 If two rows or two columns in an n Ã— n matrix, A, are switched,
the determinant of the resulting matrix equals (âˆ’1) times the determinant of the original
matrix. If A is an n Ã— n matrix in which two rows are equal or two columns are equal then
det (A) = 0. Suppose the ith row of A equals (xa1 + yb1, Â· Â· Â·, xan + ybn). Then
det (A) = x det (A1) + y det (A2)
where the ith row of A1 is (a1, Â· Â· Â·, an) and the ith row of A2 is (b1, Â· Â· Â·, bn) , all other rows
of A1 and A2 coinciding with those of A. In other words, det is a linear function of each
row A. The same is true with the word â€œrowâ€ replaced with the word â€œcolumnâ€.
Proof:
By Proposition 12.1.4 when two rows are switched, the determinant of the
resulting matrix is (âˆ’1) times the determinant of the original matrix. By Corollary 12.1.6 the
same holds for columns because the columns of the matrix equal the rows of the transposed
matrix. Thus if A1 is the matrix obtained from A by switching two columns,
det (A) = det
Â¡
AT Â¢
= âˆ’det
Â¡
AT
1
Â¢
= âˆ’det (A1) .
If A has two equal columns or two equal rows, then switching them results in the same
matrix. Therefore, det (A) = âˆ’det (A) and so det (A) = 0.
It remains to verify the last assertion.
det (A) â‰¡
X
(k1,Â·Â·Â·,kn)
sgn (k1, Â· Â· Â·, kn) a1k1 Â· Â· Â· (xaki + ybki) Â· Â· Â· ankn
= x
X
(k1,Â·Â·Â·,kn)
sgn (k1, Â· Â· Â·, kn) a1k1 Â· Â· Â· aki Â· Â· Â· ankn
+y
X
(k1,Â·Â·Â·,kn)
sgn (k1, Â· Â· Â·, kn) a1k1 Â· Â· Â· bki Â· Â· Â· ankn
â‰¡x det (A1) + y det (A2) .
The same is true of columns because det
Â¡
AT Â¢
= det (A) and the rows of AT are the columns
of A.
Deï¬nition 12.1.8 A vector, w, is a linear combination of the vectors {v1, Â· Â· Â·, vr}
if there exists scalars, c1, Â· Â· Â·cr such that w = Pr
k=1 ckvk. This is the same as saying w âˆˆ
span {v1, Â· Â· Â·, vr} .
The following corollary is also of great use.
Corollary 12.1.9 Suppose A is an n Ã— n matrix and some column (row) is a linear
combination of r other columns (rows). Then det (A) = 0.
Proof: Let A =
Â¡
a1
Â· Â· Â·
an
Â¢
be the columns of A and suppose the condition that
one column is a linear combination of r of the others is satisï¬ed. Then by using Corollary
12.1.7 you may rearrange the columns to have the nth column a linear combination of the
ï¬rst r columns. Thus an = Pr
k=1 ckak and so
det (A) = det
Â¡
a1
Â· Â· Â·
ar
Â· Â· Â·
anâˆ’1
Pr
k=1 ckak
Â¢
.
By Corollary 12.1.7
det (A) =
r
X
k=1
ck det
Â¡
a1
Â· Â· Â·
ar
Â· Â· Â·
anâˆ’1
ak
Â¢
= 0.
The case for rows follows from the fact that det (A) = det
Â¡
AT Â¢
. This proves the corollary.
Recall the following deï¬nition of matrix multiplication.

12.1.
THE MATHEMATICAL THEORY OF DETERMINANTSâˆ—
235
Deï¬nition 12.1.10 If A and B are n Ã— n matrices, A = (aij) and B = (bij),
AB = (cij) where
cij â‰¡
n
X
k=1
aikbkj.
One of the most important rules about determinants is that the determinant of a product
equals the product of the determinants.
Theorem 12.1.11 Let A and B be n Ã— n matrices. Then
det (AB) = det (A) det (B) .
Proof: Let cij be the ijth entry of AB. Then by Proposition 12.1.4,
det (AB) =
X
(k1,Â·Â·Â·,kn)
sgn (k1, Â· Â· Â·, kn) c1k1 Â· Â· Â· cnkn
=
X
(k1,Â·Â·Â·,kn)
sgn (k1, Â· Â· Â·, kn)
ÃƒX
r1
a1r1br1k1
!
Â· Â· Â·
ÃƒX
rn
anrnbrnkn
!
=
X
(r1Â·Â·Â·,rn)
X
(k1,Â·Â·Â·,kn)
sgn (k1, Â· Â· Â·, kn) br1k1 Â· Â· Â· brnkn (a1r1 Â· Â· Â· anrn)
=
X
(r1Â·Â·Â·,rn)
sgn (r1 Â· Â· Â· rn) a1r1 Â· Â· Â· anrn det (B) = det (A) det (B) .
This proves the theorem.
Lemma 12.1.12 Suppose a matrix is of the form
M =
Âµ
A
âˆ—
0
a
Â¶
(12.19)
or
M =
Âµ
A
0
âˆ—
a
Â¶
(12.20)
where a is a number and A is an (n âˆ’1) Ã— (n âˆ’1) matrix and âˆ—denotes either a column
or a row having length n âˆ’1 and the 0 denotes either a column or a row of length n âˆ’1
consisting entirely of zeros. Then
det (M) = a det (A) .
Proof: Denote M by (mij) . Thus in the ï¬rst case, mnn = a and mni = 0 if i Ì¸= n while
in the second case, mnn = a and min = 0 if i Ì¸= n. From the deï¬nition of the determinant,
det (M) â‰¡
X
(k1,Â·Â·Â·,kn)
sgnn (k1, Â· Â· Â·, kn) m1k1 Â· Â· Â· mnkn
Letting Î¸ denote the position of n in the ordered list, (k1, Â· Â· Â·, kn) then using the earlier
conventions used to prove Lemma 12.1.1, det (M) equals
X
(k1,Â·Â·Â·,kn)
(âˆ’1)nâˆ’Î¸ sgnnâˆ’1
Âµ
k1, Â· Â· Â·, kÎ¸âˆ’1,
Î¸
kÎ¸+1, Â· Â· Â·,
nâˆ’1
kn
Â¶
m1k1 Â· Â· Â· mnkn

236
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Now suppose 12.20. Then if kn Ì¸= n, the term involving mnkn in the above expression equals
zero. Therefore, the only terms which survive are those for which Î¸ = n or in other words,
those for which kn = n. Therefore, the above expression reduces to
a
X
(k1,Â·Â·Â·,knâˆ’1)
sgnnâˆ’1 (k1, Â· Â· Â·knâˆ’1) m1k1 Â· Â· Â· m(nâˆ’1)knâˆ’1 = a det (A) .
To get the assertion in the situation of 12.19 use Corollary 12.1.6 and 12.20 to write
det (M) = det
Â¡
M T Â¢
= det
ÂµÂµ
AT
0
âˆ—
a
Â¶Â¶
= a det
Â¡
AT Â¢
= a det (A) .
This proves the lemma.
In terms of the theory of determinants, arguably the most important idea is that of
Laplace expansion along a row or a column. This will follow from the above deï¬nition of a
determinant.
Deï¬nition 12.1.13 Let A = (aij) be an n Ã— n matrix. Then a new matrix called
the cofactor matrix, cof (A) is deï¬ned by cof (A) = (cij) where to obtain cij delete the ith
row and the jth column of A, take the determinant of the (n âˆ’1) Ã— (n âˆ’1) matrix which
results, (This is called the ijth minor of A. ) and then multiply this number by (âˆ’1)i+j. To
make the formulas easier to remember, cof (A)ij will denote the ijth entry of the cofactor
matrix.
The following is the main result. Earlier this was given as a deï¬nition and the outrageous
totally unjustiï¬ed assertion was made that the same number would be obtained by expanding
the determinant along any row or column. The following theorem proves this assertion.
Theorem 12.1.14 Let A be an n Ã— n matrix where n â‰¥2. Then
det (A) =
n
X
j=1
aij cof (A)ij =
n
X
i=1
aij cof (A)ij .
(12.21)
The ï¬rst formula consists of expanding the determinant along the ith row and the second
expands the determinant along the jth column.
Proof: Let (ai1, Â· Â· Â·, ain) be the ith row of A. Let Bj be the matrix obtained from A
by leaving every row the same except the ith row which in Bj equals (0, Â· Â· Â·, 0, aij, 0, Â· Â· Â·, 0) .
Then by Corollary 12.1.7,
det (A) =
n
X
j=1
det (Bj)
Denote by Aij the (n âˆ’1) Ã— (n âˆ’1) matrix obtained by deleting the ith row and the jth
column of A. Thus cof (A)ij â‰¡(âˆ’1)i+j det
Â¡
AijÂ¢
. At this point, recall that from Proposition
12.1.4, when two rows or two columns in a matrix, M, are switched, this results in multi-
plying the determinant of the old matrix by âˆ’1 to get the determinant of the new matrix.
Therefore, by Lemma 12.1.12,
det (Bj)
=
(âˆ’1)nâˆ’j (âˆ’1)nâˆ’i det
ÂµÂµ
Aij
âˆ—
0
aij
Â¶Â¶
=
(âˆ’1)i+j det
ÂµÂµ
Aij
âˆ—
0
aij
Â¶Â¶
= aij cof (A)ij .

12.1.
THE MATHEMATICAL THEORY OF DETERMINANTSâˆ—
237
Therefore,
det (A) =
n
X
j=1
aij cof (A)ij
which is the formula for expanding det (A) along the ith row. Also,
det (A)
=
det
Â¡
AT Â¢
=
n
X
j=1
aT
ij cof
Â¡
AT Â¢
ij
=
n
X
j=1
aji cof (A)ji
which is the formula for expanding det (A) along the ith column. This proves the theorem.
Note that this gives an easy way to write a formula for the inverse of an n Ã— n matrix.
Theorem 12.1.15 Aâˆ’1 exists if and only if det(A) Ì¸= 0. If det(A) Ì¸= 0, then
Aâˆ’1 =
Â¡
aâˆ’1
ij
Â¢
where
aâˆ’1
ij = det(A)âˆ’1 cof (A)ji
for cof (A)ij the ijth cofactor of A.
Proof: By Theorem 12.1.14 and letting (air) = A, if det (A) Ì¸= 0,
n
X
i=1
air cof (A)ir det(A)âˆ’1 = det(A) det(A)âˆ’1 = 1.
Now consider
n
X
i=1
air cof (A)ik det(A)âˆ’1
when k Ì¸= r. Replace the kth column with the rth column to obtain a matrix, Bk whose
determinant equals zero by Corollary 12.1.7. However, expanding this matrix along the kth
column yields
0 = det (Bk) det (A)âˆ’1 =
n
X
i=1
air cof (A)ik det (A)âˆ’1
Summarizing,
n
X
i=1
air cof (A)ik det (A)âˆ’1 = Î´rk.
Using the other formula in Theorem 12.1.14, and similar reasoning,
n
X
j=1
arj cof (A)kj det (A)âˆ’1 = Î´rk
This proves that if det (A) Ì¸= 0, then Aâˆ’1 exists with Aâˆ’1 =
Â¡
aâˆ’1
ij
Â¢
, where
aâˆ’1
ij = cof (A)ji det (A)âˆ’1 .
Now suppose Aâˆ’1 exists. Then by Theorem 12.1.11,
1 = det (I) = det
Â¡
AAâˆ’1Â¢
= det (A) det
Â¡
Aâˆ’1Â¢
so det (A) Ì¸= 0. This proves the theorem.
The next corollary points out that if an n Ã— n matrix, A has a right or a left inverse,
then it has an inverse.

238
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Corollary 12.1.16 Let A be an n Ã— n matrix and suppose there exists an n Ã— n matrix,
B such that BA = I. Then Aâˆ’1 exists and Aâˆ’1 = B. Also, if there exists C an nÃ—n matrix
such that AC = I, then Aâˆ’1 exists and Aâˆ’1 = C.
Proof: Since BA = I, Theorem 12.1.11 implies
det B det A = 1
and so det A Ì¸= 0. Therefore from Theorem 12.1.15, Aâˆ’1 exists. Therefore,
Aâˆ’1 = (BA) Aâˆ’1 = B
Â¡
AAâˆ’1Â¢
= BI = B.
The case where CA = I is handled similarly.
The conclusion of this corollary is that left inverses, right inverses and inverses are all
the same in the context of n Ã— n matrices.
Theorem 12.1.15 says that to ï¬nd the inverse, take the transpose of the cofactor matrix
and divide by the determinant. The transpose of the cofactor matrix is called the adjugate
or sometimes the classical adjoint of the matrix A. It is an abomination to call it the adjoint
although you do sometimes see it referred to in this way. In words, Aâˆ’1 is equal to one over
the determinant of A times the adjugate matrix of A.
In case you are solving a system of equations, Ax = y for x, it follows that if Aâˆ’1 exists,
x =
Â¡
Aâˆ’1A
Â¢
x = Aâˆ’1 (Ax) = Aâˆ’1y
thus solving the system. Now in the case that Aâˆ’1 exists, there is a formula for Aâˆ’1 given
above. Using this formula,
xi =
n
X
j=1
aâˆ’1
ij yj =
n
X
j=1
1
det (A) cof (A)ji yj.
By the formula for the expansion of a determinant along a column,
xi =
1
det (A) det
ï£«
ï£¬
ï£­
âˆ—
Â· Â· Â·
y1
Â· Â· Â·
âˆ—
...
...
...
âˆ—
Â· Â· Â·
yn
Â· Â· Â·
âˆ—
ï£¶
ï£·
ï£¸,
where here the ith column of A is replaced with the column vector, (y1 Â· Â· Â· Â·, yn)T , and the
determinant of this modiï¬ed matrix is taken and divided by det (A). This formula is known
as Cramerâ€™s rule.
Deï¬nition 12.1.17 A matrix M, is upper triangular if Mij = 0 whenever i > j.
Thus such a matrix equals zero below the main diagonal, the entries of the form Mii as
shown.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
âˆ—
Â· Â· Â·
âˆ—
0
âˆ—
...
...
...
...
...
âˆ—
0
Â· Â· Â·
0
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
A lower triangular matrix is deï¬ned similarly as a matrix for which all entries above the
main diagonal are equal to zero.
With this deï¬nition, here is a simple corollary of Theorem 12.1.14.

12.1.
THE MATHEMATICAL THEORY OF DETERMINANTSâˆ—
239
Corollary 12.1.18 Let M be an upper (lower) triangular matrix.
Then det (M) is
obtained by taking the product of the entries on the main diagonal.
Deï¬nition 12.1.19 A submatrix of a matrix A is the rectangular array of numbers
obtained by deleting some rows and columns of A. Let A be an m Ã— n matrix. The deter-
minant rank of the matrix equals r where r is the largest number such that some r Ã— r
submatrix of A has a non zero determinant. The row rank is deï¬ned to be the dimension
of the span of the rows. The column rank is deï¬ned to be the dimension of the span of the
columns.
Theorem 12.1.20 If A has determinant rank, r, then there exist r rows of the
matrix such that every other row is a linear combination of these r rows.
Proof: Suppose the determinant rank of A = (aij) equals r. If rows and columns are
interchanged, the determinant rank of the modiï¬ed matrix is unchanged. Thus rows and
columns can be interchanged to produce an r Ã— r matrix in the upper left corner of the
matrix which has non zero determinant. Now consider the r + 1 Ã— r + 1 matrix, M,
ï£«
ï£¬
ï£¬
ï£¬
ï£­
a11
Â· Â· Â·
a1r
a1p
...
...
...
ar1
Â· Â· Â·
arr
arp
al1
Â· Â· Â·
alr
alp
ï£¶
ï£·
ï£·
ï£·
ï£¸
where C will denote the rÃ—r matrix in the upper left corner which has non zero determinant.
I claim det (M) = 0.
There are two cases to consider in verifying this claim. First, suppose p > r. Then the
claim follows from the assumption that A has determinant rank r. On the other hand, if
p < r, then the determinant is zero because there are two identical columns. Expand the
determinant along the last column and divide by det (C) to obtain
alp = âˆ’
r
X
i=1
cof (M)ip
det (C) aip.
Now note that cof (M)ip does not depend on p. Therefore the above sum is of the form
alp =
r
X
i=1
miaip
which shows the lth row is a linear combination of the ï¬rst r rows of A. Since l is arbitrary,
this proves the theorem.
Corollary 12.1.21 The determinant rank equals the row rank.
Proof: From Theorem 12.1.20, the row rank is no larger than the determinant rank.
Could the row rank be smaller than the determinant rank? If so, there exist p rows for
p < r such that the span of these p rows equals the row space. But this implies that the
r Ã— r submatrix whose determinant is nonzero also has row rank no larger than p which is
impossible if its determinant is to be nonzero because at least one row is a linear combination
of the others.
Corollary 12.1.22 If A has determinant rank, r, then there exist r columns of the
matrix such that every other column is a linear combination of these r columns. Also the
column rank equals the determinant rank.

240
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Proof: This follows from the above by considering AT . The rows of AT are the columns
of A and the determinant rank of AT and A are the same. Therefore, from Corollary 12.1.21,
column rank of A = row rank of AT = determinant rank of AT = determinant rank of A.
The following theorem is of fundamental importance and ties together many of the ideas
presented above.
Theorem 12.1.23 Let A be an n Ã— n matrix. Then the following are equivalent.
1. det (A) = 0.
2. A, AT are not one to one.
3. A is not onto.
Proof: Suppose det (A) = 0. Then the determinant rank of A = r < n. Therefore,
there exist r columns such that every other column is a linear combination of these columns
by Theorem 12.1.20. In particular, it follows that for some m, the mth column is a linear
combination of all the others.
Thus letting A =
Â¡
a1
Â· Â· Â·
am
Â· Â· Â·
an
Â¢
where the
columns are denoted by ai, there exists scalars, Î±i such that
am =
X
kÌ¸=m
Î±kak.
Now consider the column vector, x â‰¡
Â¡
Î±1
Â· Â· Â·
âˆ’1
Â· Â· Â·
Î±n
Â¢T . Then
Ax = âˆ’am +
X
kÌ¸=m
Î±kak = 0.
Since also A0 = 0, it follows A is not one to one. Similarly, AT is not one to one by the
same argument applied to AT . This veriï¬es that 1.) implies 2.).
Now suppose 2.). Then since AT is not one to one, it follows there exists x Ì¸= 0 such that
AT x = 0.
Taking the transpose of both sides yields
xT A = 0
where the 0 is a 1 Ã— n matrix or row vector. Now if Ay = x, then
|x|2 = xT (Ay) =
Â¡
xT A
Â¢
y = 0y = 0
contrary to x Ì¸= 0. Consequently there can be no y such that Ay = x and so A is not onto.
This shows that 2.) implies 3.).
Finally, suppose 3.). If 1.) does not hold, then det (A) Ì¸= 0 but then from Theorem
12.1.15 Aâˆ’1 exists and so for every y âˆˆFn there exists a unique x âˆˆFn such that Ax = y.
In fact x = Aâˆ’1y. Thus A would be onto contrary to 3.). This shows 3.) implies 1.) and
proves the theorem.
Corollary 12.1.24 Let A be an n Ã— n matrix. Then the following are equivalent.
1. det(A) Ì¸= 0.
2. A and AT are one to one.
3. A is onto.
Proof: This follows immediately from the above theorem.

12.2.
THE CAYLEY HAMILTON THEOREMâˆ—
241
12.1.1
Exercises
1. Let m < n and let A be an m Ã— n matrix. Show that A is not one to one. Hint:
Consider the n Ã— n matrix, A1 which is of the form
A1 â‰¡
Âµ A
0
Â¶
where the 0 denotes an (n âˆ’m) Ã— n matrix of zeros. Thus det A1 = 0 and so A1 is
not one to one. Now observe that A1x is the vector,
A1x =
Âµ Ax
0
Â¶
which equals zero if and only if Ax = 0.
12.2
The Cayley Hamilton Theoremâˆ—
Deï¬nition 12.2.1 Let A be an n Ã— n matrix.
The characteristic polynomial is
deï¬ned as
pA (t) â‰¡det (tI âˆ’A)
and the solutions to pA (t) = 0 are called eigenvalues. For A a matrix and p (t) = tn +
anâˆ’1tnâˆ’1 + Â· Â· Â· + a1t + a0, denote by p (A) the matrix deï¬ned by
p (A) â‰¡An + anâˆ’1Anâˆ’1 + Â· Â· Â· + a1A + a0I.
The explanation for the last term is that A0 is interpreted as I, the identity matrix.
The Cayley Hamilton theorem states that every matrix satisï¬es its characteristic equa-
tion, that equation deï¬ned by PA (t) = 0. It is one of the most important theorems in linear
algebra. The following lemma will help with its proof.
Lemma 12.2.2 Suppose for all |Î»| large enough,
A0 + A1Î» + Â· Â· Â· + AmÎ»m = 0,
where the Ai are n Ã— n matrices. Then each Ai = 0.
Proof: Multiply by Î»âˆ’m to obtain
A0Î»âˆ’m + A1Î»âˆ’m+1 + Â· Â· Â· + Amâˆ’1Î»âˆ’1 + Am = 0.
Now let |Î»| â†’âˆto obtain Am = 0. With this, multiply by Î» to obtain
A0Î»âˆ’m+1 + A1Î»âˆ’m+2 + Â· Â· Â· + Amâˆ’1 = 0.
Now let |Î»| â†’âˆto obtain Amâˆ’1 = 0. Continue multiplying by Î» and letting Î» â†’âˆto
obtain that all the Ai = 0. This proves the lemma.
With the lemma, here is a simple corollary.
Corollary 12.2.3 Let Ai and Bi be n Ã— n matrices and suppose
A0 + A1Î» + Â· Â· Â· + AmÎ»m = B0 + B1Î» + Â· Â· Â· + BmÎ»m
for all |Î»| large enough. Then Ai = Bi for all i. Consequently if Î» is replaced by any n Ã— n
matrix, the two sides will be equal. That is, for C any n Ã— n matrix,
A0 + A1C + Â· Â· Â· + AmCm = B0 + B1C + Â· Â· Â· + BmCm.

242
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Proof: Subtract and use the result of the lemma.
With this preparation, here is a relatively easy proof of the Cayley Hamilton theorem.
Theorem 12.2.4 Let A be an n Ã— n matrix and let p (Î») â‰¡det (Î»I âˆ’A) be the
characteristic polynomial. Then p (A) = 0.
Proof: Let C (Î») equal the transpose of the cofactor matrix of (Î»I âˆ’A) for |Î»| large.
(If |Î»| is large enough, then Î» cannot be in the ï¬nite list of eigenvalues of A and so for such
Î», (Î»I âˆ’A)âˆ’1 exists.) Therefore, by Theorem 12.1.15
C (Î») = p (Î») (Î»I âˆ’A)âˆ’1 .
Note that each entry in C (Î») is a polynomial in Î» having degree no more than n âˆ’1.
Therefore, collecting the terms,
C (Î») = C0 + C1Î» + Â· Â· Â· + Cnâˆ’1Î»nâˆ’1
for Cj some n Ã— n matrix. It follows that for all |Î»| large enough,
(A âˆ’Î»I)
Â¡
C0 + C1Î» + Â· Â· Â· + Cnâˆ’1Î»nâˆ’1Â¢
= p (Î») I
and so Corollary 12.2.3 may be used. It follows the matrix coeï¬ƒcients corresponding to
equal powers of Î» are equal on both sides of this equation. Therefore, if Î» is replaced with
A, the two sides will be equal. Thus
0 = (A âˆ’A)
Â¡
C0 + C1A + Â· Â· Â· + Cnâˆ’1Anâˆ’1Â¢
= p (A) I = p (A) .
This proves the Cayley Hamilton theorem.
12.2.1
Exercises With Answers
1. Find the following determinant by expanding along the second column.
Â¯Â¯Â¯Â¯Â¯Â¯
1
3
1
2
1
5
2
1
1
Â¯Â¯Â¯Â¯Â¯Â¯
This is
3 (âˆ’1)2+1
Â¯Â¯Â¯Â¯
2
5
2
1
Â¯Â¯Â¯Â¯ + 1 (âˆ’1)1+1
Â¯Â¯Â¯Â¯
1
1
2
1
Â¯Â¯Â¯Â¯ + 1 (âˆ’1)3+2
Â¯Â¯Â¯Â¯
1
1
2
5
Â¯Â¯Â¯Â¯ = 20.
2. Compute the determinant by cofactor expansion. Pick the easiest row or column to
use.
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
2
0
0
1
2
1
1
0
0
0
0
3
2
3
3
1
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
You ought to use the third row. This yields
3
Â¯Â¯Â¯Â¯Â¯Â¯
2
0
0
2
1
1
2
3
3
Â¯Â¯Â¯Â¯Â¯Â¯
= (3) (2)
Â¯Â¯Â¯Â¯
1
1
3
3
Â¯Â¯Â¯Â¯ = 0.

12.2.
THE CAYLEY HAMILTON THEOREMâˆ—
243
3. Find the determinant using row and column operations.
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
5
4
3
2
3
2
4
3
âˆ’1
2
3
3
2
1
2
âˆ’2
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
Replace the ï¬rst row by 5 times the third added to it and then replace the second by
3 times the third added to it and then the last by 2 times the third added to it. This
yields
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
0
14
18
17
0
8
13
12
âˆ’1
2
3
3
0
5
8
4
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
Now lets replace the third column by âˆ’1 times the last column added to it.
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
0
14
1
17
0
8
1
12
âˆ’1
2
0
3
0
5
4
4
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
Now replace the top row by âˆ’1 times the second added to it and the bottom row by
âˆ’4 times the second added to it. This yields
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
0
6
0
5
0
8
1
12
âˆ’1
2
0
3
0
âˆ’27
0
âˆ’44
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
.
(12.22)
This looks pretty good because it has a lot of zeros. Expand along the ï¬rst column
and next along the second,
(âˆ’1)
Â¯Â¯Â¯Â¯Â¯Â¯
6
0
5
8
1
12
âˆ’27
0
âˆ’44
Â¯Â¯Â¯Â¯Â¯Â¯
= (âˆ’1) (1)
Â¯Â¯Â¯Â¯
6
5
âˆ’27
âˆ’44
Â¯Â¯Â¯Â¯ = 129.
Alternatively, you could continue doing row and column operations. Switch the third
and ï¬rst row in 12.22 to obtain
âˆ’
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
âˆ’1
2
0
3
0
8
1
12
0
6
0
5
0
âˆ’27
0
âˆ’44
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
Next take 9/2 times the third row and add to the bottom.
âˆ’
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
âˆ’1
2
0
3
0
8
1
12
0
6
0
5
0
0
0
âˆ’44 + (9/2) 5
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
.
Finally, take âˆ’6/8 times the second row and add to the third.
âˆ’
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
âˆ’1
2
0
3
0
8
1
12
0
0
âˆ’6/8
5 + (âˆ’6/8) (12)
0
0
0
âˆ’44 + (9/2) 5
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
.

244
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Therefore, since the matrix is now upper triangular, the determinant is
âˆ’((âˆ’1) (8) (âˆ’6/8) (âˆ’44 + (9/2) 5)) = 129.
4. An operation is done to get from the ï¬rst matrix to the second. Identify what was
done and tell how it will aï¬€ect the value of the determinant.
Âµ
a
b
c
d
Â¶
,
Âµ
a
c
b
d
Â¶
This involved taking the transpose so the determinant of the new matrix is the same
as the determinant of the ï¬rst matrix.
5. Show that for A a 2 Ã— 2 matrix det (aA) = a2 det (A) where a is a scalar.
a2 det (A) = a det (A1) where the ï¬rst row of A is replaced by a times it to get A1.
Then a det (A1) = A2 where A2 is obtained from A by multiplying both rows by a. In
other words, A2 = aA. Thus the conclusion is established.
6. Use Cramerâ€™s rule to ï¬nd y in
2x + 2y + z = 3
2x âˆ’y âˆ’z = 2
x + 2z = 1
From Cramerâ€™s rule,
y =
Â¯Â¯Â¯Â¯Â¯Â¯
2
3
1
2
2
âˆ’1
1
1
2
Â¯Â¯Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯Â¯Â¯
2
2
1
2
âˆ’1
âˆ’1
1
0
2
Â¯Â¯Â¯Â¯Â¯Â¯
= 5
13.
7. Here is a matrix,
ï£«
ï£­
et
eâˆ’t cos t
eâˆ’t sin t
et
âˆ’eâˆ’t cos t âˆ’eâˆ’t sin t
âˆ’eâˆ’t sin t + eâˆ’t cos t
et
2eâˆ’t sin t
âˆ’2eâˆ’t cos t
ï£¶
ï£¸
Does there exist a value of t for which this matrix fails to have an inverse? Explain.
det
ï£«
ï£­
et
eâˆ’t cos t
eâˆ’t sin t
et
âˆ’eâˆ’t cos t âˆ’eâˆ’t sin t
âˆ’eâˆ’t sin t + eâˆ’t cos t
et
2eâˆ’t sin t
âˆ’2eâˆ’t cos t
ï£¶
ï£¸= 5ete2(âˆ’t) cos2 t+5ete2(âˆ’t) sin2 t
= 5eâˆ’t which is never equal to zero for any value of t and so there is no value of t for
which the matrix has no inverse.
8. Use the formula for the inverse in terms of the cofactor matrix to ï¬nd if possible the
inverse of the matrix
ï£«
ï£­
1
2
3
0
6
1
4
1
1
ï£¶
ï£¸.
First you need to take the determinant
det
ï£«
ï£­
1
2
3
0
6
1
4
1
1
ï£¶
ï£¸= âˆ’59

12.2.
THE CAYLEY HAMILTON THEOREMâˆ—
245
and so the matrix has an inverse. Now you need to ï¬nd the cofactor matrix.
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Â¯Â¯Â¯Â¯
6
1
1
1
Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯
0
1
4
1
Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯
0
6
4
1
Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯
2
3
1
1
Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯
1
3
4
1
Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯
1
2
4
1
Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯
2
3
6
1
Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯
1
3
0
1
Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯
1
2
0
6
Â¯Â¯Â¯Â¯
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
=
ï£«
ï£­
5
4
âˆ’24
1
âˆ’11
7
âˆ’16
âˆ’1
6
ï£¶
ï£¸.
Thus the inverse is
1
âˆ’59
ï£«
ï£­
5
4
âˆ’24
1
âˆ’11
7
âˆ’16
âˆ’1
6
ï£¶
ï£¸
T
=
1
âˆ’59
ï£«
ï£­
5
1
âˆ’16
4
âˆ’11
âˆ’1
âˆ’24
7
6
ï£¶
ï£¸.
If you check this, it does work.
9. Find the eigenvectors and eigenvalues of the matrix, A =
ï£«
ï£­
8
âˆ’3
1
âˆ’2
7
1
0
0
10
ï£¶
ï£¸. Deter-
mine whether the matrix is defective. If nondefective, diagonalize the matrix with an
appropriate similarity transformation.
First you need to write the characteristic equation.
det
ï£«
ï£­Î»
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸âˆ’
ï£«
ï£­
8
âˆ’3
1
âˆ’2
7
1
0
0
10
ï£¶
ï£¸
ï£¶
ï£¸= det
ï£«
ï£­
Î» âˆ’8
3
âˆ’1
2
Î» âˆ’7
âˆ’1
0
0
Î» âˆ’10
ï£¶
ï£¸
(12.23)
= Î»3 âˆ’25Î»2 + 200Î» âˆ’500 = 0
Next you need to ï¬nd the solutions to this equation. Of course this is a real joy. If
there are any rational zeros they are
Â±factor of 500
factor of 1
I hope to ï¬nd a rational zero. If there are none, then I donâ€™t know what to do at this
point. This is a really lousy method for ï¬nding eigenvalues and eigenvectors. It only
works if things work out well. Lets try 10. You can plug it in and see if it works or
you can use synthetic division.
0
10
1
âˆ’25
200
âˆ’500
10
âˆ’150
500
1
âˆ’15
50
0
Yes, it appears 10 works and you can factor the polynomial as (Î» âˆ’10)
Â¡
Î»2 âˆ’15Î» + 50
Â¢
which factors further to (Î» âˆ’10) (Î» âˆ’5) (Î» âˆ’10) so you ï¬nd the eigenvalues are 5, 10,

246
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
and 10. It remains to ï¬nd the eigenvectors. First ï¬nd an eigenvector for Î» = 5. To do
this, you ï¬nd a vector which is sent to 0 by the matrix on the right in 12.23 in which
you let Î» = 5. Thus the augmented matrix of the system of equations you need to
solve to get the eigenvector is
ï£«
ï£­
5 âˆ’8
3
âˆ’1
|
0
2
5 âˆ’7
âˆ’1
|
0
0
0
5 âˆ’10
|
0
ï£¶
ï£¸
Now the row reduced echelon form is
ï£«
ï£­
1
âˆ’1
0
|
0
0
0
1
|
0
0
0
0
|
0
ï£¶
ï£¸
and so you need x = y and z = 0. An eigenvector is (1, 1, 0)T . Now you have the
glorious opportunity to solve for the eigenvectors associated with Î» = 10. You do it
the same way. The augmented matrix for the system of equations you solve to ï¬nd
the eigenvectors is
ï£«
ï£­
10 âˆ’8
3
âˆ’1
|
0
2
10 âˆ’7
âˆ’1
|
0
0
0
10 âˆ’10
|
0
ï£¶
ï£¸
The row reduced echelon form is
ï£«
ï£­
1
3
2
âˆ’1
2
|
0
0
0
0
|
0
0
0
0
|
0
ï£¶
ï£¸
and so you need x = âˆ’3
2y + 1
2z. It follows the eigenvectors for Î» = 10 are
Âµ
âˆ’3
2y + 1
2z, y, z
Â¶T
where x, y âˆˆR, not both equal to zero. Why? Let y = 2 and z = 0. This gives the
vector,
(âˆ’3, 2, 0)T
as one of the eigenvectors. You could also let y = 0 and z = 2 to obtain another
eigenvector,
(1, 0, 2)T .
If there exists a basis of eigenvectors, then the matrix is nondefective and as discussed
above, the matrix can be diagonalized by considering Sâˆ’1AS where the columns of S
are the eigenvectors. In this case, I have found three eigenvectors and so it remains
to determine whether these form a basis. Remember how to do this. You let them be
the columns of a matrix and then ï¬nd the rank of this matrix. If it is three, then they
are a basis because they are linearly independent and the vectors are in R3. This is
equivalent to the following matrix has an inverse.
ï£«
ï£­
1
âˆ’3
1
1
2
0
0
0
2
ï£¶
ï£¸
ï£«
ï£­
1
âˆ’3
1
1
2
0
0
0
2
ï£¶
ï£¸
âˆ’1
=
ï£«
ï£­
2
5
3
5
âˆ’1
5
âˆ’1
5
1
5
1
10
0
0
1
2
ï£¶
ï£¸

12.2.
THE CAYLEY HAMILTON THEOREMâˆ—
247
Then to diagonalize
ï£«
ï£­
2
5
3
5
âˆ’1
5
âˆ’1
5
1
5
1
10
0
0
1
2
ï£¶
ï£¸
ï£«
ï£­
8
âˆ’3
1
âˆ’2
7
1
0
0
10
ï£¶
ï£¸
ï£«
ï£­
1
âˆ’3
1
1
2
0
0
0
2
ï£¶
ï£¸=
ï£«
ï£­
5
0
0
0
10
0
0
0
10
ï£¶
ï£¸
Isnâ€™t this stuï¬€marvelous! You can know this matrix is nondefective at the point when
you ï¬nd the eigenvectors for the repeated eigenvalue. This eigenvalue was repeated
with multiplicity 2 and there were two parameters, y and z in the description of
the eigenvectors. Therefore, the matrix is nondefective. Also note that there is no
uniqueness for the similarity transformation.
10. Now consider the matrix,
ï£«
ï£­
2
1
0
0
1
0
âˆ’1
0
1
ï£¶
ï£¸. Find its eigenvectors and eigenvalues and
determine whether it is defective.
The characteristic equation is
det
ï£«
ï£­Î»
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸âˆ’
ï£«
ï£­
2
1
0
0
1
0
âˆ’1
0
1
ï£¶
ï£¸
ï£¶
ï£¸= 0
thus the characteristic equation is
(Î» âˆ’2) (Î» âˆ’1)2 = 0.
The zeros are 1, 1, 2. Lets ï¬nd the eigenvectors for Î» = 1. The augmented matrix for
the system you need to solve is
ï£«
ï£­
âˆ’1
âˆ’1
0
|
0
0
0
0
|
0
1
0
0
|
0
ï£¶
ï£¸
The row reduced echelon form is
ï£«
ï£­
1
0
0
|
0
0
1
0
|
0
0
0
0
|
0
ï£¶
ï£¸
Then you ï¬nd x = y = 0 and there is no restriction on z. Thus the eigenvectors are of
the form
(0, 0, z)T , z âˆˆR.
The eigenvalue had multiplicity 2 but the eigenvectors depend on only one parameter.
Therefore, the matrix is defective and cannot be diagonalized. The other eigenvector
comes from row reducing the following
2
ï£«
ï£­
1
0
0
|
0
0
1
0
|
0
0
0
1
|
0
ï£¶
ï£¸âˆ’
ï£«
ï£­
2
1
0
|
0
0
1
0
|
0
âˆ’1
0
1
|
0
ï£¶
ï£¸=
ï£«
ï£­
0
âˆ’1
0
|
0
0
1
0
|
0
1
0
1
|
0
ï£¶
ï£¸

248
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
The row reduced echelon form is
ï£«
ï£­
1
0
1
|
0
0
1
0
|
0
0
0
0
|
0
ï£¶
ï£¸
Therefore the eigenvectors are of the form (x, 0, âˆ’x)T . One such eigenvector is (1, 0, âˆ’1)T .
11. Let M be an n Ã— n matrix. Then deï¬ne the adjoint of M,denoted by M âˆ—to be the
transpose of the conjugate of M. For example,
Âµ
2
i
1 + i
3
Â¶âˆ—
=
Âµ
2
1 âˆ’i
âˆ’i
3
Â¶
.
A matrix, M, is self adjoint if M âˆ—= M. Show the eigenvalues of a self adjoint matrix
are all real. If the self adjoint matrix has all real entries, it is called symmetric. Show
that the eigenvalues and eigenvectors of a symmetric matrix occur in conjugate pairs.
First note that for x a vector, xâˆ—x = |x|2 . This is because
xâˆ—x =
X
k
xkxk =
X
k
|xk|2 â‰¡|x|2 .
Also note that (AB)âˆ—= Bâˆ—Aâˆ—because this holds for transposes.This implies that for
A an n Ã— m matrix,
xâˆ—Aâˆ—x = (Ax)âˆ—x
Then if Mx = Î»x
Î»xâˆ—x
=
(Î»x)âˆ—x = (Mx)âˆ—x = xâˆ—M âˆ—x
=
xâˆ—Mx = xâˆ—Î»x = Î»xâˆ—x
and so Î» = Î» showing that Î» must be real.
12. Suppose A is an n Ã— n matrix consisting entirely of real entries but a + ib is a complex
eigenvalue having the eigenvector, x + iy. Here x and y are real vectors. Show that
then a âˆ’ib is also an eigenvalue with the eigenvector, x âˆ’iy. Hint: You should
remember that the conjugate of a product of complex numbers equals the product of
the conjugates. Here a + ib is a complex number whose conjugate equals a âˆ’ib.
If A is real then the characteristic equation has all real coeï¬ƒcients. Therefore, letting
p (Î») be the characteristic polynomial,
0 = p (Î») = p (Î») = p
Â¡
Î»
Â¢
showing that Î» is also an eigenvalue.
13. Find the eigenvalues and eigenvectors of the matrix
ï£«
ï£­
âˆ’10
âˆ’2
11
âˆ’18
6
âˆ’9
10
âˆ’10
âˆ’2
ï£¶
ï£¸.
Determine whether the matrix is defective.
The matrix has eigenvalues âˆ’12 and 18. Of these, âˆ’12 is repeated with multiplicity
two. Therefore, you need to see whether the eigenspace has dimension two. If it does,

12.2.
THE CAYLEY HAMILTON THEOREMâˆ—
249
then the matrix is non defective. If it does not, then the matrix is defective. The row
reduced echelon form for the system you need to solve is
ï£«
ï£­
2
âˆ’2
11
|
0
âˆ’18
18
âˆ’9
|
0
10
âˆ’10
10
|
0
ï£¶
ï£¸
and its row reduced echelon form is
ï£«
ï£­
1
âˆ’1
0
|
0
0
0
1
|
0
0
0
0
|
0
ï£¶
ï£¸
Therefore, the eigenspace is of the form
ï£«
ï£­
t
t
0
ï£¶
ï£¸
This is only one dimensional and so the matrix is defective.
14. Here is a matrix. A =
ï£«
ï£­
1
2
0
0
âˆ’1
0
0
âˆ’2
1
ï£¶
ï£¸. Find a formula for An where n is an integer.
First you ï¬nd the eigenvectors and eigenvalues.
ï£«
ï£­
1
2
0
0
âˆ’1
0
0
âˆ’2
1
ï£¶
ï£¸, eigenvectors:
ï£«
ï£­
1
0
0
ï£¶
ï£¸,
ï£«
ï£­
0
0
1
ï£¶
ï£¸â†”1,
ï£«
ï£­
âˆ’1
1
1
ï£¶
ï£¸â†”âˆ’1.
The matrix, S used to diagonalize the matrix is obtained by letting these vectors be
the columns of S. Then Sâˆ’1 is given by
Sâˆ’1 =
ï£«
ï£­
1
1
0
0
âˆ’1
1
0
1
0
ï£¶
ï£¸
Then Sâˆ’1AS equals
ï£«
ï£­
1
1
0
0
âˆ’1
1
0
1
0
ï£¶
ï£¸
ï£«
ï£­
1
2
0
0
âˆ’1
0
0
âˆ’2
1
ï£¶
ï£¸
ï£«
ï£­
1
0
âˆ’1
0
0
1
0
1
1
ï£¶
ï£¸
=
ï£«
ï£­
1
0
0
0
1
0
0
0
âˆ’1
ï£¶
ï£¸â‰¡D
Then A = SDSâˆ’1 and An = SDnSâˆ’1. Now it is easy to ï¬nd Dn.
Dn =
ï£«
ï£­
1
0
0
0
1
0
0
0
(âˆ’1)n
ï£¶
ï£¸

250
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
Therefore,
An
=
ï£«
ï£­
1
0
âˆ’1
0
0
1
0
1
1
ï£¶
ï£¸
ï£«
ï£­
1
0
0
0
1
0
0
0
(âˆ’1)n
ï£¶
ï£¸
ï£«
ï£­
1
1
0
0
âˆ’1
1
0
1
0
ï£¶
ï£¸
=
ï£«
ï£­
1
1 âˆ’(âˆ’1)n
0
0
(âˆ’1)n
0
0
âˆ’1 + (âˆ’1)n
1
ï£¶
ï£¸.
15. Suppose the eigenvalues of A are Î»1, Â· Â· Â·, Î»n and that A is nondefective. Show that
eAt = S
ï£«
ï£¬
ï£­
eÎ»1t
Â· Â· Â·
0
...
...
...
0
Â· Â· Â·
eÎ»nt
ï£¶
ï£·
ï£¸Sâˆ’1 where S is the matrix which satisï¬es Sâˆ’1AS = D.
The diagonal matrix, D has the same characteristic equation as A why? and so it has
the same eigenvalues. However the eigenvalues of D are the diagonal entries and so
the diagonal entries of D are the eigenvalues of A. Now
Sâˆ’1tAS = tD
and
(tD)n =
ï£«
ï£¬
ï£­
(Î»1t)n
Â· Â· Â·
0
...
...
...
0
Â· Â· Â·
(Î»nt)n
ï£¶
ï£·
ï£¸
Therefore,
âˆ
X
n=0
1
n! (tD)n
=
âˆ
X
n=0
Â¡
Sâˆ’1tAS
Â¢n
n!
=
Sâˆ’1
âˆ
X
n=0
(tA)n
n!
S.
Now the left side equals
âˆ
X
n=0
1
n! (tD)n
=
âˆ
X
n=0
1
n!
ï£«
ï£¬
ï£­
(Î»1t)n
Â· Â· Â·
0
...
...
...
0
Â· Â· Â·
(Î»nt)n
ï£¶
ï£·
ï£¸
=
ï£«
ï£¬
ï£­
Pâˆ
n=0
(Î»1t)n
n!
Â· Â· Â·
0
...
...
...
0
Â· Â· Â·
Pâˆ
n=0
(Î»nt)n
n!
ï£¶
ï£·
ï£¸
=
ï£«
ï£¬
ï£­
eÎ»1t
Â· Â· Â·
0
...
...
...
0
Â· Â· Â·
eÎ»nt
ï£¶
ï£·
ï£¸.
Therefore,
etA â‰¡
âˆ
X
n=0
(tA)n
n!
= S
ï£«
ï£¬
ï£­
eÎ»1t
Â· Â· Â·
0
...
...
...
0
Â· Â· Â·
eÎ»nt
ï£¶
ï£·
ï£¸Sâˆ’1.

12.2.
THE CAYLEY HAMILTON THEOREMâˆ—
251
Do you think you understand this? If so, think again. What exactly do you mean by
an inï¬nite sum? Actually there is no problem here. You can do this just ï¬ne and the
sums converge in the sense that the ijth entries converge in the partial sums. Think
about this. You know what you need from calculus to see this.
16. Show that if A is similar to B then AT is similar to BT .
This is easy. A = Sâˆ’1BS and so AT = ST BT Â¡
Sâˆ’1Â¢T = ST BT Â¡
ST Â¢âˆ’1 .
17. Suppose Am = 0 for some m a positive integer. Show that if A is diagonalizable, then
A = 0.
Since Am = 0 suppose Sâˆ’1AS = D. Then raising to the mth power, Dm = Sâˆ’1AmS =
0. Therefore, D = 0. But then A = S0Sâˆ’1 = 0.
18. Find the complex eigenvalues and eigenvectors of the matrix
ï£«
ï£­
1
1
âˆ’6
7
âˆ’5
âˆ’6
âˆ’1
7
2
ï£¶
ï£¸.
Determine whether the matrix is defective.
After wading through much aï¬„iction you ï¬nd the eigenvalues are âˆ’6, 2 + 6i, 2 âˆ’6i.
Since these are distinct, the matrix cannot be defective. We must ï¬nd the eigenvectors
for these eigenvalues. The augmented matrix for the system of equations which must
be solved to ï¬nd the eigenvectors associated with 2 âˆ’6i is
ï£«
ï£­
âˆ’1 + 6i
1
âˆ’6
|
0
7
âˆ’7 + 6i
âˆ’6
|
0
âˆ’1
7
6i
|
0
ï£¶
ï£¸.
The row reduced echelon form is
ï£«
ï£­
1
0
i
0
0
1
i
0
0
0
0
0
ï£¶
ï£¸
and so the eigenvectors are of the form
t
ï£«
ï£­
âˆ’i
âˆ’i
1
ï£¶
ï£¸.
You can check this as follows
ï£«
ï£­
1
1
âˆ’6
7
âˆ’5
âˆ’6
âˆ’1
7
2
ï£¶
ï£¸
ï£«
ï£­
âˆ’i
âˆ’i
1
ï£¶
ï£¸=
ï£«
ï£­
âˆ’6 âˆ’2i
âˆ’6 âˆ’2i
2 âˆ’6i
ï£¶
ï£¸
and
(2 âˆ’6i)
ï£«
ï£­
âˆ’i
âˆ’i
1
ï£¶
ï£¸=
ï£«
ï£­
âˆ’6 âˆ’2i
âˆ’6 âˆ’2i
2 âˆ’6i
ï£¶
ï£¸.
It follows that the eigenvectors for Î» = 2 + 6i are
t
ï£«
ï£­
i
i
1
ï£¶
ï£¸.

252
EIGENVALUES AND EIGENVECTORS OF A MATRIX 4-6 OCT.
This is because A is real. If Av = Î»v, then taking the conjugate,
Av = Av = Î»v.
It only remains to ï¬nd the eigenvector for Î» = âˆ’6. The augmented matrix to row
reduce is
ï£«
ï£­
7
1
âˆ’6
|
0
7
1
âˆ’6
|
0
âˆ’1
7
8
|
0
ï£¶
ï£¸
The row reduced echelon form is
ï£«
ï£­
1
0
âˆ’1
|
0
0
1
1
|
0
0
0
0
|
0
ï£¶
ï£¸.
Then an eigenvector is
ï£«
ï£­
âˆ’1
1
âˆ’1
ï£¶
ï£¸.

Part VI
Curves, Curvilinear Motion,
Surfaces
253


255
Outcomes
Curves in Space
A. Identify the domain of a vector function.
B. Identify a curve given its parameterization.
C. Determine combinations of vector functions such as sums, vector products and scalar
products.
D. Deï¬ne limit, derivative and integral for vector functions.
E. Evaluate limits, derivatives and integrals of vector functions.
F. Find the line tangent to a curve at a given point.
G. Describe what is meant by arc length.
H. Evaluate the arc length of a curve.
I. Recall, derive and apply rules to combinations of vector functions for the following:
(a) limits
(b) diï¬€erentiation
(c) integration
Reading: Multivariable Calculus 1.6
Outcome Mapping:
A. 1
B. 2
C. 3,4
D. C1
E. C2,C3,C4
F. 5,16
G. C5
H. 6
I. 7,8,14
Curvilinear Motion
A. Sketch the curve determined by a vector function in 2-space or 3-space.
B. Parameterize a curve in 2-space or 3-space.
C. Given the position vector function of a moving object, calculate the velocity, speed,
and acceleration of the object.
D. Model and analyze curvilinear motion in applications.

256
Reading: Multivariable Calculus 1.7
Outcome Mapping:
A. D1
B. D2
C. 1
D. 3,5,6,10,18
Curvature
A. Recall the deï¬nitions of unit tangent, unit normal, binormal and osculating plane for
a space curve. Illustrate each graphically.
B. Calculate the curvature, the radius of curvature, the center of curvature and the
osculating plane for a space curve.
C. Derive formulas for the curvature of a parameterized curve and the curvature of a
plane curve given as a function.
D. Determine the tangential and normal components of acceleration for a given path.
Reading: Multivariable Calculus 1.8
Learning Module: Moving Trihedron
Outcome Mapping:
A. E1,E2
B. 1,5
C. 4
D. 2
Surfaces
A. Identify standard quadratic surfaces given their functions or graphs.
B. Sketch the graph of a quadratic surface by identifying the intercepts, traces, sections,
symmetry and boundedness or unboundedness of the surface.
Reading: Multivariable Calculus 1.4
Outcome Mapping:
A. 2,3
B. 3,4,6

Quadric Surfaces 9 Oct.
Quiz
1. Find the eigenvectors of the matrix,
ï£«
ï£­
4
1
0
2
8
3
âˆ’2
âˆ’2
3
ï£¶
ï£¸
given the eigenvalues are 6 and 3. Also tell whether the matrix is defective or non
defective.
2. Here is a matrix.
A =
ï£«
ï£­
3
4
1
4
0
1
4
3
4
0
âˆ’1
4
âˆ’1
4
1
2
ï£¶
ï£¸
Find A50. The eigenvalues of this matrix are 1 and 1/2 and eigenvectors for these
eigenvalues are
ï£«
ï£­
1
1
âˆ’1
ï£¶
ï£¸for Î» = 1 and
ï£«
ï£­
0
0
1
ï£¶
ï£¸and
ï£«
ï£­
âˆ’1
1
0
ï£¶
ï£¸for Î» = 1/2.
3. Here is a Markov matrix. This is also called a migration matrix or a stochastic matrix.
A =
ï£«
ï£­
1/2
1/3
1/4
0
1/3
1/4
1/2
1/3
1/2
ï£¶
ï£¸
Find
lim
nâ†’âˆAn
ï£«
ï£­
16
30
5
ï£¶
ï£¸.
Recall the equation of an arbitrary plane is an equation of the form ax + by + cz = d.
More generally, a set of points of the following form
{(x, y, z) : f (x, y, z) = c}
is called a level surface. There are some standard level surfaces which involve certain vari-
ables being raised to a power of 2 which are suï¬ƒciently important that they are given names,
usually involving the portentous semi-word â€œoidâ€. These are graphed below using Maple, a
computer algebra system.
257

258
QUADRIC SURFACES 9 OCT.
â€“4
â€“2
0
2
4
x
â€“4
â€“2
0
2
4
y
â€“4
â€“2
0
2
4
z2/a2 âˆ’x2/b2 âˆ’y2/c2 = 1
hyperboloid of two sheets
â€“4
â€“2
0
2
4
x
â€“4
â€“2
0
2
4
y
â€“4
â€“2
0
2
4
x2/b2 + y2/c2 âˆ’z2/a2 = 1
hyperboloid of one sheet
â€“4
â€“2
0
2
4
x
â€“4
â€“2
0
2
4
y
â€“4
â€“2
0
2
4
z = x2/a2 âˆ’y2/b2
hyperbolic paraboloid
â€“2
â€“1
0
1
2
x
â€“2
â€“1
0
1
y
0
1
2
3
4
x2/b2 + y2/c2 = z
elliptic paraboloid
â€“1
0
1
x
â€“2
â€“1
0
1
2
y
â€“2
â€“1
0
1
2
x2/a2 + y2/b2 + z2/c2 = 1
ellipsoid
â€“1
0
1
x
â€“2
â€“1
0
1
2
y
â€“2
â€“1
0
1
2
x2/b2 + y2/c2 = z2/a2
elliptic cone
Why do the graphs of these level surfaces look the way they do? Consider ï¬rst the

259
hyperboloid of two sheets. The equation deï¬ning this surface can be written in the form
z2
a2 âˆ’1 = x2
b2 + y2
c2 .
Suppose you ï¬x a value for z. What ordered pairs, (x, y) will satisfy the equation? If z2
a2 < 1,
there is no such ordered pair because the above equation would require a negative number
to equal a nonnegative one. This is why there is a gap and there are two sheets. If z2
a2 > 1,
then the above equation is the equation for an ellipse. That is why if you slice the graph by
letting z = z0 the result is an ellipse in the plane z = z0.
Consider the hyperboloid of one sheet.
x2
b2 + y2
c2 = 1 + z2
a2 .
This time, it doesnâ€™t matter what value z takes. The resulting equation for (x, y) is an
ellipse.
Similar considerations apply to the elliptic paraboloid as long as z > 0 and the ellipsoid.
The elliptic cone is like the hyperboloid of two sheets without the 1. Therefore, z can have
any value. In case z = 0, (x, y) = (0, 0) . Viewed from the side, it appears straight, not
curved like the hyperboloid of two sheets.This is because if (x, y, z) is a point on the surface,
then if t is a scalar, it follows (tx, ty, tz) is also on this surface.
The most interesting of these graphs is the hyperbolic paraboloid1, z = x2
a2 âˆ’y2
b2 . If z > 0
this is the equation of a hyperbola which opens to the right and left while if z < 0 it is a
hyperbola which opens up and down. As z passes from positive to negative, the hyperbola
changes type and this is what yields the shape shown in the picture.
Not surprisingly, you can ï¬nd intercepts and traces of quadric surfaces just as with
planes.
Example 13.0.5 Find the trace on the xy plane of the hyperbolic paraboloid, z = x2 âˆ’y2.
This occurs when z = 0 and so this reduces to y2 = x2. In other words, this trace is just
the two straight lines, y = x and y = âˆ’x.
Example 13.0.6 Find the intercepts of the ellipsoid, x2 + 2y2 + 4z2 = 9.
To ï¬nd the intercept on the x axis, let y = z = 0 and this yields x = Â±3. Thus there are
two intercepts, (3, 0, 0) and (âˆ’3, 0, 0) . The other intercepts are left for you to ï¬nd. You can
see this is an aid in graphing the quadric surface. The surface is said to be bounded if there is
some number, C such that whenever, (x, y, z) is a point on the surface,
p
x2 + y2 + z2 < C.
The surface is called unbounded if no such constant, C exists. Ellipsoids are bounded but
the other quadric surfaces are not bounded.
Example 13.0.7 Why is the hyperboloid of one sheet, x2 + 2y2 âˆ’z2 = 1 unbounded?
Let z be very large. Does there correspond (x, y) such that (x, y, z) is a point on the
hyperboloid of one sheet? Certainly. Simply pick any (x, y) on the ellipse x2 +2y2 = 1+z2.
Then
p
x2 + y2 + z2 is large, at lest as large as z. Thus it is unbounded.
You can also ï¬nd intersections between lines and surfaces.
Example 13.0.8 Find the points of intersection of the line (x, y, z) = (1 + t, 1 + 2t, 1 + t)
with the surface, z = x2 + y2.
1It is traditional to refer to this as a hyperbolic paraboloid. Not a parabolic hyperboloid.

260
QUADRIC SURFACES 9 OCT.
First of all, there is no guarantee there is any intersection at all. But if it exists, you
have only to solve the equation for t
1 + t = (1 + t)2 + (1 + 2t)2
This occurs at the two values of t = âˆ’1
2 + 1
10
âˆš
5, t = âˆ’1
2 âˆ’1
10
âˆš
5. Therefore, the two points
are
(1, 1, 1) +
Âµ
âˆ’1
2 + 1
10
âˆš
5
Â¶
(1, 2, 1) , and (1, 1, 1) +
Âµ
âˆ’1
2 âˆ’1
10
âˆš
5
Â¶
(1, 2, 1)
That is
Âµ1
2 + 1
10
âˆš
5, 1
5
âˆš
5, 1
2 + 1
10
âˆš
5
Â¶
,
Âµ1
2 âˆ’1
10
âˆš
5, âˆ’1
5
âˆš
5, 1
2 âˆ’1
10
âˆš
5
Â¶
.
A cylinder generated by a curve, C is the surface generated by moving the curve C
through space along a straight line. If you are given a level surface of the form f (x, y) = c
this will yield a cylinder parallel to the z axis. Here is why: If z = 0, then f (x, y) = c is a
curve in the plane, z = 0. If z = 1, then you get exactly the same curve but just shifted up
to a height of 1. Similarly, f (y, z) = c gives a cylinder parallel to the x axis and f (x, z) = c
gives one which is parallel to the y axis.
Example 13.0.9 Consider the cylinder x2 + y2 = 1. Sketch its graph.
y
z
x
Â¡
Â¡
You see that at every height above or below the z = 0 plane if you slice it at that level,
you will just see the graph of x2 + y2 = 1 at that level. This is a circle of radius 1. Since
the equation describing the surface does not depend on z, this is why it looks the same at
every level.

Curves In Space 10,11 Oct.
14.1
Limits Of A Vector Valued Function Of One Vari-
able
Quiz
1. Find the determinant of the matrix,
ï£«
ï£­
1
2
1
2
1
âˆ’1
âˆ’1
âˆ’3
1
ï£¶
ï£¸
2. Find all eigenspaces and eigenvalues for the matrix,
ï£«
ï£­
2
1
0
0
2
1
0
0
1
ï£¶
ï£¸
3. Find the eigenspace for the eigenvalue Î» = 2 for the matrix
ï£«
ï£­
1
1
0
0
2
0
âˆ’1
1
2
ï£¶
ï£¸
A vector valued function is just one which has vector values. For example, consider
Â¡
cos t, t2, t + 1
Â¢
where t âˆˆ[0, 2].
Each value of t corresponds to a point in R3 whose coordinates are
as given. Thus when t = 0, the point in R3 is (1, 0, 1) and when t = Ï€/2 the point is
Â³
0,
Â¡ Ï€
2
Â¢2 , Ï€
2 + 1
Â´
, etc. Often t will be considered as time. Thus, in this case, the vector
valued function gives the coordinates of a point which is moving in three dimensions as a
function of time. Imagine a ï¬‚y buzzing around the room for example. Let the origin be a
corner of the room and consider the position vector of the ï¬‚y. This position vector could
be described by a vector valued function of the form (x (t) , y (t) , z (t)) where t is in some
interval. Here x (t) is the x coordinate of the ï¬‚y, y (t) , the y coordinate, and z (t) , the z
coordinate corresponding to a given time. Later the physical signiï¬cance of all this will be
discussed more. For right now, t will just be in some interval and general vector valued
functions will be considered.
261

262
CURVES IN SPACE 10,11 OCT.
Deï¬nition 14.1.1 Let x (t) = (x1 (t) , Â· Â· Â·, xn (t)) for t âˆˆ[a, b] be a vector valued
function. The curve parameterized by this vector valued function is the set of points in
Rn which are obtained by letting t vary over the interval, [a, b]. The vector valued function is
also called a parameterization of this curve. The variable, t is called a parameter. More
generally, if x (t) = (x1 (t) , Â· Â· Â·, xn (t)) is given where each xi (t) is a formula the domain
of x is deï¬ned to be the set where each of the xi (t) is deï¬ned. It is denoted by D(x).
Example 14.1.2 Let x (t) =
Â¡ 1
t ,
âˆš
1 âˆ’t2, sin (t)
Â¢
ï¬nd the domain of x.
You need each function to make sense. Thus you must have âˆ’1 â‰¤t â‰¤1 and t Ì¸= 0. The
domain is [âˆ’1, 0) âˆª(0, 1].
In useful situations the domain will typically be an interval.
One can give a meaning to
lim
sâ†’t+ f (s) , lim
sâ†’tâˆ’f (s) , lim
sâ†’âˆf (s) ,
and
lim
sâˆ’âˆf (s) .
Deï¬nition 14.1.3 In the case where D (f) is only assumed to satisfy D (f) âŠ‡
(t, t + r) ,
lim
sâ†’t+ f (s) = L
if and only if for all Îµ > 0 there exists Î´ > 0 such that if
0 < s âˆ’t < Î´,
then
|f (s) âˆ’L| < Îµ.
In the case where D (f) is only assumed to satisfy D (f) âŠ‡(t âˆ’r, t) ,
lim
sâ†’tâˆ’f (s) = L
if and only if for all Îµ > 0 there exists Î´ > 0 such that if
0 < t âˆ’s < Î´,
then
|f (s) âˆ’L| < Îµ.
One can also consider limits as a variable â€œapproachesâ€ inï¬nity. Of course nothing is â€œcloseâ€
to inï¬nity and so this requires a slightly diï¬€erent deï¬nition.
lim
tâ†’âˆf (t) = L
if for every Îµ > 0 there exists l such that whenever t > l,
|f (t) âˆ’L| < Îµ
(14.1)
and
lim
tâ†’âˆ’âˆf (t) = L
if for every Îµ > 0 there exists l such that whenever t < l, 14.1 holds.

14.2.
THE DERIVATIVE AND INTEGRAL
263
Note that in all of this the deï¬nitions are identical to the case of scalar valued functions.
The only diï¬€erence is that here |Â·| refers to the norm or length in Rp where maybe p > 1.
Observation 14.1.4 Let f (t) = (f1 (t) , Â· Â· Â·, fn (t)) and let L = (L1, Â· Â· Â·, Ln) . Then
limtâ†’a f (t) = L if and only if limtâ†’a fk (t) = Lk for each k.
Example 14.1.5 Let f (t) =
Â¡
cos t, sin t, t2 + 1, ln (t)
Â¢
. Find limtâ†’Ï€/2 f (t) .
Using the above observation, this limit equals
Âµ
lim
tâ†’Ï€/2 cos t, lim
tâ†’Ï€/2 (sin t) , lim
tâ†’Ï€/2
Â¡
t2 + 1
Â¢
, lim
tâ†’Ï€/2 ln (t)
Â¶
=
Âµ
0, 1,
ÂµÏ€2
4 + 1
Â¶
, ln
Â³Ï€
2
Â´Â¶
.
Example 14.1.6 Let f (t) =
Â¡ sin t
t , t2, t + 1
Â¢
. Find limtâ†’0 f (t) .
Recall that limtâ†’0 sin t
t
= 1. Then using the above observation, limtâ†’0 f (t) = (1, 0, 1) .
14.2
The Derivative And Integral
The following deï¬nition is on the derivative and integral of a vector valued function of one
variable.
Deï¬nition 14.2.1 The derivative of a function, f â€² (t) , is deï¬ned as the following
limit whenever the limit exists. If the limit does not exist, then neither does f â€² (t) .
lim
hâ†’0
f (t + h) âˆ’f (x)
h
â‰¡f â€² (t)
The function of h on the left is called the diï¬€erence quotient just as it was for a scalar
valued function. If f (t) = (f1 (t) , Â· Â· Â·, fp (t)) and
R b
a fi (t) dt exists for each i = 1, Â·Â·Â·, p, then
R b
a f (t) dt is deï¬ned as the vector,
ÃƒZ b
a
f1 (t) dt, Â· Â· Â·,
Z b
a
fp (t) dt
!
.
This is what is meant by saying f âˆˆR ([a, b]) . In other words, f is Riemann integrable.
That is you can take the integral.
It is easier to write f âˆˆR ([a, b]) than to write f is Riemann integrable. Thus, if you see
f âˆˆR ([a, b]) , think:
R b
a f (x) dx exists.
This is exactly like the deï¬nition for a scalar valued function. As before,
f â€² (x) = lim
yâ†’x
f (y) âˆ’f (x)
y âˆ’x
.
As in the case of a scalar valued function, diï¬€erentiability implies continuity but not the
other way around.
Theorem 14.2.2 If f â€² (t) exists, then f is continuous at t.

264
CURVES IN SPACE 10,11 OCT.
Proof: Suppose Îµ > 0 is given and choose Î´1 > 0 such that if |h| < Î´1,
Â¯Â¯Â¯Â¯
f (t + h) âˆ’f (t)
h
âˆ’f â€² (t)
Â¯Â¯Â¯Â¯ < 1.
then for such h, the triangle inequality implies
|f (t + h) âˆ’f (t)| < |h| + |f â€² (t)| |h| .
Now letting Î´ < min
Â³
Î´1,
Îµ
1+|f â€²(x)|
Â´
it follows if |h| < Î´, then
|f (t + h) âˆ’f (t)| < Îµ.
Letting y = h + t, this shows that if |y âˆ’t| < Î´,
|f (y) âˆ’f (t)| < Îµ
which proves f is continuous at t. This proves the theorem.
As in the scalar case, there is a fundamental theorem of calculus.
Theorem 14.2.3 If f âˆˆR ([a, b]) and if f is continuous at t âˆˆ(a, b) , then
d
dt
ÂµZ t
a
f (s) ds
Â¶
= f (t) .
Proof: Say f (t) = (f1 (t) , Â· Â· Â·, fp (t)) . Then it follows
1
h
Z t+h
a
f (s) ds âˆ’1
h
Z t
a
f (s) ds =
Ãƒ
1
h
Z t+h
t
f1 (s) ds, Â· Â· Â·, 1
h
Z t+h
t
fp (s) ds
!
and limhâ†’0 1
h
R t+h
t
fi (s) ds = fi (t) for each i = 1, Â· Â· Â·, p from the fundamental theorem of
calculus for scalar valued functions. Therefore,
lim
hâ†’0
1
h
Z t+h
a
f (s) ds âˆ’1
h
Z t
a
f (s) ds = (f1 (t) , Â· Â· Â·, fp (t)) = f (t)
and this proves the claim.
Example 14.2.4 Let f (x) = c where c is a constant. Find f â€² (x) .
The diï¬€erence quotient,
f (x + h) âˆ’f (x)
h
= c âˆ’c
h
= 0
Therefore,
lim
hâ†’0
f (x + h) âˆ’f (x)
h
= lim
hâ†’0 0 = 0
Example 14.2.5 Let f (t) = (at, bt) where a, b are constants. Find f â€² (t) .
From the above discussion this derivative is just the vector valued functions whose com-
ponents consist of the derivatives of the components of f. Thus f â€² (t) = (a, b) .

14.2.
THE DERIVATIVE AND INTEGRAL
265
14.2.1
Arc Length
C is a smooth curve in Rn if there exists an interval, [a, b] âŠ†R and functions xi : [a, b] â†’R
such that the following conditions hold
1. xi is continuous on [a, b] .
2. xâ€²
i exists and is continuous and bounded on [a, b] , with xâ€²
i (a) deï¬ned as the derivative
from the right,
lim
hâ†’0+
xi (a + h) âˆ’xi (a)
h
,
and xâ€²
i (b) deï¬ned similarly as the derivative from the left.
3. For p (t) â‰¡(x1 (t) , Â· Â· Â·, xn (t)) , t â†’p (t) is one to one on (a, b) .
4. |pâ€² (t)| â‰¡
Â³Pn
i=1 |xâ€²
i (t)|2Â´1/2
Ì¸= 0 for all t âˆˆ[a, b] .
5. C = âˆª{(x1 (t) , Â· Â· Â·, xn (t)) : t âˆˆ[a, b]} .
The functions, xi (t) , deï¬ned above are giving the coordinates of a point in Rn and the
list of these functions is called a parameterization for the smooth curve. Note the natural
direction of the interval also gives a direction for moving along the curve. Such a direction
is called an orientation. The integral is used to deï¬ne what is meant by the length of such a
smooth curve. Consider such a smooth curve having parameterization (x1, Â· Â· Â·, xn) . Forming
a partition of [a, b], a = t0 < Â· Â· Â· < tn = b and letting pi = ( x1 (ti), Â· Â· Â·, xn (ti) ), you could
consider the polygon formed by lines from p0 to p1 and from p1 to p2 and from p3 to p4
etc. to be an approximation to the curve, C. The following picture illustrates what is meant
by this.
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
""""""
p0
p1
p2
p3
Now consider what happens when the partition is reï¬ned by including more points. You
can see from the following picture that the polygonal approximation would appear to be
even better and that as more points are added in the partition, the sum of the lengths of
the line segments seems to get close to something which deserves to be deï¬ned as the length
of the curve, C.
p0
p1
p2
p3

266
CURVES IN SPACE 10,11 OCT.
Thus the length of the curve is approximated by
n
X
k=1
|p (tk) âˆ’p (tkâˆ’1)| .
Since the functions in the parameterization are diï¬€erentiable, it is reasonable to expect this
to be close to
n
X
k=1
|pâ€² (tkâˆ’1)| (tk âˆ’tkâˆ’1)
which is seen to be a Riemann sum for the integral
Z b
a
|pâ€² (t)| dt
and it is this integral which is deï¬ned as the length of the curve.
Would the same length be obtained if another parameterization were used? This is a
very important question because the length of the curve should depend only on the curve
itself and not on the method used to trace out the curve. The answer to this question is
that the length of the curve does not depend on parameterization. It is proved in Section
16.2.2 which starts on Page 295.
Does the deï¬nition of length given above correspond to the usual deï¬nition of length in
the case when the curve is a line segment? It is easy to see that it does so by considering
two points in Rn, p and q. A parameterization for the line segment joining these two points
is
fi (t) â‰¡tpi + (1 âˆ’t) qi, t âˆˆ[0, 1] .
Using the deï¬nition of length of a smooth curve just given, the length according to this
deï¬nition is
Z 1
0
Ãƒ n
X
i=1
(pi âˆ’qi)2
!1/2
dt = |p âˆ’q| .
Thus this new deï¬nition which is valid for smooth curves which may not be straight line
segments gives the usual length for straight line segments.
Deï¬nition 14.2.6 A curve C is piecewise smooth if there exist points on this curve,
p0, p1, Â· Â· Â·, pn such that, denoting Cpkâˆ’1pk the part of the curve joining pkâˆ’1 and pk, it
follows Cpkâˆ’1pk is a smooth curve and âˆªn
k=1Cpkâˆ’1pk = C. In other words, it is piecewise
smooth if it consists of a ï¬nite number of smooth curves linked together.
To ï¬nd the length of a piecewise smooth curve, just sum the lengths of the smooth pieces
described above.
Example 14.2.7 The parameterization for a smooth curve is r (t) =
Â¡
t, 2t2, t2Â¢
for t âˆˆ
[0, 1] . Find the length of this curve.
From the above, the length is
Z 1
0
|râ€² (t)| dt =
Z 1
0
q
1 + (4t)2 + (2t)2dt = 1
2
âˆš
21 + 1
20
âˆš
5 ln
Â³
2
âˆš
5 +
âˆš
21
Â´
.
You need to use a trig substitution of some sort to do this integral but it is routine. Of
course, if you canâ€™t ï¬nd an antiderivative, then you solve it numerically.

14.2.
THE DERIVATIVE AND INTEGRAL
267
Example 14.2.8 The parameterization for a smooth curve is r (t) =
Â¡
t, t3, t2Â¢
for t âˆˆ[0, 1] .
Find the length of this curve.
The length is
Z 1
0
q
1 + (3t2)2 + (2t)2dt =
Z 1
0
p
1 + 9t4 + 4t2dt
and I have no clue how to ï¬nd an antiderivative for this. Therefore, I ï¬nd the integral
numerically.
Z 1
0
p
1 + 9t4 + 4t2dt = 1. 863
This is all right to do. Numerical methods are allowed and somtimes that is all you can get.
14.2.2
Geometric And Physical Signiï¬cance Of The Derivative
Suppose r is a vector valued function of a parameter, t not necessarily time and consider
the following picture of the points traced out by r.
r

r
Â©Â©Â©Â©Â©Â©Â©Â©Â©Â©Â©Â©Â©Â©
r
r              
r
r(t)
r(t + h)
-

1
Â©Â©Â©
*:
In this picture there are unit vectors in the direction of the vector from r (t) to r (t + h) .
You can see that it is reasonable to suppose these unit vectors, if they converge, converge
to a unit vector, T which is tangent to the curve at the point r (t) . Now each of these unit
vectors is of the form
r (t + h) âˆ’r (t)
|r (t + h) âˆ’r (t)| â‰¡Th.
Thus Th â†’T, a unit tangent vector to the curve at the point r (t) . Therefore,
râ€² (t)
â‰¡
lim
hâ†’0
r (t + h) âˆ’r (t)
h
= lim
hâ†’0
|r (t + h) âˆ’r (t)|
h
r (t + h) âˆ’r (t)
|r (t + h) âˆ’r (t)|
=
lim
hâ†’0
|r (t + h) âˆ’r (t)|
h
Th = |râ€² (t)| T.
In the case that t is time, the expression |r (t + h) âˆ’r (t)| is a good approximation for
the distance travelled by the object on the time interval [t, t + h] . The real distance would
be the length of the curve joining the two points but if h is very small, this is essentially
equal to |r (t + h) âˆ’r (t)| as suggested by the picture below.

268
CURVES IN SPACE 10,11 OCT.
r
r
r(t)
r(t + h)
Therefore,
|r (t + h) âˆ’r (t)|
h
gives for small h, the approximate distance travelled on the time interval, [t, t + h] divided
by the length of time, h. Therefore, this expression is really the average speed of the object
on this small time interval and so the limit as h â†’0, deserves to be called the instantaneous
speed of the object. Thus |râ€² (t)| T represents the speed times a unit direction vector, T
which deï¬nes the direction in which the object is moving. Thus râ€² (t) is the velocity of the
object. This is the physical signiï¬cance of the derivative when t is time.
How do you go about computing râ€² (t)? Letting r (t) = (r1 (t) , Â· Â· Â·, rq (t)) , the expression
r (t0 + h) âˆ’r (t0)
h
(14.2)
is equal to
Âµr1 (t0 + h) âˆ’r1 (t0)
h
, Â· Â· Â·, rq (t0 + h) âˆ’rq (t0)
h
Â¶
.
Then as h converges to 0, 14.2 converges to
v â‰¡(v1, Â· Â· Â·, vq)
where vk = râ€²
k (t) . This by Observation 14.1.4, which says that the term in 14.2 gets close
to a vector, v if and only if all the coordinate functions of the term in 14.2 get close to the
corresponding coordinate functions of v.
In the case where t is time, this simply says the velocity vector equals the vector whose
components are the derivatives of the components of the displacement vector, r (t) .
In any case, the vector, T determines a direction vector which is tangent to the curve at
the point, r (t) and so it is possible to ï¬nd parametric equations for the line tangent to the
curve at various points.
Example 14.2.9 Let r (t) =
Â¡
sin t, t2, t + 1
Â¢
for t âˆˆ[0, 5] . Find a tangent line to the curve
parameterized by r at the point r (2) .
From the above discussion, a direction vector has the same direction as râ€² (2) . There-
fore, it suï¬ƒces to simply use râ€² (2) as a direction vector for the line. râ€² (2) = (cos 2, 4, 1) .
Therefore, a parametric equation for the tangent line is
(sin 2, 4, 3) + t (cos 2, 4, 1) = (x, y, z) .
Example 14.2.10 Let r (t) =
Â¡
sin t, t2, t + 1
Â¢
for t âˆˆ[0, 5] . Find the velocity vector when
t = 1.
From the above discussion, this is simply râ€² (1) = (cos 1, 2, 1) .

14.2.
THE DERIVATIVE AND INTEGRAL
269
14.2.3
Diï¬€erentiation Rules
There are rules which relate the derivative to the various operations done with vectors such
as the dot product, the cross product, and vector addition and scalar multiplication.
Theorem 14.2.11 Let a, b âˆˆR and suppose f â€² (t) and gâ€² (t) exist. Then the fol-
lowing formulas are obtained.
(af + bg)â€² (t) = af â€² (t) + bgâ€² (t) .
(14.3)
(f Â· g)â€² (t) = f â€² (t) Â· g (t) + f (t) Â· gâ€² (t)
(14.4)
If f, g have values in R3, then
(f Ã— g)â€² (t) = f (t) Ã— gâ€² (t) + f â€² (t) Ã— g (t)
(14.5)
The formulas, 14.4, and 14.5 are referred to as the product rule.
Proof: The ï¬rst formula is left for you to prove. Consider the second, 14.4.
lim
hâ†’0
f Â· g (t + h) âˆ’fg (t)
h
= lim
hâ†’0
f (t + h) Â· g (t + h) âˆ’f (t + h) Â· g (t)
h
+ f (t + h) Â· g (t) âˆ’f (t) Â· g (t)
h
= lim
hâ†’0
Âµ
f (t + h) Â· (g (t + h) âˆ’g (t))
h
+ (f (t + h) âˆ’f (t))
h
Â· g (t)
Â¶
= lim
hâ†’0
n
X
k=1
fk (t + h) (gk (t + h) âˆ’gk (t))
h
+
n
X
k=1
(fk (t + h) âˆ’fk (t))
h
gk (t)
=
n
X
k=1
fk (t) gâ€²
k (t) +
n
X
k=1
f â€²
k (t) gk (t)
= f â€² (t) Â· g (t) + f (t) Â· gâ€² (t) .
Formula 14.5 is left as an exercise which follows from the product rule and the deï¬nition of
the cross product in terms of components given on Page 50. You can also see this is true
by using the distributive law of the cross product.
f (t + h) Ã—g (t + h) âˆ’f (t) Ã—g (t)
=
f (t + h) Ã—g (t + h) âˆ’f (t + h) Ã—g (t) + f (t + h) Ã—g (t) âˆ’f (t) Ã—g (t)
and so
1
h (f (t + h) Ã—g (t + h) âˆ’f (t) Ã—g (t))
=
f (t + h) Ã—
Âµg (t + h) âˆ’g (t)
h
Â¶
+
Âµf (t + h) âˆ’f (t)
h
Â¶
Ã—g (t)
Now assuming the cross product is continuous, (This is obvious from either the component
or the geometric description of the cross product.) you can take a limit in the above as
h â†’0 and obtain
f (t) Ã— gâ€² (t) + f â€² (t) Ã— g (t) .
It is exactly like the product rule for scalar valued functions except you need to be very
careful about the order in which things are multiplied becasue the cross product is not
commutative.

270
CURVES IN SPACE 10,11 OCT.
Example 14.2.12 Let
r (t) =
Â¡
t2, sin t, cos t
Â¢
and let p (t) = (t, ln (t + 1) , 2t). Find (r (t) Ã— p (t))â€² .
From 14.5 this equals(2t, cos t, âˆ’sin t) Ã— (t, ln (t + 1) , 2t) +
Â¡
t2, sin t, cos t
Â¢
Ã—
Â³
1,
1
t+1, 2
Â´
=
Â¡
2 (cos t) t + sin t ln (t + 1) , âˆ’(sin t) t âˆ’4t2, 2t ln (t + 1) âˆ’(cos t) t
Â¢
+
Âµ
2 sin t âˆ’cos t
t + 1, cos t âˆ’2t2,
t2
t + 1 âˆ’sin t
Â¶
= (2 (cos t) t + sin t ln (t + 1) + 2 sin t âˆ’cos t
t+1 , âˆ’(sin t) t âˆ’6t2 + cos t,
2t ln (t + 1) âˆ’(cos t) t +
t2
t+1 âˆ’sin t)
Example 14.2.13 Let r (t) =
Â¡
t2, sin t, cos t
Â¢
Find
R Ï€
0 r (t) dt.
This equals
Â¡R Ï€
0 t2 dt,
R Ï€
0 sin t dt,
R Ï€
0 cos t dt
Â¢
=
Â¡ 1
3Ï€3, 2, 0
Â¢
.
Example 14.2.14 An object has position r (t) =
Â³
t3,
t
1+1,
âˆš
t2 + 2
Â´
kilometers where t is
given in hours. Find the velocity of the object in kilometers per hour when t = 1.
Recall the velocity at time t was râ€² (t) . Therefore, ï¬nd râ€² (t) and plug in t = 1 to ï¬nd
the velocity.
râ€² (t) =
Ãƒ
3t2, 1 (1 + t) âˆ’t
(1 + t)2
, 1
2
Â¡
t2 + 2
Â¢âˆ’1/2 2t
!
=
Ãƒ
3t2,
1
(1 + t)2 ,
1
p
(t2 + 2)
t
!
When t = 1, the velocity is
râ€² (1) =
Âµ
3, 1
4, 1
âˆš
3
Â¶
kilometers per hour.
Obviously, this can be continued. That is, you can consider the possibility of taking the
derivative of the derivative and then the derivative of that and so forth. The main thing to
consider about this is the notation and it is exactly like it was in the case of a scalar valued
function presented earlier. Thus râ€²â€² (t) denotes the second derivative.
When you are given a vector valued function of one variable, sometimes it is possible to
give a simple description of the curve which results. Usually it is not possible to do this!
Example 14.2.15 Describe the curve which results from the vector valued function, r (t) =
(cos 2t, sin 2t, t) where t âˆˆR.
The ï¬rst two components indicate that for r (t) = (x (t) , y (t) , z (t)) , the pair, (x (t) , y (t))
traces out a circle. While it is doing so, z (t) is moving at a steady rate in the positive di-
rection. Therefore, the curve which results is a cork skrew shaped thing called a helix.
As an application of the theorems for diï¬€erentiating curves, here is an interesting appli-
cation. It is also a situation where the curve can be identiï¬ed as something familiar.

14.2.
THE DERIVATIVE AND INTEGRAL
271
Example 14.2.16 Sound waves have the angle of incidence equal to the angle of reï¬‚ection.
Suppose you are in a large room and you make a sound. The sound waves spread out and
you would expect your sound to be inaudible very far away. But what if the room were shaped
so that the sound is reï¬‚ected oï¬€the wall toward a single point, possibly far away from you?
Then you might have the interesting phenomenon of someone far away hearing what you
said quite clearly. How should the room be designed?
Suppose you are located at the point P0 and the point where your sound is to be reï¬‚ected
is P1. Consider a plane which contains the two points and let r (t) denote a parameterization
of the intersection of this plane with the walls of the room. Then the condition that the angle
of reï¬‚ection equals the angle of incidence reduces to saying the angle between P0 âˆ’r (t) and
âˆ’râ€² (t) equals the angle between P1 âˆ’r (t) and râ€² (t) . Draw a picture to see this. Therefore,
(P0 âˆ’r (t)) Â· (âˆ’râ€² (t))
|P0 âˆ’r (t)| |râ€² (t)|
= (P1 âˆ’r (t)) Â· (râ€² (t))
|P1 âˆ’r (t)| |râ€² (t)| .
This reduces to
(r (t) âˆ’P0) Â· (âˆ’râ€² (t))
|r (t) âˆ’P0|
= (r (t) âˆ’P1) Â· (râ€² (t))
|r (t) âˆ’P1|
(14.6)
Now
(r (t) âˆ’P1) Â· (râ€² (t))
|r (t) âˆ’P1|
= d
dt |r (t) âˆ’P1|
and a similar formula holds for P1 replaced with P0. This is because
|r (t) âˆ’P1| =
p
(r (t) âˆ’P1) Â· (r (t) âˆ’P1)
and so using the chain rule and product rule,
d
dt |r (t) âˆ’P1|
=
1
2 ((r (t) âˆ’P1) Â· (r (t) âˆ’P1))âˆ’1/2 2 ((r (t) âˆ’P1) Â· râ€² (t))
=
(r (t) âˆ’P1) Â· (râ€² (t))
|r (t) âˆ’P1|
.
Therefore, from 14.6,
d
dt (|r (t) âˆ’P1|) + d
dt (|r (t) âˆ’P0|) = 0
showing that |r (t) âˆ’P1| + |r (t) âˆ’P0| = C for some constant, C.This implies the curve of
intersection of the plane with the room is an ellipse having P0 and P1 as the foci.
14.2.4
Leibnizâ€™s Notation
Leibnizâ€™s notation also generalizes routinely. For example, dy
dt = yâ€² (t) with other similar
notations holding.
14.2.5
Exercises With Answers
1. Find the following limits if possible
(a) limxâ†’0+
Â³
|x|
x , sin 2x/x, tan x
x
Â´
= (1, 2, 1)
(b) limxâ†’0+
Â³
x
|x|, cos x, e2xÂ´
= (1, 1, 1)
(c) limxâ†’4
Â³
x2âˆ’16
x+4 , x âˆ’7, tan 7x
5x
Â´
=
Â¡
0, âˆ’3, 7
5
Â¢

272
CURVES IN SPACE 10,11 OCT.
2. Let r (t) =
Â³
4 + (t âˆ’1)2 ,
âˆš
t2 + 1 (t âˆ’1)3 , (tâˆ’1)3
t5
Â´
describe the position of an object
in R3 as a function of t where t is measured in seconds and r (t) is measured in meters.
Is the velocity of this object ever equal to zero? If so, ï¬nd the value of t at which this
occurs and the point in R3 at which the velocity is zero.
You need to diï¬€erentiate this. râ€² (t) =
Âµ
2 (t âˆ’1) , (t âˆ’1)2 4t2âˆ’t+3
âˆš
(t2+1), âˆ’(t âˆ’1)2 2tâˆ’5
t6
Â¶
.
Now you need to ï¬nd the value(s) of t where râ€² (t) = 0.
3. Let r (t) =
Â¡
sin t, t2, 2t + 1
Â¢
for t âˆˆ[0, 4] . Find a tangent line to the curve parameter-
ized by r at the point r (2) .
râ€² (t) = (cos t, 2t, 2). When t = 2, the point on the curve is (sin 2, 4, 5) . A direction
vector is râ€² (2) and so a tangent line is r (t) = (sin 2, 4, 5) + t (cos 2, 4, 2) .
4. Let r (t) =
Â¡
sin t, cos
Â¡
t2Â¢
, t + 1
Â¢
for t âˆˆ[0, 5] . Find the velocity when t = 3.
râ€² (t) =
Â¡
cos t, âˆ’2t sin
Â¡
t2Â¢
, 1
Â¢
. The velocity when t = 3 is just râ€² (3) = (cos 3, âˆ’6 sin (9) , 1) .
5. Suppose r (t), s (t) , and p (t) are three diï¬€erentiable functions of t which have values
in R3. Find a formula for (r (t) Ã— s (t) Â· p (t))â€² .
From the product rules for the cross and dot product, this equals
(r (t) Ã— s (t))â€²Â·p (t)+r (t)Ã—s (t)Â·pâ€² (t) = râ€² (t)Ã—s (t)Â·p (t)+r (t)Ã—sâ€² (t)Â·p (t)+r (t)Ã—s (t)Â·pâ€² (t)
6. If râ€² (t) = 0 for all t âˆˆ(a, b), show there exists a constant vector, c such that r (t) = c
for all t âˆˆ(a, b) .
Do this by considering standard one variable calculus and on the components of r (t) .
7. If Fâ€² (t) = f (t) for all t âˆˆ(a, b) and F is continuous on [a, b] , show
R b
a f (t) dt =
F (b) âˆ’F (a) .
Do this by considering standard one variable calculus and on the components of r (t) .
8. Verify that if â„¦Ã— u = 0 for all u, then â„¦= 0.
Geometrically this says that if â„¦is not equal to zero then it is parallel to every vector.
Why does this make it obvious that â„¦must equal zero?

Newtonâ€™s Laws Of Motionâˆ—
I assume you have seen basic mechanics as found in introductory physics course. However,
if you need a review, the following section is oï¬€ered. Read it if you need to. Otherwise,
skip it. Calculus was invented to solve problems in physics and engineering, not to do cute
geometry. The material which follows on physics of motion on a space curve will make more
sense to you if you know Newtonâ€™s laws.
Deï¬nition 15.0.17 Let r (t) denote the position of an object. Then the accelera-
tion of the object is deï¬ned to be râ€²â€² (t) .
Newtonâ€™s1 ï¬rst law is: â€œEvery body persists in its state of rest or of uniform motion in
a straight line unless it is compelled to change that state by forces impressed on it.â€
Newtonâ€™s second law is:
F = ma =mrâ€²â€² (t)
(15.1)
where a is the acceleration and m is the mass of the object.
Newtonâ€™s third law states: â€œTo every action there is always opposed an equal reaction;
or, the mutual actions of two bodies upon each other are always equal, and directed to
contrary parts.â€
Of these laws, only the second two are independent of each other, the ï¬rst law being
implied by the second. The third law says roughly that if you apply a force to something,
the thing applies the same force back.
The second law is the one of most interest. Note that the statement of this law depends
on the concept of the derivative because the acceleration is deï¬ned as a derivative. Newton
used calculus and these laws to solve profound problems involving the motion of the planets
and other problems in mechanics. The next example involves the concept that if you know
the force along with the initial velocity and initial position, then you can determine the
position.
Example 15.0.18 Let r (t) denote the position of an object of mass 2 kilogram at time
t and suppose the force acting on the object is given by F (t) =
Â¡
t, 1 âˆ’t2, 2eâˆ’tÂ¢
. Suppose
r (0) = (1, 0, 1) meters, and râ€² (0) = (0, 1, 1) meters/sec. Find r (t) .
1Isaac Newton 1642-1727 is often credited with inventing calculus although this is not correct since most
of the ideas were in existence earlier. However, he made major contributions to the subject partly in order
to study physics and astronomy. He formulated the laws of gravity, made major contributions to optics, and
stated the fundamental laws of mechanics listed here. He invented a version of the binomial theorem when
he was only 23 years old and built a reï¬‚ecting telescope. He showed that Keplerâ€™s laws for the motion of the
planets came from calculus and his laws of gravitation. In 1686 he published an important book, Principia,
in which many of his ideas are found. Newton was also very interested in theology and had strong views on
the nature of God which were based on his study of the Bible and early Christian writings. He ï¬nished his
life as Master of the Mint.
273

274
NEWTONâ€™S LAWS OF MOTIONâˆ—
By Newtonâ€™s second law, 2râ€²â€² (t) = F (t) =
Â¡
t, 1 âˆ’t2, 2eâˆ’tÂ¢
and so
râ€²â€² (t) =
Â¡
t/2,
Â¡
1 âˆ’t2Â¢
/2, eâˆ’tÂ¢
.
Therefore the velocity is given by
râ€² (t) =
Âµt2
4 , t âˆ’t3/3
2
, âˆ’eâˆ’t
Â¶
+ c
where c is a constant vector which must be determined from the initial condition given for
the velocity. Thus letting c = (c1, c2, c3) ,
(0, 1, 1) = (0, 0, âˆ’1) + (c1, c2, c3)
which requires c1 = 0, c2 = 1, and c3 = 2. Therefore, the velocity is found.
râ€² (t) =
Âµt2
4 , t âˆ’t3/3
2
+ 1, âˆ’eâˆ’t + 2
Â¶
.
Now from this, the displacement must equal
r (t) =
Âµ t3
12, t2/2 âˆ’t4/12
2
+ t, eâˆ’t + 2t
Â¶
+ (C1, C2, C3)
where the constant vector, (C1, C2, C3) must be determined from the initial condition for
the displacement. Thus
r (0) = (1, 0, 1) = (0, 0, 1) + (C1, C2, C3)
which means C1 = 1, C2 = 0, and C3 = 0. Therefore, the displacement has also been found.
r (t) =
Âµ t3
12 + 1, t2/2 âˆ’t4/12
2
+ t, eâˆ’t + 2t
Â¶
meters.
Actually, in applications of this sort of thing acceleration does not usually come to you as a
nice given function written in terms of simple functions you understand. Rather, it comes as
measurements taken by instruments and the position is continuously being updated based
on this information. Another situation which often occurs is the case when the forces on
the object depend not just on time but also on the position or velocity of the object.
Example 15.0.19 An artillery piece is ï¬red at ground level on a level plain. The angle of
elevation is Ï€/6 radians and the speed of the shell is 400 meters per second. How far does
the shell ï¬‚y before hitting the ground?
Neglect air resistance in this problem. Also let the direction of ï¬‚ight be along the positive
x axis. Thus the initial velocity is the vector, 400 cos (Ï€/6) i + 400 sin (Ï€/6) j while the only
force experienced by the shell after leaving the artillery piece is the force of gravity, âˆ’mgj
where m is the mass of the shell. The acceleration of gravity equals 9.8 meters per sec2 and
so the following needs to be solved.
mrâ€²â€² (t) = âˆ’mgj, r (0) = (0, 0) , râ€² (0) = 400 cos (Ï€/6) i + 400 sin (Ï€/6) j.
Denoting r (t) as (x (t) , y (t)) ,
xâ€²â€² (t) = 0, yâ€²â€² (t) = âˆ’g.

275
Therefore, yâ€² (t) = âˆ’gt+C and from the information on the initial velocity, C = 400 sin (Ï€/6) =
200. Thus
y (t) = âˆ’4.9t2 + 200t + D.
D = 0 because the artillery piece is ï¬red at ground level which requires both x and y to
equal zero at this time. Similarly, xâ€² (t) = 400 cos (Ï€/6) so x (t) = 400 cos (Ï€/6) t = 200
âˆš
3t.
The shell hits the ground when y = 0 and this occurs when âˆ’4.9t2 +200t = 0. Thus t = 40.
816 326 530 6 seconds and so at this time,
x = 200
âˆš
3 (40. 816 326 530 6) = 14139. 190 265 9 meters.
The next example is more complicated because it also takes in to account air resistance. We
do not live in a vacume.
Example 15.0.20 A lump of â€œblue iceâ€ escapes the lavatory of a jet ï¬‚ying at 600 miles
per hour at an altitude of 30,000 feet. This blue ice weighs 64 pounds near the earth and
experiences a force of air resistance equal to (âˆ’.1) râ€² (t) pounds.
Find the position and
velocity of the blue ice as a function of time measured in seconds. Also ï¬nd the velocity
when the lump hits the ground.
Such lumps have been known to surprise people on the
ground.
The ï¬rst thing needed is to obtain information which involves consistent units. The blue
ice weighs 32 pounds near the earth. Thus 32 pounds is the force exerted by gravity on the
lump and so its mass must be given by Newtonâ€™s second law as follows.
64 = m Ã— 32.
Thus m = 2 slugs. The slug is the unit of mass in the system involving feet and pounds.
The jet is ï¬‚ying at 600 miles per hour. I want to change this to feet per second. Thus it
ï¬‚ies at
600 Ã— 5280
60 Ã— 60
= 880 feet per second.
The explanation for this is that there are 5280 feet in a mile and so it goes 600Ã—5280 feet
in one hour. There are 60 Ã— 60 seconds in an hour. The position of the lump of blue ice will
be computed from a point on the ground directly beneath the airplane at the instant the
blue ice escapes and regard the airplane as moving in the direction of the positive x axis.
Thus the initial displacement is
r (0) = (0, 30000) feet
and the initial velocity is
râ€² (0) = (880, 0) feet/sec.
The force of gravity is
(0, âˆ’64) pounds
and the force due to air resistance is
(âˆ’.1) râ€² (t) pounds.
Newtons second law yields the following initial value problem for r (t) = (r1 (t) , r2 (t)) .
2 (râ€²â€²
1 (t) , râ€²â€²
2 (t)) = (âˆ’.1) (râ€²
1 (t) , râ€²
2 (t)) + (0, âˆ’64) , (r1 (0) , r2 (0)) = (0, 30000) ,
(râ€²
1 (0) , râ€²
2 (0)) = (880, 0)

276
NEWTONâ€™S LAWS OF MOTIONâˆ—
Therefore,
2râ€²â€²
1 (t) + (.1) râ€²
1 (t) = 0
2râ€²â€²
2 (t) + (.1) râ€²
2 (t) = âˆ’64
r1 (0) = 0
r2 (0) = 30000
râ€²
1 (0) = 880
râ€²
2 (0) = 0
(15.2)
To save on repetition solve
mrâ€²â€² + krâ€² = c, r (0) = u, râ€² (0) = v.
Divide the diï¬€erential equation by m and get
râ€²â€² + (k/m) râ€² = c/m.
Now multiply both sides by e(k/m)t. You should check this gives
d
dt
Â³
e(k/m)trâ€²Â´
= (c/m) e(k/m)t
Therefore,
e(k/m)trâ€² = 1
k e
k
m tc + C
and using the initial condition, v = c/k + C and so
râ€² (t) = (c/k) + (v âˆ’(c/k)) eâˆ’k
m t
Now this implies
r (t) = (c/k) t âˆ’1
k meâˆ’k
m t Â³
v âˆ’c
k
Â´
+ D
(15.3)
where D is a constant to be determined from the initial conditions. Thus
u = âˆ’m
k
Â³
v âˆ’c
k
Â´
+ D
and so
r (t) = (c/k) t âˆ’1
k meâˆ’k
m t Â³
v âˆ’c
k
Â´
+
Â³
u + m
k
Â³
v âˆ’c
k
Â´Â´
.
Now apply this to the system 15.2 to ï¬nd
r1 (t) = âˆ’1
(.1)2
Âµ
exp
Âµâˆ’(.1)
2
t
Â¶Â¶
(880) +
Âµ 2
(.1) (880)
Â¶
= âˆ’17600.0 exp (âˆ’.0 5t) + 17600.0
and
r2 (t) = (âˆ’64/ (.1)) t âˆ’
1
(.1)2
Âµ
exp
Âµ
âˆ’(.1)
2 t
Â¶Â¶ Âµ 64
(.1)
Â¶
+
Âµ
30000 +
2
(.1)
Âµ 64
(.1)
Â¶Â¶
= âˆ’640.0t âˆ’12800.0 exp (âˆ’.0 5t) + 42800.0
This gives the coordinates of the position. What of the velocity? Using 15.3 in the same
way to obtain the velocity,
râ€²
1 (t) = 880.0 exp (âˆ’.0 5t) ,
râ€²
2 (t) = âˆ’640.0 + 640.0 exp (âˆ’.0 5t) .
(15.4)

277
To determine the velocity when the blue ice hits the ground, it is necessary to ï¬nd the value
of t when this event takes place and then to use 15.4 to determine the velocity. It hits
ground when r2 (t) = 0. Thus it suï¬ƒces to solve the equation,
0 = âˆ’640.0t âˆ’12800.0 exp (âˆ’.0 5t) + 42800.0.
This is a fairly hard equation to solve using the methods of algebra. In fact, I do not have
a good way to ï¬nd this value of t using algebra. However if plugging in various values of t
using a calculator you eventually ï¬nd that when t = 66.14,
âˆ’640.0 (66.14) âˆ’12800.0 exp (âˆ’.0 5 (66.14)) + 42800.0 = 1.588 feet.
This is close enough to hitting the ground and so plugging in this value for t yields the
approximate velocity,
(880.0 exp (âˆ’.0 5 (66.14)) , âˆ’640.0 + 640.0 exp (âˆ’.0 5 (66.14))) = (32. 23, âˆ’616. 56) .
Notice how because of air resistance the component of velocity in the horizontal direction
is only about 32 feet per second even though this component started out at 880 feet per
second while the component in the vertical direction is -616 feet per second even though this
component started oï¬€at 0 feet per second. You see that air resistance can be very important
so it is not enough to pretend, as is often done in beginning physics courses that everything
takes place in a vacuum. Actually, this problem used several physical simpliï¬cations. It was
assumed the force acting on the lump of blue ice by gravity was constant. This is not really
true because it actually depends on the distance between the center of mass of the earth and
the center of mass of the lump. It was also assumed the air resistance is proportional to the
velocity. This is an over simpliï¬cation when high speeds are involved. However, increasingly
correct models can be studied in a systematic way as above.
15.0.6
Kinetic Energyâˆ—
Newtonâ€™s second law is also the basis for the notion of kinetic energy. When a force is
exerted on an object which causes the object to move, it follows that the force is doing work
which manifests itself in a change of velocity of the object. How is the total work done on
the object by the force related to the ï¬nal velocity of the object? By Newtonâ€™s second law,
and letting v be the velocity,
F (t) = mvâ€² (t) .
Now in a small increment of time, (t, t + dt) , the work done on the object would be approx-
imately equal to
dW = F (t) Â· v (t) dt.
(15.5)
If no work has been done at time t = 0,then 15.5 implies
dW
dt = F Â· v, W (0) = 0.
Hence,
dW
dt = mvâ€² (t) Â· v (t) = m
2
d
dt |v (t)|2 .
Therefore, the total work done up to time t would be W (t) = m
2 |v (t)|2 âˆ’m
2 |v0|2 where |v0|
denotes the initial speed of the object. This diï¬€erence represents the change in the kinetic
energy.

278
NEWTONâ€™S LAWS OF MOTIONâˆ—
15.0.7
Impulse And Momentumâˆ—
Implulse
Work and energy involve a force acting on an object for some distance. Impulse involves a
force which acts on an object for an interval of time.
Deï¬nition 15.0.21 Let F be a force which acts on an object during the time in-
terval, [a, b] . The impulse of this force is
Z b
a
F (t) dt.
This is deï¬ned as
ÃƒZ b
a
F1 (t) dt,
Z b
a
F2 (t) dt,
Z b
a
F3 (t) dt
!
.
The linear momentum of an object of mass m and velocity v is deï¬ned as
Linear momentum = mv.
The notion of impulse and momentum are related in the following theorem.
Theorem 15.0.22 Let F be a force acting on an object of mass m. Then the
impulse equals the change in momentum. More precisely,
Z b
a
F (t) dt = mv (b) âˆ’mv (a) .
Proof: This is really just the fundamental theorem of calculus and Newtonâ€™s second law
applied to the components of F.
Z b
a
F (t) dt =
Z b
a
mdv
dt dt = mv (b) âˆ’mv (a)
(15.6)
15.0.8
Conservation Of Momentumâˆ—
Now suppose two point masses, A and B collide. Newtonâ€™s third law says the force exerted
by mass A on mass B is equal in magnitude but opposite in direction to the force exerted by
mass B on mass A. Letting the collision take place in the time interval, [a, b] and denoting
the two masses by mA and mB and their velocities by vA and vB it follows that
mAvA (b) âˆ’mAvA (a) =
Z b
a
(Force of B on A) dt
and
mBvB (b) âˆ’mBvB (a)
=
Z b
a
(Force of A on B) dt
=
âˆ’
Z b
a
(Force of B on A) dt
=
âˆ’(mAvA (b) âˆ’mAvA (a))
and this shows
mBvB (b) + mAvA (b) = mBvB (a) + mAvA (a) .
In other words, in a collision between two masses the total linear momentum before the colli-
sion equals the total linear momentum after the collision. This is known as the conservation
of linear momentum. This law is why rockets work. Think about it.

279
15.0.9
Exercises With Answers
1. Show the solution to vâ€² + rv = c with the initial condition, v (0) = v0 is v (t) =
Â¡
v0 âˆ’c
r
Â¢
eâˆ’rt + (c/r) . If v is velocity and r = k/m where k is a constant for air
resistance and m is the mass, and c = f/m, argue from Newtonâ€™s second law that
this is the equation for ï¬nding the velocity, v of an object acted on by air resistance
proportional to the velocity and a constant force, f, possibly from gravity. Does there
exist a terminal velocity? What is it?
Multiply both sides of the diï¬€erential equation by ert. Then the left side becomes
d
dt (ertv) = ertc. Now integrate both sides. This gives ertv (t) = C + ert
r c. You ï¬nish
the rest.
2. Suppose an object having mass equal to 5 kilograms experiences a time dependent
force, F (t) = eâˆ’ti + cos (t) j + t2k meters per sec2. Suppose also that the object is
at the point (0, 1, 1) meters at time t = 0 and that its initial velocity at this time is
v = i + j âˆ’k meters per sec. Find the position of the object as a function of t.
This is done by using Newtonâ€™s law.
Thus 5 d2r
dt2 = eâˆ’ti + cos (t) j + t2k and so
5 dr
dt = âˆ’eâˆ’ti + sin (t) j +
Â¡
t3/3
Â¢
k + C. Find the constant, C by using the given initial
velocity. Next do another integration obtaining another constant vector which will be
determined by using the given initial position of the object.
3. Fill in the details for the derivation of kinetic energy. In particular verify that mvâ€² (t)Â·
v (t) = m
2
d
dt |v (t)|2. Also, why would dW = F (t) Â· v (t) dt?
Remember |v|2 = v Â· v. Now use the product rule.
4. Suppose the force acting on an object, F is always perpendicular to the velocity of
the object. Thus F Â· v = 0. Show the Kinetic energy of the object is constant. Such
forces are sometimes called forces of constraint because they do not contribute to the
speed of the object, only its direction.
0 = F Â· v = mvâ€² Â· v. Explain why this is
d
dt
Â³
m 1
2 |v|2Â´
, the derivative of the kinetic
energy.

280
NEWTONâ€™S LAWS OF MOTIONâˆ—

Physics Of Curvilinear Motion
12 Oct.
16.0.10
The Acceleration In Terms Of The Unit Tangent And Nor-
mal
A ï¬‚y buzzing around the room, a person riding a roller coaster, and a satellite orbiting the
earth all have something in common. They are moving over some sort of curve in three
dimensions.
Denote by R (t) the position vector of the point on the curve which occurs at time t.
Assume that Râ€², Râ€²â€² exist and is continuous. Thus Râ€² = v, the velocity and Râ€²â€² = a is the
acceleration.
Â¡
Â¡
Â¡
Â¡
s

1
R(t)
x
z
y
Lemma 16.0.23 Deï¬ne T (t) â‰¡Râ€² (t) / |Râ€² (t)| . Then |T (t)| = 1 and if Tâ€² (t) Ì¸= 0, then
there exists a unit vector, N (t) perpendicular to T (t) and a scalar valued function, Îº (t) ,
with Tâ€² (t) = Îº (t) |v| N (t) .
Proof: It follows from the deï¬nition that |T| = 1. Therefore, T Â· T = 1 and so, upon
diï¬€erentiating both sides,
Tâ€² Â· T + T Â· Tâ€² = 2Tâ€² Â· T = 0.
Therefore, Tâ€² is perpendicular to T. Let
N (t) â‰¡Tâ€²
|Tâ€²|.
Then letting |Tâ€²| â‰¡Îº (t) |v (t)| , it follows
Tâ€² (t) = Îº (t) |v (t)| N (t) .
This proves the lemma.
281

282
PHYSICS OF CURVILINEAR MOTION 12 OCT.
Deï¬nition 16.0.24 The vector, T (t) is called the unit tangent vector and the
vector, N (t) is called the principal normal.
The function, Îº (t) in the above lemma
is called the curvature.
The radius of curvature is deï¬ned as Ï = 1/Îº.
The plane
determined by the two vectors, T and N is called the osculating1 plane. It identiï¬es a
particular plane which is in a sense tangent to this space curve.
The important thing about this is that it is possible to write the acceleration as the sum
of two vectors, one perpendicular to the direction of motion and the other in the direction
of motion.
Theorem 16.0.25 For R (t) the position vector of a space curve, the acceleration
is given by the formula
a
=
d |v|
dt T + Îº |v|2 N
(16.1)
â‰¡
aT T + aNN.
Furthermore, a2
T + a2
N = |a|2.
Proof:
a
=
dv
dt = d
dt (Râ€²) = d
dt (|v| T)
=
d |v|
dt T + |v| Tâ€²
=
d |v|
dt T + |v|2 ÎºN.
This proves the ï¬rst part.
For the second part,
|a|2
=
(aT T + aNN) Â· (aT T + aNN)
=
a2
T T Â· T + 2aNaT T Â· N + a2
NN Â· N
=
a2
T + a2
N
because T Â· N = 0. This proves the theorem.
Finally, it is well to point out that the curvature is a property of the curve itself, and
does not depend on the parameterization of the curve. If the curve is given by two diï¬€erent
vector valued functions, R (t) and R (Ï„) , then from the formula above for the curvature,
Îº (t) = |Tâ€² (t)|
|v (t)| =
Â¯Â¯ dT
dÏ„
dÏ„
dt
Â¯Â¯
Â¯Â¯ dR
dÏ„
dÏ„
dt
Â¯Â¯ =
Â¯Â¯ dT
dÏ„
Â¯Â¯
Â¯Â¯ dR
dÏ„
Â¯Â¯ â‰¡Îº (Ï„) .
From this, it is possible to give an important formula from physics. Suppose an object
orbits a point at constant speed, v. In the above notation, |v| = v. What is the centripetal
acceleration of this object? You may know from a physics class that the answer is v2/r
where r is the radius. This follows from the above quite easily. The parameterization of the
object which is as described is
R (t) =
Â³
r cos
Â³v
r t
Â´
, r sin
Â³v
r t
Â´Â´
.
1To osculate means to kiss. Thus this plane could be called the kissing plane. However, that does not
sound formal enough so we call it the osculating plane.

283
Therefore, T =
Â¡
âˆ’sin
Â¡ v
r t
Â¢
, cos
Â¡ v
r t
Â¢Â¢
and Tâ€² =
Â¡
âˆ’v
r cos
Â¡ v
r t
Â¢
, âˆ’v
r sin
Â¡ v
r t
Â¢Â¢
. Thus,
Îº = |Tâ€² (t)| /v = 1
r .
I hope it is not surprising that the curvature of a circle of radius r is 1/r. It follows
a =dv
dt T + v2ÎºN =v2
r N.
The vector, N points from the object toward the center of the circle because it is a positive
multiple of the vector,
Â¡
âˆ’v
r cos
Â¡ v
r t
Â¢
, âˆ’v
r sin
Â¡ v
r t
Â¢Â¢
.
Formula 16.1 also yields an easy way to ï¬nd the curvature. Take the cross product of
both sides with v, the velocity. Then
a Ã— v
=
d |v|
dt T Ã— v + |v|2 ÎºN Ã— v
=
d |v|
dt T Ã— v + |v|3 ÎºN Ã— T
Now T and v have the same direction so the ï¬rst term on the right equals zero. Taking
the magnitude of both sides, and using the fact that N and T are two perpendicular unit
vectors,
|a Ã— v| = |v|3 Îº
and so
Îº = |a Ã— v|
|v|3
.
(16.2)
Example 16.0.26 Let R (t) =
Â¡
cos (t) , t, t2Â¢
for t âˆˆ[0, 3] . Find the speed, velocity, curva-
ture, and write the acceleration in terms of normal and tangential components.
First of all v (t) = (âˆ’sin t, 1, 2t) and so the speed is given by
|v| =
q
sin2 (t) + 1 + 4t2.
Therefore,
aT = d
dt
Âµq
sin2 (t) + 1 + 4t2
Â¶
=
sin (t) cos (t) + 4t
p
(2 + 4t2 âˆ’cos2 t)
.
It remains to ï¬nd aN. To do this, you can ï¬nd the curvature ï¬rst if you like.
a (t) = Râ€²â€² (t) = (âˆ’cos t, 0, 2) .
Then
Îº
=
|(âˆ’cos t, 0, 2) Ã— (âˆ’sin t, 1, 2t)|
Âµq
sin2 (t) + 1 + 4t2
Â¶3
=
q
4 + (âˆ’2 sin (t) + 2 (cos (t)) t)2 + cos2 (t)
Âµq
sin2 (t) + 1 + 4t2
Â¶3
Then
aN = Îº |v|2

284
PHYSICS OF CURVILINEAR MOTION 12 OCT.
=
q
4 + (âˆ’2 sin (t) + 2 (cos (t)) t)2 + cos2 (t)
Âµq
sin2 (t) + 1 + 4t2
Â¶3
Â¡
sin2 (t) + 1 + 4t2Â¢
=
q
4 + (âˆ’2 sin (t) + 2 (cos (t)) t)2 + cos2 (t)
q
sin2 (t) + 1 + 4t2
.
You can observe the formula a2
N + a2
T = |a|2 holds. Indeed a2
N + a2
T =
ï£«
ï£­
q
4 + (âˆ’2 sin (t) + 2 (cos (t)) t)2 + cos2 (t)
q
sin2 (t) + 1 + 4t2
ï£¶
ï£¸
2
+
Ãƒ
sin (t) cos (t) + 4t
p
(2 + 4t2 âˆ’cos2 t)
!2
=
4 + (âˆ’2 sin t + 2 (cos t) t)2 + cos2 t
sin2 t + 1 + 4t2
+ (sin t cos t + 4t)2
2 + 4t2 âˆ’cos2 t = cos2 t + 4 = |a|2
Some Simple Techniques
Recall the formula for acceleration is
a = aT T + aNN
(16.3)
where aT = d|v|
dt
and aN = Îº |v|2 . Of course one way to ï¬nd aT and aN is to just ï¬nd
|v| , d|v|
dt and Îº and plug in. However, there is another way which might be easier. Take the
dot product of both sides with T. This gives,
a Â· T = aT T Â· T + aNN Â· T = aT .
Thus
a = (a Â· T) T + aNN
and so
a âˆ’(a Â· T) T = aNN
(16.4)
and taking norms of both sides,
|a âˆ’(a Â· T) T| = aN.
Also from 16.4,
a âˆ’(a Â· T) T
|a âˆ’(a Â· T) T| = aNN
aN |N| = N.
Also recall
Îº = |a Ã— v|
|v|3
, a2
T + a2
N = |a|2
This is usually easier than computing Tâ€²/ |Tâ€²| . To illustrate the use of these simple obser-
vations, consider the example worked above which was fairly messy. I will make it easier by
selecting a value of t.
Example 16.0.27 Let R (t) =
Â¡
cos (t) , t, t2Â¢
for t âˆˆ[0, 3] . Find the speed, velocity, curva-
ture, and write the acceleration in terms of normal and tangential components when t = 0.
Also ï¬nd N at the point where t = 0.

285
First I need to ï¬nd the velocity and acceleration. Thus
v = (âˆ’sin t, 1, 2t) , a = (âˆ’cos t, 0, 2)
and consequently,
T =
(âˆ’sin t, 1, 2t)
q
sin2 (t) + 1 + 4t2
.
When t = 0, this reduces to
v (0) = (0, 1, 0) , a = (âˆ’1, 0, 2) , |v (0)| = 1, T = (0, 1, 0) ,
and consequently,
T = (0, 1, 0) .
Then the tangential component of acceleration when t = 0 is
aT = (âˆ’1, 0, 2) Â· (0, 1, 0) = 0
Now |a|2 = 5 and so aN =
âˆš
5 because a2
T + a2
N = |a|2. Thus
âˆš
5 = Îº |v (0)|2 = Îº Â· 1 = Îº.
Next lets ï¬nd N. From a = aT T + aNN it follows
(âˆ’1, 0, 2) = 0 Â· T +
âˆš
5N
and so
N =
1
âˆš
5 (âˆ’1, 0, 2) .
This was pretty easy.
Example 16.0.28 Find a formula for the curvature of the curve given by the graph of
y = f (x) for x âˆˆ[a, b] . Assume whatever you like about smoothness of f.
You need to write this as a parametric curve. This is most easily accomplished by letting
t = x. Thus a parameterization is
(t, f (t) , 0) : t âˆˆ[a, b] .
Then you can use the formula given above. The acceleration is (0, f â€²â€² (t) , 0) and the velocity
is (1, f â€² (t) , 0) . Therefore,
a Ã— v = (0, f â€²â€² (t) , 0) Ã— (1, f â€² (t) , 0) = (0, 0, âˆ’f â€²â€² (t)) .
Therefore, the curvature is given by
|a Ã— v|
|v|3
=
|f â€²â€² (t)|
Â³
1 + f â€² (t)2Â´3/2 .
Sometimes curves donâ€™t come to you parametrically. This is unfortunate when it occurs
but you can sometimes ï¬nd a parametric description of such curves. It should be emphasized
that it is only sometimes when you can actually ï¬nd a parameterization. General systems
of nonlinear equations cannot be solved using algebra.
Example 16.0.29 Find a parameterization for the intersection of the surfaces y + 3z =
2x2 + 4 and y + 2z = x + 1.

286
PHYSICS OF CURVILINEAR MOTION 12 OCT.
You need to solve for x and y in terms of x. This yields
z = 2x2 âˆ’x + 3, y = âˆ’4x2 + 3x âˆ’5.
Therefore, letting t = x, the parameterization is (x, y, z) =
Â¡
t, âˆ’4t2 âˆ’5 + 3t, âˆ’t + 3 + 2t2Â¢
.
Example 16.0.30 Find a parameterization for the straight line joining (3, 2, 4) and (1, 10, 5) .
(x, y, z) = (3, 2, 4) + t (âˆ’2, 8, 1) = (3 âˆ’2t, 2 + 8t, 4 + t) where t âˆˆ[0, 1] . Note where this
came from. The vector, (âˆ’2, 8, 1) is obtained from (1, 10, 5) âˆ’(3, 2, 4) . Now you should
check to see this works.
16.0.11
The Curvature Vector
The main item of interest for us is the scalar curvature deï¬ned above. Recall this was given
by
Îº = |Tâ€²|
|v| .
The curvature vector is nothing more than
Îº â‰¡Tâ€²
|v|.
16.0.12
The Circle Of Curvature*
In addition to the osculating plane, you can consider something called the circle of curvature.
The idea is that near a point on the space curve, the space curve is like a circle. This circle
has radius equal to 1/Îº, the radius of curvature, lies in the osculating plane, and its center
is located by moving a distance of 1/Îº (radiius of curvature) along the line determined by
the point on the curve and the principle normal in the direction of the principle normal. It
is an attempt to ï¬nd the circle which best resembles the curve locally.
T
-
N
?
t
t
Here is an example to illustrate this fussy concept.
Example 16.0.31 Consider the curve having a parameterization, (cos (t) , sin (t) , et). Find
the circle of curvature at the point where t = Ï€/4.

287
First ï¬nd the curvature and the two vectors, T, N. The vector, T (t) =(âˆ’sin t,cos t,et)
âˆš
1+e2t
. At
the point of interest this is
T =
Â³
âˆ’
âˆš
2
2 ,
âˆš
2
2 , eÏ€/4Â´
âˆš
1 + eÏ€/2
To ï¬nd N next appears to be painful. Therefore, I will ï¬rst ï¬nd the acceleration and then
use the formula for acceleration to dredge up N.
a =
Â¡
âˆ’cos t, âˆ’sin t, etÂ¢
Thus the curvature is easy to ï¬nd.
Îº
=
|(âˆ’cos t, âˆ’sin t, et) Ã— (âˆ’sin t, cos t, et)|
Â¡âˆš
1 + e2tÂ¢3
=
Â¯Â¯Â¡
âˆ’(sin t) et âˆ’et cos t, âˆ’(sin t) et + et cos t, âˆ’cos2 t âˆ’sin2 t
Â¢Â¯Â¯
Â¡âˆš
1 + e2tÂ¢3
=
Â¡
2e2t + 1
Â¢1/2
Â¡âˆš
1 + e2tÂ¢3
It follows that at the point of interest,
Ãƒ
âˆ’
âˆš
2
2 , âˆ’
âˆš
2
2 , eÏ€/4
!
=
Ãƒ
âˆ’
âˆš
2
2 ,
âˆš
2
2 , eÏ€/4
!
+
Â¡
2eÏ€/2 + 1
Â¢1/2
Â³âˆš
1 + eÏ€/2
Â´3
Â³
1 + eÏ€/2Â´
N
From this, you can ï¬nd the components of N without too much trouble.
âˆ’
âˆš
2
2
= âˆ’
âˆš
2
2 +
Â¡
2eÏ€/2 + 1
Â¢1/2
Â³âˆš
1 + eÏ€/2
Â´3
Â³
1 + eÏ€/2Â´
N1
and so N1 = 0. Next,
âˆ’
âˆš
2
2
=
âˆš
2
2 +
Â¡
2eÏ€/2 + 1
Â¢1/2
Â³âˆš
1 + eÏ€/2
Â´3
Â³
1 + eÏ€/2Â´
N2
and so
N2 = âˆ’
âˆš
2
qÂ¡
1 + eÏ€/2Â¢
qÂ¡
2eÏ€/2 + 1
Â¢.
Finally,
eÏ€/4 = eÏ€/4 +
Â¡
2eÏ€/2 + 1
Â¢1/2
Â³âˆš
1 + eÏ€/2
Â´3
Â³
1 + eÏ€/2Â´
N3
and so N3 = 0 also.
From this you can ï¬nd the location of the center of curvature. It is at the point
Ãƒâˆš
2
2 ,
âˆš
2
2 , eÏ€/4
!
+
ï£«
ï£¬
ï£­
Â¡
2eÏ€/2 + 1
Â¢1/2
Â³âˆš
1 + eÏ€/2
Â´3
ï£¶
ï£·
ï£¸
âˆ’1 ï£«
ï£­0, âˆ’
âˆš
2
qÂ¡
1 + eÏ€/2Â¢
qÂ¡
2eÏ€/2 + 1
Â¢, 0
ï£¶
ï£¸.

288
PHYSICS OF CURVILINEAR MOTION 12 OCT.
Simplifying this yields
Ãƒ
1
2
âˆš
2, âˆ’1
2
âˆš
22e
1
2 Ï€ + 1 + 2eÏ€
2e
1
2 Ï€ + 1
, e
1
4 Ï€
!
for the center of curvature. The circle of curvature is the circle in the osculating plane which
has the above point as the center and radius equal to
ï£«
ï£¬
ï£­
Â¡
2eÏ€/2 + 1
Â¢1/2
Â³âˆš
1 + eÏ€/2
Â´3
ï£¶
ï£·
ï£¸
âˆ’1
=
1
rÂ³
2e
1
2 Ï€ + 1
Â´
ÃƒrÂ³
1 + e
1
2 Ï€Â´!3
.
If you try the same problem for (cos (t) , sin (t) , t) , you may ï¬nd the computations much
simpler.
One can of course go on and on fussing about geometrical dodads of this sort. The
evolute is the locus of centers of curvatures. Imagine ï¬nding such a center of curvature as
above for each t and considering the resulting curve. This is the evolute. Then if you really
like to do this sort of thing, you could think about the evolute of the evolute. There are
sure to be some wonderful conclusions hidden in this procedure.
The signiï¬cant geometrical concepts are discussed in Section 16.1. These lead to the
Serrat Frenet formulas which cause some people who like this sort of thing to wax ecstatic
over their virtues. These formulas are indeed interesting, unlike the fussy stuï¬€above about
the circle of curvature. However, it is not required reading so skip it if you are not interested.
It is a system of diï¬€erential equations which completely describes the geometry of the space
curve.
16.1
Geometry Of Space Curvesâˆ—
If you are interested in more on space curves, you should read this section. Otherwise,
procede to the exercises. Denote by R (s) the function which takes s to a point on this
curve where s is arc length. Thus R (s) equals the point on the curve which occurs when
you have traveled a distance of s along the curve from one end.
This is known as the
parameterization of the curve in terms of arc length.
Note also that it incorporates an
orientation on the curve because there are exactly two ends you could begin measuring
length from. In this section, assume anything about smoothness and continuity to make the
following manipulations valid. In particular, assume that Râ€² exists and is continuous.
Lemma 16.1.1 Deï¬ne T (s) â‰¡Râ€² (s) . Then |T (s)| = 1 and if Tâ€² (s) Ì¸= 0, then there
exists a unit vector, N (s) perpendicular to T (s) and a scalar valued function, Îº (s) with
Tâ€² (s) = Îº (s) N (s) .
Proof: First, s =
R s
0 |Râ€² (r)| dr because of the deï¬nition of arc length.
Therefore,
from the fundamental theorem of calculus, 1 = |Râ€² (s)| = |T (s)| . Therefore, T Â· T = 1
and so upon diï¬€erentiating this on both sides, yields Tâ€² Â· T + T Â· Tâ€² = 0 which shows
T Â· Tâ€² = 0. Therefore, the vector, Tâ€² is perpendicular to the vector, T. In case Tâ€² (s) Ì¸= 0,
let N (s) =
Tâ€²(s)
|Tâ€²(s)| and so Tâ€² (s) = |Tâ€² (s)| N (s) , showing the scalar valued function is
Îº (s) = |Tâ€² (s)| . This proves the lemma.
The radius of curvature is deï¬ned as Ï =
1
Îº. Thus at points where there is a lot of
curvature, the radius of curvature is small and at points where the curvature is small, the
radius of curvature is large. The plane determined by the two vectors, T and N is called
the osculating plane. It identiï¬es a particular plane which is in a sense tangent to this space

16.1.
GEOMETRY OF SPACE CURVESâˆ—
289
curve. In the case where |Tâ€² (s)| = 0 near the point of interest, T (s) equals a constant and
so the space curve is a straight line which it would be supposed has no curvature. Also, the
principal normal is undeï¬ned in this case. This makes sense because if there is no curving
going on, there is no special direction normal to the curve at such points which could be
distinguished from any other direction normal to the curve. In the case where |Tâ€² (s)| = 0,
Îº (s) = 0 and the radius of curvature would be considered inï¬nite.
Deï¬nition 16.1.2 The vector, T (s) is called the unit tangent vector and the vector,
N (s) is called the principal normal. The function, Îº (s) in the above lemma is called
the curvature.When Tâ€² (s) Ì¸= 0 so the principal normal is deï¬ned, the vector, B (s) â‰¡
T (s) Ã— N (s) is called the binormal.
The binormal is normal to the osculating plane and Bâ€² tells how fast this vector changes.
Thus it measures the rate at which the curve twists.
Lemma 16.1.3 Let R (s) be a parameterization of a space curve with respect to arc
length and let the vectors, T, N, and B be as deï¬ned above. Then Bâ€² = T Ã— Nâ€² and there
exists a scalar function, Ï„ (s) such that Bâ€² = Ï„N.
Proof: From the deï¬nition of B = T Ã— N, and you can diï¬€erentiate both sides and get
Bâ€² = Tâ€² Ã— N + T Ã— Nâ€². Now recall that Tâ€² is a multiple called curvature multiplied by N
so the vectors, Tâ€² and N have the same direction and Bâ€² = T Ã— Nâ€². Therefore, Bâ€² is either
zero or is perpendicular to T. But also, from the deï¬nition of B, B is a unit vector and so
B (s) Â· B (s) = 0. Diï¬€erentiating this,Bâ€² (s) Â· B (s) + B (s) Â· Bâ€² (s) = 0 showing that Bâ€² is
perpendicular to B also. Therefore, Bâ€² is a vector which is perpendicular to both vectors,
T and B and since this is in three dimensions, Bâ€² must be some scalar multiple of N and it
is this multiple called Ï„. Thus Bâ€² = Ï„N as claimed.
Lets go over this last claim a little more. The following situation is obtained. There
are two vectors, T and B which are perpendicular to each other and both Bâ€² and N are
perpendicular to these two vectors, hence perpendicular to the plane determined by them.
Therefore, Bâ€² must be a multiple of N. Take a piece of paper, draw two unit vectors on it
which are perpendicular. Then you can see that any two vectors which are perpendicular
to this plane must be multiples of each other.
The scalar function, Ï„ is called the torsion.
In case Tâ€² = 0, none of this is deï¬ned
because in this case there is not a well deï¬ned osculating plane. The conclusion of the
following theorem is called the Serret Frenet formulas.
Theorem 16.1.4 (Serret Frenet) Let R (s) be the parameterization with respect to
arc length of a space curve and T (s) = Râ€² (s) is the unit tangent vector. Suppose |Tâ€² (s)| Ì¸= 0
so the principal normal, N (s) =
Tâ€²(s)
|Tâ€²(s)| is deï¬ned. The binormal is the vector B â‰¡T Ã— N
so T, N, B forms a right handed system of unit vectors each of which is perpendicular to
every other. Then the following system of diï¬€erential equations holds in R9.
Bâ€² = Ï„N, Tâ€² = ÎºN, Nâ€² = âˆ’ÎºT âˆ’Ï„B
where Îº is the curvature and is nonnegative and Ï„ is the torsion.
Proof:
Îº â‰¥0 because Îº = |Tâ€² (s)| . The ï¬rst two equations are already established. To
get the third, note that B Ã— T = N which follows because T, N, B is given to form a right
handed system of unit vectors each perpendicular to the others. (Use your right hand.)
Now take the derivative of this expression. thus
Nâ€²
=
Bâ€² Ã— T + B Ã— Tâ€²
=
Ï„N Ã— T+ÎºB Ã— N.

290
PHYSICS OF CURVILINEAR MOTION 12 OCT.
Now recall again that T, N, B is a right hand system. Thus N Ã— T = âˆ’B and B Ã— N = âˆ’T.
This establishes the Frenet Serret formulas.
This is an important example of a system of diï¬€erential equations in R9. It is a remarkable
result because it says that from knowledge of the two scalar functions, Ï„ and Îº, and initial
values for B, T, and N when s = 0 you can obtain the binormal, unit tangent, and principal
normal vectors.
It is just the solution of an initial value problem of the sort discussed
earlier. Having done this, you can reconstruct the entire space curve starting at some point,
R0 because Râ€² (s) = T (s) and so R (s) = R0 +
R s
0 Tâ€² (r) dr.
The vectors, B, T, and N are vectors which are functions of position on the space curve.
Often, especially in applications, you deal with a space curve which is parameterized by a
function of t where t is time. Thus a value of t would correspond to a point on this curve and
you could let B (t) , T (t) , and N (t) be the binormal, unit tangent, and principal normal at
this point of the curve. The following example is typical.
Example 16.1.5 Given the circular helix, R (t) = (a cos t) i + (a sin t) j + (bt) k, ï¬nd the
arc length, s (t) ,the unit tangent vector, T (t) , the principal normal, N (t) , the binormal,
B (t) , the curvature, Îº (t) , and the torsion, Ï„ (t) . Here t âˆˆ[0, T] .
The arc length is s (t) =
R t
0
Â¡âˆš
a2 + b2Â¢
dr =
Â¡âˆš
a2 + b2Â¢
t. Now the tangent vector is
obtained using the chain rule as
T
=
dR
ds = dR
dt
dt
ds =
1
âˆš
a2 + b2 Râ€² (t)
=
1
âˆš
a2 + b2 ((âˆ’a sin t) i + (a cos t) j + bk)
The principal normal:
dT
ds
=
dT
dt
dt
ds
=
1
a2 + b2 ((âˆ’a cos t) i + (âˆ’a sin t) j + 0k)
and so
N =dT
ds /
Â¯Â¯Â¯Â¯
dT
ds
Â¯Â¯Â¯Â¯ = âˆ’((cos t) i + (sin t) j)
The binormal:
B
=
1
âˆš
a2 + b2
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
âˆ’a sin t
a cos t
b
âˆ’cos t
âˆ’sin t
0
Â¯Â¯Â¯Â¯Â¯Â¯
=
1
âˆš
a2 + b2 ((b sin t) iâˆ’b cos tj + ak)
Now the curvature, Îº (t) =
Â¯Â¯ dT
ds
Â¯Â¯ =
rÂ³
a cos t
a2+b2
Â´2
+
Â³
a sin t
a2+b2
Â´2
=
a
a2+b2 . Note the curvature
is constant in this example. The ï¬nal task is to ï¬nd the torsion. Recall that Bâ€² = Ï„N where
the derivative on B is taken with respect to arc length. Therefore, remembering that t is a
function of s,
Bâ€² (s)
=
1
âˆš
a2 + b2 ((b cos t) i+ (b sin t) j) dt
ds
=
1
a2 + b2 ((b cos t) i+ (b sin t) j)
=
Ï„ (âˆ’(cos t) i âˆ’(sin t) j) = Ï„N

16.2.
INDEPENDENCE OF PARAMETERIZATIONâˆ—
291
and it follows âˆ’b/
Â¡
a2 + b2Â¢
= Ï„.
An important application of the usefulness of these ideas involves the decomposition of
the acceleration in terms of these vectors of an object moving over a space curve.
Corollary 16.1.6 Let R (t) be a space curve and denote by v (t) the velocity, v (t) =
Râ€² (t) and let v (t) â‰¡|v (t)| denote the speed and let a (t) denote the acceleration. Then
v = vT and a = dv
dt T + Îºv2N.
Proof:
T = dR
ds = dR
dt
dt
ds = v dt
ds. Also, s =
R t
0 v (r) dr and so ds
dt = v which implies
dt
ds = 1
v. Therefore, T = v/v which implies v = vT as claimed.
Now the acceleration is just the derivative of the velocity and so by the Serrat Frenet
formulas,
a
=
dv
dt T + v dT
dt
=
dv
dt T + v dT
ds v = dv
dt T + v2ÎºN
Note how this decomposes the acceleration into a component tangent to the curve and one
which is normal to it. Also note that from the above, v |Tâ€²| Tâ€²(t)
|Tâ€²| = v2ÎºN and so |Tâ€²|
v
= Îº
and N = Tâ€²(t)
|Tâ€²|
16.2
Independence Of Parameterizationâˆ—
This section is for those who want to really understand what is going on. If you are
content, do not read this section. It may upset you. However, if you do decide to read it,
you might learn something so there is some beneï¬t for the anguish you might endure in the
attempt.
Recall that if p (t) : t âˆˆ[a, b] was a parameterization of a smooth curve, C, the length of
C is deï¬ned as
Z b
a
|pâ€² (t)| dt
If some other parameterization were used to trace out C, would the same answer be obtained?
The answer is yes. This is indeed fortunate because the length of a curve should only depend
on the curve itself, not on some parameterization. To answer this question in a satisfactory
manner requires some hard calculus. To answer it even more satisfactorily, you need to
consider some very advanced mathematics involving something called Hausdorï¬€measure.

292
PHYSICS OF CURVILINEAR MOTION 12 OCT.
16.2.1
Hard Calculusâˆ—
Deï¬nition 16.2.1 A sequence {an}âˆ
n=1 converges to a,
lim
nâ†’âˆan = a or an â†’a
if and only if for every Îµ > 0 there exists nÎµ such that whenever n â‰¥nÎµ ,
|an âˆ’a| < Îµ.
In words the deï¬nition says that given any measure of closeness, Îµ, the terms of the
sequence are eventually all this close to a. Note the similarity with the concept of limit.
Here, the word â€œeventuallyâ€ refers to n being suï¬ƒciently large. The limit of a sequence, if
it exists, is unique.
Theorem 16.2.2 If limnâ†’âˆan = a and limnâ†’âˆan = a1 then a1 = a.
Proof: Suppose a1 Ì¸= a. Then let 0 < Îµ < |a1 âˆ’a| /2 in the deï¬nition of the limit. It
follows there exists nÎµ such that if n â‰¥nÎµ, then |an âˆ’a| < Îµ and |an âˆ’a1| < Îµ. Therefore,
for such n,
|a1 âˆ’a|
â‰¤
|a1 âˆ’an| + |an âˆ’a|
<
Îµ + Îµ < |a1 âˆ’a| /2 + |a1 âˆ’a| /2 = |a1 âˆ’a| ,
a contradiction.
Deï¬nition 16.2.3 Let {an} be a sequence and let n1 < n2 < n3, Â· Â· Â· be any strictly
increasing list of integers such that n1 is at least as large as the ï¬rst index used to deï¬ne
the sequence {an} . Then if bk â‰¡ank, {bk} is called a subsequence of {an} .
Theorem 16.2.4 Let {xn} be a sequence with limnâ†’âˆxn = x and let {xnk} be a
subsequence. Then limkâ†’âˆxnk = x.
Proof: Let Îµ > 0 be given. Then there exists nÎµ such that if n > nÎµ, then |xn âˆ’x| < Îµ.
Suppose k > nÎµ. Then nk â‰¥k > nÎµ and so
|xnk âˆ’x| < Îµ
showing limkâ†’âˆxnk = x as claimed.
There is a very useful way of thinking of continuity in terms of limits of sequences found
in the following theorem. In words, it says a function is continuous if it takes convergent
sequences to convergent sequences whenever possible.
Theorem 16.2.5 A function f : D (f) â†’R is continuous at x âˆˆD (f) if and only
if, whenever xn â†’x with xn âˆˆD (f) , it follows f (xn) â†’f (x) .
Proof: Suppose ï¬rst that f is continuous at x and let xn â†’x. Let Îµ > 0 be given. By
continuity, there exists Î´ > 0 such that if |y âˆ’x| < Î´, then |f (x) âˆ’f (y)| < Îµ. However,
there exists nÎ´ such that if n â‰¥nÎ´, then |xn âˆ’x| < Î´ and so for all n this large,
|f (x) âˆ’f (xn)| < Îµ
which shows f (xn) â†’f (x) .
Now suppose the condition about taking convergent sequences to convergent sequences
holds at x. Suppose f fails to be continuous at x. Then there exists Îµ > 0 and xn âˆˆD (f)
such that |x âˆ’xn| < 1
n , yet
|f (x) âˆ’f (xn)| â‰¥Îµ.
But this is clearly a contradiction because, although xn â†’x, f (xn) fails to converge to
f (x) . It follows f must be continuous after all. This proves the theorem.

16.2.
INDEPENDENCE OF PARAMETERIZATIONâˆ—
293
Deï¬nition 16.2.6 A set, K âŠ†R is sequentially compact if whenever {an} âŠ†K is
a sequence, there exists a subsequence, {ank} such that this subsequence converges to a point
of K.
The following theorem is part of a major advanced calculus theorem known as the Heine
Borel theorem.
Theorem 16.2.7 Every closed interval, [a, b] is sequentially compact.
Proof: Let {xn} âŠ†[a, b] â‰¡I0. Consider the two intervals
Â£
a, a+b
2
Â¤
and
Â£ a+b
2 , b
Â¤
each of
which has length (b âˆ’a) /2. At least one of these intervals contains xn for inï¬nitely many
values of n. Call this interval I1. Now do for I1 what was done for I0. Split it in half and
let I2 be the interval which contains xn for inï¬nitely many values of n. Continue this way
obtaining a sequence of nested intervals I0 âŠ‡I1 âŠ‡I2 âŠ‡I3 Â· Â·Â· where the length of In is
(b âˆ’a) /2n. Now pick n1 such that xn1 âˆˆI1, n2 such that n2 > n1 and xn2 âˆˆI2, n3 such
that n3 > n2 and xn3 âˆˆI3, etc.
(This can be done because in each case the intervals
contained xn for inï¬nitely many values of n.) By the nested interval lemma there exists a
point, c contained in all these intervals. Furthermore,
|xnk âˆ’c| < (b âˆ’a) 2âˆ’k
and so limkâ†’âˆxnk = c âˆˆ[a, b] . This proves the theorem.
Lemma 16.2.8 Let Ï† : [a, b] â†’R be a continuous function and suppose Ï† is 1 âˆ’1 on
(a, b). Then Ï† is either strictly increasing or strictly decreasing on [a, b] . Furthermore, Ï†âˆ’1
is continuous.
Proof: First it is shown that Ï† is either strictly increasing or strictly decreasing on
(a, b) .
If Ï† is not strictly decreasing on (a, b), then there exists x1 < y1, x1, y1 âˆˆ(a, b) such that
(Ï† (y1) âˆ’Ï† (x1)) (y1 âˆ’x1) > 0.
If for some other pair of points, x2 < y2 with x2, y2 âˆˆ(a, b) , the above inequality does not
hold, then since Ï† is 1 âˆ’1,
(Ï† (y2) âˆ’Ï† (x2)) (y2 âˆ’x2) < 0.
Let xt â‰¡tx1 + (1 âˆ’t) x2 and yt â‰¡ty1 + (1 âˆ’t) y2. Then xt < yt for all t âˆˆ[0, 1] because
tx1 â‰¤ty1 and (1 âˆ’t) x2 â‰¤(1 âˆ’t) y2
with strict inequality holding for at least one of these inequalities since not both t and (1 âˆ’t)
can equal zero. Now deï¬ne
h (t) â‰¡(Ï† (yt) âˆ’Ï† (xt)) (yt âˆ’xt) .
Since h is continuous and h (0) < 0, while h (1) > 0, there exists t âˆˆ(0, 1) such that
h (t) = 0. Therefore, both xt and yt are points of (a, b) and Ï† (yt) âˆ’Ï† (xt) = 0 contradicting
the assumption that Ï† is one to one. It follows Ï† is either strictly increasing or strictly
decreasing on (a, b) .
This property of being either strictly increasing or strictly decreasing on (a, b) carries
over to [a, b] by the continuity of Ï†. Suppose Ï† is strictly increasing on (a, b) , a similar

294
PHYSICS OF CURVILINEAR MOTION 12 OCT.
argument holding for Ï† strictly decreasing on (a, b) . If x > a, then pick y âˆˆ(a, x) and from
the above, Ï† (y) < Ï† (x) . Now by continuity of Ï† at a,
Ï† (a) = lim
xâ†’a+ Ï† (z) â‰¤Ï† (y) < Ï† (x) .
Therefore, Ï† (a) < Ï† (x) whenever x âˆˆ(a, b) . Similarly Ï† (b) > Ï† (x) for all x âˆˆ(a, b).
It only remains to verify Ï†âˆ’1 is continuous. Suppose then that sn â†’s where sn and s are
points of Ï† ([a, b]) . It is desired to verify that Ï†âˆ’1 (sn) â†’Ï†âˆ’1 (s) . If this does not happen,
there exists Îµ > 0 and a subsequence, still denoted by sn such that
Â¯Â¯Ï†âˆ’1 (sn) âˆ’Ï†âˆ’1 (s)
Â¯Â¯ â‰¥Îµ.
Using the sequential compactness of [a, b] there exists a further subsequence, still denoted
by n, such that Ï†âˆ’1 (sn) â†’t1 âˆˆ[a, b] , t1 Ì¸= Ï†âˆ’1 (s) . Then by continuity of Ï†, it follows
sn â†’Ï† (t1) and so s = Ï† (t1) . Therefore, t1 = Ï†âˆ’1 (s) after all. This proves the lemma.
Corollary 16.2.9 Let f : (a, b) â†’R be one to one and continuous. Then f (a, b) is an
open interval, (c, d) and f âˆ’1 : (c, d) â†’(a, b) is continuous.
Proof: Since f is either strictly increasing or strictly decreasing, it follows that f (a, b) is
an open interval, (c, d) . Assume f is decreasing. Now let x âˆˆ(a, b). Why is f âˆ’1 is continuous
at f (x)? Since f is decreasing, if f (x) < f (y) , then y â‰¡f âˆ’1 (f (y)) < x â‰¡f âˆ’1 (f (x)) and
so f âˆ’1 is also decreasing. Let Îµ > 0 be given. Let Îµ > Î· > 0 and (x âˆ’Î·, x + Î·) âŠ†(a, b) .
Then f (x) âˆˆ(f (x + Î·) , f (x âˆ’Î·)) . Let Î´ = min (f (x) âˆ’f (x + Î·) , f (x âˆ’Î·) âˆ’f (x)) . Then
if
|f (z) âˆ’f (x)| < Î´,
it follows
z â‰¡f âˆ’1 (f (z)) âˆˆ(x âˆ’Î·, x + Î·) âŠ†(x âˆ’Îµ, x + Îµ)
so
Â¯Â¯f âˆ’1 (f (z)) âˆ’x
Â¯Â¯ =
Â¯Â¯f âˆ’1 (f (z)) âˆ’f âˆ’1 (f (x))
Â¯Â¯ < Îµ.
This proves the theorem in the case where f is strictly decreasing. The case where f is
increasing is similar.
Theorem 16.2.10 Let f : [a, b] â†’R be continuous and one to one.
Suppose
f â€² (x1) exists for some x1 âˆˆ[a, b] and f â€² (x1) Ì¸= 0. Then
Â¡
f âˆ’1Â¢â€² (f (x1)) exists and is given
by the formula,
Â¡
f âˆ’1Â¢â€² (f (x1)) =
1
f â€²(x1).
Proof: By Lemma 16.2.8 f is either strictly increasing or strictly decreasing and f âˆ’1 is
continuous on [a, b]. Therefore there exists Î· > 0 such that if 0 < |f (x1) âˆ’f (x)| < Î·, then
0 < |x1 âˆ’x| =
Â¯Â¯f âˆ’1 (f (x1)) âˆ’f âˆ’1 (f (x))
Â¯Â¯ < Î´
where Î´ is small enough that for 0 < |x1 âˆ’x| < Î´,
Â¯Â¯Â¯Â¯
x âˆ’x1
f (x) âˆ’f (x1) âˆ’
1
f â€² (x1)
Â¯Â¯Â¯Â¯ < Îµ.
It follows that if 0 < |f (x1) âˆ’f (x)| < Î·,
Â¯Â¯Â¯Â¯
f âˆ’1 (f (x)) âˆ’f âˆ’1 (f (x1))
f (x) âˆ’f (x1)
âˆ’
1
f â€² (x1)
Â¯Â¯Â¯Â¯ =
Â¯Â¯Â¯Â¯
x âˆ’x1
f (x) âˆ’f (x1) âˆ’
1
f â€² (x1)
Â¯Â¯Â¯Â¯ < Îµ
Therefore, since Îµ > 0 is arbitrary,
lim
yâ†’f(x1)
f âˆ’1 (y) âˆ’f âˆ’1 (f (x1))
y âˆ’f (x1)
=
1
f â€² (x1)
and this proves the theorem.
The following obvious corollary comes from the above by not bothering with end points.

16.2.
INDEPENDENCE OF PARAMETERIZATIONâˆ—
295
Corollary 16.2.11 Let f : (a, b) â†’R be continuous and one to one. Suppose f â€² (x1)
exists for some x1 âˆˆ(a, b) and f â€² (x1) Ì¸= 0. Then
Â¡
f âˆ’1Â¢â€² (f (x1)) exists and is given by the
formula,
Â¡
f âˆ’1Â¢â€² (f (x1)) =
1
f â€²(x1).
This is one of those theorems which is very easy to remember if you neglect the diï¬ƒcult
questions and simply focus on formal manipulations. Consider the following.
f âˆ’1 (f (x)) = x.
Now use the chain rule on both sides to write
Â¡
f âˆ’1Â¢â€² (f (x)) f â€² (x) = 1,
and then divide both sides by f â€² (x) to obtain
Â¡
f âˆ’1Â¢â€² (f (x)) =
1
f â€² (x).
Of course this gives the conclusion of the above theorem rather eï¬€ortlessly and it is formal
manipulations like this which aid in remembering formulas such as the one given in the
theorem.
16.2.2
Independence Of Parameterizationâˆ—
Here is the precise deï¬nition of what is meant by a smooth curve.
Deï¬nition 16.2.12 C is a smooth curve in Rn if there exists an interval, [a, b] âŠ†
R and functions xi : [a, b] â†’R such that the following conditions hold
1. xi is continuous on [a, b] .
2. xâ€²
i exists and is continuous and bounded on [a, b] , with xâ€²
i (a) deï¬ned as the derivative
from the right,
lim
hâ†’0+
xi (a + h) âˆ’xi (a)
h
,
and xâ€²
i (b) deï¬ned similarly as the derivative from the left.
3. For p (t) â‰¡(x1 (t) , Â· Â· Â·, xn (t)) , t â†’p (t) is one to one on (a, b) .
4. |pâ€² (t)| â‰¡
Â³Pn
i=1 |xâ€²
i (t)|2Â´1/2
Ì¸= 0 for all t âˆˆ[a, b] .
5. C = âˆª{(x1 (t) , Â· Â· Â·, xn (t)) : t âˆˆ[a, b]} .
The functions, xi (t) , deï¬ned above are giving the coordinates of a point in Rn and the
list of these functions is called a parameterization for the smooth curve. Note the natural
direction of the interval also gives a direction for moving along the curve. Such a direction
is called an orientation. The integral is used to deï¬ne what is meant by the length of such
a smooth curve. Consider such a smooth curve having parameterization (x1, Â· Â· Â·, xn) .
Theorem 16.2.13 Let Ï† : [a, b] â†’[c, d] be one to one and suppose Ï†â€² exists and is
continuous on [a, b] . Then if f is a continuous function deï¬ned on [a, b] which is Riemann
integrable2,
Z d
c
f (s) ds =
Z b
a
f (Ï† (t))
Â¯Â¯Ï†â€² (t)
Â¯Â¯ dt
2Recall that all continuous functions of this sort are Riemann integrable.

296
PHYSICS OF CURVILINEAR MOTION 12 OCT.
Proof: Let F â€² (s) = f (s) . (For example, let F (s) =
R s
a f (r) dr.) Then the ï¬rst integral
equals F (d) âˆ’F (c) by the fundamental theorem of calculus. By Lemma 16.2.8, Ï† is either
strictly increasing or strictly decreasing. Suppose Ï† is strictly decreasing. Then Ï† (a) = d
and Ï† (b) = c. Therefore, Ï†â€² â‰¤0 and the second integral equals
âˆ’
Z b
a
f (Ï† (t)) Ï†â€² (t) dt =
Z a
b
d
dt (F (Ï† (t))) dt
= F (Ï† (a)) âˆ’F (Ï† (b)) = F (d) âˆ’F (c) .
The case when Ï† is increasing is similar but easier. This proves the theorem.
Lemma 16.2.14 Let f : [a, b] â†’C, g : [c, d] â†’C be parameterizations of a smooth curve
which satisfy conditions 1 - 5. Then Ï† (t) â‰¡gâˆ’1 â—¦f (t) is 1 âˆ’1 on (a, b) , continuous on
[a, b] , and either strictly increasing or strictly decreasing on [a, b] .
Proof: It is obvious Ï† is 1 âˆ’1 on (a, b) from the conditions f and g satisfy. It only
remains to verify continuity on [a, b] because then the ï¬nal claim follows from Lemma 16.2.8.
If Ï† is not continuous on [a, b] , then there exists a sequence, {tn} âŠ†[a, b] such that tn â†’t
but Ï† (tn) fails to converge to Ï† (t) . Therefore, for some Îµ > 0 there exists a subsequence,
still denoted by n such that |Ï† (tn) âˆ’Ï† (t)| â‰¥Îµ. Using the sequential compactness of [c, d] ,
(See Theorem 16.2.7 on Page 293.) there is a further subsequence, still denoted by n such
that {Ï† (tn)} converges to a point, s, of [c, d] which is not equal to Ï† (t) . Thus gâˆ’1â—¦f (tn) â†’s
and still tn â†’t. Therefore, the continuity of f and g imply f (tn) â†’g (s) and f (tn) â†’f (t) .
Therefore, g (s) = f (t) and so s = gâˆ’1 â—¦f (t) = Ï† (t) , a contradiction. Therefore, Ï† is
continuous as claimed.
Theorem 16.2.15 The length of a smooth curve is not dependent on parameter-
ization.
Proof:
Let C be the curve and suppose f : [a, b] â†’C and g : [c, d] â†’C both satisfy
conditions 1 - 5. Is it true that
R b
a |f â€² (t)| dt =
R d
c |gâ€² (s)| ds?
Let Ï† (t) â‰¡gâˆ’1â—¦f (t) for t âˆˆ[a, b]. Then by the above lemma Ï† is either strictly increasing
or strictly decreasing on [a, b] . Suppose for the sake of simplicity that it is strictly increasing.
The decreasing case is handled similarly.
Let s0 âˆˆÏ† ([a + Î´, b âˆ’Î´]) âŠ‚(c, d) . Then by assumption 4, gâ€²
i (s0) Ì¸= 0 for some i. By
continuity of gâ€²
i, it follows gâ€²
i (s) Ì¸= 0 for all s âˆˆI where I is an open interval contained
in [c, d] which contains s0. It follows that on this interval, gi is either strictly increasing
or strictly decreasing. Therefore, J â‰¡gi (I) is also an open interval and you can deï¬ne a
diï¬€erentiable function, hi : J â†’I by
hi (gi (s)) = s.
This implies that for s âˆˆI,
hâ€²
i (gi (s)) =
1
gâ€²
i (s).
(16.5)
Now letting s = Ï† (t) for s âˆˆI, it follows t âˆˆJ1, an open interval. Also, for s and t related
this way, f (t) = g (s) and so in particular, for s âˆˆI,
gi (s) = fi (t) .
Consequently,
s = hi (fi (t)) = Ï† (t)

16.3.
PRODUCT RULE FOR MATRICESâˆ—
297
and so, for t âˆˆJ1,
Ï†â€² (t) = hâ€²
i (fi (t)) f â€²
i (t) = hâ€²
i (gi (s)) f â€²
i (t) =
f â€²
i (t)
gâ€²
i (Ï† (t))
(16.6)
which shows that Ï†â€² exists and is continuous on J1, an open interval containing Ï†âˆ’1 (s0) .
Since s0 is arbitrary, this shows Ï†â€² exists on [a + Î´, b âˆ’Î´] and is continuous there.
Now f (t) = gâ—¦
Â¡
gâˆ’1 â—¦f
Â¢
(t) = g (Ï† (t)) and it was just shown that Ï†â€² is a continuous
function on [a âˆ’Î´, b + Î´] . It follows
f â€² (t) = gâ€² (Ï† (t)) Ï†â€² (t)
and so, by Theorem 16.2.13,
Z Ï†(bâˆ’Î´)
Ï†(a+Î´)
|gâ€² (s)| ds
=
Z bâˆ’Î´
a+Î´
|gâ€² (Ï† (t))|
Â¯Â¯Ï†â€² (t)
Â¯Â¯ dt
=
Z bâˆ’Î´
a+Î´
|f â€² (t)| dt.
Now using the continuity of Ï†, gâ€², and f â€² on [a, b] and letting Î´ â†’0+ in the above, yields
Z d
c
|gâ€² (s)| ds =
Z b
a
|f â€² (t)| dt
and this proves the theorem.
16.3
Product Rule For Matricesâˆ—
Another kind of multiplication is matrix multiplication. Here is the concept of the product
rule extended to matrix multiplication.
Deï¬nition 16.3.1 Let A (t) be an m Ã— n matrix. Say A (t) = (Aij (t)) . Suppose
also that Aij (t) is a diï¬€erentiable function for all i, j. Then deï¬ne Aâ€² (t) â‰¡
Â¡
Aâ€²
ij (t)
Â¢
. That
is, Aâ€² (t) is the matrix which consists of replacing each entry by its derivative. Such an mÃ—n
matrix in which the entries are diï¬€erentiable functions is called a diï¬€erentiable matrix.
The next lemma is just a version of the product rule.
Lemma 16.3.2 Let A (t) be an m Ã— n matrix and let B (t) be an n Ã— p matrix with the
property that all the entries of these matrices are diï¬€erentiable functions. Then
(A (t) B (t))â€² = Aâ€² (t) B (t) + A (t) Bâ€² (t) .
Proof: (A (t) B (t))â€² =
Â¡
Câ€²
ij (t)
Â¢
where Cij (t) = Aik (t) Bkj (t) and the repeated index
summation convention is being used. Therefore,
Câ€²
ij (t)
=
Aâ€²
ik (t) Bkj (t) + Aik (t) Bâ€²
kj (t)
=
(Aâ€² (t) B (t))ij + (A (t) Bâ€² (t))ij
=
(Aâ€² (t) B (t) + A (t) Bâ€² (t))ij
Therefore, the ijth entry of A (t) B (t) equals the ijth entry of Aâ€² (t) B (t) + A (t) Bâ€² (t) and
this proves the lemma.

298
PHYSICS OF CURVILINEAR MOTION 12 OCT.
16.4
Moving Coordinate Systemsâˆ—
Let i (t) , j (t) , k (t) be a right handed3 orthonormal basis of vectors for each t. It is assumed
these vectors are C1 functions of t. Letting the positive x axis extend in the direction of
i (t) , the positive y axis extend in the direction of j (t), and the positive z axis extend in the
direction of k (t) , yields a moving coordinate system. Now let u = (u1, u2, u3) âˆˆR3 and let
t0 be some reference time. For example you could let t0 = 0. Then deï¬ne the components
of u with respect to these vectors, i, j, k at time t0 as
u â‰¡u1i (t0) + u2j (t0) + u3k (t0) .
Let u (t) be deï¬ned as the vector which has the same components with respect to i, j, k but
at time t. Thus
u (t) â‰¡u1i (t) + u2j (t) + u3k (t) .
and the vector has changed although the components have not.
For example, this is exactly the situation in the case of apparently ï¬xed basis vectors
on the earth if u is a position vector from the given spot on the earthâ€™s surface to a point
regarded as ï¬xed with the earth due to its keeping the same coordinates relative to coordinate
axes which are ï¬xed with the earth.
Now deï¬ne a linear transformation Q (t) mapping R3 to R3 by
Q (t) u â‰¡u1i (t) + u2j (t) + u3k (t)
where
u â‰¡u1i (t0) + u2j (t0) + u3k (t0)
Thus letting v, u âˆˆR3 be vectors and Î±, Î², scalars,
Q (t) (Î±u + Î²v) â‰¡(Î±u1 + Î²v1) i (t) + (Î±u2 + Î²v2) j (t) + (Î±u3 + Î²v3) k (t)
=
(Î±u1i (t) + Î±u2j (t) + Î±u3k (t)) + (Î²v1i (t) + Î²v2j (t) + Î²v3k (t))
=
Î± (u1i (t) + u2j (t) + u3k (t)) + Î² (v1i (t) + v2j (t) + v3k (t))
â‰¡
Î±Q (t) u + Î²Q (t) v
showing that Q (t) is a linear transformation. Also, Q (t) preserves all distances because,
since the vectors, i (t) , j (t) , k (t) form an orthonormal set,
|Q (t) u| =
Ãƒ 3
X
i=1
Â¡
uiÂ¢2
!1/2
= |u| .
For simplicity, let
i (t) = e1 (t) , j (t) = e2 (t) , k (t) = e3 (t)
and
i (t0) = e1 (t0) , j (t0) = e2 (t0) , k (t0) = e3 (t0) .
Then using the repeated index summation convention,
u (t) = ujej (t) = ujej (t) Â· ei (t0) ei (t0)
3Recall that right handed implies i Ã— j = k.

16.4.
MOVING COORDINATE SYSTEMSâˆ—
299
and so with respect to the basis, i (t0) = e1 (t0) , j (t0) = e2 (t0) , k (t0) = e3 (t0) , the matrix
of Q (t) is
Qij (t) = ei (t0) Â· ej (t)
Recall this means you take a vector, u âˆˆR3 which is a list of the components of u with
respect to i (t0) , j (t0) , k (t0) and when you multiply by Q (t) you get the components of
u (t) with respect to i (t0) , j (t0) , k (t0) . I will refer to this matrix as Q (t) to save notation.
Lemma 16.4.1 Suppose Q (t) is a real, diï¬€erentiable n Ã— n matrix which preserves dis-
tances. Then Q (t) Q (t)T = Q (t)T Q (t) = I. Also, if u (t) â‰¡Q (t) u, then there exists a
vector, â„¦(t) such that
uâ€² (t) = â„¦(t) Ã— u (t) .
Proof: Recall that (z Â· w) = 1
4
Â³
|z + w|2 âˆ’|z âˆ’w|2Â´
. Therefore,
(Q (t) uÂ·Q (t) w)
=
1
4
Â³
|Q (t) (u + w)|2 âˆ’|Q (t) (u âˆ’w)|2Â´
=
1
4
Â³
|u + w|2 âˆ’|u âˆ’w|2Â´
=
(u Â· w) .
This implies
Â³
Q (t)T Q (t) u Â· w
Â´
= (u Â· w)
for all u, w. Therefore, Q (t)T Q (t) u = u and so Q (t)T Q (t) = Q (t) Q (t)T = I. This proves
the ï¬rst part of the lemma.
It follows from the product rule, Lemma 16.3.2 that
Qâ€² (t) Q (t)T + Q (t) Qâ€² (t)T = 0
and so
Qâ€² (t) Q (t)T = âˆ’
Â³
Qâ€² (t) Q (t)T Â´T
.
(16.7)
From the deï¬nition, Q (t) u = u (t) ,
uâ€² (t) = Qâ€² (t) u =Qâ€² (t)
=u
z
}|
{
Q (t)T u (t).
Then writing the matrix of Qâ€² (t) Q (t)T with respect to i (t0) , j (t0) , k (t0) , it follows from
16.7 that the matrix of Qâ€² (t) Q (t)T is of the form
ï£«
ï£­
0
âˆ’Ï‰3 (t)
Ï‰2 (t)
Ï‰3 (t)
0
âˆ’Ï‰1 (t)
âˆ’Ï‰2 (t)
Ï‰1 (t)
0
ï£¶
ï£¸
for some time dependent scalars, Ï‰i. Therefore,
ï£«
ï£­
u1
u2
u3
ï£¶
ï£¸
â€²
(t)
=
ï£«
ï£­
0
âˆ’Ï‰3 (t)
Ï‰2 (t)
Ï‰3 (t)
0
âˆ’Ï‰1 (t)
âˆ’Ï‰2 (t)
Ï‰1 (t)
0
ï£¶
ï£¸
ï£«
ï£­
u1
u2
u3
ï£¶
ï£¸(t)
=
ï£«
ï£­
w2 (t) u3 (t) âˆ’w3 (t) u2 (t)
w3 (t) u1 (t) âˆ’w1 (t) u3 (t)
w1 (t) u2 (t) âˆ’w2 (t) u1 (t)
ï£¶
ï£¸

300
PHYSICS OF CURVILINEAR MOTION 12 OCT.
where the ui are the components of the vector u (t) in terms of the ï¬xed vectors i (t0) , j (t0) , k (t0) .
Therefore,
uâ€² (t) = â„¦(t) Ã—u (t) = Qâ€² (t) Q (t)T u (t)
(16.8)
where
â„¦(t) = Ï‰1 (t) i (t0) +Ï‰2 (t) j (t0) +Ï‰3 (t) k (t0) .
because
â„¦(t) Ã— u (t) â‰¡
Â¯Â¯Â¯Â¯Â¯Â¯
i (t0)
j (t0)
k (t0)
w1
w2
w3
u1
u2
u3
Â¯Â¯Â¯Â¯Â¯Â¯
â‰¡
i (t0) (w2u3 âˆ’w3u2) + j (t0) (w3u1 âˆ’w1u3) + k (t0) (w1u2 âˆ’w2u1) .
This proves the lemma and yields the existence part of the following theorem.
Theorem 16.4.2 Let i (t) , j (t) , k (t) be as described. Then there exists a unique
vector â„¦(t) such that if u (t) is a vector whose components are constant with respect to
i (t) , j (t) , k (t) , then
uâ€² (t) = â„¦(t) Ã— u (t) .
Proof: It only remains to prove uniqueness. Suppose â„¦1 also works. Then u (t) = Q (t) u
and so uâ€² (t) = Qâ€² (t) u and
Qâ€² (t) u = â„¦Ã—Q (t) u = â„¦1Ã—Q (t) u
for all u. Therefore,
(â„¦âˆ’â„¦1) Ã—Q (t) u = 0
for all u and since Q (t) is one to one and onto, this implies (â„¦âˆ’â„¦1) Ã—w = 0 for all w and
thus â„¦âˆ’â„¦1 = 0. This proves the theorem.
Deï¬nition 16.4.3 A rigid body in R3 has a moving coordinate system with the
property that for an observer on the rigid body, the vectors, i (t) , j (t) , k (t) are constant.
More generally, a vector u (t) is said to be ï¬xed with the body if to a person on the body, the
vector appears to have the same magnitude and same direction independent of t. Thus u (t)
is ï¬xed with the body if u (t) = u1i (t) + u2j (t) + u3k (t).
The following comes from the above discussion.
Theorem 16.4.4 Let B (t) be the set of points in three dimensions occupied by a
rigid body. Then there exists a vector â„¦(t) such that whenever u (t) is ï¬xed with the rigid
body,
uâ€² (t) = â„¦(t) Ã— u (t) .

Part VII
Functions Of Many Variables
301


303
Outcomes
Functions of Several variables
A. Identify the domain and range of a function of several variables.
B. Represent a function of two variables by level curves or a function of three variables
by level surfaces.
C. Identify the characteristics of a function from its graph or from a graph of its level
curves (or level surfaces).
D. Represent combinations of multivariable functions algebraically.
Reading: Multivariable Calculus 2.1
Outcome Mapping:
A. 1
B. 2,7,8
C. 3,5,6
D. 9
Limits and Continuity
A. Describe a delta neighborhood of a point in 2- or 3-space.
B. Evaluate the limit of a function of several variables for a given approach or show that
it does not exists.
C. Determine whether a function is continuous at a given point. Interpret the deï¬nition
of continuity of a function of several variables graphically.
D. Determine whether a set in 2- or 3-space is open, closed or neither. Determine whether
a set is compact.
E. Recall and apply the Extreme Value Theorem.
Reading: Multivariable Calculus 2.2
Outcome Mapping:
A. F1
B. 1,2,3
C. 8,F4
D. F3,11,12,13
E. F2
Partial Derivatives
A. Interpret the deï¬nition of a partial derivative of a function of two variables graphically.
B. Evaluate the partial derivatives of a function of several variables.

304
D. Evaluate the higher order partial derivatives of a function of several variables.
E. State the conditions under which mixed partial derivatives are equal.
F. Verify equations involving partial derivatives.
G. Evaluate the gradient of a function.
H. Prove identities involving the gradient.
Reading: Multivariable Calculus 2.3
Outcome Mapping:
A. G1,4,18
B. 3,5,6
D. 5,7
E. G3
F. 12,15
G. 9
H. 10

Functions Of Many Variables 16
Oct.
Quiz
1. Let r (t) = (cos (t) , sin (t) , 2t). Find a,aT , and aN. Also ï¬nd Îº and write the acceler-
ation as the sum of two terms, one in the direction of the unit tangent vector and the
other in the direction of the principle normal. Find âƒ—Îº, the curvature vector which is a
completely useless concept.
2. Here is a matrix which happens to have âˆ’1 as an eigenvalue. Find the eigenspace
corresponding to this eigenvalue.
ï£«
ï£­
1
2
2
âˆ’2
âˆ’3
âˆ’2
2
2
1
ï£¶
ï£¸
Is the matrix defective or nondefective?
3. In Problem 2 ï¬nd the determinant of the matrix.
4. Raise the matrix of Problem 2 to the 15th power exactly.
17.1
The Graph Of A Function Of Two Variables
With vector valued functions of many variables, it doesnâ€™t take long before it is impossible
to draw meaningful pictures.
This is because one needs more than three dimensions to
accomplish the task and we can only visualize things in three dimensions. Ultimately, one
of the main purposes of calculus is to free us from the tyranny of art. In calculus, we are
permitted and even required to think in a meaningful way about things which cannot be
drawn. However, it is certainly interesting to consider some things which can be visualized
and this will help to formulate and understand more general notions which make sense in
contexts which cannot be visualized. One of these is the concept of a scalar valued function
of two variables.
Let f (x, y) denote a scalar valued function of two variables evaluated at the point (x, y) .
Its graph consists of the set of points, (x, y, z) such that z = f (x, y) . How does one go about
depicting such a graph? The usual way is to ï¬x one of the variables, say x and consider
the function z = f (x, y) where y is allowed to vary and x is ï¬xed. Graphing this would
give a curve which lies in the surface to be depicted. Then do the same thing for other
values of x and the result would depict the graph desired graph. Computers do this very
305

306
FUNCTIONS OF MANY VARIABLES 16 OCT.
well. The following is the graph of the function z = cos (x) sin (2x + y) drawn using Maple,
a computer algebra system.1.
Notice how elaborate this picture is.
The lines in the drawing correspond to taking
one of the variables constant and graphing the curve which results. The computer did this
drawing in seconds but you couldnâ€™t do it as well if you spent all day on it. I used a grid
consisting of 70 choices for x and 70 choices for y.
Sometimes attempts are made to understand three dimensional objects like the above
graph by looking at contour graphs in two dimensions. The contour graph of the above
three dimensional graph is below and comes from using the computer algebra system again.
â€“4
â€“2
0
2
4
y
â€“4
â€“2
2
4
x
This is in two dimensions and the diï¬€erent lines in two dimensions correspond to points
on the three dimensional graph which have the same z value. If you have looked at a weather
map, these lines are called isotherms or isobars depending on whether the function involved is
temperature or pressure. In a contour geographic map, the contour lines represent constant
altitude. If many contour lines are close to each other, this indicates rapid change in the
altitude, temperature, pressure, or whatever else may be measured.
A scalar function of three variables, cannot be visualized because four dimensions are
required. However, some people like to try and visualize even these examples. This is done
by looking at level surfaces in R3 which are deï¬ned as surfaces where the function assumes
a constant value. They play the role of contour lines for a function of two variables. As a
simple example, consider f (x, y, z) = x2 + y2 + z2. The level surfaces of this function would
be concentric spheres centered at 0. (Why?) Another way to visualize objects in higher
dimensions involves the use of color and animation. However, there really are limits to what
you can accomplish in this direction. So much for art.
However, the concept of level curves is quite useful because these can be drawn.
Example 17.1.1 Determine from a contour map where the function, f (x, y) = sin
Â¡
x2 + y2Â¢
is steepest.
1I used Maple and exported the graph as an eps. ï¬le which I then imported into this document.

17.2.
THE DOMAIN OF A FUNCTION
307
â€“3
â€“2
â€“1
1
2
3
y
â€“3
â€“2
â€“1
1
2
3
x
In the picture, the steepest places are where the contour lines are close together because
they correspond to various values of the function. You can look at the picture and see where
they are close and where they are far. This is the advantage of a contour map.
17.2
The Domain Of A Function
As usual the domain of a function is either speciï¬ed or if it is unspeciï¬ed, it is the set of
all points for which the function makes sense. If f is the name of the function its domain is
denoted as D(f).
Example 17.2.1 Find the domain of the function, f (x, y) =
p
1 âˆ’(x2 + y2).
You need to have 1 â‰¥x2+y2 and so the domain of this function is
Â©
(x, y) âˆˆR2 : x2 + y2 â‰¤1
Âª
.
This is just the inside of the unit circle centered at (0, 0) . It also includes the edge of this
unit circle.
Sometimes the domain is given to you in a very artiï¬cial way.
Example 17.2.2 Let D (f) =
Â©
(x, y) âˆˆR2 : x2 + y2 â‰¤1
Âª
âˆª(3, 7) . Let f (x, y) = x + 2y for
(x, y) âˆˆ
Â©
(x, y) âˆˆR2 : x2 + y2 â‰¤1
Âª
and let f (3, 7) = 33.
In this case, the domain of the function is as given above and the function is given the
deï¬nition just described.
Now remember from calculus of functions of one variable some of the things you did.
One of the most important was to consider the derivative of a function. Recall the deï¬nition
of the derivative, f â€² (x).
lim
yâ†’x
f (y) âˆ’f (x)
y âˆ’x
â‰¡f â€² (x) .
In order to write this deï¬nition you need to have f deï¬ned for all values of y near x, That
is, you need to have f deï¬ned on an open interval containing x of the form (x âˆ’Î´, x + Î´)
for some Î´ > 0. Otherwise, you canâ€™t consider f (y) . This is one reason for the importance
of the concepts in the next section.
17.3
Open And Closed Sets
We are going to consider functions deï¬ned on subsets of Rn and their properties. The next
deï¬nition will end up being quite important. It describe a type of subset of Rn with the
property that if x is in this set, then so is y whenever y is close enough to x. It is essential
you understand a few kinds of sets.

308
FUNCTIONS OF MANY VARIABLES 16 OCT.
Deï¬nition 17.3.1 Let x âˆˆRn. Then B (x, r) , called the ball centered at x having
radius r is deï¬ned to be the set of all points of Rn, y which have the property that these
points are closer than r to x. Thus y âˆˆB (x, r) means |y âˆ’x| < r. Written formally, this
is
B (x, r) â‰¡{y âˆˆRn : |y âˆ’x| < r} .
To say that B (x, r) âŠ†D (f) means that whenever y is closer to x than r, it follows
y âˆˆD (f) . Now recall this is the sort of thing which you must start with, even in one
dimension, to consider the concept of the derivative of a function.
Therefore, it is not
surprising that such an idea would be important in Rn.
Deï¬nition 17.3.2 Let U âŠ†Rn. U is an open set if whenever x âˆˆU, there exists
r > 0 such that B (x, r) âŠ†U. More generally, if U is any subset of Rn, x âˆˆU is an interior
point of U if there exists r > 0 such that x âˆˆB (x, r) âŠ†U. In other words U is an open set
exactly when every point of U is an interior point of U.
If there is something called an open set, surely there should be something called a closed
set and here is the deï¬nition of one.
Deï¬nition 17.3.3 A subset, C, of Rn is called a closed set if Rn \ C is an open
set. They symbol, Rn \ C denotes everything in Rn which is not in C. It is also called the
complement of C. The symbol, SC is a short way of writing Rn \ S. A bounded set is one
which is contained in a large enough ball. In Rn a set which is both closed and bounded is
compact. 2
To illustrate this deï¬nition, consider the following picture.
qx
U
B(x, r)
You see in this picture how the edges are dotted. This is because an open set, canâ€™t
include the edges or the set would fail to be open.
For example, consider what would
happen if you picked a point out on the edge of U in the above picture. Every open ball
centered at that point would have in it some points which are outside U. Therefore, such a
point would violate the above deï¬nition. You also see the edges of B (x, r) dotted suggesting
that B (x, r) ought to be an open set. This is intuitively clear but does require a proof. This
will be done in the next theorem and will give examples of open sets. Also, you can see that
if x is close to the edge of U, you might have to take r to be very small.
It is roughly the case that open sets donâ€™t have their skins while closed sets do. So
why might it be important to consider closed sets? Remember from one variable calculus
the theorem which says that a continuous function achieves its maximum and minimum
on a closed interval. The closed interval contains its â€œskinâ€, the end points of the interval.
2Actually the term compact has independent meaning and there is a theorem called the Heine Borel
theorem which states that in Rn closed and bounded sets are compact. See the section on theory for more
on this. This is not just useless jargon and gratuitous terminology.

17.3.
OPEN AND CLOSED SETS
309
Similar theorems will end up holding for functions of n variables. Here is a picture of a
closed set, C.
B(x, r)
x
q
C
Note that x /âˆˆC and since Rn \ C is open, there exists a ball, B (x, r) contained entirely
in Rn \C. If you look at Rn \C, what would be its skin? It canâ€™t be in Rn \C and so it must
be in C. This is a rough heuristic explanation of what is going on with these deï¬nitions.
Also note that Rn and âˆ…are both open and closed. Here is why. If x âˆˆâˆ…, then there must
be a ball centered at x which is also contained in âˆ…. This must be considered to be true
because there is nothing in âˆ…so there can be no example to show it false3. Therefore, from
the deï¬nition, it follows âˆ…is open. It is also closed because if x /âˆˆâˆ…, then B (x, 1) is also
contained in Rn \ âˆ…= Rn. Therefore, âˆ…is both open and closed. From this, it follows Rn is
also both open and closed.
Theorem 17.3.4 Let x âˆˆRn and let r â‰¥0. Then B (x, r) is an open set. Also,
D (x, r) â‰¡{y âˆˆRn : |y âˆ’x| â‰¤r}
is a closed set.
3To a mathematician, the statement: Whenever a pig is born with wings it can ï¬‚y must be taken as
true. We do not consider biological or aerodynamic considerations in such statements. There is no such
thing as a winged pig and therefore, all winged pigs must be superb ï¬‚yers since there can be no example of
one which is not. On the other hand we would also consider the statement: Whenever a pig is born with
wings it canâ€™t possibly ï¬‚y, as equally true. The point is, you can say anything you want about the elements
of the empty set and no one can gainsay your statement. Therefore, such statements are considered as true
by default. You may say this is a very strange way of thinking about truth and ultimately this is because
mathematics is not about truth. It is more about consistency and logic.

310
FUNCTIONS OF MANY VARIABLES 16 OCT.
Proof: Suppose y âˆˆB (x,r) . It is necessary to show there exists r1 > 0 such that
B (y, r1) âŠ†B (x, r) . Deï¬ne r1 â‰¡r âˆ’|x âˆ’y| . Then if |z âˆ’y| < r1, it follows from the above
triangle inequality that
|z âˆ’x|
=
|z âˆ’y + y âˆ’x|
â‰¤
|z âˆ’y| + |y âˆ’x|
<
r1 + |y âˆ’x| = r âˆ’|x âˆ’y| + |y âˆ’x| = r.
Note that if r = 0 then B (x, r) = âˆ…, the empty set. This is because if y âˆˆRn, |x âˆ’y| â‰¥0
and so y /âˆˆB (x, 0) . Since âˆ…has no points in it, it must be open because every point in it,
(There are none.) satisï¬es the desired property of being an interior point.
Now suppose y /âˆˆD (x, r) . Then |x âˆ’y| > r and deï¬ning Î´ â‰¡|x âˆ’y| âˆ’r, it follows that
if z âˆˆB (y, Î´) , then by the triangle inequality,
|x âˆ’z|
â‰¥
|x âˆ’y| âˆ’|y âˆ’z| > |x âˆ’y| âˆ’Î´
=
|x âˆ’y| âˆ’(|x âˆ’y| âˆ’r) = r
and this shows that B (y, Î´) âŠ†Rn \D (x, r) . Since y was an arbitrary point in Rn \D (x, r) ,
it follows Rn\D (x, r) is an open set which shows from the deï¬nition that D (x, r) is a closed
set as claimed.
A picture which is descriptive of the conclusion of the above theorem which also implies
the manner of proof is the following.
y
x
q
q
6
-
r
r1
B(x, r)
y
x
q
q
6
-
r
r1
D(x, r)
Recall R2 consists of ordered pairs, (x, y) such that x âˆˆR and y âˆˆR. R2 is also written
as R Ã— R. In general, the following deï¬nition holds.
Deï¬nition 17.3.5 The Cartesian product of two sets, AÃ—B, means {(a, b) : a âˆˆA, b âˆˆB} .
If you have n sets, A1, A2, Â· Â· Â·, An
n
Y
i=1
Ai = {(x1, x2, Â· Â· Â·, xn) : each xi âˆˆAi} .
Now suppose A âŠ†Rm and B âŠ†Rn. Then if (x, y) âˆˆA Ã— B, x = (x1, Â· Â· Â·, xm) and
y = (y1, Â· Â· Â·, yn), the following identiï¬cation will be made.
(x, y) = (x1, Â· Â· Â·, xm, y1, Â· Â· Â·, yn) âˆˆRn+m.
Similarly, starting with something in Rn+m, you can write it in the form (x, y) where x âˆˆRm
and y âˆˆRn. The following theorem has to do with the Cartesian product of two closed sets
or two open sets. Also here is an important deï¬nition.
Deï¬nition 17.3.6 A set, A âŠ†Rn is said to be bounded if there exist ï¬nite inter-
vals, [ai, bi] such that
A âŠ†
n
Y
i=1
[ai, bi] .

17.4.
CONTINUOUS FUNCTIONS
311
Theorem 17.3.7 Let U be an open set in Rm and let V be an open set in Rn.
Then U Ã— V is an open set in Rn+m. If C is a closed set in Rm and H is a closed set in
Rn, then C Ã— H is a closed set in Rn+m. If C and H are bounded, then so is C Ã— H.
Proof: Let (x, y) âˆˆU Ã— V. Since U is open, there exists r1 > 0 such that B (x, r1) âŠ†U.
Similarly, there exists r2 > 0 such that B (y, r2) âŠ†V . Now
B ((x, y) , Î´) â‰¡
ï£±
ï£²
ï£³(s, t) âˆˆRn+m :
m
X
k=1
|xk âˆ’sk|2 +
n
X
j=1
|yj âˆ’tj|2 < Î´2
ï£¼
ï£½
ï£¾
Therefore, if Î´ â‰¡min (r1, r2) and (s, t) âˆˆB ((x, y) , Î´) , then it follows that s âˆˆB (x, r1) âŠ†U
and that t âˆˆB (y, r2) âŠ†V which shows that B ((x, y) , Î´) âŠ†U Ã— V. Hence U Ã— V is open as
claimed.
Next suppose (x, y) /âˆˆC Ã— H. It is necessary to show there exists Î´ > 0 such that
B ((x, y) , Î´) âŠ†Rn+m \ (C Ã— H) . Either x /âˆˆC or y /âˆˆH since otherwise (x, y) would be a
point of C Ã—H. Suppose therefore, that x /âˆˆC. Since C is closed, there exists r > 0 such that
B (x, r) âŠ†Rm \C. Consider B ((x, y) , r) . If (s, t) âˆˆB ((x, y) , r) , it follows that s âˆˆB (x, r)
which is contained in Rm \ C. Therefore, B ((x, y) , r) âŠ†Rn+m \ (C Ã— H) showing C Ã— H is
closed. A similar argument holds if y /âˆˆH.
If C is bounded, there exist [ai, bi] such that C âŠ†Qm
i=1 [ai, bi] and if H is bounded,
H âŠ†Qm+n
i=m+1 [ai, bi] for intervals [am+1, bm+1] , Â· Â· Â·, [am+n, bm+n] . Therefore, C Ã— H âŠ†
Qm+n
i=1 [ai, bi] and this establishes the last part of this theorem.
17.4
Continuous Functions
What was done in beginning calculus for scalar functions is generalized here to include the
case of a vector valued function of possibly many variables. What follows is the correct
deï¬nition of continuity. The one you are used to seeing in terms of the value of the function
corresponding to the value of its limit is not correct in general. This one you are used to
seeing is only correct if the point of the domain of the function is a limit point of the domain,
discussed brieï¬‚y later (Donâ€™t worry about it too much. Just use the correct deï¬nition and
you will be ï¬ne.). It isnâ€™t a big deal for functions of one variables because you usually
are dealing with functions deï¬ned on intervals and it happens that all the points are limit
points. In multiple dimensions, however, the earlier deï¬nition is woefully inadequate and
will lead you to profound confusion, confusion which is so severe you will have to relearn
everything you thought you understood. I know this from bitter personal experience.
Deï¬nition 17.4.1 A function f : D (f) âŠ†Rp â†’Rq is continuous at x âˆˆD (f) if
for each Îµ > 0 there exists Î´ > 0 such that whenever y âˆˆD (f) and
|y âˆ’x| < Î´
it follows that
|f (x) âˆ’f (y)| < Îµ.
f is continuous if it is continuous at every point of D (f) .
Note the total similarity to the scalar valued case.

312
FUNCTIONS OF MANY VARIABLES 16 OCT.
17.5
Suï¬ƒcient Conditions For Continuity
The next theorem is a fundamental result which allows less worry about the Îµ Î´ deï¬nition
of continuity.
Theorem 17.5.1 The following assertions are valid
1. The function, af +bg is continuous at x when f, g are continuous at x âˆˆD (f)âˆ©D (g)
and a, b âˆˆR.
2. If and f and g are each real valued functions continuous at x, then fg is continuous
at x. If, in addition to this, g (x) Ì¸= 0, then f/g is continuous at x.
3. If f is continuous at x, f (x) âˆˆD (g) âŠ†Rp, and g is continuous at f (x) ,then g â—¦f is
continuous at x.
4. If f = (f1, Â· Â· Â·, fq) : D (f) â†’Rq, then f is continuous if and only if each fk is a
continuous real valued function.
5. The function f : Rp â†’R, given by f (x) = |x| is continuous.
The proof of this theorem is given later. Its conclusions are not surprising. For example
the ï¬rst claim says that (af + bg) (y) is close to (af + bg) (x) when y is close to x provided
the same can be said about f and g. For the second claim, if y is close to x, f (x) is close to
f (y) and so by continuity of g at f (x), g (f (y)) is close to g (f (x)) . To see the third claim
is likely, note that closeness in Rp is the same as closeness in each coordinate. The fourth
claim is immediate from the triangle inequality.
For functions deï¬ned on Rn, there is a notion of polynomial just as there is for functions
deï¬ned on R.
Deï¬nition 17.5.2 Let Î± be an n dimensional multi-index. This means
Î± = (Î±1, Â· Â· Â·, Î±n)
where each Î±i is a natural number or zero. Also, let
|Î±| â‰¡
n
X
i=1
|Î±i|
The symbol, xÎ±means
xÎ± â‰¡xÎ±1
1 xÎ±2
2 Â· Â· Â· xÎ±n
3 .
An n dimensional polynomial of degree m is a function of the form
p (x) =
X
|Î±|â‰¤m
dÎ±xÎ±.
where the dÎ± are real numbers.
The above theorem implies that polynomials are all continuous.

17.6.
PROPERTIES OF CONTINUOUS FUNCTIONS
313
17.6
Properties Of Continuous Functions
Functions of many variables have many of the same properties as functions of one variable.
First there is a version of the extreme value theorem generalizing the one dimensional case.
Theorem 17.6.1 Let C be closed and bounded and let f : C â†’R be continuous.
Then f achieves its maximum and its minimum on C. This means there exist, x1, x2 âˆˆC
such that for all x âˆˆC,
f (x1) â‰¤f (x) â‰¤f (x2) .
The above theorems are proved in an optional section.

314
FUNCTIONS OF MANY VARIABLES 16 OCT.

Limits Of A Function 17-23 Oct.
Quiz
1. The position vector of an object is r (t) =
Â¡
et, sin (t) , t2 âˆ’1
Â¢
. Find the unit tangent
vector when t = 0.
2. Show that for v (t) a vector valued function
d
dt |v (t)| = vâ€²Â·v
|v| . (Note that in the case
where v is velocity, this implies a Â· T = d
dt |v| .)
3. Suppose r (t) =
Â¡
t2, cos (t) , sin (t)
Â¢
. Find the curvature when t = 0.
4. Suppose r (t) =
Â¡
2t1/2, 2
3t3/2,
âˆš
2t
Â¢
for t âˆˆ[1, 2] . Find the length of this curve.
5. Find the matrix of the linear transformation which projects all vectors onto the line
y = x.
As in the case of scalar valued functions of one variable, a concept closely related to
continuity is that of the limit of a function. The notion of limit of a function makes sense
at points, x, which are limit points of D (f) and this concept is deï¬ned next. It is a harder
concept than the concept of continuity.
Deï¬nition 18.0.2 Let A âŠ†Rm be a set. A point, x, is a limit point of A if B (x, r)
contains inï¬nitely many points of A for every r > 0.
Deï¬nition 18.0.3 Let f : D (f) âŠ†Rp â†’Rq be a function and let x be a limit
point of D (f) . Then
lim
yâ†’x f (y) = L
if and only if the following condition holds. For all Îµ > 0 there exists Î´ > 0 such that if
0 < |y âˆ’x| < Î´, and y âˆˆD (f)
then,
|L âˆ’f (y)| < Îµ.
Theorem 18.0.4 If limyâ†’x f (y) = L and limyâ†’x f (y) = L1, then L = L1.
Proof: Let Îµ > 0 be given.
There exists Î´ > 0 such that if 0 < |y âˆ’x| < Î´ and
y âˆˆD (f) , then
|f (y) âˆ’L| < Îµ, |f (y) âˆ’L1| < Îµ.
Pick such a y. There exists one because x is a limit point of D (f) . Then
|L âˆ’L1| â‰¤|L âˆ’f (y)| + |f (y) âˆ’L1| < Îµ + Îµ = 2Îµ.
315

316
LIMITS OF A FUNCTION 17-23 OCT.
Since Îµ > 0 was arbitrary, this shows L = L1.
As in the case of functions of one variable, one can deï¬ne what it means for limyâ†’x f (x) =
Â±âˆ.
Deï¬nition 18.0.5 If f (x) âˆˆR, limyâ†’x f (x) = âˆif for every number l, there
exists Î´ > 0 such that whenever |y âˆ’x| < Î´ and y âˆˆD (f) , then f (x) > l.
The following theorem is just like the one variable version of calculus.
Theorem 18.0.6 Suppose limyâ†’x f (y) = L and limyâ†’x g (y) = K where K, L âˆˆ
Rq. Then if a, b âˆˆR,
lim
yâ†’x (af (y) + bg (y)) = aL + bK,
(18.1)
lim
yâ†’x f Â· g (y) = L Â· K
(18.2)
and if g is scalar valued with limyâ†’x g (y) = K Ì¸= 0,
lim
yâ†’x f (y) g (y) = LK.
(18.3)
Also, if h is a continuous function deï¬ned near L, then
lim
yâ†’x h â—¦f (y) = h (L) .
(18.4)
Suppose limyâ†’x f (y) = L. If |f (y) âˆ’b| â‰¤r for all y suï¬ƒciently close to x, then |L âˆ’b| â‰¤r
also.
Proof: The proof of 18.1 is left for you. It is like a corresponding theorem for continuous
functions. Now 18.2is to be veriï¬ed. Let Îµ > 0 be given. Then by the triangle inequality,
|f Â· g (y) âˆ’L Â· K| â‰¤|fg (y) âˆ’f (y) Â· K| + |f (y) Â· K âˆ’L Â· K|
â‰¤|f (y)| |g (y) âˆ’K| + |K| |f (y) âˆ’L| .
There exists Î´1 such that if 0 < |y âˆ’x| < Î´1 and y âˆˆD (f) , then
|f (y) âˆ’L| < 1,
and so for such y, the triangle inequality implies, |f (y)| < 1 + |L| . Therefore, for 0 <
|y âˆ’x| < Î´1,
|f Â· g (y) âˆ’L Â· K| â‰¤(1 + |K| + |L|) [|g (y) âˆ’K| + |f (y) âˆ’L|] .
(18.5)
Now let 0 < Î´2 be such that if y âˆˆD (f) and 0 < |x âˆ’y| < Î´2,
|f (y) âˆ’L| <
Îµ
2 (1 + |K| + |L|), |g (y) âˆ’K| <
Îµ
2 (1 + |K| + |L|).
Then letting 0 < Î´ â‰¤min (Î´1, Î´2) , it follows from 18.5 that
|f Â· g (y) âˆ’L Â· K| < Îµ
and this proves 18.2.
The proof of 18.3 is left to you.
Consider 18.4. Since h is continuous near L, it follows that for Îµ > 0 given, there exists
Î· > 0 such that if |y âˆ’L| < Î·, then
|h (y) âˆ’h (L)| < Îµ

317
Now since limyâ†’x f (y) = L, there exists Î´ > 0 such that if 0 < |y âˆ’x| < Î´, then
|f (y) âˆ’L| < Î·.
Therefore, if 0 < |y âˆ’x| < Î´,
|h (f (y)) âˆ’h (L)| < Îµ.
It only remains to verify the last assertion. Assume |f (y) âˆ’b| â‰¤r. It is required to show
that |L âˆ’b| â‰¤r. If this is not true, then |L âˆ’b| > r. Consider B (L, |L âˆ’b| âˆ’r) . Since L
is the limit of f, it follows f (y) âˆˆB (L, |L âˆ’b| âˆ’r) whenever y âˆˆD (f) is close enough to
x. Thus, by the triangle inequality,
|f (y) âˆ’L| < |L âˆ’b| âˆ’r
and so
r
<
|L âˆ’b| âˆ’|f (y) âˆ’L| â‰¤||b âˆ’L| âˆ’|f (y) âˆ’L||
â‰¤
|b âˆ’f (y)| ,
a contradiction to the assumption that |b âˆ’f (y)| â‰¤r.
The next theorem gives the correct relation between continuity and the limit.
Theorem 18.0.7 For f : D (f) â†’Rq and x âˆˆD (f) a limit point of D (f) , f is
continuous at x if and only if
lim
yâ†’x f (y) = f (x) .
Proof: First suppose f is continuous at x a limit point of D (f) . Then for every Îµ > 0
there exists Î´ > 0 such that if |y âˆ’x| < Î´ and y âˆˆD (f) , then |f (x) âˆ’f (y)| < Îµ. In
particular, this holds if 0 < |x âˆ’y| < Î´ and this is just the deï¬nition of the limit. Hence
f (x) = limyâ†’x f (y) .
Next suppose x is a limit point of D (f) and limyâ†’x f (y) = f (x) . This means that if Îµ > 0
there exists Î´ > 0 such that for 0 < |x âˆ’y| < Î´ and y âˆˆD (f) , it follows |f (y) âˆ’f (x)| < Îµ.
However, if y = x, then |f (y) âˆ’f (x)| = |f (x) âˆ’f (x)| = 0 and so whenever y âˆˆD (f) and
|x âˆ’y| < Î´, it follows |f (x) âˆ’f (y)| < Îµ, showing f is continuous at x.
The following theorem is important.
Theorem 18.0.8 Suppose f : D (f) â†’Rq. Then for x a limit point of D (f) ,
lim
yâ†’x f (y) = L
(18.6)
if and only if
lim
yâ†’x fk (y) = Lk
(18.7)
where f (y) â‰¡(f1 (y) , Â· Â· Â·, fp (y)) and L â‰¡(L1, Â· Â· Â·, Lp) .
In the case where q = 3 and limyâ†’x f (y) = L and limyâ†’x g (y) = K, then
lim
yâ†’x f (y) Ã— g (y) = L Ã— K.
(18.8)
Proof: Suppose 18.6.
Then letting Îµ > 0 be given there exists Î´ > 0 such that if
0 < |y âˆ’x| < Î´, it follows
|fk (y) âˆ’Lk| â‰¤|f (y) âˆ’L| < Îµ
which veriï¬es 18.7.

318
LIMITS OF A FUNCTION 17-23 OCT.
Now suppose 18.7 holds.
Then letting Îµ > 0 be given, there exists Î´k such that if
0 < |y âˆ’x| < Î´k, then
|fk (y) âˆ’Lk| <
Îµ
âˆšp.
Let 0 < Î´ < min (Î´1, Â· Â· Â·, Î´p) . Then if 0 < |y âˆ’x| < Î´, it follows
|f (y) âˆ’L|
=
Ãƒ p
X
k=1
|fk (y) âˆ’Lk|2
!1/2
<
Ãƒ p
X
k=1
Îµ2
p
!1/2
= Îµ.
It remains to verify 18.8. But from the ï¬rst part of this theorem and the description of the
cross product presented earlier in terms of the permutation symbol,
lim
yâ†’x (f (y) Ã— g (y))i
=
lim
yâ†’x Îµijkfj (y) gk (y)
=
ÎµijkLjKk = (L Ã— K)i .
If you did not read about the permutation symbol, you can simply write out the cross
product and observe that the desired limit holds for each component. Therefore, from the
ï¬rst part of this theorem, this establishes 18.8. This completes the proof.
Example 18.0.9 Find lim(x,y)â†’(3,1)
Â³
x2âˆ’9
xâˆ’3 , y
Â´
.
It is clear that lim(x,y)â†’(3,1)
x2âˆ’9
xâˆ’3
= 6 and lim(x,y)â†’(3,1) y = 1. Therefore, this limit
equals (6, 1) .
Example 18.0.10 Find lim(x,y)â†’(0,0)
xy
x2+y2 .
First of all observe the domain of the function is R2 \ {(0, 0)} , every point in R2 except
the origin. Therefore, (0, 0) is a limit point of the domain of the function so it might make
sense to take a limit. However, just as in the case of a function of one variable, the limit
may not exist. In fact, this is the case here. To see this, take points on the line y = 0.
At these points, the value of the function equals 0. Now consider points on the line y = x
where the value of the function equals 1/2. Since arbitrarily close to (0, 0) there are points
where the function equals 1/2 and points where the function has the value 0, it follows there
can be no limit. Just take Îµ = 1/10 for example. You canâ€™t be within 1/10 of 1/2 and also
within 1/10 of 0 at the same time.
Note it is necessary to rely on the deï¬nition of the limit much more than in the case of
a function of one variable and there are no easy ways to do limit problems for functions of
more than one variable. It is what it is and you will not deal with these concepts without
suï¬€ering and anguish.
18.1
The Directional Derivative And Partial Deriva-
tives
18.1.1
The Directional Derivative
The directional derivative is just what its name suggests. It is the derivative of a function
in a particular direction. The following picture illustrates the situation in the case of a
function of two variables.

18.1.
THE DIRECTIONAL DERIVATIVE AND PARTIAL DERIVATIVES
319
Â©Â©Â©Â©Â©Â©Â©Â©
*
6
-
Â¡
Â¡
Â¡
Â¡
Âª
x
z
y
v
(x0, y0)
z = f(x, y)
In this picture, v â‰¡(v1, v2) is a unit vector in the xy plane and x0 â‰¡(x0, y0) is a point in
the xy plane. When (x, y) moves in the direction of v, this results in a change in z = f (x, y)
as shown in the picture. The directional derivative in this direction is deï¬ned as
lim
tâ†’0
f (x0 + tv1, y0 + tv2) âˆ’f (x0, y0)
t
.
It tells how fast z is changing in this direction.
If you looked at it from the side, you
would be getting the slope of the indicated tangent line. A simple example of this is a
person climbing a mountain. He could go various directions, some steeper than others. The
directional derivative is just a measure of the steepness in a given direction. This motivates
the following general deï¬nition of the directional derivative.
Deï¬nition 18.1.1 Let f : U â†’R where U is an open set in Rn and let v be a unit
vector. For x âˆˆU, deï¬ne the directional derivative of f in the direction, v, at the point
x as
Dvf (x) â‰¡lim
tâ†’0
f (x + tv) âˆ’f (x)
t
.
Example 18.1.2 Find the directional derivative of the function, f (x, y) = x2y in the di-
rection of i + j at the point (1, 2) .
First you need a unit vector which has the same direction as the given vector. This unit
vector is v â‰¡
Â³
1
âˆš
2,
1
âˆš
2
Â´
. Then to ï¬nd the directional derivative from the deï¬nition, write the
diï¬€erence quotient described above. Thus f (x + tv) =
Â³
1 +
t
âˆš
2
Â´2 Â³
2 +
t
âˆš
2
Â´
and f (x) = 2.
Therefore,
f (x + tv) âˆ’f (x)
t
=
Â³
1 +
t
âˆš
2
Â´2 Â³
2 +
t
âˆš
2
Â´
âˆ’2
t
,
and to ï¬nd the directional derivative, you take the limit of this as t â†’0. However, this
diï¬€erence quotient equals 1
4
âˆš
2
Â¡
10 + 4t
âˆš
2 + t2Â¢
and so, letting t â†’0,
Dvf (1, 2) =
Âµ5
2
âˆš
2
Â¶
.

320
LIMITS OF A FUNCTION 17-23 OCT.
There is something you must keep in mind about this. The direction vector must always
be a unit vector1.
18.1.2
Partial Derivatives
Quiz
1. Let r (t) =
Â¡
t2, cosh (t) + t, sin (t)
Â¢
. Find Îº when t = 0. Remember Îº is the curvature.
Also ï¬nd the normal and tangential components of acceleration and the oscullating
plane at the point where t = 0.
2. Suppose |r (t)| = 33 for all t. Show that râ€² (t) Â· r (t) = 0. Does it follow that râ€² (t) = 0?
3. Suppose r (t) =
Â¡
2t1/2, 2
3t3/2,
âˆš
2t
Â¢
for t âˆˆ[1, 2] . Find the length of this curve.
There are some special unit vectors which come to mind immediately. These are the
vectors, ei where
ei = (0, Â· Â· Â·, 0, 1, 0, Â· Â· Â·0)T
and the 1 is in the ith position.
Thus in case of a function of two variables, the directional derivative in the direction
i = e1 is the slope of the indicated straight line in the following picture.
y
z = f(x, y)
Â¡
Â¡
Â¡
Â¡
Â¡
x
s
Â¡
Â¡
Â¡
Âª
e1
As in the case of a general directional derivative, you ï¬x y and take the derivative of
the function, x â†’f(x, y). More generally, even in situations which cannot be drawn, the
deï¬nition of a partial derivative is as follows.
1Actually, there is a more general formulation of the notion of directional derivative known as the Gateaux
derivative in which the length of v is not one but it is not considered here.

18.1.
THE DIRECTIONAL DERIVATIVE AND PARTIAL DERIVATIVES
321
Deï¬nition 18.1.3 Let U be an open subset of Rn and let f : U â†’R. Then letting
x = (x1, Â· Â· Â·, xn)T be a typical element of Rn,
âˆ‚f
âˆ‚xi
(x) â‰¡Deif (x) .
This is called the partial derivative of f. Thus,
âˆ‚f
âˆ‚xi
(x)
â‰¡
lim
tâ†’0
f (x+tei) âˆ’f (x)
t
=
lim
tâ†’0
f (x1, Â· Â· Â·, xi + t, Â· Â· Â·xn) âˆ’f (x1, Â· Â· Â·, xi, Â· Â· Â·xn)
t
,
and to ï¬nd the partial derivative, diï¬€erentiate with respect to the variable of interest and
regard all the others as constants. Other notation for this partial derivative is fxi, f,i, or
Dif. If y = f (x) , the partial derivative of f with respect to xi may also be denoted by
âˆ‚y
âˆ‚xi
or yxi.
Example 18.1.4 Find âˆ‚f
âˆ‚x, âˆ‚f
âˆ‚y , and âˆ‚f
âˆ‚z if f (x, y) = y sin x + x2y + z.
From the deï¬nition above, âˆ‚f
âˆ‚x = y cos x+2xy, âˆ‚f
âˆ‚y = sin x+x2, and âˆ‚f
âˆ‚z = 1. Having taken
one partial derivative, there is no reason to stop doing it. Thus, one could take the partial
derivative with respect to y of the partial derivative with respect to x, denoted by
âˆ‚2f
âˆ‚yâˆ‚x or
fxy. In the above example,
âˆ‚2f
âˆ‚yâˆ‚x = fxy = cos x + 2x.
Also observe that
âˆ‚2f
âˆ‚xâˆ‚y = fyx = cos x + 2x.
Higher order partial derivatives are deï¬ned by analogy to the above. Thus in the above
example,
fyxx = âˆ’sin x + 2.
These partial derivatives, fxy are called mixed partial derivatives.
There is an interesting relationship between the directional derivatives and the partial
derivatives, provided the partial derivatives exist and are continuous.
Deï¬nition 18.1.5 Suppose f : U âŠ†Rn â†’R where U is an open set and the
partial derivatives of f all exist and are continuous on U. Under these conditions, deï¬ne the
gradient of f denoted âˆ‡f (x) to be the vector
âˆ‡f (x) = (fx1 (x) , fx2 (x) , Â· Â· Â·, fxn (x))T .
Proposition 18.1.6 In the situation of Deï¬nition 18.1.5 and for v a unit vector,
Dvf (x) = âˆ‡f (x) Â· v.
This proposition will be proved in a more general setting later. For now, you can use it
to compute directional derivatives.
Example 18.1.7 Find the directional derivative of the function, f (x, y) = sin
Â¡
2x2 + y3Â¢
at (1, 1) in the direction
Â³
1
âˆš
2,
1
âˆš
2
Â´T
.

322
LIMITS OF A FUNCTION 17-23 OCT.
First ï¬nd the gradient.
âˆ‡f (x, y) =
Â¡
4x cos
Â¡
2x2 + y3Â¢
, 3y2 cos
Â¡
2x2 + y3Â¢Â¢T .
Therefore,
âˆ‡f (1, 1) = (4 cos (3) , 3 cos (3))T
The directional derivative is therefore,
(4 cos (3) , 3 cos (3))T Â·
Âµ 1
âˆš
2, 1
âˆš
2
Â¶T
= 7
2 (cos 3)
âˆš
2.
Another important observation is that the gradient gives the direction in which the function
changes most rapidly.
Proposition 18.1.8 In the situation of Deï¬nition 18.1.5, suppose âˆ‡f (x) Ì¸= 0. Then
the direction in which f increases most rapidly, that is the direction in which the directional
derivative is largest, is the direction of the gradient.
Thus v = âˆ‡f (x) / |âˆ‡f (x)| is the
unit vector which maximizes Dvf (x) and this maximum value is |âˆ‡f (x)| . Similarly, v =
âˆ’âˆ‡f (x) / |âˆ‡f (x)| is the unit vector which minimizes Dvf (x) and this minimum value is
âˆ’|âˆ‡f (x)| .
Proof: Let v be any unit vector. Then from Proposition 18.1.6,
Dvf (x) = âˆ‡f (x) Â· v = |âˆ‡f (x)| |v| cos Î¸ = |âˆ‡f (x)| cos Î¸
where Î¸ is the included angle between these two vectors, âˆ‡f (x) and v. Therefore, Dvf (x)
is maximized when cos Î¸ = 1 and minimized when cos Î¸ = âˆ’1. The ï¬rst case corresonds to
the angle between the two vectors being 0 which requires they point in the same direction
in which case, it must be that v = âˆ‡f (x) / |âˆ‡f (x)| and Dvf (x) = |âˆ‡f (x)| . The second
case occurs when Î¸ is Ï€ and in this case the two vectors point in opposite directions and the
directional derivative equals âˆ’|âˆ‡f (x)| .
The concept of a directional derivative for a vector valued function is also easy
to deï¬ne although the geometric signiï¬cance expressed in pictures is not.
Deï¬nition 18.1.9 Let f : U â†’Rp where U is an open set in Rn and let v be a
unit vector. For x âˆˆU, deï¬ne the directional derivative of f in the direction, v, at the point
x as
Dvf (x) â‰¡lim
tâ†’0
f (x + tv) âˆ’f (x)
t
.
Example 18.1.10 Let f (x, y) =
Â¡
xy2, yx
Â¢T . Find the directional derivative in the direction
(1, 2)T at the point (x, y) .
First, a unit vector in this direction is
Â¡
1/
âˆš
5, 2/
âˆš
5
Â¢T and from the deï¬nition, the desired
limit is
lim
tâ†’0
Â³Â¡
x + t
Â¡
1/
âˆš
5
Â¢Â¢ Â¡
y + t
Â¡
2/
âˆš
5
Â¢Â¢2 âˆ’xy2,
Â¡
x + t
Â¡
1/
âˆš
5
Â¢Â¢ Â¡
y + t
Â¡
2/
âˆš
5
Â¢Â¢
âˆ’xy
Â´
t
=
lim
tâ†’0
Âµ4
5xy
âˆš
5 + 4
5xt + 1
5
âˆš
5y2 + 4
5ty + 4
25t2âˆš
5, 2
5x
âˆš
5 + 1
5y
âˆš
5 + 2
5t
Â¶
=
Âµ4
5xy
âˆš
5 + 1
5
âˆš
5y2, 2
5x
âˆš
5 + 1
5y
âˆš
5
Â¶
.

18.1.
THE DIRECTIONAL DERIVATIVE AND PARTIAL DERIVATIVES
323
You see from this example and the above deï¬nition that all you have to do is to form
the vector which is obtained by replacing each component of the vector with its directional
derivative. In particular, you can take partial derivatives of vector valued functions and use
the same notation.
Example 18.1.11 Find the partial derivative with respect to x of the function f (x, y, z, w) =
Â¡
xy2, z sin (xy) , z3x
Â¢T .
From the above deï¬nition, fx (x, y, z) = D1f (x, y, z) =
Â¡
y2, zy cos (xy) , z3Â¢T .
Example 18.1.12 Let f, g be two functions deï¬ned on an open subset of R3 which have
partial derivatives. Find a formula for âˆ‡(fg) .
This equals
Â³
(fg)x , (fg)y , (fg)z
Â´
=
(fxg + fgx, fyg + fgy, fzg + fgz)
=
g (fx, fy, fz) + f (gx, gy, gz) = gâˆ‡f + fâˆ‡g
Example 18.1.13 Let f, g be functions and a, b be scalars, you should verify that âˆ‡(af + bg) =
aâˆ‡f + bâˆ‡g.
Example 18.1.14 Let h (x, y) =
Â½
1 if x â‰¥0
0 if x < 0 . Find âˆ‚h
âˆ‚x and âˆ‚h
âˆ‚y .
If x > 0 or if x < 0, both partial derivatives exist and equal 0. What of points like (0, y)?
âˆ‚h
âˆ‚x (0, y) does not exist but
âˆ‚h
âˆ‚y (0, y) â‰¡lim
tâ†’0
h (0, y + t) âˆ’h (0, 0)
t
= lim
tâ†’0
1 âˆ’1
t
= 0.
Do not be afraid to use the deï¬nition of the partial derivatives. Sometimes it is the only
way to ï¬nd the partial derivative.
Example 18.1.15 Let u (x, y) = ln
Â¡
x2 + y2Â¢
. Find uxx + uyy.
First ï¬nd ux. This equals 2
x
x2+y2 . Next ï¬nd uxx. This involves taking the partial deriva-
tive of ux. Thus it equals
2 y2 âˆ’x2
(x2 + y2)2
Similarly uyy = 2
x2âˆ’y2
(x2+y2)2 and so uxx + uyy = 0. Of course this assumes (x, y) Ì¸= (0, 0).
18.1.3
Mixed Partial Derivatives
Under certain conditions the mixed partial derivatives will always be equal. The simple
condition is that if they exist and are continuous, then they are equal. This astonishing
fact is due to Euler in 1734. For reasons I cannot understand, calculus books hardly ever
include a proof of this important result. It is not all that hard. Here it is.
Theorem 18.1.16 Suppose f : U âŠ†R2 â†’R
where U is an open set on which
fx, fy, fxy and fyx exist. Then if fxy and fyx are continuous at the point (x, y) âˆˆU, it
follows
fxy (x, y) = fyx (x, y) .

324
LIMITS OF A FUNCTION 17-23 OCT.
Proof: Since U is open, there exists r > 0 such that B ((x, y) , r) âŠ†U. Now let |t| , |s| <
r/2 and consider
âˆ†(s, t) â‰¡1
st{
h(t)
z
}|
{
f (x + t, y + s) âˆ’f (x + t, y) âˆ’
h(0)
z
}|
{
(f (x, y + s) âˆ’f (x, y))}.
(18.9)
Note that (x + t, y + s) âˆˆU because
|(x + t, y + s) âˆ’(x, y)|
=
|(t, s)| =
Â¡
t2 + s2Â¢1/2
â‰¤
Âµr2
4 + r2
4
Â¶1/2
=
r
âˆš
2 < r.
As implied above, h (t) â‰¡f (x + t, y + s)âˆ’f (x + t, y). Therefore, by the mean value theorem
from calculus and the (one variable) chain rule,
âˆ†(s, t)
=
1
st (h (t) âˆ’h (0)) = 1
sthâ€² (Î±t) t
=
1
s (fx (x + Î±t, y + s) âˆ’fx (x + Î±t, y))
for some Î± âˆˆ(0, 1) . Applying the mean value theorem again,
âˆ†(s, t) = fxy (x + Î±t, y + Î²s)
where Î±, Î² âˆˆ(0, 1).
If the terms f (x + t, y) and f (x, y + s) are interchanged in 18.9, âˆ†(s, t) is also un-
changed and the above argument shows there exist Î³, Î´ âˆˆ(0, 1) such that
âˆ†(s, t) = fyx (x + Î³t, y + Î´s) .
Letting (s, t) â†’(0, 0) and using the continuity of fxy and fyx at (x, y) ,
lim
(s,t)â†’(0,0) âˆ†(s, t) = fxy (x, y) = fyx (x, y) .
This proves the theorem.
The following is obtained from the above by simply ï¬xing all the variables except for the
two of interest.
Corollary 18.1.17 Suppose U is an open subset of Rn and f : U â†’R has the property
that for two indices, k, l, fxk, fxl, fxlxk, and fxkxl exist on U and fxkxl and fxlxk are both
continuous at x âˆˆU. Then fxkxl (x) = fxlxk (x) .
It is necessary to assume the mixed partial derivatives are continuous in order to assert
they are equal. The following is a well known example [3].
Example 18.1.18 Let
f (x, y) =
(
xy(x2âˆ’y2)
x2+y2
if (x, y) Ì¸= (0, 0)
0 if (x, y) = (0, 0)

18.2.
SOME FUNDAMENTALSâˆ—
325
From the deï¬nition of partial derivatives it follows immediately that fx (0, 0) = fy (0, 0) =
0. Using the standard rules of diï¬€erentiation, for (x, y) Ì¸= (0, 0) ,
fx = y x4 âˆ’y4 + 4x2y2
(x2 + y2)2
, fy = xx4 âˆ’y4 âˆ’4x2y2
(x2 + y2)2
Now
fxy (0, 0)
â‰¡
lim
yâ†’0
fx (0, y) âˆ’fx (0, 0)
y
=
lim
yâ†’0
âˆ’y4
(y2)2 = âˆ’1
while
fyx (0, 0)
â‰¡
lim
xâ†’0
fy (x, 0) âˆ’fy (0, 0)
x
=
lim
xâ†’0
x4
(x2)2 = 1
showing that although the mixed partial derivatives do exist at (0, 0) , they are not equal
there.
18.2
Some Fundamentalsâˆ—
This section contains the proofs of the theorems which were stated without proof along with
some other signiï¬cant topics which will be useful later. These topics are of fundamental
signiï¬cance but are diï¬ƒcult. They are here to provide depth. If you want something more
than a superï¬cial knowledge, you should read this section. However, if you donâ€™t want to
deal with challenging topics, donâ€™t read this stuï¬€. Donâ€™t even look at it.
Theorem 18.2.1 The following assertions are valid
1. The function, af +bg is continuous at x when f, g are continuous at x âˆˆD (f)âˆ©D (g)
and a, b âˆˆR.
2. If and f and g are each real valued functions continuous at x, then fg is continuous
at x. If, in addition to this, g (x) Ì¸= 0, then f/g is continuous at x.
3. If f is continuous at x, f (x) âˆˆD (g) âŠ†Rp, and g is continuous at f (x) ,then g â—¦f is
continuous at x.
4. If f = (f1, Â· Â· Â·, fq) : D (f) â†’Rq, then f is continuous if and only if each fk is a
continuous real valued function.
5. The function f : Rp â†’R, given by f (x) = |x| is continuous.
Proof: Begin with 1.) Let Îµ > 0 be given. By assumption, there exist Î´1 > 0 such
that whenever |x âˆ’y| < Î´1, it follows |f (x) âˆ’f (y)| <
Îµ
2(|a|+|b|+1) and there exists Î´2 > 0
such that whenever |x âˆ’y| < Î´2, it follows that |g (x) âˆ’g (y)| <
Îµ
2(|a|+|b|+1). Then let
0 < Î´ â‰¤min (Î´1, Î´2) . If |x âˆ’y| < Î´, then everything happens at once. Therefore, using the
triangle inequality
|af (x) + bf (x) âˆ’(ag (y) + bg (y))|

326
LIMITS OF A FUNCTION 17-23 OCT.
â‰¤|a| |f (x) âˆ’f (y)| + |b| |g (x) âˆ’g (y)|
< |a|
Âµ
Îµ
2 (|a| + |b| + 1)
Â¶
+ |b|
Âµ
Îµ
2 (|a| + |b| + 1)
Â¶
< Îµ.
Now begin on 2.) There exists Î´1 > 0 such that if |y âˆ’x| < Î´1, then |f (x) âˆ’f (y)| < 1.
Therefore, for such y,
|f (y)| < 1 + |f (x)| .
It follows that for such y,
|fg (x) âˆ’fg (y)| â‰¤|f (x) g (x) âˆ’g (x) f (y)| + |g (x) f (y) âˆ’f (y) g (y)|
â‰¤|g (x)| |f (x) âˆ’f (y)| + |f (y)| |g (x) âˆ’g (y)|
â‰¤(1 + |g (x)| + |f (y)|) [|g (x) âˆ’g (y)| + |f (x) âˆ’f (y)|] .
Now let Îµ > 0 be given. There exists Î´2 such that if |x âˆ’y| < Î´2, then
|g (x) âˆ’g (y)| <
Îµ
2 (1 + |g (x)| + |f (y)|),
and there exists Î´3 such that if |x âˆ’y| < Î´3, then
|f (x) âˆ’f (y)| <
Îµ
2 (1 + |g (x)| + |f (y)|)
Now let 0 < Î´ â‰¤min (Î´1, Î´2, Î´3) . Then if |x âˆ’y| < Î´, all the above hold at once and
|fg (x) âˆ’fg (y)| â‰¤
(1 + |g (x)| + |f (y)|) [|g (x) âˆ’g (y)| + |f (x) âˆ’f (y)|]
< (1 + |g (x)| + |f (y)|)
Âµ
Îµ
2 (1 + |g (x)| + |f (y)|) +
Îµ
2 (1 + |g (x)| + |f (y)|)
Â¶
= Îµ.
This proves the ï¬rst part of 2.) To obtain the second part, let Î´1 be as described above and
let Î´0 > 0 be such that for |x âˆ’y| < Î´0,
|g (x) âˆ’g (y)| < |g (x)| /2
and so by the triangle inequality,
âˆ’|g (x)| /2 â‰¤|g (y)| âˆ’|g (x)| â‰¤|g (x)| /2
which implies |g (y)| â‰¥|g (x)| /2, and |g (y)| < 3 |g (x)| /2.
Then if |x âˆ’y| < min (Î´0, Î´1) ,
Â¯Â¯Â¯Â¯
f (x)
g (x) âˆ’f (y)
g (y)
Â¯Â¯Â¯Â¯ =
Â¯Â¯Â¯Â¯
f (x) g (y) âˆ’f (y) g (x)
g (x) g (y)
Â¯Â¯Â¯Â¯
â‰¤|f (x) g (y) âˆ’f (y) g (x)|
Â³
|g(x)|2
2
Â´
= 2 |f (x) g (y) âˆ’f (y) g (x)|
|g (x)|2

18.2.
SOME FUNDAMENTALSâˆ—
327
â‰¤
2
|g (x)|2 [|f (x) g (y) âˆ’f (y) g (y) + f (y) g (y) âˆ’f (y) g (x)|]
â‰¤
2
|g (x)|2 [|g (y)| |f (x) âˆ’f (y)| + |f (y)| |g (y) âˆ’g (x)|]
â‰¤
2
|g (x)|2
Â·3
2 |g (x)| |f (x) âˆ’f (y)| + (1 + |f (x)|) |g (y) âˆ’g (x)|
Â¸
â‰¤
2
|g (x)|2 (1 + 2 |f (x)| + 2 |g (x)|) [|f (x) âˆ’f (y)| + |g (y) âˆ’g (x)|]
â‰¡M [|f (x) âˆ’f (y)| + |g (y) âˆ’g (x)|]
where
M â‰¡
2
|g (x)|2 (1 + 2 |f (x)| + 2 |g (x)|)
Now let Î´2 be such that if |x âˆ’y| < Î´2, then
|f (x) âˆ’f (y)| < Îµ
2M âˆ’1
and let Î´3 be such that if |x âˆ’y| < Î´3, then
|g (y) âˆ’g (x)| < Îµ
2M âˆ’1.
Then if 0 < Î´ â‰¤min (Î´0, Î´1, Î´2, Î´3) , and |x âˆ’y| < Î´, everything holds and
Â¯Â¯Â¯Â¯
f (x)
g (x) âˆ’f (y)
g (y)
Â¯Â¯Â¯Â¯ â‰¤M [|f (x) âˆ’f (y)| + |g (y) âˆ’g (x)|]
< M
hÎµ
2M âˆ’1 + Îµ
2M âˆ’1i
= Îµ.
This completes the proof of the second part of 2.) Note that in these proofs no eï¬€ort is
made to ï¬nd some sort of â€œbestâ€ Î´. The problem is one which has a yes or a no answer.
Either is it or it is not continuous.
Now begin on 3.). If f is continuous at x, f (x) âˆˆD (g) âŠ†Rp, and g is continuous
at f (x) ,then g â—¦f is continuous at x. Let Îµ > 0 be given. Then there exists Î· > 0 such
that if |y âˆ’f (x)| < Î· and y âˆˆD (g) , it follows that |g (y) âˆ’g (f (x))| < Îµ. It follows from
continuity of f at x that there exists Î´ > 0 such that if |x âˆ’z| < Î´ and z âˆˆD (f) , then
|f (z) âˆ’f (x)| < Î·. Then if |x âˆ’z| < Î´ and z âˆˆD (g â—¦f) âŠ†D (f) , all the above hold and so
|g (f (z)) âˆ’g (f (x))| < Îµ.
This proves part 3.)
Part 4.) says: If f = (f1, Â· Â· Â·, fq) : D (f) â†’Rq, then f is continuous if and only if each
fk is a continuous real valued function. Then
|fk (x) âˆ’fk (y)| â‰¤|f (x) âˆ’f (y)|
â‰¡
Ãƒ q
X
i=1
|fi (x) âˆ’fi (y)|2
!1/2
â‰¤
q
X
i=1
|fi (x) âˆ’fi (y)| .
(18.10)

328
LIMITS OF A FUNCTION 17-23 OCT.
Suppose ï¬rst that f is continuous at x. Then there exists Î´ > 0 such that if |x âˆ’y| < Î´,
then |f (x) âˆ’f (y)| < Îµ. The ï¬rst part of the above inequality then shows that for each
k = 1, Â· Â· Â·, q, |fk (x) âˆ’fk (y)| < Îµ. This shows the only if part. Now suppose each function,
fk is continuous. Then if Îµ > 0 is given, there exists Î´k > 0 such that whenever |x âˆ’y| < Î´k
|fk (x) âˆ’fk (y)| < Îµ/q.
Now let 0 < Î´ â‰¤min (Î´1, Â· Â· Â·, Î´q) . For |x âˆ’y| < Î´, the above inequality holds for all k and
so the last part of 18.10 implies
|f (x) âˆ’f (y)| â‰¤
q
X
i=1
|fi (x) âˆ’fi (y)|
<
q
X
i=1
Îµ
q = Îµ.
This proves part 4.)
To verify part 5.), let Îµ > 0 be given and let Î´ = Îµ. Then if |x âˆ’y| < Î´, the triangle
inequality implies
|f (x) âˆ’f (y)| = ||x| âˆ’|y||
â‰¤|x âˆ’y| < Î´ = Îµ.
This proves part 5.) and completes the proof of the theorem.
18.2.1
The Nested Interval Lemmaâˆ—
Here is a multidimensional version of the nested interval lemma.
Lemma 18.2.2 Let Ik = Qp
i=1
Â£
ak
i , bk
i
Â¤
â‰¡
Â©
x âˆˆRp : xi âˆˆ
Â£
ak
i , bk
i
Â¤Âª
and suppose that for
all k = 1, 2, Â· Â· Â·,
Ik âŠ‡Ik+1.
Then there exists a point, c âˆˆRp which is an element of every Ik.
Proof: Since Ik âŠ‡Ik+1, it follows that for each i = 1, Â· Â· Â·, p ,
Â£
ak
i , bk
i
Â¤
âŠ‡
Â£
ak+1
i
, bk+1
i
Â¤
.
This implies that for each i,
ak
i â‰¤ak+1
i
, bk
i â‰¥bk+1
i
.
(18.11)
Consequently, if k â‰¤l,
al
i â‰¤bl
i â‰¤bk
i .
(18.12)
Now deï¬ne
ci â‰¡sup
Â©
al
i : l = 1, 2, Â· Â· Â·
Âª
By the ï¬rst inequality in 18.11,
ci = sup
Â©
al
i : l = k, k + 1, Â· Â· Â·
Âª
(18.13)
for each k = 1, 2 Â· Â· Â· . Therefore, picking any k,18.12 shows that bk
i is an upper bound for
the set,
Â©
al
i : l = k, k + 1, Â· Â· Â·
Âª
and so it is at least as large as the least upper bound of this
set which is the deï¬nition of ci given in 18.13. Thus, for each i and each k,
ak
i â‰¤ci â‰¤bk
i .
Deï¬ning c â‰¡(c1, Â· Â· Â·, cp) , c âˆˆIk for all k. This proves the lemma.
If you donâ€™t like the proof,you could prove the lemma for the one variable case ï¬rst and
then do the following.

18.2.
SOME FUNDAMENTALSâˆ—
329
Lemma 18.2.3 Let Ik = Qp
i=1
Â£
ak
i , bk
i
Â¤
â‰¡
Â©
x âˆˆRp : xi âˆˆ
Â£
ak
i , bk
i
Â¤Âª
and suppose that for
all k = 1, 2, Â· Â· Â·,
Ik âŠ‡Ik+1.
Then there exists a point, c âˆˆRp which is an element of every Ik.
Proof: For each i = 1, Â· Â· Â·, p,
Â£
ak
i , bk
i
Â¤
âŠ‡
Â£
ak+1
i
, bk+1
i
Â¤
and so by the nested interval
theorem for one dimensional problems, there exists a point ci âˆˆ
Â£
ak
i , bk
i
Â¤
for all k. Then
letting c â‰¡(c1, Â· Â· Â·, cp) it follows c âˆˆIk for all k. This proves the lemma.
18.2.2
The Extreme Value Theoremâˆ—
Deï¬nition 18.2.4 A set, C âŠ†Rp is said to be bounded if C âŠ†Qp
i=1 [ai, bi] for
some choice of intervals, [ai, bi] where âˆ’âˆ< ai < bi < âˆ. The diameter of a set, S, is
deï¬ned as
diam (S) â‰¡sup {|x âˆ’y| : x, y âˆˆS} .
A function, f having values in Rp is said to be bounded if the set of values of f is a bounded
set.
Thus diam (S) is just a careful description of what you would think of as the diameter.
It measures how stretched out the set is.
Lemma 18.2.5 Let C âŠ†Rp be closed and bounded and let f : C â†’R be continuous.
Then f is bounded.
Proof: Suppose not. Since C is bounded, it follows C âŠ†Qp
i=1 [ai, bi] â‰¡I0 for some
closed intervals, [ai, bi]. Consider all sets of the form Qp
i=1 [ci, di] where [ci, di] equals either
Â£
ai, ai+bi
2
Â¤
or [ci, di] =
Â£ ai+bi
2
, bi
Â¤
. Thus there are 2p of these sets because there are two
choices for the ith slot for i = 1, Â· Â· Â·, p. Also, if x and y are two points in one of these sets,
|xi âˆ’yi| â‰¤2âˆ’1 |bi âˆ’ai| .
Observe that diam (I0) =
Â³Pp
i=1 |bi âˆ’ai|2Â´1/2
because for x, y âˆˆI0, |xi âˆ’yi| â‰¤|ai âˆ’bi| for
each i = 1, Â· Â· Â·, p,
|x âˆ’y| =
Ãƒ p
X
i=1
|xi âˆ’yi|2
!1/2
â‰¤2âˆ’1
Ãƒ p
X
i=1
|bi âˆ’ai|2
!1/2
â‰¡2âˆ’1 diam (I0) .
Denote by {J1, Â· Â· Â·, J2p} these sets determined above. It follows the diameter of each set is
no larger than 2âˆ’1 diam (I0) . In particular, since d â‰¡(d1, Â· Â· Â·, dp) and c â‰¡(c1, Â· Â· Â·, cp) are
two such points, for each Jk,
diam (Jk) â‰¡
Ãƒ p
X
i=1
|di âˆ’ci|2
!1/2
â‰¤2âˆ’1 diam (I0)
Since the union of these sets equals all of I0, it follows
C = âˆª2p
k=1Jk âˆ©C.

330
LIMITS OF A FUNCTION 17-23 OCT.
If f is not bounded on C, it follows that for some k, f is not bounded on Jk âˆ©C. Let I1 â‰¡
Jk and let C1 = C âˆ©I1. Now do to I1 and C1 what was done to I0 and C to obtain I2 âŠ†I1,
and for x, y âˆˆI2,
|x âˆ’y| â‰¤2âˆ’1 diam (I1) â‰¤2âˆ’2 diam (I2) ,
and f is unbounded on I2 âˆ©C1 â‰¡C2. Continue in this way obtaining sets, Ik such that
Ik âŠ‡Ik+1 and diam (Ik) â‰¤2âˆ’k diam (I0) and f is unbounded on Ik âˆ©C. By the nested
interval lemma, there exists a point, c which is contained in each Ik.
Claim: c âˆˆC.
Proof of claim: Suppose c /âˆˆC. Since C is a closed set, there exists r > 0 such that
B (c, r) is contained completely in Rp \ C. In other words, B (c, r) contains no points of C.
Let k be so large that diam (I0) 2âˆ’k < r. Then since c âˆˆIk, and any two points of Ik are
closer than diam (I0) 2âˆ’k, Ik must be contained in B (c, r) and so has no points of C in it,
contrary to the manner in which the Ik are deï¬ned in which f is unbounded on Ik âˆ©C.
Therefore, c âˆˆC as claimed.
Now for k large enough, and x âˆˆC âˆ©Ik, the continuity of f implies |f (c) âˆ’f (x)| < 1
contradicting the manner in which Ik was chosen since this inequality implies f is bounded
on Ik âˆ©C. This proves the theorem.
Here is a proof of the extreme value theorem.
Theorem 18.2.6 Let C be closed and bounded and let f : C â†’R be continuous.
Then f achieves its maximum and its minimum on C. This means there exist, x1, x2 âˆˆC
such that for all x âˆˆC,
f (x1) â‰¤f (x) â‰¤f (x2) .
Proof: Let M = sup {f (x) : x âˆˆC} . Then by Lemma 18.2.5, M is a ï¬nite number. Is
f (x2) = M for some x2? if not, you could consider the function,
g (x) â‰¡
1
M âˆ’f (x)
and g would be a continuous and unbounded function deï¬ned on C, contrary to Lemma
18.2.5. Therefore, there exists x2 âˆˆC such that f (x2) = M. A similar argument applies to
show the existence of x1 âˆˆC such that
f (x1) = inf {f (x) : x âˆˆC} .
This proves the theorem.
18.2.3
Sequences And Completenessâˆ—
Deï¬nition 18.2.7 A function whose domain is deï¬ned as a set of the form
{k, k + 1, k + 2, Â· Â· Â·}
for k an integer is known as a sequence. Thus you can consider f (k) , f (k + 1) , f (k + 2) ,
etc.
Usually the domain of the sequence is either N, the natural numbers consisting of
{1, 2, 3, Â· Â· Â·} or the nonnegative integers, {0, 1, 2, 3, Â· Â· Â·} . Also, it is traditional to write f1, f2,
etc. instead of f (1) , f (2) , f (3) etc. when referring to sequences. In the above context, fk is
called the ï¬rst term, fk+1 the second and so forth. It is also common to write the sequence,
not as f but as {fi}âˆ
i=k or just {fi} for short. The letter used for the name of the sequence
is not important. Thus it is all right to let a be the name of a sequence or to refer to it as
{ai} . When the sequence has values in Rp, it is customary to write it in bold face. Thus
{ai} would refer to a sequence having values in Rp for some p > 1.

18.2.
SOME FUNDAMENTALSâˆ—
331
Example 18.2.8 Let {ak}âˆ
k=1 be deï¬ned by ak â‰¡k2 + 1.
This gives a sequence. In fact, a7 = a (7) = 72 + 1 = 50 just from using the formula for
the kth term of the sequence.
It is nice when sequences come in this way from a formula for the kth term. However,
this is often not the case. Sometimes sequences are deï¬ned recursively. This happens, when
the ï¬rst several terms of the sequence are given and then a rule is speciï¬ed which determines
an+1 from knowledge of a1, Â· Â· Â·, an. This rule which speciï¬es an+1 from knowledge of ak for
k â‰¤n is known as a recurrence relation.
Example 18.2.9 Let a1 = 1 and a2 = 1. Assuming a1, Â· Â· Â·, an+1 are known, an+2 â‰¡
an + an+1.
Thus the ï¬rst several terms of this sequence, listed in order, are 1, 1, 2, 3, 5, 8,Â· Â· Â·.
This particular sequence is called the Fibonacci sequence and is important in the study of
reproducing rabbits.
Example 18.2.10 Let ak = (k, sin (k)) . Thus this sequence has values in R2.
Deï¬nition 18.2.11 Let {an} be a sequence and let n1 < n2 < n3, Â·Â·Â· be any strictly
increasing list of integers such that n1 is at least as large as the ï¬rst index used to deï¬ne
the sequence {an} . Then if bk â‰¡ank, {bk} is called a subsequence of {an} .
For example, suppose an =
Â¡
n2 + 1
Â¢
. Thus a1 = 2, a3 = 10, etc. If
n1 = 1, n2 = 3, n3 = 5, Â· Â· Â·, nk = 2k âˆ’1,
then letting bk = ank, it follows
bk =
Â³
(2k âˆ’1)2 + 1
Â´
= 4k2 âˆ’4k + 2.
Deï¬nition 18.2.12 A sequence, {ak} is said to converge to a if for every Îµ >
0 there exists nÎµ such that if n > nÎµ, then |a âˆ’aÎµ| < Îµ. The usual notation for this is
limnâ†’âˆan = a although it is often written as an â†’a.
The following theorem says the limit, if it exists, is unique.
Theorem 18.2.13 If a sequence, {an} converges to a and to b then a = b.
Proof: There exists nÎµ such that if n > nÎµ then |an âˆ’a| <
Îµ
2 and if n > nÎµ, then
|an âˆ’b| < Îµ
2. Then pick such an n.
|a âˆ’b| < |a âˆ’an| + |an âˆ’b| < Îµ
2 + Îµ
2 = Îµ.
Since Îµ is arbitrary, this proves the theorem.
The following is the deï¬nition of a Cauchy sequencein Rp.
Deï¬nition 18.2.14 {an} is a Cauchy sequence if for all Îµ > 0, there exists nÎµ such
that whenever n, m â‰¥nÎµ,
|anâˆ’am| < Îµ.
A sequence is Cauchy means the terms are â€œbunching up to each otherâ€ as m, n get
large.

332
LIMITS OF A FUNCTION 17-23 OCT.
Theorem 18.2.15 The set of terms in a Cauchy sequence in Rp is bounded in the
sense that for all n, |an| < M for some M < âˆ.
Proof: Let Îµ = 1 in the deï¬nition of a Cauchy sequence and let n > n1. Then from the
deï¬nition,
|anâˆ’an1| < 1.
It follows that for all n > n1,
|an| < 1 + |an1| .
Therefore, for all n,
|an| â‰¤1 + |an1| +
n1
X
k=1
|ak| .
This proves the theorem.
Theorem 18.2.16 If a sequence {an} in Rp converges, then the sequence is a
Cauchy sequence.
Also, if some subsequence of a Cauchy sequence converges, then the
original sequence converges.
Proof: Let Îµ > 0 be given and suppose anâ†’a. Then from the deï¬nition of convergence,
there exists nÎµ such that if n > nÎµ, it follows that
|anâˆ’a| < Îµ
2
Therefore, if m, n â‰¥nÎµ + 1, it follows that
|anâˆ’am| â‰¤|anâˆ’a| + |a âˆ’am| < Îµ
2 + Îµ
2 = Îµ
showing that, since Îµ > 0 is arbitrary, {an} is a Cauchy sequence. It remains to show the last
claim. Suppose then that {an} is a Cauchy sequence and a = limkâ†’âˆank where {ank}âˆ
k=1
is a subsequence. Let Îµ > 0 be given. Then there exists K such that if k, l â‰¥K, then
|ak âˆ’al| < Îµ
2. Then if k > K, it follows nk > K because n1, n2, n3, Â· Â· Â· is strictly increasing
as the subscript increases. Also, there exists K1 such that if k > K1, |ank âˆ’a| < Îµ
2. Then
letting n > max (K, K1) , pick k > max (K, K1) . Then
|a âˆ’an| â‰¤|a âˆ’ank| + |ank âˆ’an| < Îµ
2 + Îµ
2 = Îµ.
This proves the theorem.
Deï¬nition 18.2.17 A set, K in Rp is said to be sequentially compact if every
sequence in K has a subsequence which converges to a point of K.
Theorem 18.2.18 If I0 = Qp
i=1 [ai, bi] where ai â‰¤bi, then I0 is sequentially
compact.
Proof: Let {ai}âˆ
i=1 âŠ†I0 and consider all sets of the form Qp
i=1 [ci, di] where [ci, di]
equals either
Â£
ai, ai+bi
2
Â¤
or [ci, di] =
Â£ ai+bi
2
, bi
Â¤
. Thus there are 2p of these sets because
there are two choices for the ith slot for i = 1, Â· Â· Â·, p. Also, if x and y are two points in one
of these sets,
|xi âˆ’yi| â‰¤2âˆ’1 |bi âˆ’ai| .

18.2.
SOME FUNDAMENTALSâˆ—
333
diam (I0) =
Â³Pp
i=1 |bi âˆ’ai|2Â´1/2
,
|x âˆ’y| =
Ãƒ p
X
i=1
|xi âˆ’yi|2
!1/2
â‰¤2âˆ’1
Ãƒ p
X
i=1
|bi âˆ’ai|2
!1/2
â‰¡2âˆ’1 diam (I0) .
In particular, since d â‰¡(d1, Â· Â· Â·, dp) and c â‰¡(c1, Â· Â· Â·, cp) are two such points,
D1 â‰¡
Ãƒ p
X
i=1
|di âˆ’ci|2
!1/2
â‰¤2âˆ’1 diam (I0)
Denote by {J1, Â· Â· Â·, J2p} these sets determined above. Since the union of these sets equals
all of I0 â‰¡I, it follows that for some Jk, the sequence, {ai} is contained in Jk for inï¬nitely
many k. Let that one be called I1. Next do for I1 what was done for I0 to get I2 âŠ†I1 such
that the diameter is half that of I1 and I2 contains {ak} for inï¬nitely many values of k.
Continue in this way obtaining a nested sequence of intervals, {Ik} such that Ik âŠ‡Ik+1, and
if x, y âˆˆIk, then |x âˆ’y| â‰¤2âˆ’k diam (I0) , and In contains {ak} for inï¬nitely many values
of k for each n. Then by the nested interval lemma, there exists c such that c is contained
in each Ik. Pick an1 âˆˆI1. Next pick n2 > n1 such that an2 âˆˆI2. If an1, Â· Â· Â·, ank have been
chosen, let ank+1 âˆˆIk+1 and nk+1 > nk. This can be done because in the construction, In
contains {ak} for inï¬nitely many k. Thus the distance between ank and c is no larger than
2âˆ’k diam (I0) and so limkâ†’âˆank = c âˆˆI0. This proves the theorem.
Theorem 18.2.19 Every Cauchy sequence in Rp converges.
Proof: Let {ak} be a Cauchy sequence. By Theorem 18.2.15 there is some interval,
Qp
i=1 [ai, bi] containing all the terms of {ak} . Therefore, by Theorem 18.2.18 a subsequence
converges to a point of this interval. By Theorem 18.2.16 the original sequence converges.
This proves the theorem.
18.2.4
Continuity And The Limit Of A Sequenceâˆ—
Just as in the case of a function of one variable, there is a very useful way of thinking of
continuity in terms of limits of sequences found in the following theorem. In words, it says
a function is continuous if it takes convergent sequences to convergent sequences whenever
possible.
Theorem 18.2.20 A function f : D (f) â†’Rq is continuous at x âˆˆD (f) if and
only if, whenever xnâ†’x with xn âˆˆD (f) , it follows f (xn) â†’f (x) .
Proof: Suppose ï¬rst that f is continuous at x and let xnâ†’x. Let Îµ > 0 be given. By
continuity, there exists Î´ > 0 such that if |y âˆ’x| < Î´, then |f (x) âˆ’f (y)| < Îµ. However,
there exists nÎ´ such that if n â‰¥nÎ´, then |xnâˆ’x| < Î´ and so for all n this large,
|f (x) âˆ’f (xn)| < Îµ
which shows f (xn) â†’f (x) .
Now suppose the condition about taking convergent sequences to convergent sequences
holds at x. Suppose f fails to be continuous at x. Then there exists Îµ > 0 and xn âˆˆD (f)
such that |x âˆ’xn| < 1
n , yet
|f (x) âˆ’f (xn)| â‰¥Îµ.

334
LIMITS OF A FUNCTION 17-23 OCT.
But this is clearly a contradiction because, although xnâ†’x, f (xn) fails to converge to f (x) .
It follows f must be continuous after all. This proves the theorem.

Part VIII
Diï¬€erentiability
335


337
Outcomes
Diï¬€erentiability and the Chain Rule
A. Deï¬ne diï¬€erentiability for a function of several variables.
B. Evaluate partial derivatives from the deï¬nition. Describe the relationship between the
derivative of a multivariable function and its partial derivatives.
C. Describe the relationship between the existence of partial derivatives and the existence
of a derivative for a function of several variables.
D. Apply the chain rule to evaluate derivatives.
E. Solve related rates problems using the chain rule.
Reading: Multivariable Calculus 2.4
Outcome Mapping:
A. H1
B. H1,1
C. G2
D. 3,6
E. 4,7,8
Directional Derivatives
A. Give a graphical interpretation of the gradient.
B. Evaluate the directional derivative of a function.
C. Give a graphical interpretation of directional derivative.
D. Prove that a diï¬€erential function f increases most rapidly in the direction of the
gradient (the rate of change is then âˆ¥f(âƒ—x)âˆ¥) and it decreases most rapidly in the
opposite direction (the rate of change is then âˆ’âˆ¥f(âƒ—x)âˆ¥).
E. Find the path of a heat seeking or a heat repelling particle.
Reading: Multivariable Calculus 2.5
Outcome Mapping:
A. 1,11,H2
B. 4,6
C. 2,3,H3
D. H4
E. 5,7,8
Normal Vectors and Tangent Planes
A. Interpret the gradient of a function as a normal to a level curve or a level surface.

338
B. Find the normal line and tangent plane to a smooth surface at a given point.
C. Find the angles between curves and surfaces.
Reading: Multivariable Calculus 2.6
Outcome Mapping:
A. 1,3,4
B. 9,11
C. 14,15,16,17
Extrema of Functions of Several Variables
A. Identify local extreme values graphically.
B. Determine the local extreme values and saddle points of a function of two variables.
When possible, apply the second partial derivatives test.
C. Identify the extreme values of a function deï¬ned on a closed and bounded region.
D. Solve word problems involving maximum and minimum values.
Reading: Multivariable Calculus 2.7
Outcome Mapping:
A. 1,2
B. 3
C. 4
D. 6,9,12
Constrained Extrema
A. Graphically interpret the method of Lagrange.
B. Determine the extreme values of a function subject to side constraints by applying the
method of Lagrange.
C. Apply the method of Lagrange to solve word problems.
Reading: Multivariable Calculus 2.9
Outcome Mapping:
A. 1,2
B. 3
C. 4,8,14

Diï¬€erentiability 24-26 Oct.
19.1
The Deï¬nition Of Diï¬€erentiability
Quiz
1. Let f (x, y) = x2y + sin (xy) . Find âˆ‡f (x, y) .
2. Let f (x, y) = x2y + sin (xy) . Find Dvf (1, 1) where v is in the direction of (1, 2) .
3. Let f (x, y) = x2y + sin (xy) . Find the largest value of Dvf (1, 2) for all v. That is,
ï¬nd the largest directional derivative of this function.
First remember what it means for a function of one variable to be diï¬€erentiable.
f â€² (x) â‰¡lim
hâ†’0
f (x + h) âˆ’f (x)
h
Another way to say this is contained in the following observation.
Observation 19.1.1 Suppose a function, f of one variable has a derivative at x. Then
lim
hâ†’0
|f (x + h) âˆ’f (x) âˆ’f â€² (x) h|
|h|
= 0.
For a function of n variables, there is a similar deï¬nition of what it means for a function
to be diï¬€erentiable.
Deï¬nition 19.1.2 Let U be an open set in Rn and suppose f : U â†’R is a function.
Then f is diï¬€erentiable at x âˆˆU if for v = (v1, Â· Â· Â·, vn)
lim
|v|â†’0
Â¯Â¯Â¯f (x + v) âˆ’f (x) âˆ’Pn
k=1
âˆ‚f
âˆ‚xk (x) vk
Â¯Â¯Â¯
|v|
= 0.
Deï¬nition 19.1.3 A function of a vector, v is called o (v) if
lim
|v|â†’0
o (v)
|v|
= 0.
(19.1)
Thus the function f (x + h) âˆ’f (x) âˆ’f â€² (x) h is o (h) . When we say a function is o (h) ,
it is used like an adjective. It is like saying the function is white or black or green or fat or
thin. The term is used very imprecisely. Thus
o (v) = o (v) + o (v) , o (v) = 45o (v) , o (v) = o (v) âˆ’o (v) , etc.
339

340
DIFFERENTIABILITY 24-26 OCT.
When you add two functions with the property of the above deï¬nition, you get another one
having that same property. When you multiply by 45 the property is also retained as it
is when you subtract two such functions. How could something so sloppy be useful? The
notation is useful precisely because it prevents obsession over things which are not relevant
and should be ignored.
Deï¬nition 19.1.2 is then equivalent to the following very simple statement.
Deï¬nition 19.1.4 Let U be an open set in Rn and suppose f : U â†’R is a function.
Then f is diï¬€erentiable at x âˆˆU if
f (x + v) âˆ’f (x) âˆ’
n
X
k=1
âˆ‚f
âˆ‚xk
(x) vk = o (v) .
The ï¬rst deï¬nition says nothing more than f (x + v) âˆ’f (x) âˆ’Pn
k=1
âˆ‚f
âˆ‚xk (x) vk = o (v)
because it says
lim
|v|â†’0
Â¯Â¯Â¯f (x + v) âˆ’f (x) âˆ’Pn
k=1
âˆ‚f
âˆ‚xk (x) vk
Â¯Â¯Â¯
|v|
= 0.
The following is fundamental.
Proposition 19.1.5 If f is diï¬€erentiable at x, then f is continuous at x.
Proof: From the deï¬nition of diï¬€erentiability,
|f (x + v) âˆ’f (x)| â‰¤
Â¯Â¯Â¯Â¯Â¯
n
X
k=1
âˆ‚f
âˆ‚xk
(x) vk + o (v)
Â¯Â¯Â¯Â¯Â¯
Let Îµ > 0 be given. Then clearly if |v| is suï¬ƒciently small, the right side of the above is less
than Îµ. Thus the function is continuous at x.
So which functions are diï¬€erentiable? Are there simple ways to look at a function and
say that it is clearly diï¬€erentiable? Existence of partial derivatives is needed in order to even
write the above expression but it turns out this is not enough. Here is a simple example.
Example 19.1.6 Let
f (x, y) =
Â½
xy
x2+y2 if (x, y) Ì¸= (0, 0)
0 if (x, y) = (0, 0)
Then
fx (0, 0) â‰¡lim
hâ†’0
f (h, 0) âˆ’f (0, 0)
h
= lim
hâ†’0
0
h = 0
Also
fy (0, 0) â‰¡lim
hâ†’0
f (0, h) âˆ’f (0, 0)
h
= lim
hâ†’0
0
h = 0
so both partial derivatives exist. However, the function is not even continuous at (0, 0) . This
is because it equals zero on the entire y axis but along the line, y = x the function equals
1/2. By Proposition 19.1.5 it cannot be diï¬€erentiable.

19.2.
C1 FUNCTIONS AND DIFFERENTIABILITY
341
19.2
C1 Functions And Diï¬€erentiability
It turns out that if the partial derivatives are continuous then the function is diï¬€erentiable.
I will show this next. First, remember the Cauchy Schwarz inequality, which I will list here
for convenience.
Â¯Â¯Â¯Â¯Â¯
n
X
i=1
aibi
Â¯Â¯Â¯Â¯Â¯ â‰¤
Ãƒ n
X
i=1
a2
i
!1/2 Ãƒ n
X
i=1
b2
i
!1/2
.
Theorem 19.2.1 Suppose f : U â†’R where U is an open set. Suppose also that
all partial derivatives of f exist on U and are continuous. Then f is diï¬€erentiable at every
point of U.
Proof: If you ï¬x all the variables but one, you can apply the fundamental theorem of
calculus as follows.
f (x+vkek) âˆ’f (x) =
Z 1
0
âˆ‚f
âˆ‚xk
(x + tvkek) vkdt.
(19.2)
Here is why. Let h (t) = f (x + tvkek) . Then
h (t + âˆ†t) âˆ’h (t)
âˆ†t
= f (x + tvkek + âˆ†tvkek) âˆ’f (x + tvkek)
âˆ†tvk
vk
and so, taking the limit as âˆ†t â†’0 yields
hâ€² (t) = âˆ‚f
âˆ‚xk
(x + tvkek) vk
Therefore,
f (x+vkek) âˆ’f (x) = h (1) âˆ’h (0) =
Z 1
0
hâ€² (t) dt =
Z 1
0
âˆ‚f
âˆ‚xk
(x + tvkek) vkdt.
Now I will use this observation to prove the theorem. Let v = (v1, Â· Â· Â·, vn) with |v|
suï¬ƒciently small. Thus v = Pn
k=1 vkek. For the purposes of this argument, deï¬ne
n
X
k=n+1
vkek â‰¡0.

342
DIFFERENTIABILITY 24-26 OCT.
Then with this convention, and using 19.2,
f (x + v) âˆ’f (x)
=
n
X
i=1
Ãƒ
f
Ãƒ
x+
n
X
k=i
vkek
!
âˆ’f
Ãƒ
x+
n
X
k=i+1
vkek
!!
=
n
X
i=1
Z 1
0
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
vidt
=
n
X
i=1
Z 1
0
Ãƒ
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
vi âˆ’âˆ‚f
âˆ‚xi
(x) vi
!
dt
+
n
X
i=1
Z 1
0
âˆ‚f
âˆ‚xi
(x) vidt
=
n
X
i=1
âˆ‚f
âˆ‚xi
(x) vi +
Z 1
0
n
X
i=1
Ãƒ
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
âˆ’âˆ‚f
âˆ‚xi
(x)
!
vidt
=
n
X
i=1
âˆ‚f
âˆ‚xi
(x) vi + o (v)
and this shows f is diï¬€erentiable at x because it satisï¬es the conditions of Deï¬nition 19.1.4.
Some explanation of the step to the last line is in order. The messy thing at the end is
o (v) because of the continuity of the partial derivatives. In fact, from the Cauchy Schwarz
inequality,
Z 1
0
n
X
i=1
Ãƒ
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
âˆ’âˆ‚f
âˆ‚xi
(x)
!
vidt
â‰¤
Z 1
0
ï£«
ï£­
n
X
i=1
Â¯Â¯Â¯Â¯Â¯
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
âˆ’âˆ‚f
âˆ‚xi
(x)
Â¯Â¯Â¯Â¯Â¯
2ï£¶
ï£¸
1/2
dt
Ãƒ n
X
i=1
v2
i
!1/2
=
Z 1
0
ï£«
ï£­
n
X
i=1
Â¯Â¯Â¯Â¯Â¯
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
âˆ’âˆ‚f
âˆ‚xi
(x)
Â¯Â¯Â¯Â¯Â¯
2ï£¶
ï£¸
1/2
dt |v|
Thus, dividing by |v| and taking a limit as |v| â†’0, the quotient is nothing but
Z 1
0
ï£«
ï£­
n
X
i=1
Â¯Â¯Â¯Â¯Â¯
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
âˆ’âˆ‚f
âˆ‚xi
(x)
Â¯Â¯Â¯Â¯Â¯
2ï£¶
ï£¸
1/2
dt
which converges to 0 due to continuity of the partial derivatives of f.
This proves the
theorem.
To help you keep the various terms straight, here is a pretty diagram.

19.3.
THE DIRECTIONAL DERIVATIVE
343
Continuous
|x| + |y|
Partial derivatives
xy
x2+y2
derivative
C1
You might ask whether there are examples of functions which are diï¬€erentiable but not
C1. Of course there are. There are easy examples of this even for functions of one variable.
Here is one.
f (x) =
Â½
x2 sin 1
x if x Ì¸= 0
0 if x = 0
.
You should show that f â€² (0) = 0 but the derivative is 2x sin 1
x âˆ’cos 1
x for x Ì¸= 0 and this
function fails to even have a limit as x â†’0. This is a great test question. You ask for f â€² (0)
and it is really easy if you use the deï¬nition. However, people usually ï¬nd f â€² (x) and then
try to plug in x = 0. This is doomed to failure and makes the question very easy to grade.
19.3
The Directional Derivative
Here I will prove the formula for the directional derivative presented earlier. Recall that for
v a unit vector, (|v| = 1)
Dv (f) (x) â‰¡lim
tâ†’0
f (x+tv) âˆ’f (x)
t
.
Theorem 19.3.1 Suppose f is diï¬€erentiable at x. Then Dv (f) (x) exists and is
given by
Dv (f) (x) = âˆ‡f (x) Â· v
Proof: By diï¬€erentiability of f at x,
f (x+tv) âˆ’f (x)
t
=
1
t
Ãƒ n
X
k=1
âˆ‚f
âˆ‚xk
(x) tvk + o (tv)
!
=
n
X
k=1
âˆ‚f
âˆ‚xk
(x) vk + o (tv)
|tv|
Taking the limit as t â†’0,
Dvf (x) =
n
X
k=1
âˆ‚f
âˆ‚xk
(x) vk â‰¡âˆ‡f (x) Â· v.
This proves the theorem.
What is the direction in which the largest directional derivative results? You want to
maximize âˆ‡f (x)Â·v = |âˆ‡f (x)| |v| cos Î¸ where Î¸ is the included angle between v and âˆ‡f (x) .

344
DIFFERENTIABILITY 24-26 OCT.
Clearly this occurs when Î¸ = 0. Therefore, the largest value of the directional derivative is
when v = âˆ‡f (x) / |âˆ‡f (x)| . The value of the directional derivative in this direction, is
âˆ‡f (x) Â· âˆ‡f (x) / |âˆ‡f (x)| = |âˆ‡f (x)| .
Similarly, the smallest value for the directional derivative occurs when v = âˆ’âˆ‡f (x) / |âˆ‡f (x)|
because this corresponds to Î¸ = Ï€ and cos Î¸ = âˆ’1. The smallest value of the directional
derivative is then âˆ’|âˆ‡f (x)| .
19.3.1
Separable Diï¬€erential Equationsâˆ—
If you do not know how to solve simple diï¬€erential equations, read this section. Otherwise
skip it.
Diï¬€erential equations are just equations which involve an unknown function and some
of its derivatives. For example, a diï¬€erential equation is
yâ€² = 1 + y2.
(19.3)
You might check and see that a solution to this diï¬€erential equation is y = tan x.
Here is another easier diï¬€erential equation.
yâ€² = x2.
A solution to this one is of the form
y = x3
3 + C
where C is any constant. In general, you are familiar with diï¬€erential equations of the form
yâ€² = f (x) .
The problem is just to ï¬nd an antiderivative of the given function. However, equations like
the one in 19.3 are not so obvious. It turns out there are many recipes for ï¬nding solutions
to diï¬€erential equations of various sorts. One of the easiest kinds of diï¬€erential equations
to solve are those which are separable.
Separable diï¬€erential equations are those which can be written in the form
dy
dx = f (x)
g (y) .
The equation in 19.3 is an example of a separable diï¬€erential equation. Just let f (x) = 1
and g (y) =
1
1+y2 .
The reason these are called separable is that if you formally cross multiply,
g (y) dy = f (x) dx
and the variables are â€œseparatedâ€. Here is how you solve these. Find Gâ€² (y) = g (y) and
F â€² (x) = f (x) . That is, pick G âˆˆ
R
g (y) dy and F âˆˆ
R
f (x) dx. Suppose F (x) âˆ’G (y) = c
speciï¬es y as a diï¬€erentiable function of x, then x â†’y (x) solves the separable diï¬€erential
equation because by the chain rule,
F â€² (x) âˆ’Gâ€² (y) yâ€² = f (x) âˆ’g (y) yâ€²
and so
f (x) = g (y) yâ€²

19.3.
THE DIRECTIONAL DERIVATIVE
345
so
yâ€² = f (x)
g (y) .
This is why the solutions are given in the form
F (x) âˆ’G (y) = c
where c is a constant, or equivalently
G (y) âˆ’F (x) = c
where c is a constant.
Example 19.3.2 Find the solutions to the diï¬€erential equation,
yâ€² = x2
y
which satisï¬es the initial condition, y (0) = 1. Since there is a diï¬€erential equation along
with an initial condition, this is called an initial value problem.
To solve this you separate the variables and write
ydy = x2dx
and then from the above discussion,
y2
2 âˆ’x3
3 = C.
(19.4)
You want y = 1 when x = 0 and so you must have
1
2 = C.
The solution is
y =
s
2
Âµx3
3 + 1
2
Â¶
where it was necessary to pick the positive square root because otherwise, you would not
have y (0) = 1.
Sometimes you canâ€™t solve for y in terms of x.
Example 19.3.3 Find the solutions to the diï¬€erential equation,
yâ€² =
x2
y sin y .
In this case,
(y sin y) dy = x2
and so
R
y sin y = sin y âˆ’y cos y
sin y âˆ’y cos y âˆ’x3
3 = C
gives the solutions. I would not like to try and solve this for y in terms of x. Therefore, in
this case, it is customary to leave the solution in this form and refer to it as an implicitly
deï¬ned solution. The point is, the above equation does deï¬ne y as a function of x near
typical points on the level curve but it might not be possible to algebraically ï¬nd y as a
function of x. You notice in the above argument for ï¬nding solutions, it was never assumed
that you could algebraically ï¬nd y as a function of x in F (x) âˆ’G (y) = C.
Here is an interesting example which is non trivial.

346
DIFFERENTIABILITY 24-26 OCT.
Example 19.3.4 What is the equation of a hanging chain?
Consider the following picture of a portion of this chain.






q
q
T0
T(x)
-
6
Î¸
T(x) cos Î¸
T(x) sin Î¸
?
Ïl(x)g
In this picture, Ï denotes the density of the chain which is assumed to be constant and
g is the acceleration due to gravity. T (x) and T0 represent the magnitude of the tension in
the chain at x and at 0 respectively, as shown. Let the bottom of the chain be at the origin
as shown. If this chain does not move, then all these forces acting on it must balance. In
particular,
T (x) sin Î¸ = l (x) Ïg, T (x) cos Î¸ = T0.
Therefore, dividing these yields
sin Î¸
cos Î¸ = l (x)
â‰¡c
z }| {
Ïg/T0.
Now letting y (x) denote the y coordinate of the hanging chain corresponding to x,
sin Î¸
cos Î¸ = tan Î¸ = yâ€² (x) .
Therefore, this yields
yâ€² (x) = cl (x) .
Now diï¬€erentiating both sides of the diï¬€erential equation,
yâ€²â€² (x) = clâ€² (x) = c
q
1 + yâ€² (x)2
and so
yâ€²â€² (x)
q
1 + yâ€² (x)2 = c.
Let z (x) = yâ€² (x) so the above diï¬€erential equation becomes
dz
dx = c
p
1 + z2,
a separable diï¬€erential equation. Thus
dz
âˆš
1 + z2 = cdx.
Now
R
dz
âˆš
1+z2 = arcsinh (z) + C and so the solutions are of the form
arcsinh (z) âˆ’cx = d

19.3.
THE DIRECTIONAL DERIVATIVE
347
where d is some constant. Thus
yâ€² = z = sinh (cx + d)
and so
y (x) âˆˆ
Z
sinh (cx + C) dx = cosh (cx + d)
c
+ k
where k is some constant. Therefore,
y (x) = 1
c cosh (cx + d) + k
where d and k are some constants and c = Ïg/T0. Curves of this sort are called catenaries.
Note these curves result from an assumption the only forces acting on the chain are as
shown.
19.3.2
Exercises With Answersâˆ—
1. Find the solution to the initial value problem,
yâ€² = x
y2 , y (0) = 1.
Separating the variables, you get y2dy = xdx and so y3
3 âˆ’x2
2 = c. From the initial
condition, 1
3 = c and so the solution is
y3
3 âˆ’x2
2 = 1
3
2. Find the solution to the initial value problem,
tan (y) yâ€² = sin x, y
Â³Ï€
4
Â´
= Ï€
4 .
Separating the variables, tan (y) dy = sin (x) dx and so ln |sec (y)| + cos (x) = c. Now
from the initial condition,
ln
Â³âˆš
2
Â´
+
âˆš
2
2
= c
and so ln |sec (y)| + cos (x) = ln
Â¡âˆš
2
Â¢
+
âˆš
2
2
3. Find the solution to the initial value problem,
yâ€² = y
x, y (1) = 1.
Separating the variables, gives dy
y = dx
x and so yâˆ’x = c. But from the initial condition,
c = 0. Hence y = x.

348
DIFFERENTIABILITY 24-26 OCT.
19.3.3
A Heat Seaking Particle
Suppose the temperature is given as T (x, y, z) and a particle tries to go in the direction of
most rapid rate of change of temperature. In other words this particle likes it hot. This
means it moves in the direction of the gradient of T. In other words,
(xâ€², yâ€², zâ€²)T = k (x, y, z) âˆ‡T (x, y, z) .
Of course you donâ€™t know what k (x, y, z) is but if you did and if you also knew T, then you
would have a system of diï¬€erential equations for the position of the particle as a function
of time. If you were given an initial position, you could then ask for the solution to the
resulting intial value problem. Of course you wonâ€™t be able to solve the equations in general.
These sorts of things require numerical methods. Also, in interesting examples, everything
would also depend on t. The following pseudo application has to do with a situation which
I will cook up so that I will be able to solve everything.
Example 19.3.5 A heat seaking particle starts at (1, 2, 1) . The temperature is T (x, y, z) =
x2 + y + z3 and assume that k = 1. Find the motion of the heat seaking particle.
As explained above, you need (xâ€², yâ€², zâ€²)T = âˆ‡T (x, y, z) and so
dx
dt = 2x, dy
dt = 1, dz
dt = 3z2
because âˆ‡T =
Â¡
2x, 1, 3z2Â¢T . It is very fortunate that the equations are not coupled. Con-
sider the ï¬rst one. Separating the variables,
dx
x = 2dt
and so ln (x)âˆ’2t = c. From the initial condition which states that at t = 0, x = 1, it follows
c = 0. Therefore, x = e2t. Next consider the second of the diï¬€erential equations. This one
says y = t + c and from the initial condition, c = 2 so the second gives y = t + 2. Finally
the last equation separates to give
dz
z2 = 3dt
and so
âˆ’1
z
= 3t + c.
In this case the initial data gives c = âˆ’1. Therefore, z = âˆ’
1
3tâˆ’1. It follows the path of the
particle is of the form
Âµ
e2t, t + 2, âˆ’
1
3t âˆ’1
Â¶
.
Note that this only makes sense for t âˆˆ[0, 1
3). This type of thing is typical of nonlinear
diï¬€erential equations.
I think you can see how to do similar problems in which the particle is heat avoiding.
You just put in a minus sign by âˆ‡T.
19.4
The Chain Rule
Remember what this was all about for a function of one variable. You had z = f (y) and
y = g (x) and you wanted to ï¬nd dz
dx. Remember the answer was
dz
dx = dz
dy
dy
dx.

19.4.
THE CHAIN RULE
349
The chain rule was one of the most important rules for diï¬€erentiation. Its importance is no
less for functions of many variables.
The problem is this: z = f (y1, y2, Â· Â· Â·, yn) and yk = gk (x1, Â· Â· Â·, xm) . You want to ï¬nd
âˆ‚z
âˆ‚xk for each k = 1, 2, Â· Â· Â·, m. It turns out to be exactly the same sort of formula which
works. In this case the formula is
âˆ‚z
âˆ‚xk
=
n
X
i=1
âˆ‚z
âˆ‚yi
âˆ‚yi
âˆ‚xk
.
People who use the repeated index summation convention write this as
âˆ‚z
âˆ‚yi
âˆ‚yi
âˆ‚xk
which is formally just like it was for a function of one variable. I think this is one reason
for the attractiveness of this repeated summation convention. Here is an example.
Example 19.4.1 Suppose z = y1 + y2y2
3 and y1 = sin (x1) + x2, y2 = cos (x3) , and y3 =
x2
1 + sin x2 + x4. Find
âˆ‚z
âˆ‚x2 and
âˆ‚z
âˆ‚x4 .
From the above formula,
âˆ‚z
âˆ‚x2
= âˆ‚z
âˆ‚y1
âˆ‚y1
âˆ‚x2
+ âˆ‚z
âˆ‚y2
âˆ‚y2
âˆ‚x2
+ âˆ‚z
âˆ‚y3
âˆ‚y3
âˆ‚x2
=
1 Ã— 1 + y2
3 Ã— 0 + 2y2y3 cos x2
=
1 + 2y2y3 cos x2.
If you want to put this in terms of the x variables, it is
âˆ‚z
âˆ‚x2
=
1 + 2y2y3 cos x2
=
1 + 2 cos (x3)
Â¡
x2
1 + sin x2 + x4
Â¢
cos x2
Now consider the other partial derivative.
âˆ‚z
âˆ‚x4
=
âˆ‚z
âˆ‚y1
âˆ‚y1
âˆ‚x4
+ âˆ‚z
âˆ‚y2
âˆ‚y2
âˆ‚x4
+ âˆ‚z
âˆ‚y3
âˆ‚y3
âˆ‚x4
=
âˆ‚z
âˆ‚y1
Ã— 0 + âˆ‚z
âˆ‚y2
Ã— 0 + 2y2y3 Ã— 1
=
2 cos (x3)
Â¡
x2
1 + sin x2 + x4
Â¢
.
Be sure you can ï¬nd and place the partial derivatives in terms of the independent vari-
ables, x. It is just as correct to leave the answer in terms of y and x but sometimes people
may insist you place the answer in terms of x.
The next task is to explain why the above formula works. The argument I will give
applies to one dimension also. Therefore, you can consider it a review of what you should
have seen in beginning calculus.
Lemma 19.4.2 Suppose U is an open set in Rn and f : U â†’R. Suppose x âˆˆU and for
all v small enough,
f (x + v) âˆ’f (x) =
n
X
i=1
aivi + o (v) .
Then ai = âˆ‚f
âˆ‚xi (x) and f is diï¬€erentiable.

350
DIFFERENTIABILITY 24-26 OCT.
Proof: Let t be a small nonzero number. Then since the ith component of ek equals
zero unless i = k when it is 1,
f (x + tek) âˆ’f (x)
=
akt + o (tek)
=
akt + o (t)
Now divide by t and take a limit.
âˆ‚f
âˆ‚xk
(x) = lim
tâ†’0
f (x + tek) âˆ’f (x)
t
= lim
tâ†’0
Âµ
ak + o (t)
t
Â¶
= ak.
This proves the lemma.
Lemma 19.4.3 Let U be an open set and suppose g is diï¬€erentiable at x âˆˆU. Then
o (g (x + v) âˆ’g (x)) = o (v) .
Proof: I need to show
lim
|v|â†’0
o (g (x + v) âˆ’g (x))
|v|
= 0.
Let Îµ > 0 be given. Since g is continuous at x, there exists Î´1 > 0 such that if |v| < Î´1, then
|o (g (x + v) âˆ’g (x))|
|g (x + v) âˆ’g (x)|
< Îµ
Hence, for such v,
|o (g (x + v) âˆ’g (x))| < Îµ |g (x + v) âˆ’g (x)|
(19.5)
Since g is diï¬€erentiable at x, there exists Î´2 > 0 such that if |v| < Î´2,
Â¯Â¯Â¯g (x + v) âˆ’g (x) âˆ’Pn
k=1
âˆ‚g
âˆ‚xk (x) vk
Â¯Â¯Â¯
|v|
< Îµ
Hence for |v| < Î´2,
|g (x + v) âˆ’g (x)| <
Â¯Â¯Â¯Â¯Â¯
n
X
k=1
âˆ‚g
âˆ‚xk
(x) vk
Â¯Â¯Â¯Â¯Â¯ + Îµ |v|
(19.6)
Let Î´ â‰¤min (Î´1, Î´2) . Then if |v| < Î´, both 19.5 and 19.6 hold and so by the Cauchy Schwarz
inequality,
|o (g (x + v) âˆ’g (x))|
<
Îµ |g (x + v) âˆ’g (x)|
<
Îµ
ÃƒÂ¯Â¯Â¯Â¯Â¯
n
X
k=1
âˆ‚g
âˆ‚xk
(x) vk
Â¯Â¯Â¯Â¯Â¯ + Îµ |v|
!
â‰¤
Îµ
Ãƒ n
X
k=1
Â¯Â¯Â¯Â¯
âˆ‚g
âˆ‚xk
(x)
Â¯Â¯Â¯Â¯
2!1/2
|v| + Îµ |v| .
Dividing both sides by |v| ,
o (g (x + v) âˆ’g (x))
|v|
â‰¤Îµ
ï£«
ï£­
Ãƒ n
X
k=1
Â¯Â¯Â¯Â¯
âˆ‚g
âˆ‚xk
(x)
Â¯Â¯Â¯Â¯
2!1/2
+ 1
ï£¶
ï£¸
and since Îµ > 0 is arbitrary, this establishes the lemma.
With these lemmas, it is easy to prove the chain rule.

19.4.
THE CHAIN RULE
351
Theorem 19.4.4 Let V be an open set in Rn and let U be an open set in Rm. Also
let g :U â†’V be a vector valued function having the property that for
g (x) = (g1 (x) , Â· Â· Â·, gm (x)) ,
each gk is diï¬€erentiable at x âˆˆU. Also suppose f : V â†’R is diï¬€erentiable at g (x) . Then
for z â‰¡f â—¦g, yi = gi (x) ,
âˆ‚z
âˆ‚xk
(x) =
n
X
i=1
âˆ‚z
âˆ‚yi
(g (x)) âˆ‚yi
âˆ‚xk
(x) .
Proof: Using Lemma 19.4.3 as needed,
f â—¦g (x + v) âˆ’f â—¦g (x)
=
n
X
i=1
âˆ‚z
âˆ‚yi
(g (x)) (gi (x + v) âˆ’gi (x)) + o (gi (x + v) âˆ’gi (x))
=
n
X
i=1
âˆ‚z
âˆ‚yi
(g (x)) (gi (x + v) âˆ’gi (x)) + o (v)
=
n
X
i=1
âˆ‚z
âˆ‚yi
(g (x))
Ãƒ m
X
k=1
âˆ‚yi
âˆ‚xk
(x) vk + o (v)
!
+ o (v)
=
n
X
i=1
m
X
k=1
âˆ‚z
âˆ‚yi
(g (x)) âˆ‚yi
âˆ‚xk
(x) vk + o (v)
=
m
X
k=1
Ãƒ n
X
i=1
âˆ‚z
âˆ‚yi
(g (x)) âˆ‚yi
âˆ‚xk
(x)
!
vk + o (v)
Now by Lemma 19.4.2,
âˆ‚f â—¦g
âˆ‚xk
(x) â‰¡âˆ‚z
âˆ‚xk
=
n
X
i=1
âˆ‚z
âˆ‚yi
(g (x)) âˆ‚yi
âˆ‚xk
(x) .
This proves the theorem.
19.4.1
Related Rates Problems
Sometimes several variables are related and given information about how one variable is
changing, you want to ï¬nd how the others are changing. The following law is discussed later
in the book, on Page 519.
Example 19.4.5 Bernoulliâ€™s law states that in an incompressible ï¬‚uid,
v2
2g + z + P
Î³ = C
where C is a constant. Here v is the speed, P is the pressure, and z is the height above
some reference point. The constants, g and Î³ are the acceleration of gravity and the weight
density of the ï¬‚uid. Suppose measurements indicate that dv
dt = âˆ’3, and dz
dt = 2. Find dP
dt
when v = 7 and z = 8 in terms of g and Î³.
This is just an exercise in using the chain rule. Diï¬€erentiate the two sides with respect
to t.
1
g v dv
dt + dz
dt + 1
Î³
dP
dt = 0.

352
DIFFERENTIABILITY 24-26 OCT.
Then when v = 7 and z = 8, ï¬nding dP
dt involves nothing more than solving the following
for dP
dt .
7
g (âˆ’3) + 2 + 1
Î³
dP
dt = 0
Thus
dP
dt = Î³
Âµ21
g âˆ’2
Â¶
at this instant in time.
Example 19.4.6 In Bernoulliâ€™s law above, each of v, z, and P are functions of (x, y, z) ,
the position of a point in the ï¬‚uid. Find a formula for âˆ‚P
âˆ‚x in terms of the partial derivatives
of the other variables.
This is an example of the chain rule. Diï¬€erentiate both sides with respect to x.
v
g vx + zx + 1
Î³ Px = 0
and so
Px = âˆ’
Âµvvx + zxg
g
Â¶
Î³
Example 19.4.7 Suppose a level curve is of the form f (x, y) = C and that near a point
on this level curve, y is a diï¬€erentiable function of x. Find dy
dx.
This is an example of the chain rule. Diï¬€erentiate both sides with respect to x. This
gives
fx + fy
dy
dx = 0.
Solving for dy
dx gives
dy
dx = âˆ’fx (x, y)
fy (x, y) .
Example 19.4.8 Suppose a level surface is of the form f (x, y, z) = C. and that near a
point, (x, y, z) on this level surface, z is a C1 function of x and y. Find a formula for zx.
This is an exaple of the use of the chain rule. Diï¬€erentiate both sides of the equation
with respect to x. Since yx = 0, this yields
fx + fzzx = 0.
Then solving for zx gives
zx = âˆ’fx (x, y, z)
fz (x, y, z)
Example 19.4.9 Polar coordinates are
x = r cos Î¸, y = r sin Î¸.
Thus if f is a C1 scalar valued function you could ask to express fx in terms of the variables,
r and Î¸. Do so.

19.5.
NORMAL VECTORS AND TANGENT PLANES 26 OCT.
353
This is an example of the chain rule. f = f (r, Î¸) and so
fx = frrx + fÎ¸Î¸x.
This will be done if you can ï¬nd rx and Î¸x. However you must ï¬nd these in terms of r and
Î¸, not in terms of x and y. Using the chain rule on the two equations for the transformation,
1
=
rx cos Î¸ âˆ’(r sin Î¸) Î¸x
0
=
rx sin Î¸ + (r cos Î¸) Î¸x
Solving these using Cramerâ€™s rule yields
rx = cos (Î¸) , Î¸x = âˆ’sin (Î¸)
r
Hence fx in polar coordinates is
fx = fr (r, Î¸) cos (Î¸) âˆ’fÎ¸ (r, Î¸)
Âµsin (Î¸)
r
Â¶
19.5
Normal Vectors And Tangent Planes 26 Oct.
Quiz
1. Let z = xy2 and let x = ts + ps while y = 2s2 + t. Find âˆ‚z
âˆ‚s when (s, t, p) = (1, 1, 1) .
2. A level surface is given by x3y + z2 = 2. Find zx at the point (1, 1, 1) on the level
surface.
3. Suppose x = t3 + s and y = s3 + t. Find âˆ‚z
âˆ‚x completely in terms of partial derivativs
and functions of the new variables, s, t.
The gradient has fundamental geometric signiï¬cance illustrated by the following picture.
>
^
Â¤ âˆ‡f(x0, y0, z0)
xâ€²
1(t0)
xâ€²
2(s0)
In this picture, the surface is a piece of a level surface of a function of three vari-
ables, f (x, y, z) . Thus the surface is deï¬ned by f (x, y, z) = c or more completely as
{(x, y, z) : f (x, y, z) = c} . For example, if f (x, y, z) = x2 + y2 + z2, this would be a piece
of a sphere. There are two smooth curves in this picture which lie in the surface having
parameterizations, x1 (t) = (x1 (t) , y1 (t) , z1 (t)) and x2 (s) = (x2 (s) , y2 (s) , z2 (s)) which
intersect at the point, (x0, y0, z0) on this surface1. This intersection occurs when t = t0 and
s = s0. Since the points, x1 (t) for t in an interval lie in the level surface, it follows
f (x1 (t) , y1 (t) , z1 (t)) = c
1Do there exist any smooth curves which lie in the level surface of f and pass through the point
(x0, y0, z0)? It turns out there do if âˆ‡f (x0, y0, z0) Ì¸= 0 and if the function, f, is C1. However, this is a
consequence of the implicit function theorem, one of the greatest theorems in all mathematics and a topic
for an advanced calculus class. See the the section on the implicit function theorem for the most elementary
treatment of this theorem that I know.

354
DIFFERENTIABILITY 24-26 OCT.
for all t in some interval. Therefore, taking the derivative of both sides and using the chain
rule on the left,
âˆ‚f
âˆ‚x (x1 (t) , y1 (t) , z1 (t)) xâ€²
1 (t) +
âˆ‚f
âˆ‚y (x1 (t) , y1 (t) , z1 (t)) yâ€²
1 (t) + âˆ‚f
âˆ‚z (x1 (t) , y1 (t) , z1 (t)) zâ€²
1 (t) = 0.
In terms of the gradient, this merely states
âˆ‡f (x1 (t) , y1 (t) , z1 (t)) Â· xâ€²
1 (t) = 0.
Similarly,
âˆ‡f (x2 (s) , y2 (s) , z2 (s)) Â· xâ€²
2 (s) = 0.
Letting s = s0 and t = t0, it follows
âˆ‡f (x0, y0, z0) Â· xâ€²
1 (t0) = 0, âˆ‡f (x0, y0, z0) Â· xâ€²
2 (s0) = 0.
It follows âˆ‡f (x0, y0, z0) is perpendicular to both the direction vectors of the two indicated
curves shown. Surely if things are as they should be, these two direction vectors would
determine a plane which deserves to be called the tangent plane to the level surface of f at
the point (x0, y0, z0) and that âˆ‡f (x0, y0, z0) is perpendicular to this tangent plane at the
point, (x0, y0, z0).
Example 19.5.1 Find the equation of the tangent plane to the level surface, f (x, y, z) = 6
of the function, f (x, y, z) = x2 + 2y2 + 3z2 at the point (1, 1, 1) .
First note that (1, 1, 1) is a point on this level surface.
To ï¬nd the desired plane it
suï¬ƒces to ï¬nd the normal vector to the proposed plane. But âˆ‡f (x, y, z) = (2x, 4y, 6z) and
so âˆ‡f (1, 1, 1) = (2, 4, 6) . Therefore, from this problem, the equation of the plane is
(2, 4, 6) Â· (x âˆ’1, y âˆ’1, z âˆ’1) = 0
or in other words,
2x âˆ’12 + 4y + 6z = 0.
Example 19.5.2 The point,
Â¡âˆš
3, 1, 4
Â¢
is on both the surfaces, z = x2 + y2 and z = 8 âˆ’
Â¡
x2 + y2Â¢
. Find the cosine of the angle between the two tangent planes at this point.
Recall this is the same as the angle between two normal vectors. Of course there is some
ambiguity here because if n is a normal vector, then so is âˆ’n and replacing n with âˆ’n
in the formula for the cosine of the angle will change the sign. We agree to look for the
acute angle and its cosine rather than the optuse angle. The normals are
Â¡
2
âˆš
3, 2, âˆ’1
Â¢
and
Â¡
2
âˆš
3, 2, 1
Â¢
. Therefore, the cosine of the angle desired is
Â¡
2
âˆš
3
Â¢2 + 4 âˆ’1
17
= 15
17.
Example 19.5.3 The point,
Â¡
1,
âˆš
3, 4
Â¢
is on the surface, z = x2 +y2. Find the line perpen-
dicular to the surface at this point.
All that is needed is the direction vector of this line. The surface is the level surface,
x2 + y2 âˆ’z = 0. The normal to this surface is given by the gradient at this point. Thus the
desired line is
Â³
1,
âˆš
3, 4
Â´
+ t
Â³
2, 2
âˆš
3, âˆ’1
Â´
.

Extrema Of Functions Of
Several Variables 30 Oct.
Quiz
1. Let z = x2 sin (y) and let x = t2s+r while y = t2 âˆ’s. Find zt when (s, t, r) = (1, 1, 1) .
2. The temperature in space is given by T (x, y, z) = x + 2y2 + z2. Find the path of a
heat seeking particle which starts at the point (0, 1, 1) . This is an ill deï¬ned problem.
Make the usual assumptions.
3. The ideal gas law is PV = kT where k is a constant. Suppose at some time dT
dt =
1, dP
dt = âˆ’1. Find dV
dt at this instant if P = 2, V = 6, T = 100.
4. There are two surfaces, x2 + y2 = 1 and x2 + y2 + z2 = 5 which intersect in a curve.
Find an equation of the tangent line to this curve at the point,
Â³ âˆš
3
2 , 1
2, 2
Â´
.
5. A level surface is x2 + 2y2 + 3z2 = 6. Find the tangent plane at the point, (1, 1, 1).
Quiz
1. Let z = x sin
Â¡
x2 + y2Â¢
. Find âˆ‚z
âˆ‚x.
2. Let z3 sin (x) + y4z = 7. Find âˆ‚z
âˆ‚x.
3. Suppose f (x, y) is given by
f (x, y) =
(
2yx+x3+xy2
x2+y2
if (x, y) Ì¸= (0, 0)
0 if (x, y) = (0, 0)
Find fx (0, 0) if possible.
4. Find parametric equations of the line which is perpendicular to the surface 3x2+2y2+
z2 = 6 at the point (1, 1, 1).
5. Let u (x, y) = f (x + y) + g (x âˆ’y) . Compute uxx âˆ’uyy. Dâ€™Lambert did this problem
back in the mid 1700â€™s and it turned out to be very important.
6. A function of two variables, f (x, y) is called homogeneous of degree Î± if f (tx, ty) =
tÎ±f (x, y) . Establish Eulerâ€™s identity which states that for such homogeneous functions,
xfx (x, y) + yfy (x, y) = Î±f (x, y) .
355

356
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
This identity dates from early in the 1700â€™s also.
Hint: Use the chain rule and
diï¬€erentiate both sides of f (tx, ty) = tÎ±f (x, y) with respect to t using the chain rule
and then plug in t = 1.
Suppose f : D (f) â†’R where D (f) âŠ†Rn.
20.1
Local Extrema
Deï¬nition 20.1.1 A point x âˆˆD (f) âŠ†Rn is called a local minimum if f (x) â‰¤
f (y) for all y âˆˆD (f) suï¬ƒciently close to x. A point x âˆˆD (f) is called a local maximum
if f (x) â‰¥f (y) for all y âˆˆD (f) suï¬ƒciently close to x. A local extremum is a point of
D (f) which is either a local minimum or a local maximum. The plural for extremum is
extrema. The plural for minimum is minima and the plural for maximum is maxima.
Procedure 20.1.2 To ï¬nd candidates for local extrema which are interior points of
D (f) where f is a diï¬€erentiable function, you simply identify those points where âˆ‡f equals
the zero vector. To justify this, note that the graph of f is the level surface
F (x,z) â‰¡z âˆ’f (x) = 0
and the local extrema at such interior points must have horizontal tangent planes. Therefore,
a normal vector at such points must be a multiple of (0, Â· Â· Â·, 0, 1) . Thus âˆ‡F at such points
must be a multiple of this vector. That is, if x is such a point,
k (0, Â· Â· Â·, 0, 1) = (âˆ’fx1 (x) , Â· Â· Â·, âˆ’fxn (x) , 1) .
Thus âˆ‡f (x) = 0.
This is illustrated in the following picture.
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡Â¡
6
r
z = f(x)
Tangent Plane
A more rigorous explanation is as follows. Let v be any vector in Rn and suppose x is
a local maximum (minimum) for f. Then consider the real valued function of one variable,
h (t) â‰¡f (x + tv) for small |t| . Since f has a local maximum (minimum), it follows that h
is a diï¬€erentiable function of the single variable t for small t which has a local maximum
(minimum) when t = 0. Therefore, hâ€² (0) = 0. But hâ€² (t) = Pn
i=1
âˆ‚f
âˆ‚xi (x+tv) vi by the chain
rule. Therefore,
hâ€² (0) =
n
X
i=1
âˆ‚f
âˆ‚xi
(x) vi = 0

20.1.
LOCAL EXTREMA
357
and since v is arbitrary, it follows
âˆ‚f
âˆ‚xi (x) = 0 for each i. Thus
Â¡
fx1 (x)
Â· Â· Â·
fxn (x)
Â¢T = 0
and so âˆ‡f (x) = 0. This proves the following theorem.
Theorem 20.1.3 Suppose U is an open set contained in D (f) such that f is C1
on U and suppose x âˆˆU is a local minimum or local maximum for f. Then âˆ‡f (x) = 0.
Deï¬nition 20.1.4 A singular point
for f is a point x where âˆ‡f (x) = 0. This
is also called a critical point. By analogy with the one variable case, a point where the
gradient does not exist will also be called a critical point.
Example 20.1.5 Find the critical points for the function, f (x, y) â‰¡xyâˆ’xâˆ’y for x, y > 0.
Note that here D (f) is an open set and so every point is an interior point. Where is the
gradient equal to zero?
fx = y âˆ’1 = 0, fy = x âˆ’1 = 0
and so there is exactly one critical point, (1, 1) .
Example 20.1.6 Find the volume of the smallest tetrahedron made up of the coordinate
planes in the ï¬rst octant and a plane which is tangent to the sphere x2 + y2 + z2 = 4.
The normal to the sphere at a point, (x0, y0, z0) on a point of the sphere is
Â³
x0, y0,
p
4 âˆ’x2
0 âˆ’y2
0
Â´
and so the equation of the tangent plane at this point is
x0 (x âˆ’x0) + y0 (y âˆ’y0) +
q
4 âˆ’x2
0 âˆ’y2
0
Âµ
z âˆ’
q
4 âˆ’x2
0 âˆ’y2
0
Â¶
= 0
When x = y = 0,
z =
4
p
(4 âˆ’x2
0 âˆ’y2
0)
When z = 0 = y,
x = 4
x0
,
and when z = x = 0,
y = 4
y0
.
Therefore, letting (x, y) take the place of (x0, y0) for simplicity, the function to minimize is
f (x, y) = 1
6
64
xy
p
(4 âˆ’x2 âˆ’y2)
This is because in beginning calculus it was shown that the volume of a pyramid is 1/3 the
area of the base times the height. Therefore, you simply need to ï¬nd the gradient of this
and set it equal to zero. Thus upon taking the partial derivatives, you need to have
âˆ’4 + 2x2 + y2
x2y (âˆ’4 + x2 + y2)
p
(4 âˆ’x2 âˆ’y2)
= 0,
and
âˆ’4 + x2 + 2y2
xy2 (âˆ’4 + x2 + y2)
p
(4 âˆ’x2 âˆ’y2)
= 0.
Therefore, x2 + 2y2 = 4 and 2x2 + y2 = 4. Thus x = y and so x = y =
2
âˆš
3. It follows from
the equation for z that z =
2
âˆš
3 also. How do you know this is not the largest tetrahedron?

358
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
Example 20.1.7 An open box is to contain 32 cubic feet. Find the dimensions which will
result in the least surface area.
Let the height of the box be z and the length and width be x and y respectively. Then
xyz = 32 and so z = 32/xy. The total area is xy + 2xz + 2yz and so in terms of the two
variables, x and y, the area is
A = xy + 64
y + 64
x
To ï¬nd best dimensions you note these must result in a local minimum.
Ax = yx2 âˆ’64
x2
= 0, Ay = xy2 âˆ’64
y2
.
Therefore, yx2 âˆ’64 = 0 and xy2 âˆ’64 = 0 so xy2 = yx2. For sure the answer excludes the
case where any of the variables equals zero. Therefore, x = y and so x = 4 = y. Then z = 2
from the requirement that xyz = 32. How do you know this gives the least surface area?
Why doesnâ€™t this give the largest surface area?
20.2
The Second Derivative Test
20.2.1
Functions Of Two Variables
In the special case of a function of two variables, f (x, y) which is the only case considered
in most calculus books, the second derivative test is given in the following theorem. It is a
black box formulation of the second derivative test.
Theorem 20.2.1 (Second Derivative Test) Let f be a function of two variables
deï¬ned on an open set, U whose second order partial derivatives exist and are continuous.
That is, f âˆˆC2 (U) . Suppose (a, b) âˆˆU is a point where both partial derivatives of f
vanishes. That is fx (a, b) = fy (a, b) = 0. Let
D â‰¡fxx (a, b) fyy (a, b) âˆ’(fxy (a, b))2 .
Then
1. If D > 0 and fxx (a, b) < 0, then f has a local maximum at (a, b).
2. If D > 0 and fxx (a, b) > 0, then f has a local mimimum at (a, b) .
3. If D < 0, then f has a saddle point at (a, b) .
4. If D = 0, the test fails.
The above is really a statement about the eigenvalues of the Hessian matrix,
H â‰¡
Âµ
fxx (a, b)
fx,y (a, b)
fx,y (a, b)
fyy (a, b)
Â¶
at a point (a, b) where the partial derivatives of f vanish. It reduces to the following much
simpler statement.
If both eigenvalues of H are positive, then f has a local minimum
at (a, b) . If both eigenvalues are negative, then f has a local maximum at (a, b). If one
eigenvalue is positive and one is negative, then you have a saddle point at (a, b) . If at
least one eigenvalue equals zero, then the test fails. Here is a picture which may help you
remember this second version of this test.

20.2.
THE SECOND DERIVATIVE TEST
359
Use whichever version of this theorem you ï¬nd easiest to remember. However, in the case
of a function of many variables, the description I just gave has an obvious generalization.
This is presented next. If you are not interested in it, I think you can skip it because it isnâ€™t
included in the book for the course.
20.2.2
Functions Of Many Variablesâˆ—
There is a version of the second derivative test for a function of many variables in the case
that the function and its ï¬rst and second partial derivatives are all continuous. A discussion
of its proof is given in Section 21.4.
Deï¬nition 20.2.2 The matrix, H (x) whose ijth entry at the point x is
âˆ‚2f
âˆ‚xiâˆ‚xj (x)
is called the Hessian matrix.
The following theorem says that if all the eigenvalues of the Hessian matrix at a critical
point are positive, then the critical point is a local minimum. If all the eigenvalues of the
Hessian matrix at a critical point are negative, then the critical point is a local maximum.
Finally, if some of the eigenvalues of the Hessian matrix at the critical point are positive and
some are negative then the critical point is a saddle point. The following picture illustrates
the situation.
Theorem 20.2.3 Let f : U â†’R for U an open set in Rn and let f be a C2
function and suppose that at some x âˆˆU, âˆ‡f (x) = 0. Also let Âµ and Î» be respectively, the
largest and smallest eigenvalues of the matrix, H (x) . If Î» > 0 then f has a local minimum
at x. If Âµ < 0 then f has a local maximum at x. If either Î» or Âµ equals zero, the test
fails. If Î» < 0 and Âµ > 0 there exists a direction in which when f is evaluated on the
line through the critical point having this direction, the resulting function of one variable
has a local minimum and there exists a direction in which when f is evaluated on the line
through the critical point having this direction, the resulting function of one variable has a
local maximum. This last case is called a saddle point.
Example 20.2.4 Let f (x, y) = 2x4 âˆ’4x3 +14x2 +12yx2 âˆ’12yxâˆ’12x+2y2 +4y +2. Find
the critical points and determine whether they are local minima, local maxima, or saddle
points.
fx (x, y) = 8x3 âˆ’12x2 + 28x + 24yx âˆ’12y âˆ’12 and fy (x, y) = 12x2 âˆ’12x + 4y + 4. The
points at which both fx and fy equal zero are
Â¡ 1
2, âˆ’1
4
Â¢
, (0, âˆ’1) , and (1, âˆ’1) .

360
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
The Hessian matrix is
Âµ
24x2 + 28 + 24y âˆ’24x
24x âˆ’12
24x âˆ’12
4
Â¶
.
and the thing to determine is the sign of its eigenvalues evaluated at the critical points.
First consider the point
Â¡ 1
2, âˆ’1
4
Â¢
. This matrix is
Âµ
16
0
0
4
Â¶
and its eigenvalues are 16, 4
showing that this is a local minimum.
Next consider (0, âˆ’1) at this point the Hessian matrix is
Âµ
4
âˆ’12
âˆ’12
4
Â¶
and the eigen-
values are 16, âˆ’8. Therefore, this point is a saddle point.
Finally consider the point (1, âˆ’1) . At this point the Hessian is
Âµ
4
12
12
4
Â¶
and the
eigenvalues are 16, âˆ’8 so this point is also a saddle point.
The geometric signiï¬cance of a saddle point was explained above. In one direction it
looks like a local minimum while in another it looks like a local maximum. In fact, they
do look like a saddle. Here is a picture of the graph of the above function near the saddle
point, (0, âˆ’1) .
â€“0.1
0
0.1
0.2
x
2
â€“1.1
â€“1
â€“0.9
â€“0.8
y
4
2
0
2
4
6
You see it is a lot like the place where you sit on a saddle. If you want to get a better
picture, you could graph instead
f (x, y) = arctan
Â¡
2x4 âˆ’4x3 + 14x2 + 12yx2 âˆ’12yx âˆ’12x + 2y2 + 4y + 2
Â¢
.
Since arctan is a strictly increasing function, it preserves all the information about whether
the given function is increasing or decreasing in certain directions. Below is a graph of this
function which illustrates the behavior near the point (1, âˆ’1) .

20.2.
THE SECOND DERIVATIVE TEST
361
2
â€“1
0
1
2
x
â€“2
â€“1
0
1
y
5
1
5
0
5
1
5
Or course sometimes the second derivative test is inadequate to determine what is going
on. This should be no surprise since this was the case even for a function of one variable.
For a function of two variables, a nice example is the Monkey saddle.
Example 20.2.5 Suppose f (x, y) = arctan
Â¡
6xy2 âˆ’2x3 âˆ’3y4Â¢
. Show (0, 0) is a critical
point for which the second derivative test gives no information.
Before doing anything it might be interesting to look at the graph of this function of two
variables plotted using Maple.
This picture should indicate why this is called a monkey saddle. It is because the monkey
can sit in the saddle and have a place for his tail. Now to see (0, 0) is a critical point, note
that
âˆ‚(arctan (g (x, y)))
âˆ‚x
=
1
1 + g (x, y)2 gx (x, y)
and that a similar formula holds for the partial derivative with respect to y. Therefore, it
suï¬ƒces to verify that for
g (x, y) = 6xy2 âˆ’2x3 âˆ’3y4
gx (0, 0) = gy (0, 0) = 0.
gx (x, y) = 6y2 âˆ’6x2, gy (x, y) = 12xy âˆ’12y3

362
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
and clearly (0, 0) is a critical point. So are (1, 1) and (1, âˆ’1) . Now gxx (0, 0) = 0 and so does
gxy (0, 0) and gyy (0, 0) . This implies fxx, fxy, fyy are all equal to zero at (0, 0) also. (Why?)
Therefore, the Hessian matrix is the zero matrix and clearly has only the zero eigenvalue.
Therefore, the second derivative test is totally useless at this point.
However, suppose you took x = t and y = t and evaluated this function on this line.
This reduces to h (t) = f (t, t) = arctan(4t3 âˆ’3t4), which is strictly increasing near t = 0.
This shows the critical point, (0, 0) of f is neither a local max. nor a local min. Next let
x = 0 and y = t. Then p (t) â‰¡f (0, t) = âˆ’3t4. Therefore, along the line, (0, t) , f has a local
maximum at (0, 0) .
The following example is for a function of three variables.
Example 20.2.6 Find the critical points of the following function of three variables and
classify them as local minimums, local maximums or saddle points.
f (x, y, z) = 5
6x2 + 4x + 16 âˆ’7
3xy âˆ’4y âˆ’4
3xz + 12z + 5
6y2 âˆ’4
3zy + 1
3z2
First you need to locate the critical points. This involves taking the gradient.
âˆ‡
Âµ5
6x2 + 4x + 16 âˆ’7
3xy âˆ’4y âˆ’4
3xz + 12z + 5
6y2 âˆ’4
3zy + 1
3z2
Â¶
=
Âµ5
3x + 4 âˆ’7
3y âˆ’4
3z, âˆ’7
3x âˆ’4 + 5
3y âˆ’4
3z, âˆ’4
3x + 12 âˆ’4
3y + 2
3z
Â¶
Next you need to set the gradient equal to zero and solve the equations. This yields y =
5, x = 3, z = âˆ’2. Now to use the second derivative test, you assemble the Hessian matrix
which is
ï£«
ï£­
5
3
âˆ’7
3
âˆ’4
3
âˆ’7
3
5
3
âˆ’4
3
âˆ’4
3
âˆ’4
3
2
3
ï£¶
ï£¸.
Note that in this simple example, the Hessian matrix is constant and so all that is left
is to consider the eigenvalues. Writing the characteristic equation and solving yields the
eigenvalues are 2, âˆ’2, 4. Thus the given point is a saddle point.
20.3
Lagrange Multipliers, Constrained Extrema 31 Oct.
Lagrange multipliers are used to solve extremum problems for a function deï¬ned on a level
set of another function. For example, suppose you want to maximize xy given that x+y = 4.
This is not too hard to do using methods developed earlier. Solve for one of the variables,
say y, in the constraint equation, x+y = 4 to ï¬nd y = 4âˆ’x. Then the function to maximize
is f (x) = x (4 âˆ’x) and the answer is clearly x = 2. Thus the two numbers are x = y = 2.
This was easy because you could easily solve the constraint equation for one of the variables
in terms of the other. Now what if you wanted to maximize f (x, y, z) = xyz subject to the
constraint that x2 +y2 +z2 = 4? It is still possible to do this using using similar techniques.
Solve for one of the variables in the constraint equation, say z, substitute it into f, and then
ï¬nd where the partial derivatives equal zero to ï¬nd candidates for the extremum. However,
it seems you might encounter many cases and it does look a little fussy. However, sometimes
you canâ€™t solve the constraint equation for one variable in terms of the others. Also, what
if you had many constraints. What if you wanted to maximize f (x, y, z) subject to the
constraints x2 + y2 = 4 and z = 2x + 3y2. Things are clearly getting more involved and

20.3.
LAGRANGE MULTIPLIERS, CONSTRAINED EXTREMA 31 OCT.
363
messy. It turns out that at an extremum, there is a simple relationship between the gradient
of the function to be maximized and the gradient of the constraint function. This relation
can be seen geometrically as in the following picture.
Â¤
Â¤
q
(x0, y0, z0)
Â¤
Â¤
Â¤
Â¤
p
âˆ‡g(x0, y0, z0)
âˆ‡f(x0, y0, z0)
In the picture, the surface represents a piece of the level surface of g (x, y, z) = 0 and
f (x, y, z) is the function of three variables which is being maximized or minimized on the
level surface and suppose the extremum of f occurs at the point (x0, y0, z0) . As shown
above, âˆ‡g (x0, y0, z0) is perpendicular to the surface or more precisely to the tangent plane.
However, if x (t) = (x (t) , y (t) , z (t)) is a point on a smooth curve which passes through
(x0, y0, z0) when t = t0, then the function, h (t) = f (x (t) , y (t) , z (t)) must have either a
maximum or a minimum at the point, t = t0. Therefore, hâ€² (t0) = 0. But this means
0
=
hâ€² (t0) = âˆ‡f (x (t0) , y (t0) , z (t0)) Â· xâ€² (t0)
=
âˆ‡f (x0, y0, z0) Â· xâ€² (t0)
and since this holds for any such smooth curve, âˆ‡f (x0, y0, z0) is also perpendicular to the
surface. This picture represents a situation in three dimensions and you can see that it is
intuitively clear that this implies âˆ‡f (x0, y0, z0) is some scalar multiple of âˆ‡g (x0, y0, z0).
Thus
âˆ‡f (x0, y0, z0) = Î»âˆ‡g (x0, y0, z0)
This Î» is called a Lagrange multiplier after Lagrange who considered such problems in
the 1700â€™s. I think he did it in the context of the calculus of variations in the presence of
constraints.
Of course the above argument is at best only heuristic. It does not deal with the question
of existence of smooth curves lying in the constraint surface passing through (x0, y0, z0) .
Nor does it consider all cases, being essentially conï¬ned to three dimensions. In addition to
this, it fails to consider the situation in which there are many constraints. However, I think
it is likely a geometric notion like that presented above which led Lagrange to formulate the
method.
Example 20.3.1 Maximize xyz subject to x2 + y2 + z2 = 27.

364
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
Here f (x, y, z) = xyz while g (x, y, z) = x2+y2+z2âˆ’27. Then âˆ‡g (x, y, z) = (2x, 2y, 2z)
and âˆ‡f (x, y, z) = (yz, xz, xy) . Then at the point which maximizes this function1,
(yz, xz, xy) = Î» (2x, 2y, 2z) .
Therefore, each of 2Î»x2, 2Î»y2, 2Î»z2 equals xyz. It follows that at any point which maximizes
xyz, |x| = |y| = |z| . Therefore, the only candidates for the point where the maximum occurs
are (3, 3, 3) , (âˆ’3, âˆ’3, 3) (âˆ’3, 3, 3) , etc. The maximum occurs at (3, 3, 3) which can be veriï¬ed
by plugging in to the function which is being maximized.
The method of Lagrange multipliers allows you to consider maximization of functions
deï¬ned on closed and bounded sets.
Recall that any continuous function deï¬ned on a
closed and bounded set has a maximum and a minimum on the set. Candidates for the
extremum on the interior of the set can be located by setting the gradient equal to zero.
The consideration of the boundary can then sometimes be handled with the method of
Lagrange multipliers.
Example 20.3.2 Maximize f (x, y) = xy + y subject to the constraint, x2 + y2 â‰¤1.
Here I know there is a maximum because the set is the closed circle, a closed and bounded
set. Therefore, it is just a matter of ï¬nding it. Look for singular points on the interior of
the circle. âˆ‡f (x, y) = (y, x + 1) = (0, 0) . There are no points on the interior of the circle
where the gradient equals zero. Therefore, the maximum occurs on the boundary of the
circle. That is the problem reduces to maximizing xy + y subject to x2 + y2 = 1. From the
above,
(y, x + 1) âˆ’Î» (2x, 2y) = 0.
Hence y2 âˆ’2Î»xy = 0 and x (x + 1) âˆ’2Î»xy = 0 so y2 = x (x + 1). Therefore from the
constraint, x2 + x (x + 1) = 1 and the solution is x = âˆ’1, x = 1
2. Then the candidates for a
solution are (âˆ’1, 0) ,
Â³
1
2,
âˆš
3
2
Â´
,
Â³
1
2, âˆ’
âˆš
3
2
Â´
. Then
f (âˆ’1, 0) = 0, f
Ãƒ
1
2,
âˆš
3
2
!
= 3
âˆš
3
4 , f
Ãƒ
1
2, âˆ’
âˆš
3
2
!
= âˆ’3
âˆš
3
4 .
It follows the maximum value of this function is 3
âˆš
3
4
and it occurs at
Â³
1
2,
âˆš
3
2
Â´
. The minimum
value is âˆ’3
âˆš
3
4
and it occurs at
Â³
1
2, âˆ’
âˆš
3
2
Â´
.
Example 20.3.3 Find candidates for the maximum and minimum values of the function,
f (x, y) = xy âˆ’x2 on the set,
Â©
(x, y) : x2 + 2xy + y2 â‰¤4
Âª
.
First, the only point where âˆ‡f equals zero is (x, y) = (0, 0) and this is in the desired set.
In fact it is an interior point of this set. This takes care of the interior points. What about
those on the boundary x2 + 2xy + y2 = 4? The problem is to maximize xy âˆ’x2 subject to
the constraint, x2 + 2xy + y2 = 4. The Lagrangian is xy âˆ’x2 âˆ’Î»
Â¡
x2 + 2xy + y2 âˆ’4
Â¢
and
this yields the following system.
y âˆ’2x âˆ’Î» (2x + 2y) = 0
x âˆ’2Î» (x + y) = 0
x2 + 2xy + y2 = 4
1There exists such a point because the sphere is closed and bounded.

20.3.
LAGRANGE MULTIPLIERS, CONSTRAINED EXTREMA 31 OCT.
365
From the ï¬rst two equations,
(2 + 2Î») x âˆ’(1 âˆ’2Î») y = 0
(1 âˆ’2Î») x âˆ’2Î»y = 0
Since not both x and y equal zero, it follows
det
Âµ 2 + 2Î»
2Î» âˆ’1
1 âˆ’2Î»
âˆ’2Î»
Â¶
= 0
which yields
Î» = 1/8
Therefore,
y = 3x
(20.1)
From the constraint equation,
x2 + 2x (3x) + (3x)2 = 4
and so
x = 1
2 or âˆ’1
2
Now from 20.1, the points of interest on the boundary of this set are
Âµ1
2, 3
2
Â¶
, and
Âµ
âˆ’1
2, âˆ’3
2
Â¶
.
(20.2)
f
Âµ1
2, 3
2
Â¶
=
Âµ1
2
Â¶ Âµ3
2
Â¶
âˆ’
Âµ1
2
Â¶2
=
1
2
f
Âµ
âˆ’1
2, âˆ’3
2
Â¶
=
Âµ
âˆ’1
2
Â¶ Âµ
âˆ’3
2
Â¶
âˆ’
Âµ
âˆ’1
2
Â¶2
=
1
2
It follows the candidates for maximum and minimum are
Â¡ 1
2, 3
2
Â¢
, (0, 0) , and
Â¡
âˆ’1
2, âˆ’3
2
Â¢
.
Therefore it appears that (0, 0) yields a minimum and either
Â¡ 1
2, 3
2
Â¢
or
Â¡
âˆ’1
2, âˆ’3
2
Â¢
yields a
maximum. However, this is a little misleading. How do you even know a maximum or a
minimum exists? The set, x2 + 2xy + y2 â‰¤4 is an unbounded set which lies between the
two lines x + y = 2 and x + y = âˆ’2. In fact there is no minimum. For example, take
x = 100, y = âˆ’98. Then xy âˆ’x2 = x (y âˆ’x) = 100 (âˆ’98 âˆ’100) which is a large negative
number much less than 0, the answer for the point (0, 0).
There are no magic bullets here. It was still required to solve a system of nonlinear
equations to get the answer. However, it does often help to do it this way.
The above generalizes to a general procedure which is described in the following major
Theorem. All correct proofs of this theorem will involve some appeal to the implicit or
inverse function theorem or to fundamental existence theorems from diï¬€erential equations.
A complete proof is very fascinating but it will not come cheap. Good advanced calculus
books will usually give a correct proof. I have also given a complete proof later starting on
Page 389. First here is a simple deï¬nition explaining one of the terms in the statement of
this theorem.

366
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
Deï¬nition 20.3.4 Let A be an m Ã— n matrix. A submatrix is any matrix which
can be obtained from A by deleting some rows and some columns.
Theorem 20.3.5 Let U be an open subset of Rn and let f : U â†’R be a C1
function. Then if x0 âˆˆU is either a local maximum or local minimum of f subject to the
constraints
gi (x) = 0, i = 1, Â· Â· Â·, m
(20.3)
and if some m Ã— m submatrix of
Dg (x0) â‰¡
ï£«
ï£¬
ï£­
g1x1 (x0)
g1x2 (x0)
Â· Â· Â·
g1xn (x0)
...
...
...
gmx1 (x0)
gmx2 (x0)
Â· Â· Â·
gmxn (x0)
ï£¶
ï£·
ï£¸
has nonzero determinant, then there exist scalars, Î»1, Â· Â· Â·, Î»m such that
ï£«
ï£¬
ï£­
fx1 (x0)
...
fxn (x0)
ï£¶
ï£·
ï£¸= Î»1
ï£«
ï£¬
ï£­
g1x1 (x0)
...
g1xn (x0)
ï£¶
ï£·
ï£¸+ Â· Â· Â· + Î»m
ï£«
ï£¬
ï£­
gmx1 (x0)
...
gmxn (x0)
ï£¶
ï£·
ï£¸
(20.4)
holds.
To help remember how to use 20.4 it may be helpful to do the following. First write the
Lagrangian,
L = f (x) âˆ’
m
X
i=1
Î»igi (x)
and then proceed to take derivatives with respect to each of the components of x and also
derivatives with respect to each Î»i and set all of these equations equal to 0. The formula 20.4
is what results from taking the derivatives of L with respect to the components of x. When
you take the derivatives with respect to the Lagrange multipliers, and set what results equal
to 0, you just pick up the constraint equations. This yields n + m equations for the n + m
unknowns, x1, Â· Â· Â·, xn, Î»1, Â· Â· Â·, Î»m. Then you proceed to look for solutions to these equations.
Of course these might be impossible to ï¬nd using methods of algebra, but you just do your
best and hope it will work out.
Example 20.3.6 Minimize xyz subject to the constraints x2 + y2 + z2 = 4 and x âˆ’2y = 0.
Form the Lagrangian,
L = xyz âˆ’Î»
Â¡
x2 + y2 + z2 âˆ’4
Â¢
âˆ’Âµ (x âˆ’2y)
and proceed to take derivatives with respect to every possible variable, leading to the fol-
lowing system of equations.
yz âˆ’2Î»x âˆ’Âµ
=
0
xz âˆ’2Î»y + 2Âµ
=
0
xy âˆ’2Î»z
=
0
x2 + y2 + z2
=
4
x âˆ’2y
=
0
Now you have to ï¬nd the solutions to this system of equations. In general, this could be
very hard or even impossible. If Î» = 0, then from the third equation, either x or y must

20.3.
LAGRANGE MULTIPLIERS, CONSTRAINED EXTREMA 31 OCT.
367
equal 0. Therefore, from the ï¬rst two equations, Âµ = 0 also. If Âµ = 0 and Î» Ì¸= 0, then from
the ï¬rst two equations, xyz = 2Î»x2 and xyz = 2Î»y2 and so either x = y or x = âˆ’y, which
requires that both x and y equal zero thanks to the last equation. But then from the fourth
equation, z = Â±2 and now this contradicts the third equation. Thus Âµ and Î» are either both
equal to zero or neither one is and the expression, xyz equals zero in this case. However, I
know this is not the best value for a minimizer because I can take x = 2
q
3
5, y =
q
3
5, and
z = âˆ’1. This satisï¬es the constraints and the product of these numbers equals a negative
number. Therefore, both Âµ and Î» must be non zero. Now use the last equation eliminate x
and write the following system.
5y2 + z2 = 4
y2 âˆ’Î»z = 0
yz âˆ’Î»y + Âµ = 0
yz âˆ’4Î»y âˆ’Âµ = 0
From the last equation, Âµ = (yz âˆ’4Î»y) . Substitute this into the third and get
5y2 + z2 = 4
y2 âˆ’Î»z = 0
yz âˆ’Î»y + yz âˆ’4Î»y = 0
y = 0 will not yield the minimum value from the above example. Therefore, divide the last
equation by y and solve for Î» to get Î» = (2/5) z. Now put this in the second equation to
conclude
5y2 + z2 = 4
y2 âˆ’(2/5) z2 = 0 ,
a system which is easy to solve.
Thus y2 = 8/15 and z2 = 4/3. Therefore, candidates
for minima are
Â³
2
q
8
15,
q
8
15, Â±
q
4
3
Â´
, and
Â³
âˆ’2
q
8
15, âˆ’
q
8
15, Â±
q
4
3
Â´
, a choice of 4 points to
check. Clearly the one which gives the smallest value is
Ãƒ
2
r
8
15,
r
8
15, âˆ’
r
4
3
!
or
Â³
âˆ’2
q
8
15, âˆ’
q
8
15, âˆ’
q
4
3
Â´
and the minimum value of the function subject to the constraints
is âˆ’2
5
âˆš
30 âˆ’2
3
âˆš
3.
You should rework this problem ï¬rst solving the second easy constraint for x and then
producing a simpler problem involving only the variables y and z.
20.3.1
Exercises With Answers
1. Maximize x + 3y âˆ’6z subject to the constraint, x2 + 2y2 + z2 = 9.
The Lagrangian is L = x + 3y âˆ’6z âˆ’Î»
Â¡
x2 + 2y2 + z2 âˆ’9
Â¢
. Now take the derivative
with respect to x. This gives the equation 1 âˆ’2Î»x = 0. Next take the derivative with
respect to y. This gives the equation 3 âˆ’4Î»y = 0. The derivative with respect to
z gives âˆ’6 âˆ’2Î»z = 0. Clearly Î» Ì¸= 0 since this would contradict the ï¬rst of these
equations. Similarly, none of the variables, x, y, z can equal zero. Solving each of
these equations for Î» gives
1
2x =
3
4y =
âˆ’3
z . Thus y =
3x
2 and z = âˆ’6x. Now you
use the constraint equation plugging in these values for y and z.
x2 + 2
Â¡ 3x
2
Â¢2 +
(âˆ’6x)2 = 9. This gives the values for x as x =
3
83
âˆš
166, x = âˆ’3
83
âˆš
166. From the

368
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
three equations above, this also determines the values of z and y.
y =
9
166
âˆš
166 or
âˆ’9
166
âˆš
166 and z = âˆ’18
83
âˆš
166 or 18
83
âˆš
166. Thus there are two points to look at. One
will give the minimum value and the other will give the maximum value. You know the
minimum and maximum exist because of the extreme value theorem. The two points
are
Â¡ 3
83
âˆš
166,
9
166
âˆš
166, âˆ’18
83
âˆš
166
Â¢
and
Â¡
âˆ’3
83
âˆš
166, âˆ’9
166
âˆš
166, 18
83
âˆš
166
Â¢
. Now you just
need to ï¬nd which is the minimum and which is the maximum. Plug these in to the
function you are trying to maximize.
Â¡ 3
83
âˆš
166
Â¢
+ 3
Â¡ 9
166
âˆš
166
Â¢
âˆ’6
Â¡
âˆ’18
83
âˆš
166
Â¢
will
clearly be the maximum value occuring at
Â¡ 3
83
âˆš
166,
9
166
âˆš
166, âˆ’18
83
âˆš
166
Â¢
. The other
point will obviously yield the minimum because this one is positive and the other one
is negative. If you use a calculator to compute this you get
Â¡ 3
83
âˆš
166
Â¢
+3
Â¡ 9
166
âˆš
166
Â¢
âˆ’
6
Â¡
âˆ’18
83
âˆš
166
Â¢
= 19. 326.
2. Find the dimensions of the largest rectangle which can be inscribed in a the ellipse
x2 + 4y2 = 4.
This is one which you could do without Lagrange multipliers. However, it is easier with
Lagrange multipliers. Let a corner of the rectangle be at (x, y) . Then the area of the
rectangle will be 4xy and since (x, y) is on the ellipse, you have the constraint x2+4y2 =
4. Thus the problem is to maximize 4xy subject to x2+4y2 = 4. The Lagrangian is then
L = 4xyâˆ’Î»
Â¡
x2 + 4y2 âˆ’4
Â¢
and so you get the equations 4yâˆ’2Î»x = 0 and 4xâˆ’8Î»y = 0.
You canâ€™t have both x and y equal to zero and satisfy the constraint. Therefore, the
determinant of the matrix of coeï¬ƒcients must equal zero.
Thus
Â¯Â¯Â¯Â¯
âˆ’2Î»
4
4
âˆ’8Î»
Â¯Â¯Â¯Â¯ =
16Î»2 âˆ’16 = 0. This is because the system of equations is of the form
Âµ
âˆ’2Î»
4
4
âˆ’8Î»
Â¶ Âµ
x
y
Â¶
=
Âµ
0
0
Â¶
.
If the matrix has an inverse, then the only solution would be x = y = 0 which as
noted above canâ€™t happen. Therefore, Î» = Â±1. First suppose Î» = 1. Then the ï¬rst
equation says 2y = x. Pluggin this in to the constraint equation, x2 + x2 = 4 and so
x = Â±
âˆš
2. Therefore, y = Â±
âˆš
2
2 . This yields the dimensions of the largest rectangle to
be 2
âˆš
2 Ã—
âˆš
2.
You can check all the other cases and see you get the same thing in
the other cases as well.
3. Maximize 2x + y subject to the condition that x2
4 + y2 â‰¤1.
The maximum of this function clearly exists because of the extreme value theorem
since the condition deï¬nes a closed and bounded set in R2. However, this function
does not achieve its maximum on the interior of the given ellipse deï¬ned by x2
4 +y2 â‰¤1
because the gradient of the function which is to be maximized is never equal to zero.
Therefore, this function must achieve its maximum on the set x2
4 + y2 = 1. Thus you
want to maximuze 2x + y subject to x2
4 + y2 = 1. This is just like Problem 1. You can
ï¬nish this.
4. Find the points on y2x = 16 which are closest to (0, 0) .
You want to minimize x2 + y2 subject to y2x âˆ’16. Of course you really want to
minimize
p
x2 + y2 but the ordered pair which minimized x2 + y2 is the same as the
ordered pair which minimize
p
x2 + y2 so it is pointless to drag around the square
root. The Lagrangian is x2 + y2 âˆ’Î»
Â¡
y2x âˆ’16
Â¢
. Diï¬€erentiating with respect to x and
y gives the equations 2x âˆ’Î»y2 = 0 and 2y âˆ’2Î»yx = 0. Neither x nor y can equal
zero and solve the constraint. Therefore, the second equation implies Î»x = 1. Hence
Î» = 1
x = 2x
y2 . Therefore, 2x2 = y2 and so 2x3 = 16 and so x = 2. Therefore, y = Â±2
âˆš
2.

20.3.
LAGRANGE MULTIPLIERS, CONSTRAINED EXTREMA 31 OCT.
369
The points are
Â¡
2, 2
âˆš
2
Â¢
and
Â¡
2, âˆ’2
âˆš
2
Â¢
. They both give the same answer. Note how ad
hoc these procedures are. I canâ€™t give you a simple strategy for solving these systems
of nonlinear equations by algebra because there is none. Sometimes nothing you do
will work.
5. Find points on xy = 1 farthest from (0, 0) if any exist. If none exist, tell why. What
does this say about the method of Lagrange multipliers?
If you graph xy = 1 you see there is no farthest point. However, there is a closest
point and the method of Lagrange multipliers will ï¬nd this closest point. This shows
that the answer you get has to be carefully considered to determine whether you have
a maximum or a minimum or perhaps neither.
6. A curve is formed from the intersection of the plane, 2x + y + z = 3 and the cylinder
x2 + y2 = 4. Find the point on this curve which is closest to (0, 0, 0) .
You want to maximize x2 + y2 + z2 subject to the two constraints 2x + y + z = 3 and
x2 + y2 = 4. This means the Lagrangian will have two multipliers.
L = x2 + y2 + z2 âˆ’Î» (2x + y + z âˆ’3) âˆ’Âµ
Â¡
x2 + y2 âˆ’4
Â¢
Then this yields the equations 2x âˆ’2Î» âˆ’2Âµx = 0, 2y âˆ’Î» âˆ’2Âµy, and 2z âˆ’Î» = 0. The
last equation says Î» = 2z and so I will replace Î» with 2z where ever it occurs. This
yields
x âˆ’2z âˆ’Âµx = 0, 2y âˆ’2z âˆ’2Âµy = 0.
This shows x (1 âˆ’Âµ) = 2y (1 âˆ’Âµ) . First suppose Âµ = 1. Then from the above equa-
tions, z = 0 and so the two constraints reduce to 2y + x = 3 and x2 + y2 = 4 and
2y + x = 3. The solutions are
Â¡ 3
5 âˆ’2
5
âˆš
11, 6
5 + 1
5
âˆš
11, 0
Â¢
,
Â¡ 3
5 + 2
5
âˆš
11, 6
5 âˆ’1
5
âˆš
11, 0
Â¢
.
The other case is that Âµ Ì¸= 1 in which case x = 2y and the second constraint
yields that y = Â± 2
âˆš
5 and x = Â± 4
âˆš
5. Now from the ï¬rst constraint, z = âˆ’2
âˆš
5 + 3
in the case where y =
2
âˆš
5 and z = 2
âˆš
5 + 3 in the other case.
This yields the
points
Â³
4
âˆš
5,
2
âˆš
5, âˆ’2
âˆš
5 + 3
Â´
and
Â³
âˆ’4
âˆš
5, âˆ’2
âˆš
5, 2
âˆš
5 + 3
Â´
. This appears to have ex-
hausted all the possibilities and so it is now just a matter of seeing which of these
points gives the best answer.
An answer exists because of the extreme value the-
orem.
After all, this constraint set is closed and bounded.
The ï¬rst candidate
listed above yields for the answer
Â¡ 3
5 âˆ’2
5
âˆš
11
Â¢2 +
Â¡ 6
5 + 1
5
âˆš
11
Â¢2 = 4. The second can-
didate listed above yields
Â¡ 3
5 + 2
5
âˆš
11
Â¢2 +
Â¡ 6
5 âˆ’1
5
âˆš
11
Â¢2 = 4 also.
Thus these two
give equally good results. Now consider the last two candidates.
Â³
4
âˆš
5
Â´2
+
Â³
2
âˆš
5
Â´2
+
Â¡
âˆ’2
âˆš
5 + 3
Â¢2 = 4 +
Â¡
âˆ’2
âˆš
5 + 3
Â¢2 which is larger than 4. Finally the last candidate
yields
Â³
âˆ’4
âˆš
5
Â´2
+
Â³
âˆ’2
âˆš
5
Â´2
+
Â¡
2
âˆš
5 + 3
Â¢2 = 4 +
Â¡
2
âˆš
5 + 3
Â¢2 also larger than 4. There-
fore, there are two points on the curve of intersection which are closest to the origin,
Â¡ 3
5 âˆ’2
5
âˆš
11, 6
5 + 1
5
âˆš
11, 0
Â¢
and
Â¡ 3
5 + 2
5
âˆš
11, 6
5 âˆ’1
5
âˆš
11, 0
Â¢
. Both are a distance of 4 from
the origin.
7. Here are two lines. x = (1 + 2t, 2 + t, 3 + t)T and x = (2 + s, 1 + 2s, 1 + 3s)T . Find
points p1 on the ï¬rst line and p2 on the second with the property that |p1 âˆ’p2| is at
least as small as the distance between any other pair of points, one chosen on one line
and the other on the other line.
Hint: Do you need to use Lagrange multipliers for this?

370
EXTREMA OF FUNCTIONS OF SEVERAL VARIABLES 30 OCT.
8. Find the point on x2 + y2 + z2 = 1 closest to the plane x + y + z = 10.
You want to minimize (x âˆ’a)2+(y âˆ’b)2+(z âˆ’c)2 subject to the constraints a+b+c =
10 and x2 + y2 + z2 = 1. There seem to be a lot of variables in this problem, 6 in
all. Start taking derivatives and hope for a miracle. This yields 2 (x âˆ’a) âˆ’2Âµx =
0, 2 (y âˆ’b)âˆ’2Âµy = 0, 2 (z âˆ’c)âˆ’2Âµz = 0. Also, taking derivatives with respect to a, b,
and c you obtain 2 (x âˆ’a) + Î» = 0, 2 (y âˆ’b) + Î» = 0, 2 (z âˆ’c) + Î» = 0. Comparing
the ï¬rst equations in each list, you see Î» = 2Âµx and then comparing the second two
equations in each list, Î» = 2Âµy and similarly, Î» = 2Âµz. Therefore, if Âµ Ì¸= 0, it must
follow that x = y = z. Now you can see by sketching a rough graph that the answer
you want has each of x, y, and z nonnegative. Therefore, using the constraint for
these variables, the point desired is
Â³
1
âˆš
3,
1
âˆš
3,
1
âˆš
3
Â´
which you could probably see was
the answer from the sketch. However, this could be made more diï¬ƒcult rather easily
such that the sketch wonâ€™t help but Lagrange multipliers will.

The Derivative Of Vector
Valued Functions, What Is The
Derivative?âˆ—
If you are going to do this stuï¬€, you might as well do it right and include the case of vector
valued functions. You know everything about matrices at this point for it to all make perfect
sense. Therefore, I think it is well worth your time to read this although you are unlikely
to see it on a test. It is not any harder than what has been presented. It also tells you
what the derivative is. This is essential information if you are going to understand Newtonâ€™s
method for nonlinear systems. It is also essential if you want to read a really good book on
continuum mechanics and is needed in many other physical and engineering applications.
Also included is a proof of the second derivative test.
Recall the following deï¬nition.
Deï¬nition 21.0.7 A function, T which maps Rn to Rp is called a linear transfor-
mation if for every pair of scalars, a, b and vectors, x, y âˆˆRn, it follows that T (ax + by) =
aT (x) + bT (y) .
Recall that from the properties of matrix multiplication, it follows that if A is an n Ã— p
matrix, and if x, y are vectors in Rn, then A (ax + by) = aA (x) + bA (y) . Thus you can
deï¬ne a linear transformation by multiplying by a matrix. Of course the simplest example is
that of a 1Ã—1 matrix or number. You can think of the number 3 as a linear transformation,
T mapping R to R according to the rule Tx = 3x. It satisï¬es the properties needed for
a linear transformation because 3 (ax + by) = a3x + b3y = aTx + bTy. The case of the
derivative of a scalar valued function of one variable is of this sort. You get a number for
the derivative. However, you can think of this number as a linear transformation. Of course
it is not worth the fuss to do so for a function of one variable but this is the way you must
think of it for a function of n variables.
Deï¬nition 21.0.8 Let f : U â†’Rp where U is an open set in Rn for n, p â‰¥1 and
let x âˆˆU be given. Then f is deï¬ned to be diï¬€erentiable at x âˆˆU if and only if there
exist column vectors, vi such that for h = (h1 Â· Â·Â·, hn)T ,
f (x + h) = f (x) +
n
X
i=1
vihi + o (h) .
(21.1)
The derivative of the function, f, denoted by Df (x) , is the linear transformation deï¬ned by
multiplying by the matrix whose columns are the p Ã— 1 vectors, vi. Thus if w is a vector in
371

372THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?âˆ—
Rn,
Df (x) w â‰¡
ï£«
ï£­
|
|
v1
Â· Â· Â·
vn
|
|
ï£¶
ï£¸w.
It is common to think of this matrix as the derivative but strictly speaking, this is
incorrect. The derivative is a â€œlinear transformationâ€ determined by multiplication by this
matrix, called the standard matrix because it is based on the standard basis vectors for
Rn. The subtle issues involved in a thorough exploration of this issue will be avoided for
now. It will be ï¬ne to think of the above matrix as the derivative. Other notations which
are often used for this matrix or the linear transformation are f â€² (x) , J (x) , and even âˆ‚f
âˆ‚x or
df
dx.
Theorem 21.0.9 Suppose f is as given above in 21.1. Then
vk = lim
hâ†’0
f (x+hek) âˆ’f (x)
h
â‰¡âˆ‚f
âˆ‚xk
(x) ,
the kth partial derivative.
Proof: Let h = (0, Â· Â· Â·, h, 0, Â· Â· Â·, 0)T = hek where the h is in the kth slot. Then 21.1
reduces to
f (x + h) = f (x) + vkh + o (h) .
Therefore, dividing by h
f (x+hek) âˆ’f (x)
h
= vk + o (h)
h
and taking the limit,
lim
hâ†’0
f (x+hek) âˆ’f (x)
h
= lim
hâ†’0
Âµ
vk + o (h)
h
Â¶
= vk
and so, the above limit exists. This proves the theorem.
Let f : U â†’Rq where U is an open subset of Rp and f is diï¬€erentiable. It was just
shown
f (x + v) = f (x) +
p
X
j=1
âˆ‚f (x)
âˆ‚xj
vj + o (v) .
Taking the ith coordinate of the above equation yields
fi (x + v) = fi (x) +
p
X
j=1
âˆ‚fi (x)
âˆ‚xj
vj + o (v)
and it follows that the term with a sum is nothing more than the ith component of J (x) v
where J (x) is the q Ã— p matrix,
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ‚f1
âˆ‚x1
âˆ‚f1
âˆ‚x2
Â· Â· Â·
âˆ‚f1
âˆ‚xp
âˆ‚f2
âˆ‚x1
âˆ‚f2
âˆ‚x2
Â· Â· Â·
âˆ‚f2
âˆ‚xp
...
...
...
...
âˆ‚fq
âˆ‚x1
âˆ‚fq
âˆ‚x2
Â· Â· Â·
âˆ‚fq
âˆ‚xp
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.

21.1.
C1 FUNCTIONSâˆ—
373
This gives the form of the matrix which deï¬nes the linear transformation, Df (x) . Thus
f (x + v) = f (x) + J (x) v + o (v)
(21.2)
and to reiterate, the linear transformation which results by multiplication by this q Ã— p
matrix is known as the derivative.
Sometimes x, y, z is written instead of x1, x2, and x3. This is to save on notation and is
easier to write and to look at although it lacks generality. When this is done it is understood
that x = x1, y = x2, and z = x3. Thus the derivative is the linear transformation determined
by
ï£«
ï£­
f1x
f1y
f1z
f2x
f2y
f2z
f3x
f3y
f3z
ï£¶
ï£¸.
Example 21.0.10 Let A be a constant mÃ—n matrix and consider f (x) = Ax. Find Df (x)
if it exists.
f (x + h) âˆ’f (x) = A (x + h) âˆ’A (x) = Ah = Ah + o (h) .
In fact in this case, o (h) = 0. Therefore, Df (x) = A. Note that this looks the same as the
case in one variable, f (x) = ax.
21.1
C1 Functionsâˆ—
Given a function of many variables, how can you tell if it is diï¬€erentiable? Sometimes you
have to go directly to the deï¬nition and verify it is diï¬€erentiable from the deï¬nition. For
example, you may have seen the following important example in one variable calculus.
Example 21.1.1 Let f (x) =
Â½
x2 sin
Â¡ 1
x
Â¢
if x Ì¸= 0
0 if x = 0
. Find Df (0) .
f (h) âˆ’f (0) = 0h + h2 sin
Â¡ 1
h
Â¢
= o (h) and so Df (0) = 0. If you ï¬nd the derivative
for x Ì¸= 0, it is totally useless information if what you want is Df (0) . This is because the
derivative, turns out to be discontinuous. Try it. Find the derivative for x Ì¸= 0 and try to
obtain Df (0) from it. You see, in this example you had to revert to the deï¬nition to ï¬nd
the derivative.
It isnâ€™t really too hard to use the deï¬nition even for more ordinary examples.
Example 21.1.2 Let f (x, y) =
Âµ
x2y + y2
y3x
Â¶
. Find Df (1, 2) .
First of all note that the thing you are after is a 2 Ã— 2 matrix.
f (1, 2) =
Âµ
6
8
Â¶
.
Then
f (1 + h1, 2 + h2) âˆ’f (1, 2)

374THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?âˆ—
=
Âµ
(1 + h1)2 (2 + h2) + (2 + h2)2
(2 + h2)3 (1 + h1)
Â¶
âˆ’
Âµ
6
8
Â¶
=
Âµ
5h2 + 4h1 + 2h1h2 + 2h2
1 + h2
1h2 + h2
2
8h1 + 12h2 + 12h1h2 + 6h2
2 + 6h2
2h1 + h3
2 + h3
2h1
Â¶
=
Âµ
4
5
8
12
Â¶ Âµ
h1
h2
Â¶
+
Âµ
2h1h2 + 2h2
1 + h2
1h2 + h2
2
12h1h2 + 6h2
2 + 6h2
2h1 + h3
2 + h3
2h1
Â¶
=
Âµ
4
5
8
12
Â¶ Âµ
h1
h2
Â¶
+ o (h) .
Therefore, the standard matrix of the derivative is
Âµ
4
5
8
12
Â¶
.
Most of the time, there is an easier way to conclude a derivative exists and to ï¬nd it. It
involves the notion of a C1 function.
Deï¬nition 21.1.3 When f : U â†’Rp for U an open subset of Rn and the vector
valued functions,
âˆ‚f
âˆ‚xi are all continuous, (equivalently each âˆ‚fi
âˆ‚xj is continuous), the function
is said to be C1 (U) . If all the partial derivatives up to order k exist and are continuous,
then the function is said to be Ck.
It turns out that for a C1 function, all you have to do is write the matrix described in
Theorem 21.0.9 and this will be the derivative. There is no question of existence for the
derivative for such functions. This is the importance of the next theorem.
Theorem 21.1.4 Suppose f : U â†’Rp where U is an open set in Rn. Suppose also
that all partial derivatives of f exist on U and are continuous. Then f is diï¬€erentiable at
every point of U.
Proof: If you ï¬x all the variables but one, you can apply the fundamental theorem of
calculus as follows.
f (x+vkek) âˆ’f (x) =
Z 1
0
âˆ‚f
âˆ‚xk
(x + tvkek) vkdt.
(21.3)
Here is why. Let h (t) = f (x + tvkek) . Then
h (t + h) âˆ’h (t)
h
= f (x + tvkek + hvkek) âˆ’f (x + tvkek)
hvk
vk
and so, taking the limit as h â†’0 yields
hâ€² (t) = âˆ‚f
âˆ‚xk
(x + tvkek) vk
Therefore,
f (x+vkek) âˆ’f (x) = h (1) âˆ’h (0) =
Z 1
0
hâ€² (t) dt =
Z 1
0
âˆ‚f
âˆ‚xk
(x + tvkek) vkdt.
Now I will use this observation to prove the theorem. Let v = (v1, Â· Â· Â·, vn) with |v|
suï¬ƒciently small. Thus v = Pn
k=1 vkek. For the purposes of this argument, deï¬ne
n
X
k=n+1
vkek â‰¡0.

21.1.
C1 FUNCTIONSâˆ—
375
Then with this convention,
f (x + v) âˆ’f (x)
=
n
X
i=1
Ãƒ
f
Ãƒ
x+
n
X
k=i
vkek
!
âˆ’f
Ãƒ
x+
n
X
k=i+1
vkek
!!
=
n
X
i=1
Z 1
0
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
vidt
=
n
X
i=1
Z 1
0
Ãƒ
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
vi âˆ’âˆ‚f
âˆ‚xi
(x) vi
!
dt
+
n
X
i=1
Z 1
0
âˆ‚f
âˆ‚xi
(x) vidt
=
n
X
i=1
âˆ‚f
âˆ‚xi
(x) vi
+
Z 1
0
n
X
i=1
Ãƒ
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
âˆ’âˆ‚f
âˆ‚xi
(x)
!
vidt
=
n
X
i=1
âˆ‚f
âˆ‚xi
(x) vi + o (v)
and this shows f is diï¬€erentiable at x.
Some explanation of the step to the last line is in order. The messy thing at the end is
o (v) because of the continuity of the partial derivatives. In fact, from the Cauchy Schwarz
inequality,
Z 1
0
n
X
i=1
Ãƒ
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
âˆ’âˆ‚f
âˆ‚xi
(x)
!
vidt
â‰¤
Z 1
0
ï£«
ï£­
n
X
i=1
Â¯Â¯Â¯Â¯Â¯
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
âˆ’âˆ‚f
âˆ‚xi
(x)
Â¯Â¯Â¯Â¯Â¯
2ï£¶
ï£¸
1/2
dt
Ãƒ n
X
i=1
v2
i
!1/2
=
Z 1
0
ï£«
ï£­
n
X
i=1
Â¯Â¯Â¯Â¯Â¯
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
âˆ’âˆ‚f
âˆ‚xi
(x)
Â¯Â¯Â¯Â¯Â¯
2ï£¶
ï£¸
1/2
dt |v|
Thus, dividing by |v| and taking a limit as |v| â†’0, the quotient is nothing but
Z 1
0
ï£«
ï£­
n
X
i=1
Â¯Â¯Â¯Â¯Â¯
âˆ‚f
âˆ‚xi
Ãƒ
x+
n
X
k=i+1
vkek + tviei
!
âˆ’âˆ‚f
âˆ‚xi
(x)
Â¯Â¯Â¯Â¯Â¯
2ï£¶
ï£¸
1/2
dt
which converges to 0 due to continuity of the partial derivatives of f.
This proves the
theorem.
Here is an example to illustrate.
Example 21.1.5 Let f (x, y) =
Âµ
x2y + y2
y3x
Â¶
. Find Df (x, y) .
From Theorem 21.1.4 this function is diï¬€erentiable because all possible partial derivatives
are continuous. Thus
Df (x, y) =
Âµ
2xy
x2 + 2y
y3
3y2x
Â¶
.

376THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?âˆ—
In particular,
Df (1, 2) =
Âµ
4
5
8
12
Â¶
.
Not surprisingly, the above theorem has an extension to more variables. First this is
illustrated with an example.
Example 21.1.6 Let f (x1, x2, x3) =
ï£«
ï£­
x2
1x2 + x2
2
x2x1 + x3
sin (x1x2x3)
ï£¶
ï£¸. Find Df (x1, x2, x3) .
All possible partial derivatives are continuous so the function is diï¬€erentiable.
The
matrix for this derivative is therefore the following 3 Ã— 3 matrix
ï£«
ï£­
2x1x2
x2
1 + 2x2
0
x2
x1
1
x2x3 cos (x1x2x3)
x1x3 cos (x1x2x3)
x1x2 cos (x1x2x3)
ï£¶
ï£¸
Example 21.1.7 Suppose f (x, y, z) = xy + z2. Find Df (1, 2, 3) .
Taking the partial derivatives of f, fx = y, fy = x, fz = 2z. These are all continuous.
Therefore, the function has a derivative and fx (1, 2, 3) = 1, fy (1, 2, 3) = 2, and fz (1, 2, 3) =
6. Therefore, Df (1, 2, 3) is given by
Df (1, 2, 3) = (1, 2, 6) .
Also, for (x, y, z) close to (1, 2, 3) ,
f (x, y, z)
â‰ˆ
f (1, 2, 3) + 1 (x âˆ’1) + 2 (y âˆ’2) + 6 (z âˆ’3)
=
11 + 1 (x âˆ’1) + 2 (y âˆ’2) + 6 (z âˆ’3) = âˆ’12 + x + 2y + 6z
When a function is diï¬€erentiable at x0 it follows the function must be continuous there.
This is the content of the following important lemma.
Lemma 21.1.8 Let f : U â†’Rq where U is an open subset of Rp. If f is diï¬€erentiable,
then f is continuous at x0. Furthermore, if C â‰¥max
nÂ¯Â¯Â¯ âˆ‚f
âˆ‚xi (x0)
Â¯Â¯Â¯ , i = 1, Â· Â· Â·, p
o
, then when-
ever |x âˆ’x0| is small enough,
|f (x) âˆ’f (x0)| â‰¤(Cp + 1) |x âˆ’x0|
(21.4)
Proof: Suppose f is diï¬€erentiable. Since o (v) satisï¬es 19.1, there exists Î´1 > 0 such
that if |x âˆ’x0| < Î´1, then |o (x âˆ’x0)| < |x âˆ’x0| . But also,
Â¯Â¯Â¯Â¯Â¯
p
X
i=1
âˆ‚f
âˆ‚xi
(x0) (xi âˆ’x0i)
Â¯Â¯Â¯Â¯Â¯ â‰¤C
p
X
i=1
|xi âˆ’x0i| â‰¤Cp |x âˆ’x0|
Therefore, if |x âˆ’x0| < Î´1,
|f (x) âˆ’f (x0)|
â‰¤
Â¯Â¯Â¯Â¯Â¯
p
X
i=1
âˆ‚f
âˆ‚xi
(x0) (xi âˆ’x0i)
Â¯Â¯Â¯Â¯Â¯ + |x âˆ’x0|
<
(Cp + 1) |x âˆ’x0|
which veriï¬es 21.4. Now letting Îµ > 0 be given, let Î´ = min
Â³
Î´1,
Îµ
Cp+1
Â´
. Then for |x âˆ’x0| <
Î´,
|f (x) âˆ’f (x0)| < (Cp + 1) |x âˆ’x0| < (Cp + 1)
Îµ
Cp + 1 = Îµ
showing f is continuous at x0.

21.2.
THE CHAIN RULEâˆ—
377
21.2
The Chain Ruleâˆ—
21.2.1
The Chain Rule For Functions Of One Variableâˆ—
First recall the chain rule for a function of one variable. Consider the following picture.
I
gâ†’J
fâ†’R
Here I and J are open intervals and it is assumed that g (I) âŠ†J. The chain rule says that if
f â€² (g (x)) exists and gâ€² (x) exists for x âˆˆI, then the composition, f â—¦g also has a derivative
at x and
(f â—¦g)â€² (x) = f â€² (g (x)) gâ€² (x) .
Recall that f â—¦g is the name of the function deï¬ned by f â—¦g (x) â‰¡f (g (x)) . In the notation
of this chapter, the chain rule is written as
Df (g (x)) Dg (x) = D (f â—¦g) (x) .
(21.5)
21.2.2
The Chain Rule For Functions Of Many Variablesâˆ—
Let U âŠ†Rn and V âŠ†Rp be open sets and let f be a function deï¬ned on V having values in
Rq while g is a function deï¬ned on U such that g (U) âŠ†V as in the following picture.
U
gâ†’V
fâ†’Rq
The chain rule says that if the linear transformations (matrices) on the left in 21.5 both
exist then the same formula holds in this more general case. Thus
Df (g (x)) Dg (x) = D (f â—¦g) (x)
Note this all makes sense because Df (g (x)) is a q Ã— p matrix and Dg (x) is a p Ã— n matrix.
Remember it is all right to do (q Ã— p) (p Ã— n) . The middle numbers match. More precisely,
Theorem 21.2.1 (Chain rule) Let U be an open set in Rn, let V be an open set
in Rp, let g : U â†’Rp be such that g (U) âŠ†V, and let f : V â†’Rq. Suppose Dg (x) exists
for some x âˆˆU and that Df (g (x)) exists. Then D (f â—¦g) (x) exists and furthermore,
D (f â—¦g) (x) = Df (g (x)) Dg (x) .
(21.6)
In particular,
âˆ‚(f â—¦g) (x)
âˆ‚xj
=
p
X
i=1
âˆ‚f (g (x))
âˆ‚yi
âˆ‚gi (x)
âˆ‚xj
.
(21.7)
There is an easy way to remember this in terms of the repeated index summation con-
vention presented earlier. Let y = g (x) and z = f (y) . Then the above says
âˆ‚z
âˆ‚yi
âˆ‚yi
âˆ‚xk
= âˆ‚z
âˆ‚xk
.
(21.8)
Remember there is a sum on the repeated index. In particular, for each index, r,
âˆ‚zr
âˆ‚yi
âˆ‚yi
âˆ‚xk
= âˆ‚zr
âˆ‚xk
.
The proof of this major theorem will be given at the end of this section. It will include
the chain rule for functions of one variable as a special case. First here are some examples.

378THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?âˆ—
Example 21.2.2 Let f (u, v) = sin (uv) and let u (x, y, t) = t sin x+cos y and v (x, y, t, s) =
s tan x + y2 + ts. Letting z = f (u, v) where u, v are as just described, ï¬nd âˆ‚z
âˆ‚t and âˆ‚z
âˆ‚x.
From 21.8,
âˆ‚z
âˆ‚t = âˆ‚z
âˆ‚u
âˆ‚u
âˆ‚t + âˆ‚z
âˆ‚v
âˆ‚v
âˆ‚t = v cos (uv) sin (x) + us cos (uv) .
Here y1 = u, y2 = v, t = xk. Also,
âˆ‚z
âˆ‚x = âˆ‚z
âˆ‚u
âˆ‚u
âˆ‚x + âˆ‚z
âˆ‚v
âˆ‚v
âˆ‚x = v cos (uv) t cos (x) + us sec2 (x) cos (uv) .
Clearly you can continue in this way taking partial derivatives with respect to any of the
other variables.
Example 21.2.3 Let w = f (u1, u2) = u2 sin (u1) and u1 = x2y + z, u2 = sin (xy) . Find
âˆ‚w
âˆ‚x , âˆ‚w
âˆ‚y , and âˆ‚w
âˆ‚z .
The derivative of f is of the form (wx, wy, wz) and so it suï¬ƒces to ï¬nd the derivative of f
using the chain rule. You need to ï¬nd Df (u1, u2) Dg (x, y, z) where g (x, y) =
Âµ
x2y + z
sin (xy)
Â¶
.
Then Dg (x, y, z) =
Âµ
2xy
x2
1
y cos (xy)
x cos (xy)
0
Â¶
. Also Df (u1, u2) = (u2 cos (u1) , sin (u1)) .
Therefore, the derivative is
Df (u1, u2) Dg (x, y, z) = (u2 cos (u1) , sin (u1))
Âµ
2xy
x2
1
y cos (xy)
x cos (xy)
0
Â¶
=
Â¡
2u2 (cos u1) xy + (sin u1) y cos xy, u2 (cos u1) x2 + (sin u1) x cos xy, u2 cos u1
Â¢
= (wx, wy, wz)
Thus âˆ‚w
âˆ‚x = 2u2 (cos u1) xy+(sin u1) y cos xy = 2 (sin (xy))
Â¡
cos
Â¡
x2y + z
Â¢Â¢
xy+
Â¡
sin
Â¡
x2y + z
Â¢Â¢
y cos xy
. Similarly, you can ï¬nd the other partial derivatives of w in terms of substituting in for u1
and u2 in the above. Note
âˆ‚w
âˆ‚x = âˆ‚w
âˆ‚u1
âˆ‚u1
âˆ‚x + âˆ‚w
âˆ‚u2
âˆ‚u2
âˆ‚x .
In fact, in general if you have w = f (u1, u2) and g (x, y, z) =
Âµ
u1 (x, y, z)
u2 (x, y, z)
Â¶
, then
D (f â—¦g) (x, y, z) is of the form
Â¡
wu1
wu2
Â¢ Âµ
u1x
u1y
u1z
u2x
u2y
u2z
Â¶
=
Â¡
wu1ux + wu2u2x
wu1uy + wu2u2y
wu1uz + wu2u2z
Â¢
.
Example 21.2.4 Let w = f (u1, u2, u3) = u2
1 + u3 + u2 and g (x, y, z) =
ï£«
ï£­
u1
u2
u3
ï£¶
ï£¸=
ï£«
ï£­
x + 2yz
x2 + y
z2 + x
ï£¶
ï£¸. Find âˆ‚w
âˆ‚x and âˆ‚w
âˆ‚z .

21.2.
THE CHAIN RULEâˆ—
379
By the chain rule,
(wx, wy, wz) =
Â¡
wu1
wu2
wu3
Â¢
ï£«
ï£­
u1x
u1y
u1z
u2x
u2y
u2z
u3x
u3y
u3z
ï£¶
ï£¸
=
Â¡
wu1u1x + wu2u2x + wu3u3x
wu1u1y + wu2u2y + wu3u3y
wu1u1z + wu2u2z + wu3u3z
Â¢
Note the pattern.
wx
=
wu1u1x + wu2u2x + wu3u3x,
wy
=
wu1u1y + wu2u2y + wu3u3y,
wz
=
wu1u1z + wu2u2z + wu3u3z.
Therefore,
wx = 2u1 (1) + 1 (2x) + 1 (1) = 2 (x + 2yz) + 2x + 1 = 4x + 4yz + 1
and
wz = 2u1 (2y) + 1 (0) + 1 (2z) = 4 (x + 2yz) y + 2z = 4yx + 8y2z + 2z.
Of course to ï¬nd all the partial derivatives at once, you just use the chain rule. Thus you
would get
Â¡
wx
wy
wz
Â¢
=
Â¡
2u1
1
1
Â¢
ï£«
ï£­
1
2z
2y
2x
1
0
1
0
2z
ï£¶
ï£¸
=
Â¡
2u1 + 2x + 1
4u1z + 1
4u1y + 2z
Â¢
=
Â¡
4x + 4yz + 1
4zx + 8yz2 + 1
4yx + 8y2z + 2z
Â¢
Example 21.2.5 Let f (u1, u2) =
Âµ
u2
1 + u2
sin (u2) + u1
Â¶
and g (x1, x2, x3) =
Âµ
u1 (x1, x2, x3)
u2 (x1, x2, x3)
Â¶
=
Âµ
x1x2 + x3
x2
2 + x1
Â¶
. Find D (f â—¦g) (x1, x2, x3) .
To do this,
Df (u1, u2) =
Âµ
2u1
1
1
cos u2
Â¶
, Dg (x1, x2, x3) =
Âµ
x2
x1
1
1
2x2
0
Â¶
.
Then
Df (g (x1, x2, x3)) =
Âµ 2 (x1x2 + x3)
1
1
cos
Â¡
x2
2 + x1
Â¢
Â¶
and so by the chain rule,
D (f â—¦g) (x1, x2, x3)
=
Df(g(x))
z
}|
{
Âµ 2 (x1x2 + x3)
1
1
cos
Â¡
x2
2 + x1
Â¢
Â¶
Dg(x)
z
}|
{
Âµ
x2
x1
1
1
2x2
0
Â¶
=
Âµ (2x1x2 + 2x3) x2 + 1
(2x1x2 + 2x3) x1 + 2x2
2x1x2 + 2x3
x2 + cos
Â¡
x2
2 + x1
Â¢
x1 + 2x2
Â¡
cos
Â¡
x2
2 + x1
Â¢Â¢
1
Â¶
Therefore, in particular,
âˆ‚f1 â—¦g
âˆ‚x1
(x1, x2, x3) = (2x1x2 + 2x3) x2 + 1,

380THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?âˆ—
âˆ‚f2 â—¦g
âˆ‚x3
(x1, x2, x3) = 1, âˆ‚f2 â—¦g
âˆ‚x2
(x1, x2, x3) = x1 + 2x2
Â¡
cos
Â¡
x2
2 + x1
Â¢Â¢
.
etc.
In diï¬€erent notation, let
Âµ
z1
z2
Â¶
= f (u1, u2) =
Âµ
u2
1 + u2
sin (u2) + u1
Â¶
. Then
âˆ‚z1
âˆ‚x1
= âˆ‚z1
âˆ‚u1
âˆ‚u1
âˆ‚x1
+ âˆ‚z1
âˆ‚u2
âˆ‚u2
âˆ‚x1
= 2u1x2 + 1 = 2 (x1x2 + x3) x2 + 1.
Example 21.2.6 Let f (u1, u2, u3) =
ï£«
ï£­
z1
z2
z3
ï£¶
ï£¸=
ï£«
ï£­
u2
1 + u2u3
u2
1 + u3
2
ln
Â¡
1 + u2
3
Â¢
ï£¶
ï£¸and let g (x1, x2, x3, x4) =
ï£«
ï£­
u1
u2
u3
ï£¶
ï£¸=
ï£«
ï£­
x1 + x2
2 + sin (x3) + cos (x4)
x2
4 âˆ’x1
x2
3 + x4
ï£¶
ï£¸. Find (f â—¦g)â€² (x) .
Df (u) =
ï£«
ï£¬
ï£­
2u1
u3
u2
2u1
3u2
2
0
0
0
2u3
(1+u2
3)
ï£¶
ï£·
ï£¸
Similarly,
Dg (x) =
ï£«
ï£­
1
2x2
cos (x3)
âˆ’sin (x4)
âˆ’1
0
0
2x4
0
0
2x3
1
ï£¶
ï£¸.
Then by the chain rule, D (f â—¦g) (x) = Df (u) Dg (x) where u = g (x) as described above.
Thus D (f â—¦g) (x) =
ï£«
ï£¬
ï£­
2u1
u3
u2
2u1
3u2
2
0
0
0
2u3
(1+u2
3)
ï£¶
ï£·
ï£¸
ï£«
ï£­
1
2x2
cos (x3)
âˆ’sin (x4)
âˆ’1
0
0
2x4
0
0
2x3
1
ï£¶
ï£¸
=
ï£«
ï£­
2u1 âˆ’u3
4u1x2
2u1 cos x3 + 2u2x3
âˆ’2u1 sin x4 + 2u3x4 + u2
2u1 âˆ’3u2
2
4u1x2
2u1 cos x3
âˆ’2u1 sin x4 + 6u2
2x4
0
0
4
u3
1+u2
3 x3
2
u3
1+u2
3
ï£¶
ï£¸
(21.9)
where each ui is given by the above formulas. Thus âˆ‚z1
âˆ‚x1 equals
2u1 âˆ’u3
=
2
Â¡
x1 + x2
2 + sin (x3) + cos (x4)
Â¢
âˆ’
Â¡
x2
3 + x4
Â¢
=
2x1 + 2x2
2 + 2 sin x3 + 2 cos x4 âˆ’x2
3 âˆ’x4.
while âˆ‚z2
âˆ‚x4 equals
âˆ’2u1 sin x4 + 6u2
2x4 = âˆ’2
Â¡
x1 + x2
2 + sin (x3) + cos (x4)
Â¢
sin (x4) + 6
Â¡
x2
4 âˆ’x1
Â¢2 x4.
If you wanted
âˆ‚z
âˆ‚x2 it would be the second column of the above matrix in 21.9. Thus
âˆ‚z
âˆ‚x2
equals
ï£«
ï£¬
ï£­
âˆ‚z1
âˆ‚x2
âˆ‚z2
âˆ‚x2
âˆ‚z3
âˆ‚x2
ï£¶
ï£·
ï£¸=
ï£«
ï£­
4u1x2
4u1x2
0
ï£¶
ï£¸=
ï£«
ï£­
4
Â¡
x1 + x2
2 + sin (x3) + cos (x4)
Â¢
x2
4
Â¡
x1 + x2
2 + sin (x3) + cos (x4)
Â¢
x2
0
ï£¶
ï£¸.
I hope that by now it is clear that all the information you could desire about various partial
derivatives is available and it all reduces to matrix multiplication and the consideration of
entries of the matrix obtained by multiplying the two derivatives.

21.2.
THE CHAIN RULEâˆ—
381
21.2.3
The Derivative Of The Inverse Functionâˆ—
Example 21.2.7 Let f : U â†’V where U and V are open sets in Rnand f is one to one and
onto. Suppose also that f and f âˆ’1 are both diï¬€erentiable. How are Df âˆ’1 and Df related?
This can be done as follows. From the assumptions, x = f âˆ’1 (f (x)) . Let Ix = x. Then
by Example 21.0.10 on Page 373 DI = I. By the chain rule,
I = DI = Df âˆ’1 (f (x)) (Df (x)) .
Therefore,
Df (x)âˆ’1 = Df âˆ’1 (f (x)) .
This is equivalent to
Df
Â¡
f âˆ’1 (y)
Â¢âˆ’1 = Df âˆ’1 (y)
or
Df (x)âˆ’1 = Df âˆ’1 (y) , y = f (x) .
This is just like a similar situation for functions of one variable. Remember
Â¡
f âˆ’1Â¢â€² (f (x)) =
1/f â€² (x) . In terms of the repeated index summation convention, suppose y = f (x) so that
x = f âˆ’1 (y) . Then the above can be written as
Î´ij = âˆ‚xi
âˆ‚yk
(f (x)) âˆ‚yk
âˆ‚xj
(x) .
21.2.4
Acceleration In Spherical Coordinatesâˆ—
This is an interesting example which can be done with more elegance in a more general
setting. However, the more general approach also depends on the chain rule and this is
what it is all about, giving examples of the use of the chain rule. Read it if it interests you.
Example 21.2.8 Recall spherical coordinates are given by
x = Ï sin Ï† cos Î¸, y = Ï sin Ï† sin Î¸, z = Ï cos Ï†.
If an object moves in three dimensions, describe its acceleration in terms of spherical coor-
dinates and the vectors,
eÏ = (sin Ï† cos Î¸, sin Ï† sin Î¸, cos Ï†)T ,
eÎ¸ = (âˆ’Ï sin Ï† sin Î¸, Ï sin Ï† cos Î¸, 0)T ,
and
eÏ† = (Ï cos Ï† cos Î¸, Ï cos Ï† sin Î¸, âˆ’Ï sin Ï†)T .
Why these vectors? Note how they were obtained. Let
r (Ï, Î¸, Ï†) = (Ï sin Ï† cos Î¸, Ï sin Ï† sin Î¸, Ï cos Ï†)T
and ï¬x Ï† and Î¸, letting only Ï change, this gives a curve in the direction of increasing Ï.
Thus it is a vector which points away from the origin. Letting only Ï† change and ï¬xing Î¸ and
Ï, this gives a vector which is tangent to the sphere of radius Ï and points South. Similarly,
letting Î¸ change and ï¬xing the other two gives a vector which points East and is tangent to
the sphere of radius Ï. It is thought by most people that we live on a large sphere. The model
of a ï¬‚at earth is not believed by anyone except perhaps beginning physics students. Given we
live on a sphere, what directions would be most meaningful? Wouldnâ€™t it be the directions of
the vectors just described?

382THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?âˆ—
Let r (t) denote the position vector of the object from the origin. Thus
r (t) = Ï (t) eÏ (t) =
Â³
(x (t) , y (t) , z (t))T Â´
Now this implies the velocity is
râ€² (t) = Ïâ€² (t) eÏ (t) + Ï (t) (eÏ (t))â€² .
(21.10)
You see, eÏ = eÏ (Ï, Î¸, Ï†) where each of these variables is a function of t.
âˆ‚eÏ
âˆ‚Ï† = (cos Ï† cos Î¸, cos Ï† sin Î¸, âˆ’sin Ï†)T = 1
ÏeÏ†,
âˆ‚eÏ
âˆ‚Î¸ = (âˆ’sin Ï† sin Î¸, sin Ï† cos Î¸, 0)T = 1
ÏeÎ¸,
and
âˆ‚eÏ
âˆ‚Ï = 0.
Therefore, by the chain rule,
deÏ
dt
=
âˆ‚eÏ
âˆ‚Ï†
dÏ†
dt + âˆ‚eÏ
âˆ‚Î¸
dÎ¸
dt
=
1
Ï
dÏ†
dt eÏ† + 1
Ï
dÎ¸
dt eÎ¸.
By 21.10,
râ€² = Ïâ€²eÏ + dÏ†
dt eÏ† + dÎ¸
dt eÎ¸.
(21.11)
Now things get interesting. This must be diï¬€erentiated with respect to t. To do so,
âˆ‚eÎ¸
âˆ‚Î¸ = (âˆ’Ï sin Ï† cos Î¸, âˆ’Ï sin Ï† sin Î¸, 0)T =?
where it is desired to ï¬nd a, b, c such that ? = aeÎ¸ + beÏ† + ceÏ. Thus
ï£«
ï£­
âˆ’Ï sin Ï† sin Î¸
Ï cos Ï† cos Î¸
sin Ï† cos Î¸
Ï sin Ï† cos Î¸
Ï cos Ï† sin Î¸
sin Ï† sin Î¸
0
âˆ’Ï sin Ï†
cos Ï†
ï£¶
ï£¸
ï£«
ï£­
a
b
c
ï£¶
ï£¸=
ï£«
ï£­
âˆ’Ï sin Ï† cos Î¸
âˆ’Ï sin Ï† sin Î¸
0
ï£¶
ï£¸
Using Cramerâ€™s rule, the solution is a = 0, b = âˆ’cos Ï† sin Ï†, and c = âˆ’Ï sin2 Ï†. Thus
âˆ‚eÎ¸
âˆ‚Î¸
=
(âˆ’Ï sin Ï† cos Î¸, âˆ’Ï sin Ï† sin Î¸, 0)T
=
(âˆ’cos Ï† sin Ï†) eÏ† +
Â¡
âˆ’Ï sin2 Ï†
Â¢
eÏ.
Also,
âˆ‚eÎ¸
âˆ‚Ï† = (âˆ’Ï cos Ï† sin Î¸, Ï cos Ï† cos Î¸, 0)T = (cot Ï†) eÎ¸
and
âˆ‚eÎ¸
âˆ‚Ï = (âˆ’sin Ï† sin Î¸, sin Ï† cos Î¸, 0)T = 1
ÏeÎ¸.
Now in 21.11 it is also necessary to consider eÏ†.
âˆ‚eÏ†
âˆ‚Ï† = (âˆ’Ï sin Ï† cos Î¸, âˆ’Ï sin Ï† sin Î¸, âˆ’Ï cos Ï†)T = âˆ’ÏeÏ

21.2.
THE CHAIN RULEâˆ—
383
âˆ‚eÏ†
âˆ‚Î¸
=
(âˆ’Ï cos Ï† sin Î¸, Ï cos Ï† cos Î¸, 0)T
=
(cot Ï†) eÎ¸
and ï¬nally,
âˆ‚eÏ†
âˆ‚Ï = (cos Ï† cos Î¸, cos Ï† sin Î¸, âˆ’sin Ï†)T = 1
ÏeÏ†.
With these formulas for various partial derivatives, the chain rule is used to obtain râ€²â€² which
will yield a formula for the acceleration in terms of the spherical coordinates and these
special vectors. By the chain rule,
d
dt (eÏ)
=
âˆ‚eÏ
âˆ‚Î¸ Î¸â€² + âˆ‚eÏ
âˆ‚Ï† Ï†â€² + âˆ‚eÏ
âˆ‚Ï Ïâ€²
=
Î¸â€²
Ï eÎ¸ + Ï†â€²
Ï eÏ†
d
dt (eÎ¸)
=
âˆ‚eÎ¸
âˆ‚Î¸ Î¸â€² + âˆ‚eÎ¸
âˆ‚Ï† Ï†â€² + âˆ‚eÎ¸
âˆ‚Ï Ïâ€²
=
Î¸â€² Â¡
(âˆ’cos Ï† sin Ï†) eÏ† +
Â¡
âˆ’Ï sin2 Ï†
Â¢
eÏ
Â¢
+ Ï†â€² (cot Ï†) eÎ¸ + Ïâ€²
Ï eÎ¸
d
dt (eÏ†)
=
âˆ‚eÏ†
âˆ‚Î¸ Î¸â€² + âˆ‚eÏ†
âˆ‚Ï† Ï†â€² + âˆ‚eÏ†
âˆ‚Ï Ïâ€²
=
Â¡
Î¸â€² cot Ï†
Â¢
eÎ¸ + Ï†â€² (âˆ’ÏeÏ) +
ÂµÏâ€²
Ï eÏ†
Â¶
By 21.11,
râ€²â€² = Ïâ€²â€²eÏ + Ï†â€²â€²eÏ† + Î¸â€²â€²eÎ¸ + Ïâ€² (eÏ)â€² + Ï†â€² (eÏ†)â€² + Î¸â€² (eÎ¸)â€²
and from the above, this equals
Ïâ€²â€²eÏ + Ï†â€²â€²eÏ† + Î¸â€²â€²eÎ¸ + Ïâ€²
ÂµÎ¸â€²
Ï eÎ¸ + Ï†â€²
Ï eÏ†
Â¶
+
Ï†â€²
ÂµÂ¡
Î¸â€² cot Ï†
Â¢
eÎ¸ + Ï†â€² (âˆ’ÏeÏ) +
ÂµÏâ€²
Ï eÏ†
Â¶Â¶
+
Î¸â€²
Âµ
Î¸â€² Â¡
(âˆ’cos Ï† sin Ï†) eÏ† +
Â¡
âˆ’Ï sin2 Ï†
Â¢
eÏ
Â¢
+ Ï†â€² (cot Ï†) eÎ¸ + Ïâ€²
Ï eÎ¸
Â¶
and now all that remains is to collect the terms. Thus râ€²â€² equals
râ€²â€²
=
Â³
Ïâ€²â€² âˆ’Ï
Â¡
Ï†â€²Â¢2 âˆ’Ï
Â¡
Î¸â€²Â¢2 sin2 (Ï†)
Â´
eÏ +
Âµ
Ï†â€²â€² + 2Ïâ€²Ï†â€²
Ï
âˆ’
Â¡
Î¸â€²Â¢2 cos Ï† sin Ï†
Â¶
eÏ† +
+
Âµ
Î¸â€²â€² + 2Î¸â€²Ïâ€²
Ï
+ 2Ï†â€²Î¸â€² cot (Ï†)
Â¶
eÎ¸.
and this gives the acceleration in spherical coordinates. Note the prominent role played
by the chain rule. All of the above is done in books on mechanics for general curvilinear
coordinate systems and in the more general context, special theorems are developed which
make things go much faster but these theorems are all exercises in the chain rule.
As an example of how this could be used, consider a rocket. Suppose for simplicity that
it experiences a force only in the direction of eÏ, directly away from the earth. Of course

384THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?âˆ—
this force produces a corresponding acceleration which can be computed as a function of
time. As the fuel is burned, the rocket becomes less massive and so the acceleration will be
an increasing function of t. However, this would be a known function, say a (t). Suppose
you wanted to know the latitude and longitude of the rocket as a function of time. (There
is no reason to think these will stay the same.) Then all that would be required would be
to solve the system of diï¬€erential equations1,
Ïâ€²â€² âˆ’Ï
Â¡
Ï†â€²Â¢2 âˆ’Ï
Â¡
Î¸â€²Â¢2 sin2 (Ï†)
=
a (t) ,
Ï†â€²â€² + 2Ïâ€²Ï†â€²
Ï
âˆ’
Â¡
Î¸â€²Â¢2 cos Ï† sin Ï†
=
0,
Î¸â€²â€² + 2Î¸â€²Ïâ€²
Ï
+ 2Ï†â€²Î¸â€² cot (Ï†)
=
0
along with initial conditions, Ï (0) = Ï0 (the distance from the launch site to the center of
the earth.), Ïâ€² (0) = Ï1(the initial vertical component of velocity of the rocket, probably 0.)
and then initial conditions for Ï†, Ï†â€², Î¸, Î¸â€². The initial value problems could then be solved
numerically and you would know the distance from the center of the earth as a function of
t along with Î¸ and Ï†. Thus you could predict where the booster shells would fall to earth so
you would know where to look for them. Of course there are many variations of this. You
might want to specify forces in the eÎ¸ and eÏ† direction as well and attempt to control the
position of the rocket or rather its payload. The point is that if you are interested in doing
all this in terms of Ï†, Î¸, and Ï, the above shows how to do it systematically and you see it is
all an exercise in using the chain rule. More could be said here involving moving coordinate
systems and the Coriolis force. You really might want to do everything with respect to a
coordinate system which is ï¬xed with respect to the moving earth.
21.3
Proof Of The Chain Ruleâˆ—
As in the case of a function of one variable, it is important to consider the derivative
of a composition of two functions. The proof of the chain rule depends on the following
fundamental lemma.
Lemma 21.3.1 Let g : U â†’Rp where U is an open set in Rn and suppose g has a
derivative at x âˆˆU. Then o (g (x + v) âˆ’g (x)) = o (v) .
Proof: It is necessary to show
lim
vâ†’0
|o (g (x + v) âˆ’g (x))|
|v|
= 0.
(21.12)
From Lemma 21.1.8, there exists Î´ > 0 such that if |v| < Î´, then
|g (x + v) âˆ’g (x)| â‰¤(Cn + 1) |v| .
(21.13)
Now let Îµ > 0 be given. There exists Î· > 0 such that if |g (x + v) âˆ’g (x)| < Î·, then
|o (g (x + v) âˆ’g (x))| <
Âµ
Îµ
Cn + 1
Â¶
|g (x + v) âˆ’g (x)|
(21.14)
1You wonâ€™t be able to ï¬nd the solution to equations like these in terms of simple functions. The existence
of such functions is being assumed. The reason they exist often depends on the implicit function theorem,
a big theorem in advanced calculus.

21.3.
PROOF OF THE CHAIN RULEâˆ—
385
Let |v| < min
Â³
Î´,
Î·
Cn+1
Â´
. For such v, |g (x + v) âˆ’g (x)| â‰¤Î·, which implies
|o (g (x + v) âˆ’g (x))|
<
Âµ
Îµ
Cn + 1
Â¶
|g (x + v) âˆ’g (x)|
<
Âµ
Îµ
Cn + 1
Â¶
(Cn + 1) |v|
and so
|o (g (x + v) âˆ’g (x))|
|v|
< Îµ
which establishes 21.12. This proves the lemma.
Recall the notation f â—¦g (x) â‰¡f (g (x)) . Thus f â—¦g is the name of a function and this
function is deï¬ned by what was just written. The following theorem is known as the chain
rule.
Theorem 21.3.2 (Chain rule) Let U be an open set in Rn, let V be an open set
in Rp, let g : U â†’Rp be such that g (U) âŠ†V, and let f : V â†’Rq. Suppose Dg (x) exists
for some x âˆˆU and that Df (g (x)) exists. Then D (f â—¦g) (x) exists and furthermore,
D (f â—¦g) (x) = Df (g (x)) Dg (x) .
(21.15)
In particular,
âˆ‚(f â—¦g) (x)
âˆ‚xj
=
p
X
i=1
âˆ‚f (g (x))
âˆ‚yi
âˆ‚gi (x)
âˆ‚xj
.
(21.16)
Proof: From the assumption that Df (g (x)) exists,
f (g (x + v)) = f (g (x)) +
p
X
i=1
âˆ‚f (g (x))
âˆ‚yi
(gi (x + v) âˆ’gi (x)) + o (g (x + v) âˆ’g (x))
which by Lemma 21.3.1 equals
(f â—¦g) (x + v) = f (g (x + v)) = f (g (x)) +
p
X
i=1
âˆ‚f (g (x))
âˆ‚yi
(gi (x + v) âˆ’gi (x)) + o (v) .
Now since Dg (x) exists, the above becomes
(f â—¦g) (x + v)
=
f (g (x)) +
p
X
i=1
âˆ‚f (g (x))
âˆ‚yi
ï£«
ï£­
n
X
j=1
âˆ‚gi (x)
âˆ‚xj
vj + o (v)
ï£¶
ï£¸+ o (v)
=
f (g (x)) +
p
X
i=1
âˆ‚f (g (x))
âˆ‚yi
ï£«
ï£­
n
X
j=1
âˆ‚gi (x)
âˆ‚xj
vj
ï£¶
ï£¸+
p
X
i=1
âˆ‚f (g (x))
âˆ‚yi
o (v) + o (v)
=
(f â—¦g) (x) +
n
X
j=1
Ãƒ p
X
i=1
âˆ‚f (g (x))
âˆ‚yi
âˆ‚gi (x)
âˆ‚xj
!
vj + o (v)
because Pp
i=1
âˆ‚f(g(x))
âˆ‚yi
o (v)+o (v) = o (v) . This establishes 21.16 because of Theorem 21.0.9
on Page 372. Thus
(D (f â—¦g) (x))kj
=
p
X
i=1
âˆ‚fk (g (x))
âˆ‚yi
âˆ‚gi (x)
âˆ‚xj
=
p
X
i=1
Df (g (x))ki (Dg (x))ij .

386THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?âˆ—
Then 21.15 follows from the deï¬nition of matrix multiplication.
21.4
Proof Of The Second Derivative Testâˆ—
Deï¬nition 21.4.1 The matrix,
Â³
âˆ‚2f
âˆ‚xiâˆ‚xj (x)
Â´
is called the Hessian matrix, denoted
by H (x) .
Now recall the Taylor formula with the Lagrange form of the remainder. Since most
people donâ€™t pay any attention to this important topic when they take calculus, here is a
statement and proof of this theorem.
Theorem 21.4.2 Suppose f has n + 1 derivatives on an interval, (a, b) and let
c âˆˆ(a, b) . Then if x âˆˆ(a, b) , there exists Î¾ between c and x such that
f (x) = f (c) +
n
X
k=1
f (k) (c)
k!
(x âˆ’c)k + f (n+1) (Î¾)
(n + 1)! (x âˆ’c)n+1 .
(In this formula, the symbol P0
k=1 ak will denote the number 0.)
Proof: If n = 0 then the theorem is true because it is just the mean value theorem.
Suppose the theorem is true for nâˆ’1, n â‰¥1. It can be assumed x Ì¸= c because if x = c there
is nothing to show. Then there exists K such that
f (x) âˆ’
Ãƒ
f (c) +
n
X
k=1
f (k) (c)
k!
(x âˆ’c)k + K (x âˆ’c)n+1
!
= 0
(21.17)
In fact,
K =
âˆ’f (x) +
Â³
f (c) + Pn
k=1
f (k)(c)
k!
(x âˆ’c)kÂ´
(x âˆ’c)n+1
.
Now deï¬ne F (t) for t in the closed interval determined by x and c by
F (t) â‰¡f (x) âˆ’
Ãƒ
f (t) +
n
X
k=1
f (k) (c)
k!
(x âˆ’t)k + K (x âˆ’t)n+1
!
.
The c in 21.17 got replaced by t.
Therefore, F (c) = 0 by the way K was chosen and also F (x) = 0. By the mean value
theorem or Rolleâ€™s theorem, there exists t1 between x and c such that F â€² (t1) = 0. Therefore,
0
=
f â€² (t1) âˆ’
n
X
k=1
f (k) (c)
k!
k (x âˆ’t1)kâˆ’1 âˆ’K (n + 1) (x âˆ’t1)n
=
f â€² (t1) âˆ’
Ãƒ
f â€² (c) +
nâˆ’1
X
k=1
f (k+1) (c)
k!
(x âˆ’t1)k
!
âˆ’K (n + 1) (x âˆ’t1)n
=
f â€² (t1) âˆ’
Ãƒ
f â€² (c) +
nâˆ’1
X
k=1
f â€²(k) (c)
k!
(x âˆ’t1)k
!
âˆ’K (n + 1) (x âˆ’t1)n
By induction applied to f â€², there exists Î¾ between x and t1 such that the above simpliï¬es
to
0
=
f â€²(n) (Î¾) (x âˆ’t1)n
n!
âˆ’K (n + 1) (x âˆ’t1)n
=
f (n+1) (Î¾) (x âˆ’t1)n
n!
âˆ’K (n + 1) (x âˆ’t1)n

21.4.
PROOF OF THE SECOND DERIVATIVE TESTâˆ—
387
therefore,
K = f (n+1) (Î¾)
(n + 1) n! = f (n+1) (Î¾)
(n + 1)!
and the formula is true for n. This proves the theorem.
The term f (n+1)(Î¾)
(n+1)! (x âˆ’c)n+1 , is called the remainder and this particular form of the
remainder is called the Lagrange form of the remainder.
Now let f : U â†’R where U is an open subset of Rn. Suppose f âˆˆC2 (U) . Let x âˆˆU
and let r > 0 be such that
B (x,r) âŠ†U.
Then for ||v|| < r consider
f (x+tv) âˆ’f (x) â‰¡h (t)
for t âˆˆ[0, 1] . Then from Taylorâ€™s theorem for the case where m = 2 and the chain rule,
using the repeated index summation convention and the chain rule,
hâ€² (t) = âˆ‚f
âˆ‚xi
(x + tv) vi, hâ€²â€² (t) =
âˆ‚2f
âˆ‚xjâˆ‚xi
(x + tv) vivj.
Thus
hâ€²â€² (t) = vT H (x + tv) v.
From Theorem 21.4.2 there exists t âˆˆ(0, 1) such that
f (x + v) = f (x) + âˆ‚f
âˆ‚xi
(x) vi+1
2vT H (x + tv) v
By the continuity of the second partial derivative
f (x + v) = f (x) + âˆ‡f (x) Â· v+1
2vT H (x) v+
1
2
Â¡
vT (H (x+tv) âˆ’H (x)) v
Â¢
(21.18)
where the last term satisï¬es
lim
|v|â†’0
1
2
Â¡
vT (H (x+tv) âˆ’H (x)) v
Â¢
|v|2
= 0
(21.19)
because of the continuity of the entries of H (x) .
Recall the following important theorem from linear algebra.
Theorem 21.4.3 If A is a real symmetric matrix, then A is Hermitian and there
exists a real unitary matrix, U such that U T AU = D where D is a diagonal matrix. In
particular, it has all real eigenvalues and an orthonormal basis of eigenvectors.
Theorem 21.4.4 Suppose x is a critical point for f. That is, suppose
âˆ‚f
âˆ‚xi (x) = 0
for each i. Then if H (x) has all positive eigenvalues, x is a local minimum. If H (x) has all
negative eigenvalues, then x is a local maximum. If H (x) has a positive eigenvalue, then
there exists a direction in which f has a local minimum at x, while if H (x) has a negative
eigenvalue, there exists a direction in which f has a local maximum at x.

388THE DERIVATIVE OF VECTOR VALUED FUNCTIONS, WHAT IS THE DERIVATIVE?âˆ—
Proof: Since âˆ‡f (x) = 0, formula 21.18 implies
f (x + v) = f (x) + 1
2vT H (x) v+1
2
Â¡
vT (H (x+tv) âˆ’H (x)) v
Â¢
(21.20)
and by continuity of the second derivatives, these mixed second derivatives are equal and
so H (x) is a symmetric matrix . Thus, by Theorem 21.4.3 H (x) has all real eigenvalues.
Suppose ï¬rst that H (x) has all positive eigenvalues and that all are larger than Î´2 > 0.
Then by this corollary, H (x) has an orthonormal basis of eigenvectors, {vi}n
i=1 and so if
u is an arbitrary vector, there exist scalars, ui such that u = Pn
j=1 ujvj. Taking the dot
product of both sides with vj it follows uj = u Â· vj. Thus
uT H (x) u
=
Ãƒ n
X
k=1
ukvT
k
!
H (x)
ï£«
ï£­
n
X
j=1
ujvj
ï£¶
ï£¸
=
X
k,j
ukvT
k H (x) vjuj
=
n
X
j=1
u2
jÎ»j â‰¥Î´2
n
X
j=1
u2
j = Î´2 |u|2 .
From 21.20 and 21.19, if v is small enough,
f (x + v) â‰¥f (x) + 1
2Î´2 |v|2 âˆ’1
4Î´2 |v|2 = f (x) + Î´2
4 |v|2 .
This shows the ï¬rst claim of the theorem. The second claim follows from similar reasoning.
Suppose H (x) has a positive eigenvalue Î»2. Then let v be an eigenvector for this eigenvalue.
Then from 21.20, replacing v with sv and letting t depend on s,
f (x+sv) = f (x) +1
2s2vT H (x) v+
1
2s2 Â¡
vT (H (x+tsv) âˆ’H (x)) v
Â¢
which implies
f (x+sv)
=
f (x) +1
2s2Î»2 |v|2 +1
2s2 Â¡
vT (H (x+tsv) âˆ’H (x)) v
Â¢
â‰¥
f (x) +1
4s2Î»2 |v|2
whenever s is small enough. Thus in the direction v the function has a local minimum at
x. The assertion about the local maximum in some direction follows similarly. This proves
the theorem.

Implicit Function Theoremâˆ—
The implicit function theorem is one of the greatest theorems in mathematics. There
are many versions of this theorem which are of far greater generality than the one given
here. The proof given here is like one found in one of Caratheodoryâ€™s books on the calculus
of variations. It is not as elegant as some of the others which are based on a contraction
mapping principle but it may be more accessible. However, it is an advanced topic. Donâ€™t
waste your time with it unless you have ï¬rst read and understood the material on rank and
determinants found in the chapter on the mathematical theory of determinants. You will
also need to use the extreme value theorem for a function of n variables and the chain rule
as well as everything about matrix multiplication.
Deï¬nition 22.0.5 Suppose U is an open set in Rn Ã— Rm and (x, y) will denote a
typical point of Rn Ã— Rm with x âˆˆRn and y âˆˆRm. Let f : U â†’Rp be in C1 (U) . Then
deï¬ne
D1f (x, y)
â‰¡
ï£«
ï£¬
ï£­
f1,x1 (x, y)
Â· Â· Â·
f1,xn (x, y)
...
...
fp,x1 (x, y)
Â· Â· Â·
fp,xn (x, y)
ï£¶
ï£·
ï£¸,
D2f (x, y)
â‰¡
ï£«
ï£¬
ï£­
f1,y1 (x, y)
Â· Â· Â·
f1,ym (x, y)
...
...
fp,y1 (x, y)
Â· Â· Â·
fp,ym (x, y)
ï£¶
ï£·
ï£¸.
Thus Df (x, y) is a p Ã— (n + m) matrix of the form
Df (x, y) =
Â¡
D1f (x, y)
|
D2f (x, y)
Â¢
.
Note that D1f (x, y) is an p Ã— n matrix and D2f (x, y) is a p Ã— m matrix.
389

390
IMPLICIT FUNCTION THEOREMâˆ—
Theorem 22.0.6 (implicit function theorem) Suppose U is an open set in RnÃ—Rm.
Let f : U â†’Rn be in C1 (U) and suppose
f (x0, y0) = 0, D1f (x0, y0)âˆ’1 exists.
(22.1)
Then there exist positive constants, Î´, Î·, such that for every y âˆˆB (y0, Î·) there exists a
unique x (y) âˆˆB (x0, Î´) such that
f (x (y) , y) = 0.
(22.2)
Furthermore, the mapping, y â†’x (y) is in C1 (B (y0, Î·)).
Proof: Let
f (x, y) =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
f1 (x, y)
f2 (x, y)
...
fn (x, y)
ï£¶
ï£·
ï£·
ï£·
ï£¸.
Deï¬ne for
Â¡
x1, Â· Â· Â·, xnÂ¢
âˆˆB (x0, Î´)
n and y âˆˆB (y0, Î·) the following matrix.
J
Â¡
x1, Â· Â· Â·, xn, y
Â¢
â‰¡
ï£«
ï£¬
ï£­
f1,x1
Â¡
x1, y
Â¢
Â· Â· Â·
f1,xn
Â¡
x1, y
Â¢
...
...
fn,x1 (xn, y)
Â· Â· Â·
fn,xn (xn, y)
ï£¶
ï£·
ï£¸.
Then by the assumption of continuity of all the partial derivatives and the extreme value
theorem, there exists r > 0 and Î´0, Î·0 > 0 such that if Î´ â‰¤Î´0 and Î· â‰¤Î·0, it follows that for
all
Â¡
x1, Â· Â· Â·, xnÂ¢
âˆˆB (x0, Î´)
n and y âˆˆB (y0, Î·),
det
Â¡
J
Â¡
x1, Â· Â· Â·, xn, y
Â¢Â¢
> r > 0.
(22.3)
and B (x0, Î´0)Ã— B (y0, Î·0) âŠ†U. By continuity of all the partial derivatives and the extreme
value theorem, it can also be assumed there exists a constant, K such that for all (x, y) âˆˆ
B (x0, Î´0)Ã— B (y0, Î·0) and i = 1, 2, Â· Â· Â·, n, the ith row of D2f (x, y) , given by D2fi (x, y)
satisï¬es
|D2fi (x, y)| < K,
(22.4)
and for all
Â¡
x1, Â· Â· Â·, xnÂ¢
âˆˆB (x0, Î´0)
n and y âˆˆB (y0, Î·0) the ith row of the matrix, J
Â¡
x1, Â· Â· Â·, xn, y
Â¢âˆ’1
which equals eT
i
Â³
J
Â¡
x1, Â· Â· Â·, xn, y
Â¢âˆ’1Â´
satisï¬es
Â¯Â¯Â¯eT
i
Â³
J
Â¡
x1, Â· Â· Â·, xn, y
Â¢âˆ’1Â´Â¯Â¯Â¯ < K.
(22.5)
(Recall that ei is the column vector consisting of all zeros except for a 1 in the ith position.)
To begin with it is shown that for a given y âˆˆB (y0, Î·) there is at most one x âˆˆB (x0, Î´)
such that f (x, y) = 0.
Pick y âˆˆB (y0, Î·) and suppose there exist x, z âˆˆB (x0, Î´) such that f (x, y) = f (z, y) =
0. Consider fi and let
h (t) â‰¡fi (x + t (z âˆ’x) , y) .
Then h (1) = h (0) and so by the mean value theorem, hâ€² (ti) = 0 for some ti âˆˆ(0, 1) .
Therefore, from the chain rule and for this value of ti,
hâ€² (ti) = Dfi (x + ti (z âˆ’x) , y) (z âˆ’x) = 0.
(22.6)

391
Then denote by xi the vector, x + ti (z âˆ’x) . It follows from 22.6 that
J
Â¡
x1, Â· Â· Â·, xn, y
Â¢
(z âˆ’x) = 0
and so from 22.3 z âˆ’x = 0. (The matrix, in the above is invertible since its determinant is
nonzero.) Now it will be shown that if Î· is chosen suï¬ƒciently small, then for all y âˆˆB (y0, Î·) ,
there exists a unique x (y) âˆˆB (x0, Î´) such that f (x (y) , y) = 0.
Claim: If Î· is small enough, then the function, hy (x) â‰¡|f (x, y)|2 achieves its minimum
value on B (x0, Î´) at a point of B (x0, Î´) . (The existence of a point in B (x0, Î´) at which hy
achieves its minimum follows from the extreme value theorem.)
Proof of claim: Suppose this is not the case. Then there exists a sequence Î·k â†’0
and for some yk having |ykâˆ’y0| < Î·k, the minimum of hykon B (x0, Î´) occurs on a point
of B (x0, Î´), xk such that |x0âˆ’xk| = Î´. Now taking a subsequence, still denoted by k, it
can be assumed that xk â†’x with |x âˆ’x0| = Î´ and yk â†’y0. This follows from the fact
that
n
x âˆˆB (x0, Î´) : |x âˆ’x0| = Î´
o
is a closed and bounded set and is therefore sequentially
compact.
Let Îµ > 0. Then for k large enough, the continuity of y â†’hy (x0) implies
hyk (x0) < Îµ because hy0 (x0) = 0 since f (x0, y0) = 0. Therefore, from the deï¬nition of xk,
it is also the case that hyk (xk) < Îµ. Passing to the limit yields hy0 (x) â‰¤Îµ. Since Îµ > 0
is arbitrary, it follows that hy0 (x) = 0 which contradicts the ï¬rst part of the argument in
which it was shown that for y âˆˆB (y0, Î·) there is at most one point, x of B (x0, Î´) where
f (x, y) = 0. Here two have been obtained, x0 and x. This proves the claim.
Choose Î· < Î·0 and also small enough that the above claim holds and let x (y) denote
a point of B (x0, Î´) at which the minimum of hy on B (x0, Î´) is achieved. Since x (y) is an
interior point, you can consider hy (x (y) + tv) for |t| small and conclude this function of t
has a zero derivative at t = 0. Now
hy (x (y) + tv) =
n
X
i=1
f 2
i (x (y) + tv, y)
and so from the chain rule,
d
dthy (x (y) + tv) =
n
X
i=1
2fi (x (y) + tv, y) âˆ‚fi (x (y) + tv, y)
âˆ‚xj
vj.
Therefore, letting t = 0, it is required that for every v,
n
X
i=1
2fi (x (y) , y) âˆ‚fi (x (y) , y)
âˆ‚xj
vj = 0.
In terms of matrices this reduces to
0 = 2f (x (y) , y)T D1f (x (y) , y) v
for every vector v. Therefore,
0 = f (x (y) , y)T D1f (x (y) , y)
From 22.3, it follows f (x (y) , y) = 0. This proves the existence of the function y â†’x (y)
such that f (x (y) , y) = 0 for all y âˆˆB (y0, Î·) .
It remains to verify this function is a C1 function. To do this, let y1 and y2 be points of
B (y0, Î·) . Then as before, consider the ith component of f and consider the same argument
using the mean value theorem to write
0 = fi (x (y1) , y1) âˆ’fi (x (y2) , y2)
= fi (x (y1) , y1) âˆ’fi (x (y2) , y1) + fi (x (y2) , y1) âˆ’fi (x (y2) , y2)
= D1fi
Â¡
xi, y1
Â¢
(x (y1) âˆ’x (y2)) + D2fi
Â¡
x (y2) , yiÂ¢
(y1 âˆ’y2) .
(22.7)

392
IMPLICIT FUNCTION THEOREMâˆ—
where yi is a point on the line segment joining y1 and y2. Thus from 22.4 and the Cauchy
Schwarz inequality,
Â¯Â¯D2fi
Â¡
x (y2) , yiÂ¢
(y1 âˆ’y2)
Â¯Â¯ â‰¤K |y1 âˆ’y2| .
Therefore, letting M
Â¡
y1, Â· Â· Â·, ynÂ¢
â‰¡M denote the matrix having the ith row equal to
D2fi
Â¡
x (y2) , yiÂ¢
, it follows
|M (y1 âˆ’y2)| â‰¤
ÃƒX
i
K2 |y1 âˆ’y2|2
!1/2
= âˆšmK |y1 âˆ’y2| .
(22.8)
Also, from 22.7,
J
Â¡
x1, Â· Â· Â·, xn, y1
Â¢
(x (y1) âˆ’x (y2)) = âˆ’M (y1 âˆ’y2)
(22.9)
and so from 22.8 and 22.10,
|x (y1) âˆ’x (y2)|
=
Â¯Â¯Â¯J
Â¡
x1, Â· Â· Â·, xn, y1
Â¢âˆ’1 M (y1 âˆ’y2)
Â¯Â¯Â¯
(22.10)
=
Ãƒ n
X
i=1
Â¯Â¯Â¯eT
i J
Â¡
x1, Â· Â· Â·, xn, y1
Â¢âˆ’1 M (y1 âˆ’y2)
Â¯Â¯Â¯
2
!1/2
â‰¤
Ãƒ n
X
i=1
K2 |M (y1 âˆ’y2)|2
!1/2
â‰¤
Ãƒ n
X
i=1
K2 Â¡âˆšmK |y1 âˆ’y2|
Â¢2
!1/2
=
K2âˆšmn |y1 âˆ’y2|
(22.11)
It follows as in the proof of the chain rule that
o (x (y + v) âˆ’x (y)) = o (v) .
(22.12)
Now let y âˆˆB (y0, Î·) and let |v| be suï¬ƒciently small that y + v âˆˆB (y0, Î·) . Then
0
=
f (x (y + v) , y + v) âˆ’f (x (y) , y)
=
f (x (y + v) , y + v) âˆ’f (x (y + v) , y) + f (x (y + v) , y) âˆ’f (x (y) , y)
= D2f (x (y + v) , y) v + D1f (x (y) , y) (x (y + v) âˆ’x (y)) + o (|x (y + v) âˆ’x (y)|)
=
D2f (x (y) , y) v + D1f (x (y) , y) (x (y + v) âˆ’x (y)) +
o (|x (y + v) âˆ’x (y)|) + (D2f (x (y + v) , y) vâˆ’D2f (x (y) , y) v)
=
D2f (x (y) , y) v + D1f (x (y) , y) (x (y + v) âˆ’x (y)) + o (v) .
Therefore,
x (y + v) âˆ’x (y) = âˆ’D1f (x (y) , y)âˆ’1 D2f (x (y) , y) v + o (v)
which shows that Dx (y) = âˆ’D1f (x (y) , y)âˆ’1 D2f (x (y) , y) and y â†’Dx (y) is continuous.
This proves the theorem.

22.1.
THE METHOD OF LAGRANGE MULTIPLIERS
393
22.1
The Method Of Lagrange Multipliers
As an application of the implicit function theorem, consider the method of Lagrange mul-
tipliers.
Recall the problem is to maximize or minimize a function subject to equality
constraints. Let f : U â†’R be a C1 function where U âŠ†Rn and let
gi (x) = 0, i = 1, Â· Â· Â·, m
(22.13)
be a collection of equality constraints with m < n. Now consider the system of nonlinear
equations
f (x)
=
a
gi (x)
=
0, i = 1, Â· Â· Â·, m.
Recall x0 is a local maximum if f (x0) â‰¥f (x) for all x near x0 which also satisï¬es the
constraints 22.13. A local minimum is deï¬ned similarly. Let F : U Ã— R â†’Rm+1 be deï¬ned
by
F (x,a) â‰¡
ï£«
ï£¬
ï£¬
ï£¬
ï£­
f (x) âˆ’a
g1 (x)
...
gm (x)
ï£¶
ï£·
ï£·
ï£·
ï£¸.
(22.14)
Now consider the m + 1 Ã— n matrix,
ï£«
ï£¬
ï£¬
ï£¬
ï£­
fx1 (x0)
Â· Â· Â·
fxn (x0)
g1x1 (x0)
Â· Â· Â·
g1xn (x0)
...
...
gmx1 (x0)
Â· Â· Â·
gmxn (x0)
ï£¶
ï£·
ï£·
ï£·
ï£¸.
If this matrix has rank m + 1 then some m + 1 Ã— m + 1 submatrix has nonzero determinant.
It follows from the implicit function theorem there exists m+1 variables, xi1, Â·Â·Â·, xim+1 such
that the system
F (x,a) = 0
(22.15)
speciï¬es these m + 1 variables as a function of the remaining n âˆ’(m + 1) variables and a
in an open set of Rnâˆ’m. Thus there is a solution (x,a) to 22.15 for some x close to x0
whenever a is in some open interval. Therefore, x0 cannot be either a local minimum or a
local maximum. It follows that if x0 is either a local maximum or a local minimum, then
the above matrix must have rank less than m + 1 which requires the rows to be linearly
dependent. Thus, there exist m scalars,
Î»1, Â· Â· Â·, Î»m,
and a scalar Âµ, not all zero such that
Âµ
ï£«
ï£¬
ï£­
fx1 (x0)
...
fxn (x0)
ï£¶
ï£·
ï£¸= Î»1
ï£«
ï£¬
ï£­
g1x1 (x0)
...
g1xn (x0)
ï£¶
ï£·
ï£¸+ Â· Â· Â· + Î»m
ï£«
ï£¬
ï£­
gmx1 (x0)
...
gmxn (x0)
ï£¶
ï£·
ï£¸.
(22.16)
If the column vectors
ï£«
ï£¬
ï£­
g1x1 (x0)
...
g1xn (x0)
ï£¶
ï£·
ï£¸, Â· Â· Â·
ï£«
ï£¬
ï£­
gmx1 (x0)
...
gmxn (x0)
ï£¶
ï£·
ï£¸
(22.17)

394
IMPLICIT FUNCTION THEOREMâˆ—
are linearly independent, then, Âµ Ì¸= 0 and dividing by Âµ yields an expression of the form
ï£«
ï£¬
ï£­
fx1 (x0)
...
fxn (x0)
ï£¶
ï£·
ï£¸= Î»1
ï£«
ï£¬
ï£­
g1x1 (x0)
...
g1xn (x0)
ï£¶
ï£·
ï£¸+ Â· Â· Â· + Î»m
ï£«
ï£¬
ï£­
gmx1 (x0)
...
gmxn (x0)
ï£¶
ï£·
ï£¸
(22.18)
at every point x0 which is either a local maximum or a local minimum. This proves the
following theorem.
Theorem 22.1.1 Let U be an open subset of Rn and let f : U â†’R be a C1
function. Then if x0 âˆˆU is either a local maximum or local minimum of f subject to the
constraints 22.13, then 22.16 must hold for some scalars Âµ, Î»1, Â· Â· Â·, Î»m not all equal to zero.
If the vectors in 22.17 are linearly independent, it follows that an equation of the form 22.18
holds.
22.2
The Local Structure Of C1 Mappings
Deï¬nition 22.2.1 Let U be an open set in Rn and let h : U â†’Rn. Then h is
called primitive if it is of the form
h (x) =
Â¡
x1
Â· Â· Â·
Î± (x)
Â· Â· Â·
xn
Â¢T .
Thus, h is primitive if it only changes one of the variables. A function, F : Rn â†’Rn is
called a ï¬‚ip if
F (x1, Â· Â· Â·, xk, Â· Â· Â·, xl, Â· Â· Â·, xn) = (x1, Â· Â· Â·, xl, Â· Â· Â·, xk, Â· Â· Â·, xn)T .
Thus a function is a ï¬‚ip if it interchanges two coordinates. Also, for m = 1, 2, Â· Â· Â·, n,
Pm (x) â‰¡
Â¡
x1
x2
Â· Â· Â·
xm
0
Â· Â· Â·
0
Â¢T
It turns out that if h (0) = 0,Dh (0)âˆ’1 exists, and h is C1 on U, then h can be written
as a composition of primitive functions and ï¬‚ips. This is a very interesting application of
the inverse function theorem.
Theorem 22.2.2 Let h : U â†’Rn be a C1 function with h (0) = 0,Dh (0)âˆ’1
exists. Then there an open set, V âŠ†U containing 0, ï¬‚ips, F1, Â· Â· Â·, Fnâˆ’1, and primitive
functions, Gn, Gnâˆ’1, Â· Â· Â·, G1 such that for x âˆˆV,
h (x) = F1 â—¦Â· Â· Â· â—¦Fnâˆ’1 â—¦Gn â—¦Gnâˆ’1 â—¦Â· Â· Â· â—¦G1 (x) .
Proof: Let
h1 (x) â‰¡h (x) =
Â¡
Î±1 (x)
Â· Â· Â·
Î±n (x)
Â¢T
Dh (0) e1 =
Â¡
Î±1,1 (0)
Â· Â· Â·
Î±n,1 (0)
Â¢T
where Î±k,1 denotes âˆ‚Î±k
âˆ‚x1 . Since Dh (0) is one to one, the right side of this expression cannot
be zero. Hence there exists some k such that Î±k,1 (0) Ì¸= 0. Now deï¬ne
G1 (x) â‰¡
Â¡
Î±k (x)
x2
Â· Â· Â·
xn
Â¢T

22.2.
THE LOCAL STRUCTURE OF C1 MAPPINGS
395
Then the matrix of DG (0) is of the form
ï£«
ï£¬
ï£¬
ï£¬
ï£­
Î±k,1 (0)
Â· Â· Â·
Â· Â· Â·
Î±k,n (0)
0
1
0
...
...
...
0
0
Â· Â· Â·
1
ï£¶
ï£·
ï£·
ï£·
ï£¸
and its determinant equals Î±k,1 (0) Ì¸= 0. Therefore, by the inverse function theorem, there
exists an open set, U1 containing 0 and an open set, V2 containing 0 such that G1 (U1) = V2
and G1 is one to one and onto such that it and its inverse are both C1. Let F1 denote the
ï¬‚ip which interchanges xk with x1. Now deï¬ne
h2 (y) â‰¡F1 â—¦h1 â—¦Gâˆ’1
1
(y)
Thus
h2 (G1 (x))
â‰¡
F1 â—¦h1 (x)
(22.19)
=
Â¡
Î±k (x)
Â· Â· Â·
Î±1 (x)
Â· Â· Â·
Î±n (x)
Â¢T
Therefore,
P1h2 (G1 (x)) =
Â¡
Î±k (x)
0
Â· Â· Â·
0
Â¢T .
Also
P1 (G1 (x)) =
Â¡
Î±k (x)
x2
Â· Â· Â·
xn
Â¢T
so P1h2 (y) = P1 (y) for all y âˆˆV2. Also, h2 (0) = 0 and Dh2 (0)âˆ’1 exists because of the
deï¬nition of h2 above and the chain rule. Also, since F2
1 = identity, it follows from 22.19
that
h (x) = h1 (x) = F1 â—¦h2 â—¦G1 (x) .
(22.20)
Suppose then that for m â‰¥2,
Pmâˆ’1hm (x) = Pmâˆ’1 (x)
(22.21)
for all x âˆˆUm, an open subset of U containing 0 and hm (0) = 0,Dhm (0)âˆ’1 exists. From
22.21, hm (x) must be of the form
hm (x) =
Â¡
x1
Â· Â· Â·
xmâˆ’1
Î±1 (x)
Â· Â· Â·
Î±n (x)
Â¢T
where these Î±k are diï¬€erent than the ones used earlier. Then
Dhm (0) em =
Â¡
0
Â· Â· Â·
0
Î±1,m (0)
Â· Â· Â·
Î±n,m (0)
Â¢T Ì¸= 0
because Dhm (0)âˆ’1 exists. Therefore, there exists a k such that Î±k,m (0) Ì¸= 0, not the same
k as before. Deï¬ne
Gm+1 (x) â‰¡
Â¡
x1
Â· Â· Â·
xmâˆ’1
Î±k (x)
xm+1
Â· Â· Â·
xn
Â¢T
(22.22)
Then Gm+1 (0) = 0 and DGm+1 (0)âˆ’1 exists similar to the above. In fact det (DGm+1 (0)) =
Î±k,m (0). Therefore, by the inverse function theorem, there exists an open set, Vm+1 contain-
ing 0 such that Vm+1 = Gm+1 (Um) with Gm+1 and its inverse being one to one continuous
and onto. Let Fm be the ï¬‚ip which ï¬‚ips xm and xk. Then deï¬ne hm+1 on Vm+1 by
hm+1 (y) = Fm â—¦hm â—¦Gâˆ’1
m+1 (y) .

396
IMPLICIT FUNCTION THEOREMâˆ—
Thus for x âˆˆUm,
hm+1 (Gm+1 (x)) = (Fm â—¦hm) (x) .
(22.23)
and consequently,
Fm â—¦hm+1 â—¦Gm+1 (x) = hm (x)
(22.24)
It follows
Pmhm+1 (Gm+1 (x))
=
Pm (Fm â—¦hm) (x)
=
Â¡
x1
Â· Â· Â·
xmâˆ’1
Î±k (x)
0
Â· Â· Â·
0
Â¢T
and
Pm (Gm+1 (x)) =
Â¡ x1
Â· Â· Â·
xmâˆ’1
Î±k (x)
0
Â· Â· Â·
0 Â¢T .
Therefore, for y âˆˆVm+1,
Pmhm+1 (y) = Pm (y) .
As before, hm+1 (0) = 0 and Dhm+1 (0)âˆ’1 exists. Therefore, we can apply 22.24 repeatedly,
obtaining the following:
h (x)
=
F1 â—¦h2 â—¦G1 (x)
=
F1 â—¦F2 â—¦h3 â—¦G2 â—¦G1 (x)
...
=
F1 â—¦Â· Â· Â· â—¦Fnâˆ’1 â—¦hn â—¦Gnâˆ’1 â—¦Â· Â· Â· â—¦G1 (x)
where
Pnâˆ’1hn (x) = Pnâˆ’1 (x) =
Â¡
x1
Â· Â· Â·
xnâˆ’1
0
Â¢T
and so hn (x) is of the form
hn (x) =
Â¡
x1
Â· Â· Â·
xnâˆ’1
Î± (x)
Â¢T .
Therefore, deï¬ne the primitive function, Gn (x) to equal hn (x). This proves the theorem.

Part IX
Multiple Integrals
397


399
Outcomes
Double Integrals
A. Compare the deï¬nition of the double integral to the method of repeated integration
geometrically.
B. Evaluate double integrals over a rectangle by repeated integration.
C. Apply a double integral to calculate the volume or mass of a solid.
Reading: Multivariable Calculus 3.1
Outcome Mapping:
A. J1
B. 1,2
C. 3,4
Double Integrals Over General Regions
A. Evaluate double integrals over general regions.
B. Evaluate double integrals by interpreting them as known volumes.
C. Rewrite a double integral changing the order of integration.
D. Apply double integrals to calculate volumes of solids.
E. Evaluate the physical characteristics of a plate such as mass, centroid, center of mass
and moment of inertia.
Reading: Multivariable Calculus 3.2
Outcome Mapping:
A. 1
B. 2
C. 3
D. 4
E. 5,6
Double Integrals in Polar Coordinates
A. Represent a region in both Cartesian and polar coordinates.
B. Evaluate double integrals in polar coordinates.
C. Convert a double integral in Cartesian coordinates to a double integral in polar coor-
dinates and then evaluate.
D. Evaluate areas and volumes using polar coordinates
E. Evaluate the physical characteristics of a plate such as centroid, mass, and center of
mass using polar coordinates.

400
F. Make conversions of algebraic expressions between Cartesian coordinates and cylin-
drical coordinates.
Reading: Multivariable Calculus 3.3
Outcome Mapping:
A. 1,4
B. 5a
C. 8
D. 5b,6,7
E. 5c
F. 9,10,11,12
Triple Integrals
A. Find the volume of a solid using triple integration in Cartesian coordinates.
B. Evaluate the physical characteristics of a solid such as mass, centroid and center of
mass using Cartesian coordinates.
Reading: Multivariable Calculus 3.4
Outcome Mapping:
A. 1,4,7
B. 2,4,7
Triple Integrals in Cylindrical Coordinats
A. Describe regions in both Cartesian coordinates and cylindrical coordinates.
B. Evaluate triple integrals using cylindrical coordinates.
C. Find volumes by applying triple integration in cylindrical coordinates.
D. Evaluate the physical characteristics of a solid such as mass, centroid and center of
mass using cylindrical coordinates.
Reading: Multivariable Calculus 3.5
Outcome Mapping:
A. 2,3
B. 5
C. 6
D. 7,8,11
Triple Integrals in Spherical Coordinats
A. Describe regions in both Cartesian coordinates and spherical coordinates.
B. Evaluate triple integrals using spherical coordinates.

401
C. Find volumes by applying triple integration in spherical coordinates.
D. Evaluate the physical characteristics of a solid such as mass, centroid and center of
mass using spherical coordinates.
E. Convert a triple integral in Cartesian coordinates to cylindrical or spherical coordinates
and then evaluate.
F. Make conversions of algebraic expressions between Cartesian coordinates and spherical
coordinates.
Reading: Multivariable Calculus 3.6
Outcome Mapping:
A. 1,2,3
B. 5
C. 6
D. 7
E. 10
F. 11,12,13,14,15
The Jacobian
A. Find the Jacobian of a transformation.
B. Change variables in a multiple integration to obtain a more simple integral and then
evaluate.
Reading: Multivariable Calculus 3.7
Outcome Mapping:
A. 1
B. 3,5,6

402

The Riemann Integral On Rn
23.1
Methods For Double Integrals 1 Nov.
Quiz
1. Maximize 2x + y subject to the condition that x2
4 + y2
9 â‰¤1.
2. A curve is formed from the intersection of the plane, 2x + 3y + z = 3 and the cylinder
x2 + y2 = 4. Find the point on this curve which is closest to (0, 0, 0) .
3. Find the points on y2x = 9 which are closest to (0, 0) .
This chapter is on the Riemann integral for a function of n variables.
It begins by
introducing the basic concepts and applications of the integral. The proofs of the theorems
involved are diï¬ƒcult and are left till the end. To begin with consider the problem of ï¬nding
the volume under a surface of the form z = f (x, y) where f (x, y) â‰¥0 and f (x, y) = 0 for
all (x, y) outside of some bounded set. To solve this problem, consider the following picture.

Q
QQ
z = f(x, y)
In this picture, the volume of the little prism which lies above the rectangle Q and the
graph of the function would lie between MQ (f) v (Q) and mQ (f) v (Q) where
MQ (f) â‰¡sup {f (x) : x âˆˆQ} , mQ (f) â‰¡inf {f (x) : x âˆˆQ} ,
(23.1)
and v (Q) is deï¬ned as the area of Q. Now consider the following picture.
403

404
THE RIEMANN INTEGRAL ON RN
In this picture, it is assumed f equals zero outside the circle and f is a bounded nonneg-
ative function. Then each of those little squares are the base of a prism of the sort in the
previous picture and the sum of the volumes of those prisms should be the volume under
the surface, z = f (x, y) . Therefore, the desired volume must lie between the two numbers,
X
Q
MQ (f) v (Q) and
X
Q
mQ (f) v (Q)
where the notation, P
Q MQ (f) v (Q) , means for each Q, take MQ (f) , multiply it by the
area of Q, v (Q) , and then add all these numbers together. Thus in P
Q MQ (f) v (Q) , adds
numbers which are at least as large as what is desired while in P
Q mQ (f) v (Q) numbers
are added which are at least as small as what is desired. Note this is a ï¬nite sum because
by assumption, f = 0 except for ï¬nitely many Q, namely those which intersect the circle.
The sum, P
Q MQ (f) v (Q) is called an upper sum, P
Q mQ (f) v (Q) is a lower sum, and
the desired volume is caught between these upper and lower sums.
None of this depends in any way on the function being nonnegative. It also does not
depend in any essential way on the function being deï¬ned on R2, although it is impossible to
draw meaningful pictures in higher dimensional cases. To deï¬ne the Riemann integral, it is
necessary to ï¬rst give a description of something called a grid. First you must understand
that something like [a, b] Ã— [c, d] is a rectangle in R2, having sides parallel to the axes. The
situation is illustrated in the following picture.
c
d
a
b
[a, b] Ã— [c, d]

23.1.
METHODS FOR DOUBLE INTEGRALS 1 NOV.
405
(x, y) âˆˆ[a, b] Ã— [c, d] , means x âˆˆ[a, b] and also y âˆˆ[c, d] and the points which do this
comprise the rectangle just as shown in the picture.
Deï¬nition 23.1.1 For i = 1, 2, let
Â©
Î±i
k
Âªâˆ
k=âˆ’âˆbe points on R which satisfy
lim
kâ†’âˆÎ±i
k = âˆ,
lim
kâ†’âˆ’âˆÎ±i
k = âˆ’âˆ, Î±i
k < Î±i
k+1.
(23.2)
For such sequences, deï¬ne a grid on R2 denoted by G or F as the collection of rectangles
of the form
Q =
Â£
Î±1
k, Î±1
k+1
Â¤
Ã—
Â£
Î±2
l , Î±2
l+1
Â¤
.
(23.3)
If G is a grid, another grid, F is a reï¬nement of G if every box of G is the union of boxes
of F.
For G a grid, the expression,
X
QâˆˆG
MQ (f) v (Q)
is called the upper sum associated with the grid, G as described above in the discussion of
the volume under a surface. Again, this means to take a rectangle from G multiply MQ (f)
deï¬ned in 23.1 by its area, v (Q) and sum all these products for every Q âˆˆG. The symbol,
X
QâˆˆG
mQ (f) v (Q) ,
called a lower sum, is deï¬ned similarly. With this preparation it is time to give a deï¬nition
of the Riemann integral of a function of two variables.
Deï¬nition 23.1.2 Let f : R2 â†’R be a bounded function which equals zero for all
(x, y) outside some bounded set. Then
R
f dV is deï¬ned to be the unique number which lies
between all upper sums and all lower sums. In the case of R2, it is common to replace the
V with A and write this symbol as
R
f dA where A stands for area.
This deï¬nition begs a diï¬ƒcult question. For which functions does there exist a unique
number between all the upper and lower sums? This interesting and fundamental question
is discussed in any advanced calculus book and may be seen in the appendix on the theory
of the Riemann integral. It is a hard problem which was only solved in the ï¬rst part of the
twentieth century. When it was solved, it was also realized that the Riemann integral was
not the right integral to use. First consider the question: How can the Riemann integral
be computed? Consider the following picture in which f equals zero outside the rectangle
[a, b] Ã— [c, d] .

406
THE RIEMANN INTEGRAL ON RN
It depicts a slice taken from the solid deï¬ned by {(x, y) : 0 â‰¤y â‰¤f (x, y)} . You see these
when you look at a loaf of bread. If you wanted to ï¬nd the volume of the loaf of bread, and
you knew the volume of each slice of bread, you could ï¬nd the volume of the whole loaf by
adding the volumes of individual slices. It is the same here. If you could ï¬nd the volume
of the slice represented in this picture, you could add these up and get the volume of the
solid. The slice in the picture corresponds to constant y and is assumed to be very thin,
having thickness equal to h. Denote the volume of the solid under the graph of z = f (x, y)
on [a, b] Ã— [c, y] by V (y) . Then
V (y + h) âˆ’V (y) â‰ˆh
Z b
a
f (x, y) dx
where the integral is obtained by ï¬xing y and integrating with respect to x. It is hoped that
the approximation would be increasingly good as h gets smaller. Thus, dividing by h and
taking a limit, it is expected that
V â€² (y) =
Z b
a
f (x, y) dx, V (c) = 0.
Therefore, the volume of the solid under the graph of z = f (x, y) is given by
Z d
c
ÃƒZ b
a
f (x, y) dx
!
dy
(23.4)
but this was also the result of
R
f dV. Therefore, it is expected that this is a way to evaluate
R
f dV. Note what has been gained here. A hard problem, ï¬nding
R
f dV, is reduced to a
sequence of easier problems. First do
Z b
a
f (x, y) dx
getting a function of y, say F (y) and then do
Z d
c
ÃƒZ b
a
f (x, y) dx
!
dy =
Z d
c
F (y) dy.
Of course there is nothing special about ï¬xing y ï¬rst. The same thing should be obtained
from the integral,
Z b
a
ÃƒZ d
c
f (x, y) dy
!
dx
(23.5)
These expressions in 23.4 and 23.5 are called iterated integrals.
They are tools for
evaluating
R
f dV which would be hard to ï¬nd otherwise.
In practice, the parenthesis
is usually omitted in these expressions. Thus
Z b
a
ÃƒZ d
c
f (x, y) dy
!
dx =
Z b
a
Z d
c
f (x, y) dy dx
and it is understood that you are to do the inside integral ï¬rst and then when you have
done it, obtaining a function of x, you integrate this function of x.
I have presented this for the case where f (x, y) â‰¥0 and the integral represents a vol-
ume, but there is no diï¬€erence in the general case where f is not necessarily nonnegative.

23.1.
METHODS FOR DOUBLE INTEGRALS 1 NOV.
407
Throughout, I have been assuming the notion of volume has some sort of independent mean-
ing. This assumption is nonsense and is one of many reasons the above explanation does
not rise to the level of a proof. It is only intended to make things plausible. A careful
presentation which is not for the faint of heart is in an appendix.
Another aspect of this is the notion of integrating a function which is deï¬ned on some
set, not on all R2. For example, suppose f is deï¬ned on the set, S âŠ†R2. What is meant by
R
S f dV ?
Deï¬nition 23.1.3 Let f : S â†’R where S is a subset of R2. Then denote by f1 the
function deï¬ned by
f1 (x, y) â‰¡
Â½
f (x, y) if (x, y) âˆˆS
0 if (x, y) /âˆˆS
.
Then
Z
S
f dV â‰¡
Z
f1 dV.
Example 23.1.4 Let f (x, y) = x2y + yx for (x, y) âˆˆ[0, 1] Ã— [0, 2] â‰¡R. Find
R
R f dV.
This is done using iterated integrals like those deï¬ned above. Thus
Z
R
f dV =
Z 1
0
Z 2
0
Â¡
x2y + yx
Â¢
dy dx.
The inside integral yields
Z 2
0
Â¡
x2y + yx
Â¢
dy = 2x2 + 2x
and now the process is completed by doing
R 1
0 to what was just obtained. Thus
Z 1
0
Z 2
0
Â¡
x2y + yx
Â¢
dy dx =
Z 1
0
Â¡
2x2 + 2x
Â¢
dx = 5
3.
If the integration is done in the opposite order, the same answer should be obtained.
Z 2
0
Z 1
0
Â¡
x2y + yx
Â¢
dx dy
Z 1
0
Â¡
x2y + yx
Â¢
dx = 5
6y
Now
Z 2
0
Z 1
0
Â¡
x2y + yx
Â¢
dx dy =
Z 2
0
Âµ5
6y
Â¶
dy = 5
3.
If a diï¬€erent answer had been obtained it would have been a sign that a mistake had been
made.
Example 23.1.5 Let f (x, y) = x2y + yx for (x, y) âˆˆR where R is the triangular region
deï¬ned to be in the ï¬rst quadrant, below the line y = x and to the left of the line x = 4.
Find
R
R f dV.

408
THE RIEMANN INTEGRAL ON RN
x
y
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
4
R
Now from the above discussion,
Z
R
f dV =
Z 4
0
Z x
0
Â¡
x2y + yx
Â¢
dy dx
The reason for this is that x goes from 0 to 4 and for each ï¬xed x between 0 and 4, y goes
from 0 to the slanted line, y = x. Thus y goes from 0 to x. This explains the inside integral.
Now
R x
0
Â¡
x2y + yx
Â¢
dy = 1
2x4 + 1
2x3 and so
Z
R
f dV =
Z 4
0
Âµ1
2x4 + 1
2x3
Â¶
dx = 672
5 .
What of integration in a diï¬€erent order? Lets put the integral with respect to y on the
outside and the integral with respect to x on the inside. Then
Z
R
f dV =
Z 4
0
Z 4
y
Â¡
x2y + yx
Â¢
dx dy
For each y between 0 and 4, the variable x, goes from y to 4.
Z 4
y
Â¡
x2y + yx
Â¢
dx = 88
3 y âˆ’1
3y4 âˆ’1
2y3
Now
Z
R
f dV =
Z 4
0
Âµ88
3 y âˆ’1
3y4 âˆ’1
2y3
Â¶
dy = 672
5 .
Here is a similar example.
Example 23.1.6 Let f (x, y) = x2y for (x, y) âˆˆR where R is the triangular region deï¬ned
to be in the ï¬rst quadrant, below the line y = 2x and to the left of the line x = 4. Find
R
R f dV.
x
y
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢Â¢
4
R
Put the integral with respect to x on the outside ï¬rst. Then
Z
R
f dV =
Z 4
0
Z 2x
0
Â¡
x2y
Â¢
dy dx

23.1.
METHODS FOR DOUBLE INTEGRALS 1 NOV.
409
because for each x âˆˆ[0, 4] , y goes from 0 to 2x. Then
Z 2x
0
Â¡
x2y
Â¢
dy = 2x4
and so
Z
R
f dV =
Z 4
0
Â¡
2x4Â¢
dx = 2048
5
Now do the integral in the other order. Here the integral with respect to y will be on
the outside. What are the limits of this integral? Look at the triangle and note that x goes
from 0 to 4 and so 2x = y goes from 0 to 8. Now for ï¬xed y between 0 and 8, where does x
go? It goes from the x coordinate on the line y = 2x which corresponds to this y to 4. What
is the x coordinate on this line which goes with y? It is x = y/2. Therefore, the iterated
integral is
Z 8
0
Z 4
y/2
Â¡
x2y
Â¢
dx dy.
Now
Z 4
y/2
Â¡
x2y
Â¢
dx = 64
3 y âˆ’1
24y4
and so
Z
R
f dV =
Z 8
0
Âµ64
3 y âˆ’1
24y4
Â¶
dy = 2048
5
the same answer.
A few observations are in order here. In ï¬nding
R
S f dV there is no problem in setting
things up if S is a rectangle. However, if S is not a rectangle, the procedure always is
agonizing. A good rule of thumb is that if what you do is easy it will be wrong. There
are no shortcuts! There are no quick ï¬xes which require no thought! Pain and suï¬€ering
is inevitable and you must not expect it to be otherwise. Always draw a picture and then
begin agonizing over the correct limits. Even when you are careful you will make lots of
mistakes until you get used to the process.
Sometimes an integral can be evaluated in one order but not in another.
Example 23.1.7 For R as shown below, ï¬nd
R
R sin
Â¡
y2Â¢
dV.
x
8
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢Â¢
4
R
Setting this up to have the integral with respect to y on the inside yields
Z 4
0
Z 8
2x
sin
Â¡
y2Â¢
dy dx.
Unfortunately, there is no antiderivative in terms of elementary functions for sin
Â¡
y2Â¢
so
there is an immediate problem in evaluating the inside integral. It doesnâ€™t work out so the

410
THE RIEMANN INTEGRAL ON RN
next step is to do the integration in another order and see if some progress can be made.
This yields
Z 8
0
Z y/2
0
sin
Â¡
y2Â¢
dx dy =
Z 8
0
y
2 sin
Â¡
y2Â¢
dy
and
R 8
0
y
2 sin
Â¡
y2Â¢
dy = âˆ’1
4 cos 64 + 1
4 which you can verify by making the substitution,
u = y2. Thus
Z
R
sin
Â¡
y2Â¢
dy = âˆ’1
4 cos 64 + 1
4.
This illustrates an important idea. The integral
R
R sin
Â¡
y2Â¢
dV is deï¬ned as a number.
It is the unique number between all the upper sums and all the lower sums. Finding it is
another matter. In this case it was possible to ï¬nd it using one order of integration but
not the other. The iterated integral in this other order also is deï¬ned as a number but it
canâ€™t be found directly without interchanging the order of integration. Of course sometimes
nothing you try will work out.
23.1.1
Density Mass And Center Of Mass
Consider a two dimensional material.
Of course there is no such thing but a ï¬‚at plate
might be modeled as one. The density Ï is a function of position and is deï¬ned as follows.
Consider a small chunk of area, dV located at the point whose Cartesian coordinates are
(x, y) . Then the mass of this small chunk of material is given by Ï (x, y) dV. Thus if the
material occupies a region in two dimensional space, U, the total mass of this material is
Z
U
Ï dV
In other words you integrate the density to get the mass. Now by letting Ï depend on
position, you can include the case where the material is not homogeneous.
Here is an
example.
Example 23.1.8 Let Ï (x, y) denote the density of the plane region determined by the curves
1
3x + y = 2, x = 3y2, and x = 9y. Find the total mass if Ï (x, y) = y.
You need to ï¬rst draw a picture of the region, R. A rough sketch follows.
PPPPPPP
P
             
(3, 1)
(9/2, 1/2)
(0, 0)
x = 3y2
(1/3)x + y = 2
x = 9y
This region is in two pieces, one having the graph of x = 9y on the bottom and the
graph of x = 3y2 on the top and another piece having the graph of x = 9y on the bottom
and the graph of 1
3x + y = 2 on the top. Therefore, in setting up the integrals, with the
integral with respect to x on the outside, the double integral equals the following sum of
iterated integrals.
has x=3y2 on top
z
}|
{
Z 3
0
Z âˆš
x/3
x/9
y dy dx +
has 1
3 x+y=2 on top
z
}|
{
Z
9
2
3
Z 2âˆ’1
3 x
x/9
y dy dx

23.2.
DOUBLE INTEGRALS IN POLAR COORDINATES
411
You notice it is not necessary to have a perfect picture, just one which is good enough to
ï¬gure out what the limits should be. The dividing line between the two cases is x = 3 and
this was shown in the picture. Now it is only a matter of evaluating the iterated integrals
which in this case is routine and gives 1.
The concept of center of mass of a plate occupying the bounded open set, U is also
easy to express in terms of double integrals. Letting Ï denote the density of the plate, the
moment of a small chunk of mass having coordinates (x, y) about the y axis is xÏ (x, y) dV
and the moment of the same small chunk of mass about the x axis is yÏ (x, y) dV. Therefore
the center of mass, (xc, yc) is deï¬ned in the usual way.
Deï¬nition 23.1.9 The center of mass of a plate occuying the bounded open set, U
is deï¬ned as (xc, yc) where
xc â‰¡
R
U xÏ (x, y) dV
R
U Ï (x, y) dV , yc â‰¡
R
U yÏ (x, y) dV
R
U Ï (x, y) dV .
In other words, the total moment about the y axis equals xc times the total mass. That
is, if you placed the total mass at the single point, (xc, yc) this point mass would produce
the same moments about the x and y axes as the original plate.
Example 23.1.10 In Example 23.1.8, suppose the density is Ï (x, y) = y as it is in that
example. Find the total mass and the center of mass.
First, the total mass was found above. Then the center of mass is
xc =
R 3
0
R âˆš
x/3
x/9
xy dy dx +
R 9
2
3
R 2âˆ’1
3 x
x/9
xy dy dx
R 9
2
3
R 2âˆ’1
3 x
x/9
y dy dx +
R 3
0
R âˆš
x/3
x/9
y dy dx
=
39
16
1 = 39
16
yc =
R 3
0
R âˆš
x/3
x/9
y2 dy dx +
R 9
2
3
R 2âˆ’1
3 x
x/9
y2 dy dx
R 9
2
3
R 2âˆ’1
3 x
x/9
y dy dx +
R 3
0
R âˆš
x/3
x/9
y dy dx
= 47
80.
Thus the center of mass is
Â¡ 39
16, 47
80
Â¢
.
23.2
Double Integrals In Polar Coordinates
Remember polar coordinates,
x = r cos Î¸
y = r sin Î¸
where Î¸ âˆˆ[0, 2Ï€] and r > 0. If you assign a given value to r, the points obtained yield a circle
and if you give a value to Î¸ the points yield a ray from the origin. Thus assigning many
diï¬€erent values for r and many diï¬€erent values for Î¸ yields a grid of the sort illustrated in
the following picture.

412
THE RIEMANN INTEGRAL ON RN
     






Â¥
Â¥
Â¥
Â¥
Â¥
J
J
J
J
J
D
D
D
D
D
Q
Q
Q
Q
By contrast, the grid on the right is obtained by assigning diï¬€erent values for x and y.
For the grid on the right, if the vertical lines are dx apart and the horizontal lines are dy
apart, the area of one of those little boxes would be dxdy. This is the increment of area
in rectangular coordinates. Now consider the grid on the left which is obtained by setting
each of the two polar variables equal to various constants. What is the area of one of those
little curvy rectangles if the values for r and Î¸ are very small? Zoom in on one of them as
illustrated in the following picture.
dr
rdÎ¸
The angle between the two straight lines is dÎ¸ and so the length of one of the curved
sides is approximately rdÎ¸ while the length of the straight sides is dr. Therefore, the area
of the little curvy rectangle is approximately equal to rdrdÎ¸. This is the increment of area
in polar coordinates.
Later, I will present a uniï¬ed way to change variables. For now, consider the following
problems which illustrate the use of polar coordinates to compute integrals over areas.
Example 23.2.1 Find the area of a circle of radius R.
Denote by D this circle. Then the area of the circle is
R
dA and you need to write dA in
terms of polar coordinates. As described above, dA = rdrdÎ¸. To compute the integral, note
that in terms of the variables, Î¸ and r, this region is actually the rectangle, [0, R] Ã— [0, 2Ï€] .
Therefore, the integral equals
Z 2Ï€
0
Z R
0
rdrdÎ¸ = Ï€R2
which you have already heard about.
Example 23.2.2 Find the volume of the ball of radius R.
It is enough to ï¬nd the volume of the top half of this ball and then multiply it by 2.
Corresponding to the small curvy rectangle as described above having polar coordinates (r, Î¸)
the height of the ball over this point is
âˆš
R2 âˆ’r2. Therefore, the volume of a small prism
having as a base the small curvy rectangle described above is
âˆš
R2 âˆ’r2rdrdÎ¸. Summing
these using the integral, the desired volume is
Z
D
p
R2 âˆ’r2dA =
Z 2Ï€
0
Z R
0
p
R2 âˆ’r2rdrdÎ¸ = 2
3Ï€R3

23.2.
DOUBLE INTEGRALS IN POLAR COORDINATES
413
and so the volume of the whole ball is
4
3Ï€R3
which is another formula you might have seen.
Example 23.2.3 Find the area inside r = 1 + cos Î¸ for Î¸ âˆˆ[0, 2Ï€] .
This is the graph of a cardioid. You saw this in beginning calculus. Let its inside be
denoted by C for cardioid. Then the desired area is
Z
C
dA
and you need to set up an iterated integral and put in the correct form for dA. For each Î¸,
you have that r goes from 0 to 1 + cos Î¸. Therefore, the desired area is given by the iterated
integral,
Z 2Ï€
0
Z 1+cos Î¸
0
rdrdÎ¸ = 3
2Ï€
This was really easy because of polar coordinates. If you try to do this in rectangular
coordinates it will not work very well.
Example 23.2.4 A plate occupies the region inside the curve, r = 2 cos Î¸ for Î¸ âˆˆ
Â£
âˆ’Ï€
2 , Ï€
2
Â¤
.
The density in terms of polar coordinates is Î´ = r. Find the mass and center of mass of this
plate.
First of all the mass is given by
Z Ï€/2
âˆ’Ï€/2
Z 2 cos(Î¸)
0
r2drdÎ¸ = 32
9
The volume element is rdrdÎ¸ and I summed these up multiplied, by the density which was
r and that is the above integral.
Now to compute the center of mass, recall
xc â‰¡
R
U xÏ (x, y) dV
R
U Ï (x, y) dV , yc â‰¡
R
U yÏ (x, y) dV
R
U Ï (x, y) dV
I need to place x and y in terms of the polar coordinates. Thus x = r cos Î¸, y = r sin Î¸. A
small contribution to the moment about the y axis is r cos (Î¸) Ã— r Ã— rdrdÎ¸. Thus
xc =
R Ï€/2
âˆ’Ï€/2
R 2 cos(Î¸)
0
r2 cos (Î¸)
dA
z }| {
rdrdÎ¸
R Ï€/2
âˆ’Ï€/2
R 2 cos(Î¸)
0
r Ã—
dA
z }| {
rdrdÎ¸
= 6
5.
Similarly,
yc =
R Ï€/2
âˆ’Ï€/2
R 2 cos(Î¸)
0
r2 sin (Î¸) rdrdÎ¸
R Ï€/2
âˆ’Ï€/2
R 2 cos(Î¸)
0
r2drdÎ¸
= 0
Example 23.2.5 Let a plate occupy the region, C which is inside the polar graph, r =
2 + cos Î¸ for Î¸ âˆˆ[0, 2Ï€]. Suppose the density of this plate is given by Î´ (r, Î¸) = r. Find the
mass and center of mass of the plate.

414
THE RIEMANN INTEGRAL ON RN
Here you need to evaluate the following to get the total mass.
Z 2Ï€
0
Z 2+cos Î¸
0
r Ã— rdrdÎ¸ =
Z 2Ï€
0
Z 2+cos Î¸
0
r2drdÎ¸ = 22
3 Ï€
Now recall the center of mass is given by
xc â‰¡
R
U xÏ (x, y) dV
R
U Ï (x, y) dV , yc â‰¡
R
U yÏ (x, y) dV
R
U Ï (x, y) dV
Thus
xc =
R 2Ï€
0
R 2+cos Î¸
0
(r cos Î¸) r2drdÎ¸
22
3 Ï€
= 57
44
and
yc =
R 2Ï€
0
R 2+cos Î¸
0
(r sin Î¸) r2drdÎ¸
22
3 Ï€
= 0
I think this might be impossible if you tried to do it in rectangular coordinates. However,
it is just a little tedious in polar coordinates. Be sure you understand the set up. This is
usually the thing which gives people the most trouble in these kinds of problems.
Example 23.2.6 Let f (x, y) = sin
Â¡
x2 + y2Â¢
for (x, y) in the circle, D =
Â©
(x, y) : x2 + y2 â‰¤9
Âª
.
Find
Z
D
fdA.
You donâ€™t want to try this in rectangular coordinates even though the function is given in
rectangular coordinates. You should change it to polar coordinates for two reasons. The ï¬rst
is that x2 + y2 = r2 and it is easier to look at sin
Â¡
r2Â¢
than sin
Â¡
x2 + y2Â¢
. The main reason
is that the integration is taking place on a circle which is a rectangle in polar coordinates
and as explained earlier, it is easy to integrate over rectangles. In this case the rectangle is
[0, 2Ï€] Ã— [0, 3] . Thus the integral to work is
Z 2Ï€
0
Z 3
0
sin
Â¡
r2Â¢
dA
z }| {
rdrdÎ¸ = âˆ’Ï€ cos 9 + Ï€.
Example 23.2.7 Remember the formula for the area between two polar graphs r = f (Î¸)
and r = g (Î¸) , g (Î¸) > f (Î¸) for Î¸ âˆˆ[a, b] is given by
1
2
Z b
a
Â³
g (Î¸)2 âˆ’f (Î¸)2Â´
dÎ¸.
Show this formula from one variable calculus follows from the form of the area increment
given here.
Denote by R the region between the two graphs. Then you need to ï¬nd
Z
R
dA =
Z b
a
Z g(Î¸)
f(Î¸)
rdrdÎ¸ = 1
2
Z b
a
Â³
g (Î¸)2 âˆ’f (Î¸)2Â´
dÎ¸
(23.6)
which is the formula done earlier.
Here is an example.

23.2.
DOUBLE INTEGRALS IN POLAR COORDINATES
415
Example 23.2.8 Find the area of the region inside the cardioid, r = 1 + cos Î¸ and outside
the circle, r = 1 for Î¸ âˆˆ
Â£
âˆ’Ï€
2 , Ï€
2
Â¤
.
As is usual in such cases, it is a good idea to graph the curves involved to get an idea
what is wanted. It is very important to ï¬gure out which function is farther from the origin.
1
0
0.5
-0.5
-1
2
1.5
-0.5
1
-1
0
0.5
desired
region
Then you need
Z Ï€/2
âˆ’Ï€/2
Z 1+cos Î¸
1
rdrdÎ¸ = 2 + 1
4Ï€
You could also work it using the formula derived in 23.6 which is like what you did in one
variable calculus.
Example 23.2.9 Let f (x, y) = ex2+y2 for (x, y) in the pie shaped region P deï¬ned by
r âˆˆ[0, 2] and Î¸ âˆˆ[0, Ï€/6] .
Be sure you can see why the integral wanted is
Z 2
0
Z Ï€/6
0
er2rdÎ¸dr = 1
12e4Ï€ âˆ’1
12Ï€
Example 23.2.10 Find
Z 1
âˆ’1
Z âˆš
1âˆ’x2
âˆ’
âˆš
1âˆ’x2
p
1 + x2 + y2dydx.
In this example you are integrating the function, f (x, y) =
p
1 + x2 + y2 over the circle
of radius 1 centered at the origin. Therefore, changing to polar coordinates it equals
Z 2Ï€
0
Z 1
0
p
1 + r2rdrdÎ¸ = 4
3Ï€
âˆš
2 âˆ’2
3Ï€
In this case, I think you could have done it without changing to polar coordinates but it
would involve wading through much aï¬„iction and sorrow. Of course if you like adversity,
you could try to do it this way.
Example 23.2.11 Find
R âˆ
0
eâˆ’x2dx.

416
THE RIEMANN INTEGRAL ON RN
Let I =
R âˆ
0
eâˆ’x2dx. Then
I2
=
ÂµZ âˆ
0
eâˆ’x2dx
Â¶ ÂµZ âˆ
0
eâˆ’y2dy
Â¶
=
Z âˆ
0
Z âˆ
0
eâˆ’(x2+y2)dxdy
=
Z Ï€/2
0
Z âˆ
0
eâˆ’r2rdrdÎ¸ = 1
4Ï€
It follows I =
âˆšÏ€
2 . This is a very important formula. You showed, (hopefully) in one variable
calculus that this integral exists. Now with the aid of polar coordinates you can actually
ï¬nd it.
23.3
Methods For Triple Integrals 2-7 Nov.
23.3.1
Deï¬nition Of The Integral
The integral of a function of three variables is similar to the integral of a function of two
variables.
Deï¬nition 23.3.1 For i = 1, 2, 3 let
Â©
Î±i
k
Âªâˆ
k=âˆ’âˆbe points on R which satisfy
lim
kâ†’âˆÎ±i
k = âˆ,
lim
kâ†’âˆ’âˆÎ±i
k = âˆ’âˆ, Î±i
k < Î±i
k+1.
(23.7)
For such sequences, deï¬ne a grid on R3 denoted by G or F as the collection of boxes of the
form
Q =
Â£
Î±1
k, Î±1
k+1
Â¤
Ã—
Â£
Î±2
l , Î±2
l+1
Â¤
Ã—
Â£
Î±3
p, Î±3
p+1
Â¤
.
(23.8)
If G is a grid, F is called a reï¬nement of G if every box of G is the union of boxes of F.
For G a grid,
X
QâˆˆG
MQ (f) v (Q)
is the upper sum associated with the grid, G where
MQ (f) â‰¡sup {f (x) : x âˆˆQ}
and if Q = [a, b]Ã—[c, d]Ã—[e, f] , then v (Q) is the volume of Q given by (b âˆ’a) (d âˆ’c) (f âˆ’e) .
Letting
mQ (f) â‰¡inf {f (x) : x âˆˆQ}
the lower sum associated with this partition is
X
QâˆˆG
mQ (f) v (Q) ,
With this preparation it is time to give a deï¬nition of the Riemann integral of a function
of three variables. This deï¬nition is just like the one for a function of two variables.
Deï¬nition 23.3.2 Let f : R3 â†’R be a bounded function which equals zero outside
some bounded subset of R3.
R
f dV is deï¬ned as the unique number between all the upper
sums and lower sums.

23.3.
METHODS FOR TRIPLE INTEGRALS 2-7 NOV.
417
As in the case of a function of two variables there are all sorts of mathematical questions
which are dealt with later.
The way to think of integrals is as follows. Located at a point x, there is an â€œinï¬nites-
imalâ€ chunk of volume, dV. The integral involves taking this little chunk of volume, dV ,
multiplying it by f (x) and then adding up all such products. Upper sums are too large and
lower sums are too small but the unique number between all the lower and upper sums is
just right and corresponds to the notion of adding up all the f (x) dV. Even the notation
is suggestive of this concept of sum. It is a long thin S denoting sum. This is the fun-
damental concept for the integral in any number of dimensions and all the deï¬nitions and
technicalities are designed to give precision and mathematical respectability to this notion.
To consider how to evaluate triple integrals, imagine a sum of the form P
ijk aijk where
there are only ï¬nitely many choices for i, j, and k and the symbol means you simply add
up all the aijk. By the commutative law of addition, these may be added systematically in
the form, P
k
P
j
P
i aijk. A similar process is used to evaluate triple integrals and since
integrals are like sums, you might expect it to be valid. Speciï¬cally,
Z
f dV =
Z ?
?
Z ?
?
Z ?
?
f (x, y, z) dx dy dz.
In words, sum with respect to x and then sum what you get with respect to y and ï¬nally,
with respect to z. Of course this should hold in any other order such as
Z
f dV =
Z ?
?
Z ?
?
Z ?
?
f (x, y, z) dz dy dx.
This is proved in an appendix1.
Having discussed double and triple integrals, the deï¬nition of the integral of a function
of n variables is accomplished in the same way.
Deï¬nition 23.3.3 For i = 1, Â· Â· Â·, n, let
Â©
Î±i
k
Âªâˆ
k=âˆ’âˆbe points on R which satisfy
lim
kâ†’âˆÎ±i
k = âˆ,
lim
kâ†’âˆ’âˆÎ±i
k = âˆ’âˆ, Î±i
k < Î±i
k+1.
(23.9)
For such sequences, deï¬ne a grid on Rn denoted by G or F as the collection of boxes of the
form
Q =
n
Y
i=1
Â£
Î±i
ji, Î±i
ji+1
Â¤
.
(23.10)
If G is a grid, F is called a reï¬nement of G if every box of G is the union of boxes of F.
Deï¬nition 23.3.4 Let f be a bounded function which equals zero oï¬€a bounded set,
D, and let G be a grid. For Q âˆˆG, deï¬ne
MQ (f) â‰¡sup {f (x) : x âˆˆQ} , mQ (f) â‰¡inf {f (x) : x âˆˆQ} .
(23.11)
Also deï¬ne for Q a box, the volume of Q, denoted by v (Q) by
v (Q) â‰¡
n
Y
i=1
(bi âˆ’ai) , Q â‰¡
n
Y
i=1
[ai, bi] .
1All of these fundamental questions about integrals can be considered more easily in the context of the
Lebesgue integral. However, this integral is more abstract than the Riemann integral.

418
THE RIEMANN INTEGRAL ON RN
Now deï¬ne upper sums, UG (f) and lower sums, LG (f) with respect to the indicated grid,
by the formulas
UG (f) â‰¡
X
QâˆˆG
MQ (f) v (Q) , LG (f) â‰¡
X
QâˆˆG
mQ (f) v (Q) .
Then a function of n variables is Riemann integrable if there is a unique number between
all the upper and lower sums. This number is the value of the integral.
In this book most integrals will involve no more than three variables. However, this does
not mean an integral of a function of more than three variables is unimportant. Therefore,
I will begin to refer to the general case when theorems are stated.
Deï¬nition 23.3.5 For E âŠ†Rn,
XE (x) â‰¡
Â½
1 if x âˆˆE
0 if x /âˆˆE
.
Deï¬ne
R
E f dV â‰¡
R
XEf dV when fXE âˆˆR (Rn) .
23.3.2
Iterated Integrals
As before, the integral is often computed by using an iterated integral. In general it is
impossible to set up an iterated integral for ï¬nding
R
E fdV for arbitrary regions, E but
when the region is suï¬ƒciently simple, one can make progress. Suppose the region, E over
which the integral is to be taken is of the form E = {(x, y, z) : a (x, y) â‰¤z â‰¤b (x, y)} for
(x, y) âˆˆR, a two dimensional region. This is illustrated in the following picture in which
the bottom surface is the graph of z = a (x, y) and the top is the graph of z = b (x, y).
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
x
z
y
R
Then
Z
E
fdV =
Z
R
Z b(x,y)
a(x,y)
f (x, y, z) dzdA

23.3.
METHODS FOR TRIPLE INTEGRALS 2-7 NOV.
419
It might be helpful to think of dV = dzdA. Now
R b(x,y)
a(x,y) f (x, y, z) dz is a function of x
and y and so you have reduced the triple integral to a double integral over R of this
function of x and y. Similar reasoning would apply if the region in R3 were of the form
{(x, y, z) : a (y, z) â‰¤x â‰¤b (y, z)} or {(x, y, z) : a (x, z) â‰¤y â‰¤b (x, z)} .
Example 23.3.6 Find the volume of the region, E in the ï¬rst octant between z = 1âˆ’(x + y)
and z = 0.
In this case, R is the region shown.
@
@
@
@
@@
x
y
R
1
Thus the region, E is between the plane z = 1âˆ’(x + y) on the top, z = 0 on the bottom,
and over R shown above. Thus
Z
E
1dV
=
Z
R
Z 1âˆ’(x+y)
0
dzdA
=
Z 1
0
Z 1âˆ’x
0
Z 1âˆ’(x+y)
0
dzdydx = 1
6
Of course iterated integrals have a life of their own although this will not be explored
here. You can just write them down and go to work on them. Here are some examples.
Example 23.3.7 Find
R 3
2
R x
3
R x
3y (x âˆ’y) dz dy dx.
The inside integral yields
R x
3y (x âˆ’y) dz = x2 âˆ’4xy + 3y2. Next this must be integrated
with respect to y to give
R x
3
Â¡
x2 âˆ’4xy + 3y2Â¢
dy = âˆ’3x2 + 18x âˆ’27. Finally the third
integral gives
Z 3
2
Z x
3
Z x
3y
(x âˆ’y) dz dy dx =
Z 3
2
Â¡
âˆ’3x2 + 18x âˆ’27
Â¢
dx = âˆ’1.
Example 23.3.8 Find
R Ï€
0
R 3y
0
R y+z
0
cos (x + y) dx dz dy.
The inside integral is
R y+z
0
cos (x + y) dx = 2 cos z sin y cos y+2 sin z cos2 yâˆ’sin zâˆ’sin y.
Now this has to be integrated.
Z 3y
0
Z y+z
0
cos (x + y) dx dz =
Z 3y
0
Â¡
2 cos z sin y cos y + 2 sin z cos2 y âˆ’sin z âˆ’sin y
Â¢
dz
= âˆ’1 âˆ’16 cos5 y + 20 cos3 y âˆ’5 cos y âˆ’3 (sin y) y + 2 cos2 y.
Finally, this last expression must be integrated from 0 to Ï€. Thus
Z Ï€
0
Z 3y
0
Z y+z
0
cos (x + y) dx dz dy
=
Z Ï€
0
Â¡
âˆ’1 âˆ’16 cos5 y + 20 cos3 y âˆ’5 cos y âˆ’3 (sin y) y + 2 cos2 y
Â¢
dy
=
âˆ’3Ï€

420
THE RIEMANN INTEGRAL ON RN
Example 23.3.9 Here is an iterated integral:
R 2
0
R 3âˆ’3
2 x
0
R x2
0
dz dy dx. Write as an iterated
integral in the order dz dx dy.
The inside integral is just a function of x and y. (In fact, only a function of x.) The order
of the last two integrals must be interchanged. Thus the iterated integral which needs to be
done in a diï¬€erent order is
Z 2
0
Z 3âˆ’3
2 x
0
f (x, y) dy dx.
As usual, it is important to draw a picture and then go from there.
J
J
J
J
J
JJ
3 âˆ’3
2x = y
3
2
Thus this double integral equals
Z 3
0
Z
2
3 (3âˆ’y)
0
f (x, y) dx dy.
Now substituting in for f (x, y) ,
Z 3
0
Z
2
3 (3âˆ’y)
0
Z x2
0
dz dx dy.
Example 23.3.10 Find the volume of the bounded region determined by 3y + 3z = 2, x =
16 âˆ’y2, y = 0, x = 0.
In the yz plane, the following picture corresponds to x = 0.
@
@
@
@
@
@
3y + 3z = 2
2
3
2
3
Therefore, the outside integrals taken with respect to z and y are of the form
R 2
3
0
R 2
3 âˆ’y
0
dz dy
and now for any choice of (y, z) in the above triangular region, x goes from 0 to 16 âˆ’y2.
Therefore, the iterated integral is
Z
2
3
0
Z
2
3 âˆ’y
0
Z 16âˆ’y2
0
dx dz dy = 860
243
Example 23.3.11 Find the volume of the region determined by the intersection of the two
cylinders, x2 + y2 â‰¤9 and y2 + z2 â‰¤9.
The ï¬rst listed cylinder intersects the xy plane in the disk, x2 + y2 â‰¤9. What is the
volume of the three dimensional region which is between this disk and the two surfaces,
z =
p
9 âˆ’y2 and z = âˆ’
p
9 âˆ’y2? An iterated integral for the volume is
Z 3
âˆ’3
Z âˆš
9âˆ’y2
âˆ’âˆš
9âˆ’y2
Z âˆš
9âˆ’y2
âˆ’âˆš
9âˆ’y2 dz dx dy = 144.

23.3.
METHODS FOR TRIPLE INTEGRALS 2-7 NOV.
421
Note I drew no picture of the three dimensional region. If you are interested, here it is.
One of the cylinders is parallel to the z axis, x2 + y2 â‰¤9 and the other is parallel to
the x axis, y2 + z2 â‰¤9. I did not need to be able to draw such a nice picture in order to
work this problem. This is the key to doing these. Draw pictures in two dimensions and
reason from the two dimensional pictures rather than attempt to wax artistic and consider
all three dimensions at once. These problems are hard enough without making them even
harder by attempting to be an artist.
23.3.3
Mass And Density
As an example of the use of triple integrals, consider a solid occupying a set of points,
U âŠ†R3 having density Ï. Thus Ï is a function of position and the total mass of the solid
equals
Z
U
Ï dV.
This is just like the two dimensional case. The mass of an inï¬nitesimal chunk of the solid
located at x would be Ï (x) dV and so the total mass is just the sum of all these,
R
U Ï (x) dV.
Example 23.3.12 Find the volume of R where R is the bounded region formed by the plane
1
5x + y + 1
5z = 1 and the planes x = 0, y = 0, z = 0.
When z = 0, the plane becomes 1
5x + y = 1. Thus the intersection of this plane with the
xy plane is this line shown in the following picture.
``````````
1
5
Therefore, the bounded region is between the triangle formed in the above picture by
the x axis, the y axis and the above line and the surface given by 1
5x + y + 1
5z = 1 or
z = 5
Â¡
1 âˆ’
Â¡ 1
5x + y
Â¢Â¢
= 5 âˆ’x âˆ’5y. Therefore, an iterated integral which yields the volume
is
Z 5
0
Z 1âˆ’1
5 x
0
Z 5âˆ’xâˆ’5y
0
dz dy dx = 25
6 .
Example 23.3.13 Find the mass of the bounded region, R formed by the plane 1
3x + 1
3y +
1
5z = 1 and the planes x = 0, y = 0, z = 0 if the density is Ï (x, y, z) = z.

422
THE RIEMANN INTEGRAL ON RN
This is done just like the previous example except in this case there is a function to
integrate. Thus the answer is
Z 3
0
Z 3âˆ’x
0
Z 5âˆ’5
3 xâˆ’5
3 y
0
z dz dy dx = 75
8 .
Example 23.3.14 Find the total mass of the bounded solid determined by z = 9 âˆ’x2 âˆ’y2
and x, y, z â‰¥0 if the mass is given by Ï (x, y, z) = z
When z = 0 the surface, z = 9 âˆ’x2 âˆ’y2 intersects the xy plane in a circle of radius 3
centered at (0, 0) . Since x, y â‰¥0, it is only a quarter of a circle of interest, the part where
both these variables are nonnegative. For each (x, y) inside this quarter circle, z goes from
0 to 9 âˆ’x2 âˆ’y2. Therefore, the iterated integral is of the form,
Z 3
0
Z âˆš
(9âˆ’x2)
0
Z 9âˆ’x2âˆ’y2
0
z dz dy dx = 243
8 Ï€
Example 23.3.15 Find the volume of the bounded region determined by x â‰¥0, y â‰¥0, z â‰¥0,
and 1
7x + y + 1
4z = 1, and x + 1
7y + 1
4z = 1.
When z = 0, the plane 1
7x+y+ 1
4z = 1 intersects the xy plane in the line whose equation
is
1
7x + y = 1
while the plane, x + 1
7y + 1
4z = 1 intersects the xy plane in the line whose equation is
x + 1
7y = 1.
Furthermore, the two planes intersect when x = y as can be seen from the equations,
x + 1
7y = 1 âˆ’z
4 and 1
7x + y = 1 âˆ’z
4 which imply x = y. Thus the two dimensional picture
to look at is depicted in the following picture.
D
D
D
D
D
D
D
D
D
D
D
DD
`````````````
x + 1
7y + 1
4z = 1
y + 1
7x + 1
4z = 1
Â¡
Â¡
Â¡Â¡
R1
R2
y = x
You see in this picture, the base of the region in the xy plane is the union of the two
triangles, R1 and R2. For (x, y) âˆˆR1, z goes from 0 to what it needs to be to be on the
plane, 1
7x + y + 1
4z = 1. Thus z goes from 0 to 4
Â¡
1 âˆ’1
7x âˆ’y
Â¢
. Similarly, on R2, z goes from
0 to 4
Â¡
1 âˆ’1
7y âˆ’x
Â¢
. Therefore, the integral needed is
Z
R1
Z 4(1âˆ’1
7 xâˆ’y)
0
dz dV +
Z
R2
Z 4(1âˆ’1
7 yâˆ’x)
0
dz dV

23.3.
METHODS FOR TRIPLE INTEGRALS 2-7 NOV.
423
and now it only remains to consider
R
R1 dV and
R
R2 dV. The point of intersection of these
lines shown in the above picture is
Â¡ 7
8, 7
8
Â¢
and so an iterated integral is
Z 7/8
0
Z 1âˆ’x
7
x
Z 4(1âˆ’1
7 xâˆ’y)
0
dz dy dx +
Z 7/8
0
Z 1âˆ’y
7
y
Z 4(1âˆ’1
7 yâˆ’x)
0
dz dx dy = 7
6.
23.3.4
Exercises With Answers
1. Evaluate the integral
R 7
4
R 3x
5
R x
5y dz dy dx
Answer:
âˆ’3417
2
2. Find
R 4
0
R 2âˆ’5x
0
R 4âˆ’2xâˆ’y
0
(2x) dz dy dx
Answer:
âˆ’2464
3
3. Find
R 2
0
R 2âˆ’5x
0
R 1âˆ’4xâˆ’3y
0
(2x) dz dy dx
Answer:
âˆ’196
3
4. Evaluate the integral
R 8
5
R 3x
4
R x
4y (x âˆ’y) dz dy dx
Answer:
114 607
8
5. Evaluate the integral
R Ï€
0
R 4y
0
R y+z
0
cos (x + y) dx dz dy
Answer:
âˆ’4Ï€
6. Evaluate the integral
R Ï€
0
R 2y
0
R y+z
0
sin (x + y) dx dz dy
Answer:
âˆ’19
4
7. Fill in the missing limits.
R 1
0
R z
0
R z
0 f (x, y, z) dx dy dz =
R ?
?
R ?
?
R ?
? f (x, y, z) dx dz dy,
R 1
0
R z
0
R 2z
0
f (x, y, z) dx dy dz =
R ?
?
R ?
?
R ?
? f (x, y, z) dy dz dx,
R 1
0
R z
0
R z
0 f (x, y, z) dx dy dz =
R ?
?
R ?
?
R ?
? f (x, y, z) dz dy dx,
R 1
0
R âˆšz
z/2
R y+z
0
f (x, y, z) dx dy dz =
R ?
?
R ?
?
R ?
? f (x, y, z) dx dz dy,
R 7
5
R 5
2
R 3
0 f (x, y, z) dx dy dz =
R ?
?
R ?
?
R ?
? f (x, y, z) dz dy dx.
Answer:
R 1
0
R z
0
R z
0 f (x, y, z) dx dy dz =
R 1
0
R 1
y
R z
0 f (x, y, z) dx dz dy,
R 1
0
R z
0
R 2z
0
f (x, y, z) dx dy dz =
R 2
0
R 1
x/2
R z
0 f (x, y, z) dy dz dx,
R 1
0
R z
0
R z
0 f (x, y, z) dx dy dz =
R 1
0
hR x
0
R 1
x f (x, y, z) dz dy +
R 1
x
R 1
y f (x, y, z) dz dy
i
dx,
R 1
0
R âˆšz
z/2
R y+z
0
f (x, y, z) dx dy dz =

424
THE RIEMANN INTEGRAL ON RN
R 1/2
0
R 2y
y2
R y+z
0
f (x, y, z) dx dz dy +
R 1
1/2
R 1
y2
R y+z
0
f (x, y, z) dx dz dy
R 7
5
R 5
2
R 3
0 f (x, y, z) dx dy dz =
R 3
0
R 5
2
R 7
5 f (x, y, z) dz dy dx
8. Find the volume of R where R is the bounded region formed by the plane 1
5x+y+ 1
4z =
1 and the planes x = 0, y = 0, z = 0.
Answer:
R 5
0
R 1âˆ’1
5 x
0
R 4âˆ’4
5 xâˆ’4y
0
dz dy dx = 10
3
9. Find the volume of R where R is the bounded region formed by the plane 1
5x+ 1
2y+ 1
4z =
1 and the planes x = 0, y = 0, z = 0.
Answer:
R 5
0
R 2âˆ’2
5 x
0
R 4âˆ’4
5 xâˆ’2y
0
dz dy dx = 20
3
10. Find the mass of the bounded region, R formed by the plane 1
4x + 1
2y + 1
3z = 1 and
the planes x = 0, y = 0, z = 0 if the density is Ï (x, y, z) = y
Answer:
R 4
0
R 2âˆ’1
2 x
0
R 3âˆ’3
4 xâˆ’3
2 y
0
(y) dz dy dx = 2
11. Find the mass of the bounded region, R formed by the plane 1
2x + 1
2y + 1
4z = 1 and
the planes x = 0, y = 0, z = 0 if the density is Ï (x, y, z) = z2
Answer:
R 2
0
R 2âˆ’x
0
R 4âˆ’2xâˆ’2y
0
Â¡
z2Â¢
dz dy dx = 64
15
12. Here is an iterated integral:
R 3
0
R 3âˆ’x
0
R x2
0
dz dy dx. Write as an iterated integral in the
following orders: dz dx dy, dx dz dy, dx dy dz, dy dx dz, dy dz dx.
Answer:
Z 3
0
Z x2
0
Z 3âˆ’x
0
dy dz dx,
Z 9
0
Z 3
âˆšz
Z 3âˆ’x
0
dy dx dz,
Z 9
0
Z 3âˆ’âˆšz
0
Z 3âˆ’y
âˆšz
dx dy dz,
Z 3
0
Z 3âˆ’y
0
Z x2
0
dz dx dy,
Z 3
0
Z (3âˆ’y)2
0
Z 3âˆ’y
âˆšz
dx dz dy
13. Find the volume of the bounded region determined by 5y + 2z = 4, x = 4 âˆ’y2, y =
0, x = 0.
Answer:
R 4
5
0
R 2âˆ’5
2 y
0
R 4âˆ’y2
0
dx dz dy = 1168
375
14. Find the volume of the bounded region determined by 4y + 3z = 3, x = 4 âˆ’y2, y =
0, x = 0.
Answer:
R 3
4
0
R 1âˆ’4
3 y
0
R 4âˆ’y2
0
dx dz dy = 375
256
15. Find the volume of the bounded region determined by 3y + z = 3, x = 4 âˆ’y2, y =
0, x = 0.
Answer:
R 1
0
R 3âˆ’3y
0
R 4âˆ’y2
0
dx dz dy = 23
4

23.3.
METHODS FOR TRIPLE INTEGRALS 2-7 NOV.
425
16. Find the volume of the region bounded by x2 + y2 = 16, z = 3x, z = 0, and x â‰¥0.
Answer:
R 4
0
R âˆš
(16âˆ’x2)
âˆ’âˆš
(16âˆ’x2)
R 3x
0
dz dy dx = 128
17. Find the volume of the region bounded by x2 + y2 = 25, z = 2x, z = 0, and x â‰¥0.
Answer:
R 5
0
R âˆš
(25âˆ’x2)
âˆ’âˆš
(25âˆ’x2)
R 2x
0
dz dy dx = 500
3
18. Find the volume of the region determined by the intersection of the two cylinders,
x2 + y2 â‰¤9 and y2 + z2 â‰¤9.
Answer:
8
R 3
0
R âˆš
(9âˆ’y2)
0
R âˆš
(9âˆ’y2)
0
dz dx dy = 144
19. Find the total mass of the bounded solid determined by z = a2âˆ’x2âˆ’y2 and x, y, z â‰¥0
if the mass is given by Ï (x, y, z) = z
Answer:
R 4
0
R âˆš
(16âˆ’x2)
0
R 16âˆ’x2âˆ’y2
0
(z) dz dy dx = 512
3 Ï€
20. Find the total mass of the bounded solid determined by z = a2âˆ’x2âˆ’y2 and x, y, z â‰¥0
if the mass is given by Ï (x, y, z) = x + 1
Answer:
R 5
0
R âˆš
(25âˆ’x2)
0
R 25âˆ’x2âˆ’y2
0
(x + 1) dz dy dx = 625
8 Ï€ + 1250
3
21. Find the volume of the region bounded by x2 + y2 = 9, z = 0, z = 5 âˆ’y
Answer:
R 3
âˆ’3
R âˆš
(9âˆ’x2)
âˆ’âˆš
(9âˆ’x2)
R 5âˆ’y
0
dz dy dx = 45Ï€
22. Find the volume of the bounded region determined by x â‰¥0, y â‰¥0, z â‰¥0, and
1
2x + y + 1
2z = 1, and x + 1
2y + 1
2z = 1.
Answer:
R 2
3
0
R 1âˆ’1
2 x
x
R 2âˆ’xâˆ’2y
0
dz dy dx +
R 2
3
0
R 1âˆ’1
2 y
y
R 2âˆ’2xâˆ’y
0
dz dx dy = 4
9
23. Find the volume of the bounded region determined by x â‰¥0, y â‰¥0, z â‰¥0, and
1
7x + y + 1
3z = 1, and x + 1
7y + 1
3z = 1.
Answer:
R 7
8
0
R 1âˆ’1
7 x
x
R 3âˆ’3
7 xâˆ’3y
0
dz dy dx +
R 7
8
0
R 1âˆ’1
7 y
y
R 3âˆ’3xâˆ’3
7 y
0
dz dx dy = 7
8
24. Find the mass of the solid determined by 25x2 + 4y2 â‰¤9, z â‰¥0, and z = x + 2 if the
density is Ï (x, y, z) = x.
Answer:
R 3
5
âˆ’3
5
R 1
2
âˆš
(9âˆ’25x2)
âˆ’1
2
âˆš
(9âˆ’25x2)
R x+2
0
(x) dz dy dx =
81
1000Ï€
25. Find
R 1
0
R 35âˆ’5z
0
R 7âˆ’z
1
5 x
(7 âˆ’z) cos
Â¡
y2Â¢
dy dx dz.
Answer:
You need to interchange the order of integration.
R 1
0
R 7âˆ’z
0
R 5y
0
(7 âˆ’z) cos
Â¡
y2Â¢
dx dy dz =
5
4 cos 36 âˆ’5
4 cos 49

426
THE RIEMANN INTEGRAL ON RN
26. Find
R 2
0
R 12âˆ’3z
0
R 4âˆ’z
1
3 x
(4 âˆ’z) exp
Â¡
y2Â¢
dy dx dz.
Answer:
You need to interchange the order of integration.
R 2
0
R 4âˆ’z
0
R 3y
0
(4 âˆ’z) exp
Â¡
y2Â¢
dx dy dz
= âˆ’3
4e4 âˆ’9 + 3
4e16
27. Find
R 2
0
R 25âˆ’5z
0
R 5âˆ’z
1
5 y
(5 âˆ’z) exp
Â¡
x2Â¢
dx dy dz.
Answer:
You need to interchange the order of integration.
Z 2
0
Z 5âˆ’z
0
Z 5x
0
(5 âˆ’z) exp
Â¡
x2Â¢
dy dx dz = âˆ’5
4e9 âˆ’20 + 5
4e25
28. Find
R 1
0
R 10âˆ’2z
0
R 5âˆ’z
1
2 y
sin x
x
dx dy dz.
Answer:
You need to interchange the order of integration.
Z 1
0
Z 5âˆ’z
0
Z 2x
0
sin x
x
dy dx dz =
âˆ’2 sin 1 cos 5 + 2 cos 1 sin 5 + 2 âˆ’2 sin 5
29. Find
R 20
0
R 2
0
R 6âˆ’z
1
5 y
sin x
x
dx dz dy +
R 30
20
R 6âˆ’1
5 y
0
R 6âˆ’z
1
5 y
sin x
x
dx dz dy.
Answer:
You need to interchange the order of integration.
Z 2
0
Z 30âˆ’5z
0
Z 6âˆ’z
1
5 y
sin x
x
dx dy dz =
Z 2
0
Z 6âˆ’z
0
Z 5x
0
sin x
x
dy dx dz
= âˆ’5 sin 2 cos 6 + 5 cos 2 sin 6 + 10 âˆ’5 sin 6

The Integral In Other
Coordinates 8-10 Nov.
24.1
Diï¬€erent Coordinates
As mentioned above, the fundamental concept of an integral is a sum of things of the form
f (x) dV where dV is an â€œinï¬nitesimalâ€ chunk of volume located at the point, x. Up to
now, this inï¬nitesimal chunk of volume has had the form of a box with sides dx1, Â·Â·Â·, dxn so
dV = dx1 dx2 Â· Â· Â· dxn but its form is not important. It could just as well be an inï¬nitesimal
parallelepiped or parallelogram for example. In what follows, this is what it will be.
First recall the deï¬nition of the box product given in Deï¬nition 3.2.8 on Page 52. The
absolute value of the box product of three vectors gave the volume of the parallelepiped
determined by the three vectors.
Deï¬nition 24.1.1 Let u1, u2, u3 be vectors in R3. The parallelepiped determined
by these vectors will be denoted by P (u1, u2, u3) and it is deï¬ned as
P (u1, u2, u3) â‰¡
ï£±
ï£²
ï£³
3
X
j=1
sjuj : sj âˆˆ[0, 1]
ï£¼
ï£½
ï£¾.
Lemma 24.1.2 The volume of the parallelepiped, P (u1, u2, u3) is given by
Â¯Â¯det
Â¡
u1
u2
u3
Â¢Â¯Â¯
where
Â¡
u1
u2
u3
Â¢
is the matrix having columns u1, u2, and u3.
Proof: Recall from the discussion of the box product or triple product,
volume of P (u1, u2, u3) â‰¡|[u1, u2, u3]| =
Â¯Â¯Â¯Â¯Â¯Â¯
det
ï£«
ï£­
uT
1
uT
2
uT
3
ï£¶
ï£¸
Â¯Â¯Â¯Â¯Â¯Â¯
where
ï£«
ï£­
uT
1
uT
2
uT
3
ï£¶
ï£¸is the matrix having rows equal to the vectors, u1, u2 and u3 arranged
horizontally. Since the determinant of a matrix equals the determinant of its transpose,
volume of P (u1, u2, u3) = |[u1, u2, u3]| =
Â¯Â¯det
Â¡ u1
u2
u3
Â¢Â¯Â¯ .
This proves the lemma.
427

428
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
Deï¬nition 24.1.3 In the case of two vectors, P (u1, u2) will denote the parallelo-
gram determined by u1 and u2. Thus
P (u1, u2) â‰¡
ï£±
ï£²
ï£³
2
X
j=1
sjuj : sj âˆˆ[0, 1]
ï£¼
ï£½
ï£¾.
Lemma 24.1.4 The area of the parallelogram, P (u1, u2) is given by
Â¯Â¯det
Â¡
u1
u2
Â¢Â¯Â¯
where
Â¡
u1
u2
Â¢
is the matrix having columns u1 and u2.
Proof: Letting u1 = (a, b)T and u2 = (c, d)T , consider the vectors in R3 deï¬ned by
bu1 â‰¡(a, b, 0)T and bu2 â‰¡(c, d, 0)T . Then the area of the parallelogram determined by the
vectors, bu1 and bu2 is the norm of the cross product of bu1 and bu2. This follows directly from
the geometric deï¬nition of the cross product given in Deï¬nition 3.2.2 on Page 49. But this
is the same as the area of the parallelogram determined by the vectors u1, u2. Taking the
cross product of bu1 and bu2 yields k (ad âˆ’bc) . Therefore, the norm of this cross product is
|ad âˆ’bc|
which is the same as
Â¯Â¯det
Â¡ u1
u2
Â¢Â¯Â¯
where
Â¡
u1
u2
Â¢
denotes the matrix having the two vectors u1, u2 as columns. This proves
the lemma.
It always works this way. The n dimensional volume of the n dimensional parallelepiped
determined by the vectors, {v1, Â· Â· Â·, vn} is always
Â¯Â¯det
Â¡ v1
Â· Â· Â·
vn
Â¢Â¯Â¯
This general fact will not be used in what follows.
24.1.1
Review Of Polar Coordinates
Earlier it was shown based on geometric reasoning that the appropriate increment of area
in polar coordinates is rdrdÎ¸. Consider this again in a slightly diï¬€erent way. Recall the
transformation equations for polar coordinates,
h (r, Î¸) =
Âµ
x
y
Â¶
=
Âµ
r cos Î¸
r sin Î¸
Â¶
As before, you need to approximate the area of the curvy shape shown in the following
picture.
s
s
s
h(r, Î¸)
h(r, Î¸ + âˆ†Î¸)
h(r + âˆ†r, Î¸)

24.1.
DIFFERENT COORDINATES
429
As âˆ†Î¸ gets smaller, the curvy shape will be better and better approximated by the
dotted parallelogram shown in the picture.
So what is the area of this parallelogram?
The parallelogram is determined by the vectors, h (r, Î¸ + âˆ†Î¸) âˆ’h (r, Î¸) and h (r + âˆ†r, Î¸) âˆ’
h (r, Î¸) . For very small âˆ†r and âˆ†Î¸, these two vectors are essentially equal to hÎ¸ (r, Î¸) âˆ†Î¸
and hr (r, Î¸) âˆ†r. Therefore, from the above discussion, the increment of area is essentially
equal to
Â¯Â¯det
Â¡
hÎ¸ (r, Î¸) âˆ†Î¸
hr (r, Î¸) âˆ†r
Â¢Â¯Â¯ =
Â¯Â¯det
Â¡
hÎ¸ (r, Î¸)
hr (r, Î¸)
Â¢Â¯Â¯ âˆ†râˆ†Î¸
Now
hÎ¸ (r, Î¸) =
Âµ
âˆ’r sin Î¸
r cos Î¸
Â¶
, hr (r, Î¸) =
Âµ
cos Î¸
sin Î¸
Â¶
and so the above reduces to
Â¯Â¯Â¯Â¯
Âµ
âˆ’r sin Î¸
cos Î¸
r cos Î¸
sin Î¸
Â¶Â¯Â¯Â¯Â¯ âˆ†râˆ†Î¸ = râˆ†râˆ†Î¸
and this is why the area increment in polar coordinates is rdrdÎ¸. Note the emphasis on alge-
braic techniques to ï¬nd the area increment. The same approach works for other coordinates,
not just polar coordinates.
24.1.2
General Two Dimensional Coordinates
Suppose U is a set in R2 and h is a C1 function1 mapping U one to one onto h (U) , a set
in R2. Consider a small square inside U. The following picture is of such a square having a
corner at the point, u0 and sides as indicated. The image of this square is also represented.
-
6
u0
âˆ†u1e1
âˆ†u2e2
âˆ†V
u
h(u0)
h(u0 + âˆ†u1e1)
u
h(âˆ†V)
u
h(u0 + âˆ†u2e2)
For small âˆ†ui you would expect the sides going from h (u0) to h (u0 + âˆ†u1e1) and from
h (u0) to h (u0 + âˆ†u2e2) to be almost the same as the vectors, h (u0 + âˆ†u1e1)âˆ’h (u0) and
h (u0 + âˆ†u2e2) âˆ’h (u0) which are approximately equal to
âˆ‚h
âˆ‚u1 (u0) âˆ†u1 and
âˆ‚h
âˆ‚u2 (u0) âˆ†u2
respectively. Therefore, the area of h (âˆ†V ) for small âˆ†ui is essentially equal to the area
of the parallelogram determined by the two vectors,
âˆ‚h
âˆ‚u1 (u0) âˆ†u1 and
âˆ‚h
âˆ‚u2 (u0) âˆ†u2. By
Lemma 24.1.4 this equals
Â¯Â¯det
Â¡
âˆ‚h
âˆ‚u1 (u0) âˆ†u1
âˆ‚h
âˆ‚u2 (u0) âˆ†u2
Â¢Â¯Â¯ =
Â¯Â¯det
Â¡
âˆ‚h
âˆ‚u1 (u0)
âˆ‚h
âˆ‚u2 (u0) Â¢Â¯Â¯ âˆ†u1âˆ†u2
1By this is meant h is the restriction to U of a function deï¬ned on an open set containing U which is
C1. If you like, you can assume U is open but this is not necessary. Neither is C1.

430
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
Thus an inï¬nitesimal chunk of area in h (U) located at u0 is of the form
Â¯Â¯det
Â¡
âˆ‚h
âˆ‚u1 (u0)
âˆ‚h
âˆ‚u2 (u0) Â¢Â¯Â¯ dV
where dV is a corresponding chunk of area located at the point u0. This shows the following
change of variables formula is reasonable.
Z
h(U)
f (x) dV (x) =
Z
U
f (h (u))
Â¯Â¯det
Â¡
âˆ‚h
âˆ‚u1 (u)
âˆ‚h
âˆ‚u2 (u) Â¢Â¯Â¯ dV (u)
Deï¬nition 24.1.5 Let h : U â†’h (U) be a one to one and C1 mapping.
The
(volume) area element in terms of u is deï¬ned as
Â¯Â¯det
Â¡
âˆ‚h
âˆ‚u1 (u)
âˆ‚h
âˆ‚u2 (u) Â¢Â¯Â¯ dV (u) . The
factor,
Â¯Â¯det
Â¡
âˆ‚h
âˆ‚u1 (u)
âˆ‚h
âˆ‚u2 (u) Â¢Â¯Â¯ is called the Jacobian. It equals
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
det
ï£«
ï£¬
ï£¬
ï£­
âˆ‚h1
âˆ‚u1
(u1, u2)
âˆ‚h1
âˆ‚u2
(u1, u2)
âˆ‚h2
âˆ‚u1
(u1, u2)
âˆ‚h2
âˆ‚u2
(u1, u2)
ï£¶
ï£·
ï£·
ï£¸
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
.
It is traditional to call two dimensional volumes area. However, it is probably better
to simply always refer to it as volume. Thus there is 2 dimensional volume, 3 dimensional
volume, etc. Sometimes you can get confused by too many diï¬€erent words to describe things
which are really not essentially diï¬€erent.
Example 24.1.6 Find the area element for polar coordinates.
Here the u coordinates are Î¸ and r. The polar coordinate transformations are
Âµ
x
y
Â¶
=
Âµ
r cos Î¸
r sin Î¸
Â¶
Therefore, the volume (area) element is
Â¯Â¯Â¯Â¯det
Âµ
cos Î¸
âˆ’r sin Î¸
sin Î¸
r cos Î¸
Â¶Â¯Â¯Â¯Â¯ dÎ¸dr = rdÎ¸dr.
Example 24.1.7 Suppose x = u2 âˆ’v2 and y = 2uv Find the area element in terms of u
and v.
You are given
Âµ
x
y
Â¶
=
Âµ
u2 âˆ’v2
2uv
Â¶
and so the area element is
Â¯Â¯Â¯Â¯det
Âµ
2u
âˆ’2v
2v
2u
Â¶Â¯Â¯Â¯Â¯ dudv =
Â¡
4u2 + 4v2Â¢
dudv
Example 24.1.8 Suppose new coordinates are given by u = x + y and v = y/x ï¬nd the
area element in terms of the new coordinates, u and v.
You need to solve for x and y ï¬rst. This yields y =
uv
v+1, x =
u
v+1. Thus
Âµ
x
y
Â¶
=
Âµ
u
v+1
uv
v+1
Â¶
and so the area increment equals
Â¯Â¯Â¯Â¯Â¯det
Ãƒ
1
v+1
âˆ’
u
(v+1)2
v
v+1
u
v+1 âˆ’v
u
(v+1)2
!Â¯Â¯Â¯Â¯Â¯ dudv =
Â¯Â¯Â¯Â¯Â¯
u
(v + 1)2
Â¯Â¯Â¯Â¯Â¯ dudv

24.1.
DIFFERENT COORDINATES
431
Example 24.1.9 The area density is given by Ï (x, y, z) = x and a plate occupies the region
between x + y = 1, x + y = 4 and the lines y = x and y = 2x. Find the x coordinate of the
center of mass of this plate.
Here is a picture of the two dimensional region occupied by this plate.
@
@
@
@
@
@
@
@
@
@
Â¡
Â¡
Â¡
Â¡
Â¡
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
R
It is labeled as R. Thus the total mass and the x coordinate of the center of mass are
given respectively as
Z
R
xdA, xc =
R
R x2dA
R
R xdA
So now you just set up the iterated integrals and go to work. Good luck if you try this. It
is much better to change the variables. Let u = x + y and v = y/x as in Example 24.1.8.
In these new coordinates the horrible quadrilateral becomes the rectangle
(u, v) âˆˆ[1, 4] Ã— [1, 2] .
That is u goes between 1 and 4 while v goes between 1 and 2. Remember it is easy to
integrate over rectangles. Thus using the result of Example 24.1.8
Âµ
x
y
Â¶
=
Âµ
u
v+1
uv
v+1
Â¶
and the area element is
u
(v + 1)2 dudv
Therefore, the total mass is
Z 4
1
Z 2
1
x
z
}|
{
Âµ
u
v + 1
Â¶
dA
z
}|
{
u
(v + 1)2 dudv = 49
200
and the x coordinate of the center of mass is
R 4
1
R 2
1
Â³
u
v+1
Â´2
u
(v+1)2 dudv
49
200
= 117
196
It is a tedious problem but not all that hard if you change the variables.
24.1.3
Three Dimensions
Quiz

432
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
1. Find
R 1
0
R 1
âˆšy sin
Â¡
x3Â¢
dxdy.
2. Find the x coordinate of the center of mass of the solid whose density is Î´ (x, y, z) = x
which occupies the three dimensional region below the plane z = y and above the
triangular region in the xy plane determined by x âˆˆ[0, 1] and 0 â‰¤y â‰¤x.
3. Maximize xy subject to the constraint x2 + y2 = 1.
4. Find the tangent plane to the surface, z2 + x2 + 3y2 = 5 at the point (1, 1, 1) .
The situation is no diï¬€erent for coordinate systems in any number of dimensions al-
though I will concentrate here on three dimensions. A rectangular chunk of volume in the
u space corresponds to the curvy parallelepiped shown below which is approximated by the
parallelepiped shown on the left determined by the 3 vectors
n
âˆ‚x(u0)
âˆ‚ui dui
o3
i=1 for x = f (u)
where u âˆˆU, a subset of R3 and x is a point in V = f (U) , a subset of 3 dimensional space.
Thus, letting the Cartesian coordinates of x be given by x = (x1, x2, x3)T , each xi
being a function of u, an inï¬nitesimal box located at u0 corresponds to an inï¬nitesimal
parallelepiped located at f (u0) which is determined by the 3 vectors
n
âˆ‚x(u0)
âˆ‚ui dui
o3
i=1 . From
Lemma 24.1.2, the volume of this inï¬nitesimal parallelepiped located at f (u0) is given by
Â¯Â¯Â¯Â¯
Â·âˆ‚x (u0)
âˆ‚u1
du1, âˆ‚x (u0)
âˆ‚u2
du2, âˆ‚x (u0)
âˆ‚u3
du3
Â¸Â¯Â¯Â¯Â¯ =
Â¯Â¯Â¯Â¯
Â·âˆ‚x (u0)
âˆ‚u1
, âˆ‚x (u0)
âˆ‚u2
, âˆ‚x (u0)
âˆ‚u3
Â¸Â¯Â¯Â¯Â¯ du1du2du3
(24.1)
=
Â¯Â¯Â¯det
Â³
âˆ‚x(u0)
âˆ‚u1
âˆ‚x(u0)
âˆ‚u2
âˆ‚x(u0)
âˆ‚u3
Â´Â¯Â¯Â¯ du1du2du3
There is also no change in going to higher dimensions than 3.
Deï¬nition 24.1.10 Let x = f (u) be as described above. Then for n = 2, 3, the
symbol,
âˆ‚(x1,Â·Â·Â·xn)
âˆ‚(u1,Â·Â·Â·,un), called the Jacobian determinant, is deï¬ned by
det
Â³
âˆ‚x(u0)
âˆ‚u1
Â· Â· Â·
âˆ‚x(u0)
âˆ‚un
Â´
â‰¡âˆ‚(x1, Â· Â· Â·, xn)
âˆ‚(u1, Â· Â· Â·, un).
Also, the symbol,
Â¯Â¯Â¯ âˆ‚(x1,Â·Â·Â·,xn)
âˆ‚(u1,Â·Â·Â·,un)
Â¯Â¯Â¯ du1 Â· Â· Â· dun is called the volume element.

24.1.
DIFFERENT COORDINATES
433
This has given motivation for the following fundamental procedure often called the
change of variables formula which holds under fairly general conditions.
Procedure 24.1.11 Suppose U is an open subset of Rn for n = 2, 3 and suppose
f : U â†’f (U) is a C1 function which is one to one, x = f (u).2Then if h : f (U) â†’R,
Z
U
h (f (u))
Â¯Â¯Â¯Â¯
âˆ‚(x1, Â· Â· Â·, xn)
âˆ‚(u1, Â· Â· Â·, un)
Â¯Â¯Â¯Â¯ dV =
Z
f(U)
h (x) dV.
Now consider spherical coordinates. Recall the geometrical meaning of these coordinates
illustrated in the following picture.
-
6
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Âª
x1
(x1, y1, 0)
y1
(Ï, Ï†, Î¸)
(r, Î¸, z1)
(x1, y1, z1)
z1
Ï
r
Î¸
Ï†
â€¢
x
y
z
Thus there is a relationship between these coordinates and rectangular coordinates given
by
x = Ï sin Ï† cos Î¸, y = Ï sin Ï† sin Î¸, z = Ï cos Ï†
(24.2)
where Ï† âˆˆ[0, Ï€], Î¸ âˆˆ[0, 2Ï€), and Ï > 0. Thus (Ï, Ï†, Î¸) is a point in R3, more speciï¬cally in
the set
U = (0, âˆ) Ã— [0, Ï€] Ã— [0, 2Ï€)
and corresponding to such a (Ï, Ï†, Î¸) âˆˆU there exists a unique point, (x, y, z) âˆˆV where
V consists of all points of R3 other than the origin, (0, 0, 0) . This (x, y, z) determines a
unique point in three dimensional space as mentioned earlier. From the above argument,
the volume element is
Â¯Â¯Â¯det
Â³
âˆ‚x(Ï0,Ï†0,Î¸0)
âˆ‚Ï
âˆ‚x(Ï0,Ï†0,Î¸0)
âˆ‚Ï†
âˆ‚x(Ï0,Ï†0,Î¸0)
âˆ‚Î¸
Â´Â¯Â¯Â¯ dÏdÎ¸dÏ†.
The mapping between spherical and rectangular coordinates is written as
x =
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
Ï sin Ï† cos Î¸
Ï sin Ï† sin Î¸
Ï cos Ï†
ï£¶
ï£¸= f (Ï, Ï†, Î¸)
(24.3)
2This will cause non overlapping inï¬nitesimal boxes in U to be mapped to non overlapping inï¬nitesimal
parallelepipeds in V.
Also, in the context of the Riemann integral we should say more about the set U in any case the function,
h. These conditions are mainly technical however, and since a mathematically respectable treatment will
not be attempted for this theorem, I think it best to give a memorable version of it which is essentially
correct in all examples of interest. For a typical precise theorem see the appendix on the Riemann integral.

434
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
Therefore, det
Â³
âˆ‚x(Ï0,Ï†0,Î¸0)
âˆ‚Ï
, âˆ‚x(Ï0,Ï†0,Î¸0)
âˆ‚Ï†
, âˆ‚x(Ï0,Ï†0,Î¸0)
âˆ‚Î¸
Â´
=
det
ï£«
ï£­
sin Ï† cos Î¸
Ï cos Ï† cos Î¸
âˆ’Ï sin Ï† sin Î¸
sin Ï† sin Î¸
Ï cos Ï† sin Î¸
Ï sin Ï† cos Î¸
cos Ï†
âˆ’Ï sin Ï†
0
ï£¶
ï£¸= Ï2 sin Ï†
which is positive because Ï† âˆˆ[0, Ï€] .
Example 24.1.12 Find the volume of a ball, BR of radius R.
In this case, U = (0, R] Ã— [0, Ï€] Ã— [0, 2Ï€) and use spherical coordinates. Then 24.3 yields
a set in R3 which clearly diï¬€ers from the ball of radius R only by a set having volume equal
to zero. It leaves out the point at the origin is all. Therefore, the volume of the ball is
Z
BR
1 dV
=
Z
U
Ï2 sin Ï† dV
=
Z R
0
Z Ï€
0
Z 2Ï€
0
Ï2 sin Ï† dÎ¸ dÏ† dÏ = 4
3R3Ï€.
The reason this was eï¬€ortless, is that the ball, BR is realized as a box in terms of the
spherical coordinates. Remember what was pointed out earlier about setting up iterated
integrals over boxes.
Example 24.1.13 Find the volume element for cylindrical coordinates.
In cylindrical coordinates,
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
r cos Î¸
r sin Î¸
z
ï£¶
ï£¸
Therefore, the Jacobian determinant is
det
ï£«
ï£­
cos Î¸
âˆ’r sin Î¸
0
sin Î¸
r cos Î¸
0
0
0
1
ï£¶
ï£¸= r.
It follows the volume element in cylindrical coordinates is r dÎ¸ dr dz.
Example 24.1.14 This example uses spherical coordinates to verify an important conclu-
sion about gravitational force. Let the hollow sphere, H be deï¬ned by a2 < x2 + y2 + z2 < b2
and suppose this hollow sphere has constant density taken to equal Î±. Now place a unit mass
at the point (0, 0, z0) where |z0| âˆˆ[a, b] . Show the force of gravity acting on this unit mass
is
Âµ
Î±G
R
H
(zâˆ’z0)
[x2+y2+(zâˆ’z0)2]
3/2 dV
Â¶
k and then show that if |z0| > b then the force of gravity
acting on this point mass is the same as if the entire mass of the hollow sphere were placed
at the origin, while if |z0| < a, the total force acting on the point mass from gravity equals
zero. Here G is the gravitation constant and Î± is the density. In particular, this shows that
the force a planet exerts on an object is as though the entire mass of the planet were situated
at its center3.
3This was shown by Newton in 1685 and allowed him to assert his law of gravitation applied to the
planets as though they were point masses. It was a major accomplishment.

24.1.
DIFFERENT COORDINATES
435
Without loss of generality, assume z0 > 0. Let dV be a little chunk of material located
at the point (x, y, z) of H the hollow sphere. Then according to Newtonâ€™s law of gravity,
the force this small chunk of material exerts on the given point mass equals
xi + yj + (z âˆ’z0) k
|xi + yj + (z âˆ’z0) k|
1
Â³
x2 + y2 + (z âˆ’z0)2Â´GÎ± dV =
(xi + yj + (z âˆ’z0) k)
1
Â³
x2 + y2 + (z âˆ’z0)2Â´3/2 GÎ± dV
Therefore, the total force is
Z
H
(xi + yj + (z âˆ’z0) k)
1
Â³
x2 + y2 + (z âˆ’z0)2Â´3/2 GÎ± dV.
By the symmetry of the sphere, the i and j components will cancel out when the integral is
taken. This is because there is the same amount of stuï¬€for negative x and y as there is for
positive x and y. Hence what remains is
Î±Gk
Z
H
(z âˆ’z0)
h
x2 + y2 + (z âˆ’z0)2i3/2 dV
as claimed. Now for the interesting part, the integral is evaluated. In spherical coordinates
this integral is.
Z 2Ï€
0
Z b
a
Z Ï€
0
(Ï cos Ï† âˆ’z0) Ï2 sin Ï†
(Ï2 + z2
0 âˆ’2Ïz0 cos Ï†)3/2 dÏ† dÏ dÎ¸.
(24.4)
Rewrite the inside integral and use integration by parts to obtain this inside integral equals
1
2z0
Z Ï€
0
Â¡
Ï2 cos Ï† âˆ’Ïz0
Â¢
(2z0Ï sin Ï†)
(Ï2 + z2
0 âˆ’2Ïz0 cos Ï†)3/2 dÏ† =
1
2z0
Ãƒ
âˆ’2
âˆ’Ï2 âˆ’Ïz0
p
(Ï2 + z2
0 + 2Ïz0)
+ 2
Ï2 âˆ’Ïz0
p
(Ï2 + z2
0 âˆ’2Ïz0)
âˆ’
Z Ï€
0
2Ï2
sin Ï†
p
(Ï2 + z2
0 âˆ’2Ïz0 cos Ï†)
dÏ†
!
.
(24.5)
There are some cases to consider here.
First suppose z0 < a so the point is on the inside of the hollow sphere and it is always
the case that Ï > z0. Then in this case, the two ï¬rst terms reduce to
2Ï (Ï + z0)
q
(Ï + z0)2 + 2Ï (Ï âˆ’z0)
q
(Ï âˆ’z0)2 = 2Ï (Ï + z0)
(Ï + z0)
+ 2Ï (Ï âˆ’z0)
Ï âˆ’z0
= 4Ï
and so the expression in 24.5 equals
1
2z0
Ãƒ
4Ï âˆ’
Z Ï€
0
2Ï2
sin Ï†
p
(Ï2 + z2
0 âˆ’2Ïz0 cos Ï†)
dÏ†
!
=
1
2z0
Ãƒ
4Ï âˆ’1
z0
Z Ï€
0
Ï
2Ïz0 sin Ï†
p
(Ï2 + z2
0 âˆ’2Ïz0 cos Ï†)
dÏ†
!

436
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
=
1
2z0
Âµ
4Ï âˆ’2Ï
z0
Â¡
Ï2 + z2
0 âˆ’2Ïz0 cos Ï†
Â¢1/2 |Ï€
0
Â¶
=
1
2z0
Âµ
4Ï âˆ’2Ï
z0
[(Ï + z0) âˆ’(Ï âˆ’z0)]
Â¶
= 0.
Therefore, in this case the inner integral of 24.4 equals zero and so the original integral will
also be zero.
The other case is when z0 > b and so it is always the case that z0 > Ï. In this case the
ï¬rst two terms of 24.5 are
2Ï (Ï + z0)
q
(Ï + z0)2 + 2Ï (Ï âˆ’z0)
q
(Ï âˆ’z0)2 = 2Ï (Ï + z0)
(Ï + z0)
+ 2Ï (Ï âˆ’z0)
z0 âˆ’Ï
= 0.
Therefore in this case, 24.5 equals
1
2z0
Ãƒ
âˆ’
Z Ï€
0
2Ï2
sin Ï†
p
(Ï2 + z2
0 âˆ’2Ïz0 cos Ï†)
dÏ†
!
=
âˆ’Ï
2z2
0
ÃƒZ Ï€
0
2Ïz0 sin Ï†
p
(Ï2 + z2
0 âˆ’2Ïz0 cos Ï†)
dÏ†
!
which equals
âˆ’Ï
z2
0
Â³Â¡
Ï2 + z2
0 âˆ’2Ïz0 cos Ï†
Â¢1/2 |Ï€
0
Â´
=
âˆ’Ï
z2
0
[(Ï + z0) âˆ’(z0 âˆ’Ï)] = âˆ’2Ï2
z2
0
.
Thus the inner integral of 24.4 reduces to the above simple expression. Therefore, 24.4
equals
Z 2Ï€
0
Z b
a
Âµ
âˆ’2
z2
0
Ï2
Â¶
dÏ dÎ¸ = âˆ’4
3Ï€ b3 âˆ’a3
z2
0
and so
Î±Gk
Z
H
(z âˆ’z0)
h
x2 + y2 + (z âˆ’z0)2i3/2 dV = Î±Gk
Âµ
âˆ’4
3Ï€ b3 âˆ’a3
z2
0
Â¶
= âˆ’kGtotal mass
z2
0
.
24.1.4
Exercises With Answers
1. Find the area of the bounded region, R, determined by 3x+3y = 1, 3x+3y = 8, y = 3x,
and y = 4x.
Answer:
Let u = y
x, v = 3x + 3y. Then solving these equations for x and y yields
Â½
x = 1
3
v
1 + u, y = 1
3u
v
1 + u
Â¾
.
Now
âˆ‚(x, y)
âˆ‚(u, v) = det
Ãƒ
âˆ’1
3
v
(1+u)2
1
3+3u
1
3
v
(1+u)2
1
3
u
1+u
!
= âˆ’1
9
v
(1 + u)2 .

24.1.
DIFFERENT COORDINATES
437
Also, u âˆˆ[3, 4] while v âˆˆ[1, 8] . Therefore,
Z
R
dV =
Z 4
3
Z 8
1
Â¯Â¯Â¯Â¯Â¯âˆ’1
9
v
(1 + u)2
Â¯Â¯Â¯Â¯Â¯ dv du =
Z 4
3
Z 8
1
1
9
v
(1 + u)2 dv du = 7
40
2. Find the area of the bounded region, R, determined by 5x+y = 1, 5x+y = 9, y = 2x,
and y = 5x.
Answer:
Let u = y
x, v = 5x + y. Then solving these equations for x and y yields
Â½
x =
v
5 + u, y = u
v
5 + u
Â¾
.
Now
âˆ‚(x, y)
âˆ‚(u, v) = det
Ãƒ
âˆ’
v
(5+u)2
1
5+u
5
v
(5+u)2
u
5+u
!
= âˆ’
v
(5 + u)2 .
Also, u âˆˆ[2, 5] while v âˆˆ[1, 9] . Therefore,
Z
R
dV =
Z 5
2
Z 9
1
Â¯Â¯Â¯Â¯Â¯âˆ’
v
(5 + u)2
Â¯Â¯Â¯Â¯Â¯ dv du =
Z 5
2
Z 9
1
v
(5 + u)2 dv du = 12
7
3. A solid, R is determined by 5x + 3y = 4, 5x + 3y = 9, y = 2x, and y = 5x and the
density is Ï = x. Find the total mass of R.
Answer:
Let u = y
x, v = 5x + 3y. Then solving these equations for x and y yields
Â½
x =
v
5 + 3u, y = u
v
5 + 3u
Â¾
.
Now
âˆ‚(x, y)
âˆ‚(u, v) = det
Ãƒ
âˆ’3
v
(5+3u)2
1
5+3u
5
v
(5+3u)2
u
5+3u
!
= âˆ’
v
(5 + 3u)2 .
Also, u âˆˆ[2, 5] while v âˆˆ[4, 9] . Therefore,
Z
R
Ï dV =
Z 5
2
Z 9
4
v
5 + 3u
Â¯Â¯Â¯Â¯Â¯âˆ’
v
(5 + 3u)2
Â¯Â¯Â¯Â¯Â¯ dv du =
Z 5
2
Z 9
4
Âµ
v
5 + 3u
Â¶ Ãƒ
v
(5 + 3u)2
!
dv du = 4123
19 360.

438
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
4. A solid, R is determined by 2x + 2y = 1, 2x + 2y = 10, y = 4x, and y = 5x and the
density is Ï = x + 1. Find the total mass of R.
Answer:
Let u = y
x, v = 2x + 2y. Then solving these equations for x and y yields
Â½
x = 1
2
v
1 + u, y = 1
2u
v
1 + u
Â¾
.
Now
âˆ‚(x, y)
âˆ‚(u, v) = det
Ãƒ
âˆ’1
2
v
(1+u)2
1
2+2u
1
2
v
(1+u)2
1
2
u
1+u
!
= âˆ’1
4
v
(1 + u)2 .
Also, u âˆˆ[4, 5] while v âˆˆ[1, 10] . Therefore,
Z
R
Ï dV
=
Z 5
4
Z 10
1
(x + 1)
Â¯Â¯Â¯Â¯Â¯âˆ’1
4
v
(1 + u)2
Â¯Â¯Â¯Â¯Â¯ dv du
=
Z 5
4
Z 10
1
(x + 1)
Ãƒ
1
4
v
(1 + u)2
!
dv du
5. A solid, R is determined by 4x + 2y = 1, 4x + 2y = 9, y = x, and y = 6x and the
density is Ï = yâˆ’1. Find the total mass of R.
Answer:
Let u = y
x, v = 4x + 2y. Then solving these equations for x and y yields
Â½
x = 1
2
v
2 + u, y = 1
2u
v
2 + u
Â¾
.
Now
âˆ‚(x, y)
âˆ‚(u, v) = det
Ãƒ
âˆ’1
2
v
(2+u)2
1
4+2u
v
(2+u)2
1
2
u
2+u
!
= âˆ’1
4
v
(2 + u)2 .
Also, u âˆˆ[1, 6] while v âˆˆ[1, 9] . Therefore,
Z
R
Ï dV =
Z 6
1
Z 9
1
Âµ1
2u
v
2 + u
Â¶âˆ’1 Â¯Â¯Â¯Â¯Â¯âˆ’1
4
v
(2 + u)2
Â¯Â¯Â¯Â¯Â¯ dv du = âˆ’4 ln 2 + 4 ln 3
6. Find the volume of the region, E, bounded by the ellipsoid, 1
4x2 + 1
9y2 + 1
49z2 = 1.
Answer:
Let u = 1
2x, v = 1
3y, w = 1
7z. Then (u, v, w) is a point in the unit ball, B. Therefore,
Z
B
âˆ‚(x, y, z)
âˆ‚(u, v, w) dV =
Z
E
dV.
But âˆ‚(x,y,z)
âˆ‚(u,v,w) = 42 and so the answer is
(volume of B) Ã— 42 = 4
3Ï€42 = 56Ï€.

24.1.
DIFFERENT COORDINATES
439
7. Here are three vectors. (4, 1, 4)T , (5, 0, 4)T , and(3, 1, 5)T . These vectors determine a
parallelepiped, R, which is occupied by a solid having density Ï = x. Find the mass
of this solid.
Answer:
Let
ï£«
ï£­
4
5
3
1
0
1
4
4
5
ï£¶
ï£¸
ï£«
ï£­
u
v
w
ï£¶
ï£¸=
ï£«
ï£­
x
y
z
ï£¶
ï£¸. Then this maps the unit cube,
Q â‰¡[0, 1] Ã— [0, 1] Ã— [0, 1]
onto R and
âˆ‚(x, y, z)
âˆ‚(u, v, w) =
Â¯Â¯Â¯Â¯Â¯Â¯
det
ï£«
ï£­
4
5
3
1
0
1
4
4
5
ï£¶
ï£¸
Â¯Â¯Â¯Â¯Â¯Â¯
= |âˆ’9| = 9
so the mass is
Z
R
x dV =
Z
Q
(4u + 5v + 3w) (9) dV
=
Z 1
0
Z 1
0
Z 1
0
(4u + 5v + 3w) (9) du dv dw = 54
8. Here are three vectors. (3, 2, 6)T , (4, 1, 6)T , and (2, 2, 7)T . These vectors determine a
parallelepiped, R, which is occupied by a solid having density Ï = y. Find the mass of
this solid.
Answer:
Let
ï£«
ï£­
3
4
2
2
1
2
6
6
7
ï£¶
ï£¸
ï£«
ï£­
u
v
w
ï£¶
ï£¸=
ï£«
ï£­
x
y
z
ï£¶
ï£¸. Then this maps the unit cube,
Q â‰¡[0, 1] Ã— [0, 1] Ã— [0, 1]
onto R and
âˆ‚(x, y, z)
âˆ‚(u, v, w) =
Â¯Â¯Â¯Â¯Â¯Â¯
det
ï£«
ï£­
3
4
2
2
1
2
6
6
7
ï£¶
ï£¸
Â¯Â¯Â¯Â¯Â¯Â¯
= |âˆ’11| = 11
and so the mass is
Z
R
x dV =
Z
Q
(2u + v + 2w) (11) dV
=
Z 1
0
Z 1
0
Z 1
0
(2u + v + 2w) (11) du dv dw = 55
2 .

440
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
9. Here are three vectors. (2, 2, 4)T , (3, 1, 4)T , and (1, 2, 5)T . These vectors determine
a parallelepiped, R, which is occupied by a solid having density Ï = y + x. Find the
mass of this solid.
Answer:
Let
ï£«
ï£­
2
3
1
2
1
2
4
4
5
ï£¶
ï£¸
ï£«
ï£­
u
v
w
ï£¶
ï£¸=
ï£«
ï£­
x
y
z
ï£¶
ï£¸. Then this maps the unit cube,
Q â‰¡[0, 1] Ã— [0, 1] Ã— [0, 1]
onto R and
âˆ‚(x, y, z)
âˆ‚(u, v, w) =
Â¯Â¯Â¯Â¯Â¯Â¯
det
ï£«
ï£­
2
3
1
2
1
2
4
4
5
ï£¶
ï£¸
Â¯Â¯Â¯Â¯Â¯Â¯
= |âˆ’8| = 8
and so the density is 4u + 4v + 3w
Z
R
x dV =
Z
Q
(4u + 4v + 3w) (8) dV
=
Z 1
0
Z 1
0
Z 1
0
(4u + 4v + 3w) (8) du dv dw = 44.
10. Let D =
Â©
(x, y) : x2 + y2 â‰¤25
Âª
. Find
R
D e36x2+36y2 dx dy.
Answer:
This is easy in polar coordinates. x = r cos Î¸, y = r sin Î¸. Thus âˆ‚(x,y)
âˆ‚(r,Î¸) = r and in terms
of these new coordinates, the disk, D, is the rectangle,
R = {(r, Î¸) âˆˆ[0, 5] Ã— [0, 2Ï€]} .
Therefore,
Z
D
e36x2+36y2 dV =
Z
R
e36r2r dV =
Z 5
0
Z 2Ï€
0
e36r2r dÎ¸ dr = 1
36Ï€
Â¡
e900 âˆ’1
Â¢
.
Note you wouldnâ€™t get very far without changing the variables in this.
11. Let D =
Â©
(x, y) : x2 + y2 â‰¤9
Âª
. Find
R
D cos
Â¡
36x2 + 36y2Â¢
dx dy.
Answer:
This is easy in polar coordinates. x = r cos Î¸, y = r sin Î¸. Thus âˆ‚(x,y)
âˆ‚(r,Î¸) = r and in terms
of these new coordinates, the disk, D, is the rectangle,
R = {(r, Î¸) âˆˆ[0, 3] Ã— [0, 2Ï€]} .
Therefore,
Z
D
cos
Â¡
36x2 + 36y2Â¢
dV =
Z
R
cos
Â¡
36r2Â¢
r dV =

24.1.
DIFFERENT COORDINATES
441
Z 3
0
Z 2Ï€
0
cos
Â¡
36r2Â¢
r dÎ¸ dr = 1
36 (sin 324) Ï€.
12. The ice cream in a sugar cone is described in spherical coordinates by Ï âˆˆ[0, 8] , Ï† âˆˆ
Â£
0, 1
4Ï€
Â¤
, Î¸ âˆˆ[0, 2Ï€] . If the units are in centimeters, ï¬nd the total volume in cubic
centimeters of this ice cream.
Answer:
Remember that in spherical coordinates, the volume element is Ï2 sin Ï† dV and so the
total volume of this is
R 8
0
R 1
4 Ï€
0
R 2Ï€
0
Ï2 sin Ï† dÎ¸ dÏ† dÏ = âˆ’512
3
âˆš
2Ï€ + 1024
3 Ï€.
13. Find the volume between z = 5 âˆ’x2 âˆ’y2 and z =
p
(x2 + y2).
Answer:
Use cylindrical coordinates. In terms of these coordinates the shape is
h âˆ’r2 â‰¥z â‰¥r, r âˆˆ
Â·
0, 1
2
âˆš
21 âˆ’1
2
Â¸
, Î¸ âˆˆ[0, 2Ï€] .
Also, âˆ‚(x,y,z)
âˆ‚(r,Î¸,z) = r. Therefore, the volume is
Z 2Ï€
0
Z
1
2
âˆš
21âˆ’1
2
0
Z 5âˆ’r2
0
r dz dr dÎ¸ = 39
4 Ï€ + 1
4Ï€
âˆš
21
14. A ball of radius 12 is placed in a drill press and a hole of radius 4 is drilled out with
the center of the hole a diameter of the ball. What is the volume of the material which
remains?
Answer:
You know the formula for the volume of a sphere and so if you ï¬nd out how much stuï¬€
is taken away, then it will be easy to ï¬nd what is left. To ï¬nd the volume of what is
removed, it is easiest to use cylindrical coordinates. This volume is
Z 4
0
Z 2Ï€
0
Z âˆš
(144âˆ’r2)
âˆ’âˆš
(144âˆ’r2)
r dz dÎ¸ dr = âˆ’4096
3
âˆš
2Ï€ + 2304Ï€.
Therefore, the volume of what remains is 4
3Ï€ (12)3 minus the above. Thus the volume
of what remains is
4096
3
âˆš
2Ï€.
15. A ball of radius 11 has density equal to
p
x2 + y2 + z2 in rectangular coordinates.
The top of this ball is sliced oï¬€by a plane of the form z = 1. What is the mass of
what remains?
Answer:
Z 2Ï€
0
Z arcsin( 2
11
âˆš
30)
0
Z sec Ï†
0
Ï3 sin Ï† dÏ dÏ† dÎ¸ +
Z 2Ï€
0
Z Ï€
arcsin( 2
11
âˆš
30)
Z 11
0
Ï3 sin Ï† dÏ dÏ† dÎ¸
= 24 623
3
Ï€

442
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
16. Find
R
S
y
x dV where S is described in polar coordinates as 1 â‰¤r â‰¤2 and 0 â‰¤Î¸ â‰¤Ï€/4.
Answer:
Use x = r cos Î¸ and y = r sin Î¸. Then the integral in polar coordinates is
Z Ï€/4
0
Z 2
1
(r tan Î¸) dr dÎ¸ = 3
4 ln 2.
17. Find
R
S
Â³Â¡ y
x
Â¢2 + 1
Â´
dV where S is given in polar coordinates as 1 â‰¤r â‰¤2 and
0 â‰¤Î¸ â‰¤1
4Ï€.
Answer:
Use x = r cos Î¸ and y = r sin Î¸. Then the integral in polar coordinates is
Z
1
4 Ï€
0
Z 2
1
Â¡
1 + tan2 Î¸
Â¢
r dr dÎ¸.
18. Use polar coordinates to evaluate the following integral. Here S is given in terms of
the polar coordinates.
R
S sin
Â¡
4x2 + 4y2Â¢
dV where r â‰¤2 and 0 â‰¤Î¸ â‰¤1
6Ï€.
Answer:
Z
1
6 Ï€
0
Z 2
0
sin
Â¡
4r2Â¢
r dr dÎ¸ = âˆ’1
48Ï€ cos 16 + 1
48Ï€
19. Find
R
S e2x2+2y2 dV where S is given in terms of the polar coordinates, r â‰¤2 and
0 â‰¤Î¸ â‰¤1
3Ï€.
Answer:
The integral is
Z
1
3 Ï€
0
Z 2
0
re2r2 dr dÎ¸ = 1
12Ï€
Â¡
e8 âˆ’1
Â¢
.
20. Compute the volume of a sphere of radius R using cylindrical coordinates.
Answer:
Using cylindrical coordinates, the integral is
R 2Ï€
0
R R
0
R âˆš
R2âˆ’r2
âˆ’
âˆš
R2âˆ’r2 r dz dr dÎ¸ = 4
3Ï€R3.
24.2
The Moment Of Inertia âˆ—
In order to appreciate the importance of this concept, it is necessary to discuss its physical
signiï¬cance.
24.2.1
The Spinning Topâˆ—
To begin with consider a spinning top as illustrated in the following picture.

24.2.
THE MOMENT OF INERTIA âˆ—
443
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
x
y
z
R Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
 â„¦a
Â¡
Â¡
Â¡
Â¡
Â¡

@
@
@
@
R
u
y
Î¸
Î±
For the purpose of this discussion, consider the top as a large number of point masses,
mi, located at the positions, ri (t) for i = 1, 2, Â· Â· Â·, N and these masses are symmetrically
arranged relative to the axis of the top. As the top spins, the axis of symmetry is observed
to move around the z axis. This is called precession and you will see it occur whenever you
spin a top. What is the speed of this precession? In other words, what is Î¸â€²? The following
discussion follows one given in Sears and Zemansky [24].
Imagine a coordinate system which is ï¬xed relative to the moving top. Thus in this
coordinate system the points of the top are ï¬xed. Let the standard unit vectors of the
coordinate system moving with the top be denoted by i (t) , j (t) , k (t).
From Theorem
16.4.2 on Page 300, there exists an angular velocity vector â„¦(t) such that if u (t) is the
position vector of a point ï¬xed in the top, (u (t) = u1i (t) + u2j (t) + u3k (t)),
uâ€² (t) = â„¦(t) Ã— u (t) .
The vector â„¦a shown in the picture is the vector for which
râ€²
i (t) â‰¡â„¦a Ã— ri (t)
is the velocity of the ith point mass due to rotation about the axis of the top.
Thus
â„¦(t) = â„¦a (t) + â„¦p (t) and it is assumed â„¦p (t) is very small relative to â„¦a. In other words,
it is assumed the axis of the top moves very slowly relative to the speed of the points in the
top which are spinning very fast around the axis of the top. The angular momentum, L is
deï¬ned by
L â‰¡
N
X
i=1
ri Ã— mivi
(24.6)
where vi equals the velocity of the ith point mass. Thus vi = â„¦(t) Ã— ri and from the above

444
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
assumption, vi may be taken equal to â„¦a Ã— ri. Therefore, L is essentially given by
L
â‰¡
N
X
i=1
miri Ã— (â„¦a Ã— ri)
=
N
X
i=1
mi
Â³
|ri|2 â„¦a âˆ’(ri Â· â„¦a) ri
Â´
.
By symmetry of the top, this last expression equals a multiple of â„¦a. Thus L is parallel to
â„¦a. Also,
L Â· â„¦a
=
N
X
i=1
miâ„¦a Â· ri Ã— (â„¦a Ã— ri)
=
N
X
i=1
mi (â„¦a Ã— ri) Â· (â„¦a Ã— ri)
=
N
X
i=1
mi |â„¦a Ã— ri|2 =
N
X
i=1
mi |â„¦a|2 |ri|2 sin2 (Î²i)
where Î²i denotes the angle between the position vector of the ith point mass and the axis
of the top. Since this expression is positive, this also shows L has the same direction as â„¦a.
Let Ï‰ â‰¡|â„¦a| . Then the above expression is of the form
L Â· â„¦a = IÏ‰2,
where
I â‰¡
N
X
i=1
mi |ri|2 sin2 (Î²i) .
Thus, to get I you take the mass of the ith point mass, multiply it by the square of its
distance to the axis of the top and add all these up. This is deï¬ned as the moment of inertia
of the top about the axis of the top. Letting u denote a unit vector in the direction of the
axis of the top, this implies
L = IÏ‰u.
(24.7)
Note the simple description of the angular momentum in terms of the moment of inertia.
Referring to the above picture, deï¬ne the vector, y to be the projection of the vector, u on
the xy plane. Thus
y = u âˆ’(u Â· k) k
and
(u Â· i) = (y Â· i) = sin Î± cos Î¸.
(24.8)
Now also from 24.6,
dL
dt
=
N
X
i=1
mi
=0
z }| {
râ€²
i Ã— vi + ri Ã— mivâ€²
i
=
N
X
i=1
ri Ã— mivâ€²
i = âˆ’
N
X
i=1
ri Ã— migk

24.2.
THE MOMENT OF INERTIA âˆ—
445
where g is the acceleration of gravity. From 24.7, 24.8, and the above,
dL
dt Â· i
=
IÏ‰
Âµdu
dt Â· i
Â¶
= IÏ‰
Âµdy
dt Â· i
Â¶
=
(âˆ’IÏ‰ sin Î± sin Î¸) Î¸â€² = âˆ’
N
X
i=1
ri Ã— migk Â· i
=
âˆ’
N
X
i=1
migri Â· k Ã— i = âˆ’
N
X
i=1
migri Â· j.
(24.9)
To simplify this further, recall the following deï¬nition of the center of mass.
Deï¬nition 24.2.1 Deï¬ne the total mass, M by
M =
N
X
i=1
mi
and the center of mass, r0 by
r0 â‰¡
PN
i=1 rimi
M
.
(24.10)
In terms of the center of mass, the last expression equals
âˆ’Mgr0 Â· j
=
âˆ’Mg (r0 âˆ’(r0 Â· k) k + (r0 Â· k) k) Â· j
=
âˆ’Mg (r0 âˆ’(r0 Â· k) k) Â· j
=
âˆ’Mg |r0 âˆ’(r0 Â· k) k| cos Î¸
=
âˆ’Mg |r0| sin Î± cos
Â³Ï€
2 âˆ’Î¸
Â´
.
Note that by symmetry, r0 (t) is on the axis of the top, is in the same direction as L, u, and
â„¦a, and also |r0| is independent of t. Therefore, from the second line of 24.9,
(âˆ’IÏ‰ sin Î± sin Î¸) Î¸â€² = âˆ’Mg |r0| sin Î± sin Î¸.
which shows
Î¸â€² = Mg |r0|
IÏ‰
.
(24.11)
From 24.11, the angular velocity of precession does not depend on Î± in the picture. It
also is slower when Ï‰ is large and I is large.
The above discussion is a considerable simpliï¬cation of the problem of a spinning top
obtained from an assumption that â„¦a is approximately equal to â„¦. It also leaves out all
considerations of friction and the observation that the axis of symmetry wobbles. This is
wobbling is called nutation. The full mathematical treatment of this problem involves the
Euler angles and some fairly complicated diï¬€erential equations obtained using techniques
discussed in advanced physics classes. Lagrange studied these types of problems back in the
1700â€™s.

446
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
24.2.2
Kinetic Energyâˆ—
The next problem is that of understanding the total kinetic energy of a collection of moving
point masses. Consider a possibly large number of point masses, mi located at the positions
ri for i = 1, 2, Â· Â· Â·, N. Thus the velocity of the ith point mass is râ€²
i = vi. The kinetic energy
of the mass mi is deï¬ned by
1
2mi |râ€²
i|2 .
(This is a very good time to review the presentation on kinetic energy given on Page 277.)
The total kinetic energy of the collection of masses is then
E =
N
X
i=1
1
2mi |râ€²
i|2 .
(24.12)
As these masses move about, so does the center of mass, r0. Thus r0 is a function of t
just as the other ri. From 24.12 the total kinetic energy is
E
=
N
X
i=1
1
2mi |râ€²
i âˆ’râ€²
0 + râ€²
0|2
=
N
X
i=1
1
2mi
h
|râ€²
i âˆ’râ€²
0|2 + |râ€²
0|2 + 2 (râ€²
i âˆ’râ€²
0 Â· râ€²
0)
i
.
(24.13)
Now
N
X
i=1
mi (râ€²
i âˆ’râ€²
0 Â· râ€²
0)
=
Ãƒ N
X
i=1
mi (ri âˆ’r0)
!â€²
Â· râ€²
0
=
0
because from 24.10
N
X
i=1
mi (ri âˆ’r0)
=
N
X
i=1
miri âˆ’
N
X
i=1
mir0
=
N
X
i=1
miri âˆ’
N
X
i=1
mi
ÃƒPN
i=1 rimi
PN
i=1 mi
!
= 0.
Let M â‰¡PN
i=1 mi be the total mass. Then 24.13 reduces to
E
=
N
X
i=1
1
2mi
h
|râ€²
i âˆ’râ€²
0|2 + |râ€²
0|2i
=
1
2M |râ€²
0|2 +
N
X
i=1
1
2mi |râ€²
i âˆ’râ€²
0|2 .
(24.14)
The ï¬rst term is just the kinetic energy of a point mass equal to the sum of all the masses
involved, located at the center of mass of the system of masses while the second term
represents kinetic energy which comes from the relative velocities of the masses taken with
respect to the center of mass. It is this term which is considered more carefully in the case
where the system of masses maintain distance between each other.
To illustrate the contrast between the case where the masses maintain a constant distance
and one in which they donâ€™t, take a hard boiled egg and spin it and then take a raw egg

24.3. FINDING THE MOMENT OF INERTIA AND CENTER OF MASS 13 NOV.447
and give it a spin. You will certainly feel a big diï¬€erence in the way the two eggs respond.
Incidentally, this is a good way to tell whether the egg has been hard boiled or is raw and
can be used to prevent messiness which could occur if you think it is hard boiled and it
really isnâ€™t.
Now let e1 (t) , e2 (t) , and e3 (t) be an orthonormal set of vectors which is ï¬xed in the
body undergoing rigid body motion. This means that ri (t) âˆ’r0 (t) has components which
are constant in t with respect to the vectors, ei (t) . By Theorem 16.4.2 on Page 300 there
exists a vector, â„¦(t) which does not depend on i such that
râ€²
i (t) âˆ’râ€²
0 (t) = â„¦(t) Ã— (ri (t) âˆ’r0 (t)) .
Now using this in 24.14,
E
=
1
2M |râ€²
0|2 +
N
X
i=1
1
2mi |â„¦(t) Ã— (ri (t) âˆ’r0 (t))|2
=
1
2M |râ€²
0|2 + 1
2
Ãƒ N
X
i=1
mi |ri (t) âˆ’r0 (t)|2 sin2 Î¸i
!
|â„¦(t)|2
=
1
2M |râ€²
0|2 + 1
2
Ãƒ N
X
i=1
mi |ri (0) âˆ’r0 (0)|2 sin2 Î¸i
!
|â„¦(t)|2
where Î¸i is the angle between â„¦(t) and the vector, ri (t)âˆ’r0 (t) . Therefore, |ri (t) âˆ’r0 (t)| sin Î¸i
is the distance between the point mass, mi located at ri and a line through the center of
mass, r0 with direction, â„¦as indicated in the following picture.
Â©Â©Â©Â©Â©Â©Â©Â©Â©Â©
*
Â¡
Â¡
Â¡
Â¡
Â¡

rmi
â„¦(t)
ri(t) âˆ’r0(t)
Î¸i
Thus the expression, PN
i=1 mi |ri (0) âˆ’r0 (0)|2 sin2 Î¸i plays the role of a mass in the
deï¬nition of kinetic energy except instead of the speed, substitute the angular speed, |â„¦(t)| .
It is this expression which is called the moment of inertia about the line whose direction is
â„¦(t) .
In both of these examples, the center of mass and the moment of inertia occurred in a
natural way.
24.3
Finding The Moment Of Inertia And Center Of
Mass 13 Nov.
The methods used to evaluate multiple integrals make possible the determination of centers
of mass and moments of inertia. In the case of a solid material rather than ï¬nitely many
point masses, you replace the sums with integrals. The sums are essentially approximations
of the integrals which result. This leads to the following deï¬nition.
Deï¬nition 24.3.1 Let a solid occupy a region R such that its density is Î´ (x) for
x a point in R and let L be a line. For x âˆˆR, let l (x) be the distance from the point, x to
the line L. The moment of inertia of the solid is deï¬ned as
Z
R
l (x)2 Î´ (x) dV.

448
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
Letting (xc, yc, zc) denote the Cartesian coordinates of the center of mass,
xc
=
R
R xÎ´ (x) dV
R
R Î´ (x) dV , yc =
R
R yÎ´ (x) dV
R
R Î´ (x) dV ,
zc
=
R
R zÎ´ (x) dV
R
R Î´ (x) dV
where x, y, z are the Cartesian coordinates of the point at x.
Example 24.3.2 Let a solid occupy the three dimensional region R and suppose the density
is Ï. What is the moment of inertia of this solid about the z axis? What is the center of
mass?
Here the little masses would be of the form Ï (x) dV where x is a point of R. Therefore,
the contribution of this mass to the moment of inertia would be
Â¡
x2 + y2Â¢
Ï (x) dV
where the Cartesian coordinates of the point x are (x, y, z) . Then summing these up as an
integral, yields the following for the moment of inertia.
Z
R
Â¡
x2 + y2Â¢
Ï (x) dV.
(24.15)
To ï¬nd the center of mass, sum up rÏ dV for the points in R and divide by the total
mass. In Cartesian coordinates, where r = (x, y, z) , this means to sum up vectors of the
form (xÏ dV, yÏ dV, zÏ dV ) and divide by the total mass. Thus the Cartesian coordinates of
the center of mass are
ÂµR
R xÏ dV
R
R Ï dV ,
R
R yÏ dV
R
R Ï dV ,
R
R zÏ dV
R
R Ï dV
Â¶
â‰¡
R
R rÏ dV
R
R Ï dV .
Here is a speciï¬c example.
Example 24.3.3 Find the moment of inertia about the z axis and center of mass of the solid
which occupies the region, R deï¬ned by 9 âˆ’
Â¡
x2 + y2Â¢
â‰¥z â‰¥0 if the density is Ï (x, y, z) =
p
x2 + y2.
This moment of inertia is
R
R
Â¡
x2 + y2Â¢ p
x2 + y2 dV and the easiest way to ï¬nd this
integral is to use cylindrical coordinates. Thus the answer is
Z 2Ï€
0
Z 3
0
Z 9âˆ’r2
0
r3r dz dr dÎ¸ = 8748
35 Ï€.
To ï¬nd the center of mass, note the x and y coordinates of the center of mass,
R
R xÏ dV
R
R Ï dV ,
R
R yÏ dV
R
R Ï dV
both equal zero because the above shape is symmetric about the z axis and Ï is also sym-
metric in its values. Thus xÏ dV will cancel with âˆ’xÏ dV and a similar conclusion will hold
for the y coordinate. It only remains to ï¬nd the z coordinate of the center of mass, zc. In
polar coordinates, Ï = r and so,
zc =
R
R zÏ dV
R
R Ï dV
=
R 2Ï€
0
R 3
0
R 9âˆ’r2
0
zr2 dz dr dÎ¸
R 2Ï€
0
R 3
0
R 9âˆ’r2
0
r2 dz dr dÎ¸
= 18
7 .

24.4.
EXERCISES WITH ANSWERS
449
Thus the center of mass will be
Â¡
0, 0, 18
7
Â¢
.
A short comment about terminology is in order. When the density is constant, the center
of mass is called the centroid. Thus the centroid is a purely geometrical concept because
the densities will cancel from the integrals.
24.4
Exercises With Answers
1. Let R denote the ï¬nite region bounded by z = 4 âˆ’x2 âˆ’y2 and the xy plane. Find zc,
the z coordinate of the center of mass if the density, Ïƒ is a constant.
The region, R is a dome shaped region above the circle centered at the origin having
radius 2. Therefore, using polar or cylindrical coordinates
zc =
R
R zÏƒdV
R
R ÏƒdV
=
R 2Ï€
0
R 2
0
R 4âˆ’r2
0
zrdzdrdÎ¸
R 2Ï€
0
R 2
0
R 4âˆ’r2
0
rdzdrdÎ¸
= 4
3
2. Let R denote the ï¬nite region bounded by z = 4 âˆ’x2 âˆ’y2 and the xy plane. Find zc,
the z coordinate of the center of mass if the density, Ïƒ is equals Ïƒ (x, y, z) = z.
This problem is just like the one above except here the density is not constant. Thus
zc =
R
R z2dV
R
R zdV
=
R 2Ï€
0
R 2
0
R 4âˆ’r2
0
z2rdzdrdÎ¸
R 2Ï€
0
R 2
0
R 4âˆ’r2
0
zrdzdrdÎ¸
= 2
3. Find the mass and center of mass of the region between the surfaces z = âˆ’y2 + 8 and
z = 2x2 + y2 if the density equals Ïƒ = 1.
To ï¬nd where (x, y) is you let âˆ’y2 + 8 = 2x2 + y2 and this shows the two surfaces
intersect in the circle x2 + y2 = 4. Using cylindrical coordinates,
zc
=
R
R zdV
R
R 1dV =
R 2Ï€
0
R 2
0
R 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸) zrdzdrdÎ¸
R 2Ï€
0
R 2
0
R 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸) rdzdrdÎ¸
=
Â¡ 224
3 Ï€
Â¢
16Ï€
= 14
3
You can ï¬nd the the others the same way.
xc =
R
R xdV
R
R 1dV =
R 2Ï€
0
R 2
0
R 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸) (r cos (Î¸)) rdzdrdÎ¸
R 2Ï€
0
R 2
0
R 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸) rdzdrdÎ¸
= 0
yc =
R
R ydV
R
R 1dV =
R 2Ï€
0
R 2
0
R 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸) (r sin (Î¸)) rdzdrdÎ¸
R 2Ï€
0
R 2
0
R 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸) rdzdrdÎ¸
= 0
Thus the center of mass is
Â¡
0, 0, 14
3
Â¢
. The mass is
Z 2Ï€
0
Z 2
0
Z 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸)
rdzdrdÎ¸ = 16Ï€

450
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
4. Find the mass and center of mass of the region between the surfaces z = âˆ’y2 + 8 and
z = 2x2 + y2 if the density equals Ïƒ (x, y, z) = x2.
This is just like the problem above only now the density is not constant.
zc =
R
R zx2dV
R
R x2dV
=
R 2Ï€
0
R 2
0
R 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸) z
Â¡
r2 cos2 (Î¸)
Â¢
rdzdrdÎ¸
R 2Ï€
0
R 2
0
R 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸) r (r2 cos2 (Î¸)) dzdrdÎ¸
= 11
2
yc =
R
R yx2dV
R
R x2dV
=
R 2Ï€
0
R 2
0
R 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸) (r sin (Î¸))
Â¡
r2 cos2 (Î¸)
Â¢
rdzdrdÎ¸
R 2Ï€
0
R 2
0
R 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸) r (r2 cos2 (Î¸)) dzdrdÎ¸
= 0
xc =
R
R xx2dV
R
R x2dV
=
R 2Ï€
0
R 2
0
R 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸) (r cos (Î¸))
Â¡
r2 cos2 (Î¸)
Â¢
rdzdrdÎ¸
R 2Ï€
0
R 2
0
R 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸) r (r2 cos2 (Î¸)) dzdrdÎ¸
= 0
So in this case the center of mass is
Â¡
0, 0, 11
2
Â¢
. The mass is
Z 2Ï€
0
Z 2
0
Z 8âˆ’r2 sin2(Î¸)
2r2 cos2(Î¸)+r2 sin2(Î¸)
r
Â¡
r2 cos2 (Î¸)
Â¢
dzdrdÎ¸ = 32
3 Ï€
5. The two cylinders, x2 +y2 = 4 and y2 +z2 = 4 intersect in a region, R. Find the mass
and center of mass if the density, Ïƒ, is given by Ïƒ (x, y, z) = z2.
The ï¬rst cylinder is parallel to the z axis. Let D denote the circle of radius 2 in the xy
plane. Then the region just described has (x, y) in the circle of radius 2 and z between
âˆ’
p
4 âˆ’y2 and
p
4 âˆ’y2. It follows the total mass is
Z 2
âˆ’2
Z âˆš
4âˆ’y2
âˆ’âˆš
4âˆ’y2
Z âˆš
4âˆ’y2
âˆ’âˆš
4âˆ’y2 z2dzdxdy = 2048
45 .
By symmetry, the center of mass will be (0, 0, 0) .
6. The two cylinders, x2 +y2 = 4 and y2 +z2 = 4 intersect in a region, R. Find the mass
and center of mass if the density, Ïƒ, is given by Ïƒ (x, y, z) = 4 + z.
The total mass is
Z 2
âˆ’2
Z âˆš
4âˆ’y2
âˆ’âˆš
4âˆ’y2
Z âˆš
4âˆ’y2
âˆ’âˆš
4âˆ’y2 (4 + z) dzdxdy = 512
3
zc =
R 2
âˆ’2
R âˆš
4âˆ’y2
âˆ’âˆš
4âˆ’y2
R âˆš
4âˆ’y2
âˆ’âˆš
4âˆ’y2 z (4 + z) dzdxdy
Â¡ 512
3
Â¢
= 4
15
xc =
R 2
âˆ’2
R âˆš
4âˆ’y2
âˆ’âˆš
4âˆ’y2
R âˆš
4âˆ’y2
âˆ’âˆš
4âˆ’y2 x (4 + z) dzdxdy
Â¡ 512
3
Â¢
= 0
yc =
R 2
âˆ’2
R âˆš
4âˆ’y2
âˆ’âˆš
4âˆ’y2
R âˆš
4âˆ’y2
âˆ’âˆš
4âˆ’y2 y (4 + z) dzdxdy
Â¡ 512
3
Â¢
= 0
and so the center of mass is
Â¡
0, 0, 4
15
Â¢
.

24.4.
EXERCISES WITH ANSWERS
451
7. Find the mass and center of mass of the set, (x, y, z) such that x2
4 + y2
9 + z2 â‰¤1 if the
density is Ïƒ (x, y, z) = 4 + y + z.
This is the inside of an ellipsoid. Denote this by R. Then the total mass is
Z
R
ÏƒdV =
Z
R
(4 + y + z) dV
Lets change the variables. Let x = 2u, y = 3v, z = w. When this is done, (u, v, w)
will be in the unit ball. The Jacobian of this transformation is 6. Now changing the
variables the above integral equals
Z
B
(4 + 3v + w) 6dV
where here B is the unit ball. When integrating over a ball, you ought to suspect that
spherical coordinates would be a good idea. Change the variables again in the above
integral to spherical coordinates.
w = Ï cos Ï†, v = Ï sin Ï† sin Î¸, u = Ï sin Ï† cos Î¸.
Then the above integral in spherical coordinates is
Z Ï€
0
Z 2Ï€
0
Z 1
0
(4 + 3Ï sin (Ï†) sin (Î¸) + Ï cos Ï†) 6Ï2 sin Ï†dÏdÎ¸dÏ† = 32Ï€.
To ï¬nd the center of mass, it would be
zc =
R
R z (4 + y + z) dV
32Ï€
=
R
R z2dV
32Ï€
.
Now to get this, I have used symmetry of the region. This equals
R
R w2dV
32Ï€
=
R Ï€
0
R 2Ï€
0
R 1
0 (Ï cos (Ï†))2 6Ï2 sin Ï†dÏdÎ¸dÏ†
32Ï€
= 1
20
yc
=
R
R y (4 + y + z) dV
32Ï€
=
R
R y2dV
32Ï€
=
R Ï€
0
R 2Ï€
0
R 1
0 (3 (Ï sin (Ï†) sin (Î¸)))2 6Ï2 sin Ï†dÏdÎ¸dÏ†
32Ï€
= 9
20
I think you get the idea. You can now ï¬nd xc in the same way.
8. Let R denote the ï¬nite region bounded by z = 9 âˆ’x2 âˆ’y2 and the xy plane. Find the
moment of inertia of this shape about the z axis given the density equals 1.
Using cylindrical coordinates, this is
Z
R
r2dV =
Z 2Ï€
0
Z 3
0
Z 9âˆ’r2
0
r2rdzdrdÎ¸ = 243
2 Ï€
9. Let R denote the ï¬nite region bounded by z = 9 âˆ’x2 âˆ’y2 and the xy plane. Find the
moment of inertia of this shape about the x axis given the density equals 1.
It is like the above except diï¬€erent.
Z 2Ï€
0
Z 3
0
Z 9âˆ’r2
0
Â¡
r2 sin2 (Î¸) + z2Â¢
rdzdrdÎ¸ = 1215
2
Ï€

452
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.
10. Let B be a solid ball of constant density and radius R. Find the moment of inertia
about a line through a diameter of the ball. You should get 2
5R2M where M is the
mass.
The constant density of the ball is 3
4
M
Ï€R3 . For simplicity let the line be the z axis. I
will also use spherical coordinates since this is a ball. Then the moment of inertia is
Z Ï€
0
Z 2Ï€
0
Z R
0
3
4
M
Ï€R3 (Ï sin (Ï†))2 Ï2 sin (Ï†) dÏdÎ¸dÏ† = 2
5R2M.
11. Let B be a solid ball of density, Ïƒ = Ï where Ï is the distance to the center of the ball
which has radius R. Find the moment of inertia about a line through a diameter of
the ball. Write your answer in terms of the total mass and the radius as was done in
the constant density case.
Z Ï€
0
Z 2Ï€
0
Z R
0
Ï (Ï sin (Ï†))2 Ï2 sin (Ï†) dÏdÎ¸dÏ† = 4
9Ï€R6
Also the total mass is
M =
Z Ï€
0
Z 2Ï€
0
Z R
0
ÏÏ2 sin (Ï†) dÏdÎ¸dÏ† = Ï€R4
Therefore, the moment of inertia is
4
9MR2.
12. Let C be a solid cylinder of constant density and radius R. Find the moment of inertia
about the axis of the cylinder
You should get 1
2R2M where M is the mass.
The density is
M
Ï€R2h where h is the height of the cylinder. Using cylindrical coordinates,
the moment of inertia is
Z 2Ï€
0
Z R
0
Z h
0
Âµ M
Ï€R2h
Â¶
r2rdzdrdÎ¸ = 1
2R2M
13. Let C be a solid cylinder of constant density and radius R and mass M and let B be
a solid ball of radius R and mass M. The cylinder and the sphere are placed on the
top of an inclined plane and allowed to roll to the bottom. Which one will arrive ï¬rst
and why?
The sphere will win. This is because it takes less torque to produce a given angular
acceleration in the sphere than in the cylinder because the moment of inertia for the
sphere is less than the moment of inertia of the cylinder. Thus a given torque about
the axis of roation, which will be identical in both will produce faster rotation in the
sphere than in the cylinder. Another way to look at it is that they both have the
same total energy when they get to the bottom. This energy comes from two parts,
one involving rotation and the other translation of the center of mass. If the center of
mass of both were moving at the same speed, this would be a contradiction because
the diï¬€erent moments of inertia would then require the kinetic energy of one to be
greater than that of the other.

24.4.
EXERCISES WITH ANSWERS
453
14. Suppose a solid of mass M occupying the region, B has moment of inertia, Il about a
line, l which passes through the center of mass of M and let l1 be another line parallel
to l and at a distance of a from l. Then the parallel axis theorem states Il1 = Il +a2M.
Prove the parallel axis theorem. Hint: Choose axes such that the z axis is l and l1
passes through the point (a, 0) in the xy plane.
Consider the following picture in which, as suggested, the line, l is the z axis and l1
goes through (a, 0) in the xy plane and is parallel to l.
l1
l
B
For x a point in B, let the coordinates of this point be (x, y, z) . Then the displacement
vector from a point, (a, 0, z) on l1 to the point, (x, y, z) is (x âˆ’a, âˆ’y, 0) and so the
square of the distance is x2 âˆ’2xa + a2 + y2. Therefore, from the deï¬nition of moment
of inertia, the moment of inertia about l1 is
Il1 â‰¡
Z
B
Î´ (x, y, z)
Â¡
x2 âˆ’2xa + a2 + y2Â¢
dV
Since the line goes through the center of mass, this reduces to
Z
B
Î´ (x, y, z)
Â¡
x2 + y2Â¢
dV +
Z
B
Î´ (x, y, z) a2dV â‰¡Il + a2M
15. Using the parallel axis theorem ï¬nd the moment of inertia of a solid ball of radius R
and mass M about an axis located at a distance of a from the center of the ball. Your
answer should be Ma2 + 2
5MR2.
16. Consider all axes in computing the moment of inertia of a solid. Will the smallest
possible moment of inertia always result from using an axis which goes through the
center of mass?
The answer is yes. To see this, consider the parallel axis theorem above.

454
THE INTEGRAL IN OTHER COORDINATES 8-10 NOV.

Part X
Line Integrals
455


457
Outcomes
Line Integrals
A. Evaluate the work done by a varying force over a curved path.
B. Evaluate line integrals in general including line integrals with respect to arc length.
C. Evaluate the physical characteristics of a wire such as centroid, mass, and center of
mass using line integrals.
Reading: Multivariable Calculus 4.2
Outcome Mapping:
A. 1,9
B. 2,3,4
C. 5,6
Path Independent Line Integrals
A. Recall and apply the Fundamental Theorem for Line Integrals.
B. Determine whether or not a force ï¬eld is conservative, and if so, ï¬nd its potential.
C. Evaluate the circulation of a force ï¬eld or the work done by a force ï¬eld on a object
moving along a given path.
Reading: Multivariable Calculus 4.3
Outcome Mapping:
A. K1,1
B. 3,6
C. 2,4
Recovering a Function from its Gradient
A. Analyze the characteristics of a vector ï¬eld. Sketch a vector ï¬eld.
B. Determine whether a vector ï¬eld is a gradient.
C. Determine whether a diï¬€erential form is exact.
D. Recover a function from its gradient or diï¬€erential form, if possible.
Reading: Multivariable Calculus 4.1
Outcome Mapping:
A. 1,7
B. 5
C. 4,6
D. 4,5,7

458

Line Integrals 14 Nov.
The concept of the integral can be extended to functions which are not deï¬ned on an interval
of the real line but on some curve in Rn. This is done by deï¬ning things in such a way that
the more general concept reduces to the earlier notion. First it is necessary to consider what
is meant by arc length.
25.0.1
Orientations And Smooth Curves
Recall the notion of a smooth curve.
C is a smooth curve in Rn if there exists an interval, [a, b] âŠ†R and functions xi :
[a, b] â†’R such that the following conditions hold
1. xi is continuous on [a, b] .
2. xâ€²
i exists and is continuous and bounded on [a, b] , with xâ€²
i (a) deï¬ned as the derivative
from the right,
lim
hâ†’0+
xi (a + h) âˆ’xi (a)
h
,
and xâ€²
i (b) deï¬ned similarly as the derivative from the left.
3. For p (t) â‰¡(x1 (t) , Â· Â· Â·, xn (t)) , t â†’p (t) is one to one on (a, b) .
4. |pâ€² (t)| â‰¡
Â³Pn
i=1 |xâ€²
i (t)|2Â´1/2
Ì¸= 0 for all t âˆˆ[a, b] .
5. C = âˆª{(x1 (t) , Â· Â· Â·, xn (t)) : t âˆˆ[a, b]} .
The functions, xi (t) , deï¬ned above are giving the coordinates of a point in Rn and the
list of these functions is called a parameterization for the smooth curve. Note the natural
direction of the interval also gives a direction for moving along the curve. Such a direction
is called an orientation.
The proof that curve length is well deï¬ned for a smooth curve contains a result which
deserves to be stated as a corollary. It is proves in the Section which starts on Page 295.
This is one of those sections you should read only if you are interested.
Corollary 25.0.1 Let C be a smooth curve and let f : [a, b] â†’C and g : [c, d] â†’C be
two parameterizations satisfying 1 - 5. Then gâˆ’1 â—¦f is either strictly increasing or strictly
decreasing.
Deï¬nition 25.0.2 If gâˆ’1 â—¦f is increasing, then f and g are said to be equivalent
parameterizations and this is written as f âˆ¼g. It is also said that the two parameterizations
give the same orientation for the curve when f âˆ¼g.
459

460
LINE INTEGRALS 14 NOV.
When the parameterizations are equivalent, they preserve the direction, of motion along
the curve and this also shows there are exactly two orientations of the curve since either
gâˆ’1 â—¦f is increasing or it is decreasing. This is not hard to believe. In simple language, the
message is that there are exactly two directions of motion along a curve. The diï¬ƒculty is in
proving this is actually the case based only on the assumption that the parameterizations
of the curve are one to one.
Lemma 25.0.3 The following hold for âˆ¼.
f âˆ¼f,
(25.1)
If f âˆ¼g then g âˆ¼f,
(25.2)
If f âˆ¼g and g âˆ¼h, then f âˆ¼h.
(25.3)
Proof: Formula 25.1 is obvious because f âˆ’1 â—¦f (t) = t so it is clearly an increasing
function. If f âˆ¼g then f âˆ’1 â—¦g is increasing. Now gâˆ’1 â—¦f must also be increasing because
it is the inverse of f âˆ’1 â—¦g. This veriï¬es 25.2. To see 25.3, f âˆ’1 â—¦h =
Â¡
f âˆ’1 â—¦g
Â¢
â—¦
Â¡
gâˆ’1 â—¦h
Â¢
and so since both of these functions are increasing, it follows f âˆ’1 â—¦h is also increasing. This
proves the lemma.
The symbol, âˆ¼is called an equivalence relation.
If C is such a smooth curve just
described, and if f : [a, b] â†’C is a parameterization of C, consider g (t) â‰¡f ((a + b) âˆ’t) ,
also a parameterization of C. Now by Corollary 25.0.1, if h is a parameterization, then if
f âˆ’1 â—¦h is not increasing, it must be the case that gâˆ’1 â—¦h is increasing. Consequently, either
h âˆ¼g or h âˆ¼f. These parameterizations, h, which satisfy h âˆ¼f are called the equivalence
class determined by f and those h âˆ¼g are called the equivalence class determined by g.
These two classes are called orientations of C. They give the direction of motion on C.
You see that going from f to g corresponds to tracing out the curve in the opposite direction.
Sometimes people wonder why it is required, in the deï¬nition of a smooth curve that
pâ€² (t) Ì¸= 0. Imagine t is time and p (t) gives the location of a point in space. If pâ€² (t) is allowed
to equal zero, the point can stop and change directions abruptly, producing a pointy place
in C. Here is an example.
Example 25.0.4 Graph the curve
Â¡
t3, t2Â¢
for t âˆˆ[âˆ’1, 1] .
In this case, t = x1/3 and so y = x2/3. Thus the graph of this curve looks like the picture
below. Note the pointy place. Such a curve should not be considered smooth! If it were a
banister and you were sliding down it, it would be clear at a certain point that the curve is
not smooth. I think you may even get the point of this from the picture below.
So what is the thing to remember from all this? First, there are certain conditions which
must be satisï¬ed for a curve to be smooth. These are listed in 1 - 5. Next, if you have any
curve, there are two directions you can move over this curve, each called an orientation.
This is illustrated in the following picture.

461
p
q
p
q
Either you move from p to q or you move from q to p.
Deï¬nition 25.0.5 A curve C is piecewise smooth if there exist points on this curve,
p0, p1, Â· Â· Â·, pn such that, denoting Cpkâˆ’1pk the part of the curve joining pkâˆ’1 and pk, it
follows Cpkâˆ’1pk is a smooth curve and âˆªn
k=1Cpkâˆ’1pk = C. In other words, it is piecewise
smooth if it consists of a ï¬nite number of smooth curves linked together.
Note that Example 25.0.4 is an example of a piecewise smooth curve although it is not
smooth.
25.0.2
The Integral Of A Function Deï¬ned On A Smooth Curve
Letting r (t) , t âˆˆ[a, b] be the position vector of a smooth curve, recall that the total length
of this curve is given by
l =
Z b
a
|râ€² (t)| dt.
(25.4)
Remember that if you interpret t as time, |râ€² (t)| is the speed and the above integral says
that to get the total distance you simply integrate the speed. A small chunk of distance
traveled is dl = |râ€² (t)| dt. This says the same thing as
dl
dt = |râ€² (t)|
which was discussed earlier. Of course it follows from 25.4 and the fundamental theorem of
calculus. The distance for the parameter between a and t is
l (t) =
Z t
a
|râ€² (s)| ds
and so by the fundamental theorem of calculus,
lâ€² (t) = dl
dt = |râ€² (t)| .
For this reason, the increment of arc length is dl = |râ€² (t)| dt. Think of it as giving an
inï¬nitesimal contribution to the integral. For C a smooth curve with a parameterization,
r : [a, b] â†’C and a function, f deï¬ned on C, deï¬ne the symbol,
Z
C
fdl â‰¡
Z b
a
f (r (t)) |râ€² (t)| dt.
Example 25.0.6 Let C be a smooth curve which has parameterization given by r (t) =
(cos 2t, sin (2t) , t) for t âˆˆ[0, 2Ï€] . Suppose f (x, y, z) = x2 + y. Find
R
C fdl.

462
LINE INTEGRALS 14 NOV.
The increment of length is
q
4 cos2 (2t) + 4 sin2 (2t) + 1dt =
âˆš
5dt. Now the desired in-
tegral is
Z 2Ï€
0
Â¡
cos2 2t + sin (2t)
Â¢ âˆš
5dt =
âˆš
5Ï€
One can deï¬ne things like density with respect to arc length in the usual way. As just
explained, a little chunk of length is dl = |râ€² (t)| dt. The density is a function Î´ (x, y, z) which
has the property that a little chunk of mass is given by dm = Î´ (r (t)) dl.
Deï¬nition 25.0.7 Let Î´ be the density with respect to arc length. Then the total
mass of a smooth curve, C having parameterization r : [a, b] â†’R3 is
Z
C
Î´ (x, y, z) dl =
Z b
a
Î´ (r (t)) |râ€² (t)| dt
the center of mass can be given in a similar manner as before. Thus
xc
â‰¡
R
C xÎ´ (x, y, z) dl
R
C Î´ (x, y, z) dl , yc =
R
C yÎ´ (x, y, z) dl
R
C Î´ (x, y, z) dl
zc
=
R
C zÎ´ (x, y, z) dl
R
C Î´ (x, y, z) dl
and the only thing you need to do is to evaluate the integrals after changing everything to
give a one dimensional integral with respect to the parameter t.
Example 25.0.8 Let a smooth curve be given by the parameterization, r (t) = (cos t, sin t, t) :
t âˆˆ[0, 10] . This is a helix in case you are interested.
Suppose the density is given by
Î´ (x, y, z) = x2. Find the total mass and the center of mass.
The increment of arc length is dl =
q
sin2 (t) + cos2 (t) + 1dt =
âˆš
2dt. Then the total
mass is
Z 10
0
cos2 (t)
âˆš
2dt = 1
2
âˆš
2 cos (10) sin (10) + 5
âˆš
2
The center of mass is given by
xc =
R 10
0
(cos (t)) cos2 (t)
âˆš
2dt
1
2
âˆš
2 cos (10) sin (10) + 5
âˆš
2 =
1
3
âˆš
2 sin 10 cos2 10 + 2
3
âˆš
2 sin 10
1
2
âˆš
2 cos 10 sin 10 + 5
âˆš
2
yc =
R 10
0
(sin (t)) cos2 (t)
âˆš
2dt
1
2
âˆš
2 cos (10) sin (10) + 5
âˆš
2 = âˆ’1
3
Â¡
cos3 10
Â¢ âˆš
2 + 1
3
âˆš
2
1
2
âˆš
2 cos 10 sin 10 + 5
âˆš
2
zc =
R 10
0
t cos2 (t)
âˆš
2dt
1
2
âˆš
2 cos (10) sin (10) + 5
âˆš
2 = 5
âˆš
2 cos 10 sin 10 + 99
4
âˆš
2 + 1
4
âˆš
2 cos2 10
1
2
âˆš
2 cos 10 sin 10 + 5
âˆš
2
25.0.3
Vector Fields
A vector ï¬eld is nothing but a function which has values which are vectors. For example,
consider the force acting on a unit mass by the sun. This determines a force vector which
depends on the location of the point. Thus each point in space has associated with it a
vector which is the force which the sun exerts on a particle of mass 1 which is placed at
that point.

463
Some people ï¬nd it useful to try and draw pictures to illustrate a vector valued function
or vector ï¬eld. This can be a very useful idea in the case where the function takes points in
D âŠ†R2 and delivers a vector in R2.
For many points, (x, y) âˆˆD, you draw an arrow of the appropriate length and direction
with its tail at (x, y). The picture of all these arrows can give you an understanding of what
is happening. For example if the vector valued function gives the velocity of a ï¬‚uid at the
point, (x, y) , the picture of these arrows can give an idea of the motion of the ï¬‚uid. When
they are long the ï¬‚uid is moving fast, when they are short, the ï¬‚uid is moving slowly the
direction of these arrows is an indication of the direction of motion. The only sensible way
to produce such a picture is with a computer. Otherwise, it becomes a worthless exercise
in busy work. Furthermore, it is of limited usefulness in three dimensions because in three
dimensions such pictures are too cluttered to convey much insight.
Example 25.0.9 Draw a picture of the vector ï¬eld, (âˆ’x, y) which gives the velocity of a
ï¬‚uid ï¬‚owing in two dimensions.
â€“2
â€“1
0
1
2
y
â€“2
â€“1
1
2
x
In this example, drawn by Maple, you can see how the arrows indicate the motion of
this ï¬‚uid.
Example 25.0.10 Draw a picture of the vector ï¬eld (y, x) for the velocity of a ï¬‚uid ï¬‚owing
in two dimensions.
â€“2
â€“1
0
1
2
y
â€“2
â€“1
1
2
x
So much for art. Get the computer to do it and it can be useful. If you try to do it, you
will mainly waste time.

464
LINE INTEGRALS 14 NOV.
Example 25.0.11 Draw a picture of the vector ï¬eld (y cos (x) + 1, x sin (y) âˆ’1) for the
velocity of a ï¬‚uid ï¬‚owing in two dimensions.
â€“2
â€“1
0
1
2
y
â€“2
â€“1
1
2
x
25.0.4
Line Integrals And Work
The interesting concept of line integral has to do with integrals which involve vector ï¬elds,
not scalar valued functions as above. The most signiï¬cant application is to work.
First, it is necessary to give some discussion of the concept of orientation. Let C be a
smooth curve contained in Rp. A curve, C is an â€œoriented curveâ€ if the only parameteriza-
tions considered are those which lie in exactly one of the two equivalence classes discussed in
Deï¬nition 25.0.2, each of which is called an â€œorientationâ€. In simple language, orientation
speciï¬es a direction over which motion along the curve is to take place. Thus, it speciï¬es
the order in which the points of C are encountered. The pair of concepts consisting of the
set of points making up the curve along with a direction of motion along the curve is called
an oriented curve.
Deï¬nition 25.0.12 Suppose F (x) âˆˆRp is given for each x âˆˆC where C is a
smooth oriented curve and suppose x â†’F (x) is continuous. The mapping x â†’F (x) is
called a vector ï¬eld. In the case that F (x) is a force, it is called a force ï¬eld.
Next the concept of work done by a force ï¬eld, F on an object as it moves along the
curve, C, in the direction determined by the given orientation of the curve will be deï¬ned.
This is new. Earlier the work done by a force which acts on an object moving in a straight
line was discussed but here the object moves over a curve. In order to deï¬ne what is meant
by the work, consider the following picture.
Â¡
Â¡
Â¡
Â¡
Â¡

x(t)
F(x(t))
x(t + h)

465
In this picture, the work done by a force, F on an object which moves from the point
x (t) to the point x (t + h) along the straight line shown would equal F Â· (x (t + h) âˆ’x (t)) .
It is reasonable to assume this would be a good approximation to the work done in moving
along the curve joining x (t) and x (t + h) provided h is small enough. Also, provided h is
small,
x (t + h) âˆ’x (t) â‰ˆxâ€² (t) h
where the wriggly equal sign indicates the two quantities are close.
In the notation of
Leibniz, one writes dt for h and
dW = F (x (t)) Â· xâ€² (t) dt
Thus the total work along the whole curve should be given by the integral,
Z b
a
F (x (t)) Â· xâ€² (t) dt
This motivates the following deï¬nition of work.
Deï¬nition 25.0.13 Let F (x) be given above. Then the work done by this force
ï¬eld on an object moving over the curve C in the direction determined by the speciï¬ed
orientation is deï¬ned as
Z
C
F Â· dR â‰¡
Z b
a
F (x (t)) Â· xâ€² (t) dt
where the function, x is one of the allowed parameterizations of C in the given orientation
of C. In other words, there is an interval, [a, b] and as t goes from a to b, x (t) moves in the
direction determined from the given orientation of the curve.
Theorem 25.0.14 The symbol,
R
C F Â· dR, is well deï¬ned in the sense that every
parameterization in the given orientation of C gives the same value for
R
C F Â· dR.
Proof: Suppose g : [c, d] â†’C is another allowed parameterization. Thus gâˆ’1 â—¦f is an
increasing function, Ï†. Letting s = Ï† (t) and changing variables, and using the fact Ï† is
increasing,
Z d
c
F (g (s)) Â· gâ€² (s) ds =
Z b
a
F (g (Ï† (t))) Â· gâ€² (Ï† (t)) Ï†â€² (t) dt
=
Z b
a
F (f (t)) Â· d
dt
Â¡
g
Â¡
gâˆ’1 â—¦f (t)
Â¢Â¢
dt =
Z b
a
F (f (t)) Â· f â€² (t) dt.
This proves the theorem.
Regardless the physical interpretation of F, this is called the line integral. When F is
interpreted as a force, the line integral measures the extent to which the motion over the
curve in the indicated direction is aided by the force. If the net eï¬€ect of the force on the
object is to impede rather than to aid the motion, this will show up as negative work.
Does the concept of work as deï¬ned here coincide with the earlier concept of work when
the object moves over a straight line when acted on by a constant force?
Let p and q be two points in Rn and suppose F is a constant force acting on an object
which moves from p to q along the straight line joining these points.
Then the work
done is F Â· (q âˆ’p) . Is the same thing obtained from the above deï¬nition?
Let x (t) â‰¡
p+t (q âˆ’p) , t âˆˆ[0, 1] be a parameterization for this oriented curve, the straight line in the

466
LINE INTEGRALS 14 NOV.
direction from p to q. Then xâ€² (t) = q âˆ’p and F (x (t)) = F. Therefore, the above deï¬nition
yields
Z 1
0
F Â· (q âˆ’p) dt = F Â· (q âˆ’p) .
Therefore, the new deï¬nition adds to but does not contradict the old one.
Example 25.0.15 Suppose for t âˆˆ[0, Ï€] the position of an object is given by r (t) = ti +
cos (2t) j + sin (2t) k. Also suppose there is a force ï¬eld deï¬ned on R3, F (x, y, z) â‰¡2xyi +
x2j + k. Find
Z
C
F Â· dR
where C is the curve traced out by this object which has the orientation determined by the
direction of increasing t.
To ï¬nd this line integral use the above deï¬nition and write
Z
C
F Â· dR =
Z Ï€
0
Â¡
2t (cos (2t)) ,t2,1
Â¢
Â·
(1, âˆ’2 sin (2t) , 2 cos (2t)) dt
In evaluating this replace the x in the formula for F with t, the y in the formula for F
with cos (2t) and the z in the formula for F with sin (2t) because these are the values of
these variables which correspond to the value of t. Taking the dot product, this equals the
following integral.
Z Ï€
0
Â¡
2t cos 2t âˆ’2 (sin 2t) t2 + 2 cos 2t
Â¢
dt = Ï€2
Example 25.0.16 Let C denote the oriented curve obtained by r (t) =
Â¡
t, sin t, t3Â¢
where
the orientation is determined by increasing t for t âˆˆ[0, 2] . Also let F = (x, y, xz + z) . Find
R
C FÂ·dR.
You use the deï¬nition.
Z
C
F Â· dR
=
Z 2
0
Â¡
t, sin (t) , (t + 1) t3Â¢
Â·
Â¡
1, cos (t) , 3t2Â¢
dt
=
Z 2
0
Â¡
t + sin (t) cos (t) + 3 (t + 1) t5Â¢
dt
=
1251
14
âˆ’1
2 cos2 (2) .
25.0.5
Another Notation For Line Integrals
Deï¬nition 25.0.17 Let F (x, y, z) = (P (x, y, z) , Q (x, y, z) , R (x, y, z)) and let C
be an oriented curve. Then another way to write
R
C FÂ·dR is
Z
C
Pdx + Qdy + Rdz
This last is referred to as the integral of a diï¬€erential form, Pdx + Qdy + Rdz. The
study of diï¬€erential forms is important. Formally, dR = (dx, dy, dz) and so the integrand in
the above is formally FÂ·dR. Other occurances of this notation are handled similarly in 2 or
higher dimensions.

467
25.0.6
Exercises With Answers
1. Suppose for t âˆˆ[0, 2Ï€] the position of an object is given by r (t) = 2ti + cos (t) j +
sin (t) k. Also suppose there is a force ï¬eld deï¬ned on R3,
F (x, y, z) â‰¡2xyi +
Â¡
x2 + 2zy
Â¢
j + y2k.
Find the work,
Z
C
F Â· dR
where C is the curve traced out by this object which has the orientation determined
by the direction of increasing t.
You might think of dR = râ€² (t) dt to help remember what to do.
Then from the
deï¬nition,
Z
C
F Â· dR =
Z 2Ï€
0
Â¡
2 (2t) (sin t) , 4t2 + 2 sin (t) cos (t) , sin2 (t)
Â¢
Â· (2, âˆ’sin (t) , cos (t)) dt
=
Z 2Ï€
0
Â¡
8t sin t âˆ’
Â¡
2 sin t cos t + 4t2Â¢
sin t + sin2 t cos t
Â¢
dt = 16Ï€2 âˆ’16Ï€
2. Here is a vector ï¬eld,
Â¡
y, x2 + z, 2yz
Â¢
and here is the parameterization of a curve, C.
R (t) = (cos 2t, 2 sin 2t, t) where t goes from 0 to Ï€/4. Find
R
C FÂ· dR.
dR = (âˆ’2 sin (2t) , 4 cos (2t) , 1) dt.
Then by the deï¬nition,
Z
C
F Â· dR =
Z Ï€/4
0
Â¡
2 sin (2t) , cos2 (2t) + t, 4t sin (2t)
Â¢
Â· (âˆ’2 sin (2t) , 4 cos (2t) , 1) dt
=
Z Ï€/4
0
Â¡
âˆ’4 sin2 2t + 4
Â¡
cos2 2t + t
Â¢
cos 2t + 4t sin 2t
Â¢
dt = 4
3
3. Suppose for t âˆˆ[0, 1] the position of an object is given by r (t) = ti + tj + tk. Also
suppose there is a force ï¬eld deï¬ned on R3,
F (x, y, z) â‰¡yzi + xzj + xyk.
Find
Z
C
F Â· dR
where C is the curve traced out by this object which has the orientation determined
by the direction of increasing t. Repeat the problem for r (t) = ti + t2j + tk.
You should get the same answer in this case. This is because the vector ï¬eld happens
to be conservative. (More on this later.)

468
LINE INTEGRALS 14 NOV.
25.1
Path Independent Line Integrals 15 Nov.
Sometimes the line integral giving the work done by a force ï¬eld depends only on the
endpoints of the curve. This is very nice when it happens because it makes the line integral
very easy to compute. It also has great physical signiï¬cance.
Deï¬nition 25.1.1 A vector ï¬eld, F deï¬ned in a three dimensional region is said
to be conservative1 if for every piecewise smooth closed curve, C, it follows
R
C FÂ· dR = 0.
Deï¬nition 25.1.2 Let (x, p1, Â· Â· Â·, pn, y) be an ordered list of points in Rp. Let
p (x, p1, Â· Â· Â·, pn, y)
denote the piecewise smooth curve consisting of a straight line segment from x to p1 and
then the straight line segment from p1 to p2 Â· Â·Â· and ï¬nally the straight line segment from
pn to y. This is called a polygonal curve. An open set in Rp, U, is said to be a region if
it has the property that for any two points, x, y âˆˆU, there exists a polygonal curve joining
the two points.
Conservative vector ï¬elds are important because of the following theorem, sometimes
called the fundamental theorem for line integrals.
Theorem 25.1.3 Let U be a region in Rp and let F : U â†’Rp be a continuous
vector ï¬eld. Then F is conservative if and only if there exists a scalar valued function of p
variables, Ï† such that F = âˆ‡Ï†. Furthermore, if C is an oriented curve which goes from x
to y in U, then
Z
C
F Â· dR = Ï† (y) âˆ’Ï† (x) .
(25.5)
Thus the line integral is path independent in this case. This function, Ï† is called a scalar
potential for F.
Proof: To save space and fussing over things which are unimportant, denote by p (x0, x)
a polygonal curve from x0 to x. Thus the orientation is such that it goes from x0 to x. The
curve p (x, x0) denotes the same set of points but in the opposite order. Suppose ï¬rst F is
conservative. Fix x0 âˆˆU and let
Ï† (x) â‰¡
Z
p(x0,x)
FÂ· dR.
This is well deï¬ned because if q (x0, x) is another polygonal curve joining x0 to x, Then the
curve obtained by following p (x0, x) from x0 to x and then from x to x0 along q (x, x0) is
a closed piecewise smooth curve and so by assumption, the line integral along this closed
curve equals 0. However, this integral is just
Z
p(x0,x)
FÂ· dR+
Z
q(x,x0)
FÂ· dR =
Z
p(x0,x)
FÂ· dRâˆ’
Z
q(x0,x)
FÂ· dR
which shows
Z
p(x0,x)
FÂ· dR =
Z
q(x0,x)
FÂ· dR
1There is no such thing as a liberal vector ï¬eld.

25.1.
PATH INDEPENDENT LINE INTEGRALS 15 NOV.
469
and that Ï† is well deï¬ned. For small t,
Ï† (x + tei) âˆ’Ï† (x)
t
=
R
p(x0,x+tei) F Â· dRâˆ’
R
p(x0,x) F Â· dR
t
=
R
p(x0,x) F Â· dR+
R
p(x,x+tei) F Â· dRâˆ’
R
p(x0,x) F Â· dR
t
.
Since U is open, for small t, the ball of radius |t| centered at x is contained in U. There-
fore, the line segment from x to x + tei is also contained in U and so one can take
p (x, x + tei) (s) = x + s (tei) for s âˆˆ[0, 1]. Therefore, the above diï¬€erence quotient re-
duces to
1
t
Z 1
0
F (x + s (tei)) Â· tei ds
=
Z 1
0
Fi (x + s (tei)) ds
=
Fi (x + st (tei))
by the mean value theorem for integrals. Here st is some number between 0 and 1. By
continuity of F, this converges to Fi (x) as t â†’0. Therefore, âˆ‡Ï† = F as claimed.
Conversely, if âˆ‡Ï† = F, then if R : [a, b] â†’Rp is any C1 curve joining x to y,
Z b
a
F (R (t)) Â·Râ€² (t) dt
=
Z b
a
âˆ‡Ï† (R (t)) Â·Râ€² (t) dt
=
Z b
a
d
dt (Ï† (R (t))) dt
=
Ï† (R (b)) âˆ’Ï† (R (a))
=
Ï† (y) âˆ’Ï† (x)
and this veriï¬es 25.5 in the case where the curve joining the two points is smooth. The
general case follows immediately from this by using this result on each of the pieces of the
piecewise smooth curve. For example if the curve goes from x to p and then from p to y,
the above would imply the integral over the curve from x to p is Ï† (p) âˆ’Ï† (x) while from p
to y the integral would yield Ï† (y) âˆ’Ï† (p) . Adding these gives Ï† (y) âˆ’Ï† (x) . The formula
25.5 implies the line integral over any closed curve equals zero because the starting and
ending points of such a curve are the same. This proves the theorem.
25.1.1
Finding The Scalar Potential, (Recover The Function From
Its Gradient)
Example 25.1.4 Let F (x, y, z) = (cos x âˆ’yz sin (xz) , cos (xz) , âˆ’yx sin (xz)) . Let C be a
piecewise smooth curve which goes from (Ï€, 1, 1) to
Â¡ Ï€
2 , 3, 2
Â¢
. Find
R
C F Â· dR.
The speciï¬cs of the curve are not given so the problem is nonsense unless the vector ï¬eld
is conservative. Therefore, it is reasonable to look for the function, Ï† satisfying âˆ‡Ï† = F.
Such a function satisï¬es
Ï†x = cos x âˆ’y (sin xz) z
and so, assuming Ï† exists,
Ï† (x, y, z) = sin x + y cos (xz) + Ïˆ (y, z) .
I have to add in the most general thing possible, Ïˆ (y, z) to ensure possible solutions are not
being thrown out. It wouldnâ€™t be good at this point to add in a constant since the answer

470
LINE INTEGRALS 14 NOV.
could involve a function of either or both of the other variables. Now from what was just
obtained,
Ï†y = cos (xz) + Ïˆy = cos xz
and so it is possible to take Ïˆy = 0. Consequently, Ï†, if it exists is of the form
Ï† (x, y, z) = sin x + y cos (xz) + Ïˆ (z) .
Now diï¬€erentiating this with respect to z gives
Ï†z = âˆ’yx sin (xz) + Ïˆz = âˆ’yx sin (xz)
and this shows Ïˆ does not depend on z either. Therefore, it suï¬ƒces to take Ïˆ = 0 and
Ï† (x, y, z) = sin (x) + y cos (xz) .
Therefore, the desired line integral equals
sin
Â³Ï€
2
Â´
+ 3 cos (Ï€) âˆ’(sin (Ï€) + cos (Ï€)) = âˆ’1.
The above process for ï¬nding Ï† will not lead you astray in the case where there does not
exist a scalar potential. As an example, consider the following.
Example 25.1.5 Let F (x, y, z) =
Â¡
x, y2x, z
Â¢
. Find a scalar potential for F if it exists.
If Ï† exists, then Ï†x = x and so Ï† = x2
2 + Ïˆ (y, z) . Then Ï†y = Ïˆy (y, z) = xy2 but this
is impossible because the left side depends only on y and z while the right side depends
also on x. Therefore, this vector ï¬eld is not conservative and there does not exist a scalar
potential.
Example 25.1.6 Let F (x, y, z) =
Â¡
2yx + 1 + y, x2 + x, 1
Â¢
. Find a scalar potential for F if
it exists.
You need Ï†x = 2yx+1+y and so Ï† = yx2+x+yx+Ïˆ (y, z) . Then you need Ï†y = x2+x+
Ïˆy = x2+x which shows Ïˆy = 0 and so Ïˆ = Ïˆ (z) . Hence Ï† = yx2+x+yx+Ïˆ (z) Now ï¬nally,
Ï†z = Ïˆâ€² (z) = 1 and so Ïˆ (z) = z will work. A scalar potential is Ï† (x, y, z) = yx2+x+yx+z.
Example 25.1.7 Let F (x, y, z) =
Â¡
1, 2yz + z cos y, y2 + sin y
Â¢
. Find a scalar potential for
F if it exists.
You need Ï†x = 1 and so Ï† = x + Ïˆ (y, z) . Then you need Ï†y = Ïˆy = 2yz + z cos y and
so Ïˆ = y2z + z sin y + g (z) . Hence Ï† = x + y2z + z sin y + g (z) and you still donâ€™t know g.
But you must have Ï†z = y2 + sin y + gâ€² (z) = y2 + sin y and so g is a constant. You can take
it to equal zero. Hence Ï† = x + y2z + z sin y is a scalar potential.
When you are ï¬nding one of these scalar potentials, be sure to check your work. Take
what you think is the answer and ï¬nd its gradient. If you get the given vector ï¬eld, rejoice.
If not, it is wrong. Start over again.
Example 25.1.8 Let the vector ï¬eld, F be given in Example 25.1.7. Find
R
C FÂ·dR where
C is an oriented curve which goes from (0, Ï€, 2) to (1, Ï€/2, 2) .
This is very easy. It is just Ï† (1, Ï€/2, 2) âˆ’Ï† (0, Ï€, 2) where Ï† is the scalar potential in
this example. Thus it equals
Âµ
1 +
Â³Ï€
2
Â´2
2 + 2
Â¶
âˆ’
Â¡
Ï€22
Â¢
= 3 âˆ’3
2Ï€2

25.1.
PATH INDEPENDENT LINE INTEGRALS 15 NOV.
471
25.1.2
Terminology
For a vector ï¬eld, F (x, y, z) = F1 (x, y, z) i + F2 (x, y, z) j + F3 (x, y, z) k, F is called conser-
vative if it is the gradient of a scalar potential. Thus F is conservative if there exists a scalar
function, Ï† such that âˆ‡Ï† = F. This was discussed above. Another way to say this is that
the diï¬€erential form F1dx + F2dy + F3dz is exact. This terminology holds with obvious
modiï¬cations in any number of dimensions.

472
LINE INTEGRALS 14 NOV.

Part XI
Greenâ€™s Theorem, Integrals On
Surfaces
473


475
Outcomes
Greenâ€™s Theorem
A. Recall and verify Greenâ€™s Theorem.
B. Apply Greenâ€™s Theorem to evaluate line integrals.
C. Apply Greenâ€™s Theorem to ï¬nd the area of a region.
Reading: Multivariable Calculus 4.4
Outcome Mapping:
A. L1,L2,3,5
B. 1
C. 2
Surface Integrals
A. Determine the area of a given surface using integration.
B. Evaluate the physical characteristics of a surface such as centroid, mass, and center of
mass using surface integrals.
C. Find the ï¬‚ux of a vector ï¬eld through a surface.
Reading: Multivariable Calculus 4.5
Outcome Mapping:
A. 1
B. 1,2
C. 3
Parametric Surfaces
A. Write a parameterization for a given surface.
B. Identify a surface from its parameterization.
C. Describe a surface from its nets. Sketch a parametric surface.
Reading: Multivariable Calculus 4.6
Outcome Mapping:
A. 3,5,9
B. 1,2
C. 6,7
Integrals over Parametric Surfaces
A. Graphically describe a surface in terms of its parameterization.
B. Determine a (unit) normal vector to a surface from a parameterization of the surface.

476
C. Determine the plane tangent to a surface at a given point.
D. Evaluate the physical characteristics of parameterized surfaces such as centroid, mass,
and center of mass.
E. Find the ï¬‚ux of a ï¬‚ow through a parametric surface.
Reading: Multivariable Calculus 4.7
Outcome Mapping:
A. 1,4
B. 1,4
C. 1,4
D. 1,4
E. 1,4

Greenâ€™s Theorem 20 Nov.
Greenâ€™s theorem is an important theorem which relates line integrals to integrals over a
surface in the plane. It can be used to establish the much more signiï¬cant Stokeâ€™s theorem
but is interesting for itâ€™s own sake. Historically, it was important in the development of
complex analysis. I will ï¬rst establish Greenâ€™s theorem for regions of a particular sort and
then show that the theorem holds for many other regions also. Suppose a region is of the
form indicated in the following picture in which
U
=
{(x, y) : x âˆˆ(a, b) and y âˆˆ(b (x) , t (x))}
=
{(x, y) : y âˆˆ(c, d) and x âˆˆ(l (y) , r (y))} .
9
z
:
y
q
q
q
q
q
q
q
q
U
x = r(y)
x = l(y)
y = t(x)
y = b(x)
c
d
a
b
I will refer to such a region as being convex in both the x and y directions.
Lemma 26.0.9 Let F (x, y) â‰¡(P (x, y) , Q (x, y)) be a C1 vector ï¬eld deï¬ned near U
where U is a region of the sort indicated in the above picture which is convex in both the x
and y directions. Suppose also that the functions, r, l, t, and b in the above picture are all C1
functions and denote by âˆ‚U the boundary of U oriented such that the direction of motion is
counter clockwise. (As you walk around U on âˆ‚U, the points of U are on your left.) Then
Z
âˆ‚U
Pdx + Qdy â‰¡
Z
âˆ‚U
FÂ·dR =
Z
U
Âµâˆ‚Q
âˆ‚x âˆ’âˆ‚P
âˆ‚y
Â¶
dA.
(26.1)
Proof: First consider the right side of 26.1.
Z
U
Âµâˆ‚Q
âˆ‚x âˆ’âˆ‚P
âˆ‚y
Â¶
dA
=
Z d
c
Z r(y)
l(y)
âˆ‚Q
âˆ‚x dxdy âˆ’
Z b
a
Z t(x)
b(x)
âˆ‚P
âˆ‚y dydx
=
Z d
c
(Q (r (y) , y) âˆ’Q (l (y) , y)) dy +
Z b
a
(P (x, b (x))) âˆ’P (x, t (x)) dx.
(26.2)
477

478
GREENâ€™S THEOREM 20 NOV.
Now consider the left side of 26.1. Denote by V the vertical parts of âˆ‚U and by H the
horizontal parts.
Z
âˆ‚U
FÂ·dR =
=
Z
âˆ‚U
((0, Q) + (P, 0)) Â· dR
=
Z d
c
(0, Q (r (s) , s)) Â· (râ€² (s) , 1) ds +
Z
H
(0, Q (r (s) , s)) Â· (Â±1, 0) ds
âˆ’
Z d
c
(0, Q (l (s) , s)) Â· (lâ€² (s) , 1) ds +
Z b
a
(P (s, b (s)) , 0) Â· (1, bâ€² (s)) ds
+
Z
V
(P (s, b (s)) , 0) Â· (0, Â±1) ds âˆ’
Z b
a
(P (s, t (s)) , 0) Â· (1, tâ€² (s)) ds
=
Z d
c
Q (r (s) , s) ds âˆ’
Z d
c
Q (l (s) , s) ds +
Z b
a
P (s, b (s)) ds âˆ’
Z b
a
P (s, t (s)) ds
which coincides with 26.2. This proves the lemma.
Corollary 26.0.10 Let everything be the same as in Lemma 26.0.9 but only assume the
functions r, l, t, and b are continuous and piecewise C1 functions. Then the conclusion this
lemma is still valid.
Proof: The details are left for you. All you have to do is to break up the various line
integrals into the sum of integrals over sub intervals on which the function of interest is C1.
From this corollary, it follows 26.1 is valid for any triangle for example.
Now suppose 26.1 holds for U1, U2, Â· Â· Â·, Um and the open sets, Uk have the property that
no two have nonempty intersection and their boundaries intersect only in a ï¬nite number
of piecewise smooth curves. Then 26.1 must hold for U â‰¡âˆªm
i=1Ui, the union of these sets.
This is because
Z
U
Âµâˆ‚Q
âˆ‚x âˆ’âˆ‚P
âˆ‚y
Â¶
dA =
=
m
X
k=1
Z
Uk
Âµâˆ‚Q
âˆ‚x âˆ’âˆ‚P
âˆ‚y
Â¶
dA
=
m
X
k=1
Z
âˆ‚Uk
F Â· dR =
Z
âˆ‚U
F Â· dR
because if Î“ = âˆ‚Uk âˆ©âˆ‚Uj, then its orientation as a part of âˆ‚Uk is opposite to its orientation
as a part of âˆ‚Uj and consequently the line integrals over Î“ will cancel, points of Î“ also not
being in âˆ‚U. As an illustration, consider the following picture for two such Uk.
Â¡
Â¡
@
@
@
@
@
@
Â¡

Â¡


@
@
@
R
I

:
Âª

Âª
U1
U2

26.1.
AN ALTERNATIVE EXPLANATION OF GREENâ€™S THEOREM
479
Similarly, if U âŠ†V and if also âˆ‚U âŠ†V and both U and V are open sets for which
26.1 holds, then the open set, V \ (U âˆªâˆ‚U) consisting of what is left in V after deleting U
along with its boundary also satisï¬es 26.1. Roughly speaking, you can drill holes in a region
for which 26.1 holds and get another region for which this continues to hold provided 26.1
holds for the holes. To see why this is so, consider the following picture which typiï¬es the
situation just described.
V
:
y
9
z
U
9
z
:
y
Then
Z
âˆ‚V
FÂ·dR =
Z
V
Âµâˆ‚Q
âˆ‚x âˆ’âˆ‚P
âˆ‚y
Â¶
dA
=
Z
U
Âµâˆ‚Q
âˆ‚x âˆ’âˆ‚P
âˆ‚y
Â¶
dA +
Z
V \U
Âµâˆ‚Q
âˆ‚x âˆ’âˆ‚P
âˆ‚y
Â¶
dA
=
Z
âˆ‚U
FÂ·dR +
Z
V \U
Âµâˆ‚Q
âˆ‚x âˆ’âˆ‚P
âˆ‚y
Â¶
dA
and so
Z
V \U
Âµâˆ‚Q
âˆ‚x âˆ’âˆ‚P
âˆ‚y
Â¶
dA =
Z
âˆ‚V
FÂ·dRâˆ’
Z
âˆ‚U
FÂ·dR
which equals
Z
âˆ‚(V \U)
F Â· dR
where âˆ‚V is oriented as shown in the picture. (If you walk around the region, V \ U with
the area on the left, you get the indicated orientation for this curve.)
You can see that 26.1 is valid quite generally. This veriï¬es the following theorem.
Theorem 26.0.11 (Greenâ€™s Theorem) Let U be an open set in the plane and let
âˆ‚U be piecewise smooth and let F (x, y) = (P (x, y) , Q (x, y)) be a C1 vector ï¬eld deï¬ned
near U. Then it is often1 the case that
Z
âˆ‚U
F Â· dR =
Z
U
Âµâˆ‚Q
âˆ‚x (x, y) âˆ’âˆ‚P
âˆ‚y (x, y)
Â¶
dA.
26.1
An Alternative Explanation Of Greenâ€™s Theorem
Consider the following picture.
1For a general version see the advanced calculus book by Apostol.
The general versions involve the
concept of a rectiï¬able Jordan curve. You need to be able to take the area integral and to take the line
integral around the boundary.

480
GREENâ€™S THEOREM 20 NOV.
b
a
x
y = B(x)
y = T(x)
X
X
X
X
X
y
Â¤
Â¤
Â¤
Â¤
Â¤
T = (âˆ’ny, nx)
n = (nx, ny)
U
In this picture n is the unit outer normal to U and the vector, T shown in the picture is
the unit tangent vector in the direction of counter clockwise motion around U. To see that it
really does point in the correct direction, take the cross product, (nx, ny, 0) Ã— (âˆ’ny, nx, 0) .
This equals k. Applying the right hand rule, this shows the vector, (âˆ’ny, nx) really does
point in the direction indicated by the picture.
Next I will establish Gaussâ€™ theorem for regions like U. The boundary of U is denoted
by âˆ‚U.
Lemma 26.1.1 (Gauss)Let (H (x, y) , K (x, y)) be a C1 vector ï¬eld deï¬ned near U. Then
for n the unit outer normal,
Z
U
(Hx + Ky) dA =
Z
âˆ‚U
(H, K) Â· ndl
Proof:
A parameterization for the top is (x, T (x)) and a parameterization for the
bottom is (x, B (x)) where in both cases, x âˆˆ[a, b] . Thus dl =
q
1 + T â€² (x)2dx on the top
and dl =
q
1 + Bâ€² (x)2dx on the bottom. Thus also, on the top, you can ï¬nd the exterior
normal by considering it as the level surface, y âˆ’T (x) = 0. Thus a unit normal to this
surface is
n = (âˆ’T â€² (x) , 1)
q
1 + T â€² (x)2 = (nx, ny)
and you see that since the y component is positive, it is the outer normal, pointing away
from U. Similarly, the unit outer normal on the bottom is given by
n = (Bâ€² (x) , âˆ’1)
q
1 + Bâ€² (x)2 = (nx, ny)

26.1.
AN ALTERNATIVE EXPLANATION OF GREENâ€™S THEOREM
481
First consider
Z
U
KydA
=
Z b
a
Z T (x)
B(x)
Kydydx =
Z b
a
(K (x, T (x)) âˆ’K (x, B (x))) dx
=
Z b
a
K (x, T (x)) dx âˆ’
Z b
a
K (x, B (x)) dx
=
Z b
a
K (x, T (x))
ny
z
}|
{
1
q
1 + T â€² (x)2
dl
z
}|
{
q
1 + T â€² (x)2dx
+
Z b
a
K (x, B (x))
ny
z
}|
{
ï£«
ï£­
âˆ’1
q
1 + Bâ€² (x)2
ï£¶
ï£¸
dl
z
}|
{
q
1 + Bâ€² (x)2dx
=
Z
âˆ‚U
Knydl
Similar reasoning shows that
Z
U
HxdA =
Z
âˆ‚U
Hnxdl
Therefore, this proves the Lemma because from the above,
Z
U
(Hx + Ky) dA
=
Z
U
HxdA +
Z
U
KydA =
Z
âˆ‚U
Hnxdl +
Z
âˆ‚U
Knydl
=
Z
âˆ‚U
(H, K) Â· ndl
Now this theorem holds for many regions much more general than the one shown. In
fact, it holds for any region which is made up by pasting together regions like the above.
This is because the area integrals add and the integrals on the parts of the boundary which
are shared by two pieces cancel due to the fact they have the exterior normals which are in
opposite directions. For example, consider the following picture. If the divergence theorem
holds for each Vi in the following picture, then it holds for the union of these two.
V1
V2
This theorem is also called the divergence theorem. This is because the divergence of
the vector ï¬eld, (H (x, y) , K (x, y)) is deï¬ned as Hx (x, y) + Ky (x, y) .
Theorem 26.1.2 (Greenâ€™s Theorem) Let U be any bounded open set in R2 for
which the above Gaussâ€™ theorem holds and let
F (x, y) â‰¡(P (x, y) , Q (x, y))
be a C1 vector ï¬eld deï¬ned near U. Then
Z
U
(Qx âˆ’Py) dA =
Z
âˆ‚U
FÂ·dR

482
GREENâ€™S THEOREM 20 NOV.
where the line integral is oriented in the counter clockwise direction.
Proof: If r (t) is a parameterization of âˆ‚U near a point on âˆ‚U, then recall the unit
tangent vector, T as shown in the above picture satisï¬es |râ€² (t)| T = râ€² (t) . Thus FÂ·dR is
of the form F (r (t)) Â·râ€² (t) dt = F Â· T |râ€² (t)| dt = F Â· Tdl because dl = |râ€² (t)| dt. Then using
Lemma 26.1.1 and letting (H, K) = (Q, âˆ’P)
Z
U
(Qx âˆ’Py) dA
=
Z
U
(Hx + Ky) dA
=
Z
âˆ‚U
(H, K) Â· (nx, ny) dl
=
Z
âˆ‚U
(Q, âˆ’P) Â· (nx, ny) dl
=
Z
âˆ‚U
(P, Q) Â· (âˆ’ny, nx) dl
=
Z
âˆ‚U
F Â· Tdl =
Z
âˆ‚U
FÂ·dR.
This proves Greenâ€™s theorem.
26.2
Area And Greenâ€™s Theorem
Proposition 26.2.1 Let U be an open set in R2 for which Greenâ€™s theorem holds. Then
Area of U =
Z
âˆ‚U
FÂ·dR
where F (x, y) = 1
2 (âˆ’y, x) , (0, x) , or (âˆ’y, 0) .
Proof: This follows immediately from Greenâ€™s theorem.
Example 26.2.2 Use Proposition 26.2.1 to ï¬nd the area of the ellipse
x2
a2 + y2
b2 â‰¤1.
You can parameterize the boundary of this ellipse as
x = a cos t, y = b sin t, t âˆˆ[0, 2Ï€] .
Then from Proposition 26.2.1,
Area equals
=
1
2
Z 2Ï€
0
(âˆ’b sin t, a cos t) Â· (âˆ’a sin t, b cos t) dt
=
1
2
Z 2Ï€
0
(ab) dt = Ï€ab.
Example 26.2.3 Find
R
âˆ‚U FÂ·dR where U is the set,
Â©
(x, y) : x2 + 3y2 â‰¤9
Âª
and F (x, y) =
(y, âˆ’x) .
One way to do this is to parameterize the boundary of U and then compute the line
integral directly. It is easier to use Greenâ€™s theorem. The desired line integral equals
Z
U
((âˆ’1) âˆ’1) dA = âˆ’2
Z
U
dA.
Now U is an ellipse having area equal to 3
âˆš
3 and so the answer is âˆ’6
âˆš
3.

26.2.
AREA AND GREENâ€™S THEOREM
483
Example 26.2.4 Find
R
âˆ‚U FÂ·dR where U is the set, {(x, y) : 2 â‰¤x â‰¤4, 0 â‰¤y â‰¤3} and
F (x, y) =
Â¡
x sin y, y3 cos x
Â¢
.
From Greenâ€™s theorem this line integral equals
Z 4
2
Z 3
0
Â¡
âˆ’y3 sin x âˆ’x cos y
Â¢
dydx
=
81
4 cos 4 âˆ’6 sin 3 âˆ’81
4 cos 2.
This is much easier than computing the line integral because you donâ€™t have to break the
boundary in pieces and consider each separately.
Example 26.2.5 Find
R
âˆ‚U FÂ·dR where U is the set, {(x, y) : 2 â‰¤x â‰¤4, x â‰¤y â‰¤3} and
F (x, y) = (x sin y, y sin x) .
From Greenâ€™s theorem this line integral equals
Z 4
2
Z 3
x
(y cos x âˆ’x cos y) dydx
=
âˆ’3
2 sin 4 âˆ’6 sin 3 âˆ’8 cos 4 âˆ’9
2 sin 2 + 4 cos 2.

484
GREENâ€™S THEOREM 20 NOV.

The Integral On Two
Dimensional Surfaces In R3
27-28 Nov.
27.1
Parametrically Deï¬ned Surfaces
Deï¬nition 27.1.1 Let S be a subset of R3. Then S is a smooth surface if there
exists an open set, U âŠ†R2 and a C1 function, r deï¬ned on U such that r (U) = S, r is one
to one, and for all (u, v) âˆˆU,
ru Ã— rv Ì¸= 0.
(27.1)
This last condition ensures that there is always a well deï¬ned normal on S. This function,
r is called a parameterization of the surface. It is just like a parameterization of a curve but
here there are two parameters, u, v.
One way to think of this is that there is a piece of rubber occupying U in the plane
and then it is taken and stretched in three dimensions. This gives S. Here is an interesting
example which is already familiar.
Example 27.1.2 Let (Ï†, Î¸) âˆˆ(0, Ï€) Ã— (0, 2Ï€) and for such (Ï†, Î¸) ,
r (Ï†, Î¸) â‰¡
ï£«
ï£­
2 sin (Ï†) cos (Î¸)
2 sin (Ï†) sin (Î¸)
2 cos (Ï†)
ï£¶
ï£¸
This gives most of a sphere of radiius 2 for S. You should check condition 27.1. You will
ï¬nd that |rÏ† Ã— rÎ¸| = 4 sin (Ï†) Ì¸= 0.
Example 27.1.3 Let R > r. Consider
r (u, v) =
ï£«
ï£­
(R + r cos (u)) cos (v)
(R + r cos (u)) sin (v)
r sin (u)
ï£¶
ï£¸
where (u, v) âˆˆ(0, 2Ï€) Ã— (0, 2Ï€) . This surface is most of the surface of a torus (donut) with
small radius equal to r. It is obtained by revolving the circle of radius r centered at (R, 0, 0)
about the z axis. Here is a picture.
485

486
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
In the above I have assumed U is open. However, this is generalized later. It is amazing
how far this can be generalized in applications to integration.
In general, if you ï¬x u and consider r (u, v) as a function of v, this yields a smooth curve
which lies in the surface, S. By ï¬xing diï¬€erent values of u you obtain many diï¬€erent curves
in S. Similarly you can ï¬x v and consider r (u, v) as a function of u. The curves which result
in this way are called a net for the surface. This is the way a computer graphs a surface. It
graphs lots of diï¬€erent curves as just described. You can see this in the above picture of a
torus. The curves which make up the shape shown correspond to one of the variables in the
parameterization being ï¬xed. Now at a point, r (u, v) of S, there are two vectors tangent to
S at this point, ru (u, v) and rv (u, v) . These two vectors determine a plane which can be
considered tangent to the surface at the point, r (u, v). You can ï¬nd an equation for this
plane if you can obtain a normal vector. However, this is easy. You simply take rv Ã— ru to
obtain a vector which is normal to the tangent plane. Here is a picture. The two curves
correspond to u â†’r (u, v) and v â†’r (u, v) . The vectors ru and rv are tangent to the
respective curves as shown. Then taking the cross product gives a normal to the surface at
that point.

:






ru
rv
v â†’r(u, v)
@
@
R
u â†’r(u, v)
BBM
B
B
B
B
B
BM
rv Ã— ru
Example 27.1.4 Let S be the surface deï¬ned in Example 27.1.3 in which R = 2 and r = 1.
Find a tangent plane to the point
r
Â³Ï€
4 , Ï€
4
Â´
.
This point is
Â³âˆš
2 + 1
2,
âˆš
2 + 1
2,
âˆš
2
2
Â´
. I only need to ï¬nd a normal vector in order to ï¬nd

27.2.
THE TWO DIMENSIONAL AREA IN R3
487
the plane.
ru Ã— rv
=
ï£«
ï£­
âˆ’sin u cos v
âˆ’sin u sin v
cos u
ï£¶
ï£¸Ã—
ï£«
ï£­
âˆ’(2 + cos u) sin v
(2 + cos u) cos v
0
ï£¶
ï£¸
=
ï£«
ï£­
âˆ’(cos u) (2 + cos u) cos v
âˆ’(cos u) (2 + cos u) sin v
âˆ’
Â¡
sin u cos2 v
Â¢
(2 + cos u) âˆ’
Â¡
sin u sin2 v
Â¢
(2 + cos u)
ï£¶
ï£¸
Now plugging in the desired values of u and v, a normal vector is
ï£«
ï£­
âˆ’1 âˆ’1
4
âˆš
2
âˆ’1 âˆ’1
4
âˆš
2
âˆ’
âˆš
2 âˆ’1
2
ï£¶
ï£¸.
I donâ€™t like the minus signs so the normal vector I will use is
Âµ
1 + 1
4
âˆš
2, 1 + 1
4
âˆš
2,
âˆš
2 + 1
2
Â¶T
.
Now it follows the equation of the tangent plane is
Âµ
1 + 1
4
âˆš
2
Â¶ Âµ
x âˆ’
âˆš
2 âˆ’1
2
Â¶
+
Âµ
1 + 1
4
âˆš
2
Â¶ Âµ
y âˆ’
âˆš
2 âˆ’1
2
Â¶
+
Âµâˆš
2 + 1
2
Â¶ Âµ
z âˆ’1
2
âˆš
2
Â¶
= 0.
You could simplify this if you wanted.
Âµ
1 + 1
4
âˆš
2
Â¶
x +
Âµ
1 + 1
4
âˆš
2
Â¶
y +
Âµâˆš
2 + 1
2
Â¶
z = 5
2
âˆš
2 + 3.
27.2
The Two Dimensional Area In R3
Consider the boundary of some three dimensional region such that a function, is deï¬ned on
this boundary. Imagine taking the value of this function at a point, multiplying this value
by the area of an inï¬nitesimal chunk of area located at this point and then adding these up.
This is just the notion of the integral presented earlier only now there is a diï¬€erence because
this inï¬nitesimal chunk of area should be considered as two dimensional even though it is
in three dimensions. However, it is not really all that diï¬€erent from what was done earlier.
It all depends on the following fundamental deï¬nition which is just a review of the fact
presented earlier that the area of a parallelogram determined by two vectors in R3 is the
norm of the cross product of the two vectors.
Deï¬nition 27.2.1 Let u1, u2 be vectors in R3. The 2 dimensional parallelogram
determined by these vectors will be denoted by P (u1, u2) and it is deï¬ned as
P (u1, u2) â‰¡
ï£±
ï£²
ï£³
2
X
j=1
sjuj : sj âˆˆ[0, 1]
ï£¼
ï£½
ï£¾.
Then the area of this parallelogram is
area P (u1, u2) â‰¡|u1 Ã— u2| .

488
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
Suppose then that x = f (u) where u âˆˆU, a subset of R2 and x is a point in V, a
subset of 3 dimensional space. Thus, letting the Cartesian coordinates of x be given by
x = (x1, x2, x3)T , each xi being a function of u, an inï¬nitesimal rectangle located at u0
corresponds to an inï¬nitesimal parallelogram located at f (u0) which is determined by the 2
vectors
n
âˆ‚f(u0)
âˆ‚ui
dui
o2
i=1 , each of which is tangent to the surface deï¬ned by x = f (u) . (No
sum on the repeated index.)
dV
u0
du2
du1

:






















fu2(u0)du2
fu1(u0)du1
f(dV )








+
From Deï¬nition 27.2.1, the volume of this inï¬nitesimal parallelepiped located at f (u0)
is given by
Â¯Â¯Â¯Â¯
âˆ‚f (u0)
âˆ‚u1
du1 Ã— âˆ‚f (u0)
âˆ‚u2
du2
Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯
âˆ‚f (u0)
âˆ‚u1
Ã— âˆ‚f (u0)
âˆ‚u2
Â¯Â¯Â¯Â¯ du1du2
(27.2)
=
|fu1 Ã— fu2| du1du2
(27.3)
It might help to think of a lizard. The inï¬nitesimal parallelogram is like a very small
scale on a lizard. This is the essence of the idea. To deï¬ne the area of the lizard sum up
areas of individual scales If the scales are small enough, their sum would serve as a good
approximation to the area of the lizard.

27.2.
THE TWO DIMENSIONAL AREA IN R3
489
1.This motivates the following fundamental procedure which I hope is extremely familiar
from the earlier material.
Procedure 27.2.2 Suppose U is a subset of R2 and suppose f : U â†’f (U) âŠ†R3
is a one to one and C1 function. Then if h : f (U) â†’R, deï¬ne the 2 dimensional surface
integral,
R
f(U) h (x) dA according to the following formula.
Z
f(U)
h (x) dA â‰¡
Z
U
h (f (u)) |fu1 (u) Ã— fu2 (u)| du1du2.
Deï¬nition 27.2.3 It is customary to write |fu1 (u) Ã— fu2 (u)| = âˆ‚(x1,x2,x3)
âˆ‚(u1,u2)
because
this new notation generalizes to far more general situations for which the cross product is
not deï¬ned. For example, one can consider three dimensional surfaces in R8.
First here is a simple example where the surface is actually in the plane.
Example 27.2.4 Find the area of the region labelled A in the following picture. The two
circles are of radius 1, one has center (0, 0) and the other has center (1, 0) .
A
Ï€/3
The circles bounding these disks are x2 +y2 = 1 and (x âˆ’1)2 +y2 = x2 +y2 âˆ’2x+1 = 1.
Therefore, in polar coordinates these are of the form r = 1 and r = 2 cos Î¸.
The set A corresponds to the set U, in the (Î¸, r) plane determined by Î¸ âˆˆ
Â£
âˆ’Ï€
3 , Ï€
3
Â¤
and
for each value of Î¸ in this interval, r goes from 1 up to 2 cos Î¸. Therefore, the area of this
region is of the form,
Z
U
1 dV =
Z Ï€/3
âˆ’Ï€/3
Z 2 cos Î¸
1
âˆ‚(x1, x2, x3)
âˆ‚(Î¸, r)
dr dÎ¸.
It is necessary to ï¬nd âˆ‚(x1,x2)
âˆ‚(Î¸,r) . The mapping f : U â†’R2 takes the form
f (Î¸, r) = (r cos Î¸, r sin Î¸)T .
Here x3 = 0 and so
âˆ‚(x1, x2, x3)
âˆ‚(Î¸, r)
=
Â¯Â¯Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
âˆ‚x1
âˆ‚Î¸
âˆ‚x2
âˆ‚Î¸
âˆ‚x3
âˆ‚Î¸
âˆ‚x1
âˆ‚r
âˆ‚x2
âˆ‚r
âˆ‚x3
âˆ‚r
Â¯Â¯Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
âˆ’r sin Î¸
r cos Î¸
0
cos Î¸
sin Î¸
0
Â¯Â¯Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯Â¯Â¯
= r
1This beautiful lizard is a Sceloporus magister. It was photographed by C. Riley Nelson who is in the
Zoology department at Brigham Young University câƒ2004 in Kane Co. Utah. The lizard is a little less
than one foot in length.

490
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
Therefore, the area element is r dr dÎ¸. It follows the desired area is
Z Ï€/3
âˆ’Ï€/3
Z 2 cos Î¸
1
r dr dÎ¸ = 1
2
âˆš
3 + 1
3Ï€.
Notice how the area element reduced to the area element for polar coordinates.
Example 27.2.5 Consider the surface given by z = x2 for (x, y) âˆˆ[0, 1] Ã— [0, 1] = U. Find
the surface area of this surface.
The ï¬rst step in using the above is to write this surface in the form x = f (u) . This is
easy to do if you let u = (x, y) . Then f (x, y) =
Â¡
x, y, x2Â¢
. If you like, let x = u1 and y = u2.
What is âˆ‚(x1,x2,x3)
âˆ‚(x,y)
= |fx Ã— fy|?
fx =
ï£«
ï£­
1
0
2x
ï£¶
ï£¸, fy =
ï£«
ï£­
0
1
0
ï£¶
ï£¸
and so
|fx Ã— fy| =
Â¯Â¯Â¯Â¯Â¯Â¯
ï£«
ï£­
1
0
2x
ï£¶
ï£¸Ã—
ï£«
ï£­
0
1
0
ï£¶
ï£¸
Â¯Â¯Â¯Â¯Â¯Â¯
=
p
1 + 4x2
and so the area element is
âˆš
1 + 4x2 dx dy and the surface area is obtained by integrating
the function, h (x) â‰¡1. Therefore, this area is
Z
U
dA =
Z 1
0
Z 1
0
p
1 + 4x2 dx dy = 1
2
âˆš
5 âˆ’1
4 ln
Â³
âˆ’2 +
âˆš
5
Â´
which can be obtained by using the trig. substitution, 2x = tan Î¸ on the inside integral.
Note this all depends on being able to write the surface in the form, x = f (u) for
u âˆˆU âŠ†Rp. Surfaces obtained in this form are called parametrically deï¬ned surfaces.
These are best but sometimes you have some other description of a surface and in these
cases things can get pretty intractable. For example, you might have a level surface of the
form 3x2 + 4y4 + z6 = 10. In this case, you could solve for z using methods of algebra.
Thus z =
6p
10 âˆ’3x2 âˆ’4y4 and a parametric description of part of this level surface is
Â³
x, y,
6p
10 âˆ’3x2 âˆ’4y4
Â´
for (x, y) âˆˆU where U =
Â©
(x, y) : 3x2 + 4y4 â‰¤10
Âª
. But what if
the level surface was something like
sin
Â¡
x2 + ln
Â¡
7 + y2 sin x
Â¢Â¢
+ sin (zx) ez = 11 sin (xyz)?
I really donâ€™t see how to use methods of algebra to solve for some variable in terms of the
others. It isnâ€™t even clear to me whether there are any points (x, y, z) âˆˆR3 satisfying this
particular relation. However, if a point satisfying this relation can be identiï¬ed, the implicit
function theorem from advanced calculus can usually be used to assert one of the variables
is a function of the others, proving the existence of a parameterization at least locally. The
problem is, this theorem doesnâ€™t give us the answer in terms of known functions so this
isnâ€™t much help. Finding a parametric description of a surface is a hard problem and there
are no easy answers. This is a good example which illustrates the gulf between theory and
practice.
Example 27.2.6 Let U = [0, 12] Ã— [0, 2Ï€] and let f : U â†’R3 be given by f (t, s) â‰¡
(2 cos t + cos s, 2 sin t + sin s, t)T . Find a double integral for the surface area. A graph of
this surface is drawn below.

27.2.
THE TWO DIMENSIONAL AREA IN R3
491
It looks like something you would use to make sausages2. Anyway,
ft =
ï£«
ï£­
âˆ’2 sin t
2 cos t
1
ï£¶
ï£¸, fs =
ï£«
ï£­
âˆ’sin s
cos s
0
ï£¶
ï£¸
and
ft Ã— fs =
ï£«
ï£­
âˆ’cos s
âˆ’sin s
âˆ’2 sin t cos s + 2 cos t sin s
ï£¶
ï£¸
and so
âˆ‚(x1, x2, x3)
âˆ‚(t, s)
= |ft Ã— fs| =
p
5 âˆ’4 sin2 t sin2 s âˆ’8 sin t sin s cos t cos s âˆ’4 cos2 t cos2 s.
Therefore, the desired integral giving the area is
Z 2Ï€
0
Z 12
0
p
5 âˆ’4 sin2 t sin2 s âˆ’8 sin t sin s cos t cos s âˆ’4 cos2 t cos2 s dt ds.
If you really needed to ï¬nd the number this equals, how would you go about ï¬nding it?
This is an interesting question and there is no single right answer. You should think about
this. It is important in some physical applications to get the number even when you canâ€™t
ï¬nd the antiderivative. Here is an example for which you will be able to ï¬nd the integrals.
Example 27.2.7 Let U = [0, 2Ï€] Ã— [0, 2Ï€] and for (t, s) âˆˆU, let
f (t, s) = (2 cos t + cos t cos s, âˆ’2 sin t âˆ’sin t cos s, sin s)T .
Find the area of f (U) . This is the surface of a donut shown below. The fancy name for
this shape is a torus.
2At Volwerthâ€™s in Hancock Michigan, they make excellent sausages and hot dogs. The best are made
from â€œnatural casingsâ€ which are the linings of intestines.

492
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
To ï¬nd its area,
ft =
ï£«
ï£­
âˆ’2 sin t âˆ’sin t cos s
âˆ’2 cos t âˆ’cos t cos s
0
ï£¶
ï£¸, fs =
ï£«
ï£­
âˆ’cos t sin s
sin t sin s
cos s
ï£¶
ï£¸
and so |ft Ã— fs| = (cos s + 2) so the area element is (cos s + 2) ds dt and the area is
Z 2Ï€
0
Z 2Ï€
0
(cos s + 2) ds dt = 8Ï€2
Example 27.2.8 Let U = [0, 2Ï€] Ã— [0, 2Ï€] and for (t, s) âˆˆU, let
f (t, s) = (2 cos t + cos t cos s, âˆ’2 sin t âˆ’sin t cos s, sin s)T .
Find
Z
f(U)
h dV
where h (x, y, z) = x2.
Everything is the same as the preceding example except this time it is an integral of a
function. The area element is (cos s + 2) ds dt and so the integral called for is
Z
f(U)
h dA =
Z 2Ï€
0
Z 2Ï€
0
ï£«
ï£­
x on the surface
z
}|
{
2 cos t + cos t cos s
ï£¶
ï£¸
2
(cos s + 2) ds dt = 22Ï€2
Example 27.2.9 Let U = [âˆ’5, 5] Ã— [0, 3Ï€] and for (s, t) âˆˆU, let
f (s, t) = (3s cos t, 3s sin t, 4t) .
Find a formula for the area of f (U) in terms of integrals. This is called a helicoid. Here is
a picture of it.

27.2.
THE TWO DIMENSIONAL AREA IN R3
493
The area element is
|(3 cos (t) , 3 sin (t) , 0) Ã— (âˆ’3 sin (t) , 3s cos (t) , 4)| dsdt =
q
144 +
Â¡
9 (cos2 t) s + 9 sin2 t
Â¢2dsdt
Therefore, the area is given by the double integral,
Z 5
âˆ’5
Z 3Ï€
0
q
144 +
Â¡
9 (cos2 t) s + 9 sin2 t
Â¢2dsdt
You can deï¬ne the center of mass and density of a surface in exactly the same way as
was done before.
Deï¬nition 27.2.10 Let S be a surface with area (volume) element dS. The den-
sity with respect to area is a function which integrated gives the mass. Thus if Î´ (x) is
the density, the mass of S is
Z
S
Î´ (x, y, z) dS.
The center of mass is deï¬ned exactly as before.
xc
â‰¡
R
S Î´ (x, y, z) xdS
R
S Î´ (x, y, z) dS , yc â‰¡
R
S Î´ (x, y, z) ydS
R
S Î´ (x, y, z) dS
zc
â‰¡
R
S Î´ (x, y, z) zdS
R
S Î´ (x, y, z) dS .
There is no new thing here.
You simply are integrating over a surface rather than
a volume.
Of course you must put the variables, x, y, z as well as dS in terms of the
parameters used to compute the integrals.
Example 27.2.11 The surface is given by (x, y, z) = (sin Ï† cos Î¸, sin Ï† sin Î¸, cos Ï†) where
(Ï†, Î¸) âˆˆ(0, Ï€) Ã— (0, 2Ï€) . Thus the surface is the surface of a sphere of radius 1. Review
spherical coordinates at this time if this is not obvious to you. Suppose the density of a
point on this surface corresponding to (Ï†, Î¸) is sin2 Ï†. That is, the density is equal to the
square of the distance to the z axis. Find the total mass and the center of mass of this
surface.
First ï¬nd the area element. This equals
dS
=
|(cos Ï† cos Î¸, cos Ï† sin Î¸, âˆ’sin Ï†) Ã— (âˆ’sin Ï† sin Î¸, sin Ï† cos Î¸, 0)| dÎ¸dÏ†
=
qÂ¡
sin2 Ï† cos Î¸
Â¢2 +
Â¡
sin2 Ï† sin Î¸
Â¢2 +
Â¡
cos Ï† cos2 Î¸ sin Ï† + cos Ï† sin2 Î¸ sin Ï†
Â¢2dÎ¸dÏ†
=
sin (Ï†) dÎ¸dÏ†

494
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
To ï¬nd the total mass you must integrate this area element times the density. Thus the
total mass is
Z Ï€
0
Z 2Ï€
0
Â¡
sin2 (Ï†)
Â¢
sin (Ï†) dÎ¸dÏ† = 8
3Ï€
Next you want to ï¬nd the center of mass. By symmetry, it should be at the origin. As
before, the center of mass does not need to be in the surface just as it did not need to be
in the three dimensional shape. Now on the surface, x = sin Ï† cos Î¸, y = sin Ï† sin Î¸, and
z = cos Ï†. Consider the formulas for this.
xc
=
R Ï€
0
R 2Ï€
0
(sin (Ï†) cos (Î¸))
Â¡
sin2 (Ï†)
Â¢
sin (Ï†) dÎ¸dÏ†
Â¡ 8
3Ï€
Â¢
= 0,
yc
=
R Ï€
0
R 2Ï€
0
(sin (Ï†) sin (Î¸))
Â¡
sin2 (Ï†)
Â¢
sin (Ï†) dÎ¸dÏ†
Â¡ 8
3Ï€
Â¢
= 0,
zc
=
R Ï€
0
R 2Ï€
0
(cos (Ï†))
Â¡
sin2 (Ï†)
Â¢
sin (Ï†) dÎ¸dÏ†
Â¡ 8
3Ï€
Â¢
= 0.
Example 27.2.12 In the above example suppose Î´ (x, y, z) = z + 1. What is the mass and
center of mass?
The total mass is
Z Ï€
0
Z 2Ï€
0
(1 + cos (Ï†)) sin (Ï†) dÎ¸dÏ† = 4Ï€.
Next, the center of mass is given by
xc
=
R Ï€
0
R 2Ï€
0
(sin (Ï†) cos (Î¸)) (1 + cos (Ï†)) sin (Ï†) dÎ¸dÏ†
4Ï€
= 0,
yc
=
R Ï€
0
R 2Ï€
0
(sin (Ï†) sin (Î¸)) (1 + cos (Ï†)) sin (Ï†) dÎ¸dÏ†
4Ï€
= 0,
zc
=
R Ï€
0
R 2Ï€
0
(cos (Ï†)) (1 + cos (Ï†)) sin (Ï†) dÎ¸dÏ†
4Ï€
= 1
3
27.2.1
Surfaces Of The Form z = f (x, y)
The special case where a surface is in the form z = f (x, y) , (x, y) âˆˆU, yields a simple
formula which is used most often in this situation. You write the surface parametrically in
the form f (x, y) = (x, y, f (x, y))T such that (x, y) âˆˆU. Then
fx =
ï£«
ï£­
1
0
fx
ï£¶
ï£¸, fy =
ï£«
ï£­
0
1
fy
ï£¶
ï£¸
and
|fx Ã— fy| =
q
1 + f 2y + f 2x
so the area element is
q
1 + f 2y + f 2x dx dy.
When the surface of interest comes in this simple form, people generally use this area element
directly rather than worrying about a parameterization and taking cross products.

27.2.
THE TWO DIMENSIONAL AREA IN R3
495
In the case where the surface is of the form x = f (y, z) for (y, z) âˆˆU, the area element
is obtained similarly and is
q
1 + f 2y + f 2z dy dz.
I think you can guess what the area element is if y = f (x, z) .
There is also a simple geometric description of these area elements. Consider the surface
z = f (x, y) . This is a level surface of the function of three variables z âˆ’f (x, y) . In fact the
surface is simply zâˆ’f (x, y) = 0. Now consider the gradient of this function of three variables.
The gradient is perpendicular to the surface and the third component is positive in this case.
This gradient is (âˆ’fx, âˆ’fy, 1) and so the unit upward normal is just
1
âˆš
1+f 2
x+f 2
y (âˆ’fx, âˆ’fy, 1) .
Now consider the following picture.

B
B
B
B
BBM
6
k
n Î¸
Î¸
dV
dxdy
In this picture, you are looking at a chunk of area on the surface seen on edge and so it
seems reasonable to expect to have dx dy = dV cos Î¸. But it is easy to ï¬nd cos Î¸ from the
picture and the properties of the dot product.
cos Î¸ = n Â· k
|n| |k| =
1
q
1 + f 2x + f 2y
.
Therefore, dA =
q
1 + f 2x + f 2y dx dy as claimed. In this context, the surface involved is
referred to as S because the vector valued function, f giving the parameterization will not
have been identiï¬ed.
Example 27.2.13 Let z =
p
x2 + y2 where (x, y) âˆˆU for U =
Â©
(x, y) : x2 + y2 â‰¤4
Âª
Find
Z
S
h dS
where h (x, y, z) = x + z and S is the surface described as
Â³
x, y,
p
x2 + y2
Â´
for (x, y) âˆˆU.
Here you can see directly the angle in the above picture is Ï€
4 and so dV =
âˆš
2 dx dy. If
you donâ€™t see this or if it is unclear, simply compute
q
1 + f 2x + f 2y and you will ï¬nd it is
âˆš
2. Therefore, using polar coordinates,
Z
S
h dS
=
Z
U
Â³
x +
p
x2 + y2
Â´ âˆš
2 dA
=
âˆš
2
Z 2Ï€
0
Z 2
0
(r cos Î¸ + r) r dr dÎ¸
=
16
3
âˆš
2Ï€.
One other issue is worth mentioning. Suppose fi : Ui â†’R3 where Ui are sets in R2
and suppose f1 (U1) intersects f2 (U2) along C where C = h (V ) for V âŠ†R1. Then deï¬ne

496
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
integrals and areas over f1 (U1) âˆªf2 (U2) as follows.
Z
f1(U1)âˆªf2(U2)
g dA â‰¡
Z
f1(U1)
g dA +
Z
f2(U2)
g dA.
Admittedly, the set C gets added in twice but this doesnâ€™t matter because its 2 dimensional
volume equals zero and therefore, the integrals over this set will also be zero.
I have been purposely vague about precise mathematical conditions necessary for the
above procedures. This is because the precise mathematical conditions which are usually
cited are very technical and at the same time far too restrictive. The most general condi-
tions under which these sorts of procedures are valid include things like Lipschitz functions
deï¬ned on very general sets. These are functions satisfying a Lipschitz condition of the
form |f (x) âˆ’f (y)| â‰¤K |x âˆ’y| . For example, y = |x| is Lipschitz continuous. However,
this function does not have a derivative at every point. So it is with Lipschitz functions.
However, it turns out these functions have derivatives at enough points to push everything
through but this requires considerations involving the Lebesgue integral. Lipschitz functions
are also not the most general kind of function for which the above is valid. There are many
very interesting issues here which can keep you fascinated for years.
27.3
Flux
Imagine a surface, S which is ï¬xed in space and let v be a vector ï¬eld representing the
velocity of a ï¬‚uid ï¬‚owing through this surface. It is reasonable to ask how fast the ï¬‚uid
crosses the surface in terms of units of mass per units of time. This is expressed in terms of
the surface integral,
Z
S
Ïv Â· ndA
where Ï is the density and n is the normal vector to the surface in the direction in which the
crossing is taking place. The vector ï¬eld, Ïv is called the ï¬‚ux. To get the rate of transfer
of mass across the surface, you take the dot product of the ï¬‚ux with the appropriate unit
normal vector and integrate this over the surface. People also speak of heat ï¬‚ux. In general,
when they speak of ï¬‚ux, they mean the thing you dot with a unit normal vector and
integrate to ï¬nd the rate at which something crosses a surface. A little later, this idea will
be explored much more when the divergence theorem is established. It is a very important
idea. You should think about the physical reasons the ï¬‚ux of such a ï¬‚uid is given as above.
Why do you use the unit normal for example? Why not some normal which has diï¬€erent
length? Why do you need to take the dot product with the normal? In general situations,
people assume formulas about the ï¬‚ux in terms of other quantities such as temperature or
concentration. I will mention some later at a convenient place.
27.3.1
Exercises With Answers
1. Find a parameterization for the intersection of the planes x + y + 2z = âˆ’3 and
2x âˆ’y + z = âˆ’4.
Answer:
(x, y, z) =
Â¡
âˆ’t âˆ’7
3, âˆ’t âˆ’2
3, t
Â¢
2. Find a parameterization for the intersection of the plane 4x + 2y + 4z = 0 and the
circular cylinder x2 + y2 = 16.
Answer:

27.3.
FLUX
497
The cylinder is of the form x = 4 cos t, y = 4 sin t and z = z. Therefore, from the
equation of the plane, 16 cos t+8 sin t+4z = 0. Therefore, z = âˆ’16 cos tâˆ’8 sin t and this
shows the parameterization is of the form (x, y, z) = (4 cos t, 4 sin t, âˆ’16 cos t âˆ’8 sin t)
where t âˆˆ[0, 2Ï€] .
3. Find a parameterization for the intersection of the plane 3x + 2y + z = 4 and the
elliptic cylinder x2 + 4z2 = 1.
Answer:
The cylinder is of the form x = cos t, 2z = sin t and y = y. Therefore, from the equation
of the plane, 3 cos t+2y + 1
2 sin t = 4. Therefore, y = 2âˆ’3
2 cos tâˆ’1
4 sin t and this shows
the parameterization is of the form (x, y, z) =
Â¡
cos t, 2 âˆ’3
2 cos t âˆ’1
4 sin t, 1
2 sin t
Â¢
where
t âˆˆ[0, 2Ï€] .
4. Find a parameterization for the straight line joining (4, 3, 2) and (1, 7, 6) .
Answer:
(x, y, z) = (4, 3, 2) + t (âˆ’3, 4, 4) = (4 âˆ’3t, 3 + 4t, 2 + 4t) where t âˆˆ[0, 1] .
5. Find a parameterization for the intersection of the surfaces y + 3z = 4x2 + 4 and
4y + 4z = 2x + 4.
Answer:
This is an application of Cramerâ€™s rule. y = âˆ’2x2 âˆ’1
2 + 3
4x, z = âˆ’1
4x + 3
2 + 2x2.
Therefore, the parameterization is (x, y, z) =
Â¡
t, âˆ’2t2 âˆ’1
2 + 3
4t, âˆ’1
4t + 3
2 + 2t2Â¢
.
6. Find the area of S if S is the part of the circular cylinder x2 + y2 = 16 which lies
between z = 0 and z = 4 + y.
Answer:
Use the parameterization, x = 4 cos v, y = 4 sin v and z = u with the parameter
domain described as follows. The parameter, v goes from âˆ’Ï€
2 to 3Ï€
2 and for each v in
this interval, u should go from 0 to 4+4 sin v. To see this observe that the cylinder has
its axis parallel to the z axis and if you look at a side view of the surface you would
see something like this:
y
z
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
The positive x axis is coming out of the paper toward you in the above picture and
the angle v is the usual angle measured from the positive x axis. Therefore, the area
is just A =
R 3Ï€/2
âˆ’Ï€/2
R 4+4 sin v
0
4 du dv = 32Ï€.

498
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
7. Find the area of S if S is the part of the cone x2 + y2 = 9z2 between z = 0 and z = h.
Answer:
When z = h , x2 + y2 = 9h2 which is the boundary of a circle of radius ah. A
parameterization of this surface is x = u, y = v, z =
1
3
p
(u2 + v2) where (u, v) âˆˆ
D, a disk centered at the origin having radius ha. Therefore, the volume is just
R
D
p
1 + z2u + z2v dA =
R ha
âˆ’ha
R âˆš
(9h2âˆ’u2)
âˆ’âˆš
(9h2âˆ’u2)
1
3
âˆš
10 dv du = 3Ï€h2âˆš
10
8. Parametrizing the cylinder x2 + y2 = 4 by x = 2 cos v, y = 2 sin v, z = u, show that
the area element is dA = 2 du dv
Answer:
It is necessary to compute
|fu Ã— fv| =
Â¯Â¯Â¯Â¯Â¯Â¯
ï£«
ï£­
0
0
1
ï£¶
ï£¸Ã—
ï£«
ï£­
âˆ’2 sin v
2 cos v
0
ï£¶
ï£¸
Â¯Â¯Â¯Â¯Â¯Â¯
= 2.
and so the area element is as described.
9. Find the area enclosed by the limacon r = 2 + cos Î¸.
Answer:
You can graph this region and you see it is sort of an oval shape and that Î¸ âˆˆ[0, 2Ï€]
while r goes from 0 up to 2 + cos Î¸. Now x = r cos Î¸ and y = r sin Î¸ are the x and y
coordinates corresponding to r and Î¸ in the above parameter domain. Therefore, the
area of the limacon equals
R
P
Â¯Â¯Â¯ âˆ‚(x,y)
âˆ‚(r,Î¸)
Â¯Â¯Â¯ dr dÎ¸ =
R 2Ï€
0
R 2+cos Î¸
0
r dr dÎ¸ because the Jacobian
equals r in this case. Therefore, the area equals
R 2Ï€
0
R 2+cos Î¸
0
r dr dÎ¸ = 9
2Ï€.
10. Find the surface area of the paraboloid z = h
Â¡
1 âˆ’x2 âˆ’y2Â¢
between z = 0 and z = h.
Answer:
Let R denote the unit circle. Then the area of the surface above this circle would be
R
R
p
1 + 4x2h2 + 4y2h2 dA. Changing to polar coordinates, this becomes
R 2Ï€
0
R 1
0
Â¡âˆš
1 + 4h2r2Â¢
r dr dÎ¸ =
Ï€
6h2
Â³Â¡
1 + 4h2Â¢3/2 âˆ’1
Â´
.
11. Evaluate
R
S (1 + x) dA where S is the part of the plane 2x + 3y + 3z = 18 which is in
the ï¬rst octant.
Answer:
R 6
0
R 6âˆ’2
3 x
0
(1 + x) 1
3
âˆš
22 dy dx = 28
âˆš
22
12. Evaluate
R
S (1 + x) dA where S is the part of the cylinder x2 +y2 = 16 between z = 0
and z = h.
Answer:
Parametrize the cylinder as x = 4 cos Î¸ and y = 4 sin Î¸ while z = t and the parameter
domain is just [0, 2Ï€] Ã— [0, h] . Then the integral to evaluate would be
Z 2Ï€
0
Z h
0
(1 + 4 cos Î¸) 4 dt dÎ¸ = 8hÏ€.
Note how 4 cos Î¸ was substituted for x and the area element is 4 dt dÎ¸ .

27.3.
FLUX
499
13. Evaluate
R
S (1 + x) dA where S is the hemisphere x2 + y2 + z2 = 16 between x = 0
and x = 4.
Answer:
Parametrize the sphere as x = 4 sin Ï† cos Î¸, y = 4 sin Ï† sin Î¸, and z = 4 cos Ï† and
consider the values of the parameters. Since it is referred to as a hemisphere and
involves x > 0, Î¸ âˆˆ
Â£
âˆ’Ï€
2 , Ï€
2
Â¤
and Ï† âˆˆ[0, Ï€] . Then the area element is
p
a4 sin Ï† dÎ¸ dÏ†
and so the integral to evaluate is
Z Ï€
0
Z Ï€/2
âˆ’Ï€/2
(1 + 4 sin Ï† cos Î¸) 16 sin Ï† dÎ¸ dÏ† = 96Ï€
14. For (Î¸, Î±) âˆˆ[0, 2Ï€] Ã— [0, 2Ï€] , let
f (Î¸, Î±) â‰¡(cos Î¸ (2 + cos Î±) , âˆ’sin Î¸ (2 + cos Î±) , sin Î±)T .
Find the area of f ([0, 2Ï€] Ã— [0, 2Ï€]) .
Answer:
|fÎ¸ Ã— fÎ±|
=
Â¯Â¯Â¯Â¯Â¯Â¯
ï£«
ï£­
âˆ’sin (Î¸) (2 + cos Î±)
âˆ’cos (Î¸) (2 + cos Î±)
0
ï£¶
ï£¸Ã—
ï£«
ï£­
âˆ’cos Î¸ sin Î±
sin Î¸ sin Î±
cos Î±
ï£¶
ï£¸
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¡
4 + 4 cos Î± + cos2 Î±
Â¢1/2
and so the area element is
Â¡
4 + 4 cos Î± + cos2 Î±
Â¢1/2 dÎ¸ dÎ±.
Therefore, the area is
Z 2Ï€
0
Z 2Ï€
0
Â¡
4 + 4 cos Î± + cos2 Î±
Â¢1/2 dÎ¸ dÎ± =
Z 2Ï€
0
Z 2Ï€
0
(2 + cos Î±) dÎ¸ dÎ± = 8Ï€2.
15. For (Î¸, Î±) âˆˆ[0, 2Ï€] Ã— [0, 2Ï€] , let
f (Î¸, Î±) â‰¡(cos Î¸ (4 + 2 cos Î±) , âˆ’sin Î¸ (4 + 2 cos Î±) , 2 sin Î±)T .
Also let h (x) = cos Î± where Î± is such that
x = (cos Î¸ (4 + 2 cos Î±) , âˆ’sin Î¸ (4 + 2 cos Î±) , 2 sin Î±)T .
Find
R
f([0,2Ï€]Ã—[0,2Ï€]) h dA.
Answer:
|fÎ¸ Ã— fÎ±|
=
Â¯Â¯Â¯Â¯Â¯Â¯
ï£«
ï£­
âˆ’sin (Î¸) (4 + 2 cos Î±)
âˆ’cos (Î¸) (4 + 2 cos Î±)
0
ï£¶
ï£¸Ã—
ï£«
ï£­
âˆ’2 cos Î¸ sin Î±
2 sin Î¸ sin Î±
2 cos Î±
ï£¶
ï£¸
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¡
64 + 64 cos Î± + 16 cos2 Î±
Â¢1/2
and so the area element is
Â¡
64 + 64 cos Î± + 16 cos2 Î±
Â¢1/2 dÎ¸ dÎ±.

500
THE INTEGRAL ON TWO DIMENSIONAL SURFACES IN R3 27-28 NOV.
Therefore, the desired integral is
Z 2Ï€
0
Z 2Ï€
0
(cos Î±)
Â¡
64 + 64 cos Î± + 16 cos2 Î±
Â¢1/2 dÎ¸ dÎ±
=
Z 2Ï€
0
Z 2Ï€
0
(cos Î±) (8 + 4 cos Î±) dÎ¸ dÎ± = 8Ï€2
16. For (Î¸, Î±) âˆˆ[0, 2Ï€] Ã— [0, 2Ï€] , let
f (Î¸, Î±) â‰¡(cos Î¸ (3 + cos Î±) , âˆ’sin Î¸ (3 + cos Î±) , sin Î±)T .
Also let h (x) = cos2 Î¸ where Î¸ is such that
x = (cos Î¸ (3 + cos Î±) , âˆ’sin Î¸ (3 + cos Î±) , sin Î±)T .
Find
R
f([0,2Ï€]Ã—[0,2Ï€]) h dV.
Answer:
The area element is
Â¡
9 + 6 cos Î± + cos2 Î±
Â¢1/2 dÎ¸ dÎ±.
Therefore, the desired integral is
Z 2Ï€
0
Z 2Ï€
0
Â¡
cos2 Î¸
Â¢ Â¡
9 + 6 cos Î± + cos2 Î±
Â¢1/2 dÎ¸ dÎ±
=
Z 2Ï€
0
Z 2Ï€
0
Â¡
cos2 Î¸
Â¢
(3 + cos Î±) dÎ¸ dÎ± = 6Ï€2
17. For (Î¸, Î±) âˆˆ[0, 25] Ã— [0, 2Ï€] , let
f (Î¸, Î±) â‰¡(cos Î¸ (4 + 2 cos Î±) , âˆ’sin Î¸ (4 + 2 cos Î±) , 2 sin Î± + Î¸)T .
Find a double integral which gives the area of f ([0, 25] Ã— [0, 2Ï€]) .
Answer:
In this case, the area element is
Â¡
68 + 64 cos Î± + 12 cos2 Î±
Â¢1/2 dÎ¸ dÎ±
and so the surface area is
Z 2Ï€
0
Z 2Ï€
0
Â¡
68 + 64 cos Î± + 12 cos2 Î±
Â¢1/2 dÎ¸ dÎ±.
18. For (Î¸, Î±) âˆˆ[0, 2Ï€] Ã— [0, 2Ï€] , and Î² a ï¬xed real number, deï¬ne f (Î¸, Î±) â‰¡
(cos Î¸ (2 + cos Î±) , âˆ’cos Î² sin Î¸ (2 + cos Î±) + sin Î² sin Î±,
sin Î² sin Î¸ (2 + cos Î±) + cos Î² sin Î±)T .
Find a double integral which gives the area of f ([0, 2Ï€] Ã— [0, 2Ï€]) .
Answer:
After many computations, the area element is
Â¡
4 + 4 cos Î± + cos2 Î±
Â¢1/2 dÎ¸ dÎ±. There-
fore, the area is
R 2Ï€
0
R 2Ï€
0
(2 + cos Î±) dÎ¸ dÎ± = 8Ï€2.

Part XII
Divergence Theorem
501


503
Outcomes
Flux Density and Divergence
A. Explain what is meant by the ï¬‚ux density and divergence of a vector ï¬eld.
B. Evaluate the divergence of a vector ï¬eld.
C. Evaluate the Laplacian of a function.
D. Derive formulas involving divergence, gradient and Laplacian.
Reading: Multivariable Calculus 5.1
Outcome Mapping:
A. M1
B. 1
C. 2,3
D. 4
The Divergence Theorem
A. Recall and verify the Divergence Theorem.
B. Apply the Divergence Theorem to evaluate the ï¬‚ux through a surface.
Reading: Multivariable Calculus 5.2
Outcome Mapping:
A. N1,N2,4,8
B. 1,2

504

The Divergence Theorem 29-30
Nov.
28.1
Divergence Of A Vector Field
Here the important concepts of divergence is deï¬ned.
Deï¬nition 28.1.1 Let f : U â†’Rp for U âŠ†Rp denote a vector ï¬eld. A scalar
valued function is called a scalar ï¬eld. The function, f is called a Ck vector ï¬eld if the
function, f is a Ck function. For a C1 vector ï¬eld, as just described âˆ‡Â· f (x) â‰¡div f (x)
known as the divergence, is deï¬ned as
âˆ‡Â· f (x) â‰¡div f (x) â‰¡
p
X
i=1
âˆ‚fi
âˆ‚xi
(x) .
Using the repeated summation convention, this is often written as
fi,i (x) â‰¡âˆ‚ifi (x)
where the comma indicates a partial derivative is being taken with respect to the ith variable
and âˆ‚i denotes diï¬€erentiation with respect to the ith variable. In words, the divergence is
the sum of the ith derivative of the ith component function of f for all values of i. Also
âˆ‡2f â‰¡âˆ‡Â· (âˆ‡f) .
This last symbol is important enough that it is given a name, the Laplacian.It is also
denoted by âˆ†. Thus âˆ‡2f = âˆ†f. In addition for f a vector ï¬eld, the symbol f Â· âˆ‡is deï¬ned
as a â€œdiï¬€erential operatorâ€ in the following way.
f Â· âˆ‡(g) â‰¡f1 (x) âˆ‚g (x)
âˆ‚x1
+ f2 (x) âˆ‚g (x)
âˆ‚x2
+ Â· Â· Â· + fp (x) âˆ‚g (x)
âˆ‚xp
.
Thus f Â· âˆ‡takes vector ï¬elds and makes them into new vector ï¬elds.
This deï¬nition is in terms of a given coordinate system but later a coordinate free def-
inition of div is presented. For now, everything is deï¬ned in terms of a given Cartesian
coordinate system. The divergence has profound physical signiï¬cance and this will be dis-
cussed later. For now it is important to understand how to ï¬nd it. Be sure you understand
that for f a vector ï¬eld, div f is a scalar ï¬eld meaning it is a scalar valued function of three
variables. For a scalar ï¬eld, f, âˆ‡f is a vector ï¬eld described earlier.
505

506
THE DIVERGENCE THEOREM 29-30 NOV.
Example 28.1.2 Let f (x) = xyi + (z âˆ’y) j + (sin (x) + z) k. Find div f
First the divergence of f is
âˆ‚(xy)
âˆ‚x
+ âˆ‚(z âˆ’y)
âˆ‚y
+ âˆ‚(sin (x) + z)
âˆ‚z
= y + (âˆ’1) + 1 = y.
28.2
The Divergence Theorem
Why does anyone care about the divergence of a vector ï¬eld? The answer is contained in
this section. In short, it is because of the divergence theorem which relates the ï¬‚ux over the
boundary to a volume integral of the divergence. It is also called Gaussâ€™s theorem.
Deï¬nition 28.2.1 A subset, V of R3 is called cylindrical in the x direction if it is
of the form
V = {(x, y, z) : Ï† (y, z) â‰¤x â‰¤Ïˆ (y, z) for (y, z) âˆˆD}
where D is a subset of the yz plane. V is cylindrical in the z direction if
V = {(x, y, z) : Ï† (x, y) â‰¤z â‰¤Ïˆ (x, y) for (x, y) âˆˆD}
where D is a subset of the xy plane, and V is cylindrical in the y direction if
V = {(x, y, z) : Ï† (x, z) â‰¤y â‰¤Ïˆ (x, z) for (x, z) âˆˆD}
where D is a subset of the xz plane. If V is cylindrical in the z direction, denote by âˆ‚V the
boundary of V deï¬ned to be the points of the form (x, y, Ï† (x, y)) , (x, y, Ïˆ (x, y)) for (x, y) âˆˆ
D, along with points of the form (x, y, z) where (x, y) âˆˆâˆ‚D and Ï† (x, y) â‰¤z â‰¤Ïˆ (x, y) .
Points on âˆ‚D are deï¬ned to be those for which every open ball contains points which are in
D as well as points which are not in D. A similar deï¬nition holds for âˆ‚V in the case that
V is cylindrical in one of the other directions.
The following picture illustrates the above deï¬nition in the case of V cylindrical in the
z direction.
z = Ïˆ(x, y)
z = Ï†(x, y)
Â¡
Â¡
Â¡
Â¡
x
z
y
Of course, many three dimensional sets are cylindrical in each of the coordinate direc-
tions. For example, a ball or a rectangle or a tetrahedron are all cylindrical in each direction.

28.2.
THE DIVERGENCE THEOREM
507
The following lemma allows the exchange of the volume integral of a partial derivative for
an area integral in which the derivative is replaced with multiplication by an appropriate
component of the unit exterior normal.
Lemma 28.2.2 Suppose V is cylindrical in the z direction and that Ï† and Ïˆ are the
functions in the above deï¬nition. Assume Ï† and Ïˆ are C1 functions and suppose F is a C1
function deï¬ned on V. Also, let n = (nx, ny, nz) be the unit exterior normal to âˆ‚V. Then
Z
V
âˆ‚F
âˆ‚z (x, y, z) dV =
Z
âˆ‚V
Fnz dA.
Proof: From the fundamental theorem of calculus,
Z
V
âˆ‚F
âˆ‚z (x, y, z) dV
=
Z
D
Z Ïˆ(x,y)
Ï†(x,y)
âˆ‚F
âˆ‚z (x, y, z) dz dx dy
(28.1)
=
Z
D
[F (x, y, Ïˆ (x, y)) âˆ’F (x, y, Ï† (x, y))] dx dy
Now the unit exterior normal on the top of V, the surface (x, y, Ïˆ (x, y)) is
1
q
Ïˆ2
x + Ïˆ2
y + 1
Â¡
âˆ’Ïˆx, âˆ’Ïˆy, 1
Â¢
.
This follows from the observation that the top surface is the level surface, z âˆ’Ïˆ (x, y) = 0
and so the gradient of this function of three variables is perpendicular to the level surface.
It points in the correct direction because the z component is positive. Therefore, on the top
surface,
nz =
1
q
Ïˆ2
x + Ïˆ2
y + 1
Similarly, the unit normal to the surface on the bottom is
1
q
Ï†2
x + Ï†2
y + 1
Â¡
Ï†x, Ï†y, âˆ’1
Â¢
and so on the bottom surface,
nz =
âˆ’1
q
Ï†2
x + Ï†2
y + 1
Note that here the z component is negative because since it is the outer normal it must
point down. On the lateral surface, the one where (x, y) âˆˆâˆ‚D and z âˆˆ[Ï† (x, y) , Ïˆ (x, y)] ,
nz = 0.
The area element on the top surface is dA =
q
Ïˆ2
x + Ïˆ2
y + 1 dx dy while the area element
on the bottom surface is
q
Ï†2
x + Ï†2
y + 1 dx dy. Therefore, the last expression in 28.1 is of the
form,
Z
D
F (x, y, Ïˆ (x, y))
nz
z
}|
{
1
q
Ïˆ2
x + Ïˆ2
y + 1
dA
z
}|
{
q
Ïˆ2
x + Ïˆ2
y + 1 dx dy+
Z
D
F (x, y, Ï† (x, y))
nz
z
}|
{
ï£«
ï£­
âˆ’1
q
Ï†2
x + Ï†2
y + 1
ï£¶
ï£¸
dA
z
}|
{
q
Ï†2
x + Ï†2
y + 1 dx dy

508
THE DIVERGENCE THEOREM 29-30 NOV.
+
Z
Lateral surface
Fnz dA,
the last term equaling zero because on the lateral surface, nz = 0. Therefore, this reduces
to
R
âˆ‚V Fnz dA as claimed.
The following corollary is entirely similar to the above.
Corollary 28.2.3 If V is cylindrical in the y direction, then
Z
V
âˆ‚F
âˆ‚y dV =
Z
âˆ‚V
Fny dA
and if V is cylindrical in the x direction, then
Z
V
âˆ‚F
âˆ‚x dV =
Z
âˆ‚V
Fnx dA
With this corollary, here is a proof of the divergence theorem.
Theorem 28.2.4 Let V be cylindrical in each of the coordinate directions and let
F be a C1 vector ï¬eld deï¬ned on V. Then
Z
V
âˆ‡Â· F dV =
Z
âˆ‚V
F Â· n dA.
Proof: From the above lemma and corollary,
Z
V
âˆ‡Â· F dV
=
Z
V
âˆ‚F1
âˆ‚x + âˆ‚F2
âˆ‚y + âˆ‚F3
âˆ‚y dV
=
Z
âˆ‚V
(F1nx + F2ny + F3nz) dA
=
Z
âˆ‚V
F Â· n dA.
This proves the theorem.
The divergence theorem holds for much more general regions than this. Suppose for
example you have a complicated region which is the union of ï¬nitely many disjoint regions
of the sort just described which are cylindrical in each of the coordinate directions. Then
the volume integral over the union of these would equal the sum of the integrals over the
disjoint regions. If the boundaries of two of these regions intersect, then the area integrals
will cancel out on the intersection because the unit exterior normals will point in opposite
directions. Therefore, the sum of the integrals over the boundaries of these disjoint regions
will reduce to an integral over the boundary of the union of these. Hence the divergence
theorem will continue to hold. For example, consider the following picture. If the divergence
theorem holds for each Vi in the following picture, then it holds for the union of these two.
V1
V2
General formulations of the divergence theorem involve Hausdorï¬€measures and the
Lebesgue integral, a better integral than the old fashioned Riemann integral which has been
obsolete now for almost 100 years. When all is said and done, one ï¬nds that the conclusion
of the divergence theorem is usually true and it can be used with conï¬dence.

28.2.
THE DIVERGENCE THEOREM
509
Example 28.2.5 Let V = [0, 1] Ã— [0, 1] Ã— [0, 1] . That is, V is the cube in the ï¬rst octant
having the lower left corner at (0, 0, 0) and the sides of length 1. Let F (x, y, z) = xi+yj+zk.
Find the ï¬‚ux integral in which n is the unit exterior normal.
Z
âˆ‚V
F Â· ndS
You can certainly inï¬‚ict much suï¬€ering on yourself by breaking the surface up into 6
pieces corresponding to the 6 sides of the cube, ï¬nding a parameterization for each face and
adding up the appropriate ï¬‚ux integrals. For example, n = k on the top face and n = âˆ’k
on the bottom face. On the top face, a parameterization is (x, y, 1) : (x, y) âˆˆ[0, 1] Ã— [0, 1] .
The area element is just dxdy. It isnâ€™t really all that hard to do it this way but it is much
easier to use the divergence theorem. The above integral equals
Z
V
div (F) dV =
Z
V
3dV = 3.
Example 28.2.6 This time, let V be the unit ball,
Â©
(x, y, z) : x2 + y2 + z2 â‰¤1
Âª
and let
F (x, y, z) = x2i + yj+ (z âˆ’1) k. Find
Z
âˆ‚V
F Â· ndS.
As in the above you could do this by brute force.
A parameterization of the âˆ‚V is
obtained as
x = sin Ï† cos Î¸, y = sin Ï† sin Î¸, z = cos Ï†
where (Ï†, Î¸) âˆˆ(0, Ï€) Ã— (0, 2Ï€]. Now this does not include all the ball but it includes all but
the point at the top and at the bottom. As far as the ï¬‚ux integral is concerned these points
contribute nothing to the integral so you can neglect them. Then you can grind away and
get the ï¬‚ux integral which is desired. However, it is so much easier to use the divergence
theorem! Using spherical coordinates,
Z
âˆ‚V
F Â· ndS
=
Z
V
div (F) dV =
Z
V
(2x + 1 + 1) dV
=
Z Ï€
0
Z 2Ï€
0
Z 1
0
(2 + 2Ï sin (Ï†) cos Î¸) Ï2 sin (Ï†) dÏdÎ¸dÏ† = 8
3Ï€
Example 28.2.7 Suppose V is an open set in R3 for which the divergence theorem holds.
Let F (x, y, z) = xi + yj + zk. Then show
Z
âˆ‚V
F Â· ndS = 3 Ã— volume(V ).
This follows from the divergenc theorem.
Z
âˆ‚V
F Â· ndS =
Z
V
div (F) dV = 3
Z
V
dV = 3 Ã— volume(V ).
The message of the divergence theorem is the relation between the volume integral and
an area integral. This is the exciting thing about this marvelous theorem. It is not its utility
as a method for evaluations of boring problems. This will be shown in the examples of its
use which follow.

510
THE DIVERGENCE THEOREM 29-30 NOV.
28.2.1
Coordinate Free Concept Of Divergence, Flux Density
The divergence theorem also makes possible a coordinate free deï¬nition of the divergence.
Theorem 28.2.8 Let B (x, Î´) be the ball centered at x having radius Î´ and let F
be a C1 vector ï¬eld. Then letting v (B (x, Î´)) denote the volume of B (x, Î´) given by
Z
B(x,Î´)
dV,
it follows
div F (x) = lim
Î´â†’0+
1
v (B (x, Î´))
Z
âˆ‚B(x,Î´)
F Â· n dA.
(28.2)
Proof: The divergence theorem holds for balls because they are cylindrical in every
direction. Therefore,
1
v (B (x, Î´))
Z
âˆ‚B(x,Î´)
F Â· n dA =
1
v (B (x, Î´))
Z
B(x,Î´)
div F (y) dV.
Therefore, since div F (x) is a constant,
Â¯Â¯Â¯Â¯Â¯div F (x) âˆ’
1
v (B (x, Î´))
Z
âˆ‚B(x,Î´)
F Â· n dA
Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯div F (x) âˆ’
1
v (B (x, Î´))
Z
B(x,Î´)
div F (y) dV
Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯
1
v (B (x, Î´))
Z
B(x,Î´)
(div F (x) âˆ’div F (y)) dV
Â¯Â¯Â¯Â¯Â¯
â‰¤
1
v (B (x, Î´))
Z
B(x,Î´)
|div F (x) âˆ’div F (y)| dV
â‰¤
1
v (B (x, Î´))
Z
B(x,Î´)
Îµ
2 dV < Îµ
whenever Îµ is small enough due to the continuity of div F. Since Îµ is arbitrary, this shows
28.2.
How is this deï¬nition independent of coordinates? It only involves geometrical notions
of volume and dot product. This is why. Imagine rotating the coordinate axes, keeping
all distances the same and expressing everything in terms of the new coordinates.
The
divergence would still have the same value because of this theorem.
You also see the physical signiï¬cance of the divergence from this.
It measures the
tendency of the vector ï¬eld to â€œdivergeâ€ from a point.
28.3
The Weak Maximum Principleâˆ—
There is a fundamental result having great signiï¬cance which involves âˆ‡2 called the max-
imum principle. This principle says that if âˆ‡2u â‰¥0 on a bounded open set, U, then u
achieves its maximum value on the boundary of U. It is a very important result which ties
in many earlier topics. Donâ€™t read it if you are not interested.

28.4.
SOME APPLICATIONS OF THE DIVERGENCE THEOREMâˆ—
511
Theorem 28.3.1 Let U be a bounded open set in Rn and suppose u âˆˆC2 (U) âˆ©
C
Â¡
U
Â¢
such that âˆ‡2u â‰¥0 in U. Then letting âˆ‚U = U\U, it follows that max
Â©
u (x) : x âˆˆU
Âª
=
max {u (x) : x âˆˆâˆ‚U} .
Proof: If this is not so, there exists x0 âˆˆU such that u (x0) > max {u (x) : x âˆˆâˆ‚U} â‰¡
M. Since U is bounded, there exists Îµ > 0 such that
u (x0) > max
n
u (x) + Îµ |x|2 : x âˆˆâˆ‚U
o
.
Therefore, u (x) + Îµ |x|2 also has its maximum in U because for Îµ small enough,
u (x0) + Îµ |x0|2 > u (x0) > max
n
u (x) + Îµ |x|2 : x âˆˆâˆ‚U
o
for all x âˆˆâˆ‚U.
Now let x1 be the point in U at which u (x) + Îµ |x|2 achieves its maximum.
As an
exercise you should show that âˆ‡2 (f + g) = âˆ‡2f +âˆ‡2g and therefore, âˆ‡2 Â³
u (x) + Îµ |x|2Â´
=
âˆ‡2u (x) + 2nÎµ. (Why?) Therefore,
0 â‰¥âˆ‡2u (x1) + 2nÎµ â‰¥2nÎµ,
a contradiction. This proves the theorem.
28.4
Some Applications Of The Divergence Theoremâˆ—
There are numerous applications of the divergence theorem. Some are listed here. You
might want to read this if you are interested in applications. However, it wonâ€™t be needed
for tests.
28.4.1
Hydrostatic Pressureâˆ—
Imagine a ï¬‚uid which does not move which is acted on by an acceleration, g. Of course the
acceleration is usually the acceleration of gravity. Also let the density of the ï¬‚uid be Ï, a
function of position. What can be said about the pressure, p, in the ï¬‚uid? Let B (x, Îµ) be a
small ball centered at the point, x. Then the force the ï¬‚uid exerts on this ball would equal
âˆ’
Z
âˆ‚B(x,Îµ)
pn dA.
Here n is the unit exterior normal at a small piece of âˆ‚B (x, Îµ) having area dA. By the
divergence theorem, this integral equals
âˆ’
Z
B(x,Îµ)
âˆ‡p dV.
Also the force acting on this small ball of ï¬‚uid is
Z
B(x,Îµ)
Ïg dV.
Since it is given that the ï¬‚uid does not move, the sum of these forces must equal zero. Thus
Z
B(x,Îµ)
Ïg dV =
Z
B(x,Îµ)
âˆ‡p dV.

512
THE DIVERGENCE THEOREM 29-30 NOV.
Since this must hold for any ball in the ï¬‚uid of any radius, it must be that
âˆ‡p = Ïg.
(28.3)
It turns out that the pressure in a lake at depth z is equal to 62.5z. This is easy to see
from 28.3. In this case, g = gk where g = 32 feet/sec2. The weight of a cubic foot of water
is 62.5 pounds. Therefore, the mass in slugs of this water is 62.5/32. Since it is a cubic foot,
this is also the density of the water in slugs per cubic foot. Also, it is normally assumed
that water is incompressible1. Therefore, this is the mass of water at any depth. Therefore,
âˆ‚p
âˆ‚xi+âˆ‚p
âˆ‚y j+âˆ‚p
âˆ‚z k =62.5
32 Ã— 32k.
and so p does not depend on x and y and is only a function of z. It follows p (0) = 0, and
pâ€² (z) = 62.5. Therefore, p (x, y, z) = 62.5z. This establishes the claim. This is interesting
but 28.3 is more interesting because it does not require Ï to be constant.
28.4.2
Archimedes Law Of Buoyancyâˆ—
Archimedes principle states that when a solid body is immersed in a ï¬‚uid the net force acting
on the body by the ï¬‚uid is directly up and equals the total weight of the ï¬‚uid displaced.
Denote the set of points in three dimensions occupied by the body as V. Then for dA
an increment of area on the surface of this body, the force acting on this increment of area
would equal âˆ’p dAn where n is the exterior unit normal. Therefore, since the ï¬‚uid does not
move,
Z
âˆ‚V
âˆ’pn dA =
Z
V
âˆ’âˆ‡p dV =
Z
V
Ïg dV k
Which equals the total weight of the displaced ï¬‚uid and you note the force is directed upward
as claimed. Here Ï is the density and 28.3 is being used. There is an interesting point in the
above explanation. Why does the second equation hold? Imagine that V were ï¬lled with
ï¬‚uid. Then the equation follows from 28.3 because in this equation g = âˆ’gk.
28.4.3
Equations Of Heat And Diï¬€usionâˆ—
Let x be a point in three dimensional space and let (x1, x2, x3) be Cartesian coordinates of
this point. Let there be a three dimensional body having density, Ï = Ï (x, t).
The heat ï¬‚ux, J, in the body is deï¬ned as a vector which has the following property.
Rate at which heat crosses S =
Z
S
J Â· n dA
where n is the unit normal in the desired direction. Thus if V is a three dimensional body,
Rate at which heat leaves V =
Z
âˆ‚V
J Â· n dA
where n is the unit exterior normal.
Fourierâ€™s law of heat conduction states that the heat ï¬‚ux, J satisï¬es J = âˆ’kâˆ‡(u) where
u is the temperature and k = k (u, x, t) is called the coeï¬ƒcient of thermal conductivity.
This changes depending on the material. It also can be shown by experiment to change
1There is no such thing as an incompressible ï¬‚uid but this doesnâ€™t stop people from making this assump-
tion.

28.4.
SOME APPLICATIONS OF THE DIVERGENCE THEOREMâˆ—
513
with temperature. This equation for the heat ï¬‚ux states that the heat ï¬‚ows from hot places
toward colder places in the direction of greatest rate of decrease in temperature. Let c (x, t)
denote the speciï¬c heat of the material in the body. This means the amount of heat within
V is given by the formula
R
V Ï (x, t) c (x, t) u (x, t) dV. Suppose also there are sources for the
heat within the material given by f (x,u, t) . If f is positive, the heat is increasing while if
f is negative the heat is decreasing. For example such sources could result from a chemical
reaction taking place. Then the divergence theorem can be used to verify the following
equation for u. Such an equation is called a reaction diï¬€usion equation.
âˆ‚
âˆ‚t (Ï (x, t) c (x, t) u (x, t)) = âˆ‡Â· (k (u, x, t) âˆ‡u (x, t)) + f (x, u, t) .
(28.4)
Take an arbitrary V for which the divergence theorem holds. Then the time rate of
change of the heat in V is
d
dt
Z
V
Ï (x, t) c (x, t) u (x, t) dV =
Z
V
âˆ‚(Ï (x, t) c (x, t) u (x, t))
âˆ‚t
dV
where, as in the preceding example, this is a physical derivation so the consideration of
hard mathematics is not necessary. Therefore, from the Fourier law of heat conduction,
d
dt
R
V Ï (x, t) c (x, t) u (x, t) dV =
Z
V
âˆ‚(Ï (x, t) c (x, t) u (x, t))
âˆ‚t
dV =
rate at which heat enters
z
}|
{
Z
âˆ‚V
âˆ’J Â· n dA
+
Z
V
f (x, u, t) dV
=
Z
âˆ‚V
kâˆ‡(u) Â· n dA +
Z
V
f (x, u, t) dV =
Z
V
(âˆ‡Â· (kâˆ‡(u)) + f) dV.
Since this holds for every sample volume, V it must be the case that the above reaction
diï¬€usion equation, 28.4 holds. Note that more interesting equations can be obtained by
letting more of the quantities in the equation depend on temperature. However, the above
is a fairly hard equation and people usually assume the coeï¬ƒcient of thermal conductivity
depends only on x and that the reaction term, f depends only on x and t and that Ï and c
are constant. Then it reduces to the much easier equation,
âˆ‚
âˆ‚tu (x, t) = 1
Ïcâˆ‡Â· (k (x) âˆ‡u (x, t)) + f (x,t) .
(28.5)
This is often referred to as the heat equation. Sometimes there are modiï¬cations of this
in which k is not just a scalar but a matrix to account for diï¬€erent heat ï¬‚ow properties
in diï¬€erent directions. However, they are not much harder than the above. The major
mathematical diï¬ƒculties result from allowing k to depend on temperature.
It is known that the heat equation is not correct even if the thermal conductivity did
not depend on u because it implies inï¬nite speed of propagation of heat. However, this does
not prevent people from using it.
28.4.4
Balance Of Massâˆ—
Let y be a point in three dimensional space and let (y1, y2, y3) be Cartesian coordinates of
this point. Let V be a region in three dimensional space and suppose a ï¬‚uid having density,
Ï (y, t) and velocity, v (y,t) is ï¬‚owing through this region. Then the mass of ï¬‚uid leaving V
per unit time is given by the area integral,
R
âˆ‚V Ï (y, t) v (y, t) Â· n dA while the total mass of
the ï¬‚uid enclosed in V at a given time is
R
V Ï (y, t) dV. Also suppose mass originates at the

514
THE DIVERGENCE THEOREM 29-30 NOV.
rate f (y, t) per cubic unit per unit time within this ï¬‚uid. Then the conclusion which can
be drawn through the use of the divergence theorem is the following fundamental equation
known as the mass balance equation.
âˆ‚Ï
âˆ‚t + âˆ‡Â· (Ïv) = f (y, t)
(28.6)
To see this is so, take an arbitrary V for which the divergence theorem holds. Then the
time rate of change of the mass in V is
âˆ‚
âˆ‚t
Z
V
Ï (y, t) dV =
Z
V
âˆ‚Ï (y, t)
âˆ‚t
dV
where the derivative was taken under the integral sign with respect to t. (This is a physical
derivation and therefore, it is not necessary to fuss with the hard mathematics related to
the change of limit operations.
You should expect this to be true under fairly general
conditions because the integral is a sort of sum and the derivative of a sum is the sum of
the derivatives.) Therefore, the rate of change of mass,
âˆ‚
âˆ‚t
R
V Ï (y, t) dV, equals
Z
V
âˆ‚Ï (y, t)
âˆ‚t
dV
=
rate at which mass enters
z
}|
{
âˆ’
Z
âˆ‚V
Ï (y, t) v (y, t) Â· n dA +
Z
V
f (y, t) dV
=
âˆ’
Z
V
(âˆ‡Â· (Ï (y, t) v (y, t)) + f (y, t)) dV.
Since this holds for every sample volume, V it must be the case that the equation of
continuity holds. Again, there are interesting mathematical questions here which can be
explored but since it is a physical derivation, it is not necessary to dwell too much on them.
If all the functions involved are continuous, it is certainly true but it is true under far more
general conditions than that.
Also note this equation applies to many situations and f might depend on more than
just y and t. In particular, f might depend also on temperature and the density, Ï. This
would be the case for example if you were considering the mass of some chemical and f
represented a chemical reaction. Mass balance is a general sort of equation valid in many
contexts.
28.4.5
Balance Of Momentumâˆ—
This example is a little more substantial than the above. It concerns the balance of mo-
mentum for a continuum. To see a full description of all the physics involved, you should
consult a book on continuum mechanics. One of the most elegant, possibly the most elegant
is the book by Gurtin [13]. To read this book, you will need to know what the derivative of
a function of many variables is. This is also the case in this section.
The situation is of a material in three dimensions and it deforms and moves about in
three dimensions. This means this material is not a rigid body. Let B0 denote an open set
identifying a chunk of this material at time t = 0 and let Bt be an open set which identiï¬es
the same chunk of material at time t > 0.
Let y (t, x) = (y1 (t, x) , y2 (t, x) , y3 (t, x)) denote the position with respect to Cartesian
coordinates at time t of the point whose position at time t = 0 is x = (x1, x2, x3) . The
coordinates, x are sometimes called the reference coordinates and sometimes the material
coordinates and sometimes the Lagrangian coordinates. The coordinates, y are called the

28.4.
SOME APPLICATIONS OF THE DIVERGENCE THEOREMâˆ—
515
Eulerian coordinates or sometimes the spacial coordinates and the function, (t, x) â†’y (t, x)
is called the motion2. Thus
y (0, x) = x.
(28.7)
The derivative,
D2y (t, x)
is called the deformation gradient. Recall the notation means you ï¬x t and consider the func-
tion, x â†’y (t, x) , taking its derivative. Since it is a linear transformation, it is represented
by the usual matrix, whose ijth entry is given by
Fij (x) = âˆ‚yi (t, x)
âˆ‚xj
.
Let Ï (t, y) denote the density of the material at time t at the point, y and let Ï0 (x) denote
the density of the material at the point, x. Thus Ï0 (x) = Ï (0, x) = Ï (0, y (0, x)) . The ï¬rst
task is to consider the relationship between Ï (t, y) and Ï0 (x) .
Lemma 28.4.1 Ï0 (x) = Ï (t, y (t, x)) det (F) and in any reasonable physical motion,
det (F) > 0.
Proof: Let V0 represent a small chunk of material at t = 0 and let Vt represent the same
chunk of material at time t. I will be a little sloppy and refer to V0 as the small chunk of
material at time t = 0 and Vt as the chunk of material at time t rather than an open set
representing the chunk of material. Then by the change of variables formula for multiple
integrals,
Z
Vt
dV =
Z
V0
|det (F)| dV.
If det (F) = 0 for some t the above formula shows that the chunk of material went from
positive volume to zero volume and this is not physically possible. Therefore, it is impossible
that det (F) can equal zero.
However, at t = 0, F = I, the identity because of 28.7.
Therefore, det (F) = 1 at t = 0 and if it is assumed t â†’det (F) is continuous it follows
by the intermediate value theorem that det (F) > 0 for all t. Of course it is not known for
sure this function is continuous but the above shows why it is at least reasonable to expect
det (F) > 0.
Now using the change of variables formula,
mass of Vt
=
Z
Vt
Ï (t, y) dV =
Z
V0
Ï (t, y (t, x)) det (F) dV
=
mass of V0 =
Z
V0
Ï0 (x) dV.
Since V0 is arbitrary, it follows Ï0 (x) = Ï (t, y (t, x)) det (F) as claimed. Note this shows
that det (F) is a magniï¬cation factor for the density.
Now consider a small chunk of material, Bt at time t which corresponds to B0 at time
t = 0. The total linear momentum of this material at time t is
Z
Bt
Ï (t, y) v (t, y) dV
where v is the velocity. By Newtonâ€™s second law, the time rate of change of this linear
momentum should equal the total force acting on the chunk of material. In the following
2Apparently, the terminology is all mixed up in so far as the names Euler and Lagrange are concerned.

516
THE DIVERGENCE THEOREM 29-30 NOV.
derivation, dV (y) will indicate the integration is taking place with respect to the variable,
y. By Lemma 28.4.1 and the change of variables formula for multiple integrals
d
dt
ÂµZ
Bt
Ï (t, y) v (t, y) dV (y)
Â¶
=
d
dt
ÂµZ
B0
Ï (t, y (t, x)) v (t, y (t, x)) det (F) dV (x)
Â¶
=
d
dt
ÂµZ
B0
Ï0 (x) v (t, y (t, x)) dV (x)
Â¶
=
Z
B0
Ï0 (x)
Â·âˆ‚v
âˆ‚t + âˆ‚v
âˆ‚yi
âˆ‚yi
âˆ‚t
Â¸
dV (x)
=
Z
Bt
Ï (t, y) det (F)
Â·âˆ‚v
âˆ‚t + âˆ‚v
âˆ‚yi
âˆ‚yi
âˆ‚t
Â¸
1
det (F) dV (y)
=
Z
Bt
Ï (t, y)
Â·âˆ‚v
âˆ‚t + âˆ‚v
âˆ‚yi
âˆ‚yi
âˆ‚t
Â¸
dV (y) .
This uses the repeated index summation convention. Having taken the derivative of the
total momentum, it is time to consider the total force acting on the chunk of material.
The force comes from two sources, a body force, b and a force which act on the boundary
of the chunk of material called a traction force. Typically, the body force is something like
gravity in which case, b = âˆ’gÏk, assuming the Cartesian coordinate system has been chosen
in the usual manner. It could also be centrifugal force which might result if the body were
undergoing some sort of rigid motion. The traction force is of the form
Z
âˆ‚Bt
s (t, y, n) dA
where n is the unit exterior normal. Thus the traction force depends on position, time, and
the orientation of the boundary of Bt. Cauchy showed the existence of a linear transforma-
tion, T (t, y) such that T (t, y) n = s (t, y, n) . It follows there is a matrix, Tij (t, y) such that
the ith component of s is given by si (t, y, n) = Tij (t, y) nj. Cauchy also showed this matrix
is symmetric, Tij = Tji. (This comes from conservation of angular momentum.) It is called
the Cauchy stress. Using Newtonâ€™s second law to equate the time derivative of the total
linear momentum with the applied forces and using the usual repeated index summation
convention,
Z
Bt
Ï (t, y)
Â·âˆ‚v
âˆ‚t + âˆ‚v
âˆ‚yi
âˆ‚yi
âˆ‚t
Â¸
dV (y) =
Z
Bt
b (t, y) dV (y) +
Z
âˆ‚Bt
Tij (t, y) nj dA.
Here is where the divergence theorem is used. In the last integral, the multiplication by nj
is exchanged for the jth partial derivative and an integral over Bt. Thus
Z
Bt
Ï (t, y)
Â·âˆ‚v
âˆ‚t + âˆ‚v
âˆ‚yi
âˆ‚yi
âˆ‚t
Â¸
dV (y) =
Z
Bt
b (t, y) dV (y) +
Z
Bt
âˆ‚(Tij (t, y))
âˆ‚yj
dV (y) .
Since Bt was arbitrary, it follows
Ï (t, y)
Â·âˆ‚v
âˆ‚t + âˆ‚v
âˆ‚yi
âˆ‚yi
âˆ‚t
Â¸
=
b (t, y) + âˆ‚(Tij (t, y))
âˆ‚yj
â‰¡
b (t, y) + div (T)
where here div T is a vector whose ith component is given by
(div T)i = âˆ‚Tij
âˆ‚yj
.

28.4.
SOME APPLICATIONS OF THE DIVERGENCE THEOREMâˆ—
517
The term, âˆ‚v
âˆ‚t + âˆ‚v
âˆ‚yi
âˆ‚yi
âˆ‚t , is the total derivative with respect to t of the velocity v, written as
Ë™v. Thus you might see this written as
Ï Ë™v = b + div (T) .
The above formulation of the balance of momentum involves the spatial coordinates, y
but people also like to formulate momentum balance in terms of the material coordinates, x.
The spacial coordinates are ï¬ne if you are looking for example at the ï¬‚ow of a ï¬‚uid through
some ï¬xed region in space. However, if you are interested in the deformation of a material,
then you might want to consider things like traction boundary conditions. To make sense
of these, you should be dealing with the reference or material coordinates. Of course this
changes everything.
The momentum in terms of the material coordinates is
Z
B0
Ï0 (x) v (t, x) dV
and so, since x does not depend on t,
d
dt
ÂµZ
B0
Ï0 (x) v (t, x) dV
Â¶
=
Z
B0
Ï0 (x) vt (t, x) dV.
As indicated earlier, this is a physical derivation and so the mathematical questions related
to interchange of limit operations are ignored. This must equal the total applied force. Thus
Z
B0
Ï0 (x) vt (t, x) dV =
Z
B0
b0 (t, x) dV +
Z
âˆ‚Bt
TijnjdA,
(28.8)
the ï¬rst term on the right being the contribution of the body force given per unit volume
in the material coordinates and the last term being the traction force discussed earlier. The
task is to write this last integral as one over âˆ‚B0. For y âˆˆâˆ‚Bt there is a unit outer normal,
n. Here y = y (t, x) for x âˆˆâˆ‚B0. Then deï¬ne N to be the unit outer normal to B0 at the
point, x. Near the point y âˆˆâˆ‚Bt the surface, âˆ‚Bt is given parametrically in the form
y = y (s, t) for (s, t) âˆˆD âŠ†R2 and it can be assumed the unit normal to âˆ‚Bt near this
point is
n = ys (s, t) Ã— yt (s, t)
|ys (s, t) Ã— yt (s, t)|
with the area element given by |ys (s, t) Ã— yt (s, t)| ds dt. This is true for y âˆˆPt âŠ†âˆ‚Bt, a
small piece of âˆ‚Bt. Therefore, the last integral in 28.8 is the sum of integrals over small
pieces of the form
Z
Pt
TijnjdA
(28.9)
where Pt is parametrized by y (s, t) , (s, t) âˆˆD. Thus the integral in 28.9 is of the form
Z
D
Tij (y (s, t)) (ys (s, t) Ã— yt (s, t))j ds dt.
Using the repeated index summation convention, and the chain rule, this equals
Z
D
Tij (y (s, t))
Âµ âˆ‚y
âˆ‚xÎ±
âˆ‚xÎ±
âˆ‚s Ã— âˆ‚y
âˆ‚xÎ²
âˆ‚xÎ²
âˆ‚t
Â¶
j
ds dt.

518
THE DIVERGENCE THEOREM 29-30 NOV.
Remember y = y (t, x) and it is always assumed the mapping x â†’y (t, x) is one to one and
so, since on the surface âˆ‚Bt near y, the points are functions of (s, t) , it follows x is also a
function of (s, t) . Now by the properties of the cross product, this last integral equals
Z
D
Tij (x (s, t)) âˆ‚xÎ±
âˆ‚s
âˆ‚xÎ²
âˆ‚t
Âµ âˆ‚y
âˆ‚xÎ±
Ã— âˆ‚y
âˆ‚xÎ²
Â¶
j
ds dt
(28.10)
where here x (s, t) is the point of âˆ‚B0 which corresponds with y (s, t) âˆˆâˆ‚Bt.
Thus
Tij (x (s, t)) = Tij (y (s, t)) . (Perhaps this is a slight abuse of notation because Tij is deï¬ned
on âˆ‚Bt, not on âˆ‚B0, but it avoids introducing extra symbols.) Next 28.10 equals
Z
D
Tij (x (s, t)) âˆ‚xÎ±
âˆ‚s
âˆ‚xÎ²
âˆ‚t Îµjab
âˆ‚ya
âˆ‚xÎ±
âˆ‚yb
âˆ‚xÎ²
ds dt
=
Z
D
Tij (x (s, t)) âˆ‚xÎ±
âˆ‚s
âˆ‚xÎ²
âˆ‚t ÎµcabÎ´jc
âˆ‚ya
âˆ‚xÎ±
âˆ‚yb
âˆ‚xÎ²
ds dt
=
Z
D
Tij (x (s, t)) âˆ‚xÎ±
âˆ‚s
âˆ‚xÎ²
âˆ‚t Îµcab
=Î´jc
z
}|
{
âˆ‚yc
âˆ‚xp
âˆ‚xp
âˆ‚yj
âˆ‚ya
âˆ‚xÎ±
âˆ‚yb
âˆ‚xÎ²
ds dt
=
Z
D
Tij (x (s, t)) âˆ‚xÎ±
âˆ‚s
âˆ‚xÎ²
âˆ‚t
âˆ‚xp
âˆ‚yj
=ÎµpÎ±Î² det(F )
z
}|
{
Îµcab
âˆ‚yc
âˆ‚xp
âˆ‚ya
âˆ‚xÎ±
âˆ‚yb
âˆ‚xÎ²
ds dt
=
Z
D
(det F) Tij (x (s, t)) ÎµpÎ±Î²
âˆ‚xÎ±
âˆ‚s
âˆ‚xÎ²
âˆ‚t
âˆ‚xp
âˆ‚yj
ds dt.
Now âˆ‚xp
âˆ‚yj = F âˆ’1
pj
and also
ÎµpÎ±Î²
âˆ‚xÎ±
âˆ‚s
âˆ‚xÎ²
âˆ‚t = (xs Ã— xt)p
so the result just obtained is of the form
Z
D
(det F) F âˆ’1
pj Tij (x (s, t)) (xs Ã— xt)p ds dt =
Z
D
(det F) Tij (x (s, t))
Â¡
F âˆ’T Â¢
jp (xs Ã— xt)p ds dt.
This has transformed the integral over Pt to one over P0, the part of âˆ‚B0 which corresponds
with Pt. Thus the last integral is of the form
Z
P0
det (F)
Â¡
TF âˆ’T Â¢
ip NpdA
Summing these up over the pieces of âˆ‚Bt and âˆ‚B0 yields the last integral in 28.8 equals
Z
âˆ‚B0
det (F)
Â¡
TF âˆ’T Â¢
ip NpdA
and so the balance of momentum in terms of the material coordinates becomes
Z
B0
Ï0 (x) vt (t, x) dV =
Z
B0
b0 (t, x) dV +
Z
âˆ‚B0
det (F)
Â¡
TF âˆ’T Â¢
ip NpdA

28.4.
SOME APPLICATIONS OF THE DIVERGENCE THEOREMâˆ—
519
The matrix, det (F)
Â¡
TF âˆ’T Â¢
ip is called the ï¬rst Piola Kirchhoï¬€stress, S. An application
of the divergence theorem yields
Z
B0
Ï0 (x) vt (t, x) dV =
Z
B0
b0 (t, x) dV +
Z
B0
âˆ‚
Â³
det (F)
Â¡
TF âˆ’T Â¢
ip
Â´
âˆ‚xp
dV.
Since B0 is arbitrary, a balance law for momentum in terms of the material coordinates is
obtained
Ï0 (x) vt (t, x)
=
b0 (t, x) +
âˆ‚
Â³
det (F)
Â¡
TF âˆ’T Â¢
ip
Â´
âˆ‚xp
=
b0 (t, x) + div
Â¡
det (F)
Â¡
TF âˆ’T Â¢Â¢
=
b0 (t, x) + div S.
(28.11)
The main purpose of this presentation is to show how the divergence theorem is used
in a signiï¬cant way to obtain balance laws and to indicate a very interesting direction for
further study. To continue, one needs to specify T or S as an appropriate function of things
related to the motion, y. Often the thing related to the motion is something called the
strain and such relationships between the stress and the strain are known as constitutive
laws.
The proper formulation of constitutive laws involves more physical considerations
such as frame indiï¬€erence in which it is required the response of the system cannot depend
on the manner in which the Cartesian coordinate system was chosen. There are also many
other physical properties which can be included and which require a certain form for the
constitutive equations. These considerations are outside the scope of this book and require
a considerable amount of linear algebra.
There are also balance laws for energy which you may study later but these are more
problematic than the balance laws for mass and momentum. However, the divergence the-
orem is used in these also.
28.4.6
Bernoulliâ€™s Principleâˆ—
Consider a possibly moving ï¬‚uid with constant density, Ï and let P denote the pressure
in this ï¬‚uid. If B is a part of this ï¬‚uid the force exerted on B by the rest of the ï¬‚uid is
R
âˆ‚B âˆ’PndA where n is the outer normal from B. Assume this is the only force which matters
so for example there is no viscosity in the ï¬‚uid. Thus the Cauchy stress in rectangular
coordinates should be
T =
ï£«
ï£­
âˆ’P
0
0
0
âˆ’P
0
0
0
âˆ’P
ï£¶
ï£¸.
Then
div T = âˆ’âˆ‡P.
Also suppose the only body force is from gravity, a force of the form
âˆ’Ïgk
and so from the balance of momentum
Ï Ë™v = âˆ’Ïgk âˆ’âˆ‡P (x) .
(28.12)
Now in all this the coordinates are the spacial coordinates and it is assumed they are
rectangular. Thus
x = (x, y, z)T

520
THE DIVERGENCE THEOREM 29-30 NOV.
and v is the velocity while Ë™v is the total derivative of v = (v1, v2, v3)T given by vt + viv,i.
Take the dot product of both sides of 28.12 with v. This yields
(Ï/2) d
dt |v|2 = âˆ’Ïg dz
dt âˆ’d
dtP (x) .
Therefore,
d
dt
Ãƒ
Ï |v|2
2
+ Ïgz + P (x)
!
= 0
and so there is a constant, Câ€² such that
Ï |v|2
2
+ Ïgz + P (x) = Câ€²
For convenience deï¬ne Î³ to be the weight density of this ï¬‚uid. Thus Î³ = Ïg. Divide by Î³.
Then
|v|2
2g + z + P (x)
Î³
= C.
this is Bernoulliâ€™s3 principle. Note how if you keep the height the same, then if you raise
|v| , it follows the pressure drops.
This is often used to explain the lift of an airplane wing. The top surface is curved which
forces the air to go faster over the top of the wing causing a drop in pressure which creates
lift. It is also used to explain the concept of a venturi tube in which the air loses pressure
due to being pinched which causes it to ï¬‚ow faster. In many of these applications, the
assumptions used in which Ï is constant and there is no other contribution to the traction
force on âˆ‚B than pressure so in particular, there is no viscosity, are not correct. However, it
is hoped that the eï¬€ects of these deviations from the ideal situation above are small enough
that the conclusions are still roughly true. You can see how using balance of momentum
can be used to consider more diï¬ƒcult situations. For example, you might have a body force
which is more involved than gravity.
28.4.7
The Wave Equationâˆ—
As an example of how the balance law of momentum is used to obtain an important equation
of mathematical physics, suppose S = kF where k is a constant and F is the deformation
gradient and let u â‰¡y âˆ’x. Thus u is the displacement. Then from 28.11 you can verify
the following holds.
Ï0 (x) utt (t, x) = b0 (t, x) + kâˆ†u (t, x)
(28.13)
In the case where Ï0 is a constant and b0 = 0, this yields
utt âˆ’câˆ†u = 0.
The wave equation is utt âˆ’câˆ†u = 0 and so the above gives three wave equations, one for
each component.
3There were many Bernoullis. This is Daniel Bernoulli. He seems to have been nicer than some of the
others. Daniel was actually a doctor who was interested in mathematics.He lived from 1700-1782.

28.4.
SOME APPLICATIONS OF THE DIVERGENCE THEOREMâˆ—
521
28.4.8
A Negative Observationâˆ—
Many of the above applications of the divergence theorem are based on the assumption that
matter is continuously distributed in a way that the above arguments are correct. In other
words, a continuum. However, there is no such thing as a continuum. It has been known
for some time now that matter is composed of atoms. It is not continuously distributed
through some region of space as it is in the above. Apologists for this contradiction with
reality sometimes say to consider enough of the material in question that it is reasonable to
think of it as a continuum. This mystical reasoning is then violated as soon as they go from
the integral form of the balance laws to the diï¬€erential equations expressing the traditional
formulation of these laws. However, these laws continue to be used and seem to lead to
useful physical models which have value in predicting the behavior of physical systems.
This is what justiï¬es their use, not any fundamental truth. The possibility exists that the
reason for this is the numerical methods used to solve the partial diï¬€erential equations may
be better physical models than the ballance laws themselves. It is an area where people still
sometimes disagree.
28.4.9
Electrostaticsâˆ—
Coloumbâ€™s law says that the electric ï¬eld intensity at x of a charge q located at point, x0 is
given by
E = k q (x âˆ’x0)
|x âˆ’x0|3
where the electric ï¬eld intensity is deï¬ned to be the force experienced by a unit positive
charge placed at the point, x. Note that this is a vector and that its direction depends on
the sign of q. It points away from x0 if q is positive and points toward x0 if q is negative.
The constant, k is a physical constant like the gravitation constant. It has been computed
through careful experiments similar to those used with the calculation of the gravitation
constant.
The interesting thing about Coloumbâ€™s law is that E is the gradient of a function. In
fact,
E = âˆ‡
Âµ
qk
1
|x âˆ’x0|
Â¶
.
The other thing which is signiï¬cant about this is that in three dimensions and for x Ì¸= x0,
âˆ‡Â· âˆ‡
Âµ
qk
1
|x âˆ’x0|
Â¶
= âˆ‡Â· E = 0.
(28.14)
This is left as an exercise for you to verify.
These observations will be used to derive a very important formula for the integral,
Z
âˆ‚U
E Â· ndS
where E is the electric ï¬eld intensity due to a charge, q located at the point, x0 âˆˆU, a
bounded open set for which the divergence theorem holds.
Let UÎµ denote the open set obtained by removing the open ball centered at x0 which
has radius Îµ where Îµ is small enough that the following picture is a correct representation
of the situation.

522
THE DIVERGENCE THEOREM 29-30 NOV.
Â¡Â¡

q x0
Îµ
UÎµ
B(x0, Îµ) = BÏµ
Then on the boundary of BÎµ the unit outer normal to UÎµ is âˆ’xâˆ’x0
|xâˆ’x0|. Therefore,
Z
âˆ‚BÎµ
E Â· ndS
=
âˆ’
Z
âˆ‚BÎµ
k q (x âˆ’x0)
|x âˆ’x0|3 Â· x âˆ’x0
|x âˆ’x0|dS
=
âˆ’kq
Z
âˆ‚BÎµ
1
|x âˆ’x0|2 dS = âˆ’kq
Îµ2
Z
âˆ‚BÎµ
dS
=
âˆ’kq
Îµ2 4Ï€Îµ2 = âˆ’4Ï€kq.
Therefore, from the divergence theorem and observation 28.14,
âˆ’4Ï€kq +
Z
âˆ‚U
E Â· ndS =
Z
âˆ‚UÎµ
E Â· ndS =
Z
UÎµ
âˆ‡Â· EdV = 0.
It follows that
4Ï€kq =
Z
âˆ‚U
E Â· ndS.
If there are several charges located inside U, say q1, q2, Â· Â· Â·, qn, then letting Ei denote the
electric ï¬eld intensity of the ith charge and E denoting the total resulting electric ï¬eld
intensity due to all these charges,
Z
âˆ‚U
E Â· ndS
=
n
X
i=1
Z
âˆ‚U
Ei Â· ndS
=
n
X
i=1
4Ï€kqi = 4Ï€k
n
X
i=1
qi.
This is known as Gaussâ€™s law and it is the fundamental result in electrostatics.

Part XIII
Stokeâ€™s Theorem
523


525
Outcomes
Circulation Density and Curl
A. Explain what is meant by the circulation density and curl of a vector ï¬eld.
B. Evaluate the curl of a vector ï¬eld
C. Derive and apply formulas involving divergence, gradient and curl.
Reading: Multivariable Calculus 5.3
Outcome Mapping:
A. O1,3
B. 1,4,6
C. 2
Stokeâ€™s Theorem
A. Recall and verify Stokeâ€™s theorem.
B. Apply Stokeâ€™s theorem to calculate the circulation (or work) of a vector ï¬eld around
a simple closed curve.
C. Recall and apply the divergence and curl tests.
Reading: Multivariable Calculus 5.4
Outcome Mapping:
A. P1,1,12
B. 2,3,9
C. P2,4,5,10

526

Stokeâ€™s Theorem 4-5 Dec.
29.1
Curl Of A Vector Field
Here the important concepts of curl is deï¬ned.
Deï¬nition 29.1.1 Let f : U â†’R3 for U âŠ†R3 denote a vector ï¬eld. The curl of
the vector ï¬eld yields another vector ï¬eld and it is deï¬ned as follows.
(curl (f) (x))i â‰¡(âˆ‡Ã— f (x))i â‰¡Îµijkâˆ‚jfk (x)
where here âˆ‚j means the partial derivative with respect to xj and the subscript of i in
(curl (f) (x))i means the ith Cartesian component of the vector, curl (f) (x) . Thus the curl
is evaluated by expanding the following determinant along the top row.
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
âˆ‚
âˆ‚x
âˆ‚
âˆ‚y
âˆ‚
âˆ‚z
f1 (x, y, z)
f2 (x, y, z)
f3 (x, y, z)
Â¯Â¯Â¯Â¯Â¯Â¯
.
Note the similarity with the cross product. Sometimes the curl is called rot. (Short for
rotation not decay.)
This deï¬nition is in terms of a given coordinate system but later coordinate free deï¬ni-
tions of the curl is presented. For now, everything is deï¬ned in terms of a given Cartesian
coordinate system. The curl has profound physical signiï¬cance and this will be discussed
later. For now it is important to understand how to ï¬nd it. Be sure you understand that
for f a vector ï¬eld, curl f is another vector ï¬eld.
Example 29.1.2 Let f (x) = xyi + (z âˆ’y) j + (sin (x) + z) k. Find curl f.
curl f is obtained by evaluating
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
âˆ‚
âˆ‚x
âˆ‚
âˆ‚y
âˆ‚
âˆ‚z
xy
z âˆ’y
sin (x) + z
Â¯Â¯Â¯Â¯Â¯Â¯
=
i
Âµ âˆ‚
âˆ‚y (sin (x) + z) âˆ’âˆ‚
âˆ‚z (z âˆ’y)
Â¶
âˆ’j
Âµ âˆ‚
âˆ‚x (sin (x) + z) âˆ’âˆ‚
âˆ‚z (xy)
Â¶
+
k
Âµ âˆ‚
âˆ‚x (z âˆ’y) âˆ’âˆ‚
âˆ‚y (xy)
Â¶
= âˆ’i âˆ’cos (x) j âˆ’xk.
527

528
STOKEâ€™S THEOREM 4-5 DEC.
29.2
Greenâ€™s Theorem, A Review
Theorem 29.2.1 (Greenâ€™s Theorem) Let U be an open set in the plane and let âˆ‚U
be piecewise smooth and let F (x, y) = (P (x, y) , Q (x, y)) be a C1 vector ï¬eld deï¬ned near
U. Then it is often1 the case that
Z
âˆ‚U
F Â· dR =
Z
U
Âµâˆ‚Q
âˆ‚x (x, y) âˆ’âˆ‚P
âˆ‚y (x, y)
Â¶
dA.
Proof: Suppose the divergence theorem holds for U. Consider the following picture.
X
X
X
X
y
Â¤
Â¤
Â¤Â¤
(xâ€², yâ€²)
(yâ€², âˆ’xâ€²)
U
Since it is assumed that motion around U is counter clockwise, the tangent vector, (xâ€², yâ€²)
is as shown. Now the unit exterior normal is either
1
q
(xâ€²)2 + (yâ€²)2 (âˆ’yâ€², xâ€²)
or
1
q
(xâ€²)2 + (yâ€²)2 (yâ€², âˆ’xâ€²)
Again, the counter clockwise motion shows the correct unit exterior normal is the second
of the above. To see this note that since the area should be on the left as you walk around
the edge, you need to have the unit normal point in the direction of (xâ€², yâ€², 0) Ã— k which
equals (yâ€², âˆ’xâ€², 0). Now let F (x, y) = (Q (x, y) , âˆ’P (x, y)) . Also note the area element on
âˆ‚U is
q
(xâ€²)2 + (yâ€²)2dt. Suppose the boundary of U consists of m smooth curves, the ith of
which is parameterized by (xi, yi) with the parameter, t âˆˆ[ai, bi] . Then by the divergence
theorem,
Z
U
(Qx âˆ’Py) dA =
Z
U
div (F) dA =
Z
âˆ‚U
F Â· ndS
=
m
X
i=1
Z bi
ai
(Q (xi (t) , yi (t)) , âˆ’P (xi (t) , yi (t))) Â·
1
q
(xâ€²
i)2 + (yâ€²
i)2 (yâ€²
i, âˆ’xâ€²
i)
dS
z
}|
{
q
(xâ€²
i)2 + (yâ€²
i)2dt
=
m
X
i=1
Z bi
ai
(Q (xi (t) , yi (t)) , âˆ’P (xi (t) , yi (t))) Â· (yâ€²
i, âˆ’xâ€²
i) dt
=
m
X
i=1
Z bi
ai
Q (xi (t) , yi (t)) yâ€²
i (t) + P (xi (t) , yi (t)) xâ€²
i (t) dt â‰¡
Z
âˆ‚U
Pdx + Qdy
This proves Greenâ€™s theorem from the divergence theorem.
1For a general version see the advanced calculus book by Apostol.
The general versions involve the
concept of a rectiï¬able Jordan curve.

29.3.
STOKEâ€™S THEOREM FROM GREENâ€™S THEOREM
529
29.3
Stokeâ€™s Theorem From Greenâ€™s Theorem
Stokeâ€™s theorem is a generalization of Greenâ€™s theorem which relates the integral over a
surface to the integral around the boundary of the surface. These terms are a little diï¬€erent
from what occurs in R2. To describe this, consider a sock. The surface is the sock and its
boundary will be the edge of the opening of the sock in which you place your foot. Another
way to think of this is to imagine a region in R2 of the sort discussed above for Greenâ€™s
theorem. Suppose it is on a sheet of rubber and the sheet of rubber is stretched in three
dimensions. The boundary of the resulting surface is the result of the stretching applied to
the boundary of the original region in R2. Here is a picture describing the situation.
âˆ‚S
R
I
S
Recall the following deï¬nition of the curl of a vector ï¬eld.
Deï¬nition 29.3.1 Let
F (x, y, z) = (F1 (x, y, z) , F2 (x, y, z) , F3 (x, y, z))
be a C1 vector ï¬eld deï¬ned on an open set, V in R3. Then
âˆ‡Ã— F â‰¡
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
âˆ‚
âˆ‚x
âˆ‚
âˆ‚y
âˆ‚
âˆ‚z
F1
F2
F3
Â¯Â¯Â¯Â¯Â¯Â¯
â‰¡
Âµâˆ‚F3
âˆ‚y âˆ’âˆ‚F2
âˆ‚z
Â¶
i+
Âµâˆ‚F1
âˆ‚z âˆ’âˆ‚F3
âˆ‚x
Â¶
j+
Âµâˆ‚F2
âˆ‚x âˆ’âˆ‚F1
âˆ‚y
Â¶
k.
This is also called curl (F) and written as indicated, âˆ‡Ã— F.
The following lemma gives the fundamental identity which will be used in the proof of
Stokeâ€™s theorem.
Lemma 29.3.2 Let R : U â†’V âŠ†R3 where U is an open subset of R2 and V is an open
subset of R3. Suppose R is C2 and let F be a C1 vector ï¬eld deï¬ned in V.
(Ru Ã— Rv) Â· (âˆ‡Ã— F) (R (u, v)) = ((F â—¦R)u Â· Rv âˆ’(F â—¦R)v Â· Ru) (u, v) .
(29.1)
Proof: Start with the left side and let xi = Ri (u, v) for short.
(Ru Ã— Rv) Â· (âˆ‡Ã— F) (R (u, v))
=
ÎµijkxjuxkvÎµirs
âˆ‚Fs
âˆ‚xr
=
(Î´jrÎ´ks âˆ’Î´jsÎ´kr) xjuxkv
âˆ‚Fs
âˆ‚xr
=
xjuxkv
âˆ‚Fk
âˆ‚xj
âˆ’xjuxkv
âˆ‚Fj
âˆ‚xk
=
Rv Â· âˆ‚(F â—¦R)
âˆ‚u
âˆ’Ru Â· âˆ‚(F â—¦R)
âˆ‚v

530
STOKEâ€™S THEOREM 4-5 DEC.
which proves 29.1.
For those of you who do not know the permutation symbol and the reduction identities
used in the above, it is possible, but more trouble, to establish the identity by brute force.
Letting x, y, z denote the components of R (u) and f1, f2, f3 denote the components of F,
and letting a subscripted variable denote the partial derivative with respect to that variable,
the left side of 29.1 equals
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
xu
yu
zu
xv
yv
zv
Â¯Â¯Â¯Â¯Â¯Â¯
Â·
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
âˆ‚x
âˆ‚y
âˆ‚z
f1
f2
f3
Â¯Â¯Â¯Â¯Â¯Â¯
= (f3y âˆ’f2z) (yuzv âˆ’zuyv) + (f1z âˆ’f3x) (zuxv âˆ’xuzv) + (f2x âˆ’f1y) (xuyv âˆ’yuxv)
= f3yyuzv + f2zzuyv + f1zzuxv + f3xxuzv + f2xxuyv + f1yyuxv
âˆ’(f2zyuzv + f3yzuyv + f1zxuzv + f3xzuxv + f2xyuxv + f1yxuyv)
= f1yyuxv + f1zzuxv + f2xxuyv + f2zzuyv + f3xxuzv + f3yyuzv
âˆ’(f1yyvxu + f1zzvxu + f2xxvyu + f2zzvyu + f3xxvzu + f3yyvzu)
At this point I become clever and add in and subtract oï¬€certain terms. Then
= f1xxuxv + f1yyuxv + f1zzuxv + f2xxuyv + f2xyuyv
+f2zzuyv + f3xxuzv + f3yyuzv + f3zzuzv
âˆ’
Âµ
f1xxvxu + f1yyvxu + f1zzvxu + f2xxvyu + f2xyvyu
+f2zzvyu + f3xxvzu + f3yyvzu + f3zzvzu
Â¶
= âˆ‚f1 â—¦R (u, v)
âˆ‚u
xv + âˆ‚f2 â—¦R (u, v)
âˆ‚u
yv + âˆ‚f3 â—¦R (u, v)
âˆ‚u
zv
âˆ’
Âµâˆ‚f1 â—¦R (u, v)
âˆ‚v
xu + âˆ‚f2 â—¦R (u, v)
âˆ‚v
yu + âˆ‚f3 â—¦R (u, v)
âˆ‚v
zu
Â¶
= ((F â—¦R)u Â· Rv âˆ’(F â—¦R)v Â· Ru) (u, v) .
This proves the lemma. Not how much trouble this was and how I had to be clever by
adding in and subtracting oï¬€the appropriate terms. With the reduction identities for the
permutation symbol no cleverness at all was required. The desired identity just fell out of
completely routine manipulations. This is a good example which illustrates the utility of
good notation.
The proof of Stokeâ€™s theorem given next follows [7]. First, it is convenient to give a
deï¬nition.
Deï¬nition 29.3.3 A vector valued function, R :U âŠ†Rm â†’Rn is said to be in
Ck Â¡
U, RnÂ¢
if it is the restriction to U of a vector valued function which is deï¬ned on Rm
and is Ck. That is, this function has continuous partial derivatives up to order k.
Theorem 29.3.4 (Stokeâ€™s Theorem) Let U be any region in R2 for which the con-
clusion of Greenâ€™s theorem holds and let R âˆˆC2 Â¡
U, R3Â¢
be a one to one function satisfying
|(Ru Ã— Rv) (u, v)| Ì¸= 0 for all (u, v) âˆˆU and let S denote the surface,
S
â‰¡
{R (u, v) : (u, v) âˆˆU} ,
âˆ‚S
â‰¡
{R (u, v) : (u, v) âˆˆâˆ‚U}
where the orientation on âˆ‚S is consistent with the counter clockwise orientation on âˆ‚U (U
is on the left as you walk around âˆ‚U). Then for F a C1 vector ï¬eld deï¬ned near S,
Z
âˆ‚S
F Â· dR =
Z
S
curl (F) Â· ndS

29.3.
STOKEâ€™S THEOREM FROM GREENâ€™S THEOREM
531
where n is the normal to S deï¬ned by
n â‰¡Ru Ã— Rv
|Ru Ã— Rv|.
Proof: Letting C be an oriented part of âˆ‚U having parametrization, r (t) â‰¡(u (t) , v (t))
for t âˆˆ[Î±, Î²] and letting R (C) denote the oriented part of âˆ‚S corresponding to C,
Z
R(C)
F Â· dR =
=
Z Î²
Î±
F (R (u (t) , v (t))) Â· (Ruuâ€² (t) + Rvvâ€² (t)) dt
=
Z Î²
Î±
F (R (u (t) , v (t))) Ru (u (t) , v (t)) uâ€² (t) dt
+
Z Î²
Î±
F (R (u (t) , v (t))) Rv (u (t) , v (t)) vâ€² (t) dt
=
Z
C
((F â—¦R) Â· Ru, (F â—¦R) Â· Rv) Â· dr.
Since this holds for each such piece of âˆ‚U, it follows
Z
âˆ‚S
F Â· dR =
Z
âˆ‚U
((F â—¦R) Â· Ru, (F â—¦R) Â· Rv) Â· dr.
By the assumption that the conclusion of Greenâ€™s theorem holds for U, this equals
Z
U
[((F â—¦R) Â· Rv)u âˆ’((F â—¦R) Â· Ru)v] dA
=
Z
U
[(F â—¦R)u Â· Rv + (F â—¦R) Â· Rvu âˆ’(F â—¦R) Â· Ruv âˆ’(F â—¦R)v Â· Ru] dA
=
Z
U
[(F â—¦R)u Â· Rv âˆ’(F â—¦R)v Â· Ru] dA
the last step holding by equality of mixed partial derivatives, a result of the assumption
that R is C2. Now by Lemma 29.3.2, this equals
Z
U
(Ru Ã— Rv) Â· (âˆ‡Ã— F) dA
=
Z
U
âˆ‡Ã— FÂ· (Ru Ã— Rv) dA
=
Z
S
âˆ‡Ã— F Â· ndS
because dS = |(Ru Ã— Rv)| dA and n = (RuÃ—Rv)
|(RuÃ—Rv)|. Thus
(Ru Ã— Rv) dA
=
(Ru Ã— Rv)
|(Ru Ã— Rv)| |(Ru Ã— Rv)| dA
=
ndS.
This proves Stokeâ€™s theorem.
Note that there is no mention made in the ï¬nal result that R is C2. Therefore, it is not
surprising that versions of this theorem are valid in which this assumption is not present. It
is possible to obtain extremely general versions of Stokeâ€™s theorem if you use the Lebesgue
integral.

532
STOKEâ€™S THEOREM 4-5 DEC.
29.3.1
Orientation
It turns out there are more general formulations of Stokeâ€™s theorem than what is presented
above. However, it is always necessary for the surface, S to be orientable. This means it
is possible to obtain a vector ï¬eld for a unit normal to the surface which is a continuous
function of position on S. An example of a surface which is not orientable is the famous
Mobeus band, obtained by taking a long rectangular piece of paper and glueing the ends
together after putting a twist in it. Here is a picture of one.
There is something quite interesting about this Mobeus band and this is that it can
be written parametrically with a simple parameter domain. The picture above is a maple
graph of the parametrically deï¬ned surface
R (Î¸, v) â‰¡
ï£±
ï£²
ï£³
x = 4 cos Î¸ + v cos Î¸
2
y = 4 sin Î¸ + v cos Î¸
2
z = v sin Î¸
2
, Î¸ âˆˆ[0, 2Ï€] , v âˆˆ[âˆ’1, 1] .
An obvious question is why the normal vector, R,Î¸ Ã— R,v/ |R,Î¸ Ã— R,v| is not a continuous
function of position on S. You can see easily that it is a continuous function of both Î¸ and
v. However, the map, R is not one to one. In fact, R (0, 0) = R (2Ï€, 0) . Therefore, near
this point on S, there are two diï¬€erent values for the above normal vector. In fact, a short
computation will show this normal vector is
Â¡
4 sin 1
2Î¸ cos Î¸ âˆ’1
2v, 4 sin 1
2Î¸ sin Î¸ + 1
2v, âˆ’8 cos2 1
2Î¸ sin 1
2Î¸ âˆ’8 cos3 1
2Î¸ + 4 cos 1
2Î¸
Â¢
q
16 sin2 Â¡ Î¸
2
Â¢
+ v2
2 + 4 sin
Â¡ Î¸
2
Â¢
v (sin Î¸ âˆ’cos Î¸) +
Â¡
âˆ’8 cos2 1
2Î¸ sin 1
2Î¸ âˆ’8 cos3 1
2Î¸ + 4 cos 1
2Î¸
Â¢2
and you can verify that the denominator will not vanish. Letting v = 0 and Î¸ = 0 and 2Ï€
yields the two vectors,
(0, 0, âˆ’1) , (0, 0, 1)
so there is a discontinuity. This is why I was careful to say in the statement of Stokeâ€™s
theorem given above that R is one to one.
The Mobeus band has some usefulness. In old machine shops the equipment was run by
a belt which was given a twist to spread the surface wear on the belt over twice the area.
The above explanation shows that R,Î¸ Ã— R,v/ |R,Î¸ Ã— R,v| fails to deliver an orientation
for the Mobeus band. However, this does not answer the question whether there is some
orientation for it other than this one. In fact there is none. You can see this by looking at
the ï¬rst of the two pictures below or by making one and tracing it with a pencil. There
is only one side to the Mobeus band. An oriented surface must have two sides, one side
identiï¬ed by the given unit normal which varies continuously over the surface and the other
side identiï¬ed by the negative of this normal. The second picture below was taken by Dr.
Ouyang when he was at meetings in Paris and saw it at a museum.

29.3.
STOKEâ€™S THEOREM FROM GREENâ€™S THEOREM
533
29.3.2
Conservative Vector Fields And Stokeâ€™s Theorem
Recall the following deï¬nition.
Deï¬nition 29.3.5 A vector ï¬eld, F deï¬ned in a three dimensional region is said
to be conservative2 if for every piecewise smooth closed curve, C, it follows
R
C FÂ· dR = 0.
Stokes theorem provides an easy to use criterion for determining whether a given vector
ï¬eld is conservative.
Deï¬nition 29.3.6 A set of points in three dimensional space, V is simply connected
if every piecewise smooth closed curve, C is the edge of a surface, S which is contained
entirely within V in such a way that Stokes theorem holds for the surface, S and its edge,
C.
C
R
I
S
2There is no such thing as a liberal vector ï¬eld.

534
STOKEâ€™S THEOREM 4-5 DEC.
This is like a sock. The surface is the sock and the curve, C goes around the opening of
the sock.
As an application of Stokeâ€™s theorem, here is a useful theorem which gives a way to check
whether a vector ï¬eld is conservative.
Theorem 29.3.7 For a three dimensional simply connected open set, V and F a
C1 vector ï¬eld deï¬ned in V, F is conservative if âˆ‡Ã— F = 0 in V.
Proof: If âˆ‡Ã—F = 0 then taking an arbitrary closed curve, C, and letting S be a surface
bounded by C which is contained in V, Stokeâ€™s theorem implies
0 =
Z
S
âˆ‡Ã— F Â· n dA =
Z
C
FÂ· dR.
Thus F is conservative.
Example 29.3.8 Determine whether the vector ï¬eld,
Â¡
4x3 + 2
Â¡
cos
Â¡
x2 + z2Â¢Â¢
x, 1, 2
Â¡
cos
Â¡
x2 + z2Â¢Â¢
z
Â¢
is conservative.
Since this vector ï¬eld is deï¬ned on all of R3, it only remains to take its curl and see if
it is the zero vector.
Â¯Â¯Â¯Â¯Â¯Â¯
i
j
k
âˆ‚x
âˆ‚y
âˆ‚z
4x3 + 2
Â¡
cos
Â¡
x2 + z2Â¢Â¢
x
1
2
Â¡
cos
Â¡
x2 + z2Â¢Â¢
z
Â¯Â¯Â¯Â¯Â¯Â¯
.
This is obviously equal to zero. Therefore, the given vector ï¬eld is conservative. Can you ï¬nd
a potential function for it? Let Ï† be the potential function. Then Ï†z = 2
Â¡
cos
Â¡
x2 + z2Â¢Â¢
z
and so Ï† (x, y, z) = sin
Â¡
x2 + z2Â¢
+ g (x, y) . Now taking the derivative of Ï† with respect to
y, you see gy = 1 so g (x, y) = y +h (x) . Hence Ï† (x, y, z) = y +g (x)+sin
Â¡
x2 + z2Â¢
. Taking
the derivative with respect to x, you get 4x3 +2
Â¡
cos
Â¡
x2 + z2Â¢Â¢
x = gâ€² (x)+2x cos
Â¡
x2 + z2Â¢
and so it suï¬ƒces to take g (x) = x4. Hence Ï† (x, y, z) = y + x4 + sin
Â¡
x2 + z2Â¢
.
29.3.3
Some Terminology
If F = (P, Q, R) is a vector ï¬eld. Then the statement that F is conservative is the same as
saying the diï¬€erential form Pdx + Qdy + Rdz is exact. Some people like to say things in
terms of vector ï¬elds and some say it in terms of diï¬€erential forms. In Example 29.3.8, the
diï¬€erential form
Â¡
4x3 + 2
Â¡
cos
Â¡
x2 + z2Â¢Â¢
x
Â¢
dx + dy +
Â¡
2
Â¡
cos
Â¡
x2 + z2Â¢Â¢
z
Â¢
dz is exact.
29.3.4
Vector Identitiesâˆ—
There are many interesting identities which relate the gradient, divergence and curl.
Theorem 29.3.9 Assuming f, g are a C2 vector ï¬elds whenever necessary, the
following identities are valid.
1. âˆ‡Â· (âˆ‡Ã— f) = 0
2. âˆ‡Ã— âˆ‡Ï† = 0

29.3.
STOKEâ€™S THEOREM FROM GREENâ€™S THEOREM
535
3. âˆ‡Ã— (âˆ‡Ã— f) = âˆ‡(âˆ‡Â· f) âˆ’âˆ‡2f where âˆ‡2f is a vector ï¬eld whose ith component is
âˆ‡2fi.
4. âˆ‡Â· (f Ã— g) = gÂ· (âˆ‡Ã— f) âˆ’fÂ· (âˆ‡Ã— g)
5. âˆ‡Ã— (f Ã— g) = (âˆ‡Â· g) fâˆ’(âˆ‡Â· f) g+ (gÂ·âˆ‡) fâˆ’(fÂ·âˆ‡) g
Proof: These are all easy to establish if you use the repeated index summation conven-
tion and the reduction identities discussed on Page 60.
âˆ‡Â· (âˆ‡Ã— f)
=
âˆ‚i (âˆ‡Ã— f)i
=
âˆ‚i (Îµijkâˆ‚jfk)
=
Îµijkâˆ‚i (âˆ‚jfk)
=
Îµjikâˆ‚j (âˆ‚ifk)
=
âˆ’Îµijkâˆ‚j (âˆ‚ifk)
=
âˆ’Îµijkâˆ‚i (âˆ‚jfk)
=
âˆ’âˆ‡Â· (âˆ‡Ã— f) .
This establishes the ï¬rst formula. The second formula is done similarly. Now consider the
third.
(âˆ‡Ã— (âˆ‡Ã— f))i
=
Îµijkâˆ‚j (âˆ‡Ã— f)k
=
Îµijkâˆ‚j (Îµkrsâˆ‚rfs)
=
=Îµijk
z}|{
Îµkij Îµkrsâˆ‚j (âˆ‚rfs)
=
(Î´irÎ´js âˆ’Î´isÎ´jr) âˆ‚j (âˆ‚rfs)
=
âˆ‚j (âˆ‚ifj) âˆ’âˆ‚j (âˆ‚jfi)
=
âˆ‚i (âˆ‚jfj) âˆ’âˆ‚j (âˆ‚jfi)
=
Â¡
âˆ‡(âˆ‡Â· f) âˆ’âˆ‡2f
Â¢
i
This establishes the third identity.
Consider the fourth identity.
âˆ‡Â· (f Ã— g)
=
âˆ‚i (f Ã— g)i
=
âˆ‚iÎµijkfjgk
=
Îµijk (âˆ‚ifj) gk + Îµijkfj (âˆ‚igk)
=
(Îµkijâˆ‚ifj) gk âˆ’(Îµjikâˆ‚igk) fk
=
âˆ‡Ã— f Â· g âˆ’âˆ‡Ã— g Â· f.
This proves the fourth identity.
Consider the ï¬fth.
(âˆ‡Ã— (f Ã— g))i
=
Îµijkâˆ‚j (f Ã— g)k
=
Îµijkâˆ‚jÎµkrsfrgs
=
ÎµkijÎµkrsâˆ‚j (frgs)
=
(Î´irÎ´js âˆ’Î´isÎ´jr) âˆ‚j (frgs)
=
âˆ‚j (figj) âˆ’âˆ‚j (fjgi)
=
(âˆ‚jgj) fi + gjâˆ‚jfi âˆ’(âˆ‚jfj) gi âˆ’fj (âˆ‚jgi)
=
((âˆ‡Â· g) f + (g Â· âˆ‡) (f) âˆ’(âˆ‡Â· f) g âˆ’(f Â· âˆ‡) (g))i

536
STOKEâ€™S THEOREM 4-5 DEC.
and this establishes the ï¬fth identity.
I think the important thing about the above is not that these identities can be proved
and are valid as much as the method by which they were proved. The reduction identities on
Page 60 were used to discover the identities. There is a diï¬€erence between proving something
someone tells you about and both discovering what should be proved and proving it. This
notation and the reduction identity make the discovery of vector identities fairly routine
and this is why these things are of great signiï¬cance.
29.3.5
Vector Potentialsâˆ—
One of the above identities says âˆ‡Â· (âˆ‡Ã— f) = 0. Suppose now âˆ‡Â· g = 0. Does it follow that
there exists f such that g =âˆ‡Ã— f ? It turns out that this is usually the case and when such
an f exists, it is called a vector potential. Here is one way to do it, assuming everything
is deï¬ned so the following formulas make sense.
f (x, y, z) =
ÂµZ z
0
g2 (x, y, t) dt, âˆ’
Z z
0
g1 (x, y, t) dt +
Z x
0
g3 (t, y, 0) dt, 0
Â¶T
.
(29.2)
In verifying this you need to use the following manipulation which will generally hold under
reasonable conditions but which has not been carefully shown yet.
âˆ‚
âˆ‚x
Z b
a
h (x, t) dt =
Z b
a
âˆ‚h
âˆ‚x (x, t) dt.
(29.3)
The above formula seems plausible because the integral is a sort of a sum and the derivative
of a sum is the sum of the derivatives. However, this sort of sloppy reasoning will get you
into all sorts of trouble. The formula involves the interchange of two limit operations, the
integral and the limit of a diï¬€erence quotient. Such an interchange can only be accomplished
through a theorem. The following gives the necessary result. This lemma is stated without
proof.
Lemma 29.3.10 Suppose h and âˆ‚h
âˆ‚x are continuous on the rectangle R = [c, d] Ã— [a, b] .
Then 29.3 holds.
29.3.6
Maxwellâ€™s Equations And The Wave Equationâˆ—
Many of the ideas presented above are useful in analyzing Maxwellâ€™s equations.
These
equations are derived in advanced physics courses. They are
âˆ‡Ã— E + 1
c
âˆ‚B
âˆ‚t
=
0
(29.4)
âˆ‡Â· E
=
4Ï€Ï
(29.5)
âˆ‡Ã— B âˆ’1
c
âˆ‚E
âˆ‚t
=
4Ï€
c f
(29.6)
âˆ‡Â· B
=
0
(29.7)
and it is assumed these hold on all of R3 to eliminate technical considerations having to do
with whether something is simply connected.
In these equations, E is the electrostatic ï¬eld and B is the magnetic ï¬eld while Ï and f
are sources. By 29.7 B has a vector potential, A1 such that B = âˆ‡Ã— A1. Now go to 29.4
and write
âˆ‡Ã— E+1
c âˆ‡Ã— âˆ‚A1
âˆ‚t
= 0

29.3.
STOKEâ€™S THEOREM FROM GREENâ€™S THEOREM
537
showing that
âˆ‡Ã—
Âµ
E + 1
c
âˆ‚A1
âˆ‚t
Â¶
= 0
It follows E + 1
c
âˆ‚A1
âˆ‚t
has a scalar potential, Ïˆ1 satisfying
âˆ‡Ïˆ1 = E + 1
c
âˆ‚A1
âˆ‚t .
(29.8)
Now suppose Ï† is a time dependent scalar ï¬eld satisfying
âˆ‡2Ï† âˆ’1
c2
âˆ‚2Ï†
âˆ‚t2 = 1
c
âˆ‚Ïˆ1
âˆ‚t âˆ’âˆ‡Â· A1.
(29.9)
Next deï¬ne
A â‰¡A1 + âˆ‡Ï†, Ïˆ â‰¡Ïˆ1 + 1
c
âˆ‚Ï†
âˆ‚t .
(29.10)
Therefore, in terms of the new variables, 29.9 becomes
âˆ‡2Ï† âˆ’1
c2
âˆ‚2Ï†
âˆ‚t2 = 1
c
Âµâˆ‚Ïˆ
âˆ‚t âˆ’1
c
âˆ‚2Ï†
âˆ‚t2
Â¶
âˆ’âˆ‡Â· A + âˆ‡2Ï†
which yields
0 = âˆ‚Ïˆ
âˆ‚t âˆ’câˆ‡Â· A.
(29.11)
Then it follows from Theorem 29.3.9 on Page 534 that A is also a vector potential for B.
That is
âˆ‡Ã— A = B.
(29.12)
From 29.8
âˆ‡
Âµ
Ïˆ âˆ’1
c
âˆ‚Ï†
âˆ‚t
Â¶
= E + 1
c
Âµâˆ‚A
âˆ‚t âˆ’âˆ‡âˆ‚Ï†
âˆ‚t
Â¶
and so
âˆ‡Ïˆ = E + 1
c
âˆ‚A
âˆ‚t .
(29.13)
Using 29.6 and 29.13,
âˆ‡Ã— (âˆ‡Ã— A) âˆ’1
c
âˆ‚
âˆ‚t
Âµ
âˆ‡Ïˆ âˆ’1
c
âˆ‚A
âˆ‚t
Â¶
= 4Ï€
c f.
(29.14)
Now from Theorem 29.3.9 on Page 534 this implies
âˆ‡(âˆ‡Â· A) âˆ’âˆ‡2A âˆ’âˆ‡
Âµ1
c
âˆ‚Ïˆ
âˆ‚t
Â¶
+ 1
c2
âˆ‚2A
âˆ‚t2 = 4Ï€
c f
and using 29.11, this gives
1
c2
âˆ‚2A
âˆ‚t2 âˆ’âˆ‡2A = 4Ï€
c f.
(29.15)
Also from 29.13, 29.5, and 29.11,
âˆ‡2Ïˆ
=
âˆ‡Â· E+1
c
âˆ‚
âˆ‚t (âˆ‡Â· A)
=
4Ï€Ï + 1
c2
âˆ‚2Ïˆ
âˆ‚t2

538
STOKEâ€™S THEOREM 4-5 DEC.
and so
1
c2
âˆ‚2Ïˆ
âˆ‚t2 âˆ’âˆ‡2Ïˆ = âˆ’4Ï€Ï.
(29.16)
This is very interesting. If a solution to the wave equations, 29.16, and 29.15 can be
found along with a solution to 29.11, then letting the magnetic ï¬eld be given by 29.12 and
letting E be given by 29.13 the result is a solution to Maxwells equations. This is signiï¬cant
because wave equations are easier to think of than Maxwellâ€™s equations. Note the above
argument also showed that it is always possible, by solving another wave equation, to get
29.11 to hold.

Part XIV
Some Iterative Techniques For
Linear Algebra
539


Iterative Methods For Linear
Systems
Consider the problem of solving the equation
Ax = b
(30.1)
where A is an n Ã— n matrix. In many applications, the matrix A is huge and composed
mainly of zeros. For such matrices, the method of Gauss elimination (row operations) is
not a good way to solve the system because the row operations can destroy the zeros and
storing all those zeros takes a lot of room in a computer. These systems are called sparse.
To solve them it is common to use an iterative technique. I am following the treatment
given to this subject by Nobel and Daniel [20].
There are two main methods which are used to obtain solutions iteratively, the Jacobi
method and the Gauss Seidel method. I will illustrate with an example and then describe
the method precisely.
30.1
Jacobi Method
Example 30.1.1 Use the Jacobi method to ï¬nd the solutions to the following system of
equations.
7x + y = 11
x âˆ’5y = 7
It is profoundly stupid to use the Jacobi method on such a 2 Ã— 2 system. You should
simply use row operations. If you do, the solution is
Â©
y = âˆ’19
18, x = 31
18
Âª
. In terms of decimals
this is {y = âˆ’1. 055 555 56, x = 1. 722 222 22} . Now I will proceed to show how to use the
Jacobi method to also ï¬nd this solution.
Here are steps which describe the Jacobi method. You write the system as
Âµ 7
1
1
âˆ’5
Â¶ Âµ x
y
Â¶
=
Âµ 11
7
Â¶
Next you split the matrix as follows
Âµ
7
0
0
âˆ’5
Â¶ Âµ
x
y
Â¶
+
Âµ
0
1
1
0
Â¶ Âµ
x
y
Â¶
=
Âµ
11
7
Â¶
That is you write the matrix as the sum of a diagonal matrix plus the oï¬€diagonal terms.
Then if you have a solution, you would need
Âµ
7
0
0
âˆ’5
Â¶ Âµ
x
y
Â¶
= âˆ’
Âµ
0
1
1
0
Â¶ Âµ
x
y
Â¶
+
Âµ
11
7
Â¶
541

542
ITERATIVE METHODS FOR LINEAR SYSTEMS
You could write this as
Âµ
x
y
Â¶
=
âˆ’
Âµ
7
0
0
âˆ’5
Â¶âˆ’1 Âµ
0
1
1
0
Â¶ Âµ
x
y
Â¶
+
Âµ
7
0
0
âˆ’5
Â¶âˆ’1 Âµ
11
7
Â¶
=
Âµ
0
âˆ’1
7
1
5
0
Â¶ Âµ
x
y
Â¶
+
Âµ
11
7
âˆ’7
5
Â¶
This suggests a way to approach the problem through a process of iterations. You pick an
initial guess for (x, y) say (0, 0) . (It really doesnâ€™t matter what you pick. When the method
works it will do so for any initial choice. ) Call this initial guess
Âµ
x0
y0
Â¶
and then you obtain the next guess,
Âµ
x1
y1
Â¶
as follows
Âµ
x1
y1
Â¶
=
Âµ
0
âˆ’1
7
1
5
0
Â¶ Âµ
x0
y0
Â¶
+
Âµ
11
7
âˆ’7
5
Â¶
Then to get the next guess you do the same thing.
Âµ
x2
y2
Â¶
=
Âµ
0
âˆ’1
7
1
5
0
Â¶ Âµ
x1
y1
Â¶
+
Âµ
11
7
âˆ’7
5
Â¶
Continuing this way, this method hopefully will give guesses which are increasingly close to
the true solution. Lets apply this to this example.
Âµ x1
y1
Â¶
=
Âµ 0
âˆ’1
7
1
5
0
Â¶ Âµ 0
0
Â¶
+
Âµ
11
7
âˆ’7
5
Â¶
=
Âµ
1. 571 428 57
âˆ’1. 4
Â¶
Now you ï¬nd the next guess.
Âµ x2
y2
Â¶
=
Âµ 0
âˆ’1
7
1
5
0
Â¶ Âµ 1. 571 428 57
âˆ’1. 4
Â¶
+
Âµ
11
7
âˆ’7
5
Â¶
=
Âµ
1. 771 428 57
âˆ’1. 085 714 29
Â¶
Things are still changing so I will try the next guess.
Âµ
x3
y3
Â¶
=
Âµ
0
âˆ’1
7
1
5
0
Â¶ Âµ
1. 771 428 57
âˆ’1. 085 714 29
Â¶
+
Âµ
11
7
âˆ’7
5
Â¶
=
Âµ
1. 726 530 61
âˆ’1. 045 714 29
Â¶
Lets do another iteration.
Âµ x4
y4
Â¶
=
Âµ 0
âˆ’1
7
1
5
0
Â¶ Âµ
1. 726 530 61
âˆ’1. 045 714 29
Â¶
+
Âµ
11
7
âˆ’7
5
Â¶
=
Âµ
1. 720 816 33
âˆ’1. 054 693 88
Â¶
.

30.1.
JACOBI METHOD
543
This should be pretty close because the guesses are not changing much from one to the next.
The exact solution was
{y = âˆ’1. 055 555 56, x = 1. 722 222 22}
Actually, you donâ€™t do it this way. The following gives the way a computer would do
it. You do not invert the matrix as I did. However, for the purposes of illustration and for
small systems there is no harm in doing it as I did above, especially since for small systems
of equations it is a stupid idea to use an iterative method in the ï¬rst place.
Deï¬nition 30.1.2 The Jacobi iterative technique, also called the method of simul-
taneous corrections is deï¬ned as follows. Let x1 be an initial vector, say the zero vector or
some other vector. The method generates a succession of vectors, x2, x3, x4, Â·Â·Â· and hopefully
this sequence of vectors will converge to the solution to 30.1. The vectors in this list are
called iterates and they are obtained according to the following procedure. Letting A = (aij) ,
aiixr+1
i
= âˆ’
X
jÌ¸=i
aijxr
j + bi.
(30.2)
In terms of matrices, letting
A =
ï£«
ï£¬
ï£­
âˆ—
Â· Â· Â·
âˆ—
...
...
...
âˆ—
Â· Â· Â·
âˆ—
ï£¶
ï£·
ï£¸
The iterates are deï¬ned as
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
0
Â· Â· Â·
0
0
âˆ—
...
...
...
...
...
0
0
Â· Â· Â·
0
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£­
xr+1
1
xr+1
2...
xr+1
n
ï£¶
ï£·
ï£·
ï£·
ï£¸
=
âˆ’
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
âˆ—
Â· Â· Â·
âˆ—
âˆ—
0
...
...
...
...
...
âˆ—
âˆ—
Â· Â· Â·
âˆ—
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£­
xr
1
xr
2
...
xr
n
ï£¶
ï£·
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£¬
ï£­
b1
b2
...
bn
ï£¶
ï£·
ï£·
ï£·
ï£¸
(30.3)
The matrix on the left in 30.3 is obtained by retaining the main diagonal of A and
setting every other entry equal to zero. The matrix on the right in 30.3 is obtained from A
by setting every diagonal entry equal to zero and retaining all the other entries unchanged.
Example 30.1.3 Use the Jacobi method to solve the system
ï£«
ï£¬
ï£¬
ï£­
3
1
0
0
1
4
1
0
0
2
5
1
0
0
2
4
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
x1
x2
x3
x4
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
1
2
3
4
ï£¶
ï£·
ï£·
ï£¸
Of course this is solved most easily using row reductions. The Jacobi method is useful
when the matrix is 1000Ã—1000 or larger. This example is just to illustrate how the method
works. First lets solve it using row operations. The augmented matrix is
ï£«
ï£¬
ï£¬
ï£­
3
1
0
0
1
1
4
1
0
2
0
2
5
1
3
0
0
2
4
4
ï£¶
ï£·
ï£·
ï£¸

544
ITERATIVE METHODS FOR LINEAR SYSTEMS
The row reduced echelon form is
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
6
29
0
1
0
0
11
29
0
0
1
0
8
29
0
0
0
1
25
29
ï£¶
ï£·
ï£·
ï£¸
which in terms of decimals is approximately equal to
ï£«
ï£¬
ï£¬
ï£­
1.0
0
0
0
. 206
0
1.0
0
0
. 379
0
0
1.0
0
. 275
0
0
0
1.0
. 862
ï£¶
ï£·
ï£·
ï£¸.
In terms of the matrices, the Jacobi iteration is of the form
ï£«
ï£¬
ï£¬
ï£­
3
0
0
0
0
4
0
0
0
0
5
0
0
0
0
4
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
xr+1
1
xr+1
2
xr+1
3
xr+1
4
ï£¶
ï£·
ï£·
ï£¸= âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
xr
1
xr
2
xr
3
xr
4
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
2
3
4
ï£¶
ï£·
ï£·
ï£¸.
Multiplying by the invese of the matrix on the left, 1this iteration reduces to
ï£«
ï£¬
ï£¬
ï£­
xr+1
1
xr+1
2
xr+1
3
xr+1
4
ï£¶
ï£·
ï£·
ï£¸= âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
xr
1
xr
2
xr
3
xr
4
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
31
23
5
1
ï£¶
ï£·
ï£·
ï£¸.
(30.4)
Now iterate this starting with
x1 â‰¡
ï£«
ï£¬
ï£¬
ï£­
0
0
0
0
ï£¶
ï£·
ï£·
ï£¸.
Thus
x2 = âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
0
0
0
0
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
31
23
5
1
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
1
31
23
5
1
ï£¶
ï£·
ï£·
ï£¸
Then
x3 = âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0
ï£¶
ï£·
ï£·
ï£¸
x2
z }| {
ï£«
ï£¬
ï£¬
ï£­
1
31
23
5
1
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
31
23
5
1
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
. 166
. 26
. 2
. 7
ï£¶
ï£·
ï£·
ï£¸
x4 = âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0
ï£¶
ï£·
ï£·
ï£¸
x3
z
}|
{
ï£«
ï£¬
ï£¬
ï£­
. 166
. 26
. 2
. 7
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
31
23
5
1
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
. 24
. 408 5
. 356
. 9
ï£¶
ï£·
ï£·
ï£¸
1You certainly would not compute the invese in solving a large system. This is just to show you how the
method works for this simple example. You would use the ï¬rst description in terms of indices.

30.2.
GAUSS SEIDEL METHOD
545
x5 = âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0
ï£¶
ï£·
ï£·
ï£¸
x4
z
}|
{
ï£«
ï£¬
ï£¬
ï£­
. 24
. 408 5
. 356
. 9
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
31
23
5
1
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
. 197
. 351
. 256 6
. 822
ï£¶
ï£·
ï£·
ï£¸
x6 = âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
1
3
0
0
1
4
0
1
4
0
0
2
5
0
1
5
0
0
1
2
0
ï£¶
ï£·
ï£·
ï£¸
x5
z
}|
{
ï£«
ï£¬
ï£¬
ï£­
. 197
. 351
. 256 6
. 822
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
31
23
5
1
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
. 216
. 386
. 295
. 871
ï£¶
ï£·
ï£·
ï£¸.
You can keep going like this. Recall the solution is approximately equal to
ï£«
ï£¬
ï£¬
ï£­
. 206
. 379
. 275
. 862
ï£¶
ï£·
ï£·
ï£¸
so you see that with no care at all and only 6 iterations, an approximate solution has been
obtained which is not too far oï¬€from the actual solution.
It is important to realize that a computer would use 30.2 directly. Indeed, writing the
problem in terms of matrices as I have done above destroys every beneï¬t of the method.
However, it makes it a little easier to see what is happening and so this is why I have
presented it in this way.
30.2
Gauss Seidel Method
Example 30.2.1 Solve the following system of equations using the Gauss Seidel method.
It is the same example as in Example 30.1.1.
7x + y = 11
x âˆ’5y = 7
The solution to this system is is : {y = âˆ’1. 055 555 56, x = 1. 722 222 22} . Now I will use
the Gauss Seidel method to get this solution. The system is of the form
Âµ
7
0
1
âˆ’5
Â¶ Âµ
x
y
Â¶
+
Âµ
0
1
0
0
Â¶ Âµ
x
y
Â¶
=
Âµ
11
7
Â¶
Note the diï¬€erence! Here you split the matrix diï¬€erently. Then the iteration scheme is just
as before,
Âµ 7
0
1
âˆ’5
Â¶ Âµ x
y
Â¶
= âˆ’
Âµ 0
1
0
0
Â¶ Âµ x
y
Â¶
+
Âµ 11
7
Â¶
and so the solution satisï¬es
Âµ
x
y
Â¶
=
âˆ’
Âµ
7
0
1
âˆ’5
Â¶âˆ’1 Âµ
0
1
0
0
Â¶ Âµ
x
y
Â¶
+
Âµ
7
0
1
âˆ’5
Â¶âˆ’1 Âµ
11
7
Â¶
=
Âµ
0
âˆ’1
7
0
âˆ’1
35
Â¶ Âµ
x
y
Â¶
+
Âµ
11
7
âˆ’38
35
Â¶
The corresponding iteration scheme yields
Âµ
xn+1
yn+1
Â¶
=
Âµ
0
âˆ’1
7
0
âˆ’1
35
Â¶ Âµ
xn
yn
Â¶
+
Âµ
11
7
âˆ’38
35
Â¶

546
ITERATIVE METHODS FOR LINEAR SYSTEMS
Starting with an initial guess of x0 = y0 = 0,consider the following iterations.
Âµ x1
y1
Â¶
=
Âµ 0
âˆ’1
7
0
âˆ’1
35
Â¶ Âµ 0
0
Â¶
+
Âµ
11
7
âˆ’38
35
Â¶
=
Âµ
1. 571 428 57
âˆ’1. 085 714 29
Â¶
Âµ
x2
y2
Â¶
=
Âµ
0
âˆ’1
7
0
âˆ’1
35
Â¶ Âµ
1. 571 428 57
âˆ’1. 085 714 29
Â¶
+
Âµ
11
7
âˆ’38
35
Â¶
=
Âµ
1. 726 530 61
âˆ’1. 054 693 88
Â¶
Âµ
x3
y3
Â¶
=
Âµ
0
âˆ’1
7
0
âˆ’1
35
Â¶ Âµ
1. 726 530 61
âˆ’1. 054 693 88
Â¶
+
Âµ
11
7
âˆ’38
35
Â¶
=
Âµ
1. 722 099 13
âˆ’1. 055 580 17
Â¶
These guesses are pretty close so it seems this should be close. Note the exact solution
is {y = âˆ’1. 055 555 56, x = 1. 722 222 22} . I think you can see this method worked a little
better than the Jacobi method although both are pretty good.
The following is the precise description of the method. As before, you donâ€™t write out
the matrices and invert that matrix like above.
Deï¬nition 30.2.2 The Gauss Seidel method, also called the method of successive
corrections is given as follows. For A = (aij) , the iterates for the problem Ax = b are
obtained according to the formula
i
X
j=1
aijxr+1
j
= âˆ’
n
X
j=i+1
aijxr
j + bi.
(30.5)
In terms of matrices, letting
A =
ï£«
ï£¬
ï£­
âˆ—
Â· Â· Â·
âˆ—
...
...
...
âˆ—
Â· Â· Â·
âˆ—
ï£¶
ï£·
ï£¸
The iterates are deï¬ned as
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ—
0
Â· Â· Â·
0
âˆ—
âˆ—
...
...
...
...
...
0
âˆ—
Â· Â· Â·
âˆ—
âˆ—
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£­
xr+1
1
xr+1
2...
xr+1
n
ï£¶
ï£·
ï£·
ï£·
ï£¸
=
âˆ’
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
âˆ—
Â· Â· Â·
âˆ—
0
0
...
...
...
...
...
âˆ—
0
Â· Â· Â·
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£¬
ï£­
xr
1
xr
2
...
xr
n
ï£¶
ï£·
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£¬
ï£­
b1
b2
...
bn
ï£¶
ï£·
ï£·
ï£·
ï£¸
(30.6)

30.2.
GAUSS SEIDEL METHOD
547
In words, you set every entry in the original matrix which is strictly above the main
diagonal equal to zero to obtain the matrix on the left. To get the matrix on the right,
you set every entry of A which is on or below the main diagonal equal to zero. Using the
iteration procedure of 30.5 directly, the Gauss Seidel method makes use of the very latest
information which is available at that stage of the computation.
The following example is the same as the example used to illustrate the Jacobi method.
Example 30.2.3 Use the Gauss Seidel method to solve the system
ï£«
ï£¬
ï£¬
ï£­
3
1
0
0
1
4
1
0
0
2
5
1
0
0
2
4
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
x1
x2
x3
x4
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
1
2
3
4
ï£¶
ï£·
ï£·
ï£¸
In terms of matrices, this procedure is
ï£«
ï£¬
ï£¬
ï£­
3
0
0
0
1
4
0
0
0
2
5
0
0
0
2
4
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
xr+1
1
xr+1
2
xr+1
3
xr+1
4
ï£¶
ï£·
ï£·
ï£¸= âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
xr
1
xr
2
xr
3
xr
4
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
2
3
4
ï£¶
ï£·
ï£·
ï£¸.
Multiplying by the inverse of the matrix on the left2 this yields
ï£«
ï£¬
ï£¬
ï£­
xr+1
1
xr+1
2
xr+1
3
xr+1
4
ï£¶
ï£·
ï£·
ï£¸= âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
1
3
0
0
0
âˆ’1
12
1
4
0
0
1
30
âˆ’1
10
1
5
0
âˆ’1
60
1
20
âˆ’1
10
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
xr
1
xr
2
xr
3
xr
4
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
35
12
13
30
47
60
ï£¶
ï£·
ï£·
ï£¸
As before, I will be totally unoriginal in the choice of x1. Let it equal the zero vector.
Therefore,
x2 =
ï£«
ï£¬
ï£¬
ï£­
1
35
12
13
30
47
60
ï£¶
ï£·
ï£·
ï£¸.
Now
x3 = âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
1
3
0
0
0
âˆ’1
12
1
4
0
0
1
30
âˆ’1
10
1
5
0
âˆ’1
60
1
20
âˆ’1
10
ï£¶
ï£·
ï£·
ï£¸
x2
z
}|
{
ï£«
ï£¬
ï£¬
ï£­
1
35
12
13
30
47
60
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
35
12
13
30
47
60
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
. 194
. 343
. 306
. 846
ï£¶
ï£·
ï£·
ï£¸.
It follows
x4 = âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
1
3
0
0
0
âˆ’1
12
1
4
0
0
1
30
âˆ’1
10
1
5
0
âˆ’1
60
1
20
âˆ’1
10
ï£¶
ï£·
ï£·
ï£¸
x3
z
}|
{
ï£«
ï£¬
ï£¬
ï£­
1
35
12
13
30
47
60
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
35
12
13
30
47
60
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
. 194
. 343
. 306
. 846
ï£¶
ï£·
ï£·
ï£¸
2As in the case of the Jacobi iteration, the computer would not do this.
It would use the iteration
procedure in terms of the entries of the matrix directly. Otherwise all beneï¬t to using this method is lost.

548
ITERATIVE METHODS FOR LINEAR SYSTEMS
and so
x5 = âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
1
3
0
0
0
âˆ’1
12
1
4
0
0
1
30
âˆ’1
10
1
5
0
âˆ’1
60
1
20
âˆ’1
10
ï£¶
ï£·
ï£·
ï£¸
x4
z
}|
{
ï£«
ï£¬
ï£¬
ï£­
. 194
. 343
. 306
. 846
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
35
12
13
30
47
60
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
. 219
. 368 75
. 283 3
. 858 35
ï£¶
ï£·
ï£·
ï£¸.
Recall the answer is
ï£«
ï£¬
ï£¬
ï£­
. 206
. 379
. 275
. 862
ï£¶
ï£·
ï£·
ï£¸
so the iterates are already pretty close to the answer. You could continue doing these iterates
and it appears they converge to the solution. Now consider the following example.
Example 30.2.4 Use the Gauss Seidel method to solve the system
ï£«
ï£¬
ï£¬
ï£­
1
4
0
0
1
4
1
0
0
2
5
1
0
0
2
4
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
x1
x2
x3
x4
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
1
2
3
4
ï£¶
ï£·
ï£·
ï£¸
The exact solution is given by doing row operations on the augmented matrix. When
this is done the row echelon form is
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
6
0
1
0
0
âˆ’5
4
0
0
1
0
1
0
0
0
1
1
2
ï£¶
ï£·
ï£·
ï£¸
and so the solution is approximately
ï£«
ï£¬
ï£¬
ï£­
6
âˆ’5
4
1
1
2
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
6.0
âˆ’1. 25
1.0
. 5
ï£¶
ï£·
ï£·
ï£¸
The Gauss Seidel iterations are of the form
ï£«
ï£¬
ï£¬
ï£­
1
0
0
0
1
4
0
0
0
2
5
0
0
0
2
4
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
xr+1
1
xr+1
2
xr+1
3
xr+1
4
ï£¶
ï£·
ï£·
ï£¸= âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
4
0
0
0
0
1
0
0
0
0
1
0
0
0
0
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
xr
1
xr
2
xr
3
xr
4
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
2
3
4
ï£¶
ï£·
ï£·
ï£¸
and so, multiplying by the inverse of the matrix on the left, the iteration reduces to the
following in terms of matrix multiplication.
xr+1 = âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
4
0
0
0
âˆ’1
1
4
0
0
2
5
âˆ’1
10
1
5
0
âˆ’1
5
1
20
âˆ’1
10
ï£¶
ï£·
ï£·
ï£¸xr +
ï£«
ï£¬
ï£¬
ï£­
1
1
41
23
4
ï£¶
ï£·
ï£·
ï£¸.

30.2.
GAUSS SEIDEL METHOD
549
This time, I will pick an initial vector close to the answer. Let
x1 =
ï£«
ï£¬
ï£¬
ï£­
6
âˆ’1
1
1
2
ï£¶
ï£·
ï£·
ï£¸
This is very close to the answer. Now lets see what the Gauss Seidel iteration does to it.
x2 = âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
4
0
0
0
âˆ’1
1
4
0
0
2
5
âˆ’1
10
1
5
0
âˆ’1
5
1
20
âˆ’1
10
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
6
âˆ’1
1
1
2
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
1
41
23
4
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
5.0
âˆ’1.0
. 9
. 55
ï£¶
ï£·
ï£·
ï£¸
You canâ€™t expect to be real close after only one iteration. Lets do another.
x3 = âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
4
0
0
0
âˆ’1
1
4
0
0
2
5
âˆ’1
10
1
5
0
âˆ’1
5
1
20
âˆ’1
10
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
5.0
âˆ’1.0
. 9
. 55
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
1
41
23
4
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
5.0
âˆ’. 975
. 88
. 56
ï£¶
ï£·
ï£·
ï£¸
x4 = âˆ’
ï£«
ï£¬
ï£¬
ï£­
0
4
0
0
0
âˆ’1
1
4
0
0
2
5
âˆ’1
10
1
5
0
âˆ’1
5
1
20
âˆ’1
10
ï£¶
ï£·
ï£·
ï£¸
ï£«
ï£¬
ï£¬
ï£­
5.0
âˆ’. 975
. 88
. 56
ï£¶
ï£·
ï£·
ï£¸+
ï£«
ï£¬
ï£¬
ï£­
1
1
41
23
4
ï£¶
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£­
4. 9
âˆ’. 945
. 866
. 567
ï£¶
ï£·
ï£·
ï£¸
The iterates seem to be getting farther from the actual solution. Why is the process which
worked so well in the other examples not working here? A better question might be: Why
does either process ever work at all?.
Both iterative procedures for solving
Ax = b
(30.7)
are of the form
Bxr+1 = âˆ’Cxr + b
where A = B + C. In the Jacobi procedure, the matrix C was obtained by setting the
diagonal of A equal to zero and leaving all other entries the same while the matrix, B
was obtained by making every entry of A equal to zero other than the diagonal entries
which are left unchanged. In the Gauss Seidel procedure, the matrix B was obtained from
A by making every entry strictly above the main diagonal equal to zero and leaving the
others unchanged and C was obtained from A by making every entry on or below the main
diagonal equal to zero and leaving the others unchanged. Thus in the Jacobi procedure,
B is a diagonal matrix while in the Gauss Seidel procedure, B is lower triangular. Using
matrices to explicitly solve for the iterates, yields
xr+1 = âˆ’Bâˆ’1Cxr + Bâˆ’1b.
(30.8)
This is what you would never have the computer do but this is what will allow the statement
of a theorem which gives the condition for convergence of these and all other similar methods.
Let {Î»1, Â· Â· Â·, Î»n} be the eigenvalues.
Deï¬nition 30.2.5 The spectral radius of a matrix, M, denoted as Ï (M) is
max {|Î»1| , Â· Â· Â·, |Î»n|} .
That is it is the maximum of the absolute values of the eigenvalues of M.

550
ITERATIVE METHODS FOR LINEAR SYSTEMS
The following gives the condition under which any of these iterates as in 30.8 converge.
Theorem 30.2.6 Suppose Ï
Â¡
Bâˆ’1C
Â¢
< 1. Then the iterates in 30.8 converge to
the unique solution of 30.7.
The following deï¬nition is useful.
Deï¬nition 30.2.7 Suppose A is an n Ã— n matrix. Then A is said to be strictly
diagonally dominant if for every i,
|Aii| >
X
jÂ±i
|Aij| .
That is, the absolute value of the entry in the iith position is larger than the sum of the
absolute values of all the other entries on the ith row.
Theorem 30.2.8 In either the Jacobi or the Gauss Seidel methods, if the matrix
of coeï¬ƒcients which gets split to yield an iteration technique is strictly diagonally dominant,
then the method converges. This means the iterates get close to the solution to the original
system of equations as the iteration progresses.

Iterative Methods For Finding
Eigenvalues
Quiz
1. Let F = (x, y, zx) and let S be the surface having parameterization r (u, v) = (uv, u + v, v)
for (u, v) âˆˆ[0, 1] Ã— [0, 2] . Find the ï¬‚ux integral,
Z
S
F Â· ndS
where n is the unit normal to the surface which has the same direction as the para-
metric normal, N (u, v) = ru Ã— rv.
2. Find the ï¬‚ux integral,
Z
S
F Â· ndS
of F = âˆ‡Ã— G where G (x, y, z) =
Â³
sin
Â¡
x2yz
Â¢
, ln
Â¡
x4 + 7z2 + 1
Â¢
, ex2+y2z5 sin (z)
Â´
on
the level surface, S given by the ellipsoid x2/6 + y2/7 + z2/2 = 1.
3. Let C be the oriented curve consisting of directed line segments which go from (0, 0, 0)
to (3, 2, 1) to (1, 2, 3) to (33.5, 45.7, 67.23) and then to (1, 1, 1) . Find the line integral,
Z
C
(2xy + 1) dx +
Â¡
x2 + 1
Â¢
dy + 2zdz.
4. Find the Laplacian of x3 âˆ’3xy2.
5. Find the circulation density (curl) of the vector ï¬eld
Â¡
x2y, yz, z + x
Â¢
.
31.1
The Power Method For Eigenvalues
As indicated earlier, the eigenvalue eigenvector problem is extremely diï¬ƒcult. Consider for
example what happens if you cannot ï¬nd the eigenvalues exactly. Then you canâ€™t ï¬nd an
eigenvector because there isnâ€™t one due to the fact that A âˆ’Î»I is invertible whenever Î»
is not exactly equal to an eigenvalue. Therefore the straightforward way of solving this
problem fails right away, even if you can approximate the eigenvalues. The power method
allows you to approximate the largest eigenvalue and also the eigenvector which goes with
it.
By considering the inverse of the matrix, you can also ï¬nd the smallest eigenvalue.
551

552
ITERATIVE METHODS FOR FINDING EIGENVALUES
The method works in the situation of a nondefective matrix, A which has an eigenvalue of
algebraic multiplicity 1, Î»n which has the property that |Î»k| < |Î»n| for all k Ì¸= n. Note that
for a real matrix this excludes the case that Î»n could be complex. Why? Such an eigenvalue
is called a dominant eigenvalue.
Let {x1, Â· Â· Â·, xn} be a basis of eigenvectors for Fn such that Axn = Î»nxn. Now let u1 be
some nonzero vector. Since {x1, Â· Â· Â·, xn} is a basis, there exists unique scalars, ci such that
u1 =
n
X
k=1
ckxk.
Assume you have not been so unlucky as to pick u1 in such a way that cn = 0. Then let
Auk = uk+1 so that
um = Amu1 =
nâˆ’1
X
k=1
ckÎ»m
k xk + Î»m
n cnxn.
(31.1)
For large m the last term, Î»m
n cnxn, determines quite well the direction of the vector on the
right. This is because |Î»n| is larger than |Î»k| and so for a large m, the sum, Pnâˆ’1
k=1 ckÎ»m
k xk,
on the right is fairly insigniï¬cant. Therefore, for large m, um is essentially a multiple of the
eigenvector, xn, the one which goes with Î»n. The only problem is that there is no control
of the size of the vectors um. You can ï¬x this by scaling. Let S2 denote the entry of Au1
which is largest in absolute value. We call this a scaling factor. Then u2 will not be just
Au1 but Au1/S2. Next let S3 denote the entry of Au2 which has largest absolute value and
deï¬ne u3 â‰¡Au2/S3. Continue this way. The scaling just described does not destroy the
relative insigniï¬cance of the term involving a sum in 31.1. Indeed it amounts to nothing
more than changing the units of length. Also note that from this scaling procedure, the
absolute value of the largest element of uk is always equal to 1. Therefore, for large m,
um =
Î»m
n cnxn
S2S3 Â· Â· Â· Sm
+ (relatively insigniï¬cant term) .
Therefore, the entry of Aum which has the largest absolute value is essentially equal to the
entry having largest absolute value of
A
Âµ
Î»m
n cnxn
S2S3 Â· Â· Â· Sm
Â¶
= Î»m+1
n
cnxn
S2S3 Â· Â· Â· Sm
â‰ˆÎ»num
and so for large m, it must be the case that Î»n â‰ˆSm+1. This suggests the following
procedure.
Finding the largest eigenvalue with its eigenvector.
1. Start with a vector, u1 which you hope has a component in the direction of xn. The
vector, (1, Â· Â· Â·, 1)T is usually a pretty good choice.
2. If uk is known,
uk+1 = Auk
Sk+1
where Sk+1 is the entry of Auk which has largest absolute value.
3. When the scaling factors, Sk are not changing much, Sk+1 will be close to the eigen-
value and uk+1 will be close to an eigenvector.
4. Check your answer to see if it worked well.

31.1.
THE POWER METHOD FOR EIGENVALUES
553
Example 31.1.1 Find the largest eigenvalue of A =
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸.
The power method will now be applied to ï¬nd the largest eigenvalue for the above matrix.
Letting u1= (1, Â· Â· Â·, 1)T , we will consider Au1 and scale it.
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
1
1
1
ï£¶
ï£¸=
ï£«
ï£­
2
âˆ’4
6
ï£¶
ï£¸.
Scaling this vector by dividing by the largest entry gives
1
6
ï£«
ï£­
2
âˆ’4
6
ï£¶
ï£¸=
ï£«
ï£¬
ï£­
1
3
âˆ’2
3
1
ï£¶
ï£·
ï£¸= u2
Now lets do it again.
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£¬
ï£­
1
3
âˆ’2
3
1
ï£¶
ï£·
ï£¸=
ï£«
ï£­
22
âˆ’8
âˆ’6
ï£¶
ï£¸
Then
u3 = 1
22
ï£«
ï£­
22
âˆ’8
âˆ’6
ï£¶
ï£¸=
ï£«
ï£¬
ï£­
1
âˆ’4
11
âˆ’3
11
ï£¶
ï£·
ï£¸=
ï£«
ï£­
1.0
âˆ’. 363 636 36
âˆ’. 272 727 27
ï£¶
ï£¸.
Continue doing this
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
1.0
âˆ’. 363 636 36
âˆ’. 272 727 27
ï£¶
ï£¸=
ï£«
ï£­
7. 090 909 1
âˆ’4. 363 636 4
1. 636 363 7
ï£¶
ï£¸
Then
u4 =
ï£«
ï£­
1. 0
âˆ’. 615 38
. 230 77
ï£¶
ï£¸
So far the scaling factors are changing fairly noticeably so continue.
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
1. 0
âˆ’. 615 38
. 230 77
ï£¶
ï£¸=
ï£«
ï£­
16. 154
âˆ’7. 384 6
âˆ’1. 384 6
ï£¶
ï£¸
u5 =
ï£«
ï£­
1.0
âˆ’. 457 14
âˆ’8. 571 3 Ã— 10âˆ’2
ï£¶
ï£¸
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
1.0
âˆ’. 457 14
âˆ’8. 571 3 Ã— 10âˆ’2
ï£¶
ï£¸=
ï£«
ï£­
10. 457
âˆ’5. 485 7
. 514 3
ï£¶
ï£¸

554
ITERATIVE METHODS FOR FINDING EIGENVALUES
u6 =
ï£«
ï£­
1.0
âˆ’. 524 6
4. 918 2 Ã— 10âˆ’2
ï£¶
ï£¸
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
1.0
âˆ’. 524 6
4. 918 2 Ã— 10âˆ’2
ï£¶
ï£¸=
ï£«
ï£­
12. 885
âˆ’6. 295 1
âˆ’. 295 15
ï£¶
ï£¸
u7 =
ï£«
ï£­
1.0
âˆ’. 488 56
âˆ’2. 290 6 Ã— 10âˆ’2
ï£¶
ï£¸
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
1.0
âˆ’. 488 56
âˆ’2. 290 6 Ã— 10âˆ’2
ï£¶
ï£¸=
ï£«
ï£­
11. 588
âˆ’5. 862 6
. 137 36
ï£¶
ï£¸
u8 =
ï£«
ï£­
1.0
âˆ’. 505 92
1. 185 4 Ã— 10âˆ’2
ï£¶
ï£¸
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
1.0
âˆ’. 505 92
1. 185 4 Ã— 10âˆ’2
ï£¶
ï£¸=
ï£«
ï£­
12. 213
âˆ’6. 071 1
âˆ’7. 108 2 Ã— 10âˆ’2
ï£¶
ï£¸
u9 =
ï£«
ï£­
1.0
âˆ’. 497 1
âˆ’5. 820 2 Ã— 10âˆ’3
ï£¶
ï£¸
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
1.0
âˆ’. 497 1
âˆ’5. 820 2 Ã— 10âˆ’3
ï£¶
ï£¸=
ï£«
ï£­
11. 895
âˆ’5. 965 1
3. 486 1 Ã— 10âˆ’2
ï£¶
ï£¸
u10 =
ï£«
ï£­
1.0
âˆ’. 501 48
2. 930 7 Ã— 10âˆ’3
ï£¶
ï£¸
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
1.0
âˆ’. 501 48
2. 930 7 Ã— 10âˆ’3
ï£¶
ï£¸=
ï£«
ï£­
12. 053
âˆ’6. 017 6
âˆ’1. 767 2 Ã— 10âˆ’2
ï£¶
ï£¸
u11 =
ï£«
ï£­
1.0
âˆ’. 499 26
âˆ’1. 466 2 Ã— 10âˆ’3
ï£¶
ï£¸
At this point, you could stop because the scaling factors are not changing by much.
They went from 11. 895 to 12. 053. It looks like the eigenvalue is something like 12 which is
in fact the case. The eigenvector is approximately u11. The true eigenvector for Î» = 12 is
ï£«
ï£­
1
âˆ’.5
0
ï£¶
ï£¸
and so you see this is pretty close. If you didnâ€™t know this, observe
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
1.0
âˆ’. 499 26
âˆ’1. 466 2 Ã— 10âˆ’3
ï£¶
ï£¸=
ï£«
ï£­
11. 974
âˆ’5. 991 2
8. 838 6 Ã— 10âˆ’3
ï£¶
ï£¸
(31.2)

31.1.
THE POWER METHOD FOR EIGENVALUES
555
and
12. 053
ï£«
ï£­
1.0
âˆ’. 499 26
âˆ’1. 466 2 Ã— 10âˆ’3
ï£¶
ï£¸=
ï£«
ï£­
12. 053
âˆ’6. 017 6
âˆ’1. 767 2 Ã— 10âˆ’2
ï£¶
ï£¸.
(31.3)
31.1.1
Rayleigh Quotient
In the above procedure, you can sometimes estimate the eigenvalue a little diï¬€erently. If
Ax = Î»x then
Î» = Ax Â· x
|x|2
and so, in the method above, you might get an estimate for the eigenvalue in this way. The
above is called the Rayleigh quotient. In 31.2 where an approximate eigenvector has been
found, you could estimate the eigenvalue as
ï£«
ï£­
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
1.0
âˆ’. 499 26
âˆ’1. 466 2 Ã— 10âˆ’3
ï£¶
ï£¸
ï£¶
ï£¸Â·
ï£«
ï£­
1.0
âˆ’. 499 26
âˆ’1. 466 2 Ã— 10âˆ’3
ï£¶
ï£¸
1 + (âˆ’. 499 26)2 + (âˆ’1. 466 2 Ã— 10âˆ’3)2
=
11. 978 788
The scaling factor was 12. 053 and the Rayleigh quotient gave 11. 978 788. I guess that at
least in this case the scaling factor wins. Lets look at a symmetric matrix. The book says
the convergence of the Rayleigh quotients is about twice as fast as the scaling factors for
symmetric matrices.
Example 31.1.2 Use the Rayleigh quotient with the power method to estimate the dominant
eigenvalue for the matrix,
ï£«
ï£­
2
0
1
0
3
0
1
0
5
ï£¶
ï£¸
It turns out that the eigenvalues of this matrix are 3, 7
2 + 1
2
âˆš
13, 7
2 âˆ’1
2
âˆš
13. In terms of
decimals, 3, 5. 302 775 64, 1. 697 224 36, and so the dominant eigenvalue is 5. 302 775 64.
Use the power method with an initial approximation (1, 1, 1)T . Thus
ï£«
ï£­
2
0
1
0
3
0
1
0
5
ï£¶
ï£¸
ï£«
ï£­
1
1
1
ï£¶
ï£¸=
ï£«
ï£­
3.0
3.0
6.0
ï£¶
ï£¸
and so
u1 = 1
6
ï£«
ï£­
3.0
3.0
6.0
ï£¶
ï£¸=
ï£«
ï£­
. 5
. 5
1.0
ï£¶
ï£¸
Next iteration,
ï£«
ï£­
2
0
1
0
3
0
1
0
5
ï£¶
ï£¸
ï£«
ï£­
. 5
. 5
1.0
ï£¶
ï£¸=
ï£«
ï£­
2.0
1. 5
5. 5
ï£¶
ï£¸
Then
u2 = 1
5.5
ï£«
ï£­
2.0
1. 5
5. 5
ï£¶
ï£¸=
ï£«
ï£­
. 363 636 364
. 272 727 273
1.0
ï£¶
ï£¸

556
ITERATIVE METHODS FOR FINDING EIGENVALUES
The scaling factor, 5.5 is an approximation to the dominant eigenvalue, 5. 302 775 64. Lets
see what is obtained from the Rayleigh quotient.
ï£«
ï£­
ï£«
ï£­
2
0
1
0
3
0
1
0
5
ï£¶
ï£¸
ï£«
ï£­
. 363 636 364
. 272 727 273
1.0
ï£¶
ï£¸
ï£¶
ï£¸Â·
ï£«
ï£­
. 363 636 364
. 272 727 273
1.0
ï£¶
ï£¸
(. 363 636 364)2 + (. 272 727 273)2 + 1
= 5. 150 684 93
This is slightly better than the scaling factor, 5.5. Lets do another iteration. Lets see if we
get a dramatic increase in accuracy.
ï£«
ï£­
2
0
1
0
3
0
1
0
5
ï£¶
ï£¸
ï£«
ï£­
. 363 636 364
. 272 727 273
1.0
ï£¶
ï£¸=
ï£«
ï£­
1. 727 272 73
. 818 181 819
5. 363 636 36
ï£¶
ï£¸
Now
u3 =
1
5. 363 636 36
ï£«
ï£­
1. 727 272 73
. 818 181 819
5. 363 636 36
ï£¶
ï£¸=
ï£«
ï£­
. 322 033 899
. 152 542 373
1. 0
ï£¶
ï£¸
The scaling factor, 5. 363 636 36 is an approximation to the dominant eigenvalue, 5. 302 775 64.
Lets try the Rayleigh quotient again. This gives
ï£«
ï£­
ï£«
ï£­
2
0
1
0
3
0
1
0
5
ï£¶
ï£¸
ï£«
ï£­
. 322 033 899
. 152 542 373
1. 0
ï£¶
ï£¸
ï£¶
ï£¸Â·
ï£«
ï£­
. 322 033 899
. 152 542 373
1. 0
ï£¶
ï£¸
(. 322 033 899)2 + (. 152 542 373)2 + 1
=
5. 254 142 24
The Rayleigh quotient is still just a little bit closer.
31.2
The Shifted Inverse Power Method
This method can ï¬nd various eigenvalues and eigenvectors. It is a signiï¬cant generalization
of the above simple procedure and yields very good results. The situation is this: You have
a number, Î± which is close to Î», some eigenvalue of an n Ã— n matrix, A. You donâ€™t know
Î» but you know that Î± is closer to Î» than to any other eigenvalue. Your problem is to ï¬nd
both Î» and an eigenvector which goes with Î». Another way to look at this is to start with
Î± and seek the eigenvalue, Î», which is closest to Î± along with an eigenvector associated
with Î». If Î± is an eigenvalue of A, then you have what you want. Therefore, we will always
assume Î± is not an eigenvalue of A and so (A âˆ’Î±I)âˆ’1 exists. The method is based on the
following lemma. When using this method it is nice to choose Î± fairly close to an eigenvalue.
Otherwise, the method will converge slowly. In order to get some idea where to start, you
could use Gerschgorinâ€™s theorem but this theorem will only give a rough idea where to look.
There isnâ€™t a really good way to know how to choose Î± for general cases. As we mentioned
earlier, the eigenvalue problem is very diï¬ƒcult to solve in general.
Lemma 31.2.1 Let {Î»k}n
k=1 be the eigenvalues of A. If xk is an eigenvector of A for
the eigenvalue Î»k, then xk is an eigenvector for (A âˆ’Î±I)âˆ’1 corresponding to the eigenvalue
1
Î»kâˆ’Î±.

31.2.
THE SHIFTED INVERSE POWER METHOD
557
Proof: Let Î»k and xk be as described in the statement of the lemma. Then
(A âˆ’Î±I) xk = (Î»k âˆ’Î±) xk
and so
1
Î»k âˆ’Î±xk = (A âˆ’Î±I)âˆ’1 xk.
This proves the lemma.
In explaining why the method works, we will assume A is nondefective. This is not
necessary!
This method is much better than it might seem from the explanation we are
about to give. Pick u1, an initial vector and let Axk = Î»kxk, where {x1, Â· Â· Â·, xn} is a basis
of eigenvectors which exists from the assumption that A is nondefective. Assume Î± is closer
to Î»n than to any other eigenvalue. Since A is nondefective, there exist constants, ak such
that
u1 =
n
X
k=1
akxk.
Possibly Î»n is a repeated eigenvalue. Then combining the terms in the sum which involve
eigenvectors for Î»n, a simpler description of u1 is
u1 =
m
X
j=1
ajxj + y
where y is an eigenvector for Î»n which is assumed not equal to 0. (If you are unlucky in your
choice for u1, this might not happen and things wonâ€™t work.) Now the iteration procedure
is deï¬ned as
uk+1 â‰¡(A âˆ’Î±I)âˆ’1 uk
Sk
where Sk is the element of (A âˆ’Î±I)âˆ’1 uk which has largest absolute value. From Lemma
31.2.1,
uk+1
=
Pm
j=1 aj
Â³
1
Î»jâˆ’Î±
Â´k
xj +
Â³
1
Î»nâˆ’Î±
Â´k
y
S2 Â· Â· Â· Sk
=
Â³
1
Î»nâˆ’Î±
Â´k
S2 Â· Â· Â· Sk
ï£«
ï£­
m
X
j=1
aj
ÂµÎ»n âˆ’Î±
Î»j âˆ’Î±
Â¶k
xj + y
ï£¶
ï£¸.
Now it is being assumed that Î»n is the eigenvalue which is closest to Î± and so for large k,
the term,
m
X
j=1
aj
ÂµÎ»n âˆ’Î±
Î»j âˆ’Î±
Â¶k
xj â‰¡Ek
is very small while for every k â‰¥1, uk is a moderate sized vector because every entry has
absolute value less than or equal to 1. Thus
uk+1 =
Â³
1
Î»nâˆ’Î±
Â´k
S2 Â· Â· Â· Sk
(Ek + y) â‰¡Ck (Ek + y)
where Ek â†’0, y is some eigenvector for Î»n, and Ck is of moderate size, remaining bounded
as k â†’âˆ. Therefore, for large k,
uk+1 âˆ’Cky = CkEkâ‰ˆ0

558
ITERATIVE METHODS FOR FINDING EIGENVALUES
and multiplying by (A âˆ’Î±I)âˆ’1 yields
(A âˆ’Î±I)âˆ’1 uk+1 âˆ’(A âˆ’Î±I)âˆ’1 Cky
=
(A âˆ’Î±I)âˆ’1 uk+1 âˆ’Ck
Âµ
1
Î»n âˆ’Î±
Â¶
y
â‰ˆ
(A âˆ’Î±I)âˆ’1 uk+1 âˆ’
Âµ
1
Î»n âˆ’Î±
Â¶
uk+1â‰ˆ0.
Therefore, for large k, uk is approximately equal to an eigenvector of (A âˆ’Î±I)âˆ’1. Therefore,
(A âˆ’Î±I)âˆ’1 uk â‰ˆ
1
Î»n âˆ’Î±uk
and so you could take the dot product of both sides with uk and approximate Î»n by solving
the following for Î»n.
(A âˆ’Î±I)âˆ’1 uk Â· uk
|uk|2
=
1
Î»n âˆ’Î±
How else can you ï¬nd the eigenvalue from this? Suppose uk = (w1, Â· Â· Â·, wn)T and from
the construction |wi| â‰¤1 and wk = 1 for some k. Then
Skuk+1 = (A âˆ’Î±I)âˆ’1 uk â‰ˆ(A âˆ’Î±I)âˆ’1 (Ckâˆ’1y) =
1
Î»n âˆ’Î± (Ckâˆ’1y) â‰ˆ
1
Î»n âˆ’Î±uk.
Hence the entry of (A âˆ’Î±I)âˆ’1 uk which has largest absolute value is approximately
1
Î»nâˆ’Î±
and so it is likely that you can estimate Î»n using the formula
Sk =
1
Î»n âˆ’Î±.
Of course this would fail if (A âˆ’Î±I)âˆ’1 uk had more than one entry having equal absolute
value.
Here is how you use the shifted inverse power method to ï¬nd the eigenvalue
and eigenvector closest to Î±.
1. Find (A âˆ’Î±I)âˆ’1 .
2. Pick u1. It is important that u1 = Pm
j=1 ajxj +y where y is an eigenvector which goes
with the eigenvalue closest to Î± and the sum is in an â€œinvariant subspace corresponding
to the other eigenvaluesâ€. Of course you have no way of knowing whether this is so
but it typically is so. If things donâ€™t work out, just start with a diï¬€erent u1. You were
unlucky in your choice.
3. If uk has been obtained,
uk+1 = (A âˆ’Î±I)âˆ’1 uk
Sk
where Sk is the element of uk which has largest absolute value.
4. When the scaling factors, Sk are not changing much and the uk are not changing
much, ï¬nd the approximation to the eigenvalue by solving
Sk =
1
Î» âˆ’Î±
for Î». The eigenvector is approximated by uk+1.

31.2.
THE SHIFTED INVERSE POWER METHOD
559
5. Check your work by multiplying by the original matrix to see how well what you have
found works.
Example 31.2.2 Find the eigenvalue of A =
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸which is closest to âˆ’7.
Also ï¬nd an eigenvector which goes with this eigenvalue.
In this case the eigenvalues are âˆ’6, 0, and 12 so the correct answer is âˆ’6 for the eigen-
value. Then from the above procedure, we will start with an initial vector,
u1 â‰¡
ï£«
ï£­
1
1
1
ï£¶
ï£¸.
We want the eigenvalue closest to âˆ’7. Thus we could use the above method. First we ï¬nd
ï£«
ï£­
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸+ 7
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
ï£¶
ï£¸
âˆ’1
=
ï£«
ï£­
68
133
122
133
âˆ’65
133
4
133
15
133
4
133
âˆ’3
7
âˆ’6
7
4
7
ï£¶
ï£¸
Then beginning with u1 above, the next iterate is
u2 =
ï£«
ï£­
68
133
122
133
âˆ’65
133
4
133
15
133
4
133
âˆ’3
7
âˆ’6
7
4
7
ï£¶
ï£¸
ï£«
ï£­
1
1
1
ï£¶
ï£¸=
ï£«
ï£­
. 939 849 624
. 172 932 331
âˆ’. 714 285 714
ï£¶
ï£¸
Thus S2 = . 939 849 624 and
ï£«
ï£­
. 939 849 624
. 172 932 331
âˆ’. 714 285 714
ï£¶
ï£¸
1
. 939 849 624 =
ï£«
ï£­
1.0
. 184
âˆ’. 76
ï£¶
ï£¸
Then doing another iteration,
u3 =
ï£«
ï£­
68
133
122
133
âˆ’65
133
4
133
15
133
4
133
âˆ’3
7
âˆ’6
7
4
7
ï£¶
ï£¸
ï£«
ï£­
1.0
. 184
âˆ’. 76
ï£¶
ï£¸=
ï£«
ï£­
1. 051 488 72
2. 796 992 48 Ã— 10âˆ’2
âˆ’1. 020 571 43
ï£¶
ï£¸
Dividing by the largest element, this yields
1
1. 051 488 72
ï£«
ï£­
1. 051 488 72
2. 796 992 48 Ã— 10âˆ’2
âˆ’1. 020 571 43
ï£¶
ï£¸=
ï£«
ï£­
1.0
2. 660 030 89 Ã— 10âˆ’2
âˆ’. 970 596 651
ï£¶
ï£¸
The next iteration is
u4 =
ï£«
ï£­
68
133
122
133
âˆ’65
133
4
133
15
133
4
133
âˆ’3
7
âˆ’6
7
4
7
ï£¶
ï£¸
ï£«
ï£­
1.0
2. 660 030 89 Ã— 10âˆ’2
âˆ’. 970 596 651
ï£¶
ï£¸=
ï£«
ï£­
1. 010 030 23
3. 884 346 09 Ã— 10âˆ’3
âˆ’1. 005 998 35
ï£¶
ï£¸
The scaling factors are not changing by very much so this looks like a good time to stop.
Thus you solve the following for Î».
1
Î» + 7 = 1. 010 030 23.

560
ITERATIVE METHODS FOR FINDING EIGENVALUES
This yields
1
Î»+7 = 1. 010 030 23 which yields Î» = âˆ’6. 009 930. This is pretty close to the
true eigenvalue, âˆ’6. How well does u4 work as an eigenvector?
ï£«
ï£­
5
âˆ’14
11
âˆ’4
4
âˆ’4
3
6
âˆ’3
ï£¶
ï£¸
ï£«
ï£­
1. 010 030 23
3. 884 346 09 Ã— 10âˆ’3
âˆ’1. 005 998 35
ï£¶
ï£¸=
ï£«
ï£­
âˆ’6. 070 211 55
âˆ’5. 901 356 4 Ã— 10âˆ’4
6. 071 391 82
ï£¶
ï£¸
while
âˆ’6. 009 930
ï£«
ï£­
1. 010 030 23
3. 884 346 09 Ã— 10âˆ’3
âˆ’1. 005 998 35
ï£¶
ï£¸=
ï£«
ï£­
âˆ’6. 070 210 98
âˆ’2. 334 464 81 Ã— 10âˆ’2
6. 045 979 66
ï£¶
ï£¸.
Example 31.2.3 Consider the symmetric matrix, A =
ï£«
ï£­
1
2
3
2
1
4
3
4
2
ï£¶
ï£¸. Find the middle
eigenvalue and an eigenvector which goes with it.
Since A is symmetric, it follows it has three real eigenvalues which are solutions to
p (Î»)
=
det
ï£«
ï£­Î»
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸âˆ’
ï£«
ï£­
1
2
3
2
1
4
3
4
2
ï£¶
ï£¸
ï£¶
ï£¸
=
Î»3 âˆ’4Î»2 âˆ’24Î» âˆ’17 = 0
If you use your graphing calculator to graph this polynomial, you ï¬nd there is an eigenvalue
somewhere between âˆ’.9 and âˆ’.8 and that this is the middle eigenvalue. Of course you could
zoom in and ï¬nd it very accurately without much trouble but what about the eigenvector
which goes with it? If you try to solve
ï£«
ï£­(âˆ’.8)
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸âˆ’
ï£«
ï£­
1
2
3
2
1
4
3
4
2
ï£¶
ï£¸
ï£¶
ï£¸
ï£«
ï£­
x
y
z
ï£¶
ï£¸=
ï£«
ï£­
0
0
0
ï£¶
ï£¸
there will be only the zero solution because the matrix on the left will be invertible and the
same will be true if you replace âˆ’.8 with a better approximation like âˆ’.86 or âˆ’.855. This is
because all these are only approximations to the eigenvalue and so the matrix in the above
is nonsingular for all of these. Therefore, you will only get the zero solution and
Eigenvectors are never equal to zero!
However, there exists such an eigenvector and you can ï¬nd it using the shifted inverse power
method. Pick Î± = âˆ’.855. You know this is close to the true eigenvalue. Then you ï¬nd
ï£«
ï£­
ï£«
ï£­
1
2
3
2
1
4
3
4
2
ï£¶
ï£¸+ .855
ï£«
ï£­
1
0
0
0
1
0
0
0
1
ï£¶
ï£¸
ï£¶
ï£¸
âˆ’1
=
ï£«
ï£­
âˆ’367. 501 105
215. 955 47
83. 601 203 4
215. 955 47
âˆ’127. 169 104
âˆ’48. 753 063 2
83. 601 203 4
âˆ’48. 753 063 2
âˆ’19. 191 368 6
ï£¶
ï£¸
The ï¬rst step of the iteration is then
u1 =
ï£«
ï£­
âˆ’367. 501 105
215. 955 47
83. 601 203 4
215. 955 47
âˆ’127. 169 104
âˆ’48. 753 063 2
83. 601 203 4
âˆ’48. 753 063 2
âˆ’19. 191 368 6
ï£¶
ï£¸
ï£«
ï£­
1
1
1
ï£¶
ï£¸=
ï£«
ï£­
âˆ’67. 944 431 6
40. 033 302 8
15. 656 771 6
ï£¶
ï£¸

31.2.
THE SHIFTED INVERSE POWER METHOD
561
Dividing by the largest entry to normalize the vector on the right,
ï£«
ï£­
âˆ’67. 944 431 6
40. 033 302 8
15. 656 771 6
ï£¶
ï£¸
1
âˆ’67. 944 431 6 =
ï£«
ï£­
1. 0
âˆ’. 589 206 53
âˆ’. 230 434 949
ï£¶
ï£¸
Then the next approximation is
u2
=
ï£«
ï£­
âˆ’367. 501 105
215. 955 47
83. 601 203 4
215. 955 47
âˆ’127. 169 104
âˆ’48. 753 063 2
83. 601 203 4
âˆ’48. 753 063 2
âˆ’19. 191 368 6
ï£¶
ï£¸
ï£«
ï£­
1. 0
âˆ’. 589 206 53
âˆ’. 230 434 949
ï£¶
ï£¸
=
ï£«
ï£­
âˆ’514. 008 117
302. 118 746
116. 749 189
ï£¶
ï£¸
Divide this by the largest element.
ï£«
ï£­
âˆ’514. 008 117
302. 118 746
116. 749 189
ï£¶
ï£¸
1
âˆ’514. 008 117 =
ï£«
ï£­
1.0
âˆ’. 587 770 38
âˆ’. 227 134 913
ï£¶
ï£¸
Clearly these vectors are not changing much. An approximate eigenvector is then
ï£«
ï£­
1.0
âˆ’. 587 770 38
âˆ’. 227 134 913
ï£¶
ï£¸
and to ï¬nd the eigenvalue you solve
1
Î»+.855 = âˆ’514. 008 117, which yields Î» = âˆ’. 856 945 495.
How well does it work?
ï£«
ï£­
1
2
3
2
1
4
3
4
2
ï£¶
ï£¸
ï£«
ï£­
1.0
âˆ’. 587 770 38
âˆ’. 227 134 913
ï£¶
ï£¸=
ï£«
ï£­
âˆ’. 856 945 499
. 503 689 968
. 194 648 654
ï£¶
ï£¸
while
âˆ’. 856 945 495
ï£«
ï£­
1.0
âˆ’. 587 770 38
âˆ’. 227 134 913
ï£¶
ï£¸=
ï£«
ï£­
âˆ’. 856 945 495
. 503 687 179
. 194 642 24
ï£¶
ï£¸
I think you can see that for all practical purposes, this has found the eigenvalue and an
eigenvector.

562
ITERATIVE METHODS FOR FINDING EIGENVALUES

Part XV
The Correct Version Of The
Riemann Integral âˆ—
563


The Theory Of The Riemann
Integralâˆ—âˆ—
R
Jordan the contented dragon
A.1
An Important Warning
If you read and understand this appendix on the Riemann integral you will become ab-
normal if you are not already that way. You will laugh at atrocious puns. You will be
unpopular with well adjusted conï¬dent people. Furthermore, your conï¬dence will be com-
pletely shattered. Virtually nothing will be obvious to you ever again. Consider whether
it would be better to accept the superï¬cial presentation given earlier than to attempt to
acquire deep understanding of the integral, risking your self esteem and conï¬dence, before
proceeding further.
A.2
The Deï¬nition Of The Riemann Integral
The deï¬nition of the Riemann integral of a function of n variables uses the following deï¬ni-
tion.
565

566
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
Deï¬nition A.2.1 For i = 1, Â· Â· Â·, n, let
Â©
Î±i
k
Âªâˆ
k=âˆ’âˆbe points on R which satisfy
lim
kâ†’âˆÎ±i
k = âˆ,
lim
kâ†’âˆ’âˆÎ±i
k = âˆ’âˆ, Î±i
k < Î±i
k+1.
(1.1)
For such sequences, deï¬ne a grid on Rn denoted by G or F as the collection of boxes of the
form
Q =
n
Y
i=1
Â£
Î±i
ji, Î±i
ji+1
Â¤
.
(1.2)
If G is a grid, F is called a reï¬nement of G if every box of G is the union of boxes of F.
Lemma A.2.2 If G and F are two grids, they have a common reï¬nement, denoted here
by G âˆ¨F.
Proof: Let
Â©
Î±i
k
Âªâˆ
k=âˆ’âˆbe the sequences used to construct G and let
Â©
Î²i
k
Âªâˆ
k=âˆ’âˆbe
the sequence used to construct F. Now let
Â©
Î³i
k
Âªâˆ
k=âˆ’âˆdenote the union of
Â©
Î±i
k
Âªâˆ
k=âˆ’âˆand
Â©
Î²i
k
Âªâˆ
k=âˆ’âˆ. It is necessary to show that for each i these points can be arranged in order.
To do so, let Î³i
0 â‰¡Î±i
0. Now if
Î³i
âˆ’j, Â· Â· Â·, Î³i
0, Â· Â· Â·, Î³i
j
have been chosen such that they are in order and all distinct, let Î³i
j+1 be the ï¬rst element
of
Â©
Î±i
k
Âªâˆ
k=âˆ’âˆâˆª
Â©
Î²i
k
Âªâˆ
k=âˆ’âˆ
(1.3)
which is larger than Î³i
j and let Î³i
âˆ’(j+1) be the last element of 1.3 which is strictly smaller
than Î³i
âˆ’j. The assumption 1.1 insures such a ï¬rst and last element exists. Now let the grid
G âˆ¨F consist of boxes of the form
Q â‰¡
n
Y
i=1
Â£
Î³i
ji, Î³i
ji+1
Â¤
.
The Riemann integral is only deï¬ned for functions, f which are bounded and are equal
to zero oï¬€some bounded set, D. In what follows f will always be such a function.
Deï¬nition A.2.3 Let f be a bounded function which equals zero oï¬€a bounded set,
D, and let G be a grid. For Q âˆˆG, deï¬ne
MQ (f) â‰¡sup {f (x) : x âˆˆQ} , mQ (f) â‰¡inf {f (x) : x âˆˆQ} .
(1.4)
Also deï¬ne for Q a box, the volume of Q, denoted by v (Q) by
v (Q) â‰¡
n
Y
i=1
(bi âˆ’ai) , Q â‰¡
n
Y
i=1
[ai, bi] .
Now deï¬ne upper sums, UG (f) and lower sums, LG (f) with respect to the indicated grid,
by the formulas
UG (f) â‰¡
X
QâˆˆG
MQ (f) v (Q) , LG (f) â‰¡
X
QâˆˆG
mQ (f) v (Q) .
A function of n variables is Riemann integrable when there is a unique number between all
the upper and lower sums. This number is the value of the integral.

A.2.
THE DEFINITION OF THE RIEMANN INTEGRAL
567
Note that in this deï¬nition, MQ (f) = mQ (f) = 0 for all but ï¬nitely many Q âˆˆG so
there are no convergence questions to be considered here.
Lemma A.2.4 If F is a reï¬nement of G then
UG (f) â‰¥UF (f) , LG (f) â‰¤LF (f) .
Also if F and G are two grids,
LG (f) â‰¤UF (f) .
Proof: For P âˆˆG let bP denote the set,
{Q âˆˆF : Q âŠ†P} .
Then P = âˆªbP and
LF (f) â‰¡
X
QâˆˆF
mQ (f) v (Q) =
X
P âˆˆG
X
Qâˆˆb
P
mQ (f) v (Q)
â‰¥
X
P âˆˆG
mP (f)
X
Qâˆˆb
P
v (Q) =
X
P âˆˆG
mP (f) v (P) â‰¡LG (f) .
Similarly, the other inequality for the upper sums is valid.
To verify the last assertion of the lemma, use Lemma A.2.2 to write
LG (f) â‰¤LGâˆ¨F (f) â‰¤UGâˆ¨F (f) â‰¤UF (f) .
This proves the lemma.
This lemma makes it possible to deï¬ne the Riemann integral.
Deï¬nition A.2.5 Deï¬ne an upper and a lower integral as follows.
I (f) â‰¡inf {UG (f) : G is a grid} ,
I (f) â‰¡sup {LG (f) : G is a grid} .
Lemma A.2.6 I (f) â‰¥I (f) .
Proof: From Lemma A.2.4 it follows for any two grids G and F,
LG (f) â‰¤UF (f) .
Therefore, taking the supremum for all grids on the left in this inequality,
I (f) â‰¤UF (f)
for all grids F. Taking the inï¬mum in this inequality, yields the conclusion of the lemma.
Deï¬nition A.2.7 A bounded function, f which equals zero oï¬€a bounded set, D, is
said to be Riemann integrable, written as f âˆˆR (Rn) exactly when I (f) = I (f) . In this
case deï¬ne
Z
f dV â‰¡
Z
f dx = I (f) = I (f) .
As in the case of integration of functions of one variable, one obtains the Riemann
criterion which is stated as the following theorem.

568
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
Theorem A.2.8 (Riemann criterion) f âˆˆR (Rn) if and only if for all Îµ > 0 there
exists a grid G such that
UG (f) âˆ’LG (f) < Îµ.
Proof: If f âˆˆR (Rn), then I (f) = I (f) and so there exist grids G and F such that
UG (f) âˆ’LF (f) â‰¤I (f) + Îµ
2 âˆ’
Â³
I (f) âˆ’Îµ
2
Â´
= Îµ.
Then letting H = G âˆ¨F, Lemma A.2.4 implies
UH (f) âˆ’LH (f) â‰¤UG (f) âˆ’LF (f) < Îµ.
Conversely, if for all Îµ > 0 there exists G such that
UG (f) âˆ’LG (f) < Îµ,
then
I (f) âˆ’I (f) â‰¤UG (f) âˆ’LG (f) < Îµ.
Since Îµ > 0 is arbitrary, this proves the theorem.
A.3
Basic Properties
It is important to know that certain combinations of Riemann integrable functions are
Riemann integrable. The following theorem will include all the important cases.
Theorem A.3.1 Let f, g âˆˆR (Rn) and let Ï† : K â†’R be continuous where K is a
compact set in R2 containing f (Rn) Ã— g (Rn). Also suppose that Ï† (0, 0) = 0. Then deï¬ning
h (x) â‰¡Ï† (f (x) , g (x)) ,
it follows that h is also in R (Rn).
Proof: Let Îµ > 0 and let Î´1 > 0 be such that if (yi, zi) , i = 1, 2 are points in K, such
that |z1 âˆ’z2| â‰¤Î´1 and |y1 âˆ’y2| â‰¤Î´1, then
|Ï† (y1, z1) âˆ’Ï† (y2, z2)| < Îµ.
Let 0 < Î´ < min (Î´1, Îµ, 1) . Let G be a grid with the property that for Q âˆˆG, the diameter
of Q is less than Î´ and also for k = f, g,
UG (k) âˆ’LG (k) < Î´2.
(1.5)
Then deï¬ning for k = f, g,
Pk â‰¡{Q âˆˆG : MQ (k) âˆ’mQ (k) > Î´} ,
it follows
Î´2 >
X
QâˆˆG
(MQ (k) âˆ’mQ (k)) v (Q) â‰¥
X
Pk
(MQ (k) âˆ’mQ (k)) v (Q) â‰¥Î´
X
Pk
v (Q)

A.3.
BASIC PROPERTIES
569
and so for k = f, g,
Îµ > Î´ >
X
Pk
v (Q) .
(1.6)
Suppose for k = f, g,
MQ (k) âˆ’mQ (k) â‰¤Î´.
Then if x1, x2 âˆˆQ,
|f (x1) âˆ’f (x2)| < Î´, and |g (x1) âˆ’g (x2)| < Î´.
Therefore,
|h (x1) âˆ’h (x2)| â‰¡|Ï† (f (x1) , g (x1)) âˆ’Ï† (f (x2) , g (x2))| < Îµ
and it follows that
|MQ (h) âˆ’mQ (h)| â‰¤Îµ.
Now let
S â‰¡{Q âˆˆG : 0 < MQ (k) âˆ’mQ (k) â‰¤Î´, k = f, g} .
Thus the union of the boxes in S is contained in some large box, R, which depends only
on f and g and also, from the assumption that Ï† (0, 0) = 0, MQ (h) âˆ’mQ (h) = 0 unless
Q âŠ†R. Then
UG (h) âˆ’LG (h) â‰¤
X
QâˆˆPf
(MQ (h) âˆ’mQ (h)) v (Q) +
X
QâˆˆPg
(MQ (h) âˆ’mQ (h)) v (Q) +
X
QâˆˆS
Î´v (Q) .
Now since K is compact, it follows Ï† (K) is bounded and so there exists a constant, C,
depending only on h and Ï† such that MQ (h)âˆ’mQ (h) < C. Therefore, the above inequality
implies
UG (h) âˆ’LG (h) â‰¤C
X
QâˆˆPf
v (Q) + C
X
QâˆˆPg
v (Q) +
X
QâˆˆS
Î´v (Q) ,
which by 1.6 implies
UG (h) âˆ’LG (h) â‰¤2CÎµ + Î´v (R) â‰¤2CÎµ + Îµv (R) .
Since Îµ is arbitrary, the Riemann criterion is satisï¬ed and so h âˆˆR (Rn).
Corollary A.3.2 Let f, g âˆˆR (Rn) and let a, b âˆˆR. Then af + bg, fg, and |f| are all
in R (Rn) . Also,
Z
Rn (af + bg) dx = a
Z
Rn f dx + b
Z
Rn g dx,
(1.7)
and
Z
|f| dx â‰¥
Â¯Â¯Â¯Â¯
Z
f dx
Â¯Â¯Â¯Â¯ .
(1.8)
Proof: Each of the combinations of functions described above is Riemann integrable
by Theorem A.3.1. For example, to see af + bg âˆˆR (Rn) consider Ï† (y, z) â‰¡ay + bz. This
is clearly a continuous function of (y, z) such that Ï† (0, 0) = 0. To obtain |f| âˆˆR (Rn) , let
Ï† (y, z) â‰¡|y| . It remains to verify the formulas. To do so, let G be a grid with the property
that for k = f, g, |f| and af + bg,
UG (k) âˆ’LG (k) < Îµ.
(1.9)

570
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
Consider 1.7. For each Q âˆˆG pick a point in Q, xQ. Then
X
QâˆˆG
k (xQ) v (Q) âˆˆ[LG (k) , UG (k)]
and so
Â¯Â¯Â¯Â¯Â¯Â¯
Z
k dx âˆ’
X
QâˆˆG
k (xQ) v (Q)
Â¯Â¯Â¯Â¯Â¯Â¯
< Îµ.
Consequently, since
X
QâˆˆG
(af + bg) (xQ) v (Q)
= a
X
QâˆˆG
f (xQ) v (Q) + b
X
QâˆˆG
g (xQ) v (Q) ,
it follows
Â¯Â¯Â¯Â¯
Z
(af + bg) dx âˆ’a
Z
f dx âˆ’b
Z
g dx
Â¯Â¯Â¯Â¯ â‰¤
Â¯Â¯Â¯Â¯Â¯Â¯
Z
(af + bg) dx âˆ’
X
QâˆˆG
(af + bg) (xQ) v (Q)
Â¯Â¯Â¯Â¯Â¯Â¯
+
Â¯Â¯Â¯Â¯Â¯Â¯
a
X
QâˆˆG
f (xQ) v (Q) âˆ’a
Z
f dx
Â¯Â¯Â¯Â¯Â¯Â¯
+
Â¯Â¯Â¯Â¯Â¯Â¯
b
X
QâˆˆG
g (xQ) v (Q) âˆ’b
Z
g dx
Â¯Â¯Â¯Â¯Â¯Â¯
â‰¤Îµ + |a| Îµ + |b| Îµ.
Since Îµ is arbitrary, this establishes Formula 1.7 and shows the integral is linear.
It remains to establish the inequality 1.8. By 1.9, and the triangle inequality for sums,
Z
|f| dx + Îµ â‰¥
X
QâˆˆG
|f (xQ)| v (Q) â‰¥
â‰¥
Â¯Â¯Â¯Â¯Â¯Â¯
X
QâˆˆG
f (xQ) v (Q)
Â¯Â¯Â¯Â¯Â¯Â¯
â‰¥
Â¯Â¯Â¯Â¯
Z
f dx
Â¯Â¯Â¯Â¯ âˆ’Îµ.
Then since Îµ is arbitrary, this establishes the desired inequality. This proves the corollary.
Which functions are in R (Rn)? Begin with step functions deï¬ned below.
Deï¬nition A.3.3 If
Q â‰¡
n
Y
i=1
[ai, bi]
is a box, deï¬ne int (Q) as
int (Q) â‰¡
n
Y
i=1
(ai, bi) .
f is called a step function if there is a grid, G such that f is constant on int (Q) for each
Q âˆˆG, f is bounded, and f (x) = 0 for all x outside some bounded set.
The next corollary states that step functions are in R (Rn) and shows the expected
formula for the integral is valid.

A.3.
BASIC PROPERTIES
571
Corollary A.3.4 Let G be a grid and let f be a step function such that f = fQ on
int (Q) for each Q âˆˆG. Then f âˆˆR (Rn) and
Z
f dx =
X
QâˆˆG
fQv (Q) .
Proof: Let Q be a box of G,
Q â‰¡
n
Y
i=1
Â£
Î±i
ji, Î±i
ji+1
Â¤
,
and suppose g is a bounded function, |g (x)| â‰¤C, and g = 0 oï¬€Q, and g = 1 on int (Q) .
Thus, g is the simplest sort of step function. Reï¬ne G by including the extra points,
Î±i
ji + Î· and Î±i
ji+1 âˆ’Î·
for each i = 1, Â· Â· Â·, n. Here Î· is small enough that for each i, Î±i
ji + Î· < Î±i
ji+1 âˆ’Î·. Also let L
denote the largest of the lengths of the sides of Q. Let F be this reï¬ned grid and denote by
QÎ· the box
n
Y
i=1
Â£
Î±i
ji + Î·, Î±i
ji+1 âˆ’Î·
Â¤
.
Now deï¬ne the box, Bk by
Bk â‰¡
Â£
Î±1
j1, Î±1
j1+1
Â¤
Ã— Â· Â· Â· Ã—
h
Î±kâˆ’1
jkâˆ’1, Î±kâˆ’1
jkâˆ’1+1
i
Ã—
Â£
Î±k
jk, Î±k
jk + Î·
Â¤
Ã—
h
Î±k+1
jk+1, Î±k+1
jk+1+1
i
Ã— Â· Â· Â· Ã—
Â£
Î±n
jn, Î±n
jn+1
Â¤
or
Bk â‰¡
Â£
Î±1
j1, Î±1
j1+1
Â¤
Ã— Â· Â· Â· Ã—
h
Î±kâˆ’1
jkâˆ’1, Î±kâˆ’1
jkâˆ’1+1
i
Ã—
Â£
Î±k
jk âˆ’Î·, Î±k
jk
Â¤
Ã—
h
Î±k+1
jk+1, Î±k+1
jk+1+1
i
Ã— Â· Â· Â· Ã—
Â£
Î±n
jn, Î±n
jn+1
Â¤
.
In words, replace the closed interval in the kth slot used to deï¬ne Q with a much thinner
closed interval at one end or the other while leaving the other intervals used to deï¬ne Q the
same. This is illustrated in the following picture.
QÎ·
Q
Bk
Bk
The important thing to notice, is that every point of Q is either in QÎ· or one of the sets,
Bk. Therefore,
LF (g) â‰¥v (QÎ·) âˆ’
n
X
k=1
2Cv (Bk) â‰¥v (QÎ·) âˆ’4CLnâˆ’1nÎ·

572
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
= v (QÎ·) âˆ’KÎ·
(1.10)
where K is a constant which does not depend on Î·. Similarly,
UF (g) â‰¤v (QÎ·) + KÎ·.
(1.11)
This implies UF (g) âˆ’LF (g) < 2KÎ· and since Î· is arbitrary, the Riemann criterion veriï¬es
that g âˆˆR (Rn) . Formulas 1.10 and 1.11 also verify that
v (QÎ·) âˆˆ[UF (g) âˆ’KÎ·, LF (g) + KÎ·]
âŠ†[LF (g) âˆ’KÎ·, UF (g) + KÎ·] .
But also
Z
g dx âˆˆ[LF (g) , UF (g)] âŠ†[LF (g) âˆ’KÎ·, UF (g) + KÎ·]
and so
Â¯Â¯Â¯Â¯
Z
g dx âˆ’v (QÎ·)
Â¯Â¯Â¯Â¯ â‰¤4KÎ·.
Now letting Î· â†’0, yields
R
g dx = v (Q) .
Now let f be as described in the statement of the Corollary. Let fQ be the value of f
on int (Q) , and let gQ be a function of the sort just considered which equals 1 on int (Q) .
Then f is of the form
f =
X
QâˆˆG
fQgQ
with all but ï¬nitely many of the fQ equal zero. Therefore, the above is really a ï¬nite sum
and so by Corollary A.3.2, f âˆˆR (Rn) and
Z
f dx =
X
QâˆˆG
fQ
Z
gQ dx =
X
QâˆˆG
fQv (Q) .
This proves the corollary.
There is a good deal of sloppiness inherent in the above description of a step function
due to the fact that the boxes may be diï¬€erent but match up on an edge. It is convenient
to be able to consider a more precise sort of function and this is done next.
For Q a box of the form
Q =
k
Y
i=1
[ai, bi] ,
deï¬ne the half open box, Qâ€² by
Qâ€² =
k
Y
i=1
(ai, bi].
The reason for considering these sets is that if G is a grid, the sets, Qâ€² where Q âˆˆG are
disjoint. Deï¬ning a step function, Ï† as
Ï† (x) â‰¡
X
QâˆˆG
Ï†QXQâ€² (x) ,
the number, Ï†Q is the value of Ï† on the set, Qâ€². As before, deï¬ne
MQâ€² (f) â‰¡sup {f (x) : x âˆˆQâ€²} , mQâ€² (f) â‰¡inf {f (x) : x âˆˆQâ€²} .
The next lemma will be convenient a little later.

A.3.
BASIC PROPERTIES
573
Lemma A.3.5 Suppose f is a bounded function which equals zero oï¬€some bounded set.
Then f âˆˆR (Rn) if and only if for all Îµ > 0 there exists a grid, G such that
X
QâˆˆG
(MQâ€² (f) âˆ’mQâ€² (f)) v (Q) < Îµ.
(1.12)
Proof: Since Qâ€² âŠ†Q,
MQâ€² (f) âˆ’mQâ€² (f) â‰¤MQ (f) âˆ’mQ (f)
and therefore, the only if part of the equivalence is obvious.
Conversely, let G be a grid such that 1.12 holds with Îµ replaced with Îµ
2. It is necessary
to show there is a grid such that 1.12 holds with no primes on the Q. Let F be a reï¬nement
of G obtained by adding the points Î±i
k + Î·k where Î·k â‰¤Î· and is also chosen so small that
for each i = 1, Â· Â· Â·, n,
Î±i
k + Î·k < Î±i
k+1.
You only need to have Î·k > 0 for the ï¬nitely many boxes of G which intersect the bounded
set where f is not zero. Then for
Q â‰¡
n
Y
i=1
Â£
Î±i
ki, Î±i
ki+1
Â¤
âˆˆG,
Let
bQ â‰¡
n
Y
i=1
Â£
Î±i
ki + Î·ki, Î±i
ki+1
Â¤
and denote by bG the collection of these smaller boxes. For each set, Q in G there is the
smaller set, bQ along with n boxes, Bk, k = 1, Â· Â· Â·, n, one of whose sides is of length Î·k
and the remainder of whose sides are shorter than the diameter of Q such that the set,
Q is the union of bQ and these sets, Bk. Now suppose f equals zero oï¬€the ball B
Â¡
0, R
2
Â¢
.
Then without loss of generality, you may assume the diameter of every box in G which
has nonempty intersection with B (0,R) is smaller than R
3 . (If this is not so, simply reï¬ne
G to make it so, such a reï¬nement leaving 1.12 valid because reï¬nements do not increase
the diï¬€erence between upper and lower sums in this context either.) Suppose there are P
sets of G contained in B (0,R) (So these are the only sets of G which could have nonempty
intersection with the set where f is nonzero.) and suppose that for all x, |f (x)| < C/2.
Then
X
QâˆˆF
(MQ (f) âˆ’mQ (f)) v (Q) â‰¤
X
b
Qâˆˆb
G
Â³
M b
Q (f) âˆ’m b
Q (f)
Â´
v (Q)
+
X
QâˆˆF\ b
G
(MQ (f) âˆ’mQ (f)) v (Q)
The ï¬rst term on the right of the inequality in the above is no larger than Îµ/2 because
M b
Q (f) âˆ’m b
Q (f) â‰¤MQâ€² (f) âˆ’mQâ€² (f) for each Q. Therefore, the above is dominated by
â‰¤Îµ/2 + CPnRnâˆ’1Î· < Îµ
whenever Î· is small enough. Since Îµ is arbitrary, f âˆˆR (Rn) as claimed.
Deï¬nition A.3.6 A bounded set, E is a Jordan set in Rn or a contented set in Rn
if XE âˆˆR (Rn). Also, for G a grid and E a set, denote by âˆ‚G (E) those boxes of G which
have nonempty intersection with both E and Rn \ E.

574
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
The next theorem is a characterization of those sets which are Jordan sets.
Theorem A.3.7 A bounded set, E, is a Jordan set if and only if for every Îµ > 0
there exists a grid, G, such that
X
Qâˆˆâˆ‚G(E)
v (Q) < Îµ.
Proof: If Q /âˆˆâˆ‚G (E) , then
MQ (XE) âˆ’mQ (XE) = 0
and if Q âˆˆâˆ‚G (E) , then
MQ (XE) âˆ’mQ (XE) = 1.
It follows that UG (XE) âˆ’LG (XE) = P
Qâˆˆâˆ‚G(E) v (Q) and this implies the conclusion of the
theorem by the Riemann criterion.
Note that if E is a Jordan set and if f âˆˆR (Rn) , then by Corollary A.3.2, XEf âˆˆR (Rn) .
Deï¬nition A.3.8 For E a Jordan set and fXE âˆˆR (Rn) .
Z
E
f dV â‰¡
Z
Rn XEf dV.
Also, a bounded set, E, has Jordan content 0 or content 0 if for every Îµ > 0 there exists a
grid, G such that
X
Qâˆ©EÌ¸=âˆ…
v (Q) < Îµ.
This symbol says to sum the volumes of all boxes from G which have nonempty intersection
with E.
Note that any ï¬nite union of sets having Jordan content 0 also has Jordan content 0.
(Why?)
Deï¬nition A.3.9 Let A be any subset of Rn. Then âˆ‚A denotes those points, x with
the property that if U is any open set containing x, then U contains points of A as well as
points of AC.
Corollary A.3.10 If a bounded set, E âŠ†Rn is contented, then âˆ‚E has content 0.
Proof: Let Îµ > 0 be given and suppose E is contented. Then there exists a grid, G such
that
X
Qâˆˆâˆ‚G(E)
v (Q) <
Îµ
2n + 1.
(1.13)
Now reï¬ne G if necessary to get a new grid, F such that all boxes from F which have
nonempty intersection with âˆ‚E have sides no larger than Î´ where Î´ is the smallest of all the
sides of all the Q in the above sum. Recall that âˆ‚G (E) consists of those boxes of G which
have nonempty intersection with both E and Rn \ E.
Let x âˆˆâˆ‚E. Then since the dimension is n, there are at most 2n boxes from F which
contain x. Furthermore, at least one of these boxes is in âˆ‚F (E) and is therefore a subset
of a box from âˆ‚G (E) . Here is why. If x is an interior point of some Q âˆˆF, then there are
points of both E and EC contained in Q and so x âˆˆQ âˆˆâˆ‚F (E) and there are no other
boxes from F which contain x. If x is not an interior point of any Q âˆˆF, then the interior
of the union of all the boxes from F which do contain x is an open set and therefore, must

A.3.
BASIC PROPERTIES
575
contain points of E and points from EC. If x âˆˆE, then one of these boxes must contain
points which are not in E since otherwise, x would fail to be in âˆ‚E. Pick that box. It is in
âˆ‚F (E) and contains x. On the other hand, if x /âˆˆE, one of these boxes must contain points
of E since otherwise, x would fail to be in âˆ‚E. Pick that box. This shows that every set
from F which contains a point of âˆ‚E shares this point with a box of âˆ‚G (E) .
Let the boxes from âˆ‚G (E) be {P1, Â· Â· Â·, Pm} . Let S (Pi) denote those sets of F which
contain a point of âˆ‚E in common with Pi. Then if Q âˆˆS (Pi) , either Q âŠ†Pi or it intersects
Pi on one of its 2n faces. Therefore, the sum of the volumes of those boxes of S (Pi) which
intersect Pi on a particular face of Pi is no larger than v (Pi) . Consequently,
X
QâˆˆS(Pi)
v (Q) â‰¤2nv (Pi) + v (Pi) ,
the term v (Pi) accounting for those boxes which are contained in Pi. Therefore, for Q âˆˆF,
X
Qâˆ©âˆ‚EÌ¸=âˆ…
v (Q) =
m
X
i=1
X
QâˆˆS(Pi)
v (Q) â‰¤
m
X
i=1
(2n + 1) v (Pi) < Îµ
from 1.13. This proves the corollary.
Theorem A.3.11 If a bounded set, E, has Jordan content 0, then E is a Jordan
(contented) set and if f is any bounded function deï¬ned on E, then fXE âˆˆR (Rn) and
Z
E
f dV = 0.
Proof: Let Îµ > 0. Then let G be a grid such that
X
Qâˆ©EÌ¸=âˆ…
v (Q) < Îµ.
Then every set of âˆ‚G (E) contains a point of E so
X
Qâˆˆâˆ‚G(E)
v (Q) â‰¤
X
Qâˆ©EÌ¸=âˆ…
v (Q) < Îµ
and since Îµ was arbitrary, this shows from Theorem A.3.7 that E is a Jordan set. Now let
M be a positive number larger than all values of f, let m be a negative number smaller
than all values of f and let Îµ > 0 be given. Let G be a grid with
X
Qâˆ©EÌ¸=âˆ…
v (Q) <
Îµ
1 + (M âˆ’m).
Then
UG (fXE) â‰¤
X
Qâˆ©EÌ¸=âˆ…
Mv (Q) â‰¤
ÎµM
1 + (M âˆ’m)
and
LG (fXE) â‰¥
X
Qâˆ©EÌ¸=âˆ…
mv (Q) â‰¥
Îµm
1 + (M âˆ’m)
and so
UG (fXE) âˆ’LG (fXE)
â‰¤
X
Qâˆ©EÌ¸=âˆ…
Mv (Q) âˆ’
X
Qâˆ©EÌ¸=âˆ…
mv (Q)
=
(M âˆ’m)
X
Qâˆ©EÌ¸=âˆ…
v (Q) <
Îµ (M âˆ’m)
1 + (M âˆ’m) < Îµ.

576
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
This shows fXE âˆˆR (Rn) . Now also,
mÎµ â‰¤
Z
fXE dV â‰¤MÎµ
and since Îµ is arbitrary, this shows
Z
E
f dV â‰¡
Z
fXE dV = 0
and proves the theorem.
Corollary A.3.12 If fXEi âˆˆR (Rn) for i = 1, 2, Â·Â·Â·, r and for all i Ì¸= j, Ei âˆ©Ej is either
the empty set or a set of Jordan content 0, then letting F â‰¡âˆªr
i=1Ei, it follows fXF âˆˆR (Rn)
and
Z
fXF dV â‰¡
Z
F
f dV =
r
X
i=1
Z
Ei
f dV.
Proof: This is true if r = 1. Suppose it is true for r. It will be shown that it is true
for r + 1. Let Fr = âˆªr
i=1Ei and let Fr+1 be deï¬ned similarly. By the induction hypothesis,
fXFr âˆˆR (Rn) . Also, since Fr is a ï¬nite union of the Ei, it follows that Fr âˆ©Er+1 is either
empty or a set of Jordan content 0.
âˆ’fXFrâˆ©Er+1 + fXFr + fXEr+1 = fXFr+1
and by Theorem A.3.11 each function on the left is in R (Rn) and the ï¬rst one on the left
has integral equal to zero. Therefore,
Z
fXFr+1 dV =
Z
fXFr dV +
Z
fXEr+1 dV
which by induction equals
r
X
i=1
Z
Ei
f dV +
Z
Er+1
f dV =
r+1
X
i=1
Z
Ei
f dV
and this proves the corollary.
What functions in addition to step functions are integrable? As in the case of integrals of
functions of one variable, this is an important question. It turns out the Riemann integrable
functions are characterized by being continuous except on a very small set. To begin with
it is necessary to deï¬ne the oscillation of a function.
Deï¬nition A.3.13 Let f be a function deï¬ned on Rn and let
Ï‰f,r (x) â‰¡sup {|f (z) âˆ’f (y)| : z, y âˆˆB (x,r)} .
This is called the oscillation of f on B (x,r) . Note that this function of r is decreasing in
r. Deï¬ne the oscillation of f as
Ï‰f (x) â‰¡lim
râ†’0+ Ï‰f,r (x) .
Note that as r decreases, the function, Ï‰f,r (x) decreases. It is also bounded below by
0 and so the limit must exist and equals inf {Ï‰f,r (x) : r > 0} . (Why?) Then the following
simple lemma whose proof follows directly from the deï¬nition of continuity gives the reason
for this deï¬nition.

A.3.
BASIC PROPERTIES
577
Lemma A.3.14 A function, f, is continuous at x if and only if Ï‰f (x) = 0.
This concept of oscillation gives a way to deï¬ne how discontinuous a function is at a
point. The discussion will depend on the following fundamental lemma which gives the
existence of something called the Lebesgue number.
Deï¬nition A.3.15 Let C be a set whose elements are sets of Rn and let K âŠ†Rn.
The set, C is called a cover of K if every point of K is contained in some set of C. If the
elements of C are open sets, it is called an open cover.
Lemma A.3.16
Let K be sequentially compact and let C be an open cover of K. Then
there exists r > 0 such that whenever x âˆˆK, B(x, r) is contained in some set of C .
Proof: Suppose this is not so. Then letting rn = 1/n, there exists xn âˆˆK such that
B (xn, rn) is not contained in any set of C. Since K is sequentially compact, there is a
subsequence, xnk which converges to a point, x âˆˆK. But there exists Î´ > 0 such that
B (x, Î´) âŠ†U for some U âˆˆC. Let k be so large that 1/k < Î´/2 and |xnk âˆ’x| < Î´/2 also.
Then if z âˆˆB (xnk, rnk) , it follows
|z âˆ’x| â‰¤|z âˆ’xnk| + |xnk âˆ’x| < Î´
2 + Î´
2 = Î´
and so B (xnk, rnk) âŠ†U contrary to supposition. Therefore, the desired number exists after
all.
Theorem A.3.17 Let f be a bounded function which equals zero oï¬€a bounded set
and let W denote the set of points where f fails to be continuous. Then f âˆˆR (Rn) if W
has content zero. That is, for all Îµ > 0 there exists a grid, G such that
X
QâˆˆGW
v (Q) < Îµ
(1.14)
where
GW â‰¡{Q âˆˆG : Q âˆ©W Ì¸= âˆ…} .
Proof: Let W have content zero. Also let |f (x)| < C/2 for all x âˆˆRn, let Îµ > 0 be
given, and let G be a grid which satisï¬es 1.14. Since f equals zero oï¬€some bounded set,
there exists R such that f equals zero oï¬€of B
Â¡
0, R
2
Â¢
. Thus W âŠ†B
Â¡
0, R
2
Â¢
. Also note that if
G is a grid for which 1.14 holds, then this inequality continues to hold if G is replaced with
a reï¬ned grid. Therefore, you may assume the diameter of every box in G which intersects
B (0, R) is less than R
3 and so all boxes of G which intersect the set where f is nonzero are
contained in B (0,R) . Since W is bounded, GW contains only ï¬nitely many boxes. Letting
Q â‰¡
n
Y
i=1
[ai, bi]
be one of these boxes, enlarge the box slightly as indicated in the following picture.
Q
Â¡
Â¡Â¡
ËœQ

578
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
The enlarged box is an open set of the form,
eQ â‰¡
n
Y
i=1
(ai âˆ’Î·i, bi + Î·i)
where Î·i is chosen small enough that if
n
Y
i=1
( bi + Î·i âˆ’(ai âˆ’Î·i)) â‰¡v
Â³
eQ
Â´
,
then
X
QâˆˆGW
v
Â³
eQ
Â´
< Îµ.
For each x âˆˆRn, let rx be such that
Ï‰f,rx (x) < Îµ + Ï‰f (x) .
(1.15)
Now let C denote all intersections of the form eQâˆ©B (x,rx) such that x âˆˆB (0,R) so that C is
an open cover of the compact set, B (0,R). Let Î´ be a Lebesgue number for this open cover
of B (0,R) and let F be a reï¬nement of G such that every box in F has diameter less than Î´.
Now let F1 consist of those boxes of F which have nonempty intersection with B (0,R/2) .
Thus all boxes of F1 are contained in B (0,R) and each one is contained in some set of C.
Now let CW be those open sets of C, eQ âˆ©B (x,rx) , for which x âˆˆW and let FW be those
sets of F1 which are subsets of some set of CW . Then
UF (f) âˆ’LF (f) =
X
QâˆˆFW
(MQ (f) âˆ’mQ (f)) v (Q)
+
X
QâˆˆF1\FW
(MQ (f) âˆ’mQ (f)) v (Q) .
If Q âˆˆF1 \ FW , then Q must be a subset of some set of C \CW since it is not in any set of
CW . Therefore, from 1.15 and the observation that x /âˆˆW,
MQ (f) âˆ’mQ (f) â‰¤Îµ.
Therefore,
UF (f) âˆ’LF (f) â‰¤
X
QâˆˆFW
Cv (Q) +
X
QâˆˆF1\FW
Îµv (Q)
â‰¤CÎµ + Îµ (2R)n .
Since Îµ is arbitrary, this proves the theorem.1
From Theorem A.3.7 you get a pretty good idea of what constitutes a contented set.
These sets are essentially those which have thin boundaries. Most sets you are likely to
think of will fall in this category. However, it is good to give speciï¬c examples of sets which
are contented.
1In fact one cannot do any better. It can be shown that if a function is Riemann integrable, then it must
be the case that for all Îµ > 0, 1.14 is satisï¬ed for some grid, G. This along with what was just shown is
known as Lebesgueâ€™s theorem after Lebesgue who discovered it in the early years of the twentieth century.
Actually, he also invented a far superior integral which has been the integral of serious mathematicians since
that time. To prove the converse of this theorem would take us too far in that direction and it would not
be reasonable to pay any more attention to this inferior integral.

A.3.
BASIC PROPERTIES
579
Theorem A.3.18 Suppose E is a bounded contented set in Rn and f, g : E â†’R
are two functions satisfying f (x) â‰¥g (x) for all x âˆˆE and fXE and gXE are both in
R (Rn) . Now deï¬ne
P â‰¡{(x,xn+1) : x âˆˆE and g (x) â‰¤xn+1 â‰¤f (x)} .
Then P is a contented set in Rn+1.
Proof: Let G be a grid such that for k = f, g,
UG (k) âˆ’LG (k) < Îµ/4.
(1.16)
Also let K â‰¥Pm
j=1 vn (Qj) for all x âˆˆE.
Let the boxes of G which have nonempty
intersection with E be {Q1, Â· Â· Â·, Qm} and let {ai}âˆ
i=âˆ’âˆbe a sequence on R, ai < ai+1 for
all i, which includes
MQj (fXE) +
Îµ
4mK , MQj (fXE) , MQj (gXE) , mQj (fXE) , mQj (gXE) , mQj (gXE) âˆ’
Îµ
4mK
for all j = 1, Â· Â· Â·, m. Now deï¬ne a grid on Rn+1 as follows.
Gâ€² â‰¡{Q Ã— [ai, ai+1] : Q âˆˆG, i âˆˆZ}
In words, this grid consists of all possible boxes of the form Q Ã— [ai, ai+1] where Q âˆˆG
and ai is a term of the sequence just described. It is necessary to verify that for P âˆˆGâ€²,
XP âˆˆR
Â¡
Rn+1Â¢
. This is done by showing that UGâ€² (XP )âˆ’LGâ€² (XP ) < Îµ and then noting that
Îµ > 0 was arbitrary. For Gâ€² just described, denote by Qâ€² a box in Gâ€². Thus Qâ€² = QÃ—[ai, ai+1]
for some i.
UGâ€² (XP ) âˆ’LGâ€² (XP )
â‰¡
X
Qâ€²âˆˆGâ€²
(MQâ€² (XP ) âˆ’mQâ€² (XP )) vn+1 (Qâ€²)
=
âˆ
X
i=âˆ’âˆ
m
X
j=1
Â³
MQâ€²
j (XP ) âˆ’mQâ€²
j (XP )
Â´
vn (Qj) (ai+1 âˆ’ai)
and all sums are bounded because the functions, f and g are given to be bounded. Therefore,
there are no limit considerations needed here. Thus
UGâ€² (XP ) âˆ’LGâ€² (XP ) =
m
X
j=1
vn (Qj)
âˆ
X
i=âˆ’âˆ
Â¡
MQjÃ—[ai,ai+1] (XP ) âˆ’mQjÃ—[ai,ai+1] (XP )
Â¢
(ai+1 âˆ’ai) .
Consider the inside sum with the aid of the following picture.
MQj(g)
mQj(g)
q
a
Qj
xn+1
x
xn+1 = g(x)
xn+1 = f(x)
0
0
0
0
0
0
0
0
0

580
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
In this picture, the little rectangles represent the boxes QjÃ—[ai, ai+1] for ï¬xed j. The part
of P having x contained in Qj is between the two surfaces, xn+1 = g (x) and xn+1 = f (x)
and there is a zero placed in those boxes for which MQjÃ—[ai,ai+1] (XP )âˆ’mQjÃ—[ai,ai+1] (XP ) =
0. You see, XP has either the value of 1 or the value of 0 depending on whether (x, y) is con-
tained in P. For the boxes shown with 0 in them, either all of the box is contained in P or none
of the box is contained in P. Either way, MQjÃ—[ai,ai+1] (XP )âˆ’mQjÃ—[ai,ai+1] (XP ) = 0 on these
boxes. However, on the boxes intersected by the surfaces, the value of MQjÃ—[ai,ai+1] (XP ) âˆ’
mQjÃ—[ai,ai+1] (XP ) is 1 because there are points in this box which are not in P as well
as points which are in P. Because of the construction of Gâ€² which included all values of
MQj (fXE) +
Îµ
4mK , MQj (fXE) , MQj (gXE) , mQj (fXE) , mQj (gXE) for all j = 1, Â· Â· Â·, m,
âˆ
X
i=âˆ’âˆ
Â¡
MQjÃ—[ai,ai+1] (XP ) âˆ’mQjÃ—[ai,ai+1] (XP )
Â¢
(ai+1 âˆ’ai) â‰¤
X
{i:mQj (g)â‰¤ai<MQj (g)}
1 (ai+1 âˆ’ai) +
X
{i:mQj (f)â‰¤ai<MQj (f)}
1 (ai+1 âˆ’ai)
Â³
MQj (fXE) +
Îµ
4mK âˆ’MQj (fXE)
Â´
+
Â³
mQj (gXE) âˆ’
Â³
mQj (gXE) âˆ’
Îµ
4mK
Â´Â´
=
Â¡
MQj (gXE) âˆ’mQj (gXE)
Â¢
+
Â¡
MQj (fXE) âˆ’mQj (fXE)
Â¢
+ Îµ
2m
ï£«
ï£­
m
X
j=1
v (Qj)
ï£¶
ï£¸
âˆ’1
.
(Note the inequality.) The last two terms which add to
Îµ
2m
Â³Pm
j=1 v (Qj)
Â´âˆ’1
come from the
case where ai = MQj (f) or ai+1 = mQj (f). Therefore, by 1.16,
UGâ€² (XP ) âˆ’LGâ€² (XP ) â‰¤
m
X
j=1
vn (Qj)
Â£Â¡
MQj (gXE) âˆ’mQj (gXE)
Â¢
+
Â¡
MQj (fXE) âˆ’mQj (fXE)
Â¢Â¤
+
m
X
j=1
v (Qj) Îµ
2m
ï£«
ï£­
m
X
j=1
v (Qj)
ï£¶
ï£¸
âˆ’1
=
UG (f) âˆ’LG (f) + UG (g) âˆ’LG (g) + Îµ
2
<
Îµ
4 + Îµ
4 + Îµ
2 = Îµ.
Since Îµ > 0 is arbitrary, this proves the theorem.
Corollary A.3.19 Suppose f and g are continuous functions deï¬ned on E, a contented
set in Rn and that g (x) â‰¤f (x) for all x âˆˆE. Then
P â‰¡{(x,xn+1) : x âˆˆE and g (x) â‰¤xn+1 â‰¤f (x)}
is a contented set in Rn.
Proof: Extend f and g to equal 0 oï¬€E. The set of discontinuities of f and g is contained
in âˆ‚E and Corollary A.3.10 on Page 574 implies this is a set of content 0.
Therefore,
from Theorem A.3.17, for k = f, g, it follows that kXE is in R (Rn) because the set of

A.4.
ITERATED INTEGRALS
581
discontinuities is contained in âˆ‚E. The conclusion now follows from Theorem A.3.18. This
proves the corollary.
As an example of how this can be applied, it is obvious a closed interval is a contented
set in R. Therefore, if f, g are two continuous functions with f (x) â‰¥g (x) for x âˆˆ[a, b] , it
follows from the above theorem or its corollary that the set,
P1 â‰¡{(x, y) : g (x) â‰¤y â‰¤f (x)}
is a contented set in R2. Now using the theorem and corollary again, suppose f1 (x, y) â‰¥
g1 (x, y) for (x, y) âˆˆP1 and f, g are continuous. Then the set
P2 â‰¡{(x, y, z) : g1 (x, y) â‰¤z â‰¤f1 (x, y)}
is a contented set in R3. Clearly you can continue this way obtaining examples of contented
sets.
Note that as a special case of Corollary A.3.4 on Page 571, it follows that every box is
a contented set.
A.4
Iterated Integrals
To evaluate an n dimensional Riemann integral, one uses iterated integrals. Formally, an
iterated integral is deï¬ned as follows. For f a function deï¬ned on Rn+m,
y â†’f (x, y)
is a function of y for each x âˆˆRn+m. Therefore, it might be possible to integrate this
function of y and write
Z
Rm f (x, y) dVy.
Now the result is clearly a function of x and so, it might be possible to integrate this and
write
Z
Rn
Z
Rm f (x, y) dVy dVx.
This symbol is called an iterated integral, because it involves the iteration of two lower
dimensional integrations. Under what conditions are the two iterated integrals equal to the
integral
Z
Rn+m f (z) dV ?
Deï¬nition A.4.1 Let G be a grid on Rn+m deï¬ned by the n + m sequences,
Â©
Î±i
k
Âªâˆ
k=âˆ’âˆi = 1, Â· Â· Â·, n + m.
Let Gn be the grid on Rn obtained by considering only the ï¬rst n of these sequences and
let Gm be the grid on Rm obtained by considering only the last m of the sequences. Thus a
typical box in Gm would be
n+m
Y
i=n+1
Â£
Î±i
ki, Î±i
ki+1
Â¤
, ki â‰¥n + 1
and a box in Gn would be of the form
n
Y
i=1
Â£
Î±i
ki, Î±i
ki+1
Â¤
, ki â‰¤n.

582
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
Lemma A.4.2 Let G, Gn, and Gmbe the grids deï¬ned above. Then
G = {R Ã— P : R âˆˆGn and P âˆˆGm} .
Proof: If Q âˆˆG, then Q is clearly of this form. On the other hand, if R Ã— P is one of
the sets described above, then from the above description of R and P, it follows R Ã— P is
one of the sets of G. This proves the lemma.
Now let G be a grid on Rn+m and suppose
Ï† (z) =
X
QâˆˆG
Ï†QXQâ€² (z)
(1.17)
where Ï†Q equals zero for all but ï¬nitely many Q. Thus Ï† is a step function. Recall that for
Q =
n+m
Y
i=1
[ai, bi] , Qâ€² â‰¡
n+m
Y
i=1
(ai, bi]
Letting (x, y) = z, Lemma A.4.2 implies
Ï† (z) = Ï† (x, y) =
X
RâˆˆGn
X
P âˆˆGm
Ï†RÃ—P XRâ€²Ã—P â€² (x, y)
=
X
RâˆˆGn
X
P âˆˆGm
Ï†RÃ—P XRâ€² (x) XP â€² (y) .
(1.18)
For a function of two variables, h, denote by h (Â·, y) the function, x â†’h (x, y) and
h (x, Â·) the function y â†’h (x, y) . The following lemma is a preliminary version of Fubiniâ€™s
theorem.
Lemma A.4.3 Let Ï† be a step function as described in 1.17. Then
Ï† (x, Â·) âˆˆR (Rm) ,
(1.19)
Z
Rm Ï† (Â·, y) dVy âˆˆR (Rn) ,
(1.20)
and
Z
Rn
Z
Rm Ï† (x, y) dVy dVx =
Z
Rn+m Ï† (z) dV.
(1.21)
Proof: To verify 1.19, note that Ï† (x, Â·) is the step function
Ï† (x, y) =
X
P âˆˆGm
Ï†RÃ—P XP â€² (y) .
Where x âˆˆRâ€². By Corollary A.3.4, this veriï¬es 1.19. From the description in 1.18 and this
corollary,
Z
Rm Ï† (x, y) dVy =
X
RâˆˆGn
X
P âˆˆGm
Ï†RÃ—P XRâ€² (x) v (P)
=
X
RâˆˆGn
Ãƒ X
P âˆˆGm
Ï†RÃ—P v (P)
!
XRâ€² (x) ,
(1.22)
another step function. Therefore, Corollary A.3.4 applies again to verify 1.20. Finally, 1.22
implies
Z
Rn
Z
Rm Ï† (x, y) dVy dVx =
X
RâˆˆGn
X
P âˆˆGm
Ï†RÃ—P v (P) v (R)

A.4.
ITERATED INTEGRALS
583
=
X
QâˆˆG
Ï†Qv (Q) =
Z
Rn+m Ï† (z) dV.
and this proves the lemma.
From 1.22,
MRâ€²
1
ÂµZ
Rm Ï† (Â·, y) dVy
Â¶
â‰¡sup
( X
RâˆˆGn
Ãƒ X
P âˆˆGm
Ï†RÃ—P v (P)
!
XRâ€² (x) : x âˆˆRâ€²
1
)
=
X
P âˆˆGm
Ï†R1Ã—P v (P)
(1.23)
because
R
Rm Ï† (Â·, y) dVy has the constant value given in 1.23 for x âˆˆRâ€²
1. Similarly,
mRâ€²
1
ÂµZ
Rm Ï† (Â·, y) dVy
Â¶
â‰¡inf
( X
RâˆˆGn
Ãƒ X
P âˆˆGm
Ï†RÃ—P v (P)
!
XRâ€² (x) : x âˆˆRâ€²
1
)
=
X
P âˆˆGm
Ï†R1Ã—P v (P) .
(1.24)
Theorem A.4.4 (Fubini) Let f âˆˆR (Rn+m) and suppose also that f (x, Â·) âˆˆ
R (Rm) for each x. Then
Z
Rm f (Â·, y) dVy âˆˆR (Rn)
(1.25)
and
Z
Rn+m f (z) dV =
Z
Rn
Z
Rm f (x, y) dVy dVx.
(1.26)
Proof: Let G be a grid such that UG (f) âˆ’LG (f) < Îµ and let Gn and Gm be as deï¬ned
above. Let
Ï† (z) â‰¡
X
QâˆˆG
MQâ€² (f) XQâ€² (z) , Ïˆ (z) â‰¡
X
QâˆˆG
mQâ€² (f) XQâ€² (z) .
By Corollary A.3.4, and the observation that MQâ€² (f) â‰¤MQ (f) and mQâ€² (f) â‰¥mQ (f) ,
UG (f) â‰¥
Z
Ï† dV, LG (f) â‰¤
Z
Ïˆ dV.
Also f (z) âˆˆ(Ïˆ (z) , Ï† (z)) for all z. Thus from 1.23,
MRâ€²
ÂµZ
Rm f (Â·, y) dVy
Â¶
â‰¤MRâ€²
ÂµZ
Rm Ï† (Â·, y) dVy
Â¶
=
X
P âˆˆGm
MRâ€²Ã—P â€² (f) v (P)
and from 1.24,
mRâ€²
ÂµZ
Rm f (Â·, y) dVy
Â¶
â‰¥mRâ€²
ÂµZ
Rm Ïˆ (Â·, y) dVy
Â¶
=
X
P âˆˆGm
mRâ€²Ã—P â€² (f) v (P) .
Therefore,
X
RâˆˆGn
Â·
MRâ€²
ÂµZ
Rm f (Â·, y) dVy
Â¶
âˆ’mRâ€²
ÂµZ
Rm f (Â·, y) dVy
Â¶Â¸
v (R) â‰¤

584
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
X
RâˆˆGn
X
P âˆˆGm
[MRâ€²Ã—P â€² (f) âˆ’mRâ€²Ã—P â€² (f)] v (P) v (R) â‰¤UG (f) âˆ’LG (f) < Îµ.
This shows, from Lemma A.3.5 and the Riemann criterion, that
R
Rm f (Â·, y) dVy âˆˆR (Rn) .
It remains to verify 1.26. First note
Z
Rn+m f (z) dV âˆˆ[LG (f) , UG (f) ] .
Next, by Lemma A.4.3,
LG (f) â‰¤
Z
Rn+m Ïˆ dV =
Z
Rn
Z
Rm Ïˆ dVy dVx â‰¤
Z
Rn
Z
Rm f (x, y) dVy dVx
â‰¤
Z
Rn
Z
Rm Ï† (x, y) dVy dVx =
Z
Rn+m Ï† dV â‰¤UG (f) .
Therefore,
Â¯Â¯Â¯Â¯
Z
Rn
Z
Rm f (x, y) dVy dVx âˆ’
Z
Rn+m f (z) dV
Â¯Â¯Â¯Â¯ â‰¤Îµ
and since Îµ > 0 is arbitrary, this proves Fubiniâ€™s theorem2.
Corollary A.4.5 Suppose E is a bounded contented set in Rn and let Ï†, Ïˆ be continuous
functions deï¬ned on E such that Ï† (x) â‰¥Ïˆ (x) . Also suppose f
is a continuous bounded
function deï¬ned on the set,
P â‰¡{(x, y) : Ïˆ (x) â‰¤y â‰¤Ï† (x)} ,
It follows fXP âˆˆR
Â¡
Rn+1Â¢
and
Z
P
f dV =
Z
E
Z Ï†(x)
Ïˆ(x)
f (x, y) dy dVx.
Proof: Since f is continuous, there is no problem in writing f (x, Â·) X[Ïˆ(x),Ï†(x)] (Â·) âˆˆ
R
Â¡
R1Â¢
. Also, fXP âˆˆR
Â¡
Rn+1Â¢
because P is contented thanks to Corollary A.3.19. There-
fore, by Fubiniâ€™s theorem
Z
P
f dV
=
Z
Rn
Z
R
fXP dy dVx
=
Z
E
Z Ï†(x)
Ïˆ(x)
f (x, y) dy dVx
proving the corollary.
Other versions of this corollary are immediate and should be obvious whenever encoun-
tered.
A.5
The Change Of Variables Formula
First recall Theorem 22.2.2 on Page 394 which is listed here for convenience.
2Actually, Fubiniâ€™s theorem usually refers to a much more profound result in the theory of Lebesgue
integration.

A.5.
THE CHANGE OF VARIABLES FORMULA
585
Theorem A.5.1 Let h : U â†’Rn be a C1 function with h (0) = 0,Dh (0)âˆ’1 exists.
Then there exists an open set, V âŠ†U containing 0, ï¬‚ips, F1, Â· Â· Â·, Fnâˆ’1, and primitive
functions, Gn, Gnâˆ’1, Â· Â· Â·, G1 such that for x âˆˆV,
h (x) = F1 â—¦Â· Â· Â· â—¦Fnâˆ’1 â—¦Gn â—¦Gnâˆ’1 â—¦Â· Â· Â· â—¦G1 (x) .
Also recall Theorem 16.2.13 on Page 295.
Theorem A.5.2 Let Ï† : [a, b] â†’[c, d] be one to one and suppose Ï†â€² exists and is
continuous on [a, b] . Then if f is a continuous function deï¬ned on [a, b] ,
Z d
c
f (s) ds =
Z b
a
f (Ï† (t))
Â¯Â¯Ï†â€² (t)
Â¯Â¯ dt
The following is a simple corollary to this theorem.
Corollary A.5.3 Let Ï† : [a, b] â†’[c, d] be one to one and suppose Ï†â€² exists and is
continuous on [a, b] . Then if f is a continuous function deï¬ned on [a, b] ,
Z
R
X[a,b]
Â¡
Ï†âˆ’1 (x)
Â¢
f (x) dx =
Z
R
X[a,b] (t) f (Ï† (t))
Â¯Â¯Ï†â€² (t)
Â¯Â¯ dt
Lemma A.5.4 Let h : V â†’Rn be a C1 function and suppose H is a compact subset of
V. Then there exists a constant, C independent of x âˆˆH such that
|Dh (x) v| â‰¤C |v| .
Proof: Consider the compact set, H Ã— âˆ‚B (0, 1) âŠ†R2n. Let f : H Ã— âˆ‚B (0, 1) â†’R be
given by f (x, v) = |Dh (x) v| . Then let C denote the maximum value of f. It follows that
for v âˆˆRn,
Â¯Â¯Â¯Â¯Dh (x) v
|v|
Â¯Â¯Â¯Â¯ â‰¤C
and so the desired formula follows when you multiply both sides by |v|.
Deï¬nition A.5.5 Let A be an open set. Write Ck (A; Rn) to denote a Ck function
whose domain is A and whose range is in Rn. Let U be an open set in Rn. Then h âˆˆ
Ck Â¡
U; RnÂ¢
if there exists an open set, V âŠ‡U and a function, g âˆˆC1 (V ; Rn) such that
g = h on U. f âˆˆCk Â¡
U
Â¢
means the same thing except that f has values in R.
Theorem A.5.6 Let U be a bounded open set such that âˆ‚U has zero content and
let h âˆˆC
Â¡
U; RnÂ¢
be one to one and Dh (x)âˆ’1 exists for all x âˆˆU. Then h (âˆ‚U) = âˆ‚(h (U))
and âˆ‚(h (U)) has zero content.
Proof: Let x âˆˆâˆ‚U and let g = h where g is a C1 function deï¬ned on an open set
containing U. By the inverse function theorem, g is locally one to one and an open mapping
near x. Thus g (x) = h (x) and is in an open set containing points of g (U) and points of
g
Â¡
U CÂ¢
. These points of g
Â¡
U CÂ¢
cannot equal any points of h (U) because g is one to one
locally. Thus h (x) âˆˆâˆ‚(h (U)) and so h (âˆ‚U) âŠ†âˆ‚(h (U)) . Now suppose y âˆˆâˆ‚(h (U)) .
By the inverse function theorem y cannot be in the open set h (U) . Since y âˆˆâˆ‚(h (U)),
every ball centered at y contains points of h (U) and so y âˆˆh (U)\h (U) . Thus there exists
a sequence, {xn} âŠ†U such that h (xn) â†’y. But then, by the inverse function theorem,
xn â†’hâˆ’1 (y) and so hâˆ’1 (y) âˆˆâˆ‚U. Therefore, y âˆˆh (âˆ‚U) and this proves the two sets are
equal. It remains to verify the claim about content.

586
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
First let H denote a compact set whose interior contains U which is also in the interior
of the domain of g. Now since âˆ‚U has content zero, it follows that for Îµ > 0 given, there
exists a grid, G such that if Gâ€² are those boxes of G which have nonempty intersection with
âˆ‚U, then
X
QâˆˆGâ€²
v (Q) < Îµ.
and by reï¬ning the grid if necessary, no box of G has nonempty intersection with both U
and HC. Reï¬ning this grid still more, you can also assume that for all boxes in Gâ€²,
li
lj
< 2
where li is the length of the ith side. (Thus the boxes are not too far from being cubes.)
Let C be the constant of Lemma A.5.4 applied to g on H.
Now consider one of these boxes, Q âˆˆGâ€². If x, y âˆˆQ, it follows from the chain rule that
g (y) âˆ’g (x) =
Z 1
0
Dg (x+t (y âˆ’x)) (y âˆ’x) dt
By Lemma A.5.4 applied to H
|g (y) âˆ’g (x)|
â‰¤
Z 1
0
|Dg (x+t (y âˆ’x)) (y âˆ’x)| dt
â‰¤
C
Z 1
0
|x âˆ’y| dt â‰¤C diam (Q)
=
C
Ãƒ n
X
i=1
l2
i
!1/2
â‰¤CâˆšnL
where L is the length of the longest side of Q. Thus diam (g (Q)) â‰¤CâˆšnL and so g (Q) is
contained in a cube having sides equal to CâˆšnL and volume equal to
Cnnn/2Ln â‰¤Cnnn/22nl1l2 Â· Â· Â· ln = Cnnn/22nv (Q) .
Denoting by PQ this cube, it follows
h (âˆ‚U) âŠ†âˆªQâˆˆGâ€²v (PQ)
and
X
QâˆˆGâ€²
v (PQ) â‰¤Cnnn/22n X
QâˆˆGâ€²
v (Q) < ÎµCnnn/22n.
Since Îµ > 0 is arbitrary, this shows h (âˆ‚U) has content zero as claimed.
Theorem A.5.7 Suppose f âˆˆC
Â¡
U
Â¢
where U is a bounded open set with âˆ‚U having
content 0. Then fXU âˆˆR (Rn).
Proof: Let H be a compact set whose interior contains U which is also contained in
the domain of g where g is a continuous functions whose restriction to U equals f. Consider
gXU, a function whose set of discontinuities has content 0. Then gXU = fXU âˆˆR (Rn) as
claimed. This is by the big theorem which tells which functions are Riemann integrable.
The following lemma is obvious from the deï¬nition of the integral.

A.5.
THE CHANGE OF VARIABLES FORMULA
587
Lemma A.5.8 Let U be a bounded open set and let fXU âˆˆR (Rn) . Then
Z
f (x + p) XUâˆ’p (x) dx =
Z
f (x) XU (x) dx
A few more lemmas are needed.
Lemma A.5.9 Let S be a nonempty subset of Rn. Deï¬ne
f (x) â‰¡dist (x, S) â‰¡inf {|x âˆ’y| : y âˆˆS} .
Then f is continuous.
Proof: Consider |f (x) âˆ’f (x1)|and suppose without loss of generality that f (x1) â‰¥
f (x) . Then choose y âˆˆS such that f (x) + Îµ > |x âˆ’y| . Then
|f (x1) âˆ’f (x)|
=
f (x1) âˆ’f (x) â‰¤f (x1) âˆ’|x âˆ’y| + Îµ
â‰¤
|x1 âˆ’y| âˆ’|x âˆ’y| + Îµ
â‰¤
|x âˆ’x1| + |x âˆ’y| âˆ’|x âˆ’y| + Îµ
=
|x âˆ’x1| + Îµ.
Since Îµ is arbitrary, it follows that |f (x1) âˆ’f (x)| â‰¤|x âˆ’x1| and this proves the lemma.
Theorem A.5.10 (Urysohnâ€™s lemma for Rn) Let H be a closed subset of an open
set, U. Then there exists a continuous function, g : Rn â†’[0, 1] such that g (x) = 1 for all
x âˆˆH and g (x) = 0 for all x /âˆˆU.
Proof: If x /âˆˆC, a closed set, then dist (x, C) > 0 because if not,
there would exist
a sequence of points of C converging to x and it would follow that x âˆˆC. Therefore,
dist (x, H) + dist
Â¡
x, U CÂ¢
> 0 for all x âˆˆRn. Now deï¬ne a continuous function, g as
g (x) â‰¡
dist
Â¡
x, U CÂ¢
dist (x, H) + dist (x, U C).
It is easy to see this veriï¬es the conclusions of the theorem and this proves the theorem.
Deï¬nition A.5.11 Deï¬ne spt(f) (support of f) to be the closure of the set {x :
f(x) Ì¸= 0}. If V is an open set, Cc(V ) will be the set of continuous functions f, deï¬ned on
Rn having spt(f) âŠ†V .
Deï¬nition A.5.12 If K is a compact subset of an open set, V , then K â‰ºÏ† â‰ºV if
Ï† âˆˆCc(V ), Ï†(K) = {1}, Ï†(Rn) âŠ†[0, 1].
Also for Ï† âˆˆCc(Rn), K â‰ºÏ† if
Ï†(Rn) âŠ†[0, 1] and Ï†(K) = 1.
and Ï† â‰ºV if
Ï†(Rn) âŠ†[0, 1] and spt(Ï†) âŠ†V.

588
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
Theorem A.5.13 (Partition of unity) Let K be a compact subset of Rn and sup-
pose
K âŠ†V = âˆªn
i=1Vi, Vi open.
Then there exist Ïˆi â‰ºVi with
n
X
i=1
Ïˆi(x) = 1
for all x âˆˆK.
Proof: Let K1 = K\âˆªn
i=2Vi. Thus K1 is compact because it is the intersection of a closed
set with a compact set and K1 âŠ†V1. Let K1 âŠ†W1 âŠ†W 1 âŠ†V1 with W 1compact. To obtain
W1, use Theorem A.5.10 to get f such that K1 â‰ºf â‰ºV1 and let W1 â‰¡{x : f (x) Ì¸= 0} . Thus
W1, V2, Â· Â· Â·Vn covers K and W 1 âŠ†V1. Let K2 = K \ (âˆªn
i=3Vi âˆªW1). Then K2 is compact
and K2 âŠ†V2. Let K2 âŠ†W2 âŠ†W 2 âŠ†V2 W 2 compact. Continue this way ï¬nally obtaining
W1, Â·Â·Â·, Wn, K âŠ†W1 âˆªÂ·Â·Â·âˆªWn, and W i âŠ†Vi W i compact. Now let W i âŠ†Ui âŠ†U i âŠ†Vi , U i
compact.
Wi
Ui Vi
By Theorem A.5.10, there exist functions, Ï†i, Î³ such that U i â‰ºÏ†i â‰ºVi, âˆªn
i=1W i â‰ºÎ³ â‰º
âˆªn
i=1Ui. Deï¬ne
Ïˆi(x) =
Â½ Î³(x)Ï†i(x)/ Pn
j=1 Ï†j(x) if Pn
j=1 Ï†j(x) Ì¸= 0,
0 if Pn
j=1 Ï†j(x) = 0.
If x is such that Pn
j=1 Ï†j(x) = 0, then x /âˆˆâˆªn
i=1U i. Consequently Î³(y) = 0 for all y near x
and so Ïˆi(y) = 0 for all y near x. Hence Ïˆi is continuous at such x. If Pn
j=1 Ï†j(x) Ì¸= 0, this
situation persists near x and so Ïˆi is continuous at such points. Therefore Ïˆi is continuous.
If x âˆˆK, then Î³(x) = 1 and so Pn
j=1 Ïˆj(x) = 1. Clearly 0 â‰¤Ïˆi (x) â‰¤1 and spt(Ïˆj) âŠ†Vj.
This proves the theorem.
The next lemma contains the main ideas.
Lemma A.5.14 Let U be a bounded open set with âˆ‚U having content 0. Also let h âˆˆ
C1 Â¡
U; RnÂ¢
be one to one on U with Dh (x)âˆ’1 exists for all x âˆˆU. Let f âˆˆC
Â¡
U
Â¢
be
nonnegative. Then
Z
Xh(U) (z) f (z) dVn =
Z
XU (x) f (h (x)) |det Dh (x)| dVn
Proof: Let Îµ > 0 be given. Then by Theorem A.5.7,
x â†’XU (x) f (h (x)) |det Dh (x)|
is Riemann integrable. Therefore, there exists a grid, G such that, letting
g (x) = XU (x) f (h (x)) |det Dh (x)| ,
LG (g) + Îµ > UG (g) .

A.5.
THE CHANGE OF VARIABLES FORMULA
589
Let K denote the union of the boxes, Q of G for which mQ (g) > 0. Thus K is a compact
subset of U and it is only the terms from these boxes which contribute anything nonzero to
the lower sum. By Theorem 22.2.2 on Page 394 which is stated above, it follows that for
p âˆˆK, there exists an open set contained in U which contains p,Op such that for x âˆˆOpâˆ’p,
h (x + p) âˆ’h (p) = F1 â—¦Â· Â· Â· â—¦Fnâˆ’1 â—¦Gn â—¦Â· Â· Â· â—¦G1 (x)
where the Gi are primitive functions and the Fj are ï¬‚ips. Finitely many of these open sets,
{Oj}q
j=1 cover K. Let the distinguished point for Oj be denoted by pj. Now reï¬ne G if
necessary such that the diameter of every cell of the new G which intersects U is smaller
than a Lebesgue number for this open cover. Denote by Gâ€² those boxes of G whose union
equals the set, K. Thus every box of Gâ€² is contained in one of these Oj. By Theorem A.5.13
there exists a partition of unity,
Â©
Ïˆj
Âª
on h (K) such that Ïˆj â‰ºh (Oj). Then
LG (g)
â‰¤
X
QâˆˆGâ€²
Z
XQ (x) f (h (x)) |det Dh (x)| dx
=
X
QâˆˆGâ€²
q
X
j=1
Z
XQ (x)
Â¡
Ïˆjf
Â¢
(h (x)) |det Dh (x)| dx.
(1.27)
Consider the term
R
XQ (x)
Â¡
Ïˆjf
Â¢
(h (x)) |det Dh (x)| dx. By Lemma A.5.8 and Fubiniâ€™s the-
orem this equals
Z
Rnâˆ’1
Z
R
XQâˆ’pj (x)
Â¡
Ïˆjf
Â¢
(h (pi) + F1 â—¦Â· Â· Â· â—¦Fnâˆ’1 â—¦Gn â—¦Â· Â· Â· â—¦G1 (x)) Â·
|DF (Gn â—¦Â· Â· Â· â—¦G1 (x))| |DGn (Gnâˆ’1 â—¦Â· Â· Â· â—¦G1 (x))| |DGnâˆ’1 (Gnâˆ’2 â—¦Â· Â· Â· â—¦G1 (x))| Â·
Â· Â· Â· |DG2 (G1 (x))| |DG1 (x)| dx1dVnâˆ’1.
(1.28)
Here dVnâˆ’1 is with respect to the variables, x2, Â· Â· Â·, xn. Also F denotes F1 â—¦Â· Â· Â· â—¦Fnâˆ’1. Now
G1 (x) = (Î± (x) , x2, Â· Â· Â·, xn)T
and is one to one. Therefore, ï¬xing x2, Â· Â· Â·, xn, x1 â†’Î± (x) is one to one. Also, DG1 (x) =
âˆ‚Î±
âˆ‚x1 (x) . Fixing x2, Â· Â· Â·, xn, change the variable,
y1 = Î± (x1, x2, Â· Â· Â·, xn) .
Thus
x = (x1, x2, Â· Â· Â·, xn)T = Gâˆ’1
1
(y1, x2, Â· Â· Â·, xn) â‰¡Gâˆ’1
1
(xâ€²)
Then in 1.28 you can use Corollary A.5.3 to write 1.28 as
Z
Rnâˆ’1
Z
R
XQâˆ’pj
Â¡
Gâˆ’1
1
(xâ€²)
Â¢ Â¡
Ïˆjf
Â¢ Â¡
h (pi) + F1 â—¦Â· Â· Â· â—¦Fnâˆ’1 â—¦Gn â—¦Â· Â· Â· â—¦G1
Â¡
Gâˆ’1
1
(xâ€²)
Â¢Â¢
Â·
Â¯Â¯DF
Â¡
Gn â—¦Â· Â· Â· â—¦G1
Â¡
Gâˆ’1
1
(xâ€²)
Â¢Â¢Â¯Â¯ Â¯Â¯DGn
Â¡
Gnâˆ’1 â—¦Â· Â· Â· â—¦G1
Â¡
Gâˆ’1
1
(xâ€²)
Â¢Â¢Â¯Â¯ Â·
Â¯Â¯DGnâˆ’1
Â¡
Gnâˆ’2 â—¦Â· Â· Â· â—¦G1
Â¡
Gâˆ’1
1
(xâ€²)
Â¢Â¢Â¯Â¯ Â· Â· Â·
Â¯Â¯DG2
Â¡
G1
Â¡
Gâˆ’1
1
(xâ€²)
Â¢Â¢Â¯Â¯ dy1dVnâˆ’1
(1.29)
which reduces to
Z
Rn XQâˆ’pj
Â¡
Gâˆ’1
1
(xâ€²)
Â¢ Â¡
Ïˆjf
Â¢
(h (pi) + F1 â—¦Â· Â· Â· â—¦Fnâˆ’1 â—¦Gn â—¦Â· Â· Â· â—¦G2 (xâ€²)) Â·
|DF (Gn â—¦Â· Â· Â· â—¦G2 (xâ€²))| |DGn (Gnâˆ’1 â—¦Â· Â· Â· â—¦G2 (xâ€²))| |DGnâˆ’1 (Gnâˆ’2 â—¦Â· Â· Â· â—¦G2 (xâ€²))| Â·
Â· Â· Â· |DG2 (xâ€²)| dVn.
(1.30)

590
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—
Now use Fubiniâ€™s theorem again to make the inside integral taken with respect to x2. Exactly
the same process yields
Z
Rnâˆ’1
Z
R
XQâˆ’pj
Â¡
Gâˆ’1
1
â—¦Gâˆ’1
2
(xâ€²â€²)
Â¢ Â¡
Ïˆjf
Â¢
(h (pi) + F1 â—¦Â· Â· Â· â—¦Fnâˆ’1 â—¦Gn â—¦Â· Â· Â· â—¦G3 (xâ€²â€²)) Â·
|DF (Gn â—¦Â· Â· Â· â—¦G3 (xâ€²â€²))| |DGn (Gnâˆ’1 â—¦Â· Â· Â· â—¦G3 (xâ€²â€²))| |DGnâˆ’1 (Gnâˆ’2 â—¦Â· Â· Â· â—¦G3 (xâ€²â€²))| Â·
Â· Â· Â·dy2dVnâˆ’1.
(1.31)
Now F is just a composition of ï¬‚ips and so |DF (Gn â—¦Â· Â· Â· â—¦G3 (xâ€²â€²))| = 1 and so this term
can be replaced with 1. Continuing this process, eventually yields an expression of the form
Z
Rn XQâˆ’pj
Â¡
Gâˆ’1
1
â—¦Â· Â· Â· â—¦Gâˆ’1
nâˆ’2 â—¦Gâˆ’1
nâˆ’1 â—¦Gâˆ’1
n
â—¦Fâˆ’1 (y)
Â¢ Â¡
Ïˆjf
Â¢
(h (pi) + y) dVn.
(1.32)
Denoting by Gâˆ’1 the expression, Gâˆ’1
1
â—¦Â· Â· Â· â—¦Gâˆ’1
nâˆ’2 â—¦Gâˆ’1
nâˆ’1 â—¦Gâˆ’1
n ,
XQâˆ’pj
Â¡
Gâˆ’1
1
â—¦Â· Â· Â· â—¦Gâˆ’1
nâˆ’2 â—¦Gâˆ’1
nâˆ’1 â—¦Gâˆ’1
n
â—¦Fâˆ’1 (y)
Â¢
= 1
exactly when Gâˆ’1 â—¦Fâˆ’1 (y) âˆˆQ âˆ’pj. Now recall that h (pj + x) âˆ’h (pj) = F â—¦G (x) and
so the above holds exactly when
y
=
h
Â¡
pj + Gâˆ’1 â—¦Fâˆ’1 (y)
Â¢
âˆ’h (pj) âˆˆh (pj + Q âˆ’pj) âˆ’h (pj)
=
h (Q) âˆ’h (pj) .
Thus 1.32 reduces to
Z
Rn Xh(Q)âˆ’h(pj) (y)
Â¡
Ïˆjf
Â¢
(h (pi) + y) dVn =
Z
Rn Xh(Q) (z)
Â¡
Ïˆjf
Â¢
(z) dVn.
It follows from 1.27
UG (g) âˆ’Îµ
â‰¤
LG (g) â‰¤
X
QâˆˆGâ€²
Z
XQ (x) f (h (x)) |det Dh (x)| dx
=
X
QâˆˆGâ€²
q
X
j=1
Z
XQ (x)
Â¡
Ïˆjf
Â¢
(h (x)) |det Dh (x)| dx
=
X
QâˆˆGâ€²
q
X
j=1
Z
Rn Xh(Q) (z)
Â¡
Ïˆjf
Â¢
(z) dVn
=
X
QâˆˆGâ€²
Z
Rn Xh(Q) (z) f (z) dVn â‰¤
Z
Xh(U) (z) f (z) dVn
which implies the inequality,
Z
XU (x) f (h (x)) |det Dh (x)| dVn â‰¤
Z
Xh(U) (z) f (z) dVn
But now you can use the same information just derived to obtain equality. x = hâˆ’1 (z) and
so from what was just done,
Z
XU (x) f (h (x)) |det Dh (x)| dVn

A.6.
SOME OBSERVATIONS
591
=
Z
Xhâˆ’1(h(U)) (x) f (h (x)) |det Dh (x)| dVn
â‰¥
Z
Xh(U) (z) f (z)
Â¯Â¯det Dh
Â¡
hâˆ’1 (z)
Â¢Â¯Â¯ Â¯Â¯det Dhâˆ’1 (z)
Â¯Â¯ dVn
=
Z
Xh(U) (z) f (z) dVn
from the chain rule. In fact,
I = Dh
Â¡
hâˆ’1 (z)
Â¢
Dhâˆ’1 (z)
and so
1 =
Â¯Â¯det Dh
Â¡
hâˆ’1 (z)
Â¢Â¯Â¯ Â¯Â¯det Dhâˆ’1 (z)
Â¯Â¯ .
This proves the lemma.
The change of variables theorem follows.
Theorem A.5.15 Let U be a bounded open set with âˆ‚U having content 0. Also
let h âˆˆC1 Â¡
U; RnÂ¢
be one to one on U with Dh (x)âˆ’1 exists for all x âˆˆU. Let f âˆˆC
Â¡
U
Â¢
.
Then
Z
Xh(U) (z) f (z) dz =
Z
XU (x) f (h (x)) |det Dh (x)| dx
Proof: You note that the formula holds for f + â‰¡|f|+f
2
and f âˆ’â‰¡|f|âˆ’f
2
. Now f =
f + âˆ’f âˆ’and so
Z
Xh(U) (z) f (z) dz
=
Z
Xh(U) (z) f + (z) dz âˆ’
Z
Xh(U) (z) f âˆ’(z) dz
=
Z
XU (x) f + (h (x)) |det Dh (x)| dx âˆ’
Z
XU (x) f âˆ’(h (x)) |det Dh (x)| dx
=
Z
XU (x) f (h (x)) |det Dh (x)| dx.
A.6
Some Observations
Some of the above material is very technical. This is because it gives complete answers to
the fundamental questions on existence of the integral and related theoretical considerations.
However, most of the diï¬ƒculties are artifacts. They shouldnâ€™t even be considered! It was
realized early in the twentieth century that these diï¬ƒculties occur because, from the point
of view of mathematics, this is not the right way to deï¬ne an integral! Better results are
obtained much more easily using the Lebesgue integral. Many of the technicalities related
to Jordan content disappear almost magically when the right integral is used. However, the
Lebesgue integral is more abstract than the Riemann integral and it is not traditional to
consider it in a beginning calculus course. If you are interested in the fundamental properties
of the integral and the theory behind it, you should abandon the Riemann integral which is
an antiquated relic and begin to study the integral of the last century. An introduction to
it is in [21]. Another very good source is [11]. This advanced calculus text does everything
in terms of the Lebesgue integral and never bothers to struggle with the inferior Riemann
integral. A more general treatment is found in [17], [18], [22], and [19]. There is also a still
more general integral called the generalized Riemann integral. A recent book on this subject
is [5]. It is far easier to deï¬ne than the Lebesgue integral but the convergence theorems are
much harder to prove. An introduction is also in [17].

592
THE THEORY OF THE RIEMANN INTEGRALâˆ—âˆ—

Bibliography
[1] Apostol, T. M., Calculus second edition, Wiley, 1967.
[2] Apostol T. Calculus Volume II Second edition, Wiley 1969.
[3] Apostol, T. M., Mathematical Analysis, Addison Wesley Publishing Co., 1974.
[4] Baker, Roger, Linear Algebra, Rinton Press 2001.
[5] Bartle R.G., A Modern Theory of Integration, Grad. Studies in Math., Amer. Math.
Society, Providence, RI, 2000.
[6] Chahal J. S. , Historical Perspective of Mathematics 2000 B.C. - 2000 A.D.
[7] Davis H. and Snider A., Vector Analysis Wm. C. Brown 1995.
[8] Dâ€™Angelo, J. and West D. Mathematical Thinking Problem Solving and Proofs,
Prentice Hall 1997.
[9] Edwards C.H. Advanced Calculus of several Variables, Dover 1994.
[10] Fitzpatrick P. M., Advanced Calculus a course in Mathematical Analysis, PWS Pub-
lishing Company 1996.
[11] Fleming W., Functions of Several Variables, Springer Verlag 1976.
[12] Greenberg, M. Advanced Engineering Mathematics, Second edition, Prentice Hall,
1998
[13] Gurtin M. An introduction to continuum mechanics, Academic press 1981.
[14] Hardy G., A Course Of Pure Mathematics, Tenth edition, Cambridge University Press
1992.
[15] Horn R. and Johnson C. matrix Analysis, Cambridge University Press, 1985.
[16] Karlin S. and Taylor H. A First Course in Stochastic Processes, Academic Press,
1975.
[17] Kuttler K. L., Basic Analysis, Rinton
[18] Kuttler K.L., Modern Analysis CRC Press 1998.
[19] Lang S. Real and Functional analysis third edition Springer Verlag 1993. Press, 2001.
[20] Nobel B. and Daniel J. Applied Linear Algebra, Prentice Hall, 1977.
[21] Rudin, W., Principles of mathematical analysis, McGraw Hill third edition 1976
593

594
BIBLIOGRAPHY
[22] Rudin W., Real and Complex Analysis, third edition, McGraw-Hill, 1987.
[23] Salas S. and Hille E., Calculus One and Several Variables, Wiley 1990.
[24] Sears and Zemansky, University Physics, Third edition, Addison Wesley 1963.
[25] Tierney John, Calculus and Analytic Geometry, fourth edition, Allyn and Bacon,
Boston, 1969.

Index
C1, 374
Ck, 374
âˆ†, 505
âˆ‡2, 505
adjugate, 204, 238
agony, pain and suï¬€ering, 409
algebraic multiplicity, 215
angle between planes, 74
angle between vectors, 40
angular velocity, 55
angular velocity vector, 300
arc length, 265, 295, 459
area of a parallelogram, 49
augmented matrix, 85
back substitution, 84
balance of momentum, 514
barallelepiped
volume, 52
bases, 170
basic variables, 92
basis, 170
binormal, 289
block matrix, 145
bounded, 329
box product, 52
Cartesian coordinates, 20
catenary, 347
Cauchy Schwarz inequality, 42
Cauchy sequence, 331
Cauchy sequence, 331
Cauchy stress, 516
Cayley Hamilton theorem, 241
center of mass, 57, 445, 448
center of mass of a plate , 411
center of mass of a surface, 493
centroid, 449
chain rule, 385
change of variables formula, 433
characteristic equation, 211
characteristic polynomial, 241
characteristic value, 210
circular helix, 290
classical adjoint, 204
closed set, 308
cofactor, 200, 236
cofactor matrix, 200
complement, 308
complex eigenvalues, 227
component, 35, 63
component of a force, 46
components of a matrix, 122
conformable, 126
conservation of linear momentum, 278
conservation of mass, 514
conservative, 468, 533
consistent, 94
constitutive laws, 519
contented set, 573
continuity
limit of a sequence, 333
continuous function, 311
continuous functions
properties, 313
converge, 331
Coordinates, 19
Cramerâ€™s rule, 238
critical point, 357
cross product, 49
area of parallelogram, 49
coordinate description, 50
distributive law, 57
geometric description, 49
limits, 318
curl, 505
curvature, 282, 289
defective, 216
defective eigenvalue, 216
deformation gradient, 515
density and mass, 410
density with respect to area, 493
dependent, 116
595

596
INDEX
derivative of a function, 263
determinant, 232
product, 235
transpose, 233
diagonalizable, 219
diameter, 329
diï¬€erence quotient, 263
diï¬€erentiable matrix, 297
diï¬€erentiation rules, 269
directed line segment, 30
direction cosines, 44
direction vector, 28, 30
directional derivative, 319
distance formula, 24
divergence, 505
divergence theorem, 506
Dolittleâ€™s method, 157
domain, 262, 307
dominant eigenvalue, 552
donut, 491
dot product, 39
echelon form, 86
eigenspace, 213
eigenvalue, 210
eigenvalues, 241
eigenvector, 210
Einstein summation convention, 60
elementary matrices, 138
entries of a matrix, 122
equality of mixed partial derivatives, 325
Eulerian coordinates, 515
Fibonacci sequence, 331
force, 33
force ï¬eld, 464
free variables, 92
Frenet Serret formulas, 290
fundamental theorem line integrals, 468
Gauss Elimination, 94
Gauss elimination, 86
Gauss Jordan method for inverses, 134
Gauss Seidel method, 546
Gaussâ€™s theorem, 506
general solution, 189
geometric multiplicity, 216
Gerschgorinâ€™s theorem, 228
gradient, 321
Greenâ€™s theorem, 479, 528
grid, 404, 405, 416
grids, 565
Heine Borel, 293
Hessian matrix, 359, 386
homogeneous equations, 95
implicit function theorem, 390
impulse, 278
inconsistent, 91, 94
increment of area, 412
independent, 116
inner product, 39
intercepts, 76
intercepts of a surface, 259
interior point, 308
inverses and determinants, 206, 237
invertible, 132
iterated integrals, 406
Jacobian, 430
Jacobian determinant, 432
Jocobi method, 543
Jordan content, 574
Jordan set, 573
joule, 47
ker, 188
kernel, 188
kilogram, 56
kinetic energy, 277
Kroneker delta, 60
Lagrange multipliers, 366, 393, 394
Lagrange remainder, 387
Lagrangian coordinates, 514
Laplace expansion, 200, 236
leading entry, 86
Lebesgue number, 577
Lebesgueâ€™s theorem, 578
length of smooth curve, 266
limit of a function, 262, 315
limits and continuity, 317
line integral, 465
linear combination, 96, 111, 160, 234
linear momentum, 278
linear transformation, 182, 372
linearly independent, 167
lizards
surface area, 488
local extremum, 356
local maximum, 356
local minimum, 356
lower sum, 418, 566

INDEX
597
main diagonal, 201
mass ballance, 514
material coordinates, 514
matrix, 121
inverse, 132
left inverse, 238
lower triangular, 200, 238
right inverse, 238
self adjoint, 248
symmetric, 248
upper triangular, 200, 238
migration matrix, 224
minor, 200, 236
mixed partial derivatives, 323
moment of a force, 54
moment of inertia, 447
motion, 515
moving coordinate system, 298
multi-index, 312
nested interval lemma, 328
Newton, 36
second law, 273
Newtonâ€™s laws, 273
nondefective eigenvalue, 216
normal vector to plane, 73
null space, 188
nullity, 174
one to one, 182
onto, 182
open set, 308
orientable, 532
orientation, 464
oriented curve, 464
origin, 19
orthogonal, 41
osculating plane, 282, 288
parallelepiped, 52
parameter, 30, 262
parameterization, 262, 295
parametric equation, 30
parametrization, 265, 459
partial derivative, 321
partition of unity, 588
permutation matrices, 138
permutation symbol, 60
perpendicular, 41
Piola Kirchhoï¬€stress, 519
pivot, 91
pivot column, 87, 98, 161
pivot position, 87
plane containing three points, 75
planes, 73
polynomials in n variables, 312
position vector, 21, 22, 33
power method, 551
precession of a top, 443
pretentious jargon
hyper plane, 78, 82
oid, 257
principal normal, 282, 289
product rule
cross product, 269
dot product, 269
matrices, 297
projection of a vector, 46
quadric surfaces, 257
radius of curvature, 282, 288
rank of a matrix, 163, 239
rank theorem, 95
raw eggs, 447
recurrence relation, 331
recursively deï¬ned sequence, 331
reï¬nement of a grid, 405, 416
reï¬nement of grids, 565
resultant, 35
Riemann criterion, 567
Riemann integral, 405, 416
Riemann integral, 567
right handed system, 48
rot, 505
row equivalent, 98, 162
row operations, 96, 138, 159, 201
row reduced echelon form, 97, 160
saddle point, 359
scalar ï¬eld, 505
scalar multiplication, 20
scalar potential, 468
scalar product, 39
scalars, 20, 121
scaling factor, 552
second derivative test, 388
separable diï¬€erential equations, 344
sequences, 330
sequential compactness, 292, 332
sequentially compact, 332
shifted inverse power method, 556
simultaneous corrections, 543
singular point, 357

598
INDEX
skew lines, 81, 100
skew symmetric, 131
smooth curve, 265, 295, 459
smooth surface, 485
solution set, 83
solution space, 188
spacial coordinates, 515
span, 111, 160, 234
spanning set, 111
spectrum, 210
speed, 36
spherical coordinates, 381
standard matrix, 372
standard position, 33
Stokeâ€™s theorem, 530
support of a function, 587
symmetric, 131
symmetric form of a line, 31, 32
torque vector, 54
torsion, 289
torus, 491
trace of a surface, 259
traces, 76
triangle inequality, 26, 43
unit tangent vector, 282, 289
unit vector, 28
upper sum, 418, 566
vector, 21
vector ï¬eld, 462, 464, 505
vector ï¬elds, 463
vector potential, 536
vector valued function
continuity, 312
derivative, 263
integral, 263
limit theorems, 316
vectors, 32
velocity, 36
volume element, 432
work, 465


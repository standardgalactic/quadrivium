MULTILEVEL SECURE 
TRANSACTION PROCESSING 

The Kluwer International Series on 
ADVANCES IN DATABASE SYSTEMS 
Series Editor 
Ahmed K. Elmagarmid 
Other books in the Series: 
Purdue University 
West Lafayette, IN 47907 
FUZZY LOGIC IN DATA MODELING, Guoqing Chen ISBN: 0-7923-8253-6 
INTERCONNECTING HETEROGENEOUS INFORMATION SYSTEMS, Athman 
Bouguettaya, Boualem Benatallah, Ahmed Elmagarmid ISBN: 0-7923-8216-1 
FOUNDATIONS OF KNOWLEDGE SYSTEMS: With Applications to Databases 
and Agents, Gerd Wagner ISBN: 0-7923-8212-9 
DATABASE RECOVERY, Vijay Kumar, Sang H. Son ISBN: 0-7923-8192-0 
PARALLEL, OBJECT-ORIENTED, AND ACTIVE KNOWLEDGE BASE 
SYSTEMS,Ioannis Vlahavas, Nick Bassiliades ISBN: 0-7923-8117-3 
DATA MANAGEMENT FOR MOBILE COMPUTING, Evaggelia Pitoura, George 
Samaras ISBN: 0-7923-8053-3 
MINING VERY LARGE DATABASES WITH PARALLEL PROCESSING, Alex 
A. Freitas, Simon H. Lavington ISBN: 0-7923-8048-7 
INDEXING TECHNIQUES FOR ADVANCED DATABASE SYSTEMS, Elisa 
Bertino, Beng Chin Ooi, Ron Sacks-Davis, Kian-Lee Tan, Justin Zobel, Boris 
Shidlovsky, Barbara Catania ISBN: 0-7923-9985-4 
INDEX DATA STRUCTURES IN OBJECT-ORIENTED DATABASES, Thomas 
A. Mueck, Martin L. Polaschek ISBN: 0-7923-9971-4 
DATABASE ISSUES IN GEOGRAPHIC INFORMATION SYSTEMS, Nabil R. 
Adam, Aryya Gangopadhyay ISBN: 0-7923-9924-2 
VIDEO DATABASE SYSTEMS: Issues, Products, and Applications, Ahmed K. 
Elmagarmid, Haitao Jiang, Abdelsalam A. Helal, Anupam Joshi, Magdy Ahmed 
ISBN: 0-7923-9872-6 
REPLICATION TECHNIQUES IN DISTRIBUTED SYSTEMS, Abdelsalam A. 
Helal, AbdelsalamA. Heddaya, Bharat B. Bhargava ISBN: 0-7923-9800-9 
SEARCHING MULTIMEDIA DATABASES BY CONTENT, Christos Faloutsos 
ISBN: 0-7923-9777-0 
TIME-CONSTRAINED 
TRANSACTION 
MANAGEMENT: 
Real-Time 
Constraints in Database Transaction Systems, Nandit R. Soparkar, Henry F. 
Korth, Abraham Silberschatz ISBN: 0-7923-9752-5 
DATABASE CONCURRENCY CONTROL: Methods, Performance, and Analysis, 
Alexander Thomasian, IBM T. J. Watson Research Center ISBN: 0-7923-9741-X 

MULTILEVEL SECURE 
TRANSACTION PROCESSING 
by 
Vijay Atluri 
Rutgers University 
Sushil Jajodia 
George Mason University 
Binto George 
Westem Illinois University 
SPRINGER SCIENCE+BUSINESS MEDIA, LLC 

Library of Congress Cataloging-in-Publication Data 
Atluri, Vijay, 1956-
Multilevel secure transaction processing / by Vijay Atluri, Sushil Jajodia, Binto George. 
p. cm. -- (The Kluwer international series on advances in database systems ; 16) 
Includes bibliographical references and index. 
ISBN 978-1-4613-7055-0 
ISBN 978-1-4615-4553-8 (eBook) 
DOI 10.1007/978-1-4615-4553-8 
1. Database management. 2. Transaction systems (Computer systems) 3. Database 
security. 1. Jajodia, Sushil. II. George, Binto, 1970- III. Title. IV. Series. 
QA76.9.D3 A894 1999 
005.4' 34--dc21 
Copyright @ 2000 Springer Science+Business Media New York 
Originally published by Kluwer Academic Publishers in 2000 
Softcover reprint ofthe hardcover lst edition 2000 
99-047413 
AII rights reserved. No part of this publication may be reproduced, stored in a 
retrieval system or transmitted in any form or by any means, mechanical, photo-
copying, recording, or otherwise, without the prior written permission of the 
publisher, Springer Science+Business Media, LLC. 
Printed on acid-free paper. 

Contents 
List of Figures 
ix 
List of Tables 
xi 
Preface 
xiii 
1. INTRODUCTION 
1 
1 
Multilevel Secure Databases 
2 
2 
3 
1.1 
Why DAC is not enough? 
4 
1.2 
Additional Threats to Security: Covert Channels 
6 
1.3 
Multilevel Secure DBMS Architectures 
8 
Advanced Database Application Domains 
Organization of the book 
12 
l3 
2. TRADITIONAL TRANSACTION PROCESSING 
15 
15 
16 
18 
1 
ACID Requirements 
2 
Concurrency Control Protocols 
3 
Commit Protocols 
3. TRANSACTION PROCESSING IN MLS DATABASES 
21 
1 
Problems with solutions in traditional environment 
21 
2 
Revised Requirements 
23 
3 
Commercial Solutions 
24 
4 
Research Solutions for Replicated Architecture 
25 
5 
Research Solutions for Kemelized Architecture 
27 
5.1 
Single version algorithms 
27 
5.2 
Multiversion algorithms 
28 
5.2.1 
Keefe-Tsai's scheduler 
29 
5.2.2 
Jajodia-Atluri's scheduler 
29 
5.2.3 
Snapshot Approaches 
30 
5.3 
Weaker Correctness Criteria 
31 
5.3.1 
Levelwise Serializability 
31 
5.3.2 
One-item Read Serializability 
33 
5.3.3 
Pairwise Serializability 
34 
5.4 
Using Transaction Analysis to Achieve Serializability 
35 

vi 
MULTILEVEL SECURE TRANSACTION PROCESSING 
5.5 
Multilevel Transactions 
36 
5.6 
Secure Commit Protocols 
37 
5.6.1 
Secure Early Prepare (SEP) 
39 
4. SECURE TRANSACTION PROCESSING IN REAL-
TIME DATABASES 
43 
1 
Introduction to Real-Time Databases 
44 
1.1 
Priority Assignment Policies 
47 
2 
Additional Requirements 
47 
2.1 
Secure Priority Assignment 
47 
3 
Concurrency Control 
48 
3.1 
Real-Time protocols 
48 
3.1.1 
2PL High Priority 
49 
3.1.2 
OPT-SACRIFICE 
49 
3.1.3 
OPT-WAIT 
50 
3.1.4 
Performance Evaluation 
50 
3.2 
Adaptive Secure 2PL (AS2PL) 
52 
3.3 
Dual Approach 
54 
3.3.1 
S2PL-WAIT 
56 
3.4 
Secure Real-Time Two-Phase Locking Protocol 
56 
3.5 
Fairness 
59 
4 
Conclusions 
61 
5. SECURE WORKFLOW TRANSACTION PROCESSING 
63 
1 
Introduction to Workflow Systems 
64 
1.1 
Workflow Technology and Products 
64 
1.2 
Workflow Product Categories 
65 
1.3 
Workflow Standards 
66 
2 
Workflow Transaction Model 
67 
3 
Multilevel Secure Workflow Model 
69 
4 
Solutions to Execute MLS Workflows 
70 
4.1 
Semantic Classification of Task Dependencies in MLS 
workflows 
71 
4.2 
Redesigning of MLS Workflows 
75 
4.2.1 
Splitting the High Task: 
75 
4.2.2 
Running a Compensating Task 
76 
4.2.3 
Summary of the Redesigning Process 
78 
4.3 
Alternative Solutions: Maintaining Multiple Versions: 
78 
5 
The MLS WFMS Architecture 
79 
6. SECURE BUFFER MANAGEMENT 
81 
1 
Introduction to Buffer Management 
81 
2 
Requirements 
82 
3 
Solutions 
83 
3.1 
*-DBS Buffering System 
84 
3.1.1 
Buffer Allocation 
84 
3.1.2 
Replacement 
85 
3.1.3 
Synchronization 
85 

3.1.4 
Design Issues 
3.1.5 
Implementation 
3.2 
Performance Analysis 
4 
Secure Real-Time Database Buffer Management 
4.1 
Secure Buffering Algorithm for RTDBS 
4.1.1 
Security Features 
4.1.2 
Real-Time Features 
4.1.3 
Search and Slot Selection 
4.1.4 
Fairness Features 
4.2 
Performance Study 
5 
Conclusions 
7. APPLICATIONS TO HIERARCHICAL AND 
REPLICATED DATABASES 
1 
Hierarchical Databases 
1.1 
Hsu-Chan Protocol 
1.2 
Ammann et al. Protocol 
2 
Replicated Databases 
8. CHALLENGES 
1 
Using advanced transaction model paradigms 
2 
Secure Long Duration Transaction Processing 
3 
Recovery Methods 
References 
Index 
Contents 
vii 
86 
87 
87 
88 
88 
89 
89 
90 
91 
91 
93 
95 
95 
97 
97 
99 
105 
105 
107 
109 
111 
123 

List of Figures 
1.1 
Before the Trojan Horse Attack 
5 
1.2 
After the Trojan Horse Attack 
6 
1.3 
Example of a Covert Channel 
7 
1.4 
The Integrity Lock Architecture 
9 
1.5 
Vulnerability of The Integrity Lock Architecture 
9 
1.6 
The Kernelized Architecture 
10 
1.7 
The Replicated Architecture 
11 
1.8 
The Trusted Subject Architecture 
12 
2.1 
An incorrect concurrent execution 
16 
2.2 
A correct concurrent execution 
17 
2.3 
Basic 2PC Protocol 
19 
2.4 
EPProtocol 
20 
3.1 
A covert channel with two-phase locking protocol 
22 
3.2 
A non-serializable history 
22 
3.3 
The serialization graph corresponding the history in fig-
ure 3.2 
23 
3.4 
A Crown 
26 
3.5 
A multiversion schedule that can be generated by the 
Trusted Oracle 
33 
3.6 
The distributed schedule D 
38 
3.7 
SEP Protocol 
41 
4.1 
Utility Function 
45 
4.2 
Real-Time Database 
46 
4.3 
Secure Priority Assignment 
48 
4.4 
Secure Real-Time Concurrency Control 
48 
4.5 
Adaptive Secure 2PL 
53 
4.6 
MCC architecture for Dual Approach 
55 
4.7 
Secure Real-Time Two-Phase Locking Protocol 
57 
4.8 
GUARD Admission Control 
59 

x 
MULTILEVEL SECURE TRANSACTION PROCESSING 
5.1 
Inter -level dependencies of the multilevel workflow trans-
action in example 1 
70 
5.2 
Task dependencies in the MLS workflow in example 5.2 
72 
5.3 
Modified Workflow after executing split task for exam-
ple 5.1 
76 
5.4 
An example demonstrating the closest ancestor 
76 
5.5 
Modified Workflow after executing compensate task for 
example 5.2 
78 
5.6 
Approach for redesigning each type of dependency 
78 
5.7 
The MLS WFMS Architecture 
80 
7.1 
An Example of Class Hierarchy in a Hierarchically De-
composed Database 
96 
7.2 
A Planar Lattice 
99 
7.3 
A crown-free DAG 
101 
7.4 
Two forests produced by the DAG protocol on the DAG 
of figure 7.3 
102 
7.5 
A DAG with crowns 
103 
7.6 
A solution with a trusted node 
103 
7.7 
A non-forest solution 
104 

List of Tables 
5.1 
5.2 
A list of possible control flow dependencies 
Some RD type dependencies and their inverse 
68 
77 

Preface 
Information security has been gaining a great deal of importance as comput-
ers are increasingly being used to process sensitive information. A multilevel 
secure database management system (MLS DBMS) is designed to store, re-
trieve and process information in compliance with certain mandatory security 
requirements, essential for protecting sensitive information from unauthorized 
access, modification and abuse. Such systems are characterized by data objects 
labeled at different security levels and accessed by users cleared to appropri-
ate security levels. Unless transaction processing modules for these systems 
are designed carefully, they can be exploited by clever malicious users to leak 
sensitive information to unauthorized users. 
Considerable research effort has been devoted since 1990 that has impacted 
the design and development of trusted MLS DBMS products. This book is a 
reflection of the progress and achievements made in this area. It covers the 
state-of-the-art of the research in developing secure transaction processing for 
popular MLS DBMS architectures: kemelized, replicated, and for distributed 
MLS DBMS as well as with advanced transaction models such as workflows, 
long duration and nested. Further, it explores the technical challenges that 
require future attention. 
This book comprises of three logical parts. The first part of the book provides 
introduction and identifies the challenges in secure transaction processing. In 
particular, it gives an introduction to MLS databases including the different 
MLS DBMS architectures and an overview of traditional transaction process-
ing approaches. It then identifies the desirable properties and the additional 
requirements imposed by multilevel security on transaction processing. It ex-
plains why conventional transaction processing techniques can conflict with the 
multilevel security constraints, discusses how they must be modified to comply 
with the security policy, and notes the challenges of doing so with acceptable 
responsiveness and with little or no trusted code. 

xiv 
MULTILEVEL SECURE TRANSACTION PROCESSING 
The second part of the book provides secure transaction processing solutions 
for conventional databases. It examines the published solutions adopted by 
commercial vendors in their trusted DBMS products, that is, the extent to which 
they have succeeded in meeting the competing needs of multilevel transaction 
processing and efficiency. It presents secure concurrency control algorithms, 
based on locking and multiversion timestamp ordering, developed for replicated 
and kemelized trusted DBMS architectures, and provides an assessment of 
them. This book describes research in multilevel transaction correctness, where 
an individual transaction are able to write data at multiple security levels that 
leads to an additional trade-off between security and atomicity. It then discusses 
distributed multilevel secure DBMSs and describes the research on the impact 
of multilevel security on commit protocols for coordinating the execution of 
distributed transactions. Finally, this part discusses the impact of real-time 
constraints on secure transaction processing and reviews the research solutions 
in this area. 
The third part deals with secure transaction processing in advanced applica-
tion environments and more recent issues addressed. In particular, it discusses 
issues of transaction processing considering advanced transaction models such 
as workflow models and the buffer management issues in a MLS DBMS en-
vironment. It also presents the application of secure transaction processing 
solutions to hierarchical and replicated databases. Finally, the book concludes 
by identifying technical challenges in multilevel transaction processing that 
have yet to receive significant attention, including secure transaction process-
ing using advanced transaction models such as nested and long duration models, 
and secure recovery. 
This book is targeted towards researchers and developers in the area of mul-
tilevel secure database systems. It can also serve as a reference book for a 
graduate course on Database Security, Information Systems Security, Advanced 
Database Systems, and Transaction Processing. 
Acknowledgments: The authors would like to acknowledge many contributors 
of this book. We thank the series editors, Ahmed Elmagarmid and Scott Delman, 
for their encouragement of this project, and Melissa Fearon of Kluwer for 
handling the logistics of getting the book through the publication process. We 
are grateful to Tom Keefe for his contribution to the section on recovery in 
chapter 8, Wei-Kuang Huang for allowing to use material from his dissertation 
in chapter 5, and Jayant Haritsa for his input to chapters 4 and 6. We also 
gratefully acknowledge the research funding of the National Science Foundation 
(IRI-9624222). 

Chapter 1 
INTRODUCTION 
Infonnation security has become increasingly important with the advent of 
computers to process sensitive infonnation. Secrecy, integrity and availabil-
ity are considered as inherent components of computer security. Secrecy is 
concerned with protection of infonnation against unauthorized disclosure; in-
tegrity is concerned with prevention of improper modification of infonnation; 
and availability of service involves ensuring that all computer resources are 
available to the users whenever they are needed. Secrecy, also known as con-
fidentiality, is more understood in the security community than the other two 
components of security. 
The primary purpose of a security mechanism is to control access to data. The 
way in which access control is implemented is known as the system's security 
policy. With discretionary access control (DAC), users at their discretion can 
specify to the system who can access their files. In this type of access control, 
a user or any of the user's programs or processes can choose to share files with 
other users. These discretionary access controls are prone to certain kinds of 
leakage of infonnation.1 Since research and development in secure database 
systems have come mainly from applications (such as those of Department of 
Defense where data security is of primary importance) that call for multilevel 
security, discretionary access controls are not adequate for military applications. 
Such applications require more stringent restrictions on sharing of infonnation. 
Mandatory access controls (MAC) are more suitable for such applications. 
Under mandatory access control, users and files have fixed security attributes 
that are used by the system to detennine whether a user can access a file or not. 
The mandatory security attributes are assigned administratively or automatically 
1 The leakage can occur due to a Trojan Horse attack. An example of such an attack can be found in section 
1.1. 
V. Atluri et al., Multilevel Secure Transaction Processing
© Kluwer Academic Publishers 2000

2 
MULTILEVEL SECURE TRANSACTION PROCESSING 
by the operating system. These attributes cannot be modified by the users or 
their programs on request. This book provides an insight into the interplay 
between the three components of security in developing transaction processing 
algorithms for multilevel secure database systems. In the remainder of this 
chapter, we provide an introduction to multilevel secure databases, the major 
threats to security and the proposed multilevel DBMS architectures. Since the 
focus of this book is on transaction processing in traditional as well as advanced 
application domains,. we provide a brief discussion on the latter. 
1 
MULTILEVEL SECURE DATABASES 
In a multilevel secure database management system (MLS DBMS), every 
data item is assigned a classification based on its sensitivity level, and every user 
a clearance. The role of an MLS DBMS is to ensure users query or manipulate 
only those data to which their security clearance entitles them. All database 
accesses are mediated through a Trusted Computing Base(TCB), a small part 
of the operating system that must always be invoked, cannot be bypassed, 
and must be shown to perform only its intended functions (i.e., the code that 
implements the TCB must be verified for the presence of illicit programs), to 
ensure prevention of any unauthorized access to the data. In order to determine 
whether a user should be allowed to access a data item, the user's clearance is 
compared with the classification level of the data item. 
A classification or clearance is made up of two components. A hierarchical 
component, called the security level, and a non-hierarchical component, called 
the category. For example, the different security levels distinguished in the 
structure of military security are unclassified, confidential, secret and top secret. 
These are ordered in the increasing order of their effect on national security as 
top secret> secret> confidential> unclassified. The categories, independent 
of each other, are not ordered. Within the Department of Defense, a set of one or 
more categories such as Nato, Nuclear, Crypto from among a very large number 
of possible choices are used. These are formed on the basis of need-to-know. 
Sometimes the non-hierarchical component may be absent. Classifications and 
clearances are collectively known as security classes. 
Denning [Den76] proved that the military security classes, described above, 
form a lattice, and she presented a set of axioms that every information security 
policy must satisfy. A lattice is a mathematical structure where all the elements 
in it are partially ordered, with a dominance relation > and, therefore, all the 
security classes are also partially ordered. In a lattice, not every pair of elements 
needs to be comparable, however, every pair of elements possesses a least upper 
bound and a greatest lower bound. A security class is said to dominate the other 
only when the level of the first is greater than or equal to the level of the second, 
and the category set of the first contains all the categories of the second. 

Introduction 
3 
The U.S. Department of Defense developed its own definition of computer 
security, documented in 'Trusted Computer System Evaluation Criteria' (TC-
SEC) (also known as The Orange Book [00085]). This DoD documentation 
provides a metric to evaluate the degree of trust that can be placed in computer 
systems for the secure processing of sensitive information. These criteria are 
divided mainly into four hierarchically ordered divisions: A, B, C and D, with A 
as the highest division. Within division B, there are three subdivisions (known 
as levels), B3, B2, and Bl, and within division C, there are two levels, C2 and 
C 1, also hierarchical in nature. Division A contains only one level, AI. TCSEC 
mandates that all systems in division C should provide for discretionary pro-
tection to guard against unauthorized access. All systems at level B 1 or above 
should enforce the mandatory access control policy. The MAC policy states 
that all subjects and objects must be assigned sensitivity labels. Systems above 
B 1 level should additionally prevent indirect leakage of information via covert 
channels (Covert channels are described in more detail in section 1.2. 
By the end of year 2001, TCSEC assurance levels will be replaced by the 
Common Criteria for Information Technology Security Evaluation (Common 
Criteria) [Nat98] assurance levels. Common Criteria has been approved by 
the International Organization for Standardisation (ISO) for use as the basis 
for evaluation of the security properties of information technology products 
and systems. It has eight hierarchically ordered 'Evaluation Assurance Levels' 
EALO, EALl, ... , EAL7. With respect to assurance, it is possible to compare 
TCSEC assurance levels with EALs. Roughly, D corresponds to EALO; lev-
els Cl and C2 correspond to EAL2 and EAL3, respectively; BI, B2, and B3 
correspond to EAL4, EAL5, and EAL6, respectively; and Al corresponds to 
EAL7. 
The Bell-LaPadula Model 
The concept of mandatory access controls was first introduced by Bell and 
LaPadula. The Bell-LaPadula model [BL76] is the well accepted interpretation 
of the DoD security policy. This model is stated in terms of subjects and objects. 
All activities within a system can be viewed as sequences of operations on 
objects. An object is usually a file, a record or a field within a record. But, in 
general, anything that holds data may be an object, like, memory, directories, 
interprocess messages, network packets, I/O devices or physical media. Active 
entities that can access or manipulate objects are called subjects. At a higher 
level of abstraction, users are subjects, but within the system a subject is usually 
a process, job or task, operating on behalf of the user. As we are concerned 
with transactions initiated by the users, in the context of DBMS, subjects are 
transactions. 
The Bell-LaPadula policy is formally stated in terms of the following two 
properties. 

4 
MULTILEVEL SECURE TRANSACTION PROCESSING 
1. Simple-security property: No subject may read information classified 
above its clearance level. 
2. * -property: No subject may write information classified below its clear-
ance level. 
In brief, the Bell-LaPadula model does not allow read-ups and write-downs. 
As an example, a document with security class (secret; Nato, Nuclear) can be 
given read access to a user with security clearance (top secret; Nato, Nuclear, 
Crypto), but cannot be given read access to a user with a security clearance 
(top secret; Nato, Crypto). Also, a user with (secret; Nato) clearance cannot 
gain either write or read access to data at (secret; Crypto) level since these two 
classes are incomparable. 
At this point, it is important to distinguish between users and subjects. Users 
are the human beings that are recognized by the system with a unique identi-
fication. Subjects are processes executing in the system on behalf of users. A 
user can have many subjects running in the system, but each subject is associ-
ated with a single user. Even though users are trusted not to deliberately leak 
information, one must be aware that the processes initiated by the users may be 
programmed to deliberately cause breach of security and, therefore, subjects 
need not be trusted. It should be noted that a user does not require a computer 
system to leak information. The second property of the Bell-LaPadula is to 
prevent leakage of information by malicious subjects. The following section 
discusses how such a leakage of information can occur. 
1.1 
WHY DAC IS NOT ENOUGH? 
The purpose of DAC is to provide controlled sharing of information by 
authorized subjects. DAC is powerful in terms of its flexibility and simplicity 
in protecting owners' data and propagating authorizations, and therefore it has 
been widely adopted in commercial environments. However, as alluded to 
earlier, DAC is prone to certain kinds of leakage of information such as Trojan 
Horse attacks. We provide below an example of such an attack and show how 
MAC can prevent it. 
Trojan Horses 
Trojan Horse is a type of malicious program, which masquerades as a useful 
service but surreptitiously leaks data. This leakage may occur when trusted 
people use this service to do what they believe is legitimate work. A Trojan 
Horse program can be embedded in a word processing program, a compiler, a 
list-directory or a game. An effective Trojan Horse has no obvious effect on the 
program's expected output, and its damage may never be detected. A simple 
Trojan Horse in a text editor might discreetly make a copy of all files that the user 
asks to edit, and store the copies in a location where the penetrator, the person 

Introduction 
5 
who wrote the program, can later access them. As long as the unsuspecting 
user can voluntarily and legitimately give away the file, there is no way the 
system is able to tell the difference between a Trojan Horse and a legitimate 
program. Any file accessible to the user via the text editor is accessible to the 
Trojan Horse. The reason Trojan Horses work is because a program run by a 
user usually inherits the same unique ID, privileges and access rights of the user. 
The Trojan Horse, therefore, does its dirty work without violating any of the 
security rules of the system. A clever Trojan Horse might even be programmed 
to delete itself if the user tries to do something that might reveal its presence. 
Thus, a Trojan Horse can either copy a file, set up access modes so that the 
information can be read directly from the files at a later time, or delete, modify 
or damage information. 
Ord~r p{ocessing 
applicatIon 
I read Future Products I 
I write Personal Items 
Future Products (Tom's table) 
Personal Items (Dick's Table) 
Product 
Release Date 
Price 
Product 
Date 
Cost 
A 
Jan xx 
1000 
B 
FebYY 
200 
Figure 1.1. 
Before the Trojan Horse Attack 
The following example illustrates how a Trojan Horse attack can occur. Con-
sider a scenario where Dick, a programmer, developed a large order processing 
application to his manager, Tom. Assume there exists a table called Future 
Products created and maintained by Tom, which Tom does not want to share 
with anyone including Dick. However, Dick can easily gain access to this 
information by simply doing the following: (1) create an empty table, called 
Personal Items, (2) give write permission on it to Tom, and (3) include a Trojan 
Horse in the application that reads data from Future Products and writes into 
the Personal Items, as shown in figure 1.1. When Tom executes the order pro-
cessing application, the Trojan Horse executes with all the privileges of Tom 
and therefore will be able to gain access to Future Products and be able to write 

6 
MULTILEVEL SECURE TRANSACTION PROCESSING 
to Personal Items, as depicted in figure 1.2. It is important to note that Tom has 
no knowledge of these operations. Since Dick has access to Personal Items, he 
is now able to read its contents, which are in fact nothing but those of Future 
Products. 
Ord~r p~ocessing 
apphcatlon 
Future Products (Tom's table) 
Personal Items (Dick's Table) 
Product 
Release Date 
Price 
Product 
Date 
Cost 
A 
Jan xx 
1000 
A 
Jan XX 
1000 
B 
FebYY 
200 
B 
FebYY 
200 
Figure 1.2. 
After the Trojan Horse Attack 
This problem can be evaded by enforcing mandatory access control. Consid-
ering once again the above example, this can be easily prevented by assigning 
the following security labels: clearance levels of Tom and Dick are high and 
low, respectively, where high > low; and security levels of Future Products 
and Personal Items are high and low, respectively. With the enforcement of 
the * - property of the Bell-LaPadula model, the Trojan Horse attack can be 
prevented since high subjects cannot write to low objects. 
1.2 
ADDITIONAL THREATS TO SECURITY: COVERT 
CHANNELS 
One of the most difficult challenges of a secure system is to prevent leakage of 
information via covert channels. These channels are paths not normally meant 
for information flow that could nevertheless be used to signal information. 
Detection and prevention of these channels is beyond the capability of the 
TCB. This is because, these channels can be established without violating any 
conditions mandated by the system's security policy. 
A program that accesses some classified data on behalf of a process may 
leak those data to other processes and thus to other users. Two types of covert 

Introduction 
7 
channels have been identified thus far. A covert storage channel is any com-
munication path that results when one process causes an object to be written 
and another process observes the effect. This channel utilizes system storage 
such as temporary files or shared variables to pass information to another pro-
cess. A covert timing channel is any communication path that results when 
a process produces some effect on system performance that is observable by 
another process and is measurable with a timing base such as a real-time clock. 
Covert timing channels are much more difficult to identify and to prevent than 
covert storage channels. A system with B2 rating or high must be free of these 
channels. The most important parameter of a covert channel is its bandwidth, 
i.e., the rate (in bits per second) at which information can be communicated. 
Surprisingly, the higher the speed of the system the larger the bandwidth of 
these covert channels. According to the TCSEC standards, a channel that ex-
ceeds the rate of 100 bits per second is considered as having a high bandwidth. 
An example of a covert channel is depicted in figure 1.3. 
High User ... ---
High Trojan Horse 
infected process 
If person x' s salary> 2000 
then run a machine 
tuning program (e.g., 
compute 1,000,000!) 
Low User 
Covert Channel 
salary may be leaked to low user 
Low Trojan Horse 
infected process 
If a system response 
is perceived for 
longer than 5 sec, 
then salary> 2000 
Figure 1.3. 
Example of a Covert Channel 
Jajodia and Sandhu have distinguished signaling channels from covert chan-
nels as follows. A signaling channel is a means of information flow inherent in 
the basic algorithm or protocol, and hence appears in every implementation. A 
covert channel is a property of a specific implementation, and not of the general 
algorithm or protocol. Thus a covert channel may be present in a given imple-
mentation even if the basic algorithm is free of signaling channels. A secure 
system must prevent leakage of information occurring via these channels. 

8 
MULTILEVEL SECURE TRANSACTION PROCESSING 
1.3 
MULTILEVEL SECURE DBMS ARCHITECTURES 
One way of imposing mandatory access control is to operate the sensitive 
data in system high mode. In this type of operation, all users are cleared to the 
highest level of data stored in the database. This approach requires a qualified 
human operator to check all data that goes out of the system that is below 
system high. The major advantage with this approach is that existing DBMS's 
can be used with no change. However, this approach suffers from several 
drawbacks. First, this approach is very expensive since the cost of clearance is 
high. Second, it involves additional security risk since more people are given 
the highest clearance. Third, it may compromise security since it relies on a 
human operator, who is not always as efficient and reliable as a machine. 
Towards a solution to this problem, the Woods Hole study group, organized 
by U.S. Air Force in 1982, considered different architectures for building mul-
tilevel secure DBMSs, and proposed long-term requirements as well as near-
term solutions for multilevel DBMSs [Com83]. The near-term solutions of the 
Woods Hole study looked at how existing DBMS technology could be reused 
in order to build multilevel secure DBMSs. Among the many architectures 
proposed by this study to physically store the multilevel data, three solutions 
have gained importance: the integrity lock DBMS, the kernelized DBMS and 
the replicated (called distributed in [Com83]) DBMS, towards its near-term so-
lutions. All these architectures use a Trusted Front End (TFE), which cannot 
be bypassed. 
Integrity lock architecture 
The integrity lock architecture, also known as spray paint architecture, is based 
on the integrity lock technology. This architecture uses a single DBMS to 
manage all levels of data, as shown in figure 1.4. This figure is shown for 
four hierarchically ordered security classes used in military, namely, top secret, 
secret, confidential, and unclassified. While storing data, the TFE cryptograph-
ically generates a stamp and assigns this stamp (or paints) to every data item. 
While retrieving data, the TFE checks the stamp on the data item and makes 
sure that the simple-security property of the Bell-LaPadula policy, presented in 
section 1 is met. 
Since a user's query has access to the entire database, a clever user can deduce 
higher level information by formulating a query that modulates its output based 
on high level data. For example, consider the database shown in figure 1.5 and 
suppose a confidential user submits the following query: 
if (Dick's salary> 50000) return 1, else return 0 
Note that the value returned to the confidential user does not have a stamp 
that is higher than confidential, yet secret information can be inferred from this 

Introduction 
9 
Top Secret Secret 
Confidential 
Unclassified 
User 
User 
User 
User 
Trusted Front End 
Security Front End 
I Append I 
I 
Check 
Stamp 
Stamp 
Store 
Retrieve 
Response 
(Query) 
Untrusted Back End DBMS 
Figure 1.4. 
The Integrity Lock Architecture 
Name 
Salary 
Stamp 
Tom 
30000 
Confidential 
Dick 
70000 
Secret 
Figure 1.5. 
Vulnerability of The Integrity Lock Architecture 
query. Thus, this architecture can easily be exploited to open up high band-
width covert channels and, therefore, has been dismissed from consideration for 
systems aiming beyond B 1. This architecture has been adopted in the product 
TRUDATA. 
Kernelized architecture 
In the kemelized architecture, the multilevel database is partitioned into single-
level databases, which are then stored separately. In this architecture, there is a 
separate DBMS for each security class. Figure 1.6 shows this architecture once 
again for the four hierarchical classes, namely, top secret, secret, confidential, 
and unclassified. The t(Usted front end ensures that the users' queries are sub-
mitted to the DBMS with the same security level as that of the user, while the 
trusted back end makes sure that a DBMS at a specific security level accesses 

10 
MULTILEVEL SECURE TRANSACTION PROCESSING 
data without violating the mandatory security policy (in other words, without 
circumventing the Bell-LaPadula restrictions). Processing a user's query ac-
cessing data from multiple security levels involves expensive joins that may 
degrade the performance since the different levels of data are stored separately. 
On the other hand, since this architecture has separate DBMSs for each secu-
rity level, the scheduler that is responsible for concurrency control also can be 
separated for each level. Therefore, the scheduler can be implemented with 
untrusted code and need not be part of the TeB. This has been adopted in the 
SeaView prototype as well as in the Trusted Oracle DBMS. 
users 
users 
users 
users 
~\I/ 
~R~ 
~------~ 
,-------, 
unclassified 
DBMS 
confidential 
DBMS 
Trusted OS 
secret 
DBMS 
top secret 
DBMS 
unclassified 
top secret 
Figure 1.6. 
The Kemelized Architecture 
Replicated architecture 
Replicated architecture uses a separate database management system to manage 
data at or below each security level; a database at a security class contains all 
information at its class and below and, therefore, lower level data are replicated 
in all databases containing higher level data. The replicated DBMS architec-
ture is shown in figure 1.7. Though query processing is not expensive as in 

Introduction 
11 
the kemelized architecture, replicated architecture suffers from the following 
two difficulties since lower level data is replicated at higher level databases: (1) 
Propagating updates of the lower level data to higher level DBMSs in a secure 
and correct manner is not simple. (2) This architecture is impractical for large 
number of security levels. This architecture has been adopted by the Naval Re-
search Laboratories (NRL) in their SINTRA prototype. In this book, we focus 
on transaction processing pertaining to kemelized and replicated architectures. 
unclassified 
DBMS 
users 
users 
users 
users 
\// 
confidential 
DBMS 
r--.... 
./ 
unclassified 
confidential 
TFE 
secret 
DBMS 
1 
unc asslfied 
confidential 
secret 
Figure 1.7. 
The Replicated Architecture 
Trusted subject architecture 
top secret 
DBMS 
unc asslfied 
confidential 
secrel 
top secret 
The long-term solution proposed in the Woods Hole study is to build a com-
pletely trusted DBMS right from scratch, which is called the trusted subject 
architecture. A trusted subject is one which is exempt from the mandatory 
access control. In this architecture, all DBMS data is stored with the DBMS 
label and only DBMS code can be executed by a subject with the DBMS label. 
Subjects with DBMS label are trusted and thus are exempt from mandatory 
access control restrictions. Since the DBMS is trusted, a trusted front end is 
not required in this architecture. Instead, users communicate with the DBMS 
via an Untrusted Front End (UFE) at their respective classification level. The 
trusted subject architecture is shown in figure 1.8. Many DBMS vendors such 

12 
MULTILEVEL SECURE TRANSACTION PROCESSING 
as Sybase and ORACLE are developing trusted DBMS products based on this 
architecture. 
unclassified 
users 
UFE 
confidential 
users 
UFE 
Trusted 
DBMS 
Trusted OS 
DBMS and 
Non DBMS 
Data 
secret 
users 
UFE 
top secret 
users 
UFE 
Figure 1.B. 
The Trusted Subject Architecture 
2 
ADVANCED DATABASE APPLICATION DOMAINS 
Since this book presents the research in transaction processing in advanced 
application domains, in this section, we provide a brief introduction to such do-
mains. Traditional database systems are typically used for simple applications 
such as airline reservation systems, banking systems and electronic funds trans-
fer. Recently, the scope of database systems has expanded to non-traditional 
environments ranging from process control to cooperative work. These include 
applications such as Office Information Systems (OIS), Computer Aided De-
sign/Computer Aided Manufacturing (CAD/CAM), software development and 
publication environment. 
Transaction processing in these advanced application domains needs spe-
cial attention due to their many distinguishing characteristics from those in 
traditional domains. First, unlike traditional transactions that are short-lived 

Introduction 
13 
and simple in structure, transactions in advanced application domains are long-
lived. This poses a number of challenges because transactions are traditionally 
built on concepts such as atomicity, consistency, isolation and durability (these 
properties, known as ACID properties, are described in detail in section 1 of 
chapter 2). For example, long-lived transactions are more prone to interrup-
tions by failures because of their long execution time. Unlike short duration 
transactions, rolling them back entirely to guarantee failure atomicity is too 
expensive. Second, they access many data items. Due to the isolation require-
ment, they hold all resources until commit time, which may block many other 
transactions to proceed. Moreover, they may cause many deadlocks. Third, 
there exist inter-relationships among the different operational activities within 
a transaction to allow cooperation among them. Traditionally transactions are 
jfo.t, and therefore cannot capture these inter-relationships. 
Reacting to this, traditional transaction models have been extended in vari-
ous directions [Elm92]. These include, among the others, (1) nested transaction 
models (these models directly support the representation of nested computa-
tions), (2) workflow and extended transaction models (they represent processes 
in manufacturing and office environments and heterogeneous database man-
agement systems and are capable of capturing the inter-relationships among 
the different operational activities), and (3) long duration transactions (these 
models support environments such as CAD/CAM and OIS, where transactions 
typically have long durations, such as days, or weeks). 
As advances in database systems take place, they are also required to sup-
port multilevel security. Thus, in order to incorporate multilevel security into 
advanced database applications areas, there is a need to investigate how mul-
tilevel security constraints impact transaction processing with these extended 
transaction models. Chapter 5 and section 2 in chapter 8 discuss the research 
in this direction. 
3 
ORGANIZATION OF THE BOOK 
The remainder of the book is organized as follows. Chapter 2 gives an 
overview of traditional transaction processing approaches. Chapter 3 first iden-
tifies the desirable properties and the additional requirements imposed by multi-
level security on transaction processing, explains why conventional transaction 
processing techniques can conflict with the multilevel security constraints, dis-
cusses how they must be modified to comply with the security policy, and 
notes the challenges of doing so with acceptable responsiveness and with lit-
tle or no trusted code. It then presents secure concurrency control algorithms, 
based on locking and multiversion timestamp ordering, developed for repli-
cated and kernelized trusted DBMS architectures, and provides an assessment 
of them, and examines the published solutions adopted by commercial vendors 
in their trusted DBMS products. This chapter also presents research in multi-

14 
MULTILEVEL SECURE TRANSACTION PROCESSING 
level transaction correctness, where an individual transaction is able to write 
data at multiple security levels that leads to an additional trade-off between se-
curity and atomicity. Additionally it presents the distributed multilevel secure 
DBMS model and describes the research on the impact of multilevel security 
on commit protocols for coordinating the execution of distributed transactions. 
Chapter 4 discusses the impact of real-time constraints on secure transaction 
processing and reviews the research solutions in this area. 
Chapter 5 deals with secure transaction processing in advanced application 
environments and addresses more recent issues. In particular, it discusses issues 
oftransaction processing considering the workflow transaction model. Chapter 
6 presents the buffer management issues as well as proposed solutions in the 
MLS DBMS environment. Chapter 7 presents the application of secure trans-
action processing solutions to hierarchical and replicated databases. Finally, 
chapter 8 identifies technical challenges in multilevel transaction processing 
that have yet to receive significant attention, including secure transaction pro-
cessing using advanced transaction models such as nested and long duration 
models, and secure recovery. 

Chapter 2 
TRADITIONAL TRANSACTION PROCESSING 
The primary purpose of a database management system is to carry out trans-
actions. A transaction is a sequence of database operations either to query or 
manipulate data in a shared database. The goal of a concurrency control (CC) 
mechanism is to preserve database integrity, even in the presence of concur-
rent data accesses by multiple users, by properly synchronizing simultaneous 
executions of transactions. This goal is best illustrated by a simple example. 
EXAMPLE 2.1 [Why we need a concurrency control protocol] Consider a 
database that maintains the information of customer accounts in a bank, where 
the balance in each account is maintained. Suppose a customer, Mr. Smith, owns 
two accounts: a checking account (account A) and a savings account (account 
B), the latter account being jointly owned by Mr. Smith and his wife. Suppose 
the current balances in A and B are $1,000 and $10,000, respectively. Consider 
the following scenario: A transaction T1, initiated by Mr. Smith, moves $100 
from account A to account B, while another transaction T2 , initiated by Mrs. 
Smith, deposits $10,000 into account B. 
If these two transactions take place at about the same time, as shown in figure 
2.1, there is a potential for problem. In this concurrent execution, interference 
between Tl and T2 causes the amount deposited by T2 to be lost (even though 
T2 has successfully completed). Such concurrent executions are incorrect and 
should not be allowed by the DBMS. 
0 
1 
ACID REQUIREMENTS 
Traditionally, transactions are expected to satisfy atomicity, consistency, iso-
lation, and durability properties (known as ACID properties). Atomicity re-
quires that either all the changes of a transaction have to be made permanent 
(when the transaction commits) or the effects must be removed (when the trans-
V. Atluri et al., Multilevel Secure Transaction Processing
© Kluwer Academic Publishers 2000

16 
MULTILEVEL SECURE TRANSACTION PROCESSING 
Tl 
I T2 
Read A 
A=A-lOO 
Read B 
Write A 
B=B+10000 
ReadB 
WriteB 
CommitT2 
B =B + 100 
Write B 
CommitTl 
Figure 2.1. 
An incorrect concurrent execution 
action aborts}. Consistency requires that each transaction be correct; i.e., if 
executed alone, the transaction maps the database from one consistent state to 
another consistent state. Isolation requires that no transaction should view the 
intermediate results of other transactions. Finally, durability requires that once 
a transaction commits, all of its effects must be preserved permanently in the 
database even in the presence of failures. 
A proper concurrency control protocol ensures isolation properties of transac-
tions even in the presence of other concurrently running transactions. A proper 
failure recovery protocol ensures the atomicity and durability properties. In a 
distributed database, an atomic commit protocol is additionally required to en-
sure atomicity. Distributed transaction processing requires proper integration 
of atomic commit and concurrency control protocols. 
2 
CONCURRENCY CONTROL PROTOCOLS 
How does a proper concurrency control mechanism solve the problem il-
lustrated in figure 2.1? The interleaved execution of Tl and T2 in figure 2.1 
resulted in an incorrect database state, though each transaction would have re-
sulted in a correct database state when run individually. In other words, this 
problem would not have arisen if these two transactions had been executed one 
after the other serially, either Tl before T2 or T2 before T1. However, there is 
no need to be so strict. Even though the execution in figure 2.2 is not serial, but 
interleaved, it does not give rise to such erroneous executions. 
The concurrent execution in figure 2.2 shows that it is permissible to inter-
leave the operations of transactions as long as their effect on the database is 
the same as if they had been executed serially. Executions that can be shown 

Read A 
A=A-100 
Write A 
ReadB 
B=B+100 
Write B 
CommitTl 
Traditional Transaction Processing 
17 
ReadB 
B=B+10000 
WriteB 
CommitT2 
Figure 2.2. 
A correct concurrent execution 
equivalent to a serial execution are called serializable. A concurrency control 
protocol is used to ensure serializability. 
Though there are number of concurrency control algorithms to produce se-
rializable execution of transactions [BHG87, UIl88], the two most widely used 
protocols are two-phase locking and timestamp ordering. 
Two-phase Locking (2PL): 
The protocols based on locking delay the execution of conflicting! operations 
by setting locks on data items for read and write operations. 2PL requires that 
all transactions be well-formed and two-phase. By definition, a well-formed 
transaction must acquire a shared-lock (S-lock) on a data item before reading 
it and an exclusive lock (X-lock) before writing it. A transaction is two-phase 
if it does not acquire any more locks once it releases a lock on some data item. 
Timestamp Ordering: 
The protocols based on timestamping assign a unique timestamp (ts(Ii) to 
each transaction (Ii). Each data item (x) is associated with the timestamp of 
the latest transaction that read or wrote it (rts(x) and wts(x), respectively). A 
timestamp ordering protocol permits conflicting operations to execute only in 
increasing order of timestamps as follows: (1) Ti is not allowed to read x unless 
wts(x) :::; tS(Ti), and (2) Ti is not allowed to write x unless rts(x) :::; tS(Ti). 
1 Two operations conflict if both refer to the same data item and at least one of them is a write. 

18 
MULTILEVEL SECURE TRANSACTION PROCESSING 
3 
COMMIT PROTOCOLS 
Often, information is physically distributed over several sites for reasons 
such as increased availability, survivability, reliability, performance, or con-
venience. Such a distributed database consists of a collection of independent 
computational units connected via communication links. Transactions exe-
cuting in these systems may require access to (either to update or retrieve) 
information from a number of remote sites. When a distributed transaction is 
divided (into subtransactions) for execution at individual sites, we must ensure 
that they either all commit or all abort together. That is, if a subtransaction fails 
for any reason, we must abort all the subtransactions and roll back the overall 
distributed transaction. 
EXAMPLE 2.2 [Why we need a commit protocol] Consider once again the 
example described in section 2.1. Assume Mr. Smith's checking account 
(account A) is maintained in San Francisco (site 1) and his savings account 
(account B) in New York (site 2). Suppose he wishes to move $1,000 from his 
savings to checking account. Assume a distributed transaction is initiated at 
site 1 to perform these operations. The following set of operations have to be 
performed at the two sites: 
Site 1 
Read A 
A=A+ 1000 
Write A 
Site 2 
ReadB 
B=B-1000 
Write B 
The distributed transaction must commit only after successful execution of 
these operations at both the sites. Otherwise it should abort. It is undesirable 
to execute operations at site 1 alone or site 2 alone. 
D 
To ensure atomic execution of all the components of a distributed transaction 
(often referred as subordinates), an atomic commit protocol is required. The 
purpose of this protocol is to coordinate the commit among the subordinates, 
i.e., to make sure that either the effects of all the subordinates persist or none of 
the effects persist. When transactions (distributed as well as local) are executed 
concurrently, these systems require a distributed concurrency control protocol 
as well. An atomic commit protocol ensures the atomicity of the distributed 
transaction. The two-phase commit protocol (2PC) is one of the most popular 
atomic commit protocols. 2PC is briefly reviewed below. 
The Two-Phase Commit Protocol 
The originating site, or coordinator, sends the subtransactions to the participant 
sites and requests their execution. Each participant attempts to execute its 

Traditional Transaction Processing 
19 
subtransaction and responds with a done message if it successfully executes the 
subtransaction. Although the participant is able to commit, it does not do so 
and awaits for instructions from the coordinator. 
When the user who initiated the transaction issues the commit command, 
the coordinator initiates the first phase, called the prepare phase, of the 2PC 
protocol, in which the coordinator asks each participant for a vote. A participant 
responds with a Yes or No vote, depending on whether it was able to execute 
the subtransaction. 
coordinator 
subordiante 
work-request 
done 
E 
prepare 
.. 
yes 
E 
commit 
.. 
acknowledge 
E 
6n messages 
Figure 2.3. 
Basic 2PC Protocol 
In the second phase, known as the decision phase, based on the responses it 
has received, the coordinator instructs the participants collectively whether to 
Commit or Abort their work. If the coordinator either receives a no-vote or fails 
to receive any vote from some participant, it force-writes an abort record and 
sends abort messages to those participants that are in a prepared state or did not 
send in a vote. Otherwise, it force-writes a commit record and sends commit 
messages to all the participants. Upon receipt of a commit or an abort message, 
each participant force-writes either a commit or an abort record, and sends an 
acknowledgment to the coordinator. This last round of acknowledgments from 
the participants to the coordinator is necessary for recovery in case of failures. 
The exchange of messages is shown in figure 2.3. 

20 
MULTILEVEL SECURE TRANSACTION PROCESSING 
coordinator 
subordiante 
work-request, 
prepare 
yes 
E 
commit 
acknowledge 
E 
4n messages 
Figure 2.4. 
EP Protocol 
If there are n participants for a transaction, then 2PC requires a total of 6n 
messages -
n messages to distribute the subtransactions, n messages for the 
done responses, 2n messages during the prepare phase and another 2n messages 
during the decision phase. 
Although many distributed systems rely on the two-phase commit (2PC) 
protocol for ensuring atomic commit, most commercial systems use several 
optimizations of2PC to reduce the number of message exchanges [SC93]. In the 
presumed commit (abort) the acknowledgment is not sent from the participants 
ifthe subtransaction commits (aborts). Most commercial systems (e.g., IBM's 
LU6.2 and Digital's DECdtm) use the early prepare (EP) rather than 2PC as the 
commit protocol. This is because these systems do not support request/response 
paradigm, thus an implementation of2PC in these systems requires 6n messages 
(where n is the number of sites participating in the commit protocol) compared 
to 4n messages required by EP. We present next, the EP commit protocol. 
Early Prepare (EP): An Optimization for Two-Phase Commit 
The EP works as follows. The coordinator first generates the subtransactions, 
and then sends these subtransactions (or work-requests) to the nodes partici-
pating in the execution of the distributed transaction by tying them with the 
prepare-to-commit messages. This results in a saving of 2n message ex-
changes since it eliminates the n done responses as well. The exchange of 
messages is shown in figure 2.4. 

Chapter 3 
TRANSACTION PROCESSING IN MULTILEVEL 
SECURE DATABASES 
In this chapter, we review the research efforts towards developing secure 
transaction processing algorithms. A secure transaction processing protocol, 
in addition to complying with the two Bell-LaPadula restrictions, must be free 
of all covert channels. This requirements prohibits the use of conventional 
concurrency control and commit protocols since they are susceptible to covert 
channels. 
1 
PROBLEMS WITH SOLUTIONS IN TRADITIONAL 
ENVIRONMENT 
In the following we show how a covert channel can easily be established 
with conventional concurrency control algorithms such as 2PL and TO. In both 
locking and times tamping techniques, whenever there is contention for the 
same data items by transactions executing at different access classes, a lower 
level transaction may be either delayed or suspended in order to ensure correct 
execution. In such a scenario, two colluding transactions executing at high 
and low levels can establish an information flow channel from a high level to 
a low level by accessing selected data items according to some agreed-upon 
code. The following example illustrates how a covert channel can be created 
by malicious transactions under 2PL. 
Consider a database that stores information of two types: low and high. Any 
low information is made accessible to all users of the database by the DBMS; 
on the other hand, high information is available only to a select group of users 
with special privileges. In accordance with the security policy, a transaction 
execution on behalf of a user with no special privileges would only be able to 
access (read and write) low data elements, while a high transaction (initiated by 
V. Atluri et al., Multilevel Secure Transaction Processing
© Kluwer Academic Publishers 2000

22 
MULTILEVEL SECURE TRANSACTION PROCESSING 
a high user) would be given full access to the high data elements and read-only 
access to the low elements.l 
high: 
low: 
WI [x] will be delayed 
Figure 3.1. 
A covert channel with two-phase locking protocol 
Imagine a "conspiracy" of two transactions: Tl and T2. Tl is a transaction 
confined to the low domain; T2 is a transaction initiated by a high user and, 
therefore, able to read all data elements. Suppose that only these two transac-
tions are currently active. If T2 requests to read a low data element x, a lock will 
be placed on x for that purpose. Suppose that next Tl wants to write x. Since 
x has been locked by another transaction, Tl will be forced by the scheduler 
to wait. Tl can measure such delays, for example, by going into a busy loop 
with a counter. Thus, by selectively issuing requests to read low data elements, 
transaction T2 could modulate delays experienced by transaction Tl , effectively 
sending signals to Tl. Since T2 has full access to high data, by transmitting 
such signals, it could pass on to Tl information that the latter is not authorized 
to see. The information channel thus created is a covert channel and is shown 
in figure 3.1. 
Figure 3.2. 
A non-serializable history 
The TO protocol also suffers from the same secrecy-related flaw. Suppose 
ts(Tt} < tS(T2)' Since Tl attempts to write x after T2 has read, Tl'S write 
operation is rejected because the read timestamp of x (rts(x)) > ts(TI) and 
therefore Tl must be aborted. Since a high transaction can selectively cause a 
(cooperating) low transaction to abort, a covert channel can be established. 
The signaling channel, however, can simply be eliminated if a high transac-
tion does not acquire a lock on low data element, or relinquish the lock when 
1 It is necessary to disallow a high transaction from writing into low data elements in order to guard against 
Trojan hor.fe.v. When a high user wants to update a low data item, he or she must do so by first logging in 
as a low user and then initiating a low transaction [Den821. 

Transaction Processing in MLS Databases 
23 
a low transaction requests it. (Similarly, in case of TO, it can be eliminated 
if the high transaction does not modify the read timestamp of the low data 
element.) But, this may lead to the history shown in figure 3.2. This history is 
not serializable because the corresponding serialization graph (shown in figure 
3.3) contains a cycle (the edge from Tl to T2 is due to the conflicting operations 
r2[x] and wdx] where r2[x] precedes wr[x], and the edge from T2 to Tl is due 
to the conflicting operations wr[y] and r2[yj where wIlY] precedes r2[Y])' 
Figure 3.3. 
The serialization graph corresponding the history in figure 3.2 
Similar problems are encountered even in distributed MLS systems. Espe-
cially when lock-based protocols are used for concurrency control, in order to 
eliminate covert channels, read locks acquired by a high level subtransaction 
on a low level data item must be relinquished when a low subtransaction at-
tempts to write that low data item. Unfortunately, this requirement has grave 
implications on the commit protocols, especially the early prepare, since to 
guarantee serializability, it requires not only that each subtransaction must be 
two-phase, but that the distributed transaction as a whole must be two-phase as 
well [BHG87]. To satisfy the latter requirement, all subtransactions must hold 
all their locks until the commit of the transaction, which unfortunately cannot 
be met in MLS systems (see section 5.6 for an example). 
2 
REVISED REQUIREMENTS 
Thus, a multilevel secure DBMS, besides ensuring serializability, must also 
eliminate all illicit flow of information via covert channels. In order to meet 
these two requirements, sometimes transactions may be subjected to indefinite 
delays or suspended again and again (especially high transactions, due to re-
quests from low transactions). This problem is known as starvation. Therefore, 
the requirement of ensuring serializability while preserving security leads to 
an additional requirement for multilevel secure DBMSs: they must also avoid 
starvation. Another desirable feature of a multilevel secure DBMS is to use as 
little trusted code as possible. 
In summary, the scheduler that implements the concurrency control protocol 
in a multilevel secure database management system must possess the following 
key properties: 
1. it must ensure correct execution of transactions 

24 
MULTILEVEL SECURE TRANSACTION PROCESSING 
2. it must preserve security (i.e., it must be free of covert channels) 
3. it must be implementable with untrusted code, and 
4. it must avoid starvation. 
There are two reasons for the third requirement. First, in high assurance 
systems it is crucial that the size of the trusted code be kept as small as possible 
since all the code used for implementation has to be verified for its trustwor-
thiness [DoD85]. A rigorous verification of the code is considered to be a very 
expensive task. Therefore, it is desirable that the scheduler providing concur-
rency control be implementable with code that is not required to be trusted. 
Second, the trusted scheduler needs additional security verification and must 
be layered on top of the already evaluated TCB. There is no guarantee that 
the composition of these two secure components will be secure [McC90]. No-
tice that for implementing the scheduler with completely untrusted code, the 
scheduler must be single-level, i.e., it must be separate for each security level. 
A concurrency control protocol that satisfies this requirement, automatically 
meets requirement 1 since there are no trusted components in the protocol. The 
challenge, therefore, is to build a protocol that meets all the four requirements 
stated in this section. 
3 
COMMERCIAL SOLUTIONS 
In this section, we describe the concurrency control protocol implemented 
by the three major vendors that produce trusted DBMSs: Sybase Secure SQL 
Server [Syb93], Trusted Oracle [Ora92], and Informix-OnLine/Secure [Inf93a, 
Inf93b], and discuss how well they satisfy the key properties identified in the 
previous section. 
For concurrency control, Sybase uses the usual2PL, which was shown above 
not to be secure. 
Informix uses an approach by which a low-level transaction can acquire a 
write lock on a low data item, even if a high-level transaction holds a read 
lock on this data item. Thus, a low-level transaction is never delayed by a 
high-level transaction. The high-level transaction simply receives a warning 
that a lock on a low data item has been "broken." However, despite the broken 
lock, the high-level transaction is still committed, unless the programmer has 
explicitly added some code to perform rollback of the transaction. A major 
drawback of the Informix approach is that no information is provided to the 
high-level transaction specifying which lock has been broken if the transaction 
has acquired locks on several low data items. Thus, only very primitive handling 
of broken lock exceptions is possible at the application program level. 

Transaction Processing in MLS Databases 
25 
Trusted Oracie, on the other hand, uses a combination of locking and multi-
versioning techniques. It employs strict two-phase locking2 for scheduling the 
operations of transactions accessing data items at their own level. The write 
timestamp assigned to every version of the data item is the time at which the 
transaction that has created that version commits. When a transaction reads 
data from a lower level, the version with the largest timestamp that is smaller 
than its timestamp is given. Although this hybrid algorithm is secure, it does 
not produce one-copy serializable schedules [JA92]. In fact, it guarantees a 
weaker notion correctness, as we show in section 5.3.1. 
4 
RESEARCH SOLUTIONS FOR REPLICATED 
ARCHITECTURE 
As we know from chapter 1, the replicated architecture [Air83] provides an 
elegant way of reusing existing technology. The idea is to construct an MLS 
DBMS from single-level DBMSs. The DBMS at each level contains a replica 
of every data item that a subject at that level can access. The challenge is then 
to design a replica control protocol that will ensure one-copy serializability 
[BHG87]. In the architecture most commonly used, transactions are submitted 
to a global transaction manager (GTM). The GTM routes the transactions to their 
sites of origin and propagates the update projections to each of the dominating 
containers in tum. 
The first concurrency protocol for this architecture was proposed by Jajodia 
and Kogan [JK90]. The idea behind the protocol is as follows: The GTM 
receives transactions and delivers them to the appropriate container database. 
Transactions completing at level low are submitted to the container at the level 
covering low,3 call it high. Two transactions that conflict at level low must be 
submitted to level high in the order in which they are serialized. However, if 
a local transaction is not serialized, then its update report can be sent to the 
GTM in an arbitrary order. To ensure serializability, the protocol assumes that 
each local scheduler preserves, in its serialization order, the order in which it 
receives conflicting transactions. Examples of such schedulers are conservative 
2PL and conservative TO [BHG87]. Unfortunately, the protocol has a subtle 
flaw: there is no control over the relative serialization order of non-conflicting 
update projections. As is shown in [KK95], this lack of control can result 
in nonserializable histories. Fortunately, the problem can be easily solved by 
executing update projections serially. 
Since the Jajodia-Kogan rrotocol works correctly for only those security 
po sets that form a total order, Costich [Cos92] presents a variation of the J ajodia-
2In strict 2PL, transactions release all their locks only after commit. 
3The [JK90] protocol applies only to systems in which the security levels are totally ordered. 

26 
MULTILEVEL SECURE TRANSACTION PROCESSING 
Figure 3.4. 
A Crown 
Kogan protocol for security posets that are only partially ordered. In addition, 
the amount of trust required to implement the GTM is further reduced. However, 
along with the improvements came a new problem first described by McDermott 
et al. [MJS91]. The problem is that for some security posets, the protocol will 
deadlock, blocking any further progress by update projections, and produce 
nonserializable histories. Following this paper, several other papers attempted 
to characterize the set of "problem" security posets [AJ93, AJ94b, KK95]. In 
[AJF96] the set of problematic security lattices is shown to be exactly those that 
embed a crown. An example of a crown is shown in figure 3.4. 
Assume that the containers at levels PI, ... ,Pn have executed transactions 
TI, ... , Tn respectively, and propagate the corresponding update projections 
to sites Pn+I, ... , P2n. These sites are free to serialize the projections as they 
arrive in any convenient order. Let us assume then that TI is serialized before T2 
at site Pn+1, T2 is serialized before T3 at site Pn+2, ... ,Ii is serialized before 
Ti+1 at site Pn+1, and finally Tn is serialized before TI at site P2n. If there 
is a site that dominates all levels in the crown, it must now choose an order 
for the transactions TI, ... ,Tn which is consistent with the decisions made at 
the levels it dominates. However, upon consideration, we see that choosing an 
order is impossible. Thus, the history is nonserializable and update projections 
will make no further progress. For a more detailed explanation of this problem, 
see [AJ93] or [KK95]. 
The results of [AJF96] apply to protocols that attempt to construct an order on 
transactions dynamically as they are propagated through the poset. A method 
of avoiding this problem is to determine the order using timestamps generated 
when the transaction commits for the first time. Update projections arriving at 
a container are dispatched in timestamp order to the local transaction manager. 
This is done without aborting update projections by following the conservative 
timestamp protocol discussed in [BHG87]. Solutions based on this approach 

Transaction Processing in MLS Databases 
27 
are presented in [MJS91] and [KK95]. These solutions suffer from starvation 
as does the conservative timestamp protocol on which they are based. 
The advantage of the replicated architecture approach is its simplicity and 
its ability to reuse DBMSs in their entirety. The major disadvantages of the 
replication-based schemes are the inefficient and inflexible use of resources: 
inefficient, because updating replicas incurs a significant overhead with no 
clear benefit; and inflexible, because resources are allocated among the DBMSs 
when the system is configured. Thus, the system cannot respond effectively to 
dynamic workload variations. 
5 
RESEARCH SOLUTIONS FOR KERNELIZED 
ARCHITECTURE 
To improve the situation, it is necessary to share resources (e.g., memory, 
I/O channels, CPUs) across security levels. This is most easily achieved with 
the kernelized architecture [AirS3]. Synchronizing readers and writers in an 
MLS environment is at the heart of the secure transaction processing problem. 
The solutions proposed by [Sch74, Lam77, RK79] are optimistic. High-level 
readers, before committing, must validate the objects they read. If those ob-
jects have been updated, the transaction must abort, thus they suffer from star-
vation. Moreover, these concurrency control algorithms, although meet the 
secrecy requirements, do not satisfy correctness requirements when applied 
to transactions (since transactions contain read and write sets that are often 
related) [AJ92]. 
5.1 
SINGLE VERSION ALGORITHMS 
The signaling channels described in section 1 can be eliminated if the high 
transactions are allowed to read low data items without acquiring a read lock. 
Although this approach meets the secrecy requirement, it does not meet the 
integrity requirement since all transactions are not two-phase. For example, in 
the history shown in figure 3.1, Tl will not be delayed since T2 does not acquire 
a read-lock on x. However, as can obviously be seen from figure 3.2, it does 
not ensure serializability. 
One way to circumvent both the problems mentioned above is as follows. 
When a high transaction wishes to read a low item, it first acquires a read lock on 
that low data item as in the traditional two-phase locking protocol, but should 
release this lock if a low transaction requests a write lock on the same data 
item. When a read lock by a high transaction is broken, the high transaction 
is to be aborted. Thus, this approach eliminates signaling channels since high 
transactions cannot in any way effect the execution of the low transaction, and at 
the same time guarantees serializability since it aborts all transactions violating 
two-phase property. If we revisit our example in figure 3.1, under this approach, 

28 
MULTILEVEL SECURE TRANSACTION PROCESSING 
T2 releases its read-lock on x since the lower level transaction Tl has requested 
for a conflicting lock on the same item x. Since it has prematurely released 
its lock, T2 aborts. Although this approach meets both secrecy and integrity 
requirements, under this approach, a malicious low transaction may cause a 
high transaction to be aborted repeatedly, resulting in starvation. 
In [MJ92] McDermott and Jajodia provide a way to reduce the amount of 
starvation. According to their approach, whenever a high transaction prema-
turely releases its read lock on a low data item due to security reasons, it does 
not abort and roll-back entirely, but holds its write locks on high data items, 
marks the low data item in its private workspace as unread and retries reading 
this data item by entering into a queue. This queue maintains the list of all 
high transactions waiting for retrial to read that particular data item and enables 
the first transaction in the queue to be serviced first. The modified approach, 
however, does not always produce serializable schedules [JMR96]. 
Recently, Jajodia et al. [JMR96] proposed two secure locking protocols that 
attempt to detect all cycles in the serialization graph by "painting" certain trans-
actions and the data items accessed by the high transactions whose low locks are 
broken and by detecting a cycle at the moment it is imminent in the serialization 
graph. The first protocol produces pairwise serializable histories (see section 
5.3.3), while the second protocol produces serializable histories if the security 
levels form a total order. 
Bertino et al. [BJMR98] extend 2PL by providing a powerful set of linguistic 
constructs to the system programmer which supports exception handling, partial 
rollback, and forward recovery. The proper use of these constructs can prevent 
starvation of high-level transactions, and allows the system programmer to trade 
starvation for degree of transaction isolation. 
It should be noted that the aforementioned lock based protocols use a common 
lock table for all security classes; thus, it is not possible to implement them with 
completely untrusted code. 
5.2 
MULTIVERSION ALGORITHMS 
Since locking has been found to be unsuitable for concurrency control in mul-
tilevel secure databases, researchers have experimented with other techniques, 
such as timestamping. However, the problem with the secure timestamp or-
dering protocol is that a high transaction cannot modify the read timestamp 
after reading a low data item (which is necessary to guarantee serializability) 
since doing so amounts to writing down. Therefore, the challenge is to devise a 
protocol that does not require such write-downs yet ensures correct concurrent 
executions of transactions. 
Ammann and Jajodia [AJ92] have proposed two secure timestamp-based 
algorithms that meet all the secrecy and integrity requirements, however, they 
are prone to starvation. 

Transaction Processing in MLS Databases 
29 
One of the solutions to eliminate starvation is to maintain, instead of single 
version, multiple versions of data. The contention to the same data items by 
high and low transactions can be resolved by maintaining multiple versions of 
data where high transactions are given older versions of data. The usual notion 
of correctness for concurrent execution of transactions when multiple versions 
are maintained in the database is said to be correct when its effect is equivalent to 
that of a serial execution of the same transactions on a one-copy database, which 
is called one-copy serializability [BHG87]. The significant efforts based on 
multiversion timestamp ordering technique are due to Maimone and Greenberg 
[MG90], Keefe and Tsai [KT90], and Jajodia and Atluri [JA92]. Although 
these three protocols meet secrecy and availability requirements, Maimone and 
Greenberg's protocol does not meet the integrity requirement, i.e., it does not 
generate one-copy serializable histories. The first protocol [MG90] has already 
been discussed in section 3; we present next the latter two [JA92, KT90]. 
5.2.1 
KEEFE-TSAI'S SCHEDULER 
This scheduler uses a timestamp based protocol, which is similar to the con-
ventional multiversion timestamp ordering protocol, but differs in the way it 
assigns the timestamps to every transaction. It uses a priority queue, and trans-
actions are placed in this queue based on the security level of the transaction. 
A transaction is assigned a timestamp that is smaller than all other active 
transactions executing at lower security levels. 
As a result, a high transaction's read operation does not interfere with a 
low transaction's write operation since the high transaction is considered as an 
older transaction though it arrives later than the low transaction. For example, 
consider the history in figure 3.1 once again. Keefe-Tsai's scheduler assigns 
a lower timestamp to T2 even though it arrives later than T1. Therefore, T2'S 
T2[X] will not invalidate Tl'S W2[X], 
This scheduler meets the first three requirements presented in section 2: it is 
secure, guarantees one-copy serializability, and eliminates starvation; however, 
it uses a multilevel scheduler which therefore has to be trusted. 
5.2.2 
JAJODIA-ATLURI'S SCHEDULER 
This scheduler, like Keefe-Tsai's, is based on the multiversion timestamp-
ing protocol, but differs from it in many respects. It assigns timestamps to 
transactions in the order of their arrival and thus transactions are not given un-
usually old versions of data. However, according to this protocol, transactions 
are sometimes made to wait for the commit. 

30 
MULTILEVEL SECURE TRANSACTION PROCESSING 
Whenever a transaction reads from a lower level, it is not allowed to 
commit until all the transactions from that lower level with smaller times-
tamps commit. 
This is because a lower level transaction with a smaller timestamp can po-
tentially invalidate the read operation of the higher level transaction by issuing 
a write. 
This protocol uses single-level schedulers and therefore is secure and is 
also implementable with untrusted code. It guarantees one-copy serializabil-
ity. Though transactions are made to wait for commit, this protocol is free of 
starvation since this wait is finite. Thus it meets all the four MLS requirements 
. presented in section 2. 
5.2.3 
SNAPSHOT APPROACHES 
Recently, several snapshot algorithms have been proposed. The first, pro-
posed by Ammann et al. [AJJ92], maintains two complete snapshots at each 
level for read access by transactions at dominating levels. Ammann and Ja-
jodia [AJ94a] present a multiversion algorithm in which a snapshot of a data 
item is generated only when necessary, i.e., in cases where the snapshot value 
differs from the current value. A variation based on locking is presented by 
Pal [PaI95] together with a detailed implementation. All three protocols place 
an upper bound of two on the number of versions that need to be maintained. 
Transactions reading down, read the snapshot. Transactions accessing data 
at their own level, access the current state of the database. Periodically, the 
snapshots are synchronized with the database. Because the snapshot must be 
commit consistent, the lifetime of a transaction is generally limited to the snap-
shot period. Pal suggests some cases in which this restriction can be relaxed 
[Pal95]. 
The snapshot algorithms suffer from weaknesses similar to those of the 
MVTO-based protocol. However, they are somewhat more predictable in terms 
of storage space, staleness of data, and liveness. The snapshot algorithms re-
quire no more than two versions of a data element, the versions read are no 
more than two snapshot periods old, and transactions shorter than a snapshot 
period can be guaranteed to complete. This predictability is very important for 
ensuring the availability of the DBMS. While the snapshot algorithms constrain 
the worst-case staleness of data nicely, the average staleness can be quite high. 
A high-level transaction reading-down must access the snapshot. Thus, on av-
erage, the data read from strictly dominated levels will be one-half snapshot 
periods old when it is read, regardless of the workload. The MVTO-based pro-
tocols have no similar minimum. Similarly, the snapshot-based protocols limit 
the lifetime of a transaction that reads down to one snapshot period, where the 

Transaction Processing in MLS Databases 
31 
MVTO algorithm has no comparable upper limit. Research is needed to study 
the characteristics of these solutions for realistic workloads. 
5.3 
WEAKER CORRECTNESS CRITERIA 
A solution to the problem of secure concurrency control can be obtained by 
viewing it in a different perspective. One can argue that the traditional notion of 
correctness is too restrictive for multilevel secure databases [MG90, JA92]. This 
is supported by the fact that the integrity constraints in multilevel databases are 
different from those in conventional databases. Enforcing integrity constraints 
in MLS databases is difficult or even impossible, especially those constraints 
that are defined over data at different security levels [MJ88]. Since one can-
not enforce the integrity constraints, there is no reason to insist on preserving 
serializability for these systems. Thus, Jajodia and Atluri [JA92] argue that 
serializability requirements can be relaxed for MLS systems and propose three 
alternative notions of correctness-Ievelwise serializability, one-item read seri-
alizability, and pairwise serializability-for multilevel multiversion databases. 
These weaker notions of correctness can be used as alternatives for one-copy 
serializability. They exploit the nature of integrity constraints in MLS databases 
to improve the amount of concurrency. 
Additionally, pairwise serializability is useful in cases where there exist only 
two security levels. In such cases, the effect of ensuring pairwise serializability 
is equivalent to that of ensuring serializability. This distinction is important 
because the algorithms to provide pairwise serializability (presented in section 
5.3.3) are less expensive compared to those providing the usual serializability 
and, therefore, for an MLS system that has just two levels, there is no reason to 
use an expensive algorithm that provides serializability. 
5.3.1 
LEVELWISE SERIALIZABILITY 
This is the weakest form of correctness among all the three. This crite-
rion may be used in situations where each integrity constraint is defined over 
data belonging to only one security level. Informally, levelwise serializability 
preserves the following two properties: 
1. it ensures correct execution of transactions if transactions executing at a 
single security level alone are considered, and 
2. it gives consistent data whenever a transaction reads from a lower security 
level. 
What we mean by consistent data is explained with the help of an example. 
Consider a corporate database that contains several types of information about 
its employees: social security number, name, address, rank, years of service, 
regular hours worked, overtime hours worked, hourly rate, weekly pay, and 

32 
MULTILEVEL SECURE TRANSACTION PROCESSING 
yearly bonus. The information about hourly rate, weekly pay, and yearly bonus 
is considered sensitive, and is classified high, while the rest of the information is 
classified low. At the end of each week, weekly payroll is processed: First a low 
transaction is processed that updates the hours accumulated by each employee, 
followed by a high transaction that computes each employee's weekly pay. 
Obviously, when this high transaction is executed it is important that it is given 
a consistent copy of the low data. It would be undesirable to give the high 
transaction the regular hours worked by an employee during one week, but 
overtime hours for some other week. The second requirement has been added 
to prevent this type of anomalies. 
It is easy to build schedulers that satisfy levelwise serializability require-
ments. In fact, the concurrency control protocol that has been implemented in 
Trusted Oracle [MG90] (described in section 3) generates levelwise serializable 
histories, as shown in figure 3.5. 
In [AJB96], Atluri, Jajodia and Bertino propose a different protocol that 
exploits the presence of multiple versions of the data. This algorithm, unlike 
that of Trusted Oracle, 
• uses multiversion timestamping ordering protocol for scheduling the oper-
ations of transactions accessing data items at their own level, and therefore, 
offer better concurrency, 
• selects an appropriate committed version of the lower data items by com-
puting a read consistency point for each transaction, which is the minimum 
of timestamps of all active transactions at that level. The version given to a 
transaction reading lower level items is the latest version whose timestamp 
is smaller than its read consistency point. 
Since levelwise serializability provides a weak form of correctness, it suffers 
from inconsistent retrieval problems, especially when a transaction attempts to 
read data from more than one security level. 
Consider once again the corporate database of the above example. Suppose 
some high transactions are executed from time to time that require some low 
information about an employee in addition to some high information. Under 
levelwise serializability, there is no guarantee that these high transactions will 
see a consistent view of the database. As an example, consider the following 
high transaction T1: Tl reads the number of hours worked and the total pay 
for an employee. It is reasonable to expect that whenever Tl is executed, the 
values returned for an employee will satisfy the following constraint: 
(Number of hours worked) * (Hourly rate) = Total pay 
Since this integrity constraint involves data at two levels, there is no guarantee 
with just levelwise serializability that Tl will be provided with a consistent view. 

Transaction Processing in MLS Databases 
33 
It is possible that TI will be given for an employee the hours worked for one 
week, but the total pay for a different week. We elaborate on this next. 
For an employee, let n denote the number of hours worked during a week, 
u the hourly rate, and P the total pay. Also assume that the database maintains 
for each employee the raise in benefits, denoted by b. Thus, u, P and b are high 
items, and n is a low item. 
Assume that a low transaction T2 updates the hours worked by an employee, 
and another high transaction T3 reads the hours worked and hourly rate and 
writes the weekly pay, and as mentioned above, TI reads the hours worked and 
weekly pay for an employee and computes the raise in benefits. Assume that 
the database is initialized with versions no, Uo, Po and bo. 
high: 
/ 
Figure 3.5. 
A multiversion schedule that can be generated by the Trusted Oracle 
Consider the following scenario: Suppose the low transaction T2 arrives first, 
followed by the high transaction TI . TI issues the read request for the low item 
n and is given the version no. Before it can issue the next instruction, Tl is 
suspended. T2 completes the execution, at which point the high transaction T3 
arrives and starts execution. Once T3 completes, TI has a chance to complete as 
well, yielding the multiversion history in which the high transaction Tl reads 
no which is the hours worked by an employee during the previous week, while 
it reads PI which is the pay for the current week. Although this history, shown 
in figure 3.5, is levelwise serializable, it is clearly not desirable. Note that 
this history can also be generated by the Trusted Oracle algorithm. We need a 
stricter correctness criterion to prevent such anomalies. 
5.3.2 
ONE· ITEM READ SERIALIZABILITY 
In addition to levelwise serializability, one-item read serializability addition-
ally guarantees the following property: 
Whenever two transactions of the same security level read a data item 
from a lower level, each of them is given an appropriate version of that 
data, i.e., the version given to the later transaction (later in the serialization 
order) never precedes the version given to the earlier transaction. 
To see how one-item read serializability avoids the anomaly described in 
the history presented in the previous section, notice that T3 precedes Tl in the 

34 
MULTILEVEL SECURE TRANSACTION PROCESSING 
history and version no precedes n2. Therefore, it will not be the case that Tl is 
given an earlier version no while T3 is given a latter version n2 of the low item 
n. 
Returning to the corporate database example, suppose at the end of each 
year, each employee's rank is evaluated, and each employee is given an yearly 
bonus based on the performance during that year. Also, at the end of each year, 
every employee's hourly rate is updated based on the years of service and the 
yearly bonus, for the future year. For an employee, let x and z denote the years 
of service and rank, respectively, and let y denote the yearly bonus. Thus,.7: 
and z are classified low, while y is classified high. Suppose that the database 
has been initialized with versions Xo, Yo, and Zo, respectively. 
Assume T2 is a low transaction that updates the years of service and the rank 
for an employee, T3 is a high transaction that updates the yearly bonus based 
on the years of service and the bonus for the previous year, and Tl is a high 
transaction that updates the hourly rate of an employee based on the years of 
service and the yearly bonus. 
Suppose that the high transaction Tl arrives first, issues the read request for 
the low item x and is given the version Xo. Before it can issue the next instruc-
tion, Tl is suspended. Later the low transaction T2 arrives which completes the 
execution by updating the low items x and z, thus creating versions X2 and Z2. 
At this point, the high transaction T3 arrives and creates a new version Y3 of y 
by reading Z2 and Yo. After the execution of T3, Tl has a chance to complete as 
well. Therefore, Tl issues its read operation to read y. Although the resulting 
multiversion history is one-item read serializable, it suffers from a yet another 
inconsistent retrieval problem: The high transaction Tl reads xo, the years of 
service for the previous year, and Y3, the bonus for the current year. This is 
surely not desirable in this case. The next correctness criterion is designed to 
correct this anomaly. 
5.3.3 
PAIRWISE SERIALIZABILITY 
This is a stricter form of correctness than those provided by both levelwise 
serializability and one-item read serializability. It ensures correct execution of 
transactions at any two levels. In other words, it ensures the following property: 
The combined history of any two security levels is one-copy serializable. 
Both the protocols described in section 5.3.1 that guarantee levelwise seri-
alizability can be modified to generate pairwise serializable histories. [JA921 
proposed a modified version of the Trusted Oracle protocol. The only modifi-
cation is that this protocol 
• uses both timestamp ordering and two-phase locking to schedule the oper-
ations of transactions accessing data items at their own level. 

Transaction Processing in MLS Databases 
35 
The levelwise serializability algorithm can also be modified to produce pair-
wise serializable histories. It differs in the way in which it computes the read 
consistency point of a transaction. 
• The read consistency point of a transaction ~ is determined in such a way 
that it is never larger than the read consistency point of any transaction Tj 
at the level of Ti whose timestamp is larger than that of Ti. 
Pairwise serializability does not guarantee one-copy serializability if transac-
tions read from more than two security levels. The following example demon-
strates the inconsistencies encountered with pairwise serializability. 
Returning once again to the corporate database, suppose that there is yet an-
other security level, called very high. At this level, data about future corporate 
plans are kept. Transactions at this level typically read data at two lower levels 
and make some projections based on the data that is read. Pairwise serializ-
ability cannot guarantee that transactions at very high level will see a consistent 
database state if they read data from both the lower two security levels. 
Consider the following three transactions: Suppose a high transaction TI 
wishes to update the high data item x by first reading low data item y and then 
reading x, a low transaction T2 simply updates y, and a very high transaction T3 
that reads both y and x. Consider the following concurrent execution of these 
three transactions. Suppose first TI arrives and issues read operations on y and 
x. Before it issues the write operation on x, it gets suspended. Meanwhile, T2 
arrives and finishes its execution and thus creates a new version Y2 of y. As 
soon as T2 terminates, T3 arrives and is fortunate enough to successfully read 
both y and x. At this point of time, Tl wakes up and writes x thereby creating 
Xl. Notice that by the time T3 issues both its read operations, only Y2 and Xo 
are available. Thus T3 observes inconsistent data since it is given a new version 
Y2 but an old version Xo. One-copy serializability eliminates such anomalies. 
5.4 
USING TRANSACTION ANALYSIS TO ACHIEVE 
SERIALIZABILITY 
In [ABJ93], Atluri et al. propose an approach that shows how serializability 
can be achieved even though one uses a scheduler that guarantees only a weaker 
form of correctness. They use a prescheduler which analyzes transactions based 
on their read- and write-sets and submits them to the scheduler in a specific 
order so that the resultant schedule is always serializable. In particular, they 
have shown how the schedules generated by a Trusted Oracle, which guarantees 
only levelwise serializability, can ensure stricter correctness such as pairwise 
and one-copy serializability. This approach is based on the assumption that 
transactions that run during a certain interval are known in advance. Later, in 
[ABJ94], they have proposed a prescheduler that relaxes this assumption. 

36 
MULTILEVEL SECURE TRANSACTION PROCESSING 
5.5 
MULTILEVEL TRANSACTIONS 
Unlike the transactions we have discussed so far, multilevel transactions 
allow a user to read and write information at multiple classification levels. 
This is done by decomposing a transaction into multiple subtransactions, each 
of which is assigned a single classification level and whose actions obey the 
simple security and *-properties. Through these multiple subtransactions, the 
multilevel update transaction is able to perform its task of reading and writing at 
multiple classification levels, while at the same time satisfying the simple and 
*-properties. Each subtransaction is a flat transaction in the sense of [BHG87]. 
In many applications, it is desirable to ensure atomicity of multilevel update 
transactions. However, it turns out that this atomicity requirement conflicts with 
security requirements. To see this, consider a sequence of actions P from several 
subjects. For every classification level l we construct a new sequence PI by 
removing all of the actions in P with subject classification levels not dominated 
by l. The MLS property requires that a subject with classification level I not 
be able to distinguish between the behavior of the system in response to P and 
Pl. A scheduler that unconditionally aborts all transactions submitted to it will 
trivially ensure both atomicity and security. Such schedulers are obviously 
not very useful and we wish to exclude them from further consideration. We 
will focus on what we call reasonable schedulers [MK93], those that do not 
delay operations or abort transactions in serial histories consisting solely of flat 
transactions. Our argument that atomicity and security are conflicting is based 
on the input schedule shown below: 
a 
P= 
r(y)w(x)c 
r(x)w(y)c 
Each bracket introduces a transaction whose name appears on the left. Each 
subtransaction is introduced by its name followed by its classification level in 
parenthesis. The order of the actions from left to right indicates the order in 
which the actions are submitted for execution. Atomicity requires that if Tj 
commits, it read the original version of x and not the one written by Tiy. This 
is necessary because Ti is aborted. Thus, the only choices are for Tj to abort, 
or to commit and read the initial value of x. 
Now, consider the input schedule P L shown below, which contains all actions 
in P with security levels dominated by L. 
PL = 
Ti [ Tiy(L): r(y)w(x)c 
Tj [ Tjz(L) : 
r(x)w(y)c 
This is a serial schedule composed solely of single-level transactions. Thus, 
because we assume the scheduler is reasonable, both Ti and Tj are output 

Transaction Processing in MLS Databases 
37 
without delay and neither is aborted. The schedule executed by the scheduler is 
equivalent to the serial schedule TiTj. It is not equivalent to Tj~ as there is no 
way for Ti to read y from Tj. Therefore, in the input schedule P L, the scheduler 
maps Tj : r(x) to the version of x written by~. Transaction Tj receives 
a response to an input in the schedule p that is different than the response it 
receives in the schedule purge(L,p). Therefore, the system is not MLS. 
This demonstrates that no protocol providing atomicity will satisfy the MLS 
property. However, satisfaction of the MLS property is sufficient, but not nec-
essary to ensure security. A non-MLS system may still be considered secure if 
the interference does not allow effective communication. 
Blaustein et al. [BJMN93] show that there are two crucial data dependencies 
which can occur in a multilevel transaction T: a low write followed by a high read 
of an item and a high read followed by a low read of an item. If all dependencies 
in T are of the first kind, T can be correctly executed (without affecting the 
dependencies) using a secure low-first algorithm. Similarly, if all dependencies 
in T are of the second kind, T can be correctly executed using a secure high-
ready-wait algorithm.4 However, ifT has dependencies of both kinds, T cannot 
be correctly executed while meeting both security and atomicity requirements. 
Blaustein et al. [BJMN93] develop a model of multilevel atomicity that defines 
varying degrees of atomicity and recognizes that low security level operations 
within a transaction must be able to commit or abort independently of higher 
security level operations. 
All work on multilevel transactions cited above relies on the standard conflict 
serializability [BHG87] as the correctness criterion. Ammann et al. [AJR96] 
propose an alternative notion of semantic atomicity which guarantees that ei-
ther all or none of the actions of a multilevel transaction are present in any 
history. The notion of correct executions in this model is based on semantic 
correctness-that is, maintenance of integrity constraints-rather than serial-
izability. They give a method whereby the developer of an application can 
statically analyze the set of transactions defined by the application and deter-
mine if the set ensures semantic atomicity. 
5.6 
SECURE COMMIT PROTOCOLS 
Jajodia and McCollum [JM93] study the secure analogs of 2PL and commit 
protocols. They modify 2PL to give a secure 2PL protocol (S2PL) that yields 
serializable schedules. Every transaction in S2PL must satisfy the following 
four properties: (1) Well-formed - A transaction must obtain a shared lock (S-
lock) before reading a data item and an exclusive lock (X-lock) before writing a 
data item. (2) Two-Phase - A transaction cannot request additional locks once 
4High-ready-wait has no storage channels, but has a limited bandwidth timing channel. 

38 
MULTILEVEL SECURE TRANSACTION PROCESSING 
(coordinator of T2 ) 
(coordinator of T 1 ) 
high: rl [xl prepared 
wdzl 
Tl releases locks 
low: 
Figure 3.6. 
The distributed schedule D 
it has issued an unlock action. (3) Strict - A local transaction holds on to all its 
locks until it completes. A subtransaction, on the other hand, may release any 
S-locks once it enters a prepared state. The subtransaction keeps all X-locks 
until it receives a commit or abort decision from its coordinator. (4) Covert 
channel free - A high transaction must release its S-lock on a low data item 
when a low transaction requests an X-lock on the same data item. In such an 
event, the high transaction can either abort or start over. 
Jajodia et al. [JMB94] recognized that while straightforward modifications to 
2PC (and some optimizations of 2PC, e.g., presumed commit) yield a protocol 
that can be integrated with S2PL without any violation to global consistency, 
this is not so with EP. We illustrate the difficulty by way of an example. 
Let Tl and T2 be two distributed transactions as follows: 
Tl = rl [xlrdylwdz], 
T2 = w2[xlw2[yl, 
L(Tl) = high 
L(T2) = low 
Suppose Tl is initiated at node Nc, and T2 is initiated at node Nb. Further-
more, assume that data item x is stored at node Na, y is stored at Nb, and z is 
stored at Nc. The coordinators, Nc and Nb, generate the subtransactions, and 
send them to the corresponding remote nodes. Accordingly, Nc divides Tl into 
three subtransactions, TI,a, T1,b and T1,c, and then sends T1,a and T1,b to Na 
and Nb, respectively. Similarly, Nb, upon dividing T2 into two subtransactions 
T2 a and T2 b, sends T2 a to Na. 
, 
, 
, 
The execution of these subtransaction at each of the nodes may result in a 
distributed schedule D, as shown in figure 3.6. At Na, the following sequence 
of events takes place: After successful execution, the subtransaction TI,a votes 
yes and enters the prepared state. At this point, the low-level subtransaction 
T2,a arrives. T1,a releases its S-lock on x, enabling T2,a to acquire an X-lock on 
x. At Nb, the subtransaction T1,b is successfully executed after the commit of 
T2,b. At Nc, TI is committed after the coordinator receives the yes vote from 
all the participants. 
Clearly, the distributed schedule D is not serializable since Tl is serialized 
before T2 at Na, while the serialization order is reversed at node Nb. A mo-

Transaction Processing in MLS Databases 
39 
ment of reflection shows that this inconsistency arises because the distributed 
transaction TI is not two-phase, although it is well-formed. 
5.6.1 
SECURE EARLY PREPARE (SEP) 
Atluri et al. [ABJ95] give a different modification to EP, called Secure Early 
Prepare (SEP), which ensures global consistency of data. It modifies EP as 
follows: Assume Ti is decomposed into n subtransactions, and ~,j, the sub-
transaction at the originating node N j , is one among them. In the prepare 
phase, the coordinator generates subtransactions ~,1, ~,2"'" ~,n and sends 
them to the participating nodes N I , N2, ••• Nn , respectively. The coordinator 
also sends the security level s with each subtransaction. At each participant Nb 
Ii,k must first acquire an S-Lock (X-lock) on an item before it is read (written). 
S-locks as well as X-locks are long locks. If Ii,k completes successfully, the 
participant augments its yes vote to the coordinator with a read-low indicator. 
A one-bit read-low indicator is added whenever Ti k has read an item from a 
, 
lower level. Otherwise, it sends a no vote. 
During the decision phase, if the coordinator receives yes votes from all the 
participants and if no subtransaction has read data from lower levels, it commits 
Ii and then sends commit messages to all its participants. An extra round of 
messages is required between the coordinator and all those participants Nj that 
sent the read-low indicator with their yes vote as follows: The coordinator first 
sends to each Nj a confirm message to confirm the commit. If N j has not 
released its S-locks on any lower level data item during the time it has been 
in the prepared state, it responds with a confirmed message; otherwise, it 
sends a not-confirmed message. If the coordinator receives a confirmed 
message from all Nj's to which the coordinator has sent the additional round 
of messages, then it sends commit messages to all its participants; otherwise it 
sends abort message. On the other hand, if the coordinator receives at least 
one no vote or if it times out waiting for a vote, it aborts the transaction, and 
sends abort messages to all participants. The exchange of messages is shown 
in figure 3.7. 
SEP as described above has two drawbacks. First, if we compare the number 
of messages required by SEP to that required by EP, EP always requires about 4n 
messages (where n is the number of participants), while SEP sometimes requires 
more than 4n messages. An extra round of messages is required between the 
coordinator and those nodes where the subtransactions have read data from the 
lower levels. That is, if m subtransactions read from lower levels, SEP requires 
4n + 2m message exchanges in total. Second, SEP is overly pessimistic. Any 
high subtransaction that reads low data is aborted if any of its S-locks on the low 
data are broken while it waits for the confirm message from the coordinator. 
Thus, SEP aborts a transaction if there is a possibility of a violation of the 
two-phase requirement. It is entirely possible that the transaction is two-phase, 

40 
MULTILEVEL SECURE TRANSACTION PROCESSING 
even though some of the S-locks are broken. Atluri et al. [ABJ95] propose 
an optimization to SEP, called 03SEP. 03SEP assumes that the clocks in the 
distributed system are synchronized. During the prepare phase, the coordinator 
first computes the latest time at which locks by all subtransactions have been 
acquired, and it sends an additional round of messages to only those participants 
who has voted before this latest lock acquisition time. During the decision 
phase, the coordinator computes the earliest time at which any lock has been 
released by any subtransaction, and it aborts the transaction only if this earliest 
lock release time is less than the latest lock acquisition time. Thus, 03SEP 
reduces both the number of messages and the number of transactions being 
aborted. 
Ray et al. [RBJM96] propose an advanced secure commit protocol (ASEP), 
a modification to SEP. Like SEP, ASEP sends an additional round of messages 
to all subtransactions which have read low data items. However, in ASEP, 
subtransactions can roll back to a prespecified save point and reexecute the 
subtransaction (as in [BJMR98]) if they release their read locks on low data 
items. If a subtransaction successfully completes, then it sends a yes message. 
ASEP checks this by sending messages to these participants repeatedly until 
it receives all yes responses. However, if any of the participants sends a no 
message, the coordinator aborts the transaction. 

Transaction Processing in MLS Databases 
41 
-----------------------------------------1 
I 
coordinator 
subordiante 
work-request, 
prepare 
yes, read-low 
confirm 
confirmed 
commit 
acknowledge 
6m messages 
coordinator 
subordiante 
work-request, 
prepare 
yes 
commit 
acknowledge 
4n messages 
I ------------------------------------ ______ 1 
Figure 3.7. 
SEP Protocol 

Chapter 4 
SECURE TRANSACTION PROCESSING IN REAL-
TIME DATABASES 
A system that is specifically designed for efficiently processing transactions 
with timing constraints is referred to as a Real-Time Database System (RTDBS). 
The objective of the system here is to meet the transaction deadlines, that is, 
to process transactions before their deadline expires. RTDBS's performance 
is often measured in terms of miss percentage or the fraction of transactions 
that are unable to complete before their deadline expires. Typically, an RTDBS 
should keep the miss percentage as low as possible. In addition to improv-
ing real-time performance by helping the transactions to meet their individual 
time constraints, an RTDBS has to ensure the data integrity constraints just 
like in a conventional transaction processing scenario. In other words, RT-
DBS design becomes significantly complex due to having to address the above 
two constraints simultaneously. Over the past decade, a considerable amount 
of research has been devoted for developing priority assignment mechanisms, 
concurrency control techniques, buffer management policies, and transaction 
processing components that are specifically tailored for the RTDBS environ-
ment. However, the issue of ensuring security of RTDBS began to receive 
adequate research attention only recently. 
Many RTDBS applications arise in safety-critical installations such as mil-
itary systems, avionics and financial applications such as stock trading, elec-
tronic negotiations etc. Such environments are multilevel, in the sense that data 
stored is often classified into various classification levels based on sensitivity 
of information; and the database is shared by users belonging to many secu-
rity clearance levels. Consider a real-time stock trading system handling the 
transaction, "sell all my stocks of a particular company before 5pm if the price 
is above $10.00". Several issues are to be addressed here: (1) The task has a 
deadline, it has to be completed before 5pm. (2) The task involves processing 
of enormous amount of data. While doing so, the data integrity constraints are 
V. Atluri et al., Multilevel Secure Transaction Processing
© Kluwer Academic Publishers 2000

44 
MULTILEVEL SECURE TRANSACTION PROCESSING 
to be enforced just like in a regular database system. (3) The minimum selling 
price is sensitive and should be kept confidential. Note that buyers being given 
access to this information may reduce the seller's profit. Therefore the buyer's 
need-to-know should exclude the minimum selling price. An RTDBS is capa-
ble of addressing the first two concerns, namely real-time constraints and data 
integrity constraints. However, a generic RTDBS is not designed for handling 
sensitive information. 
We review some of these research findings of providing security in RTDBS 
in this chapter. We begin with an introduction to RTDBS and then discuss 
the complexity of ensuring security in such systems. We then move on to 
the concurrency control issues. It has been observed that security constraints 
make the system unfair to the high security level transactions. We conclude the 
chapter with a discussion on fairness mechanisms. Buffer management policies 
and related issues are discussed in chapter 6. 
1 
INTRODUCTION TO REAL-TIME DATABASES 
As discussed in the beginning of this chapter, in RTDBS transactions typi-
cally have time constraints, usually specified as deadlines. Based on how the 
application is impacted by the violation of time constraints, real-time applica-
tions can be grouped into three categories: (1) hard deadline; (2) firm deadline; 
and (3) soft deadline. In figure 4.1, the utility of completing a transaction with 
respect to time under the three categories is featured. 
For a hard deadline application, missing a deadline is equivalent to a catas-
trophe. For example, in a critical application like nuclear power plant control, 
missing a deadline can lead to a great disaster. To explain this point further, 
consider the first graph in figure 4.1 where the utility of completing a transac-
tion in a hard real-time system with respect to time is featured. Imagine that 
the deadline of a particular transaction is 5am. If the transaction is unable to 
complete at or before 5am, the utility drops to negative infinity indicating a sig-
nificant penalty for the deadline miss. Since the penalty of missing deadlines 
in such systems can be extremely high, hard real-time systems often require to 
meet all deadlines. Database systems for efficiently supporting hard deadline 
real-time applications, where all transaction deadlines have to be met, appear 
infeasible [SZ88]. 
Most applications have softer time constraints. For firm deadline or soft dead-
line applications, however, missing deadlines leads to a performance penalty 
but does not entail catastrophic results. For firm deadline applications, com-
pleting a task after the expiry of the deadline is of no utility and in fact may 
be harmful. Therefore, late tasks are required to be permanently aborted and 
discarded from the system. The utility function of a firm deadline system is 
featured in the second graph of figure 4.1. As can be observed from the graph, 
the utility function drops to zero at the expiry of the deadline. 

f 
Secure Transaction Processing in Real-Time Databases 
45 
Utility Function -
Hard Real-Time 
20 
15 
1o+-------------, 
-5 
-10 
_15 
-200~--~--~~--~----7----5~---L----~---L----~---",0 
Tim, 
Utility Function -
Firm Real-Time 
20 
15 
10'~----------------------_, 
-5 
_10 
_15 
-20 0 
10 
Tim. 
Utility Function -
Soft Real-Tim' 
20 
15 
10 
-5 
-10 
-15 
-200:----7----7----7----7----75----~---7----~--~----~,0 
Time 
Figure 4.1. 
Utility Function 

46 
MULTILEVEL SECURE TRANSACTION PROCESSING 
The distinction between firm deadline and soft deadline applications lies in 
their view of late tasks. In firm real-time systems, the tasks that miss their 
deadlines have no value. For soft deadline applications, however, there is some 
diminished utility to completing tasks even after their deadlines have expired. 
This scenario is featured in the last graph in figure 4.lc. Most of the real-time 
database applications usually fall under either firm deadline or soft deadline 
categories. 
Intime Transactions 
Input Transactions 
RTDBS 
Late Transactions 
Figure 4.2. 
Real-Time Database 
In figure 4.2, a typical RTDBS is featured. The input transactions have 
associated time constraints expressed in the form of deadlines. The output 
transactions could be partitioned into two sets: intime transactions and late 
transactions. In a firm deadline system, typically, all late transactions are killed 
and discarded at the expiry of the deadline. 
In conventional database systems, the design objective is to improve the 
response time. In an RTDBS, however, the main objective is to maximize 
the number of transactions meeting their deadline. Therefore, improving the 
response time of one transaction at the expense of another can result in a system 
performance improvement [Har91]. Imagine the case where two transactions 
arrive at the database system, one with a tight deadline and the other with a loose 
deadline. If these transactions share the system resources on an equal basis, the 
urgent transaction misses its deadline while the slack transaction completes in 
time. 
Alternatively, if the urgent transaction progresses at the expense of the slack 
transaction, it completes before its deadline. The slack transaction then, by 
virtue of its loose deadline, completes in time despite its retarded progress. 
Therefore, providing differential service to transactions in RTDBS can result 
in performance improvement. 

Secure Transaction Processing in Real-Time Databases 
47 
1.1 
PRIORITY ASSIGNMENT POLICIES 
The priority assignment policy decides the service precedence among trans-
actions to the daiabase system resources and thereby improves the system per-
formance. Popular examples of such priority assignment policies include Ear-
liest Deadline and Least Slack. The Earliest Deadline priority mapping [LL 73] 
assigns higher priority to transactions with earlier deadlines. For example, be-
tween the two transactions having deadlines 8:10am and 8:15am the Earliest 
Deadline gives preference to the 8: lOam transaction. The Least Slack [AGM92] 
policy gives maximum priority to the transaction that has the lowest comple-
tion opportunity. Imagine that the current time is 8:00am and the remaining 
execution time requirement for transactions are 5min and 10min respectively. 
Under such circumstances, Least Slack gives preference to the 8: 15am transac-
tion. Note, however, that the Least Slack priority assignment requires a priori 
knowledge of the transaction's execution time requirement and obtaining such 
knowledge may be difficult in a general real-time transaction processing sce-
nario. 
2 
ADDITIONAL REQUIREMENTS 
In addition to providing security just like in a conventional database sys-
tem, a Multi Level Secure Real-Time Database System (MLS RTDBS) has to 
simultaneously satisfy another requirement - minimize the number of killed 
transactions. However, the mechanisms for achieving the individual goals of-
ten work at cross-purposes [G+93]. In an RTDBS priority is usually assigned 
to transactions based on deadlines in order to help their timely completion, i.e., 
a tight deadline transaction is usually given higher priority than a slack deadline 
transaction. On the other hand, in an MLS DBMS, low security transactions are 
given high priority in order to avoid covert channels. Now, imagine the situa-
tion wherein a high security process submits a transaction with a tight deadline 
to an MLS RTDBS. In this case, priority assignment becomes difficult since 
assigning a high priority may cause a security violation, whereas assigning a 
low priority may result in a missed deadline. 
2.1 
SECURE PRIORITY ASSIGNMENT 
From the discussion above it is clear that assigning priority solely based on 
deadlines may result in security violations. Therefore, the priority assignment 
should be made in a secure manner such that the non-interference requirements 
are satisfied. For example, a vector P = (level, intra) could be used for 
priority assignment, where level is the transaction security level and intra the 
value assigned by the priority mechanism used within the level [GH97]. Note the 
value of intra is often dependent on the deadline of the transaction. Figure 4.3 
features the secure priority assignment, the real-time priority assignment and the 

48 
MULTILEVEL SECURE TRANSACTION PROCESSING 
secure real-time priority assignment. In a secure system, where a conflict occurs 
between a low security level transaction and a high security level transaction, 
the low security level transaction is given preference. In a real-time system the 
earliest deadline transaction could be given preference. In a secure real-time 
system, if the security levels of two conflicting transactions differ, then lower 
security level transaction of the two is given preference. If the security levels of 
two conflicting transactions are equivalent, then the earlier deadline transaction 
is given higher priority. 
LEVEL 
DEADLINE 
LEVEL 
DEADLINE 
Security 
Real·Time 
Secure Real· Time 
Figure 4.3. 
Secure Priority Assignment 
3 
CONCURRENCY CONTROL 
In this section we discuss various concurrency control protocols suggested 
for the secure real-time transaction processing domain. Though individual 
mechanisms may vary, all protocols are designed with a common objective, 
improve the real-time performance and ensure fairness as much as possible 
while simultaneously enforcing the non-interference requirements. 
3.1 
REAL-TIME PROTOCOLS 
Since most real-time concurrency control protocols are specifically tailored 
for improving the real-time performance, some of these protocols could perhaps 
be easily migrated to the MLS RTDBS environment provided they can be re-
designed to meet the non-interference requirements. In figure 4.4 the design of 
secure real-time concurrency control protocols is featured. In real-time concur-
rency control protocols, priority is usually assigned based on deadlines. Instead, 
if the secure priority assignment described in section 2.1 can be employed, the 
protocols may be adapted to the MLS RTDBS environment. 
RT CC Protocol 
+ 
Secure Priority Assignment 
Secure RT CC Protocol 
Figure 4.4. 
Secure Real-Time Concurrency Control 
However, not all o/the previously proposed real-time concurrency control al-
gorithms are amenable to supporting security requirements [GH97] even with 
the secure priority assignment mentioned in section 2.1. For example, con-
sider the 2PL Wait Promote algorithm proposed in [AGM92]: This protocol, 

Secure Transaction Processing in Real-Time Databases 
49 
which is based on 2PL, incorporates a priority inheritance mechanism [SRL87] 
wherein, whenever a requester blocks behind a lower-priority lock holder, the 
lock holder's priority is promoted to that of the requester. In other words, the 
lock holder inherits the priority of the lock requester. The basic idea here is to 
reduce the blocking time of high priority transactions by increasing the priority 
of conflicting low priority lock holders (these low priority transactions execute 
faster and therefore release their locks earlier). Note that the Wait Promote 
approach is not suitable for MLS RTDBSs. This is because, it permits the 
blocking of high priority transactions by low priority transactions, which vio-
lates the requirement of non-interference between the transactions of different 
security levels. 
In short, a real-time concurrency control protocol that permits priority inver-
sion [SRL87] to even a limited extent cannot be used in a MLS RTDBS. Apart 
from Wait Promote, other examples of real-time concurrency control algorithms 
that fall into this category include WAIT-50 [Har91] and 2PL-OSIBI [AAJ92]. 
In the reminder of this section we describe those real-time concurrency control 
protocols that can be made secure merely by resorting to the secure priority 
assignment. 
3.1.1 
2PL HIGH PRIORITY 
The 2PL High Priority (2PL-HP) scheme [AGM92] modifies the classical 
strict two-phase locking protocol (2PL) [E+76] by incorporating a high priority 
conflict resolution scheme, which ensures that high priority transactions are not 
delayed by low priority transactions. In 2PL-HP, when a transaction requests 
a lock on a data item that is held by one or more higher priority transactions 
in a conflicting lock mode, the requesting transaction waits for the item to be 
released (the wait queue for a data item is managed in priority order). On 
the other hand, if the data item is held by only lower priority transactions in 
a conflicting lock mode, the lower priority transactions are restarted and the 
requesting transaction is granted the desired lock. A new reader can join a group 
of lock-holding readers only if its priority is higher than that of all the writers 
waiting for the lock. Note that if priorities are assigned uniquely, 2PL-HP is 
inherently deadlock-free. 
3.1.2 
OPT-SACRIFICE 
The OPT-SACRIFICE algorithm [Har91] modifies the classical forward 
(or broadcast) optimistic concurrency control protocol (OPT) [Rob82, MN82, 
Har84] by incorporating a priority sacrifice mechanism. In this algorithm, a 
transaction that reaches its validation stage checks for conflicts with currently 
executing transactions. If conflicts are detected and one or more transactions 
in the conflict set is a higher priority transaction, then the validating transac-

50 
MULTILEVEL SECURE TRANSACTION PROCESSING 
tion is restarted - that is, it is sacrificed in an effort to help the higher priority 
transactions make their deadlines. 
3.1.3 
OPT-WAIT 
The OPT-WAIT algorithm [Har91] modifies the forward optimistic protocol 
by incorporating a priority wait mechanism. In this algorithm, a transaction that 
reaches validation and finds higher priority transactions in its conflict set is "put 
on the shelf", that is, it is made to wait and not allowed to commit immediately. 
This gives the higher priority transactions a chance to make their deadlines first. 
While a transaction is waiting, it is possible that it will be restarted due to the 
commit of one of the conflicting higher priority transactions. 
3.1.4 
PERFORMANCE EVALUATION 
In [GH97], George and Haritsa study the performance of secure real-time 
concurrency control protocols described above. For this purpose, they de-
veloped a full fledged simulation model of MLS RTDBS derived from the 
single-site real-time database model of [Har9l]. Their model has six compo-
nents: a Database that models the data and its layout; a Source that generates 
the transaction workload; a Transaction Manager that models the execution 
of transactions; a Resource Manager that models the hardware resources; a 
concurrency control Manager that controls access to shared data; and a Sink 
that gathers statistics on transactions exiting the system. The model was tested 
under a variety of workload and system configurations. We summarize their 
findings in this section. For the details please refer to [Geo98]. 
In their study, the comparison is made between a fully secure system in which 
covert channel security is also ensured and a system where only Bell-LaPadula 
access conditions are enforced. The purpose is to find out the performance 
cost to be paid for ensuring covert channel security in an RTDBS environment. 
Recall that in the secure system, secure versions of concurrency control pro-
tocols are used and the priority is assigned by the vector (class, deadline), 
where class is the security level and deadline is the deadline ofthe transaction 
respectively. In unsecure system, real-time concurrency control protocols are 
employed and the priority is assigned by deadline. 
Most of the experiments were reported on a two-level system that has two 
security levels namely secret and public. They observe that at low loads secure 
versions of concurrency control protocols perform worse than their unsecure 
counterparts. In contrast, under heavy loads the performance of the secure 
protocols is actually better than that of the unsecure protocols. The reasoning 
given is that while the Earliest Deadline priority assignment is excellent for a set 
of tasks that can be completed before their deadlines, it becomes progressively 
worse as the task set overloads its capacity [JLT95, Har91]. In this situation, 
the secure protocols feature of grouping the transactions into prioritized levels 

Secure Transaction Processing in Real-Time Databases 
51 
means that Earliest Deadline is operational among smaller sets of transactions, 
leading to improved performance at higher loads. Although elimination of 
covert channels results in performance degradation at low loads, it is helpful 
under heavy loads. 
Another interesting observation made by them is that the performance of 
OPT-SACRIFICE and 2PL-HP is almost the same and significantly worse than 
that of OPT-WAIT. The results for 2PL-HP and OPT-WAIT are similar to those 
observed for conventional real-time transaction concurrency control [Har91]. 
The poor performance of 2PL-HP is because it suffers from wasted restarts 
(restarts caused by a transaction that is eventually killed). In contrast, in OPT-
WAIT, all restarts are made "on demand" and at the commit time of a higher 
priority transaction. 
There is an important observation made about OPT-SACRIFICE: in the (con-
ventional) real-time domain, OPT-SACRIFICE used to provide a performance 
that was intermediate to that of OPT-WAIT and2PL-HP [GH97]. InRTDBS do-
main, OPT-SACRIFICE suffered from wasted sacrifices (sacrifices for a trans-
action that is eventually killed) and it accounts for its poor performance. The 
effect of this problem is magnified in the secure domain for the following rea-
sons [GH97]: In conventional real-time concurrency control, a transaction that 
reached validation close to its deadline would usually not have to sacrifice itself 
since it would have high priority. However, in the secure model, where the 
transaction level is also a factor in the priority assignment, secret transactions 
that manage to reach their validation stage close to their deadline may still be 
restarted due to conflict with a public transaction. The negative impact of this 
restart is much more than that in 2PL-HP since the transaction is restarted only 
after having used up its entire resource requirement. 
In the MLS RTDBS domain, optimistic protocols have a specific advantage 
over locking based protocols. The definition of conflict in forward optimistic 
concurrency control is that a conflict exists between the validating transaction 
V and an executing transaction E if and only if the intersection of the write set 
of V and the current read set of E is non-empty. For the Bell- LaPadula model, 
where blind writes are permitted due to the "read-below, write-above" paradigm, 
optimistic algorithms will correctly conclude that there is no conflict between 
items that are in the intersection of the write set of V and the write set of E but 
not in the read set of E. However, since 2PL-HP is a locking based protocol, 
such conflicts show up as write-write conflicts and result in either blocking or 
restarts, thereby further deteriorating the performance of the locking protocol. 
The locking protocol2PL-HP shows poor performance at all write probabil-
ity levels compared to the optimistic protocols and this difference is magnified 
at high write probability levels. As mentioned earlier, in optimistic protocols, 
inter-level write-write conflicts do not appear in the conflict set. Therefore, at 
high write probabilities the data conflicts in fact decrease for optimistic proto-

52 
MULTILEVEL SECURE TRANSACTION PROCESSING 
cols, whereas, for locking protocols high write probability causes blocking or 
restarting of many transactions (because of the exclusive nature of write locks). 
Their studies show that at low loads when virtually all transactions make 
their deadlines, all the concurrency control protocols are fair. However, as the 
loading increases, all the protocols become increasingly unfair. The OPT-WAIT 
protocol provides greater fairness over most of the loading range as compared 
to OPT-SACRIFICE and 2PL-HP. 
Though most of their experiments were performed on a two-security-Ievel 
system, they also investigated the performance behavior for afive-security-level 
system. They observe that there is a greater difference between the performance 
of the secure concurrency control algorithms and their un secure counterparts 
at normal loads, as compared to the equivalent two-level experiments. This is 
because in a five-level system, priority is much more level-based than deadline-
based, thereby denying Earliest Deadline its ability to complete most transac-
tions in a feasible set under normal loads. Under heavy loads, however, the 
smaller sizes of the transaction sets in each level results in Earliest Deadline 
performing well for the low security transactions. 
3.2 
ADAPTIVE SECURE 2PL (AS2PL) 
Secure 2PL (S2PL) is a protocol proposed in [SD94, Dav93] that is specif-
ically tailored for the secure transaction processing environment. S2PL tries 
to simulate the execution of conventional 2PL without blocking the actions of 
low security transactions by high security transactions and thereby ensuring 
the non-interference property. This is accomplished by providing a new lock 
type called virtual lock, which is used by low security transactions that develop 
conflicts with high security transactions. The actions corresponding to setting 
of virtual locks are implemented on private versions of the data item (similar to 
optimistic concurrency control). When the conflicting high security transaction 
commits and releases the data item, the virtual lock of the low security trans-
action is upgraded to a real lock and the operation is performed on the original 
data item. To complete this scheme, an additional lock type called dependent 
virtual lock is required apart from maintaining, for each executing transaction 
Ti, a list of the active transactions that precede 1i in the serialization order and a 
list of the active transactions that follow 1i in the serialization order. The details 
are given in [DS93, SD94, DSM95, SDT88]. In conjunction with using secure 
two phase locking, the UndolRedo algorithm [BHG87] used for recovery must 
be modified to ensure the database correctness [Dav93]. 
Note that S2PL may not be suitable for the real-time environment since it 
is not priority cognizant. Also, a real-time concurrency control protocol such 
as 2PL-HP cannot be used in the secure environment since it cannot guarantee 
covert channel security. Therefore in [SDT95] an adaptive Secure 2PL (AS2PL) 
that allows partial violations of security for improved real-time performance is 

Secure Transaction Processing in Real-Time Databases 
53 
~ .. -.. -.. -.. -.. -.. -.. -.. -.. -.. 
Security/Performance Monitor 
i~ 
2PLHP 
------ieD 
Sw;tci> ..... , 
S2PL 
Figure 4.5. 
Adaptive Secure 2PL 
proposed by Son et al. Figure 4.5 captures the working of AS2PL. Every time 
a lock request is made, the protocol switches between S2PL and 2PL-HP based 
on the value of a random variable. The probability of making the secure or 
real-time choice depends on monitored measure of security in the previous 
observation interval. 
Son et al. define a measure of the degree to which the security is being 
satisfied by the system based on various uncertainty measures associated with 
communication process. The information exchange is defined by the reduction 
in entropy. They use information theory to derive the upper limit on the rate 
at which messages can be passed through the communication channel based 
solely on how the noise affects the transmission of signals. This upper limit of 
bandwidth is called the capacity. 
Note that the goal of the designer here is to minimize the capacity whereas 
in the objective of the communication channel designer would be to maximize 
the capacity and minimize the influence of noise. 
Capacity is specified in terms of two quantities: a parameter that quantifies 
the interference from other transactions or in other words the "noise" generated 
due to other transactions (ql), and the probability that a secure choice is made 
when a data conflict occurs (Q2). The value of Ql depends on the workload 
conditions and the duration of the system clock tick, and as they point out, it is 
difficult to control. Therefore, Son et al. consider only tuning the parameter Q2. 
The system will swing from fully secure to unsecure for different values of Q2 
ranging from 0 to 1. 
For a desired value of covert channel capacity, given the current Ql, the value 
of q2 is calculated. Then the algorithm observes the resulting missed deadlines 
percentage over an interval, and if the miss percentage exceeds a threshold, the 
database administrator is intimated for re-adjusting the value of the channel 

54 
MULTILEVEL SECURE TRANSACTION PROCESSING 
capacity and/or desired miss percentage. If the current miss percentage is far 
below the threshold value, the parameter q2 is decreased towards zero, such that 
the security is improved. 
Performance Evaluation 
The results of the performance study of the AS2PL is given in [SDT95]. The 
main goals of the performance analysis are to determine the miss percentages of 
protocols under varying transactionarrival rates. Since the performance study 
was carried out in a soft deadline environment, the second performance factor 
measured was the tardy time - that is the difference between the commit time 
and the deadline of late transactions. 
Son et al. in their experiments compare the performance of S2PL with the 
performance of other protocols such as Secure OCC and Secure MVTO in terms 
of average response time. At low transaction arrival rates, they report that the 
distinction between protocols is not very clear. However as the arrival rate 
increases, in secure OCC, saturation point is reached much earlier than S2PL 
and Secure MVTO. With respect to fairness, S2PL is better than Secure OCC. 
However, the performance of secure MVTO is the best in terms of both real-
time performance and fairness since in this protocol no transaction is blocked 
or aborted. Note that this good MVTO performance is offset by the staleness 
factor - high level transactions are forced to read stale data. 
In the experiment to measure the miss percentage, the performance of AS2PL 
is measured for three different settings of the Security Factors (SF) - fully secure 
(SF=1), no security (SF=O) and partially secure (SF=0.5). Note that when SF 
is set to 1, AS2PL acts just as secure 2PL and when SF is set to 0 AS2PL acts 
like 2PL-HP. They observed that the performance of the AS2PL when SF=O.5 
comes in between the 2PL-HP performance and S2PL performance as expected. 
The average tardy times for AS2PL were also measured for three different 
settings of Security Factors as in the previous experiment. The Tardy time is 
maximum for S2PL with the curve for AS2PL (SF = 0.5) falling between that 
of S2PL and 2PL-HP. 
In summary, Son et al. argue that trade-offs must be made between security, 
timeliness and consistency on a case-by-case basis. Their simulation results 
substantiates the fact that better performance can be achieved by compromising 
the security to some extent. 
3.3 
DUAL APPROACH 
In [GH97], an analysis of data conflicts arising in an MLS RTBS environment 
is given. Data conflicts can be classified into Inter-level conflicts and Intra-level 
conflicts. Inter-level conflicts are data conflicts between transactions belong-
ing to different security levels whereas intra-level conflicts are data conflicts 
between transactions of the same level. The important point to note here is 

Secure Transaction Processing in Real-Time Databases 
55 
LevelCC 
LevelCC 
Master 
CC 
-
-
LevelCC 
Figure 4.6. 
MCC architecture for Dual Approach 
that only inter-level conflicts can result in security violations, not intra-level 
conflicts. This opens up the possibility of using different concurrency control 
strategies to resolve the different types of conflicts. In particular, mechanisms 
can be constructed such that inter-level conflicts are resolved in a secure manner 
while intra-level conflicts are resolved in a timely manner. The advantage of 
this dual approach is that RTDBS can maximize the real-time performance, 
by appropriate choice of intra-level concurrency control protocol, without sac-
rificing security. Another advantage of the dual approach is that separation 
of security and timeliness concerns makes it possible to use even non-secure 
real-time concurrency control algorithms (e.g., Wait-Promote, WAIT-50) for 
resolving intra-level conflicts. The dual approach therefore empowers the use, 
even in the secure RTDBS domain, of the rich set of real-time concurrency 
control algorithms developed during the last decade. 
When multiple concurrency control protocols are simultaneously active, 
there should be additional mechanisms to ensure serializability. A detailed 
study of a generalized version of this problem is presented by Thomas et 
al. [TSH96], wherein the transaction workload consists of a mix of transac-
tion classes and the objective is to allow each transaction class to utilize its 
preferred concurrency control mechanism. They propose a database system 
architecture wherein intra-level conflicts are handled by the class's preferred 
concurrency control manager while inter-level conflicts are handled by a new 
software module called the Master Concurrency Controller (MCC) that inter-

56 
MULTILEVEL SECURE TRANSACTION PROCESSING 
faces between the transaction manager and the multiple concurrency control 
managers as shown in figure 4.6. 
3.3.1 
S2PL-WAIT 
In this dual protocol, the inter-level conflicts are resolved by S2PL and intra-
level conflicts are resolved by OPT-WAIT. Once the intra-level conflicts are 
resolved by OPT-WAIT, the transaction enters the "virtual commit" state, and 
subsequently the inter-level data conflicts are resolved by S2PL. Once all con-
flicts are resolved, a transaction "really" commits. Note that S2PL is a non-
restart oriented algorithm, and it allows a high security level transaction to 
continue regardless of the virtual commit of a low security level conflicting 
transaction. 
Performance Evaluation 
A performance study of S2PL-WAIT against OPT-WAIT is reported by George 
and Haritsa [GH97]. They find that S2PL-WAIT performs better than OPT-
WAIT, especially at lower loading levels. The reason that this combination 
works well is that S2PL handles inter-class conflicts without resorting to restarts. 
This means that secret transactions suffer much less in this environment. At the 
same time, using OPT-WAIT for handling intra-level conflicts helps to derive 
the inherent good real-time performance associated with this protocol. At high 
loads, S2PL-WAIT performs almost identically to OPT-WAIT because, in this 
region, the primary reason for transactions missing their deadlines is resource 
contention, rather than data contention - therefore, the virtual commit feature of 
S2PL rarely provides the intended benefits. Since the dual approach opens up 
the possibility of also utilizing unsecure real-time concurrency control protocols 
for resolving intra-level conflicts, they suggest that by using sophisticated real-
time protocols such as OCC-TI [LS93], for example, in combination with S2PL, 
it may be possible to realize significant performance improvements. However, 
as per our knowledge, no performance study has been conducted to support this 
claim. 
3.4 
SECURE REAL-TIME TWO-PHASE LOCKING PROTOCOL 
Secure Real-Time Two-Phase locking Protocol (SRT-2PL) [MS95] is a two 
version protocol that attempts to meet the timeliness, security and serializability 
conditions using a primary copy and secondary copy for each data object. The 
primary copy of an object is only accessed by transactions of the same security 
level for performing a read or write operation. The secondary copy is accessed 
by transactions at a security level higher than the data object as featured in 
figure 4.7. 

Secure Transaction Processing in Real-TIme Databases 
57 
Access from same level 
Primary 
Copy 
Read from higher lebel 
Secondary 
Copy 
Data Item 
Figure 4.7. 
Secure Real-Time Two-Phase Locking Protocol 
The concurrency control mechanism suggested is similar to basic 2PL except 
that the protocol is enforced on a per copy basis. The primary and secondary 
copies are treated independently for the purpose of resolving resolving lock 
conflicts. 
The write-lock request on primary copy is never blocked due to the read locks 
on the secondary. Since the secondary copy is only accessed for read by high 
level transactions, there are no data conflicts among high-level transactions. 
The updates are placed in a queue by the data manager until they are carried out 
on the secondary copy. The high level read operation is temporarily blocked 
while the secondary copy is being modified. 
For establishing that a history is serializable and therefore correct, it is suf-
ficient to show that the serialization graph for the history is acyclic [BHG87]. 
Mukkamala and Son [MS95] prove that the transactions executing at a level 
is serializable due to the use of 2PL. Even though their model does not allow 
write-ups, when transactions read data at lower levels there can be additional 
dependencies due to reads. Their claim is that even in this case cycles cannot 
arise in the serialization graph because of the static locking policy where a 
transaction is required to procure all locks before it starts executing. 
They also discuss the design and implementation issues of secondary copy. 
One of the design decisions to be made is to decide when the the updates in the 
queue to be carried out on the data object. The data manager has to guarantee 
that the view of the object will be unaltered until a transaction is committed or 
aborted. All updates that have been carried out are automatically removed from 
the queue. The size of the queue may grow over time resulting in costly read 
operations. Therefore, for reducing the queue length, updates are to be made 
to the secondary copy as early as possible. The decision to perform an update 
depends on two factors: (1) whether the update can be carried out without 

58 
MULTILEVEL SECURE TRANSACTION PROCESSING 
missing transaction deadlines due to the required temporary blocking of read 
operations (2) increase in time for the read operation (from the queue), if an 
update is not performed. 
Finally, Mukkamala and Son suggest that some trade-offs must be made 
in terms of serializability, predictability, memory availability, and security to 
meet the specific application requirements. The suggested trade-offs are: (1) 
When meeting the real-time requirements is more important, and when it is 
semantically acceptable for a specific high level transaction to directly read the 
secondary copy, then let the transaction read the secondary copy ignoring all 
queued updates. (2) When meeting the deadline of a critical high level trans-
action is important and also when it is necessary that the transaction read the 
most up-to-date value, then let the high level transaction read the primary copy 
directly, thereby reducing the overhead of read downs. However, this is at the 
cost of possible covert channel. (3) The trade-off between real-time and ba-
sic 2PL arises when a high priority transaction has a data conflict with a low 
priority transaction. Since basic 2PL is not priority cognizant, they suggest to 
use alternate versions of 2PL to handle the problem. They also caution that 
such alternatives should be adopted only in a controlled manner or may result 
in security violations. (4) If large memory is available, the read-downs can be 
made predictable by storing complete versions of an object for high priority 
transactions. It is also possible to arrive at some innovative data structures that 
can make the read-downs more efficient as well as more predictable. Also it is 
possible to bound the size of update queue, however, only at the expense of pos-
sible covert channels. (5) Incorporate alternate correctness criteria [JA92] and 
thereby more efficiently meet the real-time and non-interference requirements. 
Their model of the database system has three components: transaction man-
ager, resource scheduler and data manager. For ensuring security their model 
requires that the transaction manager to be a trusted component. All user 
transactions arrive at the transaction manager and the transaction manager re-
quests permission from lock manager for the necessary access to data elements. 
Note that, all locks are acquired before the beginning of the transaction execu-
tion. Once the transaction starts executing, the transaction manager requests 
the scheduler for execution of individual operations of a transaction. Also 
the model does not support write-up operations. The transaction manager is 
also responsible for committing or aborting the transaction. The lock manager 
manages the locks and enforces the lock conflict rules. 
The performance issues such as, the threshold queue sizes, and their influence 
on the system's ability to meet the transaction deadlines are left for further 
investigation. Also, note that no guaranteed bound on the size of the secondary 
queue can be placed without incurring the possibility of security violations. 

Secure Transaction Processing in Real-Time Databases 
59 
Secret 
Secret 
INPUT 
OUTPUT 
SRTDBS 
Public 
Public 
GUARD L 
Figure 4.8. 
GUARD Admission Control 
3.5 
FAIRNESS 
The non-interference requirement to be satisfied for ensuring covert chan-
nel security makes the system unfair towards high security level transactions. 
Therefore, there should be additional mechanisms to ensure fairness in an MLS 
RTDBS environment. The policies for ensuring fairness can be categorized into 
static and dynamic policies. Static mechanisms for ensuring fairness such as 
resource reservation can provide fairness while still maintaining full security. 
However, these policies may considerably degrade the real-time performance 
arising out of resource wastage due to their inability to readjust themselves to 
varying workload conditions. 
Dynamic mechanisms, on the other hand, have the ability to adapt to work-
load variations. However, providing full-security fairness with dynamic mecha-
nisms appears to befundamentally impossible since the fairness-inducing mech-
anism can itself become the medium of information compromise. For example, 
consider the following situation: A conspiring secret process submits a work-
load such that the secret class performance degrades. The fairness mechanism 
subsequently tries to improve the secret class performance by reserving more 
resources for the secret class. A collaborating public transaction could now feel 
the reduction in the availability of system resources and sense the presence of 
the secret process in the system. Therefore, this mechanism could be exploited 
for covert signaling. 
A detailed study of fairness mechanisms for MLS RTBS environment is 
given in [Ge098], which designed and developed a dynamic feedback based 
admission control policy named GUARD. The basic idea here is that based on 
the imbalance in the deadline miss percentages of the transaction classes, the 
admission of transactions into the system is periodically controlled. Figure 4.8 

60 
MULTILEVEL SECURE TRANSACTION PROCESSING 
shows the feedback based GUARD admission control mechanism for ensuring 
fairness in MLS RTDBS environment. 
In GUARD, the Fair Factor variable captures the degree of unfairness dur-
ing the last observation period. Ideally the FairFactor should be 1.0 and so, if 
there is a significant imbalance (e.g. Fair Factor < 0.95 or Fair Factor> 
1.05), the admit probability of the public transactions is decreased or increased 
accordingly. The increase or decrease margin has been set to a nominal value. 
Finally, to ensure that the admission control becomes operative only when the 
secret transaction class is experiencing sustained missed deadlines, a threshold 
miss percentage is included. 
Although admission control policies may provide fairness, they may also 
cause unnecessary increases in the miss percentages of all the transaction 
classes as compared to the corresponding values without admission control. 
The GUARD policy has built-in mechanisms to counter such drifting of the 
system to higher miss percentage levels. 
When GUARD policy is in use, covert channels can arise due to the admission 
control mechanism. For example, a corrupt secret user can, by modulating 
the outcomes (increase, decrease, or constant) of the admission probability 
computation, signal information to collaborating public users. Corresponding 
to each computation, [0923 = 1.6 bits of information can be transmitted (based 
on information-theoretic considerations [Sha48]) and since the computation 
is made once every T seconds, the total channel bandwidth is 1.6/T bits per 
second. By setting this equal to 1 bit per second, the condition for being orange-
secure, we get Tmin = 1.6 seconds to be the minimum re-computation period. 
A generic N security level protocol is also suggested. Note that the channel 
capacity in this general case is [0923N-1 = 1.6(N - 1) bits and therefore 
Tmin = (N - 1) * 1.6 seconds. 
Performance Evaluation 
The performance of the GUARD protocol is studied in [Ge098] using a sim-
ulation model of the MLS RTDBS. The admission control module comprises 
of a sensory mechanism and a regulatory mechanism. The sensory mechanism 
calculates the average miss percentage of each security level over a period of 
time and makes it available to the regulatory mechanism. Depending on the 
data collected by the sensory mechanism over a period of time, the regulatory 
mechanism uses a geometrically weighted combination of the sensed values of 
the sensed values over a time span and determines the mUlti-programming level 
for each of the security levels. The sensory mechanism operates once every S 
seconds and the regulatory mechanism is scheduled once every T seconds. The 
value of T is set in accordance with the degree of security required. 
Their experiments show that GpARD protocols exhibit close to ideal fair-
ness at all work loads. Further at low loads, addition of GUARD admission 

Secure Transaction Processing in Real-Time Databases 
61 
control policy to an MLS RTDBS causes only a small increase of the overall 
miss percentage, whereas at heavy loads, GUARD improves the performance 
slightly. At low loads, due to the inherent lag involved in the feedback pro-
cess, GUARD sometimes tends to be over-conservative, and thereby increasing 
the low security level miss percentage. In contrast, under heavy loads, being 
conservative does less harm than being too liberal and GUARD improves the 
performance slightly. 
A five-security-Ievel system is also evaluated with the addition of GUARD 
admission policy. Experiments reveal that GUARD exhibits close to the ideal 
fairness characteristics throughout the loading range even in the five level sys-
tem. 
4 
CONCLUSIONS 
Ensuring the security of real-time transaction processing is a complex task 
that requires several challenges to be simultaneously addressed. In this chapter, 
we have reviewed some of the important research issues in the MLS RTDBS 
domain, particularly with respect to concurrency control. All the secure con-
currency control protocols suggested for RTDBS aim at improving the real-time 
performance and simultaneously enforcing the security constraints. However, 
the actual mechanisms for achieving the above objective may vary from one 
protocol to the other. For example, many existing real-time concurrency control 
protocols which are proven to render high performance in RTDBS domain can 
be adapted to the MLS RTDBS domain by adapting to a secure priority assign-
ment policy. Some other proposed solutions combine different concurrency 
control protocols for achieving security and real-time performance objectives. 
For example in AS2PL, a dynamic choice is made between a secure concurrency 
control protocol S2PL and real-time concurrency control protocol2PL-HP ev-
ery time a lock is requested. Another case the protocol S2PL-WAIT uses a 
dual approach of concurrency control to ensure security and simultaneously 
render better real-time performance. Finally, we describe a two version secure 
real-time concurrency control protocol SRT-2PL that is expected to improve the 
real-time performance. Though almost all concurrency control mechanisms in 
MLS RTDBS domain are designed to improve the fairness to a limited extent, 
there is an add-on fairness mechanism suggested to improve the fairness even 
further. The GUARD protocol described in the final section is such a mecha-
nism. 
Though there has been a good amount of research reported in this area, there 
are many open issues to be resolved. For example, even though it has been 
shown that optimistic protocols developed are shown to be performing best in 
the MLS RTBS environment, there are open issues remaining to be addressed for 
integrating the concurrency control mechanism to the buffer manager. Though 
GUARD was shown to be effective for achieving fairness, more intelligent 

62 
MULTILEVEL SECURE TRANSACTION PROCESSING 
fairness mechanisms could be developed. Since it is often difficult to simulta-
neously achieve properties such as security, real-time performance and database 
correctness, often trade-offs are made. The right trade-off depends on the appli-
cation domain and varies from situation to situation. Therefore, it is desirable to 
design and develop decision support tools that help the database administrators 
to decide and enforce an effective security policy. 

Chapter 5 
SECURE WORKFLOW TRANSACTION 
PROCESSING 
In the previous chapters, we have focussed on transaction processing in 
traditional application domains, where transactions are short-lived and simple 
in structure. In this chapter, we tum our attention to transaction processing in 
advanced application domains, in particular, workflow environments. 
Workflow Management has emerged as the technology to automate the co-
ordination of day-to-day activities of business processes. Today wide use of 
Workflow Management Systems (WFMS) can be found in a number of domains 
including office automation, finance and banking, healthcare, telecommunica-
tions, manufacturing and production. Business processes control which piece 
of work (task) is performed by whom and which resources are required and 
used to accomplish this. A task can be a manual or automated processing ac-
tivity. Processing entities can be humans, application programs, or database 
management systems [RS94, ED97]. 
To coordinate the execution of the various activities (or tasks) in a workflow, 
a set of constraints called task dependencies are specified among them. Task 
dependencies represent a key component in ensuring the flexibility required 
to support exceptions, alternatives, compensations and so on, which all arise 
in real-life activities. A WFMS thus aims at supporting process specification, 
enactment, monitoring, coordination and administration of workflow process 
through the execution of software whose order of execution is based on the 
workflow logic [wfm94]. 
As advances in WFMS take place and their application scope widens, they are 
also required to support security, meaning that coordination among processes at 
different security levels has to be supported, indeed without violating security; 
this chapter provides a description of solutions offered by researchers. 
V. Atluri et al., Multilevel Secure Transaction Processing
© Kluwer Academic Publishers 2000

64 
MULTILEVEL SECURE TRANSACTION PROCESSING 
1 
INTRODUCTION TO WORKFLOW SYSTEMS 
In this section, we provide an introduction to the existing workflow products, 
a categorization of the products, and workflow standards. 
1.1 
WORKFLOW TECHNOLOGY AND PRODUCTS 
Workflow technology has been widely promoted by software vendors over the 
past few years. More than a few dozen products are available on the market in-
cluding COS A Workflow, FlowMark, Staffware, Ensemble, Deskflow, InCon-
cert, OPEN/workflow, Visual Workflo, FlowMake, FlowWork, Lotus Notes, 
Microsoft Exchange ServerlMicrosoft Outlook Client, TeamWARE Flow, Ul-
timus, to name a few. These technologies and products support a range of 
business applications including banking and financing (loan process), legal, in-
surance (claim process), health care (clinical process), military, manufacturing, 
telecommunication, and office automation. This section reviews several classes 
of workflow technology and summarizes the current generation of workflow 
products. Most of the workflow products are featured to support ad-hoc and 
administrative workflows that serve as office automation type of applications. 
Some of the products are briefly described below. 
• COS A Workflow (Software Ley) is a product family supporting workflow, 
business process modeling and archiving. Its philosophy is based on an open 
concept which allows flexible definition and change of business processes 
and organizational structures using graphical editing and analysis tools. A 
distributed engine assigns users to nodes and handles case migration. It also 
features an HTML worklist handler. 
• Open/workflow (Eastman Software) provides graphical development and 
work management facilities to generate applications which can link with 
PC and host-based applications. 
• Ensemble (FileNet) is the simplest example of an ad-hoc workflow. Ensem-
ble offers distribution and home work capabilities by using the built-in mail 
system as a server. 
• FlowMark (mM) supports an organization model. With an object-oriented 
design, it offers a capability of re-usability. Distribution is achieved through 
node assignment to sub-procedures. It concentrates on procedure manage-
ment and organization modeling while leaving activity implementation to 
the programmer. 
• InConcert (lnConcert) integrates object-oriented technology, document man-
agement, and a dynamic process model that enables organizations to build 
and tailor workflows that match evolving organizational needs. The process 

Secure Workflow Transaction Processing 
65 
model enables users to dynamically modify process instances on-the-fly as 
exceptions and changes to the process arise. 
• OPEN/workflow 3.0 (Eastman Software) is a highly scalable workflow 
solution for imaging. It offers a forms-based development environment 
with integration features for transaction processing, imaging, scanners and 
OCRlICR. 
• Staffware (Staffware Corp.) uses the same form definition for many different 
operating systems. It supports distributed configurations. Activity imple-
mentation uses form definition and a scripting language. It offers a good 
compromise between production and administrative workflow requirements 
while delivering production workflow throughput. 
• Team WARE Flow (Team WARE Group) offers users tools for modeling and 
monitoring processes, for participating in processes via work lists, electronic 
mails, electronic forms, document management, user identification and form 
management. It provides dynamic capabilities with procedures that can be 
implemented on the spot and can start running while still incomplete. The 
plan for a procedure instance can be modified while the process is in progress. 
• Visual Workflo (FileNet) is a production workflow with an API to build 
tailored worklist handlers and activities. The procedure definition process 
is object-oriented. It provides classes hierarchies with inheritance, facili-
tating the definition of procedure variations with simpler diagrams. Visual 
Workflo's strength is the object-oriented procedure definition. 
• W 4 is a workgroup tool combining a workflow engine with Internet technol-
ogy for supporting production and ad-hoc workflows. It features access to 
workflow services through Internet browsers. Applications are developed 
using a graphic process modeling tool and HTML page editors. 
1.2 
WORKFLOW PRODUCT CATEGORIES 
The above products can belong to one or more of the following major cate-
gories [wfm94]. 
• Document Based: This is basically a document-centric form of workflow. It 
supports cooperation by enabling interaction through a shared document or 
collection of documents. Document management technology is concerned 
with managing the routing of electronic documents through individuals par-
ticipating in performing the business procedure. It also helps teams to 
collaborate by providing access and version control, document search, and 
status tracking. For example, Deskflow and FlowMark are document based 
workflow products. 

66 
MULTILEVEL SECURE TRANSACTION PROCESSING 
• E-mail Based: This employs the power of electronic mail and directory 
mechanism to distribute information between participants. Ensemble from 
FileNet and Microsoft Exchange are typical examples of this type. 
• Groupware Based: This category of products support and improve the inter-
action of group individuals via group discussion processes, brainstorming, 
accessing bulletin board, or scheduling activities on an ad-hoc basis. For ex-
ample, Deskflow, Lotus Notes and TeamWARE Flow Microsoft Exchange 
Server/ Microsoft Outlook Client are products in this category. 
• Transaction Based: This is to incorporate transaction management facilities 
and database management software to support certain classes of business 
transactions. FlowMake and FlowWork are typical examples of transaction 
based workflow products. 
• Internet Based: This category of products are designed for use on the In-
ternet. From the procedure definition tool, HTML forms are automatically 
generated and can be redrawn graphically using any HTML editor. One 
advantage for using the WWW for workflow systems is the feature of fill-in 
forms in HTML, which allows a form based interaction between the user and 
a task. Such systems aim to serve for dynamic Internet-based business op-
erations over distributed heterogeneous computing environments. Ultimus, 
COSA and W 4 fall into this category. 
1.3 
WORKFLOW STANDARDS 
Enabling technologies and standards are two important factors to the progres-
sion of workflow technologies. In recent years, significant efforts have been 
made in developing workflow standards such as WtMC as well as enabling 
technologies such as CORBA and DCOM. 
Founded in 1993, the Workflow management Coalition (WtMC) has intended 
to enable interoperability between heterogeneous workflow products and de-
veloped a primary standard for future workflow products. The standardization 
work ofWtMC is documented in the workflow reference model [wfm94]. 
According to the workflow reference model, an entire workflow management 
system is centered around a workflow engine(s) which is responsible for enact-
ing task execution, monitoring workflow state, and evaluating conditions related 
to intertask dependencies. A WFMS consists of several functional components 
including process definition tools, workflow enactment services, administration 
and monitoring tools, and interfaces to interoperate with client applications. 
Process specification (or process definition) cooperates closely with process 
definition tools to contain all necessary information about the workflow. This 
includes the tasks that are carried out, the component operations and primitives 
within the task, its starting and completion conditions, rules and dependencies 

Secure Workflow Transaction Processing 
67 
for navigating between tasks. A task defines some work to be done and can be 
specified in a number of ways, including a textual description in a file or an e-
mail message, a transaction, or a computer program. A dependency represents 
the order and constraints for invoking the tasks [GHS95]. The workflow spec-
ification is given during the design time through a separate process definition 
tool. The workflow process is based on a formalized workflow model that is 
used to capture data and dependencies between workflow tasks. 
The workflow enactment service comprises of workflow engine that provides 
the execution of the workflow process and enforcing intertask dependencies. 
Execution of a workflow involves monitoring the workflow state, evaluating 
execution condition, scheduling the execution order, and submitting the tasks 
for execution. In general, execution of a workflow includes: (1) enforcing 
all intertask dependencies, (2) assuring correct execution of interleaved work-
flow transactions, and (3) ensuring that the workflow terminates in one of the 
predefined acceptable states. 
Administration tools provide functions such as managing users, roles and 
security policy. Monitoring tools are used to tracking and reporting workflow 
state and data generated during workflow execution. All these components 
have application interface (API) that provide standard means of communication 
between components and workflow engine. 
2 
WORKFLOW TRANSACTION MODEL 
In order to ensure correctness and reliability, workflows are associated with 
a workflow transaction model [GHS95]. It is important to note that workflow 
transaction models must somehow be based on more "flexible" correctness 
criteria than traditional transaction models. For example, the classical "all-or-
nothing" property of the traditional transaction models is not appropriate for 
workflow transactions. A workflow transaction may need to commit some of its 
actions, while aborting other actions. To accommodate such flexibility require-
ments, a large number of workflow transaction models have been proposed. 
Although here we describe transactional workflows, in general workflows in-
clude users, activities, programs, and data. It is the ability to integrate the 
above four that sets general WFMSs apart from transactional workflow mod-
els [AAA +95]. (Refer to [AH96] for a more general definition of workflow 
model.) 
Simply, a workflow is a set of tasks with task dependencies defined among 
them. A task in its simplest form consists of a set of data operations and task 
primitives {begin, abort, commit}. Execution of a task, in addition to invoking 
operations on data items (e.g., read, write), requires invocation of these task 
primitives. All data operations in a task must be executed only after the begin 
primitive is issued. All tasks must end with either acommit or abort. A primitive 
may move a task from one state to another. 

68 
MULTILEVEL SECURE TRANSACTION PROCESSING 
I Dependency type 
begin-on-commit 
begin 
abort 
termination 
strong commit 
force begin-on-abort 
( commit/begin) 
exclusion 
weak begin-on-commit 
I Description 
A task tj cannot begin until ti commits 
be 
(represented as ti ---> tj). 
A task tj cannot begin until ti has begun 
b 
(represented as ti ---> tj). 
A task tj must abort if ti aborts 
(represented as ti ~ 
tj). 
A task tj can terminate (either commit or abort) 
only after the completion (commit or abort) of ti 
t 
(represented as ti ---> tj). 
If a task ti commits then tj must commit 
(represented as ti ~ 
tj). 
A task tj must begin if ti aborts (commitslbegins) 
fba 
fbe 
fbb 
(represented as ti ---> tj (ti ---> tj / ti ---> tj ». 
Given any two tasks ti and tj. if ti commits 
tj must abort. or vice versa 
(represented as ti -=-. tj). 
Given any two tasks ti and tj. tj can begin if ti commits 
wbe 
(represented as ti ---> tj). 
Table 5.1. 
A list of possible control flow dependencies 
To control the coordination among different tasks, dependencies are spec-
ified based on these task primitives. Task dependencies in turn can either be 
static or dynamic in nature. In the static case, the workflow is defined well in 
advance to its actual execution, whereas dynamic dependencies develop as the 
workflow progresses through its execution [SRK93]. Task dependencies may 
exist among tasks within a workflow (intra-workflow) or between two different 
workflows (inter-workflow). In [RS94], three basic types of task dependencies 
have been identified: control-flow dependencies, value dependencies and ex-
ternal dependencies. Control-flow dependencies may in turn involve explicit 
transmission of data as part of the result of a task. Such dependencies are known 
as control-flow dependencies with data flow. 
In particular, a control-flow dependency specifies the conditions, specified as 
the state sti of task ti, under which a task tj can enter state stj, where the state 
of a task can be either initial, commit or abort. Control-flow dependencies can 
be modeled based on the ACTA framework [Chr91]. Table 1 reports a list of 
possible control-flow dependencies, for two given tasks ti and tj. Other control 
flow dependencies have been devised and we refer the reader to [Elm92, Chr91] 
for a comprehensive list. Control-flow dependencies with data-flow extend 
control flow dependencies by imposing in addition transmission of data values 

Secure Workflow Transaction Processing 
69 
between tasks. Such dependencies are useful when a task needs to wait for data 
from another task. 
Value dependencies are based on more general conditions than those used in 
control flow dependencies. More specifically, it states that a task tj can enter 
state stj only after task ti's outcome satisfies a condition. The condition in the 
above statement can be a logical expression whose value is either 0 or 1. For 
example, tj can begin if ti is a success (semantically).1 
Unlike the prior types, external dependencies are caused by some parameters 
external to the system, such as time. An external dependency states that a task 
ti can enter state sti only after if a certain condition is satisfied where the 
parameters in that condition are external to the workflow. Examples include a 
task ti can start its execution only at 9:00am or task tj can start execution only 
24hrs after the completion of task tk' 
3 
MULTILEVEL SECURE WORKFLOW MODEL 
Tasks in MLS workflows may belong to different security levels. Task de-
pendencies among tasks of the same security level are referred to as intra-level 
dependencies whereas those among tasks of different security levels are as 
inter-level dependencies. Intra-level dependencies involve only single level 
dependencies, thus the dependencies can be enforced without the concerns of 
multilevel security. Therefore we need to concentrate only on inter-level depen-
dencies. Inter-level dependencies can be further divided into the following two 
categories as their treatment has to be different in a MLS environment because 
of the "no downward information flow" requirement. 
1. An inter-level dependency is high-to-low if the security level of the parent 
task is lower than that of the child task. 
2. An inter-level dependency is low-to-high if the security level of the parent 
task is higher than that of the child. 
In the following, we provide an example of a multilevel secure workflow and 
show how a high-to-low dependency causes information leakage. 
EXAMPLE 5.1 Consider a workflow transaction that computes the monthly 
salary for all employees at the end of each month. This process involves the 
following tasks. Task t1: compute the monthly number of work-hours of an 
employee (h) which is the sum of regular work-hours (n) and overtime work-
hours (0) by the employee during that month, Task t2: calculate the monthly 
payment of an employee (P) by mUltiplying h by the hourly rate of the employee 
1 Failure of a task does not necessarily mean abort of a task. A task may still semantically fail even if it 
successfully commits. 

70 
MULTILEVEL SECURE TRANSACTION PROCESSING 
(r), and Task t3: after computing the payment for the month, reset h, nand 0 
to zero. The information about an hourly rate (r) and a monthly pay (P) are 
considered sensitive, and therefore are classified high, while the rest of the 
information is classified low. According to the two BLP restrictions, since tl 
and t3 write objects at low (h, nand 0) they must be low tasks, and since t2 
reads a high object (r) and writes a high object (P), it must be a high task. 
Moreover, the following task dependencies exist among tl, t2 and t3: task t2 
can begin only after tl commits (tl ~ 
t2), and t3 can begin only after t2 
commits (t2 ~ 
t3), as shown in figure 5.1. Both tl ~ 
t2 and t2 ~ 
t3 
are inter-level dependencies where the former is a low-to-high and the latter 
high-to-low. 
0 
High Task: Compute p based on r 
begin/ t2 
on 
commit 
tl 
Low Task: Compute h,o,n 
on 
commit 
\ 
begin 
t 3 
Low Task: Reset h,o,n to zero 
Figure 5.1. 
Inter-level dependencies of the multilevel workflow transaction in example 1 
Enforcing a low-to-high dependency (e.g., tl ~ 
t2) does not violate the 
multilevel security constraints. However, enforcing a high-to-low dependency 
(e.g., t2 ~ 
t3) might induce a covert channel. For example, assume there are 
two malicious processes, one at higher security level and the other at lower, agree 
on a code to leak sensitive information. If the payroll computation time takes 
much longer than usual, it implies that person x's salary is greater than certain 
amount. This is because, a lower subject can measure the delay experienced by 
t3 in starting its execution (since t3 can begin only after t2 commits). 
In considering a multilevel secure workflow, both direct and indirect leak-
age of information must be prevented. The BLP model can ensure no direct 
information leakage, but indirect information leakage can only be prevented by 
a careful construction of the workflow. 
4 
SOLUTIONS TO EXECUTE MLS WORKFLOWS 
Recently, Atluri et al. [AHB99] have proposed a semantics based approach 
to execute MLS workflows correctly and securely. This approach is based 
on a semantic classification of the task dependencies. They classify the high-

Secure Workflow Transaction Processing 
71 
to-low dependencies in several ways: conflicting versus conflict-free, result-
independent versus result-dependent, strong versus weak:, and abortive versus 
non-abortive, and propose algorithms to automatically redesign the workflow 
and demonstrate that only a small subset among all the types of high-to-low 
dependencies requires to be executed by trusted subjects and all other types can 
be executed without compromising security. Details of the approach are in the 
following sections. 
4.1 
SEMANTIC CLASSIFICATION OF TASK DEPENDENCIES IN 
MLS WORKFLOWS 
In this section, we provide an understanding of all types of dependencies 
by providing what they semantically mean in an MLS environment. Atluri et 
al. [AHB99] argue that only certain types of high-to-low dependencies can be 
specified and other types do not exist in a correct and secure MLS workflow 
specification. 
To reason about the semantics of high-to-low dependencies (control-flow), 
one may categorize them as follows based on the source of these dependencies. 
1. This first category of dependencies arises to force the order of (conflicting) 
operations on shared data objects. 
2. The second category of dependencies arises to force properties such as atom-
icity, mutual exclusion, etc. 
Consider the task dependency t2 ~ 
t3 in example 5.1. The intention of this 
dependency is to avoid overwriting of n and 0 by ta before t2 reads them. Thus, 
the source of this dependency is to force a specific order on the conflicting 
operations, and therefore belongs to the first category. Dependencies in the 
second category are specified according to the semantics of the workflow. For 
example, the semantics require that either one of two tasks must commit but not 
both (mutual exclusion). For instance, consider a travel reservation workflow, 
where two tasks are purchasing a ticket in Delta and in United, where only 
one task must commit but not both. In the following, we provide an example 
illustrating such dependencies in an MLS environment. 
EXAMPLE 5.2 Consider a travel agency that processes requests for airline 
and hotel reservations [RSK91, RS94, SMTA95]. Assume that once the flight 
reservation is made, it cannot be canceled, whereas cancellation of hotel reser-
vation is allowed. Processing such requests typically involves the following 
three tasks. 
• tl - purchase an airline ticket, 
• t2 - book a hotel, and 

72 
MULTILEVEL SECURE TRANSACTION PROCESSING 
• t3 - cancel a hotel reservation. 
Based on booking regulations, traveler's preferences, or economic reasons, 
constraints are defined among tasks in terms of the following dependencies. 
• booking of a hotel cannot start until purchasing an airline ticket starts (t 1 ~ 
t2). 
• If hotel booking aborts, then purchasing airline ticket must abort too (t2 ~ 
tl). 
• If purchasing of airline ticket aborts but hotel booking commits, then hotel 
booking has to be canceled (tl ~ 
t3) /\ (t2 ~ 
t3) 
• Purchasing a ticket must commit before that of hotel booking if both commit 
(tl ~ 
t2) 
Suppose a person P has to make a trip from Washington D.C. to Toronto. The 
lodging place of the trip is considered as highly sensitive information and thus 
t2 and t3 is assumed high level. However, the selection of an airline company 
is not classified and thus tl is considered low. Again, the dependencies among 
b 
a 
ba (be 
e 
the tasks are tl --t t2, t2 --t tb tl --t t2 --t t3) and tl --t t2. All 
dependencies except t2 ~ 
t3 are considered as inter-level dependencies where 
t2 ~ 
tl is a high-to-low dependency and all the remaining are low-to-high 
dependencies, respectively, as shown in figure 5.2. 
0 
bc 
high: 
---_;0 t3 
low: 
Figure 5.2. 
Task dependencies in the MLS workflow in example 5.2 
The intention of the high-to-low dependency t2 ~ 
h in the above example 
is to capture the semantics of the workflow rather than forcing an order between 
conflicting operations. Thus this dependency belongs to the second category. If 
we examine once again the two types of the source of dependencies and analyze 
their effect on the workflow, we can make the following observation. Consider 
the following two scenarios: in the first, assume the high-to-low dependency 
ti ~ tj is enforced, whereas in the second, this dependency is not enforced. 
With the second category of dependency (e.g., t2 ~ 
tl in example 5.2) the 

Secure Workflow Transaction Processing 
73 
result of tj might be different if ti ~ tj is not enforced than from the case when 
it is enforced. In other words, the result of tj might be affected if ti ~ tj is not 
enforced. However, the dependency ti ~ tj does not impact the result of ti. 
Thus the second category of dependencies are called result-dependent (RD). 
On the other hand, consider the first category of dependencies (e.g., t2 ~ 
t3 
in example 5.1). If the dependency is not enforced, the result of tj will not 
be affected but that of ti will be affected. This can only occur when the two 
tasks share common data in multilevel secure systems (when this dependency 
is high-to-Iow). Thus, the first category of dependencies are conflicting (CN) 
type. On the other hand, from the perspective of task result, a dependency can 
either be result-independent or result-dependent. 
The intuitive idea behind this classification is that the result of the execution 
of either the child (in case of RD) or the parent (in case of CN) can be different 
when the dependency is enforced than from the case when it is not enforced. 
Thus, there cannot be any dependency which is both conflict-free and result-
independent. In the following, we examine how this categorization in the wake 
of multiple security levels on data items and tasks. 
• Abortive versus non-abortive 
If the dependency specifies that the child task must abort when it is enforced 
(i.e., st of the child is equal to abort), then it is said to be abortive type; 
otherwise, it is non-abortive type. For example, abortive type dependencies 
include abort, exclusion dependency, etc. whereas non-abortive type de-
pendencies are commit, strong commit, begin on commit, serial, begin on 
abort, etc. 
• Conflicting (CN) versus conflict-free (CF): 
Conflicting dependencies indicate that the parent and child tasks access some 
common data items in conflicting mode. The primary reason to enforce this 
type of dependency is to enforce the order of activation of the conflicting 
operations. More specifically, the parent task contains a read operation 
followed by the child with a write operation on the same data object (No other 
combination of read-writes or write-write are possible as per our security 
model due to the * -property of the BLP model.). 
Conflict-free dependencies can only be due to the semantics. Although two 
tasks do not conflict, sometimes for ensuring an acceptable termination state 
of the workflow, such dependencies are specified. 
• Result-dependent (RD) versus result-independent (RI): 
In an RI dependency, the result of the child does not depend on whether 
the dependency is enforced or not. However, no enforcement of this de-
pendency may produce a different resuit for the parent task. Therefore, all 

74 
MULTILEVEL SECURE TRANSACTION PROCESSING 
dependencies falling into this category should be non-abortive such as begin 
on commit, begin on abort, etc. because otherwise the result of the child 
will get affected (i.e, abort) by the parent. 
RD dependencies are specified in such a way that the execution of the child 
depends on the value or outcome of the parent. That is, if the dependency is 
CF type, altering the execution order between these two tasks would affect 
the outcome of the child. Dependencies such as force abort, termination, 
exclusion etc. that may cause the child task to get aborted (the abortive 
type) will fall under RD category. Abortive type dependencies are all RD 
because without enforcing the dependency, the child task may commit. This 
is because all abortive dependencies could possibly cause the child to abort 
and therefore are not of type RI. 
• Strong versus weak 
A dependency is a specification of a logical implication. If the dependency 
specifies the necessary condition to enforce the dependency, then it is said 
to be strong. In other words, ti ~ 
tj can be interpreted as tj enters state 
stj only ijti enters sti. An example of strong type is the begin-on-commit 
dependency (ti ~ 
tj), which states that a task tj cannot begin unless ti 
commits. (Other examples include b, t, gc etc.) In practice, strong type 
dependency is used to specify the precondition(s) for a particular event to 
occur. 
On the other hand, a weak dependency states that if ti enters state sti then tj 
enters state stj. It also means, that tj can enter stj even ti has not entered 
sti. That is, the weak type specifies the sufficient condition to enforce the 
dependency. For example, the strong commit dependency, (represented as 
ti ~ 
tj) which states "if a task ti commits then tj must commit" is weak 
type. (Other examples include a, tba, wba, wbc, c, e, etc.). In a workflow, 
the weak type can be used in situations where a particular workflow state 
has to trigger an event. 
Therefore, it is possible to have combination of CF-RD, CN-RD and CN-RI 
type. However, since a CF type dependency does not cause any effect on the 
result of the parent task, there cannot exist a CF-RI type because its presence 
neither affects the parent nor the child. When enforcing the dependencies the 
following two assumptions hold. 
No-Force-Commit (NFC) Assumption: No task execution can be guaranteed 
to commit. However, a task can be forced to begin or abort. 
No-Prevent-Abort (NPA) Assumption: No task execution can be prevented from 
aborting. However, a task can be prevented from beginning or committing. 

Secure Workflow Transaction Processing 
75 
Further examination of these dependencies reveal that all abortive type work-
flow dependencies must be weak. This is because a strong abortive type depen-
dency states that a task has to abort only if a certain condition is true. However, 
according to the NPA assumption, abort of a task is unconditional. Thus, one 
cannot guarantee that a task must abort only if certain condition arises and must 
not abort if the condition is not met. On the other hand, non-abortive type 
workflow dependencies can be either weak or strong. 
The above categorization of high-to-low dependencies (shown in figure 5.6) 
is important because each category needs to be handled according to a different 
approach in an MLS environment. The specific approach used for each category 
is presented next. 
4.2 
REDESIGNING OF MLS WORKFLOWS 
The approach to executing MLS workflows is to redesign the workflow by 
using two mechanisms - splitting a task and running a compensating task. In 
[AHB99], Atluri et al. prove that the redesigned workflow is in some sense 
equivalent to the original workflow. We elaborate these two approaches below. 
4.2.1 
SPLITTING THE HIGH TASK: 
This approach is used when the high-to-low dependencies are of conflict-
ing type. Given a high-to-low dependency k--+tj, this approach involves the 
following steps: 
1. Reorder all the operations in ti in such a way that all read operations on lower 
level data items occur before all other operations.(Since read operations are 
commutable, the reordering does not affect the correctness of the task.) 
2. Divide ti into partitions, tiOW and ti based on the security levels of data items 
it is accessing. For example, the high task t2 in example 5.1 is split into 
t~OW and t2, where t~OW contains all the low read operations, and t2 all the 
high read/write operations. 
3. Include a begin-on-commit dependency with data flow tiOW bCdat)a ti to en-
sure that data read by the read operations in tiOW in fact are carried over to 
ti even in the wake of other interfering tasks. 
4. Pick a task a closest-low-ancestor tk of ti and include tk ~ tiOW and 
tiOW ~ 
tj, and remove any redundant dependencies. (For example, in the 
workflow shown in figure 5.4, ts and tl are the closest-mid-ancestors of h 
and t3 is the closest-low-ancestor of t7' By contrast, t6 is not a closest-mid-
ancestor of h.) The first edge is to ensure that splitting does not remove any 
dependency from tk to tiow . The second dependency is of the same type of 
the original high-to-low. 

76 
MULTILEVEL SECURE TRANSACTION PROCESSING 
. 
I 5 
~ 
be 
be 
Returnmg to examp e .1, to enlorce tl ~ 
t2 and t2 ~ 
t3, they are 
be 
low 
I 
be 
. 
converted to tl ~ 
t2 
and t2°W ~ 
t3, as shown m figure 5.3. The 
low task t3 proceeds only if t~OW commits, thus preserving the high-to-low 
dependency between t2 and t3. 
high: 
low: tl 
thigh 
2 
Y t ba-,bc 
bc 
low 
t 
----~~~ t2 
~ 
3 
Figure 5.3. 
Modified Workflow after executing split task for example 5.1 
high: 
mid: 
low: 
t 3 
Figure 5.4. 
An example demonstrating the closest ancestor 
4.2.2 
RUNNING A COMPENSATING TASK 
An RD type of high-to-low dependency can be enforced by executing a 
compensating task. We show below that some high-to-low dependencies (strong 
non-abortive and weak abortive type) can be enforced by compensating a low 
task. Note that the low compensating task need to be initiated by a trusted 
subject. Also, this approach is applicable only to tasks for which there exist 
a compensating task. If such a compensating task does not exist, one should 
employ other approaches such as using a buffer at high (assume its size is 
sufficiently large) in which the commit message of the high task is stored. 
This message will first be subject to a delay of some random duration, and 
then will be transmitted to low. If several such messages of a single workflow 
get accumulated during the delay period of the first message, these messages 
cannot be sent at the same time, but must be sent individually with the delay 
incorporated in between each of them. Thus, though there exists a channel 
of downward information flow, the bandwidth of this channel would be low. 
It is important to note that this approach might affect the performance of the 

Secure Workflow Transaction Processing 
77 
I x II be I a I elba I se I 
I X-I II tba I tba I tbc I tbe I tba I 
Table 5.2. 
Some RD type dependencies and their inverse 
system because the signal from high is delayed thereby delaying all the low 
tasks unnecessarily. 
Redesigning using a compensating task approach is based on replacing the 
dependency with its inverse. Derivation of an inverse dependency is based on 
the following assumptions. 
1. Every task that starts its execution must either commit or abort 
2. Abort and commit are complementary operations 
3. A task that has not yet started is in its initial state. 
The following table lists some RD dependencies and their inverse that are 
derived based on these assumptions. 
The redesigning is done as follows. For example, if there exists a high-to-low 
dependency ti ~ 
tj' since the inverse of "be" is "tba," and "bc" is strong and 
non-abortive,theabovedependencyisreplacedwithti ~ 
tjlandtj ~ 
tjl, 
where tt is the compensating task of tj. The above substitution means that 
the compensating task tjl cannot begin until ti aborts and tjcommits. That is, 
both ti and tj can be executed independently, thus tj need not wait for ti thereby 
eliminating potential covert channels. However, if ti aborts, a compensating 
task tjl will be started. Indeed, this compensating task must be executed by a 
trusted subject, e.g., a human user. Note that this is not necessarily a serious 
disadvantage because workflow systems are designed to support and allow user 
interactions. Therefore, requiring the intervention of a user is natural in a 
workflow environment. 
This approach does not compromise security and can enforce equivalent 
compensating dependencies if all tasks are compensatable. Figure 5.5 shows 
the redesigned workflow for example 5.2. 
If there are any dependencies involving tj, (e.g., tj ~ 
tk), compensating 
tj requires tk to be compensated to capture the cascaded compensation. Since 
workflow is in general a long running activity, it is prudent to detect the com-
pensation of a task as early as possible and prevent the unnecessary execution 
of the tasks that follow this task as they will have to be compensated later. In 
other words, if the tasks following tj have not started their execution, they can 
be prevented from being executed, however, if they have already started their 

78 
MULTILEVEL SECURE TRANSACTION PROCESSING 
high: 
low: 
t 2 
bc 
t 
----. 3 
t-
t 
-I 
I--+- tl 
bc 
ba 
Figure 5.5. 
Modified Workflow after executing compensate task for example 5.2 
execution, or finished their execution by the time tt starts its execution, cas-
caded compensation cannot be avoided. In [AHB99], Atluri et al. propose an 
approach to detect such cases that minimize cascaded compensation. 
4.2.3 
SUMMARY OF THE REDESIGNING PROCESS 
In figure 5.6, we summarize how each type of dependency can be redesigned. 
Since CN-RI dependencies are conflicting type, these high-to-low dependencies 
are eliminated by using the split task approach. The approach to handle CF-
RD dependencies compensates the low task, when necessary, by executing an 
inverse transaction. CN-RD dependencies are viewed as a combination of CN-
RI and CF-RD since it could be due to the conflicting operations as well as due to 
the semantics of the workflow. So both splitting and compensating approaches 
are employed in this case. 
split and compensate 
compensate low 
compensate high 
"-
I 
/ 
'\ 
't, 
weak. 
"" 
",lion-abortive __ ' 
~ 
~eak 
strogs'", 
--'iea!l 
a ortIve 
~on-a 
rtj~~ ~<, a ortlve 
ompensate low 
RD 
, 
, 
/~rO!l~ cj;" 
,,' a ort. e 
'" 
, 
, 
RI 
non-abortive 
cP 
V 
split 
CN 
CF 
Figure 5.6. 
Approach for redesigning each type of dependency 
4.3 
ALTERNATIVE SOLUTIONS: MAINTAINING MULTIPLE 
VERSIONS: 
One may use multiple versions of data to cope with such high-to-Iow depen-
dencies. Whenever a task writes a data item d, a new version of d is created, thus 

Secure Workflow Transaction Processing 
79 
the value yet to be read by a high transaction is reserved as an older version of d. 
Costich and Jajodia [CJ92] have proposed an approach in which they associate 
an index with each read/write operation. When a multilevel transaction first 
updates d, it is indexed by 1, the next write to d is indexed by 2, and so on, and 
when it reads d, the read operation is assigned the same index as that of the pre-
vious write operation. Thus, this indexing is used to preserve the dependencies 
by allowing a high task (which they call section of the multilevel transaction) 
to read an appropriate version of d in order to enforce the dependency. This 
approach has the major drawback of requiring a special-purpose multiversion-
ing concurrency control mechanism. The approach we propose here (described 
below) does not have such requirement and therefore can be supported by any 
standard DBMS. A similar approach to preserve the high-to-Iow dependency 
has been proposed by Smith et al. [SBJN96]. This uses a cache to save data to 
be read by a high section so that even if a low section overwrites the data yet to 
be read by the high section, the dependency is still preserved. 
5 
THE MLS WFMS ARCHITECTURE 
The MLS WFMS architecture is shown in figure 5.7. For the sake of sim-
plicity, this has been demonstrated for only two security levels, namely high 
and low. The static workflow redesign module is responsible for examining the 
nature of the workflow dependencies and redesigning the workflow according 
to the approaches discussed in the previous section. Since the redesign process 
is carried out statically, this component need not be trusted. The redesigned 
workflow is forwarded to the task dispenser, which is responsible for generating 
the appropriate part of the workflow based on the security level, and dispensing 
them to the appropriate WFMS. The task dispenser must be trusted to avoid any 
covert channel. A separate WFMS and DBMS is employed at each security 
level, as shown; thus WFMS and DBMS need not be trusted. 
The portion of the workflow sent to the WFMS at each security level com-
prises all tasks at levels dominated by that level and all the dependencies among 
these tasks. Each WFMS is responsible for scheduling the tasks and submitting 
them to the DBMS at its level in such a way that all the dependencies are visible 
to it are enforced. The DBMS simply executes the submitted tasks and sends 
the responses back to the WFMS at its level. The WFMS then forwards these 
responses to the WFMSs at all dominated security levels. 
Based on the outcome of any of the tasks at its level, if a WFMS has to 
compensate a lower level task in order to enforce a dependency (according to the 
redesigned workflow W'), it sends a compensating request to the compensating 
manager. The compensating manager then notifies the corresponding WFMS to 
initiate the compensating task. Since this request is originated by a higher level 
WFMS which triggers a lower level task, the compensating manager must be a 
trusted component in order not to introduce any covert channels. In case more 

80 
MULTILEVEL SECURE TRANSACTION PROCESSING 
.... _----_._------------------. 
· 
. 
· 
. 
· 
. 
· 
' 
DBMS 
~~~::l 
Multilevel 
Worknow 
W 
SUllie 
~~~itfo 
Worl<now 
W' 
Redesign 
Module 
Figure 5.7. 
The MLS WFMS Architecture 
than one WFMS sends requests to compensate a task, the compensating manager 
initiates the compensating task only once. Note that only two components must 
be trusted, which are shown as bold-faced blocks in figure 5.7. 

Chapter 6 
SECURE BUFFER MANAGEMENT 
The focus of our previous chapters was on concurrency control related issues 
arise in MLS DBS environment. In this chapter, we shift our attention to an-
other important transaction processing component, namely, buffer management. 
Since transactions of various security clearance levels often share the buffer 
pool, there arise the possibility of exploiting some of the buffer management 
components for covert signaling. In order to ensure the security of MLS RT-
DBS, buffer managers should be designed to meet the non-interference [GM82] 
criteria. 
This chapter on secure buffer management begins with an introduction to 
database buffer management and then goes on to describing the proposed so-
lutions for buffer management in secure transaction processing environment. 
In MLS RTDBS, where transactions to be processed usually have time con-
straints, the buffer management policies may take into account of transaction 
priorities to improve the system performance. However, while doing so the 
security constraints are to be satisfied. Since these two goals can work against 
each other, the design of buffer managers for MLS RTDBS environment differs 
considerably as we will notice in the section on SABRE, a buffer management 
protocol specifically designed for MLS RTDBS environment. 
1 
INTRODUCTION TO BUFFER MANAGEMENT 
The buffer pool usually comprises of a set of slots, each of which can hold 
a single data page. A buffer page that matches the corresponding page on the 
disk is said to be "clean" whereas a modified page is said to be "dirty." Dirty 
pages may have to be flushed to disk before they can be replaced by other pages. 
There are situations in which dirty pages have to be "force-written" to disk -
for example, at transaction commit time all buffer pages containing log records 
related to the committing transaction have to be forced to the disk. 
V. Atluri et al., Multilevel Secure Transaction Processing
© Kluwer Academic Publishers 2000

82 
MULTILEVEL SECURE TRANSACTION PROCESSING 
Transactions that intend to access the data in a buffer slot are required to 
first pin the slot in the appropriate mode (read or write). The pin ensures the 
physical consistency of the page during the access period. Multiple read pins 
may simultaneously exist on a page but only a single write-pin is permitted at 
a time. When the transaction completes its current usage of the pinned page, it 
issues the corresponding unpin command. It may access this same page again 
later following the pin-unpin procedure as before. 
If a requested page is found to be already in the buffer pool, it is termed 
a "buffer hit," whereas if it has to be brought in from the disk, it is termed a 
"buffer miss." Buffer managers attempt to utilize the locality typically exhibited 
in database reference patterns to maximize the number of buffer hits and thereby 
reduce the disk IIOs. 
Three kinds of reference locality are usually observed: inter-transaction 
locality, intra-transaction locality and restart locality. 
The inter-transaction locality is basically the hotspots of the database such as 
index tables which are more frequently accessed than other pages. If these hot 
spots are to be cached, the number of disk IIOs may be reduced significantly. 
Intra-transaction locality is the transactions internal reference locality. Some 
of the pages accessed by a transaction are likely to be re-referred by it later. 
For example, sorting a number of records may require the same set of records 
to be accessed several times. If these records are cached, disk IIOs may be 
reduced considerably. The restart locality arise from the fact that the restarted 
transactions usually make the same set of accesses as their original incarnation. 
Therefore, if these pages are kept in the buffer pool, while the transaction is 
active there is a chance that these pages could be re-referred. 
There are three components commonly associated with buffer management: 
allocation, replacement and synchronization. The allocation algorithm deter-
mines the assignment of buffers to transactions. The replacement algorithm 
determines which page is to be replaced in order to accommodate a new page if 
no empty buffers are currently available. Lastly, the synchronization algorithm 
ensures that conflicting pins are not held on a page at the same time. A pin 
conflict occurs when a write-pin is attempted on a page that is already pinned 
or when a read-pin is attempted on a page that is already pinned for write. 
2 
REQUIREMENTS 
Buffer managers open up possibilities for covert channels, many more than 
those associated with concurrency control since different mechanisms could 
be exploited for covert signaling. Therefore, it is imperative that all possible 
mechanisms that could impact covert channels to be identified and eliminated 
beforehand. 
Imagine the case of a low security level transaction requesting a buffer slot 
while all buffer slots are being held by high security level transactions. In 

Secure Buffer Management 
83 
such a situation, the low security level transaction may have to wait until a 
buffer slot becomes available. In this situation high security level transactions 
indirectly making a low security level transaction to wait is a violation of non-
interference requirement and the presence or absence of delay experienced by 
low security level transaction for obtaining a buffer slot can be used as an 
encoding mechanism by conspiring transactions. 
A page that is in the cache is accessed sooner than the one from the disk. 
Assume that the buffer manager brings a page to the buffer for a high security 
level transaction. Subsequently, if a low security level transaction requests the 
same page before it getting replaced from the buffer, it would give rise to a 
scenario where the page has to be brought afresh from the disk. That means 
low security level transaction faces additional delay because of the high security 
level transaction activity. Similarly imagine the situation when a buffer page 
is brought into the buffer by a low security transaction. If the buffer manager 
ever replaces this page for a high security level transaction, then a subsequent 
request for the same page by low security level transaction would incur a delay. 
In summary, the presence or absence of a page in the buffer set would act as a 
signaling mechanism unless the buffer manager is carefully designed to avoid 
this. 
Another signaling mechanism would be the allocation of a particular physical 
buffer slot to a transaction. If the slot low level transaction receives when it 
requests for a data page is dependent on the high security level transaction 
activities, this could be exploited as a signaling mechanism. 
A good buffer management algorithm should be immune to the above de-
scribed channels. In addition, there are several problems that arise while inte-
grating security into the buffer manager. For example, buffer managers need 
to support the notion of a "pin" which ensures the physical consistency of a 
buffer page while it is being processed by a transaction, as also the notion of 
a "force write" wherein the contents of a buffer page are forced to disk. For 
a system to be covert channel free, it is essential to ensure the security of all 
buffer management components - buffer allocation, buffer replacement and pin 
synchronization [GR93]. 
3 
SOLUTIONS 
A simple solution to all of the above problems would be to divide the buffer 
pool and assign them statically for each level. However, such static allocation 
policy will give rise to inefficient use of resources. For example, imagine the 
situation where the slots are divided equally between high security level and 
low security level. Under such a scenario, if work loads of low and high security 
levels are not equal, such a division will give rise to under utilization of buffer 
slots. Therefore it is often desirable to have the same buffer pool shared by 
many security levels. However, when the buffer pool is shared by transactions 

84 
MULTILEVEL SECURE TRANSACTION PROCESSING 
of many security levels, the buffer manager should be specifically designed to 
meet the mandatory security requirements. 
3.1 
*-DBS BUFFERING SYSTEM 
Warner et al. [WLKP96] deals with buffer management for database man-
agement systems that enforce a military security policy [DoD85]. This work 
is part ofBS Buffering System* Secure Transactional Resources - Database 
System (*-DBS) project, where the objective is to develop an experimental pro-
totype multilevel secure database management system to study the performance 
of secure transaction processing, buffer management and logging techniques. 
Some components of the *-DBS are trusted such as log manager and transaction 
manager whereas the resource manager need not be trusted. *-DBS runs on a 
Distributed Trusted Operating System (DTOS) [Min95] developed at Secure 
Computing Corporation. 
In * - DB S model, a transaction executing at a site begins by contacting 
the Transaction Manager. The transaction then proceeds to Resource Manager 
for resources. Whenever a data page is needed, the Resource Manager makes 
pin/unpin requests to Buffer Manager. Warner et al. have designed and devel-
oped various secure buffer management policies. They have also conducted 
several simulation studies to profile the performance of the proposed policies. 
As per our knowledge, this is the first ever detailed study in the secure database 
buffer management. 
Rest of this section describes the suggested buffer management policies in 
brief. For details, the reader is referred to [WLKP96]. Typically, a buffer 
management policy has there components, namely, allocation, replacement and 
synchronization. 
3.1.1 
BUFFER ALLOCATION 
Since the buffer pool is partitioned by security level each security level has to 
be allocated buffer slots. Static allocation policies can result in resource wastage 
under a variable workload. Therefore, Warner et al. suggest two dynamic al-
location schemes namely arbitrary allocation and slot stealing. In arbitrary 
allocation, buffer slots move between buffer managers without regard for the 
security order in the partial order. Due to obvious reasons, this can be exploited 
for covert signaling. To regulate covert channel, they suggest that buffer slot 
redistribution should be performed only at fixed intervals. Further, for control-
ling the covert channel capacity, restrictions are placed on the changes in the 
allocation. 
Arbitrary allocation will adjust for slow changes in workloads at individual 
security levels. However, in order to cope with fast changes in the workload 
a scheme called slot stealing is also suggested. The scheme allows the buffer 
slots that are not being used at the dominated levels in the system to be offered 

Secure Buffer Management 
85 
to the transactions at other levels. When levels have more than one covering 
levels, the decision on which level to offer a slot must be taken. There are 
two approaches suggested in [WLKP96]. In the first approach, every slot is 
pre-assigned a dominating path of the security poset. Slot can only move up 
and down along this path. The second approach is to use a predetermined 
probability to decide which covering level obtains the slot. 
Note that if slot stealing is used, if delay incurs in returning a borrowed slot, 
it could be exploited as a signaling mechanism. Therefore, for ensuring non-
interference, it is required that the number of buffer slots that are clean and not 
performing 110 must be as large as the number of slots borrowed. This ensures 
that the offered slots can be immediately returned on request. Another method 
suggested for solving this problem is to delay all requests by a diskwrite time. 
This solution however will give rise to resource wastage. Yet another problem 
that needs to be solved is the problem of cleaning pages that were in use by 
dominating levels before slot reclamation occurs. If the pages are not cleaned, a 
low security level transaction may be able to obtain the contents of the page. A 
straightforward, but inefficient, solution is to zero out the pages before returning 
it and delaying all requests by the page-zeroing time [WLKP96]. 
3.1.2 
REPLACEMENT 
The replacement policy is described by two components: a page set and 
a ranking function. Based on the history of references, the Pageset function 
identifies the pages to be retained in the buffer. A page that is not output by the 
pageset function is freed. If a request for a buffer slot arrives and there are no 
free slots available, the ranking function decides the order in which the pages 
are to be replaced from the buffer. If a page is needed for replacement at a level 
and there are no free slots, the unpinned, resident page with highest ranking is 
replaced. 
When a page is replaced at a security level, it is offered to the next highest 
level. If none of the level buffer managers need the page, it is no longer stored 
in the buffer. If the security levels are not comparable, a page may have to be 
placed in more than one buffer managers. 
3.1.3 
SYNCHRONIZATION 
Warner et al. also explore several ways to synchronize readers and writers 
of different security levels. The purpose is to ensure that a writer is not be 
delayed due to high level read pins. Note that this is a necessary condition for 
non-interference. There are several approaches suggested to achieve synchro-
nization. In the no replication approach, each dominance chain contains at most 
one instance of the page. When a page is pinned for write by a transaction, no 
further pins are allowed on that page. However for security reasons, if write pin 
request from a low security level transaction is received while a slot is being 

86 
MULTILEVEL SECURE TRANSACTION PROCESSING 
pinned for read by a high security level transaction, the write pin is allowed. 
However, for ensuring consistency, when the high security level transaction 
unpins the page, the buffer manager informs that the page has been modified. 
The transaction, at its choice, can redo the work associated with the page or can 
restart. 
The replication approach for synchronization simplifies the synchronization 
of readers and writers. In this case, conflicting read and write pins of different 
security levels accesses their own private replica of the page. Two types of 
replication are suggested based on when the replication is performed: immediate 
replication or delayed replication. For immediate replication, the replication 
occurs every time a read-pin is requested on a page in a strictly dominated slot. 
However, in replication approach, additional consistency checks are required 
since the same page may be cached at various levels. Four approaches are sug-
gested for maintaining consistency in an immediate replication system. With 
immediate invalidation, the replicated page is used only for one read-pin and 
then invalidated. In the second method named propagation, the modifications 
are propagated to strictly dominating buffer slots. For every modification of a 
page, the strictly dominating buffer slots are searched and the corresponding 
pages are invalidated or updated with the changes. A third method of maintain-
ing consistency is by publishing modifications in well known places. Finally, in 
the validation server based approach, a trusted server maintains the pages held 
by each buffer manager and the outstanding write-pins. The buffer managers 
can inform the validation server when a change in the resident set occurs and 
when write-pins are granted and released. 
The delayed replication is performed only when a read/write conflict is dis-
covered. If a write-pin is requested on a page that is currently pinned by a 
higher level transaction for read, the page is copied to the reader's level. Once 
the reader finishes the read operation, the copy is invalidated. Further, all read 
pins are delayed until no write-pin exists on the slot. 
3.1.4 
DESIGN ISSUES 
The design and implementation of a MLS buffer manager following the above 
policy has been described in [Bha96]. One of the interesting design issues is to 
decide the security clearance level of the buffer manager. If the buffer manager 
is placed at the bottom of all security levels, then a read access to any of the 
clients' address space would be impossible. On the other hand, if the buffer 
manager executes at the top most security level, then the buffer manager will 
not be able to write to any of its client's address space. If the level of the buffer 
manager is fixed to be one of the intermediate levels, a combination of the two 
problems will arise. Therefore, an untrusted implementation of buffer manager 
seems to be complex [Bha96]. 

Secure Buffer Management 
87 
Warner et al. suggest that an implementation with minimum possible trust 
would be a good substitute. For example, the untrusted entity can provide 
pin/unpin interface and can control page replacement policies, free list man-
agement, etc. Functions such as slot offering, stealing and reclaiming should 
be trusted. All page requests should be controlled by the trusted component to 
ensure the compliance with Bell-LaPadula conditions for security. The trusted 
and untrusted components communicate with each other using message passing. 
Note that all messages from the untrusted components should be authenticated 
by the trusted component. 
The isolation between the trusted component and untrusted component is en-
sured by having them made to execute in different address spaces. However, this 
incurs considerable message passing overhead due to having to perform kernel 
call and several context switches. Their implementation reduces this overhead 
as much as possible by providing the recipient with a pointer to the message 
rather than really passing the whole message. Also, their design principle is 
to eliminate the communication between the trusted and untrusted partitions as 
much as possible. 
3.1.5 
IMPLEMENTATION 
The buffer management system comprises of several level buffer managers, 
one at each active level of the system. These provide traditional buffer manager 
interface to the other resource manager entities, and comprise the untrusted 
component of the system. Level buffer manager maintains a buffer pool at its 
level. The slots in the buffer pool are, however, granted by the trusted buffer 
manager. Level buffer managers can perform disk requests only through the 
trusted buffer manager. Several implementation related issues are discussed 
in [Bha96]. 
3.2 
PERFORMANCE ANALYSIS 
A performance study to investigate the performance of no replication, de-
layed replication and immediate replication to synchronize the readers and 
writers has been presented by Warner et al. [WLKP96]. The simulator used for 
the study has two components, a reference string generator and buffer manager 
simulator. The workingset algorithm [Den68] is used as the pageset algorithm 
and LRU [GR93] is used as the ranking algorithm. 
Their first experiment is to investigate the performance improvement pro-
vided by the slot stealing buffer allocation method. The experiment was con-
ducted with two security levels and a static allocation, slot stealing with 90% 
(Low) - 10% (High) division of buffer slots, slot stealing with 50%-50% allo-
cation of buffer slots, and single level LRU. 
The simulation results show that the dynamic allocation scheme outperforms 
static allocation for dynamic workloads. For static workloads, static allocation 

88 
MULTILEVEL SECURE TRANSACTION PROCESSING 
may be attractive due to low overhead and simplicity. When the buffer managers 
have enough slots to maintain their pageset, delayed and no replication outper-
forms immediate replication. Immediate replication can outperform the others 
when a buffer manager has an excess of buffer slots while a strictly dominated 
buffer manager has too few. For complete results refer to [WLKP96]. 
4 
SECURE REAL-TIME DATABASE BUFFER 
MANAGEMENT 
Unlike the traditional buffer management where the minimization of trans-
action response time is the only design objective, in real-time database buffer 
management the aim is to provide buffer services so as to maximize the number 
of transactions meeting their deadlines. In other words, the buffer manage-
ment components of such system should be priority cognizant. Consider, for 
example, the case where a tight-deadline transaction wishes to utilize the slot 
currently pinned by a slack-deadline transaction. If the urgent transaction is 
blocked, this would mean that high priority transactions are being blocked by 
low priority transactions, that is, resulting in priority inversion [SRL87]. Since 
priority inversion can cause the affected high-priority transactions to miss their 
deadlines, buffer management policies for RTDBS should be free from priority 
inversion. Therefore, the secure buffer management policies described in the 
last section may not be suitable in the MLS RTDBS environment. 
There has been a few buffer management policies developed for the pri-
oritized transaction processing [CJL89]. However, these policies are not di-
rectly applicable in real-time domain where the priority is usually a dynamic 
assignment. To address this lacuna, real-time versions of the allocation and 
replacement polices of [CJL89] were developed and evaluated in [HS90] and 
in [KS91]. Since they arrived at opposing conclusions regarding the utility of 
adding real-time information to buffer management, the results of these studies 
were inconclusive. The above RTDBS buffer management policies may not 
meet the non-interference criteria for the MLS RTDBS environment. 
4.1 
SECURE ALGORITHM FOR BUFFERING IN REAL-TIME 
ENVIRONMENT 
The SABRE (Secure Algorithm for Buffering in Real-time Environments) 
policy [GH97] provides mandatory security and simultaneously try to improve 
the real-time performance. SABRE operates on a buffer pool where the buffer 
slots are partitioned into four categories. A buffer slot containing a valid page 
that is currently being accessed by executing transactions is called pinned. 
Active buffer slot is a buffer slot that is not currently pinned, but contains a 
valid page that has been earlier accessed by an executing transaction. Dormant 
slots contain valid pages that have not been accessed by any currently executing 

Secure Buffer Management 
89 
transactions. An empty slot is the buffer slot that does not contain any valid 
data page. 
4.1.1 
SECURITY FEATURES 
Security features enable SABRE to be covert channel free. We describe 
some of the important features below: 
1. The contents of a buffer slot are not "visible" to a transaction if the existence 
of the page in the slot is a consequence of actions performed by higher 
clearance transactions. Such a slot will appear as a (pseudo)-empty slot to 
the requesting transaction. This feature prevents a low clearance transaction 
from getting immediate access to a page that has been earlier brought into 
the buffer pool by a high clearance transaction. 
2. High clearance transactions can replace the dormant slots of low clearance 
transactions. This feature is incorporated to prevent the high level transac-
tion from starving for buffer slots. If high clearance transactions could not 
replace any pages of low clearance transactions, then the buffer pool would 
very quickly fill up with the active and dormant pages of the low clearance 
transactions and after this the high clearance transactions would not be able 
to proceed any further. That is, starvation of high clearance transactions 
would occur. 
3. Dormant pages, of any level, are visible only to transactions of the highest 
clearance level; they are visible for no other transactions. This feature 
prevents a low level transaction from receiving information from a high 
level transaction by monitoring dormant slot activity. 
4. Priority-based pin preemption is supported at all clearance levels. This 
ensures that high clearance accesses can be unrestricted without causing 
covert channels since borrowed slots can be returned immediately even if 
they are currently pinned. 
5. When selecting from among the set of (really) empty slots, the slot is ran-
domly chosen and not in a pre-defined order. Thus low clearance transactions 
cannot "guess" that they are being given a pseudo-empty slot by virtue of 
the fact that the slot they receive is not the first in the list of slots that they 
perceive to be empty. 
4.1.2 
REAL-TIME FEATURES 
SABRE incorporates the following features to enhance its real-time perfor-
mance: 
1. Transactions of a particular clearance level cannot replace the pinned or 
active pages of higher priority transactions belonging to the same level. This 

90 
MULTILEVEL SECURE TRANSACTION PROCESSING 
feature helps to utilize the intra-transaction and inter-transaction locality of 
high priority transactions. 
2. Within each clearance level, priority-based pin preemption is supported thus 
preventing priority inversion. 
3. An optimized "comb" slot selection algorithm to decide the slot in which to 
host a new data page. 
4.1.3 
SEARCH AND SLOT SELECTION 
When a transaction requests a data page, if page is found in the visible 
portion of the buffer pool, SABRE returns the address of the slot in which the 
page is present. If the page is already pinned in a conflicting mode by a higher 
priority transaction, then the transaction has to wait for it to be unpinned before 
accessing the contents. Otherwise, it can access the page immediately after 
gaining a pin on the page. 
A search could be unsuccessful for one of two reasons: (a) because the page 
is really not in the buffer pool, or (b) because it is in the non-visible portion 
of the buffer pool. In the first case, if really empty buffer slots are available, 
SABRE brings the page into one of these slots and returns the address of the slot 
to the requesting transaction. Otherwise, an existing buffer page is chosen for 
replacement according to the selection algorithm described below. The victim 
page, after being flushed to disk if dirty, is replaced by the requested page and 
the associated slot address is returned to the transaction. In the second case, 
the requested page is made to appear to have been brought from disk into the 
buffer slot where it currently exists by "unveiling" the slot only after waiting 
for the equivalent disk access time. 
Comb slot selection algorithm, starting from really empty slots, the selection 
works its way up the security hierarchy looking for dormant slots and works its 
way downwards from the top clearance level looking for active and pinned slots 
until it reaches the requester's clearance level. SABRE's comb slot selection 
algorithm incorporates the security and real-time features described earlier. In 
addition, it includes the following optimizations designed to improve the real-
time performance: 
1. The search for replacement buffers is ordered based on slot category - the 
dormant slots are considered first and only if that is unsuccessful, are the 
active and pinned slots considered. This ordering is chosen to maximize the 
utilization of intra-transaction locality. 
2. Given a set of candidate replacement slots (as determined by the slot se-
lection algorithm), these candidate slots are grouped into clean and dirty 
subsets - only if the clean group is empty is the dirty group considered. 
Within each group the classical LRU (Least Recently Used) policy [EH84] 

Secure Buffer Management 
91 
is used to choose the replacement slot. This "LRU (clean I dirty)" ordering 
is based on the observation that (a) it is faster to replace a clean page than a 
dirty page since no disk writes are incurred, and (b) the cost of writing out 
a disk page is amortized over a larger number of updates to the page. 
4.1.4 
FAIRNESS FEATURES 
Due to the covert-channel-free requirements, high clearance transactions in 
secure RTDBS are discriminated against in that they usually form a dispropor-
tionately large fraction of the killed transactions. To partially address this issue, 
the following optimizations are incorporated in SABRE: 
(a) Proxy Disk Service: For security reasons, delays are to be incorporated 
when access is given to data pages that are not in the visibility region of a 
transaction. This delay is the time taken for performing a disk YO. However, 
the disk time intended for servicing this request can be used for serving the 
pending requests, if any, of higher clearance transactions for the same disk. 
That is, the high clearance transaction acts as a "proxy" for the low clearance 
transaction. 
(b) Comb Route: In the comb algorithm, the search for a replacement slot 
among the dormant slots begins with the lowest clearance level. This ap-
proach maximizes the possibility of slot-stealing from lower clearance lev-
els, resulting in increased fairness. 
4.2 
PERFORMANCE STUDY 
A detailed performance study of SABRE is reported in [GH97]. This study 
evaluates the performance of the various buffer managers for a variety of 
security-classified transaction workloads and system configurations. While 
most of their experiments are reported for a two-security-level system, they 
also evaluate the performance behavior in the context of a five-security-Ievel 
system. 
They have compared the performance of SABRE with that of representative 
unsecure buffer management policies - in particular, CONV and RT, policies 
for conventional DBMS and for RTDBS, respectively. To isolate and quantify 
the effects of buffer management on the system performance, they also evaluate 
two artificial baseline policies, ALLHIT and ALLMISS, in the simulations. 
ALLHIT models an ideal system where every page access results in a buffer 
pool "hit," while ALLMISS models the other extreme - a system where every 
page access results in a buffer pool "miss." 
In their first experiment, they evaluate the cost of providing covert-channel-
free security in the real-time environment for a system with two security levels. 
They observed that the real-time performance of SABRE and RT is similar, 
with RT holding the edge, whereas CONV is significantly worse, especially 

92 
MULTILEVEL SECURE TRANSACTION PROCESSING 
under heavy loads. RT shows better performance than CONY because it gives 
preferential treatment to urgent transactions in the following ways: (1) They 
derive more intra-transaction and restart locality since their active pages cannot 
be replaced by low priority transactions; and (2) They get "first pick" of the 
available slots due to the prioritized queuing for buffer slots. 
The reason that SABRE performs slightly worse than RT is that since pri-
orities are decided primarily based on clearance levels, it is still possible for a 
slack-deadline public transaction to restart a tight-deadline secret transaction, 
resulting in the secret transaction missing its deadline. Another observation 
is that secret transactions form a much larger proportion of the killed transac-
tions. In other words, they observed that SABRE is extremely unfair to secret 
transactions, especially under heavy loads. 
The above experiments were reported in a scenario when inter- and intra-
transaction locality are present in the data reference pattern. They also ex-
perimented with the situation when no reference locality is present. At low 
loads there is no noticeable difference in performance between the policies. At 
high loads, when there is buffer contention, RT performs marginally better than 
CONY because of the priority cognizant buffer allocation policy followed in 
RT. SABRE provides performance that falls in between CONY and RT. 
Their experiments showed that when there is no locality present in the work-
load, the SABRE buffer management policy ensures security with a very small 
drop in real-time performance even in the no locality case. However, SABRE 
is increasingly unfair at higher loads due to having to satisfy non-interference 
constraints. 
They also studied the impact of inter transaction locality on the system per-
formance. They observed that the miss percentage values in this experiment 
are higher, for all policies, than their respective miss percentage values in the 
No Locality experiment. The reason for this behavior are the hot spots result-
ing from the global skew in the data accesses. These hot spots increase data 
conflicts and, consequently, increase the number of deadline misses. 
In the experiment to isolate the effect of intra-transaction locality, they found 
that the real-time features of RT and SABRE deliver significant performance 
benefits over CONY. This is due to the fact that the deadline cognizance of 
SABRE and RT is more effective under intra-transaction locality. In this case, 
a transaction repeatedly refers to some of the pages that it accessed earlier. The 
priority-cognizant buffer management policies keep the active/pinned pages of 
the urgent transactions in the buffer and therefore they will be available when 
re-referred. This helps the urgent transactions to complete before their dead-
lines, thereby resulting in performance improvement. At high loads, however, 
SABRE performance becomes worse than RT, and that is because the security-
cognizance of SABRE results in public transactions hogging most of the buffer 
slots and thereby causing the secret hot spots to be absent from the buffer pool. 

Secure Buffer Management 
93 
Another interesting point to note here is that the overall miss percentages are 
considerably reduced for all the policies, compared to their values in the first 
experiment because the intra-locality reduces buffer misses without attracting 
the penalty of data conflicts. 
In the five-level experiment, the notable differences with regard to the overall 
miss percent is that at low loads, SABRE performs slightly worse than both 
CONY and RT. Second, the overall miss percent for SABRE is noticeably 
more than that of RT at high loads although still less than CONY. The reason 
for this behavior is that in a five-level system, priority is much more level-based 
than deadline-based. Therefore, the real-time features of SABRE have much 
less effect resulting in degraded real-time performance. 
The miss percentages are skewed among the various transaction security 
levels, with highest security level transactions having the most number of missed 
deadlines and lowest security level transactions having the least. In other words 
the security requirements make SABRE unfair to the higher security classes, 
especially at high loads. 
A limitation of this work is that it has only considered environments wherein 
the security levels are fully-ordered - in general, however, security hierarchies 
may be partially ordered, typically in the form of lattices. It is therefore im-
perative to extend the SABRE buffer manager to this more general framework. 
5 
CONCLUSIONS 
To conclude, in this chapter we present various buffer management solutions 
for secure transaction processing systems. For a transaction processing system 
to be secure, it is required that all the transaction processing components should 
meet the non-interference criteria. Warner et al. [WLKP96] suggest a set of 
interesting buffer management policies developed as part of Secure Transac-
tional Resources - Database System project. In an MLS RTDBS environment 
where the transactions to be processed has completion time constraints, buffer 
managers should take the transaction priority into account. SABRE is a buffer 
management policy specifically designed for such an environment. There are 
many open issues remain to be addressed in this area. 
The work defined in this chapter considers a buffer pool maintained on a 
single machine. New problems arise if the buffer pool is distributed, i.e., a data 
item can appear in more than one buffer pool. In a distributed environment, 
delayed replication and page stealing will be hard to implement efficiently. 
Other solutions are needed. 
Open problems remain with respect to integrating validation schemes with 
buffer management [Har84]. We feel that more research is to be dedicated to 
this area since validation techniques seems to be holding an edge over locking 
based policies, particularly in secure firm RTDBS environments. 

Chapter 7 
APPLICATIONS TO HIERARCHICAL AND 
REPLICATED DATABASES 
In this chapter, we discuss how the transaction processing solutions presented 
earlier can be applied in both hierarchical and replicated databases. In each case, 
we begin with a brief introduction and a description showing how hierarchical 
and replicated databases are structurally similar to MLS databases. 
1 
HIERARCmCAL DATABASES 
In large databases, often there exists an information hierarchy, where both 
raw and derived data co-exist. In such databases, in order to protect the raw data 
against corruptions, certain access restrictions are imposed on transactions. In 
particular, transactions that attempt to update both the raw and the derived data 
are prohibited, as are transactions that read the derived data and update the raw 
data. 
As such, transactions can be categorized based on their access characteristics. 
An off-line analysis of transactions can then be utilized in partitioning the data, 
thus forming a hierarchical decomposition of the database. A hierarchical 
decomposition method has been proposed in [HM83]. Intuitively, this method 
partitions the database as follows. It attempts to group data that belong to the 
same write set of some type of transactions into one data partition, and allow 
the data in the read set to belong to a different partition. 
To illustrate the access restrictions on transactions, we provide a specific 
example. Consider three classes of transactions, 0 1, O2 and 0 3, and three 
corresponding partitions of the database, PI, P2 and P3. Transactions in class 
01 read and write only data in Pl. Transactions of class C2 read but do not write 
data in PI, but read and write data in P2. Transactions of class 03 read but do 
not write data in PI and P2, but read and write data in P3 . The three partitions 
form an information hierarchy; the data partition PI contains raw data, while 
the data in P2 is derived from the data in PI and the data in P3 is derived from 
V. Atluri et al., Multilevel Secure Transaction Processing
© Kluwer Academic Publishers 2000

96 
MULTILEVEL SECURE TRANSACTION PROCESSING 
both PI and P2. The data partition hierarchy is depicted in figure 7.1. Though 
in this example the three partitions are all related to one another; in general, 
this need not be true. 
P3 
P2 
PI 
Figure 7.1. 
An Example of Class Hierarchy in a Hierarchically Decomposed Database 
Therefore, with respect to a given transaction, one data partition is local, 
some data partitions are raw, some are derived, and the remainder are, in effect, 
invisible. The defining access characteristics are that write to raw or invisible 
data partitions are prohibited and that reads from derived or invisible data par-
titions are prohibited. In general, these restrictions yield a relationship among 
the partitions that is a partial order. 
In summary, all the transactions are mandated to observe the following con-
straints. 
Constraints on transactions: 
1. If there is a transaction Ti that writes into data partitions Pj and Pk, then 
Pj = Pk • In other words, every transaction is allowed to write into only 
one data partition. 
2. If a transaction Ti reads from a data partition Pj, then Pj must be no greater 
than the partition that Ti writes. 
Recalling from chapter 3, similar constraints are imposed on transactions in 
multilevel secure databases. Therefore, a concurrency control mechanism for a 
hierarchically decomposed database, besides preserving serializability, should 
aim at preventing interference of the transactions that access the derived data 
from the transactions that update the raw data. Such interference extends be-
yond the specified read and write prohibitions to delays or aborts caused, for 

Applications to Hierarchical and Replicated Databases 
97 
example, by timestamp or locking protocols. In other words, in both hierarchi-
cally decomposed databases and multilevel secure databases, the interference 
from high transactions to the low transactions must be eliminated, although 
the goal in the latter case is due to security reasons. Thus, hierarchically de-
composed databases and multilevel secure databases are not only structurally 
similar but their access restrictions on transactions are also same. 
In the following, we describe two concurrency control protocols that were 
proposed in the context of hierarchical databases. 
1.1 
HSU-CHAN PROTOCOL 
The partitioned two-phase locking protocol proposed by Hsu and Chan 
provides concurrency control of transactions in hierarchically decomposed 
databases, where the structure of the hierarchically decomposed database is 
a semitree, and requires multiple versions to be maintained in the database. 
This algorithm does not allow transactions updating the derived data to inter-
fere with the transactions updating the raw data. The algorithm achieves this by 
basically serializing all transactions updating the derived data ahead of those 
updating the raw data. The details of this protocol are as follows: It uses the 
multiversion two-phase locking protocol to schedule transactions that access 
data which belongs to their partition. For those transactions that read the raw 
data and update the derived data, the algorithm transitively computes a quiescent 
point by traversing along the data partition hierarchy. 
For example, suppose a transaction Ti of partition Pi requests a read from 
a partition Pj at time tl' Suppose that there exists another partition Pk such 
that Pi > Pk > Pj . The quiescent point qIk of tl at Pk is the highest commit 
time of transactions at Pk that committed before tl' The quiescent point of 
Ti at Pj is computed as follows. The quiescent point qIj of tl at Pj is the 
highest commit time of transactions at Pj that committed before qIk. Thus, the 
quiescent point of Ti at Pj is always earlier than the starting time of any active 
transaction at Pj . After computing the quiescent point of tl at partition Pj , Ti 
is given a version of the data that was created earlier than qIj. Since Ti is given 
a older version of the data, its read operations do not interfere with the write 
operations of transactions that belong to partition Pj . 
1.2 
AMMANN ET AL. PROTOCOL 
The concurrency control algorithm proposed by Hsu-Chan [HC86] relies on 
database quiescence points. Later Ammann et al. [AAJ95] demonstrate that qui-
escence is not necessary to simultaneously maintain serializability and timely 
access to data in dominated partitions. The timestamp generation algorithm 
proposed by them is a decentralized facility and thus is suitable for distributed 
systems. 

98 
MULTILEVEL SECURE TRANSACTION PROCESSING 
Thus, instead of using the quiescent point approach, one can use Ammann 
et al.'s component based timestamp generation algorithm to derive an ordering 
sequence of transactions. 
We begin with a discussion of component based timestamps [AJ93]. Sup-
pose, we have two partitions, Hand L, where H dominates L, and three 
timestamps, the first and third generated at L and the second generated at H. 1 
At L, timestamps consist of a single timestamp component. Suppose the first 
transaction, at L, is given timestamp 1. At H, timestamps consist of a times-
tamp component for H in conjunction with a component for L. Thus the H 
transaction receives component 1 for itself and inherits the "current" times-
tamp component 1 from L to obtain a timestamp, denoted as 1.1. The next L 
transaction receives timestamp 2. 
In the component-based timestamp approach, a timestamp for a given par-
tition Pi includes a component for each partition Pj that Pi dominates. Other 
partitions are ignored. More generally, at each partition Pi, timestamp com-
ponents are generated in a conventional manner, for example as members of a 
monotonically increasing sequence of integers. A complete timestamp is a set 
of the timestamp components from all dominated partitions (including Pi). 
Timestamp comparison must be defined for every pair of timestamps where 
the class of one timestamp is comparable to the other. It is not necessary to be 
able to compare the timestamps from incomparable partitions. 
Comparison of timestamps is defined pairwise on the basis of a single com-
ponent. Only the components associated with the dominated partition are used 
for comparison. If the timestamps have the same value for this component, 
which is possible when the timestamps are from two different partitions, the 
timestamp from the dominant partition is defined to be larger. As an example, 
suppose we compare timestamp 3.2 at H with the timestamp 2 at L. The com-
ponent for comparison is the low component, which is 2 in both cases. Thus 
the low timestamp is defined to be smaller. 
The algorithm for generating component based timestamps is as follows. 
Each partition uses and updates its own local timestamp component. In addition, 
each partition obtains timestamp components from dominated partitions via 
update messages. A partition's notion of time is the union of the local time, 
reflected in the local timestamp component, and the timestamp components 
received from dominated partitions. Delays in propagation of update messages 
cause progressively higher partitions to see progressively older (smaller) time 
values for a given low partition. 
We explain the timestamp generation algorithm with an example. Consider 
the simple planar lattice structure shown in figure 7.2. Class H has two update 
1 We use H and L to denote partitions to indicate that the algorithm can also be used in an MLS environment 
where H and L can be interpreted as high and low security levels. 

Applications to Hierarchical and Replicated Databases 
99 
H 
/~ 
Figure 7.2. 
A Planar Lattice 
queues, one for each of the directly dominated partitions MI and M 2. Partitions 
MI and M2 each have one update queue; that queue is for partition L. Assume 
that the current timestamp values at partitions H, MI, M2, and L are 3.1.5.7, 
1.7,5.7, and 7, respectively and that the update queues are empty. 
Consider once again the example in the above Hsu-chan's protocol where 
Ti of partition Pi wishes to read a data item from partition Pj • Then Ti is 
assigned a timestamp which is earlier than all the active transactions at Pj as 
well as Pk. Again in this case, since the timestamp of Ii is smaller than any 
active transaction at partition Pj , Ti'S read operation will not interfere with the 
write operations of transactions at Pj. Since the timestamp ordering protocol 
rejects reads that are too late from transactions with smaller timestamps, these 
transactions will be aborted and may be subjected to starvation. The problem 
of starvation can be eliminated by maintaining multiple versions of data. 
2 
REPLICATED DATABASES 
Replication has been a popular research topic in the database community for 
the last two decades. (See [AA98] for a review of current trends in replication.) 
The reason behind this interest is that replication is a core technology that holds 
the promise for high performance, reliability and availability in information 
systems. These three measures can, in principle, be drastically improved by 
keeping several copies of data replicated throughout the nodes of a distributed 
system. In fact, one is hard pressed to find a real application where replication 
does not playa role in one way or another. From web sites to telecommunication 
and data networks, replication is a widely used technology. 
Perhaps the most studied aspect of replication is how to keep replica updates 
synchronized. A lot of effort has gone into studying protocols in which a 

100 
MULTILEVEL SECURE TRANSACTION PROCESSING 
transaction updates all the copies of the data before commiting, while at the 
same time the global executions of transactions remain serializable. These 
protocols, however, require holding resources (e.g., locks) in different nodes of 
the system for the duration of the transaction and, as such, are very vulnerable 
to failures and deadlocks. This property renders them unlikely to scale beyond 
a small number of copies [GHOS96]. That fact has been the main reason 
why almost all commercial implementations of replication protocols opt for the 
usage of lazy replication, a technique in which a transaction commits when one 
of the copies (viz., the primary copy) is updated and the rest of the replicas are 
lazily updated through independent transactions. 
Unfortunately, as we will see below, the guarantee of replica consistency 
is severely compromised by lazy replication, since the updates are made after 
the commit point of the original transaction. Nonetheless, lazy replication is 
widely used by a number of database vendors simply because of its performance. 
Even commercial designers of products which provide several models of data 
replication, such as Oracle8 [ORA97], recommend the usage oflazy replication 
to its customers. 
When using lazy replication, a designer has to be careful about the implica-
tions. One of the most important ones is that if transaction management is not 
architected in a careful manner, the result can lead to the lack of global serializ-
ability. For example, if no restrictions are imposed, two transactions Tl and T2 
that update the same data item may end up executing in one of the replicas in the 
order Tl, T2, while executing in another in the order T2, T1. While this may not 
be a problem for many applications, it can have serious implications in some 
others. (Consider for instance, the case in which the item is a bank account and 
Tl is a deposit, while T2 is a withdrawal; without the deposit executing first, 
the withdrawal may fail due to lack of funds.) 
Indeed, define a copy graph to be the directed graph in which the set of 
vertices correspond to the set of nodes in the system. There is an edge from node 
Si to Sj in the copy graph iff Si has the primary copy and Sj has the secondary 
copy for some data item x. Chundi, Rosenkrantz, and Ravi [CRR96] show 
that if replica updates are propagated lazily and indiscriminantly (i.e., without 
any restriction), then the global history is serializable iff the undirected graph 
obtained by removing the directions from all edges in the copy graph is acyclic. 
In view of this tight characterization, several researchers [ABKW98, BK97, 
BKR+99, CRR96, GHOS96] have proposed lazy update protocols that guar-
antee serializability by imposing restrictions on the way the flow of replica 
updates travel to other nodes of the system. By doing this, the protocol does 
not require any cooperation from the database transaction management soft-
ware installed at each node. This fact makes this protocol easy to implement 
in existing systems. 

Applications to Hierarchical and Replicated Databases 
101 
The most recent protocol by Brietbart et al. [BKR+99] , that we call the 
DAG protocol, imposes a much weaker requirement on data placement than 
earlier protocols. The DAG protocol works as long as the copy graph is acyclic, 
and ensures serializability in the following manner: First, it creates a forest 
(collection of trees) out of the original copy graph, in such a way that if site 8i 
is a child of site 8j in the copy graph, then 8i is a descendant of 8j in the forest. 
The update transactions are propagated along the edges of the copy graph. 
As an example, consider the copy graph in figure 7.3. The DAG protocol 
will change it to a forest such as the ones shown in figure 7.4. These forests, 
however, have the drawback that even though 82 is not interested in updates 
to a, 82 is nevertheless forced to see them and transmit them to another host, 
to preserve the imposed order of updates. This leads to extra delays in update 
propagation. Moreover, there are cases, such as systems where security is an 
issue [AJF96], where changing the way updates travel is not an option, since it 
may force a host to violate the security policy and communicate information to 
a untrusted host. 
a 
a,b 
Figure 7.3. 
A crown-free DAG 
There is really no need to change the ordering of update propagation for 
the graph in figure 7.3. Here, the requirements for the update propagation are 
identical to those for replicated architecture in section 4 of chapter 3. Moreover, 
since the graph in figure 7.3 is crown-free, the ordering of updates can be made 
globally consistent as long as the propagation of updates respect the order 
imposed by the graph. 

102 
MULTILEVEL SECURE TRANSACTION PROCESSING 
a 
(3)a 
J 
G b 
j 
G) a,b 
j 
G) b 
Figure 7.4. 
1\vo forests produced by the DAG protocol on the DAG of figure 7.3 
Let us tum our attention to DAGs that have crowns, such as the one shown 
in figure 7.5. 
In the face of security constraints, we can introduce a new node 8t, as shown 
in figure 7.6. Notice that the node 8t has a security classification which is 
less than or equal to both 83 and 84. The introduction of 8t breaks the crown 
and therefore makes global schedules serializable, while respecting the security 
constraints. 
When security is not a concern, then a solution such as the one presented by 
figure 7.7 is a viable option. Here, 83 has been chosen to serialize transactions 
for both itself and 84. Since replicated and MLS databases are structurally 
similar, the proposed approaches can be adopted for MLS DBMS environments. 

Applications to Hierarchical and Replicated Databases 
103 
b 
a,b G 
G a,b 
Figure 7.5. 
A DAG with crowns 
Figure 7.6. 
A solution with a trusted node 

104 
MULTILEVEL SECURE TRANSACTION PROCESSING 
a,b 
Figure 7.7. 
A non-forest solution 

Chapter 8 
CHALLENGES 
Several secure concurrency control protocols proposed by researchers have 
been discussed in the earlier chapters. These protocols use a number of tech-
niques including locking, timestamping, and a combination of both, whereas 
some solutions maintain multiple versions of data. As can be readily seen, due 
to the stringent constraints imposed by multilevel security, these proposed solu-
tions are not completely satisfactory; they trade either performance or recency 
for achieving security. Yet another class of solutions offer weaker correctness, 
weaker than the traditional serializability. 
In this chapter, we present some directions to future research in addressing 
this problem. In particular, we argue whether advanced transaction paradigms 
can be utilized in addressing the secure concurrency control problem. 
In addition, research in secure transaction processing, to a large extent, has 
been limited to addressing concurrency control problems. Issues such a recov-
ery has been given little attention. Moreover, research in this area has been 
limited to the traditional transaction concepts. We explore these issues in this 
chapter. 
1 
USING ADVANCED TRANSACTION MODEL 
PARADIGMS 
Research in the area of transaction processing in multilevel secure database 
management systems has substantially progressed in the past few years. The 
major emphasis of this progress can be seen in secure concurrency control, one 
of the essential components of secure transaction processing. This research has 
focused on several areas including single-level and multilevel transaction pro-
cessing in both kemelized and replicated architectures. Despite the substantial 
research activity, the existing secure transaction processing solutions offered by 
V. Atluri et al., Multilevel Secure Transaction Processing
© Kluwer Academic Publishers 2000

106 
MULTILEVEL SECURE TRANSACTION PROCESSING 
researchers are not completely satisfactory. Therefore, there is a need to inves-
tigate whether exploiting the properties of advanced transaction models yield 
better secure concurrency control solutions for traditional databases. More 
specifically, one may consider adapting the nested, split, or flexible transaction 
paradigms. 
In particular, the problem with the secure transaction processing is that if 
a data conflict between a low security level transaction and a high security 
level transaction should occur, for security reasons, the conflict must always be 
resolved in favor of the low security transaction. In other words, the low security 
transaction has the right of way whereas the high security level transaction is 
hindered or restarted. 
In the case where a transaction has to be restarted, conventional solutions re-
quire that all of the transaction's effects must be undone and then the transaction 
is re-executed from the beginning. This approach when applied to multilevel 
secure databases, however, causes resource wastage and in some cases results 
in starvation of high security level transactions. 
We wonder whether modeling a transaction as an extended transaction model 
(such as the workflow model) could reap significant gains in performance. One 
may explore mechanisms that are specifically tailored for reducing the perfor-
mance penalty for restarts by designing protocols such that (1) the probability 
of restarting a high security level transaction is minimum; and (2) even when 
a restart becomes inevitable, the portion of the transaction to be re-executed is 
minimum. In the next three paragraphs, we describe three cases where such 
optimizations are possible. 
In the first case, a particular data modification by a low security level trans-
action does not necessitate any form of corrective action by a high security level 
transaction. For example, depending on the value of a certain data item x, a 
high level transaction could follow one execution sequence or another. So if 
the value of x changes during its execution due to a write operation by a low 
level transaction, if the updated value still satisfies the condition on x, the high 
transaction need not be rolled back. 
In the second case, a data modification may require some corrective action, 
but the compensatory action is pre-specified and can be efficiently accomplished 
than restarting the transaction. Considering once again the above scenario, an-
other alternative is to execute a compensating module. (Note that this compen-
satory action is different from that in recovery where only the correctness of 
the database is the issue and not saving the transaction restarts.) 
In the third case a restart is necessary, however, the transaction need not 
have to be restarted in full. There are situations in which a transaction restart 
becomes necessary and in such a scenario, not all parts of the transaction has 
to be restarted. There has been an established model of nested transaction 
processing [BBG89] and this model describes the way to ensure the correctness 

Challenges 
107 
under the scenario where a transaction is split up into various subtransactions. 
The splitting of a transaction into subtransactions based on semantic information 
is described in [AJR98, SSV92]. The suggested mechanism keeps track of the 
data dependency between the subtransactions and if a data item is modified, only 
those subtransactions which are dependent on the data item x are restarted. 
Research in this direction by using similar techniques described above may 
enhance the performance of secure transaction processing protocols. 
2 
SECURE LONG DURATION TRANSACTION 
PROCESSING 
Traditionally, database systems are used to manage large volumes of rela-
tively uniform data. Typical applications include payroll, banking, airline reser-
vations, in which transactions are generally short, simple and run for a fraction of 
a second. More recently, the scope of database systems has widened. Now they 
are applied in such areas as computer-aided design, office information systems, 
manufacturing system, computer-aided software engineering, etc. These new 
application domains require long-duration, interactive and cooperation among 
users. 
Until recently, the work in secure transaction processing has been limited to 
the traditional transaction concepts. In this section, we first review the solutions 
proposed for secure long-duration transaction processing and identify additional 
challenges. 
A long duration or simply a long transaction typically accesses large num-
ber of data items and persists for a long duration of time in the database. As 
can be seen from chapter 3, the solutions proposed in the literature to solve 
the secure concurrency control problem are not completely satisfactory. For 
example, [AJ92, MJ92] abort a high transaction whenever a lower level trans-
action writes a data item that it has read. Aborting a long duration transaction, 
especially when it has completed most of its work results in severe' performance 
penalties. Keefe-Tsai protocol [KT90] gives excessively old versions of data 
and Jajodia-Atluri protocol [JA92] make transactions wait for the completion 
of lower level transactions and sometimes reexecute portions of transactions 
if necessary. Thus it can clearly be seen that either performance or up-to-date 
data has to be traded-off to maintain consistency of the data. This problem is 
compounded if transactions are long running. 
Recently, this problem of accommodating long duration transactions in mul-
tilevel secure databases has received attention. Ammann-Jajodia [AJ94a] pro-
pose a solution that uses limited number of versions in the database (based 
on the multiversion algorithm originally proposed by Mohan et al. [MPL92]). 

108 
MULTILEVEL SECURE TRANSACTION PROCESSING 
Instead of creating a new version whenever there is an update to a data item, 1 
it maintains versions corresponding to each externally defined version period. 
Even if several transactions update a data item x during a version period n, 
only the last update is recorded as version n of x. This protocol ensures correct 
execution of concurrent transactions as follows: Every transaction Ti is related 
to a version function V(Ti) and an assignment function a(1i). If Ti is an up-
date transaction, then V(Ti) is the first version period after which Ti'S updates 
become visible, and a(Ti) is the version of the lower level data read by 1i. If 
Ti is a read-only transaction, then it is related to a version period as follows: If 
Ti reads data from its own level from a version period n, then v(1i) = n + 1, 
otherwise V(Ti) = n + 1 where n is the version period of the lower level data 
that 1i has read.2 
Additional constraints are imposed on the transaction execution to ensure 
serializability: For every transaction 1i, then either a(Ti) = v(Td - 1, or 
a(1i) = v(1i). Effectively, this protocol allows a transaction to be running at 
most during two version periods thus setting a time limit by which a transaction 
must complete failing which results in aborting the transaction. A lock-based 
protocol, based on the above concepts, has been proposed by Pal [Pal95]. 
However, there are a number of limitations to the above approach. First, it 
supports only long read-only transactions but does not provide a solution to the 
long update transactions. Second, a long transaction cannot run for more than 
two version periods (the current version and the immediate earlier version) even 
if one maintains more than three versions. Although a transaction with a long 
duration can be accommodated by choosing a long version period, it would 
force transactions to read very old version of the data. On the other hand, a 
short version period would result in more transaction aborts. 
Therefore, research is needed (1) to develop solutions that can increase the 
duration allowed for a transaction (in terms of number of version periods) by 
maintaining more than three versions, (2) to support long update transactions, 
(3) to preserve serializability or provide consistent view even for long read-only 
transactions (4) explore ways to relax serializability requirements, similar to 
the work on supporting long duration transactions in non-secure environments 
[Pu86, Gra8I, GMS87], since it is more appropriate to relax serializability in 
MLS DBMSs because it is not possible to enforce all the integrity constraints, 
especially those that span multiple security levels [MJ88, Atl94, JA92]. 
1 Number of protocols proposed for secure concurrency control (e.g., [KT90, JA92]) generate a new version 
of the data item for each update. 
2Whether Ti is an update transaction or a read-only transaction, if it reads from more than one lower level, 
then it must read data from the same version period. 

Challenges 
109 
3 
RECOVERY METHODS 
Efflcientrecovery algorithms are crucial for good performance in transac-
tion processing systems. Write-Ahead Logging (WAL) is a popular recovery 
technique [GR93]. The method provides good performance because it does 
not force updates to disk at commit time and it does not constrain the buffer 
replacement policy. Only the log records written by the transaction must be on 
stable storage when the transaction commits. This is more efficient than forc-
ing updates because: (1) The log records describing an update are generally 
quite compact, allowing updates from several transactions to be logged with 
a single disk write; and (2) The I/O to the log is largely sequential, avoiding 
seek delays. By adopting a no-force protocol we can reduce the number of 
synchronous writes by a factor of ten. Each of these sequential writes will be 
from two to ten times faster than the random I/O that would be required with a 
force-policy. 
Recovery has received less attention from researchers than concurrency con-
trol. The reason for this seems to be that most recovery algorithms do not 
require trust (privilege). It is possible to maintain a recovery log for each ac-
tive security level. Recovery at each level can proceed independently. New 
transactions are not allowed to begin at levell until recovery is completed at 
all levels dominated by l. [KK92] proposes an Undo/No Redo protocol based 
on this observation. This protocol was adopted in the experimental prototype 
described in [WK95]. 
While simple, this approach can be quite inefficient. We will need n log 
files, one for each of the n active security levels in the system. To survive 
media failures, the log file is generally duplexed. The simplest implementation 
would allocate these 2n log replicas to 2n disk drives. In some systems this 
may be possible. However, in few cases is it a good use of resources. 
A less expensive solution would place log replicas from more than one secu-
rity level on the same drive. If we place log replicas from m security levels one 
each disk, we will reduce the cost by a factor of m. However, this sharing of the 
disk arm will result in more head movement and longer disk access times. Thus, 
we may reduce the cost by a factor of m but reduce system performance by a 
much greater factor The penalty could be as high as a factor of 10m reduction. 
The sharing of the disk arm will also require the use of a secure disk scheduling 
algorithm which will very likely increase the access times further. 
The solution described above suffers from poor performance because each 
log manager (i.e., the entity that performs the log writes) accesses the disk 
independently. It chooses a sector on the disk to write its log records without 
concern for where the disk head is currently positioned. If the individual log 
managers more carefully coordinate their actions, we can ensure that writes are 
still done sequentially. Furthermore, this coordination can be achieved without 
violating security. A method of achieving this is proposed in [PKP97]. It 

110 
MULTILEVEL SECURE TRANSACTION PROCESSING 
is based on the Write-Ahead Data Set (WADS) logging scheme described in 
[GR93]. The idea is to share a single disk partition across all levels. This is 
known as the WADS partition and is used to buffer the end of the log file. Every 
time a new disk sector comes under the disk read/write head, the disk manager 
writes any log records that are available. In this way, disk seeks and rotational 
latency are eliminated. When the disk manager prepares a buffer of log records 
to be written, it fills the buffer using log records from the lowest security level 
first. 3 When these are exhausted, the covering level is considered, and so on. 
After some time, the WADS partition will fill. When this happens, the 
log manager switches to a second wads partition and compacts the first. The 
compact version of the log file is partitioned by security level, i.e., each security 
level has a compact log file its log records are written to. During compaction, 
the WADS partition is read in its entirety and each log record is copied to the 
end of the compacted log file at its security level. 
Work in this area is just beginning. Work is needed in several areas. First, the 
design is quite complex and yet employs a trusted implementation. More work 
is needed to simplify the design so that the complexity can be reduced. Second, 
while an experimental prototype is described in [PKP97] , the performance 
evaluation is preliminary. More work is needed to determine if the complexity 
and overhead are justified by realistic workloads. Finally, the method discussed 
makes use of information about the application (i.e., logging) to improve the 
efficiency of disk access. It may be possible to identify other specific access 
patterns for which efficient and secure disk scheduling protocols can be derived. 
If so, techniques for providing hints of this sort to the 110 management portion 
of the operating system would greatly simplify problems of this sort. 
3To simplify the explanation, we will assume that the security levels are totally ordered. The algorithm does 
not require this. 

References 
[AA98] 
D. Agrawal and A. EI Abbadi. Data Replication. Bulletin of the 
IEEE Computer Society Technical Committee on Data Engineer-
ing, December 1998. 
[AAA +95] G. Alonso, D. Agrawal, A. EI Abbadi, M. Kamath, R. Gunthor, and 
C. Mohan. Advanced transaction models in workflow contexts. 
Research report, mM Almaden Research Center, 1995. 
[AAJ92] 
[AAJ95] 
[ABJ93] 
[ABJ94] 
[ABJ95] 
D. Agrawal, A. EI Abbadi, and R. Jeffers. Using Delayed Com-
mitment in Locking Protocols for Real-Time Databases. In Proc. 
of ACM SIGMOD Con/., June 1992. 
Paul Ammann, Vijayalakshmi Atluri, and Sushil Jajodia. The 
Partitioned Synchronization Rule for Planar Extendible Partial Or-
ders. IEEE Transactions on Knowledge and Data Engineering, 
7(5):797 - 808, October 1995. 
Vijayalakshmi Atluri, Elisa Bertino, and Sushil Jajodia. Achiev-
ing Stricter Correctness Requirements in Multilevel Secure 
Databases. In Proc. IEEE Symposium on Security and Privacy, 
pages 135-147, Oakland, California, May 1993. 
Vijayalakshmi Atluri, Elisa Bertino, and Sushil J ajodia. Achieving 
stricter correctness requirements in multilevel secure databases: 
The dynamic case. In Thomas Keefe and Carl Landwehr, edi-
tors, Database Security, VII: Status and Prospects, pages 135-158. 
North Holland, 1994. 
Vijayalakshmi Atluri, Elisa Bertino, and Sushil J ajodia. Degrees 
of Isolation, Concurrency Control Protocols, and Commit Pro-
tocols. In M. Morgenstern J. Biskup and C. E. Landwehr, edi-

112 
MULTILEVEL SECURE TRANSACTION PROCESSING 
tors, Database Security, VII: Status and Prospects. North Holland, 
1995. 
[ABKW98] T. Anderson, Y. Breitbart, H. F. Korth, and A. Wool. Replication, 
consistency, and practicality: Are these mutually exclusive? In 
Proceedings of the ACM SIGMOD Conference, Seattle, WA, 1998. 
[AGM92] 
R. Abbott and H. Garcia-Molina. Scheduling Real-time Trans-
actions: A Performance Evaluation. ACM Trans. on Database 
Systems, September 1992. 
[AH96] 
Vijayalakshmi Atluri and Wei-Kuang Huang. An Authorization 
Model for Workftows. In Proc. of the Fifth European Symposium 
on Research in Computer Security, September 1996. 
[AHB 99] 
Vijayalakshmi Atluri, Wei -Kuang Huang, and Elisa Bertino. A Se-
mantic Based Execution Model for Multilevel Secure Workftows. 
Journal of Computer Security, to appear, 1999. 
[Air83] 
Air Force Studies Board. Multilevel Data Management Security. 
National Research Council, National Academy Press, Washing-
ton, DC, March 1983. 
[AJ92] 
Paul Ammann and Sushi! Jajodia. A timestamp ordering algo-
rithm for secure, single-version, multi-level databases. In Carl 
Landwehr and Sushil Jajodia, editors, Database Security, II: Sta-
tus and Prospects, pages 23-25. North Holland, 1992. 
[AJ93] 
Paul Ammann and Sushil Jajodia. Distributed timestamp genera-
tion in planar lattice networks. ACM Transactions on Computer 
Systems, 11(3):205-225, August 1993. 
[AJ94a] 
Paul Ammann and Sushil Jajodia. An efficient multiversion al-
gorithm for secure servicing of transaction reads. In Proc. of the 
1 st ACM conference on Computer and Communication Security, 
pages 118-125, Fairfax, VA, November 1994. 
[AJ94b] 
Paul Ammann and Sushi! Jajodia. Planar lattice security struc-
tures for multilevel replicated database. In T. F. Keefe and C.E. 
Landwehr, editors, Database Security VII: Status and Prospects, 
pages 125-134. North-Holland, Amsterdam, 1994. 
[AJB96] 
Vijayalakshmi Atluri, Sushil Jajodia, and Elisa Bertino. Alterna-
tive correctness criteria for concurrent execution of transactions 
in multilevel secure databases. IEEE Transactions on Knowledge 
and Data Engineering, 8(5), October 1996. 

References 
113 
[AJF96] 
Paul Ammann, Sushil Jajodia, and Phyllis Frankl. Globally con-
sistent event ordering in one-directional distributed environments. 
IEEE Transactions on Parellel and Distributed Systems, 7(6), June 
1996. 
[AJJ92] 
Paul Ammann, Frank Jaeckle, and Sushil Jajodia. A two snapshot 
algorithm for concurrency control in secure multi-level databases. 
In Proceedings of the 1992 Symposium on Research in Security 
and Privacy, pages 204-215, Oakland, CA, May 1992. 
[AJR96] 
Paul Ammann, Sushil Jajodia, and Indrakshi Ray. Ensuring atom-
icity of multilevel transactions. In Proc. IEEE Symp. on Security 
and Privacy, pages 74-84, Oakland, CA, May 1996. 
[AJR98] 
Paul Ammann, Sushil Jajodia, and Indrakshi Ray. A Semantic-
Based Transaction Processing Model for Multilevel Transactions. 
Journal of Computer Security, 6(3), 1998. 
[Atl94] 
Vijayalakshmi Atluri. Concurrency Control in Multilevel Secure 
Databases. PhD thesis, School of Information Technology and 
Engineering, George Mason University, 1994. 
[BBG89] 
C. Beeri, P. Bernstein, andN. Goodman. A Model for Concurrency 
in Nested Transactions Systems. Journal of ACM, 36(2), 1989. 
[Bha96] 
A. Bhaskaran. A Slot Stealing-Based Multilevel Secure Database 
Buffer Manager. PhD thesis, Dept. of Computer Science and 
Engineering, The Pennsylvania State University, 1996. 
[BHG87] 
Philip A. Bernstein, Vassos Hadzilacos, and Nathan Good-
man. Concurrency Control and Recovery in Database Systems. 
Addison-Wesley, Reading, MA, 1987. 
[BJMN93] Barbara T. Blaustein, Sushil Jajodia, Catherine D. McCollum, and 
LouAnna Notargiacomo. A model of atomicity for multilevel 
transactions. In Proc. IEEE Symposium on Security and Privacy, 
pages 120-134, Oakland, California, May 1993. 
[BJMR98] 
Elisa Bertino, Sushil Jajodia, Luigi Mancini, and Indrajit 
Ray. Advanced transaction processing in multilevel secure file 
stores. IEEE Transactions on Knowledge and Data Engineering, 
10(1):120-135, 1998. 
[BK97] 
Y. Breitbart and H. F. Korth. Replication and consistency: Be-
ing lazy helps sometimes. In Proceedings of the ACM SIGACT-
SIGMOD-SIGART Sympposium on Principles of Database Sys-
tems, Tucson, AZ, 1997. 

114 
MULTILEVEL SECURE TRANSACTION PROCESSING 
[BKR+99] 
Y. Breitbart, R. Komondoor, R. Rastogi, S. Seshadri, and A. Sil-
berschatz. Update propagation protocols for replicated databases. 
In Proceedings of the ACM SIGMOD Conference, Philadelphia, 
PA, May 1999. 
[BL 76] 
D.E. Bell and LJ. LaPadula. Secure computer systems: Unified 
exposition and multics interpretation. Technical Report MTR-
2997, The Mitre Corporation, Bedford, MA, March 1976. 
[Chr91] 
P.K. Chrysanthis. ACTA, Aframeworkfor modeling and reasoning 
about extended transactions. PhD thesis, Department of Computer 
and Information Science, University of Massachusetts, Amherst, 
1991. 
[CJ92] 
Oliver Costich and Sushil Jajodia. Maintaining multilevel transac-
tion atomicity in MLS Database systems with Kemelized Archi-
tecture. In Carl Landwehr and Sushil Jajodia, editors, Database 
Security, v.. Status and Prospects, pages 173-189. North Holland, 
1992. 
[CJL89] 
M.1. Carey, R. Jauhari, and M. Livny. Priority in DBMS Resource 
Scheduling. In Proc. of J 5th VLDB Conf., pages 397-410, August 
1989. 
[Com83] 
Committee on Multilevel Data Management Security, Air Force 
Studies Board, National Research Council, National Academy 
Press, Washington, DC. Multilevel Data Management Security, 
March 1983. 
[Cos92] 
Oliver Costich. Transaction processing using an untrusted sched-
uler in a multilevel secure database with replicated architecture. 
In Proc. of the 6th IFIP WG J J.3 Workshop on Database Security, 
pages 227-252, August 1992. 
[CRR96] 
P. Chundi, D. 1. Rosenkratz, and S. S. Ravi. Deferred updates 
and data placement in distributed databases. In Proceedings of 
the International Conference on Data Engineering, New Orleans, 
LA, 1996. 
[Dav93] 
R. David. Secure Concurrency Control. PhD thesis, Computer 
Science Department, University of Virginia, 1993. 
. 
[Den68] 
P. 1. Denning. The Page set Model for Program Behavior. Comm. 
of ACM, November 1968. 
[Den76] 
Dorothy E. Denning. A lattice model of secure information flow. 
Communications of the ACM, pages 236-243, May 1976. 

References 
115 
[Den82] 
D. E. Denning. 
Cryptography and Data Security. 
Addison-
Wesley, Reading, MA., 1982. 
[DoD85] 
DoD Computer Security Center. Trusted Computer System Eval-
uation Criteria, December 1985. 
[DS93] 
R. David and S. Son. A Secure Two Phase Locking Protocol. In 
Proc. of Symp. on Reliable Distributed Systems, October 1993. 
[DSM95] 
Rasikan David, Sang H. Son, and Ravi Mukkamala. Support-
ing Security Requirements in Multilevel Real-time Databases. In 
Proc. IEEE Symposium on Security and Privacy, pages 199-210, 
Oakland, California, May 1995. 
[E+76] 
K. Eswaran et al. The Notions of Consistency and Predicate Locks 
in a Database Systems. Comm. of ACM, November 1976. 
[ED97] 
Ahmed K. Elmagarmid and Weimin Du. Workflow Management: 
State of the Art vs. State of the Market. In Advances in Workflow 
Management Systems and Interoperability, pages 1-17, August 
1997. 
[EH84] 
W. Effelsberg and T. Harder. Principles of Database Buffer Man-
agement. ACM Trans. on Database Systems, 9(4):560-595, De-
cember 1984. 
[Elm92] 
Ahmed K. Elmagarmid. Database Transaction Models for Ad-
vanced Applications. Morgan Kaufmann, San Mateo, California, 
1992. 
[G+93] 
1. Greenberg et al. The Secure Alpha Study (Final Summary Re-
port). Technical report, SRI International, June 1993. 
[Geo98] 
B. George. Secure Real-Time Transaction Processing. PhD thesis, 
Indian Institute of Science, Bangalore, 1998. 
[GH97] 
B. George and 1. Haritsa. Secure Transaction Processing in Firm 
Real-Time Database Systems. In Proc. of ACM SIGMOD Conf., 
May 1997. 
[GHOS96] J. Gray, P. Helland, P. O'Neil, and D. Shasha. The dangers of 
replication and a solution. In Proceedings of the ACM SIGMOD 
Conference, Montreal, Canada, May 1996. 
[GHS95] 
Dimitrios Georgakopoulos, Mark Hornick, and Amit Sheth. An 
Overview of Workflow Management: From Process Modeling to 

116 
MULTILEVEL SECURE TRANSACTION PROCESSING 
Workflow Automation Infrastructure. Distributed and ParalleL 
Databases, pages 119-153, 1995. 
[GM82] 
J. A. Goguen and J. Meseguer. Security Policy and Security Mod-
els. In Proc. IEEE Symposium on Security and Privacy, pages 
11-20, 1982. 
[GMS87] 
H. Garcia-Molina and K. Salem. Sagas. In Proceedings of the 
ACM Conference on Management of Data, pages 249-259, May 
1987. 
[GR93] 
Jim Gray and Andreas Reuter. Transaction Processing: Concepts 
and Techniques. Morgan Kaufmann, San Mateo, California, 1993. 
[Gra81] 
J. Gray. The Transaction Concept: Virtues and Limitations. In 
Proceedings of the International Conference on Very Large Data 
Bases, pages 144-154, September 1981. 
[Har84] 
T. Harder. 
Observations on Optimistic Concurrency Control 
Schemes. Information Systems, 9(2):111-120,1984. 
[Har91] 
J. Haritsa. Transaction Processing in Firm Real-Time Database 
Systems. PhD thesis, Computer Sciences Department, University 
of Wisconsin, Madison, 1991. 
[HC86] 
M. Hsu and A. Chan. Partitioned Two-phase Locking. ACM Trans-
actions on Database Systems, 11(4):431-446, December 1986. 
[HM83] 
M. Hsu and S.E. Madnick. Hierarchical Database Decomposition: 
A technique for database concurrency control. In Proceedings 
of the 2nd ACM SIGACT-SIGMOD Symposium on Principles of 
Database Systems, pages 182-193, 1983. 
[HS90] 
J. Huang and J. Stankovic. Buffer Management in Real-Time 
Databases. 
Technical Report COINS 90-65, Univ. of Mas-
sachusetts, 1990. 
[Inf93a] 
Informix. Informix-OnLineiSecure Administrator's Guide. In-
formix Software, Inc., Menlo Park, CA, April 1993. 
[Inf93b] 
Informix. 
Informix-OnLinelSecure Security Features User's 
Guide. Infonnix Software, Inc., Menlo Park, CA, April 1993. 
[JA92] 
Sushil Jajodia and Vijayalakshmi Atluri. Alternative Correctness 
Criteria for Concurrent Execution of Transactions in Multilevel 
Secure Databases. In Proc. IEEE Symposium on Security and 
Privacy, pages 216-224, Oakland, California, May 1992. 

References 
117 
[JK90] 
Sushil Jajodia and Boris Kogan. Integrating an Object-oriented 
Data Model with Multilevel Security. In Proc. IEEE Symposium 
on Security and Privacy, pages 76-85, Oakland, California, May 
1990. 
[JLT95] 
E. Jensen, C. Locke, and H. Tokuda. A Time-Driven Scheduling 
Model for Real-Time Operating Systems. In Proc. of 6th IEEE 
Real-Time Systems Symp., December 1995. 
[JM93] 
Sushil Jajodia and Catherine D. McCollum. Using Two-phase 
Commit for Crash Recovery in Federated Multilevel Secure 
Database Management Systems. Dependable Computing and 
Fault Tolerant Systems, 8:365-381, 1993. 
[JMB94] 
Sushil Jajodia, Catherine D. McCollum, and Barbara T. Blaustein. 
Integrating concurrency control and commit algorithms in dis-
tributed multilevel secure databases. In T. F. Keefe and C. E. 
Landwehr, editors, Database Security, VII: Status and Prospects, 
pages 109-121. North-Holland, Amsterdam, 1994. 
[JMR96] 
Sushil Jajodia, Luigi Mancini, and Indrajit Ray. A Secure Locing 
Protocol for Multilevel Database Management Systems. In Proc. 
of the 10th IFIP WG 11.3 Workshop on Database Security, July 
1996. 
[KK92] 
I. E. Kang and T. F. Keefe. 
On Transaction Processing for 
Multilevel-Secure Replicated Databases. In Proc. of the European 
Symposium on Reserach in Computer Security, pages 329-347, 
November 1992. 
[KK95] 
I. E. Kang and T. F. Keefe. Transaction management for multilevel 
secure replicated databases. Journal of Computer Security, 3: 115-
145, 1995. 
[KS91] 
W. Kim and J. Srivastava. Buffer Management and Disk Schedul-
ing for Real-Time Relational Database Systems. In Proc. of 3rd 
COMAD Conj, December 1991. 
[KT90] 
T. F. Keefe and W. T. Tsai. Multiversion Concurrency Control for 
Multilevel Secure Database Systems. In Proc. IEEE Symposium 
on Security and Privacy, pages 369-383, Oakland, California, 
May 1990. 
[Lam??] 
Leslie Lamport. Concurrent Reading and Writing. Comm. of 
ACM,20(11):806-811, 1977. 

118 
MULTILEVEL SECURE TRANSACTION PROCESSING 
[LL 73] 
C. Liu and J. Layland. Scheduling Algorithms for Multiprogram-
ming in a Hard Real-Time Environment. Journal of the ACM, 
1973. 
[LS93] 
J. Lee and S. H. Son. Using Dynamic Adjustment of Serialization 
Order for Real-Time Database Systems. In Proc. of 11th IEEE 
Real-Time Systems Symp., December 1993. 
[McC90] 
Daryl McCullough. A Hookup Theorem for Multilevel Security. 
IEEE Transactions on Software Engineering, 16(6):563-568, June 
1990. 
[MG90] 
William T. Maimone and Ira B. Greenberg. Single-level Multi-
version Schedulers for Multilevel Secure Database Systems. In 
Proc. 6th Annual Computer Security Applications Conj, pages 
137-147, Tucson, Arizona, December 1990. 
[Min95] 
Spencer E. Minear. Providing Policy Control Over Object Oper-
ations in a Mach Based System. In Proc. of the Fifth USENIX 
UNIX Security Symposium, June 1995. 
[MJ88] 
Catherine Meadows and Sushil Jajodia. Integrity versus Security 
in Multi-level Secure Databases. In Carl E. Landwehr, editor, 
Database Security, Status and Prospects, pages 89-101. North-
Holland, 1988. 
[MJ92] 
John McDermott and Sushi! Jajodia. Orange Locking: Channel-
free Database Concurrency Control via Locking. In Proc. of the 
6th IFIP WG 11.3 Workshop on Database Security, Vancouver, 
BC, August 1992. 
[MJS91] 
John McDermott, Sushil Jajodia, and Ravi Sandhu. A Single-
level Scheduler for Replicated Architecture for Multilevel Secure 
Databases. In Proc. 7th Annual Computer Security Applications 
Conj, pages 2-11, San Antonio, Texas, December 1991. 
[MK93] 
A. Mathur and T. F. Keefe. The concurrency control and recovery 
problem for multilevel update transactions in MLS systems. pages 
10-23, Franconia, NH, 1993. 
[MN82] 
D. Menasce and T. Nakanishi. Optimistic versus Pessimistic con-
currency control mechanisms in Database Management Systems. 
Journal of the ACM, 7(1),1982. 
[MPL92] 
C. Mohan, Hamid Pirahesh, and Raymond Lorie. Efficient and 
Flexible Methods for Transient Versioning of Records to Avoid 

References 
119 
Locking by Read-only Transactions. In Proc. ACM SIGMOD 
Int'l. Can! on Management of Data, pages 124-133, San Diego, 
CA, June 1992. 
[MS95] 
Ravi Mukkamala and Sang H. Son. A Secure Concurrency Control 
Protocol for Real-time Databases. In Proc. of the 9th IFIP WG 
11.3 Workshop on Database Security, Rensselaerville, NY, August 
1995. 
[Nat98] 
National Institute of Standards and Technology (NIST), Gaithers-
burg, MD. Common Criteriafor Information Technology Security 
Evaluation, Version 2.0, 1998. 
[Ora92] 
Oracle. Trusted Oracle Administrator's Guide. Oracle Corp., 
Redwood City, CA, 1992. 
[ORA97] 
ORACLE. 
Oracle 8 
Server 
Concepts, release 8.0, chapter 30: Database Replication. http:// 
www.oracle.com/supportidocumentation/oracle8/SCN80.pdf. 
June 1997. 
[PaI95] 
Shankar Pal. A Locking Protocol for Multilevel Secure Databases 
Providing Support for Long Transactions. In Proc. of the 9th IFfP 
WG 11.3 Workshop on Database Security, Rensselaerville, NY, 
August 1995. 
[PKP97] 
v. R. Pesati, T. F. Keefe, and S. Pal. The Design and Implemen-
tation of a Multilevel Secure Log Manager. In Proc. IEEE Symp. 
on Security and Privacy, Oakland, CA, May 1997. 
[Pu86] 
Calton Pu. On-the-fly, Incremental, Consistent Reading of Entire 
Databases. Algorithmica, 1(3):271-287, October 1986. 
[RBJM96] 
Indrajit Ray, Elisa Bertino, Sushil Jajodia, and Luigi Mancini. An 
Advanced Commit Protocol for MLS Distributed Database Sys-
tems. In Proc. Third ACM Conference on Computer and Commu-
nications Security, New Delhi, India, March 1996. 
[RK79] 
D. P. Reed and Rajendra K. Kanodia. Synchronization with event-
counts and sequencers. Comm. of ACM, 22(5): 115-123, February 
1979. 
[Rob82] 
J. Robinson. Design of concurrency control protocols for trans-
action processing systems. PhD thesis, Computer Sciences De-
partment, Carnegie-Mellon University, 1982. 

120 
MULTILEVEL SECURE TRANSACTION PROCESSING 
[RS94] 
Marek Rusinkiewicz and Amit Sheth. Specification and Exe-
cution of Transactional Workflows. In W. Kim, editor, Modem 
Database Systems: The Object Model, Interoperability, and Be-
yond. Addison-Wesley, 1994. 
[RSK91] 
Marek Rusinkiewicz, Amit Sheth, and George Karabatis. Specify-
ing Interdatabase Dependencies in a Multidatabase Environment. 
IEEE Computer, 24(12):46-53, December 1991. 
[SBJN96] 
K.P. Smith, B.T. Blaustein, S. Jajodia, and L. Notargiacomo. Cor-
rectness Criteria for Multilevel Secure Transactions. IEEE Trans-
actions on Knowledge and Data Engineering, 8( 1 ):32 - 45, Febru-
ary 1996. 
[SC93] 
James W. Stamos and Flaviu Cristiano Coordinator Log Trans-
action Execution Protocol. Distributed and Parallel Databases, 
1(1):383-408, 1993. 
[Sch74] 
M. Schaefer. Quasi-synchronization of readers and writers in a 
secure multi-level environment. Technical Report TM-5407/003, 
System Development Corp., September 1974. 
[SD94] 
S. Son and R. David. Design and Analysis of a Secure Two-
Phase Locking Protocol. In Proc. of IntI. Computer Software and 
Applications Con/., November 1994. 
[SDT88] 
S. H. Son, R. David, and B. Thuraisingham. Improving Timeliness 
in Real-Time Secure Database Systems. ACM SIGMOD Record, 
March 1988. 
[SDT95] 
S. H. Son, R. David, and B. Thuraisingham. An Adaptive Policy 
for Improved Timeliness in Secure Database Systems. In Proc. of 
AnnualIFIP WG 11.3 Conference of Database Secur ity, August 
1995. 
[Sha48] 
C. E. Shanon. A Mathematical Theory of Communications. Bell 
Syst. Tech. J., October 1948. 
[SMTA95] Munindar P. Singh, Greg Meredith, Christine Tomlinson, and 
Paul C. Attie. An event algebra for specifying and scheduling 
workflows. In Tok Wang Ling and Yoshifumi Masunaga, edi-
tors, Proc. Forth International Conference on Database Systems 
for Advanced Applications, Singapole, pages 53-60. World Sci-
entific, 1995. 

References 
121 
[SRK93] 
Amit Sheth, Marek Rusinkiewicz, and G. Karabatis. Using Poly-
transactions to Manage Interdependent Data. Bulletin of IEEE 
Technical Committee on Data Engineering, 16(2):37-40, 1993. 
[SRL87] 
L. Sha, R. Rajkumar, and J. P. Lehoczky. Priority Inheritance 
Protocols: An Approach to Real-Time Synchronization. Technical 
ReportCMU-CS-87-181, Departments ofCS, ECE, and Statistics, 
Carnegie Mellon University, 1987. 
[SSV92] 
Dennis Shasha, Eric Simon, and Patrick Valduriez. Simple Re-
lational Guidance for Chopping Up Transactions. In Proc. ACM 
SIGMOD Int'l. Con! on Management of Data, pages 298-307, 
San Diego, CA, June 1992. 
[Syb93] 
Sybase. 
Sybase Secure SQL Server Security Administrator's 
Guide. Sybase, Inc., Emeryville, CA, December 1993. 
[SZ88] 
J. Stankovic and W. Zhao. On Real-Time Transactions. In ACM 
SIGMOD Record, March 1988. 
[TSH96] 
S. Thomas, S. Sheshadri, and 1. Haritsa. Integrating Standard 
Transactions in Real-Time Database Systems. Information Sys-
tems, 21(1), March 1996. 
[U1l88] 
J. Ullman. Principles of Database and Knowledge-Base Systems, 
volume 1. Computer Science Press, Rockville, MD, 1988. 
[wfm94] 
Workflow reference model. Technical report, Workflow Manage-
ment Coalition, Brussels, Brussels, 1994. 
[WK95] 
A. Warner and T. Keefe. Version pool management in a mul-
tilevel secure multiversion transaction manager. In Proc. IEEE 
Symposium on Security and Privacy, pages 169-182, Oakland, 
California, May 1995. 
[WLKP96] A. Warner, Q. Li, T. Keefe, and S. Pal. The Impact of Multi-
level Security on Database Buffer Management. In Proc. of the 
European Symp. on Research in Computer Security, 1996. 

Index 
ACID properties, 13, IS 
AS2PL,52 
Access control, I 
discretionary, I 
mandatory, I 
Adaptive Secure 2PL, 52 
performance evaluation, 54 
Atomic commit, 16, 18 
ASEP,40 
03SEP,40 
early prepare, 20 
presumed abort, 20 
presumed commit, 20 
secure early prepare(SEP), 39 
two-phase commit, 18 
Atomicity, 13, 15-16, 18,36,71 
Availability, I, 18 
Bell-LaPadula model, 3-4, 6 
Buffer allocation, 84 
Buffer replacement, 85 
CAD/CAM,12 
CORBA,66 
COSA Workflow, 64 
Common Criteria, 3 
Concurrency control protocol, 16 
Concurrency control, 15 
Confidentiality, 1 
Consistency, 13, 15 
Covert channel, 6,21-23,70 
storage, 7 
timing, 7 
DAC, 1,4 
DCOM,66 
Data partition hierarchy, 97 
Data 
derived,95 
raw, 95 
Delayed replication, 86 
Deskflow,66 
Durability, 13, 15 
Ensemble, 64 
Evaluation Assurance Levels, 3 
FlowMark, 64 
Hierarchical databases, 95 
Hierarchical decomposition, 95 
High-ready-wait, 37 
Immediate replication, 86 
Immediate validation, 86 
InConcert, 64 
Informix-OnLinelSecure,24 
Integrity, I, 15 
Inter-transaction locality, 82 
Intra-transaction locality, 82 
Isolation, 13, 15 
Lattice, 2 
Lazy replication, 100 
Lotus Notes, 66 
Low-first, 37 
MAC, 1,4 
MLS DBMS architecture 
integrity lock, 8 
kernelized, 9, 27 
replicated, 10, 25 
trusted subject, II 
MLSDBMS,2 
Mutual exclusion, 71 
OPEN/workflow, 65 
ORACLE,12 
Objects, 3 
Office information systems, 12 
Open/workflow, 64 
Orange Book, 3 
Pageset function, 85 
Performance Evaluation 
AS2PL,54 
adaptive secure 2PL, 54 
Performance evaluation 
real-time concurrency control protocols, 50 
Propagation, 86 

124 
MULTILEVEL SECURE TRANSACTION PROCESSING 
Quiescent point, 97 
RTDBS,43 
introduction, 44 
priority assignment policies, 47 
Ranking function, 85 
Real-Time Concurrency Control protocols, 49 
2PL-HP,49 
OPT-SACRIFICE, 49 
OPT-WAIT,50 
performance evaluation, 50 
Real-Time Database System - see RTDBS, 43 
Recovery, 109 
Restart locality, 82 
SABRE,88 
fairness features, 91 
real-time features, 89 
search and slot selection, 90 
security features, 89 
SINTRA,II 
SeaView, 10 
Secrecy, I 
Secure Buffer Management, 81 
buffer allocation, 84 
buffer replacement, 85 
requirements, 82 
solutions, 83 
synchronization, 85 
Secure Priority Assignment, 47 
Secure commit, 37 
Serializability, 17, 23 
levelwise, 31 
one-copy, 29 
one-item read, 33 
pairwise, 31, 34 
Serialization graph, 23 
Signaling channel, 7 
Simple-security property, 4 
Snapshot algorithm, 30 
Staffware, 65 
Star - property, 4, 6, 73 
Subjects, 3 
Sybase Secure SQL Server, 24 
Sybase, 12 
Synchronization, 85 
System high mode, 8 
TCB,2 
TCSEC,3 
TFE,8 
TRUDATA,9 
Task dependency, 63 
control-flow, 68 
dynamic, 68 
external,69 
high-to-Iow, 69 
inter-workflow, 68 
intra-workflow, 68 
low-to-high, 69 
static, 68 
value, 69 
Task,63 
TeamWARE Flow, 65 
Timestamp ordering, 17, 21 
multiversion, 29 
Timestamp 
component based, 98 
Transaction model 
advanced, 105 
extended, 13,106 
flat, 13 
long duration, 13, 107 
nested, 13, 106 
split, 106 
workflow, 13,67, 106 
Transaction 
distributed, 18 
long-lived, 13 
multilevel, 36 
subtransaction, 36 
two-phase, 17 
well-formed, 17 
Trojan Horse, 1,4 
Trusted Computer System Evaluation Criteria, 3 
Trusted Computing Base, 2 
Trusted Oracle, 10, 24, 32, 35 
Two-phase locking, 17,21 
multiversion, 97 
partitioned, 97 
secure, 37 
strict, 25 
Ultimus,66 
UndolNo Redo, 109 
Untrusted front end, 11 
Update projection, 25 
Validation server, 86 
Version period, 108 
Visual Workflo, 65 
W4,65 
WtMC,66 
Workflow management system, 63 
documentbased,65 
e-mail based, 66 
groupware based, 66 
internet based, 66 
transaction based, 66 
Workflow reference model, 66 
Write-Ahead Logging, 109 

About the Authors 
Dr. Vijay Atluri is an Assistant Professor of Computer Information Sys-
tems in the Management Science and Information Systems Department and 
faculty associate of the Center for Information Management, Integration and 
Connectivity at Rutgers University, USA. She received her B.Tech. in Elec-
tronics and Communications Engineering from Jawaharlal Nehru Technologi-
cal University, Kakinada, India, M.Tech. in Electronics and Communications 
Engineering from Indian Institute of Technology, Kharagpur, India, and Ph.D. 
in Information Technology from George Mason University, USA. Her research 
interests include Information Systems Security, Database Systems, Workflow 
Management and Digital Libraries. She has published over 25 technical papers 
in refereed journals and conference proceedings in these areas. 
In 1999, Dr. Atluri served as a program co-chair for the IFIP WGl1.3 
Database Security conference, and co-edited a special issue on security for 
the Distributed and Parallel Databases: An International Journal, Kluwer. In 
1996, she was a recipient of the NSF CAREER Award to investigate issues 
related to incorporating multilevel security into database management systems 
for advanced application domains. She is a member of the IEEE Computer 
Society and the Association for Computing Machinery. 
Dr. Sushil Jajodia is Professor and Chairman of Department of Information 
and Software Engineering and Director of Center for Secure Information Sys-
tems at the George Mason University, Fairfax, Virginia. He joined GMU after 
serving as the director of the Database and Expert Systems Program within the 
Division of Information, Robotics, and Intelligent Systems at the National Sci-
ence Foundation. Before that he was the head of the Database and Distributed 
Systems Section in the Computer Science and Systems Branch at the Naval 
Research Laboratory, Washington and Associate Professor of Computer Sci-
ence and Director of Graduate Studies at the University of Missouri, Columbia. 
He has also been a visiting professor at the University of Milan, Italy and at 

the Isaac Newton Institute for Mathematical Sciences, Cambridge University, 
England. 
Dr. Jajodia received his PhD from the University of Oregon, Eugene. His re-
search interests include information security, temporal databases, and replicated 
databases. He has published more than 200 technical papers in the refereed 
journals and conference proceedings and has edited twelve books, including 
Temporal Databases: Research and Practice, Springer-Verlag Lecture Notes 
in Computer Science, Vol. 1399 (1998), Advanced Transaction Models and 
Architectures, Kluwer (1997), Multimedia Database Systems: Issues and Re-
search Directions, Springer-Verlag Artificial Intelligence Series (1996), and 
Information Security: An Integrated Collection of Essays, IEEE Computer So-
ciety Press (1995). He received the 1996 Kristian Beckman award from IFIP 
TC 11 for his contributions to the discipline of Information Security. 
Dr. Jajodia has served in different capacities for various journals and confer-
ences. He is the founding co-editor-in-chief of the Journal of Computer Secu-
rity. He is on the editorial boards of IEEE Concurrency, ACM Transactions on 
Information and Systems Security, and International Journal of Cooperative In-
formation Systems. He has been named a Golden Core member for his service 
to the IEEE Computer Society. He is a past chairman of the IEEE Computer 
Society Technical Committee on Data Engineering. He is a senior member of 
the IEEE and a member of IEEE Computer Society and Association for Com-
puting Machinery. The URL for his web page is http://isse.gmu.eduJ. 
Dr. Binto George is a visiting faculty in Computer Science department, 
Western Illinois University, USA. He obtained his B.Tech. in Electrical and 
Electronics from Mahatma Gandhi University, Kottayam, India, M.Tech. in 
Computer and Information Sciences from Cochin University of Science and 
Technology, Cochin, India, and Ph.D. in Computer Science from Indian In-
stitute of Science, Bangalore, India. His research interests include Real-Time 
Databases, Database Security, and Computer Security. He has been involved 
in the development of high performance real-time Concurrency Control and 
buffer management protocols for the secure real-time transaction processing 
environment. He is a member of the Association for Computing Machinery. 



Second Edition 
Computability, 
Complexity, and 
Languages 
Fundamentals of 
Theoretical Computer Science 
Martin D. Davis 
Department of Computer Science 
Courant Institute of Mathematical Sciences 
New York University 
New York, New York 
Ron Sigal 
Departments of Mathematics and Computer Science 
Yale University 
New Haven, Connecticut 
Elaine J. Weyuker 
Department of Computer Science 
Courant Institute of Mathematical Sciences 
New York University 
New York, New York 
ACADEMIC PRESS 
Harcourt, Brace & Company 
Boston 
San Diego 
New York 
London 
Sydney 
Tokyo 
Toronto 

This book is printed on acid-free paper @) 
CopyrightÂ© 1994, 1983 by Academic Press, Inc. 
All rights reserved 
No part of this publication may be reproduced or 
transmitted in any form or by any means, electronic 
or mechanical, including photocopy, recording, or 
any information storage and retrieval system, without 
permission in writing from the publisher. 
ACADEMIC PRESS, INC. 
525 B Street, Suite 1900, San Diego, CA 92101-4495 
United Kingdom Edition published by 
ACADEMIC PRESS LIMITED 
24-28 Oval Road, London NW1 7DX 
Library of Congress Cataloging-in-Publication Data 
Davis, Martin 1928-
Computability, complexity, and languages: fundamentals of 
theoretical computer science/Martin D. Davis, Ron Sigal, 
Elaine J. Weyuker. --2nd ed. 
p. em. --(Computer science and applied mathematics) 
Includes bibliographical references and index. 
ISBN 0-12-206382-1 
1. Machine theory. 2. Computational complexity. 3. Formal 
languages. I. Sigal, Ron. II. Weyuker, Elaine J. III. Title. 
IV. Series. 
QA267.D38 1994 
511.3-dc20 
93-26807 
CIP 
Printed in the United States of America 
94 95 96 97 98 BC 9 8 7 6 5 4 3 2 1 

To the memory of Helen and Harry Davis 
and to 
Hannah and Herman Sigal 
Sylvia and Marx Weyuker 
Vir;ginia Davis, Dana Latch, Thomas Ostrand 
and to 
Rachel Weyuker Ostrand 


Contents 
Preface 
Acknowledgments 
Dependency Graph 
1 Preliminaries 
1. Sets and n-tuples 
2. Functions 
3. Alphabets and Strings 
4. Predicates 
5. Quantifiers 
6. Proof by Contradiction 
7. Mathematical Induction 
Part l 
Computability 
2 Programs and Computable Functions 
1. A Programming Language 
2. Some Examples of Programs 
3. Syntax 
4. Computable Functions 
5. More about Macros 
vii 
xiii 
xvii 
xix 
1 
1 
3 
4 
5 
6 
8 
9 
15 
17 
17 
18 
25 
28 
32 

viii 
Contents 
3 Primitive Recursive Functions 
39 
1. Composition 
39 
2. Recursion 
40 
3. PRC Classes 
42 
4. Some Primitive Recursive Functions 
44 
5. Primitive Recursive Predicates 
49 
6. Iterated Operations and Bounded Quantifiers 
52 
7. Minimalization 
55 
8. Pairing Functions and Godel Numbers 
59 
4 A Universal Program 
65 
1. Coding Programs by Numbers 
65 
2. The Halting Problem 
68 
3. Universality 
70 
4. Recursively Enumerable Sets 
78 
5. The Parameter Theorem 
85 
6. Diagonalization and Reducibility 
88 
7. Rice's Theorem 
95 
*8. The Recursion Theorem 
97 
*9. A Computable Function That Is Not Primitive Recursive 
105 
5 Calculations on Strings 
113 
1. Numerical Representation of Strings 
113 
2. A Programming Language for String Computations 
121 
3. The Languages .9' and .9, 
126 
4. Post-Turing Programs 
129 
5. Simulation of .9, in :T 
135 
6. Simulation of .:Tin .9' 
140 
6 Turing Machines 
145 
1. Internal States 
145 
2. A Universal Turing Machine 
152 
3. The Languages Accepted by Turing Machines 
153 
4. The Halting Problem for Turing Machines 
157 
5. Nondeterministic Turing Machines 
159 
6. Variations on the Turing Machine Theme 
162 
7 Processes and Grammars 
169 
1. Semi-Thue Processes 
169 
2. Simulation of Nondeterministic Turing Machines by 
Semi-Thue Processes 
171 

Contents 
ix 
3. Unsolvable Word Problems 
176 
4. Post's Correspondence Problem 
181 
5. Grammars 
186 
6. Some Unsolvable Problems Concerning Grammars 
191 
*7. Normal Processes 
192 
8 Classifying Unsolvable Problems 
197 
1. Using Oracles 
197 
2. Relativization of Universality 
201 
3. Reducibility 
207 
4. Sets r.e. Relative to an Oracle 
211 
5. The Arithmetic Hierarchy 
:215 
6. Post's Theorem 
217 
7. Classifying Some Unsolvable Problems 
224 
8. Rice's Theorem Revisited 
230 
9. Recursive Permutations 
231 
Part 2 
Grammars and Automata 
235 
9 Regular Languages 
237 
1. Finite Automata 
237 
2. Nondeterministic Finite Automata 
242 
3. Additional Examples 
247 
4. Closure Properties 
249 
5. Kleene's Theorem 
253 
6. The Pumping Lemma and Its Applications 
260 
7. The Myhill-Nerode Theorem 
263 
10 Context-Free Languages 
269 
1. Context-Free Grammars and Their Derivation Trees 
269 
2. Regular Grammars 
280 
3. Chomsky Normal Form 
285 
4. Bar-Hillel's Pumping Lemma 
287 
5. Closure Properties 
291 
*6. Solvable and Unsolvable Problems 
297 
7. Bracket Languages 
301 
8. Pushdown Automata 
308 
9. Compilers and Formal Languages 
323 

X 
Contents 
11 
Context-Sensitive Languages 
327 
327 
330 
337 
1. The Chomsky Hierarchy 
2. Linear Bounded Automata 
3. Closure Properties 
Part 3 
Logic 
345 
12 Propositional Calculus 
34 7 
1. Formulas and Assignments 
347 
2. Tautological Inference 
352 
3. Normal Forms 
353 
4. The Davis-Putnam Rules 
360 
5. Minimal Unsatisfiability and Subsumption 
366 
6. Resolution 
367 
7. The Compactness Theorem 
370 
13 
Quantification Theory 
375 
1. The Language of Predicate Logic 
375 
2. Semantics 
377 
3. Logical Consequence 
382 
4. Herbrand's Theorem 
388 
5. Unification 
399 
6. Compactness and Countability 
404 
*7. Godel's Incompleteness Theorem 
407 
*8. Unsolvability of the Satisfiability Problem in Predicate Logic 
410 
Part4 
Complexity 
14 Abstract Complexity 
1. The Blum Axioms 
2. The Gap Theorem 
3. Preliminary Form of the Speedup Theorem 
4. The Speedup Theorem Concluded 
15 
Polynomial-Time Computability 
1. Rates of Growth 
2. P versus NP 
3. Cook's Theorem 
4. Other NP-Complete Problems 
417 
419 
419 
425 
428 
435 
439 
439 
443 
451 
457 

Contents 
xi 
Part 5 
Semantics 
465 
16 Approximation Orderings 
467 
1. Programming Language Semantics 
467 
2. Partial Orders 
4 72 
3. Complete Partial Orders 
475 
4. Continuous Functions 
486 
5. Fixed Points 
494 
17 Denotational Semantics of Recursion Equations 
505 
1. Syntax 
505 
2. Semantics of Terms 
511 
3. Solutions toW-Programs 
520 
4. Denotational Semantics ofW-Programs 
530 
5. Simple Data Structure Systems 
539 
6. Infinitary Data Structure Systems 
544 
18 Operational Semantics of Recursion Equations 
557 
1. Operational Semantics for Simple Data Structure Systems 
557 
2. Computable Functions 
575 
3. Operational Semantics for Infinitary Data Structure Systems 
584 
Suggestions for Further Reading 
593 
Notation Index 
595 
Index 
599 


Preface 
Theoretical computer science is the mathematical study of models of 
computation. As such, it originated in the 1930s, well before the existence 
of modern computers, in the work of the logicians Church, Godel, Kleene, 
Post, and Turing. This early work has had a profound influence on the 
practical and theoretical development of computer science. Not only has 
the Turing machine model proved basic for theory, but the work of these 
pioneers presaged many aspects of computational practice that are now 
commonplace and whose intellectual antecedents are typically unknown to 
users. Included among these are the existence in principle of all-purpose 
(or universal) digital computers, the concept of a program as a list of 
instructions in a formal language, the possibility of interpretive programs, 
the duality between software and hardware, and the representation of 
languages by formal structures. based on productions. While the spotlight 
in computer science has tended to fall on the truly breathtaking technolog-
ical advances that have been taking place, important work in the founda-
tions of the subject has continued as well. It is our purpose in writing this 
book to provide an introduction to the various aspects of theoretical 
computer science for undergraduate and graduate students that is suffi-
ciently comprehensive that the professional literature of treatises and 
research papers will become accessible to our readers. 
We are dealing with a very young field that is still finding itself. 
Computer scientists have by no means been unanimous in judging which 
xiii 

xiv 
Preface 
parts of the subject will turn out to have enduring significance. In this 
situation, fraught with peril for authors, we have attempted to select topics 
that have already achieved a polished classic form, and that we believe will 
play an important role in future research. 
In this second edition, we have included new material on the subject of 
programming language semantics, which we believe to be established as an 
important topic in theoretical computer science. Some of the material on 
computability theory that had been scattered in the first edition has been 
brought together, and a few topics that were deemed to be of only 
peripheral interest to our intended audience have been eliminated. Nu-
merous exercises have also been added. We were particularly pleased to be 
able to include the answer to a question that had to be listed as open in 
the first edition. Namely, we present Neil Immerman's surprisingly 
straightforward proof of the fact that the class of languages accepted by 
linear bounded automata is closed under complementation. 
We have assumed that many of our readers will have had little experi-
ence with mathematical proof, but that almost all of them have had 
substantial programming experience. Thus the first chapter contains an 
introduction to the use of proofs in mathematics in addition to the usual 
explanation of terminology and notation. We then proceed to take advan-
tage of the reader's background by developing computability theory in the 
context of an extremely simple abstract programming language. By system-
atic use of a macro expansion technique, the surprising power of the 
language is demonstrated. This culminates in a universal program, which is 
written in all detail on a single page. By a series of simulations, we then 
obtain the equivalence of various different formulations of computability, 
including Turing's. Our point of view with respect to these simulations is 
that it should not be the reader's responsibility, at this stage, to fill in the 
details of vaguely sketched arguments, but rather that it is our responsibil-
ity as authors to arrange matters so that the simulations can be exhibited 
simply, clearly, and completely. 
This material, in various preliminary forms, has been used with under-
graduate and graduate students at New York University, Brooklyn College, 
The Scuola Matematica lnteruniversitaria-Perugia, The University of Cal-
ifornia-Berkeley, The University of California-Santa Barbara, Worcester 
Polytechnic Institute, and Yale University. 
Although it has been our practice to cover the material from the second 
part of the book on formal languages after the first part, the chapters on 
regular and on context-free languages can be read immediately after 
Chapter 1. The Chomsky-Schiitzenberger representation theorem for con-
text-free languages in used to develop their relation to pushdown au-
tomata in a way that we believe is clarifying. Part 3 is an exposition of the 
aspects of logic that we think are important for computer science and can 

Preface 
XV 
also be read immediately following Chapter 1. Each of the chapters of Part 
4 introduces an important theory of computational complexity, concluding 
with the theory of NP-completeness. Part 5, which is new to the second 
edition, uses recursion equations to expand upon the notion of computabil-
ity developed in Part 1, with an emphasis on the techniques of formal 
semantics, both denotational and operational. Rooted in the early work of 
Godel, Herbrand, Kleene, and others, Part 5 introduces ideas from the 
modern fields of functional programming languages, denotational seman-
tics, and term rewriting systems. 
Because many of the chapters are independent of one another, this book 
can be used in various ways. There is more than enough material for a 
full-year course at the graduate level on theory of computation. We have 
used the unstarred sections of Chapters 1-6 and Chapter 9 in a successful 
one-semester junior-level course, Introduction to Theory of Computation, 
at New York University. A course on finite automata and forma/languages 
could be based on Chapters 1, 9, and 10. A semester or quarter course on 
logic for computer scientists could be based on selections from Parts 1 and 
3. Part 5 could be used for a third semester on the theory of computation 
or an introduction to programming language semantics. Many other ar-
rangements and courses are possible, as should be apparent from the 
dependency graph, which follows the Acknowledgments. It is our hope, 
however, that this book will help readers to see theoretical computer 
science not as a fragmented list of discrete topics, but rather as a unified 
subject drawing on powerful mathematical methods and on intuitions 
derived from experience with computing technology to give valuable in-
sights into a vital new area of human knowledge. 
Note to the Reader 
Many readers will wish to begin with Chapter 2, using the material of 
Chapter 1 for reference as required. Readers who enjoy skipping around 
will find the dependency graph useful. 
Sections marked with an asterisk (*) may be skipped without loss of 
continuity. The relationship of these sections to later material is given in 
the dependency graph. 
Exercises marked with an asterisk either introduce new material, refer 
to earlier material in ways not indicated in the dependency graph, or 
simply are considered more difficult than unmarked exercises. 
A reference to Theorem 8.1 is to Theorem 8.1 of the chapter in which 
the reference is made. When a reference is to a theorem in another 
chapter, the chapter is specified. The same system is used in referring to 
numbered formulas and to exercises. 


Acknowledgments 
It is a pleasure to acknowledge the help we have received. Charlene 
Herring, Debbie Herring, Barry Jacobs, and Joseph Miller made their 
student classroom notes available to us. James Cox, Keith Harrow, Steve 
Henkind, Karen Lemone, Colm O'Dunlaing, and James Robinett provided 
helpful comments and corrections. Stewart Weiss was kind enough to 
redraw one of the figures. Thomas Ostrand, Norman Shulman, Louis 
Salkind, Ron Sigal, Patricia Teller, and Elia Weixelbaum were particularly 
generous with their time, and devoted many hours to helping us. We are 
especially grateful to them. 
Acknowledgments to Corrected Printing 
We have taken this opportunity to correct a number of errors. We are 
grateful to the readers who have called our attention to errors and who 
have suggested corrections. The following have been particularly helpful: 
Alissa Bernholc, Domenico Cantone, John R. Cowles, Herbert Enderton, 
Phyllis Frankl, Fred Green, Warren Hirsch, J. D. Monk, Steve Rozen, and 
Stewart Weiss. 
xvii 

xviii 
Acknowledgments 
Acknowledgments to Second Edition 
Yuri Gurevich, Paliath Narendran, Robert Paige, Carl Smith, and particu-
larly Robert McNaughton made numerous suggestions for improving the 
first edition. Kung Chen, William Hurwood, Dana Latch, Sidd Puri, 
Benjamin Russell, Jason Smith, Jean Toal, and Niping Wu read a prelimi-
nary version of Part 5. 

Dependency Graph 
Chapter 15 
Polynombll-Time 
Computability 
Chapter 13 
a.-tlftc.tlon Theory 
A solid line between two chapters indicates the dependence of the un-
starred sections of the higher numbered chapter on the unstarred sections 
of the lower numbered chapter. An asterisk next to a solid line indicates 
that knowledge of the starred sections of the lower numbered chapter is 
also assumed. A dotted line shows that knowledge of the unstarred 
sections of the lower numbered chapter is assumed for the starred sections 
of the higher numbered chapter. 
xix 


1 
Preliminaries 
1. Sets and n-tuples 
We shall often be dealing with sets of objects of some definite kind. 
Thinking of a collection of entities as a set simply amounts to a decision to 
regard the whole collection as a single object. We shall use the word class 
as synonymous with set. In particular we write N for the set of natural 
numbers 0, 1, 2, 3,. . . . In this book the word number will always mean 
natural number except in contexts where the contrary is explicitly stated. 
We write 
aES 
to mean that a belongs to S or, equivalently, is a member of the set S, and 
aftS 
to mean that a does not belong to S. It is useful to speak of the empty set, 
written 0, which has no members. The equation R = S, where R and S 
are sets, means that R and S are identical as sets, that is, that they have 
exactly the same members. We write R ~Sand speak of Rasa subset of 
S to mean that every element of R is also an element of S. Thus, R = S if 
and only if R ~ S and S ~ R. Note also that for any set R, 0 ~ R and 
R ~ R. We write R c S to indicate that R ~ S but R =/= S. In this case R 
1 

2 
Chapter 1 Preliminaries 
is called a proper subset of S. If R and S are sets, we write R uS for the 
union of R and S, which is the collection of all objects which are members 
of either R or S or both. R n S, the intersection of R and S, is the set of 
all objects that belong to both R and S. R - S, the set of all objects that 
belong to R and do not belong to S, is the difference between R and S. S 
may contain objects not in R. Thus R - S = R- (R n S). Often we will 
be working in contexts where all sets being considered are subsets of some 
fixed set D (sometimes called a domain or a universe). In such a case we 
write S for D - S, and call S the complement of S. Most frequently we 
shall be writing S for N- S. The De Morgan identities 
R us= R. n s, 
RnS=RuS 
are very useful; they are easy to check and any reader not already familiar 
with them should do so. We write 
for the set cons1stmg of the n objects a1 , a 2 , â¢â¢â¢ , an. Sets that can be 
written in this form as well as the empty set are called finite. Sets that are 
not finite, e.g., N, are called infinite. It should be carefully noted that a 
and {a} are not the same thing. In particular, a E S is true if and only if 
{a} ~ S. Since two sets are equal if and only if they have the same 
members, it follows that, for example, {a, b, c} = {a, c, b} = {b, a, c}. That 
is, the order in which we may choose to write the members of a set is 
irrelevant. Where order is important, we speak instead of an n-tuple or a 
list. We write n-tuples using parentheses rather than curly braces: 
(al , ... ,an). 
Naturally, the elements making up an n-tuple need not be distinct. Thus 
(4, 1, 4, 2) is a 4-tuple. A 2-tuple is called an ordered pair, and a 3-tuple is 
called an ordered triple. Unlike the case for sets of one object, we do not 
distinguish between the object a and the l-tuple (a). The crucial property of 
n-tuples is 
if and only if 
... , 
and 
If S 1 , S2 , .â¢â¢ , Sn are given sets, then we write S 1 X S2 X Â· Â· Â· X Sn for the 
set of all n-tuples (a I' az' ... ' an) such that al E sl' az E Sz' ... ' an E sn. 

2. Functions 
3 
S1 X S2 X â¢â¢Â· X Sn 
is sometimes called the Cartesian product of 
S1 , S2 , â¢â¢â¢ , Sn. In case S1 = S2 = 
= Sn = S we write sn for the Carte-
sian product S1 X S2 X Â· Â· Â· X Sn. 
2. 
Functions 
Functions play an important role in virtually every branch of pure and 
applied mathematics. We may define a function simply as a set f, all of 
whose members are ordered pairs and that has the special property 
(a, b) E f and (a, c) E f 
implies 
b = c. 
However, intuitively it is more helpful to think of the pairs listed as the 
rows of a table. For f a function, one writes f(a) = b to mean that 
(a, b) E f; the definition of function ensures that for each a there can be 
at most one such b. The set of all a such that (a, b) E f for some b is 
called the domain of f. The set of all f(a) for a in the domain of f is 
called the range of f. 
As an example, let f be the set of ordered pairs (n, n2 ) for n EN. 
Then, for each n EN, f(n) = n2â¢ The domain off is N. The range off is 
the set of perfect squares. 
Functions f are often specified by algorithms that provide procedures 
for obtaining f(a) from a. This method of specifying functions is particu-
larly important in computer science. However, as we shall see in Chapter 
4, it is quite possible to possess an algorithm that specifies a function 
without being able to tell which elements belong to its domain. This makes 
the notion of a so-called partial function play a central role in computabil-
ity theory. A partial function on a set S is simply a function whose domain 
is a subset of S. An example of a partial function on N is given by g(n) 
= In, where the domain of g is the set of perfect squares. If f is a partial 
function on S and a E S, then we write f(a)J, and say that f(a) is defined 
to indicate that a is in the domain of f; if a is not in the domain of f, we 
write f(a)j and say that f(a) is undefined. If a partial function on S has 
the domain S, then it is called total. Finally, we should mention that the 
empty set 0 is itself a function. Considered as a partial function on some 
set S, it is nowhere defined. 
For a partial function f on a Cartesian product S1 X S2 X Â·Â·Â· X Sn, we 
write f(a 1 , â¢â¢â¢ , an) rather than f((a 1 , â¢â¢â¢ , an)). A partial function f on a 
set sn is called an n-ary partial function on S, or a function of n variables 
on S. We use unary and binary for 1-ary and 2-ary, respectively. For n-ary 
partial functions, we often write f(x 1 , â¢â¢â¢ , xn) instead of f as a way of 
showing explicitly that f is n-ary. 

4 
Chapter 1 Preliminaries 
Sometimes it is useful to work with particular kinds of functions. A 
function f is one-one if, for all x, y in the domain of J, f(x) = f(y) 
implies x = y. Stated differently, if x =/= y then f(x) =/= f(y). If the range of 
f is the set S, then we say that f is an onto function with respect to S, or 
simply that f is onto S. For example, f(n) = n2 is one-one, and f is onto 
the set of perfect squares, but it is not onto N. 
We will sometimes refer to the idea of closure. If S is a set and f is a 
partial function on S, then S is closed under f if the range of f is a subset 
of S. For example, N is closed under f(n) = nZ, but it is not closed under 
h(n) = ..;n (where h is a total function on N). 
3. Alphabets and Strings 
An alphabet is simply some finite nonempty set A of objects called 
symbols. An n-tuple of symbols of A is called a word or a string on A. 
Instead of writing a word as (a 1 , a2 , â¢â¢â¢ , an) we write simply a1 a2 â¢ â¢â¢ an. If 
u = a1a2 â¢â¢â¢ an, then we say that n is the length of u and write lui = n. 
We allow a unique null word, written 0, of length 0. (The reason for using 
the same symbol for the number zero and the null word will become clear 
in Chapter 5.) The set of all words on the alphabet A is written A*. Any 
subset of A* is called a language on A or a language with alphabet A. We 
do not distinguish between a symbol a E A and the ~d of length 1 
consisting of that symbol. If u, v E A*, then we write u v for the word 
obtained by placing the string v after the string u. For example, if 
A = {a, b, c}, u = bab, and v = caa, then 
uv = babcaa 
and 
vu = caabab. 
Where no confusion can result, we write uv instead of ;;u. It is obvious 
that, for all u, 
uO = Ou = u, 
and that, for all u, v, w, 
u(vw) = (uv)w. 
Also, if either uv = uw or vu = wu, then v = w. 
If u is a string, and n E N, n > 0, we write 
ulnJ = uu ... u. 
-------
n 
We also write ul01 = 0. We use the square brackets to avoid confusion with 
numerical exponentiation. 

4. Predicates 
5 
If u E A*, we write uR for u written backward; i.e., if u = a 1a2 â¢â¢â¢ an, 
for al' ... ' an E A, then uR =an ... azat. Clearly, oR = 0 and (uv)R = 
vRuR for u, v E A*. 
4. 
Predicates 
By a predicate or a Boolean-valued function on a set S we mean a total 
function P on S such that for each a E S, either 
P(a) =TRUE 
or 
P(a) = FALSE, 
where TRUE and FALSE are a pair of distinct objects called truth values. 
We often say P(a) is true for P(a) =TRUE, and P(a) is false for 
P(a) = FALSE. For our purposes it is useful to identify the truth values 
with specific numbers, so we set 
TRUE= 1 
and 
FALSE= 0. 
Thus, a predicate is a special kind of function with values in N. Predicates 
on a set S are usually specified by expressions which become statements, 
either true or false, when variables in the expression are replaced by 
symbols designating fixed elements of S. Thus the expression 
x<S 
specifies a predicate on N, namely, 
P(x) = {~ 
if x=0,1,2,3,4 
otherwise. 
Three basic operations on truth values are defined by the tables in Table 
4.1. Thus if P and Q are predicates on a set S, there are also the 
predicates -P, P & Q, P v Q. -Pis true just when Pis false; P & Q is 
true when both P and Q are true, otherwise it is false; P v Q is true when 
either P or Q or both are true, otherwise it is false. Given a predicate P 
Table 4.1 
p 
-p 
p 
q 
p&q 
pVq 
0 
1 
1 
1 
0 
0 
1 
0 
1 
0 
0 
1 
0 
0 
0 
0 

6 
Chapter 1 Preliminaries 
on a set S, there is a corresponding subset R of S, namely, the set of all 
elements a E S for which P(a) = 1. We write 
R ={a E SIP(a)}. 
Conversely, given a subset R of a given set S, the expression 
xER 
defines a predicate on S, namely, the predicate defined by 
Of course, in this case, 
P(x) = {~ 
if 
X E R 
if 
X ft. R. 
R = {x E SIP(x)}. 
The predicate P is called the characteristic function of the set R. The close 
connection between sets and predicates is such that one can readily 
translate back and forth between discourse involving one of these notions 
and discourse involving the other. Thus we have 
{x E s I P(x) & Q(x)} = {x E s I P(x)} n {xEs I Q(x)}, 
{xES I P(x) v Q(x)} ={xES I P(x)} u {xES I Q(x)}, 
{xES I -P(x)} = S- {xES I P(x)}. 
To indicate that two expressions containing variables define the same 
predicate we place the symbol = between them. Thus, 
X < 5 =X = 0 V X = 1 V X = 2 V X = 3 V X = 4. 
The De Morgan identities from Section 1 can be expressed as follows in 
terms of predicates on a set S: 
P(x) & Q(x) =- (- P(x) v - Q(x)), 
P(x) v Q(x) =- (- P(x) & - Q(x)). 
5. 
Quantifiers 
In this section we will be concerned exclusively with predicates on Nm (or 
what is the same thing, m-ary predicates on N) for different values of m. 
Here and later we omit the phrase "on N" when the meaning is clear. 

5. Quantifiers 
7 
Thus, let P(t, x 1 , â¢â¢â¢ , xn) be an (n + 1)-ary predicate. Consider the predi-
cate Q(y, x1 , â¢â¢â¢ , xn) defined by 
Q(y,x 1 , â¢â¢â¢ ,xn) -P(O,x 1 , â¢â¢â¢ ,xn) V P(l,x 1 , â¢â¢â¢ ,xn) 
V Â·Â·Â· V P(y,x 1 , â¢â¢â¢ ,xn). 
Thus the predicate Q(y, x1 , â¢â¢â¢ , xn) is true just in case there is a value of 
t ~ y such that P(t, x1 , â¢â¢â¢ , xn) is true. We write this predicate Q as 
The expression "(3 t), y" is called a bounded existential quantifier. Similarly, 
we write (Vt), YP{t, x1 , â¢â¢â¢ , xn) for the predicate 
P(O, XI' â¢â¢â¢ ' xn) & P(l, XI' â¢â¢â¢ ' xn) & ... & P(y, XI' .â¢â¢ ' xn). 
This predicate is true just in case P(t, x1 , â¢â¢â¢ , xn) is true for all t ~ y. 
The expression "(Vt), y" is called a bounded universal quantifier. We also 
write (3t) < YP(t, x1 , â¢â¢â¢ , xn) for the predicate that is true just in 
case P(t, x1 , â¢â¢â¢ , xn) is true for at least one value of t < y and 
(V t) < Y P(t, x 1 , â¢â¢â¢ , x n) for the predicate that is true just in case 
P(t, x 1 , â¢â¢â¢ , xn) is true for all values oft < y. 
We write 
for the predicate which is true if there exists some t E N for which 
P(t, XI' â¢â¢â¢ ' xn) is true. Similarly, (Vt)P(t, XI' â¢â¢â¢ ' xn) is true if 
P(t, XI' â¢â¢â¢ ' xn) is true for all t EN. 
The following generalized De Morgan identities are sometimes useful: 
-(3t),YP(t,x1 , â¢â¢â¢ ,xn)- (Vt),Y -P(t,XpÂ·Â·Â·â¢xn), 
-(3t)P(t,x1 , â¢â¢â¢ ,xn)- (Vt) -P(t,x1 , â¢â¢â¢ ,xn). 
The reader may easily verify the following examples: 
(3y)(x + y = 4) -
x ~ 4, 
(3y)(x + y = 4)- (3y), 4(x + y = 4), 
(Vy){xy = 0) -X= 0, 
(3y),z(X + y = 4)- (x + z ~ 4& X~ 4). 

8 
Chapter 1 Preliminaries 
6. 
Proof by Contradiction 
In this book we will be calling many of the assertions we make theorems 
(or corollaries or lemmas) and providing proofs that they are correct. Why 
are proofs necessary? The following example should help in answering this 
question. 
Recall that a number is called a prime if it has exactly two distinct 
divisors, itself and 1. Thus 2, 17, and 41 are primes, but 0, 1, 4, and 15 are 
not. Consider the following assertion: 
n2 - n + 41 is prime for all n EN. 
This assertion is in fact false. Namely, for n = 41 the expression becomes 
412 -
41 + 41 = 412 ' 
which is certainly not a prime. However, the assertion is true (readers with 
access to a computer can easily check this!) for all n ~ 40. This example 
shows that inferring a result about all members of an infinite set (such as 
N) from even a large finite number of instances can be very dangerous. A 
proof is intended to overcome this obstacle. 
A proof begins with some initial statements and uses logical reasoning to 
infer additional statements. (In Chapters 12 and 13 we shall see how the 
notion of logical reasoning can be made precise; but in fact, our use of 
logical reasoning will be in an informal intuitive style.) When the initial 
statements with which a proof begins are already accepted as correct, then 
any of the additional statements inferred can also be accepted as correct. 
But proofs often cannot be carried out in this simple-minded pattern. In 
this and the next section we will discuss more complex proof patterns. 
In a proof by contradiction, one begins by supposing that the assertion 
we wish to prove is false. Then we can feel free to use the negation of what 
we are trying to prove as one of the initial statements in constructing a 
proof. In a proof by contradiction we look for a pair of statements 
developed in the course of the proof which contradict one another. Since 
both cannot be true, we have to conclude that our original supposition was 
wrong and therefore that our desired conclusion is correct. 
We give two examples here of proof by contradiction. There will be 
many in the course of the book. Our first example is quite famous. We 
recall that every number is either even (i.e., = 2n for some n E N) or odd 
(i.e., = 2n + 1 for some n EN). Moreover, if m is even, m = 2n, then 
m 2 = 4n2 = 2 Â· 2n 2 is even, while if m is odd, m = 2n + 1, then m 2 = 
4n 2 + 4n + 1 = 2(2n2 + 2n) + 1 is odd. We wish to prove that the 
equation 
2 = (mjn) 2 
(6.1) 

7. Mathematical Induction 
9 
has no solution for m, n EN (that is, that fi is not a "rational" number). 
We suppose that our equation has a solution and proceed to derive a 
contradiction. Given our supposition that (6.1) has a solution, it must have 
a solution in which m and n are not both even numbers. This is true 
because if m and n are both even, we can repeatedly "cancel" 2 from 
numerator and denominator until at least one of them is odd. On the 
other hand, we shall prove that for every solution of (6.1) m and n must 
both be even. The contradiction will show that our supposition was false, 
i.e., that (6.1) has no solution. 
It remains to show that in every solution of (6.1), m and n are both 
even. We can rewrite (6.1) as 
m 2 = 2n2 , 
which shows that m2 is even. As we saw above this implies that m is even, 
say m = 2k. Thus, m2 = 4k 2 = 2n 2, or n 2 = 2k 2â¢ Thus, n 2 is even and 
hence n is even. 
â¢ 
Note the symbol â¢, which means "the proof is now complete." 
Our second example involves strings as discussed in Section 3. 
Theorem 6.1. Let x E {a, b}* such that xa = ax. Then x = a[nJ for some 
n EN. 
Proof. Suppose that xa = ax but x contains the letter b. Then we can 
write x = a[nlbu, where we have explicitly shown the first (i.e., leftmost) 
occurrence of b in x. Then 
Thus, 
bua = abu. 
But this is impossible, since the same string cannot have its first symbol be 
both b and a. This contradiction proves the theorem. 
â¢ 
Exercises 
1. 
Prove that the equation ( p 1 q )2 = 3 has no solution for p, q E N. 
2. 
Prove that if x E {a, b}* and abx = xab, then x = (ab)[nJ for some 
n EN. 
7. 
Mathematicallnduction 
Mathematical induction furnishes an important technique for proving 
statements of the form (Vn)P(n), where P is a predicate on N. One 

10 
Chapter 1 Preliminaries 
proceeds by proving a pair of auxiliary statements, namely, 
P(O) 
and 
(Vn)(/f P(n) then P(n + 1)). 
(7.1) 
Once we have succeeded in proving these auxiliary statements we can 
regard (Vn)P(n) as also proved. The justification for this is as follows. 
From the second auxiliary statement we can infer each of the infinite set 
of statements: 
If P(O) then P(l), 
If P(l) then P(2), 
If P(2) then P(3), .... 
Since we have proved P(O), we can infer P(l). Having now proven P(l) we 
can get P(2), etc. Thus, we see that P(n) is true for all n and hence 
(Vn)P(n) is true. 
Why is this helpful? Because sometimes it is much easier to prove (7.1) 
than to prove (Vn)P(n) in some other way. In proving this second auxiliary 
proposition one typically considers some fixed but arbitrary value k of n 
and shows that if we assume P(k) we can prove P(k + 1). P(/.,) is then 
called the induction hypothesis. This methodology enables us to use P(k) as 
one of the initial statements in the proof we are constructing. 
There are some paradoxical things about proofs by mathematical induc-
tion. One is that considered superficially, it seems like an example of 
circular reasoning. One seems to be assuming P(k) for an arbitrary k, 
which is exactly what one is supposed to be engaged in proving. Of course, 
one is not really assuming (Vn)P(n). One is assuming P(k) for some 
particular k in order to show that P(k + 1) follows. 
It is also paradoxical that in using induction (we shall often omit the 
word mathematical), it is sometimes easier to prove statements by first 
making them "stronger." We can put this schematically as follows. We 
wish to prove (Vn)P(n). Instead we decide to prove the stronger assertion 
(VnXP(n) & Q(n)) (which of course implies the original statement). Prov-
ing the stronger statement by induction requires that we prove 
P(O) & Q(O) 
and 
(Vn)[If P(n) & Q(n) then P(n + 1) & Q(n + 1)]. 
In proving this second auxiliary statement, we may take P(k)& Q(k) as 
our induction hypothesis. Thus, although strengthening the statement to 

7. Mathematical Induction 
11 
be proved gives us more to prove, it also gives us a stronger induction 
hypothesis and, therefore, more to work with. The technique of deliber-
ately strengthening what is to be proven for the purpose of making proofs 
by induction easier is called induction loading. 
It is time for an example of a proof by induction. The following is useful 
in doing one of the exercises in Chapter 6. 
Theorem 7.1. For all n EN we have L/~ 0(2i + 1) = (n + 1)2â¢ 
Proof. For n = 0, our theorem states simply that 1 = 12, which is true. 
Suppose the result known for n = k. That is, our induction hypothesis is 
Then 
k E (2i + 1) = <k + 1/. 
i~O 
k+ I 
k 
E (2i + 1) = E (2i + o + 2(k + 1) + 1 
i~O 
i~O 
= (k + 1)2 + 2(k + 1) + 1 
= (k + 2)2â¢ 
But this is the desired result for n = k + 1. 
â¢ 
Another form of mathematical induction that is often very useful is 
called course-of-values induction or sometimes complete induction. In the 
case of course-of-values induction we prove the single auxiliary statement 
('Vn)[Jf (Vm)m < nP(m) then P(n)], 
(7.2) 
and then conclude that (Vn)P(n) is true. A potentially confusing aspect of 
course-of-values induction is the apparent lack of an initial statement 
P(O). But in fact there is no such lack. The case n = 0 of (7.2) is 
If ('Vm)m < 0 P(m) then P(O). 
But the "induction hypothesis" (Vm)m < 0P(m) is entirely vacuous because 
there is nom EN such that m < 0. So in proving (7.2) for n = 0 we really 
are just proving P(O). In practice it is sometimes possible to give a single 
proof of (7.2) that works for all n including n = 0. But often the case 
n = 0 has to be handled separately. 
To see why course-of-values induction works, consider that, in the light 
of what we have said about the n = 0 case, (7.2) leads to the following 

12 
Chapter 1 Preliminarâ¢s 
infinite set of statements: 
P(O), 
If P(O) then P(l), 
If P(O) & P(l) then P(2), 
If P(O) & P(l) & P(2) then P(3), 
Here is an example of a theorem proved by course-of-values induction. 
Theorem 7.2. There is no string x E {a, b}* such that ax= xb. 
Proof. 
Consider the following predicate: If x E {a, b}* and lxl = n, then 
ax =/= xb. We will show that this is true for all n E N. So we assume it true 
for all m < k for some given k and show that it follows for k. This proof 
will be by contradiction. Thus, suppose that lxl = k and ax= xb. The 
equation implies that a is the first and b the last symbol in x. So, we can 
write x = aub. Then 
aaub = aubb, 
i.e., 
au= ub. 
But lui < lxl. Hence by the induction hypothesis au =/= ub. This contradic-
tion proves the theorem. 
â¢ 
Proofs by course-of-values induction can always be rewritten so as to 
involve reference to the principle that if some predicate is true for some 
element of N, then there must be a least element of N for which it is true. 
Here is the proof of Theorem 7.2 given in this style. 
Proof. Suppose there is a string x E {a, b}* such that ax = xb. Then 
there must be a string satisfying this equation of minimum length. Let x 
be such a string. Then ax= xb, but, if lui < lxl, then au =/= ub. However, 
ax= xb implies that x = aub, so that au = ub and lui < lxl. This contra-
diction proves the theorem. 
â¢ 
Exercises 
1. Prove by mathematical induction that E7~ 1 i = n(n + 1)/2. 
2. Here is a "proof' by mathematical induction that if x, y EN, then 
x = y. What is wrong? 

7. Mathematical Induction 
Let 
max( x, y) = {; 
for x, y E N. Consider the predicate 
if X ~y 
otherwise 
(Vx)(Vy)[/f max(x, y) = n, thenx = y]. 
13 
For n = 0, this is clearly true. Assume the result for n = k, and let 
max(x, y) = k + 1. Let x 1 = x- 1, y 1 = y- 1. Then max(x1 , y1) = k. 
By the induction hypothesis, x 1 = y 1 and therefore x = x 1 + 1 = 
Y1 + 1 = YÂ· 
3. HereÂ· is another incorrect proof that purports to use mathematical 
induction to prove that all flowers have the same color! What is 
wrong? 
Consider the following predicate: If S is a set of flowers containing 
exactly n elements, then all the flowers in S have the same color. The 
predicate is clearly true if n = 1. We suppose it true for n = k and 
prove the result for n = k + 1. Thus, let S be a set of k + 1 flowers. If 
we remove one flower from S we get a set of k flowers. Therefore, by 
the induction hypothesis they all have the same color. Now return the 
flower removed from S and remove another. Again by our induction 
hypothesis the remaining flowers all have the same color. But now 
both of the flowers removed have been shown to have the same color 
as the rest. Thus, all the flowers in S have the same color. 
4. Show that there are no strings x, y E {a, b}* such that xay = ybx. 
5. Give a "one-line" proof of Theorem 7.2 that does not use mathemati-
cal induction. 


Part 1 
Computability 


2 
Programs and 
Computable Functions 
1. A Programming Language 
Our development of computability theory will be based on a specific 
programming language ..:7. We will use certain letters as variables whose 
values are numbers. (In this book the word number will always mean 
nonnegative integer, unless the contrary is specifically stated.) In particu-
lar, the letters 
XI Xz X3 ... 
will be called the input variables of ..:7, the letter Y will be called the 
output variable of ..:7, and the letters 
ZI Zz z3 
will be called the local variables of ..:7. The subscript 1 is often omitted; i.e., 
X stands for X 1 and Z for Z 1â¢ Unlike the programming languages in 
actual use, there is no upper limit on the values these variables can 
assume. Thus from the outset, ..:7 must be regarded as a purely theoretical 
entity. Nevertheless, readers having programming experience will find 
working with ..:7 very easy. 
In ..:7 we will be able to write "instructions" of various sorts; a 
"program" of ..:7 will then consist of a list (i.e., a finite sequence) of 
17 

18 
Instruction 
V+- V+ 1 
V+- V-I 
Chapter 2 Programs and Computable Functions 
Table 1.1 
Interpretation 
Increase by I the value of the variable V. 
If the value of V is 0, leave it unchanged; otherwise decrease by I the 
value of V. 
IF V * 0 GOTO L If the value of V is nonzero, perform the instruction with label L next; 
otherwise proceed to the next instruction in the list. 
instructions. For example, for each variable V there will be an instruction: 
V+--V+1 
A simple example of a program of .9' is 
X+--X+ 1 
X+--X+1 
"Execution" of this program has the effect of increasing the value of X by 
2. In addition to variables, we will need "labels." In .9' these are 
AI Bl CI DI El Az Bz Cz Dz Ez A3 .... 
Once again the subscript 1 can be omitted. We give in Table 1.1 a 
complete list of our instructions. In this list V stands for any variable and 
L stands for any label. 
These instructions will be called the increment, decrement, and condi-
tional branch instructions, respectively. 
We will use the special convention that the output variable Y and the 
local variables Z; initially have the value 0. We will sometimes indicate the 
value of a variable by writing it in lowercase italics. Thus x 5 is the value of 
Xs. 
Instructions may or may not have labels. When an instruction is labeled, 
the label is written to its left in square brackets. For example, 
[B] 
Z+--Z-1 
In order to base computability theory on the language .9', we will 
require formal definitions. But before we supply these, it is instructive to 
work informally with programs of .9'. 
2. 
Some Examples of Programs 
(a) Our first example is the program 
[A] 
X+--X-1 
Y+--Y+1 
IF X =/= 0 GOTO A 

2. Some Examples of Programs 
19 
If the initial value x of X is not 0, the effect of this program is to copy x 
into Y and to decrement the value of X down to 0. (By our conventions 
the initial value of Y is 0.) If x = 0, then the program halts with Y having 
the value 1. We will say that this program computes the function 
f(x) = {~ 
if 
X= 0 
otherwise. 
This program halts when it executes the third instruction of the program 
with X having the value 0. In this case the condition X -=!= 0 is not fulfilled 
and therefore the branch is not taken. When an attempt is made to move 
on to the nonexistent fourth instruction, the program halts. A program will 
also halt if an instruction labeled L is to be executed, but there is no 
instruction in the program with that label. In this case, we usually will use 
the letter E (for "exit") as the label which labels no instruction. 
(b) Although the preceding program is a perfectly well-defined pro-
gram of our language .9', we may think of it as having arisen in an attempt 
to write a program that copies the value of X into Y, and therefore 
containing a "bug" because it does not handle 0 correctly. The following 
slightly more complicated example remedies this situation. 
[A] 
IF X-=!= 0 GOTO B 
Z+-Z+1 
IF Z -=1= 0 GOTO E 
[B] 
X+-- X- 1 
Y+-Y+1 
Z+-Z+1 
IF Z-=!= OGOTOA 
As we can easily convince ourselves, this program does copy the value of 
X into Y for all initial values of X. Thus, we say that it computes the 
function f(x) = x. At first glance.Z's role in the computation may not be 
obvious. It is used simply to allow us to code an unconditional branch. That 
is, the program segment 
Z+-Z+1 
IF Z-=!= 0 GOTO L 
(2.1) 
has the effect (ignoring the effect on the value of Z) of an instruction 
GOTOL 
such as is available in most programming languages. To see that this is true 
we note that the first instruction of the segment guarantees that Z has a 
nonzero value. Thus the condition Z -=!= 0 is always true and hence the next 
instruction performed will be the instruction labeled L. Now GOTO L is 

20 
Chapter 2 Programs and Computable Functions 
not an instruction in our language .9", but since we will frequently have use 
for such an instruction, we can use it as an abbreviation for the program 
segment (2.1). Such an abbreviating pseudoinstruction will be called a 
macro and the program or program segment which it abbreviates will be 
called its macro expansion. 
The use of these terms is obviously motivated by similarities with the 
notion of a macro instruction occurring in many programming languages. 
At this point we will not discuss how to ensure that the variables local to 
the macro definition are distinct from the variables used in the main 
program. Instead, we will manually replace any such duplicate variable 
uses with unused variables. This will be illustrated in the "expanded" 
multiplication program in (e). In Section 5 this matter will be dealt with in 
a formal manner. 
(c) Note that although the program of (b) does copy the value of X 
into Y, in the process the value of X is "destroyed" and the program 
terminates with X having the value 0. Of course, typically, programmers 
want to be able to copy the value of one variable into another without the 
original being "zeroed out." This is accomplished in the next program. 
(Note that we use our macro instruction GOTO L several times to shorten 
the program. Of course, if challenged, we could produce a legal program of 
.9" by replacing each GOTO L by a macro expansion. These macro 
expansions would have to use a local variable other than Z so as not to 
interfere with the value of Z in the main program.) 
[A] 
If X -=F 0 GOTO B 
GOTOC 
[B] 
X+-- X- 1 
Y+-Y+1 
Z+-Z+1 
GOTOA 
[C] 
IF Z -=F 0 GOTO D 
GOTOE 
[D] 
Z +-- Z- 1 
X+-X+ 1 
GOTOC 
In the first loop, this program copies the value of X into both Y and Z, 
while in the second loop, the value of X is restored. When the program 
terminates, both X and Y contain X's original value and z = 0. 
We wish to use this program to justify the introduction of a macro which 
we will write 
V+- V' 

2. Some Examples of Programs 
21 
the execution of which will replace the contents of the variable V by the 
contents of the variable V' while leaving the contents of V' unaltered. 
Now, this program (c) functions correctly as a copying program only under 
our assumption that the variables Y and Z are initialized to the value 0. 
Thus, we can use the program as the basis of a macro expansion of 
V +--- V' only if we can arrange matters so as to be sure that the corre-
sponding variables have the value 0 whenever the macro expansion is 
entered. To solve this problem we introduce the macro 
V+---0 
which will have the effect of setting the contents of V equal to 0. The 
corresponding macro expansion is simply 
[L] 
V +--- V- 1 
IF V =F 0 GOTO L 
where, of course, the label L is to be chosen to be different from any of 
the labels in the main program. We can now write the macro expansion of 
V +--- V' by letting the macro V +--- 0 precede the program which results 
when X is replaced by V' and Y is replaced by V in program (c). The 
result is as follows: 
V+---0 
[A] 
IF V' =F 0 GOTO B 
GOTOC 
[B] 
V' +--- V' - 1 
V+-V+1 
Z+-Z+1 
GOTOA 
[C] 
IF Z =F 0 GOTO D 
GOTOE 
[D] 
Z +--- Z- 1 
V' +--- V' + 1 
GOTOC 
With respect to this macro expansion the following should be noted: 
1. It is unnecessary (although of course it would be harmless) to include 
a Z +--- 0 macro at the beginning of the expansion because, as has 
already been remarked, program (c) terminates with z = 0. 
2. When inserting the expansion in an actual program, the variable Z 
will have to be replaced by a local variable which does not occur in 
the main program. 

22 
Chapter 2 Programs and Computable Functions 
3. Likewise the labels A, B, C, D will have to be replaced by labels 
which do not occur in the main program. 
4. Finally, the label E in the macro expansion must be replaced by a 
label L such that the instruction which follows the macro in the main 
program (if there is one) begins [L]. 
(d) A program with two inputs that computes the function 
is as follows: 
f(xi ,xz) =xi +xz 
Y+-XI 
Z +-X2 
[ B] 
IF Z =F 0 GOTO A 
GOTOE 
[A] 
Z +-- Z- 1 
Y+-Y+1 
GOTOB 
Again, if challenged we would supply macro expansions for "Y +-- XI" 
and "Z +-- X2" as well as for the two unconditional branches. Note that Z 
is used to preserve the value of X2 â¢ 
(e) We now present a program that multiplies, i.e. that computes 
f(xpx 2 ) =xi Â·x2 â¢ Since multiplication can be regarded as repeated addi-
tion, we are led to the "program" 
Zz +-- Xz 
[B] 
IF Z 2 =F 0 GOTO A 
GOTOE 
[A] 
Z 2 +-- Z 2 -â¢ 1 
zi +--xi+ Y 
Y+- zi 
GOTOB 
Of course, the "instruction" ZI +--XI + y is not permitted in the lan-
guage .9'. What we have in mind is that since we already have an addition 
program, we can replace the macro ZI +--XI + Y by a program for 
computing it, which we will call its macro expansion. At first glance, one 
might wonder why the pair of instructions 
zi +--xi+ Y 
Y+- zi 

2. Some Examples of Programs 
23 
was used in this program rather than the single instruction 
v~x 1 + Y 
since we simply want to replace the current value of Y by the sum of its 
value and x1 â¢ The sum program in (d) computes Y = X 1 + X2 â¢ If we were 
to use that as a template, we would have to replace X 2 in the program by 
Y. Now if we tried to use Y also as the variable being assigned, the macro 
expansion would be as follows: 
v~x1 
z~v 
[B] 
IF Z =F 0 GOTO A 
GOTOE 
[A] 
Z ~ Z- 1 
v~ Y+ 1 
GOTOB 
What does this program actually compute? It should not be difficult to see 
that instead of computing x 1 + y as desired, this program computes 2x 1 â¢ 
Since X 1 is to be added over and over again, it is important that X 1 not be 
destroyed by the addition program. Here is the multiplication program, 
showing the macro expansion of Z 1 ~ X 1 + Y: 
Zz ~xz 
[B] 
IF Z 2 =F 0 GOTO A 
GOTOE 
[A] 
Z 2 ~ Z 2 -
1 
zl ~x~ 
z3 ~ Y 
[Bz] 
IF Z3 =F 0 GOTO A 2 
Macro Expansion of 
GOTO Â£ 2 
zl ~x~ + Y 
[Az] 
Z3 ~ Z 3 -
1 
Z 1 ~ Z 1 + 1 
GOTO B2 
[Ez] 
v~z 1 
GOTOB 
Note the following: 
1:- The local variable Z 1 in the addition program in (d) must be replaced 
by another local variable (we have used Z 3) because Z 1 (the other 
name for Z) is also used as a local variable in the multiplication 
program. 

24 
Chapter 2 Programs and Computable Functions 
2. The labels A, B, E are used in the multiplication program and hence 
cannot be used in the macro expansion. We have used A 2 , B2 , Â£ 2 
instead. 
3. The instruction GOTO Â£ 2 terminates the addition. Hence, it is 
necessary that the instruction immediately following the macro ex-
pansion be labeled Â£ 2 â¢ 
In the future we will often omit such details in connection with macro 
expansions. All that is important is that our infinite supply of variables and 
labels guarantees that the needed changes can always be made. 
(f) For our final example, we take the program 
Y+-X1 
Z +-X2 
[C] 
IF Z =fo 0 GOTO A 
GOTOE 
[A] 
IF Y =fo 0 GOTO B 
GOTOA 
[B] 
Y +- Y- 1 
Z+-Z-1 
GOTOC 
If we begin with X 1 = 5, X 2 = 2, the program first sets Y = 5 and Z = 2. 
Successively the program sets Y = 4, Z = 1 and Y = 3, Z = 0. Thus, the 
computation terminates with Y = 3 = 5 - 2. Clearly, if we begin with 
X 1 = m, X 2 = n, where m ~ n, the program will terminate with Y = 
m -n. 
What happens if we begin with a value of X 1 less than the value of X 2 , 
e.g., X 1 = 2, X 2 = 5? The program sets Y = 2 and Z = 5 and successively 
sets Y = 1, Z = 4 and Y = 0, Z = 3. At this point the computation enters 
the "loop": 
[A] 
IF Y =fo 0 GOTO B 
GOTOA 
Since y = 0, there is no way out of this loop and the computation will 
continue "forever." Thus, if we begin with X 1 = m, X 2 = n, where m < n, 
the computation will never terminate. In this case (and in similar cases) we 
will say that the program computes the partial function 
if 
x 1 ~ Xz 
if x 1 <x2 â¢ 
(Partial functions are discussed in Chapter 1, Section 2.) 

3. Syntax 
25 
Exercises 
1. Write a program in Y 
(using macros freely) that computes the 
function f(x) = 3x. 
2. Write a program in Y that solves Exercise 1 using no macros. 
3. Let f(x) = 1 if x is even; f(x) = 0 if x is odd. Write a program in Y 
that computes f. 
4. Let f(x) = 1 if x is even; f(x) undefined if x is odd. Write a program 
in Y that computes f. 
5. 
Let f(x 1 , x2 ) = 1 if x1 = x 2 ; f(x 1 , x2 ) = 0 if x 1 =I= x 2 â¢ Without using 
macros, write a program in Y that computes f. 
6. Let f(x) be the greatest number n such that n 2 ~ x. Write a program 
in Y that computes f. 
7. 
Let gcd(x 1 , x2 ) be the greatest common divisor of x1 and x 2 â¢ Write a 
program in Y that computes gcd. 
3. 
Syntax 
We are now ready to be mercilessly precise about the language Y. Some 
of the description recapitulates the preceding discussion. 
The symbols 
are called input variables, 
zl Zz z3 ... 
are called local variables, and Y is called the output variable of Y. The 
symbols 
AI Bl Cl Dl Â£1 Az Bz ... 
are called labels of Y. (As already indicated, in practice the subscript 1 is 
often omitted.) A statement is one of the following: 
v~ V+ 1 
v~ v-1 
v~ v 
IF V =I= 0 GOTO L 
where V may be any variable and L may be any label. 

26 
Chapter 2 Programs and Computable Functions 
Note that we have included among the statements of Y the "dummy" 
commands V ~ V. Since execution of these commands leaves all values 
unchanged, they have no effect on what a program computes. They are 
included for reasons that will not be made clear until much later. But their 
inclusion is certainly quite harmless. 
Next, an instmction is either a statement (in which case it is also called 
an unlabeled instruction) or [L] followed by a statement (in which case the 
instruction is said to have L as its label or to be labeled L). A program is 
a list (i.e., a finite sequence) of instructions. The length of this list is called 
the length of the program. It is useful to include the empty program of 
length 0, which of course contains no instructions. 
As we have seen informally, in the course of a computation, the 
variables of a program assume different numerical values. This suggests 
the following definition: 
A state of a program .9' is a list of equations of the form V = m, where V 
is a variable and m is a number, including an equation for each variable 
that occurs in 9' and including no two equations with the same variable. 
As an example, let .9' be the program of (b) from Section 2, which contains 
the variables X Y Z. The list 
X= 4, 
Y= 3, 
z = 3 
is thus a state of .9'. (The definition of state does not require that the state 
can actually be "attained" from some initial state.) The list 
X2 = 5, 
Y= 4, 
z =4 
is also a state of .9'. (Recall that X is another name for X 1 and note that 
the definition permits inclusion of equations involving variables not actu-
ally occurring in .9'.) The list 
X= 3, 
Z=3 
is not a state of .9' since no equation in Y occurs. Likewise, the list 
X= 3, 
X=4, 
Y= 2, 
z = 2 
is not a state of .9': there are two equations in X. 
Let u be a state of .9' and let V be a variable that occurs in u. The 
value of Vat u is then the (unique) number q such that the equation 
V = q is one of the equations making up u. For example, the value of X 
at the state 
X= 4, 
Y= 3, 
Z=3 
is 4. 

3. Syntax 
27 
Suppose we have a program 9' and a state u of 9'. In order to say what 
happens "next," we also need to know which instruction of 9' is about to 
be executed. We therefore define a snapshot or instantaneous description 
of a program 9' of length n to be a pair (i, u) where 1 ~ i ~ n + 1, and u 
is a state of 9'. (Intuitively the number i indicates that it is the ith 
instruction which is about to be executed; i = n + 1 corresponds to a 
"stop" instruction.) 
If s = (i, u) is a snapshot of 9' and V is a variable of 9', then the value 
of Vats just means the value of V at u. 
A snapshot (i, u) of a program 9' of length n is called terminal if 
i = n + 1. If (i, u) is a nonterminal snapshot of 9', we define the successor 
of (i, u) to be the snapshot (j, T) defined as follows: 
Case 1. The ith instruction of 9' is V ~ V + 1 and u contains the 
equation V = m. Then j = i + 1 and T is obtained from u by 
replacing the equation V = m by V = m + 1 (i.e., the value of V 
at T ism+ 1). 
Case 2. The ith instruction of 9' is V ~ V- 1 and u contains the 
equation V = m. Then j = i + 1 and T is obtained from u by 
replacing the equation V = m by V = m - 1 if m -=!= 0; if m = 0, 
T = U. 
Case 3. The ith instruction of 9' is V ~ V. Then T = u and j = i + 1. 
Case 4. The ith instruction of 9' is IF V-=!= 0 GOTO L. Then T = u, and 
there are two subcases: 
Case 4a. u contains the equation V = 0. Then j = i + 1. 
Case 4b. u contains the equation V = m where m -=1= 0. Then, if there is 
an instruction of 9' labeled L, j is the least number such that 
the jth instruction of 9' is labeled L. Otherwise, j = n + 1. 
For an example, we return to the program of (b), Section 2. Let u be 
the state 
X= 4, 
Y= 0, 
Z=O 
and let us compute the successor of the snapshots (i, u) for various values 
of i. 
For i = 1, the successor is (4, u) where u is as above. For i = 2, the 
successor is (3, T ), where T consists of the equations 
X=4, 
Y= 0, 
Z=l. 
For i = 7, the successor is (8, u ). This is a terminal snapshot. 
A computation of a program 9' is defined to be a sequence (i.e., a list) 
s1,s2 , â¢â¢â¢ ,sk of snapshots of 9' such that s;+t is the successor of s; for 
i = 1, 2, ... , k - 1 and sk is terminal. 

28 
Chapter 2 Programs and Computable Functions 
Note that we have not forbidden a program to contain more than one 
instruction having the same label. However, our definition of successor of 
a snapshot, in effect, interprets a branch instruction as always referring to 
the first statement in the program having the label in question. Thus, for 
example, the program 
[A] 
X+--- X- 1 
IF X =1= 0 GOTO A 
[A] 
X+--- X+ 1 
is equivalent to the program 
Exercises 
[A] 
X+--- X- 1 
IF X =1= 0 GOTO A 
X+-X+ 1 
1. Let .9 be the program of (b), Section 2. Write out a computation of .9 
beginning with the snapshot (1, u ), where u consists of the equations 
X = 2, Y = 0, Z = 0. 
2. Give a program .9 such that for every computation s1, ... , sk of .9, 
k = 5. 
3. Give a program .9 such that for any n ~ 0 and every computation 
s1 = (1, u ), s2 , â¢â¢â¢ , sk of .9 that has the equation X= n in u, k = 
2n + 1. 
4. 
Computable Functions 
We have been speaking of the function computed by a program .9. It is 
now time to make this notion precise. 
One would expect a program that computes a function of m variables to 
contain the input variables X 1 , X2 , â¢â¢â¢ , Xm, and the output variable Y, 
and to have all other variables (if any) in the program be local. Although 
this has been and will continue to be our practice, it is convenient not to 
make it a formal requirement. According to the definitions we are going to 
present, any program .9 of the language Y can be used to compute a 
function of one variable, a function of two variables, and, in general, for 
each m ~ 1, a function of m variables. 
Thus, let .9 be any program in the language Y and let r 1 , â¢â¢â¢ , r m be m 
given numbers. We form the state u of .9 which consists of the equations 
... ' 
Y=O 

4. Computable Functions 
29 
together with the equations V = 0 for each variable V in go other than 
X 1 , â¢â¢â¢ , Xm, Y. We will call this the initial state, and the snapshot (1, u ), 
the initial snapshot. 
Case 1. There is a computation s 1 , s 2 , â¢â¢â¢ , s k of go beginning with the initial 
snapshot. Then we write r/J~m>(r 1 ,r2 , â¢â¢â¢ ,rm) for the value of the 
variable Y at the (terminal) snapshot sk. 
Case 2. There is no such computation; i.e., there is an infinite sequence 
s1,s2 ,s3 , â¢â¢â¢ beginning with the initial snapshot where each si+l 
is the successor of s;. In this case r/J~m>(r 1 , â¢â¢â¢ , r m) is undefined. 
Let us reexamine the examples in Section 2 from the point of view of 
this definition. We begin with the program of (b). For this program go, we 
have 
1/J~l)(x) = x 
for all x. For this one example, we give a detailed treatment. The following 
list of snapshots is a computation of go: 
(1,{X = r, Y= O,Z = 0}), 
(4, {X= r, Y = 0, Z = 0}), 
(5, {X= r- 1, Y = 0, Z = 0}), 
(6, {X= r- 1, Y = 1, Z = 0}), 
(7, {X= r- 1, Y = 1, Z = 1}), 
(1, {X= r- 1, Y = 1, Z = 1}), 
(1, {X= 0, Y = r, Z = r}), 
(2, {X= 0, Y = r, Z = r}), 
(3, {X= 0, Y = r, Z = r + 1}), 
(8, {X= 0, Y = r, Z = r + 1}). 
We have included a copy of go showing line numbers: 
[A] 
[B] 
IF X =1= 0 GOTO B 
z ~z + 1 
IF Z =I= 0 GOTO E 
x~x-1 
Y~ Y+ 1 
z ~z + 1 
IF Z =/= 0 GOTO A 
(1) 
(2) 
(3) 
(4) 
(5) 
(6) 
{7) 

30 
Chapter 2 Programs and Computable Functions 
For other examples of Section 2 we have 
{ 1 
if r = 0 
(a) 
1/J(I >(r) = 
r 
otherwise, 
(b), (c) 
1/1Â°>(r) = r, 
(d) 
1/1<2>(r1 , r2 ) = r1 + r2 , 
(e) 
1/1<2>(r1 , r2) = r1 â¢ r2 , 
(D 
1/1<2l(rl, rz) = { r; - rz 
if r1 ~ r2 
if r1 < r2 â¢ 
Of course in several cases the programs written in Section 2 are abbrevia-
tions, and we are assuming that the appropriate macro expansions have 
been provided. 
As indicated, we are permitting each program to be used with any 
number of inputs. If the program has n input variables, but only m < n 
are specified, then according to the definition, the remaining input vari-
ables are assigned the value 0 and the computation proceeds. If on the 
other hand, m values are specified where m > n the extra input values are 
ignored. For example, referring again to the examples from Section 2, we 
have 
(c) 
t/J.!J>(r1 , r 2 ) = r1 , 
{d) 
tfJ.~,I>(r 1 ) = r 1 + 0 = Â·rl> 
t/I.J1>(rl 'rz' r3) = rl + rz. 
For any program .9J and any positive integer m, the function 
t/J.~m>(x 1
, â¢â¢â¢ , xm) is said to be computed by .9. A given partial function g 
(of one or more variables) is said to be partially computable if it is 
computed by some program. That is, g is partially computable if there is a 
program .9J such that 
g(rl ,. .. ,rm) = 1/J.~m>(rl , ... ,rm) 
for all r 1 , â¢â¢â¢ , r m â¢ Here this 'equation must be understood to mean not only 
that both sides have the same value when they are defined, but also that 
when either side of the equation is undefined, the other is also. 
As explained in Chapter 1, a given function g of m variables is called 
total if g(r1 , â¢â¢â¢ , rm) is defined for all r1 , â¢â¢â¢ , rm. A function is said to be 
computable if it is both partially computable and total. 
Partially computable functions are also called partial recursive, and 
computable functions, i.e., functions that are both total and partial recur-
sive, are called recursive. The reason for this terminology is largely histori-
cal and will be discussed later. 
Our examples from Section 2 give us a short list of partially computable 
functions, namely: x, x + y, x Â· y, and x - y. Of these, all except the last 
one are total and hence computable. 

4. Computable Functions 
31 
Computability theory (also called recursion theory) studies the class of 
partially computable functions. In order to justify the name, we need some 
evidence that for every function which one can claim to be "computable" 
on intuitive grounds, there really is a program of the language Y which 
computes it. Such evidence will be developed as we go along. 
We close this section with one final example of a program of .Y: 
[A] 
X+-- X+ 1 
IF X* 0 GOTO A 
For this program 9', 
I/J.~1 >(x) is undefined for all x. So, the nowhere 
defined function (see Chapter 1, Section 2) must be included in the class of 
partially computable functions. 
Exercises 
1. Let 9' be the program 
IF Xi= OGOTOA 
[A] 
X+-- X+ 1 
IF Xi= OGOTO A 
[A] 
Y +-- Y + 1 
What is 1/J.~IJ(x )? 
2. The same as Exercise 1 for the program 
[ B] 
IF X * 0 GOTO A 
Z+-Z+1 
IF Z * OGOTO B 
[A] 
X+-X 
3. The same as Exercise 1 for the empty program. 
4. Let 9' be the program 
Y+-XI 
[A] 
IFX2 =0GOTOÂ£ 
Y+-Y+1 
Y+-Y+1 
X 2 +-- X 2 -
1 
GOTOA 
What is I/J.!)>(r1)? l/l}}>(r1 , r2)? I/Jj}>(r1 , r2 , r3)? 
5. Show that for every partially computable function f(x 1 , â¢â¢â¢ , xn), there 
is a number m ~ 0 such that f is computed by infinitely many 
programs of length m. 

32 
Chapter 2 Programs and Computable Functions 
6. (a) For every number k ~ 0, let fk be the constant function fk(x) = 
k. Show that for every k, fk is computable. 
(b) Let us call an ..9P program a straightline program if it contains no 
(labeled or unlabeled) instruction of the form IF V =F 0 GOTO 
L. Show by h.duction on the length of programs that if the length 
of a straightline program .9 is k, then 1/J.J.I)(x) ~ k for all x. 
(c) Show that, if .9 is a straightline program that computes fk, then 
the length of .9 is at least k. 
(d) Show that no straightline Y 
program computes the function 
f(x) = x + 1. Conclude that the class of functions computable by 
straightline Y programs is contained in but is not equal to the 
class of computable functions. 
7. 
Let us call an Y 
program .9 forward-branching if the following 
condition holds for each occurrence in .9 of a (labeled or unlabeled) 
instruction of the form IF V =F 0 GOTO L. If IF V =F 0 GOTO L is 
the ith instruction of .9, then either L does not appear as the label of 
an instruction in .9, or else, if j is the least number such that L is the 
label of the jth instruction in .9, then i < j. Show that a function is 
computed by some forward-branching program if and only if it is 
computed by some straightline program (see Exercise 6). 
8. Let us call a unary function f(x) partially n-computable if it is com-
puted by some Y 
program .9 such that .9 has no more than n 
instructions, every variable in .9 is among X, Y, Z 1 , â¢â¢â¢ , Zn, and every 
label in .9 is among A 1 , â¢â¢â¢ , An, E. 
(a) Show that if a unary function is computed by a program with no 
more than n instructions, then it is partially n-computable. 
(b) Show that for every n ~ 0, there are only finitely many distinct 
partially n-computable unary functions. 
(c) Show that for every n ~ 0, there are only finitely many distinct 
unary functions computed by Y programs of length no greater 
than n. 
(d) Conclude that for every n ~ 0, there is a partially computable 
unary function which is not computed by any Y 
program of 
length less than n. 
5. 
More about Macros 
In Section 2 we gave some examples of computable functions (i.e., x + y, 
x Â· y) giving rise to corresponding macros. Now we consider this process in 
general. 

5. More about Macros 
33 
Let f(x 1 , â¢â¢â¢ , xn) be some partially computable function computed by 
the program .9J. We shall assume that the variables that occur in .9J are all 
included in the list Y, XI' ... ' xn' zl' ... ' zk and that the labels that 
occur in .9J are all included in the list E, A 1 , â¢â¢â¢ , A 1â¢ We also assume that 
for each instruction of .9J of the form 
IF V =I= 0 GOTO A; 
there is in .9J an instruction labeled A; . (In other words, E is the only 
"exit" label.) It is obvious that, if .9J does not originally meet these 
conditions, it will after minor changes in notation. We write 
gJ =.9J(Y, XI, ... , xn ,zl , ... ,zk; E, AI, ... , At) 
in order that we can represent programs obtained from .9J by replacing the 
variables and labels by others. In particular, we will write 
f2' m = .9J( Zm , Z m + 1 , â¢â¢â¢ , Zm + n , Zm + n + 1 , â¢â¢â¢ , Z m + n + k ; 
for each given value of m. Now we want to be able to use macros like 
in our programs, where V1 , â¢â¢â¢ , V,, W can be any variables whatever. (In 
particular, W might be one of V1 , â¢â¢â¢ , V, .) We will take such a macro to be 
an abbreviation of the following expansion: 
zm +- 0 
zm+l +-VI 
Zm+z +- Vz 
zm+n +- v, 
Zm+n+ I +- 0 
zm+n+2 +- 0 
Here it is understood that the number m is chosen so large that none of 
the variables or labels used in t2' m occur in the main program of which the 
expansion is a part. Notice that the expansion sets the variables corre-
sponding to the output and local variables of .9J equal to 0 and those 
corresponding to X 1 , â¢â¢â¢ , Xn equal to the values of V1 , â¢â¢â¢ , V,, respec-
tively. Setting the variables equal to 0 is necessary (even though they are 

34 
Chapter 2 Programs and Computable Functions 
all local variables automatically initialized to 0) because the expansion may 
be part of a loop in the main program; in this case, at the second and 
subsequent times through the loop the local variables will have whatever 
values they acquired the previous time around, and so will need to be 
reset. Note that when t2'm terminates, the value of Zm is f(V1 , â¢â¢â¢ , V,), so 
that W finally does get the value f(V1 , â¢â¢â¢ , V,). 
If f(V1 , â¢â¢â¢ , V,) is undefined, the program t2'm will never terminate. Thus 
if f is not total, and the macro 
w +--- f( VI ' ... ' v,) 
is encountered in a program where V1 , â¢â¢â¢ , V, have values for which f is 
not defined, the main program will never terminate. 
Here is an example: 
This program computes the function f(x 1 , x2 , x 3), where 
if 
X 1 ~ Xz 
if 
X 1 < Xz. 
In particular, f(2, 5, 6) is undefined, although (2 - 5) + 6 = 3 is positive. 
The computation never gets past the attempt to compute 2- 5. 
So far we have augmented our language .9 to permit the use of macros 
which allow assignment statements of the form 
W +--- f(VI, ... , V,), 
where f is any partially computable function. Nonetheless there is avail-
able only one highly restrictive conditional branch statement, namely, 
IF V =F 0 GOTO L 
We will now see how to augment our language to include macros of the 
form 
IF P(V1 , â¢â¢â¢ , V,) GOTO L 
where P(x 1 , â¢â¢â¢ , xn) is a computable predicate. Here we are making use of 
the convention, introduced in Chapter 1, that 
TRUE= 1, 
FALSE= 0. 

5. More about Macros 
35 
Hence predicates are just total functions whose values are always either 0 
or 1. And therefore, it makes perfect sense to say that some given 
predicate is or is not computable. 
Let P(x 1 , â¢â¢â¢ , xn) be any computable predicate. Then the appropriate 
macro expansion of 
is simply 
IF P(V1 , â¢â¢â¢ , V,) GOTO L 
Z +-- P(V1 , â¢â¢â¢ , V,) 
IF Z =I= 0 GOTO L 
Note that P is a computable function and hence we have already shown 
how to expand the first instruction. The second instruction, being one of 
the basic instructions in the language .Y, needs no further expansion. 
A simple example of this general kind of conditional branch statement 
which we will use frequently is 
IF V= OGOTO L 
To see that this is legitimate we need only check that the. predicate P(x ), 
defined by P(x) =TRUE if x = 0 and P(x) =FALSE otherwise, is 
computable. Since TRUE= 1 and FALSE= 0, the following program 
does the job: 
IF X =I= 0 GOTO E 
Y+-Y+1 
The use of macros has the effect of enabling us to write much shorter 
programs than would be possible restricting ourselves to instructions of the 
original language .Y. The original "assignment" statements V +-- V + 1, 
V +-- V - 1 are now augmented by general assignment statements of the 
form W +-- f(V1 , â¢â¢â¢ , V,) for any partially computable function f. Also, the 
original conditional branch statements IF V =/= 0 GOTO L are now aug-
mented by general conditional branch statements of the form IF 
P(V1 , â¢â¢â¢ , V,) GOTO L for any computable predicate P. The fact that any 
function which can be computed using these general instructions could 
already have been computed by a program of our original language Y 
(since the general instructions are merely abbreviations of programs of .Y) 
is powerful evidence of the generality of our notion of computability. 
Our next task will be to develop techniques that will make it easy to see 
that various particular functions are computable. 

36 
Chapter 2 Programs and Computable Functions 
Exercises 
1. (a) Use the process described in this section to expand the program 
in example (d) of Section 2. 
(b) What is the length of the .9' program expanded from example 
(e) by this process? 
2. 
Replace the instructions 
in example (e) of Section 2 with the instruction Y +--- X 1 + Y, and 
expand the result by the process described in this section. If 9' is the 
resulting .9' program, what is I/I.J}>(r1 , r 2)? 
3. Let f(x), g(x) be computable functions and let h(x) = f(g(x)). Show 
that h is computable. 
4. Show by constructing a program that the predicate x 1 ~ x 2 is com-
putable. 
5. Let P(x) be a computable predicate. Show that the function f 
defined by 
is partially computable. 
if P(x 1 + x2 ) 
otherwise 
6. 
Let P(x) be a computable predicate. Show that 
EXp(r) = { ~ 
if there are at least r numbers n such that P(n) = 1 
otherwise 
is partially computable. 
7. 
Let 7T be a computable permutation (i.e., one-one, onto function) of 
N, and let 7T- 1 be the inverse of 7T, i.e., 
7T-l(y) =X 
if and only if 
7T(X) = y. 
Show that 7T- 1 is computable. 
8. Let f(x) be a partially computable but not total function, let M be a 
finite set of numbers such that f(m)j for all m EM, and let g(x) be 

5. More about Macros 
an arbitrary partially computable function. Show that 
{ g(x) 
h(x) = 
f(x) 
is partially computable. 
if X EM 
otherwise 
37 
9. Let .9"+ be a programming language that extends .9" by permitting 
instructions of the form V ~ k, for any k ~ 0. These instructions 
have the obvious effect of setting the value of V to k. Show that a 
function is partially computable by some .9"+ program if and only if it 
is partially computable. 
10. Let .9"' be a programming language defined like .9" except that its 
(labeled and unlabeled) instructions are of the three types 
v~vÂ· 
v~ V+ 1 
If V =!= V' GOTO L 
These instructions are given the obvious meaning. Show that a 
function is partially computable in .9"' if and only if it is partially 
computable. 


3 
Primitive Recursive Functions 
1. Composition 
We want to combine computable functions in such a way that the output 
of one becomes an input to another. In the simplest case we combine 
functions f and g to obtain the function 
h(x) = f(g(x)). 
More generally, for functions of several variables: 
Definition. Let f be a function of k variables and let g 1 , â¢â¢â¢ , gk be 
functions of n variables. Let 
h(xl ' ... ' xn) = f(gl(xl ' ... ' xn), ... ' gk(xl ' ... ' xn)). 
Then h is said to be obtained from f and g1, ... , gk by composition. 
Of course, the functions J, g 1 , â¢â¢â¢ , gk need not be total. h(x1 , â¢â¢â¢ , xn) 
will be defined when all of z1 = g1(x1, ... , xn), ... , zk = gk(x1, ... , xn) are 
defined and also f(z1, ... , zk) is defined. 
Using macros it is very easy to prove 
Theorem 1.1. If h is obtained from the (partially) computable functions 
f, g1, ... , gk by composition, then h is (partially) computable. 
39 

40 
Chapter 3 Primitive Recursive Functions 
The word partially is placed in parentheses in order to assert the 
correctness of the statement with the word included or omitted in both 
places. 
Proof. The following program obviously computes h: 
Z1 ~ gl(Xl , ... , Xn) 
Zk ~gk(X1, ... ,Xn) 
Y ~ f(Zl, Â· Â· Â·, Zk) 
Iff, g1 , â¢â¢â¢ , gk are not only partially computable but are also total, then 
so is h. 
â¢ 
By Section 4 of Chapter 2, we know that x, x + y, x Â· y, and x- y are 
partially computable. So by Theorem 1.1 we see that 2x = x + x and 
4x 2 = (2x) Â· (2x) are computable. So are 4x2 + 2x and 4x 2 -
2x. Note 
that 4x 2 - 2x is total, although it is obtained from the nontotal function 
x - y by composition with 4x 2 and 2x. 
2. 
Recursion 
Suppose k is some fixed number and 
h(O) = k, 
h(t + 1) = g(t, h(t)), 
(2.1) 
where g is some given total function of two variables. Then h is said to be 
obtained from g by primitive recursion, or simply recursion. 1 
Theorem 2.1. Let h be obtained from g as in (2.1), and let g be 
computable. Then h is also computable. 
Proof. We first note that the constant function f(x) = k is computable; 
in fact, it is computed by the program 
y ~ y + 1} 
Y~ Y+ 1 
k lines 
Y~ Y+ 1 
1 Primitive recursion, characterized by Equations (2.1) and (2.2), is just one specialized 
form of recursion, but it is the only one we will be concerned with in this chapter, so we will 
refer to it simply as recursion. We will consider more general forms of recursion in Part 5. 

2. Recursion 
41 
Hence we have available the macro Y +-- k. The following is a program 
that computes h(x): 
Y+-k 
[A] 
IFX=OGOTOE 
Y +-- g(Z, Y) 
Z+-Z+1 
X+--X-1 
GOTOA 
To see that this program does what it is supposed to do, note that, if Y 
has the value h(z) before executing the instruction labeled A, then it has 
the value g(z, h(z)) = h(z + 1) after executing the instruction Y +-
g(Z, Y). Since Y is initialized to k = h(O), Y successively takes on the 
values h(O), h(l), ... , h(x) and then terminates. 
â¢ 
A slightly more complicated kind of recursion is involved when we have 
(2.2) 
h(x 1 , â¢â¢â¢ ,xn,t + 1) =g(t,h(x 1 , â¢â¢â¢ ,xn,t),x1 , â¢â¢â¢ ,xn). 
Here the function h of n + 1 variables is said to be obtained by primitive 
recursion, or simply recursion, from the total functions f (of n variables) 
and g (of n + 2 variables). The recursion (2.2) is just like (2.1) except that 
parameters x 1 , â¢â¢â¢ , xn are involved. Again we have 
Theorem 2.2. 
Let h be obtained from f and g as in (2.2) and let f, g be 
computable. Then h is also computable. 
Proof. The proof is almost the same as for Theorem 2.1. The following 
program computes h(x 1 , â¢â¢â¢ , xn, xn+ 1): 
Y +-- f(Xl, Â· Â· Â·, Xn) 
[A] 
IF Xn+l = 0 GOTO E 
Y +-- g(Z, Y, X 1 , â¢â¢â¢ , Xn) 
Z+-Z+1 
GOTOA 
â¢ 

42 
Chapter 3 Primitive Recursive Functions 
3. 
PRC Classes 
So far we have considered the operations of composition and recursion. 
Now we need some functions on which to get started. These will be 
and the projection functions 
s(x) =x + 1, 
n(x) = 0, 
1 :::;; i:::;; n. 
[For example, u~(x 1
, x 2 , x 3 , x4 ) = x 3 .] The functions s, n, and u? are 
called the initial functions. 
Definition. A class of total functions ~ is called a PRC2 class if 
1. the initial functions belong to ~. 
2. a function obtained from functions belonging to ~ by either composi-
tion or recursion also belongs to ~-
Then we have 
Theorem 3.1. The class of computable functions is a PRC class. 
Proof. 
By Theorems 1.1, 2.1, and 2.2, we need only verify that the initial 
functions are computable. 
Now this is obvious; s(x) = x + 1 is computed by 
n(x) is computed by the empty program, and u?(x 1 , â¢â¢â¢ , xn) is computed 
by the program 
â¢ 
Definition. A function is called primitive recursive if it can be obtained 
from the initial functions by a finite number of applications of composition 
and recursion. 
It is obvious from this definition that 
2 This is an abbreviation for "primitive recursively closed." 

3. PRC Classes 
43 
Corollary 3.2. The class of primitive recursive functions is a PRC class. 
Actually we can say more: 
Theorem 3.3. A function is primitive recursive if and only if it belongs to 
every PRC class. 
Proof. If a function belongs to every PRC class, then, in particular, by 
Corollary 3.2, it belongs to the class of primitive recursive functions. 
Conversely let a function f be a primitive recursive function and let ~ 
be some PRC class. We want to show that f belongs to '??. Since f is a 
primitive recursive function, there is a list / 1 , f 2 , â¢â¢â¢ , fn of functions such 
that fn = f and each /; in the list is either an initial function or can be 
obtained from preceding functions in the list by composition or recursion. 
Now the initial functions certainly belong to the PRC class ~- Moreover 
the result of applying composition or recursion to functions in ~ is again a 
function belonging to '??.Hence each function in the list / 1 , â¢â¢â¢ , fn belongs 
to ~- Since fn = J, f belongs to ~-
â¢ 
Corollary 3.4. Every primitive recursive function is computable. 
Proof. 
By the theorem just proved, every primitive recursive function 
belongs to the PRC class of computable functions. 
â¢ 
In Chapter 4 we shall show how to obtain a computable function that is 
not primitive recursive. Hence it will follow that the set of primitive 
recursive functions is a proper subset of the set of computable functions. 
Exercises 
1. Let ~be a PRC class, and let g1 , g2 , g3 , g4 belong to '??.Show that if 
hz(x) = g2(x, x, x), and 
h3(w, x, y, z) = h1(giw, y), z, gi2, g4(y, z))), 
then h 2 , h 2 , h 3 also belong to '??. 
2. Show that the class of all total functions is a PRC class. 
3. Let n > 0 be some given number, and let '?? be a class of total 
functions of no more than n variables. Show that ~ is not a PRC 
class. 

44 
Chapter 3 Primitive Recursive Functions 
4. Let ~ be a PRC class, let h belong to ~' and let 
f(x) = h(g(x)) and 
g(x) = h(f(x)). 
Show that f belongs to %' if and only if g belongs to %'. 
5. Prove Corollary 3.4 directly from Theorems 1.1, 2.1, 2.2, and the proof 
of Theorem 3.1. 
4. 
Some Primitive Recursive Functions 
We proceed to make a short list of primitive recursive functions. Being 
primitive recursive, they are also computable. 
J. X+ y 
To see that this is primitive recursive, we have to show how to obtain this 
function from the initial functions using only the operations of composi-
tion and recursion. 
If we write f(x, y) = x + y, we have the recursion equations 
f(x,O) =x, 
f(x,y + 1) =f(x,y) + 1. 
We can rewrite these equations as 
f(x,O) = ul(x), 
f(x,y + 1) =g(y,f(x,y),x), 
where g(x 1 , x2 , x 3) = s(u~(x 1
, x2 , x3)). The functions ul(x), u~(x 1 , x2 , x 3), 
and s(x) are primitive recursive functions; in fact they are initial functions. 
Also, g(x 1 , x2 , x 3 ) is a primitive recursive function, since it is obtained by 
composition of primitive recursive functions. Thus, the preceding is a valid 
application of the operation of recursion to primitive recursive functions. 
Hence f(x, y) = x + y is primitive recursive. 
Of course we already knew that x + y was a computable function. So we 
have only obtained the additional information that it is in fact primitive 
recursive. 
2. xÂ·y 
The recursion equations for h(x, y) = x Â· y are 
h(x, 0) = 0, 
h(x,y + 1) = h(x,y) +x. 

4. Some Primitive Recursive Functions 
45 
This can be rewritten 
h(x,O) = n(x) 
h(x, y + 1) = g(y, h(x, y), x). 
Here, n(x) is the zero function, 
f(x 1 , x 2) is x1 + x2 , and u~(x 1 , x 2 , x3), u~(x 1
, x 2 , x3) are projection func-
tions, Notice that the functions n(x), u~(x 1
, x 2 , x3), and u~(x 1 , x 2 , x3) are 
all primitive recursive functions, since they are all initial functions. We 
have- just shown that f(x 1 , x 2 ) = x1 + x 2 is primitive recursive, so 
g(x1 , x 2 , x 3 ) is a primitive recursive function since it is obtained from 
primitive recursive functions by composition. Finally, we conclude that 
h(x,y) =xÂ·y 
is primitive recursive. 
3. 
x! 
The recursion equations are 
0!= 1, 
(x + l)!=x!Â·s(x). 
More precisely, x! = h(x ), where 
h(O) = 1, 
h(t + 1) =g(t,h(t)), 
and 
Finally, g is primitive recursive because 
and multiplication is already known to be primitive recursive. 
In the examples that follow, we leave it to the reader to check that the 
recursion equations can be put in the precise form called for by the 
definition of the operation of recursion. 

46 
Chapter 3 Primitive Recursive Functions 
4. 
xY 
The recursion equations are 
x 0 = 1, 
xy+l =xYÂ·x. 
Note that these equations assign the value 1 to the "indeterminate" 0Â°. 
5. 
p(x) 
The predecessor function p(x) is defined as follows: 
{ X- 1 
p(x) = 
0 
if 
X =fo 0 
if 
X= 0. 
It corresponds to the instruction in our programming language X ~ X -
1. 
The recursion equations for p(x) are simply 
p(O) = 0, 
p(t + 1) = t. 
Hence, p(x) is primitive recursive. 
6. 
X _:_ y 
The function x ..:... y is defined as follows: 
â¢ {X- y 
x-y= 
0 
if 
X ~y 
if 
X < y. 
This function should not be confused with the function x - y, which is 
undefined if x < y. In particular, x ..:... y is total, while x - y is not. 
We show that x ..:... y is primitive recursive by displaying the recursion 
equations: 
x-=-O=x, 
x -=-(t + 1) = p(x..:... t). 
7. lx- yl 
The function lx - yl is defined as the absolute value of the difference 
between x and y. It can be expressed simply as 
lx- yl = (x-=- y) + (y -=-x) 
and thus is primitive recursive. 

4. Some Primitive Recursive Functions 
8. 
a(x) 
The function a(x) is defined as 
a(x) = {~ 
if 
X= 0 
if 
X-=/= 0. 
a(x) is primitive recursive since 
a(x) = 1 ..:... x. 
Or we can simply write the recursion equations: 
Exercises 
a(O) = 1, 
a(t + 1) = 0. 
47 
l. 
Give a detailed argument that xY, p(x), and x ..:... y are primitive 
recursive. 
2. 
Show that for each k, the function f(x) = k is primitive recursive. 
3. 
Prove that if f(x) and g(x) are primitive recursive functions, so is 
f(x) + g(x). 
4. 
Without using x + y as a macro, apply the constructions in the 
proofs of Theorems 1.1, 2.2, and 3.1 to give an .9' program that 
computes x Â· y. 
5. 
For any unary function f(x), the nth iteration off, written r, is 
r<x) = f( ... f(x) ... ), 
where f is composed with itself n times on the right side of the 
equation. (Note that r(x) = x.) Let l/n, x) = r(x). Show that iff 
is primitive recursive, then l 1 is also primitive recursive. 
6.* 
(a) 
Let E(x) = 0 if x is even, E(x) = 1 if x is odd. Show that 
E(x) is primitive recursive. 
(b) 
Let H(x) = xj2 if x is even, (x- 0/2 if x is odd. Show that 
H(x) is primitive recursive. 
7.* Let f{O) = 0, /(1) = 1, /(2) = 22 , /(3) = 333 = 327, etc. In general, 
f(n) is written as a stack n high, of n's as exponents. Show that f is 
primitive recursive. 

48 
Chapter 3 Primitive Recursive Functions 
8. * Let k be some fixed number, let f be a function such that f(x + 1) 
< x + 1 for all x, and let 
h(O) = k 
h(t + 1) = g(h(f(t + 1))). 
Show that iff and g belong to some PRC class %',then so does h. 
[Hint: Define f'(x) = min,< xf'(x) = 0. See Exercise 5 for the 
definition of f'(x).] 
-
9.* 
Let g(x) be a primitive recursive function and let f(O, x) = g(x), 
f(n + 1, x) = f(n, f(n, x)). Prove that f(n, x) is primitive recursive. 
10. * Let COMP be the class of functions obtained from the initial 
functions by a finite sequence of compositions. 
(a) Show that for every function f(x 1 , â¢â¢â¢ , xn) in COMP, either 
f(x 1 , â¢â¢â¢ , xn) = k for some constant k, or f(x 1 , â¢â¢â¢ , xn) = 
X; + k for some 1 :::;; i :::;; n and some constant k. 
(b) An n-ary function f is monotone if for all n-tuples (x 1 , â¢â¢â¢ , xn), 
(y 1 , â¢â¢â¢ , Yn) such that 
X; :::;; Y;, 1 :::;; i :::;; n, f(x 1 , â¢â¢â¢ , xn) :::;; 
f(y 1 , â¢â¢â¢ , Yn). Show that every function in COMP is monotone. 
(c) 
Show that COMP is a proper subset of the class of primitive 
recursive functions. 
(d) Show that the class of functions computed by straightline .9' 
programs is a proper subset of COMP. [See Exercise 4.6 in 
Chapter 2 for the definition of straightline programs.] 
11. * Let 9' 1 be the class of all functions obtained from the initial 
functions by any finite number of compositions and no more than 
one recursion (in any order). 
(a) Let f(x 1 , â¢â¢â¢ , xn) belong to COMP. [See Exercise 10 for the 
definition of COMP.] Show that there is a k > 0 such that 
f(x 1 , â¢â¢â¢ ,xn):::;; max{x 1 , â¢â¢â¢ ,xn} + k. 
(b) Let 
h(o) = c 
h(t + 1) = g(t, h(t)), 
where c is some given number and g belongs to COMP. Show 
that there is a k > 0 such that h(t) :::;; tk + c. 
(c) 
Let 
h(x1 , â¢â¢â¢ ,xn,O) =f(x 1 , â¢â¢â¢ ,xn) 
h(x 1 , â¢â¢â¢ ,xn ,t + 1) = g(t,h(x 1 , â¢â¢â¢ ,xn ,t),x1 , â¢â¢â¢ ,xn), 

5. Primitive Recursive Predicates 
49 
where f, g belong to COMP. Show that there are k, I> 0 such 
that h(x1 , â¢â¢â¢ ,xn,t):::;; tk + max{x1 , â¢â¢â¢ ,xn} +I. 
(d) 
Let f(xp ... , xn) belong to 9'1 â¢ Show that there are k, I> 0 
such that f(x 1 , â¢â¢â¢ , xn) :::;; max{x1 , â¢â¢â¢ , xn} Â· k + I. 
(e) 
Show that 9'1 is a proper subset of the class of primitive 
recursive functions. 
5. 
Primitive Recursive Predicates 
We recall from Chapter 1, Section 4, that predicates or Boolean-valued 
functions are simply total functions whose values are 0 or 1. (We have 
identified 1 with TRUE and 0 with FALSE.) Thus we can speak without 
further ado of primitive recursive predicates. 
We continueÂ· our list of primitive recursive functions, including some 
that are predicates. 
9. 
X= y 
The predicate x = y is defined as 1 if the values of x and y are the same 
and 0 otherwise. Thus we wish to show that the function 
d(x,y)={~ 
if 
X= y 
if 
X =/= y 
is primitive recursive. This follows immediately from the equation 
d(x, y) = a(lx- yl ). 
JO. 
X :o;;y 
This predicate is simply the primitive recursive function a(x ..:... y ). 
Theorem 5.1. Let ~be a PRC class. If P, Q are predicates that belong to 
%',then so are -P, P v Q, and P & Q.3 
Proof. Since -P = a(P), it follows that -P belongs to ~. (a was 
defined in Section 4, item 8.) 
3 See Chapter 1, Section 4. 

50 
Chapter 3 Primitive Recursive Functions 
Also, we have 
p & Q =PÂ·Q, 
so that P & Q belongs to ~. 
Finally, the De Morgan law 
P v Q <=> -(-P& -Q) 
shows, using what we have already done, that P v Q belongs to 'lf. 
â¢ 
A result like Theorem 5.1 which refers to PRC classes can be applied to 
the two classes we have shown to be PRC. That is, taking ~ to be the class 
of all primitive recursive functions, we have 
Corollary 5.2. If P, Q are primitive recursive predicates, then so are -P, 
P v Q, and P & Q. 
Similarly taking '(? to be the class of all computable functions, we have 
Corollary 5.3. If P, Q are computable predicates, then so are - P, 
P v Q, and P & Q. 
As a simple example we have 
11. x<y 
We can write 
or more simply 
X <y <=>X :s;y & -(x =y), 
x <y <=> -(y ::s;x). 
Theorem 5.4 (Definition by Cases). Let 'lf be a PRC class. Let the 
functions g, h and the predicate P belong to ~.Let 
( g(x1 , â¢â¢â¢ , xn) 
if P(x1 , ... , xn) 
/(xi' ... ' xn) = 
h( 
) 
. 
x1 , â¢â¢â¢ , xn 
otherwise. 
Then f belongs to ~. 
This will be recognized as a version of the familiar "if ... then ... , 
else ... " statement. 
Proof. The result is obvious because 
/(xi' ... ' xn) 
= g(x1 , â¢â¢â¢ , xn) Â· P(x1 , â¢â¢â¢ , xn) + h(x1 , â¢â¢â¢ , xn) Â· a(P(x1 , â¢â¢â¢ , xn)) . 
â¢ 

5. Primitive Recursive Predicates 
51 
Corollary 5.5. Let I&' be a PRC class, let n-ary functions g 1 , â¢â¢â¢ , gm, h 
and predicates P1 , â¢â¢â¢ , Pm belong to ~'and let 
P;(x 1 , â¢â¢â¢ ,xn)& lj(x 1 , â¢â¢â¢ ,xn) = 0 
for all 1 5. i < j 5. m and all x 1 , â¢â¢â¢ , x n â¢ If 
then f also belongs to I&'. 
gm(xl ' ... ' xn) 
h(x1 , â¢â¢â¢ ,xn) 
if 
Pm(x 1 , â¢â¢â¢ , xn) 
otherwise, 
Proof. We argue by induction on m. The case for m = 1 is given by 
Theorem 5.4, so let 
Then 
gm+ I( XI' ... ' xn) 
h(x1 , â¢â¢â¢ ,xn) 
gm(xl ' ... ' xn) 
h'(xl ' ... ' xn) 
if 
pm+l(xl, ... ,xn) 
otherwise, 
if 
pm + l(xl ' ... ' xn) 
otherwise. 
if 
Pm(Xp ... ' xn) 
otherwise, 
and h' belongs to ~ by Theorem 5.4, so f belongs to ~ by the induction 
hypothesis. 
â¢ 
Exercise 
1. Let us call a predicate trivial if it is always TRUE or always FALSE. 
Show that no nontrivial predicates belong to COMP (see Exercise 4.10 
for the definition of COMP.) 

52 
Chapter 3 Primitive Recursive Functions 
6. 
Iterated Operations and Bounded Quantifiers 
Theorem 6.1. 
Let 'iff be a PRC class. If f(t, x1 , â¢â¢â¢ , xn) belongs to 'iff, 
then so do the functions 
y 
g(y,x 1 , ... ,xn) = "[.J(t,x 1 , ... ,xn) 
t=O 
and 
y 
h(y,xl, ... ,xn) = 0J(t,xl, ... ,xn). 
t=O 
A common error is to attempt to prove this by using mathematical 
induction on y. A little reflection reveals that such an argument by 
induction shows that 
all belong to 'iff, but not that the function g(y, x 1 , â¢â¢â¢ , xn), one of whose 
arguments is y, belongs to 'iff. 
We proceed with the correct proof. 
Proof. We note the recursion equations 
g(O, X]' ... ' xn) = f(O, X]' ... ' xn), 
g ( t + 1, X 1 , ... , X n) = g ( t, X 1 , ... , X n) + j( t + 1, X 1 , ... , X n), 
and recall that since + is primitive recursive, it belongs to 'iff. 
Similarly, 
h(O, X]' â¢â¢â¢ ' xn) = f(O, X]' ... ' xn), 
h(t + 1,x1 , ... ,x) = h(t,x 1 , ... ,xn) Â·f(t + 1,x1 , ... ,xn). 
â¢ 
Sometimes we will want to begin the summation (or product) at 1 
instead of 0. That is, we will want to consider 
or 
y 
g(y,xl, ... ,xn) = "[.f(t,xl, ... ,xn) 
t= I 
y 
h(y,x1 , ... ,x) = 0f(t,x 1 , ... ,xn). 
t= I 

6. Iterated Operations and Bounded Quantifiers 
Then the initial recursion equations can be taken to be 
g(O,x1 , â¢â¢â¢ ,xn) = 0, 
h(O,x 1 , â¢â¢â¢ ,xn) = 1, 
53 
with the equations for g(t + 1, x 1 , â¢â¢â¢ , xn) and h(t + 1, x 1 , â¢â¢â¢ , xn) as in 
the preceding proof. Note that we are implicitly defining a vacuous sum to 
be 0 and a vacuous product to be 1. With this understanding we have 
proved 
Corollary 6.2. If f(t, x 1 , â¢â¢â¢ , xn) belongs to the PRC class ~'then so do 
the functions 
and 
y 
g(y,x 1 , â¢â¢â¢ ,xn) = L:J(t,x1 , â¢â¢â¢ ,xn) 
t= I 
y 
h(y,x, ... ,xn) = 0f(t,x1 , â¢â¢â¢ ,xn). 
/=I 
We have 
Theorem 6.3. If the predicate P(t, x 1 , â¢â¢â¢ , x n) belongs to some PRC class 
'lf, then so do the predicates4 
and 
Proof. We need only observe that 
and 
(Vt),YP(t,x 1 , â¢â¢â¢ ,xn) = [nP(t,x 1 , â¢â¢â¢ ,xn>] = 1 
/=0 
(3t),YP(t,x 1 , â¢â¢â¢ ,xn) <=> [EP(t,x 1 , â¢â¢â¢ ,xn)] *0. 
â¢ 
/=0 
Actually for the universal quantifier it would even have been correct to 
write the equation 
y 
(Vt),YP(t,x 1 , â¢â¢â¢ ,xn) = 0P(t,x1 , â¢â¢â¢ ,xn). 
/=0 
4 See Chapter 1, Section 5. 

54 
Chapter 3 Primitive Recursive Functions 
Sometimes in applying Theorem 6.3 we want to use the quantifier 
or 
That the theorem is still valid is clear from the relations 
(3t)<yP(t,x 1 , â¢â¢â¢ ,xn) <=> (3t),Y[t =I= y & P(t,x 1 , â¢â¢â¢ ,xn)], 
(Vt)<yP(t,x 1 , â¢â¢â¢ ,xn) = (Vt),Y[t = y V P(t,x 1 , â¢â¢â¢ ,xn)]. 
We continue our list of examples. 
12. ylx 
This is the predicate "y is a divisor of x." For example, 
3112 
is true 
while 
3113 
is false. 
The predicate is primitive recursive since 
ylx = (3t),x(yÂ·t =x). 
13. 
Prime(x) 
The predicate "x is a prime" is primitive recursive since 
Prime(x) =x > 1&(\lt),xlt = 1 v t =x v- (tlx)}. 
(A number is a prime if it is greater than 1 and it has no divisors other 
than 1 and itself.) 
Exercises 
1. 
Let f(x) = 2x if x is a perfect square; f(x) = 2x + 1 otherwise. Show 
that f is primitive recursive. 
2. 
Let u(x) be the sum of the divisors of x if x =I= 0; u(O) = 0 [e.g., 
u(6) = 1 + 2 + 3 + 6 = 12]. Show that u(x) is primitive recursive. 
3. Let 7T(x) be the number of primes that are 
~ x. Show that 7T(x) is 
primitive recursive. 
4. Let SQSM(x) be true if x is the sum of two perfect squares; false 
otherwise. Show that SQSM(x) is primitive recursive. 

7. Mlnlmallzatlon 
55 
5. Let 'lf be a PRC class, let P(t, x 1 , â¢â¢â¢ , xn) be a predicate in 'lf, and let 
g(y,z,x 1 , â¢â¢â¢ ,xn) = (\ft)y:s;r:s;zP(t,XpÂ·Â·Â·,xn)and 
h(y,z,x1 , â¢â¢â¢ ,xn) = (3t)y:s;r:s;zP(t,XpÂ·Â·Â·,xn), 
(where (Vt)y:s;t:s;zP(t,x 1 , â¢â¢â¢ ,xn) and (3t)y:s;r:s;zP(t,x 1 , â¢â¢â¢ ,xn) mean 
that P(t, x 1 , â¢â¢â¢ , xn) is true for all t (respectively, for some t) from y 
to z). Show that g, h also belong to 'lf. 
6. Let RP(x, y) be true if x andy are relatively prime (i.e., their greatest 
common divisor is 1). Show that RP(x, y) is primitive recursive. 
7. Give a sequence of compositions and recursions that shows explicitly 
that Prime(x) is primitive recursive. 
7. 
Minimalization 
Let P(t, x 1 , â¢â¢â¢ , xn) belong to some given PRC class 'lf. Then by Theorem 
6.1, the function 
y 
u 
g(y,xl, ... ,xn) = E na(P(t,Xpâ¢â¢â¢,xn)) 
u=Ot=O 
also belongs to 'lf. (Recall that the primitive recursive function a was 
defined in Section 4.) Let us analyze this function g. Suppose for definite-
ness that for some value of t0 :::;; y, 
for 
t < t0 , 
but 
P(to' XI' ... ' xn) = 1, 
i.e., that t0 is the least value oft :::;; y for which P(t, x 1 , â¢â¢â¢ , xn) is true. Then 
Hence, 
if u < t0 
if u ~ t0 â¢ 
g(y,xl , ... ,xn) = L 1 =to, 
u<t0 
so that g(y, x 1 , â¢â¢â¢ , xn) is the least value oft for which P(t, x, ... , xn) is 
true. Now, we define 
if (3t):s;yP(t,x 1 , â¢â¢â¢ ,xn) 
otherwise. 

56 
Chapter 3 Primitive Recursive Functions 
Thus, min,, YP(t, x 1 , â¢â¢â¢ , xn) is the least value of t :::;; y for which 
P(t, x 1 , â¢â¢â¢ , xn) is true, if such exists; otherwise it assumes the (default) value 
0. Using Theorems 5.4 and 6.3, we have 
Theorem 7.1. If P(t, x 1 , â¢â¢â¢ , xn) belongs to some PRC class '?? and 
f(y, XI' â¢â¢â¢ ' xn) = min, :5 YP(t, XI' â¢â¢â¢ ' xn), then f also belongs to ~-
The operation "min 1 , y" is called bounded minimalization. 
Continuing our list: 
14. lx I y J 
lxjy J is the "integer part" of the quotient xjy. For example, l7 /2J = 3 
and l2/3J = 0. The equation 
lxjyj = min[(t + 1) Â·y > x] 
1:5X 
shows that lxjy J is primitive recursive. Note that according to this equa-
tion, we are taking lx/OJ = 0. 
15. 
R(x,y) 
R(x, y) is the remainder when x is divided by y. Since 
x 
R(x,y) 
- = lxjyj + --
y 
y 
we can write 
R(x,y) =x ..:..(yÂ·lxjyJ), 
so that R(x, y) is primitive recursive. [Note that R(x, 0) = x.] 
16. 
Pn 
Here, for n > 0, Pn is the nth prime number (in order of size). So that Pn 
be a total function, we set Po = 0. Thus, Po = 0, p 1 = 2, p 2 = 3, p 3 = 5, 
etc. 
Consider the recursion equations 
Po= 0, 
Pn+ 1 = 
min [Prime(t) & t > Pn]. 
I:Spn! +I 
To see that these equations are correct we must verify the inequality 
(7.1) 

7. Minimalization 
57 
To do so note that for 0 < i ~ n we have 
(pn)!+1 
1 
----=K+ -, 
Pi 
Pi 
where K is an integer. Hence (pn)! + 1 is not divisible by any of the 
primes p 1 , p 2 , â¢â¢â¢ , Pn. So, either (pn)! + 1 is itself a prime or it is divisible 
by a prime > Pn. In either case there is a prime q such that Pn < q ~ 
(pn)! + 1, which gives the inequality (7.1). (This argument is just Euclid's 
proof that there are infinitely many primes.) 
Before we can confidently assert that Pn is a primitive recursive func-
tion, we need to justify the interleaving of the recursion equations with 
bounded minimalization. To do so, we first define the primitive recursive 
function 
h(y,z) = min[Prime(t)& t > y]. 
I:SZ 
Then we set 
k(x) = h(x, x! + 1), 
another primitive recursive function. Finally, our recursion equations 
reduce to 
Po= 0, 
Pn+ 1 = k(pn), 
so that we can conclude finally that Pn is a primitive recursive function. 
It is worth noting that by using our various theorems (and appropriate 
macro expansions) we could now obtain explicitly a program of ..:7 which 
actually computes Pn . Of course the program obtained in this way would 
be extremely inefficient. 
Now we want to discuss minimalization when there is no bound. We 
write 
minP(x1 , â¢â¢â¢ , xn, y) 
y 
for the least value of y for which the predicate P is true if there is one. If 
there is no value of y for which P( x 1 , â¢ â¢ â¢ , x n , y) is true, then 
miny P(x 1 , â¢â¢â¢ , xn, y) is undefined. (Note carefully the difference with 
bounded minimalization.) Thus unbounded minimalization of a predicate 
can easily produce a function which is not total. For example, 
x- y = min [y + z = x] 
z 

58 
Chapter 3 Primitive Recursive Functions 
is undefined for x < y. Now, as we shall see later, there are primitive 
recursive predicates P(x, y) such that miny P(x, y) is a total function 
which is not primitive recursive. However, we can prove 
Theorem 7.2. If P(x 1 , ... ,xn,y) is a computable predicate and if 
g(xl ' ... ' xn) = minP(xl ' ... ' Xn 'y), 
y 
then g is a partially computable function. 
Proof. The following program obviously computes g: 
Exercises 
[A] 
IFP(Xt, ... ,Xn,Y)GOTOE 
Y+-Y+1 
GOTOA 
â¢ 
1. 
Let h(x) be the integer n such that n :::;; fix < n + 1. Show that h(x) 
is primitive recursive. 
2. 
Do the same when h(x) is the integer n such that 
n :::;; (1 + fi)x < n + 1. 
3. p is called a larger twin prime if p and p - 2 are both primes. (5, 7, 13, 
19 are larger twin primes.) Let T(O) = 0, T(n) = the nth larger twin 
prime. It is widely believed, but has not been proved, that there are 
infinitely many larger twin primes. Assuming that this is true prove 
that T(n) is computable. 
4. Let u(n) be the nth number in order of size which is the sum of two 
squares. Show that u(n) is primitive recursive. 
5. 
Let R(x, t) be a primitive recursive predicate. Let 
g(x,y) =max R(x,t), 
1:5.y 
i.e., g(x, y) is the largest value of-t :::;; y for which R(x, t) is true; if 
there is none, g(x, y) = 0. Prove that g(x, y) is primitive recursive. 
6. Let gcd(x, y) be the greatest common divisor of x and y. Show that 
gcd(x, y) is primitive recursive. 
7. 
Let lcm(x, y) be the least common multiple of x and y. Show that 
lcm(x, y) is primitive recursive. 

8. Pairing Functions and G6del Numbers 
59 
8. 
Give a computable predicate P(x 1 , â¢â¢â¢ , xn, y) such that the function 
min Y P(x 1 , â¢â¢â¢ , x n, y) is not computable. 
9.* A function is elementary if it can be obtained from the functions s, n, 
u), + , ..:... by a finite sequence of applications of composition, bounded 
summation, and bounded product. (By application of bounded summa-
tion we mean obtaining the function r.r-o f(t, x 1 , â¢â¢â¢ , xn) from 
f(t, x 1 , â¢â¢â¢ , xn), and similarly for bounded product.) 
(a) Show that every elementary function is primitive recursive. 
(b) Show that x Â· y, xY, and x! are elementary. 
(c) 
Show that if n + 1-ary predicates P and Q are elementary, then 
so are 
- P, P V Q, 
P & Q, ('Vt), YP(t, x 1 , â¢â¢â¢ , xn), 
(3t),; YP(t, x, ' ... ' xn), and min,,; YP(t, x,' ... ' xn). 
(d) Show that Prime(x) is elementary. 
(e) 
Let the binary function exp/x) be defined 
exp0(x) = x 
expy+ ,(x) = 2exp,(x). 
Show that for every elementary function f(x 1 , â¢â¢â¢ , xn), there is a 
constant k such that f(x 1 , â¢â¢â¢ , xn) ~ expk(max{x1 , â¢â¢â¢ , xn}). [Hint: 
Show that for every n there is an m ~ n such that x Â· expn(x) ~ 
expm(x) for all x.] 
(f) 
Show that exp/x) is not elementary. Conclude that the class of 
elementary functions is a proper subset of the class of primitive 
recursive functions. 
8. 
Pairing Functions and Godel Numbers 
In this section we shall study two convenient coding devices which use 
primitive recursive functions. The first is for coding pairs of numbers by 
single numbers, and the second is for coding lists of numbers. 
We define the primitive recursive function 
(x, y) = 2x(2y + 1) ..:... 1. 
Note that 2x(2y + 1) -=1= 0 so 
(x,y) + 1 = 2x(2y + 1). 
If z is any given number, there is a unique solution x, y to the equation 
(x,y)=z, 
(8.1) 

60 
Chapter 3 Primitive Recursive Functions 
namely, x is the largest number such that 2x I (z + 1), and y is then the 
solution of the equation 
2y + 1 = (z + l)j2x; 
this last equation has a (unique) solution because (z + 1)/2x must be odd. 
(The twos have been "divided out.") Equation (8.1) thus defines functions 
x = /(z), 
y = r(z). 
Since Eq. (8.1) implies that x, y < z + 1 we have 
/(z) :::;; z, 
r(z) :::;; z. 
Hence we can write 
/{z) = min [(3y), z(z = (x, y))], 
X$Z 
r(z) = min[(3x)<z(z = (x,y))], 
y,;;z 
-
so that /(z), r{z) are primitive recursive functions. 
The definition of /(z), r(z) can be expressed by the statement 
(x,y) = z =x = /(z)& y = r(z). 
We summarize the properties of the functions (x,y), /(z), and r(z) in 
Theorem 8.1 (Pairing Function Theorem). The functions (x, y ), /(z), and 
r(z) have the following properties: 
1. they are primitive recursive; 
2. l((x,y)) =x,r((x,y)) =y; 
3. (/{z), r(z)) = z; 
4. /(z), r(z):::;; z. 
We next obtain primitive recursive functions that encode and decode 
arbitrary finite sequences of numbers. The method we use, first employed 
by Godel, depends on the prime power decomposition of integers. 
We define the Godel number of the sequence (a 1 , â¢â¢â¢ , an) to be the 
number 
n 
[al, Â·Â·Â·,an] = 0Pf'. 
i=l 
Thus, the Godel number of the sequence (3, 1, 5, 4, 6) is 
[3, 1,5,4,6] = 23 .31 â¢ 55 .74 Â·116 â¢ 
For each fixed n, the function [a1 , â¢â¢â¢ , an] is clearly primitive recursive. 

8. Pairing Functions and Godel Numbers 
61 
Godel numbering satisfies the following uniqueness property: 
Theorem 8.2. If [a 1 , â¢â¢â¢ , an] = [b 1 , â¢â¢â¢ , bn], then 
i=1, ... ,n. 
This result is an immediate consequence of the uniqueness of the 
factorization of integers into primes, sometimes referred to as the unique 
factorization theorem or the fundamental theorem of arithmetic. (For a 
proof, see any elementary number theory textbook.) 
However, note that 
(8.2) 
because p~ + 1 = 1. This same result obviously holds for any finite number 
of zeros adjoined to the right end of a sequence. In particular, since 
1 = 2Â° = 2Â°3Â° = 2Â°3Â°5Â° = ... ' 
it is natural to regard 1 as the Godel number of the "empty" sequence of 
length 0, and it is useful to do so. 
If one adjoins 0 to the left end of a sequence, the Godel number of the 
new sequence will not be the same as the Godel number of the original 
sequence. For example, 
[2, 3] = 22 â¢ 33 = 108, 
and 
[2,3,0] = 22 .33 â¢ 5Â°= 108, 
but 
[0,2,3] = 2Â°.3 2 â¢ 53 = 1125. 
We will now define a primitive recursive function (x); so that if 
X = [a 1 , â¢â¢â¢ , an], 
then (x); =a;. We set 
(x); = min(-pf+ 1 lx). 
t,;x 
Note that (x)0 = 0, and (0); = 0 for all i. 
We shall also use the primitive recursive function 
Lt(x) = min{(x);-=!= O&(Vj)sx(j ::5; i v (x)j = 0)). 
i,;x 
(Lt stands for "length.") Thus, if x = 20 = 22 Â·51 = [2, 0, 1], then (x)3 = 1, 
but (x)4 = (x)5 = ... = (x)20 = 0. So, Lt(20) = 3. Also, Lt(O) = Lt(l) = 0. 

62 
Chapter 3 Primitive Recursive Functions 
If x > 1, and Lt(x) = n, then Pn divides x but no prime greater than Pn 
divides x. Note that Lt([a1 , â¢â¢â¢ , an]) = n if and only if an =I= 0. 
We summarize the key properties of these primitive recursive functions. 
Theorem 8.3 (Sequence Number Theorem). 
{ a. 
a. ([a]' ... ' an])i = 
o' 
if 1 ~ i ~ n 
otherwise. 
b. [(x) 1 , â¢â¢â¢ ,(x)n] =x 
if n ~ Lt(x). 
Our main application of these coding techniques is given in the next 
chapter. The following exercises indicate that they can also be used to 
show that PRC classes are closed under various interesting and useful 
forms of recursion. 
Exercises 
1. Let f(x 1 , â¢â¢â¢ , xn) be a function of n variables, and let f'(x) be a unary 
function defined so that f'([xl' ... ' xn]) = f(xl' ... ' xn) for all 
x 1 , â¢â¢â¢ , xn. Show that f' is partially computable if and only if f is 
partially computable. 
2. Define Sort([x1 , â¢â¢â¢ , xn]) = [y 1 , â¢â¢â¢ , Yn], where y 1 , â¢â¢â¢ , Yn is a permu-
tation of x1 , â¢â¢â¢ , xn such that y 1 ~ y2 ~ â¢â¢â¢ ~ Yn. Show that Sort(x) is 
primitive recursive. 
3. Let F(O) = 0, F(l) = 1, F(n + 2) = F(n + 1) + F(n). [F(n) is the 
nth so-called Fibonacci number.] Prove that F(n) is primitive recur-
sive. 
4. (Simultaneous Recursion) Let 
h 1(x,O) =f1(x), 
h 2(x,O) =f2(x), 
h1(x, t + 1) = g1(x, h 1(x, t), h 2(x, t)), 
hz{x, t + 1) = gz(x, h 1(x, t), hz{x, t)). 
Prove that if / 1 , / 2 , g 1 , g 2 all belong to some PRC class ~, then h 1 , h 2 
do also. 
5.* (Course-of-Values Recursion) 
(a) For f(n) any function, we write 
j{O) = 1,j{n) = [f(O),f(l), ... ,f(n- 1)] if n =I= 0. 

8. Pairing Functions and GOdel Numbers 
Let 
f(n) = g(n,j(n)) 
for all n. Show that if g is primitive recursive so is f. 
(b) Let 
f(O) = 1, 
f(l) = 4, 
/(2) = 6, 
f(x + 3) = f(x) + f(x + 1)2 + f(x + 2)3 â¢ 
Show that f(x) is primitive recursive. 
(c) 
Let 
h(O) = 3 
X 
h(x + 1) = L, h(t). 
t=O 
Show that h is primitive recursive. 
6.* (Unnested Double Recursion) Let 
f(O,y) =gl(y) 
f(x + 1,0) =gz(x) 
f(x + 1,y + 1) = h(x,y,f(x,y + 1),/(x + 1,y)). 
63 
Show that if g 1 , g2 , and hall belong to some PRC class I&', then f also 
belongs to ~-


4 
A Universal Program 
1. Coding Programs by Numbers 
We are going to associate with each program 9' of the language .9' a 
number, which we write #(9'), in such a way that the program can be 
retrieved from its number. To begin with we arrange the variables in order 
as follows: 
Y XI zi X 2 Z 2 X 3 Z 3 â¢â¢â¢â¢ 
Next we do the same for the labels: 
AI BI CI DI EI Az Bz Cz Dz Ez A3 ... . 
We write #(V), #(L) for the position of a given variable or label in the 
appropriate ordering. Thus #(X2 ) = 4, #(ZI) = #(Z) = 3, #(E)= 5, 
#(B2 ) = 7. 
Now let I be an instruction (labeled or unlabeled) of the language .9'. 
Then we write 
#(/) =(a, (b, c)) 
where 
1. if I is unlabeled, then a = 0; if I is labeled L, then a = #(L); 
2. if the variable V is mentioned in I, then c = #(V) - 1; 
65 

66 
Chapter 4 A Universal Program 
3. if the statement in I is 
v~v 
or 
v~ V+ 1 
or 
v~ v-1, 
then b = 0 or 1 or 2, respectively; 
4. if the statement in I is 
IF V-=!= 0 GOTO L' 
then b = #(L') + 2. 
Some examples: 
The number of the unlabeled instruction X ~ X + 1 is 
(0,(1,1)) = (0,5) = 10, 
whereas the number of the instruction 
[A] 
x~x+ 1 
is 
(1,(1,1)) = (1,5) = 21. 
Note that for any given number q there is a unique instruction I with 
#(/) = q. We first calculate l(q). If l(q) = 0, I is unlabeled; otherwise I 
has the l(q )th label in our list. To find the variable mentioned in I, we 
compute i = r(r(q)) + 1 and locate the ith variable V in our list. Then, 
the statement in I will be 
v~ v 
if l(r(q)) = 0, 
v~ V+ 1 
if l(r(q)) = 1, 
v~ v-1 
if l(r(q)) = 2, 
IF V-=t= OGOTO L 
if 
j = l(r(q)) - 2 > 0 
and L is the jth label in our list. 
Finally, let a program go consist of the instructions I1 , I 2 , â¢â¢â¢ , Ik. Then 
we set 
(1.1) 
Since Godel numbers tend to be very large, the number of even rather 
simple programs usually will be quite enormous. We content ourselves 
with a simple example: 
[A] 
x~x+ 1 
IF X -=I= OGOTOA 

1. Coding Programs by Numbers 
67 
The reader will recognize this as the example given in Chapter 2 of a 
program that computes the nowhere defined function. Calling these in-
structions / 1 and / 2 , respectively, we have seen that #(/1) = 21. Since / 2 
is unlabeled, 
#(/2 ) = (0, (3, 1)) = (0,23) = 46. 
Thus, finally, the number of this short program is 
221 â¢ 346 -
1. 
Note that the number of the unlabeled instruction Y ~ Y is 
(0,(0,0)) = (0,0) = 0. 
Thus, by the ambiguity in Godel numbers [recall Eq. (8.2), Chapter 3], the 
number of a program will be unchanged if an unlabeled Y ~ Y is tacked 
onto its end. Of course this is a harmless ambiguity; the longer program 
computes exactly what the shorter one does. However, we remove even 
this ambiguity by adding to our official definition of program of .9 the 
harmless stipulation that the final instruction in a program is not permitted to 
be the unlabeled statement Y ~ Y. 
With this last stipulation each number determines a unique program. As 
an example, let us determine the program whose number is 199. We have 
199 + 1 = 200 = 23 â¢ 3Â°. 52 = [3, 0, 2]. 
Thus, if #(.9) = 199, .9 consists of 3 instructions, the second of which is 
the unlabeled statement Y ~ Y. We have 
and 
Thus, the program is 
3 = (2,0) = (2,(0,0)) 
2 = (0,1) = (0,(1,0)). 
[B]Y ~ Y 
Y~Y 
Y~ Y+ 1 
a not very interesting program that computes the function y = 1. 
Note also that the empty program has the number 1 - 1 = 0. 
Exercises 
1. Compute #(.9) for .9 the programs of Exercises 4.1, 4.2, Chapter 2. 
2. Find .9 such that #(.9) = 575. 

68 
Chapter 4 A Universal Program 
2. The Halting Problem 
In this section we want to discuss a predicate HALT(x, y), which we now 
define. For given y, let go be the program such that #(go)= y. Then 
HALT(x, y) is true if 1/J~I)(x) is defined and false if 1/J.J.'>(x) is undefined. To 
put it succinctly: 
HALT(x,y)- program number y eventually halts on input x. 
We now prove the remarkable: 
Theorem 2.1. HALT(x, y) is not a computable predicate. 
Proof. Suppose that HALT(x, y) were computable. Then we could con-
struct the program go: 
[A] IF HALT(X, X) GOTO A 
(Of course go is to be the macro expansion of this program.) It is quite 
clear that go has been constructed so that 
1/l.JP( x) = { ~ndefined 
if 
if 
HALT(x,x) 
- HALT(x, x). 
Let #(go)= y0 â¢ Then using the definition of the HALT predicate, 
HALT(x,y0)- -HALT(x,x). 
Since this equivalence is true for all x, we can set x = y0 : 
HALT(y0 , y0 ) -- HALT(y0 , y0 ). 
But this is a contradiction. 
â¢ 
To begin with, this theorem provides us with an example of a function 
that is not computable by any program in the language ..:7. But we would 
like to go further; we would like to conclude the following: 
There is no algorithm that, given a program of ..:7 and an input to 
that program, can determine whether or not the given program will 
eventually halt on the given input. 
In this form the result is called the unsolvability of the halting problem. We 
reason as follows: if there were such an algorithm, we could use it to check 
the truth or falsity of HALT(x, y) for given x, y by first obtaining program 
tff with #(fff) = y and then checking whether tff eventually halts on input 
x. But we have reason to believe that any algorithm for computing on 

2. The Halting Problem 
69 
numbers can be carried out by a program of .9. Hence this would contradict 
the fact that HALT(x, y) is not computable. 
The last italicized assertion is a form of what has come to be called 
Church's thesis. We have already accumulated some evidence for it, and we 
will see more later. But, since the word algorithm has no general definition 
separated from a particular language, Church's thesis cannot be proved as 
a mathematical theorem. 
In fact, we will use Church's thesis freely in asserting the nonexistence 
of algorithms whenever we have shown that some problem cannot be 
solved by a program of .9. 
In the light of Church's thesis, Theorem 2.1 tells us that there really is 
no algorithm for testing a given program and input to determine whether it 
will ever halt. Anyone who finds it surprising that no algorithm exists for 
such a "simple" problem should be made to realize that it is easy to 
construct relatively short programs (of .9) such that nobody is in a position 
to tell whether they will ever halt. For example, consider the assertion 
from number theory that every even number ~ 4 is the sum of two prime 
numbers. This assertion, known as Goldbach's conjecture, is clearly true for 
small even numbers: 4 = 2 + 2, 6 = 3 + 3, 8 = 3 + 5, etc. It is easy to 
write a program .9' of .9 that will search for a counterexample to 
Goldbach's conjecture, that is, an even number n ~ 4 that is not the sum 
of two primes. Note that the test that a given even number n is a 
counterexample only requires checking the primitive recursive predicate 
- (3x),; n(3y),;n[Prime(x) & Prime(y) & X+ y = n]. 
The statement that .9' never halts is equivalent to Goldbach's conjecture. 
Since the conjecture is still open after 250 years, nobody knows whether 
this program .9' will eventually halt. 
Exercises 
1. Show that HALT(x, x) is not computable. 
2. Let HALT(x, y) be defined 
HALT(x, y) <=>program number y never halts on input x. 
Show that HALT(x, y) is not computable. 
3. Let HALT1(x) be defined HALT1(x) <=> HALT(l(x), r(x)). Show that 
HALT1(x) is not computable. 

70 
Chapter 4 A Universal Program 
4. Prove or disprove: If f(x 1, â¢.. , xn) is a total function such that for 
some constant k, f(xl> ..â¢ , xn) 5. k for all x 1 , â¢â¢â¢ , xn, then f is 
computable. 
5. Suppose we claim that .9 is a program that computes HALT(x, x). 
Give a counterexample that shows the claim to be false. That is, give 
an input x for which .9 gives the wrong answer. 
6. 
Let 
f(x) = {~ if Goldbach's conjecture is true 
otherwise. 
Show that f(x) is primitive recursive. 
3. 
Universality 
The negative character of the results in the previous section might lead 
one to believe that it is not possible to compute in a useful way with 
numbers of programs. But, as we shall soon see, this belief is not justified. 
For each n > 0, we define 
cl>(n)(XI ' â¢â¢â¢ ' Xn 'y) = 1/J.J.n>(xl ' ... ' Xn), 
One of the key tools in computability theory is 
where 
#(.9) = y. 
Theorem 3.1 (Universality Theorem). For each n > 0, the function 
ci><n>(x1 , â¢â¢â¢ , xn, y) is partially computable. 
We shall prove this theorem by showing how to construct, for each 
n > 0, a program ~n which computes ci><n>. That is, we shall have for each 
n > 0, 
The programs ~n are called universal. For example, ~1 can be used to 
compute any partially computable function of one variable, namely, if f(x) 
is computed by a program .9 and y = #(.9), then f(x) = ci>(I>(x, y) = 
1/J~>(x, y). The program ~n will work very much like an interpreter. It 
must keep track of the current snapshot in a computation and by "decod-
ing" the number of the program being interpreted, decide what to do next 
and then do it. 
In writing the programs ~n we shall freely use macros corresponding to 
functions that we know to be primitive recursive using the methods of 
Chapter 3. We shall also freely ignore the rules concerning which letters 
may be used to represent variables or labels of Y. 

3. Universality 
71 
In considering the state of a computation we can assume that all 
variables which are not given values have the value 0. With this under-
standing, we can code the state in which the ith variable in our list has the 
value a; and all variables after the mth have the value 0, by the Godel 
number [a 1 , â¢â¢â¢ , am]. For example, the state 
Y= 0, 
is coded by the number 
[0, 2, 0, 1] = 32 â¢ 7 = 63. 
Notice in particular that the input variables are those whose position in 
our list is an even number. 
Now in the universal programs, we shall allocate storage as follows: 
K will be the number such that the Kth instruction is about to be 
executed; 
S will store the current state coded in the manner just explained. 
We proceed to give the program ~n for computing 
y = <t><n>cx, ' ... ' xn 'xn+ ,). 
We begin by exhibiting ~n in sections, explaining what each part does. 
Finally, we shall put the pieces together. We begin: 
Z +-- Xn+ I+ 1 
n 
s +-- n (pz;)x, 
i=! 
K+--1 
If X 11 + 1 = #(.9), where .9 consists of the instructions / 1 , â¢â¢â¢ , Im, then Z 
gets the value [#(/1), â¢â¢â¢ , #(/m)] [see Eq. (1.1)]. S is initialized as 
[0, X, , 0, X 2 , â¢â¢â¢ , 0, Xn ], which gives the first n input variables their appro-
priate values and gives all other variables the value 0. K, the instruction 
counter, is given the initial value 1 (so that the computation can begin with 
the first instruction). Next, 
[C] IF K = Lt(Z) + 1 v K = 0 GOTO F 
If the computation has ended, GOTO F, where the proper value will be 
output. (The significance of K = 0 will be explained later.) Otherwise, the 
current instruction must be decoded and executed: 
U +-- r((Z)K) 
p +-- Pr(U)+ I 

72 
Chapter 4 A Universal Program 
(Z)K = (a, (b, c)) is the number of the Kth instruction. Thus, U = (b, c) 
is the code for the statement about to be executed. The variable mentioned 
in the Kth instruction is the (c + 1)th, i.e., the (r(U) + l)th, in our list. 
Thus, its current value is stored as the exponent to which P divides S: 
IF l(U) = 0 GOTO N 
IF l(U) = 1 GOTO A 
IF - ( P I S) GOTO N 
IF l(U) = 2 GOTO M 
If l(U) = 0, the instruction is a dummy V ~ V and the computation need 
do nothing to S. If l(U) = 1, the instruction is of the form V ~ V + 1, so 
that 1 has to be added to the exponent on P in the prime power 
factorization of S. The computation executes a GOTO A (for Add). If 
l(U) =/= 0, 1, then the current instruction is either of the form V ~ V- 1 
or IF V =/= 0 GOTO L. In either case, if P is not a divisor of S, i.e., if the 
current value of Vis 0, the computation need do nothing to S. If PIS and 
l(U) = 2, then the computation executes a GOTO M (for Minus), so that 
1 can be subtracted from the exponent to which P divides S. To continue, 
K ~ min [/((Z);) + 2 = l(U)] 
i :s; Lt(Z) 
GOTOC 
If l(U) > 2 and PIS, the current instruction is of the form IF V =/= 0 
GOTO L where V has a nonzero value and L is the label whose position 
in our list is l(U) - 2. Accordingly the next instruction should be the first 
with this label. That is, K should get as its value the least i for which 
l((Z)) = l(U) - 2. If there is no instruction with the appropriate label, K 
gets the value 0, which will lead to termination the next time through the 
main loop. In either case the GOTO C causes a "jump" to the beginning 
of the loop for the next instruction (if any) to be processed. Continuing, 
[M] 
S ~ lS/PJ 
GOTON 
[A] 
s~SÂ·P 
[N] 
K~K+1 
GOTOC 
1 is subtracted or added to the value of the variable mentioned in the 
current instruction by dividing or multiplying S by P, respectively. The 

3. Universality 
Z <-Xn+l + 1 
n 
s <--- n (pz;)x, 
K<---1 
[C] 
IF K = Lt(Z) + 1 v K = 0 GOTO F 
U <--- r((Z)K) 
p <--- Pr(U)+ I 
IF /(U) = 0 GOTO N 
IF /(U) = 1 GOTO A 
IF -(PIS) GOTO N 
IF /(U) = 2 GOTO M 
K <---
min [/((Z);) + 2 = /(U)] 
i,; Lt(Z) 
GOTOC 
[M] 
S <--- lS/PJ 
GOTON 
[A] 
S <--- S Â·P 
[N] 
K <--- K +I 
GOTOC 
[F] 
Y <--- (5)1 
Figure 3.1. Program V", which computes Y = <t><">(X1 , â¢â¢â¢ , X", Xn+ 1). 
73 
instruction counter is increased by 1 and the computation returns to 
process the next instruction. To conclude the program, 
[F] 
Y +- (S) 1 
On termination, the value of Y for the program being simulated is stored 
as the exponent on p/ = 2) in S. We have now completed our description 
of Wn and we put the pieces together in Fig. 3.1. 
For each n > 0, the sequence 
enumerates all partially computable functions of n variables. When we 
want to emphasize this aspect of the situation we write 
It is often convenient to omit the superscript when n = 1, writing 
<PyCx) = <P(x,y) = <f>Ol(x,y). 

74 
Chapter 4 A Universal Program 
A simple modification of the programs :il'n would enable us to prove that 
the predicates 
STP<n>(x1 , â¢â¢â¢ , xn, y, t) = Program number y halts after tor fewer 
steps on inputs x 1 , â¢â¢â¢ , xn 
= There is a computation of program y of 
length :::;; t + 1, beginning with inputs 
x1 , â¢â¢â¢ ,xn 
are computable. We simply need to add a counter to determine when we 
have simulated t steps. However, we can prove a stronger result. 
Theorem 3.2 (Step-Counter Theorem). For each n > 0, the predicate 
STP<n>(x1 , â¢â¢ :, xn, y, t) is primitive recursive. 
Proof. The idea is to provide numeric versions of the notions of snapshot 
and successor snapshot and to show that the necessary functions are 
primitive recursive. We use the same representation of program states that 
we used in defining the universal programs, and if z represents state u, 
then (i, z) represents the snapshot (i, u ). 
We begin with some functions for extracting the components of the ith 
instruction of program number y: 
LABEL(i, y) = l((y + 1);) 
VAR(i,y) = r(r((y + 1);)) + 1 
INSTR(i, y) = l(r((y + 1);)) 
LABEL'(i, y) = l(r((y + 1);))..:... 2 
Next we define some predicates that indicate, for program y and the 
snapshot represented by x, which kind of action is to be performed next. 
SKIP(x, y) = [INSTR(/(x), y) = 0 & l(x):::;; Lt(y + 1)] 
V [INSTR(/(x), y) ~ 2 & -{PvAR(I(x),y) I r(x))] 
INCR(x,y) = INSTR(/(x),y) = 1 
DECR(x, y) = INSTR(/(x), y) = 2 &pvAR(I(x),y) I r(x) 
BRANCH(x,y) = INSTR(/(x),y) > 2&pvAR(I(x),y)lr(x) 
& (3i), Lt(y+ 1>LABEL(i, y) = LABEL'(/(x), y) 

3. Universality 
75 
Now we can define SUCC(x, y), which, for program number y, gives the 
representative of the successor to the snapshot represented by x. 
SUCC(x,y) = 
We also need 
(/(x) + 1, r(x)) 
(/(x) + 1, r(x) Â· PvAR(I(x),y)) 
(/(x) + 1, lr(x)/PvAR(I(x),y)J) 
(min;, Lt(y + 1>[LABEL(i, y) = LABEL' 
(Lt(y + 1) + 1, r(x)) 
n 
INIT<n>(x1 , â¢â¢â¢ , xn) = (1, 0 (p2;)x'), 
i=l 
if SKIP( x, y) 
if INCR(x, y) 
if DECR(x, y) 
(/(x),y)], r(x)) 
if BRANCH(x, y) 
otherwise. 
which gives the representation of the initial snapshot for inputs x 1 , â¢â¢â¢ , xn, 
and 
TERM(x,y) <=>l(x) > Lt(y + 1), 
which tests whether x represents a terminal snapshot for program y. 
Putting these together we can define a primitive recursive function that 
gives the numbers of the successive snapshots produced by a given pro-
gram. 
SNAP<n>(x 1 , â¢â¢â¢ , xn, y, 0) = INIT<n>(x1 , â¢â¢â¢ , xn) 
SNAP<n>(x1 , â¢â¢â¢ , xn, y, i + 1) = SUCC(SNAP<n>(x1 , â¢â¢â¢ , xn, y, i), y) 
Thus, 
STP<n>(x 1 , â¢â¢â¢ , Xn, y, t) <=> TERM(SNAP<n>(x1 , â¢â¢â¢ , Xn, y, t), y), 
and it is clear that STP<n>(x 1 , â¢â¢â¢ , xn, y, t) is primitive recursive. 
â¢ 
By using the technique of the above proof, we can obtain the following 
important result. 
Theorem 3.3 (Normal Form Theorem). Let f(x 1 , â¢â¢â¢ , xn) be a partially 
computable function. Then there is a primitive recursive predicate 
R(x1 , â¢â¢â¢ , xn, y) such that 

76 
Chapter 4 A Universal Program 
Proof. 
Let Yo be the number of a program that computes f(x 1 , â¢â¢â¢ , xn). 
We shall prove the following equation, which clearly implies the desired 
result: 
where R(x1 , â¢â¢â¢ , xn, z) is the predicate 
STP<n>(x1 , â¢â¢â¢ , Xn, Yo, r(z)) 
& (r(SNAP<n>(x 1 , â¢â¢â¢ , xn, Yo, r(z))))I 
= /(z). 
(3.1) 
First consider the case when the righthand side of this equation is 
defined. Then, in particular, there exists a number z such that 
STP<n>(x1 , â¢â¢â¢ ,xn,Yo,r(z)) 
and (r(SNAP<n>(x1 , â¢â¢â¢ , xn, Yo, r(z))))I 
= /(z). 
For any such z, the computation by the program with number y0 has 
reached a terminal snapshot in r(z) or fewer steps and /(z) is the value 
held in the output variable Y, i.e., /(z) = f(x 1 , â¢â¢â¢ , xn). 
If, on the other hand, the right side is undefined, it must be the case that 
STP<n>(x 1 , â¢â¢â¢ ,xn,Yo,t) is false for all values oft, i.e., f(x 1 , â¢â¢â¢ ,xn)j . â¢ 
The normal form theorem leads to another characterization of the class 
of partially computable functions. 
Theorem 3.4. A function is partially computable if and only if it can be 
obtained from the initial functions by a finite number of applications of 
composition, recursion, and minimalization. 
Proof. That every function which can be so obtained is partially com-
putable is an immediate consequence of Theorems 1.1, 2.1, 2.2, 3.1, and 7.2 
in Chapter 3. Note that a partially computable predicate is necessarily 
computable, so Theorem 7.2 covers all applications of minimalization to a 
predicate obtained as described in the theorem. 
Conversely, we can use the normal form theorem to write any given 
partially computable function in the form 
t( minR(x 1 , â¢.â¢ , xn, y) ), 
y 
where R is a primitive recursive predicate and so is obtained from the 
initial functions by a finite number of applications of composition and 

3. Universality 
77 
recursion. Finally, our given function is obtained from R by one use of 
minimalization and then by composition with the primitive recursive func-
~nL 
â¢ 
When miny R(x 1 , â¢â¢â¢ , xn, y) is a total function [that is, when for each 
x 1 , â¢â¢â¢ , xn there is at least one y for which R(x 1 , â¢â¢â¢ , xn, y) is true], we say 
that we are applying the operation of proper minimalization to R. Now, if 
t( minR(x 1 , â¢â¢â¢ , xn, y)) 
y 
is total, then miny R(x 1 , â¢â¢â¢ , xn, y) must be total. Hence we have 
Theorem 3.5. A function is computable if and only if it can be obtained 
from the initial functions by a finite number of applications of composi-
tion, recursion, and proper minimalization. 
Exercises 
1. Show that for each u, there are infinitely many different numbers v 
such that for all x, <l>ix) = ci>v(x). 
2. 
(a) Let 
if ci>(x, xH 
otherwise. 
Show that H 1(x) is partially computable. 
(b) Let A = {a 1 , â¢â¢â¢ , an} be a finite set such that <I>( a;, a) j for 
1 ::; i ::; n, and let 
if ci>(x, xH 
if X EA 
otherwise. 
Show that H 2(x) is partially computable. 
(c) 
Give an infinite set B such that ci>(b, b) j for all b E B and such 
that 
is partially computable. 
ifcl>(x,x)~ 
if X E B 
otherwise 

78 
Chapter 4 A Universal Program 
(d) Give an infinite set C such that cl>(c, c)j for all c E C and such 
that 
is not partially computable. 
if cl>(x, x)J, 
if X E C 
otherwise 
3. Give a program .9J such that H,9 (x1 , x2), defined 
H,9 (x1 , x2 ) = program .9J eventually halts on inputs x1 , x 2 
is not computable. 
4. Let f(x 1 , â¢â¢â¢ , xn) be computed by program .9, and suppose that for 
some primitive recursive function g(x1 , â¢â¢â¢ , xn), 
is true for all x 1 , â¢â¢â¢ , xn. Show that f(x 1 , â¢â¢â¢ , xn) is primitive recursive. 
5.* Give a primitive recursive function counter(x) such that if ci>n is a 
computable predicate, then 
cl>/counter(n)) =- HALT(counter(n),counter(n)). 
That is, counter(n) is a counterexample to the possibility that ci>n 
computes HALT(x, x). [Compare this exercise with Exercise 2.5.] 
6. * Give an upper bound on the length of the shortest ..:7 program that 
computes the function ci>Y(x). 
4. 
Recursively Enumerable Sets 
The close relation between predicates and sets, as described in Chapter 1, 
lets us use the language of sets in talking about solvable and unsolvable 
problems. For example, the predicate HALT(x, y) is the characteristic 
function of the set {(x, y) E N 2 I HALT(x, y)}. To say that a set B, where 
B ~ Nm, belongs to some class of functions means that the characteristic 
function P(x1 , â¢â¢â¢ , xm) of B belongs to the class in question. Thus, in 
particular, to say that the set B is computable or recursive is just to say 
that P(x1 , â¢â¢â¢ , xm) is a computable function. Likewise, B is a primitive 
recursive set if P(x1 , â¢â¢â¢ , xm) is a primitive recursive predicate. 

4. Recursively Enumerable Sets 
79 
We have, for example, 
Theorem 4.1. Let the sets B, C belong to some PRC class %'. Then so do 
the sets B u C, B n C, B. 
Proof. This is an immediate consequence of Theorem 5.1, Chapter 3 . â¢ 
As long as the Godel numbering functions [x 1 , â¢â¢â¢ , xn] and (x); are 
availaole, we can restrict our attention to subsets of N. We have, for 
example, 
Theorem 4.2. Let %' be a PRC class, and let B be a subset of Nm, 
m ~ 1. Then B belongs to %' if and only if 
B' = {[x 1 , â¢â¢â¢ ,xm] ENI(x 1 , â¢â¢â¢ ,xm) EB} 
belongs to %'. 
Proof. If Pix1 , â¢â¢â¢ , xm) is the characteristic function of B, then 
is the characteristic function of B', and PB' clearly belongs to ~ if P8 
belongs to %'. On the other hand, if PB'(x) is the characteristic function of 
B', then 
is the characteristic function of B, and P8 clearly belongs to ~ if PB' 
belongs to ~-
â¢ 
It immediately follows, for example, that {[x, y] E N I HAL T(x, y )} is 
not a computable set. 
Definition. The set B c N is called recursively enumerable if there is a 
partially computable function g(x) such that 
B={xENig(xH}. 
(4.1) 
The term recursively enumerable is usually abbreviated r.e. A set is 
recursively enumerable just when it is the domain of a partially com-
putable function. If .9' is a program that computes the function g in (4.1), 
then B is simply the set of all inputs to .9' for which .9' eventually halts. If 
we think of .9' as providing an algorithm for testing for membership in B, 
we see that for numbers that do belong to B, the algorithm will provide a 

80 
Chapter 4 A Universal Program 
"yes" answer; but for numbers that do not, the algorithm will never 
terminate. If we invoke Church's thesis, r.e. sets B may be thought of 
intuitively as sets for which there exist algorithms related to B as in the 
previous sentence, but without stipulating that the algorithms be expressed 
by programs of the language .9'. Such algorithms, sometimes called semi-
decision procedures, provide a kind of "approximation" to solving the 
problem of testing membership in B. 
We have 
Theorem 4.3. If B is a recursive set, then B is r.e. 
Proof. Consider the program .9: 
[A] IF - (X E B) GOTO A 
Since B is recursive, the predicate x E B is computable and .9 can be 
expanded to a program of .9'. Let .9 compute the function h(x). Then, 
clearly, 
B = {x E N I h(x H}. 
â¢ 
If B and B are both r.e., we have a pair of algorithms that will terminate 
in case a given input is or is not in B, respectively. We can think of 
combining these two algorithms to obtain a single algorithm that will 
always terminate and that will tell us whether a given input belongs to B. 
This combined algorithm might work by "running" the two separate 
algorithms for longer and longer times until one of them terminates. This 
method of combining algorithms is called dovetailing, and the step-counter 
theorem enables us to use it in a rigorous manner. 
Theorem 4.4. The set B is recursive if and only if B and B are both r.e. 
Proof. If B is recursive, then by Theorem 4.1 so is ii, and hence by 
Theorem 4.3, they are both r.e. 
Conversely, if B and Bare both r.e., we may write 
B = {x EN I g(x) ~}, 
ii = {x ENih(xH}, 
where g and h are both partially computable. Let g be computed by 
program .9 and h be computed by program tff, and let p = #(.9), 
q = #(tff). Then the program that follows computes B. (That is, the 
program computes the characteristic function of B.) 

4. Recursively Enumerable Sets 
[A] 
IF STP(l>(X, p, T) GOTO C 
IF STP<l)( X, q, T) GOTO E 
T~ T+ 1 
GOTOA 
[C] 
Y~1 
Theorem 4.5. If B and C are r.e. sets so are B u C and B n C. 
Proof. Let 
B = {x EN I g(x) ~}, 
C = {x EN I h(x) ~ }, 
81 
â¢ 
where g and h are both partially computable. Let f(x) be the function 
computed by the program 
y ~ g(X) 
Y ~ h(X) 
Then f(x) is defined if and only if g(x) and h(x) are both defined. Hence 
B n C = {x EN I f(x) ~}, 
so that B n C is also r.e. 
To obtain the result for B u C we must use dovetailing again. Let g and 
h be computed by programs 9' and t2', respectively, and let #(9') = p, 
#(t2') = q. Let k(x) be the function computed by the program 
[A] 
IF STP(l>(X, p, T) GOTO E 
IF STP(l>(x, q, T) GOTO E 
T~ T+ 1 
GOTOA' 
Then k(x) is defined just in case either g(x) or h(x) is defined. That is, 
B u C = {x EN I k(xH}. 
Definition. We write 
W, = {x EN I <l>(x, nH}. 
Then we have 
â¢ 
Theorem 4.6 (Enumeration Theorem). A set B is r.e. if and only if there 
is an n for which B = W, . 

82 
Chapter 4 A Universal Program 
Proof. This is an immediate consequence of the definition of <l>(x, n) . â¢ 
The theorem gets its name from the fact that the sequence 
is an enumeration of all r.e. sets. 
We define 
K = {n EN In E W,}. 
Now, 
n E W, <=> <l>(n, n) t <=> HALT(n, n). 
Thus, K is the set of all numbers n such that program number n 
eventually halts on input n. We have 
Theorem 4.7. 
K is r.e. but not recursive. 
Proof. Since 
K = {n E N I <l>(n, n) t} and (by the universality 
theorem-Theorem 3.1), <l>(n, n) is certainly partially computable, K is 
clearly r.e. If K were also r.e., by the enumeration theorem we would have 
K=W; 
for some i. Then 
i E K <=> i E W; <=> i E K, 
which is a contradiction. 
â¢ 
Actually the proof of Theorem 2.1 already shows not only that 
HALT(x, z) is not computable, but also that HALT(x, x) is not com-
putable, i.e., that K is not a recursive set. (This was Exercise 2.1.) 
We conclude this section with some alternative ways of characterizing 
r.e. sets. 
Theorem 4.8. Let B be an r.e. set. Then there is a primitive recursive 
predicate R(x, t) such that B = {x EN l(3t)R(x, t)}. 
Proof. Let B = W,. Then B = {x EN l(3t)STP<0(x, n, t)}, and STPO> is 
primitive recursive by Theorem 3.2. 
â¢ 
Theorem 4.9. Let S be a nonempty r.e. set. Then there is a primitive 
recursive function f(u) such that S = {f(n) I n E N} = {f(O), f(l), 
f(2), ... } . That is, S is the range of f. 

4. Recursively Enumerable Sets 
83 
Proof. 
By Theorem 4.8 
S = {xI (3t)R(x, t)}, 
where R is a primitive recursive predicate. Let x 0 be some fixed member 
of S (for example, the smallest). Let 
f(u) = {/(u) 
Xo 
if 
R(l(u), r(u)) 
otherwise. 
Then by Theorem 5.4 in Chapter 3, f is primitive recursive. Each value 
f(u) is in S, since x 0 is automatically in S, while if R(l(u), r(u)) is true, 
then certainly (3t)R(l(u), t) is true, which implies that f(u) = l(u) E S. 
Conversely, if x E S, then R(x, t0 ) is true for some t0 . Then 
f((x, t 0 )) = l((x, t 0 )) = x, 
so that x = f(u) for u = (x, t 0 ). 
â¢ 
Theorem 4.10. 
Let f(x) be a partially computable function and let 
S = {f(x)l f(xH}. (That is, Sis the range of f.) Then Sis r.e. 
Proof. 
Let 
Since 
g(x) = { ~ 
if 
XES 
otherwise. 
S = {xI g(x)!}, 
it suffices to show that g(x) is partially computable. Let .9 be a program 
that computes f and let #(.9) = p. Then the following program computes 
g(x): 
[A] 
IF- STPO)(Z, p, T) GOTO B 
V ~ f(Z) 
IF V=XGOTOE 
[B] 
Z ~ Z + 1 
IF Z ~ T GOTO A 
T~ T+ 1 
z~o 
GOTOA 
Note that in this program the macro expansion of V ~ f(Z) will be 
entered only when the step-counter test has already guaranteed that f is 
defined. 
â¢ 

84 
Chapter 4 A Universal Program 
Combining Theorems 4.9 and 4.10, we have 
Theorem 4.11. Suppose that S -=!= 0. Then the following statements are 
all equivalent: 
1. S is r.e.; 
2. S is the range of a primitive recursive function; 
3. S is the range of a recursive function; 
4. S is the range of a partial recursive function. 
Proof. 
By Theorem 4.9, (1) implies (2). Obviously, (2) implies (3), and (3) 
implies (4). By Theorem 4.10, (4) implies (1). Hence all four statements are 
equivalent. 
â¢ 
Theorem 4.11 provides the motivation for the term recursively enumer-
able. In fact, such a set (if it is nonempty) is enumerated by a recursive 
function. 
Exercises 
1. 
Let B be a subset of Nm, m > 1. We say that B is r.e. if B = 
{(x1, ... , xm) E Nm I g(xto ... , xm)H for some partially computable 
function g(x 1 , â¢â¢â¢ , xm). Let 
Show that B' is r.e. if and only if B is r.e. 
2. 
Let K0 = {(x, y) I x E Wj}. Show that K0 is r.e. 
3. Let f be an n-ary partial function. The graph of f, denoted gr(f), is 
the set {[x1 , â¢â¢â¢ , xn ,f(x1 , â¢â¢â¢ , xn)] I f(x 1 , â¢â¢â¢ , xn)J, }. 
(a) Let W be a PRC class. Prove that if f belongs to W then gr(f) 
belongs to W. 
(b) Prove that if gr (f) is recursive then f is partially computable. 
(c) 
Prove that the recursiveness of gr(f) does not necessarily imply 
that f is computable. 
4. 
Let B = {f(n) In EN}, where f is a strictly increasing computable 
function [i.e., f(n + 1) > f(n) for all n]. Prove that B is recursive. 
5. Show that every infinite r.e. set has an infinite recursive subset. : 
6. 
Prove that an infinite set A is r.e. if and only if A = {f(n) In EN} 
for some one-one computable function f(x). 

5. The Parameter Theorem 
7. Let A, B be sets. Prove or disprove: 
(a) If A u B is r.e., then A and B are both r.e. 
(b) If A ~ B and B is r.e., then A is r.e. 
85 
8. Show that there is no computable function f(x) such that f(x) = 
<l>(x, x) + 1 whenever <l>(x, xH. 
9. (a) Let g(x), h(x) be partially computable functions. Show there is 
a partially computable function f(x) such that f(xH for pre-
cisely those values of x for which either g(xH or h(x)J, (or 
both) and such that when f(xH, either f(x) = g(x) or f(x) = 
h(x). 
(b) Can f be found fulfilling all the requirements of (a) but such 
that in addition f(x) = g(x) whenever g(xH? Proof? 
10. (a) Let A= {y l(3t)P(t,y)}, where P is a computable predicate. 
Show that A is r.e. 
(b) Let B={yl(3t1)Â·Â·Â·(3tn)Q(tl> ... ,tn,y)}, where Q is a com-
putable predicate. Show that B is r.e. 
11. Give a computable predicate R(x, y) such that {y I(Vt)R(t, y)} is not 
r.e. 
5. The Parameter Theorem 
The parameter theorem (which has also been called the iteration theorem 
and the s-m-n theorem) is an important technical result that relates the 
various functions cf><n>(x1 , x2 , â¢â¢â¢ , xn, y) for different values of n. 
Theorem 5.1 (Parameter Theorem). For each n, m > 0, there is a primi-
tive recursive function S;:.(u 1 , Uz, ... , Un, y) such that 
cf>(m+n>(xl ' ... ' Xm 'Ul ' ... ' Un 'y) = cf>(m)(XI ' â¢â¢â¢ ' Xm 's;:.(ul ' ... ' Un 'y)). 
(5.1) 
Suppose that values for variables u 1 , â¢â¢â¢ , un are fixed and we have in 
mind some particular value of y. Then the left side of (5.1) is a partially 
computable function of the m arguments x 1 , â¢â¢â¢ , x m â¢ Letting q be the 
number of a program that computes this function of m variables, we have 

86 
Chapter 4 A Universal Program 
The parameter theorem tells us that not only does there exist such a 
number q, but that it can be obtained from u1 , â¢â¢â¢ , un, y in a computable 
(in fact, primitive recursive) way. 
Proof. The proof is by mathematical induction on n. 
For n = 1, we need to show that there is a primitive recursive function 
S~(u, y) such that 
ci><m+ 1>(x1 , â¢â¢â¢ , Xm, u, y) = ci><m>(x1 , â¢â¢â¢ , Xm, S~(u, y)). 
Here S~(u, y) must be the number of a program which, given m inputs 
x1 , â¢â¢â¢ , xm, computes the same value as program number y does when 
given the m + 1 inputs x1 , â¢â¢â¢ , xm, u. Let .9 be the program such that 
#(.9) = y. Then S~(u, y) can be taken to be the number of a program 
which first gives the variable Xm+ I the value U and then proceeds tO carry 
OUt fiJ. Xm+ 1 Will be given the value U by the program 
~m+l ~ Xm+l + 1} 
. 
u 
xm+l ~ xm+l + 1 
The number of the unlabeled instruction 
is 
(0, (1, 2m + 1)) = 16m+ 10. 
So we may take 
a primitive recursive function. Here the numbers of the instructions of .9J 
which appear as exponents in the prime power factorization of y + 1 have 
been shifted to the primes Pu+Iâ¢Pu+Zâ¢Â·Â·Â·â¢Pu+Lt(y+l>Â· 
To complete the proof, suppose the result known for n = k. Then we 
have 

5. The Parameter Theorem 
87 
using first the result for n = 1 and then the induction hypothesis. But now, 
if we define 
s!+ 1(ul ' ... ' uk 'uk+ I' y) = S!(ul ' ... ' uk 's~+k(uk+ I' y)), 
we have the desired result. 
â¢ 
We next give a sample application of the parameter theorem. It is 
desired to find a computable function g(u, v) such that 
<1>/ <1>/x)) = <l>g(u, ,>(x). 
We have by the meaning of the notation that 
<1>/<1>/x)) = <l>(<l>(x, v), u) 
is a partially computable function of x, u, v. Hence, we have 
<l>u(<l>,.(x)) = <t><3>(x, u, v, z0 ) 
for some number z0 â¢ By the parameter theorem, 
<t><3>(x, u, v, z0 ) = <l>(x, Sf(u, v, z0 )) = <l>s[(u,,Â·,zix). 
Exercises 
1. Given a partially computable function f(x, y ), find a primitive recur-
sive function g(u, v) such that 
2. Show that there is a primitive recursive function g(u, v, w) such that 
<1>(3>(u, v, w, z) = <l>g(u,v,w)(z). 
3. Let us call a partially computable function g(x) extendable if there is a 
computable function f(x) such that f(x) = g(x) for all x for which 
g(xH. Show that there is no algorithm for determining of a given z 
whether or not <l>z(x) is extendable. [Hint: Exercise 8 of Section 4 
shows that <l>(x, x) + 1 is not extendable. Find an extendable function 
k(x) such that the function 
h(x t) = { <l>(x, x) + 1 
' 
k(x) 
is partially computable.] 
if 
<l>(t,t).!. 
otherwise 

88 
Chapter 4 A Universal Program 
4.* A programming system is an enumeration S = {cf>in> I i EN, n > 0} of 
the partially computable functions. That is, for each partially com-
putable function f(x 1 , â¢â¢â¢ , xn) there is an i such that f is cf>in>. 
(a) A programming system S is universal if for each n > 0, the 
function qr<n>, defined 
qr<n>(xl ' ... ' Xn 'i) = cf>in>(xl ' ... ' Xn), 
is partially computable. That is, S is universal if a version of the 
universality theorem holds for S. Obviously, 
{ <l>}n> I i E N, n > 0} 
is a universal programming system. Prove that a programming 
system S is universal if and only if for each n > 0 there is a 
computable function fn such that cf>in> = <I>J:<~> for all i. 
(b) A universal programming system S is acceptable if for each 
n, m > 0 there is a computable function s::,(u 1 , â¢â¢â¢ , un, y) such 
that 
qr<m+n>(xl ' ... ' Xm 'Ut ' â¢.â¢ ' Un 'y) 
= qr<m>(xl ' ... ' Xm 's::,(ul ' ... ' Un 'y)). 
That is, S is acceptable if a version of the parameter theorem 
holds for S. Again, {<I>?> I i EN, n > 0} is obviously an acceptable 
programming system. Prove that S is acceptable if and only if for 
each n > 0 there is a computable function gn such that <l>fn> = 
cf>t(;> for all i. 
6. 
Diagonalization and Reducibility 
So far we have seen very few examples of nonrecursive sets. We now 
discuss two general techniques for proving that given sets are not recursive 
or even that they are not r.e. The first method, diagonalization, turns on 
the demonstration of two assertions of the following sort: 
1. A certain set A can be enumerated in a suitable fashion. 
2. It is possible, with the help of the enumeration, to define an object b 
that is different from every object in the enumeration, i.e., b $.A. 
We sometimes say that b is defined by diagonalizing over A. In some 
diagonalization arguments the goal is simply to find some b $.A. We will 
give an example of such an argument later in the chapter. The arguments 
we will consider in this section have an additional twist: the definition of b 
is such that b must belong to A, contradicting the assertion that we began 

6. Dlagonalization and Reducibility 
89 
with an enumeration of all of the elements in A. The end of the 
argument, then, is to draw some conclusion from this contradiction. 
For example, the proof given for Theorem 2.1 is a diagonalization 
argument that the predicate HALT(x, y), or equivalently, the set 
{(x,y) E N 2 IHALT(x,y)}, 
is not computable. The set A in this case is the class of unary partially 
computable functions, and assertion 1 follows from the fact that .9 
programs can be coded as numbers. For each n, let .9-'n be the program 
with number n. Then all unary partially computable functions occur 
among I/I.J.1l, 1/1})/, ... . We began by assuming that HALT(x, y) is com-
putable, and we wrote a program 9' that computes I/J.J.1>. The heart of the 
proof consisted of showing that I/J.J.1> does not appear among I/J.J.1l, 1/J.J.?, .... 
In particular, we wrote 9' so that for every x, I/J.J.1>(x),l. if and only if 
,/,(1)( ) 
. 
'1'.9' x x i , I.e., 
HALT(x, #(9')) <=>- HALT(x, x), 
so I/J.J.1> differs from each function I/J.J.1l, 1/J.J.?,... on at least one input 
value. That is, n is a counterexample to the possibility that I/J.J.1> is I/J.J.1>, 
since 1/J.~>(n) ~ if and only if 1/J})>(n) i. Now we have the unary partialiy 
computable function 1/J})> that is ~ot among I/J.J.1l, <PJ):, ... , so assertion 2 is 
satisfied, giving us a contradiction. In the proof of Theorem 2.1 the 
contradiction was expressed a bit differently: Because I/J.J.1> is partially 
computable, it must appear among I/J.J.1l, 1/J.J.?, ... , and, in particular, it 
must be I/J.J.1] ~ , since 9'#(.9') is 9' by definition, but we have the counterex-
ample I/J.J.1>(#(9')),l. if and only if I/J.J.1> (#(.9-'))j, i.e., 
#(~) 
HALT(#(9'), #(9')) <=>- HALT(#(9'), #(9')). 
Since we know assertion 1 to be true, and since assertion 2 depended on 
the assumption that HALT(x, y) is computable, HALT(x, y) cannot be 
computable. 
To present the situation more graphically, we can represent the values 
of each function I/J.J.1l, I/J.J.1/, â¢â¢â¢ by the infinite array 
,,,(1)(1) 
'1'.9'o 

90 
Chapter 4 A Universal Program 
Each row represents one function. It is along the diagonal of this array 
that we have arranged to find the counterexamples, which explains the 
origin of the term diagonalization. 
We can use a similar argument to give an example of a non-r.e. set. Let 
TOT be the set of all numbers p such that p is the number of a program 
that computes a total function f(x) of one variable. That is, 
TOT= {zEN I (Vx)<l>(x, z) ~ }. 
Since 
<l>(x,zH =x E ~' 
TOT is simply the set of numbers z such that ~ is the set of all 
nonnegative integers. 
We have 
Theorem 6.1. TOT is not r.e. 
Proof. Suppose that TOT were r.e. Since TOT =I= 0, by Theorem 4.9 
there is a computable function g(x) such that TOT = {g(O), g(1), g(2), ... }. 
Let 
h(x) = <l>(x, g(x)) + 1. 
Since each value g(x) is the number of a program that computes a total 
function, <l>(u, g(x)H for all x, u and hence, in particular, h(xH for all x. 
Thus h is itself a computable function. Let h be computed by program 9', 
and let p = #(9'). Then p E TOT, so that p = g(i) for some i. Then 
h(i) = <l>(i, g(i)) + 1 by definition of h 
= <l>(i, p) + 1 
since p = g(i) 
= h(i) + 1 
since h is computed by 9', 
which is a contradiction. 
â¢ 
Note that in the proof of Theorem 6.1, the set A is TOT itself, and this 
time assertion 1 was taken as an assumption, while assertion 2 is shown to 
be true. Theorem 6.1 helps to explain why we base the study of com-
putability on partial functions rather than total functions. By Church's 
thesis, Theorem 6.1 implies that there is no algorithm to determine if an .9 
program computes a total function. 
Once some set such as K has been shown to be nonrecursive, we can 
use that set to give other examples of nonrecursive sets by way of the 
reducibility method. 

6. Diagonalization and Reducibility 
91 
Definition. Let A, B be sets. A is many-one reducible to B, written 
A ~m B, if there is a computable function f such that 
A= {x EN I f(x) E B}. 
That is, x E A if and only if f(x) E B. (The word many-one simply refers 
to the fact that we do not require f to be one-one.) 
If A ~m B, then in a sense testing membership in A is "no harder 
than" testing membership in B. In particular, to test x E A, we can 
compute f(x) and then test f(x) E B. 
Theorem 6.2. Suppose A ~m B. 
1. If B is recursive, then A is recursive. 
2. If B is r.e., then A is r.e. 
Proof. Let A = {x EN I f(x) E B}, where f is computable, and let Pix) 
be the characteristic function of B. Then 
A = {x EN I P8 (f(x))}, 
and if B is recursive then P8 (f(x)), the characteristic function of A, is 
computable. 
Now suppose that B is r.e. Then B = {x E N I g(x)!} for some partially 
computable function g, and A= {x EN I g(f(x))!}. But g(f(x)) is par-
tially computable, so A is r.e. 
â¢ 
We generally use Theorem 6.2 in the form: If A is not recursive (r.e.), 
then B is not recursive (respectively, not r.e.). For example, let 
K0 is clearly r.e. However, we can show by reducing K to K 0 , that is, by 
showing that K ~m K 0 , that K 0 is not recursive: x E K if and only if 
(x, x) E K 0 , and the function f(x) = (x, x) is computable. In fact, it is 
easy to show that every r.e. set is many-one reducible to K0 : if A is r.e., 
then 
A={xENig(x)!} 
for some partially computable g 
= {x EN I <l>(x, z0H} 
for some z0 
= {x EN I (x, z0 ) E K 0}. 

92 
Chapter 4 A Universal Program 
Definition. A set A is m-complete if 
1. A is r.e., and 
2. for every r.e. set B, B ~m A. 
So K 0 is m-complete. We can also show that K is m-complete. First we 
show that K 0 ~m K. This argument is somewhat more involved because 
K 0 seems, at first glance, to contain more information than K. K 0 
represents the halting behavior of all partially computable functions on all 
inputs, while K represents only the halting behavior of partially com-
putable functions on a single argument. We wish to take a pair (n, q) and 
transform it to a number f( ( n, q)) of a single program such that 
<l>q(nH 
if and only if 
<l>f((n,q))(f( (n, q ))H, 
i.e., such that (n,q) E K 0 if and only if f((n,q)) E K. The parameter 
theorem turns out to be very useful here. Let .9' be the program 
y ~ <f>O>(l(Xz), r(Xz)) 
and let p = #(.9'). Then r/J.'Jl'(x 1 , x 2 ) = <t>0>(l(x2 ), r(x 2 )), and 
I/J.'Jl'(x 1 ,x2 ) = <t><2>(x1 ,x2 ,p) = <t>0>(x 1 ,Sf(x2 ,p)) 
by the parameter theorem, so for any pair (n, q ), 
<t>(l>(n, q) = I/J.'Jl'(x 1 , (n, q)) = <l>~~~(n,q),p)(x 1 ). 
(6.1) 
Now, (6.1) holds for all values of x1 , so, in particular, 
<t>(l>(n, q) = <t>f/~(n,q),p)(Sf((n, q), p)), 
and therefore 
<f>O>(n, q) t 
if and only if 
<l>~~~(n, q), P>(Sf ( (n, q), p)) t, 
i.e., 
(n,q) EK0 
ifandonlyif Sf((n,q),p) EK. 
With p held constant Sf(x, p) is a computable unary function, so K 0 ~m K. 
To complete the argument that K is m-complete we need 
Theorem 6.3. If A ~m Band B ~m C, then A ~m C. 
Proof. Let A = {x E N I f(x) E B} and B = {x EN I g(x) E C}. Then 
A = {x EN I g(f(x)) E C}, and g(f(x)) is computable. 
â¢ 

6. Diagonalization and Reducibility 
93 
As an immediate consequence we have 
Corollary 6.4. If A is m-complete, B is r.e., and A ~m B, then B is 
m-complete. 
Proof. If Cis r.e. then C ~m A, and A ~m B by assumption, so C ~m B . â¢ 
Thus, K is m-complete. Informally, testing membership in an m-com-
plete set is "at least as difficult as" testing membership in any r.e. set. So 
an m-complete set is a good choice for showing by a reducibility argument 
that a given set is not computable. We expand on this subject in Chapter 8. 
Actually, we have shown both K ~m K 0 and K 0 ~m K, so in a sense, 
testing membership in K and testing membership in K 0 are "equally 
difficult" problems. 
Definition. 
A =m B means that A ~m B and B ~m A. 
In general, for sets A and B, if A =m B then testing membership in A 
has the "same difficulty as" testing membership in B. 
To summarize, we have proved 
Theorem 6.5. 
1. K and K 0 are m-complete. 
2. K =m K0â¢ 
We can also use reducibility arguments to show that certain sets are not 
r.e. Let 
EMPTY = {x E N I W., = 0}. 
Theorem 6.6. EMPTY is not r.e. 
Proof. We will show that K ~m EMPTY. K is not r.e., so by Theorem 
6.2, EMPTY is not r.e. Let .9 be the program 
y ~ <I>(Xz' Xz), 
and let p = #(.9) . .9 ignores its first argument, so for a given z, 
1/J.J}>(x, z)! for all x if and only if 
<l>(z, z)!. 
By the parameter theorem 
I/JJ.,Z>(x 1 , x2 ) = <t><2>(x 1 , x2 , p) = <I>(I>(x1 , Sf(x2 , p)), 

94 
Chapter 4 A Universal Program 
so, for any z, 
z E K if and only if 
<I>( z, z) j 
if and only if 
1/J.Ji>(x, z) j for all x 
ifandonlyif <J>O>(x,Sf(z,p))j for all x 
if and only if 
Ws/(z,p) = 0 
if and only if Sf(z,p) E EMPTY. 
f(z) = Sf(z, p) is computable, so K ::=;m EMPTY. 
Exercises 
1. Show that the proof of Theorem 4.7 is a diagonalization argument. 
â¢ 
2. Prove by diagonalization that there is no enumeration / 0 , / 1 , / 2 , â¢â¢â¢ 
of all total unary (not necessarily computable) functions on N. 
3. Let A = {x EN I <l>x(x)J, and <l>x(x) > x}. 
(a) Show that A is r.e. 
(b) Show by diagonalization that A is not recursive. 
4. Show how the diagonalization argument in the proof of Theorem 6.1 
fails for the set of all numbers p such that p is the number of a 
program that computes a partial function, i.e., the set N. 
5. 
Let A, B be sets of numbers. Prove 
(a) 
A ::=;m A. 
(b) 
A ::=;m B if and only if A ::=;m B. 
6. Prove that no m-complete set is recursive. 
7. 
Let A, B be m-complete. Show that A =m B. 
8. Prove that K :1, m K, i.e., K is not many-one reducible to K. 
9. For every number n, let An ={xI n E J.Ji.}. 
(a) Show that A; is r.e. but not recursive, for all i. 
(b) Show that A; =m Aj for all i,j. 
10. Define the predicate P(x) -
<l>x(x) = 1. Show that P(x) is not 
computable. 
11. Define the predicate 
Q(x) -
the variable Y assumes the value 1 sometime dur-
ing the computation of 1/Jg>(x), where #(!JO) = x. 

7. Rice's Theorem 
95 
Show that Q(x) is not computable. [Hint: Use the parameter theorem 
and a version of the universal program W1 .] 
12. Let INF = {x EN I Wr is infinite}. Show that INF =m TOTAL. 
13. Let FIN = {x EN I w. is finite}. Show that K :::;;m FIN. 
14.* Let 
MONOTONE = {y E N I <1>/x) is total and 
<1>/x) :::;; <1>/x + 1) for all x}. 
(a) Show by diagonalization that MONOTONE is not r.e. 
{b) Show that MONOTONE =m TOTAL. 
7. 
Rice's Theorem 
Using the reducibility method we can prove a theorem that gives us, at a 
single stroke, a wealth of interesting unsolvable problems concerning 
programs. 
Let f be some collection of partially computable functions of one 
variable. We may associate with f the set (usually called an index set) 
R r = {t E N I <1>1 E f}. 
Rr is a recursive set just in case the predicate g(t), defined g(t) = <1>1 E f, 
is computable. Consider the examples: 
1. f is the set of computable functions; 
2. r is the set of primitive recursive functions; 
3. f is the set of partially computable functions that are defined for all 
but a finite number of values of x. 
These examples make it plain that it would be interesting to be able to 
show that Rr is computable for various collections f. Invoking Church's 
thesis, we can say that R r is a recursive set just in case there is an 
algorithm that accepts programs go as input and returns the value TRUE 
or FALSE depending on whether or not the function tfJ}p,1 > does or does not 
belong to f. In fact, those who work with computer programs would be 
very pleased to possess algorithms that accept a program as input and 
which return as output some useful property of the partial function 
computed by that program. Alas, such algorithms are not to be found! This 
dismal conclusion follows from Rice's theorem. 

96 
Chapter 4 A Universal Program 
Theorem 7.1 (Rice's Theorem). Let r be a collection of partially com-
putable functions of one variable. Let there be partially computable 
functions f(x), g(x) such that f(x) belongs to r but g(x) does not. Then 
R r is not recursive. 
Proof. Let h(x) be the function such that h(x)j for all x. We assume 
first that h(x) does not belong to f. Let q be the number of 
Z ~ <I>(X2 , X 2 ) 
y ~ f(Xl) 
Then, for any i, Sf (i, q) is the number of 
X 2 ~ i 
Now 
and 
Z ~ <I>(X2 , X 2 ) 
Y ~ f(Xl) 
i E K 
implies 
<l>(i,i)J, 
implies 
<l>s:u. q)(x) = f(x) for all x 
implies 
<l>s/(i,q) E f 
implies 
Sf (i, q) E R r , 
i $. K 
implies 
<l>(i' i)j 
implies 
<l>s/(i.q)(x) i for all x 
implies 
<l>s/u. q> = h 
implies 
<l>s/(i.q) $. r 
implies 
Sf(i,q) $. Rr, 
so K :::;; m R r . By Theorem 6.2, R r is not recursive. 
If h(x) does belong to r, then the same argument with r and f(x) 
replaced by f and g(x) shows that Rr is not recursive. But Rr = Rr, so, 
by Theorem 4.1, Rr is not recursive in this case either. 
â¢ 
Corollary 7.2. There are no algorithms for testing a given program 9' of 
the language .9 to determine whether 1/Jj.l)(x) belongs to any of the classes 
described in Examples 1-3. 
Proof. In each case we only need find the required functions f(x), g(x) 
to show that Rr is not recursive. The corollary then follows by Church's 

8. The Recursion Theorem 
97 
thesis. For 1, 2, or 3 we can take, for example, f(x) = uj(x) and g(x) = 
1 - x [so that g(x) is defined only for x = 0, 1]. 
â¢ 
Exercises 
I. Show that Rice's theorem is false if the requirement for functions 
f(x ), g(x) is omitted. 
2. Show there is no algorithm to determine of a given program .9 in the 
language .9' whether rf1.9(x) = x 2 for all x. 
3. Show that there is no algorithm to determine of a pair of numbers u, v 
whether ci>u(x) = ci>v(x) for all x. 
4. Show that the set A = {x I cl>x is defined for at least one input} is r.e. 
but not recursive. 
5. Use Rice's theorem to show that the following sets are not recursive. 
[See Section 6 for the definitions of the sets.] 
(a) TOT; 
(b) EMPTY; 
(c) 
INF; 
(d) FIN; 
(e) 
MONOTONE; 
(f) 
{y E N I <~>;!) is a predicate}. 
6. Let f be a collection of partially computable functions of m variables, 
m > 1, and let R~m> = {t E N I ci>fm> E f}. State and prove a version of 
Rice's theorem for collections of partially computable functions of m 
variables, m > 1. 
7. Define the predicate 
PROPER(n) <=> minz [ cl>~2>(x, z) = 3] is an application of proper 
minimalization to the predicate 
<1>~2>( x, z) = 3. 
Show that PROPER(x) is not computable. 
8. Let f be a set of partially computable functions of one variable. Show 
that Rr is r.e. if and only if it is m-complete. 
*8. The Recursion Theorem 
In the proof that HALT(x, y) is not computable, we gave (assuming 
HALT(x, y) to be computable) a program .9 such that 
HALT(#(.9), #(.9)) <=>- HALT(#(.9), #(.9)). 

98 
Chapter 4 A Universal Program 
We get a contradiction when we consider the behavior of the program .9 
on input #(.9). The phenomenon of a program acting on its own descrip-
tion is sometimes called self-reference, and it is the source of many 
fundamental results in computability theory. Indeed, the whole point of 
diagonalization in the proof of Theorem 2.1 is to get a contradictory 
self-reference. We turn now to a theorem which packages, so to speak, a 
general technique for obtaining self-referential behavior. It is one of the 
most important applications of the parameter theorem. 
Theorem 8.1 (Recursion Theorem). Let g(z, Xp ... 'xm) be a partially 
computable function of m + 1 variables. Then there is a number e such 
that 
Discussion. Let e = #(.9), so that 
l/l.~m>(x 1
, â¢â¢â¢ , xm) = <l>~m>(x 1 , â¢â¢â¢ , xm). 
The equality in the theorem says that the m-ary function rfJ}p,m>(x 1 , â¢â¢â¢ , xm) 
is equal to g(z, x 1 , â¢â¢â¢ , xm) when the first argument of g is held constant 
at e. That is, .9 is a program that, in effect, gets access to its own number, 
e, and computes the m-ary function g(e, x 1 , â¢â¢â¢ , xm). Note that since 
x 1 , â¢â¢â¢ , xm can be arbitrary values, e generally does not appear among the 
inputs to rfJ}p,m>(x1 , .â¢â¢ , xm), so .9 must somehow compute e. One might 
suppose that .9 might contain e copies of an instruction such as Z +-
Z + 1, that is, an expansion of the macro Z +-- e, but if .9 has at least e 
instructions, then certainly #(.9) > e. The solution is to write .9 so that it 
computes e without having e "built in" to the program. In particular, we 
build into .9 a "partial description" of .9, and then have .9 compute e 
from the partial description. Let t2' be the program 
z +-- S~(Xm+ I' xm+ I) 
Y+-g(Z,X1 , â¢â¢â¢ ,Xm) 
We prefix #(t2') copies of the instruction xm +I +-- xm +I + 1 to get the 
program !Jll: 
z +-- S~(Xm+ I' xm+ I) 
Y+-g(Z,X1 , â¢â¢â¢ ,Xm) 

8. The Recursion Theorem 
99 
After the first #(f2') instructions are executed, Xm+ 1 holds the Value 
#{t2'), and S~(#(t2'), #(t2')), as defined in the proof of the parameter 
theorem, computes the number of the program consisting of #(t2') copies 
of Xm+l ~ Xm+l + 1 followed by program t2'. But that program is 9'1. So 
z ~ S~(Xm+ I' xm+ I) gives z the value #(!Jll), andy~ g(Z, XI' ... ' Xm) 
causes !Jll to output g(#(!Jll), x 1 , â¢â¢â¢ , xm). We take e to be #(!Jll) and we 
have 
We now formalize this argument. 
Proof. Consider the partially computable function 
g(S~(v,v),x 1 , â¢â¢â¢ ,xm) 
where S~ is the function that occurs in the parameter theorem. Then we 
have for some number z0 , 
g(S~(V, V), XI, ... , Xm) = cf>(m+ I)(XI, ... , Xm, V, Zo) 
= cf><m>(x1 , â¢â¢â¢ , xm, S~(v, z0 )), 
where we have used the parameter theorem. Setting v = z0 and e = 
S~(z0 , z0), we have 
g(e,x 1 , â¢â¢â¢ ,xm) = cf><m>(x 1 , â¢â¢â¢ ,xm ,e)= <I>;m>(x1 , â¢â¢â¢ ,xm). 
â¢ 
We can use the recursion theorem to give another self-referential proof 
that HALT(x, y) is not computable. If HALT(x, y) were computable, then 
f(x,y) = { 6 if HALT(y, x) 
otherwise 
would be partially computable, so by the recursion theorem there would be 
a number e such that 
that is, 
<l>e(y) = f(e, y) = { 6 if HALT(y, e) 
otherwise, 
- HALT(y, e)<=> HALT(y, e). 
So HALT(x, y) is not computable. The self-reference occurs when <l>e 
computes e, tests HALT(y, e), and then does the opposite of what 
HALT(y, e) says it does. 

100 
Chapter 4 A Universal Program 
One of the many uses of the recursion theorem is to allow us to write 
down definitions of functions that involve the program used to compute 
the function as part of its definition. For a simple example we give 
Corollary 8.2. There is a number e such that for all x 
<l>e(x) =e. 
Proof. 
We consider the computable function 
g(z, x) = uf(z, x) = z. 
Applying the recursion theorem we obtain a number e such that 
<l>e(x) = g(e, x) = e 
and we are done. 
â¢ 
It is tempting to be a little metaphorical about this result. The program 
with number e "consumes" its "environment" (i.e., the input x) and 
outputs a "copy" of itself. That is, it is, in miniature, a self-reproducing 
organism. This program has often been cited in considerations of the 
comparison between living organisms and machines. 
For another example, let 
if t = 0 
otherwise, 
where g(x, y) is computable. It is clear that f(x, t) is partially computable, 
so by the recursion theorem there is a number e such that 
if t = 0 
otherwise. 
An easy induction argument on t shows that <l>e is a total, and therefore 
computable, function. Now, <l>e satisfies the equations 
<l>e(O) = k 
<l>e(t + 1) = g(t, <l>e(t)), 
that is, <l>e is obtained from g by primitive recursion of the form (2.1) in 
Chapter 3, so the recursion theorem gives us another proof of Theorem 2.1 
in Chapter 3. In fact, the recursion theorem can be used to justify 
definitions based on much more general forms of recursion, which explains 
how it came by its name.1 We give one more example, in which we wish to 
1 For more on this subject, see Part 5. 

8. The Recursion Theorem 
101 
know if there are partially computable functions f, g that satisfy the 
equations 
f(O) = 1 
f(t + 1) = g(2t) + 1 
g(O) = 3 
g(2t + 2) = f(t) + 2. 
Let F(z, t) be the partially computable function 
if X= (0,0) 
(8.1) 
<l>z((1,2(r(x) _:_ 1))) + 1 
F(z,x) = 
3 
{
1 
if (3y), x (x = (0, y + 1)) 
if X= (1,0) 
<l>z((O, l(r(x) ..:... 2)j2J)) + 2 
if(3y),x(x = (1,2y + 2)). 
By the recursion theorem there is a number e such that 
<l>e(x) = F(e, x) 
{
1 
<l>e((1,2(r(x)..:... 1))) + 1 
= 
~e((O, l (r(x) ..:... 2) j2J)) + 2 
if X= (0,0) 
if (3y), x (x = (0, y + 1)) 
if X= (1,0) 
if ( 3 Y) ,; x (X = ( 1, 2 Y + 2)) . 
Now, setting 
we have 
f(x) = <l>e((O,x)) and g(x) = <l>e((1,x)) 
f(O) = <l>e((O, 0)) = 1 
f(t + 1) = <l>e((O,t + 1)) = cf>e((1,2t)) + 1 =g(2t) + 1 
g(O) = <l>e((1,0)) = 3 
g(2t + 2) = cf>e((1,2t + 2)) = <l>e((O,t)) + 2 = f(t) + 2, 
so f, g satisfy (8.1). 
Another application of the recursion theorem is 
Theorem 8.3 (Fixed Point Theorem). Let f(z) be a computable function. 
Then there is a number e such that 
for all x. 

102 
Chapter 4 A Universal Program 
Proof. 
Let g(z, x) = <l>f<z>(x), a partially computable function. By the 
recursion theorem, there is a number e such that 
<l>e(x) = g(e, x) = <l>f(e)(x). 
â¢ 
Usually a number n is considered to be a fixed point of a function f(x) 
if f(n) = n. Clearly there are computable functions that have no fixed 
point in this sense, e.g., s(x). The fixed point theorem says that for every 
computable function f(x), there is a number e of a program that computes 
the same function as the program with number f(e). 
For example, let P(x) be a computable predicate, let g(x) be a com-
putable function, and let while(n) = #(~n), where ~n is the program 
X 2 +--- n 
Y+-X 
[A] 
IF - P(Y) GOTO E 
Y +--- <l>x2(g(Y)) 
It should be clear that while(x) is a computable, in fact primitive recursive, 
function, so by the fixed point theorem there is a number e such that 
<l>e(x) = <l>while(e)(x). 
It follows from the construction of while(e) that 
<l>e(x) = <l>while(e)(x) = { ~e(g(x)) 
if - P(x) 
otherwise. 
Moreover, 
so 
( g(x) 
<l>e(g(x)) = <l>while(e)(g(x)) = 
<l>e(g(g(x))) 
if - P(g(x)) 
otherwise, 
<l>e(x) = <l>while(e)(x) = {;(x) 
<l>e(g(g(x))) 
if -P(x) 
if P(x) &-P(g(x)) 
otherwise, 

8. The Recursion Theorem 
103 
and continuing in this fashion we get 
if - P(x) 
if P(x) & - P(g(x)) 
if P(x) & P(g(x)) & - P(g(g(x))) 
In other words, program e behaves like the pseudo-program 
Y+-X 
WHILE P(Y) DO 
y +--- g(Y) 
END 
We end this discussion of the recursion theorem by giving another proof 
of Rice's theorem. Let r, f(x), g(x) be as in the statement of Theorem 
7.1. 
Alternative Proof of Rice's Theorem.2 Suppose that Rr were computable. 
Let 
if t ERr 
otherwise. 
That is, Pr is the characteristic function of Rr. Let 
( g(x) 
h(t,x)= 
) 
f(x 
if t ERr 
otherwise. 
Then, since (as in the proof of Theorem 5.4, Chapter 3) 
h(t, x) = g(x) Â· Pr(t) + f(x) Â· a(Pr{t)), 
h(t, x) is partially computable. Thus, by the recursion theorem, there is a 
number e such that 
( g(x) 
<l>e(x) = h(e, x) = 
f(x) 
if <l>e belongs to r 
otherwise. 
2 This elegant proof was called to our attention by John Case. 

104 
Chapter 4 A Universal Program 
Does e belong to Rr? Recalling that f(x) belongs to f but g(x) does not, 
we have 
eERr 
implies 
<l>e(x) = g(x) 
implies 
<l>e is not in r 
implies e rt Rr. 
But likewise, 
eft Rr 
implies 
<l>e(x)= f(x) 
implies 
<l>e is in r 
implies e ERr. 
This contradiction proves the theorem. 
â¢ 
Exercises 
1. 
Use the proof of Corollary 8.2 and the discussion preceding the proof 
of the recursion theorem to write a program .9J such that rf19 (x) = 
#(.9). 
2. 
Let A = {x EN I <l>x(x)J, and <l>x(x) > x}. Use the recursion theo-
rem to show that A is not recursive. 
3. Show that there is a number e such that W. = {e}. 
4. Show that there is a program .9J such that rf19 (x) ~ if and only if 
X= #(.9J). 
5. (a) Show that there is a partially computable function f that satis-
fies the equations 
What is /(2, 5)? 
f(x,O)=x+2 
f(x, 1) = 2 Â·f(x,2x) 
f(x,2t + 2) = 3 Â·f(x,2t) 
f(x,2t + 3) = 4 Â·f(x,2t + 1). 
(b) Prove that f is total. 
(c) 
Prove that f is unique. (That is, only one function satisfies the 
given equations.) 
6. Give two distinct partially computable functions f, g that satisfy the 
equations 
/(0) = 2 
g(O) =2 
f(2t + 2) = 3 Â·f(2t) 
g(2t + 2) =3. g(2t). 
For the specific functions f, g that you give, what are f{l) and g{l)? 

9. A Computable Function That Is Not Primitive Recursive 
105 
7. 
Let f(x) = x + 1. Use the proof of the fixed point theorem and the 
discussion preceding the proof of the recursion theorem to give a 
program !Jl! such that ci>#W>(x) = ci>f<HWÂ»(x). What unary function 
does !Jl! compute? 
8. 
Give a function f(y) such that, for all y, f(y) > y and <1>/x) = 
ci>f(y>(x ). 
9. Give a function f(y) such that, for all y, if cl>/x) = ci>f<Y>(x), then 
<1>/x) is not total. 
10. Show that the function while(x) defined following the fixed point 
theorem is primitive recursive. [Hint: Use the parameter theorem.] 
11. (a) Prove that the recursion theorem can be strengthened to read: 
There are infinitely many numbers e such that 
cl>~m>(x 1
, â¢â¢â¢ , Xm) = g(e, XI, â¢â¢â¢ , Xm). 
(b) Prove that the fixed point theorem can be strengthened to read: 
There are infinitely many numbers e such that 
cl>f(e)(x) = cl>e(x). 
12. Prove the following version of the recursion theorem: There is a 
primitive recursive function self(x) such that for all z 
cl>self(z)(x) = <I>;2>(self(z), x). 
13. Prove the following version of the fixed point theorem: There is a 
primitive recursive function fix(u) such that for all x, u, 
cl>fix(u)(x) = cl><l>u(fix(u/x). 
14. * Let S be an acceptable programming system with universal functions 
qr<m>. Prove the following: For every partially computable function 
g(z, x 1 , â¢â¢â¢ , xm) there is a number e such that 
qr;m>(x 1 , â¢â¢â¢ , Xm) = g(e, X 1 , â¢â¢â¢ , Xm). 
That is, a version of the recursion theorem holds for S. [See Exercise 
5.4 for the definition of acceptable programming systems.] 
*9. A Computable Function That Is Not 
Primitive Recursive 
In Chapter 3 we showed that all primitive recursive functions are com-
putable, but we did not settle the question of whether all computable 

106 
Chapter 4 A Universal Program 
functions are pnmitiVe recursive. We shall deal with this matter by 
showing how to obtain a function h(x) that is computable but is not 
primitive recursive. Our method will be to construct a computable function 
cf>(t, x) that enumerates all of the unary primitive recursive functions. That 
is, it will be the case that 
1. for each fixed value t = t0 , the function cf>(t0 , x) will be primitive 
recursive; 
2. for each unary primitive recursive function f(x), there will be a 
number t0 such that f(x) = cf>(t0 , x). 
Once we have this function 
cf> at our disposal, we can diagonalize, 
obtaining the unary computable function cf>(x, x) + 1 which must be 
different from all primitive recursive functions. (If it were primitive recur-
sive, we would have 
cf>(x, x) + 1 = cf>(t0 , x) 
for some fixed t0 , and setting x = t0 would lead to a contradiction.) 
We will obtain our enumerating function by giving a new characteriza-
tion of the unary primitive recursive functions. However, we begin by 
showing how to reduce the number of parameters needed in the operation 
of primitive recursion which, as defined in Chapter 3 (Eq. (2.2)), proceeds 
from the total n-ary function f and the total n + 2-ary function g to yield 
the n + 1-ary function h such that 
h(x1, ... ,xn,O) =J(x1, ... ,xn) 
h(x1, ... ,xn,t+ 1) =g(t,h(x1, ... ,xn,t),x1, ... ,xn). 
If n > 1 we can reduce the number of parameters needed from n to n - 1 
by using the pairing functions. That is, let 
f<x1 , ... ,xn-1) = f(x1 , ... ,xn_z,l(xn-1),r(xn-1)), 
g(t,u,x1 , ... ,xn_ 1) =g(t,u,x1 , ... ,xn_ 2 ,l(xn_ 1),r(xn_ 1)), 
h(x1 , ... ,xn_ 1 ,t) = h(x1 , ... ,xn_ 2 ,l(xn_ 1),r(xn_ 1),t). 
Then, we have 
h(x1, ... ,xn-1â¢0) =f<x1, ... ,xn-1) 
h(x1 , ... ,xn_ 1 ,t + 1) =g(t,h(x1 , ... ,xn_ 1 ,t),x1 , ... ,xn_ 1). 
Finally, we can retrieve the original function h from the equation 
h(x1 , ... ,xn,t) = h(x1 , ... ,Xn_ 2 ,(xn_ 1 ,xn),t). 

9. A Computable Function That Is Not Primitive Recursive 
107 
By iterating this process we can reduce the number of parameters to 1, 
that is, to recursions of the form 
h(x, 0) = f(x) 
(9.1) 
h(x,t + 1) =g(t,h(x,t),x) 
Recursions with no parameters, as in Eq. (2.1) in Chapter 3, can also 
readily be put into the form (9.1). Namely, to deal with 
t/J(O) = k 
1/J(t + 1) = fJ(t, 1/J(t)), 
we set f(x) = k (which can be obtained by k compositions with s(x) 
beginning with n(x)) and 
in the recursion (9.1). Then, t/J(t) = h(x, t) for all x. In particular, t/J(t) = 
h(ul(t), ul(t)). 
We can simplify recursions of the form (9.1) even further by using the 
pairing functions to combine arguments. Namely, we set 
h(x,t) = (h(x,t),(x,t)). 
Then, we have 
h(x,O) = (f(x),(x,O)) 
h(x,t + 1) = (h(x,t + 1),(x,t + 1)) 
= (g(t,h(x,t),x),(x,t + 1)) 
= g(h(x, t)), 
where 
g(u) = (g(r(r(u)),l(u),l(r(u)),(l(r(u)),r(r(u)) + 1)). 
Once again, the original function h can be retrieved from h; we can use 
the equation 
h(x, t) = l(h(x, t)). 
Now this reduction in the complexity of recursions was only possible 
using the pairing functions. Nevertheless, we can use it to get a simplified 
characterization of the class of primitive recursive functions by adding the 
pairing functions to our initial functions. We may state the result as a 
theorem. 

108 
Chapter 4 A Universal Program 
Theorem 9.1. The primitive recursive functions are precisely the func-
tions obtainable from the initial functions 
s(x),n(x),l(z),r(z),(x,y) and 
u?, 
1 ~ i ~ n 
using the operations of composition and primitive recursion of the particu-
lar form 
h(x, 0) = f(x) 
h(x,t + 1) =g(h(x,t)). 
The promised characterization of the unary primitive recursive functions 
is as follows. 
Theorem 9.2. The unary primitive recursive functions are precisely those 
obtained from the initial functions s(x) = x + 1, n(x) = 0, l(x), r(x) by 
applying the following three operations on unary functions: 
1. to go from f(x) and g(x) to f(g(x)); 
2. to go from f(x) and g(x) to (f(x), g(x)); 
3. to go from f(x) and g(x) to the function defined by the recursion 
h(O) = 0 
(
t{~) 
h(t + I)~ g( {:I)) 
if t + 1 is odd, 
if t + 1 is even. 
Proof. 
Let us write PR for the set of all functions obtained from the 
initial functions listed in the theorem using operations 1 through 3. We 
will show that PR is precisely the set of unary primitive recursive functions. 
To see that all the functions in PR are primitive recursive, it is necessary 
only to consider operation 3. That is, we need to show that if f and g are 
primitive recursive, and h is obtained using operation 3, then h is also 
primitive recursive. What is different about operation 3 is that h(t + 1) is 
computed, not from h(t) but rather from h(t/2) or h((t + l)j2), depend-
ing on whether t is even or odd. To deal with this we make use of Godel 
numbering, setting 
h(O) = 0, 
h(n) = [h(O), ... , h(n - 1)] if n > 0. 

9. A Computable Function That Is Not Primitive Recursive 
109 
We will show that iz is primitive recursive and then conclude that the same 
is true of h by using the equation3 
h(n) = (h(n + l))n+ t. 
Then (recalling that Pn is the nth prime number) we have 
h(n + 1) = h(n) Â· p:~n? 
( h(n) Â· P!~1 /ZJ> 
= 
h(n) Â· P!~~(n))lâ¢/21) 
if n is odd, 
if n is even. 
Here, we have used ln/21 because it gives the correct value whether n is 
even or odd and because we know from Chapter 3 that it is primitive 
recursive. 
Next we will show that every unary primitive recursive function belongs 
to PR. For this purpose we will call a function g(x 1 , â¢â¢â¢ , xn) satisfactory if 
it has the property that for any unary functions h 1{t), ... , hn(t) that belong 
to PR, the function g(h 1(t), ... , hn(t)) also belongs to PR. Note that a 
unary function g(t) that is satisfactory must belong to PR because g(t) = 
g(ul(t)) and ul(t) = (/(t), r(t)) belongs to PR. Thus, we can obtain our 
desired result by proving that all primitive recursive functions are satisfac-
tory.4 
We shall use the characterization of the primitive recursive functions of 
Theorem 9.1. Among the initial functions, we need consider only the 
pairing function (x1 , x2 ) and the projection functions u? where 1 ~ i ~ n. 
If h 1(t) and h2{t) are in PR, then using operation 2 in the definition of PR, 
we see that (h 1(t), h2(t)) is also in PR. Hence, (x1 , x2 ) is satisfactory. And 
evidently, if h 1(t), ... , hn(t) belong to PR, then u?(h 1(t), ... , hn(t)), which 
is simply equal to h;(t), certainly belongs to PR, so u? is satisfactory. 
To deal with composition, let 
h(xl ' ... ' xn) = f(gl(xi ' ... ' xn), ... ' gk(xl ' ... ' xn)) 
where g 1 , â¢â¢â¢ , gk and f are satisfactory. Let h 1(t), ... , hit) be given 
functions that belong to PR. Then, setting 
3 This is a general technique for dealing with recursive definitions for a given value in 
terms of smaller values, so-called course-of-value recursions. See Exercise 8.5 in Chapter 3. 
4 This is an example of what was called an induction loading device in Chapter 1. 

110 
Chapter 4 A Universal Program 
for 1 ::::;; i ::::;; k we see that each Â§; belongs to PR. Hence 
belongs to PR, and so, h is satisfactory. 
Finally, let 
h(x, 0) = f(x) 
h(x,t + 1) =g(h(x,t)) 
where f and g are satisfactory. Let r/J(O) = 0 and let r/J(t + 1) = 
h(r(t), /(t)). Recalling that 
(a, b) = 2a(2b + 1)- 1, 
we consider two cases according to whether t + 1 = 2a(2b + 1) is even or 
odd. If t + 1 is even, then a > 0 and 
1/J(t + 1) = h(b, a) 
= g(h(b, a - 1)) 
= g(rfJ(2a-l(2b + 1))) 
= g(r/J((t + 1)/2)). 
On the other hand, if t + 1 is odd, then a = 0 and 
In other words, 
rfJ(O) = 0 
1/J(t + 1) = h(b, 0) 
= f(b) 
= f(t/2). 
if t + 1 is odd, 
if t + 1 is even. 
Now f and g are satisfactory, and, being unary, they are therefore in PR. 
Since 1/J is obtained from f and g using operation 3, 1/J also belongs to PR. 
To retrieve h from 1/J we can use h(x, y) = rfJ{(x, y) + 1). So, 
h(h 1(t), hz(t)) = r/J(s((h 1(t), hz(t)))) 

9. A Computable Function That Is Not Primitive Recursive 
111 
from which we see that if h 1 and h 2 both belong to PR, then so does 
h(h 1(t), h 2(t)). Hence h is satisfactory. 
â¢ 
Now we are ready to define the function c/J(t, x), which we shall also 
write as cfJ/x ), that will enumerate the unary primitive recursive functions: 
cfJ/x) = 
X+ 1 
0 
l(x) 
r(x) 
cPt(n)( cPr(n)(x)) 
(c/JI(n/X), cPr(n)(x)) 
0 
cPt(n)((x -
1) /2) 
cPr(n)( cPr( X /2)) 
if t = 0 
if t = 1 
if t = 2 
if t = 3 
if t = 3n + 1, n > 0 
if t = 3n + 2, n > 0 
if t = 3n + 3, n > 0 and x = 0 
if t = 3n + 3, n > 0 and x is odd 
if t = 3n + 3, n > 0 and x is even 
Here cjJ0(x), c/J1(x), c/J2(x), c/J3(x) are the four initial functions. For t > 3, t 
is represented as 3n + i where n > 0 and i = 1, 2 or 3; the three 
operations of Theorem 9.2 are then dealt with for values of t with the 
corresponding value of i. The pairing functions are used to guarantee all 
functions obtained for any value of t are eventually used in applying each 
of the operations. It should be clear from the definition that c/J(t, x) is a 
total function and that it does enumerate all the unary primitive recursive 
functions. Although it is pretty clear that the definition provides an 
algorithm for computing the values of cjJ for any given inputs, for a 
rigorous proof more is needed. Fortunately, the recursion theorem makes 
it easy to provide such a proof. Namely, we set 
g(z, t, x) 
X+ 1 
0 
l(x) 
r(x) 
<t>?)(l(n), <t>Y)(r(n), x )) 
(<t>;2)(l(n), x), <t>Y)(r(n), x)) 
0 
<t>?)(l(n), lxj2J) 
<t>Y)(r(n), <t>?)(t, lxj2J)) 
if t = 0 
if t = 1 
if t = 2 
if t = 3 
if t = 3n + 1, n > 0 
if t = 3n + 2, n > 0 
if t = 3n + 3, n > 0 and x = 0 
if t = 3n + 3, n > 0 and x is odd 
if t = 3n + 3, n > 0 and x is even 

112 
Chapter 4 A Universal Program 
Then, g(z, t, x) is partially computable, and by the recursion theorem, 
there is a number e such that 
g(e,t,x) = <t>?>(t,x). 
Then, since g(e, t, x) satisfies the definition of cf>(t, x) and that definition 
determines cf> uniquely as a total function, we must have 
cf>(t,x) =g(e,t,x), 
so that cf> is computable. 
The discussion at the beginning of this section now applies and we have 
our desired result. 
Theorem 9.3. The function cf>(x, x) + 1 is a computable function that is 
not primitive recursive. 
Exercises 
1. Show that cf>(t, x) is not primitive recursive. 
2. Give a direct proof that cf>(t, x) is computable by showing how to 
obtain an ..:7 program that computes cf>. [Hint: Use the pairing func-
tions to construct a stack for handling recursions.] 

5 
Calculations on Strings 
1. Numerical Representation of Strings 
So far we have been dealing exclusively with computations on numbers. 
Now we want to extend our point of view to include computations on 
strings of symbols on a given alphabet. In order to extend computability 
theory to strings on an alphabet A, we wish to associate numbers with 
elements of A* in a one-one manner. We now describe one convenient 
way of doing this: Let A be some given alphabet. Since A is a set, there is 
no order implied among the symbols. However, we will assume in this 
chapter that the elements of A have been placed in some definite order. In 
particular, when we write A = {s1 , â¢â¢â¢ , sn}, we think of the sequence 
s1 , â¢â¢â¢ , sn as corresponding to this given order. Now, let w = s. s. 
Â·Â·Â· 
lk lk- I 
S; 1S;0 â¢ Then we associate with w the integer 
(1.1) 
With w = 0, we associate the number 0. (It is for this reason that we use 
the same symbol for both.) For example, let A consist of the symbols 
a, b, c given in the order shown, and let w = baacb. Then, the correspond-
ing integer is 
X= 2 Â· 34 + 1 Â· 33 + 1 Â· 32 + 3 Â· 31 + 2 = 209. 
113 

114 
Chapter 5 Calculations on Strings 
In order to see that the representation (1.1) is unique, we show how to 
retrieve the subscripts i0, i1, ... , ik from x assuming that x -=!= 0. We define 
the primitive recursive functions: 
R+(x y) = {R(x,y) 
' 
y 
+ 
{ lxjy J 
Q(x,y)= lxjyj-=-1 
if - (y I x) 
otherwise, 
if - (y I x) 
otherwise, 
where the functions R(x, y) and l x jy J are as defined in Chapter 3, Sec-
tion 7. Then, as we shall easily show, for y -=1= 0, 
x 
R+(x,y) 
- = Q+(x,y) + ---
y 
y 
This equation expresses ordinary division with quotient and remainder: 
x 
R(x,y) 
- = lxjyJ + --
y 
y 
as long as y is not a divisor of x. If y is a divisor of x we have 
x 
y 
R+(x, y) 
-
= lxjyJ = (lxjyJ..:... 1) +- = Q+(x,y) + ---
y 
y 
y 
Thus, what we are doing differs from ordinary division with remainders in 
that "remainders" are permitted to take on values between 1 and y rather 
than between 0 and y -
1. 
Now, let us set 
Thus, by (1.1) 
Therefore, 
u0 =x, 
u0 = ik Â·nk + ik-t Â·nk-t + Â·Â·Â· +i1 Â·n + i0, 
ut = ik . nk- t + ik -1 . nk- z + ... +it ' 
m=0,1, ... ,k. 
(1.2) 
(1.3) 
(1.4) 

1. Numerical Representation of Strings 
115 
Hence, for any number x satisfying (1.1), the string w can be retrieved. It 
is worth noting that this can be accomplished using primitive recursive 
functions. If we write 
g(O, n, x) = x, 
g(m + 1, n, x) = Q+(g(m, n, x), n), 
then 
g(m, n, x) = um 
(1.5) 
as defined by (1.2), where, of course, g is primitive recursive. Moreover, if 
we let h(m, n, x) = R+(g(m, n, x), n), then h is also primitive recursive, 
and by (1.4) 
im = h(m, n, x), 
m=0,1, ... ,k. 
(1.6) 
This method of representing strings by numbers is clearly related to the 
usual base n notation for numbers. To explore the connection, it is 
instructive to consider the alphabet 
1) = {1,2,3,4,5,6,7,8,9,)(} 
in the order shown. Then the number associated with the string 45 is 
4. 10 + 5 = 45. 
On the other hand, the number associated with 2)( is 
2. 10 + 10 = 30. 
(Perhaps we should read 2)( as "twenty-ten"!) Clearly a string on I> that 
does not include )( is simply the usual decimal notation for the number it 
represents. It is numbers whose decimal representation includes a 0 which 
now require an )(. 
Thus, in the general case of an alphabet A consisting of s1 , â¢â¢â¢ , sn, 
ordered as shown, we see that we are simply using a base n representation 
in which the "digits" range from 1 to n instead of the usual 0 to n - 1. We 
are proceeding in this manner simply to avoid the lack of uniqueness of 
the usual base n representation: 
79 = 079 = 0079 = 00079 = etc. 
This lack of uniqueness is of course caused by the fact that leading zeros 
do not change the number being represented. 

116 
Chapter 5 Calculations on Strings 
It is interesting to observe that the rules of elementary arithmetic 
(including the use of "carries") work perfectly well with our representa-
tion. Here are a few examples: 
1 7 
+1X3 
21 X 
29 
-1X 
9 
X5 
x2X 
X4X 
1XX 
3 1 4X 
which corresponds to 
which corresponds to 
which corresponds to 
17 
+203 
220 
29 
-20 
9 
105 
X30 
3150 
(Incidentally, this shows that the common belief that the modern rules of 
calculation required the introduction of a digit for 0 is unjustified.) Note in 
particular the following examples of adding 1: 
X1 
+ 
1 
X2 
3X 
+ 
1 
4 1 
3XX 
+ 
1 
4 1 1 
73XX 
+ 
1 
7 4 1 1 
49 
+ 
1 
4X 
Adding 1 to X gives a result of 1 with a carry of 1. If the string ends in 
more than one X, the carry propagates. Subtracting 1 is similar, with a 
propagating carry produced by a string ending in 1: 
1X 
1 
1 9 
X1 
1 
9X 
7 1 1 
1 
6XX 
Now we return to the general case. Given the alphabet A consisting of 
s1 , â¢â¢â¢ , sn in the order shown, the string w = s,. s,. 
Â· Â· Â· s,. s,. is called the 
k 
k- 1 
I 
0 
base n notation for the number x defined by (1.1). (0 is the base n notation 
for the null string 0 for every n.) Thus when n is fixed we can regard a 
partial function of one or more variables on A* as a function of the 
corresponding numbers. (That is, the numbers are just those which the 
given strings represent in base n notation.) It now makes perfect sense to 
speak of an m-ary partial function on A* with values in A* as being 
partially computable, or when it is total, as being computable. Similarly we 

1. Numerical Representation of Strings 
117 
can say that an m-ary function on A* is primitive recursive. Note that for 
any alphabet A = {s1 , â¢â¢â¢ , sn} with the symbols ordered as shown, s1 
denotes 1 in base n. Thus an m-ary predicate on A* is simply a total 
m-ary function on A* all of whose values are either s1 or 0. And it now 
makes sense as well to speak of an m-ary predicate on A* as being 
computable. 
As was stated in Chapter 1, for a given alphabet A, any subset of A* is 
called a language on A. Once again, by associating with the elements of 
A* the corresponding numbers, we can speak of a language on A as being 
r.e., or recursive, or primitive recursive. 
It is important to observe that whereas the usual base n notation using a 
0 digit works only for n ~ 2, the representation (1.1) is valid even for 
n = 1. For an alphabet consisting of the single symbol 1, the string 1rx1 of 
length x is the base 1 notation for the number L.f,::~ 1 Â· {1); = L.f,::~ 1 = x. 
That is, the base 1 (or unary) representation of the number x is simply a 
string of ones of length x. 
In thinking of numbers (that is, elements of N) as inputs to and outputs 
from programs written in our language ..:7, no particular representation of 
these numbers was specified or required. Numbers occur in the theory as 
purely abstract entities, just as they do in ordinary mathematics. However, 
when we wish to refer to particular numbers, we do so in the manner 
familiar to all of us, by writing their decimal representations. These 
representations are, of course, really strings on the alphabet that consists 
of the decimal digits: 
{0,1,2,3,4,5,6,7,8,9}. 
But it is essential to avoid confusing such strings with the numbers they 
represent. For this reason, for the remainder of this chapter we shall avoid 
the use of decimal digits as symbols in our alphabets. Thus, a string of 
decimal digits will always be meant to refer to a number. 
Now, let A be some fixed alphabet containing exactly n symbols, say 
A= {s 1,s2 , â¢â¢â¢ ,sn}. For each m ~ 1, we define CONCAT~m) as follows: 
CONCAT~1 >(u) = u, 
(1.7) 
where 
z = CONCAT~m>(u 1
, â¢â¢â¢ , um). 
Thus, for given strings u 1 , â¢.â¢ , um E A*, CONCAT~m>(u 1
, â¢â¢â¢ , um) is simply 
the string obtained by placing the strings u1 , â¢â¢â¢ , um one after the other, 

118 
Chapter 5 Calculations on Strings 
or, as is usually said, by concatenating them. We will usually omit the 
superscript, so that, for example we may write 
CONCATz{s2s1 , s1s1s2 ) = s2s1s1s1s2 â¢ 
Likewise, 
CONCAT6(s2s 1 , s1s1s2 ) = s2s1s1s1s2 â¢ 
However, the string s2s1 represents the number 5 in base 2 and the 
number 13 in base 6. Also, the string s1s1s2 represents the number 8 in 
base 2 and the number 44 in base 6. Finally, the string s2s1s1s1s2 
represents 48 in base 2 and 2852 in base 6. If we wish to think of 
CONCAT as defining functions on N (as will be necessary, for example, in 
showing that the functions (1.7) are primitive recursive), then the example 
we have been considering becomes 
CONCAT2(5,8) = 48 
and 
CONCAT6(13, 44) = 2852. 
The same example in base 10 gives 
CONCAT10(21, 112) = 21112. 
Bearing this discussion in mind, we now proceed to give a list of primitive 
recursive functions (on A* or N, depending on one's point of view) that we 
will need later. 
1. f(u) = lui. This "length" function is most naturally understood as 
being defined on A* and taking values in N. For each x, the number 
L.J=o nj has the base n representation s\x+ 11; hence this number is 
the smallest number whose base n representation contains x + 1 
symbols. Thus, 
lui = min [ E 
nj > u]. 
x,;u 
j=O 
2. g(u, v) = CONCATn(u, v ). The primitive recursiveness of this func-
tion follows from the equation 
CONCATn(u, v) = u Â· nivi + v. 
3. CONCAT~m>(u 1 , â¢â¢â¢ , um), as defined in (1.7), is primitive recursive 
for each m, n ~ 1. This follows at once from the previous example 
using composition. 
4. RTENDn(w) = h(O, n, w), where his as in (1.6). As a function of A*, 
RTENDn gives the rightmost symbol of a given word, as is clear from 
(1.3) and (1.6). 

1. Numerical Representation of Strings 
119 
5. LTENDn(w) = h(lwl..:... 1, n, w). LTENDn gives the leftmost symbol 
of a given nonempty word. 
6. RTRUNCn(w) = g(l, n, w). RTRUNCn gives the result of removing 
the rightmost symbol from a given nonempty word, as is clear from 
(1.3) and (1.5). When we can omit reference to the base n, we often write 
w- for RTRUNCn(w). Note that o-= 0. 
7. LTRUNCn{w) = w -=-(LTENDn(w) Â· nlwl-'- 1). In the notation of (1.3), 
for a given nonempty word w, LTRUNCn(w) = w- ik Â· nk, i.e., 
LTRUNCn(w) is the result of removing the leftmost symbol from w. 
We will now use the list of primitive recursive functions that we have 
just given to prove the computability of a pair of functions that can be used 
in changing base. Thus, let 1 :::;; n < I. Let A c A, where A is an alphabet 
of n symbols and A is an alphabet of I symbols. Thus a string that belongs 
to A* also belongs to A*. For any x E N, let w be the word in A* that 
represents x in base n. Then, we write UPCHANGEn 1(x) for the number 
which w represents in base I. For example, referring to our previous 
example, we have UPCHANGE2 6(5) = 13, UPCHANGE2 6(8) = 44, UP-
CHANGE2,6(48) = 2852. Also' UPCHANGE2,10(5) =. 21 
and UP-
CHANGE6 10(13) = 21. 
Next, for' x E N, let w be the string in A* which represents x in base I, 
and let w' be obtained from w by crossing out all of the symbols that 
belong to A- A. Then, w' E A*, and we write DOWNCHANGEn 1(x) 
for the number which w' represents in base n. For example, the string 
s2 s6s 1 
represents the number 109 
in 
base 6. To obtain 
DOWNCHANGE2,6(109) we cross out the s6 , obtaining the string s2s1, 
which represents 5 in base 2; thus DOWNCHANGE2 6(10) = 5. 
Although UPCHANGEn 1 and DOWNCHANGEn ~ are actually primi-
tive recursive functions, w~ will content ourselves with proving that they 
are computable: 
Theorem 1.1. Let 0 < n < I. Then the functions UPCHANGEn 1 and 
DOWNCHANGEn 1 are computable. 
' 
Proof. We begin with UPCHANGEn 1â¢ We write a program which ex-
tracts the successive symbols of the word that the given number represents 
in base n and uses them in computing the number that the given word 
represents in base 1: 
[A] 
IF X = 0 GOTO E 
Z -
LTENDn(X) 
X- LTRUNCn(X) 
Y-IÂ·Y+Z 
GOTOA 

120 
Chapter 5 Calculations on Strings 
DOWNCHANGEn 1 is handled similarly. Our program will extract the 
successive symbols of the word that the given number represents in base /. 
However, these symbols will only be used if they belong to the smaller 
alphabet, i.e., if as numbers they are :::;; n: 
Exercises 
[A] 
IF X = 0 GOTO E 
Z +-- LTEND1(X) 
X+-- LTRUNC1(X) 
IF Z > n GOTO A 
Y+-nÂ·Y+Z 
GOTOA 
â¢ 
1. (a) Write the numbers 40 and 12 in base 3 notation using the "digits" 
{1, 2, 3}. 
(b) Work out the multiplication 40 Â· 12 = 480 in base 3. 
(c) 
Compute CONCATi12, 15) for n = 3, 5, and 10. Why is no 
calculation required in the last case? 
(d) Compute 
the 
following: 
UPCHANGE 3 7(15), UP-
CHANGE2, 7(15), UPCHANGE2, 10(15), DOWNCHANGE3, 7(15), 
DOWNCHANGE2, 7(15), DOWNCHANGE2, 10(20). 
2. Compute each of the following for n = 3. 
(a) CONCATp>(17, 32). 
(b) 
CONCAT~ 3 >(17,32, 11). 
(c) 
RTENDn(23). 
(d) LTENDn(29). 
(e) 
RTRUNCn(19). 
(f) 
LTRUNCn(l8). 
3. Do the previous exercise for n = 4. 
4. Show that the function f whose value is the string formed of the 
symbols occurring in the odd-numbered places in the input [i.e., 
f(a 1a2a3 â¢â¢â¢ an)= a1a3 Â·Â·Â·]is computable. 
5. 
Let A = {s1 , â¢â¢â¢ , sn}, and let P(x) be the predicate on N which is true 
just when the string in A* that represents x has an even number of 
symbols. Show that P(x) is primitive recursive. 

2. A Programming Language for String Computations 
121 
6. If u * 0, let #(u, v) be the number of occurrences of u as a part of v 
[e.g., #(bab, ababab) = 2]. Also, let #(0, v) = 0. Prove that #(u, v) is 
primitive recursive. 
7. Show that UPCHANGEn,t and DOWNCHANGEn,t are primitive 
recursive. 
8. Show that when lui is calculated with respect to base n notation, 
lui ::; l!ogn uj + 1 for all u E N. 
2. A Programming Language for String Computations 
From the point of view of string computations, the language .9' seems 
quite artificial. For example, the instruction 
V+-V+1 
which is so basic for integers, seems entirely unnatural as a basic instruc-
tion for string calculations. Thus, for the alphabet {a, b, c}, applying this 
instruction to bacc produces bbaa because a carry is propagated. (This will 
perhaps seem more evident if, momentarily ignoring our promise to avoid 
the decimal digits as symbols in our alphabets, we use the alphabet {1, 2, 3} 
and write 
2133 + 1 = 2211.) 
We are now going to introduce, for each n > 0, a programming lan-
guage Y,, which is specifically designed for string calculations on an 
alphabet of n symbols. The languages Y, will be supplied with the same 
input, output, and local variables as .9', except that we now think of them 
as having values in the set A*, where A is an n symbol alphabet. Variables 
not otherwise initialized are to be initialized to 0. We use the same 
symbols as labels in Y, as in .9' and the same conventions regarding their 
use. The instruction types are shown in Table 2.1. 
The formal rules of syntax in Y, are entirely analogous to those for .9', 
and we omit them. Similarly, we use macro expansions quite freely. An 
m-ary partial function on A* which is computed by a program in Y, is 
said to be partially computable in Y, . If the function is total and partially 
computable in Y, , it is called computable in Y, . 
Although the instructions of Y, refer to strings, we can just as well 
think of them as referring to the numbers that the corresponding strings 
represent in base n. For example, the numerical effect of the instruction 

122 
Chapter 5 Calculations on Strings 
Table 2.1 
Instruction 
Interpretation 
V<- uV 
for each symbol u in the alphabet A 
v.- v-
Place the symbol u to the left of the string which is 
the value of V. 
Delete the final symbol of the string which is the 
value of V. If the value of V is 0, leave it 
unchanged. 
If V ENDS u GOTO L 
for each symbol u in the alphabet A 
and each label L 
If the value of V ends in the symbol u, execute next 
the first instruction labeled L; otherwise proceed 
to the next instruction. 
in the n symbol alphabet {s1 , â¢â¢â¢ , sn} ordered as shown is to replace the 
numerical value x by i Â· nlxl + x. Just as the instructions of .9' are natural 
as basic numerical operations, but complex as string operations, so the 
instructions of .57, are natural as basic string operations, but complex as 
numerical operations. 
We now give some macros for use in .57, with the corresponding 
expansions. 
1. The macro IF V -=!= 0 GOTO L has the expansion 
IF VENDS s1 GOTO L 
IF V ENDS s2 GOTO L 
IF V ENDS sn GOTO L 
2. The macro V ~ 0 has the expansion 
[A] 
v~ v-
IF V-=t= OGOTOA 
3. The macro GOTO L has the expansion 
IF Z ENDS s1 GOTO L 
4. The macro V' ~ V has the expansion shown in Fig. 2.1. 
The macro expansion of V' ~ V in .57, is quite similar to that in .9'. 

2. A Programming Language for String Computations 
Z+--0 
V' +-- 0 
[A] 
IF V ENDS s1 GOTO B1 
IF V ENDS s2 GOTO B2 
[C] 
IF VENDS sn GOTO Bn 
GOTOC 
V' +-- s-V' 
V+-v-} 
~;T6
~A i = 1,2, ... ,n 
IF Z ENDS s 1 GOTO D 1 
IF Z ENDS s2 GOTO D 2 
IF Z ENDS sn GOTO Dn 
GOTOE 
z +-- z- ) 
~;Tbc i = 1,2, ... ,n 
Figure 2.1. Macro expansion of V' +-- V in ~ . 
The block of instructions 
is usually written simply 
IF VENDS s 1 GOTO B 1 
IF VENDS s2 GOTO B2 
IF VENDS sn GOTO Bn 
IF V ENDS s; GOTO B; 
(1 .:5; i .:5; n) 
123 
Such a block of instructions is referred to as a filter for obvious reasons. 
Note that at the point in the computation when the first "GOTO C" is 
executed, V' and Z will both have the original value of V, whereas V will 
have the value 0. On exiting, Z has the value 0, while V' retains the 
original value of V and V has been restored to its original value. 
If f(x 1 , â¢â¢â¢ , xm) is any function that is partially computable in .?,, we 
permit the use in .?, of macros of the form 
V ~ f(Vl , ... , Vm) 
The corresponding expansions are carried out in a manner entirely analo-
gous to that discussed in Chapter 2, Section 5. 
We conclude this section with two examples of functions that are 
computable in .?, for every n. The general results in the next section will 

124 
Carry 
pro~agates 
X~ Q 
x ends s11 
x ends s; 
Chapter 5 Calculations on Strings 
x~o 
TEST X 
END 
x ends s, 
Figure 2.2. Flow chart for computing x + 1 in 5';, . 
make it clear that these two examples are the only bit of programming in 
.9;, that we shall need to carry out explicitly. 
We want to show that the function x + 1 is computable in .9;,. We let 
our alphabet consist of the symbols s 1 , s2 , â¢â¢â¢ , sn ordered as shown. The 
desired program is exhibited in Fig. 2.3; a flow chart that shows how the 
program works is shown in Fig. 2.2. 
Our final example is a program that computes x ..:... 1 base n. A flow 
chart is given in Fig. 2.4 and the actual program in .9;, is exhibited in Fig. 
2.5. The reader should check both of these programs with some examples. 
[B] 
IF X ENDS s; GOTO A; 
(1 ~ i ~ n) 
Y +--stY 
GOTOE 
[A;] 
x-x ) 
Y +-- s;+ 1Y 1 ~ i < n 
GOTOC 
[A.] 
x--x-
Y +--stY 
GOTOB 
[C] 
IF X ENDS s; GOTO D; 
(1 ~ i ~ n) 
GOTOE 
[D;l 
x-x ) 
Y +-- s;Y 
1 ~ i ~ n 
GOTOC 
Figure 2.3. Program that computes x + 1 in 5';, . 

2. A Programming Language for String Computations 
Exercises 
x~o 
x ends s, 
TEST X 
f--....,....-~x-xÂ­
x ends s1 
no 
yes 
I'Y~-J~~v~--~~x~=~o~]J-~-1END 
Carry IS 
L 
propagated 
Figure 2.4. Flow chart for computing x .:.. 1 in Y,. 0 
Carry is 
absorbed 
[B] 
IF X ENDS s; GOTO A; 
(1 !> i !> n) 
GOTOE 
[A;] 
x-x ) 
Y+-s;_ 1Y 1<i!>n 
GOTOC 
[A,] 
x ..... x-
IF X*- 0 GOTO C2 
GOTOE 
[C2l 
Y+- snY 
GOTOB 
[C] 
IF X ENDS s; GOTO D; 
(1 !> i !> n) 
GOTOE 
[D;] 
x-x ) 
Y+-s;Y 
1!>i!>n 
GOTOC 
Figure 2.5. Program that computes x .:.. 1 in Y,. 0 
125 
1. Let A = {s1 , s2}o Write out the complete expansion of the macro 
X ~ Y in ..9"2 0 

126 
Chapter 5 Calculations on Strings 
2. 
Write a program in .?, to compute the function f defined in Exercise 
1.4. 
3. Show that f(u, v) = "i7V is computable in .?, . ("i7V is the concatenation 
of u and v, defined in Chapter 1.) 
4. Let A = {s1 , â¢â¢â¢ , sn}, and let P(x) be the predicate on A* which is 
true just when x has an even number of symbols. Show that P(x) is 
computable in .?, . 
5. Write a program in .?, to compute #(u, v) as defined in Exercise 1.6. 
6. 
Give an expansion in .?, for the macro V -
Vu, which means: Place 
the symbol u to the right of the string that is the value of V. 
7. Show that f(x) = xR is computable in .?,. (xR is defined in Chapter 
1, Section 3.) 
8. 
Let A = {s1 , â¢â¢â¢ , sn}, and let g(u) = w for all strings u in A*, where 
w is the base n notation for the number of symbols in u. Show that g 
is computable in .?, . 
9. Let A = {s1 , s2}, and let fJlJ be the .9"2 program 
Y-X+ 1 
Write out the computation of fJlJ for input x = s2s2 â¢ 
10. 
Let A = {s1 , s2 , s3}, and let fJlJ be the .9"3 program 
Y-X-=-1 
Write out the computation of fJlJ for input x = s 1s1 â¢ 
11. (a) Show that Theorem 1.1 in Chapter 3 holds if we substitute 
"computable in .?," for "computable." 
(b) Show that Theorems 2.1 and 2.2 in Chapter 3 hold if we 
substitute "computable in .?," for "computable." 
(c) 
Show that if f(x 1 , â¢â¢â¢ , xn) is primitive recursive, then it is 
computable in .?, . 
3. The Languages .9 and 9, 
We now want to compare the functions that can be computed in the 
various languages we have been considering, namely, .9" and the different 
.?, . For the purpose of making this comparison, we take the point of view 
that, in all of the languages, computations are "really" dealing with 
numbers, and that strings on an n letter alphabet are simply data objects 
being used to represent numbers (using base n of course). 

3. The Languages .9" and .5';, 
127 
We shall see that in fact all of these languages are equivalent. That is, a 
function f is partially computable if and only if it is partially computable 
in each .?, and therefore, also, f is partially computable in any one .?, if 
and only if it is partially computable in all of them. 
To begin with we have 
Theorem 3.1. A function is partially computable if and only if it is 
partially computable in ..9"1 â¢ 
Proof. It is easy to see that the languages ..9" and ..9"1 are really the same. 
That is, the numerical effect of the instructions 
and 
v~ v-
in ..9"1 is the same as that of the corresponding instructions in ..9": 
V ~ V + 1 
and 
V ~ V - 1. 
Furthermore, the condition "V ENDS st" in ..9"1 is equivalent to the 
condition V -=!= 0 in ..9". (Since s 1 is the only symbol, ending in s 1 is 
equivalent to being different from the null string.) 
â¢ 
This theorem shows that results we obtain about the languages .?, can 
always be specialized to give results about ..9" by setting n = 1. 
Next we shall prove 
Theorem 3.2. If a function is partially computable, then it is also partially 
computable in .?, for each n. 
Proof. 
Let the function f be computed by a program .9 in the language 
..9". We translate .9 into a program in .?, by replacing each instruction of 
.9 by a macro in .?, as follows. 
We replace each instruction V ~ V + 1 by the macro V ~ V + 1, each 
instruction V ~ V - 1 by the macro V ~ V ..:... 1, and each instruction IF 
V -=!= 0 GOTO L by the macro IF V -=1= 0 GOTO L. Here we are using the 
fact, proved at the end of the preceding section, that x + 1 and x ..:... 1 are 
both computable in base n, and hence can each be used to define a macro 
in .?,. 
It is then obvious that the new program computes in .?, the same 
function f that .9 computes in ..9". 
â¢ 
This is the first of many proofs by the method of simulation: a program 
in one language is "simulated" step by step by a corresponding program in 
a different language. 
We could now prove directly that if a function is partially computable in 
.?, for any particular n, then it is in fact partially computable in our 
original sense. But it will be easier to delay doing so since the result will be 
an automatic consequence of our work on Post-Turing programs. 

128 
Chapter 5 Calculations on Strings 
Exercises 
1. Give a pnm1t1ve recursive function b 1(n, x) such that any partial 
function computed by an .9 program with x instructions is computed 
by some .57,. program with no more than bln, x) instructions. 
2. Give a primitive recursive function b~m>(n, x 1 , â¢â¢â¢ , xm, y) such that any 
partial function f(x 1 , â¢â¢â¢ , xm) computed by an .9 program in y steps 
on inputs x 1 , â¢â¢â¢ , xm is computed by some .57,. program in no more 
than b~m>(n, x 1 , â¢â¢â¢ , xm, y) steps. [Hint: Note that after y steps no 
variable holds a value larger than max{x 1 , â¢â¢â¢ , xm} + y.] 
3. Let n be some fixed number > 0, and let #(9') be a numbering 
scheme for .57,. programs defined exactly like the numbering scheme 
for .9 programs given in Chapter 4, except that #(I) = (a, ( b, c)), 
where 
{ 
0 if the statement in I is V ~ v-
i if the statement in I is V ~ s;V 
b= #(L') Â· n + i 
if the statement in I is IF V ENDS s; GOTO L'. 
(a) Define 
HALTn(x, y) =.57,. program y eventually halts on input x. 
Show that the predicate HALTn(x, y) is not computable in.?,.. 
(b) Define the universal function <l>~m) for m-ary functions partially 
computable in .57,. as follows: 
(Of course, 1/J.J.m> is the m-ary partial function computed by the 
.57,. 
program 9'.) Show that for each m > 0, the function 
<l>~m>(x 1 , â¢â¢â¢ , xm, y) is partially computable in .57,.. 
(c)* State and prove a version of the parameter theorem for .57,. . 
(d)* State and prove a version of the recursion theorem for .57,. . 
(e)* Show that .57,. is an acceptable programming system. [See Exer-
cise 5.4 in Chapter 4 for the definition of acceptable programming 
systems.] 
4. * Give an upper bound on the length of the shortest .9] program which 
computes the function <l>yCx) defined in Chapter 4. [See Exercise 3.6 in 
Chapter 4.] 

4. Post- Turing Programs 
129 
4. 
Post- Turing Programs 
In this section, we will study yet another programming language for string 
manipulation, the Post-Turing language :T. Unlike.'?,., the language :T has 
no variables. All of the information being processed is placed on one linear 
tape. We can conveniently think of the tape as ruled into squares each of 
which can carry a single symbol (see Fig. 4.1). The tape is thought of as 
infinite in both directions. Each step of a computation is sensitive to just 
one symbol on the tape, the symbol on the square being "scanned." We 
can think of the tape passing through a device (like a tape recorder), or we 
can think of the computer as a tapehead that moves along the tape and is 
at each moment on one definite square (or we might say "tile"). With this 
simple scheme, there are not many steps we can imagine. The symbol 
being scanned can be altered. (That is, a new symbol can be "printed" in 
its place.) Or which instruction of a program is to be executed next can 
depend on which symbol is currently being scanned. Or, finally, the head 
can move one square to the left or right of the square presently scanned. 
We are led to the language shown in Table 4.1. 
Although the formulation of :T we have presented is closer in spirit to 
that originally given by Emil Post, it was Turing's analysis of the computa-
tion process that has made this formulation seem so appropriate. This 
language has played a fundamental role in theoretical computer science. 
Turing's analysis was obtained by abstracting from the process carried 
out by a human being engaged in calculating according to a mechanical 
deterministic algorithm. Turing reasoned that there was no loss of general-
ity in assuming that the person used a linear paper (like the paper tape in 
an old-fashioned adding machine or a printing calculator) instead of 
two-dimensional sheets of paper. Such a calculator is then engaged in 
observing symbols and writing symbols. Again without loss of generality, 
we can assume that only one symbol at a time is observed, since any finite 
group of symbols can be regarded as a single "megasymbol." Finally, we 
can assume that when the calculator shifts attention it is to an immediately 
adjacent symbol. For, to look, say, three symbols to the left is equivalent to 
moving one symbol to the left three successive times. And now we have 
arrived at precisely the Post-Turing language. 
In order to speak of a function being computed by a Post-Turing 
program, we will need to deal with input and output. Let us suppose that 
Figure 4.1 

130 
Chapter 5 Calculations on Strings 
Table 4.1 
Instruction 
Interpretation 
PRINT a 
Replace the symbol on the square being scanned by a. 
IF a GOTO L 
GOTO the first instruction labeled L if the symbol currently scanned is a; 
otherwise, continue to the next instruction. 
RIGHT 
LEFT 
Scan the square immediately to the right of the square presently scanned. 
Scan the square immediately to the left of the square presently scanned. 
we are dealing with string functions on the alphabet A = {s1 , s2 , â¢â¢â¢ , sn}. 
We will use an additional symbol, written s0 , which we call the blank and 
use as a punctuation mark. Often we write B for the blank instead of s0 â¢ All 
of our computations will be arranged so that all but a finite number of 
squares on the tape are blank, i.e., contain the symbol B. We show the 
contents of a tape by exhibiting a finite section containing all of the 
nonblank squares. We indicate the square currently being scanned by an 
arrow pointing up, just below the scanned square. 
For example we can write 
to indicate that the tape consists of s 1 s 2 Bs2 s 1 with blank squares to the 
left and right, and that the square currently scanned contains the s2 
furthest to the right. We speak of a tape configuration as consisting of the 
tape contents together with a specification of one square as being currently 
scanned. 
Now, to compute a partial function f(x 1 , â¢â¢â¢ , xm) of m variables on A*, 
we need to place the m strings x 1 , â¢â¢â¢ , xm on the tape initially. We do this 
using the initial tape configuration: 
That is, the inputs are separated by single blanks, and the symbol initially 
scanned is the blank immediately to the left of x 1 . Here are a few 
examples: 
1. n = 1, so the alphabet is {s1}. We want to compute a function 
f(x 1 , x 2 ) and the initial values are x 1 = s1s1 , x 2 = s1 â¢ Then the tape 
configuration initially will be 
B s1 s1 B s1â¢ 
i 

4. Post- Turing Programs 
131 
Of course, there are infinitely many blank squares to the left and 
right of the finite section we have shown: 
... B B B B s1 s1 B s1 B B B 
i 
2. n = 2, x 1 = s1 s2 , x2 = s2s1 , x3 = s2s2 â¢ Then the tape configuration 
is initially 
B s1 s2 B s2 s1 B s2 s2 â¢ 
i 
3. n = 2, x1 = 0, x2 = s2s1, x3 = s2 â¢ Then the tape configuration is 
initially 
B B s2 s1 B s2 â¢ 
i 
4. n = 2, x 1 = s1s2 , x2 = s2s1, x 3 = 0. Then the tape configuration is 
initially 
Note that there is no way to distinguish this initial tape configuration from 
that for which there are only two inputs x 1 = s1s2 and x2 = s2s1 â¢ In other 
words, with this method of placing inputs on the tape, the number of 
arguments must be provided externally. It cannot be read from the tape. 
A simple example of a Post-Turing program is given in Fig. 4.2. 
Beginning with input x, this program outputs s2s1x. More explicitly, 
beginning with a tape configuration 
B x 
i 
this program halts with the tape configuration 
B s2 s1 x. 
i 
Figure 4.2 
PRINT s1 
LEFf 
PRINT s2 
LEFf 

132 
[A) 
Chapter 5 Calculations on Strings 
RIGHT 
IF s 1 GOTO A 
IF s2 GOTOA 
IF s3 GOTOA 
PRINTs 1 
RIGHT 
PRINT s1 
[C) 
LEFT 
IF s1 GOTO C 
IF s2 GOTOC 
IF s3 GOTO C 
Figure 4.3 
Next, for a slightly more complicated example, we consider Fig. 4.3. 
Here we are assuming that the alphabet is {s1 , s2 , s3}. Let x be a string on 
this alphabet. Beginning with a tape configuration 
B X 
i 
this program halts with the tape configuration 
B x s1 s 1â¢ 
i 
The computation proceeds by first moving right until the blank to the right 
of x is located. The symbol s 1 is then printed twice and then the 
computation proceeds by moving left until the blank to the left of x is 
again located. 
Figure 4.4 exhibits another example, this time with the alphabet {s1 , s2}. 
The effect of this program is to "erase" all of the occurrences of s2 in the 
input string, that is to replace each s2 by B. For the purpose of reading 
output values off the tape, these additional Bs are ignored. Thus, if f(x) is 
the function which this last program computes, we have, for example, 
f(szslsz) = S1, 
f(slszsl) = slsl' 
f(O) = 0. 
Of course, the initial tape configuration 
B s1 s2 s1 
i 

4. Post- Turing Programs 
[C) 
RIGHT 
IF B GOTO E 
IF s2 GOTO A 
IF s 1 GOTO C 
[A) 
PRINT B 
IF B GOTO C 
Figure 4.4 
[A) 
RIGHT 
IF B GOTO E 
PRINT M 
[B) 
RIGHT 
IF s 1 GOTO B 
[C) 
RIGHT 
IF s 1 GOTO C 
PRINT s 1 
[D) 
LEFT 
IF s 1 GOTO D 
IF B GOTOD 
PRINT s 1 
IF s 1 GOTO A 
Figure4.5 
leads to the final tape configuration 
B s1 B s1 B 
i 
but the blanks are ignored in reading the output. 
133 
For our final example we are computing a string function on the 
alphabet {s1}. However, the program uses three symbols, B, s1 , and M. 
The symbol M is a marker to keep track of a symbol being copied. The 
program is given in Fig. 4.5. Beginning with the tape configuration 
B u 
i 
where u is a string in which only the symbol s 1 occurs, this program will 
terminate with the tape configuration 
B u Bu. 
i 

134 
Chapter 5 Calculations on Strings 
(Thus we can say that this program computes the function 2x using unary 
notation.) The computation proceeds by replacing each successive s1 
(going from left to right) by the marker M and then copying the s 1 on the 
right. 
We conclude this section with some definitions. Let f(x 1 , â¢â¢â¢ , xm) be an 
m-ary partial function on the alphabet {s1 , â¢â¢â¢ , sn}. Then the program .9 in 
the Post-Turing language :T is said to compute f if when started in the 
tape configuration 
it eventually halts if and only if f(x 1 , â¢â¢â¢ , xm) is defined and if, on halting, 
the string f(x 1 , â¢â¢â¢ , xm) can be read off the tape by ignoring all symbols 
other than s1 , â¢â¢â¢ , sn. (That is, any "markers" left on the tape as well as 
blanks are to be ignored.) Note that we are thus permitting .9 to contain 
instructions that mention symbols other than s1 , â¢â¢â¢ , sn. 
The program .9 will be said to compute f strictly if two additional 
conditions are met: 
1. no instruction in .9 mentions any symbol other than s0 , s1 , â¢â¢â¢ , sn; 
2. whenever .9 halts, the tape configuration is of the form 
... BBBByBB 
i 
where the string y contains no blanks. 
... ' 
Thus when .9 computes f strictly, the output is available in a consecu-
tive block of squares on the tape. 
Exercises 
1. Write out the computation performed by the Post-Turing program in 
Fig. 4.4 on input string s1s2s2s1 â¢ Do the same for input s1s2s3s1 â¢ 
2. Write out the computation performed by the Post-Turing program in 
Fig. 4.5 on input string s 1 s 1 Bs 1 s 1 s 1 â¢ Do the same for input 
s1s1Bs1Bs1s1 â¢ 
3. For each of the following functions, construct a Post-Turing program 
that computes the function strictly. 
(a) f(u, v) = ;;v. 
(b) 
the predicate P(x) given in Exercise 2.4. 
(c) 
the function f(x) = xR (see Exercise 2.7). 
(d) 
the function #(u, v) given in Exercise 1.6. 

5. Simulation of .9';, in fT 
135 
4. For each of the following functions, construct a Post-Turing program 
using only the symbols s0 , s1 that computes the function in base 1 
strictly. 
(a) f(x,y) =x + y. 
(b) f(x) = 2x. 
(c) 
f(x, y) = x..:... y. 
(d) f(x, y) = 2x + y ..:... 1. 
5. Construct a Post-Turing program using only the symbols s0 , s1 , s2 
that computes the function s(x) = x + 1 in base 2 strictly. 
5. 
Simulation of Y, in !T 
In this section we will prove 
Theorem 5.1. If f(x 1 , â¢â¢â¢ , xm) is partially computable in Y,., then there is 
a Post-Turing program that computes f strictly. 
Let .9 be a program in Y,. which computes f. We assume that in 
addition to the input variables X 1 , â¢â¢â¢ , Xm and the output variable Y, .9 
uses the local variables zl ' ... ' zk. Thus, altogether .9 uses m + k + 1 
variables: 
We set I = m + k + 1 and write these variables, in the same order, as 
VI , ... ,Jii. 
We shall construct a Post-Turing program t!l that simulates .9 step by 
step. Since all of the information available to t!l will be on the tape, we 
must allocate space on the tape to contain the values of the variables 
V1 , â¢â¢â¢ , Vi. Our scheme is simply that at the beginning of each simulated 
step, the tape configuration will be as follows: 
B x 1 B x2 B ... B xm B z1 B ... B zk B y, 
t 
where x 1 , x 2 , â¢â¢â¢ , xm, z1 , â¢â¢â¢ , zk, y are the current values computed for 
the variables XI' Xz' ... ' xm' zl' ... ' zk' Y. This scheme is especially 
convenient in that the initial tape configuration 
B x 1 B x2 B . .. B Xm 
i 
is already in the correct form, since the remaining variables are initialized 
to be 0. So we must show how to program the effect of each instruction 

136 
Chapter 5 Calculations on Strings 
type of s-:, in the language :T. Various macros in Ywill be useful in doing 
this, and we now present them. 
The macro 
has the expansion 
The macro 
has the expansion 
Similarly the macro 
has the expansion 
The macro 
has the expansion 
GOTOL 
IF s0 GOTO L 
IF s1 GOTO L 
IF sn GOTO L 
RIGHT TO NEXT BLANK 
[A] 
RIGHT 
IF B GOTO E 
GOTOA 
LEFT TO NEXT BLANK 
[A] 
LEFT 
IF B GOTO E 
GOTOA 
MOVE BLOCK RIGHT 
[C] 
LEFT 
IF s0 GOTO A 0 
IF s1 GOTOA 1 
IF sn GOTO An 
RIGHT } 
PRINTs; . _ 
LEFT 
1- 1,2, ... ,n 
GOTOC 
RIGHT 
PRINT B 
LEFT 

5. Simulation of Y, In .'T 
137 
The effect of the macro MOVE BLOCK RIGHT beginning with a tape 
configuration 
B 
.___I - - - - - - - - '  i 
in which the string in the rectangular box contains no blanks, is to 
terminate with the tape configuration 
B B I 
i 
~-----------~ 
Finally we will use the macro 
whose expansion is 
ERASE A BLOCK 
[A] 
RIGHT 
IF B GOTO E 
PRINT B 
GOTOA 
This program causes the head to move to the right, with everything erased 
between the square at which it begins and the first blank to its right. 
We adopt the convention that a number ~ 0 in square brackets after 
the name of a macro indicates that the macro is to be repeated that 
number of times. For example, 
is short for 
RIGHT TO NEXT BLANK 
[3] 
RIGHT TO NEXT BLANK 
RIGHT TO NEXT BLANK 
RIGHT TO NEXT BLANK 
We are now ready to show how to simulate the three instruction types in 
the language~ by Post-Turing programs. We begin with 
In order to place the symbol s; to the left of the jth variable on the tape, 
the values of the variables Vj, ... , V[ must all be moved over one square to 
the right to make room. After the s; has been inserted, we must remember 

138 
Chapter 5 Calculations on Strings 
to go back to the blank at the left of the value of V1 in order to be ready 
for the next simulated instruction. The program is 
RIGHT TO NEXT BLANK [I] 
MOVE BLOCK RIGHT[/ - j + 1] 
RIGHT 
PRINTs; 
LEFf TO NEXT BLANK Ul 
Next we must show how to simulate 
The complication is that if the value of l-j is the null word, we want it left 
unchanged. So we move to the blank immediately to the right of the value 
of l-j. By moving one square to the left we can detect whether the value of 
l-j is null (if it is, there are two consecutive blanks). Here is the program: 
RIGHT TO NEXT BLANK [j] 
LEFf 
IF B GOTO C 
MOVE BLOCK RIGHT [j] 
RIGHT 
GOTOE 
[ C] 
LEFf TO NEXT BLANK [j - 1] 
The final instruction type in .57, is 
IF l-j ENDS s; GOTO L 
and the corresponding Post-Turing program is 
RIGHT TO NEXT BLANK [j] 
LEFf 
IF s; GOTO C 
GOTOD 
[ C] 
LEFf TO NEXT BLANK [j] 
GOTOL 
[D] 
RIGHT 
LEFf TO NEXT BLANK [j] 
This completes the simulation of the three instruction types of .57, . 
Thus, given our program .9 in the language .57,, we can compile a 

5. Simulation of .9';, In !T 
139 
corresponding program of :T. When this corresponding program termi-
nates, the tape configuration will be 
.. . B B B x 1 B . .. B xm B z1 B .. . B zk B y B B B ... , 
i 
where the values between blanks are those of the variables of .9 on its 
termination. However, we wish only y to remain as output. Hence to 
obtain our program ~ in the language :T we put at the end of the 
compiled Post-Turing program the following: 
ERASE A BLOCK [/- 1] 
After this last has been executed, all but the last block will have been 
erased and the tape configuration will be 
... BBBByBBB .... 
i 
Thus, the output is in precisely the form required for us to be able to 
assert that our Post-Turing program computes f strictly. 
Exercises 
1. (a) Use the construction in the proof of Theorem 5.1 to give a 
Post-Turing program that computes the function f(x) computed 
by the .9"2 program 
[A] 
IF X ENDS s1 GOTO B 
x~x-
IF X*- OGOTO A 
GOTOE 
[B] 
Y ~ s1Y 
x~xÂ­
GOTOA 
(b) Do the same as (a) for f(x 1 , x2 ). 
2. Answer question l(a) with the instruction [ B] Y ~ s 1 Y replaced by [ B] 
Y~Y+l. 
3. Give a primitive recursive function b1(n, x, z) such that any partial 
function computed by an .9;, program that has x instructions and that 
uses only variables among X to ... , X 1, Z 1 , ... , Z k , Y is computed 
strictly by a Post-Turing program with no more than b1(n, x, I+ k + 1) 
instructions. 

140 
Chapter 5 Calculations on Strings 
4. 
Give a primitive recursive function b~m>(n, x 1 , â¢â¢â¢ , xm, y, z) such that 
any partial function computed by an .?,. program in y steps on input 
XI' 0 
0 0' Xm' using only variables among XI' 0 
0 0' XI, zl' 
0 
0 0' zk' Y, is 
computed strictly by some Post-Turing program in no more than 
b~m>(n, x 1 , â¢â¢â¢ , xm, y, I + k + 1) steps. [Hint: Note that after y steps 
no variable holds a value larger than max{x 1 , â¢â¢â¢ , xm} + y.] 
5. * Give an upper bound on the length of the shortest Post-Turing 
program that computes <1>/x). [See Exercise 3.4.] 
6. 
Simulation of .:Tin Y 
In this section we will prove 
Theorem 6.1. If there is a Post-Turing program that computes the partial 
function f(x 1 , â¢â¢â¢ , xm), then f is partially computable. 
What this theorem asserts is that if the m-ary partial function f on A* 
is computed by a program of :T, then there is a program of .9 that 
computes f (regarded as an m-ary partial function on the base n numeri-
cal values of the strings). Before giving the proof we observe some of the 
consequences of this theorem. As shown in Fig. 6.1, the theorem completes 
a "circle" of implications. Thus all of the conditions in the figure are 
equivalent. To summarize: 
Theorem 6.2. Let f be an m-ary partial function on A*, where A is an 
alphabet of n symbols. Then the following conditions are all equivalent: 
1. f is partially computable; 
2. f is partially computable in .?,. ; 
Figure 6.1 

6. Simulation of .9'" in .'7' 
141 
3. f is computed strictly by a Post-Turing program; 
4. f is computed by a Post-Turing program. 
The equivalence of so many different notions of computability consti-
tutes important evidence for the correctness of our identification of 
intuitive computability with these notions, i.e., for the correctness of 
Church's thesis. 
Shifting our point of view to that of an m-ary partial function on N, we 
have 
Corollary 6.3. For any n, I ~ 1, an m-ary partial function f on N is 
partially computable in .9;, if and only if it is also partially computable in 
S'f. 
Proof. Each of these conditions is equivalent to the function f being 
partially computable. 
â¢ 
By considering the language ..9"1 we have 
Corollary 6.4. Every partially computable function is computed strictly by 
some Post-Turing program that uses only the symbols s0 , s1 â¢ 
Now we return to the proof of Theorem 6.1. Let .9 be a Post-Turing 
program that computes f. We want to construct a program ~ in the 
language ..9" that computes f. ~ will consist of three sections: 
BEGINNING 
MIDDLE 
END 
The MIDDLE section will simulate .9 in a step-by-step "interpretive" 
manner. The task of BEGINNING is to arrange the input to ~ in the 
appropriate format for MIDDLE, and the task of END is to extract the 
output. 
Let us suppose that f is an m-ary partial function on A*, where 
A = {sp ... , sn}. The Post-Turing program .9 will also use the blank B 
and perhaps additional symbols (we are not assuming that the computation 
is strict!) sn+ 1 , â¢â¢â¢ , s,. We write the symbols that .9 uses in the order 
s1 , â¢â¢â¢ ,sn,sn+ 1 , â¢â¢â¢ ,s,,B. 
The program ~ will simulate .9 by using the numbers that strings on this 
alphabet represent in base r + 1 as "codes" for the corresponding strings. 
Note that as we have arranged the symbols, the blank B represents the 
number r + 1. For this reason we will write the blank ass,+ 1 instead of s0 â¢ 
The tape configuration at a given stage in the computation by .9 will be 

142 
Chapter 5 Calculations on Strings 
kept track of by ~ using three numbers stored in the variables L, H, and 
R. The value of H will be the numerical value of the symbol currently 
being scanned by the head. The value of L will be a number which 
represents in base r + 1 a string of symbols w such that the tape contents 
to the left of the head consists of infinitely many blanks followed by w. The 
value of R represents in a similar manner the string of symbols to the right 
of the head. For example, consider the tape configuration 
. . . B B B B s2 s1 B s3 s1 s2 B B B ... . 
i 
Here r = 3, so we will use the base 4. Then we would have 
We might have 
H= 3. 
L = 2 Â· 42 + 1 Â· 4 + 4 = 40, 
R = 1Â· 4 + 2 = 6. 
An alternative representation could show some of the blanks on the left or 
right explicitly. For example, recalling that B represents r + 1 = 4, 
L = 4 Â· 43 + 2 Â· 42 + 1 Â· 4 + 4 = 296, 
R = 1 Â· 43 + 2 Â· 42 + 4 Â· 4 + 4 = 116. 
Now it is easy to simulate the instruction types of ::T by programs of .9. 
An instruction PRINTs; is simulated by 
H <r- i 
An instruction IF s; GOTO L is simulated by 
IF H = iGOTOL 
An instruction RIGHT is simulated by 
L <r- CONCAT,+ 1(L, H) 
H <r- LTEND,+ 1(R) 
R <r- LTRUNC,+ 1(R) 
IF R =1= 0 GOTO E 
R<r-r+1 

6. Simulation of .'T in .'7' 
Similarly an instruction LEFf is simulated by 
R +-- CONCAT,+ 1(H, R) 
H +-- RTEND,+ 1(L) 
L +-- RTRUNC,+ 1(L) 
IF L * 0 GOTO E 
L+-r+l 
143 
Now the section MIDDLE of tff can be assembled simply by replacing 
each instruction of go by its simulation. 
In writing BEGINNING and END we must deal with the fact that f is 
an m-ary function on {sl' ... ' sn}*. Thus the initial values of XI' ... ' xm 
for tff will be numbers that represent the input strings in base n. Theorem 
1.1 will enable us to change base as required. The section BEGINNING 
has the task of calculating the initial values of L, H, R, that is, the values 
corresponding to the tape configuration 
where the numbers x 1 , â¢â¢â¢ , xm are represented in base n notation. Thus 
the section BEGINNING of tff can simply be taken to be 
L +-r+l 
H +-r+l 
Z 1 +-- UPCHANGEn r+ 1(X1) 
Z 2 +-- UPCHANGEn. r+ I(Xz) 
Zm +-- UPCHANGEn,r+ 1(Xm) 
R 
+-- CONCAT,+ 1(Z1,r + l,Z2,r + l, ... ,r + l,Zm) 
Finally, the section END of tff can be taken simply to be 
Z +-- CONCAT,+ 1(L, H, R) 
Y +-- DOWNCHANGEn r+ I(Z) 
We have now completed the description of the program tff that simu-
lates !JO, and our proof is complete. 
â¢ 
Exercises 
I. Use the construction in the proof of Theorem 6.1 to give an .9 
program that computes the same unary function as the Post-Turing 

144 
Chapter 5 Calculations on Strings 
2. For any Post-Turing program .9, let #(.9) be #(t!!), where t!! is the 
.9 program obtained for .9 in the proof of Theorem 6.1, and let 
HALT~x, y) be defined 
HALTg-(x, y) - y is the number of a Post-Turing program 
that eventually halts on input x. 
Show that HALT~x, y) is not a computable predicate. 
3. * Show that the Post-Turing programs, under an appropriate ordering 
.90 , .91 , â¢â¢â¢ , are an acceptable programming system. [See Exercise 5.4 
in Chapter 4 for the definition of acceptable programming systems.] 

6 
Turing Machines 
1. 
Internal States 
Now we turn to a variant of the Post-Turing language that is closer to 
Turing's original formulation. Instead of thinking of a list of instructions, 
we imagine a device capable of various internal states. The device is, at any 
particular instant, scanning a square on a linear tape just like the one used 
by Post-Turing programs. The combination of the current internal state 
with the symbol on the square currently scanned is then supposed to 
determine the next "action" of the device. As suggested by Turing's 
analysis of the computation process (see Chapter 5, Section 4), we can take 
the next action to be either "printing" a symbol on the scanned square or 
moving one square to the right or left. Finally, the device must be 
permitted to enter a new state. 
We use the symbols q1 , q2 , q3 ,. â¢ â¢ to represent states and we write 
s0 , s1 , s2 , â¢â¢â¢ to represent symbols that can appear on the tape, where as 
usual s0 = B is the "blank." By a quadruple we mean an expression of one 
of the following forms consisting of four symbols: 
1. q; 
sj 
sk 
ql, 
2. q; 
sj 
R ql, 
3. q; 
sj 
L ql. 
145 

146 
Chapter 6 Turing Machines 
We intend a quadruple of type 1 to signify that in state q; scanning symbol 
sj, the device will print sk and go into state q1â¢ Similarly, a quadruple of 
type 2 signifies that in state q; scanning sj the device will move one square 
to the right and then go into state q1â¢ Finally, a quadruple of type 3 is like 
one of type 2 except that the motion is to the left. 
We now define a Turing machine to be a finite set of quadruples, no two 
of which begin with the same pair q;sj. Actually, any finite set of quadru-
ples is called a nondeterministic Turing machine. But for the present we will 
deal only with deterministic Turing machines, which satisfy the additional 
"consistency" condition forbidding two quadruples of a given machine to 
begin with the same pair q;sj, thereby guaranteeing that at any stage a 
Turing machine is capable of only one action. Nondeterministic Turing 
machines are discussed in Section 5. 
The alphabet of a given Turing machine L consists of all of the symbols 
s; wliich occur in quadruples of L except s0 â¢ 
We stipulate that a Turing machine always begins in state q1 â¢ Moreover, 
a Turing machine will halt if it is in state q; scanning sj and there is no 
quadruple of the machine which begins q;sj. With these understandings, and 
using the same conventions concerning input and output that were em-
ployed in connection with Post-Turing programs, it should be clear what it 
means to say that some given Turing machine L computes a partial 
function f on A* for a given alphabet A. 
Just as for Post-Turing programs, we may speak of a Turing machine L 
that computes a function strictly, namely: assuming that L computes f 
where f is a partial function on A*, we say that L computes f strictly if 
1. the alphabet of L is a subset of A; 
2. whenever L halts, the final configuration has the form 
By 
i 
qi 
where y contains no blanks. 
Writing s0 = B, s1 = 1 consider the Turing machine with alphabet {1}: 
ql 
B 
R 
qz 
qz 
1 
R 
qz 
qz 
B 
1 
q3 
q3 
1 
R 
q3 
q3 
B 
1 
ql. 

1. Internal States 
Table 1.1 
State 
Symbol 
B 
We can check the computation: 
8111, 8111, ... ,8111B, 81111, 81111B, 811111 
i 
i 
i 
i 
i 
i 
147 
The computation halts because there is no quadruple beginning q1l. 
Clearly, this Turing machine computes (but not strictly) the function 
f(x) = x + 2, where we are using unary (base 1) notation. The steps of the 
computation, which explicitly exhibit the state of the machine, the string of 
symbols on the tape, as well as the individual square on the tape being 
scanned, are called configurations. 
It is sometimes helpful to exhibit a Turing machine by giving a state 
versus symbol table. Thus, for example the preceding Turing machine 
could be represented as shown in Table 1.1. 
Another useful representation is by a state transition diagram. The 
Turing machine being discussed thus could be represented by the diagram 
shown in Fig. 1.1. 
We now prove 
Theorem 1.1. Any partial function that can be computed by a Post-
Turing program can be computed by a Turing machine using the same 
alphabet. 
1/R 
1/R 
Figure 1.1 

148 
Chapter 6 Turing Machines 
Proof. 
Let .9 be a given Post-Turing program consisting of the instruc-
tions Ip ... ,IK, and let s0 ,spÂ·Â·Â·,sn be a list that includes all of the 
symbols mentioned in .9. We shall construct a Turing machine L that 
simulates .9. 
The idea is that L will be in state q; precisely when .9 is about to 
execute instruction I;. Thus, if I; is "PRINT sk ,"then we place in L all of 
the quadruples 
j = 0, 1, ... , n. 
If I; is "RIGHT," then we place in L all of the quadruples 
j = 0, 1, ... , n. 
If I; is "LEFT," then we place in L all of the quadruples 
j = 0, 1, ... , n. 
Finally, if I; is "IF sk GOTO L," let m be the least number such that Im is 
labeled L if there is an instruction of .9 labeled L; otherwise let 
m = K + 1. We place in L the quadruple 
as well as all of the quadruples: 
j=0,1, ... ,n; j=l=k. 
It is clear that the actions of L correspond precisely to the instructions 
of .9, so we are done. 
â¢ 
Using Corollary 6.4 from Chapter 5 and the proof of Theorem 1.1, we 
have 
Theorem 1.2. Let f be an m-ary partially computable function on A* for 
a given alphabet A. Then there is a Turing machine L that computes f 
strictly. 
It is particularly interesting to apply this theorem to the case A = {1}. 
Thus, if f(x 1 , â¢â¢â¢ , xm) is any partially computable function on N, there is a 
Turing machine that computes f using only the symbols B and 1. The 
initial configuration corresponding to inputs x 1 , â¢â¢â¢ , x m is 
B 1[xd B . . . B 1[xmJ 
i 
q, 

1. Internal States 
149 
and the final configuration when f(xl> ... , xmH will be 
B l[f(x, , ... ,Xm)l, 
i 
qK+! 
Next we shall consider a variant notion of Turing machines: machines 
that consist of quintuples instead of quadruples. There are two kinds of 
quintuples: 
q; 
sj 
sk 
R 
qt' 
q; 
sj 
sk 
L 
qt. 
The first quintuple signifies that when the machine is in state q; scanning 
sj it will print sk and then move one square to the right and go into state 
q1â¢ And naturally, the second quintuple is the same, except that the motion 
is to the left. A finite set of quintuples no two of which begin with the 
same pair q;sj is called a quintuple Turing machine. We can easily prove 
Theorem 1.3. Any partial function that can be computed by a Turing 
machine can be computed by a quintuple Turing machine using the same 
alphabet. 
Proof. Let L be a Turing machine with states q1 , â¢â¢â¢ , qK and alphabet 
{s1 , â¢â¢â¢ , sn}. We construct a quintuple Turing machine L to simulate L. 
The states of L will be q1 , â¢â¢â¢ , qK, qK+ 1 , â¢â¢â¢ , qzKÂ· 
For each quadruple of L of_!he form q; sj R q1 we place the correspond-
ing quintuple q; sj sj R q1 in .L. Similarly, for_each quadruple q; sj L q1 in 
L, we place the quintuple q; sj sj L q1 in .L. And, for each quadruple 
q; sj sk q1 !!_I L, we place in L the quintuple q; sj sk RqK+tÂ· Finally we 
place in L all quintuples of the form 
i=l, ... ,K; j=O,l, ... ,n. 
Quadruples requiring motion are simulated easily by quintuples. But a 
quadruple requiring a "print" necessitates using a quintuple which causes 
a motion after the "print" has taken place. The final list of quintuples 
undoes the effect of this unwanted motion. The extra states qK+ 1 , â¢â¢â¢ , q2K 
serve to "remember" that we have gone a square too far to the right. 
â¢ 
Finally, we will complete another circle by proving 
Theorem 1.4. Any partial function that can be computed by a quintuple 
Turing machine can be computed by a Post-Turing program using the 
same alphabet. 

150 
Chapter 6 Turing Machines 
Combining Theorems 1.1, 1.3, and 1.4, we will have 
Corollary 1.5. For a given partial function f, the following are equivalent: 
1. f can be computed by a Post-Turing program; 
2. f can be computed by a Turing machine; 
3. f can be computed by a quintuple Turing machine. 
Proof of Theorem 1.4. Let L be a given quintuple Turing machine with 
states q1 , â¢â¢â¢ , qK and alphabet {s1 , â¢â¢â¢ , sn}. We associate with each state qi 
a label A; and with each pair qisj a label B;jÂ· Each label A; is to be 
placed next to the first instruction in the filter: 
[A;] 
IF s0 GOTO B;o 
IF s 1 GOTO B;1 
If L contains the quintuple qi sj sk Rq1, then we introduce the block of 
instructions 
[B;j] 
PRINT sk 
RIGHT 
GOTOA 1 
Similarly, if L contains the quintuple q; sj sk L q1, then we introduce the 
block of instructions: 
[B;) 
PRINT sk 
LEFT 
GOTOA 1 
Finally, if there is no quintuple in L 
beginning qisj, we introduce the 
block 
[Bij] 
GOTO E 
Then we can easily construct a Post-Turing program that simulates L 
simply by putting all of these blocks and filters one under the other. The 
order is irrelevant except for one restriction: The filter labeled A 1 must 
begin the program. The entire program is listed in Figure 1.2. 
â¢ 

1. Internal States 
Exercises 
[Ad 
IF s0 GOTO 8 10 
IF sn GOTO B,n 
IF s0 GOTO B20 
IF sn GOTO Bzn 
IF Sn GOTO BKn 
[B; 111 ] 
PRINT sk, 
RIGHT 
GOTOA 1, 
[B;2h] 
PRINT sk 2 
Figure 1.2 
1. Let T be the Turing machine consisting of the quadruples 
ql 
B 
R 
qz 
qz 
1 
R 
q3 
q3 
B 
R 
q4 
q4 
1 
B 
ql 
q4 
B 
R 
q4. 
151 
For each integer x, let g(x) be the number of occurrences of 1 on the 
tape when and if T halts when started with the read-write head one 
square to the left of the initial 1, with input 1[xJ. What is the function 
g(x)? 
2. Write out the quadruples constituting a Turing machine that com-
putes the function 
f(x) = {~ if x is a perfect square 
otherwise 
in base 1. Exhibit the state transition diagram for your machine. 
3. Give precise definitions of configuration, computation, and Turing 
machine L computes the function f. (Compare Chapter 2, Section 3.) 

152 
Chapter 6 Turing Machines 
4. For each of the following functions, construct a Turing machine that 
computes the function strictly. 
(a) f(u, v) = ;7;. 
(b) 
P(x) = x has an even number of symbols. 
(c) 
f(x) given in Exercise 1.4 in Chapter 5. 
(d) f(x) = xR. [xR is defined in Chapter 1, Section 3.] 
(e) 
#(u, v) given in Exercise 1.6 in Chapter 5. 
5. Construct Turing machines for Exercise 4.4 in Chapter 5. 
6. Construct a Turing machine for Exercise 4.5 in Chapter 5. 
7. 
Using the construction in the proof of Theorem 1.1, transform the 
Post-Turing program in Figure 4.4 of Chapter 5 into an equivalent 
Turing machine. 
8. 
Using the construction in the proof of Theorem 1.3, transform the 
Turing machine in Table 1.1 into an equivalent quintuple Turing 
machine. 
9. Construct a quintuple Turing machine that computes f(x, y) = x ..:... y 
in base 1 strictly. 
10. * Show that any partially computable function can be computed by a 
quintuple Turing machine with two states. [Hint: A quintuple Turing 
machine L 
with n states and m symbols (including s0 ) can be 
simulated by a quintuple Turing machine L' with two states and 
4mn + m symbols. The 4mn new symbols represent the current state 
and currently scanned symbol of L, as well as additional bookkeep-
ing information. Transferring this stored information to an adjacent 
square can be done by a "loop" that moves the tape head back and 
forth.] 
2. 
A Universal Turing Machine 
Let us now recall the partially computable function <l>(x, z) from Chapter 
4. For fixed z, <l>(x, z) is the unary partial function computed by the 
program whose number is z. Let L 
be a Turing machine (in either 
quadruple or quintuple form) that computes this function with alphabet 
{1}. For reasons that we will explain, it is appropriate to call this machine 
L universal. 
Let g(x) be any partially computable function of one variable and let z0 
be the number of some program in the language Y that computes g. Then 

3. The Languages Accepted by Turing Machines 
153 
if we begin with a configuration 
(where x and z0 are written as blocks of ones, i.e., in unary notation), and 
let L proceed to compute, L will compute ci>(x, z0), i.e., g(x). Thus, L 
can be used to compute any partially computable function of one variable. 
L 
provides a suggestive model of an all-purpose computer, in which 
data and programs are stored together in a single "memory." We can think 
of z0 as a coded version of the program for computing g and x as the 
input to that program. Turing's construction of a universal computer in 
1936 provided reason to believe that, at least in principle, an all-purpose 
computer would be possible, and was thus an anticipation of the modern 
digital computer. 
Exercises 
1.* (a) Define a numbering #(L) of Turing machines like the number-
ing #(.9) of Y programs given in Chapter 4. 
(b) Prove a version of the parameter theorem for Turing machines. 
(c) 
Prove a version of the recursion theorem for Turing machines. 
(d) Show that there is a Turing machine L that prints #(L) when 
started with any input tape. 
(e) Show that Turing machines are an acceptable programming sys-
tem. [Acceptable programming systems are defined in Exercise 
5.4 in Chapter 4.] 
2. * Give an upper bound on the size of the smallest universal Turing 
machine. [See Exercise 5.5 in Chapter 5.] 
3. The Languages Accepted by Turing Machines 
Given a Turing machine L with alphabet A = {s1, ... , sn}, a word u E A* 
is said to be accepted by L if when L begins with the configuration 

154 
Chapter 6 Turing Machines 
it will eventually halt. The set of all words u E A* that L accepts is called 
the language accepted by L. An important problem in the theory of 
computation involves characterizing the languages accepted by various 
kinds of computing devices. It is easy for us to solve this problem for 
Turing machines. 
Theorem 3.1. A language is accepted by some Turing machine if and only 
if the language is r.e. 
Proof. 
Let L be the language accepted by a Turing machine L with 
alphabet A. Let g(x) be the unary function on A* that L computes. 
Then g 4s a partially computable function (by Corollary 1.5 and by 
Theorem 6.2 in Chapter 5). Now, 
L = {x EA*Ig(xH}. 
(3.1) 
Hence L is r.e. 
Conversely, let L be r.e. Then there is a partially computable function 
g(x) such that (3.1) holds. Using Theorem 1.2, let L be a Turing machine 
with alphabet {s1 , â¢â¢â¢ , sn} that computes g(x) strictly. Then L accepts L. 
Naturally Theorem 3.1 is also true for quintuple Turing machines. 
Let us consider the special case A = {1}. Then we have 
â¢ 
Theorem 3.2. A set U of numbers is r.e. if and only if there is a Turing 
machine L with alphabet {1} that accepts 1[xJ if and only if x E U. 
Proof. This follows immediately from Theorem 3.1 and the fact that the 
base 1 representation of the number x is the string 1[xJ. 
â¢ 
This is an appropriate place to consider some annoying ambiguities in 
our notation of r.e. language. Thus, for example, consider the language 
L 0 = {a[nll n > 0}, 
on the alphabet {a, b}. According to our definitions, to say that L 0 is an 
r.e. language is to say that the set of numbers which the strings in L 0 
represent in base 2 is an r.e. set of numbers. But, this set of numbers is not 
determined until an order is specified for the letters of the alphabet. If we take 
a, b in the order shown, then the set of numbers which represent strings in 
L 0 is clearly 
Q1 = {2n -
11 n > 0}, 

3. The Languages Accepted by Turing Machines 
155 
while if we take the letters in the order b, a, the set of numbers which 
represents strings in L 0 is 
Now, although there is no difficulty whatever in showing that Q1 and Q2 
are both r.e. sets, it is nevertheless a thoroughly unsatisfactory state of 
affairs to be forced to be concerned with such matters in asserting that L 0 
is an r.e.language. Here Theorem 3.1 comes to our rescue. The notion of a 
given string being accepted by a Turing machine does not involve imposing 
any order on the symbols of the alphabet. Hence, Theorem 3.1 implies 
immediately that whether a particular language on a given alphabet is r.e. 
is independent of how the symbols of the alphabet are ordered. The same is 
clearly true of a language L on a given alphabet A being recursive since 
this is equivalent to L and A* - L both being r.e. 
Another ambiguity arises from the fact that a particular language may 
be considered with respect to more than one alphabet. Thus, let A be an 
n-letter alphabet and let A be an m-letter alphabet containing A, so that 
m > n. Then a language L on the alphabet A is simply some subset of 
A*, so that Lis also a language on the larger alphabet A. Thus, depending 
on whether we are thinking of L as a language on A or as a language on 
A, we will have to read the strings in L as being the notation for integers 
in base n or in base m, respectively. Hence, we are led to the unpleasant 
possibility that whether L is r.e. might actually depend on which alphabet 
we are considering. As an example, we may take A = {a} and A= {a, b}, 
and consider the language L 0 above, where 
L cA* cA*. 
0-
-
We have already seen that our original definition of L 0's being r.e. as a 
language on the alphabet A amounts to requiring that the set of numbers 
Q1 or Q2 (depending on the order of the symbols a, b) be r.e. However, if 
we take our alphabet to be A, then the relevant set of numbers is 
Q3 = {n E N I n > 0}. 
We remove all such ambiguities by proving 
Theorem 3.3. Let A k A where A and A are alphabets and let L k A*. 
Then L is an r.e. language on the alphabet A if and only if L is an r.e. 
language on A. 
Proof. Let L be r.e. on A and let L be a Turing machine with alphabet 
A that accepts L. Without loss of generality, we can assume that L begins 

156 
Chapter 6 Turing Machines 
by moving right until it finds a blank and then returns to its original 
position. Let L be obtained from L 
by adjoining to it the quadruples 
q s s q for each symbol s E A - A, and each state q of L. Thus L will 
enter an "infinite loop" if it ever encounters a symbol in A -A. Since L 
has alphabet A and accepts the language L, we conclude from Theorem 
3.1 that L is an r.e. language on A. 
Conversely, let L be r.e. as a language on A, and let L be a Turing 
machine with alphabet A that accepts L. Let g(x) be the function on A* 
that L 
computes. (The symbols belonging to A - A thus serve as 
"markers.") Since L ~A*, we have 
L = {x EA*Ig(xH}. 
Since g(x) is partially computable, it follows that L is an r.e. language on 
A. 
â¢ 
Corollary 3.4. Let A, A, L be as in Theorem 3.3. Then L is a recursive 
language on A if and only if L is a recursive language on A. 
Proof. First let L be a recursive language on A. Then Land A* - L are 
r.e. languages on A and therefore on A. Moreover, since 
A* - L =(A* -A*) u (A* - L), 
and since A* -A* is r.e., as the reader can easily show (see Exercise 6), it 
follows from Theorem 4.5 in Chapter 4 that A* - L is r.e. Hence, L is a 
recursive language on A. 
Conversely, if L is a recursive language on A, then L and A* - L are 
r.e. languages on A and therefore L is an r.e. language on A. Moreover, 
since 
A* - L =(A* - L) n A*, 
and since A* is obviously r.e. (as a language on A and therefore on A), it 
follows from Theorem 4.5 in Chapter 4 that A* - L is an r.e. language on 
A and hence on A. Thus, Lis a recursive language on A. 
â¢ 
Exercises 
1. Write out the quadruples constituting a Turing machine that accepts 
the language consisting of all words on the alphabet {a, b} of the form 
a[iJba[il. 
2. Give a Turing machine that accepts {l[ilBl[j]Bl[i+j] I i, j EN}. 
3. Give a Turing machine that accepts {w E {a, b}* I w = wR}. 

4. The Halting Problem for Turing Machines 
157 
4. Show that there is a Turing machine that accepts the language 
{llxlBliYll <1>/x) J,}. 
5. Show that there is no Turing machine that accepts the language 
{l!Yll <1>/x)t for all x EN}. 
6. Complete the proof of Corollary 3.4 by showing that A* -A* is an r.e. 
language. 
4. The Halting Problem for Turing Machines 
We can use the results of the previous section to obtain a sharpened form 
of the unsolvability of the halting problem. 
By the halting problem for a fixed given Turing machine L we mean 
the problem of finding an algorithm to determine whether L will eventually 
halt starting with a given configuration. We have 
Theorem 4.1. There is a Turing machine .% with alphabet {1} that has an 
unsolvable halting problem. 
Proof. Take for the set U in Theorem 3.2, some r.e. set that is not 
recursive (e.g., the set K from Chapter 4). Let .% be the corresponding 
Turing machine. Thus .% accepts a string of ones if and only if its length 
belongs to U. Hence, x E U if and only if .% eventually halts when started 
with the configuration 
B l[x] 
i 
ql 
Thus, if there were an algorithm for solving the halting problem for Jr, it 
could be used to test a given number x for membership in U. Since U is 
not recursive, such an algorithm is impossible. 
â¢ 
This is really a stronger result than was obtained in Chapter 4. What we 
can prove about Turing machines just using Theorem 2.1 in Chapter 4 is 
that there is no algorithm that can be used, given a Turing machine and an 
initial configuration, to determine whether the Turing machine will ever 
halt. Our present result gives a fixed Turing machine whose halting 
problem is unsolvable. Actually, this result could also have been easily 
obtained from the earlier one by using a universal Turing machine. 
Next, we show how the unsolvability of the halting problem can be used 
to obtain another unsolvable problem concerning Turing machines. We 
begin with a Turing machine .% with alphabet {1} that has an unsolvable 

158 
Chapter 6 Turing Machines 
halting problem. Let the states of.% be q 1 , â¢â¢â¢ , qk. We will construct a 
Turing machine .% by adjoining to the quadruples of .% the following 
quadruples: 
q; B B qk+l 
for i = 1, 2, ... , k for which no quadruple of.% begins q;B, and 
q; 1 1 qk+ I 
for i = 1, 2, ... , k when no quadruple of.% begins q; 1. Thus, .% eventually 
halts beginning with a given configuration if and only if .% eventually is in 
state qk+ 1 â¢ We conclude 
Theorem 4.2. There is a Turing machine .% with alphabet {1} and a state 
qm such that there is no algorithm that can determine whether.% will ever 
arrive at state qm when it begins at a given configuration. 
Exercises 
1. Prove that there is a Turing machine .L such that there is no 
algorithm that can determine of a given configuration whether .L will 
eventually halt with a completely blank tape when started with the 
given tape configuration. 
2. 
Prove that there is a Turing machine .L with alphabet {s1 , s2} such 
that there is no algorithm that can determine whether .L starting with 
a given configuration will ever print the symbol s2 â¢ 
3. Let .L0 , .L1 , â¢ â¢ â¢ be a list of all Turing machines, and let /; be the 
unary partial function computed by L;, i = 0, 1, .... Suppose g(x) is a 
total function such that for all x ~ 0 and all 0 :::;; i :::;; x, if /;CxH then 
/;(x) < g(x). Show that g(x) is not computable. 
4. Jill and Jack have been working as programmers for a year. They are 
discussing their work. We listen in: 
JACK: We are working on a wonderful program, AUTOCHECK. 
AUTOCHECK will accept Pascal programs as inputs and will return 
the values OK or LOOPS depending on whether the given program 
is or is not free of infinite loops. 
JILL: Big deal! We have a mad mathematician in our firm who has 
developed an algorithm so complicated that no program can be 
written to execute it no matter how much space and time is allowed. 
Comment on and criticize Jack and Jill's statements. 

5. Nondeterministic Turing Machines 
159 
5. 
Nondeterministic Turing Machines 
As already mentioned, a nondeterministic Turing machine is simply an 
arbitrary finite set of quadruples. Thus, what we have been calling a Turing 
machine is simply a special kind of nondeterministic Turing machine. 
For emphasis, we will sometimes refer to ordinary Turing machines as 
deterministic. 
A configuration 
is called terminal with respect to a given nondeterministic Turing machine 
(and the machine is said to halt) if it contains no quadruple beginning 
q; sj. (This, of course, is exactly the same as for deterministic Turing 
machines.) We use the symbol I- (borrowed from logic) placed between a 
pair of configurations to indicate that the transition from the configuration 
on the left to the one on the right is permitted by one of the quadruples of 
the machine under consideration. 
As an example, consider the nondeterministic Turing machine given by 
the quadruples 
ql 
B 
R 
qz 
qz 
1 
R 
q3 
qz 
B 
B 
q4 
q3 
1 
R 
qz 
q3 
B 
B 
q3 
q4 
B 
R 
q4 
q4 
B 
B 
q5 
Then we have 
B 1 1 1 li-B 1 1 111--B 1 1 111--B1 1111--B11 1 1 
i 
i 
i 
i 
i 
ql 
qz 
q3 
qz 
q3 
I- B 1 1 1 1 B BI-B 1 1 1 1 B. 
i 
i 

160 
Chapter 6 Turing Machines 
So far the computation has been entirely determined; however, at this 
point the nondeterminism plays a role. We have 
B 1 1 1 1 BI-B 1 1 1 1 B, 
i 
i 
q4 
at which the machine halts. But we also have 
B 1111 BI-B 1111 BBI--B 1111 BBBI--
i 
i 
i 
Let A = {s 1 , ... , s n} be a given alphabet and let u E A*. Then the 
nondeterministic Turing machine L is said to accept u if there exists a 
sequence of configurations y 1 , y 2 , â¢â¢â¢ , 'Ym such that y 1 is the configuration 
'Ym is terminal with respect to L, and y 1 I- y2 I- y 3 I- Â· Â· Â· I- 'Ym. In this 
case, the sequence y 1 , y 2 , â¢â¢â¢ , 'Ym is called an accepting computation by L 
for u. If A is the alphabet of L, then the language accepted by L is the set 
of all u E A* that are accepted by L. 
Of course, for deterministic Turing machines, this definition gives noth-
ing new. However, it is important to keep in mind the distinctive feature of 
acceptance by nondeterministic Turing machines. It is perfectly possible to 
have an infinite sequence 
'Y1 I- 'Yz I- 'Y3 1--
of configurations, where y 1 is 
s0 u 
i 
ql 
even though u is accepted by L. It is only necessary that there be some 
sequence of transitions leading to a terminal configuration. One some-
times expresses this by saying, "The machine is always permitted to guess 
the correct next step." 
Thus in the example given above, taking the alphabet A = {1}, we have 
that L accepts 1111. In fact the language accepted by L is {l[Znl}. (See 
Exercise 3.) 

5. Nondeterministic Turing Machines 
161 
Since a Turing machine is also a nondeterministic Turing machine, 
Theorem 3.1 can be weakened to give 
Theorem 5.1. For every r.e. language L, there is a nondeterministic 
Turing machine L that accepts L. 
The converse is also true: the language accepted by a nondeterministic 
Turing machine must be r.e. By Church's thesis, it is clear that this should 
be true. It is only necessary to "run" a nondeterministic Turing machine 
L on a given input u, following all alternatives at each step, and giving the 
value (say) 0, if termination is reached along any branch. This defines a 
function that is intuitively partially computable and whose domain is the 
language accepted by L. However, a detailed proof along these lines 
would be rather messy. 
Fortunately the converse of Theorem 5.1 will be an easy consequence of 
the methods we will develop in the next chapter. 
Exercises 
1. Explain why nondeterministic Turing machines are unsuitable for 
defining functions. 
2. Let L be the set of all words on the alphabet {a, b} that contain at 
least two consecutive occurrences of b. Construct a nondeterministic 
Turing machine that never moves left and accepts L. 
3. Show that the nondeterministic Tudng machine L used as an exam-
ple in this section accepts the set {ll2nl}. 
4. Let 
L 1 = {w E {a, b}* I w has an even number of a's}, 
L2 ={wE {a, b}* I w has an odd number of b's}. 
(a) Give deterministic Turing machines L 1 , L 2 that accept L 1 , L 2 , 
respectively, and combine them to get a nondeterministic Turing 
machine that accepts L 1 u L 2 â¢ 
(b) Give a deterministic Turing machine that accepts L 1 U L 2 â¢ 
5. Give a nondeterministic Turing machine that accepts {llnJI n is prime}. 
6. If we replace "the first instruction labeled L" by "some instruction 
labeled L" in the interpretation of Post-Turing instructions of the 
form IF u GOTO L, then we get nondeterministic Post-Turing pro-
grams. Show that a language is accepted by a nondeterministic Post-

162 
Chapter 6 Turing Machines 
Turing program if and only if it is accepted by a nondeterministic 
Turing machine (where acceptance of a language by a Post-Turing 
program is defined just like acceptance by a Turing machine). 
6. Variations on the Turing Machine Theme 
So far we have three somewhat different formulations of Turing's concep-
tion of computation: the Post-Turing programming language, Turing 
machines as made up of quadruples, and quintuple Turing machines. The 
proof that these formulations are equivalent was quite simple. This is true 
in part because all three involved a single tapehead on a single two-way 
infinite tape. But it is easy to imagine other arrangements. In fact, Turing's 
original formulation was in terms of a tape that was infinite in only one 
direction, that is, with a first or leftmost square (see Fig. 6.1). We can also 
think of permitting several tapes, each of which can be one-way or two-way 
infinite and each with its own tapehead. There might even be several 
tapeheads per tape. As one would expect, programs can be shorter when 
several tapes are available. But, if we believe Church's thesis, we certainly 
would expect all of these formulations to be equivalent. In this section we 
will indicate briefly how this equivalence can be demonstrated. 
Let us begin by considering one-way infinite tapes. To make matters 
definite, we assume that we are representing a Turing machine as a set of 
quadruples. It is necessary to make a decision about the effect of a 
quadruple q; si L qk in case the tapehead is already at the left end of the 
tape. There are various possibilities, and it really does not matter very 
much which we adopt. For definiteness we assume that an instruction to 
move left will be interpreted as a halt in case the tapehead is already at 
the leftmost square. Now it is pretty obvious that anything that a Turing 
machine could do on a one-way infinite tape could also be done on a 
two-way infinite tape, and we leave details to the reader. 
How can we see that any partially computable function can be computed 
by a Turing machine on a one-way infinite tape? One way is by simply 
examining the proof of Theorem 5.1 in Chapter 5, which shows how a 
Figure 6.1. Two-way infinite versus one-way infinite tape. 

6. Variations on the Turing Machine Theme 
163 
... 1 I I I I I I I I I I I 
1 ... 
] 1 I I I I l I I I I I IÂ·Â·Â· 
\ 
1111111 
u 
Figure 6.2 
computation in any of the languages .9';, can be simulated by a program in 
the Post-Turing language ::T. In fact, the program tff in the language :T 
which is constructed to simulate a given program fJlJ in the language .9';, 
has the particular property that when tff is executed, the tapehead never 
moves to the left of the square initially scanned. Hence, the program tff 
would work exactly as well on a one-way infinite tape whose leftmost 
square is initially scanned. And, it is an easy matter, as in the proof of 
Theorem 1.1, to convert tff into a Turing machine. 
Although this is an entirely convincing argument, we would like to 
mention another approach which is interesting in its own right, namely, we 
directly face the question, how can the information contained in a two-way 
infinite tape be handled by a Turing machine with one tapehead on a 
one-way infinite tape? The intuitive idea is to think of a two-way infinite 
tape as being "hinged" so it can be folded as in Fig. 6.2. Thus our two-way 
infinite tape can be represented by a one-way infinite tape with two 
"tracks," an "upper" and a "lower." Moreover, by adding enough symbols 
to the alphabet, we can code each pair consisting of an upper and a lower 
symbol by a single symbol. 
Thus, let us begin with a Turing machine L 
with alphabet A = 
{s1 , â¢â¢â¢ ,sn} and states qpÂ·Â·Â·â¢qK. Let L compute a unary 1 partial func-
tion g on A~, where A 0 ~A. Thus the input configuration when L is 
computing g(x) for x E A~ will be 
1 The restriction to unary functions is, of course, not essential. 

164 
Chapter 6 Turing Machines 
We will construct a Turing machine L that computes g on a one-way 
infinite tape. The initial configuration for .L will be 
# B X 
i 
ql 
where # is a special symbol that will occupy the leftmost square on the 
tape for most of the computation. The alphabet of L will be 
Au{#} u {bj IO 5o i,j 5o n}, 
where we think of the symbol bj as indicatin[ that s; is on the upper track 
and sj is on the lower track. The states of L are q1 , q2 , q3, q4 , q5 , and 
{ij;,t/;li = 1,2, ... ,K} 
as well as certain additional states. 
We can think of the quadruples constituting Las made up of three 
sections: BEGINNING, MIDDLE, and END. BEGINNING serves to copy 
the input on the upper track putting blanks on the corresponding lower 
track of each square. BEGINNING consists of the quadruples 
ql 
B 
R 
qz 
qz 
S; 
R 
qz 
i = l,2, ... ,n, 
qz 
B 
L 
q3 
q3 
S; 
b; 0 
q3 
i = O,l,2, ... ,n, 
q3 
bi 0 
L 
q3 
i = O,l,2, ... ,n, 
q3 
# 
R 
iit . 
Thus, starting with the configuration 
# B s2 s1 s3 
i 
ql 
BEGINNING will halt in the configuration 
# bg b~ bA b~ B. 
i 
ql 
Note that bg is different from s0 =B. MIDDLE will consist of quadruples 
corresponding to those of L as well as additional quadruples as indicated 

6. Variations on the Turing Machine Theme 
165 
Table 6.1 
Quadruple of .If 
Quadruple of .L 
(a) q; 
si 
sk 
qt 
li; 
bi 
m 
bk 
m 
lit 
m = 0, l, ... ,n 
ii; 
b~ 
l 
bf:' 
iit 
m = 0, l, ... ,n 
(b) qi 
si 
R qt 
li; 
bi 
m 
R 
lit 
m = 0, l, ... ,n 
ii; 
b~ 
l 
L 
iit 
m = 0, l, ... ,n 
(c) q; 
si 
L qt 
li; 
bi 
m 
L 
lit 
m = O,l, ... ,n 
ii; 
b~ 
l 
R 
iit 
m = 0, l, ... ,n 
(d) 
li; 
B 
bo 
0 
li; 
i = 1,2, ... , K 
ii; 
B 
bo 
0 
li; 
i = l,2, ... ,K 
(e) 
li; 
# 
R 
ii; 
i = l,2, ... ,K 
ii; 
# 
R 
li; 
i = l,2, ... ,K 
in Table 6.1. The states ii;, ij; correspond to actions on the upper track and 
lower track, respectively. Note in (b) and (c) that on the lower track left 
and right are reversed. The quadruples in (d) replace single blanks B by 
double blanks bg as needed. The quadruples (e) arrange for switchover 
from the upper to the lower track. It should be clear that MIDDLE 
simulates L. 
END has the task of translating the output into a word on the original 
alphabet A. This task is complicated by the fact that the output is split 
between the two tracks. To begin with, END contains the following 
quadruples: 
ii; bj 
bj 
q4) 
whenever L contains no quadruple 
m 
m 
iji bm 
b~ q4 
beginning q;sj, for m = 0, 1, ... , n; 0 ~ i, j ~ n, 
1 
1 
q4 b! 
1 
L 
q4' 
q4 # 
B 
q5. 
For each initial configuration for which L 
halts, the effect of BEGIN-
NING, MIDDLE, and this part of END is to ultimately produce a 
configuration of the form 
B b!t b!z 
b!k 
1I 
1z 
1k â¢ 
i 
q5 
The remaining task of END is to convert the tape contents into 

166 
[D] 
RIGHT TO NEXT BLANK 
MOVE BLOCK RIGHT 
RIGHT 
[C] 
RIGHT 
Chapter 6 Turing Machines 
IF bj GOTO A} 
(0 ~ i,j ~ n) 
IF B GOTOF 
GOTOC 
[A}] 
PRINTs; 
(0 < i ~ n,O ~j ~ n) 
GOTOBi 
[AJI 
PRINT# 
(0 ~j ~ n) 
GOTOBi 
[B) 
LEFTTONEXTBLANK 
(0 <j ~ n) 
PRINT si 
GOTOD 
[B0 ] 
LEFT TO NEXT BLANK 
PRINT# 
GOTOD 
[F] 
LEFT 
IF si GOTO F 
(0 < j ~ n) 
IF# GOTOG 
IF B GOTOE 
[G] 
PRINT B 
GOTOF 
Figure6.3 
Instead of giving quadruples for accomplishing this, we exhibit a program 
in the Post-Turing language :T, so that we can make use of some of the 
macros available in that language. Of course, this program can easily be 
translated into a set of quadruples using the method of proof of Theorem 
1.1. Because our macros for :T were designed for use with "blocks" of 
symbols containing no blanks, we will use # instead of s0 = B in carrying 
out our translation. One final pass will be needed to replace each # by B. 
The program is given in Fig. 6.3. 
Each bJ is processed going from left to right. bJ is replaced by s; (or by 
# if i = 0) and si (or # if j = 0) is printed on the left. The "MOVE 
BLOCK RIGHT" macro is used to make room on the tape for printing the 
successive symbols from the "lower" track. As an example let us apply the 
program of Fig. 6.3 to the configuration 

6. Variations on the Turing Machine Theme 
B 
bf 
b\1 
b/1 
i 
B 
B 
i 
B 
B 
B 
B 
B 
s1 
i 
B 
# 
i 
B 
B 
b2 I 
bll 
I 
b2 I 
bll 
I 
i 
s2 
bll 
I 
i 
s2 
bo 
I 
sl 
s2 
sl 
s2 
sl 
sl 
# 
sl 
b/1 
B 
bl 0 
B 
bl 
0 
B 
bl 0 
B 
bll 
I 
b/1 
i 
# 
b/1 
s2 
# 
sl 
s2 
Figure6.4 
167 
D 
c 
A2 
I 
Bl 
D 
B 
Ao I 
B 
D 
sl 
B 
D 
# 
sl 
B 
F 
i 
B 
E 
In Fig. 6.4 we show various stages in the computation; in each case the 
tape configuration is followed by the label on the next instruction to be 
executed. 
The technique of thinking of the tape of a Turing machine as decom-
posed into a number of parallel tracks has numerous uses. (It will appear 
again in Chapter 11.) For the moment we note that it can be used to 
simulate the behavior of a multitape Turing machine by an ordinary 
Turing machine. For, in the first place a second track can be used to show 
the position of a tapehead on a one-tape machine as in the example shown 
in Fig. 6.5; the 1 under the s3 shows the position of the head. In an entirely 
similar manner the contents of k tapes and the position of the tapehead 
on each can be represented as a single tape with 2k tracks. Using this 
representation, it is easy to see how to simulate any computation by a 
k-tape Turing machine using only one tape. The same result can also be 
obtained indirectly using the method of proof of Theorem 6.1 in Chapter 5 
to show that any function computed by a k-tape Turing machine is 
partially computable. 

168 
Chapter 6 Turing Machines 
.. Â·I 8 I s, I 53 1 8 I s, I 8 IÂ· .. 
i 
8 
s, 
SJ 
8 
s, 
8 
8 
8 
I 
8 
8 
8 
Figure 6.5 
Exercises 
1. Give a formal description of a Turing machine that uses three tapes: 
one with a "read only" head for input, one with a "write only" head 
for output, and one for "working." Give an appropriate definition of 
computability by such machines and prove the equivalence with com-
putability by ordinary Turing machines. 
2. Do the same for a Turing machine with input tape, output tape, and k 
working tapes for any k ~ 1. 
3. Let the Post-Turing language be augmented by the instructions UP, 
DOWN so that it can deal with computations on a two-dimensional 
"tape" infinite in all four directions. Supply an appropriate definition 
of what it means to compute a function by a program in this language, 
and prove that any function computed by such a program is partially 
computable. 
4. Adapt the construction in this section so that it works for binary 
functions. 

7 
Processes and Grammars 
1. Semi-Thue Processes 
In this chapter we will see how the methods of computability theory can be 
used to deal with combinatorial problems involving substitution of one 
substring for another in a string. 
Definition. Given a pair of words g, g on some alphabet, the expression 
g~g 
is called a semi-Thue production or simply a production. The term rewrite 
rule is also used. 
Thue is from Axel Thue, a Norwegian mathematician, and is pro-
nounced too-ay. 
If P is the semi-Thue production g ~ g, then we write 
to mean that there are (possibly null) words r, s such that 
u = rgs 
and 
v = rgs. 
(In other words, v is obtained from u by a replacement of g by g.) 
169 

170 
Chapter 7 Processes and Grammars 
Definition. A semi-Thue process is a finite set of semi-Thue productions. 
If n is a semi-Thue process, we write 
to mean that 
u=v 
n 
u=v 
p 
for some production P which belongs to n. Finally, we write 
if there is a sequence 
. 
u=v 
II 
u = u = u = ... = u = v. 
I n 
2 II 
n 
n 
The sequence u 1 , u 2 , â¢â¢â¢ , un is then called a derivation of v from u. In 
particular (taking n = 1) 
u * 
u. 
When no ambiguity results we often omit the explicit reference to n, 
writing simply u = v and u ~ v. 
Here is a simple example: We let n = {ab ~ aa, ba ~ bb}. Then we 
have 
aba = abb = aab = aaa. 
Thus, 
aba ~ aaa, 
and the sequence of words aba, abb, aab, aaa is a derivation of aaa from 
aba. 
Exercises 
1. Let n be the semi-Thue process with the production ba ~ ab. 
(a) Give two different derivations of aaabbb from abbaba. 
(b) Give the set of all words in {a, b}* from which aabb can be 
derived. 
(c) 
Give the set of all words which can be derived from bbaa. 
2. Let n be the semi-Thue process with productions ba ~ ab and 
ab ~ ba. Show that for all words u, v E {a, b}*, u 'fr v if and only if 
v * 
u. 

2. Simulation of Nondeterministic Turing Machines 
171 
3. Give a semi-Thue process n such that 1[xJ =if 1[Yl if and only if lx- yl 
is even. 
4. Let A= {l,2,bLbi,bf.bLc1,c2,d1,d2}. Give a semi-Thue process 
ll SUCh that bi1 â¢ â¢ â¢ bin ~ W E {1 2}* for all words bi 1 â¢ â¢â¢ bin where 
JJ 
Jn n 
' 
' 
lJ 
ln ' 
i 1 â¢â¢â¢ in, j 1 â¢â¢â¢ jn are binary representations of numbers and i 1 â¢â¢â¢ in + 
j 1 â¢â¢â¢ jn = w. [Hint: The symbols c1 , c2 are used to remember the need 
to carry 1, and d1 , d2 are used to remember the need to carry 2.] 
2. 
Simulation of Nondeterministic Turing Machines by 
Semi-Thue Processes 
Let us begin with a nondeterministic Turing machine L with alphabet 
{s1, â¢â¢â¢ , sK}, and states q 1 , q2, ... , qn. We shall show how to simulate L 
by a semi-Thue process !.(L) on the alphabet 
Each stage in a computation by L is specified completely by the current 
configuration. We shall code each such stage by a word on the alphabet of 
!.{.L). For example, the configuration 
i 
q4 
will be represented by the single word 
(2.1) 
Note that h is used as a beginning and end marker, and the symbol q4 
indicates the state of L and is placed immediately to the left of the 
scanned square. A word like (2.1) will be called a Post word. Of course, the 
same configuration can be represented by infinitely many Post words 
because any number of additional blanks may be shown on the left or 
right. For example, 
is a Post word representing the same configuration that (2.1) does. 
In general, a word huqivh, where 0 ~ i ~ n + 1, is called a Post word if 
u and v are words on the subalphabet {s0 , s1 , â¢â¢â¢ , sK}. We shall show how 
to associate suitable semi-Thue productions with each quadruple of L; 
the productions simulate the effect of that quadruple on Post words. 

172 
Chapter 7 Processes and Grammars 
1. For each quadruple of L of the form q; sj sk q1 we place in I(L) 
the production 
2. For each quadruple of L of the form q; sj R q1 we place in I(L) the 
productions 
q;sjsk ~ sjqlsk' 
q;sjh ~ sjq1s0h. 
k=O,l, ... ,K, 
3. For each quadruple of L of the form q; sj L q1 we place in I(L) the 
productions 
skqisj ~ qlsksj' 
hq;sj ~ hq1s0sj. 
k=O,l, ... ,K, 
To see how these productions simulate the behavior of L, suppose L is 
in configuration 
This configuration is represented by the Post word 
hs2q4 s1s0 s3h. 
Now suppose L contains the quadruple 
q4 si s3 qs. 
Then I(L) contains the production 
so that 
The Post word on the right then corresponds to the configuration immedi-
ately following application of the above quadruple. Now suppose that L 
contains the quadruple 

2. Simulation of Nondeterministic Turing Machines 
173 
(Of course, if L is a deterministic Turing machine, it cannot contain both 
of these quadruples.) Then I(L) contains the production 
q4s1so ~ slq3so' 
so that 
Finally if L contains the quadruple 
q4 sl Lqz, 
then I(L) contains the production 
Szq4sl ~ qzSzSJ' 
so that 
The productions involving h are to take care of cases where motion to 
the right or left would go past the part of the tape included in the Post 
word, so that an additional blank must be added. For example, if the 
configuration is 
and L contains the quadruple 
q4 sJ R q3' 
then I(L) contains the production 
q4slh ~ slq3soh 
and we have 
so that the needed blank on the right has been inserted. The reader will 
readily verify that blanks on the left are similarly supplied when needed. 
We now complete the specification of I(L): 
4. Whenever q;sp = 1, ... , n; j = 0, 1, ... , K) are not the first two 
symbols of a quadruple of L, we place in I(L) the production 
Thus, qn + 1 serves as a "halt" state. 

174 
Chapter 7 Processes and Grammars 
5. Finally, we place in !.(L) the productions 
We have 
qn+lsi~qn+l' 
qn+lh ~ qoh 
s;qo ~ qo' 
i=0,1, ... ,K, 
i=0,1, ... ,K. 
Theorem 2.1. Let L be a deterministic Turing machine, and let w be a 
Post word on the alphabet of !.(L). Then 
1. there is at most one word z such that w = z, and 
I.<L> 
2. if there is a word z satisfying (1), then z is a Post word. 
Proof. 
We have w = huq;vh. 
If 1 5. i 5. n, then 
a. if v = 0 no production of !.(L) applies to w; 
b. if v begins with the symbol si and there is a (necessarily unique) 
quadruple of L which begins q;si, then there is a uniquely applica-
ble production of !.(L) and the result of applying it will be a Post 
word; 
c. if v begins with the symbol si and there is no quadruple of L which 
begins q; si, then the one applicable production of !.(L) is 
qisj ~ qn+lsj, 
which yields another Post word when applied to w. 
If i = n + 1, then 
a. if v = 0, the only applicable production of !.(L) is 
qn+lh ~ qoh, 
which yields a Post word; 
b. if v begins with the symbol si, the only applicable production of 
!.(L) is 
which again yields a Post word. 
Finally, if i = 0, then 
a. if u = 0, no production of !.(L) can be applied; 
b. if u ends with si, the only applicable production of !.(L) is 
sjqo ~ qo' 
which yields a Post word. 
â¢ 

2. Simulation of Nondeterministic Turing Machines 
175 
Our next result makes precise the sense in which I(L) simulates L. 
Theorem 2.2. Let L 
be a nondeterministic Turing machine. Then, for 
each string u on the alphabet of L, L accepts u if and only if 
Proof. Let the alphabet of L be s 1 , â¢â¢â¢ , s K. First let us suppose that L 
accepts u. Then, if L begins in the configuration 
s0 
u 
i 
ql 
it will eventually reach a state qi scanning a symbol sk where no quadruple 
of L begins q; sk. Then we will have (for appropriate words v, w on the 
alphabet of L) 
Next suppose that L does not accept u. Then, beginning with configu-
ration 
L will never halt. Let 
and suppose that 
w =w =w =Â·Â·Â·=w 
I I.(L) 
2 I.( A) 
3 I.(L) 
!.(L) m â¢ 
Then each wj, 1 .:5; j .:5; m, must contain a symbol qi with 1 .:5; i .:5; n. Hence 
there can be no derivation of a Post word containing q0 from w1 , and so, 
in particular, there is no derivation of hq0h from w1 â¢ 
â¢ 
Definition. The inverse of the production g -+ g is the production g -+ g. 
For example, the inverse of the production ab -+ aa is the production 
aa -+ ab. 

176 
Chapter 7 Processes and Grammars 
Let us write O(L) for the semi-Thue process which consists of the 
inverses of all the productions of !.(L). Then an immediate consequence 
of Theorem 2.2 is 
Theorem 2.3. Let L 
be a nondeterministic Turing machine. Then for 
each string u in the alphabet of L, L accepts u if and only if 
Exercises 
1. 
(a) Give !.(L), where L 
is the Turing machine in Table 1.1 of 
Chapter 6. 
(b) Give a derivation that shows that hq1s0lllh = hq0h. 
'i.(.#f) 
(c) 
Describe {u I hq0h ~ hq1s0 uh}. 
n<L> 
2. Give a semi-Thue process n such that, for all words u, v E {1, 2}*, 
hq1s0us0vh 'if w E {1, 2}*, where u + v = w in binary notation. 
3. Show that for any partially computable function f(x), there is a 
semi-Thue process n such that for all x EN, l[xJ 'if l[Yl if and only if 
Y = f(x). 
3. 
Unsolvable Word Problems 
Definition. The word problem for a semi-Thue process n is the problem 
of determining for any given pair u, v of words on the alphabet of n 
whether u 'if v. 
We shall prove 
Theorem 3.1. There is a Turing machine L such that the word problem 
is unsolvable for both the semi-Thue processes !.(L) and O(L). 
Proof. 
By Theorem 3.1 in Chapter 6, there is a Turing machine L (in 
fact, deterministic) that accepts a nonrecursive language. Suppose first that 
the word problem for !.(L) were solvable. Then there would be an 
algorithm for testing given words v, w to determine whether v ~ w. By 
'i.(.#f) 
Theorem 2.2, we could use this algorithm to determine whether L will 
accept a given word u by testing whether 

3. Unsolvable Word Problems 
177 
We would thus have an algorithm for testing a given word u to see 
whether L will accept it. But such an algorithm cannot exist since the 
language accepted by L is not a recursive set. 
Finally, an algorithm that solved the word problem for O(L) would also 
solve the word problem for !.(L), since 
v ~ w 
I.<L> 
if and only if 
w ~ v. 
n<L> 
â¢ 
Definition. A semi-Thue process is called a Thue process if the inverse of 
each production in the process is also in the process. 
The fact that Thue processes are in fact "two-way" processes is a 
curious coincidence. 
We write g- g to combine the production g ~ g and its inverse 
g~g. 
For each Turing machine L, we write 
0(L) = !.(L) U O(L), 
so that 0(L) is a Thue process. We have 
Theorem 3.2 (Post's Lemma). Let L be a deterministic Turing machine. 
Let u be a word on the alphabet of L such that 
Then 
Proof. Let the sequence 
be a derivation in 0(L). Since w1 is a Post word, and each production of 
0(L) transforms Post words into Post words, we can conclude that the 
entire derivation consists of Post words. We need to show how to eliminate 
use of productions belonging to O(L) from this derivation. So let us 
assume that the last time in the derivation that a production of O(L) was 
used was in getting from W; to W; + 1 â¢ That is, we assume 
w.~w.lÂ· 
1lHL> t+ 
' 
W. 
I = WÂ· 2 
~ WI= hqoh. 
t+ 
I.<L> 
t+ 
I.<L> 

178 
Chapter 7 Processes and Grammars 
Now, O(L) consists of inverses of productions of !.(L); hence we must 
have 
Moreover, we must have i + 1 < I because no production of !.(L) can be 
applied to w1 = hq0h. Now, w;+ 1 is a Post word and 
By Theorem 2.1, we conclude that w;+ 2 = w;. Thus the transition from W; 
to W;+ 1 and then back to W;+ 2 = W; is clearly an unnecessary detour. That 
is, the sequence 
W1 , W2 , â¢â¢â¢ , W;, W;+ 3 , â¢â¢â¢ , W1 
from which W;+ 1, W;+2 have been omitted is a derivation in E>(L). 
We have shown that any derivation that uses a production belonging to 
O(L) can be shortened. Continuing this procedure, we eventually obtain a 
derivation using only productions of !.(L). 
â¢ 
Theorem 3.3 (Post-Markov). If the deterministic Turing machine L 
accepts a nonrecursive set, then the word problem for the Thue process 
E>(L) is unsolvable. 
Proof. Let u be a word on the alphabet of L. Then we have, using 
Theorems 2.2 and 3.2, 
L accepts u 
if and only if 
if and only if 
Hence, an algorithm for solving the word problem for E>(L) could be used 
to determine whether or not L will accept u, which is impossible. 
â¢ 
Now we consider semi-Thue processes on an alphabet of two symbols. 
Theorem 3.4. There is a semi-Thue process on the alphabet {a, b} whose 
word problem is unsolvable. Moreover, for each production g ~ h of this 
semi-Thue process, g, h =/= 0. 

3. Unsolvable Word Problems 
179 
Proof. Let us begin with a semi-Thue process n on the alphabet A = 
{a1 , â¢â¢â¢ , an} and with productions 
i = 1,2, ... ,m, 
whose word problem is unsolvable. We also assume that for each i = 
1, 2, ... , m, g; =/= 0 and g; =/= 0. This is legitimate because this condition is 
satisfied by the productions of I(L). 
We write 
a'. = balilb 
J 
' 
j=1,2, ... ,n, 
where there is a string of a's of length j between the two b's. Finally, for 
any word w =/= 0 in A*, 
w =a. a. Â·Â·Â·a. 
lt 12 
lk ' 
we write 
w' =a'. a'. Â·Â·Â· a' . . 
lt h 
lk 
In addition we let 0' = 0. Then, we consider the semi-Thue process ll' on 
the alphabet {a, b} whose productions are 
i = 1,2, ... ,m. 
We have 
Lemma 1. If u rt v, then u' ~ v'. 
Proof. We have u = rg;s, v = rg;s. Hence u' = r'gis', v' = r'gis', so 
that u' ~ v'. 
â¢ 
Lemma 2. If u' ~ w, then for some v E A* we have w = v' and u Ji v. 
Proof. We have u' = pg/q, w = pg/q. Now, since g; =/= 0, g/ begins and 
ends with the letter b. Hence each of p and q either begins and ends with 
b or is 0, so that p = r', q = s'. Then, u = rg;s. Let v = rg;s. Then 
w = v' and u rt v. 
â¢ 
Lemma 3. u 'fr v if and only if u' ~ v'. 
Proof. If U = Ul rf Uz rf Â·Â·Â· rf Un = V, then by Lemma 1 
U'- u' =u' = 
-
lw zw 
=u'- v' 
n' 
n -
â¢ 

180 
Chapter 7 Processes and Grammars 
Conversely, if 
U , = w = 
= ... = w = v' 
I n' Wz n' 
II' 
n 
' 
then by Lemma 2, for each W; there is a string u; E A* such that W; = u~. 
Thus, 
u' 
u' 
u' = 
=Iff zw 
By Lemma 2 once again, 
=u'- v' 
[]' n-
. 
so that u 'if v. 
â¢ 
Proof of Theorem 3.4 Concluded. By Lemma 3, if the word problem for TI' 
were solvable, the word problem for TI would also be solvable. Hence, the 
word problem for n, is unsolvable. 
â¢ 
In the preceding proof it is clear that if the semi-Thue process TI with 
which we begin is actually a Thue process, then TI' will be a Thue process 
on {a, b}. We conclude 
Theorem 3.5. There is a Thue process on the alphabet {a, b} whose word 
problem is unsolvable. Moreover, for each production g ----) h of this Thue 
process, g, h =I= 0. 
Exercises 
1. Let TI be the semi-Thue process with productions cde ----) ce, d ----) cde. 
Use the construction in the proof of Theorem 3.4 to get a semi-Thue 
process TI' with productions on {a, b} such that u =if v if and only if 
u' ~ v' for all words u, v E {c, d, e}*. 
2. A semi-Thue system is defined to be a pair (u 0 , TI), where TI is a 
semi-Thue process and u0 is a given word on the alphabet of TI. A 
word w is called a theorem of (u 0 , TI) if u0 'if w. Show that there is a 
semi-Thue system for which no algorithm exists to determine whether 
a given string is a theorem of the system. 
3. 
Let TI be a semi-Thue process containing only one production. Show 
that n has a solvable word problem. 
4.* Give an upper bound on the size of the smallest semi-Thue process 
with an undecidable word problem. [See Exercise 2.2 in Chapter 6.] 

4. Post's Correspondence Problem 
181 
4. 
Post's Correspondence Problem 
The Post correspondence problem first appeared in a paper by Emil Post 
in 1946. It was only much later that this problem was seen to have 
important applications in the theory of formal languages. 
Our treatment of the Post correspondence problem is a simplification of 
a proof due to Floyd, itself much simpler than Post's original work. 
The correspondence problem may conveniently be thought of as a 
solitaire game played with special sets of dominoes. Each domino has a 
word (on some given alphabet) appearing on each half. A typical domino is 
shown in Fig. 4.1. A Post correspondence system is simply a finite set of 
dominoes of this kind. Figure 4.2 gives a simple example of a Post 
correspondence system using three dominoes and the alphabet {a, b}. Each 
move in the solitaire game defined by a particular Post correspondence 
system consists of simply placing one of the dominoes of the system to the 
right of the dominoes laid down on previous moves. The key fact is that 
the dominoes are not used up by being played, so that each one can be used 
any number of times. The way to "win" the game is to reach a situation 
where the very same word appears on the top halves as on the bottom 
halves of the dominoes when we read across from left to right. Figure 4.3 
shows how to win the game defined by the dominoes of Fig. 4.2. (Note that 
one of the dominoes is used twice.) The word aabbbb which appears across 
both the top halves and bottom halves is called a solution of the given Post 
correspondence system. Thus a Post correspondence system possesses a 
solution if and only if it is possible to win the game defined by that system. 
huuh 
huhuu 
Figure 4.1 
DGD 
G~G 
Figure4.2 
DDGG 
G G ~ ~ 
Figure 4.3 

182 
Chapter 7 Processes and Grammars 
We shall prove 
Theorem 4.1. There is no algorithm that can test a given Post correspon-
dence system to determine whether it has a solution. 
Proof. Using Theorem 3.4, we begin with a semi-Thue process n on the 
alphabet {a, b} whose word problem is unsolvable. We modify n in the 
following trivial way: we add to the productions of n the two productions 
a ---+a, 
b---+ b. 
Naturally this addition has no effect on whether 
for given words u, v. However, it does guarantee that whenever u =if v, 
there is a derivation 
where m is an odd number. This is because with the added productions we 
have 
U; fiU; 
for each i, so that any step in a derivation (e.g., the first) can be repeated if 
necessary to change the length of the derivation from an even to an odd 
number. 
Let u and v be any given words on the alphabet {a, b}. We shall 
construct a Post correspondence system Pu,v (which depends on n as well 
as on the words u and v) such that Pu,v has a solution if and only if u =if v. 
Once we have obtained this Pu v we are through. For, if there were an 
algorithm for testing given Post' correspondence systems for possessing a 
solution, this algorithm could be applied in particular to Pu v and therefore 
to determine whether u =if v; since n has an unsolvable' word problem, 
this is impossible. 
We proceed to show how to construct Pu,v. Let the productions of ll 
(including the two we have just added) be g; ---+ h;, i = 1, 2, ... , n. The 
alphabet of Pu, v consists of the eight symbols 
abtib[] * *Â· 
For any word w on {a, b}, we write w for the word on {a, b} obtained by 
placing "- " on top of each symbol of w. Pu v is then to consist of the 
2n + 4 dominoes shown in Fig. 4.4. Note th~t because n contains the 
productions a ---+ a and b ---+ b, Pu, v contains the four dominoes 

4. Post's Correspondence Problem 
Bc;JDQJ~CIJ 
liJDDGQJG 
i =I, 2, ... ,n 
Figure 4.4 
183 
Therefore, it is clear that in our play it is legitimate to use dominoes of the 
form 
where p is any word on {a, b}, since any such dominoes can be assembled 
by lining up single dominoes selected appropriately from the previous four. 
We proceed to show that Pu,v has a solution if and only if u 'if v. 
First suppose that u 'if v. Let 
where m is an odd number. Thus, for each i, 1 :::;; i < m, we can write 
where the transition from U; to U;+ 1 is via the j;th production of II. Then 
we claim that the word 
(4.1) 
is a solution of Pu v. To see this, let us begin to play by laying down the 
dominoes: 
' 
At this stage, the word on top is 
while the word on the bottom is 

184 
Chapter 7 Processes and Grammars 
We can continue to play as follows: 
Now the word on top is 
and the word on the bottom is 
Recalling that m is an odd number we see that we can win by continuing 
as follows: 
BC!J ... []c;J~ 
00 [JBEJ 
lfm 
I 
tj,. 
I 
for, at this point the word both on top and on bottom is (4.1). 
Conversely suppose that Pu,u has a solution w. Examining Fig. 4.4, we 
see that the only possible way to win involves playing 
and 
first and last, respectively. This is because none of the other dominoes in 
Pu, u have tops and bottoms which begin (or end) with the same symbol. 
Thus, w must begin with [ and end with ]. Let us write w = [z ]y, where z 
contains no ]. (Of course it is quite possible that y = 0.) Since the only 
domino containing ] contains it on the far right on top and on bottom, we 
see that [z] itself is already a solution to Pu , . We work with this solution. 
So far we know that the game looks like this: 
B ... CD 
EJ 
B 
so that the solution [z] looks like this: 
[u*Â·Â·Â·*v]. 

4. Post's Correspondence Problem 
Continuing from the left we see that the play must go 
CJ 
0 
185 
where g; g; Â·Â·Â· g; = u. (This is necessary in order for the bottom to 
I 
2 
k 
"catch up" with the u * which is already on top.) Writing u = u 1 and 
u2 = h; 1h; 2 ... h;k we see that u 1 =fr u 2 and that the solution has the form 
[ u1 * u2 * .. Â· * v]. 
Now we see how the play must continue: 
where of cou~se u2 = gj 1gh ... gh. Again, writing u3 = hj1hh ... hh we 
have that u 2 Ii u 3 and that the solution has the form 
[ u 1 * u2 * u3 * .. Â· * v]. 
Continuing, it is clear that the solution can be written 
where 
so that u =fr v. 
â¢ 
Exercises 
1. Let II be the semi-Thue process with productions aba ----) a, b ----) aba, 
and let u = bb, v = aaaaaa. Describe the Post correspondence system 
Pu,tÂ· and give a solution to Pu,c. 
2. 
Find a solution to the Post correspondence problem defined by the 
dominoes 
6 
G 

186 
Chapter 7 Processes and Grammars 
3. Find an algorithm for Post correspondence problems whose alphabet 
consists of just one symbol. 
5. Grammars 
A phrase-structure grammar or simply a grammar is just a semi-Thue 
process in which the letters of the alphabet are separated into two disjoint 
sets called the variables and the terminals, with one of the variables singled 
out as the start symbol. It is customary (but, of course, not necessary) to 
use lower case letters for terminals, capital letters for variables, and in 
particular the letter S for the start symbol. 
Let r be a grammar with start symbol S and let r, T be the sets of 
variables and terminals of f, respectively. Then we define 
L(f) = {u E T* I S ~ u}, 
and call L(f) the language generated by r. Our purpose in this section is to 
characterize languages which can be generated by grammars. 
We first prove 
Theorem 5.1. Let U be a language accepted by a nondeterministic Turing 
machine. Then there is a grammar r such that U = L(f). 
Proof. Let U ~ T* and let L be a nondeterministic Turing machine that 
accepts U. We will construct f by modifying the semi-Thue process .O(L) 
from Section 2. Let L have the states q1 , â¢â¢â¢ , qn. Then we recall that the 
alphabet of .O(L) [which is the same as that of l(L)] consists of 
s0 , q0 , q1 , q2 , â¢â¢â¢ , qn, qn + 1 , h in addition to the letters of the alphabet of 
L. We let the terminals of f be just the letters of T, and the variables of f 
be the symbols from the alphabet of .O(L) not in T, together with the two 
additional symbols S and q. S is to be the start symbol of f. The 
productions of f are then the productions of .O(L) together with the 
productions 
qs ---+ sq 
for each sET 
qh ---+ 0. 

5. Grammars 
Now, let L accept u E T*. Then, using Theorem 2.3, we have 
S t hq0h 7' hq1s0uh t quh 7' uqh t u, 
so that u E L(f). 
187 
Conversely, let u E L(f). Then u E T* and S f u. Examining the list 
of productions of r, we see that we must in fact have 
S t hq0h 7' vqhz 1' vz = u. 
Proceeding further, we see that the symbol q could only be introduced 
using the production 
Hence, our derivation must have the form 
S 1' hq0h 7 xhq1s0 yhz 1' xqyhz 7 xyqhz 1' xyz = u, 
where of course xy = v. Thus, there is a derivation of xhq1 s0yhz from 
hq0h in f. Moreover, this must actually be a derivation in O(L) since the 
added productions are clearly inapplicable. Moreover, the productions of 
O(L) always lead from Post words to Post words. Hence, xhq1s0yhz must 
be a Post word. That is, x = z = 0 and u = xyz = y. We conclude that 
Thus by Theorem 2.3, L accepts u. 
â¢ 
Now, let us begin with a grammar r and see what we can say about 
L(f). Thus, let the alphabet of f be 
where T = {sl '0 0 0' sn} is the set of terminals, VI' 0 0 0' vk are the variables, 
and S = V1 is the start symbol. Let us order the alphabet of r as shown. 
Thus strings on this alphabet are notations for integers in the base n + k. 
We have 
Lemma 1. The predicate u ==> v is primitive recursive. 
r 
Proof. Let the productions f be g; ~ h;, i = 1, 2, ... , I. We write, for 
i = 1, 2, ... , I, 
PROD;(u, v) = (3r, s),Ju = CONCAT(r,g;, s) & v = CONCAT(r, h;, s)]. 

188 
Chapter 7 Processes and Grammars 
Since, by Chapter 5, Section 1, CON CAT is primitive recursive, each of the 
predicates PROD; is primitive recursive. But 
u =f v = PROD 1(u,v) v PROD2(u,v) v Â·Â·Â· v PROD1(u,v), 
and the result follows. 
â¢ 
We write DERIV(u, y) to mean that for some m, y = [u 1 , â¢â¢â¢ , um, 1], 
where the sequence Ut' 0 
0 0' um is a derivation of u from s in r. (The "1" 
has been added to avoid complications in case um = u = 0.) Then, since 
the value of S in base n + k is n + 1 [because S = V1 is the (n + l)th 
symbol in our alphabet], we have 
DERIV(u,y) = (3m),r(m + 1 = Lt(y) & (y) 1 = n + 1 
&(y)m =u &(y)m+l = 1 
&('t/j)<m{j = 0 V [(y)j (:> (y)j+t]}). 
Using Lemma 1, we have proved 
Lemma 2. DERIV(u, y) is primitive recursive. 
Also, by definition of DERIV(u, y ), we have for every word u on the 
alphabet of r 
S 'f u = (3y)DERIV(u, y). 
(5.1) 
Finally, (5.1) shows that 
S ~ u = minDERIV(u,y)!. 
r 
Y 
Hence, by Lemma 2 and Theorem 7.2 in Chapter 3, we see that {u IS ~ u} 
. 
B 
r 
ts r.e. 
ut 
L(f) = T* n {u IS =f u} 
(5.2) 
(where T is the alphabet of terminals of f), so that L(f) is the intersec-
tion of two r.e. sets and hence is r.e. Combining this result with Theorem 
5.1 in Chapter 6 and Theorem 5.1 in this chapter, we have 
Theorem 5.2. A language U is r.e. if and only if there is a grammar r 
such that U = L(f). 

5. Grammars 
189 
We now are able to obtain easily the promised converse to Theorem 5.1 
in Chapter 6. In fact putting Theorem 3.1 in Chapter 6 and Theorems 5.1 
and 5.2 in this chapter all together, we have 
Theorem 5.3. Let L be a given language. Then the following conditions 
are all equivalent: 
1. L is r.e.; 
2. L is accepted by a deterministic Turing machine; 
3. L is accepted by a nondeterministic Turing machine; 
4. there is a grammar f such that L = L(f). 
Theorem 5.3 involves some of the main concerns of theoretical com-
puter science: on the one hand, the relation between grammars, the 
languages they generate, and the devices that accept them; on the other 
hand, the relation, for various devices, between determinism and nondeter-
minism. 
We will conclude this section by obtaining a result that will be needed in 
Chapter 11, but can easily be proved at this point. 
Definition. A grammar f is called context-sensitive if for each production 
g---+ h of r we have lgl ~ lhl. 
Lemma 3. If r is context-sensitive, then 
is recursive. 
Proof. It will suffice to obtain a recursive bound for y in formula (5.1). 
Since 
for any derivation u 1 , â¢â¢â¢ , urn of u from S in the context-sensitive gram-
mar r, we must have 
u 1 ,u2 , â¢â¢â¢ ,um ~g(u), 
where g(u) is the smallest number which represents a string of length 
lui + 1 in base n + k. Now, since g(u) is simply the value in base n + k of 
a string consisting of lui + 1 repetitions of 1, we have 
lui 
g(u) = L (n + k)i, 
i=O 

190 
Chapter 7 Processes and Grammars 
which is primitive recursive because lui is primitive recursive. Next, note 
that we may assume that the derivation 
S = U1 ==> u2 ==> â¢ â¢ â¢ ==> Um = U 
contains no repetitions. This is because given a sequence of steps 
z = U; ==> ui+l ==> â¢â¢â¢ ==> ui+t = z, 
we could simply eliminate the steps u; + 1 , ... , u; + 1â¢ Hence the length m of 
the derivation is bounded by the total number of distinct strings of length 
::5; lui on our alphabet of n + k symbols. But this number is just g{u). 
Hence, 
m 
[u1, ... ,um,l] = 0Pt;Â·Pm+l :5;h(u), 
i=l 
where we have written h(u) for the primitive recursive function defined by 
Finally, we have 
g(u) 
h(u) = n pf<u>. Pg(u)+ I . 
i=l 
S 'if u <=> (3y)sh<u>DERIV(u,y), 
which gives the result. 
â¢ 
Theorem 5.4. If f is a context-sensitive grammar, then L{f) is recursive. 
Proof. 
We will use Lemma 3 and Eq. (5.2). Since T* is a recursive set, the 
result follows at once. 
â¢ 
Exercises 
1. 
For each of the following languages L, give a grammar f such that 
L = L(f). 
(a) 
L = {alnlblnJ In E N}. 
(b) L = {alnlblmll n ::5; m}. 
(c) 
L = {wwR I w E {a, b}*}. 
2. Use the construction in the proof of Theorem 5.1 to give a grammar f 
such that L(f) = {llmlBllnJBllm +nJI m, n ~ 0}. 
3. Write down the proof of Theorem 5.2. 

6. Some Unsolvable Problems Concerning Grammars 
191 
4. (a) Let f have the variables S, B, C, the terminals a, b, c and the 
productions 
S ~ aSBC, 
CB ~ BC, 
S ~aBC, 
bB ~ bb, 
aB ~ ab, 
bC ~ be, 
cC ~ cc. 
Prove that for each n -=/= 0, alnlblnlclnJ E L(f). 
(b)* Prove that L(f) = {alnlblnlclnJ In -=/= 0}. 
6. 
Some Unsolvable Problems Concerning Grammars 
How much information can we hope to obtain about L(f) by a computa-
tion that uses the grammar f as input? Not much at all, as we shall see. 
Let L 
be a Turing machine and let u be some given word on the 
alphabet of L. We shall construct a grammar fu as follows: 
The variables of ru are the entire alphabet of !.(L) together with S 
(the start symbol) and V. There is just one terminal, namely, a. The 
productions of ru are all of the productions of !.(L) together with 
S ~ hq1s0uh 
hq0h ~ V 
v~av 
v~a. 
Then it follows at once from Theorems 2.1 and 2.2 that S 'f:> V if and only 
if L accepts u. Thus we have 
" 
Lemma. If L accepts u, then L(f) = {alii I i -=1= 0}. If L does not accept 
u, then L(fu) = 0. 
Now we can select L so that the language it accepts is not recursive. 
Then there is no algorithm for determining for given u whether L accepts 
u. But the lemma obviously implies the equivalences 
L accepts u = L(f)-=!= 0 
= L(f) is infinite 
<=>aEL(f). 

192 
Chapter 7 Processes and Grammars 
We have obtained 
Theorem 6.1. There is no algorithm to determine of a given grammar f 
whether 
1. L(f) = 0, 
2. L(f) is infinite, or 
3. v0 E L(f) for a fixed word v0 â¢ 
We can also prove 
Theorem 6.2. There is no algorithm for determining of a given pair f, ~ 
of grammars whether 
1. L(~) ~ L(f), 
2. L(~) = L(f). 
Proof. 
Let ~ be the grammar with the single variable S, the single 
terminal a, and the productions 
S ---+ aS 
S---+ a. 
Then L(~) = {a[ill i -=/= 0}. Thus we have by the previous lemma 
L accepts u -
L(~) = L(fu) -
L(~) ~ L(fu). 
The result follows at once. 
Exercise 
â¢ 
1. 
Show that there is no algorithm to determine of a given grammar f 
whether 
(a) L(f) contains at least one word with exactly three symbols; 
(b) v0 is the shortest word in L(f) for some given word v0 ; 
(c) 
L(f) =A* for some given alphabet A. 
*7. Normal Processes 
Given a pair of words g and g we write 
gz ---+ zg 

7. Normal Processes 
193 
to indicate a kind of transformation on strings called a normal production. 
If P is the normal production gz ~ zg we write 
if for some string z we have 
u =gz, 
v =zg. 
That is, v can be obtained from u by crossing off g from the left of u and 
adjoining g to the right. A normal process is simply a finite set of normal 
productions. If 11 is a normal process, we write 
U'7V 
to mean that 
U=;1V 
for some production P in 11. Finally, we write 
u~v 
to mean that there is a sequence (called a derivation) 
The word problem for 11 is the problem of determining of two given words 
u, v whether u ~ v. 
Let TI be a semi-Thue process on the alphabet {a, b} with an unsolvable 
word problem. We shall show how to simulate TI by a normal process 11 on 
the alphabet {a, b, ii, b}. As earlier, if u E {a, b}*, we write u for the word 
on {ii, b} obtained by placing -
above each letter in u. Let the produc-
tions of TI be 
Then the productions of 11 will be 
g;z ~ zh; 
az ~ zii 
bz ~zb 
iiz ~ za 
bz ~ zb. 
i = l,2, ... ,n. 
i = 1,2, ... , n 
A word on {a, b, ii, b} is called proper if it can be written in one of the 
forms uv or uv, where u, v are words on {a, b}. We say that two words are 

194 
Chapter 7 Processes and Grammars 
associates if there is a derivation of one from the other using only the last 
four productions of 11. A word on {a, b} of length n has 2n associates, all of 
which are proper. For example, the associates of baab are as follows: 
~=~=~=~=~=~=~=~=~. 
Generally for u, v E {a, b}*, the proper words uv and uv are associates of 
each other and also of the word vu. In fact, vu is the unique word on {a, b} 
which is an associate of uv. Thus, a word is proper just in case it is an 
associate of a word on {a, b}. 
Lemma 1. If u If v, then u ~ v. 
Proof. We have u = pg;q, v = ph;q for some i. Then 
Lemma 2. If u if v, then u ~ v. 
Proof. 
Immediate from Lemma 1. 
â¢ 
â¢ 
Lemma 3. Let u be proper and let u 7 v. Then there are words r, s on 
{a, b} that are associates of u, v, respectively, such that r if s. 
Proof. If v is an associate of u, then u and v are both associates of some 
word r on {a, b}, and the result follows because r 'if r. 
If v is not an associate of u, the production used to obtain v from u 
must be one of the g;z ~ zh;. Since u is proper, we have u = g;elfi, where 
p, q are words on {a, b}. Then v = qph;. Thus, setting 
r = pg;q, 
the result follows because r If s. 
â¢ 
Lemma 4. Let u be proper and let u ~ v. Then there are words r, s on 
{a, b} that are associates of u, v, respectively, such that r =if s. 
Proof. 
By induction on the length of the derivation in 11 of v from u. The 
result is obvious if the derivation has length 1. Suppose the result is known 
for derivations of length m, and let 
By the induction hypothesis, there are words r, z on {a, b} that are 
associates of u, um, respectively, such that r 'if z. By Lemma 3, um + 1 is an 
associate of a words on {a, b} such that z =if s. Thus, r =if s. 
â¢ 

7. Normal Processes 
195 
Lemma 5. Let u, v be words on {a, b}. Then u ~ v if and only if u fr v. 
Proof. 
By Lemma 2 we know that u fr v implies u ~ v. Conversely, if 
u ~ v, by Lemma 4, r fr s, where r, s are words on {a, b} that are 
associates of u, v, respectively. But since u, v are already words on {a, b}, 
we have r = u, s = v, so that u fr v. 
â¢ 
Since n was chosen to have an unsolvable word problem, it is now clear 
that v has an unsolvable word problem. For, by Lemma 5, if we had an 
algorithm for deciding whether u ~ v, we could use it to decide whether 
u fr v. 
We have proved 
Theorem 7.1. There is a normal process on a four-letter alphabet with an 
unsolvable word problem. 
Exercise 
1. Show that there is a normal process with an unsolvable word problem 
whose alphabet contains only two letters. 


8 
Classifying Unsolvable Problems 
1. Using Oracles 
Once one gets used to the fact that there are explicit problems, such as the 
halting problem, that have no algorithmic solution, one is led to consider 
questions such as the following. 
Suppose we were given a "black box" or, as one says, an oracle, which 
somehow can tell us whether a given Turing machine with given input 
eventually halts. (Of course, by Church's thesis, the behavior of such an 
"oracle" cannot be characterized by an algorithm.) Then it is natural to 
consider a kind of program that is allowed to ask questions of our oracle 
and to use the answers in its further computation. Which noncomputable 
functions will now become computable? 
In this chapter we will see how to give a precise answer to such 
questions. To begin with, we shall have to modify the programming 
language Y introduced in Chapter 2, to permit the use of "oracles." 
Specifically, we change the definition of "statement" (in Chapter 2, Section 
3) to allow statements of the form V +-- O(V) instead of V +-- V. The 
modified version of Y thus contains four kinds of statement: increment, 
decrement, conditional branch, and this new kind of statement which we 
call an oracle statement. The definitions of instruction, program, state, 
snapshot, and terminal snapshot remain exactly as in Chapter 2. 
197 

198 
Chapter 8 Classifying Unsolvable Problems 
We now let G be some partial function on N with values in N, and we 
shall think of G as an oracle. Let .9J be a program of length n and let 
(i, u) be a nonterminal snapshot of .9, i.e., i :::;; n. We define the snapshot 
(j, T) to be the G-successor of (i, u) exactly as in the definition of 
successor in Chapter 2, Section 3, except that Case 3 is now replaced by 
Case 3. The ith instruction of .9J is V +-- O(V) and u contains the equation 
V = m. If G(m)J,, then j = i + 1 and T is obtained from u by 
replacing the equation V = m by V = G(m). If G(m) i, then 
(i, u) has no successor. 
Thus, when G(m) J,, execution of this oracle statement has the intuitive 
effect of answering the computer's question "G(m) = ?". When G(m)j, 
an "out-of-bounds" condition is recognized, and the computer halts with-
out reaching a terminal snapshot. Of course, when G is total, every 
nonterminal snapshot has a successor. 
A G-computation is defined just like computation except that the word 
successor is replaced by G-successor. A number m that is replaced by 
G(m) in the course of a G-computation (under Case 3) is called an oracle 
query of the G-computation. We define 
1/J.J.~ {;(r 1 , r 2 , â¢â¢â¢ , r m) exactly as we 
defined I/J.J.m>(r1 , r2 , â¢â¢â¢ , r m) in Chapter 2, Section 4, except that the word 
computation is replaced by G-computation. 
Now, let G be a total function. Then, the partial function 
I/J.J.~{;(x 1 , â¢â¢â¢ , xm) is said to be G-computed by .9. A partial function f is 
said to be partially G-computable or G-partial recursive if there is a 
program that G-computes it. A partially G-computable function that is 
total is called G-computable or G-recursive. Note that we have not defined 
partially G-computable unless G is a total function. 
We have a few almost obvious theorems. 
Theorem 1.1. If f is partially computable, then f is partially G-computa-
ble for all total functions G. 
Proof. Clearly, we can assume that f is computed by a program .9J 
containing no statements of the form' V +-- V. Now this program .9J is also 
1 Unlabeled statements V +- V can just be deleted, and 
can be replaced by 
[L] 
V+-V 
[L] 
V+- V+ 1 
V+-V-1. 

1. Using Oracles 
199 
a program in the new revised sense; moreover, a computation of 9' is the 
same thing as a G-computation of 9' since 9' contains no oracle state-
ments. Hence 1/JJ-~6 = 1/JJ.m> for all G. 
â¢ 
We write I for the identity function /(x) = x. (Thus, I= ulJ 
Theorem 1.2. f is partially computable if and only if f is partially 
/-computable. 
Proof. If f is partially computable, then by Theorem 1.1 it is certainly 
partially /-computable. Conversely, let 9' /-compute f. Let 9'' be ob-
tained from 9' by replacing each oracle statement V +--- O(V) by V +--- V. 
Then, 9'' is a program in the original sense and 9'' computes f. 
â¢ 
Theorem 1.3. Let G be a total function. Then G is G-computable. 
Proof. The following program2 clearly G-computes G: 
X+--- O(X) 
Y+-X 
â¢ 
Theorem 1.4. The class of G-computable functions is a PRC class. 
Proof. Exactly like the proof of Theorem 3.1 in Chapter 3. 
â¢ 
This last proof illustrates a situation, which turns out to be quite typical, 
in which the proof of an earlier theorem can be used virtually intact to 
prove a theorem relative to an "oracle" G. One speaks of a relativized 
theorem and of relativizing a proof. It is a matter of taste how much detail 
to provide in such a case. 
Theorem 1.5. Let F be partially G-computable and let G be H-computa-
ble. Then F is partially H-computable. 
Proof. Let 9' be a program which G-computes F. Let 9'' be obtained 
from 9' by replacing each oracle statement V +--- O(V) by a macro 
expansion obtained from some program which H-computes G. Then 
clearly, 9'' H-computes F. 
â¢ 
Theorem 1.6. Let G be any computable function. Then a function F is 
partially computable if and only if it is partially G-computable. 
2 Of course, we can freely use macro expansions, as explained in Chapter 2. 

200 
Chapter 8 Classifying Unsolvable Problems 
Proof. Theorem 1.1 gives the result in one direction. For the converse, let 
F be partially G-computable. By Theorem 1.2, G is /-computable. Hence, 
by Theorem 1.5, F is partially /-computable and so, by Theorem 1.2 again, 
F is partially computable. 
â¢ 
It is useful to be able to work with "oracles" that are functions of more 
than one variable. We introduce this notion by using a familiar coding 
device from Chapter 3, Section 8. 
Definition. Let f be a total n-ary function on N, n > 1. Then we say 
that g is (partially) !-computable to mean that g is (partially) G-computa-
ble, where 
G(x) = f((x) 1 , â¢â¢â¢ , (x)n). 
(1.1) 
Theorem 1.7. Let f be a total n-ary function on N. Then f is /-computa-
ble. 
Proof. 
Let G be defined by (1.1). Then 
f(xl ' ... ' xn) = G([xl ' ... ' xn]). 
Hence the following program G-computes f: 
Z ~ O(Z) 
y~z 
â¢ 
Since predicates are also total functions we can speak of a function 
being (partially) P-computable, where P is a predicate. Also, we speak of a 
function being (partially) A-computable when A ~ N; as usual, we simply 
identify A with the predicate that is its characteristic function. 
Exercises 
1. Provide a suitable definition of computability by a Post-Turing pro-
gram relative to an oracle and prove an appropriate equivalence 
theorem. 
2. 
For a given total function G from N to N, define the class Rec(G) to 
be the class of functions obtained from G and the initial functions of 
Chapter 3 using composition, recursion, and minimalization. Prove 
that every function in Rec(G) is partially G-computable. 

2. Relativization of Universality 
201 
2. 
Relativization of Universality 
We now proceed to relativize the development in Chapter 4. As in Chapter 
4, Section 1, we define an instruction number #(I) = (a, ( b, c)) for all 
instructions /. The only difference is that b = 0 now indicates an oracle 
staLement instead of one of the form V ~ V. For 9' a program, we now 
define #(9') as before. As indicated in Chapter 4, in order to avoid 
ambiguity we must not permit a program ending in the instruction whose 
number is 0. This instruction is now the unlabeled statement Y ~ O(Y). 
Hence, for complete rigor, if we wish to end a program with Y ~ O(Y), we 
will have to provide the statement with a spurious label. 
We define <t>g'>(xp ... ,xn,y) to be t/J.J.~b(x 1
, ... ,xn) where 9' is the 
unique program such that #(9') = y. We also write <l>c(x, y) for <I>g>(x, y). 
We have 
Theorem 2.1 (Relativized Universality Theorem). Let G be total. Then 
the function <l>g'>(x 1 , â¢â¢â¢ , xn, y) is partially G-computable. 
Proof. The proof of this theorem is essentially contained in the program 
of Fig. 2.1. The daggers (*) indicate the changes from the unrelativized 
universal program in Fig. 3.1 in Chapter 4. As in that case, what we have is 
essentially an interpretative program. The new element is of course the 
interpretation of oracle statements. This occurs in the following program 
segment which, not surprisingly, itself contains an oracle statement: 
[0] 
W ~ (S)r(U)+ 1 
B~w 
B ~ O(B) 
S~lSjPwjÂ·P 8 
The program segment works as follows. First, W and B are both set to the 
current value of the variable in the oracle statement being interpreted. 
Then an oracle statement gives B a new value which is G of the old value. 
Finally, this new value is stored as an exponent on the appropriate prime 
in the number S. The remainder of the program works exactly as in the 
unrelativized case. 
â¢ 
Let G be any partial function on N with values in N. Then we define 
the relativized step-counter predicate by 
STP~n>(x 1
, â¢â¢â¢ , xn, y, t) = there is a G-computation of program number 
y of length ~ t + 1 beginning with inputs 
xl , . .. ,xn. 

202 
Chapter 8 Classifying Unsolvable Problems 
z +- x.+ 1 + 1 
n 
s +- n (p2;)x, 
i=l 
K+-1 
[C] 
IF K = Lt(Z) + 1 v K = 0 GOTO F 
U +- r((Z)K) 
p +- Pr(U)+ I 
IF l(U) = 0 GOTO 0 
(:j:) 
IF l(U) = 1 GOTO A 
IF -(PIS) GOTO N 
IF l(U) = 2 GOTO M 
K+-
min [l((Z);) + 2 = l(U)] 
i,;Lt(Z) 
GOTOC 
[0] 
W +- (S)r(U)+ 1 
(:j:) 
B +- W 
(:j:) 
B+-O(B) 
(:j:) 
S +-[S/Pwj Â·P 8 
(:j:) 
GOTON 
(:j:) 
[M] 
S +-[S/PJ 
GOTON 
[A] 
S+-SÂ·P 
[N] 
K+-K+ 1 
GOTOC 
[F] 
y +- (S)I 
Figure 2.1. Program that G-computes CI>h">(xl ' ... ' x.' x.+ I). 
As in the unrelativized case, we have 
Theorem 2.2 (Relativized Step-Counter Theorem). For any total function 
G, the predicates STP~">(xp ... , x., y, t) are G-computable. 
In Chapter 4 we proved that the unrelativized predicates STP<â¢> are 
primitive recursive, but we do not need such a sharp result here. Instead, 
we modify the program in Fig. 2.1 by adding a variable Q that functions as 
a step counter. Then each time through the main loop, Q is increased by 1, 
so that the program will "know" when a given number of steps has been 

2. Relativizatlon of Universality 
203 
Z <-- Xn+ 1 + 1 
n 
s --- . n (p2i)x; 
â¢=1 
K<--1 
[C] 
Q=Q+1 
(*) 
IF Q > Xn+Z + 1 GOTO E 
(*) 
IF K = Lt(Z) + 1 v K = 0 GOTO F 
U <-- r((Z)K) 
p <-- Pr(U)+ I 
IF l(U) = 0 GOTO 0 
(:j:) 
IF /(U) = 1 GOTO A 
IF -(PIS) GOTO N 
IF /(U) = 2 GOTO M 
K<-
min [l((Z);) + 2 = l(U)] 
islt(Z) 
GOTOC 
[O] 
W <-- (S)r(U)+ I 
(:j:) 
B <-- W 
(:j:) 
B <-- O(B) 
(:j:) 
S <-[SjPwjÂ·P8 
(:j:) 
GOTON 
(:j:) 
[M] 
S <-[S/PJ 
GOTON 
[A] 
S<-SÂ·P 
[N] 
K<-K+ 1 
GOTOC 
[F] 
Y<--1 
(*) 
Figure 2.2. Program that G-computes STPhnl(X1 , â¢â¢â¢ , Xn, Xn+ 1 , Xn+ 2 ). 
exceeded. The program is given in Fig. 2.2. The asterisks (*) indicate 
changes from the relativized universal program and the daggers (:j:), as 
before, indicate the changes made in relativizing. 
We shall now consider certain partial functions with finite domains, and 
use numbers as codes for them. For every u E N we define 
{u}(i) = { ~(u))i+ I 
fori < l(u) 
fori ~ l(u). 
(2.1) 

204 
Chapter 8 Classifying Unsolvable Problems 
Thus, if /(u) = 0, then {u} = 0, the nowhere defined function. Also, if 
u = (k,[a 0 ,a1 , â¢â¢â¢ ,ak_ 1]), 
then {u}(x) =ax for x = 0, 1, ... , k - 1 and {u}(x)j for x :2: k. 
Theorem 2.3. The predicate 
P (X 1 , â¢â¢â¢ , X n , Y, I, U) <=> STI{W( X 1 , â¢â¢â¢ , X n , Y, I) 
is computable. 
Proof. We will transform the program in Fig. 2.2 into one that computes 
P(x 1 , â¢â¢â¢ ,xn,xn+lâ¢xn+zâ¢Xn+ 3). We need only replace the single oracle 
statement B ~ 0( B) by instructions that operate on X n + 3 to obtain the 
required information about {xn+ 3}. This involves first testing for 
{x n + 3}(b),!., where b is the value of the variable B. If {x n + 3}(b) i, compu-
tation should halt with output 0, because there is no computation in this 
case. Otherwise B should be given the value {xn+ 3}(b). Thus, by (2.1), it 
suffices to replace the oracle statement B ~ O(B) in the program in Fig. 
2.2 by the following pair of instructions: 
IF l(Xn+3):::;; B GOTO E 
â¢ 
Now, let G be a total function. Then, we define 
u-<G 
to mean that {u}(i) = G(i) for 0 :::;; i < /(u). [Of course, by (2.1), {u}(i)j for 
i ;;::: /(u).] For a number u such that u -< G, values of G can be retrieved 
by using the equations 
G(i) = (r(u))i+ 1 , 
i = 0, 1, ... ,/(u)- 1. 
We can use the predicate STI{~j>(x 1 , â¢â¢â¢ , xn, y, t) to obtain an important 
result that isolates the noncomputability of the relativized step-counter 
predicate in a way that will prove helpful. The simple observation on which 
this result capitalizes is that any G-computation can contain only finitely 
many oracle queries. 
Theorem 2.4 (Finiteness Theorem). Let G be a total function. Then, we 
have 

2. Relatlvlzation of Universality 
205 
Proof. First suppose that STPbn>(x1 , â¢â¢â¢ , xn, y, t) is true for some given 
values of x 1 , â¢â¢â¢ , xn, y, t, and let go be the program with #(go)= y. Let 
s 1 , s 2 , â¢â¢â¢ , s k be a G-computation of go where s 1 is the initial snapshot 
corresponding to the input values x 1 , x2 , â¢â¢â¢ , xn and where k:::;; t + 1. Let 
M be the largest value of an oracle query of this G-computation, and let 
u = (M + 1,[G(O),G(l), ... ,G(M)]). Thus, u-< G and {u}(m) = G(m) 
for all m :::;; M. Hence, s1, s2 , â¢â¢â¢ , sk is likewise a {u}-computation of go_ 
Since k :::;; t + 1, STI(~j>(x 1
, â¢â¢â¢ , xn, y, t) is true. 
Conversely, let us be given u -< G such that STI(~j>(x 1
, â¢â¢â¢ , xn, y, t) is 
true, and let #(go)= y, Let s1, s2 , â¢â¢â¢ , sk be a {u}-computation of go 
where s1 is the initial snapshot corresponding to the input values 
x 1 , x2 , â¢â¢â¢ , xn and where k :::;; t + 1. For each m that is an oracle query of 
this {u}-computation, we must have {u}(m) J,, since otherwise one of the 
snapshots in this {u}-computation would be nonterminal and yet not have a 
successor. Since u -< G, we must have {u}(m) = G(m) for all such m. 
Hence s1, s2 , â¢â¢â¢ , sk is likewise a G-computation of go_ Since k :::;; t + 1, 
STPtn>(x1 , â¢â¢â¢ , xn, y, t) is true. 
â¢ 
To conclude this section we turn to the parameter theorem (Theorem 
5.1 in Chapter 4). 
Theorem 2.5 (Relativized and Strengthened Parameter Theorem). 
For 
each n, m > 0, there is a primitive recursive function s;:.(u 1 , â¢â¢â¢ , un, y) 
such that for every total function G: 
<t>&m+nl(x1 , â¢â¢â¢ , Xm, U1 , â¢â¢â¢ , Un, y) = <t>&m>(x1 , â¢â¢â¢ , Xm, S;:.(ul, ... , Un, y)). 
(2.2) 
Moreover, the functions s;:. have the property: 
Proof. The functions s;:. are defined exactly as in the proof of Theorem 
5.1 in Chapter 4. We briefly give the proof again in a slightly different way. 
Thus, let #(go) = y; then the function 
S~(u, y) is defined to be the 
number of the program .9 obtained from go by preceding it by the 
statement 
xm+l ~xm+l + 1 
repeated u times. Since .9 on inputs x1, ... , xm will do exactly what go 
would have done on inputs x 1 , â¢â¢â¢ , x m , u we have 

206 
Chapter 8 Classifying Unsolvable Problems 
the desired result for n = 1. To complete the proof, we define s::, for 
n > 1 by the recursion 
It is now easy to prove by induction on n that if #(9') = y, then 
s::,(ul' ... ' un 'y) = #(.9), where .9 is obtained from 9' by preceding it 
by the following program consisting of un + Â·Â·Â· +u1 statements. 
~m+n +-- xm+n + 1} 
. 
u 
â¢ 
n 
xm+n +-- xm+n + 1 
Hence, .9 on inputs x 1 , â¢â¢â¢ , x m will do exactly what 9' would have done 
on inputs x 1 , â¢â¢â¢ , xm, u1 , â¢â¢â¢ , un. Thus, we obtain (2.2). 
Finally, let 
s::,(ul ' ... ' un 'y) = s::,(ul ' ... ' un 'y) = #(.9), 
and let y = #(9'). Then, .9 consists of a list of increment statements 
followed by 9', and for 1 :::;; i :::;; n, U; and u; are both simply the number of 
times the statement 
xm+i +-- xm+i + 1 
occurs in .9 preceding 9'. Thus, U; = u;. 
Exercises 
1. (a) Show that the functions s::, do not have the property: 
â¢ 
(b) Can the definition of s::, be modified so the parameter theorem 
continues to hold, but so the condition of (a) holds as well? How? 
2. 
Prove the converse of Exercise 1.2. 

3. Reducibility 
207 
3. 
Reducibility 
If A and B are sets such that A is B-recursive, we also say that A is 
Turing-reducible to B and we write A ~, B. We have 
Theorem 3.1. 
A ~, A. If A ~, B and B ~, C, then A ~,C. 
Proof. The first statement follows at once from Theorem 1.3 and the 
second from Theorem 1.5. 
â¢ 
Any relation on the subsets of N for which Theorem 3.1 is true is called 
a reducibility. Many reducibilities have been studied. For example, we 
introduced many-one reducibility in Chapter 4. We can also define a 
restricted form of many-one reducibility. 
Definition. We write A ~ 1 Band say that A is one-one reducible to B if 
there is a one-one recursive function f (i.e., f(x) = f(y) implies x = y) 
such that 
A= {x EN I f(x) E B}. 
Theorem 3.2. 
A ~ 1 B implies A ~m B implies A ~, B. 
Proof. The first implication is immediate. For the second implication, let 
A = {x EN I f(x) E B}, where f is recursive. Then the following program 
B-computes A: 
X~ f(X) 
X~ O(X) 
v~x 
Theorem 3.3. 
~ 1 and ~m are both reducibilities. 
â¢ 
Proof. Clearly A = {x EN I /(x) E A}, where I is the identity function. 
Hence A ~ 1 A and therefore A ~m A. 
Let A ~m B and B ~m C, and let 
A = {x EN I f(x) E B}, 
where J, g are recursive. Then 
B = {x E N I g(x) E C}, 
A= {x ENig(f(x)) E C}, 
so that A ~me. If, moreover, f and g are one-one and h(x) = g(f(x)), 
then h is also one-one, because 
h(x) = h(y) implies g(f(x)) = g(f(y)) 
implies f(x) = f(y) 
implies 
x = y. 
â¢ 

208 
Chapter 8 Classifying Unsolvable Problems 
Thus, we have three examples, 
:::;;1, 
::;;m, and 
:::;;~' of reducibilities. 
Polynomial-time reducibility, ::;;P , which we will study in Chapter 15, is 
another example. (In fact, historically, polynomial-time reducibility was 
suggested by many-one reducibility.) There are a number of simple 
properties that all reducibilities share. To work some of these out, let us 
write ::;;Q to represent an arbitrary reducibility. By replacing Q by 1, m, t 
(or even p) we specialize to the particular reducibilities we have been 
studying. We write A $, QB to indicate that it is not the case that 
A ::;;Q B. 
Definition. 
A =Q B means that A ::;;Q Band B ::;;Q A. 
Theorem 3.4. For any reducibility ::;;Q: 
A =Q A, 
A =Q B implies 
B =Q A, 
A =Q B and 
B =Q C implies 
A =Q C. 
Proof. 
Immediate from the definition. 
â¢ 
Definition. Let W be a collection of subsets of N and let 
::;;Q be a 
reducibility. W is called Q-closed if it has the property 
A E Wand B ::;;Q A 
implies 
B E W. 
Also, a set A E W is called Q-complete for W if for every B E W we have 
B ::;;Q A. 
NP-completeness, which will be studied in Chapter 15, is, in the present 
terminology, polynomial-time completeness for NP. Completeness of a set 
A is often proved by showing that a set already known to be complete can 
be reduced to A. 
Theorem 3.5. Let A be Q-complete for W, let BE W, and let A ::;;Q B. 
Then B is Q-complete for W. 
Proof. 
Let C E W. Then C ::;;Q A. Hence C ::;;Q B. 
â¢ 
If W is a collection of subsets of N, we write 
co-W = {A ~ N I A E W}. 
Theorem 3.6. Let co-W be Q-closed, let A be Q-complete for W, and let 
A E co-W. Then we have W = co-W. 

3. Reducibility 
209 
Proof. 
Let BE W. Then, since A is Q-complete for W, B ~Q A. Since 
A E co-W and co-W is Q-closed, BE co-W. This proves that W ~co-W. 
Next let B E co-W. Then ii E W. By what has already been shown, 
ii E co-W. Hence B E W. This proves that co-W ~ W. 
â¢ 
As we shall see, Theorem 3.6 is quite useful. Our applications will be to 
the case of one-one and many-one reducibility. For this purpose, it is 
useful to note 
Theorem 3.7. If A ~m B, then A ~m B. Likewise if A ~, B, then 
A~, ii. 
Proof. If A = {x E Nlf(x) E B}, then clearly A= {x E Nlf(x) E B} . â¢ 
Corollary 3.8. If W ism-closed or 1-closed, then so is co-W. 
Proof. Let BE co-W, A ~m B. By the theorem, A ~m B. Since BE W 
and W ism-closed, A E W. Hence A E co-W. Similarly for ~, . 
â¢ 
For a concrete example, we may take W to be the collection of r.e. 
subsets of N. (For notation, the reader should review Chapter 4, Section 
4.) We have 
Theorem 3.9. 
K is 1-complete for the class of r.e. sets. 
Proof. 
Let A be any r.e. set. We must show that A ~, K. Since A is r.e., 
we have 
A = {x EN I f(xH}, 
where f is a partially computable function. Let g(t, x) = f(x) for all t, x. 
Thus, g is also partially computable. Using the (unrelativized) universality 
and parameter theorems, we have for a suitable number e: 
Hence, 
g(t,x) = ci><2>(t,x,e) = ci>(t,Sf(x,e)). 
A = {x EN I f(x) ~} 
= {x EN I g(Sf(x, e), x) ~} 
= {x EN I ci>(Sf(x, e), Sf(x, e)H} 
= {x EN I Sf(x,e) E K}. 
Thus, A ~m K. But, by the strengthened version of the parameter theo-
rem (Theorem 2.5), Sf(x, e) is actually one-one. Hence, A ~, K. 
â¢ 

210 
Chapter 8 Classifying Unsolvable Problems 
The class of r.e. sets is easily seen to be m-closed. Thus, let f be partial-
ly computable, let A = {x EN I f(x) ~ }, and let B = {x EN I g(x) E A}, 
where g is computable. Then 
B = {x EN I f(g(x))t}, 
so that B is r.e. Applying Theorems 3.2, 3.6, and 3.9 and Corollary 3.8, we 
obtain the not very interesting conclusion: 
If Kis r.e., then the complement of every r.e. set is r.e. 
Since we know that K is in fact not r.e., this does us no good. However, 
Corollary 3.8 and Theorem 3.6 together with the fact that there is an r.e. 
set (e.g., K) whose complement is not r.e. permits us to conclude 
Theorem 3.10. If A is m-complete for the class of r.e. sets, then A is not 
r.e., so that A is not recursive. 
We conclude this section with a simple but important construction. For 
A, B ~ N we write 
A E9 B = {2x I x E A} u {2x + 11 x E B}. 
Intuitively, A E9 B contains the information in both A and B and nothing 
else. This suggests the truth of the following simple result. 
Theorem 3.11. 
A ~. A E9 B, B ~. A E9 B. If A ~. C and B ~. C, then 
A E9 B ~.C. 
Proof. The following program (A E9 B)-computes A: 
x~2x 
X~ O(X) 
Y~x 
If the first instruction is replaced by X~ 2X + 1, the program (A E9 B)-
computes B. 
Finally, let CA, C8 be the characteristic functions of A and B, respec-
tively. Assuming that A and B are both C-computable, there must be 
programs that C-compute the functions CA and C8 , respectively. Hence, 
we may use macros 

4. Sets r.e. Relative to an Oracle 
211 
in programs that have C available as oracle. Thus, the following program 
C-computes A E9 B: 
Exercises 
IF21 X GOTO D 
X+-- l(X _:_ 1)/2J 
Y +-- CiX) 
GOTOE 
[D] 
X+-- lX/2J 
Y +-- CiX) 
â¢ 
1. Let U = {x EN ll(x) E W,.<x>}. Show that U is 1-complete for the 
class of r.e. sets. 
2. 
Let K :::;; 1 A and let 
C = {x EKI<I>/x) ~A E9A}. 
Prove that A :::;;1 C, C :::;;1 A, but C ~ m A. 
3. Prove that Theorem 3.11 holds with :::;; 1 replaced by ::;;m â¢ 
4. Let FIN = {x E N I W., is finite}. Prove that K :::;;1 FIN. 
5. Prove that if B, li-=/= 0, then for every recursive set A, A :::;;m B. 
4. Sets r.e. Relative to an Oracle 
If G is a total function (of one or more arguments) we say that a set 
B ~ N is G-recursively enumerable (abbreviated G-r.e.) if there is a par-
tially G-computable function g such that 
B = {x E N I g(x) ~}. 
By Theorem 1.6, r.e. sets are then simply sets that are G-r.e. for some 
computable function G. 
It is easy to relativize the proofs in Chapter 4, Section 4, using, in 
particular, the relativized step-counter theorem. We give some of the 
results and leave the details to the reader. 
Theorem 4.1. If B is a G-recursive set, then B is G-r.e. 

212 
Chapter 8 Classifying Unsolvable Problems 
Theorem 4.2. The set B is G-recursive if and only if B and li are both 
G-r.e. 
Theorem 4.3. If B and C are G-r.e. sets, so are B u C and B n C. 
Next, we obtain 
Theorem 4.4. The set A is G-r.e. if and only if there is a G-computable 
predicate Q (x, t) such that 
A= {x EN l(3t)Q(x,t)}. 
(4.1) 
Proof. First let A be G-r.e. Then, there is a partially G-computable 
function h such that 
A = {x E N I h(x H}. 
Writing h(x) = <I>G(x, z0 ), we have 
A = {x EN I (3t)STPi}>(x, z0 , t)}, 
which gives the result in one direction. 
Conversely, let (4.1) hold, where Q is a G-computable predicate. Let 
h(x) be the partial function which is G-computed by the following pro-
gram: 
Then clearly, 
so that A is G-r.e. 
[B] 
Z +--- Q(X, Y) 
Y+-Y+1 
IF Z = OGOTO B 
A = {x E N I h(x) ~}, 
â¢ 
Corollary 4.5. The set A is G-r.e. if and only if there is a G-recursive set 
B such that 
A= {x EN l(3y)((x,y) E B)}. 
Proof. If B is G-recursive, then the predicate ( x, y) E B is G-computa-
ble (by Theorem 1.4) and hence, by the theorem, A is G-r.e. 
Conversely, if A is G-r.e., we have a G-computable predicate Q such 
that (4.1) holds. Letting B = {z EN I Q(/(z), r(z))}, B is (again by Theo-
rem 1.4) G-recursive and 
A= {x EN l(3y)((x,y) E B)}. 
â¢ 

4. Sets r.e. Relative to an Oracle 
213 
For any unary function G, we write 
w,c = {x ENI<I>c(x,n)J,}. 
(Thus W, = Wj.) For the remainder of this section, G will be a unary total 
function. We have at once 
Theorem 4.6 (Relativized Enumeration Theorem). A set B is G-r.e. if 
and only if there is an n for which B = w,c . 
We define 
G' = {n E N I n E w,c}. 
(Thus, K = /'.) G' is called the jump of G. We have 
Theorem 4.7. G' is G-r.e. but not G-recursive. 
This is just the relativization of Theorem 4.7, in Chapter 4, and the 
proof of that theorem relativizes easily. However, we include the details 
because of the importance of the result. 
Proof of Theorem 4. 7. Since 
G' = {n EN I <l>c(n, n)J,}, 
the relativized universality theorem shows that G' is G-r.e. If G' were also 
G-r.e., we would have G' = w;c for some i EN. Then 
i E G' <=> i E we <=> i E G' 
I 
' 
a contradiction. 
â¢ 
Our next result is essentially a relativization of Theorem 3.9. 
Theorem 4.8. The following assertions are all equivalent: 
a. 
A .:5;1G'; 
b. 
A .:5;m G'; 
c. 
A is G-r.e. 
Proof. It is obvious that assertion a implies b. To see that b implies c, let 
h be a recursive function such that 
x E A 
if and only if h(x) E G'. 
Then 
x E A 
if and only if 
<l>c(h(x), h(x)) L 
so that A is G-r.e. 

214 
Chapter 8 Classifying Unsolvable Problems 
Finally, to see that c implies a, let A be G-r.e., so that we can write 
A= {x EN I f{x),l.}, 
where f is partially G-computable. Let g(t, x) = f(x) for all t, x. By the 
relativized universality and parameter theorems, we have, for some num-
ber e, 
Hence, 
g(t, x) = ct>g>(t, x, e)= cl>c(t, S/(x, e)). 
A= {x EN I f(x),l.} 
= {x EN I g(S/(x, e), x),l.} 
= {x EN I <l>c(S/(x, e), Sf(x, e)),!.} 
= {x EN I S/(x,e) E G'}. 
Since, by Theorem 2.5, Sf(x, e) is one-one, we have A ~ 1 G'. 
â¢ 
Theorem 4.9. IfF and G are total unary functions and F is G-recursive, 
then F' ~ 1 G'. 
Proof. 
By Theorem 4.7, F' is F-r.e. That is, we can write 
F' = {x ENIJ(x),l.}, 
where f is partially F-computable. By Theorem 1.5, f is also partially 
G-computable. Hence F' is G-r.e. By Theorem 4.8, F' ~ 1 G'. 
â¢ 
By iterating the jump operation, we can obtain a hierarchy of problems 
each of which is "more unsolvable" than the preceding one. 
We write c<n> for the jump iterated n times. That is, we define 
We have 
G<O> = G, 
c<n+l> = (G<n>)'. 
Theorem 4.10. 0<n+ I) is 0<n>-r.e. but not 0<n>-recursive. 
Proof. 
Immediate from Theorem 4.7. 
â¢ 
It should be noted that, by Theorem 4.9, K =1 0', since I and 0 are 
both recursive and K = I'. Later we shall see that much more can be said 
along these lines. 

5. The Arithmetic Hierarchy 
215 
Exercise 
1. Show that there are sets A, B, C such that A is B-r.e. and B is C-r.e., 
but A is not C-r.e. 
5. The Arithmetic Hierarchy 
The arithmetic hierarchy, which we will study in this section, is one of the 
principle tools used in classifying unsolvable problems. 
Definition. 
I 0 is the class of recursive sets. For each n E N, In+ 1 is the 
class of sets which are A-r.e. for some set A that belongs to In. For all n, 
nn =co-In, an= Inn nn. 
Note that I 1 is the class of r.e. sets and that I 0 = ll 0 = a0 = d 1 is the 
class of recursive sets. 
Theorem 5.1. 
In~ In+l> lln ~ lln+lÂ· 
Proof. 
For any set A E In, A is A -r.e. and hence A E In+ 1 . The rest 
follows by taking complements. 
â¢ 
Theorem 5.2. 0<n> E In. 
Proof. 
By induction. For n = 0 the result is obvious. The inductive step 
follows at once from Theorem 4.10. 
â¢ 
Theorem 5.3. 
A E In+ 1 if and only if A is 0<n>-r.e. 
Proof. If A is 0<n>-r.e., it follows at once from Theorem 5.2 that 
A E In+ I' 
We prove the converse by induction. If A E l 1 , then A is r.e., so, of 
course, A is 0-r.e. Assume the result known for n = k and let A E Ik+ 2 . 
Then A is B-r.e. for some B E Ik+ 1 . By the induction hypothesis, B is 
0<k>-r.e. By Theorem 4.8, A :::;;1 B' and B :o;;10<k+ 1). By Theorem 4.9, 
B' :o;;10<k+Z>. Hence A :o;;10<k+Z>, and by Theorem 4.8 again, A is 0<k+ I)_ 
~ 
. 
Corollary 5.4. For n ~ 1 the following are all equivalent: 
A< 0(n). 
-1 
' 

216 
Chapter 8 Classifying Unsolvable Problems 
Proof. 
This follows at once from Theorems 4.8 and 5.3. 
Corollary 5.5. For n ~ 1, 0<n> is 1-complete for !.n. 
Proof. 
Immediate from Theorem 5.2 and Corollary 5.4. 
â¢ 
â¢ 
Corollary 5.6. For n ~ 1, !.n and lln are both m-closed and hence 
1-closed. 
Proof. 
Let A E !.n, B :::;;m A. Then using Corollary 5.4 twice, B :::;; m 0<n>, 
and hence B E !,n. This proves that !,n is m-closed. The result for nn is 
now immediate from Corollary 3.8. 
â¢ 
Theorem 5.7. 
A E An+ 1 if and only if A :::;;1 0<n>. 
Proof. 
Immediate from Theorems 4.2 and 5.3. 
â¢ 
In particular, since K =1 0' (actually K =1 0'), A2 consists of all sets 
that are K-recursive, that is, sets for which there are algorithms that can 
decide membership by making use of an oracle for the halting problem. 
Theorem 5.8. 
!,n u nn ~ An+ I. 
Proof. 
For n = 0, the inclusion becomes an equality, so we assume 
n ~ 1. If A E In, then by Corollary 5.4, A :::;;10<n>, so by Theorem 5.7, 
A E An+ I" If A E nn, then A ::;;10(n). But clearly A :::;;1 A(for example, 
by Theorem 1.4). Hence A :::;; 1 0<n> and by Theorem 5.7, A E An+tÂ· 
â¢ 
Theorem 5.9. For n ~ 1, 0<n> E In - An. 
Proof. 
By Theorem 4.10, 0<n> is not 0<n -!>-recursive. 
â¢ 
Theorem 5.10 (Kieene's Hierarchy Theorem). We have for n ~ 1 
1. An c !.n, An c lln; 
2. Inc In+tâ¢ nn c nn+t; 
3. In u nn c An+tÂ· 
Proof. 
1. By definition An ~ !.n, An ~ lln. By Theorem 5.9, 0<n> E In - An, 
and so 0(n)E nn -An. Thus the inclusions are proper. 
2. By Theorem 5.1 we need show only that the inclusions are proper. 
B 
0 (n+l) 
~ 
If0(n+l) 
~ b Th 
580(n+l) 
A 
ut 
E "'-n+l" 
E "'-nâ¢ ~orem . , 
E an+!â¢ 
contradicting Theorem 5.9. Likewise 0<n+t>E lln+t- lln. 

6. Post's Theorem 
217 
3. By Theorem 5.8, we need show only that the inclusion is proper. Let 
An = 0(n) (B 0(n). We shall show that An E An+ I -(In u lln). By 
Theorem 3.11 (with C = 0<n>), we have An ::5; 1 0<n>. Hence An E 
An+ 1 â¢ Also, 
0(n) = {x EN l2x E An}, 
0(n)= {x ENI2x + 1 EAn}. 
Hence 0<n> ::5; 1 An, 0<n> ::5; 1 An. Suppose that An E In. Then, by 
Corollary 5.6, 0<n>E In, so that 0<n> E An, contradicting Theorem 
5.9. Likewise if An E nn' then 0(n) E nn and hence 0(n) E An . â¢ 
Since we have now seen that for all n ~ 1, In =I= co-In, and since we 
know that for n ~ 1, In and lln are each m-closed, we may apply 
Theorem 3.6 to obtain the following extremely useful result. 
Theorem 5.11. If A ism-complete for In, then A f/=. lln. Likewise, if A 
is m-complete for lln, then A f/=. In. 
6. 
Post's Theorem 
In order to make use of the arithmetic hierarchy, we will employ an 
alternative characterization of the classes In, nn involving strings of 
quantifiers. This alternative formulation is most naturally expressed in 
terms of predicates rather than sets. Hence we will use the following 
terminology. 
We first associate with each predicate P(x 1 , â¢â¢â¢ , x.) the set 
A= {x ENIP((x)1 , â¢â¢â¢ ,(x).}}. 
Then we say that P is In or that P is a In predicate to mean that 
A E In. Likewise, we say that p is nn or An if A E nn or A E An' 
respectively. Notice that we continue to regard In and nn as consisting of 
subsets of N, and we will not speak of a predicate as being a member of 
In or nn. 
Our terminology involves a slight anomaly for unary predicates. We have 
just defined P(x) to be In (or lln) if the set A = {x EN I P((x)1)} 
belongs to In (or lln), whereas it would be more natural to speak of P(x) 
as being In (or lln) depending on whether B = {x EN I P(x)} belongs to 
In (or lln). Fortunately, there is really no conflict, for we have 

218 
Chapter 8 Classifying Unsolvable Problems 
Theorem 6.1. Let B = {x EN I P(x)}. Then P(x) is In if and only if 
B E In 0 Likewise for nn, An 0 
Proof. For n = 0, the result is obvious, so assume that n ~ 1. P(x) is In 
(or lln, or An) if and only if the set A = {x EN I P({x)1)} belongs to In 
(or lln or An). Now, 
A = {x E N I (x)1 E B}, 
and 
B={xENI2xEA}. 
Thus A =m B. By Corollary 5.6, this gives the result. 
Theorem 6.2. 
Let P(x 1 , â¢â¢â¢ , x 5 ) be a In predicate and let 
Q(tl , ... ,tk) = P{fl(tl , ... ,tk), ... ,f.{tl , ... ,tk)), 
â¢ 
where f 1 , â¢â¢â¢ , fs are computable functions. Then Q is also In. Likewise 
for nn 0 
Proof. 
Let 
A = {x EN I P((x)1, ... , (x).)}, 
B ={tEN I Q((t)l , ... ,(t)k)}. 
We shall prove that B ~m A. It will thus follow that if A E In (or lln), 
then B E In (or lln), giving the desired result. 
We have 
t E B = Q((t)1 , ... ,(t)k) 
-
p ( f1 ( ( t) 1 , ... , ( t) k), ... , fs ( ( t) 1 , ... , ( t) k)) 
-
[ fl ( ( t) I ' 0 
0 
0 
' 
( t) k)' 0 
0 
0 ,f. ( ( t) I ' 0 
0 
0 
' 
( t) k)] E A' 
so that B ~m A. 
â¢ 
Theorem 6.3. A predicate P is In (or lln) if and only if - P is lln (or 
In)â¢ 
Proof. 
A = {x EN I P((x)1, ... , (x).)} implies 
A= {x EN I -P((x)1 ,oo.,(x).)}. 
â¢ 
Theorem 6.4. Let P(x 1,. 00, x.), Q(xp 00., x,) be In (or lln). Then the 
predicates P & Q and P V Q are likewise In (or lln). 

6. Post's Theorem 
Proof. 
For n = 0, the result is obvious. Assume that n ~ 1 and let 
A = {x EN I P((x) 1 , â¢â¢â¢ , (x).)}, 
B = {x EN I Q((x) 1 , â¢â¢â¢ , (x)s)}, 
C = {x EN I P((x) 1 , â¢â¢â¢ , (x)s) & Q((x) 1 , â¢â¢â¢ , (x).)}, 
D = {x EN I P((x) 1 , â¢â¢â¢ ,(x)s) V Q((x) 1 , â¢â¢â¢ ,(x),)}. 
219 
Thus, C =An Band D =AU B. If P and Q are In, then A, BE In. 
Thus, by Theorem 5.3, A and B are both 0<n -1)-r.e. By Theorem 4.3, C 
and D are likewise 0<n- 1>-r.e., and so P & Q and P V Q are In. 
If p and Q are nn' then A, B E nn so that A, jj E In 0 By Theorems 
4.3 and 5.3, An B = (A U B) E In and AU ii = (A n B) E In . Hence 
D,C E nn, so that both p v Q and p & Q are nn. 
â¢ 
Theorem 6.5. 
Let Q(x 1, â¢â¢â¢ , xs, y) be In, n ~ 1, and let 
P(x 1 , â¢â¢â¢ ,xs) = (3y)Q(x1 , â¢â¢â¢ ,x.,y). 
Then P is also In. 
Proof. 
Let 
A = {x EN I Q((x) 1 , â¢â¢â¢ , (x)s, (x)s+ 1)}, 
B = {x EN I P((x) 1 , â¢â¢â¢ ,(x),)}. 
We are given that A E In, i.e., that A is 0<n -I>-r.e., and we must show 
that B is likewise 0<n- 1 >-r.e. 
By Theorem 4.4, we may write 
A= {x EN l(3t)R(x,t)}, 
where R is 0<n -I>_recursive. Hence, 
Thus, 
Q(x 1 , â¢â¢â¢ ,x.,y) = [x 1 , â¢â¢â¢ ,xs,y] EA 
= (3t)R([x 1 , â¢â¢â¢ ,xs,y],t). 
x E B = P((x)1, â¢â¢â¢ ,(x)s) 
= (3y)Q((x) 1 , â¢â¢â¢ , (x)s, y) 
= (3y)(3t)R([(x) 1 , â¢â¢â¢ ,(x)..,y],t) 
= (3z)R([(x) 1 , â¢â¢â¢ , (x) .. , /(z)], r(z)). 
By Theorems 1.4 and 4.4, B is 0<n -1)-r.e. 
â¢ 

220 
Chapter 8 Classifying Unsolvable Problems 
Theorem 6.6. Let Q (x 1 , â¢â¢â¢ , xs, y) be lln, n ~ 1, and let 
P(x1 , â¢â¢â¢ ,xs)- (Vy)Q(x 1 , â¢â¢â¢ ,xs,y). 
Then P is also lln. 
Proof. 
-P(x1 , â¢â¢â¢ ,x,)- (3y) -Q(xpÂ·Â·Â·Â·x.,y). 
from Theorems 6.3 and 6.5. 
The result follows â¢ 
The main result of this section is 
Theorem 6. 7 (Post's Theorem). A predicate P(x 1 , â¢â¢â¢ , x,) is In+ 1 if and 
only if there is a ll n predicate Q(x 1 , â¢â¢â¢ , X s, y) SUCh that 
P(x 1 , â¢â¢â¢ ,x)- (3y)Q(x 1 , â¢â¢â¢ ,xs,y). 
(6.1) 
Proof. If (6.1) holds, with Q a nn predicate, it is easy to see that P must 
be In+ 1 â¢ By Theorem 5.8, Q is certainly itself In+ 1 , and therefore, by 
Theorem 6.5, P is In+ 1 â¢ 
The converse is somewhat more difficult. Let us temporarily introduce 
the following terminology: we will say that a predicate P(x 1 , â¢â¢â¢ , xs) is 
3n+ I if it can be expressed in the form (6.1), where Q is nn. Then Post's 
theorem just says that the In+ 1 and the 3 n + 1 predicates are the same. We 
have already seen that all 3n+ 1 predicates are In+ 1 â¢ 
Lemma 1. If a predicate is In ' then it is 3 n + I. 
Proof. For n = 0, the result is obvious. Let n ~ 1, and let P(x 1 , â¢â¢â¢ , xs) 
be In. Let 
A= {x EN I P((x) 1 , â¢â¢â¢ ,(x),)}. 
Then A is eJ<n-J>_r.e., so by Theorem 4.4, 
A= {x EN l(3t)R(x,t)}, 
where R is 0<n -!>-recursive. Thus 
P(x 1 , â¢â¢â¢ ,xs)- (3t)R([x1 , â¢â¢â¢ ,xs],t). 
It remains to show that R([x 1 , â¢â¢â¢ , xs], t) is lln. But in fact, by Theorem 
1.4, R([x 1 , â¢â¢â¢ , xs], t) is 0n- 1-recursive, so that it is actually an and hence 
certainly nn. 
â¢ 
Lemma 2. If a predicate is nn' then it is 3 n + 1â¢ 
Proof. If P(x 1 , â¢â¢â¢ , x) is nn, we need only set 
Q(x 1 , â¢â¢â¢ ,xs,y)- P(x 1 , â¢â¢â¢ ,x), 

6. Post's Theorem 
so that, of course, 
P(x 1 , â¢â¢â¢ ,x) = (3y)Q(x 1 , â¢â¢â¢ ,x.,y). 
Since 
{x EN I Q((x) 1 , â¢â¢â¢ ,(x).,(x)s+ 1 )} = {x EN I P((x) 1 , â¢â¢â¢ ,(x)5 )}, 
the predicate Q is also nn' which gives the result. 
Lemma 3. If P(x 1 , â¢â¢â¢ , x., z) is 3n+ 1 and 
Q(x 1 , â¢â¢â¢ ,X5 ) = (3z)P(x1 , â¢â¢â¢ ,x.,z), 
then Q is 3 n + I. 
Proof.Â· We may write 
P(x 1 ,ooo,X5 ,z) = (3y)R(x 1 ,ooo,X5 ,z,y), 
where R is nn o Then 
Q(x1 ,ooo,X5 ) = (3z)(3y)R(x 1 ,ooo,X5 ,z,y) 
= (3t)R(x 1 , o o o, X 5 , l(t), r(t)), 
which is 3 n + I by Theorem 6o2o 
Lemma 4. If P and Q are 3n+1, then so are P & Q and P V Qo 
Proof. 
Let us write 
P(x1 ,ooo,x) = (3y)R(x 1 ,ooo,x.,y), 
Q(x1 ,ooo,X5 ) = (3z)S(x1 ,ooo,x.,z), 
where RandS are nn o Then 
and 
P(x 1 ,ooo,x)& Q(x 1 ,ooo,X5 )=(3y)(3z)[R(x1 ,ooo,X5 ,y) 
&S(x1 ,ooo,X5 ,z)] 
P(x1 ,ooo,X5 ) V Q(x1 ,ooo,x) = (3y)(3z)[R(x1 ,ooo,x.,y) 
VS(x 1 ,ooo,X5 ,z)]o 
221 
â¢ 
â¢ 
The result follows from Theorem 6.4 and Lemmas 2 and 30 
â¢ 
Lemma 5. If P(x 1 , 0 
0 0, x., t) is 3n+ 1 and 
Q(x 1 ,ooo,X5 ,y) = ('Vt):>.yP(x 1 ,ooo,x.,t), 
then Q is 3 n + I o 

222 
Chapter 8 Classifying Unsolvable Problems 
Proof. Let 
P(x 1 , â¢â¢â¢ ,x.,t) <=> (3z)R(x 1 , â¢â¢â¢ ,xs,t,z), 
where R is lln. Thus, 
Q(x 1 , â¢â¢â¢ ,xs,y) <=> (Vt),/3z)R(x1 , â¢â¢â¢ ,x.,t,z) 
<=> (3u)(Vt),YR(x1 , â¢â¢â¢ ,x.,t,(u)1+1), 
where we are using the Godel number u = [z0 , z1 , â¢â¢â¢ , zy] to encode the 
sequence of values of z corresponding to t = 0, 1, ... , y. Thus, 
Q(x1 , â¢â¢â¢ ,xs,y) <=> (3u)(Vt)[t > y V R(x1 , â¢â¢â¢ ,xs,t,(u)1+ 1)] 
<=> (3u)S(x1 , â¢â¢â¢ , xs, y, u), 
where S is lln. For n = 0, we have used Theorem 6.3 from Chapter 3; and 
for n > 0, we have used the fact that the predicate t > y is recursive (and 
hence certainly lln), and Theorems 6.2, 6.4, and 6.6. 
â¢ 
We now recall from Section 2 that u -< G means that 
{u}(i) = G(i) for 
0:::;; i < l(u). 
Lemma 6. 
Let R(x) be In . Then the predicate u -< R is 3 n +I. 
Proof. We have 
u-< R <=> (Vi)<l(u){[(r(u))i+ 1 = 1 & R(i)] V [(r(u))i+l = 0&- R(i)]} 
<=>l(u) = 0 V (3z)(z + 1 = /(u)&(Vi),z{[(r(u));+l = 1& R(i)] 
v[(r(u))i+l = 0&- R(i)]}). 
Thus, using Lemmas 1-5 and the fact that the predicate - R(i) is lln, we 
have the result. 
â¢ 
Proof of Theorem 6. 7 (Post's Theorem) Concluded. Let P(x 1 , â¢â¢â¢ , x,) be any 
In+ 1 predicate. Let 
A= {x EN I P((x) 1 , â¢â¢â¢ ,(x)s)}. 
Then A E In+ 1 , which means that A is B-r.e. for some set B E In. Let 
R(x) be the characteristic function of B, so that by Theorem 6.1, R is In. 
Since A is B-r.e., we are able to write 
A = {x EN I f(x}t}, 

6. Post's Theorem 
223 
where f is partially B-computable. Let f be B-computed by a program 
with number y0 . Then, using Theorem 2.4 (the finiteness theorem), we 
have 
x EA = (3t)STPk1>(x,y0 ,t) 
= (3t)(3u){u -< R & STI(W(x, Yo, t)}. 
Thus, 
P(x 1 , â¢â¢â¢ ,x,) = (3t)(3u){u-< R &STI(W([x 1 , â¢â¢â¢ ,xs],y0 ,t)}. 
Therefore by Theorem 2.3 and Lemmas 3, 4, and 6, P is 3 n + 1â¢ 
â¢ 
Now that we know that being In+ 1 and 3n+ 1 are the same, we may 
rewrite Lemma 5 as 
Corollary 6.8. If P(x 1 , â¢â¢â¢ , x s, t) is In and 
Q(x 1 , â¢â¢â¢ ,xs,y) = ('Vt)5,yP(x 1 , â¢â¢â¢ ,x.,t), 
then Q is also In. 
Also, we can easily obtain the following results. 
Corollary 6.9. A predicate P(x I ' ... ' X s) is n n + I if and only if there is a 
In predicate Q (x 1 , â¢â¢â¢ , x s, y) such that 
P(x 1 , â¢â¢â¢ , x) = ('Vy)Q(x 1 , â¢â¢â¢ , x., y). 
Proof. 
Immediate from Post's theorem and Theorem 6.3. 
â¢ 
Corollary 6.10. If P(x I' ... ' Xs' t) is nn' and 
Q(x 1 , â¢â¢â¢ ,x,y) = (3t) 5 YP(x 1 , â¢â¢â¢ ,xs,t), 
then Q is also nn. 
Proof. 
Immediate from Corollary 6.8 and Theorem 6.3. 
â¢ 
We are now in a position to survey the situation. We call a predicate 
P(x1 , â¢â¢â¢ , xs) arithmetic if there is a recursive predicate R(x 1 , â¢â¢â¢ , xs, 
y 1 , â¢â¢â¢ , Yn) such that 
P(xl , ... ,x,) = (Qiyi)(QzJ2) Â·Â·Â· (QnYn)R(xl , ... ,xs,YI , ... ,yn), 
(6.2) 
where each of Q 1 , â¢â¢â¢ , Qn is either the symbol 3 or the symbol V. We say 
that the Q; are alternating if for 1 ~ i < n when Q; is 3, then Q; + 1 is V 
and vice versa. Then we have 

224 
Chapter 8 Classifying Unsolvable Problems 
Theorem 6.11. 
a. 
Every predicate that is !.n or lln for any n is arithmetic. 
b. 
Every arithmetic predicate is !.n for some n (and also lln for some 
n). 
c. 
A predicate is !.n (or lln) if and only if it can be represented in the 
form (6.2) with Q1 = 3 (or Q1 = 'V) and the Q; alternating. 
Proof. Since !.0 and ll 0 predicates are just recursive, they are arithmetic. 
Proceeding by induction, if we know, for some particular n, that all !.n and 
lln predicates are arithmetic, then Theorem 6.7 and Corollary 6.9 show 
that the Same is true for !_n+ 1 and fin+ 1 predicateS. This proveS a. 
For b we proceed by induction on n, the number of quantifiers. For 
n = 0, we have a !.0 (and a ll 0 ) predicate. If the result is known for n = k, 
then it follows for n = k + 1 using Theorems 6.5-6.7 and Corollary 6.9. 
Finally, cis easily proved by mathematical induction using Theorem 6.7 
and Corollary 6.9. 
â¢ 
7. 
Classifying Some Unsolvable Problems 
We will now see how to apply the arithmetic hierarchy. We begin with the 
set 
TOT= {zEN I ('Vx)<l>(x, zH}, 
which consists of all numbers of programs which compute total functions. 
This set was discussed in Chapter 4, Section 6, where it was shown that 
TOT is not r.e. Without relying on this previous discussion, we shall obtain 
much sharper information about TOT. 
We begin by observing that 
TOT= {zEN I ('Vx)(3t)STP(I>(x, z, t)}, 
so that TOT E ll 2 â¢ We shall prove 
Theorem 7.1. TOT is 1-complete for ll 2 â¢ Therefore, TOT rt !.2 . 
Proof. The second assertion follows from the first by Theorem 5.11. 
Since we know that TOT E n 2' it remains to show that for any A E n2' 
we have A ~ 1 TOT. For A E ll 2 , we can write 
A ={wEN I ('Vx)(3y)R(x, y, w)}, 
where R is recursive. Let 
h(x, w) = minR(x, y, w), 
y 

7. Classifying Some Unsolvable Problems 
225 
so that h is partially computable. Let h be computed by a program with 
number e. Thus, 
(3y)R(x,y,w) = h(x,wH = <f><2>(x,w,eH = <l>(x,Sf(w,e))J, 
where we have used the parameter theorem. Hence, 
w EA = (Vx)(3y)R(x,y,w) 
= (Vx)[ <l>(x, Sf(w, e)H] 
= Sf(w, e) E TOT. 
Since, by Theorem 2.5, Sf(w, e) is one-one, we can conclude that 
A .:5;1TOT. 
As a second simple example we consider 
INF = {z E N I JJi is infinite}. 
We have 
z E INF = (Vx)(3y).(y > x & y E JJi). 
Now 
y E Ui- (3t)STP(l>(y, z, t), 
â¢ 
and hence the predicate y E JJi is !.1 . Using Theorems 6.4 and 6.5, 
(3y) (y > x & y E ~) is also !.1, and finally INF E II2 . We shall show 
that INF is also 1-complete for II 2 â¢ By Theorem 3.5, it suffices to show 
that TOT .:5;1 INF since we already know that TOT is 1-complete for II 2 â¢ 
To do this we shall obtain a recursive one-one function f(x) such that 
~ = N implies 
Uf<x> = N 
and 
~ 
=I= N implies 
Uf<x> is finite. 
Having done this we will be through since we will have 
x E TOT= f(x) E INF, 
and therefore, 
TOT .:5;1 INF. 
(7.1) 
The intuitive idea behind the construction of f is that program number 
f(x) will "accept" a given input z if and only if program number x 

226 
Chapter 8 Classifying Unsolvable Problems 
"accepts" successively inputs 0, 1, ... , z. We can write this intuitive idea in 
the form of an equation as follows: 
Uf(x) ={zEN I ('Vk),. z(k E JÂ¥,)}. 
Now it is a routine matter to use the parameter theorem to obtain f. We 
first note that, by Corollary 6.8, the predicate ('V k), z (k E W) is !.1 . 
Hence, as earlier, there is a number e such that 
('Vk),z(k E JÂ¥,) <=> ci><2>(z,x,eH 
<=> cl>(z, Sf(x, e)H 
<=> z E WS/(x,e). 
Thus the desired function f(x) is simply Sf(x, e), which is one-one, as we 
know from Theorem 2.5. 
This completes the proof that INF is 1-complete for ll 2 . Hence also, 
INF ft. !.2 â¢ 
The following notation will be useful. 
Definition. Let A, B, C ~ N. Then we write A :::;;m (B, C) to mean that 
there is a recursive function f such that 
x E A 
implies f(x) E B 
and 
x E A implies f(x) E C. 
Iff is one-one we write A :::;;1 (B, C). 
Thus A :::;;1 B is simply the assertion: A :::;;1 (B, B). 
It will be useful to note that by (7.1), we have actually proved 
TOT :::;; 1 (TOT,INF). 
(7.2) 
Now, we have 
Theorem 7.2. If A :::;; 1 (B, C), B ~ D, and C n D = 0, then A :::;; 1 D. 
Proof. 
We have a recursive one-one function f such that 
x E A 
implies f(x) E B 
implies f(x) ED 
and 
x E A implies f(x) E C 
implies f(x) E 75. 
â¢ 

7. Classifying Some Unsolvable Problems 
227 
Our final example will classify a I 3 set, and is considerably more 
difficult than either of those considered so far. 
Theorem 7 .3. Let 
COF = {x EN I J.V.. is finite}. 
Then COF is 1-complete for I 3 â¢ 
Lemma 1. COF E I 3 â¢ 
Proof. 
COF = {x EN I (3n)(Vk)(k ~ n v k E J.V..)}. 
Since the predicate in parentheses is I 1 , the result follows from Theorem 
6.11. 
â¢ 
We introduce the notation 
nÂ»i. = {m EN I STP(ll(m, x, n)}. 
Intuitively, nÂ»i- is the set of numbers that program number x "accepts" in 
~ n steps. Clearly, 
We also define 
nÂ»i.' = {m <rIm EnJ.V..}. 
We write L(n, x) to mean that 
Clearly L(n, x) is a recursive predicate. We write 
R(x,n)- (Vr)s/r E J.V..) V [L(n,x)&(3k)<n(k !tnJ.V..)]. 
Since R(x, n) is I 1 we can use the parameter theorem, as in the previous 
example, to find a recursive one-one function g(x) such that 
~<xJ = {n I R(x, n)}. 
Lemma 2. If x E TOT, then g(x) E TOT. If x It INF, then g(x) E 
COF- TOT. 
Proof. If x E TOT, then J.V.. = N, so that (Vr)<n(r E J.V..) is true for all 
rz. Hence R(x, n) is true for all n, i.e., ~<xJ = N-and g(x) E TOT. 

228 
Chapter 8 Classifying Unsolvable Problems 
Now let x f/=. INF, i.e., JÂ¥, is finite. Therefore, there is a number n0 such 
that for all n > n 0 , we have 
and 
Thus, for n > n0 , 
i.e., L(n, x) is true. Thus, n > n0 implies that R(x, n) is true, i.e., that 
n E ~<x>. We have shown that all sufficiently large integers belong to 
~<x>Â· Hence g(x) E COF. It remains to show that g(x) f/=. TOT. 
Let s be the least number not in JÂ¥,. We consider two cases. 
Case 1. 
sf/=. ~<x>Â· Then surely g(x) f/=. TOT. 
Case 2. 
s E ~<x>Â· That is, R(x,s) is true. But (Vr) 55(r E JÂ¥,) must be 
false because s f/=. JÂ¥,. Hence L(s,x) must be true and (3k) < 5(k f/=. 5JÂ¥,). 
Now this number k is less than s, which is the least number not in JÂ¥,. 
Hence k E JÂ¥,. Since k f/=. 5JÂ¥,, 
(7.3) 
Now we claim that this number n f/=. ~<x>, which will show that in this 
case also g(x) f/=. TOT. Thus, suppose that n E ~<x>, i.e., that R(x, n) 
is true. Since sf/=. JÂ¥, and n ~ s, the condition (Vr)<n(r E JÂ¥,) must 
be false. Thus we would have to have L(n, x), i.e., n+~JÂ¥,n =nJÂ¥,n. But 
by (7.3), k < s ::; n, kEn+ 1JÂ¥,, and k f/=.nJÂ¥,. This is a contradiction . â¢ 
Lemma 3. TOT ::;1 (TOT, COF - TOT). 
Proof. Let f be the recursive one-one function satisfying (7.1) and let g 
be as above. Let h(x) = g(f(x)). Then using Lemma 2 and (7.1), we have 
x E TOT implies f(x) E TOT implies 
h(x) E TOT, 
x f/=. TOT implies f(x) f/=. INF 
implies 
h(x) E COF- TOT. 
â¢ 
Now let A E !.3 â¢ We wish to show that A ::::; 1 COF. By Post's theorem, 
we can write 
x E A -
(3n)B(x, n), 
where B is ll 2 â¢ Using the pairing functions, let 
C = {t EN I (3n) 5 t(t)B(r(t ), n)}. 

7. Classifying Some Unsolvable Problems 
229 
Thus, C E TI 2 â¢ Theorem 7.1, C :o:; 1 TOT. Hence, using Lemma 3, C :o:; 1 
(TOT, COF - TOT). Let 8 be a recursive one-one function such that 
t E C 
implies 
O(t) E TOT, 
t $. C 
implies 
O(t) E COF- TOT. 
(7.4) 
Consider the l 1 predicate r(z) E We((i(z),x))' Using the parameter theo-
rem as usual, we can write this in the form z E WrJ!(x>, where 1/J is a 
one-one recursive function. Thus, 
Wofr(x) = {(k,m)lm E We((k,x))}. 
(7.5) 
The theorem then follows at once from 
Lemma 4. 
x E A if and only if 1/J(x) E COF. 
Proof. 
Let x EA. Then B(x, n) is true for some least value of n. Hence, 
for all k ~ n, we have (k, x) E C. By (7.4), O((k, x)) E TOT for all 
k ~ n. Since n is the least value for which B(x, n) is true, B(x, k) is false 
fork< n. Hence, fork< n, (k, x) $.C. Thus, by (7.4), O((k, x)) E COF 
- TOT. To recapitulate, 
k > n 
implies 
O((k, x)) E TOT, 
and 
(7.6) 
k < n 
implies 
O((k,x)) E COF- TOT. 
Thus, by (7.5) we see that for k ~ n, (k, m) E Wofr(x) for all m. For each 
k < n, We( (k, xÂ» contains all but a finite set of m. Thus, altogether, Wofr(x) 
can omit at most finitely many integers, i.e., 1/J(x) E COF. 
Now, let x $.A. Then, B(x, n) is false for all n. Therefore, (k, x) $. C 
for all k. By (7.4), 
O((k,x)) E COF- TOT forall kEN, 
and thus certainly, 
O((k, x)) $.TOT for all kEN. 
That is, for every k E N, there exists m such that m $. We( (k, x))â¢ i.e., by 
(7.5), such that (k,m) $. Wofr(x>Â· Thus, ~(x> is infinite, and hence 1/J(x) $. 
COF. 
â¢ 
Exercises 
1. Show that the following sets belong to l 3 â¢ 
(a) {x E N I there is a recursive function f such that <l>x ~ f}. 
(b) { (x, y) I x EN & y EN & W. - w;, is finite}. 

230 
Chapter 8 Classifying Unsolvable Problems 
2. 
(a) Prove that for each m, n there is a predicate U(x 1 , â¢â¢â¢ , xm, y) 
which is In, such that for every In predicate P(x 1 , â¢â¢â¢ , xm) there 
is a number Yo with 
P(x1 , â¢â¢â¢ ,xm) <=> U(x 1 , â¢â¢â¢ ,xm,Yo). 
(b) State and prove a similar result for nn. 
3. Use the previous exercise to prove that for each n, nn - In -=t= 0. 
8. 
Rice's Theorem Revisited 
In Chapter 4, we gave a proof of Rice's theorem (Theorem 7.1) using the 
original parameter theorem. We get a somewhat stronger result using the 
strengthened form of the parameter theorem. 
Definition. Let r be a set of partially computable functions of one 
variable. As in Chapter 4, Section 7, we write 
Rr ={tEN I <I>, En. 
We call r nontrivial if r -=1= 0 and there is at least one partially com-
putable function g(x) such that g $. r. 
Theorem 8.1 (Strengthened Form of Rice's Theorem). Let f be a nontriv-
ial collection of partially computable functions of one variable. Then, 
K ::5; 1 Rr or K ::5; 1 Rr, so that Rr is not recursive. 
Thus not only is Rr nonrecursive, but the halting problem can be 
"solved" using Rr as an oracle. Actually, the first proof of Rice's theorem 
already shows that either K ::5;m Rr or K ::5;m Rr. We give essentially the 
same proof here, using the strengthened form of the parameter theorem to 
upgrade the result to one-one reducibility. 
Proof. 
We recall (Chapter 1, Section 2) that 0 is a partially computable 
function, namely, the nowhere defined function. 
Case 1. 0 ft. r. Since r is nontrivial, it contains at least one function, 
say f. Since f E r and 0 ft. r, f -=1= 0; f must be defined for at least 
one value. Let 
Since 
!l(x, t) = { ~{t) 
if X E K 
if X $. K. 
xEK=<I>(x,xH, 

9. Recursive Permutations 
231 
it is clear that 0. is partially computable. Using the parameter 
theorem in its strengthened form, we can write 
O.(x, t) = <l>g(x)(t), 
where g is a one-one recursive function. Then we have 
x E K implies 
<l>g(x) = f 
implies g(x) ERr; 
x ft K implies 
<l>g(x) = 0 
implies g(x) ft Rr. 
Thus, K ~ 1 Rr. 
Case 2. 
0 E f. Now let A be the class of all partially computable 
functions not in f. Thus, Rr = RtJ. and 0 ft A. By Case 1, K ~ 1 RtJ., 
and hence by Theorem 3.7, K ~ 1 Rr. 
â¢ 
Exercises 
1. State and prove a relativized version of Rice's theorem. 
2. (a) Develop a code for partial functions from N to N with finite 
domains, writing fn for the nth such function. 
(b) Prove the Rice-Shapiro theorem: Rr is r.e. if and only if f = 0 
or there is a recursive function t(x) such that 
r = {g I (3x)(g ;;2ft<x)}. 
9. 
Recursive Permutations 
Definition. A one-one recursive function f whose domain and range are 
both N is called a recursive permutation. 
With each recursive permutation f we may associate its inverse r I: 
r
1(t) = min(t = f(x)). 
X 
Then, r I is clearly likewise a recursive permutation. 
Definition. Let A, B ~ N. Then A and B are said to be recursively 
isomorphic, written A = B, if there is a recursive permutation f such that 
x E A if and only if f(x) E B. 
Since a recursive permutation provides what is essentially a mere change 
of notation, recursively isomorphic sets may be thought of as containing 
the same "information" presented in different notation. 

232 
Chapter 8 Classifying Unsolvable Problems 
It is obvious that A = B implies A =1 B. Remarkably, the converse 
statement is also true. 
Theorem 9.1 (Myhill). If A =1 B, then A = B. 
In our proof of this theorem we shall need to code sequences of ordered 
pairs of numbers. We shall speak of the code of the sequence 
(a! 'bJ)' ... ' (an 'bn) 
of pairs of elements of N meaning the number 
(9.1) 
Thus, the numbers a;, b; can be retrieved from the code u by using the 
relations 
a;= /((r(u));)) 
b; = r((r(u));) 
i = 1,2, ... ,/(u). 
Note that every natural number is the code of a unique finite (possibly 
empty) sequence of ordered pairs. 
We say that the finite sequence (9.1) associates A and B, where A, B ~ N, 
if 
1. a; =/= aj for 1 :::;; i < j :::;; n; 
2. b; =/= bj for 1 :::;; i < j :::;; n; 
3. for each i, 1 :::;; i :::;; n, either a; E A and b; E B or a; f/=. A and b; f/=. B. 
We shall prove the 
Lemma. Let A :::;;1 B. Then there is a computable function k(u, v) such 
that if u codes the sequence (9.1) that associates A and B and a f/=. 
{a1 , a 2 , â¢â¢â¢ , an}, then there is a b such that k(u, a) codes the sequence 
(a1 ,b1), â¢â¢â¢ ,(an ,bn),(a,b) 
that also associates A and B. 
Proof. 
Let f be a recursive one-one function such that 
x E A 
if and only if f(x) E B. 
(9.2) 
(9.3) 
We provide an algorithm for computing b from u and a. k(u, a) can then 
be set equal to the code of (9.2), i.e. 
k(u,a) = (/(u) + 1,r(u) Â·pA~'>~>1 ). 

9. Recursive Permutations 
233 
The numbers f(a 1 ), f(a 2 ), â¢â¢â¢ , f(an), f(a) are all distinct, because f is 
one-one. Hence, at least one of these n + 1 numbers does not belong to 
the set {b1 , b2 , â¢â¢â¢ , bn}. Our algorithm for obtaining b begins by computing 
f(a). If f(a) ft {b1 , b2 , â¢â¢â¢ , bn}, we set b = f(a). Otherwise, f(a) = b; for 
some i and we try f(a), because 
a E A <=> f(a) = b; E B <=>a; E a; E A <=> f(a) E B. 
If f(a) ft {b1 , b2 , â¢â¢â¢ , bn}, we set b = f(a;). Otherwise, if f(a) = bj, we 
continue the process, trying f(aj). By 1 and 2, none of the a; and b; 
obtained in this way duplicate previous ones. Thus, by our earlier remark 
the process must terminate in a value b. Using (9.3), we see that either 
a E A and b E B or a ft A and b ft B. 
â¢ 
Proof of Theorem 9.1. Since A ~ 
1 B, by the Lemma there is a computable 
function k(u, v) such that if u codes (9.1) that associates A and B and 
a ft {a1 , a2 , â¢â¢â¢ , an}, then for some b, k(u, a) codes the sequence (9.2) that 
also associates A and B. But since B ~ 1 A, we can also apply the Lemma 
to obtain a computable function k(u, v) such that if u codes (9.1) that 
associates A and B and b ft {bp b2 , â¢â¢â¢ , bn}, then for some a, k(u, b) 
codes the sequence (9.2) that likewise associates A and B. 
We let v(O) = 0, which codes the empty sequence. (Note that the empty 
sequence does associate A and B.) We let 
{ 
v(2x) 
v(2x + 1) = 
k(v(2x), x) 
if x is one of the left components 
ofthe sequence coded by v(2x) 
otherwise; 
if x is one of the right components 
{ 
v(2x + 1) 
v(2x + 2) = 
k(v(2x + 1),x) 
of the sequence coded by v(2x + 1) 
otherwise. 
Thus, we have 
1. v is a computable function. 
2. For each x, v(x) codes a sequence that associates A and B. 
3. The sequence coded by v(x + 1) is identical to, or is an extension of, 
the sequence coded by v(x). 
4. For each a EN, there is an x such that a pair (a, b) occurs in the 
sequence coded by v(x). (In fact, we can take x = 2a + 1.) 
5. For each b E N, there is an x such that a pair (a, b) occurs in the 
sequence coded by v{x). (In fact, we can take x = 2b + 2.) 

234 
Chapter 8 Classifying Unsolvable Problems 
We now define the function f by setting f(a) to be the number b such 
that the pair (a, b) appears in the sequence coded by some v(x). b is 
uniquely determined because all the v(x) code sequences that associate A 
and B. f is clearly computable. In fact, 
f(a) = mjn(3i)s/(v(Za+l))[(r(v(2a + 1))); =(a, b)]. 
By 5, the range off is N; thus f is a recursive permutation and hence, 
A =B. 
â¢ 
Exercises 
1. Prove that K = U, where U is defined in Exercise 3.1. 
2. 
Prove that 

Part 2 
Grammars and 
Automata 


9 
Regular Languages 
1. Finite Automata 
Computability theory, discussed in Part 1, is the theory of computation 
obtained when limitations of space and time are deliberately ignored. In 
automata theory, which we study in this chapter, computation is studied in 
a context in which bounds on space and time are entirely relevant. The 
point of view of computability theory is exemplified in the behavior of a 
Turing machine (Chapter 6) in which a read-write head moves back and 
forth on an infinite tape, with no preset limit on the number of steps 
required to reach termination.1 At the opposite pole, one can imagine a 
device which moves from left to right on a finite input tape, and it is just 
such devices, the so-called finite automata, that we will now study. Since a 
finite automaton will have only one opportunity to scan each square in its 
motion from left to right, nothing is to be gained by permitting the device 
to "print" new symbols on its tape. 
Unlike modern computers, whose action is controlled in part by an 
internally stored list of instructions called a program, the computing 
1 The present chapter does not depend on familiarity with the material in Chapters 2-8. 
Any exercises that refer to earlier material are marked with an â¢. 
237 

238 
Chapter 9 Regular Languages 
Table 1.1 
6 
a 
b 
q, 
q2 
q4 
q2 
q2 
q3 
q3 
q4 
q3 
q4 
q4 
q4 
devices we will consider in this chapter have no such programs and no 
internal memory for storing either programs or partial results. In addition, 
since, as we just indicated, a finite automaton is permitted only a single 
pass over the tape, there is no external memory available. Instead, there 
are internal states that control the automaton's behavior and also function 
as memory in the sense of being able to retain some information about 
what has been read from the input tape up to a given point. 
Thus, a finite automaton can be thought of as a very limited computing 
device which, after reading a string of symbols on the input tape, either 
accepts the input or rejects it, depending upon the state the machine is in 
when it has finished reading the tape. 
The machine begins by reading the leftmost symbol on the tape, in a 
specified state called the initial state (the automaton is in this state 
whenever it is initially "turned on"). If at a given time, the machine is in a 
state qi reading a given symbol sj on the input tape, the device moves one 
square to the right on the tape and enters a state qk. The current state of 
the automaton plus the symbol on the tape being read completely deter-
mine the automaton's next state. 
Definition. A finite automaton Lon the alphabee A = {s1 , â¢â¢â¢ , sn} with 
states Q = {q1 , â¢â¢â¢ , qm} is given by a function 8 that maps each pair 
(q;, sj), 1 ::; i ::; m, 1 ::; j ::; n, into a state qk, together with a set F ~ Q. 
One of the states, usually q1 , is singled out and called the initial state. The 
states belonging to the set F are called the final or accepting states, 8 is 
called the transition function. 
We can represent the function 8 using a state versus symbol table. An 
example is given in Table 1.1, where the alphabet is {a, b}, F = {q3}, and q1 
2 For an introduction to alphabets and strings, see Chapter 1, Section 3. 

1. Finite Automata 
239 
is the initial state. It is easy to check that for the tapes 
I a I a I h 
h I b I 
I 
h I u I h I a I 
I u I a I 
b I a I 
I a I b I 
h I b I 
the automaton will terminate in states q3 , q4 , q4 , and q3 , respectively. We 
shall say that the automaton accepts the strings aabbb and abbb (because 
q3 E F), while it rejects the strings baba and aaba (because q4 fl. F), i.e., 
that it accepts the first and fourth of the preceding tapes and rejects the 
second and third. 
To proceed more formally, let L be a finite automaton with transition 
function 8, initial state q1 , and accepting states F. If q; is any state of L 
and u E A*, where A is the alphabet of L, we shall write l)*(q;, u) for the 
state which L will enter if it begins in state qi at the left end of the string 
u and moves across u until the entire string has been processed. A formal 
definition by recursion is 
l>*(q;,O) = q;, 
l>*(q;.usj) = t>(t>*(q;,u),sj)Â· 
Obviously, l>*(q;, sj) = l>(q;, s/ Then we say that L accepts a word u 
provided that 8*(q1 , u) E F. L rejects u means that 8*(q1 , u) E Q -F. 
Finally, the language accepted by L, written L(L), is the set of all u E A* 
accepted by L: 
L(L) = {u E A* l8*(q1 , u) E F}. 
A language is called regular if there exists a finite automaton that accepts 
it. 
It is important to realize that the notion of regular language does not 
depend on the particular alphabet. That is, if L s;;; A* and A s;;; B, then 
there is an automaton on the alphabet A that accepts L if and only if 
there is one on the alphabet B that accepts L. That is, an automaton with 
alphabet B can be contracted to one on the alphabet A by simply 
restricting the transition function 8 to A; clearly this will have no effect 

240 
Chapter 9 Regular Languages 
on which elements of A* are accepted. Likewise, an automaton L with 
alphabet A can be expanded to one with alphabet B by introducing a new 
"trap" state q and decreeing 
8(q;, b)= q for all states q; of Land all bE B- A, 
8 ( q, b) = q for all b E B. 
Leaving the set of accepting states unchanged (so that q is not an 
accepting state), we see that the expanded automaton accepts the same 
language as L. 
Returning to the automaton given by Table 1.1 with F = {q3}, it is easy 
to see that the language it accepts is 
{aln]b[m) In, m > 0}. 
(1.1) 
Thus we have shown that (1.1) is a regular language. 
We conclude this section by mentioning another way to represent the 
transition function 8. We can draw a graph in which each state is 
represented by a vertex. Then, the fact that 8(q;, sj) = qk is represented by 
drawing an arrow from vertex q; to vertex qk and labeling it sj. The 
diagram thus obtained is called the state transition diagram for the given 
automaton. The state transition diagram for the transition function of 
Table 1.1 is shown in Fig. 1.1. 
Â· 
Exercises 
1. 
In each of the following examples, an alphabet A and a language L 
are indicated with L ~A*. In each case show that L is regular by 
constructing a finite automaton L that accepts L. 
b 
Figure 1.1 

1. Finite Automata 
241 
(a) A = {1}; L = {ll6kl11 k ~ 0}. 
(b) 
A = {a, b}; L consists of all words whose final four symbols form 
the string bbab. 
(c) 
A = {a, b}; L consists of all words whose final five symbols 
include two a's and three b's. 
(d) A = {0, 1}; L consists of all strings that, when considered as 
binary numbers, have a value which is an integral multiple of 5. 
L is to be a binary addition checker in the sense that it accepts 
strings of binary triples 
such that c1c2 â¢â¢â¢ en is the sum of a1a2 â¢â¢â¢ an and b 1b 2 â¢â¢â¢ bn 
when each is ,treated as a binary number. 
(f) 
A ={a, b, c}. A palindrome is a word such that w = wR. That is, 
it reads the same backward and forward. L consists of all 
palindromes of length less than or equal to 6. 
(g) 
A ={a, b}; L consists of all strings s1s2 â¢â¢â¢ sn such that sn-z =b. 
(Note that L contains no strings of length less than 3.) 
(h) A = {a, b}; L consists of all words in which three a's occur 
consecutively. 
(i) 
A = {a, b}; L consists of all words in which three a's do not 
occur consecutively. 
2. (a) Suppose that the variable names in your favorite programming 
language are words w on the alphabet {A, ... , Z, 0, ... , 9} such 
that 1 :::;; lwl :::;; 8 and such that the first symbol of w belongs to 
{A, ... , Z}. Give a finite automaton that accepts the language 
consisting of these variable names. 
(b) Now, remove the restriction lwl :::;; 8 and give a finite automaton 
that accepts this extended language. 
3. Describe the language accepted by each of the following finite au-
tomata. In each case the initial state is q1 â¢ 

242 
Chapter 9 Regular Languages 
(a) 
81 
a 
b 
c 
q1 
qz 
q3 
q4 
qz 
qz 
q4 
q5 
q3 
q4 
q3 
q5 
q4 
q4 
q4 
q4 
q5 
q4 
q4 
q5 
(b) 82 = 81, F2 = {q4}. 
(c) 
83 
a 
b 
c 
q1 
qz 
qz 
q1 
qz 
q3 
qz 
q1 
q3 
q1 
q3 
qz 
4. 
Let A = {s1 , â¢â¢â¢ , sn}. How many finite automata are there on A with 
exactly m states, m > 0? 
5. Show that there is a regular language that is not accepted by any finite 
automaton with just one accepting state. 
6. 
For any regular language L, define rank(L) = the least number n 
such that L is accepted by some finite automaton with n states. Prove 
that for every n > 0 there is a regular language L with rank(L) = n. 
7. 
Prove or disprove the following: If L 1 , L 2 are regular languages such 
that L 1 ~ L 2 , then rank(L1) :::;; rank(L 2). 
8.* Let L be a finite automaton on the alphabet A = {s1 , â¢â¢â¢ , sn} with 
states Q = {q1 , â¢â¢â¢ , qm}, transition function 8, initial state q1 , and 
accepting states F. Give a Turing machine L' that accepts L(L). 
2. 
Nondeterministic Finite Automata 
Next we modify the definition of a finite automaton to permit transitions at 
each stage to either zero, one, or more than one states. Formally, we 
accomplish this by altering the definition of a finite automaton in the 
previous section by making the values of the transition function 8 be sets 
of states, i.e., sets of elements of Q (rather than members of Q). The devices 

2. Nondeterministic Finite Automata 
243 
Table 2.1 
6 
a 
b 
ql 
{ql, q2} 
{ql, q3} 
q2 
{q4} 
0 
q3 
0 
{q4} 
q4 
{q4} 
{q4} 
so obtained are called nondeterministic finite automata (ndfa), and some-
times ordinary finite automata are then called deterministic finite automata 
(dfa). An ndfa on a given alphabet A with set of states Q is specified by 
giving such a transition function l> [which maps each pair (q;, si) into a 
possibly empty subset of Q] and a fixed subset F of Q. For an ndfa, we 
define 
l) *(q; '0) = {q;}, 
l>*(q;, us) = 
U 
l>(q, si). 
qE li*(qi, u) 
Thus, in calculating l>*(q;, u), one accumulates all states that the automa-
ton can enter when it reaches the right end of u, beginning at the left end 
of u in state q;. An ndfa L 
with initial state q1 accepts u E A* if 
l>*(q1 , u) n F =I= 0, i.e., if at least one of the states at which L ultimately 
arrives belongs to F. Finally, L(L), the language accepted by L, is the set 
of all strings accepted by L. 
An example is given in Table 2.1 and Figure 2.1. Here F = {q4}. It is not 
difficult to see that this ndfa accepts a string on the alphabet {a, b} just in 
case at least one of the symbols has two successive occurrences in the 
string. 
In state q1 , if the next character read is an a, then there are two 
possibilities. It might be that this a is the first of the desired pair of a's. In 
that case we would want to remember that we had found one a and hence 
Figure2.1 

244 
Chapter 9 Regular Languages 
enter state q2 to record that fact. On the other hand, it might be that the 
symbol following this a will be a b. Then this a is of no help in attaining 
the desired goal and hence we would remain in q 1 â¢ Since we are not able 
to look ahead in the string, we cannot at this point determine which role 
the current a is playing and so the automaton "simultaneously" hypothe-
sizes both possibilities. If the next character read is b, then since there is 
no transition from q2 reading b, the choice has been resolved and the 
automaton will be in state q1 â¢ If instead, the character following the first a 
is another a, then since q2 E S(q1 , a) and q4 E S(q2 , a), and on any input 
the automaton once in state q4 remains in q4 , the input string will be 
accepted because q4 is an accepting state. A similar analysis can be made 
if a b is read when the automaton is in state q1 â¢ 
Strictly speaking, a dfa is not just a special kind of ndfa, although it is 
frequently thought of as such. This is because for a dfa, O{q, s) is a state, 
whereas for an ndfa it is a set of states. But it is natural to identify the dfa 
L 
with transition function S, with the closely related ndfa .ii whose 
transition function 8 is given by 
8(q, s) = {S(q, s)}, 
and which has the same final states as L. Obviously L(L) = L(.ii). 
The main theorem on nondeterministic finite automata is 
Theorem 2.1. A language is accepted by an ndfa if and only if it is 
regular. Equivalently, a language is accepted by an ndfa if and only if it is 
accepted by a dfa. 
Proof. As we have just seen, a language accepted by a dfa is also 
accepted by an ndfa. Conversely, let L = L(L), where L is an ndfa with 
transition function S, set of states Q = {q 1 , â¢â¢â¢ , qm}, and set of final states 
F. We will construct a dfa L such that L{L) = L(L) = L. The idea of 
the construction is that the individual states of L will be sets of states 
of L. 
Thus, we proceed to specify the dfa L on the same alphabet as L. The 
states of L are just the zm sets of states (including 0) of L. We write 
these as Q = {Q1 , Q2 , â¢â¢â¢ , Q2m}, where in particular Q 1 = {q1} is to be the 
initial state of L. The set .'7 of final states of L is given by 
.'7= {Q; I Q; n F =1= 0}. 
The transition function 8 of L is then defined by 
8(Q;, s) = U S(q, s). 
qEQi 

2. Nondeterministic Finite Automata 
Now, we have 
Lemma 1. Let R ~ Q. Then 
s( U Q;,s) = U 8(Q;,s). 
Q;ER 
Q;ER 
Proof. Let UQ,e R Q; = Q. Then by definition, 
B(Q, s) = U 8(q, s) 
qEQ 
= U U 8(q,s) 
Q;ER qEQ; 
= U B(Q;,s). 
Q,eR 
Lemma 2. For any string u, 
B*(Q;, u) = U 8*(q, u). 
qEQ; 
Proof. The proof is by induction on lui. If lui = 0, then u = 0 and 
B*(Q;,O)=Q;= u {q}= u 8*(q,O). 
qEQ; 
qeQ, 
245 
â¢ 
If lui = I + 1 and the result is known for lui = I, we write u = us, where 
lui =I, and observe that, using Lemma 1 and the induction hypothesis, 
B*(Q;,u) = B*(Q;,us) 
= 8(8*(Q;,u),s) 
=B{ U 8*(q,u),s) 
qeQ, 
= U B(8*(q,u),s) 
qeQ, 
= u U 8(r,s) 
qeQ, rE6*(q,v) 
= U 8*(q, us) 
qeQ, 
= U 8*(q, u). 
â¢ 
qeQ, 

246 
Chapter 9 Regular Languages 
Lemma 3. L(L) = L(L). 
Proof. 
u E L(L) if and only if B*(Q1, u) E !T. But, by Lemma 2, 
Hence, 
B*(Q1, u) = B*({q1}, u) = 8*(q1, u). 
u E L(L) if and only if 
8*(q1, u) E Y 
if and only if 
8*(q1, u) n F =F 0 
if and only if u E L(L). 
â¢ 
Proof of Theorem 2.1 Concluded. Theorem 2.1 is an immediate conse-
quence of Lemma 3. 
â¢ 
Note that this proof is constructive. Not only have we shown that if a 
language is accepted by some ndfa, it is also accepted by some dfa, but we 
have also provided, within the proof, an algorithm for carrying out the 
conversion. This is important because, although it is frequently easier to 
design an ndfa than a dfa to accept a particular language, actual machines 
that are built are deterministic. 
Exercises 
1. Describe the language accepted by each of the following ndfas. In each 
case the initial state is q1 â¢ 
(a) 
81 
a 
b 
c 
ql 
{ql 'qz' q3} 
0 
0 
qz 
0 
{q4} 
0 
F1 = {q4}. 
q3 
0 
0 
{q4} 
q4 
0 
0 
0 
(b) 
8z = 81, Fz = {ql, qz, q3}. 
(c) 
83 
a 
b 
ql 
{qz} 
0 
F3 = {qz}. 
qz 
0 
{ql 'q3} 
q3 
{ql 'q3} 
0 

3. Additional Examples 
247 
2. For each dfa L in Exercise 1.3, transform L into an ndfa L' which 
accepts L(L). Then transform L' into a dfa L" by way of the 
construction in the proof of Theorem 2.1. 
3. Let L be a dfa with a single accepting state. Consider the ndfa L' 
formed by reversing the roles of the initial and accepting states and 
reversing the direction of the arrows of all transitions in the transition 
diagram. Describe L(L') in terms of L(L). 
4. Prove that, given any ndfa L 1 , there exists an ndfa L 2 with exactly 
one accepting state such that 
5. (a) The construction in the proof of Theorem 2.1 shows that any 
regular language accepted by an ndfa with n states is accepted by 
some dfa with 2 n states. Show that there is a regular language 
that is accepted by an ndfa with two states, not accepted by any 
ndfa with fewer than two states, and accepted by a dfa with two 
states. 
(b) Show that there is a regular language that is accepted by an ndfa 
with two states and not accepted by any dfa with fewer than four 
states. 
(c) 
Show that there is a regular language that is accepted by an ndfa 
with three states and not accepted by any dfa with fewer than 
eight states. 
3. Additional Examples 
We first give two simple examples of finite automata and their associated 
regular languages. 
For our first example we consider a unary even parity checker. That is, 
we want to design a finite automaton over the alphabet {1} such that the 
machine terminates in an accepting state if and only if the input string 
contains an even number of ones. Intuitively then, the machine must 
contain two states which "remember" whether an even or an odd number 
of ones have been encountered so far. When the automaton begins, no 
ones, and hence an even number of ones, have been read; hence the initial 
state q1 will represent the even parity state, and q2 , the odd parity state. 
Furthermore, since we want to accept words containing an even number of 
ones, q 1 will be an accepting state. 

248 
Chapter 9 Regular Languages 
1 
0 
F~{,,) 
1 
Figure 3.1 
Thus the finite automaton to perform the required task is as shown in 
Fig. 3.1, and the language it accepts is 
We next consider a slightly more complicated example. Suppose we wish 
to design a finite automaton that will function as a 25t,Z candy vending 
machine. The alphabet consists of the three symbols n, d, and q (repre-
senting nickel, dime, and quarter, respectively-no pennies, please!). If 
more than 251Z is deposited, no change is returned and no credit is given 
for the overage. Intuitively, the states keep track of the amount of money 
deposited so far. The automaton is exhibited in Fig. 3.2, with each state 
labeled to indicate its role. The state labeled 0 is the initial state. Note that 
the state labeled d is a "dead" state; i.e., once that state is entered it may 
never be left. Whenever sufficient money has been inserted so that the 
automaton has entered the 25t,Z (accepting) state, any additional coins will 
send the machine into this dead state, which may be thought of as a coin 
return state. Presumably when in the accepting state, a button can be 
pressed to select your candy and the machine is reset to 0. 
Unlike the previous example, the language accepted by this finite 
automaton is a finite set. It consists of the following combinations of 
nickels, dimes, and quarters: {nnnnn, nnnnd, nnnnq, nnnd, nnnq, nndn, 
nndd,nndq, nnq,ndnn, ndnd, ndnq,ndd, ndq,nq,dnnn,dnnd,dnnq,dnd, 
dnq, ddn, ddd, ddq, dq, q}. 
q 
d 
q 
F = {25) 
Figure3.2 

4. Closure Properties 
249 
a 
a 
b 
Figure3.3 
Suppose we wish to design an automaton on the alphabet {a, b} that 
accepts all and only strings which end in bab or aaba. A real-world analog 
of this problem might arise in a demographic study in which people of 
certain ethnic groups are to be identified by checking to see if their family 
name ends in certain strings of letters. 
It is easy to design the desired ndfa: see Fig. 3.3. 
As our final example, we discuss a slightly more complicated version of 
the first example considered in Section 1: 
An ndfa L such that L(L) = L is shown in Fig. 3.4. 
These two examples of ndfas illustrate an important characteristic of 
such machines: not only is it permissible to have many alternative transi-
tions for a given state-symbol pair, but frequently there are no transitions 
for a given pair. In a sense, this means that whereas for a dfa one has to 
describe what happens for any string whether or not that string is a word 
in the language, for an ndfa one need only describe the behavior of the 
automaton for words in the language. 
a 
b 
Figure3.4 
4. 
Closure Properties 
We will be able to prove that the class of regular languages is closed under 
a large number of operations. It will be helpful that, by the equivalence 

250 
Chapter 9 Regular Languages 
theorems of the previous two sections, we can use deterministic or nonde-
terministic finite automata to suit our convenience. 
Definition. A dfa is called nonrestarting if there is no pair q, s for which 
8(q,s) =q1 , 
where q 1 is the initial state. 
Theorem 4.1. There is an algorithm that will transform a given dfa L 
into a nonrestarting dfa L such that L(L) = L(L). 
Proof. 
Let Q = {q1 , q2 , â¢â¢â¢ , qn} be the set of states of L, q1 the initial 
state, F the set of accepting states, and 8 the transition function. We 
construct L with the set of states Q = Q U {qn+ 1}, initial state q1 , and 
transition function 8 defined by 
_ 
(8(q,s) 
8(q, s) = 
qn+l 
8(qn+ I> s) = 8(q1 , s). 
if q E Q and 8(q, s) =/= q1 
if q E Q and 8(q, s) = q1 , 
Thus, there is no transition into state q1 for L. The set of accepting states 
F of L is defined by 
F-- (F 
-
F U {qn+l} 
To see that L(L) = L(L) as required, one need only observe that L 
follows the same transitions as L except that whenever L reenters q1 , L 
enters qn+IÂ· 
â¢ 
Theorem 4.2. If L and L are regular languages, then so is L u L. 
Proof. Without loss of generality, by Theorem 4.1, let L, L be non-
restarting dfas that accept L and L, respectively, with Q, q1 , F, 8 and 
Q, ij1 , F, 8 the set of states, initial state, set of accepting states, and 
transition function of L and L, respectively. We also assume that L and 
L have no states in common, i.e., Q n Q = 0. Furthermore, by the 
discussion in Section 1, we can assume that the alphabets of L and L are 
the same, say, A. We define the ndfa L with states Q, initial state q1 , set 
of accepting states F, and transition function 8 as follows: 

4. Closure Properties 
251 
(That is, L contains a new initial state q1 and all states of L and L 
except their initial states.) 
F = f F u ~ u {ql} - {ql 'ql} 
\FuF 
if 
q1 E F or q1 E F 
otherwise. 
The transition function of L is defined as follows for s E A: 
v 
_ 
( {8(q, s)} 
8(q, s)-
{ B(q, s)} 
if q E Q- {q1} 
if q E Q- {q1} 
S(q1,s) = {8(q1 ,s)} U {8(q1 ,s)}. 
Thus, since Q n Q = 0 and L and L are nonrestarting, once a first 
transition has been selected, the automaton L is locked into one of the 
two automata Land L. Hence L(L) = L u L. 
â¢ 
Theorem 4.3. Let L ~ A* be a regular language. Then A* - L is 
regular. 
Proof. Let L be a dfa that accepts L. Let L have alphabet A, set of 
states Q, and set of accepting states F. Let L be exactly like L except 
that it accepts precisely when L rejects. That is, the set of accepting states 
of .ii is Q -F. Then .ii clearly accepts A* - L. 
â¢ 
Theorem 4.4. If L 1 and L 2 are regular languages, then so is L 1 n L 2 â¢ 
Proof. Let L 1 , L 2 ~A*. Then we have the De Morgan identity: 
L 1 n L 2 =A* - ((A* - L 1) U (A* - L 2 )). 
Theorems 4.2 and 4.3 then give the result. 
Theorem 4.5. 0 and {0} are regular languages. 
â¢ 
Proof. 0 is clearly the language accepted by any automaton whose set of 
accepting states is empty. Next, the automaton with states q1 , q2 , alphabet 
{a}, accepting states F = {q1}, and transition function 8(q1 , a) = 8(q2 , a) 
= q2 clearly accepts {0}, as does any nonrestarting dfa on any alphabet 
provided F = {q1}. 
â¢ 
Theorem 4.6. Let u E A*. Then {u} is a regular language. 
Proof. For u = 0, we already know this from Theorem 4.5. Otherwise let 
u = a1a2 ... a1a1+ t> where a1 , a2 , ... , a1, a1+ 1 EA. Let L 
be the ndfa 

252 
Chapter 9 Regular Languages 
with states qpq2 , â¢â¢â¢ ,q1+ 2 , initial state q1 , accepting state q1+ 2 , and 
transition function 8 given by 
8(q;, a;) = {qi+ 1}, 
8(q;,a)=0 
Then L(L) = {u}. 
i=1, ... ,/+1, 
for 
a E A - {a;}. 
Corollary 4.7. Every finite subset of A* is regular. 
â¢ 
Proof. We have already seen that 0 is regular. If L = {u 1 , â¢â¢â¢ , un}, where 
u 1 , â¢â¢â¢ , un E A*, we note that 
L = {u 1} U {u 2} U Â·Â·Â· U {un}, 
and apply Theorems 4.2 and 4.6. 
Exercises 
â¢ 
1. 
Let A = {a, b}, let L 1 ~A* consist of all words with at least two 
occurrences of a, and let L 2 ~A* consist of all words with at least two 
occurrences of b. For each of the following languages L, give an ndfa 
that accepts L. 
(a) 
L = L 1 U L 2 â¢ 
(b) 
L =A* - L 1 â¢ 
(c) 
L =A* - L 2 â¢ 
(d) L = L 1 n L 2 â¢ 
2. Use the constructions in the proofs of Theorem 4.6 and Corollary 4.7 
to give an ndfa that accepts the language {ab, ac, ad}. 
3. (a) Let L, L' be regular languages. Prove that L - L' is regular. 
(b) Let L, L' be languages such that L is regular, L u L' is regular, 
and L n L' = 0. Prove that L' is regular. 
4. Let L 1 , L 2 be regular languages with rank(L 1) = n 1 and rank(L 2 ) = 
n 2 â¢ [See Exercise 1.6 for the definition of rank.] 
{a) Use Theorems 4.1, 4.2, and 2.1 to give an upper bound on 
rank(L 1 U L 2 ). 
(b) Use Theorems 4.1, 4.2, 4.3, 4.4, and 2.1 to give an upper bound on 
rank(L 1 n L 2 ). 
5.* Let A 1 , A 2 be alphabets, and let f be a function from A1 to subsets 
of A~. f is a substitution on A 1 if f(O) = {0} and, for all nonnull words 

5. Kleene's Theorem 
253 
a1 â¢â¢â¢ an EAr, where a1 , â¢â¢â¢ , an E A 1 , f(a 1 â¢â¢â¢ an) = f(a 1) â¢â¢â¢ f(an) = 
{u1, ... , unlu; E f(a;), 1 :::;; i:::;; n}. For L ~At, f(L) = Uwe L f(w). 
(a) Let A 1 ={a, b}, A2 = {c, d, e}, let f be the substitution on A 1 
such that f(a) = {cc,O} and f(b) ={wE A~ I w ends in e}, and 
let L = {a[mlb[nJI m, n ~ 0}. What is f(L)? 
(b) Let A 1 , A 2 be alphabets, let f be a substitution on A 1 such that 
f(a) ~A~ is a regular language for all a E A 1 , and let L be a 
regular language on A 1 â¢ Prove that f(L) is a regular language on 
Az. 
(c) 
Let A1 , A2 be alphabets, and let g be a function from Ar to 
A~. g is a homomorphism on A 1 if g(O) = 0 and, for all nonnull 
words al ... an EAr' where al' ... ' an E AI' g(al ... an) = 
g(a 1)Â·Â·Â· g(an). For L ~Ar, g(L) = {g(w)l wE L}. Use (b) to 
show that if g is a homomorphism on A 1 and L ~ Aj is regular, 
then g(L) is regular. 
5. 
Kleene's Theorem 
In this section we will see how the class of regular languages can be 
characterized as the class of all languages obtained from finite languages 
using a few operations. 
Definition. Let L 1 , L 2 ~A*. Then, we write 
Definition. Let L ~A*. Then we write 
With respect to this last definition, note that 
1. 0 E L * automatically because n = 0 is allowed; 
2. for A* the present notation is consistent with what we have been 
using. 
Theorem 5.1. If L, L are regular languages, then L Â· L is a regular 
language. 
Proof. Let L and L be dfas that accept L and L, respectively, with 
Q, q1 , F, l> and Q, ij 1 , F, 8 the set of states, initial state, set of accepting 
states, and transition function, respectively. Assume that L and L have 

254 
Chapter 9 Regular Languages 
no states in common, i.e., Q n Q = 0. By our discussion in Section 1, we 
can assume without loss of generality that the alphabets of L and L are 
the same. Consider the ndfa L formed by "gluing together" ./1 and L in 
the following way. The set Q of states of Lis Q u Q, and the initial state 
is q1 â¢ We will define the transition function 8 of L in such a way that the 
transitions of L will contain all transitions of ./1 and L. In addition 
8(q, s) will contain B(q1 , s) for every q E F. Thus, any time a symbol of 
the input string causes ./1 to enter an accepting state, L can either 
continue by treating the next symbol of the input as being from the word 
of L or as the first symbol of the word of L. Formally we define 8 as 
follows: 
{
{8(q, s)} 
for q E Q- F 
8(q, s) = 
{8(q, s)} U { B(ij1 , s)} 
for 
q E F 
{ B(q, s)} 
for 
q E Q. 
Thus, L begins by behaving exactly like ./1. However, just when ./1 has 
accepted a word and would make a transition from an accepting state, L 
may proceed as if it were L making a transition from ij1 â¢ 
Finally, if 0 E L we set F = F u F, and if 0 ~ L we set F =F. Clearly, 
L Â· L = L(L), so that L Â· L is a regular language. 
â¢ 
Theorem 5.2. If L is a regular language, then so is L *. 
Proof. 
Let ./1 be a nonrestarting dfa that accepts L with alphabet A, set 
of states Q, initial state q1 , accepting states F, and transition function 8. 
We construct the ndfa L with the same states and initial state as ./1, 
and accepting state q1 â¢ The transition function 8 is defined as follows: 
-
({S(q,s)} 
8(q, s) = 
{8(q, s)} u {qt} 
if 
8(q, s) ~ F 
if 
8(q, s) E F. 
That is, whenever ./1 would enter an accepting state, L will enter either 
the corresponding accepting state or the initial state. Clearly L * = L(L), 
so that L * is a regular language. 
â¢ 
Theorem 5.3 (K.leene's Theorem). A language is regular if and only if it 
can be obtained from finite languages by applying the three operators 
u, Â·, * a finite number of times. 
The characterization of regular languages that Kleene's theorem gives 
resembles the definition of the primitive recursive functions and the 
characterization of the partially computable functions of Theorem 3.5 in 

5. Kleene's Theorem 
255 
Chapter 4. In each case one begins with some initial objects and applies 
certain operations a finite number of times. 
Proof. Every finite language is regular by Corollary 4.7, and if L = 
L 1 U L 2 or L = L 1 â¢ L 2 or L = Lj, where L 1 and L 2 are regular, then L 
is regular by Theorems 4.2, 5.1, and 5.2, respectively. Therefore, by 
induction on the number of applications of u , Â·, and *, any language 
obtained from finite languages by applying these operators a finite number 
of times is regular. 
On the other hand, let L be a regular language, L = L(L), where L is 
a dfa with states q1 , â¢â¢â¢ , qn. As usual, q1 is the initial state, F is the set of 
accepting states, 8 is the transition function, and A = {s1 , â¢â¢â¢ , sK} is the 
alphabet. We define the sets RL, i, j > 0, k ~ 0, as follows: 
R~.j = {x E A* l8*(q;, x) = qj and L passes through no state 
q1 with I > k as it moves across x}. 
More formally, RL is the set of words x = s;1S; 2 â¢â¢â¢ s;,s;,+ 1 such that we 
can write 
8(qi' S;l) = qjl' 
8(qjl 's;) = qh' 
8(qj,_ I 's;) = qj,' 
8(qj,,si,+l) = qj, 
where j 1 , j 2 , â¢â¢â¢ , j, ::; k. Now, we observe that 
since for a word of length 1, L passes directly from state qi into state qj 
while in processing any word of length > 1, L 
will pass through some 
intermediate state q1, I~ 1. Thus R?,j is a finite set. Furthermore, we have 
R~,j 1 = R~.j u [ R~.k+l. CRLI,k+l)* Â·RLI,j]â¢ 
(5.1) 
This rather imposing formula really states something quite simple: The set 
Rtj 1 contains all the elements of RL and in addition contains strings x, 
such that L in scanning x passes through the state qk+ 1 (but through 
none with larger subscript) some finite number of times. Such a string can 
be decomposed into a left end, which L enters in state qi and leaves in 

256 
Chapter 9 Regular Languages 
state qk+ 1 (passing only through states with subscripts less than k + 1 in 
the process), followed by some finite number of pieces each of which L 
enters and leaves in state qk+ 1 (passing only through q1 with I ~ k), 
and a right end which L enters in state qk+ 1 and leaves in state qi 
(again passing only through states with subscript 
~ k in between). Now 
we have 
Lemma. Each RL can be obtained from finite languages by a finite 
number of applications of the operations u, Â·, *. 
Proof. 
We prove by induction on k that for all i, j, the set RL has the 
desired property. For k = 0 this is obvious, since RL is finite. 
Assuming the result known fork, (5.1) yields the result fork+ 1. 
â¢ 
Proof of Kleene's Theorem Concluded. We note that 
thus, the result follows at once from the lemma. 
â¢ 
Kleene's theorem makes it possible to give names to regular languages 
in a particularly simple way. Let us begin with an alphabet A = 
{s1 , s2 , â¢â¢â¢ , sk}. Then we define the corresponding alphabet: 
A= {s 1 ,s2 , â¢â¢â¢ ,sk,0,0, U, Â· ,*,(,)}. 
The class of regular expressions on the alphabet A is then defined to be the 
subset of A* determined by the following: 
1. 0, 0, s 1 , â¢â¢â¢ , s k are regular expressions. 
2. If a and {3 are regular expressions, then so is (a U {3 ). 
3. If a and {3 are regular expressions, then so is (a Â· {3 ). 
4. If a is a regular expression, then so is a*. 
5. No expression is regular unless it can be generated using a finite 
number of applications of 1-4. 
Here are a few examples of regular expressions on the alphabet A = 
{a, b, c}: 
(a Â· (h* u c*)) 
(0 U (a Â·h)*) 
(c* Â· h*). 

5. Kleene's Theorem 
257 
For each regular expression y, we define a corresponding regular 
language ( y) by recursion according to the following "semantic" rules: 3 
(s;) = {s;}, 
(0) = {0}, 
(0) = 0, 
((aU /3)) =(a) U ( {3), 
((aÂ· {3)) =(a)Â· ( {3), 
(a*)= (a)*. 
When ( y) = L, we say that the regular expression y represents L. Thus, 
We have 
((aÂ· (b* U c*))) = {ablnll n;;;:: 0} U {aclmll m;;;:: 0}, 
((O u (a Â·b)*)) = ((a Â·b)*) = {(ab)1n11 n ;;;:: 0}, 
((c* Â· b*)) = {clmlb[nJim,n;;;:: 0}. 
Theorem 5.4. For every finite subset L of A*, there is a regular expres-
sion y on A such that ( y) = L. 
Proof. If L = 0, then L = (0). If L = {0}, then L = (0). If L = {x}, 
where x = S; 1 S; 2 â¢ â¢ â¢ s;,. then 
L = ((s. Â· (s. Â· (s. Â·Â·Â· s.) Â·Â·Â· ))). 
'J 
'2 
'J 
lJ 
This gives the result for languages L consisting of 0 or 1 element. 
Assuming the result known for languages of k elements, let L have k + 1 
elements. Then we can write 
L = L 1 U {x}, 
where x E A* and L 1 contains k elements. By the induction hypothesis, 
there is a regular expression a such that (a) = L 1 â¢ By the one-element 
case already considered, there is a regular expression {3 such that ( {3 ) = 
{x}. Then we have 
( (aU {3)) = (a) U ( {3) = L 1 U {x} = L. 
â¢ 
3 For more on this subject see Part 5. 

258 
Chapter 9 Regular Languages 
Theorem 5.5 (K.leene's Theorem-Second Version). A language L ~A* 
is regular if and only if there is a regular expression y on A such that 
(y)=L. 
Proof. 
For any regular expression y, the regular language ( y) is built up 
from finite languages by applying u, Â·, * a finite number of times, so ( y) 
is regular by Kleene's theorem. 
On the other hand, let L be a regular language. If L is finite then, by 
Theorem 5.4, there is a regular expression y such that ( y) = L. Other-
wise, by Kleene's theorem, L can be obtained from certain finite language 
by a finite number of applications of the operations u, Â· , *. By beginning 
with regular expressions representing these finite languages, we can build 
up a regular expression representing L by simply indicating each use of 
the operations u, Â·, * by writing U, Â·, *, respectively, and punctuating 
with (and). 
â¢ 
Exercises 
1. (a) For each language L described in Exercise 1.1, give a regular 
expression a such that L = ( a ) . 
(b) For each dfa L described in Exercise 1.3, give a regular expres-
sion a such that L(L) = (a). 
(c) 
For each ndfa L 
described in Exercise 2.1, give a regular 
expression a such that L(L) = (a). 
2. For regular expressions a, {3, let us write a = {3 to mean that 
(a) = ( {3 ). For a, {3, y given regular expressions, prove the follow-
ing identities. 
(a) (a U a) = a. 
(b) ((aÂ· (3) U (aÂ· y)) =(aÂ· ( {3 U y)). 
(c) (( {3 â¢ a) U ( y Â· a)) = (( {3 U y) Â· a). 
(d) (a*Â· a*)= a*. 
(e) (aÂ· a*)= (a*Â· a). 
(f) 
a** =a*. 
(g) (O U (aÂ· a*))= a*. 
(h) ((aÂ· (3)* Â·a)= (aÂ· ( {3 Â·a)*). 
(i) 
( a U {3 )* = ( a * Â· {3 * )* = ( a * U {3 * )*. 
3. Using the identities of Exercise 2 prove that 
((abb)*(ba)*(b Uaa)) = (abb)*((O U (b(ab)*a))b U (ba)*(aa)). 

5. Kleene's Theorem 
259 
(Note that parentheses and the symbol "Â·" have been omitted to 
facilitate reading.) 
4. Let a, {3 be given regular expressions such that 0 f/:. ( a ) . Consider 
the equation in the "unknown" regular expression ~: 
~ = (/3 u a. a)). 
Prove that this equation has the solution 
~=({3Â·a*) 
and that the solution is unique in the sense that if ~ 1 also satisfies the 
equation, then ~ = ~ 1 â¢ 
5. Let L = {x E {a, b}* I x =/= 0 and bb is not a substring of x}. 
(a) Show that L is regular by constructing a dfa L 
such that 
L = L(L). 
(b) Find a regular expression y such that L = ( y ). 
6. Let L = (((a Â· a) u (a Â· a Â· a))*). Find a dfa L that accepts L. 
7. Describe an algorithm that, given any regular expression a, produces 
an ndfa L that accepts ( a ) . 
8. Let L 1 , L 2 be regular languages with rank(L 1) = n 1 and rank(L 2) = 
n 2 â¢ [See Exercise 1.6 for the definition of rank.] 
(a) Use Theorem 5.1 to give an upper bound on rank(L 1 â¢ L 2 ). 
(b) Use Theorem 5.2 to give an upper bound on rank(L~ ). 
9. LetA={s1 , â¢â¢â¢ ,sn}. 
(a) Give a function b 1 such that rank(( a ) ) :::;; b 1( a) for all regular 
expressions a on A. 
(b) Define the size of a regular expression on A as follows. 
size(0) 
= 1 
size(O) 
= 1 
size(s;} 
= 1 i = 1, ... , n 
size(( a U {3 )) =size( a) + size( {3) + 1 
size(( a Â· {3 )) =size( a) + size( {3) + 1 
size(a*) 
=size(a) + 1 
Give a numeric function b2 such that rank(( a)) :::;; b2(size( a)) 
for all regular expressions a on A. 
(c)* Verify that b2 is primitive recursive. 
10.* Let A = {s1 , â¢â¢â¢ , sn}, let a, {3 be regular expressions on A, and let 
Pa, Pf3 be primitive recursive predicates such that for all w E A*, 

260 
Chapter 9 Regular Languages 
Pa(w) = 1 if and only if wE (a) and P/w) = 1 if and only if 
WE ( {3 ). 
(a) Give a primitive recursive predicate 
P< au 13 > such that 
P( a u 13 >( w) = 1 if and only if w E ( ( a U {3 ) ) . 
(b) Give a primitive recursive predicate P<aÂ·fJ> such that P(aÂ·fJ>(w) 
= 1 if and only if w E ( ( a Â· {3 ) ) . 
(c) 
Give a primitive recursive predicate Pa* such that Pa.(w) = 1 if 
and only if w E (a*). 
{d) Use parts (a), (b), and (c) to show that for all regular expressions 
y on A, there is a primitive recursive predicate P-y such that 
P/ w) = 1 if and only if w E ( y). 
6. 
The Pumping Lemma and Its Applications 
We will make use of the following basic combinatorial fact: 
Pigeon-Hole Principle. If (n + 1) objects are distributed among n 
sets, then at least one of the sets must contain at least two objects. 
We will use this pigeon-hole principle to prove the following result. 
Theorem 6.1 (Pumping Lemma). 
Let L = L(L), where L is a dfa with 
n states. Let x E L, where lxl ~ n. Then we can write x = uvw, where 
v =1= 0 and uvlilw E L for all i = 0, 1, 2, 3, .... 
Proof. Since x consists of at least n symbols, L must go through at least 
n state transitions as it scans x. Including the initial state, this requires at 
least n + 1 (not necessarily distinct) states. But since there are only n 
states in all, we conclude (here is the pigeon-hole principle!) that L must 
be in at least one state more than once. Let q be a state in which L finds 
itself at least twice. Then we can write x = uvw, where 
l>*(ql 'u) = q, 
l>*(q,v)=q, 
l>*(q, w) E F. 
That is, L arrives in state q for the first time after scanning the last 
(right-hand) symbol of u and then again after scanning the last symbol of 
v. Since this "loop" can be repeated any number of times, it is clear that 
8*(q 1 , uvlilw) = 8*(q 1 , uvw) E F. 
Hence uvlilw E L. 
â¢ 

6. The Pumping Lemma and Its Applications 
261 
Theorem 6.2. Let L be a dfa with n states. Then, if L(L) -=1= 0, there is 
a string x E L(L) such that lxl < n. 
Proof. 
Let x be a string in L(L) of the shortest possible length. Suppose 
lxl;;:::: n. By the pumping lemma, x = uvw, where v -=1= 0 and uw E L(L). 
Since luwl < lxl, this is a contradiction. Thus lxl < n. 
â¢ 
This theorem furnishes an algorithm for testing a given dfa L to see 
whether the language it accepts is empty. We need only "run" L on all 
strings of length less than the number of states of L. If none is accepted, 
we will be able to conclude that L(L) = 0. 
Next we turn to infinite regular languages. If L = L(L) is infinite, then 
L must surely contain words having length greater than the number of 
states of L. Hence from the pumping lemma, we can conclude 
Theorem 6.3. If L is an infinite regular language, then there are words 
u,v, w, such that v -=1= 0 and uvlilw E L for i = 0, 1, 2, 3, .... 
This theorem is useful in showing that certain languages are not regular. 
However, for infinite regular languages we can say even more. 
Theorem 6.4. Let L be a dfa with n states. Then L(L) is infinite if and 
only if L(L) contains a string x such that n ~ lxl < 2n. 
Proof. 
First let x E L(L) with n ~ lxl < 2n. By the pumping lemma, we 
can write x = uvw, where v -=1= 0 and uvlilw E L(L) for all i. But then 
L(L) is infinite. 
Conversely, let L(L) be infinite. Then L(L) must contain strings of 
length 
;;:::: 2n. Let x E L(L), where x has the shortest possible length 
;;:::: 2n. We write x = x 1x 2 , where lx11 = n. Thus lx21 
:2::: n. Then using the 
pigeon-hole principle as in the proof of the pumping lemma, we can write 
x 1 = uvw, where 
l>*(ql 'u) = q, 
l>*(q, v) = q 
with 
1 ~ lvl ~ n, 
l>*(q, wx2 ) E F. 
Thus uwx2 E L(L). But 
luwx21 
:2::: lx21 :2::: n, 
and luwx21 < lxl, and since x was a shortest word of L(L) with length at 
least 2n, we have 
n ~ luwx21 < 2n. 
â¢ 

262 
Chapter 9 Regular Languages 
This theorem furnishes an algorithm for testing a given dfa L 
to 
determine whether L(L) is finite. We need only run Lon all strings x 
such that n :::;; lxl < 2n, where L has n states. L(L) is infinite just in 
case L accepts at least one of these strings. 
For another example of an algorithm, let L 1 , L 2 be dfas on the 
alphabet A and let us seek to determine whether L(L1) ~ L(L2 ). Using 
the methods of proof of Theorems 4.2-4.4, we can obtain a dfa L such 
that 
L(L) = L(L1) n [A* - L(L2 )]. 
Then L(L1) ~ L(L2 ) if and only if L(L) = 0. Since Theorem 6.2 
enables us to test algorithmically whether L(L) = 0, we have an algo-
rithm by means of which we can determine whether L(L1) ~ L(L2). 
Moreover, since L(L1) = L(L2 ) just when L(L1) ~ L(L2 ) and L(L2 ) ~ 
L(L1), we also have an algorithm for testing whether L(L1) = L(L2). 
The pumping lemma also furnishes a technique for showing that given 
languages are not regular. For example, let L = {a[n1b[n11 n > 0}, and 
suppose that L = L(L), where L 
is a dfa with m states. We get a 
contradiction by showing that there is a word x E L, with lxl ~ m, such 
that there is no way of writing x = uvw, with v =I= 0, so that {uv[i1w I i ~ 0} 
~ L. Let x = a[11b[11, where 2/ ~ m, and let a[11b[11 = uvw. Then either 
v = al1d or v = a[1db[121 or v = b[121, with /1,/2 :::;; I, and in each case 
uvvw rt L, contradicting the pumping lemma, so there can be no such dfa 
L, and L is not regular. 
This example and the exercises at the end of Section 7 show that finite 
automata are incapable of doing more than a limited amount of counting. 
Exercises 
1. Given a word w and a dfa L, a test to determine if w E L(L) is a 
membership test. 
(a) Let L 1 , L 2 be arbitrary dfas on alphabet A = {s1 , â¢â¢â¢ , sn}, where 
L 1 has m 1 states and L 2 has m2 states. Give an upper bound 
f(m 1 , m 2 ) on the number of membership tests necessary to 
determine if L(L1) = L(L2). 
(b)* Verify that f is primitive recursive. 
2. (a) Describe an algorithm that, for any regular expressions a and {3, 
determines if ( a ) = ( {3 ) . 
(b) Give a function g(x, y) such that the algorithm in part (a) 
requires at most g(size( a), size( {3 )) membership tests. [See Exer-
cise 5.9 for the definition of size( a).] 
(c)* Verify that g is primitive recursive. 

7. The Myhill- Nerode Theorem 
263 
7. The Myhill- Nerode Theorem 
We conclude this chapter by giving another characterization of the regular 
languages on an alphabet A. We begin with a pair of definitions. 
Definition. Let L ~A*, where A is an alphabet. For strings x,y EA*, 
we write x =L y to mean that for every w E A* we have xw E L if and 
only if yw E L. 
It is obvious that = 
L has the following properties. 
X =LX. 
If X ==. L y, then y ==. L X. 
If X ==. L y and y ==. L Z, then X ==. L Z. 
(Relations having these three properties are known as equivalence rela-
tions.) 
It is also obvious that 
If x =L y, then for all wE A*, xw =L yw. 
Definition. Let L ~A*, where A is an alphabet. Let S ~A*. Then S is 
called a spanning set for L if 
1. S is finite, and 
2. for every x E A*, there is a y E S such that x =L y. 
Then we can prove 
Theorem 7.1 (Myhili-Nerode). A language is regular if and only if it has 
a spanning set. 
Proof. 
First let L be regular. Then L = L(L), where L is a dfa with set 
of states Q, initial state q1 , and transition function 8. Let us call a state 
q E Q reachable if there exists y E A* such that 
8*(ql,y)=q. 
(7.1) 
For each reachable state q, we select one particular string y that satisfies 
(7.1) and we write it as Yq. Thus, 
8*(q1, yq) = q 
for every reachable state q. We set 
S = {Yq I q is reachable}. 
S is clearly finite. To show that S is a spanning set for L, we let x E A* 
and show how to find y E S such that x =L y. In fact, let 8*(q1, x) = q, 

264 
Chapter 9 Regular Languages 
and set y = Yq. Thus, y E S and l>*(q 1 , y) = q. Now for every w E A*, 
l>*(q 1 ,xw) = l>*(q,w) = l>*(q1 ,yw). 
Hence, l>*(q1 , xw) E F if and only if l>*(q 1 , yw) E F; i.e., xw E L if and 
only if yw E L. Thus, x =L y. 
Conversely, let L ~A* and let S ~A* be a spanning set for L. We 
show how to construct a dfa ./1 such that L(./1) = L. We define the set of 
states of ./1 to be Q = {qx I x E S}, where we have associated a state qx 
with each element x E S. Since S is a spanning set for L, there is an 
x 0 E S such that 0 =L x 0 ; we take qxo to be the initial state of ./1. We let 
the final states of ./1 be 
F= {qyly EL}. 
Finally, for a E A, we set l>(qx, a)= qy, where yES and xa =L y. Then 
we claim that for all w E A*, 
where 
xw =L y. 
We prove this claim by induction on lwl. For lwl = 0, we have w = 0. 
Moreover, l>*(qx,O) = qx and xO = x =L x. Suppose our claim is known 
for all words w such that lwl = k, and consider w E A* with lwl = k + 1. 
Then w = ua, where lui = k and a EA. We have 
l>*(qx, w) = l>(l>*(qx, u), a) = l>(qy, a) = qz, 
where, using the induction hypothesis, xu =L y and, by definition of l>, 
ya =L z. Then xw = xua =L ya =L z, which proves the claim. Now, we 
have 
L(./1) = {wE A* ll>*(qx 11 , w) E F}. 
Let l>*(qxoâ¢ w) = qy. Then by the way x0 was defined and our claim, 
W ::L XoW ::L y. 
Thus, w E L if and only if y E L, which in turn is true if and only if 
qY E F, i.e., if and only if w E L(./1). Hence L = L(./1). 
â¢ 
Like the pumping lemma, the Myhill-Nerode theorem furnishes a 
technique for showing that a given language is not regular. For example, 
let L = {alnlblnJ I n > 0} again, and let n 1 , n 2 be distinct numbers > 0. 
Then alnJib(nJI ELand aln 21blnJI $. L, so alnJI ;f:.L aln 21, and since =L is an 
equivalence relation, there can be no word w such that alnd =L w and 
aln 2J =L w. But if there were a spanning set S = {w 1 , â¢â¢â¢ , wm} for L, then 
by the pigeon-hole principle, there would have to be at least two distinct 

7. The Myhill- Nerode Theorem 
265 
words among {a, aa, ... , alm + 11}, say alii and alii, and some wk E S such 
that alii =L wk and alii =L wk, which is impossible. Therefore L has no 
spanning set, and by the Myhill-Nerode theorem, L is not regular. 
Exercises 
1. (a) For each language L described in Exercise 1.1, give a spanning 
set for L. 
(b) For each dfa L described in Exercise 1.3, give a spanning set 
for L(L). 
(c) 
For each ndfa L described in Exercise 2.1, give a spanning set 
for L(L). 
2. Prove that there is no dfa that accepts exactly the set of all words that 
are palindromes over a given alphabet containing at least two sym-
bols. (For a definition of palindrome, see Exercise l.lf.) 
3. u is called an initial segment of a word w if there is a word v such 
that w = uv. Let L be a regular language. Prove that the language 
consisting of all initial segments of words of L is a regular language. 
4. Let L be a regular language and L' the language consisting of all 
words w such that both wand w Â·ware words in L. Prove that L' is 
regular. 
5. Prove the following statement, if it is true, or give a counterexample: 
Every language that is a subset of a regular language is regular. 
6. Prove that each of the following is not a regular language. 
(a) The language on the alphabet {a, b} consisting of all strings in 
which the number of occurrences of b is greater than the 
number of occurrences of a. 
(b) The language L over the alphabet {., 0, 1, ... , 9}, consisting of all 
strings that are initial segments of the infinite decimal expansion 
of 7T. [L = {3, 3., 3.1, 3.14, 3.141, 3.1415, ... }.] 
(c) 
The language L over the alphabet {a, b} consisting of all strings 
that are initial segments of the infinite string 
babaabaaabaaaab ... 
7. Let L = {alilbUI I i =I= j}. Show that L is not regular. 
8. 
Let L = {alnlbl2nl In > 0}. Show that L is not regular. 
9. 
Let L = {alnlblmJ I 0 < n ~ m}. Show that L is not regular. 
10. Let L = {alPI I p is a prime number}. Show that L is not regular. 

266 
Chapter 9 Regular Languages 
11. Let L be a finite automaton with alphabet A, set of states Q = 
{q1 , â¢â¢â¢ , qn}, initial state q 1 , and transition function 
8. Let 
a 1 , a2 , a3, â¢â¢â¢ be an infinite sequence of symbols of A. We can think 
of these symbols as being "fed" to L in the given order producing a 
sequence of states r1 , r2 , r3 , â¢â¢â¢ , where r1 is just the initial state q1 
and r;+ 1 = 8(r;, a), i = 1, 2, 3, .... Suppose there are integers p, k 
such that 
for all 
i ~ k. 
Prove that there are integers /, s such that s :::;; np and 
ri+s = r; 
for all 
i ~I. 
[Hint: Use the pigeon-hole principle.] 
12. (a) Let L be a regular language, and let S be a spanning set for L. 
S is a minimal spanning set for L if there is no spanning set for 
L that has fewer elements than S, and S is independent if there 
is no pairs, S 1 of distinct elements of S such that s =L S 1 â¢ Prove 
that S is minimal if and only if it is independent. 
(b) Let L be a regular language, and let S, S 1 be spanning sets for 
L. S and S 1 are isomorphic if there is a one-one function f 
from S onto S 1 such that s =L f(s) for all s E S. Prove that if S 
and S 1 are both minimal, then they are isomorphic. 
(c) A dfa L is a minimal dfa for a regular language L if L = L(L) 
and if there is no dfa .4' 1 with fewer states than L such that 
L(L1) = L(L). Let L be a regular language, let L be a dfa 
that accepts L, and let S be a spanning set for L constructed 
from L as in the proof of Theorem 7.1. Prove that if L is a 
minimal dfa for L then S is a minimal spanning set for L. Why 
is the converse to this statement false? 
(d) Let L be a regular language, let S be a spanning set for L, and 
let L be the dfa constructed from S as in the proof of Theorem 
7.1. Prove that S is a minimal spanning set for L if and only if 
L is a minimal dfa for L. 
(e) 
Let L and L
1 be dfas on alphabet A with states Q and Q 1 , 
initial states q1 and q;, accepting states F and F 1 , and transi-
tion functions 8 and 8 1 â¢ L and .4' 1 are isomorphic if there is a 
one-one function g from Q onto Q 1 such that g(q1) = q;, q E F 
if and only if g(q) E F 1 , and 8 1(g(q), s) = g(8(q, s)) for all 
q E Q and s EA. (Informally, Land L
1 are identical but for 
a renaming of the states.) Prove that, if L 
and L
1 are 
both minimal dfas for some regular language L, then they are 
isomorphic. 

7. The Myhill- Nerode Theorem 
267 
13. Let L 1 , L 2 be languages on some alphabet A. The right quotient of 
L 1 by L 2 , denoted L 1/L 2 , is {x l.xy E L 1 for some y E L 2}. Prove 
that if L 1 and L 2 are regular, then L 1/L 2 is regular. 
14. Let L = {alPlblmll p is a prime number, m > 0} U {alnll n ;::: 0}. 
(a) Show that L is not regular. [Hint: See Exercise 4.3 and Exer-
cises 10 and 13 above.] 
(b) Explain why the pumping lemma alone is not sufficient to show 
that L is not regular. 
(c) 
State and prove a stronger version of the pumping lemma which 
is sufficient to show that L is not regular. 


10 
Context-Free Languages 
1. Context-Free Grammars and Their Derivation Trees 
Let 'F, T be a pair of disjoint alphabets. A context-free production on 'F, T 
is an expression 
x~h 
where X E 'F and h E ('FU T)*. The elements of 'Fare called variables, 
and the elements of T are called terminals. If P stands for the production 
X~ hand u, v E ('FU T)*, we write 
U=JV 
to mean that there are words p, q E ('FU T)* such that u = pXq and 
v = phq. In other words, v results from u by replacing the variable X by 
the word h. Productions X~ 0 are called null productions. A context-free 
grammar r with variables 'F and terminals T consists of a finite set of 
context-free productions on 'F, T together with a designated symbol 
S E 'F called the start symbol. Collectively, the set 'FU T is called the 
alphabet of r. If none of the productions of r is a null production, r is 
called a positive context-free grammar.' 
1 Those who have read Chapter 7 should note that every positive context-free grammar is a 
context-sensitive grammar in the sense defined there. For the moment we are not assuming 
familiarity with Chapter 7. However, the threads will all be brought together in the next 
chapter. 
269 

270 
Chapter 10 Context-Free Languages 
If r is a context-free grammar with variables 'F and terminals T, and if 
u,v E ('FUT)*, we write 
u=pv 
to mean that u =;tv for some production P of r. We write 
u'?v 
to mean there is a sequence u1 , â¢â¢â¢ , um where u = u1 , um = v, and 
for 
1 ~ i < m. 
The sequence u1 , â¢â¢â¢ , um is called a derivation of v from u in f. The 
number m is called the length of the derivation. 2 The symbol r below the 
==> may be omitted when no ambiguity results. Finally, we define 
L(f) = {u E T* IS ~ u}. 
L(f) is called the language generated by f. A language L ~ T* is called 
context-free If there is a context-free grammar r such that L = L(f). 
A simple example of a context-free grammar r is given by 'F= {S}, 
T = {a, b}, and the productions 
S ---+ aSb, 
S ---+ ab. 
Here we clearly have 
L(f) = {a[n)b[nll n > 0}; 
thus, this language is context-free. We showed in Chapter 9, Section 6, that 
L(f) is not regular. Later we shall see that every regular language is 
context-free. For the meanwhile we have proved 
Theorem 1.1. The language L = {a[nlb[nll n > 0} is context-free but not 
regular. 
We now wish to discuss the relation between context-free grammars in 
general and positive context-free grammars. It is obvious that if r is a 
positive context-free grammar, then 0 f/=. L(f). We shall show that except 
for this limitation, everything that can be done using context-free gram-
mars can be done with positive context-free grammars. This will require 
some messy technicalities, but working out the details now will simplify 
matters later. 
2 Some authors use the number m - 1 as the length of the derivation. 

1. Context-Free Grammars and Their Derivation Trees 
271 
Definition. We define the kernel of a given context-free grammar f, 
written ker (f), to be the set of variables V of f such that V =f 0. 
As an example consider the context-free grammar f 0 with productions 
s~XYIT, 
s ~ax, 
x~o. 
v~ o. 
Then ker(f0 ) ={X, Y, S}. This example suggests an algorithm for locating 
the elements of ker(f) for a given context-free grammar f. We let 
~~ = {VI V ~ 0 is a production of f}, 
r;+ I = r; u {VI v ~a is a production of r, where a E r;*}. 
Thus for f 0, ~u = {X, Y}, 'Y1 ={X, Y, S}, and r; = 'Y1 for all i > 1. S is 
in 'Yl because XYIT E 'Yo* . In the general case it is clear, because r has 
only finitely many variables, that a stage k will eventually be reached for 
which ~+ 1 = ~ and that then r; = 'Yk for all i > k. We have 
Lemma I. If 'Yk = 'Yk+ 1 , then ker(f) = 'Yk. 
Proof. It is clear that r; ~ ker (f) for all i. Conversely, we show that if 
V E ker(f), then V E 'Yk. We prove this by induction on the length of a 
derivation of 0 from V in f. If there is such a derivation of length 2, then 
V t 0, so that V ~ 0 is a production of f and V E 'Y0. Let us assume the 
result for all derivations of length < r and let V = a 1 = a 2 = Â·Â·Â· = 
a,_ 1 = a, = 0 be a derivation of length r in f. The words a 1 , a 2 , â¢â¢â¢ , a,_ 1 
must consist entirely of variables, since terminals cannot be eliminated by 
context-free productions. Let a 2 = V1 V2 â¢ â¢ â¢ V,. Then we have V; =f 0, 
i = 1, 2, ... , s, by derivations of length < r. By the induction hypothesis, 
each v; E 'Yk . Since f contains the production V ~ V1 V2 â¢ â¢ â¢ V, , and 
a 2 E ~* , we have V E 'Yk + 1 = 'Yk . 
â¢ 
Lemma 2. There is an algorithm that will transform a given context-free 
grammar f into a positive context-free grammar f such that L(f) = L(f) 
or L(f) = L(f) u {0}. 
Proof. We begin by computing ker (f). Then we obtain f by first adding 
all productions that can be obtained from the productions of f by deleting 
from their righthand sides one or more variables belonging to ker (f) and 
by then deleting all productions (old and new) of the form V ~ 0. (In our 
example, f 0 would have the productions S ~ XYIT, S ~ aX, S ~ a, 
S ~ Yl'X, S ~ XIT, S ~ XYY, S ~ XY, S ~ YY, S ~ IT, S ~ XX, 
S ~X, S ~ Y.) We shall show that L(f) = L(f) or L(f) = L(f) u {0}. 

272 
Chapter 1 0 Context-Free Languages 
Let v ~ f3t f3z ... f3s be a production of r that is not a production of 
r, where {31 , {32, ..â¢ , f3s E ( ~u T), and where this production was ob-
tained from a production of r of the form 
V ~ Uo f3t Ut f3z â¢â¢. f3sus' 
with u0,u1,u2, ... ,U 5 E (ker(f))*. [Of course, u0,u5 might be 0. But 
since 0 E (ker (f))*, this creates no difficulty.] Now, 
U; =f 0, 
i = 0,1,2, ... ,s, 
so that 
V t Uo f3tUI f3z â¢â¢â¢ Us-1 f3sus =f f3t f3z ..â¢ f3s Â· 
Thus, the effect of this new production of f can be simulated in f. This 
proves that L(f} ~ L(f). 
It remains to show that if v E L(f) and v -=!= 0, then v E L(f}. Let T be 
the set of terminals of r (and also of f). We shall prove by induction the 
stronger assertion: 
For any variable V, ifV =f w-=/= 0 forw E T*, then V=j? w. 
If in fact V t ~, then r contains the production V ~ w which is also a 
production of r. Otherwise we may write 
V t w0V1w1V2w2 â¢â¢â¢ V.ws 'f w, 
where VtoÂ·Â·Â·Â·V. are variables and w0 ,w1 ,w2 , â¢â¢â¢ ,W5 are (possible null) 
words on the terminals. Then w can be written 
where 
i = 1, 2, ... ' s. 
Since each V; must have a shorter derivation from V; than w has from V, 
we may proceed inductively by assuming that for each V; which is not 0, 
V;:; V;. On the other hand, if V; = 0, then V; E ker(f). We set 
r 
vo = {0 
I 
V; 
if 
V; = 0 
otherwise. 
Then v ~ WoV~w IV~w 2 â¢â¢â¢ V~ws is one of the productions of r. Hence 
we have 

1. Context-Free Grammars and Their Derivation Trees 
273 
We can now easily prove 
Theorem 1.2. A language L is context-free if and only if there is a 
positive context-free grammar r such that 
L = L(f) or L = L(f) U {0}. 
(1.1) 
Moreover, there is an algorithm that will transform a context-free gram-
mar A for which L = L(A) into a positive context-free grammar f that 
satisfies (1.1). 
Proof. If L is context-free with L = L(A) for a context-free grammar A, 
then we can use the algorithm of Lemma 2 to construct a positive 
context-free grammar f such that L = L(f) or L = L(f) u {0}. 
Conversely, if f is a positive context-free grammar and L = L(f), there 
is nothing to prove since a positive context-free grammar is already a 
context-free grammar. If L = L(f) u {0}, let S be the start symbol of f 
and let f be the context-free grammar obtained from f by introducing S 
as a new start symbol and adding the productions 
S-+ S, 
Clearly, L(f) = L(f) u {0}. 
s-+ 0. 
â¢ 
Now, let f be a positive context-free grammar with alphabet T u 'Y, 
where T consists of the terminals and r is the set of variables. We will 
make use of trees consisting of a finite number of points called nodes or 
vertices, each of which is labeled by a letter of the alphabet, i.e., an 
element of T u r. Certain vertices will have other nodes as immediate 
successors, and the immediate successors of a given node are to be in some 
definite order. It is helpful (though of course not part of the formal 
development) to think of the immediate successors of a given node as 
being physically below the given node and arranged from left to right in 
their given order. Nodes are to be connected by line segments to their 
immediate successors. There is to be exactly one node which is not an 
immediate successor; this node is called the root. Each node other than 
the root is to be the immediate successor of precisely one node, its 
predecessor. Nodes which have no immediate successors are called leaves. 
A tree is called a f -tree if it satisfies the following conditions: 
1. the root is labeled by a variable; 
2. each vertex which is not a leaf is labeled by a variable; 
3. if a vertex is labeled X and its immediate successors are labeled 
a 1 , a 2 , â¢â¢â¢ , ak (reading from left to right), then X-+ a 1 a 2 â¢â¢â¢ ak is 
a production of r. 

274 
Chapter 1 0 Context-Free Languages 
Let !T be a f-tree, and let 11 be a vertex of !T which is labeled by the 
variable X. Then we shall speak of the subtree !Tv of !T determined by 11.The 
vertices of !Tv are 11, its immediate successors in !T, their immediate 
successors, and so on. The vertices of !Tv are labeled exactly as they are in 
!T. (In particular, the root of !Tv is 11 which is labeled X.) Clearly, !Tv is 
itself a f-tree. 
If !Tis a f-tree, we write (!T) for the word that consists of the labels of 
the leaves of !T reading from left to right (a vertex to the left of a given 
node is regarded as also being to the left of each of its immediate 
successors). If the root of !T is labeled by the start symbol S of r and if 
w = (!T), then !Tis called a derivation tree for win r. Thus the tree shown 
in Fig. 1.1 is a derivation tree for a14lbl31 in the grammar shown in the same 
figure. 
Theorem 1.3. If f is a positive context-free grammar, and S =f w, then 
there is a derivation tree for w in r. 
Proof. 
Our proof is by induction on the length of a derivation of w from 
s in r. If this length is 1, then w = s and the required derivation tree 
consists of a single vertex labeled S (being both root and leaf). 
Now let w have a derivation from S of length r + 1, where the result is 
known for derivations of length r. Then we have S ~ v = w with v, w E 
('YU T)*, where the induction hypothesis applies to the derivation S ~ v. 
Thus, we may assume that we have a derivation tree for v. Now since 
v = w, we must have v = x.Xy and w = xa1 Â·Â·Â· aky, where r contains the 
production X~ a 1 ... ak. Then the derivation tree for v can be extended 
to yield a derivation tree for w simply by giving k immediate successors to 
the node labeled X, labeled a 1, ... , ak from left to right. 
â¢ 
Before considering the converse of Theorem 1.3, it will be helpful to 
consider the following derivations of a14lbPl from S with respect to the 
grammar indicated in Fig. 1.1: 
1. S = aXbY = a12l)(by = a13l)(by = a14lbY = a14lbi21Y = a14lbPl 
2. S = aXbY = aiZJ)(by = a12l)(bi2Jy = aPl)(b[2Jy = aPl)(bi3J = a14lbPl 
3. S = aXbY = aXb121Y = aXb131 = a12l)(bl31 = a13l)(bPl = a14lbl31. 
Now, if the proof of Theorem 1.3 is applied to these three derivations, the 
very same derivation tree is obtained-namely, the one shown in Fig. 1.1. 
This shows that there does not exist a one-one correspondence between 
derivations and derivation trees, but that rather, several derivations may 
give rise to the same tree. Hence, there is no unique derivation which we 
can hope to be able to read off a given derivation tree. 

1. Context-Free Grammars and Their Derivation Trees 
275 
S .... aXbY 
X-+aX 
y .... bY 
x .... a 
y .... b 
Figure 1.1. A derivation tree for af4lbf31 in the indicated grammar. 
Definition. We write u = 1 v (in f) if u = xXy and v = xzy, where X~ z 
is a production of f and X E T*. If, instead, X E (T U W")* but y E T*, 
we write u =,v. 
Thus, when u = 1 v, it is the leftmost variable in u for which a substitu-
tion is made, whereas when u =, v, it is the rightmost variable in u. A 
derivation 
is called a leftmost derivation, and then we write u1 ;;,.1 un. Similarly, a 
derivation 
is called a rightmost derivation, and we write u1 ;;,., un. In the preceding 
examples of derivations of al4lbl31 from S in the grammar of Fig. 1.1, 1 is 
leftmost, 3 is rightmost, and 2 is neither. 
Now we shall see how, given a derivation tree :T for a word w E T*, we 
can obtain a leftmost derivation of w from S and a rightmost derivation of w 
from S. Let the word which consists of the labels of the immediate 
successors of the root of :T(reading from left to right) be v0 X 1v1X 2 â¢â¢â¢ 
X,v,, where Vo, VI, ... ' v, E T*, XI' Xz, ... ' X, E r, and XI' Xz, ... ' X, 
label the vertices v1 , â¢â¢â¢ , v,, which are immediate successors of the root of 
::T. (Of course, some of the V; may be 0.) Then S ~ v0X 1v1X 2 â¢â¢â¢ X,v, is 
one of the productions of f. Now it is possible that the immediate 
successors of the root of :Tare all leaves; this is precisely the case where 
w = v0 and r = 0. If this is the case, then we have S = 1 wand S =, w, so 
that we do have a leftmost as well as a rightmost derivation of w from S. 

276 
Chapter 1 0 Context-Free Languages 
d, 
Â·~--, 
a 
I 
X 
b 
I 
y 
!all ! 
!b/! 
~ 
1/1 I 
I I I 
I 
I 
I 
I 
Ia 
X 
1 
I 
b 
I 
I I I 
I 
I 
I 
1 
I 
I 
I 
I 
I 
1 
a 
I 
I 
I 
._ ____ j 
....._ ___ ...... 
Figure 1.2. Decomposition of the tree of Fig. 1.1 as in the proof of the existence of leftmost 
and rightmost derivations. 
Otherwise, i.e., for r > 0, we consider the trees 9j =:Tv', i = 1, 2, ... , r. 
Here 9i has its root II; labeled X; and is made up of the part of :T 
consisting of II;, its immediate successors, their immediate successors, etc. 
(see Fig. 1.2). Let f; be the grammar whose productions and alphabet are 
the same as for r but which has start symbol X;. Then 9j is a derivation 
tree in f;. Let 9j be a derivation tree for w; in f;. Then, clearly, 
w = v0w1v 1w2v2 â¢â¢â¢ w,v,. 
Moreover, since each 9j contains fewer vertices than :T, we may assume 
inductively that for i = 1, 2, ... , r 
Hence we have 
and 
and 
S = 1 v0 X 1v 1X 2 â¢â¢â¢ X,v, 
~ 1 v0w1v1X 2 â¢â¢â¢ X,v, 
~ 1 v0w1v1w2 â¢â¢â¢ X,v, 
S =,v0X 1v1X 2 â¢â¢â¢ X,v, 
~,v 0 X1 v 1 X2 â¢â¢â¢ w,v, 
~,v 0 X1 v 1 w 2 â¢â¢â¢ w,v, 
~,v 0w 1 v 1w2 â¢â¢â¢ w,v, = w. 

1. Context-Free Grammars and Their Derivation Trees 
277 
So we have shown how to obtain a leftmost and a rightmost derivation of 
w from sin r. 
Now, Theorem 1.3 tells us that if w E L(f), there is a derivation tree 
for w in r. And we have just seen that if there is a derivation tree for w in 
r, then there are both leftmost and rightmost derivations of w from s in 
r [so that, in particular, w E L(f)]. Putting all of this information together 
we have 
Theorem 1.4. Let f be a positive context-free grammar with start symbol 
S and terminals T. Let w E T*. Then the following conditions are equiva-
lent: 
1. wE L(f); 
2. there is a derivation tree for w in f; 
3. there is a leftmost derivation of w from S in f; 
4. there is a rightmost derivation of w from s in r 0 
Definition. A positive context-free grammar is called branching if it has 
no productions of the form X ~ Y, where X and Y are variables. 
For a derivation tree in a branching grammar r, each vertex that is not 
a leaf cannot be the only immediate successor of its predecessor. Since we 
shall find it useful to work with branching grammars, we prove 
Theorem 1.5. There is an algorithm that transforms a given positive 
context-free grammar r into a branching context-free grammar a such 
that L(a) = L(f). 
Proof. Let 'F be the set of variables of f. First suppose that r contains 
productions 
... , 
(1.2) 
where k ~ 1 and X 1 , X 2 , â¢â¢â¢ , Xk E 'F. Then, we can eliminate the pro-
ductions (1.2) and replace each variable X; in the remaining productions 
of r by the new variable X. (If one of X 1 , â¢â¢â¢ , Xk is the start symbol, then 
X must now be the start symbol.) Obviously the language generated is not 
changed by this transformation. 
Thus, we need consider only the case where no "cycles" like (1.2) occur 
in f. If r is not branching, it must contain a production X~ Y such that 
f contains no productions of the form Y ~ Z. We eliminate the produc-
tion X~ Y, but add tor productions X~ x for each word x E ('FU T)* 
for which y ~X is a production of r. Again the language generated is 
unchanged, but the number of productions that r contains of the form 

278 
Chapter 1 0 Context-Free Languages 
U ~ V has been decreased. Iterating this process we arrive at a grammar 
a containing no productions of the form U ~ V, which is therefore of the 
required form. 
â¢ 
A path in a f-tree :Tis a sequence a 1 , a 2 , â¢â¢â¢ , ak of vertices of :T such 
that ai+ 1 is an immediate successor of a; for i = 1, 2, ... , k -
1. All of 
the vertices on the path are called descendants of a 1 â¢ 
A particularly interesting situation arises when two different vertices 
a, {3 lie on the same path in the derivation tree :T and are labeled by the 
same variable X. In such a case one of the vertices is a descendant of the 
other, say, {3 is a descendant of a. yfJ is then not only a subtree of :T but 
also of :Ta. [In fact, ('Ta)f3 = :Tf3.] We wish to consider two important 
Original tree !7 
(<>,/l are labeled by the same variable) 
f pruned 
!/spliced 
Figure 1.3 

1. Context-Free Grammars and Their Derivation Trees 
279 
operations on the derivation tree :T which can be performed in this case. 
The first operation, which we call prnning, is to remove the subtree :Ta 
from the vertex a and to graft the subtree g-f3 in its place. The second 
operation, which we call splicing, is to remove the subtree g-f3 from the 
vertex {3 and to graft an exact copy of :Ta in its place. (See Fig. 1.3.) 
Because a and {3 are labeled by the same variable, the trees obtained by 
prnning and splicing are themselves derivation trees. 
Let y; and .57. be trees obtained from a derivation tree :T in a branching 
grammar by pruning and splicing, respectively, where a and {3 are as 
before. We have (:T) = r 1(:Ta)r2 for words r 1 , r2 and (:Ta) = q1(:Tf3 )q2 
for words q1 , q2 â¢ Since a, {3 are distinct vertices, and since the grammar is 
branching, q1 and q2 cannot both be 0. (That is, q1q2 =I= 0.) Also, 
<y;> = r 1(:Tf3)r2 
and 
(.57.)= r 1 q\2 l(:T13 )q~2lr2 â¢ 
(1.3) 
Since q1q2 =I= 0, we have I(:Tf3)1 < I(:Ta)l and hence I(:TP)I < I(:T)I. From 
this last inequality and Theorem 1.4, we can easily infer 
Theorem 1.6. Let f be a branching context-free grammar, let u E L(f), 
and let u have a derivation tree :T in r that has two different vertices on 
the same path labeled by the same variable. Then there is a word 
v E L(f) such that lvl < lui. 
Proof. Since u = (:T), we need only take v = <y;>. 
â¢ 
Exercises 
1. Find a context-free grammar generating the set of arithmetic state-
ments of Pascal (or FORTRAN). 
2. Consider the grammar r with start symbol S and productions 
S---+ XXYY 
X---+ a 
X---+ XX 
y---+ b. 
y---+ yy 
Show that f generates the same language as the grammar of Fig. 1.1. 
3. Show that 0 is a context-free language. 
4. Give three languages that are context-free but not regular. Justify your 
answer. 
5. Give a context-free grammar r such that ker {f) = 'r.j and ~ =I= 
ker(f), i = 1,2,3. 

280 
Chapter 1 0 Context-Free Languages 
6. 
Let r be a context-free grammar with productions XI ~ al ' ... ' 
xn ~ an. We define the width of r as Ei~ I I a;l. 
(a) Give a function f(w) such that for any context-free grammar f 
with width w, there is a positive context-free grammar r such 
that L(f) = L(f) or L(f) = L(f) U {0} and f has no more than 
f( w) productions. 
(b) Give a grammar f with width w for which any such f has at least 
f( w) /2 productions. 
7. (a) Let f be the grammar in Exercise 2. Give two different deriva-
tion trees for aaabb. From each tree obtain a leftmost and a 
rightmost derivation of aaabb from S. 
(b) 
Let f' be the grammar in Fig. 1.1. Prove that for every wE L(f'), 
there is a unique derivation tree for W in r I â¢ 
8. 
Let r be the grammar with productions 
s~vw 
s~w 
v~bx 
v~b 
w~aw x~s 
w~x 
x~w 
w~Y 
and start symbol S. Use the construction in the proof of Theorem 1.5 
to give a branching context-free grammar a such that L(a) = L(f). 
Can any of the resulting productions be eliminated from a? 
2. Regular Grammars 
We shall now see that regular languages are generated by context-free 
grammars of an especially simple form. 
Definition. A context-free grammar is called regular if each of its produc-
tions has one of the two forms 
U ~ aV or U ~a, 
where U, V are variables and a is a terminal. 
Then we have 
Theorem 2.1. If L is a regular language, then there is a regular grammar 
r such that either L = L(f) or L = L(f) u {0}. 
Proof. 
Let L = L(.L), where L is a dfa with states q1 , â¢â¢â¢ , qm, alphabet 
{s1 , â¢â¢â¢ , sn}, transition function 8, and set of accepting states F. We 

2. Regular Grammars 
281 
construct a grammar f With variables q1, ... , qm, terminals s1, ... , Sn, and 
start symbol q1 â¢ The productions are 
1. q; ---+ s,qj whenever 8(q;, s,) = qj, and 
2. q; ---+ s, whenever 8(q;, s,) E F. 
Clearly the grammar f is regular. We shall show that L(f) is just L - {0}. 
First, suppose u E L, u -=!= 0; let u = s,. s,. Â·Â·Â· s,. s,. 
. Thus, 8*(q1, u) E 
I 
2 
I 
I+ I 
F, so that we have 
8(q1,s.)=q., 8(q.,s.)=q., ... , 8(q1.,s,. )=q,. 
EF. (2.1) 
't 
l1 
1J 
12 
12 
I 
1+1 
/+1 
Hence, the grammar f contains the productions 
q1. ---+ S; 
â¢ (2.2) 
I 
I+ I 
Thus, we have in f 
ql ==> S;,qj, 
==> S;,S;2qh 
(2.3) 
==> S;,S;2 ... S;lqh 
==> S; 1S;2 Â·Â·Â· S;1S;1+ 1 = U, 
so that u E L(f). 
Conversely, suppose that u E L(f), u = s,. s,. Â·Â·Â· s,. s,. 
. Then there is a 
I 
2 
I 
I+ 1 
derivation of u from q1 in r, which must be of the form (2.3). Hence, the 
productions listed in (2.2) must belong to r, and finally, the transitions 
(2.1) must hold in .L. Thus, u E L(L). 
â¢ 
Theorem 2.2. Let r be a regular grammar. Then L(f) is a regular 
language. 
Proof. Let r have the variables VI ' Vz ' ... ' VK' where s = VI is the start 
symbol, and the terminals sl' ... ' sn. Since r is assumed to be regular, its 
productions are of the form V; ---+ s,l-j and V; ---+ s,. We shall construct an 
ndfa L which accepts precisely L(f). 
The states of L will be V1 , V2 , â¢â¢â¢ , VK and an additional state W. V1 will 
be the initial state and W will be the only accepting state, i.e., F = {W}. 
Let 
81(V;, s,) = {J.j IV; ---+ s,l-j is a production of r}, 
if 
V; ---+ s, is a production of r 
otherwise. 

282 
Chapter 1 0 Context-Free Languages 
Then we take as the transition function 8 of L 
8(V;, s,) = 81(V;, s,) U !52(V;, s,). 
This completes the specification of L. 
Now let u = s,. s, . ... s,. s,. 
E L(f). Thus, we must have 
I 
2 
I 
I+ I 
V1 = SÂ· V = SÂ· SÂ· V ~ SÂ· SÂ· ... SÂ· V = SÂ· SÂ· ... SÂ· SÂ· 
(2.4) 
't 11 
'' 'z Jz 
't 'z 
'' lt 
'' 'z 
'' 't+l' 
where r contains the productions 
Thus, 
VI 
~ S;ll-}1 ' 
V 
~s. V, 
11 
'z Jz 
l-fl- I ~ S;ll-}1' 
l-}1 
~ S;l+ I â¢ 
l-}1 E 8(V1 , s;), 
l-}2 E S( l-}1 , s;), 
V1. E ll(V1. 
, s,. ), 
I 
1- I 
I 
WE 8(V1., S; 
). 
I 
I+ I 
Thus, WE 8*(V1 , u) and u E L(.L). 
(2.5) 
(2.6) 
Conversely, if u = s,. s, . ... s; s; 
is accepted by L, then there must be 
I 
2 
I 
I+ 1 
a sequence of transitions of the form (2.6). Hence, the productions of (2.5) 
must all belong to f, so that there is a derivation of the form (2.4) of u 
from V1â¢ 
â¢ 
In order to combine Theorems 2.1 and 2.2 in a single equivalence, it is 
necessary to show only that if L is a regular language, then so is L u {0}. 
But this follows at once from Theorems 4.2 and 4.5 in Chapter 9. 
Combining Theorems 2.1 and 2.2 with this discussion, we have 
Theorem 2.3. A language L is regular if and only if there is a regular 
grammar f such that either L = L(f) or L = L(f) u {0}. 
Since regular grammars are context-free grammars, we have 
Corollary 2.4. Every regular language is context-free. 
The converse of Corollary 2.4 is not true, however, as we have already 
observed in Theorem 1.1. 

2. Regular Grammars 
283 
There are more extensive classes of context-free grammars which can be 
shown to generate only regular languages. A particularly important exam-
ple for us (see Section 7) is the class of right-linear grammars. 
Definition. A context-free grammar is called right-linear if each of its 
productions has one of the two forms 
u~xv or 
u~x, 
(2.7) 
where U, V are variables and x =1= 0 is a word consisting entirely of 
terminals. 
Thus a regular grammar is just a right-linear grammar in which lxl = 1 
for each string x in (2.7). We have 
Theorem 2.5. Let f be a right-linear grammar. Then L(f) is regular. 
Proof. Given a right-linear grammar r, we construct a regular grammar 
r as follows. 
We replace each production of f of the form 
by the productions 
n > 1, 
Zn-2 ~ an-lzn-1' 
Zn-1 ~ anV, 
where Z 1 , â¢â¢â¢ , Zn _ 1 are new variables. Also, we replace each production 
n > 1, 
by a list of productions similar to the preceding list except that instead of 
the last production we have 
Zn-l~an. 
It is obvious that f is regular and that L(f) = L{f). 
â¢ 
Exercises 
1. (a) For each regular language L described in Exercise 1.1 of Chapter 
9, give a regular grammar r such that L(f) = L - {0}. 

284 
Chapter 10 Context-Free Languages 
(b) For each dfa L 
in Exercise 1.3 of Chapter 9, give a regular 
grammar f such that L(f) = L(L) - {0}. 
(c) 
For each ndfa L 
in Exercise 2.1 of Chapter 9, give a regular 
grammar f such that L(f) = L(L) - {0}. 
2. Let r be the grammar with productions 
S ----+ aS 
X ----+ bX 
S ----+ aX 
X ----+ bZ 
S ----+ aY 
Y----+ eX 
Z ----+ aZ 
Z ----+ bZ 
Z ----+ cZ 
Z ----+ a 
z ----+ b 
z ----+ c 
Y~cz 
and start symbol S. Give an ndfa L such that L(L) = L(f). 
3. Let f be the grammar with productions 
and start symbol S. 
S ----+aX 
S ----+ bY 
X----+ aZ 
X----+ bX 
Y---+aY 
Y----+ bZ 
Z ----+ aS 
Z ----+ bS 
Z ----+ a 
Z---+b 
(a) Use the construction in the proof of Theorem 2.2 to give an ndfa 
L with five states such that L(L) = L(f). 
(b) Transform L into a dfa L' with four states such that L(L') = 
L(f). 
4. 
Prove that for every regular language L, there is a regular grammar r 
with start symbol S such that L = L(f) or L = L(f) U {0} and such 
that every w E L(f) has exactly one derivation from S in f. 
5. 
Prove that for every n ;::: 1, there is a regular language generated by 
no regular grammar with fewer than n variables. 
6. (a) Write a context-free grammar to generate all and only regular 
expressions over the alphabet {a, b}. 
(b) Can a regular grammar generate this language? Support your 
answer. 
7. A grammar f is self-embedding if there is a variable X such that 
X=f vXw, 
where 
v,w E (W"U T)*- {0}. 
Let L be a context-free language. Prove that L is regular if and only if 
there is a non-self-embedding context-free grammar f such that 
L(f) = L. 

3. Chomsky Normal Form 
285 
8. For a language L, the reverse of L, denoted LR, is {wR I wE L}. 
(a) Let f be a regular grammar and let L = L(f). Show that there is 
an ndfa which accepts L R. 
(b) Conclude from (a) that a language L is regular if and only if L R 
is regular. 
(c) 
Let r be a grammar such that all productions are of the form 
U ~ Vs or U ~ s, where U, V are variables and s is a terminal. 
Show that L(f) is regular. 
(d) A grammar is left-linear if each of its productions is of the form 
U ~ Vx or U ~ x, where U, V are variables and x is a word 
consisting entirely of terminals. Prove that a language L is 
regular if and only if there is a left-linear grammar r such that 
L = L(f) or L = L(f) U {0}. 
3. 
Chomsky Normal Form 
Although context-free grammars are extremely simply, there are even 
simpler special classes of context-free grammars that suffice to give all 
context-free languages. Such classes are called normal forms. 
Definition. A context-free grammar f with variables 'F and terminals T 
is in Chomsky normal form if each of its productions has one of the two 
forms 
x~ yz or 
x~a, 
where X, Y, Z E 'F and a E T. 
Then we can prove 
Theorem 3.1. There is an algorithm that transforms a given positive 
context-free grammar r into a Chomsky normal form grammar A such 
that L(f) = L(A). 
Proof. Using Theorem 1.5, we begin with a branching context-free gram-
mar r with variables 'F and terminals T. We continue by "disguising" the 
terminals as variables. That is, for each a E T we introduce a new variable 
X a. Then we modify r by replacing each production X ~ X for which X is 
not a single terminal by X ~ x 1 , where x 1 is obtained from x by replacing 
each terminal a by the corresponding new variable X a. In addition all of 
the productions Xa ~ a are added. Clearly the grammar thus obtained 
generates the same language as r and has all of its productions in one of 

286 
the two forms 
x~a, 
Chapter 1 0 Context-Free Languages 
k ';:::. 2, 
(3.1) 
(3.2) 
where X, X 1 , â¢â¢â¢ , Xk are variables and a is a terminal. To obtain a 
Chomsky normal form grammar we need to eliminate all of the produc-
tions of type (3.1) for which k > 2. We can do this by introducing the new 
variables Z 1 , Z 2 , â¢â¢â¢ , Zk_ 2 and replacing (3.1) by the productions 
x~x1 Z 1 
zl ~x2z2 
zk-3 ~ xk-2zk-2 
zk-2 ~ xk-lxk Â· 
Thus we obtain a grammar in Chomsky normal form that generates L(f) . â¢ 
As an example, let us convert the grammar of Fig. 1.1 to Chomsky 
normal form .. 
Step 1. 
Eliminate productions of the form X 1 ~ X 2 : there are no such 
productions so we skip this step. 
Step 2. 
Disguise the terminals as variables: the grammar now consists 
of the productions 
Step 3. 
Obtain Chomsky normal form by replacing the production 
s ~ xaxxby by the productions 
S ~xaz 1
, 
Z1 ~xz2
, 
Z2 ~ XbY. 
The final Chomsky normal form grammar thus obtained consists of the 
productions 

4. Bar-Hillel's Pumping Lemma 
Exercises 
l. (a) Find context-free grammars f 1, f 2 such that 
L(f1) = {a[ilbln I i ~ j > 0} 
L(f2 ) = {al2ilblill i > 0}. 
287 
(b) Find Chomsky normal form grammars that generate the same 
languages. 
2. Let T = {!, p, q} be the set of terminals for the grammar f: 
s ~p, 
s ~ q, 
s ~! ss. 
Find a Chomsky normal form grammar that generates L(f). 
3.* A context-free grammar is said to be in Greibach normal form if every 
production of the grammar is of the form 
k ~ 0, 
where a E T and X, Y1, Y2 , â¢â¢â¢ , Yk E 'F. Show that there is an algo-
rithm that transforms any positive context-free grammar into one in 
Greibach normal form that generates the same language. 
4.* Show that there is an algorithm that transforms any positive context-
free grammar into a grammar that generates the same language for 
which every production is of the form 
or 
A, B, C E 'F, a E T. 
A ~a, 
A ~aB, 
A~ aBC, 
4. 
Bar-Hillel's Pumping Lemma 
An important application of Chomsky normal form is in the proof of the 
following key theorem, which is an analog for context-free languages of the 
pumping lemma for regular languages. 
Theorem 4.1 (Bar-Hillel's Pumping Lemma). Let f be a Chomsky nor-
mal form grammar with exactly n variables, and let L = L(f). Then, for 

288 
Chapter 1 0 Context-Free Languages 
every x E L for which lxl > 2n, we have x = r1q 1rq2r2 , where 
1. lq1rq2l:::;; 2\ 
2. qlq2 =I= 0; 
3. for all i ~ 0, r 1 q\ilrq~ilr2 E L. 
Lemma. Let S 'f u, where f is a Chomsky normal form grammar. 
Suppose that :T is a derivation tree for u in r and that no path in :T 
contains more than k nodes. Then lui:::;; 2k- 2â¢ 
Proof. First, suppose that :T has just one leaf labeled by a terminal a. 
Then u = a, and :T has just two nodes, which are labeled S and a, 
respectively. Thus, no path in :T contains more than two nodes and 
lui= 1 :::;; 22 - 2â¢ 
Otherwise, since r is in Chomsky normal form, the root of :T must have 
exactly two immediate successors a, {3 in :T labeled by variables, say, X 
andY, respectively. (In this case, r contains the productionS ~ XY.) Now 
we will consider the two trees 9'1 = :Ta and 92 = yfJ whose roots are 
labeled X and Y, respectively. (See Fig. 4.1.) 
In each of 9'1 and 92 the longest path must contain 
:::;; k - 1 nodes. 
Proceeding inductively, we may assume that each of 9'1, 92 have :::;; 2k-J 
leaves. Hence, 
â¢ 
Proof of Theorem 4.1. Let x E L, where lxl > 2n, and let :T be a deriva-
tion tree for X in f. Let a 1 , a 2 , â¢â¢â¢ , am be a path in :fwhere m is as large 
as possible. Then m ~ n + 2. (For, if m :::;; n + 1, by the lemma, lxl :::;; 2n - 1.) 
'----------------
1-1 
s 
L--------------J 
Figure4.l 

4. Bar-Hillel's Pumping Lemma 
289 
s 
Figure 4.2 
am is a leaf (otherwise we could get a longer path) and so is labeled by a 
terminal. a 1 , a 2 , â¢â¢â¢ , am_ 1 are all labeled by variables. Let us write 
'Yi = am+i-n-2' 
i = 1,2, ... ,n + 2 
so that the sequence of vertices y 1 , y2 , â¢â¢â¢ , 'Yn + 2 is simply the path 
consisting of the vertices 
where 'Yn + 2 = am is labeled by a terminal, and y 1 , â¢â¢â¢ , 'Yn + 1 are labeled by 
variables. Since there are only n variables in the alphabet of r, the 
pigeon-hole principle guarantees that there is a variable X that labels two 
different vertices: a = 'Y; and {3 = 'Yj, i < j. (See Fig. 4.2.) Hence, the 
discussion of pruning and splicing at the end of Section 1 can be applied. 
We let the words q1q2 , r1 , r2 be defined as in that discussion and set 
r = (:713 ). Then [recalling (1.3)] we have 
(Y;;) = r1rr2, 
(.9;) = r,q\2lrq~2lr2, 
((.9;).) = r,qplrq~lr2. 

290 
Chapter 10 Context-Free Languages 
Since pruning and splicing a derivation tree in f yields a new derivation 
tree in r, we see that all of these words belong to L(f). If, in addition, we 
iterate the splicing operation, we see that all of the words r 1 q\ilrqÂ¥lr 2 , 
i ~ 0, belong to L(f). 
Finally, we note that the path 'Y;, ... , 'Yn + 2 in !7 01 consists of ~ n + 2 
nodes and that no path in !701 can be longer. (This is true simply because if 
there were a path in !7 01 consisting of more than n + 3 - i vertices, it 
could be extended backward through a = 'Y; to yield a path in :T consist-
ing of more than m vertices.) Hence by the lemma 
â¢ 
As an example of the uses of Bar-Hillel's pumping lemma, we show that 
the language L = {a[n]b(nlc[nll n > 0} is not context-free. 
Suppose that L is context-free with L = L(f), where f is a Chomsky 
normal form grammar with n variables. Choose k so large that la[klb[klc[kll 
> 2n (i.e., choose k > 2n /3). Then we would have a[klb[klc[kJ = r 1q1rq2r2 , 
where, setting 
we have X; E L for i = 0, 1, 2, 3, .... In particular, 
Since the elements of L consist of a block of a's, followed by a block of 
b's, followed by a block of c's, we see that q1 and q2 must each contain 
only one of these letters. Thus, one of the three letters occurs neither in q1 
nor in q2 â¢ But since as i = 2, 3, 4, 5, ... , X; contains more and more copies 
of q1 and q2 and since q1q2 =I= 0, it is impossible for X; to have the same 
number of occurrences of a, b, and c. This contradiction shows that L is 
not context-free. 
We have proved 
Theorem 4.2. The language L = {a[n]b(nlc[nlln > 0} is not context-free. 
Exercises 
1. Show that {a[illi is a prime number} is not context-free. 
2. Show that {a[i2lli > 0} is not context-free. 
3. Show that a context-free language on a one-letter alphabet is regular. 

5. Closure Properties 
291 
5. 
Closure Properties 
We now consider for context-free languages, some of the closure proper-
ties previously discussed for regular languages. 
Theorem 5.1. If L 1 , L 2 are context-free languages, then so is L 1 u L 2 â¢ 
Proof. 
Let L 1 = L(f1), L 2 = L(f2 ), where f 1 , f 2 are context-free gram-
mars with disjoint sets of variables 7 1 and 7 2 , and start symbols S1 , 
S2 , respectively. Let r be the context-free grammar with variables 
~ u 7 2 u {S} and start symbol S. The productions of f are those of f 1 
and f 2 , together with the two additional productions S ~ S 1 , S ~ S2 â¢ 
Then obviously L(f) = L(f1) U L(f2 ), so that L 1 U L 2 = L(f). 
â¢ 
Surprisingly enough, the class of context-free languages is not closed 
under intersection. In fact, let f 1 be the context-free grammar whose 
productions are 
S ~ Sc, 
S ~ Xc, 
X ~ aXb, 
X ~ ab. 
Then clearly, 
L 1 = L(f1) = {alnlb[nlclmJ In, m > 0}. 
Now, let f 2 be the grammar whose productions are 
S ~ aS, 
S ~ aX, 
X ~ bXc, 
X ~ be. 
Then 
L 2 =L(f2 ) = {almJb[nJc[nJin,m > 0}. 
Thus, L 1 and L 2 are context-free languages. But 
Ll n Lz = {aln)b[nJctnJ I n > 0}' 
which, by Theorem 4.2, is not context-free. We have proved 
Theorem 5.2. There are context-free languages L 1 and L 2 such that 
L 1 n L 2 is not context-free. 
Corollary 5.3. There is a context-free language L ~A* such that A*- L 
is not context-free. 
Proof. Suppose otherwise, i.e., for every context-free language L ~A*, 
A* - L is also context-free. Then the De Morgan identity 
L 1 n L 2 =A* -((A* - L 1) n (A* - L 2 )) 
together with Theorem 5.1 would contradict Theorem 5.2. 
â¢ 

292 
Chapter 10 Context-Free Languages 
Although, as we have just seen, the intersection of context-free lan-
guages need not be context-free, the situation is different if one of the two 
languages is regular. 
Theorem 5.4. If R is a regular language and L is a context-free language, 
then R n L is context-free. 
Proof. Let A be an alphabet such that L, R s;;;A*. Let L = L(f) or 
L(f) u {0}, where r is a positive context-free grammar with variables 'J/, 
terminals A and start symbol S. Finally, let L be a dfa that accepts R 
with states Q, initial state q1 E Q, accepting states F s;;; Q, and transition 
function 8. Now, for each symbol u E A u 'J/ and each ordered pair 
p, q E Q, we introduce a new symbol uPq. We shall construct a positive 
context-free grammar f whose terminals are just the elements of A (i.e., 
the terminals of f) and whose set of variables consists of a start symbol S 
together with all of the new symbols uPq for u E A u 'J/ and p, q E Q. 
(Ncte that for a E A, a is a terminal, but aPq is a variable for each 
p, q E Q.) The productions of f are as follows: 
1. S ~ SM for all q E F. 
2. XPq ~ uf'1u{1' 2 â¢â¢â¢ un'n-lq for all productions X~ ul Uz ... un of 
f and all p,r1,r2, ... ,rn-lâ¢q E Q. 
3. aPq ~ a for all a E A and all p, q E Q such that 8(p, a) = q. 
We shall now prove that L(f) = L(f) n R. Since f is clearly a positive 
context-free grammar, and since R n L = L(t} or R n L = L(f) u {0}, 
the theorem follows from Theorem 1.2. 
First let u = a1a2 Â·Â·Â· an E L(f) n R. Since u E L(f), we have 
Using productions 1 and 2 of f, we have 
(5.1) 
where q2, q3, ... , qn are arbitrary states of L and qn + 1 is any state in F. 
(q1 is of course the initial state.) But since u E L(.L), we can choose the 
states q2,q3, ... ,qn+l so that 
i = l,2, ... ,n, 
(5.2) 
and qn+ 1 E F. In this case, not only does (5.1) hold, but also the produc-
tions 
i = l,2, ... ,n, 
(5.3) 

5. Closure Properties 
all belong to f. Hence, finally, 
Conversely, let S ~ u E A*. We shall need the following 
r 
293 
Lemma. Let uPq ~ u E A*. Then, 8*(p, u) = q. Moreover, if u is a 
r 
variable, then u =f u. 
Since S f Sq,q ? u where q E F, we can use the Lemma to conclude 
that 8*(q1 , u) = q, and S =f u. Hence, u E R n L(f). Theorem 5.4 then 
follows immediately. 
â¢ 
It remains to prove the Lemma. 
Proof of Lemma. The proof is by induction on the length of a derivation of 
u from uPq in f. If that length is 2, we must have u E A, u = u. Then, 
8*(p, u) = 8(p, u) = q. Otherwise we can write 
where we have written r 0 = p and rn = q. Thus, we have 
i = l,2, ... ,n, 
(5.4) 
where u = u1u 2 ... un. Clearly, the induction hypothesis can be applied to 
the derivations in (5.4) so that 8*(r;_ 1 , u) = r;, i = 1, 2, ... , n. Hence 
8*(p, u) = rn = q. Also, if u; is a variable, the induction hypothesis will 
give u; =f u;, while otherwise u; E A and u; = u;. Finally, 
must be a production of f. Hence, we have 
â¢ 
Let A, P be alphabets such that P ~A. For each letter a E A, let us 
write 
if 
a E P 
if 
a E A- P. 

294 
Chapter 1 0 Context-Free lanQuages 
In other words, Erp{x) is the word that results from x when all the 
symbols in it that are part of the alphabet P are "erased." If L ~A*, we 
also write 
Erp(L) = {Erp(x) I x E L}. 
Finally, if r is any context-free grammar with terminals T and if P ~ T, 
we write Erp{f) for the context-free grammar with terminals T- P, the 
same variables and start symbol as r, and productions 
for each production X~ v of r. [Note that even if r is a positive 
context-free grammar, Erp{f) may not be positive; that is, it is possible 
that Erp(v) = 0 even if v -=1= 0.] We have 
Theorem 5.5. If r is a context-free grammar and f' = Erp{f), then 
L(f) = Erp{L(f)).3 
Proof. 
Let S be the start symbol off and f'. Suppose that w E L(f). We 
have 
S =WI t Wz â¢â¢â¢ t Wm = W. 
Let V; = Erp(w), i = 1, 2, ... , m. Then clearly, 
s = vl =:> Vz ... =:> vm = Erp(w), 
r 
r 
so that Erp(w) E L{f'). This proves that L{f');;2 Erp(L(f)). 
To complete the proof it will suffice to show that whenever X ~ v E 
r 
(T- P)*, there is a word wET* such that X =f wand v = Erp(w)._We 
do this by induction on the length of a derivation of v from X in r. If 
X f v, then X~ v is a production of f', so that X~ w is a production of 
r for some w with Erp{w) = v. Proceeding inductively, let there be a 
derivation of v from X in f' of length k > 2, where the result is known 
3 Readers familiar with the terminology may enjoy noting that this theorem states that the 
"operators" Land Erp commute. 

5. Closure Properties 
295 
for all derivations of length < k. Then, we can write 
where u0 , u1 , â¢â¢â¢ , us E (T- P)* and V1 , V2 , â¢â¢â¢ , V. are variables. Thus, 
there are words u0, u1 , â¢â¢â¢ , usE T* such that u; = Erp(u;), i = 0, 1, ... , s, 
and 
is a production of r. Also we can write 
where 
i = 1, ... ' s. 
(5.5) 
Since (5.5).clearly involves derivations of length < k, the induction hypoth-
esis applies, and we can conclude that there are words V; E T*, i = 
1, 2, ... , s, such that v; = Er p(v) and V; =f v;, i = 1, 2, ... , s. Hence, we 
have 
But 
which completes the proof. 
â¢ 
Corollary 5.6. If L ~A* is a context-free language and P ~A, then 
Erp(L) is also a context-free language. 
Proof. Let L = L(f), where f is a context-free grammar, and let f = 
Erp(f). Then, by Theorem 5.5, Erp(L) = L(f), so that Erp(L) is context-
free. 
â¢ 
Exercises 
1. For each of the following, give languages L 1 , L 2 on alphabet {a, b} 
such that 
(a) L 1 , L 2 are context-free but not regular, and L 1 U L 2 is regular; 
(b) L 1 , L 2 are context-free, L 1 =/= L 2 , and L 1 U L 2 is not regular; 

296 
Chapter 1 0 Context-Free Languages 
(c) 
L 1 , L 2 are context-free but not regular, L 1 n L 2 =/= 0, and L 1 n L 2 
is regular; 
(d) 
L 1 , L 2 are context-free but not regular, L 1 =/= L 2 , and L 1 n L 2 is 
context-free but not regular. 
2. Let L, L' be context-free languages. Prove the following. 
(a) 
L Â· L' is context-free. 
(b) 
L* is context-free. 
(c) 
L R = {wR I w E L} is context-free. 
3. Give languages R, L 1 , L 2 on alphabet {a, b} such that R is regular, 
L 1 , L 2 are context-free but not regular, and 
(a) R n L 1 is regular; 
(b) 
R n L 2 is not regular. 
4. Give a context-free language L on alphabet A ={a, b} such that L is 
not regular and A* - L is context-free. 
5. Let R = {almlblnJ I m ~ 0, n > 0}, L = {alnlblnJ I n > 0}. Use the con-
struction in the proof of Theorem 5.4 to give a grammar f such that 
L(f) = R n L. 
6. Give alphabets A, P such that P =/= 0, P ~A, P =/=A, and give lan-
guages L 1 , L 2 ~A* such that 
(a) 
L 1 is not context-free and Erp(L 1) is regular; 
(b) 
L 2 is context-free and Erp(L 2 ) is not regular. 
7. Prove that if L ~ A* is regular and P ~ A, then Er p( L) is also 
regular. 
8. Let A 1 , A 2 be alphabets, let L ~ Aj be context-free, let f be a 
substitution on A 1 such that f(a) ~A~ is context-free for all a E A 1 , 
and let g be a homomorphism from Aj to A~ . [See Exercise 4.5 in 
Chapter 9 for the definitions of substitution and homomorphism.] 
(a) Prove that f(L) is context-free. 
(b) Prove that g(L) is context-free. 
9. 
Let A 1 = {a1 , â¢â¢â¢ , an}, let L ~ Aj be context-free, and let R ~ Aj be 
regular. 
(a) Let A 2 = {a'1 , â¢â¢â¢ , a~}, where A 1 n A 2 = 0, and let f be a 
substitution on A 1 such that f(a;) ={a;, a~}, 1 ~ i ~ n. Show 
that A~ Â· R n f(L) is context-free. [See Exercise 8.] 
(b) Let g be the homomorphism on A 1 U A 2 such that g(a;) = 0 
and g(a:) =a;, 1 ~ i ~ n. Show that 
g(A~ Â· R n f(L)) is 
context-free. 

6. Solvable and Unsolvable Problems 
297 
(c) 
Show that g(A~ Â·R nf(L)) = LjR, the right quotient of L by 
R. [See Exercise 7.13 in Chapter 9 for the definition of right 
quotient.] 
{d) Conclude that if L is context-free and R is regular, then L/R is 
context-free. 
*6. Solvable and Unsolvable Problems4 
Let f be a context-free grammar with terminals T and start symbol S, let 
u E T*, and let us consider the problem of determining whether u E L(f). 
First let u = 0. Then we can use the algorithms provided in Section 1 to 
compute ker (f). Since 0 E L(f) if and only if S E ker (f), we can answer 
the question in this case. For u =/= 0, we use Theorems 1.2 and 3.1 to 
obtain a Chomsky normal form grammar a such that u E L(f) if and only 
if u E L(a). To test whether u E L(a), we use the following: 
Lemma. Let a be a Chomsky normal form grammar with terminals T. 
Let V be a variable of a and let 
v =i> u E T*. 
Then there is a derivation of u from V in a of length 2lul. 
Proof. The proof is by induction on lui. If lui = 1, then u is a terminal 
and a must contain a production V ~ u, so that we have a derivation of u 
from V of length 2. 
Now, let V =:f u, where lui > 1, and let us assume the result known for 
all strings of length < lui. Recalling the definition of a Chomsky normal 
form grammar, we see that 
V=XY~u. 
Thus, we must have X~ v, Y ~ w, u = vw where I vi, lwl <lui. By the 
induction hypothesis we have derivations 
X= a 1 = a 2 = 
Y = {31 = f3z = 
Hence, we can write the derivation 
= a21vl = V, 
= f3zlwl = w. 
V=XY= a1Y= a2Y= Â·Â·Â· = a 21v1Y= v{31 = v{32 = Â·Â·Â· = v{321w 1, 
4 The â¢ does not refer to the material through Theorem 6.4. 

298 
Chapter 1 0 Context-Free Languages 
where v/321w1 = vw = u. But this derivation is of length 2lvl + 2lwl = 2lul, 
which completes the proof. 
â¢ 
Now to test u E L(/l), we simply write out all derivations from S of 
length 2lul. We have u E L(/l) if and only if at least one of these 
derivations terminates in the string u. 
We have proved 
Theorem 6.1.5 
There is an algorithm that will test a given context-free 
grammar r and a given word u to determine whether u E L(f). 
Next we wish to consider the question of whether a given context-free 
grammar generates the empty language 0. Let r be a given context-free 
grammar. We first check as previously to decide whether 0 E L(f). If 
0 E L(f), we know that L(f) =/= 0. Otherwise we us Theorems 1.2 and 1.5 
to obtain a branching context-free grammar f such that L(f) = L(f). Let 
f have n variables and set of terminals T. Suppose that L(f) =I= 0. Let 
u E L(f), where u has the shortest possible length of any word in L(f). 
Then in any derivation tree for u in f, each path contains fewer than n + 2 
nodes. This is because, if there were a path containing at least n + 2 
nodes, at least n + 1 of them would be labeled by variables, and by the 
pigeon-hole principle, Theorem 1.6 would apply and yield a word v E L(f) 
with lvl <lui. Thus, we conclude that 
L(f) =/= 0 if and only if there is a derivation tree Yin f of a word 
u E T* such that each path in Y contains fewer than n + 2 nodes. 
It is a straightforward matter (at least in principle) to write out 
explicitly all derivation trees in f in which no path has length ~ n + 2. To 
test whether L(f) =/= 0, it suffices to note whether there is such a tree Y 
for which (Y) E T*. Thus we have 
Theorem 6.2. There is an algorithm to test a given context-free grammar 
f to determine whether L(f) = 0. 
Next we seek an algorithm to test whether L(f) is finite or infinite for a 
given context-free grammar f. Such an algorithm can easily be obtained 
from the following. 
5 This result follows at once from Theorem 5.4 in Chapter 7; but the algorithm given here 
is of some independent interest. 

6. Solvable and Unsolvable Problems 
299 
Theorem 6.3. Let r be a Chomsky normal form grammar with exactly n 
variables. Then L(f) is infinite if and only if there is a word x E L(f) 
such that 
2n < lxl ~ 2n+ 1â¢ 
Proof. If there is a word X E L(f) with lxl > 2n, then by Bar-Hillel's 
pumping lemma, L(f) is infinite. 
Conversely, let L(f) be infinite. Let u be a word of shortest possible 
length such that u E L(f) and lui > 2n+ 1. By Bar-Hillel's pumping lemma, 
we have u = r1q1rqzrz where q1q2 =/= 0, lq1rqzl ~ 2n and X= r1rrz E L(f). 
Now, 
lxl ~ lr1r21 = lui - lq1rqzl > 2n. 
Since lxl <lui, the manner in which we chose u guarantees that lxl ~ 2n+ 1 . â¢ 
Theorem 6.4. There is an algorithm to test a given context-free grammar 
f to determine whether L(f) is finite or infinite. 
Proof. 
Given context-free grammar r with terminals T, we use the 
algorithms of Theorems 1.2 and 3.1 to construct a Chomsky normal form 
grammar Ll with L(f) = L(Ll) or L(Ll) u {0}. Let Ll have n variables and 
let I= 2n. Then we simply use Theorem 6.1 to test each word u E T* for 
which I <lui ~ 21 to see whether u E L(f). L(f) is infinite if and only if 
at least one of these words u does belong to L(f). 
â¢ 
Remarkably enough, there are also some very simple unsolvable prob-
lems related to context-free grammars.6 The easiest way to obtain these 
results is to associate a pair of context-free grammars with each Post 
correspondence system. 
Thus, suppose we are given the finite set of dominoes: 
~ 
[2] 
i = 1, 2, ... , n, where u;, V; E A* for some given alphabet A. We introduce 
n new symbols C1, Cz, â¢â¢â¢ , Cn and define two context-free grammars fl, f2, 
both of which have as their terminals A u {c I> c 2 ' ... ' c n}. r I has the 
6 The remainder of this section depends on Chapter 7. Readers who have not covered this 
material should move on to Section 7. 

300 
Chapter 1 0 Context-Free Languages 
single variable S1 , its start symbol, and f 2 has S2 as its only variable and 
start symbol. The productions of f 1 are 
i = 1, 2, ... , n, 
and those of r2 are 
i = 1,2, ... ,n. 
Now, the given Post correspondence system has a solution if and only if we 
can have 
UÂ· UÂ· 
â¢â¢â¢ UÂ· = VÂ· VÂ· 
â¢â¢â¢ VÂ· 
1t 
12 
1m 
1t 12 
1m 
for some sequence i1 , i2 , â¢â¢â¢ , im. Moreover, 
L(f1) = {u. UÂ· 
â¢â¢â¢ UÂ· CÂ· 
â¢â¢â¢ CÂ· CÂ·} 
lt 
lz 
lm lm 
lz lt 
and 
L(f2 ) = {VÂ·VÂ· 
â¢â¢â¢ VÂ· CÂ· 
â¢â¢â¢ CÂ· CÂ· }. 
11 12 
1m 
1m 
lz 't 
Thus, we have 
Theorem 6.5. L(f1) n L(f2 ) =F 0 if and only if the given Post correspon-
dence problem has a solution. 
Using Theorem 4.1 in Chapter 7, we conclude 
Theorem 6.6. There is no algorithm to test a given pair of context-free 
grammars f 1 , f 2 to determine whether L{f1) n L(f2 ) = 0. 
Another important unsolvability result about context-free grammars 
concerns ambiguity. 
Definition. A context-free grammar f is called ambiguous if there is a 
word u E L(f) that has two different leftmost derivations in f. If f is not 
ambiguous, it is said to be unambiguous. 
Theorem 6.7. There is no algorithm to test a given context-free grammar 
to determine whether it is ambiguous. 
Proof. 
Once again we begin with a Post correspondence system, and form 
the two context-free grammars rl' r2 used in proving Theorem 6.5. rl and 
f 2 are obviously both unambiguous. Now let r have start symbol Sand all 
of the productions of f 1 and f 2 , together with S ~ S1 and S ~ S2 â¢ Then, 
since the first step of a derivation from s in r involves an irreversible 

7. Bracket Languages 
301 
commitment to either rl or r2' r will be ambiguous just in case 
L(f1) n L(f2 ) =/= 0. By Theorem 6.5 this will be the case if and only if the 
given Post correspondence system has a solution. The result now follows 
again from Theorem 4.1 in Chapter 7. 
â¢ 
Another unsolvability result is given in Exercise 8.16. 
Exercises 
l. Let f 1 be the grammar with productions S ~aS, S ~a, and let f 2 be 
the grammar with productions S ~ SS, S ~a. 
(a) How many derivation trees are there for a161 in f 1? In f 2? 
(b) How many derivations of a141 from S are there in f 1? In f 2? 
{c) 
How many leftmost derivations of a161 from S are there in f 1? In 
rz? 
2. Write a context-free grammar r such that 
L(r) = {alilb[jlclkJ I i = j v j = k}. 
This language is an example of an inherently ambiguous language, i.e., a 
language such that every grammar that generates it is ambiguous. 
Explain why this language is inherently ambiguous. 
3. Give an unambiguous context-free grammar that generates the same 
language as the ambiguous grammar 
7. Bracket Languages 
S ~aB 
S ~Ab 
A ~aAB 
B~ABb 
A ~a 
B ~b. 
Let A be some finite set. Although we think of A as an alphabet, we will 
also wish to permit A = 0. Let B be the alphabet we get from A by 
adjoining the 2n new symbols l , 1, i = 1, 2, ... , n, where n is some given 
positive integer. We will write PARiA) for the language consisting of all 
strings in B* that are correctly "paired," thinking of each pair l , 1 as 
matching left and right brackets. More precisely, PARn(A) = L(f0), where 

302 
Chapter 10 Context-Free Languages 
f 0 is the context-free grammar with the single variable S, terminals B, and 
the productions 
1. S ---+ a for all a E A, 
2. s ---+ isL i = 1, 2, ... , n. 
3. s ---+ ss' s ---+ 0. 
The languages PARn(A) are called bracket languages. 
Let us consider the example A = {a, b, c}, n = 2. For ease of reading we 
will use the symbol ( for 
~ , ) for 
~ , [ for 
~ , and ] for 
~ . Then 
cb[(ab)c](a[b]c) E PAR 2(A), as the reader should easily verify. Also, 
{)[] E PARz{A), since we have 
S = SS ~ (S)[S] ~ ()[ ]. 
Bracket languages have the following properties. 
Theorem 7.1. PARn{A) is a context-free language such that 
a. 
A* ~ PARn(A); 
b. 
if x, y E PARn(A), so is xy; 
c. 
if x E PARn(A), so is ~x), for i = 1, 2, ... , n; 
d. 
if x E PARn(A) and x f/:. A*, then we can write x = u/v/w, for 
some i = 1, 2, ... , n, where u E A* and v, w E PARn(A). 
Proof. Since PARn(A) = L(f0) where f 0 is a context-free grammar, 
PARn(A) must be context-free. Property a follows at once on considering 
the productions 1 and 3. Forb, let S ~ x, S ~ y. Then using the produc-
tions 3, we have 
s = ss ~xy. 
For c, let S ~ x. Then using the productions 2, we have 
s = .<s.> ~.<x.> 
l 
l 
l 
l. 
To prove d, note first that we can assume lxl > 1 because otherwise 
x E A*. Then, a derivation of x from S must begin by using a production 
containing Son the right. We proceed by induction assuming the result for 
all strings of length < lxl. There are two cases. 
Case 1. 
S =/S/ ~/v/ = x, where S ~ v; the result then follows 
(without using the induction hypothesis) with u = w = 0. 
Case 2. 
S = SS ~ rs = x where S ~ r, S ~ s, and r =/= 0, s =/= 0. Clearly, 
lrl, lsi< lxl. If rEA*, then lsi> 1 and we can use the induction 
hypothesis to write s = ulv/w, where u E A* and v, w E 

7. Bracket Languages 
303 
PARn(A), and the desired result follows since ru E A*. Other-
wise, we can use the induction hypothesis to write r = u~v/w 
where u E A* and v, w E PARn(A), so that the result follows 
since ws E PARn(A) by b. 
â¢ 
Historically, the special case A = 0 has played an important role in 
studying context-free languages. The language PARn(0) is called the Dyck 
language of order n and is usually written Dn. 
Now let us begin with a Chomsky normal form grammar r, with 
terminals T and productions 
i = 1,2, ... ,n, 
(7.1) 
in addition to certain productions of the form V ~a with a E T. We will 
construct a new grammar rs which we call the separator of r. The 
terminals of rs are the symbols of T together with 2n new symbols ~ 'L 
i = 1, 2, ... , n. Thus a pair of "brackets" has been added for each of the 
productions (7.1). 
The productions of fs are 
i = 1,2, ... ,n, 
as well as all of the productions of r of the form V ~ a with a E T. 
As an example, let f have the productions 
s~XY, 
s ~rr, 
Y~zz, 
x~a, 
z~a. 
Then r is ambiguous as we can see from the leftmost derivations: 
S = XY = aY = aZZ = aaZ = aaa, 
S = IT = ZZX = aZX = aaX = aaa. 
The productions of fs can be written 
S ~ (X)Y, 
S ~ [Y]X, 
Y ~ {Z}Z, 
x~a, 
z~a, 
using ( ), [ ], and {} in place of the numbered brackets. The two derivations 
just given then become 
S = (X)Y = (a)Y = (a){Z}Z = (a){a}Z = (a){a}a, 
S = [Y]X = [{Z}Z]X = [{a}Z]X = [{a}a]X = [{a}a]a. 
fs thus separates the two derivations in f. The bracketing in the words 
(a){a}a, [{a}a]a enables their respective derivation trees to be recovered. 

304 
Chapter 10 Context-Free Languages 
If we write P for the set of brackets ~ , l, i = 1, 2, ... , n, then clearly 
f = Erp(f.). Hence by Theorem 5.5, 
Theorem 7.2. Erp(L(f.)) = L(f). 
We also will prove 
Lemma 1. 
L(r:.) ~ PARn(T). 
Proof. We show that if X'{:: w E (T uP)* for any variable X, then 
w E PARn(T). The proof is by induction on the length of a derivation of w 
from X in r .. If this length is 2, then w is a single terminal and the result 
is clear. Otherwise we can write 
where Y; ~ u and Z; ~ v. By the induction hypothesis, u, v E PARn(T). 
Is 
rs 
By band c of Theorem 7.1, so is w. 
â¢ 
Now let a be the grammar whose variables, start symbol, and terminals 
are those of r. and whose productions are as follows: 
1. all productions v---+ a from r (or equivalently r.) with a E T, 
2. all productions X; ---+ ~ Y;, i = 1, 2, ... , n, 
3. all productions V---+ a)Z;, i = 1, 2, ... , n, for which V---+ a is a pro-
duction of f with a E T. 
We have 
Lemma 2. 
L(a) is regular. 
Proof. Since a is obviously right-linear, the result follows at once from 
Theorem 2.5. 
â¢ 
Lemma 3. 
L(f.) ~ L(a). 
Proof. We show that if X'{:: u E (T u P)* then X 'i> u. If u has a 
derivation of length 2, then u E T, and X ---+ u is a production of r. and 
of f and therefore also of a. Thus X =i> u. 
Proceeding by induction, let 
X = X ==> .< y.>z. ~ < v>w = u 
l rs l 
ll 
I fs l 
I 
' 

7. Bracket Languages 
305 
where the induction hypothesis applies to Y; 'f;: v and to Z; 'f;: wo Thus, 
Y; 'i> v and Z; 'i> Wo Let v = za, a E To (See Exercise 30) Then, examining 
the productions of the grammar A, we see that we must have 
Y ~zV=za = v, 
I 
4 
4 
where V ~a is a production of fo But then we have 
â¢ 
Lemma 4. 
L(A) n PARn(T) ~ L(fJ 
Proof. 
Let X 'i> u, where u E PARn(T)o We shall prove that X it Uo 
The proof is by induction on the total number of occurrences of the 
symbols~,] in u. If this number is 0, then, examining the productions of A, 
we see that u E T and the production X ~ u is in A and hence in r. 0 
Thus X it Uo 
Now let X 'i> u, where u contains occurrences of the bracket symbols 
~ , 1 and where the result is known for words v containing fewer occur-
rences of these symbols than Uo Examining the productions of A, we see 
that our derivation of u from X must begin with one of the productions 20 
(If the derivation began with a production of the form 1, then u would be 
a terminal. If the derivation began with a production of the form 3, then 
u = a~w for some word w, which is impossible by Theorem 7o1do) There-
fore u =~z, for some word z and some i = 1, 2, 0 
0 0, no By Theorem 7o1d, 
u =~v]w, where v, w E PARn(T)o In our derivation of u in A, the symbol] 
can only arise from the use of one of the productions of the form 3, say, 
V ~ a)Z;, where a E T and V ~ a is a production of fo Then v must end 
in a, so that we can write v = ua, where 
X 
X 
<y â¢ <-v 
<- >z â¢ < > 
= ;=;; i 'f;V 
=;;vai ;'f;V;W 
and Z; 'i> Wo Moreover, since V ~a is a production of f, it is also one of 
the productions of A of the form 1. Therefore, we have in A 
y; =i> vv=; ua = vo 
Since v and w must each contain fewer occurrences of~ , l than u, we have 
by the induction hypothesis 
y; 'f v, 
s 
Z; it wo 

306 
Chapter 1 0 Context-Free Languages 
Hence, 
â¢ 
We are now ready to state 
Theorem 7.3. Let r be a grammar in Chomsky normal form with termi-
nals T. Then there is a regular language R such that 
Proof. 
Let a be defined as above and let R = L(a). The result then 
follows at once from Lemmas 1-4. 
â¢ 
Theorem 7.4 (Chomsky-Schiitzenherger Representation Theorem). A 
language L ~ T* is context-free if and only if there is a regular language 
R and a number n such that 
L = Erp(R n PARn(T)), 
where P = <Lll i = 1,2, ... ,n}. 
(7.2) 
Proof. It is clear by Theorems 7.2 and 7.3 that for every grammar r in 
Chomsky normal form, L = L(r) satisfies (7.2). For an arbitrary context-
free language L, by Theorems 1.2 and 3.1, there is a Chomsky normal 
form grammar r such that 
L = L(r) or L = L(r) U {0}. 
If 
then 
L(r) U {0} = Erp((R U {0}) n PARn(T)) 
since, by Theorem 7.1a, 0 E PARn(T). But, by Theorems 4.2 and 4.5 in 
Chapter 9, R u {0} is a regular language. 
It remains only to show that any language L that satisfies (7.2) must be 
context-free. But since, by Theorem 7.1, PARn(T) is context-free, this 
result follows at once from Theorem 5.4 and Corollary 5.6. 
â¢ 
The Chomsky-Schiitzenberger theorem is usually expressed in terms of 
the Dyck languages Dn = PARn(0). Since our form of the theorem is 
equivalent to the more usual form, we will give only a very brief sketch of 
the proof of the usual form. It is necessary to go back to the construction 

7. Bracket Languages 
307 
of r .. Each element a of T is now thought of as a "left bracket" and is 
supplied with a "twin" a' to act as its corresponding right bracket. A new 
grammar f 1 is then defined to have the same productions X; ~ 
~ Y;~, 
i = 1, 2, ... 'n, as r. but to have productions 
v~aa' 
for each production V ~a of r. Then clearly, L(f1) can be obtained from 
L(f.) by simply replacing all occurrences of letters a E T in words of 
L(f.) by aa'. By replacing a by aa' in productions of the forms 1 and 3 of 
a, we obtain a right linear grammar a' such that 
where T' = {a, a' I a E T}. But in fact L(f1) ~ Dm, where m = n + k and 
there are k letters in T. Thus, 
L(f1 ) = L(a') n Dm. 
Finally letting Q = <L 11 i = 1, 2, ... , n} u {a' I a E T}, we have 
L(f) = ErQ(L(f1)) = ErQ(L(a') n Dm). 
Thus, we get 
Theorem 7.5. A language L is context-free if and only if there is a 
regular language R, an alphabet Q, and an integer m such that 
Exercises 
1. Let A be a finite set of symbols, n a positive integer, and PARn(A) = 
L(f0 ), where f 0 is the grammar given in the definition of PARn(A). 
Show that f 0 is ambiguous. [See Section 6 for the definition of 
ambiguous grammars.] 
2. Let r be the grammar with productions 
and start symbol S. 
(a) Give r .. 
s~xz 
s~XY 
z~sY 
(b) Give a, as defined following Lemma 1, for r. 

308 
{c) 
Show that L{f5 ) 
-=/= PAR3({a, b}). 
(d) Show that L(f.) -=1= L(A.). 
Chapter 1 0 Context-Free Languages 
3. Let f be a grammar in Chomsky normal form with variables 'Y and 
terminals T, and let T u P be the terminals of fs. Prove that for all 
V E 'Y and all w such that V ~ w, w = us for some v E ('YU T uP)* 
rs 
and some s E 'YU T. 
4. 
Let f be a regular grammar, and let f' be the Chomsky normal form 
grammar derived from r by the construction in the proof of Theorem 
3.1. Prove that L{r:) is regular. 
8. 
Pushdown Automata 
We are now ready to discuss the question of what kind of automaton is 
needed for accepting context-free languages. We take our cue from 
Theorem 7.2, and begin by trying to construct an appropriate automaton 
for recognizing L{f5 ), where f is a given Chomsky normal form grammar. 
We know that L{f5 ) = R n PARn(T), where R is a regular language. 
Thus R is accepted by a finite automaton. The problem we need to solve is 
this: what additional facilities does this finite automaton require in order 
to check that some given word belongs to PARn(T)? Those familiar with 
"stacks" and their uses will see at once that what is needed is a "pushdown 
stack" as an auxiliary storage device. Such a device behaves in a last-
in-first-out manner. At each step in a computation with a pushdown stack 
one or both of a pair of operations can be performed: 
1. The symbol at the "top" of the stack may be read and discarded. 
(This operation is called popping the stack.) 
2. A new symbol may be "pushed" onto the stack. 
A stack can be used to identify a string as belonging to PARn(T) as 
follows: For each pair ~ , f, i = 1, 2, ... , n, a special symbol 1; is introduced. 
Now, as our automaton moves from left to right over a string, it pushes 1; 
onto the stack whenever it sees ~ , and it pops the stack, eliminating a 1;, 
whenever it sees f . Such an automaton will successfully scan the entire 
string and terminate with an empty stack just in case the string belongs to 
PARn(T). 
To move toward making these ideas precise, let T be a given alphabet 
and let P = {Lf I i = 1, 2, ... , n}. Let .n = {11 ,Jl' ... ,Jn}, where we have 
introduced a single symbol 1; for each pair ; , f, 1 ~ i ~ n. Let u E 
(T uP)*, say, u = c1c2 â¢â¢â¢ ck, where c1 , c2 , â¢â¢â¢ , ck E T UP. We define a 

8. Pushdown Automata 
sequence yiu) of elements of 0.* as follows: 
y 1(u) = 0 
if 
cj E T 
if 
CÂ· =( 
J 
I 
if c.=> and -v.(u) =1.a 
) 
i 
I J 
I 
' 
309 
for j = 1, 2, ... , k. Note that if cj = l, 'Yj+ 1(u) will be undefined unless 'Yj 
begins with the symbol 1; for the very same value of i. Of course, if a 
particular y,(u) is undefined, all yiu) with j > r will also be undefined. 
Definition. We say that the word u E (T u P)* is balanced if yj(u) is 
defined for 1 ::; j ::; lui + 1 and 'Yiul+ 1(u) = 0. 
The heuristic considerations with which we began suggest 
Theorem 8.1. Let T be an alphabet and let 
P = {l, ll i = 1, 2, ... , n}, 
TnP= 0. 
Let u E (T UP)*, let 0. = {11, 12 , ... , 1n}. Then u E PARn(T) if and only 
if u is balanced. 
The proof is via a series of easy lemmas. 
Lemma 1. If u E T * , then u is balanced. 
Proof. Clearly yiu) = 0 for 1 ::; j ::; lui + 1 in this case. 
â¢ 
Lemma 2. If u and v are balanced, so is uv. 
Proof. Clearly yiuv) = yj(u) for 1 ::; j ::; lui + 1. Since 'Yiul+ 1(u) = 0 = 
'Y!u!+l(uv) = y 1(v), we have 'Y!u!+iuv) = yiv) for 1 ::;j::; lvl + 1. Hence, 
'Y!uv!+ l(uv) = 'Yiul+lvl+ l(uv) = 'Yivl+ l(v) = 0. 
â¢ 
Lemma 3. Let v =/u/. Then u is balanced if and only if v is balanced. 
Proof. We have y 1(v) = 0, y 2(v) = 1;, 'Yj+ 1(v) = yiu)1;, j = 1,2, ... , 
lvl- 1. In particular, y1, 1(v) = y1u1+ 2(v) = 'Y!u!+l(u)1;. Thus, if u is bal-
anced, then 'Yiul+ 1(u) = 0, so that y 101(v) = 1; and 'Yivl+ 1(v) = 0. Con-
versely, if v is balanced, 'Yivl+ 1(v) = 0, so that y 1v1(v) must be 1; and 
'Y1u1+ l(u) = 0. 
â¢ 
Lemma 4. If u is balanced and uv is balanced, then v is balanced. 

310 
Chapter 1 0 Context-Free Languages 
Proof. 
y/uv) = y/u) for 1 ::::;; j ::::;; lui + 1. Since Yiui+ 1(u) = 0, we have 
Y1u1+/uv) = yi(v) for 1 ::::;;j::::;; lvl + 1. Finally, 
0 = Yiuvi+l(uv) = Yiui+lvi+l(uv) = Ylvi+l(v). 
â¢ 
Lemma 5. If u E PARn(T), then u is balanced. 
Proof. The proof is by induction on the total number of occurrences of 
the symbols ~ , f in u. If this number is 0, then u E T*, so by Lemma 1, u is 
balanced. 
Proceeding by induction, let u have k > 0 occurrences of the symbols 
~ , f, where the result is known for all strings with fewer than k occurrences 
of these symbols. Then, by Theorem 7.1d, we can write u = vlw[z, where 
v, w, z E PARn(T). By the induction hypothesis, v, w, z are all balanced, 
and by Lemmas 2 and 3, u is therefore balanced. 
â¢ 
Lemma 6. If u is balanced, then u E PARn(T). 
Proof. If u E T*, the result follows from Theorem 7.1a. Otherwise, we 
can write u = xy, where x E T * and the initial symbol of y is in P. By the 
definition of yj(u), we will have yj(u) = 0 for 1 ::::;; j ::::;; lxl + 1. Therefore, 
the initial symbol of y cannot be one of the f. Thus we can write u = x~z, 
and Ylxi+ 2(u) = 1;. Since u is balanced, Y[ui+ 1(u) = 0, and we can let k be 
the least integer > lxl + 1 for which Yk(u) = 0. Then yk_ 1(u) = 1; and the 
(k - l)th symbol of u must be f. Thus u = x~vJw, where k = lxl + I vi + 3. 
Thus 0 = Yixi+ivl+ 3(u) = Yixi+ivi+ 3(x~v[). Hence 
x~v[ is balanced. By 
Lemma 4, w is balanced. Since x E T*, x is balanced, and by Lemma 4 
again, / v/ is balanced. By Lemma 3, v is balanced. Since x E T*, x E 
PARn(T). Since I vi, lwl <lui, we can assume by mathematical induction 
that it is already known that v, w E PARn(T). By band c of Theorem 7.1, 
we conclude that u E PARn(T). 
â¢ 
Theorem 8.1 is an immediate consequence of Lemmas 5 and 6. 
We now give a precise definition of pushdown automata. We begin with 
a finite set of states Q = {q1 , â¢â¢â¢ , qm}, q1 being the initial state, a subset 
F ~ Q of final, or accepting, states, a tape alphabet A, and a pushdown 
alphabet .0. (We usually use lowercase letters for elements of A and 
capital letters for elements of .0.) We assume that the symbol 0 does not 
belong to either A or .0 and write A= A u {0), :0: = .0 u {0}. A transition 
is a quintuple of the form 
q;aU: Vqi 
where a E A and U, V E :0:. Intuitively, if a E A and U, V E .n, this is to 
read: "In state q; scanning a, with U on top of the stack, move one square 

8. Pushdown Automata 
311 
to the right, 'pop' the stack removing U, 'push' V onto the stack, and enter 
state qj ." If a = 0, motion to the right does not take place and the stack 
action can occur regardless of what symbol is actually being scanned. 
Similarly, U = 0 indicates that nothing is to be popped and V = 0 that 
nothing is to be pushed. A pushdown automaton is specified by a finite set 
of transitions. The distinct transitions q;aU:Vqj, q;bW:Xqk are called 
incompatible if one of the following is the case: 
1. a = b and U = W; 
2. a = b and U or W is 0; 
3. U = W and a or b is 0; 
4. a or b is 0 and U or W is 0. 
A pushdown automaton is deterministic if it has no pair of incompatible 
transitions. 
Let u E A* and let L be a pushdown automaton. Then a u-configura-
tion for L is a triple A = (k, q;, a), where 1 :s; k :s; lui + 1, q; is a state of 
L, and a E .0*. [Intuitively, the u-configuration (k, q;, a) stands for the 
situation in which u is written on .L's tape, Lis scanning the kth symbol 
of u-or, if k =lui + 1, has completed scanning u-and a is the string of 
symbols on the pushdown stack.] We speak of q; as the state at configura-
tion A and of a as the stack contents at configuration A. If a = 0, we say 
the stack is empty at A. For a pair of u-configurations, we write 
u: (k,q;,a) r-,(l,qj,{3) 
if L contains a transition q;aU:Vqj, where a= Uy, {3 = Vy for some 
y E .0*, and either 
1. I = k and a = 0, or 
2. I= k + 1 and the kth symbol of u is a. 
Note that the equation a = Uy is to be read simply a = y in case U = 0; 
likewise for {3 = Vy. 
A sequence A1 , A2 , â¢â¢â¢ , Am of u-configurations is called a u-computa-
tion by L if 
1. A1 = (1, q, 0) for some q E Q, 
2. Am =(lui+ 1, p, y) for some p E Q and y E .0*, and 
3. u: A; r-, Ai+l for 1 :s; i < m. 
This u-computation is called accepting if the state at A1 is the initial state 
q1 , the state pat Am is in F, and the stack at Am is empty. We say that L 
accepts the string u E A* if there is an accepting u-computation by .L. We 
write L(L) for the set of strings accepted by L, and we call L(.L) the 
language accepted by L. 

312 
Chapter 10 Context-Free Languages 
Acceptance can alternatively be defined either by requiring only that the 
state at Am is in F or only that y = 0. It is not difficult to prove that the 
class of languages accepted by pushdown automata is not changed by 
either of these alternatives. (See Exercise 8.) 
A few examples should provide readers with some intuition for working 
with pushdown automata. 
Example L 1 
Tape alphabet= {a, b}, pushdown alphabet ={A}, Q = 
{q1 , q2}, F = {q2}. The transitions are 
q1a0: Aq1 
q1bA: Oq2 
q2bA:Oq2 â¢ 
The reader should verify that L(L1) = {alnlblnll n > 0}. 
Example L 2 Tape alphabet= {a, b, c}, pushdown alphabet= {A, B}, 
Q = {q1 , q2}, F = {q2}. The transitions are 
q1a0: Aq1 
q1b0: Bq1 
q 1c0: Oq2 
qzaA:Oqz 
q2bB: Oq2 â¢ 
Here, L(L2) = {ucuR I u E {a, b}*}. 
Example L 3 
Tape alphabet = {a, b}, pushdown alphabet = {A, B}, Q = 
{q, 'qz}, F = {qz}, 
q1a0: Aq1 
q1b0: Bq1 
q1aA: Oq2 
q1bB: Oq2 
qzaA: Oqz 
q2bB: Oq2 â¢ 
In this case, L(L3) = {uuR I u E {a, b}*, u =F 0}. Note that while L 1, L 2 
are deterministic, L 3 is a nondeterministic pushdown automaton. Does 
there exist a deterministic pushdown automaton that accepts L(L3)? Why 
not? 

8. Pushdown Automata 
313 
L(L1), L(L2 ), and L(L3) are all context-free languages. We begin our 
investigation of the relationship between context-free languages and push-
down automata with the following theorem. 
Theorem 8.2. Let f be a Chomsky normal form grammar with separator 
r. . Then there is a deterministic pushdown automaton L 
such that 
L(L) = L(f.). 
Proof. Let T be the set ofterminals of r. By Theorem 7.3, for suitable n, 
L(f.) = R n PARn(T), 
where R is a regular language. Let P = {~, ll i = 1, 2, ... , n}. Let .10 be a 
dfa with alphabet T u P that accepts R. Let Q = {q1 , â¢â¢â¢ , qm} be the 
states of .10 , q 1 the initial state, F ~ Q the accepting states, and l> the 
transition function. We construct a pushdown automaton L 
with tape 
alphabet T u P and the same states, initial state, and accepting states as 
L 0 , Lis to have the pushdown alphabet !1 = {11 , â¢â¢â¢ , In}. The transitions 
of L are as follows for all q E Q: 
a. for each a E T, qaO: Op, where p = l>(q, a); 
b. fori= 1,2, ... ,n,qf0: l;p;, where P; = l>(q,~); 
c. for i = 1, 2, ... , n, qJ 1;: Op;, where P; = l>(q, ]). 
Since the second entry in these transitions is never 0, we see that for any 
u E (T uP)*, a u-computation must be of length lui + 1. It is also clear 
that no two of the transitions in a -c are incompatible; thus, L 
is 
deterministic. 
Now, let u E L(f.), u = c1c2 â¢â¢â¢ cK, where c1 , c2 , ... , cK E (T UP). 
Since u E R, the dfa L 0 accepts u. Thus, there is a sequence 
p 1, p 2 , â¢â¢. , PK+ 1 E Q such that p 1 = q1, PK+ 1 E F, and l>(p;, c;) =Pi+ 1 , 
i = 1, 2, ... , K. Since u E PARn(T), by Theorem 8.1, u is balanced, so that 
yiu) is defined for j = 1, 2, ... , K + 1 and 'YK + 1(u) = 0. We let 
Iii= (j,pi,yi(u)), 
j = 1,2, ... ,K+ 1. 
To see that the sequence !l.1 , !l. 2 , â¢â¢â¢ , !l. K + 1 is an accepting u-computation 
by L, it remains only to check that 
j=1,2, ... ,K. 
But this clear from the definition of yiu). 
Conversely, let L accept u = c1c2 â¢â¢â¢ cK. Thus, let !l.1 , !l. 2 , â¢â¢â¢ , !l.K+ 1 be 
an accepting u-computation by L. Let 
j = 1,2, ... ,K+ 1. 

314 
Chapter 1 0 Context-Free Languages 
Since 
j = 1,2, ... ,K, 
and y 1 = 0, we see that yi satisfies the defining recursion for yi(u) and 
hence, 'Yj = r/u) for j = 1, 2, ... ' K + 1. Since 'YK +I = 0, u is balanced 
and hence u E PARn(T). Finally, we have p 1 = qp PK+ 1 E F, and 
8(pi, ci) =Pi+ 1. Therefore the dfa L 0 accepts u, and u E R. 
â¢ 
We call a pushdown automaton atomic (whether or not it is determinis-
tic) if all of its transitions are of one of the forms 
i. paO: Oq, 
ii. pOU: Oq, 
iii. pOO: Vq. 
Thus, at each step in a computation an atomic pushdown automaton can 
read the tape and move right, or pop a symbol off the stack or push a 
symbol on the stack. But, unlike pushdown automata in general, it cannot 
perform more than one of these actions in a single step. 
Let L be a given atomic pushdown automaton with tape alphabet T 
and pushdown alphabet .n = {11 , 12 , â¢â¢â¢ , Jn}. We set 
P = {L fl i = 1, 2, ... , n} 
and show how to use the "brackets" to define a kind of "record" of a 
computation by L. Let a1 , a2 , â¢â¢â¢ , am be a (not necessarily accepting) 
v-computation by L, where v = c1c2 â¢â¢â¢ cK and ck E T, k = 1,2, ... ,K, 
and where a; = U;, P;, y;), i = 1, 2, ... , m. We set 
WI= 0 
if 
'Yi+l = Y; } 
if 
'Yi+ I = Jj'Yi 
if 
'Y; = Jj'Yi+ I 
1 ~ i < m. 
[Note that 'Yi+I = Y; is equivalent to /i+I =I;+ 1 and is the case when a 
transition Of form i is used in getting from a; to ai+ I; the remaining twO 
cases occur when transitions of the form iii or ii, respectively, are used.] 
Now let w = wm, so that Erp(w) = v and m = lwl + 1. This word w is 
called the record of the given v-computation a1 , â¢â¢â¢ , am by L. From w we 
can read off not only the word v but also the sequence of "pushes" and 
"pops" as they occur. In particular, w;, 1 < i ~ m, indicates how L goes 
from ai-l to a;. 

8. Pushdown Automata 
315 
Now we want to modify the pushdown automaton L of Theorem 8.2 so 
that it will accept L(f) instead of L(f.). In doing so we will have to give 
up determinism. The intuitive idea is to use nondeterminism by permitting 
our modified pushdown automaton to "guess" the location of the "brac-
kets" /, /. Thus, continuing to use the notation of the proof of Theorem 
8.2, we define a pushdown automaton .ii with the same states, initial state, 
accepting states, and pushdown alphabet as L. However, the tape alpha-
bet of .ii will be T (rather than T u P). The transitions of .ii are, for all 
qEQ: 
a. for each a E T, qaO: Op, where p = 8(q, a) [i.e., the same as the 
transitions a of L]; 
b. fori= 1,2, ... ,n, qOO: l;p;, where P; = 8(q,~); 
c. for i = 1, 2, ... , n, qOl;: Ojj; where P; = 8(q, ]). 
Depending on the transition function 8, .ii can certainly be nondetermin-
istic. We shall prove that L(L) = L(f). Note that .ii is atomic (although 
Lis not). 
First, let v E L(f). Then, since Er/L(f.)) = L(f), there is a word 
wE L(f.) such that Er/w) = v. By Theorem 8.2, wE L(L). Let 
a 1 , a 2 , â¢â¢â¢ , am be an accepting w-computation by L 
(where in fact 
m = lwl + 1). Let 
i = 1,2, ... ,m. 
Let n; = 1 if w: a; 1-_, ai+ I via a transition belonging to group a; 
otherwise n; = 0,1 :::;; i < m. Let 
11 = 1, 
1:::;; i < m. 
Finally let 
i = 1,2, ... ,m. 
Then, as is easily checked, 
1:::;; i < m. 
Since xm = (I vi + 1, q, 0) with q E F, we have v E L(L). 
Conversely, let v E L(L). Let XI' x2' ... ' xm be an accepting v-
computation by .L, where we may write 
i = 1,2, ... ,m. 

316 
Chapter 1 0 Context-Free Languages 
Using the fact that L 
is atomic, we can let w be the record of this 
computation in the sense defined earlier so that Er p( w) = v and m = 
lwl + 1. We write 
!l.; = (i,p;,y), 
and easily observe that 
w: fi; I-.I fii+ I' 
i = 1,2, ... ,m, 
i = 1,2, ... ,m. 
[In effect, whenever L pushes 1; onto its stack, ~ is inserted into w; and 
whenever L pops 1;, ~ is inserted into w. This makes the transitions b, c of 
L behave on w just the way the corresponding transitions of L behave 
on v.] Since Pm E F and 'Ym = 0, !l.1 , !l. 2 , â¢â¢â¢ , lim is an accepting w-compu-
tation by L. Thus, by Theorem 8.2, wE L(f.). Hence v E L(f). 
We have shown that L(f) = L(L). Hence we have proved 
Theorem 8.3. Let f be a Chomsky normal form grammar. Then there is 
a pushdown automaton L such that L(L) = L(f). 
Now let L be any context-free language. By Theorems 1.2 and 3.1 there 
is a Chomsky normal form grammar r such that L = L(f) or L(f) u {0}. 
In the former case, we have shown how to obtain a pushdown automaton 
L such that L = L(L). For the latter case we first modify the dfa L 0 
used in the proof of Theorem 8.2 so that it is nonrestarting. We know that 
this can be done without changing the regular language that L 0 accepts by 
Theorem 4.1 in Chapter 9. By carrying out the construction of a pushdown 
automaton L for which L(L) = L(f) using the modified version of 
L 0 , L will have the property that none of its transitions has q1 as its final 
symbol. That is, L will never return to its initial state. Thus, if we define 
L' to be exactly like L except for having as its set of accepting states 
F' = F u {q1}, 
we see that L(L') = L(L) u {0} = L(f) u {0}. Thus we have proved 
Theorem 8.4. For every context-free language L, there is a pushdown 
automaton L such that L = L(L). 
We will end this section by proving the converse of this result. Thus we 
must begin with a pushdown automaton and prove that the language it 
accepts is context-free. As a first step toward this goal, we will show that 
we can limit our considerations to atomic pushdown automata. 
Theorem 8.5. Let L be a pushdown automaton. Then there is an atomic 
pushdown automaton L such that L(L) = L(L). 

8. Pushdown Automata 
317 
Proof. 
For each transition 
paU: Vq 
of L for which a, U, V =/= 0, we introduce two new states r, s and let L 
have the transitions 
paO: Or, 
rOU: Os, 
sOO: Vq. 
If exactly one of a, U, V is 0, then only two transitions are needed for L. 
Finally, for each transition pOO: Oq, we introduce a new state t and replace 
pOO: Oq with the transitions pOO: Jt, tOJ: Oq, where J is an arbitrary 
symbol of the pushdown alphabet (or a new symbol if the pushdown 
alphabet of L 
is empty). Otherwise, L is exactly like L. Clearly, 
L(L) = L(L). 
â¢ 
Theorem 8.6. For every pushdown automaton L, L(L) is a context-free 
language. 
Proof. Without loss of generality, by using Theorem 8.5 we can assume 
that L is atomic. Let L 
have states Q = {qt, ... , qm}, initial state qt, 
final states F, tape alphabet T, and pushdown alphabet !1 = {Jt, ... , Jn}. 
Let P = {~,]I i = 1, ... , n}. Let L ~ (T u P)* consist of the records of 
every accepting u-computation by L, and let R = L(L0 ), where L 0 is the 
ndfa with alphabet T u P, the same states, initial state, and accepting 
states as L, and transition function 8 defined as follows. For each q E Q, 
l>(q, a) = {p E Q I L has the transition qaO: Op} for a E T, 
t>( q .~) = {p E Q I L has the transition qOO: lip}, i = 1, ... , n, 
t>( q ,/) = {p E Q I L has the transition qOJi: Op}, i = 1, ... , n. 
Let w E L be the record of an accepting u-computation at' ... ' am' 
where ai = (li, Piâ¢ y;), i = 1, ... , m. An easy induction on i shows that 
PiE 8*{qt,w;), i = 1, ... ,m, so, in particular, Pm E l>*{qt,w), which im-
plies wE R, since Pm must be an accepting state. Moreover, another easy 
induction on i shows that 'Yi(w) = 'Yiâ¢ i = 1, ... , m, which implies that 
yi(w) is defined for 1::; i::; lwl + 1 and 'Yiwl+t(w) = 'Yiwl+t = 0 (since 
at, ... , am is accepting), i.e., w is balanced. Therefore, by Theorem 8.1, 
wE R n PARn(T), and soL~ R n PARn(T). 
On the other hand, let w = ct ... c, be a balanced word in R, i.e., 
wE R n PARn(T), let u = dt ... d. be Erp{w), and let PtÂ·Â·Â·Â·â¢Pr+t be 

318 
Chapter 1 0 Context-Free Languages 
some sequence of states such that p 1 = q 1,p,+ 1 E F, and Pi+ IE 8(p;,c;) 
for i = 1, ... , r. We claim that 
where 
if 
C; E T 
otherwise, 
is an accepting u-computation by L and that w is its record. Clearly, we 
have (/1 , p 1 , y1(w)) = (1, q1 , 0), 1,+ 1 =lui + 1, Pr+ 1 E F, and 'Yr+ 1(w) = 0 
(since w is balanced), so we just need to show that 
(8.2) 
fori= 1,.,.,r. For arbitrary i = 1, ... ,r, if 'Y;+ 1(w) = Y;(w), then c; E T, 
so Pi+! E 8(p;,c;), and L has the transition P;C;O:Opi+l" Now, a simple 
induction on i shows that Erp(c1 â¢â¢â¢ c;_ 1) = d1 â¢â¢â¢ d1 _ 1, i = 1, ... , r + 1 
(where c1 â¢â¢â¢ c0 represents 0), from which we can sho~ 
if c; E T then 
d 1 = c; , 
I 
i = 1, ... , r. 
In particular, for any i = 1, ... , r, if c; E T, then 
Erp(c1 â¢â¢â¢ c;) = Erp(c 1 â¢â¢â¢ c;_ 1)c; = d 1 â¢â¢â¢ d1;_ 1c;, 
so c; must be d1 since c; is not deleted when Erp is applied to w. 
Therefore, L has'the transition p;d10:0pi+l and li+ 1 =I;+ 1, so (8.2) is 
satisfied. If, instead, 'Yi+ 1(w) = Jjyi(~) for some j = 1, ... , n, then c; =j, 
so Pi+! E 8(p;,j) and L 
has the transition p;OO: Jjpi+JÂ· Moreover, 
cj ft. T, so I;+ 1 = I;, and (8.2) is satisfied in this case as well. Finally, if 
Y;(w) =lj'Yi+l(w) for some j = 1, ... ,n, then c; =],so Pi+l E 8(p;,]) 
and L has the transition P;Olj:Opi+l" Moreover, li+ 1 = 1;, so again (8.2) 
is satisfied. Therefore, (8.1) is an accepting u-computation by L. If we set 
W; = c1 â¢â¢â¢ c;_ 1 , i = 1, ... , r + 1, then an induction on i shows that w is 
indeed the record of (8.1), so w E L, and we have R n PARiT) ~ L. 
Therefore, L = R n PARn{T), and 
L(L) = Erp(R n PARn(T)). 
Finally, by Theorems 5.4 and 7.1 and Corollary 5.6, L(L) is context-free . â¢ 

8. Pushdown Automata 
319 
Exercises 
1. Let T be an alphabet, P = {}, / I i = 1, ... , n}, w = a1 Â·Â·Â· am E 
PARn(T). Intefers j, k, where 1 5. j < k 5. m, are matched in w if 
w = a1 Â·Â·Â· aj-tiaj+t Â·Â·Â· ak_ 1]ak+t Â·Â·Â·am, for some 1 5. i 5. n, and if 
aj+ I ... ak-1 is balanced. Let r be a Chomsky normal form gram-
mar. 
(a) Let w =l x E L(f.). Prove that there is exactly one k, 1 < k 5. 
lwl, such that 1 and k are matched in w. 
(b) Show that r. is unambiguous. [See Section 6 for the definition of 
ambiguous grammars.] 
2. (a) For pushdown automaton L 1 in the examples, give the accept-
ing u1-computation for u1 = aabb. 
(b) For pushdown automaton L 2 in the examples and u 2 = abcbba, 
give the longest sequence A1 = (1, qp 0), A2 , â¢â¢â¢ , Am of u2-con-
figurations that satisfies condition 3 in the definition of u-com-
putations. 
(c) 
For pushdown automaton L 3 in the examples, give all possible 
u3-computations, accepting or not, for u 3 = aaaa. 
3. For each of the following languages L, give a pushdown automaton 
that accepts L. 
(a) {alnlbl2nll n > 0}. 
(b) 
{alnlblmll 0 < n 5, m}. 
(c) 
{alnlblmJI n =/= m}. 
(d) {alnlblmlalnll m, n > 0} U {alnlclnll n > 0}. 
4. Let L 
be the pushdown automaton with Q = {q1}, F = {q1}, and 
transitions 
What is L(L)? 
q1a0: Aq1 q1aB: Oq1 
q1b0: Bq1 q1bA: Oq1. 
5. Let L 
be the pushdown automaton with Q = {q1 , q2 , q3 , q4 , q5}, 
F = {q5}, and transitions 
q100: Zq2 
q2a0: Aq2 
q2bA:Oq3 
q2bZ:Oq4 
q3bA: Oq3 
q3bZ: Oq4 
q3a0: Oq4 
q30Z: Oq5 
q4a0: Oq4 
q4 b0: Oq4 
q40A:Oq4 
q40Z:Oq4 â¢ 

320 
Chapter 10 Context-Free Languages 
(a) What is L(.L)? 
(b) Prove that for every u E {a, b}*, there is a u-computation by L. 
6. Show that every regular language accepted by a (deterministic) finite 
automaton with n states is accepted by a (deterministic) pushdown 
automaton with n states and an empty pushdown alphabet. 
7. Show that every regular language R is accepted by a pushdown 
automaton with at most two states, and if 0 E R then R is accepted 
by a pushdown automaton with one state. 
8. Let L 
be a pushdown automaton with initial state q1 , accepting 
states F, and tape alphabet A, let u E A*, and let a1 = 
(1, q1 , 0), ... , am = (lui + 1, p, y) be a u-computation by L. We say 
that L accepts u by final state if p E F, and that L accepts u by empty 
stack if y = 0. T(L) = {u E A* I L accepts u by final state}, and 
N(L) = {u E A* I L accepts u by empty stack}. 
(a) Let .L1 , .L2 , L 3 be the pushdown automata from the examples. 
Give T(.ff;), N(.ff;), i = 1, 2, 3. 
(b) Prove that a language L is context-free if and only if L = T(L) 
for some pushdown automaton L. 
(c) 
Prove that a language L is context-free if and only if L = N(L) 
for some pushdown automaton L. 
9. Let L 
be a pushdown automaton with tape alphabet A, and let 
u E A*. An infinite sequence al' a2, .â¢â¢ of u-configurations for Lis 
an infinite u-computation by L if for some n and some x such that 
u = xy for some y, each finite sequence al' â¢.. ' an' ... ' an +m' m ~ 0, 
is an x-computation by L. It is an accepting infinite u-computation if 
a 1 , â¢â¢â¢ , ak is an accepting u-computation by L for some k. 
(a) Give a pushdown automaton L 1 and word u1 such that there is 
a nonaccepting infinite u-computation by L 1 â¢ 
(b) Give a pushdown automaton .L2 and word u2 such that there is 
an accepting infinite u2-computation (/1 , P1> y 1 ), (/2 , p 2 , y 2), ... 
by .L2 where, for some k, p1 is an accepting state for all I~ k. 
(c) 
Give a pushdown automaton L 3 and word u3 such that there is 
an accepting infinite u3-computation (/1 , p 1 , y 1 ), (/2 , p 2 , y 2), ... 
by .L3 where there is no k such that p1 is an accepting state for 
all I ~ k. 
10. Give the incompatible pairs among the following transitions. In each 
case, give the condition(s) 1, 2, 3, or 4 by which the pair is incompati-
ble. 

8. Pushdown Automata 
qiaJI: Oqi 
qibJI: Oqi 
qiaJI: Oqz 
qiaO: lzqi 
q1011: Oqi 
qiblz: liqi 
qiOO: Jiqi 
321 
11. LetT= {a, b}, P = {~, ~ ,~,~}, 0. = {11 , 12}. We will write(,),[,] for 
~,~,~,~,respectively. Give Y;(w), 1 :::;; i:::;; lwl + 1, for each of the 
following. 
(a) 
w = a(b[ba]a)b[a]. 
(b) 
w = (ab[ab)a]. 
{c) 
w = a[b ]]a. 
(d) 
w = (a([b ]a). 
12. Let f be the grammar with productions S ~ SS, S ~a. 
(a) Use the construction in the proof of Theorem 8.2 to give a 
deterministic pushdown automaton that accepts L(f.). 
{b) 
Use the construction in the proof of Theorem 8.3 to give a 
pushdown automaton that accepts L(f). 
13. (a) For pushdown automata L 1 , L 2 , L 3 in the examples, use the 
construction in the proof of Theorem 8.5 to give atomic push-
down automata ~ 
, L 2 , L 3 â¢ 
(b) Answer Exercise 2 for ~ 
, .ii2 , ~. 
14. Let L 
be the pushdown automaton with Q = {q1 , q2}, initial state 
q1 , F = {q2}, tape alphabet {a, b}, pushdown alphabet {A}, and transi-
tions 
q1a0: Oq1 
q 1b0: Oq2 
q 100: Aq1 
q 10A: Oq1 
q2a0: Oq1 
q2b0: Oq2 
q20A: Oq2 
q200: Aq2 â¢ 
Use the constructions in Theorems 8.6 and 5.4 to give a context-free 
grammar f such that L(f) = L(L). 
15. Let us call a generalized pushdown automaton a device that functions 
just like a pushdown automaton except that it can write any finite 
sequence of symbols on the stack in a single step. Show that, for every 
generalized pushdown automaton L, there is a pushdown automaton 
.ii such that L(L) = L(.ii). 
16.* Let 

322 
Chapter 1 0 Context-Free Languages 
be a set of dominoes on the alphabet A. Let B = {c1, ... , ck} be an 
alphabet such that A n B = 0. Let c $. A u B. Let 
R = {ycyR I y E A*B*}, 
L 1 = {u.u. Â·Â·Â· u.c.c. 
Â·Â·Â·c. c.} 
lt 
l2 
ln 
ln 'n-1 
l2 lt' 
L 2 = {u. VÂ· 
â¢â¢â¢ VÂ· CÂ· CÂ· 
â¢â¢â¢ CÂ· CÂ·} 
11
12 
1n 1n 
1n-l 
12 
11' 
sp = {yczR I y E Ll 'z E Lz}. 
Recall that by Theorem 6.5, the Post correspondence problem P has 
a solution if and only if L 1 n L 2 * 0. 
(a) Show that the Post correspondence problem P has no solution 
if and only if R n S P = 0. 
(b) Show that (A u B u {c})* - R and (A u B u {c})* - Sp are 
both context-free. [Hint: Construct pushdown automata.] 
(c) 
From (a) and (b) show how to conclude that there is no algo-
rithm that can determine for a given context-free grammar r 
with terminals T whether L(f) u {0} = T*. 
(d) Now show that there is no algorithm that can determine for a 
given context-free grammar rl and regular grammar r2 whether 
(i) L(f1) = L(f2 ), 
(ii) L(f1) ~ L(f2 ). 
17.* Let L be a pushdown automaton with Q = {q1 , â¢â¢â¢ , qm}, tape alpha-
bet A = {a1 , â¢â¢â¢ , an}, and pushdown alphabet .0 = {11 , â¢â¢â¢ , 11}, and 
let p,p' E Q,1,1' En. A sequence (l,p!,'YJ), ... ,(1,pk,yk) ofO-
configurations for L is a reaching sequence by L from (p, J) to 
(p',1') if p 1 = p, y 1 = 1, Pk = p', 'Yk = 1'8 for some 8 E .0*, IY;I > 0 
for 1 :::;; i:::;; k, and 0: (l,p;, y;) I-..,. (1, Pi+ 1 , 'Yi+ 1)for 1 :::;; i < k. (p, J) 
is a looping pair of L if there is a reaching sequence by L from 
(p, J) to (p, J). 
(a) Prove that if L 
has a u-computation a 1 , â¢â¢â¢ , ak =(lui + 1 , 
p, 1y) for some looping pair (p, J) of L, then L 
has an 
infinite uw-computation for every w E A*. [See Exercise 9 for 
the definition of infinite u-computations.] 
(b) Prove that if (p, J) is a looping pair for L, then there is a 
reaching sequence a 1 = (1, p, 1), ... , ak = (1, p, 18) by L from 
(p, J) to (p, J) such that 181:::;; lm [Hint: Consider the pigeon-
hole principle and the proofs of the pumping lemmas.] 

9. Compilers and Formal Languages 
323 
(c) 
Prove that if ( p, J) is a looping pair of L, then there is a 
reaching sequence A1 , â¢â¢â¢ , Ak by L from (p, J) to (p, J) with 
k ~ m(l + l)lm + 1. 
(d) Give an algorithm that will determine, for a pushdown automa-
ton Land pair (p, 1), whether or not (p, J) is a looping pair of 
L. 
(e) 
Prove that if L has an infinite u-computation, for some u E A*, 
then L 
has a looping pair. 
(f) 
Suppose now that ,(( is deterministic. Prove that there is a 
deterministic pushdown automaton L' such that 
(i) there is no infinite u-computation by L' for any u E A*; 
(ii) there is a u-computation by L' for every u E A*, and 
(iii) T(L') = T(L). [See Exercise 8 for the definition of 
T(L).] 
(g) 
A language L is a deterministic context-free language if L = T(L) 
for some deterministic pushdown automaton L. Prove that if 
L ~A* is a deterministic context-free language, then A* - L is 
also a deterministic context-free language. 
(h) Show that {alilbUlclkll i =1= j or j =1= k} is a context-free language 
which is not deterministic. 
(i) 
Show that there is an algorithm that can determine for a given 
deterministic pushdown automaton L 
and dfa L' whether 
T(L) = L(L'). 
9. 
Compilers and Formal Languages 
A compiler is a program that takes as input a program (known as the 
source program) written in a high-level language such as COBOL, FOR-
TRAN, or Pascal and translates it into an equivalent program (known as 
the object program) in a low-level language such as an assembly language 
or a machine language. Just as in Chapters 2 and 5 we found it easier to 
write programs with the aid of macros, most programmers find program-
ming in a high-level language faster, easier, and less tedious than in a 
low-level language. Thus the need for compilers. 
The translation process is divided into a sequence of phases, of which 
the first two are of particular interest to us. Lexical analysis, which is the 
first phase of the compilation process, consists of dividing the characters of 
the source program into groups called tokens. Tokens are the logical units 
of an instruction and include keywords such as IF, THEN, and DO, 

324 
Chapter 10 Context-Free Languages 
operators such as + and * , predicates such as > , variable names, labels, 
constants, and punctuation symbols such as ( and ; . 
The reason that the lexical analysis phase of compilation is of interest to 
us is that it represents an application of the theory of finite automata and 
regular expressions. The lexical analyzer must identify tokens, determine 
types, and store this information into a symbol table for later use. Typi-
cally, compiler writers use nondeterministic finite automata to design these 
token recognizers. For example, the following is an ndfa that recognizes 
unsigned integer constants. 
Digit 
Anything but a digit 
Similarly, a nondeterministic finite automaton that recognizes variable 
names might look like this: 
Letter 
Digit 
Anything but a 
letter or a digit 
We end our brief discussion of lexical analysis by noting that it is not 
always a simple task to properly determine the division into tokens. For 
example, in FORTRAN, the statements 
DO 
10 I= 1.11 
and 
DO 
10 I= 1,11 
look very similar but are in fact totally unrelated instructions. The first is 
an assignment statement that assigns to a variable named D0101 (em-
bedded blanks are ignored in FORTRAN) the value 1.11. The second is a 
DO loop that indicates that the body is to be performed 11 times. It is 
not until the "." or "," is encountered that the statement type can be 
determined. 
At the completion of the lexical analysis phase of compilation, tokens 
have been identified, their types determined, and when appropriate, the 
value entered in the symbol table. At this point, the second phase of 
compilation, known as syntactic analysis or parsing, begins. It is in this 
second phase that context-free grammars play a central role. 

9. Compilers and Formal Languages 
325 
For programming languages that are context-free, the parsing problem 
amounts to determining for a given context-free grammar f and word w 
1. whether w E L(f), and 
2. if wE L(f), how w could have been generated. 
Intuitively, the parsing phase of the compilation process consists of the 
construction of derivation or parse trees whose leaves are the tokens 
identified by the lexical analyzer. 
Thus, for example, if our grammar included the productions 
S ---+ while-statement 
S ---+ assignment-statement 
while-statement ---+ while cond do S 
cond ---+ cond v cond 
cond ---+ rei 
rei ---+ exp pred exp 
exp ---+ exp + exp 
exp ---+ var 
exp ---+ canst 
pred ---+ > 
pred ---+ = 
assignment-statement ---+ var +--- exp 
then the parse tree for the statement 
while x > y v z = 2 do w +--- x + 4 
is given by Fig. 9.1. 
The parsing is usually accomplished by simulating the behavior of a 
pushdown automaton that accepts L(f) either starting from the root of 
the tree or the leaves of the tree. In the former case, this is known as 
top-down parsing and in the latter case, bottom-up parsing. 
Most programming languages are for the most part context-free. (A 
major exception is the coordination of declarations and uses.) A common 
technique involves the definition of a superset of the programming lan-
guage which can be accepted by a deterministic pushdown automaton. This 
is desirable since there are particularly fast algorithms for parsing gram-
mars associated with deterministic pushdown automata. 

326 
Chapter 10 Context-Free Languages 
s I 
while-statement 
while------:/ ~S 
/1~ 
I 
'T 
v T 
7T~ 
rei 
rei 
var(w) 
-
exp 
/I~ 
/1~ 
/I~ 
TpfTTprT 
T+T 
var(x) 
> 
var(v) 
var(z) 
const (2) 
var(x) 
const(4) 
Figure 9.1 
Exercise 
1. Give a context-free grammar for generating valid Pascal arithmetic 
expressions over the alphabet {a, b, +, -, *, ;, j, (,)}, where variable 
names are elements of {a, b}* of length at least 1. Is the grammar 
ambiguous? What are the implications of this? 

11 
Context-Sensitive Languages 
1. The Chomsky Hierarchy 
We are now going to place our work in the context of Noam Chomsky's 
hierarchy of grammars and languages. An arbitrary (phrase structure) 
grammar (recall Chapter 7, Section 5) is called a type 0 grammar. A 
context-sensitive grammar (recall Chapter 7, Section 5) is called a type 1 
grammar. A positive context-free grammar (recall Chapter 10, Section 1) is 
called a type 2 grammar, and a regular grammar (recall Chapter 10, Section 
2) is called a type 3 grammar. The inclusions suggested by the numbering 
obviously hold: every regular grammar is context-free, and every positive 
context-free grammar is context-sensitive. (Of course, grammars contain-
ing productions of the form V---+ 0 cannot be context-sensitive.) 
For each type of grammar, there is a corresponding class of languages: 
r 
regular 
1 
r 31 
. 
context- ee 
2 
A language L IS 
fo . . 
or of type 
1 
context -sensltwe 
r.e. 
0 
[
regular l 
. 
. 
. 
ositive context- ee 
1f and only 1f there IS a p t t 
.t. fo 
grammar r 
con ex -sensl we 
phrase structure 
327 

328 
Chapter 11 Context-Sensitive Languages 
such that 
L = L(f) 
or 
L = L(f) U {0}. 
For regular languages this statement is just Theorem 2.3 in Chapter 10. 
For context-free languages, it is Theorem 1.2 in Chapter 10. For context-
sensitive languages we take it as a definition. For r.e. languages it is 
Theorem 5.2 in Chapter 7, and the special reference to {0} is not needed. 
We have 
Theorem 1.1. Every regular language is context-free. Every context-free 
language is context-sensitive. Every context-sensitive language is recursive. 
Proof. The first two statements follow simply from the corresponding 
inclusions among the types of grammar. The third follows at once from 
Theorem 5.4 in Chapter 7. 
â¢ 
We would like to show that the inclusions of Theorem 1.1 are proper, 
that is, that none of the four classes mentioned in the theorem is identical 
to any of the others. We have seen in Theorem 1.1 in Chapter 10, that the 
language L = {alnlblnlln > 0} is context-free but not regular. Similarly, we 
saw in Theorem 4.2 in Chapter 10 that the language {alnlblnlclnlln > 0} is 
not context-free, while Exercise 5.4 in Chapter 7 shows that it is context-
sensitive. This takes care of the first two inclusions of Theorem 1.1. The 
following theorem takes care of the remaining one. 
Theorem 1.2. There is a recursive language on the alphabet {1} that is not 
context -sensitive. 
Proof. We first code each context-sensitive grammar f with terminal 
alphabet {1} by a string on the five-letter alphabet A = {1, V, b, ~ , /}. 
We do this simply by replacing each variable by a distinct string of the 
form Vbln, using the arrow"~ " as usual between the left and right sides 
of productions, and using the slash "/" to separate productions. (Of 
course, not every string on this alphabet is actually the code for a 
context-sensitive grammar.) Now, the strings that code context-sensitive 
grammars may be placed in alphabetic order (or equivalently, in numerical 
order, regarding each string on A as the base 5 notation for an integer, as 
in Chapter 5). We let L; be the context-sensitive language generated by 
the ith context-sensitive grammar in this enumeration, i = 1, 2, 3, .... 
Then we set 
This is, of course, a typical diagonal construction, and we easily show that 

1. The Chomsky Hierarchy 
329 
L is not context-sensitive. For, if L = L,. , then 
() 
if and only if 
1[inl f/:. L. 
'" 
if and only if 
1[iol f/:. L. 
To see that L is recursive we note that there is an algorithm which given 
i will return a context-sensitive grammar f; that generates L;. Then 1[il 
can be tested for membership in L; using the algorithm developed in the 
proof of Theorem 5.4 in Chapter 7. 
â¢ 
For each class of languages corresponding to types 0, 1, 2, 3, we are 
concerned with questions of the following kinds: What can we determine 
algorithmically about a language from a grammar which generates it? 
What kinds of device will accept precisely the languages belonging to the 
class? Under what operations are the classes closed? We have been 
dealing with these questions for languages of types 0, 2, and 3. Now, we 
will see what can be said about languages of type 1, i.e., context-sensitive 
languages. We begin by considering the question of closure under union. 
We will need the 
Lemma. There is an algorithm that will transform a given context-
sensitive grammar r into a context-sensitive grammar A such that the left 
sides of the productions of A contain no terminals and L(f) = L(A). 
Proof. We "disguise" the terminals as variables as in the proof of 
Theorem 3.1 in Chapter 10, except that now we need to replace the 
terminals on both the left and right sides of the productions. The resulting 
grammar, A, consists of productions of the form X 1 â¢â¢â¢ Xm ~ Y1 â¢â¢â¢ Yn, 
m ~ n, and Xa ~ a, where X 1 , â¢â¢â¢ , Xm, Y1 , â¢â¢â¢ , Y,, Xa are variables and a 
is a terminal. Clearly, L(A) = L(f). 
â¢ 
Theorem 1.3. If L 1 , L 2 are context-sensitive languages, then so is 
L 1 u L 2 â¢ 
Proof. Assume L 1 = L{f1) or L{f1) U {0}, L 2 = L(f2) or L(f2 ) U {0}, 
where rl and r2 are context-sensitive grammars with disjoint sets of 
variables of the form obtained in the Lemma. We construct f from f 1 and 
f 2 exactly as in the proof of Theorem 5.1 in Chapter 10, so that r is also 
context-sensitive and L(f) = L{f1) u L(f2). Clearly, L 1 U L 2 = L{f) or 
L(f) u {0}. 
â¢ 
Exercises 
1. Show that {w E {a, b, c}*lw has an equal number of a's, b's, c's} is 
context-sensitive. 

330 
Chapter 11 Context-Sensitive Languages 
2. Let r be the grammar with productions 
S---+ AXYp 
AX---+ AaA 
AX---+ AbB 
AX---+ AccC 
Aa ---+ aA 
Ab ---+ bA 
Ba ---+ aB 
Bb ---+ bB 
Ca ---+ aC 
Cb ---+ bC 
AY---+ XYa 
BY---+ XYb 
CY---+ cc 
aX---+ Xa 
bX---+ Xb, 
where 'F= {S, X, Y, A, B,C} and T ={A, p, a, b, c}. What is L(f)? 
3. Show that {wwlw E {a, b}*} is context-sensitive. 
4. Apply the construction in the proof of the Lemma to the grammar in 
Exercise 2. 
5. Show that the proof of Theorem 1.3 fails if we do not assume that f 1 , 
r2 conform to the conditions of the Lemma. 
6. (a) Let r be a context-sensitive grammar. Show that there is a 
context-sensitive grammar f' such that L(f') = L(f) and such 
that, for every production u ---+ v in r, lui ::; 2 and lvl ::; 2. 
(b) Prove that a language L is context-sensitive if and only if it is 
generated by a grammar f, with variables 'F and terminals T, 
such that every production in f has the form uVw ---+ uvw, where 
U, WE ('FU T)*, V E 'F, and V E ('FU T)* - {0}. [Note: This 
explains the origin of the term context-sensitive.] 
2. 
Linear Bounded Automata 
We are now going to deal with the question: which devices accept context-
sensitive languages? We define a linear bounded automaton on the alpha-
bet C = {s1 , s2 , â¢â¢â¢ , sn} to be a nondeterministic Turing machine Lon the 
alphabet C u {A, p} such that the only quadruples L contains beginning 
q A or q p are of the forms q A R p and q p L p, respectively, such that L 
has a final state, written ij, where no quadruple of L begins ij, and finally 
such that for every quadruple q a b p in L, we have b -=1= A, p. Thus, when 
scanning A, L can move only right, and when scanning p, L can move 
only left, and the symbols A, p can never be printed in the course of a 
computation. Thus, the effect of the additional symbols A and p is simply 
to prevent the machine from moving beyond the confines of the given 
string on the tape. Because of this we can code a configuration of L by a 
triple (i, q, Awp), where 0 ::; i ::; lwl + 1; i gives the position of the tape-
head (i.e., of the scanned square), q is the current state; and Awp is the 

2. Linear Bounded Automata 
Quadruple in L 
qabp 
qaRp 
qaLp 
Table 2.1 
Corresponding transition 
(lul,q,uav) 1--_.,. <lul,p,ubv) 
(lul,q,uav) 1--_.,. (lui+ l,p,uav) 
(lui, q, uav) I-..,. (lui - 1, p, uav) 
331 
tape contents, w E (C u {s0})*. (Recall that s0 is the blank.) As usual, for 
configurations y, 8 we write y 1-...,. 8 to mean that one of the quadruples 
of L permits the transition from y to 8, and write y ;. ...,. 8 to mean that 
there is a sequence of configurations y = y 1 , y2 , â¢â¢â¢ , 'Yk = 8 such that 
'Y; 1-...,. 'Y;+ 1 for 1 :::;; i < k. Table 2.1 shows which transitions are permitted 
by each quadruple in L 
(here a E C u {s0 , A, p}, b E C u {s0}). (Of 
course, for a = A, p, only quadruples of the second and third kind, 
respectively, can occur in L.) 
L is said to accept a string w E C * if 
(l,q1 ,Awp) ;...,{i,ij,Aw'p), 
where q1 is the initial state of Land, of course, ij is the final state. (Note 
carefully that unlike the situation for Turing machines, a configuration will 
be regarded as "accepting" only if L is in its final state ij.) If A k C, we 
write LiL) for the set of all w E A* that are accepted by L. The main 
theorem is 
Theorem 2.1 
(Landweber-Kuroda). The language L k A* is context-
sensitive if and only if there is a linear bounded automaton L such that 
L =LiL). 
We begin with 
Lemma 1. There is an algorithm that transforms any given context-
sensitive grammar f with terminals T into a linear bounded automaton L 
such that L(f) = LT(L). 
Proof. Let 'F be the set of variables of r, and let S E 'F be the start 
symbol. The alphabet of L will be T U 'F. Let the productions of f be 
u; ~ V;, i = 1,2, ... ,m, where 
u. = a<iJa<il Â·Â·Â· a<il 
and 
v. = f3U>'f3U> ... {3(i) â¢ 
(2.1) 
I 
I 
2 
k; 
I 
I 
2 
I; 
' 
afil, a~il, ... , akil, f3fi>, f3ii>, ... , f3F> E T U 'F, 
I 
I 
and k; :::;; I;. Then we set 
aU> 
=aU> 
= 
k;+l 
k;+2 

332 
Chapter 11 Context-Sensitive Languages 
That is, we fill out the left side of each production with blanks. Since L is 
operating nondeterministically, it can seek the word V; on the tape and 
replace it by U;, thus undoing the work of the production. It will help in 
following the construction of the automaton L 
if we think of it as 
operating in one of these four phases: initialization, searching, production 
undoing, and termination. The states of .I will be the initial state q 1 , the 
search state u, the return state ii, the undomg states pji>, qji> for 1 :::;; i :::;; m 
and 1 :::;; j :::;; I; [I; is as defined in Eqs. (2.1)], and the termination states T, 7. 
Phase 1 (Initialization) We place in L the quadruples 
Thus in Phase 1, L 
operating nondeterministically "decides" to enter 
either the search or the termination phase. 
Phase 2 (Search) We place in L the quadruples 
(T 
a 
R 
(T 
a=/=p 
(T 
p 
L 
(T 
(T 
f3fi) 
f3fi) 
p~i) 
1:::;; i:::;; m 
(T 
a 
L 
(j 
a =/= ,\ 
(T 
,\ 
R 
ql. 
In Phase 2, L moves right along the tape searching for one of the initial 
symbols f3fi> of the right side of a production. Finding one, L may enter 
an undoing state. If L encounters the right end marker p while still in 
state u, it enters the return state ii and goes back to the beginning. 
Phase 3 (Production Undoing) We place in L the quadruples, for 
1 :::;; j < I;, 1 :::;; i :::;; m, 
py> 
13P> 
a~i) 
1 
qy> 
qji> 
a~i) 
1 
R 
pU> 
j+l 
p(i) 
I; 
f3(i) 
I; 
aU> 
I; 
(T 
together with the quadruples 
py> 
So 
R p\i) 
1 â¢ 
When operating in Phase 3, L has the opportunity to replace the right 
side of one of the productions on the tape by the left side (ignoring any 

2. Linear Bounded Automata 
333 
blanks that might have been introduced by previous replacements). If L 
succeeds, it can enter the return state u, return to the left, and begin 
again. 
Phase 4 (Termination) We place in L the quadruples 
T 
So 
R 
T 
T s 
R 
T 
T 
So 
R 
T 
T 
p 
L 
ij. 
Thus if Lever returns to state q1 with the tape contents 
i,j ~ 0 
(where, of course, S is the start symbol of f), then L will have the 
opportunity to move all the way to the right in this phase and to enter the 
final state ij. 
Thus, L will accept a word w E T* just in case there is a derivation of 
w from sin r. 
â¢ 
Lemma 2. If L ~A* is a context-sensitive language, then there is a linear 
bounded automaton L such that L = LiL). 
Proof. We have L = L(f) or L(f) u {0} for a context-sensitive grammar 
f. In the first case, L can be obtained as in Lemma 1. In the second case, 
we modify the automaton L 
of Lemma 1 by adding the quadruple 
q1 p L ij. The modified automaton accepts 0 as well as the strings that L 
accepts. 
â¢ 
Now, we wish to discuss the converse situation: we are given a linear 
bounded automaton L and alphabet A and wish to obtain a context-
sensitive grammar r such that L(f) = LiL) - {0}. The construction will 
be similar to the simulation, in Chapter 7, of a Turing machine by a 
semi-Thue process. However, the coding must be tighter because all the 
productions need to be non-length-decreasing. 
Let L be the given linear bounded automaton with alphabet C where 
A ~ C, initial state q1 , and final state ij. To begin with, we will only 
consider words u E C* for which lui ~ 2; such words can be written awb, 
where w E C*, a, b E C. We wish to code a configuration (i, q, Aawbp) of 
L by a word of length lawbl = lwl + 2. To help us in doing this, we will use 
five variants on each letter a E C: 
a Ia al a a. 

334 
Chapter 11 Context-Sensitive Languages 
The interpretation of these markings is 
r a: a on the left end of the word; 
al: a on the right end of the word; 
a: a on the left end, but the symbol being scanned is A, one square to 
the left of a; 
a: a on the right end, but the symbol being scanned is p, one square to 
the right of a. 
Finally, the current state will ordinarily be indicated by a subscript on the 
scanned symbol. If however, the scanned symbol is A or p, the subscript 
will be on the adjacent symbol, marked, as just indicated, by an arrow. 
Thus, if L has n states we introduce 3(n + 1) + 2n symbols for each 
a E c. (Note that a and a always have a subscript.) The examples in Table 
2.2 should make matters plain. Of course, this encoding only works for 
words Awp for which lwl ~ 2. 
Now we will construct a semi-Thue process !. such that given configura-
tions y,"l> of Land their codes y, 8, respectively, we shall have 
y 1-..,. 8 if and only if y ? 8. 
As for Turing machines, we define !. by introducing suitable productions 
corresponding to each quadruple of L. The correspondence is shown in 
Table 2.3, where we have written C for C U {s0}. 
Now, since these productions simulate the behavior of L in an obvious 
and direct manner, we see that L 
will accept the string aub, u E C*, 
a, bE C, just in case there is a derivation, from the initial word laq,ubl 
using these productions, of a word containing ij as a subscript. To put this 
result in a more manageable form, we add to the alphabet of !. the symbol 
S and add to !. the "cleanup" productions 
aS~ S, 
Sa~ S, 
(2.2) 
where a can be any one of a, Ia, al, a, or a, for any a E c. Since these 
productions will transform the codes for configurations with the final state 
Table 2.2 
Configuration 
(3, q, Aababcp) 
(1, q, Aababcp) 
(5, q, Aababcp) 
(0, q, Aababcp) 
(6, q, Aababcp) 
Code 
1abaqbcl 
raqbabcl 
1ababc! 
iiqbabcl 
1 abab~ 

2. Linear Bounded Automata 
335 
Table 2.3 
Quadruple of L 
Productions of ~ 
q a b p, 
a,b e C 
q a R p, 
aeC 
all be C 
qARp 
all a e C 
q aLp, 
aeC 
all be C 
qpLp 
all a e C 
q into the single symbol S, and since there is no other way to obtain the 
single symbol S using the productions of I, we have 
Lemma 3. 
L accepts the string aub, a, b E C, u E C*, if and only if 
Now let .n be the semi-Thue process whose productions are the inverses 
of the productions of I. (See Chapter 7, Section 2.) Then we have 
Lemma 4. 
L accepts the string aub, a, b E C, u E C*, if and only if 
Now we are ready to define a context-sensitive grammar f. Let the 
terminals of r be the members of A, let the variables of r be 
1. the symbols from the alphabet of .n that do not belong to A, and 
2. symbols a0 for each a EA. 

336 
Chapter 11 Context-Sensitive Languages 
Finally, the productions of f are the productions of !1 together with 
1::~: ::o} for all 
a, bE A. 
a0bl ~ ab 
(2.3) 
It is easy to check that f is in fact context-sensitive. [Of course, the 
productions (2.2) must be read from right to left, since it is the inverses of 
(2.2) that appear in f.] Moreover, using Lemma 4 and (2.3), we have 
Lemma 5. Let w E A*. Then w E L(f) if and only if lwl ~ 2 and 
w ELiL). 
Now let L be a given linear bounded automaton, A a given alphabet, 
and let f be the context-sensitive grammar just constructed. Then, by 
Lemma 5, we have 
LA(L) = L(f) U L 0 , 
where L 0 is the set of words w E A* accepted by L such that lwl < 2. But 
L 0 is finite, hence (Corollary 4.7 in Chapter 9) L 0 is a regular language, 
and so is certainly context-sensitive. Finally, using Theorem 1.3, we see 
that LiL) is context-sensitive. This, together with Lemma 2, completes 
the proof of Theorem 2.1. 
â¢ 
Exercises 
1. Let L 
be the linear bounded automaton with initial state q1, final 
state ij, and quadruples 
q, 
a 
R 
qz 
qz 
b 
R 
q, 
q, 
b 
R 
q3 
qz 
c 
R 
q, 
q, 
c 
R 
q, 
q3 
a 
R 
q, 
q, 
p 
L 
ij 
q3 
c 
R 
q,. 
What is L(L)? 
2. Give a deterministic linear bounded automaton L 
that accepts 
{w E {a, b, c}* I w has an equal number of a's, b's, c's}. 
3. Give a linear bounded automaton L that accepts {ww I wE {a, b}*}. 
4. Let L 
be the linear bounded automaton with initial state q1 , final 
state ij, and quadruples 

3. Closure Properties 
337 
(a) Use the construction in the proof of Theorem 2.1 to give a 
grammar f such that L(f) = L(L). 
(b) Give a derivation of aabb in r. 
5. Let r be the grammar with start symbol S and productions S ~ aSb, 
S ~ ab. 
(a) Use the construction in the proof of Theorem 2.1 to give a linear 
bounded automaton L such that L(L) = L(f). 
(b) Give an accepting computation by L for input aabb. 
6. Prove that every context-free language is accepted by a deterministic 
linear bounded automaton. 
7. Show that there is an algorithm to test a given linear bounded 
automaton L 
and word w to determine whether or not L 
will 
eventually halt on input w. That is, the halting problem is solvable for 
linear bounded automata. [Hint: Consider the pigeon-hole principle.] 
3. Closure Properties 
We have already seen that the context-sensitive languages are closed 
under union (Theorem 1.3), and now we consider intersection. Here, 
although the context-free languages are not closed under intersection 
(Theorem 5.2 in Chapter 10), we can prove 
Theorem 3.1. If L 1 and L 2 are context-sensitive languages, then so is 
L 1 nL2 â¢ 
Proof. Let L 1 = LiL1), L 2 = LiL2 ), where L 1 , L 2 
are linear 
bounded automata. The idea of the proof is to test a string w for 
membership in L 1 n L 2 by first seeing whether L 1 will also accept w and 
then, if L 1 does, to see whether L 2 will also accept w. The difficulty is 
that L 1 may destroy the input w in the process of testing it. If we were 
working with Turing machines, we would be able to deal with this kind of 
problem by saving a copy of the input on a part of the tape that remained 
undisturbed. Since linear bounded automata have no extra space, the 
problem must be solved another way. The solution uses an important idea: 
we think of our tape as consisting of a number of separate "tracks," in this 
case two tracks. We will construct a linear bounded automaton L that will 
work as follows: 
1. L will copy the input so it appears on both the upper and the lower 
track of the tape; 

338 
Chapter 11 Context-Sensitive Languages 
2. L will simulate L 1 working on the upper track only; 
3. if L 1 has accepted, L will then simulate L 2 working on the lower 
track (on which the original input remains undisturbed). 
Thus, let us assume that L 1 and L 2 both have the alphabet C = 
{s1 , s2 , â¢â¢â¢ , sn}. (Of course, in addition they may use the symbols A, p, s0.) 
L 
will be a linear bounded automaton using, in addition, the symbols 
bj, 0 :o:; i, j :o:; n. We think of the presence of the symbol bj as indicating 
that s; is on the "upper track" while sj is on the "lower track" at the 
indicated position. Finally we assume that q 1 is the initial state of L 1 , that 
q is its final state, and that q2 is the initial state of L 2 â¢ We also assume 
that the sets of states of L 1 and L 2 are disjoint. L is to have initial state 
q0 and have the same final state as L 2 â¢ L 
is to contain the following 
quadruples (for 0 :o:; i :o:; n ): 
(1) Initialization: 
qo 
S; 
bi 
I 
q 
q 
bi 
I 
R 
qo 
qo 
p 
L 
q 
q 
bi 
I 
L 
q 
q 
A 
R 
ql. 
Here q, q are not among the states of L 1 and L 2 . These quadruples 
cause L 
to copy the input on both "tracks" and then to return to the 
leftmost symbol of the input. 
(2) 
For each quadruple of L 1 , the corresponding quadruples, obtained 
by replacing each s; by bj, j = 0, 1, ... , n, are to be in L. These quadru-
ples cause L to simulate L 1 operating on the "upper" track. In addition, 
L is to have the quadruples for 0 :o:; i, j :o:; n: 
q bi 
1 
R q 
q 
p 
L 
p 
p bi 
1 
sj p 
p 
sj 
L 
p 
p 
A 
R 
qz. 
Here again p does not occur among the states of L 1 , L 2 â¢ These quadru-
ples cause L to restore the "lower" track and then to enter the initial 
state of L 2 scanning the leftmost input symbol. 

3. Closure Properties 
(3) Finally, L is to contain all the quadruples of L 2 â¢ 
Since it is plain that 
the proof is complete. 
339 
â¢ 
As an application, we obtain an unsolvability result about context-
sensitive grammars. 
Theorem 3.2. There is no algorithm for determining of a given context-
sensitive grammar r whether L(f) = 0. 
Proof. Suppose there were such an algorithm. We can show that there 
would then be an algorithm for determining of two given context-free 
grammars f 1 , f 2 whether L(f1) n L(f2) = 0, thus contradicting Theorem 
6.6 in Chapter 10. For, since f 1 , f 2 are context-sensitive, the constructive 
nature of the proofs of Theorems 2.1 and 3.1 will enable us to obtain a 
context-sensitive grammar f with L(f) = L(f1) n L(f2 ). 
â¢ 
We turn now to a question about context-sensitive languages that was 
one of the outstanding open problems in theoretical computer science for 
over two decades. In 1964 Kuroda raised the question: Are the context-
sensitive languages closed under complementation? It remained unsettled 
until 1987, when Neil Immerman showed that the answer is yes. What is 
particularly interesting is that, after more than twenty years, the solution 
turned out to be surprisingly straightforward. 
We will show that if L ~A* is accepted by a linear bounded automaton, 
then so is A* - L. Suppose that L is accepted by the linear bounded 
automaton L 
with alphabet {s1 , â¢â¢â¢ ,sn_ 1} and states {q1, ... ,qk}. (We 
take qk to be ij, and we will sometimes write A, pas sn, sn+ 1 , respectively.) 
We want to find another linear bounded automaton ./Y which accepts when 
L rejects and vice versa. This would be easy if L were deterministic, but 
suppose L is nondeterministic. If w rt L then every computation by L on 
input Awp is nonaccepting, so if we constructed ./Y to simulate L and 
enter the final state ij precisely when L halts in a state other than ij, then 
every halting computation by ./Y would enter ij and ./Y would accept w (if it 
has at least one halting computation). However, if w E L, then L could 
still have some computations which halt in some state other than ij, in 
which case ./Y would still accept w. Thus, we need ./Y to accept only when 
every computation of L fails to end in state ij. 
The problem is that it is not at all clear how to construct ./Y so that a 
single computation by ./Y can correctly gather information about every 
computation by L. We could deterministically simulate L, using a stack 

340 
Chapter 11 Context-Sensitive Languages 
to remember "branch points." However, L 
has I AwplÂ· k Â· nlwl distinct 
configurations with I Awpl tape squares, so a nonlooping computation by L 
on input Awp could run for as many as I AwplÂ· k Â· nlwl - 1 steps, and each 
step could require adding more information to the stack. There is no way, 
then, that such a stack can be stored in lwl tape squares, even using 
multiple tracks as in the proof of Theorem 3.1. Actually, there are 
simulation techniques that are much more efficient in terms of space, but 
none are known that are sufficiently parsimonious for our purposes here. 
The solution discovered by Immerman is to store sufficient information 
about the possible computations by counting configurations. The largest 
value that needs to be stored is I AwplÂ· k Â· nlwl, which for any w -=1= 0 can be 
represented in base n notation by a string of length 
~ logn IAwpl + logn k + lwl + 1 ~ c Â·lwl 
for some constant c. (We can ignore the case w = 0 since the decision to 
accept or reject 0 can be built explicitly into the quadruples of .#".) The 
important thing is that c does not depend on w, so we can construct ./Y to 
maintain each such counter on c tracks, regardless of the length of the 
input. In fact, it will be convenient to consider the c tracks holding a 
counter as a single track with c "subtracks." 
The other objects we need to represent are configurations. If the initial 
configuration is (1, q 1 , Awp), then it is clear that we can represent on a 
single track any configuration (i, q, Axp) where lxl = lwl. For example, we 
could add to the alphabet of some track new symbols sf, 0 ~ i ~ n + 1 
and 1 ~ j ~ k. Then sf in square I on this track represents L in state qj 
scanning square I (on its own tape) holding symbol s;. Not every string on 
the alphabet 
represents a configuration of L, but it is clear that the representations of 
all configurations of L with IAwpl tape squares can be written one after 
another, say, in ascending numerical order, on some track. We will call the 
ith configuration in this enumeration C;. Of these configurations, some 
may never occur in any computation by Lon input Awp. We say that a 
configuration (i, q, Axp) is reachable from w if (1, q 1 , Awp) ~..,. (i, q, Axp). 
We describe the behavior of ./Y by means of two nondeterministic 
procedures, the COUNT phase and the TEST phase. Although these are 
written in an informal high-level notation, it should be clear that ./Y can be 
constructed to carry them out, using no more than IAwpl tape squares. We 
begin with the TEST phase, described in Figure 3.1, where we will see the 
importance of being able to count the reachable configurations. Suppose 

3. Closure Properties 
COUNTER+-- 0 
fori= 1 to IAwplÂ·k Â·nlwl 
CONFIG +-- C; 
nondeterministic ally simulate some computation by .If 
on Awp until it reaches CON FIG or terminates 
if CONFIG has been reached then 
if CONFJG is accepting 
then enter q' and halt 
else COUNTER +-- COUNTER + 1 
end for 
if COUNTER = r then enter ij and halt 
else enter q' and halt 
Figure 3.1. The TEST phase of f. 
341 
we have a tape with w on track 1 and r on track 2, where r is the number 
of configurations reachable from w. We will write this tape as Aw jrp. The 
TEST phase needs four tracks in addition to tracks 1 and 2. Two are 
needed for variables i and COUNTER, which hold numbers ~ IAwplÂ· k Â· 
nlwl, one is needed for CONFIG, which holds representations of configu-
rations with I Awpl tape squares, and a fourth is needed to simulate 
computations by L on input Awp. It is clear that each track is large 
enough for its purpose. Let q' be some non final state of .IY. 
Claim 1. Executing the TEST phase, .#' accepts w jr if and only if L 
rejects w. 
If L 
accepts w, there are at most r - 1 nonaccepting reachable 
configurations, so any computation by.#' will either 
â¢ run forever simulating some computation by L; 
â¢ simulate some computation by L that halts in state ij, or 
â¢ end with COUNTER < r. 
Therefore, no computation by.#' ends in state ij, and.#' rejects w jr. If L 
rejects w then .#' can "guess" computations by L 
that reach every 
reachable configuration. None of these is accepting, so .#' finishes with 
COUNTER = r and accepts w jr. This proves Claim 1. 
Finally, we need to show that .#' can correctly compute r prior to 
entering the TEST phase. It might seem that .#'could simply guess r and 
then continue with the TEST phase. The problem is that, if .#' incorrectly 
guesses some r' < r, then some computation by.#' in the TEST phase 
might end with COUNTER= r' and accept w when it should reject it. 
Therefore, it is not enough that some computation by.#' reach the TEST 
phase with the correct value of r. We must ensure that every computation 

342 
Chapter 11 Context-Sensitive Languages 
i<--0 
COUNTER<-- 0 
NEW-COUNTER <-- I 
[Main] if NEW _COUNTER = COUNTER then 
delete all but tracks 1 and 2 from tape 
goto TEST phase 
i<-i+l 
COUNTER <-- NEW _COUNTER 
NEW _COUNTER <-- 0 
forj =I to IAwplokonlwl 
t<--0 
CONFIGI <-- Ci 
for I = I to I Awpl 0 k 0 nlwl 
CONFIG2 <-- C1 
nondeterministically simulate some computation 
by ./ton Awp until it reaches CONFIG2 or 
until i steps have been executed 
if CONFIG2 has been reached then 
end for 
t<-t+l 
if CONFIG2 = CONFIGI or 
CONFIG2 f-_, CON FIG I then 
NEW _COUNTER <-- NEW _COUNTER + 1 
leave inner loop 
if I > I Awpl o k 
0 nlwl and t < COUNTER then 
enter q' and halt 
end for 
goto Main 
Figure 3.2. The COUNT phase of AI'. 
by ./Y that gets as far as the TEST phase must do so with the correct value 
of r. We will now show that this can be done. 
For all i ~ 0, let ri be the number of configurations of L that can be 
reached from (1, q1 , Awp) in no more than i steps. Then there is some i 0 
such that rio = rio+ 1 = r. We will argue by induction that each r;, for 
1 ::::; i ::::; i 0 , is correctly computed by the COUNT phase, given in Figure 
3.2. The input is the initial tape Awp. We also need tracks to hold variables 
NEW_COUNTER, COUNTER, i, j, l, t, CONFIG1, and CONFIG2, and 
a track to use in simulating computations by .4. Again it is clear that 
sufficient space is available. We stipulate that NEW_COUNTER, which 
will eventually hold r, should be stored on track 2. 
Claim 2. 
For i ~ 0, any computation by ./Y on Awp that completes i 
executions of the main loop has the correct value of ri in NEW _COUN-
TER. 

3. Closure Properties 
343 
The claim is obvious for i = 0, so we assume it is true for some i ~ 0 
and show that it is true for i + 1. Suppose some computation completes 
i + 1 executions of the main loop. Then throughout the i + 1st execution 
of the main loop, COUNTER = r; by the induction hypothesis (since 
COUNTER is set to NEW_COUNTER at the beginning of the loop). 
CONFIGl ranges over all configurations with IAwpl tape squares, and for 
each value of CONFIGl we want NEW_COUNTER to be incremented 
just in case CONFIGl is reachable within i + 1 steps. Now, for each value 
of CONFIGl, the inner for loop1 ends either with I ~ IAwplÂ· k Â· nlwl, 
meaning that the current CONFIGl has been found to be reachable 
within i + 1 steps, or with I > I AwplÂ· k Â· nlwl and t = COUNTER, mean-
ing that all r; of the configurations reachable within i steps have been 
found and none of them leads to CONFIGl in 0 or 1 steps, i.e., CONFIGl 
is not reachable within i + 1 steps. In the first case NEW _COUNTER is 
incremented and in the second case it is not, so the claim is true for i + 1. 
To conclude we simply note that at least one computation by .AI' on Awp 
will correctly guess the appropriate computations by L to simulate and 
will execute the main loop i0 + 1 times, leaving r on track 2. Any such 
computation will then go on to execute the TEST phase, and, by Claim 1, 
.AI' will accept w if and only if L rejects w. Therefore, by Theorem 2.1 we 
have proved 
Theorem 3.3. If L ~A* is context-sensitive, then so is A* - L. 
We conclude this chapter by mentioning another major problem con-
cerning context-sensitive languages that remains open: is every context-
sensitive language accepted by a deterministic linear bounded automaton? 
Exercises 
1. Let L, L' be context-sensitive languages. Prove the following. 
(a) 
L Â· L' is context-sensitive. 
(b) 
L* is context-sensitive. 
(c) 
L R = {wR I w E L} is context-sensitive. 
2. Let L ~A* be an r.e. language. Show that there is a context-sensitive 
language L' ~(A U {c})* such that for all w E A*, we have 
wEL 
if and only if 
wc[il E L' for some i ~ 0. 
1 We are assuming here that when a loop of the form fori = 1 to n runs to completion, it 
leaves i = n + 1. 

344 
Chapter 11 Context-Sensitive Languages 
3. Show that for every r.e. language L there is a context-sensitive 
grammar r such that the grammar obtained from r by adding a single 
production of the form V ~ 0 generates L. [Hint: Use Exercise 2 and 
take c to be the variable V.] 
4. 
Give alphabets A, P and a context-sensitive language L ~A* such 
that Er p(L) is not context-sensitive. 
5. 
Let A 1 , A 2 be alphabets and let L ~Aj be context-sensitive. Let f 
be a substitution on A 1 such that for each a E A,f(a) ~A~ is 
context-sensitive and 0 $. f(a). Let g be a homomorphism from Aj to 
A~ such that g(a) =I= 0 for all a EA. [See Exercise 4.5 in Chapter 9 for 
the definitions of substitution and homomorphism.] 
(a) Prove that f(L) is context-sensitive. 
(b) Prove that g(L) is context-sensitive. 
(c) 
Give a context-sensitive language L' and homomorphism h such 
that h(L') is not context-sensitive. 

Part 3 
Logic 


12 
Propositional Calculus 
1. Formulas and Assignments 
Let A be some given alphabet and let Sit' ~ A*. Let B = A u {--,, 1\ , V , 
::::>, ~, (, )}, where we assume that these additional symbols are not 
already in A. --,, 1\, V, ::::> , ~ are called (propositional) connectives. 
Then by a propositional formula over Sit' we mean any element of B* which 
either belongs to Sit' or is obtainable from elements of Sit' by repeated 
applications of the following operations on B*: 
1. transform a into --, a; 
2. transform a and {3 into (a 1\ {3 ); 
3. transform a and {3 into ( a V {3 ); 
4. transform a and {3 into ( a ::::> {3 ); 
5. transform a and {3 into (a ~ {3 ). 
When the meaning is clear from the context, propositional formulas over Sit' 
will be called .J!t'-formulas or even just formulas for short. In this context 
the elements of Sit' (which are automatically .Jit'-formulas) are called atoms. 
To make matters concrete we can take A = {p, q, r, s, 1}, and let 
Sit'= {pl[il, qlliJ, rlliJ, s11;1li E N}. 
In this case the atoms are called propositional variables. We can think of 
the suffix 11;1 as a subscript and write P; = pl1;1, q; = ql1;1, etc. Here are a 
347 

348 
Chapter 12 Propositional Calculus 
few examples of formulas: 
((-,p:::>q):::>p), 
((((p Aq) =>r) A ((p1 Aq1) :::>r1)) :::> -,s), 
(((pi V â¢Pz) V PJ) A (-,pi V PJ)). 
Although the special case of propositional variables really suffices for 
studying propositional formulas, it is useful in order to include later 
applications, to allow the more general case of an arbitrary language of 
atoms. (In fact our assumption that the atoms form a language is not really 
necessary.) 
By an assignment on a given set of atoms .91 we mean a function v which 
maps each atom into the set {FALSE, TRUE} = {0, 1}, where (recall Chap-
ter 1, Section 4), as usual, we are identifying FALSE with 0 and TRUE 
with 1. Thus for each atom a we will have v( a) = 0 or v( a) = 1. Given 
an assignment v on a set of atoms .91, we now show how to define a value 
yv E {0, 1} for each .91-formula y. The definition is by recursion and 
proceeds as follows: 
1. if a is an atom, then av = v( a); 
. 
{1 
if 
{3v=O 
2. If y = -, {3, then yv = 
O if 
{3v = 1; 
3. (a A {3 )v = 
1 
if 
av .= {3 v = 1 
0 
otherwise; 
0 
if 
a v = {3 v = 0 
4. (a V f3)v = 
1 
otherwise; 
0 
if 
av = 1 and {3 v = 0 
5. (a :::> {3 )v = 
1 
otherwise; 
6. (a ++ {3 )v = { 01 
if 
a v = {3 v 
otherwise. 
A set .n of .91-formulas is said to be truth-functionally satisfiable, or just 
satisfiable for short, if there is an assignment v on .91 such that a v = 1 for 
all a E 0; otherwise .n is said to be (truth-functionally) unsatisfiable. If 
.n = {y} consists of a single formula, then we say that y is (truth-function-
ally) satisfiable if .n is; y is (truth-functionally) unsatisfiable if .n is 
unsatisfiable. y is called a tautology if yv = 1 for all assignments v. It is 
obvious that 
Theorem 1.1. 
y is tautology if and only if -, y is unsatisfiable. 
We agree to write a = {3 for .91-formulas a, {3 to mean that for every 
assignment v on .91, a v = {3 v. This convention amounts to thinking of an 
.91-formula as naming a mapping from {0, l}n into {0, 1} for some n EN, so 
that two .91-formulas are regarded as the same if they determine the same 

1. Formulas and Assignments 
349 
Table 1.1 
a 
f3 
-,a 
(--,aVf3) 
(a:>f3) 
(f3:>a) 
(a++f3) 
0 
1 
1 
0 
1 
1 
1 
1 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
mappings. [Thus, in high school algebra one writes x 2 -
1 = (x- 1) 
X (x + 1), although x 2 -
1 and (x - 1){x + 1) are quite different as 
expressions, because they determine the same mappings on numbers.] With 
this understanding, we are able to eliminate some of the connectives in 
favor of others in a systematic manner. In particular, the equations 
(a :l {3) = (-, a V {3), 
(a++ {3) =((a :l {3) 1\ ({3 :l a)) 
(1.1) 
(1.2) 
enable us to limit ourselves to the connectives -,, 1\, V. The truth of 
these two equations is easily verified by examining the "truth" tables in 
Table 1.1, which show all four possibilities for the pair a", {3 ". 
With our use of the equal sign, all tautologies are equal to one another 
and likewise all unsatisfiable formulas are equal to one another. Since the 
equations 
a" = 1 for all v , 
{3 " = 0 for all v 
determine a to be a tautology and {3 to be unsatisfiable, it is natural to 
write 1 for any .W-formula which is a tautology and 0 for any .W-formula 
which is unsatisfiable. Thus a = 1 means that a is a tautology, and a = 0 
means that a is unsatisfiable. 
The system of .W-formulas, under the operations -,, 1\, V and involving 
the "constants" 0, 1 obeys algebraic laws, some of which are analogous to 
laws satisfied by the real numbers under the operations -, Â·, +; but there 
are some striking differences as well. Specifically, we have, for all .W-
formulas a, {3, y 
(aAl)=a 
(a/\-,a)=O 
(a/\0)=0 
(aAa)=a 
absorption: 
(aVO)=a 
contradiction; excluded middle: 
(av--,a)=l 
(avl)=l 
idempotency: 
(aVa)=a 

350 
Chapter 12 Propositional Calculus 
commutativity: 
(a/\f3)=(f3/\a) 
(aVf3)=(f3Va) 
associativity: 
(a 1\ ( f3/\ y )) = ((a 1\ f3) 1\ y) 
(a V ( f3 V y )) = ((a V f3) V y) 
distributivity: 
(a 1\ ( f3 V y )) = ((a 1\ f3) V (a 1\ y )) 
(a V ( f3/\ y )) = ((a V f3) 1\ (a V y )) 
De Morgan laws: 
-,(a/\f3)=(-,av -,f3) 
-,(aV/3)=(-,a/\ -,f3) 
double negation: 
These equations, which are easily checked using truth tables, are the basis 
of the so-called Boolean algebra. In each row, the equations on the left 
and right can be obtained from one another by simply interchanging all 
occurrences of" V" with "A" and of "0" with "1." This is a special case of 
a general principle. The truth tables in Table 1.2 show that if we think of 0 
as representing "TRUE," and 1, "FALSE" (instead of the other way 
around), the tables for "A" and "V" will simply be interchanged. Thus a 
being from another planet watching us doing propositional calculus might 
be able to guess that that was in fact what we were doing. But this being 
would have no way to tell which truth value we were representing by 0 and 
which by 1, and therefore could not say which of the two connectives 
represents "and" and which "or." Therefore we have the 
General Principle of Duality: Any correct statement involving A, V 
and 0, 1, can be translated into another correct statement in which 
0 and 1 have been interchanged and A and V have been inter-
changed. 
Of course, in carrying out the translation, notions defined in terms of 0, 
1, A, and V must be replaced by their duals. For example, the dual of "a 
is a tautology" is "a is unsatisfiable." (The first is "a" = 1 for all v"; the 
a 
f3 
1 
0 
1 
1 
0 
0 
0 
Table 1.2 
(aAf3) 
1 
0 
0 
0 
(a V {3) 
1 
0 

1. Formulas and Assignments 
second is "a'' = 0 for all v".) Thus the dual of the correct statement 
if a is a tautology, so is (a V {3) 
is the equally correct statement 
if a is unsatisfiable, so is (a 1\ {3) . 
351 
Returning to our list of algebraic laws, we note that in particular the 
operations 1\ and V are commutative and associative. We take advantage 
of this associativity to write simply 
A a;= (a 1 1\ a 2 1\ Â·Â·Â· 1\ ak) 
isk 
V a;= (a 1 V a 2 V Â·Â·Â· V ak) 
isk 
without bothering to specify any particular grouping of the indicated 
formulas. We freely omit parentheses that are not necessary to avoid 
ambiguity. 
Exercises 
1. For each of the following formulas tell whether it is (i) satisfiable, (ii) a 
tautology, (iii) unsatisfiable. 
(a) (( p :J ( q :J r)) :J (( p :J q) :J ( p :J r))). 
(b) ((p :J (q :J r)) ++ ((p 1\ q) :J r)). 
(c) (p 1\ ..,q). 
(d) ((p v q) :J p). 
(e) (( ..,(p :J q) :J (p 1\.., q)). 
2. Apply the general principle of duality to each of the following true 
statements: 
(a) (p V .., p) is a tautology. 
(b) (p :J (q :J p)) is a tautology. 
3. Prove that if a and {3 are formulas, then a = {3 if and only if the 
formula (a ++ {3) is a tautology. 
4. Verify the laws of absorption, contradiction, etc. given in this section. 
5. Let .N be a set of atoms, and define 
.w() =.W' 
.W:,+ 1 =.W' U {..,a, (a 1\ {3), 
(a V {3), (a :J {3), (a++ {3) I a, {3 E.JÂ¥;,}. 
Show by induction on n that for all a E .w;, , the number of left 

352 
Chapter 12 Propositional Calculus 
parentheses equals the number of right parentheses. Conclude that 
any propositional formula over .91 has an equal number of left and 
right parentheses. 
6. 
Let .91, .91' be sets of atoms such that .91 c;;;, .91', and let v, v' be 
assignments on .91, .91', respectively, such that v( a) = v '(a) for all 
atoms a in .91. Define .w;. , n ~ 0, as in Exercise 5, and show by 
induction on n that v( a) = v '(a) for all formulas a E .w;.. Conclude 
that v( a) = v '(a) for all propositional formulas over .91. 
2. Tautologicallnference 
Let y 1 , y 2 , â¢â¢â¢ , 'Yn, y be .91-formulas. Then we write 
'Y1 ''Yz' Â· Â· Â·' 'Yn I= 'Y 
and call y a tautological consequence of the premises y 1 , â¢â¢â¢ , 'Yn if for every 
assignment v on .91 for which yf = y~ = Â· Â· Â· = 'Ynv = 1, we have also 
yv = 1. This relation of tautological consequence is the most important 
concept in the propositional calculus. However, we can easily prove 
Theorem 2.1. The relation y 1 , y 2 , â¢â¢â¢ , 'Yn I= y is equivalent to each of the 
following: 
1. the formula (( y 1 A y 2 A â¢Â·â¢ A 'Yn) :::> y) is a tautology; 
2. the formula ( y 1 A y 2 A Â·Â·Â· A 'Yn A -, y) is unsatisfiable. 
Proof. 
(( y 1 A y 2 A Â·Â·Â· A y) :::> y) is not a tautology just in case for 
some assignment v, ( y 1 A y 2 A Â· Â·Â· A y)v = 1 but yv = 0. That is, just in 
case for some assignment v, yf = 'Yz = Â· Â· Â· = y,:' = 1 but yv = 0, which 
means simply that it is not the case that y 1 , y 2 , â¢â¢â¢ , 'Yn I= y. Likewise 
( y 1 A y 2 A Â· Â· Â· A 'Yn A -, y) 
is satisfiable if and only if for some assignment v, yf = y~ = Â·Â·Â· = y,:' = 
(-, 'Y )v = 1, i.e., yf = 'Y~ = Â· Â· Â· = 'Ynv = 1, but 'Yv = 0. 
â¢ 
Thus the problem of tautological inference is reduced to testing a 
formula for satisfiability, or for being a tautology. Of course, in principle, 
such a test can be carried out by simply constructing a truth table. 
However, a truth table for a formula containing n different atoms will 
require 2 n rows. Hence, truth table construction may be quite unfeasible 
even for formulas of modest size. 

3. Normal Forms 
353 
Consider the example 
((p 1\ q) :::>(r 1\ s)),((pt 1\ qt) =>rt),((rt 1\ s) =>st),p,q,qt ,ptl=st. 
(2.1) 
Since there are eight atoms, a truth table would contain 28 = 256 rows. In 
this example we can reason directly. If v makes all the premises TRUE, 
then (p 1\ q)" = (p 1 1\ q1)" = 1. Therefore, (r 1\ s)" = rr = 1, and in 
particular s" = 1. Thus, (r 1 1\ s)" = 1 and finally, sr = 1. We will use 
Theorem 2.1 to develop more systematic methods for doing such problems. 
Exercises 
1. Which of the following are correct? 
(a) (p :::>q), PI= q. 
(b) (p :::> q ), q I= p. 
(c) (p :::>q), ..,q I= â¢PÂ· 
(d) (p:::>(q:::>r)),(..,svp),ql=(s:::>r). 
2. Apply Theorem 2.1 to Exercise 1. 
3. Prove or disprove each of the following. 
(a) 
a, {3 I= y if and only if a I= ( {3 :::> y ). 
(b) 
a I= {3 and {3 I= a if and only if a = {3. 
(c) 
if a I= {3 or a I= y then a I= ( {3 V y ). 
(d) 
if a I= {3 or a I= y then a I= ( {3 1\ y ). 
(e) 
if a I= {3 and a I= y then a I= ( {3 1\ y ). 
(f) 
if a I= {3 and a I= y then a I= ( {3 V y ). 
(g) if a I= .., a then .., a is a tautology. 
(h) if a, {3 I= y then a I= y or {3 I= y. 
(i) 
if a I= y then a, {3 I= y. 
(j) 
if a I= ( {3 V y) then a I= {3 or a I= y. 
4. (a) Show that if a is unsatisfiable then a 1= {3 for any formula (3. 
(b) Show that if {3 is a tautology then a I= {3 for any formula a. 
3. 
Normal Forms 
We will now describe some algebraic procedures for simplifying .91'-
formulas: 
(I) ELIMINATE :::> AND -
. 

354 
Chapter 12 Propositional Calculus 
Simply use Eq. (1.2) for each occurrence of ++ â¢ After all such occur-
rences have been eliminated, use Eq. (1.1) for each occurrence of :::> . 
Assuming (I) accomplished, we move on to 
(II) MOVE -, INWARD. 
For any occurrence of -, that is not immediately to the left of an atom 
either 
1. the occurrence immediately precedes another -,, in which case the 
pair -, -, can be eliminated using the law of double negation; or 
2. the occurrence immediately precedes an ..W-formula of the form 
(a A {3) or (a V {3 ), in which case one of the De Morgan laws can 
be applied to move the -, inside the parentheses. 
After (II) has been applied some finite number of times, a formula will 
be obtained to which (II) can no longer be applied. Such a formula must 
have each -, immediately preceding an atom. 
As an example of the use of (I) and (II) consider the formula 
(((p ++ q) :::> (r :::> s)) A (q :::> -,(pAr))). 
(3.1) 
Eliminating ++ gives 
((((p :::>q) A (q :::>p)) :::> (r :::>s)) A (q :::>-,(pAr))). 
Eliminate :::> : 
( -, (( -, p V q) A ( -, q V p)) V ( -, r V s)) A ( -, q V -, ( p A r)). (3 .2) 
Move -, inward: 
(-,(-,p V q) V -,(-,q V p) V (-,r V s)) A (-,q V -,p V -,r). 
Move -, inward: 
((p A -,q) V (q A -,p) V -,r V s) A ( -,q V -,p V -,r). (3.3) 
A formula A is called a literal if either A is an atom or A is -,a, where 
a is an atom. Note that if A = -,a, for a an atom, then -,A = -,-,a= a. 
For a an atom it is convenient to write a for -,a. 
With this notation (3.3) becomes 
((p A ij) V (q Ajj) V r V s) A (ij V jj V r). 
(3.4) 
The distributive laws can be used to carry out further simplification, 
analogous to "multiplying out" in elementary algebra. However, the fact 

3. Normal Forms 
355 
that there are two distributive laws available is a complication because the 
"multiplying out" can proceed in two directions. As we shall see, each 
direction gives rise to a specific so-called normal form. 
A handy technique that makes use of the reader's facility with elemen-
tary algebra is to actually replace the symbols A, V by + , Â· and then 
calculate as in ordinary algebra. Since there are two distributive laws 
available, correct results will be obtained either by replacing A by + and 
V by Â· or vice versa. Thus, writing Â· for A (and even omitting the Â· as in 
elementary algebra) and + for v, (3.4) can be written 
(pq + qjj + r + s) Â· (q + p + r) 
= pqq + pqp + pqr + qjjq + qjjp + qjjr + rq + rp + ,.,. 
+ sq + sp + sr 
= pq + o + pqr + o + qp + qjjr + rq 
+ rp + r + sq + sp + sr 
= (p A q) V (p A q A r) V (q A p) V (q A p A r) 
V (r A q) V (r A p) V r V (s A q) V (s A p) V (s A r), (3 .5) 
where we have used the principles of contradiction and absorption. Alter-
natively, writing + for A and Â· for V, (3.4) can be written 
(p + q)(q + p)rs + qpr 
= (pq + pp + qq + qp)rs + qpr 
= (pq + 1 + 1 + qp)rs + qpr 
= pqrs + qprs + qpr 
= (p v q v r v s) A (q v p v r v s) A (q v p v r). 
(3.6) 
Let A; be a sequence of distinct literals, 1 :::;; i :::;; n. Then the formula 
V;, n A; is called an V -clause and the formula /\;, n A; is called an 
A-clause. A pair of literals A, A' are called mates if A' = -,A. We have 
Theorem 3.1. Let A; be a literal for 1 :::;; i :::;; n. Then the following are 
equivalent: 
1. vi :5 n A; is a tautology; 
2. /\;, n A; is unsatisfiable; 
3. some pair A;, Ai, 1 :::;; i, j :::;; n, is a pair of mates. 

356 
Chapter 12 Propositional Calculus 
Proof. If \ = -, A;, then obviously, V;, n A; is a tautology and A; 5, n A; 
is unsatisfiable. If, on the other hand, the A; contain no pair of mates, then 
there are assignments v, w such that v(A) = 1, w(A) = 0 for 1 ~ i ~ n. 
Then (Vi5.n A;)w = 0, (A;5,n A;)''= 1, so that Vi5.n A; is not a tautology 
and A; 5, n A; is satisfiable. 
â¢ 
Let K;, 1 ~ i ~ n, be a sequence of distinct 
V -clauses. Then the 
.JÂ¥'-formula A;< n K; is said to be in conjunctive normal form (CNF). Dually, 
if K;, 1 ~ i ~ n, is a sequence of distinct A -clauses, then the .JÂ¥'-formula 
V; 5. n K; is in disjunctive normal form (DNF). Note that (3.6) is in CNF and 
(3.5) is in DNF. We say that (3.6) is a CNF of (3.1) and that (3.5) is a DNF 
of (3.1). It should be clear that the procedures we have been describing 
will yield a CNF and a DNF for each .w'-formula. Thus we have 
Theorem 3.2. There is an algorithm which will transform any given 
.w'-formula a into a formula {3 in CNF such that {3 = a. There is a similar 
(in fact, dual) algorithm for DNF. 
Because of Theorem 2.1, the following result is of particular importance. 
Theorem 3.3. A formula in CNF is a tautology if and only if each of its 
V -clauses is a tautology. Dually, a formula in DNF is unsatisfiable if and 
only if each of its A -clauses is unsatisfiable. 
Proof. 
Let a = A; 5,n K;, where each K; is an V -clause. If each K; is a 
tautology, then for any assignment v we have K;" = 1 for 1 ~ i ~ n, so that 
a" = 1; hence a is a tautology. If some K; is not a tautology, then there is 
an assignment v such that K;" = 0; hence a" = 0 and a is not a tautology. 
The proof for DNF is similar. Alternatively, we can invoke the general 
principle of duality. 
â¢ 
Let us try to use these methods in applying Theorem 2.1 to example 
(2.1). First, using Theorem 2.1(1), we wish to know whether the following 
formula is a tautology: 
((((p Aq):::) (r As))A((p 1 Aq1) :::)r 1) 
A((r 1 As) :::)s1) Ap A q A q1 Ap 1) :::)s1). 
Use of (I) yields 
(-,((-,(p Aq) V (r As)) A (-,(p 1 Aq1) Vr 1) 
A(-,(r 1 As) Vs 1) Ap Aq Aq1 Ap 1) Vs1). 
Use of (II) gives 
(-, (-, (p A q) V (r As)) V-, (-, (p 1 A q1) V r 1) 
V -,(-,(r 1 As) Vs 1) V -,pV -,qV -,q1 V -,p1 Vs 1). 

3. Normal Forms 
357 
Use of (II) again yields 
One final use of (II) gives 
((p A q A ( ....,r V ....,s))V(p1 A q1 A ....,r1) 
V(r 1 As A ....,s1) v .pv ....,qv â¢q1 V â¢P1 Vs1).(3.7) 
To apply Theorem 3.3, it is necessary to find a CNF of (3. 7). So we replace 
A by + and V by Â·: 
and see that the CNF of (3.7) will consist of 27 clauses. Here are three 
"typical" clauses from this CNF: 
(p V p 1 V r 1 V p Vii V ii1 V p1 V s 1) 
(r v s v q1 v r 1 v p vii v ii1 v p1 V s1) 
(q v r1 v s1 v p vii v ii1 v p1 v s 1). 
Each of these clauses contains a pair of literals that are mates: p, p in the 
first (and also p 1 , p1); q1 , ii1 in the second; and q, ii in the third (also 
s1 , s1). The same will be true for the remaining 24 clauses. But this is 
clearly not the basis for a very efficient algorithm. What if we try Theorem 
2.1(2) on the same example? Then we need to show that the following 
formula is unsatisfiable: 
Using (I) we obtain 
((...., (p A q) V (r As)) A (...., (p1 A q1) V r 1) 
A(....,(r 1 As) Vs1) Ap Aq Aq1 Ap1 A ....,s1). 
(3.9) 

358 
Chapter 12 Propositional Calculus 
Using (II) we obtain 
((-, p V -, q V (r As)) A (-, p 1 V -, q1 V r 1) 
A(-,r 1 v -,sVs 1) ApAqAq 1 Ap 1 A -,s 1). 
(3.10) 
To find a DNF formula equal to this we replace A by Â· and V by +, 
obtaining 
(p + ij + rs)(p 1 + ij1 + r 1)(r 1 + s + s1)pqq1p 1s1 â¢ 
But this is exactly the same as (3.8) except that each literal has been 
replaced by its mate! Once again we face essentially the same 27 clauses. 
Suppose we seek a formula in CNF equal to (3.10) instead of a formula 
in DNF. We need only replace A by + and V byÂ·: 
In this manner, we get a formula in which almost all "multiplying out" has 
already occurred. The CNF is simply 
(p V ij V r)A(p V ij V s)A(p 1 V ij1 V r 1) 
(3.11) 
It consists of nine short, easily obtained clauses. 
A moment's reflection will show that this situation is entirely typical. 
Because the formula of Theorem 2.1(2) has the form 
(yl A Yz AÂ·Â·Â· A Yn A -,y), 
we can get a CNF formula simply by obtaining a CNF for each of the 
(ordinarily short) formulas y 1 , y2 , â¢â¢â¢ , Yn,-, y. However, to obtain a DNF, 
which according to Theorem 3.3 is what we really want, we will have to 
multiply out (n + 1) polynomials. If, say, each of y 1 , â¢â¢â¢ , Yn, -, y is an 
V -clause consisting of k literals, then the DNF will consist of kn+ 1 
A -clauses. And the general principle of duality guarantees (as we have 
already seen in our particular example) that the same discouraging arith-
metic will emerge should we attempt instead to use Theorem 2.1(1). In this 
case a DNF will generally be easy to get, whereas a CNF (which is what we 
really want) will require a good deal of computing time. 
These considerations lead to the following problem: 
Satisfiability Problem. Find an efficient algorithm for testing an .w'-
formula in CNF to determine whether it is truth-functionally satisfiable. 

3. Normal Forms 
359 
This problem has been of central importance in theoretical computer 
science, not only for the reasons already given, but also for others that will 
emerge in Chapter 15. 
Exercises 
l. Find CNF and DNF formulas equal to each of the following. 
(a) ((p A (q V r)) V (q A (p V r))). 
2. 
(b) ((-,pV(pA -,q))A(rV(-,pAq))). 
(c) (p :::> (q ++ r)). 
Find a DNF formula that has the truth table 
p 
q 
r 
1 
1 
1 
0 
0 
1 
1 
1 
1 
0 
1 
1 
0 
0 
1 
1 
1 
1 
0 
0 
0 
1 
0 
1 
1 
0 
0 
0 
0 
0 
0 
0 
[Hint: The second row of the table corresponds to the A -clause 
(-, p A q A r ). Each row for which the value is 1 similarly determines 
an A -clause.] 
3. Show how to generalize Exercise 2 to obtain a DNF formula corre-
sponding to any given truth table. 
4. 
Describe a dual of the method of Exercise 3 which, for any formula a, 
gives a DNF formula {3 such that a= -, {3. Then show how to turn 
-, {3 into a CNF formula y such that a = y. Apply the method to the 
truth table in Exercise 2. [Hint: Each row in the truth table for which 
the value is 0 corresponds to an A -clause which should not be true.] 
5. 
Let .91 = {p, q, r}. 
(a) Give a DNF formula a over .91 such that a'Â· = 1 for exactly three 
assignments v on .91. 
(b) Give a CNF formula {3 over .91 such that {3 '' = 1 for exactly three 
assignments v on .91. 

360 
Chapter 12 Propositional Calculus 
6. 
(a) Let a be 
(p A q A r)V(p A q A --,r)V(p A --,q A r) 
V(p A --,q A --,r). 
Give DNF formulas {3, y, S with 3,2, 1 A-clauses, respectively, 
such that a = {3 = y = S. 
(b) Let a be 
(p V q V r)A(p V q V --,r)A(p V --,q V r) 
A(p V --,q V --,r). 
Give CNF formulas {3, y, S with 3, 2, 1 V -clauses, respectively, 
such that a = {3 = y = S. 
7. 
Give a CNF formula a with two v -clauses such that a =/= {3 for all 
CNF formulas {3 with one V -clause. 
8. 
Use a normal form to show the correctness of the inference 
(p:>q),(rv --,q),--,(pAr) F= â¢PÂ· 
4. 
The Davis- Putnam Rules 
In order to make it easier to state algorithms for manipulating formulas in 
CNF, it will be helpful to give a simple representation of such formulas as 
sets. From now on we use the word clause to mean V -clause. We 
represent the clause K = V j, m Aj as the set K = {AN ~ m}, and we 
represent the formula a = /\; < n K;, where each K; is a clause, as the set 
a = {K;Ii ~ n}. In so doing we lose the order of the clauses and the order 
of the literals in each clause; however, by the commutative laws, this does 
not matter. 
It is helpful to speak of the empty set of literals as the empty clause, 
written 0, and of the empty set of clauses as the empty formula, written 
simply 0. Since it is certainly true, although vacuously so, that there is an 
assignment (in fact any assignment will do) which makes every clause 
belonging to the empty formula true, it is natural and appropriate to agree 
that the empty formula 0 is satisfiable (in fact, it is a tautology). On the 
other hand, there is no assignment which makes some literal belonging to 
the empty clause o true (because there are no such literals). Thus, we 
should regard the empty clause o as being unsatisfiable. Hence any 
formula a such that 0 E a will be unsatisfiable as well. 

4. The Davis- Putnam Rules 
361 
We will give some rules for manipulating formulas in CNF that are 
helpful in designing algorithms for testing such formulas for satisfiability. 
By Theorem 3.1, a clause K is a tautology if and only if A,-, A E K for 
some literal A. Now, if K E a and K is a tautologous clause, then a is 
satisfiable if and only if a - {K} is. Hence, we can assume that the sets of 
clauses with which we deal contain no clauses which are tautologies. The 
following terminology is helpful: a clause K = {A}, consisting of a single 
literal, is called a unit. If a is a set of clauses and A is a literal, then a 
clause K is called A-positive if A E K, K is called A-negative if -,A E K, 
and K is called A-neutral if K is neither A-positive nor A-negative. Since 
tautologous clauses have been excluded, no clause can be both A-positive 
and A-negative. We write a: for the set of A-positive clauses of a, a; 
for the set of A-negative clauses of a, and a~ for the set of A-neutral 
clauses of a. Thus for every literal A, we have the decomposition a = 
a: u a; u a~. Finally, we write 
POSA(a) =a~ U {K- {A}IK E a:}, 
NEGA(a) =a~ U {K- {-, A}IK E a;}. 
Our main result is 
Theorem 4.1 
(Splitting Rule). 
Let a be a formula in CNF, and let A be 
a literal. Then a is satisfiable if and only if at least one of the pair 
POSA(a) and NEGA(a) is satisfiable. 
Proof. 
First let a be satisfiable, say au = 1. Thus Ku = 1 for all K E a. 
That is, for each K E a, there is a literal JL E K such that JLu = 1. Now, 
we must have either Au = 1 or Au = 0. Suppose first that Au = 0. We know 
that for each K E a: , there is a literal JL E K such that JLu = 1. Thus this 
JL is not A. Thus, for K E a:, (K- {A})D = 1. Hence, in this case, 
POSA(a)u = 1. If, instead, Au = 1, we can argue similarly that for each 
K E a;, (K- {-, A})u = 1 and hence that NEGA(a)u = 1. 
Conversely, let POSA(a)u = 1 for some assignment v. Then we define 
the assignment w by stipulating that 
for all literals 
JL =1= A, -, A. 
Now, if K E a~, then Kw = Ku = 1; if K E a:, then Kw = (K- {A})w = 
(K- {A})u = 1; finally, if K E a;, then Kw = 1 because (-, A)w = 1. Thus, 
aw = 1. 
If, instead, NEGA(a)u = 1 for some assignment v, we define w by 
for all literals JL =I= A, -, A. 

362 
Chapter 12 Propositional Calculus 
Then if 
K E 
a~, we have 
Kw = Kv = 1; if 
K E a;:, then 
Kw = 
(K- {--, A})w = (K- {--, A})v = 1; finally, if K E a;, then Kw = 1 because 
Aw = 1. Thus again aw = 1. 
â¢ 
This theorem has the virtue of eliminating one literal, but at the price of 
considering two formulas instead of one. For this reason, it is of particular 
interest to find special cases in which we do not need to consider both 
POSA(a) and NEGA(a). 
Thus, suppose that a;:= 0. Then NEGA(a) = a~J ~ POSA(a). Hence, 
in this case, for any assignment v we have POS A (a)" = 1 implies 
NEGA(a)v = 1. Therefore, we conclude 
Corollary 4.2 
(Pure Literal Rule). If a;:= 0, then a is satisfiable if 
and only if NEGA(a) = a~ is satisfiable. 
For another useful special case, suppose that the unit clause {A} E a. 
Then, since {A} - {A} = D , we conclude that D E POS A (a). Hence, 
POS A (a) is unsatisfiable, and we have 
Corollary 4.3 
(Unit Rule). If {A} E a, then a is satisfiable if and only if 
NEGA(a) is satisfiable. 
To illustrate this last corollary by an example, let a be (3.11), which is a 
CNF of (3.9). Using the set representation, 
a= {{jj,q,r},{jj,q,s},{jj1 ,q1 ,r1},{r1 ,s,s1},{p},{q},{q1},{p1},{s1}}. 
(4.1) 
Thus, there are nine clauses, of which five are units. Using the unit clause 
{p}, Corollary 4.3 tells us that a is satisfiable if and only if NEG,(a) is. 
That is, we need to test for satisfiability the set of clauses 
Using the unit rule again, this time choosing the unit clause {q}, we reduce 
to 
Using the unit clause {s}, we get 

4. The Davis- Putnam Rules 
Successive uses of the unit clauses {q1}, {p1}, {81} yield 
{{r}, {jj1 , r 1}, {r 1 , s 1}, {p1}, {s1}}; 
{{r}, {r 1}, {r1 , s 1}, {s1}}; 
{{r}, {r1}, {r1}}. 
363 
This last, containing the unit clauses {r 1} and {r1}, is clearly unsatisfiable. 
Or, alternatively, applying the unit rule one last time, we obtain 
which is unsatisfiable because it contains the empty clause D. 
So we have shown by this computation that (4.1), and therefore (3.9), is 
unsatisfiable. And by Theorem 2.1, we then can conclude (once again) that 
the tautological inference (2.1) is valid. 
A slight variant of this computation would begin by applying Corollary 
4.2, the pure literal rule, to (4.1), using the literal r. This has the effect of 
simply deleting the first clause. The rest of the computation might then go 
as previously, but with the initial clause deleted at each stage. 
For another example, recall (3.6), which was obtained as a CNF of (3.1). 
Written as a set of clauses this becomes 
{3 = {{p,q,r,s},{ij,p,r,s},{ij,p,r}}. 
(4.2) 
Here the pure literal rule can be applied using either of the literals r, s. 
Thus, we have that {3 is satisfiable if and only if {3,0 is satisfiable, if and 
only if {3.0 is satisfiable. And we have 
f3.o = {{ij,jj,r}}. 
From the first we see at once that {3 is satisfiable; if we wish to use the 
second, we can note by inspection that ( f3.0 )v = 1, where v(q) = v(p) = 
v(r) = 0, or we can use the pure literal rule a second time (using any of 
the three available literals) and once again arrive at the empty formula 0. 
We next turn to an example that has no unit clauses and to which the 
pure literal rule is not applicable: 
a= {{ij,p},{r,p},{jj,ij},{jj,s},{q,r},{q,s}}. 
Thus we are led to use the splitting rule forming, say, 
POS/a) = {{ij},{r},{q,r},{q,s}}, 
NEG/ a) = {{ij}, {s}, {q, r}, {q, s}}. 

364 
Chapter 12 Propositional Calculus 
Applying the pure literal rule once and then the unit rule twice to 
POS,(a), we obtain successively 
{{q}, {r}, {q, r}}, 
{{r}, {r}}, 
{ o}, 
so that POS,(a) is unsatisfiable. Doing the same to NEG,(a) we obtain 
successively 
{{q}, {s}, {q, s}}, 
{{s},{s}}, 
{ o}, 
so that NEG,(a) is likewise unsatisfiable. By Theorem 4.1, we can thus 
conclude that a is unsatisfiable. 
These examples suggest a rather systematic recursive procedure (some-
times known as the Davis-Putnam procedure) for testing a given formula 
a in CNF for satisfiability. The procedure as we shall describe it will not 
be completely deterministic; there will be situations in which one of a 
number of literals is to be selected. We will write the recursive procedure 
using two variables, y for a set of clauses and Y for a stack of sets of 
clauses. We write TOP(.Y) for the set of clauses at the top of the stack .Y, 
POP(.Y) for Y after TOP(.Y) has been removed, PUSH( {3,.9) for the 
stack obtained by putting {3 on the top of Y, and 0 for the empty stack. 
The procedure is as follows: 
y~ a; Y~ 0; 
while y =I= 0 and ( 0 $. y or Y =I= 0) 
if 0 E 'Y 
then y ~ TOP(.Y); Y ~ POP(.Y); 
else if 'YA- = 0 
then y ~ y~; 
else if {A} E y 
then y ~ NEG"( y ); 
else Y~ PUSH(NEGA(y),.Y); y ~ POSA(y); 
end while 
if y = 0 
then return SATISFIABLE 
else return UNSATISFIABLE 
Thus, this procedure will terminate returning SATISFIABLE whenever 
y is the empty formula 0, whether or not the stack Y is empty. (This is all 
right because the original formula will be satisfiable if any one of the 
formulas obtained by repeated uses of the splitting rule is satisfiable, and, 
of course, 0 is satisfiable.) The procedure will terminate returning UN-
SATISFIABLE if o E y and Y = 0. (Here, y is unsatisfiable, and no 
formulas remain in Y as the result of uses of the splitting rule.) If neither 

4. The Davis- Putnam Rules 
365 
of these termination conditions is satisfied, the algorithm will first test for 
o E y. If o E y, it rejects (since y is unsatisfiable) and "pops" the stack. 
Otherwise it attempts to apply first the pure literal rule and then the unit 
rule. If both attempts fail, it chooses (nondeterministically) some literal A, 
takes POSi y) as the new formula to work on, and "pushes" NEGA( y) 
onto the stack for future reference. 
It is not difficult to see that the algorithm just given must always 
terminate. Let us say that a set of clauses a reduces to a set of clauses {3 if 
for each clause K in {3 there is a clause "K in a such that K ~ "K. Then, at 
the beginning of each pass through the while loop, y is a set of clauses to 
which a reduces and the stack consists of a list of sets of clauses to each 
of which a reduces. Since, for a given a, there are only a finite number of 
distinct configurations of this kind, and none can be repeated, the algo-
rithm must eventually terminate. 
Exercises 
1. Let a be {{p, q, r}, {p, q}, {jj, r}}. For A = p, q, r, jj, q, r, give a;, a;, 
a~, POSA(a), NEGA(a). Which of these sets are necessarily equal? 
2. Use the Davis-Putnam rules to show the correctness of the inference 
in Exercise 3.8. 
3. Use the Davis-Putnam rules to show the correctness of the following 
inference. 
If John went swimming, then he lost his glasses and did not go to the 
movies. If John ate too much meat and did not go to the movies, then 
he will suffer indigestion. Therefore, if John ate too much meat and 
went swimming, then he will suffer indigestion. 
4. Test the following set of clauses for satisfiability: 
{p, q, r, s} 
{jj, q, r} 
{i'' s} 
{q, r} 
{p, s}. 
5. Modify the Davis-Putnam procedure so that when the answer is 
SATISFIABLE on input a, it returns an assignment v such that 
a''= 1. 

366 
Chapter 12 Propositional Calculus 
6. 
How many distinct computations can be performed by the 
Davis-Putnam procedure on input {{p, q, r}, {p, q}, {jj, r}}? 
7. 
Let a be a CNF formula with n distinct atoms. 
(a) What is the maximum number of formulas that can be on the 
stack in the Davis-Putnam procedure at any given time? 
(b) Suppose that a is satisfiable. Show that if the Davis-Putnam 
procedure always makes the right choice of A at each stage, the 
while loop executes no more than n times. 
(c) 
How many times must the while loop execute on input 
{{p, q}, {p, q}, {jj, q}, {jj, q}}? 
On input 
{{p, q, r}, {p, q, r}, {p, q, r}, {p, q, r}, 
{jj, q, r}, {jj, q, r}, {jj, q, r}, {jj, q, r}}? 
5. 
Minimal Unsatisfiability and Subsumption 
We begin with 
Theorem 5.1. Let the clauses K 1 , Kz satisfy K 1 c Kz. Then if a is a 
formula in CNF such that K 1 , Kz E a, then a is satisfiable if and only if 
a -
{K2} is satisfiable. 
Proof. Clearly, if a is satisfiable, so is a -
{K2}. 
Conversely, if (a -
{K2})V = 1, then Kf = 1, so that also Kf = 1. Hence, 
av = 1. 
â¢ 
Thus, if in fact K1, Kz E a and K1 c K2 , we may simply drop K2 and test 
a -
{K2} for satisfiability. The operation of dropping K2 in such a case is 
called subsumption. Unfortunately, there is no efficient algorithm known 
for testing a large set of clauses for the possibility of applying subsumption. 
Definition. A finite set of clauses a is called minimally unsatisfiable if 
1. a is unsatisfiable, and 
2. for all {3 c a, {3 is satisfiable. 
Definition. A finite set of clauses a is said to be linked if whenever 
A E K 1 and K 1 E a, there is a clause K 2 E a such that ..., A E K 2 . That is, 
each literal in a clause of a has a mate in another clause of a. 

6. Resolution 
367 
Then it is very easy to prove 
Theorem 5.2. Let a be minimally unsatisfiable. Then 
1. for no K1 , K2 E a can we have K1 c K2 , and 
2. a is linked. 
Proof. 
Condition 1 is an immediate consequence of Theorem 5.1. To 
verify 2, suppose that a is minimally unsatisfiable but not linked. Then, 
there is a literal A in a clause K E a such that the literal -, A occurs in 
none of the clauses of a, i.e., a;= 0. Thus, by the pure literal rule, a~ is 
unsatisfiable. But since a~ c a, this is a contradiction. 
â¢ 
Exercise 
1. Give a minimally unsatisfiable CNF formula with four clauses. 
6. 
Resolution 
Let K1 , K2 be clauses such that A E K1 and -,A E K2 â¢ Then we write 
resA(K 1 , K2 ) = (K1 -{A}) U (K2 -
{-,A}). 
The clause resA(K1 , K2 ) is then called the resolvent of K1 , K2 with respect to 
the literal A. The operation of forming resolvents has been the basis of a 
very large number of computer programs designed to perform logical 
deductions. We have 
Theorem 6.1. Let A be an atom and let K1 , K2 be clauses such that 
A E K1, -,A E K2 . Then 
Proof. 
Let v be an assignment such that Kf = 
K~ = 1. Now if Au = 1, 
then (K2 -
{-, AW = 1, while if Au= 0, then (K1 -
{A})V = 1. In either 
case, therefore, resA(K 1 , K2)u = 1. 
â¢ 
Let a be a finite set of clauses. A sequence of clauses K1 , K2 , â¢â¢â¢ , Kn = K 
is called a resolution derivation of K from a if for each i, 1 :;;; i :;;; n, either 
K; E a or there are j, k < i and a literal A such that K; = resA(Ki, Kk). A 
resolution derivation of D from a is called a resolution refutation of a. We 
define 

368 
Chapter 12 Propositional Calculus 
We have 
Theorem 6.2. 
Let a be a formula in CNF and let A be a literal. Then a 
is satisfiable if and only if RES/a) is satisfiable. 
Proof. 
First let a" = 1. Then if K E a1, we have also K E a, so that 
K" = 1. Furthermore, if K = resA(K1 , K2), with K1 E a;, Kz E a;, then 
Kf = 1, Kf = 1, so that, by Theorem 6.1, K" = 1. Since for all 
K E 
RESA(a), we have K" = 1, it follows that RESA(a)" = 1. 
Conversely, let RESA(a)" = 1. We claim that either POSA(a)" = 1 or 
NEGA(a)" = 1. For, suppose that POSA(a)" = 0. Since a1 ~ RESA(a), we 
have (a1)" = 1. So for some K1 E a;, we must have (K1 -
{A})V = 0. 
However, for all Kz E a; and this K1 , we must have resA(K 1 , K2)" = 
[(K1 -
{A}) U (K2 -
{-, A})]D = 1. Thus, for all K2 E a; we have (K2 -
{-, A})V = 1, i.e., NEG A( a)"= 1. This proves our claim that either POSA(a) 
or NEGA(a) must be satisfiable. By Theorem 4.1, i.e., the splitting rule, a 
is satisfiable. 
â¢ 
Theorem 6.2 suggests another procedure for testing a formula a in CNF 
for satisfiability. As with the Davis-Putnam rules, seek a literal of a to 
which the pure literal or unit rule can be applied. If none is to be found, 
choose a literal A of a and compute RESA(a). Continue recursively. 
As with the Davis-Putnam procedure, this procedure must eventually 
terminate in { D} or 0; this is because the number of literals is successively 
diminished. This procedure has the advantage of not requiring a stack of 
formulas, but the disadvantage that the problem may get considerably 
larger because of the use of the RESA(a) operation. Unfortunately, the 
present procedure is also called the Davis-Putnam procedure in the 
literature. To add to the confusion, it seems that computer implementa-
tions of the "Davis-Putnam procedure" have been almost exclusively of 
the procedure introduced in Section 4, whereas theoretical analyses of the 
computational complexity of the "Davis-Putnam procedure" have tended 
to deal with the procedure we have just introduced. 
Theorem 6.3. Let a be a formula in CNF and suppose that there is a 
resolution derivation of the clause 
K from a. Then a'Â· = 1 implies 
K" = 1. 
Proof. 
Let K1 , K2 , â¢â¢â¢ , Kn = K be a resolution derivation of K from a. 
We shall prove that K;" = 1 for 1 :::;; i :::;; n, which will prove the result. To 
prove this by induction, we assume that K}' = 1 for all j < i. (Of course for 
the case i = 1, this induction hypothesis is true vacuously.) Now, there are 
two cases. If K; E a, then K;" = 1. Otherwise K; = resA(Kj, Kk), where 

6. Resolution 
369 
j, k < i. Hence, by the induction hypothesis, Kj' = Kf: = 1. So by Theorem 
6.1, Kt = 1. 
â¢ 
Theorem 6.4 (Ground Resolution Theorem). The formula a in CNF is 
unsatisfiable if and only if there is a resolution refutation of a. 
Proof. First let there be a resolution refutation of a, but suppose that 
nevertheless au = 1. Then, by Theorem 6.3, ov = 1, which is impossible. 
Conversely, let a be unsatisfiable. Let A1 , A2 , â¢â¢â¢ , Ak be a list of all the 
atoms that occur in a. Let 
a 0 =a, 
i = 1,2, ... ,k. 
Clearly each a; contains only the atoms Aj for which i < j .:5; k. Hence ak 
contains no atoms at all, and must be either 0 or {0}. On the other hand, 
by Theorem 6.2, we have that a; is unsatisfiable for 0 .:5; i .:5; k. Hence 
ak = { 0}. Now, let the sequence K 1 , Kz, â¢â¢â¢ , Km of clauses consist, first, of 
all of the clauses of a 0 = a, then, all of the clauses of a 1 , and so on 
through all of the clauses of ak. But this last means that 
Km = 0. 
Moreover, it is clear from the definition of the RESA operation that 
K 1 , Kz, â¢â¢â¢ , Km is a resolution derivation. 
â¢ 
To illustrate the ground resolution theorem, we apply it to (4.1) to show, 
once again, that (3.9) is unsatisfiable. Here then is a resolution refutation 
of the formula a of (4.1): 
{jj, q, s}, {i1, s, s1}, {jj, q, i 1 , s 1}, {p}, {q, i 1 , s1}, {q}, {i1, s 1}, 
{sl}, {it}, {jjl 'iit 'rl}, {jjl 'fit}, {ql}, {jjl}, {pi}, 0. 
Exercises 
1. (a) Use the resolution method to answer Exercise 1.1. 
(b) Do the same for Exercise 2.1. 
2. Give a resolution refutation that shows the correctness of the infer-
ence of Exercise 3.8. 
3. Do the same for the inference of Exercise 4.3. 
4. Let a 0 , â¢â¢â¢ , an be CNF formulas, and let {3 be a DNF formula 
V;, m /3;. Show that a 0 , â¢â¢â¢ , an I= {3 if and only if there is a resolu-
tion refutation of U;, n a; U {..., /3; I i .:5; m}. 

370 
Chapter 12 Propositional Calculus 
5. Let A be an atom and let K1 , K2 be clauses such that A E K1 , 
-, A E K2 . Prove or disprove the following. 
(a) reSA(K1 , K2 ) I= (K1 V K2 ). 
(b) resA(K1, K2) I= K1 or resA(K" K2) I= K2 â¢ 
(c) 
resA(K1 , K2 ) I= (K1 A K2 ). 
(d) ifresA(K1 ,K2)isvalid,then (K1 A K2 ) is valid. 
6. Let a be a formula in CNF and let A be a literal. Prove or disprove 
that a is valid if and only if RESA(a) is valid. 
7. The Compactness Theorem 
Now, we will prove a theorem relating infinite sets of .W-formulas to their 
finite subsets. 
Definition. A set !1 of .W-formulas is called finitely satisfiable if for every 
finite set a ~ n, the set a is truth-functionally satisfiable. 
We have 
Theorem 7.1. Let !1 be finitely satisfiable and let a be an .W-formula. 
Then either !1 u {a} or !1 u {-, a} is finitely satisfiable. 
Proof. Suppose to the contrary that !1 is finitely satisfiable but that 
neither !1 u {a} nor !1 u {-,a} is finitely satisfiable. Then there are finite 
sets a,' .:12 ~ n such that a, u {a} and .:12 u {-,a} are both truth-
functionally unsatisfiable. But .:11 u .:12 is a finite subset of !1 and hence 
there must be an assignment v such that for each {3 E .:11 u .:1 2, we have 
{3v = 1. Now, either av = 1 or av = 0. In the first case .:1 1 u {a} is 
satisfiable, and in the second case .:1 2 u {-, a} is satisfiable. This is a 
contradiction. 
â¢ 
Now we will need to use a general property of infinite languages. 
Enumeration Principle. Let L be an infinite subset of A*, where 
A is an alphabet (and therefore is finite). Then there is an infinite 
sequence or enumeration w 0 , w 1 , w 2 , â¢ â¢ â¢ which consists of all the 
words in L each listed exactly once. 
The truth of this enumeration principle can be seen in many ways. One is 
simply to imagine the elements of L written in order of increasing length, 
and to order words of the same length among themselves like the entries 
in a dictionary. Alternatively, one can regard the strings on A as notations 

7. The Compactness Theorem 
371 
for numbers in some base (as in Chapter 5) and arrange the elements of L 
in numerical order. (Actually, as it is not difficult to see, these two 
methods yield the same enumeration.) Of course, no claim is made that 
there is an algorithm for computing W; from i. Such an algorithm can only 
exist if the language L is r.e. 
Now, let a 0 , a 1 , a 2 , â¢â¢â¢ be an enumeration of the set of all .sat-formulas. 
(By the enumeration principle, such an enumeration must exist.) Let .n be 
a given finitely satisfiable set of .sat-formulas. We define the sequence 
no= n 
( nn u {an} 
.On+l = 
.On U {--,an} 
if this set is finitely satisfiable 
otherwise. 
By Theorem 7.1, we have 
Lemma 1. Each .On is finitely satisfiable. 
Let 0 = U~~o .On. Then, we have 
Lemma 2. n is finitely satisfiable. 
Proof. Let us be given a finite set A ~ n. For each 'Y E A, 'Y E nn for 
some n. Hence A ~ .Om, where m is the maximum of those n. By Lemma 
1, A is truth-functionally satisfiable. 
â¢ 
Lemma 3. For each .sat-formula a either a En or --,a E n, but not 
both. 
Proof. Let a= an. Then a E nn+l or--, a E nn+l' so that a or--, a 
belongs tO 0. If a,--, a E 0, then by Lemma 2, the finite set {a,--, a} 
would have to be truth-functionally satisfiable. But this is impossible. 
â¢ 
Now we define an assignment v by letting v( A) = 1 if A E 0 and 
v(A) = 0 if Aft 0 for every atom A. We have 
Lemma 4. For each .sat-formula a, au = 1 if and only if a E 0. 
Proof. As we already know, it suffices to restrict ourselves to formulas 
using the connectives --,, V, A. And, in fact, the De Morgan relation 
(/31 V f3z) = --, ( --, /31 A --, f3z) 
shows that we can restrict ourselves even further, to the connectives --,, A. 
So, we assume that a is an .sat-formula expressed in terms of the connec-
tives --,, A. Our proof will be by induction on the total number of 
occurrences of these connectives in a. 

372 
Chapter 12 Propositional Calculus 
If this total number is 0, then a is an atom, and the result follows 
from our definition of v. Otherwise we must have either a= -, {3 or a= 
( {3 A y ), where by the induction hypothesis we can assume the desired 
result for {3 and y. 
Case 1. 
a = -, {3 
Then, using Lemma 3, 
Case 2. 
a = ( {3 A y) 
a'' = 1 
if and only if 
(3'' =/= 1 
if and Only if 
{3 I' $_ n 
if and only if 
a E fl. 
If a'' = 1, then {3'' = y'' = 1, so by the induction hypothesis, {3, y E 
0. If a ft. 0, then by Lemma 3, -, a E 0. But the finite set { {3, y, -, a} 
is not satisfiable, contradicting Lemma 2. Thus, a E fl. 
Conversely, if a En, then neither -, {3 nor -, 'Y can belong to n, 
because the finite sets {a, -, {3}, {a, -, y} are not satisfiable. Thus, by 
Lemma 3, {3, 'Y E n. By the induction hypothesis {3 I' = y'' = 1. Therefore, 
a''= 1. 
â¢ 
Now, since n ~ n, we see that a''= 1 for each a En. Hence, n is 
truth-functionally satisfiable. Since we began with an arbitrary finitely 
satisfiable set of ..w'-formulas n, we have proved 
Theorem 7.2 (Compactness Theorem for Propositional Calculus). Let 0. 
be a finitely satisfiable set of ..w'-formulas. Then n is truth-functionally 
satisfiable. 
Exercises 
1. Is the set of clauses 
{(p; V â¢P;+ 1)Ii = 1,2,3, ... } 
satisfiable? Why? 
2. The same for the set 
3.* Let us be given a plane map containing infinitely many countries. 
Suppose there is no way to color this map with k colors so that 

7. The Compactness Theorem 
373 
adjacent countries are colored with different colors. Prove that there is 
a finite submap for which the same is true. 
4. * Let r be a (not necessarily finite) set of .W-formulas, and let a be an 
.W-formula. We can generalize the notion of tautological consequence 
by writing r I= a to mean this: for every assignment v on .W such that 
y" = 1 for all 'Y E f, we also have a" = 1. 
(a) Show that f I= a if and only if y 1 , â¢â¢â¢ , 'Yn I= a for some y 1 , â¢â¢â¢ , 'Yn 
E r. 
(b) Show that if r is an r.e. set, then {a I r 1= a} is also r.e. 
(c) 
Give an r.e. set r such that {a I r 1= a} is not recursive. 
(d) Let r be an r.e. set of .W-formulas such that for some .W-formula 
a, both r 1= a and r 1= ..., a. Show that {a I r 1= a} is recursive. 
(e) 
Let r be an r.e. set of .W-formulas such that for every .W-formula 
a, either r 1= a or r 1= ..., a but not both. Show that {a I r 1= a} 
is recursive. 


13 
Quantification Theory 
1. The Language of Predicate Logic 
Although a considerable part of logical inference is contained in the 
propositional calculus, it is only with the introduction of the apparatus of 
quantifiers that one can encompass the full scope of logical deduction as it 
occurs in mathematics, and in science generally. We begin with an alpha-
bet called a vocabulary consisting of two kinds of symbols, relation symbols 
and function symbols. Let W be a vocabulary. For each symbol t E W, we 
assume there is an integer 8(t) called the degree oft. For t a function 
symbol, 8(t) ~ 0, while for t a relation symbol, 8(t) > 0. A function 
symbol t whose degree is 0 is also called a constant symbol. We assume 
that W contains at least one relation symbol. (What we are calling a 
vocabulary is often called a language in the literature of mathematical 
logic. Obviously this terminology is not suitable for a book on theoretical 
computer science.) In addition to W we shall use the alphabet 
Q = { ..., ' " ' v ' :::> ' +-+ 'v' 3' (' ) ' X' y' z' u' v' w' I' ,} ' 
where the boldface comma, is one of the symbols that belong to Q. The 
words that belong to the language 
{xlliJ' yllil' zllil' ullil' vllil' wllill i E N} 
375 

376 
Chapter 13 Quantification Theory 
are called variables. Again we think of strings of the form l[i1, i > 0, as 
subscripts, e.g., writing x 5 for xlllll. By a W-tenn (or when the vocabu-
lary W is understood, simply a tenn) we mean an element of (Q U W)* 
that either is a constant symbol c E W or a variable, or is obtained from 
constant symbols and variables by repeated application of the operation on 
(Q U W)* that transforms ~-t 1
, f.tz, ... , 1-tn into 
f(~-tJ '1-tz' Â· Â· Â·' 1-tn ), 
where f is a function symbol in W and S(f) = n > 0. 
An atomic W-fonnula is an element of (Q U W)* of the form 
r ( f.t 1 ' f.tz ' Â· Â· Â· â¢ 1-tn) , 
where r E W is a relation symbol, S(r) = n, and ~-t 1 , f.tz, ... , 1-tn are terms. 
Finally, a W-fonnula (or simply a fonnula) is either an atomic W-formula 
or is obtained from atomic W-formulas by repeated application of the 
following operations on (Q U W)*: 
1. transform a into -, a; 
2. transform a and f3 into (a A f3 ); 
3. transform a and f3 into (a V f3 ); 
4. transform a and f3 into (a ::> f3 ); 
5. transform a and f3 into (a -
f3 ); 
6. transform a into ( V b) a, where b is a variable; 
7. transform a into ( 3 b) a, where b is a variable. 
If b is a variable, the expressions 
(Vb) 
and 
(3b) 
are called universal quantifiers and existential quantifiers, respectively. 
Let b be a variable, let A be a formula or a term, and suppose that we 
have the decomposition A = rbs, where the leftmost symbol of s is not I. 
(This means that b is not part of a longer variable. In fact, because A is a 
formula or a term, s will have to begin either with , or with ).) Then we say 
that the variable b occurs in A. If more than one such decomposition is 
possible for a given variable b we speak, in an obvious sense, of the first 
occu"ence of b in A, the second occu"ence of b in A, etc., reading from left 
to right. 
Next suppose that a is a formula and that we have the decomposition 
a= r(Vb)f3s 
or 
a= r(3b)f3s, 
where f3 is itself a fonnula. Then the occu"ence of b in the quantifiers 
shown, as well as all occu"ences of bin {3, are called bound occu"ences of b 
in a. Any occurrence of b in a that is not bound is called a free 

2. Semantics 
377 
occu"ence of b in a. A W-formula a containing no free occurrences of 
variables is called a W-sentence, or simply a sentence. Any occurrence of a 
variable in a term is considered to be a free occu"ence. 
Thus, in the formula 
(r(x) :J (3y)s(u,y)), 
x and u each have one occurrence, and it is free; y has two occurrences, 
and they are both bound. The formula 
(Vx)(3u)(r(x) :J (3y)s(u,y)) 
is a sentence. 
Exercises 
1. Let W = {0, s, <}, where 0, s are function symbols with 8(0) = 0, 
8(s) = 1, and < is a relation symbol with 8( <) = 2. Describe the set 
of W-terms and the set of atomic W-formulas. 
2. {a) Define the height of a W-term t, denoted Ht (t ), as follows: 
Ht( x) = 1 for all variables x 
Ht(c) = 1 for all constant symbols c 
Ht{f(tpÂ·Â·Â·,tn)) = max{Ht(t) 11 -5:, i -5:, n} + 1. 
Show by induction on height that all W-terms have an equal 
number of left and right parentheses. 
{b) Do the same for W-formulas. 
2. Semantics 
In analogy with the propositional calculus, we wish to associate the truth 
values, 1 and 0, with sentences. To do this for a given sentence a will 
require an "interpretation" of the function and relation symbols in a. 
By an interpretation I of a vocabulary W, we mean a nonempty set D, 
called the domain of I, together with the following: 
1. an element c1 of D, for each constant symbol c E W; 
2. a function f 1 from D 8<f> into 1 D, for each function symbol fEW for 
which 8(/) > 0; and 
3. a function r1 from D 8<r> into {0, 1}, for each relation symbol r E W. 
1 Recall from Chapter 1, Section 1, that D" is the set of n-tuples of elements of D. 

378 
Chapter 13 Quantification Theory 
Let A be a term or a formula and let b 1 , b 2 , â¢ â¢â¢ , bn be a list of distinct 
variables which includes all the variables that have free occu"ences in A. 
Then, we write A = A(b 1 , â¢â¢â¢ , bn) as a declaration of our intention to 
regard b 1 , â¢â¢â¢ , bn as acting like parameters taking on values. In such a 
case, if t 1 , â¢â¢â¢ , tn are terms containing no occurrences of variables that 
have bound occurrences in A, we write A(t1 , â¢â¢â¢ , tn) for the term or 
formula obtained from A by simultaneously replacing b 1 by t 1 , b 2 by 
12 , â¢â¢â¢ , bn by In. 
Now let t be a W-term, t = t(b 1 , b2 , â¢â¢â¢ , bn), and let I be an interpreta-
tion of W, with domain D. Then we shall define a value t 1[d1 , d 2 , â¢â¢â¢ , dn] 
ED for all d 1 , d 2 , â¢â¢â¢ , dn ED. For the case n = 0, we write simply t 1â¢ We 
define this notion recursively as follows: 
1. If t = t(b 1 , b2 , â¢â¢â¢ , bn) and t is a variable, then t must be b; for some 
i, 1 :;;; i:;;; n, and we define t 1[d 1 , d 2 , â¢â¢â¢ , dn] = d;; 
2. If t = t(b 1 , b2 , â¢â¢â¢ , bn) and t is a constant symbol c in W, then we 
define t 1[d 1,d2 , â¢â¢â¢ ,dn] = c1 ; 
3. If t = t(b 1 , b 2 , â¢â¢â¢ , bn) = g(t 1 , t2 , â¢â¢â¢ , tm), where g is a function sym-
bol in W, 8(g) = m > 0, then we first set I; = t;(b 1 , b2 , â¢â¢â¢ , bn), 
i = 1, 2, ... , m, and we let S; = t/[d1 , d 2 , â¢â¢â¢ , dn], i = 1, 2, ... , m. Fi-
nally, we define 
Continuing, if a is a W-formula, a= a(b 1 , b2 , â¢â¢â¢ , bn), and I is an 
interpretation of W with domain 
D, we shall define a va1J.1e 
a 1[d1 , d 2 , â¢â¢â¢ , dn] E {0, 1}, for all d 1 , d 2 , â¢â¢â¢ , dn ED. Again, in the partic-
ular case n = 0 (which can happen only if a is a sentence), we simply 
write a 1â¢ The recursive definition is as follows: 
1. If a = a(b 1 , b2 , â¢â¢â¢ , bn) = r(t1 , t2 , â¢â¢â¢ , tm), where r is a relation sym-
bol in W, 8(r) = m, then we first set I; = t;(b 1 , b2 , â¢â¢â¢ , bn), i = 
1, 2, ... , m, and then let s; = t/[d1 , d 2 , â¢â¢â¢ , dn], i = 1, 2, ... , m. Fi-
nally, we define a 1[dpd2 , â¢â¢â¢ ,dn] = r1(sps 2 , â¢â¢â¢ ,sm). 
In 2-6 which follow, let {3 = {3(b 1 , â¢â¢â¢ , bn), y = y(b 1 , â¢â¢â¢ , bn), where we 
assume that {3 1[d1 , â¢â¢â¢ , dn] = k, y 1[d1 , â¢â¢â¢ , dn] =I with k, IE {0, 1}, are 
already defined for all d 1 , d 2 , â¢â¢â¢ , dn ED: 
2. If a is --, {3, a = a(b 1 , â¢â¢â¢ , bn), then we define 
if k = 0 
if k = 1. 

2. Semantics 
3. If a is ( {3 A y ), a = a(bl> ... , bn), then we define 
if k =I= 1 
otherwise. 
4. If a is ( {3 V y ), a = a(bp ... , bn), then we define 
if k =I= 0 
otherwise. 
5. If a is ( {3 :::> y ), a = a(b 1 , â¢â¢â¢ , bn), then we define 
379 
if 
k = 1 and I = 0 
otherwise. 
6. If a is ( {3 ~ y ), a = a(b 1 , â¢â¢â¢ , bn), then we define 
{ 1 
if k =I 
a'[d1 , â¢â¢â¢ ,dn] = 
0 
otherwise. 
In 7 and 8 let 
{3 = {3(b 1 , â¢â¢â¢ , bn, b), where we assume that 
{3 1[d1 , â¢â¢â¢ , dn, e] is already defined for all d1 , â¢â¢â¢ , dn, e ED: 
7. If a is (V b) {3, a = a(bl> ... , bn), then we define 
if 
{3 1[d1 , â¢â¢â¢ ,dn,e] = 1 for all e ED 
otherwise. 
8. If a is ( 3b) {3, a = a(b 1 , â¢â¢â¢ , bn), then we define 
if 
{3 1[d1 , â¢â¢â¢ ,dn,e] = 1 for some e ED 
otherwise. 
It is important to be aware of the entirely nonconstructive nature of 7 
and 8 of this definition. When the set D is infinite, the definition provides 
no algorithm for carrying out the required searches. and, indeed, in many 
important cases no such algorithm exists. 
Let us consider some simple examples. 
EXAMPLE 1. W = {c, r, s}, where c is a constant symbol, and rand s are 
relation symbols, 8(r) = 3, 8(s) = 2. Let I have the domain D = 
{0, 1, 2, 3, ... , }, let c1 = 0, and let 
if x+y=z 
otherwise, 
s/x,y) = {~ 
if 
X ::; y 
otherwise. 

380 
Chapter 13 Quantification Theory 
If a is the sentence 
(Vx)(Vy)(Vz)(r(x,y,z) :::>s(x,z)), 
then it is easy to see that a 1 = 1. For if u, v, w E D and r1(u, v, w) = 1, 
then u + v = w, so that u:::;; w and therefore s1(u, w) = 1. So if y = 
y(x, y, z) is the formula (r(x, y, z) :::> s(x, z)), then y 1[u, v, w] = 1. 
On the other hand, if {3 is the sentence 
(Vx)(3y)r(x, y, c), 
then {3 1 = 0. This is because r1(1, v, 0) = 0 for all v ED. Therefore 
r(x,y,c)1[1,v] = 0 
for all v ED. Thus, (3y)r(x,y,c)1[1] = 0, and therefore, finally, {3 1 = 0. 
EXAMPLE 2. 
W, a, {3 are as in Example 1. I has the domain 
{ ... , -3, -2, -1,0, 1,2,3, ... ,}, 
the set of all integers. c 1 , r1 , s 1 are defined as in Example 1. In this case, it 
is easy to see that a 1 = 0 and {3 1 = 1. 
An interpretation I of the vocabulary W is called a model of a W-
sentence a if a 1 = 1; I is called a model of the set .n of W-sentences if I 
is a model of each a E n. n is said to be satisfiable if it has at least one 
model. An individual W-sentence a is called satisfiable if {a} is satisfiable, 
i.e., if a has a model. a is called valid if every interpretation of W is a 
model of a. 
If a = a(b 1 , â¢â¢â¢ , bn), {3 = {3(b 1 , â¢â¢â¢ , bn) are W-formulas, we write a = {3 
to mean that a and {3 are semantically equivalent, that is, 
for all interpretations I of W and all d1 , â¢â¢â¢ , dn ED, the domain of I. 
Then, as is readily verified, all of the equations from Section 1 of Chapter 
12 hold true as well in the present context. We also note the quantifica-
tional De Morgan laws: 
..., (Vb)a = (3b)..., a;..., (3b)a = (Vb)..., a. 
(2.1) 
Again, as in the case of the propositional calculus, we may eliminate the 
connectives :::> and ++ by using appropriate equations from Chapter 12, 
Section 1. Once again, there is a "general principle of duality," but we 
omit the details. 

2. Semantics 
381 
Now, let {3 = {3(b 1 , â¢â¢â¢ , bn, b), and let the variable a have no occur-
rences in {3. Then it is quite obvious that 
(3b){3(b 1 , â¢â¢â¢ , bn, b) = (3a){3(b 1 , â¢â¢â¢ , bn, a), 
(Vb){3(b 1 , â¢â¢â¢ ,bn,b) = (Va){3(b 1 , â¢â¢â¢ ,bn,a). 
Continuing to assume that a has no occurrences in {3, we have 
Exercises 
((Va)a A {3) = (Va)(a A {3), 
((3a )a A {3) = (3a )(a A {3), 
((Va)a V {3) = (Va)(a V {3), 
((3a)a V {3) = (3a)(a V {3). 
(2.2) 
(2.3) 
1. Let W be as in Example 1. For each of the following W-sentences give 
an interpretation that is a model of the sentence as well as one that is 
not. 
(a) (Vx)(3y)(Vz)(s(x,c) :::>r(x,y,z)). 
(b) (3y)(Vx)(Vz)(s(x,c) :::>r(x,y,z)). 
(c) (Vx)(Vy)(s(x, y) :::> s(y, x)). 
2. 
Give an interpretation that is a model of (a) in Exercise 1 but not of 
(b). 
3. Let W = {ca, cb, cat, eq}, let interpretation I have domain {a, b}*, and 
let ca1 =a, cb1 = b, cat1(u,v) = 17V, and 
if u = v 
otherwise. 
For each of the following formulas a, calculate a 1â¢ 
(a) (Vx)(3y)eq(cat(ca,x),y). 
(b) ( 3y )(Vx )eq(cat(ca, x ), y ). 
(c) (Vx)(3y)eq(cat(x,y),x). 
(d) (Vx)( 3y )(eq(cat(ca, y ), x) v eq(cat(cb, y ), x)). 
(e) (3x)eq(cat(ca, x), cat(x, cb)). 
4. For each of the following formulas, tell whether it is (i) satisfiable, (ii) 
valid, (iii) unsatisfiable. 
(a) ((3x)p(x) A (Vy)--,p(y)). 
(b) (Vx)(3y)r(f(a),b). 

382 
Chapter 13 Quantification Theory 
(c) 
((Vx)(3y)r(x,y) :J (3y)(Vx)r(x,y)). 
(d) 
((3y)(Vx)r(x,y) :J (Vx)(3y)r(x,y)). 
(e) 
(3x)(Vy) < (x,y). 
(f) 
((3x)p(x) :J (3x)(Vy)p(x)). 
5. 
Let W = {0, s, +, eq}, let interpretation I have domain N, and let 
01 = 0, s 1 be the successor function, +1 be the addition function, and 
eq1 be equality (as in Exercise 3). For each of the following sets S, give 
a formula a such that 
(a) 
S = N. 
(b) S = {(x, y, z) E N 3 I x + y = z}. 
(c) 
S = {(x, y) E N 2 I x ~ y}. 
(d) S = {(x, y, z) E N 3 I z ..:. y = x}. 
(e) 
S = {x EN I x is even}. 
6. For a set of sentences .n, let Mod(!l) be the collection of all models of 
.n. Prove that 
3. 
Logical Consequence 
We are now ready to use the semantics just developed to define the notion 
of logical consequence. Let W be a vocabulary, let r be a set ofW-sentences, 
and let y be a W-sentence. Then we write 
fl=y 
and call y a logical consequence of the premises r if every model of r is 
also a model of y. If r = { y1 , â¢â¢â¢ , Yn}, then we omit the braces { , }, and 
write simply 
'Y1 'Yz' Â· Â· Â·' Yn I= Y Â· 
Note that y 1 , y 2 , â¢â¢â¢ , Yn I= y if and only if for every interpretation I of W 
for which 
y{ = yÂ£ = Â·Â·Â· = Yn1 = 1, 
we also have y 1 = 1. (Intuitively, we may think of the various interpreta-
tions as "possible worlds." Then our definition amounts to saying that y is 
a logical consequence of some premises if y is true in every possible world 

3. Logical Consequence 
383 
in which the premises are all true.) As in the case of the propositional 
calculus, logical consequence can be determined by considering a single 
sentence. The proof of the corresponding theorem is virtually identical to 
that of Theorem 2.1 in Chapter 12 and is omitted. 
Theorem 3.1. The relation y 1 , y 2 , â¢â¢â¢ , 'Yn I= y is equivalent to each of the 
following: 
1. the sentence (( y 1 1\ â¢Â· Â· 1\ y) :::) y) is valid; 
2. the sentence ( y 1 1\ Â· Â· Â· 1\ 'Yn 1\ ..., y) is unsatisfiable. 
Once again we are led to a problem of satisfiability. We will focus our 
efforts on computational methods for demonstrating the unsatisfiability of 
a given sentence. We begin by showing how to obtain a suitable normal 
form for any given sentence. 
As in Chapter 12, Section 3, we begin with the procedures 
(I) ELIMINATE :::) and -
. 
(II) MOVE ..., INWARD. 
Procedure (I) is carried out exactly as in Chapter 12. For (II), we also need 
to use the quantificational De Morgan laws (2.1). Ultimately all ...,swill 
come to immediately precede relation symbols. 
(III) RENAME VARIABLES. 
Rename bound variables as necessary to ensure that no variables occur 
in two different quantifiers, using (2.2). Thus, the sentence 
((Vx)(Vy )r(x, y) V ((Vx)s(x) 1\ (3y )s( y)) 
might become 
(Vx)(Vy)r(x,y) V ((Vu)s(u) 1\ (3v)s(v)). 
(IV) PULL QUANTIFIERS 
Using (2.3), bring all quantifiers to the left of the sentence. Where possible, 
do so with existential quantifiers preceding universal quantifiers. Thus, to 
continue our example, we would get successively 
(Vx)(Vy)r(x, y) V (3v)((Vu)s(u) 1\ s(v)) 
= (3v)((Vx)(Vy)r(x, y) V ((Vu)s(u) 1\ s(v))) 
= (3v)(Vx)(Vy)(Vu)(r(x, y) V (s(u) 1\ s(v))). 
After applying (IV) as many times as possible, we obtain a sentence 
consisting of a string of quantifiers followed by a formula containing no 

384 
Chapter 13 Quantification Theory 
quantifiers. Such a sentence is called a prenex sentence. A prenex sentence 
is also said to be in prenex normal form. 
Let y be a sentence of the form 
where n ~ 0, and a = a(b1 , b2 , â¢ â¢â¢ , bn , b). Let g be a function symbol 
which is not in a with 8(g) = n. If necessary, we enlarge the vocabulary W 
to include this new symbol g. Then we write 
Ys is called the Skolemization of y. [In the case n = 0, g is a constant 
symbol and the term 
g(bl 'bz ' ... ' bn) 
is to be simply understood as standing for g.] Skolemization is important 
because of the following theorem. 
Theorem 3.2. Let y be a W-sentence and let 'Ys be its Skolemization. 
Then 
1. every model of 'Ys is a model of y; 
2. if y has a model, then so does Ys ; 
3. y is satisfiable if and only if 'Ys is satisfiable. 
Proof. 
Condition 3 obviously follows from 1 and 2. 
To prove 1, let a, y be as previously and let 
Let I be a model of 'Ys so that y/ = 1, and let the domain of I be D. 
Then, if d1 , â¢â¢â¢ , dn are arbitrary elements of D, we have f3 1[d 1 , â¢â¢â¢ , dn] = 
1. Let e = g1(d1 , ... , dn). Thus a 1[d1 , ... , dn, e] = 1, so that 
Hence finally, y 1 = 1. 
To prove 2, let y 1 = 1, where I has the domain D. Again let d1 , â¢â¢â¢ , dn 
be any elements of D. Then, writing f3 for the formula (3b)a, so that we 
may write f3 = f3(bp ... , bn), we have f3 1[dp ... , dn] = 1. Thus, there is 
an element e E D such that a 1[d1 , â¢â¢â¢ , dn, e] = 1. Hence, we have shown 
that for each d1 , â¢â¢â¢ , dn ED, there is at least one element e ED such 
that a 1[d1 , ... , dn, e] = 1. Thus, we may extend the interpretation I to 

3. Logical Consequence 
385 
the new function symbol g by defining g1(d 1 , â¢â¢â¢ , dn) to be such an 
element 2 e, for each d 1 , â¢â¢â¢ , dn ED. Thus, for all d 1 , d2 , â¢â¢â¢ , dn ED, we 
have 
13 1[dl ' ... ' dn] = aJ[dl ' ... ' dn 'g/dl ' ... ' dn)] = 1. 
Hence, finally, y/ = 1. 
â¢ 
Since Theorem 3.2 shows that the leftmost existential quantifier in a 
prenex formula may be eliminated without affecting satisfiability, we can, 
by iterated Skolemization, obtain a sentence containing no existential 
quantifiers. We write this 
(V) ELIMINATE EXISTENTIAL QUANTIFIERS. 
In the example discussed under (IV), this would yield simply 
(Vx)(Vy)(Vu)(r(x,y) V (s(u) 1\ s(c))), 
(3.1) 
where c is a constant symbol. 
For another example consider the sentence 
(Vx)(3u)(Vy )(Vz)(3v )r(x, y, z, u, v), 
where r is a relation symbol, 8(r) = 5. Then two Skolemizations yield 
(Vx)(Vy )(Vz)r(x, y, z, g(x), h(x, y, z)). 
(3.2) 
A sentence a is called universal if it has the form ( V b 1 )( V b2 ) â¢ â¢ â¢ ( V bn )y, 
where the formula y contains no quantifiers. We may summarize the 
procedure (1)-(V) in 
Theorem 3.3. There is an algorithm that will transform any given sen-
tence 13 into a universal sentence a such that 13 is satisfiable if and only if 
a is satisfiable. Moreover, any model of a is also a model of 13. 
In connection with our procedure (1)-(V) consider the example 
((Vx)(3y)r(x,y) 1\ (Vu)(3v)s(u,v)), 
where r and s are relation symbols. By varying the order in which the 
quantifiers are pulled, we can obtain the prenex sentences 
1. (Vx)(3y)(Vu)(3v)(r(x,y) As(u,v)), 
2. (Vu)(3v)(Vx)(3y)(r(x,y) 1\ s(u,v)), 
2 Here we are using a nonconstructive set-theoretic principle known as the axiom of 
choice. 

386 
Chapter 13 Quantification Theory 
3. (Vx)(Vu)(3y)(3v)(r(x,y) As(u,v)). 
Skolemizations will then yield the corresponding universal sentences: 
1. (Vx)(Vu)(r(x, g(x)) A s(u, h(x, u))), 
2. (Vu)(Vx)(r(x, g(u, x)) A s(u, h(u))), 
3. (Vx)(Vu)(r(x, g(x, u)) A s(u, h(x, u))). 
But, for this example, one would expect that y should "depend" only on x 
and v only on u. In other words, we would expect to be able to use a 
universal sentence such as 
4. (Vx)(Vu)(r(x, g(x)) A s(u, h(u))). 
As we shall see, it is important to be able to justify such simplifications. 
Proceeding generally, let y be a sentence of the form 
where n ~ 0 and a= a(b 1 , b 2 , â¢â¢â¢ , bn, b). Let g be a function symbol 
which does not occur in y with 8(g) = n. Then we write 
'Ys is called a generalized Skolemization of y. Then we have the following 
generalization of Theorem 3.2. 
Theorem 3.4. Let y be a W-sentence and let 'Ys be a generalized 
Skolemization of y. Then we have 1-3 of Theorem 3.2. 
Proof. Again we need verify only 1 and 2. Let a, y, 8 be as above. To 
prove 1, let I be a model of 'Ys with domain D. Let {3 be defined as in the 
proof of Theorem 3.2. Then 8 1 = 1 and (Vb 1) â¢â¢â¢ (Vbn)f3 1 = 1. As in the 
proof of Theorem 3.2, we conclude that 
and so y 1 = 1. 
Conversely, let y 1 = 1, where I has domain D. Then 8 1 = 1 and 
Precisely as in the proof of Theorem 3.2, we can extend the interpretation 
I to the symbol g in such a way that 
Hence, y/ = 1. 
â¢ 

3. Logical Consequence 
387 
Henceforth we will consider the steps (IV) PULL QUANTIFIERS and 
(V) ELIMINATE EXISTENTIAL QUANTIFIERS to permit the use of 
generalized Skolemizations. Moreover, as we have seen, Theorem 3.3 
remains correct if the universal sentence is obtained using generalized 
Skolemizations. 
Exercises 
1. Consider the inference 
(Vx)(p(x) :J (Vy)(s(y,x) :Ju(x))), 
(3x)(p(x) A (3y )(s( y, x) A h( y, x))) 
F= (3x)(3y)(u(x) Ah(y,x) As(y,x)). 
(a) Find a universal sentence whose unsatisfiability is equivalent to 
the correctness of this inference. Can you do this so that Skolem-
ization introduces only constant symbols? 
(b) Using (a), show that the inference is correct. 
2. (a) Using generalized Skolemization find a universal sentence whose 
unsatisfiability is equivalent to the correctness of the inference 
(3x)(Vy)r(x, y) F= (Vy)(3x)r(x, y). 
(b) Show that the inference is correct. 
3. The same as Exercise 2(a) for the inference 
(Vx)(Vy)(Vz)(Vu)(V V )(Vw)((P(x, y, u) A P( y, z, v) A P(x, v, w)) 
:J P(u, z, w)), 
(Vx)(Vy)(3z)P(z, x, y), 
(Vx)(Vy)(3z)P(x, z, y) F= (3x)(Vy)P( y, x, y). 
4. Prove Theorem 3.1. 
5. For each sentence a in Exercise 2.1, perform the following. 
(a) Transform a into a prenex normal form sentence. 
(b) Give the Skolemization 'Ys of y. 
{c) 
Give a model I of a. 
(d) Extend I to a model of 'Ys â¢ 
6. Let y be a W-sentence, for some vocabulary W, and let 'Ys be its 
Skolemization. Prove or disprove each of the following statements. 
(a) If y is valid then 'Ys is valid. 
(b) If 'Ys is valid then y is valid. 

388 
Chapter 13 Quantification Theory 
7. 
Let W be a vocabulary, r a set ofW-sentences, and a, {3 W-sentences. 
Prove each of the following statements. 
(a) (Deduction Theorem) f u {a} 1= {3 if and only if f 1= ( a :::> {3 ) . 
(b) (Contraposition) r u {a} I= ..., {3 if and only if r u { {3} 1= ..., a. 
(c) (Reductio ad absurdum) r u {a} 1= ( {3 A -, {3) if and only if 
r I= ..., a. 
4. 
Herbrand's Theorem 
We have seen that the problem of logical inference is reducible to the 
problem of satisfiability, which in turn is reducible to the problem of 
satisfiability of universal sentences. In this section, we will prove Herbrand's 
theorem, which can be used together with algorithms for truth-functional 
satisfiability (discussed in Chapter 12) to develop procedures for this 
purpose. 
Let a be a universal W-sentence for some vocabulary W, where we 
assume that a contains all the symbols in W. If a contains at least one 
constant symbol, we call the set of all constant symbols in a the constant 
set of a. If a contains no constant symbols, we let a be some new constant 
symbol, which we add toW, and we call {a} the constant set of a. Then the 
language which consists of all W-terms containing no variables is called the 
Herbrand universe of a. The set J/1' of atomic W-formulas containing no 
variables is called the atom set of a. We will work with the set of 
propositional formulas over J/1', i.e., of J/1'-formulas in the sense of Chapter 
12, Section 1. Each of these J/1'-formulas is also a W-sentence that contains no 
quantifiers. 
Returning to the universal sentence (3.1), we see that its constant set is 
{c}, its Herbrand universe is likewise {c}, and its atom set is {r(c, c), s(c)}. 
Next, examining the universal sentence (3.2), its constant set is {a}, but 
its Herbrand universe is infinite: 
H = {a, g(a), h(a, a, a), g(g(a)), g(h(a, a, a)), h(a, a, g(a)), ... }. 
Its atom set is likewise infinite: 
Theorem 4.1. Let ~ = ~(b 1 , b2 , â¢â¢â¢ , bn) be a W-formula containing no 
quantifiers, so that the sentence 

4. Herbrand's Theorem 
389 
is universal. Let H be the Herbrand universe of y and let Sit' be its atom 
set. Then, y is satisfiable if and only if the set 
(4.1) 
of .Jit'-formulas is truth-functionally satisfiable. 
Proof. 
First let y be satisfiable, say, y 1 = 1, and let D be the domain of 
I. We now define an assignment v on Sit'. Let r be a relation symbol of W, 
S(r) = m, so that r(t1, â¢â¢â¢ , tm) E Sit' for all t 1 , ... , tm E H. Then we define 
v(r(tpÂ·Â·Â·â¢tm)) = r1(t[, ... ,t~). 
We have 
Lemma 1. 
For all .Jit'-formulas a, a 1 =a". 
Proof. As in Chapter 12, we may assume that a contains only the 
connectives --,, A. Proceeding by induction, we see that if a is an atom, 
the result is obvious from our definition of v. Thus, we may suppose that 
a = --, {3 or a = ( {3 A y ), where the result is known for {3 or, for {3 and 
y, respectively. 
In the first case, we have 
if and only if 
if and only if 
if and only if 
Similarly, in the second case 
if and only if 
if and only if 
if and only if 
{31 = 0 
{3" = 0 
av = 1. 
{3/=y/=1 
{3v = 'Yv = 1 
au= 1. 
â¢ 
Returning to the proof of the theorem, we wish to show that for all 
a En, au= 1. By Lemma 1, it will suffice to show that a 1 = 1 for 
a En. Now, since y 1 = 1, we have 
But clearly, for t 1 , ... , tn E H, 
We conclude that 0. is truth-functionally satisfiable. 

390 
Chapter 13 Quantification Theory 
Conversely, let us be given an assignment v on .W' such that av = 1 for 
all a E .n. We shall use v to construct an interpretation I of W. The 
domain of I is simply the Herbrand universe H. Furthermore, 
1. If c E W is a constant symbol, then c 1 = c. (That is, a constant 
symbol is interpreted as itself.) 
2. Iff E W is a function symbol, 8(/) = n > 0, and t 1, t2, ... , tn E H, 
then 
fJ(t 1 ,t2 , ... ,tn) =J(tpt2 , ... ,tn) E H. 
(Note carefully the use of boldface.) 
3. If r E W is a relation symbol, 8(r) = n, and t 1 , t 2 , ... , tn E H, then 
r1(t 1 , t 2 , ... , tn) = v(r(t 1 , t 2 , ... , tn)). 
(Note that the assignment v is only used in 3.) We have 
Lemma 2. For every t E H, t 1 = t. 
Proof. 
Immediate from 1 and 2. 
â¢ 
Lemma 3. For every W-formula a= a(b1 , â¢â¢â¢ , bn) containing no quanti-
fiers, and all t 1, ... , tn E H, we have 
a 1[tl , ... ,tn] = v(a(tl , ... ,tn}). 
Proof. If a is an atom, the result follows at once from 3 and Lemma 2. 
For the general case it now follows because the same recursive rules are 
used for the propositional connectives, whether we are evaluating interpre-
tations or assignments. 
â¢ 
Returning to the proof of the theorem, we wish to show that y 1 = 1. For 
this, recalling that H is the domain of I, it suffices to show that 
for all 
t 1 , ... , tn E H. 
By Lemma 3, this amounts to showing that 
for all 
t 1 , ... , tn E H. 
But this last is precisely what we have assumed about v. 
â¢ 
The usefulness of the theorem we have just proved results from combin-
ing it with the compactness theorem (Theorem 7.2 in Chapter 12). 
Theorem 4.2 
(Herbrand's Theorem). Let '' y, H, .W', and .n be as in 
Theorem 4.1. Then y is unsatisfiable if and only if there is a truth-

4. Herbrand's Theorem 
391 
functionally unsatisfiable W-formula of the form A13 e I {3 for some finite 
subset I of n. 
Proof. If there is a truth-functionally unsatisfiable .sat-formula A13 e I 
{3, 
where I ~ n, then for every assignment v on .sat, there is some {3 E I 
such that {3 v = 0. Hence I, and therefore also .n, is not truth-functionally 
satisfiable; hence by Theorem 4.1, y is unsatisfiable. 
Conversely, if y is unsatisfiable, then by Theorem 4.1, .n is not truth-
functionally satisfiable. Thus, by the compactness theorem (Theorem 7.2 in 
Chapter 12), n is not finitely satisfiable; i.e., there is a finite set I ~ .n 
such that I 
is not truth-functionally satisfiable. Then, the sentence 
A 13 e I 
{3 is truth-functionally unsatisfiable. 
â¢ 
This theorem leads at once to a family of procedures for demonstrating 
the unsatisfiability of a universal sentence y. Write n = U~ ~ 0 In , where 
I 0 = 0, In ~ In+ 1 , the In are all finite, and where there is an algorithm 
that transforms each In into In+ 1 â¢ (This can easily be managed, e.g., by 
simply writing the elements of .n as an infinite sequence.) Then we have 
the procedure 
n+---0 
WHILE 1\ {3 IS TRUTH-FUNCTIONALLY SATISFIABLE DO 
{3E"i.n 
n+-n+1 
END 
If y is unsatisfiable, the procedure will eventually terminate; otherwise it 
will continue forever. The test for truth-functional satisfiability of A13 e I" {3 
can be performed using the methods of Chapter 12, e.g., the Davis-Putnam 
rules. Using this discussion, we are able to conclude 
Theorem 4.3. For every vocabulary W the set of unsatisfiable sentences is 
recursively enumerable. Likewise the set of valid sentences is r.e. 
Proof. Given a sentence a, we apply our algorithms to obtain a universal 
sentence y that is satisfiable if and only if a is. We then apply the 
preceding procedure based on Herbrand's theorem. It will ultimately halt 
if and only if a is unsatisfiable. This procedure shows that the set of 
unsatisfiable sentences is r.e. 
Since a sentence a is valid if and only if --, a is unsatisfiable, the same 
procedure shows that the set of valid sentences is r.e. 
â¢ 
One might have hoped that the set of unsatisfiable W-sentences would 
in fact be recursive. But as we shall see later (Theorem 8.1), this is not the 

392 
Chapter 13 Quantification Theory 
case. Thus, as we shall see, we cannot hope for an algorithm that, 
beginning with sentences y 1 , y2 , â¢â¢â¢ , Yn, y as input, will return YES if 
y 1 , y 2 , â¢â¢â¢ , Yn I= y, and NO otherwise. The best we can hope for is a 
general procedure that will halt and return YES whenever the given 
logical inference is correct, but that may fail to terminate otherwise. And 
in fact, using Theorem 3.1 and an algorithm of the kind used in the proof 
of Theorem 4.3, we obtain just such a procedure. 
Now let us consider what is involved in testing the truth-functional 
satisfiability of 1\ 13 E I {3, where I is a finite subset of the set .n defined in 
(4.1). If we wish to use the methods developed in Chapter 12, we need to 
obtain a CNF of 1\ 13 E I {3. But, if for each {3 E I, we have a CNF formula 
{3 Â° such that {3 = {3 Â°, then 1\ 13 E 'i {3 Â° is clearly a CNF of 1\ f3 E 'i {3. This 
fact makes CNF useful in this context. 
In fact we can go further. We can apply the algorithms of Chapter 12, 
Section 3, to obtain CNF formulas directly for~= ~(b 1
, â¢â¢â¢ , bn). When we 
do this we are in effect enlarging the set of formulas to which we apply the 
methods of Chapter 12, by allowing atoms that contain variables. Each 
formula can then be thought of as representing all of the W-formulas 
obtained by replacing each variable by an element of the Herbrand 
universe H. In this context formulas containing no variables are called 
ground formulas. We also speak of ground literals, ground clauses, etc. 
If the CNF formula obtained in this manner from ~(b 1
, â¢â¢â¢ , bn) is given 
by the set of clauses 
(4.2) 
then each {3 E I will have a CNF 
where t 1 , â¢â¢â¢ , t n are suitable elements of H. Hence, there will be a CNF of 
1\ f3 E 'i {3 representable in the form 
{K;(t{, ... ,tDii = 1, ... ,r,j = 1, ... ,s}, 
(4.3) 
where t{, ... , t~ E H, j = 1, 2, ... , s. Thus, what we are seeking is an 
unsatisfiable set of clauses of the form ( 4.3). Of course, such a set can be 
unsatisfiable without being minimally unsatisfiable in the sense of Chapter 
12, Section 5. In fact, there is no reason to expect a minimally unsatisfiable 
set of clauses which contains, say, 
K 1(t1 , â¢â¢â¢ , tn) to also contain 
K2(t1 , â¢â¢â¢ , tn). Thus, we are led to treat the clauses in the set (4.2) 
independently of one another, seeking substitutions of elements of H for 
the variables b1 , â¢â¢â¢ , bn so as to obtain a truth-functionally inconsistent set 

4. Herbrand's Theorem 
393 
R of clauses. Each of the clauses in (4.2) can give rise by substitution to 
one or more of the clauses of R. 
Let us consider some examples. 
EXAMPLE 1. 
Consider this famous inference: All men are mortal; Socrates 
is a man; therefore, Socrates is mortal. An appropriate vocabulary would be 
{m, t, s}, where m, t are relation symbols of degree 1 (which we think of as 
standing for the properties of being a man, and of being mortal, respec-
tively), and s is a constant symbol (which we think of as naming Socrates). 
The inference becomes 
(Vx)(m(x) :::> t(x)), m(s) 1= t(s). 
Thus, we wish to prove the unsatisfiability of the sentence 
((Vx)(m(x) :::> t(x)) A m(s) A ..., t(s)). 
Going to prenex form, we see that no Skolemization is needed: 
(Vx)((-,m(x) Vt(x)) Am(s) A -,t(s)). 
The Herbrand universe is just {s}. In this simple case, Herbrand's theorem 
tells us that we have to prove the truth-functional unsatisfiability of 
((..., m(s) V t(s)) A m(s) A ..., t(s)); 
that is, we are led directly to a ground formula in CNF. Using the set 
representation of Chapter 12, Section 4, we are dealing with the set of 
clauses 
{ {m(s), t(s )}, {m(s )} , {t(s )} } . 
Using the Davis-Putnam rules (or, in this case equivalently, resolution), 
we obtain successively 
{{t(s)}, {t(s)}}, 
and 
{ D}; 
hence the original inference was valid. 
EXAMPLE 2. 
Another inference: Every shark eats a tadpole; all large white 
fish are sharks; some large white fish live in deep water; any tadpole eaten by a 
deep water fish is miserable; therefore, some tadpoles are miserable. 
Our vocabulary is {s, b, t, r, m, e}, where all of these are relation symbols 
of degree 1, except e, which is a relation symbol of degree 2. e(x, y) is to 
represent "x eats y." s stands for the property of being a shark, b of being a 
large white fish, t of being a tadpole, r of living in deep water, and m of 

394 
Chapter 13 Quantification Theory 
being miserable. The inference translates as 
(Vx)(s(x) :::> ( 3y)(t( y) t\ e(x,y))), 
(Vx)(b(x) :::> s(x)), 
(3x)(b(x) t\ r(x)), 
(Vx)(Vy)((r(x) At(y) t\e(x,y)) =>m(y)) F= (3y)(t(y) Am(y)). 
Thus, we need to demonstrate the unsatisfiability of the sentence 
( (Vx)(s(x) :::> (3y )(t( y) t\ e(x, y))) 
t\ (Vx)(b(x) =>s(x)) 
t\ (3x)(b(x) t\ r(x)) 
A(Vx)(Vy)((r(x) At(y) Ae(x,y)) =>m(y)) 
t\ -, (3y )(t( y) t\ m( y))). 
We proceed as follows. 
I. 
ELIMINATE :::> : 
((Vx)(-,s(x) V (3y)(t(y) Ae(x,y))) 
t\ ( V X)( -, b( X) V s( X)) 
A(3x)(b(x) t\ r(x)) 
t\ (Vx)(Vy )(-, (r(x) t\ t( y) t\ e(x, y)) V m( y)) 
t\-, (3y)(t( y) t\ m( y))). 
II. MOVE ..., INWARD: 
((Vx)(-, s(x) V (3y)(t( y) t\ e(x, y))) 
A(Vx)(-, b(x) V s(x)) 
A(3x)(b(x) t\ r(x)) 
A(Vx)(Vy)(-,r(x) V -,t(y) V --.e(x,y) V m(y)) 
A(Vy)(-,t(y) V --.m(y))). 
III. 
RENAME VARIABLES; 
((Vx )(-, s(x) V (3y 1 )(t( y1) t\ e(x, y 1 ))) 
A(Vz)(-, b(z) v s(z)) 
t\ ( 3u)( b(u) t\ r(u)) 
A(Vv)(Vw)( --.r(v) V --.t(w) V -,e(v,w) V m(w)) 
A(Vy)(-,t(y) V -,m(y))). 

4. Herbrand's Theorem 
395 
IV. 
PULL QUANTIFIERS (trying to pull existential quantifiers first): 
( 3u)(Vx)( 3y 1 )(Vz)(Vv )(Vw )(Vy) 
((..., s(x) V (t( y 1) A e(x,y 1))) 
A (..., b(z) v s(z)) 
Ab(u) A r(u) 
A(...,r(v) V ...,t(w) V ...,e(v,w) V m(w)) 
A(...,t(y) V ...,m(y))). 
V. 
ELIMINATE EXISTENTIAL QUANTIFIERS: 
(Vx)(Vz)(Vv )(Vw )(Vy) 
( (..., s(x) V (t(g(x)) 
A e(x,g(x)))) 
A (..., b(z) v s(z)) 
A b(c) A r(c) 
A(...,r(v) V ...,t(w) V ...,e(v,w) V m(w)) 
A(...,t(y) V ...,m(y))). 
Thus we are led to the clauses 
{s(x), t(g(x) )}, 
{s(x), e(x, g(x) )}, 
(b(z), s(z)}, 
{b(c)}, 
{r(c )}, 
{r(v), t(w), e(v, w), m(w)}, 
{t( y), m( y)}. 
The Herbrand universe is 
H = {c,g(c),g(g(c)), ... }. 
To find substitutions for the variables in H, we have recourse to Theorem 
5.2 (2) in Chapter 12. To search for a minimally unsatisfiable set of ground 
clauses, we should seek substitutions that will lead to every literal having a 
mate (in another clause). By inspection, we are led to the substitution 
X= C, 
z = c, 
v = c, 
w = g(c), 
y = g(c). 

396 
Chapter 13 Quantification Theory 
We thus obtain the set of ground clauses 
{s(c),t(g(c))}, 
{s(c), e(c, g(c))}, 
{b(c), s(c)}, 
{b(c)}, 
{r(c )}, 
{f(c), i(g(c)), e(c, g(c)), m(g(c))}, 
{i(g(c)), m(g(c))}. 
Although this set of clauses is linked, we must still test for satisfiability. 
Using the Davis-Putnam rules we obtain, first using the unit rule on 
{b(c )}, 
{s(c), t(g(c) )}, 
{S(c), e(c, g(c) )}, 
{s(c)}, 
{r(c )}, 
{f(c), i(g(c)), e(c, g(c)), m(g(c))}, 
{i(g(c)), m(g(c))}. 
Using the unit rule on {s( c)} and then on {r( c)} gives 
{t(g(c) )}, 
{e(c, g(c))}, 
{i(g(c)), e(c, g(c)), m(g(c))}, 
{i(g(c)), m(g(c))}. 
Using the unit rule on {t(g(c))} and then on {e(c, g(c))} gives 
{m(g(c))}, 
{m(g(c))}. 
Finally, we obtain the set of clauses consisting of the empty clause: 
D. 
In Examples 1 and 2 each clause of (4.2) gave rise to just one clause in 
the truth-functionally unsatisfiable set of clauses obtained. That is, we 

4. Herbrand's Theorem 
397 
obtain a truth-functionally unsatisfiable set of clauses of the form (4.3) 
with s = 1. Our next example will be a little more complicated. 
EXAMPLE 3. We consider the inference 
(Vx)(3y)(r(x, y) V r( y, x)), 
(Vx)(Vy)(r(x, y) :J r( y, y)) I= (3z)r(z, z). 
Thus, we wish to demonstrate the unsatisfiability of the sentence 
(Vx)(3y )(r(x, y) V r( y, x)) 
A (Vx)(Vy )(r(x, y) :J r(y, y)) A -, (3z)r(z, z). 
We proceed as follows: 
I, II, III. ELIMINATE :J; MOVE -, INWARD; RENAME VARI-
ABLES: 
(Vx)(3y )(r(x, y) V r( y, x)) 
A(Vu)(Vv)(-,r(u,v) V r(v,v)) A (Vz)-,r(z,z). 
IV. 
PULL QUANTIFIERS: 
(Vx)(3y)(Vu)(Vv)(Vz)( (r(x, y) V r( y, x)) 
A(-,r(u,v) V r(v,v)) A -,r(z,z)). 
V. ELIMINATE EXISTENTIAL QUANTIFIERS: 
(Vx)(Vu)(Vv)(Vz)( (r(x, g(x)) V r(g(x), x)) 
A(-,r(u,v) V r(v,v)) A -,r(z,z)). 
We thus obtain the set of clauses 
{r(x, g(x)), r(g(x), x)}, 
{r(u,v),r(v,v)}, 
{f(z,z)}. 
The Herbrand universe is 
H = {a,g(a),g(g(a)), ... }. 
How can we find a mate for r(x, g(x))? Not by using r(z, z)-whichever 
element t E H we substitute for x, r(x,g(x)) will become r(t,g(t)), 
which cannot be obtained from r(z, z) by replacing z by any element of H. 

398 
Chapter 13 Quantification Theory 
Thus the only potential mate for r(x, g(x)) is r(u, v ). We tentatively set 
u = x, v = g(x) so that the second clause becomes 
{r(x, g(x) ), r(g(x), g(x) )} . 
But now, r(u, v) is also the only available potential mate for r(g(x), x). 
Thus, we are led to also substitute v = x, u = g(x) in the second clause, 
obtaining 
{;:(g(x), x), r(x, x)}. 
Both r(g(x), g(x)) and r(x, x) can be matched with r(z, z) to produce 
mates. We thus arrive at the set of clauses 
{r(x, g(x)), r(g(x), x)}, 
{r(x, g(x)), r(g(x), g(x))}, 
{;:(g(x), x), r(x, x)}, 
{r(x, x)}, 
{r(g(x), g(x) )} . 
Now we can replace x by any element of H to obtain a linked set of 
ground clauses. For example, we can set x =a; but any other substitution 
for x will do. Actually, it is just as easy to work with the nonground clauses 
as listed, since the propositional calculus processing is quite independent 
of which element of H we substitute for x. In fact after four applications of 
the unit rule (or of resolution) we obtain D, which shows that the original 
inference was correct. 
Exercises 
1. Describe the Herbrand universe and the atom set of the universal 
sentence obtained in Exercise 3.1. 
2. 
Do the same for Exercise 3.2. 
3. Do the same for Exercise 3.3. 
4. 
Let W = {c, J, p}, where c is a constant symbol, f is a function symbol 
with 8(/) = 1, and p is a relation symbol with 8(p) = 1. Show that 
{( 3x) p(x ), -, p(c ), -, p(f(c) ), -, p(f(f(c)) ), ... } is satisfiable. 

5. Unification 
399 
5. 
Unification 
We continue our consideration of Example 3 of the previous section. Let 
us analyze what was involved in attempting to "mate" our literals. Suppose 
we want to mate r(x, g(x)) with r(z, z). The first step is to observe that 
both literals have the same relation symbol r, and that r is negated in one 
and only one of the two literals. Next we were led to the equations 
X= Z, 
g(x) = z. 
The first equation is easily satisfied by setting x = z. But then the sec-
ond equation becomes g( z) = z, and clearly no substitution from the 
Herbrand universe can satisfy this equation. Thus, we were led to consider 
instead the pair of literals r(x, g(x) ), r(u, v ). The equations we need to 
solve are then 
X= U, 
g(x) = v. 
Again we satisfy the first equation by letting x = u; the second equation 
becomes g(u) = v, which can be satisfied by letting v = g(u). So the 
literals become r(u, g(u)) and r(u, g(u)). 
This example illustrates the so-called unification algorithm for finding 
substitutions which will 
transform given literals 
r( A1 , â¢â¢â¢ , An), 
;:( p., 1, â¢â¢â¢ , p.,) into mates of one another. The procedure involves compar-
ing two terms p.,, A and distinguishing four cases: 
1. One of p.,, A (say, p.,) is a variable and A does not contain this 
variable. Then replace p., by A throughout. 
2. One of p.,, A (say, p.,) is a variable, A =F p.,, but A contains p.,. Then 
report: NOT UNIFIABLE. 
3. p.,, A both begin with function symbols, but not with the same function 
symbol. Again report: NOT UNIFIABLE. 
4. p.,, A begin with the same function symbol, say 
Then use this same procedure recursively on the pairs 
VI = 
1J1 ' 
Vz = 7Jz' 
"Â·' 
In applying the unification algorithm to 
we begin with the pairs of terms 
... ' 

400 
Chapter 13 Quantification Theory 
and apply the preceding procedure to each. Naturally, substitutions called 
for by step 1 must be made in all of the terms before proceeding. 
To see that the process always terminates, it is necessary to note only 
that whenever step 1 is applied, the total number of variables present 
decreases. 
EXAMPLE 
Let us attempt to unify 
r(g(x), y, g(g(z))) 
with 
r(u, g(u), g(v) ). 
We are led to the equations 
g(x) = u, 
y = g(u), 
g(g(z)) =g(v). 
The first equation leads to letting 
u = g(x), 
and the remaining equations then become 
y = g(g(x)) 
and g(g(z)) =g(v). 
The second is satisfied by letting 
y = g(g(x)), 
which does not affect the third equation. The third equation leads recur-
sively to 
g(z) = v, 
which is satisfied by simply setting v equal to the left side of this equation. 
The final result is 
r(g(x), g(g(x)), g(g(z))), 
r(g(x), g(g(x)), g(g(z))). 
Numerous systematic procedures for showing sentences to be unsatisfi-
able based on the unification algorithm have been studied. These proce-
dures work directly with clauses containing variables and do not require 
that substitutions from the Herbrand universe actually be carried out. In 
particular, there are linked conjunct procedures that are based on searches 
for a linked set of clauses, followed by a test for truth-functional unsatisfi-
ability. However, most computer implemented procedures have been based 
on resolution. In these procedures, when a pair of literals have been mated 
by an appropriate substitution, they are immediately eliminated by resolu-
tion. We illustrate the use of resolution on Examples 2 and 3 of the 
previous section. 

5. Unification 
401 
Beginning with the clauses of Example 2, applying the unification 
algorithm to the pair of literals s(z ), s(x ), and then using resolution, we 
get 
Next, unifying 
{b(x), t(g(x) )}, 
{b(x), e(x, g(x))}, 
{b(c)}, 
{r(c)}, 
{;:(v), i(w), e(v, w), m(w)}, 
{t(y),m(y)}. 
e(x, g(x)) 
and 
e(v, w) 
and using resolution, we get 
{b(x), t(g(x) )}, 
{b(c)}, 
{r(c )}, 
{b(x), r(x), i(g(x) ), m(g(x) )}, 
{i( y), m( y)}. 
Another stage of unification and resolution yields 
and then 
Finally, we get 
{t(g(c) )} , 
{r(c )}, 
{r(c), i(g(c)), m(g(c))}, 
{i( y), m( y)}, 
{r(c)}, 
{r(c), m(g(c) )}, 
{m(g(c))}. 
{r(c )}, 
{r(c)}, 
and, then, to complete the proof, 
D. 

402 
Chapter 13 Quantification Theory 
The combination of unification with resolution can be thought of as a 
single step constituting a kind of generalized resolution. Thus, resolution 
in the sense of Chapter 12, that is, resolution involving only ground 
clauses, will now be called ground resolution, while the unmodified word 
resolution will be used to represent this more general operation. In the 
ground case we used the notation resiK 1 , K2 ) for the resolvent of K 1 , K2 
with respect to the literal A, namely, 
(K 1 -
{A}) U (K2 -
{...,A}). 
In the general case, let A E Kz, ..., f.L E Kz, where the unification algorithm 
can be successfully applied to A and ..., f.LÂ· Thus, there are substitutions for 
the variables which yield new clauses i( 1 , i<2 such that if the substitutions 
transform A into A, they also transform ..., f.L into ..., i Then we write 
Let a be a finite set of clauses. Then a sequence of clauses K 1 , Kz, ... , Kn 
is called a resolution deri~Â·ation of Kn = K from a if for each i, 1 :::;; i :::;; n, 
either K; E 'a or there are j, k < i and literals A, f.L such that K; = 
res"Â·~-'(Kj, Kk). As in Chapter 12, a resolution derivation of o from a is 
called a resolution refutation of a. The key theorem is 
Theorem 5.1 
(J. A. Robinson's General Resolution Theorem). Let { = 
{(b 1 , â¢â¢â¢ , bn) be a W-formula containing no quantifiers, and let { be in 
CNF. Let 
Then, the sentence y is unsatisfiable if and only if there is a resolution 
refutation of the clauses of {. 
We shall not prove this theorem here, but will content ourselves with 
showing how it applies to Example 3 of the previous section. The clauses 
were 
1. {r(x, g(x)), r(g(x), x)} 
2. {f(u, v ), r(v, v )} 
3. {f(z, z)}. 
A resolution refutation is obtained as follows: 
4. {r(g(x),x),r(g(x),g(x))} (resolving 1 and 2); 
5. {r(x, x), r(g(x), g(x))} (resolving 2 and 4); 
6. {r( g( x ), g( x) )} 
(resolving 3 and 5); 
7. o 
(resolving 3 and 6). 

5. Unification 
Exercises 
1. Indicate which of the following pairs of terms are unifiable. 
(a) x, g(y ). 
(b) x,g(x). 
(c) f(x),g(y). 
(d) f(x,h(a)),f(g(y),h(y)). 
(e) f(x,x),f(g(y),a). 
(f) f(x, y, z),f(g(w, w), g(x, x), g(y, y)). 
403 
2. Prove the correctness of the inferences of Exercises 3.1-3.3 by obtain-
ing minimally unsatisfiable sets of clauses. 
3. Prove the correctness of the inferences of Exercises 3.1-3.3 by obtain-
ing resolution refutations. 
4. (a) Prove that the problem of the validity of the sentence 
(3x)(3y)(Vz)((r(x, y) :J (r(y, z) A r(z, z))) 
A((r(x,y) As(x,y)) :J (s(x,z) As(z,z)))) 
leads to the list of clauses 
{r(x, y)}, 
{s(x, y), r(y, h(x, y)), r(h(x, y), h(x, y))}, 
{r(y, h(x, y) ), r(h(x, y ), h(x, y))' 
s(x, h(x, y)), s(h(x, y), h(x, y))}. 
[Hint: Use Theorem 5.1 in Chapter 12.] 
(b) Prove the validity of the sentence in (a) by giving a resolution 
refutation. 
5. * A 
conventional 
notation 
for 
describing 
a 
substitution 
is 
{x 1/t 1, â¢â¢â¢ , xn!tn}, where x 1 , â¢â¢â¢ , xn are distinct variables and t 1 , â¢â¢â¢ , tn 
are terms. If A is a term or a formula and (} is a substitution, then A8 
denotes the result of simultaneously replacing each occurrence of X; 
in A by t;, 1 ::; i ::; n. A unifier of two terms or formulas A, JL is a 
substitution (} such that A(J and JL8 are identical. Modify the unifica-
tion algorithm so that if A, JL are unifiable, it returns a unifier of A, JL. 
Apply the modified algorithm to Exercise 1. 
6.* An V -clause with at most one literal that is not negated is called a 
Hom clause. Horn clauses are the basis of logic programming languages 
such as Pro log. Horn clauses of the form A or ( -, A1 V Â· Â· Â· V -, An V 
A), where the latter is sometimes written ( A1 A Â· Â· Â· A An :J A), are 

404 
Chapter 13 Quantification Theory 
called program clauses, and a Hom program is a set (or conjunction) of 
program clauses. The input to a Horn program 9' is a clause of the 
form ( -, A1 V Â· Â· Â· V -, "-n ), called a goal clause, and the output is a 
substitution (}, called an answer substitution, such that 
(Vx 1) â¢â¢â¢ (Vx1)9' I= (Vy 1) â¢â¢â¢ (Vyk)[(A1 AÂ·Â·Â· A "-n)O], 
where x 1 , â¢â¢â¢ , x1 are all of the variables which occur free in 9' and 
y 1 , â¢â¢â¢ , Yk are all of the variables which occur free in (A1 AÂ·Â·Â· A A)O. 
(If there is no such answer substitution then the program can either 
stop and return NO or it can run forever.) If ( A1 A Â· Â· Â· A A)O has no 
free variable occurrences, then (} is a ground answer substitution. 
(a) Let (} be a substitution such that ( A1 V Â· Â· Â· V "-n )O has no free 
variable occurrences. Show that (} is a ground answer substitution 
if and only if 
(Vx1) â¢â¢â¢ (Vx1)[9' U {(-, A1 V Â·Â·Â· V -, "-n)O}] 
is unsatisfiable. 
(b) Let 9' be the Horn program with clauses 
{edge(a, b), edge(b, c), edge(x, y) :::) connected(x, y ), 
edge(x, y) A connected( y, z) :::) connected(x, z)}. 
For each of the following goal clauses, use resolution and the 
modified unification algorithm from Exercise 5 to find all possible 
answer substitutions. 
(i) 
-,edge( a, y). 
(ii) 
-,edge(x, a). 
(iii) 
-,edge(x,y). 
(iv) 
-, connected(b, y ). 
(v) 
-, connected(a, y ). 
6. 
Compactness and Countability 
In this section we give two applications of the circle of ideas surrounding 
Herbrand's theorem that are extremely important in mathematical logic. It 
will be interesting to see if they have a role to play in the application of 
logic to computer science. 
Theorem 6.1 
(Compactness Theorem for Predicate Logic). Let 0. be a 
set of W-sentences each finite subset of which is satisfiable. Then n is 
satisfiable. 

6. Compactness and Countabillty 
405 
Proof. If n is finite, there is nothing to prove. If n is infinite, we can use 
the enumeration principle from Chapter 12, Section 7, to obtain an 
enumeration {30 , {3 1 , {32 , â¢ â¢ â¢ of the elements of !1. Let us write 
'Yn = A {3;' 
n = 0, 1,2, .... 
isn 
Let steps {1)-(V) of Section 3 be applied to each of {30 , {3 1 , {32 ,. â¢ â¢ to 
obtain universal sentences 
a; = (Vb~;>) Â·Â·Â· (Vb~~)~;(b~i>, ... , b~~ ). 
Then by Theorem 3.3, for each i, a; is satisfiable if and only if {3; is 
satisfiable, and moreover any model of a; is also a model of {3;. Now let us 
apply the same steps {1)-(V) to the sentence 'Yn. We see that if we use 
generalized Skolemization we can do this in such a way that the universal 
sentence l)n we obtain, corresponding to 'Yn in the sense of Theorem 3.3, 
consists of universal quantifiers followed by the formula 
i:5.n 
Now, by hypothesis, each 'Yn is satisfiable. Hence, by Theorem 3.3, so is 
each l>n. For each n, let Hn be the Herbrand universe of l>n. Thus, 
Ho ~ Ht ~Hz .... 
Let H = UnEN Hn. By Theorem 4.1, the sets 
"" 
-
{ A Y (1U> 
l(i))ll(i) 
1U> E H 
. - 0 1 
} 
.:.,n -
i~~ ~i 
t , â¢â¢â¢ , m, 
1 , â¢â¢â¢ , m, 
n , l -
, 
, ... , n 
are truth-functionally satisfiable. We wish to show that the set 
r = ui{tl '0 0 0' 1m) I It' lz' 0 
0 
0 E u} 
is itself truth-functionally satisfiable. By the compactness theorem for 
propositional calculus (Theorem 7.2 in Chapter 12) it suffices to prove this 
for every finite subset A of r. But for any finite subset A of r, there is a 
largest value of the subscript i which occurs, and all the lj which occur are 
in some Hk. Let I be the larger of this subscript k and this largest value of 
i. Then A is itself a subset of 
AI= {~;(1 1 , â¢â¢â¢ ,1m) I 11 ,12 , â¢â¢â¢ E H1,0:::;; i:::;; 1}. 
Moreover, since I 1 is truth-functionally satisfiable, so is A1, and therefore 
A. This shows that f is truth-functionally satisfiable. 

406 
Chapter 13 Quantification Theory 
Now, let .W' be the set of all atoms which occur in the formulas that 
belong to f. Let v be an assignment on .W' such that {3'' = 1 for all {3 E f. 
Then we use v to construct an interpretation I of W with domain H 
precisely as in the proof of Theorem 4.1. Then Lemmas 2 and 3 of that 
proof hold and precisely as in that case we have 
t/[t1 , â¢â¢â¢ , tm) = 1 
for all 
t1 , â¢â¢â¢ , tm; E Hand i EN. 
Hence, a/ = 1 for all i E N. Since any model of a; is also a model of {3;, 
we have {3/ = 1 for all i E N. Thus, I is a model of n. 
â¢ 
Now let us begin with a set 0. of W-sentences which has a model I. 
Then of course I is a model of every finite subset of n. Thus, the method 
of proof of the previous theorem can be applied to 0.. Of course, this 
would be pointless if our aim were merely to obtain a model of 0.; we 
already have a model I of 0.. But the method of proof of Theorem 6.1 
gives us a model of 0. whose domain H is a language on an alphabet. Thus, 
we have proved 
Theorem 6.2 
(Skolem-Lowenheim Theorem). Let 0. be a satisfiable set 
ofW-sentences. Then n has a model whose domain is a language on some 
alphabet. 
What makes this important and interesting is that any language satisfies 
the enumeration principle of Chapter 12, Section 7. Infinite sets that 
possess an enumeration are called countably infinite. This brings us to the 
usual form of the Skolem-LOwenheim theorem. 
Corollary 6.3. 
Let 0. be a satisfiable set of W-sentences. Then 0. has a 
model whose domain is countably infinite. 
Many infinite sets that occur in mathematics are not countable. In fact, 
the diagonal method, which was used in obtaining unsolvability results in 
Part 1 of this book, was originally developed by Cantor to prove that the 
set of real numbers is not countable. What the Skolem-LOwenheim 
theorem shows is that no set of sentences can characterize an infinite 
uncountable set in the sense of excluding countable models. 
We close this section with another useful form of the compactness 
theorem. 
Theorem 6.4. If f I= y, then there is a finite subset a of f such that 
a t= YÂ· 
Proof. Since every model of r is a model of y, the set r u {--, y} has no 
models; that is, it is not satisfiable. Thus, by Theorem 6.1, there is a finite 

7. G6del's Incompleteness Theorem 
407 
subset a of f such that a u {-, y} is unsatisfiable. Thus every model of a 
is a model of y, i.e., a 1= y. 
â¢ 
Exercises 
1. Let nl' !lz be sets of sentences such that nl u n2 is unsatisfiable. 
Prove that there is a sentence a such that !l 1 I= a, and !l2 1= -, a. 
2. Show that if a set n of sentences has models with arbitrarily large 
finite domains, then it has a model with an infinite domain. [Hint: 
Show that n u {(3xl) ... (3x)/\is;i<j$n X; =F xj In EN} is satisfi-
able.] 
3. Let W be the vocabulary {0, c, s, >},where 0, care constant symbols, s 
is a function symbol with 8(s) = 1, and > is a relation symbol with 
8( >) = 2. Use the compactness theorem to show that the set of 
sentences {c > 0, c > s( 0 ), c > s( s( 0) ), ... } is satisfiable. 
*7. Godel's Incompleteness Theorem 
Let f be a recursive set of W-sentences for some given vocabulary W. We 
think of f as being considered for use as a set of "axioms" for some part 
of mathematics. The requirement that f be recursive is natural, because, 
by Church's thesis, it simply amounts to requiring that there be some 
algorithmic method of determining whether or not an alleged "axiom" 
really is one. Often f will be finite. We define T r = {y If I= y} and call 
T r the axiomatizable theory on W whose axioms are the sentences belong-
ing to the set f. Of course, it is quite possible to have different sets of 
axioms which define the same theory. 
If T is an axiomatizable theory, we write 
I-T 'Y 
(read: "T proves y") to mean that y E T. We also write 
lf-T y to mean 
that y f/:. T. The most important fact about axiomatizable theories is given 
by the following theorem. 
Theorem 7.1. An axiomatizable theory is r.e. 
Proof. 
By Theorems 3.1 and 6.4, y E T r if and only if 
( y 1 A y2 A Â· Â·Â· A Yn A -, y) 

408 
Chapter 13 Quantification Theory 
is unsatisfiable for some 'Y!' 'Yz' ..â¢ ' 'Yn E r. Since r is recursive, it is 
certainly r.e. Thus, by Theorem 4.11 in Chapter 4, there is a recursive 
function g on N whose range is f. For a given sentence y, let 
c5(n, y) = (g(O) A g(l) A Â·Â·Â· A g(n) A ..., y) 
for all n EN. Clearly, c5(n, y) is a recursive function of n and y. 
Moreover, the sentence y belongs to T r if and only if there is an n E N 
such that c5(n, y) is unsatisfiable. But by Theorem 4.3, the set of unsatisfi-
able W-sentences is r.e. Hence there is a partially computable function h 
which is defined for a given input if and only if that input is an unsatisfi-
able W-sentence. Let h be computed by program .9 and let p = #(.9). 
Then the following "dovetailing" program halts if and only if the input y 
belongs to T r, thereby showing that T r is r.e.: 
[A] 
Z+--15(/(T),y) 
T+-T+1 
IF- STP(l>(z, p, r(T)) GOTO A 
â¢ 
We shall see in the next section that there is a f such that T r is not 
recursive. 
Now let W be some vocabulary intended for use in expressing properties 
of the natural numbers. By a numeral system for W, we mean a recursive 
function 11 on N such that for each n E N, 11(n) is a W-term containing no 
variables, and such that for all n, m E N, n =1= m implies 11(n) =1= 11(m). 
When 11 can be understood from the context, we write n for 11(n). n is 
called the numeral corresponding to n and may be thought of as a notation 
for n using the vocabulary W. A popular choice is 
n = S(S( Â·Â·Â· S(O)) Â·Â·Â· ), 
where S is a function symbol of degree 1, 0 is a constant symbol, and the 
number of occurrences of S is n. 
Let a= a(b) be a W-formula and letT be an axiomatizable theory on 
W. Then, given a numeral system for W, we can associate with a the set 
U = {n EN 11--T a(n)}. 
(7.1) 
In this case, we say that the formula a represents the set U in T. If we begin 
with a set U ~ N, we can ask the question: is there a W-formula a which 
represents U in T? We have 
Theorem 7.2. If there is a formula a which represents the set U in an 
axiomatizable theory T, then U is r.e. 

7. Godel's Incompleteness Theorem 
409 
Proof. 
Let T be an axiomatizable theory, and let a represent U in T. By 
Theorem 7.1, we know that there is a program 9! that will halt for given 
input y if and only if I-T y. Given n E N, we need only compute a(n) 
[which we can do because v(n) = n is recursive], and feed it as input to 9!. 
The new program thus defined halts for given input n E N if and only if 
I-T a(n). By (7.1), U is r.e. 
â¢ 
In fact, there are many axiomatizable theories in which all r.e. sets are 
representable. To see the negative force of Theorem 7.2, we rewrite it as 
follows. 
Corollary 7.3. 
Let T be an axiomatizable theory. Then if U ~ N is not 
r.e., there is no formula which represents U in T. 
This corollary is a form of Godel's incompleteness theorem. To obtain a 
more striking form of the theorem, let us say that the formula a quasi-
represents the set U in T if 
{n EN li-T a(n)} ~ U. 
(7.2) 
We can think of such a formula a as intended to express the proposition 
"n E U" using the vocabulary W. Comparing (7.1) and (7.2) and consider-
ing Corollary 7.3, we have 
Corollary 7.4. 
Let T be an axiomatizable theory and let U ~ N be a set 
that is not r.e. Let the formula a quasi-represent U in T. Then, there is a 
number n0 such that n0 E U but lf-T a(n0 ). 
As we can say loosely, the sentence a(n0 ) is "true" but not provable. 
Corollary 7.4 is another form of Godel's incompleteness theorem. We 
conclude with our final version. 
Theorem 7.5. 
Let T be an axiomatizable theory, and let S be an r.e. set 
that is not recursive. Let a = a( x) be a formula such that a represents S 
in T, and -, a quasi-represents S in T. Then there is a number n0 such 
that lf-T a(fi0 ) and lf-T -, a(n0). 
Proof. 
We take U = S in Corollary 7.4 to obtain a number n 0 such that 
n0 E S, but lf-T -, a(n0 ). Since n0 $ S and a represents S in T, we must 
also have lf-T a(n0). 
â¢ 
In this last case, it is usual to say that a(n0) is undecidable in T. 
Exercises 
1. 
Let r be an r.e. set of W-sentences for some vocabulary W. Show that 
{y I r 1= y} is r.e. 

410 
Chapter 13 Quantification Theory 
2. Let T be an axiomatizable theory on some vocabulary W. T is consis-
tent if there is no W-sentence a such that both I-T a and I-T -, a, 
and T is inconsistent otherwise. 
(a) Show that if T is inconsistent then I-T a for all W-sentences a. 
(b) Show that if there is a formula which represents some nonrecur-
sive set in T, then T is consistent. 
(c) 
Show that if T is consistent and the formula a represents some 
r.e. set U in T, then -, a quasi-represents fJ in T. 
3. An axiomatizable theory T on vocabulary W is complete if for all 
W-sentences a, either I-T a or I-T -,a. Show that if Tis complete 
then it is recursive. [See also Exercise 2.] 
4. An axiomatizable theory T on some vocabulary W is w-consistent if the 
following holds for all W-formulas a(b ): If I-T -, a(n) for all n E N, 
then 
lf-T (3x)a(x). Show that if T is w-consistent then it is consis-
tent. [See Exercise 2 for the definition of consistency.] 
5. 
A function f(x 1 , â¢â¢â¢ , xn) is representable in an axiomatizable theory T 
if there is a formula a(b 1 , â¢â¢â¢ , bn, b) such that if f(m 1 , â¢â¢â¢ , mn) = k 
then 
I-T a(m 1 , ... ,mn,lc) and 
1--T(Vy)(a(m 1 , ... ,mn,y) :::>y =k). 
We say that a represents f(x 1 , â¢â¢â¢ , xn) in T. Let T be a consistent 
axiomatizable theory [see Exercise 2] such that I-T 0 =/= 1 and such 
that every primitive recursive function is representable in T. 
(a) Let a(x, y, t, z) represent the function STP(I)(x, y, t) in T, and 
for every r.e. set wm' let f3m(x) be the formula ( 3t )a(x, m, t, 1). 
Show that if n E wm then I-T f3m(n). 
(b) Show that if n ft wm then I-T -, a(n, m, i, 1) for all t EN. 
(c) 
Show that if T is w-consistent then n ft Wm implies lf-T f3m(n). 
[See Exercise 4.] 
(d) Conclude that if T is an w-consistent axiomatizable theory in 
which every primitive recursive function is representable and if 
I-T 0 =/= 1, then T has an undecidable sentence. 
*8. Unsolvability of the Satisfiability Problem in 
Predicate Logic 
In 1928, the great mathematician David Hilbert called the problem of 
finding an algorithm for testing a given sentence to determine whether it is 

8. Unsolvability of the Satisflability Problem in Predicate Logic 
411 
satisfiable "the main problem of mathematical logic." This was because 
experience had shown that all of the inferences in mathematics could be 
expressed within the logic of quantifiers. Thus, an algorithm meeting 
Hilbert's requirements would have provided, in principle, algorithmic 
solutions to all the problems in mathematics. So, when unsolvable prob-
lems were discovered in the 1930s, it was only to be expected that Hilbert's 
satisfiability problem would also turn out to be unsolvable. 
Theorem 8.1 
(Church-Turing). There is a vocabulary W such that 
there is no algorithm for testing a given W-sentence to determine whether 
it is satisfiable. 
Proof. 
Our plan will be to translate the word problem for a Thue process 
into predicate logic in such a way that a solution to Hilbert's satisfiability 
problem would also yield a solution to the word problem for the given 
process. 
Thus, using Theorem 3.5 in Chapter 7, let n be a Thue process on the 
alphabet {a, b} with an unsolvable word problem. Let n have the produc-
tions g; ~ h;, i = 1, 2, ... , K, together with their inverses, where we may 
assume that for each i, g;, h; =/= 0 (recall Theorem 3.5 in Chapter 7). We 
introduce the vocabulary W = {a, b, â¢, ~}, where a, b are constant symbols, 
â¢ is a function symbol, and ~ is a relation symbol, with 8( â¢) = 8( ~) = 2. 
We will make use of the interpretation I with domain {a, b}* - {0} which 
is defined as follows: 
if and only if u if v. 
For ease of reading, we shall write â¢ and ~ in "infix" position. Thus, 
we shall write, for example, 
((x â¢a) ~y) 
instead of 
~ (â¢(x,a), y). 
For each word w E {a, b}* - {0}, we now define a W-term w# as 
follows: 
a#= a, 
b# = b, 
(8.1) 
(ua)# = (u#â¢a), 
(ub)# = (u# â¢b). 

412 
Chapter 13 Quantification Theory 
We have 
Lemma 1. For every word w E {a, b}* - {0}, we have (w#)1 = w. 
Proof. The proof is by an easy induction on lwl, using (8.1) and the 
definition of the interpretation /. 
â¢ 
Let f be the set of W-sentences obtained by prefixing the appropriate 
universal quantifiers to each W-formula in the following list: 
1. 
(x=x), 
2. 
((x=y):::>(y=x)), 
3. 
(((x =y) 1\ (y = z)) :::> (x = z)), 
4. 
(((x = y) 1\ (u = v)) :::> ((x â¢ u) = (y â¢ v))), 
5. 
(((xâ¢y)â¢z) = (xâ¢(yâ¢z))), 
5 + i. (gt = ht}, 1 :::;; i :::;; K. 
We have 
Lemma 2. The interpretation I is a model of the set of sentences r. 
Proof. The sentences of r all express in logical notation basic facts about 
concatenation of strings and about derivations in Thue processes. Detailed 
verification is left to the reader. 
â¢ 
Lemma 3. If r I= (u# = v#), then u * 
v. 
Proof. 
By the definition of logical inference and Lemma 2, we have 
(u# = v#)I = 1. Hence 
â¢ 
We next wish to establish the converse of Lemma 3. For this it will 
suffice to show that if u 'if v, then the sentence 
A a 1\ ..., (u# = v#) 
aEf 
is unsatisfiable (recall Theorem 3.1). The Herbrand universe is 
H = {a,b,aâ¢a,aâ¢b,bâ¢a,bâ¢b,aâ¢ (aâ¢a), ... }. 
Let us call a W-sentence a a Herbrand instance of a W-formula {3 if a can 
be obtained from {3 by replacing each of its free variables by an element 
of H. a is said to be rooted if it is a tautological consequence of the 
sentences 5 + i together with Herbrand instances of the formulas listed in 
1-5. Obviously, if the sentence {3 is rooted, then r I= {3. 

8. Unsolvabillty of the Satlsflablllty Problem in Predicate Logic 
413 
Lemma 4. If w = uv, where u -=!= 0 and v-=!= 0, then 
(w* ~ (u* â¢ v*)) 
(8.2) 
is rooted. 
Proof. The proof is by induction on I vi. If I vi = 1, we can assume without 
loss of generality that v = a. But in this case, the sentence (8.2) is a 
Herbrand instance of formula 1. 
Supposing the result known for v, we need to establish it for va and vb. 
We give the proof for va, that for vb being similar. So let w = uv, where 
we can assume that (8.2) is rooted. We need to show that the sentence 
((wa)* ~ (u* â¢(va)*)) 
is likewise rooted. By (8.1) this amounts to showing that 
((w* â¢a) ~ (u* â¢ (v* â¢a))) 
is rooted. But this follows from the induction hypothesis, noting that the 
following sentences are rooted. (For each of these sentences, the number 
of the corresponding formula of which it is a Herbrand instance is given.) 
(a ~ a) 
(1) 
(((w* ~ (u* â¢ v*)) 1\ (a~ a)):::> ((w* â¢a) ~ ((u* â¢ v*) â¢a))) (4) 
(((u* â¢ v*) â¢a) ~ (u* â¢ (v* â¢a))) 
(5) 
((((w* â¢a) ~ ((u* â¢ v*) â¢a)) 1\ (((u* â¢ v*) â¢a) ~ (u* â¢ (v* â¢a))))) 
:::> (((w* â¢a) ~ (u* â¢ (v* â¢a)))). 
(3) 
â¢ 
Lemma 5. If u rr v, then (u* ~ v*) is rooted. 
Proof. For some i, 1 ::; i ::; K, we have either u = pg;q, v = ph;q, or 
u = ph;q, v = pg;q, where p, q E {a, b}*. We may assume that in fact 
u = pg;q, v = ph;q, because in the other case we could use the following 
Herbrand instance of formula 2: 
((v* ~ u*) :::> (u* ~ v*)). 
The proof now divides into three cases. 
Case I. p = q = 0. Then the sentence (u* ~ v*) is just 5 + i and is 
therefore in r. 

414 
Chapter 13 Quantification Theory 
Case II. 
p = 0, q =1= 0. Using 5 + i and the following Herbrand in-
stance of formula 4: 
we see that the sentence 
is rooted. Using Lemma 4 and Herbrand instances of formulas 2 and 3 
we obtain the result. 
Case III. 
p, q =I= 0. Using Case II, the sentence ((g;q)# ~ (h;q)#) is 
rooted. Using the Herbrand instance of formula 4: 
(((p# ~p#) A ((g;q)# ~ (h;q)#)) 
::) ((p# â¢(g;q)#) ~ (p# â¢(h;q)#))), 
we see that 
is rooted. The result now follows using Lemma 4 and Herbrand 
instances of formulas 2 and 3. 
â¢ 
Lemma 6. If u 'fr v, then (u# ~ v#) is rooted. 
Proof. The proof is by induction on the length of a derivation of v from 
u. If this length is 1, then v = u, and we may use a Herbrand instance of 
formula 1. To complete the proof, we may assume that u 'fr w If v, where 
it is known that (u# ~ w#) is rooted. By Lemma 5, (w# ~ v#) is rooted. 
We then get the result by using the following Herbrand instance of 
formula 3: 
â¢ 
Combining Lemmas 3 and 6, we obtain 
Lemma 7. 
u 'fr v if and only if r 1= (u# ~ v#). 
Now it is easy to complete the proof of our theorem. If we possessed an 
algorithm for testing a given W-sentence for satisfiability, we could use it 
to test the sentence 
A a A -,(u# ~ v#) 
aEf 

8. Unsolvablllty of the Satisfiability Problem in Predicate Logic 
415 
and therefore, by Theorem 3.1, to test the correctness of the logical 
inference r 1= (u# ~ v#). This would in turn lead to an algorithm for 
solving the word problem for n, which we know is unsolvable. 
â¢ 
A final remark: We really have been working with the axiomatizable 
theory Tr. Thus what Lemma 7 states is just that 
(8.3) 
Hence we conclude that the theory T r is not recursive. [If it were, we 
could use (8.3) to solve the word problem for n.] Thus we have proved 
Theorem 8.2. There are axiomatizable theories that are not recursive. 
Exercises 
1. Prove Lemma 2. 
2. Let W be the vocabulary used in this section. Show that for every 
deterministic Turing machine L there is a finite set f of W-sentences 
and a computable function f(x) such that for any string w, L accepts 
w if and only if r I= f( w ). [Hint: See Theorems 3.3 and 3.4 in Chapter 
7.] 


Part 4 
Complexity 


14 
Abstract Complexity 
1. The Blum Axioms 
In this chapter we will develop an abstract theory of the amount of 
resources needed to carry out computations. In practical terms resources 
can be measured in various ways: storage space used, time, some weighted 
average of central processor time and peripheral processor time, some 
combinations of space and time used, or even monetary cost. The theo-
rems proved in this chapter are quite independent of which of these 
"measures" we use. We shall work with two very simple assumptions 
known as the Blum axioms after Manuel Blum, who introduced them in his 
doctoral dissertation. These assumptions are satisfied by any of the "mea-
sures" mentioned above (if given precise definitions in any natural man-
ner) as well as by many others. 
Definition. A 2-ary partial function C on N is called a complexity 
measure if it satisfies the Blum axioms: 
1. C(x,i)J, if and only if <l>;(x)J,; 
2. The predicate C(x, i) ::5; y is recursive. (This predicate is of course 
false if C(x, i) i .) 
We write C;(x) = C(x, i). We think of C;(x) as the complexity of the 
computation that occurs when the program whose number is i is fed the 
419 

420 
Chapter 14 Abstract Complexity 
input x. It is not very difficult to see that various natural ways of 
measuring complexity of computation do satisfy the Blum axioms. What is 
remarkable is that some very interesting and quite nontrivial results can be 
derived from such meager assumptions. 
Let us examine some examples of proposed complexity measures: 
1. C;(x) =the number of steps in a computation by program number ion 
input x. The first axiom is clearly satisfied; the second follows from the 
computability of the step-counter predicate STP(I). 
2. M;(x) =the largest value assumed by any variable in program number i 
when this program is given input x, if <l>;(x)!; M;(x)j otherwise. The 
definition forces the first axiom to be true. The truth of the second 
axiom is a more subtle matter. The key observation is that, for a 
given program, there are only finitely many different snapshots1 in 
which all variables have values less than or equal to a given number 
y. Hence, given numbers i, x, y we can test the condition M;(x) ::::; y 
by "running" program number i on the input x until one of the 
following occurs: 
I. A snapshot is reached in which some variable has a value > y. 
Then we return the value FALSE. 
II. The computation halts with all variables having values ::::; y. Then 
we return the value TRUE. 
III. The same snapshot is reached twice. (By the pigeon-hole principle 
this must happen eventually if neither I nor II occurs.) Then, 
recognizing that the computation is in an "infinite" loop and so 
will never terminate, we return the value FALSE. (The reader 
should note that this algorithm in no way contradicts the unsolv-
ability of the halting problem. Case I can include both halting 
and nonhalting computations.) 
We will make important use of this "maximum-space" complexity 
measure, and we reserve the notation M;(x) for it. 
3. C;(x) = <l>;(x). Although the first Blum axiom is satisfied, the second 
is certainly not; namely, choose i so that 
<1>/x) = { ~ 
for xES 
otherwise, 
where S is any given r.e. nonrecursive set. Then the condition 
<l>;(x) ::::; 0 is equivalent to x E S and hence is not recursive. 
1 The definition of snapshot is in Chapter 2, Section 3. 

1. The Blum Axioms 
421 
If P(x) is any predicate on N, we write 
P(x) 
a.e., 
and say that P(x) is true almost everywhere, to mean that there exists 
m0 EN such that P(x) is true for all x > m 0 â¢ Equivalently, P(x) is true 
for all but a finite set of numbers. We may think of a partial function on N 
as a total function with values in the set N u {oo}. That is, we write 
g(x) = oo to mean that g(x)j. We extend the meaning of < so that 
n < oo for all n EN. x ~ y continues to mean x < y or x = y, so that 
n ~ oo for n E N but also oo ~ oo. 
The second Blum axiom can be written in the equivalent forms: 
2'. The predicate C;(x) = y is recursive. 
2". The predicate C;(x) < y is recursive. 
To see that 2, 2' and 2" are all equivalent we note that 
C;(x) = y <=> (C;(x) ~ y & - (C;(x) ~ y ...:...1)) v (y = 0 & C;(x) ~ y), 
so that 2 implies 2'. 2' implies 2" because 
C;(x) < y <=> (3z)</C;(x) = z). 
Finally, 2" implies 2 because 
C;(x) ~y <=> C;(x) <y + 1. 
Let us call a recursive function r(x) a scaling factor if 
1. r is increasing, i.e., r(x + 1) ~ r(x), and 
2. limx--.oo r(x) = oo, i.e. r assumes arbitrarily large values. 
Condition 1 is obviously equivalent to the statement: x ~ y implies 
r(x) ~ r(y). Then we have 
Theorem 1.1. Let C;(x) be a complexity measure and let r(x) be a scaling 
factor. Let D;(x) = r(C;(x)). Then D;(x) is a complexity measure. 
Proof. It is clear that D satisfies the first Blum axiom. To test D;(x) ~ y, 
note that if y < r(O) then D;(x) = r(C;(x)) ~ r(O) > y. Otherwise, find the 
number t for which 
r(O) ~ r(l) ~ r(2) ~ Â·Â·Â· ~ r(t) ~ y < r(t + 1). 
We claim that D;(x) ~ y if and only if C;(x) ~ t. It remains only to verify 
this claim. If C;(x) ~ t, then 
D;(x) = r(C;(x)) ~ r(t) ~ y. 

422 
Chapter 14 Abstract Complexity 
Otherwise, if t + 1 :::;; C;(x), then 
y < r(t + 1) :::;; r(C;(x)) = D;(x). 
â¢ 
This theorem is hardly surprising. Naturally, if C;(x) is a plausible 
complexity measure, we would expect 2c,<x> to be one as well. What is 
surprising is that any pair of complexity measures are related to each other 
in a manner not so different from C and D in Theorem 1.1. 
Theorem 1.2 
(Recursive Relatedness Theorem). Let C and D be arbi-
trary complexity measures. Then there is a recursive function r(x, y) such 
that r(x, y) < r(x, y + 1), and for all i 
C;(x) :::;; r(x, D;(x)) 
a.e. 
and 
(1.1) 
D;(x):::;; r(x,C;(x)) 
a.e. 
[where we let r(x,oo) = oo for all x]. 
Proof. Note that by the first Blum axiom 
C;(x),l. 
if and only if 
<l>;(x),l. 
if and only if D;(x),l.. 
By the second Blum axiom (in the form 2'), the predicate 
C;(x) = y v 
D;(x) = y 
is recursive. Hence the function h defined as follows is recursive: 
hC 
) = { max(C;(x), D;(x)) 
l, X, y 
O 
if 
C;(x) = y 
v 
D;(x) = y 
otherwise. 
Let 
r(x,y) =y + maxj,;xmaxz,;yh(j,x,z), 
so that r(x, y) is recursive. Then 
r(x,y + 1) = (y + 1) + maxj,;xmaxz,;y+l h(j,x,z) 
> y + max j, x max z, Y h(j, x, z) 
= r(x, y) 

1. The Blum Axioms 
423 
since maximizing over a larger set of numbers cannot result in a smaller 
outcome. Moreover, using this same principle, and assuming that x ~ i, 
r(x,D;(x)) ~ maxj,xmaxzsD;(x>h(j,x,z) 
Thus, the inequality 
~ h(i, x, D;(x)) 
(since x ~ i) 
= max(C;(x), D;(x)) 
~ C;(x). 
holds for all x ~ i and hence almost everywhere. Since the definition of h is 
symmetric in C and D, the same argument shows that 
a.e. 
â¢ 
As we shall see, one use of the recursive relatedness theorem is in 
enabling us to proceed, in some cases, from the knowledge that a theorem 
is true for one particular complexity measure to the truth of that theorem 
for all complexity measures. 
Exercises 
1. Which of the following are complexity measures? 
(a) C;(x) = 0 for all i, x. (That is, all computation is "free.") 
(b) C;(x)={M
0 ;(x) 
for ift.A 
for i E A, 
where A is some given finite set such that <I>; is total for all 
i EA. (That is, the programs whose numbers belong to A can 
be run "free.") 
(c) C;(x) = 2ct>;(x>. 
C( ) _ {M;(x) 
if 
i is even 
(d) 
; x -
the number of steps in computing <l>;(x) if 
i is odd. 

424 
Chapter 14 Abstract Complexity 
2. 
Prove that if C is a complexity measure and 
D-(x) = { C;(x) 
I 
0 
for i It A 
for i E A, 
where A is as in Exercise l(b), then D is a complexity measure. 
3. Let C;(x) be the number of steps in the computation on input x by .9' 
program .9, where #(.9) = i. For some fixed n > 0, let D;(x) be the 
number of steps in the computation on input x by ~ program .9', 
where #(.9) = i and .9' is constructed from .9J as in Section 3 of 
Chapter 5, by treating each .9' instruction as a macro in ~ 
. 
(a) Show that D is a complexity measure. 
(b) Give a function r(x, y) that satisfies the recursive relatedness 
theorem for C and D. [See Exercise 3.2 in Chapter 5.] 
4. 
Let C be a complexity measure. 
(a) Show that for every i, C;(x) is partially computable. 
(b) Show that if cl>;(x) is total, then C;(x) is computable. 
5. 
Let C be a complexity measure. Show that the predicate P(i), 
defined 
P(i) <=> (Vx)(3y E N)C;(x) :::;; y, 
is not computable. 
6. Let C be an arbitrary complexity measure. Show that there is a 
recursive function t such that 
<l>;(x) :::;; t(x, C;(x)) 
a.e. 
[Hint: Use the complexity measure M;(x) and the recursive related-
ness theorem.] 
7. 
Can the result of the previous problem be improved so that t is a 
unary recursive function such that 
<l>;(x) :::;; t(C;(x)) 
a.e.? 
Prove that your answer is correct. 
8. (a) Let C be the complexity measure in Example 1. Show that for 
any computable function f(x) there is a program number i such 
that cl>;(x) = 0 and C;(x) > f(x) for all x. Conclude that there 
are arbitrarily (with respect to computable lower bounds) slow .9' 
programs that compute constant functions. 

2. The Gap Theorem 
425 
(b) Let D be an arbitrary complexity measure. Show that for any 
computable function f(x) there is a program number i such that 
<l>;(x) = 0 for all x and D;(x) > f(x) a.e. [Hint: Use (a) and the 
recursive relatedness theorem.] 
9. Let C be a complexity measure. Show that there is no computable 
function g(x,y) such that for all i, x, if <l>;(x),l. then C;(x):::;; 
g(x, <l>;(x)). Compare with Exercise 6. [Hint: Use Exercise 8.] 
10. Let C be a complexity measure. Show that for any computable 
function f(x) there is a computable function g(x) such that g(x) :::;; 1 
for all x and such that for any i, if <I>; = g then C;(x) > f(x) for 
infinitely many x. Conclude that there are arbitrarily (with respect to 
computable lower bounds) complex "small" computable functions. 
[Hint: Define 
g(x) = ( ~ 
if cl(x)(x) :::;; f(x) and <1>/(x)(x) * 1 
otherwise.] 
11. Let C be a complexity measure. Show that for any computable 
function f(x) there is a computable function g(x) such that g(x) :::;; x 
for all x and such that for any i, if <I>; = g then C;(x) > f(x) for all 
x > i. Compare with Exercise 10. 
2. The Gap Theorem 
In this section Cis some given fixed complexity measure. Suppose that t(x) 
is a complexity bound. That is, assume that we are restricted to computa-
tions for which C;(x) :::;; t(x) whenever <l>;(x),l.. Then, in response to our 
complaints, the bound is increased enormously to g(t(x)), where g is some 
recursive, rapidly increasing function, e.g., g(x) = 2x or 
.2} 
g(x) = 22 . Â· 
x 
or 
Then, we can carry out far more computations. Right? Wrong! If the 
original function t(x) is sufficiently tricky, it is possible that for every i, 
there are only finitely many values of x for which 
C;(x) :::;; g(t(x)), 
but not C;(x):::;; t(x). 
This surprising assertion is a consequence of the gap theorem. 

426 
Chapter 14 Abstract Complexity 
Theorem 2.1 
(Gap Theorem). Let g(x, y) be any recursive function 
such that g(x, y) > y. Then, there is a recursive function t(x) such that if 
x > i and C;(x) < g(x, t(x)), then C;(x) ~ t(x). (See Fig. 2.1.) 
Proof. Consider the predicate 
P(x,y) +-+ ('vi)<x(C;(x) ~y v g(x,y) ~ C;(x)). 
By the second Blum axiom, the predicate C;(x) ~ y is computable. So is 
the predicate 
g(x,y) ~ C;(x) =- (3z)<g(x.Jz = C;(x)). 
Hence, P(x, y) is also recursive. We define 
t(x) = minY P(x,y), 
(2.1) 
so that t is a partially computable function. We will show that t is total. 
Let x be a given number. Consider the set Q = {C;(x) I i < x & <l>;(x)! }. 
Let Yo = 0 if Q = 0 and let y0 be the largest element of Q otherwise. We 
Figure 2.1. For x > i, C;(x) cannot enter the "gap." 

2. The Gap Theorem 
427 
claim that P(x, y0 ) is true. To see this, choose i < x. Then if <l>;(x),!., then 
C;(x)J, and therefore C;(x) :::;; Yo. If, on the other hand, <l>;(x)j then 
C;(x) i. Since g(x, y 0 ),l., the predicate g(x, y0 ) :::;; C;(x) is true. Thus, we 
have P(x, y0 ). We have shown that for every x EN there is a number y 
such that P(x, y). Thus, t(x) defined by (2.1) is total and therefore 
recursive. 
Now let x > i and C;(x) < g(x, t(x)). Since P(x, t(x)) is true, and i < x, 
we have C;(x):::;; t(x) v g(x, t(x)):::;; C;(x). But C;(x) < g(x, t(x)). Hence 
C;(x) :::;; t(x). 
â¢ 
In their fine book, Machtey and Young (see "Suggestions for Further 
Reading") give an amusing interpretation of the gap theorem. Let us 
imagine two computers, one of which is very much faster than the other. 
We think of each computer equipped with a reasonably efficient inter-
preter for our programming language .9' so that we can speak of running a 
program qf .9' on one or another of the computers. Let C;(x) be the 
computation time of the slow computer running program number i on 
input x. Similarly for D;(x) and the fast computer. Clearly, C and D 
satisfy the Blum axioms. By the recursive relatedness theorem, there is a 
recursive function r satisfying (1.1). If we let g(x, y) = r(x, y) + y + 1, 
then we have g(x, y) > y, g(x, y + 1) > g(x, y) and 
C;(x):::;; r(x, D;(x)) < g(x, D;(x)) 
a.e. 
Now let t(x) satisfy the gap theorem for the complexity measure C with 
respect to this function g. And consider a program .9J with number i such 
that D;(x) :::;; t(x) a.e. That is, for sufficiently large inputs x, .9J runs on 
the fast machine in time bounded by t(x). Then on the slow computer, .9J 
will run in time 
C;(x) < g(x, D;(x)) :::;; g(x, t(x)) 
a.e. 
But now the gap theorem comes into play to assure us that 
C;(x) :::;; t(x) 
a.e. 
Conclusion: Any program that runs in time t(x) on the fast computer also 
runs in time t(x) (for sufficiently large x) on the slow computer! 
Exercises 
1. Let C be a complexity measure. Does the gap theorem imply that 
there is no program number i such that lxl 2 :::;; C;(x):::;; lxl 3 a.e.? 
Explain. 

428 
Chapter 14 Abstract Complexity 
2. Let C be a complexity measure. We will say that a total function f(x) 
is C-constructible if there is a program number i such that C;(x) = f(x) 
for all x. Prove or disprove that every computable function is C-con-
structible. 
3. 
Preliminary Form of the Speedup Theorem 
Computer scientists often seek programs that will obtain a desired result 
using minimum resources. The speedup theorem, which is the deepest 
theorem in this chapter, tells us that it is possible for there to be no best 
program for this purpose. Roughly speaking, the theorem states that there 
exists a recursive function that is so badly behaved that for every program 
to compute it, there is another program that computes the same function 
but which uses much less resources. The proof of the speedup theorem is 
quite intricate. In this section we will prove a preliminary version. Then in 
the next section we will use this preliminary version to obtain the full 
speedup theorem. The proof of the speedup theorem will use the parame-
ter theorem and the recursion theorem from Chapter 4 (Theorems 5.1 and 
8.1). 
We define a particular complexity measure M;(x) as follows. If <l>;(x)j, 
then M;(x)j. If <l>;(x)L then M;(x) is the largest value assumed by any 
variable in program number i when computing with input x. Thus M;(x) is 
just the complexity measure in Example 2 of Section 1. We will also work 
with MF>(x 1 , x2 ), which is defined exactly like M;(x) except that program 
number i is given the pair of inputs x 1 , x 2 â¢ M;(x) and MF>(x 1 , x 2 ) are 
related by 
Theorem 3.1. 
MF>(x, y) = Ms/(y,i)(x), where S/ is the function defined in 
the parameter theorem. 
Proof. Let i = #(.90). Then, examining the proof of Theorem 5.1 in 
Chapter 4, we see that S/(y, i) = #(.9), where .9 is a program consisting 
of y copies of the instruction X 2 +-- X 2 + 1 followed by the program .90 â¢ 
The result is now obvious. 
â¢ 
Our preliminary form of the speedup theorem is as follows. 
Theorem 3.2. Let g(x, y) be any given recursive function. Then there is a 
recursive function f(x) such that f(x) ::; x and, whenever <I>; = f, there is 

3. Preliminary Form of the Speedup Theorem 
429 
a j such that 
ci>i(x) = f(x) 
a.e. 
(3.1) 
and 
a.e. 
(3.2) 
Discussion. To see the force of the theorem take g(x, y) = 2Y. Then, 
given <I>; = f, there is a j satisfying (3.1) such that 
i.e., 
M/x) :::;; log 2 M;{x) 
a.e. 
Thus program number j computes f a.e. and uses far less resources than 
program number i. In Section 4 we shall improve this preliminary version 
of the speedup theorem by eliminating the "a.e." condition in (3.1) and by 
obtaining (3.2) for an arbitrary complexity measure, not merely for M. 
The proof of Theorem 3.2 will use a diagonal argument, but one far 
more complex than we have encountered so far. Let us recall how a simple 
diagonal argument works. When we write 
K = {n E N I n ft W,} 
we know that K is not r.e. because it differs from each r.e. set W; with 
respect to the number i, namely, i E W; if and only if i ft K. More 
generally, a diagonal argument constructs an object that is guaranteed not 
to belong to a given class by systematically ensuring that the object differs 
in some way from each member of the class. More intricate diagonal 
arguments often are carried out in an infinite sequence of stages; at each 
stage one seeks to ensure that the object being constructed is different 
from some particular member of the class. The proof of the speedup 
theorem is of this character. 
Proof of Theorem 3.2. We will proceed through "stages" x = 0, 1, 2, 3, .... 
At each stage x and for certain n, w EN, we will define a set C(n, w, x) ~ 
N. We think of the members of C(n, w, x) as numbers of programs which 
are cancelled at stage x with respect to n and w. C(n, w, x) is defined 

430 
Chapter 14 Abstract Complexity 
recursively by the equation 
C(n,w,x)={iENiw:o;;i<x&i$. UC(n,w,y) 
y<x 
(3.3) 
&M;(x) <g(x,M~2>(x,i + 1))}. 
We think of Cas a 3-ary partial function on N. (The fact that the values of 
C are finite subsets of N instead of numbers is of no importance. 
Naturally, if we wished, we could use some coding device to represent each 
finite subset of N by a particular number.) The three conditions in (3.3) 
connected by & are to be tested in order with the understanding that if the 
first or second condition is false, the succeeding conditions are simply not 
tested. Thus we have 
W ~X 
implies C(n, w, x) = 0 
for all n. 
(3.4) 
Moreover, we have obviously 
Lemma 1. If C(n, w, y) ~ for all y < x and Mp>(x, i + 1) ~ for all i such 
that w:::;; i < x, then C(n, w, xH. 
Indeed, when the conditions of Lemma 1 are satisfied, we can explicitly 
compute C(n, w, x) given knowledge of C(n, w, y) for y < x. Now clearly, 
when the conditions of Lemma 1 are not satisfied, C(n, w, x)j. Thus (3.3) 
can be used to give an algorithm for computing C and we may conclude 
that C is a partially computable function. 
Lemma 2. If i E C(n, w, x), then M;(xH and cl>;(xH. 
Proof. The truth of the condition 
M;(x) < g(x, M~ 2>(x, i + 1)) 
implies that M;(x) ~,and by the Blum axioms, this implies cl>;(x) ~. 
â¢ 
We shall now define a 3-ary partially computable function k on N such 
that if C(n, w, xH, then for each i E C(n, w, x), we will have k(x, w, n) -=!= 
cl>;(x). k is computed by using the following procedure: 
Compute C(n, w, x). If this computation terminates, compute cl>;(x) 
for each i E C(n, w, x). [By Lemma 2, each such ct>;(xH .] Finally, 
set k(x, w, n) equal to the least number which is not a member of the 
finite set 
{cl>;(x)li E C(n,w,x)}. 

3. Preliminary Form of the Speedup Theorem 
431 
It is to this function k that we apply the recursion theorem. Thus, we 
obtain a number e such that 
<1>~ 2 >(x, w) = k(x, w, e). 
(3.5) 
Lemma 3. If x ::::; w, then k(x, w, e) = 0. 
Proof. 
Let x::::; w. By (3.4), C(e, w, x) = 0. Hence, by definition, k(x, w, e) 
is the least number which does not belong to 0, namely, 0. 
â¢ 
Lemma 4. If k(x, w, e)!, then k(x, w, e) ::::; x. 
Proof. The largest possible value for k(x, w, e) would be obtained if the 
values <l>;(x) for i E C(e, w, x) were all different and were consecutive 
numbers beginning with 0. In this "worst" case, there would be as many 
values of <l>;(x) as in the set C(e, w, x). But, 
C(e, w, x) ~ {i EN I w::::; i < x} 
~ {0, 1, 2, ... , X -
1}. 
Thus, all the values of <l>;(x) would be < x and hence k(x, w, e) ::::; x. 
â¢ 
Lemma 5. 
Let x > w. Suppose that 
<t>?>(x, w + 1)!, <1>~ 2>(x, w + 2H, ... , <t>?>(x, x) ~ 
(3.6) 
and 
<t>?><o, w)!. <1>?>(1, w)!. ... , <1>~ 2>(x -
1, w H. 
(3.7) 
Then, <1>~ 2>(x,w)!, i.e., k(x,w,eH. 
The reader is referred to Fig. 3.1 in connection with this lemma. In 
effect, Lemma 5 states that if <1>~ 2 > is defined along both the horizontal and 
vertical "pincers" shown pointing at (x, w ), then it must also be defined at 
(x, w). 
Proof of Lemma 5. 
By (3.7), 
<1>~ 2>(y, w H for all y < x. By definition 
of k(y,w,e) = <t>?>(y,w), we have that C(e,w,y)! for all y <x. By 
(3.6), 
<1>~ 2>(x, i + 1)~ for all i such that w::::; i < x. Hence, likewise, 
M?>(x, i + 1)~ for these i. By Lemma 1, C(e, w, xH. But now, by defini-
tion of k, k(x, w, eH. 
â¢ 
Lemma 6. 
<1>?> is total. 
Proof. 
We shall prove by induction on x the assertion 
For all w, 
(3.8) 

432 
Chapter 14 Abstract Complexity 
X 
Figure 3.1. Horizontal and vertical "pincers" pointing at (x, w ). (Sec Lemma 5.) 
By Lemma 3, we have 
<1>~ 2>(0, w) = k(O, w, e) = 0, 
which gives the result for x = 0. Suppose that x > 0, and it is known that 
ci>~Z>(y' W) J, 
for all y < x and all w. We shall show that (3.8) then follows. 
By Lemma 3, (3.8) holds for all w ~ x. Thus, we need show only that 
(3.8) holds for w < x. That is, it suffices to show that 
ci>~ 2>(x, x - 1) J, ... , ci>~2>(x, O)J,. 
We will prove each of these in succession by using Lemma 5. That is, in 
Lemma 5, we successively set w = x - 1, x - 2, ... , 0. In each case (3.7) 
(the horizontal "pincer") is satisfied by the induction hypothesis. For 
w = x -
1, (3.6) requires only that 
cl>~ 2>(x, x)J,, and this last follows at 
once from Lemma 3. Thus by Lemma 5, ci>~ 2>(x, x -
1) J,. But this means 
that (3.6) is now satisfied with w = x - 2. Hence once again Lemma 5 
shows that cl>~ 2>(x, x - 2) J,. Continuing, we eventually obtain cl>~ 2>(x, 0) J, . â¢ 
For the remainder of the proof of Theorem 3.1, we will use the notation 
lw = {i EN I i < w} = {0, 1, ... , W- 1}. 

3. Preliminary Form of the Speedup Theorem 
433 
Lemma 7. C(n, w, x) = C(n, 0, x) - Iw. 
Proof. The proof is by induction on x. C(n, w, 0) = 0 for all n, w. Hence 
the result for x = 0 is trivially true. Suppose the result known for all y < x. 
We obtain the result for x as follows (noting {i E N I w ::; i < x} n lw = 0): 
C(n,w,x) = {i EN I w::; i <x & i $. U C(n,w,y) 
y<x 
& M;(x) < g(x, M~2>(x, i + 1))} 
= {iENiw::=;i<x & i$. U (C(n,O,y) -lw) 
y<x 
& M;(x) < g(x, M~2>(x, i + 1))} 
={iENiw::=;i<x&i$. UC(n,O,y) 
y<x 
& M;(x) < g(x, M~2>(x, i + 1))} 
={iENIO::=;i<x&i$. UC(n,O,y) 
y<x 
&M;(x) <g(x,M~2>(x,i + 1))} -lw 
= C(n,O,x)- lw. 
â¢ 
Lemma 8. 
For each w E N, there is a number mw such that for all 
x > mw, we have 
<t><2>(x w) = <t><2>(x 0) 
e 
' 
e 
' 
Â· 
Proof. 
By (3.3) [the definition of C(n, w, x)], we have C(e, 0, x) n 
C(e, 0, y) = 0 for x -=!= y. [Numbers in C(e, 0, y) for y < x are automati-
cally excluded from C(e, 0, x).] Hence each number in Iw belongs to at 
most one of the sets C(e, 0, x). If we let mw be the largest such value of x, 
then for X> mw, 
C(e,O,x) n lw = 0. 
Hence, using Lemma 7, for X > mw' 
C(e,w,x) = C(e,O,x)- lw = C(e,O,x). 

434 
Chapter 14 Abstract Complexity 
Hence, by the definition of the function k we have for X > mw' 
<I>;2>(x, w) = k(x, w, e)= k(x,O, e)= <t>?>(x,O). 
â¢ 
Note that there is no claim being made that mw is a computable 
function of w, and indeed it is not! 
We are now ready to define the function f(x) whose existence is 
asserted in Theorem 3.2. We set 
f(x) = <I>;2>(x,O). 
Lemma 9. If <I>; = f and x > i, then 
g(x, M~ 2>(x, i + 1)) :::;; M;(x). 
Proof. 
Suppose otherwise. Choose the least value of x > i with 
g(x, M~ 2>(x, i + 1)) > M;(x). 
(3.9) 
Then we claim that for y < x, i $. C(e, 0, y ). This is because 
C(e,O,y) = {j EN I j <y & j $. U C(e,O,z) 
z<y 
&M/y) <g(y,M~2>(y,j + 1))}, 
so that, if i E C(e, 0, y), we would have i < y < x, and 
g(y, M~ 2>(y, i + 1)) > M;(y), 
contradicting the choice of x as the least number > i satisfying (3.9). 
Thus, we have 
i ft. U C(e,O,y). 
y<x 
Hence, 
iEC(e,O,x)={jENij<x&j$. UC(e,O,y) 
y<x 
& Mi(x) < g(x, M~2>(x,j + 1)) }Â· 
Now k(x, 0, e) was defined to be different from all <l>i(x) for which 
j E C(e, 0, x). Hence, k(x, 0, e) =/= <l>;(x). But 
k(x,O,e) = <1>~ 2>(x,O) =f(x) = <l>;(x), 
This contradiction completes the proof. 

4. The Speedup Theorem Concluded 
435 
Proof of Theorem 3.2 Concluded. 
Let <I>; = f, and set j = Sf<i + 1, e). 
Then, by Theorem 3.1 and Lemma 9, we have for x > i, 
g( x, Mj(x)) = g(x, M;2>(x, i + 1)) 5o M;(x), 
which proves (3.2). Finally, using the parameter theorem (Theorem 5.1 in 
Chapter 4) and Lemma 8 we have for x > m;+ 1 , 
<1>/x) = <t>?>(x,i + 1) = <I>!2>(x,O) =f(x), 
which proves (3.1). 
4. The Speedup Theorem Concluded 
â¢ 
We will begin by showing how to eliminate the a.e. from Eq. (3.1) in 
Theorem 3.2. The technique we will use is a general one; to change a 
condition 
<1>/x) = f(x) 
a.e. 
into an equation valid everywhere, we need only modify program number j 
to agree with f(x) at a finite number of values. We can do this by patching 
in a "table look-up" program. More precisely, we have 
Theorem 4.1. There is a recursive function t(u, w) such that 
<I> 
{x) - { <1>/x) 
t(u,w) 
-
(r(w))x+ 1 
Mt(u,w)(x) = M/x) 
if 
x > l(w) 
if 
x 5o l(w), 
if 
x > l(w). 
Here, once again we are using the pairing functions and Godel numbers 
as coding devices (Chapter 3, Section 8). 
Proof. 
Let the numbers u, w be given. Let Pu be program number u of 
the language .Y, if this program begins with a labeled statement. Other-
wise let Pu be program number u modified by having its initial statement 
labeled by a label not otherwise occurring in the program. In either case 
let L be the label with which Pu begins. 
Let Qu w be a program of Y which computes the primitive recursive 
function (r(w))x+ 1 , which always terminates using a branch instruction, 
and which has no labels in common with Pu . Let V be a local variable that 
occurs neither in Pu nor in Qu w. Let t(u, w) be the number of the 
program indicated in Fig. 4.1. N~te that V- X is to be replaced by a 

436 
V+-X 
~=~=!) 
. 
l(w) 
V+-V-1 
IF V'i' OGOTO L 
Qu,w 
pu 
Figure 4.1 
Chapter 14 Abstract Complexity 
suitable macro expansion as in Chapter 2 and that there are /( w) state-
ments V +-- V -
1. Clearly this can all be done with t a recursive (even 
primitive recursive) function. 
Now, letx > l(w). Then after the /(w) decrement instructions V +-- V- 1 
have been executed, V will have the value x- l(w) > 0. Hence, the 
branch shown will be taken and program Pu will be executed. Hence, 
<l>,(u,w)(x) = <l>u(x). To compare the value of M,(u,w)(x) and Mu(x) we 
need to be concerned about the maximum value assumed by variables in 
the macro expansion of V +-- X. Examining this macro expansion as given 
in (c) in Chapter 2, Section 2, we see that the only possibility for a number 
> x to arise is in the case x = 0. This is because local variables need to be 
incremented to 1 in this macro expansion in order to force a branch to be 
taken.2 However, we are assuming x > l(w) ~ 0, so that x =/= 0. Hence, 
Mt(u,w)(x) = Mu(x). 
Finally, let x ~ l(w). Then after /(w) executions of V +-- V- 1, V has 
the value 0. Thus Qu,w is executed. Hence, <l>t(u,w)(x) = (r(w))x. 
â¢ 
Now we can easily prove 
Theorem 4.2. Let g(x, y) be any given recursive function. Then there is a 
recursive f(x) such that f(x) ~ x and, whenever <I>; = f, there is a j such 
that 
<1>/x) = f(x) 
and 
g( x, M/x)) ~ M;(x) 
a.e. 
2 Actually, if each unconditional branch statement in program (c), Chapter 2, Section 2, 
is directly expanded, some of the local variables used in this expansion will reach values 
> 1. The simplest way to get around this is to place the single statement Z 2 +- Z2 + 1 at the 
beginning of this program and then to replace each of the four unconditional branch 
statements GOTO L by the corresponding conditional branch statement IF Z 2 * 0 GOTO L. 

4. The Speedup Theorem Concluded 
437 
Proof. 
Let f be as in Theorem 3.2, and suppose <I>; =f. Then there is 
j EN such that (3.1) and (3.2) hold. Let <l>ix) = f(x) for x > x 0 â¢ Let 
w = (x0 ,[f(O), ... ,f(x0 )]). 
Finally, let j = t(j, w). Then using Theorem 4.1, 
if 
X> X 0 
if 
X :::;; Xo, 
i.e., <l>j =f. Theorem 4.1 also implies that M;(x) =Mix) a.e. Hence, 
using t3.2), we have almost everywhere 
â¢ 
Finally, we are ready to give the speedup theorem for arbitrary complex-
ity measures. 
Theorem 4.3 
(Blum Speedup Theorem). 
Let g(x, y) be any given recur-
sive function and let C be any complexity measure. Then there is a 
recursive function f(x) such that f(x) :::;; x and whenever <I>; = f, there is a 
j such that 
and 
Proof. 
Using the recursive relatedness theorem (Theorem 1.2), there is a 
recursive function r(x, y) such that 
r(x, y) < r(x, y + 1), 
C;(x) :::;; r(x, M;(x)) 
a.e. 
M;(x) :::;; r(x, C;(x)) 
a.e. 
Let 
h(x, y) = L, g(x, z), 
zsy 
so that h is recursive, 
h(x,y) ~ g(x,y), 
and 
h(x, y + 1) ~ h(x, y). 

438 
Chapter 14 Abstract Complexity 
Finally, let 
g(x,y) = r(x,h(x,r(x,y))). 
Now, we apply Theorem 4.2 using g as the given function g. Let f(x) be 
the recursive function obtained, so that f(x) ::; x. Let <I>; =f. Then there 
is a j such that <l>j = f and 
g( x, M/x)) ::; M;(x) 
a.e. 
Hence, we have, almost everywhere, 
r(x,g(x,C/x)))::; r(x,h(x,C/x))) 
::; r(x,h(x,r(x, M/x)))) 
=g(x,M/x)) 
::; M;(x)::; r(x,C;(x)). 
Now, if C;(x) < g(x, Cj(x)) for any value of x, we would have, for that 
value of x, 
Hence, we must have, almost everywhere, 
â¢ 
Exercises 
1. Show that for all i E N there is a j such that <l>j(x) = M;(x) and 
<1>/x) = Mj(x) for all x. Conclude that every function M;(x) has an 
"optimal" program with respect to complexity measure M. 
2. 
Let L be the set of all strings that are syntactically correct Pascal 
programs, and let 
P(x) = {~ if X E L 
otherwise. 
Does the speedup theorem imply that there is no fastest .9 program 
that computes P(x)? Explain. 

15 
Polynomial-Time Computability 
1. Rates of Growth 
In this chapter we will be working with functions f such that f(n) EN for 
all sufficiently large n E N, but which may be undefined or have negative 
values for some finite number of values of n. We refer to such functions 
briefly, and slightly inaccurately, as functions from N to N. These func-
tions f will typically have the additional property 
lim f(n) = oo. 
(1.1) 
n-+oo 
Examples of such functions are n2 , 2n, and llog 2 nJ. It will be important for 
us to understand in what sense we can say that 2n grows faster than n2 and 
that n2 grows faster than Uog 2 nJ. Although in practice, the definitions we 
are about to give are of interest only for functions that satisfy (1.1), our 
definitions will not assume that this is the case. 
Definition. Let f, g be functions from N to N. Then, we say that 
f(n) = O(g(n)) if there are numbers c and n 0 such that f(n) :;;; cg(n) for 
all n ~ n 0 â¢ If these conditions do not hold we say that f(n) =/= O(g(n)). 
If f(n) = O(g{n)) and g(n) = O{f(n)) we say that f and g have the 
same rate of growth. On the other hund, if f(n) = O(g(n)) but g(n) =/= 
O(f(n)), we say that g(n) grows faster than f(n). 
439 

440 
Chapter 15 Polynomial- Time Computability 
An example should help clarify these notions. We have 
n2 = 0(3n 2 -
6n + 5) 
since 
1 
1 
3n2 -
6n + 5 
----------~~-
3- 6/n + 5jn 2 
3 
as n ~ oo, and therefore there is a number n 0 such that for all n ~ n0 , 
nz 
3n2 -
6n + 5 :::;; 1. 
Likewise 3n2 -
6n + 5 = O(n 2 ), so that these two functions have the 
same rate of growth. 
Clearly, it is also true that 3n2 -
6n + 5 = O(n 3 ); however, 
n3 -=1= 0(3n2 -
6n + 5) 
because 
n3 
1 
--=-------- = n . 
~ oo 
3n2 -
6n + 5 
3- 6/n + 5jn 2 
as n ~ oo. Thus, we can say that n3 grows faster than 3n2 -
6n + 5. 
More generally, we can prove 
Theorem 1.1. Let f, g be functions from N to N, and let 
. 
f(n) 
hm -( 
) = {3, 
n->oo g n 
(1.2) 
where {3 is a positive real number. Then f(n) = O(g(n)) and g(n) = 
O(f(n)), so that f and g have the same rate of growth. 
If, on the other hand, 
f(n) 
lim -- = oo, 
n-+oo g(n) 
(1.3) 
then g(n) = O(f(n)) but f(n) -=!= O(g(n)), so that f(n) grows faster than 
g(n). 
Proof. If (1.2) holds, then there is a number n 0 such that for all n ~ n0 , 
f(n) 
g(n) :::;; {3 + 1. 

1. Rates of Growth 
Hence, f(n) = O(g(n)). Since (1.2) implies that 
g(n) 
1 
lim--=-
n->"' f(n) 
{3' 
the same reasoning can be used to show that g(n) = O(f(n)). 
Next, (1.3) implies that 
. 
g(n) 
hm f( ) = 0. 
n---+00 
n 
Therefore, there is a number n 0 such that n ~ n 0 implies 
g(n) 
f(n) ~ 1. 
441 
Hence, g(n) = O(f(n)). If we had also f(n) = O(g(n)), then for numbers 
c, n0 we should have for n ~ n 0 , 
f(n) 
g(n) ~ c; 
on the other hand, (1.3) implies that there is a number n 1 such that n ~ n 1 
implies 
f(n) 
g(n) > c, 
which is a contradiction. 
â¢ 
A polynomial is a function p from N to N that is defined by a formula 
of the form 
(1.4) 
where a0 , a 1, â¢â¢â¢ , a,_ 1 are integers, positive, negative, or zero, while a, is a 
positive integer. In this case the number r is called the degree of the 
polynomial p. The degree of a polynomial determines its rate of growth in 
the following precise sense. 
Theorem 1.2. Let p be a polynomial of degree r. Then p and n' have the 
same rate of growth. Moreover, p grows faster than nm if m < r, and nm 
grows faster than p if m > r. 
Proof. Letting p be as in (1.4), we have 
p(n) 
a0 
a 1 
--=-+--+Â·Â·Â·+a ~a 
n' 
n' 
nr-I 
r 
r 

442 
Chapter 15 Polynomial- Time Computability 
as n ~ oo. Also, 
so that 
p(n) 
--
~00 
nm 
p(n) 
p(n) 
__ = --Â·nr-m 
nm 
n' 
' 
if r > m, 
and 
p(n) 
--~o 
nm 
The result then follows from Theorem 1.1. 
if r < m. 
â¢ 
Next we shall see that exponential functions grow faster than any fixed 
power. 
Theorem 1.3. The function kn, with k > 1, grows faster than any polyno-
mial. 
Proof. It clearly suffices to prove that for any r E N, 
kn 
lim -, = oo. 
n-HXJ n 
One way to obtain this result is to use L'Hospital's rule from calculus; on 
differentiating the numerator and denominator of this fraction r times, a 
fraction is obtained whose numerator approaches infinity and whose de-
nominator is a constant (in fact, r!). To obtain the result directly, we first 
prove the following lemma. 
Lemma. Let g be a function from N to N such that 
g(n + 1) 
!~"" 
g(n) 
= {3 > 1. 
Then g(n) ~ oo as n ~ oo. 
Proof of Lemma. Let y be a number strictly between 1 and {3, for 
example, y = (1 + {3) ;2. Then there is a number n 0 such that n ~ n0 
implies 
Thus, for each m, 
g(n + 1) 
g(n) 
~ 'YÂ· 
g(n 0 + m) ~ yg(n 0 + m -
1) ~ Â·Â·Â· ~ ymg(n 0 ). 
Since y m ~ oo as m ~ oo, the result follows. 
â¢ 

2. P versus NP 
Proof of Theorem 1.3 Concluded. Setting 
we have 
g(n + 1) 
g(n) 
g(n) = kn jn', 
which, by the lemma, gives the result. 
Exercises 
443 
as 
n ~ oo, 
â¢ 
1. Suppose we have a computer that executes 1 million instructions per 
second. 
(a) For each of the following functions f(x), give the length of the 
longest string that can be processed in one hour if f{lwl) instruc-
tions are required to process a string w: f(x) = x; f(x) = x 2 ; 
f(x) = x 4; f(x) = 2x. 
{b) For the same functions, approximately how long would it take to 
process w if lwl = 100? 
2. What is the least x EN such that 10000x 2 :::;; 2x? 
3. For each of the following functions f(x), give a function g{x) such 
that some Turing machine on a two-symbol alphabet can calculate 
f(x) in O(g(lxl)) steps: f(x) = 2x; f(x) = x 2; f(x) = 2x; f(x) = 2<2'>. 
4. (a) Show that if p(n) is defined by (1.4), then p(n) is positive for n 
sufficiently large, so that p is a function from N to N in the 
sense defined at the beginning of this chapter. 
(b) Show that if p(n) is as in (a) with r > 0, then p(n) ~ oo as 
n ~ oo. 
5. Show that n grows faster than llog 2 nJ. 
6. Show that for any k ~ 1 and any polynomials p(x), q(x), there is a 
polynomial r(x) such that q(x) Â· kP<x> = 0(2'<x>). 
2. 
P versus NP 
Computability theory has enabled us to distinguish clearly and precisely 
between problems for which there are algorithms and those for which 

444 
Chapter 15 Polynomial- Time Computability 
there are none. However, there is a great deal of difference between 
solvability "in principle," with which computability theory deals, and solv-
ability "in practice," which is a matter of obtaining an algorithm that can 
be implemented to run using space and time resources likely to be 
available. It has become customary to speak of problems that are solvable, 
not only in principle but also in practice, as tractable; problems that may 
be solvable in principle but are not solvable in practice are then called 
intractable. 
The satisfiability problem, discussed in Chapter 12, is an example that is 
illuminating in this connection and will, in fact, play a central role in this 
chapter. The satisfiability problem is certainly solvable; in Chapter 12, we 
discussed algorithms for testing a given formula in CNF for satisfiability 
based on truth tables, on converting to DNF, on resolution, and on the 
Davis-Putnam rules. However, we cannot claim that the satisfiability 
problem is tractable on the basis of any of these algorithms or, for that 
matter, on the basis of any known algorithm. As we have seen, procedures 
based on truth tables or DNF require a number of steps which is an 
exponential function of the length of the expression representing a given 
formula in CNF. It is because of the rapid growth of the exponential 
function that these procedures can quickly exhaust available resources. 
Procedures based on resolution or on the Davis-Putnam rules can be 
designed that work well on "typical" formulas. However, no one has 
succeeded in designing such a procedure for which it can be proved that 
exponential behavior never arises, and it is widely believed (for reasons 
that will be indicated later) that every possible procedure for the satisfia-
bility problem behaves exponentially in some cases. Thus the satisfiability 
problem is regarded as a prime candidate for intractability, although the 
matter remains far from being settled. 
This association of intractability with the exponential function, coupled 
with the fact (Theorem 1.3) that an exponential function grows faster than 
any polynomial function, suggests that a problem be regarded as tractable 
if there is an algorithm that solves it which requires a number of steps 
bounded by some polynomial in the length of the input. 
To make these ideas precise, we have recourse to the Turing machine 
model of computation as developed in Chapter 6. In particular, we shall 
use the terms configuration and computation as in Chapter 6. 
Definition. A language L on an alphabet A is said to be polynomial-time 
decidable if there is a Turing machine L that accepts L, and a polynomial 
p(n), such that the number of steps in an accepting computation by L 
with input x is ::;; p(lxl). When the alphabet is understood, we write P for 
the class of polynomial-time decidable languages. 

2. P versus NP 
445 
Definition. A total function f on A*, where A is an alphabet, is said to 
be polynomial-time computable if there is a Turing machine L 
that 
computes f, and a polynomial p(n), such that the number of steps in the 
computation by L with input x is :::;; p(lxl). 
With respect to both of these definitions, we note 
1. It suffices that there exist a polynomial p(n) such that the number of 
steps in the computation by L with input x is :::;; p(lxl) for all but a 
finite number of input strings x. For, in such a case, to include the finite 
number of omitted cases as well, we let c be the largest number of 
steps used by L in these cases, and replace p(n) by the polynomial 
p(n) +c. 
2. Using 1 and Theorem 1.2, it suffices that the number of steps be 
O(lxl') for some r E N. 
The discussion leading to these definitions suggests that in analogy with 
Church's thesis, we consider the 
Cook-Karp Thesis. The problem of determining membership of strings 
in a given language L is tractable if and only if L E P. 
The evidence supporting the Cook-Karp thesis is much weaker than 
that supporting Church's thesis. Nevertheless, it has gained wide accep-
tance. Later, we shall discuss some of the reasons for this. 
The following simple result is quite important. 
Theorem 2.1. Let L E P, let f be a polynomial-time computable func-
tion on A*, and let Q = {x E A* I f{x) E L}. Then Q E P. 
Proof. Let L accept L using a number of steps which is O(lxl'), and let 
.!Y compute f(x) in a number of steps which is O(lxls). A Turing machine 
!J1I that accepts Q is easily constructed that, in effect, first runs .!Y on x to 
compute f(x) and then runs Lon f(x) to determine whether f(x) E L. 
Since a Turing machine cannot print more symbols in the course of a 
computation then there are steps in that computation, we have 
lf(x)l:::;; lxl + p(lxl), 
where p(n) = O(ns). 
By Theorem 1.2, it follows that lf{x)l = O(lxls). Hence, the number of 
steps required by !J1I on input x is O(lxlsr). 
â¢ 
Theorem 2.2. Let f, g be polynomial-time computable functions, and let 
h(x) = f(g(x)). Then h is polynomial-time computable. 
Proof. The proof is similar to that of the previous theorem. 
â¢ 

446 
Chapter 15 Polynomial- Time Computability 
It has turned out to be extremely difficult to prove that specific lan-
guages do not belong to P, although there are many likely candidates. An 
important example is the satisfiability problem discussed in Chapter 12. To 
make matters definite, we assume a set of atoms .91' = {a2 , a 2 , â¢â¢â¢ }, where 
subscripts are understood as in Section 1 of Chapter 12. We use the 
symbols 
for the atoms and their negations, simply using concatenation for disjunc-
tion. Finally, we use the symbol 1 to begin a clause. Then, any string on 
the alphabet C ={a, a, I, I} which begins 1 and in which 1 is never 
immediately followed by I, stands for a CNF formula (where in the interest 
of simplicity we are permitting empty and tautologous clauses and repeti-
tions of literals in a clause). Thus the CNF formula 
(p v q v r v s) A (ij v jJ v r v s) A (ij v jJ v r) 
from Chapter 12 could be written as 
Any string in C* which ends 1 or in which 1 is repeated represents a CNF 
formula which contains the empty clause, and hence is unsatisfiable. 
Now, we write SAT for the language consisting of all elements of C* 
that represent satisfiable CNF formulas. In spite of a great deal of 
attention to the question, it is still not known whether SAT E P. The 
starting point of the work on computational complexity that we discuss in 
this chapter is the observation that the situation changes entirely when we 
shift our attention from deterministic to nondeterministic computation. 
Nondeterministically one can discover very rapidly that a formula is 
satisfiable; it is necessary only that the satisfying assignment be "guessed." 
That is, instead of constructing an entire truth table, it suffices to construct 
a single row. To make these ideas precise, we have recourse to nondeter-
ministic Turing machines as discussed in Chapter 6, Section 5. 
Definition. A language L is said to belong to the class NP if there is a 
nondeterministic Turing machine L 
that accepts L, and a polynomial 
p(n), such that for each x E L, there is an accepting computation 
y 1 , y 2 , â¢â¢â¢ , 'Ym by L for x with m :::;; p(lxl). 
We then have readily 
Theorem 2.3. 
P ~ NP. If L E NP, then L is recursive. 

2. P versus NP 
447 
Proof. The first inclusion is obvious, since an ordinary Turing machine is 
a nondeterministic Turing machine. 
For the rest, let L E NP, let L be a nondeterministic Turing machine 
which accepts L, with corresponding polynomial p(n). We set y 1 to be the 
configuration 
Next, by examining the quadruples of L, we find all configurations y 2 
such that y 1 I- y 2 â¢ Continuing in this manner, we determine all possible 
sequences y 1 , y 2 , â¢â¢â¢ , 'Ym with m :;;; p(lxl) such that 
'Y1 I- 'Y2 I- Â· Â·Â· I- 'Ym â¢ 
Then, x E L if and only if at least one of these sequences is an accepting 
computation by L for x. This gives an algorithm for determining whether 
x E L, and so, invoking Church's thesis, we conclude that L is recursive. 
(Methods like those used in Chapter 7 could be used to prove that L is 
recursive without using Church's thesis.) 
â¢ 
In line with our discussion of the satisfiability problem viewed nondeter-
ministically, we can prove 
Theorem 2.4. SATE NP. 
Proof. Without providing all the rather messy details, we indicate how to 
construct a nondeterministic Turing machine L that accepts SAT. 
L will begin by checking that a given input string x E C* really does 
represent a CNF formula. Such a check requires only verifying that x 
begins with the symbol 1 and that no 1 is immediately followed by I. This 
can clearly be accomplished by L in a single pass over x, and therefore it 
can be done in O(lxl) steps. 
The remainder of the computation will involve successive passes over 
the string x in which truth values are assigned to literals, and clauses thus 
satisfied are labeled as being such. When a clause has been satisfied, the 
symbol 1 that introduces it is replaced by ! (so the fact that a clause still 
begins 1 indicates that it has not yet been satisfied). Also, when a literal 
a llil is assigned the value 1, all occurrences of the literal a llil in clauses 
not yet satisfied will be replaced by cpllil (so that they will not be assigned 
the value 1 in a subsequent pass). Likewise, when the literal a llil is 
assigned the value 0, all occurrences of that literal in clauses not yet 
satisfied will be replaced by cpllil. 

448 
Chapter 15 Polynomial- Time Computability 
We will speak of L as being in one of two modes: search or update. 
After verifying that the input string x does represent a CNF formula, L 
enters search mode. In search mode, L 
begins by finding the first 
occurrence of 1 remaining in x, starting from the left. If no 1 remains, 
then the formula has been satisfied and the computation halts. Otherwise, 
L has found an 1 and seeks to satisfy the clause that it heads. L scans the 
clause, moving to the right. When the symbol a or a is encountered, L is 
scanning the first symbol of a literal a I Iii or a 11i1, as the case may be. L 
thus has the opportunity to satisfy the clause by making this literal true, 
assigning a llil the value 1 in the first case and 0 in the second. L 
nondeterministically decides whether to make this assignment. (This is the 
only respect in which L 
behaves nondeterministically.) If L does not 
make the assignment, then it continues its scan. If it reaches the end of the 
clause without having made an assignment, L enters an infinite loop. If L 
does make such an assignment, it enters update mode. 
In update mode, L 
begins by marking the newly assigned literal, 
replacing a by p, or a by p, respectively. L then moves left to the 1 that 
begins the clause and replaces it by !. Finally, L moves to the right end of 
x, and then scans from right to left, checking all literals in subsequent 
clauses to see whether they match the newly assigned literal. This can be 
done by checking each block of Is against the block that follows p (or p). 
For literals that have been made true by the new assignment, the clause 
containing them is marked as satisfied, by replacing the 1 at its head by !. 
For literals that have been made false, the a or a is replaced by cp. When 
the update is complete, L reenters search mode. 
This completes the description of how L 
operates. It remains to 
estimate the number of steps that L requires for a successful computa-
tion. The number of steps between L entering and leaving each of search 
and update mode is clearly O(lxl). Since this will happen no more than lxl 
times, we conclude that the time for the entire computation is O(lxl 2 ) â¢ â¢ 
It is natural to ask whether the inclusion P ~ NP is proper, i.e., whether 
there is a language L such that L E NP - P. As we shall see, using the 
notion of NP-completeness to be defined below, it can be shown that if 
there were such a language, then it would follow that SATE NP- P. 
Unfortunately, this remains an open question. 
Definition.1 Let L, Q be languages. Then we write 
Q ::;P L, 
1 For a general discussion of reducibility, see Chapter 8. 

2. P versus NP 
449 
and say that Q is polynomial-time reducible to L, if there is a 
polynomial-time computable function f such that 
X E Q 
<=> 
f( X) E L. 
Theorem 2.5. Let R ::=;P Q and Q ::;P L. Then R ::=;P L. 
Proof. This follows at once from Theorem 2.2. 
â¢ 
Definition. A language L is called NP-hard if for every Q E NP, we have 
Q ::;P L. L is called NP-complete if L E NP and L is NP-hard. 
The significance of NP-completeness can be appreciated from the fol-
lowing result. 
Theorem 2.6. If there is an NP-complete language L such that L E P, 
then NP = P. 
Proof. We need to show that if Q E NP, then Q E P. Let Q ~A*. Since 
L is NP-hard, there is a polynomial-time computable function f such that 
Q = {x E A* I f(x) E L}. 
The result now follows from Theorem 2.1. 
â¢ 
Intuitively, one can thus think of the NP-complete languages as the 
"hardest" languages in NP. As we shall see in the next section, SAT is 
NP-complete. Thus, if it should turn out that SATE P, then every NP-
complete problem would also be in P. It is considerations like these that 
have led to the tentative conclusion that NP-complete problems should be 
regarded as being intractable. To date, however, although very many 
problems are known to be NP-complete, there is no language known to be 
in NP - P, and it thus remains possible that NP = P. 
Exercises 
1. Show that Theorem 2.1 still holds when Pis replaced by NP. 
2. Show that if 0 c L, M c A* for some alphabet A, and if L, ME P, 
then L ::=;P M. 
3. Show that L ::=;P M does not necessarily imply that M ::=;P L. 
4. Let L, ME P be languages on some alphabet A. Show that each of 
the following languages are in P: A* - L, L n M, L u M. 
5. Let L, ME NP be languages on some alphabet A. Show that each of 
the following languages are in NP: L n M, L u M. 

450 
Chapter 15 Polynomial- Time Computability 
6. Show that every regular language is polynomial-time decidable. [See 
Chapter 9.] 
7. 
Show that every context-free language is polynomial-time decidable. 
[See Chapter 10.] 
8. 
Give a language that is not polynomial-time decidable. 
9. 
Give a function that is not polynomial-time computable. 
10. Let A be an alphabet and set 
co-NP = {L ~A* I A*- L E NP}. 
Show that if there is a language L such that L is NP-complete and 
L E co-NP, then NP = co-NP. 
11. Show that Theorem 2.1 still holds when Pis replaced by NP. 
12. * Let f be a total function on N, and let A be an alphabet. A total 
unary function g(x) on A* is computed in DTIME(f) if it is 
computed by some Turing machine that always runs in 
~ f(lxl) steps 
on input x. A language L ~A* belongs to DTIME (f) if L is 
accepted by some Turing machine that runs in 
~ f(lxl) steps for 
every x E L. L belongs to NTIME (f) if L is accepted by some 
nondeterministic Turing machine that has an accepting computation 
with 
~ f(l xI) steps for every x E L. For languages L, M ~A*, we 
will write L ~ 1 M to indicate that there is a function g computable 
in DTIME(f) such that x E L if and only if g(x) EM. 
(a) Show that P = Un~o DTIME(xn). 
(b) Show that NP = Un~o NTIME(xn). 
(c) 
Prove that if L E DTIME(x 2) and M ~~ L, where f(x) = x, 
then ME DTIME(4x 2 + x). 
(d) Prove that if L E NTIME(x 2) and M ~~ L, where f(x) = x, 
then ME NTIME(4x 2 + x). 
(e) 
Let f(x) = x 2â¢ Give a function g(x) such that if L E 
DTIME(x 2 ) and M ~r L, then ME DTIME(g). 
13. * A language L belongs to EXPTIME if there is a Turing machine L 
that accepts L and a polynomial p(n) such that for every x E L, L 
runs for no more than 2P<Ixl) steps. 
(a) Let ./Y be a nondeterministic Turing machine with k states. For 
a function f(x), what is the maximum number of distinct com-
putations that ./Y can carry out in 
~ f(x) steps? 
(b) Show that NP ~ EXPTIME. [See Exercise 1.6.] 
14.* A language L belongs to PSPACE if there is a Turing machine L 
that accepts L and a polynomial p(n) such that for every x E L, L 

3. Cook's Theorem 
451 
scans at most p(lxD different squares on its tape. L belongs to 
NPSPACE if there is a nondeterministic Turing machine AI that 
accepts L and a polynomial q(n) such that for every x E L, AI has 
some accepting computation in which at most p(lxl) different tape 
squares are scanned. 
(a) Show that PSPACE = NPSPACE. 
(b) Show that NP ~ PSPACE. 
15.* (a) Let L be a Turing machine with states q1 , â¢â¢â¢ , qk and alphabet 
{s1 , â¢â¢â¢ , sn}. How many distinct configurations of L are there 
with m tape squares? 
(b) Show that PSPACE ~ EXPTIME. [Hint: Use the pigeon-hole 
principle. See the discussion in Section 1 of Chapter 14.] 
3. Cook's Theorem 
We now prove the main theorem of this chapter. 
Theorem 3.1 (Cook's Theorem). SAT is NP-complete. 
Proof. Since we know, by Theorem 2.4, that SATE NP, it remains to 
show that SAT is NP-hard. That is, we need to show that if L E NP, then 
L ::;;P SAT. Thus, let L E NP, and let L 
be a nondeterministic Turing 
machine that accepts L, with p(n) the polynomial that furnishes a bound 
on the number of steps L requires to accept an input string. Without loss 
of generality, we assume that p(n) ~ n for all n. We must show that there 
is a polynomial-time computable function that translates any input string 
u for L into a CNF formula 8u such that u is accepted by L if and only if 
8u is satisfiable. For a given input u, let t = p(lul). 
We know that if L accepts input u, it does so in :::;; t steps. Therefore, in 
order to determine whether L accepts u, we need only run it on u for at 
most t steps and check to see whether the final configuration is terminal. 
Since at each step of the computation, L can move at most one square to 
the left or right of the square currently being scanned, it follows that after 
t steps, the scanned square can be at most t squares to the left or t 
squares to the right of its original position. Since we have chosen the 
polynomial p(n) so that t ~ lui, for our present purposes it suffices to 
consider 2t + 1 squares of tape. Thus, since we are considering only t 
steps of the computation, we can completely exhibit all of the information 
on .L's tape, using a t by (2t + 1) array (see Fig. 3.1). 

452 
Chapter 15 Polynomial- Time Computability 
I' 
2t + 1 
l 
Tape at step 1 
Tape at step 2 
. 
l 
. . 
Tape at step t 
Figure3.l 
The first line of this array, corresponding to the initial tape contents, will 
then have the form 
where L 
begins in state q 1 scanning the (t + l)th symbol in this string, 
the s0 immediately preceding u. 
We will find it convenient, in this proof, to use the Turing machine 
model used in Theorem 4.2 in Chapter 6, in which acceptance of an input 
is by arrival in a unique accepting state qm. We assume, therefore, that L 
is a Turing machine of this type. Let the set of states of L 
be Q = 
{q1 , q2 , â¢â¢â¢ , qm} and let the set of tape symbols be S = {s0 , s1 , ..â¢ , sJ It 
will simplify matters if we need to check only configuration number t to 
determine acceptance. Thus, we alter our definition of accepting computa-
tion to permit any number of repetitions of consecutive configurations; 
hence we may assume that our accepting computation consists of exactly t 
steps. 
We will define a CNF formula l>u that is satisfiable if and only if u is 
accepted by L. Our set of atoms (each of length O(t 2)) will be 
.91= {Ph,j,k,ui,j,k 11 ~ h ~ m,O ~ i ~ r,1 ~j ~ 2t + 1,1 ~ k ~ t}. 
We first assume that u is accepted by L, so that we have an accepting 
computation by L for u. We assume that the above t by 2t + 1 array has 
been constructed correspondingly. We will construct the CNF formula l>u 
so that l>:; = 1, where v is the assignment on .91 defined by 
f 01 
v( Ph,j,k) = \ 
v(u .. k)=\ 1 
'Â· ], 
0 
if 
L is in state qh scanning the jth position at the 
kth step of the computation 
otherwise, 
(3.1) 
if tape symbol s; is in the jth position of the kth 
row of the array 
otherwise. 

3. Cook's Theorem 
453 
In constructing 8u, we will find the following abbreviation useful: 
V{xe 11 ~ e ~ /} = 
A (--, Xe V --,X 1) A V Xe, 
l~e<f~l 
l~e~l 
where {xe 11 ~ e ~ /} is a set of formulas. Thus, 
V{xe 11 ~ e ~ I} 
(3.2) 
is a formula whose value is TRUE (i.e., 1) under a given assignment if and 
only if exactly one of the formulas x1 , x 2 , â¢â¢â¢ , x1 has the value TRUE 
under that assignment. In the particular case that x 1 , x 2 , â¢â¢â¢ , x 1 are atoms, 
(3.2) is a CNF formula. We will need to calculate I V{xe 11 ~ e ~ I} I in this 
case. Formula (3.2) contains a clause consisting of two literals for each pair 
(e, f) with 1 ~ e < f ~ I, followed by a single clause of I literals. Since 
there are /{I -
1) /2 such pairs ( e, f), and since in our notation, with 1 
being used to separate clauses, each clause is of length 1 plus the number 
of its literals, we have 
( /{1 -
1) 
) 
IV{xel1~e~l}l= 
2 
Â·3+(/+1) Â·O(t 2 )=0(/ 2t 2 ). 
Let us write 
where 
lui= z. 
We present a sequence of CNF formulas whose conjunction 8u (which is 
then also a CNF formula) may be thought of as simulating the behavior of 
Lin accepting u. Each of these formulas has the value TRUE under the 
assignment v. We precede each formula with an English sentence in 
quotes, which may be thought of as expressing a corresponding property of 
the accepting computation by L for u; each such sentence is intended to 
make it clear that the corresponding formula is indeed true under the 
assignment v. In some cases the formula as written will not be in CNF; in 
these cases the formula written is intended to stand for a formula in CNF 
obtained from it by using the methods of Chapter 12, Section 3. 
(1) "The initial configuration has tape contents corresponding to the 
first row of the array, with L 
in state q1 scanning the symbol s0 
immediately to the left of the first symbol of u." 
A 
Uo,j,l A A uui,t+j+l,l A 
A 
Uo,t+z+j+l,l A Pt,t+t,lÂ· 
O<j~t+ I 
O<j~z 
O<j~t-z 
This expression is clearly of length O(t 3 ). 

454 
Chapter 15 Polynomial- Time Computability 
(2) "At each step of the computation there is a unique state and a 
unique scanned square." 
A V{Ph,j,kl1~h~m,1~j~2t+l}. 
1 ~k~t 
By the preceding remarks, the length of this expression is 0(15). 
(3) "Each entry of the array contains exactly one symbol." 
A 
A 
V{u;,j,k IO ~ i ~ r}. 
l~k~t l~j~21+1 
r is a constant, so that this expression is of length O(t 4 ). 
(4) "Each configuration in the computation, after the first, is identical 
to the preceding configuration, or is obtained from it by applying one of 
the quadruples of L." 
This formula will be the most complicated. Let the quadruples of L be 
as follows: 
{q; s1Â· sk q1 I a= 1,2, ... ,a}, 
a 
a 
a 
u 
(3.3a) 
{ q;b sib R q1b I b = 1, 2, ... , b}, 
(3.3b) 
{q;, si, L q1, I c = 1, 2, ... , c}. 
(3.3c) 
To make the formula easier to understand, we write it in the form 
A 
A 
(NOTHEAD(j, k) VIDENT(j, k) 
I~ k < t I ~j ~ 21 +I 
VA(j, k) V B(j, k) V C(j, k)), 
where each of these five disjuncts will be explained below. It will turn out 
that each disjunct has length O(t 2 ); hence we may conclude that the 
length of the entire formula will be O(t 4). 
We define 
NOTHEAD(j, k) = V (u;,j,k 1\ ui,j,k+l) 1\ A -, Ph,j,k 
O~i~r 
I ~h~m 
so that NOTHEAD(j, k)v = 1 for given j, k if and only if L 
is not 
scanning the jth position at the kth step of the computation. 
Next we set 
IDENT(JÂ·, k) = 
V 
V (p 
1\ u: 
1\ P 
1\ u: 
) 
h,j,k 
i,j,k 
h,j,k+l 
i,j,k+l ' 
l~h~m O~i~r 

3. Cook's Theorem 
455 
so that IDENT(j, k)v = 1 for given j, k if and only if L is scanning the 
jth position at both the kth and the (k + 1)th steps of the computation, 
and both the state and the symbol are the same in both of these configu-
rations. 
Next, 
A(j,k) = V (P;.,j.k 1\ Oj.,j.k 1\ uk.,j,k+l 1\ Pi.,j,k+l), 
lsasii 
where A(j, k)v = 1 if and only if the (k + 1)th step results from the kth 
by one of the quadruples of (3.3a). 
Similarly, we will define B(j, k) so that B(j, k)v = 1 if and only if the 
(k + 1)th step results from the kth by one of the quadruples of (3.3b). For 
j -=1= 2t + 1, we can define 
B(j,k) = V (p;b,j,k 1\ 0jb,j,k 1\ 0jb,j,k+l 1\ Pib,j+l,k+l). 
lsbsb 
This definition will not work for j = 2t + 1 because there are no atoms 
Ph,Zt + 2, k â¢ But since the computation cannot proceed beyond the bound-
aries of our array, it suffices to take B(2t + 1, k) to be any unsatisfiable 
formula, e.g., the empty clause. 
Finally, we will define C(j, k) so that C(j, k)v = 1 if and only if the 
(k + 1)th step results from the kth by one of the quadruples of (3.3c). For 
j -=1= 1, we can define 
C(j, k) = 
V (p. Â· k 1\ UÂ· 
Â· k 1\ UÂ· 
Â· k 
I 1\ Pi 
Â· I k I)Â· 
ledâ¢ 
ledâ¢ 
ledâ¢ + 
col-
â¢ + 
l:SC:SC 
This definition will not work for j = 1 because there are no atoms Ph, o, k. 
But since the computation cannot proceed beyond the boundaries of our 
array, it suffices to let C(l, k) be any unsatisfiable formula, e.g., the empty 
clause. 
(5) "The tth configuration is a terminal configuration." Equivalently, 
"At the tth step, L is in state qm ." 
V 
Pm,j,tâ¢ 
l,;;j,;;2t+l 
This expression is clearly of length O(t 3 ). 
Now, we take 8u to be simply the conjunction of the CNF formulas (1) 
through (5) above. It is clear from what has already been said that if L 
accepts u, then 8u is satisfiable; in fact, 8:; = 1. 

456 
Chapter 15 Polynomial- Time Computability 
Conversely, let v be an assignment such that 8:/ = 1. We will show that 
L accepts u. By (3), we see that for each 1 :::;; j :::;; 2t + 1, 1 :::;; k :::;; t, there 
is a unique i such that v(ui,j,k) = 1. Hence we can uniquely reconstruct 
our t by 2t + 1 array. By (2), for each row of the array there is a unique 
state qh and position j in the row such that v( Ph,j,k) = 1. Thus, each row 
can be made into a configuration of L so that (3.1) is satisfied. By (1), the 
configuration corresponding to the first row of the array is an initial 
configuration for L with input u. By (4), for each row of the array after 
the first, the corresponding configuration is identical to it or results from it 
using one of the quadruples of L. Finally, by (5) the entire sequence of 
configurations constitutes an accepting computation by L for u. Thus, u is 
accepted by .L. 
It remains to be shown that there is a polynomial-time computable 
function that maps each string u onto the corresponding CNF formula 8u . 
Now, the CNF formulas of (2)-(5) do not depend on u, and a Turing 
machine can easily be constructed to write these on a tape in a number of 
steps proportional to the length of the expression, which, as we have seen, 
is O{t 5 ), and hence polynomial in lui. It remains to consider (1), which is a 
conjunction of atoms. Some of these atoms do not depend directly on u; 
producing this part of (1) simply involves writing O(t 3) symbols. The 
remaining atoms of (1) correspond in a one-one manner to the symbols 
making up u; they can obviously be produced by a Turing machine in a 
number of steps proportional to lui. This completes the proof. 
â¢ 
Using Theorem 2.6, we have at once 
Corollary 3.2. P = NP if and only if SAT E P. 
Exercises 
1. 
Let L be the Turing machine with the single tuple q1 B a q2 , and let 
u be the string a. Give 8u for t = 1. 
2. 
For any set .Sit' of atoms, show that the set of all propositional DNF 
formulas over .Sit' that are not tautologies is NP-complete. 
3. For any set .Sit' of atoms, show that the set of all satisfiable proposi-
tional formulas over .Sit' is NP-complete. 
4. The HALF-SAT problem is this: given a propositional CNF formula y, 
determine if there is an assignment v on the atoms in yv = 1 and such 
that av = 1 for exactly half of the atoms a in y. [Hint: Show that 
SAT :::;;P HALF-SAT. Given a CNF formula y, create a new atom a' 
for each atom a in y and add clauses of the form {a, a'}, {a, a'}.] 

4. Other NP-Complete Problems 
457 
4. 
Other NP-Complete Problems 
The principal technique for proving a problem to be NP-complete is given 
by the following result: 
Theorem 4.1. Let Q be an NP-complete problem, and let Q ~P L. Then 
L is NP-hard. 
Proof. 
Let R be any language such that R E NP. Since Q is NP-com-
plete, we have R ~P Q. By Theorem 2.5, R ~P L. Thus, L is NP-hard . â¢ 
Corollary 4.2. Let Q be an NP-complete problem, let L E NP, and let 
Q ~P L. Then L is NP-complete. 
Thus, once it has been shown that a problem is NP-complete, it can be 
used to show that other problems are NP-complete. In this way many 
problems have been shown to be NP-complete. It is this fact that consti-
tutes the main evidence for regarding NP-complete problems as being 
intractable. Since the existence of a polynomial-time algorithm for even a 
single one of these problems would imply that there is a polynomial-time 
algorithm for every one of them, and, since it is argued that it is most 
unlikely that this could be the case without even one of these algorithms 
having been discovered, it is concluded that in all likelihood none of these 
problems have polynomial-time algorithms, and so they should all be 
regarded as intractable. 
We will present a very small sample of this work, showing that a few 
problems are NP-complete. We begin with a restricted form of the satisfi-
ability problem. 
The 3-SAT problem is to determine whether a formula in CNF in which 
no clause contains more than three literals is satisfiable. We show that 
3-SAT is NP-complete by showing that any CNF formula 
~ can be 
transformed in polynomial time to a CNF formula ~ 1 containing at most 
three literals per clause such that ~ is satisfiable if and only if ~ 1 is 
satisfiable. 
Theorem 4.3. 3-SAT is NP-complete. 
Proof. Since 3-SAT is a special case of SAT, and SAT is in NP, it follows 
that 3-SAT is in NP. Let 
k ~ 4, 
(4.1) 
be any one of the clauses of ~ containing more than three literals. Let 
{31 , {32 , â¢â¢â¢ , {3k _ 3 be atoms which do not appear in ~. We construct ~ 1 by 

458 
Chapter 15 Polynomial- Time Computability 
replacing (4.1) by the conjunction 
la1a2 f3Jia3{i1 f3zla4fiz f331 Â·Â·Â· lak_zfik-4f3k-31ak-1ak fik-3 Â· 
It is easy to see that ' is satisfiable if and only if C is satisfiable. 
Moreover, since the length of '' is bounded by a constant times the length 
of ', the transformation can be performed in linear time. 
â¢ 
It is interesting that there are problems which superficially appear to be 
unrelated, but between which we can readily find a polynomial-time 
transformation. Our next example is known as the COMPLETE-SUB-
GRAPH problem. A graph G consists of a finite nonempty set of vertices 
V = {v 1 , â¢â¢â¢ , vn} and a finite set of edges E. Each edge is a pair of vertices. 
The size of the graph is simply the number of vertices it contains. A 
subgraph of a graph G = (V, E) is a graph G' = (V', E') where V' ~ V, 
and E' ~E. A graph G = (V, E) is complete if there is an edge in E 
between every pair of distinct vertices in V. 
The COMPLETE-SUBGRAPH problem is this: given a graph and a 
number k, does the graph have a complete subgraph of size k? 
Theorem 4.4. COMPLETE-SUBGRAPH is NP-complete. 
Proof. We show informally that COMPLETE-SUBGRAPH is in NP. Let 
the number k and a list of the vertices and of the edges of the given graph 
be written on the tape of a Turing machine in any reasonable notation. 
The procedure begins by nondeterministically selecting a vertex and then 
decrementing k. By continuing this process until k has been decremented 
to 0, a list of k vertices is obtained. The procedure then tests [in time 
O(k 2)] whether the graph has a complete subgraph in those vertices. Since 
k :::;; n :::;; length of the string representing G on the tape, 
where G is the given graph, this shows that COMPLETE-SUBGRAPH E 
NP. 
To show that COMPLETE-SUBGRAPH is NP-hard, we show that 
SAT ::;;P COMPLETE-SUBGRAPH. Thus, we must show how to map 
each CNF formula y into a pair consisting of a number k and a graph G 
so that y is satisfiable if and only if G has a complete subgraph of size k. 
If y =I y 1 I y 2 â¢ â¢ â¢ I Yk is a CNF formula, where y 1 , y2 , â¢â¢â¢ , Yk are clauses, 
then we take the number k to be simply the number of clauses in y and 
construct the graph G = (V, E), where 
V = { ( a, i) I a is a literal in y;}, 
E = {((a,i),({3,j))l a =F -,{3 and i =Fj}. 

4. Other NP-Complete Problems 
459 
Thus we have a vertex for each occurrence of each literal in y. Edges join 
pairs of vertices that represent literals in different clauses provided one is 
not the negation of the other. This means that these literals can both be 
assigned the value "TRUE" at the same time. If y is satisfiable, there is 
some way to assign truth values to the atoms so that y evaluates to 
"TRUE." Thus at least one literal of each clause of y must be assigned 
the value "TRUE," and in G there will be an edge connecting each pair of 
"true literals." This means that the nodes of G corresponding to the "true 
literals" of y form a complete subgraph of size k. Conversely, if y contains 
a complete subgraph of size k, then since edges join pairs of literals in 
different clauses that can be true at the same time, there is a way to make 
each clause of y true at the same time. Thus y is satisfiable. Furthermore, 
G can clearly be obtained from y by a polynomial-time computable 
function. 
â¢ 
A clique in a given graph is a maximal complete subgraph of that graph; 
that is, a clique is a complete subgraph of a given graph that is not a 
subgraph of any other complete subgraph of that graph. The MAX-
CLIQUE problem is to find the size of the largest clique in a given graph. 
Of course, in this form, MAX-CLIQUE is not a language but rather a 
function, and so it does not make sense in terms of our definitions to ask 
whether it is in NP. However, since removing a vertex and all edges 
containing it from a complete subgraph yields another complete subgraph, 
we see that any algorithm for the MAX-CLIQUE problem that could 
actually be implemented using reasonable resources could easily be trans-
formed into an equally usable algorithm for the COMPLETE-SUB-
GRAPH problem. Hence, to the extent that NP-completeness can be 
regarded as implying intractability, we are entitled to conclude that MAX-
CLIQUE is likewise intractable. 
We next consider a closely related graph-theoretic problem, known as 
VERTEX-COVER. A set S is a vertex cover for a graph G = (V, E) if 
S ~ V and for every (x, y) E E, either xES or yES. The VERTEX-
COVER problem is to determine for a given graph G and integer k 
whether G has a vertex cover of size k. 
Theorem 4.5. Let G = (V, E) be a graph and let 
E' = {(x,y) I x,y E V,x =I= y, and (x,y) $. E}. 
Let us consider the graph G' = (V, E') (sometimes called the complement 
graph of G). Then S ~ V is the set of vertices of a complete subgraph of 
G if and only if V- S is a vertex cover in G'. 

460 
Chapter 15 Polynomial- Time Computability 
Proof. Let S be the set of vertices of a complete subgraph of G. Then, by 
definition, for any (x,y) E E', either x E V- S or y E V- S. Thus, 
V- Sis a vertex cover of G'. Conversely, if V- Sis a vertex cover of G', 
then for any (x, y) E E', either x E V- S or y E V- S. Thus no edge of 
G' connects two vertices in S. Thus for every u, v E S, u =/= v, we have 
(u, v) E E, and so S is the set of vertices '1f a complete subgraph of G . â¢ 
Corollary 4.6. VERTEX-COVER is NP-complete. 
The SET-COVER problem is to determine for a family of sets a= 
{S1 , S2 , â¢â¢â¢ , Sn}, and number k, whether there exists a subfamily r of a of 
size k, f = {Sm,, Sm 2 , â¢â¢â¢ , Sm)â¢ such that 
Corollary 4.7. SET-COVER is NP-complete. 
Proof. 
Let G = (V,E) be a graph with V= {v 1,v2 , â¢â¢â¢ ,vn}. Fori= 
1, 2, ... , n, Jet 
S; = {Cv;,vj)l(v;,vj) EE} U {Cvj,v)l(v;,vj) EE}. 
Clearly r = {S;,, S;2 , â¢â¢â¢ , S;) is a set cover for a = {S1 , S2 , â¢â¢â¢ , Sn} if and 
only if {v;,, V;2 , â¢â¢â¢ , v;) is a vertex cover for G. 
â¢ 
Many hundreds of NP-complete problems have been identified in quite 
diverse areas. We conclude this section with a few more examples. For 
each we indicate in brackets the nature of some known proof of NP-
hardness. 
1. HAMILTONIAN-CIRCUIT (HC): given a graph G = (V, E) with k 
vertices, determine if there is an ordering v 1 , â¢â¢â¢ , v k of the vertices in 
V such that (v;,V;+ 1) E E, 1:::;; i < k, and (vk,v 1) E E. [VERTEX-
COVER ::;;P HC.] 
2. 3-DIMENSIONAL-MATCHING(3DM): given a setS ~A X B XC, 
where A, B, Care disjoint finite sets each with q elements, determine 
if there is a subset M ~ S with q elements such that for any (a, b, c), 
(a', b', c') EM, a =/=a', b =/= b', and c =/= c'. [3SAT ::;;P 3DM.] 
3. PARTITION: given a set A = {a1 , â¢â¢â¢ , an} of positive integers, deter-
mine if there is a subset S ~A such that LaESa= LaEA-sa. 
[3DM ::;;P PARTITION.] 
4. INTEGER-PROGRAMMING (IP): given a finite set 
X= {(Cx; , ... , x~), z;) 11:::;; i:::;; m}, 

4. Other NP-Complete Problems 
461 
where all xj, Z; are integers, and given a tuple (c 1 , â¢â¢â¢ , en) of integers 
and an integer b, determine if there is a tuple (y1 , â¢â¢â¢ , Yn) of integers 
such that (xL ... , x~) Â· (y1 , â¢â¢â¢ , Yn) :::;; Z;, 1 :::;; i :::;; n, and (c I> â¢â¢â¢ , en) Â· 
(y1 , â¢â¢â¢ , Yn) ~ b. (The dot product of any two n-tuples is defined 
(xl, ... , xn) Â· (y1, ... , Yn) = E?~ 1 X;Â· Y;Â·) [3SAT ::;;P IP.] 
5. QUADRATIC-DIOPHANTINE-EQUATIONS (ODE): given posi-
tive integers a, b, c, determine if there are positive integers x, y such 
that ax 2 + by = c. [3SAT ::;;P ODE.] 
6. STRAIGHTLINE-PROGRAM-INEQUIV ALENCE (SPI): given a 
set of variables {X1 , â¢â¢â¢ , Xn}, two programs .9', ~ each being a 
sequence of assignments of the form 
V ~ IF W = X THEN Y ELSE Z, 
where V, W, X, Y, Z E {X1, ... , Xn}, and given a set of values 
{v 1 , â¢â¢â¢ , vm}, determine if there is an initial state 
{X1 = V; , â¢â¢â¢ , Xn = V; }, 
I 
n 
where each V; E {v 1, ... , vm}, such that .9', ~ end with a different 
value for sam~ variable. [3SAT ::;;P SPI.] 
Exercises 
1. The CHROMATIC-NUMBER problem is to determine for a given 
graph G = (V, E) and integer k whether there is a function f from V 
to {1, 2, ... , k} such that if (x, y) E Â£, then f(x) =/= f(y ). {Intuitively, 
this problem amounts to determining whether or not it is possible to 
"color" the vertices of G using k colors in such a way that no two 
adjacent vertices are colored the same.) Show that CHROMATIC-
NUMBER is NP-complete. [Hint: Show 3-SAT ::;;P CHROMATIC-
NUMBER.] [Further hint: Assume y = jy1jy2 â¢â¢â¢ I'Ym is a CNF 
formula such that no 'Y; contains more than three literals. Assume 
there are n atoms a 1 , a 2 , â¢â¢â¢ , an that appear either negated or 
unnegated in y. Construct a graph G with 3n + m vertices such that 
G is n + 1 colorable if and only if y is satisfiable.] 
2. The 2-COLORABILITY problem is to determine whether a given 
graph can be colored using only two colors. Show that 2-COLORA-
BILITY is in P. 
3. The 2-SAT problem is to determine whether a CNF formula in which 
no clause contains more than two literals is satisfiable. It is known 
that 2-SAT E P. Show why a technique like the one used to show 

462 
Chapter 15 Polynomial- Time Computability 
3-SA T is NP-complete does not work for 2-SAT. Show that 2-SAT is 
in P. 
4. The EXACT-COVER problem is to determine for a finite family of 
sets A = {S1 , S2 , â¢â¢â¢ , Sn} whether there exists a set cover r of A such 
that the elements of r are pairwise disjoint. Show that EXACT-
COVER is NP-complete. [Hint: Show that 
CHROMATIC-NUMBER ~P EXACT-COVER.] 
5. The SUBGRAPH-ISOMORPHISM (SI) problem is, given graphs 
G 1 = (V1 , E 1), G 2 = (V2 , Â£ 2 ), to determine if there is a one-one 
function f from V1 to V2 such that (v;, vj) E Â£ 1 if and only if 
(f(v;),f(vj)) E Â£ 2â¢ Show that SUBGRAPH-ISOMORPHISM is NP-
complete. [Hint: Show COMPLETE-SUBGRAPH ~P Sl.] 
6. The LONGEST-COMMON-SUBSEQUENCE (LCS) problem is, 
given an alphabet A, a set {w 1 , â¢â¢â¢ , wn} of strings on A, and a positive 
integer k, to determine if there is a string y E A* with lyl ~ k such 
that, for 1 ~ i ~ n, W; = x 0y 1x 1y2x 2 â¢â¢â¢ y1x1 and y = y 1 , â¢â¢â¢ , y1 for 
some x 0 , â¢ â¢â¢ , x 1, y 1 , â¢â¢â¢ , y1 E A*. Show that LCS is NP-complete. 
[Hint: Show VERTEX-COVER ~P LCS. Let G = (V, E) be a graph, 
where V = {v 1 , â¢â¢â¢ , vn} and E = {(v;,, vj,), ... , (v;m, vj)}, where 
i1 ~ j 1, 1 ~ I ~ m. For each edge (v;,, vh), create the string 
WI= VI â¢â¢â¢ V;1_ 1V;1+ 1 â¢â¢â¢ VnVI â¢â¢â¢ Vj,_,Vj,+J â¢â¢â¢ Vn, 
and also create the string u = v 1 â¢â¢â¢ vn. Show that G has a vertex 
cover of size k if and only if {w 1 , â¢â¢â¢ , wm, u} has a common subse-
quence of size n - k.] 
7. The TRAVELING-VENDOR (TV) problem is, given a set C = 
{c 1 , â¢â¢â¢ ,cn} of cities, a positive integer distance d(c;,cj) for each 
pair of cities, and a positive integer b, to determine if there is a 
Hamiltonian circuit (c;,, ... , c;) such that 
m-1 
L, d(c; ,c; 
) + d(c; ,c;) ~b. 
J 
J+ 1 
m 
1 
j=l 
Show that TV is NP-complete. [Hint: show HC ~P TV.] 
8. The SUBSET-SUM problem is, given a set {a1 , â¢â¢â¢ , an} of positive 
integers and positive integer b, to determine if there is a subset 
{b1 , â¢â¢â¢ , bm} ~ {a1 , â¢â¢â¢ , an} such that E?'~ 1 b; = b. Show that SUBSET-
SUM is NP-complete. [Hint: Show PARTITION ~P SUBSET-SUM.] 

4. Other NP-Complete Problems 
463 
9. The KNAPSACK problem is, given a set S = {(s1 , v1), â¢â¢â¢ , (sn, vn)} of 
pairs of positive integers, where s; is a size and V; is a value, 
1 ~ i ~ n, and given positive integers b, k, to determine if there is a 
subset A ~ S such that 
L s ~ b and 
L 
v ;;::: k. 
(s,l')EA 
(s,L')EA 
Show that KNAPSACK is NP-complete. [Hint: Show PARTITION 
~P KNAPSACK.] 
10. The MULTIPROCESSOR-SCHEDULING (MS) problem is, given a 
set T = {t1, â¢â¢â¢ , tn} of positive integers (task times) and positive 
integers m (number of processors) and d (deadline), to determine if 
there is a partition of T into disjoint sets T1 , â¢â¢â¢ , Tm such that for 
1 ~ i ~ m, L:, E T t ~ d. Show that MS is NP-complete. [Hint: Show 
PARTITION ~PÂ· MS.] 
11. The RECORD-ALLOCATION (RA) problem is, given a set L = 
{11 , â¢â¢â¢ , In} of positive integers (record lengths) and positive integers t 
(track length) and k (number of tracks), to determine if there is a 
partition of L into disjoint sets L 1 , â¢â¢â¢ , Lk such that for 1 ~ i ~ k, 
L:1E L I~ t. Show that RA is NP-complete. [Hint: Show PARTITION 
~p RA..l 
12. The TASK-SEQUENCING (TS) problem is, given a set 
{(tl ,dl ,pl), ... ,(tn ,dn ,pn)} 
of triples of positive integers (where for 1 ~ i ~ n, t; is the amount of 
time necessary to complete task i, d; is the deadline for task i, and P; 
is the penalty for failing to complete task i by its deadline) and given 
a positive integer b, to determine if there is a sequence (i1 , â¢â¢â¢ , in) of 
tasks such that L; E L P; ~ b, where L ~ {1, ... , n} is the set of late 
tasks, i.e., those ij with L:!~ 1 t; > d; . Show that TS is NP-complete. 
[Hint: Show PARTITION ~P TS.] 
1 


Part 5 
Semantics 


16 
Approximation Orderings 
1. Programming Language Semantics 
In Part 1 of this book we studied various classes of functions, principally 
the class of partially computable functions. In Part 5 we also investigate 
classes of functions but from a different perspective. One of the key results 
from Part 1 is that the partially computable functions can be defined by 
way of any number of substantially different formalisms. Once the equiva-
lence of ..:7 programs, Turing machines, etc., has been demonstrated, it 
becomes clear that the definition of partially computable functions in 
terms of ..:7 programs is an artifact of our particular exposition, and in 
results like Theorem 2.1 of Chapter 4 concerning the HALT predicate, the 
role of ..:7 programs recedes to the formal background. In direct propor-
tion to the accumulation of equivalent formal systems, the class of partially 
computable numeric functions takes on an independent, absolute status, 
and the status of each particular formal system declines. It is fair to say 
that computability is about a certain class of functions, however they are 
defined. 
On the more practical side of computer science, however, the formal 
description of functions has blossomed into the elaborate field of program-
ming languages, where we find thousands of formal systems far richer and 
more complex than any we describe in this book. The differences between 
467 

468 
Chapter 16 Approximation Orderings 
the two fields are entirely appropriate. In the theory of computation, where 
we are interested in the abstract mathematical properties of functions and 
classes of functions, it is appropriate to eliminate all but the most essential 
components of our formal systems. In the areas of computer science that 
support the practice of solving problems, it is appropriate to elaborate a 
wide range of programming languages to support the needs of various 
problem domains, programming styles, and philosophies of language de-
sign. 
When the various programming languages are important in their own 
right, the business of associating a function to each program is more than a 
means to the end of defining a class of functions. It becomes the subject of 
programming language semantics. 
The issue came up already in Chapter 2 when we carefully defined a 
semantics for ..:7 programs, associating a partial function 1./1.9' with each 
program .9'. In the course of defining the semantics of ..:7 programs we 
defined the notion of a computation, which characterizes a mechanical 
process of deriving a numerical output value from an input value. In 
essence we Â·have defined an abstract machine that stores the values of the 
X, Y, and Z variables and updates those values by performing various 
operations as specified by a program. A computation describes the se-
quence of states assumed by the machine in the process of deriving an 
output (if there is one), and the meaning of a program is characterized by 
all of the computations it performs, one for each possible input. This style 
of defining the meaning of programs is called operational semantics be-
cause it depends on the operation of some kind of machine. 
It is clear that programs and computations are very different sorts of 
objects. Computations are dynamic in nature; that is, they describe a 
process that evolves over time. Without the benefit of a semantics to give 
them meaning, programs are simply static sequences of syntactic symbols. 
Now, the goal of creating a semantics is to associate a function with a 
program, and at least from the perspective of set theory, a function is a 
static entity: it is simply a set of ordered pairs. We could argue, then, that 
it is a diversion to interpose the conceptual complication of computations 
between a program and its function. It would be more straightforward to 
define a function, by purely "mathematical" means, directly from the 
syntactic structure of the program. Of course, the concept of "purely 
'mathematical' means" is not precisely defined. In the present context it 
implies, at the very least, an absence of operational detail. Its connotation 
will become clearer as we proceed. This alternative to operational seman-
tics is called denotational semantics. 
The denotational approach might be preferable for its conceptual sim-
plicity, but we do not mean to imply that it is "better" than, or a 

1. Programming Language Semantics 
469 
replacement for, an operational semantics. In a practical setting the two 
are complementary. A denotational semantics can provide a succinct 
description of the meaning of a programming language, abstracted from 
the level of pragmatic details, and an operational semantics approaches 
more closely an actual implementation. In the theoretical area of com-
putability, the operational style is crucial, preceding the denotational style 
both historically and conceptually. It is the mechanical nature of an 
operational semantics that gives sense to the term computable function. If 
computability theory is more about the class of partially computable 
functions than the particular formal systems for defining them, it is just as 
much about the concept of mechanical computation which is embodied in 
the operational semantics of .9' programs, Turing machines, Pascal, LISP, 
etc. 
The exposition given here of semantics, both operational and denota-
tional, has two goals. 
1. It should broaden and deepen the understanding of computable 
functions and computation. 
2. It is an introduction to some of the ideas found in the theoretical 
study of programming languages. 
There are two ways in which we will extend the theory of computable 
functions covered in Part 1. One is to expand the class of data objects that 
are directly covered by the theory. We have accounted for computable 
functions on the natural numbers and computable functions on strings 
over arbitrary finite alphabets, but the typical programming language 
offers a much greater variety of data types like lists, arrays, and in many 
cases, user-defined data types. Now, natural numbers and strings are both 
perfectly appropriate data types for a theory of computable functions 
because of their capacity for encoding more complex structures. We 
showed in Chapter 3, for example, how finite lists of numbers can be 
encoded as a single Godel number. So a theorem like the universality 
theorem in Chapter 4 implicitly tells us that there is a partially computable 
universal function for partially computable functions on lists of numbers. 
However, by explicitly admitting a richer assortment of data types, we can 
bring the theory closer to the actual practice of computation. 
In every model of computation we covered in Part 1, a function com-
puted by some program or machine was considered to be defined for a 
given input just when there was a finite computation for that input. Indeed, 
in Chapter 2 we defined a computation as a finite sequence of instanta-
neous descriptions. When a given input leads to an infinite sequence of 
instantaneous descriptions, we did not consider that sequence a computa-
tion, and so we did not consider the program to be doing any useful work 

470 
Chapter 16 Approximation Orderings 
in this case. However, there are programs whose sole reason for being is 
the work they accomplish while they are running, rather than the output 
they produce at the end of a computation. For example, an operating 
system, the program that organizes the functioning of a computer, pro-
duces little useful output when it is terminated, and, in fact, it is designed 
to be able to run without ever terminating. The termination of an operat-
ing system might just as well indicate a failure rather than the successful 
completion of a computation. 
Our second extension, then, is to try to account for the work done in the 
course of a computation. Our perspective here is that instantaneous 
descriptions represent partial results that approximate the overall result of 
a computation. We will admit the possibility of infinite computations as 
well. In a sense, the result of an infinite computation is the computation 
itself, and each instantaneous description is a piece of the result, approxi-
mating the whole. Our data structures, therefore will come equipped with 
an ordering, an approximation ordering, which formalizes the notion of 
different partial results being more or less complete realizations of the 
total result. 
We will focus here on the equational style of function definition. In 
particular, we will work with equations like 
F(X) = H(G(X), X). 
(1.1) 
There are two kinds of variables in Eq. (1.1). X is intended to denote 
individuals, say, natural numbers, and F, G, and H denote functions. 
There is another important difference in our interpretation of these 
variables. In the equation 2x = x + x, with x ranging over the natural 
numbers, equality holds for all values of x, but the equation x 2 - 4 = 0 
calls for one or more particular values of x which make equality hold. In 
(1.1) X has the first interpretation, and F, G, and H have the second. 
That is, we are looking for functions f, g, and h that make f(X) = 
h(g(X), X) true for all values of X, in which case the assignment off, g, 
and h to variables F, G, and H constitutes a solution to (1.1). In 
equational programming we define functions by writing sets of equations 
to be satisfied. 
Normally equations are understood to be symmetric in their left and 
right sides, but the two sides of (1.1) have an important distinction. IfF is 
assigned a function there might be a number of assignments for G and H 
that solve (1.1), but assigning functions to G and H induces a unique value 
for F. In this sense we can interpret the right side as a function (some-
times called a higher order function) which takes any pair of functions g, h 
assigned to G and H and yields a unique function for F. We will make 

1. Programming Language Semantics 
471 
essential use of such higher order functions in the denotational semantics 
of recursion equations. 
We need to consider the issue of partial functions. In the simple 
equation 
F(X) = G(X), 
(1.2) 
if F and G are interpreted as total functions f and g, then there is no 
ambiguity in the requirement that f(x) = g(x) for all values of x. Suppose, 
though, that f and g are defined 
f(x) =g(x) = {0 d f" d 
un e me 
if X> 0 
if X= 0. 
What should be the meaning of f(O) = g(O)? Previously we have inter-
preted equality to mean 
1. either both sides are defined or both sides are undefined, and 
2. when both sides are defined they have the same value. 
An alternative, which we will now adopt, is to extend the universe of 
objects with a new element that represents the property of being unde-
fined. For instance, we extend N to N .L = N u { .L N}, where 
.L N 
(pronounced "bottom") is different from all natural numbers. Now we can 
define 
L (x) = g .L (x) = { ~ N 
if X> 0 
if X= 0. 
f and g are not total functions, but f .L and g .L are, and f .L (x) = g .L (x) 
for all x EN. 
There are two distinct kinds of elements in N .L: numbers and .LN. We 
can compare them by saying that numbers are completely defined ele-
ments and .L N is the unique completely undefined element. This is a 
simple example of our notion of approximation. In a sense 
.L N is an 
approximation, a very weak one, to any natural number n. The idea is 
clearer, perhaps, in a richer set like Nl , where we can say that ( .L N , .L N , 
.L N) and (3, .L N , 5) both approximate (3, 7, 5), and that (3, .L N , 5) is a 
better approximation than ( .L N , .L N , .L N ). Thus, we can think of a 
sequence like ( . .L N , .L N , .L N ), (3, .L N , 5), (3, 7, 5) as a computation, 
where ( .L N , .L N , .L N) and (3, .L N , 5) are partial results leading to the 
final value (3, 7, 5). 
In this chapter we investigate the mathematical aspects of approxima-
tion orderings and functions defined on them. In the next chapter we apply 
these ideas to the semantics of recursion equations. 

472 
Chapter 16 Approximation Orderings 
2. 
Partial Orders 
For a set D, a binary relation on D is any subset of D X D. If R is a binary 
relation on some set, we generally write a R b to mean (a, b) E R. If R is 
a binary relation on D and E ~ D, then the binary relation {(a, b) E EX 
E I aRb} on E is the restriction of R to E. 
Definition. Let D be a set and !;;;; a binary relation on D. !;;;; is a partial 
ordering of D if it has the properties of 
1. reflexivity: a !;;;; a for all a E D; 
2. antisymmetry: a !;;;; b and b !;;;; a implies a = b for all a, b E D; 
3. transitivity: a !;;;; b and b !;;;; c implies a !;;;; c for all a, b, c E D. 
If !;;;; is a partial ordering of D, then the pair (D, !;;;; ) is a partially ordered 
set, or simply a partial order. We will sometimes write a c b to mean 
a!;;;; band a-=!= b. 
It is easy to find examples of partial orders. (N, .:5; ), where 
.:5; is the 
usual ordering of N, is a partial order. (N _j_ 
, !;;;N ), where !;;;N 
is defined 
.l 
.l 
m !;;;N n if and only if m = ..L N or m = n, 
.l 
is also a partial order. Note that while 5 .:5; 7, for example, it is not true 
that 5 !;;;N 
7. If D is a set, the power set of D, denoted !Jl!(D), is the set 
of all subs~ts of D. For any set D, it is easy to see that (!Jll(D), ~Y"<D>) is a 
partial order, where 
~Y"(D) is the subset relation on the sets in !Jl!(D). 
Also, (D, =v) is a partial order for any set D, where =v is the equality 
relation on the elements of D. Although 
.:5; , !;;;N-' , ~Y"<N>, and =N are 
all partial orderings, they are quite different in structure. 
Definition. A partial ordering !;;;; of a set D is a linear ordering of D if 
for every a, bED, either a !;;;; b orb!;;;; a. (D, !;;;;) is a linearly ordered set, 
or simply a linear order. 
So, for example, (N, .:5;) is a linear order, but (N _j_, !;;;N ), (N, =N) and 
(!Jl!(N), ~9'(n)) are not. 
-' 
We will often find it useful to create new partial orders from given 
partial orders. 
Definition. Let (D 1, !;;;v, ), â¢â¢â¢ , (Dn, !;;;D") be partial orders. Then 
!;;;D,xÂ·. Â·xD", the Cartesian product ordering on D1 X â¢Â·Â· X Dn determined 

2. Partial Orders 
473 
by !;;;D 1 , â¢â¢â¢ , !;;;D" , is defined 
(d1 , â¢â¢â¢ , dn) !;;;D 1x .. Â·xD (e1 , â¢â¢â¢ , en) if and only if d; !;;;D e; for all1:::;; i:::;; n. 
n 
1 
Theorem 2.1. If (D 1 , !;;;D), â¢â¢â¢ , (Dn, !;;;D) are partial orders, then (D 1 X 
Â·Â·Â· X Dn, !;;;D 1 xÂ·. Â·xD) is a partial order. 
Proof. We will write 
!;;;;; for !;;;D 1 xÂ·. Â·xD" and !;;;;; ; for !;;;D;, 1 :::;; i :::;; n. We 
need to show that 
!;;;;; 
is reflexive, symmetric, and transitive. For any 
(dp ... , dn) E D 1 X Â· Â· Â· X Dn, we have d; !;;;;;; d; by the reflexivity of !;;;;; ; , 
1 :::;; i :::;; n, so (d)' ... ' dn) !;;;;; (d)' ... ' dn). If (d)' ... ' dn) !;;;;; (e)' ... ' en) and 
(e1 , â¢â¢â¢ , en) !;;;;; (d1 , â¢â¢â¢ , dn), then, for 1 :::;; i :::;; n, d; !;;;;;; e; and e; !;;;;;; d;, which 
implies d; = e; by the antisymmetry of 
!;;;;; ; , so we have (d1 , â¢â¢â¢ , dn) = 
(e1 , â¢â¢â¢ , en). Finally, if (d1 , â¢â¢â¢ , dn) !;;;;; (ep ... , en) and (e1 , â¢â¢â¢ , en) !;;;;; 
(f1 , â¢â¢â¢ ,fn), then, for 1:::;; i:::;; n, d; !;;;;;; e; and e; !;;;;;; f;, which implies 
d; !;;;;;; /; by the transitivity of !;;;;; ; , so we have (d1 , â¢â¢â¢ , dn) !;;;;; (f1 , â¢â¢â¢ ,fn) . â¢ 
For example, (N1 , !;;;;; ) is a partial order, where 
!;;;;; 
is the Cartesian 
product ordering on N1 determined by !;;;N 
, â¢â¢â¢ , !;;;N 
â¢ 
L 
L 
Definition. Let D and E be sets. A function whose domain is D and 
whose range is a subset of E is a function from D into E. The set of all 
functions from D into E is denoted D ~E. If !;;;;;Â£ is a partial ordering of 
E, then !;;;D .... E , the function space ordering on D ~ E determined by !;;;;;Â£ , 
is defined 
f !;;;D .... E g if and only if f(d) !;;;E g(d)for all d ED. 
We sometimes write f: D ~ E to indicate that fED ~E. 
Theorem 2.2. If D is a set and (E, !;;;E) is a partial order, then (D ~ E, 
!;;;D .... E) is a partial order. 
Proof. We will write 
!;;;;; for !;;;D_,E. Let f, g, h be arbitrary functions in 
D ~ E. For any d E D, f(d) !;;;E f(d) by the reflexivity of !;;;E , so f!;;;;; f. If 
f!;;;;; g and g !;;;;; f, then for any d E D, f(d) !;;;E g(d) and g(d) !;;;E f(d), 
which implies by the antisymmetry of !;;;E that f(d) = g(d) for all d ED, 
i.e., f =g. If f!;;;;; g and g!;;;;; h, then for any dE D, f(d) !;;;E g(d) and 
g(d) !;;;E h(d), which implies by the transitivity of !;;;E that f(d) !;;;E h(d) 
for all d E D, i.e., f!;;;;; h. 
â¢ 
For example, (N1 ~ N j_ , !;;;;; ) is a partial order, where 
!;;;;; 
is the 
function space ordering on N1 ~ N j_ determined by !;;;N 
â¢ 
L 

474 
Chapter 16 Approximation Orderings 
Definition. Let A be a set and let 9J be a function with domain A such 
that 9J(a) is a partial order for all a EA. We will write (Da, !;;;a) for 
9J(a). A 9J-choice function 1 is a function f with domain A such that 
f(a) E Da for all a EA. ch(9J) is the set of all 91-choice functions. The 
9J-choice function ordering !;;;;; ch(.!'*) is defined 
f !;;;ch(.!'*) g if and only if f(a) !;;;a g(a) for all 
a EA. 
For example, let A = {1, 2}, let 9J(i) = (Ni ~ N _]_ , !;;;N,_,N,) for i .= 1, 
2, and let f(i) = u; for i = 1, 2. That is, f(l) is the unary projection 
function, and f(2) is a binary projection function. (Recall that ul and uf 
are defined in Chapter 3.) Then f E ch(9J). 
Theorem 2.3. Let A be a set and 9J(a) a partial order for all a EA. 
Then (ch(9J), !;;;;; ch(.!'*)) is a partial order. 
Proof. The proof is identical to the proof of Theorem 2.2, except that 
instead of a single ordering !;;;;;Â£ , we have a different ordering !;;;;; a for each 
a EA. 
â¢ 
Exercises 
1. Show that (N, I) is a partial order, where mIn is the predicate "m is a 
divisor of n" defined in Chapter 3. [Note that 0 I 0 is true.] Is it linear? 
2. 
Let (D, !;;;0 ) be a partial order, and let 
;;;J0 = {(x,y) ED xDiy !;;;0 x}. 
Show that (D, ;;;J0 ) is a partial order. 
3. Let (D, !;;;0 ) be a partial order, let E ~ D, and let 
!;;;;;Â£ be the 
restriction of !;;;0 to E. 
(a) Show that (Â£, !;;;E) is a partial order. 
(b) Show that if (D, !;;;0 ) is a linear order, then (Â£, ~) is a linear 
order. 
4. 
For which set(s) A is (A, 0) a partial order? 
1 When A is infinite, proving the existence of 9"-choice functions generally requires an 
axiom from set theory known as the axiom of choice. However, our treatment of denotational 
semantics will require only 9"-choice functions with a finite domain, so we need not be 
concerned with this issue. The interested reader should consult any introductory text on set 
theory, e.g., those mentioned in "Suggestions for Further Reading." 

3. Complete Partial Orders 
475 
5. Let (D, !;;;D) be a partial order, and let d 1 , â¢â¢â¢ , dn E D be such that 
d 1 !;;;D d 2 ~ â¢ â¢ â¢ !;;;D dn !;;;D d 1â¢ Show that d 1 = d 2 = Â· Â· Â· = dn. 
6. (a) Show that there are three distinct partial orderings of {0, 1}. 
(b) Show that there are nineteen distinct partial orderings of {0, 1, 2}. 
7. 
Let D ={a, b, c}, !;;;D ={(a, a), (b, b), (c, c), (a, b), (a, c)}, E = {d, e}, 
and !;;;E = {(d,d),(e,e),(d,e)}. 
(a) What is (D X E, !;;;Dx E)? 
(b) What is (D ~ E, !;;;D-+E)? 
8. Let (D, !;;;D) be a partial order, and let 
Show that Cv is transitive and satisfies the property of asymmetry, 
namely, that x Cv y implies y ltv x, for all x, y ED. Is Cv reflex-
ive? 
9. 
Let D, E be finite sets with m, n elements, respectively. 
(a) Show by induction on m that D X E has m Â· n elements. 
(b) Show by induction on m that D ~ E has nm elements. 
10. Give linear orders (D, !;;;D) and(Â£, !;;;E) such that (D X E, !;;;DxE) 
and (D ~ E, !;;;D-+ E) are not linear orders. 
11. Give a linear order (D, !;;;D) with D =I= 0 such that (N X D, !;;;NxD) 
and (N ~ D, !;;;N-+D) are linear orders (where !;;;NxD is determined 
by 
~ , the usual ordering on N, and !;;;D). Is (D X N, !;;;DxN) a 
linear order? (D ~ N, !;;;D-+ N )? 
12. Give distinct functions f, g EN~ ~ N j_ such that f !;;;N" -+ N 
g. 
L 
L 
13. Let 9J(i) = N~ ~ N j_ for all i EN, i =1= 0. Give distinct functions 
f, g E ch(9J) such that f !;;;ch(.91) g. 
3. 
Complete Partial Orders 
We will be particularly interested in partial orders that are rich in certain 
kinds of elements. 
Definition. Let (D, !;;;D) be a partial order, and let E ~D. An element 
e0 E E is the least element of E with respect to 
!;;;D if e0 !;;;D e for all 
e E Â£, and it is the greatest element of E with respect to !;;;D if e !;;;D e0 
for all e E Â£. 

476 
Chapter 16 Approximation Orderings 
If (D, !;;;D) is a partial order and E ~ D, then E can have at most one 
least element: if e, e' are least elements of E, then e !;;;D e' and e' !;;;D e, 
so by antisymmetry e = e'. Similarly, E can have at most one greatest 
element. Therefore, we are justified in speaking about the least element of 
E and the greatest element of E. 
Definition. Let (D, !;;;D) be a partial order, and let E ~D. An element 
dE D is a lower bound of E in (D, ~)if d !;;;D e for all e E E, and.it is 
an upper bound of E in (D, !;;;D) if e !;;;D d for all e E E. Moreover, d is 
the least upper bound of E in (D, !;;;D) if it is the least element with respect 
to !;;;D of the set of all upper bounds of E in (D, !;;;D), and it is the greatest 
lower bound of E in (D, !;;;D) if it is the greatest element with respect to 
!;;;D of the set of all lower bounds of E in (D, !;;;D). If the least upper 
bound of E in (D, !;;;D) exists, it is denoted U(D, [;; o> E. If the greatest 
lower bound of E in (D, !;;;D) exists, it is denoted n(D, [;; ol E. 
Suppose (D, !;;;D) is a partial order and E ~D. Since the set of upper 
bounds of E in (D, !;;;D) can have at most one least element, it follows that 
E can have at most one least upper bound in (D, !;;;D). Similarly, E can 
have at most one greatest lower bound in (D, !;;;D). Note that U(D, [;; o> E, if 
it exists, is not necessarily an element of E, though if it is then it is the 
greatest element of E. A similar observation holds for n(D, [;; ol E. 
In our work on semantics we are interested primarily in least upper 
bounds. We will generally drop the subscript and write UE when it is 
apparent to which partial order we are referring. Occasionally we will write 
UvE. 
Partial orders can differ greatly in the existence of upper and lower 
bounds of their various subsets. Let A = {0, 1, 2} and let !;;;A be the usual 
ordering on {0, 1, 2}. Then every subset of A has one or more upper 
bounds and one least upper bound. For example, 1 and 2 are both upper 
bounds of {0, 1}, and 1 is the least upper bound. Note that 2 = n0 and 
0 = u 0. However, consider (N, ::; ). Every finite subset of N has a 
greatest element, and every nonempty subset of N has a finite set of lower 
bounds, so every nonempty subset of N has a greatest lower bound. Also, 
every nonempty subset of N has a least element, and every finite subset of 
N has a nonempty set of upper bounds, so every finite subset of N has a 
least upper bound. However, an infinite subset of N has no upper bounds. 
Note that nN0 does not exist. (Why?) 
A subset of a partial order can fail to have a least upper bound for one 
of two reasons. Either the set of upper bounds is empty, as in the case of 
an infinite subset of N, or it is nonempty but has no least element. For 
example, let (Q, ::; Q) be the ordered set of the rational numbers. Then 

3. Complete Partial Orders 
477 
A = {q E Q I q 2 < 2} has plenty of upper bounds, but it has no rational 
least upper bound. On the other hand, A has a least upper bound in 
(R, ~a>. the ordered set of the real numbers. In fact, Ua A = fi. For a 
simpler example, consider (D, !;;;D), where D = {a, b, c, d} and 
!;;;D = {(a,a),(b,b),(c,c),(d,d),(a,c),(a,d),(b,c),(b,d)}. 
Here, c and d are both upper bounds of {a, b}, but {c, d} has no least 
element. 
If we have a sequence d 0 , d 1 , d 2 , â¢â¢â¢ that represents a finite or infinite 
computation, where d 0 , d1 , d 2 , â¢ â¢ â¢ are elements in some partial order 
(D, ~ 
), then we want (D, ~) to contain some element d which repre-
sents the result of that computation. The following definition formalizes 
this idea. 
Definition. Let (D, !;;;D) be a partial order. A chain in (D, !;;;D) is a 
nonempty set C ~ D such that c !;;;D c' or c' !;;;D c for every c, c' E C. 
(D, !;;;D) is a complete partial order, or cpo, if 
1. D has a least element with respect to ~ , and 
2. U(D, r;; o> C exists for every chain C in (D, !;;;D). 
The least element in a partial order (D, !;;;D) is generally written ..l v or 
..l , and called the bottom element of (D, !;;;D), or simply bottom of 
(D, !;;;D). Note that if (D, !;;;D) is a partial order, C is a chain in (D, !;;;D), 
and !;;;; c is the restriction of !;;;D to C, then (C, !;;;; c> is a linear order. 
Every nonempty subset of N is a chain in (N, ~ ), since (N, ~) is a 
linear order, and, as we showed previously, no infinite subset of N has a 
least upper bound in (N, ~),so (N, ~)is not a cpo. However, any set can 
be turned into a cpo, in the same way that we turned N into (N _]_ , !;;;N ). 
Let D be a set, let ..l v be some new object not in D, and let D _]_"= 
D U {..l vlÂ· Then !;;;D , defined 
" 
d !;;;D 
e if and only if 
d = ..l v or d = e, 
" 
is 'the flat partial ordering of D _]_ . Every chain in (D _]_ , !;;;D 
) is either 
{ ..l vl, or {d} for some d E D, or { ..l v , d} for some d E D, so 'every chain 
in (D _]_ , !;;;D ) has a least upper bound, and therefore (D _]_ , !;;;D ) is a cpo. 
" 
" 
We call (D _]_ , !;;;D ) the flat cpo on D. For example, (N _]_ , !;;;N ) is the flat 
cpo on N. 
" 
" 
We can generalize this discussion about flat cpos. 
Theorem 3.1. Let (D, !;;;D) be a partial order, and let C be a finite chain 
in (D, ~ 
). Then u C exists and u C E C. 

478 
Chapter 16 Approximation Orderings 
Proof. We argue by induction on the size of finite chains in (D, !;;;D). If 
C = {c}, then obviously UC =c. If C = {cp ... , en+ 1}, then C' = 
{c 1 , â¢â¢â¢ , en} is also a chain, so UC' E C' by the induction hypothesis. Now, 
if cn+l !;;;D UC', then uc = UC' E c. Otherwise, UC' !;;;D cn+lâ¢ since 
C is a chain, SO U C = C n + 1 E C. 
â¢ 
We immediately get 
Corollary 3.2. Let (D, ~;;;;D) be a partial order with a bottom element. If 
every chain in (D, ~;;;;D) is finite, then (D, ~;;;;D) is a cpo. 
Corollary 3.3. Every finite partial order with a bottom element is a .cpo. 
Power sets are another source of cpos. Let D be a set, and let 
g> ~fJIJ(D). Then the union of g>, denoted ug>, is defined 
ug> = {d E D I d E E for some E E g>}. 
It is a basic mathematical fact that u g> exists. 
Theorem 3.4. Let D be a set. Then (fJIJ(D), ~.'JD<D>) is a cpo. 
Proof. We have already noted that (fJIJ(D), ~9'(DJ) is a partial order. For 
any set E EfJIJ(D), 0 ~ E, so 0 is the bottom element of(fJIJ(D), ~9'(Dl). 
Let g> ~fJIJ(D). It is clear that ug> EfJIJ(D), and we claim that ug> = ug>. 
For any E E g>, we have E ~ ug>, so ug> is an upper bound of g> in 
(fJIJ(D), ~9'<D>). Let A EfJIJ(D) be any upper bound of g>. Then for any 
d E ug>, d E E for some E E g>, and E ~A, so d EA. Therefore, 
ug> ~A, which implies ug> = ug>. This argument holds, in particular, 
when g> is a chain, so (fJIJ(D), ~9'(Dl) is a cpo. 
â¢ 
Note that in the proof of Theorem 3.4, we actually showed that every 
subset of fJIJ(D) has a least upper bound. (See Exercise 20 for more on this 
point.) 
The constructions of Section 2 can also be used to construct cpos with a 
richer structure than flat cpos. Let D 1 , â¢â¢â¢ , Dn be sets, and let D = 
D 1 X Â·Â·Â· X Dn. We define the projection functions 
~ 1: D ___.. D 1 , â¢â¢â¢ , 
~ n: D ___.. Dn as follows. For 1 :::;; i :::;; n, 
Note that if D; = N, for all 1 :::;; i :::;; n, then ~ j is the function u'j from 
Chapter 3, where 1 :::;; j :::;; n. We will write (d 1 , â¢â¢â¢ , dnH i instead of 
~ i(d1 , â¢â¢â¢ , dn). If E ~ D, we write E ~ i to denote {e ~ i I e E E}. 

3. Complete Partial Orders 
479 
Theorem 3.5. Let (D, !;;;1 ), ... , (Dn, !;;;;, ) be partial orders, and let E ~ 
D 1 X Â·Â·Â· X Dn. Then UE exists if and only if U(E t 1), ... , U (E t n) 
exist, and if uE exists, then UE = (U(E t 1), ... , u (E t n)). 
Proof. 
We will write 
!;;;;; 
for 
!;;;D,xÂ·. Â·xD â¢â¢ Suppose UE exists, and let 
UE = (e 1 , â¢â¢â¢ ,en). Then e; ED;, 1:::;; i:::;; n, and we claim that e; = 
U(E t i). For 1 :::;; i :::;; n, if e E E t i, then there is some element 
(e1 , â¢.. ,e;_ 1 ,e,ei+ 1 , ... ,en) E E, 
and (el, ... , e;_ p e, ei+ p ... , en) !;;;;; (ep ... , en) implies e !;;;;;; e;, SO e; is an 
upper bound of E t i. Let d be any upper bound of E t i. Then for any 
(e1 , ... , en) E E, ei !;;;i ei for 1 :::;; j :::;; n, j =/= i, and 
e; !;;;;;; d, so 
(e1 , â¢â¢â¢ , d, ... , en) is an upper bound of E. But (e1 , â¢â¢â¢ , en) is the least 
upper bound of E, so (e1 , â¢â¢â¢ , en) !;;;;; (e 1 , â¢â¢â¢ , d, ... , en), and, in particular, 
e; !;;;;;; d, so e; = U(E t i). Therefore, U(E t i) exists, 1 :::;; i :::;; n, and UE 
= (e 1 , â¢â¢â¢ ,en) = (u(E U), ... , U (E t n)). 
Now, suppose U(E t 1), ... , U (E t n) exist. Then ( U(E t 1), ... , 
U(E t n)) is an element of D 1 X Â·Â·Â· X Dn, and we claim that it is UE. If 
(e1, ... , en) E E, then for 1 :::;; i :::;; n, e; E E t i, which implies e; !;;;;;; 
U (E t i), so (e 1 , â¢â¢â¢ , en) !;;;;; (U(E t 1), ... , U(E t n)). Therefore, 
(u(E U), ... , U(E t n)) is an upper bound of E. Let (d1 , â¢â¢â¢ , dn) be any 
upper bound of E. For 1 :::;; i :::;; n, if e E E t i then there is some 
(e1 , â¢.â¢ ,e, ... ,en) E Â£,and (e 1 , ... ,e, ... ,en)!;;;;; (d 1 , â¢â¢â¢ ,dn) implies e !;;;;;; 
d;, so d; is an upper bound of E t i. But then U(E t i) !;;;;;; d;, 1 :::;; i:::;; n, 
which 
implies (U(E t 1), ... , U (E t n))!;;;;; (d 1 , â¢â¢â¢ , dn), 
so 
(U(EU), ... ,U(Etn))= UÂ£. 
â¢ 
Theorem 3.6. If (D1, !;;;1 ), â¢â¢â¢ , (Dn,!;;;;,) are cpos, then (D1 X â¢Â·Â· X Dn, 
!;;;0 , xÂ·. -xo) is a cpo. 
Pr(Joj. 
We will write D for D 1 X Â·â¢Â· X Dn and 
!;;;;; 
for 
!;;;D,xÂ· Â·Â·xD â¢â¢ 
(D, !;;;;;) is a partial order by Theorem 2.1. Let ..l; be the bottom element 
of (D;, !;;;;;;), 1 :::;; i :::;; n. Then (..l 1 , â¢â¢â¢ , ..l n) !;;;;; (d1 , â¢â¢â¢ , dn) for all 
(d1 , ... , dn) E D, so (D, !;;;;; ) has a bottom element. 
Now, let C be a chain in (D, !;;;;; ). We must show that u C exists. For 
1 :::;; i :::;; n, if e;, c; E C t i, then there are e, e' E C such that e; = e t i 
and c; = e' t i. Since C is a chain, either e !;;;;; e' or e' !;;;;; e, which implies 
that either e; !;;;;;; c; or c; !;;;;;; e;. Therefore C t i is a chain in cpo (D;, ~ ), so 
U(C t i) exists, 1 :::;; i:::;; n, and by Theorem 3.5, UC exists. 
â¢ 
We can prove a similar result for function space orderings. If sr ~ D ~ E 
for some sets D, E, then for any d ED we write Y(d) to denote the set 
{f(d) If E .7}. 

480 
Chapter 16 Approximation Orderings 
Theorem 3.7. Let D be a set and (Â£, !;;;E) a partial order, and let 
:T~ D ~E. Then U.<T exists if and only if U(.<T(d)) exists for all dE D, 
and if U.<T exists then ( u.<T)(d) = u (.<T(d)) for all d E D. 
Proof. 
We will write !;;;; for 
!:;;;0 _, E â¢ Suppose U.<T exists, and let d E D. 
Then (U.<T)(d) is an element of E, and we claim (U.<T)(d) = U(ff(d)). 
For any f E .9T, f !;;;; U.<T implies f(d) !;;;E ( U.<T) (d), so ( U.<T) (d) is an 
upper bound of :T(d). Let e be any upper bound of :T(d), and let 
fe: D ~ E be defined 
if X= d 
otherwise. 
Then for any f E .9T, f(x) !;;;E ( U.<T) (x) = fe(x) for x E D such that x -=!= d, 
and f(d) !;;;E e = fe(d), so fe is an upper bound of .'T. But then U.<T!;;;; fe, 
and, in particular, ( U.<T)(d) !;;;E fe(d) = e, so ( U.<T)(d) = U (.<T(d)). 
Now, suppose U(.<T(d)) exists for all dE D. Then the function 
g(d) = U(.<T(d)) for all dE D 
belongs to D ~ E, and we claim that g = U.'T. If f E .9T, then for any 
d E D, f(d) E .<T(d), which implies f(d) !;;;E U (.<T(d)) = g(d), so f!;;;; g. 
Therefore, g is an upper bound of .'T. Let h be any upper bound of .'T. For 
any d ED, if e E .<T(d), then e = f(d) for some f E .9T, and f!;;;; h implies 
e = f(d) !;;;E h(d), so h(d) is an upper bound of :T(d). But then g(d) = 
U(.<T(d)) !;;;E h(d) for all d ED, which implies g!;;;; h, so g = U.'T. 
â¢ 
Theorem 3.8. If D is a set and(Â£, !;;;E) a cpa, then (D ~ E, !;;;0 _,E) is a 
cpa. 
Proof. 
We will write 
!;;;; for 
!;;;0 _,E. (D ~ E, !;;;;) is a partial order by 
Theorem 2.2. Define the constant function ..l0 .... E (d) = ..lEfor all d ED, 
where ..l E is the bottom element of (Â£, !;;;E). Then ..l0 .... E !;;;; f for all 
fED ~ E, so (D ~ E, !;;;;) has a bottom element. 
Now, let :T be a chain in (D ~ E, !;;;; ). Then :T(d) is a chain in (Â£, !;;;E) 
for any d E D, since, for any f(d), g(d) E .<T(d), f!;;;; g implies f(d) !;;;E 
g(d) and g !;;;; f implies g(d) !;;;E f(d). Therefore u (.<T(d)) exists for all 
dE D, since(Â£, !;;;E) is a cpa, and by Theorem 3.7, U.<Texists. 
â¢ 
The proofs of the following two theorems are almost identical to the 
proofs of Theorem 3. 7 and Theorem 3.8. 
Theorem 3.9. Let A be a set, let 9'(a) be a partial order for each a E A, 
and let :T~ ch(D). Then U.<T exists if and only if U(.<T(a)) exists for all 
a E A, and if U.<Texists, then (U.<T)(a) = U(.<T(a)) for all a EA. 

3. Complete Partial Orders 
481 
Theorem 3.10. Let A be a set and let g'(a) be a cpo for each a EA. 
Then (ch{g'), ~h(.'B >) is a cpo. 
The iteration of our operations for constructing partial orders quickly 
gives us partial orders of considerable complexity. Theorems 3.6, 3.8, and 
3.10 tell us that if we start with cpos, we end up with cpos. For example, 
is the cpo of functions that transform binary functions in N~ ~ N .L into 
unary functions in N .L ~ N .L â¢ Functions that operate on other functions 
are sometimes called higher order functions. For example ldv .... v= (D ~D) 
~ (D ~D), defined ldv .... v<f) = f, is an easily described higher order 
function. One way of defining a higher order function F is to give a 
definition of the function F(f) for every function f in the domain of F. 
For example, 
Idv .... v<f) (d) = f(d) for all d ED. 
Note that ldv .... v<f)(d) is to be interpreted as Odv .... v(f)){d). Similarly, 
when we write an expression such as f(g)(h)(d), we mean ((f(g))(h))(d). 
Another example is the composition operator o: (E ~ F) X (D ~ E) ~ 
(D ~F), for some sets D, E, F, where, for any f: E ~ F and g: D ~ E, 
o (f, g) is defined 
o {f, g )(d) = f(g(d)) for all dE D. 
( o(f, g) is usually written f o g.) We will make frequent use of this sort of 
definition in the next chapter. 
One way to show that a partial order (E, !;;;E) is a cpo is to build it up 
explicitly by the constructions we have described. Another is to show that 
it is contained in another partial order (D, !;;;D) known to be a cpo and 
that the least upper bounds of all chains in (E, !;;;E) belong to E. 
Theorem 3.11. Let (D, !;;;D) be a cpo, let E ~ D, and let 
!;;;E be the 
restriction of ~ to E. If 
1. E has a least element with respect to ~ and 
2. Uv C E E for all chains C in (E, !;;;E), 
then (E, ~)is a cpo and UE C = Uv C for all chains C in (E, !;;;E). 
Proof. 
E has a least element by assumption, so we only need to show that 
every chain C in (E, !;;;E) has a least upper bound in (E, ~ 
), i.e., that 

482 
Chapter 16 Approximation Orderings 
UE C exists. If C is a chain in (Â£, !;;;E), then it is also a chain in (D, !;;;D), 
and we know that UD C exists. We claim that UD C is UE C. For any 
c E C, we have c !;;;D UDC, which implies c !;;;E UDC, since c and UD C 
are both in E. Therefore, UD Cis an upper bound of C in(Â£, !;;;Â£).Let e 
be any upper bound of C in (Â£, !;;;E). Then for all c E C, c !;;;E e implies 
c !;;;D e, so e is an upper bound of C in (D, !;;;D). Therefore, UD C !;;;D e, 
which implies UD C !;;;E e since UD C and e are both in Â£, so UD C = 
UEC. 
â¢ 
We give one application of Theorem 3.11 here and another in the next 
section. The following construction gives us a way of turning an arbitrary 
partial order into a cpo. 
Definition. Let (D, !;;;D) be a partial order. A set E ~ D is downward 
closed if for all e E E and all d E D, if d !;;;D e, then d E Â£. E is directed 
if for all c, d E Â£, there is some e E E such that c !;;;D e and d !;;;D e. An 
ideal of (D, !;;;D) is a nonempty, downward closed, directed subset of D. 
The set of all ideals of (D, !;;;D) is denoted id(D, !;;;D). The ordering 
~id(D, ~;;D> is 
~.9'(D) restricted to id(D, !;;;D). 
We will write id(D) when the ordering !;;;D is understood. 
Theorem 3.12. Let (D, !;;;D) be a partial order with a bottom element. 
Then (id(D), ~id(D>) is a cpo. 
Proof. It is easy to check that (id( D), ~ id(D)) is a partial order. An ideal is 
nonempty by definition, so 
_i D E I for any ideal I, since I is downward 
closed. Moreover, { _i D} is an ideal, and { _i D} ~ I for any ideal I, so { _i D} 
is the bottom element of (id(D), ~id(D)). Now, let J 
be a chain in 
(id(D), ~id(D)). It is obvious that id(D) ~g>(D), so by Theorem 3.11 we 
need to show only that U .9'(d)J E id(D), i.e., that UJ is an ideal. J 
is a 
nonempty set of nonempty sets, so UJ is nonempty. Let e E UJ, d E D, 
and d !;;;D e. Then e E I for some I EJ, which implies dE I since I is an 
ideal. Therefore, d E UJ, so UJ is downward closed. Now, if c, d E UJ, 
then c E I1 and d E I2 for some I1 , I2 E J, which implies c, d E I1 U I2 â¢ 
But I1 U I2 E J, since I1 ~ I2 implies I1 U I2 = I2 and I2 ~ I1 implies 
I1 U I2 = I1 â¢ Therefore, there is an e E I1 U I2 such that c !;;;D e and 
d !;;;D e, since I1 U I2 is directed, and I1 U I2 ~ UJ, so e E UJ. So UJ 
is directed, and it is an ideal. 
â¢ 
Let (D, !;;;D) be a partial order. For each e ED, the principal ideal 
generated bye, denoted pid(e), is the set {d E DId !;;;D e}. It is easy to see 
that pid(e) is an ideal of (D, !;;;D). The set of principal ideals of (D, !;;;D), 

3. Complete Partial Orders 
483 
denoted pid(D), is {pid(d) I d ED}, and ~pid(Dl is the restriction of ~.9'(D) 
to pid(D). We can think of the partial order (pid(D), ~pid<D>) as a "copy" 
of (D, !;;;D) in (id(D), ~id<D>), and any chain C in (D, !;;;D) has a "copy" 
{pid(c)l c E C} in (id(D), ~id(Dl). Going from (D, !;;;D) to (id(D), ~id(Dl), 
then, has the effect of guaranteeing the existence of a least upper bound in 
(id(D), ~id(D>) for each ("copy" of a) chain in (D, !;;;D). For this reason, 
(id(D), ~id<D>) is called the ideal completion of (D, !;;;D). For more on this 
subject, see Exercise 19. 
Exercises 
1. Give an example of a partial order that is not a cpo. 
2. Give an example of a cpo in which not every chain has a greatest 
lower bound. 
3. Let w be some object not in N. Give a binary relation !;;; such that 
(N u {w}, ~ u !;;;) is a cpo (where 
~ is the usual ordering of N). 
4. Let (D, !;;;D) be a cpo, let C be a chain in (D, !;;;D), and let d E D. 
Show that if c !;;;D d for all c E C, then u C !;;;D d. 
5. Let (D, !;;;D) be a cpo, and let C1 u C2 be a chain in (D, !;;;D), where 
C1 ,C2 ~D. Show that U(C1 U C2) = U{UC1 , UC2}. 
6. 
Let (D, !;;;D) be a cpo, and let C1 , C2 be chains in (D, !;;;D). 
(a) Show that if for all c 1 E C 1 there is a c 2 E C 2 such that 
cl !;;;D c2, then ucl !;;;D uc2. 
(b) Show that if for all c1 E C1 there is a c2 E C2 such that 
C 1 !;;;D c2, and if for all C2 E C2 there is a c1 E C1 SUCh that 
Cz !;;;D cl' then ucl = UCz. 
7. Let(D, !;;;D),(E, !;;;E)becpos,andletCbeachainin(D XÂ£, !;;;DxÂ£). 
Show that UC = U(C ~ 1 XC ~2). 
8. (a) Let (D, !;;;D), (Â£, !;;;Â£) be partial orders such that the largest 
chain in (D, !;;;D) has m E N elements and the largest chain in 
(E, !;;;Â£) has n E N elements. What is the size of the largest 
chain in (D X E, !;;;DxÂ£)? 
(b) Let (D 1, !;;;D)â¢ â¢â¢. , (Dn, !;;;D) be partial orders such that the 
largest chain in (D;, !;;;D.) has m; E N elements, 1 ~ i ~ n. 
Prove by induction on n that all chains in 
(DI X â¢â¢â¢ X Dn' !;;;D XÂ·. Â·XD ) 
I 
" 
are finite. 

484 
Chapter 16 Approximation Orderings 
9. Let D be a set with m E N elements, and let (E, !;;;E) be a partial 
order in which the largest chain has n E N elements. What is the size 
of the largest chain in (D ~ E, !;;;D_,E)? 
10. Let D be a set and (E, !;;;E) a partial order. Show that (E, !;;;E) is a 
cpa if and only if (D ~ E, !;;;D_,E) is a cpa. 
II. Let (D, !;;;v ), (E, !;;;E) be cpos. Show that 
(<D X E)~ (D X E), !;;;(DXE)->(DXE)) 
is a cpa. 
12. Let D be a set and (E, !;;;E) a cpa. Show that 
(<D ~E)~ (D ~E), !;;;(D->E)->(D->E)) 
is a cpa. 
13. Let D be a set, let E r;;,D, let g>E(D) ={A E.9(D)I E r;;,A}, and 
let 
r;;,Y'E<D> be the restriction of 
r;;,Y'<D> to .9E(D). Show that 
(.9iD), r;;,9'Â£<D>) is a cpa. 
14. For sets D, E, let D ~ E be the set of all partial functions f on D 
p 
such that the range off is a subset of E, and let r;;,D .... E be defined 
as follows: for all f, g ED --; E, f r;;,D 7 E g if and only lf f r;;, g. Show 
that (D ~ E, r;;,D .... E) is a cpa. [Hint: Note that D ~Eisa subset of 
p 
p 
p 
.9(D X E) and r;;,D 7 E is the restriction of r;;,Y'(D xEJ to D --;E.] 
15. (a) Give a partial order (D, !;;;D) and a chain C in (D, !;;;D) such that 
C is not an ideal. 
(b) Give a partial order (D, !;;;D) and an ideal I of (D, !;;;D) such 
that I is not a chain. 
16. Let (D, !;;;v) be a partial order. 
(a) Show that if (D, !;;;D) has a bottom element, and if UE exists for 
every directed set E r;;, D, then (D, !;;;D) is a cpa. 
(b)* Show that if (D, !;;;D) is a cpa, then UE exists for every directed 
set E r;;, D. 
17. * Let ( D, !;;;D ), ( E, !;;;E) be partial orders. The lexicographic ordering on 
D X E, denoted !;;;L<D xE>, is defined 
(d1,e1) !;;;L<DxEJ(d2 ,e2 ) 
ifandonlyif 
d1 !;;;D d 2 or (d1 = d 2 and e1 !;;;E e2 ). 
Show that if (D, !;;;D), (E, !;;;;;Â£)are cpos, then (D X E, !;;;L<DxE>) is a 
cpa. 

3. Complete Partial Orders 
485 
18.* For any sets D, E, a function f: D ~ E is onto if the range off is all 
of E. Let (D, !;;;D), (E, !;;;E) be partial orders. An isomorphism from 
(D, !;;;D) to (E, !;;;E) is a one-one, onto function f: D ~ E such that 
d !;;;D d' if and only if f(d) !;;;E f(d') for all d, d' E D. (D, !;;;D), 
(E, !;;;E) are isomorphic if there is an isomorphism from (D, !;;;D) to 
(E, !;;;E). If f: D ~ E is one-one, then the inverse off, denoted r I' 
is defined r I = {(e, d) E E X D I /(d) = e}. 
(a) Show that iff is an isomorphism from partial order (D, !;;;D) to 
partial order (E, !;;;E), then f- 1 is an isomorphism from (E, !;;;E) 
to (D, !;;;D). 
(b) Let (D, !;;;D), (E, !;;;E) be isomorphic partial orders. Show that 
(D, !;;;D) is a cpa if and only if (E, !;;;E) is a cpa. 
(c) 
Let D be a set with n E N elements and let (E, !;;;E) be a partial 
order. Show that (En, ~Â·) and (D ~ E, !;;;D __.E) are isomorphic. 
(d) Let (D, !;;;D ), â¢â¢â¢ , (Dn, ~ ) be partial orders, let A = {1, ... , n}, 
and let g-(i) = (D;, !;;;D ), "1 ::; i ::; n. Show that (D 1 X Â·â¢Â· X Dn, 
!;;;D 1 xÂ·. Â·x D) and (ch(g-), ~h(Â£11)) are isomorphic. 
19.* Let (D, !;;;D) be a partial order. 
(a) Let I be a principal ideal generated by some d ED. Show that 
I is an ideal of (D, !;;;D). 
(b) Show that (D, !;;;D) and (pid(D), ~ id(D)) are isomorphic. [See 
Exercise 18 for the definition of isofnorphic partial orders.] 
20.* A partial order (D, !;;;D) is a lattice if U{d, e} and n{d, e} exist for 
every d, e ED. It is a complete lattice if uE and nE exist for every 
E~D. 
(a) Give an example of a cpa that is not a lattice. 
(b) Give an example of a lattice that is not a cpa. 
(c) 
Let (D, !;;;D) be a lattice. Show that for every nonempty finite set 
E ~ D, UE and nE exist. 
(d) Show that for any set D, (!Jl!(D), ~Y"(D)) is a complete lattice. 
(e) 
Show that for any set D,(!Jl!(D), 29"(D)) is a complete lattice. 
21.* Let (D, !;;;D) be a partial order with D =F 0. (D, !;;;D) is bounded-
complete if UB exists for every B ~ D that has an upper bound in 
(D, ~). 
(a) Give an example of a cpa that is not bounded-complete. 
(b) Give an example of a bounded-complete partial order that is not 
a cpa. 
(c) 
Show that every bounded-complete partial order has a bottom 
element. [Hint: Consider U0.] 

486 
Chapter 16 Approximation Orderings 
(d) Let (D, !;;;D) be a bounded-complete partial order. Show that for 
every nonempty E ~ D, nE exists. [Hint: Consider the least 
upper bound of U{d ED I d !;;;D e for all e E E}.] 
(e) (D, !;;;D) is well-founded if every nonempty subset of D has a 
least element. Prove that every nonempty well-founded partial 
order is bounded-complete. 
(f) 
Let (D, !;;;D), (E, !;;;E) be bounded-complete cpos. Show that 
(D X E, !;;;DxE) is a bounded-complete cpo. 
(g) 
Let D be a set and (E, !;;;E) a bounded-complete cpo. Show that 
(D ~ E, !;;;D__. E) is a bounded-complete cpo. 
22.* Let (D, !;;;D) be a cpo. An element d ED is compact (sometimes 
called finite) if for every chain C in (D, !;;;D) such that d !;;;D u C, 
there is a c E C such that d !;;;D c. The set of compact elements in 
(D, ~) is denoted K(D). (D, !;;;D) is algebraic if for every dE D, 
there is a chain C ~ K(D) such that d = u C. 
(a) Let (D, !;;;D) be a cpo, let d E K(D), and let C be a chain in 
(D, !;;;D) such that d = u C. Show that d E C. 
. 
(b) Let (D, !;;;D) be a cpo in which every chain is finite. Show that 
(D, !;;;D) is algebraic. 
(c) 
Let D be a set. Show that (.9J(D), ~9'(D)) is an algebraic cpo. 
(d) Give an example of a cpo (D, !;;;D), a compact d E K(D), and an 
infinite chain C in ( D, !;;;D ) such that d = U C. 
(e) 
Give an example of a cpo that is not algebraic. 
{f) 
Show that if (D, !;;;D), (E, !;;;E) are algebraic cpos, then so is 
(D X E, !;;;DxE). 
(g) Let (D, !;;;D) be a partial order. Show that (id(D), ~id(D)) is an 
algebraic cpo and that K(id(D)) = pid(D). 
(h) Show that if (D, !;;;D) is an algebraic cpo, then (D, ~) and 
(id(K(D)), ~id(K(D))) are isomorphic. [See Exercise 18 for the 
definition of isomorphic partial orders.] 
4. 
Continuous Functions 
Consider a computable function f composed with a partially computable 
function g applied to a number n, where g(n) i. How should we under-
stand the composition f(g(n))? One interpretation is that the computation 
of g(n) never terminates, so f never gets a result from g(n) and f(g{n)) 
must be undefined. In fact, this is the treatment of composition given in 
Chapter 3. 

4. Continuous Functions 
487 
Definition. Let (D 1 , 
~;;; 1 ), â¢â¢â¢ , (Dn, ~ ), (E, !;;;;;Â£)be cpos with bottom ele-
ments ..L 01 , ... , ..L o., ..LE â¢ A function f: D 1 X â¢â¢â¢ X Dn ~ E is strict if 
f(d 1 , â¢â¢â¢ , dn) = ..L E for all (d 1 , â¢â¢â¢ , dn) such that d; = ..L 0 
for one or 
more 1 ~ i ~ n. 
' 
Let D1, â¢â¢â¢ , Dn, E be sets, and let ((D) l. , ~;;;;(D,>" ), 1 ~ i ~ n, and 
(E l. , !;;;;;Â£ ) be the flat cpos on D 1 , â¢.â¢ , Dn, E, with bottom elements ..L 0 
, 
1 ~ i ~ n", and 
..L E. For any partial function f on D 1 X Â·Â·Â· X Dn with 
range contained in E, the strict extension fl. : (D 1) l. X Â·Â·Â· X (Dn) l. ~ E l. 
of f is defined. 
fl. (xl ' ... ' xn) 
{
..LE if(x1 , â¢â¢â¢ ,xn)f/=.D1 XÂ·Â·Â·XDn 
= 
..LE if(xpÂ·Â·Â·,xn) E D 1 XÂ·Â·Â· X Dn and f(x 1 , â¢â¢â¢ ,xn)i 
f(xl ' ... ' xn) otherwise. 
Clearly any such fl. is strict. We have not defined what it means for a 
function in N1 ~ N l. , for example, to be computable, but iff is partially 
computable, then it certainly would be reasonable to consider fl. to be 
partially computable. In the next chapter we will use strict functions in 
some situations, but we will not require computable functions to be strict. 
For example, the function g(x) = 3 for all x E N l. is not strict, but it 
certainly should be considered computable. However, some restrictions on 
computable functions appear to be reasonable. 
Consider the elements ( ..L N , ..L N ), ( ..L N , 3), and (7, 3) of N l. X N l. . If 
( ..L N , ..L N) approximates (7, 3) and ( ..L N , 3) better approximates (7, 3), 
then we should expect a function f: N l. X N l. ~ N l. to behave such that 
f( ..L N , ..L N) approximates /(7, 3) and f( ..L N , 3) better approximates /(7, 3). 
That is, if a function gets more information to compute with, it should be 
able to give a more informative result. It makes little sense, for our 
purposes, to consider a function f such that, for example, f( ..L N , ..L N) = 6, 
f( ..L N , 3) = 8, and /(7, 3) = ..L N â¢ Since 6 and 8 are completely defined, 
neither approximates the other, and since 
..L N is completely undefined, 
neither 6 nor 8 approximates ..LN. We formalize this notion as follows. 
Definition. Let (D, ~;;;;0 ) and (E, ~;;;;Â£) be partial orders. A function 
f: D ~ E is monotonic if, for all d, d' E D, d ~;;;;0 d' implies f(d) ~;;;;Â£ 
f(d'). 
It is easy to see that any strict function f: N1 ~ N l. is. monotonic, 
though the reverse is not necessarily true. (See Exercise 1.) Though we will 
not require computable functions to be strict, we will certainly expect them 
to be monotonic. For example, let eq: N l. X N l. ~ N l. be the equality 

488 
Chapter 16 Approximation Orderings 
predicate on N _j_ , that is, 
eq(x,y)={~ 
if X= y 
if X-=/= y, 
where as before 1 represents TRUE and 0 represents FALSE. Then eq is 
not monotonic, since ( ..l N, ..l N) !;;;N xN 
( ..l N, 0) and eq( ..l N, ..l N) = 
1 !;t N 0 = eq( ..l N , 0). Now let <I> _j_ (~, y) be the strict extension of the 
unive~sal function <l>(x, y) defined in Chapter 4, and let n _j_ be the strict 
extension of the function n(x) = 0. Then for all x, y EN, 
HALT(x,y) = eq(O,n_j_ (<I> _j_ (x,y))). 
But we showed in Chapter 4 that HALT(x, y) is not computable, so if n _j_ 
and <I> _j_ are partially computable, then eq certainly is not. 
We will make frequent use of the following simple theorem. First we 
introduce a new piece of notation. Iff is a function with domain D, and 
E ~ D, then f(E) denotes the set {f(e) I e E Â£}. 
Theorem 4.1. Let (D, !;;;D), (Â£, !;;;E) be partial orders and f: D ~ E 
monotonic. If Cis a chain in (D, !;;;D), then f(C) is a chain in (E, ~E). 
Proof. Let f(d 1),f(d2 ) E f(C). Then either d 1 !;;;D d2 , which implies 
f(d 1) !;;;E f(d 2), or d2 !;;;D d1, which implies f(d 2 ) !;;;E f(d 1). 
â¢ 
Ordered sets like (N~ , !;;;N") are fairly simple, since all chains in 
J. 
(N~ , !;;;N") are finite, but when we go on to consider richer structures, we 
J. 
will require a property that is, in general, stronger than mono tonicity. Let 
(D, !;;;D) and(Â£, !;;;E) be cpos, and let C be a chain in (D, !;;;D). C might be 
an infinite set, so that we reach u C by way of an infinite chain of 
approximations. For a function f: D ~ E we would like to be able to 
reach f(UC) by way of the approximations {f(c) IcE C}. 
Definition. Let (D, !;;;D) and (Â£, !;;;E) be partial orders. A function 
f: D ~ E is continuous if, for any chain C in (D, !;;;D) such that u C exists, 
Uf(C) exists and f(UC) = Uf(C). [D ~ E] denotes the set of all contin-
uous functions in D ~E. The continuous function space ordering on 
[D ~ E] determined by(Â£, !;;;E), which is denoted 
!;;;[D~EJ' is the restric-
tion of !;;;D~E to [D ~ E]. 
Note that if (D, !;;;D) is a cpa then we can drop the reference to the 
existence of u C. 

4. Continuous Functions 
489 
Theorem 4.2. Let (D, !:;;;0 ), (E, !;;;;Â£)be partial orders, and let fED ~ E. 
1. If f is continuous then it is monotonic. 
2. If f is monotonic and C is a finite chain in ( D, !:;;;0 ), then u C and 
Uf(C) exist and f(UC) = Uf(C). 
3. If all chains in (D, !:;;;0 ) are finite, then f is continuous if and only if 
it is monotonic. 
Proof. Let f be continuous, and let d 1 , d2 E D be such that d 1 !:;;;0 d 2 â¢ 
Then U{f(d1), f(d 2 )} = f(U{dp d 2}) = f(d 2 ), so f(d 1) !;;;;Â£ f(d 2 ), and 
therefore f is monotonic. 
Now, let f be monotonic, and let C be a finite chain in (D, !:;;;0 ). Then 
f(C) is a finite chain by Theorem 4.1, so UC and Uf(C) both exist by 
Theorem 3.1. We have UC E C by Theorem 3.1, so f(UC) Ef(C), which 
implies f(UC) !;;;;Â£ Uf(C). Also, c !:;;;0 U C for all c E C implies f(c) !;;;;Â£ 
f( U C) for all c E C by the monotonicity of f, so that f( u C) is an upper 
bound of f(C). Therefore Uf(C) !;;;;Â£ f(UC), and we have f(UC) = 
uf(C) by the antisymmetry of !;;;;Â£. 
Finally, part 3 follows immediately from parts 1 and 2. 
â¢ 
Suppose (D, !:;;;0 ), (E, !;;;;Â£) are cpos, f: D ~ E is monotonic, and C is a 
chain in (D, ~).Then UC exists, and f(C) is a chain by Theorem 4.1, so 
.Uf(C) exists. Therefore, in these circumstances we can drop the reference 
to the existence of both u C and Uf( C). The following theorem simplifies 
matters a bit further and suggests a technique for proving continuity that is 
often more convenient than going to the definition. 
Theorem 4.3. Let (D, !:;;;0 ), (E, !;;;;Â£) be cpos, and let f E D ~ E. Then f 
is continuous if and only if: (1) f is monotonic, and (2) f( u C) !;;;;Â£ U f(C) 
for all chains C in ( D, ~ 
). 
Proof. 
Let f be monotonic and let C be a chain in (D, ~).Then for all 
c E C, c !:;;;0 u C implies f(c) !;;;;Â£ f( U C), so f( U C) is an upper bound of 
f(C) and Uf(C) !;;;;Â£ f(UC). Then by assumption (2) we have f(UC) = 
Uf(C), and f is continuous. The other direction follows from Theorem 
u 
â¢ 
It follows from Theorem 4.2 and Exercise 3.8 that in N1 ~ N _j_ 
, for 
instance, monotonicity and continuity are equivalent properties. In gen-
eral, however, they are not. For example, let T: (N _j_ ~ N _j_) ~ N _j_ 
be 
defined 
T(f) = ( 1 
..l_N 
if f(n) -=!= ..l N for all n E N 
otherwise . 

490 
Chapter 16 Approximation Orderings 
That is, T(f) is true just in case f is a total function when its domain and 
range are restricted to N. We will simply say that f is a total function. T is 
monotonic since for any g, h EN_~_~ N _~_,if g !;;;N 
.... N 
h then either g 
is not total and T(g) = ..l N !;;;N 
T(h), or g is totat whi'ch implies that h 
must be total, so that T(g) = 1 ~ T(h). However, T is not continuous. For 
all m EN, let nm: N _j_ ~ N _j_ be the "step function" defined by 
{ 
..l N 
if X = ..l N 
nm(x) = 
0 
if x-=/= ..l N and 0 ~ x ~ m 
..l N 
if X -=/= ..l N and X > m. 
Then {nm I m EN} is a chain, and T(nm) = ..l N for all m EN, so 
U{T(nm) I m EN}= ..lN. But (U{nm I m E N})(x) = 0 for all x EN, so 
T(u{nm I m EN})= 1. 
In the next chapter continuity will play a major role in our treatment of 
computable functions. 
With the help of the following lemmas we can prove a version of 
Theorems 3.7 and 3.8 for continuous functions. 
Exchange Lemma. Let (D, !;;;D) and(Â£, !;;;E) be partial orders, let (F, ~) 
be a cpa, let f: D X E ~ F be monotonic, and let C1 and C2 be chains in 
(D, !;;;D), (E, !;;;E), respectively. Then 
U{U{f(x,y) I y E C2 } I x E C1}, U {U{f(x,y) I x E C1} I y E C2 } 
exist, and they are equal. 
Proof. 
For all c 1 E C1 , {f(c1 , y) I y E C2} is a chain by the monotonicity 
of the unary function f(c 1 , y) and Theorem 4.1, so u{f(cp y) I y E C2} 
exists. Also, if cl 'c'l E cl and cl !;;;D c'l' then for all Cz E Cz' 
f(c 1 , c2 ) !;;;F f(c'1 , c2 ) !;;;F U {f(c;, y) I y E C2}, 
so U {f(c 1 , y) I y E C2} !;;;F U {f(c'1 , y) I y E C2}. Therefore, 
{U{f(x,y) I y E C2} I x E C1} 
is a chain and u { u {f(x, y) I y E C2} I x E C1} exists. A similar argument 
holds for U{ u{f(x, y) I X E Cl} I y E Cz}. 
Let cl E cl. Then for all Cz E Cz, 
f(c 1,c2 ) !;;;F u{f(x,c2)lx E C1} !;;;F u{u{f(x,y)lx E C1}ly E C2}, 
so 

4. Continuous Functions 
491 
But (4.1) is true for all c 1 E C 1, so 
U{U{f(x,y) I y E Cz} I X E c,} !;;;F u {u{f(x,y) I X E c,} I y E Cz}. 
Similarly, 
u{u{f(x,y)lx E c,}ly E Cz} !;;;F u{u{f(x,y)ly E Cz}lx E c,}, 
so the lemma follows by the antisymmetry of !;;;F â¢ 
â¢ 
Let (D, !;;;D),(Â£, !;;;;Â£)be cpos and define apply: ([D ~ E] X D) ~ E by 
apply(/, d) = f(d). 
Then apply is monotonic, since if (f, d 1) !;;;ID .... EJxD (g, d 2), then 
apply(/, d 1) = f(d 1) 
!;;;E g(d 1) 
since f !;;;[D .... EJ g 
!;;;E g(d2 ) 
since d 1 !;;;D d 2 and g is monotonic 
= apply(g,d2 ). 
Now we can prove 
Theorem 4.4. If (D, !;;;D) and(Â£,!;;;;Â£) are cpos, then ([D ~ E], !;;;[D->EJ) 
is a cpa. Moreover, if :F is a chain in ([D ~ E], !;;;[D->EJ), then (u:F)(d) 
= U(:T(d)) for all dE D. 
Proof. The bottom element ..l D .... E defined in the proof of Theorem 3.8 
is continuous, so [D ~ E] has a bottom element. By Theorem 3.11, we 
only need to show that for all chains :F in ([ D ~ E ], !:;;;1 D .... EJ ), u D .... E :FE 
[D ~ E], that is, the least upper bound of :Fin (D ~ E, !;;;D .... E), which we 
know to exist by Theorem 3.8, is continuous. Let :F be a chain in 
([D ~ E], !;;;[D->EJ) and let C be a chain in (D, !;;;D). Then 
(UD_,E!F)(UC) 
= U{f(UC)I/E!F} 
byTheorem3.7 
= U{U{f(c)lc E C}IJE!F} 
since each f E :F is continuous 
= u { u {apply(/, c) I c E C} If E :F} 
= u { u {apply(/, c) I f E !F} I c E C} 
by the exchange lemma 
= u{u{f(c)I/E!F}Ic E C} 
= U{(UD_,E!F)(c)lc E C} 
= U((UD_,E!F)(C)), 
by Theorem 3. 7 
so UD_, E:Fis continuous, and ([D ~ E], !;;;[D->EJ) is a cpa. 
â¢ 

492 
Chapter 16 Approximation Orderings 
We conclude this section with a result that will be applied in the next 
chapter. (Also, see Exercise 5.10.) The proof is very similar to the proof of 
the exchange lemma, so we leave it to Exercise 13. 
Diagonal Lemma. Let ( D, i;;;D ) be a partial order and ( E, i;;;E) a cpo, let 
f: D X D ~ E be monotonic, and let C be a chain in (D, ~:;;;D). Then 
u{u{f(x,y)ly E C}lx E C} = u{f(x,x)lx E C}. 
Exercises 
1. Let (D 1 , I;;;D, ), â¢â¢. , (Dn, I;;;D" ), (Â£, !;;;Â£) be flat cpos. 
(a) Show that every strict function in D 1 X Â·Â·Â· X Dn ~ E is mono-
tonic. 
(b), Give an example of a monotonic function in D 1 X ... X Dn ~ E 
that is not strict. 
(c) 
Give a partial order (D, i;;;D) and a strict function in D ~ D 
that is not monotonic. 
2. 
Let f: N l ~ N j_ satisfy 
where h: N';'_ ~ N j_ and g;: Nl ~ N j_ 
, 1 ~ i ~ n, are strict. Show 
that f is strict. 
3. 
Let (D, ~;;;D),(Â£,~:;;;Â£) be partial orders, let e E Â£,and let fe: D ~ E 
be the constant function f/d) = e. Show that fe is continuous. 
4. 
Let (D, i;;;D) be a partial order, and let IdD: D ~ D be the identity 
function IdD(d) =d. Show that IdD is continuous. 
5. 
Let (D 1 , i;;;D, ), ..â¢ , (Dn, i;;;D") be partial orders. Show that for 1 ~ i ~ 
n, the projection function ~ i: D 1 X Â·Â·Â· X Dn ~ D; is continuous. 
6. 
Let D, E be sets, let fED ~ E, and define /: /Ji!(D) ~/Ji!(Â£) as 
/(A) = {f(a) I a E A}. Show that /is continuous. 
7. 
Let (D, i;;;D ), (Â£, ~:;;;Â£ ), (F, i;;;F) be cpos, and let 
o: [Â£ ~ F] X [D ~ E] ~ (D ~F) 
be the composition operator on continuous functions. Show that the 
composition of continuous functions is a continuous function. That is, 
for all f E [Â£ ~ F], g E [D ~ E], show that f o g E [D ~ F]. 

4. Continuous Functions 
493 
8. 
Let (D, i;;;D ), (E1 , i;;;Â£1 ), â¢â¢â¢ , (En, i;;;E ) be cpos, and define the func-
tion construct: [D ~ E 1] X ... X [D ~En] ~ (D ~ (E1 X Â·Â·Â· X 
E)) as 
Let J: E [D ~ E;], 1 :o:; i :o:; n. Show that construct(f1 , ... ,f) E 
[D ~ (E1 X ... X En)]. 
9. 
Let (D, i;;;D ), (E, i;;;E) be cpos, and let applyn: [D ~ E] ~ (Dn ~ En) 
be defined as applyn(f)(dp ... ,dn) = (f(d 1), ... ,f(dn)). 
(a) Let f E [D ~ E]. Show that apply/f) E [Dn ~ P]. 
(b) Show that applyn E [[D ~ E] ~ [Dn ~ P]], i.e., applyn is con-
tinuous. 
10. Let (D, i;;;D ), (E, i;;;E ), (F, i;;;F) be cpos and define curry: [D X E ~ 
F] ~ (D ~ (E ~ F)) as curry(f)(d)(e) = f(d, e). Let f E [D X 
E ~ F] and d E D. 
(a) Show that curry(f)(d) E [E ~ F]. 
(b) Show that curry(/) E [D ~ [E ~ F]]. 
(c) 
Show that curry E [[D X E ~ F] ~ [D ~ [E ~ F]]]. 
11. Let (D, ~:;;;D), (E, ~:;;;Â£) be cpos, let D ---; E be the set of all strict 
functions in D ~ E, and let i;;;D ~ E be the restriction of i;;;D ~ E to 
D ~ E. Show that (D ---; , ~:;;; D ~ ;) is a cpo. 
s 
~ 
12. Let ( D, i;;;D ), ( E, i;;;E) be cpos, let D -;;: E be the set of all monotonic 
functions in D ~ E, and let i;;;D ~ E be the restriction of i;;;D ~ E to 
D -;;: E. Show that (L -;;: E, ~:;;;D ~ ~) is a cpo. 
13. Prove the diagonal lemma. 
14.* Let (D, !;;;D), (E, !;;;Â£)be isomorphic cpos, and let f be an isomor-
phism from (D, i;;;D) to (E, i;;;E ). Show that f is continuous. [See 
Exercise 3.18 for the definition of isomorphic partial orders.] 
15.* Let (Dp i;;;D 1 ), ... ,(Dn, ~;;;D), (E, !;;;Â£)be partial orders and let fE 
D 1 X ... X Dn ~E. We say that f is monotonic (continuous) in the 
ith position, 1 :o:; i :o:; n, if for all dj E Dj, 1 :o:; j :o:; n and j =F i, the 
unary function f(d 1 , â¢â¢â¢ ,d;_ 1 ,x,d;+ 1 , â¢â¢â¢ ,dn) is monotonic (respec-
tively, continuous). 
(a) Let (D, i;;;D ), (E, i;;;E ), (F, i;;;F) be cpos, let f: D X E ~ F be 
monotonic, and let C be a chain in (D X E, i;;;Dx E). Show that 
U{U{f(x, y) I x E C U} I y E C ~2} and Uf(C) exist, and that 
u{u{f(x,y) I x E C ~ 1} I y E C ~2} = Uf(C). 

494 
Chapter 16 Approximation Orderings 
(b) Let (D, !;;;D), (Â£, !;;;E), (F, !;;;F) be cpos, and let f E D X E ~ F. 
Use part (a) to show that f is continuous if and only if f is 
continuous in the first and second positions. 
(c) 
Let (D, !;;;D) and (Â£, !;;;E) be cpos. Show that the function 
apply: ([D ~ E] X D) ~ E is continuous. 
(d) Show that o, defined in Exercise 7, is continuous. 
(e) 
Generalize part (b) so that it applies to n-ary functions, n ~ 1. 
(f) 
Show that construct, defined in Exercise 8, is continuous. 
16.* A Scott domain is a bounded-complete algebraic cpa. [See Exercises 
3.21 and 3.22 for the definitions of bounded-complete and algebraic 
cpos.] 
(a) Show that if (D, !;;;D), (Â£, !;;;E) are Scott domains, then so is 
(D X E, !;;;DXE). 
(b) 
(c) 
Show that (N _]_ ~ N _]_, !;;;N 
.... N ) is a Scott domain. 
" 
" 
Show that if (D, !;;;D), (E, !;;;E) are Scott domains, then so is 
([D ~ E], !;;; 1D .... EJ). [Hint: For each dE K(D), e E K(Â£), de-
fine (d ~ e): D ~ E as 
(d ~e)(x) = {~E 
if d !;;;D X 
otherwise. 
Show that every function (d ~ e) is continuous and compact. 
Then show that for every d 1 , ... , dn E K(D) and e1 , ... , en E 
K(E), the function (d1 ~ e1) U Â·Â·Â· U (dn ~en) is continuous 
and compact. Use these functions to show that ([D ~ E], 
!;;;;; (D .... EJ) is algebraic.] 
(d) Give an algebraic cpo (D, !;;;D) such that ([D ~ D], !;;; 1D .... DJ) is 
not algebraic. 
5. 
Fixed Points 
We will now prove a fundamental theorem that will facilitate our work on 
denotational semantics. 
Definition. Let (D, !;;;D) be a partial order, and let fED ~D. An 
element d ED is a fixed point of f if f(d) = d, and it is the least fixed 
point off in (D, !;;;D) if d !;;;D e for every fixed point e ED of f. The least 
fixed point off in (D, !;;;D), if it exists, is denoted IL(D, r;;;; 0 /. 
We will generally omit the subscript and write 11-f when the partial 
order (D, !;;;D) is understood. 

5. Fixed Points 
495 
Fixed points play a fundamental role in denotational semantics, and 
continuity is important because it allows us to guarantee the existence of 
least fixed points. Before we can prove the fixed point theorem, we need 
some notation and a lemma. Let D be a set and let fED ---+D. For each 
n EN we define a function fn: D ---+ D, called the nth iteration of f, as 
follows: 
fo(x) = x 
r+ 1(x) = f(r(x)). 
Note that these equations can also be understood as defining a single 
binary function JY(x): D X N ---+ D. 
Lemma 1. Let (D, !;;;D) be a partial order with bottom element ..l D , and 
let f: D ---+ D be monotonic. Then r< ..l D) !;;;D r + 1( ..l D) for all n E N, 
and <r< ..l D) In E N} is a chain in (D, !;;;D). 
Proof. 
First we argue by induction on n that r<..L D) !;;;D r+m(..l D) for 
all n, m EN. If n = 0, then / 0( ..l D) = ..l D !;;;D fm( ..l D), so assume 
r<..L D) !;;;D r+m(..L D) for all m EN. Then for any m EN, 
r+ I( ..l D)= f(r( ..l D)) 
!;;;D f(r +m( ..l D)) 
by the induction hypothesis and 
the monotonicity of f 
The lemma now follows immediately. 
â¢ 
Theorem 5.1 
(Fixed Point Theorem for cpos). Let (D, !;;;D) be a cpo, 
and let f: D ---+ D be continuous. Then the least fixed point J.t.f exists, and 
J.l.f = u{r(..l D) In EN}. 
Proof. 
By Lemma 1, <r< ..l D) In E N} is a chain, so u <r< ..l D) In E N} 
exists. Moreover, u <r< ..l D) In E N} is a fixed point off, since 
f(u{r(..LD)In EN}) 
= Uf({r( ..l D) In EN}) 
by the continuity of f 
u{fn+l(..l D) In EN} 
u(<r+ 1(..LD)In EN} u {f0(..LD)}) 

496 
Chapter 16 Approximation Orderings 
Finally, if e is any fixed point of J, then we argue by induction on n that 
r<l_ o> !;;;D e for all n EN. If n = 0 then / 0(1_ o> = j_ D !:o e, so as-
sume r< j_ o> !;;;D e. Then 
r+l(l_o)= f(r(l_D)) 
!:;;;0 f(e) 
by the induction hypothesis and 
the monotonicity of f 
= e 
since e is a fixed point of J, 
so e is an upper bound of {r( 1_ 0 ) I n E N}. Therefore, 
u{r{l_ 0)ln EN} !:;;;0 e, 
and u {r( 1_ 0 ) I n E N} is the least fixed point of f. 
â¢ 
The fixed point theorem has a variety of applications. One, as we will 
show in the next chapter, is to justify recursive definitions of functions. 
Another is to justify inductive definitions of sets. Consider, for example, 
the definition of propositional formulas in Chapter 12. For simplicity we 
let .91', the set of atoms, be {p, q}, and we consider only formulas with the 
connectives ..., and :::) , so that the alphabet B = {p, q, ..,, :::) , (, )}. An 
alternative statement of the definition of propositional formulas over .91' is 
the following: the set of propositional formulas over .91' is the smallest (with 
respect to ~) subset of B* that 
1. contains .91', 
2. is closed under the operation that transforms a to ..., a, and 
3. is closed under the operation that transforms a and {3 to (a :::) {3 ). 
In other words, the set of propositional formulas over .91' is the smallest set 
X ~ B* that satisfies 
.91' U {...,a I a EX} U {(a:::) {3) I a, {3 EX} ~X. 
(5.1) 
Moreover, since (5.1) would still be satisfied if any element of X not 
required by 1, 2, or 3 were removed, and since we are looking for the 
smallest X which satisfies (5.1), we can rewrite (5.1) as the equality 
X =.91' U {...,a I a EX} U {(a:::) {3) I a, {3 EX}. 
(5.2) 
One way of looking at this equation is to consider the right side as a 
function <1>: 9'{B*) ---+ 9'{B*) that takes subsets Z ~ B* and transforms 
them to 
<I>(Z) =.91' U {...,a I a E Z} U {(a:::) {3) I a, {3 E Z}. 
A solution to (5.2), then, is some X such that, when <I> is applied to X, the 
result is still X; that is, X is a fixed point of <1>. For example, let Z be 

5. Fixed Points 
497 
some arbitrary fixed point of <1>. Then 
Z = <I>(Z) =~ U {-,a I a E Z} U {(a:::) {3) I a, {3 E Z}, 
so Z is a solution to (5.2). 
Now, the definition calls for the smallest such set, i.e., JL<I>, so for the 
definition to make sense we need to know that JL<I> exists. This is where 
the fixed point theorem is useful. We have already shown that the partial 
order (!Jl!(D), ~9"<D>) is a cpo for any set D, so (9'(8*), ~9"<8 .>) is a cpo. 
If <I> is continuous then JL<I> exists by the fixed point theorem. Let '(? be a 
chain of subsets of 8*. Then U'(f = U'(f in (9'(8*), ~9"(8 .>), and 
<I>(U'(f) =~ U {-,a I a E U'lf} U {(a:::) {3) I a, {3 E U'(f} 
=~ U ( U {{-,a I a E C} ICE 'lf}) 
U ( U {{(a :::) {3) I a, {3 E C} I C E 'lf}) 
U {~ U {-,a I a E C} U {(a:::) {3) I a, {3 E C} ICE '(f} 
U{<I>(C) IcE '(f} 
u <I>( '(f)' 
so <I> is continuous, and JL<I> exists. 
Note that, although the preceding definition of the set of propositional 
formulas mentions the operations that transform a to -, a and a, {3 to 
(a :::) {3 ), no mention is made of a process of building up the set from ~ 
by repeated application of these operations. The set we define is simply a 
certain solution to a certain equation. On the other hand, the definition 
given in Chapter 12 does mention repeated applications of these opera-
tions. The fixed point theorem, which not only tells us that JL<I> exists, but 
also that JL<I> = u{<l>;(0) I i EN}, makes the connection between these 
two versions of the definition. <1>(0), <1> 2(0), ... , are subsets of JL<I> built 
up by ever more applications of the formula building operations. 
Another way of formalizing the notion of "repeated applications" is to 
give a context-free grammar 2 r such that L(f) is the set of propositional 
formulas over ~. In particular, let f consist of the productions 
s~.s 
s~p 
S ~ (S :::) S) 
S ~ q, 
2 The reader who is unfamiliar with Chapter 10 may skip to the definition of admissible 
predicates. 

498 
Chapter 16 Approximation Orderings 
where S is the start symbol. By definition, L(f) = {u E B* IS ~ u}. Sup-
pose a, {3 E L(f). Then S ~ a and S ~ {3, which implies 
S = (S :::> S) ~ (a:::> S) ~ (a:::> {3), 
so (a :::> {3) E L(f). Similarly, if a E L(f), then S = ..., S ~ ..., a and 
..., a E L(f). 
It seems, then, that L(f) is the set of propositional formulas over .w', but 
how can we prove it? By Theorem 1.4 of Chapter 10, a E L(f) if and only 
if there is a derivation tree for a in f. We define the height of a derivation 
tree g; denoted h(.9), as follows. If ::T consists of exactly one vertex, the 
root, then h(.9) = 1. If ::T consists of a root with successors v1 , â¢â¢â¢ , vn, 
then 
h(::T) = max{h(!:T"'), ... , h(!:T"")} + 1, 
where ::T"; is the subtree of ::T with root v;, 1 ~ i ~ n. For each n E N, we 
define 
Ln = {u E B* I there is a derivation tree ::Tfor u in r with h(::T) ~ n + 1}. 
Clearly, L(f) = Un EN Ln. If we can show that cl>n(0) = Ln for all 
n E N, then we will have 
JLcl> = un EN cl>n(0) = un EN Ln = L(f). 
If n = 0 then <1> 0(0) = 0 = L 0 , since the only derivation tree of height 1 
with root S does not yield a word in B*. For n > 0 we argue by induction 
on n. If n = 1 then <1> 1(0) = {p, q} = L 1 â¢ For n + 2 we have 
cl>n+2(0) = cl>(cl>n+l(0)) 
= ci>(Ln+ 1) 
by the induction hypothesis 
= {p,q} U {-,a I a E Ln+l} U {(a:::> {3)1 a,{3 E Ln+l}. 
Also, it is clear from the definition of Ln + 2 and the nature of f that 
Ln+Z = Ln+! U {...,a I a E Ln+l} U {(a:::> {3) I a, {3 E Ln+l}. 
Since {p,q} ~Ln+lâ¢ we have cl>n+ 2(0) ~Ln+ZÂ· On the other hand, by 
Lemma 1 we have Ln+ 1 = ci>n+ 1(0) ~ cl>n+ 2(0), which implies Ln+Z ~ 
ci>n+ 2(0), and so we have cl>n+ 2(0) = Ln+Zâ¢ completing the induction and 
the proof that JL<I> = L(f). 
These various treatments of the definition of propositional formulas 
help to illustrate some of the ideas in the next two chapters. On the one 
hand, we have an abstract mathematical characterization of the set of 
propositional formulas as the smallest solution to equation (5.2). On the 
other hand, we have L(f) = {u E B* IS ~ u}, the set of words generated 

5. Fixed Points 
499 
from S by derivations in f. We can give a "deterministic" characterization 
of L(f) as the set of words in B* for which there exists a leftmost 
derivation from the start symbol. A somewhat more abstract characteriza-
tion of the same set is Un eN Ln, which is given in terms of derivation 
trees, without any reference to the details of the choices made in the 
construction of derivation sequences. In the terminology of semantics, #Let> 
is a denotational definition, and L(f) is an operational definition. The link 
between JLcl> and L(f) is given by the fixed point theorem and its 
characterization of JLcl> as u{ct>i(0) I i EN}. 
A useful tool for reasoning about fixed points is embodied in the fixed 
point induction principle. 
Definition. Let (D, !;;;D) be a cpo. A predicate P(x) on D is admissible if 
the following holds for all chains C in (D, !;;;D): 
if P(c) for all c E C, then P(UC). 
Theorem 5.2 
(Fixed Point Induction Principle). Let (D, !;;;D) be a cpo, 
f: D ~ D a continuous function, and P(x) an admissible predicate on D. 
If 
1. P(l_ D), and 
2. P(fi( ..l D)) implies P(l + 1( ..l D)) for all i E N, 
then P( JLJ}. 
Proof. Ordinary induction shows that P(l( ..l D)) holds for all i E N. The 
set {l(..iD)IiEN} is a chain by Lemma 1, so P(U{l(..iD)IiEN}) 
holds by the admissibility of P(x), and of course u{fi(..l D) I i EN} = JLf. â¢ 
For example, suppose we define Y as the smallest subset of B* that 
1. contains {p, q, -, p, -, q}, 
2. is closed under the operation that transforms a to -, a, 
3. is closed under the operation that transforms a and {3 to (a :::) {3 ). 
Is Y equal to the set of propositional formulas over Sit'? Let 
'I'(Z) = {p,q, -,p, -,q} U {-,a I a E Z} U {(a:::) {3) I a, {3 E Z}. 
'I' is continuous, by an argument almost identical to the argument that cl> 
is continuous, so JLW exists. The question, then, is whether JLW = JLcl>. We 
argue by fixed point induction that JLW ~ JLcl>. Let P(X) be the predicate 
X~ JLcl>. P(X) is admissible, since if %' is a chain in (.9(B*), ~9"(B*>) 
and C ~ JLcl> for all C E %', then clearly u %' ~ JLcl>. The bottom element 

500 
Chapter 16 Approximation Orderings 
of (.9'(B*), ~9'(8 .>) is 0, and 0 ~ JL<I>. Also, if '1';(0) ~ JL<I>, then 
-qri+l(0) = 'l'('l'i(0)) 
~ W(JL<I>) by the induction hypothesis and the monotonicity of 'I' 
= {p,q, -,p, -,q} U {-,a I a E JL<I>} U {(a:::> f3) I a, {3 E JL<I>} 
= {p,q} U {-,a I a E JL<I>} U {(a:::> {3) Ia, {3 E JL<I>} 
since -,p, -,q E {-,a I a E JL<I>} 
= <I>( JL<I>) 
= JL<I>. 
Therefore, by the fixed point induction principle, we have JLW ~ JL<I>. A 
similar induction argument on the admissible predicate X~ JLW shows 
that JL<I> ~ JLW, so we have JLW = JL<I>. The point is that both definitions 
characterize the same set, and the second definition, with its unnecessary 
reference to -, p and -, q, can be simplified to the first definition. When 
fixed points are used to define the meaning of programs, the same 
technique can be used to show that two programs are equivalent, or to 
simplify programs. 
For sets defined like the set of propositional formulas over .91, fixed point 
induction is closely related to a form of induction known as structural 
induction. Let P(x) be a property of propositional formulas over .91 (rather 
than sets of formulas). If 
1. P( a) for every a E .91', 
2. P( a) implies P(-, a) for all propositional formulas a over .91, 
3. P( a) and P( {3) implies P(( a :::> {3 )) for all propositional formulas a, 
{3 over .91, 
then the structural induction principle allows us to conclude P( a) for all 
propositional formulas a over .91. The assumptions P( a) and P( {3) in 2 
and 3 are the structural induction hypotheses. To see why the conclusion is 
valid, let P(X) be the property on sets of propositional formulas over .91 
defined 
P(A) ifandonlyif P(a)forall a EA. 
P(X) is admissible: if '?? is a chain in (.9'(B*), ~9'(8 .>) and P(C) for all 
C E '??, then for each C E ~ we have P(a) for all a E C, which implies 
P(a) for all a E u~, that is, P(U~). Now, assumptions 1, 2, and 3 
enable us to prove 
â¢ P(0), 
â¢ P(<l>i(0)) implies P(<t>i+ 1(0)) for all i EN, 

5. Fixed Points 
501 
from which we conclude, by fixed point induction, P( JLcf> ), i.e., P( a) for all 
a E JLcf>, the set of propositional formulas over .91'. We will give several 
structural induction arguments in the next chapter. 
The reader will recall that we proved a theorem in Chapter 4 that was 
also called a fixed point theorem. The two are closely related. The 
recursion theorem from Chapter 4 is sometimes called, for historical 
reasons, the second recursion theorem. In fact, the earlier fixed point 
theorem, which follows from (and just as easily implies) the second 
recursion theorem, is itself sometimes called the second recursion theorem. 
The fixed point theorem in this chapter is a version of a classical theorem 
from computability theory that is sometimes called the first recursion 
theorem. 
The names of these two recursion theorems come from the fact that 
they can both be used in proving functions to be partially computable, 
particularly functions defined by recursion. However, there is a significant 
distinction between the two theorems. The fixed point theorem in this 
chapter gives a fixed point for each continuous function on a cpo. In 
particular, if F: [N _]_ ~ N _]_] ~ [N _]_ ~ N _]_] is continuous, then we get a 
function f E [N _]_ ~ N _]_]such that F(f) =f. On the other hand, the fixed 
point theorem in Chapter 4 is more directly concerned with programs than 
functions. A computable function g gets the effect of transforming a 
function cf>#(9'J to the function cf>g(#(9")) by acting on the (code of the) 
program that computes cf>#Wl, and the fixed point theorem in Chapter 4 
gives a program tff such that cf>*<"> = cf>g(#(<f)). It would be reasonable, 
then, to call that earlier theorem a syntactic fixed point theorem and to 
call the current theorem a semantic fixed point theorem. 
Just as the second recursion theorem gives a partially computable 
function cf>*<"> that satisfies cf>#(<fl = cf>g(#(<f)), so too does the first recur-
sion theorem give, for the appropriate kind of F, a partially computable 
JLF. We will say no more about the first recursion theorem in its classical 
form (other than to direct the reader to Exercise 11), but as we shall see, 
the main point of the next chapter is to use the fixed point theorem for 
cpos to define partially computable functions. 
Exercises 
1. Give functions f, g, h: N _]_ ~ N _]_ such that 
(a) f has no fixed points; 
(b) g has exactly one fixed point; 
(c) 
h has infinitely many fixed points. 

502 
Chapter 16 Approximation Orderings 
2. 
Give a function f: N _L ~ N _L such that f is not continuous and JL! 
exists. 
3. Give a partial order (D, !;;;D) and a function f: D ~ D such that f is 
continuous, f has at least one fixed point, and JL! does not exist. 
4. Give a fixed point characterization of the set of W-terms defined at 
the beginning of Chapter 13, where W is some vocabulary. 
5. Let (D, !;;;D) be a cpa, let f E [D ~ D], let E = {e ED I f(e) = e}, 
and let 
!;;;;;Â£ be the restriction of !;;;D to E. Show that (E, !;;;E) is a 
cpa. 
6. 
Let (D, !;;;D) be a cpa. 
(a) Let P(X) be the predicate on fJIJ(D) defined "X is a finite set." 
In the context of (fJIJ(D), ~9"<D>), is P admissible? 
(b) Let Q(X) be the predicate on fJIJ(D) defined "X is an infinite 
set." In the context of(fJIJ(D), ~9"(D)), is Q admissible? 
(c) 
Let R(f) be the predicate on D ~ D defined "f is strict." In 
the context of (D ~ D, !;;;D .... D), is R admissible? 
7. Let (D, !;;;D), (E, !;;;E) be cpos, let P(x), Q(x) be admissible predi-
cates on D, and let R(x, y) be an admissible predicate on D X E. 
(a) Show that P(x) & Q(x) is admissible. 
(b) Show that P(x) v Q(x) is admissible. 
(c) 
Show that (Vd E D)R(d, y) is an admissible predicate on E. 
8. * (a) Let r be a context-free grammar with variables 'F and termi-
nals T. Give a fixed point characterization of L(f). [Hint: 
Define a function <I> such that JL<I>(V) = {w E T* I V ~ w} for 
all V E 'F.] 
(b) Let f be the grammar with 'F= {S}, T = {a}, and the single 
production S ~ aSa. Show by fixed point induction that L(f) = 
0. 
9.* Let (D, !;;;D) be a complete lattice, and let f: D ~ D be monotonic. 
Show that JL! exists and that JL! = n{d ED I f(d) !;;;D d}. [See Exer-
cise 3.20 for the definition of complete lattices.] 
10.* Let (D, !;;;D) be a cpa, and define f.LD: [D ~ D] ~ D as f.LD(f) = JL! 
for all JE [D ~ D]. 
(a) Let f, g E [D ~ D], and suppose f !;;;!D .... DJ g. Show by induc-
tion on n that r !;;;(D .... D) gn for all n EN. 
(b) Let :F be a chain in ([D ~ D], !;;;(D->DJ). Show by induction on 
n that (ug-)n = u{r If E :F}. [Hint: Use the diagonal lemma.] 

5. Fixed Points 
503 
(c) Show that JLv is continuous. [Hint: Use parts (a) and (b) and the 
exchange lemma.] 
11.* As in Exercise 3.14, N ~ N is the set of all partial functions on N. 
For each finite function Pe = {(x1 , y 1), â¢â¢â¢ , (xn, Yn)} in N ~ N, n ~ 0, 
-
p 
we encode e as e E N, where 
li = O'! 
pY;+l 
v 
z= 1 
X; 
â¢ 
A function F: (N ~ N) ~ (N ~ N) is a recursive operator if there is 
some partially cooi'putable function h(y, x) such that 
F(g) (x) = z if and only if h(O, x) = z for some e ~g. 
(a) Let G:(N--; N) ~ (N--; N) be defined G(f)(x) = 2 Â·f(x). 
Show that G is a recursive operator. 
(b) Show that every recursive operator is monotonic and continuous. 
(c) 
Show that, if F is a recursive operator, then there is a com-
putable function f such that F( <I>) = <l>f<x> for all x E N. 
(d) (First Recursion Theorem) Prove that, for every recursive opera-
tor F, p,F exists and is partially computable. 


17 
Denotational Semantics 
of Recursion Equations 
1. Syntax 
Now that we have developed a theory of approximation orders, we can 
define recursion equations and give them a denotational semantics. The 
operational semantics, given in the next chapter, will show that the 
functions defined by recursion equations are, in a reasonable sense, 
computable. 
As in Chapter 13,1 where we defined the terms and formulas of 
quantification theory, we begin with a small alphabet 
A= {t,x,f,l, X,-,#,(,,),=} 
of symbols that are always available. The members of 
VART = {tl1i11 i EN} 
are type variables, and a type is 
â¢ a type variable, or 
â¢ 
T 1 X Â·Â·Â· X Tn, n ~ 1, where T 1 , â¢.â¢ , Tn are type variables, or 
â¢ 
T 1 X Â·Â·Â· X Tn-T, n ~ 1, where T 1, â¢â¢â¢ , Tn, T are type variables. 
1 Knowledge of Chapter 13 is not assumed, but there is a substantial overlap in the 
treatment of the syntax and semantics of terms. 
505 

506 
Chapter 17 Denotational Semantics of Recursion Equations 
These three kinds of types are individual types, product types, and function 
types, respectively. For a given T EVART, the members of VARI = 
{x#T#I1;1 I i E N} are individual variables of type T, and 
U VARI 
is the set of all individual variables. For a function type T1 X Â·Â·Â· X Tn-T, 
the members of 
VAR~,x ... XT.-T = {f#T 1 XÂ·Â·Â· X Tn -T#IIi] I i EN} 
are the function variables of type T1 X Â·Â·Â· X Tn -T, and 
is the set of all function variables. Also, VAR = VAR1 U VARF. We will 
let X, Y, Z (possibly subscripted) stand for individual variables and F, G, H 
(possibly subscripted) stand for function variables.2 Occasionally we will 
write V for an arbitrary variable of either kind. We will also use more 
suggestive names in the examples. If 0 is any of the syntactic objects 
defined in this section (W-terms, W-programs, etc.), then IV(O) is the set 
of all individual variables which occur in 0, FV(O) is the set of all function 
variables which occur in 0, and V(O) = IV(O) u FV(O). 
A typed vocabulary is a pair (W, T ), where W is a finite set of function 
symbols distinct from the symbols in A, and T is a function on W such that 
for each f E W, T(f) is either an individual type or a function type. We say 
that T(f) is the type of f. f is a constant symbol if T(f) is an individual type, 
and it is a proper function symbol otherwise. Given T, it is easy to 
determine the arity of any f E W, denoted ar(f). If f is a constant symbol, 
then ar(f) = 0, and if T(f) = T1 X Â·Â·Â· X Tn-T, then ar(f) = n. It will also 
be useful to supplement T with the functions 8 and p, which give the 
domain type and range type, respectively, of symbols in W. For constant 
symbols c E W, S(c) is undefined and p(c) = T(c), and for proper function 
symbols fEW with T(f) = T1 XÂ·Â·Â· X Tn-T, 
S(f) = T1 X Â·Â·Â·X Tn 
and 
p(f) = T. 
2 Note that the letter X is not itself an individual variable. It is what we sometimes call a 
rnetavariable. That is, it is a variable, which we use in talking about the syntax of recursion 
equations, whose values are individual variables. Similarly, F is a metavariable whose values 
are function variables. We also use metavariables such as T, c, f, t, and P, whose values are 
type variables, constant symbols, function symbols, terms, and programs, respectively. 

1. Syntax 
507 
We extend rand p to VAR and 8 to VARF in the obvious way. For ex-
ample, T(X#T#I[il) = T and 8(f#T 1 XÂ·Â·Â· X Tn -T#I[il) = T1 X Â·Â·Â·X Tn. 
For a typed vocabulary (W, r ), TV(W, r) is the set of all type variables that 
occur in the types of all of the symbols in W. We will omit r and write 
TV(W) for TV(W, r ). 
is 
Let (W, T) be a typed vocabulary. For any T E TV(W), a W-term of type T 
â¢ an individual variable of type T, or 
â¢ c E W, where r(c) = T, or 
â¢ g(t 1 , â¢â¢â¢ ,t), where g E W, r(g) = T1 XÂ·Â·Â· X Tn-T, and t; is a 
W-term of type T;, 1 :::;; i :::;; n, or 
â¢ F(t 1 , â¢â¢â¢ , t), where FE VARF, r(F) = T1 XÂ·Â·Â· X Tn-T, and t; is a 
W-term of type T;, 1 :::;; i :::;; n. 
We extend T so that r(t) = T for any term t of type T. For V0 ~ VAR, 
TMMV0 ) is the set of all W-terms t of type T such that V(t) ~ V0 , and 
TMw(V0 ) is the set of all W-terms t such that V(t) ~ V0 â¢ Also, we will 
write TM~ for TMM0) and TMw for TMw(0). Terms in TMw, that is, 
W-terms without variables, are sometimes called ground W-terms. 
For example, let N be a type variable, and let (W1 , r 1) be the typed 
vocabulary with W1 = {O,s}, r 1(0) = N and r 1(s) = N -N. We have 
TV(W1) = {N} and TMw1 = {0, s(O), s(s(O)), ... }. This is a vocabulary 
suitable for naming the natural numbers. We call terms of the form 
n 
...----... 
s( Â·Â·Â· s) (0) Â·Â·Â· ), n EN, 
numerals, which we will generally write as nor sn(o). 
Now, let NL be a type variable distinct from N, and let (W2 , r 2 ) be the 
typed vocabulary with W2 = {O,s,nil,cons}, r 2(0) = N, r 2(s) = N -N, 
rz{nil) = NL, and r 2(cons) = N X NL-NL. Then TV(W2 ) = {N, NL} and 
â¢w2 is 
TMw1 U {nil, cons(O, nil), cons(s(O), nil), cons(O, cons(O, nil)), ... }. 
We might use this vocabulary for naming lists of numbers. The idea is that 
a list is either empty or it is constructed from a first element and a list of 
all succeeding elements. (The reader familiar with the programming lan-
guage LISP will recognize cons and nil.) 
A W-recursion equation is an equation of the form F(Xp ... , X) = t, 
where, for some T E TV(W) 
1. X1 , â¢â¢â¢ , Xn are distinct individual variables, F is a function variable, 
and r(F) = r(X 1) X Â·Â·Â· X r(Xn) -T, and 
2. t E TM~{{XI, ... ,Xn} U VARF). 

508 
Chapter 17 Denotational Semantics of Recursion Equations 
If E is theW-recursion equation F(X 1 , â¢â¢â¢ , Xn) = t, then F is the principal 
function variable of E, denoted PF(E), and any function variable that 
occurs in t is an auxiliary function variable of E. Note that a function 
variable can be both principal and auxiliary in a given equation. AF(E) is 
the set of auxiliary function variables of E. A W-recursion program (or 
simply W-program) is a finite set {E1 , â¢â¢â¢ , En}, n ~ 0, of W-recursion 
equations such that 
1. PF(E;) * PF(Ej) for 1 :::;; i < j :::;; n, and 
2. U f~ 1 AF(E;) ~ {PF(E1), ... , PF(En)}. 
If equation E in W -program P is F( X 1 , â¢â¢â¢ , X n) = t, then E is the defining 
equation for F in P. The first restriction in the definition of W-programs 
prevents inconsistencies, and the second ensures that every function vari-
able that occurs on the right side of any equation is defined. When some 
program P is given and F( X 1 , â¢â¢â¢ , X n) = t is the defining equation for F in 
P, we will sometimes write rhs(F) to denote the term t on the righthand 
side of the equation. 
Note that we require each function to be defined by exactly one 
equation, while in Chapter 3 we used two equations to define a function by 
recursion. For example, 
+(x,O) =x 
+ (x,y + 1) =s(+(x,y)) 
is a (somewhat informal) definition of addition. Another way of describing 
addition is 
+(x,y) = {;(+(x,y..:... 1)) 
if y = 0 
otherwise, 
(1.1) 
which can be construed as a single equation if the if-then-else test is itself 
a function. That is, given the function 
if(b, X, y) = {~ 
we can rewrite (1.1) as 
if b =TRUE 
otherwise, 
+(x,y) = if(y = O,x,s(+(x,y..:... 1))). 
(1.2) 
Of course, we also need the predicate y = 0 and the predecessor function 
y ..:... 1 for (1.2) to be meaningful. Therefore, we impose the following 
conditions on the vocabularies we will use. 
Let Bool be some particular type variable and let tt, ff be two new 
symbols. (It does not matter which type variable we choose for Bool, but it 

1. Syntax 
509 
will remain fixed throughout.) A standard constructor vocabulary is any 
vocabulary (We, Te) such that tt, ff E We, with Te(tt) = Te(ff) = Bool, and 
such that for each T E TV(We) there is at least one constant symbol 
c EWe with Te(c) = T. The latter requirement is not strictly necessary, but 
it will turn out to be convenient. We create a set of built-in function 
symbols for a standard constructor vocabulary (We , T) as follows. Let 
we-= we - {tt, ff} and let Pe be the range type function derived from Te. 
For each T E TV(W) we create the new symbol if T, for each f E we- we 
define the set of new symbols 
B(f) = {is_f} u {f;- 1 11 ::;; i ::;; ar(f)}, 
and we define 
B(We) = {ifT IT E TV(W)} U U B(f). 
rewc-
Note that {f;- 1 11 ::;; i ::;; ar(f)} = 0 if f is a constant symbol. We assign 
types to these new symbols with T B( w, > : 
for each T E TV(We) 
where Pe(f) = T 
A standard vocabulary is any typed vocabulary (W, T) such that 
for some standard constructor vocabulary (We, Te). The symbols in We are 
constructor symbols: they are used to build up data objects. tt and ff, in 
particular, will be used to represent TRUE and FALSE. The is_f symbols 
are discriminator function symbols: they are used to determine how an 
object is constructed. The f;- 1 symbols are selector function symbols: they 
are used to decompose compound objects. 
For example, we can expand the typed vocabulary (W1 , T1) given above 
to the standard constructor vocabulary (W3 , T3), where W3 = {tt, ff, 0, s}, 
T3(tt) = T3(ff) = Bool, T3(0) = N, and Tis)= N-+ N. Then 

510 
Chapter 17 Denotational Semantics of Recursion Equations 
where 
T8(wJif8001 ) = Bool X Bool X Booi-+Bool 
T8 (wJifN) = Bool X N X N -+N 
T8(wJis_O) = N -+Bool 
T8 <wJis_s) = N-+ Bool 
T8(wJs 1- 1 ) = N -+N, 
and we can rewrite Eq. (1.2) as 
+(X,Y) = ifN(is_O(Y),X,s(+(X,s1- 1(Y)))), 
(1.3) 
where + E VARF. 
Similarly, we can expand (W2 , T 2 ) 
to (W4 , T4 ), where W4 = 
{tt, ff, 0, s, nil, cons}, Titt) = T4(ff) = Bool, T4(0) = N, T4(s) = N-+ N, T4(nil) 
= NL, and T4(cons) = N X NL -+ NL. Then 
B(W4 ) = B(W3 ) U {ifNL,is_nil,is_cons,cons1- 1,cons; 1}. 
Henceforth, we let (WN, TN) be the standard vocabulary based on (W3 , T 3) 
and we let (WNL, TNL) be the standard vocabulary based on (W4 , T4 ). 
That is, 
WN = {tt, ff, 0, s, if Bool, if N, is_O, is_s, s1- 1} 
and 
WNL = WN U {nil,cons,ifNL,is_nil,is_cons,consl 1,cons:Z 1}. 
Generally we will just write T for TN or TNL. 
Note that we intend to interpret s1- 1 as the predecessor function in Eq. 
(1.3). At this point, of course, (1.3) has no meaning at all. The task of 
giving a meaning to equations like (1.3) begins in the next section. 
Exercises 
1. 
Let T(X) = N and T(F) = N X N -+ N. Describe TMwN({X, F}). 
2. 
Let we = {tt, ff, 0, s, leaf, tree}, and let Tc(tt) = Tc(ff) = Bool, Tc(O) = N, 
Tc(s) = N -+ N, Tc(leat) = T, and Tc(tree) = N X T X T -+T. 
(a) Describe TM~ for each T E TV(Wc). 
c 
(b) Describe (B(Wc), T8<w)Â· 

2. Semantics of Terms 
511 
(c) 
Let (W, T) =(We U B(W), Te U T8<w)Â· Describe â¢w for each 
T E TV(W). 
3. Let T(X) = N, T(Y) = NL, T(F) = N X N -N, T(G) = N -N, and 
T(H) = N - NL. Which of the following are WNL -terms? 
(a) F(s(X)). 
(b) cons( G( X), cons( X, nil)). 
(c) cons(nil,cons(O, Y)). 
(d) cons1- 1(H(O)). 
(e) 
if N(tT, cons( 0, nil), 0 ). 
(f) if NL(tT, cons( 0, nil), H( if N( tT, X, 0)) ). 
4. Assume that each of the following are WNL -terms. Give the types of 
the variables in each term. 
(a) ifN(F(O),G(O),s(H(O))). 
(b) s1- 1(F(cons(X,G(s(Y))))). 
(c) is_cons(F(s1- 1 (X), if N(G(O), H( G(Y)), 0), X)). 
(d) F(is_O(F(X))). 
5. Describe the values of T and the types of X, Y, F, G, H that make 
F(ifT(tt,G(F(X), Y),H(s(Y)))) a WNL-term. 
6. Let (W, T) be a vocabulary, let V ~ V AR, and extend T to (A u W)* 
so that T(w) = T if and only if w = cf>u for some cf> E W U V with 
p(cf>) = T. 
(a) Give a fixed point definition of TMw(V) in the manner of the 
definition of the set of propositional formulas given in Section 5 
of Chapter 16. 
(b) State and prove a structural induction principle for TMw(V). 
2. Semantics of Terms 
We develop the semantics of W-programs in several stages. In this section 
we work on the semantics of terms, beginning with the semantics of 
vocabularies. We will work exclusively with standard vocabularies, so 
throughout the rest of this chapter we take (W, T) to be some arbitrary 
standard vocabulary based on some standard constructor vocabulary 
(We, Te). We will generally refer simply to W rather than (W, T ). 
Definition. A type assignment for W is a function :T with domain TV(W) 
such that 
1. for each T E TV(W), :T( T) is a partial order (DCT(T), !;;;CT(T)) with 
bottom element ..LCT(T), and, in particular, 

512 
Chapter 17 Denotational Semantics of Recursion Equations 
2. Y(Bool) is the flat cpa on some set with exactly two elements . 
.57 is a complete type assignment for W if .57( T) is a cpa for each T E TV(W). 
A type assignment for W gives a meaning for each type variable in 
TV(W). For example, if YN(Bool) is the flat cpa on {TRUE,FALSE} and 
~(N) = (N _L 
, ~N ), then YN is a complete type assignment for WN. 
L 
When .57 is understood, we will write (Boo!, ~Boo!) for Y(Bool) and 
..L Bool for the bottom element of Y(Bool). For an arbitrary T E TV(W), 
we will often write (DT, ~T) for (IJ.'T(T)' ~.'T(T)) and ..L T for ..L.'T(T). Also, 
we will sometimes write 
(DT 1XÂ·Â·Â·XT '~T XÂ·Â·Â·XT) 
n 
I 
" 
for 
In particular, D6(F) = DT 1x ... XTn if T(F) = T 1 X ... X Tn -+T. 
It will be useful to define the following notation. For sets D, E and 
f: D ~ E, ran f is the range of f. Also, if e E E, then ran e = {e}. In 
effect, we are treating e as a function of 0 arguments. 
Definition. Let .57 be a type assignment for W. A Y-interpretation for W is 
a function J with domain W that satisfies the following conditions. 
1. For all constant symbols c E We with T(c) = T, .f(c) E DT - { ..L T}. 
We will write tt for .f(tt) and ff for J(tT). 
2. For all proper function symbols f Ewe with T(f) = Tl X ... X Tn -+T, 
a . .f(f) E D 
X .. Â· X D 
~ D Â· 
TJ 
T 11 
T' 
b. if .f(f)(dp ... , dn) =.f(f)(ep ... , en)=/= ..L T, then (dp ... , dn) = 
(e] ' ... 'en); 
c. if d; =/= ..L T', 1 ~ i ~ n, then .f(f)(d1, ... , dn) =/= ..L T . 
3. For all f, g E We such that p(f) = p(g), ran J(f) and ran .f(g) can 
have at most ..L p(f) in common; that is, 
(ran J(f) n ran .f(g)) - { ..L p(r) = 0. 
4. For all T E TV(W),.f(ifT): Bool X DT X DT ~ DT is defined 
J(ifT)(b, d, e) = {~ 
j_T 
if b = tt 
if b = ff 
if b = ..L Boo! â¢ 

2. Semantics of Terms 
513 
5. For all constant symbols c E we- with T(c) = T, J(is_c): DT ~ Bool 
is defined 
if d =J(c) 
J(is_c)(d) = 
ff 
{
tt 
..l Boo! 
if d =I= ..l T and d =I=J(c) 
ifd=..l_T; 
for all proper function symbols f EWe with T(f) = T 1 X Â·Â·Â· X Tn -+T, 
J(is_f): DT ~ Bool is defined 
J(is_f)(d) = 
ff 
{
tt 
..l Boo! 
if d =I= ..l T and d E ran J(f) 
if d =I= ..l T and d ft. ran J(f) 
if d = j_ T â¢ 
6. For all proper function symbols f Ewe with T(f) = Tl X ... X Tn -+T 
and for all 1 :::;; i :::;; n, J(f;- 1 ): DT ~ DT, is defined 
if d =I= ..l T and d =J(f)(d1 , â¢â¢â¢ , dn) for 
some (d1, ... , dn) EDT, X Â·Â·Â· X DT" 
otherwise. 
If J(f) is continuous for all proper function symbols f E W, then J is a 
continuous !7-interpretation. A W-structure is a pair !, = ( g; J), where .9"" is 
a type assignment for W and J is a Y-interpretation for W. !. is a complete 
W-structure if .9"" is a complete type assignment, and it is a continuous 
W-structure if J is a continuous .7-interpretation. 
A Y-interpretation for W gives a meaning to each symbol in W, using 
the objects made available by .9"" in the sets DT. Conditions 4, 5, and 6 
require a specific interpretation for the built-in function symbols. Note in 
particular that J(f;- 1 ) is a well-defined function because of condition 2b. 
Conditions 2c and 3 are imposed to make certain information about the 
objects of a W-structure available at the syntactic level of W-terms. 
Condition 2c implies that the meaning of a ground term is never the 
bottom element, so that it makes sense, for example, to replace s! 1(s(O)) 
with 0, since J(sXJ(O)) =I= ..lN. Condition 3 implies that it makes sense to 
replace a term such as is_f ( g( c)) with fT. As we will see in the next 
chapter, the replacement of terms by equivalent terms is the basis of the 
operational semantics of recursion equations, so these conditions are 
included to make the operational semantics work correctly. 

514 
Chapter 17 Denotatlonal Semantics of Recursion Equations 
For an example, let WN and YN be as before. If 
JN(tt) = TRUE 
JN(tT) = FALSE 
JN(O) = 0 
JN(s)(m) = { ..L N 
m + 1 
{
TRUE 
JN(is_s)(d) = 
FALSE 
..L Boo! 
ifm=..LN 
otherwise 
if b =TRUE 
if b =FALSE 
if b = ..L Boo! 
if b =TRUE 
if b =FALSE 
if b = ..L Boo! 
if d = 0 
if d =I= ..L N and d > 0 
ifd=..LN 
if d =I= ..L N and d > 0 
if d = 0 
ifd=..LN 
if d =I= ..L N and d > 0 
otherwise, 
then JN is a YN-assignment for WN. We will write !.N for (.5JN, JN ). It is 
easy to check that !,N is complete and continuous. 
We now have a way of interpreting the symbols of W, but before we can 
give a meaning to arbitrary terms, we need a way of interpreting variables. 
Definition. Let .9"" be a type assignment for W and V a set of variables. A 
variable assignment for V based on .9"" is a function a with domain V such 
that 
1. a(X) EDT for each individual variable X E V with T(X) = T, and 
2. a(F) EDT, X Â·Â·Â· X DT. ~ DT for each function variable FE V with 
T{F) = T 1 X Â·Â·Â· X Tn -+T. 
a is a continuous variable assignment for V if a(F) is continuous for each 
function variable F E V . .W7 (V) is the set of all variable assignments for V 

2. Semantics of Terms 
515 
based on :T, and ~.W'y(V) is the set of all continuous variable assignments 
for V based on :T. 
Let (!T.,J) be a W-structure and V a set of variables. For any a E.W'y(V), 
we extend a to a function aJ with domain TMw(V) as follows: 
aJ (c) =J(c) 
aJ (X) = a(X) 
for all constant symbols c E W 
for all individual variables X E V 
aJ (f(tl ' ... 'tn)) =J(f)(aJ (tl ), ... 'aJ (tn)) 
aJ(F(t 1 , â¢â¢â¢ ,tn)) = a(F)(aJ(t 1), â¢â¢â¢ ,aJ(tn)) 
where fEW 
where FE V. 
aJ is a function we can use to assign a meaning to any term in TMw(V). 
Note that 0 is the unique assignment in .W'y(0), and if t E TMw, i.e., t 
contains no variables, then 0J is sufficient for interpreting t. When J is 
understood, we will often write a for aJ. 
For example, let V = {X, Y, F}, and let 
a(X) = 3, 
a(Y) = 5, 
a(F) = + _j_, 
where + 
_j_ is the strict extension of +. Then a E ~.W'yN(V), and 
aJN(s(F(X, s(Y)))) =JN(s)(aJN(F(X, s(Y) ))) 
=JN(s)(a(F)(aJN(X), aJN(s(Y)))) 
=JN(s)( a (F)( a (X), JN(s)( aJN(Y)))) 
=JN(s)( a(F )( a(X),JN(s)( a(Y)))) 
=JN(s)( a(F )(3,JN(s)(5))) 
= JN (s)( a (F )(3, 6)) 
=JN(s)(9) 
= 10. 
The next theorem shows that aJ (t) assigns a value to term t in the 
appropriate set, namely, DT(t). We need it to verify that the definition of 
a 
J makes sense. For example, when we define 
where r(f) = T 1 X Â·Â·Â· X Tn -+T, we have J(f) EDT X Â·â¢Â· X DT ~ DT, so 
I 
n 
we want to know that aJ (t;) EDT , 1 :::;; i :::;; n. 
I 

516 
Chapter 17 Denotational Semantics of Recursion Equations 
Theorem 2.1. 
Let (9'; J) be a W-structure, V a set of variables, a E 
.Wy-(V), and t E TMW(V) for some T E TV(W). Then a5 (t) EDT. 
Proof. 
We argue by structural induction on t. If t is a constant symbol 
c E W, then a,y(c) =J(c) EDT, and if tis an individual variable X E V, 
then aJ (X) = a(X) EDT 0 If t is f(tl '0 0 
0 'tJ, where fEw, T(f) = 
T1 XÂ·Â·Â· X Tn-T, and t; E TM~(V), 1:::;; i:::;; n, then a,y(t;) EDT,' 1:::;; 
i:::;; n, by the induction hypothesis, and J(f) EDT, X Â·Â·Â· X DT., ~ DT, so 
Similarly, if tis F(t 1, â¢â¢â¢ , tJ, where FE V, T(F) = T1 X Â·Â·Â· X Tn-T, and 
t; E TMw(V), 1 :::;; i:::;; n, then a(F) EDT x Â·Â·Â· x DT ~ DT and 
I 
n 
Let .<T be a complete type assignment for W and V a set of variables. 
Then for each individual variable X E V with T(X) = T, ( DT, !;;;T) is a 
cpa by assumption, and for each function variable F E V with T(F) = 
Tl X â¢â¢â¢ X Tn-T, 
is a cpa by Theorems3 16.3.6 and 16.4.4. Now, let g-v be the function with 
domain V such that 
g-v(X) = (DT, !;;;T) 
for each X E V with T(X) = T 
g-v(F) = ([DT, X ... XDT ~DT], !;;;[D 
x .. Â·xD ->D J) 
II 
TJ 
T 11 
T 
for each FE V with T(F) = T1 X Â·Â·Â· X Tn-T. 
Then a continuous variable assignment for V based on .<Tis a g-v-choice 
function and vice versa, so ~.Wy(V) = ch(g-v), and (ch(g-v), !;;;ch(9lvl) is a 
cpa by Theorem 16.3.10. Writing 
!;;;;~~'"'" <V> for !;;;ch<9lvl, we have proved 
Theorem 2.2. Let .<T be a complete type assignment for W and V a set of 
variables. Then (%'-Wy(V), !;;;w.~,(VJ) is a cpa. 
Note that the bottom element of (~.Wy(V), !;;;t::W''T(Vl), which we will 
3 We will refer to theorems in Chapter 16 frequently here, so we adopt the convention of 
writing Theorem 16.3.6, for example, to refer to Theorem 3.6 in Chapter 16. 

2. Semantics of Terms 
write Or-;:w,.<V>â¢ or simply 0 when Yand V are understood, satisfies 
O(X) = ..L T(X) 
for each individual variable X E V 
517 
0 (F )( d 1 , â¢â¢â¢ , d n) = ..Lr<F > for each function variable F E V and all 
(dl , ... ,dn) E DS(F)' 
Note that if V = 0, th~ Oy,:w-, (V > = 0. Given a ground term t E TM w , 
we wiJI generally write O(t) to interpret t. 
The next theorem says, in effect, that the function that extends assign-
ments a to a,. is monotonic and continuous. 
Theorem 2.3. Let (!T, J) be a complete, continuous W-structure, V a set 
of variables, and t E TMw(V) for some T E TV(W). 
1. For a, {3 E %'~'7(V), a !;;;l<:w.,.(V) {3 implies a_y (t) !;;;T li.Y (t). 
2. For a chain .W' in (%'.W'7 (V); !;;;y,:w:.,.<v>), U.W'y(t) = U{a,y (t) I a E.W'}. 
Proof. Both parts can be proven by structural induction on t, and part 1 is 
straightforward, so we leave it as an exercise and concentrate on part 2. 
Let .W' be a chain in (%'~'7(V), !;;;;; y,:w:T <V> ). Then U.W' exists by Theorem 
2.2. If t is a constant symbol c E W, then 
U.W'(c) =J(c) = u{J(c) I a E.W'} = U{a(c) I a E.W'}, 
and if t is an individual variable X E V, then 
U.W'(X) = ( U.W')(X) 
= U{a(X) I a E.W'} 
= U{a(X)IaE.W'}. 
If tis f(t 1 , â¢â¢â¢ , tn), where fEW, then 
U.W'(f(tlâ¢Â·Â·Â·â¢tn)) 
=J(f)(U.W'(tl), ... , U.W'(tn)) 
by Theorem 16.3.9 
=J(f)(U{a(t 1)1 a E.W'}, ... , u{a(tn)l a E.W'}) 
by the induction 
hypothesis 
=J(f)(U{(a(tl), ... , a(tn))l a E.W'}) 
by Theorem 16.3.5 
= U{J(f)(a(t 1), ... , a(tn)) I a E.W'} 
since J(f) is continuous and 
{(a(t 1), â¢â¢â¢ , a(tn))l a E.W'} is a chain by part 1 
= U{a(f(t 10 ... ,tn))l a E.W'}. 
Finally, Jet t be F(t 10 ... ,tn), where F is a function variable in V, and let 
r: %'.W'g-(V) X 'IF.W'g-(V) ~ DT be defined f(a, {3) = a(F)( {i(tl), ... ' li<tn)). 
Then r is monotonic by part 1 and by the monotonicity of a(F) for all 

518 
Chapter 17 Denotatlonal Semantics of Recursion Equations 
a EJ/1', so 
UJ!i'(F(tl, ... ,tn)) 
= (UJ!i')(F)(UJ!i'(tl), ... , UJ!i'(tn)) 
= (U{a(F) I a EJ/1'}XUJ!i'(t 1), â¢â¢â¢ , UJ!i'(tn)) by Theorem 16.3.9 
= U{a(F)(UJ11'(t 1), ... , UJ!i'(tn))l a EJ/1'} 
by Theorem 16.3.7 
= U{a(F)(U{ /3(t 1) I {3 EJ/1'}, ... , U{ /3(tn) I {3 EJ/1'}) I a EJ/1'} 
by 
the induction hypothesis 
= U{a(F)(U{( /3(t 1), â¢â¢â¢ , j3(tn)) I {3 E Jli'}) I a E J/1'} 
by Theorem 
16.3.5 
= U{U{a(F)(j3(t 1), ... ,/3(tn))1 {3 EJ/1'}1 a EJ/1'} 
since a(F)iscon-
tinuous for each a EJ/1' and {( /3(t 1), â¢â¢â¢ , j3(tn)) I {3 EJ/1'} 
is a chain by part 1 
= U { U {f( a, {3) I {3 E J/1'} I a E J/1'} 
= u {f( a, a) I a E J/1'} 
by the diagonal lemma 
= U{a(F)(a(tl), ... ' a(tn)) I a EJ/1'} 
= U{a(F(t 1 , ... ,t))l a EJ/1'}. 
â¢ 
We also prove one more result about variable assignments that we will 
use in the next section. 
Coincidence Lemma. Let (Y,J) be a W-structure, let V1 , V2 be sets of 
variables, let a EJ/1'y(V1) and {3 E~9'"(V2 ), and let 
V = {V E V1 n V2 I a(V) = {3(V)}. 
Then for all t E TMw(V), a(t) = j3(t). 
Proof. We argue by structural induction on t. If tis an individual variable 
X E V, then a(X) = a(X) = {3(X) = j3(X). If tis a constant symbol c E W, 
then a(c) =J(c) = j3(c). If tis f(t 1 , â¢â¢â¢ , t), where fEW, then 
a(f(tl ' ... 'tn)) =J(f)(a(tl), ... ' a(tn)) 
=J(f)( j3(t 1 ), â¢â¢â¢ , j3(tn)) 
by the induction hypothesis 
= j3(f(tp ... ,tn)), 
and if tis F(t 1 , â¢â¢â¢ , tn), where FE V, then 
a(F(t 1 , ... ,tn)) = a(F)(a(t 1), ... , a(tn)) 
= a(F )( j3(t1 ), ... , j3(tn)) 
by the induction hypothesis 
= {3(F )( /3(t 1 ), ... , j3(tn)) 
since F E V 
= j3(F(tp ... ,tn)). 
â¢ 

2. Semantics of Terms 
519 
Exercises 
1. Show that !.N is a complete, continuous WN-structure. 
2. Let 9'"(Bool) = ({ _!_, 0, 1}, {( _!_, _!_ ), ( _!_, 0), ( _!_, 1), (0, 0), (1, 1)}), 9'"(N) 
= (Nl_ , !;;;N" ), J(tt) = 0, and J (ff) = 1. Extend J to a 7-interpre-
tation for WN. 
3. Let 9'" be a type assignment for WNL with 9'"(N) = 9'"(NL) = 
(N j_, !;;;N ). 
(a) Let J(nil) = 0 and J(cons) = + j_ (the strict extension of + ). 
Show that J cannot be extended to a 7-interpretation for WNL. 
(b) Give a continuous 7-interpretation J' for WNL. [Hint: Consider 
the pairing function ( x, y) from Chapter 3.] What is 
0J.(cons(s(O), nil))? 
4. Let !, = (9'; J) be a W-structure such that 9'"( T) is a flat cpo for all 
T E TV(W). Show that for every built-in function symbol f E W, J(f) 
is continuous. 
5. Let a(X) = 3, a(Y) = 2, a(F) = + j_ 
, and a(G) = Â· j_ (the strict 
extension of the multiplication function). Calculate aJN(t), where t is 
as follows. 
(a) F(s(X),G(s(X),F(X, y))). 
(b) s1- 1(F(s1- 1(s(X)), Y)). 
(c) 
if N(is_O( G( X, s1- 1 (si 1 ( 0)) )), X, Y). 
(d) if Bool(is_s(X), is_O(X), is_O(X)). 
6. 
Let J,;(O) = 0 and Y,;(s) = e j_ , where e(n) = 2n for all n E N. 
(a) Extend Y,; to a .9N-interpretation for WN. 
(b) Calculate a .. /t) for each term t given in Exercise 5. 
e 
7. 
Let 9'"(Bool) = 9'"N(Bool), 9'"(N) = 9'"N(N), and 9'"(NL) = 
(TUP j_ , !;;;TUP ), where TUP is the set of all tuples of natural 
" 
numbers and (TUP j_, !;;;TUP ) is the flat cpo on TUP. Give a 7-inter-
pretation for W NL . 
" 
8. 
Let 9'"(Bool) = 9'"N(Bool), 9'"(N) = 9'"N(N), and 9'"(NL) = 
(.9r(N), ~9"t(NJ), where .9r(N) consists of all the finite subsets of N, 
and let J(consXe, {d1 , â¢â¢â¢ , dn}) = {e, d1 , â¢â¢â¢ , dn}. Explain why J can-
not be extended to a 7-interpretation for WNL. 
9. Let !, = (9';J) be a continuous W-structure, and let f EWe. Show 
that for all d,e E D(<r> such that j_p(fl =/= d !;;;p(f) e, if dE ranJ(f) 
then e E ran J(f). Hint: Use is_f.] 
10. Let !. = (9';J) be a continuous W-structure, and let f EWe. Show 
that for all d, e E D5(r)' if 
_!_ p(f) =/=J(f)(d) !;;;p(f) J(f)(e), then d 
!;;;5<r> e. [Hint: Use f;- 1, 1 :::;; i :::;; ar(f).] 

520 
Chapter 17 Denotational Semantics of Recursion Equations 
11. Let (Y,J) = !.N, and let X E VAR7. Give_ a term t E â¢wN({X}) 
and a, {3 E 'IF.W'y({X}) such that aJ (t) !;;;N" {3J (t) and {3 c~:~" <lXI> a. 
12. Let (Y,J) be a W-structure, let V be a set of variables, and let 
a, {3 E 'IF.Wy(V). Show that if aJ (t) !;;;T(I) aJ (t) for all t E TMw(V), 
then a !;;;%'.#_,(v>f3Â· 
13. Prove part 1 of Theorem 2.3. 
14. Let !. = (Y, J) be a complete, continuous W-structure, and let V be 
a set of variables. 
(a) Define a function g~ such that .W'y(V) = ch(g~), and show 
that (.Wy(V), !;;;,..,._,(V)) is a cpo. 
(b) Show that part 1 of Theorem 2.3 holds for (~9"(V), !;;;,~"w/ 
(c) 
Show that part 2 of Theorem 2.3 fails for (~9"(V), !;;;,~"w/ 
3. 
Solutions to W-Programs 
Now that we have the tools for giving a meaning to terms, we can take the 
first step toward defining the denotational semantics of programs. Let 
be a W-program, and let !, = (Y, J) be a W-structure. We want to define 
the meaning ofF; in terms oft;, 1 ::; i ::; m. The idea is that we start with 
a variable assignment a E 'IF.Wy(FV(P)) which gives a meaning to each 
function variable in t;. Then for any possible input (d 1 , â¢â¢â¢ , dn) E D8(F;) 
we extend a with the assignment {3 = {(X 1 , d 1), ... , (Xn;, dn)} and use 
a U {3 to interpret t;, giving us an output value for input (d 1 , â¢â¢â¢ , dn ). 
It will be convenient to introduce a special notation for the assignment 
{3 in the previous paragraph. Given an equation F( X~' ... , X) = t and 
d = (d1, ... , dn) E D8<F >' the variable assignment a<d 1 â¢â¢â¢â¢ ,d.,P also written 
ad, is {(X1, d1 ), ... , (Xn, dn)}; that is, 
a(d 
d (X.)= d. 
1 â¢ Â· â¢ Â· â¢ 
n) 
I 
I ' 
1::=;i::=;n. 
The particular equation that determines the variables in the domain of ad 
will always be clear from the context in which ad is used. 
Definition. Let !, = (Y, J) be a W-structure and P a W-program. We 
associate with P the higher order function <1>~: 'IF.W'y(FV(P)) ~.wy(FV(P)), 
defined as follows. For each F E FV(P), with defining equation 

3. Solutions to W-Programs 
521 
When !. is understood we will write cl>p for ct>~. 
It is clear from the definition that ct>~ E 'i&".W'g-(FV(P)) ~ .W'g-(FV(P)), but 
if !. is complete and continuous we can prove something stronger. 
Theorem 3.1. Let !. = (Y,J) be a complete, continuous W-structure and 
let P be a W-program. Then ct>~ E 'i&"~?(FV(P)) ~ 'i&".W'g-(FV(P)). 
Proof. We will write cl>p for ct>~. Let a E 'i&".W'g-(FV(P)). We need to show 
that cl>p(a)(F) E [Ds(F> ~ Dr<F>] for each FE FV(P). It follows from 
Theorem 2.1 that ci> p( a )(F) E D8<F > ~ Dr<F >, so we just need to show 
that cl>p(a)(F) is continuous. Let F(X 10 ... ,X) = t be an equation in P 
with 
T(F) = T 1 X Â·Â·Â· X Tn -+ T, and let C 
be a chain in 
(DT X ... X T ' [;;;T X ... X T ). Then {a u ac I c E C} 
is a chain in 
I 
'' 
I 
n 
('6".W'y-(V), [;;;><:<>'><V>), where V= {X 1, ..â¢ ,Xn} U FV(P), so U{a U ac lc E 
C} exists by Theorem 2.2. Moreover, for any G E FV(P), 
(aU auc)(G) = a(G) = U{(a U a)(G)Ic E C}, 
and for X;, 1 :::;; i :::;; n, 
(aU auc)(X;) = (UC)~ i = U(C ~ i) = U{(a U a)(X;) IcE C}, 
so by Theorem 16.3.9, aU auc = U{a U ac IcE C}. Therefore, 
cl>p(a)(F)(UC) =aU auc(t) 
= U {a U ac I c E C}(t) 
= U {aU ac(t) I c E C} 
= U{cl>p(a)(F)(c) IcE C} 
= Ucl>p(a)(F)(C), 
and ci> p( a )(F) is continuous. 
by Theorem 2.3 
â¢ 
Since a program P is a set of equations, it makes sense to try to solve 
these equations to find the meaning of P. 

522 
Chapter 17 Denotatlonal Semantics of Recursion Equations 
Definition. Let I = (Y,J) be a W-structure and P a W-program. A 
solution toP in I is any a E..W7 (FV(P)) such that 
(3.1) 
for every equation F(XH ... ,X)= tin P and every (d1 , ... , dn) E D5(F)' 
In other words, every function variable in P is assigned a function such 
that every equation in P is satisfied for every possible value taken by the 
individual variables. Note that an equivalent statement of (3.1) is 
(aU a(d~>Â·Â·Â·Â·d)_;r(F(X 1 , ... ,Xn)) =(aU a(d,, ... ,d)J(t) 
for every equation F(X 1 , â¢â¢â¢ ,Xn) =tin P and every (d1 , â¢â¢â¢ , dn) E D5<F>Â· 
It is important to understand that for an arbitrary a E ..W7 (FV(P)), 
<l>p(a) is not necessarily a solution to P. Consider the WN-structure 
IN = (.o/N ,JN) and the WN-program Q with equations 
F(X) = G(X) 
G(X) = F(X). 
The problem is that <I>Q(a)(F) is defined in terms of a(G), but applying 
<I>Q to a changes the function assigned toG from a(G) to <I>Q(a)(G). For 
example, if a(F) is the constant function of a(F)(x) = 3 and a(G) is the 
constant function a(G)(x) = 7, then 
but 
<I>Q(a)(F)(O) =au a 0(G(X)) = a(G)(O) = 7, 
<I>Q(a) U a 0(rhs(F)) = <I>Q(a) U a 0(G(X)) 
= <I>Q(a)(G)(O) 
=aU a 0(F(X)) = a(F)(O) = 3, 
so <I>Q( a) is not a solution to Q. 
What we need is an a such that <I>Q leaves a(F) and a(G) unchanged; 
that is, we need a fixed point of <I>Q. If a is some fixed point of <I>Q, then 
for any d E N _]_ we have 
a(F)(d) = <I>Q(a)(F)(d) =aU a/rhs(F)) 
a(G)(d) = <I>Q(a)(G)(d) =aU a/rhs(G)), 
so a is a solution to Q. More generally, we can prove 

3. Solutions to W-Programs 
523 
Theorem 3.2. Let !. = (9';J) be a W-structure, and let P be a W-pro-
gram. Then a E ~.W"y(FV(P)) is a solution toP in !. if and only if a is a 
fixed point of ct>~. 
Proof. If a E ~.Wy(FV(P)) is a solution to P in !., then 
a(F)(d) =aU airhs(F)) = ct>~(a)(F)(d) 
holds for all FE FV(P) and all dE D6<F>â¢ so ct>~(a) = a. On the other 
hand, if a E ~.W"y(FV(P)) is a fixed point of ct>~, then 
a(F)(d) = ct>~(a)(F)(d) =aU airhs(F)) 
holds for all F E FV(P) and all d E D6<F >, so a is a solution to P in !. . â¢ 
Going back to the example, we still have the problem that ct>Q has more 
than one fixed point. Any a that assigns the same function f to both F and 
G, where f could be anything from the everywhere undefined function to 
(some extension to N _~_ of) the total predicate HALT(X, X), is a solution 
to Q. Clearly, there is nothing in program Q to indicate that the program-
mer meant to specify a solution to the halting problem. For that matter, 
there is no indication that the programmer meant to solve any problem at 
all in writing program Q. The sensible approach is to focus on the least 
fixed point of ct>Q, which would be a(FXd) = a(G)(d) = ..L N for all 
d E N _~_ , i.e., a = .n. 
Of course, we do not know that JLcl>~ exists in general, for an arbitrary 
W-structure !. and an arbitrary W-program P. 
Theorem 3.3. Let !. = (9'; J) be a complete, continuous W-structure and 
let P be a W-program. Then JLcl>~ exists, and JLcl>~ E ~.W"y(FV(P)). 
Proof. We will write cl>p for ct>~. If JLcl>p exists, then Theorem 3.1 implies 
that JLcl>p E ~.W"y(FV(P)), so we just need to show that it exists. By 
Theorem 2.2, it is sufficient to show that 
so by Theorem 3.1 we only need to show that cl>p is continuous. Let .W be 
a chain in (~.Wy(FV(P)), !;;;W...,~<FV(P))), F(X1,. â¢â¢ ,Xn) =tan equation in P, 
and d E D6<F >. It is easy to see that {a U ad I a E .W} is a chain in 
(~.W"y(V), !;;;w...,~ <V>), where V = {X1, â¢â¢â¢ , Xn} U FV(P), and that 
(3.2) 

524 
Chapter 17 Denotational Semantics of Recursion Equations 
so we have 
<f>p( U.w')(F)(d) 
= U.w' U a/t) 
= U {aU ad I a E.w'}(t) 
= U{a U a/t) I a E.w'} 
= U{<f>p(a)(F)(d) I a E.w'} 
= (U{<f>p(a)(F) I a E.w'})(d) 
= (U{<f>p(a) I a E.w'})(F)(d) 
= (U<f>p(.w'))(F)(d). 
by (3.2) 
by Theorem 2.2 
by Theorem 16.3.7 
by Theorem 16.3.9 
(Note that F and d were arbitrarily chosen, so U { <f>p( a )(F )(d) I a E .w'} 
exists for all d E Da(F > and U { <f> p( a )(F) I a E .w'} exists for all F E FV(P), 
justifying the use of Theorems 16.3.7 and 16.3.9.) Now, <f>p(U.w')(F)(d) = 
( U <f>p(.w'))(F )(d) for all d E D8(F > and all F E FV(P), so <f>p( U .w') = 
U <f>p(.w') and <f>p is continuous. 
â¢ 
The fixed point theorem not only tells us that JL<f>p exists, but it also 
gives us a way of calculating JL<f>p, since JL<f>p = u{<t>~(!l) I i EN}. For 
example, let ADD be the WN-program with the equation 
+(X,Y) = ifN(is_O(Y),X,s(+(X,s 1- 1 (Y)))). 
Then, writing J for JN, in kN we have, for any d E N j_ and any n E N, 
<f>~J(fl)( + )(d, j_ N) 
and 
= <f>l'oo( n) u a(d, j_ Jif N (is_O(Y)' X, s( + (X, sl- I (Y))))) 
= J(if N )(J(is_O)( j_ N)' d' <f>~oo< n) u a(d, j_ N /s( + (X, sl- I (Y))))) 
=J(ifN)( _l 8001 ,d,<f>~00 (!1) U a(d,l_Js(+(X,s 1- 1(Y))))) 
<t>~;J(!l)( + )(d, 0) 
= <f>l'o 0 (!1) U a(d,o)(ifN(is_O(Y),X,s( + (X,s 1- 1(Y))))) 
=J(if N )(J(is_O)(O), d, <1>~00 (!1) U a(d,o)(s( +(X, s 1- 1 (Y)) ))) 
=J(ifN)(tt, d,<f>~00 (!1) U a(d,o/s( + (X,s 1- 1(Y))))) 
= d, 

3. Solutions to W-Programs 
525 
so JL<I>AD 0 ( + )(d, _iN) = _i N and JL<I>Aoo< + )(d, 0) = d. The situation is 
more complicated if the second argument is > 0. For example, 
ci>ADD(O)( + )(d, 1) 
=J(ifN)(J(is_O)(l),d,H U a(d,l)(s( + (X,s 1- 1(Y))))) 
= n U a<d.I)(s( +(X, s 1- 1 (Y)))) 
=J(s)(O( + )(d,J(sl 1)0))) 
=J(s)(_i N) 
but if we iterate <I> ADo n + 2 times, n ;;::: 0, we get 
ci>~i(O)(+)(d,1) =ci>~;rJ(O) U a<d.l)(s(+(X,si 1(Y)))) 
=J(s)( ci>~rJ(O)( + )(d, 0)) 
Similarly, 
=J(s)(d) 
= {d + 1 
j_N 
ifd EN 
otherwise. 
cl>fo 0 (!1)( + )(d,2) = ci>ADD(O) U a(d,Z)(s( + (X,s 1- 1(Y)))) 
=J(s)(ci>AD 0 (!1)( + )(d, 1)) 
but if we iterate <I> ADo n + 3 times, n ;;::: 0, we get 
<1>~~(!1)( + )(d,2) = ci>~;i<O) u a<d. 2>(s( + (X,s 1- 1(Y)))) 
=J(s)(ci>~i(O)( + )(d, 1)) 
= {d + 2 
j_N 
ifd EN 
otherwise. 
In general, it can be shown by induction on n that, for any n E N, 
if d, e E Nand 0 :::;; e < n (3_3) 
otherwise, 

526 
Chapter 17 Denotational Semantics of Recursion Equations 
so, for all d, e E N _j_ , 
JL<I>AD 0 ( + )(d, e) = U {<1>~00 (0)( + )(d, e) In EN} 
= {d + e 
..LN 
if d, e EN 
otherwise . 
That is, JL<I>A00( +) is the strict extension of +. 
We can also use the fact that JL<I>Aoo is a fixed point of <I>Aoo to verify 
that JL<I>A00( + )(d, e) = d + 
_j_ e for any given d, e E N _j_ â¢ For example, 
JL<I>Aoo< + )(3, 2) 
= <I>Aoo< JL<I>Aoo)( + )(3, 2) 
= JL<I>ADD U a(3,Z)(ifN(is_O(Y),X,s( + (X,s! 1(Y))))) 
=J(s)( JL<I>ADo( + )(3, 1)) 
=J(s)( <f>ADD( JL<f>ADD )( + )(3, 1)) 
=J(s)(JL<I>ADD U a(3, l)(ifN (is_O(Y), X, s( +(X, s,- 1 (Y))) ))) 
=J(s)(J(s)( JL<I>A00 ( + )(3,0))) 
= J(s)(J(s)( <I>Aoo< JL<I>Aoo)( + )(3, 0))) 
=J(s)(J(s)(JL<I>ADD U a(3,0)(ifN(is_O(Y),X,s( + (X,s! 1(Y))))))) 
= J(s)(J(s)(3)) 
= 5. 
Before we go on we prove the following useful lemma. 
Extension Lemma. Let (9'; J) be a complete, continuous W-structure, 
and let P, Q be W-programs such that P ~ Q. 
1. For all FE FV(P), JL<I>p(F) = JL<I>Q(F). 
2. For all t E TMw(FV(P)), ~(t) = JL<I>Q(t). 
Proof. 
Let F(X 1, ... ,Xn) = t be the defining equation for Fin P. First 
we prove by induction on i that 
<I>~(O)(F) = <I>~(O)(F) 
for all i EN. 
(3.4) 

3. Solutions to W-Programs 
527 
If i = 0 then <I>~{O.)(F) = O.(F) = <I>g(O.)(F), so assume <I>~{O.)(F) = 
<I>Q(O.)(F) and let dE D8<F>Â· Then 
<1>~+ 1(0.)(F)(d) = <1>~(0.) U ait) 
= <1>~(0.) U ait) 
= <l>b+ l(O.)(F)(d), 
by the induction hypothesis and 
the coincidence lemma 
and d is an arbitrary element of D8<FP so we have 
<1>~+ 1 (0.)(F) = 
<I>Q+ 1(0.){F), concluding the induction. Now, 
JL<I>p(F) = 
U{<I>~(O.) I i E N}(F) 
U{<I>~(O.)(F) I i EN} 
U{<I>~(O.)(F) I i EN} 
U{<I>~(O.) I i E N}(F) 
= JL<I>Q(F), 
by the fixed point theorem 
by Theorem 16.3.9 
by (3.4) 
by Theorem 16.3.9 
which completes the proof of part 1. Part 2 follows immediately from part 
1 by the coincidence lemma. 
â¢ 
We have one more step to take before we define the denotational 
semantics of W-programs. In the next section we will select from any 
complete, continuous W-structure l certain objects, the data objects, to 
get a data structure system A. We will then give the denotational seman-
tics of W-programs in terms of A. However, it will turn out that IN is 
already a data structure system, which we will also call AN , so we can 
anticipate the next section and give the denotational semantics in AN for 
WN-programs.4 The idea is to give a single function that assigns a meaning 
to all WN-programs. 
Definition. The denotational meaning function for AN, denoted fg11N, is 
defined 
for all WN-programs P. 
4 The reader who wishes to go on at this point to the chapter on operational semantics will 
be able to read the first two sections of that chapter as they apply to the particular structure 
lN. We simply need to remark that lN is a simple WNÂ·structure (as defined in Section 5 of 
the current chapter) and that aN = rep(lN) = lN (as defined in Section 4 of the current 
chapter) is a simple data structure system for WN. 

528 
Chapter 17 Denotational Semantics of Recursion Equations 
Exercises 
1. 
Let !, = (9';J) be a complete, continuous W-structure, let P be a 
W-program, and let <l>p = <I>i. Prove the following statements. 
(a) 
<1>~(!1) E 'IF.Wy(FV(P)) for all i E N. 
(b) 
{<1>~(!1) I i EN} is a chain in (~.Wy(FV(P)), !;;;,.,_,...:r <FV<PÂ»). [Hint: 
See Lemma 1 in Section 5 of Chapter 16.] 
2. 
Give aWN-program p such that JL<I>iN = n. 
3. Give a WN-program P such that <l>j;N has infinitely many fixed points. 
4. Show that <1>,&0 has exactly one fixed point. 
5. Give a WN-program P with FV(P) = {F} such that JL<I>iN(F) is not 
strict. 
6. 
Prove (3.3). 
7. 
Let !, = !.N and let P be the WN-program with the equation 
F(X) = if N (is_O(X), 2, s(F(s1- 1 (X)))). 
(a) Show by induction on n that, for any n E N, 
<I>~(O)(F)(x) = {x + 2 
.l_N 
for all x E N _j_ â¢ 
if x E N and 0 ::; x < n 
if x = ..L N or x ~ n 
(b) Show that JL<I>p(F) = f _j_, where f(x) = x + 2 for all x EN. 
8. 
Let P be the WN-program with equations 
F(X,Y) = ifN(is_O(Y),X,s(F(X,s! 1(Y)))) 
G(X) = F(X,X) 
H(X) = ifN(is_O(X),s(O),G(H(s1- 1(X)))) 
and let <l>p = <I>iN. 
(a) Let a(F) = Â· _j_ (the strict extension of the multiplication func-
tion), a(G)(x) = x + _j_ 2 for all x EN _j_, and a(H)(x) = 3 for 
all 
x E N _j_. 
What is 
<1>p(a)(F)(3, 5)? 
<1>p(a)(G)(7)? 
<l>p( a )(H)(13)? 
(b) What is <l>~(a)(F)(3,5)? <l>~(a)(G)(7)? <l>~(a)(H)(13)? 
(c) 
Describe <I>~(O)(F), <I>~(O)(G), and <I>~(O)(H) for all i EN. 
(d) Describe JL<I>p(F), JL<I>p(G), and JL<I>p(H). 

3. Solutions to W-Programs 
529 
9. Let P be the WNÂ·program with equations 
F(X) = ifN(is_O(X),s(O),G(X,X)) 
G(X,Y) = ifN(is_O(Y),F(X),s(G(X,s1- 1(X)))) 
and let <l>p = cf>~N. 
(a) Let a(F)(x) = 3 for all x EN j_, and let a{ G)= + j_ 
â¢ What is 
<1>p(a)(F)(3)? <1>p(a)(G)(3,2)? 
(b) What is <l>~(a)(F)(3)? <l>~(a)(G)(3, 2)? 
(c) 
Describe <1>~(0.) for each i E N. 
(d) Describe JL<I>p. 
10. Give a WNÂ·program P such that JL<I>~N(F) = Â· j_ (the strict extension 
of the multiplication function) for some F E FV(P). 
11. Give a WNÂ·program P such that JL<I>~N(F) = F j_, where F(n) is the 
nth Fibonacci number, for some F E FV(P). [See Exercise 8.3 in 
Chapter 3 for the definition of Fibonacci numbers.] 
12. Let &, v, -
be the usual operations on truth values. Give a 
W-program P such that, in any W-structure, 
JL<I>p(&) = & j_, 
JL<I>p(V) = v j_, and JL<I>p(-) =- j_ 
, where 
r(&) = r( V) = Bool X Bool - Bool 
and r(""') = Bool - Bool. 
13. Let l = (.9'; J) be a W-structure. Suppose we extend the standard 
vocabulary W to W' by adding the symbols is_tt, is_tT, and suppose 
we give is_tt, is_tT their natural interpretations J(is_tt), J(is_tT) as 
in condition 5 on Y:interpretations. Give a W-program P with func-
tion variables ls_tt, ls_tT such that JL<I>p(Is_tt) =J(is_tt) and 
JLilip(Is_tT) = J(is_tT). 
14. Let l = (9'""N,~), where~ is given in Exercise 2.6. What is JL<I>fo0 ? 
15.* Let l = (.9';J) be a complete, continuous W-structure, and let P be 
a W-program. Define w;: ..Wg-(FV(P)) ---+ ..Wg-(FV(P)) exactly like <I>~ 
except that its domain is ..Wg-(FV(P)). [This exercise requires the 
results of Exercise 2.14.] 
(a) Show that a E ..Wg-(FV(P)) is a solution to P in l if and only if a 
is a fixed point of w;. 
(b) Show that w; is monotonic. 
(c) 
Give a W-program Q such that 'I'J is not continuous. [Hint: Let 
f be a function and C a chain such that f( U C) =F Uf(C). For 

530 
Chapter 17 Denotational Semantics of Recursion Equations 
all c E C, let ac(F) = f and let ac(G) be the constant function 
ac(G)(x) =c. Put the equation H(X) = F(G(X)) in Q.] 
(d) Show that ('l'i)i(O.) E %'.W'y(FV(P)) for all i EN. [Hint: Use 
Theorem 2.3 in the induction step.] 
(e) 
Show that u{('l'i)i(O.) I i EN} exists and is JLWi. 
4. 
Denotational Semantics of W-Programs 
Next we turn to the treatment of data structures. There are two properties 
that they should satisfy: 
1. Since the semantics of program P is to be based on the function <l>p, 
we want data structures to be rich enough to guarantee the existence 
of J.L<I>p for every program P. 
2. Since we need to be able to specify the inputs to a program, we want 
every element in a data structure to be the meaning of some term. 
These two properties may seem to be contradictory. Property 1 requires 
data structures to have enough elements to give meanings to programs, 
and property 2 requires that data structures not have too many elements. 
We deal with these requirements in two steps. Theorem 3.3 guarantees 
that JL<I>i exists when !. is a complete, continuous W-structure, so we 
begin with such structures and pare them down so that property 2 is 
satisfied. 
Definition. Let !. = (Y,J) be a complete, continuous W-structure. 
1. An element d E DY(T)' for some T E TV(W), is representable in W if 
there is some W-program P and some term t E TMMFV(P)) such 
that d = (JL<I>i)J (t). rep (DY(T)) is the set 
{ d E D.9"(T) I d is representable in w}' 
!;;;rep(Y(T)) is the restriction of !;;;Y(T) to rep (D.'T(T)), and rep (!T) is 
defined 
rep(!T)(T) = (rep(DY(T), !;;;rcp(.'T(T))) 
for all T E TV(W). 
2. For any function 
f E D.9"(T,) X ... X D.9"(T.) ---+ D:T(T)' 
where T1, ... , Tn, T E TV(W), let 
rep( f) E rep(D:T(T,) X Â· Â·Â· X rep(p'T(T) ---+ P<T(T) 

4. Denotational Semantics of W-Programs 
531 
be defined 
rep(f)(dl ' ... ' dn) = f(dl ' ... ' dn) 
for all (d1, â¢â¢â¢ , dn) E rep(p'T(T,)) X Â·Â·Â· X rep(DY(T). Then rep(J) 
is defined 
rep(J)(c) =J(c) 
for all constant symbols c E W 
rep(J)(f) = rep(J(f)) 
for all proper function symbols f E W. 
3. Finally, rep (I) = (rep(.?'), rep (J)) is the data structure system for W 
based on I. 
The point is that an arbitrary W-structure I might contain objects that 
we can never use as data since there is no way to refer to them. Therefore, 
in defining functions that we wish to consider computable, we will restrict 
our attention to the representable objects in rep (I). We might call these 
the data objects of I. 
It is important to understand that even if d1 , â¢â¢â¢ , dn are representable, 
rep (f)(dp ... , dn) may not be representable if f is some arbitrary func-
tion. However, we will show that rep (f)(d1 , â¢â¢â¢ , dn) is representable when 
f is the interpretation J(f) of some f E W. 
When 7 
is understood, we will generally write (D,(T)' !;;;,(T)) for 
rep(.?')( T ), 
D,(T,)X ... Xr(T.l 
for rep(DY(T) XÂ·Â·Â· X rep(DY(T.l), 
D,(T,)X ... x r(T.)""'r(T) for rep(DY(T,)) XÂ·Â·Â· X rep(DY(T.l) ~ rep(DY(T)), 
and, when 8(F) = T 1 XÂ·Â·Â· X Tn -+T, D,( 6(F)) for D,(T,)X ... xr(T.l" 
Let I = (.9'; J) be a W-structure. Then for every constant symbol 
c E W, J(c) is representable since JL<I>i(c) =J(c) for any W-program P. 
Moreover, for every T E TV(W), ..l T is representable: let c E we be a 
constant symbol with T(c) = T, and let P be the W-program with equation 
B(X) = B(X), where T(X) = T and T(B) = T -+T. Then JL<I>i(B(c)) = 
O(B(c)) = ..l T. (This explains, by the way, our requirement that We 
contain a constant symbol of type T for every T E TV(We).) 
In IN, then, it is clear that every element in Bool is representable in 
WN. Moreover, every element in N .L is representable in WN: for all 
n E N, JL<I>;(n) = n, where P is any WN-program, e.g., the empty program. 
(In a case like this we can simply say O(n) = n.) Therefore, rep (Â§N) = .9N, 
rep(JN) =JN, and rep(IN) =IN. We will write AN for IN when we 
want to emphasize that IN is a data structure system for WN. 
Now, let Vr be a set of function variables, and let d = JL<I>i(t). It is 
useful to note that simply by changing the function variables in P and t, we 
can always find a W-program Q and a term u E TMw(FV(Q)) such that 

532 
Chapter 17 Denotatlonal Semantics of Recursion Equations 
FV(Q) n Jj = 0 and JLcl>~(u) = JLcl>~(t). Therefore, given representable 
elements d1 , â¢â¢â¢ , dn, we can always find W-programs P1 , â¢â¢â¢ , Pn such that 
FV(P;) n FV(lj) = 0, 1 ::::; i < j ::::; n, and terms t 1 , â¢â¢â¢ , tn such that t; E 
TMw(FV(P;)) and d; = JL<I>~(t;), 1 ::::; i ::::; n. We will say that P1 , â¢â¢â¢ , Pn are 
consistent if FV(P;) n FV(lj) = 0, 1 ::::; i < j::::; n. 
The first thing we need to do is show that data structure systems are 
W-structures. We begin with a lemma that shows that, for all proper 
function symbols f E W with T{f) = T 1 X Â· Â· Â· X T n -+ T, rep (J) (f) E 
Dr(T 1) X Â·Â·Â· X Dr(T.) ~ Dr(T). In other words, data structure systems are 
closed under the interpretations of the function symbols. 
Lemma 1. Let !. = (.5T,J) be a complete, continuous W-structure, and 
let fEw with T(f) = Tl X ... X Tn -+T. If (dl ' ... ' dn) E Dr(TJ)X 00. X r(T.)' 
then rep {J){f){d1 , â¢â¢â¢ , dn) E Dr(T). 
Proof. Let d; = ( JL<I>P)J (t;), 1 ::::; i ::::; n, where P1 , â¢â¢â¢ , Pn are consistent. 
Then P = U ?~ 1 P; is a W-program, and 
{ JLcl>p)J {f(tl, ... , tn)) 
= J{f){ ( JL<I>p )J{tl)' ... ' { JLcl>p)J{tn)) 
=J{f)( { JL<I>p)J{tl ), â¢ â¢ â¢, { JLcl>p)J{tn)) 
=J{f){dl 'â¢ â¢ â¢' dn) 
by the extension lemma 
= rep(J){f){d1 , â¢â¢â¢ , dn) 
since (dl ' ... ' dn) E Dr(T))X ... X r(T.) â¢ 
â¢ 
We will use the next lemma when considering the interpretations of the 
built-in function symbols is_f and f;- 1. 
Lemma 2. Let !. = (.57, J) be a complete, continuous W-structure, and 
let f Ewe with T{f) = Tl X ... X Tn -+T. If(dl , ... , dn) E DTJX ... XTn and 
J{f){dl ' ... ' dn) E Dr(T) -
{ .l T}, then (dl ' ... ' dn) E Dr(TJ)X ... X r(Tn). 
Proof. Let J(f){d1 , â¢â¢â¢ , dn) = ( JL<I>p)J {t). Then for 1 ::::; i ::::; n, 
( JLWp)J(r;-1 (t)) =J(f;- 1 )( ( JL<I>p)J (t)) 
=J{f;-1 ){J(f){dl 'â¢ â¢ â¢' dn)) 
â¢ 
Theorem 4.1. Let !. = (.57, J) be a complete, continuous W-structure. 
Then rep(!,) is a W-structure. 

4. Denotatlonal Semantics of W-Programs 
533 
Proof. 
For any T E TV(W), (Dr(T)' !;;;r(T)) is clearly a partial order with 
bottom element 
_l_ T , and ( Dr(Bool), !;;;r(Bool)) = 9'"(Bool), so rep ( 9'") is a 
type assignment for W. 
Now we need to show that rep (J) is a rep (g)-interpretation for W. 
Note that in the context of the type assignment rep (.9'), each reference to 
a set DT in the definition of Y-interpretations should be understood as 
referring to rep (D7 (T)), i.e., Dr(T). Also, each reference there to J should 
be understood as a reference to rep (J). It is clear that rep (J) (c) = J(c) 
for all constant symbols 
C E We, SO condition 1 in the definition 
of .9=interpretations is Satisfied. If f E We is a proper function symbol 
with T(f) = T 1 X Â·Â·Â· X Tn -
T, and d; E Dr(T;)' 1 ::; i ::; n, then 
rep (J)(f)(d1 , â¢â¢â¢ , dn) E Dr(T) by Lemma 1, so condition 2a is satisfied. 
Conditions 2b, 2c, and 3 follow immediately from the definition of rep (J). 
Condition 4 follows immediately from Lemma 1 and the definition of 
rep (J), as does condition 5 for all constant symbols in We-, SO let f E We 
with T(f) = T 1 XÂ·Â·Â· X Tn-T, and let dE Dr(T)" If d = _l_ T 
then 
rep (J)(is_f)(d) = J(is_f)(d) = _l_ Bool, so assume d -=/= _l_ T â¢ If d E 
ran(rep (J)(f)), then obviously d E ran J(f), so that rep (J)(is_f)(d) = 
J(is_f)(d) = tt. Now, if d E ran J(f), then d =J(f)(d1 , â¢â¢â¢ , dn) for some 
(d 1 , â¢â¢â¢ ,dn) E Dr< 5<rn by Lemma 2, sod E ran(rep(J)(f)). Therefore, if 
d ft. ran(rep (J)(f)), then 
d ft. ran J(f), and rep (J)(is_f)(d) = 
J(is_f)(d) = ff, so condition 5 is satisfied. Finally, condition 6 is satisfied 
by a similar argument, so rep (J) is a rep (.57)-interpretation, and rep(!,) 
is a W-structure. 
â¢ 
We can now define the denotational semantics of recursion programs. 
For a complete, continuous W-structure (9';J) and a variable assignment 
a E.JÂ¥'7 (V), where V is a set of function variables, let rep(a) be the 
function on V defined by 
rep(a)(F) = rep(a(F)) 
for all FE V. 
Note that the domain of a(F) is Dr< 5(F)) for all F E V, but rep (a) is a 
variable assignment in ~ep(.9")(V) if and only if 
rep( a )(F )(d) E Dr< p(F)) 
for all F E Vandall d E Dr< 5<FÂ». 
Definition. Let !, be a complete, continuous W-structure, and let A = 
rep(!,). The denotational meaning function for A, denoted 9Jt:., is defined 
9Jt:.(P) = rep( JL<I>~) 
for all W-programs P. 
For a W-program P and F E FV(P), we have 
9Jt:.(P)(F) =rep( JL<I>~)(F) =rep( JL<I>~(F)). 

534 
Chapter 17 Denotational Semantics of Recursion Equations 
The point is that, rather than taking JL<I>~(F) as the function assigned to 
F, we assign to F a function whose domain consists only of representable 
objects. 
We showed that JL<I>~ is a solution to P in any complete, continuous 
W-structure I. We now show that 9111(P) is a solution to P in a = rep (I). 
That is, we still have a solution when we restrict our attention to the data 
structure system based on I. We begin with three lemmas that let us 
ignore nonrepresentable objects when applying JL<I>~ to terms. In particu-
lar, we want to show that if {3 E.W:.ep(.'7J(V), where Vis a set of individual 
variables, then for any term t E TMw(FV(P) U V), 
(JL<I>~ U {3)5 (t) = (rep(JL<I>~) U f3)rcp(J)(t). 
It will follow easily, then, that 9111(P) = rep ( JL<I>~) is a solution to Pin a. 
Lemma 3. Let I = (Y,J) be a complete, continuous W-structure, let P 
be a W-program, let V ~ VAR 1, and let a E Jlfrcp(.'7J(V). Then for any 
term t E TMw(V U FV(P)), ( JL<I>~ U a)5 (t) E Dr(T(t))Â· 
Proof. 
We have 
JL<I>~ U a E ~'T(V U FV(P)), which implies 
( JL<I>~ u a )5 (t) E DT(t) by Theorem 2.1, so we need to show only that 
( JL<I>~ u a ~Y (t) is representable. We argue by structural induction on t. If 
t is a constant symbol c E W, then ( JL<I>~ u a ~Y (c) is clearly repre-
sentable, and if tis X E V, then ( JL<I>~ u a ~Y (X) = a(X) is representable 
by assumption. If t is f( t 1 , â¢â¢â¢ , tn ), where f E W, then 
( JL<I>i U a )J (f(t 1 , â¢â¢â¢ , tn)) 
=J(f)((JL<I>i U a)J(t 1), â¢â¢â¢ ,(JL<I>~ U a)J(tn)) 
= J(f)( ( JL<I>~, L.- (u 1 ), â¢â¢â¢ , ( JL<I>~.)f (un)) 
for some P;, u;, 1 ::;; i ::;; n, by the induction 
hypothesis, where P1 , â¢â¢â¢ , Pn are consistent 
by the extension lemma, where P0 = U:'~ 1 P; 
= ( JL<I>~o)J (f(u 1 , â¢â¢â¢ , un)) 
E DT(t) by Theorem 2.1, 
and ( JL <I>~" ).Y (f( u 1 , â¢â¢â¢ , u n)) is representable, so it is in D,< T(t)) â¢ The argu-
ment is similar if tis F(t 1 , â¢â¢â¢ , t) with F E FV(P). 
â¢ 

4. Denotational Semantics of W-Programs 
535 
Lemma 4. Let I = (Y,J) be a complete, continuous W-structure, let 
a= rep(I), and let P be a W-program. Then rep( JL<I>~) E~cp(.'TJ(FV(P)). 
Proof. Let F(Xu ... , X) = t be an equation in P, and let d E D,<~<F Â». 
Then 
rep( JL<I>~)(F)(d) =rep( JL<I>~(F))(d) 
= JL<I>~(F)(d) 
since d E D,< S(F Â» 
= ( JL<I>~ U ad)J (t) 
E Dr(T(I)) 
since JL<I>~ is a solution to P 
by Lemma 3. 
â¢ 
Lemma 5. Let I = (Y, J) be a complete, continuous W-structure, let 
V; ~ VAR 1 and ~ ~ VARF, let a E.JÂ¥'7(~) be such that rep(a) E 
~cp(YJ(~), and let {3 E~cp(Yl(V;). Then for any term t E TMw(~ U V;), 
(a U {3 )J (t) = (rep( a) U {3 )rep (J)(t). 
Proof. 
Note that rep(a) U {3 E~ep(Yl(~ U V;), so by Theorems 2.1 and 
4.1, 
(rep (a) U {3 )rcp(J)(t) E Dr(T(t)) 
for any t E TMw(~ U V;). (4.1) 
We argue by structural induction on t. If t is a constant symbol c E W, 
then 
(aU f3)J(c) =J(c) = rep(J)(c) = (rep(a) U f3)rep(Jl(c), 
and if tis X E V;, then 
(aU f3)J(X) = {3(X) = (rep(a) U f3)rep(J)(X). 
If't is f(t 1,. .. , t), where fEW, then 
(aU f3)J(f(tp ... ,tn)) 
=J(f){(a U f3)J(t 1), ... ,(a U f3)J(tn)) 
=J(f)((rep(a) U f3)rep(J)(t 1), ... ,(rep(a) U f3)rep(J)(tn)) 
by the induction hypothesis 
= rep(J)(f)((rep(a) U f3)rep(J)(t 1), ... ,(rep(a) U f3)rep(J)(tn)) 
by (4.1) 
The argument is similar if t is F( t 1 , â¢â¢â¢ , t n), where F E ~. 
â¢ 

536 
Chapter 17 Denotatlonal Semantics of Recursion Equations 
Theorem 4.2. Let ~ = (9'; J) be a complete, continuous W-structure, 
and let a = rep(~). Then for any W-program P, g-ip) is a solution toP in 
a. 
Proof. 
Let p be a W-program, let F( XI' ... ' xn) = t be an equation in P, 
and let d E D,< S(F Â». Then 
sg~(P)(F)(d) =rep ( JLci>~)(F)(d) 
= ( JLcl>~ U ad)J (t) 
as in the proof of Lemma 4 
= (rep(JLcl>~) U ad)rep(Jl(t) 
by Lemmas 4 and 5 
â¢ 
Let a be a data structure system for W. Now that we have a meaning in 
a for every W-program, it makes sense to ask if a given W-program P 
defines the functions we want it to define. That is, we can ask if P is 
correct. Determining that a program is correct is known as program 
verification. 
Definition. Let a be a data structure system for W, let P be a W-pro-
gram, and let f E Dr(Tt)X ... X r(Tn)-r(T) for some Tl' ... ' Tn' T E TV(W). 
We say that Pis partially co"ect with respect to f if 
for some F E P, and we say that P is totally co"ect with respect to f if 
giP)(F) = f for some FE P. 
For example, we indicated in the previous section that JL<I>fj;0 ( +) = 
+ .l , and g~JADD) = JL<I>fi;0 since aN = ~N, so ADD is totally correct 
with respect to + .l â¢ Recall that the correctness argument for ADD was 
based on ordinary induction. We will now give an application of fixed point 
induction in establishing a partial correctness result. 
Let eq: N .l X N .l ~ Bool (where Bool = { ..l Boo! , TRUE, FALSE} here) 
be the strict function defined by 
{
TRUE 
eq(x, y) = 
FALSE 
..l Bool 
if X, y E N and X = y 
if x, y E N and x =/= y 
otherwise, 

4. Denotational Semantics of W-Programs 
537 
and let EQ be the WN-program with the equation 
E(X, Y) = if 8001(is_O(X), if 8001 (is_O(Y), tt, ff), E(s 1- 1 (X), s1- 1 (Y))). 
Writing Â¥5'.<# for Â¥f'.W'yN({E}), let aeq E Â¥5'.<# be the assignment aeq(E) = eq, 
and let P(x) be the predicate on Â¥5'.<# defined by 
P(a) ={TRUE 
FALSE 
if a ~@".<>' aeq 
otherwise. 
If .W' is a chain in ( Â¥5'.<#, ~w.w) such that P( a) holds for all a E.<#, then 
clearly P( U .<#) holds, so P(x) is admissible. We want to show by fixed 
point induction that P( f.t<I>EQ) holds. Let n 
= n@".<>'. It is obvious that 
P(O) holds, so we assume P(<I>~Q(O)) and show that P(<I>~"Q\!1)) holds. 
We have <I>~Q(O) ~w.w aeq by the induction hypothesis, which implies 
<1>~0
1 (!1) ~@".<>' <I>EQ(aeq) by the monotonicity of <I>EQ' so if we can show 
that. <I>EQ(aeq) ~w.w aeq' then we will have 
<1>~0
1 (!1) ~@".<>' aeq' i.e., 
P(<l>~~1 (!1)). If x = .l N then it is easy to see that 
<I>EQ(aeq)(E)(x,y) =.l 8001 = ae/E)(x,y), 
so assume that x E N. If x = 0, then 
{
TRUE 
<I>EQ(aeq)(E)(x,y) = 
FALSE 
.l Bool 
if y =X 
if y E N and y =F x 
if y = .l N 
= aeq(E)(x,y), 
and ff x > 0, then 
{
TRUE 
= 
FALSE 
.l Bool 
~Bool ae/E)(x,y). 
if X= y 
if y E N -
{0} and x =F y 
otherwise 
This concludes the proof of P( <1>~0
1 (!1)), so P( f.t<I>EQ) holds by fixed point 
induction. Therefore, 
and EQ is partially correct with respect to eq. 

538 
Chapter 17 Denotatlonal Semantics of Recursion Equations 
Exercises 
1. 
Let P1 = {F(X) = F(s(X))}, P2 = {F(X) = s(F(X))} be WN-pro-
grams. Give consistent WN-programs Q1 , Q2 such that, in any WN-
structure l, JL<I>ij 1 uQ 2(G 1) = JL<I>'i;1(F) and JL<I>~ 1 uQ 2(G 2 ) = JL<I>'i;2(F). 
2. Give a standard vocabulary W = We u B(We) and a W-structure 
l = (.9; J) such that, for some T E TV(W), there is an element 
d E DT -
{ ..l T} that is not in the range of J(f) for any f E We. 
3. Give aWN-structure l such that rep(l) =Fl. 
4. Show that Lemma 2 is not necessarily true for all f E W. 
5. 
(a) Give a standard vocabulary W, a W-structure l = (.9; J), and a 
function f E D T 1 ~ D Tz , for some T I> T 2 E TV(W), such that 
rep(/) f/:. Dr( Ttl ~ Dr(Tz). 
(b) Give an assignment a E .JÂ¥'7 ({F}), where F is a function variable, 
such that rep(a) f/:..W..ep(.9"J({F}). 
6. 
Let l = (.9; J) be a complete, continuous W-structure, let V oe a 
set of function variables, and let a, {3 E ~.>&(r(V). Show that if 
a !;;;;>r.w:, (V > {3, then rep (a )(F) !;;;;,< 5(F Â» .... r<F > rep ( {3 )(F) for all F E V. 
7. 
Show that EQ is not totally correct with respect to eq. 
8. 
Use fixed point induction to show that ADD is partially correct with 
respect to + .l â¢ 
9. 
Let l = (g; J) be a complete, continuous W-structure such that 
rep(l) = l, let P be a W-program, and let a E ~w_'7 (FV(P)) be a 
solution to P in l. 
(a) Show that JL<I>i; !;;;;w.w,(FV(P))a. 
(b) Show that for all F E FV(P), Pis partially correct with respect to 
a(F). 
10.* Let l = (Y,J) be a complete, continuous W-structure, and let 
nl = nW.~q(VARtl" For any T E TV(W), we will say that an element 
dE DT is constructed if d = TI";(t) for some t E â¢w,(VAR 1). 
(a) Let P be a W-program. Show by induction on i that for all 
V ~ V AR 1 , all {3 E .JÂ¥'7 ( V) such that {3(X) is constructed for all 
X E V, and all t E TMw(FV(P) u V), <l>~(!l) u {3(t) is con-
structed. [Hint: For cases i = 0 and i = k + 1, argue by struc-
tural induction on t.] 
(b) Let T E TV(W), and let d E D,(Tl. Show that d = U C for some 
chain C of constructed elements. [Hint: Use part (a) with 
V= 0.] 
(c) 
Let T E TV(W). Show that for every d E D,(T) -
{ ..l T}, d E 
ran J(f) for some f E We. [Hint: See Exercise 2.9.] Compare 
with Exercise 2. 

5. Simple Data Structure Systems 
539 
5. 
Simple Data Structure Systems 
So far AN is the only data structure system we have seen, so in this section 
we give some more examples. In particular, we look at a rather simple 
form of data structure system. 
Definition. Let !. = (.9'; J) be a W-structure. !. is a simple W-structure if 
1. Y( T) is a flat cpo for all T E TV(W), and 
2. J(f) is Strict for every proper constructor function symbol f E We. 
A = rep(!.) is a simple data structure system for W if !. is a simple 
W-structure. 
It is easy to see that any simple W-structure is complete and continuous. 
Clearly, AN is simple. For another example, we extend AN with tuples of 
natural numbers to create a simple WNL -structure, where WNL is the 
vocabulary for lists described in Section 1. Let TUP(N) be the set of all 
tuples of natural numbers, including the "empty" tuple ( ), and let 
(TUP(N) _]_, !;;;TUP(N)") be the flat cpo on TUP(N). We define !.NL = 
(~L,JNL) as follows: 
YNL (Boot) = YN (Bool) 
~L(N) = YN(N) 
~L(NL) = (TUP(N) _]_, !;;;TUP(N)") 
JNL(O) = 0 
JNL(s) = S _!_ 
JNL(nil) = () 
JNL(cons) =cons_]_ 
(the strict extension of cons) 
where cons: N X TUP(N) ~ TUP(N) is defined 
For built-in function symbols f, JNL(f) is defined according to conditions 
4-6 on Y.:interpretations. It is easy to check that !.NL is a simple W-struc-
ture. Moreover, we have ll(nil) = ( ), and for any tuple (m 1 , 0 â¢â¢ , mn) we 
have 
O(cons(m1 , cons(m 2 , â¢ â¢ â¢ cons(mn, nil) Â· Â· Â· ) ) ) = (m 1 , â¢â¢â¢ , mn) o 
Therefore, every element in TUP(N) _]_ 
is representable, so we have 
!.NL = rep (!.NL), and !.NL is a simple data structure system for WNL. We 

540 
Chapter 17 Denotatlonal Semantics of Recursion Equations 
will write ANL for INL when we are interested in INL as a data structure 
system. 
Now, let LIST be the WNL-program with equations 
Length(X) = ifN(is_nii(X), 0, s(Length(cons; 1 (X)))) 
Nth(X, Y) = ifN(is_O(X),cons 1- 1(Y),Nth(s! 1(X),cons; 1(Y))) 
Cat(X, Y) = ifNL(is_nii(X), Y, cons(cons! 1 (X), Cat(cons; 1 (X), Y))) 
Rev(X) = ifNL(is_nii(X), 
X, Cat(Rev( cons2 1 (X)), 
cons(cons 1- 1 (X), nil))). 
Then f:g. (LIST)(Length) evaluates the length of a list. More precisely, if 
'-'NL 
len((m 1 , â¢â¢â¢ , mn)) = n for any list (m 1 , â¢â¢â¢ , mn) E TUP(N), then 
gll.NL(LIST)(Length) = len _L â¢ It is clear that g~~.N,~LIST)(Length) is strict, 
so we argue by induction on the length of lists: 
g~~. (LIST)(Length)(( )) 
Nl. 
= JL<I>usT(Length)(( )) 
= <l>usT( JL<I>usT)(Length)(( )) 
= JL<I>usT u a 0 (ifN(is_nii(X),O,s(Length(cons; 1(X))))) 
= JNL(if N ){JNL(is_nil){( )), 0, JLCI>usT U a< l(s(Length(cons; 1 (X))))) 
= 0, 
and 
gll.NL(LIST)(Length)((m 1 , â¢â¢â¢ , mn+ 1)) 
= JLCI>usT U a(m,, ... ,m.+ 1>(ifN(is_nii(X),O,s(Length(cons; 1(X))))) 
=JNL(s)(JL<I>usT(Length)(JNL(cons; 1 )((m 1 , â¢â¢â¢ , mn + 1)))) 
=JNL{s)( JL<f>LIST(Length)((m2, ... , mn + 1 ))) 
=JNL(s){n) 
by the induction hypothesis 
= n + 1. 
We leave it to the reader to verify that gll.N,_(LIST)(Nth)(n, I) returns the 
nth element, starting from 0, of list I (if it exists); gll.N,_(LIST)(Cat) 
concatenates two lists; and gll. (LIST)(Rev) reverses a list. 
Nl. 

5. Simple Data Structure Systems 
541 
In the previous paragraph we referred to lists rather than tuples. Why? 
The real question is, what is a list? The vocabulary WNL was created to let 
us name lists of numbers, based on our intuitive understanding of the 
nature of lists. Our point of view is that a list of numbers is just an element 
of Y(NL), where (9'; J) is any WNL -structure. This is essentially an 
axiomatic approach, where we express the properties we expect from our 
data objects, without specifying just what those objects are. So TUP(N) _L 
is one set of objects that can serve as lists of numbers, but there are 
others. An alternative WNL-structure will be given by the construction 
preceding Theorem 5.1. 
For a third example, we show that .9 programs, defined in Chapter 2, 
can be incorporated into a simple data structure system. We start with type 
variables Bool, N, V, L, S, I, and P, to be assigned truth values, numbers, 
variables (of .9), labels, statements, instructions, and .9 programs, respec-
tively. For a constructor vocabulary we take 
where 
\Â¥.:. = {tt, ff, 0, s, var, lab, skip, incr, deer, goto, unlab_instr, 
lab_instr, empty, cons}, 
T(O) = N 
T(var) = N -+V 
T0ab) = N -+L 
T(Skip) = V -+S 
T(incr) = V -+S 
T(decr) = V -+S 
T(s)=N-+N 
T(goto) = V X L -+S 
T(unlab_instr) = S -+1 
T(lab_instr) = LX S-+ I 
T(empty) = P 
T(cons) =I X P -+P, 
and we set Wy, = We U B(Wc). Now we set ~,( T) = SfN( T) for T = Bool, N, 
and we set ~,( T) to be the flat cpa on .9 variables, labels, statements, 
instructions, and .9 programs for 
T = V, L, S, I, P, respectively. Let 
V0 , V1 , â¢â¢â¢ and L 0 , L 1 , â¢â¢â¢ enumerate the .9 variables and labels, as in the 
beginning of Chapter 4 (except that we begin counting at 0 rather than 1). 
Then for the constructor symbols we define ._Yy, as follows: 
...Yy,(O) = 0 
._Yy,(s) = s _L 
..;:y,(var) = var .L 
..;:y,(lab) = lab _L 
..;:.,,(skip) = skip .L 
..;:.,,(incr) = incr _L 
..;:.,,(deer) = deer _L 
..;:y,(goto) = goto .L 
..;:.,,(unlab_instr) = unlab_instr .L 
..;:.,,(lab_instr) = lab_instr .L 
..;:y,(empty) = empty program 
..;:.,,(cons) = cons _L 
, 

542 
where 
var(n) = V, 
lab(n) = Ln 
Chapter 17 Denotational Semantics of Recursion Equations 
goto(V, L) =IF V * 0 GOTO L 
unlab_instr(S) = S 
skip(n) = V +--- V 
incr(V) = V +--- V + 1 
deer( V) = V +--- V - 1 
lab_instr(L, S) = [L] 
S 
For example, 
a 
O(cons( lab_instr( lab( 0 ), deer( var( 1)) ), 
cons( unlab_instr( incr( var( 0)) ), 
cons( unlab_instr( goto( var( 1 ), lab( 0))) 
empty)))) 
[A] 
X+-X- 1 
Y+-Y+l 
IF Xo!= OGOTOA 
dy = (rep(~,), rep(..;:,,>) = (~,, ..;:y,) is the simple data structure system 
of .9' programs. 
One of the standard problems in programming language theory is to 
show the existence of structures that satisfy a given set of conditions. We 
can ask, for example, is there a strict data structure system for every 
standard vocabulary? We will show that there is. We define Ir(W) = 
(.9';-(W)â¢~(w), the simple Herbrand W-structure, as follows.5 For each 
T E TV(W), let .9';-(W)( T) be the flat cpa on TM~, . We will write ..l T for 
the bottom element of g;.(W)( T ). Next we define ,y;,.(W): 
~(W)(c) = c for each constant symbol c E W,. 
if t; * ..l T,, 1 :::;; i :::;; n 
otherwise, 
for each f E We with T(f) = T 1 X Â· Â· Â· X Tn -+T. For each f E B(W,.), ~(W)(f) 
is defined according to conditions 4-6 on Y-interpretations. It is clear that 
I;r(W) is a simple W-structure, and an easy structural induction shows that 
5 The idea of creating structures based on terms comes from the field of mathematical 
logic. For example, Herbrand unicerses are defined and play a significant role in Chapter 13. 

5. Simple Data Structure Systems 
543 
O(t) = t for all t E â¢w,, so every element in IJI''(W) is representable. 
Therefore, a%'(W) = rep (l_;r(W)) = I.:r(W) is a simple data structure system 
for W, and we have proved 
Theorem 5.1. There is a simple data structure system for every standard 
vocabulary. 
Exercises 
I. Show that any simple W-structure is complete and continuous. 
2. Let I = (Y,J) be a simple W-structure, and let f Ewe with T(f) = 
T -T. What is JL.f(f)? 
3. Show that INL is a simple WNL-structure. 
4. Show by induction on n that 
{ m. 
~11 (LIST)(Nth)(i, (m 0 , â¢â¢â¢ , mn)) = 
..L' 
NL 
N 
5. Show by induction on n that 
if 0:::;; i:::;; n 
if i > n. 
~t..NL(LIST)(Cat)((l 1
, â¢â¢â¢ , In), (m 1 , â¢â¢â¢ , m,)) = (/1 â¢â¢ â¢ In, m 1 , â¢â¢â¢ , m,). 
6. 
Using Exercise 5, show by induction on n that 
~t..NL(LIST)(Rev)((m 1 , â¢â¢â¢ ,mn)) = (mn , ... ,m1). 
7. Give a WNL-program P such that 
8. Give the ~yAerm t such that O(t) is 
[Cd 
Z 2 +--- Z 3 -
1 
n 
IF Z 3 =I= 0 GOTO C1 
9. 
Describe I.:r(WNd. 
10. Complete the proof of Theorem 5.1 by showing that rep CI-r(W)) = 
I.:r(W) for any standard vocabulary W. 
11.* Let I = (Y,J) be a W-structure. For any T E TV(W), we will say 
that an element d E DT is ground if d = ..L T or d = O(t) for some 

544 
Chapter 17 Denotational Semantics of Recursion Equations 
t E TMw , and we will say that I is term-generated if for all T E 
TV(W), e~ery element of DT is ground. 
(a) Show that there is a term-generated W-structure for every 
standard vocabulary W. 
(b) Give a standard vocabulary W and a simple W-structure that is 
not term-generated. 
(c) 
Let I = (9'; J) be a simple W-structure, and let P be a W-pro-
gram. Show by induction on i that for all V ~ VAR1, all 
{3 E .Wy(V) such that {3(X) is ground for all X E V, and all 
t E TMw(FV(P) U V), 
<1>~(.0) U {3(t) is ground. [Hint: For 
cases i = 0 and i = k + 1, argue by structural induction on t.] 
(d) Let I = (9';J) be a simple W-structure. Show that rep(I) is 
term-generated. [Hint: Use part (c) with V = 0.] 
12.* Let I= (9';J), I' = (::T',J') be W-structures. We say that I, I' 
are isomorphic if there is a set of functions {JT I T E TV(W)} such 
that 
â¢ for all T E TV(W), fT is an isomorphism from ::T( T) to ::T'( T ), 
â¢ for all constant symbols c E We with r(c) = T, fT(J(c)) = ::T'(c), 
and 
e for all proper function symbols f E We with r(f) = T 1 X Â· Â· Â· X 
Tn -+T, 
/T(J(f)(d) , ... ,dn)) =J'(f)(/T,(d)), ... ,fT.(dn)) 
for all (dp ... ,dn) E D5<r>Â· 
[See Exercise 3.18 in Chapter 16 for the definition of isomorphic 
partial orders.] 
(a) Show that IN, I2'<WN> are isomorphic. 
(b) Show that INL, I2'(WNd are isomorphic. 
(c) 
Let I be a simple W-structure. Use Exercise 11 to show that 
rep (I), I2'(W) are isomorphic. 
6. 
lnfinitary Data Structure Systems 
Simple data structure systems are too elementary to demonstrate the 
power of the framework we developed in Chapter 16, so in this section we 
look at more complex systems. 
Definition. Let I = (9'; J) be a complete, continuous W-structure. I is 
an infinitary W-structure if for every proper constructor function symbol 

6. lnfinltary Data Structure Systems 
545 
f E We , ..l p(f) f/=. ran J(f). d = rep ( !_) is an in finitary data Structure system 
for W if !. is an infinitary W-structure. 
Suppose we try to define, as an interpretation J(s) for s E WN, a 
continuous successor function s"' for N _L such that 
..L N f/=. ran s"". If s"' 
and s, the ordinary successor function, agree on N then we still get the 
positive natural numbers s""(O), s"'(s"'(O)), ... by repeated application of s"' 
to 0, but we get other objects as well. Since 
..L N f/=. ran s"", we get 
..L N c s"'( ..L N ), which implies s""( ..L N) c s""(s""( ..L N )) by the monotonicity 
of s"" and by condition 2b on Y:interpretations. The idea is that we know 
that s""(..L N) is the successor of something, but that is all we know. Now, 
s"'(s""( ..L N )) is also the successor of something, but we also know that it is 
the successor of something that is the successor of something, so 
s"'(s""( ..L N )) is more defined than s""( ..L N ). Moreover, an object like s""( ..L N) 
must be different than every natural number: s""( ..L N) = 0 would violate 
condition 3 on Y:interpretations, and s""( ..L N) = n + 1 = s""(n) would vio-
late condition 2b. What we get, therefore, is an infinite chain of distinct 
new objects: 
But then we need yet another new object u {(s"');( ..L N) I i E N} if we are to 
have a cpo. 
It is not at all obvious, then, that infinitary W-structures exist. In fact, we 
will show that they do. We begin by defining a variation on the Herbrand 
structures of Section 5. For each T E TV(W), we create a new constant 
symbol6 J.T with T(J.T) = T, and we set 
For each T E TV(W) we define the ordering !;;;T+ on TM~+ as follows: 
(
t =.l.T ort = uor 
[t = f(t 1 , â¢â¢â¢ ,tn) and u = f(u 1 , â¢â¢â¢ ,un), for 
t c + u if and only if 
. 
-T 
some f Ewe Wtth T(f) = Tl X ... X Tn -+T, and 
t; !;;;T~ D;, 1 :::;; i :::;; n]. 
I 
For WN we have w~ = {tt, ff, 0, s, J.Bool' J.N}, TM~t = {J.Boolâ¢ tt, ft'}, and 
6 Note that the symbol .l.T is introduced into the semantics of W-programs. The vocabu-
lary W remains the same. That is, .l.T cannot appear in a W-program. 

546 
Chapter 17 Denotational Semantics of Recursion Equations 
TM~~ = {s;(O) I i EN} u {si(l.N) I i EN}. The definition of 
~;;;N. im-
plies, for example, 
.l.N ~;;;N. s;(O) 
for all i EN, 
.l.N !;;;N+ s(.l.N) I;;;N+ s(s(.l.N)) I;;;N+ s(s(O)), and 
.l.N I;;;N+ S(.l.N) I;;;N+ s(S(.l.N)) I;;;N+ s(s(s(.l.N))) I;;;N+ 
Now, for each T E TV(W), (TM~+, I;;;T+ ) is a partial order with bottom 
element l.T, but we still have the problem that (TM~+, I;;;T+ ) is not, in 
general, a cpo. For example, {si(l.N) I i EN} is a chain in (TM~~, I;;;N+ ) 
without a least upper bound. Here is where we apply the ideal construc-
tion. 
Definition. The Herbrand ideal type assignment for W, denoted g;~<W>â¢ is 
defined, for all T E TV(W), 
.9';~(W)( T) = (id(TM~+ ), ~ id(TMi.+ >). 
(As usual, we are writing id(TM~+) for id(TM~+ , I;;;T+ ).) The Herbrand 
ideal 9';~(Wfinterpretation, denoted ~Â·(W)â¢ is defined 
e for all COnstant symbols C E We, With T(C) = T, 
e for all proper function symbols f EWe, with T(f) = T1 X Â·Â·Â· X Tn -+T, 
~Â·(W){f){/1 ' ... 'In) = {f(tp ... 'tn) It; E I;' 1 ::; i ::; n} u {.l.T} 
for all (/1 , ... , In) E id(TMwÂ·) X Â·Â·Â· X id(TM~. ); 
â¢ for all f E B(We), ~~(W)(f) is defined according to conditions 4-6 on 
9'=interpretations. 
The Herbrand ideal W-structure, denoted I..r~<W>â¢ is (g;.<W>â¢~'<w)Â· 
When W is understood we will write (!T"',J"') for (.9';~<W>Â·~Â·<W>) 
(DT., ~;;;T.) for g;.(W)( T ), and ..l Tâ¢ for the bottom element of (DT., ~;;;T. ). 
As usual, we will write tt for J""(tt) and ff for J""(tT). 
For a first example we consider I..rÂ·(WN> = (g;.<WN>'~~<WN>), which 
we will write as I~. There are three kinds of elements in DN., including 
two kinds of principal ideals. For each numeral n there is pid(n) = 
{si( l. N) I i ::; n} u {n}, and for each term of the form sn( l. N) there is 
pid(sn(l.N)) = {si(l.N) I i::; n}. For all numerals n we will write n for 
pid(n), e.g., 3 = pid(3), where pid(n) is distinguished from the natural 

6. lnfinitary Data Structure Systems 
547 
number n by context. Somewhat ambiguously, perhaps, we will call these 
objects numbers. [Indeed, by our discussion about lists in Section 5, we 
could make a case that pid(3) is the natural number 3.] Also, we will write 
n _j_ for pid(sn( l. N )). These two kinds of objects look very similar, but the 
significant difference is that no object n can occur in an infinite chain. 
This is because n is the greatest element of n, so n cannot be a proper 
subset of any larger ideal. Therefore, there is no object d distinct from n 
such that n !;;;N~ d. In other words, n is not an approximation of any other 
element, so we can say that n is completely defined. On the other hand, 
{n _j_ I n E N} is an infinite chain, and 
u {n _j_ I n E N} = u {n _j_ I n E N} 
= U{pid(sn(.LN))In EN} 
= {sn(.LN) In EN}. 
In fact, {sn( l. N) I n E N} is the unique infinite ideal of Y"'(N), and we will 
write it as w. Clearly, w is not a principal ideal. 
We will now show that, for any standard vocabulary W, !.2'~<W> is an 
infinitary W-structure. 
Lemma 1. Let IE DT~ for some T E TV(W). 
1. If c E /, where c E we is some constant symbol, then I =Joe( c). 
2. If f( tp ... , tn) E /, where f E We is some proper function symbol, 
then I =Joo(f)(/1 , â¢â¢â¢ , In), where, for 1 ::; i ::; n, 
I;= {u E TM~+ lf(t 1 , â¢â¢â¢ ,t;_ 1 ,u,ti+PÂ·Â·Â·,tn) E /}. 
Proof. Let c E I be some constant symbol. I is directed, so no term 
u E TM~+ of the form g or g( 01 '0 0 
0 'um ), where g E we is distinct from 
c, can be in I since, by definition of !;;;; T+ , there is no term v E TM~+ 
such that c, u !;;;T+ v. Also, l.T E I since I is downward closed, so I= 
{l.T,c} =J"'(c). 
Now, let f(t 1, â¢â¢â¢ , t) E /, for some f EWe with T(f) = T 1 X Â·Â·Â· X 
Tn -+T, and for 1 ::; i ::; n let / 1 , â¢â¢â¢ , In be as defined in the statement of 
the lemma. Firs.t we show that / 1 , â¢â¢â¢ , In are ideals. For 1 ::; i ::; n, t; E I;, 
so I; is nonempty. If u E I;, v E TM~+, and v !;;;Ti u, then 
f(t 1 , â¢â¢â¢ ,t;_ 1 ,u,ti+ 1 , â¢â¢â¢ ,tn) E I 
and 

548 
Chapter 17 Denotatlonal Semantics of Recursion Equations 
which implies f(t 1, ... , t;_ 1, v, t;+ 1, ... , tJ E I since I is downward closed. 
Therefore, v E I; and I; is downward closed. If u, v E I;, there is a term 
f(tp ... ,t;_ 1,w,t;+PÂ·Â·Â·,tJ E I such that 
f(tl , ... ,ti-1 ,u,ti+l , ... ,tn) !;;;T+ f(tl , ... ,ti-1 ,w,ti+l , ... ,tn), 
f(tl , ... ,ti-1 ,v,ti+l , ... ,tn) !;;;T+ f(tl , ... ,ti-l ,w,ti+l , ... ,tn), 
since I is directed. Then wE I; and u, v !;;;T+ w, so I; is directed. There-
fore, I; is an ideal, 1 ::; i ::; n, i.e., I; E DT~. ' 
We claim that I =J"'(f)(/1, ... , In). Let u E I. If u = .L T, then u E 
J"'(f)(/1, ... , In) by definition of J"'(f). Otherwise, u must be of the form 
f( u 1 , ... , u J since I is directed. Then there is a term f( v ~' ... , vJ E I 
such that 
which implies, for 1 ::; i ::; n, 
f(tp ... ,t;_pU;,ti+PÂ·Â·Â·,tn) !;;;T+ f(vpÂ·Â·Â·,vn), 
so that 
f(tl , ... ,ti-1 ,u;,ti+l , ... ,tn) E I. 
Therefore, we have u; E I;, 1 ::; i ::; n, which implies f( up ... , uJ E 
J"'(f)(/1, ... , In), and so I ~ Joo(f)(/1, ... , In). 
Now, let u E 
J"'(f)(/1, ... , In). If u = .L T then u E I. Otherwise, u is of the form 
f( u 1, ... , uJ, where u; E I;, 1 ::; i ::; n. Therefore, by definition of I;, 
and since I is directed, a simple induction on n shows that there is a term 
f(vp ... ,vJ E I such that 
Then u; !;;;T+ v;, 1 ::; i ::; n, which implies f( up ... , uJ !;;;T+ f( vp ... ,vJ, 
so 
f( u I ' . : . 'u n) E I' 
since 
I 
is 
downward 
closed. 
Therefore, 
J"'{f)(/1, ... , In)~ I, and so I =J"'(f)(/1, ... , In). 
â¢ 
Theorem 6.1. 
IK~(W> is an infinitary W-structure. 
Proof. It is clear that y+"' is a type assignment for W, and it is complete 
by Theorem 16.3.12. Therefore, we begin by showing that J"' is a 
::T"' -interpretation for W. Note that .L T occurs in every ideal of 
(TMw+, !;;;T+ ), and {.LT} is an ideal, so {.LT} is the bottom element _iT" 

6. lnfinitary Data Structure Systems 
549 
of (DTx' !;;;;Tx ). Now, for any constant symbol c E we with T(c) = T' 
J"'(c) = {.LT, c} * _1_ T' is an ideal of (TMw+, !;;;;T+ ), so condition 1 is 
satisfied. Let f E ~Â· be a proper function symbol with T(f) = T 1 X Â· Â· Â· X 
T,. -+T, let (/1 '0 0 0' /,.) E DTx X 000 X DTx' and let I =J"'(f)(/1 '0 0 0' I,.). I 
is nonempty since .LT E /. S~ppose that "r(t1, ... , t) E I, u E TMw+, and 
u !;;;;T+ f(tp ... , t). If u =.LT or u = f(tp ... , t) then u E I. 
Otherwise, by the definition of !;;;;T+, u must be a term of the form 
f(u 1, ... ,u,.), where u; !;;;;T+ t;, 1 ~ i ~ n. Then u; E I;, 1 ~ i ~ n, since I; 
is downward closed, so f( u 1 , ... , u,.) E I and I is downward closed. 
Suppose t, u E /. If t !;;;;T+ u or u !;;;;T+ t then either t, u !;;;;T+ t E I or t, u 
!;;;;T+ u E /, so suppose otherwise. Then t, u must be terms of the form 
f(t'p ... ,t,.),f(upÂ·Â·Â·â¢u), respectively, and t;,u; E I; implies there is a 
W; E I; such that t;,u; !;;;;T+ W;, 1 ~ i ~ n, so f(t 1 , â¢â¢â¢ ,t,.),f(u1, ... ,u) 
!;;;;T+ f( w1, ... , w) E I. Therefore, I is directed and it is an ideal, so 
I E DT, and condition 2a is satisfied. If (/1, ... , In), (J1 , â¢â¢â¢ , Jn) E 
DT, X Â·Â·Â· X DT, are distinct, then clearly J"'(f)(/1, ... , In) * J"'(f) 
I 
n 
(11, ... , J,.), so J"'(f) is one-one and condition 2b is satisfied. Conditions 
2c, 3, 4, 5, and 6 follow immediately from the definition of J"'(f), so 
(.?""',J"") is a complete W-structure. 
It is immediate from the definition that 
_1_ P <O f/:. ran J"'(f) for every 
proper constructor function symbol f, so it remains for us only to show that 
J"'(f) is continuous for every proper f E w. Let f E we be a proper 
function symbol with T(f) = T1 X Â·Â·Â· X Tn -+T, and let'?? be a chain in the 
cpa (DT, x ... x T', !;;;;Tx x ... x Tx ). It is easy to see J"'(f) is monotonic, so by 
I 
" 
I 
II 
Theorem 16.4.3 we just need to show that J"'(f)( U %') !;;;; Tx 
UJ"'(f)(%'); that is, 
J"'(f)( U%') ~ UJ"'(f)(%'). 
Lett EJ"'(f)(U'?J') =J"'(f)(u(%' p), ... , u (%' t n)). If t = .LT then t E 
J"'(f)(/) for all I E '??,so that t E UJ"'(f)('?J'). Otherwise, tis some term 
f(t 1, ... ,t,.), where t; E U('?J' t i), 1 ~ i ~ n. Now, for 1 ~ i ~ n, if t; E 
U(%' t i), then t; E (If, ... ,/~) t i for some (If, ... , I~) E C, and since 
{(If, ... , ID 11 ~ i ~ n} is a finite subset of the chain '??, there is some 
(/1, ... , In) E %' such that 
Then t; E 1;, 1 ~ i ~ n, which implies f(tp 00., tn) EJ"'(f)(/1 ,. 00, In), so 
that f( t P 00 . , t) E u J"'(f) ( '??) = u J"'(f)( %'). Therefore, J"'(f) ( u %') 
~ UJ"'(f)(%'). 
We turn now to the built-in function symbols. Again, it is easy to see 
that each J"'(ifT), J""(is_f), and J"'(f;- 1 ) is monotonic, using Lemma 1 in 
the latter two cases. Let T E TV(W), and let %' be a chain in 

550 
Chapter 17 Denotational Semantics of Recursion Equations 
{DBoolx X Tx X Tx' !;;;Boolx X Tx X Tx ). If { U '/FH 1 = ..1_ Bootâ¢ then I J, 1 = ..1_ Bootâ¢ 
for all I E ~ and we have 
If (U~)J,l = tt then J"'(ifT)(U%') = (U%'H2 = U(~ J,2). Now, if 
t E U(%' J,2), then t E I for some (b, I, J) E %', and there is some 
(tt,I',J') E ~such that I !;;;Tx /',so 
Therefore, J""(ifT){u~) ~ UJ""(ifT){~). Similarly, if (U'IF)J,l = ff then 
J""(ifT){u~) = (U~H3 ~ uJ"'(ifT){~), so J"'(ifT) is continuous. 
Next, let f E we with p(f) = T' and let ~ be a chain in (DTx' !;;;Tx ). If 
~ = {..L Tx} then J""(is_f)(U%') = ..L Bootâ¢ = UJ""(is_f){%'), so assume~ 
contains some I * ..L T â¢. Then there is some term in I ~ U ~ of the form g 
or g( u 1,. â¢â¢ , Um ), where g EWe, SO I, U ~ E ran J"'(g) by Lemma 1. 
Moreover, for all J E ~' if J *..LT. then J ~ U~ implies that J also 
contains a term of the form g or g( v1 , â¢â¢â¢ , vm ), so J E ran J"'(g). There-
fore, if f, g are the same then J""(is_f){u~) = tt = UJ"'(is_f){~), 
and if they are distinct then, by condition 3 on Y-interpretations, 
J"'(is_f) ( u ~) = ff = u J""(is_f) ( ~). 
We conclude with the functions J"'(f;- 1 ). Let f Ewe with T(f) = 
T1 XÂ·Â·Â· X Tn-T, and let%' be a chain in (DTx' !;;;Tx). If U%' $. ran...Y"'(f), 
then J"'(f;- 1 ){u~) = ..L T", and, for any IE%', if there were a term 
f(t 1 ,. â¢â¢ , tJ E I, then f(t 1', â¢â¢â¢ , tn) E U~ would imply by Lemma 1 that 
U %' E ran J"'(f), so ...Y""(f;- 1 ){/) = ..L T" and U...Y"'(f;- 1 ){~) =..LT â¢. Sup-
pose, then, that U %' = ...Y""(f){/1 , â¢ .'. , In) for some (/1 , â¢â¢â¢ : In) E 
DTÂ·x ... xTâ¢, so that J"'(f;- 1){U%') =I;, and lett E I;. Then there is a 
ter~ u or"the form f(u 10 â¢â¢â¢ ,u;_ 10 t,ui+ 1 , â¢â¢â¢ ,uJ E U%', which implies 
u E I for some IE~. Therefore, by Lemma 1, t E...Y"'(f;- 1)(/) ~ 
u ...Y""(f;- 1 ){~), and we have J""(f;- 1){U%') ~ u ...Y""(f;- 1 ){~), so 
...Y"'(f;- 1 ) is continuous. 
â¢ 
We immediately get 
Corollary 6.2. There is an infinitary data structure system for every 
standard vocabulary. 
For example, A~ = rep(!.~) is an in finitary data structure system for 
WN. It is clear that each element in D 8001 â¢ is representable. It turns out, 
moreover, that every element in Dw is representable. Certainly each 

6. lnfinitary Data Structure Systems 
n E DN~ is representable, since O(n) = n. For example, 
0(2) =J"'(s)(J"'(s)(J"'(O))) =J"'(s)(J"'(s)( {.LN, 0})) 
=J""(s)({.LN, s(.LN), s(O)}) 
= {.LN , s(.LN), s(s(.LN)), s(s(O) )} 
= 20 
Now, let P be the WN-program with equations 
B(X) = B(X) 
G(X) = s(G(X))o 
Then for any n j_ E DN", n j_ = JLcl>p(sn(B(O))). For example, 
JLcl>p(s(s(B(O)))) =J""(s)(J"'(s)(O(B)(J"'(O)))) 
= J""(s)(J"'(s)( {.LN})) 
=J""(s)({.LN 's(.LN)}) 
= {.LN 's(.LN), s(s(.LN) )} 
551 
Also, an easy induction on n E N shows that ci>~(O)(G)(O) = n j_ for all 
n EN (where 0 = {l.N, 0}), so 
JL<I>p(G(O)) = u {ci>~(O)(G(O)) In EN} 
= U{ci>~(O)(G)(O)In EN} 
= U{nj_ In EN} 
= Wo 
Therefore, every element of DN~ is representable, and A""N = !.~ 0 
The situation is more interesting when we consider 
and A"'NL = rep(!.~L)o As in the previous example, .r"(N) = rep(.r")(N), 
but now .r"(NL) =F rep (.9""") (NL)o Again there are three kinds of elements 
in DNL" 0 For each term of the form cons(t 1 , .. o cons(tn, nil) .. o ), n ~ 0, 

552 
Chapter 17 Denotational Semantics of Recursion Equations 
where t; E TM~NL, 1 ~ i ~ n, there is a principal ideal. For example, 
pid(cons(l, cons(.l.N, nil))) 
= {.l.NL, cons(.l.N, .l.NL), cons(s(.l.N ), .l.NL), cons(1, .l.NL), 
cons(.l.N, cons(.l.N, .l.NL)), cons(s(.l.N), cons(.l.N, .l.NL)), 
cons(l, cons(.l.N, .l.NL)), cons(.l.N, cons(.l.N, nil)), 
cons(s(.l.N), cons(.l.N, nil)), cons(l, cons(.l.N, nil))}. 
Note that pid(l.N) = {l.N}, pid(nil) = {l.NL,nil} =J""(nil), 
pid(cons(.l.N, nil)) = {.l.NL, cons(.l.N, .l.NL), cons(.l.N, nil)} 
=J"'(cons)(pid(.l.N ), pid(nil)), 
and, as the reader can verify, 
pid(cons(l, cons(.l.N, nil))) =J"'(cons)(pid(l), pid(cons(.l.N, nil))). 
That is, pid(cons(l,cons(l.N,nil))) is built up from pid(nil), the list with 
no elements, by applying J"'(cons) to (pid(l.N), pid(nil)) to get 
pid(cons(l.N,nil)), a list with one element (namely, pid(l.N)), and then 
applying J"'(cons) again to (pid(l), pid(cons( l. N, nil))) to get a list with 
two elements (namely, pid(l) and pid(l.N)). Therefore, we write 
pid(cons(l,cons(l.N,nil))) as (pid(l),pid(l.N)), or simply (1, ..LN.). In 
general, we write pid(nil) as the empty list 
( ) and pid(cons 
(t1 , â¢â¢â¢ cons(tn,nil) Â·Â·Â· )) as the finite list (d1 , â¢â¢â¢ , dn), where d; = pid(t;), 
1 ~ i ~ n. 
For each term of the form cons(t 1 , â¢â¢â¢ cons(tn, l.NL) Â·Â·Â· ), n ~ 0, where 
t; E TM~NL, 1 ~ i ~ n, there is also a principal ideal. For example, 
pid(cons( 1, cons( l. N, l. NL) )) is 
{.l.NL , cons(.l.N , .1. NL), cons(s(.l.N), .l.NL), 
cons(l,.l.NL), cons(.l.N ,cons(.l.N ,.l.NL)), 
cons(s(.l.N), cons(.l.N, .l.NL)), cons(l, cons(.l.N ,.l.NL) )} . 
Here we have 
pid(cons(l,cons(.l.N ,.l.NL))) =J"'(cons)(l,Joc(cons)( ..L N
00 ' ..L NL")), 
which we write (1, ..L Noo) _]_ . In general we write elements of the form 
pid(cons(tHÂ·Â·Â·cons(tn,l.NL) Â·Â·Â·))as (d1 , â¢â¢â¢ ,dn)_j_, where d; = pid(t;), 

6. lnfinitary Data Structure Systems 
553 
1 :::;; i :::;; n. We call these objects prefix lists, since 
for any e I ' 
0 
0 
0 
' em E D N"Â· 
We also have elements in DNL" which are nonprincipal ideals. The 
difference between ( d 1 , â¢â¢â¢ , d n) and ( d 1 , â¢â¢â¢ , d n) _!_ is that the former is 
built up from the completely defined empty list J"'(nil) and the latter 
is built up from the completely undefined list 
.L NL"Â· Now, we can 
have infinite chains of finite lists, e.g., { ( n _]_ ) I n E N}, where we write 
u { ( n _]_ ) I n E N} as ( w), but the least upper bound is always a finite list 
(though not a principal ideal). On the other hand, chains of prefix lists can 
lead to infinite lists. For example, { ( 0, ... , n) _]_ I n E N} is a chain, since 
(0) _!_ !;;;NL" (0, 1) _!_ !;;;NL" (0, 1, 2) _!_ !;;;NL" â¢â¢â¢' 
and the least upper bound of { ( 0, ... , n) _]_ I n E N} is the infinite ideal 
which we write as the infinite list (0, 1, 2, 3, ... ). There are other interest-
ing kinds of non principal ideals, such as u { ( n _]_ ) _]_ I n E N} and 
n 
u{<~>_j_ In EN}, 
which we leave to the reader to explore. 
It is interesting to observe that even these infinite lists, or the repre-
sentable ones, more precisely, can be quite useful. In fact, a family of 
programming languages known as lazy functional languages has been devel-
oped based on the use of such objects. The typical use of infinite lists in 
these languages is to define an infinite list of desired objects and then to 
select some particular object from the list. The word lazy refers to the fact 
that, in practice, it is not necessary to generate an entire infinite list before 
performing the selection: it is necessary to generate only enough of the list 
so that the desired object appears. For a simple example, let P be the 
program with equations 
F(X) = cons(X, F(s(X))) 
Nth(X, Y) = ifNL(is_O(X), cons1- 1 (Y), Nth(s 1- 1 (X), cons; 1 (Y)) ). 
Then 
~(F(O)) = (0, 1,2, ... ) and ~(Nth(n,F(O))) = n for all n EN. 
In the next chapter we will show that the infinite list ( p 1 , p 2 , â¢â¢â¢ ) of all 
primes is also representable. 

554 
Chapter 17 Denotational Semantics of Recursion Equations 
Unlike a""N, we now have objects that are not representable. For exam-
ple, the infinite list L = (HALT(O, 0), HALT(l, 1), ... ) is not repre-
sentable, where HALT(x, x) is the predicate defined in Chapter 4. If it 
were representable, say L = JLCI>Q(t), then we would have 
JLCI>PuQ(Nth(n,t)) = HALT(n,n) for all n EN, 
which, informally at least, would imply that HALT(x, x) is computable. 
For our final example, we take a vocabulary, WR, suitable for represent-
ing the decimal expansions of real numbers x in the interval 0 :::;; x < 1. 
(This interval is usually written [0, 1).) This time we begin with constant 
symbols d0,d1 , ... ,d9 , with T(d0) = 
Â·Â·Â· = T(d9 ) = D, to represent deci-
mal digits. We will build up lists of decimal digits with dnil and dcons, 
where T(dnil) = DL and T(dcons) = D X DL-DL. We also include in 
WR the symbols of WNL. We call the elements of D 0 x decimal digits, which 
we write as .l 0 ., 0, 1, ... , 9. Again we have three kinds of elements in 
D 0 Lx: (1) finite lists of decimal digits, which we write .d1d 2 â¢â¢â¢ dn, n ;:::: 0; 
(2) prefix lists of decimal digits, which we write .d1 d 2 â¢â¢â¢ dn _j_ 
, n ;:::: 0; and 
(3) infinite lists of decimal digits, which we write .d 1 d2 â¢â¢â¢â¢ It is clear that 
there is an object in D 0 Lx for every real number in [0, 1). (Actually, there 
is more than one object for some real numbers, since, for example, 
.29999 ... and .3 are distinct in DoL x, but that problem will not concern us 
here.) We call the elements of D 0 Lx computable real numbers7 in [0, 1). It is 
a basic mathematical fact that there are more real numbers than there are 
WR-programs and WR-terms, so there are certainly objects in D 0 Lx that are 
not computable real numbers. It is clear that every nonrepeating rational 
number in [0, 1) is computable, e.g., .33 = O(dcons(d3 ,dcons(d3 ,dnil))). 
It is also clear, intuitively, that we could write a WR-program to define long 
division, so every repeating rational number in [0, 1) is computable. In the 
next chapter we will show, moreover, that some irrational numbers are 
computable as well. 
Exercises 
1. 
Let We = {tt, ff, c, f} be a standard constructor vocabulary with Te(c) 
= T and Te(f) =TXT -T, and let W =We U B(We). 
(a) Give an infinite chain in (TM~., !;;;T+ ). 
(b) Let J"" =~'(W)â¢ What is fl5 ,(f(X,f(c,c)))? 
7 Really we should call these objects representable real numbers at this point, but the 
operational semantics in the next chapter will justify the more traditional name. 

6. lnfinitary Data Structure Systems 
555 
2. 
What is pid(3) in !l"'N? What is pid(s3(l.N))? 
3. Show that n _j_ !;;;Noc n for all n E N. [Note that n has two different 
meanings here.] 
4, 
Let ,_f~ =~~(WN)' 
(a) Show that ..Y"'(s) (2) = 3. 
(b) Show that ..Y"'(s)(2 _j_) = 3 _j_ â¢ 
(c) 
Show that ..Y""(s1- 1 )(3 _j_) = 2 _j_ â¢ 
5. (a) What is gll~N(ADD)( + )(3, 2)? 
(b) What is gll'N(ADD)( + )(3 _j_ , 2)? 
(c) 
What is gll'N(ADD)( + )(3, 2 _j_ )? 
6. What is ( ..L N"' 1) in !l"'NL? What is ( ..L N"' 1) _j_ ? 
7. 
Show that (n) _j_ !;;;NL~ (n) for all n EN. 
8. Let J"' = ..Yr(WNd. 
(a) Show that ..Y"'(cons)(O, (1)) = (0, 1). 
(b) Show that ..Y"'(cons)(O, (1) _j_) = (0, 1) _j_. 
(c) 
Show that ..Y"'(cons; 1)((0, 1) _j_) = (1) _j_. 
9. 
(a) 
(b) 
10. (a) 
(b) 
What is g.oc (LIST)(Length)( (2, 2))? 
'-'NL 
What is g â¢â¢ (LIST)(Length)( (2, 2, 2, ... ))? 
'-'NL 
What is g â¢â¢ (LIST)(Cat)((2,3), (4,5))? 
'-'NL 
What is gll'NL(LIST)(Cat)((2, 3), (4, 5, 6, ... ))? 
11. Write a WNL-program P with F E FV( P) such that 
for all representable lists of numbers (n 1 , n 2 , â¢â¢â¢ ) E DNL". 
12. Write a WNL-program P with FE FV(P) such that 
gll'NL(P)(F)((m 1 , m 2 , ... ), (n 1 , n 2 , ... )) = (m 1 + n1 , m 2 + n 2 , ... ) 
for all representable lists of numbers (m 1 , m 2 , â¢â¢â¢ ), (n 1 , n 2 , â¢â¢â¢ ) E 
DNL"Â· 
13. Show that (1, 1, 1 ... ) is representable. 
14. Show that ( 1 _j_ , 1 _j_ , 1 _j_ 
â¢â¢â¢ ) is representable. 
15. Show that for any n EN, (n, n + 1, n + 2, ... ) is representable. 
16. Let ( w) _j_ = u{(n _j_) _j_ In EN}. Describe ( w) _j_, and show that it is 
representable. 

556 
Chapter 17 Denotatlonal Semantics of Recursion Equations 
17. Let 
n 
(w,w, ... ) = u{<~)_~_ In EN}. 
Describe ( w, w, ... ), and show that it is representable. 
18. Prove the assertion made in the proof of Theorem 6.1: for every 
f E W, J"'(f) is monotonic. 

18 
Operational Semantics 
of Recursion Equations 
1. Operational Semantics for Simple Data 
Structure Systems 
The definition of gt:. accomplishes our goal of directly assigning a meaning 
to programs without the intermediary notion of a computation. On the 
other hand, at this point we have no a priori reason for believing that the 
functions defined by recursion programs are (partially) computable. It 
turns out, though, that they are computable in a very reasonable sense. In 
this chapter we go back to basics and define a notion of computation, one 
appropriate for recursion programs, which has much in common with 
computations of Y programs. This new kind of computation will be the 
basis for the operational semantics of W-programs. The idea is that the 
operational semantics will give us a way to compute the functions defined 
by the denotational semantics. We say that an operational semantics is 
correct with respect to the denotational semantics gt:. for a data structure 
system a if it gives every W-program P the same meaning given by gt:., 
that is, :;gt:.(P). The precise details depend on the nature of the particular 
data structure systems in which we wish to compute, so in this section we 
concentrate on an operational semantics appropriate for simple data 
structure systems. 
An Y program computation is a sequence of snapshots, and the relation 
between a snapshot and its successor is easily described. Computations of 
recursion programs are similar in nature. The idea, as before, is that we 
557 

558 
Chapter 18 Operational Semantics of Recursion Equations 
treat an equation like F(X) = G(H(X)) as a definition ofF in terms of 
G(H(X)). Given a term F(3), for example, we attempt to determine its 
value by replacing it with G(H(3)). Of course, G and H should also be 
defined, so we replace them by their definitions as well, and continue 
replacing until we get a numeral. We formalize this idea as follows. 
Again we let W be some arbitrary standard vocabulary throughout the 
chapter. A W-substitution is a finite function {(X 1 , t), ... , (Xn, tn)} such 
that X1 , â¢â¢â¢ , X, are distinct individual variables and t 1 , â¢â¢â¢ , tn are W-terms 
such that T(t) = T(X), 1 :o:; i :o:; n. The application of a W-substitution (} 
to a W-term t is written t (}, and the result of applying (} to t is the W-term 
obtained from t by simultaneously replacing each occurrence of X; by t;, 
1 :o:; i :o:; n. Note that each variable in the domain of (} is replaced by a 
term of the same type, so if t is a W-term, then t(} is also a W-term. We 
can give a more formal definition as follows. Let (} be a W-substitution. 
Then 
c (} 
= c 
for constant symbols c E W 
XO 
=t 
for X E VAR 1 such that (X,t) E (} 
X(} 
= X 
for X E V AR 1 such that X $. the domain of (} 
f(t 1 , â¢â¢â¢ ,tn)O =f(t 10, ... ,tn0) wherefE W 
F(t 1 , â¢â¢â¢ ,tn)O=F(t 10, ... ,tn(}) whereF E VARF. 
The following useful lemma shows that we can sometimes trade in part 
of a variable assignment for a substitution. 
Substitution Lemma. Let l = (Y,Y) be a W-structure, let v; ~ VAR 1 
and Vr ~ VARF, let a E..w"y(Vr), and let f3 E..w"y(JI;) be a variable assign-
ment and (} a substitution such that, for all X E v;, V(X (}) ~ Vr and 
{3(X) = a(XO). Then for all t E â¢w<v; u Vr), au {3(t) = a(tO). 
Proof. 
We argue by structural induction on t. If t is a constant symbol 
c E W, then 
a u {3(c) =Y(c) = Y(c (}) = a(c (} ), 
and if t is X E v;, then a u fj(X) = {3(X) = a(X (}) by assumption. If t is 
f(t 1 , â¢â¢â¢ , tn), where fEW, then 
aU {3(f(t 1 , ... ,tn)) 
=Y(f)(a U {3(t 1 ), â¢â¢â¢ , a U {3(tn)) 
=Y(f)(a(t 1 0), ... , a(tnO)) 
by the induction hypothesis 
= a(f(t 10, ... ,tn0)) 
= a(f(t~' ... ,tn)O). 

1. Operational Semantics for Simple Data Structure Systems 
If t is F( tp ... , t" ), where F E J-f, then 
a U ,B(F(tp ... , tn)) 
= a(F)(a U ,B(t 1), â¢â¢â¢ ,a U ,B(tn)) 
= a(F)( a(t 1 e), ... , a(tn e)) 
by the induction hypothesis 
= a(F(t 1 e, ... , tn 8)) 
= a(F(tp ... ,tn)e). 
559 
â¢ 
A W-term rewrite rule is a pair of W-terms, written u -+v, such that 
/V(v) ~ /V(u) and such that no individual variable occurs more than once 
in u. A W-term rewriting system is a set of W-term rewrite rules. 1 We say 
that a W-term t matches a rewrite rule u -+v with substitution e if t = ue. 
A W-term rewriting system T is deterministic if no W-term matches more 
than one rewrite rule in T. 
In order to use a W-term rewriting system T, we associate with T a 
rewriting strategy u which selects, for every W-term t, a (possibly empty) set 
of occurrences2 of subterms of t. These are the (T, u )-redexes of t. A 
W-term w is a T -rewrite of W-term t if t matches some rewrite rule u -+ v in 
T with substitution e, and w = ve. Given a strategy u, a W-term w is a 
(T, u )-rewrite of a W-term t, denoted 
t ==> w, 
T,u 
if w is the result of replacing every (T, u )-redex t' oft by aT-rewrite oft'. 
A W-term t is (T, u )-normal if the set of (T, u )-redexes is empty. A 
(T, u )-computation for W-term tis a (possibly infinite) sequence of W-terms 
t 0 , t 1 , â¢â¢â¢ such that 
1. t 0 is t, 
2. t; ==> t;+ 1 for all t; occurring in the sequence, and 
T,u 
3. for all t; in the sequence, t; is (T, u )-normal if and only if t; is the 
last term in the sequence. 
For our definition of a computation to be reasonable, it is crucial that 
the process of finding the term t;+ 1 that follows term t; should itself be 
"mechanical," that is, computable in some sense. Moreover, the test that a 
1 Although the definition permits rules of the form c -+d, where c, d are constant symbols, 
they play no role in our treatment of operational semantics, so we assume that such rules do 
not occur in any rewriting system referred to in this chapter. 
2 Note that we distinguish between subterms and occurrences of subterms. For example, 
F( G( 0 ), G( 0)) has two occurrences of the subterm G( 0 ), and a strategy u might select the 
occurrence on the left without selecting the occurrence on the right. 

560 
Chapter 18 Operational Semantics of Recursion Equations 
term is (T, u )-normal must also be computable, so that we know when a 
computation terminates. Now, three kinds of steps are involved in finding 
ti+ 1: 
â¢ finding the set of (T, u )-redexes oft;; 
â¢ for each (T, u )-redex oft;, finding a rule u -+v in T and a substitution 
e such that t; matches u -+v with e; and 
â¢ applying (J to v. 
It is clear that finding and applying substitutions are fairly simple opera-
tions. (For a more detailed treatment see the unification algorithm in 
Chapter 13.) Therefore, we must look more closely at rewriting strategies 
and sets of rewrite rules. We begin with four commonly defined rewriting 
strategies. 
Definition. LetT be a W-term rewriting system. A W-term is T-rewritable 
if it matches some rule in T. Let t be a W-term. An innermost occurrence 
of a T-rewritable subterm oft is one which has no T-rewritable subterms.3 
An outermost occurrence of a T-rewritable subterm oft is one which is not 
a subterm of any T-rewritable subterm oft. The leftmost innermost strat-
egy, denoted uu, selects the leftmost of the innermost occurrences of 
T -rewritable subterms of t. If there is no such subterm then uu selects the 
empty set. The parallel innermost strategy, denoted up; , selects the (possi-
bly empty) set of all innermost occurrences of T-rewritable subterms oft. 
The leftmost outermost strategy, denoted u10 , selects the leftmost of the 
outermost occurrences of T-rewritable subterms of t, or the empty set if 
there is no such subterm. The parallel outermost strategy, denoted upo, 
selects the (possibly empty) set of all outermost occurrences of T-rewrita-
ble subterms of t. 
Note that in each of the four strategies, the choice of (T, u )-redexes 
depends on the particular set T. Therefore, the computability of applying 
u depends on the nature ofT. 
To illustrate, let T consist of the rewrite rules 
F(X) -+s(F(s(X))) 
F(X) -+X 
G(O,Y)-+Y 
G(s(X),s(Y)) -+s(G(X, Y)). 
Then the underlined subterm of G(s(F( 0 )), s(F( O))) is the (T, uu)-redex, 
and the underlined subterm of G(F(G(O,O)),F(O)) is the (T, u10)-redex. 
3 Note that we are not considering a term to be a subterm of itself. 

1. Operational Semantics for Simple Data Structure Systems 
561 
Each of 
F(2) ~ 
2 
-- T,uli 
F(2) ~ 
s(F(3)) ~ 
s(s(F(4))) ~ 
s(s(4)) = 6 
-- T,uu --
T,uli 
T,uu 
F(2) ~ 
s(F(3)) ~ 
s(s(F(4))) ~ 
s(s(s(F(S)))) ~ 
-- T, uu --
T, u1; 
T, uu 
T, uu 
is a (T, uu)-computation for F(2), where the underlined terms are the 
(T, uu)-redexes, and 
G(F(F(O)),F(2)) ==> G(s(F(s(F(O)))),s(F(3))) 
T, Upo 
==> s(G(F(s(F(O))),F(3))) 
T,upo 
--
==> s(G(s(F(O)),s(F(4)))) 
T,upo 
==> s(s(G(F(O),F(4)))) 
T,upo 
----
==> s(s(G( 0, s( F(S)) ))) 
T, Upo 
==> s(s(s(F(S)))) 
T,upo 
==> s(s(s(S))) = 8 
T,upo 
is a (T, uP0 )-computation for G(F(F(O)),F(2)), where the underlined 
terms 
are the (T, CTP0 )-redexes. 
It is clear from the example that computations for a given term t are not 
necessarily unique, which would make them unsuitable for the definition 
of functions. However, we can associate with each W-program a determin-
istic W-term rewriting system, which does give unique computations. 
Definition. Let P be a W-program. TheW-term rewriting system associated 
with P for simple data structure systems, denoted T.(P), consists of 
F(X1 , â¢â¢â¢ ,Xn) -u for each equation F(X1 , â¢â¢â¢ ,Xn) = u in P, 
together with 
â¢ for each T E TV(W), 
ifT(tt, X, Y)- X 
ifT(tT,x, Y) -v 

562 
Chapter 18 Operational Semantics of Recursion Equations 
(The choice of particular individual variables X, Y is unimportant, as 
long as they are distinct and of the appropriate type.) 
e for each constant symbol C E We-, With r(c) = T, 
is-c(c) -tt 
is-c(t) -rr 
for each t E â¢k - {c} 
â¢ for each proper function symbol f EWe, with ar(f) = n and p(() = T, 
is-f(f(t 1 , â¢â¢â¢ , tn)) -tt 
for each f(t 1 , â¢â¢â¢ , tn) E TM~ 
( 
is-f(u) -rr 
for each u E TM~, not of the form f(t 1 , â¢â¢â¢ , tn) 
f;-l (f(t 1 , â¢â¢â¢ , tn)) -t; 
for each f(t 1 , â¢â¢â¢ , tn) E TM~, 
It is clear that for any W-program P, T.(P) is deterministic, so T.(P) gives 
a unique (T.(P), u )-computation for any W-term t, where u is any rewrit-
ing strategy. It is also easy to see that for any W-term t, the process of 
finding a matching rewrite rule in T.(P), if one exists, is straightforward. It 
follows that finding the set of (T.(P), u )-redexes for t, where u is any of 
the strategies just defined, is also straightforward. Moreover, the test that a 
term is (T.(P), u )-normal is easy, so we have four reasonable notions of a 
computation. 
To illustrate, T.(ADD) has the rules 
+(X, Y) -if N(is_O(Y), X, s( +(X, s1- 1 (Y)) )) 
if 8001(tt, X, Y)- X 
if 8001(fT, X, Y)- Y 
ifN(tt,X, Y)- X 
ifN(tT,X,Y) -v 
is-0(0) -tt 
is-O(s(n)) -rr 
is-s(s(n)) -tt 
is-s(O) -rr 
s.- 1 (s(n)) -n 
for all n EN 
for all n EN 
for all n EN, 

1. Operational Semantics for Simple Data Structure Systems 
and 
+(3,2) 
=====:::) if N(is-0(2),3, s( + (3, s 1- 1(2)))) 
T,(ADD), u1, 
=====:::) ifN(tT,3,s( + (3,s 1- 1(2)))) 
T,(ADD), u1, 
=====:::) s( + (3, s 1- 1 (2))) 
T,(ADD), u 1, 
=====:::) s(if N(is-O(s 1- 1 (2)),3, s( + (3, s 1- 1(s 1- 1(2)))))) 
T,(ADD), CTf0 
=====:::) s(ifN(is-0(1), 3, s( + (3, s 1- 1 (sl 1(2)))))) 
T,(ADD), u1, 
=====:::) s(if N (IT, 3, s( + (3, s1- 1 (s1- 1 (2)))))) 
T,.(ADD), u 10 
=====:::) s(s( + (3, s 1- 1 (s! 1(2)) ))) 
T,(ADD), u 1, 
=====:::) s(s(ifN(is-O(s 1- 1(1)),3, s( + (3, s 1- 1(s! 1(s 1- 1(2)))))))) 
T,(ADD), u1, 
=====:::) s(s(ifN(is-0(0),3, s( + (3, s! 1(s 1- 1(s 1- 1(2)))))))) 
T,(ADD), ulo 
=====:::) s(s(if N(tt,3, s( + (3, s1- 1 (s! 1(s 1- 1 (2))))) ))) 
T,(ADD), u 1, 
==:::::;'> s(s(3)) = 5 
T,(ADD), CT/0 
is the (T.(ADD), u10)-computation for + (3,2). 
563 
Now, in this example, the leftmost outermost strategy gives us exactly 
what we want with respect to aN, computing 5 from+ (3,2). The leftmost 
innermost strategy, on the other hand, is a different story. Consider the 
simple program 
P = {G(X) = G(X),H(X) = 3}. 
For G(O) we get the infinite (T.(P), uu)-computation 
G(O) 
G(O) 
Â·Â·Â·, 
-- T,(P), uu -- T,(P), uu 
which is entirely appropriate since gaN(P)(G) is the everywhere undefined 
function and gaN(PXG)(O) =..lN. However, uu also gives the infinite 
computation 
H(G(O)) ====> H(G(O)) ===> H(G(O)) ===> 
T,(P), uu 
T,(P), uu 
T,(P), uu 

564 
Chapter 18 Operational Semantics of Recursion Equations 
which is not what we want, with respect to aN, since 
DdN(P)(H)( ..lN) = JL<I>p(H)( ..lN) 
u ( {<I>~+ 1(0)(H)( ..lN) I i E N} u {<I>~(O)(H)( ..lN)}) 
U ({<1>~(0) U a_~_N(3) I i EN} U { ..lN}) 
= 3. 
The problem is that the nonstrict function assigned to H by 9J6)P) can 
completely ignore its input and produce an output value, but the leftmost 
(or parallel) innermost strategy requires the computation to try forever to 
compute G( 0 ). On the other hand, the leftmost (or parallel) outermost 
strategy gives the finite computation 
H(G(O)) 
3, 
T,(P), ulo 
which is exactly what we want. 
The point is that innermost strategies may be fine in a context where all 
functions are strict, but they are not successful in general. For our 
purposes they are not appropriate since, even if we interpret all construc-
tor function symbols with strict functions, the interpretations of the if T 
symbols are necessarily not strict. This is quite sensible, since we do not 
want J(ifT)(b, d, e) to depend on both d and e, but only on (at most) one 
of d or e, according to the value of b. We choose a strategy which is 
neither purely innermost nor outermost, but which is closer in spirit to an 
outermost strategy, since it does not depend on completing the computa-
tion of innermost subterms. It will be convenient to define it only for 
deterministic W-term rewriting systems. 
Definition. Let T be a deterministic W-term rewriting system, and let t be 
a W-term. In the full rewriting strategy, denoted uf, the (T, uf)-rewrite4 oft 
4 Technically, the definition of (T, ur )-rewrites varies somewhat from the general definition 
of (T, u )-rewrites given earlier, since we replace subterms c!Â»(t 1 , â¢â¢â¢ , tn) by the T-rewrite of 
clÂ»(rry(t 1), â¢â¢â¢ ,rry(tn)), i.e., ry(clÂ»(rry(t 1), â¢â¢â¢ ,rry(tn))), rather than by the T-rewrite of 
clÂ»( t 1 ,, â¢â¢â¢ , tn ). The difference is of no concern, however, and the definition of (T, ur )-compu-
tations, which depends only on (T, ur )-rewrites and (T, ur )-normality, conforms to the general 
definition of (T, u )-computations. 

1. Operational Semantics for Simple Data Structure Systems 
is rr T(t), where rr T(t) is defined in two stages: 
u8 if ~(t 1
, â¢â¢â¢ , tn) matches a rule 
~(u 1
, â¢â¢â¢ , un) -+u in T with 
rT(~(t 1 , â¢â¢â¢ ,tn)) = 
substitution 8 
= c for all constant symbols c E W 
rrT(~(t 1 , â¢â¢â¢ , tn) )= r T( ~(rr T(t 1 ), â¢â¢â¢ , rr T(tn)) ). 
565 
The W-term t is (T, ur )-normal if there is no subterm ~(t 1 , â¢â¢â¢ , tn) of t 
such that ~(rrT(t 1 ), â¢â¢â¢ , rrT(tn)) matches a rewrite rule in T. 
Throughout this section we will write r P and rr P for r T,<P> and rr T,<P>, 
respectively. Just as with uu, up;, u 10 , and upo, the computability of 
applying ur depends on the rewriting system T, and T.(P) and ur give us 
another reasonable notion of a computation. Note that if t 0 , t 1 , t 2 , â¢â¢â¢ is 
an infinite (T,(P), ur)-computation, then rr~(t 0 ) = t; for all i EN. In other 
words, rr~(t 0 ), rr~(t 0 ), rr~(t 0 ), â¢â¢â¢ is by definition the (T.(P), ur)-computa-
tion for t 0 when the computation is infinite. If t 0 , t 1 , â¢â¢â¢ , tn is a finite 
(T.(P), ur)-computation, then rr~(t 0 ) = t; for 1 ~ i ~ n, and rr~(t 0 ) = tn 
for i > n. That is, rr p(t) = t if t is (T.(P), ur )-normal. 
It is clear that for any term f(t 1, â¢â¢â¢ , tn ), where f is a constructor symbol, 
since there are no rewrite rules in T.(P) for f. In WN, for example, we have 
rrp(s(t)) = s(rrp(t)), and, in particular, rrp(n) = n for any numeral n. To 
illustrate ur we give the (T.(ADD), ur)-computation for + (3,2): 
+(3,2) =====> rrp(+(3,2)) 
T,.(ADD), u 1 
( 
= rp(+ rrp(3), rrp(2))) 
= rp(+ (3,2)) 
= ifN (is-0(2),3, s( + (3, s1- 1(2))))) 
==:::;'> rrp(ifN(is_0(2),3,s( + (3,s 1- 1(2))))) 
T,(ADD), u 1 
( 
= rp(ifN(rrp(is-0(2)),rrp(3),rrp(s +(3,s1 1(2)))))) 
= r p(if N(r p(is_O( rr p(2))), 3, r p(s( rr p( + (3, s1- 1( 2) )))))) 
= r p(if N(r p(is-0(2)), 3, r p(s( r p( + ( rr p(3), rr p(s1 1( 2)))))))) 
= r p(if N(ff, 3,r p(s( r p( + (3, r p(s 1- 1( rr p(2))))))))) 

566 
Chapter 18 Operational Semantics of Recursion Equations 
= rp(ifN(tT,3,rp(s(rp( + (3,rp(s1- 1(2)))))))) 
= r p(if N(tT, 3, r p(s( r p( + (3, I)))))) 
= r p(if N( tT, 3, r p(s(if N( is-0(1), 3, s( + (3, s 1- 1(1)))) )))) 
= r p(if N( tT, 3, s(if N( is-0( 1), 3, s( + (3, sj" 1(1))))) )) 
= s(if N(is-0( I), 3, s( + (3, s 1- 1( I))))) 
====> rrp(s(if N(is-0(1),3, s( + (3, s 1- 1(1)))))) 
T,(ADD), u 1 = Â· Â· Â· = s(s( if N(is-0( 0 ), 3, s( + (3, sj" 1 ( 0)))))) 
====> rrp(s(s(ifN(is-0(0),3,s( +(3,sj" 1(0))))))) 
T,(ADD), u 1 = ... = 5. 
We use (Ts(P), ur)-computations to define the operational semantics of 
W-programs with respect to simple data structure systems. In particular, 
we use the final terms in finite computations to determine the functions 
defined by P. Moreover, the value of these terms should be independent of 
the denotational semantics of P, so we use the least informative variable 
assignment, .n, to interpret them. 
Definition. Let a = rep(!.) be a simple data structure system for W. The 
operational meaning function for a, denoted &'t:., is defined as follows. For 
all W-programs P0 , all FE FV(P0 ), and all (d1 , â¢â¢â¢ , dn) E D,<~<FÂ»â¢ 
where 
if the (Ts(P), ur )-computation for 
F(t 1 , â¢â¢â¢ , tn) is finite and ends with t 
otherwise, 
â¢ d; = JL<I>~(t;), 1 ::::; i ::::; n, 
â¢ P0 , P1 , â¢â¢. Â·, Pn are consistent, and 
â¢ P = U7=o P;. 
The idea is that we can compute &'t:.(P0)(F)(dp ... , dn) by extending 
program P0 with programs P1 , â¢â¢â¢ , Pn and then carrying out the 
(Ts( U7 = 0 P; ), ur )-computation for F( t 1 , â¢â¢â¢ , t J. In aN , of course, n = O(n) 
for all n EN, so to compute a function on (m 1 , â¢â¢â¢ , mn) E Nn, we can 
simply let P; = 0, 1 ::::; i ::::; n. Moreover, to represent 
..LN we can just 
include the equation B(X) = B(X). 
Note, however, that in general there are many different choices of 
programs P1 , â¢â¢â¢ , Pn and terms t 1 , â¢â¢â¢ , tn which characterize the same tuple 

1. Operational Semantics for Simple Data Structure Systems 
567 
(d1 , â¢â¢â¢ , dn), and these might give different computations. It is not obvious, 
then, that the definition of &t:. makes sense. We need to know that we get 
the same result in all cases, even if the computations differ. Theorem 1.1 
later in this section, which shows that the operational and denotational 
semantics are equivalent for simple data structure systems, implies that 
we do. 
We should note that the purist might object that the operational 
semantics of W-programs is not really independent of the denotational 
semantics since the initial term F( t 1 , â¢â¢â¢ , t n) and the program P depend on 
the condition d; = JL<I>~(t;), 1 ~ i ~ n. Indeed, we could give an alterna-
tive operational semanti'cs in which the input to a program P is simply a 
sequence such as ((P1 , t 1), â¢â¢â¢ , (Pn, tn)) and the output is the final term in 
the (Ts(U7~o P;), ur)-computation for F(t1 , â¢â¢â¢ , t). Of course, Theorem 
1.1 would need to be reformulated in a suitable way. For our purposes, 
however, the important thing is that there is some term F( t 1 , â¢â¢â¢ , t) from 
which the correct value of g"iP0)(F)(d1 , â¢â¢â¢ , dn) can be computed. That 
fact is sufficient to justify calling the function gt:.(P0) (F) computable. 
We now turn to the proof of Theorem 1.1, beginning with four lemmas. 
In Lemma 1 we finally apply condition 2c on Y-interpretations. Lemma 3, 
which is proved by an induction based on Lemma 2, is the heart of the 
argument. It shows that the terms of a computation (interpreted by .n) 
correctly approximate the value of the function being computed. Lemma 4 
guarantees that, if a function has a non-bottom value for some given input, 
then the computation for that input will eventually terminate. 
Lemma 1. Let I = (:T, J) be a W-structure and let a be any variable 
assignment based on :T. Then for any term t E â¢w,, a(t) =/= .L T(t) â¢ 
Proof. We argue by structural induction on t. If t is a constant symbol 
c E we' then a(c) = J(c) E DT(c) -
{ .L T(c)} by condition 1 on Y-interpre-
tations. Otherwise, t is of the form f( tH ... 't), where f E we. Then 
a(f(tl,â¢â¢â¢â¢tn)) =J(f)(a(tl), ... , a(tn)) =/= _LT(I) by the induction hypoth-
eSiS and condition 2c on Y-interpretations. 
â¢ 
Lemma 2. 
Let I = (:T, J) be a complete, continuous W-structure, and 
let P be a W-program. Then for all t E TMw(FV(P)) and for all i E N, 
<1>~+ 1(0)(t) = <I>~(O)(rrp(t)). 
Proof. 
Let i EN. We argue by structural induction on t. If tis a constant 
symbol c E W, then 
<1>~+ 1 (0)(c) =J(c) =J(rrp(c)) = <I>~(O)(rrp(c)). 

568 
Chapter 18 Operational Semantics of Recursion Equations 
If t is of the form f(t 10 â¢â¢â¢ , tn), where fEW, then rrp(f(t 1, â¢â¢â¢ , tn)) = 
rp(f(rrp(t 1), â¢â¢â¢ , rrp(tn))). If f(rrp(t 1), â¢â¢â¢ , rrp(tn)) does not match any 
rewrite rule in T.(P), then rrp(f(t 1 , â¢â¢â¢ ,tJ) = f(rrp(t 1), â¢â¢â¢ ,rrp(tn)), and 
cl>~+ I(O,)(f(tl ' ... 'tn)) 
=J(f)( cl>~+ I ( {} )(tl ), ... , cl>~+ I (0, )(tn)) 
= ci>~(O.)(f(rrp(t 1 ), â¢â¢â¢ ,rrp(tn))) 
= ci>~(O.)(rrp(f(t 1 , â¢â¢â¢ ,tn))). 
by the induction 
hypothesis 
Otherwise, f(rrp(t 1), â¢â¢â¢ , rrp(tn)) does match some rewrite rule in T.(P). 
Suppose tis ifT(u,v,w). If rrp(u) = tt then 
and 
rrp(ifT(u,v,w)) = rp(ifT(rrp(u),rrp(v),rrp(w))) = rrp(v), 
=J(ifT)(cl>~+ 1(0.)(u), cl>~+ 1(0,)(v), cl>~+ 1(0,)(w)) 
= J(ifT )( <1>~(0. )(rr p(U) ), <I>~( n )(rr p( V) ), <I>~( n )(rr p(W) )) 
by the induction hypothesis 
= J(ifT )(tt, <1>~(0. )(rr p(v) ), ci>~(O.)(rr p(w) )) 
= ci>~(O.)(rrp(v)) 
= ci>~(O.)(rrp(ifT(u,v,w))). 
Similarly, if rrp(u) = tT then 
Next, suppose, t is is_c( u) for some constant symbol c E we-. If rr p(u) 
= c then 

1. Operational Semantics for Simple Data Structure Systems 
and 
<1>~+ 1(!l)(is-c(u)) 
=J(is_c)(cl>~+ 1(!l)(u)) 
=J(is_c)( cl>~( n )(rr p(u))) 
= J(is_c)(J(c)) 
by the induction hypothesis 
= tt 
by conditions 1 and 5 on Â§.:interpretations 
= cl>~(!l)(rrp(is_c(u))). 
569 
Othetwise, rr p(u) = g or g( u 1 , ... , um) E TMw for some constant symbol 
or proper function symbol g distinct from c, so 'that 
rrp(is_c(u)) = rp(is_c(rrp(u))) =fT. 
Suppose rr p(c) = g. Then 
<1>~+ 1(!l)(is_c(u)) 
= J(is_c)( cl>~+ 1 ( n )(u)) 
=J(is_c)(cl>~(!l)(rrp(u))) 
by the induction hypothesis 
= J(is_c)(J(g)) 
= ff 
by conditions 1, 3, and 5 on Â§.:interpretations 
Similarly, if rrp(u) = g(u 1, â¢â¢â¢ ,um) then 
<1>~+ 1(!l)(is_c(u)) 
=J(is-c)(cl>~+ 1(!l)(u)) 
= J(is_c)( cl>~( !l)(rr p(u))) 
by the induction hypothesis 
=J(is_c)(J(g)(ci>~(!l)(u 1 ), â¢â¢â¢ , cl>~(!l)(um))) 
= ff 
by Lemma 1 and conditions 2c, 3, and 5 on 
g.: interpretations 

570 
Chapter 18 Operational Semantics of Recursion Equations 
The argument is nearly the same if t is is-f( u), where f is a proper 
function symbol in we . 
Now suppose tis fi- 1(u) and rrp(u) = f(tuÂ·Â·Â·â¢tn) E â¢w,. Then 
and 
=...Y(fi-1 )(<I>~+ I(.O)(u)) 
=...Y(fi- 1 )( <1>~(.0 )(rrp(u))) 
by the induction hypothesis 
by Lemma 1 and conditions 2c on 6 on 
Y.:interpretations 
Finally, suppose t is F(t 1, â¢â¢â¢ , t), where F E FV(P), and let 
F( X 1 , â¢â¢. , X n) = u be the defining equation for F in P. Then 
= <f>~+ I(.O)(F)(<f>~+I(.O)(tl), ... ,<I>~+ I(.O)(tn)) 
= <I>~+ I ( n )(F)( <I>~( n )(rr p(tl))' ... '<I>~( n )(rr p(tn))) 
by the induction hypothesis 
= <1>~(.0) U a(u) 
where 
a(Xj) = <I>~(.O)(rrp(tj)), 1 ~j ~ n 
= <I>~(.O)(uO) 
by the substitution lemma 
â¢ 

1. Operational Semantics for Simple Data Structure Systems 
571 
Lemma 3. Let l = (Y, J) be a complete, continuous W-structure, and 
let P be a W-program. Then for all t E TMw(FV(P)) and for i EN, 
<I>~( n ){t) = O(rr~{t)). 
Proof. We argue by induction on i. If i = 0 then for all t E TMw(FV(P)), 
<l>~(!l)(t) = O{t) = O(rr~(t)), 
so assume the lemma is true for i = k. Then for all t E TMw(FV(P)), 
<1>~+ 1{!l)(t) = <l>~(!l)(rrp(t)) 
= O(rr~(rr p(t))) 
= O(rr~+ 1(t)). 
by Lemma2 
by the induction hypothesis 
(See Exercise 12.) 
â¢ 
Lemma 4. Let l = (Y,J) be a simple W-structure, let P be a W-pro-
gram, and let t E TMw(FV(P)). If O{t) =F ..l T(t), then O(rrp(t)) = O(t) 
and rrp(t) E â¢w,. 
Proof. Let T(t) = T. We argue by structural induction on t, assuming 
throughout that O(t) =F ..l T . If t is a constant symbol c E W, then rr p(c) = 
C E TMw. If tis f(tpÂ·Â·Â·,tn), where f EWe and T(f) = T 1 XÂ·Â·Â· X Tn 
-+T, the~ O(t;) =F ..l T , 1 :::;; i:::;; n, by the strictness of J(f), so O(rrp(t;)) 
= O(t;) and rrp(t;) E'TMw by the induction hypothesis, 1 :::;; i:::;; n, and 
we have 
' 
O(f(t 1 , â¢â¢â¢ ,tn)) =J(f)(O(t 1), â¢â¢â¢ ,0(tn)) 
=J(f){O(rrp(t 1 )), â¢â¢â¢ , O(rrp(tn))) 
where f(tp ... , tn) E TMw. 
Suppose t is ifT( u, v, w ). Then O(u) =F ..l Boo! , so O(rr p{u)) = O(u) and 
rrp(u) E TMw by the induction hypothesis. If O(u) = tt then rrp(u) must 
be tt by condition 3 on .:7-interpretations, and rrp(ifT(u,v,w)) = rrp(v). 

572 
Chapter 18 Operational Semantics of Recursion Equations 
Therefore, 
O(ifT(u,v,w)) 
=J(ifT)(O(u), O(v), O(w)) 
= O(v) 
= O(rrp(v)) 
by the induction hypothesis, since 
O(v) = O(ifT(u, v, w)) * _1_ T 
= O(rrp(ifT(u,v,w))), 
and by the induction hypothesis, rrp(ifT(u,v,w)) = rrp(v) E TMw. If 
O(u) = ff then we get O(ifT( u, v, w )) = O(rrp(ifT( u, v, w ))) 'and 
rrp(ifT(u,v,w)) = rrp(w) E TMw by a similar argument. 
Next, suppose t is is_f(u) for some f EWe with T(f) = T1 X Â· Â· Â· x 
Tn-T. If O(is_f(u)) = tt, then _iT* O(u) E ranJ(f), so O(rrp(u)) = 
O(u) and rrp(u) E TMw by the induction hypothesis. Moreover, O(rrp(u)) 
E ran J(f) implies rrp(~) must be of the form f(t 1 , â¢â¢â¢ , tJ by condition 3 
on Y:interpretations, so rr p(is_f( u )) = tt E TMw and O(rr p(is_f( u))) = 
tt = O(is-f(u)). If O(is_f(u)) = ff, then 
_1_ ~ * O(u) ft. ranJ(f), so 
O(rrp(u)) = O(u) and rrp(u) E TMw by the induction hypothesis. More-
over, O(rrp(u)) ft. ran J(f) implies that rrp(u) cannot be of the form 
f(tp ... ,tJ, so rrp(is_f(u)) = tTE TMw and O(rrp(is_f(u))) = ff = 
O(is_f( u) ). The argument is similar if t 'is is_c( u) for some constant 
symbol C E We-. 
Now suppose tis f;- 1(u), where T(f) = T1 X Â· Â· Â· X Tn-T. Then 
_1_ T 
* O(u) E ran J(f), so O(rrp(u)) = O(u) and rrp(u) E TMw by the induc-
tion hypothesis. Again, rrp(u) must be of the form rft 1, â¢â¢â¢ , tJ, so 
rrp(f;- 1(u)) = t; E â¢w,, and 
O(f;- 1(u)) =J(f;- 1)(0(u)) 
= J(f;- 1 )(O(rr p(u))) 
=J(f;- 1 )(O(f(tl, ... , tn))) 
= J(f;- 1 )(J(f)(O(t 1), â¢â¢â¢ , O(tn))) 
= O(t;) since J(f)(O(t 1), â¢â¢â¢ , O(tn)) * _1_ T 
= O(rrp(f;- 1(u))). 
Finally, if t is of the form F(tpÂ·Â· .,tn), where FE FV(P), then 
O(F(t 1 , â¢â¢â¢ , tJ) = _1_ T and there is nothing to prove in this case. 
â¢ 

1. Operational Semantics for Simple Data Structure Systems 
573 
Theorem 1.1. Let a = rep(~) be a simple data structure system for W. 
Then&!!. =~tJ.Â· 
Proof. 
Let P0 be a W-program, let F E FV(P0 ), and let (d1 , 0 0 0, dn) E 
D,(B(F)) 0 For 1 ::; i ::; n let d; = JLCI>p,(t;) for some W-program P; and some 
t; E TMw(FV(P;)) such that P0 , P1 , o o o, Pn are consistent, and let P = 
U7~o P;o Then 
~!J.(P0 )(F)(d 1 , o o o, dn) 
=~!J.(P0 )(F)(~(t 1 ), o o o, JLCI>p.,(tn)) 
= JLcl>p11(F)(~(tl ), o o o, JLCI>p.,(tn)) 
= JLcl>p(F)(~(tl ), o o o, ~(tn)) 
= ~(F(tl, o o o, tn)) 
= U{cl>~(fl)liEN}(F(t 10 ooo,tn)) 
= U {ct>~(O)(F(tl, 0 0 0, tn)) li EN} 
by the extension lemma 
by Theorem 1702030 
By Theorem 170203 the set {ct>~(O)(F(t 1 ,ooo,tn))li EN} is a chain in the 
flat cpo (D,<F>â¢ ~;;;,<F>), so if u{ct>~(O)(F(t 1 ,. 0 0, tJ) I i EN} * .L p(FJ, then 
U{ci>~(O)(F(t 1 ,. 0 0, t)) I i EN} = ct>~(O)(F(t 1 , 0 0 0, t)) for some small-
est i0 E No Now, 
ci>~(O)(F(t 10 ooo,tn)) = ll(rr~'(F(tpooo,tn))) 
by Lemma 3 
= fi(rr~'+ 1(F(t 1 , o o o, tn))) 
by Lemma4, 
where 
rr~'+
1 (F(t 1 ,ooo,tJ) E TMw, so that 
rr~'+ 1 (F(t 1 ,ooo,tJ) is a 
(T.(P), ur )-normal term, and 
' 
~!J.(P0 )(F)(d 1 ,ooo,dn) = ll(rr~'+
1 (F(t 1 ,ooo,tn))) 
= &!J.(P0 )(F)(d1 , o o o, dn)o 
Otherwise, u {ct>~(O)(F(t 10 0 0 0, t)) I i E N} = .L p(FJ, and 
by Lemma 3, so either the (T.(P), Ur )-computation for F( t I '0 0 0 't n) is 
infinite or it ends with a (T_.(P), ur)-normal term rr~(F(t 1 , 0. 0, t)) such 

574 
Chapter 18 Operational Semantics of Recursion Equations 
that O(rr~(F( t 1 , â¢â¢â¢ , tn ))) = ..L P (Fl , and in either case 
g-a(P0)(F)(dp ... , dn) = ..L p(F) = &'a(P0 )(F)(d1 , ... , dn). 
The choice of P0 , F, and (d 1 , â¢â¢â¢ , dn) was arbitrary, so g-a =&'a. 
â¢ 
Thus &'a is correct with respect to g-a for every simple data structure 
system a. Moreover, there is no ambiguity in the definition of sga, so 
Theorem 1.1 implies that the definition of &'a is independent of the choice 
of programs and terms used to denote values in a. Theorem 1.1 also 
justifies the following 
Definition. Let a be a simple data structure system for W. A function f 
is a-computable if there is a W-program P and F E FV(P) such that 
f = ga(P)(F). 
It follows, then, from our work in the previous chapter that + _j_ is a 
aN-computable function. We have also seen some examples of aNL-com-
putable functions. In the next section we will examine the aN-computable 
functions more closely. 
Exercises 
1. Let (} = {(X 1 ,F(O)),(X3 , s(X 2 ))}. What is G(s(X 2),X)8? 
2. 
Let a be a variable assignment such that a(X) = 1, a(Y) = 2. 
Give a substitution 
(} 
such that 
JL<I>Aoo u a(+ (s(X), Y)) 
= JLci>ADo(+(s(X),Y)8) in !.N. 
3. We have left open the possibility of a W-term rewriting system T, a 
strategy u, and a W-term t such that t is not (T, u )-normal but there 
is no (T, u )-rewrite oft. Verify that this situation does not occur for 
Ts(P) and u, where P is any W-program and u is any of the five 
strategies we have defined. 
4. LetT be the WN-term rewriting system with rewrite rules 
F(X, Y) -s(Y) 
F(X, Y) -F(Y, Y), 
and lett= F(F(2,3),F(4,5)). Give two distinct (T, u)-computations 
for t, where u is (a) uu; (b) u1o; (c) up;; (d) upo. 
5. Give the (Ts(ADD), u )-computation for + (3, 2), where u is (a) uu; 
(b) Up;; (c) upo. 
6. 
Let P be a W-program and let t E TMw(FV(P)). Show that for all 
i EN, rr~(t) E TMw(FV(P)). 

2. Computable Functions 
7. 
Let P be the WN-program with the equation 
F(X) = ifN(is-O(X),1,s(F(s;- 1(X)))). 
(a) Describe T,(P). 
(b) Give the (T,(P), ar)-computation for F(2). 
8. 
Let P =ADDU {B(X) = B(X)}, where r(B) = N -+N. 
(a) Give the (T5(P), ar )-computation for + ( 3, s(s( B( 0))) ). 
(b) Give the (T,(P), ar )-computation for + ( s( s( s( B( 0))) ), 2 ). 
575 
9. 
Without using Theorem 1.1, give the value of each of the following. 
(a) 
&'!':. (ADD)(+ )(3, 2). 
N 
(b) 
&'!':. (ADD)(+)( l_ N '2). 
N 
(c) 
&'!':. (ADD)(+ )(3, _iN). 
N 
10. Let t be the WNL -term cons( 1, cons( 2, nil)). Describe T,(LIST), and 
give the (T,(LIST), ar )-computation for each of the following WNL-
terms. [LIST is defined in Section 5 of Chapter 17.] 
(a) Length( t). 
(b) Nth(1,t). 
(c) 
Cat( t, t ). 
(d) Rev(t). 
11. Without using Theorem 1.1, give the value of each of the following. 
(a) 
&'!':. (LIST) (Length) ( (2, 3) ). 
NL 
(b) &'I':.Nc(LIST)(Length)( l_ NL). 
(c) 
&'I':.Nc(LIST) (Nth) (2, (2, 3, 4) ). 
(d) 
&'!':. (LIST) (Nth) ( l_ NL ' (2, 3, 4 > ). 
NL 
(e) 
&'I':.Nc(LIST)(Cat)( (2, 3), (4, 5) ). 
(f) 
&'I':.Nc(LIST)(Cat)(_l_ NL, (4,5)). 
(g) 
&'!':. (LIST)( Cat) ( (2, 3), _i NL). 
NL 
(h) &'I':.Nc(LIST)(Rev) ( (2, 3) ). 
12. Let D be a set, let fED ----) D, and let d ED. Show that for all 
n EN, r+ 1(d) = r(f(d)). 
2. 
Computable Functions 
Now that we have a new class of computable numeric functions, It IS 
reasonable to compare it to the partially computable functions defined in 
Part 1 of the book. We will show that they are essentially the same, just as 

576 
Chapter 18 Operational Semantics of Recursion Equations 
we showed that a function is partially computable if and only if it is 
computable by Turing machines or Post-Turing programs. The difference 
here is that, technically, the two classes contain different kinds of func-
tions, since in this chapter we have defined computable functions on N1 , 
n ;;::: 1, rather than partially computable functions on Nn. However, this 
distinction is easily overcome. 
We begin with the primitive recursive functions. 
Lemma 1. Iff is primitive recursive, then f _]_ is aN-computable. 
Proof. We argue by induction on the number of compositions and recur-
sions by which f is obtained from the initial functions. The aN-computa-
bility of the initial functions is given by the following programs: 
P, = {S(X) = s(X)} 
Pn = {N(X) = ifN(is-O(X),O,O)} 
where Pocr' is 
{Def1(X) = if8001(is-O(X), tt, tt)} 
and, for n ;;::: 1, Poerâ¢â¢, is Pocrâ¢ together with the equation 
Note that we include Def;, 1 ~ i ~ n, to enforce the strictness of 
JL<I>p .... wn. 
Now let 
where f, g 1, â¢â¢â¢ , g k are primitive recursive. By the induction hypothesis 
there are programs P0 , P1 , â¢â¢â¢ , Pk with function variables F, G 1, â¢â¢â¢ , Gk 
such that f _]_ = JL<I>p (F) and g;j_ = JL<I>p(G;), 1 ~ i ~ n. We assume that 
II 
' 
P0 , P1 , â¢â¢â¢ , Pk are consistent and do not contain the function variable H, 
and we set P to 
n 
UP; U {H(X 1 , â¢â¢â¢ ,Xn) = F(G 1(X 1 , â¢â¢â¢ ,Xn), ... ,Gk(X 1 , â¢â¢â¢ ,Xn))}. 
i=O 

2. Computable Functions 
577 
Then for any (x1 , â¢â¢â¢ , xn) EN~ we have 
JL<I>p(H)(x 1 , â¢â¢â¢ , xn) 
= <l>p( JL<I>p)(H)(x 1 , â¢â¢â¢ , xn) 
= J,t<l>p(F)( J,t<l>p(G 1 )(x 1 , â¢â¢â¢ , Xn ), ... , JL<I>p(G k )(x 1 , â¢â¢â¢ , Xn)) 
= JL<I>p0(F)(JL<I>p,(G 1)(x1 , â¢â¢â¢ , xn), ... , J,t<l>pk(Gk)(x 1 , â¢â¢â¢ , xn)) 
by the extension lemma 
= L (g~ (x 1 , â¢â¢â¢ ,xn), ... ,g: (x 1 , â¢â¢â¢ ,xn)). 
L (g~ (xl , ... ,xn), ... ,g: (xl , ... ,xn)) 
= f( g 1 (X 1 , â¢â¢â¢ , X n ) , ... , g k (X 1 , â¢â¢â¢ , X n )} 
and if not then 
so JL<I>p(H) = h _]_ . 
Finally, let 
h(x1 , â¢â¢â¢ ,xn,O) =f(x1 , â¢â¢â¢ ,xn) 
h(x1 , â¢â¢â¢ ,xn,y + 1) =g(y,h(xJ>Â·Â·Â·â¢xn,y),x 1 , â¢â¢â¢ ,xn), 
where g, h are primitive recursive. By the induction hypothesis there are 
programs P1, Pg with function variables F, G such that JL<I>plF) = f _]_ and 
JL<I>p8(G) = g _]_ . We assume that P1, Pg are consistent and do not contain 
the function variable H, and we set P to P1 u Pg u {H(X 1, â¢â¢â¢ ,Xn, Y) = t}, 
where tis 
if N (is-O(Y), 
F(X 1 , â¢â¢â¢ ,Xn), 
G(s 1- 1(Y),H(X 1 , â¢â¢â¢ ,Xn,s1- 1(Y)),X 1 , â¢â¢â¢ ,Xn)). 
Let (xi> 0 
0 0' xn) EN~ 
0 It is clear that JL<I>p(H)(xl '0 0 
0 'Xn' .LN) = .LN' 
so to conclude the proof we argue by induction on y 
that 

578 
Chapter 18 Operational Semantics of Recursion Equations 
= JLcl>p(F)(x 1 , â¢â¢â¢ ,xn) 
= JL<I>p/F)(x 1 , ... , xn) 
Assume, now, that JL<I>p(H)(x 1 , â¢â¢â¢ , xn, y) = h _j_ (x1 , â¢â¢â¢ , xn, y). Then 
JLcl>p(H)(x 1 , â¢â¢â¢ , xn, y + 1) 
= JL<I>p u a(xl , ... ,x. ,y+ l)(t) 
= g _!_ (y, h _!_ (x 1 , â¢â¢â¢ , Xn, y ), X 1 , â¢â¢â¢ , Xn) 
â¢ 
Theorem 2.1. Iff is partially computable, then f _j_ is aN-computable. 
Proof. 
Let f be a partially computable n-ary function. By Theorem 3.3 in 
Chapter 4, there is a primitive recursive predicate R(x 1 , â¢â¢â¢ , xn, y) such 
that 
f(x 1 , â¢â¢â¢ , xn) = /( minR(x 1 , â¢â¢â¢ , xn, z)), 
z 
and by Lemma 1 there are WN-programs PR, P1 with function variables R 
and L such that JLcl>p (R) = R _j_ and JL<I>p(L) = I _j_ â¢ We assume that PR, P1 
R 
I 
are consistent and do not include function variables F, G, and we set P to 
PR u P1 together with the equations 
F(X 1 , ... ,Xn) = L(G(Xp ... ,Xn ,0)) 
G(X 1 , â¢â¢â¢ , Xn, Y) = ifN(is-s(R(X 1, â¢â¢â¢ ,Xn, Y)), Y,G(X1 , â¢â¢â¢ ,Xn ,s(Y))). 

2. Computable Functions 
Let (x 1 , â¢â¢â¢ , xn) E N1. It is clear that, for all y EN, 
JL<I>p(G)(x 1 , â¢â¢â¢ , Xn, y) 
-
{yl_N 
JL<I>p(G)(x 1 , â¢â¢â¢ , xn, y + 1) 
and so 
if (xI> ... ' xn 'y) ft Nn +I 
if R(x1 , â¢â¢â¢ ,xn,y) 
otherwise, 
if 
(X I> ... , X n , 0) ft N n + 1 
if f( X 1 , â¢ â¢ â¢ , X n ) j 
otherwise. 
Therefore, if (xl' ... ' xn) E Nn and f(xl ' ... ' xn) J,' then 
JL<I>p(F)(x 1 , â¢â¢â¢ , xn) =I _j_ (minz R(x1 , â¢â¢â¢ , xn, z)) 
= /(minz R(x 1 , â¢â¢â¢ , xn, z)) 
=f(xl, ... ,xn), 
and 
otherwise, so that 
JL<I>p(F)(x1 , â¢â¢â¢ , xn) = f _j_ (x1 , â¢â¢â¢ , xn) 
for all (x1, ... , xn) E N1, i.e., JL<I>p(F) = f _j_ â¢ 
579 
â¢ 
To prove a result in the other direction, we need a way of going from 
functions on N1 to partial functions on Nn. 
Definition. For any function f: N1 ~ N _j_ , let f" be the partial n-ary 
function on Nn defined 
if f(x 1 , â¢â¢â¢ , xn) =F l_N 
otherwise. 
Theorem 2.2. Iff is AN-computable, then f" is partially computable. 
Proof. The proof is similar to the proof in Chapter 4 that the STP<n> 
predicates are primitive recursive. We encode WN-terms, WN-programs, 
and WN-substitutions as numbers and give a numeric version of the rrp 

580 
Chapter 18 Operational Semantics of Recursion Equations 
functions. All WN-terms are words in the 20 symbol alphabet A u WN, so 
for any word wE (AU WN)*, we let # 20(w) be the numeric value ofw in 
base 20 notation, as defined in Chapter 5. We will use # 20 to encode 
variables and function symbols. We encode each WN-term t as #(t), where 
#(<!>) = (#zo(<f>), 0) 
#(<f>(tl ' ... 'tn)) 
if <1> is a constant symbol or 
individual variable 
= ( # 20 ( <I>), [ #(t 1 ), â¢â¢â¢ , #(tn)]) 
if <I> is a proper function 
symbol or function variable. 
If Pis aWN-program 
{F1(Xl, ... ,X~) = t 1, â¢â¢â¢ ,Fm(x;n, ... ,X::',.) = tm}â¢ 
then we associate with P the finite numeric function 
and if () is aWN-substitution {(X 1 , t 1), ... , (Xn, tn)}, then we associate with 
() the finite numeric function 
We encode any finite numeric function c/J = {(xp y 1 ), â¢â¢â¢ , (xn, Yn)} as 
n 
J> = 0PJ;+I, 
i= 1 
' 
and we set #(P) = J>P and #(0) = (b6 for any WN-program P and WN-sub-
stitution 0. 
Now we define some numeric functions for handling WN-terms, their 
values, and their encoding numbers: 
NUM(O) = #(0) 
NUM(x + 1) = (#20(s), [NUM(x)]) 
TERM(z, XI' .â¢. ' xn) = (z, [NUM(xl), ... ' NUM(xn)]) 
{
0 
ifx=#(O) 
EVAL(x) = 
EiVAL((r(x)) 1) + 1 
if /(x) = # 20(s) 
otherwise 
if X= #(0) 
if /(x) = # 20(s) 
otherwise. 

2. Computable Functions 
581 
It is clear that NUM(n) = #(n) and EVAL(#(n)) = n for any n EN; 
TERM(#20(F), m 1 , â¢â¢â¢ , mn) = #(F(m 1 , â¢â¢â¢ , mn )) for any function variable 
F; and IS-NUM(x) is the predicate that tests whether x = #(n) for some 
numeral n. 
Next we define two functions for handling WN-substitutions: 
Lt(r(l )) 
MAKE-SUB(y,t)= 0 
<r(I)),+I 
. 
P(l((Y)/(1).:.1)), 
â¢=1 
(s)1 ..:... 1 
if r(t) = 0 and (s)1 =/= 0 
if r(t) = 0 and (s)1 = 0 
APPLY -SUB(s, t) = 
Lt(r(t)) 
(l(t), n PiAPPLY_SUB(s,(r(t)),)) 
i=l 
otherwise. 
If Pis aWN-program and F(X 1, â¢â¢â¢ , X) = u is an equation in P, then 
Also, if () is aWN-substitution and t is aWN-term, then 
APPLY -SUB(#(O), #(t)) =#(tO). 
Finally, we define some functions for handling (T.(P), ur)-rewriting: 
RP(y, t) = 
APPLY -SUB (MAKE-SUB (y, t), r((y )1<1> ..:... 1)) 
(r(t ))2 
(r(t ))3 
(r(t ))2 
(r(t ))3 
#(tt) 
#(tT) 
if (y )/(1) =/= 0 
if l(t) = # 20(if800L) and (r(t))1 = #(tt) 
if l(t) = # 20(if800L) and (r(t))1 = #(tT) 
if l(t) = # 20(if N) and (r(t ))1 = #(tt) 
if l(t) = # 20(if N) and (r(t ))1 = #(tT) 
if l(t) = # 20(is_O) and (r(t))1 = #(0) 
if l(t) = # 20(is_O) and l((r(t))1) = # 20(s) 
and IS-NUM(t) 
#(tt) 
if l(t) = # 20(is_s) and l((r(t ))1) = # 20(s) 
and IS-NUM(t) 
#(tT) 
if l(t) = # 20(is_s) and (r(t))1 = #(0) 
(r((r(t))1))1 
if l(t) = # 20(s1- 1 ) and l((r(t))1) = # 20(s) 
and IS-NUM(t) 
otherwise 

582 
Chapter 18 Operational Semantics of Recursion Equations 
( 
Lt(r(t)) 
) 
RRP(y, t) = RP y, (/(t), I1 P;RRP(y,(r(t)),)) 
RRP*(y,t,O) = t 
RRP*(y,t,r + 1) = RRP(y,RRP*(y,t,r)) 
RRPT*(y, z, XI' ... ' Xn 'r) = RRP*(y, TERM(z, XI' ... ' xn), r) 
END(y, z, XI' ... ' xn) = min [RRPT*(y, z, XI' ... ' xn 'r) 
r 
= RRPT*(y, z, XI' ... ' xn 'r + 1)]. 
If Pis aWN-program and tis aWN-term, then RP(#(P), #(t)) = #(rp(t)), 
RRP(#(P), #(t)) = #(rrp(t)), and RRP*(#(P), #(t), i) = #(rr~(t)). Also, if 
F is a function variable and (m1, ... , mn) E Nn, then 
RRPT*(#(P),#20(F),m1, ... ,mn,i) = #(rr~(F(m 1
, ... ,mn))), 
and END(#(P), # 20(F), m 1 , â¢ â¢â¢ , mn) is the smallest i such that 
rr~(F(m 10 â¢â¢â¢ ,m)) is (Ts(P), ur)-normal, if such ani exists, and is unde-
fined otherwise. 
Now, let f: N~ ~ N _~_ be a AN-computable function, let f = JL<I>p(F), 
let a = # 20(P), and let b = # 20(F). Then it is clear that 
f71"(xl ' ... ' xn) = EV AL(RRPT*(a, b, XI' ... ' xn 'END( a, b, XI' ... ' xn))) 
for all (x 1 , ... ,xn) ENn. EVAL, RRPT*, and END are partially com-
putable, so !71" is partially computable. 
â¢ 
Exercises 
1. Show that for all n > 0 and all (x 1 , ... , xn) EN~, 
( TRUE 
if(x1 , ... ,xn) E Nn 
9J!J. (PDef")(Defn)(x1 , â¢â¢â¢ , xn) = 
h 
Â· 
N 
..l Bool 
ot erw1se. 
2. For each of the following functions f from Chapter 3, give a WN-pro-
gram P with F E FV(P) such that 9Jt:.N(P) (F) = f _~_ . 
(a) f(x, y) = x Â· y. 
(b) f(x) = x!. 
(c) 
f(x, y) = xY. 
(d) f(x) = p(x ). 
(e) 
f(x, y) = x...:... y. 
(f) 
f(x, y) = lx- yl. 
(g) 
f(x) = a(x). 

2. Computable Functions 
583 
3. Let P(x), Q(x) be primitive recursive predicates, and let P, Q be 
WN-programs such that gllN(P)(Fp) = P _]_ and gllJP)(FQ) = Q _]_ . For 
each of the following predicates R(x), give a WN-program R with 
F R E FV(R) such that gil (R)(F R) = R _]_ . [Also see Exercise 3.12 in 
N 
Chapter 17.] 
(a) R(x) = - P(x). 
(b) 
R(x) = P(x) & Q(x). 
(c) 
R(x) = P(x) v Q(x). 
(d) 
R(x) = (3z),xP(z). 
(e) 
R(x) = (Vz),xP(z). 
4. Let P(x, y) be a primitive recursive predicate, and let P be a WN-
program such that gllN(P)(Fp) = P _]_. Give a WN-program R with 
MinR E FV(R) such that gllN(R)(MinR) is the strict extension of 
minz, x P(z, y). 
5. For each of the following predicates P from Chapter 3, give a 
WN-program P with Fp E FV(P) such that gllN(P)(Fp) = P _]_. 
(a) 
P(x, y) = x = y. 
(b) 
P(x,y) =x ~y. 
(c) 
P(x,y) =x <y. 
(d) 
P(x, y) =xI y. 
(e) 
P(x) = Prime(x). 
6. For each of the following functions f from Chapter 3, give a WN-pro-
gram P with FE FV(P) such that gllN(P)(F) = f _]_. 
(a) f(x,y) = lxjyj. 
(b) f(x, y) = R(x, y). 
(c) f(x) = PxÂ· 
(d) f(x,y) = (x,y). 
(e) f(x) = l(x). 
(f) 
f(x) = r(x). 
(g) f(xl, ... ,xn) = [xlâ¢Â·Â·Â·â¢xn]. 
(h) f(x, y) = (x)Y. 
(i) 
f(x) = Lt(x). 
7. Show that RRPT* is primitive recursive. 
8. Let #: {Y programs} ~ N be the coding function for Y programs 
given in Chapter 4. Give a Wy-program P with C E FV(P) such that 
gil (P)(C) =#_]_.[See Section 5 in Chapter 17 for the definitions ofWy 
and ay.] 

584 
Chapter 18 Operational Semantics of Recursion Equations 
9.* (a) Give a WN-program SMN with S E FV(SMN) such that 
9~JSMN)(S) is the strict extension of S/. [See the parameter 
theorem in Chapter 4 for the definition of S/.] 
(b) Let sl: N X {3" programs} ~ {3" programs} be defined: For all 
3" programs .9 and all u E N, 
<t><2l(x, u, #(.9)) = <1>( x, #(s l (u, .9))). 
Give a Wy -program SMN with S E FV(SMN) such that 
9~)SMN) (S) is the strict extension of s l . 
3. 
Operational Semantics for lnfinitary Data 
Structure Systems 
We turn now to the operational semantics for infinitary data structure 
systems. It differs in two respects from the operational semantics we gave 
for simple data structure systems. First, for a term such as is_f( f( t) ), we 
cannot be sure that JL<I>p(is_f(f(t))) = tt when J(f) is strict because 
JL<I>p(t) might be .l r(tl â¢ Therefore, we defined Ts(P) so that we rewrite 
is_f( f( t)) to tt only when t is free of variables, which guarantees that 
JL<I>p(t) * .l T(l). Moreover, if JL<I>p(t) * .l r(t)' then <l>~(!l)(t) * .l r(t) for 
some smallest i 0 , and Lemmas 3 and 4 guarantee that twill eventually be 
rewritten to some term which is free of variables. In an infinitary data 
structure system, however, this problem does not arise because we always 
have JL<I>p(f( t)) * .l p(fl â¢ Therefore, we can replace T,(P) with a simpler, 
indeed finite, term rewriting system. 
Definition. 
Let P be a W-program. The infinitary W-term rewriting system 
associated with P, denoted T;(P), consists of 
F(X 1 , â¢â¢â¢ , X) -+u for each equation F(X 1 , â¢â¢â¢ , Xn) = u in P, 
together with 
â¢ for each T E TV(W), 
ifT(tt, X, Y)-+ X 
ifT(ff, X, Y)-+ y 

3. lnflnltary Data Structure Systems 
585 
e for each COnstant symbol C EWe-, With T(C) = T, 
is-c(c) -tt 
is-c(d) -ff 
for each d EWe-- {c} With T(d) = T 
is-c(f(Xl ' ... 'Xn)) -ff for each f Ewe with 
T(f) = T 1 X â¢â¢â¢ X Tn-T 
â¢ for each proper function symbol f E We , with ar(f) = n and p(() = T, 
is-f(f(X 1 , â¢â¢â¢ , Xn)) -u 
is-f(c) -ff 
is-f(g(X 1 , â¢â¢â¢ , Xm)) -ff 
for each C EWe With T(c) = T 
for each g EWe - {f} with 
ar(g) = m and p(g) = T 
for 1 ::; i ::; n . 
Again, the choice of particular individual variables is unimportant, as long 
as they are of the appropriate type and in each rewrite rule the variables 
are distinct. 
It is easy to see that all term rewriting systems T;(P) are deterministic 
and that (T;(P), uc )-computations are a reasonable sort of computation. In 
this section we will write r P and rr P for r T,(P> and rr T,(P>, respectively. 
The other difference we need to address is that there are infinite chains 
in infinitary data structure systems. In particular, there are terms t such 
that O(rr~(t)) = <I>~( n )(t) CT(I) JL<I>p(t) for all i E N, so that we get an 
infinite computation 
t =====> rrp(t) =====> rr~(t) ===> 
T1(P), u 1 
T1(P), u 1 
T1(P), u 1 
which never reaches the desired value JL<I>p(t). Therefore, we cannot 
expect to base the operational semantics on the final terms of finite 
computations. Instead, we take the point of view that an infinite computa-
tion produces ever better approximations to the actual value and that the 
entire computation gives the meaning of the function being computed. 
Definition. Let a = rep(I) be an infinitary data structure system for W. 
The operational meaning function for a, denoted &!!. , is defined as follows. 
For all W-programs P0 , all FE FV(P0 ), and all (d1 , â¢â¢â¢ , dn) E D,( 5(F)), 
&!J.(P0)(F)(d1 , â¢â¢â¢ , dn) = U {O(rr~(F(t 10 â¢â¢â¢ , tn))) I i EN}, 

586 
Chapter 18 Operational Semantics of Recursion Equations 
where 
â¢ d; = JL<I>i(t), 1 :::;; i :::;; n, 
â¢ P0 , P1 , â¢â¢â¢ ', Pn are consistent, and 
â¢ P= U7~oP;. 
The proof that &11 is correct with respect to 9111 for all infinitary data 
structure systems is very much like the proof of Theorem 1.1. 
Lemma 1. 
Let I = (.5T,J) be an infinitary W-structure, and let P be a 
W-program. Then for all t E TMw(FV(P)) and for all i EN, <1>~+ 1(.0)(t) = 
<I>~( n )(rr p(t)). 
Proof. The proof is almost identical to the proof of Lemma 2 in Section 
1. The only differences occur in the cases where tis of the form is_f(u) or 
f;- 1(u). In particular, if is_f(rrp(u)) or f;- 1(rrp(u)) match a rewrite rule in 
T;(P), then we do not necessarily have rrp(u) E TMw , so we cannot 
c 
appeal to Lemma 1 in Section 1 to show that <I>~(.O)(rrp(u)) =F ..l T(u). 
However, rr p(u) must be of the form g or g( ul ' ... ' um) for some g E we' 
so that <I>~( n )(rr p(u)) =F ..l T(u) is certainly true in an infinitary W-struc-
ture, and the argument goes through unchanged but for the reference to 
Lemma 1 in Section 1. 
â¢ 
Repeating the proof of Lemma 3 in Section 1 gives us 
Lemma 2. 
Let I = (.57, J) be an in finitary W-structure, and let P be a 
W-program. Then for all t E TMw(FV(P)) and for all i EN, <I>~(.O)(t) = 
O(rr~(t)). 
Now the proof of Theorem 3.1 is even simpler than the proof of 
Theorem 1.1. 
Theorem 3.1. Let A = rep(I) be an infinitary data structure system for 
W. Then &'11 = 9111 â¢ 
Proof. 
Let P0 be a W-program, let F E FV(P0), and let (d1 , â¢â¢â¢ , dn) E 
Dr(B(F)). For 1 :::;; i :::;; n let d; = JL<I>p,(t) for some W-program P; and some 
t; E TMw(FV(P)) such that P0 , P1 , .â¢â¢ , Pn are consistent, and let P = 
U7=o P;. Then 
9J11(P0)(F)(d1 , â¢â¢â¢ , dn) 
U {<I>~(.O)(F(t 1 , â¢â¢â¢ , tn)) I i EN} as in the proof of Theorem 1.1 
u {!1(rr~(F(t 1 , â¢â¢â¢ , tn))) I i EN} 
by Lemma 2 
= &'11(P0)(F)(d1 , â¢â¢â¢ , dn). 
â¢ 

3. lnfinitary Data Structure Systems 
587 
As we did for simple data structure systems, we can now define com-
putable functions in infinitary data structure systems. 
Definition. Let a be an infinitary data structure system for W. A function 
f is a-computable if there is a W-program P and FE FV(P) such that 
f = .f~iP)(F). 
We can also use Lemma 2 to justify the name computable real numbers 
used in the previous chapter. 
Definition. Let l be an infinitary W-structure, and let T E TV(W). An 
element d E DT is computable if there is some W-program P and some 
term t E TMw(FV(P)) such that d = U{O(rr~(t)) I i EN}. 
Now we can easily prove 
Theorem 3.2. Let l be an infinitary W-structure, and let T E TV(W). 
Then an element d EDT is representable if and only if it is computable. 
Proof. 
Let d E DT. If d is representable then there is a W-program P 
and a term t E TMw(FV(P)) such that 
d =~(t) 
= U{ci>~(O) I i E N}(t) 
= u { ci>~(O)(t) I i EN} 
= u {n(rr~(t)) 1 i EN} 
by Theorem 17 .2.3 
by Lemma 2, 
so d is computable. Similarly, if d is computable then there is a W-pro-
gram P and a term t E TMw(FV(P)) such that 
d = u {O(rr~(t)) I i EN} = JL<I>p(t), 
and so d is representable. 
â¢ 
Now that we have available all of the strict extensions of the partially 
computable functions, we conclude with two promised examples of com-
puting with infinite objects. We will be working in infinitary data struc-
tures, but it is not hard to verify that if f(x 1 , â¢â¢â¢ , xn) is partially com-
putable and Pis aWN-program obtained in the proof of Theorem 2.1 such 
that gr,)P) (F) = f .L , then 
(3.1) 

588 
Chapter 18 Operational Semantics of Recursion Equations 
Primes(X) = Sieve(Seq(X)) 
Seq(X) = cons(X, Seq(s(X))) 
Sieve(L) = cons(cons! 1 (L), Sieve(Eiim(cons1- 1 (L),consz- 1 (L)))) 
Elim(X,L) = ifNdX lcons 1- 1(L), 
Elim(X,cons2 1 (L) ), 
cons(cons 1- 1 (L), Elim(X,consz- 1 (L)) )) 
Figure 3.1. The main part of program PR. 
for all (x1 , ... , xn) E Nn. We will now freely write f(xt> ... , xn) as a 
macro in WNL-programs and Wa-programs when f(x 1 , â¢â¢â¢ , xn) is partially 
computable. If P(x 1 , â¢â¢â¢ , xn) is a computable predicate, then when we 
write P(x 1 , â¢â¢â¢ , xn) in a program its range should be understood as 
{ ..l Boot ' tt, ff}. 
The first example is a WNL -program for generating the list ( p 1 , p 2 , â¢â¢â¢ ) 
of all prime numbers. It is based on the method known as Eratosthenes' 
sieve, where we start with the list (2, 3, 4, ... ), eliminate all numbers 
divisible by 2, then eliminate all numbers divisible by 3, all numbers 
divisible by 5, etc. Let PR be the WNL-program with the equations in Fig. 
3.1 along with the definition of the predicate xI y, i.e., "x divides y." Then 
in I~L we have 
JL<I>pa (Primes(2)) 
= J,t<l>pa(Sieve)( J,t<1>pa(Seq)(2)) 
= J,t<1>pa(Sieve)((2, 3, 4, ... )) 
= (2, JL<I>pa(Sieve)( JL<1>pa(Eiim)(2, (3, 4, 5, ... )))) 
= (2, JL<I>pa(Sieve)((3, 5, 7, ... ))) 
= (2, 3, JL<I>pa(Sieve)( JL<1>pa(Eiim)(3, (5, 7, 9, ... )))) 
= (2, 3, JL<I>pa(Sieve)((5, 7, 11, ... ))) 
= (2,3,5, ... ) 
where the notation (2, JL<I>pa(Sieve)( (3, 5, 7, ... ))) means the list with 2 
followed by the elements of the list JL<I>pa(Sieve) ( (3, 5, 7, ... ) ). Therefore, 
the list of primes is representable. Moreover, Theorem 3.2 shows that it 
can be generated by a (T;(PR), ur )-computation. 

3. lnfinitary Data Structure Systems 
589 
Finally, we show that there are computable irrational numbers. It is a 
mathematical fact that the well-known irrational number e = 2. 7182 ... 
can be expressed as 2 + if + -t + Â· Â· Â· , and we can use this fact to show 
that e/10 = .27182 ... is computable.5 The idea is to consider 1f + }f 
+ Â· Â· Â· as a sort of base notation system analogous to the decimal system, 
where .354, for example, represents -fh + 1 ~, + ~~-' . All we have to do, 
then, is to change 2 + if + -t + Â· Â· Â· (divided by 10) into its decimal 
representation. The procedure consists of taking the integer part as the 
first decimal digit, multiplying the fractional part by 10, normalizing the 
result (i.e., reducing the numerators by carrying), and then repeating these 
steps with the normalized result. The correct method for carrying is given 
by the equations 
m 
n + 1! 
(n + 1)lmj(n + 1)j + R(m, n + 1) 
n + 1! 
lmj(n + l)j 
n! 
+ 
R(m, n + 1) 
n + 1! 
For example, starting with 2 + if + -t + .,& we get 2 as the first decimal 
digit, and then we get 
10 . (if + -t + .,& ) = -w + w + * 
14 
() 
2 
7 
0 
0 
2 
= 2T + 3T + 4T = 
+ 2T + 3T + 4!â¢ 
so 7 is the second decimal digit. Next we get 
10 ( 0 
0 
2 ) 
0 
0 
20 
. 2!+3!+4! =2!+3!+4! 
0 
50 
0 
I 
2 
0 
= 2T + 3T + 4T = 
+ 2T + 3T + 4f, 
so 0 is the third decimal digit. Notice, however, that if we start with 
2 + if + -t + .,& + -tr , we get .271 instead of .270. That is, we get more 
precision by starting with more terms. However, if we want to get the 
decimal expansion of the infinite sum 2 + I:~~ 2 ;!r , then at any given 
iteration we certainly cannot perform the entire multiplication by 10 
before beginning the carry step. Fortunately, we need to perform only 
enough of it so that multiplying and normalizing any additional terms 
would not change the decimal digit produced by the current iteration. 
5 This example is due to D. A. Turner, who credits E. W. Dijkstra with the idea. 

590 
Chapter 18 Operational Semantics of Recursion Equations 
K(X) = cons(X, K(X)) 
Convert(L) = dcons(Digit(First(L)), 
Convert(Norm(2, cons(O, MultlO(Rest(L)))))) 
Norm(C, L) = ifNdSecond(L) + 9 < C, 
cons(First(L), Norm(s(C), Rest(L)) ), 
Carry(C, cons(First(L), Norm(s(C), Rest(L))))) 
Carry(C, L) = cons(First(L) + [Second(L) jCJ 
cons(R(Second(L), C), Rest2(L))) 
MultlO(L) = cons(IO Â· First(L), MultlO(Rest(L))) 
First( L) = cons 1- 1 ( L) 
Second(L) = cons 1- 1 (cons 2- 1 (L)) 
Rest(L) = cons 2- 1 (L) 
Rest2(L) = cons; 1 (cons;! 1(L)) 
Figure 3.2. The main part of program E. 
Now, the carry procedure leaves fractions of the form R(m, n)jn!, where 
R(m, n) < n, so when we multiply by 10 we get 10 Â· R(m, n)jn!, and then 
we carry llO Â· R(m, n)jnJ < 10njn = 10. So the maximum possible carry 
is always 9, and if we have a term mjn! such that m + 9 < n, there will 
never be a carry out of mjn!, regardless of what is carried into mjn!. 
Therefore, when we reach such a term, we can be sure that we have 
enough information to produce a correct decimal digit. Let E be the 
program with the equations in Fig. 3.2 together with the appropriate 
definitions of addition, multiplication, integer division, and remainder. 
Also, Digit, with T(Digit) = N -+ D, must be defined so that JL<f>iDigit) 
turns natural numbers 0, ... , 9 into decimal digits 0, ... , 9. Now, JL<f>iK(l)) 
is the infinite list (1, 1, ... ) E DNL", which we use to represent L:~ ~ 2 ljn!. 
We leave it as an exercise to check that JL<f>iConvert(cons(2,K(l)))) = 
e j10 E DoL". So e j10 is representable, and Theorem 3.2 justifies calling 
ej10 a computable real number. 
Exercises 
1. Let P =ADDU {B(X) = B(X)}, where T(B) = N -+N. Give T;(P), 
and give the (T;(P), ur )-computation for each of the following. 
(a) 
+ (3,2). 
(b) 
+(3,s(s(B(O)))). 
(c) 
+ (s(s(s(B(0)))),2). 

3. lnflnltary Data Structure Systems 
591 
2. 
Without using Theorem 3.1, give the value of each of the following. 
(a) 
&'~'N(ADD)( + )(3, 2). 
(b) 
&'~'N(ADD) ( +) (3, 2 j_ ). 
(c) 
&'~'N(ADD) ( +) (3 j_, 2). 
3. Let P = LIST U {B(X) = B(X), BL(X) = BL(X)}, where T(B) = 
N -+Nand T(BL) = NL -+NL, lett be cons(B(O),cons(l,nil)), and 
let u be cons( 0, cons( 1, BL( nil))). [LIST is defined in Section 5 of 
Chapter 17.] Give T;(P), and give the (T;(P), ur)-computation for each 
of the following. 
(a) Length(t). 
(b) Length( u). 
(c) 
Nth( 0, t). 
(d) Nth( I, u). 
(e) 
Cat(t,t). 
(f) 
Cat(t, u). 
(g) 
Cat(u, t). 
(h) Reverse( t). 
(i) 
Reverse( u). 
(j) Length( Cat(Rev( t), t) ). 
4. 
Let /1 = (..1. N~, 1), /2 = (0, 1) j_. Without using Theorem 3.1, give 
the value of each of the following. 
(a) 
&'~'NL(LIST) (Length) (/1 ). 
(b) 
&'~'NL(LIST)(Length}(/2 ). 
(c) 
&'sr.L(LIST)(Nth)(O, /1). 
(d) 
&'~'NL(LIST)(Nth) (1, /2). 
(e) 
&'~'NL(LIST)(Cat}(/ 1 , / 1). 
(f) 
&'~'NL(LIST)(Cat)(/ 1
, / 2 ). 
(g) 
&'~'NL(LIST)(Cat)(/2 , / 1). 
(h) 
&'~'NL(LIST) (Rev) (/1 ). 
(i) 
&'~~ (LIST)(Rev)(/2). 
NL 
5. 
Give a WN-program with F E FV(P) such that g~N(P) (F) (0) = ..1. N 
and 
g~'N(P)(F)(O) = 0. Verify that 
&'~N(P)(F)(O) = ..1. N 
and 
&'~~(P)(F)(O) = 0. 
N 
6. 
Verify the sentence containing (3.1). 
7. 
Give the (T;(PR u LIST), ur)-computation for Nth(l, Primes(2)). 
8. Suppose we change the defining equation for Norm in E to 
Norm(C, L) = Carry(C, cons(First(L), Norm(s(C), Rest(L)))). 
Now what is ~(Convert(cons(2, K(l))))? 

592 
Chapter 18 Operational Semantics of Recursion Equations 
9. Show that if A is a nonempty r.e. set, then there is a computable list 
I= (i 0 , i 1 , .â¢â¢ ) in DNL" such that A = {i EN I i occurs in 1}. 
10.* Show that if I = (i0 , i1 , â¢â¢â¢ ) is a computable list of numbers in DNL', 
then {i EN I i occurs in I} is r.e. [Hint: Adapt the proof of Theorem 
2.2.] 

Suggestions for Further Reading 
C. L. Chang and R. C. T. Lee, Symbolic Logic and Mechanical Theorem Proving. Academic 
Press, New York, 1973. 
A very readable treatise on resolution-based algorithms for satisfiability in quantification 
theory. 
Martin Davis, Computability and Unsolvability. Dover, New York, 1983. 
Originally published in 1958. The 1983 reprint includes an appendix on unsolvable 
problems in number theory. 
Martin Davis (editor), The Undecidable. Raven, New York, 1965. 
A collection of basic papers in computability theory. Included are the original papers in 
which Church announced his "thesis," in which Turing defined his machines and produced a 
universal computer, in which Post stated his "problem," and in which Turing introduced 
"oracles." 
Herbert P. Enderton, A Mathematical Introduction to Logic. Academic Press, New York, 1972. 
An introductory textbook on mathematical logic for mathematically mature readers. 
Michael R. Garey and David S. Johnson, Computers and Intractability: A Guide to the Theory 
of NP-Completeness. Freeman, New York, 1979. 
This treatise includes a comprehensive list of NP-complete problems. 
Carl A. Gunter, Semantics of Programming Languages. MIT Press, Cambridge, Massachusetts, 
1992. 
A treatment of denotational semantics for the sophisticated reader. 
Paul R. Halmos, Naive Set Theory. Van Nostrand, Princeton, New Jersey, 1964. 
A short, classic introduction to set theory. 
593 

594 
Suggestions for Further Reading 
Michael Harrison, Introduction to Formal Language Theory. Addison-Wesley, Reading, Mas-
sachusetts, 1978. 
A comprehensive, up-to-date, readable treatise on formal languages. 
Karel Hrbacek and Thomas Jech, Introduction to Set Theory, second edition. Marcel Dekker, 
New York, 1984. 
Another introduction to set theory, somewhat more detailed than the book by Halmos. 
Harry R. Lewis and Christos H. Papadimitriou, Elements of the Theory of Computation. 
Prentice-Hall, Englewood Cliffs, New Jersey, 1981. 
Another introduction to theoretical computer science. 
Jacques Loeckx and Kurt Sieber, The Foundations of Program Verification, second edition. 
John Wiley and Sons, New York, 1987. 
A well-written treatment of programming language semantics with an emphasis on 
program verification. 
Donald W. Loveland, Automated Theorem Proving: A Logical Basis. North-Holland Publ., 
Amsterdam, 1978. 
A well-organized account of resolution theory. 
Michael Machtey and Paul Young, An Introduction to the General Theory of Algorithms. 
North-Holland Publ., Amsterdam, 1978. 
A well-written account of computability and complexity theory. 
Hartley Rogers, Theory of Recursive Functions and Effective Computability. McGraw-Hill, New 
York, 1967. 
The classic comprehensive treatise on computability and noncomputability. 
David Schmidt, Denotational Semantics: A Methodology for Language Development. Wm. C. 
Brown Publishers, Dubuque, Iowa, 1988. 
A good general introduction to the denotational semantics of programming languages. 
Joseph R. Shoenfield, Degrees of Unsolvability. North-Holland Publ., Amsterdam, 1971. 
A short and clearly written monograph on the subject going well beyond the material 
covered in this book. 
Robert I. Soare, Recursively Enumerable Sets and Degrees: The Study of Computable Functions 
and Computably Generated Sets. Springer-Verlag, Berlin and New York, 1987. 
A modem treatment of advanced recursive function theory. 
Joseph E. Stoy, Denotational Semantics: The Scott-Strachey Approach to Programming Lan-
guage Theory. MIT Press, Cambridge, Massachusetts, 1977. 
An early treatment of denotational semantics. 

Notation Index 
E 
1 
(3t), (Vt) 
7 
0 
1 
â¢ 
9 
c;;;;,c 
1 
..:7 
17 
u,n 
2 
1/J~m>(r,, ... ' rm) 
29 
R-S 
2 
s(x), n(x), ur<x,,. .. ' xn) 
42 
s 
2 
X..:. y 
46 
{a 1, â¢â¢â¢ ,an} 
2 
a(x) 
47 
(a 1, â¢â¢â¢ ,an) 
2 
min,,Y P(t,x 1, â¢â¢â¢ ,xn) 
55 
X 
2 
lxjyJ, R(x,y) 
56 
sn 
3 
Pn 
56 
f(a)L f(a)j 
3 
miny P(x 1, â¢â¢â¢ , xn, y) 
57 
lui 
4 
(x,y) 
59 
A* 
4 
/(z), r(z) 
60 
uv 
4 
[a,, ... ' an] 
60 
uln] 
4 
(x); 
61 
- ,&, v 
5 
Lt(x) 
61 
{a E SIP(a)} 
6 
#(.9J) 
65,201 
= 
6 
HALT(X,Y) 
68 
(3t) :5 Y' (Vt) :5 y 
7 
<t><n>(x,, ... ' Xn, y) 
70 
595 

596 
Notation Index 
vn 
70 
Ll Â·L2 
253 
<l>}nl(xl, ... ' xn) 
73 
L* 
253 
<1>/x) 
73 
0,0, u, Â·,* 
256 
STP<nl(x1, â¢â¢â¢ , xn, y, t) 
74 
(y) 
257 
w, 
81 
=L 
263 
K 
82 
X--+ h 
269 
s::,(up ... ' un, y) 
85,205 
u=u 
270 
r 
~m 
91 
ker(f) 
271 
- m 
93 
Y' 
274 
Rr 
95 
<Y> 
274 
w 
119 
Er/L), Er/f) 
294 
3';, 
121 
PARn(A) 
301 
y 
129 
r, 
303 
I-
159 
I-_, 
311 
g-+h 
169 
LiL) 
331 
â¢ 
u = u, u = u 
170 
-,,I\, v, :J' +-+ 
347 
n 
n 
k(L) 
171 
y'' 
348 
!1(L) 
176 
YJÂ·Â·Â·Â·â¢Yn F Y 
352,382 
8(L) 
177 
D 
360 
L(f) 
186,270 
+ 
-
0 
a\, aA , aA 
361 
1/l},.~J(rl, Â· Â· Â·, rm) 
198 
POSA(a), NEGA(a) 
361 
STP~nl(xp ... , xn, y, t) 
201 
RESA(a) 
367 
{u}(i) 
203 
8(t) 
375 
u-<G 
204 
V, 3 
375 
~~' ~] 
207 
A= A(bl, ... ,bn) 
378 
-Q 
208 
a 1[dpÂ·Â·Â·â¢dn] 
378 
co-W 
208 
fFy 
382 
AfBB 
210 
Ys 
384 
we 
n 
213 
Tr 
407 
G' 
213 
I-T y 
407 
c<n) 
214 
w# 
411 
kn, nn, dn 
215 
M;(x) 
420 
TOT 
224 
a.e. 
421 
rl 
231 
00 
421 
L(L) 
239, 243, 311 
O(g(n)) 
439 

Notation Index 
597 
p 
444 
F(XH ... ,Xn) = t 
507 
SAT 
446 
PF(E), AF(E) 
508 
NP 
446 
rhs(E) 
508 
:s;p 
448 
Bool, tt, ff 
508 
V{xe II :::;; e :::;; /} 
453 
ifT, is_f, f;- 1 
509 
.l_N 
471 
(We U B(W), Tc U T B(W) 
509 
!;;;;; 
472 
(WN, TN), (WNLâ¢ TNL) 
510 
!YJ(D) 
472 
( D.'T( T )' !;;;;; .'T( T )), ..L .'T( T) 
511 
!;;;D,x Â·Â·Â· xD., 
472 
(Boo), !;;;Bool ), ..Lsool 
512 
(D ~ E, !;;;D-+ E) 
473 
(DT, !;;;;; T), ..L T 
512 
(ch(g), !;;;ch(-'fil) 
474 
(DT X Â· Â· Â· X T ' i;;;T X Â· Â· Â· X T ) 
512 
I 
n 
I 
n 
uE, nE 
476 
D8(F) 
512 
(D .i, !;;;D ) 
477 
ran f 
512 
L 
ug> 
478 
tt, ff 
512 
d ~ i 
478 
!_N = (9'""N,JN) 
514 
!T(d) 
479 
a(F), a(X) 
514 
f(g)(d) 
481 
S41~V) 
514 
(id(D), ~ id(D)) 
482 
~S41~V) 
515 
pid(e) 
482 
ajt), a(t) 
515 
f.l 
487 
!;;;;; 1\:W:"(V) 
516 
f(E) 
488 
Og:w:"(V) 
517 
([D ~ E], !;;;[D-+ EJ) 
488 
a(d, ... d.>â¢ ad 
520 
t4 
494 
ci>l 
p 
520 
r<x) 
495 
ADD 
524 
x,~ 
505 
g!J.N 
527 
VARI, VARF, VAR 
506 
(rep(D.'T(T)), !;;;;; rep(.'T(T))) 
530 
IV(O), FV(O), V(O) 
506 
rep( f) 
530 
(W, T) 
506 
rep(!,) = (rep(9'""), rep(J)) 
531 
T(f), S(f), p(f) 
506 
(Dr(T)' !;;;;; r(T) 
531 
ar(f) 
506 
Dr(T 1)X Â·Â·Â·X r(T 0 ) 
531 
TV(W) 
507 
Dr(T 1)X Â·Â·Â· Xr(T 0 )--+r(T) 
531 
TMw(V), TMw(V) 
507 
Dr(8(F)) 
531 
â¢wÂ·â¢w 
507 
rep( a) 
533 
N,NL 
507 
g!J. 
533 
n, sn(o) 
507 
!_NL = (.5JNL, JNL) 
539 
0, s, nil, cons 
507 
aNL 
540 

598 
Notation Index 
LIST 
540 
Wa 
554 
ay= (~,...Yy) 
542 
od1d2 00 
0 
554 
!_K(W) = (~(W)> ~(W)) 
542 
te 
558 
aK(W) 
543 
u~v 
559 
w+ 
545 
t=w 
559 
(TM~+' !;;;T+), .iT 
545 
T,u 
560 
Uti> Upi> Uto> Upo 
!.Kx(W) = (~x(W)> ~x(W)) 
546 
Ty(P) 
561 
(DTx' !;;;Tx ), .l Tx 
546 
ur 
564 
!.~ 
546 
rp,rrp 
565,585 
nj_,w 
547 
&'!J. 
566,585 
A'" N 
550 
f./xl, 0 
0 0' xn) 
579 
!_~L' a'"NL 
551 
# 2o(w), #(t), #(P), #(e) 
580 
(d1,ooo,dn)l_ 
552 
T;(P) 
584 

Index 
A 
Algorithms, 3, 68, 79-80, 95, 411 
Almost everywhere, 421 
Alphabets, 4, 113, 347, 375, 505 
of linear bounded automata, 330 
of pushdown automata, 310 
of Turing machines, 146 
Approximation orderings, 470-471, 487, 505, 
see also Complete partial orders; Partial 
orders 
Arithmetic hierarchy, 215-230 
Arithmetic predicates, 223 
Assignments 
complete type, 512, 513, 516, 539 
continuous variable, 514-515, 539 
Herbrand ideal type, 546 
on propositional atoms, 348, 406, 455-456 
type, 511-512 
variable, 514-515 
Atoms, 347 
Automata theory, 237 
Axiomatizable theories, 407-410, 415 
complete, 410 
consistent, 410 
nonrecursive, 415 
w-consistent, 410 
Axiom of choice, 474 
8 
Balanced words, 309-310 
Bar-Hillel's pumping lemma, 287-290, 299 
Base n notation, 116, 328, 580 
Binary relations, 472 
restrictions of, 4 72 
Blum axioms, 419-425 
Blum, Manuel, 419 
Blum speedup theorem, 437-438 
Boolean algebra, 350 
Bottom elements, 477 
Bracket languages, 301-308 
Cantor, Georg, 406 
Cartesian product, 3 
Case, John, 103 
Chains, 477, 499 
Chang, C. L., 593 
c 
Chomsky hierarchy, 327-330 
Chomsky normal form grammars, 285-287, 
287-290, 297, 303, 306, 308, 313, 316, 
319 
separators of, 303-308, 313, 319 
Chomsky-Schiitzenberger representation 
theorem, 306 
599 

600 
Church, Alonzo, 411 
Church's thesis, 69, 80, 90, 95, 96, 141, 161, 
162, 197, 407, 445, 447 
Clauses, 355-356 
linked sets of, 366-367, 396, 398, 400 
minimally unsatisfiable sets of, 366-367, 
392, 395 
unit, 361, 362, 363 
Closure properties, 4 
for context-free languages, 291-297 
for context-sensitive languages, 329, 
337-344 
for regular languages, 249-256 
CNF, see Conjunctive normal form 
COBOL, 323 
Coding 
of finite functions, 203-204, 580 
of pairs, 59-60, see also Pairing functions 
of programs, 65-67 
of sequences, 60-63, see also Godel num-
bers 
of states of programs, 71 
of strings, 113-115 
of WN-programs, 579-582 
of WN-substitutions, 579-582 
of WN-terms, 579-582 
COF, 227 
Coincidence lemma, 518, 527 
COMP, 48 
Compact elements of a cpo, 486 
Compactness theorem 
for predicate logic, 390, 404-406 
for propositional calculus, 370-373, 390, 
404 
Compilers, 323-326 
Complete partial orders, 475-486, 516, see 
also Partial orders; Orderings 
algebraic, 486, 494 
compact elements of, 486 
of continuous variable assignments, 
516-517 
flat, 477, 539 
Complexity 
abstract, 419-438 
measures, 419 
Composition, 39-40, 42, 76, 77, 108, 200, 576 
Computability theory, 17, 18, 31, 70, 98, 113, 
169, 237, 443 
Computable functions, see Functions, com-
putable 
Index 
Computable real numbers, 554, 587, 589-590 
Computations 
infinite, 469-470, 563 
by linear bounded automata, 339-343 
by nondeterministic Turing machines, 160, 
171, 447-448, 451-456 
by pushdown automata, 311-323 
by Y' programs, 27, 28, 29, 70-75, 420, 
468, 557 
for W-terms, 559-575, 584-587, 588 
Configurations 
of linear bounded automata, 330, 331 
of Post-Turing programs, 130 
of pushdown automata, 311 
of Turing machines, 147, 159-160, 444, 
452-456 
Conjunctive normal form, 356-360, 364, 368, 
392, 393, 444, 446, 447-448, 451-456, 
457-458, 461 
co-NP, 450 
Constant symbols, 375, 506 
Context-free grammars, 269-280, 326, 327, 
497-499, see also Chomsky normal form 
grammars 
ambiguity of, 300, 303, 307, 319 
branching, 277, 279, 285 
Chomsky normal form, see Chomsky nor-
mal form grammars 
derivations in, 270, 275, 277 
derivation trees in, see Derivation trees 
Greibach normal form, 287 
kernels of, 271 
languages generated by, 270 
left-linear, 285 
leftmost derivations in, 275-277 
positive, 269, 271, 273, 277, 327 
productions of, 269 
regular, 280-285, 327 
right-linear, 283, 304 
rightmost derivations in, 275-277 
self-embedding, 284 
unsolvable problems involving, 297-301 
Context-free languages, 269-326, 327-328, 
337, 450, see also Context-free gram-
mars 
closure properties for, 291-297 
and compilers, 323-326 
deterministic, 323 
infinite, 298-299 
inherently ambiguous, 301 

Index 
nonregular, 270 
and pushdown automata, 308-323 
regular, 280-285 
Context-sensitive grammars, 189-190, 
327-330 
unsolvable problems involving, 339 
Context-sensitive languages, 327-344 
closure properties for, 329, 337-344 
open questions concerning, 343 
Cook-Karp thesis, 445 
Cook's theorem, 451-456 
Corollaries, 8 
Correctness 
of operational semantics, 557 
of programs, 536 
Course-of-values recursion, 62-63 
cpo, 477, see also Complete partial orders 
flat, 477 
D 
Data structure systems, 531, 536 
infinitary, 544-556, 584-592 
simple, 539-544, 557-575 
Davis, Martin, 593 
Davis-Putnam rules, 360-366,368, 391, 393, 
396, 444 
Definition by cases, 50-51 
Degree 
of function symbols, 375 
of polynomials, 441 
of relation symbols, 375 
De Morgan identities, 2, 6, 7, 50, 251, 291, 
350, 354, 371, 380 
Derivation trees, 274-279, 298, 498 
pruning and splicing of, 279, 289 
dfa, 243, see also Finite automata 
minimal, 266 
Diagonalization, 88-90, 94, 106, 328, 406, 
429 
Diagonal lemma, 492, 502, 518 
Disjunctive normal form, 356-360, 444 
DNF, see Disjunctive normal form 
Dovetailing, 80, 81, 408 
DTIME, 450 
Duality, general principle of, 350, 356, 358, 
380 
Dycklanguages, 303,306 
E 
Empty program, 26 
Enderton, Herbert P., 593 
Enumeration principle, 370, 371, 405 
Enumeration theorem, 81 
Equivalence relations, 263 
Euclid, 57 
Exchange lemma, 490, 491, 503 
EXPTIME, 450, 451 
601 
Extension lemma, 526-527, 532, 534, 573 
F 
Fibonacci numbers, 62, 529 
Finite automata, 237-242 
deterministic, 243, 280, 292 
nondeterministic, 242-249, 281, 324 
nonrestarting, 250, 316 
Finiteness theorem, 204 
Finite satisfiability, 370-372 
First-order logic, see Quantification theory 
Fixed point induction principle, 499, see also 
Induction, fixed point 
Fixed points, 494-503 
as solutions to W-programs, 523 
Fixed point theorem, 101, 501, see also Re-
cursion theorem 
for cpos, 495-496, 497, 499, 501, 524 
Floyd, R. W., 181 
Formulas 
atomic, 376 
ground, 392 
Herbrand instances of, 412-414 
in predicate logic, 376 
propositional, 347, 496-500 
quasi-representing sets, 409 
representing functions, 410 
representing sets, 408, 409 
semantically equivalent, 380 
FORTRAN, 279, 323, 324 
Functions, 3 
binary, 3 
Boolean-valued, 5 
characteristic, 6, 78, see also Predicates 
computable, 28-32, 41, 42, 77, 112, 116, 
121,469,567 
computed by Post-Turing programs, 134, 
135, 140, 141, 147, 149, 150 
computed by .Y programs, 19, 22-25, 30 
computed strictly, 134, 135, 139, 146, 148 

602 
computed by Turing machines, 146-152 
constructible, 428 
continuous, 486-494, 495 
~-computable, 574, 575-584 
domain of, 3 
elementary, 59 
exponential, 425, 442, 444, 450 
G-computable, 198 
G-partial recursive, 198 
G-recursive, 198 
higher order, 470, 481, 520 
intuitively computable, see Church's thesis 
monotonic, 487-494 
n-ary, 3, 200 
nowhere defined, 3, 31 
one-one,4,84 
onto, 4 
partial, 3, 24 
partially computable, 30, 39, 70, 73, 75, 76, 
83, 96, 116, 127, 140, 148, 198, 199,467, 
501,578-582 
partially computable in .9;,, 121, 127, 135, 
140, 141 
partially G-computable, 198 
partial recursive, 30, 84 
polynomial time computable, 445 
primitive recursive, 39-63, 84, 105-112, 
117, 188, 262, 576-578 
range of, 3, 82, 83, 84 
recursive, 30, 84, 426, 428, 436, 437 
strict, 487, 539, 564 
strict extensions of, 487 
on strings, 116-117 
total, 3, 5, 30, 40, 42, 90, 198, 199, 200, 201, 
202, 204, 205 
unary, 3 
Function symbols, 375, 506 
arity of, 506 
built-in, 509 
constructor, 509 
degree of, 375 
discriminator, 509 
proper, 506 
selector, 509 
r-trees, 273-274, 278 
paths in, 278 
G 
Gap theorem, 425-428 
Garey, Michael R., 593 
Index 
Godel, Kurt, 60 
Godel numbers, 60-63, 66, 67, 71, 74-75, 79, 
84,108,200,204,217-218,222,232,435, 
580 
Godel's incompleteness theorem, 407-410 
Goldbach's conjecture, 69, 70 
Grammars, 186-191, see also Context-free 
grammars; Context-sensitive grammars; 
Regular grammars 
derivations in, 188, 270 
languages generated by, 186, 270,327-329 
left-linear, 285 
null productions of, 269, 327, 344 
phrase structure, 186, 327 
positive context-free, 269, 327 
productions of, 186, 269 
regular, 280-285, 327 
right-linear, 283, 304 
self-embedding, 284 
start symbol of, 186, 269 
terminals of, 186, 269 
unsolvable problems concerning, 191-192, 
300-301, 322, 339 
variables of, 186, 269 
Graphs, 458-460 
cliques in, 459 
complete, 458-459 
Greatest lower bounds, 476 
Greibach normal form grammars, 287 
G-r.e. sets, 211-215 
Ground clauses, 392 
Ground resolution, 369, see also Resolution, 
in propositional calculus 
Ground resolution theorem, 369 
Gunter, Carl A., 593 
H 
Halmos, Paul R., 593 
Halting problem, 68-70, 78, 82, 89, 97-98, 
99, 197, 467, 488, 523, 554, see also Un-
solvability of, halting problem 
for Post-Turing programs, 144 
for Turing machines, 157-158 
Harrison, Michael, 594 
Herbrand instances of formulas, 412-414 
Herbrand's theorem, 388-398, 404 
Herbrand universes, 388, 389, 390, 392, 393, 
395,398,405,406,412,542 
Hilbert, David, 410, 411 
Homomorphisms, 253, 296, 344 

Index 
Horn clauses, 403 
Horn programs, 404 
Hrbacek, Karel, 594 
Ideals, 482-483, 546-556 
principal, 482, 552 
Immerman, Neil, 339 
Index sets, 95, 103, 230-231 
Induction, 9-13 
complete, 11-12 
course-of-values, 11-12, 302 
fixed point, 499-500, 536-537 
hypothesis, 10 
loading, 11, 109 
mathematical, 9-13, 52, 86, 206, 215, 245, 
256, 264, 271, 293, 304, 305, 433, 495, 
540, 571, 576 
structural, 500-501, 516, 517, 518, 534, 
535, 558, 567, 571 
INF, 225 
Initial functions, 42, 76, 108 
Instantaneous descriptions, 27, see also 
Snapshots of a program 
Instructions, 17-18, 26, 65-67, 197 
conditional branch, 18, 35 
decrement, 18 
increment, 18 
labeled, 26 
numbers of, 65-67, 74, 201 
oracle, 197-198, 201 
of Post-Turing programs, 129 
of~ programs, 122 
unlabeled, 26 
while, 103 
Interpretations 
continuous, 513, 539 
Herbrand ideal, 546 
for predicate logic vocabularies, 377, 382, 
411 
for typed vocabularies, 512-513, 539, 546 
Interpreters, 70 
Intractability, 444, 457, 459 
Isomorphisms, 485 
Iteration theorem, see Parameter theorem 
J 
Jech, Thomas, 594 
Johnson, David S., 593 
Jumps of sets, 213 
K 
Kleene's hierarchy theorem, 216 
Kleene's theorem, 253-260 
Kuroda, S. Y., 331, 339 
Labels, 18, 25 
Landweber, P. S., 331 
L 
Languages, 4, see also Vocabularies 
accepted by deterministic Turing ma-
chines, 189 
accepted by finite automata, 239 
accepted by linear bounded automata, 
330-337 
603 
accepted by nondeterministic finite au-
tomata, 243 
accepted by nondeterministic Turing ma-
chines, 160-161, 186, 189 
accepted by pushdown automata, 311-323 
accepted by Turing machines, 153-157, 189 
context-free, see Context-free languages 
context-sensitive, see Context-sensitive 
languages 
generated by grammars, 186, 189, 270, 
327-328 
inherently ambiguous, 301 
in NP, see NP 
NP-complete, see NP-completeness 
NP-hard, 449 
polynomial-time decidable, see P 
r.e., see r.e. languages 
recursive, 156, 190, 328, 446 
recursively enumerable, see r.e. languages 
regular, see Regular languages 
spanning sets for, 263 
Lattices, 485, 502 
Lazy functional languages, 553 
Least upper bounds, 476 
Lee, R. C. T., 593 
Lemmas, 8 
Lewis, Harry R., 594 
Lexical analysis, 323 
L'Hospital's rule, 442 
Linear bounded automata, 330-343 
deterministic, 336, 337, 343 
Lin ears orders, 4 72, see also Partial orders 
Linked conjunct procedures, 400 
LISP, 507 

604 
Lists 
finite, 541, 552 
infinite, 553, 588-590 
prefix, 553 
Literals, 354, 447-448 
Loeckx,Jacques, 594 
Logic, see Propositional calculus; Quantifi-
cation theory 
Logical consequence, 382-388 
Logic programming languages, 403 
Loveland, Donald W., 594 
M 
Machtey, Michael, 427, 594 
Macro expansions, 20-24, 33, 35, 121-123 
Macros, 20, 32-37, 122-123, 127 
for Post-Turing programs, 136, 137 
Many-one reducibility, 90-95, 207-211 
Markov, A. A., 178 
Mates, 355, 357-358, 366-367, 397-398, 399, 
see also Linked conjunct procedures 
Mathematical induction, see Induction, 
mathematical 
m-completeness, 92, 93, 97, 210, 217 
Meaning functions 
denotational, 527, 533, 536, 557, 573-574, 
586 
operational, 566, 573-574, 585-586 
Metavariables, 506 
Minimalization, 55-59, 75, 76, 77, 200, 578 
bounded, 55-58 
proper, 77 
unbounded, 57-58 
Minimal unsatisfiability, 366-367, 392, 395, 
see also Satisfiability, truth functional 
Models, 380, 382, 384, 385, 406, 412 
countable, 406 
Myhill, John, 232, 263 
Myhill-Nerode theorem, 263-264 
N 
Natural numbers, 1 
ndfa, 243, see also Finite automata, nonde-
terministic 
Nerode, Ani!, 263 
Normal form theorem, 75 
Normal processes, 192-195 
Normal productions, 193 
NP, 446-451, 456, 457-463 
Index 
NP-completeness, 208,448, 449,451,457-463 
NP-complete problems 
CHROMATIC-NUMBER, 461,462 
COMPLETE-SUBGRAPH, 458-459, 462 
EXACT-COVER, 462 
HALF-SAT, 456 
HAMILTONIAN-CIRCUIT, 460, 462 
INTEGER-PROGRAMMING, 460 
KNAPSACK, 463 
LONGEST-COMMON-SUBSE-
QUENCE, 462 
MAX-CLIQUE, 459 
MULTIPROCESSOR-SCHEDULING, 
463 
PARTITION, 460, 462, 463 
QUADRATIC-DIOPHANTINE-EQUA-
TIONS, 461 
RECORD-ALLOCATION, 463 
SAT, 446-456, 458 
SET-COVER, 460 
STRAIGHTLINE-PROGRAM-IN-
EQUIVALENCE, 461 
SUBGRAPH-ISOMORPHISM, 462 
SUBSET-SUM, 462 
TASK-SEQUENCING, 463 
3-DIMENSIONAL-MATCHING, 460 
3-SAT, 457-458, 460, 461, 462 
TRAVELING-VENDOR, 462 
2-COLORABILITY, 461 
2-SAT, 461-462 
VERTEX-COVER, 459-460,462 
NP-hardness, 449, 451, 457 
NPSPACE, 451 
NTIME, 450 
n-tuples, 2 
Numeral systems, 408 
0 
1-completeness, 209, 226 
One-one reducibility, 207-211 
Oracles, 197-200 
Ordered pairs, 2 
Orderings, see also Partial orders 
Cartesian product, 472, 479 
continuous function space, 488 
.91-choice function, 474, 481 
function space, 473, 480 
lexicographic, 481 

Index 
p 
P,444-446, 448,449,456,461 
Pairing functions, 59-60, 65, 66, 67, 74-75, 
83, 106, 107, 110, 203,228,232,435 
Pairing function theorem, 60 
Palindromes, 241, 265 
Papadimitriou, Christos H., 594 
Parameter theorem, 85-88, 92, 93, 96, 99, 
209, 225, 226, 229, 231, 428, 435 
relativized, 205-206, 209 
Parsing, 324-326 
Partially computable functions, see Func-
tions, partially computable 
Partial orders, 472-475, see also Complete 
partial orders 
bottom elements of, 471, 477 
bounded-complete, 485, 494 
chains in, 477, 499 
complete, see Complete partial orders 
ideal completions of, 483 
ideals of, 482-483, 546-556 
isomorphic, 485, 486, 493, 544 
principal ideals of, 482 
Partial recursive functions, see Functions, 
partial recursive 
Pascal, 279, 323, 326, 438 
Pigeon-hole principle, 260, 264, 266, 451 
n., 215-230 
as a collection of sets, 215 
as a property of predicates, 217 
Polynomials, 441, 442 
Polynomial-time computability, 445 
Polynomial-time decidability, 444 
Polynomial-time reducibility, 208, 448-449, 
457 
Post correspondence problem, 181-186, 
299-301, 322 
Post correspondence system, 181, 300 
Post, Emil L., 129, 178, 181 
Post's lemma, 177 
Post's theorem, 217-224, 228 
Post-Turing programs, 129-144, 145, 149, 
150, 161, 200 
Post words, 171-175, 177, 187 
Power sets, 472 
PRC classes, 42-44, 49-56, 62, 63, 79, 84, 
199 
Predecessor function, 46 
Predicate logic, 375-415 
605 
Predicates, 5-7, 10, 34-35, 68, 217-224, 419 
admissible, 499, 502, 537 
arithmetic, 223-224 
computable, 34-35, 50, 68, 85 
primitive recursive, 49-51, 78, 187, 
259-260 
recursive, 419 
Prenex normal form, 384 
Prenex sentences, 384 
Prime numbers, 8, 54, 56-57, 69, 265, 267, 
588 
Primitive recursive functions, see Functions, 
primitive recursive 
Productions, see Context-free grammars, 
productions of; Semi-Thue productions 
null, 269 
Programming systems, 88 
acceptable, 88, 105, 128, 144, 153 
Programs, 17-28, 65-67, 197, see also W-
programs 
computations of, 27 
correct, 536 
as a data structure system, 542 
functions computed by, 19, 22-24, 28-32, 
198 
G-computations of, 198, 201 
instantaneous descriptions of, 27 
length of, 26 
numbers of, 65-67, 68, 70, 74-76, 86, 89, 
90,95-97,97-105,201,224,225 
with oracles, 197-198 
Post-Turing, see Post-Turing programs 
snapshots of, 27-28, 29, 70, 74-76, 
197-198,420 
states of, 26, 197 
straightline, 32 
for string computations, 121-26 
universal, 70 
Program verification, 536 
Projection functions, 42, 478 
Prolog, 403 
Proof by contradiction, 8-9 
Proofs, 8 
Propositional calculus, 347-373 
Propositional connectives, 347 
Propositional variables, 347 
PSPACE, 450, 451 
Pumping lemma, 260-262, 264, 267, see also 
Bar-Hillel's pumping lemma 

606 
Pure literal rule, 362, 363 
Pushdown automata, 30H-323 
atomic, 314,315,316 
computations hy, 311-323 
deterministic, 311,313, 315 
generalized, 321 
languages accepted hy, 311-323 
nondeterministic, 312, 315 
transitions of, 310 
Q 
Quadruples, 145, 330-333, 338, 454-455 
Quantification theory, 375-415 
Quantifiers, 7-H 
alternating, 223 
hounded, 7, 53-55 
in predicate logic, 376 
Quintuples, 149 
R 
Rates of growth, 439-443 
Recursion, 40, 41, 44, 45, 76, 77, 100, 200, 
34H, 496, 501, 576 
course-of-values, 62, I 09 
primitive, 40, 41, 100, 106, 108 
simultaneous, 62 
unnested double, 63 
Recursion equations, 40, 41, 44, 45, 46, 47, 
62, 63, 100, 104, 309, 314, 505, 508, see 
also W-recursion equations 
Recursion theorem, 97-105, Ill, 428,431, 
see also Fixed point theorem 
first, 501, 503 
second, 501 
Recursion theory, 31, see also Computability 
theory 
Recursive functions, see Functions, recursive 
Recursive function theory, see Computability 
theory 
Recursive isomorphisms, 231-234 
Recursive languages, see Languages, recur-
sive 
Recursively enumerable languages, see r.e. 
languages 
Recursively enumerable sets, see r.e. sets 
Recursive operators, 503 
Recursive permutations, 231-234 
Recursive relatedness theorem, 422, 424, 425, 
437 
Recursive sets, see Sets, recursive 
Redexes, 559 
Index 
Reducibilities, 90-95, 96, 207-211, see also 
Many-one reducibility; One-one re-
ducibility; Polynomial-time reducibility 
completeness with respect to, 92-93, 208, 
210 
Regular expressions, 256-260 
Regular grammars, 280-285, 327 
Regular languages, 237-267, 280-285, 292, 
304, 306, 327-32H, 450, see also Regular 
grammars 
closure properties for, 249-260 
examples of, 247-249 
and finite automata, 237-242 
infinite, 261 
and nondeterministic finite automata, 
242-247 
represented by regular expressions, 257 
spanning sets for, 263-267 
r.e. languages, 154-157, 161, 188, 189,327, 
344,371,391-392,407-408 
Relation symbols, 375 
Relativization, 199 
of computability, 198 
of enumeration theorem, 213 
of parameter theorem, 205-206 
of recursive enumerability, 211-215 
of step-counter theorem, 202-204 
of universality theorem, 201-202 
Remainder, 56, 114 
Representable elements, see W-structures, 
representable elements of 
r.e. sets, 78-85,88,90-95, 209-211,231,391, 
407,408,409,592 
relative to an oracle, 211-215 
Resolution 
derivations, 367, 402 
in predicate logic, 400-404 
in propositional calculus, 367-370,393, 
402,444 
refutations, 367, 402 
Resolvents, 367 
Rewrite rules, 169, 559, see also W-term 
rewriting systems 
Rewriting strategies, 559, 560, 564-565 
Rice-Shapiro theorem, 231 
Rice's theorem, 95-97, 103, 230-231 
Right quotients, 266, 297 
Robinson's general resolution theorem, 402 

Index 
Rogers, Hartley, 594 
Rooted sentences, 412-414 
SAT, 446-456, 458 
Satisfiability 
finite, 370 
s 
in predicate logic, 380, 383, 384, 385, 
389-391,394,411 
problem, 358 
truth-functional, 348-352, 352-353, 
355-356,358,364,366-367,372, 
389-391, 405, 444, 446-456 
Scaling factors, 421 
Schmidt, David, 594 
Scott domains, 494 
Self-embedding grammars, 284 
Self-reference, 98, 99 
Self-reproducing program, 100 
Semantics, see also Meaning functions 
denotational, 468, 499, 505-556 
of formulas in predicate logic, 377-382 
operational, 468, 499, 557-592 
of programming languages, 467-468 
of propositional formulas, 348 
of regular expressions, 257 
of .Y programs, 26-28 
ofW-terms, 511-520 
Semi-decision procedures, 80, 392 
Semi-Thue processes, 169-180, 182, 186, 334, 
see also Thue processes 
derivations in, 170 
Semi-Thue productions, 169 
inverses of, 175-176, 177, 335 
Sentences, 377 
models of, 380 
prenex, 384 
rooted, 412, 413, 414 
satisfiable, see Satisfiability, in predicate 
logic 
semantically equivalent, 380 
Skolemized, 384-387, 393, 405 
undecidable, 409 
universal, 385, 388 
valid, 380, 383, 391 
Sentential logic, see Propositional calculus 
Sequence number theorem, 62 
Sets I, 78 
characteristic functions of, 6, 78, 79 
complement of, 2 
computable, 78 
difference of, 2 
empty, I 
equality of, I 
finite, 2 
G-r.e., see G-r.e. sets 
G-recursive, 211-214 
infinite, 2 
intersection of, 2 
r.e., see r.e. sets 
recursive, 78, 80, 88, 91, 210 
recursively isomorphic, 231 
union of, 2 
Shoenfield, Joseph R., 594 
Sieber, Kurt, 594 
l., 215-230 
as a collection of sets, 215 
607 
as a property of predicates, 217 
Simulation, 70-75, 127, 135-144, 163-167, 
171-176, 333-336 
Skolemization, 384-387 
generalized, 386-387, 405 
Skolem-Uiwenheim theorem, 406 
s-m-n theorem, see Parameter theorem 
Snapshots of a program, 27-28, 70, 74-76, 
197, 420 
initial, 29 
successor of, 27 
terminal, 27, 75, 76, 197 
Soare, Robert 1., 594 
Socrates, 393 
Solutions to W-programs, 520-530, 534, 536 
Spanning sets, 263 
Speedup theorem, 428-438 
Splitting rule, 361, 363, 364 
Stacks, 310-311 
Start symbols, 269 
Statements, 25, see also Instructions 
States, 26, 145, 197, 238 
accepting, 238, 310 
dead, 248 
final, 238, 310, 330, 331 
initial, 29, 238, 310, 331 
State transition diagrams, 147, 240 
Step-counter predicates, 74-75, 78, 81, 82, 
83, 224, 225 
relativized, 201-205, 211, 212 
Step-counter theorem, 74 
relativized, 202-204 
Stoy, Joseph E., 594 

608 
Strings, 4, 12, 113-144 
computable functions on, 116, 121 
computations on, 113, 121 
concatenation of, 117-ll8 
length of, 4, 118 
numerical representation of, 113-117 
partially computable functions on, 116, 121 
primitive recursive functions on, 117 
programming languages for, 121-126 
Subsets, I 
proper, 2 
Substitution lemma, 558, 570 
Substitutions 
on alphabets, 252, 296, 344 
answer, 404 
to individual variables, 395, 397-398, 
399-404, see also W-substitutions 
Subsumption, 366 
Symbols, 4, see also Constant symbols; Func-
tion symbols; Relation symbols 
Syntax 
of propositional calculus, 347-348 
of quantification theory, 375-377 
of Y~ programs, 121 
of Y' programs, 25-26 
of W-programs, 505-511 
T 
Tautological consequence, 352, 373 
Tautological inference, 352-353 
Tautologies, 348, 352, 355-356 
Terminals, 186, 269 
Terms, 376, see also W-terms 
Theorems, 8 
Theoretical computer science, 189, 359, see 
also Automata theory; Complexity; 
Computability theory; Languages; Se-
mantics, of programming languages 
Thue, Axel, 169 
Thue processes, 177-181,411 
TOT, 90, 224-229 
Tractability, 444 
Transition functions, 238 
Truth values, 5 
Turing, Alan, 129, 153, 411 
analysis of computation process, 129, 145 
Turing machines, 145-168, 176, 191, 197, 
415, 444, 445, 447, see also Configura-
tions, of Turing machines; Linear 
bounded automata 
Index 
deterministic, 146, 159, 174, 176, 177, 178, 
189, 415 
functions computed by, 146-152 
languages accepted by, 153-157, 186-187, 
189 
multiple tape, 167, 168 
multiple track, 163-168,337-343 
nondeterministic, 146, 159-162, 171-176, 
186, 189, 330, 446-456 
one-way infinite, 162-167 
quintuple, 149, 150 
universal, 152-153 
Turing reducibility, 207-211 
Types, 505-506 
domain, 506 
function, 506 
individual, 506 
product, 506 
range, 506 
u 
Unconditional branch, 19-20 
Undecidable sentences, 409 
Unification, 399-404, 560 
algorithm, 399 
Unifiers, 403 
Unit clauses, 361, 362, 363 
Unit rule, 362 
Universal computers, 153 
Universality theorem, 70, 209 
relativized, 201-202 
Universal programs, 70 
Universal sentences, 385 
Unsolvability of 
halting problem, 68, 82, 99, 157-158, 197, 
420 
Post's correspondence problem, 181-186 
problems involving grammars, 191-192, 
297-301, 339 
problems involving programs, see Rice's 
theorem 
satisfiability problem in predicate logic, 
392, 410-415 
word problems, 176-180, 195, 411 
v 
Validity, see Sentences, valid 
Variables, 17, 25, 65 
auxilliary function, 508 

Index 
bound, 376-377 
free, 376-377 
function, 506 
in grammars, 186, 269 
individual, 506 
initialization of, 18, 121 
input, 17, 25 
local, 17, 25 
occurrences of, 376-377 
output, 17, 25 
in predicate logic, 375-376 
princip<jJ function, 508 
propositional, 347 
of Y, 17,25 
type, 505 
Vocabularies, 375, 506 
interpretations of, 377 
standard, 509, 511, 543, 550 
standard constructor, 509, 511 
typed, 506 
w 
W-formulas, 376, see also Formulas 
Word problems, 176-180,411 
of normal processes, 193- 195 
Words, 4, see also Strings 
length of, 4 
W-programs, 508, 580 
denotational semantics for, 530-538 
operational semantics for, 557-592 
partially correct, 536 
solutions to, 520-530, 534, 536 
totally correct, 536 
W-recursion equations, 507-508 
W-sentences, 377, see also Sentences 
W-structures, 513 
complete, 513, 539 
computable elements of, 587 
continuous, 513, 539 
data structure systems based on, 531 
Herbrand ideal, 546 
609 
infinitary, 544-545, 548, 587 
representable elements of, 530-531, 534, 
539, 551, 554, 587 
simple, 539 
simple Herbrand, 542 
W-substitutions, 558, 580 
W-term rewriting systems, 559 
for infinitary data structure systems, 
584-585 
for simple data structure systems, 561-562 
strategies for, 559, 560, 564-565 
W-terms, 376, 507, 580 
ground, 507 
normal, 559, 565 
semantics of, 511-520 
y 
Young, Paul, 427, 594 


Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
Wave Theory of Information
Understand the relationship between information theory and the physics of wave
propagation with this expert guide. Balancing fundamental theory with engineering
applications, it describes the mechanism and limits for the representation and
communication of information using electromagnetic waves. Information-theoretic
laws relating functional approximation and quantum uncertainty principles to entropy,
capacity, mutual information, rate–distortion, and degrees of freedom of bandlimited
radiation are derived and explained. Both stochastic and deterministic approaches
are explored, and applications for remote sensing and signal reconstruction, wireless
communication, and networks of multiple transmitters and receivers are reviewed.
With end-of-chapter exercises and suggestions for further reading enabling in-depth
understanding of key concepts, it is the ideal resource for researchers and graduate
students in electrical engineering, physics, and applied mathematics looking for a fresh
perspective on information theory.
Massimo Franceschetti is a Professor in the Department of Electrical and Computer
Engineering at the University of California, San Diego, and a Research Afﬁliate of
the California Institute of Telecommunications and Information Technology. He is the
coauthor of Random Networks for Communication (Cambridge, 2008).

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
“This is an excellent textbook that ties together information theory and wave theory in a
very insightful and understandable way. It is of great value and highly recommended for
students, researchers and practitioners. Professor Franceschetti brings a highly valuable
textbook based on many years of teaching and research.”
Charles Elachi, California Institute of Technology and Director Emeritus of the Jet
Propulsion Laboratory (NASA)
“This book is about the physics of information and communication. It could be
considered to be an exposition of Shannon information theory, where information
is transmitted via electromagnetic waves. Surely Shannon would approve of it.”
Sanjoy K. Mitter, Massachusetts Institute of Technology
“Communication and information are inherently physical. Most of the literature,
however, abstracts out the physics, treating them as mathematical or engineering
disciplines. Although abstractions are necessary in the design of systems, much is lost
in understanding the fundamental limits and how these disciplines ﬁt together with
the underlying physics. Franceschetti breaks the disciplinary boundaries, presenting
communication and information as physical phenomena in a coherent, mathematically
sophisticated, and lucid manner.”
Abbas El Gamal, Stanford University

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
Wave Theory of Information
MASSIMO FRANCESCHETTI
University of California, San Diego

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314–321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre, New Delhi – 110025, India
79 Anson Road, #06-04/06, Singapore 079906
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of
education, learning, and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781107022317
DOI: 10.1017/9781108165020
© Cambridge University Press 2018
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2018
Printed in the United Kingdom by TJ International Ltd. Padstow Cornwall
A catalogue record for this publication is available from the British Library
Library of Congress Cataloging-in-Publication Data
Names: Franceschetti, Massimo, author.
Title: Wave theory of information / Massimo Franceschetti, University of California, San Diego.
Description: Cambridge : Cambridge University Press, 2017. |
Includes bibliographical references.
Identiﬁers: LCCN 2017032961 | ISBN 9781107022317 (hardback)
Subjects: LCSH: Information theory. | Electromagnetic waves. |
Wave-motion, Theory of.
Classiﬁcation: LCC Q360.F73 2017 | DDC 003/.54–dc23
LC record available at https://lccn.loc.gov/2017032961
ISBN 978-1-107-02231-7 Hardback
Cambridge University Press has no responsibility for the persistence or accuracy
of URLs for external or third-party internet websites referred to in this publication
and does not guarantee that any content on such websites is, or will remain,
accurate or appropriate.

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
About the Cover
The picture represents the electromagnetic emission from stellar dust pervading our
galaxy, measured by the Planck satellite of the European Space Agency. The colors
represent the intensity, while the texture reﬂects the orientation of the ﬁeld. The
intensity of radiation peaks along the galactic plane, at the center of the image, where
the ﬁeld is aligned along almost parallel lines following the spiral structure of the Milky
Way. Cloud formations are visible immediately above and below the plane, where the
ﬁeld’s structure becomes less regular. The emission carries information regarding the
evolution of our galaxy, as the turbulent structure of the ﬁeld is related to the processes
taking place when stars are born.
Image copyright: European Space Agency and the Planck Collaboration.
Acknowledgment: M.-A. Miville-Deschênes, CNRS, Institut d’Astrophysique Spatiale,
Université Paris-XI, Orsay, France.

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
To my wife Isabella,
opera in my head.

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
Contents
Preface
page xvii
Notation
xx
1
Introduction
1
1.1
The Physics of Information
1
1.1.1
Shannon’s Laws
1
1.1.2
Concentration Behaviors
2
1.1.3
Applications
4
1.2
The Dimensionality of the Space
5
1.2.1
Bandlimitation Filtering
5
1.2.2
The Number of Degrees of Freedom
7
1.2.3
Space–Time Fields
9
1.2.4
Super-resolution
11
1.3
Deterministic Information Measures
13
1.3.1
Kolmogorov Entropy
14
1.3.2
Kolmogorov Capacity
14
1.3.3
Quantized Unit of Information
17
1.4
Probabilistic Information Measures
18
1.4.1
Statistical Entropy
19
1.4.2
Differential Entropy
21
1.4.3
Typical Waveforms
22
1.4.4
Quantized Typical Waveforms
23
1.4.5
Mutual Information
25
1.4.6
Shannon Capacity
26
1.4.7
Gaussian Noise
29
1.4.8
Capacity with Gaussian Noise
30
1.5
Energy Limits
33
1.5.1
The Low-Energy Regime
33
1.5.2
The High-Energy Regime
34
1.5.3
Quantized Radiation
35
1.5.4
Universal Limits
37
1.6
Tour d’Horizon
41

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
x
Contents
1.7
Summary and Further Reading
43
1.8
Test Your Understanding
44
2
Signals
48
2.1
Representations
48
2.2
Information Content
49
2.2.1
Bandlimited Signals
50
2.2.2
Timelimited Signals
51
2.2.3
Impossibility of Time–Frequency Limiting
52
2.2.4
Shannon’s Program
53
2.3
Heisenberg’s Uncertainty Principle
55
2.3.1
The Uncertainty Principle for Signals
55
2.3.2
The Uncertainty Principle in Quantum Mechanics
56
2.3.3
Entropic Uncertainty Principle
58
2.3.4
Uncertainty Principle Over Arbitrary Measurable Sets
58
2.3.5
Converse to the Uncertainty Principle
59
2.4
The Folk Theorem
60
2.4.1
Problems with the Folk Theorem
61
2.5
Slepian’s Concentration Problem
63
2.5.1
A “Lucky Accident”
65
2.5.2
Most Concentrated Functions
66
2.5.3
Geometric View of Concentration
68
2.6
Spheroidal Wave Functions
70
2.6.1
The Wave Equation
71
2.6.2
The Helmholtz Equation
72
2.7
Series Representations
75
2.7.1
Prolate Spheroidal Orthogonal Representation
76
2.7.2
Other Orthogonal Representations
78
2.7.3
Minimum Energy Error
79
2.8
Summary and Further Reading
81
2.9
Test Your Understanding
83
3
Functional Approximation
87
3.1
Signals and Functional Spaces
87
3.2
Kolmogorov N-Width
88
3.3
Degrees of Freedom of Bandlimited Signals
90
3.3.1
Computation of the N-Widths
91
3.4
Hilbert–Schmidt Integral Operators
95
3.4.1
Timelimiting and Bandlimiting Operators
98
3.4.2
Hilbert–Schmidt Decomposition
100
3.4.3
Singular Value Decomposition
102
3.5
Extensions
104
3.5.1
Approximately Bandlimited Signals
104
3.5.2
Multi-band Signals
105

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
Contents
xi
3.5.3
Signals of Multiple Variables
108
3.5.4
Hybrid Scaling Regimes
111
3.6
Blind Sensing
113
3.6.1
Robustness of Blind Sensing
115
3.6.2
Fractal Dimension
116
3.7
Compressed Sensing
118
3.7.1
Robustness of Compressed Sensing
119
3.7.2
Probabilistic Reconstruction
121
3.7.3
Information Dimension
122
3.8
Summary and Further Reading
123
3.9
Test Your Understanding
124
4
Electromagnetic Propagation
130
4.1
Maxwell’s Equations
130
4.2
Propagation Media
132
4.2.1
Perfectly Conductive Media
134
4.2.2
Dielectric Media
137
4.3
Conservation of Power
138
4.4
Plane Wave Propagation
139
4.4.1
Lossless Case
140
4.4.2
Lossy Case
141
4.4.3
Boundary Effects
142
4.4.4
Evanescent Waves
143
4.5
The Wave Equation for the Potentials
144
4.6
Radiation
146
4.6.1
The Far-Field Region
149
4.6.2
The Fraunhofer Region
151
4.7
Equivalence and Uniqueness
152
4.8
Summary and Further Reading
153
4.9
Test Your Understanding
154
5
Deterministic Representations
157
5.1
The Spectral Domains
157
5.1.1
Four Field Representations
157
5.1.2
The Space–Frequency Spectral Domain
158
5.2
System Representations
159
5.2.1
Linear, Time-Invariant Systems
159
5.2.2
Linear, Time-Invariant, Homogeneous Media
160
5.2.3
Green’s Function in Free Space for the Potential
160
5.2.4
Green’s Function in Free Space for the Field
161
5.2.5
Green’s Function for Cylindrical Propagation
162
5.3
Discrete Radiating Elements
164
5.3.1
Single Transmitter–Receiver Pair
164
5.3.2
Multiple Transmitters and Receivers
166

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
xii
Contents
5.3.3
Singular Value Decomposition
166
5.4
Communication Systems: Arbitrary Radiating Elements
167
5.4.1
Hilbert–Schmidt Decomposition
168
5.4.2
Optimal Communication Architecture
170
5.5
Summary and Further Reading
171
5.6
Test Your Understanding
172
6
Stochastic Representations
173
6.1
Stochastic Models
173
6.2
Green’s Function for a Random Environment
174
6.2.1
Linear, Time-Varying Systems
174
6.2.2
Linear, Space–Time-Varying Systems
176
6.3
Multi-path
176
6.3.1
Frequency-Varying Green’s Function: Coherence
Bandwidth
179
6.3.2
Time-Varying Green’s Function: Coherence Time
181
6.3.3
Mutual Coherence Function
183
6.3.4
Spatially Varying Green’s Function: Coherence Distance
186
6.4
Karhunen–Loève Representation
186
6.4.1
Time-Varying Green’s Function
187
6.4.2
Optimality of the Karhunen–Loève Representation
190
6.4.3
Stochastic Diversity
191
6.4.4
Constant Power Spectral Density
193
6.4.5
Frequency-Varying Green’s Function
194
6.4.6
Spatially Varying Green’s Function
195
6.5
Summary and Further Reading
196
6.6
Test Your Understanding
197
7
Communication Technologies
200
7.1
Applications
200
7.2
Propagation Effects
200
7.2.1
Multiplexing
203
7.2.2
Diversity
204
7.3
Overview of Current Technologies
205
7.3.1
OFDM
205
7.3.2
MC-CDMA
205
7.3.3
GSM
206
7.3.4
DS-CDMA
206
7.3.5
MIMO
207
7.4
Principles of Operation
208
7.4.1
Orthogonal Spectrum Division
209
7.4.2
Orthogonal Code Division
212
7.4.3
Exploiting Diversity
216
7.4.4
Orthogonal Spatial Division
217

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
Contents
xiii
7.5
Network Strategies
219
7.5.1
Multi-hop
219
7.5.2
Hierarchical Cooperation
221
7.5.3
Interference Alignment
222
7.5.4
A Layered View
224
7.5.5
Degrees of Freedom
225
7.6
Summary and Further Reading
226
7.7
Test Your Understanding
227
8
The Space–Wavenumber Domain
230
8.1
Spatial Conﬁgurations
230
8.2
Radiation Model
231
8.3
The Field’s Functional Space
232
8.4
Spatial Bandwidth
233
8.4.1
Bandlimitation Error
234
8.4.2
Phase Transition of the Bandlimitation Error
236
8.4.3
Asymptotic Evaluation
238
8.4.4
Critical Bandwidth
240
8.4.5
Size of the Transition Window
243
8.5
Degrees of Freedom
244
8.5.1
Hilbert–Schmidt Decomposition
246
8.5.2
Sampling
248
8.6
Cut-Set Integrals
250
8.6.1
Linear Cut-Set Integral
251
8.6.2
Surface Cut-Set Integral
253
8.6.3
Applications to Canonical Geometries
256
8.7
Backscattering
258
8.8
Summary and Further Reading
261
8.9
Test Your Understanding
261
9
The Time–Frequency Domain
265
9.1
Frequency-Bandlimited Signals
265
9.2
Radiation with Arbitrary Multiple Scattering
266
9.2.1
Two-Dimensional Circular Domains
267
9.2.2
Three-Dimensional Spherical Domains
269
9.2.3
General Rotationally Symmetric Domains
270
9.3
Modulated Signals
272
9.4
Alternative Derivations
273
9.5
Summary and Further Reading
274
9.6
Test Your Understanding
274
10
Multiple Scattering Theory
275
10.1
Radiation with Multiple Scattering
275
10.1.1
The Basic Equation
276

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
xiv
Contents
10.1.2
Multi-path Propagation
277
10.2
Multiple Scattering in Random Media
278
10.2.1
Born Approximation
281
10.2.2
Complete Solutions
281
10.2.3
Cross Sections
283
10.3
Random Walk Theory
284
10.3.1
Radiated Power Density
289
10.3.2
Full Power Density
289
10.3.3
Diffusive Regime
290
10.3.4
Transport Theory
290
10.4
Path Loss Measurements
291
10.5
Pulse Propagation in Random Media
292
10.5.1
Expected Space–Time Power Response
292
10.5.2
Random Walk Interpretation
296
10.5.3
Expected Space–Frequency Power Response
297
10.5.4
Correlation Functions
298
10.6
Power Delay Proﬁle Measurements
299
10.7
Summary and Further Reading
300
10.8
Test Your Understanding
301
11
Noise Processes
303
11.1
Measurement Uncertainty
303
11.1.1
Thermal Noise
303
11.1.2
Shot Noise
304
11.1.3
Quantum Noise
306
11.1.4
Radiation Noise
306
11.2
The Black Body
307
11.2.1
Radiation Law, Classical Derivation
307
11.2.2
Thermal Noise, Classical Derivation
311
11.2.3
Quantum Mechanical Correction
312
11.3
Equilibrium Conﬁgurations
314
11.3.1
Statistical Entropy
316
11.3.2
Thermodynamic Entropy
317
11.3.3
The Second Law of Thermodynamics
318
11.3.4
Probabilistic Interpretation
319
11.3.5
Asymptotic Equipartition Property
320
11.3.6
Entropy and Noise
321
11.4
Relative Entropy
322
11.5
The Microwave Window
323
11.6
Quantum Complementarity
325
11.7
Entropy of a Black Body
326
11.7.1
Total Energy
326
11.7.2
Thermodynamic Entropy
327
11.7.3
The Planck Length
328

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
Contents
xv
11.7.4
Gravitational Limits
329
11.8
Entropy of Arbitrary Systems
329
11.8.1
The Holographic Bound
330
11.8.2
The Universal Entropy Bound
330
11.9
Entropy of Black Holes
331
11.10 Maximum Entropy Distributions
333
11.11 Summary and Further Reading
336
11.12 Test Your Understanding
338
12
Information-Theoretic Quantities
343
12.1
Communication Using Signals
343
12.2
Shannon Capacity
344
12.2.1
Sphere Packing
347
12.2.2
Random Coding
349
12.2.3
Capacity and Mutual Information
350
12.2.4
Limiting Regimes
352
12.2.5
Quantum Constraints
354
12.2.6
Capacity of the Noiseless Photon Channel
355
12.2.7
Colored Gaussian Noise
356
12.2.8
Minimum Energy Transmission
357
12.3
A More Rigorous Formulation
358
12.3.1
Timelimited Signals
359
12.3.2
Bandlimited Signals
360
12.3.3
Reﬁned Noise Models
362
12.4
Shannon Entropy
364
12.4.1
Rate–Distortion Function
364
12.4.2
Rate–Distortion and Mutual Information
366
12.5
Kolmogorov’s Deterministic Quantities
368
12.5.1
ǫ-Coverings, ǫ-Nets, and ǫ-Entropy
369
12.5.2
ǫ-Distinguishable Sets and ǫ-Capacity
370
12.5.3
Relation Between ǫ-Entropy and ǫ-Capacity
370
12.6
Basic Deterministic–Stochastic Model Relations
371
12.6.1
Capacity
371
12.6.2
Rate–Distortion
373
12.7
Information Dimensionality
374
12.7.1
Metric Dimension
374
12.7.2
Functional Dimension and Metric Order
376
12.7.3
Inﬁnite-Dimensional Spaces
377
12.8
Bandlimited Signals
378
12.8.1
Capacity and Packing
378
12.8.2
Entropy and Covering
378
12.8.3
ǫ-Capacity of Bandlimited Signals
379
12.8.4
(ǫ,δ)-Capacity of Bandlimited Signals
381

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
xvi
Contents
12.8.5
ǫ-Entropy of Bandlimited Signals
382
12.8.6
Comparison with Stochastic Quantities
383
12.9
Spatially Distributed Systems
384
12.9.1
Capacity with Channel State Information
384
12.9.2
Capacity without Channel State Information
387
12.10 Summary and Further Reading
388
12.11 Test Your Understanding
389
13
Universal Entropy Bounds
391
13.1
Bandlimited Radiation
391
13.2
Deterministic Signals
392
13.2.1
Quantization Error
393
13.2.2
Kolmogorov Entropy Bound
394
13.2.3
Saturating the Bound
396
13.3
Stochastic Signals
397
13.3.1
Shannon Rate–Distortion Bound
398
13.3.2
Shannon Entropy Bound
398
13.4
One-Dimensional Radiation
400
13.5
Applications
401
13.5.1
High-Energy Limits
401
13.5.2
Relation to Current Technologies
402
13.6
On Models and Reality
403
13.7
Summary and Further Reading
405
13.8
Test Your Understanding
406
Appendix A Elements of Functional Analysis
407
Appendix B Vector Calculus
422
Appendix C Methods for Asymptotic Evaluation of Integrals
428
Appendix D Stochastic Integration
433
Appendix E Special Functions
434
Appendix F Electromagnetic Spectrum
437
Bibliography
438
Index
447

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
Preface
Claude Elwood Shannon, the giant who ignited the digital revolution, is the father
of information theory and a hero for many engineers and scientists. There are many
excellent textbooks describing the many facets of his work, so why add another one?
The ambitious goal is to provide a completely different perspective. The writing reﬂects
my desire to abhor duplication and to attempt to break through the compartmentalized
walls of several disciplines. Rather than copying a Picasso, I have tried to frame it and
place it in a broader context.
The motivation also came from my experience as a teacher. The Electrical and
Computer Engineering Department of the University of California at San Diego,
in the spotlight of its annual workshop on information theory and applications,
attracting several hundred participants from around the world, may be considered a
holy destination for graduate students in information theory. Many gifted young minds
join our department every year with the ultimate goal of earning a PhD in this venerable
subject. Here, thanks to the work of many esteemed colleagues, they can become experts
in coding and communication theories, point-to-point and network information theories,
and wired and wireless information systems. Over my years of teaching, however,
I have noticed that sometimes students are missing the master plan for how these
topics are tied together and how are they related to the fundamental sciences. Some
questions that may catch them off guard are: How much information can be radiated
by a waveform at the most fundamental level? How is the physical entropy related
to the information-theoretic limits of communication? How does the energy and the
quantum nature of radiation limit information? How is information theory related to
other branches of mathematics besides probability theory, like functional analysis and
approximation theory? On top of these, there is the overarching question, of paramount
importance for the engineer, of how communication technologies are inﬂuenced by
fundamental limits in a practical setting. To ﬁll these gaps, this book focuses on
information theory from the point of view of wave theory, and describes connections
with different branches of physics and mathematics.
David Hilbert, studying functional representations in terms of orthogonal basis sets
in early twentieth-century Germany, contributed to underpinning the mathematical con-
cept of information associated with a waveform. After Shannon’s breakthrough work,
his approach was later followed by the Soviet mathematician Andrey Kolmogorov,
who developed information-theoretic concepts in a purely deterministic setting, and by
Shannon’s colleagues at Bell Laboratories: Henry Landau, Henry Pollack, and David

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
xviii
Preface
Slepian. Their mathematical works are the basis of the wave theory of information
presented in this book. We expand upon them, and place them in the context of
communication with electromagnetic waves.
From the physics perspective, much has been written on the relationship between
thermodynamics and information theory. Parallels between the statistical mechanics
of Boltzmann and Gibbs and Shannon’s deﬁnition of entropy led to many important
advancements in the analysis and design of complex engineering systems. Once again,
repetita iuvant, sed continuata secant. We brieﬂy touch upon these topics, but focus on
the less beaten path, uncovering the relationship between information theory and the
physics of Heisenberg, Maxwell, and Planck. We describe how information physically
propagates using waves, and what the limitations are for this process. What was ﬁrst
addressed in the pioneering works of Dennis Gabor and Giuliano Toraldo di Francia
is revisited here in the rigorous setting of the theory of functional approximation.
Using these tools we provide, for the ﬁrst time in a book, a complete derivation
of the information-theoretic notion of degrees of freedom of a wave starting from
the Maxwell equations, and relate it to the concept of entropy, and to the principles
of quantized radiation and of quantum indeterminacy. We also provide analogous
derivations for stochastic processes and discuss communication technologies from the
point of view of functional representations, which turns out to be very useful to uncover
the core architectural ideas fundamental to communication systems. When these are
viewed in the context of physical limits, one realizes that there still is “plenty of
room at the bottom.” Engineers are far from reaching the limits that nature imposes
on communication: our students have a bright future in front of them!
Now, a word on style and organization. Although the treatment requires a great
deal of mathematics and assumes that the reader has some familiarity with probability
theory, stochastic processes, and real analysis, this is not a mathematics book. From
the outset, I have made the decision to avoid writing a text as a sequence of theorems
and proofs. Instead, I focus on describing the ideas that are behind the results, the
relevant mathematical techniques, and the philosophy behind their arguments. When
not given, rigorous proofs should follow easily once these basic concepts are grasped.
This approach is also reﬂected in the small set of exercises provided at the end of each
chapter. They are designed to complement the text, and to provide a more in-depth
understanding of the material. When given, solutions are often sketchy, emphasize
intuition over rigor, and encourage the reader to ﬁll in the details. Pointers to research
papers for further reading are also provided at the end of each chapter.
A grouping into an introductory sequence of topics (Chapters 1–6), central results
(Chapters 8 and 9), and an in-depth sequence (Chapters 10–13) is the most natural
for using the book to teach a two-quarter graduate course. The demarcation line
between these topics can be somewhat shifted, based on the taste of the instructor.
The book could also be used in a one-semester course with a selection of the in-depth
topics, and limiting the exposition of some of the details of the central results. Within
this organization, Chapter 7 is an intermezzo, focusing on wireless communication
technologies, and on how they exploit information-theoretic representations. Of course,
this can only scratch the surface of a large ﬁeld of study, and the interested reader should

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
Preface
xix
refer to the wide range of literature for a more in-depth account. A tour d’horizon of the
book’s content is provided at the end of the ﬁrst chapter.
The material has been tested over the course of seven years in the annual graduate
course I have taught at the University of California at San Diego. I wish to thank the
many students who attended the course and provided feedback on the lecture notes,
which were early incarnations of this book, especially the students in my research group,
Taehyung Jay Lim and Hamed Omidvar, who read many sections in detail and provided
invaluable comments. I also enjoyed interactions with my colleague Young-Han Kim,
who read parts of the manuscript and provided detailed feedback. The presentation of
blind sensing and compressed sensing in Chapter 3 has been enriched by conversations
with my colleague Rayan Saab. Recurrent visits to the group led by Bernard Fleury at
Aalborg University, Denmark, inﬂuenced the presentation of the material on stochastic
models and their relationship to communication systems presented in Chapters 6 and
10. Sergio Verdú of Princeton University kindly offered some stylistic suggestions
to improve the presentation of the material in Chapters 1 and 12. Many exchanges
with Edward Lee of the University of California at Berkeley and with my colleague
George Papen on the physical meaning of information helped to shape the presentation
in Chapter 13. Interactions with Sanjoy Mitter of the Massachusetts Institute of
Technology also stimulated many of the physical questions addressed in the book.
My editors Phil Meyler and Julie Lancashire at Cambridge University Press were very
patient with my eternal postponement of manuscript delivery, and provided excellent
professional advice throughout.
A ﬁnal “thank you” goes to my family, who patiently accepted, with “minimal”
complaint, my lack of presence, due to the long retreats in my downstairs hideout.
Massimo Franceschetti

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
Notation
Asymptotics
f(x) ∼g(x) as x →x0
⇐⇒limx→x0 f(x)/g(x) = 1
f(x) = o(g(x)) as x →x0
⇐⇒limx→x0 f(x)/g(x) = 0
f(x) = O(g(x)) as x →x0
⇐⇒limx→x0 |f(x)/g(x)| < ∞
Approximations
f(x) ≃g(x)
g(x) is a ﬁnite-degree Taylor polynomial of f(x)
f(x) ≈g(x)
f(x) is approximately equal to g(x) in some numerical sense
f(x) ≫g(x)
f(x) is much greater than g(x) in some numerical sense
f(x) ≪g(x)
f(x) is much smaller than g(x) in some numerical sense
Domains
t ∈R
time
ω ∈R
angular frequency
r ∈R3
spatial
k ∈R3
wavenumber
φ ∈[0,2π]
angular
λ ∈R+
wavelength
w ∈R+
scalar wavenumber
f(t) ↔F(ω)
time–angular frequency Fourier transform pairs
f(r) ↔f(k)
space–wavenumber Fourier transform pairs
f(φ) ↔f(w)
angle–wavenumber Fourier transform pairs

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
Notation
xxi
Signals

angular frequency bandwidth
W
wavenumber bandwidth
sinc (t)
waveform (sint)/t
rect (t/T)
rectangular waveform of support T and unitary amplitude
U(t)
Heaviside’s step function: U(t) = 0 for t < 0, U(t) = 1 for x ≥0
δ(t)
Dirac’s impulse distribution
Complex Numbers
j
imaginary unit
f ∗(·)
conjugate of complex signal f
ℜf(·)
real part of complex signal f
ℑf(·)
imaginary part of complex signal f
M†
conjugate transpose of matrix M
Functional Spaces
L2
square-integrable signals
B
bandlimited signals of spectral support [−,]
TT
timelimited signals of time support [−T/2,T/2]
N0
Nyquist number, N0 = T/π
α2(T)
fraction of a signal’s energy in [−T/2,T/2]
β2()
fraction of a signal’s energy in [−,]
E (ǫT)
the set of ǫT-concentrated, bandlimited signals
∥· ∥
norm
⟨·⟩
inner product
Sn ⊂S
an n-dimensional subspace of the space S
DSn(A )
deviation of the set A from Sn
dn(A ,S )
Kolmogorov n-width of the set A in S
Nǫ(A )
number of degrees of freedom at level ǫ of the set A

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
xxii
Notation
Fields
¯x, ¯y, ¯z
unit vectors along the coordinate axes
f(x,y,z)
a scalar ﬁeld
f(x,y,z)
a vector ﬁeld: fx(x,y,z) ¯x + fy(x,y,z) ¯y + fz(x,y,z) ¯z
∇f
gradient
∇· f
divergence
∇× f
curl
∇2f
scalar Laplacian
∇2f
vector Laplacian
g(r,t)
dyadic space–time Green’s function
G(r,ω)
dyadic space–frequency Green’s function
Physical Constants
ℓp
Planck’s length [m]
¯h
reduced Planck’s constant [J s]
kB
Boltzmann’s constant [J K−1]
ǫ0
permittivity of the vacuum [F m−1]
µ0
permeability of the vacuum [H m−1]
ǫ = ǫrǫ0
permittivity of the medium [F m−1]
µ = µrµ0
permeability of the medium [H m−1]
σ
conductivity of the medium [S m−1]
c = 1/√ǫµ
propagation speed of electromagnetic wave [m s−1]
β = 2π/λ = ω/c
propagation coefﬁcient [m−1]
Probability
A
a set
|A |
cardinality of the set A
Z
a random variable
P(Z ∈A )
probability that realization of random variable Z is in A
E(Z)
expected value of Z
fZ(z)
probability density function of random variable Z
Z(t)
a random process
sZ(t,t′)
autocorrelation of Z(t)
sZ(τ)
autocorrelation of wide-sense stationary process Z(t)
SZ(ω)
power spectral density of wide-sense stationary process Z(t)

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Frontmatter
More Information
www.cambridge.org
© in this web service Cambridge University Press
Notation
xxiii
Entropy and Capacity
HC
Thermodynamic entropy [J K−1]
HB
Boltzmann entropy [J K−1]
HG
Gibbs entropy [J K−1]
H
Shannon entropy [bits]
Hǫ
Kolmogorov ǫ-entropy [bits]
¯Hǫ
Kolmogorov ǫ-entropy per unit time [bits s−1]
RN
Shannon rate distortion function [bits s−1]
C
Shannon capacity [bits s−1]
Cǫ
Kolmogorov ǫ-capacity [bits]
¯Cǫ
Kolmogorov ǫ-capacity per unit time [bits s−1]
Cδ
ǫ
ǫ-delta capacity [bits]
¯Cδ
ǫ
ǫ-delta capacity per unit time [bits s−1]
h(f)
differential entropy of probability density function f
D(p||q)
relative entropy between probability mass functions p and q
I(X;Y)
mutual information between random variables X and Y

1
Introduction
Nihil est in intellectu quod non prius fuerit in sensu.1
1.1
The Physics of Information
This book describes the limits for the communication of information with waves.
How many ideas can we communicate by writing on a sheet of paper? How well
can we hear a concert? How many details can we distinguish in an image? How
much data can we get from our internet connection? These are all questions related
to the transport of information by waves. Our sensing ability to capture the differences
between distinct waveforms dictates the limits of the amount of information that is
delivered by a propagating wave. The problem of quantifying this precisely requires a
mathematical description and a physical understanding of both the propagation and the
communication processes.
We focus on the propagation of electromagnetic waves as described by Maxwell’s
theory of electromagnetism, and on communication as described by Shannon’s theory
of information. Although our treatment is mostly based on classical ﬁeld theory, we also
consider limiting regimes where the classical theory must give way to discrete quantum
formulations. The old question of whether information is physics or mathematics
resounds here. Information is certainly described mathematically, but we argue that
it also has a deﬁnite physical structure. The central theme of this book is that
Shannon’s information-theoretic limits are natural. They are revealed by observing
physical quantities at certain asymptotic scales where ﬁnite dimensionality emerges and
observational uncertainties are averaged out. These limits are also rigorous, and obey
the mathematical rules that govern the model of reality on which the physical theories
are based.
1.1.1
Shannon’s Laws
Originally introduced by Claude Elwood Shannon in 1948, and continuing up to its lat-
est developments in multi-user communication networks, information theory describes
1
Empiricist claim adopted from the Peripatetic school. A principle subscribed to by Aristotle, St. Thomas,
and Locke; opposed by Plato, St. Augustine, and Leibniz.
03
11:58:56, subject to the Cambridge Core

2
Introduction
in the language of mathematics the limits for the communication of information. These
limits are operational, of real engineering signiﬁcance, and independent of the semantic
aspects associated with the communication process. They arise from the following set
of constraints expressing our inability to appreciate the inﬁnitum:
• Finite energy: Communication occurs by a ﬁnite expenditure of energy.
• Finite dimensionality: Communication occurs by selecting among a range of
possible choices, each identiﬁed by a ﬁnite number of attributes.
• Finite resolution: Each attribute can be observed with limited precision.
According to Shannon, reliable communication of information occurs if the probabil-
ity of miscommunication tends to zero in appropriate limiting regimes. In these regimes,
some interesting physical phenomena also occur: the space–time ﬁelds used to convey
information become amenable to a discrete representation, and the ﬁnite dimensionality
of the physical world is revealed. This allows us to view information-theoretic results
as being imposed by the laws of nature. Information theory classically considers time
asymptotics, used to describe point-to-point communication. Spatial asymptotics are
their natural counterparts, used to extend the description to communication between
multiple transmitters and receivers, and to the remote sensing of the world around us.
There is a beautiful duality between the two, and this book attempts to capture it by
providing a uniﬁed treatment of space–time waveforms.
1.1.2
Concentration Behaviors
At the basis of the asymptotic arguments leading to information-theoretic limits is the
notion of concentration.
Consider a space–time waveform f(x,y,z,t) of ﬁnite energy, transmitted for T
seconds. As T →∞, we can deﬁne the effective frequency bandwidth of the waveform
as the effective spectral support in the Fourier-transformed angular frequency domain –
see Figure 1.1. This deﬁnition is made possible by the mathematics at the basis of wave
theory that predict spectral concentration. As the time domain support is stretched, the
signal, when viewed in the frequency domain, can be more and more concentrated inside
the bandwidth. Thanks to this phenomenon, electromagnetic signals can be considered,
for large T, as occupying an essentially ﬁnite bandwidth. Signals of ﬁnite energy and
ﬁnite bandwidth enjoy another important mathematical property. They exhibit a limit
on the amount of variation they can undergo in any given time interval and thus,
when viewed at ﬁnite resolution, on the amount of information they can carry over
time. The same limitation also applies to the spatial domain. As the region where the
signal is observed is stretched by scaling all of its coordinates, spectral concentration
occurs, and this allows the deﬁnition of the effective bandwidth in the wavenumber
domain, that is the Fourier transform of the spatial domain. This limits the number
of spatial conﬁgurations of the waveform, and thus, when viewed at ﬁnite resolution,
03
11:58:56, subject to the Cambridge Core

1.1 The Physics of Information
3
f(x,y,z,t)
t
0
0
T
2
F(x,y,z,ω)
ω
∞
Fig. 1.1
Spectral concentration.
f(t)
t
0
Fig. 1.2
Probabilistic concentration.
the amount of information it can carry over space. This limitation is important in the
context of network information theory, when multiple transmitters and receivers in a
communication system are distributed in space. It is also important in the context of
imaging systems, where it leads to spatial resolution limits of the constructed image.
When considering space and time asymptotics, another kind of concentration
phenomenon also occurs. The precision level at which the signal can be observed
probabilistically concentrates around its typical value. Every physical apparatus
measuring a signal is affected by a measurement error: repeated measurements appear
to ﬂuctuate randomly by a small amount. This is a consequence of the quantized nature
of the world observed at the microscopic scale. Over many repetitions, the uncertainty
with which the signal is observed is typically contained within its standard deviation –
see Figure 1.2. This allows us to view the uncertainty of the observation as concentrated
around its typical value and determines a resolution limit at which the signal can be
observed. Combined with the constraints on the form of the signal due to spectral
concentration mentioned above, it poses an ultimate limit on the amount of information
that can be transported by waves in time and space.
The same concentration behaviors leading to information-theoretic limits are also at
the basis of quantum mechanics and statistical mechanics. Spectral concentration is at
the basis of Heisenberg’s uncertainty principle, stating that physical quantities related by
03
11:58:56, subject to the Cambridge Core

4
Introduction
Fourier transforms cannot be determined simultaneously, as pinpointing one precisely
always implies the smearing of the other. On the other hand, probabilistic concentration
is at the heart of statistical mechanics. Due to probabilistic concentration, only some
realizations of a stochastic process have non-negligible probability of occurrence, and
these typical outcomes are nearly equally probable. Based on this premise, statistical
mechanics explains the thermodynamic behavior of large systems in terms of typical
outcomes of random microscopic events. The information-theoretic approach is another
instance of this method, as it exploits probabilistic concentration to describe the typical
behavior of encoding and decoding systems formed by large ensembles of random
variables.
1.1.3
Applications
When discussing information carried by electromagnetic waves, it is also natural to
mention practical applications. In the context of electromagnetics, the operational
aspects of information theory have found vast applications ranging from remote sensing
and imaging to communication systems. During the wireless revolution of the turn
of the millennium, the physical layer of digital communications has successfully
been abstracted using approximate probabilistic models of the propagation medium.
Information theory has fruitfully developed in this framework, and important theoretical
contributions have inspired creative minds, who improved engineering designs. Many
practical advancements have been inﬂuenced by a clearer understanding of the
information-theoretic limits of different channel models. As theoretical studies found
practical applications, industry ﬂourished. Sometimes, however, this has come at the
expense of casting a shadow on the physical limits of communication. The fundamental
question posed by Shannon regarding the ultimate limits of communication has been
obfuscated by the myriad results regarding different approximate models of physical
reality.
Research in wireless communication has expanded the knowledge tree into an
intricate forest of narrow cases, more of interest to the practitioner than to the
scientist seeking a fundamental understanding. These cases have provided useful design
guidelines, but they have also somewhat hidden the fundamental limits. This situation
is somehow natural: as a ﬁeld becomes more mature, improvements tend to become
more sectorized, and technology, rather than basic advancements, becomes the main
driver for progress. Maturity, however, should also open up the opportunity of revisiting,
reordering, reinterpreting, and pruning the knowledge tree, revealing its basic skeleton.
With it, we also wish to reveal the misconception that a rigorous physical treatment is
too complex to provide valuable engineering insights. A physical treatment not only
shows that Shannon’s theory is part of the fundamental description of our physical
universe, but it also provides insights into the design and operation of real engineering
systems. After all, if, as Alfréd Rényi (1984) put it, information theory came out of the
realization that the ﬂow of information, like other physical quantities, can be expressed
numerically, then our engineering designs should best exploit its physical nature.
03
11:58:56, subject to the Cambridge Core

1.2 The Dimensionality of the Space
5
For this reason, we devote Chapter 7 to discussing communication technologies, and
we revisit these technologies in the light of our physical treatment of information. Of
course, we can only discuss some of the fundamental principles; for a more in-depth
perspective the reader should refer to the wide range of available engineering literature.
1.2
The Dimensionality of the Space
To quantify the amount of information carried by electromagnetic waves of ﬁnite
energy, we represent the possible messages that the transmitter may send by an ensemble
of waveforms, and then attempt to quantify the amount of information transferred by
the correct selection at the receiver of one element from this ensemble, according
to Shannon’s theory. This selection implies that a certain amount of information has
been transported between the two. To realize this program, we are faced with a ﬁrst
fundamental question:
How many distinct waveforms can we possibly select from?
The answer depends on the size of the signals’ space, or the number of degrees of
freedom, available for communication and on the resolution at which each degree of
freedom can be observed. The number of degrees of freedom is limited by the nature
of the wave propagation process, while the resolution is limited by the uncertainty
associated with the observation process.
1.2.1
Bandlimitation Filtering
Any radiated waveform has an essentially ﬁnite bandwidth. Due to the interaction
with the propagation medium and with the measurement apparatus used to detect the
signal, the high-frequency components are cut off, and any signal appears as the output
of a linear ﬁlter. Typically this ﬁlter also shapes the frequency proﬁle, distorting the
transmitted waveform. A simple example of this phenomenon occurs in a scattering
environment. Due to multiple scattering, multiple copies of the transmitted waveform,
carrying different delays and attenuations while traveling on different paths, may
overlap at the receiver, creating interference and distorting the original signal – see
Figure 1.3. While in ideal free space the magnitude of the frequency response of the
propagation environment would be ﬂat across all frequencies, and the phase of the
response would be proportional to the distance between transmitter and receiver, in the
presence of a large amount of scattering the response is a highly varying signal, due to
the interference over the multiple scattered paths.
An analogous effect occurs in the spatial domain. The signal observed along any
spatial cut-set that separates transmitters and receivers is the image of all the elements
radiating from one side of the cut to the other. These radiating waveforms may interfere
along the cut, leading to spatial ﬁltering – see Figure 1.4.
03
11:58:56, subject to the Cambridge Core

6
Introduction
|i(t)|
t
0
t
|f(t)|
0
Fig. 1.3
Spreading of the signal in the time domain due to multiple scattering. Transmitted signal: i(t);
received signal: f(t).
Cut-Set
transmitters
receivers
scatterer
Fig. 1.4
Multiple signals overlap over the cut-set boundary.
i(s,t)
f (s,t)
W,  
Fig. 1.5
Propagation ﬁltering.
A block diagram of the propagation ﬁltering effect that occurs when a scalar source
current i(s,t) of one spatial and one temporal variable produces a scalar electromagnetic
ﬁeld f(s,t) observed along a given cut-set boundary is depicted in Figure 1.5. The ﬁgure
shows that the effect of propagation is analogous to that of a linear ﬁlter of frequency
cut-off  and of wavenumber cut-off W. The form of the transfer function depends on
the features of the environment where propagation occurs and is studied in Chapters 8
and 9. This ﬁltering operation limits the number of distinct space–time waveforms
that can be observed at the receiver, and makes the space of electromagnetic signals
essentially bandlimited and suitable for an information-theoretic analysis.
03
11:58:56, subject to the Cambridge Core

1.2 The Dimensionality of the Space
7
1.2.2
The Number of Degrees of Freedom
The physics of propagation dictate that any observed electromagnetic ﬁeld is an
essentially bandlimited function. This basic property allows us to deﬁne the size
of the signals’ space in terms of the number of degrees of freedom. Consider a
one-dimensional, real, scalar waveform f of a single scalar variable t. We assume that f
is square-integrable, and
 ∞
−∞
f 2(t)dt ≤E.
(1.1)
This ensures that the waveform can be expanded in a series of, possibly complex,
orthonormal basis functions {ψn},
f(t) =
∞

n=1
anψn(t),
(1.2)
where
an =
 ∞
−∞
f(t)ψ∗
n (t)dt.
(1.3)
The equality in (1.2) is intended in the “energy” sense:
lim
N→∞
 ∞
−∞
[f(t) −fN(t)]2dt = 0,
(1.4)
where
fN(t) =
N

n=1
anψn(t).
(1.5)
In the language of mathematics, f is in L2(−∞,∞), and it can be viewed as a point in
an inﬁnite-dimensional space of coordinates given by the coefﬁcients {an} in (1.3). By
varying the values of these coefﬁcients, we can create distinct waveforms and use them
to communicate information. If the orthonormal set of basis functions {ψn} is complete,
then using (1.2) we can construct any element in the space of signals deﬁned by (1.1).
By associating a waveform in this space with a given message that the transmitter wishes
to communicate, the correct selection of the same waveform at the receiver implies that
a certain amount of information is transferred between the two. One may reasonably
expect that only a ﬁnite number of coefﬁcients is in practice needed to specify the
waveform up to any given accuracy, while using a larger number does not signiﬁcantly
improve the resolution at the receiver. It turns out that the question of what the smallest
N is beyond which varying higher-order coefﬁcients does not change the form of the
waveform signiﬁcantly has a remarkably precise answer.
Determining this number over all possible choices of basis functions is a question ﬁrst
posed by Kolmogorov in 1936, and corresponds to determining the number of degrees of
freedom of the waveform. Consider an observation interval [−T/2,T/2], and introduce
the norm
∥f∥=
 T/2
−T/2
f 2(t)dt
1/2
.
(1.6)
03
11:58:56, subject to the Cambridge Core

8
Introduction
The problem amounts to determining the interval of values of N for which the
approximation error for any signal f,
∥eN∥= ∥f(t) −fN(t)∥,
(1.7)
transitions from being close to its maximum to being close to zero.
For signals of spectral support in an interval of size 2, and observed over a time
interval of size T, the angular frequency bandwidth  and the size of the observation
interval play a key role in determining the number of degrees of freedom, as the
approximation error undergoes a phase transition at the scale of the product T.
For any ϵ > 0, letting Nϵ be the minimum number of basis functions for which the
normalized energy of the error
∥eNϵ∥2
E
≤ϵ,
(1.8)
and letting
N0 = T
π ,
(1.9)
we have
lim
N0→∞
Nϵ
N0
= 1.
(1.10)
This result was a crowning achievement of three scientists, Henry Landau, Henry
Pollak, and David Slepian, working at Bell Laboratories in the 1960s and 70s, and we
discuss it in detail in Chapters 2 and 3. It identiﬁes the number of degrees of freedom
with the time-bandwidth product N0 = T/π, and shows that this number is, up to ﬁrst
order, independent of the level of approximation ϵ. This is evident by rewriting (1.10) as
Nϵ = N0 + o(N0) as N0 →∞,
(1.11)
where the dependence on ϵ appears hidden as a pre-constant of the second-order
term o(N0) in the phase transition of the number of degrees of freedom. Varying the
approximation level does not affect the ﬁrst-order scaling of the result. Figure 1.6 shows
the transition of the approximation error for the optimal choice of basis functions.
According to (1.10), the transition occurs in a small interval and tends to become a
step function when viewed at the scale of N0.
A widely used representation for bandlimited waveforms is the cardinal series that
uses sampled values of the waveform as coefﬁcients {an} and real functions of the form
sinc(t) = (sint)/t as the basis set {ψn}, yielding the Kotelnikov–Shannon–Whittaker
sampling representation
f(t) =
∞

n=−∞
f(nπ/)sinc(t −nπ),
(1.12)
which interpolates the signal from regularly spaced samples at frequency /π. This
representation is suboptimal in terms of the approximation error (1.7). The optimal
interpolating functions, called prolate spheroidal wave functions, that achieve the
03
11:58:56, subject to the Cambridge Core

1.2 The Dimensionality of the Space
9
n
N0
1
 e 
o(1)
n
E
O
||
||
1
2
ϵ
Fig. 1.6
Phase transition of the approximation error. The transition becomes sharper, and its width,
viewed at the scale of N0, shrinks to zero as N0 →∞.
smallest approximation error are obtained by solving an eigenvalue problem that we
study in Chapters 2 and 3.
1.2.3
Space–Time Fields
The electromagnetic ﬁeld is in general a function of four scalar variables: three spatial
and one temporal. It follows that in order to appreciate the total ﬁeld’s informational
content in terms of degrees of freedom, we need to extend the treatment above to higher
dimensions.
Let us ﬁrst consider the canonical case of a two-dimensional domain of cylindrical
symmetry, in which an electromagnetic ﬁeld is radiated by current sources located
inside a circular domain of radius r, and oriented perpendicular to the domain. The
sources can also be induced by multiple scattering inside the domain. In any case, the
radiated ﬁeld away from the sources is completely determined by the ﬁeld on the cut-set
boundary surrounding the sources and through which it propagates – see Figure 1.7. On
this boundary, we can refer to a scalar ﬁeld f(φ,t) that is a function of only two scalar
variables: one angular and one temporal. The corresponding four representations, linked
by Fourier transforms, are depicted in Figure 1.8, where ω indicates the transformed
coordinate of the time variable t and w indicates the wavenumber that is the transformed
coordinate of the angular variable φ.
Letting  be the angular frequency bandwidth and W be the wavenumber bandwidth,
we now wish to determine the total number of degrees of freedom of the space–time
ﬁeld f(φ,t). To visualize the phase transition, we ﬁx the bandwidth  and the size of
the angular observation interval S = 2π, and scale the time support where the signal
is observed T →∞and the wavenumber bandwidth W →∞. Using the results of the
monodimensional case, we have that as T →∞the number of time–frequency degrees
of freedom is of the order of
N0 = T
π .
(1.13)
03
11:58:56, subject to the Cambridge Core

10
Introduction
r
O
Fig. 1.7
Cylindrical propagation, cut-set boundary.
Fig. 1.8
Four ﬁeld representations linked by Fourier transforms.
In a symmetric fashion, letting the wavenumber bandwidth W →∞, we have that the
number of space–wavenumber degrees of freedom over an observation interval S = 2π
is of the order of
N0 = W2π
π
.
(1.14)
The wavenumber bandwidth is related to the frequency of transmission. As we shall see
in Chapter 8, every positive frequency component ω > 0 of the signal has a wavenumber
bandwidth that is, for any possible conﬁguration of sources and scatterers inside the
circular radiating domain, at most W = ωr/c. It follows that the appropriate asymptotic
regime W →∞can be obtained by letting r →∞, so that by (1.14) the number of
space–wavenumber degrees of freedom at angular frequency ω becomes of the order of
N0(ω) = 2πrω
cπ
= 2πr
λ/2 ,
(1.15)
where λ = 2πc/ω is the radiated wavelength, and c is the propagation speed of the ﬁeld.
The total number of degrees of freedom can now be obtained by integrating (1.15) over
the bandwidth, and multiplying the result by T/π. It follows that as r,T →∞the total
number of degrees of freedom is of the order of
N0 = 2πrT
cπ2
 
0
ωdω = 2πrT2
2cπ2
.
(1.16)
03
11:58:56, subject to the Cambridge Core

1.2 The Dimensionality of the Space
11
Rearranging terms, we have
N0 = T
π
2πr
2πc .
(1.17)
Equation (1.17) shows that the number of degrees of freedom is the product of
two factors, each viewed in an appropriate asymptotic regime: one accounting for the
number of degrees of freedom in the time–frequency domain, T/π, as T →∞, and
the other accounting for the number of degrees of freedom in the space–wavenumber
domain, 2πr/(2πc), as r →∞. The second factor corresponds to the perimeter of
the disc of radius r normalized by an interval of wavelengths 2πc/. Its physical
interpretation is that of a spatial cut-set imposing a limit on the amount of information,
per time–frequency degree of freedom, that can ﬂow from the interior of the domain to
its exterior.
An analogous computation yields the number of degrees of freedom of the ﬁeld
radiated by arbitrary sources and scatterers in three-dimensional space. For spherical
systems of radius r, we have the number of space–wavenumber degrees of freedom at
frequency ω,
N0(ω) = 4πr2ω2
(cπ)2 = 4πr2
(λ/2)2 ,
(1.18)
and the total number of degrees of freedom:
N0 = 4πr2T
c2π3
 
0
ω2dω = 4πr2T3
3c2π3
.
(1.19)
Once again, rearranging terms we have
N0 = T
π
4πr22
3c2π2 .
(1.20)
The number of degrees of freedom is the product of two factors, each viewed in an
appropriate asymptotic regime: one accounting for the number of degrees of freedom in
the time–frequency domain, T/π, as T →∞, and the other accounting for the number
of degrees of freedom in the space–wavenumber domain, 4πr22/(3c2π2), as r →
∞. The second factor corresponds to the spatial cut-set represented by the normalized
surface area of the sphere of radius r through which the information must ﬂow. This
imposes a limit on the amount of information, per time–frequency degree of freedom,
that can ﬂow across the cut.
The results above lead to an interpretation of information as a conservative quantity
that is radiated from a volume to the outside space. This interpretation is supported by
a rigorous theory, which is presented in Chapters 8 and 9.
1.2.4
Super-resolution
We have argued that the number of degrees of freedom of square-integrable, bandlimited
signals undergoes a phase transition. For signals of a single variable, this number
quickly drops to zero in correspondence with a critical value N0, given by the product of
the size of the observation interval in the natural time and the size of the support set of
03
11:58:56, subject to the Cambridge Core

12
Introduction
a1
a2
aN0
f (t)
ψ
ψ
ψ
+
Fig. 1.9
Multiple channels resulting from a signal’s decomposition.
the signal in the transformed frequency domain. The number of degrees of freedom of
the signal is in principle inﬁnite, but the phase transition dictates that no improvement
of the accuracy of the observation can go substantially beyond a given characteristic
scale. It follows that any waveform in the space can be identiﬁed, up to arbitrary
accuracy, by essentially N0 = T/π real numbers. Modulating the signal’s coefﬁcients
in excess of N0 does not change the shape of the waveform signiﬁcantly, so that these
higher-order coefﬁcients cannot be used to communicate any additional information.
On the other hand, the most signiﬁcant N0 coefﬁcients essentially determine the shape
of the waveform, and we can view them as independent channels that can be used
to multiplex different streams of information from a transmitter to a receiver. Any
assignment of coefﬁcients in the signal’s expansion (1.2) compatible with the constraint
(1.1) identiﬁes one element of the space, and its correct selection at the receiver amounts
to communicating N0 real numbers – see Figure 1.9.
One could argue that by reducing the value of ϵ we can increase the number of
degrees of freedom within the transition window. This amounts to having detectors
capable of capturing the decaying tail of the approximation error, so that they are able to
distinguish between signals that differ in their higher-order coefﬁcients. These detectors
can in principle achieve unbounded super-resolution – see Figure 1.10.
In imaging systems operating in the space–wavenumber domain, a number of
technologies have been proposed to achieve super-resolution, including near-ﬁeld
microscopy and the use of meta-materials that facilitate beamforming. The steepness of
the phase transition, however, imposes an exponential cost on all of these technologies.
The same mathematics at the basis of basic quantum indeterminacy principles ensures
that all efforts aimed at breaking the informational limit on the number of degrees of
freedom are eventually doomed when viewed at the scale of N0, when the transition
in the approximation error becomes a step function. Chapter 12 further discusses
super-resolution limits. The conclusion is that the rules of spectral concentration dictate
an asymptotic limit independent of the technology employed:
03
11:58:56, subject to the Cambridge Core

1.3 Deterministic Information Measures
13
n
N0
1
SUPER-RESOLUTION
O
1
 e n
E
||
|| 2
Fig. 1.10
Super-resolving detectors aim to capture the decaying tail of the number of degrees of freedom
of the signal.
It is impossible to resolve a number of dimensions larger than is naturally imposed
by propagation constraints and expressed asymptotically by the number of degrees
of freedom of the ﬁeld.
1.3
Deterministic Information Measures
Every electromagnetic waveform is essentially identiﬁed by N0 real numbers. This
limitation is due to the laws of propagation that impose bandlimitation and hence a
certain amount of smoothness of the corresponding waveform. The mathematics at the
basis of this result are the rules of spectral concentration. The result, however, does not
pose a limit on the amount of information carried by the signal. The N0 real numbers
identifying the waveform can be speciﬁed up to arbitrary precision, and this results in
an inﬁnite number of possible waveforms that can be used for communication. Even
if the rules of spectral concentration pose a limit on the effective dimensionality of the
signals’ space in terms of degrees of freedom, the amount of information associated with
the selection of one waveform in the space is still unbounded if each coordinate can be
speciﬁed with arbitrary accuracy. Some kind of indeterminacy must be introduced to
limit the resolution at which the signal can be observed. Such indeterminacy follows
from the intrinsic uncertainty associated with the observation process.
Waveforms that are distinct as mathematical objects cannot be resolved as distinct
physical quantities at a scale smaller than the noise affecting the measurement process.
For this reason, we may consider two waveforms distinguishable only if, for a given
ϵ > 0,
∥f1 −f2∥> ϵ.
(1.21)
03
11:58:56, subject to the Cambridge Core

14
Introduction
This point of view yields a discretization of the space consistent with the quantized
nature of the observable world, and allows an information-theoretic description of the
space of signals in terms of the universal unit of information: the bit.
1.3.1
Kolmogorov Entropy
Following up on his 1930’s investigations on the number of degrees of freedom of
functions, and while working under the inﬂuence of Shannon’s 1948 work, in 1956
Andrey Nikolaevich Kolmogorov introduced the notion of ϵ-entropy. This is deﬁned as:
The minimum number Hϵ of bits that can represent any signal in the space within
accuracy ϵ.
Viewing bandlimited signals observed over a large time interval and subject to the
constraint (1.1) as points in a space of N0 dimensions and of radius
 ∞
−∞
f 2(t)dt
1/2
=
 N0

n=1
a2
n
1/2
≤
√
E,
(1.22)
the ϵ-entropy geometrically corresponds to the logarithm base two of the covering
number Qϵ(E), that is, the minimum number of balls of radius ϵ such that their union
contains the whole space, and we have
Hϵ = logQϵ(E) bits.
(1.23)
In this way, any point in the space can be identiﬁed, in the sense of (1.21), with the
center of a corresponding ϵ-ball covering it – see Figure 1.11 – and any signal can be
represented by essentially Hϵ bits.
The ϵ-entropy is related to the number of degrees of freedom. Given the inter-
pretation of the number of degrees of freedom as the effective dimensionality of an
inﬁnite-dimensional functional space, and since we may expect the covering number of
this space to be roughly of the order of the volume of the space divided by the volume
of the ϵ-ball, a reasonable guess may be that the entropy is roughly
Hϵ = log(
√
E/ϵ)N0 = N0 log(
√
E/ϵ).
(1.24)
As we shall see in Chapter 12, this intuition is essentially correct; the entropy grows
linearly with the number of degrees of freedom, and logarithmically with the ratio of
the maximum energy of the signal to the energy of the noise. This latter term, called
the signal-to-noise ratio, can be geometrically viewed as imposing a quantization of the
space at level ϵ due to the noise inherent in the measurement process.
1.3.2
Kolmogorov Capacity
Another information-theoretic notion introduced by Kolmogorov is the ϵ-capacity. This
is deﬁned as:
03
11:58:56, subject to the Cambridge Core

1.3 Deterministic Information Measures
15
ϵ
f
E
√
2 
Fig. 1.11
Every signal in the space is within ϵ of the center of a covering ball.
The maximum number Cϵ of bits that can be communicated by selecting one signal
at the transmitter, and observing it at the receiver with perturbation at most ϵ/2.
Geometrically, the ϵ-capacity corresponds to the logarithm base two of the packing
number Mϵ(E), that is, the maximum number of disjoint balls of radius ϵ/2 with their
centers situated inside the signals’ space – see Figure 1.12. Since any two signals in the
packing are at least ϵ apart, they cannot be confused when each of them is observed
with perturbation at most ϵ/2, and we have
Cϵ = logMϵ(E) bits.
(1.25)
An upper bound on the ϵ-capacity can be computed in terms of ϵ-entropy and using
(1.24), yielding
Cϵ ≤Hϵ/2 = N0 log(2
√
E/ϵ).
(1.26)
The inequality in (1.26) follows from the observation that if, by contradiction, Mϵ >
Qϵ/2, then there must be one ϵ-ball that covers at least two points that are more than ϵ
apart, which is impossible – see Figure 1.13.
A lower bound on the ϵ-capacity can be obtained by constructing a codebook
composed of a subset of signals in the space, each corresponding to a given message.
A transmitter can select any one of these signals, which is observed at the receiver
with perturbation at most ϵ/2. By choosing signals in the codebook to be at distance at
03
11:58:56, subject to the Cambridge Core

16
Introduction
least ϵ from each other, the receiver can decode the message without error. Counting
the signals in the codebook, and taking the logarithm, gives a lower bound on the
capacity. A simple codebook follows from the lattice packing depicted in Figure 1.14.
This codebook is, however, far from being optimal. In higher dimensions, the volume
of the hypersphere tends to concentrate on its boundary, a phenomenon called sphere
hardening. The packing in the inscribed hypercube then captures only a vanishing
fraction of the volume available in the hypersphere, leading to an extremely inefﬁcient
packing of the space by balls of ﬁxed radii. A random codebook argument, although
not constructive, leads to a tighter capacity bound than the one obtained with the simple
packing depicted in Figure 1.14, and shows that the capacity can scale linearly with the
number of degrees of freedom, and logarithmically with the signal-to-noise ratio. These
results are described in Chapter 12.
ϵ/2
E
√
2 
Fig. 1.12
Any two signals identiﬁed by the centers of the packing balls are at a distance of at least ϵ from
each other.
ϵ
Fig. 1.13
Illustration of the bound Cϵ ≤Hϵ/2.
03
11:58:56, subject to the Cambridge Core

1.3 Deterministic Information Measures
17
ϵ/2
E
√
  /N
E
√
2
0
Fig. 1.14
Lattice packing for the computation of a simple lower bound on the ϵ-capacity.
1.3.3
Quantized Unit of Information
We now summarize. Introducing a resolution limit in the signals’ space leads to two
mathematical notions that quantify information in a discrete way, in terms of the
universal unit of information: the bit. The entropy represents the smallest number of
bits needed to identify any signal in the space, and is geometrically represented in terms
of sphere covering at resolution ϵ, while the capacity represents the largest number of
bits carried by any signal in the space, and is geometrically represented in terms of
sphere packing at resolution ϵ/2. Both entropy and capacity are linearly related to the
number of degrees of freedom and logarithmically related to the ratio of the maximum
norm of the signal to the maximum norm of the perturbation with which the signal is
observed.
The mathematical deﬁnitions of entropy and capacity are used to quantify information
in terms of degrees of freedom, corresponding to the “massiveness” or effective
dimensionality of the space, and in terms of resolution uncertainty, corresponding to the
signal-to-noise ratio. These quantities are determined by the physics. Wave propagation
ensures that the observed ﬁeld is a ﬁltered version of the transmitted one through a
propagation operator that projects it onto the space of effectively bandlimited signals
of essentially N0 dimensions. The signal-to-noise ratio is determined by the signal’s
energy and by the intrinsic uncertainty associated to the measurement process that is
imposed by the quantized nature of the world. Even if ﬁelds are continuous quantities,
they can only be observed in a quantized fashion, dictated by the measurement
uncertainty.
03
11:58:56, subject to the Cambridge Core

18
Introduction
Kolmogorov’s ϵ-entropy and capacity are also closely related to the probabilistic
notions of entropy and capacity used in information theory and statistical mechanics.
In this context, Shannon’s entropy represents the smallest number of bits needed to
identify any typical realization of a continuous stochastic process, quantized at level ϵ.
Shannon’s capacity represents the largest number of bits that can be transported by any
signal in the space, when this is subject to a stochastic perturbation of standard deviation
ϵ. In this setting, a geometric picture completely analogous to the one described in the
deterministic case arises, where the value of ϵ is replaced by the statistical dispersion of
the noise associated with the measurement process, which probabilistically concentrates
around its typical value.
1.4
Probabilistic Information Measures
The functional theory of information, based on the notions of ϵ-entropy and ϵ-capacity
and developed by the Russian school of Kolmogorov and his disciples, was strongly
inﬂuenced by the probabilistic theory of information developed by Shannon and his
disciples in the West. The probabilistic point of view is based on the observation that
highly unpredictable signals maximize the “surprise” of observing a given outcome.
This surprise is identiﬁed with the amount of information that is transported by the
signal. It follows that a highly unpredictable signal is highly informative, in the sense
that the message it represents maximally alters the state of the receiver, and thus it must
be described by a large number of bits.
To make these considerations precise, we model the coefﬁcients in the signal’s
representation as random variables described by a probability distribution, and introduce
the concept of statistical dispersion, or entropy, associated with this distribution.
Informationally rich signals, having long bit representations, correspond to distributions
with large entropy. For signals having the maximum entropy distribution, compat-
ible with the signals’ energy constraint, the typical values of the coefﬁcients are
maximally dispersed in a space of N0 dimensions, and they are all roughly equally
probable. This is the asymptotic equipartition property, expressed rigorously by the
Shannon–McMillan–Breiman theorem, and ensures that the surprise associated with
a typical realization is always maximized.
When signals are subject to noisy observations, the Shannon capacity is deﬁned in
terms of the largest number of bits that can be transported from transmitter to receiver
by any one signal in the space, with negligible probability of error. This can be described
in terms of another probabilistic quantity, the mutual information, which depends
only on the joint distribution of the observed and transmitted signals. The supremum
of the mutual information, over all possible distributions for the transmitted signals
and compatible with the energy constraint, is the Shannon capacity. This statement is
Shannon’s coding theorem, which relates the operational deﬁnition of capacity to the
maximization of a given information functional.
03
11:58:56, subject to the Cambridge Core

1.4 Probabilistic Information Measures
19
0.2
0.4
0.6
0.8
1.0
0.2
0.4
0.6
0.8
1.0
p
H(p)
0
Fig. 1.15
Entropy of a Bernoulli random variable of parameter p.
1.4.1
Statistical Entropy
The probabilistic quantity used to describe the extent of the random ﬂuctuations of the
signal’s coefﬁcients is the statistical entropy. This arises in thermodynamics, statistical
mechanics, and information theory with slightly different deﬁnitions that are all related
to each other.
The Shannon (1948) entropy of a discrete random variable X taking values in a set
X with probability mass function pX(x) is
H = −

x∈X
p(x)logp(x)
= −EX logp(X),
(1.27)
where we deﬁne, by continuity, 0log0 = 0. The logarithm is taken base two, and this
only amounts to choosing the unit of measure to be bits. A change of base simply
corresponds to multiplication by a constant factor and thus to a change of units, as
logb p = logb aloga p.
The Shannon entropy represents a measure of the uncertainty of the random variable.
Viewing this uncertainty as the “surprise” of observing a given outcome, it also
measures the amount of information transferred in the observation process. When the
random variable takes only one value with probability one, there is no surprise in the
realization and the entropy is zero. When the outcome is unknown, the probability
mass has a wider distribution and the entropy becomes positive. When the distribution
becomes uniform among all possible outcomes, the entropy is maximized and it
corresponds to the logarithm of the number of possible outcomes. A plot of the entropy
for the simplest case of a Bernoulli random variable that takes values with probability p
and 1 −p is depicted in Figure 1.15.
03
11:58:56, subject to the Cambridge Core

20
Introduction
When our random variable can assume any one of N values, it can be viewed as a
statistical mechanical system, where each state n ∈{1,2,...,N} occurs with probability
pn. The Gibbs (1902) entropy of such a system is
HG = −kB
N

n=1
pn logpn,
(1.28)
where kB is Boltzmann’s constant, which has the units of energy divided by temperature.
This normalization constant makes the deﬁnition consistent with the phenomenological
notion of entropy developed by Clausius (1850–65) in a series of papers on the
mechanical theory of heat. According to Clausius, a variation of thermodynamic entropy
is deﬁned by a corresponding variation of energy, in terms of heat absorbed by the
system, at a given absolute temperature TK, namely
dHC = dE
TK
.
(1.29)
To see the correspondence between Gibbs’ and Clausius’ deﬁnitions, consider an
isolated system in thermal equilibrium, where all states are equally probable. In this
case, letting pn = 1/N, the Gibbs entropy (1.28) achieves its maximum value and
reduces to the Boltzmann (1896–8) form2
HB = kB logN.
(1.30)
Assuming that each state of the system is to be deﬁned by an ensemble of M elementary
modes that take binary values, we have N = 2M and, by (1.30),
HB = kBM.
(1.31)
Assuming that at equilibrium each elementary mode, due to thermal agitations, has
average energy proportional to the temperature and equal to kBTK, then the total average
energy of the system is
E = kBTKM.
(1.32)
Differentiating both sides, dividing by TK, and using (1.31), it follows that the
phenomenological deﬁnition of the thermodynamic entropy of an isolated system
in thermal equilibrium and subject to an average energy constraint coincides with
the statistical mechanical deﬁnition, and corresponds to the conﬁguration in which
all states are equally probable, so that the Gibbs entropy is maximized and attains
Boltzmann’s form. The conclusion is that at equilibrium statistical and thermodynamic
entropies coincide, and Shannon’s deﬁnition is simply a dimensionless version of the
thermodynamic entropy.
We further examine the connection between statistical and thermodynamic entropies
in Chapter 11.
2 Both the Gibbs and Boltzmann entropies are usually expressed in natural units, or nats, taking the
logarithm base e. In this book we ﬁx the units of information to be bits and take the logarithm base two,
to favor the comparison with the Shannon entropy. The equivalent expression in nats can easily be
obtained by dividing the value in bits by log2 e.
03
11:58:56, subject to the Cambridge Core

1.4 Probabilistic Information Measures
21
1.4.2
Differential Entropy
To appreciate the relationship between statistical entropy and the number of degrees
of freedom of an electromagnetic waveform, we introduce the differential entropy of a
continuous random variable X of probability density function gX(x). This is also due to
Shannon (1948), and is given by
hX(g) = −

X
g(x)logg(x)dx
= −EX logg(X),
(1.33)
where X is the support set of the random variable. The joint differential entropy of two
random variables is
hX,Y(g) = −

X

Y
g(x,y)logg(x,y)dxdy
= −EX,Y logg(X,Y),
(1.34)
and when the two variables are independent, we have
hX,Y = hX + hY.
(1.35)
The deﬁnition naturally extends to random vectors. If X1,X2,...,XN0 are mutually
independent, we have
h(X1,X2,...,XN0) =
N0

n=1
hXn,
(1.36)
and if they also have the same distribution, we have
h(X1,X2,...,XN0) = N0h,
(1.37)
where h = hX1(f) is the differential entropy of a single random variable.
Although each random variable in the set {Xn} can take values over an arbitrary,
possibly inﬁnite, support, for any ﬁnite value of h the rules of probabilistic concentration
ensure that the typical realizations occur over a much smaller set. More precisely, the
asymptotic equipartition property ensures that as N0 →∞the volume of the support set
of the random variables is only roughly 2N0h and all realizations inside this volume are
roughly equally probable. This situation can be visualized by having realizations inside
an equivalent hypercube in a space of N0 dimensions, with each side length of range
2h, and it provides an interpretation of the differential entropy as the logarithm of the
equivalent side length of the smallest volume that contains most of the probability – see
Figure 1.16.
The precise statement is given by deﬁning the typical support set Vϵ(N0) composed
of sequences (x1,x2,...,xN0) ∈RN0 such that, for all ϵ > 0,
2−N0(h+ϵ) ≤g(x1,x2,...,xN0) ≤2−N0(h−ϵ).
(1.38)
The aggregate probability of the typical set for N0 sufﬁciently large is

Vϵ(N0)
g(x1,x2,...,xN0)dx1dx2 ···dxN0 > 1 −ϵ,
(1.39)
03
11:58:56, subject to the Cambridge Core

22
Introduction
and its volume is
(1 −ϵ)2N0(h−ϵ) ≤

Vϵ(N0)
dx1dx2 ···dxN0 ≤2N0(h+ϵ).
(1.40)
1.4.3
Typical Waveforms
By identifying the sequence of random variables {Xn} with the coefﬁcients representing
a given waveform, we can view each possible waveform as a random point in a space
of N0 dimensions. It follows that a large typical volume corresponds to signals that are
widely dispersed, while a small typical volume means that most signals are concentrated
in a small portion of the space and asymptotic equipartition can be expressed by the
statement:
Almost all observed signals are almost equally probable.
To obtain an information-theoretic description of the typical waveforms, we proceed
in an analogous way to the one followed in the deterministic setting: to bound the size of
the typical volume we introduce an energy constraint that restricts the possible signal’s
conﬁgurations. Then, in order to obtain a bit representation, we introduce a resolution
limit at which each waveform can be observed.
To realize this program, we consider random waveforms of the form
f(t) =
N0

n=1
anψn(t),
(1.41)
where {an} are realizations of an independent and identically distributed (i.i.d.)
real-valued random process {An}, and ψn are deterministic, orthonormal, real basis
functions. Each random variable in the process is distributed as A and satisﬁes the
expected squared norm per degree of freedom constraint,
E(A2) ≤P.
(1.42)
2h
2h
2h
0
N =3
Fig. 1.16
The volume of the typical set coincides with the volume of an equivalent hypercube of side
length 2h.
03
11:58:56, subject to the Cambridge Core

1.4 Probabilistic Information Measures
23
This constraint should be compared with (1.22). While in the deterministic setting we
imposed an energy constraint on the whole signal, here we impose an average constraint
separately on each dimension of the space.
Maximizing the differential entropy subject to (1.42), we obtain the upper bound
hA ≤1
2 log(2πeP).
(1.43)
The maximizing density that achieves the bound in (1.43) is a zero-mean Gaussian of
variance P – see Problem 1.15. By asymptotic equipartition, the volume of the typical
set is then roughly at most
2N0hA ≤(2πeP)N0/2.
(1.44)
It follows that whatever distribution we choose for the signal’s coefﬁcients, compatible
with the constraint on the expected squared norm per degree of freedom (1.42), the
signals’ space is always conﬁned within an effective volume at most of the order of the
square root of the imposed constraint raised to the number of degrees of freedom. The
largest effective volume is achieved by a Gaussian probability density function, which
provides the largest possible statistical dispersion of the observed waveform.
1.4.4
Quantized Typical Waveforms
The amount of information of our continuum state space of signals depends not only
on its statistical dispersion, but also on the level of quantization at which each signal
is observed. If we quantize each dimension of the signals’ space at level ϵ, then the
number of quantized typical signals is roughly
N = 2N0hA
ϵN0
≤(2πeP)N0/2
ϵN0
=
√
2πeP
ϵ
N0
,
(1.45)
and since these are roughly equiprobable, the corresponding statistical entropy of the
equipartitioned system of quantized signals is
H = logN ≤N0 log
√
2πeP
ϵ

,
(1.46)
where the equality is achieved by a zero-mean Gaussian of variance P. The statistical
entropy in (1.46) also corresponds to the number of bits needed on average to represent
any quantized signal in the space. This follows from the observation that there are N
quantized typical signals, and each can be identiﬁed by a sequence of H = logN bits.
The bound (1.46) should be compared with (1.24). In the deterministic case the
Kolmogorov ϵ-entropy of the signals’ space grows at most linearly with the number
of degrees of freedom and at most logarithmically with the signal-to-noise ratio
√
E/ϵ.
In the stochastic case, the Shannon entropy of the space of ϵ-quantized signals grows
at most linearly with the number of degrees of freedom and at most logarithmically
with the expected signal-to-noise per degree of freedom ratio
√
P/ϵ. While in the
deterministic model we bounded the energy of the signal and introduced quantization
03
11:58:56, subject to the Cambridge Core

24
Introduction
ϵ
a
g(a)
Fig. 1.17
Quantization of a continuous random variable of probability density function g(a).
in terms of Euclidean distances, in the stochastic model we introduced an average
constraint and a quantization on each dimension of the space.
The result (1.46) can be derived by writing the statistical entropy of the space
of ϵ-quantized signals in terms of the entropy of a single quantized coefﬁcient.
Consider a coefﬁcient A distributed according to a continuous density g(a). We perform
quantization as indicated in Figure 1.17. By continuity, for all k we let a(k) be a value
such that
g(a(k))ϵ =
 (k+1)ϵ
kϵ
g(a)da.
(1.47)
The quantized random variable Aϵ is deﬁned as
Aϵ = a(k) if kϵ ≤A < (k + 1)ϵ.
(1.48)
We then have
lim
ϵ→0(HAϵ + logϵ) = hA.
(1.49)
This result can be derived by performing the following computation:
HAϵ = −
∞

k=−∞
g(a(k))ϵ log[g(a(k))ϵ]
= −
∞

k=−∞
g(a(k))ϵ log[g(a(k))] −
∞

k=−∞
g(a(k))ϵ logϵ
= −
∞

k=−∞
g(a(k))ϵ log[g(a(k))] −logϵ
∞

k=−∞
 (k+1)ϵ
kϵ
g(a)da
= −
∞

k=−∞
g(a(k))ϵ log[g(a(k))] −logϵ.
(1.50)
As ϵ →0, the ﬁrst term approaches the differential entropy of A by deﬁnition of
Riemann integrability, establishing (1.49).
It now follows that for small values of ϵ, by drawing N0 independent random
coefﬁcients subject to the constraint (1.42), the entropy of the quantized sequence is
03
11:58:56, subject to the Cambridge Core

1.4 Probabilistic Information Measures
25
approximately
H = N0HAϵ
= N0(hA −logϵ)
≤N0 log
√
2πeP
ϵ

,
(1.51)
establishing (1.46).
1.4.5
Mutual Information
Shannon’s entropy is a probabilistic quantity that measures information in terms of
the number of bits needed to identify any typical realization of a random waveform,
quantized at level ϵ on each dimension of the space, and subject to an expected norm
per degree of freedom constraint. It is the stochastic analog of Kolmogorov’s entropy,
which counts the number of bits needed to identify any waveform in the space, quantized
at level ϵ and subject to an energy constraint on the whole signal.
Another information-theoretic quantity, analogous to Kolmogorov’s capacity, is used
to quantify in a stochastic setting the largest amount of information that can be
transported by any waveform in the space. In this case, the stochastic model assumes
that observations are random variables governed by a probability distribution. It follows
that the signals’ coefﬁcients that are selected randomly at the transmitter and observed
at the receiver have a joint probability density g(x,y). Given any transmitted random
coefﬁcient X, the observed coefﬁcient Y has a conditional distribution g(y|x), and we
say that X is observed through the probabilistic channel depicted in Figure 1.18.
The conditional differential entropy represents the remaining uncertainty about the
outcome of Y given the outcome of X,
hY|X = −

X

Y
g(x,y)logg(y|x)dxdy
= −EX,Y logg(Y|X).
(1.52)
A simple calculation yields the following chain rule:
hX,Y = hX + hY|X,
(1.53)
and when the two variables are independent, we have
hY|X = hY,
(1.54)
so that (1.53) reduces to (1.35).
g(y|x)
X
Y
Fig. 1.18
Probabilistic channel.
03
11:58:56, subject to the Cambridge Core

26
Introduction
(X;Y)
h
h
h
h
h
I
X|Y
Y|X
Y
X
(X,Y)
Fig. 1.19
Diagram describing the mutual information.
The mutual information between random variables X and Y is
I(X;Y) =

X

Y
g(x,y)log g(x,y)
g(x)g(y)dxdy.
(1.55)
This can be interpreted as the reduction in the uncertainty of Y due the knowledge of
the outcome of X, or vice versa, and can also be written as
I(X;Y) = hY −hY|X
= hX −hX|Y
= hX + hY −hX,Y
= hX,Y −hX|Y −hY|X.
(1.56)
Figure 1.19 gives a graphical interpretation of these results.
Using (1.56) and (1.49), we have that
I(X;Y) = hY −hY|X
= lim
ϵ→0(HYϵ −logϵ −HYϵ|Xϵ + logϵ)
= lim
ϵ→0(HYϵ −HYϵ|Xϵ),
(1.57)
showing that the mutual information between two random variables is the limit of the
mutual information in the discrete setting between their quantized versions.
1.4.6
Shannon Capacity
We now consider the supremum of the mutual information over all possible distributions
for the channel’s input, namely
C = sup
gX
I(X;Y),
(1.58)
where gX varies over the set of admissible distributions for the random variable X. In the
case of the stochastic model of signals considered so far, these would be all distributions
03
11:58:56, subject to the Cambridge Core

1.4 Probabilistic Information Measures
27
2N  h
0
2N  hY
0
2N  hY
0
X
X
X  ,
 N  0 
1 X  ,
2
X  
(
)
...,
Y  ,
 N  0 
1 Y  ,
2
Y  
(
)
... ,
|
Fig. 1.20
Uncertainty spread due to noise.
satisfying the average constraint
E(X2) ≤P.
(1.59)
The quantity in (1.58) can be given an operational interpretation in terms of the largest
number of bits per degree of freedom that can be transported by N0 usages of the channel
with negligible probability of error, namely the capacity of the channel. To view such
a correspondence, consider a signal whose coefﬁcients are drawn independently and
satisfy (1.59), and use it to represent a given message that a transmitter wishes to
communicate to the receiver. This signal is selected at the transmitter and observed
at the receiver through a noisy channel. Due the noise, given the knowledge of the
N0 coefﬁcients there may be a residual uncertainty about the observed signal. It
follows that the coefﬁcients of the received signal can be modeled as a random vector
(Y1,Y2,...,YN0) of conditional probability g(y1,y2,...,yN0|x1,x2,...,xN0). Assuming
the noise acts independently on all signals’ coefﬁcients, using X and Y as a shorthand
notation for X1 and Y1, we also have
h(Y1,Y2,...,YN0 |X1,X2,...,XN0) = N0hY|X.
(1.60)
The volume of the state space of the typical outputs given the input signal is 2N0hY|X,
and all signals within this volume are roughly equally probable. Thus, each of them is
an equally good candidate for the received signal, given the transmitted one. Figure 1.20
gives a visual representation of this situation. Since the total volume of the typical
outputs is 2N0hY, and each signal carries an uncertainty volume of 2N0hY|X, it follows
that we can distinguish without confusion among at most a number of signals
N ≤2N0(hY−hY|X).
(1.61)
The correct selection at the receiver of one among these signals corresponds to the act
of communicating information, and the amount of information communicated is at most
logN bits, corresponding to the number of binary digits that identify the signal.
03
11:58:56, subject to the Cambridge Core

28
Introduction
hX|Y
0
hX
C
ACHIEVABLE
REGION
SLOPE = 1
IMPOSSIBLE
REGION
Fig. 1.21
Residual uncertainty as a function of the entropy of the input.
By (1.61), and using the deﬁnition of mutual information (1.56), an upper bound on
the number of bits per degree of freedom transported by the signal is
logN
N0
≤hY −hY|X
= hX −hX|Y
= I(X;Y)
≤sup
gX
I(X;Y) bits per degree of freedom.
(1.62)
It follows that the largest number of bits that can be reliably transported by the
waveform, namely the capacity of the channel, is upper bounded by the quantity
in (1.58). The term “reliable” here is intended in the sense that the probability of
identifying the wrong signal tends to zero as N0 →∞, and probabilistic concentration
of signals around their typical values occurs. A more detailed argument provides a
matching lower bound, and shows that the rate in (1.58) is achievable, with vanishing
probability of error. As in the deterministic case, this requires the construction of a
codebook composed of a subset of signals in the space, each corresponding to a given
message. A transmitter can select any one of these signals, whose noisy version is
observed at the receiver through the probabilistic channel. By an appropriate choice of
the codebook and of the decoding function applied to the received signal, the receiver
can then reliably identify the correct message.
The bound in (1.62) can also be written as
hX|Y ≥hX −C,
(1.63)
providing a lower bound on the residual uncertainty of the variable X given the
observation of Y as a function of the entropy of the input variable – see Figure 1.21.
The residual uncertainty is zero, provided that every coefﬁcient of the signal carries at
most C bits.
The operational setting we have just described is depicted in Figure 1.22. One
among N possible messages is selected at the transmitter and encoded into N0 random
coefﬁcients {Xn}, using an encoding function XN0 : {1,2,...,N} →RN0. Each coefﬁcient
goes through a channel governed by a conditional distribution g(y|x), forming an
03
11:58:56, subject to the Cambridge Core

1.4 Probabilistic Information Measures
29
Encoder
Decoder
Channel
m
{Xn
{Y }
n
g(y|x) 
d({Y }) 
n
{1,2,...,N}
{1,2,...,N}
}
X  
N0(m) 
Fig. 1.22
Communication system.
output waveform represented by random coefﬁcients {Yn} that are observed at the
receiver. The decoding function d : RN0 →{1,2,...N} selects a “guess” d({Yn}) for the
possible transmitted message, given the observed coefﬁcients. The rate of this (N,N0)
communication system is
R = logN
N0
bits per degree of freedom,
(1.64)
and the probability of error for message m is
pe(m) = P(d({Yn}) ̸= m|{xn} = XN0(m)).
(1.65)
A rate R > 0 is achievable if there exist a sequence of (⌈2N0R⌉,N0) communication
systems with encoding and decoding functions over blocks of N0 coefﬁcients, whose
rate is at least R, and such that
lim
N0→∞
max
m∈{1,2,...,N}pe(m) = 0.
(1.66)
The supremum of the achievable rates is the Shannon capacity of the channel, and
Shannon’s channel coding theorem is stated as follows:
The supremum of the mutual information between input and output coincides with
the Shannon capacity of the channel.
1.4.7
Gaussian Noise
The Shannon capacity can be viewed as a limit due to an effective resolution level
imposed by the noise inherent in the observation of the waveform. This makes the
results above consistent with the deterministic view expressed by (1.26), showing that
the capacity grows at most linearly with the number of degrees of freedom, but only
logarithmically with the signal-to-noise ratio. We provide this interpretation in the
context of Gaussian uncertainty, for which the Shannon capacity is expressed in terms
of number of degrees of freedom, signal’s energy, and noise constraints.
The uncertainty in the signal’s observation is due to a variety of causes, but all of
them can be traced back to the quantized nature of the world arising at the microscopic
scale. At a very small scale, continuum ﬁelds are described by quantum particles whose
conﬁgurations are uncertain, and repeated measurements appear to ﬂuctuate randomly
around their average values. A mathematical model for this situation adds a zero
03
11:58:56, subject to the Cambridge Core

30
Introduction
+
  
Xn
Yn
Zn
Fig. 1.23
The additive Gaussian channel.
mean Gaussian random variable of standard deviation ϵ independently to each ﬁeld’s
coefﬁcient, obtaining the channel depicted in Figure 1.23:
Yn = Xn +Zn .
(1.67)
The model is justiﬁed as follows. The entropy associated with the noise measures
its statistical dispersion: low differential entropy implies that noise realizations are
conﬁned to a small effective volume, while high differential entropy indicates that
outcomes are widely dispersed. The probability density function that maximizes the
uncertainty of the observation, subject to the average constraint
E(Z2
n) ≤ϵ2,
(1.68)
is the Gaussian one that achieves the maximum differential entropy
hZn = 1
2 log(2πeϵ2).
(1.69)
This distribution provides the most surprising observation, for the given moment
constraint. In addition, by the central limit theorem, the Gaussian assumption is valid
in a large number of practical situations where the noise models the cumulative effect
of a variety of random effects. A treatment of maximum entropy distributions and their
relationship with noise modeling, statistical mechanics, and the second law is given in
Chapter 11.
1.4.8
Capacity with Gaussian Noise
In Chapter 12, we provide a derivation of the capacity of the additive Gaussian noise
channel considering communication with waveforms of N0 degrees of freedom, subject
to the constraint
 N0

n=1
x2
n
1/2
≤
	
PN0.
(1.70)
Compared to (1.59), the more stringent constraint (1.70) is an empirical average rather
than a statistical one, since from (1.70) we have
1
N0
N0

n=0
x2
n ≤P.
(1.71)
03
11:58:56, subject to the Cambridge Core

1.4 Probabilistic Information Measures
31
This ensures that all transmitted signals are inside a hypersphere of radius √PN0,
and requires that any realization of a random waveform must draw N0 coefﬁcients
in a dependent fashion. Geometrically, since we are only allowed to pick signals at
random inside the hypersphere of radius √PN0, signals that are close to the boundary
along one dimension must necessarily be far from it along the other dimensions. In
contrast, in Section 1.4.6 we drew coefﬁcients independently, subject to the weaker
constraint (1.59). Fortunately, however, probabilistic concentration ensures that for any
independent construction,
lim
N0→∞
1
N0
N0

n=1
x2
n = E(X2) ≤P,
(1.72)
and the probability of constructing a random signal that violates the deterministic
constraint (1.70) is arbitrarily small as N0 →∞. The geometric interpretation of (1.72)
is that by drawing independent coefﬁcients subject to the constraint (1.59), signals are
contained with high probability inside the high-dimensional sphere of radius √PN0, as
N0 →∞.
The capacity of the additive Gaussian channel subject to (1.70) is arguably the most
notorious formula in information theory:
C = 1
2 log

1 + P
ϵ2

bits per degree of freedom.
(1.73)
Consistent with the deterministic setting, this result shows that the largest number
of bits carried by any signal in the space scales linearly with the number of degrees of
freedom and logarithmically with the signal-to-noise ratio.
The result in (1.73) can be viewed in a geometric setting by considering the standard
deviation ϵ as the uncertainty level around each coordinate, and √PN0 as the effective
radius of the signals’ space. In this way, a picture of Shannon’s capacity that is
completely analogous to the one of Kolmogorov’s 2ϵ-capacity arises. The geometric
insight based on sphere packing of the space on which the two models are built is the
same. However, while in Kolmogorov’s deterministic model packing is performed with
“hard” spheres of radius ϵ and communication in the presence of arbitrarily distributed
noise over a bounded support is performed without error, in Shannon’s stochastic model
packing is performed with “soft” spheres of effective radius √N0ϵ and communication
in the presence of Gaussian noise of unbounded support is performed with arbitrarily
low probability of error. In both cases, spectral concentration ensures that the size of the
signals’ space is essentially of N0 dimensions. Probabilistic concentration ensures that
the noise in Shannon’s model concentrates around its standard deviation, so that results
are similar in the two cases.
In Shannon’s model, when Gaussian noise is added to each signal’s dimension, it
creates a region of uncertainty around the signal’s value. In high dimensions this region
is of radius √N0ϵ, and has a spherical shape. The shape of the region can be visualized
03
11:58:56, subject to the Cambridge Core

32
Introduction
xn
yn
zn
Fig. 1.24
Cross section along the nth dimension of the uncertainty sphere.
as follows. By (1.70), signals correspond to points located inside a high-dimensional
sphere of radius √PN0. Consider the difference vector of the coordinates of the points
corresponding to transmitted and received signals,
(y1 −x1,y2 −y2,...,yn −xn) = (z1,z2,...,zN0).
(1.74)
According to our channel model, this vector is composed of N0 independent Gaussian
variables, whose joint probability density function factorizes into the product of the
individual densities, which depends only on the sum 
N0
n=1 z2
n. This shows that the region
of uncertainty around each received point is spherical – see Figure 1.24. If each signal
has a typical range that is within the ball of radius √PN0, when it is observed it appears
as having a range of radius
	
N0(P + ϵ2). Comparing the volume of the observed signal
space to the one of the uncertainty ball deﬁned by the noise, it follows that the number
of distinguishable signals is roughly given by
N =
⎛
⎝

N0(P + ϵ2)
N0ϵ2
⎞
⎠
N0
=

1 + P
ϵ2
N0
.
(1.75)
By taking the logarithm and dividing by N0, we have that the correct selection at the
receiver of one signal in this ensemble corresponds to communicating a number of bits
per degree of freedom given by (1.73).
Finally, compared to (1.22) the constraint (1.70) allows the norm of the signal to
scale with √N0, rather than being a constant. The reason for this should be clear: since
the noise is assumed to act independently on each signal’s coefﬁcient, the statistical
spread of the output, given the input signal, corresponds to an uncertainty ball of radius
√N0ϵ. It follows that the norm of the signal should also be proportional to √N0, to
avoid a vanishing signal-to-noise ratio as N0 →∞. In contrast, in the case of (1.22) the
capacity is computed assuming an uncertainty ball of ﬁxed radius ϵ.
A detailed comparison between the deterministic and the stochastic model is given in
Chapter 12.
03
11:58:56, subject to the Cambridge Core

1.5 Energy Limits
33
1.5
Energy Limits
Electromagnetic waveforms can transport an amount of information that scales linearly
with the number of degrees of freedom, representing the dimensionality of the space,
and logarithmically with the ratio of the maximum energy of the signal to the energy
of the noise. In the deterministic model of Kolmogorov, in which a bounded amount of
noise of norm at most ϵ is added to the signal, we can communicate a number of bits
proportional to the number of degrees of freedom. In the stochastic model of Shannon,
in which random noise of standard deviation at most ϵ is added independently on each
coordinate of the signal’s space and the signal’s energy is proportional to the number
of coordinates, we can communicate a number of bits proportional to the number of
degrees of freedom, with arbitrarily low probability of error.
We now explore two additional limiting regimes. In a low signal-to-noise ratio
regime, where the energy of the signal vanishes compared to the energy of the noise, the
capacity is directly proportional to the energy of the signal and inversely proportional
to the number of degrees of freedom. On the other hand, if we keep a constant
signal-to-noise ratio and increase the number of degrees of freedom, the capacity
increases. However, this also requires energy expenditure, and it turns out that the
number of degrees of freedom is ultimately limited by the laws of high-energy physics,
and cannot grow beyond what is imposed by general relativity to keep the radiating
system gravitationally stable.
1.5.1
The Low-Energy Regime
To explore the low-energy regime, we consider bandlimited signals with N0 degrees of
freedom subject to the ﬁxed energy constraint (1.22), and assume the addition of random
Gaussian noise independently to each degree of freedom, subject to (1.68). In this case,
the energy of the signal is bounded, while the total amount of noise is proportional to
N0, and we have
1
N0
N0

n=1
x2
n ≤E
N0
,
(1.76)
which tends to zero as N0 →∞. Substituting E/N0 for P into (1.73) and using a
ﬁrst-order Taylor expansion of the logarithmic function, we have
C = 1
2 log

1 +
E
N0ϵ2

≃
E
2N0ϵ2 loge bits per degree of freedom.
(1.77)
It follows that in a regime where the energy of the signal is negligible compared to
the energy of the noise, the total amount of information carried by any one signal in
the space and expressed in bits is proportional to the energy of the signal, and remains
bounded even if the number of degrees of freedom tends to inﬁnity. On the other hand,
the capacity per degree of freedom in (1.77) vanishes as N0 →∞. This is due to the
03
11:58:56, subject to the Cambridge Core

34
Introduction
O
ωc
−ωc
ω


  ,ω)
~~
~~
ω1
ω2
Fig. 1.25
Radiation over a bandwidth  centered around ωc.
signal being spread over a large number of degrees of freedom, while a constant amount
of noise is added to each degree of freedom.
1.5.2
The High-Energy Regime
In both the deterministic model of Kolmogorov and the stochastic model of Shannon,
we can increase the amount of information associated with the waveforms in the signals’
space by increasing the signal-to-noise ratio. By (1.24), (1.26), (1.46), and (1.73), this
increases entropy and capacity by a logarithmic factor. We now ask whether we can also
spend energy to obtain a linear increase of the amount of information, keeping a ﬁxed
signal-to-noise ratio. A possible strategy seems to be to increase the number of degrees
of freedom, since this increases entropy and capacity linearly, and by (1.15) and (1.18)
it can be accomplished by increasing the frequency of radiation. It turns out, however,
that high-frequency signals are also observed at a coarser resolution, so that increasing
the frequency while keeping the signal-to-noise ratio constant requires a corresponding
increase of the energy per degree of freedom of the radiated signal, and an ultimate limit
to the amount of information is imposed by the laws of high-energy physics.
To view these effects in more detail, let us have a closer look at the quantities
determining the number of degrees of freedom. By (1.15), in a two-dimensional
setting the number of space–wavenumber degrees of freedom at every frequency
ω depends on size of the cut-set boundary and on the frequency of radiation. For
any arbitrary conﬁguration of sources and scatterers, we can increase the number
of space–wavenumber degrees of freedom by transmitting at higher and higher
frequencies. This improves the spatial resolution of the received waveform on the cut-set
boundary. Similarly, in a three-dimensional setting (1.18) shows that the number of
spatial degrees of freedom at each frequency ω increases with the frequency of radiation.
When radiation occurs over a range of frequencies of support 2 centered around
the origin, the total number of degrees of freedom is given by (1.16) and (1.19), in two
and three dimensions respectively. These equations show that the number of degrees of
freedom grows with the largest frequency  of the radiated signal.
Finally, when radiation occurs over a bandwidth  centered around a carrier
frequency ωc ≫, as depicted in Figure 1.25, a computation analogous to (1.16) gives
the following total number of degrees of freedom in the two-dimensional setting:
N0 = T
π
2πr
cπ
 ω2
ω1
ωdω
03
11:58:56, subject to the Cambridge Core

1.5 Energy Limits
35
= T
π
2πr
cπ
(ω2
2 −ω2
1)
2
= T
π
2πrωc
cπ
.
(1.78)
Similarly, a computation analogous to (1.19) gives the following total number of degrees
of freedom in the three-dimensional setting:
N0 = T
π
4πr2
c2π2
 ω2
ω1
ω2dω
= T
π
4πr2
c2π2
(ω3
2 −ω3
1)
3
= T
π
4πr2
c2π2
(ω2
2 + ω2
1 + ω1ω2)
3
= T
π
4πr2ω2
c
(cπ)2 (1 + κ),
(1.79)
where the constant κ ≈0 for ωc ≫. It follows that the total number of degrees of
freedom is essentially given by
N0 = T
π
4πr2ω2
c
(cπ)2 = T
π
4πr2
(λc/2)2 ,
(1.80)
where λc = 2πc/ωc is the carrier wavelength of the radiated waveform. These results
show that information increases with the carrier frequency, which can be considered the
representative frequency of the radiated signal, or equivalently with the inverse of the
carrier wavelength.
The information gain obtained at high frequency, however, comes at a price: transmit-
ting at higher and higher frequencies while maintaining a constant signal-to-noise ratio
requires a corresponding increase of the energy per degree of freedom of the radiated
signal. To illustrate this point in more detail, we need to take a closer look at the quantum
mechanical nature of the radiation process.
1.5.3
Quantized Radiation
Consider a space–time waveform of ﬁxed angular frequency bandwidth , centered
around ωc, and radiated in the two-dimensional geometric setting described in
Section 1.2.3. This waveform has a total number of degrees of freedom in the
space–wavenumber and time–frequency domains given by (1.78), and it can be
expanded in terms of orthonormal, real basis functions as
f(φ,t) =
N0

n=1
anψn(φ,t).
(1.81)
Electromagnetic radiation occurs in discrete quanta of energy called photons. The
square of each term in the series (1.81) can be interpreted as an energy density that is
proportional to the number of photons observed at space–time point (φ,t) along the
03
11:58:56, subject to the Cambridge Core

36
Introduction
nth dimension of the signal’s space. Integration of [anψn(φ,t)]2 over a space–time
observation window gives the energy detected in a typical observation, over the
given window, and along the nth dimension of the space spanned by the given basis
representation. By orthonormality, it follows that the total amount of energy that can be
detected along the nth dimension of the signal’s space is given by
En =
 π
−π
 ∞
−∞
[anψn(φ,t)]2dtdφ = a2
n,
(1.82)
and we also have
an =
 π
−π
 ∞
−∞
f(φ,t)ψn(φ,t)dtdφ.
(1.83)
The total energy of the signal, proportional to the number of photons detected over all
dimensions of the space, is then given by
 π
−π
 ∞
−∞
f 2(φ,t)dtdφ =
 π
−π
 ∞
−∞
N0

n=1
[anψn(φ,t)]2dtdφ
+
 π
−π
 ∞
−∞
N0

n,m=1
n̸=m
anamψn(φ,t)ψm(φ,t)dtdφ
=
N0

n=1
a2
n.
(1.84)
The photons in our signal have a range of frequencies of bandwidth  centered
around ±ωc, as depicted in Figure 1.25. Their energy, measured in joules, is
proportional to their positive frequency of radiation according to Planck’s equation,
Ep = ¯hω [J],
(1.85)
where ¯h is the reduced Planck’s constant and ω > 0. For ωc ≫, the energy of each
radiated photon is essentially ¯hωc and an increase of the carrier frequency yields a
corresponding increase of the energy of all the photons composing the signal. It follows
that maintaining a constant signal-to-noise ratio at increasing frequencies of radiation
requires an increase in the energy of the signal along each dimension of the space,
since any given level of uncertainty in the number of detected photons corresponds to
wider energy ﬂuctuations in the observed signal level. Figure 1.26 illustrates this point,
showing the quantized energy levels along the nth dimension of the signal’s space for
two different values of the carrier frequency ω(1)
c
< ω(2)
c . At the lower frequency, a given
number of detected photons on any dimension corresponds to a lower energy compared
to the higher frequency level.
We now summarize. The observed waveform is the image of the sources and
scatterers through a propagation operator. This operator essentially behaves as a ﬁlter,
projecting the number of observable waveform conﬁgurations onto a lower-dimensional
space with a number of degrees of freedom that critically depends on the spatial
03
11:58:56, subject to the Cambridge Core

1.5 Energy Limits
37
extension of the cut-set boundary of the radiating system, and on the frequency
of radiation. It follows that we can increase the number of degrees of freedom by
increasing the radiated frequency. This, however, comes at the price of increasing
the radiated energy per degree of freedom, due to the coarser resolution at which
signals are observed that is imposed by the more energetic photons composing the
signal.
A natural question regarding the high-energy limit of information transmission then
arises:
Can we increase information indeﬁnitely by spending more and more energy?
It turns out that even with no constraint on the energy expenditure, nature precludes
information growing arbitrarily large. In the high-energy regime we are faced with yet
another basic physical constraint: a volume packed with high-energy conﬁgurations of
radiating elements can become gravitationally unstable, and as it collapses it reaches its
ultimate informational limit.
1.5.4
Universal Limits
Information is ultimately limited by the laws of high-energy physics, which preclude
it increasing indeﬁnitely. In order to increase the amount of information transported by
waves, we need to increase the radiated energy, and this requires packing more and more
energy at the source of the radiation. Eventually, this strategy comes to an abrupt stop,
and a limiting value for the amount of information that can be transported is reached.
This limit is well beyond the capability of any practical communication system and is
of purely theoretical interest.
ω
ω
n
n
c
c
O
O
(1)
(2)
E  n
[joules]
E  n
[joules]
Fig. 1.26
A given number of photons along the nth dimension of the signals’ space yields a higher energy
value when photons are radiated at higher frequencies.
03
11:58:56, subject to the Cambridge Core

38
Introduction
To describe this information bound we recall that according to general relativity any
three-dimensional spherical region is characterized by a critical Schwarzschild radius
rS = 2Gm
c2 ,
(1.86)
where G is Newton’s gravitational constant and m is the mass of the region. If the radius
of the region is smaller than its Schwarzschild radius it will collapse into a black hole.
The surface at the Schwazschild radius is called the event horizon and it represents the
boundary of the black hole. By Einstein’s relation
E = mc2,
(1.87)
increasing the energy inside a volume effectively increases its mass, and eventually
causes it to collapse into a black hole.
To appreciate how this phenomenon limits the amount of information inside any
volume, consider a spherical region of space of radius r and volume V, without
restrictions on its content. We discretize the space into a grid where the cell spacing
is given by the Planck length ℓp. We shall deﬁne this length precisely in Chapter 11
based on quantum indeterminacy principles; here, we simply posit it to be the smallest
length scale at which any physical object can be localized. In this ﬁrst-order calculation
we assume cubic cells and do not worry about modifying the shape of the cells near
the boundary of the region. We place one quantum state per cell. We assume that
each quantum state has two possible energy levels, and that the energy of each state
is bounded above by the Planck energy, corresponding to the largest amount of energy
that can be localized to a Planck-sized cell without producing a tiny black hole there. It
follows that the size of the state space of this system is roughly
N = 2V/ℓ3p,
(1.88)
and assuming the system is in equilibrium and all states are equally probable, the
Shannon entropy is
H = V/ℓ3
p.
(1.89)
The problem with this estimate is that most of the states are too massive to be
gravitationally stable. The gravitational limit imposed by the Planck spacing is too weak
to prevent black hole formation at larger scales. Even if we have been careful to limit
the energy in each Planck cell to avoid its collapse, it is still possible that the union of
the cells composing our volume has conﬁgurations that lead to gravitational collapse
of the whole volume. It is then natural to ask, how many states does a volume that is
gravitationally stable have?
The holographic information bound, originally proposed in 1993 by the Dutch
physicist Gerard ’t Hooft and the American physicist Leonard Susskind, states that
this number must be less than the number of states of a black hole of the same size
as our region, and this black hole has a much smaller number of states than our original
03
11:58:56, subject to the Cambridge Core

1.5 Energy Limits
39
estimate, given by
N = 2H,
(1.90)
where
H = 4πr2
4ℓ2p
loge bits
(1.91)
is the black hole’s entropy. We derive the Bekenstein–Hawking entropy formula for
black holes (1.91), named after Israeli physicist Jacob Bekenstein and British physicist
Stephen Hawking, in Chapter 11. The holographic bound then follows from the basic
observation that if the entropy of our system exceeds (1.91), then one could add mass to
it and turn it into a black hole having a lower entropy, violating the second law; or more
precisely, the generalized second law that includes black hole entropy.
Consider an isolated system of mass m and entropy H′. Let r be the radius of the
smallest sphere that surrounds the system and S its surface area. We assume that r > rS,
so that the system is gravitationally stable. The system can be converted into a black
hole by collapsing a spherical shell of mass m −mBH and entropy H′′ onto itself, where
mBH is the mass of a black hole of the same surface area S. The collapsing process
preserves spherical symmetry and ensures that the new system has radius r < rS, so that
it collapses into a black hole. The initial entropy in this thermodynamic process is
H = H′ + H′′.
(1.92)
The ﬁnal entropy is given by (1.91). Since by the discussion in Section 1.4.1 the
Shannon entropy times the Boltzmann constant kB coincides with the thermodynamic
entropy of an equipartitioned system at thermal equilibrium, by the second law we then
have
H′ + H′′ ≤4πr2
4ℓ2p
loge.
(1.93)
Since the entropy of the shell must be non-negative, the result follows.
The conclusion is that the most informational object that ﬁts in our region is a
black hole with entropy proportional to its event horizon, and any quantum mechanical
conﬁguration inside a spherical region of space of surface boundary S can be represented
by at most loge bits of information per four Planck areas on the boundary of the region
– see Figure 1.27.
Consider now a waveform observed on a spherical boundary surrounding our region,
radiated from sources all internal to the region. Viewing the given conﬁguration of
sources as a point in the states’ space deﬁned by the quantum cells producing a
corresponding point in the signals’ space, and letting the number of possible source
points be N and the number of possible signal points be M, the number of waveforms
that can be generated cannot exceed the total number of source conﬁgurations inside the
region, and we have M ≤N.
An important consequence of this limitation is that we cannot increase the number of
degrees of freedom of the waveform arbitrarily: this requires increasing the number of
03
11:58:56, subject to the Cambridge Core

40
Introduction
0
0
0
0
1
1
1
1
0
p
2
4
/log e
Fig. 1.27
Illustration of the holographic information bound. Similar to a hologram, the whole
three-dimensional information can be encoded on the volume’s surface area.
degrees of freedom of the source of radiation, and it eventually leads to its gravitational
collapse. Gravity imposes that the radiation wavelength can only be decreased up to the
Planck scale. Past this limit, the radiated waveform requires excessive energy and the
radiating system collapses into a black hole, with entropy proportional to the area of its
cut-set boundary expressed in Planck-scale units.
We now summarize. In classical ﬁeld theory, a number of possible ﬁeld conﬁgurations
is projected onto a lower-dimensional space, of dimension proportional to the cut-set
area viewed at the wavelength scale. When quantization of energy and gravity are
taken into account, the dimension can be at most proportional to the cut-set area
viewed at the Planck scale. These constraints are due to very different physical reasons:
namely, propagation ﬁltering and gravitational collapse, and occur at very different
scales.
To have an idea of the very different scales involved, according to the holographic
bound the information contained in an object of size of the order of one meter, and
so energetic as to be on the verge of gravitational collapse, would be of the order
of 1070 bits. To recover the whole information on the cut-set boundary surrounding
the object through electromagnetic radiation we would need to convert the whole
mass of the object into radiation of wavelength of the order of 10−35 meters. In
comparison, microwaves for radio communication have wavelengths of the order of
a centimeter, optical wavelengths are around 10−7 meters, and gamma rays start at a
mere 10−12 meters – see Appendix F. In nature, most high-frequency detections come
from astronomical sources, and the lowest wavelength of gamma rays that has been
detected is around 10−20 meters, a long way from our hypothetical, highly informative
ﬁeld. Holographic bounds are clearly not going to matter for any practical system.
Nevertheless, from a physical perspective they are very relevant to identify the ultimate
information-theoretic limits imposed by nature. In Chapters 11 and 13 we take a closer
look at these limitations, considering universal entropy bounds for noise signals and
bandlimited signals.
03
11:58:56, subject to the Cambridge Core

1.6 Tour d’Horizon
41
1.6
Tour d’Horizon
This chapter has provided a roadmap for the topics addressed in the remainder of the
book. A central result is the cut-set area bound on the number of degrees of freedom per
radiated frequency, leading to the total number of degrees of freedom of electromagnetic
signals, (1.19) and (1.80). A complete derivation of these results appears in Chapters 8
and 9, where they are also extended to more general cut-set surfaces with rotational
symmetry. Chapters 2, 3, 4, and 5 build all the necessary background for this
derivation and take us on a journey through the theory of functional approximation,
decomposition of operators in inﬁnite-dimensional Hilbert spaces, and electromagnetic
wave propagation. Chapter 6 provides an analogous description of signals from a
stochastic perspective, and Chapter 7 is an intermezzo that precedes the more technically
demanding content of Chapters 8 and 9; Chapter 7 describes how the degrees of freedom
and the stochastic diversity of electromagnetic waveforms are exploited in current
communication technologies. It discusses the principles behind orthogonal frequency
division, code division, time division, and multiple-antenna systems, viewing all of
these technologies through the lens of the orthogonal representations examined in the
previous chapters. It also gives an overview of the methods that have been proposed
to operate next-generation communication systems arising in a network setting. The
remaining chapters provide an additional in-depth look at some selected topics in wave
theory and at their relationship with information theory.
We now provide a brief summary of the contents of the single chapters. In Chapter 2
we introduce the communication problem, deﬁne the signals’ space, introduce Slepian’s
concentration problem, and discuss how this is related to the number of degrees of
freedom of bandlimited functions. We show that the prolate spheroidal wave functions,
solving the concentration problem and serving as the optimal representation basis for
bandlimited signals, also arise in the context of wave propagation. We also discuss
how Slepian’s problem is related to the impossibility of simultaneous localization
of signals in time and frequency, which provides the mathematical justiﬁcation for
Heisenberg’s uncertainty principle in quantum mechanics. Thus, the same mathematics
of spectral concentration at the basis of information-theoretic results is at the basis of
the observational limits of our world.
In Chapter 3 we revisit these concepts in the more rigorous setting of approximation
theory, giving the precise deﬁnition of the number of degrees of freedom through the
notion of Kolmogorov’s N-width. We compute the number of degrees of freedom of
bandlimited signals in terms of eigenvalues of a certain Fredholm integral equation that
arises from Slepian’s concentration problem, and place the problem in the more general
setting of the Hilbert–Schmidt decomposition of operators in inﬁnite-dimensional
spaces. Slepian’s problem corresponds to the decomposition of one such operator, that
can be performed explicitly. We also provide the generalization of the problem for
multi-band signals and for signals of multiple variables, presenting basic results in
analysis that are used in subsequent chapters to determine the number of degrees of
freedom of electromagnetic space–time signals. Finally, we introduce the problem of
03
11:58:56, subject to the Cambridge Core

42
Introduction
signal reconstruction from blind measurements and relate it to the analogous problem
of compressed sensing in a discrete setting.
Having developed all of this theory to determine the information associated with
bandlimited signals, we need to prepare the stage for the application to the space of
electromagnetic signals. Chapters 4 and 5 provide all the necessary ingredients for
this. Electromagnetic signals are effectively bandlimited functions that are the image
of the sources through a propagation operator. To study this bandlimitation property,
in Chapter 4 we review the basic concepts of Maxwell’s theory of electromagnetic
wave propagation and discuss the physics behind the radiation process. In Chapter 5 we
introduce Green’s propagation operator, which effectively ﬁlters the ﬁeld onto the space
of bandlimited functions. The Hilbert–Schmidt decomposition of this operator leads
to the information-theoretic optimal representation of the ﬁeld in the time–frequency
and space–wavenumber domains, and provides the number of parallel channels that
can be used for communication, corresponding to the number of degrees of freedom
of the ﬁeld. This representation can be used to multiplex information in space, using
multiple antennas, and in time–frequency domains, using appropriate modulation
techniques. We also show in Chapter 6 that an analogous representation arises when
the Green’s kernel is modeled as a stochastic process. In this case, the analog of the
Hilbert–Schmidt decomposition is the Karhunen–Loève decomposition, leading to the
notion of “richness” of stochastic variations of the Green’s function in the different
domains. These representations are applied in Chapter 7, which is devoted to the
description of communication technologies.
In Chapters 8 and 9, we establish results on the number of degrees of freedom by ﬁrst
showing that the ﬁeld radiated in an arbitrary scattering environment and ﬁltered by the
Green’s operator is an effectively bandlimited function, and then applying the theory
of spectral concentration in the time–frequency and space–wavenumber domains. We
study the spatial bandlimitation property leading to (1.15) and (1.18) in different
geometric settings, and introduce the notion of a cut-set integral, measuring the richness
of the information content of the waveform with respect to the boundary surface through
which the information must ﬂow. In Chapter 9 we extend these results by considering
the time–frequency domain in conjunction with the space–wavenumber domain, and
establishing (1.16) and (1.19) using the functional analysis machinery developed in the
previous chapters. In Chapter 10 we further discuss the stochastic diversity that arises
from the physics of propagation, using multiple scattering theory.
Having established the results on number of degrees of freedom and stochastic
diversity, we turn to the problem of communication in the presence of noise. Chapter 11
is devoted to exploring the connection between information theory and thermodynamics
in the context of developing useful noise models. We show that these mathematical
models are rooted in the quantum mechanical nature of the world. We also show that
the second law of thermodynamics provides an interpretation of the noise as a signal
of maximum uncertainty subject to imposed physical constraints. In this context, we
explore the relationship between statistical entropy and thermodynamic entropy.
Chapter 12 considers the notions of entropy, capacity, and dimensionality in the
deterministic model of Kolmogorov and in the stochastic model of Shannon, and
03
11:58:56, subject to the Cambridge Core

1.7 Summary and Further Reading
43
explores relationships between the two. The book concludes with a discussion in
Chapter 13 of universal information bounds, accounting for quantum uncertainty and
energy limits.
1.7
Summary and Further Reading
The topics of this chapter are developed in the remainder of the book, and we provide
more extensive references in the following chapters. Here we just list some key
references. Shannon’s (1948) original paper is still a “must read” for any information
theory student. Shannon (1949) provides more of an engineering point of view, and
focuses on continuous waveform signals. Classic texts in information theory are Cover
and Thomas (2006) and Gallager (1968).
Slepian’s concentration problem, along with a description of the main results on
bandlimited signals obtained with his colleagues at Bell Laboratories, are surveyed in
Slepian (1976, 1983) and Landau (1985). The monograph by Hogan and Lakey (2012)
provides some additional updated material.
The problem of best approximating a functional space over all ﬁnite-dimensional
subspaces was formulated by Kolmogorov (1936). Kolmogorov was acquainted with
Shannon’s work in the early 1950s and immediately recognized that “his mathematical
intuition is remarkably precise.” The deﬁnitions of ϵ-entropy and capacity appearing
in Kolmogorov (1956) were certainly inﬂuenced by Shannon’s work, as well as by his
long-standing interest in the approximation of functions in metric spaces from the early
1930s. A review appears in Kolmogorov and Tikhomirov (1959). Standard references
for these topics are the books by Pinkus (1985) and Lorentz (1986).
The thermodynamic deﬁnition of entropy was introduced by Clausius (1850–65).
Statistical deﬁnitions are due to Boltzmann (1896–8) for equipartitioned systems,
and Gibbs (1902) for non-equipartitioned ones. Jaynes (1965) explored connections
between thermodynamic and statistical deﬁnitions via the asymptotic equipartition
property. Cover (1994) studied the second law of thermodynamics from the point
of view of stochastic processes and information theory. Further connections between
statistical physics and information theory are discussed by Mézard and Montanari
(2009), and Merhav (2010).
The question of how much information an electromagnetic waveform can carry in
time and space was ﬁrst posed in the works of Toraldo di Francia (1955, 1969) and
Gabor (1953, 1961). The cut-set area bound on the number of spatial degrees of freedom
of time-harmonic electromagnetic ﬁelds was derived by Bucci and Franceschetti (1987,
1989), and extended by Franceschetti (2016) to frequency-bandlimited waveforms.
More details on the quantum nature of radiation can be found in many classic texts,
among which Heitler (1954) and Loudon (2000) are good representative examples. The
holographic information bound was ﬁrst proposed by ’t Hooft (1993). Susskind (1995)
gave it a string-theoretic formulation. A comprehensive review appears in Bousso
(2002).
03
11:58:56, subject to the Cambridge Core

44
Introduction
1.8
Test Your Understanding
Problems
1.1
There is no such thing as a “thin shell of small volume.” Consider the
N-dimensional box CN(1) of side length 1. Show that all the volume in high dimensions
concentrates on the surface of the box; namely, for any 0 < ϵ < 1,
lim
N→∞Vol(CN(1) −CN(1 −ϵ)) = 1.
(1.94)
1.2
Consider a shell that gets thinner as N →∞. Compute, for all a > 0,
lim
N→∞Vol(CN(1) −CN(1 −a/N)),
(1.95)
and notice that the choice of a determines the fraction of volume that is trapped inside
the shell.
1.3
The volume of the N-dimensional ball BN(r) of radius r is
Vol(BN(r)) = 2πN/2rN
(N/2)N ,
(1.96)
where (·) is Euler’s Gamma function. Verify that for r = 1 this has a maximum at
N = 5 and decreases to zero as N →∞.
1.4
Consider a box of side length two and a ball of radius one inscribed inside it. Show
that the ball contains only a negligible fraction of the whole volume in high dimensions,
namely
lim
N→∞
Vol(CN(2))
Vol(BN(1)) = ∞.
(1.97)
1.5
Consider a ball of radius one. Compute the side length of the largest box inscribed
inside it. Show that in this case the box contains only a negligible fraction of the whole
volume in high dimensions.
1.6
Are the volumes in Problems 1.4 and 1.5 growing or shrinking as N →∞?
1.7
Show that the volume of an N-dimensional ball BN(1) of radius one in high
dimensions is concentrated near the boundary; namely, for all 0 < ϵ < 1,
lim
N→∞
Vol(BN(1) −BN(1 −ϵ))
Vol(BN(1))
= 1.
(1.98)
Compare this result with the analogous one for the box.
1.8
Consider a box of side length 2R and the smallest ball containing it. Show that the
radius of the ball in N dimensions is R
√
N.
1.9
Show that
lim
N→∞Vol(BN(R
√
N)) =
 0
if R ≤1/
√
2πe
∞
if R > 1/
√
2πe.
03
11:58:56, subject to the Cambridge Core

1.8 Test Your Understanding
45
1.10
Compute a lower bound on the ϵ-capacity of bandlimited functions using the
lattice packing in Figure 1.14 and assuming that the number of degrees of freedom
is equal to N0. Show that as N0 →∞the lower bound grows as √N0 so that the
corresponding bound on the number of bits per degree of freedom carried by any
waveform in the space tends to zero.
1.11
Explain why the integration interval in (1.16) and (1.19) is [0,] rather than
[−,].
1.12
Verify that (1.78) and (1.79) reduce to (1.16) and (1.19) when the signal has a
spectral support of size 2 centered around the origin.
1.13
To compute the capacity of the additive white Gaussian noise channel, we have
considered communication with waveforms of N0 degrees of freedom, subject to the
constraint
 ∞
−∞
f 2(t)dt =
N0

n=1
a2
n ≤PN0,
(1.99)
where {an} are the coefﬁcients of an orthonormal signal representation of f over the
real line. In this way, signals correspond to points inside the high-dimensional sphere
of radius √PN0. In his original derivation, Shannon considered bandlimited signals and
used the sampling representation (1.12). Show that in this case, we have
 ∞
−∞
f 2(t)dt = π

N0

n=1
a2
n.
(1.100)
1.14
Given the result in Problem 1.13, what is the appropriate constraint that allows
us to view each signal as a point inside a sphere of radius √PN0, in Shannon’s model?
1.15
Show that among all continuous random variables deﬁned over the reals and
having ﬁnite differential entropy and variance σ 2, the differential entropy is maximized
only for Gaussians.
Solution
Let f and g be continuous probability density functions over the reals. By Jensen’s
inequality, we have

R
f log(g/f)dx ≤log

R
f(g/f)dx = log

R
g(x)dx = 0.
(1.101)
It follows that
−

R
f logfdx ≤−

R
f loggdx,
(1.102)
where equality holds if and only if g = f. We now let
g(x) =
1
√
2πσ 2 exp

−(x −μ)2/2σ 2
,
(1.103)
03
11:58:56, subject to the Cambridge Core

46
Introduction
and we get
h(f) ≤logeE[(X−μ)2]
2σ 2
+ 1
2 log(2πσ 2)
= loge σ 2
2σ 2 + 1
2 log(2πσ 2)
= 1
2 log(2πeσ 2),
(1.104)
where the right-hand side can be veriﬁed to be the differential entropy of the chosen
Gaussian distribution.
1.16
Assume the coefﬁcients that represent a signal f(t) are drawn independently from
a distribution with bounded support of size S. The coefﬁcients are quantized at level ϵ,
and the entropy of a single quantized coefﬁcient is HAϵ. The entropy of the quantized
signal is denoted by H. Show that the signal can be identiﬁed by a sequence of bits of
average length at most N0HAϵ.
Solution
We give a sketch of the proof. We divide the space of quantized signals into the set of
atypical signals A and the set of typical signals A c. By (1.46), there are roughly N = 2H
typical quantized signals. On the other hand, the number of atypical quantized signals
is N′ ≤(S/ϵ)N0. We order the quantized signals and identify them by their indexes. The
indexing of the typical signals requires at most H bits, while that of the atypical signals
requires at most N0 log(S/ϵ) bits. We consider a quantized signal q, of bit-length ℓ(q),
obtained by the realization of N0 random coefﬁcients, each quantized at level ϵ. The
expected length of the sequence of bits identifying this signal is
E(ℓ) =

q∈A c
p(q)ℓ(q) +

q∈A
p(q)ℓ(q)
≤

q∈A c
p(q)H +

q∈A
p(q)N0 log(S/ϵ)
= P(A c)H + P(A )N0 log(S/ϵ).
(1.105)
For small ϵ, from the results in Section 1.4.4 we have H ≈N0HAϵ, so that
E(ℓ) ≤N0[P(A c)HAϵ + P(A )log(S/ϵ)].
(1.106)
Finally, by choosing N0 large enough we have that P(A c) can be made arbitrarily close
to one and P(A ) arbitrarily close to zero, completing the proof sketch.
1.17
Consider N0 i.i.d. Gaussian random variables of zero mean and variance P.
Compute upper and lower bounds on the size of the typical set in terms of differential
entropy as N0 →∞, and compare them with the volume of the N0-dimensional ball of
radius √PN0, that follows from (1.72).
Solution
From (1.40) and using the differential entropy of a Gaussian random variable (1.104),
03
11:58:56, subject to the Cambridge Core

1.8 Test Your Understanding
47
we have
(1 −ϵ)(
√
2πeP)N0
2N0ϵ
≤

Vϵ(N0)
dx1dx2 ···dxN0 ≤(
√
2πeP)N02N0ϵ.
(1.107)
On the other hand, as N0 →∞the volume of the ball can be approximated by
Vol(BN0) = (
√
2πeP)N0
√N0π
,
(1.108)
which falls between the upper and lower bounds in (1.107).
03
11:58:56, subject to the Cambridge Core

2
Signals
For whatsoever is capable of sufﬁcient differences, and those perceptible to the sense, is in nature
competent to express cogitations.1
2.1
Representations
We consider a class of real functions that we call signals that are deﬁned on a real line.
Signals have important physical interpretations in terms of real, measured quantities.
For example, f(t) can represent the voltage measured at time t across two terminals of an
antenna. In what follows, we do not deal directly with units of measure, however these
units should be made explicit whenever a given physical interpretation is considered.
The instantaneous power of a signal f(t) is deﬁned by f 2(t), and its energy by
 ∞
−∞
f 2(t)dt < ∞.
(2.1)
From (2.1) it follows that the space of signals for the communication engineer coincides
with the L2(−∞,∞) space of the square-integrable functions for the mathematician.
An alternative representation of the signal f(t) is given by the Fourier transform
F(ω) =
 ∞
−∞
f(t)exp(−jωt)dt,
(2.2)
where ω is a real line called angular frequency and the engineering notation j is used to
denote the imaginary unit. F(ω) is called the spectrum of the signal. For real f(t), we
have that |F(ω)| = |F∗(−ω)|, and the absolute value of the spectrum is an even function
of the frequency. The inverse Fourier transform
f(t) = 1
2π
 ∞
−∞
F(ω)exp(jωt)dω
(2.3)
allows the modeling of the signal as the continuous superposition of different spectral
components, each at angular frequency ω. The equality in the inversion formula (2.3) is
1
F. Bacon (1605). The Advancement of Learning, reprinted 1869, Clarendon Press, p. 166.
04
12:22:31, subject to the Cambridge Core

2.2 Information Content
49
intended here in the energy sense,2
lim
→∞
 ∞
−∞
|f(t) −
 
−
F(ω)exp(jωt)dω|2dt = 0.
(2.4)
Parseval’s theorem states that the energy is conserved in the spectrum, so that the
function F(ω) is also integrable in the absolute square, namely
1
2π
 +∞
−∞
|F(ω)|2dω =
 ∞
−∞
f 2(t)dt < ∞.
(2.5)
In practical settings, the real, measured quantities that the signals represent exhibit
amplitude spectra of ﬁnite support and are observed for a ﬁnite time. For example,
electromagnetic signals used for communication are conﬁned to frequency bands that
are used for different applications (e.g., radio broadcast, cellular telephones, internet
access, etc.). The set B of signals bandlimited to  is deﬁned as the set of all signals
whose amplitude spectra vanish for |ω| > . The set TT of signals timelimited to T is
deﬁned as the set of all signals that vanish for |t| > T/2.
Sometimes, rather than signals in L2(−∞,∞) it is useful to consider periodic signals
that are in L2(a,a+T0), for arbitrary a ∈R and fundamental period T0 = 2π/ω0. These
have the Fourier series representation
f(t) =
∞

n=−∞
cn exp(jnω0t),
(2.6)
where convergence here is intended as
lim
N→∞
 a+T
a
|f(t) −
N

n=−N
cn exp(jnω0t)|2dt = 0.
(2.7)
The coefﬁcients of the series are given by
cn = 1
T0
 a+T0
a
f(t)exp(−jnω0t)dt.
(2.8)
Parseval’s theorem in this case states that the average power over a period is
conserved in the spectrum, namely
∞

n=−∞
|cn|2 = 1
T0
 a+T0
a
f 2(t)dt.
(2.9)
2.2
Information Content
The information a signal carries is its form, in the sense of Plato’s eidos. Signals with
different shapes can have different meanings, and choosing one among them implies
2 A much stronger convergence result holds. Carleson’s theorem states that the inverse Fourier transform of
square-integrable functions converges point-wise almost everywhere. Throughout the book we are only
concerned with convergence in the energy sense, which is the most relevant for physical applications.
04
12:22:31, subject to the Cambridge Core

50
Signals
that a certain amount of information is transmitted via the selection process. Using
mathematics, this concept can be made rigorous by geometrically representing a signal
as a point that can be uniquely identiﬁed by its coordinates in a high-dimensional
space. These coordinates allow us to distinguish among different signals, numerically
representing the amount of information that can be communicated by selecting one of
them.
A bandlimitation of the spectral support of the signal reﬂects into a limitation on how
diverse the corresponding waveform can be, and in turn on how many coordinates are
required to identify it. The number of coordinates that uniquely identify any bandlimited
signal undergoes a phase transition around a critical value representing the effective
dimension of the signal’s space. This means that a number of coordinates slightly below
the critical value does not sufﬁce to identify any signal, while a number slightly above
it does sufﬁce. The meaning of the word “slightly” here refers to the width of the
transition around the critical point that can be characterized precisely, and the sharp
cut-off occurring within this interval provides a fundamental limit on the amount of
information that can be communicated by transmitting an arbitrary signal in the space.
2.2.1
Bandlimited Signals
The Kotelnikov–Shannon–Whittaker interpolation formula (cardinal series, for short) is
widely used in engineering. It states that if f(t) is bandlimited to , then
f(t) =
∞

n=−∞
f(nπ/) sinc(t −nπ),
(2.10)
with the conventional notation sinct = (sint)/t.
It follows that any bandlimited signal can be identiﬁed by specifying a discrete
sequence of real numbers representing the sampled values of the signal spaced by π/.
The signal can be constructed using the interpolating functions sinc(·), each centered at
a sampled point and whose amplitude is adjusted to that of the corresponding sample.
The interpolating functions have value zero at all sampled points except the one where
they are centered.
In the context of linear ﬁlters, the interpolation can be viewed as applying an
ideal low-pass ﬁlter of cut-off frequency , whose impulse response is sinc(t), and
whose input is the ideal δ-pulse train modulated by the samples f(nπ/), as shown in
Figure 2.1. This interpretation makes the formula attractive and widely used in practical
settings. Relaxing the condition on the ideal ﬁlter cut-off and on the ideal δ-pulses leads
to a reconstruction error that can be minimized using higher sampling rates.
A mathematical derivation of the formula can be easily obtained by replicating the
spectrum F(ω) along the ω axis at every interval of size 2. This gives a periodic signal
Fs(ω) of the fundamental period 2. By applying (2.6) and (2.8) with the substitutions
t →ω, T0 →2, and ω0 →π/, we obtain
Fs(ω) =
∞

n=−∞
F(ω −2n) = π

∞

n=−∞
f(nπ/)exp[−jn(π/)ω].
(2.11)
04
12:22:31, subject to the Cambridge Core

2.2 Information Content
51
f(t)
t
IDEAL 
LOW-PASS
FILTER
t
O
O
       
f
n
8
8
s
)
(
∑
=
|F (ω)|






Fig. 2.1
Physical interpretation of the cardinal series interpolation formula.
The original signal is then given by the replicated signal times the rectangular window
of support 2, namely
F(ω) = rect
 ω
2

Fs(ω)
= rect
 ω
2
 π

∞

n=−∞
f(nπ/)exp[−jn(π/)ω]
=
∞

n=−∞
f(nπ/) π
rect
 ω
2

exp[−jn(π/)ω].
(2.12)
Finally, taking the inverse transform of (2.12) and using the Fourier transform pair

π sinc(t) ←→rect
 ω
2

,
(2.13)
the formula (2.10) follows. In this last step, we have assumed that the signal is
sufﬁciently well behaved that term by term integration is allowed.
The intuitive justiﬁcation for the cardinal series is that, if f(t) contains no angular
frequencies higher than , then it cannot change to a substantially new value in a time
less than one half-cycle of the highest frequency, corresponding to the sampling interval
π/, and all values in between samples can be perfectly reconstructed. In short, a
bandwidth limitation translates into a limitation of the diversity of the signal, namely of
the amount of variation that the signal can undergo in any given sampling interval.
2.2.2
Timelimited Signals
If f(t) is also timelimited to [−T/2,T/2], then the non-zero terms in (2.10) occur only
for |n| ≤T/(2π); the number of sinc(·) functions needed to interpolate any arbitrary
bandlimited and timelimited signal is ﬁnite, and it is given by the Nyquist number3
N0 = T/π.
(2.14)
3 We use the convention throughout that the Nyquist number is rounded to the largest integer no greater
than T/π + 1.
04
12:22:31, subject to the Cambridge Core

52
Signals
It follows that a waveform f(t), viewed in a simple environment such as the real
plane, can be replaced by a simpler entity such as a point, viewed in a complex
environment such as an N0-dimensional space. In the context of communication, this
means that using signals bandlimited to  one can transmit only N0 real numbers
in time T, and these numbers can be used to completely identify any one signal of
duration T.
This remarkable intuition, due to Shannon (1948, 1949), allows engineers to
treat continuous signals as discrete ensembles corresponding to their coordinates
in an N0-dimensional space, and is the essential idea supporting today’s digital
communication technology. As powerful as it is, the intuition is nevertheless imprecise,
and it requires a great deal of mathematical argument to make it stand on solid
ground.
2.2.3
Impossibility of Time–Frequency Limiting
The ﬁrst problem that we encounter is that the only signal that is both bandlimited
and timelimited is the trivial always-zero signal. A mathematical proof of the above
statement is the following: consider the complex extension of f(t) deﬁned in (2.3) to the
upper complex half-plane, by taking {t = x +jy : y > 0,x,y ∈R}. If F(ω) is bandlimited
to , then by the Paley–Wiener theorem stated in Appendix A.1, this extension is an
entire function of exponential type . Such a function is holomorphic over the whole
complex plane and therefore it equals its own Taylor series everywhere. Now, if f(t) = 0
in an interval, then all of its derivatives would also be zero inside this interval and the
Taylor expansion would require it to be zero everywhere. The above proof uses tools
from analysis; the solution to Problem 2.10 provides a more elementary derivation.
It follows that the perfect reconstruction formula (2.10) is only valid if one observes
bandlimited signals over an inﬁnite time, and this is not possible in practice. Signals of
ﬁnite time support cannot be rigorously identiﬁed by N0 numbers, and if one attempts
to use this ﬁnite number of samples then a certain reconstruction error must occur.
This interpolation error is due to the missing samples of the tails of the signals in the
time domain. On the other hand, a timelimited signal cannot be bandlimited and its
reconstruction from N0 samples has an aliasing error due to the overlaps of the tails
of the replicas in the spectral domain. The situation is depicted in Figure 2.2, and
creates a problem in deﬁning precisely the amount of information that is carried by
the transmitted signals.
One possible way around these issues is to argue that signals can be, if not perfectly
timelimited and bandlimited, at least approximately so. The aliasing and interpolation
errors can then be neglected, if the approximation is sufﬁciently accurate. The problem
is then to appropriately deﬁne the meaning of the term “approximately” and to relate this
deﬁnition to the quality of the reconstruction. After Shannon’s breakthrough proposal,
this objective kept communication engineers busy for a long time building rigorous
foundations for their methods.
04
12:22:31, subject to the Cambridge Core

2.2 Information Content
53
f (t)
O
T/2
-T/2
O
T/2








-T/2
O
f(t)
t
t
-2
2
O
-2
2
Fig. 2.2
It is impossible to achieve simultaneous concentration of signals in time and frequency.
Sampling reconstruction carries either an interpolation error due to the missing samples of the
tails of the signals in the time domain, or an aliasing error due to the overlap of the tails of the
replicas in the spectral domain.
2.2.4
Shannon’s Program
We ﬁrst provide a non-rigorous, but intuitively appealing, argument for how bandlimited
signals can be used for communication. In the late 1940s, this was Shannon’s program,
which sparked the digital communication revolution.
We view a set of N0 numbers as coordinates of a point in a space of N0 dimensions.
Each selection of these numbers identiﬁes a point in this space. Ignoring the problem
of the impossibility of simultaneous concentration of signals in time and frequency,
we assume for the moment that there is exactly one point corresponding to each
signal approximately bandlimited to  and with approximate duration T. We make
some comments on the geometric insights provided by such an approximate, discrete
representation.
Consider a point x = (x1,...,xN0), whose coordinates correspond to the samples of a
signal fx(t). The distance of x from the origin of the space is given by
d(x,0) =




N0

n=1
x2n.
(2.15)
By (2.10), using Parseval’s theorem and (2.13), it follows that the total energy of the
signal is given by
 ∞
−∞
f 2
x (t)dt = π

N0

n=1
x2
n.
(2.16)
Letting the average power over time T be
P = 1
T
 T/2
−T/2
f 2
x (t)dt,
(2.17)
04
12:22:31, subject to the Cambridge Core

54
Signals
we have
d(x,0) =


π
 ∞
−∞
f 2x (t)dt
=

PT
π
=
	
PN0.
(2.18)
It follows that the space of signals whose average power is at most P corresponds to the
set of points within the sphere of radius
r =
	
PN0.
(2.19)
Similarly, one can compute the distance between any two signals fy(t) and fz(t) as the
geometric distance between the corresponding points, namely
d(y,z) =




N0

n=1
|yn −zn|2 =


π
 T/2
−T/2
[fy(t) −fz(t)]2dt.
(2.20)
An additional quantity that has an important role in the space of signals is the noise
that is inevitably added to every measured signal. When noise is added to the signal,
the corresponding point is moved a certain random distance in the space, roughly
proportional to the standard deviation of the noise. This produces a small region of
uncertainty about each point in the space and provides a resolution limit at which one
can distinguish different signals. Points that are too close together have overlapping
regions of uncertainty and cannot be distinguished from one another with reasonable
accuracy. With an obvious physical analogy, a set of signals in the space is called a
constellation, an example of which is depicted in Figure 2.3.
We conclude that there are three main features that limit the amount of information
that can be communicated by transmitting signals. One is the dimension of the space,
which determines the number of coordinates that are communicated by transmitting
the signal. Another is the noise, which limits the resolution at which we can observe
each coordinate. Finally, the average power used for transmission limits the amount of
separation between signals in the space.
In a digital communication system that uses continuous waveforms for communica-
tion, a transmitter can select any one of the signals in the space, subject to the geometric
constraint (2.19), by specifying N0 coordinate points. The signal is then observed at the
receiver, with typical perturbation of the order of the standard deviation of the noise.
By keeping the possible transmitted signals sufﬁciently well separated in the signal
space, the receiver can likely correctly identify which one of them has been selected at
the transmitter, so that an amount of information proportional to N0 is communicated
at each transmission event. This communication system relies on the basic idea of
treating continuous signals as discrete ensembles corresponding to their coordinates
in an N0-dimensional space.
04
12:22:31, subject to the Cambridge Core

2.3 Heisenberg’s Uncertainty Principle
55
0
0
√PN
f (t)
z
f (t)
y
(y ,y  , ... , y )
1
2
N0
(z ,z  , ... , z )
1
2
N0
(x ,x  , ... , x )
1
2
N0
f (t)
x
r=
Fig. 2.3
Signal constellation. The uncertainty region about each signal point is depicted as a gray disc.
2.3
Heisenberg’s Uncertainty Principle
A signiﬁcant obstacle to Shannon’s program of having a discrete representation of the
signals’ space is the impossibility of achieving simultaneous concentration of signals
and their spectra. Any bandlimited signal cannot be timelimited, and so it cannot be
determined by a ﬁnite number of samples. This mathematical result is also at the basis
of Heisenberg’s uncertainty principle. Published by Werner Heisenberg (1927), the
principle states that it is impossible to determine both the position and the momentum
of an elementary particle with an arbitrary degree of accuracy. In mathematical terms,
it provides an upper bound on the amount of simultaneous concentration of a signal and
its Fourier transform.
2.3.1
The Uncertainty Principle for Signals
To formulate the principle in the context of signal theory, let us consider a normalized
signal f(t) of unit energy, so that
1
2π
 +∞
−∞
|F(ω)|2dω =
 ∞
−∞
f 2(t)dt = 1.
(2.21)
In this way, we can interpret f 2(t) and |F(ω)|2/(2π) as probability densities, and their
variances σ 2
t and σ 2
ω as measures of concentration of these densities. The uncertainty
principle states that
σ 2
t σ 2
ω ≥1
4.
(2.22)
In words, there is a uniform bound on the amount of simultaneous concentration that
signals can have in the time and frequency domains.
04
12:22:31, subject to the Cambridge Core

56
Signals
In order to show that (2.22) holds, since the value of the variance is invariant to
translation of the signal, we can assume that signals have zero mean time and zero
mean frequency. In this case, we have
σ 2
t =
 ∞
−∞
t2f 2(t)dt,
(2.23)
σ 2
ω = 1
2π
 ∞
−∞
ω2|F(ω)|2dω.
(2.24)
The derivative property of the Fourier transform, in conjunction with Parseval’s
theorem, yields
1
2π
 ∞
−∞
ω2|F(ω)|2dω = 1
2π
 ∞
−∞
|jωF(ω)|2dω =
 ∞
−∞

d
dtf(t)

2
dt.
(2.25)
Combining (2.23), (2.24), and (2.25), and using the Schwarz inequality, we have
σ 2
t σ 2
ω ≥

 ∞
−∞
tf(t) d
dtf(t)dt

2
= 1
4

 ∞
−∞
d
dt(tf 2(t))dt −
 ∞
−∞
f 2(t)dt

2
= 1
4,
(2.26)
where the last equality holds since the ﬁrst integral is zero and the second integral is one
by virtue of (2.21).
2.3.2
The Uncertainty Principle in Quantum Mechanics
We now consider the physical implications of the above result. The uncertainty principle
arises in the context of the wave–particle duality of electromagnetic waves. Heisenberg
considered the following example to show the uncertainty principle in action. Consider
illuminating an electron with a microscope. The interaction of the electron with the
illuminating light corresponds to a collision of at least one photon with the electron.
Classical optics predicts that larger wavenumbers (i.e., shorter wavelengths) allow
higher observational resolutions. However, the collision of light with the electron
disturbs the momentum of the electron. The larger the wavenumber of the illuminating
light, and thus the precision of the determined position, the larger this unknown change
in momentum can be. In Heisenberg own words:
At the instant of time when the position is determined, that is, at the instant when the photon
is scattered by the electron, the electron undergoes a discontinuous change in momentum. This
change is the greater the smaller the wavelength of the light employed, i.e., the more exact the
determination of the position. At the instant at which the position of the electron is known, its
momentum therefore can be known only up to magnitudes which correspond to that discontinuous
change; thus, the more precisely the position is determined, the less precisely the momentum is
known, and conversely. (Heisenberg 1927, pp. 174–5).
04
12:22:31, subject to the Cambridge Core

2.3 Heisenberg’s Uncertainty Principle
57
A mathematical explanation of why the above attempt to determine accurately both
the momentum and the position of the electron fails can be given as follows. A scalar
electric ﬁeld is a real function of space and time. Viewing the ﬁeld at a given location
as a time signal f(t), we have the usual Fourier transform pair
f(t) ←→F(ω),
(2.27)
and we have seen above that this signal cannot be simultaneously concentrated in time
and frequency. Analogous considerations hold if instead we consider the ﬁeld at a given
time as a signal of space. Considering one-dimensional space for simplicity, the signal
is characterized by the Fourier transform pair
g(z) ←→g(kz),
(2.28)
where z is the spatial coordinate and kz = ω/c is the wavenumber that constitutes the
spectral variable corresponding to the spatial coordinate z. It follows that the ﬁeld
cannot be simultaneously concentrated in space and in the wavenumber spectrum:
electromagnetic concentration in an arbitrarily small region of space results in a
wavenumber amplitude spectrum of large support, and vice versa.
In relativistic mechanics, the momentum p of a single photon is proportional to the
wavenumber of the associated electromagnetic ﬁeld. This can be seen by combining
Einstein’s equation for the energy,
E = mc2,
(2.29)
with the equation for the momentum,
p = mv,
(2.30)
and letting m be the relativistic mass
m =
m0
	
1 −v2/c2 ,
(2.31)
where m0 is the rest mass, v is the velocity, and c is the speed of light. By (2.30) and
(2.31), we have
p2c2 = m2
0v2c2
1 −v2/c2
= m2
0c4v2/c2
1 −v2/c2
= m2
0c4 
v2/c2 −1

1 −v2/c2
+
m2
0c4
1 −v2/c2
= −m2
0c4 + (mc2)2;
(2.32)
rearranging terms and using (2.29), we obtain the relativistic energy–momentum
equation

p2c2 + m2
0c4 = mc2 = E.
(2.33)
04
12:22:31, subject to the Cambridge Core

58
Signals
Finally, letting the rest mass of the photon tend to zero and using Planck’s formula for
the energy of a photon, we have
p = E
c = ¯hω
c = ¯hkz,
(2.34)
where ω is the angular frequency of the associated electromagnetic signal, kz is the
wavenumber, and ¯h is the reduced Planck’s constant.
We now conclude that the uncertainty on the wavenumber of the signal in the spectral
domain translates into an uncertainty on the momentum of the corresponding photon
particle. Heisenberg’s principle in quantum mechanics immediately follows:
σ 2
p σ 2
z ≥¯h2
4 .
(2.35)
The only kind of signal with a deﬁnite momentum is an inﬁnite regular periodic
oscillation over all space, which has no deﬁnite position. Conversely, the only kind
of signal with a deﬁnite position is concentrated at one point, and such a signal has an
indeﬁnite momentum. In quantum mechanics, it is impossible to describe particles with
a deﬁnite position and a deﬁnite momentum. The more precise the position, the less
precise the momentum.
2.3.3
Entropic Uncertainty Principle
Heisenberg’s uncertainty principle considers signals of unit energy and their variances
as measures of concentration. An analogous result can be derived using the differential
entropy deﬁned in (1.33) as a concentration measure. Similar to the variance, this
measures the statistical dispersion of a signal around its typical value. In this case, the
uncertainty principle provides a lower bound on the sum of the temporal and spectral
differential entropies of a unit energy signal satisfying (2.21), and we have
h(f 2) + h(|F|2/(2π)) ≥log(πe).
(2.36)
In both (2.22) and (2.36), the equality holds in the case of Gaussian signals
f 2(t) =
1
√
2πσ
exp[−t2/(2σ 2)],
(2.37)
for which
σ 2
t σ 2
ω = 1
4
(2.38)
and
h(f 2) + h(|F|2/(2π)) = log(πe).
(2.39)
2.3.4
Uncertainty Principle Over Arbitrary Measurable Sets
Another uncertainty principle can be derived considering the portion of the signal’s
energy inside a given measurable set as a measure of concentration over this set. For
04
12:22:31, subject to the Cambridge Core

2.3 Heisenberg’s Uncertainty Principle
59
example, consider signals of unit energy whose fraction of energy outside the interval
[−T/2,T/2] is at most ϵ2
T, namely
1 −
 T/2
−T/2
f 2(t)dt ≤ϵ2
T,
(2.40)
and whose fraction of energy outside the interval [−,] is at most ϵ2
, namely
1 −1
2π
 
−
|F(ω)|2dω ≤ϵ2
.
(2.41)
In this case, we have the uncertainty principle
T
π ≥[1 −(ϵT + ϵ)]2,
(2.42)
showing that a signal and its Fourier transform cannot be both arbitrarily concentrated
over intervals of size T and 2. This result also holds for arbitrary measurable sets that
are not necessarily intervals – see Appendix A.2.
2.3.5
Converse to the Uncertainty Principle
The uncertainty principle provides an upper bound on the amount of simultaneous
concentration in time and frequency of any signal in the space. One can also ask
about the converse, namely what the most concentrated signals are that satisfy (2.22),
(2.36), or (2.42). For example, we may wish to determine signals of given frequency
concentration that achieve the largest time concentration, or vice versa.
When concentration is measured in terms of variance, or entropy, then the
Gaussian achieves the largest simultaneous concentration in time and frequency –
see Problems 2.8 and 2.9. On the other hand, when concentration is measured in
terms of the fraction of the signal’s energy over given measurable sets in time and
frequency, as in (2.40) and (2.41), then the most concentrated signals are somewhat
more difﬁcult to determine. These signals, however, have the additional remarkable
property of providing an optimal orthogonal basis representation for any bandlimited
signal. The error associated with this representation drops sharply to zero when slightly
more than a critical number N0 of basis functions are used for the approximation, and N0
can be identiﬁed with the effective dimensionality of the space of bandlimited signals.
It turns out that these highly concentrated basis functions are the solutions of an
integral equation deﬁned on the sets of concentration. They can be obtained explicitly
in the case of intervals, and the critical number of functions needed to represent any
signal up to arbitrary accuracy can be determined in an asymptotic order sense.
It took communication engineers a great deal of effort to rigorously derive the
above results. For a long time, the standard “hand waving” argument to determine
the asymptotic dimensionality of the space of bandlimited signals using an orthogonal
04
12:22:31, subject to the Cambridge Core

60
Signals
basis representation relied on the somewhat simpler, but sub-optimal, cardinal series
sampling representation (2.10). The idea was to approximate bandlimited signals using
a ﬁnite number N0 = T/π of terms of the cardinal series, corresponding to sampled
signal values collected inside a time interval of size T. Then, noticing that as T →∞
a vanishing portion ϵ2
T →0 of the signal’s energy is neglected and a better and better
approximation is achieved, one may consider N0 as being the asymptotic dimensionality
of the space. In this way, any real bandlimited signal of unbounded time support can
be approximated by a ﬁnite number of samples collected inside a ﬁnite interval, and
thus appears as a point in a high-dimensional space. Since Shannon’s ﬁrst outline of
this argument in 1948, it has been the undisputed “folk theorem” of communication
engineering.
2.4
The Folk Theorem
The communication engineer wants to work with signals that are somewhat concen-
trated in both the time and the frequency domains. These are the kind of signals that
seem to be the most natural, because physical devices are characterized by a ﬁnite
frequency response, and because signals are really observed for a ﬁnite time. Signals of
this kind can be represented via the cardinal series by a discrete set of N0 sampled points,
and open the possibility of using a geometric approach to the design of communication
systems.
On the other hand, the same mathematics that is at the basis of the fundamental
physical indeterminacy laws of quantum mechanics seems to prohibit this. Signals that
appear highly concentrated in frequency must be widely dispersed in time, and vice
versa. For some time engineers ignored the issue. The dilemma was hand-waved as
being only an apparent one, more of interest to the mathematician than the practitioner.
They made the observation that as N0 = T/π →∞, the number of samples grows
and both the sampled values of the original signal and the reconstruction error of the
cardinal series become negligible. So – they argued – if the error cut-off is sufﬁciently
sharp, then N0 can still be considered the asymptotic dimension of the signals’ space.
This led to the formulation of the following folk theorem that engineers have used with
great success to design sophisticated, real communication systems:
For large T, the space of signals of approximate duration T and approximate
bandwidth  has approximate dimension N0 = T/π.
The “theorem” is far from being rigorous, and its imprecise statement is rightfully
belittled by the analyst. Nevertheless, it gives the right intuition of representing signals
as points in a space of N0 dimensions, and allows communication engineers to use
geometric arguments to design communication systems and study their performance.
This geometric intuition creates a bridge between the continuous world of signals and
Shannon’s vision of a digital world of information.
04
12:22:31, subject to the Cambridge Core

2.4 The Folk Theorem
61
2.4.1
Problems with the Folk Theorem
Let us have a closer look at the representation error of the cardinal series. We show
that using the cardinal series, the reconstruction error decays slowly and makes it
impossible to identify the desired sharp error cut-off required to deﬁne the effective
dimensionality of the signals’ space. This raises the question of whether there is a better,
more concentrated basis than the one used in the cardinal series to optimally represent
any bandlimited signal.
In the following discussion we ﬁx the bandwidth  and consider a signal of
unbounded time support. The treatment by ﬁxing the time duration and considering
unbounded bandwidth is completely analogous. We consider signals in the set E (ϵT) of
bandlimited signals subject to the energy constraint
 ∞
−∞
f 2(t)dt ≤1,
(2.43)
and with at most a fraction of ϵ2
T energy outside the interval [−T/2,T/2], namely
1 −
 T/2
−T/2 f 2(t)dt
 ∞
−∞f 2(t)dt ≤ϵ2
T.
(2.44)
As T →∞we can also let ϵT →0, as the signal resembles more and more a timelimited
one. Consider the truncation error,
eN(t) = f(t) −
N

n=−N
f(nπ/) sinc(t −nπ)
=

|n|>N
f(nπ/) sinc(t −nπ).
(2.45)
By Parseval’s theorem and (2.13) it follows that the total energy of the error is given by
 ∞
−∞
(eN(t))2dt = π


|n|>N
f 2(nπ/),
(2.46)
which tends to zero as N →∞, because
π

∞

n=−∞
f 2(nπ/) =
 ∞
−∞
f 2(t)dt < ∞.
(2.47)
The value of N required to guarantee a small error depends on the signal we wish
to approximate. Ideally, we would like to be able to represent any bandlimited signal
using roughly N0 = T/π interpolating functions, and have the energy of the error be
of the order of the energy of the signal outside the interval [−T/2,T/2], as T →∞.
In this way, all signals with negligible energy outside [−T/2,T/2] would not present
a signiﬁcant error when represented using N0 samples collected inside this interval.
Letting T →∞, the energy of the error and ϵT would tend to zero at the same rate, and
N0 could be identiﬁed with the effective dimension of the signals’ space, consistent with
our version of the folk theorem.
04
12:22:31, subject to the Cambridge Core

62
Signals
T
T
2
T
ϵ
ϵ
ϵ
1
0
Fig. 2.4
Decay of the energy of the error using N0 samples for reconstruction. When the tail of the
signal’s energy ϵ2
T is below ϵ, i.e., not detectable, the energy of the error can be within the
highlighted area, i.e., detectable.
Unfortunately, the desire above cannot be fulﬁlled. It can be shown that for any
arbitrarily large constants a and N′, there exists a concentrated signal f with ϵT small
enough such that
 ∞
−∞
(eN0+N′(t))2dt > aϵ2
T.
(2.48)
Thus, retaining N0 plus any constant number of terms of the cardinal series, we cannot
approximate every concentrated signal to a degree proportional to its unconcentrated
energy. There are signals for which, as T →∞, the energy of the approximation is
greater than an arbitrarily large multiple of the unconcentrated energy.
Another negative result regarding the approximation using the cardinal series states
that even an almost linear increase in the number of terms does not sufﬁce for
approximating f to within a constant factor of ϵ2
T. Namely, for any γ < 1, there exists
η > 0 and a signal f with ϵT small enough, such that
 ∞
−∞
(eN0+Nγ
0 (t))2dt > (1 + η)ϵ2
T.
(2.49)
On the other hand, for all f and ϵT, what holds is a weaker bound of the type
 ∞
−∞
(eN0(t))2dt ≤πϵT + ϵ2
T.
(2.50)
To see what problem this creates, assume that the accuracy of our measurement is a
small ϵ > 0. Since ϵT approaches zero as T →∞at a slower rate than ϵ2
T, we can choose
T so large that
ϵ2
T < ϵ < ϵT
(2.51)
(see Figure 2.4). Consider now a signal that is practically timelimited to T, in the sense
that our measurement cannot detect it after T seconds because its energy outside the
observation interval, which is bounded by ϵ2
T, has vanished to a level below ϵ. According
to (2.50), this signal can be reconstructed by collecting N0 samples placed inside the
04
12:22:31, subject to the Cambridge Core

2.5 Slepian’s Concentration Problem
63
detectable time window with error at most of the order of ϵT, strictly above ϵ. Thus,
an undetectable portion of the signal may lead to a detectable error. This undesirable
physical behavior is due to the signal’s energy approaching zero as T →∞at a faster
rate than the reconstruction error, something our imprecise statement of the folk theorem
did not take into account. By (2.49), using the cardinal series with N0 plus any number
of terms that grows sublinearly with N0, we cannot reconstruct every bandlimited and
almost timelimited signal to a degree proportional to the unconcentrated part ϵ2
T of its
energy. The degree of the approximation can only be bounded by the square root of the
unconcentrated energy.
This opens the question of whether a better representation than the cardinal series can
achieve an error proportional to the unconcentrated part of the signal’s energy using a
number of terms only slightly larger than N0. It turns out that this optimal representation
is obtained by solving the problem of what are the most time-concentrated, orthogonal,
signals of given frequency bandwidth. These are the most concentrated signals that
still satisfy the uncertainty principle (2.42), and can be used as the optimal basis set to
represent bandlimited signals.
2.5
Slepian’s Concentration Problem
There are problems with the statement of the folk theorem and the way the
approximation error decays. In the early days of communication theory these were
overlooked for some time. The dimension of the space of signals the communication
engineer worked with was set to approximately N0, for large T. The precise meaning
of “approximately” and “large” was cautiously swept under the carpet.
In the 1960s, David Slepian and his colleagues Henry Landau and Henry Pollak at
Bell Laboratories, moved by an effort to have rigorous engineering models that are
practically relevant, ﬁnally asked the right question, came up with a precise answer,
and provided a theory that has been reﬁned over the span of roughly two decades. In
doing so, the discrete geometrical approach of representing continuous signals as points
in a ﬁnite-dimensional space, put forth by Shannon, was placed on solid mathematical
ground.
Slepian’s concentration problem can be stated as follows. For a given T > 0, deﬁne
the concentration of a signal f(t) as
α2(T) =
 T/2
−T/2 f 2(t)dt
 ∞
−∞f 2(t)dt ,
(2.52)
namely the fraction of the signal’s energy that lies in a given time interval of width T
centered at the origin. If f(t) is timelimited to T, then the concentration has its largest
value, one. Similarly, for a given  > 0, deﬁne the concentration of the spectrum of f(t)
as
β2() =
 
− |F(ω)|2dω
 ∞
−∞|F(ω)|2dω,
(2.53)
04
12:22:31, subject to the Cambridge Core

64
Signals
namely the fraction of the signal’s energy that lies in a given frequency band of width
2 centered at the origin. If f(t) is bandlimited to , then the spectral concentration has
its largest value, one. The concentration problem is now stated:
Determine how large α(T) can be for f(t) ∈B(t), and how large β() can be for
f(t) ∈TT.
Solving this question leads to the identiﬁcation of a family of bandlimited signals
highly concentrated in the interval [−T/2,T/2] that can serve as an optimal basis to
represent all bandlimited signals. The dimension of this basis exhibits a phase transition
in the neighborhood of N0 = T/π, whose threshold window can be characterized
precisely, providing a rigorous justiﬁcation for the engineering folk theorem, and a solid
foundation for the geometric approach to the study of communication with signals.
To investigate the concentration problem, we write
α2(T) =
 T/2
−T/2 dt
1
2π
 
− F( ˜ω)exp(j ˜ωt)d ˜ω
1
2π
 
− F∗(ω)exp(−jωt)dω
1
2π
 ∞
−∞|F(ω)|2dω
=
 
− dω
 
− d ˜ω sin(ω −˜ω)T/2
π(ω −˜ω)
F( ˜ω)F∗(ω)
 
− F(ω)F∗(ω)dω
.
(2.54)
To maximize α2(T) we can solve the eigenvalue equation (see Problem 2.1)
α2(T)F(ω) =
 
−
sin T
2 (ω −˜ω)
π(ω −˜ω) F( ˜ω) d ˜ω.
(2.55)
An analogous derivation leads to
β2()f(t) =
 T/2
−T/2
sin(t −˜t)
π(t −˜t)
f(˜t) d˜t.
(2.56)
Equations (2.55) and (2.56) can be written as
 1
−1
sinc0(x −y)
π(x −y)
ϕ(y)dy = λϕ(x), |x| < 1,
(2.57)
where (2.55) is obtained by letting
y = ˜ω
, x = ω
, ϕ(y) = F(y), λ = α2(T), c0 = T
2 ,
(2.58)
and (2.56) is obtained by letting
y = ˜t 2
T , x = t 2
T , ϕ(y) = f
T
2 y

, λ = β2(), c0 = T
2 .
(2.59)
Equation (2.57) is recognized as a homogeneous Fredholm integral equation of the
second kind. The Fredholm operator is compact, and the solutions to (2.57) are a
04
12:22:31, subject to the Cambridge Core

2.5 Slepian’s Concentration Problem
65
countable set of real eigenfunctions ϕ0(x),ϕ1(x),ϕ2(x),..., orthogonal, and complete
in L2(−1,1), and the corresponding eigenvalues λ0 ≥λ1 ≥λ2 ≥··· are positive, real,
and such that limn→∞λn = 0.
It follows that any signal f(t) ∈B can have at most a λ0-fraction of its energy
inside a time interval [−T/2,T/2]. The signal that achieves such a concentration is the
one whose spectral representation is given by the eigenfunction ϕ0(x), evaluated in the
interval (−1,1), with an appropriate change of scale given by (2.58). Similarly, any
signal f(t) ∈TT can have at most a λ0-fraction of its energy inside a spectral interval
(−,), and the one that achieves such a concentration has a time representation again
given by ϕ0(x), for x ∈(−1,1), with the appropriate change of scale given by (2.59).
2.5.1
A “Lucky Accident”
The solutions {ϕn(x)} of the integral equation (2.57) can be obtained by solving the
differential equation
d
dx(1 −x2)dϕ(x)
dx
+

χ −c2
0x2
ϕ(x) = 0, |x| < 1.
(2.60)
This follows by deﬁning the differential and integral operators
Pϕ = d
dx(1 −x2)dϕ
dx −c2
0x2ϕ,
(2.61)
Qϕ =
 1
−1
sinc0(x −y)
π(x −y)
ϕ(y)dy,
(2.62)
and noticing the commutative property
QPϕ = PQϕ.
(2.63)
Since commuting operators admitting a complete set of eigenfunctions share the
same eigenfunctions, it follows that the solutions of (2.60) are also solutions of (2.57).
Slepian refers to this as a “lucky accident” that allowed him and his collaborators to ﬁnd
the solution to the concentration problem in terms of that of a well-known equation in
physics.
The solutions of (2.60) arise in the context of the wave equation, and are known as
the prolate spheroidal wave functions of the ﬁrst kind and of order zero. They are a set
of solutions of the Helmholtz equation when this is expressed in a suitable coordinate
system. The corresponding eigenvalues are positive, discrete reals χ0 ≤χ1 ≤χ2 ≤···.
When the eigenfunctions are indexed by increasing values of χ, they agree with
the notation of indexing by decreasing values of λ used above. It follows that the
prolate spheroidal wave functions are the most concentrated, orthogonal, bandlimited
functions, and enjoy many properties useful for applications in mathematical physics.
Among those, one key property is that their energy falls off sharply beyond a critical
phase transition point corresponding to values of the indexes in the neighborhood of
N0 = T/π. This leads to the notion of the asymptotic dimensionality of the space of
bandlimited signals.
04
12:22:31, subject to the Cambridge Core

66
Signals
2.5.2
Most Concentrated Functions
The eigenfunctions {ϕn(x)} of (2.57) are also well deﬁned for all x ∈R. It can be
shown that they are orthonormal in L2(−∞,∞) and complete in B1 there, as well as
orthogonal and complete in L2(−1,1), as already noted. They have exactly n zeros in
(−1,1), and they are even or odd as n is even or odd. We have
 ∞
−∞
ϕn(x)ϕm(x)dx =
 1
if n = m,
0
otherwise;
(2.64)
 1
−1
ϕn(x)ϕm(x)dx =
 λn
if n = m,
0
otherwise.
(2.65)
The notation conceals the fact that both the {ϕn} and the {λn} depend on the parameter
c0 = T/2. When necessary, we write λn = λn(c0), ϕn(x) = ϕn(c0,x). It turns out that
{λn(c0)} are continuous functions of c0, and for ﬁxed c0 they fall off rapidly with
increasing n, once n exceeds the Nyquist number N0 = 2c0/π = T/π. This is the
phase transition behavior referred to above, which allows the determination of the exact
dimension of the space of bandlimited signals spanned by the eigenfunctions {ϕn}.
Another important property is that the Fourier transform of ϕn(x) restricted to |x| < 1
has the same form as ϕn(x), except for a scale factor (see also Section 2.6.2 below),
namely
jn

λn
2πc0
ϕn(x) = 1
2π
 1
−1
ϕn(s)exp(jc0xs)ds.
(2.66)
We can now deﬁne the signals
ψn(t) =

2
T ϕn
2t
T

,
(2.67)
n(ω) = 1
jn

2π
 ϕn
 ω


,
(2.68)
and by virtue of (2.66), the corresponding direct and inverse transforms are
n(ω) =
1
√λn
n(ω)rect(ω/(2)),
(2.69)
ξn(t) =
1
√λn
ψn(t)rect(t/T).
(2.70)
It can be readily veriﬁed that
 ∞
−∞
ψn(t)ψm(t)dt =
 1
if n = m,
0
otherwise;
(2.71)
 T/2
−T/2
ψn(t)ψm(t)dt =
 λn
if n = m,
0
otherwise;
(2.72)
ψn(t) ∈B,
(2.73)
04
12:22:31, subject to the Cambridge Core

2.5 Slepian’s Concentration Problem
67
O T/2
-T/2
t
0
(a)
(b)


ψ
ξ


0
0
O T/2
-T/2
t
0
O
O
Fig. 2.5
Illustration of the minimum energy spill. In (a), ψ0(t) contains a λ0-fraction of its energy in the
interval [−T/2,T/2]. Its Fourier transform is 0(ω) ∈B. In (b), 0(ω) contains a λ0-fraction
of its energy in the interval (−,). Its inverse Fourier transform is ξ0(t) ∈TT.
and
1
2π
 ∞
−∞
n(ω)∗
m(ω)dω =
 1
if n = m,
0
otherwise;
(2.74)
1
2π
 
−
n(ω)∗
m(ω)dω =
 λn
if n = m,
0
otherwise;
(2.75)
n(ω) ∈TT.
(2.76)
The signals ψn(t) ∈B viewed in the spectral domain have their whole energy inside
the interval (−,), while viewed in the time domain they have only a λn-fraction of
their energy concentrated in the time interval [−T/2,T/2]. Their timelimited versions
ξn(t) have the opposite appearance: their whole energy is concentrated in the time
interval [−T/2,T/2], while only a λn-fraction of their energy lies inside the interval
(−,). Notice that ξn(t) is obtained by timelimiting ψn(t) and renormalizing by
1/√λn to have unit energy. The effect of the timelimitation operation thus appears as
producing a corresponding energy spill in the spectrum. For ψ0(t) the timelimitation
operation causes the minimum energy spill in the spectrum, as shown in Figure 2.5(a).
A similar behavior is observed regarding the energy spill in time of ψn(t) caused by the
bandlimitation operation of n(ω), as shown in Figure 2.5(b).
We conclude that ψ0(t) is the most concentrated signal in [−T/2,T/2] among all
signals in B, and its concentration is α2(T) = λ0. Among signals in B that are
orthogonal to ψ0(t), ψ1(t) is the most concentrated; for it, α2(T) = λ1. In general, ψn(t),
whose concentration is λn, is the most concentrated signal in B that is orthogonal
to ψ0,ψ1,...,ψn−1. The same holds for signals n(ω) ∈TT: the most concentrated is
0(ω), having concentration β2() = λ0, followed by 1(ω), and so on.
04
12:22:31, subject to the Cambridge Core

68
Signals
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
2
2
T=0
T=1
T=2
T=4

β
α



Fig. 2.6
Permissible regions of concentration lie below the curves parametrized by T. As T →∞,
higher concentrations are allowed.
The answer to the concentration problem is now stated:
If f(t) ∈B then α(T) ≤
	
λ0;
(2.77)
if f(t) ∈TT then β() ≤
	
λ0.
(2.78)
The permissible region of concentration can also be computed in greater generality
as a function of α and β, and is depicted in Figure 2.6 for different values of T. The
excluded region of the square 0 ≤α2 ≤1, 0 ≤β2 ≤1 lies above the ellipse
cos−1 α + cos−1 β = cos−1 	
λ0.
(2.79)
Notice that as T →∞, higher concentrations are allowed.
2.5.3
Geometric View of Concentration
An alternative view of Slepian’s concentration problem can be given in a geometric set-
ting. In this context, Heisenberg’s uncertainty principle corresponds to the impossibility
of having an arbitrarily small angle between the subspaces corresponding to timelimited
and bandlimited signals.
04
12:22:31, subject to the Cambridge Core

2.5 Slepian’s Concentration Problem
69
We begin by noticing that complex functions in L2(−∞,∞) form a Hilbert space
with inner product
⟨f,g⟩=
 ∞
−∞
f(t)g∗(t)dt
(2.80)
and squared norm given by the energy,
∥f∥2 = ⟨f,f⟩.
(2.81)
The signals in TT and in B form two linear subspaces of this Hilbert space. The
observation made previously that a bandlimited function that vanishes for |t| > T/2
must vanish everywhere can be written as
TT ∩B = ∅,
(2.82)
namely the two subspaces have no function in common except the all-zero function.
The inner product allows us to deﬁne the angle between two functions f and g. First,
notice that
−1 ≤ℜ⟨f,g⟩
∥f∥· ∥g∥≤1,
(2.83)
since
|ℜ⟨f,g⟩| ≤|⟨f,g⟩|,
(2.84)
and by the Schwarz inequality,
|⟨f,g⟩| ≤∥f∥· ∥g∥.
(2.85)
By (2.83), we can deﬁne the angle
θ(f,g) = cos−1 ℜ⟨f,g⟩
∥f∥· ∥g∥,
(2.86)
where the extreme values 0 and π for the angle θ(f,g) can be reached only if f and
g are proportional (so that equality holds in the Schwarz inequality) and ⟨f,g⟩is real.
Now, deﬁne the projection operators that assign to every function its projection onto the
subspace of timelimited and bandlimited functions:
T f =
 f
if |t| ≤T/2,
0
if |t| > T/2;
(2.87)
Bf = F−1 
BFf,
(2.88)
where F indicates the Fourier transform, and

BF =
 F
if |ω| ≤,
0
if |ω| > .
(2.89)
04
12:22:31, subject to the Cambridge Core

70
Signals
f
f
-1
 f
cos
-1
cos α
Fig. 2.7
Function f and its projection angles onto the timelimited and bandlimited subspaces. Ff
indicates the Fourier transform of f.
Recalling (2.52), (2.53), and Parseval’s theorem, we have
θ(f(t),T f(t)) = cos−1 ∥T f(t)∥
∥f(t)∥= cos−1 α,
(2.90)
θ(f(t),Bf(t)) = cos−1 ∥
BF(ω)∥
∥F(ω)∥= cos−1 β.
(2.91)
See Figure 2.7 for a representation.
The concentration result can now be viewed in terms of the existence of a least angle
between the two subspaces B and TT, so that a bandlimited function and a timelimited
one cannot be very close together. If f(t) ∈B then β = 1, and by (2.77) this minimum
angle equals cos−1 √λ0 and is achieved by ϕ0(2t/T) ∈B. On the other hand, if f(t) ∈
TT then α = 1 and, by (2.78), the minimum angle cos−1 √λ0 is achieved by ϕ0(ω/) ∈
TT – see Figure 2.8. Finally, the requirement of being below the ellipses in Figure 2.6,
namely
cos−1 α + cos−1 β ≥cos−1 	
λ0,
(2.92)
corresponds to the sum of the projection angles onto the timelimited and bandlimited
subspaces being at least cos−1 √λ0.
2.6
Spheroidal Wave Functions
Many of the properties of the eigenfunctions and eigenvalues in (2.57) described in the
previous section can be obtained through their interpretation as prolate spheroidal wave
functions that are extensively studied in physics. These functions are used to describe
04
12:22:31, subject to the Cambridge Core

2.6 Spheroidal Wave Functions
71
0
0
 0
-1
0
0
√
ϕ
ϕ
ϕ
ϕ
cos
 f
f
Fig. 2.8
The minimum angle between subspaces is achieved by the most concentrated function ϕ0.
the spatial component of the wave propagation process, and we describe them in this
context next.
2.6.1
The Wave Equation
The wave equation is a hyperbolic partial differential equation that describes the
propagation of signals through a medium. It arises in a number of ﬁelds, including
acoustics, electromagnetism, and ﬂuid dynamics. In vector form, the homogeneous
version is

∇2 −1
v2
∂
∂t2

u(r,t) = 0,
(2.93)
where u(r,t) is a space–time ﬁeld, r = (x,y,z) is the spatial coordinate, ∇2 is the spatial
Laplacian, and v is the velocity of the wave. Here, we show an easy derivation of
the scalar one-dimensional case, u(r,t) = u(x,t), obtained by considering the waves
formed by a stretched elastic string. The analogous derivation for the vector case in
three-dimensional space is given in Chapter 4, by considering the waves describing the
propagation of electromagnetic signals.
With reference to Figure 2.9, consider the string to be initially horizontal. At time
t = 0, we displace it in the vertical direction and then we release it, so that it starts
oscillating. We want to write the equation that describes the vertical displacement u(x,t)
as a function of space x and time t. We assume that the density of the string ρ is constant
and its displacement is small, so that the mass of a length ℓof the string is ρℓ≈ρx.
Consider two points spaced by ℓalong the string. We write the tension forces there as
F1 and F2 respectively. Since there is no horizontal motion, we have
F1 cosθ1 = F2 cosθ2 = F.
(2.94)
04
12:22:31, subject to the Cambridge Core

72
Signals
u
2
F2
1
F1
O
x
x
Fig. 2.9
Example of the one-dimensional, scalar wave equation: the vibrating string.
For the vertical components, we have
ρx∂2u(x,t)
∂t2
= F2 sinθ2 −F1 sinθ1.
(2.95)
Dividing by Fx and using (2.94), we obtain
ρ
F
∂2u(x,t)
∂t2
= 1
x(tanθ2 −tanθ1)
= 1
x
∂u(x + x,t)
∂x
−∂u(x,t)
∂x

,
(2.96)
and letting x →0 we ﬁnally obtain
1
v2
∂2u(x,t)
∂t2
= ∂2u(x,t)
∂x2
,
(2.97)
which is the one-dimensional, scalar version of (2.93), where v2 = F/ρ is the square of
the propagation velocity of the vertical displacement of the string along the horizontal
axis.
2.6.2
The Helmholtz Equation
We now look for a solution to the wave equation in the case of a scalar space–time ﬁeld
in three-dimensional space, u(r,t), r = (x,y,z). Assuming u(r,t) is separable, we write
u(r,t) = γ (r)υ(t).
(2.98)
Substituting (2.98) into (2.93), we obtain
∇2γ
γ
=
1
v2υ
d2υ
dt2 .
(2.99)
Since the expression on the left-hand side depends only on r, whereas the right-hand
side depends only on t, both sides of the equation must be equal to a constant value.
04
12:22:31, subject to the Cambridge Core

2.6 Spheroidal Wave Functions
73
Hence, we obtain the two coupled equations
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∇2γ
γ
= −a2,
1
v2υ
d2υ
dt2 = −a2.
(2.100a)
(2.100b)
We rewrite the time-independent component (2.100a) as
(∇2 + a2)γ (r) = 0,
(2.101)
which is an elliptic partial differential equation called the scalar Helmholtz equation.
This can also be solved by separation of variables, in a suitable coordinate system.
Two of the eleven coordinate systems in which the scalar Helmholtz equation (2.101)
is separable are the prolate and oblate spheroidal coordinates.4 These are formed
by rotating the two-dimensional elliptic coordinate system, consisting of ellipses and
hyperbolas, about the major and minor axes of the ellipses, respectively. Due to
the geometrical symmetry, solutions obtained in the prolate and those in the oblate
spheroidal system are related by simple transformations. We focus on solutions in the
prolate case. The prolate coordinate system with interfocal distance d is depicted in
Figure 2.10. The orthogonal curvilinear coordinates are (η,ξ,θ), with
x = dξη,
(2.102)
y = d
	
(ξ 2 −1)(1 −η2)cosθ,
(2.103)
z = d
	
(ξ 2 −1)(1 −η2)sinθ,
(2.104)
|ξ| > 1 and |η| < 1.
(2.105)
In the limit of d →0 the system reduces to the spherical coordinate system. For d ﬁnite,
the surface of constant ξ becomes spherical as ξ →∞.
Writing (2.101) in prolate spheroidal coordinates, and letting c0 = ad/2, the separated
solutions are
"γn,m(η,ξ,θ) = Sm,n(c0,η)Rm,n(c0,ξ)cos(mθ),
γn,m(η,ξ,θ) = Sm,n(c0,η)Rm,n(c0,ξ)sin(mθ),
(2.106a)
(2.106b)
where {Sm,n(c0,η)} and {Rm,n(c0,ξ)} are called angular prolate spheroidal functions of
order m and degree n, and radial prolate spheroidal functions of order m and degree n,
respectively.
4 The complete set is composed of: Cartesian, confocal ellipsoidal, confocal paraboloidal, conical,
cylindrical, elliptic cylindrical, paraboloidal, parabolic cylindrical, prolate spheroidal, oblate spheroidal,
and spherical coordinates. Laplace’s equation is obtained for a = 0 and is separable in the two additional
bispherical coordinates and toroidal coordinates.
04
12:22:31, subject to the Cambridge Core

74
Signals
d
z
η
η
η
η
η
η
η
η
x
-1
ξ
ξ
ξ
ξ
=1.18
ξ=1.34
ξ=1.54
ξ=1.81
Fig. 2.10
The prolate spheroidal coordinate system. The coordinates η and ξ are depicted. The third
coordinate θ corresponds to the rotation angle around the z axis, starting from the plane y = 0
and moving towards the oriented y axis.
Both angular and radial functions are solutions of the Sturm–Liouville eigenvalue
problem
d
dx(1 −x2)df(c0,x)
dx
+

χm,n −c2
0x2 −
m2
1 −x2

f(c0,x) = 0.
(2.107)
However, the range of the variable is different: for the radial wave function, |x| > 1; for
the angular wave function, |x| < 1. Solutions that are ﬁnite at x = ±1 are called prolate
spheroidal functions of the ﬁrst kind. We focus on solutions of the ﬁrst kind and of order
zero. In this case, the ordinary differential equation (2.107) simpliﬁes to
d
dx(1 −x2)df(c0,x)
dx
+

χ0,n −c2
0x2
f(c0,x) = 0,
(2.108)
and a ﬁrst set of solutions obtained for |x| < 1 are the eigenfunctions {S0,n(c0,x)}, each
corresponding to a real positive eigenvalue in the set χ0,0(c0) ≤χ0,1(c0) ≤χ0,2(c0) ≤···.
04
12:22:31, subject to the Cambridge Core

2.7 Series Representations
75
Each eigenfunction is real for real x, and can be extended to be an entire function on the
whole complex plane. The eigenfunctions are orthogonal and complete in L2(−1,1).
S0,n(c0,t) has exactly n zeros in (−1,1), and is even or odd as n is even or odd. The
eigenvalues are continuous functions of c0 and χ0,n(0) = n(n + 1), n = 0,1,2,...
A second set of solutions, obtained for |x| > 1, are the radial functions, which differ
from the angular functions only by a real scale factor, namely
S0,n(c0,x) = κn(c0)R0,n(c0,x).
(2.109)
The functions {S0,n(c0,x)} and {R0,n(c0,x)} have a number of important properties that
can be deduced from (2.108). For c0 = 0, we have S0,n(0,x) = Pn(x), the nth Legendre
polynomial.
There are several expansions, approximations, and integral relations for the prolate
spheroidal wave functions in the literature. Among these, we have
2c0
π R2
0,n(c0,1)S0,n(c0,x) =
 1
−1
sin c0(x −s)
π(x −s)
S0,n(c0,s)ds.
(2.110)
From this, we obtain the integral equation (2.57) studied in the previous section, letting
λn(c0) = 2c0
π R2
0,n(c0,1),
(2.111)
and using the normalization
ϕn(c0,x) =

λn(c0)
 1
−1 S2
0,n(c0,x)dx
S0,n(c0,x),
(2.112)
where the notation of indexing the eigenfunctions {S0,n} by increasing values of the
eigenvalues {χn} agrees with the indexing of the eigenfunctions {ψn} by decreasing
values of the eigenvalues {λn}. Another useful integral relation is
2jnR0,n(c0,1)S0,n(c0,x) =
 1
−1
exp(jc0xs)S0,n(c0,s)ds,
(2.113)
which leads to (2.66) and the Fourier transform pairs studied thereafter. Figure 2.11
puts (2.113) into action, as it shows the energy spill of S0,0(1,s) when it is restricted
to the interval |s| < 1 and inverse Fourier transformed, obtaining the scaled signal
R0,0(1,1)S0,0(1,x)/π. The result is analogous to that in Figure 2.5.
2.7
Series Representations
The orthogonality and concentration properties of the prolate spheroidal functions make
them a natural candidate for the reconstruction of bandlimited signals observed over a
ﬁnite domain. The possibility of extrapolating the whole signal from a local observation
should not be surprising. Since f ∈B is an entire function, a familiar representation
of f is in terms of its Taylor series, which must converge at any point. In principle,
one could extrapolate the whole function by calculating successive derivatives at a
single point. In some sense, the bandlimitation property makes the signal so smooth
04
12:22:31, subject to the Cambridge Core

76
Signals
  R   (1,1)
0,0
0,0
0,0
S (1,s)
s,x
1
–1
Fig. 2.11
The inverse transform of S0,0(1,s) restricted to |s| < 1 has the same form as S0,0(1,s), except for
a scale factor. Both representations are shown on the same plot for comparison.
that knowing it with arbitrary precision at some point allows us to predict its behavior
everywhere. In practice, however, the resulting ﬁnite-degree polynomial of a truncated
Taylor series would give a poor approximation to the function sufﬁciently far from the
point of expansion, no matter how large the number of terms retained.
The cardinal series formula provides a ﬁrst alternative based on an orthogonal
representation. Although not requiring knowledge of derivatives at any given point, it
does require observing the signal at different sampling points over the whole real line.
It then interpolates the signal using sinc(·) functions placed at those sampling points.
Truncating the formula to a sufﬁciently large number of terms leads to a reconstruction
error that remains small for all t. However, we have seen in Section 2.4.1 that the error
decay is of the order of the square root of the unconcentrated part of the signal’s energy.
This makes the formula attractive mainly for the simplicity of the practical realization
through sampling and low-pass ﬁltering.
The prolate spheroidal functions provide a second, more attractive, alternative.
Thanks to their double orthogonality property over both (−∞,∞) and [−T/2,T/2],
they combine the advantages of the Taylor and the cardinal series. On the one hand, it is
possible to extrapolate the signal everywhere using only local knowledge of the signal
over [−T/2,T/2]. On the other hand, taking a sufﬁciently large number of interpolating
functions, the reconstruction error remains small for all t. In addition to these two
properties, the reconstruction is also optimal in a precise energy sense.
2.7.1
Prolate Spheroidal Orthogonal Representation
Let f(t) ∈B; we wish to approximate f using the orthonormal prolate spheroidal basis
functions {ψn}. We consider the N-dimensional approximation
fN(t) =
N−1

n=0
anψn(t),
(2.114)
04
12:22:31, subject to the Cambridge Core

2.7 Series Representations
77
where, since the functions {ψn} are real, the coefﬁcients are given by
an =
 ∞
−∞
f(t)ψn(t)dt.
(2.115)
It is easy to show that this choice of coefﬁcients, which is valid for any real orthonormal
basis, minimizes the energy of the error associated with the approximation. This follows
by letting
gN =
N−1

n=0
bnψn(t),
(2.116)
computing the energy of the approximation error for gN using the orthonormality (2.71)
of ψn in (−∞,∞),
 ∞
−∞
(f(t) −gN(t))2dt =
 ∞
−∞
(f(t) −
N−1

n=0
bnψn(t))2dt
=
 ∞
−∞
f 2(t)dt +
N−1

n=0
b2
n −2
N−1

n=0
bn
 ∞
−∞
f(t)ψn(t)dt
=
 ∞
−∞
f 2(t)dt +
N−1

n=0
b2
n −2
N−1

n=0
anbn
=
 ∞
−∞
f 2(t)dt −
N−1

n=0
a2
n +
N−1

n=0
(an −bn)2,
(2.117)
and noticing that (2.117) is minimum when its last term is zero, namely when
bn = an, 0 ≤n ≤N −1.
(2.118)
It follows that the minimum energy of the error is
 ∞
−∞
(eN(t))2dt =
 ∞
−∞
(f(t) −fN(t))2dt
=
 ∞
−∞
f 2(t)dt −
N−1

n=0
a2
n.
(2.119)
Since the energy of the error is non-negative for all N, it follows that
N−1

n=0
a2
n ≤
 ∞
−∞
f 2(t)dt,
(2.120)
and since N is arbitrary and the right-hand side does not depend on N, we also have
∞

n=0
a2
n ≤
 ∞
−∞
f 2(t)dt < ∞,
(2.121)
that is, Bessel’s inequality.
04
12:22:31, subject to the Cambridge Core

78
Signals
At this point, it is not yet clear whether (2.119) converges to zero or not. Indeed, the
Riesz–Fischer theorem ensures that
lim
N→∞
 ∞
−∞
(eN(t))2dt = 0,
(2.122)
so that f(t) can be represented in the L2(−∞,∞) sense by the series
f(t) =
∞

n=0
anψn(t).
(2.123)
It follows that in this case Bessel’s inequality (2.121) assumes Parseval’s form,
∞

n=0
a2
n =
 ∞
−∞
f 2(t)dt.
(2.124)
Substituting (2.124) into (2.119), we have
 ∞
−∞
(eN(t))2dt =
∞

n=N
a2
n,
(2.125)
which can be made as small as desired by taking N large, by virtue of (2.121).
By analogous reasoning, using the orthogonality (2.72) of the {ψn} in [−T/2,T/2],
we have
an = 1
λn
 T/2
−T/2
f(t)ψn(t)dt,
(2.126)
and the quality of the ﬁt of fN to f in the interval [−T/2,T/2] is
 T/2
−T/2
(eN(t))2dt =
∞

n=N
a2
nλn.
(2.127)
Notice that since λn →0, it is possible that (2.127) is small for values of N for which
(2.125) is still large. Perhaps not surprisingly, the goodness of ﬁt inside the interval is
not an indication of the goodness of ﬁt elsewhere.
2.7.2
Other Orthogonal Representations
The prolate spheroidal representation is only one example of the decomposition of a
normed space over an orthogonal basis set. Another example commonly encountered
is the Fourier series (2.6), where the basis functions are the complex exponentials
{exp(jnω0t)} whose coefﬁcients are given by (2.8), and Parseval’s theorem is given by
(2.9). This representation can be used to represent any square-integrable signal over the
ﬁnite interval [−T0/2,T0/2], where T0 = 2π/ω0. A third, also common, example is the
cardinal series representation (2.10) that uses the basis functions {sinc(t −nπ)} that
are orthogonal over the whole real line, complete over the class of square-integrable
signals that are bandlimited to the interval [−,], and whose coefﬁcients are the
sampled signal’s values taken every interval of length π/. Thus, they can be used to
04
12:22:31, subject to the Cambridge Core

2.7 Series Representations
79
represent any bandlimited signal over the whole real line. More examples of orthogonal
decompositions are given in Appendix A.3.
Among the different representations, the prolate spheroidal one is optimal in a precise
approximation-theoretic sense. It provides the minimum energy error for all functions
in the space, among all possible basis representations. This result is described next,
and leads to the notion of information associated with the waveform in terms of the
asymptotic dimensionality of the optimal approximating subspace.
2.7.3
Minimum Energy Error
Let E (ϵT) be the set of signals f(t) ∈B subject to the energy constraint (2.43) and
concentration constraint (2.44). The best basis {un} that, for any given N, achieves the
minimum energy error for all signals in the space,
inf
{un}N−1
0
sup
f∈E (ϵT)
inf
{an}N−1
0
 ∞
−∞

f(t) −
N−1

n=0
anun(t)
2
dt,
(2.128)
is composed of the N linearly independent, most concentrated, bandlimited functions,
which are the prolate spheroidal ones {ψn(t)}. In this case, we have
 ∞
−∞
(eN0(t))2dt = O(ϵ2
T) as ϵT →0,
(2.129)
namely, the N0 best basis functions sufﬁce to approximate any concentrated signal to
a degree proportional to the “unconcentrated” part ϵ2
T of its energy. This main result,
which we shall prove in the next chapter, should be contrasted with (2.50), mentioned
earlier for the cardinal series.
We can then ask: what does it take to bring the proportionality constant in the order
notation in (2.129) arbitrarily close to one? It turns out that for any ϵ > 0, a number
of functions N = T/π + N′ for any constant N′ cannot sufﬁce for approximating f to
within (1 + ϵ)ϵ2
T, but a logarithmically growing extra number of terms does. Namely,
there is a constant C(ϵ) such that, for N = N0 + ClogN0 and T large enough, we have
 ∞
−∞
(eN(t))2dt ≤(1 + ϵ)ϵ2
T.
(2.130)
The two results in (2.129) and (2.130), the latter attributed by Landau and Pollak
(1962) to Shannon,5 are very much related to the width of the n-interval in which the
eigenvalues {λn} fall from one to zero. We examine this transition next, and return to the
proof of (2.129) and (2.130) in the next chapter.
Figure 2.12 shows the behavior of the eigenvalues for different values of N0 = T/π.
It appears that for n ≪N0 most of the eigenvalues are close to one, while for n ≫N0
5 In a conversation with the author, Landau remarked that the problem of determining the precise width of
the transition window of the eigenvalues was posed to him by Shannon. Indeed, Landau and
Pollak (1962) attribute Theorem 4 of their paper to Shannon. Landau solved the problem completely,
together with Widom, in 1980.
04
12:22:31, subject to the Cambridge Core

80
Signals
O
10
20
30
n
1
n
N  = 5
0
O
10
20
30
1
n
N  = 10
0
n
n
O
10
20
30
n
1
N  = 25
0
Fig. 2.12
Behavior of the eigenvalues for different values of N0.
most of them are nearly zero. When n ≈N0, λn ≈1/2. As N0 gets large, the width of
the n-interval of the transition increases as logN0.
The exact width of the transition can be computed. It was ﬁrst established
non-rigorously by Slepian (1965), and later rigorously by Landau and Widom (1980).
The result is as follows:
lim
N0→∞λn(N0) =
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
0
for n = ⌊(1 + ν)N0⌋,
(1 + exp(kπ2))−1
for n = ⌊N0 + klog(N0π/2)⌋,
1
for n = ⌊(1 −ν)N0⌋,
(2.131)
where ν is an arbitrarily small positive constant, and we have used the notation ⌊·⌋to
indicate the largest integer less than or equal to the argument inside the brackets.
Pictorially, the phase transition represented by (2.131) can be better viewed at the
scale of N0. When the eigenvalues are plotted on an index scale normalized by N0,
Figure 2.13 shows that the transition becomes sharper as N0 →∞and tends to become
a step function. From (2.131), it follows that for all 0 < ϵ < 1, choosing
n = N0 + 1
π2 log
1 −ϵ
ϵ

log(N0π/2) + o(logN0)
= N0 + O(logN0) as N0 →∞,
(2.132)
04
12:22:31, subject to the Cambridge Core

2.8 Summary and Further Reading
81
N0
log N0
n
N0
n
1
1/2
ϵ
(
(
1
Fig. 2.13
Phase transition of the eigenvalues. Viewed at the scale of N0, the transition occurs within an
arbitrarily small interval of 2ν. The transition becomes sharper as N0 →∞, and tends to become
a step function.
we have λn(N0) < ϵ. This result suggests that as T →∞, the number of eigenfunctions
that are useful to interpolate bandlimited signals within ϵ accuracy is only slightly larger
than N0, because all the remaining eigenfunctions have negligible energy inside the
interval [−T/2,T/2] and thus they provide, as T →∞, a negligible contribution to the
reconstruction. We are then led to deﬁne the asymptotic dimension of the signals’ space,
as T →∞, to be the number in (2.132). The dimension of the signals’ space deﬁned in
this way depends only logarithmically on the accuracy of the approximation ϵ, and only
in its second-order term, which is a very desirable property in practice.
In view of the above arguments, Shannon’s program of having a geometric theory
of signals, based on ﬁnite ensembles of discrete numbers and supported by a rigorous
version of the folk theorem, seems now to be within reach. To complete the picture,
however, we need to make the above considerations rigorous, and provide the
connection between the behavior of the eigenvalues (2.131) and the approximation error
bounds (2.129) and (2.130). This can be done in the context of the Kolmogorv N-width,
a topic discussed in the next chapter.
2.8
Summary and Further Reading
The leitmotiv of this chapter has been that bandlimited signals used for communication
can be represented as points in high-dimensional space. This follows from the cardinal
series sampling representation of signals that was introduced by Shannon (1948,
1949) in information theory, and by Kotelnikov (1933) in the Russian literature. In
mathematics, it is known as the cardinal series and goes as far back as to Whittaker
(1915), or even earlier to Borel (1897). An excellent historical account on the origin
04
12:22:31, subject to the Cambridge Core

82
Signals
of the cardinal series is given in Higgins’ (1985) paper. Extensive reviews on sampling
theory are in the papers by Jerry (1977), Unser (2000), and Garcia (2000).
Shannon’s (1949) landmark paper contains many of the fundamental ideas regarding
the signals’ space used for communication. It was perhaps written earlier than his most
celebrated work, which appeared in 1948, adopting more of an engineering point of
view than a mathematical one, and focusing on continuous-time waveforms. The exact
time placement of Shannon’s (1949) paper has often been a source of debate in the
history of science. Since the original publication states a submission date of 1940, some
argued that it was written in 1940 but remained classiﬁed until after the end of World
War II. Shannon, however, stated in an interview that the 1940 date must have been a
typographical error. Nevertheless, he cites this work in his earlier 1948 paper, leading
to the belief that its contents were probably in circulation in the United States by 1948.
Shannon’s work posed the fundamental problem of determining the interface between
the continuous world of signals and the discrete world of information, namely how long
a discrete sequence of real numbers must be to represent continuous signals. On the one
hand, some non-rigorous arguments suggest that this should be close to T/π, where 
is the bandwidth of the signal and T its time duration. We called this critical length the
Nyquist number after Nyquist (1928), who pointed out the fundamental importance of
the time interval π/ in connection with telegraphy. On the other hand, signals cannot
be simultaneously timelimited and bandlimited, and soon one realizes that asymptotics
must come into the picture.
In an analogy with physics, the indeterminacy principle of Heisenberg (1927)
prevents signals being simultaneously concentrated in time and frequency, and leads
to what we have called Slepian’s concentration problem. This has been investigated
for about twenty years and the main reference is a sequence of ﬁve papers by Slepian
and Pollak (1961), Landau and Pollak (1961, 1962), and Slepian (1964, 1978). Two
survey papers by Slepian (1976, 1983) and one by Landau (1985) also outline some
of the main ideas. Details on the asymptotic analysis of the eigenvalues can be found
in an additional paper by Slepian (1965) and Landau and Widom (1980). We also
point out the monograph by Flammer (1957), which contains many details on prolate
spheroidal functions, and the one by Hogan and Lakey (2012) containing some updated
material. Our treatment of the problem loosely followed the references above, putting
more emphasis on the physical insights.
The cardinal series provides a suboptimal representation of bandlimited signals
in terms of discrete coefﬁcients achieving a given reconstruction error that are the
sampled values of the signal. An optimal representation is obtained by interpolating
the prolate spheroidal functions that are solutions of Slepian’s concentration problem.
The coefﬁcients of this orthogonal representation, however, are not the sampled signal’s
values. Alternative sampling representations with good truncation errors have been
proposed in the literature, including the approximate prolate spheroidal sampling
functions introduced by Knab (1979, 1983).
Rigorous derivations of Heisenberg’s uncertainty principle are due to Kennard (1927)
and Weyl (1928). The entropic uncertainty principle was proved by Beckner (1975),
following a conjecture by Hirschman (1957), while the case of concentration over
04
12:22:31, subject to the Cambridge Core

2.9 Test Your Understanding
83
arbitrary measurable sets was proved by Donoho and Stark (1989). We provide their
derivation in Appendix A.2. A mathematical survey of the uncertainty principle appears
in Folland and Sitaram (1997). The concentration properties of Gaussian signals led
Gabor (1946) to propose using shifted and modulated Gaussians as non-orthogonal
basis functions to represent signals. He called these basis functions “logons,” from
the Greek λ´εγ ω, representing the minimal volume that any signal can occupy in
the time–frequency plane according to the uncertainty principle. These kinds of
representation are at the basis of wavelet analysis in signal processing. Vetterli,
Kova˘cevi´c, and Goyal (2014a, 2014b) provide an up to date overview of the ﬁeld.
Finally, we wish to make some additional remarks on an important aspect of this
chapter: the encounter with the concept of a phase transition, which arises repeatedly
throughout the book. Phase transitions are commonly encountered in information theory
and statistical physics when stochastic concentration around typical sets occurs. Here,
we have instead encountered it in the deterministic setting of vector spaces subject to
regularity constraints. The idea is that when certain asymptotic regimes are studied
under a set of constraints, a concentration behavior occurs and a sharp transition can be
identiﬁed. In our case, the transition is due to the bandlimitation constraint and reveals
the limit on the amount of information associated with this class of signals.
2.9
Test Your Understanding
Problems
2.1
Show that α2(T) ≤λ0, and that to achieve the highest concentration λ0 we must
solve (2.55).
Solution
We use the compact notation deﬁned in Section 2.5.3, and also let T F = FT F−1F. We
let {ϕn} be solutions of 
BT ϕn = λϕn. By the Schwarz inequality and the completeness
of {ϕn}, we have:
α2 = ∥T f∥2
∥f∥2
= ⟨T f,f⟩
∥f∥2
= ⟨T F,F⟩
∥F∥2
= ⟨
BT F,F⟩
∥F∥2
≤∥
BT F∥∥F∥
∥F∥2
= ∥
BT 
∞
n=0 anϕn∥
∥F∥
04
12:22:31, subject to the Cambridge Core

84
Signals
= ∥
∞
n=0 anλnϕn∥
∥F∥
≤λ0
∥
∞
n=0 anϕn∥
∥F∥
= λ0.
(2.133)
To achieve the above bound with equality we can now pick a bandlimited function F to
be the solution corresponding to λ0 of the eigenvalue equation

BT F = α2F,
(2.134)
which corresponds to (2.55).
2.2
Determine limc0→0 λn(c0) for all n. (Hint: recall the energy interpretation of λn).
2.3
Determine the relation between T in Slepian’s concentration problem and the
geometry of the prolate spheroidal coordinates in which the Helmholtz equation can be
solved. What does the limit T →∞in Slepian’s case correspond to in the context of the
time-independent solution of the wave equation in prolate spheroidal coordinates?
2.4
Verify that (2.113) leads to (2.66), and use it to derive the Fourier transforms (2.69)
and (2.70).
2.5
Verify the orthonormality relations (2.71), (2.72) and (2.74), (2.75).
2.6
Rewrite the uncertainty principle for arbitrarily measurable sets (2.42) in the case
that the signal is subject to the energy constraint
 ∞
−∞
f 2(t) ≤E,
(2.135)
and provide the corresponding proof by modifying the proof in Appendix A.2.
2.7
Show that the set of functions {1,x,x2,...} is not pairwise orthogonal. Is it possible
to provide a representation of f ∈B using such a set?
2.8
Show that a signal with Gaussian instantaneous power achieves the highest
simultaneous time–frequency concentration permitted by Heisenberg’s uncertainty
principle,
σ 2
t σ 2
ω = 1
4.
(2.136)
Solution
Let f(t) be the unit-energy signal with Gaussian instantaneous power,
f 2(t) =
1
√
2πσ
exp[−t2/(2σ 2)].
(2.137)
The Fourier transform of the Gaussian in (2.137) can be computed to be
Ff 2 = exp(−ω2σ 2/2).
(2.138)
04
12:22:31, subject to the Cambridge Core

2.9 Test Your Understanding
85
The Fourier transform of the square root of (2.137) can be computed using (2.138) to
be
Ff =
√
2π
√
2σ
	√
2πσ
exp(−σ 2ω2).
(2.139)
From this, it follows that
(Ff)2 =
√
2π2σ exp(−2σ 2ω2)
= 2π 2σ
√
2π
exp

−
ω2
2[1/(2σ)2]
#
.
(2.140)
We then have
σ 2
ω = 1
2π
 ∞
−∞
ω2(Ff)2(ω)dω =
1
4σ 2 .
(2.141)
Since
σ 2
t =
 ∞
−∞
t2f 2(t)dt = σ 2,
(2.142)
the result now follows.
2.9
Show that a signal with Gaussian instantaneous power achieves the highest
entropic time–frequency concentration
h(f 2) + h(|F|2/(2π)) = log(πe).
(2.143)
Solution
Using (2.141) and, since in the case of the Gaussian given by (2.137) we have
h(f 2) = 1
2 log(2πeσ 2),
(2.144)
it follows that
h(f 2) + h(|F|2/(2π)) = 1
2 log(2πeσ 2) + 1
2 log
2πe
4σ 2

= log(πe).
(2.145)
2.10
Provide an elementary proof of the impossibility of time–frequency limiting.
Solution
Consider a square-integrable, timelimited signal such that f(t) = 0 for |t| > T/2, and
whose Fourier transform is F(ω). Let T0 > T, and consider the periodic signal
g(t) =
∞

n=−∞
f(t + nT0).
(2.146)
From (2.8) it follows that the Fourier series coefﬁcients of g(t) are
cn = F(nω0)/T0,
(2.147)
04
12:22:31, subject to the Cambridge Core

86
Signals
where ω0 = 2π/T0. Assume by contradiction that F(ω) is also bandlimited, namely
F(ω) = 0 for |ω| > . It follows that cn = 0 for |n| > /ω0, and g(t) is a trigonometric
polynomial
P(z) =
M

n=−M
cnzn,
(2.148)
where z = exp(jω0t) and M = ⌊/ω0⌋. Since g(t) = 0 in the interval [T/2,T0 −T/2],
then P(z) must have inﬁnitely many roots on the unit circle, namely it must be zero at
every point between exp(jω0T/2) and exp[jω0(T0 −T/2)] = exp(−jω0T/2), which is
impossible.
04
12:22:31, subject to the Cambridge Core

3
Functional Approximation
Functional analysis of inﬁnite-dimensional spaces is never fully convincing; you don’t get a
feeling of having done an honest day’s work.1
3.1
Signals and Functional Spaces
The set of signals for the communication engineer corresponds to an inﬁnite-dimensional
functional space for the mathematician. This can be viewed as a vector space on
which norm (i.e., length), inner product (i.e., angle), and limits can be deﬁned. The
engineering problem of determining the effective dimension of the signals’ space then
falls in the mathematical framework of approximation theory that is concerned with
ﬁnding a sequence of functions with desirable properties whose linear combination best
approximates a given limit function.
Approximation is a well-studied problem in analysis. In terms of abstract Hilbert
spaces, the problem is to determine what functions can asymptotically generate a given
Hilbert space in the sense that the closure of their span (i.e., ﬁnite linear combinations
and limits of those) is the whole space. Such a set of functions is called a basis, and its
cardinality, taken in a suitable limiting sense, is the Hilbert dimension of the space.
Various differential equations arising in physics have orthogonal solutions that can
be interpreted as bases of Hilbert spaces. One example is the solution of the wave
equation leading to the prolate spheroidal wave functions examined in the previous
chapter. Another notable example arises in quantum mechanics in the context of the
Schrödinger differential equation.
In this chapter, we describe the connection between physical properties, such as
the energy concentration of a wave function, and the mathematics of Hilbert spaces,
showing that Slepian’s concentration problem is a special case of the eigenvalue
problem arising from the spectral decomposition of a self-adjoint operator on a Hilbert
space. It turns out that this decomposition provides the optimal approximation for any
function in the space. The effective dimension, or degrees of freedom, of the space
is then deﬁned as the cardinality of such an optimal representation. The notion of
cardinality here is intended in a limiting sense, made precise with the notion of the
1 G.-C. Rota (1985). Mathematics, philosophy and artiﬁcial intelligence. A dialogue with Gian-Carlo Rota
and David Sharp. Los Alamos Science, 12, pp. 94–104. Reprinted with permission.
05
11:58:59, subject to the Cambridge Core

88
Functional Approximation
Kolmogorov N-width. The effective dimension of the signals’ space is then given by
the minimum number of orthogonal eigenfunctions that are required to approximate
any signal in the space up to arbitrary precision.
These mathematical results tell us all about N-widths, in the sense that they give
an explicit calculation in terms of subspaces and operators. However, they also tell us
very little, since in order to compute them we have to determine the eigenfunctions and
eigenvalues of these operators. Only when this is possible, as in the case of N-widths of
bandlimited signals observed over an interval, is the dimension of the space determined
completely and the approximating eigenfunctions shown to be the orthogonal prolate
spheroidal ones.
3.2
Kolmogorov N-Width
Let X be a normed linear space and XN any N-dimensional subspace of X . For any
f ∈X , the distance of XN from f is deﬁned as
inf{∥f −g∥: g ∈XN}.
(3.1)
If instead of a single element f ∈X we are given a subset A ⊆X , a measure of an
N-dimensional approximation of A is given by the extent to which the “worst element”
of A can be approximated from XN. This deviation is deﬁned as
DXN(A ) = sup
f∈A
inf
g∈XN ∥f −g∥.
(3.2)
One might also ask how well it is possible to approximate A by N-dimensional
subspaces of X . The idea of allowing XN to vary within X was ﬁrst proposed
by Kolmogorov (1936), and the level of approximation goes under the name of the
Kolmogorov N-width of A in X , deﬁned as
dN(A ,X ) =
inf
XN⊆X DXN(A ).
(3.3)
When the space X is clear from the context we use the shorthand notation dN(A ).
Since dN(A ) measures the extent to which A can be approximated by N-dimensional
subspaces of X , it is in a certain sense a measure of the “massiveness” or “thickness”
of A .
Consider now the set of bandlimited signals that are in E (ϵT) as deﬁned by the pair
of constraints (2.43) and (2.44), and assume that ϵT ≤√1 −λN. We equip these signals
with the L2(−∞,∞) norm
∥f(t)∥2 =
 ∞
−∞
f 2(t)dt.
(3.4)
In this case, we have the following result:
dN(E (ϵT)) =
ϵT
√1 −λN
,
(3.5)
05
11:58:59, subject to the Cambridge Core

3.2 Kolmogorov N-Width
89
and the optimal spanning eigenfunctions are the prolate spheroidal ones in the set YN =
{ψ0(t),ψ1(t),...,ψN−1(t)}, namely
DYN(E (ϵT)) = dN(E (ϵT)).
(3.6)
From this result we can immediately recover (2.129). Letting N = N0+o(1) as N0 →∞,
and using (2.131), we have
d2
N(E (ϵT)) =
ϵ2
T
1 −λN0
→2ϵ2
T.
(3.7)
Similarly, letting N = N0 +O(logN0) as N0 →∞, for any 0 < ϵ < 1 and T large enough
we have
d2
N(E (ϵT)) =
ϵ2
T
1 −λN
≤
ϵ2
T
1 −ϵ ,
(3.8)
which recovers (2.130) since ϵ is arbitrarily small.
A similar result holds by considering the L2[−T/2,T/2] norm
∥f∥2 =
 T/2
−T/2
f 2(t)dt.
(3.9)
In this case, the signal is observed over a ﬁnite time interval and the accuracy of the
approximation is evaluated over the observation interval rather than over the whole real
axis. We have
dN(E (ϵT)) = ϵT
√λN
√1 −λN
.
(3.10)
Compared to (3.3), since λN →0 the accuracy of the ﬁt in this case is higher, as
expected. Letting N = N0 + o(1) as N0 →∞, and using (2.131), we have
d2
N(E (ϵT)) = ϵ2
TλN0
1 −λN0
→ϵ2
T,
(3.11)
showing that in this case using N0 eigenfunctions we achieve a better approximation
compared to (3.7). Similarly, letting N = N0 +O(logN0) as N0 →∞, for any 0 < ϵ < 1
and T large enough we have
d2
N(E (ϵT)) = ϵ2
TλN
1 −λN
≤
ϵ
1 −ϵ ϵ2
T,
(3.12)
showing that an additional logarithmic number of eigenfunctions can drive the N-width
as close to zero as desired, since ϵ is arbitrarily small. This should be compared with
(3.8), where using the same number of eigenfunctions the N-width is made only as close
to ϵT as possible.
We also have the following additional results for bandlimited signals subject to the
single constraint (2.43). In the L2[−T/2,T/2] norm
dN(B) =
	
λN,
(3.13)
and in the L2(−∞,∞) norm
dN(B) = 1.
(3.14)
05
11:58:59, subject to the Cambridge Core

90
Functional Approximation
A derivation of (3.5), (3.10), (3.13), and (3.14) is shown in the next section and provides
the link between the behavior of the eigenvalues in Slepian’s concentration problem and
the approximation error bounds given in the previous chapter.
3.3
Degrees of Freedom of Bandlimited Signals
We apply the Kolmogorov N-width to determine the number of degrees of freedom of
bandlimited signals with respect to a given norm. In the context of physics, the number
of degrees of freedom is deﬁned as:
The minimum number of parameters sufﬁcient to describe a system within a given
precision.
In an analogy to the physical deﬁnition, we consider a subset A of a normed linear
space X of signals of norm at most one, and deﬁne the number of degrees of freedom
at level ϵ as
Nϵ(A ) = min{N : d2
N(A ) ≤ϵ}.
(3.15)
In this way, the number of degrees of freedom corresponds to the dimension of the
minimal subspace representing the elements of A within ϵ accuracy.
Letting B be the set of all square-integrable, bandlimited signals subject to the
energy constraint (2.43) and equipped with the L2[−T/2,T/2] norm, using (3.13) and
(2.132), we have
Nϵ(B) = min{N : d2
N(B) ≤ϵ}
= min{N : λN ≤ϵ}
= N0 + O(logN0), as N0 →∞.
(3.16)
This basic result follows directly from the computation of the N-width of bandlimited
signals and from the behavior of the eigenvalues in Slepian’s concentration problem. It
shows that any bandlimited signal can be described over a ﬁnite observation interval and
up to arbitrary precision by a number of coefﬁcients only slightly larger than N0. The
dependence on ϵ appears hidden as a pre-constant of the second-order term O(logN0)
in the number of degrees of freedom, and we have
lim
N0→∞
Nϵ(B)
N0
= 1.
(3.17)
In addition, by (2.132) the pre-constant of O(logN0) grows only as log(1/ϵ) as ϵ →0.
A completely analogous result holds if the signals are subject to the constraint (1.1)
by deﬁning the number of degrees of freedom as the minimum index N such that the
largest energy of the error becomes an arbitrarily small fraction of the largest energy of
the signal, namely
Nϵ(B) = min{N : d2
N(B)/E ≤ϵ}.
(3.18)
05
11:58:59, subject to the Cambridge Core

3.3 Degrees of Freedom of Bandlimited Signals
91
Table 3.1 Degrees of freedom of bandlimited signals.
L2[−T/2,T/2]
L2(−∞,∞)
Nϵ(B) = N0 + O(logN0)
Nϵ(B) = ∞
NϵT (E (ϵT)) = N0 + o(1)
NϵT/(1−ϵ)(E (ϵT)) = N0 + O(logN0)
On the other hand, when signals in B are observed over the whole real line, the
appropriate norm used to compute the number of degrees of freedom is the L2(−∞,∞)
one, and by virtue of (3.14) the number of degrees of freedom is undetermined. In this
case, since it is not possible to approximate all signals in the space using any ﬁnite
number of coefﬁcients, we posit that Nϵ(B) = ∞.
We now consider the set of bandlimited signals E (ϵT) that are subject to the pair of
constraints (2.43) and (2.44). In this case, in the L2[−T/2,T/2] norm, using (3.11) we
have
NϵT(E (ϵT)) = N0 + o(1), as N0 →∞.
(3.19)
In the L2(−∞,∞) norm, by (3.8) we have
NϵT/(1−ϵ)(E (ϵT)) = N0 + O(logN0), as N0 →∞.
(3.20)
These results are summarized in Table 3.1. By (3.19), a number N0 of coefﬁcients
can represent bandlimited signals over a ﬁnite observation interval, up to a degree of
approximation ϵT given by the unconcentrated part of the signal. On the other hand,
to obtain a representation with a degree of approximation arbitrarily close to ϵT over
the entire real line, by (3.20) we need to add an extra logarithmic number of terms. By
(3.16), this same number of extra terms is needed to represent any bandlimited signal up
to arbitrary accuracy over a ﬁnite observation interval, while it is impossible to obtain
such an approximation over the entire real line.
In all cases where an approximation is feasible, the dependence of the degrees of
freedom on the level of approximation ϵ is weak. By (2.132), this number grows at
most as log(1/ϵ) as ϵ →0, and it affects only a second-order term as N0 →∞. This
makes the effective dimensionality of the signals’ space asymptotically independent of
the accuracy of the approximation. This is very desirable, as it makes the degrees of
freedom a principal feature of our mathematical model of the real world of transmitted
signals, one that is practically insensitive to small changes of a secondary feature of the
model, such as the accuracy of the measurement with which the signals are detected, and
is a direct consequence of the phase transition of the eigenvalues expressed by (2.131).
A physical regularity constraint of smoothness of the transmitted signal, expressed by
their bandlimitation property, allows us to deﬁne the degrees of freedom as an intrinsic
feature of the model.
3.3.1
Computation of the N-Widths
We now show the validity of the results in (3.13) and (3.14), as well as in (3.5) and
(3.10). The derivations follow the same outline, with some minor variations. We proceed
05
11:58:59, subject to the Cambridge Core

92
Functional Approximation
in two steps. First, we determine an upper bound on dN(B) by computing DXN(B)
for a certain choice of the approximating subspace XN. In a second step, we show that
the obtained quantity is a lower bound as well. We start by describing the proof of
(3.13), then we show the variation needed to obtain (3.5) and (3.10).
Recall, as discussed in Section 2.5, that the {ψn(t)} are the solutions of the Fredholm
integral equation
 T/2
−T/2
sin(t −t′)
π(t −t′)
ψn(t′)dt′ = λnψn(t), t ∈[−T/2,T/2],
(3.21)
and satisfy the following properties:
ψn(t) ∈B,
(3.22)
 T/2
−T/2
ψn(t)ψm(t)dt =
 λn
if n = m,
0
otherwise.
(3.23)
When these functions are extended over the whole real line, we also have
 ∞
−∞
ψn(t)ψm(t)dt =
 1
if n = m,
0
otherwise.
(3.24)
Let f(t) ∈B, then by the completeness of the {ψn} we have
f(t) =
∞

n=0
anψn(t).
(3.25)
Consider the N-dimensional approximation
fN(t) =
N−1

n=0
anψn(t).
(3.26)
By (2.127), we have, in the L2[−T/2,T/2] norm,
∥f −fN∥2 = inf
g∈XN ∥f −g∥2 =
∞

n=N
a2
nλn.
(3.27)
From the monotonicity of the {λn}, we have
inf
g∈XN ∥f −g∥2 =
∞

n=N
a2
nλn
≤λN
∞

n=N
a2
n.
(3.28)
On the other hand, by Parseval’s theorem, we have
 ∞
−∞
f 2(t)dt =
∞

n=0
a2
n ≤1.
(3.29)
Combining (3.28) and (3.29), it then follows that
dN(B) ≤
	
λN.
(3.30)
05
11:58:59, subject to the Cambridge Core

3.3 Degrees of Freedom of Bandlimited Signals
93
Next, we show that the upper bound is achievable. Consider the N +1-dimensional ball
UN+1 in the L2[−T/2,T/2] norm, deﬁned by
g(t) =
N

n=0
anψn(t), ∥g∥≤
	
λN.
(3.31)
We have (see Problem 3.7) that
dN(UN+1) =
	
λN.
(3.32)
Since, by (3.22), the {ψn} are bandlimited functions, UN+1 ∈B. We only have to verify
that the energy over (−∞,∞) is at most one. By (3.23) and (3.31), we have
∥g∥2 =
N

n=0
a2
n∥ψn∥2
=
N

n=0
a2
nλn ≤λN.
(3.33)
From the monotonicity of the {λn} and (3.33), we ﬁnally have
 ∞
−∞
g2(t)dt =
N

n=0
a2
n
≤
N

n=0
a2
n
λn
λN
≤1,
(3.34)
and the proof of (3.13) is complete. As for (3.14), this follows by the same steps and
using (3.24) instead of (3.23) when computing the L2(−∞,∞) norm.
We now turn to the variation needed to obtain (3.10). In this case, the proof of the
upper bound proceeds in the same way until (3.28). Then, the following different upper
bound is used:
λN
∞

n=N
a2
n ≤λN
∞

n=0
a2
n
1 −λn
1 −λN
≤
ϵ2
T
1 −λN
λN,
(3.35)
where the ﬁrst inequality follows from the monotonicity of the {λn}. The second
inequality follows by recalling that f ∈E (ϵT) and, assuming the validity of term by
term integration of the series below, formally performing the computation:
ϵ2
T ≥
 ∞
−∞
(f(t) −T f(t))2dt
=
 ∞
−∞
 ∞

n=0
anψn(t) −anT ψn(t)
2
dt
05
11:58:59, subject to the Cambridge Core

94
Functional Approximation
=
 ∞
−∞
 ∞

n=0
anψn(t) −an
	
λnξn(t)
2
dt
=
∞

n=0
a2
n(1 −2λn + λn)
=
∞

n=0
a2
n(1 −λn),
(3.36)
where T is the projection operator (2.87) onto the subspace of timelimited signals and
ξn is deﬁned in (2.70). A completely rigorous derivation is possible, but requires a more
detailed argument.
To show that the upper bound is achievable, one needs to consider the N +
1-dimensional ball UN+1 in the L2[−T/2,T/2] norm, deﬁned by
g(t) =
N

n=0
angn(t), ∥g∥≤ϵT
√λN
√1 −λN
,
(3.37)
with
gn(t) =
ϵT
√1 −λn
ψn(t),
(3.38)
and assume that ϵT/√1 −λN ≤1. With the same argument as in Problem 3.7, we have
dN(UN+1) = ϵT
√λN
√1 −λN
.
(3.39)
We now need to check that the energy of g is at most one, and that the amount of
energy outside [−T/2,T/2] is at most ϵ2
T. We have
∥g∥2 =
N

n=0
a2
n∥gn∥2
=
N

n=0
a2
n
ϵ2
Tλn
1 −λn
≤ϵ2
TλN
1 −λN
,
(3.40)
where the inequality follows from (3.37). From the monotonicity of the {λn}, we have
that, for all n = 0,1,...,N,
ϵ2
Tλn
1 −λn
≥ϵ2
TλN
1 −λN
,
(3.41)
which, combined with (3.40), shows that
N

n=0
a2
n ≤1.
(3.42)
05
11:58:59, subject to the Cambridge Core

3.4 Hilbert–Schmidt Integral Operators
95
Using (3.42), it follows that
 ∞
−∞
g2(t)dt =
N

n=1
a2
n
ϵ2
T
1 −λn
≤
N

n=1
a2
n
ϵ2
T
1 −λN
≤
ϵ2
T
1 −λN
,
(3.43)
which is at most one. Combining (3.43) and (3.40), we also have
 ∞
−∞
(g(t) −T g(t))2dt ≤ϵ2
T.
(3.44)
The proof of (3.5) proceeds with computations analogous to those above, using the
L2(−∞,∞) norm, and it is left as an exercise.
3.4
Hilbert–Schmidt Integral Operators
The results on the N-width and degrees of freedom of bandlimited signals can
be viewed in a more general setting by considering the behavior of the singular
values of Hilbert–Schmidt integral operators, which are a class of operators that were
extensively studied by the great German mathematician David Hilbert and his student
Erhard Schmidt. The optimal orthogonal representation of any signal in B() is
obtained by solving Slepian’s concentration problem (3.21) and then constructing the
ﬁnite-dimensional approximation (3.26) in terms of prolate spheroidal wave functions.
When the accuracy of the approximation is evaluated over the observation interval
[−T/2,T/2], by (3.13) it turns out that this is at most √λN.
We now view bandlimited signals as the image of square-integrable signals through
a bandlimiting operator, show that their optimal representation corresponds to the
Hilbert–Schmidt decomposition of this image space, and that the accuracy of the
approximation corresponds to the square root of the Nth eigenvalue of the associated
self-adjoint operator.
Consider the Hilbert spaces X = L2(a,b), Y = L2(c,d). A Hilbert–Schmidt kernel
K(x,y) on (a,b) × (c,d) is deﬁned by the property
 d
c
 b
a
|K(x,y)|2dxdy < ∞.
(3.45)
This naturally induces an operator K : Y →X such that, for any g ∈Y ,
(Kg)(x) =
 d
c
K(x,y)g(y)dy.
(3.46)
05
11:58:59, subject to the Cambridge Core

96
Functional Approximation
Similarly, consider another operator K′ : X →Y induced by the Hilbert–Schmidt
kernel K′(x,y) on (a,b) × (c,d) such that, for any h ∈X ,
(K′h)(y) =
 b
a
K′(x,y)h(x)dx.
(3.47)
The operator K′ is called an adjoint operator of K if and only if, for any g ∈Y and
h ∈X ,
 b
a
(Kg)(x) h∗(x)dx =
 d
c
g(y) (K′h)∗(y)dy.
(3.48)
A self-adjoint operator is its own adjoint. Consider the operator K′K, with symmetric
Hermitian kernel
K′K(x,y) = (K′K)∗(y,x) =
 b
a
K(z,x)K∗(z,y)dz
(3.49)
of Hilbert–Schmidt type
 d
c
 d
c
|K′K(x,y)|2dxdy < ∞.
(3.50)
This operator is self-adjoint since, for all g,h ∈Y ,
 d
c
K′Kg(x)h∗(x)dx =
 d
c
g(x)(K′Kh)∗(x)dx.
(3.51)
It is also non-negative; namely, for all g ∈Y ,
 d
c
K′Kg(x)g∗(x)dx ≥0.
(3.52)
A basic property of Hilbert–Schmidt operators is that they are compact, meaning that
for any bounded sequence {xn} in X , there exists a bounded subsequence of {Kxn} that
converges in Y . The spectral theorem states that compact, self-adjoint operators admit
either a ﬁnite or a countably inﬁnite orthonormal basis of eigenvectors {ϕn}, and that
the corresponding eigenvalues {λn} are real and such that λn →0. In the case that the
operator is non-negative, the eigenvalues are also non-negative. The eigenvalues and
eigenvectors are solutions of the equation
K′Kϕn = λnϕn.
(3.53)
The singular values of K are deﬁned as the square roots of the eigenvalues λ0 ≥λ1 ≥
λ2 ≥···. Letting ψn = Kϕn, n = 1,2,..., it can be easily veriﬁed that the function ψn is
an eigenfunction, with corresponding eigenvalue λn, of the operator KK′, namely
KK′ψn = λnψn.
(3.54)
Consider now the subset A ⊂X that is the image of the unit-norm elements of Y
through the Hilbert–Schmidt operator induced by the kernel K, namely
A =

f(x) : f(x) = (Kg)(x) =
 d
c
K(x,y)g(y)dy, ∥g∥≤1
#
.
(3.55)
05
11:58:59, subject to the Cambridge Core

3.4 Hilbert–Schmidt Integral Operators
97
Another basic property of Hilbert–Schmidt operators is that the N-width of the set
of signals in (3.55) is given by their Nth singular value, when these are arranged in
non-increasing order, namely:
The N-width of the image of the unit-norm elements of Y through the operator K
is
dN(A ) =
	
λN,
(3.56)
and the eigenfunctions in the set {ψ0,ψ1,...,ψN−1} span an optimal approximating
N-dimensional subspace XN.
It follows that the singular values of a Hilbert–Schmidt operator determine the level
of approximation for the optimal decomposition of the elements of the image space of
the operator into basis functions. This decomposition is useful when one wants to study
the asymptotic dimensionality of a Hilbert–Schmidt operator. The solution to Slepian’s
concentration problem corresponds to the decomposition of one such operator, and leads
to the exact determination of the asymptotic dimensionality of the associated operator.
The basic result (3.56) follows directly from a duality result through the Schwarz
inequality, and from the variational characterization of the eigenvalues of compact
self-adjoint operators on Hilbert spaces.
By the deﬁnition of N-width, we have
dN(A ) = inf
Xn sup
∥g∥≤1
inf
h∈Xn ∥Kg −h∥.
(3.57)
The duality result referred to above is
inf
h∈Xn ∥Kg −h∥=
sup
z⊥Xn,∥z∥=1
⟨Kg,z⟩,
(3.58)
where by z ⊥Xn we mean that z is orthogonal to every element of Xn, and ⟨u,v⟩
indicates the inner product in L2(c,d):
⟨u,v⟩=
 d
c
u(x)v∗(x)dx.
(3.59)
Letting Kg = f, we write
f = f⊥+ f−,
(3.60)
where f−∈Xn and f⊥⊥Xn, so that the distance of f from Xn is
inf
h∈Xn ∥f −h∥= ∥f⊥∥,
(3.61)
and to establish the duality result (3.58) we need to prove that
∥f⊥∥=
sup
z⊥Xn,∥z∥=1
⟨f,z⟩.
(3.62)
05
11:58:59, subject to the Cambridge Core

98
Functional Approximation
By again using the orthogonal decomposition (3.60) and the Schwarz inequality, we
have that, for all z ⊥Xn,
⟨f,z⟩= ⟨f⊥,z⟩
≤∥f⊥∥∥z∥,
(3.63)
where the equality holds for z = f⊥/∥f⊥∥, establishing (3.62) and hence (3.58).
Substituting (3.58) into (3.57), by the deﬁnition of an adjoint operator and using once
again the Schwarz inequality in the case of equality, it follows that
dN(A ) = inf
Xn sup
∥g∥≤1
sup
z⊥Xn,∥z∥=1
⟨Kg,z⟩
= inf
Xn
sup
z⊥Xn,∥z∥=1
sup
∥g∥≤1
⟨g,K′z⟩
= inf
Xn
sup
z⊥Xn,∥z∥=1
∥K′z∥
=

inf
Xn
sup
z⊥Xn,∥z∥=1
⟨KK′z,z⟩
1/2
=
	
λN,
(3.64)
where the last equality follows from the min-max principle, see Appendix A.4.1, and it
is easy to see that the inﬁmum and supremum in (3.64) are obtained by letting Xn =
span {ψ0,ψn,...,ψn−1} and z = ψn.
The derivation above did not explicitly calculate the optimal approximation for the
set A . In Section 3.3.1, we gave an explicit computation of the approximation when
A is the space of bandlimited signals observed over an interval. In this case, it turns
out that the prolate spheroidal functions can optimally approximate bandlimited signals
up to arbitrary precision, and that timelimiting and bandlimiting operators are the
corresponding adjoint operators associated with this approximation.
3.4.1
Timelimiting and Bandlimiting Operators
A special case of Hilbert–Schmidt integral operators is that of timelimiting and
bandlimiting operators with complex exponential kernels that can be viewed as adjoint
operators acting on L2 space. In this case, the decomposition can be computed explicitly,
and it turns out that the prolate spheroidal functions can optimally approximate
bandlimited signals up to arbitrary precision.
First, we check that the timelimiting operator K′ with complex exponential kernel is
indeed the adjoint of the bandlimiting operator K with corresponding conjugate kernel.
For any real h ∈L2[−T/2,T/2], we let
(K′h)(ω) =
1
√
2π
 T/2
−T/2
h(t)exp(−jωt)dt
(3.65)
05
11:58:59, subject to the Cambridge Core

3.4 Hilbert–Schmidt Integral Operators
99
and
(K′h)∗(ω) =
1
√
2π
 T/2
−T/2
h(t)exp(jωt)dt.
(3.66)
For any g ∈L2[−,], we let
(Kg)(t) =
1
√
2π
 
−
g(ω)exp(jωt)dω.
(3.67)
We can now verify (3.48) by writing
 T/2
−T/2
 
−
g(ω)exp(jωt)h(t)dωdt =
 
−
 T/2
−T/2
h(t)exp(jωt)g(ω)dtdω.
(3.68)
In the case of prolate spheroidal functions, using (2.67)–(2.70), we have
(K′ψn)(ω) =
1
√
2π
 T/2
−T/2
ψn(t)exp(−jωt)dt
=
1
√
2π
	
λnn(ω),
(3.69)
(Kn)(t) =
1
√
2π
 
−
n(ω)exp(jωt)dω
=
√
2π
	
λnψn(t).
(3.70)
Using (3.69) and (3.70), we have that (3.48) can also be veriﬁed as follows:
1
√
2π
 
−
(K′ψn)∗(ω)m(ω)dω =
√λn
2π
 
−
∗
n(ω)m(ω)dω
=
 √λnλn
if n = m,
0
otherwise;
(3.71)
1
√
2π
 T/2
−T/2
Kn(t)ψm(t)dt =
	
λn
 T/2
−T/2
ψn(t)ψm(t)dt
=
 √λnλn
if n = m,
0
otherwise.
(3.72)
Similarly, the eigenvalue equation (3.54) follows from (3.69) and (3.70), writing
(KK′ψn)(t) =
	
λn
	
λnψn(t)
= λnψn(t).
(3.73)
We can ﬁnally recover (3.21) by rewriting the above eigenvalue equation as
(KK′ψn)(t) = 1
2π
 
−
 T/2
−T/2
ψn(t′)exp(−jωt′)dt′ exp(jωt)dω
=
 T/2
−T/2
ψn(t′)
 

exp(jω(t −t′))dωdt′
05
11:58:59, subject to the Cambridge Core

100
Functional Approximation
=
 T/2
−T/2
sin(t −t′)
π(t −t′)
ψ(t′)dt′
= λnψn(t).
(3.74)
It follows that the Fredholm integral equation arising from Slepian’s concentration
problem is a special case of the eigenvalue equation determining the singular values
associated with a given Hilbert–Schmidt kernel – that is, the complex exponential.
This equation also deﬁnes the N-width of the set of signals that are the image of the
bandlimiting operator with such a kernel. The equation can be solved explicitly, and the
solution is found in terms of the optimal prolate spheroidal basis, which are real singular
functions providing the optimal approximation.
3.4.2
Hilbert–Schmidt Decomposition
We now provide a construction of the optimal approximation of every element in the
image space of a Hilbert–Schmidt operator in terms of singular functions, and give a
geometric interpretation of the Hilbert–Schmidt decomposition. Consider the following
approximation problem for a Hilbert–Schmidt kernel K(x,y):
eN(K) = min
⎧
⎨
⎩
 b
a
 d
c
K(x,y) −
N−1

n=0
un(x)vn(y)

2
dxdy :
un ∈L2(a,b),vn ∈L2(c,d)
#
.
(3.75)
Schmidt (1907) proved that the minimum of (3.75) is obtained by choosing the functions
{un} and {vn} to be the eigenfunctions of the self-adjoint operator K′K : L2(c,d) →
L2(c,d), with symmetric Hermitian kernel
K′K(x,y) = (K′K)∗(y,x) =
 b
a
K(z,x)K∗(z,y)dz
(3.76)
of Hilbert–Schmidt type
 d
c
 d
c
|K′K(x,y)|2dxdy < ∞.
(3.77)
This operator is non-negative, compact, and self-adjoint. Call its eigenvalues λ1 ≥λ2 ≥
··· > 0 and associated orthonormal eigenvectors ϕn, n = 1,2,..., left singular functions.
Let ψn(x) = (Kϕn)(x) =
 d
c K(x,y)ϕn(y)dy. The orthogonal functions ψn, n = 1,2,...,
are eigenvectors of KK′, are called right singular functions, and
 b
a ψ(x)ψ∗(x)dx = λn.
A minimizer of (3.75) is then given by
KN(x,y) =
N−1

n=0
ψn(x)ϕ∗
n(y),
(3.78)
and the minimum value of the error equals the tail sum of the eigenvalues:
eN(K) =
 b
a
 d
c
|K(x,y) −KN(x,y))|2dxdy
05
11:58:59, subject to the Cambridge Core

3.4 Hilbert–Schmidt Integral Operators
101
=
∞

n=N
λn.
(3.79)
It follows that the optimal way to represent the operator induced by K(x,y) is given
by the combination (3.78) of its left and right singular functions, and the error of the
approximation is given by the sum of the tail of the eigenvalues, or singular values
squared, as a function of their indices.
This also implies that an optimal N-dimensional approximation of any function that
is the image of the operator induced by the kernel K on (a,b), namely of the elements
in the set A in (3.55), is
fN(x) =
 d
c
KN(x,y)g(y)dy
=
 d
c
N−1

n=0
ψn(x)ϕ∗
n(y)g(y)dy
=
N−1

n=0
 d
c
ϕ∗
n(y)g(y)dy ψn(x)
=
N−1

n=0
 d
c
ϕ∗
n(y)g(y)dy
	
λnξn(x),
(3.80)
where ξn(x) is the normalized version of ψn(x), so that both ξn(x) and ϕn(y) have unit
norm over L2(a,b) and L2(c,d), respectively. The error of the approximation, over all
possible functions f in the space, is in this case given in terms of Kolmogorov’s N-width
by (3.56).
Recalling the inner product deﬁnition (2.80), the geometric interpretation of (3.80)
is that of writing the image of g through the operator K by ﬁrst projecting g onto the
coordinate axes corresponding to the eigenvectors {ϕn}, then scaling these coordinates
by the corresponding singular values, and ﬁnally obtaining the new representation in
terms of these scaled coordinates viewed in the image space through the operator K of
the eigenvectors {ϕn}, namely the space spanned by the eigenvectors {ξn}. A sketch of
this geometrical interpretation is depicted in Figure 3.1.
K
g
1
k
f
ξ
ξ
1
k
1
√
k
√
ϕ
ϕ
Fig. 3.1
Geometric view of the Hilbert–Schmidt decomposition.
05
11:58:59, subject to the Cambridge Core

102
Functional Approximation
Substituting (3.78) into (3.76), we also have the Hilbert–Schmidt decomposition of
the kernel K′K,
K′KN(x,y) =
N−1

n=0
λnϕn(x)ϕ∗
n(y),
(3.81)
that achieves the minimum approximation error
eN(K′K) =
 b
a
 d
c
K′K(x,y) −
N−1

n=0
λnϕn(x)ϕ∗
n(y)

2
dxdy
=
∞

n=N
λ2
n.
(3.82)
The approximation errors in (3.79) and (3.82), as well as the Kolmogorov N-width
(3.56), tend to zero as N →∞, ensuring L2 convergence of the Hilbert–Schmidt
representation. Sometimes, it is convenient to obtain stronger convergence properties.
This can be done provided that some regularity assumptions are made besides
square-integrability. For example, for any Hilbert–Schmidt, self-adjoint operator that is
non-negative and has a continuous kernel, Mercer’s theorem, stated in Appendix A.4.2,
ensures that the representation (3.81) converges absolutely and uniformly (and hence
point-wise) as N →∞.
3.4.3
Singular Value Decomposition
When working on his decomposition, Schmidt was apparently unaware of the parallel
work on singular values of ﬁnite matrices. His representation is the continuous version
of the diagonalization of a ﬁnite-dimensional matrix operator. Any (real or complex)
matrix A of size M × N has the singular value decomposition (SVD)
A = UV† =
R−1

n=0
σnU[n]V†[n],
(3.83)
where U is an M × M unitary matrix, U[n] is its nth column, V† is an N × N unitary
matrix representing the conjugate transpose of V, and V[n] is its nth column. The
columns of V are the eigenvectors of A†A, called the right singular vectors. The columns
of U are the eigenvectors of AA†, called left singular vectors. The matrix  is an M ×N
diagonal matrix of non-negative real numbers, partitioned in the form
 =
R
0
0
0

,
(3.84)
with R an r × r square diagonal matrix with positive entries
R = diag (σ0,σ1,...,σR−1)
(3.85)
called the singular values of A and corresponding to the square roots of the eigenvalues
of AA† and A†A. These are arranged in decreasing order, so that σ0 ≥σ1 ≥··· ≥
05
11:58:59, subject to the Cambridge Core

3.4 Hilbert–Schmidt Integral Operators
103
σR−1 > 0. Geometrically, the SVD can then be viewed as follows. A is decomposed
into three transformations: a rotation V†, a scaling  along the coordinate axes {ϕn},
and a second rotation U. The singular values represent the lengths of the axes of the
ellipsoid obtained.
Associated with the SVD there is a family of reduced-rank matrices {AK} obtained
by keeping only the ﬁrst K < R terms in the expansion:
AK =
K−1

n=0
σnU[n]V†[n].
(3.86)
The matrices A and AK agree in their largest K singular values, but the last R −K
singular values of A are replaced by zeros in AK.
The L2 matrix norm is deﬁned in terms of the vector norm as
∥A∥L2 =
sup
∥X∥L2̸=0
∥AX∥L2
∥X∥L2 =
sup
∥X∥L2=1
∥AX∥L2,
(3.87)
where X is an N × 1 column vector and its norm is the square root of the sum of its
absolute value elements,
∥X∥L2 =
	
|x1|2 + |x2|2 + ··· + |xN|2.
(3.88)
The Frobenius norm – which is not derived from a vector norm – is the square root of
the sum of the absolute values squared of all matrix elements,
∥A∥F =




M−1

n=0
N−1

k=0
|An,k|2 =
	
(A†A) =
	
tr (A∗A).
(3.89)
The reduced-rank approximation theorem states that within the set of M ×N matrices
of rank K < R, the matrix that most closely approximates A in the L2 or the Frobenius
norm is the matrix AK. Namely, over all N × M matrices B of rank K, the distance
∥A −B∥is minimized when B = AK. Furthermore, the minimized distance is
∥A −AK∥L2 = σK,
(3.90)
∥A −AK∥F = (σ 2
K + ··· + σ 2
R−1)1/2.
(3.91)
This result is an essential tool in signal processing, data compression, statistics, and
many areas in physics. In these applications one often works with matrices that are full
rank, but their singular values tend to cluster into two groups, those that are large and
those that are small, so that one can perform dimensionality reduction and deﬁne an
effective rank of the matrix that is the discrete analog of the Kolmogorow N-width in
continuous spaces. The approximation result (3.90) corresponds to (3.56), and (3.91)
corresponds to (3.79).
05
11:58:59, subject to the Cambridge Core

104
Functional Approximation
3.5
Extensions
We have shown that the N-width of signals in B can be determined in terms of
the asymptotic behavior of the eigenvalues of a certain Fredholm integral equation.
This leads to a natural deﬁnition of the number of degrees of freedom of bandlimited
signals that represents the effective dimensionality of the signals’ space. As T →∞,
any bandlimited signal can be represented by specifying only slightly more than N0 real
numbers, corresponding to its coordinates in the space. The folk theorem has been made
precise.
In practice, however, it is often the case that signals are only approximately
bandlimited, and it is therefore necessary to study the approximation involved in
the substitution of signals that are not strictly bandlimited with elements in the set
B. Another extension regards signals that are not bandlimited to single intervals in
frequency, or observed over single intervals in time, but are supported over the union
of a ﬁnite number of ﬁxed, disjoint intervals. Finally, signals can also be functions of
several variables, and be supported over arbitrary measurable sets, where these variables
and their transformed pairs can take values. All of the above extensions are relevant for
electromagnetic waveforms that are functions of space and time, are only approximately
ﬁltered by the propagation process, and that, depending on the application, can span
multiple bands in the spatial, temporal, frequency, and wavenumber domains.
In the following, we sketch the theory in the most general form and apply the results
in subsequent chapters in the speciﬁc context of the propagation of electromagnetic
signals.
3.5.1
Approximately Bandlimited Signals
Consider signals in a subset E ⊂L2[−T/2,T/2] that are not necessarily strictly
bandlimited. Their Kolmogorov N-width can be bounded by the sum of the deviation
from the set of bandlimited signals plus the N-width of bandlimited signals:
dN(E ) ≤DB(E ) + dN(B).
(3.92)
This bound can be evaluated by ﬁrst projecting the space of signals onto B,
determining the error associated with this projection, and then adding it to the error
associated with the best N-dimensional subspace approximation of B.
To derive the result, we consider the deviation DXn(E ) between E and an arbitrary
N-dimensional subspace
DXN(E ) = sup
f∈E
inf
g∈XN ∥f −g∥
= sup
f∈E
DXN(f).
(3.93)
From this, we can derive the following “triangle” inequality:
DXN(E ) ≤DB(E ) + DXN(B).
(3.94)
05
11:58:59, subject to the Cambridge Core

3.5 Extensions
105
The derivation of the inequality is as follows. For any h ∈B, we write
DXN(f) = inf
g∈XN ∥f −g∥
≤inf
g∈XN(∥f −h∥+ ∥h −g∥)
= ∥f −h∥+ inf
g∈XN ∥h −g∥
= ∥f −h∥+ DXN(h)
≤∥f −h∥+ DXN(B).
(3.95)
Since h is arbitrary, we have
DXN(f) ≤inf
h∈B
∥f −h∥+ DXN(B)
= DB(f) + DXN(B).
(3.96)
Now, taking the supremum of f on both sides, we have
sup
f∈E
DXN(f) ≤sup
f∈E
DB(f) + DXN(B)
= DB(E ) + DXN(B).
(3.97)
The inequality (3.94) then follows from (3.93). This is useful to get an upper bound on
the N-width of E in terms of the N-width of the set of signals B. Taking the inﬁmum
with respect to XN of (3.94), we obtain (3.92).
It follows that in order to bound the degree to which any set of signals can
be approximated by ﬁnite-dimensional subspaces, it is sufﬁcient to determine their
deviation from the set of bandlimited signals and then apply the approximation results
for bandlimited signals. In contrast to the ﬁrst term, the second term in (3.92) depends
on . A typical technique is to project the space E onto B and then determine the error
associated with this projection. This is then added to the approximation error obtained
by a ﬁnite interpolation of a strictly bandlimited signal. We make use of this technique
extensively in subsequent chapters to determine the number of degrees of freedom of
electromagnetic signals propagating in multiple scattering environments.
3.5.2
Multi-band Signals
We now consider signals that are supported over multiple intervals. These can model, for
example, transmission in communication systems where users multiplex messages over
disjoint time or frequency intervals of the electromagnetic spectrum, and the receiver
observes a multi-band signal composed of the superposition of the different transmitted
waveforms in the time or frequency domain. An example is depicted in Figure 3.2,
where an ideal real-valued signal
f(t) = f0(t) + f1(t)cos(ω1t) + f2(t)cos(ω2t)
(3.98)
is composed of ﬁve disjoint frequency bands {n}, multiplexing information from three
different users, and the occupied portion of the spectrum is 5 = {∪nn} for −2 ≤n ≤2.
05
11:58:59, subject to the Cambridge Core

106
Functional Approximation
O
ω1
ω2
−ω1
−ω2
  = {   Δ }
5

ω
|F  (ω)|
0
|F (ω)|
1
|F (ω)|
2
Δ1
Δ2
Δ−2
Δ−1
Δ0
n
n
|    f(ω)|
Fig. 3.2
Multi-band signal resulting from transmission from three different users.

ω
−
      
[−,]
T  
t
f(t)
|    f(ω)|
Fig. 3.3
Multi-band signal and its observation set.
In general, the different modulation techniques employed for transmission determine the
occupied portion of the spectrum.
To compute the number of degrees of freedom, we let ℓ⊂[−,] be the union of
ℓdisjoint, ﬁxed intervals in the frequency domain, over which the signal is transmitted,
and Tκ be the union of κ disjoint, ﬁxed intervals in the time domain where the signal is
observed – see Figure 3.3. We assume the normalized energy constraint (2.43).
The Fredholm integral equation (2.57) arising from Slepian’s concentration problem
becomes, in this case,
BℓTTκψ(t) = λψ(t), t ∈Tκ,
(3.99)
where the operators TTκ and Bℓcorrespond to timelimiting over the set Tκ, and
bandlimiting the signal over the set ℓ. Using F to indicate the Fourier transform, and
deﬁning the indicator function
1Tκ(t) =
 1
if t ∈Tκ,
0
otherwise,
(3.100)
05
11:58:59, subject to the Cambridge Core

3.5 Extensions
107
they are deﬁned as
TTκf = 1Tκf,
(3.101)
Bℓf = F−11ℓFf.
(3.102)
The operator in (3.99) can be compactly represented as TTκBℓTTκ and is positive,
self-adjoint, compact, and bounded by one. It admits a countable set of eigenvalues,
representing the asymptotic dimension of the signals in Bℓ, while its eigenfunctions
correspond to the optimal approximating basis. Considering the norm
∥f∥2 =

Tκ
f 2(t)dt,
(3.103)
we have that the number of eigenvalues in (3.99) that are above level ϵ corresponds
to the number of degrees of freedom Nϵ(Bℓ) of multi-band signals of total spectral
support ℓ, observed over the time support Tκ, namely to the dimension of the minimal
subspace approximating the elements of Bℓwithin ϵ accuracy over the set Tκ.
To obtain a concentration behavior, we let the spectral support ℓbe a ﬁxed set,
while the observation set scales as αTκ, with α →∞. Due to symmetry, the case in
which the observation set is ﬁxed and the spectral support scales as αℓ, with α →∞,
is completely analogous. For all 0 < ϵ < 1, we have
Nϵ(Bℓ) = m(ℓ)m(Tκ)
2π
α + κℓ
π2 log
1 −ϵ
ϵ

logα + o(logα)
= m(ℓ)m(Tκ)
2π
α + O(logα),
(3.104)
where m(·) indicates Lebesgue measure. This result should be compared with (2.132).
The latter equation is immediately recovered from (3.104) by letting ℓ= κ = 1,
m(1) = 2, αm(T1) = T. In the multi-band case, for ℓand κ greater than one, the
time bandwidth product in (2.132) is replaced by the product of the measures of the
occupied portions of the spectrum in the time and frequency domains, and we have that:
The occupied portion of the spectrum in the time and frequency domain determines,
up to ﬁrst order, the scaling of the number of degrees of freedom.
Compared to the single-interval case, the weak dependence of the number of degrees
of freedom on the approximation level ϵ still holds, since ϵ appears only in the
higher-order terms of the number of degrees of freedom. However, in the case of (3.104)
we also have a linear dependence in the second-order term on the product of the number
of occupied intervals κℓ. When one of the two support sets is scaled by the factor
α →∞, having ﬁxed the number of intervals and the level of approximation, the phase
transition is revealed, and we have
lim
α→∞
Nϵ(Bℓ)
α
= m(ℓ)m(Tκ)
2π
,
(3.105)
05
11:58:59, subject to the Cambridge Core

108
Functional Approximation
which is the analog of (3.17) in the multi-band case. The optimal approximating
eigenfunctions {ψn(t)} that are solutions of (3.99) depend in this case on the multi-band
sets ℓand Tκ.
We can also ask what it takes to drive the approximation error to zero. For all ν > 0,
interpolating a number of eigenfunctions
N = (1 + ν)m(ℓ)m(Tκ)
2π
α,
(3.106)
it is possible to approximate any multi-band signal with vanishing error as α →∞. The
analogous result for signals supported over single intervals in the time and frequency
domains is given in (2.131), namely, interpolating (1+ν)T/π prolate spheroidal wave
functions, the approximation error tends to zero as N0 →∞.
3.5.3
Signals of Multiple Variables
The notion of number of degrees of freedom and asymptotic dimensionality of the
signals’ space can be extended from signals of a single variable to signals of multiple
variables. This extension is particularly useful for electromagnetic waveforms that are
functions of space and time.
Consider two subsets P and Q of RN having ﬁnite measure; an arbitrary point p ∈RN;
and a positive scalar α > 0. We indicate by αP the set of points of the form αx with x ∈P;
we immediately have that
m(αP) = αNm(P),
(3.107)
where m(·) represents Lebesgue measure in RN. Similarly, for x = (x1,x2,...,xN), we
have
m{(αx1,x2,...,xn) : x ∈P} = αm(P).
(3.108)
For any two points x = (x1,...,xN), u = (u1,...,uN) we let x·u = x1u1 +···+xNuN. We
consider square-integrable signals f ∈RN with Fourier transform
Ff =

RN f(x)e−jx·udx,
(3.109)
and subject to the energy constraint

RN f 2(x)dx ≤1.
(3.110)
Among these, we pick the subsets of signals supported in P and those whose Fourier
transform is supported in Q. These will play the role of the sets of timelimited and
bandlimited signals considered previously. Accordingly, we let
TP = {f : f(x) = 0, x ̸∈P},
(3.111)
BQ = {f : Ff(u) = 0, u ̸∈Q}.
(3.112)
05
11:58:59, subject to the Cambridge Core

3.5 Extensions
109
Letting the indicator function
1P(x) =
 1
if x ∈P,
0
otherwise,
(3.113)
we consider the following operators:
TPf = 1P(x)f(x),
(3.114)
BQf = F−11QFf =

RN h(x −y)f(y)dy,
(3.115)
where
Fh = 1Q.
(3.116)
It should be clear that for signals of a single variable supported over intervals, operators
analogous to (3.114) and (3.115) are the timelimiting and bandlimiting operators
corresponding to the multiplication by a rect (·) and convolution by a sinc(·) of
the time-domain representation of the signal, and the convolution by a sinc(·) and
multiplication by a rect (·) of the frequency-domain representation of the signal. We
consider here the general case of arbitrarily measurable sets in arbitrary dimensions.
The Fredholm integral equation (2.57) arising from Slepian’s concentration problem
for signals of a single variable is then generalized as
BQTPψ(x) = λψ(x), x ∈P,
(3.117)
and it can be compactly represented with the operator TPBQTP, which is positive,
self-adjoint, compact, and bounded by one. It admits a countable set of eigenvalues,
representing the asymptotic dimension of BQ, while its eigenfunctions correspond to
the optimal approximating basis. Considering the norm
∥f∥=

P
f 2(x)dx
1/2
,
(3.118)
the number of eigenvalues above level ϵ corresponds to the number of degrees of
freedom Nϵ(BQ), namely to the dimension of the minimal subspace approximating the
elements of BQ within ϵ accuracy over the set P.
In order to compute the number of degrees of freedom, we let Q be a ﬁxed set, while P
varies over the family αP′, with P′ ﬁxed. Due to symmetry, the asymptotic dimension of
TP can similarly be obtained by considering the operator BQTPBQ and scaling Q = αQ′
while keeping P ﬁxed. The situation is depicted in Figure 3.4 in the case of a space–time
ﬁeld with N = 4.
As α →∞and one of the two support sets is scaled to inﬁnity, the number of
signiﬁcant eigenvalues of the operator TαP′BQTαP′ undergoes a phase transition around
αN and the width of the transition is o(αN), so that the eigenvalues arranged in
non-increasing order appear, as a function of their indexes, as a step function when
viewed at the scale of αN. The number of degrees of freedom corresponds to the
transition point of this step function, that is of the order of αN, when the support set
is appropriately scaled by “blowing up” all of its coordinates.
05
11:58:59, subject to the Cambridge Core

110
Functional Approximation
P= P´
Q
k
k
ω
x
y
x
y
t
z
kz
O
O
α
α
Fig. 3.4
Scaling of the support set P.
n
λ
o(1)
1
n
N
α
ϵ
Fig. 3.5
Phase transition of the eigenvalues.
This result is depicted in Figure 3.5 and should be compared to the case of signals
of a single variable deﬁned over intervals depicted in Figure 2.13. In the case of signals
of multiple variables the width of the transition is characterized as o(1), as α →∞;
in the single-variable case the transition window is more precisely characterized as
O[(logN0)/N0]. The optimal approximating eigenfunctions are the prolate spheroidal
ones in the case of one-dimensional intervals, while in the general case they depend
on the sets of concentration P and Q. Despite these differences, the phase transition
determining the effective dimensionality of the space also occurs for bandlimited signals
of multiple variables deﬁned over arbitrary measurable sets, and is obtained by blowing
up either one of the support sets in the original or transformed domain, at rate α.
The precise result is stated as follows. For any arbitrary level of precision 0 < ϵ < 1,
we have
lim
α→∞
Nϵ(BQ)
αN
= (2π)−Nm(P′)m(Q).
(3.119)
As in the single-variable case, the scale αN of the transition is independent of the
accuracy level ϵ of the approximation, and the ϵ dependence appears hidden as a
pre-constant of the second-order term of the number of degrees of freedom. This is
05
11:58:59, subject to the Cambridge Core

3.5 Extensions
111
evident by rewriting (3.119) as
Nϵ(BQ) = (2π)−Nm(P′)m(Q)αN + o(αN).
(3.120)
The pre-constant of the term o(αN) also contains the dependence on the geometry of the
sets of concentration. This is made explicit, in terms of number of intervals, along with
the dependence on the approximation level, for the single-variable case in (3.104). The
analogous statement for signals of a single variable bandlimited to  and observed over
an interval of size T is easily recovered by letting N = 1, Q be the angular frequency
support, P be the time support, m(P′) = 1, m(Q) = 2, α = T, and letting T →∞, so
that the number of degrees of freedom in (3.119) becomes
Nϵ(B) = T/π + o(T) as T →∞,
(3.121)
in agreement with (3.16). In the case of (3.16), the transition is more precisely
characterized as being of the order of O(logT), and by (2.132) we also have an explicit
characterization of the ϵ dependence that diverges at most as log(1/ϵ) as ϵ →0, as
already noted.
We now summarize the results with the following statement:
The number of degrees of freedom of signals of multiple variables scales as
the product of the measure of the spectral support set times the measure of the
observation set, as all coordinates of one of the two sets tend to inﬁnity.
3.5.4
Hybrid Scaling Regimes
So far, spectral concentration has been achieved by scaling all coordinates of one of
the two support sets P and Q by the factor α. An analogous result holds by scaling any
combination of coordinates of both support sets. For example, for N = 2 we can let P
and Q be ﬁxed; consider the families
Pα = {(αx1,x2) : (x1,x2) ∈P},
(3.122)
Qα = {(x1,αx2) : (x1,x2) ∈Q},
(3.123)
and scale the ﬁrst coordinate of P and the second coordinate of Q by the factor α, while
maintaining the other coordinates ﬁxed. For any arbitrary level of precision ϵ > 0, let
Nϵ(α) be the number of eigenvalues of TPαBQαTPα not smaller than ϵ; then we have
lim
α→∞
Nϵ(α)
α2
= (2π)−2m(P)m(Q).
(3.124)
05
11:58:59, subject to the Cambridge Core

112
Functional Approximation
The coordinates’ scaling of the two support sets does not need to be uniform, and we
can also use two distinct scaling parameters in place of α. Consider the families
Pτ = {(τx1,x2) : (x1,x2) ∈P},
(3.125)
Qρ = {(x1,ρx2) : (x1,x2) ∈Q};
(3.126)
letting Nϵ(τ,ρ) be the number of eigenvalues of TPτ BQρTPτ not smaller than ϵ, we have
lim
τ,ρ→∞
Nϵ(τ,ρ)
τρ
= (2π)−2m(P)m(Q).
(3.127)
These results can be extended to any number of variables, where any combination of
coordinates’ scaling in the original or transformed domain can be performed. It follows
that the number of degrees of freedom of signals of multiple variables scales as the
product of the measure of the spectral support set times the measure of the observation
set, as any subset of coordinates of one set and the remaining coordinates of the other
set tend to inﬁnity.
A more general result holds for any invertible linear transform of the support sets,
subject to a limiting condition, that includes the scalings described above as special
cases. Let P and Q be measurable sets in RN. For any real matrix A of size N × N, we
indicate by AP the set of points of the form Ax, where x is the column vector composed
of the elements of x ∈P. We also indicate with |A| the determinant of A, with AT the
transpose of A, and with U the unit ball in L2(RN). We let A = A(τ) and B = B(ρ)
for real parameters τ and ρ. The case in which either matrix is constant, or depends on
multiple parameters, is completely analogous.
For any 0 < ϵ < 1, let Nϵ(A,B) be the number of eigenvalues of TAPBBQTAP not
smaller than ϵ. If
lim
(τ,ρ)→∞BTAU = RN,
(3.128)
then we have
lim
(τ,ρ)→∞
Nϵ(A,B)
|A||B| = (2π)−Nm(P)m(Q).
(3.129)
Equation (3.119) is a special case where A is a scalar matrix and B is the identity.
Equation (3.127) corresponds to the special case
A =
 τ
0
0
1

, B =
 1
0
0
ρ

.
(3.130)
This result is shown in Appendix A.6 and is applied in Chapter 9 to determine the
number of degrees of freedom of bandlimited electromagnetic waveforms in time and
space.
05
11:58:59, subject to the Cambridge Core

3.6 Blind Sensing
113

ω
−
Q
[−,]
t
f(t)
|    f(ω)|
-T/2
m(Q)  ≤2′    2 
-T/2
Fig. 3.6
Illustration of a sparse multi-band signal observed over a single time interval.
3.6
Blind Sensing
The results in Sections 3.5.2 and 3.5.3 indicate that it is possible to reconstruct
square-integrable signals of spectral support Q from a number of measurements
performed along the dimensions of the space spanned by the eigenfunctions of (3.117).
The reconstruction procedure amounts to ﬁrst solving (3.117) to obtain the basis
functions {ψn} and the eigenvalues {λn}. From the behavior of the eigenvalues we can
then determine the number of functions in the approximation
fN(x) =
N

n=1
anψn(x)
(3.131)
that give a satisfactory error
eN =

P
[f(x) −fN(x)]2dx
(3.132)
as the observation set P is scaled up to inﬁnity. The coefﬁcients {an} are obtained by
integration:
an = 1
λn

P
f(x)ψn(x)dx.
(3.133)
Consider now a signal f(t) ∈B of a single variable t, subject to (2.43), and such
that its Fourier transform
F(ω) = 0, for ω ̸∈Q,
(3.134)
where Q is a subset of [−,] whose measure is at most 2′ ≪2, and the observation
set is P = [−T/2,T/2]. Since the portion of the occupied spectrum is much smaller
than the total bandwidth, we call this signal “sparse” in the frequency domain – see
Figure 3.6.
We deﬁne a measurement vector
y = Mf(t),
(3.135)
05
11:58:59, subject to the Cambridge Core

114
Functional Approximation
where M(·) is a linear measurement operator that maps multi-band signals into
M-dimensional vectors. Since f(t) ∈B, by (2.131) and (3.13) for any ν > 0 the signal
can be reconstructed with vanishing error over the observation interval as T →∞using
N measurements, where
N = (1 + ν)T/π.
(3.136)
In this case, each component of y corresponds to the projection of the signal onto a
prolate spheroidal basis function that is a solution of
BTTψ(t) = λψ(t), t ∈[−T/2,T/2].
(3.137)
On the other hand, by (3.106), if we have knowledge of the support set Q, namely of
the size and positions of all the sub-bands, then reconstruction with vanishing error is
also possible using S measurements, where
S = (1 + ν)′T/π.
(3.138)
In this case, each component of y corresponds to the projection of the signal onto an
eigenfunction of (3.117).
Assume now that we only know that f(t) is a multi-band signal supported over a
set of size 2′, but we do not know the sizes and positions of its sub-bands. Without
further investigation we can only conclude from the results above that a number
of measurements M sufﬁcient to perfectly recover f(t) over the observation interval
satisﬁes
S ≤M ≤N.
(3.139)
Determining the minimum value of M within these bounds is called the blind
sensing problem.
A basic result in blind sensing states that roughly 2S measurements are necessary and
sufﬁcient to blindly recover any multi-band signal f(t) without any error. It follows that
the price to pay for blind reconstruction compared to reconstruction with full knowledge
of the support set is a factor of two. More precisely, for any real multi-band signal f,
using a number of measurements
M = 2S,
(3.140)
we can construct a signal fM such that the approximation error
∥f −fM∥=
 T/2
−T/2
[f(t) −fM(t)]2dt
1/2
(3.141)
tends to zero as T →∞. On the other hand, there exist signals that cannot be
reconstructed without error as T →∞using a number of measurements
M = 2′T/π + o(T).
(3.142)
05
11:58:59, subject to the Cambridge Core

3.6 Blind Sensing
115
Combining (3.138), (3.140), and (3.142), deﬁning the measurement rate
R = M
T ,
(3.143)
and recalling that ν is arbitrary, we have that as T →∞the smallest number of
measurements per unit time that guarantees a vanishing error must satisfy
R > 2′/π.
(3.144)
Twice the occupied portion of the frequency bandwidth determines the smallest
measurement rate sufﬁcient for blind reconstruction of multi-band signals.
3.6.1
Robustness of Blind Sensing
As elegant as they are, the information-theoretic results described above lack some
practical considerations. For example, we may ask whether the approximation obtained
is robust to small perturbations of the measurement, and whether this approximation
can be obtained in a computationally efﬁcient way.
To start with the ﬁrst question, we consider the measurement
y = Mf(t) + e,
(3.145)
where e is a measurement error having vector norm
∥e∥=
 M

n=1
e2
n
1/2
.
(3.146)
We would like a small measurement error not to lead to a huge approximation error. For
this reason, we require that the norm of the approximation error (3.141) and the norm of
the measurement error (3.146) be linearly related. This requirement can be satisﬁed if
the measurement somewhat preserves distances; namely, if, for all f satisfying (3.134),
we have
1 −μ ≤∥Mf∥
∥f∥
≤1 + μ,
(3.147)
where μ < 1 is a positive constant. The smallest constant μ for which (3.147) holds is
called the isometry constant of the measurement operator M.
One can show that there exist operators satisfying (3.147), provided that
M = cμS,
(3.148)
where cμ ≥2 increases with decreasing μ. In this case, we can construct a signal fM(t)
such that, as T →∞,
∥f −fM∥≤kμ∥e∥.
(3.149)
It follows that by appropriately choosing the operator M we can ensure that small
perturbations in the measurements lead to correspondingly small values of the
approximation error.
05
11:58:59, subject to the Cambridge Core

116
Functional Approximation
Measurement operators with smaller isometry numbers require larger values of cμ
and lead to smaller values of kμ. Finding computationally efﬁcient reconstruction
methods using measurement maps satisfying (3.147), and having reasonably small
values for both constants cμ and kμ, is an area of research. The smallest possible
value kμ = 1 in (3.149) is easily achieved by using a number of measurements M =
(1 + ν)T/π, where ν is an arbitrarily small constant. In this case, using the noisy
measurements
bn = 1
λn
 T/2
−T/2
f(t)ψn(t)dt + en,
(3.150)
we can construct
fN(t) =
N

n=1
bnψn(t),
(3.151)
where the {ψn} are the eigenfunctions of (3.137). It follows that
fN(t) =
N

n=1
1
λn
 T/2
−T/2
f(t)ψn(t)dt ψn(t) +
N

n=1
enψn(t)
=
N

n=1
anψn(t) +
N

n=1
enψn(t).
(3.152)
Using the orthogonality of the {ψn}, and λn ≤1 for all n, it follows that
∥f −fN∥≤
$$$$$
N

n=1
enψn(t)
$$$$$ +
$$$$$f(t) −
N

n=1
anψn(t)
$$$$$
≤
 N

n=1
λ2
ne2
n
1/2
+
$$$$$f(t) −
N

n=1
anψn(t)
$$$$$
≤∥e∥+
$$$$$f(t) −
N

n=1
anψn(t)
$$$$$.
(3.153)
For M = (1 + ν)T/π, letting T →∞, the second term in (3.153) tends to zero.
Comparing this result with (3.149), we realize that using this number of mea-
surements may be overkill to have a reasonable approximation error. In practice, for
sufﬁciently sparse signals blind sensing can achieve an approximation error very close
to ∥e∥using a considerably smaller number of measurements. If ′ ≪, then larger
values of cμ in (3.148) may still ensure a small number of measurements while leading
to smaller values of kμ.
3.6.2
Fractal Dimension
The problem of signal reconstruction is related to that of determining the effective
dimensionality, or degrees of freedom, of the signals’ space. For bandlimited signals, the
effective number of dimensions can be identiﬁed with the Nyquist number N0 = T/π.
05
11:58:59, subject to the Cambridge Core

3.6 Blind Sensing
117
For multi-band signals for which the location and widths of all the sub-bands is known
a priori, it can be identiﬁed with “the sparsity number” S0 = ′T/π. On the other
hand, without any a priori knowledge, we need to account for the additional degrees
of freedom of allocating the sub-bands in the frequency domain, and the effective
dimensionality increases to 2S0.
To make these considerations precise, we consider an information-theoretic quantity
that measures the effective dimensionality of a set in metric space, namely its fractal
dimension, which corresponds to the rate of growth of the ϵ-entropy of successively
ﬁner discretizations of the space, and represents the “degree of fractality” of the set.
This is also known as the Minkowksi–Bouligand dimension, and we examine it in detail
in Chapter 12.
Consider multi-band signals of energy at most one that are in B and whose spectral
support is of measure at most 2′. Since these signals are bandlimited, they can be
approximated, with vanishing error as T →∞, by a set X of vectors of size N =
(1 + ν)T/π. Each vector contains the coefﬁcients of the optimal interpolation basis
for bandlimited signals.
The fractal dimension of X is deﬁned as
dimF(X ) = lim
ϵ→0
Hϵ(X )
−logϵ ,
(3.154)
where Hϵ is the Kolmogorov ϵ-entropy introduced in Chapter 1.
This deﬁnition is related to the number of measurements required for reconstruction.
By considering the Minkowski sum
X ⊕X = {x1 + x2 : x1,x2 ∈X },
(3.155)
we have the following basic result: with a number of measurements greater than
M = dimF(X ⊕X ),
(3.156)
it is possible to blindly reconstruct any multi-band signal with vanishing error as T →
∞. On the other hand, there exist signals that cannot be blindly reconstructed with
vanishing error as T →∞using a smaller number of measurements.
An intuitive interpretation of these results is as follows. The set of all multi-band
signals is the union of inﬁnitely many subsets, each corresponding to the multi-band
signals of a given sub-band allocation. The Minkowski sum in (3.155) takes into
account the additional degrees of freedom of allocating the sub-bands in the frequency
domain. Within any subset, any multi-band signal is speciﬁed by essentially dimF(X )
coordinates, but when considering the union of all subsets, it is speciﬁed by essentially
dimF(X ⊕X ) coordinates. For multi-band signals of a given sub-band allocation
we have dimF(X ⊕X ) = dimF(X ), while for multi-band signals of arbitrary
sub-band allocation we have dimF(X ⊕X ) = 2dimF(X ). It then follows that, in
general, the relevant information-theoretic quantity that characterizes the possibility of
reconstruction is the fractal dimension of the dilation set in (3.155).
05
11:58:59, subject to the Cambridge Core

118
Functional Approximation
1
2
3
4
...
...
N-1 N
1
2
3
4
...
...
N-1 N
X
x
Φ
Fig. 3.7
A discrete vector with a sparse representation.
3.7
Compressed Sensing
The blind sensing problem is closely related to the one of compressed sensing,
which attempts to reconstruct a sparse N-dimensional vector from a small number of
measurements. In this case, we consider a vector x such that
x = X,
(3.157)
where  is an N × N orthogonal matrix that plays a role similar to an inverse Fourier
transform in the continuous setting, and X is an N-dimensional vector with S < N
non-zero elements. If S ≪N, we say that X is a sparse representation of x – see
Figure 3.7.
We deﬁne a measurement vector
y = Ax,
(3.158)
where A is an M × N matrix, and M is the number of measurements.
Cleary, x can be recovered from N measurements by observing all the elements of x.
In this case, the N ×N measurement matrix A is diagonal. If we know the position of the
non-zero elements of X, then S measurements are also enough to perfectly reconstruct
x. In this case, each measurement extracts the nth coefﬁcient of X from −1x, and
the signal is recovered by performing a ﬁnal multiplication by . However, assuming
we only know that x has a sparse representation, but we do not know the positions of
the non-zero elements of X, without further investigation we can only conclude that a
number of measurements M sufﬁcient for reconstruction satisﬁes S ≤M ≤N.
Determining the minimum value of M within these bounds in this discrete setting
is called the compressed sensing problem.
A little thought reveals that any sparse vector can be blindly recovered from M = 2S
measurements, by choosing a measurement matrix A such that any 2S columns of A are
linearly independent. If we assume by contradiction that this is not the case, then there
are two distinct vectors x ̸= x′ with at most S non-zero elements such that Ax = Ax′.
This implies that A(x−x′) = 0. However, (x−x′) has at most 2S non-zero elements, so
there must be a linear dependence between 2S columns of A, which is impossible.
05
11:58:59, subject to the Cambridge Core

3.7 Compressed Sensing
119
A reconstruction procedure suggested from the above argument is the following: ﬁnd
the sparsest solution x∗compatible with the measurements (3.158); namely, let
x∗= argmin
" N

n=1
1{xn̸=0} : Ax∗= y
%
.
(3.159)
Unfortunately, solving this optimization problem is computationally prohibitive.
A more attractive alternative is to consider a measurement matrix whose elements are
the complex exponentials
An,k = exp(−j2πnk/N), n ∈{1,2,...,2S}, k ∈{1,2,...,N},
(3.160)
so that the 2S measurements correspond to 2S consecutive discrete Fourier coefﬁcients
of x:
yn =
N

k=1
xk exp(−k2πnk/N), n ∈{1,2,...,2S}.
(3.161)
The objective is now to recover the whole vector x from these coefﬁcients. A procedure
called Reed–Solomon decoding allows us to transform the problem into that of solving
an S × S Toeplitz system, and then taking an inverse discrete Fourier transform.
It can be shown that 2S measurements are also necessary for reconstruction – see
Problem 3.12. It follows that the same phase transition behavior occurring in the
continuous setting for blind sensing also occurs in the discrete setting for compressed
sensing.
In both the continuous and discrete settings, the number of linear measurements
necessary and sufﬁcient for reconstruction is equal to twice the sparsity level of the
signal.
The main differences between the two settings are as follows. The compressed
sensing formulation assumes knowledge of the matrix , corresponding to the basis
where the discrete signal is sparse. In the case of blind sensing, it is only assumed
that the signal does not occupy the whole frequency spectrum, but the discrete basis set
required for the optimal representation is unknown a priori. In addition, in blind sensing
the reconstruction error tends to zero as T →∞, while in compressed sensing perfect
reconstruction occurs for all N.
3.7.1
Robustness of Compressed Sensing
While computationally attractive and using the smallest possible number of mea-
surements, the reconstruction procedure based on Reed–Solomon decoding is not
entirely clear of obstacles either. It turns out that it is terribly ill-conditioned and very
sensitive to perturbations in the measurements: small measurement errors lead to huge
reconstruction errors.
05
11:58:59, subject to the Cambridge Core

120
Functional Approximation
Assuming a measurement model
y = Ax + e,
(3.162)
we would like the norm of the reconstruction error to be linearly related to the norm of
the measurement error. Such a robust reconstruction can be performed if, for all vectors
x having at most S non-zero elements, the measurement matrix A satisﬁes
1 −μ ≤∥Ax∥
∥x∥≤1 + μ,
(3.163)
where μ < 1 is a positive constant. The smallest constant μ for which (3.163) holds is
the isometry constant of the measurement matrix A.
One can show that there exist matrices satisfying (3.163), provided that
M = cμSlog(N/S),
(3.164)
where cμ increases with decreasing μ. In this case, we can construct x∗by solving
x∗= argmin
" N

n=1
|xn| : Ax∗= y
%
,
(3.165)
and the reconstructed vector satisﬁes
∥x −x∗∥≤kμ ∥e∥.
(3.166)
Since (3.165) is a convex optimization problem, it can be efﬁciently solved by linear
programming methods. The same procedure can also be applied to reconstruct vectors
x that are not necessarily sparse, namely they can have more than S non-zero elements,
using only S measurements. In this case, we have
∥x −x∗∥≤kμ ∥e∥+ k′S−1/2
N

n=1
|xn −x(S)
n |,
(3.167)
where x(S) is the vector x with all but the largest S components set to zero. If x has
only S non-zero components then x = x(S), and (3.167) reduces to (3.166). In general,
the error is the sum of two contributions. The ﬁrst is proportional to the measurement
error. The second contribution corresponds to the error one would obtain in the absence
of measurement error and by knowing the location of the S largest values of x and
measuring those directly. In this case a converse result also holds, namely if there exists
a matrix A and a reconstruction algorithm such that
∥x −x∗∥≤k′S−1/2
N

n=1
|xn −x(S)
n |,
(3.168)
then the number of measurements must satisfy
M ≥cSlog(N/S),
(3.169)
where c depends on k′ but does not depend on the isometry number of A.
05
11:58:59, subject to the Cambridge Core

3.7 Compressed Sensing
121
Encoder
Decoder
x
y
x
Fig. 3.8
Source coding view of compressed sensing.
3.7.2
Probabilistic Reconstruction
The problem of compressed sensing can also be formulated in a probabilistic setting. In
this case, the discrete signal to be recovered is modeled as a stochastic process and
the objective is to reconstruct the signal with arbitrarily small probability of error,
given a sufﬁciently long observation sequence. Viewing the measurement operator
as an encoder and the reconstruction operator as a decoder acting on a sequence of
independent, identically distributed (i.i.d.) real-valued random variables, in the parlance
of communication theory this setup corresponds to lossless source coding of analog
memoryless sources when the encoding operation is multiplication by a real-valued
matrix. This coding setup is depicted in Figure 3.8.
A real vector x ∈RN is mapped into y ∈RM by a linear encoder (compressor)
C : RN →RM corresponding to the given measurement matrix. The decoder (decom-
pressor) D : RM →RN corresponds to the reconstruction operator. The decoder receives
the measured signal y and outputs the reconstructed signal ˆx. The compression rate is
given by the number of measurements divided by the signals’ dimension,
R = M
N .
(3.170)
Modeling x as a random vector composed of N independent random variables
distributed as X, we deﬁne R∗(X,ϵ) as the smallest number of measurements per signal
dimension R > 0 such that there exist a sequence of encoders C(N) : RN →R⌊RN⌋and
decoders D(N) : R⌊RN⌋→RN that for sufﬁciently large N make the probability of error
arbitrarily small, namely
P(D(N)C(N)(X1,X2,...,XN) ̸= (X1,X2,...,XN)) ≤ϵ.
(3.171)
Compared to the deterministic compressed sensing setup, where reconstruction
is required for all possible source signals, here the performance is measured on a
probabilistic basis by considering long block lengths and averaging with respect to
the distribution of the source signal. The model can be easily extended to include a
measurement error, and imposing different constraints on the design of the encoder and
decoder.
To capture the notion of sparsity in a probabilistic setting, we consider the following
mixture distribution for the source signal:
pX(x) = (1 −γ )δ(x) + γ p′(x),
(3.172)
where δ(·) is Dirac’s distribution, 0 ≤γ ≤1, and p′ is an absolutely continuous
probability measure. By the law of large numbers, the weight γ in (3.172) represents, for
large values of N, the level of sparsity of the signal in terms of the fraction of its non-zero
05
11:58:59, subject to the Cambridge Core

122
Functional Approximation
elements. Given this source model, a basic result for probabilistic reconstruction is
that the threshold R∗for the smallest measurement rate that guarantees reconstruction
with arbitrarily small probability of error depends only on the sparsity level and is
independent of p′, and we have
R∗(X,ϵ) = γ .
(3.173)
It follows that, in a probabilistic setting the number of linear measurements necessary
and sufﬁcient for reconstruction with negligible probability of error is equal to the
sparsity level of the signal and is independent of the prior distribution of its non-zero
elements.
This result should be compared with the analogous one in the deterministic setting,
where a number of measurements equal to twice the sparsity level is required to
perfectly reconstruct any signal in the space. The weaker requirement of probabilistic
reconstruction compared to reconstruction for all instances of the problem yields an
improvement in the number of measurements of a factor of two.
3.7.3
Information Dimension
The operational deﬁnition of R∗(X,ϵ) can be given an equivalent information-theoretic
formulation in terms of “information dimension,” introduced by Rényi (1959). For any
real random variable X, consider its quantized version Xϵ obtained from the discrete
probability measure induced by partitioning the real line into intervals of size ϵ and
assigning to the quantized variable the probability of lying in each interval, as described
in Section 1.4.4.
The information dimension of X is then deﬁned as
dimI(X) = lim
ϵ→0
HXϵ
−logϵ ,
(3.174)
where H is the Shannon entropy introduced in Chapter 1.
In the case that the limit in (3.174) does not exist, then lower and upper information
dimensions are deﬁned by taking liminf and limsup, respectively.
The deﬁnition immediately extends to random vectors and should be compared to
its deterministic counterpart (3.154). For a vector of N independent random variables
distributed as X, we have
dimI(X1,X2,...,XN) = NdimI(X).
(3.175)
Assuming the limit exists, if H(⌊X⌋) is ﬁnite we have
0 ≤dimI(X) ≤1,
(3.176)
05
11:58:59, subject to the Cambridge Core

3.8 Summary and Further Reading
123
where the value zero is achieved if the variable is discrete, and the value one is achieved
if the variable is continuous – see Problem 3.13. For a mixture distribution such as
(3.172), we also have
dimI(X) = γ .
(3.177)
It follows by combining (3.173), (3.175), and (3.177) that the number of linear
measurements necessary and sufﬁcient for reconstruction of sparse random vectors
composed of N independent components having the mixture distribution (3.172) and
such that H(⌊X⌋) < ∞scales with the number of dimensions:
dimI(X1,X2,...,XN) = γ N.
(3.178)
On the other hand, the number of linear measurements necessary and sufﬁcient for
the reconstruction of random vectors composed of N independent components having
an absolutely continuous distribution and such that H(⌊X⌋) < ∞is
dimI(X1,X2,...,XN) = N.
(3.179)
By comparing (3.178) and (3.179), it follows that while the geometrical and
information-theoretical concepts of dimension coincide for absolutely continuous
probability distributions, the information-theoretic dimension shrinks for sparse signals.
Mathematically, this is due to the presence of the discrete measure δ(·), which forces a
fraction of elements of the vector to zero. The practical implication is that this allows
reconstruction of the signal using a smaller number of measurements.
Similar considerations apply to the continuous case for the fractal dimension
introduced in Section 3.6.2. In Chapter 12 we show that for any A ⊂RN such that
m(A) < ∞, we have
dimF(A) = N.
(3.180)
On the other hand, for sets in an inﬁnite-dimensional space such as multi-band signals,
the effective dimensionality can be computed using an approximation argument, and
this leads to the number of measurements (3.156).
3.8
Summary and Further Reading
Slepian’s problem has been placed in a more general setting considering N-widths
in approximation theory. We have limited the treatment to N-widths in the sense of
Kolmogorov (1936), but alternative deﬁnitions also exist. The main reference for this
area of analysis is the book by Pinkus (1985). The concept of N-width naturally leads
to the deﬁnition of degrees of freedom of signals, which again we have taken from an
analogy with physics. An historical perspective on the singular value decomposition
can be found in Stewart (1993). Classic reference texts for functional representations
are by Hilbert and Courant (1953), Kolmogorov and Formin (1954), and Riesz and
Nagy (1955). More recent treatments, among others, are by Reed and Simon (1980)
and Conway (1990).
05
11:58:59, subject to the Cambridge Core

124
Functional Approximation
We have pointed out that the concentration problem is a special case of a self-adjoint
Hilbert–Schmidt kernel acting on the space spanned by the prolate spheroidal functions.
Other bounded operators can also be studied in the same framework, and their
information content is limited by an analogous analysis of their singular values. This
is further explored in subsequent chapters in the context of electromagnetic propagation
and scattering.
For the computation of the N-widths we followed Jagerman (1969) and
Pinkus (1985). The triangle inequality trick of Section 3.5.1 is taken from Bucci and
Franceschetti (1989). Results for multi-band signals are by Landau and Widom (1980).
Extensions to higher dimensions are by Landau (1975). The hybrid scaling results in
higher dimensions are by Franceschetti (2015).
The problem of optimally reconstructing multi-band signals without knowledge of
their support sets in the time and frequency domains is a topic of research called blind
sensing, and related to the ﬁeld of compressed sensing. The main results for blind
sensing that we presented are by Lim and Franceschetti (2017a). Earlier results that
require knowledge of the number of sub-bands and their widths were given by Feng and
Bresler (1996a, 1996b), Venkataramani and Bresler (1998), Mishali and Eldar (2009),
Izu and Lakey (2009), and Davenport and Wakin (2012). The Minkowski–Bouligand
dimension is a way of determining the fractal dimension of a set in a metric space
and is related to the number of degrees of freedom for reconstruction of multi-band
signals. It is named after the German mathematician Hermann Minkowski and the
French mathematician Georges Bouligand – see Falconer (1990).
There is a huge literature on compressed sensing that includes both theoretical
and practical developments, and we have given only a sample of some of the key
results in the area. The topic was ignited by the papers by Candés, Romberg, and
Tao (2006) and Donoho (2006). Some key results are also summarized in a note
by Candés (2008). The reviews by Candés (2006) and Candés and Wakin (2008)
provide excellent introductions. The monograph by Foucart and Rauhut (2013) gives
a rigorous and complete description of the main theoretical aspects. Wu and Verdú
(2010, 2012) studied the problem from an information-theoretic perspective and
related the possibility of reconstruction to the information dimension, introduced
by Rényi (1959). Additional information-theoretic results are given by Donoho,
Javanmard, and Montanari (2014). A ﬁrst operational characterization of the Rényi
information dimension appeared in Kawabata and Dembo (1994).
3.9
Test Your Understanding
Problems
3.1
Prove (3.5) following the steps given in Section 3.3.1.
3.2
Verify Eq. (3.54).
05
11:58:59, subject to the Cambridge Core

3.9 Test Your Understanding
125
P
N
√
Fig. 3.9
Approximating a sphere with a two-dimensional plane.
3.3
“A Slepian’s theorem.” Using the results on the Kolmogorov N-width and on the
number of degrees of freedom, prove the following theorem that appears in Slepian
(1976):
Consider the set of bandlimited signals E (ϵT) and the L2(−∞,∞) norm. For any
ϵ > ϵT, we have
lim
T→∞
Nϵ(E (ϵT))
T
= 
π .
(3.181)
3.4
Provide the analogous statement of the theorem in Problem 3.3 for the set of
signals E (ϵ) timelimited to [−T/2,T/2] whose fraction of energy outside the interval
[−,] is at most ϵ2
.
3.5
Compute the number of degrees of freedom N√
2ϵT(E (ϵT)) in the L2(−∞,∞)
norm, as T →∞.
3.6
Compute the number of degrees of freedom NϵϵT(E (ϵT)) in the L2[−T/2,T/2]
norm, as T →∞.
3.7
Show that dN(UN+1) = √λN, where UN+1 is deﬁned in (3.31).
Solution
This can be proven to an arbitrarily high level of rigor. We ﬁrst sketch the proof
using some simple geometric considerations. It is useful to visualize the problem in
three dimensions, i.e., let N + 1 = 3. In this case, we wish to approximate points in a
sphere of radius
√
λ by points in an intersecting two-dimensional plane. According
to the deﬁnition (3.3), we wish to choose a two-dimensional plane for which the
“worst” point in the sphere can be “best” approximated by a point on the plane.
The best such choice is given by any plane passing through the center of the sphere,
for which the “worst element” (farthest point) on the sphere is within a distance of
√λN from the point on the plane corresponding to the center of the sphere – see
Figure 3.9.
To make the above sketch more precise, consider an N-dimensional subspace XN and
pick a point y ̸∈XN. Let x be a point of XN that minimizes the distance to y and let the
05
11:58:59, subject to the Cambridge Core

126
Functional Approximation
x
y
ru
w
n
O
u
Fig. 3.10
Construction of the best approximation.
unit norm vector
u =
y −x
∥y −x∥.
(3.182)
Clearly, there exists a scalar r ∈R such that ru ∈XN. Let
w =
 −√λNu
if r ≥0,
√λNu
if r < 0.
(3.183)
Now observe that the given choice of w maximizes the distance of any point z ∈UN+1
from XN, namely
sup
z∈UN+1
inf
a∈XN ∥z −a∥= inf
a∈XN ∥w −a∥,
(3.184)
and this maximal distance can be computed as follows:
inf
a∈XN ∥w −a∥= ∥w −ru∥
=
	
λN + |r|.
(3.185)
The above expression is then minimized by letting y lie on XN. In this case r = 0, the
subspace XN passes through the center of the ball of radius
√
λN, and we have
dN(UN+1) = inf
XN sup
z∈UN+1
inf
a∈XN ∥z −a∥
= ∥w∥
=
	
λN;
(3.186)
see Figure 3.10.
3.8
Show that for ϵT > √1 −λN, we have dN(E (ϵT)) = 1 using the L2(−∞,∞) norm,
and dN(E (ϵT)) = √λN using the L2[−T/2,T/2] norm.
Solution
We give only a sketch of the proof. A signal f ∈E (ϵT) that can have an energy
spill outside [−T/2,T/2] greater than 1 −λN must lie outside the span of the most
concentrated basis functions {ψ0,ψ1,...,ψN−1}, since a linear combination of those can
only generate signals with an energy spill at most equal to 1 −λN. It follows that f can
have some energy along a direction orthogonal to all of these basis functions. Letting
05
11:58:59, subject to the Cambridge Core

3.9 Test Your Understanding
127
all of its energy lie along this orthogonal direction leads to the desired result on the
N-width.
3.9
Show how the N-widths of bandlimited signals change if instead of a unit energy
constraint we assume that
 ∞
−∞
f 2(t)dt ≤E.
(3.187)
Does the number of degrees of freedom change as well?
3.10
Consider the Hilbert–Schmidt operator K′K : L2(−,) →L2(−,) with
kernel
K′K(ω,ω′) =
 T/2
−T/2
K(t,ω)K∗(t,ω′)dt.
(3.188)
Verify that it is self-adjoint on B.
Solution
For all g ∈B,
K′Kg(ω) =
 
−
 T/2
−T/2
K(t,ω)K∗(t,ω′)g(ω′)dt dω′.
(3.189)
On the other hand, for all h ∈B,
(K′Kh)∗(ω) =
 
−
 T/2
−T/2
K∗(t,ω)K(t,ω′)h∗(ω′)dt dω′.
(3.190)
It then easily follows, using the compact notation deﬁned in Section 2.5.3, that
⟨K′Kg(ω),h(ω)⟩= ⟨g(ω),K′Kh(ω)⟩.
(3.191)
3.11
Show that (3.79), (3.82), and (3.56) tend to zero as N →∞, ensuring L2
convergence of the Hilbert–Schmidt representation.
Solution
The vanishing behavior of (3.56) is clear by compactness. For (3.79), we have that the
energy of the error is
eN(K) =
 b
a
 d
c
|K(x,y) −KN(x,y))|2dxdy
=
 b
a
 d
c
K(x,y) −
N−1

N=0
ψn(x)ϕ∗
n(y)

2
dxdy
=
 b
a
 d
c
|K(x,y)|2dxdy −2
N−1

N=0
λn +
N−1

N=0
λn
05
11:58:59, subject to the Cambridge Core

128
Functional Approximation
=
 b
a
 d
c
|K(x,y)|2dxdy −
N−1

n=0
λn.
(3.192)
Since the energy of the error is positive for all N, and K(x,y) is square-integrable, it
follows that
∞

n=0
λn < ∞,
(3.193)
and
lim
N→∞
∞

n=N
λn = 0.
(3.194)
A similar computation takes care of the convergence of (3.82).
3.12
Show that in the context of compressed sensing, 2S measurements are necessary
for reconstruction.
Solution
For reconstruction to be possible the measurement matrix A should map distinct
elements of its domain to distinct elements of its codomain. It follows that if A(x1 −
x2) = 0 then x1 = x2 for all sparse vectors x1 and x2. Consider the set X of all sparse
vectors in RN, and the set
Y = X ⊕X = {x1 + x2 : x1,x2 ∈X } = {x1 −x2 : x1,x2 ∈X }.
(3.195)
Letting y = x1 −x2, we have that if Ay = 0 then y = 0 for all y ∈Y , or, equivalently,
non-zero elements of Y cannot be in the null space of A. The set Y contains all the
vectors in RN that have a sparse representation with at most 2S non-zero elements.
We now argue that if the number of rows of A is smaller than 2S then there exists
a non-zero element of Y in the null space of A, which is a contradiction. To see
why this is the case, consider the subset Z ⊂Y of all vectors z that have a sparse
representation where all but the ﬁrst 2S coefﬁcients are zero. It is easy to see that
Z is a subspace of RN of dimension 2S. The null space condition is now equivalent
to having the dimension of the projection Az be at least 2S. This cannot occur if
the number of rows of A (corresponding to the number of measurements) is less
than 2S.
3.13
Show that the information dimension of a discrete random variable of ﬁnite
entropy is zero, and of a continuous random variable of ﬁnite differential entropy is
one.
05
11:58:59, subject to the Cambridge Core

3.9 Test Your Understanding
129
Solution
For a discrete random variable, the numerator of (3.174) coincides with its entropy
and the denominator diverges, so that the ratio tends to zero. For a continuous random
variable, as ϵ →0 by (1.50) the numerator of (3.174) diverges as hX −logϵ and the
denominator also diverges as −logϵ, so that the ratio tends to one.
05
11:58:59, subject to the Cambridge Core

4
Electromagnetic Propagation
“O tell me, when along the line
From my full heart the message ﬂows,
What currents are induced in thine?
One click from thee will end my woes.”
Through many an Ohm the Weber ﬂew,
And clicked the answer back to me, —
“I am thy Farad, staunch and true,
Charged to a Volt with love for thee.”1
4.1
Maxwell’s Equations
In this chapter we specify the signals used to transfer information as being of
electromagnetic type. We provide the background on electromagnetic waves that is
needed to apply the theory of orthogonal signal decomposition described in the previous
chapters, and characterize the information content of electromagnetic space–time ﬁelds.
The consideration at the basis of what is discussed here is that electric charges and
electric currents act as sources for electric and magnetic ﬁelds. An accurate model of this
phenomenon is provided by Maxwell’s equations. Presented by James Clerk Maxwell
to the Royal Society of London on December 8, 1864, these equations mark one of
the greatest advancements in science. Quantum theorist Richard Feynman went as far
as saying that their discovery should be regarded as the most signiﬁcant event in the
history of the nineteenth century. They predict the existence of electromagnetic waves
at all frequencies, traveling at the speed of light. They were ﬁrst veriﬁed experimentally
by Heinrich Hertz in 1887, eight years after Maxwell’s death, with the detection of
electromagnetic radiation at microwave frequencies; and were perhaps best exploited
commercially by the entrepreneur Guglielmo Marconi, founder in 1897 of the Wireless
Telegraph & Signal Company, which remained active until 2006, when it was eventually
acquired by the Swedish ﬁrm Ericsson.
1 James Clerk Maxwell, “Valentine by a telegraph clerk ♂to a telegraph clerk ♀” (circa 1860). Reprinted
in J. Warburg, ed. (1958) The Industrial Muse: The Industrial Revolution in English Poetry, Oxford
University Press.
06
11:58:59, subject to the Cambridge Core

4.1 Maxwell’s Equations
131
Table 4.1 Field constituents.
Symbol
Name
Units
e
Electric ﬁeld
V m−1
h
Magnetic ﬁeld
A m−1
d
Electric induction
C m−2
b
Magnetic induction
W m−2 = T
i
Electric current density
A m−2
ρ
Electric charge density
C m−3
In modern form, due to Oliver Heaviside, the equations are:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
∇× e = −∂b
∂t ,
∇× h = ∂d
∂t + i,
∇· d = ρ,
∇· b = 0,
(4.1a)
(4.1b)
(4.1c)
(4.1d)
where all the ﬁelds involved are space (r) and time (t) dependent. The ﬁeld constituents,
together with their measurement units, are summarized in Table 4.1. We can easily
derive the relation between the current and charge densities by substituting (4.1c) into
the divergence of (4.1b), yielding
∇· (∇× h) = ∂
∂t(∇· d) + ∇· i
= ∂ρ
∂t + ∇· i = 0.
(4.2)
The electric current and charge densities may include both induced (id,ρd) and
impressed (is,ρs) components, the latter being identiﬁed as sources of the ﬁeld and
excited by appropriate devices. Accordingly, we have
i = id + is,
(4.3)
ρ = ρd + ρs,
(4.4)
where, by deﬁnition,
∇· id + ∂ρd
∂t = 0,
(4.5)
∇· is + ∂ρs
∂t = 0.
(4.6)
The asymmetry in the equations (4.1) is due to the presence of electric charges
and currents only, not magnetic ones. Since there are no magnetic charges, magnetic
induction ﬁeld lines neither begin nor end but make loops or extend to inﬁnity and
back, and the total magnetic ﬂux through any Gaussian surface is zero. In other words,
the magnetic induction is a solenoidal vector ﬁeld. The asymmetry, however, is only an
06
11:58:59, subject to the Cambridge Core

132
Electromagnetic Propagation
apparent one. The equations can be extended in the natural way allowing for a density
of magnetic charge ρ(m) and a magnetic current density i(m):
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
∇× e = −∂b
∂t −i(m),
∇× h = ∂d
∂t + i,
∇· d = ρ,
∇· b = ρ(m).
(4.7a)
(4.7b)
(4.7c)
(4.7d)
Dirac’s (1931) work on relativistic quantum electrodynamics showed that the
hypothetical existence of magnetic monopoles in Maxwell’s equations, leading to
symmetric equations under the interchange of the electric and magnetic ﬁelds, would
explain the quantization of electric charges that is observed in practice.2 Another
symmetric representation is obtained when all electric charges and electric currents are
zero. This happens outside the region where sources are located, in materials where
induced currents and charges are not generated.
4.2
Propagation Media
We classify propagation media based on the properties of the electric and magnetic
induction. We focus on materials where the electric induction depends only on the
electric ﬁeld and the magnetic induction only on the magnetic ﬁeld. This is a common
case in practice, except for chiral media that include some classes of sugar solutions,
amino acids, and DNA. A ﬁrst distinction is between media that are dispersive and those
that are non-dispersive. For spatially non-dispersive media the induction at any point
in space depends only on the corresponding ﬁeld value at the same location in space.
Similarly, for time non-dispersive media the induction at any point in time depends
only on the corresponding ﬁeld value at that point in time. For dispersive media that are
linear and space–time invariant the relations between induction and ﬁeld are expressed
in terms of convolutions. For example, for spatially non-dispersive, time-dispersive,
linear, time-invariant media, we have
d(r,t) =
 ∞
−∞
fe(r,t −τ)e(r,τ)dτ,
(4.8)
b(r,t) =
 ∞
−∞
fh(r,t −τ)h(r,τ)dτ,
(4.9)
where the functions fe(·) and fh(·) are the electric and magnetic impulse responses of
the medium, i.e., the induction generated by Dirac-shaped unit ﬁelds.
2 In modern physics, grand uniﬁed theories and superstring theories allow for magnetic monopoles. The
detection of magnetic monopoles is an open problem in experimental physics and is considered by some
as the safest bet that one can make about physics not yet seen.
06
11:58:59, subject to the Cambridge Core

4.2 Propagation Media
133
Anisotropic media do not behave the same way in all directions, and have at least
one impulse response that is a dyad. On the other hand, for isotropic media the impulse
responses in (4.8) and (4.9) are scalar functions. Causality requires that induction cannot
appear at a time preceding its source application. It follows that fi(r,t) = 0 for t < 0,i ∈
{e,h}, and (4.8) and (4.9) become
d(r,t) =
 t
−∞
fe(r,t −τ)e(r,τ)dτ,
(4.10)
b(r,t) =
 t
−∞
fh(r,t −τ)h(r,τ)dτ.
(4.11)
This shows that the induction at time t and point r depends upon the ﬁeld existing at the
same point for all times preceding and including time t. The medium possesses a sort of
“hereditary”’ property that enforces its effect on the induction itself. This explains the
meaning of the term “time dispersive.”
If all impulse responses do not depend on r, the medium is called homogeneous. For
homogeneous, isotropic, (space and time) non-dispersive media, we have
fe(t) = ϵδ(t),
(4.12)
fh(t) = μδ(t),
(4.13)
and substituting into (4.10) and (4.11), we have
d(r,t) = ϵ e(r,t),
(4.14)
b(r,t) = μ h(r,t),
(4.15)
where ϵ [F m−1] and μ [H m−1] are the permittivity and permeability of the medium.
It is common to normalize these to the permittivity and permeability of the vacuum,
hence ϵ = ϵrϵ0, μ = μrμ0, where the normalization constants are ϵ0 = 8.84×10−12 and
μ0 = 1.256 × 10−6.
The relation between the electric ﬁeld and the induced current density for linear,
time-invariant media is typically of non-dispersive type,
id(r,t) = σe(r,t),
(4.16)
where σ [S m−1] is the conductivity of the medium.
It follows from (4.14), (4.15), and (4.16) that for homogeneous, isotropic,
non-dispersive media, the ﬁrst two Maxwell equations (4.1a) and (4.1b) simplify to
⎧
⎪⎪⎨
⎪⎪⎩
∇× e = −μ∂h
∂t ,
∇× h = ϵ ∂e
∂t + σe + is,
(4.17a)
(4.17b)
where the impressed current density is, i.e., the source of the ﬁeld, has been explicitly
indicated.
06
11:58:59, subject to the Cambridge Core

134
Electromagnetic Propagation
Table 4.2 Examples of homogeneous isotropic
media.
Non-dispersive
Time dispersive
σ = 0
Vacuum
Pure water
σ ̸= 0
Fog, rain, moisture
Ocean
Table 4.3 Examples of inhomogeneous media.
Isotropic
Anisotropic
σ ̸= 0
Stratiﬁed air
Stratiﬁed ionosphere
The simplest homogeneous, isotropic, non-dispersive medium is the vacuum, where
we also have σ = 0. A good approximation to the vacuum is clean air, provided that
the rate of change of the propagating ﬁelds in time is not too large (up to the GHz
range). In the presence of fog, rain, clouds, moisture, etc., the air can still be considered
homogeneous, isotropic, and non-dispersive, but with σ ̸= 0 and, as we shall see below,
the ﬁeld undergoes an exponential attenuation as it propagates, due to absorption.
Pure (distilled) water is a homogeneous, isotropic, time-dispersive medium with
σ = 0. Water is composed of polar molecules that are forced to be oriented in the
direction of the propagating electric ﬁeld and contribute to the electric induction. This
process requires time, resulting in the dispersive effect and hereditary property modeled
by (4.10). Hence, dispersion in the water is due to its electric properties, while its
magnetic response is the same as that of vacuum, i.e., fh(t) = μ0δ(t) and b = μ0h. When
water is not pure and free electrons are present, due for example to dissolved salts and
minerals, it can still be considered homogeneous, isotropic, and time dispersive, but
with σ ̸= 0.
The upper part of the atmosphere, where free electrons are present due to ionization
from the Sun, is called the ionosphere. This is an inhomogeneous medium, composed of
homogeneous layers, each anisotropic, time dispersive, and with σ ̸= 0. The anisotropic
effect is due to the free electrons, pushed by the propagating electric ﬁeld, interacting
with the earth’s static magnetic ﬁeld, so that the induced current is not necessarily
aligned with the electric ﬁeld and σ in (4.16) becomes a dyad.
The stratiﬁed layers of air immediately above sea level contain a vapor density proﬁle
and constitute another example of an inhomogeneous medium. The change of the vapor
content provides a corresponding local change in the electromagnetic properties of the
material. Examples of typical propagation media are given in Tables 4.2 and 4.3.
4.2.1
Perfectly Conductive Media
A perfect electric conductor is a homogeneous, isotropic, non-dispersive medium with
σ →∞. It shields itself from an elecromagnetic ﬁeld through the induction of a surface
charge and a surface current density. Inside the conductor, propagation is inhibited: the
06
11:58:59, subject to the Cambridge Core

4.2 Propagation Media
135
x
y
z
x
y
z
dielectric (z>0)
conductor (z<0)
Fig. 4.1
The boundary of a conductive medium.
electric ﬁeld is zero and there can only be a static magnetic ﬁeld. At the boundary, the
normal component of the electric ﬁeld is discontinuous, being zero inside and non-zero
outside the conductor. The tangential component of the electric ﬁeld is continuous and
equal to zero on both internal and external sides of the conductor. The time-varying
normal component of the magnetic ﬁeld is also continuous and equal to zero on both
sides of the conductor, while the time-varying tangential component of the magnetic
ﬁeld is discontinuous, being zero inside and non-zero outside the conductor.
All of the above properties follow directly from Maxwell’s equations. The electric
ﬁeld inside the conductor is zero, otherwise by (4.16) an inﬁnite current density would
be induced by any ﬁnite ﬁeld. This implies by (4.14) that the electric induction is also
zero. On the other hand, by (4.1a) and by (4.15) it follows that only a static magnetic
ﬁeld and induction can be present inside the conductor.
At the boundary of the conductor a ﬁeld discontinuity arises if an electromagnetic
ﬁeld is present outside it. Due to the outside ﬁeld, a surface charge density ˜ρd [C m−2]
and a corresponding surface current density ˜id [A m−1] are induced on the surface
boundary of the conductive medium represented by the plane z = 0 in Figure 4.1.
Accordingly, we have
ρd = ˜ρd δ(z) [Cm−3],
(4.18)
id = ˜id δ(z) [Am−3].
(4.19)
Integrating Maxwell’s equation (4.1c) over the small volume V = xyz placed
at the boundary of the conductor, as depicted in Figure 4.1, and applying Gauss’ theorem
(B.34), we have
lim
z→0
  
V
∇· d dV = (d · ¯z) xy
= lim
z→0
  
V
˜ρdδ(z) dV = ˜ρdxy,
(4.20)
06
11:58:59, subject to the Cambridge Core

136
Electromagnetic Propagation
where the ﬁrst equality follows from d being different from zero only outside the
conductor, so that the outgoing ﬂux is only computed for z > 0, and from the width
z tending to zero, so that the outgoing ﬂux is only through the upper surface xy. It
then follows that the component of the electric induction normal to the surface boundary
of the conductor is different from zero outside the conductor, due to the presence of
˜ρd ̸= 0. It also follows from (4.14) that the normal component of the electric ﬁeld is
different from zero outside the conductor. The normal components of the electric ﬁeld
and induction are discontinuous at the boundary, being zero on one side and different
from zero on the other side of the boundary.
We can also integrate (4.1d) over the volume V, and by a similar argument obtain
(b · ¯z)xy = 0,
(4.21)
from which it follows that the time-varying component of the magnetic induction
normal to the surface boundary of the conductor is continuous and equal to zero on both
sides of the boundary. From (4.15), it follows that the same holds for the time-varying
normal component of the magnetic ﬁeld.
Next, we focus on the components of the ﬁelds tangential to the boundary. Assuming
that the tangential component of the electric ﬁeld is directed along ¯x, we start computing
the ﬂux of (4.1a) over the cross section xz along the ¯y direction. By Stokes’ theorem
(B.35) we have
lim
z→0
 
S
∇× e dS = e × ¯z x
= lim
z→0
 
S
−∂b
dt dS = lim
z→0−∂by
dt xz
= 0,
(4.22)
where the ﬁrst equality follows from e being different from zero only outside the
conductor, so that the contour integral along the perimeter of S is only computed
for z > 0, and from the width z tending to zero, so that the contour integral reduces
to only a line integral along the segment x. It then follows that the component of the
electric ﬁeld tangential to the boundary of the conductor is continuous and equal to zero
on both sides of the boundary. Proceeding in the same way for (4.1b) and assuming that
the tangential component of the magnetic ﬁeld is along ¯x, we obtain
h × ¯z x = ˜idx,
(4.23)
showing that the tangential component of the magnetic ﬁeld is different from zero on
the outside boundary of the conductor due to the presence of ˜id ̸= 0. It follows that:
The induced current on the surface of the conductor is proportional to the tangential
component of the magnetic ﬁeld there.
06
11:58:59, subject to the Cambridge Core

4.2 Propagation Media
137
4.2.2
Dielectric Media
Dielectric media have σ < ∞and in contrast to perfect conductors they permit
propagation. Propagation is lossless for σ = 0 (perfect dielectric), lossy for σ > 0,
and completely inhibited for σ →∞(perfect conductors). The losses are due to small
amounts of induced currents and charges inside the dielectric that have an absorption
effect on the propagation process. As we shall see below, in the homogeneous, isotropic,
non-dispersive case, the propagation loss due to absorption is of exponential type and
depends on the ratio σ/ϵ, which dictates the rate of the exponential attenuation with the
propagation distance.
To determine the induced current inside the dielectric, it is useful to consider the
Fourier transform of (4.1b),
∇× H(r,ω) = jωD + I,
(4.24)
where, assuming the dielectric to be time non-dispersive, we have
D(r,ω) = ϵE(r,ω)
(4.25)
and
I(r,ω) = σE(r,ω).
(4.26)
It then follows that
∇× H = jωϵE + σE,
(4.27)
which can be rewritten in two different forms:
∇× H = jω

ϵ + σ
jω

E,
(4.28)
∇× H = (σ + jωϵ)E.
(4.29)
The ﬁrst case models propagation in a dielectric medium of (frequency-) dispersive
dielectric constant ˆϵ = ϵ +σ/(jω). The second case models propagation in a conductive
medium of (frequency-) dispersive conductivity ˆσ = σ + jωϵ. This latter case can be
also written as
∇× H = jωϵ0E + (σ + jω(ϵ −ϵ0))E,
(4.30)
where the induced current term is given by
Id = (σ + jω(ϵ −ϵ0))E.
(4.31)
It follows that:
The induced current inside the dielectric is linearly related to the electric ﬁeld there.
06
11:58:59, subject to the Cambridge Core

138
Electromagnetic Propagation
V
n
s
S
Fig. 4.2
The geometry corresponding to the Poynting–Umov theorem.
4.3
Conservation of Power
The (instantaneous, local) power surface density associated with the electromagnetic
ﬁeld is given by the Poynting–Umov vector, named after John Henry Poynting and
Nikolay Alekseevich Umov,
s(r,t) = e(r,t) × h(r,t) [Wm−2].
(4.32)
Computing the divergence of s,
∇· s = ∇· (e × h) = h · (∇× e) −e · (∇× h),
(4.33)
and using (4.17), we obtain for homogeneous, isotropic, non-dispersive media,
∇· s + ∂w
∂t + σe2 = −e · is,
(4.34)
where
w = 1
2ϵ e2 + 1
2μh2 [Jm−3].
(4.35)
Integration over a volume V closed by a bounding surface S, as depicted in Figure 4.2,
leads to
⃝

S
s · ¯n dS + ∂W
∂t + Pd = Ps,
(4.36)
where
Ps(t) = −

V
e · is dV,
(4.37)
Pd(t) =

V
σe2dV,
(4.38)
and
W(t) =

V
w dV.
(4.39)
06
11:58:59, subject to the Cambridge Core

4.4 Plane Wave Propagation
139
O
y
x
z
e
h
Fig. 4.3
Plane waves.
Relation (4.36), known as the Poynting–Umov theorem, is a conservation equation if
we read Ps(t) as the instantaneous power delivered to the ﬁeld by the sources inside
V, Pd(t) as the instantaneous power dissipated for joule effect inside the volume, W(t)
as the electromagnetic energy stored inside the volume at time t, and ∂W/∂t as the
corresponding instantaneous power. Hence, the power delivered by the sources, minus
the power converted into heat inside the volume, minus the outgoing ﬂux of the Poynting
vector outside the volume corresponds to the power stored inside the volume. For
lossless media σ = 0, so that by (4.38) there is no dissipated power and by (4.16) there
are no induced currents.
4.4
Plane Wave Propagation
We now study the canonical case of propagation far away from the sources and in a
one-dimensional space interval. In this interval propagation is described by the solution
of Maxwell’s equations in the absence of sources, enforcing a boundary condition
corresponding to the value of the electric ﬁeld at the beginning of the space interval
and a radiation condition along the positive z axis.
Consider an unbounded, homogeneous, isotropic, non-dispersive medium described
by the parameters ϵ = ϵ0ϵr, μ = μ0μr, σ, and ﬁelds e(z,t), h(z,t) that depend only on
one spatial coordinate z. In addition, we assume that the electric ﬁeld is polarized along
the x axis, namely e(z,t) = e(z,t)¯x. Let h = hx¯x + hy¯y + hz¯z. By (4.17a), we have
∂e
∂t ¯y = −μ
∂hx
∂t ¯x + ∂hy
∂t ¯y + ∂hz
∂t ¯z

,
(4.40)
showing that the magnetic ﬁeld components hx and hz are time invariant. Considering
propagation along z and disregarding the static components of the magnetic ﬁeld by
letting hx = hz = 0, we have that the magnetic and electric ﬁelds are mutually orthogonal,
and orthogonal to the direction of propagation z – see Figure 4.3. Letting hy = h,
06
11:58:59, subject to the Cambridge Core

140
Electromagnetic Propagation
O
e(z-ct )
z
c
t1
t2
Δz
Δz
i
c(t - t )=z - z =
2
1
 Δz
2
1
Fig. 4.4
Propagation in the lossless case.
expansion in component form of (4.17b) leads to
−∂h
∂z = ϵ ∂e
∂t + σe,
(4.41)
while (4.17a) becomes
∂e
∂z = −μ∂h
∂t .
(4.42)
Taking the derivative with respect to z of (4.42) and substituting into (4.41), we obtain
∂2e
∂z2 −1
c2
∂2e
∂t2 −1
c2τ
∂e
∂t = 0,
(4.43)
where c = 1/√ϵμ is the propagation velocity [m s−1] of the waveform in the medium
and τ = ϵ/σ [s] is called the relaxation time of the medium.
4.4.1
Lossless Case
In the lossless case, σ = 0, τ →∞, and (4.43) reduces to
∂2e
∂z2 −1
c2
∂2e
∂t2 = 0,
(4.44)
showing that any waveform with functional dependence of the type e(z ± ct) is
consistent with (4.44), because ∂e/∂z = −c∂e/∂t. In the following, we consider the
solution e(z−ct), namely propagation along the positive z axis. Accordingly, the electric
ﬁeld propagates along the positive direction of z, without any deformation, with velocity
c – see Figure 4.4.
The associated magnetic ﬁeld is obtained from either (4.41) or (4.42) by substituting
∂/∂z = −c∂/∂t, so that
ζh = e,
(4.45)
where ζ = √μ/ϵ [ohm] is the intrinsic impedance (in this case a resistance) of the
space. In free space, c = 3 × 108 m s−1 and ζ = 377 [ohm].
Finally, the (local, instantaneous) surface density power
s = e × h = 1
ζ e2(z −ct)¯z = ζh2(z −ct)¯z
(4.46)
06
11:58:59, subject to the Cambridge Core

4.4 Plane Wave Propagation
141
shows that the instantaneous power transported by the ﬁeld through a unitary surface
orthogonal to the z axis is proportional to the square of the ﬁeld and propagates along
the positive direction of the z axis with velocity c and without any attenuation.
4.4.2
Lossy Case
In the lossy case, we have to account for the presence of the last term in (4.43).
Assuming that the solution is of the type
e(z,t) = C(z,t)e(z −ct),
(4.47)
we consider the case of small losses, so that C(z,t) is a slowly varying function of z and
t, and its second-order derivatives can be neglected. Substituting into (4.43) and taking
into account that ∂e/∂z = −∂e/∂(ct), we get
∂e
∂z

2∂C
∂z + 1
cτ C

−1
c2
∂C
∂t

2∂e
∂t + e
τ

= 0.
(4.48)
A possible solution to (4.48) is
2∂C
∂z + 1
cτ C = 0,
∂C
∂t = 0.
(4.49)
By solving for C(z) and substituting into (4.47), we obtain that the plane wave solution
for the electric ﬁeld in a slightly lossy medium takes the form
e(z,t) = e(z −ct)exp

−z
2cτ

.
(4.50)
The magnetic ﬁeld h(z,t) is computed by substituting the solution (4.50) for e(z,t) into
(4.42), obtaining
∂e(z −ct)
∂t
exp

−z
2cτ

+ 1
2τ e(z −ct)exp

−z
2cτ

= ζ ∂h(z,t)
∂t
.
(4.51)
If the second term on the left-hand side can be neglected, using (4.50) we obtain
ζh(z,t) = e(z,t).
(4.52)
It follows that the electric and magnetic ﬁelds propagate along the positive direction of
z with velocity c, and their waveforms are not deformed but only exponentially scaled –
see Figure 4.5.
06
11:58:59, subject to the Cambridge Core

142
Electromagnetic Propagation
O
e(z,t)
z
c
t1
t2
t <t <t <t
1
2
3
t3
t4
4
Fig. 4.5
Propagation in a medium with absorption. The electromagnetic wave undergoes exponential
attenuation with the propagation distance.
The approximation made to obtain (4.52) from (4.51) is possible if

1
2τ e(z −ct)
 ≪

∂e(z −ct)
∂t
,
(4.53)
which holds for fast-varying waveforms inside media with large relaxation time. It can
also be easily shown that if this condition is satisﬁed, then the approximation made to
obtain (4.48) from (4.43), namely that the second derivatives of C(z,t) can be neglected,
also holds. An integral condition can be used for practical purposes by deﬁning the
effective time rate of change of the waveform as
1
T′ =




 ∞
−∞(∂e/∂t)2 dt
 ∞
−∞e2dt
,
(4.54)
and imposing the condition
T′ ≪2τ.
(4.55)
Finally, the (local, instantaneous) surface density power transported by the ﬁeld
through a unitary surface orthogonal to the z axis,
s = e × h = e2(z −ct)
ζ
exp

−z
cτ

¯z = ζh2(z −ct)exp

−z
cτ

¯z,
(4.56)
exponentially attenuates as the wave propagates in the medium, due to its ohmic losses.
4.4.3
Boundary Effects
We now consider plane wave propagation across two homogeneous lossless half-spaces
separated by the plane z = 0. A plane wave from the left half-space z ≤0 hits the right
half-space z ≥0 with an incident angle θ1. We let the incident electric ﬁeld be polarized
along ¯y,
e(s −c1t) = e(s −c1t) ¯y,
(4.57)
and propagate at velocity
c1 =
1
√ϵ1μ1
= c
n1
,
(4.58)
06
11:58:59, subject to the Cambridge Core

4.4 Plane Wave Propagation
143
where
n1 =
ϵ1μ1
ϵ0μ0
(4.59)
is the normalized refractive index of the medium, and s is the coordinate along the
direction of propagation. The magnetic ﬁeld is orthogonal to the electric one and to the
direction of propagation.
Considering two constant wavefronts orthogonal to the direction of propagation, the
distance between them is
ds = dxsinθ1 = dzcosθ1,
(4.60)
from which it follows that the velocities of propagation along the x and z axes are
cx = dx
dt =
c1
sinθ1
,
(4.61)
cz = dz
dt =
c1
cosθ1
.
(4.62)
Due to the discontinuity, reﬂected and transmitted waves are formed in the two
half-spaces, moving in reﬂected and transmitted directions at angles θ′
1 and θ2
respectively. The values of cx for the reﬂected and transmitted waves must be coincident
with the corresponding value for the incident ﬁeld in order to be able to enforce
continuity conditions of the tangential component of the ﬁeld at the boundary between
the two media. It follows that
c1
sinθ1
=
c1
sinθ′
1
,
(4.63)
c1
sinθ1
=
c2
sinθ2
,
(4.64)
where c2 is the velocity of propagation of the transmitted ﬁeld. We then obtain Snell’s
law, according to which the incident angle equals the reﬂection angle,
θ1 = θ′
1,
(4.65)
and the ratio of the sines of the angles of incidence and transmission equals the
reciprocal of the ratio of the indices of refraction,
sinθ1
sinθ2
= n2
n1
.
(4.66)
4.4.4
Evanescent Waves
As the wave passes the border between media, depending upon the refractive indices of
the two media, it will approach either the normal direction to the separating boundary,
or the tangent direction to the separating boundary – see Figure 4.6. When n2 < n1,
and the angle of incidence is large enough, Snell’s law (4.66) seems to require that
the sine of the angle of the transmitted wave is greater than one. This is, of course,
impossible, and the largest critical angle occurs at θ2 = π/2, so that the transmitted
06
11:58:59, subject to the Cambridge Core

144
Electromagnetic Propagation
wave travels along the separating surface in the y direction and its intensity is constant
along the wavefront. For larger values of sinθ1, the transmitted wave continues to travel
in the y direction along the separating surface, but becomes evanescent, exponentially
attenuating along the z coordinate – see Figure 4.7 and Problem 4.8.
4.5
The Wave Equation for the Potentials
We assume propagation in a homogeneous, isotropic, non-dispersive, lossless medium,
so that d = ϵe, b = μh, the permittivity ϵ and permeability μ are scalar constants, and
σ = 0. Since the magnetic induction b is solenoidal, it can be represented as
b = ∇× a,
(4.67)
where a [W m−1] is the vector potential.
1
z
x
2
n  >
2 n  1
1
z
x
2
n  <
2 n  1
Fig. 4.6
Snell’s law.
z
y
e(z)
x
Fig. 4.7
Evanescent waves.
06
11:58:59, subject to the Cambridge Core

4.5 The Wave Equation for the Potentials
145
Substitution into (4.1a) leads to
∇×

e + ∂a
∂t

= 0,
(4.68)
which deﬁnes an irrotational vector that can be represented as
e + ∂a
∂t = −∇φ,
(4.69)
where φ(r,t) [V] is the scalar potential.
It follows that the electromagnetic ﬁelds, e and h = b/μ, can be represented in terms
of the vector and scalar potentials via (4.67) and (4.69), respectively. Accordingly, we
proceed to the derivation of the deﬁning equations for a and φ as functions of the
impressed sources is(r,t) and ρs(r,t). Substituting (4.67) and (4.69) into (4.1b), with
id = 0 by virtue of (4.16) and σ = 0, leads to
∇× ∇× a + ϵμ∂2a
∂t2 + ϵμ∇∂φ
∂t = μis,
(4.70)
which can be rewritten in more convenient form by using the vectorial relation ∇2a =
∇∇· a −∇× ∇× a and recalling that ϵμ = 1/c2, yielding
∇2a −1
c2
∂2a
∂t2 = −μis + ∇

∇· a + 1
c2
∂φ
∂t

.
(4.71)
Similarly, substituting (4.69) into (4.1c), where ρd = 0 is constant by virtue of (4.5)
and can be taken equal to zero, and adding at both sides the term (∂2φ/∂t2)/c2, we get
∇2φ −1
c2
∂2φ
∂t2 = −ρs
ϵ −∂
∂t

∇· a + 1
c2
∂φ
∂t

.
(4.72)
The two equations (4.71) and (4.72), deﬁning the vector and scalar potentials, are now
coupled by the common term in the brackets, which can be made equal to zero by
enforcing the gauge invariance
∇· a + 1
c2
∂φ
∂t = 0.
(4.73)
To enforce the gauge invariance, notice that by redeﬁning the potentials,
a →a + ∇ψ, φ →φ −∂ψ
∂t ,
(4.74)
where ψ is an arbitrary differentiable function, and substituting the new potentials (4.74)
into (4.67) and (4.69), the magnetic induction b and the electric ﬁeld e do not change.
Now, substituting (4.74) into the gauge invariance equation (4.73), we obtain
∇· a + 1
c2
∂φ
∂t + ∇2ψ −1
c2
∂ψ
∂2t2 = 0,
(4.75)
06
11:58:59, subject to the Cambridge Core

146
Electromagnetic Propagation
and we can choose the function ψ to satisfy the equation
∇2ψ −1
c2
∂ψ
∂t2 = −

∇· a + 1
c2
∂φ
∂t

.
(4.76)
Considering the right-hand side of (4.76) as a source f(r,t), we obtain the wave equation
∇2ψ −1
c2
∂ψ
∂t2 = f(r,t).
(4.77)
This need not be solved explicitly; since any solution ψ ensures that the gauge
invariance condition is satisﬁed, the potentials decouple and we get the equivalent wave
equation forms
∇2a −1
c2
∂2a
∂t2 = −μis,
(4.78)
∇2φ −1
c2
∂2φ
∂t2 = −ρs
ϵ ,
(4.79)
and the electromagnetic ﬁelds do not change.
4.6
Radiation
The electromagnetic potentials introduced in the previous section can be used to study
the propagation of the ﬁeld radiated by given sources in the whole three-dimensional
space. As before, we consider a homogeneous, isotropic, non-dispersive, lossless
medium.
Let the source be an elementary electric dipole, composed of two opposite charges
±q(t) located along the z axis and separated by a distance ℓ. The dipole corresponds to
a source charge density [C m−3]
ρs(t,r) = q(t) δ(x) δ(y)
&
δ

z −ℓ
2

−δ

z + ℓ
2
'
= −q(t) ℓδ(x) δ(y) δ′(z),
(4.80)
where the last equality holds in the distributional limit as ℓ→0. A physical model
of the above idealized situation is that of a short conductive wire of length ℓwith
capacitors at its endpoints, where charges can accumulate. We now wish to determine
the corresponding source current density is = isz that ﬂows along the wire. From (4.6),
we get the scalar equation
∇· is + ∂ρs
∂t = ∂is
∂z + ∂ρs
∂t
= ∂is
∂z −∂q(t)ℓ
∂t
δ(x)δ(y)δ′(z)
= 0,
(4.81)
06
11:58:59, subject to the Cambridge Core

4.6 Radiation
147
x
y
z
r
e
er
h
p+
Fig. 4.8
Radiation from an elementary dipole.
and by integrating with respect to z and letting the dipole moment be p(t) = q(t)ℓ, we
obtain the source current density vector
is(r,t) = ∂p(t)
∂t δ(r)¯z.
(4.82)
Substituting the source current density (4.82) into the wave equation (4.78) and
solving the differential equation with the radiation condition at inﬁnity leads to the
vector potential
a(r,t) = μ
4πr
∂p(t −r/c)
∂t
¯z;
(4.83)
using (4.67) we can get the magnetic induction ﬁeld b, and from (4.15) the magnetic
ﬁeld h. Then, the electric ﬁeld e can be obtained from either (4.1a) or (4.1b). With
reference to a system of spherical coordinates (r,θ,φ), the ﬁnal result is depicted in
Figure 4.8 and is given by
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
er(r,t∗) = ζ
2π
 1
r2
dp(t∗)
dt
+ c
r3 p(t∗)

cosθ,
eθ(r,t∗) = ζ
4π
 1
cr
d2p(t∗)
dt2
+ 1
r2
dp(t∗)
dt
+ c
r3 p(t∗)

sinθ,
hφ(r,t∗) = 1
4π
 1
cr
d2p(t∗)
dt2
+ 1
r2
dp(t∗)
dt

sinθ,
(4.84a)
(4.84b)
(4.84c)
where t∗= t −r/c is the retarded time, namely the propagation time of the signal from
the origin of the coordinate system where the dipole is located to the receiving point at
distance r, and ζ = √μ/ϵ is the intrinsic impedance of the space.
The (local, instantaneous) density power is given by
s = e × h = eθhφ¯r −erhφ ¯θ.
(4.85)
06
11:58:59, subject to the Cambridge Core

148
Electromagnetic Propagation
Performing some calculations, the density power is divided into the sum of irreversible
and reversible components,
s = sirr + srev,
(4.86)
where
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
sirr(r,t∗) = ζ sin2 θ
(4πcr)2
d2p
dt2
2
¯r,
srev(r,t∗) = ζ sinθ
(4πcr)2
c
r
d
dt
(dp
dt
2
(¯rsinθ −¯θ cosθ)
+cp
r
dp
dt + c
2rp

(¯rsinθ −2¯θ cosθ)
'
.
(4.87a)
(4.87b)
To express the instantaneous radiated power associated with the vector sirr, it is
convenient to provide the relationship between the source current and the dipole
moment. Integration of (4.82) over (x,y) provides the source current vector i. With
further integration along z, we get
iℓ= ∂p
∂t ¯z.
(4.88)
The instantaneous radiated power associated with the vector sirr in a solid angle dθdφ
is now given by the ﬂux of (4.87a) over the surface r2dθdφ,
sirr · ¯r r2dθdφ =
ζ
(4πcr)2 sin2 θ
 d2p
dt∗2
2
r2dθdφ
=
ζℓ2
(4πc)2 sin2 θ
 ∂i
∂t∗
2
dθdφ,
(4.89)
where the second equality follows from (4.88). From (4.89) it follows that there is a
power ﬂux along the positive radial direction ﬂowing within any constant solid angle.
The total instantaneous power ﬂux radiated in space is given by
 2π
0
 π
0
ζℓ2
(4πc)2 sin2 θ
 ∂i
∂t∗
2
dθdφ = ζℓ2
(4c)2
 ∂i
∂t∗
2
;
(4.90)
this ﬂux is a constant, independent of r, for any given retarded time t∗. It follows that
there is a continuous transfer of energy between the antenna and inﬁnity. This energy
can be used for communication over long distances and, not being recovered from the
transmitting antenna, it is irreversibly lost – see Figure 4.9(a).
On the other hand, the power ﬂux associated with the vector srev for any given t∗
decreases along the radial direction so that there is no power ﬂux at inﬁnity; namely, in
any solid angle dθdφ we have
lim
r→∞srev · ¯r r2dθdφ = 0.
(4.91)
In this case, from (4.87b) it follows that the power ﬂux circulates around the dipole and
is continuously emitted and reversed into the generator associated with the transmitting
06
11:58:59, subject to the Cambridge Core

4.6 Radiation
149
z
y
z
y
(a)
(b)
0
0
0
0
Fig. 4.9
Orientation of the power density vector ﬁeld in the (y,z) plane: (a) irreversible component, (b)
reversible component.
antenna – see Figure 4.9(b). Furthermore, since by (4.87b) srev is given in terms of a time
derivative, it follows that integration over time at any point in space of the reversible
component gives zero total energy density, provided that the source current is of ﬁnite
duration, namely its extremal points are zero. This implies that this energy is eventually
completely recovered by the generator. The electromagnetic signal associated with
the reversible component of the power ﬂux is typically not used for communication
but ﬁnds other applications, like medical ones, wireless power transfer, and near-ﬁeld
microscopy.
4.6.1
The Far-Field Region
From (4.84), and using (4.88), it follows that at large distances from the source, the
dominant ﬁeld components are
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
eθ(r,t∗) = ζ
4π
1
cr
d2p
dt2 sinθ = ζ
4π
ℓ
cr
di
dt sinθ,
hφ(r,t∗) = 1
4π
1
cr
d2p
dt2 sinθ = 1
4π
ℓ
cr
di
dt sinθ.
(4.92a)
(4.92b)
We want to determine the condition on the distance at which the above approximation
holds. First, we deﬁne the effective time width of a pulsed waveform of maximum
amplitude e0 as the width of the corresponding rectangular window of amplitude e0
having the same energy as the waveform, namely
T =
 ∞
−∞e2dt
e2
0
.
(4.93)
06
11:58:59, subject to the Cambridge Core

150
Electromagnetic Propagation
As a preliminary example, consider now a Gaussian pulse
p(t) = p0 exp

−t2
2T2
0

.
(4.94)
This has effective time width T = √πT0, and we have
dp
dt = −πt
T2 p(t)
(4.95)
d2p
dt2 = −π
T2 p(t)

1 −πt2
T2

.
(4.96)
Substituting into (4.84), we note that since the ratios of the 1/r and the 1/r2, 1/r3 ﬁelds
are of the order of r/cT and (r/cT)2 respectively, the 1/r ﬁelds are dominant for r ≫cT.
Accordingly, (4.92) are referred to as far ﬁelds, in contrast to the other ﬁeld components
scaled by 1/r2 and 1/r3 that are referred to as near ﬁelds. Notice also that the far ﬁelds
are associated with the irreversible component of the power sirr, while the near ﬁelds are
associated with the reversible one srev.
The above example shows that the far ﬁeld deﬁnition (4.97) for the Gaussian
waveform depends on its effective time width. The same result holds referring to the
effective time width of general waveforms, and we have that the far-ﬁeld condition for
pulsed waveforms is
r ≫cT.
(4.97)
However, when the waveform is modulated by a sinusoidal carrier of fundamental
period Tc ≪T, one needs to refer to the effective width of the carrier, given by
 Tc/2
−Tc/2
sin2 ωtdt = Tc
2 .
(4.98)
It follows that the far-ﬁeld condition for modulated waveforms is
r ≫cTc
2 = λc
2 ,
(4.99)
where λc is the wavelength of the sinusoidal carrier.
Finally, the (local, instantaneous) density power in the far ﬁeld is
s = e × h = eθhφ¯r = 1
ζ e2
θ ¯r = ζh2
φ¯r =
1
(4π)2
ζ
(cr)2 sin2 θ

ℓdi
dt
2
¯r,
(4.100)
which coincides with the irreversible one and decreases as 1/r2. In the case of a lossy
medium, an additional exponential attenuation factor arises, as in the case of (4.56).
The total instantaneous radiated power in an angle dθdφ is given by
dP =
1
(4π)2
ζ
(cr)2 sin2 θ

ℓdi
dt
2
r2dθdφ,
(4.101)
which remains constant with the distance from the antenna, whereas the density
power decreases as 1/r2. The total instantaneous radiated power over a sphere enclosing
06
11:58:59, subject to the Cambridge Core

4.6 Radiation
151
z
r
L
Fig. 4.10
Radiation from a linear antenna.
the transmitter is obtained by integration, yielding
P(t∗) = ζ ℓ2
16c2
di
dt
2
,
(4.102)
which coincides with (4.90).
4.6.2
The Fraunhofer Region
The radiation from a linear wire antenna of length L along z in the far ﬁeld is
immediately given by generalizing (4.92) via superposition of the radiations from
elementary dipoles. Referring to Figure 4.10, we have
r′2 = r2 + z2 −2rzcosθ.
(4.103)
By solving for r′ and using the Maclaurin expansion of the square root up to terms of
order (z/r)2, we get
r′ ≃r −zcosθ.
(4.104)
The truncated expansion holds for z2 ≪2r. This condition leads to the deﬁnition of
the Fraunhofer condition, named after Bavarian physicist Joseph von Fraunhofer, where
geometrically r and r′ can be considered parallel. The Fraunhofer condition for antennas
of length L is
r ≫L
2
√
2.
(4.105)
In this case, we have
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
eθ(r,t∗) = ζ sinθ
4πcr
 L/2
−L/2
∂i(z, t∗+ (z/c)cosθ)
∂t
dz,
hφ(r,t∗) = 1
ζ eθ(r,t∗).
(4.106a)
(4.106b)
Notice that we have deﬁned the far ﬁeld with respect only to the time length of the
source, whereas we have deﬁned the Fraunhofer region with respect only to the spatial
06
11:58:59, subject to the Cambridge Core

152
Electromagnetic Propagation
Δ
Δa
S
n
t
Fig. 4.11
Flux through an elementary surface crossing the boundary S.
length of the source. The radiation from a linear antenna has been obtained assuming
both the far-ﬁeld and Fraunhofer region. Following the same line of reasoning, the
radiated power can also be generalized to the case of a linear antenna by simply
substituting ℓdi/dt in (4.100) with the appropriate integral along the antenna.
4.7
Equivalence and Uniqueness
Consider an electromagnetic ﬁeld (e,h) generated by sources bounded by a closed
surface S, and a new ﬁeld (e′,h′) coincident with the previous one outside S and equal
to zero inside S. We show that this new ﬁeld can be generated by removing all sources
inside S and placing electric and magnetic surface density currents on the boundary S:
"
˜i = ¯n × h,
˜i(m) = ¯n × e.
(4.107a)
(4.107b)
An immediate consequence of this result is the following:
Knowledge of the ﬁeld on the boundary surface S uniquely determines the ﬁeld
outside it.
To derive the result, notice that both the ﬁeld outside S and the null ﬁeld inside S
are separate solutions of Maxwell’s equations and are discontinuous at the boundary S.
The new impressed currents ensure that the discontinuity at the boundary S is veriﬁed.
We show the computation for the electric ﬁeld only – the magnetic case is completely
analogous. We compute the ﬂux of (4.7a) and (4.7b) across an elementary surface S
orthogonal to S, of length ℓand height a →0, as depicted in Figure 4.11, and apply
Stokes’ theorem (B.35). Letting ¯t′ be the unit vector tangent to S and normal to S, and
¯t the unit vector tangent to S and such that
¯t = ¯t′ × ¯n,
(4.108)
06
11:58:59, subject to the Cambridge Core

4.8 Summary and Further Reading
153
we have
lim
a→0

s
∇× e · ¯t′ ds = e · ¯tℓ
= lim
a→0

s
−∂b
∂t · ¯t′ ds −

s
i(m) · ¯t′ ds

= −i(m) · ¯t′ℓ,
(4.109)
where the ﬁrst equality follows from Stokes’ theorem (B.35) and the ﬁeld being
zero inside S, the second equality follows from Maxwell’s equation (4.7a), and the
last equality follows from having impressed the surface density current (4.107b). By
dividing both sides of (4.109) by ℓ, it follows that
e · ¯t = −i(m) · ¯t′.
(4.110)
Using (4.108), we also have
e · ¯t = e · ¯t′ × ¯n = −e · ¯n × ¯t′ = −e × ¯n · ¯t′.
(4.111)
Substituting (4.111) into (4.110), we obtain (4.107b).
The uniqueness of the solution of Maxwell’s equations ensures that the constructed
possible ﬁeld (e′,h′) is the only ﬁeld associated with the new sources satisfying the
given boundary conditions. In practice, the effect of the surface currents (4.107a) and
(4.107b) is to provide null boundary conditions on the internal face of S, so that the ﬁeld
in the region inside S is zero, while providing boundary conditions on the external face
of S corresponding to the ﬁeld values generated by the original sources, so that the new
ﬁeld outside S is the same as the old one.
The equivalence result can be regarded as a rigorous statement of the Huygens’
principle: each point over S becomes a source of radiation for the outside space,
while the internal ﬁeld is zero. This does not imply that the equivalent sources do
not individually generate any ﬁeld inside S, only that their collective result is zero. By
symmetry, the same result holds by removing all sources external to S and considering
a ﬁeld that is zero outside S and coincident with (e,h) inside S.
The equivalence result can greatly simplify certain radiation problems. A complicated
boundary value problem, such as a multiple scattering problem, can be reduced to the
problem of radiation from a closed surface surrounding the scattering system, provided
that one can compute the ﬁeld on this boundary. We make use of this property in the
context of determining the information content of scattered ﬁelds in Chapter 8.
4.8
Summary and Further Reading
We have illustrated the main elements of Maxwell’s theory of electromagnetic wave
propagation. Rather than following the classical approach of starting with steady-state
propagation of harmonic waves in the phasor domain, we developed the theory
for general waveforms in the time domain. This less standard approach favors the
06
11:58:59, subject to the Cambridge Core

154
Electromagnetic Propagation
comparison with the theory of signals elaborated in the previous chapters. We believe
that starting with the time domain is more natural, and it has the additional advantage
of being better suited to the information-theoretic analysis that is the main focus of
this book. The theory presented should provide the reader with a bit more than just a
ﬂavor of electromagnetics. A different presentation of the material can be found in many
classic texts, including the reprint of Maxwell’s (1873) original treatise. Among the
many modern classics, Jackson (1962), Stratton (1941), Papas (1965), and Van Bladel
(1985) are standard references. Another book that favors the time domain approach is
Franceschetti (1997).
We have shown that in homogeneous, isotropic, non-dispersive media, plane waves
propagate without deformation, and in the presence of small absorption they undergo
an exponential attenuation with the propagation distance. In Chapter 9 we show that
exponential attenuation due to absorption also occurs in the presence of multiple
scattering, where the medium effectively becomes analogous to a homogeneous but
dispersive one.
Radiation in free space shows that the elecromagnetic ﬁeld is generally related to
the derivative of the source currents. The far-ﬁeld condition is deﬁned with respect to
the effective time-width of the radiated pulse, or to the wavelength of the sinusoidal
carrier in the case of a modulated waveform. The Fraunhofer condition is deﬁned
with respect to the length of the transmitting antenna. The power associated with
the electromagnetic ﬁeld is subject to a conservation equation (the Poynting–Umov
theorem), and the radiated power is constant in any solid angle centered at the source.
The (local, instantaneous) density power is singular at the origin and is divided into a
reversible component that does not propagate at inﬁnity, and an irreversible component
that propagates in the radial direction. Finally, an electromagnetic ﬁeld generated by
sources bounded by a closed surface and measured in the space external to it can be
generated by placing equivalent sources on the boundary surface that depend only on
the ﬁeld there.
4.9
Test Your Understanding
Problems
4.1
Verify that summing (4.87a) and (4.87b) yields (4.86).
4.2
Determine the equivalence result (4.107a) for the magnetic ﬁeld.
4.3
Write the energy density for the electric and magnetic ﬁelds.
4.4
Show that the energy density for a plane wave in free space equals ϵ0e2, where e
indicates the electric ﬁeld.
4.5
The momentum density of the electromagnetic ﬁeld can be obtained from
Maxwell’s equations by computing the force density exerted by the sources of the ﬁeld,
06
11:58:59, subject to the Cambridge Core

4.9 Test Your Understanding
155
and it is given by
g = s/c2,
(4.112)
where s is the Poynting vector and c is the propagation velocity of the electromagnetic
ﬁeld. Use this formula to show that the magnitude of the momentum density for a plane
wave in free space equals ϵ0e2/c.
4.6
The electromagnetic ﬁeld carries energy and momentum in the direction of the
Poynting vector with velocity c. This results in a certain amount of pressure exerted
by the ﬁeld impinging on material objects. Compute the radiation pressure exerted by
the electromagnetic ﬁeld in free space on a totally absorbing screen. (Hint: the rate of
change in momentum over time is a force, and pressure is force spread over an area).
4.7
Recall from the discussion in Section 1.5.3 that radiation occurs in discrete quanta
of energy, called photons. The radiation pressure computed in Problem 4.6 can be
viewed as the aggregate effect of quantum particles hitting a given area. Use the
results of Problems 4.4 and 4.5 and Planck’s equation of energy (1.85) to compute the
momentum of a single photon, then compare the result with the relativistic calculation
performed in Section 2.3.2, leading to (2.34). The aggregate effect of these quanta
impinging over an area yields the radiation pressure.
4.8
Apply Snell’s law with n1 > n2 and sinθ1 > n2/n1, and discuss the features of the
transmitted wave for a sinusoidal signal.
Solution
For a sinusoidal transmitted waveform we have
cos(ωt −s/c2) = ℜexp[j(ωt −sω/c2)],
(4.113)
so that in the following we can refer to the corresponding complex exponential signal.
From Snell’s law (4.66) we have θ2 = π/2 if sinθ1 = n2/n1. For larger values of θ1 we
consider the complex extension
θ2 = π/2 −jθ′
2.
(4.114)
The phase changes of the signal along x and z are given by
x = ωt −ωxsinθ2
c2
,
(4.115)
z = ωt −ωzcosθ2
c2
,
(4.116)
where c2 is the velocity of the transmitted wave. We now have
sinθ2 = sin(π/2 −jθ′
2) = cos(−jθ′
2) = coshθ′
2,
(4.117)
cosθ2 = cos(π/2 −jθ′
2) = sin(−jθ′
2) = −jsinhθ′
2.
(4.118)
It follows that
x = ωt −ωxcoshθ′
2
c2
(4.119)
06
11:58:59, subject to the Cambridge Core

156
Electromagnetic Propagation
and
z = ωt + jωzsinhθ′
2
c2
.
(4.120)
Accordingly, the ﬁeld along the x direction is given by
ℜexp
&
j

ωt −ωxcoshθ′
2
c2
'
= cos

ωt −ωxcoshθ′
2
c2

,
(4.121)
which implies propagation along the x direction. On the other hand, along the z direction
the ﬁeld is given by
ℜexp
&
j

ωt + jωzsinhθ2
c2
'
= exp
&−ωzsinhθ′
2
c2
'
cos(ωt),
(4.122)
which implies that there is no propagation along the z direction but only exponential
attenuation along the z direction. This surface waveform is called an evanescent wave.
4.9
Find the relation between the incident and the imaginary part of the angle of
transmission for sinusoidal evanescent waves.
Solution
We enforce continuity of the phase change along x at z = 0. For the incident and reﬂected
waves, we have
x = ωt −ωxsinθ1
c1
,
(4.123)
and using (4.119) for the transmitted wave, we get
ωxsinθ1
c1
= ωxcoshθ′
2
c2
,
(4.124)
from which it follows that
sinθ1
coshθ′
2
= c1
c2
= n2
n1
.
(4.125)
The obtained expression (4.125) should be compared with (4.66).
06
11:58:59, subject to the Cambridge Core

5
Deterministic Representations
I present myself to you in a form suitable to the relationship I wish to achieve with you.1
5.1
The Spectral Domains
An information-theoretic analysis of the electromagnetic ﬁeld requires an appropriate
mathematical representation of this physical quantity. The linearity of the Maxwell
equations allows us to view the ﬁeld as the output of a linear system, excited by a
source signal. This yields a discrete representation in terms of the Hilbert–Schmidt
decomposition that provides the number of degrees of freedom, and thus the number
of channels that can be used for communication. This representation occurs in the
time–frequency and space–wavenumber domains, in a completely symmetric fashion.
5.1.1
Four Field Representations
Field quantities are functions of space and time, deﬁned in the domain provided
by their initial and boundary conditions. A suitable mathematical representation for
physically realizable sources and ﬁelds is the L2 space of square-integrable functions.
Occasionally, the space can be extended to include Dirac δ-distributions to model, for
example, idealized concentrated sources, such as point charges and line currents. All
mathematical derivations can be extended in this case, following appropriate limiting
arguments. It follows that ﬁelds can be represented as superpositions of spectral
components via space and time Fourier transforms. For example, the electric ﬁeld on the
whole space–time is represented by superposition of angular frequency components as
e(r,t) = 1
2π
 ∞
−∞
E(r,ω)exp(jωt)dω,
(5.1)
and by superposition of wavenumber components as
e(r,t) =
1
(2π)3

R3e(k,t)exp(jk · r)dk,
(5.2)
1 L. Pirandello (1917). The Pleasure of Honesty, act 1, scene 8. Reprinted in 1936 as Each in His Own Way
and Two Other Plays, E. P. Dutton.
07
12:00:56, subject to the Cambridge Core

158
Deterministic Representations
Fig. 5.1
Spectral solution Fourier transform pairs.
where
E(r,ω) =
 ∞
−∞
e(r,t)exp(−jωt)dt,
(5.3)
e(k,t) =

R3 e(r,t)exp(−jωk · r)dr.
(5.4)
Accordingly, we deﬁne the natural space–time (r,t) domain, the partial spectral
space–frequency (r,ω) and wavenumber–time (k,t) domains, and the full spectral
wavenumber–frequency (k,ω) domain. The spectral solutions in these domains are
e(r,t), E(r,ω), e(k,t), and E(k,ω). Their Fourier transform pairs are depicted in
Figure 5.1.
5.1.2
The Space–Frequency Spectral Domain
Maxwell’s equations (4.1) in the space–frequency domain are
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∇× E = −jωB,
∇× H = jωD + I,
∇· D = ρ,
∇· B = 0.
(5.5a)
(5.5b)
(5.5c)
(5.5d)
By (4.8) and (4.9), we have that for linear, space non-dispersive, time-invariant media,
D(r,ω) = ϵ(r,ω)E(r,ω),
(5.6)
B(r,ω) = μ(r,ω)H(r,ω),
(5.7)
where ϵ(r,ω) and μ(r,ω) are the Fourier transforms of the electric and magnetic
impulse responses of the medium. For isotropic media they are scalars; for anisotropic
media they are dyads. For homogeneous media they do not depend on r, and for time
non-dispersive media they also do not depend on ω.
07
12:00:56, subject to the Cambridge Core

5.2 System Representations
159
5.2
System Representations
The electromagnetic ﬁeld can be viewed as the output of a linear system excited by a
source signal. Viewing the source x(t) as the superposition of Dirac’s pulses,
x(t) =
 ∞
−∞
x(τ)δ(t −τ)dτ,
(5.8)
we deﬁne the Green’s function, named after British mathematical physicist George
Green, as the electromagnetic response to a unit pulse excitation. This function leads
to a number of ﬁeld representations that are used for the analysis and design of
communication systems, and to determine the amount of information carried by the
propagating wave.
5.2.1
Linear, Time-Invariant Systems
A linear, time-invariant system transforms an input signal x(t) into an output signal y(t)
that is given by the convolution integral
y(t) =
 ∞
−∞
g(t −τ)x(τ)dτ =
 ∞
∞
g(τ)x(t −τ)dτ,
(5.9)
where g(t) is the response to the Dirac impulse δ(t), and g(t) is the Green’s function of
the system,
g(t) =
 ∞
−∞
g(t −τ)δ(τ)dτ =
 ∞
−∞
g(τ)δ(t −τ)dτ;
(5.10)
see Figure 5.2. In the frequency domain, we immediately get, from (5.9),
Y(ω) = G(ω)X(ω),
(5.11)
where G(ω) is the spectral Green’s function, and
y(t) = 1
2π
 ∞
−∞
G(ω)X(ω)exp(jωt)dω.
(5.12)
Figure 5.3 gives the corresponding block diagram.
g(t-τ)δ(τ)dτ = g(t)
LTI
SYSTEM
t
∞
Fig. 5.2
Green’s function of a linear, time-invariant (LTI) system.
G(ω)
y(t)
x(t)
Fig. 5.3
Input–output representation of a linear, time-invariant system.
07
12:00:56, subject to the Cambridge Core

160
Deterministic Representations
g(r,t). z
LTIH
MEDIUM
δ(r)δ(t) z
Fig. 5.4
Green’s function of a linear, time-invariant, homogeneous (LTIH) medium.
5.2.2
Linear, Time-Invariant, Homogeneous Media
We now place the deﬁnitions above in the context of electromagnetic signals. For linear,
time-invariant, homogeneous media, the system is the propagation environment, the
input is the applied current at the transmitter, the output is the electromagnetic ﬁeld at
the receiver. The associated Green’s function is the ﬁeld radiated by a unit pulse current
density vector,
i(r,t) = δ(r)δ(t)¯z.
(5.13)
As described in Section 4.6, the ﬁeld is not generally oriented as the source current.
It follows that the Green’s function is a dyad, g(r,t), referred to as the dyadic Green’s
function – see Appendix B.5. A block diagram representation is given in Figure 5.4.
With reference to the electric ﬁeld in the natural domain, we have the space–time
convolution relation
e(r,t) =

V
 ∞
−∞
g(|r −r′|,t −τ) · i(r′,τ)dr′dτ,
(5.14)
where the ﬁrst integral is extended to the transmitting source volume V. Analogous
relations are immediately obtained in the partial spectral domains,
e(k,t) =
 ∞
−∞
g(k,t −τ) ·i(k,τ)dτ,
(5.15)
E(r,ω) =

V
G(|r −r′|,ω) · I(r′,ω)dr′,
(5.16)
and in the full spectral domain,
E(k,ω) = G(k,ω) ·I(k,ω).
(5.17)
The geometry of the space convolution is depicted in Figure 5.5.
5.2.3
Green’s Function in Free Space for the Potential
A simple example of a linear, time-invariant, homogeneous medium is free space. In
this case, the Green’s function can be computed directly from Maxwell’s equations.
To compute the Green’s function in free space for the ﬁeld, we ﬁrst evaluate the
corresponding function for the vector potential. By (4.82) and (4.83), the vector
potential has the same orientation as the source current, so that the Green’s function for
the potential is a monad, and can be obtained by solving the wave equation (4.78) with
the radiation condition at inﬁnity and is given by (5.13). This physically corresponds
to placing a source dipole moment concentrated at the origin, which appears as a step
07
12:00:56, subject to the Cambridge Core

5.2 System Representations
161
V
O
r
r-
x
y
z
Q
Fig. 5.5
Geometry of the three-dimensional space convolution at point Q for a radiating volume V.
function at t = 0 and is oriented along the ¯z direction. By (4.82) and (4.83), we have the
four representations of the Green’s function in free space for the potential:
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
g(r,t) ¯z = μ
4πrδ(t −r/c) ¯z,
G(r,ω) ¯z = μ
4πr exp(−jωr/c) ¯z,
g(k,t) ¯z = μ
4πt exp(−jkct) ¯z,
G(k,ω) ¯z = μ
4π U(kc + ω) ¯z,
(5.18a)
(5.18b)
(5.18c)
(5.18d)
where U(·) is the Heaviside step function. From (5.18a) it follows that the space–time
pulse propagates with velocity c. From (5.18b) and (5.18c) it follows that the
propagation delay in the natural domain corresponds to a phase shift in frequency or
wavenumber in the corresponding partial spectral domains.
5.2.4
Green’s Function in Free Space for the Field
The electromagnetic ﬁeld can be computed from the vector potential following the
outline described in Section 4.6. Proceeding in the space–frequency domain, from (4.67)
and (5.7) we get
μH = ∇× A,
(5.19)
and from (5.5b) at any point away from the source, and (5.6), we get
∇× H = jωϵ E.
(5.20)
07
12:00:56, subject to the Cambridge Core

162
Deterministic Representations
From which it follows that
E =
1
jωμϵ ∇× ∇× A
=
1
jωμϵ (∇∇· A −∇2A).
(5.21)
Using the homogeneous wave equation resulting from (4.78) with is = 0 in the
space–frequency domain, and using c2 = 1/(ϵμ), we ﬁnally obtain
E(r,ω) = −jωA(r,ω) +
1
jωϵμ∇(∇· A(r,ω)),
(5.22)
from which it follows that the Green’s function in free space for the electric ﬁeld in the
space–frequency domain is:
G(r,ω) = −jωμ
4π
&
I + ∇∇
β2
' exp(−jβr)
r
,
(5.23)
where β = ω√ϵμ and the dyadic operator in the square brackets in Cartesian coordinates
is represented by the matrix
⎛
⎜⎜⎜⎜⎜⎜⎝
1 + 1
β2
∂2
∂x2
1
β2
∂2
∂x∂y
1
β2
∂2
∂x∂z
1
β2
∂2
∂x∂y
1 + 1
β2
∂2
∂y2
1
β2
∂2
∂y∂z
1
β2
∂2
∂x∂z
1
β2
∂2
∂y∂z
1 + 1
β2
∂2
∂z2
⎞
⎟⎟⎟⎟⎟⎟⎠
.
Finally, using (5.23) we obtain the convolution equation for the electric ﬁeld:
E(r,ω) =

V
G(|r −r′|,ω) · I(r′,ω)dr′
= −jωμ
4π

V
&
I + ∇∇
β2
'
· exp(−jβ|r −r′|)
|r −r′|
I(r′,ω)dr′.
(5.24)
5.2.5
Green’s Function for Cylindrical Propagation
In the special two-dimensional case of cylindrical propagation when ﬁelds and sources
do not depend on z, Maxwell’s equations take a particularly simple form. In the case of
an inﬁnitely long wire antenna oriented along ¯z, the electric ﬁeld is also oriented along
¯z and satisﬁes the wave equation in the space–frequency domain
∇2E + ω2
c2 E = jωμI.
(5.25)
By solving (5.25) with I = δ(r)¯z and the radiation condition at inﬁnity, we obtain the
Green’s function for the electric ﬁeld in free space with cylindrical symmetry in the
space–frequency domain:
07
12:00:56, subject to the Cambridge Core

5.2 System Representations
163
x
y
z
O
r
S
Q
Fig. 5.6
Geometry of the two-dimensional space convolution at point Q for a radiating surface S.
G(r,ω) ¯z = −ωμ
4 H(2)
0 (βr) ¯z,
(5.26)
where H(2)
0 (·) is the Hankel function of the second kind and of order zero – see
Appendix E.3.
We then have the convolution integral
E(r,ω) =

S
G(|r −r′|,ω)I(r′,ω)dr′¯z
= −ωμ
4

S
H(2)
0 (β|r −r′|)I(r′,ω)dr′¯z,
(5.27)
where the integral is over a surface perpendicular to ¯z. The geometry of the
two-dimensional space convolution is depicted in Figure 5.6.
From Maxwell’s equations, it immediately follows that the magnetic ﬁeld is oriented
along φ and is given by
H(r,ω) = −1
jωμ∇× E.
(5.28)
07
12:00:56, subject to the Cambridge Core

164
Deterministic Representations
X1
X2
Y1
Y2
Y3
X3
Y4
X4
Y5
Y6
G(r,ω)
Fig. 5.7
Communication system with four inputs and six outputs.
5.3
Communication Systems: Discrete Radiating Elements
Systems using electromagnetic waves for communication of information range from
single a transmitter–receiver pair to multiple antenna systems and spatially distributed
networks, where cooperation between the transmitters and the receivers can occur
in many different ways. A general model considers a discrete set of |T | radiating
elements and |R| receiving ones. Every radiating element produces an input signal that
is transmitted through the propagation medium. Every receiving element measures the
linear superposition of all the received signals. The Green’s function can then be used to
describe the input–output relationships, treating the environment as a linear ﬁlter acting
on the transmitted signals. Figure 5.7 gives a representation of this input–output model.
5.3.1
Single Transmitter–Receiver Pair
The free-space electromagnetic channel for a single transmitter–receiver pair is a linear
time-invariant system transforming a signal x(t) at the transmitter into the corresponding
signal y(t) at the receiver. This system is modeled as having impulse response
g(t) = aδ(t −r/c),
(5.29)
where r is the distance between transmitter and receiver, c is the velocity of the
propagating waveform, and the scaling coefﬁcient a is the signal attenuation along the
straight path connecting the two. The corresponding frequency response is
G(ω) = aexp(−jωr/c),
(5.30)
07
12:00:56, subject to the Cambridge Core

5.3 Discrete Radiating Elements
165
X(ω)
(a)
Y (ω)
X(ω)
(b)
Y(ω)
G(ω)
G(ω)
Fig. 5.8
System transfer function in free space: (a) single antenna pair system, (b) multiple antenna
system. G(ω) is a scalar function, G(ω) is an |R| × |T | matrix; X(ω) and Y(ω) are scalar
functions, X(ω) is a |T | × 1 column vector, and Y(ω) is an |R| × 1 column vector.
and the received signal is given by
y(t) = a
 ∞
−∞
x(τ)δ(t −r/c −τ)dτ = ax(t −r/c).
(5.31)
It follows that the output signal is a time-delayed version of the input, scaled by an
appropriate multiplicative factor. By (5.31), in the frequency domain the output is a
phase-shifted, magnitude-scaled version of the input,
Y(ω) = G(ω)X(ω) = aexp(−jωr/c)X(ω);
(5.32)
see Figure 5.8(a).
The representation in Figure 5.8 is appropriate to describe the input–output
relationship between an applied current at the transmitter and the vector potential at
the receiver. By (5.18a), the vector potential has the same orientation as the applied
current and is simply a time-shifted, magnitude-scaled version of the current signal. In
this case, the scaling factor a is
a = μ/4πr.
(5.33)
The model is also appropriate to describe the electric and magnetic far ﬁelds
generated by elementary dipoles, provided that one considers the derivative of the
current as the input signal to the linear system. In this case, according to (4.92), the
output ﬁelds, polarized orthogonally to the direction of propagation and orthogonally to
each other, are a magnitude-scaled, time-delayed version of the derivative of the current.
In the near ﬁeld, however, the response (5.31) applies only to the vector potential, as the
ﬁelds are related to the sources in a more involved way expressed by (4.84) and the
appropriate Green’s function of the form given in Section 5.2.4.
The presence of a singular point at r = 0 in the scaling coefﬁcient (5.33) is often a
source of confusion, since from a system engineering perspective it seems to lead to
an unbounded signal gain as the receiver gets closer to the transmitter. This apparent
inconsistency is often considered as being a near-ﬁeld effect not observable in practice.
Physical insight, however, should bring clarity to the picture: the Green’s function
being singular at the origin is consistent with Maxwell’s theory and does not violate
any physical law. As shown in Section 4.6, although the instantaneous density power
07
12:00:56, subject to the Cambridge Core

166
Deterministic Representations
diverges at the origin, the total instantaneous radiated power is a constant, independent
of r. Integration over a solid angle makes the singularity disappear.
5.3.2
Multiple Transmitters and Receivers
The system’s representation for a single transmitter and receiver can be easily extended
to multiple transmitters and receivers. As usual, the simplest case to model is that of
propagation in free space. Consider a pair (i,k) of transmitting and receiving antennas
separated by a distance ri,k. By (5.30), we have
Gi,k(ω) = ai,k exp(−jωri,k/c).
(5.34)
In the presence of |T | transmitting and |R| receiving antennas, and assuming these do
not change the free-space characteristics of the propagation environment, the signal at
the kth receiving antenna is given by the superposition
Yk(ω) =

i∈T
Gi,k(ω)Xi(ω).
(5.35)
This linear relation can be written in convenient matrix form as
Y(ω) = G(ω)X(ω),
(5.36)
where Y is an |R|×1 column vector, G is an |R|×|T | matrix whose entries are given
by (5.34), and X is a |T | × 1 column vector. Figure 5.8(b) shows the corresponding
block diagram.
Equation (5.35) is the discrete analog of the two-dimensional spatial convolution
(5.14), where the integral over the whole transmitting surface is replaced by a discrete
sum over the antenna ensemble. For homogeneous media, each element Gi,k in the sum
(5.35) depends only on the distance between the ith transmitting and the kth receiving
antenna, and not on their individual positions, consistent with (5.14) where the Green’s
function inside the convolution integral depends only on the distance |r −r′|.
5.3.3
Singular Value Decomposition
The matrix G(ω) can be decomposed into singular values, see Section 3.4.3, yielding
G = UV†
=
R−1

n=0
σnU[n]V†[n],
(5.37)
where
R = rankG ≤min{|T |,|R|},
(5.38)
σ0 ≥σ2 ≥··· ≥σR−1 are the singular values of G, U[n] is the nth column (left singular
vector) of the unitary |R| × |R| matrix U, and V[n] is the nth column (right singular
vector) of the unitary |T | × |T | matrix V. By (5.36), it then follows that
Y = UV†X.
(5.39)
07
12:00:56, subject to the Cambridge Core

5.4 Arbitrary Radiating Elements
167
X~
V
V
U
U
Y~
0
R-1
x
x
X
Y
Pre-processing
Post-processing
†
†
Fig. 5.9
Parallel spatial channels arising from the singular value decomposition.
Letting +X = V†X, +Y = U†Y, and recalling that for any unitary matrix U we have
U†U = UU† = I, pre-multiplication of (5.39) by U† yields
+Y = +X.
(5.40)
Thus, we have an equivalent input–output representation in terms of R parallel spatial
channels:
+
Yn = σn +
Xn, n = 0,...,R −1;
(5.41)
see Figure 5.9 for the corresponding block diagram. The obtained representation can
be interpreted in terms of coordinate transformations. With appropriate choice of
pre-processing and post-processing matrices V and U†, the input is expressed in a
coordinate system deﬁned by the columns of V, and the output is expressed in terms
of a coordinate system deﬁned by the columns of U. In this new coordinate system the
input–output relationship has the very simple form given by (5.41).
5.4
Communication Systems: Arbitrary Radiating Elements
The multiple transmitters and receivers representation in (5.41) is an orthogonal spatial
representation for a discrete ensemble of transmitting and receiving antennas obtained
starting from (5.35) and decomposing the corresponding linear operator into singular
values. If instead we start from the spatial convolution representations (5.14) or
(5.27) for a continuous radiating volume or surface, that can include both sources
and scatterers, and perform the Hilbert–Schmidt decomposition of the linear operator
induced by the Green’s function, we obtain an analogous orthogonal representation
in continuous space for the ﬁeld generated by an arbitrary conﬁguration of sources
and scatterers. Performing the same decomposition in the time domain leads to the
corresponding orthogonal time representation.
Performing the Hilbert–Schmidt decomposition requires a square-integrable kernel.
For an arbitrary radiating volume in free space, the Green’s function is square-integrable
over the whole space outside the volume where the sources are located. On the other
07
12:00:56, subject to the Cambridge Core

168
Deterministic Representations
hand, it follows from (5.29) and (5.32) that the Green’s function is not square-integrable
over the whole time axis, since ideal free space provides a constant response at all
frequencies. In practice, in any reasonable propagation environment the high-frequency
components of the signal are highly attenuated and the environment acts as a linear ﬁlter
with an effectively ﬁnite bandwidth, so that instead of having an impulsive Green’s
function with a constant spectrum, we can assume a continuous, square-integrable
Green’s function, and the Hilbert–Schmidt decomposition can be performed.
5.4.1
Hilbert–Schmidt Decomposition
We derive the orthogonal time representation in the case of cylindrical propagation, so
that both ﬁeld and sources are directed along ¯z and we can refer to the scalar component
of the ﬁeld only. We have
e(r,t) =

S
 ∞
−∞
g(|r −r′|,t −τ)i(r′,τ)dτdr′.
(5.42)
Dropping, for convenience of notation, the dependence on the spatial variables, we write
the inner integral as the convolution
(Gi)(t) =
 ∞
−∞
g(t −τ)i(τ)dτ.
(5.43)
The Hilbert–Schmidt representation is obtained by following the same steps that lead to
(3.80). We ﬁrst approximate the kernel in (5.43) by
gN(t −τ) =
N−1

n=0
ψn(t)ϕ∗
n(τ),
(5.44)
where {ψn} and {ϕn} are the left and right singular functions of the operator G. These
can be obtained by the eigendecomposition of the self-adjoint compact operators G′G
and GG′. Using angle brackets to denote inner product, and letting G′ be the adjoint
operator of G, deﬁned by
⟨Gi,e⟩= ⟨i,G′e⟩,
(5.45)
we have
 (G′Gϕn)(τ) = λnϕn(τ),
(GG′ψn)(t) = λnψn(t).
(5.46)
It follows that an N-dimensional approximation of (5.43) is
(Gi)N(t) =
 ∞
−∞
N−1

n=0
ψn(t)i(τ)ϕ∗
n(τ)dτ
=
N−1

n=0
 ∞
−∞
ϕ∗
n(τ)i(τ)dτ
	
λnξn(t)
=
N−1

n=0
	
λn⟨i,ϕn⟩ξn(t)
07
12:00:56, subject to the Cambridge Core

5.4 Arbitrary Radiating Elements
169
=
N−1

n=0
anξn(t),
(5.47)
where ξn(t) is the normalized version of ψn(t), so that the basis functions in the
interpolation have unit norm over the ﬁnite interval [−T/2,T/2]. The representation
(5.47) shows that an N-dimensional approximation of the ﬁeld due to the current i(t) is
obtained by a linear combination of the (normalized) left singular functions {ξn}, and
that the coefﬁcients {an} in the interpolation are given by the product of the nth singular
value √λn times the projection of i(t) onto the nth right singular function ϕn(t).
As discussed in Chapter 3, the representation (5.47) optimally approximates the
image of any function i(t) under G using N basis functions. The error associated with
the approximation over the interval [−T/2,T/2] is √λN, and since λN →0, it is
possible to optimally approximate the radiated ﬁeld at any desired level of accuracy
ϵ by choosing N large enough. The number of degrees of freedom of the ﬁeld radiated
by an arbitrary conﬁguration of sources and scatterers placed inside the transmitting
domain corresponds to the index of the ﬁrst singular value having magnitude smaller
than ϵ. We can then write the inﬁnite series expansion
(Gi)(t) =
∞

n=0
	
λn⟨i,ϕn⟩ξn(t),
(5.48)
where convergence is intended in the energy sense.
For timelimited ﬁelds in the set TT, letting the bandwidth of observation  →∞,
the phase transition of the number of degrees of freedom allows truncation of the
series (5.48) to essentially N0 = T/π terms as  →∞. It follows that an arbitrary
timelimited ﬁeld can be represented by the superposition of N0 wide-band pulses ξn(t)
that concentrate more and more in time as  →∞. The analogous result holds for
bandlimited ﬁelds in the set B observed over a time interval of width T →∞, as they
can be represented by the superposition of N0 bandlimited functions that concentrate
more and more in frequency as T →∞.
A completely analogous picture arises by deriving the orthogonal spatial repre-
sentation of the system. From the spatial convolution relation (5.27), we obtain the
Hilbert–Schmidt representation
(GI)(r) =
∞

n=0
	
λn⟨I,ϕn⟩ξn(r),
(5.49)
and it follows that any ﬁeld observed in a bounded spatial domain of size S and of
wavenumber bandwidth W →∞can be represented by N0 = WS/π spatial pulses that
concentrate more and more in space as W →∞. The corresponding physical picture
has been anticipated in Section 1.2.3, and is discussed in detail in Chapter 8.
These orthogonal representations essentially diagonalize the channel into parallel
subchannels that can be used to multiplex different streams of information. The diag-
onalization provides orthogonal division of the time–frequency or spatial–wavenumber
resource that can then be used in parallel by a given system’s implementation. The
optimal division corresponds to the system architecture with the highest multiplexing
07
12:00:56, subject to the Cambridge Core

170
Deterministic Representations
x0
x   
N 0
i(t)
∫dt
N  –1 
0
∫dt
+
e(t)
ξ*
ξN  –1
0
*
∫dt
∫dt
y0
y   
N 0
0
λ
√
N
λ 0
√
0
0
ξ N  –1
0
ξ0
–1
x
x
N  –1
0
x0
x   
N 0
0
–1
+
–1
Pre-processing
Post-processing
–1
Fig. 5.10
Communication architecture based on orthogonal parallel channels.
capability. The largest number of channels obtained in this way corresponds to
the time–frequency T/π or space–wavenumber WS/π degrees of freedom of the
propagating ﬁeld, and provides a fundamental limit on the amount of information that
the electromagnetic waveform can carry across the boundary of its radiating volume.
5.4.2
Optimal Communication Architecture
In terms of system representation, letting the input to the system be
i(t) =
N0−1

n=0
xnϕn(t)
(5.50)
and the output
e(t) =
N0−1

n=0
ynξn(t),
(5.51)
it follows that for each input coefﬁcient
xn = ⟨i(t),ϕn(t)⟩
(5.52)
we have a corresponding output
yn = ⟨e(t),ξn(t)⟩.
(5.53)
By (5.47), we then have the input–output representation in terms of N0 parallel channels:
yn =
√
λnxn, n = 0,...,N0 −1 = T/π.
(5.54)
This leads to the communication architecture depicted in Figure 5.10, which performs
multiplexing of N0 parallel streams of information through the propagation channel
based on the Hilbert–Schmidt representation of the signal. The N0 input coefﬁcients
{x0,x1,...,xN0−1}, representing parallel communication streams, are recovered at the
receiver by integration of the received signal e(t), after multiplication by the appropriate
basis functions {ξ ∗
n (t)}. Thus, the architecture uses N0 real numbers to communicate a
signal of essentially N0 degrees of freedom, and is information-theoretically optimal.
Each coefﬁcient of the signal’s representation can be interpreted as transporting one
07
12:00:56, subject to the Cambridge Core

5.5 Summary and Further Reading
171
degree of freedom of the signal, or as serving as one channel from transmitter to
receiver. In this way, a communication rate of one real number per degree of freedom,
corresponding to one real number per channel use, is achieved.
The same architecture could also be implemented using a suboptimal orthogonal
basis representation. For example, we could use the cardinal series with the sampled
ﬁeld’s values as coefﬁcients, or the Fourier series restricted to a given interval of
observation. In this case, however, to communicate a signal of N0 degrees of freedom
we would need N > N0 coefﬁcients, which results in a less efﬁcient usage of the
physical resource, yielding a communication rate of N0/N < 1 real numbers per degree
of freedom.
Finally, we can derive the input–output representation in terms of N0 spatial channels
formally identical to (5.54). Each input coefﬁcient in this case is
+Xn = ⟨I(r),ϕn(r)⟩,
(5.55)
and the corresponding output is
+Yn = ⟨E(r),ξn(r)⟩.
(5.56)
The parallel spatial channels are
+Yn =
√
λn+Xn, n = 0,...,N0 −1 = WS/π.
(5.57)
This representation should be compared with its discrete analog (5.41), where the rank
R plays the role of the number of degrees of freedom N0.
5.5
Summary and Further Reading
The electromagnetic ﬁeld is a signal of space and time and it has four spectral
representations, linked by Fourier transforms. The medium where propagation occurs
can be viewed as a linear system and the (dyadic) Green’s function as its impulse
response. The ﬁeld at any point in space radiated by an arbitrary source volume
including transmitters and scatterers can be computed via a convolution equation
and decomposed using appropriate orthogonal basis sets. We have applied the theory
developed in the previous chapters to derive optimal decompositions that yield the
number of degrees of freedom of the ﬁeld in the time–frequency and space–wavenumber
domains. These representations apply to waveforms generated by arbitrary sources
immersed in arbitrary scattering environments, and they provide a limit on the number
of channels that can be used for communication, suggesting an optimal architecture for
communication systems.
The material discussed in this chapter draws on standard concepts in electromag-
netics, linear systems, and analysis. Some references have been given in Chapters 3
and 4.
07
12:00:56, subject to the Cambridge Core

172
Deterministic Representations
5.6
Test Your Understanding
Problems
5.1
Provide a complete derivation of (5.18).
5.2
Check that (5.26) is a solution for (5.25).
5.3
Show that the Green’s function for an arbitrary radiating volume in free space is
square-integrable over the space outside the volume.
5.4
Show that the Green’s function for an arbitrary radiating volume in free space is
not square-integrable over time.
5.5
Provide a physical justiﬁcation of the mathematical results in Problems 5.3 and
5.4.
5.6
Describe the relationship between the Hilbert–Schmidt and the singular value
decomposition of the radiated ﬁeld.
5.7
Explain the physical signiﬁcance of the Green’s function being singular at the
origin.
5.8
How many orthogonal parallel channels can a radiating system of radius r and
angular frequency bandwidth  support?
07
12:00:56, subject to the Cambridge Core

6
Stochastic Representations
One should always be a little improbable.1
6.1
Stochastic Models
Sometimes it is convenient to use stochastic representations of the electromagnetic
ﬁeld in place of deterministic ones to describe average observations in complex
environments, and in this case the number of degrees of freedom depends on the
parameters of the stochastic process used to represent the ﬁeld. These should be chosen
so that the model is consistent with the physics, and can predict average observations.
While representations in a deterministic setting consider the ﬁeld radiated by an
arbitrary environment, in a stochastic setting we consider the ﬁeld radiated by a random
environment. In this case, the analog of the number of degrees of freedom is the amount
of stochastic diversity of the received waveform. A larger diversity corresponds to more
unpredictable waveforms that require, on average, a larger number of coefﬁcients to be
represented to a given accuracy.
Both deterministic degrees of freedom and stochastic diversity have applications in
communications. The number of degrees of freedom provides an upper bound over
all possible environments on the number of channels that can be used to multiplex
different streams of information over different dimensions of the signals’ space. In the
stochastic setting, the amount of diversity provides a limit on the reliability that can
be achieved by performing transmissions over multiple realizations of the channel. If
the received signal is modeled as a random process in time, frequency, and space, then
redundant transmissions over multiple frequency bands, multiple time slots, or multiple
antennas can improve the probability that at least one of these transmissions is received
successfully. In short, the number of degrees of freedom is used to measure the rate gain
that can be achieved by performing multiple parallel transmissions over the channel, and
the stochastic diversity is used to measure the reliability gain that can be achieved by
performing repeated transmissions over multiple realizations of the channel.
In a stochastic setting, the analog of the Hilbert–Schmidt representation leading to
the number of degrees of freedom is the Karhunen–Loève representation leading to
the stochastic diversity. This was named after the Finnish and Israeli mathematicians
1 O. Wilde (1894). Phrases and philosophies for the use of the young. The Chameleon, 1(1), p. 2.
08
12:00:54, subject to the Cambridge Core

174
Stochastic Representations
g(t,t−τ)δ(τ) dτ
LTV
SYSTEM
t
   = g(t,τ)
∞
−∞
Fig. 6.1
Time-varying Green’s function for a linear, time-varying (LTV) system.
Kari Karhunen and Michel Loève, and provides the optimal ﬁnite-dimensional
approximation of a stochastic process of given autocorrelation function in terms of
orthogonal deterministic basis functions and uncorrelated stochastic coefﬁcients.
6.2
Green’s Function for a Random Environment
We consider stochastic models of the Green’s function that describe, on average, the
effect of a random conﬁguration of radiating elements on the received waveform. These
stochastic models should be applied with care. In the wrong context, they easily lead to
non-physical results. Stochastic assumptions may hold in speciﬁc physical situations of
interest, for example rich scattering, and may not be applicable in all cases.
While in Chapter 5 we have considered time-invariant radiating systems composed
of an arbitrary conﬁguration of ﬁxed sources and scatterers, we now consider systems
composed of a stochastic conﬁguration of radiating elements that can also be time
varying. We consider a discrete ensemble of transmitters and receivers, representing
the inputs and outputs of the communication system, and model the Green’s function as
randomly varying in frequency, time, and space across different antennas. The frequency
and space variations are due to the presence of multiple scattering; the time variations
are due to mobility in the environment.
The stochastic convolution integral used to represent the received waveform is
intended in the mean square sense – see Appendix D.
6.2.1
Linear, Time-Varying Systems
We ﬁrst consider the input–output relationship for a linear, time-varying system in a
deterministic setting. In the time domain, we have
y(t) =
 ∞
−∞
g(t,τ)x(t −τ)dτ =
 ∞
−∞
g(t,t −τ)x(τ)dτ,
(6.1)
where g(t,τ) is the time-varying Green’s function – see Figure 6.1. This input–output
relationship should be compared with its time-invariant counterpart (5.9). In the
time-variant case, the Green’s function has four possible representations, linked by
Fourier transforms, that are depicted in Figure 6.2, where τ ↔ω and t ↔˜ω are Fourier
variable pairs. Of particular interest are the time-varying impulse response g(t,τ) and
the time-varying transfer function G(t,ω). The physical interpretation of g(t,τ) is the
response of the system to an impulse applied at time τ. This response varies with time,
08
12:00:54, subject to the Cambridge Core

6.2 Green’s Function for a Random Environment
175
Fig. 6.2
Green’s function Fourier transform pairs.
G(t,ω)
y(t)
x(t)
Fig. 6.3
Input–output representation of a linear, time-varying system.
and its Fourier transform with respect to the time variable is ¯g( ˜ω,τ). Analogously, the
physical interpretation of G(t,ω) is the response of the system to a sinusoidal excitation
of angular frequency ω. This response varies with time, and its Fourier transform with
respect to the time variable is ¯G( ˜ω,ω).
We now derive the relationship between input and output in the frequency domain
using the time-varying transfer function
G(t,ω) =
 ∞
−∞
g(t,τ)exp(−jωτ)dτ.
(6.2)
Writing the input as a Fourier integral of its spectrum,
x(t −τ) = 1
2π
 ∞
−∞
X(ω)exp(−jωτ)exp(jωt)dω,
(6.3)
and substituting into (6.1), we obtain
y(t) = 1
2π
 ∞
−∞
X(ω)G(t,ω)exp(jωt)dω,
(6.4)
which shows that the output in the time domain is given by the inverse transform of the
product of the time-varying transfer function and the Fourier transform of the input – see
Figure 6.3. This input–output relationship should be compared with its time-invariant
counterpart (5.12), depicted in Figure 5.3. Finally, by taking the Fourier transform with
respect to the time variable, it follows that the relationship between input and output in
the frequency domain is given by
Y( ˜ω) = 1
2π
 ∞
−∞
¯G( ˜ω −ω,ω)X(ω)dω,
(6.5)
which should be compared with its time-invariant counterpart (5.11).
08
12:00:54, subject to the Cambridge Core

176
Stochastic Representations
+
G   (t,ω)
1 ,k
G   (t,ω)
2 ,k
y  (t)
k
G   (t,ω)
,k
x (t)
1 
x  (t)
2
x  (t)
Fig. 6.4
Response at the kth receiving antenna.
6.2.2
Linear, Space–Time-Varying Systems
When communication occurs using multiple antennas at different locations in space,
we have to account for the superposition of signals from the different transmitters. For
a discrete ensemble of antennas, the signal at the kth receiving antenna is given by the
linear superposition
Yk(t,ω) =

i∈T
Gi,k(t,ω)Xi(ω),
(6.6)
which is the generalization of (5.35) to a time-varying medium. For homogeneous
media, each element Gi,k in the sum (6.6) depends only on the distance between the
ith transmitting and the kth receiving antenna and not on their individual positions,
so that the sum is the discrete equivalent of the continuous spatial convolution (5.14).
Figure 6.4 gives the corresponding block diagram representation. The relationship can
be put in convenient matrix form as
Y(t,ω) = G(t,ω)X(ω),
(6.7)
where Y is an |R| × 1 column vector, G is an |R| × |T | matrix, and X is a |T | ×
1 column vector. Each element of the matrix G represents the time-varying transfer
function between the ith transmitting and the kth receiving antenna.
Stochastic models consider the entries of G to be realizations of the elements of a ran-
dom matrix, forming a discrete spatial random process, each being a two-dimensional
time–frequency random process, and average responses are considered.
6.3
Multi-path
We begin our stochastic description by considering the response to a given input signal
in a time-invariant setting and at a given radiation frequency. The model is later extended
to a time-varying setting.
Random multi-path is a simpliﬁed model of propagation where the radiating elements
represent a random conﬁguration of scattering elements. Like the multi-antenna channel
representation, it is based on the linearity of the electromagnetic propagation channel.
By (5.35), the signal at each receiving antenna is the superposition of the signals from all
transmitting antennas. Each term in this sum is now modeled as a linear superposition of
08
12:00:54, subject to the Cambridge Core

6.3 Multi-path
177
different signals traveling along different multiple scattered paths from one transmitter
to one receiver.
A signal along the mth path from the ith transmitter to the kth receiver carries
a phase shift −jωτ (m)
i,k , where τ (m)
i,k
is the propagation delay along the path, and a
geometric real attenuation coefﬁcient d(m)
i,k . In addition, there is a complex scattering
coefﬁcient b(m)
i,k exp(jβ(m)
i,k ) that accounts for the absorption and phase shifts occurring at
all scattering points. Letting the set of multiple scattered paths be P and
d(m)
i,k b(m)
i,k = a(m)
i,k ,
β(m)
i,k −ωτ (m)
i,k = φ(m)
i,k ,
(6.8)
we rewrite (5.35) as
Yk(ω) =

i∈T
Xi(ω)Gi,k(ω)
=

i∈T
Xi(ω)ai,k exp(jφi,k)
=

i∈T
Xi(ω)

m∈P
a(m)
i,k exp(jφ(m)
i,k )
=

i∈T
Xi(ω)

m∈P
ℜ(a(m)
i,k exp(jφ(m)
i,k )) + jℑ(a(m)
i,k exp(jφ(m)
i,k )).
(6.9)
We now model the attenuations and the phase shifts as independent random variables
and assume that the phase of the resulting random vector in the second sum of (6.9)
is uniformly distributed in [0,2π]. This latter assumption is reasonable if the phase of
each component in the sum is random and uniformly distributed so that the resulting
sum vector does not have a preferred phase. In the case that a direct ﬁeld path is present,
the assumption is not valid and a modiﬁcation of the model is required that leads to a
more general form of the resulting distribution – see Problem 6.1.
In the presence of a large (ideally inﬁnite) number of paths, by the central limit
theorem the sums over all paths of the real and imaginary parts in (6.9) approach two
Gaussian random variables ℜGi,k and ℑGi,k of zero mean and variance σ 2
i,k. Considering
the resulting complex random variable
Gi,k = ℜGi,k + jℑGi,k = Ai,k exp(ji,k),
(6.10)
we have
E(ℜGi,kℑGi,k) = E(A2
i,k sini,k cosi,k)
= E(A2
i,k)E(sini,k cosi,k)
= 0,
(6.11)
where the second equality follows from independence, and the last equality follows
from the uniform phase assumption. Since the real and imaginary parts are uncorrelated
08
12:00:54, subject to the Cambridge Core

178
Stochastic Representations
and Gaussian, they are also independent, and their joint distribution over the complex
plane factorizes into the product form
fℜGi,k,ℑGi,k(ai,k) =
1
2πσ 2
i,k
exp(−a2
i,k/(2σ 2
i,k)),
(6.12)
which depends only on the amplitude ai,k and not on the phase φi,k of the complex
vector. It follows that each transmitted signal in the frequency domain is multiplied by
a complex variable distributed as a circularly symmetric Gaussian, and is characterized
by a single parameter σi,k.
Noting the differential relationships
fℜGi,k,ℑGi,kdℜGi,kdℑGi,k = fAik,i,kdAi,kdi,k,
(6.13)
dℜGi,kdℑGi,k = Ai,kdAi,kdi,k,
(6.14)
fAi,k(ai,k) =
 2π
0
fAi,k,i,k(ai,k,φi,k)dφi,k,
(6.15)
we obtain the Rayleigh distribution for the amplitude
fAi,k(ai,k) = ai,k
σ 2
i,k
exp(−a2
i,k/(2σ 2
i,k)),
(6.16)
which is also characterized by the single parameter σi,k. Fixing this parameter
corresponds to ﬁxing the average attenuation between transmitter i and receiver k,
namely
E(Ai,k) =
π
2 σi,k.
(6.17)
From (6.16), it immediately follows that A2
i,k has the exponential distribution
fA2
i,k(ai,k) =
1
2σ 2
i,k
exp(−ai,k/2σ 2
i,k),
(6.18)
and the choice of σi,k also controls the average attenuation power
E(A2
i,k) = 2σ 2
i,k.
(6.19)
Since the variance
E(A2
i,k) −E2(Ai,k) = 4σ 4
i,k,
(6.20)
it follows that the standard deviation of the power equals its average, indicating that
ﬂuctuations occur at the order of the entire power. This relation can be further extended
to higher moments:
E(An
i,k) = n!En(Ai,k).
(6.21)
We now summarize. The random multi-path model leads to a stochastic Green’s
function distributed as a zero-mean complex Gaussian random variable in the frequency
domain, with Rayleigh-distributed magnitude representing the ﬁeld’s attenuation,
08
12:00:54, subject to the Cambridge Core

6.3 Multi-path
179
0
i,k
i
k
Fig. 6.5
Realization of Rayleigh fading for the random multi-path model.
uniform phase representing the ﬁeld’s phase shift, and exponentially distributed squared
magnitude, representing power attenuation.
The parameter σi,k, corresponding to the average attenuation between transmitter i
and receiver k, is usually modeled as a decreasing function of the distance between
transmitter and receiver, called the average path loss. Models used in the literature
include exponential and inverse power-law path loss functions. The average path
loss in the case of a uniform distribution of point scatterers can also be computed
analytically using multiple scattering theory in random media – see Chapter 10. This
theory predicts an exponential decay with distance, due to absorption, and a singular
point at zero distance from the transmitter. The exponential attenuation is consistent
with the case of propagation without scattering in a continuum absorbing medium
examined in Section 4.4.2. The singular behavior at the origin is consistent with
free-space propagation near the transmitting antenna, and it disappears when integration
is performed to compute the radiated power. A plot of a typical realization of the
magnitude of the Green’s function for the random multi-path model with a σi,k that
decays with the distance between transmitter and receiver |ri −rk|, is depicted in
Figure 6.5. Fluctuations around the average value indicated with a dashed line are called
random fading.
6.3.1
Frequency-Varying Green’s Function: Coherence Bandwidth
The notation used to derive the probabilistic model for the Green’s function in multiple
scattering environments concealed the fact that the random variables Gi,k depend on ω.
By (6.8), each random path carries a random phase shift that depends on ω through a
random propagation delay. Furthermore, scattering by different objects along the path
introduces additional phase shifts and attenuations that also depend on frequency. For
narrow-band signals the Green’s function can be considered ﬂat for all frequencies
within the band of the signal, so that the response can be described by a single random
08
12:00:54, subject to the Cambridge Core

180
Stochastic Representations
variable that in the random multi-path model has a Rayleigh magnitude with average
decreasing with distance, and uniform phase. However, for wide-band signals this is
not the case, and the statistics of the response can change within the frequency band
of the signal. In this case, different frequency components of the transmitted signal are
affected in different ways by the propagation environment, and the signal tends to lose
coherence due to propagation.
The corresponding stochastic model for this situation is obtained by letting
the Green’s function be a zero-mean random process Gi,k(ω), whose frequency
autocorrelation
sGi,k(ω,ω + ω′) = E(Gi,k(ω)G∗
i,k(ω + ω′))
(6.22)
vanishes for ω′ →∞. The critical value for the frequency difference ω′ at which
the correlation practically disappears, or decreases to a speciﬁed level, is called the
coherence bandwidth of the channel.
The coherence bandwidth, measuring how rapidly the channel changes with
frequency, is inversely related to the time broadening of the signal, or delay spread, due
to multi-path propagation. We ﬁrst give an informal picture. Consider two paths carrying
phase shifts ωτ1(t) and ωτ2(t) respectively. The phase difference per unit frequency is
dφ
dω = τ1(t) −τ2(t),
(6.23)
so that a phase variation of 2π occurs for
dω =
2π
τ1(t) −τ2(t).
(6.24)
It follows that, neglecting contributions due to different scattering coefﬁcients along the
paths, signiﬁcant variations in the received signal occur for frequency intervals of the
order of the inverse of the time delay between paths. Since the coherence bandwidth
measures the frequency interval along which channel variations are negligible, its
inverse is also an approximate measure of the time delay spread among different
received paths. This reasoning can be made more rigorous by taking the inverse Fourier
transform of (6.22) with respect to the increment frequency ω′,
ˇsGi,k(ω,¯t) = 1
2π
 ∞
−∞
sGi,k(ω,ω + ω′)exp(jω′¯t)dω′.
(6.25)
We then have
sGi,k(ω,ω + ω′) =
 ∞
−∞
ˇsGi,k(ω,¯t)exp(−jω′¯t)d¯t,
(6.26)
sGi,k(ω,ω) =
 ∞
−∞
ˇsGi,k(ω,¯t)d¯t
= σ 2
i,k(ω),
(6.27)
where the last equality follows directly from (6.22). The quantity ˇsGi,k(ω,¯t) is called
the power delay proﬁle and can be identiﬁed with the distribution of the average power
as a function of the delay due to propagation. Its integral along the time axis is the
08
12:00:54, subject to the Cambridge Core

6.3 Multi-path
181
total average power response of the stochastic channel at angular frequency ω. The
range of values for which the power delay proﬁle is non-zero is the delay spread of
the propagation channel. The broader the delay spread, the narrower the coherence
bandwidth representing the support of its transform.
If the autocorrelation (6.22) depends only on the increment ω′ and not on ω then the
channel is frequency wide-sense stationary. In this case, although the Green’s function
varies randomly in frequency, its ﬁrst- and second-order statistics remain constant. This
is immediately evident from (6.22), having
sGi,k(ω,ω) = E(Gi,k(ω)G∗
i,k(ω))
= σ 2
i,k(ω)
= σ 2
i,k,
(6.28)
and from the zero-mean assumption. The physical signiﬁcance of the frequency
wide-sense stationary model is that the power delay proﬁle is the Dirac impulse
ˇsGi,k(¯t) = σ 2
i,kδ(¯t),
(6.29)
corresponding to the average power output of the channel that is concentrated around
t = ¯t, so that the correlation between signals along different paths is essentially zero if
the corresponding delay between them is larger than a tiny interval. For this reason, a
frequency wide-sense stationary channel model is also called an uncorrelated scattering
channel. Notice that this assumption is required for the convergence to a Gaussian
distribution described in the previous section, where signals along different paths were
assumed to be independent.
6.3.2
Time-Varying Green’s Function: Coherence Time
We now extend the treatment of multiple-path models to media that are linear, but may
be time varying. In this case, the impulse response gi,k(t,τ) at time t at the receiver k
due to an impulsive input at time t = τ at the transmitter i is a real function of both t and
τ. Only when the medium is time invariant does g become a function only of the time
difference t −τ. We have, in general,
yk(t) =
 ∞
−∞
xi(t −τ)gi,k(t,τ)dτ.
(6.30)
Writing the input as a Fourier integral of its spectrum,
xi(t −τ) = 1
2π
 ∞
−∞
Xi(ω)exp(−jωτ)exp(jωt)dω,
(6.31)
and substituting into (6.30), we obtain
yk(t) = 1
2π
 ∞
−∞
Xi(ω)Gi,k(t,ω)exp(jωt)dω,
(6.32)
where
Gi,k(t,ω) =
 ∞
−∞
gi,k(t,τ)exp(−jωτ)dτ.
(6.33)
08
12:00:54, subject to the Cambridge Core

182
Stochastic Representations
The physical signiﬁcance of Gi,k(t,ω)/(2π) is that it is the output signal at time t when
the input is a time-harmonic signal of angular frequency ω. This follows immediately
by letting
xi(t) = cos(ω0t) = ℜ(exp(−jω0t))
(6.34)
and substituting into (6.32):
yk(t) = ℜ
 1
2π
 ∞
−∞
δ(ω −ω0)Gi,k(t,ω)exp(jωt)dω

= 1
2π ℜ(Gi,k(ω0,t)exp(jω0t))
= 1
2π ℜ(Gi,k(ω0,t)exp(−jω0t)).
(6.35)
We can then model Gi,k(t,ω) as a zero-mean two-dimensional stochastic process
Gi,k(t,ω) representing the time–frequency varying transfer function of the medium. We
have seen that frequency variations are related to multi-path delays and characterized by
the coherence bandwidth and time delay spread. Time variations are instead related to
changes in the channel due to mobility and are characterized by the coherence time and
Doppler frequency spread. The treatment, in this case, is the exact dual of the previous
section.
To appreciate this duality, we deﬁne the mutual coherence function between the
output ﬁelds due to the time-harmonic inputs at two different frequencies ω and
ω + ω′ as
sGi,k(ω,ω + ω′,t,t + t′) = E(G(ω,t)G∗(ω + ω′,t + t′)).
(6.36)
By the same reasoning as applied for (6.25)–(6.27), the inverse Fourier transform with
respect to the frequency increment ω′ of (6.36),
ˇsGi,k(ω,¯t,t,t + t′) = 1
2π
 ∞
−∞
sGi,k(ω,ω + ω′,t,t + t′)exp(jω′¯t)dω′,
(6.37)
provides for t′ = 0 the power delay proﬁle at time t as a function of the delay ¯t. Similarly,
the Fourier transform with respect to the time increment t′ of (6.36),
ˆsGi,k(ω,ω + ω′,t, ¯ω) =
 ∞
−∞
sGi,k(ω,ω + ω′,t,t + t′)exp(−j ¯ωt′)dt′,
(6.38)
provides for ω′ = 0 the Doppler power proﬁle at frequency ω as a function of the
Doppler frequency ¯ω. The frequency interval in which the Doppler power proﬁle is
non-zero indicates the amount of frequency spread of the signal due to propagation in a
time-varying medium. The inverse of the Doppler frequency spread is an indication of
the time difference at which the time autocorrelation of the signal practically disappears,
or decreases to a speciﬁed level, and is deﬁned to be the coherence time.
A simple heuristic explanation of the Doppler spread and coherence time due to
mobility can also be given in this case. Consider one multiple scattered path carrying
08
12:00:54, subject to the Cambridge Core

6.3 Multi-path
183
a delay τ(t) that changes over time due to the relative motions in the channel. The
corresponding phase shift is ωτ(t). The phase shift per unit time is
dφ
dt = ωdτ(t)
dt
,
(6.39)
so that a phase variation of 2π occurs for
dτ(t)
dt
= 2π
ω .
(6.40)
Letting v be the velocity at which the path length changes over time, this phase variation
occurs in time 2πc/(ωv), where c is the speed of propagation. It follows that signiﬁcant
variations in the phase of the received signal occur for time variations of the order of
the inverse of ωv/c, which is the Doppler shift experienced by the signal along the path.
The inverse of the overall Doppler spread gives an indication of the coherence time of
the channel.
6.3.3
Mutual Coherence Function
We now summarize. According to our stochastic model, when a sinusoidal wave of
angular frequency ω propagates in a time-varying medium, the output wave at the
receiver randomly ﬂuctuates over time. The correlation of the received wave at two
different times t and t + t′ decreases as the separation t′ increases. The time difference
at which the correlation practically disappears, or decreases to a speciﬁed level, is
the coherence time. It is an indication of how a wave at angular frequency ω is
correlated in time. The Fourier transform of the time correlation with respect to t′ is
the Doppler power proﬁle, whose support indicates the Doppler spread of the signal due
to propagation. Its inverse roughly corresponds to the coherence time of the channel.
These effects are due to the time-varying nature of the medium due to mobility.
On the other hand, we can compare waves sent at different frequencies, reporting
the outputs on the same time axis, and observe the corresponding random ﬂuctuations
in frequency. The correlation in frequency decreases as the separation ω′ increases.
The frequency difference at which the correlation practically disappears, or decreases
to a speciﬁed level, is the coherence bandwidth. It is a measure of how the waves
at two different frequencies are correlated at the same time. The inverse Fourier
transform of the frequency correlation with respect to ω′ is the power delay proﬁle,
whose support indicates the delay spread of the signal due to propagation. Its inverse
roughly corresponds to the coherence bandwidth of the channel. These effects are due
to multi-path propagation only and are not due to mobility.
These concepts can be illustrated by sketching the magnitude of the mutual coherence
function (6.36) as a function of the difference frequency ω′ and the difference time
t′ at a given center time t and a given center frequency ω – see Figure 6.6. When
the shapes indicated in the plot do not depend on the center time t, but only on t′,
the channel is called time wide-sense stationary. If they do not depend on the center
frequency ω, but only on ω′, the channel is called frequency wide-sense stationary,
or uncorrelated scattering. These two cases are time–frequency duals of each other. If
08
12:00:54, subject to the Cambridge Core

184
Stochastic Representations
c

Fig. 6.6
Mutual coherence function, showing the coherence bandwidth c and the coherence time Tc.
the mutual coherence function is independent of both ω and t, the channel is called
wide-sense uncorrelated scattering.
A waveform of bandwidth ω smaller than the coherence bandwidth c of the
channel does not undergo signiﬁcant distortion in frequency. In this case, the response
of the channel is said to be frequency ﬂat, and the different frequency components of
the signal are treated coherently – see Figure 6.7(a). In this case, the power delay proﬁle
appears as a short impulse, and the received waveform has essentially the same shape as
the transmitted one. A waveform of bandwidth ω larger than c undergoes signiﬁcant
distortion in frequency due to multi-path delays, and exhibits a lack of correlation
among different frequency components at the receiver. The response of the channel is
said to be selective in frequency – see Figure 6.7(b). The net effect in the time domain
is that the power delay proﬁle appears broader, and the received waveform does not
resemble the same shape as the transmitted one, due to the large amount of interference
of the delayed returns.
Similarly, a waveform transmitted over a time interval smaller than the coherence
time Tc of the channel does not undergo signiﬁcant distortion in time, and the response
is said to be time ﬂat. A signal transmitted over a time interval larger than Tc undergoes
a signiﬁcant distortion in time due to mobility in the environment, and exhibits a lack
of correlation among different time components at the receiver. The net effect in the
frequency domain is that the Doppler power proﬁle appears broader in frequency and
the response of the channel is said to be selective in time.
The different cases are summarized in Figure 6.8, where (a) represents a channel that
is essentially frequency ﬂat but time selective, (b) represents a channel that is essentially
time ﬂat but frequency selective, and (c) represents a channel that is both time and
frequency selective.
08
12:00:54, subject to the Cambridge Core

6.3 Multi-path
185
c
Δω
Δω
|G(ω)|
|G(ω)|
ω
ω
O
O
(a)
(b)
c


Fig. 6.7
(a) The transmitted signal’s bandwidth ω < c, and the signal remains coherent. (b) The
transmitted signal’s bandwidth ω > c, and the signal is subject to distortion.
ω
t
ω
t
|G(ω,t)|
|G(ω,t)|
ω
t
|G(ω,t)|
(a)
(b)
(c)
Fig. 6.8
(a) Time-selective Green’s function. (b) Frequency-selective Green’s function. (c) Time- and
frequency-selective Green’s function.
08
12:00:54, subject to the Cambridge Core

186
Stochastic Representations
6.3.4
Spatially Varying Green’s Function: Coherence Distance
We now consider the spatial correlation between received signals. In the presence
of multiple scattering, one may expect that the correlation between signals at any
two receiving points decreases as the distance between the transmitting antennas
increases. Similarly, one may expect the correlation to decrease as the distance between
the receiving antennas increases. The idea is that in both cases signals travel along
increasingly different multiple paths to the receiver. The minimum distance at which
these correlations practically disappear is called the coherence distance of the channel.
The corresponding mathematical model considers the elements Gi,k(t,ω) to be
realizations of the elements of a random matrix G, each entry Gi,k(t,ω) being a
two-dimensional time–frequency random process as described in the previous section.
In the simplest case, these random processes are assumed to be i.i.d. and Gaussian.
This corresponds to having a separation distance between the antennas larger than the
coherence distance, so that the correlation between any two antenna pairs is zero. On
the other hand, in the case that the antennas are in close proximity to each other, we can
have a non-zero spatial correlation between channel responses at (i,k) and (i′,k′):
sG(i,k;i′k′) = E(Gi,kG∗
i′,k′).
(6.41)
If the spatial correlation is assumed to be a local phenomenon occurring separately
at the transmitter and the receiver, letting !T and !R be deterministic matrices
whose entries represent, respectively, the correlation between the transmit and receive
antennas, and Gw an i.i.d. random matrix, we have
G = !1/2
R Gw!1/2
T .
(6.42)
This means that the correlation between two transmit antennas is the same regardless of
the receive antenna at which the observation is made, and vice versa. A slightly more
general model is
G = !1/2
R GRT1/2GT!1/2
T ,
(6.43)
where GR and GT are both i.i.d. Gaussian matrices, and T1/2 describes the transfer
matrix between the transmitter and receiver’s environments. The rank of this transfer
matrix can be as small as one, in the case, for example, of a single-mode waveguide
between transmitter and receiver. In this case, the matrix G is forced to have rank one
as well, even if !1/2
R , !1/2
T
have full rank. This is called the keyhole effect and can arise
in indoor propagation in corridors, or in outdoor propagation along city streets.
6.4
Karhunen–Loève Representation
The Karhunen–Loève decomposition of the stochastic Green’s function is an orthogonal
representation of the stochastic process that is convergent with respect to the mean
square norm, and provides the minimum number of orthogonal basis functions that
can approximate the process in the mean square sense up to arbitrary accuracy.
08
12:00:54, subject to the Cambridge Core

6.4 Karhunen–Loève Representation
187
The representation is based on the spectral decomposition of the autocorrelation
function of the process, and it leads to the notion of stochastic diversity available for
communication.
We ﬁrst consider the Karhunen–Loève decomposition of the time-varying stochastic
process G(t), then extend it in the natural way to describe the stochastic process G(ω,t),
which includes random variations in frequency, and ﬁnally to describe the stochastic
process Gi,k(ω,t), which includes random variations across different antennas in space.
There is an analogy between the results described here, the singular values repre-
sentation described in Section 5.3.3, and the time–frequency and space–wavenumber
Hilbert–Schmidt representations described in Section 5.4.1. In the deterministic setting
we refer to radiation from an arbitrary environment, and the number of degrees of
freedom indicates the amount of information in terms of the minimum number of
coordinates required to specify the process up to a given level of approximation. In
the stochastic case we refer to radiation from a random environment that induces
random variations of the ﬁeld in time, frequency, and space, and the stochastic diversity
represents the amount of information, in terms of the minimum number of uncorrelated
coordinates required to specify the process up to a given level of approximation, in a
certain average sense.
While in a deterministic setting the number of degrees of freedom depends only on
the bandwidth and on the size of the interval where the waveform is observed, in the
stochastic setting the diversity depends on the form of the autocorrelation function of the
process. This should not be surprising: the random environment shapes the response by
imposing a certain autocorrelation function, which inﬂuences the amount of stochastic
variation of the received signal.
6.4.1
Time-Varying Green’s Function
We consider a stochastic process G(t) that is deﬁned over a time interval [−T/2,T/2]
and subject to the squared norm constraint
∥G(t)∥2 = ⟨G(t)G∗(t)⟩= E[G(t)G∗(t)] < ∞.
(6.44)
This constraint is analogous to imposing square-integrability of the process with respect
to its probability measure, namely

S
|G(t,z)|2dP(z) < ∞,
(6.45)
where S is the sample space of the stochastic process
G : [−T/2,T/2] × S →R,
(6.46)
equipped by an appropriate σ-algebra over S .
The stochastic process also has zero mean and is mean-square continuous, namely
lim
→0E[(G(t + ) −G(t))2] = 0.
(6.47)
08
12:00:54, subject to the Cambridge Core

188
Stochastic Representations
This immediately implies that the autocorrelation function
sG(t,t′) = E[G(t)G∗(t′)]
(6.48)
is also continuous.
We consider the integral operator whose kernel is the autocorrelation function
(Sf)(t) =
 T/2
−T/2
sG(t,t′)f(t′)dt′.
(6.49)
This is of Hilbert–Schmidt type since, by continuity,
 T/2
−T/2
 T/2
−T/2
|sG(t,t′)|2dtdt′ < ∞.
(6.50)
The Hilbert–Schmidt operator in (6.49) is non-negative, and self-adjoint; it follows
that it has a complete set of orthonormal eigenvectors {ψn}, and non-negative, real
eigenvalues {λn} that are the solutions of the equation
 T/2
−T/2
sG(t,t′)ψn(t′)dt′ = λnψn(t).
(6.51)
By the results in Section 3.4.2, we have the Hilbert–Schmidt decomposition of the
symmetric kernel
sG(t,t′) =
∞

n=0
λnψ∗
n (t)ψn(t′).
(6.52)
It then follows that
sG(t,t) =
∞

n=0
λn|ψn(t)|2,
(6.53)
where the equalities in (6.52) and (6.53) hold in the L2[−T/2,T/2] sense, and also
absolutely and uniformly by virtue of Mercer’s theorem in Appendix A.4.2. From
(6.53), it follows that
 T/2
−T/2
sG(t,t)dt =
∞

n=0
λn.
(6.54)
The Karhunen–Loève representation of the stochastic Green’s function is obtained by
using the deterministic eigenfunctions in (6.51) as an orthonormal basis representation
for the process, and we have
G(t) =
∞

n=0
Anψn(t),
(6.55)
where the coefﬁcients of the expansion are zero-mean random variables, are uncorre-
lated, and have variances
E(AnA∗
n) = λn.
(6.56)
08
12:00:54, subject to the Cambridge Core

6.4 Karhunen–Loève Representation
189
The stochastic convergence of (6.55) is intended in the mean square sense:
lim
N→∞E
⎛
⎝
G(t) −
N−1

n=0
Anψn(t)

2⎞
⎠= 0.
(6.57)
The existence of the representation and the convergence result follow from the
Hilbert–Schmidt decomposition of the autocorrelation function. Consider the mean
squared error
eN(t) = E
⎛
⎝
G(t) −
N−1

n=0
Anψn(t)

2⎞
⎠.
(6.58)
By expanding the square inside the expectation, using (6.56), and since the coefﬁcients
{An} are uncorrelated, we have
eN(t) = E(|G(t)|2) + E
N−1

n=0
N−1

k=0
AnA∗
kψn(t)ψ∗
k (t)

−2E

G(t)
N−1

n=0
Anψn(t)

= sG(t,t) +
N−1

n=0
λn|ψn(t)|2 −2E

G(t)
N−1

n=0
Anψn(t)

.
(6.59)
Substituting
An =
 T/2
−T/2
G(t′)ψ∗
n (t′)dt′
(6.60)
into (6.59), we have
eN(t) = sG(t,t) +
N−1

n=0
λn|ψn(t)|2 −2E
N−1

n=0
G(t)ψn(t)
 T/2
−T/2
G(t′)ψ∗
n (t′)dt′

= sG(t,t) +
N−1

n=0
λn|ψn(t)|2 −2
N−1

n=0
 T/2
−T/2
E[G(t)G(t′)]ψ∗
n (t′)dt′ ψn(t)
= sG(t,t) +
N−1

n=0
λn|ψn(t)|2 −2
N−1

n=0
 T/2
−T/2
sG(t,t′)ψ∗
n (t′)dt′ ψn(t).
(6.61)
Substituting (6.51) into (6.61), it follows that
eN(t) = sG(t,t) −
N−1

n=0
λn|ψn(t)|2,
(6.62)
which tends to zero as N →∞for all t ∈[−T/2,T/2], since (6.53) converges point-wise
in this interval.
One can think of the Karhunen–Loève representation adapting to the process through
the choice of the deterministic basis functions based on the autocorrelation of the
process, in order to produce the best possible basis for its representation. On the other
hand, the joint probability law of the expansion coefﬁcients remains unknown, in the
08
12:00:54, subject to the Cambridge Core

190
Stochastic Representations
absence of information other than the second-order properties of the process. If the
Green’s function is distributed as a zero-mean complex Gaussian in the frequency
domain, then the coefﬁcients in the expansion are independent and Gaussian, and the
series expansion is almost sure convergent.
6.4.2
Optimality of the Karhunen–Loève Representation
The Karhunen–Loève representation is optimal, among all orthonormal basis decompo-
sitions of a stochastic process, in a precise energy sense. This property makes it ideal
to deﬁne the stochastic diversity of a family of processes of a given autocorrelation
function in the same way that we deﬁned the number of degrees of freedom of a family
of functions that are the image of a Hilbert–Schmidt operator.
Arranging the eigenvalues in decreasing order, by combining (6.53) and (6.62) it
follows that the average energy of the error is given by
 T/2
−T/2
eN(t)dt =
 T/2
−T/2
E
⎛
⎝
G(t) −
N−1

n=0
Anψn(t)

2⎞
⎠dt
=
∞

n=N
λn.
(6.63)
Using a Lagrangian formulation, we show that this is the minimum energy of the error
among all orthonormal basis representations of the stochastic process G(t).
By the orthonormality of the {ψn} and the linearity of expectation, we have
E
⎛
⎝
 T/2
−T/2
G(t) −
N−1

n=0
Anψn(t)

2
dt
⎞
⎠= E
 T/2
−T/2
|G(t)|2dt −
N−1

n=0
|An|2

=
 T/2
−T/2
E|G(t)|2dt −
N−1

n=0
E|An|2.
(6.64)
Since the ﬁrst term is independent of the choice of the {ψn}, the energy of the error is
minimized when 
N−1
n=0 E|An|2 is maximized. We need to maximize
N−1

n=0
E(|An|2) =
N−1

n=0
E
 T/2
−T/2
G(t)ψn(t)dt
 T/2
−T/2
G∗(t′)ψ∗
n (t′)dt′

=
N−1

n=0
 T/2
−T/2
 T/2
−T/2
E

G(t)G∗(t′)

ψn(t)ψ∗
n (t′)dt′dt
=
N−1

n=0
 T/2
−T/2
 T/2
−T/2
sG(t,t′)ψn(t)ψ∗
n (t′)dt′dt,
(6.65)
with the orthonormality constraint
 T/2
−T/2
ψn(t)ψ∗
n (t)dt =
 1
if n = m,
0
if n ̸= m.
(6.66)
08
12:00:54, subject to the Cambridge Core

6.4 Karhunen–Loève Representation
191
We aim to maximize the Lagrangian
"({ψn}) =
N−1

n=0
 T/2
−T/2
 T/2
−T/2
sG(t,t′)ψn(t)ψ∗
n (t′)dt′dt
−λn
 T/2
−T/2
ψn(t)ψ∗
n (t)dt −1

.
(6.67)
Differentiating with respect to ψ∗
i and setting the derivative to zero, we get
∂"({ψn})
∂ψi(t)
=
 T/2
−T/2
 T/2
−T/2
sG(t,t′)ψi(t′)dt′ −λiψi(t)

dt = 0,
(6.68)
which is satisﬁed when the integrand is zero, namely when the {ψi} are solutions of the
integral equation (6.51).
6.4.3
Stochastic Diversity
By analogy with the deterministic setting, we deﬁne the stochastic diversity of a set of
stochastic processes as:
The minimum number of uncorrelated parameters sufﬁcient for the average
description of any process in the set within a given precision.
To give a corresponding mathematical deﬁnition, we consider the set of processes of
given autocorrelation function
Ss =

G(t) : E[G(t)G∗(t′)] = sG(t,t′)

,
(6.69)
whose average energy over the observation interval is
ET =
 T/2
−T/2
sG(t,t)dt =
 T/2
−T/2
E(|G(t)|2)dt.
(6.70)
We let the minimum energy error associated with the optimal N-dimensional
Karhunen–Loève approximation be
EN(Ss) =
 T/2
−T/2
eN(t)dt
08
12:00:54, subject to the Cambridge Core

192
Stochastic Representations
= E
⎛
⎝
 T/2
−T/2
G(t) −
N−1

n=0
Anψn(t)

2⎞
⎠dt
=
∞

n=N
λn,
(6.71)
and deﬁne the stochastic diversity of Ss as
Nϵ(Ss) = min{N : EN(Ss)/ET ≤ϵ}.
(6.72)
This deﬁnition should be compared with (3.18). In the deterministic setting, the
number of degrees of freedom corresponds to the dimension of the minimal subspace
representing any signal within ϵ accuracy. For bandlimited signals, the number of
degrees of freedom depends only on the bandwidth  and on the duration of the
observation interval T. In the stochastic setting, the diversity of stochastic processes of
a given autocorrelation function corresponds to the dimension of the minimal subspace
representing on average any process in the set within ϵ accuracy. The stochastic diversity
depends on the duration of the observation interval T, but also on the form of the
autocorrelation function. The comparison between the deterministic and the stochastic
cases is summarized in Table 6.1.
We now consider the error associated with the N-dimensional approximation. In the
deterministic setting, by (3.56) the energy of the error is given by the Nth eigenvalue
of the self-adjoint operator arising from Slepian’s concentration problem. The phase
transition of the eigenvalues (2.131) ensures that the number of degrees of freedom of
bandlimited signals is essentially given by the time–bandwidth product N0 = T/π.
In the stochastic case, by (6.71) the energy of the error is given by the tail sum of the
eigenvalues of the self-adjoint operator whose kernel is the autocorrelation function.
By (6.51), the tail decay of the eigenvalues as a function of their indexes depends on
the particular form of the autocorrelation function. Highly concentrated autocorrelation
functions correspond to highly varying processes that require a large number of terms
to be represented at a given level of accuracy. On the other hand, processes with spread
out correlation functions have somewhat “less wild” stochastic variations and require a
smaller number of terms to be represented, on average, at the same level of accuracy.
Problems 6.3, 6.4, and 6.5 provide examples of this general behavior.
Table 6.1 Degrees of freedom and stochastic diversity.
Deterministic
Stochastic
Bandwidth/autocorrelation

sG
Observation interval
[−T/2,T/2]
[−T/2,T/2]
Fredholm equation
BT ψ(t) = λψ(t)
Sψ(t) = λψ(t)
Degrees of freedom
Nϵ(B) =
Nϵ(Ss) =
min{N : λN ≤ϵ}
min

N : (
∞
n=N λn)/ET ≤ϵ

Effective dimensionality
N0 = T/π
Depends on sG
08
12:00:54, subject to the Cambridge Core

6.4 Karhunen–Loève Representation
193
We also have the following general bound on the error for a wide-sense stationary
process of autocorrelation sG(t,t′) = sG(τ) and power spectral density SG(ω). Letting
N = T
2π

SG(ω)∈(a,b)
dω,
(6.73)
we have that, as T →∞, the error
∞

n=N
λn ≤T
2π
 ∞
−∞
SG(ω) dω −a

SG(ω)∈(a,b)
dω

+ o(T).
(6.74)
This bound is a consequence of the Kac–Murdock–Szegö (1953) theorem – see
Appendix A.7.
6.4.4
Constant Power Spectral Density
We now compute the stochastic diversity in the special case of a wide-sense stationary
Green’s function whose autocorrelation has the form
sG(t,t′) = sG(τ) = 
π sinc(τ),
(6.75)
where t′ = t−τ. By taking the Fourier transform of (6.75), the power spectral density is
SG(ω) =
"
1 for ω ∈[−,],
0 otherwise.
(6.76)
In this case, an impulsive correlation in the time domain corresponds to a spread out
rectangular window for the power spectral density in the frequency domain. We then
expect that larger values of , roughly corresponding to a larger bandwidth in the
deterministic setting, yield a larger stochastic diversity.
To evaluate the stochastic diversity, we ﬁrst note that (6.51) now corresponds to
Sψ(t) = BT ψ(t) = λψ(t),
(6.77)
where T and B are the timelimiting and bandlimiting operators deﬁned in (2.87)
and (2.88). It follows that the eigenvalues and eigenfunctions in the stochastic and
the deterministic cases coincide, as they are both solutions of Slepian’s concentration
problem.
We then note that from (6.76) we have the variance constraint
E(|G(t)|2) = 1
2π
 ∞
−∞
SG(ω)dω = 
π ,
(6.78)
from which it follows that the average energy over the observation interval is
ET =
 T/2
−T/2
sG(t,t)dt
=
 T/2
−T/2
E(|G(t)|2)dt
08
12:00:54, subject to the Cambridge Core

194
Stochastic Representations
= T
π
= N0.
(6.79)
Finally, substituting (6.71) and (6.79) into (6.72), we have
Nϵ(Ss) = min
"
N : 1
N0
∞

n=N
λn ≤ϵ
%
.
(6.80)
From the behavior of the eigenvalues in (2.131), it follows that, as N0 →∞,
Nϵ(Ss) = (1 −ϵ)N0.
(6.81)
This result is illustrated in Figure 6.9. The phase transition of the eigenvalues occurs
in a window of size 2ϵN0, and there are (1 −ϵ)N0 eigenvalues having value one. As
N0 →∞the phase transition of the eigenvalues tends to a step function, and the shaded
area in the ﬁgure that represents the tail sum of the eigenvalues normalized to N0 tends
to ϵ. It follows that by interpolating (1 −ϵ)N0 eigenfunctions we capture most of the
energy of the process. A more rigorous derivation is given in Appendix A.7.2.
6.4.5
Frequency-Varying Green’s Function
A completely analogous treatment can be given if we assume that the process G(ω)
varies randomly in frequency rather than in time, and is observed over the interval
[−,]. In this case, we have
G(ω) =
∞

n=0
Anψn(ω),
(6.82)
where the deterministic basis functions {ψn} are the solutions of the eigenvalue equation
 
−
sG(ω,ω′)ψn(ω′)dω′ = λnψn(ω),
(6.83)
where
sG(ω,ω′) = E[G(ω)G∗(ω′)],
(6.84)
n
Ο
n
1
N   
0
1
ϵ
Fig. 6.9
Evaluation of the tail sum of the eigenvalues.
08
12:00:54, subject to the Cambridge Core

6.4 Karhunen–Loève Representation
195
and the random coefﬁcients {An} are zero-mean random variables, uncorrelated, and
with variances given by the corresponding eigenvalues {λn}.
6.4.6
Spatially Varying Green’s Function
The number of uncorrelated spatial channels that are available through propagation
depends on the spatial correlation structure of the matrix G, and can be obtained
from the spatial Karhunen–Loève decomposition that is the discrete analog of the one
presented in Section 6.4.2. The Karhunen–Loève image ˜G of G is the |R|×|T | random
matrix with uncorrelated entries
˜Gi,k =

m∈R

n∈T
Am,n ψ∗
i,k(m,n),
(6.85)
where the expansion kernel {ψi,k(m,n)} is a set of complete orthonormal basis functions
formed by the eigenfunctions of the spatial autocorrelation of G, namely, for all k ∈R,
i ∈T ,

m′∈R

n′∈T
sG(m,n;m′,n′)ψi,k(m′,n′) = λi,kψi,k(m,n).
(6.86)
The variances of the entries of the Karhunen–Loève image are given by
E( ˜Gi,k ˜G∗
i,k) = E(Ai,kA∗
i,k) = λi,k.
(6.87)
In practice, after estimating the spatial autocorrelation of the process, one can derive
through (6.86) and (6.87) the elements of the image ˜G where the channel response
focuses most of the signal’s energy. These are the principal spectral components of G
corresponding to the largest eigenvalues of its autocorrelation function.
Now, if the expansion kernel can be factorized as
ψi,k(m,n) = ui(m)vk(n),
(6.88)
then we also have the decomposition
G = U ˜GV†,
(6.89)
where U and V are unitary and deterministic, with Ui,m = ui(m) and Vn,k = v∗
k(n).
Matrices whose autocorrelations are separable into two marginal correlations that are
functions of (m,n) and (m′,n′) are automatically factorable, and λi,k = λiλk, where λi
and λk are the ith and kth eigenvalues of the two marginal correlations whose product
equals SG.
From (6.89), we can then obtain an approximation of G retaining only the entries in
the image corresponding to the largest N principal components. Letting P be the index
pair set of these principal components, we let
GN(i,k) =

(m,n)∈P
Ui,m ˜Gm,nVn,k.
(6.90)
08
12:00:54, subject to the Cambridge Core

196
Stochastic Representations
Of course, we expect that the smaller the ignored eigenvalues, the more accurate the
approximation. Indeed, by the uncorrelation of the elements of ˜G, and since U and V
are unitary, we have the approximation error in terms of Frobenius norm
eN = E(∥G −GN∥F)2
= E
⎛
⎝
(m,n)̸∈P
| ˜Gm,n|2
⎞
⎠
=

(m,n)̸∈P
λm,n.
(6.91)
As before, the error is a minimum among all orthonormal basis decompositions of the
random matrix G. The error decreases monotonically with the number of terms retained
in the expansion, at a rate that depends on the tail behavior of the eigenvalues {λn}, and
in turn on the autocorrelation function of the spatial process. The more correlated the
process, the smaller the number of terms needed to achieve a small error. On the other
hand, if the process is weakly correlated, then a larger number of terms is needed.
6.5
Summary and Further Reading
Probabilistic models of multiple scattering attempt an average description of the ﬁeld in
multiple-scattering environments and require a stochastic extension of the propagation
model. One of the ﬁrst stochastic characterizations of the linear, time-varying transfer
function for wireless communication appears in Bello (1963). Tse and Viswanath (2005)
give a communication-theoretic perspective on different stochastic models used in the
literature, and discuss the range of applicability and the modeling philosophy in many
practical cases. As the authors point out, these models are generally far less accurate
than the stochastic models of additive noise; they are important, however, in guiding the
design of real systems. A similar communication-theoretic perspective is provided in
Goldsmith (2005).
The
classic
book
by
Ishimaru
(1978)
provides
a
rigorous
treatment
of
multiple-scattering theory in random media, giving additional physical insight into
the different stochastic models. We discuss this theory in Chapter 10. There, we also
describe a stochastic model of multiple scattering based on random walks in continuum
space that leads to closed-form expressions for the path loss, power-delay proﬁle, and
coherence bandwidth of the channel.
For any probabilistic model, the Karhunen–Loève decomposition provides the
number of uncorrelated coefﬁcients that describes the evolution of the stochastic process
up to arbitrary accuracy. This number depends on the propagation environment through
the form of the autocorrelation function of the process, and is the stochastic counterpart
of the number of degrees of freedom in the deterministic setting. A proof of the
optimality of this decomposition with respect to the mean square error appears in Brown
(1960).
08
12:00:54, subject to the Cambridge Core

6.6 Test Your Understanding
197
By analogy with the number of degrees of freedom in the deterministic case, we
have deﬁned the stochastic diversity in terms of the minimum number of coefﬁcients
needed to achieve a given mean square truncation error. This led to a parallel between
the Hilbert–Schmidt decomposition and the Karhunen–Loève one in determining the
optimal functional representation of the process. Another possibility is to minimize the
expected number of coefﬁcients required to obtain a given truncation error. In this case,
however, the decomposition depends in a detailed manner on the probability distribution
of the process and is not determined just by its correlation function, except in the
Gaussian case – see Algazi and Sakrison (1969).
In the spatial domain, modeling degrees of freedom and stochastic diversity in the
discrete setting of multiple antenna systems is a problem closely related to the study
of the spectral properties of random matrices. Several monographs exist on the topic,
including Tao (2012), and Tulino and Verdú (2004), the latter focusing on applications
of random matrix theory to wireless communication.
When multiple antennas are deployed in a distributed setting to form a network,
the typical approach is to consider random geometric matrices. In this case, a random
spatial distribution for the locations of the transmitters and receivers is assumed. The
randomness in the resulting channel’s matrix is induced by the underlying random
spatial distribution of the transmitters and receivers. This approach is part of a larger
research area that models wireless networks using random graphs, percolation theory,
and stochastic geometry, and is described in the books by Franceschetti and Meester
(2007) and Haenggi (2013), as well as in the tutorial paper by Haenggi et al. (2009).
6.6
Test Your Understanding
Problems
6.1
Consider a random multi-path model in the presence of a deterministic
line-of-sight component A0 exp(jφ0), and determine the resulting distribution for
Gi,k(ω).
Solution
We choose the phase reference so that φ0 = 0 and drop the i,k from the notation for
convenience. We then consider the complex random variable
ℜ(G −A0) + jℑG.
(6.92)
Using the central limit theorem, the real and imaginary parts have Gaussian dis-
tributions, and by the same reasoning as in Section 6.3, we have that the joint
distribution
fℜG,ℑG =
1
2πσ 2 exp

−(ℜg −A0)2 + ℑg2
2σ 2

.
(6.93)
08
12:00:54, subject to the Cambridge Core

198
Stochastic Representations
The probability density for the amplitude A can then be computed to be
fA(a) =
 2π
0
fℜG,ℑG(ℜg,ℑg)adφ
= a
σ 2 exp

−a2 + A2
0
2σ 2

I0
A0a
σ 2

,
(6.94)
where I0(·) denotes the zeroth-order modiﬁed Bessel function of the ﬁrst kind. This is
the Rice distribution. The factor
K = A2
0
2σ 2
(6.95)
denotes the ratio between the power of the line-of-sight path component and the power
of the Rayleigh component. As K →0, since I0(0) = 0, the Rice distribution approaches
a Rayleigh as the line-of-sight component becomes negligible. On the other hand, as
K →∞, the Rice distribution tends to a Gaussian.
6.2
Consider the case in which the stochastic Green’s function is white Gaussian,
namely
SG(ω, ˜ω) = Nδ(ω −˜ω).
(6.96)
Show that the Karhunen–Loève expansion consists of a sequence of i.i.d. Gaussian
random variables of variance equal to N.
6.3
Widom (1964) provides several results that are useful in relating the form of the
autocorrelation and its concentration properties to the behavior of the eigenvalues in
(6.51). Consider the stochastic process G(t) deﬁned in [−1,1] with the exponential
correlation model
sG(t,t′) = k(τ) = 1
2 exp(−|τ|/ℓ),
(6.97)
where τ = t −t′. Use Theorem 1 in Widom (1964) to determine the decay of the
eigenvalues of the Fredholm integral equation,
 1
−1
k(t −t′)ψ(t′)dt′ = λψ(t).
(6.98)
6.4
Consider the stochastic process G(t) deﬁned in [−1,1] with the Gaussian
correlation model
sG(t,t′) = k(τ) = 1
2 exp(−τ 2/ℓ2),
(6.99)
where τ = t −t′. Use Theorem 3 in Widom (1964) to determine the decay of the
eigenvalues of the Fredholm integral equation,
 1
−1
k(t −t′)ψ(t′)dt′ = λψ(t),
(6.100)
and compare the results with the ones in Problem 6.3. Which of the two models suggests
a larger number of degrees of freedom?
08
12:00:54, subject to the Cambridge Core

6.6 Test Your Understanding
199
6.5
Fix the value of ℓsuch that the quantity
c =
 ∞
0 τk(τ)dτ
 ∞
0 k(τ)dτ
(6.101)
is the same for the two models in Problems 6.3 and 6.4. This quantity is a measure of
the average spread of the correlation function. Investigate numerically the decay of the
eigenvalues and the number of degrees of freedom of the two families of processes.
How does this change as c decreases?
08
12:00:54, subject to the Cambridge Core

7
Communication Technologies
Technology and the machine resurrected San Francisco while Pompeii still slept in her ashes.1
7.1
Applications
The theory developed in the previous chapters provides an information-theoretic
description of electromagnetic waves in terms of their effective dimensionality.
Natural applications of this theory are in communication and sensing systems. In
communication systems, waves are used to carry information from transmitters to
receivers. In remote sensing systems, they are used to extract information from the
environment, probing the surrounding space. When viewed in appropriate asymptotic
regimes, the amount of information that waves can carry is limited, and their effective
dimensionality, expressed in terms of the number of degrees of freedom, depends on
the Green’s function that describes the input–output relationship of the system. Only
a limited number of orthogonal communication channels can be established between
transmitters and receivers, or a limited number of coordinates need to be speciﬁed to
identify any sensed object using electromagnetic radiation. This limitation is a basic
result imposed by the physics of propagation, and mathematically described by the
theory of spectral concentration of bandlimited functions.
In recent decades, many technologies have been developed to best exploit the
available number of degrees of freedom of electromagnetic waveforms. In this chapter,
we focus on some of the communication aspects, showing how different technologies
are subject to information-theoretic limits.
7.2
Propagation Effects
We start by considering two ideal canonical cases. In the ﬁrst case, communication
occurs by transmitting a sequence of closely spaced short pulses in a ﬁxed time interval
of size T, spread over a large frequency bandwidth . In this case, the total number of
degrees of freedom of the signal grows as T/π as  →∞, and increasingly shorter
pulses can be more closely packed together – see Figure 7.1(a). In the second case,
1 Silas Bent (1930). Machine Made Man, Farrar and Rinehart, p. 326.
09
12:00:51, subject to the Cambridge Core

7.2 Propagation Effects
201
T
∞
ω
Δω
0
T
∞
t


Δt
0
(a)
(b)
Fig. 7.1
(a) Wide-band transmission. (b) Narrow-band transmission.
t
t
TRANSMITTED
RECEIVED
Fig. 7.2
Time broadening and distortion causes inter-symbol interference in wide-band transmission. The
time pulses are broadened at the receiver and partially overlap.
communication occurs by transmitting a sequence of narrow frequency pulses in a ﬁxed
bandwidth , spread over a long time interval of size T. In this case, the total number of
degrees of freedom of the signal grows as T/π as T →∞– see Figure 7.1(b). These
two basic strategies are employed by communication systems, depending on whether
their operation uses wide-band or narrow-band transmission.
When propagation occurs in a multiple scattering environment, these strategies
are affected by environmental limitations. Because of multiple scattering, multiple
delayed copies of each transmitted pulse reach the receiver along different paths
carrying different phase shifts, and this results in time broadening and distortion of the
received waveform. It follows that communication cannot be performed using a train
of arbitrarily concentrated time pulses narrowly separated in time, as their broadened
versions overlap at the receiver causing inter-symbol interference – see Figure 7.2. This
effect can also be explained in the frequency domain by considering the environment
as a frequency ﬁlter. Each transmitted pulse occupies a large frequency band and is
09
12:00:51, subject to the Cambridge Core

202
Communication Technologies
t
ω
TRANSMITTED
RECEIVED
ω
t
Fig. 7.3
Frequency broadening and distortion causes frequency interference in narrow-band
transmission. The frequency pulses are broadened at the receiver and partially overlap.
distorted by the ﬁlter, due to multi-path interference. As discussed in Chapter 6 and
indicated in Figure 6.7(b), when the bandwidth of a transmitted pulse exceeds the
coherence bandwidth of the channel, some frequency components fall into regions
of deep fade of the ﬁlter’s transfer function, and cannot be excited at the receiver.
This limits the number degrees of freedom of wide-band transmission, thus limiting
the multiplexing capability of any system exploiting this technology. On the other
hand, multiple scattering increases the amount of stochastic frequency diversity, as the
coherence band decreases and the signal appears to be less correlated over frequency.
Another limitation to the number of degrees of freedom arises for narrow-band
communication when the environment is time varying due to mobility of transmitters,
receivers, or scatterers. In this case, due to the frequency broadening of the received
waveform, communication cannot be performed using highly concentrated narrow-band
frequency pulses of arbitrary long time, as the environment acts as a time ﬁltering
operator for any given frequency component. The situation is the dual of the time
broadening described above – see Figure 7.3. When multiple narrow-band signals
are transmitted over the same time interval, the corresponding frequency pulses are
broadened by propagation effects and may overlap at the receiver, causing interference
in the frequency domain. On the other hand, mobility increases the amount of stochastic
time diversity, as the coherence time decreases and the signal appears to be less
correlated over time.
Table 7.1 summarizes the different environmental effects. The physics that forms
the basis of these effects is described in more detail in the next chapters. Here we
concentrate on their inﬂuence on system design principles.
09
12:00:51, subject to the Cambridge Core

7.2 Propagation Effects
203
Table 7.1 Environmental effects.
Limits
Increases
Scattering
Wide-band transmission
Frequency diversity
Mobility
Narrow-band transmission
Time diversity
Physical limitations also occur in space. When communication occurs in an arbitrary
scattering environment using spatially distributed antennas, the physical size of the
system composed by the antennas and the scatterers limits the space–wavenumber
degrees of freedom. This effect is studied in detail in Chapter 8.
7.2.1
Multiplexing
Multiplexing different streams of information from different users can be implemented
in frequency, time, or space, by dividing the frequency band into small narrow-band
portions, the time axis into different time slots, or by sharing the spatial bandwidth
among multiple transmitting and receiving antennas. The ﬁrst case exploits narrow-band
communication by dividing the frequency band into small portions allocated to the
different users. Since narrow-band signals have longer time durations, they are less
affected by time spread due to multiple scattering as the overlap between successive
signals is typically a small fraction of the signal’s length. In the language of
communications, inter-symbol interference is typically negligible. On the other hand,
they are more sensitive to frequency spread due to mobility, which can generate
signiﬁcant interference between the narrow-band carriers. The second case divides time
into small portions; it is less affected by frequency spread, but is more sensitive to time
spread. The third case divides space, and is limited by the spatial spread of the signal.
The three cases correspond to the three different operating regimes depicted in
Table 7.2. Frequency multiplexing is exploited in the large time-length regime, when
orthogonal frequency signals are multiplexed, each occupying a narrow frequency band.
As more and more users transmit using shorter and shorter frequency pulses, their
signals are spread over a longer time interval. Time multiplexing is exploited in the
wide-band regime, when orthogonal time signals are multiplexed, each occupying a
short time interval. As more and more users transmit using shorter and shorter time
pulses, their signals are spread over a longer frequency interval. Letting W be the
bandwidth in the wavenumber domain, spatial multiplexing is exploited in the large
wavenumber band regime, when orthogonal signals are multiplexed in the wavenumber
domain, each concentrated in a small spatial interval.
All multiplexing strategies are based on the idea of diagonalizing the channel into
parallel time–frequency or space–wavenumber subchannels using appropriate basis
functions. As discussed in Chapter 5, the optimal strategy yielding the largest number of
parallel channels would require solving an eigenvalue problem for the Green’s function
at hand. This is not possible in practice, due to incomplete knowledge of the complex
scattering environment and its variability over time and space. For this reason, ad hoc
strategies have been developed based on different choices of the basis functions used to
09
12:00:51, subject to the Cambridge Core

204
Communication Technologies
Table 7.2 Multiplexing limitations in different
regimes.
Regime
Multiplexing
Limitation
T →∞
Frequency
Frequency spread
 →∞
Time
Time spread
W →∞
Space
Spatial spread
perform the decomposition. Each strategy is affected by different trade-offs, and their
performance depends on the features of the environment where propagation occurs.
7.2.2
Diversity
The trade-off between diversity and multiplexing is one of the most important concepts
in modern communication systems. The degrees of freedom of electromagnetic signals
can be exploited for communication by either multiplexing different streams of
information from different users, or by improving the reliability of transmission and
mitigating the random effects of the channel variations. In the ﬁrst case, independent
signals are sent through the channel to maximize the total information ﬂow. In the
second case, correlated signals are sent to achieve reliability by exploiting the stochastic
diversity available in the channel. The idea to exploit diversity is that rather than making
the success of a transmission entirely dependent on a single realization of the channel,
it is possible to hedge the transmission’s success across multiple realizations in order to
decrease the probability of communication failure due to the signal experiencing a deep
fade. A simple example of this is repetition. Consider propagation in a time-varying
multiple scattering environment. The electromagnetic signal is affected by random
fading occurring in time, frequency, and space. By transmitting the same signal
over multiple frequency bands, multiple time slots, or multiple antennas, and if the
realizations of the channel are independent over frequency, time, or space, then the
probability that at least one transmission does not occur in a deep fade can be made
close to one. Of course, this comes at the expense of consuming degrees of freedom,
reducing the multiplexing capability of the system. To minimize this consumption, one
may note that requiring multiple independently faded copies of the same signal is clearly
an overly stringent requirement to achieve reliability. Enter coding: instead of repeating
the same signal at different time, frequency, or space blocks, one can encode the signal
using an error-correcting code and transmit it over multiple realizations of the channel.
In this case, correlated signals are transmitted through the channel providing the desired
reliability against fading without sacriﬁcing excessive amounts of resources, as in the
case of repetition.
The frequency spread that limits the performance of frequency division technologies
introduces time diversity, while the time spread that limits the multiplexing performance
of time division technologies introduces frequency diversity. It follows that frequency
multiplexing transmission occurring over multiple carriers can exploit time diversity,
improving the reliability of transmission through coding over time. Time multiplexing
09
12:00:51, subject to the Cambridge Core

7.3 Overview of Current Technologies
205
transmission occurring over multiple time slots can exploit frequency diversity,
improving reliability of transmission through coding over frequency. Similarly, spatial
diversity, occurring when multiple transmitting and receiving antennas are sufﬁciently
spaced apart in a scattering environment, can also be exploited for reliability by coding
over space. As a simple example, if two receive antennas are sufﬁciently spaced
apart, the same signal is received over independently faded paths and this squares the
probability of experiencing a deep fade.
7.3
Overview of Current Technologies
Communication systems aim to achieve the maximum multiplexing of information,
while guaranteeing reliability of transmission. In the following, we restrict our
discussion to the fundamental principles of some of the current communication
technologies. The interested reader may refer to the wide range of existing literature
for a more in-depth technological perspective.
7.3.1
OFDM
The orthogonal frequency division multiple access technology (OFDM) used for
broadband internet service and cellular systems in the United States is based on the
superposition of many narrow-band signals tightly packed in frequency – see Figure 7.4.
These signals partially overlap in the frequency domain, and to control the interference
between them the amount of overlap is chosen so that the peak of one signal occurs at the
null of the other ones – from which comes the name orthogonal frequency division. The
different frequencies are then assigned to different users, which can access the channel
simultaneously. User mobility means that OFDM is sensitive to frequency spread, as this
can cause loss of orthogonality between signals and interference between the different
narrow-band signals. On the other hand, mobility introduces time diversity, and each
narrow-band signal experiencing independent time variations allows users to perform
coding across time blocks to improve reliability of transmission.
7.3.2
MC-CDMA
In the presence of frequency diversity due to multiple scattering, coding in OFDM
systems can also occur over frequency by assigning multiple frequencies to each user.
This is an example of the trade-off between multiplexing and reliability, as by coding
over different frequencies each user consumes degrees of freedom to provide reliability
of transmission rather than using them for multiplexing signals from different users.
An extreme case of coding over frequencies is multiple carrier code division multiple
access (MC-CDMA), in which every user has access to all frequency channels, encodes
its signal across all frequencies to keep it distinct from the other users, and then spreads
it over time by transmitting it over the multiple narrow-band channels.
09
12:00:51, subject to the Cambridge Core

206
Communication Technologies
ω
|F(ω)|
ω
Fig. 7.4
Representation of the OFDM spectrum as a sum of orthogonal narrow-band components.
Users in MC-CDMA systems modulate individual codes to carry their signals. The
coding is chosen so that the resulting signals are well separated; ideally, the correlation
between the encoded signals is zero. Discrimination among the received signals can
then be made by correlating the received signal with the locally generated code of the
desired user. If the signal matches the desired user’s code then the correlation is high
and the system can extract that signal. Otherwise, the correlation is close to zero and the
system can reject that signal.
7.3.3
GSM
The global system for mobile communication (GSM), widely used for cellular systems
around the world and especially in Europe, is based on orthogonal time division
multiple access technology (TDMA), which divides the spectrum into different portions
and allocates each portion to different users at different times. Thus, users transmit
waveforms occupying the same portion of the spectrum in orthogonal time slots. This
technology is sensitive to time spread due to multiple scattering, which can cause
excessive overlap and interference between signals in time. On the other hand, multiple
scattering introduces frequency diversity, and users can hop between different spectrum
allocations to improve reliability of transmission.
7.3.4
DS-CDMA
In the presence of time diversity, coding in TDMA systems can also occur over time by
assigning multiple time slots to each user. This is an example of the trade-off between
09
12:00:51, subject to the Cambridge Core

7.3 Overview of Current Technologies
207
Transmitters
Receivers
Transmitters
Receivers
Fig. 7.5
Representation of 2 × 2 MIMO. The four channels can be diagonalized into two parallel
channels providing multiplexing gain, or they can be used for redundant transmissions providing
reliability gain.
multiplexing and reliability, as by coding over different time slots each user consumes
degrees of freedom to provide reliability of transmission rather than using them for
multiplexing signals from different users. An extreme case is direct sequence code
division multiple access (DS-CDMA). This is the time dual of MC-CDMA, and is based
on spread spectrum transmission. Every user has access to the channel at all times,
encodes its signal over time to keep it distinct from the other users, and then spreads it
over frequency by transmitting it over the entire spectrum. Reliability of transmission
over the frequency-varying channel is obtained by choosing codes that, when correlated
with a delayed version of the transmitted signal, also result in a value close to zero.
Because the delayed versions of the transmitted codes will have poor correlation with
the original code, they will appear as another user, which is ignored at the receiver. In
other words, the correlation properties of the codes are such that as long as the multiple
scattering introduces a large enough delay among the received signals, these appear
uncorrelated with the intended signal, and they are thus ignored.
7.3.5
MIMO
When communication occurs using multiple antennas, the available spatial degrees
of freedom can be exploited either by multiplexing signals over independent spatial
channels or by improving the reliability of transmission by sending redundant signals
through coding to mitigate random spatial channel variations. Figure 7.5 provides a
schematic representation of a 2×2 multiple-input multiple-output (MIMO) system. The
four channels between transmitter and receiver can be diagonalized into two parallel
channels supporting independent streams of information, as described in Section 5.3.2.
This requires pre-processing and post-processing operations as depicted in Figure 5.9.
Each transmitter antenna sends a linear combination of all input signals through
the channel, and these are jointly decoded at the receivers. Alternatively, the four
channels can be used for redundant signaling from each transmitting antenna to both
receiving antennas. If the channel experiences independent realizations across space,
then appropriate coding schemes can be designed to maximize the probability of
successful communication despite the random channel realizations.
As for time–frequency transmission technologies, for MIMO there is also an inherent
trade-off between multiplexing and diversity. To increase multiplexing, independent
09
12:00:51, subject to the Cambridge Core

208
Communication Technologies
signals must be sent over parallel channels. To increase reliability, redundancy needs
to be introduced through coding to mitigate the channel variations so that correlated
signals are sent over independent realizations of the channel. In the simplest case
of repetition coding, the same signal is sent over multiple realizations so that the
multiplexing gain is zero, and channel diversity is fully exploited for reliability. With
more sophisticated coding schemes, it is possible to trade-off between the two.
7.4
Principles of Operation
The leitmotiv of all digital communication technologies is that they rely on an
orthogonal representation of the channel using appropriate basis functions, and then
use this representation to provide multiplexing and reliability. Due to multiple scattering
and incomplete knowledge of the Green’s function, an optimal decomposition providing
the largest possible number of parallel channels cannot be performed, and ad hoc
basis functions are used in practice that work well under certain assumptions on the
channel. Nevertheless, the basic principle remains the same, and the degrees of freedom
view developed in the previous chapters can be used to provide upper bounds on the
multiplexing capability that can be achieved in practice.
A block diagram for the operation of time–frequency decomposition technologies,
such as OFDM, TDMA, and CDMA, is depicted in Figure 7.6. One for the operation
of space–wavenumber decomposition technologies, such as MIMO, is shown in
Figure 7.7. In these diagrams, the basis functions used for the representation of the input
and output signals are generally not the optimal ones arising from the Hilbert–Schmidt
decomposition of the Green’s function. In the case of the Hilbert–Schmidt representa-
tion, the effect of the channel would simply be that of multiplying each coefﬁcient xn by
the singular value √λn of the channel’s decomposition, as indicated in Figure 5.10, and
in its discrete equivalent for a spatial channel – Figure 5.9. When a different choice of
basis functions is performed, the effect on the channel and the multiplexing capability
of the system depend on how well the given representation matches the channel’s
conditions.
x0
x   
N-1
+
*
∫dt
∫dt
y0
y   
0
g(τ)
(t-τ)
N-1(t-τ)
∫dτ
0(t)
N-1(t)
N-1
Transmitter
Channel
Receiver
*
ξ
ξ
Fig. 7.6
Block diagram of continuous time orthogonal time–frequency decomposition technology.
09
12:00:51, subject to the Cambridge Core

7.4 Principles of Operation
209
x0
x   
N-1
+
*
*
∫
∫
y0
y   
0
g(
(
N-1
∫d
0(
N-1( )
N-1
Transmitter
Channel
Receiver
(
)
r
dr
dr
r
ξ
ξ
Fig. 7.7
Block diagram of continuous space orthogonal space–wavenumber decomposition technology.
The basis functions for the OFDM technology correspond to N narrow-band
orthogonal signals, each carrying a symbol xm from the mth user. The basis functions
for the TDMA technology correspond to N non-overlapping time pulses representing
the time allocation slots for the different users, each carrying a symbol xm from
the mth user. The basis functions for the DS-CDMA technology correspond to N
wide-band orthogonal signals representing code sequences spread over the entire
spectrum, each carrying a symbol xm for mth user. Finally, in the case of MC-CDMA, the
N narrow-band basis functions of the OFDM signal are employed by each user by ﬁrst
encoding the signal in frequency using a wide-band code, and then sending it over the N
narrow-band subcarriers. For space–wavenumber technologies, the basis functions for
the MIMO system correspond to the columns of the pre-processing and post-processing
matrices performing modulation of the signal in space.
These orthogonal representations effectively divide the time, frequency, or space
spectrum among the different users, and also allow for coding across the spectrum.
To clarify the basic principles and the fundamental limitations of their operation, we
consider three examples in more detail. The ﬁrst example is OFDM. In this case, users
transmit over different portions of the frequency spectrum, sharing the time–frequency
degrees of freedom provided by the electromagnetic channel. The second example is
CDMA. In this case, users transmit over the whole spectrum, but again the effective
bandwidth available to each user is only a portion of the entire spectrum. Due to coding,
only a portion of the transmitted bits are effectively used as data by each user, so that
the total spectral resource is again shared among the different users. The multiplexing
capability of the system in this case depends on the number of orthogonal codes that
can ﬁt in the given spectrum and it is again limited by the time–frequency degrees of
freedom provided by the physical channel. Finally, the same basic principles are applied
to MIMO, where rather than the time–frequency degrees of freedom, the resource
divided among the users is represented by the space–wavenumber degrees of freedom.
7.4.1
Orthogonal Spectrum Division
Consider an OFDM system with N subcarriers, an (approximate) angular frequency
band  and an (approximate) signal duration T. The ﬁrst part of the signal, of duration
09
12:00:51, subject to the Cambridge Core

210
Communication Technologies
t
0
Tp
T
Fig. 7.8
Cyclic preﬁx for an OFDM signal.
Tp, is the cyclic preﬁx that is a copy of the last part of the signal that is prepended to the
transmitted one – see Figure 7.8. The signal transmitted in one transmission period of
length T is
x(t) =
N−1

m=0
xmϕm(t),
(7.1)
where {xm} are points from the signal’s constellation and {ϕm} are functions orthonormal
in [Tp,T]:
ϕm(t) =
⎧
⎪⎪⎨
⎪⎪⎩


πN exp
&
jm 
πN (t −Tp)
'
if t ∈[0,T],
0
if t ̸∈[0,T],
(7.2)
where
T = πN
 + Tp.
(7.3)
In a multi-user system, the interpretation is that the OFDM signal uses N subcarriers,
each carrying a symbol xm from the mth user in a time period of length T. Note that for
T ∈[0,Tp], we have ϕm(t) = ϕm(t + πN/).
We now make two assumptions about the channel. First, we assume that the channel
is time invariant for the duration of the transmission period. This corresponds to a block
fading model where the duration of each block is synchronized with the duration of the
transmitted signal. Second, we assume that the length of the cyclic preﬁx is larger than
the support of the impulse response of the channel, so that subsequent signals that are
sent through the channel and are affected by time spread due to multi-path, signiﬁcantly
overlap only within the cyclic preﬁx, which can be ignored at the receiver. With these
assumptions, we can write the signal at the receiver as
y(t) =
 Tp
0
x(t −τ)g(τ)dτ
=
 Tp
0
N−1

m=0
xmϕm(t −τ)g(τ)dτ.
(7.4)
For all m ∈[0,N −1] and t ∈[Tp,T], we have
 Tp
0
ϕm(t −τ)g(τ)dτ =
 Tp
0


πN exp
&
jm 
πN (t −τ −Tp)
'
g(τ)dτ
09
12:00:51, subject to the Cambridge Core

7.4 Principles of Operation
211
= ϕm(t)
 Tp
0
g(τ)exp
&
−jm 
πN τ
'
dτ
= G
m
πN

ϕm(t),
(7.5)
where G(m/(πN)) is the sampled frequency response of the channel at the mth
subchannel frequency ω = m/(πN). By combining (7.4) and (7.5) and letting Gm =
G(m/(πN)), it follows that
y(t) =
N−1

m=0
Gm xm ϕm(t),
(7.6)
and the received signal can be viewed as the superposition of different complex
exponentials {ϕm(t)}, spaced in frequency by /(Nπ), and each weighted by the
sampled frequency response.
The nth receiver can now recover the nth input coefﬁcient of the signal, ignoring the
cyclic preﬁx, by computing
yn =
 T
Tp
y(t)ϕ∗
n(t)dt
=
 T
Tp
 Tp
0
N−1

m=0
xmϕm(t −τ)g(τ)dτ ϕ∗
n(t)dt
=
N−1

m=0
Gm xm
 T
Tp
ϕm(t)ϕ∗
n(t)dt
= Gn xn.
(7.7)
The input–output representation in terms of N parallel frequency channels is then
yn = Gnxn, n = 0,...,N −1,
(7.8)
and should be compared with the result for the Hilbert–Schmidt orthogonal represen-
tation (5.54). By chosing an orthogonal representation where the basis functions are
the complex exponentials, the effect of the channel is to multiply each coefﬁcient
of the representation by a sampled value of the Green’s function in the frequency
domain, rather than multiplying it by a singular value of the Green’s operator. With the
assumptions made on the channel’s response, the output at each subcarrier is centered
at the same frequency as the input, so that there is no inter-carrier interference in the
frequency domain. In addition, the problem of interference between successive signals
in the time domain is solved using the cyclic preﬁx. Thanks to the preﬁx, the technology
is not affected by time spread, as long as the length of the preﬁx is large enough.
On the other hand, since the cyclic preﬁx is ignored at reception, this causes the
channel to be idle for some time. From (7.3), we have that the number of orthogonal
channels that are used is
N = (T −Tp)
π
,
(7.9)
09
12:00:51, subject to the Cambridge Core

212
Communication Technologies
causing a loss in degrees of freedom of Tp/π. In order to minimize this overhead, one
would like to have
N ≫Tp
π .
(7.10)
Letting Tc be the coherence time of the channel, the transmission time T over which the
channel can be considered constant is necessarily
T ≪Tc.
(7.11)
Letting Ts be the time spread of the channel, it must also be that the length of the cyclic
preﬁx is
Tp ≫Ts.
(7.12)
It then follows, by combining (7.9), (7.10), (7.11), and (7.12), that the OFDM
technology performs well when
Ts
π
≪N ≪Tc
π ,
(7.13)
namely when the coherence time of the channel is much larger than the delay spread,
so that the number of subcarriers can be chosen large enough to compensate for the
degrees of freedom loss due to the cyclic preﬁx, but still smaller than the degrees
of freedom offered by the channel during the time it can be considered constant. To
better understand this latter limitation, recall that as N grows larger and more and more
pulses are tightly packed in frequency, the transmission time of each signal over each
subcarrier also grows. During this long transmission period the channel may not be
considered constant anymore, and this leads to the limitation on the right-hand side of
(7.13). The analogous frequency interpretation is that the frequency spread does not
allow the transmission of frequency pulses arbitrarily close to each other.
7.4.2
Orthogonal Code Division
Consider now a DS-CDMA system with N codes, an (approximate) angular frequency
band , and a signal of (approximate) duration T. The principle of operation represented
by the OFDM block diagram in Figure 7.6 still holds. A signal x(t) is constructed and
sent through the channel over an interval of time T:
x(t) =
N−1

m=0
xmϕm(t).
(7.14)
However, in this case the basis functions {ϕm}, rather than being narrow-band carriers
centered across the spectrum as depicted in Figure 7.4, are fast-varying orthonormal
code sequences, as depicted in Figure 7.9, each occupying the whole bandwidth .
The signal ϕm(t) is used to transmit a single coefﬁcient xm over time T using κ short
pulses each of time length T0, called “chips” in CDMA jargon. The spreading factor κ
is called the processing gain of the system. As κ grows larger, and more and more chips
are closely packed together in time, the bandwidth  of the transmitted code sequence
09
12:00:51, subject to the Cambridge Core

7.4 Principles of Operation
213
0
T0
T=κT0
m(t)
Fig. 7.9
Direct sequence CDMA spreading waveform.
becomes of the order of 1/T0, and the time–bandwidth product of ϕm(t) becomes of the
order of κ. It follows that
κ = T
T0
≈T
π
(7.15)
roughly represents the number of degrees of freedom of the electromagnetic signal
in terms of the total time–frequency resource available. For any given length of
transmission T, the number of degrees of freedom is increased by transmitting chips of
shorter duration T0, which corresponds to increasing the bandwidth  of the transmitted
signal. On the other hand, for any given bandwidth , the number of degrees of
freedom is increased by transmitting a larger number of chips, increasing the length
of transmission T. As in the OFDM case, both of these strategies are limited by the
properties of the channel: the time spread does not allow T0 to be arbitrarily small, and
also the channel cannot be considered time invariant for an arbitrarily long duration
interval T.
If orthogonality is preserved when the resulting signal is passed through the channel,
the transmission coefﬁcients can be recovered at the receiver in the usual way by
correlation with the appropriate coding sequence. In free space, the channel only
provides delay and attenuation, so that if proper synchronization can be maintained
between transmitter and receiver, orthogonality is preserved. However, in the presence
of multi-path, multiple copies of the transmitted codeword may reach the receiver.
In this case, appropriate codes are required that are not only pair-wise orthogonal, to
discriminate among users, but that when correlated with their delayed version received
through multi-path propagation also result in a value close to zero. In the case that
pseudo-random spreading sequences are used, this amounts to using sequences whose
autocorrelation at the receiver is almost impulsive, so that they appear almost as white
noise and can be discarded.
Assuming a multi-path model where on each path the received signal is a delayed and
attenuated version of the transmitted signal, we have
y(t) =

m∈P
amx(t −τm),
(7.16)
09
12:00:51, subject to the Cambridge Core

214
Communication Technologies
where P is the set of multiple scattered paths, am is the attenuation along the mth path,
and τm is the delay along the mth path. The corresponding Green’s function is
g(t) =

m∈P
amδ(t −τm).
(7.17)
We assume that the coefﬁcients {am} do not vary during each transmitted block of κ
chips. This corresponds to having a channel that is invariant for the duration of each
transmitted signal, so that T ≪Tc and, by virtue of (7.15),
κ ≪Tc
π .
(7.18)
We also assume that the Green’s function (7.17) falls off rapidly, so that the overlap
between subsequent signals affected by time spread due to multi-path is negligible. In
this way, we can consider the response to each transmitted signal x(t) in isolation. This
corresponds to having T ≫Ts and, by virtue of (7.15),
κ ≫Ts
π .
(7.19)
Notice that the constraints (7.18) and (7.19) are equivalent to (7.13), and we are again in
the case of an underspread channel, where the coherence time is signiﬁcantly larger than
the delay spread. The number of different frequencies N used to transmit the OFDM
signal over the frequency band  corresponds to the number of different chips κ used
to transmit the CDMA signal over time T, so that κ plays the role of the multiplexing
capability of the system, and in (7.14) we let N < κ to ensure the orthogonality of all
the transmitted coding sequences.
With these assumptions, we can write the signal at the receiver as simply the
convolution
y(t) =
 ∞
−∞
x(τ)g(t −τ)dτ.
(7.20)
We also have the cardinal series expansion for the bandlimited signal x(t):
x(t) =
∞

m=−∞
x
mπ


sinc(t −mπ).
(7.21)
By substituting (7.21) into (7.20) and using the Fourier transform pair
sinc(t) ←→π
rect
 ω
2

,
(7.22)
we have
y(t) = 1
2π
∞

m=−∞
 
−
G(ω)x
mπ

 π
rect
 ω
2

exp
,
jω

t −mπ

-
dω
= π

∞

m=−∞
g

t −mπ


x
mπ


= π

∞

m=−∞
x

t −mπ


g
mπ


.
(7.23)
09
12:00:51, subject to the Cambridge Core

7.4 Principles of Operation
215
Letting
gm = (π/)g(mπ/),
(7.24)
we have
y(t) =
∞

m=−∞
x(t −mπ/)gm.
(7.25)
It follows that the received signal can be seen as the discrete superposition of different
time components, spaced in time by π/, and each weighted by the sampled Green’s
function. This should be compared with (7.6), which provides the analogous frequency
domain interpretation for the OFDM case, where the signal is seen as the superposition
of different frequency components, spaced in frequency by /(Nπ).
By retaining only the indexes corresponding to the most signiﬁcant terms in the series,
the corresponding Green’s function can be written as
g(t) =

m∈M
gmδ(t −mπ/),
(7.26)
and, comparing it with the multi-path model in (7.17), we deduce that a signal of
bandwidth  achieves a multi-path delay resolution of π/. We can then model the
received signal with a tapped ﬁnite delay line with tap spacing π/ and tap coefﬁcients
gm – see Figure 7.10. The number of taps |M | is in practice bounded by Ts/π ≪κ.
The corresponding physical picture is given in Figure 7.11.
If the coding sequences remain orthogonal when subject to multi-path delay, then the
nth transmitted coefﬁcient can be recovered by correlating the received signal with the
nth spreading waveform, and we have
yn =
 T
0
y(t)ϕn(t)dt
=
 T
0

m∈M
x(t −mπ/)gmϕn(t)dt
=
N−1

k=0

m∈M
xkgm
 T
0
ϕn(t)ϕk(t −mπ/)dt
= xn

m∈M
gm,
(7.27)
and the input–output representation in terms of N parallel channels is
yn = xn

m∈M
gm, n = 0,...,N −1.
(7.28)
Comparing (7.28) and (7.8), it follows that the channel’s weight corresponding to one
value of the frequency response in the case of OFDM now corresponds to the sum of
different samples of the Green’s function in time representing the different multi-path
components.
09
12:00:51, subject to the Cambridge Core

216
Communication Technologies
x(t)
x
x
g1
g
+

π
x
g2
y(t)

π

π
x
g0
Fig. 7.10
Tapped delay line model of the CDMA channel.
T0
t
τ
T0
Ts
T
Fig. 7.11
Sequence of chips and corresponding delayed multi-path returns.
7.4.3
Exploiting Diversity
In CDMA, reliability of transmission can be improved by exploiting the frequency
diversity provided by the multiple delayed paths. The output can be correlated with
different delayed versions of the candidate transmitted sequence ϕn spaced by multiples
of π/, which is of the order of the chip time T0. The result of each correlation
provides an estimate for the coefﬁcient xn; it is weighted by the corresponding channel
coefﬁcient, and then summed with all the others. In this way, branches with strong
correlations are further ampliﬁed, while weak correlations are weighted less – see
Figure 7.12. This improves the accuracy of the detection of the desired coefﬁcient xn, as
shown in Problem 7.6.
This type of processing is called a rake receiver, and it improves reliability by
extracting frequency diversity in an attempt to collect the signal’s energy from all
the delayed signal paths that fall within the span of the receiver’s delay line at
resolution π/. With a bit of imagination, its action is somewhat analogous to that
of a garden rake, hence its name. Rake processing is effective because transmission of
09
12:00:51, subject to the Cambridge Core

7.4 Principles of Operation
217
∫dt
∫dt
∫dt
x
t)
n
     (t-T )
n 
0
x
     (t-
T )
n 
0
x
y(t)
x
x
x
g0
g1
g
+
xn
(
Fig. 7.12
Rake receiver for detection of the mth coefﬁcient of the signal.
each coefﬁcient xm occurs over a wide-band , allowing for multi-path time resolution
of the order of the chip time T0.
In the case of OFDM, transmission of each coefﬁcient occurs over a narrow frequency
band, resulting in a very coarse multi-path time resolution, and a rake receiver is not
effective. In this case, frequency diversity for reliability can be exploited by assigning
multiple frequencies to each user, which can then perform coding over frequency. Of
course, this comes at the expense of consuming degrees of freedom, as correlated coded
signals would be transmitted over the channel rather than independent signals from
different users.
A similar trade-off between frequency diversity and multiplexing also occurs for
CDMA. Achieving higher reliability in the rake receiver requires using a larger number
of taps, corresponding to different resolvable paths. This requires having a higher delay
spread in the channel, which, by virtue of (7.19), requires a larger number of degrees of
freedom.
7.4.4
Orthogonal Spatial Division
Orthogonal frequency division and orthogonal code division technologies share the
time–frequency degrees of freedom among the different users of the system. In contrast,
technologies using multiple antennas are designed to share the space–wavenumber
degrees of freedom. In the time–frequency case, provided that the number of users
is smaller than the available time–frequency resource T/π, we can allocate an
orthogonal channel to each user and transmit over small, non-overlapping time or
frequency intervals. In the space–wavenumber case, we can allocate an orthogonal
channel to each user, provided that the number of users is smaller than the number of
09
12:00:51, subject to the Cambridge Core

218
Communication Technologies
(b)
(a)
Fig. 7.13
(a) Distributed MIMO. (b) Point-to-point MIMO.
space–wavenumber degrees of freedom WS/π, where W is the wavenumber bandwidth
and S is the spatial extension of the domain where the signal is observed. This product
corresponds to the number of singular values that are signiﬁcantly greater than zero
in the Hilbert–Schmidt decomposition of the signal in space. In a discrete setting, it
corresponds to the rank of the channel’s matrix described in Section 5.3.2.
In a MIMO system, the optimal resource allocation is performed by transforming
the channel into orthogonal parallel spatial channels using the SVD decomposition.
The space–wavenumber degrees of freedom are then divided among the users, much
like in the time–frequency case. Establishing these communication channels requires
cooperation at the transmitting and receiving antennas. As shown in Figure 5.9, each
signal coefﬁcient that is sent through the diagonal channel is a linear combination of all
the inputs through the matrix V. Similarly, the received signal coefﬁcients are obtained
via a linear combination of all the outputs through the matrix U†. These operations
are somewhat easy to perform with little overhead if all the transmitting and receiving
antennas are co-located in a single physical device and they can share their input and
output signals, but they can be more difﬁcult in the case of spatially distributed systems,
where a single transmitter or receiver does not have access to all the signals – see
Figure 7.13. This makes the exploitation of the spatial resource more difﬁcult than in the
time–frequency case, where each user can transmit and receive signals independently
of the others.
A little reﬂection reveals the core of this problem. Recall that in a spatial setting the
coefﬁcients {xn} in Figure 7.7 are multiplied by spatially varying basis functions {ϕn}
and then sent through the channel via a convolution operation that occurs over space. It
follows that the effect of each coefﬁcient is present in the ﬁeld emitted by all the sources.
Similarly, the output coefﬁcients {yn} are obtained through a projection operation that
requires knowledge of the ﬁeld at all receiving points. In a distributed system, acquiring
this knowledge requires cooperation between the spatially distributed users and must
occur for every MIMO transmission. In contrast, each input coefﬁcient in Figure 7.6
only affects the transmitted signal over time, and each output coefﬁcient is obtained
through a projection that only requires knowledge of the signal over time. In this case,
each user located at a given point in space can independently construct and receive its
own signals.
09
12:00:51, subject to the Cambridge Core

7.5 Network Strategies
219
As in the time–frequency case, the number of space–wavenumber degrees of freedom
is limited by the physics of propagation. As we shall see in Chapter 8, the wavenumber
bandwidth of an arbitrary scattering system depends on the characteristic size of the
system, and on the frequency of transmission. This provides an upper bound on the
amount of spatial multiplexing that can be achieved using arbitrary technologies and
in arbitrary scattering environments. The frequency dependence also shows that the
number of space–wavenumber and time–frequency degrees of freedom are related to
each other, and the total number of degrees of freedom can be computed by taking into
account such dependence. Before examining these physical limitations in detail, we
give an overview of some of the proposed techniques to exploit the spatial resource in a
distributed network setting.
7.5
Network Strategies
While technologies that exploit the time–frequency resource are well established and
widely used in modern communication systems, determining the best way to exploit
the spatial resource in a distributed communication system with minimum cooperation
overhead is an open problem, and several proposals have been made in the literature.
These studies fall into the realm of network information theory, which is a research
topic that has gained notable momentum in recent years and inspired several innovative
engineering designs. Their theoretical performance cannot exceed MIMO systems with
co-located transmitters and receivers, which allows full cooperation to encode and
decode messages. In this case, for each time–frequency degree of freedom we can have a
number of parallel channels at most equal to the number of space–wavenumber degrees
of freedom. The proposed strategies attempt to come as close as possible to this limit,
to achieve the full multiplexing gain available in the given propagation environment.
Which strategy will have the most practical impact, however, has yet to be determined.
7.5.1
Multi-hop
In a spatially distributed system of N users sharing a ﬁxed time–frequency resource
without any form of spatial multiplexing, a fraction 1/N of the total resource is available
to each user. It follows that as N →∞, the time–frequency degrees of freedom are
saturated, and the performance degrades considerably as the number of users increases.
This is the case, for example, for the OFDM and CDMA technologies discussed above.
The objective of distributed network strategies is to improve this situation by exploiting
the additional spatial resource.
A ﬁrst strategy that exploits spatial reuse is multi-hop relay. In this case, routing paths
are established between sources and destinations, and point-to-point transmissions are
performed at each hop along the paths, treating interference from other nodes as noise.
This scheme can be analyzed under a variety of models for the propagation channel
and, in a geometric setting in which nodes are scattered in a region of the plane of
size proportional to N, sources and destinations are randomly selected, and signals are
09
12:00:51, subject to the Cambridge Core

220
Communication Technologies
Fig. 7.14
Illustration of the multi-hop strategy exploiting parallel communications across the cut.
strongly attenuated over distances so that the interference from distant users can be
neglected, it improves utilization of the resource by a factor
√
N. In other words, a
fraction 1/
√
N of the resource is now available to each user, and performance degrades
more gracefully compared to the case where the spatial resource is unused.
The result can be explained as follows. Due to the random assignment of sources
and destinations, roughly half of the nodes located on one side of the network wish to
communicate with the other half located on the other side. The transmission rate that
each source can sustain to its intended destination depends on the number of routing
paths that can be established simultaneously across the cut that separates these two
regions – see Figure 7.14. We can construct these paths by letting each node select, as
the next relay, a nearby neighbor located at most a constant distance from it. In this
way, each hop transmission generates a small interference footprint around it. If routing
paths crossing the cut are sufﬁciently spaced from each other, so that the corresponding
interference footprints do not overlap too much, then the total amount of interference
at each hop can be controlled, and the routes can be performed simultaneously, at
constant rate. Since the number of nodes accessing these paths is of the order of N,
and the number of paths crossing the cut and spaced by at least a constant distance
is proportional to the length of the cut, i.e., it is of the order of
√
N, the result
follows.
This strategy requires the number of space–wavenumber degrees of freedom to be at
least proportional to
√
N. If this is not the case, then the performance is limited by the
rank of the channel’s matrix between sources and destinations. For example, a keyhole
effect, such as the one described in Section 6.3.4, may allow only one interference-free
transmission per unit time across the cut.
09
12:00:51, subject to the Cambridge Core

7.5 Network Strategies
221
Fig. 7.15
Illustration of the hierarchical cooperation strategy exploiting distributed MIMO transmissions.
7.5.2
Hierarchical Cooperation
A second strategy that exploits the spatial resource is hierarchical cooperation using
distributed MIMO transmissions between clusters of nodes. This strategy further
improves the utilization of the spatial resource, and can make an almost constant
fraction of the resource available to each user, under some assumptions on the channel
model.
The network area is recursively divided into clusters, and within each cluster nodes
form a MIMO system performing joint encoding and communicating with the nodes
in another cluster that perform joint decoding. Within each cluster, information is then
redistributed recursively by iterating the above scheme. This situation is depicted in
Figure 7.15. Each node distributes a constant number of messages among the nodes
in its cluster, one for each node, and the nodes within each cluster form a distributed
MIMO transmission to send these messages in parallel to the cluster where the
destination node lies. Each node in the destination cluster sends its received message to
the intended destination within their cluster, which can ﬁnally decode all the messages.
In this way, the ﬁrst and last steps of the scheme, corresponding to message distribution
and message collection within each cluster, involve only local communication and
can be performed in parallel among different clusters, while MIMO transmissions
occur sequentially among different clusters. The problem of message distribution and
collection within each cluster is treated as a scaled version of the original problem, and
can be performed recursively using the same strategy as above, by dividing the cluster
into subclusters and letting nodes cooperate to perform MIMO transmissions between
subclusters.
A recursive computation shows that if the number of space–wavenumber degrees
of freedom is at least of the order of Nℓ/(ℓ+1), and signals are weakly attenuated over
09
12:00:51, subject to the Cambridge Core

222
Communication Technologies
distances, then this strategy allocates
Nℓ/(ℓ+1)
N
= N−1/(ℓ+1)
(7.29)
degrees of freedom to each node, where ℓis the number of recursive clustering layers.
For ℓ= 1, we have
N1/2
N
=
1
√
N
,
(7.30)
and the performance of the scheme is the same as in the multi-hop case. For large
values of ℓthe performance improves considerably, as each node is allocated an almost
constant number of degrees of freedom.
On the other hand, if the number of space–wavenumber degrees of freedom is
less than Nℓ/(ℓ+1), then we are in a limited spatial degree of freedom regime, where
performance is limited by the rank of the channel’s matrix between all sources and
destinations, which depends on environmental constraints.
7.5.3
Interference Alignment
A third strategy that exploits the spatial resource is interference alignment. Here, the
main idea is to design the transmitted signals so that at each receiver the interference
represented by unwanted signals occupies only a small portion of the signals’ space.
This is achieved by pre-coding over multiple time or frequency realizations of the
channel. Each receiver can then recover the desired signal by projecting the received
linear combination onto the subspace orthogonal to the interference subspace. The
concept is visually illustrated in Figure 7.16, where signals from three users are encoded
over three dimensions. Each receiver observes a linear combination of two interfering
signals that lie in a two-dimensional subspace, and a third signal that is not fully
contained in this subspace. The dimension orthogonal to the interfering subspace can
then be used to communicate over an interference-free channel.
Fig. 7.16
Illustration of interference alignment with three users signaling over three dimensions.
09
12:00:51, subject to the Cambridge Core

7.5 Network Strategies
223
x1(1)
x2(1)
xN(1)
x1(2)
x2(2)
xN(2)
x1(M) =
x2(M) =
xN(M) =
y1(M)
y2(M)
yN(M)
y1(2)
y2(2)
yN(2)
y1(1)
y2(1)
yN(1)
G(1)
G(2)
G(M)
x1(1)
x2(1)
xN(1)
Fig. 7.17
Interference alignment block representation.
The most important feature of an interference alignment scheme is its ability to
construct signals which, for a given Green’s function, successfully align at the receivers.
In a model where the Green’s function evolves randomly in time or frequency, and
can provide M ≫N independent realizations, it is possible to do so by constructing
signals that span multiple realizations and collectively achieve a multiplexing gain of
N/2, where N is the number of transmitter–receiver pairs. Namely, the N receivers
can successfully recover NM/2 distinct messages sent over M realizations, yielding
a number of messages per channel use of
NM
2M = N
2 .
(7.31)
As an example to illustrate a possible construction, consider the repetition scheme
depicted in Figure 7.17. In the ﬁrst realization of the channel, when the channel’s
matrix is G(1), users send signals {x1(1),x2(1),...,xN(1)}. Then, they keep sending
different signals in subsequent realizations until they repeat in the Mth realization the
same signals they sent in the ﬁrst realization.
Assuming the channel’s matrix in the Mth realization is the complement of the one
in the ﬁrst realization,
G(M) =
⎡
⎢⎢⎢⎣
G1,1(1)
−G1,2(1)
···
−G1,N(1)
−G2,1(1)
G2,2(1)
···
−G2,N(1)
...
...
...
...
−GN,1(1)
−GN,2(1)
···
GN,N(1)
⎤
⎥⎥⎥⎦,
(7.32)
then in the Mth realization each receiver has access to
yn(M) = Gn,n(1)xn(1) −

k̸=n
Gn,k(1)xk(1),
(7.33)
while in the ﬁrst realization each receiver had access to
yn(1) = Gn,n(1)xn(1) +

k̸=n
Gn,k(1)xk(1).
(7.34)
Adding the two signals gives
yn(1) + yn(M) = 2Gn,n(1)xn.
(7.35)
09
12:00:51, subject to the Cambridge Core

224
Communication Technologies
It follows that with the assumption that in the Mth realization we have the complemen-
tary matrix of the ﬁrst realization, with two channel uses each user can send one signal
over an interference-free channel. This channel has diagonal form
" =
⎡
⎢⎢⎢⎣
2G1,1(1)
0
···
0
0
2G2,2(1)
···
0
...
...
...
...
0
0
···
2GN,N(1)
⎤
⎥⎥⎥⎦,
(7.36)
and provides a multiplexing gain of N/2 at the expense of letting the channel evolve
over a block of M independent realizations.
The key for this scheme to be successful is the appearance of a complementary matrix
every M realizations. Clearly, for a real channel any given complementary matrix has
zero probability of occurrence, since a speciﬁc set of real numbers can never occur in a
random realization. However, in the presence of signal quantization it is enough for the
matrix to be only fairly close to its complementary value to obtain the desired result.
An ergodic argument then shows that almost all quantized channel states are matched
with complementary states when M is large enough, i.e., the fraction of unmatched
states approaches zero as M →∞. This means that at the expense of letting the channel
evolve a large number of times, we can in principle match every realization with its
complementary one and achieve the desired multiplexing gain.
To obtain the promised gain, interference alignment requires the number of
space–wavenumber degrees of freedom to be at least of the order of N; in other
words, the rank of the channel’s matrix should be large enough to provide concurrent
transmissions in each block of Figure 7.17, so that each user can be allocated 1/2
degrees of freedom at each usage of the channel. If this is not the case, we are in a regime
where the number of spatial degrees of freedom is limited by environmental constraints.
In practice, these constraints can severely limit the performance of interference
alignment as well as hierarchical cooperation schemes.
7.5.4
A Layered View
Interference alignment may be perceived as a scheme by which “everybody gets half of
the cake,” where the cake here represents the ﬁxed time–frequency resource available to
all users. This is only an apparent paradox: the cake has different “layers” corresponding
to the space–wavenumber degrees of freedom. This additional spatial resource must be
proportional to the number of users in the network and ensures that each user gets half of
each layer, rather than of the whole cake, while the other half is lost due to interference.
Another key assumption regards the construction of signals over a large number of
independent realizations of the channel. A detailed ergodic argument shows that for
alignment to occur with high probability, the logarithm of the number of independent
realizations must be of the order of N2. This means that the amount of stochastic
diversity must grow exponentially with the number of users. It follows that interference
alignment guarantees that everyone gets half the cake, provided that the cake is layered
and it can be made arbitrarily large.
09
12:00:51, subject to the Cambridge Core

7.5 Network Strategies
225
N
M
Fig. 7.18
Cake-cutting interpretation of interference alignment.
Table 7.3 Degrees of freedom of different interference management
schemes. MIMO: co-located multiple antennas; IA: interference alignment;
HC: hierarchical cooperation; MH: multi-hop relay.
MIMO
IA
HC
MH
Degrees of freedom
N
N/2
Nℓ/(ℓ+1)
N1/2
Degrees of freedom per user
1
1/2
N−1/(ℓ+1)
N−1/2
This interpretation is illustrated in Figure 7.18, where M corresponds to the amount
of stochastic diversity in the time–frequency domain, and N corresponds to the number
of space–wavenumber degrees of freedom, representative of the number of spatial
dimensions available in the channel.
7.5.5
Degrees of Freedom
The results for interference networks are summarized in Table 7.3. All the schemes are
limited by the performance of MIMO with co-located multiple antennas, which allows
full cooperation among the transmitters and among the receivers. When the rank of
the Green’s function is not a bottleneck for the number of degrees of freedom, MIMO
provides a multiplexing gain proportional to the number of antennas in the system.
Interference alignment comes close to this bound and is order optimal, sacriﬁcing half of
the degrees of freedom to align the interference onto low-dimensional subspaces. This
is the price to pay in this case for the distributed operation of the network. Hierarchical
cooperation is almost order optimal, sacriﬁcing an arbitrarily small power of the number
of degrees of freedom to perform cooperation. Multi-hop relay is perhaps the simplest
strategy, but needs to sacriﬁce the square root of the number of degrees of freedom.
All the schemes assume that the rank of the channel matrix is sufﬁciently large and
that it is not a bottleneck for their performance. In practice, several limitations may
arise due to speciﬁc propagation conditions. Due to propagation and absorption in
the environment, signals may be highly attenuated over some channels that become
useless for communication. In this case, the effective rank of the channel matrix
decreases and becomes the performance bottleneck imposed by physical constraints. In
speciﬁc propagation environments characterized by high signal attenuation, short-range
09
12:00:51, subject to the Cambridge Core

226
Communication Technologies
communication performed via multi-hop is the order-optimal operation strategy. An
analysis of the available spatial degrees of freedom in arbitrary environments is provided
in Chapter 8.
Additional practical limitations arise from the amount of diversity available,
knowledge of the Green’s function at all nodes, and the protocol overhead required to set
up the network cooperation schemes. The impact of such limitations on the development
of future communication technologies exploiting spatial resource in a distributed setting
is a current topic of research in wireless communications.
7.6
Summary and Further Reading
We have provided an overview of current communication technologies based on the
theory of orthogonal signal representations developed in the previous chapters. These
technologies divide the available time–frequency or space–wavenumber resource into
parallel channels. The number of degrees of freedom of the electromagnetic ﬁeld
poses a limit on the number of orthogonal channels that can be obtained using these
representations. The amount of stochastic diversity poses a limit on the level of
reliability that can be obtained with multiple transmissions over these channels.
While an optimal decomposition based on the Hilbert–Schmidt theory is difﬁcult
to obtain because it depends on the speciﬁc form of the Green’s function, practical
solutions use basis functions that work well in certain propagation conditions, and this
choice forms the basis of the different technologies employed. Technologies exploiting
the time–frequency resource, as well as point-to-point MIMO systems, are discussed
extensively in wireless communication textbooks, such as Biglieri (2005), Tse and
Visvanath (2005), and Goldsmith (2005). Viterbi (1995) is a standard reference for
CDMA technology. There are many texts that focus on digital communications, not
limited to wireless, each with its own style. Proakis and Salehi (2007), Gallager (2008),
and Lapidoth (2009) are some popular examples. The text by El Gamal and Kim (2011)
focuses on the information-theoretic aspects of communication in a networked setting.
The optimal operation of interference networks is a topic of research. The
information-theoretic performance of multi-hop networks was originally studied by
Gupta and Kumar (2000). Subsequent works include Xie and Kumar (2004), Lévêque
and Telatar (2005), Franceschetti (2007a), and Franceschetti et al. (2007). The general
conclusion that can be drawn from these works is that this strategy is order optimal for
networks where signals are strongly attenuated over distances.
Hierarchical cooperation was introduced by Özgür, Lévêque, and Tse (2007),
and provides an improvement upon multi-hop operation for low signal attenuation
channels supporting a large number of space–wavenumber degrees of freedom.
Physical limitations to the number of degrees of freedom have been identiﬁed by
Franceschetti, Migliore, and Minero (2009), and further discussed by Franceschetti
et al. (2011). The identiﬁcation of different operating regimes where the network is
either degree-of-freedom limited, or amenable to the spatial multiplexing gain offered
by hierarchical cooperation, is discussed by Özgür, Lévêque, and Tse (2013) and Lee
09
12:00:51, subject to the Cambridge Core

7.7 Test Your Understanding
227
and Chung (2012). Ghaderi, Xie, and Shen (2009) and Hong and Caire (2015) provide
reﬁned analyses of the performance of hierarchical cooperation in the light of these
operating regimes.
The idea of interference alignment ﬁrst appeared in works on coding theory, but the
main references in the context of wireless networks are Cadambe and Jafar (2008) and
the monograph by Jafar (2011). Practical engineering challenges for the realization
of interference alignment are discussed by El Ayach, Peters, and Heath (2013). The
interference alignment scheme based on complementary channels is due to Nazer et al.
(2012). The impact of limited diversity on the performance of interference alignment
was investigated by Li and Özgür (2016).
Many works on interference management techniques for next-generation wireless
networks are discussed in communication and information theory journals and
conferences worldwide.
7.7
Test Your Understanding
Problems
7.1
Digital communication technologies rely on orthogonal signal representations
using appropriate basis functions. Under what conditions on the propagation channel
do the prolate spheroidal functions provide the optimal representation?
7.2
Verify that the functions {ϕm(t)} in (7.2) are orthonormal in [Tp,T].
7.3
Explain how the environmental effects described in Table 7.1 are related to the
coherence bandwidth, coherence time, and coherence distance described in Chapter 5.
7.4
Consider a propagation model where there are two signiﬁcant multi-path
components separated by t seconds. For an orthogonal code division communication
system, what is the minimum bandwidth required to distinguish them?
7.5
Consider an orthogonal spectrum division communication system in which the
received signal is the superposition of three frequency components separated by an
interval of frequency ω. Assuming the cyclic preﬁx occupies negligible time, what is
the minimum transmission time required to resolve them? If we account for the length
of the cyclic preﬁx Tp, what is the degree of freedom loss compared to the previous
case?
7.6
Explain why the processing of the rake receiver depicted in Figure 7.12 improves
the detection capability of the symbol xn from the nth user. Discuss the quality of the
solution.
Solution
From (7.26) and (7.14), we have
y(t) =

m
gmx(t −mπ/)
09
12:00:51, subject to the Cambridge Core

228
Communication Technologies
=

m
gm
N−1

i=1
xiϕi(t −mπ/).
(7.37)
Assuming low autocorrelation of the signal ϕn(t) and low cross-correlations between
ϕn(t) and all the signals {ϕm(t)}, the operation on the mth branch of the rake receiver
corresponds to despreading the mth term of the received signal, while the remaining
terms contribute to a small error ϵm, so that the output to the mth branch is
y(m) = gmxn + ϵm.
(7.38)
By repeating over all branches, weighting the result on each branch by a coefﬁcient wm,
and then summing, we get
yn =

m
wmy(m) = xn

m
wmgm + wmϵm.
(7.39)
The additive noise terms can be modeled as independent, zero-mean, random variables
with variance
E(ϵ2
m) = σ 2.
(7.40)
This corresponds to assuming that the noise variance of the despreading error is constant
across different branches. It follows that the total noise variance is
E
⎡
⎣

m
wmϵm
2⎤
⎦= σ 2 
m
w2
m.
(7.41)
On the other hand, by the Cauchy–Schwarz inequality, the square of the signal
component in (7.39) is
x2
n

m
wmgm
2
≤x2
n

m
w2
m

m
g2
m.
(7.42)
The ratio of the signal to the average noise power is therefore
SNR ≤x2
n

m g2
m
σ 2
,
(7.43)
where the equality is achieved by choosing wm = gm in the Cauchy–Schwarz inequality.
The SNR on any single branch is
SNRm = x2
ng2
m
σ 2 ;
(7.44)
it follows that choosing the weights corresponding to the channel’s coefﬁcients, the
SNR in the rake receiver is improved compared to that of a single branch since

m
g2
m ≥g2
m.
(7.45)
This SNR maximization improves the estimation capability of the system. Dividing
both sides of (7.39) by 
m wmgm, we construct the estimate
˜xn =
yn

m wmgm
= xn +

m wmϵm

m wmgm
,
(7.46)
09
12:00:51, subject to the Cambridge Core

7.7 Test Your Understanding
229
whose mean squared error is
E[(˜xn −xn)2] = E[(
m wmϵm)2]
(
m wmgm)2
.
(7.47)
By (7.41) and the Cauchy–Schwarz inequality, we get
E[(˜xn −xn)2] ≥
σ 2

m g2m
,
(7.48)
where the equality is achieved by choosing wm = gm. We conclude that this choice leads
to the best linear estimate in terms of mean squared error.
09
12:00:51, subject to the Cambridge Core

8
The Space–Wavenumber Domain
Space: the ﬁnal frontier.1
8.1
Spatial Configurations
In this chapter we consider the information associated with the different spatial
conﬁgurations of a waveform. We consider propagation of sinusoidal waves in arbitrary
multiple scattering environments, and we compute the spatial bandwidth of the ﬁeld
measured in the space. By “spatial bandwidth” we mean the measure of the support
set of the signal in the wavenumber domain – a more appropriate term would be
“wavenumber bandwidth.” A minor disquieting fact of life is that the name spatial
bandwidth is standard, and we shall use the two interchangeably. We obtain a simple
formula for the number of spatial degrees of freedom of the received signal, showing
that this number is limited by the wavelength-normalized size of the cut through which
the information must ﬂow.
A heuristic principle of communication is that in the presence of multiple scattering
the number of possible spatial conﬁgurations of the ﬁeld, and thus the amount of
information it can carry over space, is increased. Loosely speaking, the superposition
in space of many waveforms from many different multiple scattered paths “creates”
bandwidth and provides independent parallel channels in the spatial–wavenumber
domain. In the jargon of communication theory, the term “rich scattering” is used
to denote an environment capable of providing an unlimited number of parallel
spatial channels between transmitters and receivers. The intuition used to explain this
phenomenon is that if the received ﬁeld is the superposition of many waveforms
coming from many different multiple scattered paths to the receivers, and each path
can carry an independent stream of information, then many communications can
occur in parallel by using a pair of antennas for communication along each scattered
path. If the environment provides a sufﬁciently large number of independent paths,
then this spatial multiplexing capability can grow proportionally to the number of
antennas.
For any arbitrary scattering environment, however, the wavenumber bandwidth is
not an unlimited resource. A physical limitation is given by the size of the minimum
1 Opening line of Captain James T. Kirk’s monologue in Star Trek (1966).
10
12:00:51, subject to the Cambridge Core

8.2 Radiation Model
231
r´
R
r
O
rm
s
S
V
a
Fig. 8.1
Geometry of the radiation model.
cut-set boundary through which the information must ﬂow as the ﬁeld propagates from
transmitters to receivers. We can view each parallel channel as occupying a certain
amount of spatial resource on the cut, proportional to the wavelength of transmission,
and if these occupation regions overlap, then the corresponding information streams
cannot be resolved at the receivers.
The results described in this chapter can be applied in the context of communication
systems, using spatially distributed transmitting antennas, and in the context of remote
sensing, providing a limit on the spatial resolution of the obtained image. In the
ﬁrst case, they limit the spatial dimensionality of the signals that can be used for
communication across the cut. In the second case, they limit the dimensionality of the
image constructed by illuminating an arbitrary scattering system using electromagnetic
waveforms and measuring the scattered response at the receivers.
8.2
Radiation Model
We consider a spherical coordinate system r = (r,θ,φ) and a spherical volume V of
radius a with center at the origin of the space. We let rm be the minimum distance
between the center of V and an indeﬁnite analytic curve S, all external to V. We deﬁne
a curvilinear coordinate s = s(r) along S, normalized to rm. This geometry is depicted
in Figure 8.1.
A density I(r′) [A m−2], r′ ∈V, of sinusoidal currents of angular frequency ω is
distributed over the radiating elements contained inside V. This may include currents
impressed by the sources, or induced due to multiple scattering.
10
12:00:51, subject to the Cambridge Core

232
The Space–Wavenumber Domain
We refer to the ﬁeld measured over the curve S as E(s). By (5.16), we have
E(s) =

V
G(|r −r′|) · I(r′)dr′,
(8.1)
and we consider the L2(−∞,∞) norm
∥E(s)∥2 =
 ∞
−∞


V
G(|r −r′|) · I(r′)dr′

2
ds
≤sup
r′∈V
 ∞
−∞
|G(s,r′)|2ds

·

V
|I(r′)|dr′
2
,
(8.2)
where the last step follows from Young’s inequality – see Appendix A.4.3.
8.3
The Field’s Functional Space
We wish to work with the functional space of square-integrable functions so that we
can use the analysis tools developed in the previous chapters to determine the spatial
degrees of freedom of the scattered ﬁeld. This requires (8.2) to be ﬁnite. We assume that
currents inside V are uniformly bounded, so that the second integral in (8.2) is clearly
bounded. This assumption also implies that currents are square-integrable functions. On
the other hand, the ﬁrst integral in (8.2) is bounded provided that |r −r′| > 0, namely
the observation curve is external to V. This is immediately evident by examining the
form of the Green’s function. Letting R = |r −r′| = R(s,r′), from (5.23) we have
G(s,r′) = −jωμ
4π
&
I + ∇∇
β2
'
· exp(−jβR)
R
= −jωμ
4π
&exp(−jβR)
R
I + ∇∇
β2
exp(−jβR)
R
'
= −exp(−jβR) jωμ
4πR
&
I + R
β2 exp(jβR)∇∇
exp(−jβR)
R
'
= −exp(−jβR) jωμ
4πR N,
(8.3)
where the dyad I is the identity and the dyad ∇∇is represented by the matrix
⎛
⎜⎜⎜⎜⎜⎜⎝
∂2
∂x2
∂2
∂x∂y
∂2
∂x∂z
∂2
∂x∂y
∂2
∂y2
∂2
∂y∂z
∂2
∂x∂z
∂2
∂y∂z
∂2
∂z2
⎞
⎟⎟⎟⎟⎟⎟⎠
.
If R > 0, then (8.3) is square-integrable. The expression (8.3) also shows that the
ﬁeld generated by a current ﬂowing through an elementary volume at r′ undergoes a
phase shift exp(−jβR), a 1/R geometric attenuation, and a dyadic transformation N
that affects the radiated ﬁeld’s amplitude and orientation. When R →∞, (8.3) attains
10
12:00:51, subject to the Cambridge Core

8.4 Spatial Bandwidth
233
a very simple form, representing the far ﬁeld radiated from an elementary volume, and
given by the Fourier transform of (4.92a) – see Problem 8.1. The total ﬁeld is given by
the superposition of such elementary contributions.
For convenience, we assume that the ﬁeld is normalized to its maximum norm (8.2),
so that the set of scattered ﬁelds is contained in the unit sphere of the corresponding
functional space. We also extract from the ﬁeld the propagation factor exp(−jβr), where
β = ω√ϵμ = ω/c is the propagation constant, and refer to the reduced ﬁeld
E(s) →exp(jβr)E(s).
(8.4)
Letting the reduced Green’s function
G(s,r′) →exp(jβr)G(s,r′),
(8.5)
the reduced ﬁeld is then given by
E(s) = −exp[jβ(r −R)] jωμ
4πR

V
N · I(r′)dr′
=

V
G(s,r′) · I(r′)dr′,
(8.6)
where the Green’s operator acts on the appropriate functional space,
G : L2(V) →L2(S).
(8.7)
8.4
Spatial Bandwidth
To determine the spatial bandwidth of the radiated ﬁeld, we apply a rectangular ﬁlter
of bandwidth w > 0 in the wavenumber domain, and evaluate the error associated with
the ﬁltered ﬁeld representation. The study of this error as a function of w reveals the
existence of a critical value of the spatial bandwidth beyond which the error quickly
drops to zero. The critical bandwidth can then be related to the geometry of the
scattering system and used for the computation of the number of spatial degrees of
freedom.
The ﬁltered ﬁeld is given by the convolution in the spatial domain
Ew(s) = 1
π
sin(ws)
s
∗E(s)
= 1
π
sin(ws)
s
∗

V
G(s,r′) · I(r′)dr′
= 1
π
 ∞
−∞
sin[w(s −s′)]
s −s′

V
G(s′,r′) · I(r′)dr′ds′
=

V
Gw(s,r′) · I(r′)dr′,
(8.8)
where
Gw(s,r′) = 1
π
 ∞
−∞
sin[w(s −s′)]
s −s′
G(s′,r′)ds′
10
12:00:51, subject to the Cambridge Core

234
The Space–Wavenumber Domain
=
1
2πj
 ∞
−∞
exp[jw(s′ −s)]
s′ −s
G(s′,r′)
−exp[−jw(s′ −s)]
s′ −s
G(s′,r′)
#
ds′.
(8.9)
We now deﬁne
G(s,r′) = Gw(s,r′) −G,
(8.10)
and
E(s) = Ew(s) −E(s) =

V
G(s,r′)I(r′)dr′.
(8.11)
The error obtained using bandlimited ﬁelds in the set Bw to represent the radiated ﬁeld
is then given in terms of the error obtained when the Green’s function is substituted by
its ﬁltered version,
∥E(s)∥=
 ∞
−∞
|E|2ds
1/2
=
 ∞
−∞


V
G(s,r′)I(r′)dr′

2
ds
1/2
≤sup
r′∈V
 ∞
−∞
|G(s,r′)|2ds
1/2
·

V
|I(r′)|dr′,
(8.12)
where, as before, the last step follows from Young’s inequality. This bandlimitation
error corresponds to the deviation DBw(E ) of the space of bandlimited ﬁelds Bw from
the whole space of radiated ﬁelds E . Since the currents are uniformly bounded, the
second integral in (8.12) contributes only a constant factor to the error, and the behavior
of DBw(E ) is essentially dictated by the ﬁrst term. Letting β = 2π/λ, where λ is the
wavelength of radiation, asymptotic evaluation as βa →∞reveals that the error has
a phase transition, tending to zero for values of w slightly larger than βa, and to its
maximum value for values of w slightly smaller than βa. The transition is illustrated in
Figure 8.2, and leads to the notion of spatial bandwidth of an arbitrary radiating system
of radius a given by βa+o(βa). In the following, we provide the derivation of this basic
result and then use it to compute the number of degrees of freedom of the radiated ﬁeld.
8.4.1
Bandlimitation Error
In order to bound the error in (8.12), we need to evaluate G(s,r′). According to (8.10),
this amounts to performing the integration in (8.9). The integral is over a real line and
has one singularity at s′ = s. It is convenient to extend it to the complex plane and
perform contour integration of the two factors inside the integral separately. Integrating
10
12:00:51, subject to the Cambridge Core

8.4 Spatial Bandwidth
235
o(1)
w
1
a
0
~~
Fig. 8.2
Phase transition of the deviation from the space of bandlimited functions.
C +
C –
s
s
Fig. 8.3
Contour integration in the complex plane.
the ﬁrst factor along the upper closed contour U that does not contain the singularity, as
depicted in Figure 8.3, we have, by Cauchy’s theorem,
1
2πj
U
exp[jw(s′ −s)]
s′ −s
G(s′,r′)ds′ = 0.
(8.13)
Since the integrand in (8.13) tends to zero as ℜ(s′) →∞, the two vertical paths of the
contour do not provide any contribution to the integral, and we let
G+
w(s,r′) =
1
2πj

ℜ
exp[jw(s′ −s)]
s′ −s
G(s′,r′)ds′
=
1
2πj

C+
exp[jw(s′ −s)]
s′ −s
G(s′,r′)ds′
= −ωμ
2π

C+
exp[jw(s′ −s) + jψ(s′,r′)]
4π(s′ −s)R(s′,r′)
N(s′,r′)ds′,
(8.14)
10
12:00:51, subject to the Cambridge Core

236
The Space–Wavenumber Domain
where ψ(s,r′) = β[r(s) −R(s,r′)], and the last equality follows from (8.3). Similarly,
integration of the second factor along the lower closed contour L leads to
−1
2πj
L
exp[−jw(s′ −s)]
s′ −s
G(s′,r′)ds′ = G(s,r′),
(8.15)
where the residue G(s,r′) is due to the presence of the singularity inside L. Since the
two vertical paths of the contour again do not provide any contribution to the integral,
we let
G−
w(s,r′) = −1
2πj

ℜ
exp[−jw(s′ −s)]
s′ −s
G(s′,r′)ds′
= −1
2πj

C−
exp[−jw(s′ −s)]
s′ −s
G(s′,r′)ds′ + G(s,r′)
= ωμ
2π

C−
exp[−jw(s′ −s) + jψ(s′,r′)]
4π(s′ −s)R(s′,r′)
N(s′,r′)ds′ + G(s,r′).
(8.16)
We now have
G(s,r′) = Gw −G = G+
w + G−
w −G,
(8.17)
and the problem has been reduced to the computation of the two complex integrals in
(8.14) and (8.16). Their evaluation reveals a sharp transition of the bandlimitation error
around a critical value of w. The error sharply drops to zero for values of w slightly larger
than the critical value, and it quickly rises to its maximum for values slightly smaller
than the critical value. The width of the transition can be characterized precisely, and
leads to the asymptotic notion of the spatial bandwidth of the radiated ﬁeld.
8.4.2
Phase Transition of the Bandlimitation Error
The Green’s function bandlimitation error (8.17) is evaluated using a limiting argument.
This is based on the idea that when computing (8.14) and (8.16) we have a vanishing
contribution to the integrals from intervals of s′ where the integrands are rapidly
oscillating, while most of the contribution comes from the vicinity of points where
the phase of the integrands does not change. The evaluation can then be restricted to
the stationary points by letting the parameter regulating the velocity of the oscillations
away from the stationary points tend to inﬁnity. See also Appendix C for an illustration
of this method.
To identify the parameter regulating the velocity of the oscillations, we write
˙ψ(s′,r′) = ∂ψ(s′,r′)
∂s′
= β ∂[r(s′) −R(s′,r′)]
∂s′
,
(8.18)
and notice that provided that (r −R) tends to a well-deﬁned limit as s′ →±∞, we have
lim
s′→±∞
˙ψ(s′,r′) = 0.
(8.19)
10
12:00:51, subject to the Cambridge Core

8.4 Spatial Bandwidth
237
w0
s0
O
Fig. 8.4
The function ˙ψ has a maximum at s0.
The proof of this last statement is a cute exercise in analysis – see Problem 8.5. It follows
that ˙ψ has at least one extremal point – see Figure 8.4. We then let
w0(r′) = max
s′ | ˙ψ(s′,r′)| = | ˙ψ(s0,r′)|,
(8.20)
and
χ = w
w0
.
(8.21)
The integrals in (8.14) and (8.16) can now be written in the standard form suitable for
asymptotic evaluation:

ρ
f(s′)exp[w0(s′)]ds′,
(8.22)
where
(s′) = ±jχ(s′ −s) + jψ(s′)
w0
,
(8.23)
the path ρ is either C+ or C−, and f(s′) is a slowly varying function of s′, except in the
neighborhood of the singular point s′ = s. The saddle point method for the asymptotic
evaluation of integrals of this type is described in Appendix C.3, and is applied in the
next section by letting the parameter regulating the velocity of the oscillations w0 →∞.
The result is that the integral (8.22) exhibits a phase transition around the critical
point χ = 1, tending to zero as w0 →∞for χ > 1, and to its maximum value for
χ < 1. It follows that w = w0 represents the critical value of the bandwidth around
which the error’s behavior drastically changes. To better visualize the transition, notice
that exp[w0(s′)] rapidly oscillates as w0 →∞, except at the stationary points where
∂(s′)
∂s′
= 0,
(8.24)
10
12:00:51, subject to the Cambridge Core

238
The Space–Wavenumber Domain
corresponding to
"
w = ˙ψ(s′,r′),
−w = ˙ψ(s′,r′).
(8.25a)
(8.25b)
If χ > 1 as w0 →∞, then w > | ˙ψ(s′)| for all s′ and there are no stationary points. It
follows that G rapidly decays to zero due to the rapid oscillations of the integrands.
On the other hand, if χ < 1 then the error G increases due to the presence of stationary
points. The error reaches its maximum value as soon as the contribution of the highest
stationary point is taken into account.
8.4.3
Asymptotic Evaluation
Assuming that w0 corresponds to a maximum of
˙ψ, we can focus on G−and
asymptotically evaluate the integral in (8.16) using the saddle point method described
in Appendix C.3. Expanding (s′) in the neighborhood of s0, we have
(s′) ≃(s0) + ˙(s0)(s′ −s0) + 1
2
¨(s0)(s′ −s0)2 + 1
3!
...
(s0)(s′ −s0)3
= (s0) + ˙(s0)(s′ −s0) + 1
3!
...
(s0)(s′ −s0)3
= −jχ(s0 −s) + jψ(s0)
w0
−j(χ −1)(s′ −s0) −j
6
|
...
ψ(s0)|
w0
(s′ −s0)3. (8.26)
This third-order expansion is only valid in the neighborhood of s0; however, as w0 →
∞we can substitute it inside (8.16) and integrate along the entire path rather than just
around s0, since away from the stationary point the rapid oscillations let the integral
tend to zero. It follows that
G(s,r′) ∼ωμ
2π exp[jψ(s0) + jw(s −s0)] N(s0,r′)
4πR(s0,r′)

C−
1
(s′ −s) exp
"
w0
(
−j(χ −1)(s′ −s0)
−j
6
|
...
ψ(s0)|
w0
(s′ −s0)3
4%
ds′.
(8.27)
We now deform the integration path, as we may by Cauchy’s theorem, so that it passes
through the stationary point s0 in such a way that the angle approaching the stationary
point makes the real part of (s′) exhibit the steepest maximum while keeping the
imaginary part stationary. This allows us to capture the largest possible contribution to
the integral at the stationary point, and is obtained by letting
τ = −j(s′ −s0)

|
...
ψ(s0)|
2
1/3
,
(8.28)
10
12:00:51, subject to the Cambridge Core

8.4 Spatial Bandwidth
239
C –
s´
C*
τ
Δ
Δ
Fig. 8.5
Changing the integration path from C−to C∗.
so that the integral (8.27) becomes
G(s,r′) ∼G(s0,r′)exp[jw(s −s0)]
1
2πj

C⋆
exp(στ −τ 3/3)
τ + j

...
ψ(s0)
2

1/3
(s −s0)
dτ,
(8.29)
where C⋆is the vertical integration path in the left complex half-plane shown in
Figure 8.5, and
σ(r′) =

...
ψ(s0)
2

−1/3
w0(χ −1).
(8.30)
With a further manipulation, (8.29) can be expressed as a tail integral of the Airy
function times a phase term (see Problem 8.2), and we have
G(s,r′) ∼−G(s0,r′)exp[jw0(s −s0)]
 ∞
σ(r′)
Ai(x)exp
⎡
⎣j

...
ψ(s0)
2

1/3
(s −s0)x
⎤
⎦dx.
(8.31)
Finally, an application of Parseval’s theorem (see Problem 8.3) leads to
 ∞
−∞
|G(s,r′)|2ds ∼|G(s0,r′)|22π

...
ψ(s0)
2

−1/3  ∞
σ(r′)
Ai 2(x)dx.
(8.32)
The tail integral on the right-hand side of (8.32) is depicted in Figure 8.6. It is
controlled by the value of σ(r′), sharply decaying to zero as σ →∞, while growing
to its maximum value for σ →−∞.
The behavior of this integral is responsible for the phase transition of the
bandlimitation error. Letting χ > 1 in (8.30), we show that for all r′ ∈V,
lim
w0→∞σ(r′) = ∞.
(8.33)
10
12:00:51, subject to the Cambridge Core

240
The Space–Wavenumber Domain
–4
4
1
–8
0
σ
Fig. 8.6
Tail integral behavior governing the Green’s function bandlimitation error.
In this regime, the rate of decay of the tail integral on the right-hand side of (8.32) can
be computed, and when substituted into (8.32) it shows that the bandlimitation error
drops to zero. On the other hand, for χ < 1 the minimum value of σ(r′) tends to −∞
and the bandlimitation error sharply increases. By (8.21), the regime χ > 1 corresponds
to having values of the bandwidth w slightly larger than the critical value w0, and in this
case the error is negligible for all r′ ∈V. The regime χ < 1 corresponds to having values
of the bandwidth slightly smaller than w0 and in this case the error may be substantial
for some r′ ∈V.
8.4.4
Critical Bandwidth
To determine the asymptotic behavior of the tail integral on the right-hand side of (8.32),
we consider the smallest value,
σm = inf
r′∈V σ(r′),
(8.34)
and the largest value of the critical bandwidth,
W = sup
r′∈V
w0(r′),
(8.35)
and let
χm = w
W ,
(8.36)
|
...
ψ|M = sup
r′∈V
|
...
ψ(s0,r′)|.
(8.37)
With these deﬁnitions, we have from, (8.30),
σm = inf
r′∈V

2
|
...
ψ(r′)|
1/3
(w −w0)
10
12:00:51, subject to the Cambridge Core

8.4 Spatial Bandwidth
241
=

2
|
...
ψ|M
1/3
(w −W)
=

2
|
...
ψ|M
1/3
(χm −1)W.
(8.38)
Some geometric considerations yield bounds on |
...
ψ|M and W. For s′ ∈S and having
normalized the arc length s′ to rm, we have
˙ψ(s′) = β ∂(r −R)
∂s′
= βrm(¯r −¯R) · ¯t,
(8.39)
where ¯t is the unit vector tangent to the observation curve – see Figure 8.7. Considering
the extremal conﬁguration depicted in Figure 8.8 and corresponding to
r′ = a, r = rm,
(8.40)
we have, from (8.39), that this achieves
ds
dr
r
r
t
O
Fig. 8.7
Derivative of a position vector along a curve. dr = dscosθ = ds ¯r · ¯t.
O
s
M
M
2
t
R
r-
    rm
a
R
S
V
Fig. 8.8
Extremal geometric conﬁguration.
10
12:00:51, subject to the Cambridge Core

242
The Space–Wavenumber Domain
a
r´
R
r
O
rm
s
M
S
V
Fig. 8.9
Geometric interpretation.
˙ψ(s′) = βrm|¯r −¯R|cos θM
2
= βrm2sin θM
2 cos θM
2
= βa.
(8.41)
It then follows that
W =
sup
s′∈S,r′∈V
| ˙ψ(s′)| ≥βa.
(8.42)
With similar geometric considerations, one can also show that
|
...
ψ|M ≤κβa,
(8.43)
where the positive constant κ depends only on the shape of the observation curve.
Substituting (8.42) and (8.43) into (8.38), we obtain
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
σm ≥
 2
κ
1/3
(βa)2/3(χm −1)
for χm > 1,
σm ≤
 2
κ
1/3
(βa)2/3(χm −1)
for χm < 1.
(8.44a)
(8.44b)
It follows that as βa →∞we have that σm →±∞depending on the value of χm. For
χm > 1, we have w > W and σm →∞; for χm < 1, we have w < W and σm →−∞.
From (8.39), we also have
| ˙ψ(s′)| ≤βrm|¯r −¯R| = βrm2sin θ
2
= βrm
sinθ
cos θ
2
,
(8.45)
10
12:00:51, subject to the Cambridge Core

8.4 Spatial Bandwidth
243
where θ is the angle between ¯r and ¯R – see Figure 8.9. The right-hand side is maximized
when θ = θM, so that by taking the supremum on both sides we get
W =
sup
s′∈S,r′∈V
| ˙ψ|
≤βrm
sinθM
cos θM
2
≤
√
2βa,
(8.46)
where the last inequality follows from sinθM = a/rm and θM ≤π/2. Putting together
(8.42) and (8.46), we have that the critical bandwidth W that determines the diverging
behavior of σm is geometrically related to the size of the scattering system a. Since the
propagation constant β = ω/c = 2π/λ, the critical transition point occurs when w is of
the order of the circle 2πa, normalized to the transmission wavelength λ.
The critical bandwidth is an intrinsic property of the scattering system, that depends
only on its overall dimension, normalized to the transmission wavelength.
In the far ﬁeld, namely when rm ≫a, the factor
√
2 in (8.46) can be replaced by
one, and the critical bandwidth that marks the transition from large to small values of
the error becomes essentially βa. In practice, for values of rm slightly larger than a the
upper and lower bounds on W are already very close. For example, for rm = 1.5a, the
upper bound becomes W ≤1.07βa.
8.4.5
Size of the Transition Window
To give a precise characterization of the excess bandwidth required to achieve a small
error, we let w > W, and evaluate the rate of decay of the bandlimitation error as βa →
∞. In this case, by (8.44a) we have σ(r′) →∞for all r′ ∈V, and using standard
asymptotic expansions of the Airy function, we obtain
 ∞
σ(r′)
Ai2(x)dx = O
(
exp

−2
3σ 3/2(r′)
2
σ(r′)
4
.
(8.47)
By (8.12) and using (8.32), (8.47), and (8.30), we obtain the bound on the bandlimitation
error:
DBw(E ) = sup
r′∈V
 ∞
−∞
|G(s,r′)|2ds
1/2
·

V
|I(r′)|dr′
= O
⎡
⎣
exp

−2
3σ 3/2
m

√w −W
⎤
⎦
= O
⎡
⎣
exp

−2
3σ 3/2
m

√(χm −1)W
⎤
⎦.
(8.48)
10
12:00:51, subject to the Cambridge Core

244
The Space–Wavenumber Domain
By (8.44a) and (8.42) it follows that, as βa →∞,
DBw(E ) = O
⎧
⎨
⎩
exp
,
−2
3
 2
κ
1/2 (χm −1)3/2βa
-
√(χm −1)βa
⎫
⎬
⎭,
(8.49)
and the deviation can be made arbitrarily small by choosing
χm = 1 +
1
o(βa),
(8.50)
so that (χm −1)βa →∞. By (8.36), the value of the spatial bandwidth that leads to
arbitrarily small deviation is now precisely determined as
w =

1 +
1
o(βa)

W.
(8.51)
Using the bounds (8.42) and (8.46), this leads to the following asymptotic notion of
spatial bandwidth:
The spatial bandwidth of the ﬁeld radiated by an arbitrary scattering system of
radius a is
βa + o(βa) ≤W0 ≤
√
2βa + o(βa), as βa →∞,
(8.52)
and in the far ﬁeld the spatial bandwidth is
W0 = βa + o(βa), as βa →∞.
(8.53)
Figure 8.2 depicts the transition of the deviation of the space of ﬁelds from the space
of bandlimited functions. The deviation tends to zero for values of the bandwidth only
slightly larger than βa. The transition window viewed at the scale of βa vanishes, and
the deviation tends to become a step function.
This result shows that as βa →∞, the space of scattered ﬁelds can be approximated
with vanishing error by the space of functions essentially bandlimited to βa. In order to
bound the degree to which the space of ﬁelds can be approximated by ﬁnite-dimensional
subspaces, we can then apply the results for bandlimited functions developed in
Chapters 2 and 3. By projecting E onto the space of bandlimited ﬁelds Bw and adding
this approximation error to the one obtained by a ﬁnite interpolation of a strictly
bandlimited signal, we obtain a bound on the overall error of the approximation. This
technique was illustrated in Section 3.5.1, and we apply it next in the context of
electromagnetic signals.
8.5
Degrees of Freedom
In order to bound the number of degrees of freedom of scattered ﬁelds, we consider an
interval (−S/2,S/2) of interest on the observation curve S. We want to determine the
10
12:00:51, subject to the Cambridge Core

8.5 Degrees of Freedom
245
number of degrees of freedom of ﬁelds approximately bandlimited to βa, in the sense of
(8.53), and observed over this ﬁnite interval, as βa →∞. This amounts to determining
the dimension of the minimal subspace representing the elements of E within ϵ accuracy
in the interval (−S/2,S/2), namely the minimum number of approximating functions
that makes the Kolmogorov N-width d2
N(E ) < ϵ.
We proceed with the technique developed in Section 3.5.1. By (3.92) we have that
the Kolmogorov N-width of the scattered ﬁeld is bounded by
dN(E ) ≤DBw(E ) + dN(Bw).
(8.54)
We can make d2
N(E ) < ϵ, by ensuring that both the deviation DBW(E ) and the N-width
dN(Bw) are arbitrarily small. This can be done by ﬁrst choosing w = W0, so that
DBw(E ) tends to zero as βa →∞in the L2(−∞,∞) norm, and hence also in the
L2(−S/2,S/2) norm, and then choosing
N = W0S
π
+ 1
π log
1 −ϵ
ϵ

log
W0S
2

+ o(logW0S/π),
(8.55)
so that by (2.132) and (3.13), d2
N(Bw) < ϵ as βa →∞.
We now let
N0 = βaS
π ,
(8.56)
which corresponds to the space-bandwidth product corresponding to the Nyquist
number in our spatial setting. By substituting the upper bound in (8.52) into (8.55)
and using (8.56), it follows that we can choose
N =
√
2N0 + O(logN0)
(8.57)
and make d2
N(E ) < ϵ. We can now state the ﬁnal result:
The number of spatial degrees of freedom at level ϵ of the ﬁeld radiated by an
arbitrary scattering system of radius a is
Nϵ(E ) ≤
√
2N0 + O(logN0), as N0 →∞,
(8.58)
where the dependence on ϵ appears hidden as a pre-constant of the second-order
term O(logN0) in the phase transition of the number of degrees of freedom. In far
ﬁeld conditions, we also have the tighter bound
Nϵ(E ) ≤N0 + O(logN0), as N0 →∞.
(8.59)
This shows that as βa →∞, the number of eigenfunctions that are needed to
interpolate the ﬁeld within ϵ accuracy is only slightly larger than N0 = βaS/π. It follows
that the effective dimension of the signal’s space essentially corresponds to the Nyquist
number N0.
10
12:00:51, subject to the Cambridge Core

246
The Space–Wavenumber Domain
    rm
r
r´
    a
S
V
Fig. 8.10
Cylindrical conﬁguration.
The close relationship between effective bandwidth and degrees of freedom must be
stressed: it is the existence of an effective bandwidth for the ﬁeld that makes the number
of degrees of freedom practically insensitive to the error level, namely a well-deﬁned
quantity.
8.5.1
Hilbert–Schmidt Decomposition
We now attempt a more direct route to determine the number of degrees of freedom
of the scattered ﬁeld, bypassing the intermediate step of computing the bandlimitation
error. This entails computing the Hilbert–Schmidt decomposition of the Green’s
operator (8.7) and then, according to (3.56), studying the behavior of the Nth singular
value of this operator to determine the quality of the approximation and thus the degrees
of freedom of the ﬁeld. This direct method relies on the basic result discussed in
Chapter 3, that computing the singular values of a Hilbert–Schmidt kernel leads to the
optimal decomposition of the kernel into basis functions.
As attractive as it seems, this route is not entirely clear of obstacles. One problem is
that we have to compute the spectral decomposition of the Green’s operator explicitly,
and this computation depends on the geometry under consideration. Nevertheless, in
some cases direct computation is possible, and we give one such example next.
We consider the special case of cylindrical propagation, in which V is a disc of radius
a, and the observation domain is a circle S concentric and external to V – see Figure 8.10.
In cylindrical coordinates, we have r = (ρ,φr,z), r′ = (ρ′,φr′,z) with ρ = r, ρ′ = r′, and
z = 0. The current density is directed along ¯z, so that the observed electric ﬁeld has only
the ¯z component, and we can refer to the scalar ﬁeld only.
By (5.27), the scalar representation of the ﬁeld is given by
E(r) = (GI)(r) = −β2
4ωϵ0

V
H(2)
0 (β|r −r′|)I(r′)dr′,
(8.60)
10
12:00:51, subject to the Cambridge Core

8.5 Degrees of Freedom
247
where H(2)
0 (·) is the Hankel function of the second kind and order zero. Using the
addition theorem for Hankel functions (see Appendix E.5.3), (8.60) becomes
E(r) = −β2
4ωϵ0

V
I(r′)
∞

n=−∞
Jn(βr′)H(2)
n (βr)exp[jn(φr −φr′)]dr′.
(8.61)
We consider the decomposition of (8.60),
(GI)(r) =
∞

n=−∞
	
λn⟨I,ϕn⟩ξn(r),
(8.62)
where ⟨I,ϕn⟩denotes the inner product
⟨I,ϕn⟩=

V
I(r′)ϕ∗
n(r′)dr′.
(8.63)
To compute the coefﬁcients of the decomposition, we choose the following basis
functions:
ξn(r) = −H(2)
n (βr)exp(jnφr)
√βr|H(2)
n (βr)|
(8.64)
and
ϕn(r) =
Jn(βr′)exp(jnφr′)
√
2π
 a
0 |Jk(βr′)|2r′dr′1/2 ,
(8.65)
where Jn(·) is the Bessel function of the ﬁrst kind of order n, and H(2)
n (·) is the Hankel
function of the second kind and of order n. Comparing (8.62) and (8.61), and using
(8.64) and (8.65), we obtain
	
λn = πβ2(βr)1/2
2ωϵ0
√
2π
H(2)
n (βr)

 a
0
|Jn(βr′)|2r′dr′
1/2
.
(8.66)
The integral in (8.66) can be computed directly and leads to the ﬁnal result
	
λn =
μ0
ϵ0
√π
4 βa(βr)1/2 H(2)
n (βr)
[(Jn(βa))2 −Jn−1(βa)Jn+1(βa)]1/2.
(8.67)
The expansion (8.62) is the bilateral Hilbert–Schmidt decomposition of the Green’s
operator, and the obtained coefﬁcients {√λn} are the singular values of the operator,
while {ξn} and {ϕn} are the left and right singular functions. The decomposition can be
put in the usual monolateral form used in Section 5.4.1 by using the connection formulas
in Appendix E.4 and performing the substitutions
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
√λn
→
2√λn
for all n > 0,
ξn(r)
→
−H(2)
n (βr)cos(nφr)
√βr|H(2)
n (βr)|
for all n ≥0,
ϕn(r)
→
Jn(βr′)cos(nφr′)
√
2π
 a
0 |Jk(βr′)|2r′dr′1/2
for all n ≥0.
(8.68)
10
12:00:51, subject to the Cambridge Core

248
The Space–Wavenumber Domain
n
N0
1
O
a1
a2
1
 n
√
a2
a >
1
Fig. 8.11
Phase transition of the singular values.
It follows that the electric ﬁeld on the observation circle lies on a Hilbert space whose
dimension depends on the behavior of the singular values (8.67). Figure 8.11 depicts
the behavior of the singular values for two different values of βa. The singular values
undergo a phase transition around the critical value
N0 = βaS
π
= βa
π
2πr
rm
.
(8.69)
When viewed at the scale of N0, the transition tends to become a step function as a →∞.
Results analogous to (8.58) and (8.59) can now be derived by studying the asymptotic
behavior of √λn around the critical point n = N0, as N0 →∞. This is obtained by some
analytic manipulations and using uniform asymptotic expansions for which both the
argument and the index of the special functions in (8.61) tend to inﬁnity.
8.5.2
Sampling
For any given ﬁeld conﬁguration, the coefﬁcients of the optimal ﬁeld representation
leading to the notion of degrees of freedom are functionals of the ﬁeld, and thus their
computation involves knowledge of the ﬁeld over the whole observation domain. It is
of practical interest to consider simpler interpolations using only sampled ﬁeld values,
provided that the loss of optimality in the representation can be tolerated. From the
results in Section 2.4.1 it follows that if we use the cardinal series to interpolate strictly
bandlimited signals, then any constant increase in the number of sampled values does
not lead to an approximation error comparable to the one of the optimal Hilbert–Schmidt
representation. Nevertheless, alternative sampling representations can achieve the same
error as the optimal representation with a number of extra samples that grows only
sublinearly with N0, and thus provide only a marginal increase in the number of
coefﬁcients compared to the optimal Hilbert–Schmidt representation. The advantage
of such sampling interpolations is that, while being slightly suboptimal, they require
10
12:00:51, subject to the Cambridge Core

8.5 Degrees of Freedom
249
O
λ/2
S
V
Fig. 8.12
Sampling interpolation over arcs of circumference.
only a discrete set of ﬁeld values for reconstruction. These interpolation schemes can
also be applied to the almost bandlimited signals considered in this chapter, with the
same marginal order increase in the number of sampled ﬁeld values.
In the case of a circular observation domain, the number of sampled ﬁeld values used
for reconstruction depends only on the angular aperture of the observation domain and
not on its spatial extension. This follows directly from (8.69), where the arc length S is
normalized to rm, so that only the angular aperture of the observation domain plays a
role in determining the number of degrees of freedom. It follows that uniform sampling
can be performed over the angular domain rather than over the length of the observation
domain – see Figure 8.12.
For a full circle, letting λ be the transmission wavelength and recalling that β = 2π/λ,
we have
N0 = 2π
λ
a
π
2πr
rm
= 2πa
λ/2 .
(8.70)
It follows that an angular spacing corresponding to half a wavelength on the perimeter
of the radiating disc is the appropriate minimum separation distance for sampling
representations of the ﬁeld. Sampling points can be several wavelengths apart when
the receiving domain is far from the radiating system, and more tightly packed together
when the receiver is close to the radiating system. The theoretical minimum distance
λ/2 is reached in the proximity of the radiating system and corresponds to the minimum
non-redundant spacing given by the Nyquist sampling criterion over the sources’
perimeter.
This sampling result can be used to justify the practical design principles of cellular
communication systems. Consider a base station equipped with multiple antennas
located along S, serving users distributed inside the cell V. The antennas along S
can be widely separated when the base station is distant from the users, while they
are more closely packed together when the base station is closer to the users, as in
the case of microcells. An analogous argument holds for remote sensing systems,
10
12:00:51, subject to the Cambridge Core

250
The Space–Wavenumber Domain
where the objective is to construct an image of the sources or scatterers inside V. An
optimal placement of detectors occurs at constant angular spacing, so that their optimal
separation increases with their distance from the radiating system.
8.6
Cut-Set Integrals
We now provide a reﬁned computation of the number of degrees of freedom, based on
the notion of a cut-set integral, that resolves some problems in the physical interpretation
of the mathematical results obtained in the previous sections.
From the scaling property of the Fourier transform, as a signal is stretched in the
wavenumber domain, spreading its spatial bandwidth, it becomes more concentrated
over the angular observation domain. It is then natural to expect, as a →∞, that
sampling points on the observation curve that are far away from the source of radiation
provide a vanishing contribution to the information useful for reconstruction. Using
sparser sampling in regions far from the scattering system, as illustrated in Figure 8.13,
should be sufﬁcient for reconstruction. On the other hand, since the number of degrees
of freedom
N0 = βaS
π
(8.71)
increases proportionally to the length of the observation curve normalized to rm, if
the minimum distance from the scattering system is kept constant, then the number of
spatial samples required for reconstruction increases proportionally to the length of the
observation curve. In this case, the information carried by waves propagating through
rm
O
S
V
Fig. 8.13
Sampling interpolation over an arbitrary curve.
10
12:00:51, subject to the Cambridge Core

8.6 Cut-Set Integrals
251
an indeﬁnite open curve may be larger than the information ﬂow through any circular
enclosure of the whole scattering system, given by (8.70). Since one expects a circular
enclosure to capture the whole information ﬂow radiating from the scattering system,
the degrees of freedom would be a non-conservative information measure – something
that should make us slightly uncomfortable, to say the least.
Fortunately, a closer look at the derivation in Section 8.4 makes the model again
consistent with our physical intuition, showing that the number of degrees of freedom
is indeed a conservative quantity. The crux of the matter is that in our derivation we
have considered the largest bandwidth along the observation curve given by (8.20), and
then computed the number of degrees of freedom using such a maximum value. As a
consequence, unless the spatial bandwidth is constant along the curve, as in the case of
a circular observation domain, the samples’ spacing can be unnecessarily small in the
regions where the spatial bandwidth is signiﬁcantly smaller than its maximum value,
and this can lead to redundancy in the corresponding ﬁeld representation.
Repeating the derivation in Section 8.4 while accounting for the variability of the
spatial bandwidth along an arbitrary observation curve, we obtain a non-redundant
representation of the ﬁeld over the curve, expressed in integral form. The number
of degrees of freedom can then be computed as a cut-set integral representing the
amount of information ﬂowing through any arbitrary observation domain external to
the radiating system. As expected, the maximum value of this integral corresponds to a
closed circular enclosure of the radiating system, and in this case it reduces to the value
2βa = 2πa/(λ/2) obtained previously.
8.6.1
Linear Cut-Set Integral
We consider the ﬁeld radiated by a scattering system enclosed in a convex domain V
bounded by a surface with rotational symmetry obtained by rotating an analytic curve
lying in the plane φ = 0 about the z axis. We let ζ : S →R be an arbitrary coordinate
along the observation curve, s : S →R be the arc length coordinate, and ¯t the unit vector
tangent to s – see Figure 8.14. All distance lengths are normalized to the wavelength of
transmission.
The reduced ﬁeld is obtained by extracting an analytical phase factor ψ(ζ) from the
radiated ﬁeld that depends on the chosen coordinate along the curve. Namely,
E(ζ) →exp(jψ(ζ))E(ζ).
(8.72)
Following the same steps as in Section 8.4, we let the local bandwidth at point ζ be
w(ζ) = max
r′∈V

∂
∂ζ (ψ(ζ) −2πR(r′,ζ))
.
(8.73)
The phase factor ψ is chosen so that w(ζ) is minimized for all ζ. This is accomplished
by letting the derivative of ψ be equal to the average between the maximum and
minimum values of 2π∂R/∂ζ,
∂ψ
∂ζ = π

max
r′∈V
∂R
∂ζ + min
r′∈V
∂R
∂ζ

.
(8.74)
10
12:00:51, subject to the Cambridge Core

252
The Space–Wavenumber Domain
r
r´
R
R
t
x
y
z
s
S
V
Fig. 8.14
Linear cut-set integral geometry.
Substituting (8.74) into (8.73), we obtain
w(ζ) = π

max
r′∈V
∂R
∂ζ −min
r′∈V
∂R
∂ζ

.
(8.75)
Next, the curvilinear coordinate ζ is chosen so that w is constant along the curve,
namely w(ζ) = W for all ζ ∈M. This amounts to imposing the local bandwidth to be
constant along the curve by choosing a certain curvilinear coordinate that depends on W.
The number of degrees of freedom is obtained by ﬁrst substituting into (8.75)
∂R
∂ζ = ∂R
∂s
∂s
∂ζ ,
(8.76)
obtaining
Wdζ = π

max
r′∈V
∂R
∂s −min
r′
∂R
∂s

ds.
(8.77)
Then, we integrate along the curve S, obtaining
W

S
dζ = π
 S
0

max
r′∈V
∂R
∂s −min
r′
∂R
∂s

ds
= π
 S
0

max
r′∈V
¯R · ¯t −min
r′
¯R · ¯t

ds.
Finally, we compute the Nyquist number associated with the bandwidth W as
N0 = W

S dζ
π
=
 S
0

max
r′∈V
¯R · ¯t −min
r′∈V
¯R · ¯t

ds.
(8.78)
This dimensionless quantity corresponds to the space-bandwidth product of the radiated
ﬁeld. It represents the effective dimensionality of the space of signals of bandwidth W,
observed over the domain S. The approximation error can be related to the geometric
parameters of the scattering system and of the observation curve, and it becomes
10
12:00:51, subject to the Cambridge Core

8.6 Cut-Set Integrals
253
arbitrarily small when taking appropriate limits of these parameters. An accurate ﬁeld
reconstruction is possible by interpolating only slightly more than N0 basis functions.
As we shall see in the next section, in the case of an arbitrary meridian curve lying on
the plane φ = 0, the relevant parameter is the wavelength-normalized meridian length
of the scattering system. In the case of an arbitrary circle lying on a plane at constant z,
the relevant parameter depends on both the z coordinate and the radius of the circle, but
it is always bounded by the wavelength-normalized length of the maximum latitudinal
circle of the scattering system.
We call (8.78) the cut-set integral associated with the cut S and the radiating system V.
It measures the total incremental (maximum minus minimum) variation of the tangential
component of R along the cut S, over all possible scatterer conﬁgurations. A physical
interpretation is the following:
The cut-set integral measures the richness of the information content of a multiple
scattered ﬁeld in terms of a variational measure of the scattering system with respect
to the cut through which the information must ﬂow.
This interpretation implies that if the combined effect of the scattering environment
and the receiver’s geometries are such that R is “highly variable” in the sense of the
integral above, then the scattered ﬁeld has a large information content.
8.6.2
Surface Cut-Set Integral
The results in the previous section can be generalized to surface cuts by considering
two-dimensional observation manifolds. The idea is to refer to a set of appropriate
coordinate curves on the observation manifold and decompose the ﬁeld separately
along each coordinate. The resulting surface integral, measuring the number of spatial
degrees of freedom, is interpreted as a surface boundary through which the information
must ﬂow and provides a limit on the amount of information that can radiate from the
scattering system.
We consider a cylindrical coordinate system (ρ,φ,z) and a scattering system enclosed
in a convex domain V bounded by a surface with rotational symmetry obtained by
rotating an analytic curve lying in the plane φ = 0 about the z axis. The ﬁeld is observed
over a surface of revolution S external to V and generated by the rotation of an analytic
meridian curve Sz lying on the plane φ = 0 about the z axis – see Figure 8.15. We let
ρ′(z′) be the radial coordinate of V and
max
z′ ρ′(z′) = a.
(8.79)
Consider the representation of the ﬁeld on the circumference Sφ obtained by
intersecting S with a plane at z = zs. The number of degrees of freedom of the ﬁeld
on this circumference can be computed by evaluating the corresponding cut-set integral
(8.78). Due to the cylindrical symmetry, the extreme values of ∂R/∂s are in this case
opposite and constant along Sφ. It follows that the bandwidth is constant along the curve
10
12:00:51, subject to the Cambridge Core

254
The Space–Wavenumber Domain
x
y
z
z´
z
´
a
s
zs
s
S
S
V
Fig. 8.15
Surface cut-set integral geometry.
and, as in the cylindrical case examined in Section 8.5.1, we can choose the azimuthal
angular parametrization of the curve ζ = φs. The cut-set integral (8.78) can then be
computed and leads to a Nyquist number that depends on ρs and zs,
N0(ρs,zs) = 2π max
z′
	
(zs −z′)2 + (ρs + ρ′(z′))2 −
	
(zs −z′)2 + (ρs −ρ′(z′))2

≤2πa
λ/2 .
(8.80)
As the circle is moved to the proximity of V and ρs →ρ′(z′), we also have the limiting
result as ρ′(z′) →∞:
N0(ρs,zs) ∼2πρ′(z′)
λ/2
.
(8.81)
It follows that the number of degrees of freedom of an azimuthal circle placed near the
boundary of V can be approximated as ρ′(z′) →∞, or equivalently a →∞, by the
number of samples equispaced by half a wavelength of the corresponding latitudinal
curve on the surface of the radiating system. The upper bound obtained when the
10
12:00:51, subject to the Cambridge Core

8.6 Cut-Set Integrals
255
circle is in correspondence with the largest radial coordinate of V coincides with (8.70),
obtained in the cylindrical case – see also Figure 8.12.
As in the cylindrical case, the optimal representation of the ﬁeld on Sφ is given in
terms of cylindrical harmonics,
E(ρs,φs,zs) =
∞

n=−∞
En(ρs,zs)ejnφs,
(8.82)
where the vector coefﬁcients {En(ρs,zs)} do not depend on φ and represent the ﬁeld
along the meridian curves Sz obtained by intersecting S with any plane at constant
φ. Comparing (8.82) with the analogous expansion for the cylindrical case given in
Section 8.5.1, we see that while the basis functions are the same complex exponentials
in the both cases, the coefﬁcients in this case are vectors. This is because the ﬁeld is not
necessarily oriented along z as in the cylindrical case, and we cannot refer to a unique
scalar ﬁeld representation. In addition, the coefﬁcients depend on both ρ and z, while
they were independent of z in the cylindrical propagation case.
The number of degrees of freedom of each vector coefﬁcient En(ρS,zs) can also be
computed by evaluating the one-dimensional cut-set integral in (8.78) on the meridian
curve Sz and choosing an appropriate parametrization ζ = ζ(ρs,zs) of the curve. We
choose the parametrization for which the spatial bandwidth
W = ℓ/λ
(8.83)
is constant along the curve and equals the meridian length of the scattering system ℓ,
namely the curve obtained by intersecting V with a plane at constant φ, normalized by
the wavelength. For this parametrization, any meridian closed curve encircling V covers
a 2π range, and by (8.78) the corresponding Nyquist number is given by
N0(ℓ) = 2πℓ
πλ =
ℓ
λ/2.
(8.84)
It follows that the number of degrees of freedom on any meridian curve encircling the
radiating system can be approximated as ℓ→∞, or equivalently a →∞, by the number
of equispaced samples on a meridian of the system’s surface.
Now let S be a surface enclosing V; the total number of degrees of freedom on S has
a phase transition in the neighborhood of
N0 = 1
2

Sz
N0(ℓ)N0(ρs,zs)dz
∼
ℓ
λ/2
π
λ/2

Sz
ρ′(z′)dz′
= A(S)
(λ/2)2 ,
(8.85)
where A(S) is the area of S and the asymptotic equality holds by virtue of (8.81) and
(8.84) as S is placed in the proximity of V and is made conformal to its boundary, and
as the system’s size a →∞.
10
12:00:51, subject to the Cambridge Core

256
The Space–Wavenumber Domain
This result has an appealing physical interpretation: the number of degrees of
freedom of arbitrary scattered ﬁelds is essentially coincident with those of an array
of radiating elements spaced by λ/2 and conforming to the surface enclosing the
radiating system. It follows that a non-redundant ﬁeld representation should use a
number of parameters at most of the order of (8.85). This is relevant for remote
sensing and inverse scattering problems. Using a number of sensors larger than the
obtained physical bound is essentially useless. Similarly, in communications, using
a number of antennas larger than the obtained physical bound does not lead to
additional multiplexing gain. In other words, the obtained bound applies to the spatial
decompositions described in Sections 5.3.2 and 5.4.1, providing a physical limit on the
rank of the propagation operator. The propagation environment works as a low-pass
spatial ﬁlter, whose spatial band is ultimately limited by the size of the scattering
system.
Although the obtained bound holds for arbitrary scattering systems, in many practical
cases the spatial bandwidth can be smaller than what has been computed here, or,
in the case of the superposition of a number of occupied sub-bands as described in
Section 3.5.2, the ﬁeld can be represented using a signiﬁcantly smaller number of basis
functions.
8.6.3
Applications to Canonical Geometries
The cut-set integral can be used to bound the effective dimensionality of the ﬁeld
anywhere outside a closed surface bounding the radiating system. By symmetry, it
also applies to the ﬁeld anywhere inside a closed surface not containing the radiating
system. This follows from the equivalence result in Section 4.7 stating that knowledge
of the ﬁeld on any closed surface containing the sources uniquely determines the ﬁeld
outside it. Similarly, for outside sources, knowledge of the ﬁeld on the surface uniquely
determines the ﬁeld inside. In the following, we illustrate how these results can be
applied to different canonical geometries.
Consider an arbitrary radiating environment bounded by a sphere of radius a placed
in three-dimensional space. Consider another sphere concentric to the radiating one and
of the same order of radius, say 2a, as depicted in Figure 8.16(a). The number of degrees
of freedom of the ﬁeld measured anywhere inside the annular volume delimited by radii
a and 2a can be computed in terms of the cut-set integral of the inner annular surface.
From the results in Section 8.6.2, as a →∞this is given by
N0 = O(a2/λ2).
(8.86)
In this spherical geometry the cut-set area, normalized by the wavelength squared,
provides a resolution limit for any ﬁeld observation.
Consider now the half-spherical geometry depicted in Figure 8.16(b). This can
represent communication using multiple antennas in an urban environment, where
high-rise buildings may be present near the city center, while only lower constructions
can be present at the periphery of the city. It is easy to check that (8.86) applies in this
case as well. The situation changes when the scattering system does not scale in all three
10
12:00:51, subject to the Cambridge Core

8.6 Cut-Set Integrals
257
a
a
h
a
(a)
(b)
(c)
Fig. 8.16
Three canonical geometries.
dimensions. Consider, for example, a model of communication in suburban, residential,
and rural areas where the heights of the scattering objects are uniformly bounded. The
scattering system can in this case be modeled as a cylinder of radius a and of ﬁxed
height h – see Figure 8.16(c). The number of degrees of freedom of the ﬁeld inside the
outer cylindrical annulus of height h and delimited by radii a and 2a can be computed
in terms of the cut-set integral of its bounding surface to be
N0 = O
ha
λ2

,
(8.87)
as a →∞. This shows that since only two-dimensional scaling occurs as a →∞, the
number of degrees of freedom scales in this case as a rather than a2.
This result follows by ﬁrst computing the cut-set integral on the perimeter of the
rectangle Q depicted on the right-hand side of Figure 8.17, and then extending the
integral to a surface cut following the procedure described in Section 8.6.2, namely
considering a rotation of the cut-set perimeter along a 2π azimuthal angle. In the
following, we assume that the edges of the rectangle are smoothed, so that the
observation curve is analytical, and that the rectangle is separated from the radiating
domain so that the observation surface is external to the radiating system. With these
assumptions, the cut-set integral (8.78) along the perimeter of the rectangle Q can be
decomposed into four terms, each corresponding to one side of the rectangle. Since the
rectangle has constant height h, the cut-set integral along each vertical side is at most
2h/λ and, by symmetry, the integrals along the two horizontal sides are equal. It follows
that it sufﬁces to compute the integral over the upper horizontal side corresponding to
ρ = s, y = 0, z = h, and s ∈[0,a]. From elementary geometry it is clear that the extreme
10
12:00:51, subject to the Cambridge Core

258
The Space–Wavenumber Domain
z
ρ
R2
t
r´2
r´1
s
O
Q
Fig. 8.17
Linear cut-set integral for the cylindrical geometry.
values of ¯R·¯t correspond to the positions r′
1 and r′
2 illustrated in Figure 8.17, for which
we have
¯R1 · ¯t = 1,
(8.88)
¯R2 · ¯t =
1
	
1 + (h/s)2 .
(8.89)
Letting the normalized lengths h∗= h/λ and a∗= a/λ, we have
 a∗
0

max
r′
¯R · ¯t −min
r′
¯R · ¯t

ds
=
 a∗
0
1 −
1
	
1 + (h/s)2 ds
≤
 a∗
0
(h∗/s)2
1 + (h∗/s)2 ds
= h∗arctana∗
< 2h∗
=
h
λ/2,
(8.90)
where the ﬁrst inequality follows from
√
1 + x2 ≤1 + x2 for all x. It follows that the
cut-set integral along the perimeter of the rectangle Q is
N0 < 4h
λ/2.
(8.91)
Extending the computation along the azimuthal angle as in (8.85) amounts to
multiplying by an additional factor of order of 2πa/λ, and we obtain (8.87).
8.7
Backscattering
So far, we have considered all sources and scatterers to be enclosed in convex domains
bounded by a cut-set surface with rotational symmetry. We now consider the effect of
scatterers placed outside the radiating system. These can change the ﬁeld conﬁguration
10
12:00:51, subject to the Cambridge Core

8.7 Backscattering
259
a
V
Fig. 8.18
Propagation in the presence of scatterers outside V.
at any observation point outside V. However, the number of distinct ﬁeld conﬁgurations
that can be observed by only varying the scattering environment and the currents inside
V remains the same. Since the number of degrees of freedom refers to the ﬁeld generated
by all possible conﬁgurations of sources and scatterers inside the scattering system, it
follows that:
The number of degrees of freedom of the ﬁeld radiated by V does not change in the
presence of external scatterers.
Restated intuitively in terms of information ﬂow, the result is that a closed surface
surrounding the radiating system captures the whole information ﬂow coming out of
the system and the backscattered ﬁeld does not provide any new information.
We ﬁrst provide an heuristic explanation of this result. The radiating and scattering
elements that we can vary to produce different ﬁeld conﬁgurations are inside V. By the
equivalence result in Section 4.7, the electric ﬁeld at any point outside V is uniquely
determined by the ﬁeld on the surface of V. This is composed of the ﬁeld radiated from
inside V, which we know has a number of degrees of freedom proportional to a/λ, and
the ﬁeld backscattered from outside V. This latter term changes the ﬁeld conﬁguration
without adding additional degrees of freedom since it is also due to the sources inside
V. It follows that the external scatterers can only modulate the spatial modes of the
signal, but in the limit a →∞the phase transition behavior around a/λ of the number
of degrees of freedom is preserved. Next, we provide a more rigorous derivation.
Let the electric ﬁeld at any point outside V be given by the superposition of the
direct contribution due to the currents inside V and the scattered contribution due to the
induced currents on the outside scatterers:
E(r) = ED(r) + ES(r).
(8.92)
We show that both ﬁeld vectors in (8.92) have a number of degrees of freedom
corresponding to the Nyquist number N0 = O(a2/λ2), as a →∞.
Let us ﬁrst focus on ED, the ﬁeld vector due to the source currents and to the induced
currents inside V. The induced currents are due to the scattered ﬁeld inside V, and also to
10
12:00:51, subject to the Cambridge Core

260
The Space–Wavenumber Domain
the ﬁeld backscattered from outside V. Letting I(r′), r′ ∈V, be the total current density,
we have
ED(r) =

V
G(r −r′)I(r′)dr′.
(8.93)
In the analysis of previous sections the number of degrees of freedom of the ﬁeld
has been determined by assuming an arbitrary current density in V, so that the same
analysis applies here and the number of degrees of freedom of the direct ﬁeld ED still
corresponds to the Nyquist number N0 = O(a2/λ2).
We now focus on ES(r), the ﬁeld due to the currents induced on the scatterers outside
V. Letting V′ be the volume occupied by the scattering elements outside V and I(r′),
r′ ∈V′ be the induced current density in V′, we have
ES(r) =

V′ G(r −r′)I(r′)dr′.
(8.94)
From (4.31) the induced current I(r′) is not arbitrary but is linearly related to the
electric ﬁeld in V′,
I(r′) = [σ(r′) + jω(ϵ(r′) −ϵ0)]E(r′),
(8.95)
where ϵ is the permittivity of the dielectric material of the scatterer and σ its
conductivity. In the case of perfect conductors (8.95) cannot be used since σ →∞.
In this case, the analysis is completely equivalent by using (4.23) in lieu of (8.95) and
referring to the magnetic ﬁeld. Substituting (8.95) into (8.94), we obtain the scattered
ﬁeld
ES(r) = jω

V′ G(r −r′)[σ(r′)(ϵ(r′) −ϵ0]E(r′)dr′,
(8.96)
which shows that ES is linearly related to the ﬁeld on the scatterers.
Substituting (8.96) into (8.92), we obtain the integral equation
E(r) = ED(r) +

V′ G(r −r′)[σ(r′)(ϵ(r′) −ϵ0]E(r′)dr′.
(8.97)
This is an inhomogeneous Fredholm integral equation of the second kind, whose
resolvent formalism leads to the Liouville–Neumann series – see Appendix A.5.2. We
have already encountered the homogeneous version of this equation in the context of
Slepian’s concentration problem in Chapter 2. The equation forms the basis of the
multiple scattering theory examined in Chapter 9. What is important here is that it shows
a linear relationship between the direct and the total ﬁeld. Since by (8.96) the scattered
and the total ﬁelds are also linearly related, it follows that ES and ED are linearly
related and thus have the same number of degrees of freedom. The linear operation
cannot increase the spatial bandwidth of the signal and the phase transition behavior
is preserved in the usual asymptotic sense. We conclude that the electric ﬁeld outside
V can be expressed as the superposition of two ﬁeld vectors, ED(r′) and ES(r′), each
lying in a Hilbert space whose dimension is limited by the range space of the radiation
operator G, yielding as a →∞a number of degrees of freedom corresponding to the
Nyquist number N0 = O(a2/λ2).
10
12:00:51, subject to the Cambridge Core

8.9 Test Your Understanding
261
8.8
Summary and Further Reading
The question of determining the spatial information content of electromagnetic ﬁelds
in terms of degrees of freedom dates back to at least Toraldo di Francia (1955, 1969)
and Gabor (1961). Our treatment followed the rigorous developments by Bucci and
Franceschetti (1987, 1989) on the spatial bandwidth and the degrees of freedom of
scattered ﬁelds based on the approximation-theoretic arguments developed in Chapter 3.
We ﬁrst showed that the electromagnetic ﬁeld radiated by an arbitrary scattering system
is essentially spatially bandlimited. From this result it follows that the number of
degrees of freedom undergoes a phase transition at a critical point that is an intrinsic
property of the system, depending only on its size. The same result can be obtained by
performing the Hilbert–Schmidt decomposition of the Green’s operator, whenever the
geometric conﬁguration of the radiation problem permits computation in closed form.
These results were placed in an information-theoretic setting by Franceschetti, Migliore,
and Minero (2009). An interpretation of the degrees of freedom in terms of information
ﬂow leads to the deﬁnition of a cut-set integral that was developed by Franceschetti et
al. (2011), following the work of Bucci, Gennarelli, and Savarese (1998). Near-ﬁeld
effects on the number of degrees of freedom were considered by Janaswamy (2011) and
Franceschetti et al. (2015).
Related results appeared at different points in the literature, including in optics, by
Miller (2000) and Piestun and Miller (2000); in information theory, by Poon, Brodersen,
and Tse (2005); and in signal processing, by Kennedy et al. (2007).
Sampling interpolations can provide a valid alternative to optimal representations.
The truncation errors of these representations have been extensively studied in signal
processing. Extensive reviews by Jerri (1977), Unser (2000), and Garcia (2000) contain
a compendium of references. Approximate prolate spheroidal sampling functions were
introduced by Knab (1979, 1983) and provide order-optimal reconstruction. Their
truncation error in the context of approximately bandlimited electromagnetic signals
was examined by Bucci and Di Massa (1988).
8.9
Test Your Understanding
Problems
8.1
Consider a sinusoidal source current and use (8.3) to compute the ﬁeld radiated
by an elementary dipole placed at the origin of the space. Show that in the far ﬁeld the
solution coincides with the Fourier transform of (4.92a).
Solution
Considering a sinusoidal current density at coordinate r′ = 0, so that R = r, we have
i(r′,t) = ℜ[I0ℓexp(jωt)δ(r′)] ¯z [Am−2].
(8.98)
10
12:00:51, subject to the Cambridge Core

262
The Space–Wavenumber Domain
We proceed using the complex notation and extract the real part at the end of the
computation. For the considered source current, (4.92a) becomes
E(s) = ζ
4π
I0ℓ
cr jωexp(jωt∗)sinθ ¯θ = I0ℓexp(jωt)jωμ
4πr exp(−jβr)sinθ ¯θ.
(8.99)
We want to show that the same result holds by using (8.3) to compute the electric ﬁeld
when r →∞. Comparing (8.99) with (8.3) we see that to show the equivalence of the
two derivations we need to show that the dyad transformation N amounts to performing
a rotation from the ¯z axis to the ¯θ axis and a multiplication by −sinθ. By applying the
dyad N to the column vector
exp(−jβr)
r
¯z =
⎛
⎜⎜⎝
0
0
exp(−jβr)
r
⎞
⎟⎟⎠,
and considering for convenience the electric ﬁeld radiated in the plane y = 0, we get for
the dominant ﬁeld components, as r →∞,
⎛
⎜⎜⎜⎜⎝
rexp(jβr) 1
β2
∂2
∂x∂z
exp(−jβr)
r
0
1 + rexp(jβr) 1
β2
∂2
∂z2
exp(−jβr)
r
⎞
⎟⎟⎟⎟⎠
=
⎛
⎝
−sinθ cosθ
0
1 −cos2 θ
⎞
⎠= sinθ
⎛
⎝
−cosθ
0
sinθ
⎞
⎠= −sinθ ¯θ.
Due to the symmetry along the φ coordinate, the above result is valid in any plane
containing the z axis.
8.2
Perform the computation to obtain (8.31) from (8.29).
Solution
The Airy integral is deﬁned as
Ai(x) = 1
2π
 ∞
−∞
exp(jy3/3 + jxy)dy.
(8.100)
By the change of variable jy = τ and moving the integration path to the left complex
half-plane, it can also be written as
Ai (x) =
1
2πj
 j∞
−j∞
exp(−τ 3/3 + xτ)dτ
=
1
2πj

C⋆exp(−τ 3/3 + xτ)dτ.
(8.101)
Letting
z =

...
ψ(s0)
2

1/3
(s −s0),
(8.102)
10
12:00:51, subject to the Cambridge Core

8.9 Test Your Understanding
263
we have  ∞
σ
Ai (x)exp(jzx)dx =
1
2πj
 ∞
σ
exp(jzx)

C⋆exp(−τ 3/3 + xτ)dτdx
=
1
2πj

C⋆exp(−τ 3/3)
 ∞
σ
exp(jzx + xτ)dxdτ
= −1
2πj

C⋆exp(−τ 3/3)exp(jzσ + τσ)
jz + τ
dτ
= −exp[j(s −s0)(w −w0)]
1
2πj

C⋆
exp(−τ 3/3 + τσ)
jz + τ
dτ,
(8.103)
where the last equality follows from (8.30) and (8.102). Substituting (8.103) into (8.31),
we obtain (8.29).
8.3
Perform the computation to obtain (8.32) from (8.31).
Solution
Letting
z = −

...
ψ(s0)
2

1/3
(s −s0),
(8.104)
we have, from (8.31),
 ∞
−∞
|G(s,r′)|2ds ∼|G(s,r′)|2

...
ψ(s0)
2

1/3
 ∞
−∞

 ∞
−∞
Ai (x)U(x −σ)exp(−jzx)dx

2
dz,
(8.105)
where U(·) is Heaveside’s step function. By Parseval’s theorem, the result now follows.
8.4
Derive (8.47) using the following properties of the Airy function, valid for x > 0:
 ∞
x
Ai 2(x)dx = −xAi 2(x) +
∂Ai(x)
∂x
2
,
(8.106)
Ai (x) ≤exp[(−2/3)x3/2]
2√πx1/4
,
(8.107)

∂Ai(x)
∂x
 ≤x1/4 exp[(−2/3)x3/2]
2√π

1 +
7
48x3/2

.
(8.108)
8.5
“A Hardy Old Problem”:2 Suppose that f(x) and ∂f(x)/∂x are continuous and that
their limits for x →∞exist. Show that this implies that
lim
x→∞
∂f
∂x = 0.
(8.109)
2 This problem was posed and discussed by G. H. Hardy in 1908; see Landau and Jones (1983).
10
12:00:51, subject to the Cambridge Core

264
The Space–Wavenumber Domain
Discuss how this result relates to (8.19) and the associated geometrical conditions for
the existence of the limits in this case.
Solution
Using L’Hôpital’s rule, we have
lim
x→∞f(x) = lim
x→∞
exf(x)
ex
= lim
x→∞
ex[f(x) + ∂f(x)/∂x]
ex
= lim
x→∞f(x) + ∂f(x)/∂x.
(8.110)
10
12:00:51, subject to the Cambridge Core

9
The Time–Frequency Domain
Henceforth space by itself, and time by itself, are doomed to fade away into mere shadows, and
only a kind of union of the two will preserve an independent reality.1
9.1
Frequency-Bandlimited Signals
In this chapter we extend the information-theoretic treatment of single-frequency
sinusoidal waves presented in Chapter 8 to frequency-bandlimited signals, and compute
the total number of degrees of freedom in the space–wavenumber and time–frequency
domains. This corresponds to determining the total amount of information associated
with the spatial and temporal conﬁgurations of the waveform, expressed in terms of the
effective dimensionality of the corresponding functional space.
We derive the results in a deterministic setting, and refer to scalar space–time
waveforms f(r,t). By relying on the theory developed in Chapter 3 for signals of
multiple variables, and applying it to the case of radiation from an arbitrary multiple
scattering environment, we show that in appropriate asymptotic regimes the total
number of degrees of freedom is given by the product of the time–frequency and
space–wavenumber degrees of freedom.
Since the space–wavenumber and time–frequency degrees of freedom are not
independent of each other, to perform our computation we need to take particular care in
scaling the support sets of the signals in the natural and transformed domains to achieve
the desired spectral concentration results.
The general conclusion is that the total number of degrees of freedom is proportional
to the spatial extension of the cut-set through which the information must ﬂow,
which represents the number of space–wavenumber degrees of freedom, and to the
time-bandwidth product, representative of the number of time–frequency degrees of
freedom.
1 Herman Minkowski (1908). Address to the society of German natural scientists and physicians at Köln,
September 21.
11
12:00:51, subject to the Cambridge Core

266
The Time–Frequency Domain
0
z
x
y
S
f (r,t)
Fig. 9.1
Geometry of the signal model.
9.2
Radiation with Arbitrary Multiple Scattering
Let S be the surface of the sphere of radius r. We consider real space–time waveforms
f : S × R →R on this surface that satisfy the energy constraint
 ∞
−∞

S
f 2(r,t)drdt ≤E.
(9.1)
These waveforms are assumed to have angular frequency support [−,] and to be the
image of uniformly bounded sources i(r′,t) that are internal to the sphere of radius r,
through the convolution integral
f(r,t) =
 ∞
−∞

V
g(r −r′,t −t′)i(r′,t′)dr′dt′,
(9.2)
where V is the interior of S containing all the sources, r′ < r, and g is the free-space
Green’s function. This geometric setting is depicted in Figure 9.1.
We let S be the set of square-integrable signals satisfying (9.1) and B,S ⊂S the
subset of signals frequency-bandlimited to [−,] and spatially limited to S satisfying
(9.2). We consider the norm
∥f∥=
 T/2
−T/2

S
f 2(r,t)drdt
1/2
,
(9.3)
and determine the number of degrees of freedom at level ϵ of B,S in S , namely
Nϵ(B,S) = min{N : d2
N(B,S,S )/E ≤ϵ},
(9.4)
where dN(B,S,S ) is the Kolmogorov N-width of the space B,S in S , introduced in
Chapter 3.
11
12:00:51, subject to the Cambridge Core

9.2 Radiation with Arbitrary Multiple Scattering
267
This setup extends the one discussed in Chapter 3 for time–frequency signals and
the one discussed in Chapter 8 for space–wavenumber signals. In the former case, we
considered bandlimited signals f(t) of a single variable, subject to the constraint
 ∞
−∞
f 2(t)dt ≤E,
(9.5)
and equipped with the norm
∥f∥=
 T/2
−T/2
f 2(t)dt
1/2
,
(9.6)
and we have shown that the number of degrees of freedom is given by
Nϵ = T/π + o(T)
as T →∞,
(9.7)
where the ϵ-dependency appears only in the pre-constant of the second-order term o(T).
The frequency bandwidth is assumed to be ﬁxed and the observation interval is scaled
as T →∞to achieve the desired spectral concentration result.
In the case of Chapter 8, we considered time-harmonic signals f(r) on the surface S
of a sphere of radius r, subject to the constraint

S
f 2(r)dr ≤E,
(9.8)
and equipped them with the norm
∥f∥=

S
f 2(r,t)dr
1/2
.
(9.9)
In this case, we have shown that the number of degrees of freedom is given by
Nϵ = 4π
ωr
cπ
2
+ o(r2)
as r →∞,
(9.10)
where once again the ϵ-dependency appears only in the pre-constant of the second-order
term o(r2). The solid angle 4π over which the signal is observed is ﬁxed, and
the wavenumber bandwidth is scaled as r →∞to achieved the desired spectral
concentration result.
We have also shown the additional result that for two-dimensional circular systems
radiating a sinusoidal waveform observed over a circumference of radius r,
Nϵ = 2π ωr
cπ + o(r)
as r →∞.
(9.11)
We now consider the more general setting where square-integrable signals f(r,t)
observed over a ﬁnite angular domain are not composed of a single frequency, but have
spectral support [−,].
9.2.1
Two-Dimensional Circular Domains
We start with a two-dimensional domain of cylindrical symmetry in which an
electromagnetic waveform is radiated by a conﬁguration of currents located inside a
11
12:00:51, subject to the Cambridge Core

268
The Time–Frequency Domain
w
0
Q
 
0
P
T/2
t
−T/2



φ
Fig. 9.2
Support sets in the natural and transformed domains.
circular domain of radius r, oriented perpendicular to the domain, and constant along
the direction of ﬂow. This can model, for example, an arbitrary scattering environment
where a spatial distribution of transmitters is placed inside the circular domain, and
communicate with receivers placed outside the domain. The same model applies to a
remote sensing system where objects inside the domain are illuminated by an external
waveform, and the scattered ﬁeld is recovered by sensors placed outside the domain.
The radiated ﬁeld away from the scattering system and measured at the receivers is
completely determined by the ﬁeld on the cut-set boundary through which it propagates.
On this boundary, we can refer to a scalar ﬁeld f(φ,t) that is a function of only two scalar
variables: one angular, and one temporal. The geometry is the same as in Figure 1.7, and
the corresponding four ﬁeld representations, linked by Fourier transforms, are depicted
in Figure 1.8, where the angular frequency ω indicates the transformed coordinate of the
time variable t, the wavenumber w indicates the transformed coordinate of the angular
variable φ, and the Fourier transform Ff(φ,t) = F(w,ω).
We wish to determine the number of degrees of freedom of the space–time ﬁeld f(φ,t)
on the cut-set boundary. In Section 1.2.3 we gave an informal argument, integrating the
number of spatial degrees of freedom along the frequency bandwidth, and neglecting
the possible accumulation of the approximation error. To derive rigorous results, we
now apply the theory developed in Chapters 2 and 3, and take appropriate scaling limits
of the support sets of the ﬁeld.
To determine the scaling limits to apply, let us examine the geometric constraints
imposed by the model. On the one hand, the observation domain is limited to φ ∈
[−π,π]. In this case, as discussed in Chapter 8, the wavenumber bandwidth w is related
to the frequency of transmission in such a way that for any possible conﬁguration of
sources and scatterers inside the circular radiating domain, we have
w = ωr/c + o(ωr) as ωr →∞.
(9.12)
It follows that the wavenumber bandwidth increases linearly with the frequency of
radiation up to a maximum value r/c. To apply the scaling results developed in
Sections 3.5.3 and 3.5.4, we then need to scale the support sets of the ﬁeld depicted
in Figure 9.2.
We can scale the spectral support Q while keeping P ﬁxed by letting  →∞. In this
case, by (3.119) the number of degrees of freedom over a ﬁxed transmission time and
11
12:00:51, subject to the Cambridge Core

9.2 Radiation with Arbitrary Multiple Scattering
269
cut-set interval of width 2π, in the wide-band frequency regime, is
Nϵ(TP) = 22rT2π
c(2π)2
+ o(2)
= T
π
2πr
2πc + o(2) as  →∞.
(9.13)
On the other hand, our geometric conﬁguration does not allow scaling of the support
P while keeping Q ﬁxed because the cut-set domain is limited to an angle 2π. In this
case we can apply the general result (3.129) to obtain the number of degrees of freedom
over a ﬁxed frequency band, by scaling the time coordinate of P and the wavenumber
coordinate of Q, and we have
Nϵ(BQ) = T
π
2πr
2πc + o(rT) as T,r →∞.
(9.14)
Equations (9.13) and (9.14) conﬁrm the intuition that the number of degrees of
freedom is given by the product of two factors, each viewed in an appropriate asymptotic
regime: one accounting for the number of degrees of freedom in the time–frequency
domain, T/π, and the other accounting for the number of degrees of freedom in the
space–wavenumber domain, 2πr/(2πc). The latter factor physically corresponds to
the perimeter of the disc of radius r normalized by an interval of wavelengths 2πc/,
and can be interpreted as the spatial cut-set through which the information must ﬂow.
The idea is that for any ﬁnite-size system, the wavenumber bandwidth is a limited
resource. Each parallel channel occupies a certain amount of spatial resource on the cut,
proportional to the wavelength of transmission, and these channels must be sufﬁciently
spaced along the cut for the corresponding waveforms to provide independent streams
of information. The total number of channels is then given by the total spatial resource,
given by the cut length 2πr, divided by the total occupation cost, given by the
wavelength interval 2πc/.
The results are analogous to the ones obtained in Section 1.2.3. Namely, (9.13) and
(9.14) correspond to the rigorous derivation of (1.16).
9.2.2
Three-Dimensional Spherical Domains
We now extend the results to three spatial dimensions by considering a spherical
radiating system of radius r. In this case, the surface of the sphere is interpreted as a
cut-set through which the information must ﬂow and provides a limit on the amount
of information that can radiate from the interior of the domain to the outside space.
The ﬁeld f(θ,φ,t) on the cut-set boundary is a function of two curvilinear coordinates,
identifying a point on the surface, and one temporal one. The measure of the support set
P of the ﬁeld in the natural domain is the same as that of a prism of base 4π, indicating
the measure of the solid angle subtended at the center of the sphere, and of height T,
indicating the measure of the time observation interval. The measure of the support set
Q of the ﬁeld in the transformed domain is the same as that of the bow-tie shape shown
11
12:00:51, subject to the Cambridge Core

270
The Time–Frequency Domain
0
r/c
r/c

ω
w1
w2
0
t
4π
φ
T
Fig. 9.3
Equivalent volumes of the support sets in the natural and transformed domains.
in Figure 9.3, of base 4(r/c)2 and of height . We then have
m(P) = 4πT,
(9.15)
m(Q) = 8
3c2 3r2.
(9.16)
By using (3.128) and (3.129), with
A =
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠, B =
⎛
⎝
ρ
0
0
0
ρ
0
0
0
ρ
⎞
⎠,
(9.17)
and using (9.15) and (9.16), it follows that the number of degrees of freedom in the
wide-band frequency regime is
Nϵ(TP) = 4πr2 3T
3c2π3 + o(3) as  →∞,
(9.18)
where  = ρ′ with ′ ﬁxed and ρ →∞. With the alternative scaling
A =
⎛
⎝
τ
0
0
0
1
0
0
0
1
⎞
⎠, B =
⎛
⎝
1
0
0
0
ρ
0
0
0
ρ
⎞
⎠,
(9.19)
we also have that the number of degrees of freedom over a ﬁxed frequency band for
large radiating systems and transmission time is
Nϵ(BQ) = 4πr2 3T
3c2π3 + o(r2T) as T,r →∞,
(9.20)
where T = τT′, r = ρr′, with T′,r′ ﬁxed and τ,ρ →∞.
Once again, the results are analogous to the ones obtained in Section 1.2.3. Namely,
(9.18) and (9.20) correspond to the rigorous derivation of (1.19).
9.2.3
General Rotationally Symmetric Domains
The results can be further generalized by considering a radiating system enclosed in a
convex domain bounded by a surface with rotational symmetry. Consider a cylindrical
coordinate system (r,φ,z), a closed analytic curve ζ = ζ(r,z) lying in the plane φ = 0
11
12:00:51, subject to the Cambridge Core

9.2 Radiation with Arbitrary Multiple Scattering
271
ζ(r,z)
r(z)
z
Fig. 9.4
Cut-set boundary of three-dimensional rotationally symmetric domain.
and symmetric with respect to the z axis, and the surface of revolution obtained by
rotating the curve about the z axis – see Figure 9.4. In this case, we can choose a pair of
coordinates (θ,φ) on the surface such that any meridian curve covers a 2π range, and
the spatial bandwidth along the meridian is constant and given by
w1 = ωℓ
2πc + o(ωℓ) as ωℓ→∞,
(9.21)
where ℓis the Euclidean length of the curve, normalized to the speed of light so that ωℓ
is dimensionless. This should be compared with the geometric constraint for circular
curves given in (9.12), where the value of ℓcorresponds to 2πr. In the same fashion,
any latitude line is a circle of radius r(z) that also covers a 2π range, and the spatial
bandwidth along this line is given by
w2 = ωr(z)
c
+ o(ωr) as ωr →∞.
(9.22)
It follows that in this case, while the support set P covers a solid angle 4π, the set Q
varies along z according to the meridian curve parametrization ζ = ζ(r,z), and we have
m(P) = 4πT,
(9.23)
m(Q) = 83
3c2
ℓ
2π
π
2

ζ
r(z)dz
= 83
3c2
ℓ
4π

ζ
πr(z)dz
= 83
3c2
A
4π ,
(9.24)
where A = A(S) is the surface area of the observation domain.
By (3.128) and (3.129), and using (9.23) and (9.24), we have that the number of
degrees of freedom in the wide-band frequency regime is
Nϵ(TP) = A3T
3c2π3 + o(3) as  →∞,
(9.25)
11
12:00:51, subject to the Cambridge Core

272
The Time–Frequency Domain
w
0
Q
w = ωr/c
ω
Δω
ωc
ω1
ω2
Fig. 9.5
Support set of a modulated signal in the transformed domain.
where  = ρ′ with ′ ﬁxed and ρ →∞. The analogous result is obtained by letting
the transmission time T = τT′, with T′ ﬁxed and τ →∞, and scaling all the coordinates
of the radiating volume, so that both ℓ= ρℓ′ and r = ρr′(z) tend to inﬁnity with ℓ′ and
r′ ﬁxed and ρ →∞. In this case, by (3.128) and (3.129) with the scaling in (9.19), and
using (9.23) and (9.24), we have
Nϵ(BQ) = A3T
3c2π3 + o(AT) as T,A →∞,
(9.26)
where A = ρ2A′, with A′ ﬁxed and ρ →∞.
9.3
Modulated Signals
We now extend the results to handle the case of modulated signals. Consider the case of
a real signal modulating a sinusoid of carrier frequency ωc, occupying a bandwidth
ω = [ω1,ω2] centered around ωc – see Figure 9.5. In the two-dimensional case,
following the same procedure as the previous sections, letting T = τT′, r = ρr′, and
ρ,τ →∞, we obtain
Nϵ(BQ) = T
π
2πr
cπ
(ω2
2 −ω2
1)
2
+ o(rT)
= Tω
π
2πrωc
cπ
+ o(rT)
(9.27)
as r,T →∞.
On the other hand, letting ω = ρω′, ωc = ρωc′, and ρ →∞, we have
Nϵ(BQ) = Tω
π
2πrωc
cπ
+ o(ωcω)
(9.28)
as ωc,ω →∞. The number of degrees of freedom is proportional to the
time-bandwidth product and to the cut-set boundary normalized by the radiating carrier.
For spherical domains, letting T = τT′, r = ρr′, and τ,ρ →∞, we have
Nϵ(BQ) = T
π
4πr2
c2π2
(ω3
2 −ω3
1)
3
+ o(Tr2)
11
12:00:51, subject to the Cambridge Core

9.4 Alternative Derivations
273
= Tω
π
4πr2
c2π2
(ω2
2 + ω2
1 + ω1ω2)
3
+ o(Tr2)
= Tω
π
4πr2ω2
c
c2π2 (1 + δ) + o(Tr2)
(9.29)
as T,r →∞, where δ is a constant that depends only on the ratio ω/ωc. Analogously,
letting ω = ρω′, ωc = ρωc′, ρ →∞, we have
Nϵ(BQ) = Tω
π
4πr2ω2
c
c2π2 (1 + δ) + o(ω2
cω)
(9.30)
as ωc,ω →∞.
Results (9.29) and (9.30) can also be combined using the scaling matrices
A =
⎛
⎝
τ
0
0
0
1
0
0
0
1
⎞
⎠, B =
⎛
⎝
β
0
0
0
ρβ
0
0
0
ρβ
⎞
⎠
(9.31)
and letting τβ,ρβ →∞, obtaining
Nϵ(BQ) = Tω
π
4πr2ω2
c
c2π2 (1 + δ) + o(r2ω2
cωT)
(9.32)
as rωc,ωT →∞.
Finally, for general rotationally symmetric domains, using (9.31) we have
Nϵ(BQ) = Tω
π
Aω2
c
c2π2 (1 + δ) + o(Aω2
cωT)
(9.33)
as Aω2
c,ωT →∞, where A = ρ2A′ with A′ ﬁxed.
For narrow-band signals, ω/ωc ≪1, and the constant δ can be made arbitrarily
small, so that the number of degrees of freedom in three dimensions is essentially given
by the ﬁrst term of (9.33), which is the natural extension of the single-frequency result
in Chapter 8, accounting for a non-zero frequency band around frequency ωc. It follows
that the number of degrees of freedom per unit time and per unit angular frequency band
is essentially given by
N0 = Aω2
c
c2π3 .
(9.34)
9.4
Alternative Derivations
Analogous results to the ones described in this chapter have been given in a less rigorous
setting in Section 1.2.3. In that case, we have simply integrated the number of spatial
degrees of freedom over the whole bandwidth. Although non-rigorous, that method
provides the right intuition at the basis of the rigorous computation. Each waveform is
composed of a spectrum of frequencies, and each frequency carries a number of degrees
of freedom that scales with the wavelength-normalized size of the cut-set boundary.
Since the wavelength is inversely proportional to the frequency, summing all degrees of
freedom in the two-dimensional setting corresponds to integrating a linear function of
11
12:00:51, subject to the Cambridge Core

274
The Time–Frequency Domain
the frequency, and in a three-dimensional setting corresponds to integrating a quadratic
function of the frequency, yielding the expressions in (9.13) and (9.14), (9.18) and
(9.20).
9.5
Summary and Further Reading
We have computed the number of degrees of freedom of bandlimited signals
propagating in arbitrary time-invariant media using the theory developed in Chapters 3
and 8. For signals observed along a spatial cut-set boundary that separates transmitters
and receivers, or between radiating elements and sensing devices in an electromagnetic
remote sensing system, the number of degrees of freedom corresponds to the
effective number of parallel channels available through the cut-set boundary in the
time–frequency and space–wavenumber domains. Thus, they provide a bound on the
amount of spatial and frequency multiplexing achievable using arbitrary technologies
and in arbitrary scattering environments. The number of degrees of freedom turns out to
be proportional to the time-bandwidth product, and to the area of the cut-set boundary
expressed in units of wavelengths. Results in this chapter appear in Franceschetti (2015),
extending the works of Bucci and Franceschetti (1987, 1989) to frequency-bandlimited
signals, and using the mathematical formulation of Landau (1975), described in
Chapter 3.
9.6
Test Your Understanding
Problems
9.1
Explain why the equivalent volumes in the natural and transformed domains
depicted in Figure 9.3 have measures equal to m(P) and m(Q).
9.2
How are the sets P and Q related to the equivalent volumes in Figure 9.3?
9.3
Check that the value of δ in (9.33) can be made arbitrarily small as ωc →∞.
9.4
Explain the difference between the argument given in Section 1.2.3 to compute
the total number of degrees of freedom and the one given in this chapter, and identify
what makes the former not completely rigorous.
11
12:00:51, subject to the Cambridge Core

10
Multiple Scattering Theory
My entire being rebels against order.
But without it I would die, scattered to the winds.1
10.1
Radiation with Multiple Scattering
In this chapter we illustrate how the physical constraints imposed by propagation in
complex environments limit the amount of information that can be transported by a
propagating wave.
We consider the stochastic diversity of signals propagating in a time-invariant random
medium and relate it to the parameters of the stochastic model used to describe the
medium. We rely on the stochastic frequency representations developed in Chapter 6,
and provide additional insights into the design trade-offs discussed in Chapter 7.
From a physical perspective, the general effect of multiple scattering is a damping
of the transmitted coherent wave and the creation of an incoherent energy coda –
see Figure 10.1. The term “coherent” is used to denote the part of the waveform
whose frequency components are not signiﬁcantly distorted by propagation ﬁltering,
so that the waveform retains the original transmitted shape. On the other hand, the term
“incoherent” is used to denote the part of the waveform whose frequency components
are signiﬁcantly distorted, so that the original transmitted shape is broadened in time.
The transfer of coherent energy into incoherent energy through multiple scattering, as
well as the absorption associated with the multiple-scattering process, are responsible
for the exponential attenuation of the coherent part of the response. The incoherent
response appears delayed and spread, due to the delayed arrival of the different
multiple-scattered contributions that combine at the receiver.
From an information-theoretic perspective, as the signal loses coherence due
to multiple scattering, it becomes more unpredictable in frequency, increasing the
stochastic diversity of the process used to model its frequency variation. From a
practical perspective, as the coherence bandwidth decreases, communication using a
sequence of short pulses is somewhat inhibited by the multiple scattering process,
due to the overlap of the broadened pulses at the receiver. This generally limits the
1 Albert Camus (1950). Notebooks, 1942–1951, Volume 2. English translation by J. O’Brien (2010),
Ivan R. Dee.
12
12:02:41, subject to the Cambridge Core

276
Multiple Scattering Theory
t
COHERENT
INCOHERENT
Td
Ts
TRANSMITTED
RECEIVED
Fig. 10.1
Impulse response consists of a coherent part and an incoherent part. The incoherent part is
delayed by Td, the overall response is spread by Ts.
performance of time multiplexing technologies. On the other hand, scattering allows for
smaller frequency correlation intervals, and favors technologies that use tightly packed
frequency pulses that are more spread out in time.
Historically, there are two approaches to studying these phenomena. One is multiple
scattering theory proper, and the other is transport theory. Both theories originated
in the 1950s and 60s. At the basis of multiple scattering theory is the Fredholm
integral equation of the second kind describing the multiple scattering process for
the ﬁeld. This approach is rigorous, but complete solutions cannot be obtained for
complex environments, and approximate solutions that are useful for a speciﬁc range
of parameters are considered. On the other hand, transport theory deals directly with the
transport of energy through a scattering medium using the equation of transfer, which
is equivalent to the Maxwell–Boltzmann collision equation used in the kinetic theory
of gases, without considering wave propagation effects. In this chapter, we describe
these two classical theories and relate them to a more recent probabilistic model of
propagation based on random walk theory.
10.1.1
The Basic Equation
We start by considering a single-frequency scalar signal propagating in an environment
ﬁlled with an arbitrary distribution of scattering objects. At the basis of any multiple
scattering formulation is the inhomogeneous Fredholm integral equation of the second
kind,
E(r) = ED(r) +

V′ K(r,r′)E(r′)dr′,
(10.1)
where ED is the direct ﬁeld due to the sources, K(r,r′) is a propagation kernel
relating the total ﬁeld on the scatterers to the reradiated ﬁeld at any point in the space
surrounding them, and the integral is over the space V′ occupied by the scattering
12
12:02:41, subject to the Cambridge Core

10.1 Radiation with Multiple Scattering
277
elements. For an electric ﬁeld, this is the scalar version of (8.97), showing a linear
relationship between the direct and the total ﬁeld. The equation can be solved iteratively,
as described in Appendix A.5.2, yielding the successive approximations
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
E0(r) = ED(r),
E1(r) = ED(r) +

V′ K(r,r′)ED(r′)dr′,
E2(r) = ED(r) +

V′ K(r,r′)ED(r′)dr′
+

V′

V′ K(r,r′)K(r′,r′′)ED(r′′)dr′′dr′,
E3(r) = ···
...
(10.2a)
(10.2b)
(10.2c)
(10.2d)
In (10.2a)–(10.2c), E0 accounts for the direct ﬁeld component only, E1 approximates
the ﬁeld up to single scattering, E2 up to double scattering, and so on.
It is in principle possible to obtain a solution using this iterative procedure up to
any desired level of accuracy, but this requires complete knowledge of the scatterers’
geometry and of the kernel K(r,r′) relating the ﬁeld at the scatterers to the reradiated
ﬁeld. In principle, this kernel can be obtained using knowledge of the constitutive
properties of the scatterers in terms of the conductivity σ(r′) and permittivity ϵ(r′)
at any point r′ ∈V′. By (4.31), one could compute the induced current and then
use the free-space Green’s function to compute the scattered ﬁeld at any point in
space. This corresponds to solving (8.97). In practice, however, complete knowledge
of the scatterers’ properties cannot be obtained for all environments, and approximate
solutions must be considered. An alternative approach is to model the environment
as a random medium, speciﬁed by a small set of parameters, and derive results for
the average ﬁeld over multiple spatial realizations. We describe this approach next,
starting with the analysis of sinusoidal sources and then extending the treatment to pulse
propagation in random media.
10.1.2
Multi-path Propagation
For a single-frequency signal, we have encountered a ﬁrst model of multiple scattering
in Section 6.3, namely the random multi-path model of propagation. The received signal
was modeled as the linear superposition of different signals traveling along different
multiple-scattered paths from transmitter to receiver. Each path carries a random phase
shift and a random attenuation, and summing all of them we argued that the amplitude
of the received waveform must be scaled by a Rayleigh random variable of parameter
σ, and the phase must be shifted by a uniform random variable in the interval [−π,π].
This leads to a model that multiplies the transmitted signal by a complex, circularly
symmetric, Gaussian random variable of standard deviation σ to obtain the received
signal. Fixing σ corresponds to ﬁxing the average attenuation, or path loss, between
12
12:02:41, subject to the Cambridge Core

278
Multiple Scattering Theory
Vn
rn
O
r
V
Fig. 10.2
Multiple scattering conﬁguration.
transmitter and receiver. In the presence of absorption at all scattering points, we also
argued that the path loss must rapidly decrease with the distance between transmitter
and receiver.
We then extended the multi-path model from the sinusoidal regime to propagation
of bandlimited signals, introducing the concepts of coherence bandwidth and power
delay proﬁle. In this case, we argued that multiple scattering increases the amount of
frequency diversity by decreasing the coherence bandwidth, and it increases the time
spread of the received waveform. By using multiple scattering theory, it is possible
to study these phenomena in more detail, and relate them to the parameters of the
stochastic model used to describe the propagation environment.
10.2
Multiple Scattering in Random Media
The theory of multiple scattering in random media starts with a discrete formulation
and obtains an integral equation for the average ﬁeld. Consider a discrete ensemble of
N scattering elements {V1,V2,...,VN} placed at coordinates {r1,r2,...,rN} with rN ∈V,
as depicted in Figure 10.2. The total ﬁeld at coordinate r in the space surrounding the
scatterers is
E(r) = ED(r) +
N

n=0
KnED(r) +
N

n=1
N

m=1
m̸=n
KnKmED(r)
+
N

n=1
N

m=1
m̸=n
N

l=1
l̸=m
KnKmKlED(r) + ··· ,
(10.3)
where
KnED(r) =

Vn
Kn(r,r′)ED(r′)dr′
(10.4)
12
12:02:41, subject to the Cambridge Core

10.2 Multiple Scattering in Random Media
279
Vn
Vm
Vl
ED
Vm
Vn
ED
(a)
(b)
Fig. 10.3
Triple scattering involving (a) three distinct scatterers, and (b) only two scatterers.
indicates the ﬁeld at point r due to the scatterer Vn centered at rn, when a ﬁeld ED(r′) due
to the sources only is present at the scatterer, and it depends on the position as well as on
the characteristics of the scatterer. In (10.3), the ﬁrst term is the direct ﬁeld component
due to the sources, the second term represents all the single scattering contributions,
the third term represents all the double scattering contributions, and so on. In the third
summation the terms m = n and l = m are excluded, but the term l = n is not. This means
that the summation includes triple scattering, where the propagation path goes through
the same particle more than once. We can then rewrite the triple scattering summation
to indicate a component that involves three distinct scatterers Vn,Vm,Vl, as depicted in
Figure 10.3(a), and a component that involves only two scatterers Vn and Vm, as depicted
in Figure 10.3(b), and we have
N

n=1
N

m=1
m̸=n
N

l=1
l̸=m
KnKmKlED(r) =
N

n=1
N

m=1
m̸=n
N

l=1
l̸=m
l̸=n
KnKmKlED(r)
+
N

n=1
N

m=1
m̸=n
KnKmKnED(r).
(10.5)
In general, the complete ﬁeld can be divided into a contribution of all the
multiple-scattered waves involving paths of distinct scatters, and a contribution
involving scatterers more than once.
The theory of multiple scattering developed by American mathematical physicist
Victor Twersky in the 1950s includes only the ﬁrst contribution, and neglects the second
contribution. This corresponds to considering the ﬁeld
E(r) = ED(r) +
N

n=0
KnED(r) +
N

n=1
N

m=1
m̸=n
KnKmED(r)
+
N

n=1
N

m=1
m̸=n
N

l=1
l̸=m
l̸=n
KnKmKlED(r) + ··· .
(10.6)
For large values of N, Twersky’s approximation includes almost all multiple-scattering
paths. Since an M-fold summation for M > 2 in the exact expansion contains N(N −
1)M−1 terms, while in the approximate expansion contains N!/(N −M)! terms, the ratio
12
12:02:41, subject to the Cambridge Core

280
Multiple Scattering Theory
between the number of terms in the approximation and in the exact expansion tends
to one, and this suggests that the approximation should give accurate results when the
number of scatterers is large.
The approximation is also useful to compute the average ﬁeld over a random location
of the scatterers. In this case, we have to average the ﬁrst sum in (10.6) over the random
location of one scatterer, the second sum over the random location of two scatterers,
and so on. Letting p(r) be the probability density function of the coordinate where a
scatterer is located, and assuming the locations of the scatterers are independent and
identically distributed, we have
E[E(r)] = ED(r) + N

V
K1ED(r)p(r1)dr1
+ N(N −1)

V
K1K2ED(r)p(r1)p(r2)dr1dr2
+ N(N −1)(N −2)

V
K1K2K3ED(r)p(r1)p(r2)p(r3)dr1dr2dr3
+ ··· ,
(10.7)
where the integrals are over the whole space where the scatterers are distributed. Letting
p(r) = ρ(r)/N, where ρ is a density function indicating the number of scatterers per unit
volume and whose integral over the whole space equals N, we have
E[E(r)] = ED(r) +

V
K1ED(r)ρ(r1)dr1
+ (N −1)
N

V
K1K2ED(r)ρ(r1)ρ(r2)dr1dr2
+ (N −1)(N −2)
N2

V
K1K2K3ED(r)ρ(r1)ρ(r2)ρ(r3)dr1dr2dr3
+ ··· ,
(10.8)
which for large values of N is approximately
E[E(r)] = ED(r) +

V
K1ED(r)ρ(r1)dr1
+

V
K1K2ED(r)ρ(r1)ρ(r2)dr1dr2
+

V
K1K2K3ED(r)ρ(r1)ρ(r2)ρ(r3)dr1dr2dr3
+ ··· .
(10.9)
The obtained relation (10.9) is the iterated version of the integral equation for the
average ﬁeld,
E[E(r)] = ED(r) +

V
Kr′ E[E(r)]ρ(r′)dr′,
(10.10)
12
12:02:41, subject to the Cambridge Core

10.2 Multiple Scattering in Random Media
281
where the operator Kr′ indicates the response due to a scatterer at location r′, and the
integral in (10.10) is over all possible locations of the scatterer. This should be compared
with the basic equation (10.1) obtained in the deterministic setting. While in (10.1) the
integral is over the space V′ occupied by the scattering elements, in (10.10) the integral
is over the whole space V and it indicates the contribution of the average scatterer.
It follows that Twersky’s approach corresponds to ﬁrst determining the response for
a single scatterer, and then computing the statistical spatial average for an ensemble
of scatterers. Assuming a simple form of the scattering operator Kr′, this approach
leads to closed-form solutions. On the other hand, for complex scattering scenarios
the method of successive approximations must be employed, neglecting higher-order
scattering contributions.
10.2.1
Born Approximation
The Born approximation, named after German physicist Max Born, corresponds to
considering only single scattering events. This is equivalent to taking the direct ﬁeld
due to the sources in place of the total ﬁeld on the right-hand side of (10.1) or (10.10),
and is the ﬁrst-order approximation for the resolvent identity of the integral equation.
In the case of (10.1), we have
E(r) = ED(r) +

V′ K(r,r′)ED(r′)dr′.
(10.11)
On the other hand, for (10.10) we have
E[E(r)] = ED(r) +

V
Kr′ E[ED(r)]ρ(r′)dr′.
(10.12)
Similarly, the nth-order Born approximation corresponds to the nth approximation, and
the zeroth order corresponds to ED(r).
10.2.2
Complete Solutions
The Twersky equation (10.10) can be solved directly by assuming a simple form for the
operator Kr′ relating the ﬁeld at the scatterers to the reradiated ﬁeld.
Consider a plane wave ED(z) = exp(jβz), where β = ω/c is the wavenumber,
propagating along the z axis and impinging on a random medium composed of a
constant density ρ of scattering elements ﬁlling the whole space z > 0, as depicted
in Figure 10.4.
We assume that the observation point at coordinate r inside the medium is in the
far ﬁeld of all the scatterers. Since the geometry of the medium and of the impinging
wave are independent of the x and y coordinates, the average ﬁeld inside the medium
depends on z only. The average ﬁeld propagates along the ¯z direction, since the average
scattering contributions in the positive and negative directions of the x and y axes cancel
each other. We then have, from (10.10),
E[E(z)] = exp(jβz) +

z>0
Kr′ E[E(z)]ρdr′.
(10.13)
12
12:02:41, subject to the Cambridge Core

282
Multiple Scattering Theory
z
ED
r′
r
z
z′
O
s
Fig. 10.4
A plane wave impinges on a random medium of a constant density of scatterers.
Since the observation point is assumed to be in the far ﬁeld of all the scatterers, letting
¯s be the unit vector in the direction of (r −r′), we can approximate the scattering
operator by
Kr′ E[E(z)] = γ exp(jβ|r −r′|)
|r −r′|
E[E(z′)],
(10.14)
where γ (¯z,¯s) is a complex scattering coefﬁcient that depends on the characteristics
of the scatterer. With this choice, the total average scattered ﬁeld at depth z inside
the medium due to a scatterer at r′ = (x′,y′,z′) is proportional to the average ﬁeld at
depth z′; it undergoes a phase shift proportional to the path length |r −r′|, a geometric
attenuation inversely proportional to |r−r′|, and is multiplied by an additional scattering
coefﬁcient γ accounting for the absorption by the scatterer and for an additional phase
shift. Substituting (10.14) into (10.13), we get
E[E(z)] = exp(jβz) +
 ∞
−∞
dx′
 ∞
−∞
dy′
 ∞
0
γ exp(jβ|r −r′|)
|r −r′|
E[E(z′)]ρdz′.
(10.15)
The integrals with respect to x′ and y′ can be computed approximately using the method
of stationary phase – see Appendix C.2 The stationary point is given by x′ = x, y′ = y,
and for these coordinate values the scattering coefﬁcient becomes
"
γ (¯z,¯s) = γ (¯z, ¯z)
if z > z′,
γ (¯z,¯s) = γ (−¯z, ¯z) if z < z′.
(10.16)
Application of the integration method yields
E[E(z)] = exp(jβz) + 2πj
β ργ (¯z, ¯z)
 z
0
exp(jβ|z −z′|)E[E(z′)]dz′
+ 2πj
β ργ (−¯z, ¯z)
 ∞
z
exp(jβ|z′ −z|)E[E(z′)]dz′.
(10.17)
The second integral accounts for the backward scattering occurring for z′ > z and is
typically negligible compared to the ﬁrst integral accounting for the scattering in the
2 The method described in Appendix C can be extended to the stationary phase evaluation of a multiple
integral.
12
12:02:41, subject to the Cambridge Core

10.2 Multiple Scattering in Random Media
283
forward direction. Neglecting this term, we obtain
E[E(z)] = exp(jβz)
&
1 + 2πj
β ργ (¯z, ¯z)
 z
0
exp(jβz′)E[E(z′)]dz′
'
,
(10.18)
which is ﬁnally in a form that can be solved directly. Substituting
E[E(z′)] = exp(jKz′)
(10.19)
into (10.18), we obtain
K = β + 2πργ (¯z, ¯z)
β
.
(10.20)
Since K is a complex number that depends on γ (¯z, ¯z), it turns out that the plane
wave solution (10.19) attenuates as it propagates through the medium and the rate of
attenuation can be expressed in terms of the absorption and scattering cross sections of
the scattering elements.
10.2.3
Cross Sections
A wave hitting a scatterer has part of the incident power scattered in different directions,
part absorbed by the particle, and part passed through undisturbed. The scattered and
absorbed fractions are called the scattering cross section σs and absorption cross section
σa, respectively. The total cross section is σt = σs + σa. By expressing the scattering
coefﬁcient γ (¯z, ¯z) in terms of cross sections, the squared amplitude of the average ﬁeld
can be computed from (10.19) and (10.20), and we have
|E[E(z)]|2 = exp(−ρσtz).
(10.21)
A similar, but more involved, computation than the one in Section 10.2.2 also leads
to an approximate expression for the average of the squared amplitude of the ﬁeld:
E[|E(z)|2] = exp(−ρσaz).
(10.22)
By expressing the random ﬁeld as the sum of an average and a random ﬂuctuating
component,
E(z) = E[E(z)] + E′(z),
(10.23)
assuming E′(z) has zero mean, we have that the total average power is
E[|E(z)|2] = E[|E[E(z)] + E′(z)|2]
= |E[E(z)]|2 + E[|E′(z)|2],
(10.24)
and by combining (10.21), (10.22), and (10.24), the average ﬂuctuating power is
E[|E′(z)|2] = E[|E(z)|2] −|E[E(z)]|2
= exp(−ρσaz) −exp(−ρσtz).
(10.25)
Equations (10.21) and (10.25) represent the coherent and incoherent responses for
a given radiated sinusoidal waveform, respectively. Equation (10.22) represents the
12
12:02:41, subject to the Cambridge Core

284
Multiple Scattering Theory
z
0
Fig. 10.5
Total (top continuous line), coherent (bottom continuous line), and incoherent (dashed line)
response for plane wave propagation in a random medium as a function of the propagation
depth.
total response. The coherent response exponentially attenuates due to absorption and
scattering. The total response exponentially attenuates due to absorption only, and if
σa = 0 there is no power attenuation. These results are consistent with plane wave
propagation in a homogeneous medium with absorption as described in Chapter 4. The
incoherent response initially increases for short propagation depth due to the transfer
of coherent energy into incoherent energy through scattering, but eventually decreases
as the waveform penetrates more into the medium and absorption becomes dominant –
see Figure 10.5. These effects are also apparent using alternative models of multiple
scattering, based on random walk theory and transport theory, and can also be extended
to the case of pulse propagation in random media.
10.3
Random Walk Theory
The results obtained using multiple scattering theory essentially predict an exponential
attenuation of the average power as a plane wave propagates into the medium. The rate
of attenuation depends on the density of the scatterers and on their absorption cross
section. The conversion of coherent energy into incoherent energy is governed by the
scattering cross section and is responsible for a slower rate of decay of the total response
compared to the coherent response. When scatterers are non-absorbing, the rate of decay
of the average power tends to zero and the plane wave propagates without attenuation
into the medium.
These results can be revisited using a simple model of wave propagation based on
continuous random walks that is closely related to transport theory. In this case, we
consider a sinusoidal wave of a given frequency radiated at the origin of a space ﬁlled
by a uniform random distribution of point scatterers. The radiating wave is modeled as
a constant stream of photons propagating in the environment, each carrying a quantum
of energy. Each radiated photon proceeds along a straight line for a random length, until
it hits an obstacle. The photon is then either absorbed by the obstacle, with probability
(1 −α), or it is scattered uniformly in a random direction, with probability α. In this
12
12:02:41, subject to the Cambridge Core

10.3 Random Walk Theory
285
r
Fig. 10.6
Random walk model. One photon path from a single radiating element is depicted.
way, each photon propagates in the environment according to a random piecewise linear
trajectory due to scattering – see Figure 10.6.
For a uniform distribution of obstacles, the path length between successive collisions
is an exponential random variable of parameter η, indicating the number of scatterers
per unit length, so that 1/η is the mean free path, and the probability density function
(pdf) for the length of each step is
q(r) = ηexp(−ηr).
(10.26)
Since each step is taken in a direction uniform in [0,2π], the pdf of having the ﬁrst
collision at coordinate r depends only on the distance r from the origin, and we have
⎧
⎪⎪⎨
⎪⎪⎩
p(r) = p(r) =
η
2πr exp(−ηr),
p(r) = p(r) =
η
4πr2 exp(−ηr),
(10.27)
in two and three dimensions, respectively. These pdfs integrate to one over the plane
and over the space:
 2π
0
dφ
 ∞
0
r η
2πr exp(−ηr)dr = 1,
(10.28)
 2π
0
dφ
 π
0
sinθdθ
 ∞
0
r2 η
2πr exp(−ηr)drdθdφ = 1.
(10.29)
We now consider the probability density g of the photon’s absorption at coordinate r.
Once again, this depends only on the distance r from the origin, so that g(r) = g(r), and
it satisﬁes the integral equation
g(r) = (1 −α)p(r) + α
 r
0
g(r −r′)p(r′)dr′,
(10.30)
12
12:02:41, subject to the Cambridge Core

286
Multiple Scattering Theory
indicating that the photon is either absorbed in the ﬁrst step, with probability (1 −
α), or is scattered in a random direction with probability α. In the latter case, the
resulting position is the sum of two random vectors, given by the convolution of the
corresponding probability densities. The successive approximations constituting the
resolvent formalism for the integral equation are
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
g0(r) = (1 −α)p(r),
g1(r) = (1 −α)p(r) + α
 r
0
g(r −r′)p0(r′)dr′,
g2(r) = (1 −α)p(r) + α
 r
0
g(r −r′)p0(r′)dr′
+ α2
 r
0
 r
0
p(r −r′)g(r′ −r′′)p0(r′′)dr′′dr′,
g3(r) = ··· ,
...
(10.31a)
(10.31b)
(10.31c)
(10.31d)
where g0(r) describes the event that the photon is absorbed at the ﬁrst step of the random
walk, hitting a single obstacle; g1(r) describes the event that the photon is absorbed
either at the ﬁrst step or at the second; and so on.
For the given kernel (10.27), the integral equation (10.30) can be solved directly by
taking the Fourier transform of (10.30). Letting g(w) be the Fourier transform of g(r)
andq(w) be the Fourier transform of q(r), we have
g(w) = (1 −α)p(w) + αp(w)g(w)
(10.32)
and
g(w) = (1 −α)p(w)
1 −αp(w) .
(10.33)
The solution g(r) in the spatial domain can then be obtained by taking the inverse
transform of (10.33).
Using (10.27), exact expressions for g(r) can be obtained in the two-dimensional
case in terms of a series of Bessel polynomials. Useful closed-form approximations can
also be obtained in two and three dimensions in terms of exponential functions and a
modiﬁed Bessel function of the second kind. These are
⎧
⎪⎪⎨
⎪⎪⎩
g(r)
= (1 −α)η
2πr
,
exp

−(1 −α2)ηr

+ αηrK0
	
(1 −α2)ηr
-
,
g(r)
= (1 −α)η
4πr2
,
αηrexp(−
√
1 −α2ηr) + exp

−(1 −α2)ηr
-
,
(10.34)
where K0(·) indicates the modiﬁed Bessel function of the second kind and of order zero
– see Appendix E.3.
In both cases, as α →0 and the obstacles become perfectly absorbing, g(r) tends to
q(r) given by (10.26) and the random walk reduces to a single exponentially distributed
12
12:02:41, subject to the Cambridge Core

10.3 Random Walk Theory
287
O
r
Fig. 10.7
Net power ﬂux in the radial direction is the average number of photons per unit time absorbed
outside the sphere of radius r.
step. All expressions are valid probability densities as they integrate to one over the
plane, and over the space, respectively.
From these probabilistic results we can compute the average radiated power density
S(r) and the average full power density P(r) at a given distance r from the transmitter.
Referring to the three-dimensional case, integration of g(r) over the whole space
outside the sphere of radius r centered at the transmitter gives the probability of a
photon’s absorption outside the sphere. Multiplying this probability by the total number
of radiated photons we obtain the average net energy ﬂux in the radial direction and,
when this is taken per unit time, we obtain the average net power ﬂux – see Figure 10.7.
The radiated power density in units of W m−2 is then obtained by dividing the power ﬂux
by the surface of the sphere, and it corresponds to the expected value of the amplitude
of the Poynting vector of the radiated ﬁeld.
Modeling the sinusoidal source as radiating photons at a constant rate A, in two
dimensions we have
S(r) =
A
2πr
 2π
0
dφ
 ∞
r
r′g(r′)dr′,
(10.35)
where the factor A represents the transmitted power. Similarly, in three dimensions we
have
S(r) =
A
4πr2
 2π
0
dφ
 π
0
sinθdθ
 ∞
r
r′2g(r′)dr′.
(10.36)
On the other hand, the full power density is given by all the photons that enter per
unit time an elementary volume placed between radii r and r + dr from the source, and
coming from all directions. This takes into account not only the radiated power but also
the scattered power reﬂected towards the observation point from all directions – see
Figure 10.8. To compute the full power density we ﬁrst consider the probability density
g′(r) of having a scattering event at distance r from the origin. Since (1 −α) is the
12
12:02:41, subject to the Cambridge Core

288
Multiple Scattering Theory
r
dr
O
Fig. 10.8
Full power density is the average number of photons per unit time entering an elementary
volume between r and r + dr, coming from all directions.
conditional density of absorption given that a scattering event occurs, we have
g′(r) =
g(r)
(1 −α).
(10.37)
The conditional density of having a scattering event inside the elementary volume, given
that the photon reaches it, is given by
lim
r→0q(r) = η.
(10.38)
It follows that the probability density of a photon reaching distance r from the origin is
given by
g′(r)
η
=
g(r)
(1 −α)η.
(10.39)
Multiplying (10.39) by the total number of radiated photons gives the average energy
density at distance r from the origin, and for a constant rate A of radiated photons we
have that the average full power density is
P(r) =
Ag(r)
(1 −α)η.
(10.40)
In the absence of scattering, we expect the full power density and the radiated power
density to coincide, because all the power propagates in the radial direction. On the other
hand, since scattering can provide radiation from different directions, the two quantities
can generally be different.
12
12:02:41, subject to the Cambridge Core

10.3 Random Walk Theory
289
10.3.1
Radiated Power Density
Letting A = 1, combining (10.34) and (10.35), in two dimensions we can compute
S(r) =
1
2πr
 2π
0
dφ
 ∞
r
r′g(r′)dr′
=
1
2πr
(
1
1 + α exp
8
−(1 −α2)ηr
9
+

1 −α
1 + α αηrK1
	
1 −α2ηr
4
,
(10.41)
where K1(·) is the modiﬁed Bessel function of the second kind and of order one – see
Appendix E.3.
In the limit of no scattering (η →0) or scattering with no absorption (α →1), we
have
S(r) =
1
2πr,
(10.42)
which is what we expect for free-space propagation of a cylindrical wave, and what we
expect by conservation of power for propagation in a scattering environment with no
absorption.
In three dimensions, combining (10.34) and (10.36) we have
S(r) =
1
4πr2
 2π
0
dφ
 π
0
sinθdθ
 ∞
r
r′2g(r′)dr′
= (1 −α)η
4πr2

ηα
 ∞
r
r′ exp
,
−
	
1 −α2ηr′-
dr′ +
 ∞
r
exp
8
−(1 −α2)ηr′9
dr′
#
=
1
4πr2
1
(1 + α)
:
α
	
1 −α2ηr + 1

exp
,
−
	
1 −α2ηr
-
+ exp
8
−(1 −α2)ηr
9
.
(10.43)
In the limit of no scattering (η →0) or scattering with no absorption (α →1), we have
S(r) = 1/(4πr2). Again, this is what we expect on physical grounds for propagation of
a spherical wave in free space or for propagation in a scattering environment with no
absorption.
10.3.2
Full Power Density
Letting A = 1, combining (10.34) and (10.40), in two dimensions we have
P(r) =
1
2πr
,
exp

−(1 −α2)ηr

+ αηrK0
	
(1 −α2)ηr
-
,
(10.44)
and in three dimensions
P(r) =
1
4πr2
,
αηrexp

−
	
1 −α2ηr

+ exp

−(1 −α2)ηr
-
.
(10.45)
As expected, in the limit of no scattering (η →0) the full power density coincides
with the radiated power density, as it tends to 1/(2πr) and 1/(4πr2) in two and three
dimensions, respectively.
12
12:02:41, subject to the Cambridge Core

290
Multiple Scattering Theory
Table 10.1 Limiting cases for the full power, and radiated power densities.
Medium
2D
3D
α →0
Absorbing
P(r) = e−ηr/(2πr)
P(r) = e−ηr/(4πr2)
α →1
Lossless
P(r) = ∞
P(r) = 1/(4πr2) + η/(4πr)
η →0
Free space
1/(2πr)
1/(4πr2)
α →0
Absorbing
S(r) = e−ηr/(2πr)
S(r) = e−ηr/(4πr2)
α →1
Lossless
S(r) = 1/(2πr)
S(r) = 1/(4πr2)
η →0
Free space
S(r) = 1/(2πr)
S(r) = 1/(4πr2)
10.3.3
Diffusive Regime
The limiting cases of the random walk model of multiple scattering are summarized
in Table 10.1. As η →0 the model reduces to free-space propagation and the radiated
power density and full power density coincide. On the other hand, for η > 0 and as
α →1 the radiated power density in three dimensions tends to 1/(4πr2), satisfying the
conservation of power, while the full power density tends to
P(r) = 1/(4πr2) + η/(4πr).
(10.46)
This phenomenon of slow decay of the full power density is due to part of the energy
being reﬂected back towards the source; this is responsible for the second term in
(10.46), and it indicates that propagation tends to become diffusive.
Under the same conditions of a lossless scattering medium, in two dimensions the
full power density diverges. This follows from (10.44) because of the singular behavior
of K0(·) at the origin.
These phenomena can be explained in terms of the recurrence properties of random
walks. In two dimensions the random walk is recurrent. This means that in a lossless
scattering medium any photon’s trajectory revisits inﬁnitely often the same location in
space, and this leads to the divergence of the full power density. On the other hand, in
three dimensions the probability of revisiting the same location multiple times is less
than one, and in this case a lossless medium leads only to a decrease in the rate of decay
of the full power density from 1/r2 to 1/r, due to a ﬁnite number of recurrent visits and
the eventual diffusion of photons in space.
The diffusion effect becomes negligible in a medium with absorption, and power
attenuation in three dimensions tends to become of the type e−ηr/(4πr2), as α →0.
10.3.4
Transport Theory
The results for the random walk model can be compared with those from transport
theory. This theory deals with the transport of energy through a medium with scattering
elements. When the scatterers are assumed to be small compared to the wavelength,
the energy of the wave impinging on a scatterer is diffused uniformly in all directions.
In this case, the results from transport theory are similar to those for the random walk
model.
12
12:02:41, subject to the Cambridge Core

10.4 Path Loss Measurements
291
dB
3
2
1
0
–1
–2
–3
–4
–5
–6
  σtr
(a)
(b)
W0
r
3
2
1
0
–1
–2
–3
–4
–5
–6
dB
α
η
Fig. 10.9
Comparison of results from the random walk model of propagation (a), parametrized by
increasing values of the scattering probability α, with results from transport theory (b),
parametrized by increasing values of the albedo W0.
In Figure 10.9(a) we plot 4πr2S(r) (continuous line curves) and 4πr2P(r) (dashed
line curves) for the three-dimensional case, and compare them with the corresponding
quantities derived in transport theory and plotted in Figure 10.9(b). The parameters of
the two theories are different: the random walk model uses the scattering probability α
and the average step length (1/η), whereas transport theory uses the number of particles
per unit volume (ρ) and the absorption (σa), scattering (σs), and total (σt = σa + σs)
particle cross sections. In transport theory, the curves are parametrized by increasing
values of the albedo W0 = σs/σt, while the curves of the random walk model are
parametrized by increasing values of the scattering probability α. The plots are in
logarithmic scale (dB).
In both cases, the normalized full power density (dashed line) initially increases for
short propagation depths, but eventually decreases as the waveform penetrates more
into the medium and absorption become dominant. For strongly absorbing scatterers
the power density attenuates exponentially. On the other hand, the normalized radiated
power density (continuous line) for lossless scatterers tends to the 0 dB line. Since we
have normalized the ﬂux by the geometric attenuation coefﬁcient 1/4πr2, the situation
in this case is similar to that of a plane wave that propagates without attenuation.
10.4
Path Loss Measurements
We now summarize. Propagation losses due to multiple scattering of single-frequency
signals can be described by an exponential power attenuation, due to absorption, times
a free-space geometric attenuation coefﬁcient. This suggests using an approximate path
12
12:02:41, subject to the Cambridge Core

292
Multiple Scattering Theory
loss formula,
L(r) = Aexp(−γ r)
4πr2
,
(10.47)
indicating the attenuation of the power density measured at distance r from the
transmitter. The parameter γ = γ (α,η) accounts for both scattering and absorption in
the medium. The parameter A accounts for the transmitted power. The model indicates
a smooth transition from free-space propagation conditions in the near ﬁeld (γ r ≪1),
to exponential attenuation in the far ﬁeld (γ r ≫1). For weakly absorbing scatterers, the
model can also be heuristically extended to include a diffusive component,
L(r) = γ Aexp(−γ r)
4πr2
+ (1 −γ )A
4πr
.
(10.48)
These simpliﬁed formulas can be used to predict attenuation when transmitting
and receiving antennas are immersed in environments containing a multitude of small
scattering objects. The diffusive effect is more evident in indoor environments, due to
reverberation effects, and is often negligible outdoors.
Figure 10.10 shows the ﬁt of the theoretical model (10.47), expressed in dB, to
outdoor experimental data collected in the center of Rome, Italy, for sinusoidal radiation
at 900 MHz over distances up to 350 m and an antenna height of 1.5 m. Data points
refer to local space–time averages of the power received at different locations at a given
distance from the transmitter. The mean squared error over all sampled points is 3–4 dB.
Similar results are obtained by ﬁtting the full power density model (10.45) using values
of η = 0.13 and α = 0.9, corresponding to an average inter-obstacle distance of 13 m
and an absorption coefﬁcient of 10%. These are typical values for measurements taken
with low antennas immersed in an urban scattering environment.
10.5
Pulse Propagation in Random Media
We now extend the results for the random walk model to the case of propagation of
a general waveform rather than a sinusoidal one. In this case, rather than a constant
stream of photons, we consider a density of photons f(t) radiated at the origin of the
space, each carrying a quantum of energy. We wish to determine the expected density of
photons h(r,t) at distance r from the origin, according to the random walk model. This
corresponds to the expected power response over multiple realizations of the scattering
environment, and is obtained by computing a path integral, namely by averaging the
number of photons reaching the observation point over all possible multiple-scattered
paths of total length ℓ≥r.
10.5.1
Expected Space–Time Power Response
We perform the computation in a two-dimensional setting and start considering the joint
pdf of having the ﬁrst scattering event at coordinate r, having traveled a distance ℓ. This
12
12:02:41, subject to the Cambridge Core

10.5 Pulse Propagation in Random Media
293
0
50
100
150
200
250
300
350
–20
–30
–40
–50
–60
–70
–80
–90
–100
–110
]
B
d
[ re
w
o
P
[m]
Fig. 10.10
Path loss model ﬁtted to experimental data.
depends only on the distance r from the origin, so that p(r,ℓ) = p(r,ℓ), and, using
(10.27) and Dirac’s δ(·), it is written as
p(r,ℓ) =
η
2πr exp(−ηr)δ(ℓ−r).
(10.49)
The position after (n + 1) steps, and the total path length traveled, are given by the
sum of (n + 1) i.i.d. random vectors. It follows that the joint pdf of having the (n + 1)th
collision at coordinate r, having traveled a total path length ℓ, can be computed by
performing n convolution operations of the single steps, and we have
pn(r,ℓ) =
n
;
<=
>
p(r,ℓ) ∗p(r,ℓ) ∗··· ∗p(r,ℓ).
(10.50)
The n-fold convolution can be computed by taking the product of n + 1 Fourier
transforms of p(r,ℓ),
pn(r,ℓ) = F−1[Fp]n+1,
(10.51)
where the index n = 0,1,... indicates the number of scattering events, n+1 the number
of steps, and the Fourier transform in spherical coordinates is
Fp(w,χ) =
 2π
0
dθ
 ∞
0
rdr
 ∞
0
p(r,ℓ)e−jrwcosθe−jχℓdℓ,
(10.52)
where (w,χ) are variables in the transformed domain corresponding to (r,ℓ).
12
12:02:41, subject to the Cambridge Core

294
Multiple Scattering Theory
Letting U(·) be the Heaviside step function and $(·) the Gamma function, it is
possible to compute (10.51) in closed form and obtain
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
p0(r,ℓ) =
η
2πr exp(−ηr)δ(ℓ−r),
pn(r,ℓ) = η
π exp(−ηℓ)
2√πηn
2n−1$
n + 1
2

$
n
2
(ℓ2 −r2)(n−2)/2U(ℓ−r),
(10.53)
where the ﬁrst equation coincides with (10.49) and is for one step, and the second
equation is for n > 0 steps.
We now consider the joint pdf of reaching coordinate r in any number of steps, having
traveled a total path length ℓ, and having a collusion there. This is given by
g(r,ℓ) =
∞

n=0
αnpn(r,ℓ).
(10.54)
Dividing (10.54) by the conditional density of having a scattering event at coordinate
r given that the photon reaches it, which is given by (10.38), we get the joint pdf of
reaching coordinate r in any number of steps, having traveled a total path length ℓ.
Finally, letting c be the velocity of the photons, we compute the expected response:
h(r,t) =
 ct
r
g(r,ℓ)
η
f(t −ℓ/c)dℓ
 ct
r
∞

n=0
αn
η pn(r,ℓ)f(t −ℓ/c)dℓ
 ct
r
(
αn
η p0(r,ℓ) +
∞

n=1
αn
η pn(r,ℓ)
4
f(t −ℓ/c)dℓ,
(10.55)
where the integration limits are imposed by the geometric constraints r ≤ℓ≤ct. The
series in (10.55) can be computed, and we get
h(r,t) = exp(−ηr)
2πr
f(t −ℓ/c) + αη
2π
 ct
r
f(t −ℓ/c)
exp
,
−η

ℓ−α
√
ℓ2 −r2
-
√
ℓ2 −r2
dℓ.
(10.56)
In the case that the transmitted power density is a Dirac impulse, photons reach the
observation point at time t = r/c along the line-of-sight path, and at time t > r/c along
multiple-scattered paths. It follows that the expected impulse response density is
hδ(r,t) = exp(−ηr)
2πr
δ (t −r/c) + cαη
2π
exp
,
−η

ct −α
	
(ct)2 −r2
-
	
(ct)2 −r2
U(t −r/c),
(10.57)
where the units are photons m−1 s−1.
12
12:02:41, subject to the Cambridge Core

10.5 Pulse Propagation in Random Media
295
.0005
20
40
60
80
.001
.015
t
h δ (r,t)
(2)
.001
.002
.003
20
40
60
80
t
r
h δ (r,t)
(2)
(a)
(b)
η
Fig. 10.11
(a) Multiple-scattered part of the impulse response with c = 1, α = 1, r = 10, and η ∈[0.5,0.8].
(b) Multiple-scattered part of the impulse response with c = 1, α = 1, η = 1, and r ∈[6,9].
The ﬁrst term of (10.57),
h(1)
δ (r,t) = exp(−ηr)
2πr
δ (t −r/c),
(10.58)
represents the response due to the single direct line-of-sight path, and the second term,
h(2)
δ (r,t) = cαη
2π
exp
,
−η

ct −α
	
(ct)2 −r2
-
	
(ct)2 −r2
U(t −r/c),
(10.59)
represents the response due to multiple scattering, including the multiple-scattering
straight-line path and paths with different angles and directions. It follows that the direct
part of the response is an exponentially attenuated pulse, while the multiple-scattered
part, depicted in Figure 10.11, consists of an initial fast-decaying peak, followed
by a broadened tail, due to the delayed arrivals of the different multiple-scattering
contributions. This tail exhibits a larger spread and is delayed as η or r are increased, and
the signal undergoes a larger amount of scattering. The initial, fast-decaying peak in the
ﬁgure is due to the singularity of (10.59) at t = r/c, which is also the time at which the
direct line-of-sight contribution (10.58) arrives. Physically, it represents the combined
effect of all the multiple-scattering contributions occurring along the straight-line path
between the transmitter and receiver.
A closer inspection reveals that the energy associated with the paths occurring
along a single straight line tends to disappear as the amount of scattering increases,
and the coherent energy associated with the straight-line paths is converted into an
incoherent energy coda associated with the non-line-of-sight paths. This is evident from
Figure 10.12, showing a steeper decrease of the initial peak of the response as η is
increased, indicating that the area under the curve becomes negligible as the amount of
scattering increases and the non-line-of-sight paths become dominant.
12
12:02:41, subject to the Cambridge Core

296
Multiple Scattering Theory
.0012
h δ (r,t)
.0008
(2)
.0016
10
40
20
30
50
.0004
η=.3
η=.4
η=.5
η=.6
Fig. 10.12
Multiple-scattered part of the impulse response with c = 1, α = 1, r = 10. The line-of-sight
contribution disappears as the impulse response becomes more spread out.
10.5.2
Random Walk Interpretation
We now provide an interpretation of the main features of the impulse response in terms
of the properties of the random walk. The average distance traveled from the origin
in our random walk model grows with the square root of the number of reﬂections. It
follows that after n reﬂections the distance from the origin is roughly (1/η)√n, where
1/η is the typical step length of the random walk. Neglecting absorption, a photon
undergoes roughly T/τ reﬂections in a time interval of duration T, where τ = 1/(ηc) is
the typical propagation time between successive reﬂections. Accordingly, the distance
from the origin of a multiple-scattered path after time T is of the order of
r = 1
η
√n =

cT
η .
(10.60)
The same distance r is reached by the direct line-of-sight path in time r/c. It follows
that the time lag between the non-line-of-sight contribution and the line-of-sight one is
t = T −r
c
= ηr2
c −r
c
+ r
c(ηr −1).
(10.61)
This heuristic computation shows that the time delay increases with the distance r and
with the amount of scattering η. On the other hand, the time spread is related to the
variance of the distance traveled by the random walk, which grows with the number of
reﬂections n. For this reason, the incoherent contribution not only appears delayed, as
the distance increases, but also more spread out in time.
12
12:02:41, subject to the Cambridge Core

10.5 Pulse Propagation in Random Media
297
10.5.3
Expected Space–Frequency Power Response
Taking the Fourier transform of (10.57), we obtain the expected space–frequency
response density
H(r,ω) =
 ∞
−∞
hδ(r,t)exp(−jωt)dt,
(10.62)
where the units are photons m−1. This is composed of two factors,
H(r,ω) = H1(r,ω) + H2(r,ω),
(10.63)
where
H1(r,ω) = exp[−ηr(1 + jω/(ηc))]
2πr
(10.64)
represents the direct line-of-sight part of the response, which is exponentially attenuated
and phase shifted by ωr/c, and
H2 = cαη
2π
 ∞
−∞
exp
,
−η

ct −α
	
(ct)2 −r2
-
	
(ct)2 −r2
U(t −r/c)exp(−jωt)dt
= αη
2π
∞

r
∞

r/c
δ (t −ℓ/c)
exp
,
−η

ℓ−α
√
ℓ2 −r2
-
√
ℓ2 −r2
exp(−jωt)dtdℓ
(10.65)
represents the multiple-scattered part of the response. Expanding the exponential
function in a Taylor series around ℓ2 −r2 = 0, and integrating term by term, changing
the integration variable to u = ℓ/r, the incoherent response (10.65) can be written in
terms of the series
H2 = αη
2π
 ∞
r
exp[−ℓ(η + jω/c)]exp
,
αη
√
ℓ2 −r2
-
√
ℓ2 −r2
dℓ
= αη
2π
 ∞
r
exp[−ℓ(η + jω/c)]
∞

n=0
(αη)n(ℓ2 −r2)
n−1
2
n!
dℓ
= αη
2π
∞

n=0
(αηr)n
n!
 ∞
1
exp[−ur(η + jω/c)](u2 −1)
n−1
2 du
= αη
2π
∞

n=0
αn
$( n
2 + 1)2n/2
(ηr)n/2
(1 + jω/(ηc))n/2 Kn/2[ηr(1 + jω/(ηc))],
(10.66)
where Kn(·) is the modiﬁed Bessel function of the second kind of order n, and $(·) is the
Gamma function. This series can be numerically evaluated or analytically approximated
using asymptotic expansions of the Gamma function and the modiﬁed Bessel function.
Its magnitude is a decreasing function of ω > 0, whose rate of decay depends on the
scattering density, absorption, and distance from the transmitter.
The frequency response in (10.66) also includes straight-line contributions occur-
ring along multiple-scattered paths. To evaluate the frequency response of the
12
12:02:41, subject to the Cambridge Core

298
Multiple Scattering Theory
=.5
=1
=.5
=1
ω
t
×10–4
2
4
6
8
10
12
20
40
60
80
100
120
140
.001
.01
.1
–.04
–.02
0
.02
.04
H 2(r,ω)
h δ (r,t)
(2)
η
η
η
η
Fig. 10.13
Incoherent part of the impulse response without line-of-sight contribution and with c = 1, α = 1,
r = 10.
non-line-of-sight components only, we can numerically compute the Fourier transform
of the impulse response with the initial peak removed, as shown in Figure 10.13. As the
amount of scattering increases and the signal loses coherence and is broadened in time,
the ﬁgure shows that the energy concentrates in a smaller frequency band. As we shall
see below, the expected space–frequency power response of the random walk model
corresponds to the frequency autocorrelation function of a multi-path channel model,
and this behavior indicates a decrease of the coherence bandwidth of the signal due to
scattering.
10.5.4
Correlation Functions
The time autocorrelation of the received signal in a multi-path channel describing a
time-invariant environment having stochastic Green’s function G(r,τ) is
E[Y2(r,t)] =
 ∞
−∞
 ∞
−∞
x(r,t −τ)x(r,t −τ ′)E[G(r,τ)G(r,τ ′)]dτdτ ′.
(10.67)
In Chapter 6, we deﬁned the uncorrelated scattering multi-path channel to be the one for
which the correlation between wave contributions along paths carrying different delays
is zero. This also corresponds to a frequency wide-sense stationary model, as described
in Section 6.3.1. It follows that for this channel model, the correlation function between
the impulse responses G(r,τ) and G(r,τ ′) is
E[G(r,τ)G(r,τ ′)] = A(r,τ)δ(τ ′ −τ),
(10.68)
indicating zero correlation unless the delays τ and τ ′ corresponding to different path
lengths are equal. In this case, A(r,τ) represents the average power attenuation at
coordinate r and at time t experienced by the signal along any path carrying a delay
τ = ℓ/c ≥r/c.
According to the random walk model, the average power attenuation corresponds to
the joint density of photons reaching coordinate r in any number of steps and having
12
12:02:41, subject to the Cambridge Core

10.6 Power Delay Profile Measurements
299
traveled a total path length ℓ= cτ, and is obtained by dividing the joint density (10.54)
by the conditional density (10.38), yielding
A(r,τ) =
∞

n=0
αn
η pn(r,τ).
(10.69)
Substituting (10.68) into (10.67), taking into account (10.69), and that r/c ≤τ ≤t,
we obtain
E[Y2(r,t)] =
 t
r/c
x2(t −τ)A(r,τ)dτ
=
 t
r/c
x2(t −τ)
∞

n=0
αn
η pn(r,τ)dτ.
(10.70)
It follows that we can interpret the average power attenuation A(r,τ) in (10.69) as
the response to a δ-input power x2(t) = δ(t), and comparing (10.70) with (10.55), with
the substitution τ = ℓ/c, it follows that the density of radiated photons f(t) corresponds
to the instantaneous power x2(t) of the transmitted signal, and the expected response
of the random walk model corresponds to the autocorrelation function of a frequency
wide-sense stationary, uncorrelated scattering multi-path channel model.
The quantity hδ(r,t) in (10.57) represents the power delay proﬁle, and is the inverse
transform of the frequency autocorrelation function; we have
E[G(r,ω′)G∗(r,ω′′)] = H(r,ω) =
 ∞
−∞
hδ(r,t)exp(−jωt)dt,
(10.71)
which for the given channel depends only on the difference ω = ω′′ −ω′ and is given by
(10.57).
It follows that the spreading properties of the random walk model and the creation
of an incoherent energy coda now reﬂect in the shape of the frequency autocorrelation
function and power delay proﬁle of the channel.
10.6
Power Delay Profile Measurements
We now summarize. The main features of the power delay proﬁle predicted by the
random walk model are a fast-rising coherent response, followed by the incoherent
response that exhibits a decaying tail. The two parts of the response tend to merge
as the distance between transmitter and receiver increases or the density of scattering
increases, and the incoherent response becomes dominant. Figure 10.14 illustrates this
point, showing results for the theoretical model in the case of a transmitted rectangular
pulse of width T, obtained by substituting f(t) = rect(t/T) into (10.56).
12
12:02:41, subject to the Cambridge Core

300
Multiple Scattering Theory
30T
100T
r = 30cT
r = 20cT
h(r,t)
h(r,t)
30T
r = 15cT
h(r,t)
Fig. 10.14
Power response for a rectangular pulse of support T with α = 0.9 and η = 0.3cT.
1
×10–7
Time [sec]
–40
–50
3
Power [dB]
2
Fig. 10.15
Comparison between the power response of the theoretical model (dashed line) and experimental
data (continuous line).
These main features are also apparent in real data. Figure 10.15 shows a comparison
between the theoretical power response, expressed in dB, and experimental data
collected for waves propagating in a classroom ﬁlled with furniture. The average
response for a transmitted pulse of width 1 ns was measured over different antenna
locations placed at a distance of 6 m from each other and with an occluded line-of-sight
path. The average measurement is compared with results from the incoherent response
of the random walk model obtained for a rectangular pulse of the same width, an
inter-obstacle distance 1/η = 1 m, and a scattering probability of α = 0.95. As in the
case of the path loss described in Section 10.4, the theoretical model matches well the
data for physically reasonable values of the parameters.
10.7
Summary and Further Reading
The main reference for the theory of multiple scattering in random media is the
book by Ishimaru (1978). The formulation we provided here was originally developed
12
12:02:41, subject to the Cambridge Core

10.8 Test Your Understanding
301
by Twersky (1957, 1964). A standard reference for transport theory is the book by
Chandrasekar (1960). The random walk model of propagation was introduced by
Franceschetti, Bruck, and Schulman (2004), and extended by Franceschetti (2004).
These works also describe the experimental measurements that we reported here. A
general reference for the topic of continuous random walks is the book by Hughes
(1995). The version considered here is a variation of the Pearson–Rayleigh random
walk where the steps of the walk are independent, identically distributed random
vectors of exponential length and uniform orientation, so that the walk is well suited
to model an underlying uniform spatial distribution of scatterers. Many variations of the
Pearson–Rayleigh continuous random walk have been studied in mathematical physics,
with applications ranging from colloid chemistry to stellar dynamics. Additional
mathematical results for the distribution of the position of the random walker after a
ﬁxed number of steps appear in Stadje (1987), Masoliver, Porrá, and Weiss (1993),
and in Franceschetti (2007b). Variations of the problem include walks with different
step-length distributions by Le Caër (2010, 2011) and Pogorui and Rodriguez-Dagnino
(2011), and walks with drift by Grosjean (1953), Barber (1993), and De Gregorio
(2012).
10.8
Test Your Understanding
Problems
10.1
Check that the integrals (10.28) and (10.29) integrate to one.
10.2
Provide a justiﬁcation for (10.41) using the identity

xK0(x)dx = xK1(x).
(10.72)
10.3
Compute the limits for η →0 and α →1 of (10.41), and check that they reduce
to (10.42).
Solution
lim
η→0S(r) =
1
2πr
(
1
1 + α + lim
η→0

1 −α
1 + α αηrK1
	
1 −α2ηr
4
=
1
2πr
(
1
1 + α +

1 −α
1 + α
α
√
1 −α2
4
=
1
2πr.
(10.73)
lim
α→1S(r) =
1
2πr
&1
2 + 1
2
'
=
1
2πr.
(10.74)
12
12:02:41, subject to the Cambridge Core

302
Multiple Scattering Theory
10.4
Check that the approximate solutions in (10.34) are valid probability densities,
as they integrate to one over the plane, and over the space, respectively.
Solution
For the ﬁrst integral, we have
 2π
0
dφ
 ∞
0
rg(r)dr = (1 −α)η

1
η(1 −α2) +
α
η(1 −α2)

= 1.
(10.75)
For the second integral, we have
 2π
0
dφ
 π
0
sinθdθ
 ∞
0
r2g(r)dr = 4π (1 −α)η
4π
&
α
(1 −α2)η +
1
(1 −α2)η
'
= 1.
(10.76)
10.5
Go through all the steps to derive (10.53); see Franceschetti (2004).
10.6
Go through all the steps required to compute the series in (10.55) and obtain
(10.56); see Franceschetti (2004).
12
12:02:41, subject to the Cambridge Core

11
Noise Processes
We must break at all cost from this restrictive circle of pure sounds and conquer the inﬁnite variety
of noise-sounds.1
11.1
Measurement Uncertainty
Every physical apparatus measuring a signal is affected by a measurement error: the
measurement appears to ﬂuctuate randomly by a small amount. The signal describing
these random ﬂuctuations is called noise, and is added to the desired signal. The
presence of the noise ensures that the amount of information that can be communicated
using signals is always ﬁnite. For electromagnetic signals, noise arises from a variety
of causes. All of them can be traced back to the basic reason that at the microscopic
level the world behaves in a quantized fashion. Despite the complexities associated with
microscopic modeling, the net effect is that the noise can, most often, be very precisely
modeled as a random signal having values governed by a probability distribution.
11.1.1
Thermal Noise
Thermal noise measured at a receiving antenna is due to free electron charges in the
electric conductor material constituting the antenna. At any non-zero temperature these
charges move by thermal effects, following random trajectories. Although on average
the charges are uniformly distributed and do not affect the received signal, at any given
instant of time their non-uniform placement creates a certain random voltage across the
terminals of the antenna that varies from instant to instant, creating small ﬂuctuations
in the measured signal. These ﬂuctuations result from the combined effect of many
independent random trajectories of the electron charges, and affect all frequencies of
the signal in the same way. They are modeled as a zero-mean, white Gaussian process
Z(t). Physically, this process delivers an average power kBTKdω/(2π) [W] to any
measurement device acting over an elementary angular frequency interval dω. Here,
kB is the Boltzmann constant, approximately 1.3806488 × 10−23 [J K−1], and TK is the
absolute temperature of the electric conductor. Mathematically, the process has a power
1 Luigi Russolo (1913). The Art of Noises. Reprinted 1986, Something Else Press.
13
12:02:41, subject to the Cambridge Core

304
Noise Processes
spectral density
SZ = kBTK for all ω ∈R.
(11.1)
By the Wiener–Khinchin theorem, taking the inverse Fourier transform of (11.1) we
obtain the (generalized) autocorrelation
sZ(τ) = E(Z(t)Z(t + τ)) = kBTKδ(τ),
(11.2)
which corresponds physically to having a coupling between the values of the noise that
is essentially zero at any two instants of time separated by more than a tiny interval.
What is described above is clearly an idealized situation, and particular care
should be used when talking about random processes with impulsive correlations. The
mathematical meaning of having a random process with an inﬁnite variance given
by sZ(0) should raise an eyebrow, to say the least. Nevertheless, the situation can
be backed up by a rigorous formulation. The white Gaussian process can also be
obtained in an appropriate limiting sense as the generalized derivative of the Wiener
process. This process is deﬁned as having independent Gaussian increments over
disjoint intervals. Using an appropriate smoothing function and letting the derivative at
any point be a functional rather than a real number, a rigorous mathematical treatment
of white Gaussian noise in a distributional sense can be developed. Despite these
mathematical sophistications, in many applications it is possible to think about the
white Gaussian process as simply producing essentially independent Gaussian random
variables at distinct time intervals. In practice, white Gaussian noise is always observed
as a convolution integral through a linear system of ﬁnite bandwidth that acts as a
smoothing function, and this makes any real measurement consistent with the rigorous
mathematical formulation.
From the physical perspective, however, a simple thermal noise model with constant
power spectral density remains problematic. Even if our linear system measurement
may not show it, the total expected power of the noise theoretically diverges if its power
spectral density were to be integrated over all frequencies. This divergence is unnatural
to describe the thermal agitations occurring in nature at ﬁnite temperatures. As we shall
see below, this inconsistency can be solved by taking a closer look at the quantum
mechanical nature of the thermal agitation that leads to the noise process. This provides
a more precise expression for (11.1) that avoids divergence of the expected power and
can also explain the Gaussian form of the distribution in a manner consistent with the
second law of thermodynamics.
11.1.2
Shot Noise
As with thermal noise, the shot noise at a receiving antenna arises because of the
quantization of electric charges. Any current is composed of a stream of charges that
cross a given surface. At the microscopic level, these charges do not cross the surface at
regular intervals, but have random spatial separations between them. Hence, the number
of charges that pass through the surface ﬂuctuates randomly over time around a given
value. A mathematical model to describe the situation is given by a Poisson process
13
12:02:41, subject to the Cambridge Core

11.1 Measurement Uncertainty
305
Fig. 11.1
Convergence of the Poisson distribution to the Gaussian distribution.
Z(t) of intensity ℓ. This is a continuous process that counts random events occurring
uniformly over time. It is deﬁned by two properties: for mutually disjoint intervals
of time the numbers of crossing charges are independent, and the probability that the
number of charges crossing in an interval of length T is equal to k is given by the Poisson
formula
P(Z(T) = k) = (ℓT)k
k!
exp(−ℓT).
(11.3)
It follows from this deﬁnition that in any ﬁnite time interval of length T there are
only ﬁnitely many crossing charges, on average ℓT, and these are distributed uniformly
inside the interval. For a large average number of crossings, the deviation from the
mean Z(T) −ℓT approaches a zero-mean Gaussian random variable of variance ℓT.
This is somewhat evident from Figure 11.1, and Problem 11.1 takes a closer look at
the mathematical derivation. It follows that if a large number of charges is counted
in the measurement process, then the shot noise is essentially Gaussian. In this
case, the shot noise is mathematically no different than the thermal noise considered
earlier. Physically, however, it is due to impressed sources and is proportional to the
current, while the thermal noise occurs in equilibrium in the absence of sources and is
proportional to the temperature.
When the value of the current is increased, in addition to becoming Gaussian, the
effect of the shot noise tends to vanish when seen at the scale of the measured signal
level. Since the standard deviation of the shot noise is
√
ℓT, while its average is ℓT, it
follows that as ℓT →∞the ﬂuctuations in the measured signal around the average value
become negligible, and the effect of the noise disappears compared to the overwhelming
signal.
On the other hand, with very small currents and considering observations at shorter
time scales, the effect of the shot noise and its deviation from the Gaussian distribution
13
12:02:41, subject to the Cambridge Core

306
Noise Processes
can be signiﬁcant. In conductors, however, even at the very small scale the shot noise is
mostly suppressed by the repulsive action of the Coulomb force that acts against charge
build-ups and smoothes out accumulation points, reducing the ﬂuctuations around the
mean that one would expect assuming independent interactions.
11.1.3
Quantum Noise
Poisson shot noise also arises in electromagnetic radiation in the framework of
individual photon detection when appropriate care is taken to reduce the thermal
noise that is otherwise dominant, for example by cooling the receiving apparatus and
transmitting at high frequency with limited energy.
Electromagnetic waves are composed of a stream of photons propagating in the
environment, and at high frequency only a limited number of highly energetic photons
can be transmitted with a given energy budget. Fluctuations can then be observed in
the numbers of received photons over a given time interval. The statistics of these
ﬂuctuations are similar to those occurring in the case of current charges, and are due to
individual photons not being evenly spaced when crossing a given surface, but having
random spatial separations among them. In this case, the shot noise models quantum
effects due to the quantized nature of propagation, and is called quantum noise. For
a ﬁxed radiation energy this noise increases linearly with frequency, since at high
frequency a smaller number of photons are radiated and quantum ﬂuctuations become
more evident.
Quantum noise is closely related to the uncertainty principle examined in Chapter 2.
The uncertainty in the number of photons received in a given time interval is due to the
quantum ﬂuctuations of the time of arrival of each photon composing the signal, and any
reduction of this uncertainty must be balanced by a corresponding quantum uncertainty
in the frequency of the radiated photons. Since the energy of a photon is proportional to
its frequency of radiation, this results in an additional energy uncertainty. Furthermore,
since the signal received at a given distance from the transmitter is phase shifted by
an amount proportional to the frequency of radiation, this results in a corresponding
phase uncertainty. It follows that time–frequency, time–energy, as well as number
of photons–phase, are all pairs subject to the principle of quantum complementarity,
as they cannot be simultaneously measured accurately. Quantum noise represents the
irreducible amount of noise imposed by these quantum constraints. At the macroscopic
scale, for a large number of radiated photons, it reduces to thermal noise, having a
Gaussian distribution. At the microscopic scale, for a small number of radiated photons,
it is better described by a Poisson distribution.
11.1.4
Radiation Noise
Another kind of noise that is also of thermal origin but not intrinsic to the transmitting
or receiving apparatus is due to the propagation environment. Even in free-space
propagation conditions, a certain amount of background radiation from distant objects
13
12:02:41, subject to the Cambridge Core

11.2 The Black Body
307
contributes to the noise at the receiver. Mathematically, this kind of noise follows the
same statistics, and is indistinguishable from the intrinsic thermal noise of conductors.
In idealized free space there is no radiation noise; however, in practice there are a
large number of sources of radiation that affect any antenna measurement. Even pointing
an antenna towards the cold sky it is possible to detect cosmic background radiation
from the boundary of our universe consistent with the possible remains of the Big Bang.
Radiation noise occurs by natural emission of electromagnetic waves due to thermal
effects inside radiating bodies. Any body at a given temperature radiates energy to,
and absorbs energy from, the environment. The black body is an idealized version of
a real body that is in perfect thermal equilibrium, so that the radiating and absorbing
energies coincide at all frequencies. Loosely speaking, all the incoming radiation is
“thermalized” inside the body and re-emitted in the form of electromagnetic waves.
Due to the random mechanism of radiation associated with the random thermal motions
of the elementary constituents of the body, the radiated electromagnetic ﬁeld has a
Gaussian intensity, and the expected power is proportional to the temperature and
distributed across a wide range of frequencies.
11.2
The Black Body
The radiation law of a black body is at the basis of all thermal noise models and its
maximum entropy properties provide insight into the largest amount of information
associated with electromagnetic radiation.
11.2.1
Radiation Law, Classical Derivation
The radiation law can be derived by ﬁrst determining the energy absorbed per unit
volume inside the body, and then imposing the condition that at equilibrium the
absorbed energy equals the radiated energy. Since at the boundary of the body with
the outside space the incoming power ﬂux is equal to the emitted one, the net ﬂux is
zero and we have
e × h · ¯n = 0,
(11.4)
where e is the electric ﬁeld, h is the magnetic ﬁeld, and ¯n is the normal to the boundary.
It follows that we can assume that at the boundary the tangential electric ﬁeld is zero, or
the tangential magnetic ﬁeld is zero. Assuming the former, the electric ﬁeld inside the
body can be written as the superposition of standing waves, that we call modes, with
zero tangential component at the boundaries. Counting all possible conﬁgurations of
these modes leads to an expression for the internal energy absorbed by the body that
must be equal to the radiated energy.
From Maxwell’s equations, the electric ﬁeld must satisfy a wave equation that is the
three-dimensional generalization of (4.44):
∂e
∂x2 + ∂e
∂y2 + ∂e
∂z2 = 1
c2
∂e
∂t2 .
(11.5)
13
12:02:41, subject to the Cambridge Core

308
Noise Processes
α
L
L
λ
λx
λy
x
y
O
β
Fig. 11.2
Standing wave forming angles α and β with respect to the coordinate axes inside the black body.
The solution can be obtained as a superposition of sinusoidal components, each
corresponding to a standing wave of wavelength λ inside the body. We consider a
cubical volume V of side length L. The cubical geometry is taken for convenience,
but the result easily generalizes to different geometries. Waves traveling orthogonally
to the faces of the cube exhibit an electric ﬁeld that is polarized along the coordinate
axes tangential to the faces of the cube. These sinusoidal waves must satisfy the zero
boundary condition, and we have
nλ = 2L,
(11.6)
for any positive integer n.
For waves that travel at angles α,β,γ with respect to the coordinate axes, we have
three sets of standing waves along the three coordinate axes that form a complete modal
expansion of the ﬁeld inside the cube with the prescribed boundary condition, provided
that
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
λx =
λ
cosα ,
λy =
λ
cosβ ,
λz =
λ
cosγ ,
(11.7a)
(11.7b)
(11.7c)
where λx,λy,λz are the wavelength spacings measured along the three axes – see
Figure 11.2. It then follows that
⎧
⎪⎪⎨
⎪⎪⎩
nxλ = 2Lcosα,
nyλ = 2Lcosβ,
nzλ = 2Lcosγ ,
(11.8a)
(11.8b)
(11.8c)
13
12:02:41, subject to the Cambridge Core

11.2 The Black Body
309
nx
ny
O
Fig. 11.3
Approximating the number of lattice points in the positive quadrant within radius ρ from the
origin with the area of the positive quadrant subtended by the circle of radius ρ.
for positive integers nx,ny,nz. Squaring and summing the three equations in (11.8) and
using cos2 α + cos2 β + cos2 γ = 1, we have that the permitted wavelengths of the
standing waves are
λ =
2L

n2x + n2y + n2z
.
(11.9)
We now count the number M(λ) of modes that can meet this condition. This amounts
to counting all possible combinations of the positive integer values nx,ny,nz leading to
the different values of λ. Consider the n-space deﬁned by the integer cubical lattice with
coordinates (nx,ny,nz). In this space, the quantity
ρ =

n2x + n2y + n2z
(11.10)
represents the magnitude of a vector in the positive octant of the space, and each value
of ρ gives a corresponding value for λ. In the scaling limit of L →∞, the discrete
space of possible vectors can be seen as being essentially continuous, and the counting
process corresponds to computing the volume of the three-dimensional positive octant
of the sphere of radius ρ. The error in treating the radius ρ as a continuous variable is of
the order of the surface of the sphere, and is thus negligible compared to its volume; see
Figure 11.3 for a two-dimensional picture of the situation. Accounting for an additional
factor of two for the two possible mode polarizations, we have
M(λ) = 2 1
8
4π
3 (n2
x + n2
y + n2
z)3/2 = π
3 (n2
x + n2
y + n2
z)3/2,
(11.11)
and substituting (11.9) into (11.11), we get
M(λ) = 8πL3
3λ3 .
(11.12)
13
12:02:41, subject to the Cambridge Core

310
Noise Processes
P
dr


312
Noise Processes
L
Fig. 11.5
Thermal noise between coupled resistors.
and the number of modes per unit frequency is
∂n(ω)
∂ω
= L
πv.
(11.23)
In equilibrium, each mode has average energy equal to kBTK, so the average energy per
unit frequency and per unit length in the transmission line is
∂2u(ω)
∂ω∂L = kBTK
πv .
(11.24)
Finally, we need to convert the energy density inside the transmission line into power
ﬂow from one end to the other of the transmission line. This is simply done multipying
by v, and obtaining the total average power per unit frequency generated by two
resistors. It follows that the average power per unit frequency of one resistor is
P(ω) = SZ(ω)
2π
= kBTK
2π .
(11.25)
The average power per unit frequency of the noise is in this case proportional to the
temperature and constant across the frequency spectrum. The similarity between this
result and (11.20) should be clear. The space of the transmission line has dimension one
rather than three, so that the frequency dependence is ω0 in the case of (11.25), rather
than ω2 as in the case of (11.20).
11.2.3
Quantum Mechanical Correction
There is a problem with the results obtained by the classical derivation of the thermal
and radiation noises: the total expected power obtained by integrating the power spectral
density over all frequencies diverges. This is clearly a paradox, as the expected power is
in both cases due to thermal agitations in equilibrium at ﬁnite temperature, so it cannot
be inﬁnite. The phenomenon is clear from the divergence of (11.20) at high frequencies,
and is called the “ultraviolet catastrophe” prediction of classical physics. Similarly,
the power spectral density of white noise in (11.25), although not diverging, remains
constant across all frequencies, and its integral across the whole spectrum also gives an
inﬁnite expected power.
Fortunately, quantum mechanics comes to the rescue and brings the model closer to
reality. The quantum mechanical correction of the formulas (11.1) and (11.20) is based
on the fundamental observation that the electromagnetic modes cannot have arbitrary
13
12:02:41, subject to the Cambridge Core

11.2 The Black Body
313
energy values, but these are quantized and radiant energy can only exist in discrete
quanta n¯hω, n = 1,2,3,..., where ¯h is the reduced Planck’s constant. This effectively
means that the energy is proportional to a discrete number of emitted photons, each
carrying a quantum of energy ¯hω. In this case, the exponential distribution (11.14) for
the energy becomes discrete, taking values with probability
P(E = n¯hω) =
exp(−n¯hω/(kBTK))

∞
n=0 exp(−n¯hω/(kBTK)),
(11.26)
and the average energy per mode is then given by
E(E) =

∞
n=0 n¯hωexp(−n¯hω/(kBTK))

∞
n=0 exp(−n¯hω/(kBTK))
.
(11.27)
In order to perform the computation in (11.27) we write x = exp(−¯hω/(kBTK)), so that
we have
E(E) = ¯hω

∞
n=0 nxn

∞
n=0 xn
= ¯hωx

∞
n=0 nxn−1

∞
n=0 xn
= ¯hωx(1 −x)−2
(1 −x)−1
= ¯hωx
1 −x
=
¯hω
1/x −1
=
¯hω
exp(¯hω/(kBTK)) −1.
(11.28)
By using (11.28) in place of kBTK in the derivations of the power spectral densities
given before, we obtain in one dimension the power spectral density per unit angular
frequency, and in three dimensions the power spectral density per unit angular
frequency, per unit area, and per unit solid angle:
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
S(1)
Z (ω) =
¯hω
exp(¯hω/(kBTK)) −1,
S(3)
Z (ω) =
¯hω3
2π2c2
1
exp(¯hω/(kBTK)) −1.
(11.29a)
(11.29b)
The analogous expressions per unit wavelength are immediately obtained using the
chain rule with ω = 2πc/λ:
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
S(1)
Z (λ) = (2π)2c2 ¯h
λ3
1
exp(2π ¯hc/(λkBTK)) −1,
S(3)
Z (λ) = (2π)22c2 ¯h
λ5
1
exp(2π ¯hc/(λkBTK)) −1.
(11.30a)
(11.30b)
13
12:02:41, subject to the Cambridge Core

314
Noise Processes
Z
(1)
Z
(1)
k
10
–11
k
10
–3
Z
(3)
Z
(3)
0
0
(a)
(b)
k
10
–11
k
10
–3
Fig. 11.6
Power spectral densities of radiation noise (a) and ideal thermal noise (b), with corresponding
classical approximations (dashed lines).
The results from classical mechanics obtained earlier are recovered by letting the
energy of a photon ¯hω →0 and using the Maclaurin expansion
exp(¯hω/(kBTK)) = 1 +
¯hω
kBTK
+ O((¯hω)2).
(11.31)
Exact formulas and their low-frequency approximations are depicted in Figure 11.6. The
left-hand side of the ﬁgure depicts the power spectral density in the three-dimensional
case, and the right-hand side depicts the one-dimensional case. When communication
occurs in the range of frequencies ¯hω/kB ≈10−11ω ≪TK, the white noise approxi-
mation applies. For example, for communication up to the GHz range at an ambient
temperature of 300 K, the approximation is satisﬁed with four orders of magnitude. On
the other hand, at higher frequencies and lower temperatures, quantum effects become
relevant. Figure 11.7 depicts the power spectral density in the three-dimensional case for
different values of the temperature, showing that the peak of the curve shifts to the right
and the curve becomes spread out over a larger wavelength interval as the temperature
decreases.
11.3
Equilibrium Configurations
In the derivations above, we have assumed that thermally excited electromagnetic
modes have exponentially distributed energy. We justiﬁed this assumption by stating
that at equilibrium it allows all energy values due to random thermal oscillations, but
weights higher energy values by an exponentially decaying probability of occurrence. A
13
12:02:41, subject to the Cambridge Core

11.3 Equilibrium Configurations
315
0.2
0.6
1
1.4
0
0.5
1.0
1.5
2.0
[W/m ]
3
0.4
0.8
1.2
4000 K
5000 K
6000 K
7000 K
Z
10
14
[m]10
–6
(3)
Fig. 11.7
Power spectral density of three-dimensional radiation noise as a function of the wavelength for
different temperature values.
consequence of the exponential assumption is that the electromagnetic radiated signal
has a Gaussian distributed amplitude at all frequencies. This can also be justiﬁed
by noting that thermal excitations are the result of a large number of independent
random trajectories of the elementary constituents of the body. For example, in a
resistor each conduction band electron produces a random elementary current wave as it
thermally agitates. The total current is then the result of the addition of these elementary
independent components, and one can invoke the central limit theorem to justify the
resulting Gaussian distribution. The main idea is that by adding many independent
random variables and hence convolving their distributions, a Gaussian shape is obtained
in the limit. This ﬁxed point distribution has the property that additional convolutions
with Gaussians have only scaling effects but do not change its overall shape. Although
the argument can be made mathematically rigorous, its physical foundation based on
the random elementary effects still appears somewhat heuristic.
An alternative justiﬁcation for the choice of the probability distribution of the noise
can be given in terms of the second law of thermodynamics, according to which at
equilibrium the energy of the thermally excited modes inside the body assumes a
conﬁguration that maximizes the entropy of the radiating system. This also provides
an interpretation of the noise as the most random process, for a given average energy
constraint, namely the one for which we have the least amount of information regarding
its actual physical state. Instead of the central limit theorem, the mathematical tool for
this physical interpretation is the law of large numbers.
13
12:02:41, subject to the Cambridge Core

316
Noise Processes
11.3.1
Statistical Entropy
To relate our probabilistic modeling to the second law, we recall some statistical
mechanical deﬁnitions introduced in Chapter 1. Consider a system that can assume any
one of N discrete states, where the nth state occurs with probability pn. The Shannon
entropy of this system is
H = −
N

n=1
pn logpn,
(11.32)
where the logarithm is base two, we deﬁne by continuity 0log0 = 0, and the entropy
is measured in bits. A simple change of base leads to a change of units, as logb pn =
logb aloga pn. The entropy formula was proposed by Shannon in 1948 as a measure of
information: the entropy of a system can be thought of as the amount of information we
lack about it.
In statistical mechanics, the Boltzmann (1896–8) entropy of a system with N states
is deﬁned as
HB = kB logN.
(11.33)
Apart from the scaling factor kB, this corresponds to letting pn = 1/N in (11.32), so that
H = logN. This choice of distribution yields a maximization of (11.32) subject to the
constraint on the support of the interval where the state distribution is deﬁned to be of
size N. The proof follows easily from Jensen’s inequality:
H =
N

n=1
pn log1/pn
≤log
N

n=1
pn/pn
= logN,
(11.34)
where it can be immediately veriﬁed that the equality occurs if and only if pn = 1/N. The
normalization factor kB in Boltzmann’s formula gives to the entropy the dimensions of
energy over temperature and, as we shall see below, allows us to interpret the statistical
deﬁnition as the thermodynamic entropy of an isolated system in thermal equilibrium,
where all states are equally probable and occur independently.
Another statistical mechanical deﬁnition of entropy, that is analogous to Shannon’s
formula, was introduced by Gibbs in 1902 as a generalization of Boltzmann’s entropy:
HG = −kB
N

n=1
pn logpn.
(11.35)
This is aimed at describing systems in thermal equilibrium whose states are not
necessarily uniformly distributed. For example, a black body is a closed system in
thermal equilibrium that is not isolated, as it continuously exchanges energy with the
outside world. The energy is not uniformly distributed among all the electromagnetic
13
12:02:41, subject to the Cambridge Core

11.3 Equilibrium Configurations
317
modes inside the body, but only the average energy per mode is kept constant, while
higher energy modes are exponentially less likely than the lower ones.
Finally, in a system having a continuous density of states, an integral expression
analogous to (11.32) is given by the differential entropy,
h(f) = −

S
f(x)logf(x)dx,
(11.36)
where x is the realization of a continuous random variable representing the state of the
system having support S and probability density function f. In this case, the uniform
distribution is the maximum entropy distribution over a given support interval, the
exponential distribution is the maximum entropy distribution among all distributions
with given non-zero support subject to a ﬁrst moment constraint, and the Gaussian
distribution has maximum entropy among all distributions subject to a second moment
constraint (see Problems 11.2, 11.3, and 1.15).
Given the deﬁnitions above, it is now clear that the probability distributions employed
in the description of thermal noise are consistent with the principle of statistical entropy
maximization: each noise mode is assumed to have uniform phase, exponentially
distributed energy subject to an average constraint, and Gaussian intensity subject to a
variance constraint, corresponding to the average power of the noise. The probability
distributions for the shot noise are also consistent with the same principle. Fixing
the intensity of the Poisson process corresponds to ﬁxing the average time between
successive arrivals, and under this constraint the inter-arrival time is exponentially
distributed. Fixing the length of any time interval, the distribution of the arrivals in
the given interval is uniform, and this maximizes the entropy given the support of the
distribution.
We conclude that using the maximum entropy principle to model the noise has two
appealing features. On the one hand, by maximizing the Shannon entropy subject to
physical constraints, the noise maximizes the lack of information regarding its actual
physical state. On the other hand, maximization of entropy is consistent with the second
law of thermodynamics, according to which a system in thermal equilibrium always
reaches a conﬁguration where entropy is maximized. To better understand this latter
connection, we now take a closer look at the relationship between the statistical and
thermodynamic deﬁnitions of entropy.
11.3.2
Thermodynamic Entropy
The phenomenological deﬁnition of thermodynamic entropy is stated in variational
terms as:
A variation of thermodynamic entropy is given by the ratio of energy absorbed by
the system in the form of heat and the macroscopic temperature of the system.
13
12:02:41, subject to the Cambridge Core

318
Noise Processes
This deﬁnition was proposed by Clausies (1850–65). Expressed mathematically, we
have
dHC = dE
TK
,
(11.37)
where the entropy has the dimensions of energy divided by temperature.
The fundamental insight of statistical mechanics is that phenomenological obser-
vations reﬂect the average of the microscopic conﬁgurations of the system. Once
equilibrium is reached, microscopic energy ﬂuctuations are smoothed out and only
average values over the possible microscopic energy conﬁgurations are observed.
Consider a black body in thermal equilibrium containing M electromagnetic modes,
each of average energy kBTK. The body has total average energy
E = MkBTK.
(11.38)
Differentiating (11.38) and dividing both sides by the temperature TK, we have
dE
TK
= kBdM.
(11.39)
Putting together (11.37), (11.38), and (11.39), it follows that the quantity kBM
corresponds to the thermodynamic entropy. Namely, the entropy is proportional to the
number of elementary modes of the system. This equivalence holds only on average,
with statistical variations that tend to disappear as the number of elementary modes
increases, or equivalently according to (11.12) in the scaling limit of large bodies. A
larger body allows a larger number of possible electromagnetic modes inside it and, as
the number of modes grows, this number also converges to the thermodynamic entropy.
The equivalence is limited to thermodynamic equilibrium, as a macroscopic notion of
temperature can only be deﬁned at equilibrium.
11.3.3
The Second Law of Thermodynamics
The second law of thermodynamics can be stated in many different ways. At the end of
his presentation to the Philosophical Society of Zurich in 1865, Rudolf Clausius stated
that the entropy of the universe tends to a maximum. More precisely, we have that:
The entropy of a system that moves from one state of thermal equilibrium to another
can only increase.
It is important to emphasize that the increase in entropy is measured only between
states of thermal equilibrium. During the time it takes for the system to reach
equilibrium, thermodynamic entropy is not deﬁned and one can only talk of an increase
(if any) in statistical entropy.
Statistical mechanics provides an explanation of the second law in terms of
microscopic conﬁgurations that the system can attain at equilibrium. Consider M modes,
each having two equally probable binary energy levels. They generate N = 2M equally
13
12:02:41, subject to the Cambridge Core

11.3 Equilibrium Configurations
319
probable states of the system, and this distribution maximizes statistical entropy subject
to a constraint on the number of states being equal to N. In this case, we immediately
have
HB = kB logN
= kB log2M
= kBM
= HC.
(11.40)
The binary cardinality of the energy levels of the elementary modes does not really
matter, as long as it remains ﬁnite and yields a ﬁnite cardinality N of the number of
states on which to maximize the entropy. Letting the set of permitted energy levels be
E and its cardinality be |E | rather than 2, we also have
HB = kB log|E |M = kBM log|E |,
(11.41)
where the factor log|E | can be eliminated by changing the units with which we measure
entropy by an appropriate choice of the base of the logarithm. It follows that for systems
in equilibrium composed of elementary modes whose energy takes discrete values in a
set of ﬁnite cardinality, and where all states are equally probable, the statistical and
thermodynamic entropies coincide. Gibbs’ deﬁnition (11.35) attains Boltzmann’s form
(11.33), corresponding to its maximum, and Shannon’s deﬁnition (11.34) corresponds
to the analogous dimensionless quantity.
11.3.4
Probabilistic Interpretation
So far, we have considered discrete systems in thermal equilibrium composed of
elementary modes that can have a ﬁnite number of energy levels. In this case, the
statistical and thermodynamic deﬁnitions of entropy coincide. In the scaling limit of
large systems, however, the cardinality of the permitted energy levels of the elementary
modes increases, so that the factor log|E | in (11.41) diverges, and the relationship
between statistical and thermodynamic entropy becomes unclear. In addition, systems
in equilibrium may also have states that are not equally probable, and in this case Gibbs’
and Shannon’s deﬁnitions of entropy differ from Boltzmann’s. It follows that to obtain
a satisfactory probabilistic interpretation of the second law, we need a more elaborate
description that takes into account these discrepancies.
At the basis of this generalization is the asymptotic equipartition property for a
statistical ensemble of microstates. This property ensures that as the number of modes
M →∞and the system converges to equilibrium, only a subset of the inﬁnitely many
possible states of the system are typical, and these are essentially equally probable. The
cardinality of the typical states is an exponential function of the statistical entropy of
the elementary modes and of the number of modes, and (11.40) becomes
HB = kB log2MH(p)
= kBMH(p),
(11.42)
13
12:02:41, subject to the Cambridge Core

320
Noise Processes
where p = pE(e) is the common distribution of the energy levels of the modes, and H(p)
is its Shannon entropy. The cardinality of the energy levels being ﬁnite or not does not
matter anymore; as long as their entropy is ﬁnite, the number of typical states 2MH(p) is
well deﬁned. Since the typical states are essentially equiprobable, Gibbs’, Shannon’s,
and Boltzmann’s deﬁnitions once again assume the same form.
Comparing (11.40) and (11.42), it follows that the quantity MH(p) in (11.42)
can be interpreted as the number of equiprobable binary modes of an equivalent
thermodynamic system in equilibrium, and maximizing the statistical entropy H(p)
subject to energy constraints corresponds to maximizing the thermodynamic entropy
of this system.
11.3.5
Asymptotic Equipartition Property
In mathematical terms, the asymptotic equipartition property states that the probability
of a sequence of M i.i.d. random variables {Ek} with common distribution pE(e) over the
set E is close to 2−MH(p), and the number of such “typical sequences” is approximately
2MH(p).
Asymptotic equipartition follows from the law of large numbers. The (weak) law of
large numbers states that the sample average (1/N)
N
k=1 Ek converges in probability to
its expected value. Consider now another sequence of random variables {Wk}, such that
each Wk takes the value w(e) = −logpE(e) for Ek = e. The independence of the Ek’s
implies that the Wk’s are also independent, with mean
E(Wk) = −

e∈E
pE(e)logpE(e) = H(p).
(11.43)
By applying the law of large numbers to the sample average
W1 +··· + WM
M
= 1
M
M

k=1
−logpE(ek),
(11.44)
we have
lim
M→∞P

1
M
M

k=1
−logpE(ek) −H(p)
 > ϵ

= 0.
(11.45)
This result essentially states that given a long sequence of i.i.d. random variables, there
is a typical set of possible realizations deﬁned by
2−M(H(p)+ϵ) ≤P(E1 = e1,E2 = e2,...,EM = eM) ≤2−M(H(p)−ϵ),
(11.46)
whose aggregate probability is almost one. All the elements of the typical set are nearly
equiprobable, each having probability roughly 2−MH(p), and the number of elements in
the typical set is roughly 2MH(p) – see Figure 11.8.
13
12:02:41, subject to the Cambridge Core

11.3 Equilibrium Configurations
321
2MH(p)
Fig. 11.8
Asymptotic equipartition of the set E . The typical set is composed of approximately 2MH(p)
elements.
Consider now a system in thermal equilibrium constituted by a large number M of
elementary modes, each having energy Ek. The entropy of each mode is
H(p) = −

e∈E
pE(e)logpE(e).
(11.47)
By asymptotic equipartition, all states are typical only when the energy is uniformly
distributed among them, while in general only a small number 2MH(p) of states
have non-negligible probability and these are nearly equally probable. It follows that
choosing a distribution for the energy of each mode that maximizes the entropy subject
to physical constraints corresponds to choosing the distribution of system states that
leads to the largest number of nearly equiprobable states. Physically, this is the most
natural distribution that is generated by the largest number of possible conﬁgurations
of the elementary constituents of the system. By (11.42), the Boltzmann entropy of
the equipartitioned system composed of the typical states is maximized by choosing an
appropriate energy distribution for the modes maximizing their Shannon entropy. Since
the Boltzmann entropy corresponds to the thermodynamic entropy of an equivalent
isolated system in thermal equilibrium where all states are equally probable, it follows
that this maximization is consistent with the second law of thermodynamics.
11.3.6
Entropy and Noise
We now summarize the basic results on thermodynamic and statistical entropy,
asymptotic equipartition, and their relationship with the noise model. For systems that
take a discrete number of states, statistical entropy coincides with thermodynamic
entropy at thermal equilibrium. The equivalence is immediate for isolated systems,
where states are equally probable and the statistical entropy attains Boltzmann’s form
H = logN. For systems that are not isolated, the asymptotic equipartition property shows
that as the state space grows, it concentrates around the typical set, and the statistical
entropy of this set again attains Boltzmann’s form H = log2MH(p). By choosing the
maximum entropy distribution for the mode energies H(p), we can maximize the
thermodynamic entropy of a system in equilibrium having MH(p) equiprobable binary
modes. Deﬁning the noise process in this way is consistent with the classical statement
of the second law.
13
12:02:41, subject to the Cambridge Core

322
Noise Processes
11.4
Relative Entropy
Of course, one can wonder what happens to the statistical entropy as the system
reaches equilibrium. Classical thermodynamics has little to say in this case, since
thermodynamic entropy can only be deﬁned at equilibrium. However, for systems
whose states evolve according to a Markov chain, and whose equilibrium distribution is
uniform over a ﬁnite state space, it can be shown that statistical entropy increases over
time.
This result follows from some properties of the relative entropy between probability
distributions p(x) and q(x),
D(p||q) =

x∈X
p(x)log p(x)
q(x).
(11.48)
The ﬁrst property is the information inequality,
D(p||q) ≥0,
(11.49)
where equality holds if and only if p = q. The second property is the chain rule,
D[p(x,y)||q(x,y)] = D[p(x)||q(x)] + D[p(y|x)||q(y|x)],
(11.50)
where
D(p(y|x)||q(y|x)) =

x
p(x)

y
p(y|x)log p(y|x)
q(y|x).
(11.51)
From (11.50), it follows that given any two joint probability distributions p(xn,xn+1) and
q(xn,xn+1) on the state space of the Markov chain, where xn indicates the state at time n
and xn+1 the state at time n + 1, we have
D[p(xn,xn+1)||q(xn,xn+1)]
= D[p(xn)||q(xn)] + D[p(xn+1|xn)||q(xn+1|xn)]
= D[p(xn+1)||q(xn+1)] + D[p(xn|xn+1)||q(xn|xn+1)].
(11.52)
From (11.49), and since p(xn+1|xn) = q(xn+1|xn), it follows that
D[p(xn)||q(xn)] ≥D[p(xn+1)||q(xn+1)].
(11.53)
In the case that there is a unique stationary distribution, and letting q(xn) = q(xn+1) be
such a distribution, the result implies that any state distribution gets closer and closer to
the stationary one as time goes by.
The relative entropy of a Markov chain evolving towards a unique stationary
distribution is non-increasing in time.
Finally, if the stationary distribution is the uniform one, we have, by (11.48),
D[p(xn)||q(xn)] =

xn∈X
p(xn)logp(xn) +

xn∈X
p(xn)log|X |
13
12:02:41, subject to the Cambridge Core

11.5 The Microwave Window
323
= −H(pXn) + log|X |,
(11.54)
so that the monotonic decrease in relative entropy implies a monotonic increase in
entropy.
We now return to the physics. For systems in thermal equilibrium, each state is
identiﬁed by a sequence of M i.i.d. random variables {Ek} representing the values of the
energy of its elementary modes. When the number of modes is large then asymptotic
equipartition ensures that only a typical set of states have non-negligible probability of
occurrence and these are essentially equiprobable. Assuming that the system evolves
towards thermal equilibrium in a Markovian fashion, it follows that the stationary
distribution is uniform and the statistical entropy increases over time.
In practice, however, the equiprobable approximation can lead to entropy decrease
over short time scales. From (11.46), it follows that the quality of the approximation
improves as ϵ →0. However, small values of ϵ require larger values of M for
the probability of the typical set (11.45) to be close to one. When M is large but
ﬁxed, the state distribution admits non-typical states that have small, but non-zero,
aggregate probability. As the system evolves towards equilibrium and the state
distribution approaches the almost-uniform stationary one, small ﬂuctuations exhibiting
a decrease in entropy are possible – see Problem 11.7. In the study of non-equilibrium
thermodynamics, ﬂuctuation theorems bound the probability of such events.
11.5
The Microwave Window
In the design of communication systems, it is desirable to keep the noise to a minimum
level. The laws of radiation show that the noise from different sources affecting the
transmitted signal depends on the temperature and on the frequency of radiation.
It turns out that in the microwave range of frequencies between 1 and 60 GHz the
environmental radiation noise is at a minimum, which makes it an ideal candidate band
for communication using electromagnetic signals.
Clearly, radiation noise is signiﬁcant in the visible part of the spectrum. For example,
the sun’s surface temperature is around 5800 K and its emission peaks at a wavelength
of around 0.5 × 10−6 m. The emission appears as white light because around its peak
the power spectral density is approximately constant. This peak makes it difﬁcult to
communicate at visible optical frequencies during daytime. In comparison, the earth’s
surface temperature at around 300 K peaks in the infrared part of the spectrum at a
wavelength of around 10−5 m. The intensity of radiation is much lower, about a 10−6
scale factor compared to the sun. The cosmic background radiation is isotropic across
the whole sky and corresponds to a universal temperature of 2.73 K, which constitutes
a noise ﬂoor for the microwave window.
Figure 11.9 shows the combined effect of the different forms of natural radiation
noise in space. It plots the equivalent noise temperature spectral density, namely
TN = SZ
kB
.
(11.55)
13
12:02:41, subject to the Cambridge Core

324
Noise Processes
101
102
100
10–1
101
102
103
100
103
2.73
H2O
O2
Galactic  noise
Quantum noise
Atmospheric noise 
Cosmic background
TN
GHz
Fig. 11.9
Radiation noise contributions. The peaks in the atmospheric noise are due to the resonant
absorption frequencies for water and oxygen.
We now identify the different contributions. First there is “galactic noise,” the
combined radiation noise from the Milky Way. This noise rises steeply below 1 GHz.
Second, there is the 2.73 K cosmic background, the relic radiation emitted at the
beginning of the universe and reaching us today from the boundaries of space.
Third, there is “quantum noise,” representing a fundamental quantum limitation on the
receiver’s sensitivity. This reﬂects vacuum ﬂuctuations and represents the uncertainty
of detecting a single photon of energy ¯hω. Letting
kBTN = ¯hω,
(11.56)
we get
TN = ¯hω
kB
.
(11.57)
It follows that the quantum noise temperature increases linearly with frequency and
becomes the dominant form of noise at high frequencies.
Attenuation in the atmosphere reduces the received galactic noise level, but it replaces
it with the noise from the hotter atmosphere that in ﬁrst approximation acts as a black
body itself, emitting the absorbed radiation. For example, a galactic black-body noise
source of temperature Tb viewed through a lossy medium, such as the atmosphere, of
loss γ and temperature Ta, yields an effective temperature of
Te = γ Tb + (1 −γ )Ta.
(11.58)
Atmospheric noise peaks at the resonant absorption frequencies for the different
elements in the atmosphere, at which absorption is maximum. The atmospheric noise
contribution also depends on the angle between the direction of the antenna and the
13
12:02:41, subject to the Cambridge Core

11.6 Quantum Complementarity
325
zenith. Looking to the zenith, the antenna is facing the smallest possible thickness of
the atmosphere. Looking towards the horizon, the antenna signal path needs to pass
through a much longer section of atmosphere. Above 1 GHz, the galactic noise falls
below the isotropic cosmic background, and above about 60 GHz the quantum noise
exceeds the cosmic background. Thus, the frequency range of 1–60 GHz is ideal for
communication as natural noise is at a minimum. This is the microwave window in free
space.
With the development of personal communication systems and the myriad man-made
devices operating in the microwave range, the collective interference from other devices
can also become a source of noise. The noise at the terminals of the antenna becomes
a weighted average of the natural noise sources, plus the man-made noise. Man-made
noise, emanating from electrical motors and power devices, typically plays little part
in the microwave range and appears mostly in the low frequencies. However, some
household devices also operate in the microwave range and can cause unwanted noise
effects on communication.
11.6
Quantum Complementarity
Thermal and radiation noise fall off as ω →∞. If these were the only kinds of noise
present, then one could mitigate their effect by simply transmitting signals at increasing
frequencies. Quantum limitations do not allow such a scenario. They arise from the
quantization of charges or photons that propagate in the environment and they are
another manifestation of the uncertainty principle illustrated in Chapter 2.
Recall that the uncertainty principle states that it is impossible to achieve simultane-
ous concentration in time and frequency. The more a signal is localized in time, the less
it is localized in frequency, and vice versa. For a unit energy signal f(t) of spectrum
F(ω), this means that
σ 2
t σ 2
ω ≥1
4,
(11.59)
where
σ 2
t =
 ∞
−∞
t2f 2(t)dt
(11.60)
σ 2
ω = 1
2π
 ∞
−∞
ω2|F(ω)|2dω.
(11.61)
For a single photon, Planck’s equation,
Ep = ¯hω,
(11.62)
shows that localization in frequency is equivalent to the determination of its energy, and
we have the time–energy indetermination inequality
σ 2
t σ 2
Ep ≥¯h2
4 .
(11.63)
13
12:02:41, subject to the Cambridge Core

326
Noise Processes
The more accurately we know the energy of a photon, the more monochromatic its wave
function becomes, and the less accurately we can measure the time of arrival. On the
other hand, in a small time interval the detected energy is uncertain and it is impossible
to determine the presence or absence of a photon in time: the vacuum is subject to
quantum ﬂuctuations over small time scales.
The illustrated relations are examples of the general principle of quantum comple-
mentarity, according to which not all aspects of a system can be viewed simultaneously,
as complementary quantities cannot be simultaneously measured accurately. This
principle leads to the fundamental limitation labeled as quantum noise in Figure 11.9.
For any given energy level, quantum noise increases linearly with frequency, as quantum
ﬂuctuations become more evident when a smaller number of highly energetic photons
are radiated in space.
11.7
Entropy of a Black Body
Noise signals maximize entropy, subject to energy constraints. It follows that thermal
radiation can be used to compute the largest amount of information, in terms of entropy,
associated with a radiating body of given average energy. To illustrate this point, we
note that by (11.12) the number of internal states of a black body is proportional
to its volume expressed in units of wavelength. These states, however, are not all
equally probable as higher frequency modes are weighted by a lower probability of
occurrence that also depends on the temperature, according to (11.26). By the results
described in Section 11.3, only a smaller number of states are typical, and these are
essentially equally probable. The amount of information inside the body, integrated
over all frequencies of radiation, can then be computed by taking the logarithm of the
number of typical states that are thermally excited, which, for large systems in thermal
equilibrium, coincides with the thermodynamic entropy. Finally, the thermodynamic
entropy can be derived using (11.37) from the total average energy of the system,
considering the variation of energy occurring at a given temperature due to an amount
of heat absorbed by the system of constant volume. In the following, we describe these
computations.
11.7.1
Total Energy
To compute the total average energy inside the body, we ﬁrst compute the energy per
unit volume lying between ω and ω+dω. Using (11.29b) and recalling (11.17), we have
the expected energy per unit frequency and per unit volume:
4π
c
S(3)
Z (ω)
2π
= ¯hω3
π2c3
1
exp(¯hω/(kBTK)) −1.
(11.64)
Letting x = ¯hω/(kBTK), the total average energy per unit volume is then given by
 ∞
0
¯hω3
π2c3
dω
exp(¯hω/(kBTK)) −1 = (kBTK)4
π2 ¯h3c3
 ∞
0
x3
exp(x) −1dx.
(11.65)
13
12:02:41, subject to the Cambridge Core

11.7 Entropy of a Black Body
327
The integral on the right-hand side can be computed to be π4/15, so that the total
average energy of a black body of volume V and temperature TK is given by
E =
π2
15¯h3c3 (kBTK)4V = αVT4
K,
(11.66)
where we have deﬁned
α =
π2
15¯h3c3 k4
B.
(11.67)
11.7.2
Thermodynamic Entropy
The total average energy of a black body depends on both its volume and its temperature.
To derive an expression for the entropy, we consider a process where the volume is held
constant and the energy variation is due to a temperature variation only. By (11.66), we
have
dHC = dE
TK
= 4αVT2
K dTK.
(11.68)
Integrating both sides and imposing that the entropy vanishes at zero temperature, we
get
HC = 4
3αVT3
K.
(11.69)
Since at equilibrium the thermodynamic entropy corresponds to the logarithm of the
number of typical states of the system, it follows that the number of degrees of
freedom of the system is an exponential function of the volume and of the cube of the
temperature. This functional law is somehow expected: by (11.12), the total number
of modes of a given wavelength that can be thermally excited inside the body is
proportional to the volume and inversely proportional to the cube of the wavelength.
Smaller wavelengths yield a larger number of modes, but these require larger energy
values to be excited. Since the average energy is proportional to the temperature, larger
values of the temperature increase the number of degrees of freedom by decreasing the
wavelength of the typical modes of radiation.
By combining (11.66) and (11.69), we obtain
HC = 4
3E3/4V1/4α1/4.
(11.70)
It follows that the entropy of a black body is proportional to the total average energy
of the body to the power 3/4. By increasing the energy, the typical modes of radiation
occur at higher frequency and carry a larger amount of information.
A natural question then arises of whether information can grow arbitrarily large by
increasing the energy inside the body. It turns out that the high-frequency thermal states
require high energy to be excited and can eventually lead to gravitational instability of
the source of the radiation. Even if we have assigned an exponentially low probability of
occurrence to the high-frequency modes, the typical states responsible for the entropy
in (11.69) are still too massive for large volumes and large temperatures, and can lead
13
12:02:41, subject to the Cambridge Core

328
Noise Processes
to gravitational collapse of the body into a black hole. On the other hand, if we keep
the average energy inside the body below a value that leads to gravitational collapse,
we obtain an entropy bound that depends only on the size of the system expressed in a
certain unit of length, called the Planck length. In the following, we ﬁrst introduce this
length based on quantum uncertainty relations, and then turn to the computation of the
entropy bound.
11.7.3
The Planck Length
The main idea for deﬁning the Planck length is to introduce a unit of length so small
that quantum ﬂuctuations of energy within its radius may be so large that they could
create a small black hole. By the time–energy uncertainty principle for a single photon
(11.63), we have
σtσEp ≥¯h
2,
(11.71)
from which we have the localization–energy uncertainty
σℓσEp ≥¯hc
2 ,
(11.72)
expressing the general principle that photons of precise energy, or equivalently precise
frequency, cannot be localized accurately due to the uncertainty on their time of arrival
at any given point in space. A larger energy uncertainty allows for a smaller distance
uncertainty, and vice versa. A bound on the reduction in distance uncertainty is imposed
by general relativity. For distances smaller than the Schwarzschild radius
rS = 2Gm
c2
= 2Gmc2
c4
= 2GσEp
c4
,
(11.73)
where m is the mass of the radiating body and G is Newton’s gravitational constant,
the laws of general relativity predict that such a precise localization requires energy
ﬂuctuations so large that they can lead to gravitational collapse and the formation of
a small black hole of radius rS. It follows that the distance uncertainty can be reduced
only up to
σℓ≥2GσEp
c4
≥
G¯hc
2c42σℓ
,
(11.74)
from which it follows that
σ 2
ℓ≥G¯h
c3 .
(11.75)
We deﬁne the square root of the right-hand side of (11.75) as the Planck length:
ℓp =

G¯h
c3 .
(11.76)
13
12:02:41, subject to the Cambridge Core

11.8 Entropy of Arbitrary Systems
329
The Planck length represents the smallest standard deviation of the distance that
can be appreciated before the energy required to achieve such resolution becomes
so large as to create a black hole at the measurement point.
11.7.4
Gravitational Limits
We now determine a bound on the entropy of a black body in terms of its size expressed
in Planck length units. We consider a spherical black body of radius r, and we impose
the condition that the total average energy is below the Schwarzschild energy value
leading to gravitational collapse. It follows that we need
r ≥rS = 2Gm
c2 .
(11.77)
Multiplying both sides by c2 and using E = mc2, we have that the energy of the black
body must satisfy
E ≤c4r
2G.
(11.78)
Using (11.78) as a bound for the average energy in (11.70), it follows that an upper
bound on the entropy is
HC ≤4
3
c3r3/4
(2G)3/4
4
3πr3
1/4
α1/4.
(11.79)
Finally, substituting (11.67) into (11.79), we obtain
HC ≤kBα′
 r
ℓp
3/2
,
(11.80)
where α′ is a dimensionless constant close to one. The gravitational bound (11.80)
states that the entropy of a black body can be at most proportional to the square root
of its volume, expressed in Planck length units. This contrasts with the original estimate
(11.69), for which the entropy is proportional to the volume of the body and to the cube
of the temperature. For bodies of moderate size and temperature, however, (11.69) can
be much smaller than (11.80). The entropy can be proportional to the volume and to the
cube of the temperature until the body is so large and so energetic that the additional
constraint of not collapsing into a black hole expressed by (11.80) becomes dominant –
see Problem 11.11.
11.8
Entropy of Arbitrary Systems
So far, we have provided maximum entropy bounds for black bodies ﬁlled with thermal
electromagnetic radiation. These correspond to systems radiating at all frequencies
and subject to average energy and quantum radiation constraints, as described in
13
12:02:41, subject to the Cambridge Core

330
Noise Processes
Section 11.2.3. Similar bounds can be obtained for general matter systems and for
electromagnetic systems radiating over a ﬁnite frequency bandwidth. The holographic
information bound introduced in Chapter 1 is one such example. Another example is
the universal entropy bound that we introduce below and discuss further in Chapter 13.
11.8.1
The Holographic Bound
The argument for the holographic bound given in Chapter 1 is that the entropy of any
arbitrary system cannot be larger than the entropy of a black hole of the same size, and
this is proportional to the area of the event horizon, measured in Planck length units.
Expressing the results in Section 1.5.4 in terms of Boltzmann’s entropy, we have
HB ≤kBπr2
ℓ2p
loge.
(11.81)
Compared to the gravitational entropy bound for a black body (11.80), the bound (11.81)
is somewhat more generous, allowing entropy to scale with the area, rather than the
area to the power 3/4, expressed in Planck length units. The bound (11.81) holds for
arbitrary systems, and a black body, like many other systems, falls short of saturating
it. Other trivial examples that fall short of reaching the holographic bound are a sphere
surrounding empty space or a crystal at zero temperature, which both have zero entropy.
For systems far from gravitational collapse, gravitational bounds such as (11.81) and
(11.80) are generally loose. Most systems of “reasonable” energy have a much smaller
entropy that can be bounded directly in terms of their energy.
11.8.2
The Universal Entropy Bound
A general bound for matter systems of reasonable size and energy is the universal
entropy bound, introduced by Jacob Bekenstein in 1981. This states that the entropy
of any matter system enclosed in a sphere of radius r with energy at most E is
HB ≤kB2πrE
¯hc
loge.
(11.82)
Imposing gravitational stability, we have
E ≤c4r
2G,
(11.83)
which substituted into (11.82) recovers the holographic bound (11.81). The Bekenstein
bound, however, is intended for weak self-gravitating systems. These are systems far
from gravitational collapse for which E ≪c4r/(2G), and for those it provides a tighter
entropy bound than the holographic one.
As we shall see in Chapter 13, while the black body entropy formula (11.70) applies
to a maximum-entropy electromagnetic system radiating at all frequencies according
to Planck’s law (11.29b), the universal entropy bound (11.82) can be achieved by a
maximum-entropy electromagnetic system radiating photons over a ﬁxed bandwidth. In
both cases the entropy is proportional to the volume of the body, but the energy scalings
13
12:02:41, subject to the Cambridge Core

11.9 Entropy of Black Holes
331
Table 11.1 Entropy limits.
Black body
Black body
General
General
classical
gravitational
holographic
universal
HC =
4
3
5/4
(απ)1/4(rE)3/4
HC ≤α′
 r
ℓp
3/2
H ≤π
 r
ℓp
2
loge
H ≤2πrE
¯hc
loge
are different. While the energy of a black body grows with the cube of the radius of
the radiating body, in the case of bandlimited radiation achieving the universal entropy
bound, the energy grows only with the square of the radius.
The different entropy bounds are summarized in Table 11.1. Compared to what is
achieved with current technologies, these bounds are all extremely lenient. For example,
a current ﬂash memory card of size of the order of a centimeter, and weight of a
few grams, can store about one terabyte of data, that is, of the order of 1012 bits. In
comparison, expressing the universal bound (11.82) in bits, rather than energy over
temperature, and using E = mc2, we obtain the entropy bound
H ≤2πrE
¯hc
loge = 2πrmc
¯h
loge ≈1038 bits.
(11.84)
What allows this bound to be so large is that it requires conversion of all matter into
radiation carrying information, and by doing so it considers all available degrees of
freedom at the most fundamental level.
If we were to increase the mass of our memory card and bring it to the verge of
becoming a black hole, the entropy would be limited by the holographic bound,
H ≤πr2
ℓ2p
loge ≈1065 bits,
(11.85)
which is 53 orders of magnitude more than the current technology and 27 orders of
magnitude more than the universal bound for objects of reasonable energy. Clearly,
these bounds, although important for matters of principle, are of no great practical
use for current technologies, and are not likely to be for many years to come. To
view the whole information content of our hypothetical memory card on the verge
of gravitational collapse, we would need to convert all matter into radiation and
observe a waveform on a cut-set boundary surrounding our object carrying 1065 bits
of information. This would require radiation at a wavelength of
λ ≈1/
√
1065 ≈10−33 m.
(11.86)
11.9
Entropy of Black Holes
The Bekenstein–Hawking formula for the entropy of a black hole is at the basis of the
derivation of the holographic entropy bound. This formula states that the entropy of a
black hole is proportional to the surface area of its event horizon, measured in Planck
13
12:02:41, subject to the Cambridge Core

332
Noise Processes
length units. The holographic bound then states that the entropy of any system cannot
be larger than the entropy of a black hole of the same size.
The Bekenstein–Hawking result can be obtained from a clever physical argument
based on quantum uncertainty relations. This was Bekenstein’s original argument and
we describe it below. From the time–frequency uncertainty principle for a single photon
(11.59), we have
σtσω ≥1
2,
(11.87)
from which we have the localization–frequency uncertainty
σℓσω ≥c
2
(11.88)
and the localization–wavelength uncertainty
2π σℓ
σλ
≥1
2.
(11.89)
It follows that a lower bound on the uncertainty of the localization is
σℓ≥σλ
4π .
(11.90)
We now choose a wavelength of radiation λ ≈4πrS for a photon directed at a black
hole of radius rS, so that when the photon is localized within the radius of the black
hole, it disappears inside the hole. In this way, when the photon disappears, knowledge
of its existence within a radius rS of the center of the hole is lost. We assume that after
the photon falls in, we have no information about whether the photon exists or not inside
the hole, and we have lost one bit of information.
The energy increment of the black hole due the photon is
E = ¯hω = ¯h2πc
4πrS
.
(11.91)
The new radius of the black hole resulting from adding this amount of energy is
r′
S = rS + r
= 2Gm
c2
+ 2G
c2
E
c2
= 2Gm
c2
+ 2G¯h2π
c34πrS
.
(11.92)
The new area of the horizon due to the increase in radius is
A′ = A + A
= 4π(rS + r)2
= 4πr2
S(1 + r/rS)2
= 4πr2
S + 4πr2
S
8
2r/rS + (r/rS)29
.
(11.93)
13
12:02:41, subject to the Cambridge Core

11.10 Maximum Entropy Distributions
333
It follows, by combining (11.93), (11.92), and (11.76), that
A = 4πr2
S
(
4G¯h2π
c34πr2
S
+
2G¯h2π
c34πr2
S
24
= 8πℓ2
p + 4πℓ4
p/r2
S.
(11.94)
The key observation now is that the ﬁrst term in (11.94) is independent of the size of
the black hole. Ignoring the second term, which is much smaller than the ﬁrst one, it
follows that adding one bit of information increases the area of the horizon of a black
hole of any size by an amount proportional to ℓ2
p. This suggests that, adding information
bit by bit, the total amount of entropy in our black hole is
H ≈4πr2
S
8πℓ2p
=
A
8πℓ2p
bits,
(11.95)
which is proportional to the area of the event horizon, measured in Planck length units.
The proportionality constant 1/(8π) is the one originally obtained by Bekenstein’s
(1973) calculation, and is only approximate. It is based on the assumption that the
localization of the photon occurs precisely within a radius rS, while in reality this cannot
be so sharply determined because it is only expressed in terms of standard deviation.
Bekenstein (1973) noticed that the approximation is the price to pay for having given
a semi-classical argument and not a full quantum one. A more detailed calculation by
Hawking (1975) obtained
H = 4πr2
S
4ℓ2p
loge bits.
(11.96)
This result leaves open the question of what the actual physical states present in a
black hole are. In statistical mechanics, physical states are identiﬁed as conﬁgurations
of elementary particles, or radiating modes in a black body. Providing a similar
interpretation for black holes is a topic of current research in physics, with some recent
proposals made in the areas of string theory.
11.10
Maximum Entropy Distributions
We conclude this chapter by deriving the statistics of the noise process as maximum
entropy distributions subject to energy constraints. Since the energy of a signal is related
to its average power, to its magnitude, and to its waveform shape, we show that the
maximum entropy distributions associated with the noise process are all related to each
other through elementary transformations of random variables. We consider a frequency
regime ¯hω ≪kBTK, so that quantum effects can be ignored and we can assume
continuous probability densities. We then show the required modiﬁcations to obtain
the analogous results valid for all frequencies, taking into account quantum effects. We
do not provide a fully rigorous treatment in terms of generalized functions, and instead
resort to a less rigorous but more intuitive argument that emphasizes physical aspects.
Let E be the energy of a thermally excited standing wave of angular frequency ω
composing the noise signal. The average radiated power at angular frequency ω, which
13
12:02:41, subject to the Cambridge Core

334
Noise Processes
has the dimensions of energy per unit time, is proportional to the energy of the standing
wave via a coupling coefﬁcient indicating the elementary bandwidth over which it is
radiated, and we have
P = Edω
2π .
(11.97)
In this way, the energy of the standing wave can be interpreted as the average radiated
power per unit frequency, namely the average power of a single sinusoid,
x(t) = Acos(ωt + φ), A ≥0,
(11.98)
which is given by
P = A2
2 .
(11.99)
From (11.99) and (11.97), we have the differential relationships
dP = AdA,
(11.100)
dEdω = 2πdP.
(11.101)
We now use these relationships to derive the distribution of the noise starting from the
exponential distribution of the energy of each mode given by (11.14). It follows from
(11.101) that the average power is distributed as
fP(p) =
2π
kBTKdω exp(−2πp/(kBTKdω)),
(11.102)
and from (11.100) that the amplitude is distributed according to the Rayleigh probability
density function
gA(a) =
2πa
kBTKdω exp(−2πa2/(2kBTKdω)), a ≥0.
(11.103)
We now turn to considering the distribution of the random phasor representing the
noise signal,
A = Aexp(jφ).
(11.104)
Since the thermal excitations of different modes are not synchronized in time, φ can be
assumed to be distributed uniformly at random according to maximum entropy with the
only constraint being that it lies inside the interval [0,2π]. The probability that the tip
of the phasor lies in an elementary area dS at distance between a and a + da from the
origin is
qA(m)dS =
dS
2πadagA(a)da = gA(a)
2πa dS;
(11.105)
see Figure 11.10. It follows that the probability density function of the phasor over the
plane is
qA(a) = gA(a)
2πa =
1
kBTKdω exp(−2πa2/(2kBTKdω)),
(11.106)
which is a two-dimensional, circularly symmetric Gaussian distribution. This density
13
12:02:41, subject to the Cambridge Core

11.10 Maximum Entropy Distributions
335
dS 
A
A
(
)
A
(
)
X 
jY
O 
Fig. 11.10
A phasor of Rayleigh-distributed magnitude and uniform phase is distributed as a circularly
symmetric complex Gaussian over the complex plane.
depends only on the magnitude A and not on the phase φ of the signal. When viewed in
the complex plane, it corresponds to a zero-mean vector oriented uniformly in [0,2π]
with a Rayleigh-distributed magnitude and variance:
E(A2) = 2E(P) = kBTKdω/π.
(11.107)
Since A2 = ℜ(A)2 + ℑ(A)2, the two-dimensional distribution factorizes into two
mono-dimensional Gaussians,
qA(a) =
1
kBTKdω exp(−2π(ℜ(a)2 + ℑ(a)2)/(2kBTKdω))
=
1
√kBTKdω exp(−2π(ℜ(a)2/(2kBTKdω))
1
√kBTKdω exp(−2π(ℑ(a)2/(2kBTKdω)),
(11.108)
that are in the form (1/
√
2πσ 2)exp(−x2/(2σ 2)), with σ = √kBTKdω/2π, x ∈
{ℜ(a),ℑ(a)}, and correspond to the distributions of the real and imaginary parts of the
phasor.
We now consider the distribution of the instantaneous real signal produced by the
random thermal excitation of a single mode,
X(t) = ℜ(Aexp(jωt)) = Acos(ωt + φ).
(11.109)
13
12:02:41, subject to the Cambridge Core

336
Noise Processes
Multiplication of the phasor by exp(jωt) corresponds to a rotation in the complex
plane and does not change its distribution, which depends only on the magnitude.
We then obtain the Gaussian probability density function for the real signal X(t)
representing the instantaneous amplitude of the noise,
qX(x) =
1
√kBTKdω exp(−2πx2/(2kBTKdω)).
(11.110)
Finally, if the noise is composed of the superposition of 2/dω thermally excited
modes, each distributed as X(t),
Z(t) =
2/dω

n=1
Xn(t).
(11.111)
Assuming independence of modes separated in frequency by an elementary interval dω,
the resulting distribution is still Gaussian, and we have
qZ(z) =
1
√kBTK2 exp(−2πx2/(2kBTK2)),
(11.112)
where the variance has been scaled by the number of modes 2/dω. The expected
power of the noise, accounting for the contribution from all modes, is then given by
E(Z2) = kBTK2/2π.
(11.113)
Throughout, we have assumed ¯hω/(kBTK) ≪1, so that we could deal with continuous
distributions. This results in the two drawbacks already mentioned: an average power
that increases with the bandwidth, and an impulsive autocorrelation function that allows
independence of modes separated by an arbitrarily small frequency interval. Using
the discrete distribution described in Section 11.2.3 and going through the same steps
substituting the quantum mechanical correction (11.28) for the frequency-dependent
expected energy of a mode in place of kBTK leads to a colored Gaussian distribution for
the noise, with expected power
E(Z2) = 1
2π
 
−
¯hω
exp(¯hω/(kBTK)) −1dω,
(11.114)
rather than the one in (11.113).
11.11
Summary and Further Reading
Noise provides a fundamental limitation on the resolution at which we can detect
electromagnetic signals. Thermal radiation occurs due to random agitations of charges
inside media. The precise spectrum of the noise signal can be determined by considering
the superposition of elementary radiating electromagnetic modes with appropriate
boundary conditions, and accounting for the quantized levels of the electromagnetic
radiation. The stochastic distribution of the noise follows from basic thermodynamics
13
12:02:41, subject to the Cambridge Core

11.11 Summary and Further Reading
337
considerations. The second law provides an interpretation of the noise signal in terms
of maximum uncertainty subject to imposed physical constraints. Shot noise is related
to quantum limitations arising from Heisenberg’s uncertainty principle, and provides an
ultimate irreducible uncertainty limit.
A classic reference for the treatment of black body radiation is the book by Kittel
and Kroemer (1980). The ﬁrst volume of the Feynman lectures on physics (Feynman,
Leighton, and Sands, 1964), of course, also covers thermal radiation. A theoretical
treatment of thermal and quantum noises from an engineering perspective is given in the
classic paper of Oliver (1965). The statistical mechanics interpretation of thermal noise
is due to Nyquist (1928). Connections between statistical entropy and thermodynamic
entropy have a long and rich history, dating back to Boltzmann (1872); see Ufﬁnk
(2008) for a historical account. Modern theories allow for a temporary decrease of
entropy in systems in dynamical evolution, while the increase in entropy always holds
macroscopically, as the system evolves from one state of thermal equilibrium to another.
In this context, ﬂuctuation theorems give quantitative estimates for the probability of
a decrease in statistical entropy in systems in dynamical evolution. In our treatment
we considered equilibrium states only, where thermodynamic entropy and statistical
entropies coincide.
A different point of view was taken by Cover (1994), who studied, independent of
the physics, the class of ﬁnite state Markov stochastic processes for which the second
law holds. He showed that statistical entropy increases over time only for Markov
processes for which the equilibrium distribution is uniform over the ﬁnite state space.
He argued that an appropriate generalized statement of the second law in the context
of all stochastic processes should be that the relative entropy of the current distribution
with respect to the stationary distribution always decreases.
The connection between statistical entropy and thermodynamic entropy at equi-
librium via the asymptotic equipartition property was argued by Jaynes (1965). The
asymptotic equipartition property was ﬁrst stated by Shannon (1948), who proved
the result for i.i.d. processes and stated it for stationary ergodic processes. Several
extensions are provided in the literature, including for ergodic countable processes and
ergodic, real-valued ones. Jaynes (1982) argued for the use of the maximum entropy
principle in a broader context. More recently, connections between thermodynamic
and statistical notions of entropy led to many advances in engineering and computer
science, described, for example, in the book of Mézard and Montanari (2009). Further
connections between statistical physics and information theory appear in Merhav
(2010).
For a detailed introduction to the holographic information bound see Susskind (1995).
A review of information limits due to gravitation with a focus on the holographic
information bound and its extensions appears in Bousso (2002). The universal entropy
bound is due to Bekenstein (1981). The calculation of the entropy of black holes is
due to Bekenstein (1973) and Hawking (1975), who settled the proportionality constant
at 1/4. The microscopic origin of black hole entropy has been investigated from a
string-theoretic perspective by Strominger and Vafa (1996).
13
12:02:41, subject to the Cambridge Core

338
Noise Processes
11.12
Test Your Understanding
Problems
11.1
Show that the deviation from the mean Y = N −α of a Poisson random variable
N of mean α approaches, for large values of α, a zero-mean Gaussian of variance α.
Solution
We have the Poisson probability mass function
pN(n) = αn exp(−α)
n!
.
(11.115)
Substituting n = α + y into (11.115), we get the probability density function
pY(y) = α(α+y) exp(−α)
(α + y)!
.
(11.116)
By using Stirling’s formula, for large values of α this is approximately
α(α+y) exp(−α)
(α + y)(α+y) exp[−(α + y)]√2π(α + y) =
exp(y)
√2π(α + y)
α(α+y)
(α + y)(α+y)
=
exp(y)
√2π(α + y)

1 + 1
α/y
−(α+y)
=
exp(y)
√2π(α + y)
(
1 + 1
α/y
−α/y4 y2+αy
α
.
(11.117)
For large values of α, the right-most term tends to exp(−y)exp(−y2/α), and (α+y) ≈α.
It follows that the probability density function of the deviation from the mean becomes
approximately
pY(y) = exp(−y2/α)
√
2πα
.
(11.118)
A more rigorous derivation can be performed considering the characteristic function of
the random variable Yα = (N −α)/√α,
φYα(t) = exp[−jt√α + α(exp(jt/√α) −1)],
(11.119)
and showing that
lim
α→∞φYα(t) = exp(−t2/2),
(11.120)
which is the characteristic function of the standard zero-mean, unit-variance, Gaussian.
11.2
Show that among all continuous random variables deﬁned over a real interval and
having ﬁnite differential entropy, the differential entropy is maximized only for those
having uniform distribution over that interval.
13
12:02:41, subject to the Cambridge Core

11.12 Test Your Understanding
339
11.3
Show that among all continuous random variables deﬁned over the positive reals
and having ﬁnite differential entropy and mean μ, the differential entropy is maximized
only for exponentials.
11.4
Prove the information inequality (11.49) using Jensen’s inequality.
11.5
Prove the chain rule (11.50).
11.6
Show that the ratio of typical states to the total number of states in a system
approaches zero exponentially in the number of states. Thus, for large systems, the vast
majority of states are atypical. Their probabilistic contribution, however, is negligible.
11.7
In the asymptotic equipartition property, the typical set depends on the choice of
ϵ. By (11.46), as ϵ →0, the elements in the typical set become equiprobable, but for
the probability of deviation from the typical set in (11.45) to be negligible, we require a
larger number M of random variables. Show at what rate ϵ can be decreased as M →∞
to tighten the bounds in (11.46).
Solution
Apply the Chebyshev inequality to bound the deviation in (11.45) for all M, and express
ϵ as a function of M.
11.8
Consider a system composed of a continuous set of elementary modes that
are stochastic signals of N0 degrees of freedom. Use the continuum version of the
asymptotic equipartition property described in Section 1.4.2 to choose a distribution
for the signals’ coefﬁcients that is consistent with the second law. Compare your results
with the probabilistic interpretation of the second law given in Section 11.3.4.
Solution
The system of signals is described by N0 i.i.d. stochastic coefﬁcients. The typical
volume of the state space is then roughly
V = 2N0h,
(11.121)
where h = hX(f) is the differential entropy of each coefﬁcient, having density function
f. Assuming signals are observed quantized at level ϵ, we obtain a number of observable
typical states
N = 2N0h
ϵN0 ,
(11.122)
that are all roughly equiprobable. It follows that the cardinality of the observable typical
set is an exponential function of the differential entropy h and of the number N0 of
elementary modes of the system. The entropy of the observable typical set is
H = N0 log 2h
ϵ .
(11.123)
We now choose a Gaussian probability distribution for the signals’ coefﬁcients that
maximizes the differential entropy subject to the constraint E(X2) ≤P, so that
h = log(
√
2πeP),
(11.124)
13
12:02:41, subject to the Cambridge Core

340
Noise Processes
and substituting into (11.123) this choice maximizes the entropy of the observable
typical set,
H = N0 log
√
2πeP
ϵ
.
(11.125)
For any given signal to quantum resolution constraint, the chosen distribution
maximizes the entropy of an equivalent thermodynamic system in equilibrium formed
by the typical quantized states, and in this case the entropy is proportional to the
number of degrees of freedom. This probabilistic interpretation is consistent with the
one provided in Section 11.3.4 for a discrete ensemble of elementary modes. Comparing
(11.123) and (11.125) with (11.42), it follows that N0 plays the role of the number M of
elementary modes of the system, and h−logϵ plays the role of the entropy H(p) of each
quantized mode of radiation. In Chapter 13 we relate the quantum resolution ϵ to the
frequency of radiation, and obtain an expression for the maximum entropy of radiation
for a system of given size over a ﬁxed frequency band.
11.9
Complete the computation of the integral in (11.65).
11.10
Compute the value of the constant α′ in (11.80), using kB = 1.381×10−23 J K−1,
ℓp = 1.62 × 10−35 m, ¯h = 1.055 × 10−34 J s, and verify that it is dimensionless.
11.11
Verify that for a black body of the size of the sun, r ≈6.96 × 108 m, and
temperature T ≈5800 K, the entropy in (11.69) proportional to the volume is much
smaller than the gravitational bound (11.80). Provide an example of a body of given
temperature and radius that reaches the gravitational bound (11.80).
11.12
Wien’s displacement law states that the black body radiation curve for different
temperatures peaks at a wavelength inversely proportional to the temperature. Provide
the precise formula.
11.13
Compute the expected power of a black body per unit wavelength dλ, per unit
surface area dA, and radiated in all directions out from dA. This corresponds to the
irradiance of a black body and requires extending (11.17) to all outgoing directions
from the surface of the body.
Solution
Consider radiation of the black body per unit wavelength, per unit surface area, and in a
direction forming an angle θ with the normal to the surface, as depicted in Figure 11.11.
Letting dA be the area subtended by the angle d on the surface of the body, the
projected area orthogonal to the direction of observation is dAcosθ. By (11.17), the
radiation per unit wavelength in the direction of P, and per unit projected area, is
SZ(λ) = c
4π
∂2u(λ)
∂λ∂V .
(11.126)
It follows that the radiation per unit wavelength in the direction of P, and per unit area
dA on the surface of the body, is
SZ(λ,θ) = SZ(λ)cosθ.
(11.127)
13
12:02:41, subject to the Cambridge Core

11.12 Test Your Understanding
341
P
dr
z
y
O
O
z
y
r
r
2
dA
Fig. 11.11
Geometry of the radiated energy at an angle θ with the normal to the surface of the body as seen
by a detector at P.
P
P
O
O
Fig. 11.12
Conservation of radiance. The detector observes the source through an elementary solid angle
d and at an angle θ with the normal to the surface of the body. The source radiates through an
elementary solid angle d towards a detector at an angle θ with the normal to the surface of the
body.
We now compute the total radiation per unit wavelength in all directions through dA
by letting P vary over the hemisphere U surrounding the area dA – see Figure 11.12.
By conservation of radiance, the radiation emitted by the source at an angle θ into an
elementary solid angle d is the same as that received by a detector observing the
source at the same angle θ, through an elementary solid angle, and since
d = sinθdθdφ,
(11.128)
we obtain the irradiance

U
SZ(λ,θ)d = SZ(λ)
 2π
0
 π/2
o
cosθ sinθdφdθ = πSZ(λ).
(11.129)
This leads to Planck’s formula per unit area on the surface of the body and the associated
Rayleigh–Jeans limit,
πSZ(λ) = (2πc)2
λ5
¯h
exp
 2π ¯hc
λkBTK

−1
≈2πckBTK
λ4
,
(11.130)
13
12:02:41, subject to the Cambridge Core

342
Noise Processes
which differ from (11.30b) and (11.20) by the factor π.
11.14
The Stefan–Boltzmann law states that the power radiated per unit surface area
by a black body in all directions and over all frequencies is proportional to the fourth
power of the temperature. Provide the precise formula.
Solution
Assuming isotropic radiation of energy in all directions, a c/4π fraction of the energy
per unit volume passes per unit time through the elementary area dA subtended by a
solid angle d in the direction of the observer on the surface of the body. Adding a
factor of π to account for radiation in all directions, as explained in Problem 11.13, and
using (11.66), we obtain
∂P
∂A = E
V
c
4π π = c
4αT4 =
π2
15¯h34c2 (kBTK)4.
(11.131)
13
12:02:41, subject to the Cambridge Core

12
Information-Theoretic Quantities
Nec scire fas est omnia.1
12.1
Communication Using Signals
The act of communication occurs by encoding a message we wish to send at the
transmitter, and decoding it at the receiver. The encoding process translates the message
into a form suitable for transmission over a given channel. For us, the channel allows
transmission and reception of real electromagnetic signals, so that the translation
process amounts to representing the message by a point in the signals’ space described
in Section 2.2.4. By transmitting the signal, we identify one point in the space
corresponding to a given message, and communicate a certain amount of information
that is at most equal to the entropy of the space.
A continuum signals’ space, however, contains an uncountable number of points
and a single signal can represent inﬁnitely many possible messages. It follows that the
amount of information transferred in the communication process is unbounded.
To limit the amount of information, we impose a regularity constraint on the space
of transmitted signals. This constraint is expressed in terms of bandlimitation of the
spectral support of the signal, which is reﬂected in a limitation on the number of
coordinates required to identify it. As discussed in Chapter 2, any signal in B can
be identiﬁed by essentially N0 = T/π real numbers as N0 →∞. It follows that the
encoding process translates the message into a signal’s codeword composed of roughly
N0 real numbers. The decoding process translates the received signal back into the
corresponding message by observing the N0 real numbers of the codeword. By (3.16)
it follows that the communication rate achieved by transmitting a bandlimited signal of
ﬁnite energy, and observing it with arbitrary accuracy in the interval [−T/2,T/2], is
given by
lim
T→∞
N0 + O(logN0)
T
= 
π reals per second.
(12.1)
Measuring information using real numbers may seem, however, somewhat artiﬁcial,
because a real number carries inﬁnite precision and can be speciﬁed by using an inﬁnite
number of bits. If we insist on making binary digits suitable units of information, we
1 Horace, Carmina IV (circa 30 BC).
14
12:22:54, subject to the Cambridge Core

344
Information-Theoretic Quantities
also need to introduce a limit on the resolution at which signals can be observed. A
remarkable intuition of Shannon was that the noise poses a resolution limit at which
different signals can be reliably distinguished, and ensures that only a ﬁnite number of
bits per second can be transmitted reliably.
In the presence of the noise, only a “corrupt” version of the signal can be received, so
that the decoding process can only “guess” what the transmitted signal was. A ﬁrst-order
model of the noise process is additive, white Gaussian. Without delving into the
mathematical sophistications required to develop a rigorous spectral theory of random
functions, in Chapter 11 we have argued that it is possible to think about the white
Gaussian process as producing essentially independent Gaussian random variables at
distinct time intervals. This physically corresponds to having a coupling between the
values of the noise that is essentially zero at any two instants of time separated by more
than a tiny interval. As shown in Figure 2.3, each point of the signals’ space is perturbed
by a certain random distance that is roughly proportional to the standard deviation of
the noise, and this produces a certain region of uncertainty, providing a resolution limit
at which one can distinguish different signals. In order to avoid decoding errors, signals
must be separated enough that the corresponding uncertainty regions do not overlap.
However, because of their limited energy, signals cannot be separated arbitrarily far in
space. It then follows that for any channel with a given noise variance, and an available
energy budget, one can only communicate at most at a ﬁnite bit rate: the capacity of the
channel.
12.2
Shannon Capacity
The Shannon capacity is deﬁned as the largest rate of communication that can be
achieved between a transmitter and a receiver with vanishing probability of error. When
presenting his theory in the context of continuum waveforms, Shannon considered
transmitting a bandlimited signal f(t) ∈B, subject to an energy constraint that scales
linearly with the number of dimensions,
 T/2
−T/2
f 2(t)dt ≤PN0.
(12.2)
He also assumed the transmitted signal to be essentially timelimited, in the sense that
only a negligible fraction of its energy falls outside the interval [−T/2,T/2]. Finally, he
assumed the received signal g(t) to be corrupted by additive noise z(t), so that
g(t) = f(t) + z(t).
(12.3)
Shannon then asked what the largest amount of information is that can be transferred
by sending f(t) and receiving g(t), over an interval of T seconds.
To answer this question, he considered a codebook composed of a subset of
waveforms in the space, each corresponding to a given message. He let a transmitter
select any one of these signals, and a receiver observe the same signal corrupted by the
noise. He then noticed that for many reasonable noise models one could choose signals
14
12:22:54, subject to the Cambridge Core

12.2 Shannon Capacity
345
in the codebook sufﬁciently separated from each other that the receiver could recover
the message with an arbitrarily small probability of error.
Shannon deﬁned the capacity, measured in bits, as the logarithm base two of the
largest number of messages Mδ(P) that can be communicated with probability of error
at most δ > 0:
C(δ) = logMδ(P) bits.
(12.4)
Clearly, this depends on the choice of the noise model and on the value of δ. For a
Gaussian noise model, he showed that the capacity measured per unit time over a large
time interval,
C = lim
T→∞
logMδ(P)
T
bits per second,
(12.5)
is proportional to the bandwidth and to the logarithm of the energy of the signal divided
by the energy of the noise; most surprisingly, it is independent of δ.
Shannon’s result is striking. A priori, it is not even clear whether it is possible to
communicate at a non-zero rate with arbitrarily small error probability. It may very
well be that to decrease the error, the amount of spacing we need to add between the
codewords in the signals’ space to combat the noise drives the rate to zero. Indeed, in
the early days of communication theory it was believed that the only way to decrease the
probability of error was to proportionally reduce the rate. Shannon showed this belief
to be incorrect: by accurately choosing the encoding and decoding processes, one can
communicate at a strictly positive rate, and at the same time with as small a probability
of error as desired. Furthermore, there is a highest achievable critical rate C, called the
capacity of the channel, for which this can be done. If one attempts to communicate at
rates above the channel capacity, then it is impossible to do so with arbitrarily low error
probability.
To give a more precise mathematical description of this result, we need to provide
more details on the communication model. We consider the noise z(t) to be the
realization of a white Gaussian noise process Z(t) having power spectral density
SZ(ω) =
 N
|ω| ≤
0
|ω| > .
(12.6)
This stochastic process has ﬁnite average power
E(Z2(t)) = sZ(0) = 1
2π
 ∞
−∞
SZ(ω)dω = N
π ,
(12.7)
where sZ is the autocorrelation of the noise. The average energy of the noise within the
observation interval [−T/2,T/2] is given by
 T/2
−T/2
E(Z2(t))dt = NT
π
= NN0.
(12.8)
14
12:22:54, subject to the Cambridge Core

346
Information-Theoretic Quantities
+
+
+
+
f (t)
z(t)
g(t)
x1
z1
x2
z2
xN
zN
0
0
yN0
y2
y1
Fig. 12.1
The additive Gaussian channel and its discrete representation.
By combining (12.2) and (12.8), we deﬁne the signal-to-noise ratio in Shannon’s model
to be
SNRS = P/N.
(12.9)
Since signals are essentially timelimited, the input can also be expressed as a linear
combination of N0 orthonormal bandlimited basis functions,
f(t) =
N0

n=1
xnψn(t).
(12.10)
In this case, the energy constraint (12.2) essentially corresponds to
N0

n=1
x2
n ≤PN0,
(12.11)
and the projection of the noise onto the space spanned by the basis functions forms
a discrete i.i.d. Gaussian process that adds an independent Gaussian random variable
of zero mean and variance N to each signal’s coordinate xn, so that the continuum
time communication model (12.3) becomes the discrete superposition of N0 parallel
communication channels,
yn = xn + zn,
(12.12)
where n ∈{1,2,...,N0}; see Figure 12.1.
14
12:22:54, subject to the Cambridge Core

12.2 Shannon Capacity
347
Using this model, Shannon showed that:
It is possible to communicate information at the rate
C < 
2π log

1 + SNRS

bits per second
(12.13)
with the probability of error tending to zero as T →∞, while it is not possible to
communicate at a higher rate than C with vanishing probability of error.
The result is obtained by exploiting two main arguments called sphere packing and
random coding. First, one observes that the noise turns the coded signal point into a
cloud sphere of an essentially ﬁxed radius. Then, by counting how many disjoint noise
spheres ﬁt into the whole signals’ space, one argues that no reliable transmission at a
rate larger than the capacity is possible. The second argument relies on the probabilistic
method, which gained prominence in combinatorial mathematics around the same time
that Shannon’s information theory came about. This is a non-constructive method for
proving the existence of a prescribed kind of mathematical object. It works by showing
that if one randomly chooses objects from a speciﬁed class, the probability of picking
the “right” object of the prescribed kind is larger than zero. Although the proof uses
probability, the ﬁnal conclusion of existence is certain.
In our case, the mathematical object picked at random is a particular encoding system.
A given selection of points in the signals’ space depicted in Figure 2.3 corresponds
to a particular encoding system. In order to show the existence of an encoding system
capable of transmitting C bits per second with vanishing probability of error, it is enough
to show that by performing the selection of M = 2CT points at random inside the sphere,
the probability of error in the transmission of the messages associated with such points
vanishes as T →∞. It then follows that using a random encoding system it is possible
to communicate with a vanishing probability of error at rate (logM)/T = C bits per
second, so that there must exist a particular coding scheme that allows transmission
at such a rate and with probability of error tending to zero. In Shannon’s (1949) own
words:
It turns out, rather surprisingly, that it is possible to choose our M signal functions at random from
the points inside the sphere of radius √PN0, and achieve the most that is possible.
In fact, not only does this argument show that there exists a capacity-achieving code,
but since a random selection leads to a capacity-achieving code, almost all codes must
share this property.
12.2.1
Sphere Packing
The main idea of the sphere packing argument is that the noise places a resolution
limit on the possible signals that can be distinguished at the receiver. In the geometrical
representation of signals, each signal point is surrounded by a small region of
uncertainty due to the noise. The perturbations of the different coordinates in the
14
12:22:54, subject to the Cambridge Core

348
Information-Theoretic Quantities
(P+N)
N


√
√
Fig. 12.2
The geometry of Shannon’s outer bound.
signals’ space are Gaussian and independent. Let us focus on the difference between
the transmitted signal and the received one, namely on the differences between the
coordinates of the corresponding points. The joint probability density function of the
coefﬁcients,
(y1 −x1,y2 −x2,...,yN0 −xN0) = (z1,z2,...,zN0),
(12.14)
is given by the product of N0 Gaussian densities, which depends only on the sum

N0
i=1 z2
i . This shows that the region of uncertainty about each received point in the
signals’ space is spherical. Furthermore, for large T, the noise perturbation about each
point probabilistically concentrates around its typical value √N0N. This means that
while ﬂuctuations of the value of the perturbation are possible, these tend to vanish
as T →∞, so that the received signal lies with high probability on the surface of a
hard sphere of radius √N0N centered at the transmitted point. By a similar argument,
since the received signals have an average power at most P + N, they all lie with
high probability inside the sphere of radius √N0(P + N). In order to recover different
transmitted codewords without error, we need to space them sufﬁciently far apart so
that their noise-perturbed versions are still distinguishable. An upper bound on the
number of distinguishable signals is then given by the volume of the sphere of radius
√N0(P + N) divided by the volume of the sphere of radius √N0N, since overlap of the
noise spheres results in message confusion. A geometric picture is given in Figure 12.2.
It follows that an upper bound on the number of distinguishable signals is given by
M ≤

P + N
N
N0
,
(12.15)
and the rate is bounded by
C = logM
T
≤
2π log

1 + P
N

bits per second.
(12.16)
14
12:22:54, subject to the Cambridge Core

12.2 Shannon Capacity
349
O
B
A
N
PN




P+N
(P+N)
P
h=
C
√
√
√
Fig. 12.3
The geometry of Shannon’s inner bound.
12.2.2
Random Coding
The main idea of the random coding argument is to let the M encoded signals be picked
uniformly at random inside the sphere of radius √N0P. The ratio of the volume of the
shell between r and r −ϵ, for ϵ ∈(0,r), and the whole sphere is given by
rN0 −(r −ϵ)N0
rN0
= 1 −

1 −ϵ
r
N0 ,
(12.17)
and tends to one as N0 →∞. It follows that in high dimensions nearly all the volume
is very close to the surface, so that the transmitted signals are, with high probability,
very close to the surface of the sphere and by symmetry they all have the same
probability of error. Figure 12.3 shows a cross section through the high-dimensional
sphere deﬁned by a typically transmitted signal B, received signal A, and the origin
O. The high-dimensional lens-shaped region whose boundary is highlighted in gray,
identiﬁed by the intersection of the surface of the signals’ space sphere of radius √N0P
and the surface of the sphere of radius √N0N centered in A, contains all possible signals
that might have caused A, since the distance between a transmitted and a received signal
is concentrated around √N0N. The volume of this region is smaller than that of the
sphere of radius ρ = BC/2. The radius ρ can be computed by solving the following
equation, where the area of the triangle OAB appears on both sides:
1
2ρ
	
N0(P + N) = 1
2
	
N0P
	
N0N,
(12.18)
yielding
ρ =

N0
PN
P + N .
(12.19)
Since the M encoded signals are chosen uniformly at random among the points inside
the signals’ space of radius √N0P, it follows that the probability of a signal point being
14
12:22:54, subject to the Cambridge Core

350
Information-Theoretic Quantities
inside the lens-shaped region is less than the ratio of the volume of the signals’ space
sphere and the volume of the sphere of radius ρ, and it is given by
⎛
⎜⎜⎝

N0
PN
P + N
√N0P
⎞
⎟⎟⎠
N0
=

N
P + N
N0/2
.
(12.20)
The transmitted signal B is decoded correctly when the remaining M −1 possible
encoded signals are all outside the lens-shaped region. By the union bound, the
probability of this latter event can be made greater than (1 −δ) by appropriately
choosing M, such that
1 −(M −1)

N
P + N
N0/2
> (1 −δ),
(12.21)
from which it follows that we need
(M −1) <
P + N
N
N0/2
δ.
(12.22)
By taking logarithms on both sides and dividing by T, we have
log(M −1)
T
< 
2π log

1 + P
N

+ logδ
T
.
(12.23)
It follows that as T →∞any communication rate R = (logM)/T arbitrarily close to the
capacity C = (/2π)log(1 + P/N) can be achieved.
12.2.3
Capacity and Mutual Information
Shannon also showed that the capacity of the channel coincides with the largest mutual
information between a transmitted signal and the corresponding received one corrupted
by the noise. This result is known as Shannon’s channel coding theorem, and links
an operational quantity, such as the largest achievable communication rate, with an
information-theoretic one, such as the largest mutual information between transmitted
and received signals.
The mutual information between a single transmitted random coefﬁcient of the signal
and the corresponding received one is deﬁned as the residual uncertainty associated
with the transmitted coefﬁcient, given the received one. Viewing the uncertainty as
the “surprise” that a transmitter can generate at the receiver by selecting one of the
possible signals in the constellation, this also corresponds to the amount of information
transferred in the communication process. It follows that the capacity can be obtained
by maximizing the mutual information over all possible distributions of the coefﬁcients
compatible with the given power constraint.
To rephrase this intuition in the language of mathematics, we consider a bandlimited
signal f(t) corrupted by additive white Gaussian noise. By (12.10), we can view this
signal as a point in high-dimensional space subject to the constraint (12.11). To deﬁne
the mutual information, we introduce a probability distribution for the input coefﬁcients,
14
12:22:54, subject to the Cambridge Core

12.2 Shannon Capacity
351
and we let {xn} be realizations of i.i.d. random variables {Xn} having probability density
function pX(x). As discussed in Chapter 1, probabilistic concentration ensures that the
constraint (12.11) is essentially equivalent, as N0 →∞, to
E(X2) ≤P,
(12.24)
so that we can maximize the mutual information over all input distributions subject to
the second moment constraint (12.24).
The mutual information between random variables X and Y is deﬁned in terms of the
differential entropy,
hX = −
 ∞
−∞
pX(x)logpX(x)dx,
(12.25)
and conditional differential entropy,
hX|Y = −
 ∞
−∞
pX,Y(x,y)log(pX|Y(x,y))dxdy,
(12.26)
as
I(X;Y) = hX −hX|Y,
(12.27)
and can be interpreted as the residual uncertainty associated with X once Y is known.
Shannon’s coding theorem is stated as follows:
The information capacity of the channel Y = X + Z with average power constraint
P is
C =
sup
pX(x):E(X2)≤P
I(X;Y) bits.
(12.28)
Consider now the parallel channels used to transmit N0 coefﬁcients of a bandlimited
signal. For each elementary channel, we have
I(X;Y) = hX −hX|Y
= hY −hX+Z|X
= hY −hZ|X
= hY −hZ,
(12.29)
where the last equality follows from Z being independent of X. Since X and Y are also
independent, we have
E(Y2) = E(X + Z)2 = E(X2) + N ≤P + N.
(12.30)
Recalling from Problem 1.15 that a zero-mean Gaussian random variable maximizes
the differential entropy for a given variance constraint, and
hZ = 1
2 log(2πeN),
(12.31)
14
12:22:54, subject to the Cambridge Core

352
Information-Theoretic Quantities
we have, from (12.29),
I(X;Y) ≤1
2 log[2πe(P + N)] −1
2 log(2πeN)
= 1
2 log

1 + P
N

bits,
(12.32)
where equality is attained when X is a zero-mean Gaussian random variable with
variance P. This expression can be interpreted as the largest amount of information
transferred in a realization of one of the parallel channels in Figure 12.1.
Transmitting a bandlimited signal corresponds to having N0 = T/π parallel
realizations, each with energy per degree of freedom P and noise variance per degree of
freedom N, yielding a capacity
C = N0
2 log

1 + SNRS

bits,
(12.33)
or, in terms of bits per degree of freedom,
C = 1
2 log

1 + SNRS

bits per degree of freedom,
(12.34)
and in bits per unit time,
C = 
2π log

1 + SNRS

bits per second,
(12.35)
which coincides with the expression of the operational capacity given earlier.
12.2.4
Limiting Regimes
The Shannon capacity shows a “law of diminishing returns” as SNRS is increased.
While for low values of SNRS the capacity grows linearly with SNRS, for large values
it grows only logarithmically with SNRS; see Figure 12.4. In the ﬁrst case we are
in a power-limited regime and a small increase in the transmitted power results in a
corresponding increase in capacity, while in the second case the capacity is practically
insensitive to increases in power.
The low-SNRS regime was investigated in Section 1.5.1. In that case, rather than
(12.11), we assumed the ﬁxed energy constraint
N0

n=1
x2
n ≤E,
(12.36)
yielding a signal-to-noise ratio
SNRS =
E
N0N ,
(12.37)
which tends to zero as N0 →∞. It follows that the capacity in this low-SNRS regime
is proportional to the energy, and vanishes as N0 →∞:
C = 1
2 log

1 +
E
N0N

14
12:22:54, subject to the Cambridge Core

12.2 Shannon Capacity
353
≃
E
2N0N loge bits per degree of freedom.
(12.38)
To obtain a low-SNRS regime with a non-vanishing capacity, we can replace the
energy constraint (12.36) with
 T/2
−T/2
f 2(t)dt ≤¯PT,
(12.39)
where ¯P has the units of energy per unit time. This new constraint essentially
corresponds to
N0

n=1
x2
n ≤¯PT,
(12.40)
and we have
SNRS =
¯PT
N0N = π ¯PT
TN = π ¯P
N .
(12.41)
It follows that in this case SNRS tends to zero as N →∞, and if we keep N ﬁxed and
let  →∞, the capacity in the wide-band regime tends to a constant proportional to the
signal’s average power, namely
C = 
2π log

1 + π ¯P
N

≃
2π
π ¯P
N loge
=
¯P
2N loge bits per second.
(12.42)
All of the above results differ in the applied energy constraint. With the constraint
(12.11), SNRS is independent of N0, and the formula (12.35) holds. With the constraint
C
0
SNRS
HIGH-SNR
REGIME
LOW-SNR
REGIME
S
S
Fig. 12.4
Capacity of the additive white Gaussian channel.
14
12:22:54, subject to the Cambridge Core

354
Information-Theoretic Quantities
(12.36), SNRS vanishes as N0 →∞, and leads to the capacity formula (12.38). With
the constraint (12.40), SNRS vanishes as N →∞and leads to the capacity formula
(12.42).
12.2.5
Quantum Constraints
The capacity results described above hold only in the regime where the quantum nature
of radiation can be neglected. To illustrate additional quantum constraints, recall from
the discussion in Chapter 11 that for the white noise assumption to hold we need kBTK ≫
¯hω for all radiated frequencies, where kB is the Boltzmann constant, ¯h is the reduced
Planck constant, and TK is the absolute temperature of the noise, so that (11.31) can
be used as a valid approximation for the noise. It follows that if we transmit over an
interval of frequencies [−,], we need
N = kBTK ≫¯h,
(12.43)
and this leads to the noise-to-bandwidth constraint,
N
 ≫¯h.
(12.44)
Assuming the energy constraint (12.39), and plugging (12.44) into (12.35), we get
C = 
2π log

1 + π ¯P
N

≤
¯P
2N loge
≪
¯P
2¯h loge bits per second,
(12.45)
where the ﬁrst inequality follows from log(1 + x) ≤xloge for all x > 0.
In the wide-band regime, the ﬁrst inequality in (12.45) is tight because N ≫P, and
we have
C ≃
¯P
2N loge ≪
¯P
2¯h loge.
(12.46)
It follows that in the wide-band regime the capacity can be proportional to the average
power only when the power spectral density of the noise is much larger than ¯h. When
this physical constraint is violated, (12.42) no longer holds and, as we shall see below,
the capacity becomes proportional to the square root of the average power.
For radio communications, and typical values of the frequency and temperature, the
white noise assumption holds and quantum mechanical effects can indeed be neglected.
For example, for transmission up to the GHz range at a temperature of 300 K, we have
that ¯h ≈10−25, while kBTK is of the order of 10−21. This value of kBTK can be regarded
as a minimum noise attainable by an ideal receiver at a temperature around 300 K.
Noise is usually greater by some factor expressed in decibels and known as the noise
ﬁgure. It follows that (12.43) is typically satisﬁed in many operating conditions. For low
temperature and high frequency of radiation, however, quantum effects kick in, noise
14
12:22:54, subject to the Cambridge Core

12.2 Shannon Capacity
355
distributions different from the Gaussian one become more appropriate, including the
Poisson shot-noise models discussed in Chapter 11, and different capacity computations
are required.
12.2.6
Capacity of the Noiseless Photon Channel
For small values of the noise compared to the largest frequency of radiation times the
reduced Planck constant, (12.43) is violated and the white Gaussian noise model must
be replaced with a more appropriate quantum noise model. In the limiting case of the
absence of noise, the signal itself is subject to quantized radiation constraints; in this
case the capacity reduces to the entropy per unit time of the waveform reaching the
receiver, and is proportional to the square root of the average radiated power.
To illustrate this result, let us model the signal as a stochastic process f(t) of given
average power
E(f2(t)) = 1
2π
 ∞
−∞
(SZ(ω))dω = ¯P.
(12.47)
The maximum entropy of the quantized process is achieved by black body radiation
in one dimension, having a power spectral density of the form (11.29a), and discrete
energy per mode of distribution (11.26). Assuming a single polarization state, the
capacity can then be computed to be
C =

¯Pπ
3¯h loge bits per second.
(12.48)
A simple derivation of the functional form in (12.48) is given as follows. Assume
that each radiated photon has a characteristic frequency ωc and carries an energy ¯hωc.
In a modulated communication system, ωc corresponds to the carrier frequency of the
signal and ¯hωc to the minimum detectable energy of a single mode of radiation at that
frequency. Assume that the detection of each photon carries one bit of information, and
occurs at the time scale of one signal oscillation. Then, we have
¯P/(¯hωc) ≈C ≈ωc;
(12.49)
namely, the capacity is roughly of the same order as the average number of radiated
photons per second and the number of oscillations per second. By transmitting one
quantum of energy per second, the signal carries at most one bit per second, and evolves
at a rate of one cycle per second. Eliminating ωc from (12.49) leads to
¯P ≈¯hC2,
(12.50)
from which it follows that
C ≈

¯P
¯h ,
(12.51)
which has the same functional form as (12.48).
14
12:22:54, subject to the Cambridge Core

356
Information-Theoretic Quantities
This simple derivation indicates the optimal transmission strategy, captured by the
slogan: “one quantum–one bit–one mode.”
To achieve capacity one can modulate the signal once a period, by transmitting a
sequence of sinusoidal modes, each of which lasts roughly one period. The effective
bandwidth of the modulated signal corresponds to an interval of frequencies ranging
from zero, corresponding to transmitting a ﬁxed oscillation at frequency ωc, to a
maximum value ωc, corresponding to transmitting an oscillation varying at the same
time scale as the carrier. It follows that achieving capacity requires transmission over a
bandwidth that is of the same order as the characteristic frequency of radiation.
A more rigorous derivation of the capacity of the noiseless photon channel (12.48)
can be obtained using a statistical mechanical argument to maximize the number
of possible discrete energy conﬁgurations that the one-dimensional signal can have,
by allocating a given number of quanta of energy ¯hω at each frequency, subject to
an average power constraint. This approach was used in Chapter 11 to compute the
maximum entropy of three-dimensional radiating bodies. In the one-dimensional case,
it leads to a capacity of the form (12.51) that corresponds to the entropy rate of black
body radiation in one dimension.
12.2.7
Colored Gaussian Noise
In some frequency regimes a colored Gaussian distribution that decays at high
frequencies may be a more appropriate model of the thermal noise, and the capacity
results should be modiﬁed accordingly.
In this case, by dividing the band into small sub-bands where the spectral density of
the noise is approximately constant, the total capacity is given by
C =
 
0
1
2π log

1 + P(ω)
N′(ω)

dω,
(12.52)
where N′ is the one-sided power spectral density of the noise, and P(ω) is the power
allocation of the signal, chosen to maximize (12.52) subject to the constraint
 
0
P(ω)dω = ¯P.
(12.53)
The problem is standard in the calculus of variations, and amenable to a Lagrangian
formulation. Letting (·)+ = max{0,·}, the optimal solution corresponds to a piecewise
constant value of the power,
P(ω) = [κ −N′(ω)]+,
(12.54)
with κ satisfying
 
0
[κ −N′(ω)]+dω = ¯P.
(12.55)
14
12:22:54, subject to the Cambridge Core

12.2 Shannon Capacity
357
0
ω

N′(ω)
κ
P(ω)
Fig. 12.5
Water ﬁlling interpretation of the optimal power allocation.
Substituting the optimal power allocation (12.54) into (12.52), we obtain the capacity
of the additive (colored) Gaussian channel,
C =
 
0
1
2π log

1 + [κ −N′(ω)]+
N′(ω)

dω.
(12.56)
The solution has the graphical interpretation given in Figure 12.5. We can think of the
total energy of the signal as being poured into a reservoir whose bottom is shaped by
the power spectral density of the noise. The level to which the water rises is κ, while
P(ω) is the depth of the water at various locations in the reservoir.
If one varies the shape of N(ω), while keeping its integral across the spectrum
constant, and adjusts P(ω) to obtain the maximum transmission rate, it turns out that the
worst shape is indeed the constant one corresponding to white Gaussian noise. Namely,
white Gaussian is the “worst noise” among all possible Gaussian noises from a capacity
standpoint. This result holds in greater generality, as the white Gaussian noise is the
worst kind of additive noise among all noises of a given variance.
12.2.8
Minimum Energy Transmission
We have so far considered the highest reliable transmission rate under a given energy
constraint. We now assume a given transmission rate and determine the minimum
energy per bit needed to achieve it. This leads to the classic Shannon limit on the energy
needed to reliably communicate one bit of information over a white Gaussian channel.
Consider transmission of bits at rate R, and average energy per bit Eb. The average
energy per unit time is ¯P = REb. Using (12.35), we have
R ≤
2π log

1 + π ¯P
N

= 
2π log

1 + 2πREb
N′

,
(12.57)
14
12:22:54, subject to the Cambridge Core

358
Information-Theoretic Quantities
Eb
log  (e)
–1
0
Q
Fig. 12.6
Shannon’s minimum energy limit.
where N′ = 2N is the one-sided power spectral density. From (12.57), we have
2πR

≤log

1 + 2πREb
N′

.
(12.58)
The quantity
Q = 2πR/
(12.59)
is the spectral efﬁciency, namely the rate of transmission per unit frequency. Substituting
(12.59) into (12.58), it follows that
Eb
N′ ≥2Q −1
Q
.
(12.60)
This gives a bound on the normalized energy per bit of information when transmitting at
rate R over the Gaussian channel using signals in B. As Q →0, the function converges
to its minimum,
E∗
b
N′ = log−1(e) = 0.6931...,
(12.61)
where E∗
b represents Shannon’s energy limit in the classical regime: the minimum
energy necessary to transmit one bit of information is a factor of about 0.7 times the
one-sided noise spectral density; see Figure 12.6.
12.3
A More Rigorous Formulation
All the results described so far swept under the carpet a number of issues required
to provide a completely rigorous mathematical formulation. We have argued that
communication occurs over a time interval of length T, and that results hold in the
limit of T →∞, provided that a negligible amount of the signal’s energy falls outside
the interval [−T/2,T/2]. However, any strictly bandlimited signal has an inﬁnite time
duration, and in the computation of the capacity we have not accounted for the degrees
of freedom present in the vanishing tail of the signal. A rigorous derivation requires
14
12:22:54, subject to the Cambridge Core

12.3 A More Rigorous Formulation
359
a more careful account, and use of the notions of time concentration and frequency
concentration developed in Chapter 2.
Fortunately, the phase transition of the number of degrees of freedom ensures that
Shannon’s results are robust, and do not change signiﬁcantly when a more precise
derivation is performed. On the other hand, a more detailed account reveals that more
accurate noise models must be developed to provide a sound physical interpretation of
the results.
12.3.1
Timelimited Signals
Consider ﬁrst a signal strictly timelimited to [−T/2,T/2], and subject to the energy
constraint
 ∞
−∞
f 2(t)dt =
 T/2
−T/2
f 2(t)dt ≤¯PT.
(12.62)
This signal is spread over an inﬁnite bandwidth, but its frequency concentration over
the interval [−,], as deﬁned in (2.53), is given by β2() ≥1 −ϵ2
; see Figure 12.7.
By considering white Gaussian noise of constant power spectral density
SZ(ω) = N
(12.63)
to be added to the signal over all frequencies, we obtain a more precise version of
Shannon’s formula:
C = 
2π log

1 + (1 −ϵ2
) π ¯P
N

+ ϵ2

¯P
2N loge bits per second.
(12.64)
By letting ϵ →0, the signal’s energy tends to be concentrated inside the band and
the capacity reduces to the familiar form (12.35), where SNRS is given by (12.41).
As ϵ →1 and more and more of the signal’s energy falls outside the band, the
capacity approaches that obtained in the wide-band regime (12.42). On the other hand,
for any 0 < ϵ < 1 we have two contributions to the capacity: a contribution due to
the concentrated part of the signal that scales logarithmically with the average power
(1 −ϵ2
)¯P, and a contribution due to the unconcentrated part that scales linearly with
the average power ϵ2
 ¯P. As ϵ →0, the latter part disappears and the whole contribution
is due to the concentrated part of the signal only.
The generalized formula (12.64) is obtained by expanding the signal into a prolate
spheroidal basis set and then summing the contribution to capacity due to the ﬁrst N0
coefﬁcients associated with the concentrated part of the signal and the contribution of
the remaining coefﬁcients associated with the unconcentrated part of the signal. The
ﬁrst contribution occurs over N0 degrees of freedom and is of the form of (12.35). The
second contribution occurs over an inﬁnite number of degrees of freedom and is of the
form of (12.42). A completely rigorous derivation requires using the phase transition
results presented in Chapters 2 and 3. We provide here a sketch of the main steps.
14
12:22:54, subject to the Cambridge Core

360
Information-Theoretic Quantities
~~
~~
|F(ω)|2

0
∞
∞
−
Ω
−
(1−     )PT

2
Fig. 12.7
Spectral concentration of a timelimited signal. The area of the shaded region is at most
(1 −ϵ2
)¯PT.
By allocating the energy (1 −ϵ2
)¯PT over the ﬁrst N0 = T/π degrees of freedom,
the ﬁrst contribution to the capacity is
C1 = 
2π log

1 + (1 −ϵ2
)
¯PT
N0N

= 
2π log

1 + (1 −ϵ2
) π ¯P
N

bits per second.
(12.65)
Letting ∞> , by allocating the remaining ϵ2
T ¯PT energy over (∞−)T/π
degrees of freedom, a second contribution to the capacity is
C∞
2 = ∞−
2π
log

1 +
ϵ2
π ¯P
N(∞−)

bits per second.
(12.66)
Finally, letting ∞→∞, we have
C2 =
lim
∞→∞C∞
2
= ∞−
2π
ϵ2
π ¯P
N(∞−) loge
= ϵ2

¯P
2N loge bits per second.
(12.67)
The total capacity is now given by the sum of (12.65) and (12.67), which coincides with
(12.64).
12.3.2
Bandlimited Signals
An analogous result is obtained for bandlimited signals of inﬁnite time duration, of
energy constraint
 ∞
−∞
f 2(t)dt ≤¯PT,
(12.68)
and of time concentration α2(T) ≥1 −ϵ2
T, as deﬁned in (2.52) and depicted in
Figure 12.8. In this case, we need to distinguish two different time scales. The ﬁrst
time scale is representative of the concentrated part of the signal and corresponds to the
14
12:22:54, subject to the Cambridge Core

12.3 A More Rigorous Formulation
361
T/2
–T/2
0
T  ∞
~~
~~
(1–ϵ )PT
T
2
f  (t)
2
/2
–T  ∞/2
Fig. 12.8
Time concentration of a bandlimited signal. The area of the shaded region is at most (1−ϵ2
T)¯PT.
time duration T, which is chosen large enough so that with the ﬁrst part of the signal we
can communicate
C1 = T
2π log

1 + (1 −ϵ2
T) π ¯P
N

bits
(12.69)
with an arbitrarily small error probability. The second time scale is representative of the
inﬁnite tail of the signal and corresponds to the interval (T∞−T). Letting T∞→∞and
having ﬁxed T, at this time scale we can communicate
C2 =
lim
T∞→∞
(T∞−T)
2π
log

1 + ϵ2
T
π ¯PT
(T∞−T)N

= (T∞−T)
2π
πϵ2
T ¯PT
(T∞−T)N loge
= ϵ2
T ¯PT
2N loge bits.
(12.70)
It follows that the total number of bits communicated by transmitting a single
bandlimited signal of inﬁnite duration is given by the sum of (12.69) and (12.70).
A problem now arises in the deﬁnition of communication rate. If we simply sum
(12.69) and (12.70) and divide by T, we obtain
C = 
2π log

1 + (1 −ϵ2
T) π ¯P
N

+ ϵ2
T
¯P
2N loge bits per second,
(12.71)
which is the analog of (12.64) for bandlimited, rather than timelimited, signals. This
formula, however, has a limited physical meaning. Since any bandlimited signal has
inﬁnite duration, the interpretation of communication rate over any interval of ﬁxed
length T is problematic. When viewed at the scale of T∞the signal carries, strictly
speaking, zero capacity. This follows by summing (12.69) and (12.70), dividing by T∞,
and taking the limit for T∞→∞. Since the energy constraint does not scale with T∞,
similar to the case in (12.38), the number of bits per unit time tends to zero. On the other
hand, in the timelimited signal model leading to (12.64), where the linear contribution
to the capacity is due to the inﬁnite tail of the signal in the frequency domain
14
12:22:54, subject to the Cambridge Core

362
Information-Theoretic Quantities
rather than in the time domain, a meaningful notion of communication rate can be
deﬁned.
12.3.3
Refined Noise Models
We now summarize. A communication model based on bandlimited signals does not
allow a rigorous deﬁnition of communication rate, since each transmitted signal has an
inﬁnite time duration. A communication model based on timelimited signals is perhaps
more natural, as it allows an operational interpretation of the capacity in terms of
communication rate using signals of ﬁnite time, leading to (12.64). However, a problem
arises in this case as well: since noise of constant power spectral density is added over
all frequencies of a signal of inﬁnite bandwidth, the average power of the noise must be
inﬁnite. As we have discussed in Chapter 11, this is not a good physical model and it
leads to the ultraviolet catastrophe paradox.
It follows that to carefully account for the spectral concentration properties of signals
and fully generalize Shannon’s formula, we need to develop more accurate models of the
noise process. Ideally, we would like to work with timelimited signals and bandlimited
noise, so that we could have a meaningful deﬁnition of communication rate as well as
avoiding physical impossibilities in the noise model. Any noise model that is zero in
some frequency range, however, would immediately lead to unbounded capacity. By
allocating some signal power over this frequency range, SNRS and hence the capacity
would be arbitrarily high.
One way to circumvent these difﬁculties has been anticipated in Sections 12.2.5 and
12.2.6. We could assume a Gaussian noise model that vanishes outside a certain range
of frequencies, as well as a quantum noise model that kicks in at high frequencies. The
Gaussian model may account for the amount of information carried by the concentrated
part of the signal, while the quantum model may describe the amount of information
carried by radiation at high frequencies, where quantum constraints are dominant. When
the Gaussian noise vanishes and only quantum constraints are present, the capacity must
become proportional to the square root of the average radiated power, and reduce to
(12.48).
Another possibility that retains a continuous signal modeling approach but still
provides valuable physical insight is to assume that the noise is given by the
superposition of a bandlimited Gaussian component acting within the frequency interval
[−,], as well as a small Gaussian residual noise acting out of band. The bandlimited
component may approximate thermal noise, while the small noise ﬂoor may represent
an inherent measurement uncertainty acting on the degrees of freedom of the signal
occurring within the unconcentrated part the spectrum.
If the model of the noise is the one depicted in Figure 12.9, given by
SZ(ω) =
 N
|ω| ≤,
ν2
N
|ω| > ,
(12.72)
14
12:22:54, subject to the Cambridge Core

12.3 A More Rigorous Formulation
363

0
~~
~~
S  (ω)
−
ω
z
N  
ν 2
N  
Fig. 12.9
Bandlimited noise with vanishing ﬂoor.
then we have
C = 
2π log

1 + (1 −ϵ2
) π ¯P
N

+
ϵ
ν
2 ¯P
2N loge bits per second.
(12.73)
If we now assume ν →0, then the noise becomes essentially bandlimited and physi-
cally well deﬁned, but the capacity diverges because the fraction ϵ2
 of unconcentrated
energy of the signal lies in a portion of the spectrum where there is vanishing noise
and thus can be used to communicate at inﬁnite rate. However, if ϵ approaches zero as
well, then the capacity can remain bounded.
By interpreting ν and ϵ as limits on the measurement accuracy of the high
frequency components, the former measuring the noise and the latter measuring the
signal, it is reasonable to assume that they tend to zero at the same rate, as T →∞.
In this case, a ﬁrst contribution to capacity is due to the concentrated part of the signal
that is affected by essentially constant noise on each degree of freedom, and as T →∞
it tends to Shannon’s original result obtained for a bandlimited white Gaussian noise
process. A second contribution is due to the unconcentrated part of the signal and,
assuming that both the out-of-band signal and noise contributions tend to zero at the
same rate, this contribution tends to a constant value, as T →∞.
Shannon’s classic formula and its extension are compared in Table 12.1. The
second term in the extended formula quantiﬁes the ability to capture the higher-order
components in the phase transition of the number of degrees of freedom of the signal,
and use them to communicate information. If the signal-to-noise ratio in the tail of
the transition window of the number of degrees of freedom is large, this constant
term can be substantial. In the context of spatial signals, an analogous term arises
in the computation of the capacity of imaging systems where sensors attempt to
capture the vanishing tail of the number of degrees of freedom of the signal in the
space–wavenumber domain to improve the quality of the constructed image, and it leads
to the super-resolution gain that we discussed in Section 1.2.4.
It is important to point out that all technologies developed in this framework must
obey the bounds imposed by the laws of physics and information theory that are based
on spectral concentration occurring in the appropriate asymptotic regimes. While the
14
12:22:54, subject to the Cambridge Core

364
Information-Theoretic Quantities
Table 12.1 Shannon capacity.
Classical
Extended
C = 
2π log

1 + π ¯P
N

C = 
2π log

1 + (1 −ϵ2
) π ¯P
N

+(ϵ/ν)2 ¯P
2N loge
amount of super-resolution depends on the properties of the noise model, any reasonable
model dictates that unbounded super-resolution is a physical impossibility, since the
second term of (12.71) must always remain bounded, even if ν →0.
12.4
Shannon Entropy
We now turn to consider another information-theoretic notion introduced by Shannon:
the entropy, which captures the “surprise” associated with a given stochastic realization,
as discussed in Section 1.4.
While the Shannon capacity is closely related to the problem of packing the signals’
space with balls of a given radius, the entropy is closely related to the geometric problem
of covering the space with balls of a given radius. Each source signal, modeled as a
stochastic process, corresponds to a random point in the space, and by quantizing all
coordinates at a given resolution, the Shannon entropy corresponds to the number of
bits needed on average to represent the quantized signal. Thus, the entropy depends
on both the probability distribution of the process and the quantization step along the
coordinates of the space.
A quantizer, however, does not need to act independently on each coordinate, and
can be more generally viewed as a discrete set of balls covering the space. The source
signal is represented by the closest center of a ball covering it, and the distance to the
center of the ball represents the distortion measure associated with this representation.
In this context, the Shannon rate–distortion function provides the minimum number of
bits that must be speciﬁed per unit time to represent the source process with a given
average distortion.
12.4.1
Rate–Distortion Function
We consider a zero-mean white Gaussian stochastic process f(t) of constant power
spectral density P of support [−,]:
Sf(ω) =
 P
|ω| ≤,
0
|ω| > .
(12.74)
This stochastic process has ﬁnite average power
E(f2(t)) = sf(0) = 1
2π
 ∞
−∞
Sf(ω)dω = P
π ,
(12.75)
14
12:22:54, subject to the Cambridge Core

12.4 Shannon Entropy
365
f
f
PN0
√
NN0
√
Fig. 12.10
Codebook representation of a stochastic process.
where sf is the autocorrelation of the process. When observed over the interval
[−T/2,T/2], the process can be viewed as a random point having essentially N0
independent Gaussian coordinates of zero mean and variance P, and distributed inside
the ball of radius
 T/2
−T/2
E(f2(t))dt
1/2
=
PT
π
1/2
=
	
PN0.
(12.76)
A source codebook is composed of a subset of points in the space, and each codebook
point is a possible representation of the stochastic process. The distortion associated
with the representation of f(t) using codebook point ˆf(t) is deﬁned in terms of
mean-squared energy,
d(f,ˆf) =
 T/2
−T/2
E[f(t) −ˆf(t)]2dt.
(12.77)
We let LN(P) be the smallest number of codebook points that can be used to
represent the source process with distortion at most NN0. This roughly corresponds to
the minimum number of balls of radius √NN0 that cover the whole ball of radius √PN0;
see Figure 12.10. The deﬁnition of LN(P) should be compared with that of Mδ(P) used
for capacity, representing the largest number of messages that can be communicated
with probability of error at most δ, which roughly corresponds to the maximum number
of balls of radius √NN0 that can be packed inside the ball of radius √PN0.
The rate–distortion function is then deﬁned as
RN = lim
T→∞
logLN(P)
T
bits per second,
(12.78)
and it represents the minimum number of bits per unit time needed to represent the
stochastic process with distortion at most NN0.
14
12:22:54, subject to the Cambridge Core

366
Information-Theoretic Quantities
Letting SNRS = P/N be the signal-to-distortion ratio, Shannon’s formula for the
rate–distortion function of this Gaussian source is
RN =
⎧
⎨
⎩

2π log(SNRS) bits per second
if P ≥N,
0
if P < N.
(12.79)
The rate–distortion formula (12.79) should be compared with the capacity formula
(12.35). Both formulas scale linearly with the bandwidth and logarithmically with
the signal-to-noise ratio. It should also be compared with the entropy formula (1.46).
While the entropy considers the number of bits needed on average to represent a
stochastic process quantized at ﬁxed resolution along each dimension of the space, the
rate–distortion function considers a more general quantization model where signals are
represented by codebook points that are arbitrarily located inside the signals’ space, and
represents the number of bits needed to approximate the stochastic process with a given
distortion value.
The geometric insight behind (12.79) should be clear. The minimum number of balls
needed to cover the whole space is given by at least the volume of the ball of radius
√N0P divided by the volume of the ball of radius √N0N, and we have
LN(P) ≥

P
N
N0
.
(12.80)
By taking the logarithm and dividing by T, a lower bound matching (12.79) follows. A
rigorous proof complements the lower bound with a random coding argument, similar
to the one given for the capacity, showing the achievability of this minimum rate.
Namely, random coding shows that there exists a collection of balls of cardinality at
most (P/N)N0/2 that covers the whole space with high probability, as T →∞.
12.4.2
Rate–Distortion and Mutual Information
The operational deﬁnitions of capacity and rate–distortion have geometric interpreta-
tions in terms of maximum-cardinality sphere packing and minimum-cardinality sphere
covering of the space. The corresponding information-theoretic interpretations are given
in terms of the largest mutual information between transmitted and received signals, and
the smallest mutual information between a signal and its perturbed representation. We
discussed the information-theoretic interpretation of capacity in Section 12.2.3. We now
give an analogous interpretation for the rate–distortion.
We begin by considering the smallest mutual information between a Gaussian random
variable X and its codebook representation ˆX,
RN(X; ˆX) =
inf
p ˆX|X:E[( ˆX−X)2]≤N
I(X; ˆX) bits,
(12.81)
14
12:22:54, subject to the Cambridge Core

12.4 Shannon Entropy
367
where p ˆX|X is the conditional density of the codebook representation ˆX, given X. This
naturally extends to a sequence {Xn} of N0 independent random variables distributed as
X, and we have
RN({Xn};{ ˆXn}) = N0RN(X; ˆX) bits.
(12.82)
As N0 →∞the mutual information I({Xn};{ ˆXn}) can be identiﬁed with the mutual
information between the stochastic processes f(t) and ˆf(t), and (12.82) represents the
growth rate of the corresponding rate–distortion function. When this is measured per
unit time it agrees with (12.79).
To prove this latter statement, we ﬁrst compute a lower bound on the mutual
information:
I(X; ˆX) = hX −hX| ˆX
= 1
2 log(2πeP) −hX−ˆX| ˆX
≥1
2 log(2πeP) −hX−ˆX
≥1
2 log(2πeP) −hZ,
(12.83)
where Z is a Gaussian random variable of zero mean and variance E[(X −ˆX)2], which
maximizes the differential entropy hX−ˆX. Since E[(X −ˆX)2] ≤N, we get
I(X; ˆX) ≥1
2 log(2πeP) −1
2 log(2πeN)
≥1
2 log P
N .
(12.84)
Next, to ﬁnd the conditional density p ˆX|X that achieves this lower bound, assuming
N ≤P, we choose
X = ˆX + Z,
(12.85)
where ˆX is a Gaussian random variable of mean zero and variance P −N, Z is another
Gaussian of mean zero and variance N, and ˆX and Z are independent of each other. The
resulting “test channel” that gives the original signal from the estimate by adding noise
is depicted in Figure 12.11. It follows that we have E[(X −ˆX)2] = N, and
I(X; ˆX) = hX −hX| ˆX
= hX −hZ
= 1
2 log(2πeP) −1
2 log(2πeN)
= 1
2 log P
N ,
(12.86)
achieving the lower bound (12.84). If N ≤P, we can choose ˆX = 0 with probability one,
so that the rate–distortion function is zero.
14
12:22:54, subject to the Cambridge Core

368
Information-Theoretic Quantities
+
X
X
Z~
~
~
(0,N)
(0,P)
(0,P-N)
Fig. 12.11
Density achieving the lower bound for the rate–distortion function.
RN
N
P
O
Fig. 12.12
Rate–distortion function for a Gaussian source.
Substituting (12.86) into (12.82), dividing by T, and taking the limit for T →∞, we
obtain (12.79). A plot of the rate–distortion function for a Gaussian source is depicted
in Figure 12.12. We can also invert (12.79) and obtain the distortion in terms of rate,
N = P2−2RNπ/,
(12.87)
showing that each additional bit of description of the stochastic process reduces the
distortion per degree of freedom by a factor of four. This exponential decrease of
the distortion with the rate should be compared to the exponential decrease of the
probability of error for rates below the capacity in the computation performed in
Section 12.2.2.
12.5
Kolmogorov’s Deterministic Quantities
The stochastic notions of entropy and capacity put forth by Shannon can be related
to analogous ones developed by the Russian school of Kolmogorov in a deterministic
setting.
A ﬁrst deterministic information-theoretic notion is the Kolmogorov N-width that we
introduced in Chapter 3. For bandlimited signals, the existence of a steep transition point
in the N-width leads to the deﬁnition of the number of degrees of freedom that describes
the effective “size” or “massiveness” of the space. This number can be computed in a
precise asymptotic setting by solving Slepian’s concentration problem. The relationship
14
12:22:54, subject to the Cambridge Core

12.5 Kolmogorov’s Deterministic Quantities
369
between the number of degrees of freedom and the largest achievable information rate
when using bandlimited signals for communication is evident in Shannon’s formulas,
C = lim
T→∞
N0
T log(1 + SNRS) bits per second,
(12.88)
RN = lim
T→∞
N0
T log(SNRS) bits per second,
(12.89)
where the number of degrees of freedom appears in front of the logarithmic term. The
number of degrees of freedom corresponds to the effective number of parallel channels
available in the space, and the logarithmic term reﬂects the power constraint.
In addition to the N-width, Kolmogorov introduced notions of capacity and entropy
in the purely deterministic setting of functional approximation. This led to the
development of a functional theory of information that, as mentioned in Chapter 1,
has several points of contact with the probabilistic theory of information developed in
the West.
12.5.1
ϵ-Coverings, ϵ-Nets, and ϵ-Entropy
Let A be a subset of a metric space X , and let ϵ > 0. A family {U1,U2,...,UQ} of
subsets of X is called an ϵ-covering of A if the largest distance between any two points
in Uk is at most 2ϵ and if the family covers A, namely A ⊂∪Q
k=1Uk. The minimum
number of sets in an ϵ-covering is an invariant of the set A, which depends only on ϵ,
and is denoted by Qϵ(A). The ϵ-entropy of A is deﬁned as the base two logarithm
Hϵ(A) = logQϵ(A).
(12.90)
This is also called the metric entropy, in contradistinction to the statistical entropy used
in Shannon’s theory.
A set of points {x1,x2,...,xP} of X is called an ϵ-net for A if for each x ∈A there is
at least an xk of the net at a distance from x not exceeding ϵ, namely ∥x −xk∥≤ϵ. The
minimum number of points in an ϵ-net is denoted by QX
ϵ (A) and may also depend, as
the notation indicates, on the larger space X in which A is contained. The ϵ-entropy of
A relative to X is deﬁned as the base two logarithm
HX
ϵ (A) = logQX
ϵ (A).
(12.91)
For centerable spaces, ϵ-coverings naturally induce ϵ-nets and the two entropies
deﬁned above coincide. A set U ⊂X is centerable in X if there exists a center point
x0 ∈X such that ∥x −x0∥≤ϵ for all x ∈U. If all sets Uk are centerable, then we can
think about them as “balls” and their centers form an ϵ-net for A. It follows that, in this
case,
HX
ϵ (A) = Hϵ(A).
(12.92)
Figure 12.13 provides an illustration of an ϵ-covering and the associated ϵ-net.
14
12:22:54, subject to the Cambridge Core

370
Information-Theoretic Quantities
2
xk
Uk
A
Fig. 12.13
An ϵ-covering of the set A and its associated ϵ-net.
yk
A
Fig. 12.14
A set of ϵ-distinguishable points for the set A.
12.5.2
ϵ-Distinguishable Sets and ϵ-Capacity
Points y1,y2,...,yM of X are called ϵ-distinguishable if the distance between any two
of them exceeds ϵ, namely ∥yi −yk∥> ϵ for all i ̸= k. The maximum number of
ϵ-distinguishable points is an invariant of A that depends only on ϵ, and is denoted
by Mϵ(A). The ϵ-capacity of A is deﬁned as the base two logarithm
Cϵ(A) = logMϵ(A).
(12.93)
Figure 12.14 provides an illustration of distinguishable points for a set A.
12.5.3
Relation Between ϵ-Entropy and ϵ-Capacity
A general relation between entropies and capacities is
C2ϵ(A) ≤Hϵ(A) ≤HX
ϵ (A) ≤Cϵ(A).
(12.94)
14
12:22:54, subject to the Cambridge Core

12.6 Basic Deterministic–Stochastic Model Relations
371
It follows that a typical technique to estimate entropy and capacity is to ﬁnd a lower
bound for C2ϵ and an upper bound for Hϵ, and if these are close to each other, then they
are good estimates for both capacity and entropy.
The basic relation (12.94) is obtained as follows. Since every ϵ-net also induces an
ϵ-covering, we have
Qϵ(A) ≤QX
ϵ (A).
(12.95)
Given a set of 2ϵ-distinguishable points of A of cardinality m and an ϵ-covering of A
of cardinality n, then we have m ≤n, otherwise two distinguishable points would be
contained in the same element of the covering set, which is impossible. This situation
is illustrated in Figure 1.13.
It follows that
M2ϵ(A) ≤Qϵ(A).
(12.96)
Finally, a maximal set of ϵ-distinguishable points of A of cardinality m = Mϵ(A) also
form an ϵ-net for A, so that
QX
ϵ (A) ≤Mϵ(A).
(12.97)
Taking logarithms of (12.95), (12.96), and (12.97), we obtain (12.94).
12.6
Basic Deterministic–Stochastic Model Relations
To investigate the relationship between the Shannon and Kolmogorov entropies and
capacities, we consider an ϵ-bounded noise channel and revisit Shannon’s results in this
setting.
12.6.1
Capacity
Consider a probabilistic communication setting in which the input signal belongs to a
subset A of a metric space X and the output signal belongs to the same metric space,
but is perturbed by random noise that has bounded support. Namely, with probability
one the output signal is at most at distance ϵ from the input signal. This means that
given a class P of probability distributions of the input signals,
P ⊆{PX : PX(B) = P(X ∈B)},
(12.98)
where B ⊂A, we can use the probabilistic kernel
K(x,Q) = P(Y ∈Q|X = x)
(12.99)
to represent the channel in Figure 12.15 with the output distribution
P(Y ∈Q) =

A
K(x,Q)PX(dx)
(12.100)
14
12:22:54, subject to the Cambridge Core

372
Information-Theoretic Quantities
K(x,Q)
X
Y
Fig. 12.15
A probabilistic channel.
and the joint distribution
P[(X,Y) ∈U] = PXY(dx,dy) =

U
K(x,dy)PX(dx).
(12.101)
For this probabilistic communication channel, the restriction on the support of the
noise is expressed as K ∈Kϵ, where
Kϵ = {K : K(x,{y : ∥x −y∥≤ϵ}) = 1}.
(12.102)
The mutual information between input and output in this general model is
I(X;Y) = hX −hX|Y
=

A

Q
PXY(dx,dy)log
PXY(dx,dy)
PX(dx)PY(dy),
(12.103)
and the Shannon capacity is
C(K) = sup
PX∈P
I(X;Y),
(12.104)
which clearly depends on the noise model through the kernel K.
We now consider the smallest Shannon capacity over all noise distributions of
bounded support ϵ, namely
C∗
ϵ = inf
K∈Kϵ C(K).
(12.105)
The largest amount of information in this worst-case condition is then related to the
ϵ-entropy and ϵ-capacity as follows:
C2ϵ(A) ≤C∗
ϵ ≤Hϵ(A) ≤Cϵ(A).
(12.106)
The smallest Shannon capacity for an ϵ-noise perturbed channel falls between the
ϵ and the 2ϵ Kolmogorov capacities of the corresponding functional space.
To show the validity of the bounds in (12.106), we notice that there exists a noise
distribution kernel ˜K such that
C( ˜K) = Hϵ(A).
(12.107)
This kernel simply assigns with probability one to each x ∈A one of the points y ∈Q
of an optimal ϵ-net deﬁning the ϵ-entropy of A. Clearly, this choice implies (12.107),
since for all input distributions we have
I(X;Y) = hX −hX|Y
= hY −hY|X
14
12:22:54, subject to the Cambridge Core

12.6 Basic Deterministic–Stochastic Model Relations
373
= hY
≤log(Qϵ)
= Hϵ(A),
(12.108)
and the equality in the derivation above is achieved by choosing the input corresponding
to a uniform distribution of the output over the ϵ-net. By combining (12.105) and
(12.107), and using the basic relation (12.94), it now follows that
C∗
ϵ ≤Hϵ(A) ≤Cϵ(A).
(12.109)
The lower bound on the left-hand side of (12.106) follows by choosing one input
signal uniformly at random among the M2ϵ(A) input signals described by the points of a
maximal 2ϵ distinguishable set deﬁning the 2ϵ capacity of A. In this way, the probability
of error at the output is zero, and
I(X;Y) = hX −hX|Y
= hX
= log(M2ϵ)
= C2ϵ.
(12.110)
12.6.2
Rate–Distortion
To obtain a relation between the Shannon capacity and the Kolmogorov capacity,
we have considered the largest mutual information compatible with a class of input
probability distributions and then minimized it among all bounded probabilistic
perturbations. We now consider the smallest mutual information compatible with any
bounded probabilistic perturbation and maximize it among the class of possible input
distributions. We let the Shannon rate–distortion function
Rϵ(PX) = inf
K∈Kϵ I(X;Y),
(12.111)
which clearly depends on the model for the input distribution PX ∈P. If we maximize
it over all input probability distributions, we obtain
R∗
ϵ(A) = sup
PX∈P
Rϵ(PX).
(12.112)
The quantities (12.105) and (12.112) are duals of each other, and we have the following
inequalities:
C∗
2ϵ(A) ≤R∗
ϵ(A) ≤C∗
ϵ.
(12.113)
The largest ϵ-bounded rate–distortion function falls between the smallest Shannon
capacities of an ϵ- and a 2ϵ-noise channel.
14
12:22:54, subject to the Cambridge Core

374
Information-Theoretic Quantities
The inequality on the right-hand side follows immediately from the general duality
formula
sup
x
inf
y f(x,y) ≤inf
y sup
x
f(x,y).
(12.114)
The inequality on the left-hand side follows from C2ϵ(A) ≥C∗
2ϵ(A) by (12.109) and the
existence of a PX ∈P for which
Rϵ(PX) ≥C2ϵ(A).
(12.115)
This distribution is simply the uniform one assigning to each of the Q2ϵ points of a
maximal 2ϵ-separated set the probability 1/Q2ϵ.
12.7
Information Dimensionality
Kolmogorov’s entropy and capacity measure the amount of information associated with
functional spaces. The larger their values, the more complicated the structure of the
space is. This suggests the introduction of a notion of information dimensionality based
on these information-theoretic quantities. One approach deﬁnes the fractal dimension
as the order of growth of the entropy and capacity at increasingly higher levels of
discretization of the space. For many functional spaces, however, entropy and capacity
diverge for any ﬁxed value of ϵ. In this case, a second approach considers a sequence of
ﬁnite-dimensional approximating subspaces, and determines the order of growth of the
dimensionality of successive elements of this sequence.
We start by investigating the ﬁrst approach, and distinguish the two cases of
informationally meager functional sets, having a slower rate of growth of the ϵ-entropy
and ϵ-capacity, and informationally abundant functional sets, having a faster rate of
growth as ϵ →0.
12.7.1
Metric Dimension
Let A be a bounded subset of Rd with non-zero interior; then we have, as ϵ →0,
Hϵ(A) = dlog 1
ϵ + O(1),
(12.116)
Cϵ(A) = dlog 1
ϵ + O(1).
(12.117)
The derivation of (12.116) and (12.117) follows from the computation of the entropy
and the capacity in the one-dimensional case. For an interval A = [a,b] and Euclidean
metric, Qϵ(A) is the smallest n ≥(b −a)/(2ϵ), and Mϵ(A) is the largest m ≤(b −a)/ϵ.
It follows that
Qϵ(A) = b −a
2ϵ
+ O(1),
(12.118)
Mϵ(A) = b −a
ϵ
+ O(1),
(12.119)
14
12:22:54, subject to the Cambridge Core

12.7 Information Dimensionality
375
and by taking logarithms,
Hϵ(A) = log(1/ϵ) + O(1),
(12.120)
Cϵ(A) = log(1/ϵ) + O(1).
(12.121)
The entropy and capacity for the higher-dimensional case can now be estimated by
considering a Cartesian product of intervals deﬁning a parallelepiped and using the
results for the individual factors. Let A = ?d
k=1 Ak ⊂Rd and each Ak be an interval of
the kth coordinate axis; then
Hϵ(A) ≤
d

k=1
Hϵ/
√
d(Ak),
(12.122)
C2ϵ(A) ≥
d

k=1
C2ϵ/
√
d(Ak).
(12.123)
Using (12.120) and (12.121) to express the ϵ-entropy of the monodimensional sets {Ak}
and the basic relation (12.94), the result (12.116) and (12.117) now follows in the case
that A ⊂Rd is a parallelepiped. The extension to more general compact sets with interior
points follows from the monotonicity of entropy and capacity, in conjunction with an
approximation argument.
The exact determination of the factor O(1) in (12.116) corresponds to determining
the most economical covering of the space Rd, and in (12.117) the tightest packing by
balls of unit radius. The exact solution to these geometric problems is not known for
d > 3. The solution to the packing problem in three dimensions was ﬁrst conjectured
by Kepler in 1611. A computer-aided proof, which has been accepted by the wider
mathematical community, was only provided in 1998 by Thomas Hales. Without having
to solve this difﬁcult combinatorial problem, the ﬁrst-order characterization in (12.116)
shows that as ϵ →0 the behavior of the ϵ-entropy and ϵ-capacity for subsets of the
ﬁnite-dimensional Euclidean space is principally given by the dimension d of the space.
Their rate of divergence as ϵ →0 is logarithmic, indicating that subsets of this kind are
“informationally meager.”
To characterize the massiveness of other kinds of sets, it is then natural to compare
them to the massiveness of subsets of the ﬁnite-dimensional Euclidean space. This is
easily done by deﬁning the metric dimension of a subset A ⊂X as
D(A) = lim
ϵ→0
Hϵ(A)
log(1/ϵ),
(12.124)
or
D(A) = lim
ϵ→0
Cϵ(A)
log(1/ϵ),
(12.125)
provided that these limits exist, or otherwise deﬁning the corresponding upper and lower
metric dimensions using limsup and liminf, respectively. These deﬁnitions are due
to Kolmogorov, and (12.124) is also known as the Minkowski–Bouligand, or fractal,
dimension used to determine the “degree of fractality” of a set. We have encountered
14
12:22:54, subject to the Cambridge Core

376
Information-Theoretic Quantities
this quantity in Section 3.6.2 in the context of the reconstruction of multi-band signals
from linear measurements. This deterministic quantity should also be compared with
the analogous information dimension (3.174) introduced by Alfred Rényi in a stochastic
setting, based on the Shannon entropy, and discussed in Section 3.7.3.
It is easy to see that, by virtue of (12.116) and (12.117), for any bounded subset of
Rd we have
D(A) = d,
(12.126)
and the metric dimension coincides with the dimension of the space. On the other
hand, for inﬁnite-dimensional convex sets the order of growth of Hϵ(A) and Cϵ(A)
always exceeds log(1/ϵ), and the metric dimension diverges. Inﬁnite-dimensional sets
are “informationally abundant.” For this reason, Kolmogorov concludes:
Metric dimension is useless for distinguishing the massiveness of sets of this sort. (Kolmogorov
and Tikhomirov, 1959)
To quantify the amount of information of informationally abundant sets, Kolmogorov
then suggested considering alternative notions of metric dimension.
12.7.2
Functional Dimension and Metric Order
One example of an informationally abundant set is an inﬁnite-dimensional compact set
of ﬁnite smoothness. The growth of the ϵ-entropy and capacity is typically a power
of (1/ϵ) with an exponent that decreases as the supply of smoothness of A increases.
This conﬁrms the intuition that the faster Hϵ(A) tends to inﬁnity as ϵ →0, the more
complicated the structure of A is.
Stated more precisely, for any inﬁnite-dimensional set A of functions of s variables
deﬁned on a unit hypercube with uniformly bounded partial derivatives up to order r,
Kolmogorov showed that
Hϵ(A) = O
1
ϵ
s/r
as ϵ →0,
(12.127)
Cϵ(A) = O
1
ϵ
s/r
as ϵ →0.
(12.128)
On the other hand, he also showed that the order of magnitude of the ϵ-entropy and
ϵ-capacity for analytic functions is essentially different, being only
Hϵ(A) = O[log(1/ϵ)]s+1 as ϵ →0,
(12.129)
Cϵ(A) = O[log(1/ϵ)]s+1 as ϵ →0.
(12.130)
When viewed together, these results show that:
The information content of analytic functions is much less than that of functions of
ﬁnite smoothness.
14
12:22:54, subject to the Cambridge Core

12.7 Information Dimensionality
377
For these functional classes, the most natural measures of information growth are
what Kolmogorov calls the functional dimension,
F(A) = lim
ϵ→0
logHϵ(A)
loglog(1/ϵ),
(12.131)
and the metric order,
M(A) = lim
ϵ→0
logHϵ(A)
log(1/ϵ) .
(12.132)
By (12.129) it follows that the functional dimension of an analytic function is roughly
the number of variables, or, geometrically, the dimension of the manifold where the
function is supported. For smooth functions, by (12.127) we have that the metric order
corresponds to the exponent of smoothness s/r.
A derivation of a slightly weaker upper bound than (12.127) for smooth functions
can be obtained easily. Consider a function of one variable with bounded derivatives up
to order r deﬁned on the unit interval [0,1]. Divide the interval into equal subintervals
of size ϵ1/r and compute the value of the function and its derivatives up to order r
with accuracy ϵ at the endpoints of each subinterval. One can then compute the value
of the function at any point with accuracy of the order of ϵ by ﬁnding the subinterval
containing the given point and using the rth-order Taylor expansion about its left end
point. In this way, the remainder of the Taylor expansion is of the order of ϵ for any
point in [0,1]. Now, to specify one derivative at a given point within accuracy ϵ we
need of the order of 1/ϵ numbers, and (1/ϵ)r+1 to specify all derivatives. Since the total
number of points is ϵ−1/r, we need a total number of (1/ϵ)(r+1)ϵ1/r numbers to compute
any value of the function in [0,1] within accuracy ϵ. Taking the logarithm, this leads
to an upper bound on the ϵ-entropy of the order of (1/ϵ)1/r log1/ϵ. The correct upper
bound, without the additional logarithmic factor, can be obtained using a more accurate
calculation.
12.7.3
Infinite-Dimensional Spaces
For spaces where the ϵ-entropy and ϵ-capacity are inﬁnite, we can consider a sequence
of ﬁnite-dimensional approximating subspaces, and provide asymptotic results on the
order of growth of the dimensionality as the accuracy of the approximation increases.
Following this approach, we argued in Chapter 3 that for bandlimited signals the
effective dimensionality corresponds to the number of degrees of freedom of the space,
as the size of the observation interval T →∞. We then extend this notion to multi-band
signals, considering the Minkowski–Bouligand fractal dimension in place of the number
of degrees of freedom. We now follow the approximation approach to determine the
order of growth of the ϵ-entropy and ϵ-capacity of bandlimited signals as T →∞.
14
12:22:54, subject to the Cambridge Core

378
Information-Theoretic Quantities
12.8
Bandlimited Signals
We consider the entropy and capacity of the inﬁnite-dimensional space of bandlimited
signals. In this case, all the results obtained in Shannon’s probabilistic setting have direct
analogs in a deterministic setting. We ﬁrst summarize the geometric insight that forms
the basis of the two models, and then give the details of the mathematical results.
12.8.1
Capacity and Packing
Shannon’s capacity is closely related to the problem of geometric packing of
“billiard balls” in high-dimensional space. Roughly speaking, each transmitted signal,
represented by the coefﬁcients of an orthonormal basis expansion, corresponds to a
point in the space, and balls centered at the transmitted points represent the probability
density of the uncertainty of the observation performed at the receiver. A certain amount
of overlap between the balls is allowed to construct dense packings corresponding
to codebooks of high capacity, as long as the overlap does not include typical noise
concentration regions, and this allows us to achieve reliable communication with
vanishing probability of error.
Similarly, in Kolmogorov’s deterministic setting, communication between a trans-
mitter and a receiver occurs without error, balls of ﬁxed radius ϵ representing the
uncertainty introduced by the noise about each transmitted signal are not allowed to
overlap, and his notion of 2ϵ-capacity corresponds to the Shannon capacity of the
ϵ-bounded noise channel with the probability of error equal to zero.
We now wish to introduce a notion of vanishing error in a deterministic setting as
well. With this goal in mind, we allow a certain amount of overlap between the ϵ-balls.
In a deterministic setting, a codebook is composed of a subset of waveforms in the
space, each corresponding to a given message. A transmitter can select any one of these
signals, which is observed at the receiver with perturbation at most ϵ. If signals in the
codebook are at distance less than 2ϵ of each other, then a decoding error may occur
due to the overlap region between the corresponding ϵ-balls. The total volume of the
error region, normalized by the total volume of the ϵ-balls in the codebook, represents
a measure of the fraction of the space where the received signal may fall and result in
a communication error. We then deﬁne the (ϵ,δ)-capacity as the logarithm base two of
the largest number of signals that can be placed in a codebook having a normalized
error region of size at most δ. It turns out that the (ϵ,δ)-capacity grows linearly with the
number of degrees of freedom, but only logarithmically with the signal-to-noise ratio,
and, when taken per unit time, as in the case of Shannon’s capacity, it is independent of
the value of δ. This was Shannon’s original insight, expressed by (12.35), which remains
invariant when the model is subject to a deterministic formulation.
12.8.2
Entropy and Covering
Shannon’s rate–distortion function is closely related to the geometric problem of
covering a high-dimensional space with balls of a given radius. Roughly speaking, each
14
12:22:54, subject to the Cambridge Core

12.8 Bandlimited Signals
379
source signal, modeled as a stochastic process, corresponds to a random point in the
space, and a set of covering balls provides a possible representation of any signal in
the space. By representing the signal with the closest center of a ball covering it, the
distance to the center of the ball represents the distortion measure associated with this
representation. Shannon’s rate–distortion function provides the minimum number of
bits that must be speciﬁed per unit time to represent the source process with a given
average distortion.
In Kolmogorov’s deterministic setting, the ϵ-entropy is the logarithm of the minimum
number of balls of radius ϵ needed to cover the whole space and, when taken per unit
time, it corresponds to the Shannon rate–distortion function, as it also represents the
minimum number of bits that must be speciﬁed per unit time to represent any source
signal with distortion at most ϵ. It turns out that the ϵ-entropy of bandlimited functions
grows linearly with the number of degrees of freedom and logarithmically with the ratio
of the norm of the signal to the norm of the distortion. Once again, this was Shannon’s
key insight, expressed by (12.79), which remains invariant when the model is subject to
a deterministic formulation.
12.8.3
ϵ-Capacity of Bandlimited Signals
We consider waveforms f ∈B, namely bandlimited, one-dimensional, real, scalar
waveforms of a single scalar variable and supported over an angular frequency interval
[−,]. We assume that waveforms are square-integrable, and satisfy the energy
constraint
 ∞
−∞
f 2(t)dt ≤E.
(12.133)
These bandlimited waveforms have unbounded time support, but are observed over a
ﬁnite interval [−T/2,T/2] and equipped with the norm
∥f∥=
 T/2
−T/2
f 2(t)dt
1/2
.
(12.134)
By the results in Chapter 3 it follows that any signal can be expanded in terms of a
suitable set of orthonormal basis functions, and for T large enough it can be seen as a
point in a space of essentially
N0 = T/π
(12.135)
dimensions, corresponding to the number of degrees of freedom of the waveform, and
of radius
√
E.
We consider an uncertainty sphere of radius ϵ centered at each signal point,
representing the energy of the noise that is added to the observed waveform. In this
model, due to Kolmogorov, the signal-to-noise ratio is
SNRK = E/ϵ2.
(12.136)
A codebook is composed of a subset of waveforms in the space, each corresponding
to a given message. A transmitter can select any one of these signals, which is observed
14
12:22:54, subject to the Cambridge Core

380
Information-Theoretic Quantities
at the receiver with perturbation at most ϵ. By choosing signals in the codebook to be at
a distance of at least 2ϵ from each other, the receiver can decode the message without
error. The 2ϵ-capacity is the logarithm base two of the maximum number M2ϵ(E) of
distinguishable signals in the space. This corresponds geometrically to the maximum
number of disjoint balls of radius ϵ with their centers situated inside the signals’ space,
and it is given by
C2ϵ = logM2ϵ(B) bits.
(12.137)
We also deﬁne the capacity per unit time:
¯C2ϵ = lim
T→∞
logM2ϵ(B)
T
bits per second.
(12.138)
Comparing this communication model with Shannon’s as described in Section 12.2,
it is clear that the geometric insight on which each is built is the same. However, while in
Kolmogorov’s deterministic setting packing is performed with “hard” spheres of radius
ϵ and communication in the presence of arbitrarily distributed noise over a bounded
support is performed without error, in Shannon’s stochastic model packing is performed
with “soft” spheres of effective radius √NN0 and communication in the presence of
Gaussian noise of unbounded support is performed with arbitrarily low probability of
error δ.
Shannon’s energy constraint (12.2) scales with the number of dimensions, rather than
being a constant. The reason for this should be clear: if the noise is assumed to act
independently on each signal’s coefﬁcient, the statistical spread of the output, given the
input signal, corresponds to an uncertainty ball of radius √NN0. It follows that the norm
of the signal should also be proportional to √N0, to avoid a vanishing signal-to-noise
ratio as N0 →∞. In contrast, in the case of Kolmogorov the capacity is computed
assuming an uncertainty ball of ﬁxed radius ϵ and the energy constraint is constant. In
both cases, spectral concentration ensures that the size of the signals’ space is essentially
of N0 dimensions. Probabilistic concentration ensures that the noise in Shannon’s model
concentrates around its standard deviation, so that the functional form of the capacity is
similar in the two cases, and we have
⎧
⎪⎪⎨
⎪⎪⎩
¯C2ϵ ≤
π log

1 +
	
SNRK/2

bits per second,
¯C2ϵ ≥
π

log
	
SNRK −1

bits per second.
(12.139)
(12.140)
When comparing (12.139) and (12.140) with (12.35), it is important to keep in mind
that while Shannon’s formula refers to communication with vanishing probability of
error over a Gaussian channel, Kolmogorov’s formula refers to communication with
zero error over a bounded noise channel. To provide a more precise comparison, we
extend the deterministic model introducing the possibility of having a decoding error
by allowing signals in the codebook to be at a distance of less than 2ϵ from each other.
14
12:22:54, subject to the Cambridge Core

12.8 Bandlimited Signals
381
a
b
c
d
e
f
g
h
i
Fig. 12.16
Illustration of the error region for a signal in the space. The letters indicate the volumes of the
corresponding regions of the ball N 1, and 1 = (c + e + f + g + i)/(a + b + c + d + e + f+
g + h + i).
This yields the notion of (ϵ,δ)-capacity that is the analog of Shannon’s capacity in a
deterministic setting.
12.8.4
(ϵ,δ)-Capacity of Bandlimited Signals
For any yi ∈{y1,y2,...,ym} ⊂B, we let the noise ball be
N i = {x : ∥x −yi∥≤ϵ},
(12.141)
where ϵ is a positive real number, and we let error region be
Di = {x ∈N i : ∃j ̸= i : ∥x −yj∥≤∥x −yi∥}.
(12.142)
We then deﬁne the error measure for the ith signal,
i = vol(Di)
vol(N i),
(12.143)
where vol(·) indicates volume in X , and the cumulative error measure
 = 1
M
M

i=1
i.
(12.144)
Figure 12.16 provides an illustration of the error region for a given signal in the space.
Clearly, we have 0 ≤ ≤1. For any δ > 0, a set of points in B is an
(ϵ,δ)-distinguishable set if  ≤δ. The maximum cardinality of an (ϵ,δ)-distinguishable
set is an invariant of the space, which depends only on ϵ and δ, and is denoted by
Mδ
ϵ(B). The (ϵ,δ)-capacity of B is deﬁned as the base two logarithm
Cδ
ϵ = logMδ
ϵ(B) bits;
(12.145)
see Figure 12.17. We also deﬁne the (ϵ,δ)-capacity per unit time:
¯Cδ
ϵ = lim
T→∞
logMδ
ϵ(B)
T
bits per second.
(12.146)
14
12:22:54, subject to the Cambridge Core

382
Information-Theoretic Quantities
Fig. 12.17
Illustration of the (ϵ,δ)-capacity of bandlimited functions. An overlap among the ϵ-balls is
allowed, provided that the cumulative error measure  ≤δ.
In this case, we have, for any ϵ,δ > 0,
⎧
⎪⎪⎨
⎪⎪⎩
¯Cδ
ϵ ≤
π log

1 +
	
SNRK

bits per second,
¯Cδ
ϵ ≥
π log
	
SNRK bits per second.
(12.147)
(12.148)
As in Shannon’s case, these results do not depend on the size of the error region δ,
and are the deterministic analog of (12.35).
12.8.5
ϵ-Entropy of Bandlimited Signals
In the case of ϵ-entropy, a source codebook is composed of a subset of points in the
space, and each codebook point is a possible representation for the signals that are
within radius ϵ of itself. If the union of the ϵ balls centered at all codebook points
covers the whole space, then any signal in the space can be encoded by its closest
representation. The radius ϵ of the covering balls provides a bound on the largest
estimation error between any source f(t) and its codebook representation ˆf(t). When
signals are observed over a ﬁnite time interval [−T/2,T/2], this corresponds to
d(f, ˆf) =
 T/2
−T/2
[f(t) −ˆf(t)]2dt ≤ϵ2.
(12.149)
The signal-to-distortion ratio in this source coding model is SNRK =
√
E/ϵ.
The Kolmogorov ϵ-entropy is the logarithm base two of the minimum number Qϵ(E)
of ϵ-balls covering the whole space, and it is given by
Hϵ = logQϵ(E) bits.
(12.150)
14
12:22:54, subject to the Cambridge Core

12.8 Bandlimited Signals
383
Table 12.2 Comparison of stochastic and deterministic models.
Stochastic
Deterministic
Signal constraint
 T/2
−T/2 f 2(t)dt ≤PN0
 ∞
−∞f 2(t)dt ≤E
Additive noise
E
N0
i=1 Z2
i = NN0

∞
i=1 z2
i ≤ϵ2
Signal-to-noise ratio
SNRS = P/N
SNRK = E/ϵ2
Capacity
C = 
π log(√1 + SNRS)
¯Cδ
ϵ ≤
π log(1 + √SNRK)
¯Cδ
ϵ ≥
π log√SNRK
Source constraint
 T/2
−T/2 E(f2(t))dt = PN0
 ∞
−∞f 2(t)dt ≤E
Distortion
d(f,ˆf) ≤N0N
d(f, ˆf) ≤ϵ2
Rate–distortion function
RN = 
π log√SNRS
¯Hϵ = 
π log√SNRK
We also deﬁne the ϵ-entropy per unit time:
¯Hϵ = lim
T→∞
logQϵ(E)
T
bits per second.
(12.151)
In this case, we have
¯Hϵ =
⎧
⎨
⎩

π log
√SNRK

bits per second
if E ≥N,
0
if E < N,
(12.152)
which is the deterministic analog of (12.79).
12.8.6
Comparison with Stochastic Quantities
Table 12.2 provides a comparison between the results in the deterministic and stochastic
settings. In the computation of capacity, a transmitted signal subject to a given energy
constraint is corrupted by additive noise. Due to spectral concentration, the signal has
an effective number of dimensions N0. In a deterministic setting, the noise represented
by the deterministic coordinates {zi} can take any value inside a ball of radius ϵ. In
a stochastic setting, due to probabilistic concentration, the noise represented by the
stochastic coordinates {Zi} can take values essentially uniformly at random inside a
ball of effective radius NN0. In both cases, the maximum cardinality of the codebook
used for communication depends on the error measure δ > 0, but the capacity in bits
per unit time does not, and it depends only on the signal-to-noise ratio. The special case
δ = 0 does not appear in the table and corresponds to the Kolmogorov 2ϵ-capacity, the
analog of the Shannon zero-error capacity of an ϵ-bounded noise channel.
In the computation of the rate–distortion function, a source signal is modeled as
either an arbitrary or a stochastic process of given energy constraint. The distortion
measure corresponds to the estimation error incurred when this signal is represented
14
12:22:54, subject to the Cambridge Core

384
Information-Theoretic Quantities
by an element of the source codebook. The minimum cardinality of the codebook used
for representation depends on the distortion constraint, and so does the rate–distortion
function.
The functional form of the results indicates that in both Kolmogorov’s and Shannon’s
settings, capacity and entropy grow linearly with the number of degrees of freedom,
but only logarithmically with the signal-to-noise ratio. This was Shannon’s original
insight, which transcends the details of the stochastic or deterministic description of
the information-theoretic model.
12.9
Spatially Distributed Systems
We now extend the capacity results to communication with spatially distributed
transmitters and receivers. These results can be obtained using the singular value
decomposition described in Section 5.3.3 that provides the number of spatial degrees
of freedom per unit frequency in terms of the rank of the Green’s matrix G = {Gi,k(ω)}
connecting transmitters and receivers. To obtain the total capacity, we sum the rate
contributions of all spatial channels expressed in bits per time–frequency degree of
freedom and given by Shannon’s formula (12.34). The derivation follows the usual
steps, using an orthogonal decomposition of the signal in space rather than in frequency,
and with each spatial channel contributing to the capacity by a value given by (12.34).
12.9.1
Capacity with Channel State Information
We assume a deterministic, time-invariant setting, where the transmitters have
knowledge of the Green’s function, so that they can optimally allocate energy over the
different spatial channels. By (5.41), each parallel channel corresponds to a non-zero
singular value of the matrix G(ω), and the total number of channels per unit frequency
of transmission is physically limited by the number of spatial degrees of freedom per
unit frequency N0(ω) of the radiating system, as studied in Chapter 8.
We consider adding an independent, zero-mean, Gaussian noise random variable of
standard deviation N on each spatial channel in (5.41), obtaining
+
Yn = σn +
Xn + zn, n = 1,...,N0(ω).
(12.153)
We also assume a communication system with a number of transmitter–receiver pairs
equal to the number of spatial degrees of freedom N0(ω), and that the transmitters
are subject to an energy constraint that scales linearly with the number of degrees of
freedom:
N0(ω)

n=1
+X2
n ≤PN0(ω).
(12.154)
The signal-to-noise ratio per unit frequency of transmission is then given by
SNRS = PN0(ω)
NN0(ω) = P
N .
(12.155)
14
12:22:54, subject to the Cambridge Core

12.9 Spatially Distributed Systems
385
n
κ
Pn
Fig. 12.18
Illustration of water ﬁlling in the discrete spatial setting.
In this case, the Shannon capacity, in bits per (time–frequency) degree of freedom,
equals the supremum of the sum of the achievable rates from the transmitters to the
receivers, exploiting all the spatial degrees of freedom available for communication at
any given frequency.
To compute the capacity, we observe that the scaling of the signal +Xn by σn in
each channel of (12.153) is equivalent to adding an amount of noise that is inversely
proportional to the corresponding singular value. The capacity can then be obtained
using an optimal power allocation similar to the one described in Section 12.2.7, where
water ﬁlling is performed in space rather than in the frequency domain. The solution
corresponds to a value of the power analogous to (12.54),
Pn = [κ −Nσ −2
n ]+,
(12.156)
with κ satisfying
N0(ω)

n=1
[κ −Nσ −2
n ]+ = PN0(ω),
(12.157)
and the capacity in bits per (time–frequency) degree of freedom is obtained by summing
over all spatial channels,
C =
N0(ω)

n=1
1
2 log

1 + σ 2
n Pn
N

=
N0(ω)

n=1
1
2 log

1 + σ 2
n [κ −Nσ −2
n ]+
N

,
(12.158)
which is the analog of (12.56), where the noise here is modulated by the singular values
of the channel over space rather than over frequency. An illustration of the water ﬁlling
solution is depicted in Figure 12.18.
An upper bound on the capacity is obtained using Jensen’s inequality,

N0(ω)
n=1 logxn
N0(ω)
≤log

N0(ω)
n=1 xn
N0(ω)

,
(12.159)
14
12:22:54, subject to the Cambridge Core

386
Information-Theoretic Quantities
yielding
C =
N0(ω)

n=1
1
2 log

1 + Pnσ 2
n
N

≤N0(ω)
2
log
(
1
N0(ω)

N0(ω) +

N0(ω)
n=1 Pnσ 2
n
N
4
= N0(ω)
2
log

1 +

N0(ω)
n=1 Pnσ 2
n
NN0(ω)

.
(12.160)
By comparing(12.160) with (12.34), we observe a rate gain due to the N0(ω) spatial
channels, while the second term inside the logarithm corresponds to the signal-to-noise
ratio at the receivers, with the power on each channel being scaled by the corresponding
singular value. The bound is tight when the singular values are all equal, and in this case
the optimal power allocation corresponds to having equal power on all channels. This
suggests the general principle that the less spread out the singular values are, the larger
the capacity. By deﬁning the condition number of the matrix G as the ratio between the
maximum and the minimum singular values, we have that:
Multiple antenna systems have higher capacities in well-conditioned propagation
environments.
On the other hand, by the results in Chapter 8 it follows that the number of degrees of
freedom in any propagation environment is bounded by the size of the minimum cut-set
boundary containing all the transmitters, and the capacity cannot increase beyond this
value imposed by the physics of the propagation process. Any model of the Green’s
function that violates this constraint is a physical impossibility. Furthermore, since by
(5.38) the number of degrees of freedom is at most equal to the minimum between the
number of transmitter and receiver antennas, increasing the number of antennas cannot
increase capacity beyond the number of degrees of freedom. Finally, if the singular
values are all close to one, then the capacity becomes
C = N0(ω)
2
log

1 + SNRS

,
(12.161)
and the rate gain of N0(ω) parallel channels is now directly comparable with the
single point-to-point Shannon formulas (12.33) and (12.34). In (12.33), N0 refers to
the number of time–frequency degrees of freedom of a point-to-point channel, and
the capacity is measured in bits; (12.34) gives the corresponding formula in bits per
time–frequency degree of freedom. On the other hand, in (12.161), N0 = N0(ω) refers
to the number of spatial–wavenumber degrees of freedom available at every frequency,
and the capacity is measured in bits per time–frequency degree of freedom. It follows
that the spatial system provides a rate gain equal to the number of spatial–wavenumber
degrees of freedom.
14
12:22:54, subject to the Cambridge Core

12.9 Spatially Distributed Systems
387
All of the above results depend on the power constraint (12.154), which has been
assumed to grow proportionally to the number of degrees of freedom. On the other
hand, in the presence of a ﬁxed energy constraint
N0(ω)

n=1
+X2
n ≤E,
(12.162)
we need to choose κ satisfying
N0(ω)

n=1
[κ −Nσ −2
n ]+ = E,
(12.163)
and for equal power allocation over unitary channels, we get
C = N0(ω)
2
log

1 +
E
N0(ω)N

.
(12.164)
In this case, as N0(ω) →∞we enter a vanishing signal-to-noise ratio regime, and the
capacity tends to the limiting value
C = E
2N log2 e.
(12.165)
Comparing (12.165) with (12.161), it follows that, consistent with the physics,
increasing the capacity by increasing the number of degrees of freedom requires energy
expenditure. For ﬁxed energy systems, as the number of degrees of freedom increases,
what matters eventually is only the total energy expenditure.
12.9.2
Capacity without Channel State Information
In a time-invariant setting, the Green’s function can easily be estimated by sending
a pilot signal and measuring it at the receivers. The estimate can then be sent back
to the transmitters to perform power allocation as discussed above. When the Green’s
function is modeled as randomly time-varying, however, this estimation process may
not be completed in a time where the channel is constant and all we can hope for is
to have channel state information at the receivers only. The best strategy in this case is
to allocate equal power on all channels, and the capacity, computed averaging over the
channel variations over time, is
C = 1
2
N0(ω)

n=1
E
8
log

1 + SNRSσ 2
n
9
≤N0(ω)
2
E
(
log

1 + SNRS

N0
n=1 σ 2
n
N0
4
.
(12.166)
Clearly, the capacity now depends on the statistical properties of the singular values.
If the stochastic Green’s function is statistically well conditioned, then the rate gain
equals the number of degrees of freedom, and the system achieves its full multiplexing
capability. In this case, the model chosen for the Green’s function is crucial to
14
12:22:54, subject to the Cambridge Core

388
Information-Theoretic Quantities
determining the value of the capacity, and the use of random matrix models in the
analysis of communication systems has become widespread. Probabilistic estimates of
the eigenvalues of these matrices are a key tool for capacity predictions in different
scenarios. However, one should keep in mind that the chosen stochastic model must
always be consistent with physical reality, predicting a multiplexing gain no larger than
what can be physically supported by the channel.
12.10
Summary and Further Reading
Shannon’s model of communication using bandlimited signals was introduced in
Shannon (1949). The rigorous formulation of his capacity result for continuous signals
is given by Wyner (1965). Close examination of these results shows that reﬁned noise
models are required to account for additional quantum constraints and super-resolution
effects occurring at high frequencies. A more accurate physical model assumes
Gaussian noise that vanishes at high frequencies, where quantum noise kicks in. In
this case, there is a contribution to capacity associated with the continuous wave nature
of the signal that follows Shannon’s classic formula, and a contribution associated with
the discrete particle nature of the signal that depends on the details of the quantum noise
model, which is dominant at high frequencies. The book by Papen and Blahut (2018)
gives a compendium of additional results on the capacity of electromagnetic channels
subject to quantum constraints in different regimes.
In the absence of noise, and in the presence of quantized radiation constraints, the
formula (12.48) has been obtained by Lebedev and Levitin (1966), Bowen (1967),
and Pendry (1983), following previous work by Gordon (1962) and Stern (1960).
Lachmann, Newman, and Moore (2004) point out that the formula corresponds to the
entropy rate of black body radiation in one dimension. We studied black body radiation
in Chapter 11 using a statistical physics approach. Different derivations of the capacity
formula (12.48) are reviewed by Caves and Drummond (1994). A general derivation
considers all possible representations of the radiated signal that include non-orthogonal
basis sets and arbitrary quantum measurement operators. Using a theorem by Holevo
(1973), extended to inﬁnite-dimensional systems by Yuen and Ozawa (1993), it uses
the full machinery of quantum mechanics to conﬁrm the capacity bound (12.48) in a
general setting.
Parallel to Shannon’s theory, there is a deterministic theory of information that
originates from the Russian school of Kolmogorov. The notions of ϵ-entropy and
ϵ-capacity were introduced by Kolmogorov (1956). His functional theory of infor-
mation is reviewed in Kolmogorov and Tikhomirov (1959). The basic relations with
Shannon’s theory of information discussed in Section 12.6 appear in Appendix II of
that paper. Bounds on the ϵ-entropy and ϵ-capacity of bandlimited functions were
ﬁrst given by Jagerman (1969, 1970) and Wyner (1973). We presented some improved
versions obtained by Lim and Franceschetti (2017b), who also introduced the concept
of (ϵ,δ)-capacity. The connection between deterministic and stochastic models is
discussed by Donoho (2000).
14
12:22:54, subject to the Cambridge Core

12.11 Test Your Understanding
389
The capacity of systems composed of multiple antennas is a well-studied topic in
communication theory. Comprehensive treatments can be found in the books by Biglieri
(2005), Goldsmith (2005), and Tse and Viswanath (2005). Relationships with random
matrix theory are described by Tulino and Verdú (2004).
12.11
Test Your Understanding
Problems
12.1
Check that SNRS in (12.37) and (12.41) is in dimensionless units.
12.2
Check that the capacity in (12.46) is in bits per second.
12.3
Write the expression for the Shannon capacity of the additive white Gaussian
noise channel subject to the following power constraints:
 T/2
−T/2
f 2(t) ≤PN0,
(12.167)
 T/2
−T/2
f 2(t) ≤¯PT,
(12.168)
 T/2
−T/2
f 2(t) ≤E,
(12.169)
where E, P, and ¯P are constants.
12.4
Rewrite (12.38) in bits per second and compare it with (12.42).
12.5
Consider a signal subject to the energy per degree of freedom constraint (12.167).
Show the following quantum limitation and compare it to (12.45):
C ≪
P
2π ¯h loge bits per second.
(12.170)
12.6
In Sections 12.2 and 12.2.3 we stated that the energy constraints (12.2)
and (12.39) “essentially” correspond to (12.11) and (12.40). Discuss the kind of
approximation that has been made in these statements.
12.7
Provide a lower bound on the probability of error in an additive white Gaussian
noise channel for any achievable rate R < C in terms of T, C, and R.
12.8
Explain the difference between a probabilistic proof for the existence of a
capacity-achieving code by showing that a random codebook selection implies the
selection of a capacity-achieving codebook with positive probability, and the proof
given in Section 12.2.2.
12.9
Check that the units of (12.45) and (12.48) are bits per second.
12.10
Provide a geometric argument that justiﬁes (12.122) and (12.123).
14
12:22:54, subject to the Cambridge Core

390
Information-Theoretic Quantities
12.11
Show that for high values of SNRK the Kolmogorov 2ϵ-capacity is
(/π)(log
	
SNRK −1) ≤¯C2ϵ ≤(/π)(log
	
SNRK −1/2).
(12.171)
12.12
Show that if √SNRK ≥
√
2/(
√
2 −1) then ¯Cδ
ϵ > ¯C2ϵ.
12.13
Provide an intuitive justiﬁcation for the factor 1+√SNRK inside the logarithm
in (12.147), compared to √1 + SNRS in Shannon’s setting.
12.14
Verify that the capacity in (12.166) can also be written as
C = 1
2 E
8
log det

I + SNRSGG†9
.
(12.172)
14
12:22:54, subject to the Cambridge Core

13
Universal Entropy Bounds
Lo! thy dread Empire, CHAOS! is restor’d;
Light dies before thy uncreating word:
Thy hand, great Anarch! lets the curtain fall;
And Universal Darkness buries All.1
13.1
Bandlimited Radiation
In Chapter 11, we modeled black body radiation as a stochastic process radiating
at all frequencies and having a Gaussian distribution whose power spectral density
has a vanishing tail according Planck’s law. The entropy of black body radiation is
proportional to (r ¯E)3/4, where ¯E is the average energy of the radiating body and r is
its radius. This result follows by imposing quantized radiation at all frequencies, while
keeping the total average energy of the body ﬁnite. The average energy is proportional
to the volume, so that radiation from a black body carries an amount of information
proportional to its volume. In the one-dimensional case, black body radiation achieves
the capacity of the noiseless photon channel expressed by (12.48).
Radiation over all frequencies, however, is an idealization as it allows emission of
photons of arbitrarily high energy. Even though highly energetic photons are weighted
by a small probability of emission to avoid the “ultraviolet catastrophe,” transient
radiation of unbounded energy is still possible. A different point of view adopted in this
chapter starts with the premise that any radiated signal must be a strictly bandlimited
function, and computes entropy bounds by imposing quantized radiation constraints in
this setting.
We consider both a deterministic and a stochastic model of radiation. Deterministic
signals of energy at most E can be arbitrarily distributed over a ﬁnite frequency
bandwidth, and stochastic signals of expected energy at most ¯E are assumed to have a
Gaussian distribution whose power spectral density is constant over a ﬁnite bandwidth.
By combining the results on the degrees of freedom in Chapter 9 with quantized
radiation constraints, and using the entropy formulas derived in Chapter 12, we show
that in a three-dimensional setting of radiation from a sphere of radius r, the entropy
is proportional to (rE), and it achieves the universal bound (11.82). This bound is tight
when the energy is proportional to r2, so that the maximum entropy of bandlimited
1 Alexander Pope (1743). The Dunciad, Bk. IV, L. 649.
15
12:02:45, subject to the Cambridge Core

392
Universal Entropy Bounds
radiation is also proportional to the volume of the radiating body. However, the energy
expenditure to achieve this bound scales with the surface of the radiating body, while
for black body radiation the expected energy scales with the volume of the body.
13.2
Deterministic Signals
Our derivation follows from the three constraints highlighted at the beginning of the
book: an energy constraint, a dimensionality (or bandlimitation) constraint, and a
resolution (or quantization) constraint.
We ﬁrst consider arbitrary square-integrable, bandlimited waveforms propagating
with velocity c from the interior of a sphere of radius r to the outside space. The
waveform on the surface of the sphere is identiﬁed by one temporal coordinate t and
two angular ones (θ,φ). The propagating signal is assumed to be the image of sources
internal to the sphere, impressed or induced through multiple scattering, through the
Green’s free space propagation operator, and is observed on the external surface of the
sphere for a time interval of duration T = r/c. We consider the norm
∥f∥2 =
 T/2
−T/2

S
f 2(θ,φ,t)dθdφdt,
(13.1)
and we assume that the signal is subject to the energy constraint
 ∞
−∞

S
f 2(θ,φ,t)dθdφdt ≤E.
(13.2)
As the sphere expands, the entropy of the observation increases and our objective is to
ﬁnd its scaling law as a function of r. This geometric setup is depicted in Figure 9.1.
We consider the Fourier transform
f(θ,φ,t) ←→F(w1,w2,ω),
(13.3)
and two sets, P and Q, corresponding to the coordinate points of the space–time region
where the norm is computed, and of the support set of the Fourier transform of the
signal. By (9.15), the observation domain P has measure 4πT. By (9.16), the spectral
support Q has measure of the order of 83r2/(3c2) as r →∞. By letting T = r/c in
(9.18), and using the norm (9.3), it follows that the Nyquist number representing the
effective dimensionality of the space is
N0 = 4π
3
r
cπ
3
;
(13.4)
in other words, the number of degrees of freedom grows with the volume of the radiating
system, normalized to the cube of the bandwidth expressed in wavelength units. By the
results in Chapter 12 we also have that the ϵ-entropy is
Hϵ =
"
N0 log(
√
E/ϵ) + o(N0) bits
if
√
E/ϵ ≥1,
0
if
√
E/ϵ < 1
(13.5)
15
12:02:45, subject to the Cambridge Core

13.2 Deterministic Signals
393
as r →∞. The ϵ-entropy grows linearly with the number of degrees of freedom and
logarithmically with the signal-to-quantization ratio.
On the other hand, for radiation of single-frequency, time-harmonic signals subject
to the constraint (9.8), and considering the norm (9.9), using (9.10) we have an effective
number of dimensions per unit frequency of
N0(ω) = 4π
ωr
cπ
2
;
(13.6)
in other words, the number of spatial degrees of freedom grows with the surface of the
radiating system, normalized by half a wavelength squared.
13.2.1
Quantization Error
We now introduce the notion of quantization error, and provide a lower bound on the
resolution at which signals can be observed due to the physical nature of the propagation
process. Due to the quantized nature of radiation, there is a minimum resolution at which
each frequency component of the signal can be observed on any spatial dimension, and
by virtue of (13.6) this leads to a cumulative quantization error per unit frequency that
grows with the square of the radius of the system. It follows that to keep a non-vanishing
signal-to-quantization ratio, the energy of the signal must also be subject to the same
scaling law. When these constraints are taken into account in (13.5), they lead to a
maximum entropy expression matching the universal entropy bound (11.82).
We let q(t,r) be the error committed when a signal f is identiﬁed with the closest
center ˆf of an ϵ-ball of a minimum-cardinality covering of the space Q, namely
q(t,r) = f −ˆf,
(13.7)
where
ˆf = argmin
f ∗∈Q
∥f −f ∗∥.
(13.8)
Clearly, for all signals in the space we have
 T/2
−T/2

S
q2(t,r)drdt ≤ϵ2.
(13.9)
Consider the largest possible error occurring for signals located on the boundary of
an ϵ-ball and approximated by the center of the ball. For all ω ∈R, we decompose the
frequency spectrum of the error using a basis set {ψn(r)}, orthonormal over the surface
S of the sphere of radius r, obtaining
Fq(ω,r) =
∞

n=1
an(ω)ψn(r),
(13.10)
where F indicates the Fourier transform operator with respect to the variable t. Using
orthonormality, we have that the energy of the error per unit angular frequency is
ϵ2(ω) =

S
|Fq(ω,r)|2dr ≥
N0(ω)

n=1
|an(ω)|2,
(13.11)
15
12:02:45, subject to the Cambridge Core

394
Universal Entropy Bounds
f
a
a
(ω)
|a |
1
|a  |
2
〈
f
Fig. 13.1
Illustration of the lower bound on the quantization error.
where N0(ω) is given by (13.6). Since radiation is quantized in photonic units of energy
¯hω > 0, we can model the energy of the error per unit angular frequency occurring on
each of the N0(ω) spatial dimensions as taking discrete values that are multiples of ¯h/2,
namely
|an(ω)|2 = k¯h/2, k ∈{0,1,2,...}
(13.12)
In this way, each pair of opposite frequencies of any real signal located on the boundary
of the sphere is subject to a quantization error that is a multiple of ¯h. Some of the
components in (13.12) can be zero, but the cumulative error must be proportional to the
number of dimensions, namely
ϵ2(ω) =
N0(ω)

n=1
|an(ω)|2 ≥N0(ω)¯h/2.
(13.13)
This follows by noticing that all signals located on the boundary of the ϵ-ball have the
same cumulative error per unit angular frequency ϵ2(ω) > 0. Consider one such signal,
for which |an(ω)| = a(ω) > 0 for all 0 ≤n ≤N0(ω); see Figure 13.1. In this case, we
have
ϵ2(ω) = N0(ω)a2(ω).
(13.14)
Since a2(ω) is strictly positive, and by (13.12) it is also quantized, we have a2(ω) ≥¯h/2.
It follows that ϵ2(ω) ≥N0(ω)¯h/2, which is the desired lower bound on the energy of the
error per unit angular frequency. This result shows that the quantization error per unit
angular frequency has energy proportional to the number of dimensions per unit angular
frequency of the radiating signal.
13.2.2
Kolmogorov Entropy Bound
The universal entropy bound for the ϵ-entropy of bandlimited radiation is now stated.
15
12:02:45, subject to the Cambridge Core

13.2 Deterministic Signals
395
The ϵ-entropy of the space of bandlimited signals radiated from the sphere of radius
r →∞is
Hϵ ≤2πrE
¯hc
1
2πe loge + o(rE) bits
(13.15)
if
√
E/ϵ ≥1, and zero otherwise. The bound is tight for signals carrying (1/2)loge
bits per degree of freedom.
The derivation of (13.15) goes as follows. By (13.9) and Parseval’s theorem, we have
ϵ2 ≥
 T/2
−T/2

S
q2(t,r)drdt
= 1
2π
 
−

S
|Fq(ω,r)|2drdω
−ϵ2
T
 ∞
−∞

S
q2(t,r)drdt
≥1
2π
 
−

S
|Fq(ω,r)|2drdω −ϵ2
TE,
(13.16)
where
ϵ2
T = 1 −
 T/2
−T/2

S q2(r,t)drdt
 ∞
−∞

S q2(r,t)drdt .
(13.17)
We now consider a signal located on the boundary of an ϵ-ball and the corresponding
error occurring by approximating it with the center of the ball. Integrating (13.11) over
the bandwidth and using (13.13), we get
 
−

S
|Fq(ω,r)|2drdω ≥
 
−
N0(ω)

n=1
|an(ω)|2dω
≥
 
0
¯hN0(ω)dω.
(13.18)
Substituting (13.18) into (13.16) and using (13.6), we obtain the lower bound
ϵ2 ≥1
2π
 
0
4π
ωr
cπ
2
¯hdω −ϵ2
TE
= ¯h
2π
4πr2
c2π2
3
3 −ϵ2
TE.
(13.19)
Letting the signal-to-quantization ratio
E/ϵ2 = SNRK ≥1,
(13.20)
and using (13.19), we have
E ≥¯h
2π
4πr2
c2π2
3
3 SNRK −Eϵ2
TSNRK.
(13.21)
15
12:02:45, subject to the Cambridge Core

396
Universal Entropy Bounds
From this, it follows that
ϵ2
TE ≥
ϵ2
T
(1 + ϵ2
TSNRK)
¯h
2π
4πr2
c2π2
3
3 SNRK.
(13.22)
Since ϵ2
T tends to zero as T = r/c →∞, it follows that for any ﬁxed value of SNRK ≥1
the right-hand side is o(r2). Plugging this result into (13.19), we have the asymptotic
relation
ϵ2 ≥¯h
2π
4πr2
c2π2
3
3 −o(r2).
(13.23)
Finally, applying (13.20) and (13.23) to (13.5), we obtain
Hϵ = 4π
3
r
cπ
3
log
	
SNRK + o(r3)
≤2πrE
¯hc
logSNRK
2πSNRK
+ o(rE).
(13.24)
Since the function (logSNRK)/SNRK is maximized for SNRK = e, (13.15) follows.
13.2.3
Saturating the Bound
The bound (13.15) is tight when the energy of the quantization error is a minimum,
namely (13.23) holds with equality, and the signal-to-quantization ratio is SNRK = e.
In this case, roughly N0 log√e bits are needed to identify any signal in the space, and the
universal entropy bound is saturated. It follows that in this case the quantized signals’
space is equivalent to a system of
N0 log
	
SNRK = N0 log√e
(13.25)
binary spins, forming
N = 2(N0/2)loge
(13.26)
states, and having Boltzmann entropy
HB = logN
= (N0/2)loge bits,
(13.27)
where N0 is given by
N0 = 4π
3
r
cπ
3
.
(13.28)
By (13.21), the bound (13.15) also requires the energy to be proportional to r2 and
to the cube of the bandwidth. It follows that the entropy of bandlimited radiation can
be proportional to the volume of the radiating body and increases with the cube of the
bandwidth of the radiation. This result should be compared with the analogous one for
black body radiation over an inﬁnite spectrum of frequencies. In that case, the expected
energy is proportional to r3 and to the temperature raised to the fourth power, and the
entropy is proportional to ( ¯Er)3/4. It follows that in both cases the entropy can scale
15
12:02:45, subject to the Cambridge Core

13.3 Stochastic Signals
397
with the volume of the radiating body, although the energy expenditures are different.
Black body radiation reﬂects an equilibrium conﬁguration where the expected energy
per unit volume remains constant as the radiating system expands. On the other hand,
bandlimited radiation achieves the universal entropy bound with an energy per unit
volume that vanishes as the radiating system expands. For bandlimited radiation, the
entropy increases with the bandwidth, while for thermal radiation the temperature plays
the role of an “effective bandwidth” in its entropy expression.
13.3
Stochastic Signals
Analogous results are obtained by considering the rate–distortion function of stochastic
signals having a Gaussian distribution with constant power spectral density over the
bandwidth. In this case, we replace the energy constraint (13.2) with the average one,
 T/2
−T/2

S
E(f2(θ,φ,t))dθdφdt ≤PN0.
(13.29)
By the results in Section 12.4, we have that letting the quantization error associated with
the representation of f(t) using codebook point ˆ
f(t) be
q(t,r) = f −ˆf,
(13.30)
and imposing the average energy per degree of freedom constraint
1
N0
 T/2
−T/2

S
E[q2(t,r)]drdt ≤σ 2,
(13.31)
then the rate–distortion function, measured in bits, is given by
"
Rσ 2 = N0 log(
√
P/σ) + o(N0) bits
if
√
P/σ ≥1,
0
if
√
P/σ < 1.
(13.32)
We now impose a lower bound on the quantization energy σ 2. In the deterministic
setting, dividing the lower bound (13.19) by N0 and using (13.28), we obtain a lower
bound on the empirical mean energy per signal dimension of the quantization error:
ϵ2
N0
≥¯hc
2r −o(1/r).
(13.33)
As shown in Section 13.2.1, the error on any given signal dimension can be zero, but
since the cumulative error has energy proportional to the number of dimensions, the
positive lower bound on the empirical mean (13.33) follows. We now impose the same
constraint on the stochastic mean energy per signal dimension of the quantization error,
σ 2 ≥¯hc
2r −o(1/r);
(13.34)
in other words, the average energy per signal dimension cannot be smaller than one
photonic unit of radiation.
15
12:02:45, subject to the Cambridge Core

398
Universal Entropy Bounds
13.3.1
Shannon Rate–Distortion Bound
The universal bound on the rate–distortion function is now stated.
The Shannon rate–distortion function of the space of Gaussian signals of constant
power spectral density over a ﬁxed bandwidth radiated from the sphere of radius
r →∞is
Rσ 2 ≤2πr ¯E
¯hc
1
2πe loge + o(r ¯E) bits,
(13.35)
where ¯E = PN0. This result holds if
√
P/σ ≥1, otherwise the rate distortion is zero.
The bound is tight for signals carrying (1/2)loge bits per degree of freedom.
The derivation follows by substituting (13.34) into (13.32) and going through the
same steps as in the deterministic case. We have, as r →∞,
Rσ 2 ≤N0 log




P
¯hc
2r −o(1/r)
≤
N0P
¯hc
2r [1 −o(1)]
loge
2e
= 2πr ¯E
¯hc
1
2πe loge[1 + o(1)]
= 2πr ¯E
¯hc
1
2πe loge + o(r ¯E),
(13.36)
which coincides with (13.35).
The bound is tight if the energy of the quantization error is a minimum, so that the
ﬁrst inequality holds with equality, and if P is of the order of 1/r, so that the second
inequality is also tight. By (13.28), this means that the energy of the signal must be of
the order of r2, which is consistent with the analogous result in the deterministic setting.
The results in the two settings are completely analogous and follow from the minimum
quantization error constraints (13.33) and (13.34), which impose an energy of the error
increasing as r2 or, equivalently, an energy of the error per degree of freedom decreasing
as 1/r. The essential steps in the derivation are to use the minimum quantization allowed
by quantum indetermination constraints in the entropy and rate–distortion formulas,
scale the energy to maintain a constant signal-to-quantization ratio, and write the result
in terms of the energy and radius of the system.
13.3.2
Shannon Entropy Bound
By comparing (13.15) and (13.35) with (11.82), we observe a discrepancy of a factor
1/(2πe) between the bounds for radiated signals and the universal entropy bound for the
thermodynamic entropy of arbitrary systems. This is due to the way we have performed
15
12:02:45, subject to the Cambridge Core

13.3 Stochastic Signals
399
quantization in the signal model. In the computation of the ϵ-entropy and rate–distortion
we considered the minimum cardinality covering of the space, and then performed
quantization of each signal dimension with respect to this covering. If instead we
compute the Shannon entropy of an equipartitioned signals’ space where quantization
of the energy is performed separately on each dimension, we obtain an entropy bound
matching (11.82).
The Shannon entropy of the space of Gaussian signals of constant power spectral
density over a ﬁxed bandwidth radiated from the sphere of radius r →∞and whose
energy is quantized at level σ on each dimension of the signals’ space is
H ≤2πr ¯E
¯hc
loge + o(r ¯E) bits,
(13.37)
where ¯E = PN0 and
√
P/σ ≥1.
This result follows by noticing that the number of typical quantized signals is in this
case given by the volume of the typical set divided by the volume of the box of side
length σ,
N =
√
2πeP
σ
N0
.
(13.38)
These signals are roughly equally probable, each having probability of occurrence
pn = 1/N for all n ∈[1,N]. This corresponds to an equilibrium conﬁguration where
the Shannon entropy attains Boltzmann’s form:
H =
N

n=1
−pn logpn
= logN
= N0 log
	
2πeP/σ 2.
(13.39)
We now apply the lower bound on the energy of the quantization error per degree of
freedom and substitute (13.34) into (13.39). Letting ¯E = PN0 and following the usual
steps, we obtain
H ≤2πeP2r
¯hc
N0
2e loge + o(rN0)
= 2πr ¯E
¯hc
loge + o(rE) bits,
(13.40)
which coincides with (13.37).
The geometric reason for the disappearance of the factor 1/(2πe) should be clear: in
the case of the Kolmogorov entropy we cover a space of signals of radius E with balls
15
12:02:45, subject to the Cambridge Core

400
Universal Entropy Bounds
of radius ϵ, so that the number of quantized signals is roughly
N =
E
ϵ
N0
.
(13.41)
Similarly, in the case of the rate–distortion function, the volume of the typical set is
(
√
2πeP)N0 and the volume of the typical error region is (
√
2πeσ)N0, so that the number
of quantized signals is roughly
N =
√
2πeP
√
2πeσ
N0
=
P
σ
N0
.
(13.42)
On the other hand, in the case of the Shannon entropy the volume of the typical set is
(
√
2πeP)N0, but quantization occurs independently on each coordinate of the space,
which geometrically corresponds to covering the space using a grid, with each cell
having volume σ N0, and we have
N =
√
2πeP
σ
N0
.
(13.43)
It is important to note, however, that from the discussion in Section 1.4.4, we have that
(13.43) holds only for σ 2 ≪P, namely in the high signal-to-quantization ratio regime.
Since tightness of the bound requires P and σ 2 to be of the same order, the accuracy
of the derivation, which is precise for high values of the signal-to-quantization ratio, is
traded off by the tightness of the obtained bound.
13.4
One-Dimensional Radiation
We now revisit results in the one-dimensional case, where N0 = T/π. Given a
minimum quantization energy
ϵ2 ≥¯h,
(13.44)
we obtain the entropy bound
Hϵ = N0 log(
√
E/ϵ) + o(N0)
≤N0
E
ϵ2
loge
2e + o(N0)
≤TE
¯h
loge
2πe + o(T) bits.
(13.45)
Similarly, in the stochastic setting using the lower bound on the quantization error per
signal dimension
σ 2 ≥¯h
N0
,
(13.46)
we obtain the rate–distortion bound
Rσ ≤T ¯E
¯h
loge
2πe + o(T) bits,
(13.47)
15
12:02:45, subject to the Cambridge Core

13.5 Applications
401
and also the entropy bound
H ≤T ¯E
¯h loge + o(T) bits.
(13.48)
These bounds are tight when the energy of the quantization error is a minimum and the
signal’s energy is a constant of the same order as the error. As in the three-dimensional
case, the largest entropy signal carries (1/2)loge bits per degree of freedom, and the
size of the observation interval T plays the role of the volume of the space.
13.5
Applications
We now discuss in what settings the universal entropy bounds computed in the previous
sections are applicable. First, we observe that they ignore gravitational constraints, so
they must be intended for weak self-gravitating systems, that are far from gravitational
collapse. Second, by identifying information with energy, they consider all available
degrees of freedom at the most fundamental level. Exploiting all of these degrees of
freedom would require incredible technological advancements and the emergence of
novel quantum communication architectures.
13.5.1
High-Energy Limits
From a physical perspective, the main conclusion that can be drawn from the
computations above is that information can be identiﬁed with energy, but also that
space can be considered as an information-bearing object. For a single “burst” of
energy of vanishing density propagating inside a spherical volume and occupying a
ﬁnite bandwidth, the entropy is proportional to the volume of the sphere and to the
energy, which scales with the square of the radius. Larger volumes allow for a larger
number of waveform conﬁgurations, and these in turn lead to larger entropy values. For
one-dimensional systems, the transmission time plays the role of the volume, and the
entropy of a waveform of ﬁxed energy grows with the transmission time. In comparison,
black body radiation occurring over an inﬁnite bandwidth achieves the same entropy
scaling with the volume of the system, but requires an energy expenditure proportional
to the volume of the expanding system.
When the energy of the system becomes so large that gravitational constraints must
be taken into account, universal entropy bounds must be modiﬁed accordingly. By
imposing a quantization of the error per signal dimension, the ϵ-balls must grow
with the number of dimensions, as indicated in (13.23). It follows that to have a
signal-to-quantization ratio SNRK ≥1, the energy of the signal must also grow at least
as the square of the radius of the system, as indicated in (13.21). This scaling eventually
conﬂicts with gravitational constraints.
As discussed in Chapter 11, general relativity imposes that the energy can be at most
a linear function of the radius, to avoid collapse of the source of radiation and formation
of a black hole. In practice, however, the quadratic lower bound (13.21) and the linear
15
12:02:45, subject to the Cambridge Core

402
Universal Entropy Bounds
upper bound (11.78) from the theory of gravity can be satisﬁed with orders of magnitude
to spare. To get an idea of the scales involved, assuming a signal-to-quantization ratio
SNRK = e, and unit bandwidth, for lower and upper bounds to be comparable r must
be a huge number, orders of magnitude larger than the size of the observable universe.
It follows that at any reasonable scale the universal entropy bound provides a good
indication of the maximum entropy growth.
One may also ask what happens if for a reasonable scale of r the energy approaches
the critical value for gravitational collapse. In this case of extremely high energy,
substituting (11.78) for E into (13.15) and (13.37), using the deﬁnition of Planck length
(11.76) and ignoring second-order terms, we obtain the black hole entropy bound for
the ϵ-entropy of
Hϵ ≤4πr2
4ℓ2p
1
2πe loge bits,
(13.49)
and for the Shannon entropy of
H ≤4πr2
4ℓ2p
loge bits,
(13.50)
which corresponds to (11.96). This simple substitution does not account for the complex
physical processes that occur as E approaches the critical value for collapse, but it
provides the correct result supported by more detailed physical arguments.
In comparison, the analogous gravitational bound for black body radiation (11.80)
computed in Chapter 11 indicates that a lower entropy value is approached as the black
body approaches gravitational collapse. Not surprisingly, black body radiation using a
higher energy expenditure to obtain the same volumetric order of growth of the entropy
reaches gravitational collapse at a lower entropy value.
13.5.2
Relation to Current Technologies
Since universal entropy bounds take into account all available degrees of freedom,
when compared to current state-of-the-art engineering systems they are extremely loose.
Let us consider a few examples. A ﬂash memory card of the characteristic size of a
centimeter and weight of a few grams can store about one terabyte of data, about 1012
bits. One gram of DNA of the same size can be used to store about 1015 bits. The
internet connects some 3 billion users, and it is projected to grow to 8–10 billions of
connected devices in the next 10 years. Assuming each connected device also stores
one terabyte of information, the entire state of the internet could be described by 1022
bits. These may seem like large numbers, but they quickly disappear in comparison to
nature’s limits. If we apply the universal entropy bound to our little memory card using
Einstein’s equation for the energy, E = mc2, we obtain a bound of the order of 1038 bits.
Although this shows much room for technological improvement, by (13.27), saturating
the universal entropy bound would require converting the entire mass–energy of our
memory card into radiation carrying information at a rate of (1/2)loge bits per degree
of freedom, and by doing so it considers all available degrees of freedom at the most
15
12:02:45, subject to the Cambridge Core

13.6 On Models and Reality
403
fundamental level. Conventional devices use many more degrees of freedom than are
available in nature to store their bits. Nature’s limits, although important for matters of
principle, are currently far from our reach.
Conceptually, the derivation of the universal entropy bounds provided in
Sections 13.2 and 13.3 is very simple: we computed the maximum entropy and
rate–distortion of bandlimited radiated signals when quantization is the minimum
allowed by quantum indetermination constraints, and wrote the result in terms of
the energy and radius of the system. This means that in order to exploit this amount of
information, we need to be able to observe all the states of the system up to the quantum
level. In this way, all degrees of freedom of the signal are saturated. Although we are
far from reaching this capability, with the emergence of computing and communication
devices using novel quantum architectures, degrees of freedom will be exploited more
and more efﬁciently. For example, atomic-scale data storage has demonstrated storage
of one bit of information using 12 iron atoms tightly packed over a copper substrate
of size of the order of 1 nm2. Ignoring the substrate over which the atoms are placed,
the universal entropy bound for a nanometer ball ﬁlled by the same atomic mass gives
about 1010 bits. Subatomic-scale storage, exploiting degrees of freedom of quantum
wave functions representing subatomic particles, promises to shrink the gap even
further. The possibility of storing information using the degrees of freedom of waves
at the quantum level is becoming not so far-fetched. It seems reasonable to expect
that progress will eventually meet nature’s ultimate information limits, and that in
the far future universal entropy bounds will play an important role in assessing the
technological capabilities of human-made systems.
13.6
On Models and Reality
Information-theoretic bounds are aimed at describing information in physical systems
at the most fundamental level. For continuous signals they arise by imposing the natural
constraints of information theory on energy, dimensionality, and resolution. The energy
determines the size of the signals’ space, and its increase leads to a corresponding
increase in the amount of information required to specify any arbitrary signal in the
space. The dimensionality determines the number of coordinates required to identify
the signal, and corresponds to the degrees of freedom of the space. The resolution
determines the inherent uncertainty at which these coordinates can be observed.
Physically, these constraints are not independent of each other. The energy depends
on the frequency of radiation, and this inﬂuences the number of degrees of freedom and
the granularity at which each signal can be observed. The size of the system also plays
a role in determining the number of degrees of freedom and the resolution.
Of course, a physical system can in principle contain much more information than can
be externally observed and imposed by the above constraints. As noted in Chapter 1,
without any quantization constraint on the observation, the amount of information,
measured in bits, is inﬁnite. In this case, a continuum information model would consider
the whole volume of the signals’ space, containing inﬁnitely many points, as a relevant
15
12:02:45, subject to the Cambridge Core

404
Universal Entropy Bounds
information measure. In a stochastic setting, by the asymptotic equipartition property,
this depends on the differential entropy and on the number of degrees of freedom of the
signals’ space. On the other hand, physics dictates that continuous signals are always
observed in a quantized fashion, so discrete measures of information may be more
appropriate in practical settings, where physical quantities are observed.
The question of whether the amount of information “observed” and the amount
“contained” in a system differ from each other takes us into the realm of the philosophy
of science. Much has been written on whether physics is inherently digital or not,
namely whether unobservable information should be considered equivalent to the
absence of information and reality should be considered “digital.” In short: should we
consider a continuum volume of information, or a discrete set of bits?
Proponents of the digital hypothesis believe that physical systems can only have a
ﬁnite number of discrete states, and therefore physical processes can be reduced to
digital computations. This leads to a strong positivist interpretation of universal entropy
bounds as ultimate physical limits. If instead one embraces the point of view that
the digital hypothesis is an unscientiﬁc ansatz in the sense of Popper, then a weaker
interpretation of universal bounds would simply be that no measurement can ever
prove that a physical system must be described by more than a ﬁnite number of bits.
Nevertheless, a continuum of information hidden to the observer remains a physical
possibility. Both points of view are consistent with information-theoretic derivations,
but each requires a radically different philosophical stand.
A more relaxed position is the instrumentalist point of view, summarized by the
phrase: “shut up and calculate!” This advocates refraining from any philosophical
interpretation of the results of a mathematical model. According to this school of
thought, scientiﬁc models are not meant to represent reality but are only instruments that
allow us to describe and predict observations. In this case, whether reality is continuous
or discrete does not matter at all. Maxwell’s equations and quantum mechanics are
regarded as mere inventions, each appropriate for describing phenomena observed
within a certain range of conditions. A profound consequence of this point of view is that
physics, like mathematics, is an evolving science that cannot be reduced to a ﬁnite set of
rules. Every model is inherently incomplete and subject to paradigm changes. Physics,
much like mathematics, is subject to ambiguity and paradox, and these naturally lead to
its continuous evolution. There may be no ultimate physical laws, and scientiﬁc crises
are natural and unavoidable.
In this book, we refrained from defending any of the above points of view. We
argued that information-theoretic limits, including universal entropy bounds, arise in
a mathematical model of continuous signals subject to constraints. The identiﬁcation
of these constraints is required for any rigorous derivation, but whether they can be
relaxed or are the pillars of an untouchable physical theory is not something we can, or
wish to, address. The mathematical laws of spectral and probabilistic concentration that
are at the basis of our arguments are also at the basis of quantum uncertainty relations
and statistical mechanics. Whether “real” information obeys these rules or whether they
are ad hoc inventions, is irrelevant for a theory of information that rigorously predicts
limits within the context of a speciﬁc mathematical model. As we stated at the beginning
15
12:02:45, subject to the Cambridge Core

13.7 Summary and Further Reading
405
of the book, information is described mathematically, and this description requires
conﬁnement to a model with constraints. Information also has a physical structure, in
the sense that the model applies to the real world, or at least to the world that we are able
to observe. Although in what sense the model is applicable is subject to interpretation,
the theory itself should not be. Instead, a fundamental requirement is that the theory
should be robust, insensible to nuisances and perturbations of second-order details of
the model.
A robust theory is more likely to be widely accepted by positivists and less prone to
the paradigm shifts required by instrumentalists. Throughout the book, we have shown
that the basic results of wave theory of information are robust. They remain essentially
unaltered whether one works in a probabilistic or a deterministic setting, and whether
one adopts a continuum approach based on degrees of freedom, typical volumes, and
differential entropies, or an approach based on discrete information measures like
entropies and capacities. After all, if we could summarize in one sentence the point of
view of this book, we do not need to run onto Scylla wishing to avoid Charybdis, but we
can safely navigate one of the many paths that lead to a rigorous notion of information.
13.7
Summary and Further Reading
The universal entropy bound for arbitrary matter systems was ﬁrst proposed by
Bekenstein (1981a). Its original derivation was based on a thought experiment involving
black hole thermodynamics. Consider an object much smaller than a black hole and
drop it into the black hole. As a result, the area of the event horizon of the black hole
increases. The area increase is also proportional to the increase in entropy of the black
hole. The computation of the area increase and an application of the second law leads to
the universal entropy bound on the entropy of the object. This derivation was criticized
and defended on several occasions in the literature; see Bekenstein (2005) for a review.
An earlier proposal of a similar bound for one-dimensional radiation was also made
by Bremermann (1967, 1982), using an heuristic argument that treated quantization as
Gaussian uncertainty. Physically, a linear energy bound on entropy is generally accepted
for weakly self-gravitating isolated objects, and several mathematical derivations based
solely on quantum state counting and not involving gravity appeared in the literature;
see, for example, Bekenstein and Schiffer (1990) for a review, and Schiffer (1991).
The derivation provided here is for the speciﬁc setting of bandlimited electromagnetic
radiation; it is based on spectral concentration arguments, and appeared in Franceschetti
(2017).
A more general entropy bound for high-energy systems is the holographic bound
discussed in Chapter 11. A general conclusion to be drawn from this, as well as
the universal entropy bound, is that the number of degrees of freedom available
in nature is much larger than is exploited in current engineering systems. Full
exploitation of nature’s degrees of freedom would require conversion of all matter into
radiation energy and manipulation of the resulting quantum wave functions. Clearly,
as information-theoretic bounds become more general, their range of applicability
15
12:02:45, subject to the Cambridge Core

406
Universal Entropy Bounds
becomes more limited. Universal bounds are useful, however, to understand physical
limits and to have an idea of the orders of magnitude at which natural information
systems can be described. For the encoding capabilities of current technologies that
were brieﬂy discussed in Section 13.5.2 see Moon et al. (2009), Church, Gao, and
Kosuri (2012), and Loth et al. (2012).
Finally, the philosophical discussion of whether universal bounds reﬂect real limits
or are the result of artiﬁcial constraints imposed in a mathematical model is ongoing.
This debate is related to whether reality is discrete or continuous, and to whether
a complete physical theory can ever be developed. Views here vary widely. Some
interesting points on the philosophy of information theory are raised by Lee (2017),
and for the role of ambiguity in mathematical theories see Byers (2007). Robustness of
information-theoretic models has been prominently advocated by Slepian (1976).
13.8
Test Your Understanding
Problems
13.1
The lower bounds (13.33) and (13.34) are the fundamental ingredients of the
derivation of the universal entropy bounds. Provide a justiﬁcation for these bounds in
terms of the uncertainty principle.
Solution
The standard deviation of the time spread of the quantum uncertainty multiplied by the
frequency spread must be at least 1/2. The time spread of the energy is roughly T, and
the frequency spread of the energy is roughly . Since T = r/c, it follows that
 ≥c/(2r).
(13.51)
Since each photon carries a quantum of energy ¯hω, the energy uncertainty of the signal
is roughly σ 2 = ¯h, and multiplying both sides of (13.51) by ¯h we obtain the desired
result.
13.2
Discuss the tightness of the universal bounds on the ϵ-entropy, rate–distortion
function, and Shannon entropy.
13.3
Illustrate the difference between the entropy of black body radiation and the
maximum entropy of bandlimited radiation in one- and three-dimensional settings.
13.4
Consider one-dimensional bandlimited radiation and recall that universal entropy
bounds are tight when the signal carries log√e bits per degree of freedom. Show that
this also corresponds to a number of bits per second proportional to
	¯P/¯h, where ¯P is
the energy per unit time. (Hint: follow the argument given in Section 12.2.6. Note that
each degree of freedom is represented by a mode of radiation of bandwidth , compute
the minimum detectable energy of one mode of radiation, and from this an upper bound
on the number of possible detectable modes. Finally, write an equation analogous to
(12.49) to compute the number of bits per second.)
15
12:02:45, subject to the Cambridge Core

Appendix A: Elements of Functional
Analysis
A.1
Paley–Wiener Theorem
The Paley–Wiener theorem relates the decay properties of the Fourier transform at
inﬁnity and the analyticity properties of its corresponding inverse transform. The most
extreme form of decay at inﬁnity of a Fourier transform, which is of interest to us, is to
be bandlimited, namely to have compact support in the spectral domain. This yields a
strong regularity property of the function, namely the inverse transform is the restriction
over the real line of an entire function of a complex variable, and thus it can be expanded
in a power series that converges everywhere. Furthermore, the rate of growth at inﬁnity
of the entire function is bounded by an exponential of .
Let f ∈L2(−∞,∞) : R →R. We have that f ∈B() if and only if the complex
extension
f(z) =
 ∞
−∞
F(ω)exp(jωz)dω, z = x + jy, y > 0, x,y ∈R
(A.1)
is an entire function of exponential type , namely there exists a constant C such
that
|f(z)| ≤Cexp(|z|).
(A.2)
A.2
Uncertainty Principle over Arbitrary Measurable Sets
Let f ∈L2, with Fourier transform
Ff = F(ω) =
 ∞
−∞
f(t)exp(jωt)dt,
(A.3)
and norm
∥f∥=
 ∞
−∞
|f(t)|2dt
1/2
= 1.
(A.4)
By Parseval’s theorem, we have
∥F∥= 2π.
(A.5)
16
12:04:44, subject to the Cambridge Core

408
Elements of Functional Analysis
Let ST and S be sets of measure T and 2, and f be ϵT-concentrated over ST, namely
1 −

ST
f 2(t)dt ≤ϵ2
T,
(A.6)
and ϵ-concentrated over S, namely
1 −1
2π

S
|F(ω)|2dω ≤ϵ2
.
(A.7)
The uncertainty principle poses a lower bound on the product of the measures of
the concentration sets, namely
T
π ≥[1 −(ϵT + ϵ)]2.
(A.8)
To prove the uncertainty principle, we need some deﬁnitions and some properties of
operators on Hilbert spaces. Let the timelimiting operator be
T f =
 f
t ∈ST,
0
otherwise,
and the bandlimiting operator be
Bf = F−1 
BFf,
(A.9)
where

BF =
 F
ω ∈S,
0
otherwise.
It follows that f is ϵT-concentrated on ST if and only if
∥f −T f∥< ϵT,
(A.10)
and f is ϵ-concentrated on Sω if and only if
∥f −Bf∥< ϵ.
(A.11)
Consider the integral operator
Pf(t) =
 ∞
−∞
P(s,t)f(s)ds.
(A.12)
The operator norm is deﬁned as
∥P∥=
sup
f∈L2,f̸=0
∥Pf∥
∥f∥,
(A.13)
and the Hilbert–Schmidt norm as
∥P∥HS =
 ∞
−∞
 ∞
−∞
|P(s,t)|2dsdt
1/2
.
(A.14)
16
12:04:44, subject to the Cambridge Core

A.3 Generalized Fourier Series
409
The operator norm is dominated by the Hilbert–Schmidt norm,
∥P∥≤∥P∥HS;
(A.15)
namely, for all f,
∥Pf∥≤∥f∥· ∥P∥HS.
(A.16)
This domination result is a consequence of the Schwartz inequality,

 ∞
−∞
P(s,t)f(s)ds

2
≤
 ∞
−∞
|P(s,t)|2ds ·
 ∞
−∞
|f(s)|2ds.
(A.17)
Integrating both sides of (A.17), we get
 ∞
−∞

 ∞
−∞
P(s,t)f(s)ds

2
dt ≤
 ∞
−∞
 ∞
−∞
|P(s,t)|2ds ·
 ∞
−∞
|f(s)|2ds

dt
=
 ∞
−∞
|f(s)|2ds ·
 ∞
−∞
 ∞
−∞
|P(s,t)|2dsdt,
(A.18)
which is equivalent to (A.16).
Using the deﬁnitions above, the Hilbert–Schmidt norm of the operator BT can easily
be computed to be
∥BT ∥HS =
T
π
1/2
.
(A.19)
It then follows from the domination result that
∥BT ∥≤
T
π
1/2
.
(A.20)
The proof of (A.8) is completed by performing the following computation:
1 −∥BT f∥= ∥f∥−∥BT f∥
≤∥f −BT f∥
= ∥f −Bf + Bf −BT f∥
≤∥f −Bf∥+ ∥Bf −BT f∥
≤∥f −Bf∥+ ∥B∥· ∥f −T f∥
≤ϵT + ϵ,
(A.21)
where the last inequality follows from ∥B∥= 1, (A.10), and (A.11).
A.3
Generalized Fourier Series
Orthogonal basis representations of a normed space can be viewed as a generalization
of the Fourier series representation. Consider the set of functions in L2[a,b]
 = {φn : [a,b] →C}
(A.22)
16
12:04:44, subject to the Cambridge Core

410
Elements of Functional Analysis
that are pairwise orthogonal with respect to the inner product
⟨φn,φm⟩w =
 b
a
φn(x)φ∗
m(x)w(x)dx =
 ∥φn∥w
if n = m,
0
otherwise,
(A.23)
where w(x) is the weight function.
The generalized Fourier series of f ∈L2[a,b] with respect to the orthogonal set  is
f(x) =
∞

n=0
cnφn(x),
(A.24)
where the coefﬁcients are given by
cn = ⟨f,φn⟩w
∥φn∥w
.
(A.25)
If  is a complete set, then the equality in the representation holds for all f ∈L2[a,b] in
the given norm. Namely,
lim
N→∞∥f(x) −fN(x)∥w = 0,
(A.26)
where
fN(x) =
N−1

n=0
cnφn(x).
(A.27)
The Bessel inequality for any set of orthogonal functions  is
∞

n=0
|cn|2 ≤
1
∥φn∥w
 b
a
|f(x)|2dx,
(A.28)
and the Parseval identity for any complete set of orthogonal functions  is
∞

n=0
|cn|2 =
1
∥φn∥w
 b
a
|f(x)|2dx.
(A.29)
Many orthogonal representations are commonly used to represent square-integrable
functions. The corresponding basis functions arise as solutions of boundary value
problems in physics. The complex exponentials of the Fourier series are one common
example. The sinc(·) functions of the cardinal series and the prolate spheroidal wave
functions are other notable examples. We summarize some common classes of special
orthogonal functions in Table A.1.
A.4
Self-Adjoint Hilbert–Schmidt Operators
Some auxiliary results on self-adjoint Hilbert–Schmidt operators introduced in
Chapter 3 are described below.
16
12:04:44, subject to the Cambridge Core

A.4 Self-Adjoint Hilbert–Schmidt Operators
411
Table A.1 Orthogonal representations.
Name
f(x)
[a,b]
w(x)
Fourier
exp(jnx)
(−π,π)
1
Cardinal
sinc(x −n)
(−∞,∞)
1
Prolate spheroidal
ϕn(x)
(−∞,∞), and (−1,1)
1
Chebyshev polynomials, 1st kind
Tn(x)
(−1,1)
(1 −x2)−1/2
Chebyshev polynomials, 2nd kind
Un(x)
(−1,1)
(1 −x2)−1/2
Hermite polynomials
Hn(x)
(−∞,∞)
exp(−x2)
Laguerre polynomials
Lα
n(x)
[0,∞)
exp(−x)
Legendre polynomials
Pn(x)
(−1,1)
1
Jacobi polynomials
P(ν,μ)
n
(x)
(−1,1)
(1 −x)ν(1 −x)μ
A.4.1
Variational Characterization of the Eigenvalues
For a wide class of linear self-adjoint operators, the eigenvalues can be characterized
by three basic variational principles. Here, we refer to Hilbert–Schmidt self-adjoint
operators with scalar product
⟨u,v⟩=
 d
c
u(x)v∗(x)dx,
(A.30)
but more general versions of these results are also available in the literature. Consider a
self-adjoint Hilbert–Schmidt operator K : H →H . Let XN ⊂H be an N-dimensional
subspace of H . Let the Rayleigh quotient be
RK(x) = ⟨Kf,f⟩
⟨f,f⟩, f ̸= 0.
(A.31)
Let λ0 ≥λ1 ≥··· be the positive eigenvalues of K, counted with multiplicities, and
ψ0,ψ1,... the corresponding eigenfunctions satisfying
Kψn = λnψn.
(A.32)
Assume that there are at least N + 1 positive eigenvalues, and indicate by f ⊥Xn if f is
orthogonal to each element of Xn. We have:
• Rayleigh’s principle:
λN = sup{RK(f) : ⟨f,ψn⟩= 0, n = 0,...,n −1}.
(A.33)
• Max–min principle of Poincaré:
λN = sup
XN+1
inf
f∈XN+1,f̸=0RK(f).
(A.34)
• Min–max principle of Courant, Fisher, and Weyl:
λN = inf
XN
sup
f⊥XN,f̸=0
RK(f).
(A.35)
16
12:04:44, subject to the Cambridge Core

412
Elements of Functional Analysis
The negative eigenvalues of the operator can be characterized by the same principles
as above by replacing the minimum by the maximum and vice versa.
A.4.2
Mercer’s Theorem
Sometimes it is convenient to obtain a series representation of a self-adjoint
Hilbert–Schmidt kernel with stronger convergence properties than L2 convergence.
Mercer’s theorem provides a general result for the absolute and uniform convergence
of the series representation of a self-adjoint Hilbert–Schmidt kernel based on the
spectral representation of the corresponding operator, provided that some regularity
assumptions are made beside square-integrability.
Consider a Hilbert–Schmidt, non-negative, self-adjoint operator K′K : L2[c,d] →
L2[c,d], with continuous kernel K′K(x,y). If {λn} and {φn} are the eigenvalues and
corresponding eigenvectors of K′K, then for all x,y ∈[c,d] we have
K′K(x,y) =
∞

n=0
λnφn(x)φ∗
n(y),
(A.36)
where convergence is absolute and uniform (and hence point-wise) on [c,d]×[c,d].
A.4.3
Young’s Inequality
Let p,q,r ≥1 be such that
1 + 1
r = 1
p + 1
q.
(A.37)
Let f ∈Lp(Rn) and g ∈Lq(Rn). Young’s inequality for convolutions is
∥f ∗g∥Lr ≤∥f∥Lp · ∥g∥Lq,
(A.38)
where
∥f(x)∥Lp =

Rn |f(x)|pdx
1/p
(A.39)
and
f ∗g(x) =

Rn g(x −y)f(y)dy.
(A.40)
In Chapter 8, we encountered Young’s inequality in the following context: let p = 2,
q = 1, r = 2, g ∈L2(P), and f ∈L1(V), where
P = {x −y,x ∈S,y ∈V}.
(A.41)
In this case, we have
∥f ∗g∥2
L2(S) =

S
|f ∗g(x)|2dx =

S


V
g(x −y)f(y)dy

2
dx,
(A.42)
16
12:04:44, subject to the Cambridge Core

A.5 Fredholm Integral Equations
413
and Young’s inequality is modiﬁed as follows:

S


V
g(x −y)f(y)dy

2
dx ≤sup
y∈V

S
|g(x −y)|2dx ·

V
|f(y)|dy
2
.
(A.43)
This result follows by ﬁrst noting that, by the Schwarz inequality,


V
g(x −y)f(y)dy

2
≤

V
|g(x −y)| · |f(y)|dy
2
≤

V
|g(x −y)|2|f(y)|dy

V
|f(y)|dy

,
(A.44)
and, integrating over S and using Fubini’s theorem, we obtain

S


V
g(x −y)f(y)dy

2
dx ≤

S

V
|g(x −y)|2|f(y)|dy

V
|f(y)|dy

dx
=

V
|f(y)|dy

S

V
|g(x −y)|2|f(y)|dydx
=

V
|f(y)|dy

V
|f(y)|

S
|g(x −y)|2dxdy
≤

V
|f(y)|dy

V
|f(y)|

sup
y∈V

S
|g(x −y)|2dx

dy
= sup
y∈V

S
|g(x −y)|2dx

V
|f(y)|dy
2
.
(A.45)
In the case S = Rn, we have

Rn |g(x −y)|2dx =

Rn |g(x)|2dx,
(A.46)
and we obtain (A.38) without having to take the supremum.
A.5
Fredholm Integral Equations
A.5.1
First Kind
A Fredholm integral equation of the ﬁrst kind is of the form
g(x) =
 b
a
K(x,y)f(y)dy,
(A.47)
where g and K are given and f is the unknown function. If all the functions are in
L2, the limits are inﬁnite, and the kernel K is of the special form K(x −y), then the
right-hand side is a convolution and the solution is immediately given by the inverse
Fourier transform,
f(x) = F−1
 F[f(x)](y)
F[K(x)](y)
#
.
(A.48)
16
12:04:44, subject to the Cambridge Core

414
Elements of Functional Analysis
A.5.2
Second Kind
An inhomogeneous Fredholm integral equation of the second kind is of the form
f(x) = α
 b
a
K(x,y)f(y)dy + h(x),
(A.49)
where h and K are given and f is the unknown function. Its homogeneous version is
obtained by letting h(x) = 0, yielding
f(x) = α
 b
a
K(x,y)f(y)dy.
(A.50)
The resolvent formalism for the inhomogeneous Fredholm integral equation of the
second kind (A.49) is the Liouville–Neumann series
f(x) =
∞

n=0
αnφn(x),
(A.51)
where
φ0(x) = h(x),
(A.52)
φn(x) =
 b
a
Kn(x,y)f(y)dy, n ≥1,
(A.53)
and
Kn(x,y) =
 b
a
 b
a
···
 b
a
K(x,y1)K(y1,y2)···K(yn−1,y)dy1 ···yn−1.
(A.54)
This is obtained by solving the equation by iterative approximations. Taking the
function f−1(x) = 0 as the ﬁrst approximation, we obtain the successive approximations
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
f0(x) = h(x)
f1(x) = h(x) + α
 b
a
K(x,y1)h(y1)dy1
f2(x) = h(x) + α
 b
a
K(x,y1)h(y1)dy1 + α2
 b
a
 b
a
K(x,y1)K(y1,y2)h(y2)dy1dy2
...
We are then led to the series
lim
n→∞fn(x) =
∞

n=0
αnφn(x).
(A.56)
The resolvent formalism is indeed a solution if the series converges uniformly:
If the Liouville–Neumann series converges uniformly, then its sum is a solution of
the Fredholm integral equation of the second kind (A.49).
16
12:04:44, subject to the Cambridge Core

A.6 Spectral Concentration: Arbitrary Measurable Sets
415
This result follows immediately: noting that in this case term by term integration is
permissible, and substituting (A.51) into the integral in (A.49), we obtain f(x) −h(x).
The resolvent kernel for the Fredholm integral equation of the second kind is then
deﬁned as
K(x,y;α) =
∞

n=0
αnKn+1(x,y),
(A.57)
so that the solution is also written in integral form as
f(x) =
 b
a
K(x,y,α)f(y)dy.
(A.58)
A.6
Spectral Concentration over Arbitrary Measurable Sets
A special case of a Hilbert–Schmidt self-adjoint operator is the duration frequency
limiting one TPBQTP encountered in Section 3.5.3. A general result regarding the
behavior of the eigenvalues of this operator is stated in Section 3.5.4 and proved here.
Let P and Q be measurable sets in RN. For any real matrix A of size N × N, we
indicate by AP the set of points of the form Ax, where x is the column vector composed
of the elements of x ∈P. We also indicate with |A| the determinant of A, with AT the
transpose of A, and with U the unit ball in L2(RN). We let A = A(τ) and B = B(ρ)
for real parameters τ and ρ. The case in which either matrix is constant or depends on
multiple parameters is completely analogous.
For any 0 < ϵ < 1, let Nϵ(A,B) be the number of eigenvalues of TAPBBQTAP not
smaller than ϵ. If
lim
(τ,ρ)→∞BTAU = RN,
(A.59)
then we have
lim
(τ,ρ)→∞
Nϵ(A,B)
|A||B| = (2π)−Nm(P)m(Q).
(A.60)
We let KA,B(x,y) be the kernel of the operator UA,B = TAPBBQTAP having eigenvalues
{λk}. We have
KA,B(x,y) = 1AP(x)1AP(y)hB(x −y),
(A.61)
so that
KA,B(x,x) = 1AP(x)hB(0).
(A.62)
All that is required is to establish Claim A.1 and Claim A.2 below, as they imply the
statement of the result. The two claims state that all eigenvalues must be either close
to one or close to zero, since both their sum and the sum of their squares have the
same scaling order. The result then follows by noticing that the sum of the eigenvalues
16
12:04:44, subject to the Cambridge Core

416
Elements of Functional Analysis
essentially corresponds to the number of non-zero eigenvalues and is of the order of
|A||B|. We prove the two claims ﬁrst, and then justify the implication that yields the
ﬁnal result.
Claim A.1 
k λk = |A||B|(2π)−Nm(P)m(Q).
Claim A.2 
k λ2
k = |A||B|(2π)−Nm(P)m(Q) + o(|A||B|).
Proof of Claim A.1
By Mercer’s theorem, there exists an orthonormal basis set {φk} for L2(RN) such that
KA,B(x,y) =

k
λkφk(x)φk(y).
(A.63)
By orthonormality and (A.62), we have

RN 1AP(x)hB(0)dx =

k
λk,
(A.64)
and performing the computation

RN 1AP(x)hB(0)dx = hB(0)m(AP)
= (2π)−Nm(BQ)m(AP)
= (2π)−N|A| |B| m(Q)m(P)
=

k
λk
(A.65)
establishes Claim A.1.
□
Proof of Claim A.2
We let K(2)
A,B(x,y) be the kernel of the operator U2
A,B = (TAPBBQTAP)2 having eigenvalues
{λ2
k}. We have
K(2)
A,B(x,y) =

RN KA,B(x,z)KA,B(z,y)dz
=

RN 1AP(x)1AP(z)hB(x −z)
1AP(z)1AP(y)hB(z −y)dz
= 1AP(x)1AP(y)

AP
hB(x −z)hB(z −y)dz.
(A.66)
By Mercer’s theorem, there exists an orthonormal basis set {ψn} for L2(RN) such that
K(2)
A,B(x,y) =

k
λ2
kψk(x)ψk(y).
(A.67)
By orthonormality, we have

RN K(2)
A,B(x,x)dx =

k
λ2
k.
(A.68)
16
12:04:44, subject to the Cambridge Core

A.6 Spectral Concentration: Arbitrary Measurable Sets
417
By (A.66) it follows that

RN K(2)
A,B(x,x)dx =

AP

AP
|hB(x −y)|2dxdy.
(A.69)
We apply the change of variable x = Ap, obtaining

RN K(2)
A,B(x,x)dx = |A|

AP
dy

P
|hB(Ap −y)|2dp
= |A|

P
dp

AP
|hB(Ap −y)|2dy.
(A.70)
We apply another change of variable u = Ap −y, obtaining

RN K(2)
A,B(x,x)dx = |A|

P
dp

A(p−P)
|hB(u)|2du.
(A.71)
Substituting (A.71) into (A.68) and dividing by |A||B|, we have
1
|A||B|

k
λ2
k =

P
FA,B(p)dp,
(A.72)
where
FA,B(p) = |B|−1

A(p−P)
|hB(u)|2du.
(A.73)
The function FA,B(p) is dominated as
FA,B(p) ≤|B|−1

RN |hB(u)|2du
= (2π)−N|B|−1m(BQ)
= (2π)−Nm(Q),
(A.74)
which is integrable over P. Next, we show that
lim
(τ,ρ)→∞FA,B(p) = (2π)−Nm(Q),
(A.75)
so that by Lebesgue’s dominated convergence theorem, we have
lim
(τ,ρ)→∞
1
|A||B|

k
λ2
k =

P
lim
(τ,ρ)→∞FA,B(p)dp
= (2π)−Nm(P)m(Q),
(A.76)
establishing Claim A.2.
□
What remains is to prove (A.75). Substituting the result in Claim A.3 below into
(A.73) and performing the change of variable BTu = v, we have
FA,B(p) = |B|−1

A(p−P)
|B|2 |h(BTu)|2du
=

BTA(p−P)
|h(v)|2dv.
(A.77)
16
12:04:44, subject to the Cambridge Core

418
Elements of Functional Analysis
Assuming the boundary of P has measure zero,1 we can assume that p is an interior
point of P, so that the set p −P contains a ball of non-zero measure centered at the
origin. It then follows from Parseval’s theorem that the integral (A.77) converges to
(2π)−Nm(Q) as (τ,ρ) →∞, and the proof is complete.
□
Claim A.3 hB(u) = |B|h(BTu).
Proof of Claim A.3
We have
FhB(y) = 1BQ(y) = 1Q(B−1y),
(A.78)
and the proof of Claim A.3 follows by computing the inverse Fourier transform:
hB(u) = F−11BQ(u)
= (2π)−N

RN 1BQ(y)exp(ju · y)dy
= (2π)−N

RN 1Q(B−1y)exp(ju · (BB−1y))dy
= (2π)−N|B|

RN 1Q(z)exp(ju · (Bz))dz
= (2π)−N|B|

RN 1Q(z)exp(jz · (BTu))dz
= |B|h(BTu).
□
Proof of (A.60)
We are now ready to prove that Claim A.1 and Claim A.2 imply the ﬁnal result. We ﬁrst
show that the number of “intermediate eigenvalues” that are not close to one nor to zero
is o(|A||B|). Let
S =

k
λk(1 −λk) =

k
λk −

k
λ2
k.
(A.79)
By Claim A.1 and Claim A.2, we have S = o(|A||B|). For every 0 < ϵ1 < ϵ2 < 1, we have
that the eigenvalues in the range λk ∈[ϵ1,ϵ2] contribute to S by an amount no smaller
than
min{ϵ1(1 −ϵ1),ϵ2(1 −ϵ2)} [Nϵ1(A,B) −Nϵ2(A,B)] ≤S = o(|A||B|).
(A.80)
It follows that
lim
(τ,ρ)→∞
Nϵ1(A,B) −Nϵ2(A,B)
|A||B|
= 0.
(A.81)
Next, we show that
limsup
(τ,ρ)→∞
Nϵ2(A,B)
|A||B|
≤1
ϵ2
(2π)−Nm(p)m(Q),
(A.82)
1 If the boundary has positive measure, one can obtain the same result using an approximation argument.
16
12:04:44, subject to the Cambridge Core

A.7 Kac–Murdock–Szegö Theorem
419
and
liminf
(τ,ρ)→∞
Nϵ1(A,B)
|A||B|
≥(1 −ϵ1)(2π)−Nm(p)m(Q).
(A.83)
Combining (A.81), (A.82), and (A.83), the result follows.
We have

k
λk ≥ϵ2Nϵ2(A,B),
(A.84)
from which it follows from Claim A.1 that
Nϵ2(A,B) ≤1
ϵ2
|A||B|(2π)−Nm(P)m(Q).
(A.85)
Dividing by (|A||B|) and taking the limit for (τ,ρ) →∞, (A.82) follows.
We have

k
λ2
k =
Nϵ1−1

k=0
λ2
k +
∞

k=Nϵ1
λ2
k
≤Nϵ1(A,B) + ϵ1
∞

k=Nϵ1
λk
≤Nϵ1(A,B) + ϵ1
∞

k=0
λk.
(A.86)
From this, it follows from Claim A.2 that
Nϵ1(A,B) ≥

k
λ2
k −ϵ1

k
λk
= |A∥B|(2π)−Nm(P)m(Q) + o(|A∥B|) −ϵ1|A∥B|(2π)−Nm(P)m(Q)
= (1 −ϵ1)|A∥B|(2π)−Nm(P)m(Q) + o((|A∥B|).
(A.87)
Dividing by (|A∥B|) and taking the limit for (τ,ρ) →∞, (A.83) follows.
□
A.7
Kac–Murdock–Szegö Theorem
Let f(t) be an even, continuous almost everywhere, uniformly bounded, and
square-integrable function, and let its Fourier transform be F(ω). Consider the
eigenvalue equation
 T/2
−T/2
f(t −τ)ψn(τ)dτ = λnψn(t), t ∈[−T/2,T/2].
(A.88)
16
12:04:44, subject to the Cambridge Core

420
Elements of Functional Analysis
If (a,b) does not contain zero and the set where F(ω) = a or F(ω) = b has measure
zero, then the number of eigenvalues that satisfy
a < λn < b
(A.89)
is
NT(a,b) = T
2π

F(ω)∈(a,b)
dω + o(T).
(A.90)
This result is useful in the following context encountered in Chapter 6. If f(t) is the
autocorrelation function of a wide-sense stationary stochastic process, we can use the
theorem to determine the number of terms in its Karhunen–Loève representation that
are sufﬁcient to obtain a satisfactory approximation.
Recall that by (6.71), the error of an N-term Karhunen–Loève representation of the
process is given by the tail sum of the eigenvalues of (A.88). From Claim A.1 of
Section A.6, we have
N−1

n=0
λn +
∞

n=N
λn = T
2π
 ∞
−∞
F(ω) dω.
(A.91)
Letting
N = T
2π

F(ω)∈(a,b)
dω,
(A.92)
by (A.90) we have that, as T →∞,
∞

n=N
λn ≤T
2π
 ∞
−∞
F(ω) dω −a

F(ω)∈(a,b)
dω

+ o(T).
(A.93)
This provides a bound on the error (6.71) in terms of the power spectral density of the
process.
A.7.1
Constant Power Spectral Density
In the case that the power spectral density is F(ω) = 1 for ω ∈[−,] and zero
otherwise, letting a = (1 −ϵ) and b > 1 in (A.93) and (A.92), we obtain
∞

n=N
λn ≤ϵ T
π + o(T),
(A.94)
N = T
π .
(A.95)
Recalling that the total energy of the process over the observation interval is
 T/2
−T/2
1
2π
 ∞
−∞
F(ω)dωdt = T
π ,
(A.96)
16
12:04:44, subject to the Cambridge Core

A.7 Kac–Murdock–Szegö Theorem
421
by dividing (A.94) by (A.96) it follows that the energy error normalized to the total
energy of the process is at most ϵ.
This result should come as no surprise. By (A.90), the number of eigenvalues that are
within ϵ from one is asymptotically equal to N0, and by (A.96) the total energy of the
process is N0. It then follows that retaining N0 terms in the approximation is enough to
capture most of the energy of the process. A tighter result is described next, and is used
in Section 6.4.4 to determine the stochastic diversity.
A.7.2
Evaluation of the Stochastic Diversity
We evaluate the tail sum of the eigenvalues in (A.88) when F(ω) = 1 for ω ∈[−,]
and zero otherwise. This result is useful to determine the stochastic diversity of the class
of processes studied in Section 6.4.4.
From Claim A.1 of Section A.6, we have
N−1

n=0
λn +
∞

n=N
λn = N0.
(A.97)
Letting N = (1 −ϵ)N0, dividing both sides of the equation by N0, taking the limit for
N0 →∞, and using (2.131), we have that
(1 −ϵ) + lim
N0→∞
1
N0
∞

n=N
λn = 1,
(A.98)
from which it follows that
∞

n=N
λn = ϵN0 + o(N0).
(A.99)
This result should be compared with (A.94). In the case of the upper bound (A.94)
we have used the Kac–Murdock–Szegö theorem, which tells us that the number of
eigenvalues close to one is essentially N0. In the case of (A.99), we have used Slepian’s
concentration result, which tells us even more: there are only slightly less than N0
eigenvalues having value one. It then follows that (1 −ϵ)N0 terms can capture most
of the energy of the process, and we obtain a tight expression for the energy of the error
of the N-dimensional Karhunen–Loève approximation of the process.
16
12:04:44, subject to the Cambridge Core

Appendix B: Vector Calculus
B.1
Coordinate Systems
Figure B.1 displays the notation and conventions for the Cartesian coordinate system.
Figure B.2 does the same for cylindrical coordinates, while Figure B.3 illustrates
spherical coordinates.
B.2
Coordinate Transformations
B.2.1
Rectangular–Spherical
x = rsinθ cosφ
y = rsinθ sinφ
z = rcosθ
(B.1)
x
y
z
O
(x,y,z)
x
y
z
Fig. B.1
Cartesian coordinates.
17
12:04:57, subject to the Cambridge Core

B.3 Differential Operators
423
x
y
z
O
r
z
z
Fig. B.2
Cylindrical coordinates.
x
y
z
r
r
O
Fig. B.3
Spherical coordinates.
B.2.2
Rectangular–Cylindrical
x = rcosφ
y = rsinφ
z = z
(B.2)
B.3
Differential Operators
B.3.1
Rectangular Coordinates
∇f = ∂f
∂x ¯x + ∂f
∂y ¯y + ∂f
∂z ¯z
(B.3)
17
12:04:57, subject to the Cambridge Core

424
Vector Calculus
∇· f = ∂fx
∂x + ∂fy
∂y + ∂fz
∂z
(B.4)
∇× f =
∂fz
∂y −∂fy
∂z

¯x +
∂fx
∂z −∂fz
∂x

¯y +
∂fy
∂x −∂fx
∂y

¯z
(B.5)
∇2f = ∇· ∇f = ∂2f
∂x2 + ∂2f
∂y2 + ∂2f
∂z2
(B.6)
∇2f = ∇2fx¯x + ∇2fy¯y + ∇2fz¯z
(B.7)
B.3.2
Cylindrical Coordinates
∇f = ∂f
∂r ¯r + 1
r
∂f
∂φ
¯φ + ∂f
∂z ¯z
(B.8)
∇· f = 1
r
∂(rfr)
∂r
+ 1
r
∂fφ
∂φ + ∂fz
∂z
(B.9)
∇× f =
1
r
∂fz
∂φ −∂fφ
∂z

¯r +
∂fr
∂z −∂fz
∂r

¯φ + 1
r
∂(rfφ)
∂r
−∂fr
∂φ

¯z
(B.10)
∇2f = 1
r
∂
∂r

r∂f
∂r

+ 1
r2
∂2f
∂φ2 + ∂2f
∂z2
(B.11)
∇2f =

∇2fr −2
r2
∂fφ
∂φ −fr
r2

¯r +

∇2fφ + 2
r2
∂fr
∂φ −fφ
r2

¯y + (∇2fz)¯z
(B.12)
B.3.3
Spherical Coordinates
∇f = ∂f
∂r ¯r + 1
r
∂f
∂θ
¯θ +
1
rsinθ
∂f
∂φ
¯φ
(B.13)
∇· f = 1
r2
∂(r2fr)
∂r
+
1
rsinθ
∂fθ sinθ
∂θ
+
1
rsinθ
∂fφ
∂φ
(B.14)
17
12:04:57, subject to the Cambridge Core

B.4 Differential Relationships
425
∇× f =

1
rsinθ
∂(fφ sinθ)
∂θ
−∂fθ
∂φ

¯r + 1
r
 1
sinθ
∂fr
∂φ −(r∂fφ)
∂r

¯θ
+1
r
∂(rfθ)
∂r
−∂fr
∂θ

¯φ
(B.15)
∇2f = 1
r2
∂
∂r

r2 ∂f
∂r

+
1
r2 sinθ
∂
∂θ

sinθ ∂f
∂θ

+
1
r2 sin2 θ
∂2f
∂φ2
(B.16)
∇2f =
&
∇2fr −2
r2

fr + fθ cotθ + csc θ ∂fφ
∂φ + ∂fθ
∂θ
'
¯r
+
&
∇2fθ −1
r2

fθcsc 2θ −2∂fr
∂θ + 2cotθ csc θ ∂fφ
∂φ
'
¯θ
+
&
∇2fφ −1
r2

fφ csc2θ −2csc θ ∂fr
∂φ −2cot θ csc θ ∂fθ
∂φ
'
¯φ
(B.17)
B.4
Differential Relationships
∇· ∇f = ∇2f
(B.18)
∇(fg) = f∇g + g∇f
(B.19)
∇· (fg) = g · ∇f + f∇· g
(B.20)
∇× (fg) = f∇× g −g × ∇f
(B.21)
∇· (f × g) = g · (∇× f) −f · (∇× g)
(B.22)
∇× (f × g) = f(∇· g) −g(∇· f) + (g · ∇)f −(f · ∇)g
(B.23)
∇(f · g) = f × (∇× g) + g × (∇× f) + (g · ∇)f + (f · ∇)g
(B.24)
∇× ∇f = 0
(B.25)
∇· (∇× f) = 0
(B.26)
∇× ∇× f = ∇(∇· f) −∇2f
(B.27)
17
12:04:57, subject to the Cambridge Core

426
Vector Calculus
B.5
Dyadic Analysis
In vector analysis, the scalar product transform A = cB lets the vector A retain the same
direction as B. This is written in matrix form as
⎛
⎝
Ax
Ay
Az
⎞
⎠= c
⎛
⎝
Bx
By
Bz
⎞
⎠.
(B.28)
A more general linear transformation allows each component of B to inﬂuence
each component of A, so that the transformation changes the direction as well as the
magnitude of the original vector. This dyadic transformation is written as
A = C · B,
(B.29)
where the dyad C is deﬁned as
C = cxx¯x¯x + cxy¯x¯y + cxz¯x¯z +
(B.30)
cyx¯y¯x + cyy¯y¯y + cyz¯y¯z +
(B.31)
czx¯z¯x + czy¯z¯y + czz¯z¯z.
(B.32)
The dyadic transformation can be written in convenient matrix form as
⎛
⎝
Ax
Ay
Az
⎞
⎠=
⎛
⎝
cxx
cxy
cxz
cyx
cyy
cyz
czx
czy
czz
⎞
⎠
⎛
⎝
Bx
By
Bz
⎞
⎠.
(B.33)
The matrix C is a second-rank tensor and each element describes the inﬂuence of one
ﬁeld quantity on another. For example, cxy describes the ¯x component of ﬁeld B due to
the ¯y component of ﬁeld A.
The dyad C obeys the usual rules for dot product, but care should be taken in
observing the order of the unit vectors and the dot product. For example, ¯x¯y · B =
¯xBy, but interchanging the order of the unit vectors gives a different result, namely
¯y¯x · B = ¯yBx. Similarly, C · B ̸= B · C. This should not be surprising given the matrix
interpretation.
B.6
Flux Theorems
Volumes V, surfaces S, lines C, and unit vectors ¯n and ¯c are deﬁned in Figure B.4.
All surfaces and curves are sufﬁciently regular that tangent and normal vectors can be
deﬁned unambiguously.
Gauss’ theorem relates the integral of the divergence of a vector ﬁeld A over a volume
V to the outgoing ﬂux of the vector ﬁeld through its bounding surface S, namely

V
∇· AdV = ⃝

S
A · ¯ndS.
(B.34)
17
12:04:57, subject to the Cambridge Core

B.6 Flux Theorems
427
n
S
V
S
C
n
c
Fig. B.4
Volume, surfaces, and lines of integration.
Stokes’ theorem relates the ﬂux of the curl of a vector ﬁeld A through an open surface
S to the line integral of the vector ﬁeld over its boundary C, namely
 
S
∇× A · ¯ndS =
@
C
A · ¯cdc.
(B.35)
17
12:04:57, subject to the Cambridge Core

Appendix C: Methods for
Asymptotic Evaluation of Integrals
C.1
Exponential-Type Integrals: Laplace’s Method
Consider the integral
I(W) =
 b
a
f(x)exp[−W(x)]dx,
(C.1)
where (x) is a real-valued function and f(x) and (x) are smooth enough to be
replaced by local Taylor approximations of appropriate degree. The idea of the method
to approximate this integral as W →∞goes back to Laplace (1774). According to
Laplace, the major contribution to the integral arises from the immediate vicinity of
those points of the interval [a,b] at which (x) assumes its smallest value.
Suppose the exponent has a single minimum at a critical point x0 interior to [a,b] so
that ˙(x0) = 0, ¨(x0) > 0. We expand the integrand’s functions as
f(x) ≃f(x0), (x) ≃(x0) + 1
2
¨(x0)(x −x0)2,
(C.2)
obtaining
I(W) ∼
 x0+ϵ
x0−ϵ
f(x0)exp
&
W(x0) + W
2
¨(x0)(x −x0)2
'
dx.
(C.3)
Since ¨(x0) > 0, this integral vanishes as W →∞for all x ̸= x0. We can then extend
the integration limits to the whole interval (−∞,∞), obtaining, as W →∞,
I(W) ∼f(x0)exp[−W(x0)]
 ∞
−∞
exp
&
−W
2
¨(x0)(x −x0)2
'
dx,
(C.4)
which leads to the ﬁnal result:
I(W) ∼f(x0)exp[−W(x0)]

2π
W ¨(x0) as W →∞.
(C.5)
A factor of 1/2 needs to be introduced if the minimum occurs at one of the end points,
x0 = a or x0 = b.
18
12:04:58, subject to the Cambridge Core

C.2 Fourier-Type Integrals: Stationary Phase Method
429
C.2
Fourier-Type Integrals: Stationary Phase Method
Consider the integral
I(W) =
 b
a
f(x)exp[jW(x)]dx,
(C.6)
where the phase φ(x) is a real-valued function and f(x) and (x) are smooth enough to
be replaced by local Taylor approximations of appropriate degree.
The idea is that as W →∞the exponential becomes rapidly oscillating, leading to
cancellations between positive and negative contributions to the integral. It follows that
the integral tends to zero except at points where the phase is stationary, namely ˙(x) = 0.
Suppose the phase has only one stationary point x0, with ˙(x0) = 0, ¨(x0) ̸= 0. We
expand the integrand’s functions as
f(x) ≃f(x0), (x) ≃(x0) + 1
2
¨(x0)(x −x0)2,
(C.7)
obtaining
I(W) ∼
 x0+ϵ
x0−ϵ
f(x0)exp
&
jW(x0) + jW
2
¨(x0)(x −x0)2
'
dx.
(C.8)
This integral vanishes as W →∞for all x ̸= x0. We can then extend the integration
limits to the whole interval (−∞,∞), obtaining, as W →∞,
I(W) ∼f(x0)exp[Wj(x0)]
 ∞
−∞
exp
&
jW
2
¨(x0)(x −x0)2
'
dx,
(C.9)
which leads to the ﬁnal result:
I(W) ∼f(x0)exp[Wj(x0)]

2π
W ¨|(x0)| exp[±π/4] as W →∞.
(C.10)
The presence of the plus or minus depends on whether the stationary point is a
minimum, ¨(x0) > 0, or a maximum, ¨(x0) < 0.
In the case that ¨(x0) = 0, the method is appropriately modiﬁed by taking into
account higher-order terms in the Taylor expansion.
C.3
Complex Integrals: Saddle Point Method
Consider the integral
I(W) =

ρ
f(z)exp[W(z)]dz,
(C.11)
where (z) = u(x,y) + jv(x,y) is a complex-valued function of complex argument z =
(x,y), the integration is along the path ρ in the complex plane, and f(z) and (z) are
analytic at z0.
As in the stationary phase method, the idea is that as W →∞the dominant
contribution to the integral comes from the points of stationary phase along the
integration path. In addition, the path is deformed so that it passes through the critical
18
12:04:58, subject to the Cambridge Core

430
Methods for Asymptotic Evaluation of Integrals
points along the direction for which the real part u(x,y) is maximized while the
imaginary part v(x,y) is stationary. This allows us to capture the largest possible
contribution to the integral at the stationary point.
We require, at the stationary point,
∂u
∂x = ∂u
∂y = ∂v
∂x = ∂v
∂y = 0,
(C.12)
or, equivalently,
∂
∂z = 0.
(C.13)
At such a critical point,  has neither a maximum nor a minimum, but has a saddle
point. This follows from the Cauchy–Riemann conditions that require u and v to satisfy
the Laplace equations
∂2u
∂x2 + ∂u
∂2y2 = 0,
(C.14)
∂2v
∂x2 + ∂2v
∂y2 = 0.
(C.15)
If, for example,
∂2u
∂x2 > 0,
(C.16)
then
∂2u
∂y2 < 0,
(C.17)
and hence the saddle.
Suppose there is only one stationary point z0. Since both u(x,y) and v(x,y) have
saddle points at z0, a trajectory through such a saddle point can exhibit a maximum
or a minimum, depending on the angle from which the saddle point is approached.
To evaluate the integral, we deform the integration path, as we may by Cauchy’s
theorem, so that it goes through the saddle point in such a way that the angle
approaching the critical point makes u(x,y) exhibit the steepest maximum while keeping
v(x,y) stationary. This is possible because the Cauchy–Riemann conditions ensure that
the level surfaces of u(x,y) and v(x,y) are perpendicular to each other – see Figure C.1.
Expanding the integrand’s functions as
f(x) ≃f(z0), (z) ≃(z0) + 1
2
¨(z0)(z −z0)2,
(C.18)
and expressing in polar coordinates
¨(z0) = | ¨(z0)|exp(jθ),
(C.19)
(z −z0) = rexp(jφ),
(C.20)
we have
(z) ≃(z0) + 1
2| ¨(z0)|r2 exp[j(θ + 2φ)].
(C.21)
18
12:04:58, subject to the Cambridge Core

C.3 Complex Integrals: Saddle Point Method
431
z0
Fig. C.1
The dashed lines represent the locus of points of stationary phase, where v(x,y) is constant.
Perpendicular to these are the dotted lines representing the locus of points where u(x,y) is
constant. The directed line is the integration path of stationary phase and steepest ascent and
descent through the critical point z0.
It follows that moving away from z0 by performing a step of length | ¨(z0)|r2/2, the real
part makes the maximum decrease if the direction of this step is such that
exp[j(θ + 2φ)] = −1.
(C.22)
This implies that choosing the angle φ = −θ/2±π/2 makes the real part rise as fast as
possible to a maximum at z0 and subsequently decrease as rapidly as possible. The plus
or minus ambiguity depends on the integration proceeding in the forward or backward
direction through the critical point and thus depends on the order of the extrema in the
integration path. The two directions obviously differ by an angle π.
The integration can now proceed in the usual way by extending the limits of
integration to inﬁnity:
I(W) =

ρ
f(z)exp[W(z)]dz
≈f(z0)exp[W(z0)]exp(jφ)
 ∞
−∞
exp
&
−W | ¨(z0)|
2
r2
'
dr
≈f(z0)exp[W(z0)]exp(jφ)

2π
W| ¨(z0)|.
(C.23)
18
12:04:58, subject to the Cambridge Core

432
Methods for Asymptotic Evaluation of Integrals
In the case that ¨(x0) = 0, the method is appropriately modiﬁed by taking into
account higher derivatives in the Taylor expansion. In addition, if singularities of f(z)
are crossed in the deformation of the path, their residual contribution should also be
taken into account.
18
12:04:58, subject to the Cambridge Core

Appendix D: Stochastic Integration
In order to deﬁne the integral of a stochastic process rigorously, we may consider
integration at every sample point of the process. In this case, the integral is deﬁned
as a linear functional of the process. Convergence issues should then be handled
with care, because one needs to assume that the sample paths of the random process
are well behaved, so that the integral can be deﬁned for every sample function. A
simpler approach is to consider mean square integration, which relies much less on
the properties of the sample paths and is the typical interpretation when only statistical
averages are of interest rather than individual sample functions.
D.1
Mean Square Integration
A sequence of random variables {Xn} converges to a random variable X in the mean
square sense if all the random variables are deﬁned on the same probability space,
E(Xn) < ∞for all n,
(D.1)
and
lim
n→∞E[(Xn −X)2] = 0.
(D.2)
Let G(t) be a stochastic process and x(t) be a deterministic function. We deﬁne the
integral
I =
 b
a
G(t)x(t)dt
(D.3)
as the mean-square limit of the corresponding Riemann sum.
Namely, given a partition of the interval (a,b] of the form (t0,t1],(t1,t2],...,(tn−1,tn],
where n ≥0, a = t0 < t1 ··· < tn = b, and a sampling point from each subinterval,
sk ∈(tk−1,tk], for 1 ≤k ≤n, the equality in (D.3) is deﬁned if for any ϵ > 0 there exists
a δ > 0 such that if
max
k
|tk −tk−1| < δ,
(D.4)
then
E
( n

k=1
G(sk)x(sk)(tk −tk−1) −I
4
< ϵ.
(D.5)
19
12:05:04, subject to the Cambridge Core

Appendix E: Special Functions
We give the deﬁnitions of some special functions. For integral representations and
asymptotic expansions, the reader should consult Olver et al. (2010).
E.1
Gamma Function
The Gamma function is an extension of the factorial function to complex numbers:
$(n) = (n −1)!,
(E.1)
$(z) =
 ∞
0
exp(−t)tz−1dt.
(E.2)
E.2
Airy Function
The Airy function is the solution to the second-order linear differential equation
∂2y
∂z2 = zy,
(E.3)
y(z) = Ai (z).
(E.4)
In integral form,
1
2πj
 j∞
−j∞
exp(−t3/3 + zt)dt.
(E.5)
For properties and asymptotic expansions, see Vallée and Soares (2010).
E.3
Bessel and Hankel Functions
Bessel functions of order n are solutions to the second-order linear differential equation
z2 ∂2Bn
∂z2 + (z2 + n2)Bn = 0.
(E.6)
20
12:23:40, subject to the Cambridge Core

E.5 Addition Theorems
435
This exhibits two independent solutions: the Bessel functions of the ﬁrst kind Jn(z) and
of the second kind Yn(z). Their linear combination gives the Hankel functions of the
ﬁrst kind,
H(1)
n (z) = Jn(z) + jYn(z),
(E.7)
and the Hankel functions of the second kind,
H(2)
n (z) = Jn(z) −jYn(z).
(E.8)
The modiﬁed Bessel functions are obtained with the substitution z →jz, leading to the
modiﬁed Bessel equation
z2 ∂2Bn
∂z2 −(z2 + n2)Bn = 0,
(E.9)
whose solutions are the modiﬁed Bessel function of the ﬁrst kind,
In(z) = exp

−jnπ
2

Jn(jz),
(E.10)
and of the second kind,
Kn(z) = πj
2 exp

jnπ
2

H(1)
n (jz).
(E.11)
The spherical Bessel functions of the ﬁrst, second, and third kinds are obtained when
n →n + 1/2, yielding
jn(z) =
 π
2xJn+1/2(z) = xn
&
−1
z
∂
∂z
'n sinz
z ,
(E.12)
yn(z) =
 π
2zYn+1/2(z) = xn
&
−1
z
∂
∂xz
'n cosz
z
,
(E.13)
h(1,2)
n
(z) =
 π
2zH(1,2)
n+1/2(z).
(E.14)
E.4
Basic Connection Formulas
J−n(z) = (−1)nJn(z)
(E.15)
Y−n(z) = (−1)nYn(z)
(E.16)
H(1)
−n(z) = (−1)nH(1)
n (z)
(E.17)
H(2)
−n(z) = (−1)nH(2)
n (z)
(E.18)
E.5
Addition Theorems
For more general forms, see Olver et al. (2010).
20
12:23:40, subject to the Cambridge Core

436
Special Functions
r
r
R
r
0
O´
0
R
x´
y´
x
y
O
P
Fig. E.1
Geometry of the addition theorem.
E.5.1
Cylinder Functions
Cn(z ± y) =
∞

m=−∞
Cn∓m(z)Jm(y) for |y| < |z|,
(E.19)
where Cn(z) denotes Jn(z), Yn(z), H(1)
n (z), H(2)
n (z), or any linear combination of these
functions, the coefﬁcients in which are independent of z and n. The restriction |y| < |z|
is unnecessary when C = J, and we have
Jn(z + y) =
∞

m=−∞
Jn−m(z)Jm(y).
(E.20)
E.5.2
Modified Bessel Functions
Zn(z ± y) =
∞

m=−∞
(±1)mZn+m(z)Im(y) for |y| < |z|,
(E.21)
where Zn(z) denotes In(z), exp[jnπ]Kn(z), or any linear combination of these functions,
the coefﬁcients in which are independent of z and n. The restriction |y| < |z| is
unnecessary when Z = I.
E.5.3
Hankel Functions
Hn(kR)exp[jn(θR −θ0)] =
∞

m=−∞
Jm(kr0)Hn+m(kr)exp[j(n + m)(θr −θ0)];
(E.22)
see Figure E.1.
20
12:23:40, subject to the Cambridge Core

Appendix F: Electromagnetic
Spectrum
3 x 10   m
7
3 x 10  m
6
30,000 m
THE RADIO SPECTRUM
VERY LOW FREQUENCY (VLF)
M
F
M
A
Microwaves
LF
MF
HF
VHF
UHF
SHF
EHF
-1
3 x 10   m
3 x 10  m
5
3,000 m
300 m
30 m
3 m
-2
3 x 10  m
-3
3 x 10  m
0
10 Hz
100 Hz
1  KHz
3  KHz
10  KHz
100  KHz
1 MHz
10 MHz
100 MHz
1 GHz
10 GHz
100 GHz
300 GHz
WAVELENGTH
FREQUENCY
13
INFRARED
VISIBLE  ULTRAVIOLET
X-RAY
GAMMA-RAY
-4
3 x 10  m
-5
3 x 10  m
-8
3 x 10  m
-6
3 x 10  m
-7
3 x 10  m
3 x 10  m
-13
3 x 10  m
-12
3 x 10  m
-11
3 x 10  m
-9
3 x 10  m
-10
300 GHz
1 THz
1x10   Hz
14
1x10   Hz
15
1x10   Hz
16
1x10   Hz
17
1x10   Hz
18
1x10   Hz
19
1x10   Hz
20
1x10   Hz
21
1x10   Hz
...
WAVELENGTH
FREQUENCY
Fig. F.1
The electromagnetic spectral bands.
21
12:06:58, subject to the Cambridge Core

Bibliography
V. R. Algazi, D. J. Sakrison (1969). On the optimality of the Karhunen–Loève expansion. IEEE
Transactions on Information Theory, 15(2), pp. 319–21.
B. C. Barber (1993). The non-isotropic two-dimensional random walk. Waves in Random Media,
3, pp. 243–56.
W. Beckner (1975). Inequalities in Fourier analysis. Annals of Mathematics, 102(6), pp. 159–82.
J. D. Bekenstein (1973). Black holes and entropy. Physical Review D, 7(8), pp. 2333–46.
J. D. Bekenstein (1981a). Universal upper bound on the entropy-to-energy ratio for bounded
systems. Physical Review D, 23(2), pp. 287–98.
J. D. Bekenstein (1981b). Energy cost of information transfer. Physical Review Letters, 46(10),
pp. 623–6.
J. D. Bekenstein (2005). How does the entropy/information bound work? Foundations of Physics,
35, pp. 1805–23.
J. D. Bekenstein, M. Schiffer (1990). Quantum limitations on the storage and transmission of
information. International Journal of Modern Physics C, 1(4), pp. 355–422.
P. Bello (1963). Characterization of randomly time-variant linear channels. IEEE Transactions on
Communications, 11(4), pp. 360–93.
E. Biglieri (2005). Coding for Wireless Channels. Springer.
L. Boltzmann (1872). Weitere Studien über das Wärmegleichgewicht unter Gasmolekülen.
Wiener Berichte, 66, pp. 275–370. English translation: S. G. Brush (tr.) (2003). The Kinetic
Theory of Gases. Imperial College Press.
L. Boltzmann (1896–8). Vorlesungen über Gastheorie. J. A. Barth. English translation: S. G.
Brush (tr.) (1964). Lectures on Gas Theory. University of California Press.
E. Borel (1897). Sur l’interpolation. Comptes rendus de l’Académie des sciences de Paris, 124,
pp. 673–6.
R. Bousso (2002). The holographic principle. Reviews of Modern Physics, 74(3), pp. 825–74.
J. Bowen (1967). On the capacity of a noiseless photon channel. IEEE Transactions on
Information Theory, 13(2), pp. 230–6.
H. J. Bremermann (1967). Quantum noise and information. In L. M. Le Cam and J. Neyman
Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability.
University of California Press.
H. J. Bremermann (1982). Minimum Energy Requirements of Information Transfer and
Computing. International Journal of Theoretical Physics, 21(3–4), pp. 203–217.
J. L. Brown Jr (1960). Mean square truncation error in series expansions of random functions.
Journal of the Society of Industrial and Applied Mathematics, 8(1), pp. 28–32.
O. M. Bucci, G. Di Massa (1988). The truncation error in the application of sampling series to
electromagnetic problems. IEEE Transactions on Antennas and Propagation, 36(7), pp. 941–9.
22
12:07:01, subject to the Cambridge Core

Bibliography
439
O. M. Bucci, G. Franceschetti (1987). On the spatial bandwidth of scattered ﬁelds. IEEE
Transactions on Antennas and Propagation, 35(12), pp. 1445–55.
O. M. Bucci, G. Franceschetti (1989). On the degrees of freedom of scattered ﬁelds. IEEE
Transactions on Antennas and Propagation, 37(7), pp. 918–26.
O. M. Bucci, C. Gennarelli, C. Savarese (1998). Representation of electromagnetic ﬁelds over
arbitrary surfaces by a ﬁnite and nonredundant number of samples. IEEE Transactions on
Antennas and Propagation, 46(3), pp. 351–9.
W. Byers (2007). How Mathematicians Think: Using Ambiguity, Contradiction, and Paradox to
Create Mathematics. Princeton University Press.
V. R. Cadambe, S. A. Jafar (2008). Interference alignment and degrees of freedom of the K-user
interference channel. IEEE Transactions on Information Theory, 54(8), pp. 3425–41.
E. Candés (2006). Compressive sampling. In M. Sanz-Solé, J. Soria, J. L. Varona, J. Verdera
(eds.) Proceedings of the International Congress of Mathematicians, Madrid, Spain.
E. Candés (2008). The restricted isometry property and its implications for compressed sensing.
Comptes rendus de l’Académie des sciences. Série I. Mathématique, 346, pp. 589–92.
E. Candés, J. Romberg, T. Tao (2006). Robust uncertainty principles: Exact signal reconstruction
from highly incomplete frequency information. IEEE Transactions on Information Theory,
52(2), pp. 489–509.
E. Candés, M. Wakin (2008). An introduction to compressive sampling. IEEE Signal Processing
Magazine, 25(2), pp. 21–30.
C. M. Caves, P. D. Drummond (1994). Quantum limits on bosonic communication rates. Reviews
of Modern Physics, 66(2), pp. 481–537.
S. Chandrasekhar (1960). Radiative Transfer. Dover.
G. M. Church, Y. Gao, S. Kosuri (2012). Next-generation digital information storage in DNA.
Science, 337, p. 1628.
R. Clausius (1850–65). The Mechanical Theory of Heat – with its Applications to the Steam
Engine and to Physical Properties of Bodies. John van Voorst.
J. B. Conway (1990). A Course in Functional Analysis, 2nd edn. Springer.
T. M. Cover (1994). Which processes satisfy the second law? In J. J. Halliwell, J. Perez-Mercader,
W. H. Zurek (eds.), Physical Origins of Time Asymmetry. Cambridge University Press, pp.
98–107.
T. M. Cover, J. Thomas (2006). Elements of Information Theory, 2nd edn. John Wiley & Sons.
M. A. Davenport, M. B. Wakin (2012). Compressive sensing of analog signals using discrete
prolate spheroidal sequences. Applied Computational Harmonic Analysis, 33, pp. 438–72.
A. De Gregorio (2012). On random ﬂights with non-uniformly distributed directions. Journal of
Statistical Physics, 147(2), pp. 382–411.
P. Dirac (1931). Quantised singularities in the electromagnetic ﬁeld. Proceedings of the Royal
Society of London A, 133, pp. 60–72.
D. L. Donoho (2000). Wald Lecture I: Counting Bits with Shannon and Kolmogorov. Technical
report, Stanford University.
D. L. Donoho (2006). Compressed sensing. IEEE Transactions on Information Theory, 52(4), pp.
1289–1306.
D. L. Donoho, A. Javanmard, A. Montanari (2013). Information-theoretically optimal compressed
sensing via spatial coupling and approximate message passing. IEEE Transactions on
Information Theory, 59(11), pp. 7434–64.
D. L. Donoho, P. B. Stark (1989). Uncertainty principles and signal recovery. SIAM Journal of
Applied Mathematics, 49, pp. 906–31.
22
12:07:01, subject to the Cambridge Core

440
Bibliography
O. El Ayach, S. W. Peters, R. W. Heath Jr (2013). The practical challenges of interference
alignment. IEEE Wireless Communications, 20(1), pp. 35–42.
A. E. Gamal, Y. Kim (2011). Network Information Theory. Cambridge University Press.
K. Falconer (1990). Fractal Geometry: Mathematical Foundations and Applications. John Wiley
& Sons.
P. Feng, Y. Bresler (1996a). Spectrum-blind minimum-rate sampling and reconstruction of
multi-band signals. Proceedings of the IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), 3, pp. 1688–91.
P. Feng, Y. Bresler (1996b). Spectrum-blind minimum-rate sampling and reconstruction of 2D
multi-band signals. Proceedings of the IEEE International Conference on Image Processing,
1, pp. 701–4.
R. Feynman, R. Leighton, M. Sands (1964). The Feynman Lectures on Physics, vols. 1–3.
Reprinted 2005. Addison Wesley.
C. Flammer (1957). Spheroidal Wave Functions. Stanford University Press.
G. B. Folland, A. Sitaram (1997). The uncertainty principle: A mathematical survey. Journal of
Fourier Analysis and Applications, 3(3), pp. 207–38.
S. Foucart, H. Rauhut (2013). A Mathematical Introduction to Compressive Sensing. Springer.
G. Franceschetti (1997). Electromagnetics: Theory, Techniques, and Engineering Paradigms.
Plenum Press.
M. Franceschetti (2004). Stochastic rays pulse propagation. IEEE Transactions on Antennas and
Propagation, 52(10), pp. 2742–52.
M. Franceschetti (2007a). A note on Levéque and Telatar’s upper bound on the capac-
ity of wireless ad hoc networks. IEEE Transactions on Information Theory, 53(9),
pp. 3207–11.
M. Franceschetti (2007b). When a random walk of ﬁxed length can lead uniformly anywhere
inside a hypersphere. Journal of Statistical Physics, 127, pp. 813–23.
M. Franceschetti (2015). On Landau’s eigenvalue theorem and information cut-sets. IEEE
Transactions on Information Theory, 61(9), pp. 5042–51.
M. Franceschetti (2017). Quantum limits on the entropy of bandlimited radiation. Journal of
Statistical Physics, 169(2), pp. 374–94.
M. Franceschetti, J. Bruck, L. J. Schulman (2004). A random walk model of wave propagation.
IEEE Transactions on Antennas and Propagation, 52(5), pp. 1304–17.
M. Franceschetti, O. Dousse, D. Tse, P. Thiran (2007). Closing the gap in the capacity of
wireless networks via percolation theory. IEEE Transactions on Information Theory, 53(3),
pp. 1009–18.
M. Franceschetti, R. Meester (2007). Random Networks for Communication. Cambridge
University Press.
M. Franceschetti, M. D. Migliore, P. Minero (2009). The capacity of wireless networks:
Information-theoretic and physical limits. IEEE Transactions on Information Theory, 55(8),
pp. 3413–24.
M. Franceschetti, M. D. Migliore, P. Minero, F. Schettino (2011). The degrees of freedom of
wireless networks via cut-set integrals. IEEE Transactions on Information Theory, 57(11), pp.
3067–79.
M. Franceschetti, M. D. Migliore, P. Minero, F. Schettino (2015). The information carried by
scattered waves: Near-ﬁeld and non-asymptotic regimes. IEEE Transactions on Antennas and
Propagation, 63(7), pp. 3144–57.
D. Gabor (1946). Theory of communication. Journal of the Institution of Electrical Engineers,
Part III: Radio and Communication Engineering, 93, pp. 429–57.
22
12:07:01, subject to the Cambridge Core

Bibliography
441
D. Gabor (1953). Communication theory and physics. IRE Professional Group on Information
Theory, 1(1), pp. 48–59.
D. Gabor (1961). Light and information. In E. Wolf (ed.), Progress in Optics. Elsevier, vol. I, pp.
109–53.
R. G. Gallager (1968). Information Theory and Reliable Communication. John Wiley & Sons.
R. G. Gallager (2008). Principles of Digital Communication. Cambridge University Press.
A. G. Garcia (2000). Orthogonal sampling formulas: A uniﬁed approach. SIAM Review, 42(3),
pp. 499–512.
J. Ghaderi, L.-L. Xie, X. Shen (2009). Hierarchical cooperation in ad hoc networks: Optimal
clustering and achievable throughput. IEEE Transactions on Information Theory, 55(8), pp.
3425–36.
W. Gibbs (1902). Elementary Principles of Statistical Mechanics Developed with Especial
Reference to the Rational Foundation of Thermodynamics. Reprinted 1960, Dover.
A. Goldsmith (2005). Wireless Communications. Cambridge University Press.
J. P. Gordon (1962). Quantum effects in communication systems. Proceedings of the IRE, 50(9),
pp. 1898–908.
C. C. Grosjean (1953). Solution of the non-isotropic random ﬂight problem in the k-dimensional
space. Physica, 19, pp. 29–45.
P. Gupta, P. R. Kumar (2000). The capacity of wireless networks. IEEE Transactions on
Information Theory, 42(2), pp. 388–404.
M. Haenggi (2013). Stochastic Geometry for Wireless Networks. Cambridge University Press.
M. Haenggi, J. Andrews, F. Baccelli, O. Dousse, M. Franceschetti (2009). Stochastic geometry
and random graphs for the analysis and design of wireless networks. IEEE Journal on Selected
Areas in Communications, 27(7), pp. 1029–46.
S. W. Hawking (1975). Particle creation by black holes. Communications in Mathematical
Physics, 43, pp. 199–220.
W. Heisenberg (1927). Über den anschaulichen Inhalt der quantentheoretischen Kinematik und
Mechanik. Zeitschrift für Physik, 43, pp. 172–98. English translation: In J. A. Wheeler, W.
H. Zurek (eds.) (1983). Quantum Theory and Measurement. Princeton University Press, pp.
62–84.
W. Heitler (1954). The Quantum Theory of Radiation, 3rd edn. Reprinted 2000. Dover.
J. R. Higgins (1985). Five short stories about the cardinal series. Bulletin of the American
Mathematical Society, 12(1), pp. 46–89.
D. Hilbert, R. Courant (1953). Methods of Mathematical Physics. Vols. 1, & 2, 2nd edn. Springer.
I. I. Hirschman, Jr (1957). A note on entropy. American Journal of Mathematics, 79, pp. 152–6.
J. A. Hogan, J. D. Lakey (2012). Duration and Bandwidth Limiting: Prolate Functions, Sampling,
and Applications. Birkhäuser.
A. Holevo (1973). Bounds for the quantity of information transmitted by a quantum communica-
tion channel. Problems of Information Transmission, 9, pp. 177–83.
S. N. Hong, G. Caire (2015). Beyond scaling laws: On the rate performance of dense
device-to-device wireless networks. IEEE Transactions on Information Theory, 61(9), pp.
4735–50.
B. D. Hughes (1995). Random Walks and Random Environments. Volume I: Random Walks.
Oxford University Press.
A. Ishimaru (1978). Wave Propagation and Scattering in Random Media. IEEE Press.
S. Izu, J. Lakey (2009). Time–frequency localization and sampling of multiband signals. Acta
Applicandae Mathematicae, 107(1), pp. 399–435.
22
12:07:01, subject to the Cambridge Core

442
Bibliography
J. D. Jackson (1962). Classical Electrodynamics. John Wiley & Sons.
S. A. Jafar (2011). Interference alignment: A new look at signal dimensions in a communication
network. Foundations and Trends in Communications and Information Theory, 7(1), pp. 1–134.
D. Jagerman (1969) ϵ-entropy and approximation of bandlimited functions. SIAM Journal on
Applied Mathematics, 17(2), pp. 362–77.
D. Jagerman (1970). Information theory and approximation of bandlimited functions. Bell
Systems Technical Journal, 49(8), pp. 1911–41.
R. Janaswamy (2011). On the EM degrees of freedom in scattering environments. IEEE
Transanctions on Antennas and Propagation, 59(10), pp. 3872–81.
E. T. Jaynes (1965). Gibbs vs. Boltzmann entropies. Americal Journal of Physics, 33(5), pp.
391–8.
E. T. Jaynes (1982). On the rationale of maximum entropy methods. Proceedings of the IEEE, 70,
pp. 939–52.
A. Jerri (1977). The Shannon sampling theorem – its various extensions and applications: A
tutorial review. Proceedings of the IEEE, 65(11), pp. 1565–96.
M. Kac, W. L. Murdock, G. Szegö. (1953). On the eigenvalues of certain Hermitian forms.
Journal of Rational Mechanics and Analysis, 2, pp. 767–800.
T. Kawabata, A. Dembo (1994). The rate–distortion dimension of sets and measures. IEEE
Transactions on Information Theory, 40(5), pp. 1564–72.
E. H. Kennard (1927). Zur Quantenmechanik einfacher Bewegungstypen. Zeitschrift für Physik,
44(4–5), pp. 326–52.
R. A. Kennedy, P. Sadeghi, T. D. Abhayapala, H. M. Jones (2007). Intrinsic limits of
dimensionality and richness in random multipath ﬁelds. IEEE Transactions on Signal
Processing, 55(6), pp. 2542–56.
C. Kittel, H. Kroemer (1980). Thermal Physics, 2nd edn. W. H. Freeman & Co.
J. J. Knab (1979). Interpolation of bandlimited functions using the approximate prolate series.
IEEE Transactions on Information Theory, 25(6), pp. 717–19.
J. J. Knab (1983). The sampling window. IEEE Transactions on Information Theory, 29(1), pp.
157–9.
A. N. Kolmogorov (1936). Über die beste Annäherung von Funktionen einer gegebenen
Funktionenklasse. Annals of Mathematics, 37(1), no. 1, pp. 107–10 (in German).
A. N. Kolmogorov (1956). On certain asymptotic characteristics of completely bounded metric
spaces. Uspekhi Matematicheskikh Nauk, 108(3), pp. 385–8 (in Russian).
A. N. Kolmogorov, S. V. Formin (1954). Elements of the Theory of Functions and Functional
Analysis, vols. 1, 2. Graylock.
A. N. Kolmogorov, V. M. Tikhomirov (1959). ϵ-entropy and ϵ-capacity of sets in functional
spaces. Uspekhi Matematicheskikh Nauk, 14(2), pp. 3–86. English translation: (1961).
American Mathematical Society Translation Series, 2(17), pp. 277–364.
V. A. Kotelnikov (1933). On the transmission capacity of “ether” and wire in electrocommunica-
tions. Proceedings of the First All-Union Conference on Questions of Communication, January
1933. English translation reprint in J. J. Benedetto, P. J. S. G. Ferreira (eds.) (2000), Modern
Sampling Theory: Mathematics and Applications, Birkhauser.
M. Lachmann, M. E. Newman, C. Moore (2004). The physical limits of communication or why
any sufﬁciently advanced technology is indistinguishable from noise. American Journal of
Physics, 72(10), pp. 1290–3.
H. J. Landau (1975). On Szegö’s eigenvalue distribution theorem and non-Hermitian kernels.
Journal d’Analyse Mathematique, 28, pp. 335–57.
22
12:07:01, subject to the Cambridge Core

Bibliography
443
H. J. Landau (1985). An overview of time and frequency limiting. In J. F. Prince (ed.), Fourier
Techniques and Applications. Plenum Press, pp. 201–20.
M. D. Landau, W. Jones (1983). A Hardy old problem. Mathematics Magazine, 56(4),
pp. 230–2.
H. J. Landau, H. O. Pollak (1961). Prolate spheroidal wave functions, Fourier analysis and
uncertainty, II. Bell Systems Technical Journal, 40, pp. 65–84.
H. J. Landau, H. O. Pollak (1962). Prolate spheroidal wave functions, Fourier analysis and
uncertainty, III. Bell Systems Technical Journal, 41, pp. 1295–336.
H. J. Landau, H. Widom (1980). Eigenvalue distribution of time and frequency limiting. Journal
of Mathematical Analysis and Applications, 77(2), pp. 469–81.
A. Lapidoth (2009). A Foundation in Digital Communication. Cambridge University Press.
P. S. Laplace (1774). Mémoires de Mathématique et de Physique, Tome Sixiéme. English
translation: S. M. Stigler (tr.) (1986). Memoir on the probability of causes of events. Statistical
Science, 1(19), pp. 364–78.
D. S. Lebedev, L. B. Levitin (1966). Information transmission by electromagnetic ﬁeld.
Information and Control, 9, pp. 1–22.
G. Le Caër (2010). A Pearson–Dirichlet random walk. Journal of Statistical Physics, 140, pp.
728–51.
G. Le Caër (2011). A new family of solvable Pearson–Dirichlet random walks. Journal of
Statistical Physics, 144, pp. 23–45.
E. A. Lee (2017). Plato and the Nerd. The Creative Partnership of Humans and Technology. MIT
Press.
S. H. Lee and S. Y. Chung (2012). Capacity scaling of wireless ad hoc networks: Shannon meets
Maxwell. IEEE Transactions on Information Theory, 58(3), pp. 1702–15.
O. Lévêque, E. Telatar (2005). Information theoretic upper bounds on the capacity of large
extended ad hoc wireless networks. IEEE Transactions on Information Theory, 51(3), pp.
858–65.
C. T. Li, A. Özgür (2016) Channel diversity needed for vector space interference alignment. IEEE
Transactions on Information Theory, 62(4), pp. 1942–56.
T. J. Lim, M. Franceschetti (2017a). Deterministic coding theorems for blind sensing: Optimal
measurement rate and fractal dimension. arXiv: 1708.05769.
T. J. Lim, M. Franceschetti (2017b). Information without rolling dice. IEEE Transactions on
Information Theory, 63(3), pp. 1349–63.
G. Lorentz (1986). Approximation of Functions, 2nd edn. AMS Chelsea Publishing.
S. Loth, S. Baumann, C. P. Lutz, D. M. Eigler, A. J. Heinrich (2012). Bistability in atomic-scale
antiferromagnets. Science, 335, pp. 196–9.
R. Loudon (2000). The Quantum Theory of Light, 3rd edn. Oxford University Press.
M. Masoliver, J. M. Porrá, G. H. Weiss (1993). Some two- and three-dimensional persistent
random walks. Physica A, 193, pp. 469–82.
J. K. Maxwell (1873). A treatise on electricity and magnetism. Reprinted 1998, Oxford University
Press.
N. Merhav (2010). Statistical physics and information theory. Foundations and Trends in
Communications and Information Theory, 6(1–2), pp. 1–212.
M. Mézard, A. Montanari (2009). Information, Physics, and Computation. Oxford University
Press.
D. A. B. Miller (2000). Communicating with waves between volumes: Evaluating orthogonal
spatial channels and limits on coupling strengths. Applied Optics, 39(11), pp. 1681–99.
22
12:07:01, subject to the Cambridge Core

444
Bibliography
M. Mishali, Y. Eldar (2009). Blind multi-band signal reconstruction: Compressed sensing for
analog signals. IEEE Transactions on Signal Processing, 57(3), pp. 993–1009.
C. R. Moon, L. S. Mattos, B. K. Foster, G. Zeltzer, H. C. Manoharan (2009). Quantum holographic
encoding in a two-dimensional electron gas. Nature Nanotechnology, 4, pp. 167–72.
B. Nazer, M. Gastpar, S. A. Jafar, S. Vishwanath (2012). Ergodic interference alignment. IEEE
Transactions on Information Theory, 58(10), pp. 6355–71.
H. Nyquist (1928). Thermal agitations of electric charges in conductors. Physical Review, 32, pp.
110–13.
B. M. Oliver (1965). Thermal and quantum noise. Proceedings of the IEEE, 53(5),
pp. 436–54.
F. W. J. Olver, D. W. Lozier, R. F. Boisvert, C. W. Clark (eds.) (2010). National Institute of
Standards Handbook of Mathematical Functions. Cambridge University Press.
A. Özgür, O. Lévêque, D. N. C. Tse (2007). Hierarchical cooperation achieves optimal capacity
scaling in ad hoc networks. IEEE Transactions on Information Theory, 53(10), pp. 3549–72.
A. Özgür, O. Lévêque, D. N. C. Tse (2013). Spatial degrees of freedom of large distributed MIMO
systems and wireless ad hoc networks. IEEE Journal on Selected Areas in Communications,
31(2), pp. 202–14.
C. H. Papas (1965). Theory of Electromagnetic Wave Propagation. Dover.
G. C. Papen, R. E. Blahut (2018). Lightwave Communication Systems. Preprint, to be published
by Cambridge University Press.
J. B. Pendry (1983). Quantum limits to the ﬂow of information and entropy. Journal of Physics
A: Mathematical and General, 16, pp. 2161–71.
R. Piestun, D. A. B. Miller (2000). Electromagnetic degrees of freedom of an optical system.
Journal of the Optical Society America, 17(5), pp. 892–902.
A. Pinkus (1985). n-Widths in Approximation Theory. Springer.
A. A. Pogorui, R. M. Rodriguez-Dagnino (2011). Isotropic random motion at ﬁnite speed with
k-Erlang distributed direction alternations. Journal of Statistical Physics, 145, pp. 102–12.
A. S. Y. Poon, R. W. Brodersen, D. N. C. Tse (2005). Degrees of freedom in multiple-antenna
channels: A signal space approach. IEEE Transactions on Information Theory, 51(2), pp.
523–36.
J. Proakis, M. Salehi (2007). Digital Communications. McGraw-Hill.
M. Reed, B. Simon (1980). Functional Analysis. Elsevier.
A. Rényi (1959). On the dimension and entropy of probability distributions. Acta Mathematica
Hungarica, 10(1–2), pp. 193–215.
A. Rényi (1985). A Diary on Information Theory. John Wiley & Sons.
F. Riesz, B. Sz.-Nagy (1955). Functional Analysis. Ungar.
M. Schiffer (1991). Quantum limit for information transmission. Physical Review A, 43(10), pp.
5337–43.
E. Schmidt (1907). Zur Theorie der linearen und nichtlinearen Integralgleichungen. Mathematis-
che Annalen, 63, pp. 433–76.
C. E. Shannon (1948). A mathematical theory of communication. Bell System Technical Journal,
27, pp. 379–423, 623–56.
C. E. Shannon (1949). Communication in the presence of noise. Proceedings of the IRE, 37, pp.
10–21.
D. Slepian (1964). Prolate spheroidal wave functions, Fourier analysis and uncertainty, IV.
Extensions to many dimensions: Generalized prolate spheroidal functions. Bell Systems
Technical Journal, 43, pp. 3009–58.
22
12:07:01, subject to the Cambridge Core

Bibliography
445
D. Slepian (1965). Some asymptotic expansions for prolate spheroidal wave functions. Journal
of Mathematics and Physics, 44, pp. 99–140.
D. Slepian (1976). On bandwidth. Proceedings of the IEEE, 64(3), pp. 292–300.
D. Slepian (1978). Prolate spheroidal wave functions, Fourier analysis and uncertainty, V. The
discrete case. Bell Systems Technical Journal, 57, pp. 1371–430.
D. Slepian (1983). Some comments on Fourier analysis, uncertainty and modeling. SIAM Review,
25(3), pp. 379–93.
D. Slepian, H. O. Pollak (1961). Prolate spheroidal wave functions, Fourier analysis and
uncertainty, I. Bell Systems Technical Journal, 40, pp. 43–64.
W. Stadje (1987). The exact probability distribution of a two-dimensional random walk. Journal
of Statistical Physics, 46, pp. 207–16.
T. E. Stern (1960). Some quantum effects in information channels. IEEE Transactions on
Information Theory, 6, pp. 435–40.
G. W. Stewart (1993). On the early history of the singular value decomposition. SIAM Review,
35(4), pp. 551–66.
J. A. Stratton (1941). Electromagnetic Theory. McGraw-Hill.
A. Strominger, C. Vafa (1996). Microscopic origin of the Bekenterin–Hawking entropy. Physics
Letters B, 379(1), pp. 99–104.
L. Susskind (1995). The world as a hologram. Journal of Mathematical Physics, 36(11), pp.
6377–96.
T. Tao (2012). Topics in Random Matrix Theory. Graduate Studies in Mathematics, vol. 132.
American Mathematical Society.
G. ’t Hooft (1993). Dimensional reduction in quantum gravity. In A. Ali, J. Ellis, S.
Randjbar-Daemi (eds.), Salamfestschrift: A Collection of Talks from the Conference on
Highlights of Particle and Condensed Matter Physics, World Scientiﬁc Series in 20th Century
Physics, vol. 4. World Scientiﬁc.
G. Toraldo di Francia (1955). Resolving power and information. Journal of the Optical Society of
America, 45(7), pp. 497–501.
G. Toraldo di Francia (1969). Degrees of freedom of an image. Journal of the Optical Society of
America, 59(7), pp. 799–804.
D. N. C. Tse, P. Visvanath (2005). Fundamentals of Wireless Communication. Cambridge
University Press.
A. Tulino, S. Verdú (2004). Random matrix theory and wireless communications. Foundations
and Trends in Communications and Information Theory, 1(1) pp. 1–182.
V. Twersky (1957). On multiple scattering and reﬂection of waves by rough surfaces. IRE
Transactions on Antennas and Propagation, 5, p. 81.
V. Twersky (1964). On propagation in random media of discrete scatterers. Proceedings of the
American Mathematical Society Symposium on Stochastic Processes in Mathematics, Physics,
and Engineering, 16, pp. 84–116.
J. Ufﬁnk (2008). Boltzmann’s work in statistical physics. In E. N. Zalta (ed.), The Stanford
Encyclopedia of Philosophy, Winter 2008 edn. Published online.
M. Unser (2000). Sampling – 50 years after Shannon. Proceedings of the IEEE, 88(4),
pp. 569–87.
O. Vallée, M. Soares (2010). Airy Functions and Applications to Physics. World Scientiﬁc.
J. Van Bladel (1985). Electromagnetic Fields. Hemisphere.
R. Venkataramani, Y. Bresler (1998). Further results on spectrum blind sampling of 2D signals.
Proceedings of the IEEE International Conference on Image Processing, 2, pp. 752–6.
22
12:07:01, subject to the Cambridge Core

446
Bibliography
M. Vetterli, J, Kova˘cevi´c, V. Goyal (2014a). Foundations of Signal Processing. Cambridge
University Press.
M. Vetterli, J, Kova˘cevi´c, V. Goyal (2014b). Fourier and Wavelet Signal Processing. Cambridge
University Press.
A. J. Viterbi (1995). CDMA: Principles of Spread Spectrum Communication. Addison Wesley.
H. Weyl (1928). Gruppentheorie und Quantenmechanik. S. Hirzel.
E. T. Whittaker (1915). On the functions which are represented by the expansions of the
interpolation theory. Proceedings of the Royal Society of Edinburgh, 35, pp. 181–94.
H. Widom (1964). Asymptotic behavior of the eigenvalues of certain integral equations II. Archive
for Rational Mechanics and Analysis, 17(3), pp. 215–29.
Y. Wu, S. Verdú (2010). Rényi information dimension: Fundamental limits of almost lossless
analog compression. IEEE Transactions on Information Theory, 56(8), pp. 3721–48.
Y. Wu, S. Verdú (2012). Optimal phase transitions in compressed sensing. IEEE Transactions on
Information Theory, 58(10), pp. 6241–63.
A. Wyner (1965). Capacity of the band-limited Gaussian channel. Bell Systems Technical Journal,
45, pp. 359–95.
A. Wyner (1973). A bound on the number of distinguishable functions which are time-limited
and approximately band-limited. SIAM Journal of Applied Mathematics, 24(3), pp. 289–97.
L. L. Xie, P. R. Kumar (2004). A network information theory for wireless communication: Scaling
laws and optimal operation. IEEE Transactions on Information Theory, 50(5), pp. 748–67.
H. Yuen, M. Ozawa (1993). Ultimate information carrying limit of quantum systems. Physical
Review Letters, 70(4), pp. 363–6.
22
12:07:01, subject to the Cambridge Core

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Index
More Information
www.cambridge.org
© in this web service Cambridge University Press
Index
achievable rate, 29, 345, 369, 385
addition theorems
for Hankel functions, 247
for special functions, 435
adjoint operator, 96
Airy function, 239, 243, 263
Airy integral, 262
anisotropic media, 133, 134
asymptotic equipartition, 18, 22, 319–321, 323, 337,
339, 404
backscattering, 258
bandlimitation, 5, 49
bandlimitation error, 236
bandlimited signals, 11, 14, 17, 33, 40, 41, 45, 49,
50, 52, 53, 55, 59–61, 63–66, 68, 69, 75, 79,
81, 82, 88–91, 95, 98, 104, 105, 116, 117, 125,
127, 192, 200, 214, 244, 248, 249, 278, 343,
344, 346, 350, 352, 358, 360–362, 368, 369,
378, 379, 381, 382, 392, 395
approximate, 98
of multiple variables, 108, 110, 169
bandlimiting operator, 69, 98, 106, 109
Bessel’s inequality, 77
black body, 307, 311, 316, 318, 324, 326, 329, 330,
333, 337, 391, 396, 402
average energy, 327
entropy, 326, 327, 330
radiation, 307, 330, 341
black hole, 38, 328, 329, 331–333, 401, 405
entropy, 38, 39, 330, 331, 402
blind sensing, 42, 113, 114, 124
Boltzmann’s constant, 303
Born approximation, 281
Bouligand dimension, 117
capacity
AWGN channel, 30, 31, 347, 359, 362
colored Gaussian channel, 357
Kolmogorov, 14, 368, 369, 371–375, 377, 378,
380, 383
Shannon, 26, 28, 344, 345, 347, 350–354,
356–366, 368, 369, 372, 373, 378, 381,
385–387
zero-error, 383
capacity-achieving code, 347
cardinal series, 8, 50, 51, 59–63, 76, 78, 79, 81,
248
carrier frequency, 34–36, 355
carrier wavelength, 35
Cauchy’s theorem, 235, 238
CDMA, 205–209, 212, 214, 216, 217, 219, 226
chips, 212–214
codebook, 15, 16, 28, 344, 345, 365–367, 378–380
codeword, 343, 345, 348
coding theorem, 18, 29, 350, 351
coherence
bandwidth, 180–184, 196, 202, 227
distance, 186, 227
time, 181–184, 202, 212, 214, 227
coherent energy, 275
coherent response, 275, 283, 284, 295, 299
compact operator, 64, 96, 97, 100, 107, 109, 168
compressed sensing, 118, 124
compression rate, 121
compressor, 121
concentration, 2, 107, 124
frequency, 63, 169
geometric view, 68
probabilistic, 3, 31
Slepian’s problem, 41, 63, 64, 68, 82, 87, 90, 95,
97, 100, 106, 109, 192, 193, 260, 368
spatial, 169, 250
spectral, 2, 41, 111, 265, 267
time, 63, 169
conductivity, 133, 137
conductor, 134–137, 146
connection formulas, 247, 435
convolution
space–time, 160
spatial, 160, 161, 163, 166, 167, 169, 176, 233
convolution integral, 159, 162, 163, 166, 168, 171,
174
covering number, 14
critical bandwidth, 240, 243

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Index
More Information
www.cambridge.org
© in this web service Cambridge University Press
448
Index
cross section
absorption, 283, 284
scattering, 283, 284
cut-set, 5, 6, 9, 11, 34, 36, 40, 230, 256, 257, 265,
268, 269, 272–274
cut-set integral, 250, 251, 253–258, 261
linear, 251
surface, 253
cyclic preﬁx, 210
decoder, 121
decoding function, 29
decoding process, 343
decoding system, 4
decompressor, 121
degrees of freedom, 5, 7–13, 41, 87, 90, 91, 95,
104–112, 173, 187, 192, 200–202, 204, 205,
207–209, 212, 213, 217, 219, 222, 224–226,
230, 232, 233, 244–246, 248–257, 259–261,
265–270, 272–274, 327, 331, 358–360, 362,
363, 368, 369, 378, 379, 384–387, 391–393,
401–405
space–wavenumber, 11, 203, 207, 209, 217–222,
224–226, 265, 269
time–frequency, 11, 209, 219, 265, 269
total, 265
wide-band regime, 270, 271
density power, 140, 142, 147, 148, 150
deviation, 88, 104, 105, 234, 235, 244, 245
dielectric, 137
dielectric constant, 137
diffusion, 290
dimensionality reduction, 103
dipole, 147, 148, 151
moment, 147, 148
Dirac’s delta, 121, 123, 304
Dirac’s impulse, 132, 157, 159, 181, 293, 294
dispersive media, 132
distortion, 184, 201
Doppler
frequency, 182
frequency spread, 182, 183
power proﬁle, 182–184
dyad, 160, 426
(ǫ,δ)-capacity, 378, 381
ǫ-capacity, 370, 372–376, 378–380
ǫ-covering, 369
ǫ-entropy, 369, 370, 372, 374–377, 379, 382,
392–395, 399, 402
ǫ-net, 369
effective dimension, 87, 91, 392
Einstein’s equation, 38, 57, 402
electric dipole, 146
electric induction, 132
encoder, 121
encoding function, 28, 29
encoding process, 343
encoding system, 4, 347
energy constraint, 12, 14, 18, 20, 22, 23, 25, 27, 29,
30, 33, 40, 45, 61, 79, 84, 344, 346, 352, 353,
357, 359, 361, 379, 380, 383, 384, 387, 392,
397
entropy
Boltzmann, 20, 316, 321, 396
Clausius, 20, 318
conditional, 25
differential, 21, 23, 24, 30, 45–47, 317, 338, 339,
351, 367, 404
Gibbs, 20, 316
Kolmogorov, 14, 369, 371, 374, 375, 377, 378,
394, 399
relative, 322, 323
Shannon, 19, 316, 317, 320, 321, 364, 366, 368,
399, 402
statistical, 19, 316–323
thermodynamic, 317–322, 391, 398
equivalence principle, 153
evanescent waves, 143, 144, 156
event horizon, 38
fading, 179
far ﬁeld, 150
Fourier series, 49
Fourier transform, 48
fractal dimension, 116, 117, 123, 124
Fraunhofer condition, 151
Fraunhofer region, 151
Fredholm integral equation, 92, 100, 104, 106, 109,
192, 198, 413–415
second kind, 64, 260, 276
Fredholm operator, 64
frequency diversity, 202, 204, 205, 216,
217
frequency response, 5, 164, 211, 216, 297
frequency spread, 203–205, 212
frequency-dispersive media, 137
Frobenius norm, 103, 196
Fubini’s theorem, 413
functional dimension, 377
galactic noise, 324, 325
gauge invariance, 145, 146
Gaussian process, 345
Green’s function, 42, 159–162, 164–168, 171, 174,
179, 200, 211, 214–216, 222, 225, 226, 232,
234, 236, 386
dyadic, 160
frequency-varying, 194
reduced, 233
spatially varying, 195
spectral, 159

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Index
More Information
www.cambridge.org
© in this web service Cambridge University Press
Index
449
stochastic, 178–181, 186, 188, 190, 193, 198
time-varying, 174, 187
Green’s operator, 233, 246, 247, 261, 392
GSM, 206
Hardy’s problem, 263
Helmholtz equation, 65, 72, 84
hierarchical cooperation, 221, 224–227
Hilbert dimension, 87
Hilbert space, 41, 69, 87, 95, 97, 248, 260, 408
Hilbert–Schmidt
decomposition, 41, 42, 95, 100, 102, 157,
167–170, 173, 187–189, 208, 211, 218, 226,
246–248, 261
kernel, 95, 96, 100, 246
operator, 95–98, 100, 188
Holevo’s theorem, 388
holographic information bound, 38–40, 330–332,
337, 405
homogeneous media, 133, 134, 137, 139, 144, 146
Huygens’ principle, 153
imaginary unit, 48
impulse response, 158, 164, 171
incoherent energy, 275
incoherent response, 275, 283, 284, 295–297, 299,
300
information dimension, 376
instantaneous power, 48
inter-symbol interference, 201
interference alignment, 222, 224, 225, 227
isotropic media, 133, 134, 137–139, 144, 146
Kac–Murdock–Szegö theorem, 193, 419
Karhunen–Loève representation, 42, 173, 186–191,
195, 196, 198, 420
keyhole effect, 186, 220
Kolmogorov capacity see also ǫ-capacity 370
Kolmogorov entropy see also ǫ-entropy 369
Liouville–Neumann series, 260, 414
logons, 83
lossless media, 144, 146
magnetic induction, 132
Maxwell’s equations, 130, 132, 133, 135, 139, 152,
153, 157, 158, 160, 162, 163
Maxwell–Boltzmann collision equation, 276
measurement error, 115, 119, 120
measurement operator, 113, 115, 388
measurement rate, 115
Mercer’s theorem, 102, 412, 416
metric entropy see also Kolmogorov entropy 369
metric order, 377
microwave window, 323
MIMO, 207–209, 218, 219, 221, 225, 226
Minkowski dimension, 117
Minkowski sum, 117
monopoles, 132
multi-band signals, 105, 376
multi-hop, 219, 222, 225, 226
multi-path, 176, 178–180, 182–184, 197, 277
multiple scattering theory, 42, 276, 278
mutual coherence function, 182–184
mutual information, 25, 26, 350, 351, 366, 367, 372,
373
N-width, 41, 81, 88–91, 95, 97, 100–105, 123, 368
narrow-band transmission, 201, 202
near ﬁeld, 150
network information theory, 219
Newton’s constant, 38, 328
noise, 54
additive, 344, 350, 383
colored Gaussian, 356
Gaussian, 29, 303–306, 344, 345, 350, 359
temperature, 323
thermal, 303–307, 312, 317, 325
non-dispersive media, 132–134, 137–139, 144, 146
Nyquist number, 51, 66, 82, 245, 252, 254, 255,
259, 260
OFDM, 205, 208–210, 212–217, 219
packing number, 15
Paley–Wiener theorem, 52
Parseval’s theorem, 49, 70, 78, 92, 239, 418
path loss, 278, 291, 292, 299, 300
permeability, 133, 144
permittivity, 133, 144
phase transition, 8, 9, 11, 12, 50, 64–66, 79, 80, 83,
91, 107, 109, 110, 192, 236
photon, 35, 284–288, 290, 292, 294, 296, 298, 299,
306, 325, 326, 332, 333
energy, 36, 58, 313, 314, 324–326, 332
Planck’s constant, 36, 58, 313
Planck’s energy, 38
Planck’s equation, 36, 325
Planck’s length, 38, 328–330, 332, 333, 402
Planck’s radiation formula, 330, 341, 391
Planck’s scale, 40
plane wave, 139, 141, 142, 154, 155
Poisson distribution, 305
Poisson noise, 306, 355
Poisson process, 305, 317
power delay proﬁle, 278
power density
full, 287–292
radiated, 287–291
Poynting vector, 155
Poynting–Umov theorem, 139, 154

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Index
More Information
www.cambridge.org
© in this web service Cambridge University Press
450
Index
Poynting–Umov vector, 138, 139
probabilistic channel, 25, 28
probabilistic method, 347
prolate spheroidal wave functions, 70, 74–76, 78,
79, 82, 87–89, 95, 98–100, 110, 114, 359
angular, 73
approximate, 82
radial, 73
propagation velocity, 140–143, 155, 161, 164
quanta of energy, 35
quantum complementarity, 306, 325, 326
quantum ﬂuctuations, 326
quantum noise, 306, 324, 326
radiation, 35, 130, 139, 146, 147, 151–153, 231,
275, 307, 393, 402, 403, 405
entropy, 391
noise, 306, 307, 311, 312, 323–325
pressure, 155
quantized, 394
thermal, 307, 391
rake receiver, 216, 217, 227, 228
random coding, 347, 349
random walk, 276, 284, 286, 290–292, 296,
298–300
recurrence, 290
rate gain, 173
rate–distortion, 364, 366–369, 373, 378, 379, 383,
384, 398–400
Rayleigh distribution, 178, 180, 334
Rayleigh–Jeans limit, 341
reconstruction error, 119, 120
reduced rank theorem, 103
Reed–Solomon decoding, 119
relaxation time, 140
reliability gain, 173
Riesz–Fisher theorem, 78
saddle point method, 237, 238
sampling, 8, 45, 50, 59, 76, 81, 82, 248–250, 261
interval, 51, 76
scalar potential, 145
Schrödinger equation, 87
Schwarz inequality, 56, 69, 83, 97, 98
Schwarzschild radius, 38, 328
second law of thermodynamics, 42, 318
self-adjoint operator, 87, 95–97, 100, 102, 107, 109,
127, 168, 188, 192
Shannon’s energy limit, 358
Shannon–McMillan–Breiman theorem, 18
shot noise, 304–306, 337
signal constellation, 54, 55
signal-to-noise ratio, 14, 16, 17, 23, 29, 31–36, 346,
352, 353, 363, 366, 378–380, 383, 384, 386,
387
singular value decomposition, 102, 166, 172,
187
singular values, 96, 97, 100–103, 124, 166, 167,
169, 211, 218, 246–248
Snell’s law, 143, 155
solenoidal ﬁeld, 131, 144
source codebook, 382
source coding, 121
sparsity number, 117
spatial bandwidth, 230, 233, 234, 236, 244, 250,
251, 255, 256, 260, 261
spatial spread, 203
spectral efﬁciency, 358
spectral theorem, 96
spectrum, 48
sphere covering, 17
sphere packing, 17, 31, 347
spheroidal coordinates
oblate, 73
prolate, 73
stochastic diversity, 42, 173, 187, 191–193, 197,
204, 216, 224–226, 275
Sturm–Liouville eigenvalue problem,
74
super-resolution, 11, 12, 363, 364
tap, 215, 217
tapped delay line, 215
TDMA, 206, 208, 209
time diversity, 202, 204–206
time spread, 203, 204, 206, 210–214
time-dispersive media, 133, 134
timelimited signals, 49, 51, 52, 55, 61, 63,
68, 69, 94, 108, 169, 234, 344, 359,
361, 362
timelimiting operator, 69, 98, 106,
109
transport theory, 276, 284, 290, 291
Twersky equation, 280, 281
Twersky’s theory, 279
typical modes, 327
typical sequence, 320
typical set, 21, 320, 321, 323, 339
typical states, 319–321, 323, 326, 327, 339
typical waveforms, 22, 23
ultraviolet catastrophe, 312
uncertainty principle, 306
over arbitrary measurable sets, 58, 82, 117,
407, 408
converse, 59
entropic, 58, 82
Heisenberg, 41, 55, 56, 82, 325, 337
in quantum mechanics, 56
for signals, 55, 325, 406
for a single photon, 328, 332

Cambridge University Press
978-1-107-02231-7 — Wave Theory of Information
Massimo Franceschetti 
Index
More Information
www.cambridge.org
© in this web service Cambridge University Press
Index
451
underspread channel, 214
universal entropy bound, 330, 337, 391, 394, 396,
398, 401–406
vector potential, 144
water ﬁlling, 357, 385
wave equation, 65, 71, 87
wavelet, 83
wavenumber bandwidth see also spatial bandwidth
230
wide-band transmission, 201, 202
Wiener–Khinchin theorem, 304
Young’s inequality, 232, 234, 412


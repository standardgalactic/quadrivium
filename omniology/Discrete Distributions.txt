Leif Mejlbro
Discrete Distributions
Probability Examples c-5
Download free books at

2 
Leif Mejlbro
Probability Examples c-5
Discrete Distributions
Download free eBooks at bookboon.com

3 
Probability Examples c-5 – Discrete Distributions
© 2009 Leif Mejlbro & Ventus Publishing ApS
ISBN 978-87-7681-521-9
Download free eBooks at bookboon.com

Discrete Distributions
 
4 
Contents
 
Introduction  
5
1  
Some theoretical background  
6
1.1  
The binomial distribution  
6
1.2  
The Poisson distribution   
8
1.3  
The geometric distribution  
8
1.4  
The Pascal distribution  
9
1.5  
The hypergeometric distribution  
11
2  
The binomial distribution  
12
3  
The Poisson distribution  
26
4  
The geometric distribution  
33
5  
The Pascal distribution  
51
6  
The negative binomial distribution  
54
7  
The hypergeometric distribution  
56
 
Index  
72
Contents
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Discrete Distributions
 
5 
Introduction
Introduction
This is the ﬁfth book of examples from the Theory of Probability. This topic is not my favourite,
however, thanks to my former colleague, Ole Jørsboe, I somehow managed to get an idea of what it is
all about. The way I have treated the topic will often diverge from the more professional treatment.
On the other hand, it will probably also be closer to the way of thinking which is more common among
many readers, because I also had to start from scratch.
The prerequisites for the topics can e.g. be found in the Ventus: Calculus 2 series, so I shall refer the
reader to these books, concerning e.g. plane integrals.
Unfortunately errors cannot be avoided in a ﬁrst edition of a work of this type. However, the author
has tried to put them on a minimum, hoping that the reader will meet with sympathy the errors
which do occur in the text.
Leif Mejlbro
26th October 2009
Download free eBooks at bookboon.com

Discrete Distributions
 
6 
1. Some theoretical background
1
Some theoretical background
1.1
The binomial distribution
It is well-known that the binomial coeﬃcient, where α ∈R and k ∈N0, is deﬁned by

α
k

:= α(α −1) · · · (α −k + 1)
k(k −1) · · · 1
= α(k)
k! ,
where
α(k) := α(α −1) · · · (α −k + 1)
denotes the k-th decreasing factorial. If in particular, α = n ∈N, and n ≥k, then
 n
k

=
n!
k!(n −k)!
is the number of combinations of k elements chosen from a set of n elements without replacement.
If n ∈N0 and n < k, then
 n
k

= 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Discrete Distributions
 
7 
1. Some theoretical background
Also the following formulæ should be well-known to the reader

n
k

=

n −1
k −1

+

n −1
k

,
k, n ∈N.
k

n
k

= n

n −1
k −1

,
k, n ∈N.
Chu-Vandermonde’s formula:

n + m
k

=
k

i=0

n
i
 
m
k −i

,
k, m, n ∈N,
k ≤n + m.
(−1)k
 −(α + 1)
k

=
 α + k
k

,
α ∈R,
k ∈N0.
The binomial series:
(1 + x)α =
+∞

k=0
 α
k

xk,
x ∈] −1, 1[,
α ∈R.
(1 −x)−(β) =
+∞

k=0
 −(β + 1)
k

(−1)kxk =
+∞

k=0
 β + k
k

xk.
If in particular α = n ∈N, then
(a + b)n =
n

k=0

n
k

ak bn−k,
a, b ∈R.

−1
2
k

(−4)k =
 2k
k

,
and

1
2
k

(−4)k =
1
2k −1
 2k
k

.
Stirling’s formula
n! ∼
√
2πn
n
e
n
,
where ∼denotes that the proportion between the two sides tends to 1 for n →+∞.
A Bernoulli event is an event of two possible outcomes, S and F, of the probabilities p and q = 1 −p,
resp.. Here S is a shorthand for Success, and F for Failure. The probability of success is of course p.
Assume given a sequence of n Bernoulli events of the same probability of success p. Then the proba-
bility of getting precisely k successes, 0 ≤k ≤n, is given by
 n
k

pk (1 −p)n−k,
where the binomial coeﬃcient indicates the number of ways the order of the k successes can be chosen.
A random variable X is following a binomial distributions with parameters n (the number of events)
and p ∈]0, 1[ (the probability), if the possible values of X are 0, 1, 2, . . . , n, of the probabilities
P{X = k} =
 n
k

pk (1 −p)n−k,
k = 0, 1, 2, . . . , n.
Download free eBooks at bookboon.com

Discrete Distributions
 
8 
1. Some theoretical background
In this case we write X ∈B(n, p). It is of course possible here to include the two causal distributions,
where p = 0 or p = 1 give degenerated binomial distributions.
If X ∈B(n, p) is following a Bernoulli distribution, then
E{X} = np
and
V {X} = np(1 −p) = npq.
If X ∈B(n, p) and Y ∈B(m, p) are independent and binomially distributed random variables, then
the sum X + Y ∈B(n + m, p) is again binomially distributed. We say that the binomial distribution
is reproductive in the parameter of numbers for ﬁxed probability parameter.
1.2
The Poisson distribution
A random variable X is following a Poisson distribution of parameter a ∈R+, and we write X ∈P(a),
if the possible values of X lie in N0 of the probabilities
P{X = k} = ak
k! e−a,
k ∈N0.
In this case,
E{X} = a
and
V {X} = a.
When we compute the probabilities of a Poisson distribution it is often convenient to apply the
recursion formula
P{X = k} = a
k P{X = k −1},
for k ∈N.
Assume that {Xn} is a sequence of Bernoulli distributed random variables,
Xn ∈B

n , a
n

,
a > 0 and n ∈N med n > a.
Then {Xn} converges in distribution towards a random variable X ∈P(a), which is Poisson dis-
tributed.
1.3
The geometric distribution
A random variable X is following a geometric distribution of parameter p ∈]0, 1[, and we write
Pas(1, p), if the possible values of X lie in N of the probabilities
P{X = k} = pqk−1 = p(1 −p)k−1,
k ∈N.
We have the following results
P{X ≤k} = 1 −qk
and
P{X > k} = qk,
k ∈N,
Download free eBooks at bookboon.com

Discrete Distributions
 
9 
1. Some theoretical background
and
E{X} = 1
p
and
V {X} = q
p2 .
Consider a number of tests under identical conditions, independent of each other. We assume in each
test that an event A occurs with the probability p.
Then we deﬁne a random variable X by
X = k,
if A occurs for the ﬁrst time in test number k.
Then X is geometrical distributed, X ∈Pas(1, p).
For this reason we also say that the geometric distribution is a waiting time distribution, and p is
called the probability of success.
If X ∈Pas(1, p) is geometrically distributed, then
P{X > m + n | X > n} = P{X > m},
m, n ∈N,
which is equivalent to
P{X > m + n} = P{X > m} · P{X > n},
m, n ∈N.
For this reason we also say that the geometric distribution is forgetful: If we know in advance that the
event A has not occurred in the ﬁrst n tests, then the probability that A does not occur in the next
m tests is equal to the probability that A does not occur in a series of m tests (without the previous
n tests).
1.4
The Pascal distribution
Assume that Y1, Y2, Y3, . . . , are independent random variables, all geometrically distributed with the
probability of success p. We deﬁne a random variable Xr by
Xr = Y1 + Y2 + · · · + Yr.
This random variable Xr has the values r, r + 1, r + 2, . . . , where the event {Xr = k} corresponds to
the event that the r-th success occurs in test number k. Then the probabilities are given by
P {Xr = k} =
 k −1
r −1

prqk−r,
k = r, r + 1, r + 2, . . . .
We say that Xr ∈Pas(r, p) is following a Pascal distribution of the parametres r ∈N and p ∈]0, 1[,
where r is called the parameter of numbers, and p is called the parameter of probability. In this case
E{X} = r
p
and
V {X} = rq
p2 .
If X ∈Pas(r, p) and Y ∈Pas(s, p) are independent Pascal distributed random variables of the same
parameter of probability, then the sum X +Y ∈Pas(r+s, p) is again Pascal distributed. We say that
the Pascal distribution is reproductive in the parameter of numbers for ﬁxed parameter of probability.
Download free eBooks at bookboon.com

Discrete Distributions
 
10 
1. Some theoretical background
Clearly, the geometric distribution is that particular case of the Pascal distribution, for which the
parameter of numbers is r = 1. We also call the Pascal distributions waiting time distributions.
It happens quite often that we together with Xr ∈Pas(r, p) consider the reduced waiting time, Zr =
Xr −r. This is following the distribution
P {Zr = k} =

k + r −1
k

prqk = (−1)k

−r
k

prqk,
k ∈N0,
with
E {Zr} = rq
p
and
V {Zr} = rq
p2 .
We interpret the random variable Zr by saying that it represents the number of failures before the
r-th success.
If r ∈N above is replaced by κ ∈R+, we get the negative binomially distributed random variable
X ∈NB(κ, p) with the parameters κ ∈R+ and p ∈]0, 1[, where X has N0 as its image, of the
probabilities
P{X = k} = (−1)k
 −κ
k

pκqk =
 k + κ −1
k

pκqk,
k ∈N0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Discrete Distributions
 
11 
1. Some theoretical background
In particular,
E{X} = κq
p
and
V {X} = κq
p2 .
If κ = r ∈N, then X ∈NB(r, p) is the reduced waiting time X = Xr −r, where Xr ∈Pas(r, p) is
Pascal distributed.
1.5
The hypergeometric distribution
Given a box with the total number of N balls, of which a are white, while the other ones, b = N −a,
are black. We select without replacement n balls, where n ≤a + b = N. Let X denote the random
variable, which indicates the number of white balls among the chosen n ones. Then the values of X
are
max{0 , n −b}, . . . , k, . . . min{a, n},
each one of the probability
P{X = k} =
 a
k
 
b
n −k


a + b
n

.
The distribution of X is called a hypergeometric distribution. We have for such a hypergeometric
distribution,
E{X} =
na
a + b
and
V {X} =
nab(a + b −n)
(a + b)2(a + b −1).
There are some similarities between an hypergeometric distribution and a binomial distribution. If we
change the model above by replacing the chosen ball (i.e. selection with replacement), and Y denotes
the random variable, which gives us the number of white balls, then
Y ∈B

n ,
a
a + b

is binomially distributed,
with
P{Y = k} =

n
k
 ak · bn−k
(a + b)n ,
k ∈N0,
and
E{Y } =
na
a + b
and
V {Y } =
nab
(a + b)2 .
Download free eBooks at bookboon.com

Discrete Distributions
 
12 
2. The binomial distribution
2
The binomial distribution
Example 2.1 Prove the formulæ
 n
k

=
 n −1
k −1

+
 n −1
k

,
k, n ∈N,
k

n
k

= n

n −1
k −1

,
k, n ∈N,
either by a direct computation, or by a combinatorial argument.
By the deﬁnition

n −1
k −1

+

n −1
k

=
(n −1)!
(k −1)!(n −k)! +
(n −1)!
k!(n −k −1)! =
(n −1)!
k!(n −k)! {k + (n −k)}
=
n!
k!(n −k)! =
 n
k

.
It is well-known that we can select k elements out of n in

n
k

ways.
This can also be described by
1) we either choose the ﬁrst element, and then we are left with k −1 elements for n −1 places; this
will give in total
 n −1
k −1

, or
2) we do not select the ﬁrst element, so k elements should be distributed among n −1 places. This
gives in total

n −1
k

ways.
The result follows by adding these two possible numbers.
By the deﬁnition,
k

n
k

= k ·
n!
k!(n −k)! =
n(n −1)!
(k −1)!(n −k)! = n

n −1
k −1

.
Example 2.2 Prove the formula
n

k=r

k
r

=

n + 1
r + 1

,
r, n ∈N0,
r ≤n.
Hint: Find by two diﬀerent methods the number of ways of choosing a subset of r+1 diﬀerent numbers
from the set of numbers 1, 2, dots, n + 1.
Clearly, the formula holds for n = r,
r

k=r
 k
r

=
 r
r

= 1 =
 r + 1
r + 1

=
 n + 1
r + 1

.
Download free eBooks at bookboon.com

Discrete Distributions
 
13 
2. The binomial distribution
Assume that the formula holds for some given n ≥r. Then we get for the successor n + 1, according
to the ﬁrst question of Example 2.1,
n+1

k=r
 k
r

=
n

k=r
 k
r

+
 n + 1
r

=
 n + 1
r + 1

+
 n + 1
r

=
 n + 2
r + 1

=
 (n + 1) + 1
r + 1

,
and the claim follows by induction after n for every ﬁxed r.
Since r ∈N0 can be any number, the claim follows in general.
Alternatively the formula can be shown by a combinatorial argument. Given the numbers 1, 2, 3,
. . . , n+1. From this we can choose a subset of r +1 diﬀerent elements in

n + 1
r + 1

diﬀerent ways.
Then we split according to whether the largest selected number k + 1 is r + 1, r + 2, . . . , n + 1, giving
(n + 1) −k disjoint subclasses.
Consider one of these classes with k+1, k = r, . . . , n, as its largest number. Then the other r numbers
must be selected from 1, 2, 3, . . . , k, which can be done in
 k
r

ways. Hence by summation and
identiﬁcation,
n

k=r
 k
r

=
 n + 1
r + 1

.
Example 2.3 Prove by means of Stirling’s formula,
 2n
n

∼
1
√πn 22n.
Remark: One can prove that
	
2n
2n + 1 · 22n
√πn <

2n
n

< 22n
√πn.
Stirling’s formula gives for large n,
n! ∼
√
2πn · nn · e−n,
hence
 2n
n

= (2n)!
n!n! ∼
√
2π · 2n · (2n)2ne−2n

√
2πn · nn · e−n2
= 2√πn · 22n · n2n · e−2n
2πn · n2n · e−2n
=
1
√πn · 22n.
Download free eBooks at bookboon.com

Discrete Distributions
 
14 
2. The binomial distribution
Example 2.4 Prove by means of the identity
(1 + x)n =
n

k=0

n
k

xk,
x ∈R,
n ∈N,
the formulæ
n(1 + x)n−1 =
n

k=1
k

n
k

xk−1 = n
n

k=1

n −1
k −1

xk−1,
x ∈R,
n ∈N,
and
(1 + x)n−1 −1
n + 1
=
n

k=0
1
k + 1
 n
k

xk+1,
x ∈R,
n ∈N.
The former result follows by a partial diﬀerentiation and the second result of Example 2.1.
The latter result follows by an integration from 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Discrete Distributions
 
15 
2. The binomial distribution
Note that
1
k + 1
 n
k

=
1
k + 1 ·
n!
k!(n −k)! =
1
n + 1 ·
(n + 1)!
(k + 1)!((n + 1) −(k + 1)) =
1
n + 1
 n + 1
k + 1

,
so we also have
(1 + x)n+1 −1
n + 1
=
1
n + 1
n

k=0

n + 1
k + 1

xk+1 =
1
n + 1
n+1

k=1

n + 1
k

xk.
Example 2.5 A random variable X is binomially distributed, X ∈B(n, p). Prove that
E{X(X −1)} = n(n −1)p2,
and then ﬁnd the variance of X.
By a direct computation,
E{X(X −1)}
=
n

k=1
k(k −1) P{X = k} =
n

k=2
k(k −1)
 n
k

pk(1 −p)n−k
=
n
n

k=2
(k −1)
 n −1
k −1

pk(1 −p)n−k = n(n −1)
n

k=2
 n −2
k −2

pk(1 −p)n−k
=
n(n −1)p2
n−2

ℓ=0

n −2
ℓ

pℓ(1 −p)(n−2)−ℓ= n(n −1)p2.
Since E{X} = np, or if one prefers to compute,
E{X}
=
n

k=1
k P{Xk} =
n

k=1
k
 n
k

pk(1 −p)n−k = n
n

k=1
 n −1
k −1

pp(1 −p)k
=
np
n−1

ℓ=0
 n −1
ℓ

pℓ(1 −p)(n−1)−ℓ= np,
we get the variance
V {X}
=
E

X2
−(E{X})2 = E

X2
−E{X} + E{X} −(E{X})2
=
E{X(X −1)} + E{X} −(E{X})2 = n(n −1)p2 + np −n2p2 = −np2 + np
=
np(1 −p) = npq,
where we as usual have put q = 1 −p.
Download free eBooks at bookboon.com

Discrete Distributions
 
16 
2. The binomial distribution
Example 2.6 Consider a sequence of Bernoulli experiments, where there in each experiment is the
probability p of success (S) and the probability q of failure (F). A sequence of experiments may look
like
SFSSSFFSSFFFFSFSFF · · · .
1) Find the probability that FF occurs before FS.
2) Find the probability that FF occurs before før SF.
1) Both FF and FS assume that we ﬁrst have an F, thus
P{FF before FS} = P{F} = q.
2) If just one S occurs, then FF cannot occur before SF, thus
P{FF before SF} = P{F in the ﬁrst experiment} · P{F in the second experiment} = q2.
Example 2.7 We perform a sequence of independent experiments, and in each of them there is the
probability p, where p ∈]0, 1[, that an event A occurs.
Denote by Xn the random variable, which indicates the number of times that the event A has occurred
in the ﬁrst n experiments, and let Xn+k denote the number of times the event A has occurred in the
ﬁrst n + k experiments.
Compute the correlation coeﬃcient between Xn and Xn+k.
First note that we can write
Xn+k = Xn + Yk,
where Yk is the number of times that A has occurred in the latest k experiments.
The three random variables are all Bernoulli distributed,
Xn ∈B(n, p),
Yk ∈B(k, p)
and
Xn+k ∈B(n + k, p).
Since Xn and Yk are independent, we have
Cov (Xn, Xn + Yk) = V {Xn} = np(1 −p).
Furthermore,
V {Xn+k} = (n + k)p(1 −p).
Then
ϱ (Xn, Xn+k) =
Cov (Xn, Xn+k)

V {Xn} V {Xn+k}
=

V {Xn}
V {Xn+k} =
	
n
n + k .
Download free eBooks at bookboon.com

Discrete Distributions
 
17 
2. The binomial distribution
Example 2.8 1. Let the random variable X1 be binomially distributed, X1 ∈B(n, p), (where p ∈
]0, 1[). Prove that the random variable X2 = n −X1 is binomially distributed, X2 ∈B(n, 1 −p).
Let F denote a experiment of three diﬀerent possible events A, B and C. The probabilities of these
events are a, b and c, resp., where
a > 0,
b > 0,
c > 0
and
a + b + c = 1.
Consider a sequence consisting of n independent repetitions of F.
Let X, Y and Z denote the number of times in the sequence that A, B and C occur.
2. Find the distribution of the random variables X, Y and Z and ﬁnd
V {X},
V {Y }
and
V {X + Y }.
3. Compute Cov(X, Y ) and the correlation coeﬃcient ϱ(X, Y ).
4. Compute P{X = i ∧Y = j} for i ≥0, j ≥0, i + j ≤n.
1) This is almost trivial, because
P {Xk = k} = P {X1 = n −k} =

n
n −k

pn−k(1 −p)k =

n
k

(1 −p)kpn−k
for k = 0, 1, 2, . . . , n, thus X2 ∈B(n, 1 −p).
We express this by saying that X1 counts the successes and X2 counts the failures.
2) Since X ∈B(n, a) and Y ∈B(n, b) and Z ∈B(n, c), it follows immediately that
V {X} = na(1 −a)
and
V {Y } = nb(1 −b).
We get from X + Y + Z = n that X + Y = n −Z ∈B(n, 1 −c), so
V {X + Y } = V {n −Z} = V {Z} = nc(1 −c).
3) From a + b + c = 1 follows that c = 1 −(a + b) and 1 −c = a + b. Then it follows from (2) that
Cov(X, Y )
=
1
2 (V {X + Y } −V {X} −V {Y })
=
n
2 {[1 −(a + b)](a + b) −a(1 −a) −b(1 −b)}
=
n
2

(a + b) −(a + b)2 −(a + b) +

a2 + b2
=
n
2 (−2ab) = −nab.
Hence,
ϱ(X, Y ) =
Cov(X, Y )

V {X} · V {Y }
=
−nab

na(b + c)nb(a + c)
= −

ab
(a + c)(b + c) = −

ab
(1 −a)(1 −b).
Download free eBooks at bookboon.com

Discrete Distributions
 
18 
2. The binomial distribution
4) We can select i events A in

n
i

ways, and then j events B in

n −i
j

ways, so
P{X = i, Y = j} =
 n
i
  n −i
j

aibjcn−i−j.
Remark 2.1 We get by a reduction,

n
i
 
n −i
j

=
n!
i!(n −i)! ·
(n −i)!
j!(n −i −j)! =
n!
i!j!(n −i −j)! :=

n
i , j

analogously to the binomial distribution. Note that i + j + (n −i −j) = n, cf. the denominator.
Therefore, we also write
P{X = i, Y = j} =

n
i , j

aibj(1 −ab)n−i−j,
and the distribution of (X, Y ) is a trinomial distribution (multinomial distribution or polynomial
distribution). ♦
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Discrete Distributions
 
19 
2. The binomial distribution
Example 2.9 An event H has the probability p, where 0 < p < 1, to occur in one single experiment.
An experiment consists of 10n independent experiments.
Let X1 denote the random variable which gives us the number of times which the event H occurs in
the ﬁrst n experiments.
Then let X2 denote the random variable, which indicates the number of times that the event H occurs
in the following 2n experiments.
Let X3 denote the random variable, which indicates the number of times the event H occurs in the
following 3n experiments.
Finally, X4 denotes the random variable, which gives us the number of times the event H occurs in
the remaining 4n experiments.
1. Find the distributions of X1, X2, X3 and X4.
Using the random variables X1, X2, X3 and X4 we deﬁne the new random variables
Y1 = X2 + X3,
Y2 = X3 + X4,
Y3 = X1 + X2 + X3
and
Y4 = X2 + X3 + X4.
2. Compute the correlation coeﬃcients ϱ (Y1, Y2) and ϱ (Y3, Y4).
1) Clearly,
P {X1 = k}
=

n
k

pk(1 −p)n−k,
k = 0, 1, . . . , n,
P {X2 = k}
=

2n
k

pk(1 −p)2n−k,
k = 0, 1, . . . , 2n,
P {X3 = k}
=

3n
k

pk(1 −p)3n−k,
k = 0, 1, . . . , 3n,
P {X4 = k}
=
 4n
k

pk(1 −p)4n−k,
k = 0, 1, . . . , 4n.
2) Since Y1 = X2 +X3 is the number of times which H occurs in a sequence of experiments consisting
of 5n experiments, then
P {Y1 = k} =

5n
k

pk(1 −p)5n−k,
k = 0, 1, . . . , 5n.
Analogously,
P {Y2 = k}
=

7n
k

pk(1 −p)7n−k,
k = 0, 1, . . . , 7n,
P {Y3 = k}
=
 6n
k

pk(1 −p)6n−k,
k = 0, 1, . . . , 6n,
P {Y4 = k}
=

9n
k

pk(1 −p)9n−k,
k = 0, 1, . . . , 9n.
Since in general for a binomial distribution X ∈B(m, p),
E{X} = mp
and
V {X} = mp(1 −p),
we get here
V {Y1} = 5np(1 −p),
V {Y2} = 7np(1 −p),
V {Y3} = 6np(1 −p),
V {Y4} = 9np(1 −p).
Download free eBooks at bookboon.com

Discrete Distributions
 
20 
2. The binomial distribution
Since the Xi-s are mutually independent, we get
Cov (Y1, Y2)
=
E {Y1Y2} −E {Y1} E {Y2}
=
E {(X2+X3)(X3+X4)} −(E {X2}+E {X3}) (E {X3}+E {X4})
=
E {X2 (X3 + X4)} −E {X2} · (E {X3} + E {X4})
+E

X2
3

−(E {X3})2 + E {X3X4} −E {X3} E {X4}
=
0 + V {X3} + 0 = 3np(1 −p),
hence
ϱ (Y1mY2) =
Cov (Y1, Y2)

V {Y1} · V {Y2}
=
3np(1 −p)

5np(1 −p) · 7np(1 −p)
=
3
√
35 = 3
√
35
35 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Discrete Distributions
 
21 
2. The binomial distribution
Analogously,
Cov (Y3, Y4)
=
E {Y3Y4} −E {Y3} E {Y4}
=
E {(X1 + X2 + X3) (X2 + X3 + X4)}
−E {X1 + X2 + X3} E {X2 + X3 + X4}
=
E {X1 (X2 + X3 + X4)} −E {X1} E {X2 + X3 + X4}
+E {(X2 + X3) X4} −E {X2 + X3} E {X4}
+E

(X2 + X3)2
−(E {X2 + X3})2
=
0 + 0 + V {X2 + X3} = V {Y1} = 5np(1 −p),
hence
ϱ (Y3, Y4) =
Cov (Y3, Y4)

V {Y3} · V {Y4}
=
5np(1 −p)

6np(1 −p) · 9np(1 −p)
=
5
3
√
6 = 5
√
6
18 .
Example 2.10 An assembly of 20 politicians consists of 10 from party A, 8 from party B and 2 from
parti C. A committee is going to be set up.
1) The two politicians from party C want that the committee has such a size that the probability of C
being represented in the committee, when its members are chosen randomly, is at least 25 %. How
big should the committee be to fulﬁl this demand?
2) One agrees on the size of the committee of 3. If the members are chosen randomly among the 20
politicians, one shall ﬁnd the distribution of the 2-dimensional random variable (XA, XB).
(Here, XA indicates the number of members in the committee from A, and XB the number of
members from B, and XC the number of members from C).
3) Find the distribution for each of the random variables XA, XB and XC.
4) Find the mean and variance for each of the random variables XA, XB and XC.
1) Let n, 1 ≤n ≤20, denote the size of the committee. We shall compute the probability that C is
not represented in the committee. Clearly, n ≤18. Then the probability that C is not represented
is given by
P{n chosen among the 18 members of A and B} = 18
20 · 17
19 · · · 19 −n
21 −n.
The requirement is that this probability is ≤75 %. By trial-and-error we get
n = 1 :
18
20 = 0, 90,
n = 2 :
18
20 · 17
19 = 0, 8053,
n = 3 :
18
20 · 17
19 · 16
18 = 0, 7158.
We conclude that the committee should at least have 3 members.
Download free eBooks at bookboon.com

Discrete Distributions
 
22 
2. The binomial distribution
2) We can choose 3 from 20 members in a total of
 20
3

= 20 · 19 · 18
1 · 2 · 3
= 20 · 19 · 3 = 1140 ways.
This gives us the following probabilities of the various possibilities:
P{3 from A} =
1
1140
 10
3

= 120
1140 = 2
19,
P{2 from A and 1 from B} =
1
1140
 10
2
  8
1

= 360
1140 = 6
19,
P{2 from A and 1 from C} =
1
1140
 10
2
  2
1

=
90
1140 = 3
38,
P{1 from A and 2 from B} =
1
1140
 10
1
  8
2

= 280
1140 = 14
57,
P{1 from each of A and B and C} =
1
1140

10
1
 
8
1
 
2
1

= 160
1140 = 8
57,
P{1 from A and 2 from C} =
1
1140

10
1
 
2
2

=
10
1140 =
1
114,
P{3 from B} =
1
1140

8
3

=
56
1140 = 14
285,
P{2 from B and 1 from C} =
1
1140

8
2
 
2
1

=
56
1140 = 14
285,
P{1 from B and 2 from C} =
1
1140

8
1
 
2
2

=
8
1140 =
2
285.
By suitable interpretations, e.g.
P {XA = 2, XB = 0} = P {XA = 2, XC = 1} ,
etc., we obtain the distribution of (XA, XB),
XB \XA
0
1
2
3
X⋆
B
0
0
1
114
3
38
2
19
11
57
1
2
285
8
57
6
19
⋆
44
95
2
14
285
14
57
⋆
⋆
28
95
3
14
285
⋆
⋆
⋆
14
285
X⋆
A
2
19
15
38
15
38
2
19
1
Table 1: The distribution of (XA, XB).
Download free eBooks at bookboon.com

Discrete Distributions
 
23 
2. The binomial distribution
P {XA = 3, XB = 0} = 2
19,
P {XA = 2, XB = 1} = 6
19,
P {XA = 2, XB = 0} = 3
38,
P {XA = 1, XB = 2} = 14
57,
P {XA = 1, XB = 1} = 8
57,
P {XA = 1, XB = 0} =
1
114,
P {XA = 0, XB = 3} = 14
285,
P {XA = 0, XB = 2} = 14
285,
P {XA = 0, XB = 1} =
2
285,
P {XA = 0, XB = 0} = 0.
This distribution is best described by the table on page 20.
3) The distributions of XA and XB are marked in the table by X⋆
A and X⋆
B.
We compute the distribution of XC by
P {XC = 2}
=
P {XA = 1, XC = 2} + P {XB = 1, XC = 2} =
1
114 +
2
285 =
3
190
P {XC = 1}
=
P {XA = 2, XC = 1} + P {XA = 1, XB = 1, XC = 1}
+P {XB = 2 XC = 1}
=
3
38 + 8
57 + 14
285 = 51
190,
P {XC = 0}
=
P {XA = 3} + P {XA = 2, XB = 1}
+P {XA = 1, XB = 2} + P {XB = 3}
=
2
19 + 6
19 + 14
57 + 14
285 = 68
95.
Summing up we obtain the distributions given on page 22.
Download free eBooks at bookboon.com

Discrete Distributions
 
24 
2. The binomial distribution
0
1
2
3
XA
2
19
15
38
15
38
2
19
XB
11
57
44
95
28
95
14
285
XC
68
95
51
190
3
190
0
Table 2: The distributions of XA, XB and XC.
4) The means are
E {XA} = 1 · 15
38 + 2 · 15
38 + 3 · 2
19
=
1
38 (15 + 30 + 12)
=
57
38
=
3
2,
E {XB} = 1 · 44
95 + 2 · 28
95 + 3 · 14
285
=
1
95 (44 + 56 + 14)
=
114
95
=
6
5,
E {XC} = 1 · 51
190 + 2 ·
3
190
=
57
190
=
3
10,
which can also be found in other ways. We have e.g.
E {XA} + E {XB} + E {XC} = 3
and
E {XA} = 3
2,
and then we only have to compute E {XC}.
Download free eBooks at bookboon.com

Discrete Distributions
 
25 
2. The binomial distribution
Furthermore,
E

X2
A

= 1 · 15
38 + 4 · 15
38 + 9 · 4
38
=
1
38 (15 + 60 + 36)
=
111
38 ,
E

X2
B

= 1 · 44
95 + 4 · 28
95 + 9 · 14
285
=
1
95 (44 + 112 + 42)
=
198
95 ,
E

X2
C

= 1 · 51
190 + 4 ·
3
190
=
1
190 (51 + 12)
=
63
190,
so
V {XA}
=
111
38 −9
4 = 51
76,
V {XB}
=
198
95 −36
25 = 306
475,
V {XC}
=
63
190 −
9
100 = 459
1900.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Discrete Distributions
 
26 
3. The Poisson distribution
3
The Poisson distribution
Example 3.1 Let X and Y be independent Poisson distributed random variables of the parameters
a and b, resp.. Let Z = X + Y . Find
P{X = k | Z = n},
k = 0, 1, 2, . . . , n.
Since X and Y are independent, we get for k = 0, 1,2, . . . , n, that
P{X = k | Z = n}
=
P{X = k ∧X + Y = n}
P{Z = n}
= P{X = k ∧Y = n −k}
P{Z = n}
=
P{X = k} · P{Y = n −k}
P{Z = n}
.
Since Z = X + Y is Poisson distributed with parameter a + b, we get
P{X = k | Z = n}
=
ak
k! e−a ·
bn−k
(n −k)! e−b
(a + b)n
n!
e−(a+b)
=
n!
k!(n −k)!
akbn−k
(a + b)n
=

n
k
 
a
a + b
k
·

b
a + b
n−k
,
which describes a binomial distribution B

n,
a
a + b

.
Example 3.2 Let X and Y be random variables for which
X is Poisson distributed,
X ∈P(a),
and
P{Y = k | X = n} =
 n
k

pk(1 −p)n−k,
k = 0, 1, . . . , n,
where n ∈N0 and p ∈]0, 1[.
Prove that Y is Poisson distributed, Y ∈P(ap).
We get for k ∈N0,
P{Y = k}
=
∞

n=k
P{Y = k | X = n} · P{X = n} =
∞

n=k
 n
k

pk(1 −p)n−k · an
n! e−a
=
∞

n=k
n!
k!(n −k)!
e−a
n! pk(1 −p)n−k · an = e−a
k! akpk
∞

n=k
1
(n + k)! (1 −p)n−kan−k
=
e−a
k! akpk
∞

n=0
1
n! {(1 −p)}n = e−a
k! akpk
∞

n=0
1
n! {(1 −p)a}n
=
e−a
k! akpk · e(1−p)a = 1
k! (ap)ke−ap,
proving that Y ∈P(ap).
Download free eBooks at bookboon.com

Discrete Distributions
 
27 
3. The Poisson distribution
Example 3.3 We have in Ventus: Probability c-2 introduced skewness of a distribution.
Compute the skewness γ(X) of a random variable X, which is Poisson distributed with parameter a.
What happens to γ(X), when a →∞?
The skewness is deﬁned as
γ (Xa) = 1
σ3 E

(Xa −μ)3
,
where
E

(Xa −μ)3
= E

X3
a

−μ

3σ2 + μ2
.
We have for a Poisson distributed random variable Xa ∈P(a),
P {Xa = k} = ak
k! e−a,
k ∈N0,
a > 0,
and
E {Xa} = μ = a
and
V {Xa} = σ2 = a.
It follows from
E {Xa (Xa −1) (Xa −2)} =
∞

k=3
k(k −1)(k −2) ak
k! e−a = a3
∞

k=3
ak−3
(k −3)! e−a = a3,
and
X3
a
=
Xa (Xa −1) (Xa −2) + 3X2
a −2Xa
=
Xa (Xa −1) (Xa −2) + 3Xa (Xa −1) + Xa,
that
E

X3
a

= a3 + 3a2 + a,
hence
E {(Xa −μ)} = E

X3
a

−μ

3σ2 + μ2
=

a3 + 3a2 + a

−a

3a −a3
= a.
We get the skewness by insertion,
γ (Xa) = 1
σ3 E

(Xa −μ)3
=
a
a√a =
1
√a.
Finally, it follows that
lim
a→∞γ (Xa) = lim
a→∞
1
√a = 0.
Download free eBooks at bookboon.com

Discrete Distributions
 
28 
3. The Poisson distribution
Xi
ni
0
29
1
42
2
21
3
16
4
7
5
2
6
3
≥7
0
120
Example 3.4 A carpark where one may park at most 1 hour, has 192 parking places. On the ﬁrst
ﬁve days (Monday – Friday) in one particular week someone has counted the number of vacant places
at the time intervals of 5 minutes between 2:00 PM and 4:00 PM, i.e. performed 24 observations per
day, thus in total 120 observations. The results of this investigation (which comes from Los Angeles)
is shown in the table. Here Xi denotes the number of vacant places and ni the number of times there
were observed Xi vacant places.
1) We assume that the number X of vacant places at any time between 2 PM and 4 PM is Poisson
distributed of the same mean as in the given results of the observations. Compute the expected
number of observations. corresponding to
X = 0,
X = 1,
X = 2,
X = 3,
X = 4,
X = 5,
X = 6,
X ≥7.
2) A cardriver tries once a day on each of the ﬁrst ﬁve days of the week to park in the carpark between
2 PM and 4 PM. Find the probability that he ﬁnds a vacant place on every of these days, and ﬁnd
the probability that he ﬁnds a vacant place just one of the ﬁve days.
1) We ﬁrst determine λ by
λ =
1
120 (1 · 42 + 2 · 21 + 3 · 16 + 4 · 7 + 5 · 2 + 6 · 3) = 47
30 ≈1.5667.
Using this λ we get
P{X = k} = λk
k! e−λ,
k ∈N0.
When these probabilities are multiplied by the total number 120, we obtain the following table:
n
0
1
2
3
4
5
6
≥7
Xobserved
29
42
21
16
7
2
3
0
Xexpected
25.0
39.2
30.7
16.1
6.3
2.0
0.5
0.2
2) The probability that he ﬁnds a vacant place on one day is
P{X ≥1} = 1 −P{X = 0} = 1 −e−λ = 1 −exp

−47
30

≈0.79.
Download free eBooks at bookboon.com

Discrete Distributions
 
29 
3. The Poisson distribution
The probability that he ﬁnds a vacant place on all 5 days is
(P{X ≥1})5 =

1 −exp

−47
30
5
≈0.31.
The probability that he ﬁnds a vacant place on one particular day (Monday – Friday), but not on
any other of the days is
P{X ≥1} · (P{X = 0})4 =

1 −exp

−47
30

· exp

−4 · 47
30

.
The probability that he ﬁnds a vacant place on precisely one of the 5 days is
5 · P{X ≥1} · (P{X = 0})4 = 5

1 −exp

−47
30

· exp

−4 · 47
30

≈0.0075.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Discrete Distributions
 
30 
3. The Poisson distribution
Example 3.5 A schoolboy tries modestly to become a newspaper delivery boy. He buys every morning
k newspapers for 8 DKK for each, and he tries to sell them on the same day for 12 DKK each. The
newspapers which are not sold on the same day are discarded, so the delivery boy may suﬀer a loss by
buying too many newspapers.
Experience shows that the number of newspapers which can be sold each day, X, can be assumed
approximately to follow a Poisson distribution of mean 10, thus
P{X = k} = 10k
k! e−10,
k ∈N0.
k
k
m=0
1
m! 10me−10
0
0.00005
1
0.0005
2
0.0028
3
0.0104
4
0.0293
5
0.0671
6
0.1302
7
0.2203
6
0.3329
9
0.4580
10
0.5831
Let Ek denote his expected proﬁt per day (which possibly may be negative), when he buys k newspapers
in the morning.
1) Prove that
Ek =
k

m=0
(12m −8k) 10m
m! e−10 +
∞

m=k+1
4k 10m
m! e−10.
2) Prove that
Ek+1 −Ek = 4 −12
k

m=0
10m
m! e−10.
3) Compute by using the table, Ek+1 −Ek and Ek for k = 0, 1, . . . , 10 (2 decimals).
4) Find the value of k, for which his expected proﬁt Ek is largest.
1) He spends in total 8k DKK, when he buys k newspapers and earns 12 DKK for each newspaper
he sells. Thus
Ek
=
k

m=0
(12m −8k) 10m
m! e−10 +
∞

m=k+1
(12k −8k) 10m
m! e−10
=
k

m=0
(12m −8k) 10m
m! e−10 + 4k
∞

k+1
10m
m! e−10.
Download free eBooks at bookboon.com

Discrete Distributions
 
31 
3. The Poisson distribution
2) Then, using the result of (1),
Ek+1 −Ek
=
k+1

m=0
(12m −8k −8) 10m
m! e−10 + 4(k + 1)
∞

m=k+2
10m
m! e−10
−
k

m=0
(12m −8k) 10m
m! e−10 −4k
∞

m=k+1
10m
m! e−10
=
k

m=0
(12m −8k) 10m
m! e−10 −8
k

m=0
10m
m! e−10
+4(k + 1) · 10k+1
(k + 1)! e−10 + 4k
∞

m=k+2
10m
m! e−10
+4
∞

m=k+2
10m
m! e−10 −
k

m=0
(1.5m −k) 10m
m! e−10
−4k
∞

m=k+2
10m
m! e−10 −4k · 10k+1
(k + 1)! e−10
=
−
k

m=0
10m
m! e−10 + 4
∞

m=k+1
10m
m! e−10
=
4
∞

m=0
10m
m! e−10 −1.5
k

m=0
10m
m! e−10
=
4 −12
k

m=0
10m
m! e−10.
k
Ek+1 −Ek
Ek
0
3.9994
0.00
1
3.9940
4.00
2
3.9664
7.99
3
3.8752
11.96
4
3.6484
15.84
5
3.1948
19.48
6
2.4376
22.68
7
1.3564
25.12
8
0.0052
26.47
9
-1.4960
26.48
10
-2.9972
24.98
3) Using the formulæ
Ek+1 −Ek = 4 −12
k

m=0
m

m=0
10m
m! e−10
Download free eBooks at bookboon.com

Discrete Distributions
 
32 
3. The Poisson distribution
and
Ek+1 = (Ek+1 −Ek) + Ek
we get the table on the previous page.
4) We obtain the largest expected proﬁt, 26.48 DKK, for k = 9, because
E9 −E8 > 0
and
E10 −E9 < 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Discrete Distributions
 
33 
4. The geometric distribution
4
The geometric distribution
Example 4.1 Given a random variable X of the values 1, 2, . . . of positive probabilities. Assume
furthermore,
P{X > m + n} = P{X > m} · P{X > n},
m, n ∈N.
Prove that the distribution of X is a geometric distribution.
It follows from the assumptions that
P{X > n} = P{X > 1 + · · · + 1} = (P{X > 1})n,
n ∈N.
Putting a = P{X > 1} > 0 we get
P{X > n} = an,
hence
P{X = n} = P{X > n −1} −P{X > n} = an−1 −an = (1 −a)an−1.
This shows that X is geometrically distributed with p = 1 −a and q = a, hence X ∈Pas(1, 1 −a).
Example 4.2 Let X1, X2, . . . be independent random variables, which are following a geometric
distribution,
P {Xi = k} = p qk−1,
k ∈N;
i ∈N,
where p > 0, q > 0 and p + q = 1.
Prove by induktion that Yr = X1 + X2 + · · · + Xr has the distribution
P {Yr = k} =

k −1
r −1

prqk−r,
k = r, r + 1, . . . .
Find the mean and variance of Yr.
The claim is obvious for r = 1.
If r = 2 and k ≥2, then
P {Y2 = k}
=
k−1

ℓ=1
P {X1 = ℓ} · P {X2 = k −ℓ} =
k−1

ℓ=1
p qℓ−1p qk−ℓ−1
=
(k −1)p2qk−2 =
 k −1
1

p2qk−2 =
 k −1
2 −1

p2qk−2,
proving that the formula holds for r = 2.
Assume that the formula holds for some r, and consider the successor r + 1 with Yr+1 = Xr+1 + Yr
and k ≥r + 1. Then
P {Yr+1 = k}
=
k−r

ℓ=1
P {Xr+1 = ℓ} · P {Yr = k −ℓ} =
k−r

ℓ=1
p qℓ−1
 k −ℓ−1
r −1

prqk−ℓ−r
=
pr+1qk−(r+1)
k−r

ℓ=1
 k −ℓ−1
r −1

.
Download free eBooks at bookboon.com

Discrete Distributions
 
34 
4. The geometric distribution
It follows from Example 2.2 using some convenient substitutions that
k−r

ℓ=1
 k −ℓ−1
r −1

=
k−2

j=r−1

j
r −1

=

k −1
r

,
hence by insertion,
P {Yr+1 = k} =

k −1
r

pr+1qk−(r+1).
This is exactly the same formula, only with r replaced by r + 1.
The claim now follows by induction.
Finally,
E {Yr} = r E {X1} = r
p
and
V {Yr} = r V {X1} = rq
p2 .
Example 4.3 An (honest) dice is tossed, until we for the ﬁrst time get a six.
Find the probability pn that the ﬁrst six occurs in toss number n.
Let the random variable X denote the sum of the pips in all the tosses until and included the ﬁrst toss
in which we obtain a six. Find the mean E{X}.
Clearly,
pn = P{Y = n} = 1
6 ·
5
6
n−1
,
n ∈N.
If the ﬁrst six occurs in toss number n, there are only in the ﬁrst n −1 tosses possible to obtain 1, 2,
3, 4, 5 pips, each of the probability 1
5. Hence the mean of the ﬁrst n −1 tosses is
1
5 (1 + 2 + 3 + 4 + 5) = 1
5 · 15 = 3.
In toss number n we get 6 pips, so the expected number of pips in n tosses under the condition that
the ﬁrs six occurs in toss number n is
(n −1) · 3 + 6 = 3(n + 1).
Then the mean of X is given by
E{X}
=
∞

n=1
3(n + 1)pn =
∞

n=1
3(n + 1) · 1
6 ·
5
6
n−1
= 1
2
 ∞

n=1
n
5
6
n−1
+
∞

n=1
5
6
n−1
=
1
2 ·
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1

1 −5
6
2 +
1
1 −5
6
⎫
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎭
= 1
2 {36 + 6} = 21.
Download free eBooks at bookboon.com

Discrete Distributions
 
35 
4. The geometric distribution
Example 4.4 A box contains N diﬀerent slips of paper. We then draw randomly one slip from the
box and then replace it in the box. Hence all experiments are identical and independent.
Let the random variable Xr,N denote the number of draws until we have got r diﬀerent slips of paper,
(r = 1, 2, · · · , N). Prove that
E {Xr,N} = N
 1
N +
1
N −1 + · · · +
1
N −r + 1

.
Hint: Use that
Xr,N = X1,N + (X2,N −X1,N) + · · · + (Xr,N −Xr−1,N) ,
and that each of these random variables is geometrically distributed.
Find E {Xr,N} in the two cases N = 10, r = 5 and N = 10, r = 10.
Prove that
N ln
N + 1
N −r + 1 < E {Xr,N} < N ln
N
N −r,
and show that for r ﬁxed and large N,
N ln
N
N −r
is a good approximation of E {Xr,N}.
⎛
⎜
⎝An even better approximation is N ln
N + 1
2
N −r + 1
2
⎞
⎟
⎠.
Since Xk−1,N denotes the number of draws for obtaining k−1 diﬀerent slips, Xk,N −Xk−1,N indicates
the number of draws which should be further applied in order to obtain a new slip. When we have
obtained k −1 diﬀerent slips of paper, then we have in the following experiments the probability
N −(k −1)
N
= N −k + 1
N
of getting an diﬀerent slip. This proves that Xk,N −Xk−1,N is geometrically distributed with
p = N −k + 1
N
and mean
1
p =
N
N −k + 1,
k = 1, 2, . . . , N,
where we use the convention that X0,N = 0, thus X1,N −X0,N = X1,N.
The mean is
{Xr,N}
=
E {X1,N} + E {X2,N −X1,N} + · · · + E {Xr,N −Xr−1,N}
=
N
 1
N +
1
N −1 +
1
N −2 + · · · + dfrac1N −r + 1

.
Putting for convenience
ar,N =
N

k=N−r+1
1
k ,
Download free eBooks at bookboon.com

Discrete Distributions
 
36 
4. The geometric distribution
this is written in the following short version
E {Xr,N} = N · ar,N.
If N = 10 and r = 5, then
E {X5,10} = 10
 1
10 + 1
9 + 1
8 + 1
7 + 1
7

= 6, 46.
If N = 10 and r = 10, then
E {X10,10} = 10
 1
10 + 1
9 + 1
8 + · · · + 1

= 29, 29.
We shall ﬁnally estimate
ar,N =
N

k=N−r+1
1
k .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Discrete Distributions
 
37 
4. The geometric distribution
The sequence
1
k

is decreasing, hence by the criterion of integral,
ar,N >
" N+1
N−r+1
1
x dx = ln
N + 1
N −r + 1
and
ar,N <
" N
N−r
1
x dx = ln
N
N −r,
hence
N · ln
N + 1
N −r + 1 < N · ar,N = E {Xr,N} < N ·
N
N −r.
Remark 4.1 Note that the diﬀerence between the upper and the lower bound can be estimated by
N

ln
N
N −r −ln
N + 1
N −r + 1

= N ln

N
N + 1 · N −r + 1
N −r

= N

ln

1 +
1
N −r

−ln

1 + 1
N

∼N

1
N −r −1
N

= N ·
r
(N −r) · N =
r
N −r < r
N ,
which shows that if r is kept ﬁxed and N is large, then
N ln
N + 1
N −r + 1
and
N ln
N
N −r
are both reasonable approximations of E {Xr,N}. ♦
Numerical examples
r = 5
r = 5
r = 20
N = 10
N = 20
N = 150
E {Xr,N}
6.4563
5.5902
21.3884
N ln
N + 1
N −r + 1
6.0614
5.4387
21.3124
N ln
N + 1
2
N −r + 1
2
6.4663
5.5917
21.3885
Download free eBooks at bookboon.com

Discrete Distributions
 
38 
4. The geometric distribution
Example 4.5 A box contains h white balls, r red balls and s black balls (h > 0, r > 0, s > 0). We
draw at random a ball from the box and then return it to the box. This experiment if repeated, so the
experiments are identical and independent.
Denote by X the random variable which gives the number of draws, until a white ball occurs for the
ﬁrst time, and let Y denote the number of draws which are needed before a red ball occurs for the ﬁrst
time.
1) Find the distributions of the random variables X and Y .
2) Find the means E{X} and E{Y }.
3) Find for n ∈N the probability that the ﬁrst white ball is drawn in experiment number n, and that
no red ball has occurred in the previous n −1 experiment, thus P{X = n ∧Y > n}.
4) Find P{X < Y }.
1) Since X and Y are following geometric distributions, we have
P{X = n} =
h
h + r + s

1 −
h
h + r + s
n−1
,
n ∈N,
P{Y = n} =
r
h + r + s

1 −
r
h + r + s
n−1
,
n ∈N.
2) Then,
E{X} = h + r + s
h
and
E{Y } = h + r + s
r
.
3) When the ﬁrst white ball is drawn in experiment number n, and no red ball has occurred in the
previous n−1 experiment, then all drawn balls in the ﬁrst n−1 experiments must be black. Hence
we get the probability
P{X = n ∧Y > n} =

s
h + r + s
n−1
·
h
h + r + s,
n ∈N.
4) First solution. It follows by using (3),
P{X < Y }
=
∞

n=1
P{X = n ∧Y > n} =
∞

n=1

s
h + r + s
n−1
·
h
h + r + s
=
h + r + s
h + r
·
h
h + r + s =
h
h + r.
Second solution. The number of black balls does not enter the problem when we shall ﬁnd
P{X < Y }, so we may without loss of generality put s = 0. Then by the deﬁnition of X and Y ,
P{X < Y } = P{X = 1} =
h
h + r.
In fact, if Y > X, then X must have been drawn in the ﬁrst experiment.
Download free eBooks at bookboon.com

Discrete Distributions
 
39 
4. The geometric distribution
Example 4.6 Given a roulette, where the event of each game is either “red” or “black”. We assume
that the games are independent.
The probability of the event “red” in one game is p, while the
probability of the event “black” is q, where p > 0, q > 0, p + q = 1.
Let the random variable X denote the number of games in the ﬁrst sequence of games, where the
roulette shows the same colour as in the ﬁrst game.
1) Find for n ∈N the probability that X = n.
2) Find the mean of the random variable X.
3) Find the values of p, for which the mean is bigger than 8.
4) Find for p = 1
3 the variance of the random variable X.
1) We ﬁrst note that
P{X = n}
=
P{the ﬁrst n games give red and number (n + 1) gives black}
+P{the ﬁrst n games give black and number (n+1) gives red}
=
pnq + qnp = pq

pn−1 + qn−1
.
Notice that
∞

n=1
P{X = n} =
∞

n=1
(P nq + qnp) = pq

1
1 −p +
1
1 −q

= pq
1
q + 1
p

= p + q = 1.
2) We infer from
∞

n=1
n zn−1 =
1
(1 −z)2
for |z| < 1,
that the mean is
E{X}
=
pq
# ∞

n=1
npn−1 +
∞

n=1
nqn−1
$
= pq

1
(1 −p)2 +
1
(1 −q)2

= pq
 1
q2 + 1
p2

=
p
q + q
p = p2 + q2
pq
= (p + q)2 −2pq
pq
= 1
pq −2.
3) Now q = 1 −p, so
E{X} = 1
pq −2 =
1
p(1 −p) −2
is bigger than 8 for
1
p(1 −p) > 10,
hence for
p2 −p + 1
10 > 0.
Download free eBooks at bookboon.com

Discrete Distributions
 
40 
4. The geometric distribution
Since p2 −p + 1
10 = 0 for
p = 1
2 ±
	
1
4 −1
10 = 1
2 ±
	
25 −10
100
= 1
2 ±
√
15
10 ,
we get the two possibilities
p ∈
%
0, 1
2 −
√
15
10
&
eller
p ∈
%
1
2 +
√
15
10 , 1
&
,
or approximately,
0 < p < 0.1127
or
0.8873 < p < 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Discrete Distributions
 
41 
4. The geometric distribution
4) In general,
V {X}
=
E

X2
−(E{X})2 = E{X(X −1)} + E{X} −(E{X})2
=
∞

n=2
n(n −1) {pn1 + qnp} + p
q + q
p −
p
q + q
p
2
=
p2q
∞

n=2
n(n −1)pn−2 + pq2
∞

n=2
n(n −1)qn−2 + p
q + q
p −p2
q2 −q2
p2 −2
=
p2q ·
2
(1 −p)3 + pq2 ·
2
(1 −q)3 + p
q + q
p −p2
q2 −q2
p2 −2
=
2 p2q
q2 + 2 pq2
+
p
q + q
p −p2
q2 −q2
p2 −2 = p2
q2 + q2
p2 + p
q + q
p −2
=
p
q
p
q + 1

+ q
p
q
p + 1

−2 = p
q2 + q
p2 −2.
Inserting p = 1
3 and q = 2
3 we get
V {X} = 1
3 · 9
4 + 2
3 · 9 −2 = 3
4 + 6 −2 = 19
4 .
Example 4.7 We perform a sequence of independent experiments.
In each of these there is the
probability p of the event A, and the probability q = 1 −p of the event B (0 < p < 1). Such a sequence
of experiments may start in the following way,
AAABBABBBBBBAAB · · · .
Every maximal subsequence of the same type is called a run. In the example above the ﬁrst run AAA
has the length 3, the second run BB has length 2, the third run A has length 1, the fourth run BBBBBB
has length 6, etc.
Denote by '
Xn the length of the n-th run, n ∈N.
1) Explain why the random variables Xn, n ∈N, are independent.
2) Find P {X1 = k}, k ∈N.
3) Find E {X1} and V {X1}.
4) Find P {X2 = k}, k ∈N.
5) Find E {X2} and V {X2}.
6) Prove that
E {X2} ≤E {X1}
and
V {X2} ≤V {X1} .
1) The probability of the event {Xn = k} depends only of the ﬁrst element of the n-th run, proving
that Xn, n ∈N, are independent.
Download free eBooks at bookboon.com

Discrete Distributions
 
42 
4. The geometric distribution
2) It follows like in Example 4.6 that (k rersults of A, resp. B)
E {X1} = P{A · · · AB} + P{B · · · BA} = pkq + qkp = pq

pk−1 + qk−1
.
3) The mean is, cf. Example 4.6,
E {X1} =
∞

k=1
pq · k

pk−1 + qk−1
= pq

1
(1 −p)2 +
1
(1 −q)2

= p
q + q
p = p2 + q2
pq
= 1
pq −2.
Furthermore,
E {X1 (X1 −1)}
=
∞

k=2
k(k −1)pkq +
∞

k=2
k(k −1)qkp = p2q
∞

k=2
k(k −1)pk−2 + pq2
∞

k=2
k(k −1)qk−2
=
p2q · 2
q3 + pq2 · 2
p3 = 2
p2
q2 + q2
p2

,
hence
V {X1}
=
E {X1 (X1 −1)} + E {X1} −(E {X1})2 = 2
p2
q2 + q2
p2

+ p
q + q
p −
p
q + q
p
2
=
p2
q2 + q2
p2 + p
q + q
p −1 = p
q2 + q
p2 −2.
4) If we have ℓcopies of A and k copies of B in the ﬁrst sum, and ℓcopies of B and k copies of A ib
the second sum, then
P {X2 = k}
=
∞

ℓ=1
P{A · · · AB · · · BA} +
∞

ℓ=1
P{B · · · BA · · · AB} =
∞

ℓ=1
pℓqk · p +
∞

ℓ
qℓpkq
=
p2qk
∞

ℓ=1
pℓ−1 + q2pk
∞

ℓ=1
qℓ−1 = p2qk
1 −p + q2pk
1 −q = p2qk−1 + q2pk−1.
5) The mean is
E {X2} =
∞

k=1
kp2qk−1 +
∞

k=1
kq2qk−1 =
p2
(1 −q)2 +
q2
(1 −p)2 = p2
p2 + q2
q2 = 2.
Furthermore,
E {X2 (X2 −1)}
=
∞

k=2
k(k −1)q2qk−1 +
∞

k=2
k(k −1)q2pk−1
=
p2q
∞

k=2
k(k −1)qk−2 + pq2
∞

k=2
k(k −1)pk−2
=
p2q ·
2
(1 −q)3 + pq2 ·
2
(1 −p)3 = 2p2q
p3
+ 2pq2
q3
= 2
p
q + q
p

,
Download free eBooks at bookboon.com

Discrete Distributions
 
43 
4. The geometric distribution
hence
V {X2}
=
E {X2 (X2 −1)} + E {X2} −(E {X2})2
=
2
p
q + q
p

+ 2 −4 = 2
p
q + q
p −1

.
6) Now ϕ(x) = x + 1
x has a minimum for x = 1, so if we put x = p
q , then
E {X1} = p
q + q
p = p
q + 1
p
q
≥1 + 1 = 2 = E {X2} .
It follows from
V {X1} = x2 + 1
x2 + x + 1
x −2
and
V {X2} = 2x + 2
x −2,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Discrete Distributions
 
44 
4. The geometric distribution
that
V {X1} −V {X2}
=
x2 + 1
x2 −x −1
x = x(x −1) + 1 −x
x2
= (x −1)

x3 −1

x2
=
x −1
x
2 
x2 + x + 1

≥0,
hence V {X1} ≥V {X2}. Equality is only obtained for x = p
q = 1, i.e. for
p = q = 1
2.
Example 4.8 We toss a sequence of tosses with an (honest) dice.
The tosses are identical and
independent. We deﬁne a random variable X of values in N0 by
X = n, if the ﬁrst six is in toss number n + 1.
1) Find the distribution and mean of X.
2) Find for k = 0, 1, 2, . . . , n, the probability that there in n tosses are precisely k ﬁves and no six.
3) Find the probability pn,k that the ﬁrst six occurs in toss number n + 1, and that we have had
precisely k ﬁves, n = k, k + 1, . . . . in the ﬁrst n tosses.
4) Find for every k ∈N0 the probability pk that we have got exactly k ﬁves before we get the ﬁrst six.
5) Find the expected number of ﬁves before we get the ﬁrst six.
1) It is immediately seen that
P{X = n} =
5
6
n
· 1
6 =
5n
6n+1 ,
n ∈N0.
Hereby we get the mean
E{X} =
∞

n=0
n P{X = n} = 5
62
∞

n=1
n
5
6
n−1
= 5
62 ·
1

1 −5
6
2 = 5.
2) If in n tosses there are k ﬁves and no six, this is obtained by choosing k places out of n, which
can be done in
 n
k

sways. The remaining n −k tosses are then chosen among the numbers
{1, 2, 3, 4}, so
P{k ﬁves and no six in n tosses} =

n
k
 1
6
k 4
6
n−k
,
for k = 0, 1, . . . , n.
Download free eBooks at bookboon.com

Discrete Distributions
 
45 
4. The geometric distribution
3) If n ≥k, then
pn,k
=
P{k ﬁves and no six in n tosses} × P{one six in toss number n + 1}
=
 n
k

·
1
6
k
·
4
6
n−k
· 1
6 =
 n
k
 1
6
k+1 2
3
n−k
.
4) This probability is
pk
=
∞

n=k
pn,k =
1
4k · 6
∞

n=k

n
k
 2
3
n
=
1
4k · 6
∞

n=k+1

n −1
k
 2
3
n−1
=
1
4k · 6
∞

n=k+1

n −1
(k + 1) −1
 2
3
n−(k+1) 1
3
k+1 2
3
k 1
3
−k−1
=
1
4k · 6 · 3 · 2k
∞

n=k+1

n −1
(k + 1) −1
 1
3
k+1 2
3
n−(k+1)
=
1
2k+1 ,
k ∈N0,
because the sum is the total probability that Y ∈Pa

k + 1, 1
3

occurs, hence = 1.
An alternative computation is the following
pk
=
∞

n=k
pn.k =
1
6
k+1
∞

n=k

n
k
 2
3
n−k
=
1
6
k+1 ∞

ℓ=0

ℓ+ k
k
 2
3
ℓ
=
1
6
k+1 ∞

ℓ=0
 ℓ+ k
ℓ
 2
3
ℓ
=
1
6
k+1 
1 −2
3
−(k+1)
=
1
2
k+1
,
k ∈N0.
Alternatively we see that every other toss than just ﬁves or sixes are not relevent for the
question. If we neglect these tosses then a ﬁve and a six occur with each the probability 1
2. Then
the probability of getting exactly k ﬁves before the ﬁrst six is
1
2
k
· 1
2 =
1
2
k+1
,
k ∈N0.
5) The expected number of ﬁves coming before the ﬁrst six is then
∞

k=0
k pk =
∞

k=1
k
2k+1 = 1
4
∞

k=1
k
1
2
k−1
= 1
4 ·
1

1 −1
2
2 = 1.
Here ones usual intuition fails, because one would guess that the expected number is < 1.
Download free eBooks at bookboon.com

Discrete Distributions
 
46 
4. The geometric distribution
Example 4.9 Let X1 and X2 be independent random variables, both following a geometric distribution
P {Xi = k} = pqk−1,
k ∈N,
i = 1, 2.
Let Y = min {X1, X2}.
1) Find the distribution of Y .
2) Find E{Y } and V {Y }.
1) If k ∈N, then
P{Y = k}
=
P {X1 = k, X2 > k} + P {X1 > k, X2 = k} + P {X1 = k, X2 = k}
=
2 P {X1 = k} · P {X2 > k} + (P {X1 = k})2
=
2pqk−1 · qk + p2q2k−2 = q2k−2 
2pq + p2
=

q2k−1 ·

(p + q)2 −q2
=

1 −q2 
q2k−1 ,
proving that Y is geometrically distributed, Y ∈Pas

1, q2
.
Alternatively we ﬁrst compute
P{Y > k} = P {X1 > k, X2 > k} = P {X1 > k} · P {X2 > k} = q2k,
and use that
P{Y > k −1} = P{Y = k} + P{Y > k},
hence by a rearrangement,
P{Y = k}
=
P{Y > k −1} −P{Y > k} = q2k−2 −q2k
=

1 −q2 
q2k−1 ,
for k ∈N,
and it follows that Y is geometrically distributed, Y ∈Pa

1, q2
.
2) Using a formula and replacing p by 1 −q2, and q by q2, we get
E{Y } =
1
1 −q2
and
V {Y } =
q2
(1 −q2)2 .
Download free eBooks at bookboon.com

Discrete Distributions
 
47 
4. The geometric distribution
Example 4.10 Let X1 and X2 be independent random variables, both following a geometric distri-
bution,
P {Xi = k} = p qk−1,
k ∈N,
i = 1, 2.
Put Z = max {X1, X2}.
1) Find P{Z = k}, k ∈N.
2) Find E{Z}.
1) A small consideration gives
P{Z ≤n} = P {max (X1, X2) ≤n} = P {X1 ≤n} · P {X2 ≤n} ,
which is also written
n

k=1
P{Z = k} =
n

ℓ=1
P {X1 = ℓ} ·
n

m=1
P {X2 = m} .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Discrete Distributions
 
48 
4. The geometric distribution
Then
P{Z = b}
=
n

k=1
P{Z = k} −
n−1

k=1
P{Z = k}
=
n−1

ℓ=1
P {X1 = ℓ} + P {X1 = n}

·
n−1

m=1
P {X2 = m} + P {X2 = n}

−
n−1

ℓ=1
P {X1 = ℓ} ·
n−1

m=1
P {X2 = m}
=
P {X1 = n} ·
n

m1
P {X2 = m} + P {X2 = n} ·
n−1

ℓ=1
P {X1 = ℓ}
=
pqn−1
n

k=1
pqk−1 + pqn−1
n−1

k=1
pqk−1
=
p2qn−1
n

k=1
qk−1 + p2qn−1
n−1

k=1
qk−1
=
p2qn−1 · 1 −qn
1 −q + p2qn−1 · 1 −qn−1
1 −q
=
pqn−1 
1 −qn + 1 −qn−1
= p

2qn−1 −q2n−2 −q2n−1
.
Check:
∞

n=1
P{Z = n}
=
2p
∞

n=1
qn−1 −p
∞

n=1

q2n−1 −pq
∞

n1

q2n−1
=
2p ·
1
1 −q −p(1 + q) ·
1
1 −q2
=
2p
p −
p(1 + q)
(1 −q)(1 + q) = 2 −1 = 1,
proving that the probabilities found here actually are summed up to 1. ♦
2) From

n = 1∞n zn−1 =
1
(1 −z)2
for |z| < 1,
follows that
E{Z}
=
∞

n=1
n P{Z = n} = 2p
∞

n=1
n qn−1 −(1 + q)p
∞

n=1
n

q2n−1
=
2p ·
1
(1 −q)2 −(1 + q)p ·
1
(1 −q2)2 = 2p
p2 −
(1 + q)p
{(1 + q)p}2
=
2
p −
1
(1 + q)p = 2 + 2q −1
p(1 + q)
= 1 + 2q
1 −q2 .
Download free eBooks at bookboon.com

Discrete Distributions
 
49 
4. The geometric distribution
Example 4.11 Let X1 and X2 be independent random variables, both following a geometric distri-
bution,
P {Xi = k} = pqk−1,
k ∈N;
i = 1, 2.
Lad Y = X1 −X2.
1) Find P{Y = k}, k ∈Z.
2) Find E{Y } and V {Y }.
1) Since X1 and X2 are independent, we get
P{Y = k} =

i
P {X1 = k + i ∧X2 = i} =

i
P {X1 = k + i} · P {X2 = i} .
Then we must split the investigations into two cases:
a) If k ≥0, then
P{Y = k}
=
∞

i=1
pqk+i−1 · pqi−1 =
∞

i=1
p2qkq2i−2 = p2qk
∞

i=1

q2i−1 =
p2
1 −q2 qk
=
p
1 + q qk = 1 −q
1 + q qk.
b) If instead k < 0, then
P{Y = k}
=
∞

i=−k+1
pqk+i−1pqi−1 =
∞

i=−k+1
p2qk 
q2i−1 = p2qk ·

q2−k
1 −q2
=
p2
1 −q2 · q−k =
p
1 + q q−k = 1 −q
1 + q q−k.
Alternatively if follows for k < 0 by the symmetry that
P{Y = k}
=
P {X1 −X2 = −|k|} = P {X2 −X1 = |k|}
=
p
1 + q q|k| =
p
1 + q qk
for k ∈N0.
Summing up,
P{Y = −k} = P{Y = k} =
p
1 + q qk
for k ∈N0.
2) The mean exists and is given by
E{Y } = E {X1 −X2} = E {X1} −E {X2} = 0.
Since X1 and X2 are independent, the variance is
V {Y } = V {X1 −X2} = V {X1} + V {−X2} = V {X1} + V {X2} = 2q
p2 .
Download free eBooks at bookboon.com

Discrete Distributions
 
50 
4. The geometric distribution
Example 4.12 A man has N keys, of which only one ﬁts into one particular door. He tries the keys
one by one:
Let X be the random variable, which indicates the number of experiments until he is able to unlock
the door. Find E{X}.
If he instead of each time taking a key at random (without bothering with, if the key already has been
tested) put the tested keys aside , how many tries should he use at the average?
1) In the ﬁrst case we have a uniform distribution,
P{X = k} = 1
N
for k = 1, 2, . . . , N,
hence
E{X} = 1
N
N

k=1
k = N + 1
2
.
2) In this case the corresponding random variable is geometrically distributed with p = 1
N , hence
P{Y = k} = 1
N

1 −1
N
k−1
,
k ∈N,
Y ∈Pas

1, 1 −1
N

.
Then ﬁnally, by some formula,
E{Y } = N.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Discrete Distributions
 
51 
5. The Pascal distribution
5
The Pascal distribution
Example 5.1 Let X and Y be random variables, both having values in N0. We say that the random
variable X is stochastically larger than Y , if
P{X > k} ≥P{Y > k}
for every k ∈N0.
1) Prove that if X is stochastically larger than Y , and if E{X} exists, then
E{Y } ≤E{X}.
2) Let X ∈Pas (r, p1) and Y ∈Pas (r, p2), where r ∈N and 0 < p1 < p2 < 1. Prove that X is
stochastically larger than Y .
1) We use that
E{X} =
∞

k=0
P{X > k},
and analogously for Y . Then
E{X} =
∞

k=0
P{X > k} ≥
n

k=0
P{Y > k} = E{Y }.
2) Intuitively the result is obvious, when one thinks of the waiting time distribution. This is, however,
not so easy to prove, what clearly follows from the following computations.
Obviously, P{X > k} ≥P{Y > k} is equivalent to
P{X ≤k} ≤P{Y ≤k}.
Hence, we shall prove or the Pascal distribution that the function
ϕr,k(p) =
k

k=r
 j −1
r −1

pr(1 −p)j−r
is increasing in p ∈]0, 1[ for every r ∈N and k ≥r.
Writing more traditionally x instead of p, we get
ϕr,k(x)
=
k

j=r

j −1
r −1

xr(1 −x)j−r = xr
k−r

j=0

j + r −1
r −1

(1 −x)j
=
xr
k−r

j=0
 j + r −1
j

(1 −x)j.
We put a convenient index on X, such that we get the notation
P {Xr ≤k} = ϕr,k(x),
x = p ∈]0, 1[.
Download free eBooks at bookboon.com

Discrete Distributions
 
52 
5. The Pascal distribution
Then by a diﬀerentiation,
ϕ′
r,k(x)
=
r
x ϕr,k(x) −xr
k−r

j=0
j
 j + r −1
j

(1 −x)j−1
=
r
x ϕr,k(x) −r xr
k−r

j=1

j + r −1
j −1

(1 −x)j−1
=
r
x ϕr,k(x) −r
x xr+1
k−r−1

j=0

j + r
j

(1 −x)j
=
r
x
⎧
⎨
⎩ϕr,k(x) −xr+1
k−(r+1)

j=0
 j + (r + 1) −1
j

(1 −x)j
⎫
⎬
⎭
=
r
x {ϕr,k(x) −ϕr+1,k(x)}
=
r
x (P {Xr ≤k} −P {Xr+1 ≤k}) .
The event {Xr+1 ≤k} corresponds to that success number (r+1) comes at the latest in experiment
number k. The probability is ≤the probability that success number r occurs at the latest in
experiment number k, thus
ϕ′
r,k(x) = r
x (P {Xr ≤k} −P {Xr+1 ≤k}) ≥0,
x ∈]0, 1[,
so ϕr,k(x) is increasing in x. Therefore, if X ∈Pas(r, p1) and Y ∈Pas(r, p2), where r ∈N and
0 < p1 < p2 < 1, then
P{X ≤k} ≤P{Y ≤k}
for k ≥r.
This is equivalent to the fact that X is stochastically larger than Y .
Alternatively, the result can be proved by induktion after r. If r = 1, then
P{X > k} = (1 −p1)k > (1 −p2)k = P{Y > k},
and the result is proved for r = 1.
When r > 1, then we write
X =
r

i=1
Xi
and
Y =
r

i=1
Yi,
where
Xi ∈Pas (1, p2)
and
Yi ∈Pas (1, p2) ,
and where the Xi-s (resp. the Yi-s) are mutually independent.
The condition
P{X > k} ≥P{Y > k},
for every k ∈N0,
Download free eBooks at bookboon.com

Discrete Distributions
 
53 
5. The Pascal distribution
does only concern the distributions of X, resp. Y . Therefore, we can assume that X and Y are
independent, and that all the Xi-s and Yj-s above also are independenty.
We shall prove the following lemma:
If X1, X2, Y1, Y2 are independent random variables with values in N0, where X1 is stochas-
tically larger than Y1, and X2 is stochastically larger than Y2, then X1 +X2 is stochastically
larger that Y1 + Y2.
Proof. If k ∈N0, then
P {X1 + X2 > k}
=
∞

i=0
P {X2 = i ∧X1 > k −i}
=
∞

i=0
P {X2 = i} · P {X1 > k −i}
≥
∞

i=0
P {X2 = i} · P {Y1 > k −i}
=
P {Y1 + X2 > k} =
∞

i=0
P {Y1 = i} · P {X2 > k −i}
≥
∞

i=0
P {Y1 = i} · P {Y2 > k −i} = P {Y1 + Y2 > k} ,
and the claim is proved. □
Then write
X =
#r−1

i=1
Xi
$
+ Xr
and
Y =
#r−1

i=1
Xi
$
+ Yr.
It follows form the assumption of induction that r−1
i=1 Xi is stochastically larger than r−1
i=1 Yi.
Since Xr also is stochastically larger than Yr, it follows from the result above that X is stochasti-
cally larger than Y .
Download free eBooks at bookboon.com

Discrete Distributions
 
54 
6. The negative binomial distribution
6
The negative binomial distribution
Example 6.1 A random variable X has its distribution given by
P{X = k} =
 −κ
k

pκ(−q)k,
k ∈N0,
where p > 0, q > 0, p + q = 1 and κ > 0, thus X ∈NB(κ, p).
Find the mean and variance of X.
First note that
(−1)n

−κ
n

=

n + κ −1
n

,
hence the mean is given by
E{X}
=
∞

n=1
n

n + κ −1
n

pκqn =
∞

n=1
κ

n + κ −1
n −1

pκqn = n
∞

n=0

n + κ
n

qκqn+1
=
κq
p
∞

n=0

n + {κ + 1} −1
n

pκ+1qn = κq
p .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Discrete Distributions
 
55 
6. The negative binomial distribution
Furthermore,
E{X(X −1)}
=
∞

n=2
n(n −1)
 n + κ −1
n

pκqn =
∞

n=2
κ(κ + 1)
 n + κ −1
n −2

pκqn
=
κ(κ + 1)
∞

n=0
 n + {κ + 2} −1
n

pκqn+2
=
κ(κ + 1)
∞

n=0

n + {κ + 2} −1
n

pκ+2qn = κ(κ + 1) · q2
p2 ,
whence
V {X}
=
E{X(X −1)} + E{X} −(E{X})2 = κ(κ + 1) q2
p2 + κ q
p −κ2 q2
p2
=
κ
q2
p2 + q
p

= κq
p2 (q + p) = κq
p2 .
Download free eBooks at bookboon.com

Discrete Distributions
 
56 
7. The hypergeometric distribution
7
The hypergeometric distribution
Example 7.1 Banach’s match stick problem. A person B has the habit of using two boxes of
matches at the same time. We assume that a matchbox contains 50 matches. When B shall apply a
match, he chooses at random one of the two matchboxes without noticing afterwards if it is empty. Let
X denote the number of matches in one of the boxes, when we discover that the other one is empty,
and let Y denote the number of matches in the ﬁrst box, when the second one is emptied.
It can be proved that
ar = P{X = r} =

100 −r
50
 1
2
100−r
,
r = 0, 1, . . . , 50.
Find analogously
br = P{Y = r},
r = 1, 2, . . . , 50.
For completeness we give the proof of the result on ar.
We compute P{X = 50 −k}, thus we shall use 50 matches form one of the boxes and k matches from
the other one, and then in choice number 50 + k + 1 = 51 + k choose the empty box.
In the ﬁrst 50 + k choices we shall choose the box which is emptied in total 50 times, corresponding
to the probability
 50 + k
50
 1
2
50 1
2
k
=
 50 + k
k
 1
2
50+k
.
In the next choice we shall choose the empty box, which can be done with the probability 1
2. Finally,
we must choose between 2 boxes, so we must multiply by 2. Summing up,
P{X = 50 −k} =
 50 + k
k
 1
2
50+k
,
k = 0, 1, . . . , 50.
Then by the substitution r = 50 −k,
ar = P{X = r} =
 100 −r
50
 1
2
100−r
,
r = 0, 1, . . . , 50.
In order to ﬁnd br we shall compute P{Y = 50 −k}, i.e. we shall use 49 matches from one of the
boxes and k matches from the other one, and then choose the box, in which there is only 1 match
left. Analogously to the above we get
P{Y = 50 −k} =
 49 + k
k
 1
2
49+k
,
k = 0, 1, . . . , 50.
Then by the substitution r = 50 −k,
br = P{Y = r} =
 99 −r
49
 1
2
99−r
,
r = 0, 1, . . . , 50.
Download free eBooks at bookboon.com

Discrete Distributions
 
57 
7. The hypergeometric distribution
Example 7.2 . (Continuation of Example 7.1).
1) Find an explicit expression for the mean μ of the random variable X of Example 7.1.
2) Find, by using the result of Example 2.3, an approximate expression of the mean μ.
Hint to question 1: Start by reducing the expression
50 −μ =
50

r=0
(50 −r)ar.
It follows from Example 7.1 that
ar = P{X = r} =
 100 −r
50
 1
2
100−r
,
r = 0, 1, . . . , 50.
The text of the example is confusing for several reasons:
(a) On the pocket calculator TI-92 the mean is easily computed by using the command

(r ⋆nCr(100 −r, 50) ⋆2ˆ(r −100), r, 0, 50),
corresponding to
50

r=0
r

100 −r
50
 1
2
100−r
≈7.03851.
The pocket calculator is in fact very fast in this case.
(b) The hint looks very natural. The result, however, does not look like any expression containing

2n
n

, which one uses in the approximation in Example 2.3. I have tried several variants, of
which the following is the best one:
1) By using the hint we get
50 −μ =
50

r=0
(50 −r)ar =
49

r=0
(50 −r)

100 −r
50
 1
2
100−r
.
Then by Example 2.2,
 n + 1
r + 1

=
n

k=r
 k
r

,
r, n ∈N0,
r ≤n.
When we insert this result, we get
(50 −r)
 100 −r
50

=
(50 −r) (100 −r)!
50!(50 −r)! = 51 · (100 −r)!
51!(49 −r)! = 51
 100 −r
51

=
51
99−r

k=50

k
50

= 51
49−r

j=0

50 + j
50

= 51
49−r

j=0

50 + j
j

.
Download free eBooks at bookboon.com

Discrete Distributions
 
58 
7. The hypergeometric distribution
Then continue with the following
50 −μ
=
49

r=0
(50 −r)

100 −r
50
 1
2
100−r
= 51
49

r=0

100 −r
51
 1
2
100−r
=
51
49

r=0
49−r

j=0

50 + j
50
 1
2
100−r
= 51
49

j=0

50 + j
50
 49−j

r=0
1
2
100−r
=
51
49

j=0

50 + j
50
 1
2
51+j
+ · · · +
1
2
100
=
51
49

j=0
 50 + j
50
 1
2
50+j
−51 ·
1
2
100 49

j=0
 50 + j
50

=
51
49

j=0
 50 + j
50
 1
2
50+j
−51 ·
1
2
100
99

k=50
 k
50

=
51
50

r=1
 100 −r
50
 1
2
100−r
−51
1
2
100  99 + 1
50 + 1

(according to Example 2.2)
=
51
 50

r=0
 100 −r
50
 1
2
100−r
−
 100
50
 1
2
100
−51
 100
51
 1
2
100
=
51 −51
 100
50

+
 100
51
 1
2
100
= 51 −51
 100
50

+ 50
51
 100
50
 1
2
100
,
hence by a rearrangement,
μ = 101

100
50
 1
2
100
−1.
2) It follows from Example 2.3 that

2n
n

∼22n
√πn,
i.e.

100
50

∼2100
√
50π
for n = 50,
hence
μ = 101

100
50
 1
2
100
−1 ∼
101
√
50π −1 ≈7.05863.
Additional remark. If we apply the more precise estimate from Example 2.3,
	
100
101 · 2100
√
50π <

100
50

< 2100
√
50π ,
then it follows by this method that
7.01864 < μ < 7.05863.
It was mentioned in the beginning of this example that a direct application of the pocket calculator
gives
μ ≈7.03851,
which is very close to the mean value of the upper and lower bound above.
Download free eBooks at bookboon.com

Discrete Distributions
 
59 
7. The hypergeometric distribution
Example 7.3 A box contains 2N balls, of which 2h are white, 0 < h < N.
Another box contains 3N balls, of which 3h are white.
If we select two balls from each of the boxes, for which box do we have the largest probability of getting
2 white balls?
Which box has the largest probability of obtaining at least one white ball?
1) Traditionally the other balls are black, so N = h + s.
We are dealing with hypergeometric distributions, so
p2 = P {2 white balls from U2} =

2h
2
 
2s
0

 2N
2

=

2h
2

 2N
2
 = 2h(2h −1)
2N(2N −1) = h
N · 2h −1
2N −1,
and analogously,
p3 = P {2 white balls from U3} =
 3h
2


3N
2
 = 3h(3h −1)
3N(3N −1) = h
N · 3h −1
3N −1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Discrete Distributions
 
60 
7. The hypergeometric distribution
It follows from
3h −1
3N −1 −2h −1
2N −1
=
1
(3N −1)(2N −1) {(6hN −3h −2N + 1) −(6hN −2h −3N + 1)}
=
N −h
(3N −1)(2N −1) > 0,
that p2 < p3.
2) By interchanging h and s we get analogously that we have the largest probability of obtaining 2
black balls from U3.
Since ({at least one white ball} is the complementary event of {two black balls}, we have the
largest probability of obtaining at least one white ball from U2.
Example 7.4 A box contains 10 white and 5 black balls. We select without replacement 4 balls. Let
the random variable X denote the number of white balls among the 4 selected balls, and let Y denote
the number the number of black balls among the 4 selected balls.
1) Compute P {X = i}, i = 0, 1, 2, 3, 4, and P{Y = i}, i = 0, 1, 2, 3, 4.
2) Find the means E {X} and E {Y }.
3) Compute the variances V {X} and V {Y }.
4) Compute the correlation coeﬃcient between X and Y .
1) This is an hypergeometric distribution with a = 10, b = 5 and n = 4, thus
P{X = i} =
 10
i
 
5
4 −i


15
4

and
P{Y = i} =
 5
i
 
10
4 −i


15
4

.
By some computations,
P{X = 0} = P{Y = 4} =

10
0
 
5
4

 15
4

= 1 · 5
1365 =
1
273 ≈0, 0037,
P{X = 1} = P{Y = 3} =

10
1
 
5
3

1365
= 10 · 5·4
1·2
1365
= 20
273 ≈0, 0733,
P{X = 2} = P{Y = 2} =

10
2
 
5
2

1365
=
10·9
1·2 · 5·4
1·2
1365
= 90
273 ≈0, 3297,
P{X = 3} = P{Y = 1} =
 10
3
  5
1

1365
=
10·9·8
1
· 2·3 · 5
1365
= 120
273 ≈0, 4396,
Download free eBooks at bookboon.com

Discrete Distributions
 
61 
7. The hypergeometric distribution
P{X = 4} = P{Y = 0} =

10
4
 
5
0

1365
=
10·9·8·7
1·2·3·4
1365
= 42
273 ≈0, 1538,
where of course
90
273 = 30
91,
120
273 = 40
91
and
42
273 = 2
13.
2) The means are
E{X} = n ·
a
a + b = 4 ·
10
10 + 5 = 8
3,
and
E{Y } = n ·
b
a + b = 4 ·
5
10 + 5 = 4
3
(= 4 −E{X}).
3) The variance is
V {X} = V {Y } =
nab(a + b −n)
(a + b)2(a + b −1) = 4 · 10 · 5 · (10 + 5 −4)
(10 + 5)2(10 + 5 −1) = 200 · 11
225 · 14 = 44
63.
4) It follows from Y = 4 −X that
Cov(X, Y )
=
E{XY } −E{X}E{Y } = E{X(4 −X)} −E{X}E{4 −X}
=
4 E{X} −E

X2
−4 E{X} + (E{X})2 = −V {X},
hence
ϱ(X, Y ) =
Cov(X, Y )

V {X} V {Y }
= −V {X}
V {X} = −1.
Example 7.5 A collection of 100 rockets contains 10 duds. A customer buys 20 of the rockets.
1) Find the probability that exactly 2 of the 20 rockets are duds.
2) Find the probability that none of the 20 rockets is a dud.
3) What is the expected number of duds among the 20 rockets?
This is an hypergeometric distribution with a = 10 (the duds) and b = 90 and n = 20. Let X denote
the number of duds. Then
(1) P{X = i} =
 10
i
 
90
20 −i

 100
20

,
i = 0, 1, 2, . . . , 10.
Download free eBooks at bookboon.com

Discrete Distributions
 
62 
7. The hypergeometric distribution
1) When i = 2, it follows from (1) that
P{X = 2}
=

10
2
 
90
18

 100
20

= 10!
2!8! ·
90!
18!72! · 20!80!
100! = 90!
100! · 80!
72! · 20!
18! · 10!
8! · 1
2!
=
80 · 79 · 78 · 77 · 76 · 75 · 74 · 73
100 · 99 · 98 · 97 · 96 · 95 · 94 · 93 · 92 · 91 · 20 · 19 · 10 · 9
2
= 101355025
318555566 ≈0, 3182.
2) When i = 0, it follows from (1) that
P{X = 0}
=
 10
0
  90
20


100
20

=
90!
70!20! · 80!20!
100! = 90!
100! · 80!
70!
=
80 · 79 · 78 · 77 · 76 · 75 · 74 · 73 · 72 · 71
100 · 99 · 98 · 97 · 96 · 95 · 94 · 93 · 92 · 91 = 15149909
159277783 ≈0, 0951.
3) The expected number of duds is the mean
E{X} = n ·
a
a + b = 20 · 10
100 = 2.
Example 7.6 A box U1 contains 2 white and 4 black balls. Another box U2 contains 3 white and 3
black balls, and a third box U3 contains 4 white and 2 b black balls.
An experiment is described by selecting randomly a sample consisting of three balls from each of the
three boxes. The numbers of the white balls in each of these samples are random variables, which we
denote by X1, X2 and X3.
1. Compute E {X1}, E {X2}, E {X3} and E {X1 + X2 + X3} and V {X1 + X2 + X3}.
2. Find the probability P {X1 = X2 = X3}.
Then collect all 18 balls in one box, from which we take a sample consisting of 9 balls. Then the
number of white balls in the sample is a random variable, which is denoted by Y .
Find E{Y } and V {Y }.
The boxes U1, U2 and U3 are represented by
U1 = {h, h, s, s, s, s},
U2 = {h, h, h, s, s, s},
U3 = {h, h, h, h, s, s}.
Download free eBooks at bookboon.com

Discrete Distributions
 
63 
7. The hypergeometric distribution
1) We have in all three cases hypergeometric distributions. Hence,
P {X1 = i}
=

2
i
 
4
3 −i

 6
3

,
i = 1, 0, 1, 2,
P {X2 = i}
=

3
i
 
3
3 −i

 6
3

,
i = 1, 0, 1, 2, 3,
P {X3 = i}
=

4
i
 
2
3 −i


6
3

,
i = 1, 0, 1, 2, 3.
The means and the variances can now be found by formulæ in any textbook. However, we shall
here compute all probabilities in the “hard way”, because we shall need them later on:
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Discrete Distributions
 
64 
7. The hypergeometric distribution
a) When we draw from U1 we get
P {X1 = 0}
=
 3
0

· 4
6 · 3
5 · 2
4 = 1
5,
P {X1 = 1}
=

3
1

· 2
6 · 4
5 · 3
4 = 3
5,
P {X1 = 2}
=
 3
2

· 2
6 · 1
5 · 4
4 = 1
5,
P {X1 = 3}
=
0,
where
E {X1} = 1 · 3
5 + 2 · 1
5 = 1
and
E

X2
1

= 1 · 3
5 + 4 · 1
5 = 7
5,
hence
V {X1} = 7
5 −12 = 2
5.
b) When we draw from U2 we get analogously
P {X2 = 0}
=
 3
0

· 3
6 · 2
5 · 1
4 = 1
20,
P {X2 = 1}
=

3
1

· 3
6 · 3
5 · 2
4 = 9
20,
P {X2 = 2}
=
 3
2

· 3
6 · 2
5 · 3
4 = 9
20,
P {X2 = 3}
=

3
3

· 3
6 · 2
5 · 1
4 = 1
20,
hence
E {X2}
=
1 · 9
20 + 2 · 9
20 + 3 · 1
20 = 9 + 18 + 3
20
= 3
2,
E

X2
2

=
1 · 9
20 + 4 · 9
20 + 9 · 1
20 = 9 + 36 + 9
20
= 54
20 = 27
10,
V {X2}
=
54
20 −9
4 = 54 −45
20
= 9
20.
c) When we draw from U3 we get complementary to the draw from U1 that
P {X3 = 0}
=
0,
P {X3 = 1}
=
1
5,
P {X3 = 2}
=
3
5,
P {X3 = 3}
=
1
5.
Hence we get (there are here several variants)
E {X3} = 1 · 1
5 + 2 · 3
5 + 3 · 1
5 = 1 + 6 + 3
5
= 2 = 3 −E {X1} ,
Download free eBooks at bookboon.com

Discrete Distributions
 
65 
7. The hypergeometric distribution
and
V {X3} = V {X1} = 2
5.
Then
E {X1 + X2 + X3} = E {X1} + E {X2} + E {X3} = 1 + 3
2 + 2 = 9
2.
Since X1, X2 and X3 are stochastically independent, we get
V {X1 + X2 + X3} = V {X1} + V {X2} + V {X3} = 2
5 + 9
20 + 2
5 = 8 + 9 + 8
20
= 25
20 = 5
4.
2) It follows that
P {X1 = X2 = X3}
=
3

k=0
P {X1 = k} · P {X2 = k} · P {X3 = k}
=
1
5 · 1
20 · 0 + 3
5 · 9
20 · 1
5 + 1
5 · 9
20 · 3
5 + 0 · 1
20 · 1
5 = 2 · 3
5 · 9
20 · 1
5 = 27
250.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Discrete Distributions
 
66 
7. The hypergeometric distribution
3) We have in this case the hypergeometric distribution
P{Y = k} =

9
k
 
9
9 −k


18
9

=

9
k
2

18
9
,
i = 0, 1, . . . , 9
with a = b = n = 9, hence
E{Y } =
na
a + b = 9 · 9
9 + 9 = 9
2,
and
V {Y } =
nab(a + b −n)
(a + b)2(a + b −1) = 9 · 9 · 9 · 9
182 · 17
=
81
4 · 17 = 81
68.
Example 7.7 Given a sequence of random variables (Xn), for which
P {Xn = k} =
 an
k
 
bn
m −k

 an + bn
m

,
max (0, m −bn) ≤k ≤min (an, m) ,
where m, an, bn ∈N, and where an →∞and bn →∞in such a way that
an
an + bn
→p,
p ∈]0, 1[.
Prove that the sequence (Xn) converges in distribution towards a random variable X, which is bino-
mially distributed, X ∈B(m, p).
Give an intuitiv interpretation of this result.
When n is suﬃciently large, then an ≥m and bn ≥m. We choose n so big that this is the case. Then
for 0 ≤k ≤m,
P {Xn =k} =
an!
k! (an−k)! ·
bn!
(m−k)! ·
m!
(bn−m+k)! · (an+bn−m)!
(an+bn)!
=

m
k

· an (an−1)· · ·(an−k+1) · bn (bn−1)· · ·(bn−m+k+1)
(an+bn) (an+bn−1) · · · (an+bn−m+1)
=

m
k

·
an
an+bn
·
an−1
an+bn−1 · · ·
an−k+1
an+bn−k+1 ·
bn
an+bn−k · · · bn−m+k+1
an+bn−m+1
→
 m
k

pk(1 −p)m−k
for k = 0, 1, . . . , m
n˚ar n →∞.
Remark 7.1 If we have a large number of white balls an, and a large number of black balls, then it is
almost unimportant if we draw with replacement (binomial) or without replacement (hypergeometric).
Download free eBooks at bookboon.com

Discrete Distributions
 
67 
7. The hypergeometric distribution
An alternative proof, in which we apply Stirling’s formula, is the following. Let
P{X = k} =
 m
k

pk(1 −p)m−k,
k = 0, 1, . . . , m,
X ∈B(m, p).
Choose N, such that bn ≥m and an ≥m for all n ≥N. Then it follows from Stirling’s formula for
n ≥N,
P{X = k} −P {Xn = k} =
 m
k

pk(1 −p)m−k −

an
k
 
bn
m −k


an + bn
m

=

m
k

pk(1 −p)m−k −
an!
k! (an −k)! ·
bn!
(m −k)! (bn −m + k)! · m! (an + bn −m)!
(an + bn)!
=

m
k

pk(1 −p)m−k −
m!
k!(m −k)! ·
(an + bn −m)!an!bn!
(bn −m + k)! (an −k)! (an + bn)!
=
 m
k

pk(1 −p)m−k −

m
k

·
(an + bn −m)!
(bn −m + k)! (an −k)! ·
an!bn!
(an + bn)!,
where
(an+bn−m)! ∼

2π(an+bn−m) · (an +bn−m)an+bn−m exp(−(an+bn−m)),
(bn−m+k)! ∼

2π(bn−m! +k) · (bn −m+k)bn−m+k exp(−(bn−m+k)),
(an −k)! ∼

2π(an −k) · (an −k)an−k exp(−(an −k)),
an! ∼
√
2π · aan
n · exp (−an) ,
bn! ∼
√
2π · bbn
n · exp (−bn) ,
(an + bn)! ∼

2π(an + bn) · (an + bn)an+bn · exp (−(an + bn)) .
Since the exponentials disappear by insertion, we get the following
(an + bn −m)!an!bn!
(bn −m + k)!(an −k)!(an + bn)!
∼

2π(an + bn −m) · 2πan · 2πbn
2π(bn −m + k) · 2π(an −k) · 2π(an + bn) ×
×
(an + bn −m)an+bn−maan
n bbn
n
(bn −m + k)bn−m+k(an −k)an−k(an + bn)an+bn
=
	
an + bn −m
an + bn
·
an
an −k ·
bn
nn −m + k ×
×
an+bn−m
an + bn
an+bn
ak
nbm−k
n
(an+bn−m)m

an
an−k
an−k 
bn
bn−m+k
bn−m+k
→1 · lim
n→∞

1−
m
an+bn
an+bn
· lim
n→∞

1+
k
an−k
an−k
×
× lim
n→∞

1+
m −k
bn−m+k
bn−m0k
· lim
n→∞

an
an+bn−m
k 
bn
an+bn−m
m−k
= 1 · e−m · ek · em−k · pk(1 −p)m −k = pk(1 −p)m−k.
Download free eBooks at bookboon.com

Discrete Distributions
 
68 
7. The hypergeometric distribution
Finally, we get by insertion
lim
n→∞(P{X = k} −P {Xn = k}) = 0,
and we have proved that Xn →X ∈B(m, p) in distribution.
Example 7.8 A deck of cards consists of the 13 diamonds. The court cards are the 3 cards jack,
queen and king. We let in this example ace have the value 1.
We take a sample of 4 cards.
Let X denote the random variable, which indicates the number of cards which are not court cards
among the 4 selected cards.
1) Compute the probabilities
P{X = i},
i = 1, 2, 3, 4.
2) Find the mean E{X}.
3) We now assume that among the chosen 4 cards are precisely 2 cards which are not court cards,
thus X = 2.
What is the expected sum (the mean) of these 2 cards which are not court cards?
1) Here X is hypergeometrically distributed with a = 10, b = 3, n = 4, so we get
P{X = i} =
 10
i
 
3
4 −i


13
4

=
1
715
 10
i
 
3
4 −i

,
i = 1, 2, 3, 4.
Hence,
P{X = 1}
=
10
715 =
2
143,
P{X = 2}
=
135
715 = 27
143,
P{X = 3}
=
360
715 = 72
143,
P{X = 4}
=
210
715 = 42
143.
2) Then by a convenient formula the mean is
E{X} =
na
a + b = 4 · 10
13
= 40
13.
Alternatively we get by a direct computation,
E{X} =
1
143 (1 · 2 + 2 · 27 + 3 · 72 + 4 · 42) = 440
143 = 40
13.
Download free eBooks at bookboon.com

Discrete Distributions
 
69 
7. The hypergeometric distribution
3) Given that we have two cards which are not court cards. The ﬁrst one can have the values
1, 2, 3, . . . , 10, and the expected value is
1
10 {1 + 2 + 3 + · + 10} = 11
2 .
The second card has also the expected value 11
2 , hence the expected sum is
11
2 + 11
2 = 11.
Alternatively and more diﬃcult we write down all 90 possibilities of the same probability, where
we are aware of the order of the two cards:
1 + 2,
2 + 1,
3 + 1,
· · · ,
10 + 1,
1 + 3,
2 + 3,
3 + 2,
· · · ,
10 + 2,
1 + 4,
2 + 4,
3 + 4,
· · · ,
10 + 3,
...
...
...
...
1 + 10,
2 + 10,
3 + 10,
· · · ,
10 + 9.
The total sum is
18 · (1 + 2 + 3 + · · · + 10) = 18 · 55 = 990.
Then the average is
990
90 = 11.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Discrete Distributions
 
70 
7. The hypergeometric distribution
Example 7.9 We shall from a group G1 consisting of 2 girls and 5 boys choose a committee with
3 members, and from another group G2 of 3 girls and 4 boys we shall also select a committee of 3
members.Let X1 denote the number of girls in the ﬁrst committee, and X2 the number of girls in the
second committee.
1. Find the means E {X1} and E {X2}.
2. Find P {X1 = X2}.
The two groups G1 and G2 are then joined into one group G of 14 members. We shall from this group
choose a committee of 6 members. Let Y denote the number of girls in this committee.
3. Find the mean E{Y } and the variance V {Y }.
We are again considering hypergeometric distributions.
1) Since G1 consists of 2 girls and 5 boys, of which we shall choose 3 members, we get
a = 2,
b = 5,
a + b = 7
and
n = 3,
hence
E {X1} =
na
a + b = 3 · 1
7
= 6
7.
Since G2 consists of 3 girls and 4 boys, of which we shall choose 3 members, we get
a = 3,
b = 4,
a + b = 7
and
n = 3,
hence
E {X2} =
na
a + b = 3 · 3
y
= 9
7.
2) It follows from
P {X1 = k} =

2
k
 
5
3 −k


7
3

= 1
35
 2
k
 
5
3 −k

for k = 0, 1, 2,
and
P {X2 = k} =
 3
k
 
4
3 −k

 7
3

= 1
35

3
k
 
4
3 −k

for k = 0, 1, 2, 3,
and P {X1 = 3} = 0 that
P {X1 = X2} = P {X1 = 0}·P {X2 = 0}+P {X1 = 1}·P {X2 = 1}+P {X1 = 2}·P {X2 = 2} .
Here
P {X1 = 0} · P {X2 = 0} =
 2
0
  5
3

35
·
 3
0
  4
3

35
= 10 · 4
352
= 40
352 ,
Download free eBooks at bookboon.com

Discrete Distributions
 
71 
7. The hypergeometric distribution
P {X1 = 1} · P {X2 = 1} =

2
1
 
5
2

35
·

3
1
 
4
2

35
= 2 · 10 · 3 · 6
352
= 360
352 ,
P {X1 = 2} · P {X2 = 2} =
 2
2
  5
1

35
·
 3
2
  4
1

35
= 5 · 3 · 4
352
= 60
352 ,
which gives by insertion
P {X1 = X2} = 40 + 360 + 60
352
= 460
352 = 92
245.
3) Since G consists of 5 girls and 9 boys, of which we shall choose 6 members, we get
a = 5,
b = 9,
a + b = 14
and
n = 6,
hence
E{Y } =
na
a + b = 6 · 5
14 = 15
7 ,
and
V {Y } =
nab(a + b −n)
(a + b)2(a + b −1) = 6 · 5 · 9 · (14 −6)
142 · (14 −1)
= 5 · 6 · 8 · 9
4 · 13 · 49 = 540
637.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Discrete Distributions
 
72 
Index
Index
Banach’s match stick problem, 54
Bernoulli event, 5
binomial coeﬃcient, 4
binomial distribution, 4, 10, 64
binomial series, 5
Chu-Vandermonde’s formula, 5
convergence in distribution, 64
criterion of integral, 35
decreasing factorial, 4
geometric distribution, 6, 31
hypergeometric distribution, 9, 54
negative binomial distribution, 8, 52
Pascal distribution, 7, 49
Poisson distribution, 6, 24
reduced waiting time, 8
skewness, 25
Stirling’s formula, 5, 11, 56, 65
stochastically larger random variable, 49
waiting time distribution, 7
waiting time distributions, 8
Download free eBooks at bookboon.com


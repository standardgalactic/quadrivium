Leif Mejlbro
Discrete Distributions
Probability Examples c-5
Download free books at

2 
Leif Mejlbro
Probability Examples c-5
Discrete Distributions
Download free eBooks at bookboon.com

3 
Probability Examples c-5 â€“ Discrete Distributions
Â© 2009 Leif Mejlbro & Ventus Publishing ApS
ISBN 978-87-7681-521-9
Download free eBooks at bookboon.com

Discrete Distributions
 
4 
Contents
 
Introduction  
5
1  
Some theoretical background  
6
1.1  
The binomial distribution  
6
1.2  
The Poisson distribution   
8
1.3  
The geometric distribution  
8
1.4  
The Pascal distribution  
9
1.5  
The hypergeometric distribution  
11
2  
The binomial distribution  
12
3  
The Poisson distribution  
26
4  
The geometric distribution  
33
5  
The Pascal distribution  
51
6  
The negative binomial distribution  
54
7  
The hypergeometric distribution  
56
 
Index  
72
Contents
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Discrete Distributions
 
5 
Introduction
Introduction
This is the ï¬fth book of examples from the Theory of Probability. This topic is not my favourite,
however, thanks to my former colleague, Ole JÃ¸rsboe, I somehow managed to get an idea of what it is
all about. The way I have treated the topic will often diverge from the more professional treatment.
On the other hand, it will probably also be closer to the way of thinking which is more common among
many readers, because I also had to start from scratch.
The prerequisites for the topics can e.g. be found in the Ventus: Calculus 2 series, so I shall refer the
reader to these books, concerning e.g. plane integrals.
Unfortunately errors cannot be avoided in a ï¬rst edition of a work of this type. However, the author
has tried to put them on a minimum, hoping that the reader will meet with sympathy the errors
which do occur in the text.
Leif Mejlbro
26th October 2009
Download free eBooks at bookboon.com

Discrete Distributions
 
6 
1. Some theoretical background
1
Some theoretical background
1.1
The binomial distribution
It is well-known that the binomial coeï¬ƒcient, where Î± âˆˆR and k âˆˆN0, is deï¬ned by

Î±
k

:= Î±(Î± âˆ’1) Â· Â· Â· (Î± âˆ’k + 1)
k(k âˆ’1) Â· Â· Â· 1
= Î±(k)
k! ,
where
Î±(k) := Î±(Î± âˆ’1) Â· Â· Â· (Î± âˆ’k + 1)
denotes the k-th decreasing factorial. If in particular, Î± = n âˆˆN, and n â‰¥k, then
 n
k

=
n!
k!(n âˆ’k)!
is the number of combinations of k elements chosen from a set of n elements without replacement.
If n âˆˆN0 and n < k, then
 n
k

= 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Discrete Distributions
 
7 
1. Some theoretical background
Also the following formulÃ¦ should be well-known to the reader

n
k

=

n âˆ’1
k âˆ’1

+

n âˆ’1
k

,
k, n âˆˆN.
k

n
k

= n

n âˆ’1
k âˆ’1

,
k, n âˆˆN.
Chu-Vandermondeâ€™s formula:

n + m
k

=
k

i=0

n
i
 
m
k âˆ’i

,
k, m, n âˆˆN,
k â‰¤n + m.
(âˆ’1)k
 âˆ’(Î± + 1)
k

=
 Î± + k
k

,
Î± âˆˆR,
k âˆˆN0.
The binomial series:
(1 + x)Î± =
+âˆ

k=0
 Î±
k

xk,
x âˆˆ] âˆ’1, 1[,
Î± âˆˆR.
(1 âˆ’x)âˆ’(Î²) =
+âˆ

k=0
 âˆ’(Î² + 1)
k

(âˆ’1)kxk =
+âˆ

k=0
 Î² + k
k

xk.
If in particular Î± = n âˆˆN, then
(a + b)n =
n

k=0

n
k

ak bnâˆ’k,
a, b âˆˆR.

âˆ’1
2
k

(âˆ’4)k =
 2k
k

,
and

1
2
k

(âˆ’4)k =
1
2k âˆ’1
 2k
k

.
Stirlingâ€™s formula
n! âˆ¼
âˆš
2Ï€n
n
e
n
,
where âˆ¼denotes that the proportion between the two sides tends to 1 for n â†’+âˆ.
A Bernoulli event is an event of two possible outcomes, S and F, of the probabilities p and q = 1 âˆ’p,
resp.. Here S is a shorthand for Success, and F for Failure. The probability of success is of course p.
Assume given a sequence of n Bernoulli events of the same probability of success p. Then the proba-
bility of getting precisely k successes, 0 â‰¤k â‰¤n, is given by
 n
k

pk (1 âˆ’p)nâˆ’k,
where the binomial coeï¬ƒcient indicates the number of ways the order of the k successes can be chosen.
A random variable X is following a binomial distributions with parameters n (the number of events)
and p âˆˆ]0, 1[ (the probability), if the possible values of X are 0, 1, 2, . . . , n, of the probabilities
P{X = k} =
 n
k

pk (1 âˆ’p)nâˆ’k,
k = 0, 1, 2, . . . , n.
Download free eBooks at bookboon.com

Discrete Distributions
 
8 
1. Some theoretical background
In this case we write X âˆˆB(n, p). It is of course possible here to include the two causal distributions,
where p = 0 or p = 1 give degenerated binomial distributions.
If X âˆˆB(n, p) is following a Bernoulli distribution, then
E{X} = np
and
V {X} = np(1 âˆ’p) = npq.
If X âˆˆB(n, p) and Y âˆˆB(m, p) are independent and binomially distributed random variables, then
the sum X + Y âˆˆB(n + m, p) is again binomially distributed. We say that the binomial distribution
is reproductive in the parameter of numbers for ï¬xed probability parameter.
1.2
The Poisson distribution
A random variable X is following a Poisson distribution of parameter a âˆˆR+, and we write X âˆˆP(a),
if the possible values of X lie in N0 of the probabilities
P{X = k} = ak
k! eâˆ’a,
k âˆˆN0.
In this case,
E{X} = a
and
V {X} = a.
When we compute the probabilities of a Poisson distribution it is often convenient to apply the
recursion formula
P{X = k} = a
k P{X = k âˆ’1},
for k âˆˆN.
Assume that {Xn} is a sequence of Bernoulli distributed random variables,
Xn âˆˆB

n , a
n

,
a > 0 and n âˆˆN med n > a.
Then {Xn} converges in distribution towards a random variable X âˆˆP(a), which is Poisson dis-
tributed.
1.3
The geometric distribution
A random variable X is following a geometric distribution of parameter p âˆˆ]0, 1[, and we write
Pas(1, p), if the possible values of X lie in N of the probabilities
P{X = k} = pqkâˆ’1 = p(1 âˆ’p)kâˆ’1,
k âˆˆN.
We have the following results
P{X â‰¤k} = 1 âˆ’qk
and
P{X > k} = qk,
k âˆˆN,
Download free eBooks at bookboon.com

Discrete Distributions
 
9 
1. Some theoretical background
and
E{X} = 1
p
and
V {X} = q
p2 .
Consider a number of tests under identical conditions, independent of each other. We assume in each
test that an event A occurs with the probability p.
Then we deï¬ne a random variable X by
X = k,
if A occurs for the ï¬rst time in test number k.
Then X is geometrical distributed, X âˆˆPas(1, p).
For this reason we also say that the geometric distribution is a waiting time distribution, and p is
called the probability of success.
If X âˆˆPas(1, p) is geometrically distributed, then
P{X > m + n | X > n} = P{X > m},
m, n âˆˆN,
which is equivalent to
P{X > m + n} = P{X > m} Â· P{X > n},
m, n âˆˆN.
For this reason we also say that the geometric distribution is forgetful: If we know in advance that the
event A has not occurred in the ï¬rst n tests, then the probability that A does not occur in the next
m tests is equal to the probability that A does not occur in a series of m tests (without the previous
n tests).
1.4
The Pascal distribution
Assume that Y1, Y2, Y3, . . . , are independent random variables, all geometrically distributed with the
probability of success p. We deï¬ne a random variable Xr by
Xr = Y1 + Y2 + Â· Â· Â· + Yr.
This random variable Xr has the values r, r + 1, r + 2, . . . , where the event {Xr = k} corresponds to
the event that the r-th success occurs in test number k. Then the probabilities are given by
P {Xr = k} =
 k âˆ’1
r âˆ’1

prqkâˆ’r,
k = r, r + 1, r + 2, . . . .
We say that Xr âˆˆPas(r, p) is following a Pascal distribution of the parametres r âˆˆN and p âˆˆ]0, 1[,
where r is called the parameter of numbers, and p is called the parameter of probability. In this case
E{X} = r
p
and
V {X} = rq
p2 .
If X âˆˆPas(r, p) and Y âˆˆPas(s, p) are independent Pascal distributed random variables of the same
parameter of probability, then the sum X +Y âˆˆPas(r+s, p) is again Pascal distributed. We say that
the Pascal distribution is reproductive in the parameter of numbers for ï¬xed parameter of probability.
Download free eBooks at bookboon.com

Discrete Distributions
 
10 
1. Some theoretical background
Clearly, the geometric distribution is that particular case of the Pascal distribution, for which the
parameter of numbers is r = 1. We also call the Pascal distributions waiting time distributions.
It happens quite often that we together with Xr âˆˆPas(r, p) consider the reduced waiting time, Zr =
Xr âˆ’r. This is following the distribution
P {Zr = k} =

k + r âˆ’1
k

prqk = (âˆ’1)k

âˆ’r
k

prqk,
k âˆˆN0,
with
E {Zr} = rq
p
and
V {Zr} = rq
p2 .
We interpret the random variable Zr by saying that it represents the number of failures before the
r-th success.
If r âˆˆN above is replaced by Îº âˆˆR+, we get the negative binomially distributed random variable
X âˆˆNB(Îº, p) with the parameters Îº âˆˆR+ and p âˆˆ]0, 1[, where X has N0 as its image, of the
probabilities
P{X = k} = (âˆ’1)k
 âˆ’Îº
k

pÎºqk =
 k + Îº âˆ’1
k

pÎºqk,
k âˆˆN0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Discrete Distributions
 
11 
1. Some theoretical background
In particular,
E{X} = Îºq
p
and
V {X} = Îºq
p2 .
If Îº = r âˆˆN, then X âˆˆNB(r, p) is the reduced waiting time X = Xr âˆ’r, where Xr âˆˆPas(r, p) is
Pascal distributed.
1.5
The hypergeometric distribution
Given a box with the total number of N balls, of which a are white, while the other ones, b = N âˆ’a,
are black. We select without replacement n balls, where n â‰¤a + b = N. Let X denote the random
variable, which indicates the number of white balls among the chosen n ones. Then the values of X
are
max{0 , n âˆ’b}, . . . , k, . . . min{a, n},
each one of the probability
P{X = k} =
 a
k
 
b
n âˆ’k


a + b
n

.
The distribution of X is called a hypergeometric distribution. We have for such a hypergeometric
distribution,
E{X} =
na
a + b
and
V {X} =
nab(a + b âˆ’n)
(a + b)2(a + b âˆ’1).
There are some similarities between an hypergeometric distribution and a binomial distribution. If we
change the model above by replacing the chosen ball (i.e. selection with replacement), and Y denotes
the random variable, which gives us the number of white balls, then
Y âˆˆB

n ,
a
a + b

is binomially distributed,
with
P{Y = k} =

n
k
 ak Â· bnâˆ’k
(a + b)n ,
k âˆˆN0,
and
E{Y } =
na
a + b
and
V {Y } =
nab
(a + b)2 .
Download free eBooks at bookboon.com

Discrete Distributions
 
12 
2. The binomial distribution
2
The binomial distribution
Example 2.1 Prove the formulÃ¦
 n
k

=
 n âˆ’1
k âˆ’1

+
 n âˆ’1
k

,
k, n âˆˆN,
k

n
k

= n

n âˆ’1
k âˆ’1

,
k, n âˆˆN,
either by a direct computation, or by a combinatorial argument.
By the deï¬nition

n âˆ’1
k âˆ’1

+

n âˆ’1
k

=
(n âˆ’1)!
(k âˆ’1)!(n âˆ’k)! +
(n âˆ’1)!
k!(n âˆ’k âˆ’1)! =
(n âˆ’1)!
k!(n âˆ’k)! {k + (n âˆ’k)}
=
n!
k!(n âˆ’k)! =
 n
k

.
It is well-known that we can select k elements out of n in

n
k

ways.
This can also be described by
1) we either choose the ï¬rst element, and then we are left with k âˆ’1 elements for n âˆ’1 places; this
will give in total
 n âˆ’1
k âˆ’1

, or
2) we do not select the ï¬rst element, so k elements should be distributed among n âˆ’1 places. This
gives in total

n âˆ’1
k

ways.
The result follows by adding these two possible numbers.
By the deï¬nition,
k

n
k

= k Â·
n!
k!(n âˆ’k)! =
n(n âˆ’1)!
(k âˆ’1)!(n âˆ’k)! = n

n âˆ’1
k âˆ’1

.
Example 2.2 Prove the formula
n

k=r

k
r

=

n + 1
r + 1

,
r, n âˆˆN0,
r â‰¤n.
Hint: Find by two diï¬€erent methods the number of ways of choosing a subset of r+1 diï¬€erent numbers
from the set of numbers 1, 2, dots, n + 1.
Clearly, the formula holds for n = r,
r

k=r
 k
r

=
 r
r

= 1 =
 r + 1
r + 1

=
 n + 1
r + 1

.
Download free eBooks at bookboon.com

Discrete Distributions
 
13 
2. The binomial distribution
Assume that the formula holds for some given n â‰¥r. Then we get for the successor n + 1, according
to the ï¬rst question of Example 2.1,
n+1

k=r
 k
r

=
n

k=r
 k
r

+
 n + 1
r

=
 n + 1
r + 1

+
 n + 1
r

=
 n + 2
r + 1

=
 (n + 1) + 1
r + 1

,
and the claim follows by induction after n for every ï¬xed r.
Since r âˆˆN0 can be any number, the claim follows in general.
Alternatively the formula can be shown by a combinatorial argument. Given the numbers 1, 2, 3,
. . . , n+1. From this we can choose a subset of r +1 diï¬€erent elements in

n + 1
r + 1

diï¬€erent ways.
Then we split according to whether the largest selected number k + 1 is r + 1, r + 2, . . . , n + 1, giving
(n + 1) âˆ’k disjoint subclasses.
Consider one of these classes with k+1, k = r, . . . , n, as its largest number. Then the other r numbers
must be selected from 1, 2, 3, . . . , k, which can be done in
 k
r

ways. Hence by summation and
identiï¬cation,
n

k=r
 k
r

=
 n + 1
r + 1

.
Example 2.3 Prove by means of Stirlingâ€™s formula,
 2n
n

âˆ¼
1
âˆšÏ€n 22n.
Remark: One can prove that
	
2n
2n + 1 Â· 22n
âˆšÏ€n <

2n
n

< 22n
âˆšÏ€n.
Stirlingâ€™s formula gives for large n,
n! âˆ¼
âˆš
2Ï€n Â· nn Â· eâˆ’n,
hence
 2n
n

= (2n)!
n!n! âˆ¼
âˆš
2Ï€ Â· 2n Â· (2n)2neâˆ’2n

âˆš
2Ï€n Â· nn Â· eâˆ’n2
= 2âˆšÏ€n Â· 22n Â· n2n Â· eâˆ’2n
2Ï€n Â· n2n Â· eâˆ’2n
=
1
âˆšÏ€n Â· 22n.
Download free eBooks at bookboon.com

Discrete Distributions
 
14 
2. The binomial distribution
Example 2.4 Prove by means of the identity
(1 + x)n =
n

k=0

n
k

xk,
x âˆˆR,
n âˆˆN,
the formulÃ¦
n(1 + x)nâˆ’1 =
n

k=1
k

n
k

xkâˆ’1 = n
n

k=1

n âˆ’1
k âˆ’1

xkâˆ’1,
x âˆˆR,
n âˆˆN,
and
(1 + x)nâˆ’1 âˆ’1
n + 1
=
n

k=0
1
k + 1
 n
k

xk+1,
x âˆˆR,
n âˆˆN.
The former result follows by a partial diï¬€erentiation and the second result of Example 2.1.
The latter result follows by an integration from 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Discrete Distributions
 
15 
2. The binomial distribution
Note that
1
k + 1
 n
k

=
1
k + 1 Â·
n!
k!(n âˆ’k)! =
1
n + 1 Â·
(n + 1)!
(k + 1)!((n + 1) âˆ’(k + 1)) =
1
n + 1
 n + 1
k + 1

,
so we also have
(1 + x)n+1 âˆ’1
n + 1
=
1
n + 1
n

k=0

n + 1
k + 1

xk+1 =
1
n + 1
n+1

k=1

n + 1
k

xk.
Example 2.5 A random variable X is binomially distributed, X âˆˆB(n, p). Prove that
E{X(X âˆ’1)} = n(n âˆ’1)p2,
and then ï¬nd the variance of X.
By a direct computation,
E{X(X âˆ’1)}
=
n

k=1
k(k âˆ’1) P{X = k} =
n

k=2
k(k âˆ’1)
 n
k

pk(1 âˆ’p)nâˆ’k
=
n
n

k=2
(k âˆ’1)
 n âˆ’1
k âˆ’1

pk(1 âˆ’p)nâˆ’k = n(n âˆ’1)
n

k=2
 n âˆ’2
k âˆ’2

pk(1 âˆ’p)nâˆ’k
=
n(n âˆ’1)p2
nâˆ’2

â„“=0

n âˆ’2
â„“

pâ„“(1 âˆ’p)(nâˆ’2)âˆ’â„“= n(n âˆ’1)p2.
Since E{X} = np, or if one prefers to compute,
E{X}
=
n

k=1
k P{Xk} =
n

k=1
k
 n
k

pk(1 âˆ’p)nâˆ’k = n
n

k=1
 n âˆ’1
k âˆ’1

pp(1 âˆ’p)k
=
np
nâˆ’1

â„“=0
 n âˆ’1
â„“

pâ„“(1 âˆ’p)(nâˆ’1)âˆ’â„“= np,
we get the variance
V {X}
=
E

X2
âˆ’(E{X})2 = E

X2
âˆ’E{X} + E{X} âˆ’(E{X})2
=
E{X(X âˆ’1)} + E{X} âˆ’(E{X})2 = n(n âˆ’1)p2 + np âˆ’n2p2 = âˆ’np2 + np
=
np(1 âˆ’p) = npq,
where we as usual have put q = 1 âˆ’p.
Download free eBooks at bookboon.com

Discrete Distributions
 
16 
2. The binomial distribution
Example 2.6 Consider a sequence of Bernoulli experiments, where there in each experiment is the
probability p of success (S) and the probability q of failure (F). A sequence of experiments may look
like
SFSSSFFSSFFFFSFSFF Â· Â· Â· .
1) Find the probability that FF occurs before FS.
2) Find the probability that FF occurs before fÃ¸r SF.
1) Both FF and FS assume that we ï¬rst have an F, thus
P{FF before FS} = P{F} = q.
2) If just one S occurs, then FF cannot occur before SF, thus
P{FF before SF} = P{F in the ï¬rst experiment} Â· P{F in the second experiment} = q2.
Example 2.7 We perform a sequence of independent experiments, and in each of them there is the
probability p, where p âˆˆ]0, 1[, that an event A occurs.
Denote by Xn the random variable, which indicates the number of times that the event A has occurred
in the ï¬rst n experiments, and let Xn+k denote the number of times the event A has occurred in the
ï¬rst n + k experiments.
Compute the correlation coeï¬ƒcient between Xn and Xn+k.
First note that we can write
Xn+k = Xn + Yk,
where Yk is the number of times that A has occurred in the latest k experiments.
The three random variables are all Bernoulli distributed,
Xn âˆˆB(n, p),
Yk âˆˆB(k, p)
and
Xn+k âˆˆB(n + k, p).
Since Xn and Yk are independent, we have
Cov (Xn, Xn + Yk) = V {Xn} = np(1 âˆ’p).
Furthermore,
V {Xn+k} = (n + k)p(1 âˆ’p).
Then
Ï± (Xn, Xn+k) =
Cov (Xn, Xn+k)

V {Xn} V {Xn+k}
=

V {Xn}
V {Xn+k} =
	
n
n + k .
Download free eBooks at bookboon.com

Discrete Distributions
 
17 
2. The binomial distribution
Example 2.8 1. Let the random variable X1 be binomially distributed, X1 âˆˆB(n, p), (where p âˆˆ
]0, 1[). Prove that the random variable X2 = n âˆ’X1 is binomially distributed, X2 âˆˆB(n, 1 âˆ’p).
Let F denote a experiment of three diï¬€erent possible events A, B and C. The probabilities of these
events are a, b and c, resp., where
a > 0,
b > 0,
c > 0
and
a + b + c = 1.
Consider a sequence consisting of n independent repetitions of F.
Let X, Y and Z denote the number of times in the sequence that A, B and C occur.
2. Find the distribution of the random variables X, Y and Z and ï¬nd
V {X},
V {Y }
and
V {X + Y }.
3. Compute Cov(X, Y ) and the correlation coeï¬ƒcient Ï±(X, Y ).
4. Compute P{X = i âˆ§Y = j} for i â‰¥0, j â‰¥0, i + j â‰¤n.
1) This is almost trivial, because
P {Xk = k} = P {X1 = n âˆ’k} =

n
n âˆ’k

pnâˆ’k(1 âˆ’p)k =

n
k

(1 âˆ’p)kpnâˆ’k
for k = 0, 1, 2, . . . , n, thus X2 âˆˆB(n, 1 âˆ’p).
We express this by saying that X1 counts the successes and X2 counts the failures.
2) Since X âˆˆB(n, a) and Y âˆˆB(n, b) and Z âˆˆB(n, c), it follows immediately that
V {X} = na(1 âˆ’a)
and
V {Y } = nb(1 âˆ’b).
We get from X + Y + Z = n that X + Y = n âˆ’Z âˆˆB(n, 1 âˆ’c), so
V {X + Y } = V {n âˆ’Z} = V {Z} = nc(1 âˆ’c).
3) From a + b + c = 1 follows that c = 1 âˆ’(a + b) and 1 âˆ’c = a + b. Then it follows from (2) that
Cov(X, Y )
=
1
2 (V {X + Y } âˆ’V {X} âˆ’V {Y })
=
n
2 {[1 âˆ’(a + b)](a + b) âˆ’a(1 âˆ’a) âˆ’b(1 âˆ’b)}
=
n
2

(a + b) âˆ’(a + b)2 âˆ’(a + b) +

a2 + b2
=
n
2 (âˆ’2ab) = âˆ’nab.
Hence,
Ï±(X, Y ) =
Cov(X, Y )

V {X} Â· V {Y }
=
âˆ’nab

na(b + c)nb(a + c)
= âˆ’

ab
(a + c)(b + c) = âˆ’

ab
(1 âˆ’a)(1 âˆ’b).
Download free eBooks at bookboon.com

Discrete Distributions
 
18 
2. The binomial distribution
4) We can select i events A in

n
i

ways, and then j events B in

n âˆ’i
j

ways, so
P{X = i, Y = j} =
 n
i
  n âˆ’i
j

aibjcnâˆ’iâˆ’j.
Remark 2.1 We get by a reduction,

n
i
 
n âˆ’i
j

=
n!
i!(n âˆ’i)! Â·
(n âˆ’i)!
j!(n âˆ’i âˆ’j)! =
n!
i!j!(n âˆ’i âˆ’j)! :=

n
i , j

analogously to the binomial distribution. Note that i + j + (n âˆ’i âˆ’j) = n, cf. the denominator.
Therefore, we also write
P{X = i, Y = j} =

n
i , j

aibj(1 âˆ’ab)nâˆ’iâˆ’j,
and the distribution of (X, Y ) is a trinomial distribution (multinomial distribution or polynomial
distribution). â™¦
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Discrete Distributions
 
19 
2. The binomial distribution
Example 2.9 An event H has the probability p, where 0 < p < 1, to occur in one single experiment.
An experiment consists of 10n independent experiments.
Let X1 denote the random variable which gives us the number of times which the event H occurs in
the ï¬rst n experiments.
Then let X2 denote the random variable, which indicates the number of times that the event H occurs
in the following 2n experiments.
Let X3 denote the random variable, which indicates the number of times the event H occurs in the
following 3n experiments.
Finally, X4 denotes the random variable, which gives us the number of times the event H occurs in
the remaining 4n experiments.
1. Find the distributions of X1, X2, X3 and X4.
Using the random variables X1, X2, X3 and X4 we deï¬ne the new random variables
Y1 = X2 + X3,
Y2 = X3 + X4,
Y3 = X1 + X2 + X3
and
Y4 = X2 + X3 + X4.
2. Compute the correlation coeï¬ƒcients Ï± (Y1, Y2) and Ï± (Y3, Y4).
1) Clearly,
P {X1 = k}
=

n
k

pk(1 âˆ’p)nâˆ’k,
k = 0, 1, . . . , n,
P {X2 = k}
=

2n
k

pk(1 âˆ’p)2nâˆ’k,
k = 0, 1, . . . , 2n,
P {X3 = k}
=

3n
k

pk(1 âˆ’p)3nâˆ’k,
k = 0, 1, . . . , 3n,
P {X4 = k}
=
 4n
k

pk(1 âˆ’p)4nâˆ’k,
k = 0, 1, . . . , 4n.
2) Since Y1 = X2 +X3 is the number of times which H occurs in a sequence of experiments consisting
of 5n experiments, then
P {Y1 = k} =

5n
k

pk(1 âˆ’p)5nâˆ’k,
k = 0, 1, . . . , 5n.
Analogously,
P {Y2 = k}
=

7n
k

pk(1 âˆ’p)7nâˆ’k,
k = 0, 1, . . . , 7n,
P {Y3 = k}
=
 6n
k

pk(1 âˆ’p)6nâˆ’k,
k = 0, 1, . . . , 6n,
P {Y4 = k}
=

9n
k

pk(1 âˆ’p)9nâˆ’k,
k = 0, 1, . . . , 9n.
Since in general for a binomial distribution X âˆˆB(m, p),
E{X} = mp
and
V {X} = mp(1 âˆ’p),
we get here
V {Y1} = 5np(1 âˆ’p),
V {Y2} = 7np(1 âˆ’p),
V {Y3} = 6np(1 âˆ’p),
V {Y4} = 9np(1 âˆ’p).
Download free eBooks at bookboon.com

Discrete Distributions
 
20 
2. The binomial distribution
Since the Xi-s are mutually independent, we get
Cov (Y1, Y2)
=
E {Y1Y2} âˆ’E {Y1} E {Y2}
=
E {(X2+X3)(X3+X4)} âˆ’(E {X2}+E {X3}) (E {X3}+E {X4})
=
E {X2 (X3 + X4)} âˆ’E {X2} Â· (E {X3} + E {X4})
+E

X2
3

âˆ’(E {X3})2 + E {X3X4} âˆ’E {X3} E {X4}
=
0 + V {X3} + 0 = 3np(1 âˆ’p),
hence
Ï± (Y1mY2) =
Cov (Y1, Y2)

V {Y1} Â· V {Y2}
=
3np(1 âˆ’p)

5np(1 âˆ’p) Â· 7np(1 âˆ’p)
=
3
âˆš
35 = 3
âˆš
35
35 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Discrete Distributions
 
21 
2. The binomial distribution
Analogously,
Cov (Y3, Y4)
=
E {Y3Y4} âˆ’E {Y3} E {Y4}
=
E {(X1 + X2 + X3) (X2 + X3 + X4)}
âˆ’E {X1 + X2 + X3} E {X2 + X3 + X4}
=
E {X1 (X2 + X3 + X4)} âˆ’E {X1} E {X2 + X3 + X4}
+E {(X2 + X3) X4} âˆ’E {X2 + X3} E {X4}
+E

(X2 + X3)2
âˆ’(E {X2 + X3})2
=
0 + 0 + V {X2 + X3} = V {Y1} = 5np(1 âˆ’p),
hence
Ï± (Y3, Y4) =
Cov (Y3, Y4)

V {Y3} Â· V {Y4}
=
5np(1 âˆ’p)

6np(1 âˆ’p) Â· 9np(1 âˆ’p)
=
5
3
âˆš
6 = 5
âˆš
6
18 .
Example 2.10 An assembly of 20 politicians consists of 10 from party A, 8 from party B and 2 from
parti C. A committee is going to be set up.
1) The two politicians from party C want that the committee has such a size that the probability of C
being represented in the committee, when its members are chosen randomly, is at least 25 %. How
big should the committee be to fulï¬l this demand?
2) One agrees on the size of the committee of 3. If the members are chosen randomly among the 20
politicians, one shall ï¬nd the distribution of the 2-dimensional random variable (XA, XB).
(Here, XA indicates the number of members in the committee from A, and XB the number of
members from B, and XC the number of members from C).
3) Find the distribution for each of the random variables XA, XB and XC.
4) Find the mean and variance for each of the random variables XA, XB and XC.
1) Let n, 1 â‰¤n â‰¤20, denote the size of the committee. We shall compute the probability that C is
not represented in the committee. Clearly, n â‰¤18. Then the probability that C is not represented
is given by
P{n chosen among the 18 members of A and B} = 18
20 Â· 17
19 Â· Â· Â· 19 âˆ’n
21 âˆ’n.
The requirement is that this probability is â‰¤75 %. By trial-and-error we get
n = 1 :
18
20 = 0, 90,
n = 2 :
18
20 Â· 17
19 = 0, 8053,
n = 3 :
18
20 Â· 17
19 Â· 16
18 = 0, 7158.
We conclude that the committee should at least have 3 members.
Download free eBooks at bookboon.com

Discrete Distributions
 
22 
2. The binomial distribution
2) We can choose 3 from 20 members in a total of
 20
3

= 20 Â· 19 Â· 18
1 Â· 2 Â· 3
= 20 Â· 19 Â· 3 = 1140 ways.
This gives us the following probabilities of the various possibilities:
P{3 from A} =
1
1140
 10
3

= 120
1140 = 2
19,
P{2 from A and 1 from B} =
1
1140
 10
2
  8
1

= 360
1140 = 6
19,
P{2 from A and 1 from C} =
1
1140
 10
2
  2
1

=
90
1140 = 3
38,
P{1 from A and 2 from B} =
1
1140
 10
1
  8
2

= 280
1140 = 14
57,
P{1 from each of A and B and C} =
1
1140

10
1
 
8
1
 
2
1

= 160
1140 = 8
57,
P{1 from A and 2 from C} =
1
1140

10
1
 
2
2

=
10
1140 =
1
114,
P{3 from B} =
1
1140

8
3

=
56
1140 = 14
285,
P{2 from B and 1 from C} =
1
1140

8
2
 
2
1

=
56
1140 = 14
285,
P{1 from B and 2 from C} =
1
1140

8
1
 
2
2

=
8
1140 =
2
285.
By suitable interpretations, e.g.
P {XA = 2, XB = 0} = P {XA = 2, XC = 1} ,
etc., we obtain the distribution of (XA, XB),
XB \XA
0
1
2
3
Xâ‹†
B
0
0
1
114
3
38
2
19
11
57
1
2
285
8
57
6
19
â‹†
44
95
2
14
285
14
57
â‹†
â‹†
28
95
3
14
285
â‹†
â‹†
â‹†
14
285
Xâ‹†
A
2
19
15
38
15
38
2
19
1
Table 1: The distribution of (XA, XB).
Download free eBooks at bookboon.com

Discrete Distributions
 
23 
2. The binomial distribution
P {XA = 3, XB = 0} = 2
19,
P {XA = 2, XB = 1} = 6
19,
P {XA = 2, XB = 0} = 3
38,
P {XA = 1, XB = 2} = 14
57,
P {XA = 1, XB = 1} = 8
57,
P {XA = 1, XB = 0} =
1
114,
P {XA = 0, XB = 3} = 14
285,
P {XA = 0, XB = 2} = 14
285,
P {XA = 0, XB = 1} =
2
285,
P {XA = 0, XB = 0} = 0.
This distribution is best described by the table on page 20.
3) The distributions of XA and XB are marked in the table by Xâ‹†
A and Xâ‹†
B.
We compute the distribution of XC by
P {XC = 2}
=
P {XA = 1, XC = 2} + P {XB = 1, XC = 2} =
1
114 +
2
285 =
3
190
P {XC = 1}
=
P {XA = 2, XC = 1} + P {XA = 1, XB = 1, XC = 1}
+P {XB = 2 XC = 1}
=
3
38 + 8
57 + 14
285 = 51
190,
P {XC = 0}
=
P {XA = 3} + P {XA = 2, XB = 1}
+P {XA = 1, XB = 2} + P {XB = 3}
=
2
19 + 6
19 + 14
57 + 14
285 = 68
95.
Summing up we obtain the distributions given on page 22.
Download free eBooks at bookboon.com

Discrete Distributions
 
24 
2. The binomial distribution
0
1
2
3
XA
2
19
15
38
15
38
2
19
XB
11
57
44
95
28
95
14
285
XC
68
95
51
190
3
190
0
Table 2: The distributions of XA, XB and XC.
4) The means are
E {XA} = 1 Â· 15
38 + 2 Â· 15
38 + 3 Â· 2
19
=
1
38 (15 + 30 + 12)
=
57
38
=
3
2,
E {XB} = 1 Â· 44
95 + 2 Â· 28
95 + 3 Â· 14
285
=
1
95 (44 + 56 + 14)
=
114
95
=
6
5,
E {XC} = 1 Â· 51
190 + 2 Â·
3
190
=
57
190
=
3
10,
which can also be found in other ways. We have e.g.
E {XA} + E {XB} + E {XC} = 3
and
E {XA} = 3
2,
and then we only have to compute E {XC}.
Download free eBooks at bookboon.com

Discrete Distributions
 
25 
2. The binomial distribution
Furthermore,
E

X2
A

= 1 Â· 15
38 + 4 Â· 15
38 + 9 Â· 4
38
=
1
38 (15 + 60 + 36)
=
111
38 ,
E

X2
B

= 1 Â· 44
95 + 4 Â· 28
95 + 9 Â· 14
285
=
1
95 (44 + 112 + 42)
=
198
95 ,
E

X2
C

= 1 Â· 51
190 + 4 Â·
3
190
=
1
190 (51 + 12)
=
63
190,
so
V {XA}
=
111
38 âˆ’9
4 = 51
76,
V {XB}
=
198
95 âˆ’36
25 = 306
475,
V {XC}
=
63
190 âˆ’
9
100 = 459
1900.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Discrete Distributions
 
26 
3. The Poisson distribution
3
The Poisson distribution
Example 3.1 Let X and Y be independent Poisson distributed random variables of the parameters
a and b, resp.. Let Z = X + Y . Find
P{X = k | Z = n},
k = 0, 1, 2, . . . , n.
Since X and Y are independent, we get for k = 0, 1,2, . . . , n, that
P{X = k | Z = n}
=
P{X = k âˆ§X + Y = n}
P{Z = n}
= P{X = k âˆ§Y = n âˆ’k}
P{Z = n}
=
P{X = k} Â· P{Y = n âˆ’k}
P{Z = n}
.
Since Z = X + Y is Poisson distributed with parameter a + b, we get
P{X = k | Z = n}
=
ak
k! eâˆ’a Â·
bnâˆ’k
(n âˆ’k)! eâˆ’b
(a + b)n
n!
eâˆ’(a+b)
=
n!
k!(n âˆ’k)!
akbnâˆ’k
(a + b)n
=

n
k
 
a
a + b
k
Â·

b
a + b
nâˆ’k
,
which describes a binomial distribution B

n,
a
a + b

.
Example 3.2 Let X and Y be random variables for which
X is Poisson distributed,
X âˆˆP(a),
and
P{Y = k | X = n} =
 n
k

pk(1 âˆ’p)nâˆ’k,
k = 0, 1, . . . , n,
where n âˆˆN0 and p âˆˆ]0, 1[.
Prove that Y is Poisson distributed, Y âˆˆP(ap).
We get for k âˆˆN0,
P{Y = k}
=
âˆ

n=k
P{Y = k | X = n} Â· P{X = n} =
âˆ

n=k
 n
k

pk(1 âˆ’p)nâˆ’k Â· an
n! eâˆ’a
=
âˆ

n=k
n!
k!(n âˆ’k)!
eâˆ’a
n! pk(1 âˆ’p)nâˆ’k Â· an = eâˆ’a
k! akpk
âˆ

n=k
1
(n + k)! (1 âˆ’p)nâˆ’kanâˆ’k
=
eâˆ’a
k! akpk
âˆ

n=0
1
n! {(1 âˆ’p)}n = eâˆ’a
k! akpk
âˆ

n=0
1
n! {(1 âˆ’p)a}n
=
eâˆ’a
k! akpk Â· e(1âˆ’p)a = 1
k! (ap)keâˆ’ap,
proving that Y âˆˆP(ap).
Download free eBooks at bookboon.com

Discrete Distributions
 
27 
3. The Poisson distribution
Example 3.3 We have in Ventus: Probability c-2 introduced skewness of a distribution.
Compute the skewness Î³(X) of a random variable X, which is Poisson distributed with parameter a.
What happens to Î³(X), when a â†’âˆ?
The skewness is deï¬ned as
Î³ (Xa) = 1
Ïƒ3 E

(Xa âˆ’Î¼)3
,
where
E

(Xa âˆ’Î¼)3
= E

X3
a

âˆ’Î¼

3Ïƒ2 + Î¼2
.
We have for a Poisson distributed random variable Xa âˆˆP(a),
P {Xa = k} = ak
k! eâˆ’a,
k âˆˆN0,
a > 0,
and
E {Xa} = Î¼ = a
and
V {Xa} = Ïƒ2 = a.
It follows from
E {Xa (Xa âˆ’1) (Xa âˆ’2)} =
âˆ

k=3
k(k âˆ’1)(k âˆ’2) ak
k! eâˆ’a = a3
âˆ

k=3
akâˆ’3
(k âˆ’3)! eâˆ’a = a3,
and
X3
a
=
Xa (Xa âˆ’1) (Xa âˆ’2) + 3X2
a âˆ’2Xa
=
Xa (Xa âˆ’1) (Xa âˆ’2) + 3Xa (Xa âˆ’1) + Xa,
that
E

X3
a

= a3 + 3a2 + a,
hence
E {(Xa âˆ’Î¼)} = E

X3
a

âˆ’Î¼

3Ïƒ2 + Î¼2
=

a3 + 3a2 + a

âˆ’a

3a âˆ’a3
= a.
We get the skewness by insertion,
Î³ (Xa) = 1
Ïƒ3 E

(Xa âˆ’Î¼)3
=
a
aâˆša =
1
âˆša.
Finally, it follows that
lim
aâ†’âˆÎ³ (Xa) = lim
aâ†’âˆ
1
âˆša = 0.
Download free eBooks at bookboon.com

Discrete Distributions
 
28 
3. The Poisson distribution
Xi
ni
0
29
1
42
2
21
3
16
4
7
5
2
6
3
â‰¥7
0
120
Example 3.4 A carpark where one may park at most 1 hour, has 192 parking places. On the ï¬rst
ï¬ve days (Monday â€“ Friday) in one particular week someone has counted the number of vacant places
at the time intervals of 5 minutes between 2:00 PM and 4:00 PM, i.e. performed 24 observations per
day, thus in total 120 observations. The results of this investigation (which comes from Los Angeles)
is shown in the table. Here Xi denotes the number of vacant places and ni the number of times there
were observed Xi vacant places.
1) We assume that the number X of vacant places at any time between 2 PM and 4 PM is Poisson
distributed of the same mean as in the given results of the observations. Compute the expected
number of observations. corresponding to
X = 0,
X = 1,
X = 2,
X = 3,
X = 4,
X = 5,
X = 6,
X â‰¥7.
2) A cardriver tries once a day on each of the ï¬rst ï¬ve days of the week to park in the carpark between
2 PM and 4 PM. Find the probability that he ï¬nds a vacant place on every of these days, and ï¬nd
the probability that he ï¬nds a vacant place just one of the ï¬ve days.
1) We ï¬rst determine Î» by
Î» =
1
120 (1 Â· 42 + 2 Â· 21 + 3 Â· 16 + 4 Â· 7 + 5 Â· 2 + 6 Â· 3) = 47
30 â‰ˆ1.5667.
Using this Î» we get
P{X = k} = Î»k
k! eâˆ’Î»,
k âˆˆN0.
When these probabilities are multiplied by the total number 120, we obtain the following table:
n
0
1
2
3
4
5
6
â‰¥7
Xobserved
29
42
21
16
7
2
3
0
Xexpected
25.0
39.2
30.7
16.1
6.3
2.0
0.5
0.2
2) The probability that he ï¬nds a vacant place on one day is
P{X â‰¥1} = 1 âˆ’P{X = 0} = 1 âˆ’eâˆ’Î» = 1 âˆ’exp

âˆ’47
30

â‰ˆ0.79.
Download free eBooks at bookboon.com

Discrete Distributions
 
29 
3. The Poisson distribution
The probability that he ï¬nds a vacant place on all 5 days is
(P{X â‰¥1})5 =

1 âˆ’exp

âˆ’47
30
5
â‰ˆ0.31.
The probability that he ï¬nds a vacant place on one particular day (Monday â€“ Friday), but not on
any other of the days is
P{X â‰¥1} Â· (P{X = 0})4 =

1 âˆ’exp

âˆ’47
30

Â· exp

âˆ’4 Â· 47
30

.
The probability that he ï¬nds a vacant place on precisely one of the 5 days is
5 Â· P{X â‰¥1} Â· (P{X = 0})4 = 5

1 âˆ’exp

âˆ’47
30

Â· exp

âˆ’4 Â· 47
30

â‰ˆ0.0075.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Discrete Distributions
 
30 
3. The Poisson distribution
Example 3.5 A schoolboy tries modestly to become a newspaper delivery boy. He buys every morning
k newspapers for 8 DKK for each, and he tries to sell them on the same day for 12 DKK each. The
newspapers which are not sold on the same day are discarded, so the delivery boy may suï¬€er a loss by
buying too many newspapers.
Experience shows that the number of newspapers which can be sold each day, X, can be assumed
approximately to follow a Poisson distribution of mean 10, thus
P{X = k} = 10k
k! eâˆ’10,
k âˆˆN0.
k
k
m=0
1
m! 10meâˆ’10
0
0.00005
1
0.0005
2
0.0028
3
0.0104
4
0.0293
5
0.0671
6
0.1302
7
0.2203
6
0.3329
9
0.4580
10
0.5831
Let Ek denote his expected proï¬t per day (which possibly may be negative), when he buys k newspapers
in the morning.
1) Prove that
Ek =
k

m=0
(12m âˆ’8k) 10m
m! eâˆ’10 +
âˆ

m=k+1
4k 10m
m! eâˆ’10.
2) Prove that
Ek+1 âˆ’Ek = 4 âˆ’12
k

m=0
10m
m! eâˆ’10.
3) Compute by using the table, Ek+1 âˆ’Ek and Ek for k = 0, 1, . . . , 10 (2 decimals).
4) Find the value of k, for which his expected proï¬t Ek is largest.
1) He spends in total 8k DKK, when he buys k newspapers and earns 12 DKK for each newspaper
he sells. Thus
Ek
=
k

m=0
(12m âˆ’8k) 10m
m! eâˆ’10 +
âˆ

m=k+1
(12k âˆ’8k) 10m
m! eâˆ’10
=
k

m=0
(12m âˆ’8k) 10m
m! eâˆ’10 + 4k
âˆ

k+1
10m
m! eâˆ’10.
Download free eBooks at bookboon.com

Discrete Distributions
 
31 
3. The Poisson distribution
2) Then, using the result of (1),
Ek+1 âˆ’Ek
=
k+1

m=0
(12m âˆ’8k âˆ’8) 10m
m! eâˆ’10 + 4(k + 1)
âˆ

m=k+2
10m
m! eâˆ’10
âˆ’
k

m=0
(12m âˆ’8k) 10m
m! eâˆ’10 âˆ’4k
âˆ

m=k+1
10m
m! eâˆ’10
=
k

m=0
(12m âˆ’8k) 10m
m! eâˆ’10 âˆ’8
k

m=0
10m
m! eâˆ’10
+4(k + 1) Â· 10k+1
(k + 1)! eâˆ’10 + 4k
âˆ

m=k+2
10m
m! eâˆ’10
+4
âˆ

m=k+2
10m
m! eâˆ’10 âˆ’
k

m=0
(1.5m âˆ’k) 10m
m! eâˆ’10
âˆ’4k
âˆ

m=k+2
10m
m! eâˆ’10 âˆ’4k Â· 10k+1
(k + 1)! eâˆ’10
=
âˆ’
k

m=0
10m
m! eâˆ’10 + 4
âˆ

m=k+1
10m
m! eâˆ’10
=
4
âˆ

m=0
10m
m! eâˆ’10 âˆ’1.5
k

m=0
10m
m! eâˆ’10
=
4 âˆ’12
k

m=0
10m
m! eâˆ’10.
k
Ek+1 âˆ’Ek
Ek
0
3.9994
0.00
1
3.9940
4.00
2
3.9664
7.99
3
3.8752
11.96
4
3.6484
15.84
5
3.1948
19.48
6
2.4376
22.68
7
1.3564
25.12
8
0.0052
26.47
9
-1.4960
26.48
10
-2.9972
24.98
3) Using the formulÃ¦
Ek+1 âˆ’Ek = 4 âˆ’12
k

m=0
m

m=0
10m
m! eâˆ’10
Download free eBooks at bookboon.com

Discrete Distributions
 
32 
3. The Poisson distribution
and
Ek+1 = (Ek+1 âˆ’Ek) + Ek
we get the table on the previous page.
4) We obtain the largest expected proï¬t, 26.48 DKK, for k = 9, because
E9 âˆ’E8 > 0
and
E10 âˆ’E9 < 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Discrete Distributions
 
33 
4. The geometric distribution
4
The geometric distribution
Example 4.1 Given a random variable X of the values 1, 2, . . . of positive probabilities. Assume
furthermore,
P{X > m + n} = P{X > m} Â· P{X > n},
m, n âˆˆN.
Prove that the distribution of X is a geometric distribution.
It follows from the assumptions that
P{X > n} = P{X > 1 + Â· Â· Â· + 1} = (P{X > 1})n,
n âˆˆN.
Putting a = P{X > 1} > 0 we get
P{X > n} = an,
hence
P{X = n} = P{X > n âˆ’1} âˆ’P{X > n} = anâˆ’1 âˆ’an = (1 âˆ’a)anâˆ’1.
This shows that X is geometrically distributed with p = 1 âˆ’a and q = a, hence X âˆˆPas(1, 1 âˆ’a).
Example 4.2 Let X1, X2, . . . be independent random variables, which are following a geometric
distribution,
P {Xi = k} = p qkâˆ’1,
k âˆˆN;
i âˆˆN,
where p > 0, q > 0 and p + q = 1.
Prove by induktion that Yr = X1 + X2 + Â· Â· Â· + Xr has the distribution
P {Yr = k} =

k âˆ’1
r âˆ’1

prqkâˆ’r,
k = r, r + 1, . . . .
Find the mean and variance of Yr.
The claim is obvious for r = 1.
If r = 2 and k â‰¥2, then
P {Y2 = k}
=
kâˆ’1

â„“=1
P {X1 = â„“} Â· P {X2 = k âˆ’â„“} =
kâˆ’1

â„“=1
p qâ„“âˆ’1p qkâˆ’â„“âˆ’1
=
(k âˆ’1)p2qkâˆ’2 =
 k âˆ’1
1

p2qkâˆ’2 =
 k âˆ’1
2 âˆ’1

p2qkâˆ’2,
proving that the formula holds for r = 2.
Assume that the formula holds for some r, and consider the successor r + 1 with Yr+1 = Xr+1 + Yr
and k â‰¥r + 1. Then
P {Yr+1 = k}
=
kâˆ’r

â„“=1
P {Xr+1 = â„“} Â· P {Yr = k âˆ’â„“} =
kâˆ’r

â„“=1
p qâ„“âˆ’1
 k âˆ’â„“âˆ’1
r âˆ’1

prqkâˆ’â„“âˆ’r
=
pr+1qkâˆ’(r+1)
kâˆ’r

â„“=1
 k âˆ’â„“âˆ’1
r âˆ’1

.
Download free eBooks at bookboon.com

Discrete Distributions
 
34 
4. The geometric distribution
It follows from Example 2.2 using some convenient substitutions that
kâˆ’r

â„“=1
 k âˆ’â„“âˆ’1
r âˆ’1

=
kâˆ’2

j=râˆ’1

j
r âˆ’1

=

k âˆ’1
r

,
hence by insertion,
P {Yr+1 = k} =

k âˆ’1
r

pr+1qkâˆ’(r+1).
This is exactly the same formula, only with r replaced by r + 1.
The claim now follows by induction.
Finally,
E {Yr} = r E {X1} = r
p
and
V {Yr} = r V {X1} = rq
p2 .
Example 4.3 An (honest) dice is tossed, until we for the ï¬rst time get a six.
Find the probability pn that the ï¬rst six occurs in toss number n.
Let the random variable X denote the sum of the pips in all the tosses until and included the ï¬rst toss
in which we obtain a six. Find the mean E{X}.
Clearly,
pn = P{Y = n} = 1
6 Â·
5
6
nâˆ’1
,
n âˆˆN.
If the ï¬rst six occurs in toss number n, there are only in the ï¬rst n âˆ’1 tosses possible to obtain 1, 2,
3, 4, 5 pips, each of the probability 1
5. Hence the mean of the ï¬rst n âˆ’1 tosses is
1
5 (1 + 2 + 3 + 4 + 5) = 1
5 Â· 15 = 3.
In toss number n we get 6 pips, so the expected number of pips in n tosses under the condition that
the ï¬rs six occurs in toss number n is
(n âˆ’1) Â· 3 + 6 = 3(n + 1).
Then the mean of X is given by
E{X}
=
âˆ

n=1
3(n + 1)pn =
âˆ

n=1
3(n + 1) Â· 1
6 Â·
5
6
nâˆ’1
= 1
2
 âˆ

n=1
n
5
6
nâˆ’1
+
âˆ

n=1
5
6
nâˆ’1
=
1
2 Â·
â§
âª
âª
âª
â¨
âª
âª
âª
â©
1

1 âˆ’5
6
2 +
1
1 âˆ’5
6
â«
âª
âª
âª
â¬
âª
âª
âª
â­
= 1
2 {36 + 6} = 21.
Download free eBooks at bookboon.com

Discrete Distributions
 
35 
4. The geometric distribution
Example 4.4 A box contains N diï¬€erent slips of paper. We then draw randomly one slip from the
box and then replace it in the box. Hence all experiments are identical and independent.
Let the random variable Xr,N denote the number of draws until we have got r diï¬€erent slips of paper,
(r = 1, 2, Â· Â· Â· , N). Prove that
E {Xr,N} = N
 1
N +
1
N âˆ’1 + Â· Â· Â· +
1
N âˆ’r + 1

.
Hint: Use that
Xr,N = X1,N + (X2,N âˆ’X1,N) + Â· Â· Â· + (Xr,N âˆ’Xrâˆ’1,N) ,
and that each of these random variables is geometrically distributed.
Find E {Xr,N} in the two cases N = 10, r = 5 and N = 10, r = 10.
Prove that
N ln
N + 1
N âˆ’r + 1 < E {Xr,N} < N ln
N
N âˆ’r,
and show that for r ï¬xed and large N,
N ln
N
N âˆ’r
is a good approximation of E {Xr,N}.
â›
âœ
âAn even better approximation is N ln
N + 1
2
N âˆ’r + 1
2
â
âŸ
â .
Since Xkâˆ’1,N denotes the number of draws for obtaining kâˆ’1 diï¬€erent slips, Xk,N âˆ’Xkâˆ’1,N indicates
the number of draws which should be further applied in order to obtain a new slip. When we have
obtained k âˆ’1 diï¬€erent slips of paper, then we have in the following experiments the probability
N âˆ’(k âˆ’1)
N
= N âˆ’k + 1
N
of getting an diï¬€erent slip. This proves that Xk,N âˆ’Xkâˆ’1,N is geometrically distributed with
p = N âˆ’k + 1
N
and mean
1
p =
N
N âˆ’k + 1,
k = 1, 2, . . . , N,
where we use the convention that X0,N = 0, thus X1,N âˆ’X0,N = X1,N.
The mean is
{Xr,N}
=
E {X1,N} + E {X2,N âˆ’X1,N} + Â· Â· Â· + E {Xr,N âˆ’Xrâˆ’1,N}
=
N
 1
N +
1
N âˆ’1 +
1
N âˆ’2 + Â· Â· Â· + dfrac1N âˆ’r + 1

.
Putting for convenience
ar,N =
N

k=Nâˆ’r+1
1
k ,
Download free eBooks at bookboon.com

Discrete Distributions
 
36 
4. The geometric distribution
this is written in the following short version
E {Xr,N} = N Â· ar,N.
If N = 10 and r = 5, then
E {X5,10} = 10
 1
10 + 1
9 + 1
8 + 1
7 + 1
7

= 6, 46.
If N = 10 and r = 10, then
E {X10,10} = 10
 1
10 + 1
9 + 1
8 + Â· Â· Â· + 1

= 29, 29.
We shall ï¬nally estimate
ar,N =
N

k=Nâˆ’r+1
1
k .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Discrete Distributions
 
37 
4. The geometric distribution
The sequence
1
k

is decreasing, hence by the criterion of integral,
ar,N >
" N+1
Nâˆ’r+1
1
x dx = ln
N + 1
N âˆ’r + 1
and
ar,N <
" N
Nâˆ’r
1
x dx = ln
N
N âˆ’r,
hence
N Â· ln
N + 1
N âˆ’r + 1 < N Â· ar,N = E {Xr,N} < N Â·
N
N âˆ’r.
Remark 4.1 Note that the diï¬€erence between the upper and the lower bound can be estimated by
N

ln
N
N âˆ’r âˆ’ln
N + 1
N âˆ’r + 1

= N ln

N
N + 1 Â· N âˆ’r + 1
N âˆ’r

= N

ln

1 +
1
N âˆ’r

âˆ’ln

1 + 1
N

âˆ¼N

1
N âˆ’r âˆ’1
N

= N Â·
r
(N âˆ’r) Â· N =
r
N âˆ’r < r
N ,
which shows that if r is kept ï¬xed and N is large, then
N ln
N + 1
N âˆ’r + 1
and
N ln
N
N âˆ’r
are both reasonable approximations of E {Xr,N}. â™¦
Numerical examples
r = 5
r = 5
r = 20
N = 10
N = 20
N = 150
E {Xr,N}
6.4563
5.5902
21.3884
N ln
N + 1
N âˆ’r + 1
6.0614
5.4387
21.3124
N ln
N + 1
2
N âˆ’r + 1
2
6.4663
5.5917
21.3885
Download free eBooks at bookboon.com

Discrete Distributions
 
38 
4. The geometric distribution
Example 4.5 A box contains h white balls, r red balls and s black balls (h > 0, r > 0, s > 0). We
draw at random a ball from the box and then return it to the box. This experiment if repeated, so the
experiments are identical and independent.
Denote by X the random variable which gives the number of draws, until a white ball occurs for the
ï¬rst time, and let Y denote the number of draws which are needed before a red ball occurs for the ï¬rst
time.
1) Find the distributions of the random variables X and Y .
2) Find the means E{X} and E{Y }.
3) Find for n âˆˆN the probability that the ï¬rst white ball is drawn in experiment number n, and that
no red ball has occurred in the previous n âˆ’1 experiment, thus P{X = n âˆ§Y > n}.
4) Find P{X < Y }.
1) Since X and Y are following geometric distributions, we have
P{X = n} =
h
h + r + s

1 âˆ’
h
h + r + s
nâˆ’1
,
n âˆˆN,
P{Y = n} =
r
h + r + s

1 âˆ’
r
h + r + s
nâˆ’1
,
n âˆˆN.
2) Then,
E{X} = h + r + s
h
and
E{Y } = h + r + s
r
.
3) When the ï¬rst white ball is drawn in experiment number n, and no red ball has occurred in the
previous nâˆ’1 experiment, then all drawn balls in the ï¬rst nâˆ’1 experiments must be black. Hence
we get the probability
P{X = n âˆ§Y > n} =

s
h + r + s
nâˆ’1
Â·
h
h + r + s,
n âˆˆN.
4) First solution. It follows by using (3),
P{X < Y }
=
âˆ

n=1
P{X = n âˆ§Y > n} =
âˆ

n=1

s
h + r + s
nâˆ’1
Â·
h
h + r + s
=
h + r + s
h + r
Â·
h
h + r + s =
h
h + r.
Second solution. The number of black balls does not enter the problem when we shall ï¬nd
P{X < Y }, so we may without loss of generality put s = 0. Then by the deï¬nition of X and Y ,
P{X < Y } = P{X = 1} =
h
h + r.
In fact, if Y > X, then X must have been drawn in the ï¬rst experiment.
Download free eBooks at bookboon.com

Discrete Distributions
 
39 
4. The geometric distribution
Example 4.6 Given a roulette, where the event of each game is either â€œredâ€ or â€œblackâ€. We assume
that the games are independent.
The probability of the event â€œredâ€ in one game is p, while the
probability of the event â€œblackâ€ is q, where p > 0, q > 0, p + q = 1.
Let the random variable X denote the number of games in the ï¬rst sequence of games, where the
roulette shows the same colour as in the ï¬rst game.
1) Find for n âˆˆN the probability that X = n.
2) Find the mean of the random variable X.
3) Find the values of p, for which the mean is bigger than 8.
4) Find for p = 1
3 the variance of the random variable X.
1) We ï¬rst note that
P{X = n}
=
P{the ï¬rst n games give red and number (n + 1) gives black}
+P{the ï¬rst n games give black and number (n+1) gives red}
=
pnq + qnp = pq

pnâˆ’1 + qnâˆ’1
.
Notice that
âˆ

n=1
P{X = n} =
âˆ

n=1
(P nq + qnp) = pq

1
1 âˆ’p +
1
1 âˆ’q

= pq
1
q + 1
p

= p + q = 1.
2) We infer from
âˆ

n=1
n znâˆ’1 =
1
(1 âˆ’z)2
for |z| < 1,
that the mean is
E{X}
=
pq
# âˆ

n=1
npnâˆ’1 +
âˆ

n=1
nqnâˆ’1
$
= pq

1
(1 âˆ’p)2 +
1
(1 âˆ’q)2

= pq
 1
q2 + 1
p2

=
p
q + q
p = p2 + q2
pq
= (p + q)2 âˆ’2pq
pq
= 1
pq âˆ’2.
3) Now q = 1 âˆ’p, so
E{X} = 1
pq âˆ’2 =
1
p(1 âˆ’p) âˆ’2
is bigger than 8 for
1
p(1 âˆ’p) > 10,
hence for
p2 âˆ’p + 1
10 > 0.
Download free eBooks at bookboon.com

Discrete Distributions
 
40 
4. The geometric distribution
Since p2 âˆ’p + 1
10 = 0 for
p = 1
2 Â±
	
1
4 âˆ’1
10 = 1
2 Â±
	
25 âˆ’10
100
= 1
2 Â±
âˆš
15
10 ,
we get the two possibilities
p âˆˆ
%
0, 1
2 âˆ’
âˆš
15
10
&
eller
p âˆˆ
%
1
2 +
âˆš
15
10 , 1
&
,
or approximately,
0 < p < 0.1127
or
0.8873 < p < 1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Discrete Distributions
 
41 
4. The geometric distribution
4) In general,
V {X}
=
E

X2
âˆ’(E{X})2 = E{X(X âˆ’1)} + E{X} âˆ’(E{X})2
=
âˆ

n=2
n(n âˆ’1) {pn1 + qnp} + p
q + q
p âˆ’
p
q + q
p
2
=
p2q
âˆ

n=2
n(n âˆ’1)pnâˆ’2 + pq2
âˆ

n=2
n(n âˆ’1)qnâˆ’2 + p
q + q
p âˆ’p2
q2 âˆ’q2
p2 âˆ’2
=
p2q Â·
2
(1 âˆ’p)3 + pq2 Â·
2
(1 âˆ’q)3 + p
q + q
p âˆ’p2
q2 âˆ’q2
p2 âˆ’2
=
2 p2q
q2 + 2 pq2
+
p
q + q
p âˆ’p2
q2 âˆ’q2
p2 âˆ’2 = p2
q2 + q2
p2 + p
q + q
p âˆ’2
=
p
q
p
q + 1

+ q
p
q
p + 1

âˆ’2 = p
q2 + q
p2 âˆ’2.
Inserting p = 1
3 and q = 2
3 we get
V {X} = 1
3 Â· 9
4 + 2
3 Â· 9 âˆ’2 = 3
4 + 6 âˆ’2 = 19
4 .
Example 4.7 We perform a sequence of independent experiments.
In each of these there is the
probability p of the event A, and the probability q = 1 âˆ’p of the event B (0 < p < 1). Such a sequence
of experiments may start in the following way,
AAABBABBBBBBAAB Â· Â· Â· .
Every maximal subsequence of the same type is called a run. In the example above the ï¬rst run AAA
has the length 3, the second run BB has length 2, the third run A has length 1, the fourth run BBBBBB
has length 6, etc.
Denote by '
Xn the length of the n-th run, n âˆˆN.
1) Explain why the random variables Xn, n âˆˆN, are independent.
2) Find P {X1 = k}, k âˆˆN.
3) Find E {X1} and V {X1}.
4) Find P {X2 = k}, k âˆˆN.
5) Find E {X2} and V {X2}.
6) Prove that
E {X2} â‰¤E {X1}
and
V {X2} â‰¤V {X1} .
1) The probability of the event {Xn = k} depends only of the ï¬rst element of the n-th run, proving
that Xn, n âˆˆN, are independent.
Download free eBooks at bookboon.com

Discrete Distributions
 
42 
4. The geometric distribution
2) It follows like in Example 4.6 that (k rersults of A, resp. B)
E {X1} = P{A Â· Â· Â· AB} + P{B Â· Â· Â· BA} = pkq + qkp = pq

pkâˆ’1 + qkâˆ’1
.
3) The mean is, cf. Example 4.6,
E {X1} =
âˆ

k=1
pq Â· k

pkâˆ’1 + qkâˆ’1
= pq

1
(1 âˆ’p)2 +
1
(1 âˆ’q)2

= p
q + q
p = p2 + q2
pq
= 1
pq âˆ’2.
Furthermore,
E {X1 (X1 âˆ’1)}
=
âˆ

k=2
k(k âˆ’1)pkq +
âˆ

k=2
k(k âˆ’1)qkp = p2q
âˆ

k=2
k(k âˆ’1)pkâˆ’2 + pq2
âˆ

k=2
k(k âˆ’1)qkâˆ’2
=
p2q Â· 2
q3 + pq2 Â· 2
p3 = 2
p2
q2 + q2
p2

,
hence
V {X1}
=
E {X1 (X1 âˆ’1)} + E {X1} âˆ’(E {X1})2 = 2
p2
q2 + q2
p2

+ p
q + q
p âˆ’
p
q + q
p
2
=
p2
q2 + q2
p2 + p
q + q
p âˆ’1 = p
q2 + q
p2 âˆ’2.
4) If we have â„“copies of A and k copies of B in the ï¬rst sum, and â„“copies of B and k copies of A ib
the second sum, then
P {X2 = k}
=
âˆ

â„“=1
P{A Â· Â· Â· AB Â· Â· Â· BA} +
âˆ

â„“=1
P{B Â· Â· Â· BA Â· Â· Â· AB} =
âˆ

â„“=1
pâ„“qk Â· p +
âˆ

â„“
qâ„“pkq
=
p2qk
âˆ

â„“=1
pâ„“âˆ’1 + q2pk
âˆ

â„“=1
qâ„“âˆ’1 = p2qk
1 âˆ’p + q2pk
1 âˆ’q = p2qkâˆ’1 + q2pkâˆ’1.
5) The mean is
E {X2} =
âˆ

k=1
kp2qkâˆ’1 +
âˆ

k=1
kq2qkâˆ’1 =
p2
(1 âˆ’q)2 +
q2
(1 âˆ’p)2 = p2
p2 + q2
q2 = 2.
Furthermore,
E {X2 (X2 âˆ’1)}
=
âˆ

k=2
k(k âˆ’1)q2qkâˆ’1 +
âˆ

k=2
k(k âˆ’1)q2pkâˆ’1
=
p2q
âˆ

k=2
k(k âˆ’1)qkâˆ’2 + pq2
âˆ

k=2
k(k âˆ’1)pkâˆ’2
=
p2q Â·
2
(1 âˆ’q)3 + pq2 Â·
2
(1 âˆ’p)3 = 2p2q
p3
+ 2pq2
q3
= 2
p
q + q
p

,
Download free eBooks at bookboon.com

Discrete Distributions
 
43 
4. The geometric distribution
hence
V {X2}
=
E {X2 (X2 âˆ’1)} + E {X2} âˆ’(E {X2})2
=
2
p
q + q
p

+ 2 âˆ’4 = 2
p
q + q
p âˆ’1

.
6) Now Ï•(x) = x + 1
x has a minimum for x = 1, so if we put x = p
q , then
E {X1} = p
q + q
p = p
q + 1
p
q
â‰¥1 + 1 = 2 = E {X2} .
It follows from
V {X1} = x2 + 1
x2 + x + 1
x âˆ’2
and
V {X2} = 2x + 2
x âˆ’2,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Masterâ€™s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top masterâ€™s programmes
â€¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
â€¢ 1st place: MSc International Business
â€¢ 1st place: MSc Financial Economics
â€¢ 2nd place: MSc Management of Learning
â€¢ 2nd place: MSc Economics
â€¢ 2nd place: MSc Econometrics and Operations Research
â€¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier â€˜Beste Studiesâ€™ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Discrete Distributions
 
44 
4. The geometric distribution
that
V {X1} âˆ’V {X2}
=
x2 + 1
x2 âˆ’x âˆ’1
x = x(x âˆ’1) + 1 âˆ’x
x2
= (x âˆ’1)

x3 âˆ’1

x2
=
x âˆ’1
x
2 
x2 + x + 1

â‰¥0,
hence V {X1} â‰¥V {X2}. Equality is only obtained for x = p
q = 1, i.e. for
p = q = 1
2.
Example 4.8 We toss a sequence of tosses with an (honest) dice.
The tosses are identical and
independent. We deï¬ne a random variable X of values in N0 by
X = n, if the ï¬rst six is in toss number n + 1.
1) Find the distribution and mean of X.
2) Find for k = 0, 1, 2, . . . , n, the probability that there in n tosses are precisely k ï¬ves and no six.
3) Find the probability pn,k that the ï¬rst six occurs in toss number n + 1, and that we have had
precisely k ï¬ves, n = k, k + 1, . . . . in the ï¬rst n tosses.
4) Find for every k âˆˆN0 the probability pk that we have got exactly k ï¬ves before we get the ï¬rst six.
5) Find the expected number of ï¬ves before we get the ï¬rst six.
1) It is immediately seen that
P{X = n} =
5
6
n
Â· 1
6 =
5n
6n+1 ,
n âˆˆN0.
Hereby we get the mean
E{X} =
âˆ

n=0
n P{X = n} = 5
62
âˆ

n=1
n
5
6
nâˆ’1
= 5
62 Â·
1

1 âˆ’5
6
2 = 5.
2) If in n tosses there are k ï¬ves and no six, this is obtained by choosing k places out of n, which
can be done in
 n
k

sways. The remaining n âˆ’k tosses are then chosen among the numbers
{1, 2, 3, 4}, so
P{k ï¬ves and no six in n tosses} =

n
k
 1
6
k 4
6
nâˆ’k
,
for k = 0, 1, . . . , n.
Download free eBooks at bookboon.com

Discrete Distributions
 
45 
4. The geometric distribution
3) If n â‰¥k, then
pn,k
=
P{k ï¬ves and no six in n tosses} Ã— P{one six in toss number n + 1}
=
 n
k

Â·
1
6
k
Â·
4
6
nâˆ’k
Â· 1
6 =
 n
k
 1
6
k+1 2
3
nâˆ’k
.
4) This probability is
pk
=
âˆ

n=k
pn,k =
1
4k Â· 6
âˆ

n=k

n
k
 2
3
n
=
1
4k Â· 6
âˆ

n=k+1

n âˆ’1
k
 2
3
nâˆ’1
=
1
4k Â· 6
âˆ

n=k+1

n âˆ’1
(k + 1) âˆ’1
 2
3
nâˆ’(k+1) 1
3
k+1 2
3
k 1
3
âˆ’kâˆ’1
=
1
4k Â· 6 Â· 3 Â· 2k
âˆ

n=k+1

n âˆ’1
(k + 1) âˆ’1
 1
3
k+1 2
3
nâˆ’(k+1)
=
1
2k+1 ,
k âˆˆN0,
because the sum is the total probability that Y âˆˆPa

k + 1, 1
3

occurs, hence = 1.
An alternative computation is the following
pk
=
âˆ

n=k
pn.k =
1
6
k+1
âˆ

n=k

n
k
 2
3
nâˆ’k
=
1
6
k+1 âˆ

â„“=0

â„“+ k
k
 2
3
â„“
=
1
6
k+1 âˆ

â„“=0
 â„“+ k
â„“
 2
3
â„“
=
1
6
k+1 
1 âˆ’2
3
âˆ’(k+1)
=
1
2
k+1
,
k âˆˆN0.
Alternatively we see that every other toss than just ï¬ves or sixes are not relevent for the
question. If we neglect these tosses then a ï¬ve and a six occur with each the probability 1
2. Then
the probability of getting exactly k ï¬ves before the ï¬rst six is
1
2
k
Â· 1
2 =
1
2
k+1
,
k âˆˆN0.
5) The expected number of ï¬ves coming before the ï¬rst six is then
âˆ

k=0
k pk =
âˆ

k=1
k
2k+1 = 1
4
âˆ

k=1
k
1
2
kâˆ’1
= 1
4 Â·
1

1 âˆ’1
2
2 = 1.
Here ones usual intuition fails, because one would guess that the expected number is < 1.
Download free eBooks at bookboon.com

Discrete Distributions
 
46 
4. The geometric distribution
Example 4.9 Let X1 and X2 be independent random variables, both following a geometric distribution
P {Xi = k} = pqkâˆ’1,
k âˆˆN,
i = 1, 2.
Let Y = min {X1, X2}.
1) Find the distribution of Y .
2) Find E{Y } and V {Y }.
1) If k âˆˆN, then
P{Y = k}
=
P {X1 = k, X2 > k} + P {X1 > k, X2 = k} + P {X1 = k, X2 = k}
=
2 P {X1 = k} Â· P {X2 > k} + (P {X1 = k})2
=
2pqkâˆ’1 Â· qk + p2q2kâˆ’2 = q2kâˆ’2 
2pq + p2
=

q2kâˆ’1 Â·

(p + q)2 âˆ’q2
=

1 âˆ’q2 
q2kâˆ’1 ,
proving that Y is geometrically distributed, Y âˆˆPas

1, q2
.
Alternatively we ï¬rst compute
P{Y > k} = P {X1 > k, X2 > k} = P {X1 > k} Â· P {X2 > k} = q2k,
and use that
P{Y > k âˆ’1} = P{Y = k} + P{Y > k},
hence by a rearrangement,
P{Y = k}
=
P{Y > k âˆ’1} âˆ’P{Y > k} = q2kâˆ’2 âˆ’q2k
=

1 âˆ’q2 
q2kâˆ’1 ,
for k âˆˆN,
and it follows that Y is geometrically distributed, Y âˆˆPa

1, q2
.
2) Using a formula and replacing p by 1 âˆ’q2, and q by q2, we get
E{Y } =
1
1 âˆ’q2
and
V {Y } =
q2
(1 âˆ’q2)2 .
Download free eBooks at bookboon.com

Discrete Distributions
 
47 
4. The geometric distribution
Example 4.10 Let X1 and X2 be independent random variables, both following a geometric distri-
bution,
P {Xi = k} = p qkâˆ’1,
k âˆˆN,
i = 1, 2.
Put Z = max {X1, X2}.
1) Find P{Z = k}, k âˆˆN.
2) Find E{Z}.
1) A small consideration gives
P{Z â‰¤n} = P {max (X1, X2) â‰¤n} = P {X1 â‰¤n} Â· P {X2 â‰¤n} ,
which is also written
n

k=1
P{Z = k} =
n

â„“=1
P {X1 = â„“} Â·
n

m=1
P {X2 = m} .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Discrete Distributions
 
48 
4. The geometric distribution
Then
P{Z = b}
=
n

k=1
P{Z = k} âˆ’
nâˆ’1

k=1
P{Z = k}
=
nâˆ’1

â„“=1
P {X1 = â„“} + P {X1 = n}

Â·
nâˆ’1

m=1
P {X2 = m} + P {X2 = n}

âˆ’
nâˆ’1

â„“=1
P {X1 = â„“} Â·
nâˆ’1

m=1
P {X2 = m}
=
P {X1 = n} Â·
n

m1
P {X2 = m} + P {X2 = n} Â·
nâˆ’1

â„“=1
P {X1 = â„“}
=
pqnâˆ’1
n

k=1
pqkâˆ’1 + pqnâˆ’1
nâˆ’1

k=1
pqkâˆ’1
=
p2qnâˆ’1
n

k=1
qkâˆ’1 + p2qnâˆ’1
nâˆ’1

k=1
qkâˆ’1
=
p2qnâˆ’1 Â· 1 âˆ’qn
1 âˆ’q + p2qnâˆ’1 Â· 1 âˆ’qnâˆ’1
1 âˆ’q
=
pqnâˆ’1 
1 âˆ’qn + 1 âˆ’qnâˆ’1
= p

2qnâˆ’1 âˆ’q2nâˆ’2 âˆ’q2nâˆ’1
.
Check:
âˆ

n=1
P{Z = n}
=
2p
âˆ

n=1
qnâˆ’1 âˆ’p
âˆ

n=1

q2nâˆ’1 âˆ’pq
âˆ

n1

q2nâˆ’1
=
2p Â·
1
1 âˆ’q âˆ’p(1 + q) Â·
1
1 âˆ’q2
=
2p
p âˆ’
p(1 + q)
(1 âˆ’q)(1 + q) = 2 âˆ’1 = 1,
proving that the probabilities found here actually are summed up to 1. â™¦
2) From

n = 1âˆn znâˆ’1 =
1
(1 âˆ’z)2
for |z| < 1,
follows that
E{Z}
=
âˆ

n=1
n P{Z = n} = 2p
âˆ

n=1
n qnâˆ’1 âˆ’(1 + q)p
âˆ

n=1
n

q2nâˆ’1
=
2p Â·
1
(1 âˆ’q)2 âˆ’(1 + q)p Â·
1
(1 âˆ’q2)2 = 2p
p2 âˆ’
(1 + q)p
{(1 + q)p}2
=
2
p âˆ’
1
(1 + q)p = 2 + 2q âˆ’1
p(1 + q)
= 1 + 2q
1 âˆ’q2 .
Download free eBooks at bookboon.com

Discrete Distributions
 
49 
4. The geometric distribution
Example 4.11 Let X1 and X2 be independent random variables, both following a geometric distri-
bution,
P {Xi = k} = pqkâˆ’1,
k âˆˆN;
i = 1, 2.
Lad Y = X1 âˆ’X2.
1) Find P{Y = k}, k âˆˆZ.
2) Find E{Y } and V {Y }.
1) Since X1 and X2 are independent, we get
P{Y = k} =

i
P {X1 = k + i âˆ§X2 = i} =

i
P {X1 = k + i} Â· P {X2 = i} .
Then we must split the investigations into two cases:
a) If k â‰¥0, then
P{Y = k}
=
âˆ

i=1
pqk+iâˆ’1 Â· pqiâˆ’1 =
âˆ

i=1
p2qkq2iâˆ’2 = p2qk
âˆ

i=1

q2iâˆ’1 =
p2
1 âˆ’q2 qk
=
p
1 + q qk = 1 âˆ’q
1 + q qk.
b) If instead k < 0, then
P{Y = k}
=
âˆ

i=âˆ’k+1
pqk+iâˆ’1pqiâˆ’1 =
âˆ

i=âˆ’k+1
p2qk 
q2iâˆ’1 = p2qk Â·

q2âˆ’k
1 âˆ’q2
=
p2
1 âˆ’q2 Â· qâˆ’k =
p
1 + q qâˆ’k = 1 âˆ’q
1 + q qâˆ’k.
Alternatively if follows for k < 0 by the symmetry that
P{Y = k}
=
P {X1 âˆ’X2 = âˆ’|k|} = P {X2 âˆ’X1 = |k|}
=
p
1 + q q|k| =
p
1 + q qk
for k âˆˆN0.
Summing up,
P{Y = âˆ’k} = P{Y = k} =
p
1 + q qk
for k âˆˆN0.
2) The mean exists and is given by
E{Y } = E {X1 âˆ’X2} = E {X1} âˆ’E {X2} = 0.
Since X1 and X2 are independent, the variance is
V {Y } = V {X1 âˆ’X2} = V {X1} + V {âˆ’X2} = V {X1} + V {X2} = 2q
p2 .
Download free eBooks at bookboon.com

Discrete Distributions
 
50 
4. The geometric distribution
Example 4.12 A man has N keys, of which only one ï¬ts into one particular door. He tries the keys
one by one:
Let X be the random variable, which indicates the number of experiments until he is able to unlock
the door. Find E{X}.
If he instead of each time taking a key at random (without bothering with, if the key already has been
tested) put the tested keys aside , how many tries should he use at the average?
1) In the ï¬rst case we have a uniform distribution,
P{X = k} = 1
N
for k = 1, 2, . . . , N,
hence
E{X} = 1
N
N

k=1
k = N + 1
2
.
2) In this case the corresponding random variable is geometrically distributed with p = 1
N , hence
P{Y = k} = 1
N

1 âˆ’1
N
kâˆ’1
,
k âˆˆN,
Y âˆˆPas

1, 1 âˆ’1
N

.
Then ï¬nally, by some formula,
E{Y } = N.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Discrete Distributions
 
51 
5. The Pascal distribution
5
The Pascal distribution
Example 5.1 Let X and Y be random variables, both having values in N0. We say that the random
variable X is stochastically larger than Y , if
P{X > k} â‰¥P{Y > k}
for every k âˆˆN0.
1) Prove that if X is stochastically larger than Y , and if E{X} exists, then
E{Y } â‰¤E{X}.
2) Let X âˆˆPas (r, p1) and Y âˆˆPas (r, p2), where r âˆˆN and 0 < p1 < p2 < 1. Prove that X is
stochastically larger than Y .
1) We use that
E{X} =
âˆ

k=0
P{X > k},
and analogously for Y . Then
E{X} =
âˆ

k=0
P{X > k} â‰¥
n

k=0
P{Y > k} = E{Y }.
2) Intuitively the result is obvious, when one thinks of the waiting time distribution. This is, however,
not so easy to prove, what clearly follows from the following computations.
Obviously, P{X > k} â‰¥P{Y > k} is equivalent to
P{X â‰¤k} â‰¤P{Y â‰¤k}.
Hence, we shall prove or the Pascal distribution that the function
Ï•r,k(p) =
k

k=r
 j âˆ’1
r âˆ’1

pr(1 âˆ’p)jâˆ’r
is increasing in p âˆˆ]0, 1[ for every r âˆˆN and k â‰¥r.
Writing more traditionally x instead of p, we get
Ï•r,k(x)
=
k

j=r

j âˆ’1
r âˆ’1

xr(1 âˆ’x)jâˆ’r = xr
kâˆ’r

j=0

j + r âˆ’1
r âˆ’1

(1 âˆ’x)j
=
xr
kâˆ’r

j=0
 j + r âˆ’1
j

(1 âˆ’x)j.
We put a convenient index on X, such that we get the notation
P {Xr â‰¤k} = Ï•r,k(x),
x = p âˆˆ]0, 1[.
Download free eBooks at bookboon.com

Discrete Distributions
 
52 
5. The Pascal distribution
Then by a diï¬€erentiation,
Ï•â€²
r,k(x)
=
r
x Ï•r,k(x) âˆ’xr
kâˆ’r

j=0
j
 j + r âˆ’1
j

(1 âˆ’x)jâˆ’1
=
r
x Ï•r,k(x) âˆ’r xr
kâˆ’r

j=1

j + r âˆ’1
j âˆ’1

(1 âˆ’x)jâˆ’1
=
r
x Ï•r,k(x) âˆ’r
x xr+1
kâˆ’râˆ’1

j=0

j + r
j

(1 âˆ’x)j
=
r
x
â§
â¨
â©Ï•r,k(x) âˆ’xr+1
kâˆ’(r+1)

j=0
 j + (r + 1) âˆ’1
j

(1 âˆ’x)j
â«
â¬
â­
=
r
x {Ï•r,k(x) âˆ’Ï•r+1,k(x)}
=
r
x (P {Xr â‰¤k} âˆ’P {Xr+1 â‰¤k}) .
The event {Xr+1 â‰¤k} corresponds to that success number (r+1) comes at the latest in experiment
number k. The probability is â‰¤the probability that success number r occurs at the latest in
experiment number k, thus
Ï•â€²
r,k(x) = r
x (P {Xr â‰¤k} âˆ’P {Xr+1 â‰¤k}) â‰¥0,
x âˆˆ]0, 1[,
so Ï•r,k(x) is increasing in x. Therefore, if X âˆˆPas(r, p1) and Y âˆˆPas(r, p2), where r âˆˆN and
0 < p1 < p2 < 1, then
P{X â‰¤k} â‰¤P{Y â‰¤k}
for k â‰¥r.
This is equivalent to the fact that X is stochastically larger than Y .
Alternatively, the result can be proved by induktion after r. If r = 1, then
P{X > k} = (1 âˆ’p1)k > (1 âˆ’p2)k = P{Y > k},
and the result is proved for r = 1.
When r > 1, then we write
X =
r

i=1
Xi
and
Y =
r

i=1
Yi,
where
Xi âˆˆPas (1, p2)
and
Yi âˆˆPas (1, p2) ,
and where the Xi-s (resp. the Yi-s) are mutually independent.
The condition
P{X > k} â‰¥P{Y > k},
for every k âˆˆN0,
Download free eBooks at bookboon.com

Discrete Distributions
 
53 
5. The Pascal distribution
does only concern the distributions of X, resp. Y . Therefore, we can assume that X and Y are
independent, and that all the Xi-s and Yj-s above also are independenty.
We shall prove the following lemma:
If X1, X2, Y1, Y2 are independent random variables with values in N0, where X1 is stochas-
tically larger than Y1, and X2 is stochastically larger than Y2, then X1 +X2 is stochastically
larger that Y1 + Y2.
Proof. If k âˆˆN0, then
P {X1 + X2 > k}
=
âˆ

i=0
P {X2 = i âˆ§X1 > k âˆ’i}
=
âˆ

i=0
P {X2 = i} Â· P {X1 > k âˆ’i}
â‰¥
âˆ

i=0
P {X2 = i} Â· P {Y1 > k âˆ’i}
=
P {Y1 + X2 > k} =
âˆ

i=0
P {Y1 = i} Â· P {X2 > k âˆ’i}
â‰¥
âˆ

i=0
P {Y1 = i} Â· P {Y2 > k âˆ’i} = P {Y1 + Y2 > k} ,
and the claim is proved. â–¡
Then write
X =
#râˆ’1

i=1
Xi
$
+ Xr
and
Y =
#râˆ’1

i=1
Xi
$
+ Yr.
It follows form the assumption of induction that râˆ’1
i=1 Xi is stochastically larger than râˆ’1
i=1 Yi.
Since Xr also is stochastically larger than Yr, it follows from the result above that X is stochasti-
cally larger than Y .
Download free eBooks at bookboon.com

Discrete Distributions
 
54 
6. The negative binomial distribution
6
The negative binomial distribution
Example 6.1 A random variable X has its distribution given by
P{X = k} =
 âˆ’Îº
k

pÎº(âˆ’q)k,
k âˆˆN0,
where p > 0, q > 0, p + q = 1 and Îº > 0, thus X âˆˆNB(Îº, p).
Find the mean and variance of X.
First note that
(âˆ’1)n

âˆ’Îº
n

=

n + Îº âˆ’1
n

,
hence the mean is given by
E{X}
=
âˆ

n=1
n

n + Îº âˆ’1
n

pÎºqn =
âˆ

n=1
Îº

n + Îº âˆ’1
n âˆ’1

pÎºqn = n
âˆ

n=0

n + Îº
n

qÎºqn+1
=
Îºq
p
âˆ

n=0

n + {Îº + 1} âˆ’1
n

pÎº+1qn = Îºq
p .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planetâ€™s 
electricity needs. Already today, SKFâ€™s innovative know-
how is crucial to running a large proportion of the 
worldâ€™s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Discrete Distributions
 
55 
6. The negative binomial distribution
Furthermore,
E{X(X âˆ’1)}
=
âˆ

n=2
n(n âˆ’1)
 n + Îº âˆ’1
n

pÎºqn =
âˆ

n=2
Îº(Îº + 1)
 n + Îº âˆ’1
n âˆ’2

pÎºqn
=
Îº(Îº + 1)
âˆ

n=0
 n + {Îº + 2} âˆ’1
n

pÎºqn+2
=
Îº(Îº + 1)
âˆ

n=0

n + {Îº + 2} âˆ’1
n

pÎº+2qn = Îº(Îº + 1) Â· q2
p2 ,
whence
V {X}
=
E{X(X âˆ’1)} + E{X} âˆ’(E{X})2 = Îº(Îº + 1) q2
p2 + Îº q
p âˆ’Îº2 q2
p2
=
Îº
q2
p2 + q
p

= Îºq
p2 (q + p) = Îºq
p2 .
Download free eBooks at bookboon.com

Discrete Distributions
 
56 
7. The hypergeometric distribution
7
The hypergeometric distribution
Example 7.1 Banachâ€™s match stick problem. A person B has the habit of using two boxes of
matches at the same time. We assume that a matchbox contains 50 matches. When B shall apply a
match, he chooses at random one of the two matchboxes without noticing afterwards if it is empty. Let
X denote the number of matches in one of the boxes, when we discover that the other one is empty,
and let Y denote the number of matches in the ï¬rst box, when the second one is emptied.
It can be proved that
ar = P{X = r} =

100 âˆ’r
50
 1
2
100âˆ’r
,
r = 0, 1, . . . , 50.
Find analogously
br = P{Y = r},
r = 1, 2, . . . , 50.
For completeness we give the proof of the result on ar.
We compute P{X = 50 âˆ’k}, thus we shall use 50 matches form one of the boxes and k matches from
the other one, and then in choice number 50 + k + 1 = 51 + k choose the empty box.
In the ï¬rst 50 + k choices we shall choose the box which is emptied in total 50 times, corresponding
to the probability
 50 + k
50
 1
2
50 1
2
k
=
 50 + k
k
 1
2
50+k
.
In the next choice we shall choose the empty box, which can be done with the probability 1
2. Finally,
we must choose between 2 boxes, so we must multiply by 2. Summing up,
P{X = 50 âˆ’k} =
 50 + k
k
 1
2
50+k
,
k = 0, 1, . . . , 50.
Then by the substitution r = 50 âˆ’k,
ar = P{X = r} =
 100 âˆ’r
50
 1
2
100âˆ’r
,
r = 0, 1, . . . , 50.
In order to ï¬nd br we shall compute P{Y = 50 âˆ’k}, i.e. we shall use 49 matches from one of the
boxes and k matches from the other one, and then choose the box, in which there is only 1 match
left. Analogously to the above we get
P{Y = 50 âˆ’k} =
 49 + k
k
 1
2
49+k
,
k = 0, 1, . . . , 50.
Then by the substitution r = 50 âˆ’k,
br = P{Y = r} =
 99 âˆ’r
49
 1
2
99âˆ’r
,
r = 0, 1, . . . , 50.
Download free eBooks at bookboon.com

Discrete Distributions
 
57 
7. The hypergeometric distribution
Example 7.2 . (Continuation of Example 7.1).
1) Find an explicit expression for the mean Î¼ of the random variable X of Example 7.1.
2) Find, by using the result of Example 2.3, an approximate expression of the mean Î¼.
Hint to question 1: Start by reducing the expression
50 âˆ’Î¼ =
50

r=0
(50 âˆ’r)ar.
It follows from Example 7.1 that
ar = P{X = r} =
 100 âˆ’r
50
 1
2
100âˆ’r
,
r = 0, 1, . . . , 50.
The text of the example is confusing for several reasons:
(a) On the pocket calculator TI-92 the mean is easily computed by using the command

(r â‹†nCr(100 âˆ’r, 50) â‹†2Ë†(r âˆ’100), r, 0, 50),
corresponding to
50

r=0
r

100 âˆ’r
50
 1
2
100âˆ’r
â‰ˆ7.03851.
The pocket calculator is in fact very fast in this case.
(b) The hint looks very natural. The result, however, does not look like any expression containing

2n
n

, which one uses in the approximation in Example 2.3. I have tried several variants, of
which the following is the best one:
1) By using the hint we get
50 âˆ’Î¼ =
50

r=0
(50 âˆ’r)ar =
49

r=0
(50 âˆ’r)

100 âˆ’r
50
 1
2
100âˆ’r
.
Then by Example 2.2,
 n + 1
r + 1

=
n

k=r
 k
r

,
r, n âˆˆN0,
r â‰¤n.
When we insert this result, we get
(50 âˆ’r)
 100 âˆ’r
50

=
(50 âˆ’r) (100 âˆ’r)!
50!(50 âˆ’r)! = 51 Â· (100 âˆ’r)!
51!(49 âˆ’r)! = 51
 100 âˆ’r
51

=
51
99âˆ’r

k=50

k
50

= 51
49âˆ’r

j=0

50 + j
50

= 51
49âˆ’r

j=0

50 + j
j

.
Download free eBooks at bookboon.com

Discrete Distributions
 
58 
7. The hypergeometric distribution
Then continue with the following
50 âˆ’Î¼
=
49

r=0
(50 âˆ’r)

100 âˆ’r
50
 1
2
100âˆ’r
= 51
49

r=0

100 âˆ’r
51
 1
2
100âˆ’r
=
51
49

r=0
49âˆ’r

j=0

50 + j
50
 1
2
100âˆ’r
= 51
49

j=0

50 + j
50
 49âˆ’j

r=0
1
2
100âˆ’r
=
51
49

j=0

50 + j
50
 1
2
51+j
+ Â· Â· Â· +
1
2
100
=
51
49

j=0
 50 + j
50
 1
2
50+j
âˆ’51 Â·
1
2
100 49

j=0
 50 + j
50

=
51
49

j=0
 50 + j
50
 1
2
50+j
âˆ’51 Â·
1
2
100
99

k=50
 k
50

=
51
50

r=1
 100 âˆ’r
50
 1
2
100âˆ’r
âˆ’51
1
2
100  99 + 1
50 + 1

(according to Example 2.2)
=
51
 50

r=0
 100 âˆ’r
50
 1
2
100âˆ’r
âˆ’
 100
50
 1
2
100
âˆ’51
 100
51
 1
2
100
=
51 âˆ’51
 100
50

+
 100
51
 1
2
100
= 51 âˆ’51
 100
50

+ 50
51
 100
50
 1
2
100
,
hence by a rearrangement,
Î¼ = 101

100
50
 1
2
100
âˆ’1.
2) It follows from Example 2.3 that

2n
n

âˆ¼22n
âˆšÏ€n,
i.e.

100
50

âˆ¼2100
âˆš
50Ï€
for n = 50,
hence
Î¼ = 101

100
50
 1
2
100
âˆ’1 âˆ¼
101
âˆš
50Ï€ âˆ’1 â‰ˆ7.05863.
Additional remark. If we apply the more precise estimate from Example 2.3,
	
100
101 Â· 2100
âˆš
50Ï€ <

100
50

< 2100
âˆš
50Ï€ ,
then it follows by this method that
7.01864 < Î¼ < 7.05863.
It was mentioned in the beginning of this example that a direct application of the pocket calculator
gives
Î¼ â‰ˆ7.03851,
which is very close to the mean value of the upper and lower bound above.
Download free eBooks at bookboon.com

Discrete Distributions
 
59 
7. The hypergeometric distribution
Example 7.3 A box contains 2N balls, of which 2h are white, 0 < h < N.
Another box contains 3N balls, of which 3h are white.
If we select two balls from each of the boxes, for which box do we have the largest probability of getting
2 white balls?
Which box has the largest probability of obtaining at least one white ball?
1) Traditionally the other balls are black, so N = h + s.
We are dealing with hypergeometric distributions, so
p2 = P {2 white balls from U2} =

2h
2
 
2s
0

 2N
2

=

2h
2

 2N
2
 = 2h(2h âˆ’1)
2N(2N âˆ’1) = h
N Â· 2h âˆ’1
2N âˆ’1,
and analogously,
p3 = P {2 white balls from U3} =
 3h
2


3N
2
 = 3h(3h âˆ’1)
3N(3N âˆ’1) = h
N Â· 3h âˆ’1
3N âˆ’1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Discrete Distributions
 
60 
7. The hypergeometric distribution
It follows from
3h âˆ’1
3N âˆ’1 âˆ’2h âˆ’1
2N âˆ’1
=
1
(3N âˆ’1)(2N âˆ’1) {(6hN âˆ’3h âˆ’2N + 1) âˆ’(6hN âˆ’2h âˆ’3N + 1)}
=
N âˆ’h
(3N âˆ’1)(2N âˆ’1) > 0,
that p2 < p3.
2) By interchanging h and s we get analogously that we have the largest probability of obtaining 2
black balls from U3.
Since ({at least one white ball} is the complementary event of {two black balls}, we have the
largest probability of obtaining at least one white ball from U2.
Example 7.4 A box contains 10 white and 5 black balls. We select without replacement 4 balls. Let
the random variable X denote the number of white balls among the 4 selected balls, and let Y denote
the number the number of black balls among the 4 selected balls.
1) Compute P {X = i}, i = 0, 1, 2, 3, 4, and P{Y = i}, i = 0, 1, 2, 3, 4.
2) Find the means E {X} and E {Y }.
3) Compute the variances V {X} and V {Y }.
4) Compute the correlation coeï¬ƒcient between X and Y .
1) This is an hypergeometric distribution with a = 10, b = 5 and n = 4, thus
P{X = i} =
 10
i
 
5
4 âˆ’i


15
4

and
P{Y = i} =
 5
i
 
10
4 âˆ’i


15
4

.
By some computations,
P{X = 0} = P{Y = 4} =

10
0
 
5
4

 15
4

= 1 Â· 5
1365 =
1
273 â‰ˆ0, 0037,
P{X = 1} = P{Y = 3} =

10
1
 
5
3

1365
= 10 Â· 5Â·4
1Â·2
1365
= 20
273 â‰ˆ0, 0733,
P{X = 2} = P{Y = 2} =

10
2
 
5
2

1365
=
10Â·9
1Â·2 Â· 5Â·4
1Â·2
1365
= 90
273 â‰ˆ0, 3297,
P{X = 3} = P{Y = 1} =
 10
3
  5
1

1365
=
10Â·9Â·8
1
Â· 2Â·3 Â· 5
1365
= 120
273 â‰ˆ0, 4396,
Download free eBooks at bookboon.com

Discrete Distributions
 
61 
7. The hypergeometric distribution
P{X = 4} = P{Y = 0} =

10
4
 
5
0

1365
=
10Â·9Â·8Â·7
1Â·2Â·3Â·4
1365
= 42
273 â‰ˆ0, 1538,
where of course
90
273 = 30
91,
120
273 = 40
91
and
42
273 = 2
13.
2) The means are
E{X} = n Â·
a
a + b = 4 Â·
10
10 + 5 = 8
3,
and
E{Y } = n Â·
b
a + b = 4 Â·
5
10 + 5 = 4
3
(= 4 âˆ’E{X}).
3) The variance is
V {X} = V {Y } =
nab(a + b âˆ’n)
(a + b)2(a + b âˆ’1) = 4 Â· 10 Â· 5 Â· (10 + 5 âˆ’4)
(10 + 5)2(10 + 5 âˆ’1) = 200 Â· 11
225 Â· 14 = 44
63.
4) It follows from Y = 4 âˆ’X that
Cov(X, Y )
=
E{XY } âˆ’E{X}E{Y } = E{X(4 âˆ’X)} âˆ’E{X}E{4 âˆ’X}
=
4 E{X} âˆ’E

X2
âˆ’4 E{X} + (E{X})2 = âˆ’V {X},
hence
Ï±(X, Y ) =
Cov(X, Y )

V {X} V {Y }
= âˆ’V {X}
V {X} = âˆ’1.
Example 7.5 A collection of 100 rockets contains 10 duds. A customer buys 20 of the rockets.
1) Find the probability that exactly 2 of the 20 rockets are duds.
2) Find the probability that none of the 20 rockets is a dud.
3) What is the expected number of duds among the 20 rockets?
This is an hypergeometric distribution with a = 10 (the duds) and b = 90 and n = 20. Let X denote
the number of duds. Then
(1) P{X = i} =
 10
i
 
90
20 âˆ’i

 100
20

,
i = 0, 1, 2, . . . , 10.
Download free eBooks at bookboon.com

Discrete Distributions
 
62 
7. The hypergeometric distribution
1) When i = 2, it follows from (1) that
P{X = 2}
=

10
2
 
90
18

 100
20

= 10!
2!8! Â·
90!
18!72! Â· 20!80!
100! = 90!
100! Â· 80!
72! Â· 20!
18! Â· 10!
8! Â· 1
2!
=
80 Â· 79 Â· 78 Â· 77 Â· 76 Â· 75 Â· 74 Â· 73
100 Â· 99 Â· 98 Â· 97 Â· 96 Â· 95 Â· 94 Â· 93 Â· 92 Â· 91 Â· 20 Â· 19 Â· 10 Â· 9
2
= 101355025
318555566 â‰ˆ0, 3182.
2) When i = 0, it follows from (1) that
P{X = 0}
=
 10
0
  90
20


100
20

=
90!
70!20! Â· 80!20!
100! = 90!
100! Â· 80!
70!
=
80 Â· 79 Â· 78 Â· 77 Â· 76 Â· 75 Â· 74 Â· 73 Â· 72 Â· 71
100 Â· 99 Â· 98 Â· 97 Â· 96 Â· 95 Â· 94 Â· 93 Â· 92 Â· 91 = 15149909
159277783 â‰ˆ0, 0951.
3) The expected number of duds is the mean
E{X} = n Â·
a
a + b = 20 Â· 10
100 = 2.
Example 7.6 A box U1 contains 2 white and 4 black balls. Another box U2 contains 3 white and 3
black balls, and a third box U3 contains 4 white and 2 b black balls.
An experiment is described by selecting randomly a sample consisting of three balls from each of the
three boxes. The numbers of the white balls in each of these samples are random variables, which we
denote by X1, X2 and X3.
1. Compute E {X1}, E {X2}, E {X3} and E {X1 + X2 + X3} and V {X1 + X2 + X3}.
2. Find the probability P {X1 = X2 = X3}.
Then collect all 18 balls in one box, from which we take a sample consisting of 9 balls. Then the
number of white balls in the sample is a random variable, which is denoted by Y .
Find E{Y } and V {Y }.
The boxes U1, U2 and U3 are represented by
U1 = {h, h, s, s, s, s},
U2 = {h, h, h, s, s, s},
U3 = {h, h, h, h, s, s}.
Download free eBooks at bookboon.com

Discrete Distributions
 
63 
7. The hypergeometric distribution
1) We have in all three cases hypergeometric distributions. Hence,
P {X1 = i}
=

2
i
 
4
3 âˆ’i

 6
3

,
i = 1, 0, 1, 2,
P {X2 = i}
=

3
i
 
3
3 âˆ’i

 6
3

,
i = 1, 0, 1, 2, 3,
P {X3 = i}
=

4
i
 
2
3 âˆ’i


6
3

,
i = 1, 0, 1, 2, 3.
The means and the variances can now be found by formulÃ¦ in any textbook. However, we shall
here compute all probabilities in the â€œhard wayâ€, because we shall need them later on:
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Discrete Distributions
 
64 
7. The hypergeometric distribution
a) When we draw from U1 we get
P {X1 = 0}
=
 3
0

Â· 4
6 Â· 3
5 Â· 2
4 = 1
5,
P {X1 = 1}
=

3
1

Â· 2
6 Â· 4
5 Â· 3
4 = 3
5,
P {X1 = 2}
=
 3
2

Â· 2
6 Â· 1
5 Â· 4
4 = 1
5,
P {X1 = 3}
=
0,
where
E {X1} = 1 Â· 3
5 + 2 Â· 1
5 = 1
and
E

X2
1

= 1 Â· 3
5 + 4 Â· 1
5 = 7
5,
hence
V {X1} = 7
5 âˆ’12 = 2
5.
b) When we draw from U2 we get analogously
P {X2 = 0}
=
 3
0

Â· 3
6 Â· 2
5 Â· 1
4 = 1
20,
P {X2 = 1}
=

3
1

Â· 3
6 Â· 3
5 Â· 2
4 = 9
20,
P {X2 = 2}
=
 3
2

Â· 3
6 Â· 2
5 Â· 3
4 = 9
20,
P {X2 = 3}
=

3
3

Â· 3
6 Â· 2
5 Â· 1
4 = 1
20,
hence
E {X2}
=
1 Â· 9
20 + 2 Â· 9
20 + 3 Â· 1
20 = 9 + 18 + 3
20
= 3
2,
E

X2
2

=
1 Â· 9
20 + 4 Â· 9
20 + 9 Â· 1
20 = 9 + 36 + 9
20
= 54
20 = 27
10,
V {X2}
=
54
20 âˆ’9
4 = 54 âˆ’45
20
= 9
20.
c) When we draw from U3 we get complementary to the draw from U1 that
P {X3 = 0}
=
0,
P {X3 = 1}
=
1
5,
P {X3 = 2}
=
3
5,
P {X3 = 3}
=
1
5.
Hence we get (there are here several variants)
E {X3} = 1 Â· 1
5 + 2 Â· 3
5 + 3 Â· 1
5 = 1 + 6 + 3
5
= 2 = 3 âˆ’E {X1} ,
Download free eBooks at bookboon.com

Discrete Distributions
 
65 
7. The hypergeometric distribution
and
V {X3} = V {X1} = 2
5.
Then
E {X1 + X2 + X3} = E {X1} + E {X2} + E {X3} = 1 + 3
2 + 2 = 9
2.
Since X1, X2 and X3 are stochastically independent, we get
V {X1 + X2 + X3} = V {X1} + V {X2} + V {X3} = 2
5 + 9
20 + 2
5 = 8 + 9 + 8
20
= 25
20 = 5
4.
2) It follows that
P {X1 = X2 = X3}
=
3

k=0
P {X1 = k} Â· P {X2 = k} Â· P {X3 = k}
=
1
5 Â· 1
20 Â· 0 + 3
5 Â· 9
20 Â· 1
5 + 1
5 Â· 9
20 Â· 3
5 + 0 Â· 1
20 Â· 1
5 = 2 Â· 3
5 Â· 9
20 Â· 1
5 = 27
250.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENTâ€¦
     RUN FASTER.
          RUN LONGER..
                RUN EASIERâ€¦
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Discrete Distributions
 
66 
7. The hypergeometric distribution
3) We have in this case the hypergeometric distribution
P{Y = k} =

9
k
 
9
9 âˆ’k


18
9

=

9
k
2

18
9
,
i = 0, 1, . . . , 9
with a = b = n = 9, hence
E{Y } =
na
a + b = 9 Â· 9
9 + 9 = 9
2,
and
V {Y } =
nab(a + b âˆ’n)
(a + b)2(a + b âˆ’1) = 9 Â· 9 Â· 9 Â· 9
182 Â· 17
=
81
4 Â· 17 = 81
68.
Example 7.7 Given a sequence of random variables (Xn), for which
P {Xn = k} =
 an
k
 
bn
m âˆ’k

 an + bn
m

,
max (0, m âˆ’bn) â‰¤k â‰¤min (an, m) ,
where m, an, bn âˆˆN, and where an â†’âˆand bn â†’âˆin such a way that
an
an + bn
â†’p,
p âˆˆ]0, 1[.
Prove that the sequence (Xn) converges in distribution towards a random variable X, which is bino-
mially distributed, X âˆˆB(m, p).
Give an intuitiv interpretation of this result.
When n is suï¬ƒciently large, then an â‰¥m and bn â‰¥m. We choose n so big that this is the case. Then
for 0 â‰¤k â‰¤m,
P {Xn =k} =
an!
k! (anâˆ’k)! Â·
bn!
(mâˆ’k)! Â·
m!
(bnâˆ’m+k)! Â· (an+bnâˆ’m)!
(an+bn)!
=

m
k

Â· an (anâˆ’1)Â· Â· Â·(anâˆ’k+1) Â· bn (bnâˆ’1)Â· Â· Â·(bnâˆ’m+k+1)
(an+bn) (an+bnâˆ’1) Â· Â· Â· (an+bnâˆ’m+1)
=

m
k

Â·
an
an+bn
Â·
anâˆ’1
an+bnâˆ’1 Â· Â· Â·
anâˆ’k+1
an+bnâˆ’k+1 Â·
bn
an+bnâˆ’k Â· Â· Â· bnâˆ’m+k+1
an+bnâˆ’m+1
â†’
 m
k

pk(1 âˆ’p)mâˆ’k
for k = 0, 1, . . . , m
nËšar n â†’âˆ.
Remark 7.1 If we have a large number of white balls an, and a large number of black balls, then it is
almost unimportant if we draw with replacement (binomial) or without replacement (hypergeometric).
Download free eBooks at bookboon.com

Discrete Distributions
 
67 
7. The hypergeometric distribution
An alternative proof, in which we apply Stirlingâ€™s formula, is the following. Let
P{X = k} =
 m
k

pk(1 âˆ’p)mâˆ’k,
k = 0, 1, . . . , m,
X âˆˆB(m, p).
Choose N, such that bn â‰¥m and an â‰¥m for all n â‰¥N. Then it follows from Stirlingâ€™s formula for
n â‰¥N,
P{X = k} âˆ’P {Xn = k} =
 m
k

pk(1 âˆ’p)mâˆ’k âˆ’

an
k
 
bn
m âˆ’k


an + bn
m

=

m
k

pk(1 âˆ’p)mâˆ’k âˆ’
an!
k! (an âˆ’k)! Â·
bn!
(m âˆ’k)! (bn âˆ’m + k)! Â· m! (an + bn âˆ’m)!
(an + bn)!
=

m
k

pk(1 âˆ’p)mâˆ’k âˆ’
m!
k!(m âˆ’k)! Â·
(an + bn âˆ’m)!an!bn!
(bn âˆ’m + k)! (an âˆ’k)! (an + bn)!
=
 m
k

pk(1 âˆ’p)mâˆ’k âˆ’

m
k

Â·
(an + bn âˆ’m)!
(bn âˆ’m + k)! (an âˆ’k)! Â·
an!bn!
(an + bn)!,
where
(an+bnâˆ’m)! âˆ¼

2Ï€(an+bnâˆ’m) Â· (an +bnâˆ’m)an+bnâˆ’m exp(âˆ’(an+bnâˆ’m)),
(bnâˆ’m+k)! âˆ¼

2Ï€(bnâˆ’m! +k) Â· (bn âˆ’m+k)bnâˆ’m+k exp(âˆ’(bnâˆ’m+k)),
(an âˆ’k)! âˆ¼

2Ï€(an âˆ’k) Â· (an âˆ’k)anâˆ’k exp(âˆ’(an âˆ’k)),
an! âˆ¼
âˆš
2Ï€ Â· aan
n Â· exp (âˆ’an) ,
bn! âˆ¼
âˆš
2Ï€ Â· bbn
n Â· exp (âˆ’bn) ,
(an + bn)! âˆ¼

2Ï€(an + bn) Â· (an + bn)an+bn Â· exp (âˆ’(an + bn)) .
Since the exponentials disappear by insertion, we get the following
(an + bn âˆ’m)!an!bn!
(bn âˆ’m + k)!(an âˆ’k)!(an + bn)!
âˆ¼

2Ï€(an + bn âˆ’m) Â· 2Ï€an Â· 2Ï€bn
2Ï€(bn âˆ’m + k) Â· 2Ï€(an âˆ’k) Â· 2Ï€(an + bn) Ã—
Ã—
(an + bn âˆ’m)an+bnâˆ’maan
n bbn
n
(bn âˆ’m + k)bnâˆ’m+k(an âˆ’k)anâˆ’k(an + bn)an+bn
=
	
an + bn âˆ’m
an + bn
Â·
an
an âˆ’k Â·
bn
nn âˆ’m + k Ã—
Ã—
an+bnâˆ’m
an + bn
an+bn
ak
nbmâˆ’k
n
(an+bnâˆ’m)m

an
anâˆ’k
anâˆ’k 
bn
bnâˆ’m+k
bnâˆ’m+k
â†’1 Â· lim
nâ†’âˆ

1âˆ’
m
an+bn
an+bn
Â· lim
nâ†’âˆ

1+
k
anâˆ’k
anâˆ’k
Ã—
Ã— lim
nâ†’âˆ

1+
m âˆ’k
bnâˆ’m+k
bnâˆ’m0k
Â· lim
nâ†’âˆ

an
an+bnâˆ’m
k 
bn
an+bnâˆ’m
mâˆ’k
= 1 Â· eâˆ’m Â· ek Â· emâˆ’k Â· pk(1 âˆ’p)m âˆ’k = pk(1 âˆ’p)mâˆ’k.
Download free eBooks at bookboon.com

Discrete Distributions
 
68 
7. The hypergeometric distribution
Finally, we get by insertion
lim
nâ†’âˆ(P{X = k} âˆ’P {Xn = k}) = 0,
and we have proved that Xn â†’X âˆˆB(m, p) in distribution.
Example 7.8 A deck of cards consists of the 13 diamonds. The court cards are the 3 cards jack,
queen and king. We let in this example ace have the value 1.
We take a sample of 4 cards.
Let X denote the random variable, which indicates the number of cards which are not court cards
among the 4 selected cards.
1) Compute the probabilities
P{X = i},
i = 1, 2, 3, 4.
2) Find the mean E{X}.
3) We now assume that among the chosen 4 cards are precisely 2 cards which are not court cards,
thus X = 2.
What is the expected sum (the mean) of these 2 cards which are not court cards?
1) Here X is hypergeometrically distributed with a = 10, b = 3, n = 4, so we get
P{X = i} =
 10
i
 
3
4 âˆ’i


13
4

=
1
715
 10
i
 
3
4 âˆ’i

,
i = 1, 2, 3, 4.
Hence,
P{X = 1}
=
10
715 =
2
143,
P{X = 2}
=
135
715 = 27
143,
P{X = 3}
=
360
715 = 72
143,
P{X = 4}
=
210
715 = 42
143.
2) Then by a convenient formula the mean is
E{X} =
na
a + b = 4 Â· 10
13
= 40
13.
Alternatively we get by a direct computation,
E{X} =
1
143 (1 Â· 2 + 2 Â· 27 + 3 Â· 72 + 4 Â· 42) = 440
143 = 40
13.
Download free eBooks at bookboon.com

Discrete Distributions
 
69 
7. The hypergeometric distribution
3) Given that we have two cards which are not court cards. The ï¬rst one can have the values
1, 2, 3, . . . , 10, and the expected value is
1
10 {1 + 2 + 3 + Â· + 10} = 11
2 .
The second card has also the expected value 11
2 , hence the expected sum is
11
2 + 11
2 = 11.
Alternatively and more diï¬ƒcult we write down all 90 possibilities of the same probability, where
we are aware of the order of the two cards:
1 + 2,
2 + 1,
3 + 1,
Â· Â· Â· ,
10 + 1,
1 + 3,
2 + 3,
3 + 2,
Â· Â· Â· ,
10 + 2,
1 + 4,
2 + 4,
3 + 4,
Â· Â· Â· ,
10 + 3,
...
...
...
...
1 + 10,
2 + 10,
3 + 10,
Â· Â· Â· ,
10 + 9.
The total sum is
18 Â· (1 + 2 + 3 + Â· Â· Â· + 10) = 18 Â· 55 = 990.
Then the average is
990
90 = 11.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Discrete Distributions
 
70 
7. The hypergeometric distribution
Example 7.9 We shall from a group G1 consisting of 2 girls and 5 boys choose a committee with
3 members, and from another group G2 of 3 girls and 4 boys we shall also select a committee of 3
members.Let X1 denote the number of girls in the ï¬rst committee, and X2 the number of girls in the
second committee.
1. Find the means E {X1} and E {X2}.
2. Find P {X1 = X2}.
The two groups G1 and G2 are then joined into one group G of 14 members. We shall from this group
choose a committee of 6 members. Let Y denote the number of girls in this committee.
3. Find the mean E{Y } and the variance V {Y }.
We are again considering hypergeometric distributions.
1) Since G1 consists of 2 girls and 5 boys, of which we shall choose 3 members, we get
a = 2,
b = 5,
a + b = 7
and
n = 3,
hence
E {X1} =
na
a + b = 3 Â· 1
7
= 6
7.
Since G2 consists of 3 girls and 4 boys, of which we shall choose 3 members, we get
a = 3,
b = 4,
a + b = 7
and
n = 3,
hence
E {X2} =
na
a + b = 3 Â· 3
y
= 9
7.
2) It follows from
P {X1 = k} =

2
k
 
5
3 âˆ’k


7
3

= 1
35
 2
k
 
5
3 âˆ’k

for k = 0, 1, 2,
and
P {X2 = k} =
 3
k
 
4
3 âˆ’k

 7
3

= 1
35

3
k
 
4
3 âˆ’k

for k = 0, 1, 2, 3,
and P {X1 = 3} = 0 that
P {X1 = X2} = P {X1 = 0}Â·P {X2 = 0}+P {X1 = 1}Â·P {X2 = 1}+P {X1 = 2}Â·P {X2 = 2} .
Here
P {X1 = 0} Â· P {X2 = 0} =
 2
0
  5
3

35
Â·
 3
0
  4
3

35
= 10 Â· 4
352
= 40
352 ,
Download free eBooks at bookboon.com

Discrete Distributions
 
71 
7. The hypergeometric distribution
P {X1 = 1} Â· P {X2 = 1} =

2
1
 
5
2

35
Â·

3
1
 
4
2

35
= 2 Â· 10 Â· 3 Â· 6
352
= 360
352 ,
P {X1 = 2} Â· P {X2 = 2} =
 2
2
  5
1

35
Â·
 3
2
  4
1

35
= 5 Â· 3 Â· 4
352
= 60
352 ,
which gives by insertion
P {X1 = X2} = 40 + 360 + 60
352
= 460
352 = 92
245.
3) Since G consists of 5 girls and 9 boys, of which we shall choose 6 members, we get
a = 5,
b = 9,
a + b = 14
and
n = 6,
hence
E{Y } =
na
a + b = 6 Â· 5
14 = 15
7 ,
and
V {Y } =
nab(a + b âˆ’n)
(a + b)2(a + b âˆ’1) = 6 Â· 5 Â· 9 Â· (14 âˆ’6)
142 Â· (14 âˆ’1)
= 5 Â· 6 Â· 8 Â· 9
4 Â· 13 Â· 49 = 540
637.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Discrete Distributions
 
72 
Index
Index
Banachâ€™s match stick problem, 54
Bernoulli event, 5
binomial coeï¬ƒcient, 4
binomial distribution, 4, 10, 64
binomial series, 5
Chu-Vandermondeâ€™s formula, 5
convergence in distribution, 64
criterion of integral, 35
decreasing factorial, 4
geometric distribution, 6, 31
hypergeometric distribution, 9, 54
negative binomial distribution, 8, 52
Pascal distribution, 7, 49
Poisson distribution, 6, 24
reduced waiting time, 8
skewness, 25
Stirlingâ€™s formula, 5, 11, 56, 65
stochastically larger random variable, 49
waiting time distribution, 7
waiting time distributions, 8
Download free eBooks at bookboon.com


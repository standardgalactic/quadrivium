© 2001 by CRC Press LLC
"Frontmatter"
The CRC Handbook of Modern Telecommunications
Ed. Patricia Morreale and Kornel Terplan
Boca Raton, CRC Press LLC. 2001

MODERN
TELECOMMUNICATIONS
THE
CRC HANDBOOK
OF

Patricia Morreale
Kornel Terplan
E D I T O R S - I N - C H I E F
MODERN
TELECOMMUNICATIONS
THE
CRC HANDBOOK
OF
Boca Raton   London   New York   Washington, D.C.
CRC Press
© 2001 by CRC Press LLC

This book contains information obtained from authentic and highly regarded sources. Reprinted material is quoted with
permission, and sources are indicated. A wide variety of references are listed. Reasonable efforts have been made to publish
reliable data and information, but the author and the publisher cannot assume responsibility for the validity of all materials
or for the consequences of their use.
Neither this book nor any part may be reproduced or transmitted in any form or by any means, electronic or mechanical,
including photocopying, microﬁlming, and recording, or by any information storage or retrieval system, without prior
permission in writing from the publisher.
All rights reserved.  Authorization to photocopy items for internal or personal use, or the personal or internal use of speciﬁc
clients, may be granted by CRC Press LLC, provided that $.50 per page photocopied is paid directly to Copyright clearance
Center, 222 Rosewood Drive, Danvers, MA 01923 USA.  The fee code for users of the Transactional Reporting Service is
ISBN 0-8493-3337-7/01/$0.00+$.50.  The fee is subject to change without notice.  For organizations that have been granted
a photocopy license by the CCC, a separate system of payment has been arranged.
The consent of CRC Press LLC does not extend to copying for general distribution, for promotion, for creating new works,
or for resale. Speciﬁc permission must be obtained in writing from CRC Press LLC for such copying.
Direct all inquiries to CRC Press LLC, 2000 N.W. Corporate Blvd., Boca Raton, Florida 33431.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for
identiﬁcation and explanation, without intent to infringe.
© 2001 by CRC Press LLC
No claim to original U.S. Government works
International Standard Book Number 0-8493-3337-7
Library of Congress Card Number 00-062155
Printed in the United States of America  1  2  3  4  5  6  7  8  9  0
Library of Congress Cataloging-in-Publication Data
The CRC handbook of modern telecommunications / editors-in-chief, Patricia Morreale
and Kornel Terplan.
p.   cm.
     Includes bibliographical references and index.
           ISBN 0-8493-3337-7 (alk. paper)
 1. Telecommunication--Handbooks, manuals, etc. I. Morreale, Patricia. II. Terplan,
             Kornel.
TK5101 .C72 2000
621.382—dc21
00-062155
 
© 2001 by CRC Press LLC

© 2001 by CRC Press LLC
Acknowledgments
The Editors-in-Chief would like to thank all their contributors for their excellent, timely work. Special
thanks are due to our Associate Editors, Teresa Piliouras and James Anderson. Without their help, we
would not have been able to submit this manuscript on time. We thank Mihaela Bucut, our Ph.D. student
at Stevens Institute of Technology for her valuable help with voice and data communications.
We are particularly grateful to Dawn Mesa, who has supported our editorial work by providing
signiﬁcant administrative help from CRC Press. We would also like to thank Ramila Saldana, who greatly
assisted the co-editors with the care and attention she provided to many details of the book.
Special thanks is due to Felicia Shapiro who particularly managed the production and Steve Menke
for his excellent project editing work.

© 2001 by CRC Press LLC
Foreword
In the preparation of this book, our objective was to provide an advanced understanding of emerging
telecommunications systems, their signiﬁcance, and the anticipated role these systems will play in the
future. With the help of our talented associated editors and contributors, we believe we have accomplished
this. By addressing voice, Internet, trafﬁc management, and future trends, we feel our readers will be
knowledgeable about current and future telecommunications systems.
In Section 1, the techniques of voice communication systems are outlined, with attention paid to both
basic and advanced systems. Advanced intelligent networks (AIN) and computer telephony integrated
(CTI) are key building blocks for future voice systems. Finally, voice over IP, and the anticipated inte-
gration of voice and IP data is closely examined. The second part of this section concentrates on state-
of-the-art solutions for local area networks. In addition to data communication capabilities, multimedia
attributes of LANs are also addressed.
Section 2 provides a detailed explanation of the Internet, including elements of its structure and
consideration of how future services will be handled on the Internet. Internet management and security
are discussed. A detailed discussion of virtual private networks (VPNs) is provided, as well as presentation
of web design and data warehousing concepts. Electronic commerce and Internet protocols are presented
in detail, permitting the reader to understand and select with insight from the available web-based
technology choices.
Section 3 continues the exploration of advanced telecommunications concepts, focusing on network
management and administration. As the services and features provided the network become larger in
scale and scope, network management will become even more crucial and important than it is today.
Telecommunications network management (TNM) and Telecommunications Information Networking
Architecture (TINA) are presented. The telecommunications support process is outlined, including
management frameworks and customer network management. A detailed consideration of outsourcing
options, which will become even more frequent, is presented. The performance impact of network
management is detailed.
Finally, in Section 4, future trends and directions are considered, with a view toward satisfying user
needs in parallel with application trends, which will require system and service integration. While we
know the future will hold new products and services, accounting for these services is a challenge, and
an examination of telecommunications tarifﬁng is also provided.
We hope our readers ﬁnd this book an excellent guide to emerging telecommunications trends.
Patricia Morreale
Advanced Telecommunications Institute
Stevens Institute of Technology
Hoboken, NJ

© 2001 by CRC Press LLC
Editors-in-Chief
Patricia Morreale, Ph.D., is Director of the Advanced Telecommunications Institute (ATI) and an Asso-
ciate Professor in the School of Applied Sciences and Liberal Arts at Stevens Institute of Technology.
Since joining Stevens in 1995, she has established the Multimedia Laboratory at ATI and continued the
work of the Interoperable Networks Lab in network management and performance, wireless systems
design, and mobile agents.
Dr. Morreale holds a B.S. from Northwestern University, a M.S. from the University of Missouri, and
a Ph.D. from the Illinois Institute of Technology, all in Computer Science. She holds a patent in the
design of real-time database systems and has numerous journal and conference publications. With Dr.
Terplan, she co-authored The Telecommunications Handbook, published by CRC Press.
Prior to joining Stevens, she was in industry, working in network management and performance. She
has been a consultant on a number of government and industrial projects.
Dr. Morreale’s research has been funded by the National Science Foundation (NSF), U.S. Navy, U.S.
Air Force, Allied Signal, AT&T, Lucent, Panasonic, Bell Atlantic, and the New Jersey Commission on
Science and Technology (NJCST). She is a member of the Association for Computing Machinery (ACM)
and a senior member of the Institute of Electrical and Electronic Engineers (IEEE). She has served as
guest editor for IEEE Communications magazine, special issue on active, programmable, and mobile code
networking. In addition, she is an editorial board member of the Journal of Multimedia Tools and
Applications (Kluwer Academic).
Kornel Terplan, Ph.D., is a telecommunications expert with more than 25 years of highly successful
multinational consulting experience. His book, Communication Network Management, published by
Prentice-Hall (now in its second edition), and his book, Effective Management of Local Area Networks,
published by McGraw-Hill (now in its second edition), are viewed as the state-of-the-art compendium
throughout the community of international corporate users. He has provided consulting, training, and
product development services to over 75 national and multinational corporations on four continents,
following a scholarly career that combined some 140 articles, 19 books, and 115 papers with editorial
board services.
Over the last 10 years, he has designed ﬁve network management-related seminars and given some
55 seminar presentations in 15 countries. He received his doctoral degree at the University of Dresden
and completed advanced studies, researched, and lectured at Berkeley, Stanford University, University of
California at Los Angeles, and Rensselaer Polytechnic Institute.
His consulting work concentrates on network management products and services, operations support
systems for the telecommunications industry, outsourcing, central administration of a very large number of
LANs, strategy of network management integration, implementation of network design and planning guide-
lines, products comparison, selection, benchmarking systems, and network management solutions.

© 2001 by CRC Press LLC
His most important clients include AT&T, AT&T Solutions, Georgia Paciﬁc Corporation, GTE, Walt
Disney World, Boole and Babbage, Salomon Brothers, Kaiser Permanente, BMW, Siemens AG, France
Telecom, Bank of Ireland, Dresdner Bank, Commerzbank, German Telecom, Unisource, Hungarian Tele-
communication Company, Union Bank of Switzerland, Creditanstalt Austria, and the State of Washington.
He is Industry Professor at Brooklyn Polytechnic University and at Stevens Institute of Technology in
Hoboken, NJ.

© 2001 by CRC Press LLC
Contributors
John Amoss
Lucent Technologies
Holundel, New Jersey
James Anderson
Alcatel
Richardson, Texas
John Braun
Weston, Connecticut
Karen M. Freundlich
TCR, Inc.
Princeton, New Jersey
Joe Ghetie
Telcordia
Piscataway, New Jersey
Michel Gilbert
Hill Associates, Inc.
Colchester, Vermont
Takeo Hamada
Fujitsu Laboratories America
Sunnyvale, California
Stephanie Hogg
Telsta Research
Victoria, Australia
Hiroshi Kamata
OKI Electric
Red Bank, New Jersey
Matthew Kolon
Hill Associates, Inc.
Colchester, Vermont
Carel Marsman
CMG
The Netherlands
Patricia Morreale
Stevens Institute of Technology
Hoboken, New Jersey
Dermot Murray
Iona College
New Rochelle, New York
Mihir Parikh
Polytechnic University
Brooklyn, New York
Teresa Piliouras
TCR, Inc.
Weston, Connecticut
Andrew Resnick
Citicorp
New York, New York
Endre Sara
Goldman, Sachs & Co.
New York, New York
Endre Szebenyi
Industry Consultant
Budapest, Hungary
Kornel Terplan
Industry Consultant and Professor
Hackensack, New Jersey

© 2001 by CRC Press LLC
Contents
1 Voice and Data Communications
Patricia Morreale
1.1 
Advanced Intelligent Networks (AIN)
Patricia Morreale
1.2 
Computer Telephone Integrated (CTI)
Michel Gilbert  
1.3 
Voice over IP
Matthew Kolon
1.4 
Local Area Networks
John Amoss
1.5 
Token Ring Specifics
John Amoss
1.6 
Summary
2 Intranets
Teresa Piliouras and Andrew Resnick
Introduction
2.1 
Internet and Intranet Management Concepts
Teresa Piliouras
2.2 
Internet Security
John Braun
2.3 
Virtual Private Networking Solutions
Endre Sara
2.4 
Effective Website Design
Karen M. Freundlich
2.5 
Web-Enabled Data Warehousing
Dermot Murray
2.6 
E-commerce Technologies: A Strategic Overview
Mihir Parikh
2.7 
Internet Protocols
John Braun
3 Network Management and Administration
Kornel Terplan
Introduction.
3.1 
Management Concepts
Joe Ghetie
3.2 
Management of Emerged and Emerging Technologies
Kornel Terplan
3.3 
Commercial Network and Systems Management Standards
Kornel Terplan
3.4 
Telecommunications Management Network (TMN)
Endre Szebenyi
3.5 
TINA
Takeo Hamada, Hiroshi Kamata, and Stephanie Hogg
3.6 
Telecommunications Support Processes
Kornel Terplan                    
3.7 
Management Frameworks and Applications
Kornel Terplan
3.8 
Customer Network Management
Kornel Terplan
3.9 
Aspects of Managing Outsourcing Solutions: Aiming for Success
Carel Marsman
3.10 Support Systems for Telecommunication Providers
Kornel Terplan
3.11 Performance Management of Intranets
Kornel Terplan
4 Future Telecommunications: Trends and Directions
James Anderson
4.1 
Introduction
4.2 
User Needs
4.3 
Application Trends

© 2001 by CRC Press LLC
4.4 
Systems and Service Integration
4.5 
New Product and Service Creation
4.6 
Telecommunications Tariffing
4.7 
Telecommunications Strategies

© 2001 by CRC Press LLC
Patricia Morreale et al. ‘‘Voice and Data Communications’’
The CRC Handbook of Modern Telecommunications
Ed. Patricia Morreale and Kornel Terplan
Boca Raton, CRC Press LLC. 2001

© 2001 by CRC Press LLC
1
Voice and Data
Communications
1.1 
Advanced Intelligent Networks (AIN)
Deﬁnition • Overview • Network Evolution • Introduction 
of IN • Beneﬁts of INs • Local Number Portability • The 
Call Model • AIN Releases • AIN Service Creation 
Examples • Other AIN Services • Acronyms
1.2 
Computer Telephone Integrated (CTI) .
Abstract • Basic Deﬁnitions • A Brief History of CTI • 
Components and Models • CTI Applications and Trends • 
Conclusion
1.3 
Voice over IP
The Coming Integration of Voice and IP Data • Applications 
for Voice over IP (VoIP) • A Component-based Overview • 
Keys to Successful Deployment • Acronyms
1.4 
Local Area Networks
Overview • IEEE 802.3 (CSMA/CD Speciﬁcs) • IEEE 802.2 
Logical Link Control Layer • Building Cabling Speciﬁcations
1.5 
Token Ring Speciﬁcs 
Topology • Station Attachment • Token Ring Operation • 
Priority Feature • Management • Physical Attributes • 
Formats
1.6 
Summary
1.1
Advanced Intelligent Networks (AIN)
Patricia Morreale
1.1.1
Deﬁnition
Intelligent network (IN) is a telephone network architecture originated by Bell Communications Research
(Bellcore) in which the service logic for a call is located separately from the switching facilities, allowing
services to be added or changed without having to redesign switching equipment. According to Bell
Atlantic, IN is a service-speciﬁc architecture. That is, a certain portion of a dialed phone number, such
as 800 or 900, triggers a request for a speciﬁc service. A later version of IN called advanced intelligent
network (AIN) introduces the idea of a service-independent architecture in which a given part of a
telephone number can be interpreted differently by various services depending on factors such as time
of day, caller identity, and type of call. AIN makes it easy to add new services without having to install
new phone equipment.
Patricia Morreale
Stevens Institute of Technology
Michel Gilbert
Hill Associates, Inc.
Matthew Kolon
Hill Associates, Inc.
John Amoss
Lucent Technologies

© 2001 by CRC Press LLC
1.1.2
Overview
This chapter discusses how the network has evolved from one in which switch-based service logic provides
services to one in which service-independent AIN capabilities allow for service creation and deployment.
As the IN evolves, service providers will be faced with many opportunities and challenges. While the
IN provides a network capability to meet the ever-changing needs of customers, network intelligence is
becoming increasingly distributed and complicated. For example, third-party service providers will be
interconnecting with traditional operating company networks. Local number portability (LNP) presents
many issues that can only be resolved in an IN environment to meet government mandates. Also, as
competition grows with companies offering telephone services previously denied to them, the IN provides
a solution to meet the challenge.
1.1.3
Network Evolution
1.1.3.1
Plain Old Telephone Service (POTS)
Prior to the mid-1960s, the service logic (Figure 1.1.1) was hard-wired in switching systems. Typically,
network operators met with switch vendors, discussed the types of services customers required, negotiated
the switching features that provided the services, and ﬁnally agreed upon a generic release date for feature
availability. After this, the network operator planned for the deployment of the generic feature/service
in the switching network fabric.
This process was compounded for the network operator with switching systems from multiple vendors.
As a result, services were not offered ubiquitously across an operator’s serving area. So, a customer in
one end of a city, county, or state may not have had the same service offerings as a person in another
part of the area.
Also, once services were implemented, they were not easily modiﬁed to meet individual customer’s
requirements. Often, the network operator negotiated the change with the switch vendor. As a result of
this process, it took years to plan and implement services. This approach to new service deployment
required detailed management of calling patterns, and providing new trunk groups to handle calling
patterns. As customer calling habits changed — such as longer call lengths, larger calling areas, and
multiple lines in businesses and residences — the demand on network operators increased.
1.1.3.2
Stored Program Control (SPC)
In the mid-1960s, stored program control (SPC) switching systems were introduced. SPC was a major
step forward because now service logic was programmable where, in the past, the service logic was hard
wired. As a result, it was now easier to introduce new services. Nevertheless, this service logic concept
was not modular. It became increasingly more complicated to add new services because of the dependency
between the service and the service-speciﬁc logic. Essentially, service logic that was used for one service
FIGURE 1.1.1
Plain old telephone service (POTS).
New originating - service logic:
New terminating - service logic:
Three-way calling
  Speed Dialing
   Call Waiting
Call Forwarding
called party
calling party
Switching
  System
Switching
  System

© 2001 by CRC Press LLC
could not be used for another. As a result, if customers were not served by a SPC switching system, new
services were not available to them.
1.1.3.3
Common Channel Signaling Network (CCSN)
Another aspect of the traditional service offerings was the call setup information — the signaling and
call supervision that took place between switching systems and the actual call. When a call was set up, a
signal and talk path used the same common trunk from the originating switching system to the termi-
nating switching system. Often there were multiple ofﬁces involved in the routing of a call. This process
seized the trunks in all of the switching systems involved. Hence, if the terminating end was busy, all of
the trunks were set up unnecessarily.
The network took a major leap forward in the mid-1970s with the introduction of the common channel
signaling network (CCSN), or SS7 network for short. Signaling system number 7 (SS7) is the protocol
that runs over the CCSN. The SS7 network consists of packet data links and packet data switching systems
called signaling transfer points (STPs).
The SS7 network (Figure 1.1.2) separates the call setup information and talk path from the common
trunks that run between switching systems. The call setup information travels outside the common trunk
path over the SS7 network. The type of information transferred includes permission for the call setup,
whether or not the called party is busy.
SS7 technology frees up trunk circuits between switching systems for the actual calls. The SS7 network
enabled the introduction of new services, such as caller ID. Caller ID provides the calling party’s telephone
number, which is transmitted over the SS7 network. The SS7 network was designed before the IN concept
was introduced. However, telephone operators realized that there were many advantages to implementing
and using SS7 network capabilities.
1.1.4
Introduction of IN
During the mid-1980s, regional Bell operating companies (RBOCs) began requesting features that met
the following objectives:
• Rapid deployment of services in the network
• Vendor independence and standard interfaces
• Opportunities for non-RBOCs to offer services for increased network usage
Bell Communications Research (Bellcore) responded to this request and developed the concept of
Intelligent Network 1 (IN/1, Figure 1.1.3).
FIGURE 1.1.2
Common channel signaling (CCS).
called party
calling party
Switching
  System
Switching
  System
Signaling Network
    Signaling
Transfer Points
      (STPs)
Introduction of Common Channel Signaling (CCS) for trunk signaling in 1976
- Reduced delay
- Improved reliability
- Reduction in fraud
- Ability to signal during stable call

© 2001 by CRC Press LLC
The introduction of the IN/1 marked the ﬁrst time that service logic was external to switching systems
and located in databases called service control points (SCPs). Two services evolved that required IN/1
service logic — the 800 (or Freephone) service and the calling card veriﬁcation (or alternate billing
service, ABS) service. Because of the service-speciﬁc nature of the technology, these services required two
separate SCPs. In order to communicate with the associated service logic, software was deployed in
switching systems. This switching system software enabled the switching system to recognize when it was
necessary to communicate with a SCP via the SS7 network. With the introduction of the SCP concept,
new operations and management systems became necessary to support service creation, testing, and
provisioning. In Figure 1.1.3, note the term “service-speciﬁc management systems” under the box labeled
“service management system.” This means that the software-deﬁned “hooks” or triggers are speciﬁc to
the associated service. For example, an 800 service has an 800-type trigger at the switching system, an
800-service database at the SCP, and an 800-service management system to support the 800 SCP. In this
service-speciﬁc environment, the 800-service set of capabilities cannot be used for other services (e.g.,
900 service). Although the service logic is external to the switching system, it is still service-speciﬁc.
At ﬁrst glance, Figure 1.1.4 looks similar to the previous diagram. However, there is one fundamental
difference. Notice the wording “service-independent management systems” under the box labeled “service
management system.” Now, following the IN/1 800 service-speciﬁc example, the AIN service-independent
software has a three-digit trigger capability that can be used to provide a range of three-digit services
(800, 900, XXX, etc.) as opposed to 800 service-speciﬁc logic. Likewise, the SCP service logic and the
service management system are service independent, not service speciﬁc. AIN is a service-independent
network capability!
1.1.5
Beneﬁts of INs
The main beneﬁt of INs is the ability to improve existing services and develop new sources of revenue.
To meet these objectives, providers require the ability to:
Introduce New Services Rapidly 
IN provides the capability to provision new services or modify existing services throughout the network
with physical intervention.
Provide Service Customization 
Service providers require the ability to change the service logic rapidly and efﬁciently. Customers are also
demanding control of their own services to meet their individual needs.
FIGURE 1.1.3
Intelligent Network (IN/1).
Switching
  System
New service - specific "hooks"
         in switch software
    Service - specific
management systems
      Service 
  Management
Systems (SMS)
Use of SCP for centralized services:
- Calling card service
- 800 service
STP
SCP

© 2001 by CRC Press LLC
Establish Vendor Independence 
A major criteria for service providers is that the software must be developed quickly and inexpensively.
To accomplish this, suppliers have to integrate commercially available software to create the applications
required by service providers.
Create Open Interfaces 
Open interfaces allow service providers to introduce network elements quickly for individualized cus-
tomer services. The software must interface with other vendors’ products while still maintaining stringent
network operations standards. Service providers are no longer relying on one or two vendors to provide
equipment and software to meet customer requirements.
AIN technology uses the embedded base of stored program-controlled switching systems and the SS7
network. The AIN technology also allows for the separation of service-speciﬁc functions and data from
other network resources. This feature reduces the dependency on switching system vendors for software
development and delivery schedules. Service providers have more freedom to create and customize services.
The SCP contains programmable service-independent capabilities (or service logic) that are under the
control of service providers. The SCP also contains service-speciﬁc data that allows service providers and
their customers to customize services. With the IN, there is no such thing as one size ﬁts all — services
are customized to meet individual needs.
Since service logic is under the service provider’s control, it is easier to create services in a cost-effective
manner. Network providers can offer market-focused service trials by loading service logic in a SCP and
triggering capabilities in one or more switching systems.
Accepted standards and open, well-documented interfaces provide a standard way of communicating
between switching systems and SCPs, especially in a multi-vendor environment.
1.1.6
Local Number Portability
The Telecommunications Act of 1996 is having a profound impact on the U.S. telecommunications
industry. One area of impact that is being felt by everyone is Local Number Portability (LNP). For LNP,
the Federal Communications Commission (FCC) requires the nation’s local exchange carriers (LECs) to
allow customers to keep their telephone numbers if they switch local carriers. The LECs must continue
to maintain the quality of service and network reliability that the customer has always received.
The rules required that all LECs begin a phased deployment of a long-term service provider portability
solution no later than October 1, 1997 in the nation’s largest metropolitan statistical areas.
FIGURE 1.1.4
Advanced intelligent network (AIN) architecture.
Switching
  System
Service - independent
management systems
      Service 
  Management
Systems (SMS)
Generic SCP Platform
- Service-independent capabilities
- Application software
STP
SCP
Generic call processing
- Service-independent capabilities
- Triggers

© 2001 by CRC Press LLC
Wireless carriers are also affected by LNP. December 31, 1998 was the deadline date that wireless
carriers had to be able to complete a call to a ported wire–line number. By June 30, 1999, the Act called
for full portability between wireless and wireline, including roaming capabilities.
AIN is a logical technology to help service providers meet this mandate. Many providers are looking
to AIN LNP solutions because of the ﬂexibility that AIN provides without the burden of costly network
additions.
1.1.7
The Call Model
The call model is a generic representation of service switching point (SSP) call processing activities
required to establish, maintain, and clear a basic call. The call model consists of Point in Calls (or PICs),
Detection Points (DPs), and triggers. These are depicted in Figure 1.1.5.
PICs represent the normal switching system activities or states that a call goes through from origination
to termination. For example, the null state or the idle state is when the SSP is actually monitoring the
customer’s line. Other examples of states, or PICs, are off-hook (or origination attempt), collecting
information, analyzing information, routing, alerting, etc.
Switching systems went through similar stages before AIN was developed. However, the advent of AIN
introduced a formal call model that all switching systems must adhere to. In this new call model, trigger
detection points (TDPs) were added between the PICs. SSPs check TDPs to see if there are any active triggers.
There are three types of triggers: subscribed or line-based triggers, group-based triggers, and ofﬁce-
based triggers. Subscribed triggers are provisioned to the customer’s line, so that any calls originating
from or terminating to that line would encounter the trigger. Group-based triggers are assigned to groups
of subscribers, e.g., business or Centrex groups. Any member of a software-deﬁned group will encounter
the trigger. Ofﬁce-based triggers are available to everyone connected to the telephone switching ofﬁce or
has access to the North American numbering plan. Ofﬁce-based triggers are not assigned to individuals
or groups.
If an active trigger is detected, normal switching system call processing is suspended until the SSP and
SCP complete communications. For example, in Figure 1.1.5, suppose an AIN call has progressed through
the null state or PIC, the off-hook PIC, and is currently at the collecting information PIC. Normal call
processing is suspended at the information collected TDP because of an active off-hook delayed trigger.
Before progressing to the next (analyzing information) PIC, the SSP assembles an information collected
message and sends it to the SCP over the SS7 network. After SCP service logic acts on the message, the
SCP sends an analyze route message that tells the SSP how to handle the call before going to the next
PIC (analyzing information).
FIGURE 1.1.5
The call model: basic concept.
Analyze_Route
Info_Collected
   AIN
Service
  Logic
SCP
  Call
Model
AIN
Switch
(SSP)
Triggers (e.g. , Off - Hook Delayed)
Points in Call (PICs)
(e.g. , Collecting Information)
Trigger Detection Points (TDPS)
(e.g. , Information Collected)

© 2001 by CRC Press LLC
Essentially, when the SSP recognizes that a call has an associated AIN trigger, the SSP suspends the
call processing while querying the SCP for call routing instructions. Once the SCP provides the instruc-
tion, the SSP continues the call model ﬂow until completion of the call. This is basically how a call model
works, and it is a very important part of AIN.
This concept differs from the pre-AIN switching concept in which calls were processed from origination
state to the call termination state without call suspension.
1.1.8
AIN Releases
The demand for AIN services far exceeded the availability of network functionality. Service providers
could not wait for all the features and functionality as described in AIN Release 1. AIN Release 1 deﬁned
all types of requirements, which made the capability sets too large to be adopted by the industry.
In North America, the industry agreed to develop subsets of AIN Release 1 that provided for a phased
evolution to AIN Release 1. AIN 0.1 was the ﬁrst subset targeted for use.
Bellcore developed functionality to address the FTS 2000 requirements set forth by the U.S. Govern-
ment. The RBOCs AIN turn adopted these requirements to meet their customers’ immediate needs. This
effort resulted in AIN Release 0, which had a time frame before the availability of AIN 0.1.
Meanwhile, the global standards body, the International Telecommunications Union (ITU), embraced
the concepts put forth in the AIN Release 1 requirements. The ITU developed an international IN standard
called Capability Set 1, or CS-1. As with AIN Release 1 in North America, CS-1 was encompassing a rich
functionality. To meet the market demand, the ITU formed a subgroup called European Telecommuni-
cations Standards Institute (ETSI) to focus on the immediate needs. This subgroup developed the Core
INAP capabilities. Many Post Telegraph and Telecommunications (PTT) organizations and their switch
vendors have adopted the ETSI Core INAP as the standard and are providing Core Intelligent Network
Application Protocol (INAP) capabilities.
1.1.8.1
AIN Release 1 Architecture
Figure 1.1.6 shows the target AIN Release 1 architecture, as deﬁned in Bellcore AIN Generic Requirements
(GRs).
The SSP in this diagram is an AIN-capable switching system. In addition to providing end users with
access to the network and performing any necessary switching functionality, the SSP allows access to the
set of AIN capabilities. The SSP has the ability to detect requests for AIN-based services and establish
communications with the AIN service logic located at the SCPs. The SSP is able to communicate with
other network systems (e.g., intelligent peripherals) as deﬁned by the individual services. The SCP
FIGURE 1.1.6
AIN Release 1.
Operations
Systems
(OSs)
Intelligent
Peripheral
(IP)
CCS
Network
Point (SCP)
Adjunct
Service
Switching
Point
(SSP)
Service
Control

© 2001 by CRC Press LLC
provides the service control. There are two basic parts to a SCP. One part is the application functionality
in which the service logic is installed after the services have been created. This application functionality
sits on top of the second basic SCP part: a set of generic platform functionalities that are developed by
SCP vendors. This platform functionality is shared among the service logic application programs in the
application functionality. The platform functionality also provides the SS7 interface to switching systems.
As shown in Figure 1.1.6, the SCP is connected to SSPs by the SS7 network.
The intelligent peripheral (IP) provides resources such as customized and concatenated voice
announcements, voice recognition, and dual tone multi-frequencies (DTMF) digit collection. The IP
contains a switching matrix to connect users to these resources. In addition, the IP supports ﬂexible
information interactions between an end user and the network. It has the resource management capa-
bilities to search for idle resources, initiate those resources, and then return them to their idle state.
The interface between the SSP and the IP is an integrated services digital network (ISDN), primary
rate interface (PRI) and/or basic rate interface (BRI). The IP has the switching functionality that provides
the ISDN interface to the switching system. The adjunct shown in Figure 1.1.6 is functionally equivalent
to a SCP, but it is connected directly to a SSP. A high-speed interface supports the communications
between an adjunct and a SSP. The application-layer messages are identical in content to those carried
by the SS7 network between the SSP and SCP.
1.1.8.2
AIN Release 0
The AIN Release 0 call model has three trigger checkpoints (TCPs). At each TCP there are one or more
triggers. For example, the off-hook TCP includes the off-hook immediate trigger. If a subscriber’s line is
equipped with this trigger, communications with the SCP will occur if the switching system detects an
off-hook condition. For an off-hook delayed trigger, one or more digits are dialed before triggering to
the SCP. At the digit collection and analysis TCP, collected digits are analyzed before triggering. Triggering
may also occur at the routing stage of a call. This call model is shown in Figure 1.1.7.
When a switching system recognizes that a call needs AIN involvement, it checks for overload condi-
tions before communicating with the SCP. This process is called code gapping. Code gapping allows the
SCP to notify the switching system to throttle back messages for certain NPAs or NPA-NXXs. When code
gapping is in effect, some calls may receive ﬁnal treatment. For others, a provide instruction message is
sent to the SCP. Depending on the SCP service logic, it will respond to the switching system with any of
the call processing instructions shown in Figure 1.1.8.
AIN Release 0 provided 75 announcements at the switching system. Release 0 was based on American
National Standards Industry (ANSI) Transaction Capability Application Part (TCAP) issue 1. TCAP is
at layer 7 of the SS7 protocol stack. This means that there is only one message sent from the SSP to the
SCP, no matter what trigger is hit at any of the three TCPs.
FIGURE 1.1.7
AIN Release 0 call model.
Idle
TCPs
Off - Hook
Off - Hook Immediate
Off - Hook Delay
Incoming Trunk Seizure
PRI B - channel
BRI Feature Activators
PODP Feature Code (*XX)
Customized Dialing Plan
Codes
Shared Interoffice Trunk
(Access Tandem trigger)
Digit
Collection
&
Analysis
Automatic Flexible
Routing
Directory Number
Routing

© 2001 by CRC Press LLC
1.1.8.3
AIN Release 0.1
AIN 0.1 is the ﬁrst subset of AIN Release 1. There are two fundamental differences between AIN Release 0
and AIN 0.1 The ﬁrst is a formal call model and the second is the messaging sets between the switching
system and the SCP. The formal call model is separated into the originating call model (originating half
call) and the terminating call model (terminating half call). The AIN Release 0 call model did not
distinguish between originating and terminating. A standard or formal call model is necessary as we
evolve to the Target AIN Release 1 capability, because the capabilities will have more PICs and TDPs.
Also, there will be multiple switch types and network elements involved. Therefore, the service logic will
need to interact with every element that will be required in the network.
AIN 0.1 includes several other major features. There are 254 announcements at the switching system,
which provide more ﬂexible messages available to customers. There are additional call-related and non-
call-related functions as well as three additional triggers — the N11 trigger, the 3–6–10-digit trigger, and
the termination attempt trigger. More triggers provide additional opportunities for SCP service logic to
inﬂuence call processing. (Note: TCP was an AIN Release 0 term that changed to TDP in AIN 0.1). There
are several AIN 0.1 non-call-related capabilities. The SCP has the ability to activate and deactivate
subscribed triggers. The AIN 0.1 SCP can also monitor resources. In addition to sending a call routing
message to the switching system, the SCP may request that the switching system monitor the busy/idle
status of a particular line and report changes. AIN 0.1 also supports standard ISDN capabilities.
As mentioned previously, there is a distinction between the originating side and the terminating side
of a service switching point. This means that both originating and terminating triggers and service logic
could inﬂuence a single call. Figure 1.1.9 shows a portion of the AIN 0.1 originating call model. The
AIN 0.1 originating call model includes four originating trigger detection points — origination attempt,
information collected, information analyzed, and network busy.
The AIN 0.1 terminating call model includes one TDP — termination attempt, as depicted in the
partial call model in Figure 1.1.10.
1.1.8.4
AIN 0.1: SSP–SCP Interface
The AIN 0.1, as shown in Figure 1.1.11, is based on ANSI TCAP issue 2, which means that the message
set is different than the message set in ANSI TCAP issue 1. For example, in AIN Release 0, there is only
one message sent from the SSP to the SCP no matter what trigger is hit at any of the three TCPs. In
AIN 0.1, separate messages are sent for the four originating and one terminating TDP.
FIGURE 1.1.8
AIN Release 0 functions.
AIN Release 0 trigger hit
Provide Instructions
Code Gapping
checked
Final treatment
TCAP Query sent
to SCP
Play announcement and collect digits
Route call
Terminate call to announcement
Notify SCP when call is cleared
Code Gapping information

© 2001 by CRC Press LLC
1.1.8.5
AIN Release 0.2
AIN 0.2 builds on AIN 0.1 with additional capabilities to support two service drivers — Phase 2 personal
communication service (PCS) and voice activated dialing (VAD). While AIN 0.2 is focused on capabilities
to support PCS and VAD, all requirements for these capabilities are deﬁned in a service-independent
manner. AIN 0.2 capabilities will include:
• ISDN-based SSP–IP interface
• Busy and no-answer triggers
FIGURE 1.1.9
AIN 0.1 originating call model.
FIGURE 1.1.10
AIN 0.1 terminating call model.
FIGURE 1.1.11
AIN 0.1 SSP–SCP interface.
SCP
STP
SSP

© 2001 by CRC Press LLC
• Next event lists processing
• Default routing, and
• Additional functions in all operations areas (e.g., network testing).
The two primary AIN 0.2 capabilities are the ISDN interface between a switching system and an ISDN-
capable device (such as an IP) and the addition of busy and no-answer triggers.
Next event lists processing is another important capability. In addition to TDPs, AIN 0.2 includes
event detection points (EDPs). With EDPs, the SCP will have the ability to send a next event list to the
SSP. This next event list is used by the SSP to notify the SCP of events listed in the next event list. These
events may include busy, no answer, terminating resource available, etc.
AIN 0.2 also includes default routing capabilities. This means that when calls encounter error condi-
tions, they can be sent to a directory number, an announcement, etc., as opposed to sending it to ﬁnal
treatment, as is the case in AIN 0.1.
1.1.8.6
AIN 0.2 SSP–IP Interface
AIN Release 0 and AIN 0.1 assumed that the announcements were switch-based. With the introduction
of AIN 0.2, announcements can reside in an external database, such as an IP. If the SCP sends a send-
to-resource message to the switching system to have the IP play an announcement or collect digits, the
switching system connects the customer to the IP via the SSP–IP ISDN interface. The end user exchanges
information with the IP. The IP collects the information and sends it to the switching system. The
switching system forwards the information to the SCP. One of the fundamental switching system capa-
bilities is the interworking of SS7 (SCP) messages with ISDN messages (SSP–IP).
In addition the SSP may control IP resources without SCP involvement. VAD is an example. A VAD
subscriber could be connected to the IP voice recognition capabilities upon going off-hook. The VAD
subscriber says “call mom,” and the IP returns mom’s telephone number to the switching system. The
switching system recognizes mom’s number as if the subscriber had actually dialed the number.
1.1.9
AIN Service Creation Examples
The previous modules addressed the architecture and the theory of the AIN. This section will discuss
various aspects of service creation — the tool that builds the representation of the call ﬂow for each
individual customer. Many AIN software vendors have paired service creation software with state-of-the-
art computer graphics software to eliminate the need for traditional programming methods. Through
the use of menu-driven software, services are created by inputting various service parameters.
1.1.9.1
Building Block Approach
Figure 1.1.12 provides an example of a building-block approach to creating AIN services. Play announce-
ment, collect digits, call routing, and number translation building blocks are shown here. The SSP has
the ability to play announcements and collect digits, as does the IP. Routing the call is a SSP function,
and number translation is a SCP capability. By arranging these four capabilities or building blocks in
various combinations, services such as 800 calling with interactive dialing, outgoing call screening, and
area number calling can be created.
1.1.9.2
Service Creation Template
Figure 1.1.13 represents what a service creation template might look like. For an outgoing call screening
service, the process begins with the customer’s telephone number.
This example allows the customer to screen 900 numbers, while still having the ability to override 900
screening by entering a PIN. Except for 703-974-1234, all non-900 calls are processed without screening.
1.1.9.3
Digit Extension Dialing Service
A 5-digit extension dialing service is displayed in Figure 1.1.14. It allows for abbreviated dialing beyond
central ofﬁce boundaries. If an employee at location 1 wants to call an employee at location 2 by dialing
the extension number 1111, 21111 would be dialed.

© 2001 by CRC Press LLC
Although 21111 is not a number that a switching system can use to route the call, a customized dialing
plan trigger is encountered after 21111 is dialed and a query is sent to the SCP. Service logic at the SCP
uses the 21111 number to determine the “real” telephone number of the called party.
1.1.9.4
Disaster Recover Service
Figure 1.1.15 illustrates a disaster recovery service. This service allows businesses to have calls routed to
one or more alternate locations based on customer service logic at the SCP. Calls come into the switching
system served by the normal location. After triggering, communication with the SCP occurs. Based on
the service logic, the call could be either routed to the normal business location or to one or more
alternate business locations.
FIGURE 1.1.12
AIN service example: building block approach.
FIGURE 1.1.13
AIN service example: building block approach.
Other
Other
Carrier
Carrier
Route
Route
Terminate
Terminate
Result
900
4755
7039743458
Ask For Pin
Dialed
Number
Outgoing  Call
Screening Service
  7039748072     ANI

© 2001 by CRC Press LLC
1.1.9.5
Area Number Calling Service
An area number calling (ANC) service is shown in Figure 1.1.16. This service is useful for companies or
businesses that want to advertise one telephone number but want their customer’s calls routed to the
nearest or most convenient business location. The SCP service logic and data (e.g., zip codes) are used
FIGURE 1.1.14
AIN service example: building block approach.
FIGURE 1.1.15
Disaster recovery service.
FIGURE 1.1.16
Area number calling (ANC) service.
Dial
21111
RING !!!
Switching
System
STP
SCP
Location 1
1111
2222
3333
Location 2
1111
2222
3333
Switching
System
Switching
System
Switching
System
STP
SCP
lncoming
Calls
Normal Business
Location
Alternate Business
Location(s)
Company Location 1
Company Location 2
STP
Incoming
Calls
SCP
Switching
System
Switching
System

© 2001 by CRC Press LLC
to match the calling party’s telephone number and their geographical location. The call is then routed
to the company or business location that is closest to or most convenient for the calling party.
1.1.9.6
Do Not Disturb Service
Finally, a do not disturb service is displayed in Figure 1.1.17. This is a service in which the Smith family
has terminating screening service logic at the SCP. Whenever someone calls them, the service logic
determines whether the call should be routed to the Smith’s telephone or play an announcement. In this
particular case, a telemarketer calls the Smiths. The SCP tells the switching system to route the telemar-
keter to an announcement. The customer’s SCP service logic may also contain a list of numbers that they
want to get through while do not disturb is active. In that case, if the SCP ﬁnds a match between the
calling party number and a number on the list, the call is routed to the Smiths.
1.1.10
Other AIN Services
The following list describes the services that companies have developed using AIN/IN technology. Some
services are tariffed, deployed in the network, and generating revenues. Others are in market or technical
trials, getting ready for deployment. There are other services that are either planned for deployment or
were developed for demonstration purposes.
N11 access service: With this service, a unique code is used to access a service gateway to information
service providers (ISPs), such as newspapers or libraries. The subscriber may either preselect an ISP for
automatic routing or request block calls to ISPs.
Basic routing: Allows the subscriber to route calls to a single destination as deﬁned in the system.
Single number service: Allows calls to have different call treatments based on the originating geo-
graphical area and the calling party identiﬁcation.
Routing by day of week: Allows the service subscriber to apply variable call routings based on the day
of the week that the call is placed.
Routing by time of day: Allows service subscribers to apply variable call routings based on the time
of the day that the call is made.
Selective routing: Tied to the call forwarding feature generally offered as a switch-based feature. With
the AIN, when a call to a selective routing customer is forwarded, the SCP determines where to route
the forwarded call based on the caller’s number.
Call allocator: Allows the service subscriber to specify the percentage of calls to be distributed randomly
for up to ﬁve alternate call handling treatments.
FIGURE 1.1.17
Do not disturb service.
Switching
System
Switching
System
STP
SCP
2
3
1
4
"The party you are 
trying to reach is
not available.
Please call later."
Smith
Family
555-3333
Telemarketer
Dials
555-3333

© 2001 by CRC Press LLC
Alternate destination on busy day: Allows the service subscriber to specify a sequence of destinations
to which calls will be routed if the ﬁrst destination is busy.
Command routing: A service subscriber predeﬁnes a set of alternate call treatments to handle trafﬁc
in cases of emergency, unanticipated or anticipated demand peaks, or for any other reason that warrants
an alternate call treatment.
Call gate: This is a versatile outgoing call screening service. Call gate supports a personal identiﬁcation
number (PIN) and screening based on time of day and day of week.
Personal access: A type of “follow me” service. A virtual telephone number is assigned to the personal
access service subscriber. When a caller dials this number, the software determines how to route the call.
Calling party pays: A service offered to cellular customers. It notiﬁes the calling party that they are
trying to reach a cellular number. If they choose to complete the call, they will incur the connect charge
of the called party. If they elect not to incur the cost, the call may either be terminated or routed to the
called party’s voice mail.
Remote access to call forwarding (ultraforward): Allows remote access to call forwarding. Callers may,
from any location in the world, call in remotely and activate and/or change their call forwarding number.
Portable number service: PNS Features enhanced call forwarding for large business subscribers. It
provides subscribers with the ability to maintain a personal itinerary which includes time-of-day, day-
of-week (TOD/DOW) schedules, call searching schedules, and call routing information. PNS subscribers
also have the ability to override their schedules with default routing instructions. This service is intended
for companies with employees who are in highly mobile environments, requiring immediate availability.
Enhanced 800 service: (Freephone) A customer’s call to an 800 service subscriber can be routed to
different destinations; instances of routing include the geographical location of the caller, the time and day
the call is made, and the caller responses to prompts. The subscriber sets alternate routing parameters for
the call if the destination is busy or unavailable, thereby redirecting and allowing for completion of the call.
Mass calling service: MCS A polling and information service that permits simultaneous calling by a
large number of callers to one or more telephone numbers. MCS provides a variety of announcement-
related services that connect a large number of callers (who dial an advertised number) to recorded
announcement devices. Two types of offerings are mass announcements, such as time and weather, and
televoting, which allows callers to register their opinions on a topic of general interest.
Automatic Route Selection/Least Cost Routing: Subscribers design a priority route for every telephone
number dialed. The system either directs calls or blocks calls to restricted privilege users.
Work-at-home: Allows an individual to be reached at home by dialing an ofﬁce number, as well as
allowing the employee to dial an access code from home, make long distance calls, and have them billed
and tracked to a business telephone number.
Inmate service: Routes prisoners’ calls, tracks the call information, and offers call control features
such as prompts for PINs, blocking certain called numbers, and time or day restrictions.
Holding room: Transportation companies’ passengers use this service to inform families or business
associates of transportation delays or cancellations.
Call prompter: Allows a service subscriber to provide an announcement that requests the caller to
enter a digit or series of digits via a dual tone multi-frequency (DTMF) telephone. These digits provide
information used for direct routing or as a security check during call processing.
Call counter: Increases a counter in the televoting (TV) counting application when a call is made to
a televised number. The counts are managed in the SCP, which can accumulate and send the results
during a speciﬁc time period.
500 access service: Allows personal communications service (PCS) providers the ability to route calls
to subscribers who use a virtual 500 number.
PBX extend service: Provides a simple way for users to gain access to the Internet network.
Advertising effectiveness service: Collects information on incoming calls (for example, ANI, time,
and date). This information is useful to advertisers to determine customer demographics.

© 2001 by CRC Press LLC
Virtual foreign exchange service: Uses the public switched network to provide the same service as
wired foreign exchange service.
ACNA originating line blocking: ACNA (Automated Customer Name and Address), with the ability
to block their line from being accessed by the service.
AIN for the case teams: Allows technicians to dial from a customer premise location anywhere in the
service region and connect to a service representative supported by an ACD. Through voice prompts,
the technician is guided to the speciﬁc representative within a case team pool within seconds, with no
toll charges to the customer.
Regional intercept: Instructs callers of new telephone numbers and locations of regional customers.
This service also forwards calls to the new telephone number of the subscriber. Various levels of the
service can be offered, based upon the customer’s selection.
Work at home billing: A person who is working at home dials a 4-digit feature access code which
prompts the system to track and record the billing information for the calls. Calls tracked in this manner
are billed directly to the company rather than to the individual.
Inbound call restriction: Allows a customer to restrict certain calls from coming into the subscriber’s
location. This service is ﬂexible enough to restrict calls either by area code, NNX, or particular telephone
numbers. Restrictions may even be speciﬁed by day of week or time of day.
Outbound call restriction: Allows a customer to restrict certain calls from being completed from the
subscriber’s location. This service is ﬂexible enough to restrict calls by either area code, NNX, or particular
telephone numbers. Restrictions may even be speciﬁc to day of week or time of day.
Flexible hot line: Allows a customer to pick up a telephone handset and automatically connect to a
merchant without dialing any digits. An example of this is a rent-a-car phone in an airport, which allows
a customer to notify the rent-a-car company to pick them up at the terminal.
Acronyms
ABS
Alternative billing source
AIN
Advanced intelligent network
AMP
AIN maintenance parameter
API
Applications programming interface
ASE
Application service elements
BCSM
Basic call state model
BRI
Basic rate interface
BSTP
Broadband signaling transfer point
CCM
Call control module
CCSN
Common channel signaling network
CFM
Call failure message
CSM
Call segment model
DAA
Directory assistance automation
DAP
Data access point
DCN
Data communications network
DP
Detection point
DTMF
Dual tone multi-frequencies
EDP
Event detection point
EML
Element management layer
ETC
Event trapping capability
FE
Functional entity
GDI
Generic data interface
IF
Information ﬂow
IN
Intelligent network
INAP
Intelligent network application protocol

© 2001 by CRC Press LLC
IN/1
Intelligent Network 1
IP
Intelligent peripheral
IPC
Intelligent peripheral controller
IPI
Intelligent peripheral interface
ISCP
Integrated service control point
LEC
Local exchange carriers
LIDB
Line information database
LNP
Local number portability
MP
Mediation point
MSC
Message sequence chart
NAP
Network access point
NCAS
Non-call associated signaling
NCP
Network control point
NE
Network element
NEL
Next event list
NML
Network management layer
NNI
Network-to-network interface
OBCM
Originating basic call model
ONA
Open network architecture
OOP
Object-oriented programming
OPC
Originating point code
PCS
Personal communications service
PIC
Points in call
PP
Physical plane
RBOC
Regional bell operating companies
RDC
Routing determination check
RE
Resource element
RVT
Routing veriﬁcation test
SCE
Service creation environment
SCMS
Service creation and maintenance system
SCP
Service control point
SDP
Service data point
SIBB
Service-independent building block
SLE
Service logic editor
SLEE
Service logic execution environment
SLI
Service logic interpreter
SLL
Service logic language
SLP
Service logic program
SM
Session management
SMS
Service management system
SN
Service node
SOP
Service order provisioning
SP
Service plane
SPC
Stored program control
SSP
Service switching point
STP
Signalling transfer point
TBCM
Terminating basic call model
TCP
Trigger check point
TCP
Test call parameter
TDP
Trigger detection point
TSC
Trigger status capability
WIN
Wireless intelligent network

© 2001 by CRC Press LLC
References
1. http://www.telecordia.com
2. Uyless D. Black, The Intelligent Network: Customizing Telecommunication Networks and Services,
Prentice Hall Series in Advanced Communications Technologies, 1998.
Further Readings
1. Uyless D. Black, The Intelligent Network: Customizing Telecommunication Networks and Services,
Prentice Hall Series in Advanced Communications Technologies, 1998.
2. Bill Douskalis, IP Telephony, Hewlett Packard Professional Books, Prentice Hall PTR, 2000.
3. William Stallings, High Speed Networks: TCP/IP and ATM Design Principles, Prentice Hall, 1997.
4. Kornel Terplan, Telecom Operations Management Solutions with NetExpert, CRC Press, 1998.
5. Uyless Black, ISDN and SS7: Architectures for Digital Signaling Networks, Prentice Hall, 1997.
6. John G. van Bosse, Signaling in Telecommunication Networks, Wiley & Sons, 1997.
7. Paul Ferguson and Goeff Huston, Quality of Service, Delivering QoS on the Internet and Corporate
Networks, Wiley & Sons, 1998.
8. Daniel Minoli and Emma Minoli, Delivering Voice over IP Networks, Wiley & Sons, 1998.
1.2
Computer Telephone Integrated (CTI)
Michel Gilbert
1.2.1
Abstract
In the universe of telecommunications, the worlds of voice and data have long been resistant to uniﬁca-
tion. The basic principles that underlie the two worlds have led to, at best, an uneasy truce. In recent
times, however, integration has become the buzzword. The industry has seen the emergence of one
technology after another that attempts to draw these two domains into closer proximity. Computer–tele-
phone integration (CTI) is yet another arena in which data and voice encounter one another. In the CTI
arena, however, voice and data appear to be on the cusp of a working relationship. This paper introduces
and reviews the concepts that underlie the world of CTI, the elements that comprise a CTI application,
and the standards that have emerged.
1.2.2
Basic Deﬁnitions
In a 1990 article titled “PBX/Host Interfaces: What’s Real, What’s Next” (Probe Research Conference
Digest), Lois B. Levick of Digital Equipment Corporation deﬁned CTI as, “A technology platform that
merges voice and data services at the functional level to add tangible beneﬁts to business applications.”
There are four key elements to this deﬁnition: 1) identifying CTI as a technology, 2) a focus on the
integration of voice and data, 3) specifying a functional integration, and 4) the need to derive tangible
beneﬁts in a business environment.
First, some would dispute the notion that CTI is a new technology. They would suggest that CTI is
actually a new application for pre-existing technologies. This is indeed the case. Not only is CTI simply
a place to reuse existing technologies, it is also not (as we shall see) particularly new.
Second, the integration of voice and data is a key element in CTI, as the name itself implies. CTI builds
on some remarkable convergence points in the evolution of computing and telephony. One of the earliest
telephone exchanges was designed in 1889 by a frustrated funeral director! Almond B. Strowger was tired
of seeing his competitor get the bulk of the funeral business by virtue of the fact that his competitor’s
spouse happened to operate the local telephone exchange. To deal with the problem, Strowger designed
a telephone exchange that became generally known as a step-by-step (or stepper) exchange. Fifty-four
years later, with funding from IBM, Howard Aiken created the Harvard Mark I. Both systems were entirely

© 2001 by CRC Press LLC
electromechanical, monstrous in size, and highly rigid in their design. Over the years, however, both
computers and switches became entirely electronic and based on solid-state technologies.
Where early switches and computers tended to be hardwired, modern switches and computers are
both stored-program machines and very ﬂexible. The switch uses a stored-program model to handle call
routing operations. The computer uses a variety of stored programs to support end-user applications.
Both depend on a data communications infrastructure to exchange control information. Finally, the
telephone network is rapidly converging to the digital communications model, which computers have
used almost from the outset.
Telephone switches have become specialized computers designed to provide a switching function, and
exchanging information via a complex digital data communications infrastructure.
The third major part of the deﬁnition, functional integration, requires a brief sidetrack to examine
the anatomy of a phone call. A phone call can be divided into two logical activities, commonly referred
to as call control and media processing. Call control is concerned with originating, maintaining, and
terminating a call. It includes activities like going off-hook, dialing the phone, routing a call through a
network, and terminating a call. Media processing is concerned with the purpose of the phone call. It
deals with the type of information being conveyed across the call, and the format in which that infor-
mation is presented.
Functional integration means the computer and switch collaborate in call control and/or media
processing operations. They may actually interchange functions to meet the needs of an application. Data
stored in the computer might be useful for routing incoming and/or outgoing calls. Perhaps the simplest
example is an autocall application where the user can click on a name stored in a local application and
the computer retrieves the associated phone number and dials the call automatically. Alternatively, call-
related data can be used to trigger information retrieval from the computer. For example, automatic
number identiﬁcation (ANI) can provide the calling number, which can be used to key a database lookup
to retrieve a particular customer’s account information before the phone even rings. In both examples,
the data of the computer and the routing of a call are bound together to do work.
Another form of functional integration is when computer and telephone peripherals begin to be used
interchangeably. For example, computer peripherals can become alternative call control elements instru-
mental in call monitoring, and telephone network peripherals can become an alternative method for
moving data between people and computers. There is even a degree of functional integration achieved
when the computer and telephone system are managed from a single point.
The fourth and ﬁnal element of Levick’s deﬁnition concerns the beneﬁts CTI brings to business
applications. One of the obvious goals of any business application is to provide better service to customers.
CTI can increase responsiveness, reduce on-hold waiting times, provide the customer with a single point
of contact, and make it easier to provide a broader range of services.
CTI can also increase effectiveness by eliminating many of the mechanical tasks associated with
telephony (e.g., dialing phones, looking up phone numbers, etc.), providing a better interface to the
telephone system, and integrating control of the phone system into a familiar and regularly used computer
interface (e.g., the familiar Windows desktop).
Perhaps the most telling beneﬁt CTI brings to the corporate world (and the one most likely to garner
the attention of the decision makers) is the potential for reductions in operating costs. Correctly applied,
CTI can mean faster call handling, which translates to reduced call charges. Automation of call-related
tasks means potentially fewer personnel, or greater capacity for business with existing personnel. Some
CTI implementers have claimed 30% improvement in productivity.
1.2.3
A Brief History of CTI
Although CTI appears to be a recent introduction into the telecommunications arena, there were attempts
to integrate voice and data into competitive business applications as early as the 1960s. In his book
Computer Telephone Integration (ISBN 0-89006-660-4), Rob Walters describes an application put together
by IBM for a German bookstore chain.

© 2001 by CRC Press LLC
The bookstores were looking for a way to automate their ordering process. IBM produced a small,
hand-held unit that each store manager could use to record the ISBN numbers of books they needed,
together with the desired quantity of each. These small units were then left attached to the telephone at
the end of the day. Overnight, an IBM 360 located at company headquarters would instruct the IBM
2570 PABX to dial each store in turn.
Once the connection was formed, the IBM mainframe would download the order and then instruct
the PABX to release the connection and proceed to the next store. The link between the IBM 360 and
the 2750 PABX was called teleprocessing line handling (TPLH). By the end of the night, the 360 would
produce a set of shipping speciﬁcations for each store, the trucks would be loaded, and the books delivered.
In 1970, a Swedish manufacturer of ball bearings (SKF) replaced its data collection infrastructure with
a CTI application that was also based on the IBM 360/2570 complex. Rather than using data collectors
who would travel from shop to shop, local shop personnel provided the data directly. On a daily basis,
they would dial a number that accessed the IBM 360/2750 complex at headquarters. Data was entered
using push-button phones. The switch would pass an indicator of the numbers pressed to the 360 via
the TPLH connection, and the computer would return an indication of acceptance or rejection of the
data to the switch. The switch would, in turn, produce appropriate tones to notify the user of the status
of the information exchange.
These two examples underscore the ﬂexibility of this early system. Note that both outbound (IBM 360
initiates the calls) and inbound (users call the IBM 360) applications were supported. This system
exhibited two classic hallmarks of a CTI application. First, the phone connection is used for media
processing (i.e., the information being passed back and forth). Second, there is a linkage between the
computer and the switch to exert call control.
Amazingly, after IBM’s introduction of the 360/2570 applications, there was an attempt at a form of
electromechanical CTI, albeit a short-lived one. In 1975, and largely in response to the IBM 360/2570
solution, the Plessey company designed a computer link to their crossbar PABX. Every line and every
control register of the switch was wired to the computer so its status could be monitored and controlled.
The computer could intercept dialed digits, make routing decisions, and instruct the switch to route a
call in a particular fashion. Called the System 2150, only two were deployed before electronic switching
rendered the technology obsolete.
At about the same time, a group of Bellcore researchers formed the Delphi Corporation to build a
system for telephone answering bureaus. These bureaus were essentially answering services for multiple
companies. At the end of the day, the company phones were essentially forwarded to these bureaus, where
a person would answer the line and take a message. However, it was important for the person answering
the phone to know what company was being called, and to be able to answer the phone as a representative
of that company. Delphi 1, released in 1978, was the answer to the problem.
All calls were rerouted to a computer that could tell by the speciﬁc line being rung which company
was being called. The computer would then retrieve the text for that company’s standard greeting, as
well as any special instructions for handling the call, and pass the call and instructions to an attendant.
The answering bureaus saw a 30% increase in efﬁciency and the concept caught on quickly.
Through the 1980s, niche applications continued to appear, and new players entered the market. These
included British Telecom (a telemarketing application), Aircall (paging), and the Telephone Broadcasting
Systems (a predictive dialing system). Perhaps one of the best-known CTI applications to emerge in the
1980s was Storeﬁnder™. The results of a collaboration between Domino’s Pizza and AT&T, Storeﬁnder™
used ANI to route a call to the Domino’s Pizza nearest that customer. Before the phone in the store could
ring, Storeﬁnder™ provided the personnel at that store with the customer’s order history, signiﬁcantly
enhancing the level of customer service.
Many early attempts to integrate computers and telephony focused on the media processing aspect of
communication. This includes early versions of voice mail and interactive voice response (IVR) systems.
These simple technologies did not need much more than specialized call receiving hardware in a computer
system, and a hunt group. When a caller dialed in to the service, the telephone network switched the call
to one of the access lines in the hunt group. The computer then proceeded to provide voice prompts to

© 2001 by CRC Press LLC
guide the user through the service. In the case of voice mail, the user was prompted to leave or retrieve
recorded messages. In the case of IVR, the user was prompted to provide, by touch-tone or voice, the
information necessary to perform a database lookup (e.g., current credit card balances, history of charges,
mailing address, payment due dates, etc.).
Modern voice-mail and IVR systems, and more advanced CTI applications, include a strong call control
component. They can transfer calls, provide outward dialing, and even paging. This requires a more
complex physical and logical integration of the computer and telephony worlds. The two worlds must
be physically connected, making it possible for data from the telephone network to be passed to the
computer and call control information from the computer to be passed to the network. Logically, the
integration of data from both the telephone network and the computer must be used to create new
applications that give the corporation a competitive edge.
Today, the call center scenario dominates that CTI world. Resulting applications typically utilize the
most advanced call control and media processing functions. CTI enables new call center models. A single
call center can be logically partitioned to function as multiple smaller call centers, or multiple distributed
call centers can be logically integrated to act as one. Modern CTI applications provide the knife, or the
glue, to make these models possible.
1.2.4
Components and Models
The basic components of a CTI application are depicted in Figure 1.2.1. At the heart of the application
lies the computer and the switch. The computer houses end-user data and hosts the end-user interface
to the CTI application. The switch provides the ability to make and receive calls and hosts the network
interface to the CTI application. The computer provides a set of peripherals (e.g., keyboard, screen, etc.)
by which the user accesses the CTI application, and the switch provides the peripheral (e.g., telephone)
by which the user communicates. Between the computer and switch there must exist a connection or
link, the nature of which differs depending on the type of CTI application.
Consider the automated attendant application. A person needing to speak with someone within the
company dials the company’s published phone number. The switch routes the call to a computer that
begins to play back a recorded message. The message prompts the caller to use the touch-tone buttons
to select from an array of options. The caller can enter the extension of the person they wish to reach,
in which case the computer directs the switch to reroute the call to that extension. The caller can use the
keypad to enter the name of the person being reached. The computer has to translate each tone to the
associated letter values, and determine if there is a match in the company personnel listing. If there is
none, or if the match is ambiguous (e.g., “Sam” and “Pam” use the same key combination), the computer
asks the caller to hold and transfers the call to an operator. If a single, unambiguous match is found, the
FIGURE 1.2.1
Basic components of a CTI application.
Computer Network
Computer 
CTI Link
CTI Application
Switch

© 2001 by CRC Press LLC
computer can ask the caller to conﬁrm the match, retrieve the extension from the database, and direct
the switch to transfer the call. At any point the caller can force the computer to transfer the call to an
operator by pressing 0.
1.2.4.1
Media Processing
As has been noted, any phone call can be broken down into two broad activities: media processing and
call control. CTI applications typically support both, albeit in different degrees of complexity and by
using different strategies. However, a complete suite of CTI services requires both media processing and
call control services.
Media processing is perhaps the easiest to understand. When a fax machine calls another fax machine,
the transmission of the encoded image across the connection is media processing. When an end user
uses their modem to dial in to the local Internet Service Provider (ISP), the exchange of data across the
connection is also media processing.
In the CTI arena, the hardware required for media processing is relatively simple. It often takes the
form of voice processing, speech digitization and playback, and fax circuitry. Many products integrate
these functions into a single printed circuit board that can be installed in a desktop computer. Many of
these integrated boards support multiple lines and hardwire the circuitry to each channel. This is
sometimes referred to as dedicated media processing hardware (see Figure 1.2.2). Companies that provide
such integrated boards include Dialogic Corporation (www.dialogic.com), Pika Technologies, Inc.
(www.pika.ca), and Rhetorex (www.rhetorex.com). Rhetorex is now a subsidiary of Lucent Technologies
(www.lucent.com).
This approach is appropriate for small-scale applications. For example, a company providing voice
mail services in a small town might equip a standard desktop system with a four-line integrated board.
A user dialing into the service would be switched by the network to one of the four lines. Based on the
FIGURE 1.2.2
Dedicated media processing hardware.

© 2001 by CRC Press LLC
tones provided by the user (e.g., “Please enter your mailbox number”) or ANI information provided by
the network, the user can retrieve recorded messages from the computer and play them back.
In these simple environments, standard application programming interfaces (API) are often adequate
for controlling the resources. For example, the Microsoft Windows or Solaris APIs that are used to play
sound ﬁles through a local speaker can also be used to send and receive multimedia content over a
telephone connection.
Large-scale applications, however, are more complex. In these environments, sharing resources is more
economically viable. A business person may be willing to purchase four complete sets of media processing
circuitry, knowing that at any given time only a few components associated with any particular line are
going to be used. However, equipping every line in a large application with all of the circuitry it might
be called upon to use is not cost effective. For example, consider a large-scale application that implements
a pool of four T1 circuit interfaces (96 voice channels). Usage patterns may show that this application
needs 96 voice digitizers and playback units, but only 16 speech recognizers, 16 fax processing circuits,
and 36 analog interfaces for headsets.
Assembling components at a more modular level is more cost effective and can scale more easily, but
it also places new demands on the system. New APIs and standards are required for interconnecting,
using, and managing these resources. There are two leading architectures for building such systems: the
multi-vendor integration protocol (MVIP) and SCbus. In addition to describing the hardware architec-
ture needed to interconnect telephony-related components, both GO-MVIP and SCSA deﬁne software
APIs required to use and manage those resources (see Figure 1.2.3). The SCSA Telephony Application
Objects (TAO) Framework™ is the API deﬁned by the SCSA.
On the hardware side, both MVIP and SCbus describe a time-division bus for talk-path interconnec-
tion, and a separate communication mechanism for coordinating the subsystems. MVIP (www.mvip.org)
is administered by the Global Organization for the MVIP (GO-MVIP). SCbus was originally developed
by the Signal Computing System Architecture (SCSA™) working group (www.scsa.org). SCSA has since
FIGURE 1.2.3
Architecture for sharing media processing hardware.

© 2001 by CRC Press LLC
become part of the Enterprise Computer Telephony Forum (ECTF), a non-proﬁt organization actively
prompting the development of interoperability agreements for CTI applications (www.ectf.org). SCbus,
announced in 1993, is now also an ANSI standard.
Both GO-MVIP and the ECTF also deﬁne a set of application program interfaces (API) for media
processing.
1.2.4.2
Call Control
The other major activity a CTI application needs to support is call control. Call control is concerned
with the successful establishment, maintenance, and termination of calls. To support these activities, the
switching nodes in the telephone network must communicate with one another and with the end-user’s
terminal equipment. The process by which the switches do this is called signaling. Signaling can be done
in-band or out-of-band. In-band signaling occurs on the same channel occupied by user information.
This is common for terminal equipment (i.e., telephones), and has become less common within the
network itself. Out-of-band signaling occurs on a separate channel from that occupied by user data. This
approach is common within the telephone network, and less common between the user and the network
(ISDN notwithstanding).
In addition to differentiating between in-band and out-of-band signaling, it is important to note that
signaling between the network and the user is bidirectional. The user signals the network by going off-
hook, dialing a phone number, and hanging up a phone. This signaling is well standardized. The most
common standard today is dual tone multi-frequency (DTMF), the familiar tones we hear as we press
buttons on a touch-tone phone. The network signals the user in-band by providing dial tone, busy signals,
ringing tones, fast busy, and so forth. Each of these has a distinct meaning, but the sounds have not been
well standardized internationally. This is a signiﬁcant challenge for the CTI environment. Out-of-band
network-to-user signaling is somewhat more standardized. Examples include the D-channel on an inte-
grated services digital network (ISDN) interface, the proprietary interfaces deﬁned by digital telephones,
and dedicated CTI interfaces to private branch exchanges (PBX) and switches.
Perhaps the most challenging aspect of CTI applications is achieving accurate and reliable call control.
In most applications, out-of-band signaling is preferred. Each option, however, has its scope, strengths,
and weaknesses. In an ISDN environment, D-channel signaling can be used by the CTI application. One
possible CTI application is a network-based automatic call distributor (ACD). Naturally the scope is
limited to the domain for which the ISDN signaling is meaningful. For example, the ACD application
may not be completely effective when calls cross some public network boundaries.
A CTI application could also leverage the proprietary signaling between a PBX and a digital telephone.
Again, such an application may be limited to the scope of the PBX or a group of PBXs from the same
manufacturer.
In the public network, the switch-to-switch signaling protocol is called Signaling System 7 (SS7). The
domain for SS7 signaling can be as large as an entire public telephone network. Unfortunately, SS7 is
usually not available to the CTI application. Closely associated with the internal operation of the public
network, SS7 access is jealously guarded by most carriers. Where access is available to the corporate
customer, a CTI application based on SS7 requires sophisticated customer premises equipment (CPE)
that can handle the complexity of SS7. As a result, this signaling option is usually only appropriate for
call centers handling large volumes of calls.
One of the most popular strategies for CTI applications is the dedicated CTI link implemented by
many modern PBXs and some public exchange switches. The domain for a dedicated CTI link is a single
telephone switch or a small number of tightly integrated switches or PBXs. These facilities are designed
for CTI, and tend to offer the rage of signaling options best suited to this environment. These dedicated
facilities can implement proprietary or standard call control strategies. Examples of proprietary strategies
include Nortel’s Meridian Link Protocol (MLP) and AT&T’s ASAI Protocol.
Naturally, the industry is leaning strongly to standards-based strategies. The predominant standard is the
Computer-Supported Telephony Application (CSTA) from the ECMA (formerly European Computer Man-
ufacturers Association). Adopted in 1990, the CSTA protocol (www.ecma.ch) has now been implemented by

© 2001 by CRC Press LLC
such major players as Siemens ROLM, Ericsson, and Alcatel, to name a few. It is important to note that,
although CSTA is a standard, the features any particular vendor elects to implement can vary. As a result,
CSTA implementations from different vendors are not necessarily interoperable.
1.2.4.3
First-Party and Third-Party CTI
CTI applications can be broken into two broad classes based on the relationship between the computer
and the switch. In ﬁrst-party CTI, the computer is essentially on an extension to the line on which a call
is being received. The computer can exert the same call control functions a human attendant could exert
via a standard telephone set attached to the telephone system. This implies that call control is on a call-
by-call basis. First-party CTI call control includes such activities as going off-hook, detecting dial tone,
dialing a call, monitoring call status signals (e.g., ring, ring no-answer, answer, busy, and fast busy)
conditions, and terminating the call.
In the ﬁrst-party CTI model (Figure 1.2.4) the computer, the keyboard and screen, and the phone are
all on the same line. The computer will tend to use the dedicated media processing hardware model, and
tend to be a user end-system (as opposed to being a server). First-party CTI is further subdivided into
basic and enhanced ﬂavors. Essentially, basic systems use in-band signaling and have limited capability.
Enhanced systems use out-of-band signaling, usually either ISDN or proprietary signaling to the PBX.
While there are basic ﬁrst-party CTI platforms on the market, the industry is more interested in enhanced
ﬁrst-party CTI systems.
The classic example of an inbound ﬁrst-party CTI application is the voice mail system. In a voice mail
application, an inbound call is received by the computer. The computer activates the local voice mail
software to record and store, or retrieve and playback, voice mail. The simplest example of an outbound
ﬁrst-party CTI application is autocall.
APIs for ﬁrst-party call control ﬁrst appeared from the manufacturers of network access equipment
(e.g., modems, fax boards, etc.). The only such API that achieved de facto standards status was the Hayes
modem command set. Now universally understood by modem products, the Hayes command set deﬁnes
basic commands for initiating and terminating calls, and altering the conﬁguration of the modem.
Third-party CTI is the more sophisticated model. In third-party CTI, the computer exerts call control
via a dedicated connection to the switch or PBX (Figure 1.2.5). This naturally implies out-of-band
signaling. It also implies that call control can be exerted over several calls, or over the switch itself. The
call control functions a third-party CTI application could exert are similar to those a human attendant
could exert using a specialized telephone set with enhanced privileges, such as an operator’s console.
In the third-party CTI application, the computer, the keyboard and screen, and the phone have no
relationship to one another unless the computer establishes one. These environments tend to use the
shared media processing hardware model, and tend to perform signaling via SS7 or (more commonly)
FIGURE 1.2.4
First-party CTI model.

© 2001 by CRC Press LLC
dedicated CTI links implementing the CSTA protocol. The CTI link typically terminates in a server rather
than a speciﬁc application end-system.
There are three basic ﬂavors of third-party CTI, which reﬂect the essential relationship between the
computer and the switch. In the compeer model, the computer and switch are on equal terms. Each
operates as the master of its own realm, passing information and receiving instructions from the other
across a specialized interface. In the dependent model, the computer rules and the switch obeys. The
switch has no innate call handling capability, and is actually incapable of processing calls without receiving
instructions from the computer. Finally, the primary model is virtually identical to the compeer model,
but the computer and switch do not share a specialized link. Rather, the computer attaches via a standard
trunk or line port. Over the years, the dependent and primary models have seen diminishing emphasis
as the market moves toward the compeer model. Unless explicitly identiﬁed as dependent or primary,
third-party CTI is usually assumed to operate on the compeer model.
Automatic call routing applications are classic examples of third-party CTI. A server-based application
is alerted, by the switch, to the arrival of a call. Based on ANI information, or the speciﬁc DNIS (i.e.,
called number), the computer directs the switch to divert the call to a speciﬁc line.
As with ﬁrst-party CTI, the ﬁrst third-party APIs were developed by manufacturers to support appli-
cations running on their own systems. Examples included the CallPath API from IBM, and the Computer-
Integrated Telephony (CIT) API from Digital Equipment Corporation (DEC). Unlike the Hayes command
set, however, none of these have achieved de facto standard status.
In the 1990s, three major APIs emerged, all strongly associated with a particular computing environ-
ment. Novell (www.novell.com) and Lucent collaborated to create the Telephony Services API (TSAPI).
Novell’s commercial product based on TSAPI is called NetWare Telephony Services, which links appli-
cations on remote clients with telephone system driver modules. TSAPI deﬁnes the boundary between
CTI application software, and the drivers that control the links and signaling into the network.
Microsoft (www.microsoft.com) and Intel collaborated to create the Telephony API (TAPI). Like TSAPI,
TAPI is concerned with call control. However, the TAPI architecture actually deﬁnes two distinct interfaces
(see Figure 1.2.6). The ﬁrst interface resides bet ween CTI applications and the  Windows operating system
(OS). This interface, which unfortunately has the same name as the overall architecture, provides a standard
means for CTI applications to access the telephony services provided by the Windows OS.
FIGURE 1.2.5
Third-party CTI model.

© 2001 by CRC Press LLC
The second interface resides between the Windows OS and the CTI hardware drivers. Known as the
telephony service providers interface (TSPI), this interface provides a standard mechanism for hardware
vendors to write drivers that can support the telephony services provided by Windows. It is Microsoft’s
job to ensure that TAPI-compliant applications can access all of the resources provided by TSPI-compliant
hardware drivers.
The third call control API is the more recent, introduced in October 1996, and brings CTI into the
world of the Internet and the World Wide Web (WWW). Developed jointly by design teams from Sun,
IBM, Intel, Lucent, Nortel, and Novell, the Java Telephony API (JTAPI) deﬁnes a call control interface
for CTI applications running as Java applets. This opens the door to creating Web-based CTI applications.
The Sun Microsystems product that implements this API is called JavaTel™.
Figure 1.2.7 integrates the various standa rds and concepts introduced in this paper in to a single CTI
model. A CTI application can be either ﬁrst-party or third-party. First-party applications tend to use
local, proprietary APIs (e.g., the Windows APIs) to access local call control and media processing services,
and the Hayes command set to control dedicated telephony hardware.
Third-party CTI applications tend to use sophisticated call control APIs like TAPI, TSAPI, or JTAPI,
and standardized media processing APIs like those deﬁned by the ECTF. The link between the CTI server
FIGURE 1.2.6
The TAPI architecture.

© 2001 by CRC Press LLC
and the switch commonly implements the CSTA protocols. The server typically uses shared telephony
hardware that is interconnected using the MVIP or SCbus architecture.
It is also possible to build a CTI server that supports several APIs and standards simultaneously. Such
a product would have to map requests from all APIs into a single common function set. Dialogic’s CT-
Connect product takes this approach. It supports both the TAPI and TSAPI interfaces and includes built-
in drivers for the ECMA CSTA link protocol and several other proprietary CTI link protocols.
FIGURE 1.2.7
Combining the standards and components.

© 2001 by CRC Press LLC
1.2.5
CTI Applications and Trends
A few of the more common, and simpler, CTI applications have already been noted: voice mail, autocall,
and automatic attendant. Each of these is commonly implemented as ﬁrst-party CTI applications using
dedicated media processing hardware. Digital dictation is another CTI application that is virtually
identical to voice mail, but typically supports longer record times. The recorded dictation is usually
retrieved and transcribed locally.
Many companies are beginning to provide interactive or on-demand fax services. For example, the
real estate company could provide automated faxes of current properties for sale. In such a service, the
user dials in and, using a touch-tone driven menu system, requests a particular fax or group of faxes and
provides the number to which the fax is to be sent. The service retrieves the fax from a local ﬁle, initiates
an outbound call to the speciﬁed number, and transmits the fax. As with the automated attendant
application, interactive fax could be implemented as a ﬁrst-party of third-party application.
Many pay-per-call applications are CTI applications. This is a common strategy for implementing fee-
for-access Internet services. The user dials a 900 number and the PBX routes the call to the CTI
application. The user is prompted to provide a code identifying the service they are trying to access. The
CTI application provides an access code that permits the user to access the web site. The phone service
bills the user for the 900 call and passes the majority of the fee to the pay-per-call service provider. The
pay-per-call service provider takes an additional cut and passes the remainder of the fee to the company
hosting the web service.
Perhaps the most common third-party CTI application is the inbound and outbound call center.
Inbound call centers typically integrate an automatic attendant to collect initial customer information
(i.e., credit card numbers, zip codes, pin numbers, etc.) and provide core services (e.g., account balances,
mailing addresses, account histories, a list of service or product options, automated order taking, etc.).
The caller always has the option, however, to abandon the automated system and speak to a person. In
this case, the CTI application routes the call to an available attendant and provides all information the
user has submitted. The application may also provide any call information provided by the phone network
and any customer data retrieved from the computer’s database.
The CTI market is showing clear signs of accelerated growth, fueled by a number of enabling factors
in the industry. The pervasive deployment of LANs and internetworks provides the infrastructure over
which many ﬁrst-party and third-party CTI applications operate. The growth in digital communications
and integrated networks that provide enhanced signaling capabilities (e.g., ISDN and digital telephones)
create a rich set of network information on which CTI applications can be built.
The emergence of standard APIs in both the media processing and call control arenas has furthered
equipment and service interoperability. Furthermore, the increasing maturity of voice processing tech-
nology makes interactive voice response (IVR) systems easier to deploy and use. Finally, the industry is
seeing a broad array of CTI application development toolkits. Examples of these include OmniVox from
Apex Voice Communications (www.apexvoice.com), Visual Voice from Artisoft (www.artisoft.com), Mas-
terVox from Mastermind Technologies (www.mastermind-tech.com), and IVS Builder and IVS Server
from Mediasoft Telecom (www.mediasoft.com).
1.2.6
Conclusion
The CTI market is a young one, but the technologies coming together into this application environment
are relatively mature. As the CTI-related standards themselves mature, interoperability agreements emerge,
and economies of scale begin to apply, CTI applications are likely to become pervasive. Furthermore, with
the emergence of JTAPI and the increasing drive toward voice over IP (and hence over the Internet), CTI
applications are ﬁnding a new niche in which to grow. The Internet is a signiﬁcant niche indeed!
For further information, the reader is recommended to visit the various web sites identiﬁed in this
chapter. There are also two periodical publications dedicated to CTI, both of which can be accessed via
the Internet: Computer Telephony (www.computertelephony.com) and CTI Magazine (www.tmcnet.com).

© 2001 by CRC Press LLC
1.3
Voice over IP
Matthew Kolon
1.3.1
The Coming Integration of Voice and IP Data
Companies in the U.S. spend $100B on long-distance and international telephony every year. Most of
that money goes to the basic transit of voice and fax from one location to another. With the continued
pervasiveness of intelligent peripheral (IP) networking, a new class of products and services has evolved
to move some of that trafﬁc from its traditional home on the public switched telephone network (PSTN)
to a variety of packet-switched networks. While many of these new “voice” networks have not previously
been considered telephony-class, they are nonetheless attractive because of their low cost.
The IP telephony scene has jumped from being a hobbyist’s realm of custom solutions and cobbled-
together software to a $400M per year industry hotly pursued by industry giants of hardware and software.
Continued improvements in digital signal processor (DSP) technology, voice packetization techniques,
and the networks that IP voice runs over have combined to make the start of the 21st century into the
era that IP telephony begins the transition to a mainstream solution for business.
There are a number of reasons for the inevitability of this transformation, but all of them come back
to the relief of high-cost long-distance telephone services. Reviewing a few comparative facts regarding
the PSTN and voice over IP (VoIP) presents some compelling realities:
• One can ﬁt more voice on an IP network than one can on the PSTN. The Bell System deﬁnition
of a single voice channel as a 64kbps DS-0 has led to a long-standing institutional belief that 64k
is necessary to carry a voice conversation. Thus a T-1 is commonly referred to supporting
23 “voice” channels over its 1.544 Mbps. Yet today’s VoIP products can carry hundreds of voice
conversations over that same amount of unchannelized bandwidth.
• Packet networks are much better than they used to be. Improvements in the quality of physical-
layer packet networks over the past 30 years have resulted in a large general improvement in data
integrity. The same forces that make simple frame relay an effective replacement for the robust
X.25 protocol mean that even connectionless IP data — and voice — may be entrusted to today’s
connectionless networks and still have an excellent chance of getting through in a reasonable
amount of time and with few errors (or little delay) of consequence.
• Control of IP data networks rests largely in the hands of the customer. As long as a minimum
quality of service — particularly the establishment of maximum delay guidelines — is met,
virtually every service available over IP is controllable from the sending and receiving stations.
For example, packets may be routed over the Internet for free if tolerant of lower quality, over a
private IP network if demanding of higher quality, or even over the PSTN if necessary — all at
the discretion of the originating node.
These are just a few of the reasons why many network managers are examining the current possibilities
for placing at least some of their voice trafﬁc into IP networks.
1.3.2
Applications for Voice over IP (VoIP)
Of course, with long-distance services being the single most expensive portion of any company’s telephony
budget, the application of VoIP to the interexchange carrier (IEC) realm is taking the forefront when it
comes to the immediate application of the technology. The basic design of such a network is rather
simple: gateways within local calling areas connected by an IP network which spans the distance previously
covered by the IEC.
While a company implementing VoIP for the purpose of saving charges on interofﬁce communications
may have a design as simple as that in Figure 1.3.1, it is more likely that the IP network will connect
multiple sites, each with its own gateway, each of which may then contact another dynamically when it

© 2001 by CRC Press LLC
has a voice call destined for that site. The connectionless nature of IP ensures that new gateways may be
added at will, with little need for reconﬁguration at the other stations.
Many variations of this scheme are possible, depending upon the nature of the service one is trying
to implement. For tie-line replacement and business-to-business calls, the simplest to exploit is that
shown in Figure 1.3.1, that is, two or more gateways connected by an IP network. The reason that most
pundits consider this setup to be the ﬁrst area to exploit VoIP is because the difﬁcult part — getting the
voice to a few places where it can be digitized and packetized into IP — is already done. The private
branch exchange (PBX) that currently connects via a leased line or IEC to another PBX can easily have
that connection replaced by IP — with no changes in how users place calls.
Another application that is generating a large amount of industry interest is that of business-to
residential telephony (Figure 1.3.2), to allow telemarketers or call centers to physically centralize while
obtaining low-cost long-distance service via VoIP. In this scenario, residential customers are able to dial
a local number and access a VoIP gateway which connects them to the implementer’s customer support
or sales ofﬁce — wherever it may be. The customer makes a free call, and receives the same service had
an 800 number been dialed, but the company avoids the cost of maintaining 800 service. It is also able
to supply customers with a “local” number to call for service, which can enhance the company’s image.
Reversing the above strategy — that is, using the remote gateway to place local calls rather than accept
them — allows telemarketers access to large, yet distant, markets without the need to place large numbers
of long-distance calls to get to them.
FIGURE 1.3.1
Business IEC replacement using VoIP.
FIGURE 1.3.2
Business-to-residential VoIP network.
IP Network
LATA  I
LATA  II
VolP Gateway
VolP Gateway
 PBX or other
phone system
 PBX or other
phone system
IP Network
LATA  I
LATA  II
Remote
VolP Gateway
Local
VolP Gateway
Call center or
Telemarketers
POTS
Residential
Customers

© 2001 by CRC Press LLC
Yet another option exists for those eager to exploit the possibility of VoIP at their businesses or campus:
replacing the PBX and its network with an IP network. Most businesses are already halfway there; they
have local area networks (LANs), routers, and digital wide area network (WAN) facilities capable of
handling IP trafﬁc. New products, such as 100- and 1000-Mbps Ethernet, as well as the cost-effective
speed of LAN switching, mean that network managers can build an enormous amount of capacity into
their local and enterprise networks — capacity which might well be used to carry voice trafﬁc. Traditional
models for business trafﬁc have always involved the creation and management of two separate networks,
one for voice and one for data. The encapsulation of voice in IP packets means that the consolidation
of voice into the data network is now possible, with the corresponding reduction in the need for
equipment, data facilities, stafﬁng, and expertise in several types of systems. Consolidation of voice trafﬁc
and data trafﬁc into the same end-to-end network opens the door to true integration of messaging and
telephony systems, such as integrated email and voice mail, and IP-based fax messaging.
The ﬁnal area of interest for VoIP proponents is that of residential-to-residential connectivity, that is,
friends and relatives speaking to each other from handsets or speakerphones integrated into Internet-
connected PCs. While this is the application that “proved” the possibility of VoIP, it remains the most
difﬁcult to ensure acceptable quality for. The difﬁculty of obtaining quality voice this way has nothing
to do with the equipment at the ends of the link, but rather with the lack of guaranteed, or even reliable,
values for delay and delay variation over the Internet. Indeed, improvements in low-cost digitization
hardware and “Internet telephony” software have made it possible to have a full-featured, high-quality
VoIP gateway for the cost of a new PC. But even the best-quality digital voice will be unintelligible if
only half of it arrives at the intended destination.
These are just the basic categories that some of the most obvious applications for VoIP fall into. But
applications are as numerous as those for the telephone itself — perhaps even more so. The lower cost
of VoIP means that some uses for telephony that were once deemed uneconomical may now be justiﬁed.
And the integration of voice and data trafﬁc over a single IP network may make some forms of integration
possible that were unthinkable just a few years ago.
1.3.3
A Component-based Overview
What are the components of a successful IP telephony system? While there are of course a number of
different approaches, there are a few basic ingredients that all systems must implement — although the
use and location of parts changes with different network designs.
The VoIP Network: In the list of VoIP components (Figure 1.3.3), the IP network(s) over which the
voice will travel is of primary importance. IP is ﬁrst and fundamentally a connectionless protocol, with
no guarantees concerning the trafﬁc that it carries. It cannot ensure a maximum delay or variability of
delay, cannot retransmit errored or lost packets, and does not even promise that its payload will arrive
at all. The quality of service one receives from the PSTN, and that provided by even the most carefully
managed and overbuilt IP network do not bear comparison. And for those thinking about using the
Internet as the equivalent of their current expensive IEC service…well, sufﬁce it to say that when a web
page often takes 60 seconds to download, sending real-time voice trafﬁc over that same series of links
will be a challenge. Until the Internet infrastructure is managed under an agreement which includes
concrete plans to provide some limited and predictable delay — in an interprovider fashion — voice
trafﬁc cannot travel the Internet and maintain the quality that business customers demand. It’s worth
mentioning that this agreement is nowhere in sight.
That does not mean that today’s Internet has no place in the voice network, however. VoIP gateways
can use the Internet to provide the non-real-time services that constitute much of today’s “voice” trafﬁc.
The most obvious one of these is facsimile transmission. While fax machines thrive on the dedicated
lines of the circuit-switched PSTN, there is no reason why their transmissions cannot be placed in IP for
long-distance transit. Delay — the reason why interactive voice is so difﬁcult over the Internet — doesn’t
affect fax transmissions at all, and transmission control protocol/Internet protocol (TCP/IP) can resend

© 2001 by CRC Press LLC
data until the network gets it right without bothering the receiver. The same could be said for voice mail
messages.
The next step between the very public Internet and a completely private IP network is the ISP backbone
itself, which is nothing more than a single provider’s portion of the Internet. If this network extends
close to the points where gateways will be placed, IP trafﬁc between them may remain solely on that
network. In almost all circumstances, this will result in less delay and better predictability for trafﬁc of
all types. But while the statistics for network performance may improve in a single-provider environment,
the lack of user control over these fundamentally public networks may be unacceptable for the network
manager who seeks to have some inﬂuence over the environment in which his trafﬁc travels. Single
Internet service provider (ISP) IP telephony, though, has the lowest cost of any of the non-Internet
options, and therefore is attractive as long as acceptable quality can be achieved. This may be a matter
of simply trialing a number of ISP networks and choosing the one with the best performance, or may
actually involve a level of performance — with stated delay and throughput characteristics — to be
speciﬁed in the user contract.
Luckily, the Internet and its constituent networks are not the only options for long-distance carriage
of VoIP. Many of the larger ISPs offer, in addition to their public Internet network, access to a separate
IP network designed for virtual private network (VPN), intranet, extranet, and other semiprivate usage.
These networks are not any more remarkable in concept than an average ISP’s network, except for their
managed nature, that is, the knowledge the provider has of just how much trafﬁc any one user is likely
(or allowed) to subject the network to at any one time — something unheard of on an Internet access
network. This knowledge allows the provider to predict and maintain a high level of quality, which can
result in service level agreements in which end-to-end delay is speciﬁed to be well below 0.5 seconds —
the point at which telephony starts becoming reasonable. In this environment, SLAs are becoming the
rule rather than the exception.
The ultimate VoIP network, however, is the one where all aspects of IP trafﬁc and performance can
be managed by the users — a completely private intranet. Formed from private (leased) lines, with
perhaps some links composed of frame relay or asynchronous trafﬁc mode (ATM), the distinguishing
characteristic of these networks is that they are completely under the control of the network managers
who deploy and run them. Therefore, the amount of bandwidth reserved for voice trafﬁc can be strictly
controlled, as can the throughput of routers and other connectivity equipment. How those resources are
FIGURE 1.3.3
VoIP network components.
ITSP Network
VolP Gateway
VolP Gateway
Internet
Intranet/VPN
LAN
PC with VolP software
PC with VolP software
PC with VolP software
PBX
IP Router
LEC/PSTN
Modem

© 2001 by CRC Press LLC
actually apportioned may vary from protocol-based reservation systems like reservation protocol (RSVP)
to completely manual intervention, but whatever the method, the manager has the ability to restrict the
effect of data trafﬁc that interferes with voice. While this sounds like — and in fact is — the ideal
environment for packetized voice, it comes with a price. Completely private IP networks are by far the
most expensive way to ship IP from one location to another. Whether the establishment of such a network
is worth the ability to carry voice effectively depends on how much money can be saved by eliminating
IEC charges from the IT budget.
If the number of options and the headaches of managing another network service are a serious
disincentive, another possibility is to leave the network and its management to the specialists — that is,
to contract with one of the growing number of Internet (or IP) telephony service providers (ITSPs). An
ITSP functions as a plug-and-play replacement for a traditional IEC, by providing the gateway, network,
and management needed to make VoIP successful. The tradeoff here, of course, is that since the ITSP
does all the work, they also reap some of the rewards. Typically, ITSPs function like an IEC in terms of
billing, with per-minute rates that range from one half to three quarters that of comparative IECs.
That level of discount may change before long, however. Much of the savings that ITSPs are able to
pass on to their customers are possible because of a May 1997 FCC ruling that classiﬁes ISPs and ITSPs
as end users of the PSTN rather than as carriers. This classiﬁcation currently makes it impossible for
LECs to charge ITSPs the same access charges they demand from traditional IECs. Those access charges,
when passed on to the IEC customer, can account for as much as one half of the average IEC bill. It is
the lack of these charges, more than the technological beneﬁts of VoIP, that allows ITSPs to sell services
for so much less than their IEC counterparts.
While the level of savings on recurring charges is the least with the ITSP option, it may well be
compensated for by the simplicity of setup and management, and the lack of gateway hardware or software
costs. The users who beneﬁt from the access charge loophole, however, may have some hard decisions
to make if, as many believe will occur, the FCC reverses itself and decides to consider ITSPs as carriers.
In that market, much of the price differential would disappear, and users would have to make their
decisions based more on quality, service, and other points rather than price (Figure 1.3.4).
All of these networks can and will beneﬁt from work currently underway to allow efﬁcient prioritization
of packets containing voice over those containing non-real-time data. Gigabit-speed routers, faster
switches, better routing and path-reservation protocols, and the continued addition of cheap bandwidth
are all reasons why VoIP quality will continue to increase.
In summary, there are a number of network options for VoIP. Which one best suits a particular need
depends on a number of factors, primarily revolving around the level of expected quality. For those
looking for a way to lower the cost of interofﬁce communications — an application where the “internal”
aspect may allow slightly lower quality than that required for communications with customers — some
of the lower-cost options like single-ISP VoIP networking may sufﬁce. Those wishing to completely
replace their IEC contract with an IP-based IEC solution are faced with replacing a complex network
from the ground up, and will have to plan, and pay for, a much more robust service. And for the time
FIGURE 1.3.4
VoIP network options compared.
Network
Gateway
Cost
User
Control
Performance
Internet
User-provided
Least
Least
Worst
Single ISP
User-provided
Managed IP
User-provided
Private IP
User-provided
Most
Best
ITSP
Included in
contract
Most
N/ A
N/ A

© 2001 by CRC Press LLC
being, at least, voice over the public Internet remains in the realm of a hobby for those willing to tolerate
indifferent and completely unpredictable voice quality.
Gateway Software and Hardware: The hard work of actually taking analog v oice and sending it over
an IP network, as well as receiving IP and converting it back into voice, is the job of the gateway. It is
easiest to examine the issues related to this complex task if  we break it down into its components
(Figure 1.3.5):
Accept analog or digital voice: A gateway must have some connection to the non-IP world where the
voice trafﬁc originates, usually consisting of either a bank of dial-in plain old telephone service (POTS)
ports or a digital connection to a PBX.
Prepare the voice signal: In order to use the available bandwidth as efﬁciently as possible, the voice
signal must go through a number of transformations before it is ready to be digitized. First, it must be
“cleaned up” by having as much noise and echo removed as possible. The techniques for doing this have
been well established in the traditional telephony world for years, but the cooperation of the various
systems and gateways through which voice may pass is esse ntial. This means that calls traveling through
a LEC o n their way to the VoIP gateway may need to be treated differently than those coming directly
from a PBX.
Second, it must be stripped of unnecessary silence, to avoid making the gateway send hundreds or
thousands of packets per second carrying nothing. Most gateways have adjustable options for when
silence suppression “closes off” and stops transmitting on behalf of a user, but the effectiveness of default
settings may depend on usage characteristics that are themselves dependent on cultural factors. Some
adjustment of this setting to achieve the best compromise between quality and throughput is usually
necessary. Related to the subject of silence suppression is the modeling and regeneration (at the remote
end) of background noise, without which users can become disconcerted.
Compress and digitize the voice signal: The standard compression and digitization of voice provided by
traditional 64k PCM produces a stream of digital data that is enormous compared to that available by
many newer codecs. While some vendors have achieved good results with proprietary schemes, most of
the industry is settling down to the use of one or another International Telecommunications Union (ITU)
G-series codecs, as speciﬁed in their H.323 standard. H.323 is a c omplex speciﬁcation for point-to-point
FIGURE 1.3.5
VoIP protocol components.
G.7xx
RTP
UDP
RSVP
IP
Network

© 2001 by CRC Press LLC
and multipoint teleconferencing, data sharing, and telephony over IP. While the full effect of this standard
on “VoIP-only” products remains to be seen, the G.711, G.723, and G.729 codec speciﬁcations referenced
by it are current favorites for coding voice.
These three standards differ primarily in the amount of work that the DSP must do in order to process
the analog signal, and the number of bits that it takes to represent a given amount of voice. While recent
advances in DSP design and manufacture have allowed vast improvement in these areas, there remains
an inverse relationship between them, and also therefore a higher cost for greater efﬁciency. Nevertheless,
the most aggressive of the standards — G.729 — can represent 10 msecs of voice with only 10 octets of
IP data. The less intensive G.711 and G.723 trade higher trafﬁc volume for higher quality. Many gateways
can be conﬁgured to use whichever one of these standards provides the most acceptable trade-off between
quality and trafﬁc level.
Route the call: Once a gateway has a potential stream of packets ready to send, it must have some way
to identify the address of the gateway it will send them to, and to inform that gateway of which local
user it is destined for (or what local number to dial.) For simple point-to-point applications, IP address
can be a manually conﬁgured variable, since there is only one destination possible. But in cases where a
multipoint network means that packets may be simultaneously distributed among a number of destina-
tions, there must be a process in which the called number is translated into an IP address.
Informing the destination gateway of the called phone number has its complications, too, because
many of the codecs used in current gateways compress the analog signal so much that the dual-tone
multi-frequency (DTMF) tones produced by phones become unreliable. Therefore, the calling gateway
must be able to transform those DTMF tones into a code representing the called number and transmit
them to the destination gateway for correct routing at the called end.
Packetize and send digital voice in IP datagrams: At ﬁrst glance, this is the simple part. After all, IP
stacks on end stations and routers have been performing this function since the late 1960s. Yet some of
the characteristics of packet-switched networks with regard to real-time trafﬁc are different than those
regarded as common knowledge by those used to thinking of IP as data-only transport.
For example, the ﬂexible size of an IP datagram, while an advantage in the transmission of data,
complicates the problem of achieving low variability of delay, since IP routers handle packets of various
sizes differently, and may tend to process smaller packets more quickly than larger ones. The destination
gateway would then need to account for the tendency of larger packets to take longer, and thus delay
reassembly. In practice, VoIP gateways by default transmit packets of a single size or small range of sizes
in order to obviate this problem, but this is one area where the capabilities of the gateways and the
network(s) over which they will transmit must be closely matched. Setting the maximum packet size of
the gateway to any amount higher than the maximum transmission unit (MTU) of the underlying
network will introduce latency as routers fragment datagrams that are too big to travel through networks
attached to them.
Enabling routers to prioritize packets containing voice can enable voice and data to coexist on the
same network more easily. Methods for doing this include enabling priority queuing based on transport
layer port number, packet size, and source and destination addresses. RSVP can be used to reserve router
bandwidth and processing capability, as well as network segment bandwidth, for packets that meet certain
criteria, but implementing RSVP demands a network path in which all routers are RSVP-compliant,
something that is not likely in a multiprovider (or even some single-provider) scenarios.
Receive, buffer, and decode the incoming stream of VoIP data: Again this is a well-understood process
for data, which generally depends upon the IP suite’s TCP protocol to retransmit lost data and reassemble
segments in the proper sequence before it is passed to the application. VoIP software seldom makes use
of TCP, largely because the services it provides introduce far too much latency into the transmission
process for them to be useful (an exception to this rule is fax transmission, for which TCP makes sense
given the lack of need for real-time treatment of data.) Instead, most gateways can use real time protocol
(RTP) as the protocol in which voice data rides. While having no control over delay imposed by the
network, RTP makes it possible to trade a small amount of additional delay for a reduction in the amount

© 2001 by CRC Press LLC
of delay variation. This is accomplished by transmitting each packet with a timestamp that can be read
by the receiver and used to pass data to the upper layers of the VoIP software with something like the
transmitted amount of inter-packet delay.
Alternatively, some gateways have the option to send digitized voice in user datagram protocol (UDP)
packets, which travel in an unstructured stream, free of sequence numbers, timestamps, and acknowl-
edgments — but also free of the delay imposed by processing these variables. Since the audio stream at
the remote end must go on regardless of the actual receipt of data, large numbers of packets that are lost
en route simply result in “holes” or “dropouts” in the audio signal. While this sounds as though it would
spell the end for reproduction of any reasonable quality, in fact it takes the loss of a relatively large number
of packets to create noticeable holes in outbound audio at anything but the highest compression levels.
Whether the control and complexity of RTP or the simplicity and speed of UDP will prove to be the
most effective way to carry datagram voice remains to be seen.
1.3.4
Keys to Successful Deployment
The large number of conﬁgurable variables and the many options within each make conﬁguring VoIP
networks a considerable challenge, especially since these networks’ main role is to replace some of the
most bulletproof networks in the world: those of the PSTN. Aside from performance issues, questions
of interoperability abound, particularly for those users who wish to deploy distributed VoIP networks
consisting of hardware and software from more than one vendor, and networks from more than one
provider.
One thing is certain, though: IP telephony is here to stay. Despite the challenges that network managers
face in order to reduce their IEC bills, in at least some applications the payoff is great enough to make
the decision to at least trial the technology obvious. The astute manager, however, remembers a few things:
• Few, if any, of the products currently available for VoIP networking work well “out of the box.”
Nearly everyone who has implemented gateways on either a point-to-point or multipoint basis
has a story to tell about the setup and conﬁguration of their system, and the shakedown and
subsequent adjustments, that had to occur before the network settled down. Almost as invariably,
though, they can recount the time that things began to work well, and now can point to users
who are happy with the price and performance of the VoIP network.
• All VoIP products aren’t the same. Vendors are scrambling to improve quality and add features,
and that translates into large variations in product lines — at least until the next revision is
introduced.
The good news is that there are many positive signs for those considering putting their trust into VoIP.
The current standards situation for components of VoIP products seems to be stabilizing. While any
emerging technology — especially ones with such high visibility — generates a large number of propri-
etary solutions which get narrowed down by the market, VoIP is one example of how vendors can
cooperate. Most of the standards for encoding (the ITU G-series) seem to be settling down for a long
period of maturity.
With regard to the network technologies in use, a new generation of network designers and engineers
feels more comfortable with IP than with any other technology — including voice trafﬁc. The ubiquity
of the Internet and of IP itself have created a large pool of experience from which managers can draw
when deploying VoIP. As for the future, a knowledge of the workings of Internet protocols is commonplace
among graduates of almost any technical program.
While the public telephone network has existed for years, fast public data networks have not existed
until recently, and new data networks are being constructed at a staggering rate. Many of these networks
will be suitable for voice trafﬁc, and thus can extend the reach of VoIP networking. And the rapid pace
of network improvement means that end-to-end latency will continue to drop, which can only mean
good things for the quality, and success, of VoIP.

© 2001 by CRC Press LLC
Acronyms
ATM — Asynchronous transfer mode
DSP — Digital signal processor
DTMF — Dual-tone multi-frequency
FCC — Federal communications commission
IEC — Interexchange carrier
IETF — Internet engineering task force
IP — Internet protocol
IP — Intelligent peripheral
ITSP — Internet (IP) telephony service provider
LAN — Local area network
LEC — Local exchange carrier
PBX — Private branch exchange
PSTN — Public switched telephone network
RSVP — Reservation protocol
RTP — Realtime protocol
UDP — User datagram protocol
VoIP — Voice over IP
WAN — Wide area network
1.4
Local Area Networks
John Amoss
1.4.1
Overview
1.4.1.1
Standards 
The Institute of Electrical and Electronics Engineers (IEEE) 802 Local and Metropolitan Area Network
Standards Committee has the basic charter to create, maintain, and encourage the use of standards for
local and metropolitan networks. In the IEEE 802 Committee context the term “local” implies a campus-
wide network and the term “metropolitan” implies intracity networks. The IEEE 802 Committee deﬁnes
interface and protocol speciﬁcations for access methods for various Local Area Network (LAN) and
Metropolitan Area Network (MAN) technologies and topologies. The project has had a signiﬁcant impact
on the size and structure of the LAN market. 
The standards are jointly published by the IEEE, the International Organization for Standardization
(ISO) and the International Electrotechnical Commission (IEC). An overview of the standards is pub-
lished by these bodies. [1,2]
1.4.1.2
Reference Model
Figure 1.4.1 relates the speciﬁc protocol layers deﬁned by the IEEE 802 Committee, which include
Physical, Media Access Control (MAC) and Logical Link Control (LLC) layers, to the layers of the Open
Systems Interconnection (OSI) Reference Model. [3] The protocol architecture shown in Figure 1.4.1,
including the Physical, MAC and LLC layers, is generally referred to as the IEEE 802 Reference Model. 
Working from the bottom up, the Physical layer of the IEEE 802 Reference Model corresponds to the
Physical layer of the OSI Reference Model and includes the following functions.
• Encoding/decoding the signals to be transmitted in a manner appropriate for the particular
medium, e.g., the use of Manchester or Non-return to Zero encoding schemes;
• Achievement of synchronization, e.g., by the addition of a preamble ﬁeld at the beginning of a
data frame; 

© 2001 by CRC Press LLC
• Bit transmission and reception; 
• Speciﬁcation of the physical and electro/optical characteristics of the transmission media (e.g.,
ﬁber, twisted pair wire); and 
• Network topology (e.g., bus, ring).
Above the Physical layer are functions concerned with providing the frame transmission service to LAN
users. Such functions include the following.
• Governing access to the LAN transmission medium.
• Performing error detection (e.g., via addition of a Frame Check Sequence ﬁeld);
• Assembling the frame for transmission; and
• Upon reception, performing address recognition.
These functions are collectively associated with a MAC sublayer, shown in Figure 1.4.1. As indicated in
the ﬁgure, a number of MAC layers are deﬁned within the IEEE 802 Reference Model including access
control techniques such as Carrier Sense Multiple Access/Collision Detection (CSMA/CD) — also gen-
erally referred to as Ethernet — Token Bus and Token Ring.
Finally, the Logical Link Control (LLC) layer is responsible for providing services to the higher layers
regardless of media type or access control method (such as those speciﬁed for CSMA/CD, Token Bus,
Token Ring, and so on). The LLC layer provides a High-level Data Link Control (HDLC)-like interface
to the higher layers and essentially hides the details of the many MAC schemes shown in Figure 1.4.1
from the higher layers. The LLC layer provides a multiplexing function, supporting multiple connections,
each speciﬁed by an associated destination service access point (DSAP) and source service access point
(SSAP), discussed later. As shown in Figure 1.4.1, the LLC layer provides both connectionless and con-
nection-oriented services, depending on the needs of the higher layers. 
1.4.1.3
Overview of the Major MAC Standards
Since its inception at Xerox Corporation in the early 1970s, the carrier sense multiple access with collision
detection (CSMA/CD) method, also commonly termed Ethernet, has been the dominant LAN access
control technique. The CSMA/CD method was the ﬁrst to be speciﬁed by the IEEE, under the IEEE 802.3
working group, and was closely modeled after the earlier joint Digital/Intel/Xerox (DIX) Ethernet spec-
iﬁcation. [4] Ethernet has, by far, the highest number of installed ports and provides the greatest cost
performance relative to other access methods such as Token Ring, Fiber Distributed Data Interface (FDDI)
and the newer Asynchronous Transfer Mode (ATM) technology. Recent and in-progress extensions to
Ethernet include Fast Ethernet, which, under the auspices of the IEEE 802.3u working group, increased
Ethernet speed from 10 Mbps to 100 Mbps thereby providing a simple, cost-effective option for higher
speed backbone and server connectivity, and Gigabit Ethernet, which under the auspices of the 802.3z
working group increased the speed to 1000 Mbps.
FIGURE 1.4.1
IEEE 802 reference model.

© 2001 by CRC Press LLC
The IEEE 802.4 Token Bus speciﬁcations were developed primarily in response to requirements for
the deterministic performance of a token passing scheme, coupled with a bus-oriented topology. The use
of a broadband technology option provided the additional beneﬁts of increased bandwidth, geographic
coverage, and number of terminations.
The IEEE 802.5 Token Ring speciﬁcation was developed with major support from IBM and reﬂected
IBM’s perspective on local area networking. Improvements over the IEEE 802.3 scheme include deter-
ministic performance and the speciﬁcation of a priority mechanism.
As shown in Figure 1.4.1, work has been completed in several new technology areas including wireless
LANs (IEEE 802.11) [5] and Cable Modems (IEEE 802.14). [6]
Due to their wide market acceptance, this section focuses on the details of the IEEE 802.3 (CSMA/CD)
and 802.5 (Token Ring) speciﬁcations. The section also addresses the Logical Link Control layer and
presents an overview of building wiring considerations which would ensure that the building cabling
meets the requirements of the various LAN types.
1.4.2
IEEE 802.3 (CSMA/CD) Speciﬁcs
1.4.2.1
Frame Structure
As mentioned, the carrier sense multiple access with collision detection (CSMA/CD) method was the
ﬁrst to be speciﬁed by the IEEE and was closely modeled after the Digital/Intel/Xerox (DIX) Ethernet
speciﬁcation. Although there are differences between the Ethernet and the 802.3 speciﬁcations, manu-
facturers now typically produce hardware that can support both, so that effectively the two are compatible.
Differences in the packet format are resolved in ﬁrmware for a particular implementation. We use the
terms Ethernet and IEEE 802.3 CSMA/CD interchangeably.
The frame format in the original DIX speciﬁcation is shown in Figure 1.4.2(a). Frame ﬁelds are as
follows.
• Preamble — To allow synchronization by the receiving station and to indicate the start of frame,
the frame starts with an eight byte sequence, the ﬁrst seven of which have the format (10101010),
and the eighth the format (10101011). 
• Source and destination addresses are 48 bits each (a little-used option allows for 16 bits) and have
the structure shown in Figure 1.4.2(b) except for a minor variation in the second bit of the address.
• EtherType — The EtherType ﬁeld (16 bits) allows for the multiplexing of data streams from
different higher level protocols and identiﬁes the particular higher level protocol data steam carried
FIGURE 1.4.2
DIX and IEEE 802.3 frame formats.

© 2001 by CRC Press LLC
by this frame, e.g., an EtherType of Ox08-001 indicates a frame carrying an IP datagram. Values
for the EtherType ﬁeld can be found in [7].
• Data — The Data ﬁeld carries the service data unit from the higher layer protocol entity and
ranges in length from 46 (including an added PAD ﬁeld if the service data unit is less than 46
bytes) to a maximum of 1500 bytes. 
• Frame Check Sequence (FCS) — Finally, a four-byte FCS ﬁeld is added for error detection
purposes.
The IEEE 802.3 frame format is shown in Figure 1.4.2(b). The major difference in format arises from
the need to accommodate other MAC speciﬁcations under the IEEE umbrella which may have no
equivalent of the EtherType ﬁeld. As a result, this multiplexing capability is included in the next higher
layer of the IEEE 802 Reference Model, the LLC layer (see Figure 1.4.1). The method used to provide this
additional protocol information is the Subnetwork Access Protocol (SNAP). A SNAP encapsulation is
indicated by the LLC layer SSAP and DSAP ﬁelds both being set to OxAA. The SNAP header is ﬁve bytes
long: the ﬁrst three bytes consist of an organization code, which is assigned by the IEEE; the second two
bytes use the EtherType value set from the Ethernet speciﬁcations. Using this scheme, the multiplexing
service afforded by the EtherType ﬁeld is available at the LLC layer, independent of the individual MAC
layer capabilities. Note that several layers of multiplexing are available at the LLC layer; one provided by
the LLC Destination Address/Source Address ﬁelds in Figure 1.4.2(b), and the other by the LLC/SNAP
ﬁelds shown in the ﬁgure (which include the EtherType ﬁeld). Again, when the length of MAC layer data
ﬁeld is less than 46 bytes, a PAD ﬁeld is added to ensure a minimum data plus PAD ﬁeld length of 46
bytes. The PAD ﬁeld consists of an arbitrary array of bits.
1.4.2.2
Sample Frame Transmission
For a transmission media operating at a data rate of 10 Mbps, typical of many 802.3 speciﬁcations,
Figure 1.4.3 shows the successful transmission of a frame between two stations at the ends of the cable,
from station A (shown on the left) to station B (shown on the right). Cable length is assumed to be 500
meters, the approximate maximum length for a number of IEEE 802.3 conﬁgurations (per Section 13 of
[8]). A frame size of 1518 bytes is assumed, also the maximum as per the IEEE 802.3 speciﬁcation. From
the ﬁgure, station A begins transmitting at time t = 0 and some time later the leading edge of the signal
FIGURE 1.4.3
Example of successful frame transmission.
1This notation indicates a string of bytes (groups of eight bits) with the values of the bytes given in hexadecimal
form; thus Ox08-00 represents the two bytes 00001000–00000000.

© 2001 by CRC Press LLC
appears at station B. This time is determined by the propagation speed of the signal on the particular
media, with the speeds for a number of media shown in Table 1.4.1. Assuming a propagation speed of
.77c, where c is the speed of light (3 × 108 m/s), yields a propagation delay of about 2.2 µs for the example
in Figure 1.4.3.
The total signal transmission time, neglecting a short initial synchronization period when the preamble
and start of frame delimiter are transmitted is 
Thus station A completes transmitting the signal at t = 1214.4 µs and station B begins receiving the signal
at t = 2.2 µs and receives the entire signal at time t = 1216.6 µs.
After a brief delay period to allow recovery time for other stations and the physical medium, termed
the interframe gap, another frame can be transmitted if available. An interframe gap of 9.6 µs or 96 bit
times for a 10 Mbps implementation is speciﬁed by the standard. This value is chosen to account for
variability in the gap as frames travel over the media and connecting repeaters (discussed below). This
variability occurs because two successive frames may experience different bit loss in their preambles. If
the ﬁrst packet experiences greater bit loss than the second, the gap will shrink as the repeater reconstructs
the preamble and therefore introduces delay. If the second frame experiences greater bit loss, the gap will
expand.
1.4.2.3
Carrier Sense Multiple Access
A simple addition to the above scheme is to require each station to “listen before talking,” i.e., require a
station to sense the medium to determine if another station’s signal is present and defer transmission if
this is the case. This situation is shown in Figure 1.4.4 where a third station at the middle of the cable
TABLE 1.4.1
Minimum Propagation Speeds 
for Sample Media
Media Type
Minimum 
Propagation Speed
Coax (10BASE5)
0.77 c
Coax (10BASE2)
0.65 c
Twisted Pair (10BASE-T)
0.585 c
FIGURE 1.4.4
Use of carrier sense multiple access (CSMA).
1518
8
10
1214 4
 bytes
(
) × (
)
=
µ
bits bytes
Mbps
s
.

© 2001 by CRC Press LLC
begins sending at time t = 0. Due to signal propagation delays, signal reception begins at both A and B
at time t = 1.1 µs. In the ﬁgure, while sensing the presence of carrier from C, A and B both receive frames
from higher layers to transmit but, adhering to the CSMA scheme, defer transmitting until some time
after station C’s transmission is completed. 
For typical CSMA schemes, a number of strategies can be employed to determine when to begin
transmitting after deferring to a signal already on the medium. These strategies typically involve invoking
one of the persistency schemes shown in Table 1.4.2. The persistency parameter “p” relates to the prob-
ability that a station sends its frame immediately after the medium is sensed idle. To obtain maximum
channel utilization, the choice of the persistency value, 0.1, 0.2, 0.3, …, etc. is dependent on the trafﬁc
offered by the stations. A low level of trafﬁc would operate best with a persistency value, p, near 1.0 (here,
typically only a single station will be ready to send and thus should send immediately with high proba-
bility) and a high level of trafﬁc would operate best with a lower value of p (here multiple stations will
likely be ready to send and the lower value of p will make it more likely that only one station attempts
to transmit). It should be noted that the above retransmission algorithm is not related to the binary
exponential backoff algorithm discussed below, associated with resolving collisions. Also of note is that
the IEEE 802.3 standard speciﬁes the 1-persistent scheme, ensuring a collision if two or more stations
are deferring to an ongoing transmission.
1.4.2.4
Adding Collision Detection
A problem with the CSMA scheme is depicted in Figure 1.4.5, where stations A and B both have something
to send at t = 0 and, sensing the medium idle (no carrier) both begin transmission. For example, this
case would occur if both have been deferring to another station transmitting on the medium and used
the 1-persistent backoff scheme. At some short time later, the signals will collide at stations A and B (and
at all other stations on the medium). In this case, no useful information is transferred for the entire
transmission time of the frame, approximately 1200 µs for a frame of maximum length. 
A solution to this problem is the addition of the collision detection mechanism depicted in Figure 1.4.6.
The addition of such a mechanism reduces the wasted transmission time as both stations will stop
transmitting upon detection of the collision. Here the stations have the added capability of detecting the
occurrence of a “collision” of the two signals on the medium. With this added functionality, the stations
can stop transmitting upon detecting collisions and immediately undertake a backoff scheme to allow
one station to capture the medium.
TABLE 1.4.2
Typical Persistency Algorithms
Persistency Scheme
Description
Non-persistent
• idle ⇒ transmit
• busy ⇒ wait random time and repeat
1-persistent 
• idle ⇒ transmit
• busy ⇒ wait until idle then transmit immediately
(Note that if 2 or more stations are waiting to transmit, a collision is guaranteed)
p-persistent*
• idle ⇒ transmit with probability p and delay one time unit with probability 1-p; time unit is 
typically the maximum propagation delay
• busy ⇒ continue to listen until channel is idle and repeat above for idle
• delayed one time unit ⇒ repeat above for idle
* Issue is choice of p
• Need to avoid instability under heavy load. 
• If n stations are waiting to send, the expected number transmitting is np. np > 1 ⇒ collision is likely. 
• New transmissions will also begin to compete with retries and network will collapse: all stations waiting to
transmit, constant collisions, no throughput.
• Thus np must be <1; but heavy load means p must be small and time will be wasted even on a lightly loaded
line, e.g., p = 0.1 ⇒ on average, will transmit in tenth interval on an idle line.

© 2001 by CRC Press LLC
1.4.2.4.1
Collision Backoff Scheme
Two stations, A and B, implementing the CSMA/CD media access control technique are shown in
Figure 1.4.7. If, as shown in the ﬁgure, they both sense the media idle and begin to transmit at t = 0, a
collision will occur and IEEE 802.3 speciﬁes a “truncated binary exponential backoff” randomization
scheme so that one of the stations can obtain control of the media. As shown in the ﬁgure, with this
scheme, ﬁrst one of two “slots” is chosen randomly by each station to attempt to capture the medium.
The slot time is chosen based on factors that include the round trip transmission time between two
stations at the ends of the medium, and the time required to detect a collision. It is speciﬁed in bit times;
the IEEE 802.3 standard speciﬁes a slot time of 512 bit times (51.2 µs for a 10 Mbps system). If a collision
FIGURE 1.4.5
Wasted resources with CSMA.
FIGURE 1.4.6
CSMA/CD.

© 2001 by CRC Press LLC
occurs again (i.e., they both choose the same slot), one of four slots is chosen in the next attempt; then
eight; then sixteen; etc. The number of slots grows in this manner to 210 and truncates at this value. After
a total of 16 retransmission attempts fail, this event is reported as an error. 
With the CSMA/CD scheme, for reasonable trafﬁc levels, a station should capture the medium in a
rather short time, especially when compared to the CSMA scheme. For instance, consider the two stations
in Figure 1.4.6 implementing a 1-persistent scheme (the recommended IEEE 802.3 scheme used after
deferring to a transmitting station). Each will begin to transmit when station C stops transmitting and
thus will suffer a collision on this ﬁrst transmission. The binary exponential backoff scheme of Figure 1.4.7
will yield the following results.
For the two stations competing for the medium, the following outcomes are equally likely during the
ﬁrst retransmission.
(0,0), i.e., station A picks slot 0 and station B also picks slot 0, 
(0,1), i.e., station A picks slot 0 and station B picks slot 1, 
(1,0), i.e., station A picks slot 1 and station B picks slot 0,
(1,1), i.e., station A picks slot 1 and station B also picks slot 1.
Two of these outcomes, (0,1) and (1,0), will result in a station capturing the medium, station A in the
ﬁrst of these and station B in the latter. Two of the outcomes, (0,0) and (1,1) result in collisions in slots
0 and 1, respectively. Thus with a probability of _ = 0.5, one of the stations will capture the medium in
this ﬁrst retransmission period.
FIGURE 1.4.7
Retransmission attempts with the binary exponential backoff scheme.

© 2001 by CRC Press LLC
If there is a collision for the ﬁrst retransmission, the second retransmission uses 4 slots chosen at
random by the stations (numbered 0, 1, 2 and 3 in Figure 1.4.7) , resulting in 16 possible outcomes.
Using similar notation as above, these outcomes are
(0,0), (0,1), (0,2), (0,3)
(1,0), (1,1), (1,2), (1,3)
(2,0), (2,1), (2,2), (2,3)
(3,0), (3,1), (3,2), (3,3)
and only four of these, (0,0), (1,1), (2,2), and (3,3), result in collisions yielding a probability of 12/16 or
_ for a successful outcome. Thus the probability of exactly two retransmissions is (prob of collision on
ﬁrst retransmission) × (prob of success on second) = _ x _ = 3/8 = .375.
Similarly, the probability of exactly three retransmissions is 
and that for four is
The likelihood of more than four transmissions is rather small. 
Finally, the average number of retransmissions is 
On average then, with two stations competing for the medium, one will capture the medium during
the second retransmission attempt. Note that this saves medium resources when compared to Figure 1.4.5
where over 1200 µs are wasted due to the collision and additional time will be spent in some sort of
backoff scheme.
It is interesting to note that for 3 stations competing, media capture will occur more quickly. Here,
the three stations will collide on the ﬁrst transmission attempt and the eight possible outcomes for the
ﬁrst retransmission are (0,0,0), (0,0,1), (0,1,0), (0,1,1), (1,0,0), (1,0,1), (1,1,0) and (1,1,1), with only
(0,0,0) and (1,1,1) resulting in unsuccessful outcomes. For example, the (0,0,1) outcome will result in
media capture: stations A and B will collide in slot 0 and station C will capture the medium in slot 1.
Thus the probability of one station capturing the medium in the ﬁrst retransmission is _, greater than
the case with two stations competing. The average number of retransmissions for this case can be shown
in the above manner to be on the order of 1.27. The reduced average number of retransmissions in the
case of three stations competing is somewhat of an anomaly; for more stations competing, the average
number of retransmissions steadily increases. Of course, the average waiting time for a particular station
to capture the medium will increase with the number of stations competing for the medium.
1.4.2.5
CSMA/CD System Components
As mentioned, the IEEE 802.3 speciﬁcations support multiple media types, including coaxial cable,
twisted pair, and ﬁber. Thus one of the component interfaces will of necessity vary with the media type.
For example, the physical media dependent interface for twisted pair differs in a number of respects from
that for ﬁber, including the physical connector (or plug), the electrical vs. optical nature of the interface
1 2
1 4
7 8
0 109
×
×
= .
,
1 2
1 4
1 8
240 256
0 0146
×
×
×
= .
.
i
i =
∞
∑× (
) =
×
(
)+
×
(
)+
×
(
)+
×
(
)…
=
+
+
+
… =
1
1
0 5
2
0 375
3
0 109
4
0 0146
0 5
0 75
0 327
0 058
1 635
prob of i retransmission
.
.
.
.
.
.
.
.
.

© 2001 by CRC Press LLC
and the encoding scheme (translating a logical sequence of bits to the electrical/optical signal on the
medium), e.g., Manchester or Non-return to Zero (NRZ) schemes. In a number of IEEE implementations,
this interface is remote from the station itself and the IEEE speciﬁcation provides a media-independent
manner of extending the station interface to the media-dependent interface. 
This general situation is shown in Figure 1.4.8, where the various functional components that make
up the CSMA/CD system are also shown. The Medium Dependent Interface (MDI) is shown on the right
of the ﬁgure and provides the direct electrical/optical connection. Examples include an 8 pin connector
(RJ-45 telephone style jack) for twisted pair, a coaxial cable clamp for coax medium and a spring-loaded
bayonet connector (termed an ST connector) for ﬁber. The Medium Attachment Unit (MAU) shown in
the ﬁgure, also commonly known as a transceiver, is also speciﬁc to the type of medium and performs
signal encoding (e.g., Manchester or NRZ). Transceivers also contain a jabber protection circuit that
protects the network from a station or transceiver that is transmitting frames whose length exceeds the
maximum allowed.
The interconnection of the station to the remote transceiver is accomplished by the Attachment Unit
Interface (AUI), also termed the transceiver cable. This 15-lead cable is media independent and carries
data receive, data transmit and collision detection signals along with power from the station to the
transceiver. The maximum length of the transceiver cable is 50 m. The associated 15 pin AIU connector
is commonly found on Ethernet station cards.
It is important to note that many implementations include the transceiver as part of the Ethernet
station card. For example, twisted pair implementations use this scheme. In this case, only the media-
dependent interface in visible on the station Ethernet card; e.g., the RJ-45 type interface is commonly
found on interface cards connecting to twisted pair medium.
1.4.2.6
Example Implementations
IEEE 802.3 standards are characterized by a shorthand notation to facilitate their description. The
notation (e.g., 10BASE5) is composed of three elements as shown in Figure 1.4.9, indicating the trans-
mission rate of the system in Mbps, the modulation scheme, baseband or broadband, and the maximum
length of the segment in hundreds of meters. With standards adopted more recently, such as 10BASE-T,
the IEEE has been more descriptive with its notation. For example, the “T” in the 10BASE-T notation
is short for “twisted-pair wiring.” 
This section describes the most commonly implemented versions of the IEEE 802.3 speciﬁcation.
FIGURE 1.4.8
CSMA/CD system components.

© 2001 by CRC Press LLC
1.4.2.6.1
10BASE5
10BASE5 was the ﬁrst version of the IEEE speciﬁcation to be developed and it most closely resembles
the earlier DIX versions 1 and 2. [4] The 10BASE5 speciﬁcation employs a “thick Ethernet” 50-ohm
coaxial cable. While this cable is difﬁcult and relatively expensive to install, it provides advantages over
other implementations in terms of distance and the number of terminations permitted for each segment.
This speciﬁcation uses the standard outboard transceiver option discussed above: the station adapter
board connects to the standard Attachment Unit Interface (AUI) cable; this in turn is connected to the
transceiver which is connected to the Ethernet trunk cable via a “vampire” tap. Up to 100 devices can
be placed on a 500-meter segment, with a maximum of 1024 devices on a multi-segment network,
discussed below. 
1.4.2.6.2
10BASE2
10BASE2 (also known as “thin Ethernet” or “Cheapernet”) employs a thin ﬂexible coaxial cable (RG-58).
In earlier implementations, the transceiver functions were onboard the station and the connection to the
station was by means of a media-dependent BNC “T” connector. To provide the ﬂexibility to use the
station board for either the 10BASE 5 or 10BASE2 systems, station boards have been developed which
provide options for both external 10BASE5 “vampire” taps and 10BASE2 BNC connectors. Board man-
ufacturers commonly provide boards with built-in transceivers that can be switched on or off for a
particular application.
The standard 10BASE2 LAN can support only 30 terminations on each coaxial cable segment of 185
meters. While this may seem like a constraint, it is often adequate for most work area environments.
Where a requirement exists for interconnecting multiple work areas, or work areas with multiple 10BASE2
segments, a backbone 10BASE5 segment can be employed to provide intersegment connectivity. Table 3
highlights the differences between the 10BASE5 and 10BASE2 systems.
1.4.2.6.3
1BASE5
This standard approach was contributed by AT&T to accommodate its earlier Starlan products. It operates
at 1 Mbps, and as such is often most useful for small work areas or low-trafﬁc environments. 1BASE5
FIGURE 1.4.9
IEEE 802.3 nomenclature.
TABLE 1.4.3
802.3 10BASE5/10BASE2 Comparison
10BASE5
10BASE2
Common name
802.3 “Ethernet”
Cheapernet, THIN Ethernet, 
THINWIRE Ethernet, etc.
Type of cable
50 Ω Thick dual shield
50 Ω RG-58
Maximum segment length
500 m.
185 m.
Spacing of devices on cable
2.5 m. minimum
0.5 m. minimum
Maximum number of taps for a segment
100
30
Maximum number of full repeaters in a 
path between two stations
2
2
Type of taps
Vampire or coax
BNC “T” connector for “daisy chaining”

© 2001 by CRC Press LLC
also employs inexpensive twisted-pair wire interconnected through a hierarchical system of concentrator
hubs. The hubs emulate a bus conﬁguration by broadcasting data and collision information on all ports.
1.4.2.6.4
10BASE-T
One of the most important developments in the IEEE 802.3 area was the speciﬁcation of the 10 Mbps
unshielded twisted-pair (UTP) Ethernet system, 10BASE-T. Virtually every vendor active in the Ethernet
market now offers 10BASE-T products.
Like the 1BASE5 speciﬁcation, this system uses a hub concentrator to interconnect multiple stations
and emulate bus operation. These implementations are limited to 100-meter segments due to the greater
attenuation and signaling difﬁculties of twisted pair. This does not present any unusual problems since
these connections only reach to the communications closet. From there, ﬁber and coax segments can be
used to concatenate and extend the LAN system. 10BASE-T systems use one twisted pair for transmitting
data and a separate pair for receiving. “Collisions” are detected by sensing the simultaneous occurrence
of a signal on both the transmit and receive pairs. 
It is imperative, however, that organizations planning these networks have their existing twisted-pair
wire certiﬁed for both attenuation and capacitance before making any assumptions on its reuseabilty.
1.4.2.6.5
10BASE-FL
The 10BASE-FL speciﬁcation allows for the use of two ﬁber optic cables as the medium, one for signal
transmission and one for reception. Such a medium allows advantages of greater distances, e.g., the
standard allows segment lengths of up to 2000 meters, evolution to higher transmission speed, and
isolation from electromagnetic radiation. The system components are identical to those shown in
Figure 1.4.8 with the use of a ﬁber-optic Medium Attachment Unit (MAU).
1.4.2.6.6
10BROAD36
The 10BROAD36 implementation uses much of the same hardware as the baseband implementations.
The speciﬁcation enables an organization to use its existing workstation boards for connection to either
a baseband or broadband system. The essential difference is the substitution of a broadband electronics
unit and a passive broadband tap for the baseband MAU. The primary function of the broadband
electronics unit is to create the frequency-derived data channels and to monitor for collisions. It also
converts the signals from the baseband-coded signal on the AUI to the analog signal necessary on the
broadband channel. Workstations can be placed up to 1800 meters from the “head-end” of the broadband
cable plant. By placing the head-end in the center of the conﬁguration, workstations can be installed up
to 3600 meters from each other. In recent years, this standard has been less frequently used.
1.4.2.7
Topology Extensions
 1.4.2.7.1 Repeaters
Repeaters regenerate the signals from one LAN segment for retransmission to all the others. The earliest
repeaters were simple two-port devices that linked a couple of coaxial cable segments. Later, repeaters
evolved to multiport devices deployed as the hub of a star topology. Since with repeaters, all segments
are part of a uniﬁed LAN, the nature of the shared channel must be preserved by broadcasting all
information to all attached devices. An aspect of these repeaters is that they must be capable of retrans-
mitting collisions as well as data frames. In the case of IEEE 802.3 CSMA/CD LANs, physical LAN segment
connection standards for repeaters are well developed and mature. The latest speciﬁcations for imple-
mentation of 10BASE-T repeaters are contained in the IEEE 802.3 speciﬁcations (see Section 9 of [8]).
In addition to the functions described above, repeaters can provide an optional “partitioning” feature
between segments. This function is designed to address an abnormal situation such as a cable break or
network card failure. Thus, if conditions on a given segment are causing an extensive proliferation of
collisions, the rest of the LAN can be protected from this anomaly. The repeater will count the number
of collisions from the source segment and when excessive, isolate these from transmission to the next
segment. 

© 2001 by CRC Press LLC
1.4.2.7.2
Switched Hubs (Ethernet Switches)
In Ethernet switching, the interconnecting device (termed a switching hub) has intelligence to use the
MAC-layer address of a received frame to determine the speciﬁc port on which the destination station
is attached and transmit the frame on only that port. No other stations are aware of the frame. If frames
arrive destined for a busy port, that port can momentarily holds them in its buffer; the size of port buffers
differs by vendor from a few hundred to more than a thousand packets. When the busy port becomes
free, frames are released from the buffer and sent to the port. This mechanism works well unless the
buffer overﬂows, in which case packets are lost. To avoid this, some vendors offer a throttling capability;
when a port’s buffer begins to ﬁll up, the hub begins to transmit packets back to the workstations. This
effectively stops the stations from transmitting and relieves the congested state.
Some LAN switching products offer a choice of packet-switching modes, e.g., fast forward and “store
and forward.” These modes affect the amount of packet latency, the time from which the ﬁrst byte of a
packet is received until that byte is forwarded. Each mode reads a certain number of bytes of a packet
before forwarding. This creates a trade-off among latency, collision, and error detection. The greater
number of bytes read, the greater the latency, but the fewer errored or collision terminated frames
propagated through the network. The fast-forward mode passes packets shortly after receiving the
Destination Address portion of the Ethernet frame (see Figure 1.4.2) and, based on this ﬁeld, determines
the appropriate destination port. In this mode, typical latencies are on the order of tens of µs for a 10
Mbps Ethernet system. Store-and-forward mode receives entire frames and performs error detection via
the FCS ﬁeld (see Figure 1.4.2) before forwarding, resulting in increased latency but maximizing frame
error detection. Here, for a maximum length frame, latency will be greater than 1200 µs. Some LAN
switching devices support only 1 address per port, while others support 1500 or more. Some devices are
capable of dynamically learning port addresses and allowing or disallowing new port addresses. Disal-
lowing new port addresses enhances hub security; in ignoring new port addresses, the corresponding
port is disabled, preventing unauthorized access. These and other techniques used in conjunction with
port switching enhance overall network performance by eliminating the contention problem that occurs
in shared Ethernet networks.
1.4.2.7.3
Multi-segment Guidelines
The IEEE 802.3 speciﬁcations provide guidelines for the number and types of segments that can be
interconnected via repeaters and switch hubs. A number of example conﬁgurations are presented which
would be typical of many implementations. For example, one possible conﬁguration that taxes the
CSMA/CD conﬁguration guidelines has a distance of about 1800 meters between stations, with three 500
meter segments and two shorter segments interconnected by four repeaters. For more complex cases, the
speciﬁcations provide guidelines for calculating system parameters such as the round-trip delay time and
interframe gap shrinkage to ensure that these parameters are within allowed limits.
1.4.2.8
Higher Speed Extensions (100 Mbps and 1000 Mbps)
1.4.2.8.1
Fast Ethernet (100 Mbps) Overview
100 Mbps CSMA/CD operation, also termed Fast Ethernet, is speciﬁed in the IEEE 802.3u supplement
to the standard. While attaining a ten-fold increase in transmission speed, other important aspects,
including the frame format and the CSMA/CD access control scheme, remain unchanged from a 10
Mbps system, making the transition to higher speeds straightforward for network managers. The 100
Mbps speciﬁcation uses Ethernet’s traditional CSMA/CD protocol and is designed to work with existing
medium types, including Category 3 and Category 5 twisted-pair and ﬁber media. In addition, Fast
Ethernet will look identical to lower-speed Ethernet from the LLC layer upward. Since Fast Ethernet
signiﬁcantly leverages existing Ethernet technology, network managers will be able to use their existing
knowledge base to manage and maintain Fast Ethernet networks.
The Fast Ethernet speciﬁcations include mechanisms for Auto-Negotiation of the media speed. This
makes it possible to provide dual-speed Ethernet interfaces that can be installed and run automatically
at either 10 Mbps or 100 Mbps. 

© 2001 by CRC Press LLC
There are three media varieties that have been speciﬁed for transmitting 100-Mbps Ethernet signals:
100BASE-T4, 100BASE-TX, and 100BASE-FX. The third part of the identiﬁer provides an indication of
the segment type. 
“T4” is a twisted-pair segment that uses four pairs of telephone-grade twisted-pair wire. 
“TX” segment type is a twisted-pair segment that uses two pairs of wires and is based on data-grade
twisted-pair physical medium standards, covered later in this section. 
“FX” segment type is a ﬁber-optic link segment that uses two strands of ﬁber cable and is based on
the ﬁber optic physical medium standard developed by ANSI. 
The TX and FX medium standards are collectively known as 100BASE-X. The 100BASE-TX and
100BASE-FX media standards used in Fast Ethernet are both adopted from physical media standards
ﬁrst developed by ANSI for the Fiber Distributed Data Interface (FDDI) LAN standard (ANSI standard
X3T9.5), and are widely used in FDDI LANs. Rather than “re-inventing the wheel” when it came to
signaling at 100 Mbps, the Fast Ethernet standard adapted these two ANSI media standards for use in
the new Fast Ethernet medium speciﬁcations. The T4 standard was also provided to make it possible to
use lower-quality twisted-pair wire for 100 Mbps Ethernet signals.
1.4.2.8.2
Gigabit Ethernet (1000 Mbps) Overview
Gigabit Ethernet, under the auspices of the IEEE 802.z working group, builds on the CSMA/CD MAC
scheme and increases the transmission speed to 1000 Mbps. A key feature of Fast Ethernet implementa-
tions is the autoconﬁguration capability, and Gigabit Ethernet solutions providing 10/100/1000 Mbps
operation allow comparable features.
It should be noted that, as in the case for Fast Ethernet, a number of challenges involved in achieving
rapid time to market for Gigabit Ethernet were resolved by merging existing technologies:
1. IEEE 802.3 CSMA/CD and 
2. ANSI X3T11 Fibre Channel; Fibre Channel encoding/decoding integrated circuits (ICs) and optical
components were readily available and optimized for high performance at relatively low cost. 
Leveraging these two technologies meant that the Gigabit Ethernet standard could take advantage of
the existing, proven high-speed physical interface technology of Fibre Channel while maintaining the
IEEE 802.3 Ethernet frame format, backward compatibility for installed media, and use of CSMA/CD.
This strategy helped minimize complexity and resulted in a technology that could be quickly standardized.
Figure 1.4.10 shows how key components from each technology have been leveraged to form Gigabit
Ethernet. As a result the Gigabit Ethernet standard based on ﬁber optics for the MAC and Physical layers
has progressed rapidly. Unshielded twisted-pair media did not have the advantage of a proven, existing
technology base and the standards remained in further development; this work is under the auspices of
the IEEE 802.3 Working Group and referred to as 1000BASE-T.
Physical Layer Characteristics of Gigabit Ethernet 
The initial Gigabit Ethernet speciﬁcation from the IEEE 802.3z working group calls for three transmission
media: single-mode and multimode ﬁber and balanced shielded 150-ohm copper cable. There are two
supported types of multimode ﬁber: 62.5-micron and 50-micron diameter ﬁbers. The IEEE 802.3ab
committee is examining the use of unshielded twisted pair (UTP) cable for Gigabit Ethernet transmission
(1000BASE-T). The distances for the media supported under the IEEE 802.3z standard and those pro-
jected for the IEEE 802.3ab are summarized in Figure 1.4.11.
Fiber Optic Media (1000Base-SX and 1000Base-LX)
As mentioned, the Fibre Channel physical medium dependent speciﬁcation was employed for Gigabit
Ethernet to speed standardization. This standard provides 1.062 gigabaud in full duplex mode and Gigabit
Ethernet will increase this rate to 1.25 gigabaud with an 8B/10B encoding scheme allowing a data
transmission rate of 1000 Mbps. In addition, the connector type for Fibre Channel was also speciﬁed for
both single-mode and multimode ﬁber. 

© 2001 by CRC Press LLC
The standard supports two laser types, a short-wave laser type (termed 1000Base-SX) and a long-wave
laser type (termed 1000Base-LX). Both short-wave and long-wave lasers are supported over multimode
ﬁber. There is no support for short-wave laser over single-mode ﬁber. The key issues between the use of
long-wave and short-wave laser technologies are cost and distance. Short-wave lasers are readily available
since variations of these lasers are used in compact-disc technology. Long-wave lasers take advantage of
attenuation dips at longer wavelengths in the cable and suffer lower attenuation. The net result is that
short-wave lasers will cost less, but traverse a shorter distance. In contrast, long-wave lasers will be more
expensive but will traverse longer distances. 
FIGURE 1.4.10
Gigabit Ethernet and the ANSI Fibre Channel Standard.
FIGURE 1.4.11
Distance speciﬁcations for gigabit ethernet media.

© 2001 by CRC Press LLC
The 62.5-micron ﬁber is typically seen in vertical campus and building cable plants and has been used
for Ethernet, Fast Ethernet, and FDDI backbone trafﬁc. However, this type of ﬁber has a lower modal
bandwidth (the ability of the cable to transmit light), especially with short-wave lasers. This means that
short-wave lasers over 62.5-micron will generally traverse shorter distances. The 50-micron ﬁber has
signiﬁcantly better modal bandwidth characteristics and will be able to traverse longer distances with
short-wave lasers relative to 62.5-micron ﬁber. 
150-Ohm Balanced Shielded Copper Cable (1000Base-CX) 
For shorter cable runs (of 25 meters or less), Gigabit Ethernet will allow transmission over a new type
of special balanced shielded 150-ohm cable (termed 1000Base-CX). Because of a distance limitation of
25 meters, this cable will likely have limited use. 
1.4.2.9
Full Duplex Operation
The previous discussion has focused on half-duplex operation, where only a single communications
channel was available and thus data could be transmitted in only one direction at a time. The CSMA/CD
Media Access Control mechanism was required to determine which station could use the single channel.
Full-duplex is an optional point-to-point mode of operation between a pair of devices allowing simul-
taneous communication between the devices and thus a doubling of aggregate capacity. Two separate
communications channels are needed in this case to allow both stations to simultaneously transmit and
receive. Thus the allowed physical media for this operation are only those with the capability of supporting
two simultaneous channels, e.g., 10BASE-T provides independent transmit and receive data paths that
can be simultaneously active. Full duplex operation cannot be supported on coaxial cable systems since
they do not provide independent transmit and receive data paths. The optional full duplex mode of
operation is speciﬁed by the 802.3x supplement to the standards. 
1.4.3
IEEE 802.2 Logical Link Control Layer
The IEEE 802.2 Logical Link Control (LLC) layer speciﬁcations [9] include those Data Link Layer
functions that are common to all 802 LAN MAC sublayer alternatives. The LLC frame format is shown
in Figure 1.4.2.
Three basic types of service are deﬁned in the standard.
Type 1 (Connectionless)
This service provides a best-effort delivery mechanism between origin and destination nodes. No call or
logical circuit establishment procedures are invoked. Each frame is treated as an independent entity by
the network. The type of frame used to provide this service is the unnumbered type; no ﬂow control or
acknowledgments are provided with this service. If the packet does not arrive at the destination, it is the
responsibility of higher layers to resolve the problem through time-outs and retransmission. This type
of service would be provided to the IP network layer protocol.
Type 2 (Connection Oriented)
Many wide area network protocols require that a logical circuit or call be established for the duration of
the exchange between the origin and destination nodes. Packets usually travel in sequence over this logical
circuit and are not routed as independent entities. LLC Type 2 provides this type of service. The service
involves a number of control frames to manage the logical circuit (establishment, disconnection) and
numbered frames for information transfer. Positive acknowledgments and ﬂow control mechanisms
based on this frame numbering are an integral part of this service. LLC Type 2 is commonly found in
implementations of IBM’s Systems Network Architecture (SNA).
Type 3 (Acknowledged Connectionless)
No circuit is established in this service variation, but acknowledgments are required from the destination
node. This type of service adds additional reliability to Type 1, but without the overhead of Type 2.

© 2001 by CRC Press LLC
These LLC alternatives are summarized in Table 1.4.4.
1.4.4
Building Cabling Speciﬁcations
The major components of a building cabling architecture that apply to the implementation of LANs are
shown in Figure 1.4.12 and include the following.
• Equipment room — This location houses major data center processing and communications
equipment for the building including servers, routers, and LAN switches. For a campus environ-
ment involving a number of buildings, one such location would serve as a data processing and
communications center for the campus; other equipment rooms on the campus would serve
speciﬁc buildings.
• Telecommunications closet — This is an area, typically located on each ﬂoor of a building, that
houses data and telecommunications equipment providing wiring concentration, cross-connect
and hubbing functions. 
• Backbone cabling — This cabling provides connectivity between equipment in the equipment
room and the telecommunications closets. It includes vertical connections between the ﬂoors and
connections between buildings.
• Horizontal cabling — This cabling extends from the telecommunications closet to the individual
work areas on the building ﬂoors. 
The American National Standards Institute (ANSI), the Electronics Industry Association (EIA) and
the Telecommunications Industry Association (TIA) develop speciﬁcations for commercial building
TABLE 1.4.4
Summary of Logical Link Control Alternatives
Service Type
Type 1
Type 2
Type 3
Description
Connectionless
Connection
Acknowledged 
Connectionless
Acknowledgments
No
Yes
Yes
Error recovery
No
Yes
Yes
Flow control
No
Yes
No
FIGURE 1.4.12
Architecture for building/campus telecommunications cabling.

© 2001 by CRC Press LLC
cabling standards. This set of standards, referred to as ANSI/EIA/TIA 568A, deﬁnes the installation
practices, certiﬁcation, and physical, electrical, and optical characteristics for various physical media such
as unshielded twisted-pair and ﬁber-optic cable [10]. The intent of the standard is to provide a guideline
by which a cabling system can be designed and implemented as part of the overall design of a new
building, even if the systems that the cabling must support are not yet deﬁned. It guides the user toward
the selection of cabling that will support current and future communications needs. The 568A standard
and other EIA standards are recognized by architectural and engineering ﬁrms as deﬁnitive guidelines
to use during the design phase of a building.
The 568A standard deﬁnes the requirements of a cabling system on a generic level that is appropriate
to a commercial environment. The standard allows certain options, such as the use of various cabling
media, including the following.
• 100-ohm unshielded twisted-pair cable in a four-pair conﬁguration;
• 150-ohm shielded twisted-pair cable in a two-pair conﬁguration;
• 50-ohm coaxial cable (not recommended for new installations); and
• 62.5 micron optical ﬁber cable in a two-pair conﬁguration.
These cables exhibit performance that varies greatly depending on the frequency of the signal that is
carried. For example, at Mbps speeds, a signal on a twisted-pair cable deteriorates in quality over a fairly
short distance. The 568A standard provides performance criteria for the above cabling which must be
met to be classiﬁed as 568A compliant. A summary of the attenuation limits speciﬁed by 568A for certain
cable types is shown in Figure 1.4.13.
References
1. ISO/IEC TR 8802-1, Overview of LAN/MAN Standards.
2. IEEE Std. 802, Overview and Architecture.
3. ISO/IEC 7498-1: 1994, Open Systems Interconnection Basic Reference Model.
4. The Ethernet, A Local Area Network, Data Link Layer and Physical Layer Speciﬁcations, Digital
Equipment Corp., Maynard, MA; Intel Corp., Santa Clara, CA; Xerox Corp., Stamford, CT; Version
1.0, Sept. 30, 1980, and Version 2.0, Nov. 1982.
5. IEEE 802.11, Wireless LAN Medium Access Control (MAC) Sublayer and Physical Layer Speciﬁ-
cations.
6. IEEE 802.14, Standard Protocol for Cable-TV Based Broadband Communication Network.
7. RFC 1700, ASSIGNED NUMBERS, J. Reynolds, J. Postel. Oct. 1994.
FIGURE 1.4.13
Attenuation of various media — EIA 568A.

© 2001 by CRC Press LLC
8. International Standard ISO/IEC 8802-3: 1996(E), ANSI/IEEE Std 802.3, 1996 Edition, Part 3:
Carrier sense multiple access with collision detection (CSMA/CD) access method and physical layer
speciﬁcations.
9. ANSI/IEEE Std 802.2 [ISO/IEC 8802-3], Logical Link Control.
10. ANSI/EIA/TIA 568A, Commercial Building Telecommunications Cabling.
1.5
Token Ring Speciﬁcs
John Amoss
The IEEE token-passing ring local area network architecture, ﬁrst approved in 1985 by the IEEE 802.5
Working Committee, has undergone a variety of additions and modiﬁcations and its essential speciﬁca-
tions are complete. The token ring architecture speciﬁcation addresses the media access control (MAC)
layer and the physical layers shown in Figure 1.5.1 and the current version is deﬁned in [1]. 
1.5.1
Topology
An IEEE 802.5 token ring consists of a star wired system of stations with each station connected by lobe
cabling to a trunk coupling unit (TCU) on a concentrator. Figure 1.5.1 shows a sample token ring
conﬁguration consisting of two concentrators with three stations attached to each concentrator. The TCU
provides a mechanism for insertion of a station into the ring and removal of the station from the ring.
For example, note in the ﬁgure that station 5 is in bypass mode and is not participating in the ring
operation. Concentrators can support multiple TCUs and are in turn serially connected via a trunk cable
between ring in and ring out ports. 
In addition to the architecture shown in the ﬁgure, a supplement to the IEEE 802.5 standard [2],
deﬁnes a dual ring architecture intended for applications that require very high availability and recovery
from media and station failures. This architecture uses two separate counter-rotating token passing rings;
counter-rotating implies that information ﬂow is in opposite directions on the two rings. Thus in addition
to the ring shown in Figure 1.5.1, there is a separate independent secondary ring. The primary ring is
normally the operational ring, with the secondary ring becoming operational in case of ring element
FIGURE 1.5.1
Examples of token passing rings conﬁrmation.

© 2001 by CRC Press LLC
failures. A typical reconﬁguration to account for a failure condition involves wrapping the signal around
from the primary ring to the secondary ring.
1.5.2
Station Attachment
As mentioned, each TCU provides insertion to or bypass from the ring for the station. Prior to insertion,
while in the bypass mode, a station can perform self tests of the attaching lobe cabling and its station
circuitry to assure proper operation. To attach to the ring, the station sends a signal over the lobe cable
and the TCU switches from bypass to insert mode.
1.5.3
Token Ring Operation
Unlike CSMA/CD operation, each station regenerates and repeats each bit. Information on the token
ring is transferred sequentially from one inserted station to the next. A given station transfers information
onto the ring, where the information circulates from one station to the next. The addressed destination
station copies the information and the station that transmitted the information removes it from the ring.
Note also that the concentrators shown in Figure 1.5.1 may be pass ive (with no active elements) or active
(performing a repeater function).
A station gains the right to transmit its information onto the medium when it detects a “token,” a
special media access control signal, passing on the medium. The token format is discussed in a later
section. Any station may “capture” the token by modifying it slightly and appending additional ﬁelds
including information to be transferred to a destination station. At the completion of the information
transfer, the station initiates a new token, which provides other stations with access to the media.
The current speciﬁcation includes an “Early Token Release” feature intended to make more efﬁcient
use of the available bandwidth on physically large rings operating with particularly small frames. In
earlier versions of the token-passing protocol, a new free token could not be released by the sending
station until it recognized the address in its own frame coming back around the ring to itself. If the frame
was small, and the ring was large, there was a great deal of wasted time on the medium. Using Early
Token Release, a sending station can release the free token immediately upon completing its transmission.
The unused capacity on the ring can now be used by other stations. When coupled with the 16 Mbps
ring operation, this new feature has signiﬁcant advantages in terms of performance.
1.5.4
Priority Feature
An important feature of the IEEE token ring architecture, not found in the CSMA/CD architecture, is
the support of multiple levels of priority. These priority levels are available for use by applications with
varying classes of service needs. For example, the standard mentions real-time voice and network man-
agement as potentially high-priority applications. The priority mechanism operates in such a way that
fairness is obtained for all stations within a priority level. This is accomplished by having the same station
that raised the service priority level of the ring return the ring to the original service priority. 
As mentioned, station access to the physical medium is controlled by passing a token around the ring.
Operation of the priority feature is outlined in Figure 1.5.2 and is governed by the following parameters.
• Pmsg = Priority of the frame to be transmitted by the station
• Prcvd = Priority of the received token or frame (contained in the header)
• Rrcvd = Reservation level of the received token or frame (contained in the header)
As a token passes a station it has the opportunity to transmit one or more frames or place a request
for a token of the appropriate priority. If a frame from another station passes the station, it may also
place a request for a token of the appropriate priority. As shown in the ﬁgure, the station will send a
frame if a token is received and the priority of the waiting frame is greater than or equal to the priority
of the token. If the token has higher priority than the frame or if it is a frame from another station that
is received, the station will attempt a reservation.

© 2001 by CRC Press LLC
1.5.5
Management
Special stations are deﬁned on the ring for system management purposes. Termed server stations, these
stations act as data collection points on the ring, gathering information from the ring stations on any
errors encountered, such as lost tokens, and lost or errored frames.
The server stations interact with a ring management system to provide the information necessary to
manage the ring.
1.5.6
Physical Attributes
Distances covered by a token ring network are speciﬁed in terms of what is termed ring segment length,
which is the transmission path between repeaters (i.e., stations or active concentrators). Since an active
concentrator performs a repeater function, it can be seen from Figure 1.5.1 that allowable lobe cable
lengths are longer for active than passive concentrators. For example, the token ring speciﬁcation (Annex
B) presents sample designs with lobe lengths on the order of 65 meters for passive concentrators and
200 meters for active concentrators. 
The initial version of the IEEE token-passing ring was a 4 Mbps implementation which ran on shielded
twisted-pair (STP) wire. The current token ring speciﬁcation supports transmission rates of 4 and 16
Mbps and both STP and unshielded twisted pair (UTP) media. 
Based on a number of factors such as the media type, data rate, and concentrator type, the number
of station on a ring may be up to 250.
1.5.7
Formats
1.5.7.1
Tokens
The token, or control signal, has the format shown in Figure 1.5.3. This signal is the means of passing
the right to transmit from station to station on the ring. As shown in the ﬁgure, the token consists of
three octets.
The starting delimiter (SD) and ending delimiter (ED) each consist of a ﬁxed sequence of symbols,
some of which deliberately violate the differential Manchester encoding scheme (see Figure 1.5.4). Because
of these violations, these delimiters cannot be part of the ﬁll sequence, deﬁned as a stream of validly
encoded symbols, sent on the media between frames. Thus false SD and ED indications will not occur.
FIGURE 1.5.2
802.5 Token passing ring — priority and reservation scheme.

© 2001 by CRC Press LLC
The access control ﬁeld is a key ﬁeld associated with many of the services provided by a token ring
network.
• A 3-bit priority ﬁeld indicates the priority of the token. When frames are eventually sent on the
media, they carry the same priority as the token by which they gained access to the network.
• A 1-bit ﬁeld indicates whether this is a token (value of 0), or the beginning of a frame transmitted
by a station (value of 1).
• A 1-bit monitor ﬁeld, set initially to 0 and set to 1 by a monitoring station, is used to prevent
certain tokens or data frames from continuously circulating around the ring.
• A 3-bit reservation ﬁeld allows stations with higher priority protocol data units to gain quicker
access to the media.
1.5.7.2
Frames
Frames are the resulting messages sent between stations. It is important to note that two very different
types of frames may be sent. One carries LLC messages, or user data, containing higher level protocol
FIGURE 1.5.3
Token format.
FIGURE 1.5.4
Differential Manchester encoding.

© 2001 by CRC Press LLC
exchanges as described in the MAC section. A second type of frame, described in detail in the IEEE 802.5
standard carries MAC messages used to manage the MAC layer. 
The format of these frames is shown in Figure 1.5.5. 
In addition to those ﬁelds contained in a token, these frames contain additional ﬁelds, including the
following.
• A Frame Control (FC) ﬁeld indicates the frame type – MAC (either a MAC protocol frame or a
MAC management frame) or LLC.
• A 48-bit source address (SA) ﬁeld and a 48-bit destination address (DA) ﬁeld identify the sending
station and the receiving station. The destination address can represent an individual station, a
group of stations or all stations (broadcast). The source address ﬁeld contains 1 bit which is used
to indicate whether routing information is included in the frame, as would be the case for a source-
routed network. 
• A routing information (RI) ﬁeld is used in the case of source routing to specify the route for the
frame through a bridged network.
• The information ﬁeld contains octets destined for the MAC Protocol or Management entity or
the LLC entity. A 32-bit frame check sequence ﬁeld is used for error detection.
• A frame status (FS) ﬁeld is used to provide an indication as to whether or not a frame reached
its destination and whether it was successfully read from the media. “A” bits are set by a destination
station to indicate that it has recognized its address in the DA ﬁeld and “C” bits are set to indicate
that it has successfully copied the frame.
• Finally, an interframe gap (IFG) ﬁeld is added to account for variability in the gap between frames.
References
1. Token ring access method and physical layer speciﬁcations, ISO/IEC 8802-5 : 1998, ANSI/IEEE
Standard 802.5, 1998 Edition.
2. Recommended practice for dual ring operation with wrapback reconﬁguration, IEEE Standard 802.5c-
1991, Supplement to 802.5 
1.6
Summary
This section presented the current trends in voice and data networks regarding voice services; the
evolution from the connection oriented networks to the connectionless networks, and uniﬁcation
between voice and data networks.
An Intelligent Network (IN) is a telephone network with a “service speciﬁc” architecture. It evolved
in an Advanced Intelligent Network (AIN), which has a “service independent” architecture. This means
that a given part of a phone number won’t trigger a request for a speciﬁc service, but it can be interpreted
FIGURE 1.5.5
Frame format.

© 2001 by CRC Press LLC
differently by various services, depending on different factors such as time of day, caller identity, and
type of call.
CTI is a technology platform that merges voice and data services at the functional level to add tangible
beneﬁts to business applications. In fact, CTI is a new application for pre-existing technologies. The most
important beneﬁt CTI brings to the corporate world is the potential to reduce operating expenses.
The low cost of the public Internet resulted in a great increase of data trafﬁc in the last couple of years.
Besides this, the quality of service (QoS) has been considerably improved, and offered acceptable delays
for trafﬁc less tolerant to delays. This raised a considerable interest in transporting packetized voice (not
tolerant to delays) over data, in particular, IP networks. The support for voice communications over the
Internet Protocol is usually called Voice over IP (VoIP).
Local area networks don’t lose their importance for users independently of how much bandwidth is
available in the wide area. They serve as local connectors between end-user-devices, server farms, storage
farms, and the ingress/engress nodes of wide area networks. Their limitation to data is changing by
supporting multimedia applications up to the desktop. Step-by-step, throughput capabilities could be
improved; in this respect, Giga Ethernet seems to be the winner for high-speed LAN technology.

© 2001 by CRC Press LLC
Teresa Piliouras et al. ‘‘Intranets’’
The CRC Handbook of Modern Telecommunications
Ed. Patricia Morreale and Kornel Terplan
Boca Raton, CRC Press LLC. 2001

© 2001 by CRC Press LLC
2
Intranets
Introduction
2.1 
Internet and Intranet Management Concepts .
Management Overview of the Internet and Intranets • Intranet 
Planning and Management • Technical Overview • Intranet 
Components • Intranet Implementation • Intranet 
Deployment • Intranet Security Issues • Summary
2.2 
Internet Security
Physical Security • Modems • Data Security • Passwords • 
Workstation Security • TCP/IP Security
2.3 
Virtual Private Networking Solutions.
Layer 2 Protocols • Layer 3 Tunneling Protocols • Frame 
Relay • Layer 2 or Layer 3 Comparison
2.4 
Effective Website Design.
Goals Deﬁned • Production and Maintenance Efforts • A 
System for Measuring Effectiveness • Intuitive Layout
2.5 
Web-enabled Data Warehousing
Introduction • Data Warehousing Overview • Web-enabled 
Data Warehousing • Vendors • Future Trends • Conclusion
2.6 
E-commerce Technologies: A Strategic Overview
E-commerce Technologies • Strategic Challenges • Emerging 
Trends for the Future
2.7 
Internet Protocols
Addressing for Internet • Communication Protocols in 
Internet • Information Transfer in Internet • Types of 
Internet Access • Internet E-mail • Telnet in Internet • File 
Transfer in Internet • News and Usenet • Mailing Lists in 
Internet • Information Search in Internet • Netscape and 
Microsoft
Introduction
Teresa Piliouras
The Internet started as a technological revolution, designed to protect national interests by ensuring
redundancy and resiliency in governmental networks, particularly in time of war. It has spawned world-
wide cultural revolution, fostering universal communication exchange with limitless geographic, time,
and subject matter boundaries. The extent and ease of the Internet’s adoption has had profound impli-
cations on all — including personal, business, and governmental — aspects of life. There is no place on
earth that cannot be reached by the Internet.
In this chapter, we review the basic technological underpinnings of the Internet and discuss why it is
so ﬂexible. As we explore the evolution of the Internet, which continues at an ever-increasing pace, we
also examine corresponding effects on communication paradigms, particularly in a business context.
Teresa Piliouras
TCR, Inc.
Andrew Resnick
Citicorp
John Braun
Endre Sara
Goldman, Sachs & Co.
Karen M. Freundlich
TCR, Inc.
Dermot Murray
Iona College
Mihir Parikh
Polytechnic University

© 2001 by CRC Press LLC
Once caution was the order of the day. Now, businesses small and large alike are racing to join the Internet
bandwagon and to have a “Web” presence.
In a dynamic environment that changes faster than words can be put into print, we can only hope to
scratch the surface on the Internet’s development and major trends in this Chapter.
2.1
Internet and Intranet Management Concepts
Teresa Piliouras
2.1.1
Management Overview of the Internet and Intranets
An Intranet is a company speciﬁc, private network based on Internet technology, and as such, it is a form
of local area network (LAN). However, one of the major distinctions between traditional LANs and
Intranets is the reliance of the latter on TCP/IP, packet switching, and Internet technologies. In the case
of the Internet, the technology is deployed over a public network, while in the case of Intranets, the
technology is deployed within a private network.
According to George Eckel, author of Intranet Working, one of the important beneﬁts of Intranets is
that they provide a cost-effective vehicle for communication, since the expense of reaching one person
or one million people is essentially the same. Intranets are becoming the corporate world’s equivalent of
a town hall where people can meet, chat, and exchange information.
The emergence of Intranets promises to change the way companies communicate with their employees
and how they conduct their business. For example, after years of using satellite feeds to disseminate
information to its 208 network afﬁliates, CBS News now uses an Intranet to provide afﬁliates with point-
and-click access to information on upcoming news stories. Access to this information is provided through
the CBS Newspath World Wide Web home page.
2.1.1.1
Beneﬁts of Intranets
Intranets offer many potential beneﬁts, including:
• Reduced operating costs
• Improved employee productivity
• Streamlined processing ﬂows
• Improved internal and external communication
• New and improved customer service
• Cross-platform capability
We will discuss some of the ways these beneﬁts can be achieved.
2.1.1.1.1
The Paper-less Ofﬁce
Many companies ﬁnd that Intranets simplify corporate-wide communications, and reduce printed mate-
rial costs by eliminating the need for many paper-based processes. For example, some organizations offer
complete manuals on their corporate Web site in electronic form, instead of distributing the information
in printed form. Companies can beneﬁt immediately from an Intranet by replacing their printed mate-
rials, little by little, with electronic versions. Electronic media is cheaper to produce, update, and distribute
than printed material. Often times, printed material is out of date by the time it is distributed. Electronic
documents, however, can be easily modiﬁed and updated as the need arises.
2.1.1.1.2
Improved Customer Service
For many organizations, having the right information at the right time can make a signiﬁcant difference
in their ability to close a sale or meet a deadline. In today’s competitive business environment, companies
are also under constant pressure to improve productivity while reducing costs. To achieve these productivity

© 2001 by CRC Press LLC
gains, companies must constantly improve their relationships with employees, customers, vendors, and
suppliers. Intranets provide an important avenue for making these advancements.
Using an Intranet, vendors, employees, and customers can access information as it is needed, alleviating
delays associated with mailing or distributing printed materials. For example, Intranets have been used to:
• Distribute software updates to customers, reducing the need to send printed materials and ﬂoppy
diskettes or CD-ROMs
• Handle customer orders on-line
• Process and respond to customer inquiries and questions about products and services
• Collect customer and survey data
Using an Intranet, all these activities can be completed electronically in a matter of minutes.
2.1.1.1.3
Improved Help Desks
Intranets have been used to augment help desk services. For example, when someone in the organization
learns about a new technology or how to perform a new task (for example, running virus software),
he/she can put information and instructions for others on a personal Web page. Others within the
organization, including help desk staff, can then access this information as needed. In an organization
empowered by an Intranet, all employees can leave the imprints of their expertise.
2.1.1.1.4
Improved Corporate Culture
Intranets help to cultivate a corporate culture that encourages the free ﬂow of information. Intranets
place information directly into the hands of employees, promoting a more democratic company structure.
The danger of “information democracy” is that once it is in place and taken for granted, management
can not easily revert to older, more controlled forms of communication without seriously damaging
employee morale and cooperation. Every individual in an Intranet environment is empowered to access
and distribute information, both good and bad, on a scale heretofore unknown in the corporate realm.
Intranets dissolve barriers to communication created by departmental walls, geographical location,
and decentralized organizations. Placing information directly in the hands of those who need it allows
organizations to decentralize and ﬂatten decision making and organizational processes, while maintaining
control over the information exchange. Individuals and groups can distribute ideas freely, without having
to observe traditional channels of information (i.e., an individual, a printed document, etc.) that are far
less effective in reaching geographically dispersed individuals.
2.1.1.1.5
Cross-platform Compatibility
Since the early 1980s, organizations with private networks have struggled with connecting and dissem-
inating information between different types of computers — such as PCs, Macintoshes, and Unix-based
machines. To help manage potential barriers to electronic communication posed by hardware and
software incompatibilities, many companies have instituted strict standards limiting corporate users to
speciﬁc hardware and software platforms. Even today, if a company uses PCs, Macs, and Unix-based
machines, sharing a simple text document can be a challenge.
Intranets provide a means to overcome many of these software and hardware incompatibilities, since
Internet technologies (such as TCP/IP) are platform independent. Thus, companies using Intranets no longer
need to settle on one operating system, since users working with Macintosh, PC, or Unix-based computers
can freely share and distribute information. In the sections that follow, we will explain why this is so.
2.1.2
Intranet Planning and Management
To implement an Intranet, a company needs a dedicated Web server, communications links to the Intranet,
and browser software. Unfortunately, Intranets do not come prepackaged and fully assembled. They require
careful planning and construction, if they are to be effective in meeting the needs of the organization. In
the sections that follow, we discuss recommendations for planning and implementing an Intranet.

© 2001 by CRC Press LLC
2.1.2.1
Gaining Support
The ﬁrst step toward a successful Intranet implementation is to obtain company-wide support for the
project, including endorsement from upper management. A quality presentation should be made to both
management and staff to explain the beneﬁts of the Intranet project. Some of these are tangible and easy
to measure, while others are intangible and difﬁcult to measure. To gain widespread support for the
Intranet project, decision makers must be shown what an Intranet is and how it will beneﬁt the organi-
zation. There are many resources (including complete presentations) available on the World Wide Web
to help promote the Intranet in a corporate environment.
2.1.2.2
Planning the Intranet Strategy
After selling upper management on the idea of an Intranet, the next step is to deﬁne the goals, purpose,
and objectives for the Intranet. This is an essential part of the Intranet project planning.
The Intranet project plan should include an overview of the organizational structure and its technical
capabilities. The current communication model used to control information ﬂows within the organiza-
tion should be examined with respect to its strengths and weaknesses in supporting workﬂow processes,
document management, training needs, and other key business requirements. It is important to under-
stand and document existing systems within the organization before implementing the Intranet.
The Intranet plan should clearly deﬁne the business objectives to be achieved. The objectives should
reﬂect the needs of the Intranet’s potential users. Conducting interviews with employees and managers
can help identify these needs. For example, the human resource department may wish to use the Intranet
to display job opportunities available within the organization. If this need is to be satisﬁed, the Intranet
should be designed to display job information and job application forms on a Web server, so applicants
can apply for positions electronically. The human resource department might also wish to offer employees
the ability to change their 401K information by using the Intranet. Each identiﬁed goal shapes and deﬁnes
the functionality that the Intranet must support. An employee survey is also an excellent way to collect
ideas on how to employ the Intranet within the organization.
In summary, the following questions are helpful in deﬁning the requirements of the Intranet project:
• Will Intranet users need to access existing (legacy) databases?
• What type of training and support will Intranet users require?
• Who will manage, create, and update the content made available through the Intranet?
• Will individual departments create their own Web pages autonomously?
• Will there be a central authority that manages changes to the content offered on the Intranet?
• Do users need remote access to the Intranet?
• Will the Intranet need to restrict access to certain users and content?
• Will a Webmaster or a team of technicians/managers be assigned to coordinate and manage the
maintenance of the Intranet?
• Will the Intranet be managed internally or will it be outsourced?
2.1.2.3
Selecting the Implementation Team
After the Intranet project plan has been developed and approved, the implementation team should be
assembled. If the organization does not have an infrastructure in place that is capable of implementing
the Intranet, additional staff and resources will need to be hired or the project will need to be outsourced
to a qualiﬁed vendor.
It is important that the Intranet team has the requisite skills to successfully execute the project plan.
A number of skill assessment checklists are provided below to help evaluate the resources available within
an organization and their abilities to successfully support the Intranet implementation.
Technical Support Skills Checklist 
The Intranet project will require staff with the technical skills needed to solve network problems, under-
stand network design, troubleshoot hardware and software compatibility problems, and implement

© 2001 by CRC Press LLC
client-server solutions (such as integrating network databases). Thus, the following skills are required to
support an Intranet:
• Knowledge of network hardware and software
• Understanding of TCP/IP and related protocols
• Experience implementing network security
• Awareness of client-server operations
• Practice with custom programming
• Abilities of database management
Content Development and Design Checklist 
A typical organization has many sources of information: human resource manuals, corporate statements,
telephone directories, departmental information, work instructions, procedures, employee records, and
much more. To simplify the collection of information that will be made available through the Intranet,
it is advisable to involve people familiar with the original documentation and also those who can author
content for Intranet Web pages. If possible, the original authors of the printed material should work
closely with the Intranet content developers to ensure that nothing is lost in translation.
The following technical skills are needed to organize and present information (content) in browser-
readable format:
• Experience in graphic design and content presentation
• Basic understanding of copyright law
• Knowledge of document conversion techniques (to convert spreadsheet data, for example, into a
text document for HTML editing)
• Experience in page layout and design
• Experience with Web browsers and HTML document creation
• Knowledge of image-conversion techniques and related software
• Knowledge of programming languages and programming skills
• CGI programming and server interaction
Management Support Skills Checklist 
As previously discussed, the company’s management should be involved in the planning and implemen-
tation of the Intranet. Ideally, management should have a good understanding of the Intranet beneﬁts,
and the expected costs and time frames needed for the project completion. Managers with skills relating
to quality-control techniques, process-management approaches, and effective communication are highly
desirable. Thus, the following management skills are recommended:
• Understanding of the organization’s document ﬂow
• Experience with the re-engineering process
• Knowledge of quality-control techniques
• Knowledge of the company’s informal ﬂow of information
• Experience with training and project coordination
2.1.2.4
Funding Growth
The initial cost of setting up a simple Intranet is often quite low and may not require top management’s
approval. However, when complex document management systems are needed to integrate database
access, automate workﬂow systems, implement interactive training, and other advanced features, the
Intranet should be funded with the approval of top management. To gain approval for the project, upper
management must be convinced that the Intranet is an integral part of the company’s total informa-
tion–technology deployment strategy. This involves quantifying the tangible beneﬁts of the Intranet to
the organization. Management also needs to understand how the Intranet will change the way people
work and communicate.

© 2001 by CRC Press LLC
2.1.2.5
Total Quality Management (TQM)
Effective deployment of an Intranet often involves re-engineering current process ﬂows within the
organization. Employees are usually most receptive to changes that make their jobs easier. To avoid
perceptions that the Intranet is an intimidating intrusion of yet another technology, it is advisable to
involve staff as early on as possible in the deployment planning. This will facilitate the transition to the
Intranet, and encourage employee participation in the Intranet’s success.
After migrating the company’s work processes to the Intranet, it is up to managers and employees to
adhere to the procedures that have been put in place to improve productivity and teamwork. Management
should not assume that because employees have a new tool — the Intranet — that this alone is sufﬁcient
to ensure that the desired attitudes and service levels will be attained. Instead, managers should view the
Intranet as one aspect of their quest for total quality management (TQM).
TQM involves creating systems and workﬂows that promote superior products and services. TQM also
involves instilling a respect for quality throughout the organization. TQM and the successful deployment
of Intranets represent a large-scale organizational commitment, which upper management must support.
2.1.2.6
Training Employees
If employees are expected to contribute content to the Intranet, they will need to be given tools and
training so they can author HTML and XML documents. In general, it is a good idea to encourage
employees to contribute to the content on display through the Intranet. To do otherwise means that the
organization may have to depend on only a few people to create HTML and XML documents.
After initial training, users should be surveyed to determine if the tools they have been provided satisfy
their needs. Many users ﬁnd that creating HTML/XML documents is difﬁcult. If so, then they may also
need special training. In corporations, this training is often provided by one person in each department
who has been given responsibility for training the rest of the department.
In summary, the following actions are recommended to help develop an effective program for training
employees to author high-quality HTML/XML documents:
• Conduct a survey to assess user training needs and wants
• Train users how to develop HTML/XML content
• Provide users with HTML/XML authoring tools that complement what they already know (for
example, the Internet Assistant for Microsoft Word is a good choice for users already familiar with
Microsoft Word)
• Review the design and ﬂow of material that will be “published” on the Intranet
• Give feedback to HTML/XML authors on ways to improve the site appearance and ease of use
2.1.2.7
Organizational Challenges
In addition to technological challenges, companies may also face the following organizational challenges
after the initial release of an Intranet:
• Marketing the Intranet within the organization so that all employees will support its growth and
continued use
• Obtaining additional funding on an ongoing basis to implement new capabilities
• Encouraging an information-sharing culture within the company so that all employees will con-
tribute toward building a learning organization
• Merging a paper-based culture with the new culture of electronic documentation
• Ensuring that the content on the Intranet is updated on a regular basis
• Preventing one person or group from controlling (monopolizing) the content on the Intranet
• Instructing employees to author HTML/XML content so they can contribute material to the Intranet
• Informing employees on Intranet etiquette, thereby facilitating courteous on-line discussion
forums and other forms of user interaction on the Intranet

© 2001 by CRC Press LLC
• Using the Intranet as an integral part of working with customers and vendors
• Measuring the Intranet’s overall effectiveness and contribution to the organization
As is the case when introducing any new information technology to an enterprise, Intranet deployment
requires careful planning, effective implementation, and employee training. In the short term, most of
the organizational focus is usually on the technical aspects of the Intranet deployment. But as time goes
on, organizational issues relating to how the Intranet is used within the organization must be managed.
When an organization actively examines and works toward resolving these issues, they are better able to
achieve a culture of teamwork and collaboration.
2.1.2.8
Management Summary
The following list summarizes key points surrounding the use of Intranets:
• An Intranet is a company-based version of the Internet. Intranets provide an inexpensive solution
for information sharing and user communication.
• An Intranet provides an easy way for users to communicate and share common documents, even
if they are using different machines, such as IBM compatible and Macintosh personal computers.
• Some organizations have expanded their Intranet to allow customers to access internal databases
and documents.
• Many companies can establish a functional Intranet using in-house personnel with a minimal
amount of new equipment.
Internet technology adheres to open standards that are well documented. This, in turn, encourages the
development of cost-effective and easy-to-implement Intranet solutions. As the popularity of Intranets has
increased, so has the demand for new tools and Web-based solutions. This demand has fueled competition
among software manufacturers which, in turn, has resulted in better and less expensive Intranet products.
In summary, Intranets can be used to improve productivity, simplify workﬂows, and gain a competitive
advantage over those who have yet to learn how to capitalize on the beneﬁts of Intranets.
2.1.3
Technical Overview
2.1.3.1
Internet Basics
2.1.3.1.1
Packet Switching
Packet switching was introduced in the late 1960s. In a packet-switched network, programs break data
into pieces, called packets, which are transmitted between computers. Each packet contains the sender’s
address, the destination address, and a portion of the data to be transmitted. For example, when an
e-mail message is sent over a packet-switched network, the e-mail is ﬁrst split into packets. Each packet
intermingles with other packets sent by other computers on the network. Network switches examine the
destination address contained in each packet, and route the packets to the appropriate recipient. Upon
reaching their destination, the packets are collected and reassembled to reconstitute the e-mail message.
2.1.3.1.2
TCP/IP
The U.S. Advanced Research Projects Agency (ARPA) was a major driving force in the development and
adoption of packet-switched networking. The earliest packet-switched network was called the ARPAnet. The
ARPAnet was the progenitor to today’s Internet. By the early 1980s, ARPA needed a better protocol for
handling the packets produced and sent by various network types. The original ARPAnet was based on the
Network Control Protocol (NCP). In January 1983, NCP was replaced by the Transport Control Proto-
col/Internet Protocol (TCP/IP). TCP/IP speciﬁes the rules for the exchange of information within the Internet
or an Intranet, allowing packets from many different types of networks to be sent over the same network.
2.1.3.1.3
Connecting to the Internet
One way to connect to the Internet is to install a link from the company network to the closest computer
already connected to the Internet. When this method is chosen, the company must pay to install and

© 2001 by CRC Press LLC
maintain the communications link (which might consist of a copper wire, a satellite connection, or a
ﬁber optic cable) to the Internet. This method was very popular with early adopters of the Internet,
which included universities, large companies, and government agencies. However, the costs to install and
maintain the communications link to the Internet can be prohibitive for smaller companies.
Fortunately, specialized companies — called Internet Service Providers (ISPs) — are available to
provide a low-cost solution for accessing the Internet. ISPs pay for an (expensive) connection to the
Internet, which they make accessible to others through the installation of high-performance servers, data
lines, and modems. Acting as middlemen, the ISPs rent time to other users who want to access the Internet.
Two important decisions must be made when deciding what type of Internet connection is the most
appropriate. The ﬁrst decision is the company budget allocated for Internet connectivity, and the second
is the Internet connection speed needed to support the business requirements. Both decisions are inter-
related. ISPs offer a variety of options for connecting to the Internet, ranging from a simple dial-up
account over phone wires to high-speed leased lines from the company to the ISP. Dial-up accounts are
typically available for a low, ﬂat monthly fee, and are generally much cheaper than a leased line connection.
However, the leased line connection is usually much faster than the dial-up connection.
When a dial-up account is used, a modem and a phone line are used to call and log into the ISP server
(or computer), which, in turn, acts as the doorway to the Internet. The transmission speed of the
connection is limited by the speed of the modems employed by the user and the ISP. A modem is
unnecessary when a leased line connection is available to the ISP. Leased lines are offered in many different
conﬁgurations with a variety of options. The most common link types are ISDN (which support trans-
mission speeds from 56 Kbps to 128 Kbps), T1 (transmitting at speeds up to 1.54 Mbps), and T3
(transmitting at speeds up to 45 Mbps).
If a company only needs to make an occasional connection to the Internet — for example, less than
20 to 50 hours per month for all users — a dial-up account should be sufﬁcient. However, if a company
needs faster data transfer speeds or has several users who must access the Internet for substantial periods
of time over the course of a month, a leased line connection should be considered.
The fastest growing segment of Internet users are those who connect to the Internet through an ISP
via an ordinary telephone connection. There are two major protocols for connecting to the Internet in
this way: Serial Line Internet Protocol (SLIP) and Point-to-Point Protocol (PPP). SLIP is the older
protocol and is available in many communications packages. The faster PPP is newer and therefore it is
not as widely supported.
Principles of queuing analysis can be applied to the problem of sizing the links needed to support the
Internet access, whether or not that access is to an ISP or to a direct Internet connection. The reader is referred
to Chapter 2, Network Design: Management and Technical Perspectives by Mann-Rubinson and Terplan, for
speciﬁc techniques on how to estimate the throughput and performance characteristics associated with using
different size link capacities. This analysis can be used to determine whether or not a dial-up or leased line
connection is sufﬁcient to support the bandwidth requirements with tolerable transmission delays.
2.1.3.1.4
Basic Terminology
In this section, we deﬁne commonly used Internet and Intranet terminology.
The World Wide Web 
The World Wide Web — or Web — is a collection of seamlessly interlinked documents that reside on
Internet servers. The Web is so named because it links documents to form a web of information across
computers worldwide. The documents available through the Web can support text, pictures, sounds, and
animation. The Web makes it very easy for users to locate and access information contained within
multiple documents and computers. “Surﬁng” is the term used to describe accessing (through a Web
browser) a chain of documents through a series of links on the Web.
Web Browsers 
To access and fully utilize all the features of the Web, special software — called a Web browser — is
necessary. Its main function is to allow the user to traverse and view documents on the Web. Browser

© 2001 by CRC Press LLC
software is widely available for free, either through a download from the Internet or from ISPs. Com-
mercial on-line services — such as America Online and Prodigy — also supply browsers as part of their
subscription products. The two most commonly used browsers are Netscape Navigator and Microsoft
Internet Explorer. Some of the common tasks which both support include:
• Viewing documents created on a variety of platforms
• Creating and revising content
• Participating in threaded discussions and news groups
• Watching and interacting with multimedia presentations
• Interfacing with existing legacy data (non-HTML based data) and applications
• Gaining seamless access to the Internet
It should be noted that the same Web browser software used for accessing the Internet is also used for
accessing documents within an Intranet.
Uniform Resource Locator (URL) 
The Web consists of millions of documents that are distinguished by a unique name called a URL
(Uniform Resource Locator), or more simply, a Web address. The URL is used by Web browsers use to
access Internet information. Examples of URLs include:
http://www.netscape.com
ftp://ftp.microsoft.com
A URL consists of three main parts:
1. A service identiﬁer (such as http)
2. A domain name (such as www.ups.com)
3. A path name (such as www.ups.com/tracking)
The ﬁrst part of the URL, the service identiﬁer, tells the browser software which protocol to use to
access the ﬁle requested. The service identiﬁer can take one of the following forms:
• http:// — This service identiﬁer indicates that the connection will use the hypertext transport
protocol (HTTP). HTTP deﬁnes the rules that software programs must follow to exchange infor-
mation across the Web. This is the most common type of connection. Thus, when Web addresses
start with the letters “http” it indicates that the documents are retrieved according to the conven-
tions of the HTTP protocol (hypertext transport protocol).
• ftp:// — This service identiﬁer indicates that the connection will use the ﬁle transfer protocol (FTP).
This service identiﬁer is typically used to download and copy ﬁles from one computer to another.
• gopher:// — This service identiﬁer indicates that the connection will utilize a gopher server to
provide a graphical list of accessible ﬁles.
• telnet:// — This service identiﬁer indicates that a telnet session will be used to run programs from
a remote computer.
The second part of the URL, the domain name, speciﬁes which computer is to be accessed when
running server software. An example of a domain name is: www.tcrinc.com.
The ﬁnal part of the URL, the path name, speciﬁes the directory path to the speciﬁc ﬁle to be accessed.
If the path name is missing from the URL, the server assumes that the default page (typically, the
homepage) should be accessed. Large, multi-page Web sites can have fairly long path names. For example,
these URLs request speciﬁc pages within a given Web site:
• http://www.apple.com/documents/productsupport.html
• http://www.bmwusa.com/ultimate/5series/5series.html
• ftp://ftp.ncsa.uiuc.edu/Mac/Mosaic
• http://www.microsoft.com/Misc/WhatsNew.htm

© 2001 by CRC Press LLC
Home Pages 
Companies, individuals, and governments that publish information on the Internet usually organize that
information into “pages,” much like the pages of a book or a sales brochure. The ﬁrst page that people
see in a sales brochure is the cover page, which may contain an index and summary of the brochure
contents. Similarly, a home page is the ﬁrst page that users see when they access a particular Web site.
The home page is to the Web site what the cover page is to a sales brochure. Both must be appealing,
concise, informative, and well organized to succeed in maintaining the reader’s interest. The home page
is usually used to convey basic information about the company and what it is offering in the way of
products and/or services.
Many companies publish the Internet address (or URL) of their home page on business cards, televi-
sion, magazines, and radio. To access a Web site, a user has merely to type the URL into the appropriate
area on the Web browser screen.
Client Programs and Browsers 
Across the Internet, information (i.e., programs and data) is stored on the hard disks of thousands of
computers called servers. These are so named because, upon request, they serve (or provide) users with
information. A server is a remote computer that may be conﬁgured to run several different types of server
programs (such as Web server, mail server, and ftp server programs).
A client program is used to initiate a session with a server. Client programs are so named because they
ask the server for service. In the case of the Web, the client program is the Web browser. All client–server
interactions take the same form. To start, the client connects to the server and asks the server for
information. The server, in turn, examines the request and then provides (serves) the client with the
requested information. The client and server may perform many request–response interactions in a
typical session.
Software programs — such as a browser — use HTTP commands to request services from an HTTP
server. An HTTP transaction consists of four parts: a connection, a request, a response, and a close.
Where Web Documents Reside 
When users publish Web pages, they actually store the pages as ﬁles that are accessible through a ﬁle
server. Typically, Web pages reside on the same computer on which the server program is running, but
this is not necessarily true. For security reasons, it may be necessary to limit accessibility to various ﬁles
on the Web server. Obviously, it might be disastrous if internal documents and data were made available
to competitors. To prevent this type of security risk, a WebMaster (or Systems Administrator) can
conﬁgure the Web server so it only allows speciﬁc clients to access conﬁdential information, based on a
need-to-know basis. The WebMaster can control access to the server by requiring users to log-in with a
username and password that has predetermined access privileges.
HTML — The Language of the World Wide Web 
The European Particle Physics Laboratory at CERN, in Geneva, Switzerland, developed Hypertext Markup
Language (HTML) in the late 1980s and early 1990s. HTML is the language of the World Wide Web.
Every site on the Web uses HTML to display information.
Each Web document contains a set of HTML instructions that tell the browser program (e.g., Netscape
Navigator or Microsoft Internet Explorer) how to display the Web page. When you connect to a Web
page using a browser, the Web server sends the HTML document to your browser across the Internet.
Any computer running a browser program can read and display HTML, regardless of whether that
computer is a personal computer running Windows, a Unix-based system, or a Mac.
If word processor formatted ﬁles — such as Microsoft Word — were used to create Web pages, only
users with access to Microsoft Word would be able to view the Web page. HTML was designed to overcome
this potential source of incompatibility. All users can access Web pages from their browser since all Web
pages conform to HTML standards. A HTML Web page is a plain text ﬁle (i.e., an ASCII text ﬁle) that
can be created and read by any text editor. There are many software programs available to convert
document ﬁles to HTML equivalents. In addition, many standard presentation and word processing

© 2001 by CRC Press LLC
packages offer built-in routines to convert a standard document into a Web-ready HTML ﬁle. This type
of conversion might be helpful, for example, if you wanted to convert a Microsoft PowerPoint presentation
into a set of HTML ﬁles for display on the Web.
After HTML ﬁles are transferred to a Web site, anyone with a browser can view them. HTML provides
the browser with two types of information:
1. “Mark-up” information that controls the text display characteristics and speciﬁes Web links to
other documents.
2. “Content” information consisting of the text, graphics, and sounds that the browser displays.
Hypertext and Hyperlinks 
Documents on the Web can be interconnected by specifying links (called hyperlinks) that allow the user
to jump from one document to another. The HTML code, which drives all Web pages, supports hypertext.
Hypertext, in turn, supports the creation of multimedia documents (containing pictures, text, animation,
sound, and links) on the Web.
Hyperlinks (or simply, links) are visually displayed on the Web pages as pictures or underlined text.
When a user clicks on a hyperlink displayed on their browser screen, the browser responds by searching
for and then loading the document speciﬁed by the hyperlink. The document speciﬁed in the hyperlink
may reside on the same computer as the Web page on display or it may reside on a different computer
on the other side of the world. Much of the Web’s success has been attributed to the simplicity of the
hyperlink point-and-click user interface.
There are four basic layouts for linking Web pages with hyperlinks: linear, hierarchical, Web, and
combination. Which layout is the most appropriate depends on the type of information that is being
presented and the intended audience.
FTP — The File Transfer Protocol 
The FTP (ﬁle transfer protocol) is a standard protocol for transferring and copying ﬁles from one
computer to another. Depending on the conﬁguration of the FTP server program, you may or may not
need an account on the remote machine to access system ﬁles. In many cases, you can access a remote
computer with FTP by logging on with a username of “anonymous,” and by entering your e-mail address
as the password. This type of connection is referred to as “anonymous FTP session.”
After logging in to the remote FTP server, it is possible to list a directory of the ﬁles that are available
for viewing and/or copying. The systems administrator determines which ﬁles can be accessed on the
remote server, and who has access privileges. When system security is a major concern, the system
administrator may require a speciﬁc username and password (as opposed to allowing an anonymous
log-on procedure) to gain access to system ﬁles.
FTP is very useful in accessing the millions of ﬁles available on the World Wide Web. Most browsers have
built-in FTP capabilities to facilitate downloading ﬁles stored at FTP sites. To access a FTP site using your
browser, you type in the FTP site address, much like entering a Web address. For example, to access the
Microsoft FTP site, the address “ftp://ftp.microsoft.com” would be entered into the browser address window.
Java 
Java is a new programming language released by Sun Microsystems that closely resembles C++. Java is
designed for creating animated Web sites. Java can be used to create small application programs, called
applets, which browsers download and execute. For example, a company might develop a Java applet for
their Web site to spin the company’s logo, to play music or audio clips, or to provide other forms of
animation to improve the appeal and effectiveness of the Web page.
Network Computers 
Although personal computers (PCs) are becoming more and more common, the number of households
with a PC is still only about one third the number of households with a television. The main reason is that
PCs are still too expensive for the masses. The network computer is a scaled-down, cheaper version (under
$500) of the PC. A network computer is designed to operate exclusively with the Internet and Java applets.

© 2001 by CRC Press LLC
2.1.4
Intranet Components
This section will provide an overview of the components necessary to create an Intranet. The ﬁnal
selection of the Intranet components depends upon on the company’s size, level of expertise, user needs,
and future Intranet expansion plans. In addition, we also examine some of the costs associated with the
various Intranet components.
An Intranet requires the same basic components found on the Internet, including:
1. A computer network for resource sharing.
2. A network operating system that supports the TCP/IP protocol.
3. A server computer that can run Internet server software.
4. Server software that supports hypertext transport protocol (HTTP) requests from browsers (clients).
5. Desktop client computers equipped with network software capable of sending and receiving
TCP/IP packet data.
6. Browser software installed on each client computer.
It should be noted that if a company does not want to use an internal server, an ISP can be used to
support the Intranet. It is very common for organizations to use an ISP, especially when there is little
information content or interest in maintaining a corporate-operated Intranet server. ISPs are also used
when the organizational facilities can not support the housing of an Intranet server.
In addition to the software and hardware components listed above, HTML/XML documents must be
prepared to provide information displays on the Intranet. The creation and conversion of documents to
HTML/XML format is very easy using commercial software packages, such as Microsoft’s FrontPage.
Third-party sources are also available to provide this service at a reasonable cost.
2.1.4.1
Network Requirements
The ﬁrst requirement for an Intranet is a computer network. For the purpose of this discussion, we
assume that a basic computer network is in place. We now focus on the hardware and software modiﬁ-
cations needed to support an Intranet.
Most computer networks are local-area networks (LANs). LANs are based on a client–server computing
model that uses a central, dedicated computer — called the server — to fulﬁll client requests. The
client–server computing model divides the network communication into two sides: a client side and a
server side. By deﬁnition, the client requests information or services from the server. The server, in turn,
responds to the client’s requests. In many cases, each side of a client–server connection can perform both
client and server functions.
Network servers are commonly used to send and receive e-mail, and to allow printers and ﬁles to be
shared by multiple users. In addition, network servers normally have a storage area for server programs
and to backup ﬁle copies. Server applications provide speciﬁc services. For example, a corporate-wide
e-mail system typically uses a server process that is accessible from any computer within the company’s
network.
A server application (or server process) usually initializes itself and then goes to sleep, spending much
of its time simply waiting for a request from a client application. Typically, a client process will transmit
a request (across the network) for a connection to the server, and then it will request some type of service
through the connection. The server can be located at either a local or remote site.
Every computer network has a physical topology by which it is connected. The most common topol-
ogies used to connect computers are the star, token ring, and bus topologies.
A network-interface card (NIC) is needed to physically connect a computer to the network. The
network-interface card resides in the computer and provides a connector to plug into the network.
Depending on the network, twisted-pair wiring, ﬁber optic, or coaxial cable may be used to physically
connect the network components. The network-interface card must be compatible with the underlying
network technology employed in the network (e.g., Ethernet or Token Ring).

© 2001 by CRC Press LLC
2.1.4.2
Network Operating Systems
The Internet supports connectivity between various hardware platforms running various operating
systems. In theory, there is no reason why an organization must stay with one type of machine or operating
system when implementing an Intranet. However, in practice, many organizations use only one network
operating system to simplify the task of managing the network.
The primary choices for network operating systems are: UNIX, Windows NT, and Novell NetWare.
We now discuss each of these operating systems and important considerations surrounding their use.
UNIX 
Many larger companies use UNIX-based machines as their primary business application server platform.
UNIX is a proven operating system that is well suited for the Internet’s open system model. Unfortunately,
learning how to use UNIX is not easy. Also, using a UNIX-based machine limits the choices available for
developing interactive Intranets and other software applications. Many programmers, for example, prefer
to develop applications using Windows-based machines and programming languages (such as Microsoft’s
Visual Basic or Borland’s Delphi).
Windows NT 
Many companies choose Windows NT over UNIX because NT is easy to install, maintain, and administer.
Windows NT, like UNIX and OS/2, provides a high-performance, multi-tasking workstation operating
system. It also supports advanced server functions (including HTTP, FTP, and Gopher) and communi-
cations with clients running under MS-DOS, Windows 3.1, Windows 95, Windows for Workgroups,
Windows NT Workstation, UNIX, or Macintosh operating systems. The latest version of Windows NT
Server includes a free Internet Information Server (IIS) and a free Web browser (Internet Explorer).
Microsoft designed the IIS so that it can be installed and up and running on a Windows NT workstation
in less than 10 minutes. The Windows NT Server comes with a built-in remote access services feature
that supports remote access to the Intranet through a dial-up phone connection.
Novell NetWare (IPX/SPX) 
The NetWare operating system provides network-wide ﬁle and printer sharing for Ethernet or token ring
networks. It runs on all major computer platforms, including UNIX, DOS, Macintosh, and Windows.
However, behind the scenes, NetWare sends and receives data packets based on the Internetwork Packet
Exchange/Sequenced Packet Exchange (IPX/SPX) protocol. Like TCP/IP, the IPX/SPX protocol deﬁnes a
set of rules for coordinating network communication between network components.
Many companies use Novell network products within LANs to operate ﬁle and print servers. Therefore,
it is important to understand how Novell’s NetWare products can be used in an Intranet implementation
strategy. If a company has an existing NetWare network, it might choose not to provide TCP/IP software
to its network clients. Instead, the local-area network features provided by the NetWare software can be
used.
A true Intranet uses Internet technology. This implies that an Internet Protocol (IP) address is assigned
to each network computer (actually to each network-interface card), and that the TCP/IP protocol is
used in the network. However, it is possible to run an Intranet on top of a NetWare LAN using various
software products that translate IPX to IP. Many of these software packages provide IPX to IP translation
while leaving the existing LAN infrastructure unchanged.
For example, the Novell product, IntranetWare, does not require assignment of an IP address to each
client on a network. Instead, an IP address is only assigned to the NetWare Web server. The software
performs IPX to IP translation on the client side, translating the TCP/IP protocols used by a Web browser
to IPX protocols. After the protocol translation on the client side, the messages travel across the network
until they reach the NetWare Web Server. At this point, the IntranetWare Server running on the NetWare
Web server translates the IPX messages back into TCP/IP so they can be sent on to other servers on the
network.

© 2001 by CRC Press LLC
Another way to create an Intranet using a NetWare-based LAN is to use the Internet gateway product
Inetix from Micro Computer Systems, Inc. Inetix runs on NetWare, Windows NT, or UNIX servers. Inetix
does not require that the NetWare client machines support TCP/IP, only that a single IP address be
assigned to the Intranet Web server. The Inetix client software allows a Web browser to execute on the
client side, even though the client machine does not use TCP/IP-based software.
In a Mac-based network, NetWare for Macs, or AppleTalk1 can be used to support the Intranet.
2.1.4.3
Server Hardware
Server machines run the network operating system and control how network computers share server
resources. Large businesses with thousands of users typically use high-speed Unix-based machines for
their servers. Small and medium-sized companies normally use less expensive Intel-based machines. The
load (i.e., the number of users and the amount of network trafﬁc) on the Intranet server machine will
inﬂuence the selection of a speciﬁc processor type.
There is considerable debate in the industry as to which machine makes a better Intranet server: a
Unix workstation, an Intel-based machine, or a PowerPC-based system. In general, the server choice
depends on the plans for the Intranet and the level of familiarity the company has with each of these
platforms. The server hardware selection also depends on the network operating system in use.
If a Unix-based server is chosen, a company will pay more for an equivalent amount of computing
power provided by an Intel-based machine. Unix machines still carry a price premium over PCs, because
they are made from custom parts, while Intel-based machines are made from commodity components
available from many hardware vendors and suppliers. For example, a high-end Pentium machine with
the capacity to serve over 1000 client machines can be bought for about one tenth of the cost of a
comparable Unix server. Macintosh-based systems are more expensive than comparable Intel-based
machines, but they are still much less expensive than Unix-based machines.
The decision to use a Unix-based machine vs. an Intel-based machine as the Intranet server is also
inﬂuenced by maintenance costs. Maintaining a Unix-based machine requires more resources than
maintaining an Intel-based machine. Hardware upgrades for Intel-based machines are also cheaper than
hardware upgrades for Unix workstations. A Macintosh server will cost more to upgrade than an Intel-
based machine. However, these costs are still lower than a comparable upgrade on a Unix-based machine.
The debate over Unix- and Intel-based machines focuses primarily on their performance in supporting
business application servers. For example, companies that use large accounting and ﬁnancial software
packages often use Unix servers. On the other hand, companies that do not want to pay the price premium
for Unix machines and/or are not familiar with Unix machines often select Intel-based machines as their
business application servers.
A Pentium-class machine supports a vast array of software applications and server software. Many
industry experts believe that Pentium-class machines will take a signiﬁcant amount of the market share
away from Unix workstations. This, in turn, means that more and more Intranet-based applications will
use Pentium-class machines in the future.
2.1.4.4
Web Server Software
A working Intranet requires server software that can handle requests from browsers. In addition, server
software is needed to retrieve ﬁles and to run application programs (which might, for instance, be used
to search a database or to process a form containing user-supplied information).
For the most part, selecting a Web server for an Intranet is similar to selecting a Web server for an
Internet site. However, Internet servers must generally handle larger numbers of requests and must deal
with more difﬁcult security issues. The performance of the Web server has a major impact on the overall
1AppleTalk — AppleTalk is Apple’s proprietary network operating system for supporting a LAN. Unlike AppleTalk,
TCP/IP supports both LANs and WANs.

© 2001 by CRC Press LLC
performance of the Intranet. Fortunately, it is fairly easy to migrate from a small Web server to a larger,
high-performance Web server as the system usage increases over time.
Web servers are available for both Windows NT and Unix from Microsoft, Netscape, and a number of
other companies. The Web server selection is limited by the server operating system. In the discussion that
follows, we describe Web servers for Unix, Windows NT, Windows 95, and Macintosh operating systems.
It is expected that in the next two to ﬁve years, stand-alone Web servers will be replaced by servers
that are an integral part of the operating system. In addition, these Web servers will handle many tasks
that require custom programming today, such as seamless connection to databases, video and audio
processing, and document management.
UNIX Web Servers 
One of the best and oldest Web servers for Unix-based machines is the National Center for Supercom-
puting Application’s (NCSA) HTTP Web Server. Much of the Internet’s growth is primarily due to the
popularity of this server, which is free. NCSA is committed to the continued development of its Web
server, which provides both common gateway scripting (CGI) capabilities and server side includes (SSI)
software. SSI software is used by Web servers to display and/or capture dynamic (changing) information
on an HTML/XML page. For example, SSI can be used to display a counter showing the number of
visitors to a Web site. The NCSA Web server also allows the creation of virtual servers on the same
machine. The virtual servers can have their own unique universal resource identiﬁer (URL). This is useful,
for example, for assigning a different IP address to different departments using the same machine.
Netscape Communications Corporation offers the most popular commercial Unix-based Web servers:
Enterprise Server and FastTrack Server. Enterprise Server software is designed for building large Intranets.
FastTrack is easier to install than Enterprise Server but it offers less functionality. FastTrack is well suited
for companies that plan to build small- to medium-sized Intranets. These Web servers are also available
for Windows NT.
Windows NT Web Servers 
As discussed in the previous section, both Netscape’s Enterprise Server and FastTrack Server are available
for Windows NT. These servers should be considered when both Windows NT and Unix-based machines
are used as servers.
If cross-platform Web server software is not needed, Microsoft’s IIS should be considered. This server
is used to drive Microsoft’s Internet site, and it works well for a large organization. At the time of this
writing, Microsoft offers this server free of charge. It also comes bundled with Microsoft’s Windows NT
Server software. IIS is easy to install and allows new users to be added to the Intranet with minimal
effort. IIS comes with an FTP server.
WebSite Professional, from O’Reilly & Associates, Inc., is another popular Windows NT Web server.
WebSite Professional is very easy to install. This server has built-in search capabilities, a Web site man-
agement tool, and the popular HTML editing tool, HotDog.
Macintosh Web Server 
Not many Web servers are available for Macintosh computers. If your organization does not have a
Macintosh network already in place, then it advisable to avoid installing a Mac-based Intranet server.
The largest market share of the Macintosh Web server market belongs to the WebStar server from
Quarterdeck Corporation. This server is a mature product and is a good choice for a Mac-based Web
server. WebStar is very easy to install and maintain.
NetWare Web Server 
The NetWare Web Server from Novell is an excellent choice for companies that have a NetWare network
already in place. When using a NetWare Web server, IPX to IP translation software must be installed on
each client machine. When this is done, an IP address does not need to be assigned to each client machine.

© 2001 by CRC Press LLC
2.1.4.5
Desktop Clients Running TCP/IP
TCP/IP must be installed on each client machine running on the Internet.2 To use an Internet-based
application (such as a Web browser) on a Windows-based machine, a TCP/IP stack must be present.
Windows 95, Windows NT, and IBM’s OS/2 Warp operating systems include the TCP/IP protocol suite.
Most Unix-based systems use TCP/IP as their main network communication protocol.
If a company has Windows 3.1 clients, they should consider upgrading the clients to Windows 95 or
Windows NT. Many of the advanced Internet and Intranet applications are only available for Unix,
Windows 95, and Windows NT operating systems. If the Windows 3.1 clients can not be upgraded, then
TCP/IP software must be installed for each client. One of the more popular TCP/IP software applications
for Windows 3.1 is Trumpet Winsock. Trumpet Winsock can be downloaded from the Internet for free.
2.1.4.6
Web Browsers
The last component needed to make a functional Intranet is a Web browser. There are two major choices
for a browser: Microsoft’s Internet Explorer or Netscape’s Navigator.
It is expected that Netscape will retain its dominance in the Unix market, since Microsoft has chosen
not to support a Unix platform. Therefore, if the Intranet must support Unix, Macintosh, and Windows
clients, and the company requires standardization on a single browser, Netscape’s Navigator product is
the only viable option.
Browsers, as we know them today, will probably not exist in a few years. For example, it is expected
that eventually Microsoft will integrate the browser’s functionality into its business application software
(such as Word, Excel, etc.) and operating system.
2.1.4.7
Intranet Component Summary
In this section, the basic components of an Intranet were examined. We recapitulate below some of the
key concepts covered in this section:
• Intranets are based on a client–server network computing model. By deﬁnition, the client side of
a network requests information or services and the server side responds to a client’s requests.
• The physical components of an Intranet include network interface cards, cables, and computers.
• Suites of protocols, such as TCP/IP and IPX/SPX, manage data communication for various network
technologies, network operating systems, and client operating systems.
• IPX to IP translation programs provide NetWare users with the ability to build an Intranet without
running the TCP/IP suite of protocols on their network.
• Windows-based Intranets are easier and less expensive to deploy than Unix-based Intranets.
• Netscape and Microsoft provide both low- and high-end server software products designed to
meet the needs of large, medium, and small organizations.
• Netscape’s Navigator and Microsoft’s Internet Explorer provide advanced browser features for
Intranet applications.
2.1.5
Intranet Implementation
2.1.5.1
Information Organization
After the physical components of the Intranet are in place, the next step is to design the information
content of the Intranet and/or Internet Web pages. This task involves identifying the major categories
and topics of information that will be made available on the Intranet. Information can be organized by
department, function, project, content, or any other useful categorization scheme. It is advisable to use
2If the network does not support TCP/IP, a gateway application that translates TCP/IP for the network operating
system protocol must be used.

© 2001 by CRC Press LLC
cross-functional design teams to help deﬁne the appropriate informational categories that should be
included on the corporate Web site. The following types of information are commonly found on corporate
Intranet homepages:
• What’s new
• Corporate information (history and contacts)
• Help desk and technical support
• Software and tools library
• Business resources
• Sales and marketing information
• Product information
• Human resources related information (beneﬁts information, etc.)
• Internal job postings
• Customer feedback
• Telephone and e-mail directory
• Quality and system maintenance records
• Plant and equipment records
• Finance and accounting information
• Keyword search/index capability
2.1.5.2
Content Structure
After the main topics of information to be displayed on the corporate Web page(s) have been identiﬁed,
the ﬂow and manner of presentation on the Intranet must be developed. Four primary ﬂow models are
used to structure the ﬂow of presentation at an Intranet Web site: linear, hierarchical, non-linear (or
Web), and combination information structures.
A linear information structure is similar in layout to a book in that information is linked sequentially,
page by page. When a linear layout is used, the Web pages are organized in a “slide show” format. This
layout is good for presenting pages that should be read in a speciﬁc sequence or order. Since linear layouts
are very structured, they limit the reader’s ability to explore and browse the Web page contents in a non-
sequential (or non-linear) manner.
When a hierarchical layout is used to structure the information, all the Web pages branch off from
the home page or main index. This layout is used when the material in the Web pages does not need to
be read in any particular order. A hierarchical information structure creates linear paths that only allow
up and down movements within the document structure.
A non-linear, or Web, structure links information based on related content. It has no apparent
structure. Non-linear structures allow the reader to wander through information spontaneously by
providing links that allow forward, backward, up and down, diagonal, and side-to-side movement within
a document. A non-linear structure can be confusing, and readers may get lost within the content, so
this structure should be chosen with care. The World Wide Web uses a non-linear structure. The advantage
of a non-linear structure is that it encourages the reader to browse freely.
The combination Web page layout, as the name implies, combines elements of the linear, Web, and
hierarchical layouts. Regardless of the type of ﬂow sequence employed, each Web page typically has links
that allow the user to move back and forth between pages and back to the home page.
Over the lifetime of the Intranet, it is likely that the layout and organization of information on the
corporate Web pages will change many times. It is often helpful to use ﬂow charting tools to help manage
and document the updated information ﬂows. Visio for Windows by Visio Corp. and ABC Flowcharter
by Micrografx are two excellent tools for developing ﬂowcharts. In addition, some of the Web authoring
tools offer ﬂowcharting and organizational tools to help design and update the information structure on
the Web pages.

© 2001 by CRC Press LLC
2.1.5.3
Interface Design
After deﬁning the Intranet’s structure, the next step is to deﬁne the functionality and user interface. The
Intranet design should be consistent with the organization’s corporate image. For example, items such
as corporate images, logos, trademarks, icons, and related design themes add a familiar look and feel to
the content. Where possible, they should be included in the Web page design. It is also advisable to work
with the marketing department when designing the web page layouts to ensure that a consistent theme
is maintained in all the company communications that will be viewed by the outside world.
A technique called storyboarding is frequently used to design the web page layout. Storyboards are
used by ﬁlm producers, story writers, and comic strip artists to organize the content and sequence of
their work. A storyboard depicts the content, images, and links between pages of the Intranet in the form
of a rough outline.
Software — such as Microsoft PowerPoint or a similar presentation program — can be used to develop
a storyboard and sample Web pages. It is a good idea to test the interface design to ensure that the icons,
buttons, and navigational tools are logical and intuitive. An Intranet without intuitive navigational tools is
like a road without signs. Just as it would be difﬁcult for drivers to ﬁnd their way from one city to another
without the aid of signs, street names, and directional information, Intranet users will ﬁnd it difﬁcult to
retrieve information without easy to follow categories, buttons, and links. It is often helpful to employ graphic
designers and marketing–communications staff to create effective graphics and images for the web site.
2.1.5.4
Free Clip Art and Images
Many icons and navigational signs are available as clip art that comes with word processing, page layout,
and presentation software programs. In addition, many Web sites offer images and clip art, which can
be downloaded for free. However, the licensing agreements for downloading free images may have
restrictions and requirements that should be observed.
2.1.5.5
Intranet Functionality
The required functionality of an Intranet dictates many of the design and user interface features. One of
the goals in designing the Intranet should be to improve existing systems and infrastructures. After
examining the current information structure, it may become clear which aspects of the structure work
well and which ones need improvement.
Workﬂow processes, document management, and work collaboration are areas that the organization
should strive to improve through the use of an Intranet. A workﬂow analysis should consider ways in
which the Intranet can automate various organizational tasks and processes. For example, if a company
has a geographically dispersed project team, the Intranet might be used to post and update project
information as various tasks are completed. Other team members could then visit the Intranet page at
any time to check the project status.
The following checklist is helpful when developing a list of the functions that need to be supported
by the Intranet:
Functionality Checklist 
• The user interface must be intuitive and tested
• The Intranet’s design should support continuous updates
• The Intranet may need to be integrated with database management systems to allow users to access
information (such as customer and product data)
• The Intranet should support existing (legacy) applications, as needed
• The Intranet should have built-in directories, such as corporate telephone numbers and e-mail
addresses
• The Intranet should incorporate groupware applications
• Support (or future expansion) for on-line conferencing should be considered
• The Intranet should provide division-speciﬁc and corporate-wide bulletin boards for electronic
postings

© 2001 by CRC Press LLC
• The Intranet should be designed with a document sharing and management process in mind
• The Intranet should foster teamwork and collaboration by enhancing channels of information
distribution
• Search engines — which simplify a user’s ability to locate and access information — should be
made available
• The Intranet should support e-mail
• The Intranet should support (for future expansion) multimedia applications that use text, images,
audio, and video
• Automated real-time Web-page generation should be encouraged
• The Intranet should be designed so it can interface, at least potentially, with factory equipment,
other manufacturing devices, or other critical legacy systems
• The Intranet should support the automation of organization workﬂows
2.1.5.6
Content Management
Many organizations struggle with the tasks of information creation, management, and dissemination.
They are time consuming and difﬁcult to control. The Intranet alone cannot solve information manage-
ment problems unless speciﬁc Intranet solutions are implemented that directly address the need for
document management. The following list identiﬁes content-management tasks that should be consid-
ered in the Intranet plan:
• Users must have the ability to easily add or update content on a regular basis
• Users must have the ability to protect their content from changes by other users
• A content-approval process should be deﬁned and in place. This process should encompass ways
to manage and control document revisions, especially changes to shared documents
As policies and procedures relating to content management are formulated, it is important to designate
responsibilities to speciﬁc individuals to ensure that they are put into place and followed. An Intranet
style guide should be developed that provides page layout, design elements, and HTML/XML code
guidelines. The style guide will help the organization to maintain a consistent look and feel throughout
the Intranet’s Web pages. The style guide should contain information on where to obtain standard icons,
buttons, and graphics, as well as guidelines on page dimensions and how to link to other pages. As part
of the style guide, it is helpful to create Web page templates. These templates consist of HTML/XML
ﬁles, and are used to provide a starting point for anyone interested in developing Web pages or content
for the Intranet. Although it is very easy to create a working Web page and to publish it for mass viewing,
the real challenge is in producing a well-conceived Web page.
2.1.5.7
Training and Support
After the Intranet is up and running, efforts should be focused on how to maintain the information
content and on employee training. Part of the document-management strategy should encompass the
selection of content stakeholders. Content stakeholders are individuals in different departments or work
groups who are responsible for the creation and maintenance of speciﬁc content. Stakeholders can be
department managers, team leaders, or content authors and publishers.
Some organizations create a position called a Webmaster. This position is responsible for maintaining
and supporting the content published on the Intranet. A good Webmaster should have the following skills:
• Basic Internet skills, including an understanding of e-mail, FTP, and Telnet
• A thorough understanding of HTML/XML document creation
• Experience with CGI programming
• Programming experience with languages such as Perl, C/C++, and Java
• Experience with content creation and the conversion of text and images
• Knowledge of client–server processing
• Experience with server setup and maintenance

© 2001 by CRC Press LLC
• Knowledge of your organization’s structure and inner workings
• Organizational and training skills
It is possible that the organization may choose to decentralize the maintenance of the information
content. In this case, individuals from various departments might be selected to maintain the content
relating to their respective department. These individuals should be trained to handle a variety of
maintenance issues. A decentralized approach depends on having more than one individual with the
necessary skills available to maintain the web pages. A decentralized support structure gives authors and
content owners direct control and responsibility for publishing and maintaining information. This can
help prevent bottlenecks in making information available in a timely fashion.
Training for stakeholders, Webmasters, and Intranet users is an important part of an Intranet strategy.
Intranet customers and content stakeholders should be trained to understand the Intranet and how it
will improve the organization and the way the company does business. They should also be given training
on how to create, utilize, and maintain content on the Web page(s). Companies that invest in the
education and training of their employees will have a better chance of creating and maintaining a
successful Intranet.
2.1.6
Intranet Deployment
Since Intranets are easy to set up, many companies do not realize what the true resource requirements
are to maintain the Intranet with up-to-date information. The goal of this section is to provide a realistic
perspective on how organizations are most likely to achieve long-lasting beneﬁts from the Intranet.
Some companies invest much more than their competitors in information technology, such as an
Intranet, but still fail to effectively compete in the marketplace. Computers alone do not, and cannot,
create successful companies. A good start is to empower all employees to contribute to the Intranet. As
is true for any collaborative effort, every member is responsible for the overall success of the team.
2.1.6.1
Technological Considerations
The major technological challenges facing the organization after the initial implementation of an Intranet
include:
• Converting existing paper documents into electronic documents that employees can access elec-
tronically via the Intranet.
• Connecting existing databases to the Intranet so they are accessible by a wide range of computing
platforms (such as Windows- and Mac-based systems).
• Coordinating the use of multiple servers used across departmental lines.
• Continuously enhancing the Intranet’s features and capabilities to keep employees motivated to
use the Intranet.
• Installing security features within the Intranet to prevent unauthorized access to conﬁdential or
sensitive information.
Intranet technology, and information technology in general, is changing so fast that keeping up with
the latest software and hardware solutions requires a substantial ongoing organizational commitment.
Conversion of Paper Documents Into Electronic Form 
The ﬁrst issue facing companies after the initial Intranet release is how to convert large numbers of
existing paper documents into electronic format ready for distribution on an Intranet. There are many
tools, such as HTML Transit, that can be used to convert documents from most electronic formats to
HTML or XML format. Microsoft’s Internet Assistant for Microsoft Word can also be used to easily
convert existing Word documents into HTML or XML documents. After paper documents have been
converted to HTML or XML and placed on the Intranet, the next challenge is to keep the documents up
to date.

© 2001 by CRC Press LLC
Obsolete information can frustrate Intranet users and may encourage them to revert to old ways of
information gathering (i.e., calling people, walking to various ofﬁces, and writing memos). One way to
minimize this problem is to create a database containing the document title, date of last change, and
frequency of update in a database. Other useful information that can be used to track the status and
nature of documents on the Intranet is shown in Table 2.1.1. A program can then be written to search
the Intranet for documents that have not been updated recently. The program can then issue e-mail to
the document owner to request an update.
Interface to Legacy Database(s) 
Connecting databases to the Intranet is not an easy task, and may require additional staff or reassignment
of current programming staff. Legacy database vendors are currently working on various Intranet solu-
tions to facilitate the implementation of this requirement.
Companies may need to connect the Intranet to legacy databases in order to access:
• Financial reports (regarding project costs, product costs, the overall ﬁnancial health of the enter-
prise, etc.)
• Document-management systems
• Human resources information (e.g., so employees can review details on health care and beneﬁts)
Use of Multiple Servers 
As the Intranet becomes more complex, multiple servers will be needed. This is especially true for
companies that have a large number of divisions and business units using the Intranet. For example, a
product-development group may need to provide team members the ability to search project-speciﬁc
databases, submit forms to various databases, and to use a private on-line discussion group. The Web-
master may ﬁnd it impossible to support these service needs in a timely manner. When this happens,
companies frequently relegate the task of server maintenance to each respective department.
Over the next few years, installing and using a Web server will become as easy as installing and using
word processor software. Web servers will probably become part of the Windows NT server operating
system. When each department is responsible for maintaining their own Web server, it is particularly
important to choose server software that is easy to install and maintain. A Pentium-class machine running
Windows NT server software and Microsoft’s IIS is a good choice for small departments. Another way
to provide departments with their own domain name and disk space is to use a virtual domain name.
Companies use virtual servers to reduce hardware costs. In the case of the Web, an HTTP-based server
runs on a server computer. For example, a company may need two types of Web servers, one that allows
easy access and one that requires usernames and passwords. In the past, the company would have
purchased two different computers to run the Web server software. Today, however, the company can
run both servers on the same system — as virtual servers.
Standardizing Hardware and Software 
To avoid supporting multiple hardware and software components, it is important to standardize the
server software, hardware, HTML and XML editing tools, and browser software. This will help to
minimize the potential for unexpected network errors and incompatibilities.
TABLE 2.1.1
Intranet Document Tracking Information
Data For Tracking Intranet Documents
Name of document
Document description
Page owner
Type of document (i.e., ofﬁcial, unofﬁcial, personal)
Conﬁdentiality status (i.e., conﬁdential, non-conﬁdential, etc.)
Original publish date
Date document last modiﬁed
Frequency of update (i.e., daily, weekly, monthly, etc.)

© 2001 by CRC Press LLC
2.1.6.2
Maintaining the Information Content on the Intranet
One of the major challenges organizations must face is how to transition from paper-based systems to
computer-based systems, while keeping information up to date.
Automating HTML/XML Authoring 
After establishing a policy for the distribution of Intranet documents, it is advisable to develop a set of
guidelines that clearly speciﬁes who is responsible for keeping them current. Inaccurate information
greatly reduces the effectiveness of the Intranet. If employees lose conﬁdence in the accuracy of the on-
line information, they will revert to calling people to ﬁnd information. Unfortunately, many people tend
to ignore the need to update information, irrespective of its form (i.e., electronic or print).
In some cases, the Intranet will contain information that employees must update daily, weekly, or
monthly. Spreadsheets can be used to capture highly time-sensitive data. Macros can be written (typically,
by a staff programmer) that automatically convert the spreadsheet data into HTML or XML format.
Managing Document Links 
In a traditional document-management system, documents often reference one another. In most cases,
authors list the applicable references at the top of each new document. Intranets, unfortunately, create
a situation where organizations cannot easily control the accuracy of links in documents.
HTML or XML document developers can use links freely and, in many cases, without checking the
accuracy of those links. Even if employees test the initial accuracy of their document links, it is difﬁcult
to maintain and check the accuracy of those links after the document is released. If you have ever
encountered a “broken” link when surﬁng the Web, you know that it can be frustrating. People depend
on links within a Web document to ﬁnd information. Today, however, there are a few mechanisms
available to assure the accuracy of document links. Employees must understand that other people may
link to their pages, and that they should not freely move the location of their documents. Employees
must view their needs in the total organizational context.
2.1.6.3
Centralized vs. Distributed Control
The implementation of an Intranet is a major change for any organization. Although change is not easy,
people are more inclined to modify their behavior when leaders have a clear sense of direction, involve
employees in developing that direction, and are able to demonstrate how the Intranet will positively
affect the employees’ well being. Managers should work with their employees to show that Intranets can
free them from the routine aspects of their job. This, in turn, will allow employees to spend more time
learning and developing new ideas for the corporation.
Some of the beneﬁts that can be obtained using a distributed model of Intranet control are:
• Employees can tap into the knowledge of everyone in the organization, making everyone a part
of a solution.
• The power of any one Webmaster to dictate the Intranet’s form and function is limited.
• It empowers departments to create their own information databases and to work with outside
customers and vendors.
2.1.7
Intranet Security Issues
By their very nature, Intranets encourage a free ﬂow of information. This means that it is also very easy
for information to ﬂow directly from the Intranet to the desktops of those who might seek to gain access
to information they should not have. To guard against this situation, adequate security measures should
be in place when the Intranet is deployed. In the discussion that follows, we review various security
techniques to protect an Intranet from unauthorized external and internal use.
2.1.7.1
Firewalls
The Internet was designed to be resistant to network attacks in the form of equipment breakdowns,
broken cabling, and power outages. Unfortunately, the Internet today needs additional technology to
prevent attacks against user privacy and company security. Luckily, a variety of hardware and software

© 2001 by CRC Press LLC
solutions exist to help protect an Intranet. The term ﬁrewall is a basic component of network security.
A ﬁrewall is a collection of hardware and software that interconnects two or more networks and, at the
same time, provides a central location for managing security. It is essentially a computer speciﬁcally
fortiﬁed to withstand various network attacks. Network designers place ﬁrewalls on a network as a ﬁrst
line of network defense. It becomes a “choke point” for all communications that lead in and out of an
Intranet. By centralizing access through one computer (known as a ﬁrewall-bastion host), it is easier to
manage the network security and to conﬁgure appropriate software on one machine. The bastion host
is also sometimes referred to as a server.
The ﬁrewall is a system that controls access between two networks. Normally, installing a ﬁrewall
between an Intranet and the Internet is a way to prevent the rest of the world from accessing a private
Intranet. Many companies provide their employees with access to the Internet long before they give them
access to an Intranet. Thus, by the time the Intranet is deployed, the company has typically already
installed a connection through a ﬁrewall. Besides protecting an Intranet from Internet users, the company
may also need to protect or isolate various departments within the Intranet from one another, particularly
when sensitive information is being accessed via the Intranet. A ﬁrewall can protect the organization
from both internal and external security threats.
Most ﬁrewalls support some level of encryption, which means data can be sent from the Intranet,
through the ﬁrewall, encrypted, and sent to the Internet. Likewise, encrypted data can come in from the
Internet, and the ﬁrewall can decrypt the data before it reaches the Intranet. By using encryption,
geographically dispersed Intranets can be connected through the Internet without worrying about some-
one intercepting and reading the data. Also, a company’s mobile employees can use encryption when
they dial into your system (perhaps via the Internet) to access private Intranet ﬁles.
In addition to ﬁrewalls, a router can be used to ﬁlter out data packets based on speciﬁc selection
criteria. Thus, the router can allow certain packets into the network while rejecting others.
One way to prevent outsiders from gaining access to an Intranet is to physically isolate it from the
Internet. The simplest way to isolate an Intranet is to not physically connect it to the Internet. Another
method is to connect two sets of cables, one for the Intranet and the other for the Internet.
Even without a connection to the Internet, an organization is susceptible to unauthorized access. To
reduce the opportunity for intrusions, a policy should be implemented that requires frequent password
changes and keeping that information conﬁdential. For example, disgruntled employees, including those
who have been recently laid off, can be a serious security threat. Such employees might want to leak
anything from source code to company strategies to the outside. In addition, casual business conversa-
tions, overheard in a restaurant or other public place, may lead to a compromise in security. Unfortunately,
a ﬁrewall cannot solve all these speciﬁc security risks.
It should be noted that a ﬁrewall can not keep viruses out of a network. Viruses are a growing and very
serious security threat. Prevention of viruses from entering an Intranet from the Internet by users who
upload ﬁles is necessary. To protect the network, everyone should run anti-virus software on a regular basis.
The need for a ﬁrewall implies a connection to the outside world. By assessing the types of commu-
nications expected to cross between an Intranet and the Internet, one can formulate a speciﬁc ﬁrewall
design. Some of the questions that should be asked when designing a ﬁrewall strategy include:
• Will Internet-based users be allowed to upload or download ﬁles to or from the company server?
• Are there particular users (such as competitors) that should be denied all access?
• Will the company publish a Web page?
• Will the site provide telnet support to Internet users?
• Should the company’s Intranet users have unrestricted Web access?
• Are statistics needed on who is trying to access the system through the ﬁrewall?
• Will a dedicated staff be implemented to monitor ﬁrewall security?
• What is the worst-case scenario if an attacker were to break into the Intranet? What can be done
to limit the scope and impact of this type of scenario?
• Do users need to connect to geographically dispersed Intranets?

© 2001 by CRC Press LLC
There are three main types of ﬁrewalls: network level, application level, and circuit level. Each type of
ﬁrewall provides a somewhat different method of protecting the Intranet. Firewall selection should be
based on the organization’s security needs.
Network, Application, and Circuit-level Firewalls 
Network-level Firewall 
A network-level ﬁrewall is typically a router or special computer that examines packet addresses, and
then decides whether to pass the packet through or to block it from entering the Intranet. The packets
contain the sender and recipient IP address, and other packet information. The network-level router
recognizes and performs speciﬁc actions for various predeﬁned requests. Normally, the router (ﬁrewall)
will examine the following information when deciding whether to allow a packet on the network:
• Source address from which the data is coming
• Destination address to which the data is going
• Session protocol such as TCP, UDP, or ICMP
• Source and destination application port for the desired service
• Whether the packet is the start of a connection request
If properly installed and conﬁgured, a network-level ﬁrewall will be fast and transparent to users.
Application-level Firewall 
An application-level ﬁrewall is normally a host computer running software known as a proxy server. A
proxy server is an application that controls the trafﬁc between two networks. When using an application-
level ﬁrewall, the Intranet and the Internet are not physically connected. Thus, the trafﬁc that ﬂows on
one network never mixes with the trafﬁc of the other because the two network cables are not connected.
The proxy server transfers copies of packets from one network to the other. This type of ﬁrewall effectively
masks the origin of the initiating connection and protects the Intranet from Internet users.
Because proxy servers understand network protocols, they can be conﬁgured to control the services
performed on the network. For example, a proxy server might allow ftp ﬁle downloads, while disallowing
ftp ﬁle uploads. When implementing an application-level proxy server, users must use client programs
that support proxy operations.
Application-level ﬁrewalls also provide the ability to audit the type and amount of trafﬁc to and from
a particular site. Because application-level ﬁrewalls make a distinct physical separation between an
Intranet and the Internet, they are a good choice for networks with high-security requirements. However,
due to the software needed to analyze the packets and to make decisions about access control, application-
level ﬁrewalls tend to reduce the network performance.
Circuit-level Firewalls 
A circuit-level ﬁrewall is similar to an application-level ﬁrewall in that it, too, is a proxy server. The
difference is that a circuit-level ﬁrewall does not require special proxy–client applications. As discussed
in the previous section, application-level ﬁrewalls require special proxy software for each service, such
as ftp, telnet, and HTTP.
In contrast, a circuit-level ﬁrewall creates a circuit between a client and server without needing to
know anything about the service required. The advantage of a circuit-level ﬁrewall is that it provides
service for a wide variety of protocols, whereas an application-level ﬁrewall requires an application-level
proxy for each and every service. For example, if a circuit-level ﬁrewall is used for HTTP, ftp, or telnet,
the applications do not need to be changed. You simply run existing software. Another beneﬁt of circuit-
level ﬁrewalls is that they work with only a single proxy server, making it easier to manage, log, and
control a single server than multiple servers.
2.1.7.1.1
Firewall Architectures
Combining the use of both a router and a proxy server into the ﬁrewall can maximize the Intranet’s
security. The three most popular ﬁrewall architectures are the dual-homed host ﬁrewall, the screened
host ﬁrewall, and the screened subnet ﬁrewall. The screened-host and screened-subnet ﬁrewalls use a
combination of routers and proxy servers.

© 2001 by CRC Press LLC
Dual-homed Host Firewalls 
A dual-homed host ﬁrewall is a simple, yet very secure conﬁguration in which one host computer is
dedicated as the dividing line between the Intranet and the Internet. The host computer uses two separate
network cards to connect to each network. When using a dual-home host ﬁrewall, the computer routing
capabilities should be disabled, so the two networks do not accidentally become connected. One of the
drawbacks of this conﬁguration is that it is easy to inadvertently enable internal routing.
Dual-homed host ﬁrewalls use either an application-level or a circuit-level proxy. Proxy software
controls the packet ﬂow from one network to another. Because the host computer is dual-homed (i.e.,
it is connected to both networks), the host ﬁrewall can examine packets on both networks. It then uses
proxy software to control the trafﬁc between the networks.
Screened-host Firewalls 
Many network designers consider screened-host ﬁrewalls more secure than a dual-homed host ﬁrewall.
This approach involves adding a router and placing the host computer away from the Internet. This is a
very effective and easy-to-maintain ﬁrewall. A router connects the Internet to your Intranet and, at the
same time, ﬁlters packets allowed on the network. The router can be conﬁgured so that it sees only one
host computer on the Intranet network. Users on the network who want to connect to the Internet must
do so through this host computer. Thus, internal users appear to have direct access to the Internet, but
the host computer restricts access by external users.
Screened-subnet Firewalls 
A screened-subnet ﬁrewall architecture further isolates the Intranet from the Internet by incorporating
an intermediate perimeter network. In a screened-subnet ﬁrewall, a host computer is placed on a
perimeter network which users can access through two separate routers. One router controls Intranet
trafﬁc and the second controls the Internet trafﬁc. A screened-subnet ﬁrewall provides a formidable
defense against attack. The ﬁrewall isolates the host computer on a separate network, thereby reducing
the impact of an attack to the host computer. This minimizes the scope and chance of a network attack.
2.1.7.2
CGI Scripting
Web Sites that provide two-way communications use CGI (common gateway scripting). For example, if
you ﬁll in a form and click your mouse on the form’s Submit button, your browser requests the server
computer to run a special program, typically a CGI script, to process the form’s content. The CGI script
runs on the server computer, which processes the form. The server then returns the output to the browser
for display.
From a security perspective, the danger of CGI scripts is that they give users the power to make a
server perform a task. Normally, the CGI process works well, providing an easy way for users to access
information. Unfortunately, it is also possible to use CGI scripts in ways they were never intended. In
some cases, attackers can shut down a server by sending potentially damaging data through the use of
CGI scripts. From a security perspective, it is important to make sure that users cannot use CGI scripts
to execute potentially damaging commands on a server.
2.1.7.3
Encryption
Encryption prevents others from reading your documents by “jumbling” the contents of your ﬁle in such
a way that it becomes unintelligible to anyone who views it. You must have a special key to decrypt the
ﬁle so its contents can be read. A key is a special number, much like the combination of a padlock, which
the encryption hardware or software uses to encrypt and decrypt ﬁles. Just as padlock numbers have a
certain number of digits, so do encryption keys. When people talk about 40-bit or 128-bit keys, they are
simply referring to the number of binary digits in the encryption key. The more bits in the key, the more
secure the encryption and less likely an attacker can guess your key and unlock the ﬁle. However, attackers
have already found ways to crack 40-bit keys.
Several forms of encryption can be used to secure the network, including: link encryption, document
encryption, secure-sockets layer (SSL), and secure HTTP (S-HTTP). The following sections describe
these encryption methods in more detail.

© 2001 by CRC Press LLC
2.1.7.3.1
Public-key Encryption
Public-key encryption uses two separate keys: a public key and a private key. A user gives his/her public
key to other users so anyone may send them encrypted ﬁles. The user activates his/her private key to
decrypt the ﬁles (which were encrypted with a public key).
A public key only allows people to encrypt ﬁles, not to decrypt them. The private user key (designed
to work in conjunction with a particular public key) is the only key that can decrypt the ﬁle. Therefore,
the only person that can decrypt a message is the person holding the private key.
2.1.7.3.2
Digital Signatures
A digital signature is used to validate the identity of the ﬁle sender. A digital signature prevents clever
programmers from forging e-mail messages. For example, a programmer who is familiar with e-mail
protocols can build and send an e-mail using anyone’s e-mail address, such as BillGates@microsoft.com.
When using public-key encryption, a sender encrypts a document using a public key, and the recipient
decodes the document using a private key. With a digital signature, the reverse occurs. The sender uses
a private key to encrypt a signature, and the recipient decodes the signature using a public key. Because
the sender is the only person who can encrypt his or her signature, only the sender can authenticate
messages. To obtain a personal digital signature, you must register a private key with a certiﬁcate authority
(CA), which can attest that you are on record as the only person with that key.
2.1.7.3.3
Link Encryption
Link encryption is used to encrypt transmissions between two distant sites. It requires that both sites
agree on the encryption keys that will be used. It is commonly used by parties that need to communicate
with each other frequently. Link encryption requires a dedicated line and special encryption software. It
is an expensive way to encrypt data. As an alternative to this, many routers have convenient built-in
encryption options. The most common protocols used for link encryption are PAP (Password Authen-
tication) and CHAP (Challenge Handshake Authentication Protocol). Authentication occurs at the data
link layer and is transparent to end-users.
2.1.7.3.4
Document Encryption
Document encryption is a process by which a sender encrypts documents that the recipient(s) must later
decrypt. Document encryption places the burden of security directly on those involved in the commu-
nication. The major weakness of document encryption is that it adds a step to the process by which a
sender and receiver exchange and receive documents. Because of this extra step, many users prefer to
save time by skipping the encryption. The primary advantage of document encryption is that anyone
with an e-mail account can use document encryption. Many document encryption systems are available
free or for little cost on the Internet.
2.1.7.3.5
Pretty Good Privacy (PGP)
Pretty good privacy (PGP) is a free (for personal use) e-mail security program developed in 1991 to
support public-key encryption, digital signatures, and data compression. PGP is based on a 128-bit key.
Before sending an e-mail message, PGP is used to encrypt the document. The recipient also uses PGP
to decrypt the document. PGP also offers a document compression option. Besides making a document
smaller, compression enhances the ﬁle security because compressed ﬁles are more difﬁcult to decode
without the appropriate key. According to the PGP documentation, it would take 300 billion years for
someone to use brute force methods to decode a PGP-encrypted, compressed message.
2.1.7.3.6
Secure Socket Layer (SSL)
The Secure Socket Layer (SSL) was developed by Netscape Communications to encrypt TCP/IP commu-
nications between two host computers. SSL can be used to encrypt any TCP/IP protocol, such as HTTP,
telnet, and ftp. SSL works at the system level. Therefore, any user can take advantage of SSL because the
SSL software automatically encrypts messages before they are put onto the network. At the recipient’s
end, SSL software automatically converts messages into a readable document.
SSL is based on public-key encryption and works in two steps. First, the two computers wishing to
communicate must obtain a special session key (the key is valid only for the duration of the current

© 2001 by CRC Press LLC
communication session). One computer encrypts the session key and transmits the key to the other
computer. Second, after both sides know the session key, the transmitting computer uses the session key
to encrypt messages. After the document transfer is complete, the recipient uses the same session key to
decrypt the document.
2.1.7.3.7
Secure HTTP (S-HTTP)
Secure HTTP is a protocol developed by the CommerceNet coalition. It operates at the level of the HTTP
protocol. S-HTTP is less widely supported than Netscape’s Secure Socket Layer. Because S-HTTP works
only with HTTP, it does not address security concerns for other popular protocols, such as ftp and telnet.
S-HTTP works similarly to SSL in that it requires both the sender and receiver to negotiate and use
a secure key. Both SSL and S-HTTP require special server and browser software to perform the encryption.
2.1.7.4
Intranet Security Threats
This section examines additional network threats that should be considered when implementing Intranet
security policies.
2.1.7.4.1
Source-routed Trafﬁc
As discussed earlier, packet address information is contained in the packet header. When source routing
is used, an explicit routing path for the communication can be chosen. For example, a sender could map
a route that sends packets from one speciﬁc computer to another, through a speciﬁc set of network nodes.
The road map information contained in the packet header is called source routing, and it is used mainly
to debug network problems. It is also used in some specialized applications. Unfortunately, clever pro-
grammers can also use source routing to gain (unauthorized) access into a network. If a source-routed
packet is modiﬁed so that it appears to be from a computer within your network, a router will obediently
perform the packet routing instructions, permitting the packet to enter the network, unless special
precautions are taken. One way to combat such attacks is simply to direct your ﬁrewall to block all source-
routed packets. Most commercial routers provide an option to ignore source-routed packets.
2.1.7.4.2
Protecting Against ICMP Redirects (Spooﬁng)
ICMP stands for Internet Control Message Protocol. ICMP deﬁnes the rules routers use to exchange
routing information. After a router sends a packet to another router, it waits to verify that the packet
actually arrived at the speciﬁed router. Occasionally, a router may become overloaded or may malfunction.
In such cases, the sending router might receive an ICMP-redirect message that indicates which new path
the sending router should use for transmission.
It is fairly easy for knowledgeable “hackers” to forge ICMP-redirect messages to reroute communication
trafﬁc to some other destination. The term spooﬁng is used to describe the process of tricking a router
into rerouting messages in this way. To prevent this type of unauthorized access, it may be necessary to
implement a ﬁrewall that will screen ICMP trafﬁc.
2.1.8
Summary
Intranets are being used to improve the overall productivity of the organization. Important Intranet
concepts covered in this chapter are summarized below:
• TCP/IP was created because of the need for reliable networks that could span the globe. Because
of its reliability and ease of implementation, TCP/IP has become the standard language (or
protocol) of the Internet. TCP/IP deﬁnes how programs exchange information over the Internet.
• An Intranet is based on Internet technology. It consists of two types of computers: a client and a
server. A client asks for and uses information that the server stores and manages.
• Telnet, ftp, and gopher are widely used network programs that help users connect to speciﬁc
computers and to transfer and exchange ﬁles.
• The World Wide Web (or Web) is a collection of interlinked documents that users can access and
view as “pages” using a special software program called a browser. The two most popular browser
programs are Netscape Navigator and Microsoft Internet Explorer.

© 2001 by CRC Press LLC
• HTML (hypertext markup language) and XML (extended markup language) are used to describe
the layout and contents of pages on the Web.
• Java is a new computer programming language that allows users to execute special programs (called
applets) while accessing and viewing a Web page.
• A network computer is a low-cost, specialized computer designed to work in conjunction with
the Internet and Java application programs.
To be effective, the Intranet must deliver quality information content. To ensure this, management must
be in a proactive role in assigning staff who will keep the corporate information reservoirs on the Intranet
current and relevant. The following is a checklist of some of the ways to encourage the development of
a high-quality Intranet:
• Give users access to document management systems and various corporate databases.
• Distribute the responsibility of maintaining the Intranet to increase the number of staff involved
in developing and enhancing Intranet content.
• Create a corporate culture based on information sharing.
• Place employee training at the center of an Intranet deployment strategy.
• Design and implement appropriate security measures as soon as possible.
• Use ﬁrewalls to control access to the network.
• Use anti-virus software.
• Implement a security plan that controls the access that employees and outsiders have to the
network.
• Design and implement CGI scripts with security in mind.
• Encourage users to encrypt ﬁles before sending conﬁdential data across the Internet/Intranet.
2.2
Internet Security
John Braun
The Internet offers the ability for anyone, from an individual computer owner to a major multinational
corporation, to make information and computing resources available to the world. Alas, since the Internet
was initially designed with ease of communications, rather than security, in mind, care must be taken to
ensure that sensitive information is not made available to the world as well. This section will discuss
several aspects of protection information that one makes available via a network or the Internet.
2.2.1
Physical Security
Although not as trendy or exciting as some of the exotic attacks that can be made against a network on
the protocol level, physical security is nonetheless important. You can have the best security software in
the world installed on your network, but it may do you little good if an attacker can waltz up to a key
piece of network or computing equipment and disable it!
Key pieces of network hardware, such as routers, ﬁrewalls, and servers, should be stored in a secure
room with some sort of access control such as a traditional or electronic lock, card reader, or other means
which can limit access to authorized individuals. Access to especially sensitive devices should be further
restricted by placing them in a locked cabinet. Also, access to buildings that contain these rooms should
also be controlled with security guards or other access control so that visitors can’t wander about.
Exposed network cables, especially those connected to routers, hubs, and other devices which carry
trafﬁc for several other users, should also be physically protected. If an attacker has access to these cables,
they could physically cut them and insert equipment that could monitor and even generate network
trafﬁc. For maximum protection, network cables should be placed inside of pressurized pipes with sensors
placed on various locations along the piping. Network monitoring tools should also be used so that a
break in a cable can be identiﬁed quickly.

© 2001 by CRC Press LLC
2.2.2
Modems
Modems present two security threats. First, modems offer a channel for data to leave your premises,
circumventing security and auditing measures that may be in place for the rest of the network. A review
of services that are accessed by modem should be made, and, if possible, this access should be rerouted
over a secure internal network. Second, modems offer a potential method for unauthorized individuals
to access your network from the outside. Since there may be a need for users who are on the road or
working from home to access a network remotely, additional security measures need to be taken for these
connections. One measure is a dial-back modem, which will call back the user at a predetermined number
before allowing access to the network. Another is a system where each user is provided with an electronic
card that displays a random number every few minutes. A similar device that performs the same calcu-
lation to produce this number is located on the network one wishes to access. Without the card, and the
ability to produce this number, the remote user is denied access.
2.2.3
Data Security
There are many aspects to data security. One is to prevent ﬁles from being viewed by someone other
than the owner. On a multi-user system, this concern can be addressed by proper system administration.
Users should not be allowed access to directories or ﬁles that do not belong to them. On a single-user
system, the ability to share ﬁles over the network should be closely monitored, so that users don’t
inadvertently allow access of their hard drive contents to anyone who cares to look. Another aspect of
data security is to protect the contents of ﬁle or network data via encryption. This can be especially
important for data travelling over a network, since the data can be broadcast to other terminals or network
devices which are not the intended recipients of the data. By using encryption, data that falls into the
wrong hands will be unusable unless an encryption key or password is also known. Although security
at the TCP/IP level is still a ways off, several third-party products provide the ability to protect and
encrypt data.
2.2.4
Passwords
Since passwords usually comprise an initial layer of defense against an attack, they should be chosen and
implemented with care. A written policy and/or enforcement by the operating system can help. Passwords
should not be dictionary words, should be as long as possible, contain a series of letters, numbers, and other
characters, and be changed on a regular basis. When deciding on a policy, care should be taken to balance
security needs vs. ease of use. If a policy is too tedious to follow, users may end up writing their passwords
down somewhere near their terminal, eliminating any sort of beneﬁt the password policy could have offered.
2.2.5
Workstation Security
Unattended workstations could be a great danger to the entire system, and a security system could be
completely wasted if an unauthorized person could access someone else’s logged-in workstation. For that
reason, users need to be aware of this danger and be properly trained how to secure an unattended
workstation, either by logging off or by using a screen saver or screen lock which activates after a short
amount of inactivity.
2.2.6
TCP/IP Security
Since the current version of TCP/IP was designed to provide a robust, standard method of moving data
on a network, rather than security, one should be aware of several attacks which could compromise
network security or availability.
2.2.6.1
IP Spooﬁng
Spooﬁng is the act of altering the contents of a TCP or IP packet header in order to trick the remote
system into thinking the packet is valid. One trick is to change the source IP address of a packet to an

© 2001 by CRC Press LLC
address that is valid on a network behind a ﬁrewall or router. Older equipment that would have otherwise
blocked the packet will allow it to go through since it appears to be coming from a friendly network.
There are attacks where a connection can be hijacked or terminated by combining IP address spooﬁng
with the spooﬁng of the SEQ and ACK ﬁelds in a TCP header. The SEQ and ACK ﬁelds help synchronize
trafﬁc between two hosts. If these ﬁelds are modiﬁed by attackers, the attackers can take over connections,
while legitimate hosts lose the connection since their packets now appear to be out of order. Additional
ﬁelds could be activated so that the connection is terminated prematurely.
2.2.6.2
Denial of Service (DoS)
Many DoS attacks take advantage of nuances in the method used to establish a TCP/IP connection. Since
connections may take a while to establish, portions of the TCP/IP establishment process include timeouts
so that slow equipment or busy networks will not cause a connection attempt to fail. However, a program
which intentionally completes only a portion of this negotiation will result in a host waiting for a
connection to complete, when it never will. While the host is waiting for the connection attempt to time
out, system resources are being used. If enough of these bogus attempts are made, the host will run out
of resources, and future connection attempts will be refused.
Another major type of attack involves the sending of single packets, whose contents have been modiﬁed
to some unexpected or invalid data. This can result in the remote system crashing with a nasty Blue Screen
of Death, system bomb, or core dump. One attack sends an ICMP (a special type of IP packet, with no
TCP) echo request, also known as a ping, whose data payload is very large. Since a ping packet normally
has no data associated with it, some implementations that don’t expect this data will grind to a halt when
receiving this type of packet. Another attack interferes with the data offset ﬁeld in the TCP header, so that
the remote host is tricked into trying to read packet data where none exists, also causing a crash.
2.3
Virtual Private Networking Solutions
Endre Sara
Today’s large corporate networks are geographically distributed and clients or employees need access to
corporate information from different locations. The cost of a long-distance dialup session is very high,
and it is also not efﬁcient to deploy point-to-point connection between each possible location.
Virtual private network (VPN) is a concept of securely transferring sensitive corporate information
between various geographically dispersed sites over a public network, such as the Internet. The market
numbers for these services tell a success story and a win/win situation for both providers and users. But,
while the market numbers are good, numerous concerns remain. They are:
• There are not enough integration services to help users deploy VPNs
• The products are not yet interoperable
• Security standards are not yet unique
• There are different protocol standards
• Many users are not yet fully comfortable using Internet technologies in the mainline business
Typical users of VPNs are driving the implementation of VPN services. The most important beneﬁts
and points to consider are summarized in Table 2.3.1. Weights are not included in this consideration.
This document describes and compares the available standards and products for VPN solutions.
The VPN is a network that uses a private address space which operates over another network infra-
structure. It means that the VPN will use the same physical cabling, switches, bridges, and routers, but
it uses a different address space. This is accomplished by encapsulating the VPN trafﬁc (which doesn’t
have to be IP) into secure protocols. The emerging standards concentrate around Layer 2 and Layer 3
protocols.

© 2001 by CRC Press LLC
2.3.1
Layer 2 Protocols
Layer 2 protocols enable the transfer of data from a remote client to the private network of an enterprise
by creating a virtual private network most often across a TCP/IP-based data network. Layer 2 protocols
support on-demand, multi-protocol virtual private networking over public networks, such as the Internet.
Internet access is provided by an internet service provider (ISP), who wishes to offer services other than
traditional registered IP address-based service to dial-up users of the network.
This architecture is transparent to the end systems. In case of connecting two distant local area networks
(LANs) through a VPN, the users will notice no difference while their trafﬁc is being encapsulated in IP
packets and transmitted to the remote VPN access server, which puts them back to the remote LAN. If
a remote user wants to connect to the private network of the enterprise through a VPN connection,
his/her computer has to support the implemented VPN protocol to be able to encapsulate the trafﬁc.
Although this encapsulation provides some security against intercepting the actual data, additional
encryption should be implemented to provide secure communication.
2.3.1.1
Point-to-Point Tunneling Protocol (PPTP)
The PPTP networking technology is deﬁned as an extension to the remote access Point-to-Point Protocol
(RFC1171). PPTP is a network protocol that encapsulates PPP packets into IP datagrams for transmission
over the Internet or other public TCP/IP-based networks.
After the client has made the initial PPP connection to the ISP, a second dial-up networking call is
made over the existing PPP connection. Data sent using this second connection is in the form of IP
datagrams that contain PPP packets, referred to as encapsulated PPP packets.
The second call creates the VPN connection to a PPTP server on the private enterprise LAN, referred
to as a tunnel. This is shown in Figure 2.3.1.
The secure communication using the PPTP protocol typically involves three processes, each of which
requires successful completion of the previous process:
PPP Connection and Communication — The PPTP client uses PPP to connect to an ISP by using a
standard phone line or ISDN line. This connection uses the PPP protocol to establish the connection
and encrypt data packets.
PPTP Control Connection — Using the connection to the Internet established by the PPP protocol,
the PPTP protocol creates a control connection from the PPTP client to the PPTP server over the Internet.
This connection uses TCP to establish the connection and is called a PPTP tunnel.
TABLE 2.3.1
VPN Beneﬁts and Concerns
VPN Applications
Beneﬁts
Points to Consider
Dial access for 
remote users
Outsource modems reduce dial-in costs
Client software
Eliminate access lines
Are appropriate tunneling protocols 
supported in client software?
Connecting 
branch ofﬁces
Reduces number of dedicated lines
Encryption performance issues
Lets IT managers consolidate central-site WAN 
equipment
Does VPN access-control system integrate 
with existing user access privileges?
Extranet
Gives trading partners and customers access to intranet
Does system scale well?
Makes collaborating with contractors and consultants 
much easier
Are there tools to handle the administrative 
burden of adding new users?
New business
Can create just-in-time networks for short-term projects
Interoperability of different VPN 
equipment
Can give worldwide sites access much sooner than 
waiting for leased lines
Management of mixed equipment 
environment is not easy

© 2001 by CRC Press LLC
PPTP Data Tunneling — Finally, the PPTP protocol creates IP datagrams containing encrypted PPP
packets, which are sent through the PPTP tunnel to the PPTP server. The PPTP server disassembles the
IP datagrams and decrypts the PPP packets, and then routes the decrypted packets to the private network.
Note that the encapsulated PPP packet can contain multi-protocol data such as TCP/IP, IPX, or
NetBEUI protocols. Because the PPTP server is conﬁgured to communicate across the private network
by using private network protocols, it is able to read multi-protocol packets.
Figure 2.3.2 illustrates the multi-protocol support built into PPTP. A packet sent from the PPTP client
to the PPTP server passes through the PPTP tunnel to a destination computer on the private network.
PPTP encapsulates the encrypted and compressed PPP packets into IP datagrams for transmission
over the Internet. These IP datagrams are routed over the Internet until they reach the PPTP server that
is connected to the Internet and the private network. The PPTP server disassembles the IP datagram into
a PPP packet and then decrypts the PPP packet using the network protocol of the private network. As
mentioned earlier, the network protocols on the private network that are supported by PPTP are IPX,
NetBEUI, or TCP/IP.
An ISP’s network access server may require initial dial-in authentication. If this authentication is
required, it is strictly to log on to the ISP network; it is not related to the PPTP server authentication.
There are different options to provide data encryption between the PPTP client and the server.
Microsoft uses the RAS “shared secret” encryption process. It is referred as a shared secret, because both
ends of the connection share the encryption key. Under Microsoft’s implementation of RAS, the shared
secret is the user password. Other encryption methods base the encryption on some key available in
public; this method is known as public key encryption. Microsoft’s PPTP uses the PPP encryption and
PPP compression schemes called Microsoft Point-to-Point Encryption (MPPE). The Compression Con-
trol Protocol (CCP) used by PPP is used to negotiate encryption. The encryption key is derived from
the hashed password stored at both the client and the server. The RSA RC4 standard is used to create
the 40-bit session key based on the client password. This key is used to encrypt all data that is passed
over the Internet, keeping the connection private and secure.
FIGURE 2.3.1
The PPTP tunnel.
FIGURE 2.3.2
Connecting a dial-up networking PPTP client to the private network.
TCP connection
PPP connection
PPTP control connection
IP datagrams
Internet
PPTP/Server
Client
Network
Access
Server
Virtual Private
Network
Valid Internet
ID addresses used
Internal addressing
schemes used
encrypted
PPP
GRE
PPP
GRE
PPP
IP     IPX   NetBEUI
IP     IPX    NetBEUI
IP     IPX    NetBEUI
DATA
DATA
DATA
PSTN or
ISDN
Network Access
Server
PPTP
Server
Private
Network
Internet

© 2001 by CRC Press LLC
PPTP is aimed primarily at Internet-based remote access. The main advantages are multi-protocol
support and simplicity, because it functions on the Layer 2 level. This can be a preferred solution in a
multi-protocol environment, but data security is a concern. The proposed standard does not provide a
data encryption solution, although Microsoft has a vendor-speciﬁc solution as discussed earlier. The
other difference is, compared to Layer 3 solutions, PPTP only provides a single point-to-point connection.
It means that there can be no simultaneous Internet access while using a VPN connection. With multi-
point tunneling, such as a Layer 3 solution discussed later, a user could have an Internet session at the
same time as several VPN connections. This is also an inherent consequence from the PPTP architecture
being a client–server model-based solution, while Layer 3 solutions are based on a more general host-
to-host model.
2.3.1.2
Layer 2 Forwarding (L2F)
L2F achieves private network access through a public system by building a secure “tunnel” across the
public infrastructure that connects directly to a user’s home gateway. Multiple corporate networks can
use a single local telephone number terminated on a service provider’s dialup switch or access server.
The access server establishes identity, sets up a private tunnel to the user’s home gateway router, and
tunnels clients to that gateway. The gateway is responsible for authentication of the remote user, thereby
ensuring client control of access security and addressing.
A key component of the virtual dialup service is tunneling, a vehicle for encapsulating packets inside
a protocol that is understood at the entry and exit points of a given network. These entry and exit points
are deﬁned as a tunnel interfaces. The tunnel interface itself is similar to a hardware interface, but is
conﬁgured in software.
Figure 2.3.3 shows the format in which a packet would traverse the network within a tunnel.
Tunneling involves the following three types of protocols:
• The passenger protocol is the protocol being encapsulated; in a dialup scenario, this protocol could
be PPP, SLIP, or text dialog
• The encapsulating protocol is used to create, maintain, and tear down the tunnel. Cisco supports
several encapsulating protocols including the L2F protocol, which is used for virtual dialup services
• The carrier protocol is used to carry the encapsulated protocol; IP will be the ﬁrst carrier protocol
used by the L2F protocol, because of IP’s robust routing capabilities, ubiquitous support across
different medias, and deployment within the Internet
No dependency exists between the L2F protocol and IP. In subsequent releases of the L2F functionality,
Frame Relay, X.25 VCs, and asynchronous transfer mode (ATM) switched virtual circuits (SVCs) could
be used as a direct Layer 2 carrier protocol for the tunnel.
Cisco’s L2F implementation provides several management features. End system transparency ensures
that neither the remote end system nor its corporate hosts should require any special software to use this
service. Authentication is provided by dialup PPP, Challenge Handshake Authentication Protocol
(CHAP), or Password Authentication Protocol (PAP), including Terminal Access Controller Access Con-
trol System Plus (TACACS+) and Remote Authentication Dial-In User Service (RADIUS) solutions, as
well as support for smart cards and one-time passwords; the authentication will be manageable by the
user independent of the ISP. Addressing will be as manageable as dedicated dialup solutions; the address
will be assigned by the remote user’s respective corporation, and not the ISP. Authorization will be
FIGURE 2.3.3
Tunneling packet format.
PPP (Data)
L2F
IP/UDP
Carrier
Protocol
Encapsulator
Protocol
Passenger
Protocol

© 2001 by CRC Press LLC
managed by the corporation’s remote users, as it would be in a direct dialup solution. Accounting will
be performed both by the ISP (for billing purposes) and by the user (for charge back and auditing).
Figure 2.3.4 illustrates the topology of a virtual private connection using Cisco’s L2F.
In a traditional dialup scenario, the ISP using the NAS in conjunction with a security server follows
an authentication process by challenging the remote user for both the username and password. If the
remote user passes this phase, the authorization phase can begin.
For the virtual dialup service, the ISP pursues authentication to the extent required to discover the
users’ apparent identity (and by implication, their desired corporate gateway). No password interaction
is performed at this point. As soon as the corporate gateway is determined, a connection is initiated with
the authentication information gathered by the ISP. The corporate gateway completes the authentication
by either accepting or rejecting the connection. (For example, the connection is rejected in a PAP request
in which the username or password are found to be incorrect.) Once the connection is accepted, the
corporate gateway can pursue another phase of authentication at the PPP layer. These additional authen-
tication activities are outside the scope of the speciﬁcation, but might include proprietary PPP extensions,
or textual challenges carried within a TCP/IP Telnet session.
For each L2F tunnel established, L2F tunnel security generates a unique random key to resist spooﬁng
attacks. Within the L2F tunnel, each multiplexed session maintains a sequence number to prevent the
duplication of packets.
Cisco provides the ﬂexibility of allowing users to implement compression at the client end. In addition,
encryption on the tunnel can be done using IPSEC.
There are similar advantages of L2F as of PPTP, because the two solutions are similar, and being merged
to a common standard called L2TP. L2F also has a nice advantage of connecting multi-protocol networks,
because of its Layer 2 functionality. But again there is no support in the proposed standard for VPN data
encryption. Cisco refers to the IPSec standard as a possible encryption method for IP trafﬁc carried with
L2F. For non-IP trafﬁc L2F lacks the solution for security. In comparison with Layer 3 solutions, L2F
provides only a single point-to-point connection, which makes parallel Internet access impossible while
being connected to the VPN.
2.3.1.3
L2TP
The IETF draft for PPTP titled as “Point-to-Point Tunneling Protocol,” draft-ietf-pptp-00.txt was sub-
mitted to the Internet Engineering Task Force (IETF) in June 1996 by the companies of the PPTP Forum,
which includes Microsoft Corporation, Ascend Communications, 3Com/Primary Access, ECI Telematics
and US Robotics. Cisco’s proposal for L2F was submitted to the IETF for approval as a proposed standard.
Northern Telecom, Inc., and Shiva Corporation have announced their support for L2F. At the June 1996
IETF meeting in Montreal, the IETF PPP Extensions working group agreed to combine Cisco’s proposal
with PPTP proposed by Microsoft Corporation. The emerging proposed standard, Layer 2 Tunneling
Protocol (L2TP) is currently drafted by Cisco Systems, Microsoft, Ascend, 3Com, and US Robotics. The
latest L2TP draft (09) was submitted in January 1998.
A typical connection scenario would start with the remote user initiating a PPP connection to an ISP
via either the PSTN or ISDN. The ISP’s access point (LAC) accepts the connection and the PPP link is
established. The ISP may now undertake a partial authentication of the end system/user. Only the
username ﬁeld would be interpreted to determine whether the user requires a Virtual dial-up service. It
FIGURE 2.3.4
Virtual dialup topology.
Remote
User
PSTN
PRI
NAS
ISP/Internet
Corporate
Gateway
Corporate
LAN

© 2001 by CRC Press LLC
is expected — but not required — that usernames will be structured (e.g., username@company.com).
Alternatively, the ISP may maintain a database mapping users to services. In the case of Virtual dial-up,
the mapping will name a speciﬁc endpoint, the L2TP Network Server (LNS). If a virtual dial-up service
is not required, standard access to the Internet may be provided.
If no tunnel connection currently exists to the desired LNS, one is initiated. L2TP is designed to be
largely insulated from the details of the media over which the tunnel is established; L2TP requires only
that the tunnel media provide packet-oriented point-to-point connectivity. Obvious examples of such
media are UDP, Frame Relay PVCs, or X.25 VCs. Once the tunnel exists, an unused slot within the tunnel,
a “Call ID,” is allocated, and a connect indication is sent to notify the LNS of this new dial-up session.
The LNS either accepts the connection, or rejects it. The initial connect notiﬁcation may include the
authentication information required to allow the LNS to authenticate the user and decide to accept or
decline the connection. In the case of CHAP, the set-up packet includes the challenge, username, and
raw response. For PAP or text dialog, it includes username and clear text password. The LNS may choose
to use this information to complete its authentication, avoiding an additional cycle of authentication.
If the LNS accepts the connection, it creates a “virtual interface” for PPP in a manner analogous to
what it would use for a direct-dialed connection. With this virtual interface in place, link layer frames
may now pass over this tunnel in both directions. Frames from the remote user are received at the POP,
stripped of CRC, link framing, and transparency bytes, encapsulated in L2TP, and forwarded over the
appropriate tunnel. The LNS accepts these frames, strips L2TP, and processes them as normal incoming
frames for the appropriate interface and protocol. The virtual interface behaves very much like a hardware
interface, with the exception that the hardware in this case is physically located at the ISP POP. The other
direction behaves analogously, with the LNS encapsulating the packet in L2TP, and the LAC stripping
L2TP before transmitting it out via the physical interface to the remote user.
At this point, the connectivity is a point-to-point PPP session whose endpoints are the remote user’s
networking application on one end and the termination of this connectivity into the LNS’s PPP support
on the other. Because the remote user has become simply another dial-up client of the LNS, client
connectivity can now be managed using traditional mechanisms with respect to further authorization,
protocol access, and packet ﬁltering.
Accounting can be performed at both the L2TP Access Concentrator (LAC) as well as the LNS. This
document illustrates some accounting techniques that are possible using L2TP, but the policies surround-
ing such accounting are outside the scope of this speciﬁcation.
For the virtual dial-up service, the ISP pursues authentication only to the extent required to discover
the user’s apparent identity (and by implication, their desired LNS). This may involve no more than
detecting DNIS information when a call arrives, or may involve full LCP negotiation and initiation of PPP
authentication. As soon as the apparent identity is determined, a call request to the LNS is initiated with
any authentication information gathered by the ISP. The LNS completes the authentication by either
accepting the call, or rejecting it. The LNS may need to protect against attempts by third parties to establish
tunnels to the LNS. Tunnel establishment can include authentication to protect against such attacks.
L2TP, like other Layer 2 protocols, does not provide any further security for data encryption, but
rather refers to Layer 3 encryption techniques for IP trafﬁc, such as IPSec.
Although L2TP seems to be the result of different Layer 2 Tunneling initiatives, it still does not have
the widest acceptance in the industry. Its predecessor PPTP is popular, because of the large number of
Windows NT users, but IPSec, a general initiative to add security to the IP protocol, has the strongest
support by manufacturers and suppliers. In a multi-protocol environment there will still be a need for
Layer 2 Tunneling with the addition of encryption for IP trafﬁc (using IPSec). But in an IP-only
environment Layer 3 solutions are more effective. Unless augmented with IPSec these Layer 2 solutions
cannot support extranets, because extranets require keys and key management.
In comparison with other Layer 2 protocols, L2TP has better features, such as an addition to PPTP
and L2F, and the support for ATM or SONET as an underlying transmission medium, which is only
planned for the other two protocols.

© 2001 by CRC Press LLC
2.3.2
Layer 3 Tunneling Protocols
In the previously discussed architecture the PPP connection begins at the remote client and terminates
at the corporate network’s L2TP server, going through the ISP access point and the corporate gateway.
Layer 3 tunneling proposes a different scenario initiating the PPP connection from the remote client,
but terminating it at the ISP. This requires the re-encapsulation of the PPP frame and transmitting the
Layer 3 information only to the corporate access router. This access router does not need to support any
additional standard, but it acts only as a simple router. The difference from traditional dial-up services
is that the ISP’s IP Gateway will provide the IP address to the client. It can roam with this address as
long as the IP Gateway does the reframing of the IP PPP packet and sends it to the corporate gateway
as a standard IP packet.
The advantage of the latter scheme is that there is no need to support L2TP at either the remote client
end or at the corporate gateway end. The remote client only needs a standard IP stack, and the corporate
gateway acts as a simple IP router. In this case if the remote node is a router connecting a sub-network
to the corporate network, the packets can be routed just like any other trafﬁc trough the ISP network.
In either case an additional functionality is needed to provide security on the IP Layer (network layer).
Although L2TP can encrypt the PPP packets on Layer 2, it does not claim to be secure against denial of
service attacks or man-in-the-middle attacks (someone modifying the PPP frames in the tunnel).
2.3.2.1
IPSec
IPSec is a protocol suite deﬁned by the IETF working group on IP security to secure communication at
the Layer 3 (network layer) between communicating peers. The goal of the IPSec protocol suite is to
provide secure tunneled transport of IP data only. Essentially, it takes private IP packets, performs data
security functions such as encryption, authentication, and integrity, then wraps these secured packets in
other IP packets for transport across the Net. Key management functions also will be a part of the IPSec
protocol suite. The IETF has issued ﬁve requests for comments — RFC 1825 through 1829. An interesting
note is that if IPv6 succeeds in replacing IPv4, IPSec will be the automatic Internet VPN standard since
it is integrated into the IPv6 speciﬁcations.
Like the Layer 2 VPN protocols, IPSec works as a LAN-to-LAN and dialup-to-LAN solution. It is
designed to support multiple encryption protocols, a feature that allows users to choose a desired amount
of data privacy. Obviously, IPSec will only be of value to companies that want to tunnel IP exclusively
since it doesn’t support other data protocols.
There are several different scenarios where IPSec can be used. In case two hosts A and B want to
communicate with each other through a ﬁrewall, the host A can tunnel packets to the ﬁrewall, the ﬁrewall
can decrypt/authenticate the packets, and send them to B based on its rules. In a different setup there
can be a secure tunnel between host A and host B, where the ﬁrewall is authorized to act as a key
management proxy, and has the capability to decrypt the packets and apply its packet ﬁltering policy. A
third setup is a combination of the previous two, where the inner payload is secured from host A to B,
and the outer payload is secured and tunneled through the ﬁrewall. The advantage of this scheme is that
the ﬁrewall is able to authenticate packets and decide whether to allow the packet without applying its
ﬁltering rules. This is typical of what happens today, where an employee gets into the network via dialup
PPP.
There is a different scheme, where packets have to be secured while travelling the Internet. In this case
IPSec will secure packets between two or more border routers of a topologically distributed organization.
In this case since security associations are set up between the border routers, any trafﬁc should go through
these routers. All packets between the two routers must contain valid IPSec, otherwise they will be dropped.
A growing number of VPN, security, and major network companies either support or plan to support
IPSec. It is also strongly supported by a user group consisting of manufacturers and suppliers. Although
it deals with IP-only trafﬁc, it is the most often recommended or chosen solution to ensure privacy in
VPN communication. It can be implemented as a single Layer 3 solution, but it can be implemented
over a Layer 2 solution to provide data encryption for IP trafﬁc. It is capable of maintaining multiple

© 2001 by CRC Press LLC
tunnels, including simultaneous VPN and public access connection inherently from its general host-to-
host model. It can also support extranets with its built-in key management functionality, which is missing
in other Layer 2 solutions.
2.3.2.2
Mobile IP
Mobile IP is intended to enable nodes to move from one IP subnet to another. It is just as suitable for
mobility across homogeneous media as it is for mobility across heterogeneous media. That is, Mobile IP
facilitates node movement from one Ethernet segment to another as well as it accommodates node
movement from an Ethernet segment to a wireless LAN, as long as the mobile node’s IP address remains
the same after such a movement.
Mobile IP introduces the following new functional entities:
Mobile node — A host or router that changes its point of attachment from one network or subnetwork
to another. A mobile node may change its location without changing its IP address; it may continue to
communicate with other Internet nodes at any location using its (constant) IP address, assuming link-
layer connectivity to a point of attachment is available.
Home agent — A router on a mobile node’s home network which tunnels datagrams for delivery to
the mobile node when it is away from home, and maintains current location information for the mobile
node.
Foreign agent — A router on a mobile node’s visited network which provides routing services to the
mobile node while registered. The foreign agent detunnels and delivers datagrams to the mobile node
that were tunneled by the mobile node’s home agent. For datagrams sent by a mobile node, the foreign
agent may serve as a default router for registered mobile nodes.
A mobile node is given a long-term IP address on a home network. This home address is administered
in the same way as a permanent IP address is provided to a stationary host. When away from its home
network, a care-of address is associated with the mobile node and reﬂects the mobile node’s current
point of attachment. In this case Mobile IP uses protocol tunneling to hide a mobile node’s home address
from intervening routers between its home network and its current location. The tunnel terminates at
the mobile node’s care-of address. The care-of address must be an address to which datagrams can be
delivered via conventional IP routing. At the care-of address, the original datagram is removed from the
tunnel and delivered to the mobile node.
The Mobile IP standard contains a very strong authentication between the mobile node and the home
agent to authenticate themselves. The default algorithm is keyed MD5, with a key size of 128 bits. This
will result in the mobile node’s trafﬁc being tunneled to its care-of address. But the standard does not
provide privacy protection; it rather refers to other IP encryption standards, such as IPSec.
The Mobile IP standard as with other Layer 3 standards has the advantage of scalability, security, and
reliability, but they are more complex to develop and, inherent from their Layer 3 functionality, they
only support a speciﬁc protocol, which is IP in this case.
2.3.3
Frame Relay
Traditionally, VPNs were provided in the form of broadband packet switched services, such as Frame
Relay, X.25, or ATM. Now, with growth of the Internet as a viable service infrastructure, it is possible to
run VPNs over an alternative protocol. As it can be seen from the previously discussed standards, these
latter solutions tend to be more popular. The reason is fairly simple: while traditional VPNs are very
useful for ﬁxed LAN-to-LAN connectivity, they do not easily accommodate individual users whose only
access to the outside world is in the form of their PC, a modem, and the public switched telephone
network. VPNs that run over IP are easily accessed by these users.
With any choice of the above-mentioned protocols, virtual circuit connections can be deﬁned between
remote locations, and the LAN trafﬁc can be bridged over these circuits. This usually provides an emulated
Layer 2 network for the users, which can be used to transmit multiprotocol trafﬁc. Although proper
authentication and encryption also has to be taken care of in these solutions, these networks are not as

© 2001 by CRC Press LLC
sensitive to security threats, as they are usually a private ATM, Frame Relay backbone of the provider,
but in any case less public than the Internet in the previous solutions.
The advantages of this solution are the built-in quality of service (QoS) guarranties that are part of the
virtual circuit deﬁnitions. Where the bandwidth availability could be argued in the past for an Internet-
based VPN solution compared to these solutions, nowadays the ISPs can also provide high-speed Internet
connections, especially when they utilize only their backbone network to provide VPN services. The
disadvantage of the Frame Relay, ATM, or X.25-based VPN services is that these means of access should
be available at each location where the VPN service has to be used. It usually means special equipment,
wiring, and additional management needs. The user mobility is not solved with these solutions.
2.3.4
Layer 2 or Layer 3 Comparison
The goal of Layer 2 tunneling protocols is to transport Layer 3 protocols such as AppleTalk, IP, and IPX across
the Internet. To achieve this, the architects of PPTP and L2F leveraged the existing Layer 2 PPP standard,
which is designed to transport different Layer 3 protocols across serial links. In these schemes, Layer 3 packets
are encased in PPP frames, which are then encased in IP packets for transport across the Internet.
From a security standpoint, Layer 2 tunneling protocols are insufﬁcient to be secure VPN solutions
on their own. None of these protocols provide the data encryption, authentication, or integrity functions
that are critical to maintaining VPN privacy. The L2TP speciﬁcation disclaims any data security functions
and refers IP data security to IPSec, but no serious security provisions or references are made for the
other Layer 2 protocols. In addition, none of these protocols provide a mechanism for key management,
which limits their scalability.
PPTP and L2F are vendor-speciﬁc, proprietary protocols, so interoperability is limited to products
from supporting vendors. In contrast, L2TP is a multivendor effort, so interoperability is not as much
of a problem. It is important to note that when utilizing tunneling protocols besides IP, users will have
to rely on vendor-speciﬁc data security features. On the upside, PPTP, L2F, and L2TP can transport
multiple protocols. They also function both in LAN-to-LAN and dial-up-to-LAN tunneling modes,
allowing them to cover the applications most desired for VPN.
In case of Layer 3 Tunneling there is no need for a globally unique address space, which is a requirement
with L2TP for remote client address assignments. This global address space doesn’t have to be registered,
since it is seen from the corporate network but not visible from the public network (Internet).
Another difference is that the tunneling causes an additional overhead as opposed to Layer 3 Tunneling,
which only sends regular IP packets after the ISP’s IP Gateway to the corporate network. (The tunneling
takes place only between the remote client and the IP Gateway.) This might make Layer 3 solutions more
scalable, but with the loss of the additional features, such as the freedom of network protocols that can
be used over the PPP layer.
References
Bay Networks: http://www.baynetworks.com/Solutions/vpn/
Cisco: http://www.cisco.com/warp/public/779/servpro/solutions/vpn/
Microsoft: http://www.microsoft.com/ntserver/nts/commserv/exec/feature/VPNFeatures.asp
Shiva: http://www.shiva.com/remote/vpn.html
3Com: http://www.3com.com/enterprise/vpn/
2.4
Effective Website Design
Karen M. Freundlich
The goal of an effective website should be to achieve the desired results using the best available technology
for the job. Many webmasters fall prey to the seduction of ﬁtting the goals of their site to the sophisticated

© 2001 by CRC Press LLC
tools now freely available. A measurable objective should be narrowly deﬁned before the site is even
designed. As in any good novel, the plot must be carefully laid out before the writing and editing take
place. This segment will review four steps to consider when creating a website to provide a business
solution.
In today’s world of “what you see is what you get” website creation tools it is easy to construct sites
similarly to what we did as children, creating anything out of the available blocks or Tinkertoys in the
bag. Instead, really effective websites, especially ones focused on ﬁnancially rewarding e-commerce, must
avoid the temptation to “build.” Measurable goals must be deﬁned, the production and maintenance
efforts determined, a system for measuring effectiveness put in place, and the intuitive layout tested and
determined before the “building” tools are contracted. This methodical process will more readily guar-
antee the site’s success.
2.4.1
Goals Deﬁned
The goals of the business behind the site must be strictly deﬁned. The website should be regarded as a
tool to run the business; a store, not just a storefront. E-commerce software is often accompanied by e-
commerce consulting for just this reason. It is simply not enough to have a clean, attractive, well-ﬁlled
store; the products must be suited to the market and carefully distributed according to supply and
demand. It is the same in the virtual world of the internet. There is a destructive misconception that
everything on the internet occurs in shortened time, for example, that a “web year” is only four months.
Like any computer, the web is run by humans, and just like the chain is as strong as the weakest link,
the speed of the web is only at fast as the humans that run it. Admittedly, the web can reduce commu-
nication times among the parties, but the thinking and planning process to design the systems takes the
same time on the part of the human. Trying to rush this is guaranteed to cause errors, especially in
business decisions. The web cannot make informed, intelligent business decisions more quickly. You must
engage the best minds to provide the best solutions. Only when the goals are clearly deﬁned to achieve
the business solutions should one proceed to the smaller details of implementing them.
2.4.2
Production and Maintenance Efforts
Once the goals are deﬁned, the next step is to determine what levels of production and maintenance are
needed to meet the goals. Since, presumably, the website is blazing a trail, it is prudent to use the least
complex solutions to meet your goals, then provide a term for evaluating the results and to better deﬁne
the needs, and ﬁnally upgrade according to your growth. It is useful to review and critique the approaches
used by similar or parallel industries. The web makes competitor research very accessible. The site should
be produced with maintenance objectives easily implemented. For instance, an administrator page can
be included which allows password-protected users to perform a variety of maintenance tasks on-line.
The inclusion of the maintenance goal in the site design will assure that a comprehensive and complete
job is achieved. Quality control should be an important objective, best planned for in advance.
2.4.3
A System for Measuring Effectiveness
In order to determine if the site is meeting its goals, there must be inherent systems in place to measure
the results. It takes much less effort to build your site with systems to track measurable objectives than
it is to try to ﬁgure out how and what to measure once the site and systems are designed. The types of
measures will vary based on the goals, but one way to approach the problem is to design the reports that
will communicate the results. Once the report’s answers to questions are put in writing, it is much easier
to determine what pieces of information will be needed. The pieces will usually reside in two places.
First, they will consist of direct responses from the users, such as orders and return mail. This type of
information is designed during the production of the site and should be carefully constructed to provide
measurement. Second, there will be data stored on the server’s log report of all activity on the site. Know

© 2001 by CRC Press LLC
in advance what these data are because they can provide important demographic and navigational
information that can be strategically used to provide measurement of the site’s objectives. If the site’s
hyperlinks are carefully designed, much information about the user’s thought process while navigating
the site can be gleaned from the log report’s data regarding the order in which the pages were requested
from the server. A much more efﬁcient evaluation of the site’s effectiveness can be achieved when
measurable objectives are included as part of the site design.
2.4.4
Intuitive Layout
Finally, no web browser likes to be “lost in the funhouse.” Chances are, there was enough work expended
just to ﬁnd your site via a search engine, word of mouth, or reputation. Once there, the user wants to
ﬁnd solutions with speed and comfort. If they came to browse, the information must be presented and
found quickly. Everyone knows how easy it is to get lost, frustrated, and eventually one leaves without
much thought of returning. Intuitive, concise, and neatly laid out site design will make the user feel
comfortable there. Remember, unlike traditional stores, it is much easier to “walk out” of a website. And
even though good help is hard to ﬁnd in those traditional stores, web users don’t expect that they will
even need help. The design should be uncluttered, the directions and navigation should be intuitive, and
there should always be a link that will allow the user to return to the home page. Remember that the
original goal behind the hyperlink was to allow non-linear access to information. All of your website’s
offerings should not be shown on one page. A carefully designed linking system will allow the user to
access information in an intuitive way, without having to muddle through the clutter. Overuse of graphics
and animation should be avoided. Remember to use the best available tools to meet the objective without
burdening the user or the computer resources with extraneous efforts.
2.5
Web-enabled Data Warehousing
Dermot Murray
2.5.1
Introduction
Since the early 1980s, business analysts have identiﬁed the inherent value in analyzing the huge amounts
of data generated in production online transaction processing (OLTP) systems. Hidden deep inside such
data repositories lies key information that can make a product or service more marketable, that can make
a customer more proﬁtable, and that can make processes more efﬁcient. This process of analyzing
production data in order to unearth that critical business intelligence is known as decision support
systems (DSS). The problem has always been getting at that information in such a way that it adds value
to the business as a whole. William H. Inmon promoted the concept of data warehousing3 in the 1980s
as a means of separating operational systems from DSS in order to extract the data necessary for business
intelligence without impacting mission critical processing systems. Since then, there has been a huge
growth in the market for data warehousing and decision support tools, with all of the major database
vendors such as Oracle, IBM, and Sybase developing products to satisfy the demand.
This paper examines the next logical step in the evolution of data warehousing technology; i.e., Web-
enabling the applications that provide access to this key business data. With the growth in the use of
intranets, extranets, and the Internet in general, such Web-enabled data warehousing products have
revolutionized the way business analysts generate the reports and charts they need in order to analyze
the trends and patterns in the operational data. We will explore the concepts behind Web-enabled data
warehousing and look at the technology that makes it all happen.
3The Essential Client/Server Survival Guide — Orfali, Harkey, and Edwards, (John Wiley, 1996).

© 2001 by CRC Press LLC
2.5.2
Data Warehousing Overview
2.5.2.1
Concepts
The concept behind data warehousing ﬁrst emerged in the client/server platform environment of the
1980s. Although Inmon ﬁrst started writing about data warehousing in 1981, it wasn’t until the early
1990s that industry giants such as IBM, Oracle, and Sybase started taking the technology seriously. Today,
the worldwide market for data warehousing products is estimated at over $30 Billion.4 What drives the
popularity of data warehousing is the need for executives and business analysts to gain competitive
advantage from analyzing trends and patterns hidden within the mountains of data that companies
generate in their day-to-day transactions. Data warehousing is the process of extracting data from various
OLTP systems into a centralized format that can be analyzed using DSS and executive information systems
(EIS) tools, which provide more business-speciﬁc and powerful queries for higher level managers and
executives. Collectively, these tools are known as online analytical processing (OLAP) or multidimensional
analysis (MDA). The data warehouse itself can either be a single or distributed specialized database
management system (DBMS) that contains replicated data from different sources within the organization.
This data is usually extracted from internal data production sources such as OLTP and enterprise resource
planning (ERP) systems but increasingly from external sources such as Dow Jones, Reuters, and even the
Internet itself. The data is typically cleansed and transformed to a format that can be analyzed by DSS
tools. Information about the content and format of the data is stored as “metadata” in information
directories that can be accessed by both business analysts and database administrators (DBA) alike.
Figure 2.5.1 demonstrates the various steps involved in data warehousing.
2.5.2.2
Advantages
The main advantage of a data warehouse is that it provides executives with up-to-date data that can be
queried for reporting and analysis in order to assist them in making strategic decisions about the operations
of their organizations. The SQL queries generated by OLAP tools are typically very sophisticated and can
contain multiple criteria such as sales by region, state, and customer. The results of these queries are displayed
as reports or charts that can then be presented in a concise form to executives who typically don’t want all
of the detail that is contained in the original production data. Due to their multiple-criteria nature, these
queries are usually long-lived and may even result in lost or stray processes that could jeopardize the data
integrity of a real-time production environment such as OLTP or ERP. Therefore, data warehouses are
usually separate DBMSs that store replicated and transformed copies of the production data and are not
required to provide the same fast response times that are necessary for mission-critical OLTP systems.
However, the new genre of operational data stores5 provides faster response to DSS/EIS queries by using
near real-time transactional data for the most current query and analysis. In addition, DSS/EIS tools provide
drill down capabilities that allow executives to get more detailed information on a particular trend or subject
matter by generating more detailed queries on that subject. Data warehouses also support data mining tools
that provide analysts with the ability to discover unexpected patterns in data by using fuzzy logic searches.
2.5.2.3
Data Marts
A very popular form of data warehousing is the data mart, which is typically a subset of the data warehouse
that contains data of speciﬁc interest to a particular department, such as marketing, sales, or human
resources. These data marts are less expensive to build and maintain than enterprise-wide data warehouses
and offer immediate business value to the departments that they service. However, the very independence
that data marts provide tends to act as an obstacle to true enterprise integration in information reporting
and analysis. While the debate between centralized enterprise-wide data warehousing and decentralized
data marts has consumed industry advocates for the last ten years, a compromise concept of dependent
4Database Solutions White Paper — Palo Alto Management Group, Inc., July 1998.
5Data Warehousing Management/ Productivity Tools — Datamation White Paper, 1997.

© 2001 by CRC Press LLC
data marts has potentially bridged the gap. This hybrid solution provides for the extraction of data from
the centralized data warehouse to the departmental data marts so that each department is working off
the same enterprise data, albeit formatted to their individual needs.6
2.5.2.4
Future Growth
In spite of the technical and organizational challenges that data warehouse implementations pose, the
ultimate beneﬁts that the technology offers has led to a huge growth in its adoption. For instance, 90%
of the Global 2000 companies had either implemented or planned to implement data warehouses by
June 1998.7 The future of data warehousing also seems pretty secure with market estimates expecting to
grow exponentially to over $100 billion by 2002 (Figure 2.5.2).8 One of the biggest factors in the expected
growth of data warehouse implementation is the ease of access to timely reports and queries that Web-
based data warehouse tools provide.
2.5.3
Web-enabled Data Warehousing
2.5.3.1
Beneﬁts
Perhaps the most perplexing aspects of implementing a data warehouse strategy have been the escalating
costs of installing and maintaining the GUI-based clients that are used to generate the queries. According
FIGURE 2.5.1
Data warehouse/OLAP environment.
6The Middle Ground — CIO Magazine, January 1999.
7Data Warehousing Is Worth The Investment — InternetWeek, June 1998.
8Database Solutions White Paper — Palo Alto Management Group, Inc., July 1998.
Back End
1. Data Extraction
3.  Ad-hoc Data
Reporting & Analysis
Front End
2.  Data Structuring
Operational Database
Servers
Data Warehouse
Servers
Order Entry
General Ledger
OLAP Server
Ad-hoc Reports & Queries
(Clients)
Multidimensional Data Analysis

© 2001 by CRC Press LLC
to a Meta Group, Inc. study in 1997, the average cost of implementing a data warehouse project was
$1.9 million,9 which accounted for up to 19% of the total IT budgets of those companies surveyed. One
solution to minimizing the costs associated with data warehouse implementations is Web-enabled data
warehousing, a concept that has grown in popularity since it ﬁrst appeared in the mid 1990s. These tools
have given IT departments a more cost-effective option of allowing access to the enterprise-wide data
warehouses for a larger group of users. By allowing users to generate queries and reports through their
Web browsers, IT departments can roll out data warehouse installations in a much shorter period of
time by simply allowing the users to have the appropriate level of access to the DSS application server.
With the advent of virtual private networking (VPN) and subsequent extranet technology, various levels
of data warehouse access can even be provided to external users such as suppliers and customers. This
is particularly critical in today’s supply chain environment where companies have to work closely with
their strategic partners, i.e., suppliers and customers, and therefore have to provide a certain level of
access to their enterprise data warehouses for reporting and analysis. This Web-enabled approach has
had the effect of reducing the costs associated with hardware and software by using the “thin” client
approach, as opposed to the “fat” client requirements of the previous client/server model. For instance,
the Aberdeen Group was able to cut costs from $1000 per seat for client/server DSS tools to $50 for Web-
based DSS access, primarily by consolidating most of the software and hardware costs at the server.10
This server-centric model also has the effect of reducing software licensing costs by switching from a per-
seat basis to a more cost-effective server-based licensing model. The move away from dependence on fat
desktop clients to the thin browser-based client model may also allow the new breed of Internet appliances,
such as mobile phones and network computers, to be able to access information from data warehouses
over the Internet. For instance, the advent of “push” technology could allow a sales executive to be paged
if sales ﬁgures for a certain region fall below a predeﬁned threshold.
2.5.3.2
Making it Happen
Web-enabled data warehousing involves a combination of HTML, XML, HTTP, and mobile component-
based technology, typically Java or ActiveX. In this model, a user can generate a SQL query using a
HTML/XML form that embeds the control information for the search criteria into a HTML/XML query
and sends it to the remote Web server that in turn passes the request to a Web Gateway server. The
Gateway server converts the HTML/XML query into an OLAP-speciﬁc request and passes it to the OLAP
FIGURE 2.5.2
Worldwide data warehousing solutions market (Source: Palo Alto Management Group).
9The Middle Ground — CIO Magazine, January 1999.
10Warehousing And The ‘Net — Marriage Made In Heaven — Insurance & Technology, July 1997.
$120.0
$100.0
$80.0
$60.0
$40.0
$20.0
$0.0
1996
1997
1998
1999
2000
2001
2002
$8.8
$14.5
$23.2
$36.3
$54.8
$80.1
$113.5
Market Value ($B)

© 2001 by CRC Press LLC
server, which executes the query directly against the data warehouse. Typically the Web gateway and
OLAP Server are bundled into the vendor-supplied DSS Server application package. When that applica-
tion retrieves the resulting data, it sends the data embedded in an HTML/XML ﬁle back to the Web
server which forwards it back to the Web client.11 Some DSS servers also store predeﬁned DSS objects
such as reports and queries that can be executed by the user to generate more standardized reports. This
type of interaction is similar to three-tier client/server architecture and in fact most industry analysts
would argue that Web-enabling traditional client/server applications are just the next step in the evolution
of client/server — the intergalactic model (see Figure 2.5.3).
The interaction between the DSS server application and data warehouse is provided using distributed-
object-computing protocols, such as CORBA, IIOP, or DCOM, which provide the client/server state
management unlike HTTP, which is a stateless protocol.12 In some cases, the DSS server downloads client-
based applications, such as Java applets or ActiveX controls, that actually run within the Web browser’s
memory space and interact directly with the application server. The purpose of this is to provide an
improved graphical user interface to the end-user in situations where plain old HTML is not sufﬁcient
for multi-dimensional data visualization, such as graphs and charts. These applets also provide more
robust object-based communications between client and application server by using CORBA Internet
Inter-ORB Protocol (“IIOP”) and Java remote method invocation (“RMI”), with HTTP only being used
for downloading the applets to the client. Figure 2.5.4 demonstrates the different levels of interaction
FIGURE 2.5.3
Web-based client/server architecture.
FIGURE 2.5.4
Object-based architecture (Source: Adapted from DB2 Magazine).
11A Methodology For Client/Server And Web Application Development — Roger Fournier (Yourdon Press, 1998).
12Building Web Information Systems — Byte Magazine, July 1998.
Client Web Browser
Web Server
Web Gateway
OLAP Server
Data Warehouse
HTML Query
Request
HTML Query
Results
Query
Query Results
Web Application Server
Data Warehouse Server
Web Server
Web Browser
JAVA Client
CORBA Client
Active X Client
HTML
IIOP
HTTP
HTTP API
JAVA VM
Server Application
ORB
DCOM
Interface

© 2001 by CRC Press LLC
between the client, application server, and data warehouse server using both the Web and object-based
protocols such as CORBA and DCOM.
The advent of Web-enabled data warehousing has spawned the growth of information delivery, whereby
speciﬁc business information is “pushed” to information consumers at predeﬁned intervals. In effect,
consumers subscribe to information by searching the metadata stored in the information directories
using Java/ActiveX agents or Web-based search engines in order to locate the information that is of
interest to them.13 The information delivery itself can either be schedule-driven, meaning that the
appropriate decision support objects are executed at user-deﬁned intervals and the resulting information
delivered to a Web server, or event-driven when particular business events occur. These user-deﬁned
events, also known as triggers, could also indicate that new data has arrived from the source DBMS.
“Push” technology is also being incorporated into the channel method of delivering information, whereby
a business analyst can subscribe to a channel, which in turn delivers the most up-to-date information
on a particular subject.14
2.5.3.3
Obstacles and Limitations
Although Web-enabled data warehousing has revolutionized the distribution of enterprise-wide business
information, it has not been the panacea for all data warehouse access problems. There are limitations
to what “power users” can do with Web-enabled DSS tools and in those cases, the traditional desktop
approach is still being used. Such functionality as drill-down analysis and multidimensional queries are
still not as effective with the Web-based tools and so users who need this functionality still require the
more mature client/server tools to perform these analyses. As Web-enabled tools become more sophis-
ticated, this problem will be rectiﬁed, but in the meantime, IS managers must deal with the reality that
both Web-enabled and desktop data warehouse access must coexist for the foreseeable future.
Ironically, the ActiveX and Java components that are designed to make user interaction with the data
warehouse environment possible can cause incompatibility problems if the end-user’s Web browser
cannot support them. This might be the case if the browser is an older version or if the particular
downloadable component only works with either Netscape or Internet Explorer. Of course, the solution
to this problem is to standardize Web browsers and plug-in components, a daunting task for organizations
with potentially thousands of users, but nevertheless manageable.
Security has always been a problem with enabling external users to access corporate information. As
mentioned earlier, the growth in supply-chain management has dictated that companies work in tandem
with their suppliers and customers in order to develop their products and services. This trend has forced
companies to open up their corporate information systems, including data warehouses, to these external
users. Of course, this has led to fears that such mission-critical and conﬁdential information could be
compromised in transit between users across a public network such as the Internet. Another potential
security hole is the remote access required by mobile users, such as sales agents, who need to pull up
reports using dial-up connections over the Internet. Improvements in corporate ﬁrewall and VPN tech-
nology, coupled with better encryption algorithms and Public Key Infrastructure, have eased these fears.
However, organizations implementing Web-enabled data warehousing must constantly be vigilant not
only with potential hackers from the outside but also with internal users who should only have the level
of access to reports and data that they require. One potential solution to this problem that has been
promoted since Web-enabled data warehousing began is to distribute the data among data marts, thereby
limiting the users’ access to only the data stored in those data marts.15
Like every new technology, Web-enabled data warehousing has its downside as illustrated above.
However, with vendors addressing these issues all the time, the usage of this technology has had a
phenomenal growth rate and while traditional client/server warehouse access is not going away any time
soon, Web-enabled DSS tools are making strong inroads into the marketplace.
13Building Web Information Systems — Byte Magazine, July 1998.
14Warehouses Webicum: Evolution Of A Species — DB2 Magazine, April 1998.
15Just Browsing Thanks — CIO Magazine, October 1996.

© 2001 by CRC Press LLC
2.5.4
Vendors
This burgeoning sector of the industry has produced a number of leading products from both established
and startup vendors. While most of the major database vendors have been very proactive in providing
Web-based access to their data warehouse products, the market for packaged applications has come
mostly from startup vendors like MicroStrategy. As there are too many vendors to mention in this market
segment, I will discuss a sample of the products being offered by these different categories of vendors.
2.5.4.1
Major Database Vendors
Oracle Corp., arguably the biggest name in the relational database market, has its presence thanks to the
Oracle Express suite, which includes the Express Server and Express Web Agent modules. This product
was actually acquired from Information Resources in 1995 and accounted for 21% of the OLAP market
in 1998.16 The Oracle Express Server is the actual back-end OLAP engine that performs the end-user
queries. This is the component that provides the interaction between the Express OLAP Server and the
Web server. This module takes advantage of the “Network Computing Architecture” model, which is
Oracle’s blueprint for a three-tier thin client environment, and the Express Stored Procedure Language
cartridge to provide the communication between the Web server and the Express Server. The Express
Agent Developer’s Toolkit, which comes with the Agent, supports the development of both HTML pages
and components such as Java and ActiveX to produce customizable reports and analyses for the end user.
For their part, IBM provides Web access to the DB2 OLAP Server through the use of its Net.Data
development platform. This product supports standard SQL statements as well as C++ and Java enablers
that allow developers to write macros that automate SQL queries. On the server side, it supports FastCGI,
which is a high-performance Web server interface that provides better performance for Net.Data appli-
cations. In addition, IBM integrated its OLAP Server with Hyperion’s Essbase Web Gateway, which is a
Web application server component similar to Oracle’s Web Agent.
As for the other major database vendors, it appears that they have been slow to fully integrate their
OLAP products with Web access, including Microsoft. For their part Informix has tried to buy its way
into the Web access market by acquiring Red Brick Systems, who in turn had teamed up with Web
application specialists Caribou Lake17 to deliver Web-based OLAP products. Therefore, this is still a
market segment where the niche DSS vendors have been able to take the lead, at least for now.
2.5.4.2
Startup DSS Vendors
MicroStrategy Inc., the market leader in Web-based OLAP products, develops a suite of DSS applications
that interacts with DBMS platforms such as Oracle, Informix, and DB2 among others. This suite includes
the DSS Web Server and DSS Broadcaster products that use the Internet and the World Wide Web as the
communications medium. DSS Web Server, at version 5.5 as of this writing, is a Web-based interface
that works with the company’s DSS Server OLAP engine to allow users to generate reports and analyses
over the Web. Because the company uses ActiveX and Java components in DSS Web Server 5.5, they have
been able to provide features such as drill down analysis and report pivoting, that were only available in
their Windows-based DSS Agent product. Its DSS Broadcaster product is an application that allows for
customized information delivery to be pushed to Web-based clients such as browsers and Internet
appliances. The product won the “IT Manager’s Choice” Product of the Year in the data warehouses and
data marts category organized by Datamation magazine.18 In addition, the company announced 1999
ﬁrst quarter revenue of $35 million, an increase of 80% over the corresponding quarter in 1998, gaining
55 new clients, including First USA Bank, France Telecom, and Kmart.19
InfoSpace, Inc. is the developer of SpaceOLAP, a fully Java-compliant solution that provides client/server-
like interfaces for reporting and analysis. It includes a Java Application Server that resides on the Web server
16End-User Query and Reporting Tools — ComputerWire PLC., March 1998.
17PR Newswire Association, Inc, December 1998.
18Datamation — February 1999.
19MicroStrategy’s Revenue, Proﬁt Soars — InformationWeek, April 1999.

© 2001 by CRC Press LLC
and supports data extracts from Oracle Express, DB2 OLAP, and Hyperion Essbase OLAP servers. Admin-
istration is provided through the SpaceOLAP Administrator module, while the SpaceOLAP Designer lets
users and developers create customized reports and graphs based on the results of their queries. Reports,
or “Presentations” as they are referred to by the company, can be displayed in HTML or Java applet
supporting format, supporting pivot and drill down capabilities as well as multidimensional analyses.20
Information Builders services the Web-enabled data warehousing market through its WebFOCUS
product line. WebFOCUS provides reporting and publishing via the Web from both legacy databases and
data in ERP applications. The company takes advantage of Java applets that provides for customized
reporting and a Java Developers Workbench for designing customized reports from a Web browser.
Among the other startup companies that have made a name for themselves in the Web-enabled data
warehousing market are Cognos Corporation, the makers of Impromptu and PowerPlay, and Information
Advantage, Inc., developers of the DecisionSuite OLAP product line.
2.5.4.3
Established DSS Vendors
The more established data warehousing companies such as Prism Solutions and Brio Technology have
also developed Web-based interfaces into their OLAP products. The Brio Enterprise Server suite provides
two modules that allow the user to extract information from data warehouses via the Web — the
OnDemand Server and the Broadcast Server, competitor products to MicroStrategy’s DSS Web Server
and DSS Broadcaster, respectively. Prism Solutions added Web-enabled functionality to its Warehouse
Directory product in 1997 when it introduced the Web Access module, which allows users to view and
query the Directory from their Web browsers. Having been acquired by Ardent Software, Inc. in April
1999, this module has since been integrated as a standard feature of the Warehouse Directory package
and indeed, Ardent’s Vice President Peter Fiore sees Extensible Markup Language (XML) as the future
conduit for sharing access to corporate data warehouses.21
The following table summarizes the list of vendors and products mentioned above.
2.5.5
Future Trends
The trend to add more functionality to Web-based DSS tools will continue to grow as more companies
realize the beneﬁts of implementing Web-based access to their corporate data warehouses. However, don’t
expect to see the demise of pure client/server-based DSS tools anytime soon as analysts predict that a
coexistence of both forms of access will prevail for a time.22 As mentioned earlier, the main reason is that
there are still features and functionality available with mature client/server-based tools and not with Web
access tools. DSS vendors are working hard to add that functionality to their Web-based offerings so that
in the future, even power users will be able to get the reports and analyses they need via the Web. Overall,
20SpaceOLAP — DBMS, August 1997.
21Ardent’s Peter Fiore Is Passionate About Data Warehousing — InfoWorld, May 1999.
Vendor
Product(s)
Technologies 
Supported
Oracle Corp.
Express Server (Web Agent)
HTML, Java, ActiveX
IBM
Net.Data, DB2 OLAP (Hyperion Essbase)
SQL, C++, Java, CGI
MicroStrategy
DSS Server OLAP, DSS Web Server, DSS Broadcaster
HTML, Java, ActiveX
InfoSpace
SpaceOLAP
Java, HTML
Information Builders
WebFOCUS
Java
Brio Technology
Enterprise Server (OnDemand Server, Broadcast Server)
CGI, Java Runtime 
Environment (JRE)
Prism Solutions
Warehouse Directory (Web Access module)
Java, XML
22Self Storage; Lower Data Warehouse-Management Costs With Web Access — Communications News, August 1998.

© 2001 by CRC Press LLC
the emphasis will shift away from desktop access and more toward the centralized “intergalactic” Web-
based model as data warehouse implementations become larger and more users require access.
2.5.5.1
Web Farming
One area of interaction between data warehouses and the Web that is set to grow steadily in the next few
years is the concept of Web Farming. This is deﬁned as “systematic business intelligence by farming the
information resources of the Web, so as to enhance the contents of a data warehousing system.”23
Essentially, it is the process of using the Web not as a means of distributing data warehouse information
to the end user but as a means of obtaining the raw data that goes into the warehouse itself. Such external
data sources include commercial databases, e.g., the IBM database of patents, and public databases such
as The Electronic Data Gathering, Analysis, and Retrieval (EDGAR) operated by the U.S. Security and
Exchange Commission to track publicly traded companies.24
Similar to extracting production data from OLTP systems into data warehouses, Web data must be
reﬁned to be suitable for use by the warehouse applications. This process, known as acquisition can vary
depending on the sources of the Web content and is crucial in ensuring the usability and reliability of
the data content as it becomes part of the decision support structure. While this sector is still in its
infancy, it won’t be long before the major vendors turn their expertise to developing products that will
make this form of data collection more reliable and efﬁcient.
2.5.6
Conclusion
The value of using the World Wide Web to disseminate decision support information is evident from the
growth in the use of Web-based DSS and OLAP tools. By providing access to corporate data warehouses
over the Web, companies are empowering a larger user base with the necessary tools to analyze the business
data. This in turn has opened up the decision-making process to more people within the organization.
The lower costs of rolling out and maintaining Web-based data warehouse access has made this medium
very attractive to companies that wish to gain the most value from their data warehouses. In addition, the
trend to open up information systems to the customers and suppliers who are part of the supply chain
has made easier access to data warehouse information more critical to these external users.
While this technology has grown in leaps and bounds and looks set to take over as the predominant
form of data warehouse access, IS departments will still have to grapple with coexistence between Web-
based and client/server-based access. But with the improving feature set of the various products being
offered by the many vendors in this arena, Web-enabled data warehouse access will become the de facto
access medium for this powerful business information.
References
Carrickhoff, Rich — SpaceOLAP (DBMS, August 1997).
ComputerWire PLC — End-User Query and Reporting Tools (March 1998).
Datamation White Paper — Data Warehousing Management/Productivity Tools.
(Datamation, 1997).
Davis, Beth — MicroStrategy’s Revenue, Proﬁt Soars (InformationWeek, April 1999).
Fournier, Roger — A Methodology for Client/Server and Web Application Development (Yourdon Press, 1998).
Hackathorn, Richard — Web Farming For The Data Warehouse (The Morgan Kaufmann Series in Data
Management, 1998).
Hackathorn, Richard — Rouging the Web for Your Data Warehouse (DBMS, August 1998).
Koch, Christopher — The Middle Ground (CIO Magazine, January 1999).
Orfali, Robert. Harkey, Dan. Edwards, Jeri — The Essential Client/Server Survival Guide (John Wiley &
Sons, Inc., 1996).
23Web Farming For The Data Warehouse — Richard D. Hackathorn (Morgan Kaufmann Publishers, 1998).
24Routing The Web For Your Data Warehouse — DBMS, August 1998.

© 2001 by CRC Press LLC
Palo Alto Management Group, Inc., — Database Solutions White Paper (July 1998).
PR Newswire Association, Inc. — Red Brick and Caribou Lake Software Team Up (December 1998).
Reinauer, Rob — Self Storage; Lower Data Warehouse-Management Costs with Web Access (Communi-
cations News, August 1998).
Row, Heath — Just Browsing Thanks (CIO Magazine, October 1996).
Schroeck, Mike — Data Warehousing is Worth the Investment (InternetWeek, June 1998).
Schwartz, Susana — Warehousing And The ‘Net — Marriage Made In Heaven (Insurance & Technology,
July 1997).
Scott, Jim — Warehousing Over the Web (Association for Computing Machinery, Communications of
the ACM, September 1998).
Vizard, Michael — Ardent’s Peter Fiore Is Passionate About Data Warehousing (InfoWorld, May 1999).
White, Colin — Building Web Information Systems (Byte Magazine, July 1998).
White, Colin — Warehouses Webicum: Evolution of a Species (DB2 Magazine, April 1998).
2.6
E-commerce Technologies: A Strategic Overview
Mihir Parikh
The changes sweeping through electronic communications will transform the world’s economies,
politics, and societies — but they will ﬁrst transform companies.
—Frances Cairncross, in The Death of Distance, 1997, p. 119.
E-commerce is a result of these changes. In simple terms, e-commerce is deﬁned as a business conducted
over the Internet with the use of computer and communications technologies. There are three major
types of e-commerce: business to business (B2B) e-commerce, business to consumer (B2C) e-commerce,
and electronic markets. B2B e-commerce deals with one business providing products and services to
another business typically as a part of the supply chain or as an enabler of business processes. Examples
of B2B e-commerce include an auto-part manufacturer supplying an auto company, or a bank providing
credit card payments and other ﬁnancial services to a retailer. For many years, electronic data interchange
(EDI) handled B2B transactions in many companies. Now, Web-based open system applications are
replacing proprietary EDI systems. B2C e-commerce deals with a business providing products and services
to consumers at the end of the supply chain. Examples include a book retailer selling books to a reader
(Amazon.com) and a broker providing ﬁnancial trade executions to an individual investor (E*Trade).
Electronic markets provide marketspaces on the Internet, as opposed to marketplaces in the physical
world, where buyers and sellers can meet and exchange products and services. Electronic markets are of
two types: consumer markets and business hubs. Ebay, priceline.com, and accompany.com are typical
examples of electronic markets for consumers. Chemdex, Metalsite.com, and Ultraprise.com are typical
examples of electronic markets for businesses.
Recently, International Data Corporation (IDC) estimated that yearly worldwide e-commerce would
increase to more than $1 trillion by year 2003.25 IDC estimated that by then non-U.S. countries would
account for half of worldwide e-commerce. Are the current businesses ready for it? Well, not really. A
recent survey conducted by the Cutter Consortium found that 65% of companies did not have an overall
e-commerce strategy and nearly 25% lacked even a basic business and implementation plan for e-com-
merce.26 A major reason for this is that a few companies have aligned their business strategies with
information technology (IT) strategies. A large percentage of companies still do not involve IT in high-
level strategic planning. IT in these companies is relegated as a support function. The companies have
failed to recognize the changing role of IT from business support to business enabler. The strategic
implications of IT and the use of IT as a strategic weapon are not well understood.
25International Data Corporation Report, NET062899PR.htm, June 28, 1999.
26InternetWeek, September 6, 1999. Page 29.

© 2001 by CRC Press LLC
Information technologies are key enablers of e-commerce. A combination of many information tech-
nologies ranging from a Web browser to logistics management systems makes e-commerce possible.
Identifying and implementing the right technologies to execute business processes is critical to succeed
in the e-commerce space.
2.6.1
E-commerce Technologies
Figure 2.6.1 shows different categories of e-commerce technologies. All of these technologies are required
and play signiﬁcant roles in managing and conducting business over the Internet.
2.6.1.1
Client Systems
Client systems reside on customers’ computers. Customers utilize these technologies to participate in
e-commerce activities.
Web browser and cookies: The most common of all client systems is a Web browser. Web browsers
provide an interface through which a user can view information on the Internet. In the last ﬁve years,
Web browsers have evolved from a small software using simple hypertext markup language (HTML) to
a very sophisticated software that uses new technologies such as Java, ActiveX, VRML, XML, and different
types of plug-ins and Web applications in addition to a much advanced version of HTML. Most web
browsers utilize cookies whereby they transmit basic information about a user to the Web server on the
other end for identiﬁcation purposes. Such identiﬁcation can be used to personalize services for the user.
Multiple levels of cookies are used. Low-level cookies simply provide user name and password, while
high-level cookies may include information about credit card, mailing address, previous purchase pat-
terns, and browsing habits.
Communications software and hardware: Communications software and hardware help customers’
computers to connect to the Internet via one of the multiple modes including modem, cable modem,
satellite links, and local area networks.
FIGURE 2.6.1
E-commerce technologies
E-Commerce Technologies
Client Systems
The Internet
Front-end
System
Back-end
Systems
Supply Chain
System
Internal
to the
e-commerce
enterprise
Customer

© 2001 by CRC Press LLC
Plug-ins: Plug-ins are independent software applications utilized to show special data ﬁles within a
Web browser. Plug-ins enable a Web browser to show a multimedia presentation or to play a streamed
audio piece. Shockwave Flash, RealPlayer, and Adobe Acrobat Reader are plug-ins.
Software agents: Agents are other types of independent software that can assist users in carrying out
some speciﬁc activities. Such activities include ﬁltering information, searching the Web to ﬁnd the right
information, and comparison shopping.
Biometric identiﬁcation: Biometric identiﬁcation is a technology that uses a measurable physical
characteristic to recognize the identity, or verify the claimed identity, of an enrollee.27 It utilizes physical
characteristics such as ﬁngerprints, facial design, iris patterns, retina patterns, hand geometry, signature
veriﬁcation, and voice recognition instead of keys, passwords, and plastic cards. Several advantages,
including reduction in fraud and never losing the identiﬁcation, have prompted several large companies
such as IBM and Motorola to invest heavily in biometric identiﬁcation. Biometric identiﬁcation is one
of the fastest-growing areas on client systems and security.
Push technologies: These publish and subscribe technologies enable delivery of possibly useful infor-
mation without the recipient asking for it. Push technologies utilize extensive user proﬁles containing
preferences of each user to match and deliver information over the Internet. Push evolved as an alternative
to the Web (pull technology) from PointCast’s personalized broadcasting technology in 1996. In the last
three years, it has gone through the full length of the hype curve. Recently, it has returned with reasonable
expectations and clear understanding of what the technologies can and cannot do. Several companies,
including Marimba and BackWeb, provide push technology solutions.
2.6.1.2
The Internet
Many advanced telecommunications technologies have been utilized to create and operate the Internet.
Most prominent are optical ﬁber, routers, digital switches, Synchronous Optical Network (SONET) rings,
asynchronous transfer mode (ATM), frame relay, Transmission Control Protocol (TCP), and Internet
Protocol (IP). These hardware and software technologies provide the backbone lines and the transmission
rules to carry out information exchange over the Internet. As the Internet is growing, new technologies
are emerging to increase speed and volume of data transmission. The continuing convergence of textual
data with audio and video data over the Internet is prompting new technologies to support quality of
service (QoS) which recognizes the differences between data types and assigns appropriate priority for
transmission.
2.6.1.3
Front-end Systems
Front-end systems are the ones with which the customers interact. They provide a face to an e-commerce
business.
Web pages: Web pages are the data ﬁles containing HTML-coded information. In the early days of
e-commerce, Web pages were static and generated by human Web programmers. Now, in most e-com-
merce sites, Web pages are dynamic and are generated by Web page management systems. These systems
work with other front-end systems and back-end systems to develop HTML-coded content that the
customers receive.
Trafﬁc management: Due to a special sales event or some top news, sometimes an unexpected number
of visitors go to a Web site all at once. Such an unexpected load puts pressure on the Web server and
dramatically reduces its speed and increases download time. If the load extends for a longer period of
time, it crashes the system. To avoid such a mishap, trafﬁc management tools are used. These reduce
massive congestion by spreading trafﬁc load on multiple servers and increase overall network efﬁciency.
Search engines: Several types of search engines are utilized by e-commerce companies. Some search
engines provide capabilities to search a speciﬁc product based on description, features, or other infor-
mation. Some search engines provide capabilities to search and locate Web content based on key words
or phrases. Search engines can be used to search a speciﬁc Web site, a regional part of the Web, or the
27Digital Imaging: Connecticut Biometric Imaging Project. http://www.dss.state.ct.us/digital.htm

© 2001 by CRC Press LLC
whole Web. Often directory engines are used along with search engines to automatically categorize Web
content for future searches.
Site servers: Site servers are the most comprehensive tools for e-commerce. They provide support
ranging from creating an electronic storefront to controlling and managing it. Site servers include support
for site building, standard code sharing, code library development and management, dynamic Web page
generation, product promotion, product cataloging, order taking and processing, securing transactions,
and managing payment systems. Site servers help provide product information, dynamic pricing infor-
mation, marketing and promotion, shopping cart services, tax calculations, shipping and handling calcu-
lations, and automated post-sales follow-up. In addition, they capture market demographic information
and coordinate with back-end systems. Several off-the-shelf products are available to support small busi-
ness shopping services including IBM’s Net.Commerce and Microsoft’s Site Server. However, a major
e-commerce site requires custom application development to support the above-discussed activities.
Shopping engines: These types of software enable customers to ﬁnd and compare different products
or services on features and prices. Some shopping engines also include product reviews from independent
agencies such as Consumers Digest. Some shopping engines also provide reviews from and discussion
with current users of the products to help others make more informed purchase decisions. It is a very
useful tool in e-retailing. Amazon.com’s Junglee division and Inktomi are the two leading companies
that provide shopping engines.
Customer relationship management (CRM): Through a marketing agreement, E-commerce mer-
chants pay between $0.90 to $2.67 per visitor referred by a portal.28 As the search cost for every new
customer is staggering, increasing customer satisfaction and loyalty is crucial to maintain the current
customer base and for e-commerce success. CRM systems provide the critical customer support and
services. They provide a database of frequently asked questions (FAQ), searchable knowledge base,
multiple way (e-mail, Internet telephony, video conferencing, etc.) to assist shoppers in real time, follow-up
support, order tracking, return processing, after-sales services, and warranty processing. They also help
maintain proﬁles of buyers that contain their shopping behaviors and preferences. The currently available
CRM-related technologies include enterprise portal, mobile computing, net telephony, desktop video
conferencing, speech recognition, call center systems, and data warehousing.
Personalization: As the number of e-commerce businesses increases, a key differentiator would be
how well an e-commerce business customizes its storefront for each customer. Knowing the customer
and his or her preferences will improve customer service and retention. This personalization or mass
customization requires creating user proﬁles from purchase patterns and browsing patterns, and applying
business rules and inference on the information collected in the user proﬁles to create new knowledge
about the users. Key technologies used for personalization are databases, dynamic Web pages, business
rules, inference engines, cookies, and push technologies.29 In some cases, data warehousing and mining
technologies can also be used for personalization.
Security: More than half of B2C e-commerce transactions are paid with credit cards. Protecting
transmission of credit card and other private, conﬁdential information over the Internet and securing
the information on merchants’ Web servers are two of the most pressing issues in e-commerce. Encryption
of data through secure sockets layer (SSL) and private and public keys are the most commonly used
technologies to secure Internet transmission of conﬁdential data.
2.6.1.4
Back-end Systems
While the front-end systems manage the interface with customers, the back-end systems carry out
operations and manage e-commerce organizations.
Enterprise Resource Planning (ERP): ERP systems are commercial software packages utilized to
integrate information ﬂowing through different functions of an organization, such as ﬁnancial and
accounting, human resources, sales and customer service, supply chain, etc. ERP systems coordinated
28International Data Corporation Report, May 4, 1999. NET050499PR.htm
29The Web gets personal. Byte, special section on E-business Technology, June 1998.

© 2001 by CRC Press LLC
with front-end systems can capture orders, provide order conﬁrmation, accept payment, check credit
cards for approval, process coupons and other promotions, handle billing and invoicing, control inventory
and procurement, integrate with payment systems, and coordinate with fulﬁllment systems for order
execution. Before ERP systems, these processes were handled by various, independent information sys-
tems indigenous to different functional divisions in the organization. Information systems from one
functional division were often not compatible with the information systems in other functional divisions.
This brought inefﬁciencies in business processes and overall higher costs. ERP systems promise seamless
integration and easy information ﬂow among different functional divisions. However, successful imple-
mentation of an ERP system has been one of the major critical issues in the utilizing ERP systems.
Databases/Data Warehouses/Data Mining: Databases and data warehouses are at the center of run-
ning a business in this information economy, especially an e-commerce business. They provide reposi-
tories for information collected through business processes. This information is the lifeblood of
organizations and its optimum use is very important. Several emerging database technologies such as
multidimensional databases provide holistic perspective and better understanding of the information.
When used in conjunction with data mining technologies, e-commerce businesses can ﬁnd and exploit
hidden relationships and buying behaviors to increase market share and sales.
2.6.1.5
Supply Chain Systems
These systems work with business alliance partners and other enablers. They provide smooth transfer of
information with partners to carry out outsourced business processes. Some of these systems are external
to e-commerce organizations and implemented by the partners.
Supply chain management: Most emerging e-commerce companies are not vertically integrated. They
depend on many partners and intermediaries on both sides of the supply chain (a value addition sequence
through which raw material ﬂows to become a ﬁnished product). Supply chain management systems help
businesses coordinate their processes with suppliers, manufacturers, raw material providers, shippers,
distributors, and associated retailers. In e-commerce businesses, greater efﬁciencies are achieved by moving
information rather than actual products along the supply chain. Actual products are generally delivered
directly to the consumer by the product manufacturers without any supply chain intermediaries handling
or storing the products. Supply chain management systems help control product life cycle, forecast demand,
arrange advanced scheduling, plan manufacturing and distribution, and enable order promising and
processing. Several companies provide supply chain software. The leaders are i2 Technologies, Manugistics,
and Numetrix. Several ERP companies are also moving into this area by extending ERP capabilities.
Payment systems: A majority of payments, almost 89%, over the Internet is conducted through credit
cards and checks. Payment systems help e-commerce businesses coordinate with banks and credit card
companies to approve credit card purchases and clear checks. Some e-commerce companies also utilize
these systems to work with soft cash providers such as CyberCash and electronic money.
Fulﬁllment/Logistics management: These systems coordinate with logistics partners such as FedEx,
UPS, and independent warehouse operators. These systems work with back-end and front-end systems
to help determine shipping and handling charges, delivery terms, delivery schedule, order tracking, freight
management, custom and excise duty clearance, and other fulﬁllment issues.
2.6.2
Strategic Challenges
In the ﬁrst chapter of the e-commerce storybook, the technology largely drove business models. Now
the business models are driving technology.
—Peter G. Keen, Computerworld, September 13, 1999
2.6.2.1
High Cost of Small Errors
In e-commerce, there is little room for errors. In most cases, you do not get a second chance. Integrating
right strategies with right technologies and continuously improving competitive position are important for
survival and success. Any error in technology or strategy implementation can lead to a serious loss in market
position and the ability to carry out the business in future. Once, eBay’s stock price dipped more than 50%

© 2001 by CRC Press LLC
largely due to recurring, unexpected shutdowns of its Web site. Most of these shutdowns were not more
than a few hours long. However, persistence and a strong commitment by the company’s top management
to improve technological infrastructure lead to a rebound in the stock price. E-commerce businesses pay a
very high cost for small errors. As an e-commerce business expands, the probability of making such errors
increases. While most managers and leaders assume that being there ﬁrst is the key to success, a study has
found that many pioneers fail and most current leaders are not pioneers.30 The study found that ﬁve factors
(vision, persistence, commitment, innovation, and asset leverage) are critical to success. These factors often
lead to making fewer errors and quickly correcting the errors when they are made.
2.6.2.2
Building Relationships
Technology is a double-edged knife. While it provides unprecedented advantages to you, it also provides
the same beneﬁts to your competitors and future competitors. It enables your existing competitors to
quickly react to your moves. It also reduces the barriers to entry for new competitors. Assuming that if
you build it, they will come and stay is one of the fallacies of e-commerce. To pre-empt this, you have
to build a strong customer base and retain it. Building a strong customer base requires building relation-
ships and providing useful services and content to your customers. Often it is measured in terms of
“stickiness” of the Web site, that is, how long a visitor stays on the Web site. Media Metrix reports eBay
(125.5 average minutes per user), E*Trade (66.5), Microsoft sites (66.0), and Yahoo sites (64.6) are the
top four stickiest Web sites.31 It is not a coincidence that the companies with the stickiest sites are the
most successful in e-commerce.
2.6.2.3
Speed
The speed of doing business has increased tremendously with the Internet. Speed is required in growth,
in decision making, in adapting to the changing conditions, and in supporting and servicing customers.
An e-commerce company has to continuously innovate and improve its business processes and Web-
based storefront. It is always vulnerable to the quick imitation of its processes and its innovative shopping
features by its competitors. Often not only the Web site designs but also business models are copied
overnight by competitors. This also requires building ﬂexibility in the front-end and back-end systems
to adapt to the continuously changing conditions and stay ahead of competitors. Therefore, industry
experts recommend an open and adaptive architecture for enterprise information systems.32
Operationally, when a customer visits an electronic storefront, performance of the Web site becomes
an important issue. First of all, the customer wants quick downloading of the Web pages and immediate
response to any search queries. If the Web site is slow, the customer will very likely move on to another
competing store. This invariably happens when an unexpected number of customers come to the Web
site at the same time. Several technologies can help improve speed of the Web site. Use of trafﬁc
management tools can balance the load on a Web server and increase the speed of interaction. Scalability
of the hardware and software provide quick integration of additional resources. The use of better search
engines and shopping engines can also increase the speed of searching the requested product from millions
of product proﬁles stored in the databases. Good shopping engines can also help identify related, com-
plementary products for cross selling and up selling.
2.6.2.4
Security
With the growth of e-commerce, more and more business processes and databases are put on the Internet.
This is required to improve customer service and increase organizational efﬁciency. However, this also
makes the processes and databases vulnerable to malicious forces including business spies, computer
hackers, and disgruntled former employees. Having complete control over who gets to see what and who
30First to Market, First to Fail? Real Causes of Enduring Market Leadership. By Gerard Tellis and Peter Golder.
Sloan Management Review, 32(2), 1996, 65-75.
31Snapshot: Sticky Sites. Computerworld, September 13, 1999. Page 42.
32Designing a growing back end. InfoWorld, August 23, 1999. Pages 34-35.

© 2001 by CRC Press LLC
gets to change what is extremely important. Security management software, virus protection software,
and intrusion detection software can help increase security of the Web site.
2.6.2.5
Technology Evolution
E-commerce technologies are in a constant state of ﬂux. E-commerce businesses have to continuously
evaluate emerging technologies and adapt them quickly to stay competitive. As the technologies are
constantly evolving, few standards exist. The ability to choose a right technology with a strong future
becomes a critical skill in managing e-commerce.
2.6.3
Emerging Trends for the Future
What is my ROI (Return on Investment) on e-commerce? Are you crazy? This is Columbus in the
New World. What was his ROI?
—Andy Grove, Intel Chairman
New technologies are emerging every day. Some of these technologies may become a killer app for e-com-
merce. While it is almost impossible to predict them, using standard measures (such as ROI) to evaluate
may fail, too. However, several underlying trends may help to identify and evaluate the right technologies.
The drop in the cost of computing continues. Storage, processing, and distribution cost of information
is decreasing to a level where the cost is less than the value of information. This has lead to the development
of new enterprises that provide free products and services in return for information and loyalty. On the
hardware side, we have FreePC, PeoplePC, eMachine+Compuserve alliance, etc. On the applications side,
we have HotMail, when.com, Yahoo, etc. On the Internet access side, we have NetZero, Freeserve, etc.
On the Web site hosting side, we have GeoCity, Tripod, etc. More and more of these types of e-commerce
businesses will continue to rise, making it difﬁcult for the current businesses to compete.
While the power of computer processors is going up, their sizes are decreasing. In addition, computer
processors are now used in many devices and products (such as autos, refrigerators, washing machines,
dishwashers, etc.). As computing technologies continue to expand to household products and appliances,
communications technologies will soon follow. These products and appliances, when connected to the
Internet, will create new e-commerce opportunities. For example, a refrigerator in the future may be able
to automatically buy groceries for you. The jar of milk is put in the refrigerator in one location where
there is a sensor that notices how much milk is left. As soon as milk reaches the reorder level (determined
based on your consumption pattern), the refrigerator will automatically connect with a grocery store on
the Web (maybe NetGrocer, PeaPod, or WebVan) and place an order for milk.
New software applications are emerging every day. Each one makes e-commerce business processes
more efﬁcient and effective. This will enable more and more people to go online for their shopping,
entertainment, business and home management needs. New e-commerce models will rise to support
these changes in customer behavior.
2.7
Internet Protocols
John Braun
The purpose of this section is to describe concepts that are essential to understanding how Internet
protocols work. Basic addressing at the device or client level will be described, followed by protocols that
are used to exchange information among these devices. Although the term device may seem vague at
ﬁrst, the types of devices that communicate via the Internet have matured from simple text-based
information, to the use of audio, video, animation, and other forms of communication. Once the basics
of communication are laid out, some of the speciﬁc protocols and applications that utilize these basics
will be discussed. Security aspects of these protocols and applications will be covered. Useful search tools
that can help locate information on the Internet will be covered, and a discussion of some of the major
industry players will conclude this section.

© 2001 by CRC Press LLC
2.7.1
Addressing for Internet
All devices on a network that supports Internet Protocol (IP) have a unique numeric address, 32 bits in length.
The most common way of representing a device’s IP address is by using a “dotted quad” — four
decimal numbers ranging from 0 to 255, separated by periods. This results in a theoretical range of 0.0.0.0
to 255.255.255.255.
There are currently ﬁve classes of IP addresses:
A Class D address has the following format:
1110 MMMMMMMMMMMMMMMMMMMMMMMMMMMM(28)
A class E address has the following format:
1111 XXXXXXXXXXXXXXXXXXXXXXXXXXXX(28)
N = Network portion of address
L = Local portion of address
M = Multicast address
X = Undeﬁned
This results in the following valid ranges:
Class D and Class E addresses, which have the ﬁrst (high order) bits of the address set to 111, are
classiﬁed as part of an undeﬁned “extended addressing mode.”
There are a few addresses that have special purposes and should not be assigned to a device.
If the network ID portion of an address is set to all zeros, it means “this network.” For example, address
0.0.0.42 would mean host 42 on this network.
If the host ID portion of an address is set to all ones, it means “all hosts.” For example, the address
199.27.24.255 would mean all hosts on the 199.27.24 network.
The class A address with a network ID of 127 is deﬁned as a loopback address, where any packet sent
by the host will be returned to the host without being sent across the network. This can be used for
Class
Begin
End
A
0.0.0.0
127.255.255.255
B
128.0.0.0
191.255.255.255
C
192.0.0.0
223.255.255.255
D
224.0.0.0
239.255.255.255
E
240.0.0.0
255.255.255.255
Class A
Class B
Class C
Bit  31
23
15
Network
ID
Host
ID
Network
ID
Host
ID
Network
ID
Host
ID
0
1 0
1 1 0
7
0

© 2001 by CRC Press LLC
testing purposes, or as a sanity check to determine if one’s TCP/IP implementation is working properly.
The typical value used for this purpose is 127.0.0.1.
References
Postel, J., “Internet Protocol,” RFC 791, USC/Information Sciences Institute, September 1981.
Reynolds, J. and J. Postel, “Assigned Numbers,” RFC 943, USC/Information Sciences Institute, April 1985.
*** Latest is RFC 990
2.7.1.1
DNS
The domain name system (DNS) is a global network of computers that can translate a numerical IP
address to a human-readable name, and also translate a name to the corresponding IP address. This
makes navigation of a TCP/IP network much easier. For example, www.crcpress.com is much easier to
remember than 199.29.24.3, the IP address which corresponds to this address.
Before DNS, each computer on a network would have to maintain a large ﬁle (typically called hosts)
with all known IP address and name pairs. This is obviously impossible to do now with the sheer number
of hosts on the Internet, but can still be useful for small, private TCP/IP networks which are not directly
connected to the Internet.
When conﬁguring a client to access DNS services, multiple DNS servers should be speciﬁed, if available.
Modern TCP/IP implementations are smart enough to try another DNS server if the initial one is
unavailable.
A DNS client will submit a DNS request to a server, and receive one of three types of reply. The client
will be told that the lookup was successful and be given the name, that the server couldn’t perform the
lookup but knows another server that may, or that the lookup failed.
References
RFC 1035 — Domain names — implementation and speciﬁcation. P.V. Mockapetris. Nov-01-1987
2.7.2
Communication Protocols in Internet
There are many protocols used on the Internet. Most are classiﬁed at either the lower layers (Layer 3,
Network and Layer 4, Transport) of the OSI model, or at the application level (Layer 9).
IP 
RFC 791
The IP in TCP/IP refers to the Internet Protocol used at the network layer. The basic purpose of this
protocol is to try to deliver packets. It does not offer such services as acknowledgment, retransmission,
error correction, ﬂow control or guarantee of order of delivery. This is the job of higher-level protocols.
The advantage of IP is that it offers a common framework for devices to communicate.
An IP header looks like this:
Version
IHL
Type of Service
Identification
Flags
Total Lengh
Fragment Offset
Header Checksums
Protocol
Time to Live
Source Address
Destination Address
TCP Header, Then your data *******************************

© 2001 by CRC Press LLC
UDP 
RFC 768
User Datagram Protocol (UDP) is a connectionless protocol that provides a means to send data with
a low protocol overhead. Only the source port, destination port, length, and checksum are added to the
raw data. However, it does not guarantee delivery or protection from duplicates. For applications where
performance is critical, a small loss of data is not critical, and link is known to be reliable, UDP may be
used. Streaming audio or video are examples of applications where high performance is more important
than a possible, but potientially correctable, loss of data.
TCP 
RFC 793
Transmission Control Protocol (TCP) offers a more robust, connection-oriented method for reliably
sending data. Flow control and multiplexing are supported. Unlike UDP, TCP supports the concept of
a continuous stream of data between two hosts. Unlike IP, TCP will make multiple attempts to deliver
data if the initial attempt fails. If the integrity of data is critical, TCP should be used.
A TCP header looks like this:
2.7.3
Information Transfer in Internet
A socket is a virtual communications channel that is established between two hosts. It can use either
UDP or TCP for the transport protocol. Each socket has a unique descriptor, and multiple sockets can
reside on the same port. This allows multiple clients to take advantage of a service on a single port
without the server getting confused. In order to prevent congestion on a single port, many application-
level protocols have a control connection on a known port, and then negotiate another port for subsequent
data transfer.
2.7.4
Types of Internet Access
There are two basic types of Internet access. One is a full-time direct connection; the other is via a
telephone line with a modem. The disadvantage of a direct connection is mostly cost in both dollars and
network maintenance. The advantage is speed, where T1 (1.5 Mb/sec) or T3 (45 Mb/sec) rates are
common measures. The disadvantage of a modem connection is reliability (line noise) availability (busy
signals) and speed. Certain schemes such as v.90 can achieve up to 56k bps download speeds. The
advantage of a modem connection is the cost of both equipment and service, with 56k bps modems
available for under $300 U.S., and unlimited service for around $20 U.S. a month.
Integrated services digital network (ISDN) can provide 128 kb/sec transfer rates. It never seemed to
catch on due to the difﬁculty in conﬁguring the equipment and inconsistent pricing plans across the
country. Many providers charge a ﬂat rate, with some adding charges for each unit of time or unit of
data sent, whereas a local telephone line typically allows unlimited usage.
Source Port
Destination Port
Sequence Number
Acknowledgment Number
Data Offset
Reserved
Checksum
Windows
Urgent Point
Your Data *** next 100 octets ************************

© 2001 by CRC Press LLC
Asymmetric digital subscriber line (ADSL) is a relative newcomer that takes advantage of existing
twisted-pair wiring, and can reach speeds of 6 Mb/sec for downloading, and 640 kb/sec for uploading.
It is being deployed in major cities, but it remains to be seen how widespread the service will become.
Cable modems are slowly becoming available, and offer up to 10 Mb/sec transfer rates. Typically, the
upstream connection consists of high-speed ﬁber, with the ﬁnal connection to the cable modem being
made with coax.
There are hybrid solutions, such as a satellite (with speeds of 400 kb/sec and higher) that uses a phone
line for uploading data, and a satellite for data download. This can be a good solution for scenarios such
as surﬁng the web, where the amount of data sent to request information is much less than bandwidth-
intensive data types, such as graphics and sound, which comprise the data.
There are two major protocols used for establishing a TCP/IP connection over phone lines. Point to
Point Protocol (PPP) is the more modern method, and PPP software is included with nearly every major
operating system. Serial Link Internet Protocol (SLIP) is an older standard that is being phased out in
favor of PPP.
2.7.5
Internet E-mail
SMTP — STD10
POP — STD53
IMAP4 — RFC2060
There are a few standards for sending and receiving Internet e-mail. The most popular protocol for
receiving e-mail is Post Ofﬁce Protocol (POP) which defaults to TCP port 110. A newer protocol, Internet
Message Access Protocol (IMAP4), used for receiving e-mail, resides on TCP port 143.
The most common protocol used for sending e-mail is Simple Mail Transfer Protocol (SMTP) which
defaults to TCP port 25. Note that POP can also be used for sending e-mail, but this feature is an optional
extension of the POP protocol, and is not supported by many e-mail clients and servers.
SMTP 
The SMTP service allows the sending of e-mail by providing relevant information, in a speciﬁc order, to
a SMTP server. The conversation between the client (sender) and the server consists of human-readable
text commands that are assigned four-letter codes, and three-digit code numbers as responses. A typical
exchange may look like this:
This system works well enough, with servers cooperating when mail needs to be forwarded to a domain
outside of their own. Messages can be delivered in minutes, if all the servers in a message’s path respond
in a timely manner.
Although standard SMTP does not have any provision for conﬁrmation of receipt (there are proposed
standards, but they are not widely implemented yet) problems encountered along the path of the message
are usually reported to the sender. If a server ﬁnds that another server is unavailable, it will usually try
C: HELO megacorp.com
(sender identification)
S: 250 smtp.conhugeco.com
(server identification)
C: MAIL FROM:<johnbraun@megacorp.com>
(who is this from?)
S: 250 OK
C: RCPT TO:<dilbert@anotherdomain.net>
(who is the recipient?)
S: 250 OK
C: DATA
(ready for data?)
S: 354 Go ahead…
(sure)
C: Hey Dilbert get to work!
C: .
(end of input)
S: 250 OK
C: QUIT
(sender all done)
S: 221 Bye…
(server says seeya)

© 2001 by CRC Press LLC
to send the message several times before giving up. If a server crashes while processing mail, the message
may get lost forever. For critical documents, it may be wise to ask the receiver to conﬁrm receipt.
One problem with current SMTP implementations is that they don’t conﬁrm the validity of the sender’s
address. This has led to massive abuse by junk e-mail senders, who really don’t want you to respond via
e-mail, anyway. An attempt to reply will result in getting a message saying that the (bogus) return address
doesn’t exist, or that it has already been shut down.
There are proposed methods of authenticating an entity wishing to use an SMTP server, in hopes of
reducing spam and other evils. There are also solutions, mainly using public key encryption and digital
signatures, to conﬁrm the identity of the sender. These features are not available at the protocol level yet.
POP 
The POP protocol is used to retrieve e-mail for a speciﬁc user. Session establishment can be done with
a simple username and password scheme, or can optionally use more sophisticated means like APOP.
APOP never sends the user’s password over the connection, instead applying the MD5 algorithm to the
password and other time-sensitive data. Not all POP servers support APOP, and will return an error
message if this method of login is attempted but not supported.
After a message is retrieved, it is usually deleted. Some POP servers allow the user to keep their e-mail
on the server after it has been retrieved, but this is not guaranteed. The IMAP4 protocol is better suited
for keeping e-mail on a server.
Some POP servers offer the ability to send e-mail as well as receive it. The advantage of using POP
for sending e-mail is that it requires users to identify themselves, thereby reducing the generation of
spam and other unauthorized e-mail. It also eliminates the need to maintain a separate SMTP server.
The disadvantage of this method is that not all e-mail clients and servers support sending via POP.
IMAP4 
IMAP4 is a newer protocol for retrieving e-mail. It offers a richer set of features than POP, including
keeping e-mail on a remote server, searching e-mail before retrieval, and a greater number of authenti-
cation schemes.
The option of keeping e-mail on a server helps reduce resource requirements on the client, but increases
the need for more disk space, as well as regular backups to ensure the historical data is preserved. This
scheme can beneﬁt devices such as portable computers, network computers, and portable digital assis-
tants, which may have limited storage capacity. It can also offer convenience to mobile users, since their
mail can be stored on the server, rather than being spread among multiple clients.
2.7.6
Telnet in Internet
Telnet — STD8 
Telnet is a service that resides on TCP port 23 and offers terminal services to remote users. The client
and server can negotiate features of the connection, which can range from a simple ASCII exchange, to
enhanced services such as cursor control and styled text.
A telnet client can be used to interact with other TCP services that exchange text or binary data,
provided that the telnet client allows one to specify the port one wants to connect to. For example, a
telnet client can connect to an SMTP server on port 25 and send e-mail. This can be handy for debugging
or investigative purposes.
2.7.7
File Transfer in Internet
FTP 
Request for Comments: 959
RFC 990 — Port #
File Transfer Protocol (FTP) provides a reliable, standard way to transfer both text and binary ﬁles
between hosts. There are two types of connections when using FTP, a control connection and a data

© 2001 by CRC Press LLC
connection. A control connection is used for the client and server to exchange commands and status
information. The default port for a control connection is 21. The default client data port is 21, and the
default server data port is 20.
In most cases, a data port with a value outside the range of commonly known services is selected for
security reasons. The client can request a speciﬁc data port via the PORT command, or can ask the server
to select one with the PASV command.
Although FTP makes every attempt to deliver a ﬁle, circumstances beyond the user’s control (modem
disconnect, network failure) may cause the transfer to be interrupted. Most, but not all, FTP implemen-
tations support a restart mode where ﬁle transfer can begin at a point other than the beginning of the
ﬁle, allowing data transfer to begin at the point of interruption and complete.
2.7.8
News and Usenet
NNTP — RFC977
The most common way of distributing news on the Internet is by using the Network News Transfer
Protocol (NNTP). The system, originally created to exchange messages in a university environment, has
evolved to a worldwide messaging system with thousands of distinct topics of interest, called newsgroups.
Each newsgroup name consists of several elements, with the general classiﬁcation at the beginning of
the name, and the speciﬁc subject matter at the end. For example, the group comp.sys.mac.hardware.video
is a computer-related group about video cards for Mac systems.
The most popular hierarchies are sometimes referred to as the “big seven” and include comp (com-
puting), misc (miscellaneous), news (newsgroup-related), rec (recreation), sci (science), soc (social), and
talk (discussion). There are many other hierarchies for alternative and localized content.
Clients to allow one to read and post news are available as both stand-alone applications, or integrated
with other Internet clients such as a web browser. Sophisticated clients can allow one to organize messages
about a similar topic into threads, or ﬁlter messages based on certain criteria. Although most messages
pertain to the topic of the newsgroup, there are those who “spam” the newsgroups with ads or other
material unrelated to the group.
Care should be taken when conﬁguring your news client where it asks for your e-mail address. Your
address is normally added to any messages that you post, with the intent of making it easier for others
to make a personal reply. Alas, there are those who scan the newsgroups for e-mail addresses, which are
then used for the purpose of sending junk e-mail. A good strategy is to add some text to your address
that a human would know to delete before making a personal reply, such as john@ihatespam.mega-
corp.com.
A dedicated news host is required before clients can post and retrieve messages. Typically, a news host
will server a large group of clients, and will exchange articles with another upstream host. The connection
between hosts is referred to as a feed, and is meant for propagating articles and control messages, and
not meant for direct client connections. If all hosts along a certain path agree to exchange the same
group, a message posted in a particular group on one host will eventually propogate to all other news
hosts. Hosts can also choose to restrict the groups they carry, which can help conserve disk space and
bandwidth.
Care must be taken in deciding which groups a host should carry, and how long the articles should
remain on the server before being deleted or expired. Inaccurate estimates can result in a dreaded disk
full error, which causes many hosts to reject articles until someone clears some disk space.
2.7.9
Mailing Lists in Internet
A mailing list is a method of allowing Internet users to communicate about speciﬁc topics via e-mail.
To join a mailing list, one sends an e-mail to a special address, and indicates a desire to join the list, as
well as what e-mail address should receive further information from the list. The subscriber will then
receive information on how to submit messages to the list, and how to perform administrative functions,

© 2001 by CRC Press LLC
such as being removed from the list. If the list is unmoderated, any message submitted to the list will be
sent to all other subscribers. If the list is moderated, an administrator will decide which submissions
should be distributed to other members. Messages may be sent one-by-one, or grouped and sent in a
digest. Choosing to receive messages in digest form is a good idea for busy lists, lest your mailbox gets
ﬁlled with hundreds of messages.
http://www.lsoft.com/listserv-hist.stm
http://www.wrt.tcu.edu/wrt/wri/ﬁles/barnes.htm
listserv distribute protocol information — rfc1429.txt
2.7.10
Information Search in Internet
2.7.10.1
Spiders
There are several tools, referred to as “spiders” which basically “crawl” through a web site, acquiring some
or all of the information on each page. They then allow users to search through this information, in
hopes of ﬁnding a resource relating to the topic of interest.
Many spiders have evolved into portals, which not only allow you to ﬁnd information drawn from
the Internet, but also provide links to other commonly used services, from maps to news headlines to
package tracking.
Some of the more popular spiders are:
Alta Vista
http://www.altavista.digital.com
Excite
http://www.excite.com
Infoseek
http://www.infoseek.com
Lycos
http://www.lycos.com
WebCrawler
http://www.webcrawler.com
2.7.10.2
Category Indexes
Another method of ﬁnding what you are looking for is to use a tool where sites are scanned by actual
humans, and then placed into one or more catagories. This method of locating information is a good
complement to using a spider, since a spider may return too much information to be useful. The most
popular of these services is Yahoo at http://www.yahoo.com.
2.7.11
Netscape and Microsoft
Netscape (NASDAQ: NSCP) and Microsoft (NASDAQ: MSFT) are two of the major players in the
commercial Internet client and server markets. Netscape, established in 1994, was the ﬁrst company to
offer a commercial Internet browser, followed by server products. Microsoft was late to the party, but
now also offers a full suite of Internet client and server products. Whereas Microsoft server products run
on Windows, and their client programs span Windows, Mac, and UNIX, Netscape offers server products
on both the Windows and UNIX platforms, and client programs for Windows, Mac, and UNIX.
Netscape has evolved from being a browser-only company, to one that now depends on server software
and access to their NetCenter site for revenue. This is no doubt due to Microsoft’s giving away their
Internet Explorer browser. Netscape eventually made the source code to their browser available, in an
attempt to gain back some browser market share by opening up their product.
Although the tight integration of Microsoft’s browser and server products with the Windows operating
system can offer increased functionality, it also tends to lock one into Windows. This can be a problem
when attempting to use Microsoft products with more standards-based solutions like those from
Netscape. Proprietary technologies like ActiveX controls and VBScript don’t always work well with
Microsoft products.
In the fast-moving world of the Internet, it is hard to predict which, if either, company will win. The
strength of Netscape is that they provide solutions for a wider variety of systems, especially in the UNIX

© 2001 by CRC Press LLC
realm, and that they don’t lock you into a single environment. The strength of Microsoft is that their
products are on almost all desktop systems. For a solution where both client and server products are
guaranteed to come from Microsoft, the increased functionality of being closely linked to Windows can
be worth it.
To put things in perspective, the most popular web server application as of this writing is the freeware
Apache web server, with almost 50% market share.
Intranet References
The following web sites provide comprehensive sources of information and links to other sites for
reference material relating to the Internet:
http://www.cisco.com
http://www.intel.com
http://www.microsoft.com
http://www.sun.com
http://www.wcom.com
Glossary of Intranet Acronyms and Terms
ARPAnet
Earliest packet switched network; the progenitor to today’s Internet.
ASCII
Plain text ﬁle, containing only regular keyboard characters.
BCP
Bridging Control Protocol, used to conﬁgure bridge protocol parameters on both
ends of a point-to-point link. 
C/C++
High-level programming language allowing program control of system hardware,
and designed for code portability on all types of computers. C++ is a programming
language that extends the Object Oriented capabilities of C.
CA
A Certiﬁcate Authority: a third-part which can attest that you are on record as the
only person with the key associated with a personal digital signature. 
CCITT
International Telegraph and Telephone Consultative Committee.
CHAP
Challenge Handshake Authentication Protocol: a commonly used protocol for link
encryption. Authentication occurs at the data link layer and is transparent to end
users.
CGI
Common Gateway Scripting: used to support two-way browser communication. 
Digital Signature
Used to validate the identity of the ﬁle sender’s e-mail.
DNS
Domain Name Server protocol, part of TCP/IP. 
ECP
Encryption Control Protocol, part of the PPP suite. 
Ethernet
LAN protocol as speciﬁed in the IEEE 802.3 standard.
Firewall
Collection of hardware and software that interconnects two or more networks and,
at the same time, provides a central location for managing security. 
FTP
The File Transfer Protocol: a standard protocol for transferring and copying ﬁles
from one computer to another.
Kbps
1,024 bits per second.
Home Page
The ﬁrst page that users see when they access a particular Web site.
HTML
HyperText Markup Language: used to describe the layout and contents of pages on the
Web. The Hypertext Markup Language (HTML) is the language of the World Wide Web. 
ICMP
Internet Control Message Protocol: deﬁnes the rules routers use to exchange routing
information.

© 2001 by CRC Press LLC
ICMPv6
Internet Control Message Protocol Version 6.
IGMP
The Internet Group Management Protocol is used by IP hosts to report host group
clusters to neighboring multicast routers. 
IGRP
Interior Gateway Routing Protocol, part of TCP/IP.
IPCP
IP Control Protocol, used to conﬁgure IP parameters on both ends of the PPP link. 
IPX
Internetwork Packet Exchange: Novell’s implementation of the Xerox Internet
Datagram Protocol (IDP), used to deﬁne a set of rules for coordinating network
communication between network components. 
ISDN
Integrated Services Digital Network: provides digital communications circuit that
allows transmission of voice, data, video, and graphics at very high speeds (from
56 Kbps to 128 Kbps), over standard communication lines. 
ISP
Internet Service Providers act as middlemen, renting time to other users who want
to access the Internet.
JAVA
A computer programming language that allows users to execute special programs
(called applets) while accessing and viewing a Web page. Java is designed for creating
animated Web sites.
LAN
Local Area Network. 
LCP
Link Control Protocol: conﬁgures and tests the data link connection, and is part of
the PPPsuite. 
MARS
Multicast Address Resolution Server.
Mbps
1,000,000 bytes per second.
MPEG
Motion Picture Experts Group deﬁning standards for handling video and audio
compression.
OS/2
A high-performance, multi-tasking workstation operating system. 
OSPF
Open Shortest Path First, link-state routing protocol used for IP routing.
PAP
Password Authentication Protocol: used for transparent session authentication
occurring at the data link layer.
PGP
Pretty Good Privacy is a free (for personal use) e-mail security program developed
in 1991 to support public-key encryption, digital signatures, and data compression.
PGP is based on a 128-bit key.
PPP
Point-to-Point Protocol is one of the major protocols used to connect to the Internet.
PPP is newer and faster than SLIP.
PKE
Public Key Encryption allows a sender to encrypt a document using a public key,
which the recipient decodes using a private key.
POP3
Post Ofﬁce Protocol version 3, allowing dynamic workstation access to mail drops
on a server host.
PPTP
Point-to-Point Tunneling Protocol.
RIP
Routing Information Protocol: maintains network exchange and topology information.
RFC
Request for Comments: discussion notes, recommendations, and speciﬁcations for
the Internet.
Router
This hardware device can be used to ﬁlter out data packets-based speciﬁc selection
criteria. Thus, the router can allow certain packets into the network while rejecting
others.
S-HTTP
Secure HTTP: a protocol developed by the CommerceNet coalition that operates at
the level of the HTTP protocol.

© 2001 by CRC Press LLC
SLIP
Serial Line Internet Protocol: one of the major protocols used to connect to the
Internet. It predates PPP. 
SMTP
Simple Mail Transfer Protocol: speciﬁes the format and delivery handling of elec-
tronic messages.
SPX
The Sequenced Packet Exchange protocol deﬁnes a set of rules for coordinating
network communication between network components. 
SSI
SSI software is used by Web servers to display and/or capture dynamic (changing)
information on an HTML page.
SSL
The Secure Socket Layer (SSL) was developed by Netscape Communications to
encrypt TCP/IP communications between two host computers.
Subnet
Partition on an IP network based on class, involving use and movement of a subnet
mask.
Surﬁng
Term used to describe accessing (through a Web browser) a chain of documents
through a series of links on the Web. 
T1
A leased line which can support transmission speed to 1.54 Mbps.
TCP/IP
The Transmission Control Protocol/Internet Protocol: speciﬁes the rules for the
exchange of information within the Internet or an Intranet, allowing packets from
many different types of networks to be sent over the same network.
TCM
Total quality management involves creating systems and workﬂows that promote
superior products and services.
Telnet
This service allows connection to a remote Internet host so that programs can be
executed from a remote computer.
UDP
User Datagram Protocol provides message service for TCP/IP.
UNIX
UNIX is an operating system well suited to the Internet’s open system model.
URL
Millions of documents that are distinguished by a unique name called a URL (Uni-
form Resource Locator), or more simply, a Web address. The URL is used by Web
browsers use to access Internet information
UUCP
Unix to Unix Copy: allows ﬁles to be copied from the Unix system to another.
XML
Extended Markup Language: designed to provide a self-descriptive, platform-inde-
pendent mechanism for exchanging management information between applications. 
Web Browser
Allows you to traverse and view documents on the World Wide Web.
Windows NT
Provides a high-performance, multi-tasking workstation operating system. 
WWW
The World Wide Web — or Web — is a collection of seamlessly interlinked documents
that reside on Internet servers. The Web is so named because it links documents to
form a web of information across computers worldwide.

© 2001 by CRC Press LLC
Joe Ghetie et al. “Network Management and Administration’’
The CRC Handbook of Modern Telecommunications
Ed. Patricia Morreale and Kornel Terplan
Boca Raton, CRC Press LLC. 2001

© 2001 by CRC Press LLC
© 2001 by CRC Press LLC
3
Network Management
and Administration
3.1 
Management Concepts 
Management and Data Communications • Management 
Requirements • Management Paradigms • Open 
Management Systems • Distributed Management 
Systems • Management Systems Topological 
Frameworks • Management Systems Evolution
3.2 
Management of Emerged and Emerging Technologies
Introduction • Foundation Concepts for Networking 
Technologies • Management Solutions for Emerged 
Technologies • Emerging Technologies • Summary
3.3 
Commercial Network and Systems Management 
Standards
Manager–Agent Relationship • Common Management 
Information Protocol (CMIP) • Simple Network 
Management Protocol (SNMPv1, SNMPv2, and SNMPv3) • 
Remote Monitoring (RMON1 and RMON2) • Desktop 
Management Interface (DMI) from Desktop Management Task 
Force (DMTF) • Object Management Architecture (OMA) 
from OMG • Standards for Web-based Systems and Network 
Management • Lightweight Directory Access Protocol (LDAP) • 
Summary
3.4 
Telecommunications Management Network (TMN)
Introduction • Network Management Key Concepts • 
Functions and Architecture of a TMN • Interconnecting 
Managed Networks • Management of SDH/SONET • TMN 
and GIS (Geographic Information System) • Trends of 
Evolution of TMN
3.5 
TINA 
Introduction • Partitioning Axis • Functional Axis • 
Computational Axis • Life Cycle Axis • Summary and 
Further Work • Acronyms
3.6 
Telecommunications Support Processes
High-Level Breakdown of Support Processes • Customer Care 
Processes • Service Development and Operations Processes • 
Network and Systems Management Processes • Summary and 
Trends
3.7 
Management Frameworks and Applications.
Evolving Management Frameworks • Features and Attributes 
of Management Frameworks • Management Framework 
Examples for Telecommunications Providers • Management 
Framework Examples for Enterprise Users • Management 
Applications • Summary
Joe Ghetie
Bellcore
Kornel Terplan
Industry Consultant and Professor
Endre Szebenyi
Industry Consultant
Takeo Hamada
Fujitsu Laboratories America
Hiroshi Kamata
OKI Electric
Stephanie Hogg
Telsta Research
Carel Marsman
CMG

© 2001 by CRC Press LLC
3.8 
Customer Network Management
Deﬁnitions • Concerns of Customers • Basic Structures 
and Core Components • AccessCNM from Objective 
Systems Integrators • Summary
3.9 
Aspects of Managing Outsourcing Solutions: Aiming 
for Success
Introduction • Outsourcing — the Evolution • Managing 
the Strategic Relationship — Supplier Management • 
Business Processes and Outsourcing — What’s to be 
Managed? • The Partnership Approach of Service 
Management • Organizing for Success — A People 
Business • Conclusions
3.10 
Support Systems for Telecommunication Providers
Status, Deﬁnitions, and Markets of Operations, Business, and 
Marketing Support Systems • Market Drivers for 3SSs • 
Strategic Beneﬁts of Advanced 3SSs • Providers of 
Operational, Business, and Marketing Support Systems • 
Positioning and Evaluating Products • Future of Telecom 
3SSs • Acronyms
3.11 
Performance Management of Intranets
Abstract • Introduction — Internet, Intranets, and 
Extranets • Generic Intranet Management Challenges • 
Speciﬁc Challenges to Intranet Performance Management • 
Content Management • Log File Analysis • Wire 
Monitors • Web Server Management • Load Balancing • 
Look-through Measurements • Trends of Intranet 
Performance Management
Introduction
It is not enough to develop, implement, and rollout new technologies by telecommunications service
providers. These technologies should be properly administered and managed. Over a ﬁve-year period,
management and administration would take up to 85% of operating expenses; acquiring the technology,
just 15%. It is a very important metric for cost-justifying investments into management and administration.
This segment addresses administration and management issues. Management concepts outline the
basics of managers and managed entities. Concepts include several management models, such as central
and decentral, concentrated and distributed, and the use of hierarchical schemes supported by umbrella
managers. Also, open management is addressed, including the open systems conceptual model, associated
systems concepts, and requirements for open management systems. Distribution of management pro-
cesses and functions will play a key role in future management solutions. Managed entities must be
connected with element managers and management platforms using in-band or out-of-band commu-
nication schemes. This contribution gives examples for both alternatives.
Administration and management are usually an afterthought when considering the deployment of
innovative technologies. This contribution tries to bring the technology deployment with the selection
and implementation of management solutions into synchronization. Each technology that is considered
innovative, such as frame relay, FDDI/CDDI, Switched MultiMegabit Data Service (SMDS), ATM,
Sonet/SDH, Cable, mobile and xDSL, is investigated for how far management and administration solu-
tions are available and implementable. In particular, the availability and structure of MIBs (management
information base) are analyzed. In most cases, MIBs support most of fault, conﬁguration, performance,
security, and accounting management functions. MIBs in combination with SNMP managers do useful
work for history-type of data visualization, analysis, and reporting. State-of-the-art technology needs
additional management tools and applications that help with real-time decision support.
Management and administration depend to a large extent on management standards. There are two
principal groups: standards for enterprise-level administration and management, and standards for

© 2001 by CRC Press LLC
speciﬁc telecommunications environments. The management standards contribution focuses on enter-
prise-level standards, such as SNMP, RMON, and DMI, ﬁrst. Components of telecommunications stan-
dards are also discussed in some depth, e.g., CMIP, Corba, and DCOM. This contribution prepares the
readers for telecommunication network management (TMN) and Telecommunications Information
Networking Architecture (TINA), and for better understanding management framework products and
management applications.
TMN is a very simple model for streamlining management and administration. It uses four layers in
addition to the network element’s layer at the bottom. Management processes, fucntions, and tools may
be categorized in accordance with these layers. This TMN contribution goes into depth and discusses
various TMN models (information, fucntional, and physical), TMN elements (operations systems func-
tion, workstation functions, mediation functions, Q adapter function, network element function), TMN
internal and external interfaces (Q3, Qx, X, F, and M), and the most appropriate use of Data Commu-
nication Network (DCN).
TINA goes one step behind TMN and offers four dimensions of considerations: life cycle management,
computational infrastructure, partitioning by layers and domains, and functional representing fault,
conﬁguration, accounting, performance, and security management. TINA and TMN can work together,
but they are not identical. TINA puts more weight on service fulﬁllment and service assurance. Also,
resources are described in much more depth. TINA can be tailored to the needs of particular service
providers.
The TeleManagement Forum offers guidance for deploying and re-engineering telecommunications
business processes. This contribution uses the basic business model of breaking down support processes
into two dimensions: life cycle of services, such as fulﬁllment, service assurance, and billing processes,
then hierarchy of services, such as customer care processes, service development and operations processes,
and networks and systems management processes. This contribution handles all 16 principal support
processes, individually. Also, their links to each other, to the customers, and to the physical networks are
addressed.
Management frameworks are the heart of support systems for telecommunications providers. They
consist of an application platform and of management applications. This contribution outlines the
principal attributes, such as architecture, application programming interfaces, protocol support, hardware
and software platforms, graphical user interface, application programming interfaces, management func-
tions supported, security modules, modeling capabilities, and internal systems services. For both tele-
communications and enterprise environments, framework products are listed, and a few of them, such
as OpenView from Hewlett-Packard, TNG from Computer Associates, FrontLine Manager from Man-
ageCom, NetExpert from Objective Systems Integrators, and TeMIP from Compaq/Digital are analyzed
in some depth. Over the next couple of years, frameworks are expected to embed the best of suite
management applications with the result of full functionality to implement operations, business, and
marketing support systems.
It is expected that telecommunications service providers and their customers will connect their man-
agement systems and applications. The name of this concept is customer network management (CNM).
This contribution outlines the joint work, principal management processes and functions, and also legal
issues. If successful, information exchange between providers and customers can be accelerated, and
duplicated functions can be eliminated.
Service management is in the center of the next contribution. Service quality may be improved when
certain management functions are outsourced to third parties. This contribution details the drivers for
outsourcing and critical success factors of outsourcing alliances. Reporting on principal services metrics
is key in all relationships. This contribution deals with practical examples for performance indicators
and service level reports. Service management means more than element management. Service manage-
ment is targeting more consolidated metrics in the TMN architecture.
Web technology is going to change the way management and administration systems work. Using Java
applets and components of Web-Base Enterprise Management (Wbem) standards, management and

© 2001 by CRC Press LLC
administration can be uniﬁed and simpliﬁed. This contribution handles Web basics (URL, Web server,
Web browser, HTML, XML, and HTTP), evolving standards (Java and Wbem), and many application
examples from framework vendors and management application vendors. This technology is expected
to penetrate and change the way present operations, business, and marketing support systems work.
Support systems of telecommunications providers represent a very complex but increasingly signiﬁcant
segment of the communications industry. This contribution starts with the market drivers for support
systems, such as network complexity, customer in focus, more standards, very high growth rates, dereg-
ulation, and convergence, followed by startegical beneﬁts of such support systems. This contribution also
identiﬁes the suppliers of support systems, such as consulting companies, computer manufacturers,
equipment manufacturers, software companies, and outsourcers. The remaining part of this contribution
focuses on positioning products in terms of supporting markets (voice, data, Internet, cable, and wireless),
supporting management areas (customer care and billing, provisioning and order processing, and net-
work operations management), and compliance to TMN layers, such as business, service, network, and
element management layers.
Intranets are penetrating both the telecommunications providers and enterprise infrastructures. This
contribution targets the management of these kinds of networks. In detail, it identiﬁes sensitive compo-
nents that may cause congestion or bottlenecks. Special emphasis is on log ﬁle analysis, wire monitoring,
look-through measurements, trafﬁc shapers to conserve bandwidth, and on administering Web server
farms. For each subject area, product examples are also included. In most cases, Web content is expected
to drive decisions about resource facilities and equipment reservation/allocation.
3.1
Management Concepts
Joe Ghetie
The last decade of the past millennium witnessed one of the most dramatic advancements of commu-
nications technologies and services in human history. Communication, as a way of conveying and
exchanging management information, had found in the Internet one of the best examples of the explosive
growth with a tremendous impact on the current and future abilities of humans to share information.
The dream of universal access to information, the dream of a giant village, the dream of fast, reliable,
content-rich information exchange are today closer to reality than anybody has anticipated. Data com-
munications, video communications, and both wired and wireless communications media have increased
our ability to control through communications large, global enterprises and businesses.
Network and systems management are specialized systems targeting, monitoring, and controlling the
vast array of network and computing systems resources used in communications, manufacturing, com-
merce, ﬁnance, banking, and education, as well as in research and development.
Management systems were born out of necessity to prevent, diagnose, conﬁgure, and solve problems
raised by the size, complexity, and heterogeneity of multivendor, multiprotocol, and multitechnology
environments that characterize the underlying network and computing systems.
Although management systems are value-added components to communications technologies, they
are as vital as the transmission, switching, and operations systems in order to supervise and maintain
the normal information exchange.
3.1.1
Management and Data Communications
Management systems aimed at monitoring and controlling communications systems represent conceptual
design and associated infrastructure that, essentially, resemble particular implementation of open systems.
3.1.1.1
Communications General Model
A simpliﬁed view of any point-to-point communication assumes an information source (sending party)
and the information destination (receiving party). The communication takes place over a transmission

© 2001 by CRC Press LLC
media which can be a pair of copper wires, coaxial cable, ﬁber optic, or a wireless media such as radio,
microwave, satellite, or infrared rays (Figure 3.1.1).
The information source can be a telephone set, a computer, a TV pattern, a facsimile, or an instru-
mentation process. The information can be another telephone set or a computer, a TV set, a fax machine,
or a control panel.
In order to be transmitted, the native information sources, voice, computer or instrumentation data,
graphics, or video images require successive data/signal conversions, according to adopted communica-
tions media and transmission technologies, and a rigorous security control of the access to shared network
resources. Therefore, new components such as data/signal converters and access control systems should
be added to the communication model. This communication can be asymmetric, i.e., taking place only
in one direction, or it can be symmetric, i.e., taking place bidirectionally (Figure 3.1.2).
As a further consideration, the box representing the transmission media becomes more than a single
conduit; a mixture of transmission/transport components and switching components in the form of
circuits, links, nodes, routers, and switches participates in the design of a shared network environment.
3.1.1.2
Network Management General Model
The task of management, as derived from the general model of communications, is very clear: to be able
to supervise, monitor, and control all the components that participate in the process of communications
from the source to destination. That might include various computer hosts and terminals as sources/des-
tinations of information, the devices performing data/signal conversions (protocol converters, emulators,
concentrators, multiplexers), devices required to control the access to the network (security access,
authorization, encoding, encryption), and all the components used in transmission, switching, and
routing (Figure 3.1.3).
The task is not only clear but quite challenging when the list of actual devices is spelled out. Many
dozens of different technologies implemented in hundreds of different components, developed, designed,
and manufactured by thousands of vendors, are all potential subjects of management systems, especially
when it comes to the point of providing end-to-end, enterprise-wide management services from moni-
toring, diagnostics, control, and reporting.
FIGURE 3.1.1
Communication network conceptual model.
FIGURE 3.1.2
Communication network conceptual model.
Source
Destination

© 2001 by CRC Press LLC
PCs, workstations, minicomputers, servers, mainframe computers, terminals, test equipment, phones,
PBXs, TV sets, set-top boxes, cameras, modems, multiplexers, protocol converters, CSU/DSUs, statistical
multiplexers, packet assembler dissassemblers, ISDN adapters, NIC cards, codecs, data encoders, data
compression devices, gateways, front-end processors, line trunks, repeaters, regenerators, matrix switches,
DCS/DACs, bridges, routers, and switches just begin a list of devices which should be or might be
managed.
The management picture is complete only if we consider, in addition to the management of network
resources, the management of computing systems resources such as thousands of different businesses,
users, systems applications, databases, and complex, specialized, large operations systems.
All the information collected and exchanged in conjunction with management operations is translated
in management data which is manipulated using techniques similar to those employed by data commu-
nications networks. However, substantial differences exist between data communication exchange and
management information exchange to claim a specialized technical ﬁeld, specialized communication
protocols, information models, and specialized skills to design and operate management systems and
interpret fault, performance, conﬁguration, or security management information.
The following subsections will explain what is peculiar in management systems, the major requirements
appended to management systems, the management paradigms adopted in management, and the historic
and technical evolution of management systems.
3.1.2
Management Requirements
The diversity of managed resources, as found in traditionally distinct ﬁelds of communication such as
voice, data, and video communications, generate different views on what should be the management
functions and management requirements associated with management systems.
3.1.2.1
High-level Management Functions
Regardless of the diverse management views, three high-level management functions top the list: mon-
itoring, controlling, and reporting. Monitoring represents the continuous collection of management
information about the status of management resources, delivered in the form of events and alarm
notiﬁcations when the threshold attached to managed resource parameters is exceeded. Controlling is
the targeted attempt of the manager or management application to change the status or conﬁguration
of selected managed resources. Reporting consists of delivering and displaying the management infor-
mation in an accessible form for reading, viewing, searching, and ultimately interpreting the reported
information.
In practice, several other functions are associated with management systems and management appli-
cations according to particular business needs such as provisioning, service activation, capacity planning,
network/systems administration, inventory management, backup and recovery management, and man-
agement operations automation. Many of these complex functions include or are built on basic moni-
toring, controlling, and reporting.
FIGURE 3.1.3
Network management conceptual model.

© 2001 by CRC Press LLC
3.1.2.2
High-level Users Management Requirements
Based on the users’ perspective on management, we can derive a set of high-level requirements associated
with management, as listed below:
a. Ability to monitor and control end-to-end network and computing systems components.
b. Remote access and conﬁguration of managed resources.
c. Ease of installation, operation, and maintenance of the management systems and their applications.
d. Secure management operations, user access, and secure transfer of management information.
e. Ability to report meaningful management-related information.
f. Real-time management and automation of routine management operations.
g. Flexibility regarding systems expansion and ability to accommodate various technologies.
h. Ability to backup and restore management information.
3.1.2.3
Driving Forces behind Management Technologies
Although the term of “network management” gained a clear acceptance only in the mid 1980s with the
advancement of IBM management tools (later incorporated into the IBM NetView family of management
products), network management was equally driven by the development of telecommunications, data
communications, and computing systems networking. For telecommunications and data communica-
tions, the management technologies were concentrate on management of transmission and switching
equipment (hardware devices, connections, circuits) along with conversion and access control devices.
In the case of computing systems, the management technologies were concentrate on managing large
computing system resources (hardware, interfaces, memory, data storage devices, etc.) and applica-
tions/databases.
With the convergence of telecommunications and computing systems, which embraces various tech-
nologies commonly known as computer telephony integration (voice over Internet is one of the most
recent developments), the common point of these major ﬁelds becomes the network which connects
these systems and the management of large data communications networks. This will be the dominant
factor of the networks of the future.
3.1.2.4
Justifying Network Management Investment
It is well known that management systems are perceived as overhead cost. However, the cost of not being
able to prevent major network and systems problems or to quickly ﬁnd and restore a system to normal
functionality is even higher and can be crippling for many businesses relying on information exchange.
The following reasoning can be used in justifying the investment in network management. Some points
can be quantiﬁed and used as a basis for a front-end analysis when selecting management systems.
a. Reducing downtime of critical components of networks and computing systems.
b. Controlling the corporate networks as strategic investment assets.
c. Controlling the performance, growth, and complexity of user application.
d. Improving services in customer support and security of data transfer.
e. Controlling the cost of information technology deployment and operations.
3.1.3
Management Paradigms
Before analyzing the capabilities or the openness expected from management systems, we have to under-
stand the fundamental paradigms used in management and the views associated with these paradigms.
3.1.3.1
Management Basic Model
Conceptually, the management systems are based on a simple model. In this model, management is the
interaction/cooperation between two entities: the managing entity and the managed entity. The man-
agement entity represents a management system, a management platform, and/or a management appli-
cation. The managed entity represents the managed resources. Looking at this simple model, it is

© 2001 by CRC Press LLC
important to note its similarity to the basic communication model presented at the beginning of this
chapter (Figure 3.1.4).
In order to communicate with the managed resources, which do not have any native mechanism to
pass management information, there is a need to create an intermediary component, the agent. The agent
is also called management agent or managed agent. The manager is the management entity, while the
agent hides the interaction between the manager and the actual managed resources (Figure 3.1.5).
The manager–agent model is very common, used in describing the interaction between the manage-
ment entity and the managed entity at a high level. This is the reason that all the paradigms natively
created for management purposes closely follow the manager–agent model. In reality, the manager–agent
model is more complex (Figure 3.1.6).
The complexity becomes more evident when we consider the interactions between the manager or the
management applications and the human operators. Other components, less visible but also very impor-
tant because they shape the nature of interactions between managers and agents, are the management
policies and the operational instructions given to the manager and implicitly to the operator.
There are other paradigms such as client–server and applications–object server that can be used for
management information exchange. Natively, these paradigms have been conceived for building distrib-
uted applications or distributed object environments. Nevertheless, these general paradigms can be
applied for management and there are products that use variations of these paradigms for management
purposes.
3.1.3.2
Management Views and Associated Models
Management assumes, as a primary function, the communication between the managing entity and
managed entity. The management communication is based on the request–reply paradigm. The manager
FIGURE 3.1.4
Management basic model.
FIGURE 3.1.5
Manager-agent model.
FIGURE 3.1.6
Real manager-agent model.

© 2001 by CRC Press LLC
will request from the agent speciﬁc management information and the managed entity, through the agent,
will reply with a message containing the information requested. If the request–reply communication is
used continuously in order to reach each agent and the corresponding managed objects, the mechanism
is called polling and it is primarily used in the management of Internet environments based on the Simple
Network Management Protocol (SNMP) (Figure 3.1.7).
The request–reply mechanism is considered a synchronous communication mechanism, i.e., the man-
ager expects an answer from the agent in a limited time frame before taking any action. If the reply is
not received, a request for retransmission should be initiated by the manager.
There is an additional mechanism for communication between the manager and agents, called noti-
ﬁcation. The notiﬁcation is an asynchronous mechanism initiated by the agent that communicates to
the manager important changes in the status of managed resources which require either manager attention
or intervention.
When building management systems, there are many aspects that should be taken into consideration.
In addition to the communication model, several other models are used in conjunction with the man-
ager–agents relationship, as follows: architectural model, organizational model, functional model, and
informational model (Figure 3.1.8).
The architectural model deals with the design and structure of the components participating in the
management process, i.e., the manager or managers and the agents supplying management information
according to the network topology. The manager can be designed as a management platform that consists
of a management framework and a suite of management applications providing the actual management
functionality such as conﬁguration, fault, and performance management. More details will be provided
in the sections dealing with management platforms.
FIGURE 3.1.7
Manager-agent communication model.
FIGURE 3.1.8
Manager-agent relationship models.

© 2001 by CRC Press LLC
The operational model deals with the operator’s interface to the management system and speciﬁes the
nature and the type of interactions available to the user such as controlling managed objects, displaying
and searching for speciﬁc events, dialog with the systems, and alerting the operator in case of critical
alarms. Most of the operational speciﬁcations are included in the product’s technical speciﬁcations such
as user guide, administrative guide, etc.
The functional model refers to the structure of management functions performed by the management
system through management applications. The functional model is considered a layered model where
basic management functions such as conﬁguration, fault, performance, security, and accounting man-
agement are the foundation of the functional model. Several other management functions such as trouble
ticket administration, help desk, provisioning/service activation, and capacity planning consist of a
combination of the basic management functions. At the pinnacle of the functional model, there are
applications performing complex functions such as alarms/events correlation, expert systems, and man-
agement automation.
The organizational model is tightly linked to the overall management policies and operational proce-
dures. This model speciﬁes management domains, partition of management realm among the manage-
ment operators, access of the user to the management systems, customer-based network management,
interchangeability of the roles between managers and agents, and the overall cooperation between the
manager and other managers or management applications.
The information model, although mentioned at the end of this list, is critical in handling all the
management aspects. Given the variety of managed resources, in order to support their management in
a common way, there is a need of an abstraction of managed resources in the form of a common
information model, known by both manager and agents. The management information model establishes
the basis for deﬁning, naming, and registering the managed resources. Managed objects are considered
abstractions of physical and logical managed resources. Therefore, the term of managed objects implies
the use of an information model. Access to the managed resources is allowed only through the use of
managed objects. The conceptual repository of management information is called management infor-
mation base (MIB). When we refer to a particular MIB, that means a collection of managed object
deﬁnitions that describe a particular management domain or environment. The deﬁnition of managed
objects is standardized and on this basis a manager implementing a particular protocol and information
model can communicate with distributed agents which implement the same MIB.
3.1.3.3
Management Domains
Historically, as we mentioned earlier, the notion of network management was launched by IBM. The
IBM NetView products were in fact a combination between mainframe systems management and network
management. Since then, the concept of management has evolved. At the beginning, the management
products have reﬂected the division, typical to most of the businesses, between network and computing
systems management. With the advent of management platforms, the difference between network and
systems management is blurring since the nature of the application and not the platform framework will
determine the use of management systems.
Currently, it is commonly accepted that two major management domains can be considered when
discussing the nature of managed resources: managing physical resources and logical resources. Physical
resources are considered all the hardware components of the telecommunications and data communications
networks that participate in the process of exchanging information. This management domain is known
as network management. The management of computing systems’ physical resources such as processors,
memory, input/output interfaces, and storage devices, are considered part of systems management.
The management of logical resources is built around applications management and databases man-
agement, both associated with computing systems. Service management, user management, management
of distributed transaction services, and data ﬂow management are also considered system management
of logical resources (Figure 3.1.9).
There is a separate domain which deals with the management of speciﬁc logical resources, i.e., the
protocols used in standards-based communications. Layered protocols, layered service primitives, and

© 2001 by CRC Press LLC
embedded management services are examples of protocol management. This type of management is
applied to interfaces of particular technologies such as ATM, SONET, and WDM in the form of embedded
channels or embedded layer management entities (LMEs). This type of management is conceptualized
in the OSI Basic Reference Model, the foundation of standardized layered architecture and management.
3.1.4
Open Management Systems
In order to evaluate the management systems, there is a need for a reference model. This reference model
is the open system and its corresponding model, the open management system.
3.1.4.1
Open Systems Conceptual Model
The open systems conceptual model assumes a design of systems modeled by the presence of four entities
and by the relationship between these entities: application platform, applications, application program-
ming interface (APIs), and platform external interface (PEI). This model can be applied to any com-
puting system as part of the overall design and implementation. What makes any computing system
(which runs software programs or applications) an open system is the separation of applications from
the applications platform through APIs (Figure 3.1.10).
FIGURE 3.1.9
Management domains classiﬁcation.
FIGURE 3.1.10
Open systems conceptual model.
Physical
Resources
Logical
Resources
Network Management
System Management
Application Management
Database Management
Protocol Management
Host
WS
PC
Application
Application
F
A
C
Data
PDU

© 2001 by CRC Press LLC
3.1.4.2
Open Management Systems Concept
The open systems conceptual model can also be applied to management systems, i.e., to managers and
managed agents. In this case, the applications will be specialized management applications providing
fault, conﬁguration, performance, security, and accounting management. The management platform is
a management framework which consists of, in addition to the computing platform, speciﬁc management
services such as event management services, communication services, graphical user interface services,
or database services (Figure 3.1.11).
As mentioned earlier, key components to open systems are the APIs. In this case, the APIs are speciﬁc
management APIs that allow the development of management applications by using speciﬁc management
platform services. Last but not least, the management platforms are not isolated; they communicate with
managed agents (as a minimum) or with other management systems which may be modeled as open
management systems. The platform external interface, in this case, will be an open standardized interface
with well deﬁned management operations, services, and protocols.
3.1.4.3
Requirements for Open Management Systems
Four high-level requirements characterize open systems and open management systems: operability,
interoperability, portability, and scalabilty (Figure 3.1.12).
Operability represents the ability of management systems to provide easy installation, operations, and
maintenance, as well as adequate reliability and performance. Interoperability represents the ability of
management platforms to transparently exchange management information with managed agents or
peers’ management systems. Portability expresses the ability of management platforms and/or manage-
ment systems applications to be ported to a different environment (computing platform) with minimum
changes or no changes. Scalabilty refers to the ability of management systems to be expanded in coverage,
user domain, and management functions without the need to change the initial design.
FIGURE 3.1.11
Open management system.
FIGURE 3.1.12
Open management system major requirements.

© 2001 by CRC Press LLC
3.1.5
Distributed Management Systems
Most of the computing systems, telecommunications, and data communications networks are distributed,
i.e., interconnected by a communication network designed to transfer information and messages related
to speciﬁc business needs. Management means managing various network and systems resources which
in most instances are physically separated. Therefore, by its very nature, the management is distributed.
A system is considered autonomous (it can be simple processor or multiple processor based) if the
processes that constitute the system share the same memory. In contrast, distributed systems consist of
interconnected autonomous systems with no shared memory. Since any networked computing environ-
ment is inherently a distributed system, the management of these systems is also inherently distributed.
3.1.5.1
Distributed Network and Computing Systems
The true nature of management, as distributed or centralized, is determined not by the physical distri-
bution of its components (managers and agents) but by the centralization and processing of management
information (Figure 3.1.13).
If the system is designed to collect all the management information from all the agents (which constitute
the management domain) in one point, we deal with a centralized type of management. If the collection
of management information takes place in several interconnected processes and the information may be
held in distributed databases, we deal with distributed management systems.
3.1.5.2
Distributed Management Systems Architectures
In a truly distributed management system, multiple management users or operators as management
clients access the management server through a local or a wide area network. The actual manager runs
the management applications and it is the holder of a MIB for a particular management domain. Each
manager is responsible for the agents that are part of his/her domain.
The ability to exchange management information between servers (managers), keep in synchronization
the shared MIB information, take over the management domain of a failed manager, and of the operators
to interact with multiple managers, creates a truly distributed management system architecture
(Figure 3.1.14).
FIGURE 3.1.13
Management of  distributed systems.
CPU
CPU
AGENT
Memory
I/O
Devices
CPU
CPU
MANAGER
Memory
Management Applications
I/O
Devices
CPU
CPU
AGENT
Memory
I/O
Devices
Communications
Network

© 2001 by CRC Press LLC
We have to emphasize that in all these examples we assume that the manager has a high degree of
remotely accessing and conﬁguring agents, with each agent acting as a management agent for a collection
of managed objects and management processes.
3.1.5.3
In-band and Out-of-band Management Systems
All of the diagrams were presented in order to introduce the management concepts, and the properties
of the management ﬁeld included representation of interconnecting networks that carry management
information. It is important to emphasize that these networks have rarely been designed as management-
only infrastructures. Most of the management systems use for management information exchange the
very network that carries the business-related data, voice, or video information. For the purpose of
management, specialized protocols, operations, and application entities have been created and used.
However, the management information is carried on the same physical infrastructure and on the same
communication stack as the business information. In this case, we deal with the in-band type of man-
agement. This is a very cost-effective solution. However, there are some issues. By sharing the same service
channels, the management information may take a signiﬁcant chunk of the available bandwidth, and
this may affect the overall performance of data exchange. That puts restrictions on how much and how
often information is collected.
This is the reason that some management systems are built using out-of-band channels. The out-of-
band management solutions may include unused bandwidth from a current channel allocation. A good
example is the use of the low-band portion (50Hz–200Hz) of the voice grade channels as a dedicated
data channel for management purposes. This solution is used for the management of the modems that
share the same infrastructure with the voice communications. Other solutions consist of reserving a bit
from the normal bit stream (for example, T1 multiplexer) to create a dedicated data channel for man-
agement purposes or assigning ﬁelds in each of the transmitted frames or cells for management purposes
as it happens in the SONET and WDM technologies.
FIGURE 3.1.14
Distributed management systems architecture.
Management
Operators
WS
PC
WS
Local Communications Network
Data Communications Network
Telecommunications Network
Video Communications Network
MANAGER
Management
Applications
MANAGER
Management
Applications
Agent
Agent
Agent
Managed
Resources
NE
NE
EMS
Network
Elements
Element
Management
System

© 2001 by CRC Press LLC
3.1.6
Management Systems Topological Frameworks
At a very high level, the architectural model of management systems is understood as the relationship
between the main components of management systems, the managers and agents. The accepted term for
the architectural layout of the management network is called topology and in most cases follows the
business network infrastructure.
The topological view is the basis for the representation of the network and systems components using
graphical user interfaces. An elaborate collection of graphical icons representing logical and physical
resources, the links between these icons and the colors associated with the status of managed resources
allow the management operators or the users to have a view of the management components along with
their status.
Three major topological frameworks are considered when designing management systems: single
manager, manager of managers, and network of managers.
3.1.6.1
The Single Manager
The single manager topology framework uses one management system which concentrates the collection
and processing of management information from various managed resources such as routers, bridges,
multiplexers, matrix switches, etc. Thus, the manager is the only point of exercising control over the
network.
The system playing the role of the manager is usually a monolithic application that performs man-
agement operations and stores the management information received from all the managed resources.
The single manager topology is fully centralized. Historically, most of the management systems started
as host-based, centralized systems. Today, they still represent the most common topological framework
(Figure 3.1.15).
Regarding the single manager framework, we emphasize its weaknesses as follows: concentration of
network management functions and applications in one point; limitation of the number of resources to
be managed (lack of scalability); and the high vulnerability of these systems when the manager fails. This
topological framework is used for the management of small- to medium-size networks and systems.
3.1.6.2
The Manager of Managers
The manager of managers (MOM) topology is a logically centralized framework with distributed control
capabilities. The MOM acts as a single integration point for several distributed element management
systems (EMSs) (Figure 3.1.16).
The actual management of managed resources/devices is provided by the EMSs that monitor and
control a particular management domain, which may consist of a group of network components and
associated applications. Usually, EMSs are designed to manage a family of similar products built around
a particular technology. In other instances the management domain is determined by geographical,
administrative, or jurisdictional considerations.
FIGURE 3.1.15
The “single manager” topological framework.

© 2001 by CRC Press LLC
This topology is used for medium and large networks. Only vital, critical information such as alarms,
security alerts, and capacity planning-related information is elevated to the level of MOM, which acts as
a management integrator.
3.1.6.3
The Network of Managers
The network of managers topological framework provides fully distributed management based on coop-
erative management between integrated network managers (INMs).
In this topological framework, management information can be exchanged between peer managers.
Each INM is responsible for the management of its own grand domain. Cooperative links between INMs
allow management information exchange. More than that, each INM can take over the management
functions of an adjacent manager. Within each domain, the INM acts as the focal point of distributed
management provided by several EMSs (Figure 3.1.17).
3.1.6.4
The Management Platforms
Management platforms do not represent a new topological framework; thus, they can be used in any of
the topologies described in this section. The management platforms are designed as open management
systems to allow the development and operation of portable distributed management applications. By
employing, as part of the platform framework, advanced management services such as directory, security,
and time services, in addition to basic communication, event management, graphical user interface, and
database services, the management platforms can manage large, heterogeneous, multivendor, multitech-
nology, and multiprotocol environments.
Several management platform components such as graphical user interfaces, management databases,
and management applications can be distributed among several computing platforms. Multiple management
FIGURE 3.1.16
The “manager of managers” topological framework.
FIGURE 3.1.17
The “network of managers” topological framework.

© 2001 by CRC Press LLC
platforms can communicate to each other in order to manage large administrative domains
(Figure 3.1.18).
3.1.7
Management Systems Evolution
Although the management systems have been established as specialized systems to manage large and
complex network and computing systems since the mid 1980s, distinct events and distinct phases can
be identiﬁed in the technical and chronological evolution of management systems.
3.1.7.1
Management Systems Technical Evolution
The ﬁrst phase in the management systems development is exempliﬁed by passive monitoring systems
targeted solely toward network components management and providing test, instrumentation, and
protocol analysis results. This was characteristic for the management systems designed in the late 1970s
and early 1980s.
The next major phase was the build-up of element management systems (EMSs) which provide
monitoring and controlling capabilities of individual systems. Acting as stand-alone systems, the EMSs
target the management families of network elements, equipment, and hosts. Generally, the EMSs contain
a single management application bundled with the computing platform, forming the actual run-time
operational management environment. These types of management systems were typical in the 1980s
and they covered the management of modems, multiplexers, T1 multiplexers, matrix switches, etc. In
most cases, the management was limited to one type of equipment provided by a single vendor
(Figure 3.1.19).
FIGURE 3.1.18
Management platform framework.
FIGURE 3.1.19
Management systems evolution.

© 2001 by CRC Press LLC
Management platforms are the third major generation in the history of management systems devel-
opment that go beyond the run-time operational environment which characterized the earlier stages. By
adding an application development environment with tools and APIs which allow multiple and portable
applications to run on top of management platform framework, the management platforms have
embraced many of the concepts of distributed open management systems.
The run-time environment is represented by common management services provided by the platform
and reﬂects the overall operability aspects of a management platform. The development environment
includes the run-time environment and provides portability for management applications and integration
of these management applications with the platform services. In order to develop management applica-
tions that use platform management services, the development environment should include the run-
time environment. In addition, the complexity of management platforms requires an implementation
environment for testing and conformance to standards or vendor speciﬁcations. The implementation
environment provides means to assess the management platforms’ interoperability. More details about
management platforms will be provided in the following two sections.
3.1.7.2
Management Systems Chronological Events
In order to fully understand the evolution of management systems, it is very important to provide a
more detailed, chronological order of all the events that ultimately shaped the ﬁeld of management. It
is well known that, traditionally, management systems trail the development of new network or com-
puting technologies. In many instances, management solutions are afterthoughts, ad-hoc, and patched
solutions. That creates later problems because the management is modeled, designed, and processed in
systems outside of the technologies that are managed. The technical design and the economies of a later-
added management are always inferior to the one delivered as part of the native technology. This is a
major shortcoming of overall network and computing systems management and it will very likely persist
in a wild competitive environment where the designers and manufacturers of new technologies continue
to rush their products to market even though the management of that technology is missing or is not
mature enough. This situation explains the existing differences in achieving standardization for network
and computing systems management.
The following list of events, in chronological order, attempts to capture the evolution of management
systems.
IBM mainframe-based data extraction and performance analysis tools, 1980–1985
IBM NetView, and integrated systems and network management, 1986
AT&T Uniﬁed Network Management Architecture (UNMA), 1987
DEC, Enterprise Management Architecture (EMA), 1987
Element Management Systems (EMSs) from Timeplex, Paradyne, Codex, GDC, 1987-1989
OSI Management International Standardization (starting in 1989)
Network management platform concept advanced by DEC, HP, IBM, OSF, 1989
Adoption of graphical user interface for network/systems topological display, 1990
Internet SNMP-based recommended standards adoption, 1990
OSF Distributed Management Environment (DME) proposal and subsequent RFPs, 1990
SUN SunNet Manager, Unix-based workstation management solution, 1990
OSF DME technology candidates selection, 1991
ITU-T, ANSI T1M1, ETSI contribution toward TMN interface standardization, 1992
Commercial management platforms, HP OpenView, Digital DECmcc, Tivoli TME, 1992
IBM SystemView blueprint and SystemsView for AIX platform, 1993
Home-grown management platforms from NetLabs, Cabletron, OSI, 1994
OSF DME delivered as the network management option (NMO), a major failure, 1994
HP OperationsCenter and HP AdministrationCenter systems management platforms, 1994
NM Forum, SPIRIT management platform requirements, version 1, 1995
DSET provides stand-alone management applications and agent development tools, 1995

© 2001 by CRC Press LLC
Tivoli Systems TME acquired by IBM, integration plan with SystemsView for AIX, 1996
AT&T OneVision management platform-based integration solutions is proposed, 1996
HP OpenView DM 4.x, advanced distributed management versions, 1996-1997
Computer Associates Unicenter TNG management platforms, 1997
SUN Solstice Enterprise Manager platform and family of products, 1997
By the end of 1997 most of the surviving management platforms had become mature products although
they were still far from the ideals of fully open, distributed management platforms. Evidently, the biggest
failure was the standardization of management platform framework, as a collection of interchangeable
management services. Few management platforms are designed and built according to the advanced
features of object-oriented information modeling.
With all of these shortcomings the management platforms are the best hope to build enterprise-wide
management systems where integration of management solutions and scalability are two major issues.
3.1.7.3
Management Platforms for Enterprise-wide Management
In the previous sections we indicated the complexity and difﬁculties confronting the management ﬁeld.
We also indicated the shortcomings in the historical development of management systems. Management
platforms do not solve these shortcomings overnight but they bring a ﬂexible approach to the manage-
ment of multivendor, multitechnology, multiprotocol networks and systems environments. This is why
a close look at the design of management platforms is necessary.
Management platforms, either as autonomous or interworking management systems, should provide
several basic management functions. First of all, a management platform has to communicate with the
external world through platform external interfaces, i.e., it has to provide communications services. Next,
management information is exchanged in the form of management events that have to be stored, processed,
and named. Therefore, there is a need for event management services. Furthermore, the events, as related
to the management of network or computing systems resources, should be displayed (after the necessary
processing) by using graphical user interfaces. Management information and all the components associated
with management are organized using managed object models. Therefore, there is a need for a service
that provides manipulation of managed objects. Ultimately, the management information about network
conﬁgurations and managed resource status and parameters has to be stored in databases. Such database
service may allow near real-time presentation of the status of the managed systems and components.
Additional management operation services are needed to support all of the other platform services. In
addition to these management services, there is a need for other distributed management services such as
time service (synchronization), directory service (naming), and security services (Figure 3.1.20).
User interface services provide support for presentation of management information and support for
interactions between users/human operators and distributed management applications used to manage
network and computing systems. These services support both graphical user interfaces (GUIs) and
asynchronous command line interfaces, by providing network and computing systems layout display
based on visual icons, windows environment manipulation, on-line information, and general support
for common applications development. The user interface is the most visible point of integration between
various platform components and management applications.
Event management services provide common services to other platform management services and to
the management applications running on top of management platforms. The events can be generated
by network/computing systems, components state changes, systems errors, applications, and by
users/operators. Common event operations include event collection, event logging, event ﬁltering, and
event administration.
Management communications services, either object-based or message-based, provide support for
communications interfaces, management protocols, and communications stacks used to carry manage-
ment information. Primarily, this support targets standardized management protocols such as Simple
Network Management Protocol (SNMP), Common Management Information Protocol (CMIP) and
Services (CMIS), and Remote Procedure Calls (RPCs).

© 2001 by CRC Press LLC
Object manipulation services provide support for information exchange between objects as abstrac-
tions of physical and logical resources ranging from network devices and computing systems resources
to applications and management services. Primarily, this support targets object interfaces as deﬁned by
the OMG Common Object Request Broker Architecture (CORBA) and OMG Interface Deﬁnition Lan-
guage (IDL). Object manipulation includes operations on the MIB, object support services providing
location transparency for objects exchanging requests and responses, persistent storage for MIBs, and
support for object-oriented applications development.
Database services provide support for management data storage and retrieval along with its integration
with various platform services and management applications. Management information can include
dynamic instance information related to conﬁguration events, fault events, or performance events, to
historical and archive information for security audit trail. The database services include database man-
agement systems (DBMSs), standardized database access and retrieval mechanisms such as structured
query language (SQL), database concurrence mechanisms, and data backup mechanisms.
Management operations support provides common services to the management platform core and to
the management applications running on top of the platforms. It includes management of the background
processes associated with the platform hardware and operating system, and the handling of management
applications.
Regarding management applications running on top of the management platforms, they can be classiﬁed
in two major groups: core management applications and resource-speciﬁc applications. Core applications
include functional management applications to manage speciﬁc management functional areas (SMFAs)
such as conﬁguration, fault, performance, security, and accounting management. Core applications also
include compound applications which provide cross-area functionalities, as required by user needs and
business considerations (e.g., trouble ticketing). Resource-speciﬁc applications, as the name indicates, are
built speciﬁcally to manage particular network devices or computing system components.
The management applications development environment mirrors the run-time environment and
includes speciﬁc development tools for user interfaces, event management, communications, object manip-
ulation, and database operations. The APIs for various platform management services are critical compo-
nents in the development environment. The development environment includes the run-time environment
since the newly developed management applications are tested against the run-time environment.
The management implementation environment mainly refers to the overall acceptance testing of
management platforms as they are used in the management of real network and computing systems.
Implementation testing is different from the testing done on various platform hardware and software
FIGURE 3.1.20
Management platform architectural components.

© 2001 by CRC Press LLC
components during their development and production. The implementation environment covers effec-
tive systems testing based on test criteria, test procedures, and test tools used to operate, maintain, and
troubleshoot complex distributed systems, which are tailored to management platforms.
As we mentioned earlier, the management platform consists of management services and the actual
management applications that run on top of management services. Both management services and
management applications make use of the computing hardware and operating system. Although the
design of the hardware and operating system is outside the scope of management platform design, it is
important to understand the differences between various computing systems and to understand the
alternatives in selecting hardware and operating systems. In a truly open system environment, the
platform services and applications are supposed to be hardware and operating system independent.
3.2
Management of Emerged and Emerging Technologies
Kornel Terplan
3.2.1
Introduction
Telecommunications services offered by domestic and international providers are based on a mixture of
emerged and emerging technologies. Emerged technologies include private leased lines on T/E-basis,
ISDN, traditional voice networks, message switching, packet switching, and SS7-based signalling. Emerg-
ing technologies are the following: frame relay, FDDI, ATM, SDH/Sonet, SMDS, wireless, cable, and xDSL.
Due to rapid progress made in technologies and infrastructures, the number of choices to offer certain
telecommunications services is continuously increasing. In layered communications structures, technol-
ogy usually occupies the lower layers. It is true for both TMN and OSI layers. If IP-based services are
the goal, this service can be deployed in various ways. The supporting lower-layer infrastructure is going
to be reassessed. Besides the traditional IP-ATM-Sonet/SDH combination, other options are also under
consideration. This alternative is conservative, less risky from the engineering point of view, and achiev-
able now. It may, however, lead to low efﬁciency and to high costs. Enhanced frame relay may substitute
ATM everywhere, offering lower costs at good quality of service (QoS). But, in certain areas, there are
no QoS standards available. ATM transport may eliminate the Sonet/SDH-layer offering ATM ring
functions similar to distributed ATM switches. There are just a few vendors who consider this option.
ATM/IP hybrids on the basis of Sonet/SDH would reduce the number of routers, with the result of lower
management expenses. This technology is in the test phase, still unproven. IP over Sonet/SDH eliminates
the ATM layer completely. If megarouters are at cost parity with ATM switches, this alternative would
be the low-cost IP delivery solution. This technology is unproven; operating costs of megarouters are
difﬁcult to predict. Optical IP would be the lowest cost delivery of IP services. In this case, IP is directly
connected to the optical subnetwork of Layer 1, neither using ATM nor Sonet/SDH. It is the least proven
technology; there are serious concerns about fault management with this alternative.
This contribution investigates the manageability of both emerged and emerging technologies. Each
technology discussed will be introduced brieﬂy without details. Emphasis is on the availability of manage-
ment information bases (MIBs), management protocols used, and management products usually deployed.
3.2.2
Foundation Concepts for Networking Technologies
The majority of emerged and emerging technologies has a few basic foundation principles. These will
be addressed in this segment. The basics for this segment can be found in more details in (BLAC94) and
(TERP98).
3.2.2.1
Connection-oriented and Connectionless Communications
Communication systems that employ the concepts of circuits and virtual circuits are said to be connec-
tion-oriented. Such systems maintain information about the users, such as their addresses and their

© 2001 by CRC Press LLC
ongoing QOS needs. Often, these types of systems use state tables that contain rules governing the manner
in which the user interacts with the network. While these state tables clarify the procedures between the
user and the communication network, they do add overhead to the communication process.
In contrast, communication systems that do not employ circuits and virtual circuits are said to be
connectionless systems. They are also known as datagram networks and are widely used throughout the
industry. The principal difference between connection-oriented and connectionless operation is that
connectionless protocols do not establish a virtual circuit for the end user communication process.
Instead, trafﬁc is presented to the service provider in a somewhat ad hoc fashion. Handshaking arrange-
ments, mutual conﬁrmations are minimal and perhaps nonexistent. The network service points and the
network switches maintain no ongoing knowledge about the trafﬁc between the two end users. State
tables as seen with connection-oriented solutions are not maintained. Therefore, datagram services
provide no a priori knowledge of user trafﬁc and they provide no ongoing current knowledge of the user
trafﬁc — but they introduce less overhead.
3.2.2.2
Physical and Virtual Circuits
End users operating terminals, computers, and client equipment communicate with each other through
a communication channel called the physical circuit. These physical circuits are also known by other
names, such as channels, links, lines, and trunks. Physical circuits can be conﬁgured wherein two users
communicate directly with each other through one circuit, and no one uses this circuit except these two
users. They can operate this circuit in half-duplex or full-duplex. This circuit is dedicated to the users.
This concept is still widely used in simple networks without serious bandwidth limitations.
In more complex systems, such as networks, circuits are shared with more than one user-pair. Within
a network, the physical circuits are terminated at intermediate points at machines that provide relay
services on another circuit. These machines are known by such names as switches, routers, bridges,
gateways, etc. They are responsible for relaying the trafﬁc between the communicating users. Since many
communication channels have the capacity to support more than one user session, the network device,
such as the switch, router, or multiplexer is responsible for sending and receiving multiple user trafﬁc
to/from a circuit.
In an ideal arrangement, a user is not aware that the physical circuits are being shared by other users.
Indeed, the circuit provider attempts to make this sharing operating transparent to all users. Moreover,
in this ideal situation, the user thinks that the circuit directly connects only the two communicating
parties. However, it is likely that the physical circuit is being shared by other users.
The term “virtual circuit” is used to describe a shared circuit wherein the sharing is not known to the
circuit users. The term was derived from computer architectures in which an end user perceives that a
computer has more memory than actually exists. This other, additional virtual memory is actually stored
on an external storage device.
There are three types of virtual circuits:
• Permanent virtual circuits (PVC) — A virtual circuit may be provisioned to the user on a
continuous basis. In this case, the user has the service of the network any time. A PVC is established
by creating entries in tables in the network nodes’ databases. These entries contain a unique
identiﬁer of the user payload which is known by various names, such as a logical channel number
(LCN), virtual channel identiﬁer (VCI), or virtual path identiﬁer (VPI).
• Network features such as throughput, delay, security, and performance indicators are also provi-
sioned before the user starts with operations. If different types of services are desired, and if
different destination endpoints must be reached, then the user must submit a different PVC
identiﬁer with the appropriate user payload to the network. This PVC is provisioned to the different
endpoint, and perhaps with different services.
• Switched virtual circuits (SVC) — A switched virtual circuit is not preprovisioned. When a user
wishes to obtain network services to communicate with another user, it must submit a connection

© 2001 by CRC Press LLC
request packet to the network. It must provide the address of the receiver, and it must also contain
the virtual circuit number that is to be used during the session. SVCs entail some delay during
the setup phase, but they are ﬂexible in allowing the user to select dynamically the receiving party
and the negotiation of networking parameters on a call-by-call basis.
• Semi-permanent virtual circuits (SPVC) — With this approach, a user is preprovisioned, as in
a regular PVC. Like a PVC, the network node contains information about the communicating
parties and the type of services desired. But these types of virtual circuits do not guarantee that
the users will obtain their level of requested service. In case of congested networks, users could
be denied the service.
In a more likely scenario, the continuation of a service is denied because the user has violated some
rules of the communications. Examples are higher bandwidth demand and higher data rates than agreed
with the supplier.
3.2.2.3
Switching Technologies
Voice, video, and data signals are relayed in a network from one user to another through switches. This
section provides an overview on prevalent switching technologies.
Circuit switching provides a direct connection between two networking components. Thus, the com-
municating partners can utilize the facility as they see it — within bandwidth and tariff limitations. Many
telephone networks use circuit switching systems. Circuit switching provides clear channels; error check-
ing, session establishment, frame ﬂow control, frame formatting, selection of codes, and protocols are
the responsibility of the users. Today, the trafﬁc between communicating parties is usually stored in fast
queues in the switch and switched on to an appropriate output line with time division multiplexing
(TDM) techniques. This technique is known as circuit emulation switching (CES). In summary:
• Direct connection end-to-end
• No intermediate storage unless CES used
• Few value-added functions
• Modern systems use TDM to emulate circuit switching
Message switching was the dominating switching technology during the last two decades. The tech-
nology is still widely used in certain applications, such as electronic mail, but it is not employed in a
backbone network. The switch is usually a specialized computer. It is responsible for accepting trafﬁc
from attached terminals and computers. It examines the address in the header of the message and switches
the trafﬁc to the receiving station. Due to the low number of switching computers, this technology suffers
under backup problems, performance bottlenecks, and lost messages due to congestion. In summary:
• Use of store-end-forward technology
• Disk serves as buffers
• Extensive value-added functions
• Star topology due to expense of switches
Packet switching relays small pieces of user information to the destination nodes. Packet switching
has become the prevalent switching technology of data communications networks. It is used in such
diverse systems as private branch exchanges (PBXs), LANs, and even with multiplexers. Each packet only
occupies a transmission line for the duration of the transmission; the lines are usually fully shared with
other applications. This is an ideal technology for bursty trafﬁc. Modern packet switching systems are
designed to carry continuous, high-volume trafﬁc as well as asynchronous, low-volume trafﬁc, and each
user is given an adequate bandwidth to meet service level expectations.
The concept of packet and cell switching is similar; each attempts to process the trafﬁc in memory as
quickly as possible. But cell switching is using much smaller PDUs relative to packet switching. The PDU
size is ﬁxed with cell switching. The PDU size may vary with packet switching. In summary:

© 2001 by CRC Press LLC
• Hold-and-forward technology
• RAM serves as buffers
• Extensive value-added-functions for packet, but not many for cells
Switching will remain one of the dominating technologies in the telecommunications industry.
3.2.2.4
Routing Technologies
There are two techniques to route trafﬁc within and between networks: source routing and non-source
routing. The majority of emerging technologies use non-source routing.
Source routing derives its name from the fact that the transmitting device — the source — dictates
the route of the protocol data unit (PDU) through a network or networks. The source places the addresses
of the “hops” in the PDU. The hops are actually routers representing the internetworking units. Such an
approach means that the internetworking units need not perform address maintenance, but they simply
use an address in the PDU to determine the destination of the PDU.
In contrast, non-source routing requires that the interconnecting devices make decisions about the route.
They don’t rely on the PDU to contain information about the route. Non-source routing is usually associated
with bridges and is quite prevalent in LANs. Most of the emerging new technologies implement this approach
with the use of a VCI. This label is used by the network nodes to determine where to route the trafﬁc.
The manner in which a network stores its routing information varies. Typically, routing information
is stored in a software table, called a directory. This table contains a list of destination nodes. These
destination nodes are identiﬁers with some type of network address. Along with the network address (or
some type of label, such as a virtual circuit identiﬁer) there is an entry describing how the router is to
relay the trafﬁc. In most implementations, this entry simply lists the next node that is to receive the
trafﬁc in order to relay it to its destination.
Small networks typically provide a full routing table at each routing node. For large networks, full
directories require too many entries, and are expensive to maintain. In addition, the exchange of routing
table information can impact the available bandwidth for user payload. These networks are usually
subdivided into areas, called domains. Directories of routing information are kept separately in domains.
Broadcast networks contain no routing directories. Their approach is to send the trafﬁc to all destinations.
Network routing control is usually categorized as centralized or distributed. Centralized uses a network
control center to determine the routing of the packets. The packet switches are limited in their functions.
Central control is vulnerable; a backup is absolutely necessary, which increases the operating expenses.
Distributed requires more intelligent switches, but they provide a more resilient solution. Each router
makes its own routing decisions without regard to a centralized control center. Distributed routing is
also more complex, but its advantages over the centralized approach have made it the preferred routing
method in most communications networks.
3.2.2.5
Multiplexing Technologies
Most of the emerged and emerging technologies use some form of multiplexing. Multiplexers accept low-
speed voice or data signals from terminals, telephones, PCs, and user applications and combine them into
one higher-speed stream for transmission efﬁency. A receiving multiplexer demultiplexes and converts the
combined stream into the original lower-speed signals. There are various multiplexing techniques:
Frequency Division Multiplexing (FDM) — This approach divides the transmission frequency range
into channels. The channels are lower frequency bands; each is capable of carrying communication
trafﬁc, such as voice, data, or video. FDM is widely used in telephone systems, radio systems, and
cable television applications. It is also used in microwave and satellite carrier systems. FDM
decreases the total bandwidth available to each user, but even the narrower bandwidth is usually
sufﬁcient for the users’ applications. Isolating the bands from each other costs some bandwidth,
but the simultaneous use outweighs this disadvantage.
Time Division Multiplexing (TDM) — This approach provides the full bandwidth to the user or
application, but divides the channel into time slots. Each user or application is given a slot and

© 2001 by CRC Press LLC
the slots are rotated among the attached devices. The TDM multiplexer cyclically scans the input
signals from the entry points. TDMs are working digitally. The slots are preassigned to users and
applications. In case of no trafﬁc at the entry points, the slots remain empty. This approach works
well for constant bit rate applications, but leads to waste capacity for variable bit rate applications.
Statistical Time Division Multiplexing (STDM) — This approach allocates the time slots to each port
on a STDM. Consequently, idle termninal time does not waste the capacity of the bandwidth. It
is not unusual for two to ﬁve times as much trafﬁc to be accomodated on lines using STDMs in
comparison to a TDM solution. This approach can accomodate bursty trafﬁc very well, but does
not perform too well with continuous, nonbursty trafﬁc.
Wave Division Multiplexing (WDM) — WDM is the optical equivalent of FDM. Lasers operating at
different frequencies are used in the same ﬁber, thereby deriving multiple communications chan-
nels from one physical path. There is a more advanced form of this technology with even better
efﬁciency, called Dense Wave Division Multiplexing (DWDM).
3.2.2.6
Addressing and Identiﬁcation Schemes
In order for user trafﬁc to be sent to the proper destination, it must be associated with an identiﬁer of
the destination. Usually, there are two techniques in use:
An explicit address has a location associated with it. It may not refer to a speciﬁc geographical location
but rather a name of a network or a device attached to a network. For example, the Internet Protocol
(IP) address has a structure that permits the identiﬁcation of a network, a subnetwork attached to the
network, and a host device attached to the subnetwork. The ITU-T X.121 address has a structure which
identiﬁes the country, a network within that country, and a device within the network. Other entries are
used with these addresses to identify protocols and applications running on the networks. Explicit
addresses are used by switches, routers, and bridges as an entry into routing tables. These routing tables
contain information about how to route the trafﬁc to the destination nodes.
Another identifying scheme is known by the term of label, although other terms may be more widely
used. Those terms are logical channel number (LCN) or virtual circuit identiﬁer (VCI). A label contains
no information about network identiﬁers or physical locations. It is simply a value that is assigned which
identiﬁes each data unit of a user’s trafﬁc.
Almost all connectionless systems use explicit addresses, and the destination and source addresses
must be provided with every PDU in order for it to be routed to the proper destination.
3.2.2.7
Control and Congestion Management
It is very important in communication networks to control the trafﬁc at the ingress and egress points of
the network. The operation by which user trafﬁc is controlled by the network is called ﬂow control. Flow
control should assure that the trafﬁc does not saturate the network or exceed the network’s capacity.
Thus, ﬂow control is used to manage congestion.
There are three ﬂow control alternatives with emerged and emerging technologies:
• Explicit ﬂow control — This technique limits how much user trafﬁc can enter the network. If the
network issues an explicit ﬂow control message to the user, the user has no choice but to stop
sending trafﬁc or to reduce trafﬁc. Trafﬁc can be sent again after the network has notiﬁed the user
about the release of the limitations.
• Implicit ﬂow control — This technique does not absolutely restict the ﬂow. Rather, it recommends
that the user reduce or stop trafﬁc it is sending to the network if network capacity situations require
a limitation. Typically, the implicit ﬂow control message is a warning to the user that the user is
violating its service level agreement with the internal or external supplier regarding network conges-
tion. In any case, if the user continues to send trafﬁc, it risks having trafﬁc discarded by the network.
• No ﬂow control — Flow control may also be established by not controlling the ﬂow at all. Generally,
an absence of ﬂow control means that the network can discard any trafﬁc that is creating problems.
While this approach certainly provides superior congestion management from the standpoint of
the network, it may not meet the performance expectations of the users.

© 2001 by CRC Press LLC
3.2.3
Management Solutions for Emerged Technologies
The present status for emerged technologies, such as private leased lines, voice networks, SS7-based
signalling techniques, message and packet switching, can be summarized as follows:
• Proprietary solutions dominate: this means that the management protocols selected are controlled
by the supplier of the equipment or facilities vendors.
• Re-engineered by SNMP: many of the equipment vendors include SNMP agents into their devices
to meet the requirements of customers. The SNMP agents provide information for performance
management and reporting, but they usually do not change the real-time processing of status data
within the devices.
• The management structures are very heterogeneous: in most cases, these structures are hierarchical
including a manager of managers. This manager is using a proprietary architecture. Most of the
interfaces to element managers and managed devices are proprietary.
• TMN has a very low penetration: suppliers have recognized the need for a generic standard, but
they are not willing to invest heavily into supporting it. Some of the providers go as supporting
the Q3-interface.
• Operating Support Systems are heavy: the legacy-type OSSs support the emerged technologies on
behalf of the suppliers well, but they are ﬂexible enough to address future needs. They lack in
separating operations functionality from operations data, in using ﬂexible software, and in sepa-
rating network management from service management.
3.2.4
Emerging Technologies
This segment gives an overview on telecommunication technologies that are either in use but still
considered new technology, or are considered for near-future implementation. These technologies include
frame relay, FDDI, SMDS, ATM, Sonet/SDH, and mobile and wireless communications. The capabilities
of these technologies are presented using the same format, including technology description ﬂowed by
evaluating management capabilities.
3.2.4.1
Frame Relay
The purpose of a frame relay network is to provide an end user with a high-speed virtual private network
(VPN) capable of supporting applications with large bit-rate transmission requirements. It gives a user
T1/E1 access rates at a lesser cost than can be obtained by leasing comparable T1/E1 lines. It is actually
a virtually meshed network.
The design of frame relay networks is based on the fact that data transmission systems today are
experiencing far fewer errors and problems than they did decades ago. During that period, protocols
were developed and implemented to cope with error-prone transmission circuits. However, with the
increased use of optical ﬁbers, protocols that expend resources dealing with errors become less important.
Frame relay takes advantage of this improved reliability by eliminating many of the now unnecessary
error checking and correction, editing, and retransmission features that have been part of many data
networks for almost two decades.
Frame relay has been working for many years. It represents a scaled-down version of LAPD. The
ﬂexibility of assigning bandwidth on demand is somewhat new. Frame relay is one of the alternatives of
fast packet switching.
MIB availability
The frame relay objects are organized into three object groups:
• Data link connection management interface group
• Circuit group
• Error group

© 2001 by CRC Press LLC
These groups are stored in tables in the MIB and can be accessed by the SNMP manager. Figure 3.2.1
illustrates where SNMP operates, and lists the names of these three groups.
The frDlcmiTable contains 10 objects. Their purpose is to identify each physical port at the uniﬁed
network interface (UNI), its IP address, the size of the DLCI header that is used on this interface, timers
for invoking status and status inquiry messages, the maximum number of DLCIs supported at the
interface, whether or not the interface uses multicasting, and miscellaneous operations.
The frCircuitTable contains 14 objects. Their purpose is to identify each PVC, its associated DLCI, if
the DLCI is active, the number of BECNs and FECNs received since the PVC was created, statistics on
the number of frames and octets sent and received since the DLCI was created, the DLCI’s Bc and Be,
and miscellaneous operations.
The third table is the frErrTable containing 4 objects. Their purpose is to store information on the
types of errors that have occurred at the DLCI (unknown or illegal), and the time the error was detected.
One object contains the header of the frame that created the error.
SNMP-based and proprietary solutions compete for management. Basically, each physical and logical
component can be managed by periodically polling the PDUs in the MIB. Any powerful management
platform can accomodate frame relay management, but the polling overhead over the wide area should
be carefully controlled.
3.2.4.2
Fiber Distributed Data Interface (FDDI)
FDDI was developed to support high-capacity LANs. To obtain this goal, the original FDDI speciﬁcations
stipulated the use of optical ﬁber as the transport media, although it is now available on twisted pair
cable (CDDI). FDDI has been deployed in many corporations to serve as a high-speed backbone network
for other LANs, such as Ethernet and Token Ring.
Basically, the standard operates with 100 Mbps rate. Dual rings are provided for the LAN, so the full
speed is actually 200 Mbps, althrough the second ring is used typically as a backup to the primary ring.
In practice. most installations have not been able to utilize the full bandwidth of FDDI. The standard
deﬁnes multimode optical ﬁber, although single mode optical ﬁber can be used as well.
FIGURE 3.2.1
Communications paths between manager, agents, subagents, and managed objects.
FRS
FRS
R
FRS
Router MIB
R
FRS
Router MIB
FRS
Frame Relay Switch
R
Router
MIB
Management Information Base
FRS - MIB
fr dlcmiTable
fr CircuitTable
frerrTable

© 2001 by CRC Press LLC
FDDI was designed to make transmission faster on an optical ﬁber transport. Due to the high-capacity
100 Mbps technology, FDDI has a tenfold increase over the widely used Ethernet, and a substantial
increase over Token Ring. FDDI was also to extend the distance of LAN interconnectivity. It permits the
network topology to extend up to 200 km (14 miles).
FDDI II, which is able to incorporate voice, is not receiving enough industry interest.
FDDI is actually not a new technology, but internetworked FDDIs offer new alternatives in metropol-
itan areas competing with other technologies, such as frame relay and SMDS.
MIB Availability
The FDDI MIB has the following ﬁve groups (Figure 3.2.2):
• SMT Group — Contains a sequence of entries, one for each SMT implementation. The entries
describe the station I.D., the operation I.D., the highest and lowest version I.D., the number of
MACs in this station or concentrator, and other conﬁguration and status information.
• MAC Group — A list of MAC tables, one for each MAC implementation across all SMTs. Each
table describes the SMT index and the MIB-II If Index associated with this MAC, and status and
conﬁguration information for the given MAC.
• Enhanced MAC Counters Group — The MAC Counters table contains a sequence of MAC
Counters entries. Each entry stores information about the number of tokens received, number of
times TVX expired, number of received frames that have not been copied to the receive buffer,
number of TRT expirations, number of times the ring entered operational status, and threshold
information.
• PATH Group — Contains a sequence of PATH entries. Each entry starts with an index variable
uniquely identifying the primary, secondary, and local PATH object instances. The entries also
store information on different min and max time values for MACs in the given PATH.
FIGURE 3.2.2
FDDI management information base.

© 2001 by CRC Press LLC
• PATH Conﬁguration Table — A table of path conﬁguration entries. This table contains a conﬁg-
uration entry for all the resources that may be in this path.
• PORT Group — Contains a list of PORT entries. Each entry describes the SMT index associated
with this port, a unique index for each port within the given SMT, the PORT type, and then
neighbor type, connection policies, the list of permitted PATHs, the MAC number associated with
the PORT (if any), the PMD entity class associated with this port and other capabilities and
conﬁguration information.
FDDI has its own management capabilities, deﬁned in SMT, but they have never really taken off.
Instead, suppliers are concentrating on SNMP capabilities. Using the MIB of FDDI agents, any SNMP
manager can be used to manage FDDI.
3.2.4.3
Switched Multi-megabit Data Service
SMDS is a high-speed connectionless packet switching service which extends LAN-like performance
beyond a subscriber’s location. Its purpose is to ease the geographic limitations that exist with low-speed
wide area networks. SMDS is designed to connect LANs, MANs, and also WANs.
The major goals of SMDS are to provide high-speed interfaces into customer systems, and at the same
time allow customers to take advantage of their current equipment and hardware. Therefore, the SMDS
operations are not performed by the end user machine; they are performed by customer promises
equipment (CPE), such as a router.
SMDS is positioned as a service. If SMDS is considered unto itself, it is a new technology; it offers no
new method for designing or building networks. This statement is emphasized because SMDS uses the
technology of DQDB (dual queue dual bus), and then offers a variety of value-added services, such as
bandwidth on demand, to the SMDS customer.
SMDS is targeted for large customers and sophisticated applications that need a lot of bandwidth, but
not permanently. Generally, SMDS is targeted for data applications that transfer a lot of information in
a bursty manner.
However, applications that use SMDS can be interactive. For example, two applications can exchange
information interacting through SMDS, such as an X-ray, a document, etc. The restriction of SMDS is
based on the fact that SMDS is not designed for real-time, full-motion video applications. Notwithstand-
ing, it does support an interactive dialog between users, and allows them to exchange large amounts of
information in a very short time. For example, it takes only one to two seconds for a high-quality color
graphic image to be sent over a SMDS network. For many applications, this speed is certainly adequate.
MIB Availability
Figure 3.2.3 shows the location of the MIB in relation to the SMDS network. The SIP layers are also
known in this ﬁgure to aid in reading the following material. The MIB is organized around managed
objects which are contained in major groups. The groups, in turn, are deﬁned in tables. As shown in the
ﬁgure, at the bottom, the major entries in the MIB are the SMDS address, which is the conventional
60-bit address preceded by the 4 bits to signify individual group addresses. Thereafter, the groups are
listed with their object identiﬁer name. These names will be used in this segment to further describe the
entries.
The sipL3Table contains the layer 3 (L3-PDU) parameters used to manage each SIP port. It contains
entries such as port numbers, statistics on received trafﬁc, information on errors such as unrecognized
addresses, as well as various errors that have occurred at this interface.
The sipL2Table, as its name implies, contains information about layer 2 (L2-PDU) parameters and
the state variables associated with information on the amount of the number of level 2 PDUs processed,
error information such as violation of length of PDU, sequence number errors, MID errors, etc.
The sipDS1PLCP Table contains information on DS1 parameters and state variables for each port.
The entries in the table contain error information such as DS1 severely erred framing seconds (SEFS),
alarms, unavailable seconds encountered by the PLPC, etc.

© 2001 by CRC Press LLC
The sipDS3PLPC Table contains information about DS3 interfaces and state variables for each SIP
port. Like its counterpart in DS1, this table contains information on severity-erred framing seconds,
alarms, unavailable seconds, etc.
The ipOverSMDS Table contains information relating to operations of IP running on top of SMDS.
It contains information such as the IP address, the SMDS address relevant to the IP station, addresses
for ARP resolution, etc.
The smdsCarrierSelection group contains information on the inter-exchange carrier selection for the
transport of trafﬁc between LATAs.
Finally, the sipL3PDU Error Table contains information about errors encountered in processing the
layer 3 PDU. Entries such as destination error, source error, invalid BAsize, invalid header extension,
invalid PAD error, BEtag mismatch, etc. form the basis for this group.
As mentioned earlier, SNMP is used to monitor the MIBs and report on alarm conditions that have
occurred based on the deﬁnitions in the MIBs.
SMDS management is expected to be ambitious. Switching systems are intelligent devices with the
need of real-time decision making. In such situations, SNMP is not necessarily the right choice. However,
SNMP may be used to transmit MIB entries to the manager for performance reporting.
3.2.4.4
Asynchronous Transfer Mode
The purpose of ATM is to provide a high-seed, low-delay, multiplexing and switching network to support
any type of user trafﬁc, such as voice, data, or video applications. ATM is one of four fast relay services.
ATM segments and multiplexes user trafﬁc into small, ﬁxed-length units called cells. The cell is 53 octets,
with 5 octets reserved for the cell header. Each cell is identiﬁed with virtual circuit identiﬁers that are
contained in the cell header. An ATM network uses these identiﬁers to relay the trafﬁc through high-
speed switches from the sending CPE to the receiving CPE.
ATM provides limited error detection operations. It provides no retransmission services, and few
operations are performed on the small header. The intention of this approach — small cells and with
FIGURE 3.2.3
Structure of SNMP-based management services.
SMDS - MIB
SMDS Address
sipL3Table
sipL2Table
sipDSIPLCPTable
sipDS3PLCPTable
ipOverSMDSTable
simds-CarrierSelection
sip-L3PDUErrorTable
R
R
R
Switching
System
Switching
System
Switching
System
R
MIB
Router
Management Information Base

© 2001 by CRC Press LLC
minimal services performed — is to implement a network that is fast enough to support multimegabit
transfer rates.
ATM is a new technology. ATM is supposed to be the foundation of providing the convergence,
multiplexing, and switching operations. ATM resides on top of the physical layer.
MIB Availability
The ATM Forum has published a MIB as part of its Interim Local Management Interface Speciﬁcation
(ILMI) (Figure 3.2.4). The ATM MIB is registered under the enterprise node of the standard SMI in
accordance with the Internet. MIB objects are therefore preﬁxed with 1.3.6.1.4.1.353.
Each physical link (port) at the UNI has a MIB entry that is deﬁned in the atmfPortTable. This table
contains a unique value for each port, an address, the type of port (DS3, SONET, etc.) media type (coaxial
cable, ﬁber, etc.), status of port, (in service, out of service, etc.) and other miscellaneous objects.
The atmfAtmLayerTable contains information about the UNIs physical interface. The table contains:
the port id, maximum number of VCCs, VPCs supported and conﬁgured on this UNI, active VCI/VPI
bits on the UNI, and a description of public or private for the UNI.
The atmVpcTable and atmVccTable contain similar entries for the VPCs and VCCs, respectively, on
the UNI. These tables contain the port id, VPI or VCI values for each connection, operational status (up,
down, etc.), trafﬁc shaping and policing descriptors (to describe the type of trafﬁc management applicable
to the trafﬁc), and any applicable QoS that is applicable to the VPI or VCI.
The ATM Forum has deﬁned two aspects of UNI network management:
• ATM layer management at the M plane, and
• Interim Local Management Interface (ILMI) speciﬁcation.
M-Plane Management — Most of the functions for ATM M-plane management are performed with
the SONET F1, F2, and F3 information ﬂows. ATM is concerned with F3 and F4 information ﬂows.
ILMI — Because the ITU-T and the ANSI have focused on C-plane and U-plane procedures, the ATM
Forum has published a interim speﬁcication called ILMI. The major aspects of ILMI are the use of SNMP
FIGURE 3.2.4
The ATM management information base.

© 2001 by CRC Press LLC
and a MIB. The ILMI stipulates the following procedures. First, each ATM device supports the ILMI,
and one UNI ILMI MIB instance for each UNI. The ILMI communication protocol stack can be
SNMP/UDP/IP/AAL over a well-known VPI/VCI value. SNMP is employed to monitor ATM trafﬁc and
the UNI VCC/VPC connections based on the ATM MIB with the SNMP get, Get-Next, Set, and Trap
operations.
3.2.4.5
Sonet and SDH
Sonet/SDH is an optical-based carrier (transport) network utilizing synchronous operations between the
network components. The term Sonet is used in North America, and SDH is used in Europa and Japan.
Attributes of this technology are:
• A transport technology that provides high availability with self-healing topologies
• A multivendor that allows multivendor connections without conversions between the vendors’
systems
• A network that uses synchronous operations with powerful multiplexing and demultiplexing
capabilities
• A system that provides extensive OAM&P services to the network user and administrator
Sonet/SDH provides a number of attractive features when compared with current technology. First,
it is an integrated network standard on which all types of trafﬁc can be transported. Second, the
Sonet/SDH standard is based on the optical ﬁber technology which provides superior performance in
comparison to microwave and cable systems. Third, because Sonet/SDH is a worldwide standard, it is
now possible for different vendors to interface their equipment without conversion.
Fourth, Sonet/SDH efﬁciently combines, consolidates, and segregates trafﬁc from different locations
through one facility. This concept, known as grooming, eliminates back hauling and other inefﬁcient
techniques currently being used in carrier networks. Back hauling is a technique in which user payload is
carried past a switch that has a line to the user and sent to another endpoint. Then, the trafﬁc to the other
user is dropped, and the ﬁrst users’ payload is sent back to the switch and relayed back to the ﬁrst user.
In present conﬁgurations, grooming is eliminated, but expensive conﬁgurations, such as back-to-back
multiplexers that are connected with cables, panels, or electronic cross-connect equipment are required.
Fifth, Sonet/SDH eliminates back-to-back multiplexing overhead by using new techniques in the
grooming process. These techniques are implemented in a new type of equipment, called an add-drop
multiplexer (ADM).
Sixth, the synchronous aspect of Sonet/SDH makes for more stable network operations. These types
of networks experience fewer errors than the older asynchronous networks, and provide much better
techniques for multiplexing and grooming payloads.
Seventh, Sonet/SDH has notably improved OAM&P features relative to current technology. Approx-
imately 5% of the bandwidth is devoted to management and maintenance.
Eighth, Sonet/SDH employs digital transmission schemes. Thus, the trafﬁc is relatively immune to noise
and other impairments on the communications channel, and the system can use efﬁcient TDM operations.
Sonet have been around a couple of years. The technology is not completely new, but its implemen-
tation is new.
MIB Availability
The Sonet/SDH MIB consists of eight groups. Each of the following groups have two tables: the Current
Table and the Interval Table (Figure 3.2.5).
• The Sonet/SDH XXX Current Table contains various statistics that are being collected for the
current 15-minute interval. The Sonet/SDH XXX Interval Table contains various statistics being
collected by each system over a maximum of the previous 24 hours of operation. The past 24 hours
may be broken into 96 completed 15-minute intervals. A system is required to store at least
4 completed 15-minute intervals. The default value is 32 intervals.

© 2001 by CRC Press LLC
• The Sonet/SDH Medium Group: Sonet/SDH interfaces for some applications may be electrical
interfaces and not optical interfaces. This group handles the conﬁguration information for both
optical Sonet/SDH interfaces and electrical Sonet/SDH interfaces, such as signal type, line coding,
line type, and the like.
• The Sonet/SDH Section Group: This group consists of two tables:
The Sonet/SDH Section Current Table and
The Sonet/SDH Section Interval Table
These tables contain information on interface status, counters on errored seconds, severely
errored seconds, severely errored framing seconds, and coding violations.
• The Sonet/SDH Line Group: This group consists of two tables:
The Sonet/SDH Line Current Table and
The Sonet/SDH Line Interval Table
These tables contain information on line status, counters on errored seconds, severely errored
seconds, severely errored framing seconds, and unavailable seconds.
• The Sonet/SDH Far End Line Group: This group may only be implemented by Sonet/SDH (LTEs)
systems that provide for far end block error (FEBE) information at the Sonet/SDH Line Layer.
This group consists of two tables:
The Sonet/SDH Far End Line Table and
The Sonet/SDH Far End Line Interval Table
• The Sonet/SDH Path Group: This group consists of two tables:
The Sonet/SDH Path Current Table and
The Sonet/SDH Path Interval Table
These tables contain information on interface status, counters on errored seconds, severely
errored seconds, severely errored framing seconds, and coding violations.
FIGURE 3.2.5
The Sonet/SDH management information base.

© 2001 by CRC Press LLC
• The Sonet/SDH Far End Path Group: This group consists of two tables:
The Sonet/SDH Far End Path Current Table and
The Sonet/SDH Far End Path Interval Table
• The Sonet/SDH Virtual Tributary Group: This group consists of two tables:
The Sonet/SDH VT Current Table and
The Sonet/SDH VT Interval Table
For SDH signals, virtual tributaries are called Vcs instead of Vts.
VT1.5
VC11
VT2
VC12
VT3
none
VT6
VC3
These tables contain information on virtual tributaries width and status, counters on errored seconds,
severely errored seconds, severely errored framing seconds, and unavailable seconds.
• The Sonet/SDH Far End VT Group: This group consists of two tables:
The Sonet/SDH Far End VT Current Table and
The Sonet/SDH Far End VT Interval Table
The operation, administration, and maintenance (OAM) functions are associated with the hierarchical,
layered design of Sonet/SDH. Figure 3.2.6 shows the ﬁve levels of the corresponding OAM operations,
which are labeled F1, F2, F3, F4, and F5. F1, F2, and F3 functions reside at the physical layer; F4 and F5
functions reside at the ATM layer.
The Fn tags depict where the OAM information ﬂows between two points, as shown in Figure 3.2.7.
FIGURE 3.2.6
Relationships in ATM layers.

© 2001 by CRC Press LLC
James Anderson "Future Telecommunications: Trends and Directions"
The CRC Handbook of Modern Telecommunications
Ed. Patricia Morreale and Kornel Terplan
Boca Raton, CRC Press LLC. 2001

© 2001 by CRC Press LLC
4
Future
Telecommunications:
Trends and Directions
4.1 
Introduction .
4.2 
User Needs
Types of Users • Different Users Have Different Needs • End 
User Requirements Summary
4.3 
Application Trends
Application Functionality • Functionality Implementation
4.4 
Systems and Service Integration
Introduction • Drivers for Integration • Integration for 
Service Providers • Integration for Business Users • 
Integration for Mobile Professionals • Integration for SOHO 
Users • Integration for Residential Users
4.5 
New Product and Service Creation.
Introduction • Drivers and Constraints • New Service 
Creation • Increasing Bandwidth
4.6 
Telecommunications Tarifﬁng
Introduction • Regulatory Trends • Service Pricing Trends • 
Impact of New Technologies
4.7 
Telecommunications Strategies 
Introduction • The Players • Goals
4.1
Introduction
Imagine for a moment how daily life would be affected if the telecommunications services and applica-
tions that we take for granted were to be removed. The daily paper would contain mainly local news and
any international stories would be describing events that were weeks or months old. We would spend
much of our time during the week traveling from house to house and town to town as we tried to keep
in touch with our friends and business associates. We would tend to live close to where we were born
and raised otherwise we would risk losing contact with friends and family. Finally, the number of
envelopes, paper, and stamps sold would be constantly increasing as people wrote letters in order to have
their presence felt in far-off locations without having to travel.The contrast between our everyday life
and this example clearly shows just how signiﬁcant the impact of today’s telecommunications services
has been on how we communicate. As hard as it is to imagine a day without the communications systems
and services that have become such an integral part of our lives, so too will it be impossible for future
generations to imagine living in our times with our “primitive” telecommunications infrastructures and
applications!
James Anderson
Alcatel

© 2001 by CRC Press LLC
In this chapter we will be looking at where the ﬁeld of telecommunications is evolving to. This type
of prediction is not without a great deal of risk: a similar analysis done as recently as 1990 could not
have hoped to accurately identify the impact that the Internet now has on the way we communicate
today! However, the basic building blocks that will control the evolution of the ﬁeld of telecommunica-
tions, the telecommunications DNA if you will, are reﬂected in the state-of-the-art services, applications,
and equipment available today. We will look at the current trends along with the end-user requirements
and competitive market forces that will shape the future of telecommunications.
To help focus the consideration of such a large topic as the future of telecommunications, it is helpful
to have a model to frame the discussion. The model that we will use in this chapter to identify future
trends in telecommunications is shown in Figure 4.1.
This telecommunications trend lifecycle model that we will be using is intended to provide a high-
level view of how the effects of changes “ripple” throughout the telecommunications ﬁeld. We will be
discussing the model in a sequential manner, starting with an analysis of the changing needs of end users.
It is important to keep in mind that innovation and change in real life is often chaotic and seems to resist
following orderly models. Therefore, as long as we understand that a new telecommunications trend can
potentially start at any step of the trend lifecycle model (i.e., a new equipment technology is invented in
a research lab and only later is it understood well enough to be used to address end-user needs), then
we will be able to correlate this chapter’s analysis and the real world.
4.2
User Needs
The modern world is currently undergoing its third major communications transformation. It took
38 years for radio to garner 50 million listeners; likewise, it took 13 years for television to achieve a similar
number of viewers. Incredibly, the worldwide computer communications network known as the Internet
has required only 4 years to reach that milestone. In the U.S., as of this writing, there are more than
62 million Internet users and another 7 million are estimated to be joining them soon. These users will
be joining a worldwide community of over 100 million Internet users. As is to be expected, when more
people make use of the Internet, more information needs to be processed by the networks and computers
FIGURE 4.1
Telecommunications trend lifecycle model.
5. Telecommunications
Tariffing
4. New 
Product & Service
Creation
6. Telecommunications
Strategies
1. User
Needs
2. Application
Trends
3. System & Service
Integration

© 2001 by CRC Press LLC
that make up the Internet. The U.S. Commerce Department (as of April 15, 1998) estimates that the
amount of information processed over the Internet is doubling every 100 days. The needs of these users
and others like them will form the drivers of telecommunication trends in the future.
In this section we will examine the user needs that will form the basis — and demands of — tomorrow’s
telecommunications systems and applications (Figure 4.2). We will start by determining exactly what
types of users’ needs we have to understand. Next, we’ll explore the speciﬁc problems and challenges that
each group of users is currently trying to solve. Finally, we’ll identify several general trends in user needs
that will have the greatest impact on future telecommunications services.
4.2.1
Types of Users
It can be argued that almost everyone in industrialized countries could be considered to be an end user
of telecommunications services and applications. A recent study by the International Telecommunications
Union (ITU) standards body reported that in high-income countries (per capita GDP of more than U.S.
$8955) there exists a “teledensity” of more than 50 phone lines for every 100 people. This would lead
one to conclude that in these countries, telecommunications services and technologies will evolve to meet
the needs of the general public. However, in order to identify speciﬁc future trends in telecommunications,
we need to limit our focus to only those users who either have the ﬁnancial resources or sheer numbers
to generate and sustain a trend in telecommunications. We will also avoid focusing on narrow vertical
application segments such as healthcare and banking in order to identify trends because their inﬂuences
on future applications and services can be safely generalized into broader end-user groups without losing
their contribution. In this chapter, we will segment end users into four primary groups for further study.
These groups can be characterized in the following ways:
• Businesses: This segment of telecommunications end users is deﬁned to be a group working
toward a common goal at one or more locations. As a rule, businesses need to interconnect each
of their workers on a frequent basis. Depending on the size and type of business, this intercon-
nection requirement can result in the need for large amounts of bandwidth. The business segment
FIGURE 4.2
Trend analysis — user needs.
2. Application
Trends
3. System & Service
lntegration
4. New
Product & Service
Creation
5. Telecommunication
Tariffing
6. Telecommunications
Strategies
1. User
Needs

© 2001 by CRC Press LLC
is also characterized by its growing need for 7 days per week × 24 hours per day × 365 days per
year connectivity in order to support globally distributed operations. Businesses are fairly price
resistant — they are willing to pay more for access to applications that they feel will provide
enough of a competitive advantage to recover their costs.
• Mobile Professionals: These end users generally interact with business segment end users. The
difference between these segments is that mobile professionals generally operate either by them-
selves or as part of small focused teams. Mobile professionals don’t have a ﬁxed location connected
to telecommunications services; rather, they need to have services ﬁnd them or permit them to
access the services from a wide variety of remote locations. Once again, the mobile professional
segment is fairly price insensitive to the price of telecommunications services that have a direct
correlation to a competitive advantage.
• SOHO: The small ofﬁce/home ofﬁce (SOHO) segment is a rapidly growing portion of the market,
as larger businesses discover it is more economical to outsource many of the tasks they used to
perform internally. Tax incentives from many local and federal governments designed to decrease
commuting congestion and pollution have also added to the economic incentive for this segment
to experience explosive growth. Telecommunications applications have been crucial to fueling the
growth of this segment. Existing applications have permitted home ofﬁce workers to have access
to similar communications resources that centralized workers also enjoy. The SOHO segment is
price sensitive; however, their large numbers can often be used to create attractive business cases
for both the end users and the service providers.
• Residential: This segment of end users wants to have telecommunications services delivered to
their homes. The telecommunications applications desired by this segment often are used to
communicate with other residential end users, businesses, or for entertainment. This segment is
very price sensitive; in order to pay for a telecommunications application or service, something
else will have to be given up. Each application is subjected to a tradeoff evaluation by the end user.
4.2.2
Different Users Have Different Needs
Each of the different user groups we have identiﬁed is facing a different set of challenges that can be
addressed in a variety of ways by telecommunications services. In this section we will explore the
environmental and social drivers that have created these end-user needs. In the ﬁnal section of this chapter
we will identify the common drivers that apply to each segment of end users. As you read this section,
it is important to keep in mind that although the speciﬁc details of how end user problems will be
addressed may change over time, the core set of conditions that have created the needs will not change.
4.2.2.1
Business End-User Needs
Businesses exist to earn a proﬁt and they do this by offering some combination of better products, lower
prices, or by meeting the speciﬁc needs of a particular customer better than any other ﬁrm. For the
purposes of this discussion, we group together businesses of all sizes from the very small to the very
large. Although the speciﬁcs of the problems they are trying to solve may differ, all businesses face the
same basic set of challenges.
The communications needs of business end users can be divided into two basic groups: internal needs
and external needs. A business’s internal needs relate to how it communicates the way that it wants to
do business to its employees and how those employees communicate status and learned information
throughout the ﬁrm. The external communication needs of a ﬁrm relate to how it exchanges information
with members of its business environment. These members include other businesses (trading partners)
and customers alike. We will now examine the drivers in each of these different groups of needs in detail.
In the last decade, ﬁrms have come to realize that one of their primary sources of competitive advantage
can come from how well they exchange information internally. Having used the recent explosion of
networking and computer storage technology to collect, store, and distribute large amounts of information,

© 2001 by CRC Press LLC
ﬁrms are now looking to reﬁne their operations. What businesses have realized is that they have a major
challenge of providing everyone in their organization with access to the speciﬁc types of information
that they require in order to perform their jobs better. A key challenge is that each employee in a ﬁrm
performs a different task (or performs the same task in a different business context) and therefore needs
to have access to different types of information at different times. How to provide such connectivity
presents a signiﬁcant challenge to businesses of all sizes.
One of a business’s most valuable resources is its internal knowledge of how problems were identiﬁed
and solved in the past. A key communications objective for a ﬁrm is to ﬁnd a way to share problem-solving
experiences throughout the ﬁrm. Meeting this challenge is critical for the ﬁrm, otherwise it will face the
expense of solving the same problem for the ﬁrst time over and over again. The solution involves com-
munication solutions that not only provide access to detailed records of past projects, but also include
identiﬁcation and access to the employees who were involved in solving the problems. Only by ﬁnding a
way to meet this challenge can ﬁrms reﬁne their problem-solving processes and become more competitive.
The cost of producing products or services has received a great deal of attention in recent years.
Businesses have implemented a wide range of control and monitoring systems that are able to evaluate
the operations of different internal processes. Such systems include enterprise resource planning systems
that can control the supply chain of a product’s production process, quality improvement tracking
systems, and just-in-time manufacturing systems. One of the primary purposes of each system is to
permit a ﬁrm to more effectively use its resources and raw materials — in other words, they help a ﬁrm
run a “lean operation” in which all of its assets are fully utilized. Such tightly run operations require a
business to establish and maintain a wide variety of communications between its internal divisions no
matter where they may be located. Additionally, there is a direct correlation between how fully the ﬁrm’s
assets are utilized and how rapid communications between the different parts of the ﬁrm are executed.
These processes and systems force a business to walk a tightrope between operating at peak efﬁciency
and not having to correct materials to operate at all. Firms must identify what communication is required
to support such mission-critical systems and then implement and use their telecommunications solutions
to gain a competitive advantage.
Finally, businesses are often thought of as a collection of employees who come together at company-
owned locations to perform work. Businesses are now starting to realize that the arrival of relatively
inexpensive computing resources, coupled with the availability of numerous communications services,
call for rethinking about how they conduct their daily operations. Firms have already realized that many
of the noncritical or nonstrategic processes they perform can be effectively outsourced to other ﬁrms
that are able to perform these processes more efﬁciently and at a lower cost. Firms are now starting to
reexamine how and where their remaining employees work and interact. The popularity of telecommuting
and rotating “work from home” days shows how ﬁrms are starting to explore these uncharted waters.
One of the primary keys to making a widely distributed workforce successful is to identify communica-
tions solutions that permit the ﬁrm’s employees to interact as though they were together in an ofﬁce,
without the actual expense of the ofﬁce.
Advances in transportation and communication have permitted businesses of all sizes to compete on
a global scale. New businesses are able to offer their products to almost any international market, starting
on their ﬁrst day of operation. Existing businesses that have saturated their traditional domestic markets
are able to seek new revenue streams in unexplored global markets. One side effect of operating and
competing on a global scale is that all of the telecommunication systems that a business established to
facilitate internal communications for its domestic operations must now be extended to become both
location and distance insensitive. This requirement affects all forms of communication including voice,
video, and data. As a clear conﬁrmation of this growing need, the Federal Communications Commission’s
(FCC’s) statistics show that since 1987 the growth of the U.S. long distance market has been propelled
by a 14.5% compound annual growth rate (CAGR) in international long-distance revenues. Traditionally,
such services have been very distance sensitive, thus making telecommunications expenses a signiﬁcant
expenditure for a globally distributed business. As the number of ﬁrms that operate internationally has

© 2001 by CRC Press LLC
increased, so too has the number of telecommunications service providers. This increase in service
providers has provided businesses with an opportunity to seek out and use those providers who are able
to help them minimize their telecommunications costs. Once again, the FCC’s statistics show that the
composite cost of an international phone call has dropped from U.S. $1.00 in 1992 down to U.S. $0.68
in 1997.
As businesses study how they can maximize their proﬁts, they have realized they can reduce their costs
by streamlining interactions with their suppliers. This new understanding has led to the sharing of
information, such as current sales results and stocking data between retailers and their many suppliers.
The high volume and near real-time characteristics of this information have created a growing need for
more sophisticated telecommunications services. Once again, since retailers and suppliers may be located
in different areas, the telecommunications systems must be distance insensitive.
Finally, the most important interaction that a business has is with its customers. Customers are
demanding that it become easier and quicker to interact with a ﬁrm. They want to see updated product
lists and information; in some cases they want to be able to custom-design their own solution from a
ﬁrm’s product lines; and they want to be able to review and perhaps pay their bills electronically. This
increased level of interaction with customers who are not physically located in a ﬁrm’s place of business
demands an entirely new set of sophisticated telecommunications services.
4.2.2.2
Mobile Professional End-User Needs
As business become more decentralized and at the same time more customer-focused, the ranks of the
mobile professionals are swelling. This new breed of employee can no longer be thought of as being only
a salesperson; rather, the mobile employee may be part of any one of a number of project teams that
have been brought together to solve a speciﬁc problem. As more and more employees start to operate
away from the ﬁrm’s ofﬁces for longer periods of time, the ability to use communications systems and
services to provide information, obtain status updates, and share learned knowledge becomes even more
important. Let’s take a look at some of the speciﬁc needs of this group of end users.
Arguably, the most critical need of a mobile professional end user is his need for up-to-date informa-
tion. Since a mobile user is operating away from a centralized ofﬁce environment, his ability to learn
about changes in products or company strategy is limited to what information is sent to him — the
critical real-world “water cooler” information exchange system is no longer available to him. New means
of identifying important information need to be created along with an effective two-way system for
distributing that information and getting end user responses and feedback.
Since the mobile end user is often away from the ofﬁce and in fact may be spending much of the time
with a customer, it is impractical to carry all of the product and service reference material that may be
required to perform the tasks. Therefore, it’s important that the mobile end user be able to quickly access
all of the material that may be required to support the current task. Note that the information required
may take many forms including text, pictures, animation, and video. Many ﬁrms that sell large, complex
software systems have changed the way that they now perform product demonstrations. Instead of taking
complex computer systems to the customer’s site, they use a standard laptop and establish a communi-
cations link back to their ofﬁce, where the application is running on the more complex hardware system.
This is one way for the ﬁrm to better utilize its expensive resources and better support its mobile users
at a lower cost. Such services are only the start of what will be required to support the growing mobile
user community.
The type of data that can be accessed by mobile users is another critical issue. Current analog modem
links over voice-grade phone lines limit mobile users to a bandwidth between 28.8k bps and 56k bps,
which is acceptable for accessing small- to medium-sized text documents. As more and more information
is stored in richer formats such as video and integrated multimedia documents, new telecommunications
services will have to be created to support mobile users. The need for access to multimedia information
is especially critical for mobile users whose ﬁrms design, manufacture, or sell complex products. The
multimedia information for these products can help the mobile user to shorten the selling cycle by

© 2001 by CRC Press LLC
permitting such complex products to be clearly and simply communicated. New telecommunications
solutions are required to ensure that mobile users are able to access all of the information they require
in order to perform their jobs.
A unique requirement of mobile end users is that, unlike stationary users, information must “ﬁnd”
its way to the mobile user. The mobile user is expected to change locations quite often and can’t be
expected to be reached via an addressing scheme that requires the user to always be at a given geographic
location. This applies not only to voice communication but also to all forms of electronic information
interchange. This issue has been partially addressed by some of today’s current telecommunications
solutions; however, such solutions generally work only within a limited geographical area (country or
artiﬁcially determined service provider territory) and completely different solutions have been designed
for voice and data services. Mobile users require solutions that provide seamless integrated voice and
data solutions of ubiquitous coverage.
Although many of the needs of a mobile end user relate to ensuring reachability at all times, the
opposite is also a concern. One of a mobile user’s more valuable resources is time. Giving others the
ability to communicate with the mobile user also gives them the ability to appropriate time. The mobile
user needs to be able to limit who has what level of access. Additionally, the mobile user needs to be able
to decide if and how to respond to each request for valuable time.
A mobile user’s toolkit consists of several groups of information to help do the job on a daily basis.
These groups of information consist of a variety of phone lists, customer names and addresses, customer
lead lists, internal corporate directories, etc. As this collection of data grows in size, so too does it grow
in value to both the mobile user and the company as a whole. The telecommunications challenge is how
this information can be shared among the wide variety of communication devices used by the mobile
user without having to retype the information each time.
The demanding lifestyle that being a mobile user requires often results in the lines between a worker’s
personal and professional lives being blurred. Since the mobile user may be away from home for long
periods of time, it is critical that personal messages from various sources and in varying formats must
be able to ﬁnd their way to where the mobile professional is. Additionally, personal communications
must be clearly identiﬁed as such and must be easily differentiated from work-related communications.
Both mobile workers and the ﬁrms that employ them appear to be drivers for this type of requirement —
both parties realize that good communications can help a mobile worker strike the correct balance
between different roles and responsibilities.
Change and movement are key components of a mobile user’s typical day. Because of this, there is no
single best way for messages and information to reach the mobile user. Therefore, the mobile user needs
to be able to access a message in any one of several different ways: e-mail via the phone, and voicemail
via the laptop. It is critical that the information is able to reach the mobile end user as quickly as possible
without restricting how the user chooses to retrieve the information.
The era in which groups of the same people worked together for years or even entire careers is quickly
coming to a close. Mobile users are at the forefront of this change and represent the new breed of worker:
they are part of dynamic teams quickly created to solve speciﬁc problems. Once the problem has been
solved and a solution implemented, the team is then dissolved and its members go on to join other
dynamic teams. From a communications perspective, the mobile user needs to be able to easily exchange
and work on the same information with other members of the dynamic team during the time that the
team exists. The security associated with such communication is a critical factor. In today’s customer-
focused markets, employees of the customer may be part of the same dynamic team as the mobile user.
In such cases, the ability to ﬁlter and restrict a team member’s access to sensitive data is required in order
to ensure that the internal and external team members are able to work together smoothly.
4.2.2.3
SOHO End User Needs
In contrast to large established ﬁrms, employees of small ﬁrms have different communications needs.
We include in this group those workers, who may work for ﬁrms of almost any size, operating out of

© 2001 by CRC Press LLC
their homes. Corporate outsourcing and the increasing number of new businesses have caused this small
ofﬁce/home ofﬁce (SOHO) group of end users to increase in size on a yearly basis. As the telecommu-
nications service marketplace becomes more and more competitive, the SOHO segment of end-users has
started to receive the attention of telecommunications service providers. The key to a provider being able
to successfully serve this market will be an ability to correctly identify the needs that will motivate the
SOHO end-users to purchase telecommunications services.
Unlike either the business or the mobile user, the SOHO end-user is extremely price conscious. Smaller
organizations naturally tend to have smaller budgets and therefore will have less to spend on telecom-
munications services of any kind. However, SOHO end users are generally involved in very competitive
market niches and so they feel that it’s necessary to their continued survival that they arm themselves
with any tools that provide a competitive advantage. The end result of these two conﬂicting conditions
is that the SOHO end user will purchase or subscribe only to those telecommunications services that are
priced within budget and which can be clearly demonstrated to give a competitive advantage.
SOHO end users do share some of the same basic needs that mobile end users have. Speciﬁcally, those
SOHO end users who operate out of their homes will have the need to be able to separate personal
messages from business messages. This issue is a little more complex than it was for mobile end users
because all of the messages are delivered to a single location — the user’s home. An extension to this
need is that the at-home SOHO end user, just like the mobile end user, needs to be able to control who
can communicate and when. Since all requests for time (phone calls, e-mail, etc.) will come to home,
the SOHO end user needs to be supported by telecommunications services that can be told which role
the end-user is currently playing — homeowner or worker.
Most SOHO establishments share a desire to one day be bigger then they are now. As a move in that
direction, SOHO end users want to be able to start projecting a “big company” facade at all times when
dealing with customers. This requirement manifests itself in several different ways: addresses and stafﬁng
levels. In the days prior to electronic addresses, small ﬁrms could use postal boxes to obscure their less
impressive residential or strip mall addresses. As we move into the future of electronically linked busi-
nesses and electronic commerce, the importance of an impressive electronic address will take the place
of the postal box. Additionally, since SOHO operations are generally staffed at very lean levels (i.e.,
perhaps a single employee), SOHO-end users are always on the lookout for telecommunications services
that can take the place of additional nonexistent staff members and which can be used to provide superior
customer contact. An example of such an application would be the “automated attendant” feature on
many small business phone systems which automatically provides company information and basic direc-
tory services.
For the SOHO end user, the previous requirement can be further extended. It is once again the limited
amount of staff available in the SOHO environment that generates the need for additional telecommu-
nications services. These services are needed to permit potential customers to easily show themselves the
SOHO ﬁrm’s products, prequalify themselves, and then get in touch with actual employees. This use of
telecommunications services to handle initial customer interest and then using valuable human resources
only when the customer has demonstrated that they are a viable potential customer may be one of the
most important drivers for SOHO telecommunications requirements. It certainly is one of the easiest to
justify spending money!
Like the mobile end user, a SOHO end user must often work with others in order to secure large
business orders, due to a SOHO’s small size. This can often result in a SOHO establishment being required
to ad-hoc partner with another business on a per-project basis. The telecommunications requirements
that would be driven by this opportunistic type of limited partnering would be to support the exchange
among the temporary partners of such information as schedules and project information. Once again,
security would be critical; just because partnering is occurring on this project does not exclude the
possibility that these partners may be competing against each other in the future.
Unlike the mobile end user, the SOHO end user has a “base of operations” — an ofﬁce. It will be used
to store almost all of the information related to the SOHO operation. This organizational structure

© 2001 by CRC Press LLC
produces a telecommunications need to permit the SOHO end user to access the information while away
from home. Such access requirements include the ability to retrieve voice messages, electronic data, and
any other information or formats that may be required. There is also the need for notifying SOHO end
users that new information has arrived at the ofﬁce in their absence. Note that once again, the information
can arrive in a multitude of different formats.
Finally, since a SOHO end user faces the dual dilemma of operating under a tight budget today but
believing that the operation will grow larger tomorrow, whatever telecommunications decisions are made
today must be able to grow and change with the business. Solutions that must be removed and replaced
are unacceptable both in terms of costs and time lost.
4.2.4
Residential End-User Needs
Our ﬁnal segment of end users is also arguably the largest. In the U.S. there are currently over 120 million
homes; it is these residential end users to whom a wide variety of service providers hope to sell additional
telecommunications services. The marketing success of standard telephone service and the mixed success
of various cable and Internet-related services clearly shows that the residential end-user community is a
complex and multi-faceted group. The service providers hoping to capture a signiﬁcant share of this
diverse group must be willing to spend the time to understand what shared needs are currently unsatisﬁed.
Perhaps the most important factor that must be considered when attempting to understand the needs
of the residential end-user is that, unlike the other end user segments that we’ve studied, the residential
end-user has a relatively ﬁxed budget from year to year. The result of this is the simple fact that every
purchase is a tradeoff: if a new telecommunications service is to be purchased, then something else must
be passed over. In most cases, this means that any service that does not provide a clear return for the
residential end-user’s investment is certain to fail. A good example of this occurred when the next
generation of phone services based on the Integrated Services Digital Network (ISDN) technology were
introduced. Despite the technology being sound, one reason that they failed was because residential end
users judged them to not provide enough of a beneﬁt to justify their cost.
As communication systems have improved our lives, they have also permitted us to move faster
throughout the day and get more done. The result of this has been that the residential end user views
the ability to manage time as a critical need. Any product or service that can provide more control over
how limited time resources are spent seems attractive. However, as we have previously discussed, other
factors such as price and availability will still play a very signiﬁcant role in determining the residential
end user’s ﬁnal acceptance.
As more and more information arrives at a residence, a striking advantage of postal mail over telephone
service starts to emerge: information that is delivered via the postal system clearly identiﬁes its intended
recipient. On the other hand, a phone call arrives with no attached address and so whoever is ﬁrst to
answer the phone is required to perform a crude routing function in order to ensure proper delivery.
This problem will only continue to grow as Internet access requires separate e-mail addresses and cable
services permit channel and scheduling selections to be customized on a per-viewer basis. Any services
that seek to address these needs of the residential end user must make sure that they are able to handle
information that arrives in a variety of formats and that both end-user addresses and information
processing preferences are handled by the service.
People are tribal by our very nature — we accomplish our daily activities by interacting with a wide
variety of other people in our community, neighborhood, and extended family. Residential end users
have a need to stay in touch with their contact group which resides locally as well as their extended
families who may not live locally. The speciﬁc relationship deﬁnes the frequency of this contact and the
format where it needs to occur. Today, such contact is mainly limited to text (letters or e-mail) and voice
(via the phone). However, the arrival of the Internet and its support for a diverse set of multimedia
communication formats has started to acquaint residential users with new options for communicating.
A very important constraint on any new telecommunications service is that it must be easy for the
residential end user to use. Since the educational background and technical sophistication of residential

© 2001 by CRC Press LLC
users can vary widely, the majority of residential end users require that systems they purchase be easy
and intuitive to use. One of the reasons that basic telephone service has been such a success is that the
service is intuitive and simple to use. Note that the amount of end-user training time that it takes to
learn to use a phone is very short! A key point for service providers to remember when introducing new
services is that, in the mind of the residential end user, ease of use is a more important factor than
additional bells and whistles.
Residential end users are always on the lookout for bargains whenever they are preparing to make a
purchase. This mentality can be seen in the types of retail establishments that dominate the U.S. landscape:
Wal-Mart, Kmart, and an almost inﬁnite variety of strip malls. One of the greatest advantages of the
Internet as it exists today is it permits skilled users to rapidly perform comparison shopping prior to
going out and making a purchase. In the future, telecommunications services that standardize such
comparisons and permit product offerings to be compared on multiple criteria including price, features,
and availability would meet a need of the residential end user.
As we move into a new millennium, it is becoming evident that the skills required to survive and
thrive in the modern world are changing. An example is found in automobile repair. The number of
residential end users who service and maintain their car themselves has dropped substantially due to
increased complexity in automobile design (anti-lock brakes, turbo-charged engines, etc.) and a decrease
in the amount of time available to perform such basic tasks. Interestingly enough, when a car is taken
to a repair shop to be worked on, one of the ﬁrst steps that the mechanics perform is to attach computer
input cables to various parts of the car in order to diagnose its operational health. Residential end users
understand that this change in required life skills is occurring and they are eager to not be left behind.
Therefore, they see access to education and information resources as a critical need and they desire
telecommunications products and services that can improve, supplement, or provide greater access to
such educational resources.
One of the greatest beneﬁts of modern communications services is allowing people to interact with
others who share a common interest. Without such services, perhaps these people would otherwise never
know about each other. Residential end users desire services that will permit them to interact with other
(potentially) remote end users who share a common interest. Examples would be collectors, fantasy-
league sports players, on-line action houses, and support groups. New telecommunications services offer
the possibility of permitting such interactions to occur on a global scale.
In the past, if a residential end user wished to gain access to valuable resources such as technical help,
a stockbroker, etc., they had few options: schedule an appointment and then travel to meet with the
resource provider face-to-face or phone them and either wait on hold or wait for them to call back.
Telecommunications services that can streamline access to such valuable and limited resources are desired
by all residential end users.
Access also plays a key role when it comes to a residential end user’s ﬁnances. Better access to ﬁnancial
resources such as loan information, checking/savings account information, and stock portfolios has
always been desired but not widely available. Key barriers to such services in the past have been concerns
regarding both the security of transactions and the inability to validate the identity of the user, and the
lack of appropriate equipment at the end user’s residence to support such services. Both of these issues
are being dealt with and will not continue to be barriers.
Residential end users seek ways to supplement other activities and thereby produce a richer experience
for themselves. Users desire a way to gain more information or to follow up on something else that they
have read about or seen. An example would be PBS’s Nova programs, which display different Web links
that point to supplemental material about the portion of the show that is currently being viewed.
Additionally, residential end users would like to be able to follow up and obtain more information on
advertised products that they see in different media — note that this accounts for the fact that Web
addresses have become a standard part of any auto advertisement!
In a fashion similar to both mobile and SOHO end users, residential end users are very concerned
about both their privacy and how they spend their valuable time. Residential end users want to be able

© 2001 by CRC Press LLC
to control who is able to get access to them and when such access is permitted. Therefore, they are
interested in ﬁnding solutions that permit them to control who is able to send them information and
how they are notiﬁed when that information arrives.
Finally, the ultimate beneﬁt of technology is that it permits residential end users to plan events around
their schedule rather than the other way around. Residential end users would like to be able to pick what
time they want to be entertained instead of having to arrange their lives around external entertainment
schedules.
4.2.3
End User Requirements Summary
As we conclude this section, it is important that we review the needs that are facing the four main
segments of end users who will be driving the evolution of telecommunications into the future: business,
mobile, SOHO, and residential. It is important to note that each of these segments is attempting to
accomplish a different set of goals with different sets of available resources. This simple fact becomes
quite evident when one looks at the differences in how much each of the different segments is going to
be willing to spend on new telecommunications applications and services.
Although there are signiﬁcant differences between each of the major end-user segments, several
common themes have emerged. One of the most fundamental needs that each segment is trying to address
is the ability to better control how its time is spent. Telecommunications services have the unique ability
to eliminate distances and to permit time to be “shifted” — that is, to allow interaction between different
parties to occur when it is most convenient for all of the involved parties. This need is further supported
by each segment’s desire to be in control of when and how they communicate with someone. The curse
of modern technology is that it severely limits our ability to make ourselves unreachable when we so
desire. The ability to regain this ability is a need that has been expressed by end users in all segments.
Finally, the realization that end users are working harder at their jobs and the fear that this will cause
their professional and personal lives to blur into an undistinguishable mass has generated a common set
of needs. Users are seeking a way to be able to clearly distinguish communication and information that
is associated with one role that they play from their other roles.
The recognition of these common basic end-user needs provides a clear prioritization for the devel-
opment and deployment of future telecommunications applications and services. At its core, telecom-
munications is a ﬁeld that exists to improve lives and solve problems. Advances in telecommunications
often appear to be based on the latest “gee-whiz” technologies; however, for a new service or application
to be successful, it must address one or more of the basic end-user needs that we have identiﬁed.
4.3
Application Trends
Telecommunications applications provide solutions to the problems faced by people who wish to
exchange information (“end users”). We deﬁne telecommunications applications as the software that
provides end users with access to the functions that permit information to be exchanged. A wide variety
of telecommunications applications are in use today: the software in telephone switches that provides
such services as emergency 911, caller I.D., three-way calling, etc; e-mail and Internet Web browsers;
distributed synchronized databases such as Lotus Notes™, etc. Each type of application was developed
to solve a speciﬁc set of end-user problems. Future telecommunications applications will also be developed
to meet the needs of end users.
The types of future telecommunications applications will be directly related to the end-user needs
discussed in the previous section. In order to focus our investigation into telecommunications applica-
tions, we will use the same segmentation of end users from the previous section. Figure 4.3 shows the
stage of the Telecommunications Trend Analysis Model that is covered by this section. Our investigation
will consist of two main parts: application functionality and functionality implementation. Looking at
the application functionality trends that are occurring will help us to understand how application

© 2001 by CRC Press LLC
developers and service providers are working to address end-user communication needs. We will explore
how this new functionality will be deployed in the real world when we go one step farther and look at
how vendors and service providers are planning to implement the new application functionality.
4.3.1
Application Functionality
All four major categories of end users will require more functionality from their telecommunications
applications. Because of the large purchasing power of each segment, competition among service pro-
viders has started to increase in the past few years. This trend is most noticeable in the U.S. and England;
however, the arrival of the European Union (EU) and a uniﬁed currency (the Euro) in western Europe
is also helping to make those telecommunications markets competitive.
A result of multiple competing service providers means that, at the very least, all segments of end
users will shortly be presented with multiple sources for all existing services. Additionally, the number
of services offered to end users will increase more rapidly than in the past due to the need for providers
to distinguish their offerings from each other.
The eventual result will be that the telecommunications applications offered to all end user segments
will become more customized in order to meet the speciﬁc needs of a particular segment. Since end users
are best suited to determining their exact needs, the process of subscribing to a telecommunications
application will change from the selection of “all-or-nothing” applications in which the end user had
little or no choice to participating in a “build-your-own” functionality selection in order to create a
customized application.
This ability for end users to design their own applications will be the arrival of true multimedia
applications that combine voice, video, and data features into a single customized application. This
customization will cause the functionality provided by applications to increase over what is available in
today’s applications. New functionality will be apparent in the following ﬁve areas: Internet services,
e-mail, videoconferencing, wireless services, and enhancements to traditional services. We will now look
at application functionality improvements we can expect in each of these areas.
FIGURE 4.3
Trend analysis — application trends.
6. Telecommunications
Strategies
1. User
Needs
3. System & Service
lntegration
4. New
Product & Service
Creation
5. Telecommunication
Tariffing
2. Application
Trends

© 2001 by CRC Press LLC
4.3.1.1
Internet Applications
The recent explosion in the popularity of the Internet (an unmanaged collection of interconnected
computer networks that are all able to “speak” the same communications protocols) has forever changed
what telecommunications applications will be expected to do. Studies of Internet usage are difﬁcult to
do because of its rapid growth; however, in the early 90s the Internet was used by a handful of researchers
and scientists, and studies eventually predicted that the Internet was expected to reach more than
200 million end users and 60 million hosts by 2000. With this kind of growth, it is very conceivable that
the usage of the Internet will catch up to, and perhaps surpass, the use of the telephone in the not so
distant future.
Today’s Internet applications lack the functionality required for end-users to perform e-commerce
transactions efﬁciently. Electronic commerce (“e-commerce”), the use of the Internet to facilitate the
buying and selling of goods, is viewed by many as “the next big thing.” The Internet offers sellers of goods
the ultimate virtual storefront: without having to rent physical space, they can display and demonstrate
their products for potential buyers. What is currently missing is the end-user’s ability to feel conﬁdent
making a purchase of the displayed goods directly over the Internet. The reasons for this lack of conﬁdence
are varied: lack of a secure environment, lack of an appropriate exchange mechanism, and privacy
concerns.
Users are well aware of the fact that as they exchange information with a retailer’s Internet application,
it is possible for a malicious user to monitor and record their transaction. This could result in the
malicious user obtaining credit card or bank account identiﬁcation information that could then be used
to steal funds from the unsuspecting user. Enhancements to functionality are being made to both the
retailer’s and the end-user’s applications. Basic encryption is now available that can be used to secure
the transaction information before it is transmitted in order to negate the effect of any interception of
the transmission. As this type of functionality is added to end-user’s browsers and Internet-aware appli-
cations, user conﬁdence in secure Internet transactions will increase and e-commerce can be expected
to grow at an explosive rate. In the short term, some service providers are offering guarantees to make
good on any losses incurred while using their networks in order to “jump-start” e-commerce activities.
E-commerce is currently complicated by the lack of an agreed upon form of “digital cash.” Despite
gains in the past decade regarding the increasing use of credit and debit cards, the majority of retail
transactions still occur using either paper money or checks. Neither of these two popular forms of
exchange translate well to being used in the Internet’s all-electronic environment. Once again, several
different approaches to this problem are currently being investigated. Recent agreements among many
of the major credit card companies have identiﬁed the required functional and exchange procedures that
will be required to support electronic forms of currency for existing and new Internet applications.
Finally, as more and more of everyday life becomes computerized, consumers are starting to become
concerned about how much information retailers are able to obtain regarding personal habits and buying
patterns. As the use of the Internet to purchase goods increases, a retailer’s ability to track the user’s
entire buying experience will also be increased. Such information could include a history of goods that
the consumer looked at but did not purchase, how often and at what times of day the user visited a
speciﬁc electronic “store,” and all of the products that the customer has ever purchased. Consumers have
become alarmed that retailer’s applications will be able to “mine” their purchasing history to target other
goods for advertising purposes or that retailers will sell their information to other retailers for their use
in trying to sell goods to the consumer. As Internet e-commerce applications mature, consumers are
going to insist that retailers clearly identify what consumer-related information is being tracked and post
their polices regarding use or sale of that data. Internet applications will have their functionality enhanced
to support and enforce such privacy policies.
Although the Internet is a worldwide phenomenon, the majority of its content has been created in
the English language. The reasons for this are varied; however, the origination of the Internet in the U.S.
and the high availability of both computers and Internet access in English-speaking countries has deﬁ-
nitely played a major role. Future Internet applications will be required to be able to deal with multiple

© 2001 by CRC Press LLC
languages. The tools to make this possible are slowly starting to emerge. Internet-based language trans-
lation products are now available that offer translation services for several languages. Whereas the amazing
translation devices seen in some popular science-ﬁction movies may still be a long way off, the ability
to translate text found on the Internet into another language or the ability to select a language for the
purchasing process are just around the corner.
• Some service providers who are deploying high-speed digital access services are also establishing
on-line communities built around high-speed access. These communities provide an opportunity
for businesses to set up on-line shops, as well as a place for both residential and business customers
to receive e-mail, purchase goods, access applications, and ﬁnd out current event information for
their local areas.
• Service providers are starting to explore the opportunities presented by integrated bills, accepting
payment and providing customer care electronically over the Internet. Voice services and Internet
services can be consolidated onto a single bill. Additional applications can electronically present
the bill to customers and accept payments over the Internet. This type of application can be used
with all types of telecommunications services including paging, IP voice, and long distance. An
additional beneﬁt of this approach is that it permits targeted marketing of speciﬁc customers and
offers a better chance of capturing an impulse buying opportunity.
• Many vendors are looking for ways to replace today’s ubiquitous fax machines. Some of the more
innovative solutions are coming from companies that are trying to reduce their product support
costs. One approach to directly provide a user with only speciﬁcally requested information uses
Internet based “push” technology. This information delivery technique requires a user to log on
to the company’s server via an Internet connection. Then the company is able to “push” or force
the display of speciﬁc information. The true power of this approach becomes clear when the user
is able to talk with the company at the same time by using a separate line. These hybrid solutions
are a cross between e-mail and fax services. Companies have found that this type of solution works
best when the company has a great deal of information that the user would otherwise have to
work through in order to ﬁnd what is needed.
• Firms are discovering that an estimated 10% of customers sometimes need assistance when using
the ﬁrm’s Web site. So-called “chat” applications are being added to Web sites to provide customers
with the ability to receive real-time one-on-one guidance from employees of the ﬁrm.
4.3.1.2
E-mail Applications
E-mail has become such a critical part of how so many people communicate that we choose to treat its
functionality separately from that of Internet applications. A 1998 survey by Forester Research revealed
that 83% of Internet users send e-mail, making it the most popular on-line activity. Surﬁng the Web is
the second most popular and attracts 81% of users.
• Adding voice and video to e-mail represents the next step in e-mail’s evolution. Some service
providers are now able to deliver e-mail that contains embedded links to additional voice and
video components of the message. The additional e-mail components are then sent to the user
through streaming technology that uses a service provider’s computers to do the majority of the
required processing, and then ships only the resulting images to be displayed on the end user’s
Web browser application. The challenge is to avoid disappointing the end users with poor appli-
cation performance that causes them to revert to standard text-only messages.
• Estimates show that up to 40% of users’ time on the Internet is spent on e-mail. In 1997, America
Online (AOL) had 11 million members and it processed 15 million e-mails per day, which roughly
relates to 23% of its members on-line time.
• E-mail is fairly pervasive, fast, and relatively free. One of the next logical steps is to make it secure.
Currently, the majority of ﬁnancial and legal communications occur using either paper or the

© 2001 by CRC Press LLC
somewhat dated electronic data interchange (EDI) systems. The problem with existing e-mail is
that it can be easily faked. Internet security has ﬁve key requirements: access control, authentica-
tion, privacy, integrity, and non-repudiation.
• Current secure e-mail solutions use a public key infrastructure (PKI). PKI is a set of security services
that can be used to provide security over public networks. PKI services consist of encryption, digital
signatures, and digital certiﬁcates. PKI services require the use of a two-part key: a public key and
a private key. Information is sent to a user after having been encrypted using their publicly advertised
“public key,” and can only be decrypted using the user’s secret “private key.” Every PKI exchange is
monitored and authenticated by a company that provides digital security services.
4.3.1.3
Video Conferencing Applications
• Videoconferencing (Figure 4.4) offers many beneﬁts, including savings in corporate travel and
savings in employees’ time. The U.S. market for videoconferencing service revenue is projected to
top $27 billion by 2002. In 1995, videoconferencing service revenue was $2.5 billion. Important
pieces must be in place for videoconferencing to happen: rising demand from multinational
corporations, improvements in technology, solidiﬁcation of key standards, and proliferation of
standards-compliant video-enabled products from heavy hitters such as Microsoft and Intel. Key
issues for service providers are reliability, quality, and ease of use. Current standards include:
FIGURE 4.4
Videoconferencing environments.
Video Conferencing Standard
Purpose
H.320
Videoconferencing over ISDN
H.323
Videoconferencing over LANs, WANs, intranets, and the Internet
H.324
Videoconferencing over regular dial-up telephone lines
H.320
Desktop
System
ISDN Links
ISDN Link
Public Network
ATM
lnternet
Ethernet
ISDN Links
H.320
Desktop
System
H.320
Room
System
H.320
Room
System
H.320
Room
System
Single
Videoconferencing
Standard
Environment
Multiple
Videoconferencing
Standards
Environment
H.320
Desktop
System
Multipoint
Control
Unit (MCU)
Multipoint
Control
Unit (MCU)
LAN
H.324
Terminal
H.323
Desktop
System
H.323
Desktop
System
H.323
Desktop
System

© 2001 by CRC Press LLC
• According to networkMCI Conferencing, about 250,000 videoconferencing-capable devices are
currently in place worldwide; by 2000, there will be over 50 million. A big user issue is service
complexity: it can take 40 to 50 minutes to set up a call because all endpoints need to be conﬁgured
to the same line speed, audio rate, frame speed, and resolution rate. How both vendors and service
providers have interpreted standards can also affect the service: a mismatch in interpretations can
result in dropped calls. Videoconferencing systems that are able to talk to different standards-
compliant endpoints are now becoming available (e.g., H.323/H.320 gateways).
• IP muliticasting will be able to provide multipoint H.323 videoconferencing. IP multicasting will
save users’ bandwidth on packet networks because the information needs to be transmitted only
once over a given link, with routers replicating information as required. One challenge associated
with multicasting is that it imposes a signiﬁcant communications load on the processor at each
endpoint since each endpoint, must send information to every other endpoint. This means that
IP multicasting is not currently scalable for large videoconferences.
4.3.1.4
Wireless Applications
• Wireless data service providers are starting to shift their focus from vertical to horizontal appli-
cations. In the past, wireless data applications have been traditionally targeted at the public safety
and utility markets. Newer applications target members of the ﬁnancial community, such as
bankers, analysts, and traders, by providing real-time access to stock information. One of the key
success factors to entering horizontal business markets will depend on the service provider’s ability
to create appealing service bundles.
• In the U.S., the future of the mobile data market is based on the cellular digital packet data (CDPD)
technology. CDPD is TCP/IP implemented over cellular networks. CDPD is well suited for certain
types of transmission, especially short ﬁle transfers. CDPD was ﬁrst speciﬁed in 1992; however,
it has been slow to be adopted and there are currently fewer than 500,000 data customers on all
U.S. cellular networks. Although CDPD may be well suited to supporting Internet-related appli-
cations, it is currently limited by two factors. The ﬁrst is the fact that CDPD-based services are
only available in selected markets. The second is that CDPD’s bandwidth is currently limited to
19.2k bps and actual connection throughput can drop as low as 2.4k bps when network voice
trafﬁc is high. CDPD transmission rates as high as 56k bps have been discussed; however, support
for such rates is not currently provided.
4.3.1.5
Enhancements to Traditional Services
• Vendors are starting to work with service providers to create service solutions that meet end user
needs. Uniﬁed messaging products are the ﬁrst examples of such services. The service alerts users
that they have e-mail via their service provider’s Web site and their own voice/fax mailbox. This
will be provided as a ﬁrst step for customers who only want basic service. To be added: products
that use text-to-speech technology. Good approach: everybody doesn’t need everything. Future
services include integrated e-mail, voice, and fax mailbox; non-subscriber voice connect — allows
e-mail users to send voice messages to anyone; and consolidated wireless/wireline mailbox with
improved phones that contain text display screens.
• The Universal International Freephone Number (UIFN) system allows a single toll-free number
to be used around the world. Users apply to the Internatinoal Telecommunications Union (ITU)
for an eight-digit number that is accessible by dialing the appropriate international access code,
“800,” and then the new number.
• Many new telecommunications applications are being developed for call centers. These applica-
tions are being designed to help companies gather information about their customers and make
sure that the products and services that the company offers are meeting the needs of their
customers. This type of application uses computer telephony integration (CTI), automatic call
distributors (ACDs), and interactive voice response (IVR) systems.

© 2001 by CRC Press LLC
• One of the primary motivations for ﬁrms to use virtual private networks (VPNs) is to avoid the
costs of expensive dedicated leased lines. Vendors are now making VPN products that contain
combinations of functions, including serving as IP routers, corporate ﬁrewalls, and certiﬁcate
authorities, along with the required VPN functions of encryption and authentication. A key
drawback to today’s VPN products is that the processing power required to perform VPN functions
such as encryption severely limit the throughput of the devices.
4.3.2
Functionality Implementation
The enhanced telecommunications application functionality described in the previous section requires
that the way applications are designed must be radically altered. As the computing equipment available
to end users continues to improve, the intelligence required to support the application is migrating from
within the network to the endpoints. In new and emerging applications, much of an application’s
functionality may reside in the end-user’s equipment. This is dramatically changing how networks are
designed. We will investigate these types of changes later in this chapter.
The competitive environment that service providers are starting to operate in will no longer permit
deployment of new applications at the current somewhat leisurely rate. End users will demand new
applications as soon as they identify problems they need to solve. The service provider who is the ﬁrst
to be able to offer a solution to such end users stands the best chance of capturing the largest share of
the market. Past history has clearly shown providers that it is better to be ﬁrst to market and bring
additional functionality later rather than wait until a new application is perfect.
The arrival of networking equipment that is able to provide exponentially larger amounts of bandwidth
will aid developers of new telecommunications applications. Table 4.1 identiﬁes several of the network
bandwidths now available for use with new applications. The result of greater bandwidth availability is
that less development time will have to be spent attempting to minimize the amount of data that
telecommunications applications exchange. This reduced development time will result in applications
that are richer in functionality, being made available to end-users more rapidly.
Recent increases in the amount of bandwidth provided by data networking equipment, coupled with
the initial availability of products that can provide voice services over a data network, have fueled a focus
on Internet Protocol (IP)-based networks. As competitive service providers build new networks to provide
services, they are selecting networking equipment that permits them to build IP-based networks rather
than the traditional Class 5 voice switches. These new service providers believe that in the very near
future all information (voice, video, and data) transported by a provider will be viewed as data and can
be encapsulated in the IP data network protocol.
If current application trends continue as expected, almost all future telecommunications applications
will be “Web-Aware.” Simply put, this means that such applications will have the ability to obtain
information from and provide information to other applications via World Wide Web (WWW) Internet
protocols. Although still in its infancy and facing an unsure future, the Java programming language has
popularized a highly distributed programming model that will inﬂuence the design of such future
applications. In this model, the network has the responsibility for advertising what applications it supports
and storing the logic required to provide the application. The end-user’s customer premises equipment
TABLE 4.1
Standard 
Transport Bandwidths
Transport Type
Bandwidth
OC-3
156M bps
OC-12
622M bps
OC-48
2.5G bps
OC-192
10G bps

© 2001 by CRC Press LLC
(CPE) will then download the needed functionality and execute it locally, thus distributing application
processing from the network’s limited resources.
The use of data networks for telecommunications application interconnection will have the interesting
side effect of causing what has been called the “death of distance.” Because end users are currently charged
for the size of the connection that they use to access the Internet, it no longer matters how far the data
travels once it is transmitted. This will result in a greater use of more widely distributed applications. A
more detailed discussion of the effects of changes in telecommunications application pricing is provided
later in this chapter.
One of the greatest bottlenecks in deploying new telecommunications applications resides in the back
ofﬁce operations of the service providers themselves. After an end user selects a service and negotiates
any customizations, there is often a delay (sometimes, signiﬁcant delay) as the provider processes the
order and reconﬁgures its network to deliver the requested service. The telecommunications applications
introduced in a competitive environment must be deployed with minimal support costs and must start
to generate revenue as quickly as possible. One of the most promising means of accomplishing both of
these goals simultaneously is to automate the telecommunications application service ordering process.
Assuming that the obvious security issues can be solved, interfacing the application directly to the
provider’s operation support systems (OSS) will reduce the support costs for the application while at the
same time decreasing the delay between when the service is ordered and when the application is available
to the end user.
Finally, the near-panic caused by the so-called Year 2000 (Y2K) bug, which caused some applications
to be unable to distinguish between 1900 and 2000 due to historical efforts by software developers to
minimize the amount of memory required to execute an application, will forever change how telecom-
munications applications are developed. Immediately after having experienced the expense and turmoil
caused by the hunt for potential Y2K errors in hundreds if not thousands of hardware platforms, operating
systems, and applications, end users can be expected to demand protection from future errors. Although
complete protection from software errors can never be guaranteed, new telecommunications applications
will most certainly contain enhanced testing capabilities that will permit the end user to simulate program
execution in an off-line environment in order to determine how it will react to a given set of inputs.
Let’s now take a close look at some of the issues surrounding how some of this enhanced application
functionality will be implemented in two important segments of telecommunications applications: Inter-
net applications and wireless applications.
4.3.2.1
Internet Functionality Implementation
• A carrier-grade IP telephony gatekeeper that complies with the emerging H.323 standard is now
available in evaluation versions. This product can be used to tie together IP and public network
gateway systems from other vendors. This product is signiﬁcant because it represents the ﬁrst
phase of multivendor interoperability. Ericsson plans on using applications to differentiate its
gatekeeper product — speciﬁcally for applications that are better suited to reside inside the carrier
network.
• Hammer Technologies has introduced an IP test system that monitors the quality of voice on IP
networks. The system automatically tests voice quality, measures audio quality, and includes a
Voice over Internet (VoIP) protocol analysis tool as an IP trafﬁc generator.
• Microsoft, Netscape/AOL, and Sun are all competing to supply commercial Web server and
application platforms to public network service providers. Each of these companies has a different
vision of what the next generation of telecommunications applications will look like. Microsoft
sees applications being built on top of low-cost PC-based Microsoft operating systems.
Netscape/AOL sees applications as being distributed and platform independent. Sun sees appli-
cations running on open, fault-tolerant systems that use the Java language.
• Some service providers are aggressively deploying advanced high-speed digital subscriber services.
Many of these service providers own and operate switch-based networks and feel that a switched

© 2001 by CRC Press LLC
network infrastructure routes packets faster and more reliably than a routed one. Such providers
are offering Internet access and LAN-like services.
• In order to provide access to popular Internet content to users in other countries, creative appli-
cations are being developed to distribute the information. Using a combination of satellite links,
multicasting software, and local caching, service providers are using public Internet kiosks to
permit users to view the most popular web pages. This eliminates long waits for dial tones and
conﬂicts over access to what precious bandwidth exists. This new approach “pushes” content to
where the user is instead of requiring the user to pull content off of North American servers.
• Dynamic HTML will allow designers to make richer, multilayered pages. Dynamic HTML will
allow designers to create Web pages more efﬁciently so each link of information doesn’t have to
be downloaded from the server.
4.3.2.2
Wireless Functionality Implementation
• Microsoft has announced that it is developing a non-standard microbrowser as a part of its goal
to enter the wireless data marketplace. The microbrowser will permit wireless users to browse the
Web, provision services, and access billing information. The Wireless Application Protocol Forum
released the open wireless mark-up language (WML) microbrowser speciﬁcation.
• Researchers have been able to crack the messaging encryption algorithm used in U.S.-based CDMA
and TDMA digital cellular networks. The researchers have broken the Cellular Message Encryption
Algorithm (CEMA) code. The CEMA code has been designed to safeguard dialed digits that are
sent over the airwaves. Different encryption algorithms are employed for user authentication and
voice privacy. The reason that the researchers were able to crack the CEMA code was, in part, due
to the fact that the wireless industry has watered down its security algorithms in order to appease
the U.S. federal government.
4.4 Systems and Service Integration (Figure 4.5)
4.4.1
Introduction
As telecommunications technologies are able to provide end users with more and more complex services
with an increasing number of interrelated features, end users have started to complain. Just as when you
go to purchase a car, you don’t want to be required to make decisions regarding issues that are relatively
unimportant to you. An example of this would be when taking an airline ﬂight, you do care about ﬂight
times and where you sit; however, you don’t care what altitude you ﬂy at or what movie is shown. End
users simply want to be able to use telecommunications services to make their lives easier and to make
themselves more productive — they don’t want to be telecommunications experts in order to select and
use such services.
Service providers and network equipment vendors are responding to these needs by integrating what
are currently separate service offerings into new feature-rich services and by consolidating technology-
speciﬁc networks into single networks that are simultaneously able to handle voice, video, and data
information exchanges. In this section, we will explore some of the drivers for service and system
integration and identify how these are going to affect the telecommunications services available to end
TABLE 4.2
Digital Subscriber Line Bandwidth
Symmetrical Service Rates
Asymmetrical Service Rates
256k bps
4M bps downstream, 1M bps upstream
512k bps
7M bps downstream, 1M bps upstream
768k bps
1M bps

© 2001 by CRC Press LLC
users in the future. We will then take a look at speciﬁc trends in service and system integration as they
relate to each one of the four classes of end users identiﬁed earlier in this chapter.
4.4.2
Drivers for Integration
Integration of services and systems requires both considerable effort and expense. In order to make such
an investment worthwhile, there needs to be a future payoff for service providers and application
developers who make the integrated solutions. In fact, there are several distinctly different motivations
that are in the process of creating integrated solutions. The drivers for integrated services are as follows:
• Competitive differentiation: As the number of service providers is increasing, the number of end
users in each segment is remaining relatively constant. This means that service providers will only
be able to grow by wooing end users away from their current providers. In order to accomplish
this, a provider will have to be able offer the end user a compelling reason to switch. Integrated
services can be such an enticement, and such tactics are starting to appear in the form of “follow
me” offerings where voice, paging, and mobile services are linked to a single service. With such a
service, someone trying to contact the end user dials a single number which then attempts to
establish a connection with the called party via each different communication method. If the
desired end user is not reachable, then a message can be left on a voice mail system that the end
user can check via any of the available technologies.
• Single provider: Recent surveys of end users have revealed that, all other things (such as price)
being equal, users desire to receive all of their services from a single provider. The reasons are
simple: a single provider means a single bill and one number to call in the event of any problems
with the service.
• Technology advancements: The integration of multiple services into a single offering to the end
user has its own potential risks. An integrated application requires a signiﬁcant amount of end-
user customization in order to provide the maximum beneﬁt. An example of an integrated service
that has suffered from low end-user acceptance due in part to its complex conﬁguration is the
Integrated Digital Services Network (ISDN). Advances in network intelligence and equipment
processing allow the conﬁguration of new services to be simpliﬁed and have allowed much of the
FIGURE 4.5
Trend analysis — system and service integration.
6. Telecommunications
Strategies
1. User
Needs
2. Application
Trends
3. System & Service
lntegration
4. New
Product & Service
Creation
5. Telecommunication
Tariffing

© 2001 by CRC Press LLC
conﬁguration process to be performed automatically by the network equipment itself. Additionally,
improved network element processing has permitted multiple elements in different technology
domains to exchange the required information to support integrated services.
• Improved billing systems: Amazingly enough, one of the greatest limitations on integrating
services has been the billing system used by service providers. Such large and complex billing
applications were originally designed to support a speciﬁc set of services offered via a single
technology network.
Likewise, there are several identiﬁable drivers working together to motivate service providers to create
integrated systems. The drivers for integrated systems are:
• Reduced network deployment costs: End users are starting to demand services that have voice,
video, and data components. Service providers will have the choice of building separate redundant
networks to provide such services or of building a single high-speed network to handle all three
forms of communication. As you may well imagine, the decision to build a single network becomes
very straightforward once the economics of building a single network to deliver all services is
considered.
• Reduced operations costs: A signiﬁcant cost of delivering a service to an end user can be attributed
to the operational expenses required to keep the network working correctly. The use of an inte-
grated network reduces the number of network elements required to deliver services, simplifying
operational requirements and thereby lowering the ongoing cost of offering the service.
• Bandwidth breakthroughs: The possibility of using an integrated system to offer services to end
users could not be realized until improvements in network equipment and transport technologies
occurred. Recent increases in the bandwidth that can be provided by a single network have made
it possible to build a single network that can support multiple services.
• Tarifﬁng: Existing tarifﬁng of telecommunications systems was designed years ago when the
primary offerings to end users were voice services. Data networks are currently free of many of
the limitations that restrict what and where services can be offered via traditional voice networks.
We will discuss speciﬁc tarifﬁng-based motivations later in this chapter.
4.4.3
Integration for Service Providers
• The arrival of wavelength division multiplexing (WDM) systems has caused service providers to
reevaluate their existing time division multiplexing (TDM) systems. Network planners currently
believe that the two different approaches can be used together to create networks that provide the
lowest bandwidth costs.
• As service providers prepare to reshape their circuit-switched networks into IP packet-routed
networks, the issue arises about what type of operation support system (OSS) will be needed.
Service providers have a range of functions to support: provisioning of new services, service
assurance, and network management. Existing service providers will most probably address this
problem by reusing part or all of their existing billing or customer care systems. The greatest
challenges will come in the areas of network management and provisioning.
• Sprint Corporation announced in June 1998 that they were planning on carrying all of their voice,
video, and data trafﬁc over its asynchronous transfer mode (ATM) network. Sprint CEO William
T. Esrey predicted that this approach would cut Sprint’s cost of delivering a voice call by 70%.
Sprint expects to achieve this level of cost reduction because of the much higher performance:cost
ratio of data switches vs. conventional circuit-switched voice switches.
• Traditional circuit-switched system service providers are watching the success of facilities-based
Internet service providers (ISPs) and their packet-switching and routing forwarding networks. In
addition, the large circuit-switched network equipment vendors are also modifying their equip-
ment in order to transition them to work in a packet-switched environment. Many are forgetting

© 2001 by CRC Press LLC
that the existing circuit-switched, connection-oriented public network infrastructure has been
built up over decades and includes layers of resiliency and fault tolerance built in. Another key
point is that time-slotting information in hardware allows for guaranteed latency and delay
parameters that simply cannot be achieved in many packet-switched systems. It is possible that
in the future, the circuit-switched network will serve as a mission-critical backup system for the
public packet-based system and will only be used for those cases where the “call must go through.”
A possible casuality of the move to a packet-based public network could be the current computer-
telephony integration (CTI) market.
• ISPs are now at the front line of telecommunications equipment design. Today’s ISPs are building
their own facilities, laying their own optical ﬁber and installing their own carrier-class switches
in points-of-presence (POPs). Traditional circuit-switched service providers have been taking more
data and even voice trafﬁc off traditional circuit switches and putting the trafﬁc on packet-switched
networks that were formerly considered to be “data-overlay” networks.
• The current public network consists of voice switches interconnected via transport systems. New
user demands are causing this network architecture to be reshaped to now support voice, video,
and data services. This new network architecture (Figure 4.6) uses a transport infrastructure which
supplies the required interconnectivity to create its foundation. The architecture’s switching layer
provides the call set up and teardown functions throughout the network that are required to
deliver services using a variety of protocols. Finally, a routing layer is used to provide the ﬁnal
step in the process of delivering data services to end users.
• Within the telecommunications industry there is still disagreement about whether packet switch-
ing, ATM, or traditional circuit switching has the best performance:cost ratio. Peter Sevcik, a senior
associate at Northeast Consulting Resources, Inc. has shown that each successful new generation
of switching technology cuts the performance:cost doubling time in half. Sevcik says that tradi-
tional central ofﬁce circuit-switched telephone switches double their performance:cost ratio every
80 months; ATM switches do the same every 40 months; packet switches and routers double their
ratio every 20 months; frame relay switches double their ratio every 10 months.
• Hewlett-Packard (H-P) is helping ISPs develop an architecture that delivers fast, consistent, and
differentiated service over the Internet. The goal is to enable individual businesses to guarantee
service levels to end customers over the Internet and to offer predictable and differentiated services.
FIGURE 4.6
A new network architecture.
Layer 3: Routing
ADM / Cross-
Connect
ADM / Cross-
Connect
ADM / Cross-
Connect
Voice Switch
lP Router
Layer 1: Transporting
Layer 2: Switching
Frame Relay Switch
ATM Switch
lP Router

© 2001 by CRC Press LLC
H-P’s product is a bundle of special H-P hardware and software, along with add-on services from
Cisco Systems, that will offer ISPs end-to-end, mission-critical service level guarantees and Internet
service level agreements. Technically put, this product offering will integrate control, measurement,
and management across the servers in the network in order to guarantee delivery of service levels.
• Sprint’s announcement of their plan to build an integrated on-demand (IOD) ATM-based network
to deliver voice, video, and data sets the standard for future public network developments.
• As service providers start to offer services that use multiple technologies, equipment vendors are
modifying their existing equipment to support the providers’ new needs. Traditional voice switch
vendors are enhancing their wireline switching products to also support wireless services. Some
switch architectures are so ﬂexible that providers can mix and match wireline and wireless modules
to permit subscribers to connect to a cell site or the public network using the same switch.
• U.S. West reports that the average voice call is approximately 5 minutes. The average data call is
about 32 minutes — this is causing congestion in the central ofﬁce.
• U.S. West is conducting trials with two ISPs to weed out Internet trafﬁc from voice calls. Using
distributed SS7 technology, ISP-bound data calls are identiﬁed at the user’s ingress switch and
immediately routed to the ISP over a parallel data network. This differs from the way traditional
end-user data calls are set up. Data calls are normally routed to the ISP through the phone network
via ISDN or digital switched services such as channelized T1. The data call rides the public network
the whole way.
• Service providers are showing a renewed interest in video services. Vendors are demonstrating
products that can push 26 Mbps over existing twisted pair wiring for up to 4000 feet. Broadband
wireless equipment vendors are mainly focused on data applications; however, some have dem-
onstrated videoconferencing and distance learning applications.
4.4.4
Integration for Business Users
• Some observers suggest that more than 60% of the costs associated with modern data networking
lie in the cost of ownership.
TABLE 4.3
Application Driving Network Growth*
Application
Data Types/Sizes
Network Trafﬁc Implication
Network Need
Scientiﬁc 
modeling, 
engineering
Data ﬁles
100s of megabytes to 
gigabytes
Large ﬁles increase 
bandwidth required
Higher bandwidth for desktops, 
servers, and backbones
Publications, 
medical data 
transfer
Data ﬁles
100s of megabytes to 
gigabytes
Large ﬁles increase 
bandwidth required
Low transmission latency
High volume of data streams
Higher bandwidth for desktops, 
servers, and backbones
Internet/Intranet
Data ﬁles now
Audio now
Video is emerging
High transaction rate
Large ﬁles, 1 MB to 100 MB
Large ﬁles increase 
bandwidth required
Low transmission latency
High volume of data streams
Higher bandwidth for desktops, 
servers and backbones
Low latency
Data 
warehousing, 
network 
backup
Data ﬁles
Gigabytes to terabytes
Large ﬁles increase 
bandwidth required
Transmitted during ﬁxed 
time period
Higher bandwidth for desktops, 
servers, and backbones
Low latency
Desktop video 
conferencing, 
interactive 
whiteboarding
Constant data stream
1.5 to 3.5 Mbps at the 
desktop
Class of service reservation
High volume of data streams
Higher bandwidth for desktops, 
servers, and backbones
Low latency
Predictable latency
*Source: Gigabit Ethernet Alliance

© 2001 by CRC Press LLC
• Ethernet LANs typically offer 10 Mbps of shared bandwidth. As the volume of network trafﬁc
increases, however, this amount of bandwidth quickly becomes inadequate to maintain acceptable
performance to support demanding applications. These trafﬁc jams are fueling the need for higher-
speed networks. Fast Ethernet, or 100BASE-T, has become the leading choice of high-speed LAN
technologies. Building on the near-universal acceptance of 10BASE-T Ethernet, Fast Ethernet
technology provides a smooth, nondisruptive evolution to 100 Mbps performance. The growing
use of 100BASE-T connections to servers and desktops, however, is creating a clear need for an
even higher-speed network technology at the backbone and server level. Ideally, this technology
should also provide a smooth upgrade path, be cost effective and not require retraining. The most
appropriate solution now in development is Gigabit Ethernet. Gigabit Ethernet will provide 1 Gbps
bandwidth for campus networks with the simplicity of Ethernet at a lower cost then other tech-
nologies of comparable speed. Gigabit Ethernet will be an ideal backbone interconnect technology
for use between 10/100BASE-T switches, as a connection to high-performance servers and as an
upgrade path for future high-end desktop computers requiring more bandwidth than 100BASE-T
can offer.
• Although Gigabit Ethernet is primarily an enterprise LAN technology, several service providers
(most of them ISPs) have begun evaluating it for use in local and metropolitan area sections of
their networks. Gigabit Ethernet can connect network equipment such as the server, routers, and
switches within a service provider’s POP, both inexpensively and at high speeds. One of Gigabit
Ethernet’s biggest selling points is that its cheaper and faster than asynchronous transfer mode
(ATM) or Synchronous Optical Network (SONET), which many service providers now use to link
gear in their POPs. Gigabit Ethernet’s heavy data orientation and distance limitations are red ﬂags,
however, for established telcos looking for technologies that can support voice, video, and data.
• Inverse Multiplexing for ATM (IMA) is a speciﬁcation for provisioning multiple ATM circuits in
T1 increments. IMA was created to bridge the bandwidth gap between T1 (1.544M bps) and T3
(45M bps). Using IMA, several low-cost T1 lines can be used to aggregate the bandwidth and
distribute ATM trafﬁc across multiple physical circuits.
• Frame relay speed and capacity improvements are being designed in order to keep pace with the
needs of the new public network for data services. The two major changes to frame relay are the
emerging frame relay over SONET (FROSONET) and multi-link frame relay (MLFR) standards.
FROSONET provides speciﬁcations for frame relay to run at OC3/STM-1 or OC12/STM-4 speeds.
MLFR adds scalability to frame relay networks, thus helping service providers keep pace with
growing trafﬁc demands while providing an incremental capacity jump for users who are out-
growing T1 capacity but are not ready for the speed or expense of DS3/E3 lines. The FROSONET
speciﬁcation uses the same high-level data link control (HDLC) over SONET mapping that is
being used for Point-To-Point Protocol (PPP) over SONET (PoS). This saves costs by allowing the
same hardware to be used for both PPP and frame relay interfaces. MLFR trunks combine multiple
physical links between switches in the public network into a single higher-capacity logical facility.
Additionally, frame relay’s existing quality of service (QoS) functionality permits it to be used by
service providers to offer such capabilities as service level agreements (SLAs) and customer network
management (CNM) functionality.
• Time division multiplexing (TDM) is used to combine individual connections in order to traverse
longer distances. Switched circuits, such as those used in telephone networks, provide dedicated
connections between two points. Switched-packet protocols, such as Ethernet, provide good uti-
lization of the backbone but have no provisions for providing the equivalent of a switched circuit
over a network. Switched-cell protocols such as ATM provide good utilization of the backbone
and have provisions for providing CBR and UBR virtual circuits, but are expensive when compared
to the newer switched packet systems such as Gigabit Ethernet.
• Wide area networks (WANs) tend to be rings like FDDI or various star conﬁgurations. Generally,
the number of entry points into the network tend to be very limited. WANs are designed to

© 2001 by CRC Press LLC
transmit data over long distances, tend to be focused on isochronous data, and lean toward circuit
switching because they were often devised by telephone companies to carry voice.
• Five leading car and truck manufacturers have banded together to lead the Automotive Network
Exchange (ANX) project. This is designed to create a specialized high-end, Internet-like VPN to
link North American automakers and their suppliers. This very reliable and secure network may
act as the beginnings of a parallel “Business Internet.”
4.4.5
Integration for Mobile Professionals
• The CDMA Development Group (CDG) is coordinating location technology trials among member
carriers and vendors. Trial focus will be on the three types of technology: global-positioning
system-based, network-based, and a combination of the two. The FCC has mandated that carriers
are required to be able to locate callers within 125 meters. Some carriers believe that they will
ultimately implement multiple location technologies. Network-based solutions are less precise but
may meet the FCC mandate. A handset-based solution may locate users more accurately and, if
used with a network-based solution, may allow a carrier to offer enhanced services such as location-
sensitive billing and concierge services.
• A spokesman for the CDG reports that wireless data represents only 10% of the total wireless
airtime in the U.S. New CDMA products have been introduced recently that will increase data
rates to 64 kbps.
• Service providers and equipment vendors are currently working on developing standards for the
third generation of wireless products: “3G.” The ﬁrst generation was analog, the second was digital,
and the third will be wireless broadband that will be used to support high-speed mobile data services.
• What standard will be used for the next generation of wireless services is still undecided. Possi-
bilities include W-TDMA which suffers from limitations in growth and W-CDMA which suffers
from limitations in power and processing.
4.4.6
Integration for SOHO Users
• In September 1998, the ITU ratiﬁed a single standard (V.90) for 56k bps access over the Public
Switched Telephone Network (PSTN). V.90 data transmission technology overcomes the theoret-
ical limitations imposed on standard analog modems by using the digital server connections that
most Internet and online service providers use at their end connection to the PSTN.
• Community area networks (CANs), as represented by cable modems, have a unique topology that
is not served well by existing LAN or WAN topologies. They have a large connection count of shared
wire like LANs, but have distances like those of WANs. Current CAN implementations generally
utilize a single downstream CATV channel that is shared by all network participants. A separate
upstream CATV channel is also shared for transmitting from the home to the cable head end.
FIGURE 4.7
Worldwide CDMA subscribers (source: CDMA Development Group).
North America
(3.23M)
Asia (875M)
Central &
South America
(130k)

© 2001 by CRC Press LLC
• Low and medium earth orbit satellite systems (LEO and MEO).
• Geostationary (GEO) satellite systems are being used to deliver data broadcasts. There are two
primary types of GEO services: very small aperture terminals (VSAT) and direct broadcast satellite
(DBS). Two DBS services have been announced: an entertainment service and a data service. The
data service will serve corporate customers with occasional and regular broadcasts, along with
residential customer service. One possible use of the data service is for software distribution.
Another company is offering three versions of its DBS Internet access service: direct delivery of
text ﬁles at 12 Mbps, multimedia at 3 Mbps, and Internet access at 400 kbps. This service is
asymmetrical: customers send information requests to the service provider via telephone lines and
receive data via the customer’s 24-inch antenna. VSATs are used for corporate broadcasts of data
including price updates. VSATs operate at speeds between 14 kbps and 64 kbps, with high-speed
bidirectional communication
• Home based LANs
• According to U.S. West, DSL services can be up to 250 times faster than a 28.8 kbps modem.
• Virtual private networks (VPNs) provide an alternative to leased-line connections. VPNs provide
an inexpensive way to extend the corporate network to telecommuters, home workers, day extend-
ers, remote ofﬁces, and business partners.
• VPNs are implemented through tunneling, in which the data to be transmitted and header
information are encapsulated inside standard IP packets, usually after encryption and sometimes
after compression as well.
• Three VPN tunneling protocols are currently in line to become industry standards: PPTP (Point-
to-Point Tunneling Protocol), L2TP, and IPSec (IP Security).
• Security is a critical component of a VPN implementation, especially for those implemented over
the public Internet. Encryption delivers the “private” in virtual private networking, but it is very
process intensive. Because of this, hardware-based VPN products deliver the best performance.
• Infonetics Research estimates that service providers’ share of the VPN market will grow to U.S.
$8.8B by the end of 2001.
• Despite severe quality limitations, users have already started using the Internet to deliver video-
conferencing. Many of these early systems use 56 kbps links to deliver video images of 160 by
160 pixels at a rate of 12 to 14 frames per second — that translates to a small video box conﬁned
to just part of the computer screen, showing a jumpy image. Existing corporate videoconferencing
systems deliver full-screen images at 30 frames per second — a much higher quality image.
Internet-based videoconferencing services won’t be real until new QoS standards are in place such
as the Resource Reservation Protocol (RSVP) and IP version 6 (IP6).
4.4.7
Integration for Residential Users
• Researchers are issuing cautions regarding unsolved problems with digital subscriber line (DSL)
and cable modem services. Complexities have been identiﬁed regarding the mixing of POTS and
DSL services without using a splitter. One of the biggest issues concerns what happens when a
user picks up a telephone handset in a splitterless service — the result is an immediate change in
load on the local loop, resulting in a loss of one to two orders of magnitude in signal amplitude.
Additionally, crosstalk can occur when several POTS twisted pairs in the same bundle are used to
provide DSL service.
• A new service that is being considered combines Internet and television services so that end users
can simultaneously surf the Internet and watch enhanced broadcast television at home. One
approach uses cable television systems to deliver downstream data to advanced set-top boxes.
Other approaches are more software oriented and don’t necessarily need set-top boxes.

© 2001 by CRC Press LLC
• The FCC has mandated that broadcasters must have started offering some high-deﬁnition televi-
sion (HDTV) digital programming in 1999 and complete their transition to digital by 2006.
• MSNBC, the cable broadcaster owned by Microsoft and NBC, has experimented with technology
that allows broadcasters to send digital signals embedded within television signals to PCs. MTV,
along with Intel, launched Intercast Jam, which broadcasts videos to PCs alongside rock artist
information via a webbrowser in the broadcast signal.
• For years, TV stations have been beaming out data in small doses — in the form of closed
captioning, test signals, ghost canceling, and messages to afﬁliates. That data is carried mainly
through the vertical blanking interval (VBI). This offers a total of between 150 and 200 kbps of
available bandwidth. This bandwidth is now being used by the Intercast consortium to transmit
ancillary data streams to PCs via the VBI.
• Vendors are starting to create products that support videophone services. One such product puts
a video camera in a set-top box and displays its image on any cable-ready TV. A touch-tone phone
provides audio, dialing, and navigation of the system’s on-screen controls. The system includes a
built-in 10BaseT Ethernet interface to link directly to a cable modem, digital subscriber line
modem, or corporate network.
4.5 New Product and Service Creation (Figure 4.8)
4.5.1
Introduction
The increasingly competitive telecommunications environment will require service providers to create
and deploy new products and services faster than ever before. Providers were able to create new services
at a much more leisurely rate in the past. A provider could wait until the next generation of technology
had been deployed into the network before introducing the services that used the new technology.
FIGURE 4.8
Trend analysis — new product and service creation.
6. Telecommunications
Strategies
1. User
Needs
2. Application
Trends
3. System & Service
lntegration
4. New
Product & Service
Creation
5. Telecommunication
Tariffing

© 2001 by CRC Press LLC
As the number of service providers increases, it is generally agreed that the ones who will succeed are
those who are able to best understand their end user’s needs and deploy services that best meet those
needs. In order to accomplish this, a provider will need a new approach to designing and deploying
future services.
Changes in network equipment and in the types of networks that are being deployed are equipping
providers with the essential tools. In this section we will ﬁrst look at some of the drivers and constraints
that providers are facing as they struggle to change how they create services. Next, we’ll focus on how
services will be created in tomorrow’s network. Finally, we’ll investigate how network bandwidth affects
the types of services that can be created and what is being done to provide more bandwidth for new
services.
4.5.2
Drivers and Constraints
• U.S. West is now offering a PCS service that includes mobile dial tone and advanced messaging
and routing capabilities. The dial tone service combines a handset-generated and network-gener-
ated dial tone. The handset portion allows users to hear dial tone while dialing, and the network-
generated portion allows users to hear a dial tone while they are initiating features. Customers
have said that they associate dial tone with reliability and quality. The service also includes a same-
number feature that routes calls made to a home, ofﬁce, or PCS number to a PCS phone. It can
also route all messages to a single mailbox, notifying users of messages via a light on the handset.
• Two different standards are being considered for using ATM to switch IP trafﬁc: Multiprotocol
over ATM (MPOA); ATM Forum; based on LAN emulation, seen as a campus backbone solution.
Multiprotocol Label Switching (MPLS); IETF; designed with the large-scale WAN in mind
• Service providers are looking for new ways to rapidly introduce new services to meet growing
customer demands. Existing circuit switches can generally only be modiﬁed by their vendors,
which takes too long and costs too much. Programmable switches (Figure 4.9) consist of three
main parts: a programmable switching fabric, controlling software (“host program”), and external
media used to provide enhanced-services functions. New functions can be added to programmable
switches by simply adding services and features to the host program. Open interfaces and APIs
permit third-party developers to create vast libraries of available services.
FIGURE 4.9
Programmable switch architecture.
Public
Network
Programmable
Switching
Fabric
Enhanced
Services
Resources
Host Program
Programmable Switch

© 2001 by CRC Press LLC
• In order to help ISPs that are not ready to make a full-ﬂedged investment in electronic commerce
with an option to start a little smaller, e-commerce software vendors are getting creative. One
vendor permits ISPs to operate small on-line stores (less than ten items to be sold) for free. As
they increase the size of the “store,” they then start paying the vendor for the use of the software.
• As of 1997, there were more than 1.3 billion televisions in the world, compared with 245 million
PCs and 741 million telephone lines.
• Both the competing standards for digital television, one promoted by the U.S.’s Advanced Televi-
sion Systems Committee and the other promoted by Europe’s Digital Video Broadcasting Group,
offer an almost unlimited potential to broadcast data to end users. In tests, broadcasters have been
able to transmit 60 Mbytes of data during a 51-second commercial.
• Some cable operators are now able to offer traditional switched voice services using their cable
networks. These services are proving to be very popular and in fact are more popular then the
highly touted cable modem services.
• Joint research done by International Data Corporation, Zona Research Inc., and Literature Searches
reveals that U.S. corporations spend more than U.S. $14 billion annually for their own expensive
but reliable data networks.
• In order to permit Internet trafﬁc to be prioritized, two sets of networking protocols are working
their way through the IETF. The ﬁrst set is called Differentiated Services (DiffServ). It provides
routing mechanisms designed to manage various QoS proﬁles, or performance parameters. The
other set of protocols is the multiprotocol label switching protocol. MPLS is a routing mechanism
designed to group all packets within an IP session into a single “ﬂow” at the networking layer
(Layer 3) and “tag” each session as such for expedited passage through router hops.
• Wireless service providers who use TDM are looking for ways to differentiate their services. Their
latest attempt is called Wireless Ofﬁce Services. These allow users to access PBX features from
their wireless phones while they are in the ofﬁce and when they leave the ofﬁce. This permits them
to use such features as four-digit dialing and call forwarding in all environments.
• AIN platforms and capabilities are ways that new services can be introduced into the public
network. However, the change to a packet-based network puts the future of AIN services in some
doubt.
• Vendors’ research labs are starting to produce products that implement some of the latest advances
in speech recognition technology. This type of interface is seen as a major step toward the
convergence of telephony and Internet applications. Call center applications are expected to be
among the ﬁrst to beneﬁt from these types of products.
• Vendors are offering service development products for Internet protocol-based voice service pro-
viders. Service providers will be able to use these products to add key features such as universal
messaging, follow-me services, and paging to their IP/public network gateways. All incoming
messages are stored in a single mailbox and can be converted to a variety of formats that the user
can then access via Web-browser, e-mail, or telephone.
• The Voice-over-IP (VoIP) Forum recently ratiﬁed an implementation agreement that deﬁned an
interoperability proﬁle based on the H.323 standard from the ITU. H.323 was designed to be a
technology for multipoint-multimedia communications over packet-based networks, which
include IP-based networks, such as the Internet. It can be applied in a variety of ways — audio
only (IP telephony or VoIP); audio and video (video telephony); audio and data; and audio, video,
and data.
• One debate in the communications community is how to successfully deliver QoS and implement
service-level agreements (SLAs). QoS, a network-wide performance characteristic, refers to the
network’s ability to fulﬁll a trafﬁc contract — the SLA — between the WAN network provider
and the subscriber for the minimum service provided by the network.

© 2001 by CRC Press LLC
• ISPs are replicating content across multiple services in order to balance user demand loads. Vendors
are now starting to offer products that allow service providers to automatically route end-user
requests to the replicated server that has a low enough load to facilitate the request.
• User demand for access to multimedia Internet content has resulted in novel solutions being
created by vendors. One approach uses satellite links to bypass the Internet and deliver multimedia
content to local ISPs where it can be cached for access by local users. This approach can be further
extended to caching of popular websites in order to speed up local access speeds.
4.5.3
New Service Creation
• The emerging consumer vehicle tracking service is called telematics. Telematics systems combine
GPS and cellular networks to offer safety and concierge services to consumers in automobiles.
The number of users is expected to grow from 58,000 subscribers this year to 1.2 million by 2003,
according to the Strategis Group. Most U.S. telematics operate on AMPS because of its near-
ubiquitous coverage.
• Smaller ISPs are using audio and video conferencing capabilities to distinguish themselves from
competitors. These service providers are starting to investigate using client and server software
solutions that permit videoconferencing over the Internet. Initial users include schools that have
a need to provide a one-on-one tutoring experience but don’t need an elaborate room-based
videoconferencing system. The supporting software systems are all H.323 compliant.
• Business travelers want to be able to access the Internet even when they are traveling internationally.
This is currently not possible — such travelers must reach their ISPs POP in order to access the
Internet. Some service providers are attempting to build international POPs to meet this need.
Other smaller ISPs are banding together to create consortia to offer Internet access to their
collective customers. An additional service that is being investigated would offer roaming users
access to their corporate intranets via secure tunneling.
• Bell Atlantic Mobile is offering utilities the ability to read customer’s meters automatically via
wireless data transmission using the cellular digital packet data (CDPD) network. The service
would allow utilities automatically to read meters and monitor energy ﬂows, among other services,
from a central location, skipping the need to send personnel to customer locations. This service
offers utility companies an advantage in a deregulated market because they can offer their cus-
tomers a better picture of their usage patterns and then offer them a special deal to keep them
from going to other utility providers.
• As of 1998, the Strategis Group reported that CDPD services had only 17,000 subscribers.
• Service providers are starting to offer enhanced fax services. These services include mailbox, which
provides a secure fax mailbox accessible from any location; never-busy fax transparent service
stores faxes for later delivery; fax-on-demand lets businesses create a library of faxable documents
that customers can access; and fax broadcast delivers a document to as many as 10,000 locations
with just one transmission.
• ISPs are starting to roll out Internet protocol voice services to corporate users. Initially, business
users can connect their PBXs to the ISPs IP network, thereby cutting costs on internal long-distance
calling. The next step is to combine IP voice with extranets. Businesses would then be able to call
other businesses at remote locations using ﬁve-digit dialing.
• Many PC games now come with multi-player Internet options. Users ﬁrst connect to the Internet,
then select a speciﬁc server which “hosts” a gaming session. Then as the end-user plays the game
in multi-player mode, the server allows them to exchange information with other players in real
time.
• Consumers and businesses will soon have the ability to both view and pay bills via the Internet
thanks to various forms of electronic bill presentation and payment (EBPP). This new service will

© 2001 by CRC Press LLC
allow billers to cut paper processing costs and garner customer loyalty and website hits. Financial
institutions, bill consolidators, Internet portals, and makers of personal ﬁnancial manager (PFM)
software products look forward to capturing market share.
• Studies show that 40% of U.S. homes have a PC and only 20% of those are plugged into the Internet.
• Electronic commerce is struggling with the issue of how to reach customers who are not connected.
Companies that have to use both the telephone and the Internet to reach customers are looking
for a way to tie the two systems together — “v-commerce.” These ﬁrms want to develop new
applications that will link voice and data, telephone, and PC to let Internet vendors reach customers
who can’t reach their Web pages. These new applications will use Motorola’s VoxML markup
language which simpliﬁes embedding speech into Web pages.
• The Web provides an opportunity for delivering a new type of picture called immersive photog-
raphy. This technology allows you to use your PC to navigate around a digitized 360-degree photo.
This technology is targeted toward Internet retailers who want to give their customers a wrap-
around view of their goods, including high-end real estate agents, travel agents, cruise lines, and
destination marketers.
• Visual communication services are poised for proliferation as new advances eliminate the ﬁnal
technological and market obstacles. The ideal solution for multimedia services combines the
organization and simplicity of the telephone system with the multimedia and open nature of the
Internet.
• Telemedicine is a broad term for several facets of medical care. Collaborative videoconferences
between sites, on-line access to patient records, medical libraries and databases, and continuing
medical education all fall under the term. Most telemedicine programs today are either simple
store-and-forward systems or ISDN videoconferencing systems adapted for use in a health care
setting.
• Automobiles are being equipped with more and more electronics and telecommunications devices.
Many cars now have Global Positioning System (GPS) receivers and computers to help the driver
from becoming lost. The U.S. government, state governments, and a variety of industries are
considering spending U.S. $200 billion on the Intelligent Transportation System (ITS) initiative.
ITS will provide automated cross-border ﬂeet services for North America, enhanced driver navi-
gation, automated accident reporting, and toll collection. Futurists foresee a day in which a car
monitors its “health” and can then use wireless communications to identify repair stations in the
event that a potential part failure is detected.
• Prepaid wireless services have become a big business in the U.S. The industry may see more than
U.S. $650 million in prepaid card service revenues in 2000. Customers generally must pay to have
their wireless service activated, then they must purchase a prepaid denomination, often in the
form of a card from a retail distributor. The next step is to initialize the prepaid service via an
interactive voice response (IVR) service.
4.5.4
Increasing Bandwidth
• Wireless cable operators are starting to offer high-speed Internet access services using multichannel
multipoint distribution service (MMDS). Without converting to digital, the most wireless cable
operators could offer in video is 33 channels, which can’t compete with average landline cable or
satellite providers.
• Wavelength Division Multiplexing (WDM) technology is being added to the network in order to
increase backbone capacity to handle new high-speed access technologies. Initial WDM systems
were only 2 to 4 channels. Recently, 32 channels appeared in dense WDM. Now hyperdense or
ultradense WDM (UDWDM) systems with channel densities of 40 and up and capacities of
400 Gbps are becoming available. Providers are upgrading a few ﬁbers on a route and then

© 2001 by CRC Press LLC
upgrading the others over a few years. One vendor boasts that its terabit demo could carry the
Internet’s entire trafﬁc on a single ﬁber.
• GTE is using multichannel multipoint distribution service (MMDS) technology to deliver 68 video
channels, 32 music options, and near video-on-demand with 40 channels of pay-per-view. GTE
has also rolled out hybrid ﬁber/coax (HFC) cable-based digital video networks. These systems
transmit at 750 MHz downstream and 40 MHz upstream.
• U.S. West has introduced a VDSL platform that provides subscribers with integrated digital TV
and high-speed Internet access. Included in U.S. West’s bundle are on-screen caller ID, voice
messaging, and 120 channels of programming, including pay-per-view. The service operates at
256 kbps.
• Bell Canada now uses its HFC system with 10 Mbps downstream and 1 Mbps upstream to offer
a picture-within-a-picture service that allows users to go online and watch television from the
same screen simultaneously.
• Cable operators that want to start offering high-speed Internet access services to their subscribers
without having to perform expensive upgrades to make their cable network two-way are getting
creative. They are using their existing one-way cable networks to deliver content to end users while
the end-users use their telephone to send information requests. Although this solution may be
well suited for rural cable providers who will never have the funds to make their systems two-way,
this one-way approach may not provide the bandwidth required by the growing SOHO market.
• Cable operators are able to offer residential subscribers Internet access at 1.5 Mbps rates using a
cable modem. A report from Forester Research concedes the residential market to cable operators
over telephone companies: cable operators are predicted to have 13.6 million cable modem cus-
tomers by 2002, while telecos will have only 2.2 million ADSL users.
• U.S. West markets its DSL services to three types of residential users: consumer/Web browsers
(want “always on”), gamers (“entertainment”), and work-at-home users (“looking for bandwidth
and the user experience”).
• The cable company MediaOne has found, through internal studies, that nearly all cable modem
owners use their Internet connections seven to nine times more often than when they had a dial-
up connection.
• MediaOne marketing cites a recent study that claimed the average Internet user wastes a total of
50 hours a year waiting to connect to the Internet and waiting for pages to download.
• Telecos, ISPs, and CLECs that are rolling out ADSL services are ﬁnding that the earliest adopters
of the services are in the small business market. Telephone companies will stress the security of
ADSL over cable modem’s shared media to small business owners.
• In the U.S., the FCC has auctioned off 1.3 GHz of spectrum in the 28 and 31 GHz ranges for use
in local multipoint distribution service (LMDS) two-way services.
Time Period
WDM Capabilities
1980s
• 2 channel
• Wideband WDM
• 1310, 1550 nm
Early 1990s
• 2–4 channels
• 3–5 nm spacing
• Passive WDM components/parts
1996
• 16 or more channels
• 0.8 nm spacing
• DWDM, integrated systems

© 2001 by CRC Press LLC
• Broadband wireless networks have many beneﬁts: they are fast and easy to deploy; they have
minimal infrastructure and real estate requirements; they feature grow-as-you-go network build-
out; and they can deliver voice, video, and data services from 64 kbps to 155 Mbps.
• LMDS can be used to offer many services. Business-oriented services include wire speed LAN
interconnect and fractional and full T-1. Teleworking at 10 Mbps is virtually as fast as being at
the ofﬁce. Megabit per second Internet access is geared to residential users. Other services include
100 broadcast video channels in competition with cable, and second and third phone lines at
home or the ofﬁce.
• LMDS services compete with DSL and hybrid ﬁber/coax (HFC) services. LMDS is better than
both DSL and HFC at offering high-speed symmetrical services.
• Wireless cable operators have spectrum in the 2.5 GHz range (MMDS).
• The H.323 protocol, used to provide VoIP services, deﬁnes ways in which multimedia formats
such as phone calls, computer data, pictures, or video can be exchanged and managed seamlessly
across packet-switched networks.
• A variety of broadband wireless providers have already introduced services that use multichannel
multipoint distribution service (MMDS) and local multipoint distribution service (LMDS).
MMDS service providers have been around for awhile, whereas LMDS providers have only recently
bought their licenses. MMDS offers a broader coverage reach while LMDS offers greater capacity.
Current service offerings use either the public network or a cable modem for the return path.
4.6 Telecommunications Tarifﬁng (Figure 4.10)
4.6.1
Introduction
Perhaps no aspect of telecommunications is as overlooked as how services are priced. All segments of
end users have differing amounts of funds available to spend on telecommunications services. Pricing a
FIGURE 4.10
Trend analysis — telecommunication tarifﬁng.
5. Telecommunications
Tariffing
6. Telecommunications
Strategies
1. User
Needs
2. Application
Trends
3. System & Service
Integration
4. New 
Product & Service
Creation

© 2001 by CRC Press LLC
service too high will cause end users to seek lower price alternatives. Pricing a service too low will result
in the service provider missing out on revenues that could have been used to fund the next service.
In the past, service providers have enjoyed monopoly status in both North America and Western
Europe. Under this system, prices for services were closely regulated by governments. This is in the process
of changing, and in the future service prices will be driven by market factors. This change will require
existing service providers to change the metrics used to measure service and the pricing philosophies
that have been used to create service rates in the past.
In this section, we will explore trends in tarifﬁng in the leading markets of North America and Western
Europe. The effect of competition on service pricing will also be examined. Finally, we’ll discuss the
impact that new technologies will have on the pricing of future services.
4.6.2
Regulatory Trends
• The FCC is proposing that the Bell companies be permitted to create separate subsidiaries to offer
data communication services. These subsidiaries would be less regulated and could set interstate
service prices without ﬁling to the FCC. The Bell’s regulated units would still be required to sell
capacity to competitors but the separate subsidiaries wouldn’t.
4.6.3
Service Pricing Trends
• The average long-distance call in the U.S. costs about 13 cents per minute, but the average
international price is 89 cents per minute. Telco revenues per minute on international calls are
predicted to fall more than 20% annually through 2001.
• Cable & Wireless USA hopes to use pricing and inexpensive long distance to draw residential
customers to its Internet service. CWIX will offer customer 150 hours of on-line service, e-mail,
and a free Web page for a monthly fee of U.S. $14.95. Some analysts doubt if bundling long distance
with Internet access will attract new customers. They point out that the intersection of households
that are on-line and use long distance heavily is not large — perhaps 15% of the total.
• In most U.S. telco service areas today, termination fees of up to U.S. $36,000 to break a tariffed
service contract are still alive and kicking, despite efforts by competition to eliminate them. These
contracts can prevent a customer from purchasing the services offered by a competitive provider
because they still have a year or two to go on their current contracts.
• The paging industry grew by 14% in 1997 to a total of 50 million subscribers. However, in 1997
four of the top ﬁrms, which together control almost 40% of the market, reported almost a half-
billion dollars in losses on combined record revenues of more than $2 billion. Many paging
companies are suffering from expensive network buildouts. Paging companies seem to hold a high
number of customers who refuse to upgrade beyond basic plans, according to analysts. Price wars
and new technologies have driven down the costs of average basic local service from $20 a month
a decade ago to less than $10 a month. In some markets, the price has shrunk to less than $5 a
month.
• Although extending wireless service to the high percentage of credit-challenged users was a chief
driver in the development of prepaid service, wireless carriers are discovering that prepaid strat-
egies may be almost as critical to future growth of their overall customer bases as traditional post-
paid service. BellSouth Mobility intends to have prepaid accounts for 30% of its new sign-ups.
• Traditional methods of buying and selling bandwidth are not adequate in today’s competitive
market. A new Internet-based service permits providers with bandwidth to sell their available
bandwidth for bidding purposes. Buyers are then able to see the available bandwidth along with
information regarding destination country, size (T1, OC-3, etc.), and the length of the contract.
If a qualiﬁed registrant posts a bid, then the service puts the bidder in touch with the service
provider to see if they can work out a deal.

© 2001 by CRC Press LLC
• An interexchange carrier has entered into a partnership with one of Florida’s tourism groups. The
carrier will share its proﬁts with hotel property owners when hotel guests make calls from their
hotel rooms using the carrier’s service.
• Cable operators that provide Internet access services via their cable networks are already dropping
the price of their service in order to capture more of the Internet access market. Some cable
operators see this as the only way to push their Internet access service beyond the early adopters.
These cable providers hope to use their lower prices to attract lighter users and cut into the market
share of ISPs.
• U.S. cell phone users pay for all incoming and outgoing calls that use their phone. About 80 to
85% of all cellular calls originate from a wireless phone — this means that cellular subscribers are
either not giving out their phone numbers or they are turning off their phones. One way to balance
trafﬁc is to upgrade equipment to accommodate calling party pays (CPP) billing. The caller
typically pays 35 to 45 cents a minute, an average rate for an outbound call from a cell phone.
• One reason that domestic long-distance services have not switched to an IP network is because
circuit-switched voice is already cheap: rates are below $0.05 per minute for corporate customers
and below $0.09 for residential customers. The bottom line is that to make the numbers work
domestically requires 10,000 minutes a month to a single location to justify the cost of a private
IP telephony network.
• The cost to complete an international voice call is much higher. Carriers charge as much as
U.S. $4.00 per minute to complete a call to North Korea and other countries where it is hard to
ﬁnd a good termination.
• To make greater wireless penetration and increased billable minutes a reality, carriers must embrace
“calling party pays” (CPP) as the prevalent billing model, rather than “wireless party pays” (WPP).
• Juan Fernandez of Frost & Sullivan reports that when CPP was implemented in Argentina, the
market grew from 700,000 subscribers to 2.1 million in 11 months.
• Giving a customer the ﬁrst incoming minute of a call for free is an interim way that service
providers are trying to increase the number of billable minutes.
• ISPs jumped en masse onto the ﬂat-rate bandwagon in 1996, only to ﬁnd that “all-you-can-eat”
pricing has a way of eating away at the bottom line. Some service providers have found that the
ﬂat-rate strategy delivers something that they wanted to get all along: lots and lots of customers.
• Flat-rate pricing can be a nightmare for providers, especially if their costs are largely dependent on
usage and that usage is difﬁcult to predict. Frame relay and Internet services fall into this category.
• Providers gain from using ﬂat-rate pricing because they don’t have to cover the cost of adminis-
trating usage-base pricing. That can be a signiﬁcant gain considering that these expenses can run
as high as 18% of the total cost of the service.
• Usage-based pricing becomes just as attractive as ﬂat-rate pricing if the cost to deliver a service
increases substantially as service usage grows.
• Wireless service providers are starting to offer prepaid services in order to address the 20 to 40%
of the market that didn’t qualify for service because of bad or nonexistent credit histories.
• Prepaid systems have become more attractive in recent years due to several improvements: they
lacked a real-time billing engine (couldn’t cut off calls in mid-conversation), and they didn’t
accommodate incoming calls.
• Wireless service providers can use either a switch-based or a handset-based approach to imple-
menting prepaid services. Most providers have selected the switch-based approach because it works
with any handset and it is less prone to tampering.
• The initial investment in prepaid infrastructure can be heavy, but payback periods can be quick.
Along with expanding the potential customer base, prepaid wireless lowers the cost of acquiring
a customer, since it eliminates the need to do a credit check.

© 2001 by CRC Press LLC
• Despite the upfront charges (for a phone), prepaid services aren’t necessarily a tough sell to credit-
challenged customers. The per minute charges are comparable to those levied under low- and
mid-tier pricing plans, and they include taxes and interconnection charges. Prepaid customers
aren’t charged monthly access fees.
• Carrier consolidation and interconnection, increased competition, service bundling, and new
technology introductions all are contributing to the need for more intelligent and ﬂexible customer
care and billing systems.
• Convergent billing means using a single billing system to create all bills — it does not necessarily
mean sending a customer a single bill!
• New and existing service providers competing against each other are selling telephone services
that are roughly the same. Their goal is to avoid a commodity war of attrition.
• In the United Kingdom, there are 150 licensed telecommunications providers contending to supply
the country’s 30-odd million adults with ﬁxed wireless, data, voice, and video communications.
• Although pricing is becoming increasingly important in telecommunications (especially voice
telephony service), customer service, branding, billing, and value-added services are all keys to
success.
• Types of carefully constructed rates and calling plans include bundling, demographic proﬁling,
“loss leaders,” incentive schemes, “ﬂattened” prices, calling circles, postalized rates, and special
rates.
• Service providers seek to bundle multiple telecommunications services in order to provide one-
stop shopping for their customers.
• There are concerns that bundling may reduce churn for a company as a whole, but not necessarily
for individual lines of business.
• When customers are asked which company they would use for bundled services, customers
overwhelmingly prefer local and long-distance carriers.
• WorldCom has announced its International Business Links (IBLs) and end-to-end ATM services
within Europe and to the U.S. The announcement is a culmination of its transatlantic Gemini
cable project with Cable & Wireless, together with its European ﬁber-laying activities to link the
former islands of MFS’ metropolitan networks. WorldCom has made a habit of breaking the
traditional telecom mold. This service announcement is no exception. Other ATM services have
been slow to emerge on the commercial market. WorldCom will launch constant bit rate (CBR),
variable bit rate (VBR), near-real time (NRT), and available bit rate (ABR) services, pegged
favorably against existing leased circuit and frame relay tariffs.
4.6.4
Impact of New Technologies
• Networkwide QoS is needed to deliver priority service to higher-paying customers. Service pro-
viders want to use QoS as a basis for offering various classes of services to different segments of
end users. By doing this, they can create different pricing tiers that correspond to QoS levels. That
might be one of the best ways to offer new revenue-generating services in public data networks.
• Smaller ISPs are using centralized functionality to improve their competitive situation. Most ISPs
store subscriber information on up to ﬁve different servers, thus preventing them from using data
mining tools that are essential to customizing services. This opens the door to content-based
billing. Software can be used to create something similar to the call detail records used with voice
calls, but it will consider a subscriber’s proﬁle.
• PCS services have reduced many of the advantages of paging through longer battery life, ﬁrst
minute free, free/bundle voicemail, free caller I.D., prepaid plans for less creditworthy customers,
and competitive pricing.

© 2001 by CRC Press LLC
• From a connectivity perspective, the Internet is well suited for telephony because of its global
reach. From an engineering perspective, it is efﬁcient — a dedicated T1 can support as many as
130 IP voice calls vs. 24 simultaneous calls as in today’s carrier networks.
• When talking about billing for IP services, the two key issues are metering and settlements. Metering
is relatively straightforward. Settlements introduce trouble because the number of billing arrange-
ments between carriers grows exponentially with the number of Internet telephony service providers.
• According to Duane Ackerman, chairman and CEO of BellSouth, 17% of new PCS customers in
Louisiana recently signed up for “untethered” service as a replacement for wireline.
• Finland has the greatest wireless penetration of all markets: 42% at the end of 1997.
• Some service providers — BellSouth and Paciﬁc Bell among them — are now betting millions of
dollars that Web-based electronic billing systems are essential for hooking lucrative but ﬁnicky
business customers — and eventually even some residential ones — who are interested in fast,
responsive billing.
• Many service providers expect less than 5% of all telecom customers to use Internet billing in the
near future.
• On-line billing, however, has its challenges. It not only requires Internet access but is also costly
and complicated to set up, especially for big service providers with massive billing systems already
in place.
• One of the major beneﬁts of electronic billing is that it saves the service provider money. The
more that customers opt to pay their bills through a Web site, the lower the cost of running a
paper-based billing system. By some estimates, the entire paper trail from stufﬁng an envelope,
mailing the bill, and processing the payment costs a service provider 75 cents to $1.50 per account
every billing cycle. BellSouth estimates that it spends 7 cents to send every printed page.
• Initially, the IP did a poor job of tracking and generating the appropriate data to accurately measure
usage for customer billing. Changes are being investigated because of the interest in using IP
telephony for voice and fax.
• Many different usage-based services are currently being planned: least-cost routing, time of day
routing, dynamic bandwidth allocation, volume discount rates, callback, security enhancements,
Web hosting, e-mail, chat lines, whiteboards, videoconferencing, work group collaboration and
multimedia sessions, software applications distribution, applications rental, and classes of service
quality.
• Many technical challenges of IP-based services must be tackled. Foremost are extrapolating and
scrubbing down trafﬁc information from routers and switches and matching that against customer
account data for bills. This invoices tracking packet volumes, counting bits or bytes and logging
origination or destination IP addresses.
4.7 Telecommunications Strategies (Figure 4.11)
4.7.1
Introduction
The brave new world that represents the future of telecommunications will consist of a group of aggressive
global service providers who are competing for the same segments of end users. How each of the service
providers hopes to succeed at the expense of its competitors is a fundamental part of its long-term strategy.
A provider’s strategy for increasing its market share must be in part based on its current situation. In
this section we will look at the current situations that describe many of today’s up-and-coming service
providers as well as some of the well-established players. We will examine their business goals and how
they may go about achieving them. Finally, we’ll identify some of the possible events that could dramat-
ically change existing strategies.

© 2001 by CRC Press LLC
4.7.2
The Players
The value chain of products and services will dictate the positioning of telecommunications service
providers. The positioning process usually starts with answering a number of questions, such as:
• What is the perceived quality of my network?
• Is the network keeping pace with the growth of subscribers?
• How much should be invested?
• Where do I need to invest?
• How can I get more revenue out of existing services?
• How can I reduce operating costs?
• How do I know if a problem is just a solitary abnormality or a building problem?
• How can I reduce customer churn?
• How can I predict future capital expenditures?
• How can I get system usage information to improve marketing and sales?
The traditional value chain was very simple. The equipment suppliers — a closed market of monolithic
suppliers — have provided hardware with hard- or soft-wired integrated services. This equipment was
key for network and service providers who have based their service offers to their customers on the
capabilities of this equipment. Change cycles and service creation were extremely long, hardly meeting
the customer’s expectations.
The actual value chain includes the following principal components (TERP01):
1. Infrastructure
IT component suppliers
OSS application suppliers
Network element suppliers
System integrators
FIGURE 4.11
Trend analysis — telecommunications strategies.
6. Telecommunications
Strategies
1. User
Needs
2. Application
Trends
3. System & Service
lntegration
4. New
Product & Service
Creation
5. Telecommunication
Tariffing

© 2001 by CRC Press LLC
2. Network Products and Services
Network operators
Service providers
3. Hosting and Processing
Hosting services providers
Processing services providers
4. Applications and Media
Applications services providers
Context, content packaging, and management
Content services providers
5. Customer
The players are not yet evenly distributed. Most of them are still emerging from the traditional service
providers, and can be allocated to Network Products and Services. Examples are:
• ILEC (Incumbent Local Exchange Carrier): Strong provider who owns a considerable amount of
telecommunications facilities and doesn’t want to give away this position easily. Most likely,
number of legacy support systems with little interoperability and integration in use. The result is
high operating costs.
• CLEC (Competitive Local Exchange Carrier): Smaller, ﬂexible provider who owns little or no
telecommunications facilities (facility-less). By offering excellent customer care and new services,
they try to build the support structure step-by-step. Their support systems are state-of-the-art,
lightweight, and less expensive to operate. In certain cases, they use service bureaus for billing and
provisioning.
• IEX (Inter Exchange Carriers): Primarily responsible for long-distance services with stepwise
penetration of the local exchange area. They can be both incumbent and competitive providers
with the result of the need for very heterogeneous support systems.
• PTT (Post, Telegraphy, and Telephone): Strong provider who owns a considerable amount of
telecommunications facilities and doesn’t want to give away this position easily. Most likely,
number of legacy support systems with little interoperability and integration in use. The result is
high operating costs.
• CAP (Competitive Access Provider): Facilities-based or non-facilities-based; similar to the ILEC,
but have carefully selected local loops for high-proﬁt commercial customers.
• NSP (Network Service Provider): Responsible for providing a highly reliable networking infra-
structure, consisting of equipment and facilities. Its responsibilities are usually limited to the
physical network only, but element management systems are usually included into their offers.
However, integration is important for many customers. Thus, ISPs and ICPs will play an important
role as well. The short deﬁnitions are:
• ISP (Internet Services Provider): Its main goal is to provide Internet access to business and attract
customers. Major challenges include peering to each other and to other carriers, managing quality,
and offering acceptable performance.
• ICP (Integration Communications Provider): Emerging provider with integrated services offer,
concentrating on next generation, high-speed data and wireless services, in particular for proﬁtable
business users. Its acceptance in the marketplace is expected to be high. In terms of support
systems, they buy instead of build; occasionally, they use service bureaus for billing and provi-
sioning. They take advantage of the fact that intranet, extranet, virtual private networks, eCom-
merce, and multimedia applications require more bandwidth than is available over traditional
circuit-switched voice networks.

© 2001 by CRC Press LLC
Hosting and processing will be most likely dominated by traditional mainframe and server manufacturers
that are ﬂexible enough to make the necessary facelifts to their equipment to meet requirements of load
distribution, load balancing, storage management, and security. IBM and Compaq may be mentioned here
as examples of providing reasonable services, using server farms with high availability features.
Application and media need many new competitive players. At the beginning, ASPs and ESPs will
dominate this market. The short deﬁnitions are:
• ASP (Application Services Provider): Emerging service provider, which must combine application,
systems, and network management. Service level expectations are extremely high; the whole
business of customers may rely on this provider.
• ESP (Enterprise Services Provider): Emerging service provider from the enterprise environment.
It offers services for a limited user community with similar attributes to the provider. It uses and
customizes its existing support systems that may not scale well.
Hosting and processing enable Web presence and interactivity on the Internet. They are typically
provided by ISPs and NextGen service providers which are active in IP services. They mainly include
hosting of Web server infrastructures and content and Web-enabled transaction software and hardware
which allow the execution of online transactions. It is an infrastructure type of activity, although it is
characterized by added value and signiﬁcant amount of additional services. Service offer alternatives are:
• Web hosting: Keeping content on Web server farms and offering access with good performance
• Value-added Web hosting: In addition, content, and database maintenance and Webmaster services
• Data hosting by offering Storage Area Networks
• Data management services, including search machines
• Public Key Infrastructure services, including trust center functions
• Centralized Web transaction services
• Web community and Internet account management
• Transaction authentication services
The typical customers of these services are businesses. While large businesses previously deployed their
own Web infrastructure in-house, they now also realize the efﬁciencies of lower complexity and economics
of scale given by professional service providers. This customer base can easily extend to the future. The
key differentiators will be the service level and complexity of services offered to customers. Hosting and
value-added hosting emerge as a volume business. Large data centers with server farms, load balancers,
and trafﬁc shapers combined with high availability and excellent performance will take business away
from smaller service providers with lower availability and limited Internet access capability. Most service
providers are inexperienced in this area. 
The continuation of the value chain is dominated by innovative services that are to a certain extent
IP-based. It means that the traditional circuit-switched architectures are replaced by packet-switched
architectures. For the underlying physical architecture, there are many choices, including:
1. IP + ATM + SDH/Sonet = B-ISDN
Traditional approach, which has the most supporting net-
work elements and their element managers
2. ATM transport
Includes both SDH/Sonet-less ATM transport and
ATM/SDH/Sonet hybrids
3. Switched routing
ATM/IP hybrids
4. IP over SDH/Sonet
PPP or HDLC-framed IP mapped to SDH/Sonet
5. Optical IP
Transport of PPP or HDLC-framed IP over WDM with fast
photonic restoration
6. Use of enhanced frame relay
Substitution of ATM by frame relay in any of the
approaches 1, 2 or 3
There is no doubt that the new area of competition is content. 

© 2001 by CRC Press LLC
Content delivery management is taking off and service providers are well positioned to earn revenues
there. Content delivery management helps content owners to provide seamless and fast website access
for customers by
• Large scale caching
• Distribution of Web server farms
• Complex Internet routing services on managed network segments
All these aspects help to deliver reasonable performance. Processing is an increasingly important
revenue generation opportunity as traditional infrastructure business shrinks. Transaction, and therefore
processing, which is the infrastructure and software enabler of transactions, is believed to grow to become
the single most important revenue input of the Internet value chain. Processing by no means is related
to the core business of service providers, but it is important for eCommerce service offers. Services can
be created by the IT organization of service providers in collaboration with systems integrators.
The Application and Media elements of the value chain create and translate traditional and digital
content into Web-ready format and creates the actual interface between the digital product and the
customer. This service is targeting an end-to-end process which covers creation, manufacturing, delivery,
and presentation of content to customers. This is believed to be the most promising business opportunity
of the Internet. It carries the highest growth potential, but at the same time the highest risks, too.
Telecommunications service providers, IT companies, media enterprises, retail chains, and several other
industries are competing for revenues. 
Service offers are:
• Application Services to be provided by service providers, integrating IT, software, system integra-
tion, telecommunication, and consulting skills.
• Content authoring, auditing, deployment, and maintenance combined with bandwidth manage-
ment, server-load management and trafﬁc management, supporting generic, corporate, and spe-
cialized niche portals, and B2B and B2C operations.
• Content creation targeting videos, movies, audio, photo archives, encyclopedic articles, analyst
reports, ﬁnancial evaluation, and many others. Music and also written material combine with
broadband access to support multimedia to be delivered over the Internet.
These innovative service areas must be seriously investigated by service providers. In other traditional
areas, the proﬁt margins are narrowing; in the IP area they have to face other competition. To be successful,
innovative minds are required. It means more collaboration with customers, mergers, acquisitions,
investment into smaller companies that may be acquired later, and ﬂexibility in service creation, fulﬁll-
ment, and quality assurance.
Another Internet-based service is Immersive Photography, allowing customers to use PCs to navigate
around a digitized 360-degree photo. This technology is targeted toward Internet retailers who want to
give customers a wraparound view of their goods, such as high-end real estate agents, travel agents, cruise
lines and destination marketers. 
Whether retailers are between service providers and customers depends on the marketplace. No general
guidelines can be given in this respect.
4.7.3
Goals
The goals are different for each cluster of service providers. Table 4.4 summarizes the most obvious goals
and future targets for each cluster of service providers, referenced in segment 4.7.2 (TERP01).
References
TERP01Terplan, K.: OSS Essentials: Support System Solutions for Service Providers, John Wiley & Sons,
New York, 2001 (in production).

© 2001 by CRC Press LLC
TABLE 4.4 
Goals and Future Business Targets for Service Provider Clusters
Service Provider Clusters
Goals 
Business Targets
Infrastructure
IT component suppliers
Sell more software
Sell professional services
Replace legacy solutions
Acquire OSS application suppliers
Integrate legacy and innovative systems
OSS application suppliers
Sell more software
Sell more professional services
Full-line of offerings of support systems
Target ILEC legacy replacement
Acquire other vendors of support systems
Compete with system integrators
Network element suppliers
Sell more equipment via best of 
breed and best of suite offers
Outsource element management systems to 
vendors of support systems
Use of open interfaces
Develop solutions for eCommerce
System integrators
Sell custom design, development, 
and deployment
Sell custom integration
Sell consulting
Acquire vendors of support systems
Conduct many projects
Consolidate products
Compete with OSS suppliers
Network Products and Services
Network operators (ILECs, 
PTTs, IEXs, NSPs, CAPs 
and global carriers)
Rapid introduction of new services
Cost reduction
Customer retention
Multi-vendor management
Convergent ordering
Up-to-date asset management
Less internal software development
More use of systems integrators
More packaged software of support systems
Pervasive interconnection of support systems
Customer relationship management
Self-care with support systems for customers
Service providers (CLECs, 
ISPs, ICPs)
Build network capacity
Customer acquisition
Improve service quality
Add facilities
More carrier interconnection
Support of micropayment and 
prepaid services
Minimal internal development
Automated processes
More packaged software for support systems
Less service bureaus
Integration of support systems
Customer relationship management
Self-care with support systems for customers
Hosting and Processing
Hosting (mainframe 
manufacturers, server 
manufacturers)
Use existing storage resources
Reengineer business processes
Use load balancers
Penetrate the Web market
Support of eCommerce
Advanced asset management
Support of Storage Area Networks (SAN)
Processing (mainframe 
manufacturers, server 
manufacturers)
Use existing processing resources
Reengineer business processes
Use caching
Penetrate the Web market
Support of eCommerce
Advanced asset management
Applications and Media
Applications (ASPs, ESPs)
Sell service
Customer acquisition
Early proﬁtability
Resource integration
Good management of the infrastructure
Advanced asset management
Excellent service levels
Use of packaged software
Context, content packaging 
& management (ISPs, 
ESPs)
Real-time rating
Service creation on-the-ﬂy
Mid-range proﬁtability
Usage-based billing
Multimedia support
Multicasting for distribution
Content providers (ISPs, 
ASPs)
Real-time rating
Service creation on-the-ﬂy
Mid-range proﬁtability
Billing for content value
Web switching technology

© 2001 by CRC Press LLC
Customer
Increase service reliability
Lower transport costs
Faster service provider 
responsiveness
Customer network management
Self provisioning via Web
Custom quality of service reporting
Flexible billing formats
Electronic bill presentment and payment
Usage-based accounting
TABLE 4.4 (continued)
Goals and Future Business Targets for Service Provider Clusters
Service Provider Clusters
Goals 
Business Targets


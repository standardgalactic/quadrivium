

Demystifying AWS
Unleash the Power of the Cloud
Dive into AWS with this guide, designed
for hands-on learning!
Explore core services, from storage to
compute, with step-by-step examples.
Mukesh Kumar Das

About the Author
Mukesh Kumar Das is an experienced IT professional with
more than 20 years of expertise. He has held several positions
such as consultant, architect, and manager in multinational
corporations.
He is having extensive knowledge and experience in
infrastructure management, cloud computing and migration,
program management as well as other technologies beyond
AWS, such as AI and automation.
Mukesh is having certification such as AWS Solution Architect,
Azure (AZ-900, DP-900, AI-900), ITIL Expert and PMP.
Previously, he authored "Cloud Migration - The Definitive
Guide", demonstrating his passion for knowledge sharing.
In this book, Mukesh Kumar Das leverages his extensive
experience to construct a solid understanding for AWS Cloud
technology by the help of theory and practical approach.

TABLE OF CONTENTS
Chapter 1 Introduction to Cloud Computing
1. What is Cloud Computing
2. Deployment Models of the Cloud
2.1 Private cloud
2.2 Public cloud
2.3 Hybrid cloud
3. Cloud computing service models
3.1 Infrastructure as a Service (IaaS)
3.2 Platform as a Service (PaaS)
3.3 Software as a Service (SaaS)
4. Relationship between different service models:
5. What is cloud service provider
6. Multi-Choice Question and Answer
Chapter 2 Amazon Web Service (AWS)
1. Overview of Amazon Web Services (AWS)
2. AWS Cloud History
3. AWS Services
4. Benefits of the AWS cloud
5. AWS Global Infrastructure: A Digital Ecosystem
5.1. Regions
5.2 Availability Zones:
5.3 Edge Locations
5.4 Regional Edge Cache
6. Different method to access or connect AWS
7. AWS Higher Availability-Brief Overview
8. AWS Shared Responsibility Model

9. Multi-Choice Question and Answer
Chapter 3 Amazon Identity and Access Management (IAM)
1.  Identity and Access Management
1.1 IAM Authentication Process
1.1. IAM Authorization Process
1.3 IAM Auditing Process
2. IAM: Users and Groups and Roles
3. IAM Guidelines & Best Practices
4. Multi-Choice Questions and Answer
Chapter 4 AWS Compute Services
1. Different AWS compute services
2. Amazon Elastic Compute Cloud (Amazon EC2)
2.1. Different type of Amazon EC2 instances
2.2 Different pricing models for Amazon EC2 instances
2.3 Amazon Machine Image (AMI)
2.4 Classical Port in Windows and Linux
2.5 Create Setup to use Amazon EC2
3.Get Information about your instance
4. Connect Newley created EC2 Instance
5. Connect to an EC2 instance using PuTTY
6. Multi-Choice Questions and Answer
Chapter 5 AWS Load Balancer and Autoscaling
1. Overview of Load Balancer
2. Different type of Load Balancer
2.1 Application Load Balancer (ALB)
2.2 Network Load balancer

2.3 Gateway Load Balancer (GLB)
2.4 Classic Load Balancer (CLB)
3. What’s an Auto Scaling Group
3.1 Auto Scaling Components
4. Multi-Choice Questions and Answer
Chapter 6 Amazon Storage Service
1. AWS Storage System
2. Simple Storage Service(S3)
2.1 S3 – What is Bucket?
2.2 Different Futures in Simple Storage Service(S3)
3. Amazon S3 – Static Website Hosting
4. Amazon S3 Replication
5. Amazon S3 Transfer Acceleration
6. Multi-Choice Questions and Answers
Chapter 7 Amazon Virtual Private Cloud (VPC)
1. Overview of Amazon Virtual Private Cloud
2. CIDR Block – Classic Inter-Domain Routing block
3. Amazon Virtual Private Cloud (Amazon VPC) components
3.1 Subnet
3.2 Route Table
3.3 Security groups
3.4 Network Access Control Lists (ACLs)
3.5 NAT Instance
3.6 NAT gateway
4. VPC with All components:
5. VPC Endpoints
6. VPC Connectivity Methods

6.1 VPC Transition Peering
6.2 VPC Transit Gateways
6.3 AWS Site-to-Site VPN
7. Multi-Choice Questions and Answer
Chapter 8 Amazon Route53
1. Amazon Route 53
2. Building Block of Route53: Route, Alias and CNAME
3. Differences between alias and CNAME records
4. How Route 53 Works
5. Route53 - Routing Policy
5.1 Simple routing policy
5.2 Weighted routing policy
5.3 Failover routing policy
5.4 Geolocation routing policy
6. Multi-Choice Questions and Answer
Chapter 9 Amazon Database Services
1. Overview of AWS Databases
1.1 Relational database
1.2 Non-Relational Database
2. Different Available AWS Databases:
3. Choosing the right AWS database
3.1 EC2 hosted Databases
3.2 AWS managed databases
4. Amazon Relational Database Service (RDS)
4.1 Amazon Aurora
4.2 Aurora Global Database
5. Amazon DynamoDB

5.1 Amazon DynamoDB Accelerator (DAX)
5.2 DynamoDB – Global Tables
6. Amazon Redshift
7. AWS memory-based databases:
7.1 Amazon ElastiCache
8. Multi-Choice Questions and Answer
Chapter 10 Miscellaneous Services
1. Amazon CloudFront
1.1 How does CloudFront work?
2. Amazon CloudWatch
2.1 Amazon CloudWatch Important Metrics
2.2 Amazon CloudWatch Alarms
2.3 Amazon CloudWatch Logs
3. AWS CloudTrail
3.1 CloudTrail Working Methodology
3.2 Auditing with AWS CloudTrail, AWS Audit Manager, and AWS Config
4. Multi-Choice Questions and Answer
AWS pricing plans
1.1. Compute Purchasing Option in AWS
1.2 Savings Plans
1.3 Capacity Reservations
Case Study
AWS Highly Scalable Application Architecture
Quick References
Important AWS Links

Different AWS Service Logo for Reference

CHAPTER 1  INTRODUCTION TO
CLOUD COMPUTING

1.  WHAT IS CLOUD COMPUTING
Cloud computing is the delivery of IT
resources and services over the internet. These services are
available on demand, with pay-as-you-go pricing.
This means that you can access servers, storage, databases,
and applications without having to build and maintain your own
infrastructure. You can simply provision the resources you need,
when you need them, and pay only for what you use. Also, Cloud
computing is a flexible and scalable solution for businesses of all
sizes. It can help you to reduce costs, improve efficiency, and
accelerate innovation.
Here are some applications for cloud computing:
•  To host a website or application: Cloud computing may be
used to host a wide range of websites and apps, from basic
personal webpages to big business programs.
•  Data storage: Cloud computing may be used to store many
forms of data, including files, databases, and backups.
•  Software development and testing: Cloud computing may be
used to build and test software without the need for costly on-

premises gear and software.
•  To execute HPC workloads: Cloud computing may be utilized to
run HPC tasks such as scientific simulations and video rendering.
Define On-Prem or traditional IT infrastructure
and Associate Problems
Traditional IT infrastructure is consisting of the physical
hardware and software components that businesses use to run
their applications and store their data. This infrastructure can be
expensive to purchase and maintain, and it can be difficult to
scale up or down to meet changing needs.
Below are certain issues associated with conventional IT
infrastructure:
  Higher Initial Capital Investment: Traditional IT
infrastructure can be expensive to purchase and deploy. This
might provide a hindrance for small enterprises and startups
trying to enter the market.
  Ongoing maintenance costs: Traditional IT infrastructure
requires ongoing maintenance and support. This can add up to a

significant expense over time.
  Lack of scalability: Conventional IT infrastructure may pose
challenges when it comes to adjusting capacity to accommodate
fluctuating demands. This might provide a challenge for
businesses that encounter periodic variations in demand.
  Security risks: Traditional IT infrastructure can be vulnerable
to security attacks. Businesses need to invest in security
measures to safeguard their data and systems.
  Complexity: Traditional IT infrastructure can be complex to
manage. This can require a team of expert IT professionals.
Cloud computing can assist organizations in overcoming
many of the issues associated with traditional IT infrastructure.
Because cloud computing is a pay-as-you-go service, businesses
do not need to make a significant initial investment. Cloud
computing possesses the qualities of scalability, security, and
remote manageability.
Advantage of cloud computing
Cloud computing has several benefits compared to
conventional IT infrastructure, such as:
  Cost savings: Cloud computing enables enterprises to
achieve cost savings by eliminating the necessity of procuring
and managing their own hardware and software.

  Scalability: Cloud computing offers the capacity to simply
adjust the amount of resources allocated to a firm, allowing for
effortless addition or removal of resources as required. This
might be advantageous for organizations that experience
seasonal fluctuations in demand.
  Agility: Cloud computing can help businesses to be more
agile and responsive to change. Businesses can quickly deploy
new applications and services, or scale up or down their
resources as needed.
  Security: Cloud service providers provide a range of security
solutions to safeguard the data and systems of their customers.
  Reliability: Cloud companies implement multiple systems
and architecture to guarantee exceptional availability and
dependability.
cloud computing offers several advantages over traditional IT
infrastructure. It can help businesses to save money, improve
scalability and agility, and enhance security.
Here are some examples of how cloud computing is being
used by businesses today to gain advantages:
  A small business can leverage cloud computing to host its
website and email server, eliminating the need for costly
investments in hardware and software.

  A medium-sized business can utilize cloud computing to
operate its CRM and ERP systems, without the need to expand
its infrastructure to accommodate high demand
  A large enterprise can leverage cloud computing to create
and evaluate new applications, without the necessity of
constructing and managing its own exclusive development
environment.
Cloud computing is a rapidly growing industry, and it is
changing the way that businesses of all sizes use IT. If you are
looking for a way to improve your IT efficiency and agility, cloud
computing is a good option to consider.
The Five Characteristics of Cloud Computing
• On-demand self-
service: Users can provision resources and use them without

human interaction from the service provider.
• Broad network access: Resources available over the
network, and can be accessed by diverse client platforms
• Multi-tenancy and resource pooling: Multiple
customers can share the same infrastructure and applications
with security and privacy. Multiple customers are serviced from
the same physical resources
• Rapid elasticity and scalability:  Automatically and
quickly acquire and dispose resources when needed quickly and
easily scale based on demand
• Measured service: Usage is measured, users pay
correctly for what they have used

2.  DEPLOYMENT MODELS OF THE
CLOUD
A deployment model in cloud computing refers to the method
of deploying and managing servers, storage, and other
resources. It focuses on determining the ownership and
control of the infrastructure, as well as the accessibility
granted to users. There are three main deployment models of
the cloud:
  Public cloud
  Private Cloud
  Hybrid Cloud

2.1  PRIVATE CLOUD
A private cloud is a cloud computing environment that is
dedicated to a single organization. It can be hosted on-
premises or by a third-party provider, but it is not accessible
to the public. Private clouds offer a high degree of control and
security, and they can be customized to meet the specific
needs of the organization.
Examples of Private Cloud:
  Deutsche Bank: Uses a private cloud infrastructure to
handle sensitive financial data and transactions. This model
ensures maximum control and security for their critical
financial systems.
  General Motors: Employs a private cloud for research and
development activities, allowing them to securely test and
deploy new technologies within their own environment.

  Johnson & Johnson: Leverages a private cloud for internal
applications and data analytics, enabling them to maintain
tight control over sensitive healthcare data.

2.2  PUBLIC CLOUD
A public cloud is a cloud
computing environment that is shared by multiple
organizations. It is owned and operated by a third-party cloud
service provider, and it is accessible to the public over the
internet. Public clouds are typically the most cost-effective
cloud deployment model, and they offer a wide range of
services.
Examples of Public cloud:
  Netflix: Streams movies and TV shows to millions of users
worldwide using AWS, a public cloud platform. The scalability
and cost-effectiveness of the public cloud allow Netflix to
handle fluctuating demand seamlessly.
  Dropbox: Stores files and folders for millions of users using
Google Cloud Platform. This public cloud model allows
Dropbox to offer affordable storage solutions without
managing its own infrastructure.

  Airbnb: Matches guests with accommodations around the
world using Microsoft Azure. The public cloud provides the
necessary computing power and network resources to
manage bookings and listings across the globe.

2.3  HYBRID CLOUD
A hybrid cloud is a cloud
computing environment that combines both private and
public clouds. This allows organizations to keep some of their
data and applications on-premises, while taking advantage of
the cost-effectiveness and scalability of the public cloud.
Hybrid clouds can be complex to manage, but they offer a
good balance of control, security, and cost-effectiveness.
Example of hybrid cloud:
  BMW: Combines AWS for public-facing applications like their
website and customer portal with a private cloud for sensitive
production data and manufacturing processes. This hybrid
approach balances scalability and security based on specific
needs.
  Siemens: Uses a hybrid cloud to connect its on-premises
infrastructure with Azure for IoT applications. This allows
them to collect and analyze data from connected devices in
real-time while keeping sensitive industrial information
secure.

  Starbucks: Utilizes a hybrid cloud strategy, running their
loyalty program and mobile app on AWS while keeping
customer financial data on a private cloud. This approach
offers flexibility while ensuring secure handling of sensitive
data.

3.  CLOUD COMPUTING SERVICE
MODELS
Cloud computing offers different service models, each
providing access to resources in distinct ways. Understanding
these models empowers you to choose the best fit for your
specific needs. The three main types of cloud computing are
  Infrastructure as a Service (IaaS)
  Platform as a Service (PaaS)
  Software as a Service (SaaS).

3.1  INFRASTRUCTURE AS A
SERVICE (IAAS)
IaaS provides the basic building blocks of cloud computing, such
as computing, storage, and networking resources. IaaS
customers can provision and manage these resources on
demand, without having to purchase and maintain their own
hardware and software.
In other words, IaaS is like renting a space in a data center
and having access to all the necessary resources, such as
servers, storage, and networking, without having to worry about
the management and maintenance of the hardware and
software. This can be a great way to save money and resources,
especially for small businesses.
One of the key benefits of IaaS is that it is scalable. This
means that you can easily add or remove resources as needed,
based on your business demands. This can be very helpful for
businesses that experience seasonal fluctuations in traffic or
that are growing rapidly.
IaaS is also a very secure way to store and process data.
Cloud providers offer a variety of security features to protect
their customers' data, including encryption, access control, and
intrusion detection.

Examples of IaaS providers include Amazon Web Services
(AWS), Microsoft Azure, and Google Cloud Platform (GCP).

3.2  PLATFORM AS A SERVICE
(PAAS)
PaaS provides a platform for developers to build, deploy, and
manage applications. PaaS customers can focus on developing
their applications, without having to worry about the underlying
infrastructure. Examples of PaaS providers include AWS Elastic
Beanstalk, Azure App Service, and Google App Engine.

3.3  SOFTWARE AS A SERVICE
(SAAS)
SaaS delivers software applications over the internet. SaaS
customers can access and use these applications without having
to install or manage any software on their own devices.
In case of SaaS application login run in the cloud whereas
non-SaaS application it is running on user’s computer.
Examples of SaaS providers include Salesforce, Microsoft
Office 365, and Google Workspace.
A SaaS application may be accessed through a browser or
through an app. Online email applications that the user access
through a browser, such as Gmail and Office 365, are common
examples of SaaS applications.
SaaS users subscribe to an application rather than purchasing
it once and installing it. Users can log into and use a SaaS
application from any compatible device over the Internet. The
actual application runs in cloud servers that may be far removed
from a user's location.
Organizations can choose to use one or more types of cloud
computing, depending on their needs. For example, a small

business might use SaaS for all of its IT needs, while a large
enterprise might use a combination of IaaS, PaaS, and SaaS.

4. RELATIONSHIP BETWEEN
DIFFERENT SERVICE MODELS:
The pyramid shape suggests that IaaS is
the foundational layer, with PaaS and SaaS building upon it. This
is an accurate representation of the relationship between these
service models.
IaaS provides the basic building blocks of IT
infrastructure, such as virtual servers, storage, and
networking. Users have full control over the underlying
infrastructure and manage their own operating systems and
applications. PaaS sits on top of IaaS and provides a platform for
developing, deploying, and managing applications SaaS sits on
top of PaaS and delivers software applications over the
internet, typically on a subscription basis. Users don't have to
install or manage the software themselves; they simply access it
through a web browser or mobile app.
Services in scope for On-prem, IaaS, PaaS and
SaaS

As we aware that traditionally we are having different
services running in on-prem as well as cloud environment like
application, data, middleware, OS, Server, Storage etc.
Now we need to understand different resources managed by
customer (or You) and Cloud Service Provider as per different
service models and On-Prem environment.
Based on the diagram above, the services in scope for each
cloud service provider are as follows:
Infrastructure as a Service (IaaS):
  Computing: Servers, virtual machines, containers
  Storage: Block storage, object storage, file storage
  Networking: Virtual networks, load balancers, firewalls etc.
Platform as a Service (PaaS):

  Application development platforms: Java, Python, Node.js,
Ruby, etc.
  Integration platforms as a service (iPaaS): Connecting and
integrating applications and data
  API management: Creating, publishing, and managing APIs
  Mobile back-ends: Developing and deploying mobile
applications
  Other services: Container orchestration, serverless computing,
managed databases
  All IaaS Components
Software as a Service (SaaS):
  All service needs to manage by Cloud Service Provider (CSP)
On-Premises Environment:
  All services need to manage by customer
Hence in the case of On-Premises, you or customer need to
manage everything whereas in SaaS it is fully managed by Cloud
Service provider.

5.WHAT IS CLOUD SERVICE
PROVIDER
A cloud service provider (CSP) is a company that provides cloud
computing services to businesses and individuals. CSPs offer a
wide range of services, including infrastructure as a service
(IaaS), platform as a service (PaaS), and software as a service
(SaaS). Some of the key cloud service provider are as follows:
Cloud Service Provider’s play a vital role in the global
economy. They help businesses of all sizes to access the latest IT
resources and services without having to make a large upfront
investment. Here's a brief summary of the top 5 contenders:
  
  Amazon Web Services (AWS)
❍  Market leader: Pioneered the cloud computing market,
boasting the most extensive service portfolio and global

infrastructure.
❍  Strengths: Broad range of services, scalability, reliability,
mature offerings.
❍  Weaknesses: Can be complex for beginners, potentially higher
costs for certain services.
  
  Microsoft Azure
❍  Integration: Seamless integration with Microsoft products and
services like Office 365.
❍  Strengths: Strong hybrid cloud capabilities, security focus,
enterprise-friendly tools.
❍  Weaknesses: Less extensive service portfolio compared to
AWS, vendor lock-in concerns.
  
 Google Cloud Platform (GCP)
❍  AI & ML: Leader in artificial intelligence and machine learning
with cutting-edge tools like BigQuery and TensorFlow.
❍  Strengths: Competitive pricing, open-source friendly,
innovative solutions.
❍  Weaknesses: Smaller market share compared to AWS and
Azure, less mature in some areas.

  
  IBM Cloud
❍  Focus: Blockchain, enterprise applications, and hybrid cloud
solutions.
❍  Strengths: Strong focus on security and compliance, robust
enterprise services.
❍  Weaknesses: Limited-service portfolio compared to other
giants, less user-friendly interface.
  
  Alibaba Cloud
❍  Dominant in China: Largest cloud provider in China, offering
competitive services.
❍  Strengths: Strong local presence, competitive pricing, focus on
big data and AI.
❍  Weaknesses: Limited global reach outside China, language
barrier for non-Mandarin speakers.
Cloud Service Provider’s offer a wide range of services, so it is
important to choose a provider that meets your specific needs.
Factors to consider includes:
  Types of services that you need,
  Level of control that you require

  Security features that are important to you.

6. MULTI-CHOICE QUESTION AND
ANSWER
1.  Which of the following is NOT a service model in cloud
computing?
a) Infrastructure as a Service (IaaS)
b) Platform as a Service (PaaS)
c) Software as a Service (SaaS)
d) Data as a Service (DaaS)
Answer: d) Data as a Service
Explanation: (DaaS) is not a standard service model in
cloud computing. It refers to accessing and managing data
on-demand as a cloud service, but it generally falls under the
IaaS or PaaS categories depending on the context.
2. What is the main benefit of using a public cloud provider
like AWS or Azure?
a) Increased security and control
b) Greater cost-effectiveness and scalability
c) Reduced maintenance and technical expertise required
d) All of the above
Answer: d) All of the above.

Explanation: Public cloud providers offer economies of
scale, pay-as-you-go pricing, and managed infrastructure,
allowing businesses to operate cost-effectively and scale
resources up or down as needed, while reducing the need for
in-house expertise.
3. What is the main drawback of using a public cloud
compared to a private cloud?
a) Higher costs and potential vendor lock-in
b) Less control over security and data privacy
c) Limited availability and reliability
d) All of the above
Answer: b) Less control over security and data privacy.
Explanation: Public clouds share infrastructure with other
users, reducing the level of control over security and data
privacy compared to a private cloud, which is dedicated to a
single organization.
4. What is the term for automatically adjusting the number
of resources in a cloud application based on demand?
a) Load balancing
b) High availability
c) Auto scaling
d) Resource pooling

Answer: c) Auto scaling.
Explanation: Auto scaling dynamically adjusts the number
of resources (e.g., servers) in a cloud application based on
real-time demand, maximizing performance and optimizing
costs.
5.Which of the following is NOT a common security concern
when using cloud computing?
a) Data breaches and unauthorized access
b) Compliance with data privacy regulations
c) Dependence on vendor security practices
d) Physical hardware failure
Answer: d) Physical hardware failure.
Explanation: Cloud providers manage the physical
infrastructure, leaving hardware failures less of a concern for
users compared to on-premises deployments. However, other
security concerns like data breaches, compliance, and
reliance on vendor security still need to be considered.

CHAPTER 2  AMAZON WEB
SERVICE (AWS)

1.  OVERVIEW OF AMAZON WEB
SERVICES (AWS)
The world's largest and most popular cloud platform is
Amazon Web Services (AWS), which is run from data centers
all over the world. It provides more than 200 fully prepared
services. AWS is used by many clients, such as large
businesses, well-known government agencies, and startups
that are growing quickly, to reduce costs, improve flexibility,
and speed up innovation.
AWS provides a comprehensive range of global computing,
storage, database, analytics, application, and deployment
services that assist enterprises in increasing their speed,
reducing IT expenses, and expanding their applications.
These services may be accessed whenever needed and are
charged on either an hourly or monthly basis, ensuring that
businesses only pay for what they use.
The Gartner Magic Quadrant for
Cloud Infrastructure & Platform
Services:
A company's ability to execute its vision and its
completeness in comparison to other technologies and
market standards in a certain market are represented
graphically in the Gartner Magic Quadrant. It is a widely
respected tool for evaluating vendors in a particular market.

The Gartner Magic Quadrant for Cloud Infrastructure &
Platform Services (CIPS) graph illustrates the degree to which
a company possesses the capacity to execute its vision in a
defined market, relative to other technologies and market
standards.
There are four quadrants in the Magic Quadrant:
  Leaders: Leaders execute well against their current vision
and are well positioned for tomorrow.
  Visionaries: Visionaries understand where the market is
going or have a vision for changing market rules, but do not
yet execute well.
  Challengers: Challengers have a strong market presence
and perform well today, but they may lack innovation or may
be facing disruption from new technologies.
  Niche Players:  Niche Players either lack focus and fail to
outperform or innovate others, or they successfully
concentrate on a narrow market area.

The 2022 Gartner Magic Quadrant for Cloud Infrastructure
& Platform Services (CIPS) graph is shown below:
As per the above diagram related to Year 2022 leaders for
the Cloud infrastructure and platform services Magic
Quadrant include:
  Amazon Web Services
  Microsoft
  Google
AWS is regarded as the market leader in terms of breadth
and depth of capabilities, and has contributed to the
establishment of industry standards. Furthermore, the vendor
currently holds the largest market share in the industry,
surpassing Microsoft by a factor of two times. Additionally,
the company's healthy and successful ecosystem attracts

partners, thereby contributing to the expansion of its market
share.
Nevertheless, AWS has a tendency to prioritize short-term
gains in its customer interactions, according to Gartner,
which may result in lower brand loyalty.

2.AWS CLOUD HISTORY
In the early 2000s, Amazon engineers were looking for a way
to make the company's infrastructure more efficient and
scalable. This is where the idea for AWS came from. Amazon
had a huge e-commerce business at the time, and it needed
a better way to handle its infrastructure.
Amazon Simple Storage Service (S3) was the first major
service that AWS offered in 2006. For organizations, S3
offered a scalable and reliable storage solution. Amazon
Elastic Compute Cloud (EC2), which offered scalable
computing resources in the cloud, was introduced by AWS in
2008.
AWS continued to grow quickly over the next few years.
AWS opened its first cloud computing area outside of the US
in 2013 In 2014, AWS announced that it had passed $10
billion in revenue.
Currently, AWS is the world's leading cloud computing
platform. It offers over 200 services to millions of customers
worldwide. AWS is used by businesses of all sizes, from
startups to Fortune 500 firms. Some of the key milestones in
AWS history include:
  2003: Amazon begins to develop AWS internally.
  2006: AWS launches its first major service, Amazon Simple Storage Service
(S3).
  2008: AWS launches Amazon Elastic Compute Cloud (EC2).

  2010: Amazon moves its own e-commerce business to AWS.
  2013: AWS launches its first cloud computing region outside of the United
States.
  2014: AWS announces that it has passed $10 billion in revenue.
  2015: AWS launches Amazon Elasticsearch Service and Amazon Managed
Services for Hadoop.
  2016: AWS launches Amazon Lambda and Amazon SageMaker.
  2017: AWS launches Amazon Rekognition and Amazon Comprehend.
  2018: AWS launches Amazon SageMaker Neo and Amazon Outposts.
  2019: AWS launches AWS Wavelength and Amazon Managed Services for
Kubernetes.
  2020: AWS launches AWS Graviton2 processors and Amazon Quantum Ledger
Database.
  2021: AWS launches Amazon Aurora Serverless v2 and Amazon SageMaker
Canvas.
  2022: AWS launches Amazon Nimble Studio and Amazon CodeCatalyst.

3.AWS SERVICES
A wide range of cloud computing services that are provided by
Amazon Web Services (AWS) are referred to as AWS Services.
These services are constantly being expanded. A wide variety of
functions are made available by them, which can be utilized for
the purpose of creating, deploying, and managing workloads
and applications in the cloud. You can find different AWS
products and service in this link:
https://aws.amazon.com/products/ in product categories.
These services span across various categories, including:
Compute:
  Virtual servers for running applications (EC2)
  Serverless functions for running code without managing
infrastructure (Lambda)

  Container orchestration for managing and deploying
containers (ECS, EKS)
  Batch computing for running large-scale, high-performance
workloads (Batch)
Storage:
  Object storage for any type of data (S3)
  Block storage for EC2 instances (EBS)
  Managed file systems for EC2 instances (EFS)
  Hybrid cloud storage for connecting on-premises data to AWS
(File Storage Gateway)
  Low-cost archive storage for long-term data retention (S3
Glacier, S3 Glacier Deep Archive)
Database:
  Managed relational databases (RDS, Aurora)
  NoSQL databases for high-performance applications
(DynamoDB)
  Managed data warehouses for large-scale data analysis
(Redshift)
  Managed document databases (DocumentDB)

  Graph databases for storing and analyzing relationships
(Neptune)
Networking:
  Private virtual networks (VPC)
  Private connections between on-premises networks and AWS
(Direct Connect)
  DNS services (Route 53)
  Network traffic management (Transit Gateway)
  Load balancing for distributing traffic across multiple targets
(Application Load Balancer)
  Managed firewalls (Network Firewall)
Other Notable Categories:
  Security services for protecting applications and data
  Analytics services for analyzing data and building machine
learning models
  Artificial intelligence services for developing and deploying AI
applications

  Application development services for building and deploying
web and mobile applications
  Business productivity services for email, collaboration, and
other office tasks
  Internet of Things (IoT) services for connecting and managing
devices
  Game development services for building and hosting games
  And many more
Key Characteristics of AWS Services:
  On-demand, pay-as-you-go: You only pay for the resources
you use, and you can scale up or down as needed.
  Globally available: AWS services are available in multiple
regions and availability zones around the world.
  Secure and reliable: AWS has a strong track record of
security and reliability.
  Innovative: AWS is constantly adding new services and
features.

Snapshot of current AWS service from AWS site:
  Log in to your AWS account: https://console.aws.amazon.com/
using Username and Password
  Click the "Services" menu at the top of the console.


4. BENEFITS OF THE AWS CLOUD
Because of its economies of scale, AWS Cloud provides several
advantages, such as:
•Lower costs: AWS can purchase hardware and software in
bulk and negotiate lower prices with suppliers. These savings
are given back to customers in the form of lower pay-as-you-go
pricing.
•More efficient use of resources: AWS can combine the
computing power of its millions of customers and make better
use of them than any one customer could on their own. This can
lead to significant cost savings, especially for businesses with
fluctuating or variable workloads.
•Access to the newest technology:  AWS continuously
makes investments in new software and hardware, and its
clients gain instant returns on these expenditures. A Businesses
can access the latest and greatest technologies without having
to make a significant upfront investment.
•Global reach: AWS has a network of data centers all over
the world, so companies can put their data and apps close to the
users they have all over the world. This might make things run
faster and reduce latency
•Scalability and elasticity: AWS makes it easy to scale
your infrastructure up or down as needed. This can help
businesses to save money by not buying too many tools at once.

Overall, AWS Cloud offers a number of benefits based on
economies of scale, which can help businesses to save money,
improve efficiency, and scale their operations.
AWS Three Key Characteristics
In the cloud world, especially with AWS, we are having three
characteristics that are crucial for building robust and successful
applications. These characteristics are:
  High availability
  Elasticity
  Agility
High availability
AWS Cloud offers a number of features and services that can
help businesses to achieve high availability and AWS is always

design for failure.
  Redundant infrastructure: AWS has a redundant global
infrastructure, with multiple data centers in each region. This
means that if one data center fails, traffic can be routed to
another data center without any downtime.
  Load balancing: AWS offers a variety of load balancing services
that can distribute traffic across multiple instances of an
application. This can help to improve the performance and
reliability of the application.
  Auto scaling: AWS Auto Scaling can automatically scale up or
down the number of instances of an application based on
demand. This can help to ensure that the application is always
available while preventing excessive provisioning.
  Multi AZ RDS Support
  Simple storage service(S3) high availability and durability
Further details will be provided in the appropriate section.
Elasticity
AWS Cloud is highly elastic, meaning that businesses can
easily scale their infrastructure up or down as needed. This is
because AWS has a large pool of resources that it can allocate to
customers on demand. The elasticity of AWS Cloud offers a
number of benefits, including:

  Reduced costs: Businesses can avoid overprovisioning
resources by using AWS's elasticity. This can lead to significant
cost savings, especially for businesses with fluctuating
workloads.
  Improved performance: Businesses can ensure that their
applications always have the resources they need to perform
optimally by using AWS's elasticity.
  Increased agility: Businesses can quickly deploy new
applications and services, or scale existing applications and
services, to meet changing demands by using AWS's elasticity.
Agility
AWS Cloud can help businesses to become more agile by
providing them with access to a wide range of services and tools
on demand. This can help businesses to quickly develop, test,
and deploy new applications and services. The agility of AWS
Cloud offers a number of benefits, including:
  Reduced time to market: Businesses can get their products
and services to market faster by using AWS Cloud to
develop, test, and deploy them.
  Increased innovation: AWS Cloud can help businesses to
innovate more quickly by giving them access to a wide range of
services and tools.

  Improved customer responsiveness: Businesses can quickly
respond to changing customer demands by using AWS Cloud to
scale their applications and services up or down as needed.
Overall, the benefits of AWS Cloud based on high availability,
elasticity, and agility include:
  Reduced costs
  Improved performance
  Increased agility
  Reduced time to market
  Increased innovation
  Improved customer responsiveness

5. AWS GLOBAL
INFRASTRUCTURE: A DIGITAL
ECOSYSTEM
AWS Global Infrastructure is the most extensive and reliable
cloud platform in the world. There are several different types
of AWS global infrastructure, each with its own purpose and
benefits:
  
Regions: Regions are
isolated from each other, and each region has its own power,
cooling, and networking infrastructure. This helps to ensure
that AWS services are available even in the event of a major
outage in one region.

  Availability Zones (AZs): AZs are isolated within each
region. Each AZ has its own power, cooling, and networking
infrastructure. This helps to protect your applications from
failures in a single AZ.
  Edge Locations (Point of Presence): Edge locations are
located closer to end users than data centers, which reduces
latency and improves performance. Edge locations are used
to deliver content and services such as Amazon CloudFront,
Amazon Route 53, and Amazon Web Application Firewall
(WAF).
  Regional Edge Cache: It is a technology used in Content
Delivery Networks (CDNs) like Amazon CloudFront to improve
website and application performance for users around the
world. It works by storing frequently accessed content on
servers located closer to users than the origin server (where
the content actually resides). This reduces the distance data
needs to travel, resulting in faster loading times and a more
responsive user experience.

Apart from this, we can consider also some other
resources with respect to infrastructure consideration:
  Local Zones: Local Zones are located within metropolitan
areas. They provide single-digit millisecond latency to
applications that need to be close to end users, such as
mobile and IoT applications.
  Wavelength Zones: Wavelength Zones are located within
telecommunications providers' facilities. They provide single-
digit millisecond latency to applications that need to be close
to mobile networks.
  Outposts: Outposts bring AWS services, infrastructure, and
APIs to your on-premises data center. This allows you to run
AWS-native workloads on-premises while still benefiting from
the scalability, reliability, and security of the AWS Cloud.
AWS Global Infrastructure link: https://infrastructure.aws/

5.1.  REGIONS
An AWS Region is a physical location in the world where AWS
has multiple Availability Zones. Regions are isolated from each
other, so that if there is a disruption in one Region, the other
Regions will continue to operate.
As of October 2023, AWS has 26 Regions around the world:
  North America: US East (N. Virginia), US East (Ohio), US West
(Oregon), US West (N. California), Canada (Central), Canada
(East), AWS GovCloud (US), AWS GovCloud (US-East)
  South America: São Paulo (Brazil)
  Europe: London (UK), Ireland, Frankfurt (Germany), Paris
(France), Stockholm (Sweden), Milan (Italy), Madrid (Spain),
Athens (Greece), AWS GovCloud (US-West)
  Asia Pacific: Tokyo (Japan), Seoul (South Korea), Singapore,
Sydney (Australia), Osaka (Japan), Hong Kong, Mumbai (India),
Jakarta (Indonesia), New Delhi (India), AWS GovCloud (Asia
Pacific (Seoul))
  Middle East: Bahrain
  Africa: Cape Town (South Africa)
From the Console, you can choose a region after login to
your AWS account as shown Below:


5.2  AVAILABILITY ZONES:
  Each region has many availability zones (usually 3, min is
3, max is 6).
  Example:
•  ap-southeast-2a
•  
ap-southeast-2b
•  ap-southeast-2c
  An Availability Zone (AZ) is a physically isolated location
within an AWS Region.
  AZs are designed to be highly available and fault-tolerant.
  Each AZ has its own power, cooling, and networking
infrastructure.
  AWS places AZs within a Region far enough apart to
minimize the impact of a physical disruption, such as a flood

or earthquake. However, AZs are close enough together to
provide low latency network connectivity between them.

5.3  EDGE LOCATIONS
AWS Edge Locations are
distributed points of presence that provide low-latency access
to AWS services. Edge Locations are located close to major
population centers and internet exchanges, making them
ideal for delivering content and applications to users around
the world.
Amazon CloudFront uses a global network of 550+ Points
of Presence and 13 regional edge caches in 100+ cities
across 50 countries. Amazon CloudFront Edge locations map
is available in:
https://aws.amazon.com/cloudfront/features/

5.4  REGIONAL EDGE CACHE
It is a technology used in Content Delivery Networks (CDNs)
to improve website and application performance for users
around the world and it is related to Amazon CloudFront.
Here's how it works:
Function:
  Location: Imagine a series of geographically distributed
data centers called "Points of Presence" (PoPs) closer to users
than the website's origin server. Within these PoPs, Regional
Edge Caches are larger storage spaces holding frequently
accessed content.
  Content Storage: Popular content like images, videos, and
static files get stored in these caches. When a user from that
region requests any of that content, it's served directly from
the Regional Edge Cache instead of traveling all the way to
the origin server.
As of today, AWS has 13 Regional Edge Caches
strategically distributed across the globe
AWS Wavelength Zones and AWS
Local Zones
AWS Wavelength Zones and AWS Local Zones are two
types of edge computing infrastructure that AWS offers. Edge

computing is a distributed computing paradigm that brings
computation and data storage closer to the sources of data,
such as IoT devices and mobile devices. This reduces latency
and improves performance for applications that need to
process data quickly.
AWS Wavelength Zones are located at the edge of the
mobile network, inside telecom providers' facilities. This
allows AWS customers to deploy applications that require
ultra-low latency and high bandwidth to mobile users. For
example, AWS Wavelength Zones can be used to deploy
mobile gaming applications, augmented reality (AR) and
virtual reality (VR) applications, and live video streaming
applications.
AWS Local Zones are located within metropolitan areas,
close to end users. This allows AWS customers to deploy
applications that require single-digit millisecond latency to
end users. For example, AWS Local Zones can be used to
deploy financial trading applications, healthcare applications,
and industrial control applications.
Both AWS Wavelength Zones and AWS Local Zones provide
a range of AWS services, including compute, storage,
networking, and databases. This allows customers to deploy
and run their applications at the edge of the network, without
having to manage the complexity of doing so themselves.

Identification and Selection Process
of right Region
One of the most important choices in any cloud
deployment is selecting the right region. We need to think
about where the data needs to reside, how to comply with
regulations, and how latency can help us make this choice.
Amazon Web Services (AWS) gives people a lot of tools and
information to help them make decisions that meet their
operational and business needs.
When choosing a region for your AWS resources, there are
a few factors to consider, such as:
• Target audience: Where do your users come from? If
you have users in more than one area, you might want to
pick a region that is close to them to get better performance.
• Compliance requirements: Some regions are more
compliant than others. One example is that if you need to
follow the General Data Protection Regulation (GDPR) of the
European Union, you will need to pick a region in the EU.
• Budget: The cost of AWS services varies by region. One
example is that services in the US-East-1 region usually cost
more than services in the US-West-2 region.

What are Data Centers:
Physical locations where
AWS stores its servers, storage, and networking devices are
referred to as data centers. They know how to use technology
well, are reliable and safe, and have been set up to support
AWS services. AWS has data centers in 32 countries and 102
availability zones around the world. This makes sure that data
stored close to AWS customers can improve speed and
reduce latency.
Physical security measures, like fences, guards, and video
tracking that runs 24 hours a day, seven days a week, also
keep the data centers safe. The AWS data centers are always
being checked on and fixed to make sure they are always
accessible and dependable. AWS also has a team of experts
who are dedicated to security.
Every data center is equipped with redundant power and
cooling systems and is climate-controlled

6. DIFFERENT METHOD TO ACCESS
OR CONNECT AWS
There are different methods to access or connect to AWS login,
depending on your needs and preferences. Here are some of the
most common methods:
AWS Management Console
The AWS Management Console is a web-based interface that
allows you to manage your AWS resources. To access the AWS
Management Console, you will need to go to the AWS website
and log in with your AWS account credentials.
To access or connect to AWS login, you can use the following
steps:
1.  Go to the AWS website at https://console.aws.amazon.com/
2.  Click on the Sign-In to the Console button.
3.  Enter your AWS account email address and password.
4.  Click on the Sign In button.
If you are using two-factor authentication (2FA), you will also
need to enter your 2FA code. If you are successful, you will be
logged in to the AWS Management Console.

AWS CLI
The AWS CLI is a command-line tool that allows you to
manage your AWS resources. To use the AWS CLI, you will need
to install the AWS CLI on your computer and configure it with
your AWS account credentials.
AWS SDKs
AWS SDKs are libraries that allow you to manage your AWS
resources from within your own code. AWS SDKs are available
for a variety of programming languages, including Python, Java,
JavaScript, and C#.
AWS Mobile App
The AWS Mobile App is a mobile app that allows you to
manage your AWS resources from your mobile device. To use the
AWS Mobile App, you will need to download the app from the

App Store or Google Play and log in with your AWS account
credentials.
AWS SAML Provider
The AWS SAML Provider allows you to federate with an
existing identity provider, such as Active Directory or Okta. This
allows you to use your existing credentials to log in to AWS.
AWS SSO
AWS SSO is a service that provides a single sign-on (SSO)
experience for your users. With AWS SSO, your users can log in
to AWS using a single set of credentials, regardless of how they
are accessing AWS.

7.  AWS HIGHER AVAILABILITY-
BRIEF OVERVIEW
To achieve high availability in AWS, you can use the following
strategies:
  Deploy your applications across multiple Availability Zones
(AZs). AZs are isolated from each other, so if one AZ fails, your
applications will continue to run in the other AZs.
  Use auto scaling to automatically scale your resources up or
down based on demand. This helps to ensure that your
applications can handle sudden spikes in traffic as well as failure
in any instance.
  Use load balancing to distribute traffic across multiple
instances of your applications. This helps to improve
performance and reliability.
  Use database replication to replicate your data across multiple
databases. This ensures that your data is always available, even
if one database fails.
  Use backup and recovery solutions to back up your data and
recover quickly from failures.

Advantage of having multiple Regions in AWS
There are many advantages to having multiple Regions in
AWS, including:
Improved availability: Regions are isolated from each other,
so if there is an outage in one Region, your applications and
data can still be accessed from other Regions.
Reduced latency: Regions are located around the world, so
you can deploy your applications closer to your users to
reduce latency and improve performance.
Compliance: Some data sovereignty and compliance
regulations require data to be stored in specific geographic
regions. By having multiple Regions, you can meet these
requirements.
Disaster recovery: If there is a major disaster in one Region,
such as a natural disaster or a terrorist attack, you can
recover your applications and data from other Regions.

Scalability: Regions offer a wide range of compute, storage,
and networking resources. By having multiple Regions, you
can scale your applications to meet the needs of your
business.
Here are some examples of how businesses can benefit from
having multiple Regions in AWS:
A global e-commerce company could deploy its applications
and data in multiple Regions to improve availability and
performance for its customers around the world.
A financial services company could deploy its applications
and data in multiple Regions to meet data sovereignty and
compliance requirements.
A healthcare company could deploy its applications and data
in multiple Regions to ensure that its patient data is always
available and to comply with healthcare regulations.
Additional recommendations for utilizing multiple AWS
regions are as follows:
Use a Region strategy. A Region strategy is a plan for how
you will use multiple Regions to meet your specific needs.
For example, you may decide to deploy your production
applications in one Region and your development
applications in another Region.
Employ AWS services that are region-independent.
Numerous AWS services, including Amazon RDS, Amazon
EFS, and Amazon S3, support multiple regions. This feature

enables the deployment of applications and data across
multiple regions without the need for manual management
of associated complexities.
Employ AWS management tools for multiple regions. AWS
offers a variety of tools, including AWS Systems Manager
and AWS CloudFormation, that facilitate the management of
multiple regions. You can automate the process of deploying
and managing your applications and data across multiple
regions with the assistance of these tools.
The utilization of multiple AWS Regions can offer numerous
advantages to organizations of varying scales. Multiple Regions
can assist organizations in enhancing their overall operations by
achieving compliance, reducing latency, increasing availability,
and facilitating disaster recovery.

8. AWS SHARED RESPONSIBILITY
MODEL
The AWS shared responsibility model is a security model that
defines the responsibilities of AWS and its customers. It is a
shared responsibility between AWS and its customers to protect
their data, applications, and systems in the AWS Cloud. Please
refer the diagram below and it clearly Articulates the
responsibilities of different resources between AWS and
Customer.
AWS is responsible for the security of the cloud. This includes
the security of the physical infrastructure, the operating system,
and the virtualization layer. AWS is also responsible for the
security of the underlying services that support AWS services,
such as networking, storage, and compute.
Customers are responsible for the security in the cloud. This
includes the security of their applications, data, and systems.

Customers are also responsible for the security of their AWS
account and the AWS resources that they create.
The following table shows some examples of AWS and
customer responsibilities:
The AWS shared responsibility model can help customers to
improve their security posture by providing a clear framework
for understanding and managing their security responsibilities.
Here are some tips for following the AWS shared
responsibility model:
  Use Identity and Access Management (IAM) to control access
to your AWS resources. IAM allows you to create users and
groups, and to assign them permissions to access your AWS
resources.
  Use security groups to control network access to your AWS
resources. Security groups allow you to specify which IP
addresses can access your AWS resources.
  Use encryption to protect your data. AWS offers a variety of
encryption options, such as Amazon S3 Server-Side Encryption
(SSE) and Amazon Elastic Block Store (EBS) encryption.

  Monitor your AWS resources for suspicious activity. AWS offers
a variety of monitoring tools, such as AWS CloudTrail and
Amazon CloudWatch.
  Keep your AWS software up to date. AWS regularly releases
security updates for its software. It is important to apply these
updates as soon as possible.
Shared responsibilities include:
  Security: Both AWS and the customer are responsible for
implementing security controls to protect their workloads. For
example, AWS is responsible for patching the underlying
infrastructure, while the customer is responsible for patching
their guest operating systems and applications.
  Compliance: Both AWS and the customer are responsible for
ensuring that their workloads comply with all applicable
regulations. For example, AWS provides a variety of compliance
tools and resources, but the customer is ultimately responsible
for ensuring that their workloads are compliant.
Shifting between AWS responsibilities and customer
responsibilities
AWS responsibilities and customer responsibilities can shift,
depending on the service used. The reason for this is that each
AWS service offers a various amount of abstraction and
functionality. For example, a service like Amazon RDS (Relational
Database Service) is a fully managed database service, which

means that AWS is responsible for managing the database
infrastructure, including patching and security. However, a
service like Amazon EC2 (Elastic Compute Cloud) provides bare-
metal servers, which means that the customer is responsible for
all aspects of managing the server, including patching and
security.
Here is a table that shows how AWS responsibilities and
customer responsibilities can shift, depending on the service
used:
As you can see, the more abstraction and functionality that a
service provides, the more responsibility AWS takes for
managing the underlying infrastructure. However, the customer
is still responsible for configuring and managing the service to
meet their specific needs.
Security controls and protect their resources on AWS.
Customers can secure their resources on AWS using a variety
of tools, including:
  AWS Identity and Access Management (IAM): IAM allows
you to create and manage users and groups, and to assign
permissions to those users and groups. This can help you to

control who has access to your AWS resources and what they
can do with those resources.
  AWS Key Management Service (KMS): KMS allows you to
create and manage encryption keys. You can use these keys to
encrypt your data at rest and in transit.
  AWS Config: AWS Config allows you to monitor and assess
your AWS resource configuration. This can help you to identify
any deviations from your desired configuration.
  AWS CloudTrail: AWS CloudTrail provides a record of all
activity in your AWS account. This record can help you to
investigate security incidents and troubleshoot problems.
  AWS Security Hub: AWS Security Hub aggregates and
analyzes security findings from other AWS security services. This
can help you to get a comprehensive view of your security
posture.

9. MULTI-CHOICE QUESTION AND
ANSWER
Question 1:
Which of the following best describes Amazon Web
Services (AWS)?
(a) A collection of physical data centers owned and operated
by Amazon.
(b) A comprehensive suite of cloud computing services
offered by Amazon.
(c) A software platform for developing and deploying web
applications.
(d) A marketplace for buying and selling cloud resources from
various providers.
Answer: (b)
Explanation: AWS is a comprehensive suite of cloud
computing services encompassing various categories like
storage, compute, databases, networking, etc.
Question 2:
What was the initial service launched by AWS in 2006?
(a) Amazon S3

(b) Amazon EC2
(c) Amazon CloudFront
(d) Amazon Route 53
Answer: (b)
Explanation: Amazon EC2 (Elastic Compute Cloud) was the
first service launched by AWS, offering virtual servers on-
demand.
Question 3:
Which of the following is NOT included in the AWS Services
categories?
(a) Storage
(b) Machine Learning
(c) Development Tools
(d) Operating Systems
Answer: (d)
Explanation: AWS doesn't offer its own operating systems;
instead, it provides VMs and containers pre-configured with
various OS options.
Question 4:
What are some key benefits of using the AWS cloud?
(a) Reduced upfront costs and improved scalability.

(b) Increased complexity and security concerns.
(c) Limited-service options and longer deployment times.
(d) Vendor lock-in and less control over resources.
Answer: (a)
Explanation: AWS offers pay-as-you-go pricing, eliminating
upfront investments. It also scales effortlessly to meet
changing needs.
Question 5:
What is the primary purpose of AWS Edge Locations?
(a) To provide disaster recovery solutions.
(b) To house origin servers for websites and applications.
(c) To cache static content and deliver it closer to users.
(d) To manage user authentication and authorization.
Answer: (c)
Explanation: Edge Locations are strategically placed
servers worldwide to cache content and reduce latency for
users.

CHAPTER 3  AMAZON IDENTITY
AND ACCESS MANAGEMENT (IAM)

1.
 IDENTITY AND ACCESS
MANAGEMENT
Identity and Access Management (IAM) is a cloud security
service that helps you securely control access to cloud
resources. IAM is a core service of all major cloud providers,
including Amazon Web Services (AWS), Microsoft Azure, and
Google Cloud Platform (GCP).
IAM offers the following features:
  Authentication: Authentication is a security measure
implemented by IAM to verify the identity of users. This can be
accomplished through the use of federated identities,
passwords, or multi-factor authentication (MFA).
  Authorization: IAM authorizes authenticated users to access
and manage cloud resources. This is done by assigning users to
roles, which are sets of permissions.
  Auditing: Auditing capabilities are provided by IAM to monitor
who accesses and manages cloud resources, as well as the
actions they execute.
IAM Functional Picture
The following diagram shows a high-level overview of how
IAM works:

Steps:
1.  An IAM principal, which can be a user, group, or role, initiates
an access request to an AWS resource.
2.  In order to verify the identity of the principal, IAM executes
authentication.
3.  IAM verifies that the principal holds the necessary resource
access permissions.
4.  In the event that the principal owns the requisite
authorizations, IAM proceeds with the request.
5.  In the event when principal does not possess the necessary
authorizations, the request will be denied by IAM.
IAM (Identity and Access Management) is a framework for
controlling access to organizational infrastructure.

1.1  IAM AUTHENTICATION
PROCESS
The process of IAM authentication is illustrated in the
subsequent diagram:
Process of AWS Identity and Access Management (IAM)
authentication:
1. An AWS resource access request is submitted by an IAM
principal on behalf of a user, group, or role.
2. The request is communicated to the IAM service.
3. In order to authenticate the principal, the IAM service
verifies the principal's credentials. This objective may be
achieved by implementing federated identities, multi-factor
authentication (MFA), or passwords.
4. A security token is issued to the principal by the IAM
service following a successful authentication attempt.

5. Following this, subsequent queries to AWS resources are
signed by the principal using the security token.
6. On each successive request, the IAM service verifies the
security token to ensure that the principal remains
authenticated.
Example: A user wants to access an Amazon S3 bucket. A
request is transmitted by the user to the Amazon S3 service.
The request is forwarded to the IAM service via the Amazon
S3 service. By comparing the user's password, the IAM
service verifies their identity. Upon effective authentication,
the IAM service will provide the user with a security token.
The user subsequently signs successive requests to the
Amazon S3 bucket using the security token. On each
successive request, the Amazon S3 service verifies the
security token to ensure that the user remains authenticated.
The implementation of IAM authentication is critical for
securing AWS resources. You can ensure that only authorized
users and applications have access to your resources by
implementing IAM authentication.

1.1.  IAM AUTHORIZATION
PROCESS
Steps of the IAM Authorization Process:
1. An IAM principal submits a request to access an AWS
resource on behalf of a user, group, or role.
2. In order to authenticate the principal, the IAM service
verifies the principal's credentials.
3. The IAM service verifies that the principal maintains the
necessary resource access permissions. This is accomplished
through a review of the principal's IAM policies.
4. Upon verification that the principal possesses the
necessary authorizations, the IAM service fulfills the request.
5. If the principal lacks the required authorizations, the
IAM service will reject the request.
Example: A user wants to create an Amazon EC2 instance.
A request is sent by the user to the Amazon EC2 service. The
request is forwarded to the IAM service via the Amazon EC2
service. By comparing the user's password, the IAM service
verifies their identity. Upon successful authentication, the IAM
service verifies whether the user is authorized to establish an
Amazon EC2 instance. This is achieved by reviewing the IAM
policies of the user. When the user possesses the requisite
authorizations, the IAM service grants the request. Following
this, the user is able to establish the Amazon EC2 instance.
When the user lacks the requisite authorization, the IAM

service rejects the request. The user is unable to establish an
instance on Amazon EC2.
Authorization via IAM is an essential element of AWS
resource security. By implementing IAM authorization, you
can ensure that only authorized users and applications can
perform operations on your resources.

1.3  IAM AUDITING PROCESS
The IAM auditing process can be visualized with the following
diagram:
IAM auditing process
IAM Auditing Process Steps:
Steps: 1. Define the scope of the audit: Determine the IAM
activities and resources that will be subject to audit.
2. Audit data collection: Acquire the audit data via the IAM
service. There are several approaches that can be utilized to
collect this data, such as implementing the AWS CloudTrail
service, the AWS CloudWatch Logs service, or the IAM service
directly.
3. Analyze the audit data: Examine the audit data with the
purpose of identifying any irregular or suspicious behavior.
4. Remediate any issues: If any suspicious or unusual activity
is identified, take steps to remediate the issue. Changes in IAM
policies, disabling users, or implementing alternative remedial
measures may be required.

5. Report on the findings of the audit: Produce a report that
summarizes the findings of the audit. It is advisable to distribute
this report to the relevant stakeholders.
As an example:
An organization wants to audit access to its Amazon S3
buckets. The audit scope is established by the organization to
encompass all authorized access to Amazon S3 buckets. Audit
data is subsequently gathered by the organization via the AWS
CloudTrail service. A user accessed an Amazon S3 bucket without
authorization, according to the audit data. After conducting an
investigation, the organization concludes that the user's IAM
policies were incorrectly configured. The organization restricts
the user's access to Amazon S3 bucket by modifying their IAM
policies. The organization subsequently produces a report
summarizing the audit's findings and distributes it to the
relevant stakeholders.
IAM Benefits
A number of advantages are provided by IAM, including:
  Improved security: IAM assists you in enhancing the security of
your cloud resources through the management of who can
access them and what they are able to do.
  Reduced complexity: By centralizing the management of
permissions, IAM optimizes the process of administering access
to cloud resources, thereby reducing complexity.

  Increased compliance: IAM can assist you to comply with a
numerous industry regulation, such as HIPAA and PCI DSS.

2.  IAM: USERS AND GROUPS AND
ROLES
Identity and Access Management is having four key
components:
  
Users
  Groups
  Policy
  Roles
What is User:
An IAM user is a unique identity within your AWS account that
can be used to interact with AWS services and resources. It
represents a person or application that needs to access your
AWS resources. Each IAM user has its own credentials (password
or access keys) for sign-in and permissions that define what
actions it can perform. IAM users are distinct from the root
user, which has full administrative control over the account.
IAM User Key Characteristics:
  Credentials: IAM users can have different types of credentials:
o  Password: For signing in to the AWS Management Console.

o  Access keys: For programmatic access using the AWS
CLI, SDKs, or APIs.
  Permissions: IAM users are granted permissions through
policies that define what actions they can perform on specific
AWS resources.
Examples of Users:
1.  Developer:
o  User name: developer1
o  Permissions: Can create and manage EC2 instances, S3
buckets, and Lambda functions.
2.  Data Analyst:
o  User name: analyst2
o  Permissions: Can access and analyze data in S3 and
Redshift, but cannot modify resources.
3.  Application:
o  User name: appserver1
o  Permissions: Can read and write data to a specific S3
bucket, but cannot access other AWS services.

Lab 1: AWS IAM User Creation
To create an IAM user, you need to do the following:
1.  Login to AWS Management Console from the main AWS
sign-in URL (https://console.aws.amazon.com/) you must choose
your user type, either Root user or IAM user Open the IAM
console. Click on the “Users” link.
2.  Click on the “Create user” button.
3.  Enter the user’s name – demouser.

4.  Select Provide user access to the AWS Management Console
– optional option.
5.  Select User type: “I want to create an IAM user”
6.  Select Console password: “Custom Password” and give
your password

7.  Add the user to one or more groups. You are having three
different options
a.  Add user to group
b.  Copy permissions
c.  Attach policies directly.
8.  Click on the “Create user” button.

9.  On the Retrieve password page, select Download .csv
file to save a .csv file with the user credential information
(Connection URL, user name, and password).
10.  You can find demouser.csv file in your download directory.
Keep this file in secure place for future reference.
What is Group

AWS IAM groups function similarly to teams for your IAM
users. Assigning permissions to individual users is not feasible;
rather, permissions can be granted to the group as a whole on
the basis of their duties and responsibilities. This streamlines the
process of managing access and guarantees that all personnel
are equipped with the appropriate tools for their respective
roles.
Consider the following scenario:
You have a team of developers building a web application on
AWS. While access is required for the creation of EC2 instances,
S3 buckets, and Lambda functions, the ability to delete
resources or administer IAM users should be restricted.
One possible alternative to generating distinct policies for
each developer is to establish a group called "AppDev" to which
you can append a policy that provides the required permissions.
Simply incorporate all of your developers into the group, and
you're done! They are granted the necessary access without the
burden of administering individual policies.
Here are some key things to know about IAM groups:
•  Centralized permissions: It simplifies the process of adding,
removing, and updating permissions for multiple users
simultaneously.
•  Enhanced organizational capabilities: Users are categorized
according to their projects or responsibilities, thereby
establishing a straightforward framework for managing access.

•  Streamlined administration: Rather than administering
individual policies, users can be added or removed from groups.
•  Granular control: For even more granular control, you can still
designate individual policies to users within a group.
Group: Developers
Permissions:
  Create and manage EC2 instances
  Create and manage S3 buckets
  Create and manage Lambda functions
  View CloudWatch logs
Users:
  developer1
  developer2
  developer3
By adding these developers to the "Developers" group, they
automatically inherit the group's permissions, allowing them to
work on the web application without needing individual policies.
Lab 2: AWS IAM Group Creation

1.  Login to AWS Management Console from the main AWS
sign-in URL (https://console.aws.amazon.com/) you must choose
your user type, either Root user or IAM user Open the IAM
console. Click on the “Groups” link.
2.  Click on the “Create group” button.
3.  Enter a name for the group- “demogroup”
4.  Add one or more policies to the group. We are selecting
“Adminstratoraccess” policy.
In AWS Identity and Access Management (IAM), the
AdministratorAccess policy is a managed policy that grants
extensive, near-complete permissions to manage all aspects of
your AWS account and its resources. This includes:
•  Full access to IAM: Create, modify, and delete
users, groups, roles, and policies.
•  Manage all AWS services: Perform any action on any AWS
service within your account, like creating and deleting EC2

instances, S3 buckets, RDS databases, etc.
•  View billing information and reports: Access detailed
information about your AWS usage and costs.
5.  Click on the “Create group” button. This will create the
group name by demogroup.

6.  This group is having “Adminstratoraccess” policy as select
during group creation.
Lab 3: Add user to available group
1.  Login to AWS Management Console from the main AWS
sign-in URL (https://console.aws.amazon.com/) you must choose
your user type, either Root user or IAM user Open the IAM
console. Click on the “Groups” link.
2.  Click on the name of the group you want to add the user to
like demogroup

3.  Go to the Users tab and Click on the Add Users tab.
4.  Click on the Add users button.
5.  You can find demouser under group – demogroup.


What is Role:
Permissions that may be allocated to specific users or
organizations. Roles are versatile in nature; they can be
assigned to individuals or groups indefinitely or temporarily to
access resources.
Important aspects of IAM roles:
  Temporary credentials: Roles provide limited-duration, task-
specific credentials, minimizing the risk of unauthorized access.
  Improved security: No long-term passwords or access keys are
associated with roles, making them less vulnerable to
compromise.
  Granular control: You can define exactly what actions a role
can perform on specific resources.
  Scalability: Roles are ideal for automated tasks or workloads
where frequent user changes occur.
Here's an example of an IAM role in action:
Role: EC2TaskRole
Permissions:
  Start and stop EC2 instances
  Read and write data to a specific S3 bucket
  Execute a predefined Lambda function

Used by:
  Auto Scaling group for scaling the web application
  CI/CD pipeline for deploying new code versions
When the Auto Scaling group needs to launch a new EC2
instance, it assumes the EC2TaskRole. This role grants the
instance the necessary permissions to start up, access the S3
bucket for configuration files, and run the deployment Lambda
function. Once the instance finishes its task, the role expires,
and its access is automatically revoked.
AWS IAM Role Creation:
Role can be created in three different steps:
1)  Select trusted entity.
In AWS IAM role creation, a trusted entity refers to the entity
that can assume the role and utilize its associated permissions.
It's like granting someone a temporary security pass to access
specific resources within your AWS account.
Here are the five main types of trusted entities:
  AWS service: Allow AWS services like EC2, Lambda, or others
to perform actions in this account.
  AWS account: Allow entities in other AWS accounts belonging
to you or a 3rd party to perform actions in this account.

  Web identity: Allows users federated by the specified external
web identity provider to assume this role to perform actions in
this account.
  SAML 2.0 federation: Allow users federated with SAML 2.0
from a corporate directory to perform actions in this account.
  Custom trust policy: Create a custom trust policy to enable
others to perform actions in this account.
2)Add permissions
AWS permission policies are essentially JSON documents that
define the permissions associated with an IAM identity (users,
groups, or roles) or a resource. These policies determine what
actions the identity or resource can perform on specific AWS
services and resources.
Currently AWS is having 914 Permissions Policies.
3)Add role name, review and create it.
Lab 4:  AWS IAM Role creation
To create an IAM role, you need to do the following:

1.  Login to AWS Management Console from the main AWS
sign-in URL (https://console.aws.amazon.com/) you must choose
your user type, either Root user or IAM user Open the IAM
console. Click on the “Roles” link.
2.  Click on the “Create role” button.
3.  Select the type of trusted entity type-AWS Service and EC2
Service for the role.

4.  Choose the permissions that the role should have like
AdministratorAccess.
5.  Add tags to the role if needed.

6.  Click on the “Create role” button. This will create the role-
demorole and you can find it in roles user screen.

What is Policy
A document that defines the permissions an IAM user or
group has to access and administer AWS resources is known as
an IAM policy. Policies may be associated with resources, users,
or organizations and are expressed in JSON format.
In order to determine whether a request to access an AWS
resource is granted or denied by an IAM user or group, the
policies associated with that user or group are reviewed by IAM.
IAM policies are used to implement the principle of least
privilege, which states that users should only be granted the
permissions that they need to perform their job duties.
Example IAM policy:
JSON
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"s3:ListBucket"
],
"Resource": "arn:aws:s3:::my-bucket"
}
]
}
The following table describes the main elements of an IAM
policy:


Multi-Factor Authentication in IAM
Multi-Factor Authentication (MFA) in IAM is a security feature
that adds an extra layer of protection to your AWS account by
requiring users to provide two factors of authentication when
signing in.
The first factor of authentication is typically a username and
password. The second factor of authentication can be a variety
of things, such as a one-time password (OTP) generated by an
authenticator app, a code sent to your phone via SMS, or a
hardware security key.
MFA helps to protect your AWS account from unauthorized
access, even if an attacker has your username and password.
Benefits of using MFA in IAM:
  Improved security: MFA adds an extra layer of protection to
your AWS account and makes it more difficult for attackers to
gain access.
  Reduced risk: MFA can help to reduce the risk of data
breaches, compliance violations, and other security incidents.

  Increased compliance: MFA can help you to comply with a
variety of industry regulations, such as HIPAA and PCI DSS.
Use cases for MFA in IAM:
  Protecting your AWS root account: You should enable MFA for
your AWS root account to protect it from unauthorized access.
  Protecting sensitive resources: You should enable MFA for
users who need access to sensitive resources, such as S3
buckets and RDS databases.
  Complying with regulations: If you are subject to industry
regulations that require MFA, you should enable MFA for all users
of your AWS account.
Lab 5: Enable MFA in the IAM console
1.  Go to the IAM console.
2.  Click Users or Groups.
3.  Click the name of the user or group that you want to enable
MFA for.
4.  Click the Security credentials tab.
5.  Click Assign MFA Design Tab in Multi-factor authentication
(MFA) Section

6.  Choose a type of MFA and follow the on-screen instructions.
The key benefit of MFA is if a password is stolen or hacked,
the account is not compromised.

Different MFA Device options in AWS
AWS supports a variety of MFA devices, including:
  
Authenticator apps: Authenticator apps are
software applications that generate one-time passwords
(OTPs). Some popular authenticator apps include Google
Authenticator, Authy, and Microsoft Authenticator.
  
Hardware security keys: Hardware security keys are
physical devices that generate OTPs. Some popular hardware
security keys include YubiKeys and Feitian keys.
  SMS: You can also use your phone to receive OTPs via
SMS. However, SMS is the least secure MFA option and should be
avoided if possible.
Which MFA device should I use for my root account?
  You should use the most secure MFA device possible for your
root account. This is because the root account has full access to
all of the resources in your AWS account.

  We recommend that you use a hardware security key for your
root account. However, if you are unable to use a hardware
security key, you can use an authenticator app
What is access key
An access key is an authentication credential that allows
users to access AWS resources using the AWS CLI, AWS SDKs, or
AWS APIs. Access keys are composed of two parts: an access key
ID and a secret access key.
Example:
Access key ID: AKIAIOSFODNN7EXAMPLE
Secret access key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
Access keys can be used to perform a wide range of actions
on AWS resources, such as creating and deleting resources,
modifying permissions, and managing data.
Use cases for access keys:
  Automating tasks: Access keys can be used to automate
tasks on AWS, such as creating and deleting
resources, modifying permissions, and managing data.
  Creating IAM users and groups: Access keys can be used
to create IAM users and groups, which can then be assigned to
specific roles and policies.
  Accessing AWS resources from outside of the AWS
Management Console: Access keys can be used to access

AWS resources from outside of the AWS Management
Console, such as using the AWS CLI, AWS SDKs, or AWS APIs.
Security best practices for using access keys:
  Only share access keys with authorized users.
  Rotate access keys regularly.
  Use strong passwords for access keys.
  Store access keys in a secure location.
  Use IAM policies to restrict the permissions that access keys
have.
  Access Keys are generated through the AWS Console •
  Users manage their own access keys.

3.  IAM GUIDELINES & BEST
PRACTICES
  Avoid using the root account. Only use the root account
to set up your AWS account and then create IAM users and
groups to manage your AWS resources.
  Create a unique IAM user for each physical user. This
will help you to track who is accessing your AWS resources
and what actions they are performing.
  Assign users to groups and assign permissions to
groups. This is a more efficient way to manage permissions
than assigning permissions to individual users.
  Create a strong password policy. Your password policy
should require users to create strong passwords that are
difficult to guess.
  Require users to use MFA. MFA adds an extra layer of
security to your AWS account by requiring users to enter a
code from their phone in addition to their password when
signing in.
  Use IAM roles to grant permissions to AWS
services. IAM roles allow you to grant permissions to AWS
services without having to share access keys with them.

  Use access keys for programmatic access. Access keys
allow you to programmatically access AWS resources using
the AWS CLI, AWS SDKs, or AWS APIs.
  Audit the permissions of your account regularly. Use
the IAM Credentials Report and IAM Access Advisor to identify
any suspicious or unusual activity.
  Never share IAM users and access keys. IAM users and
access keys should be kept secret and should only be shared
with authorized users.

4.  MULTI-CHOICE QUESTIONS
AND ANSWER
1. Identity and Access Management (IAM) in AWS controls:
(a) Network security policies
(b) User access to AWS resources
(c) Data encryption within S3 buckets
(d) Virtual machine configurations
Answer: (b)
Explanation: IAM is specifically designed to manage user
access to AWS resources, including S3 buckets, EC2
instances, and Lambda functions.
2. During the IAM authentication process, which
component verifies a user's identity?
(a) IAM user group
(b) IAM role
(c) Multi-factor authentication (MFA) device
(d) AWS Security Token Service (STS)
Answer: (d)
Explanation: STS tokens are typically used for short-lived,
temporary access and are verified by AWS upon request. MFA

adds an extra layer of security but does not directly verify
identity.
3. IAM audit logs record information about:
(a) User login attempts only
(b) API calls made by all users
(c) Resource creation and deletion actions
(d) All of the above
Answer: (d)
Explanation: IAM audit logs offer comprehensive tracking
of user activity, including logins, API calls, resource changes,
and even detailed information about each action.
4. When should you use an IAM user vs. an IAM role:
(a) Users for frequent management tasks, roles for
automated processes
(b) Roles for granular access control, users for broader
permissions
(c) Users for short-term access, roles for long-term
permissions
(d) It doesn't matter, they offer the same access control
Answer: (a)
Explanation: Users are best for individuals performing
management tasks, while roles are ideal for automated

processes or applications that don't have a specific user
associated with them.
5. Which IAM best practice encourages least privilege access:
(a) Assigning users to predefined groups with broad
permissions
(b) Granting individual users access to all resources they
might need
(c) Creating roles with the minimum permissions required for
each task
(d) Sharing credentials across teams for faster
troubleshooting
Answer: (c)
Explanation: The principle of least privilege dictates
granting users and roles only the specific permissions they
need to perform their tasks, mitigating the impact of
potential security breaches.

CHAPTER 4  AWS COMPUTE
SERVICES


1. DIFFERENT AWS COMPUTE
SERVICES
AWS Compute is a collection of services that furnish resources
for a wide range of workloads on a cloud-based infrastructure.
By providing flexibility, scalability, and cost-effectiveness, it
enables the execution of processes and applications without
the need for physical hardware management.
Some of the key services of AWS Compute are as follows:
1. Amazon Elastic Compute Cloud (Amazon EC2):
  Core compute service: Provides virtual servers (instances)
with varying CPU, memory, storage, and networking
configurations.

  On-demand scaling: Launch and terminate instances as
needed, paying only for what you use.
  Instance types: Choose from a wide range of instance types
optimized for different workloads, including general
purpose, compute-optimized, memory-optimized, GPU-
accelerated, and storage-optimized instances.
2. AWS Lambda:
Serverless compute service: Runs code without provisioning
or managing servers.
Event-driven: Automatically triggers code in response to
events (e.g., file uploads, database changes, HTTP
requests).
Pay-per-use: Costs are based on the number of executions
and duration of code execution.
3. Amazon Elastic Container Service (Amazon ECS):
  Container orchestration service: Manages containers at scale
across multiple EC2 instances.
  Task-based: Groups containers into tasks for deployment and
scaling.
  Integration with other AWS services: Works seamlessly with
services like load balancing, auto scaling, and monitoring.

4. Amazon Elastic Kubernetes Service (Amazon EKS):
Managed Kubernetes service: Runs Kubernetes clusters on
AWS without manual setup and operation.
Highly available and scalable: Automatically manages
Kubernetes master nodes and provides high availability.
Integration with AWS services: Works with other AWS
services like networking, storage, and security.
5. AWS Fargate:
  Serverless compute engine for containers: Runs containers
without managing servers or clusters.
  Pay-per-use: Costs are based on the resources used by
containers.

2.  AMAZON ELASTIC COMPUTE
CLOUD (AMAZON EC2)
EC2 is a web service that provides secure, resizable
compute capacity in the cloud. It is designed to make it easy
for customers to launch and manage virtual servers, known
as instances. EC2 instances run on the Amazon Web Services
(AWS) global infrastructure, and are available in a wide range
of configurations to meet the specific needs of any workload.
EC2 instances are billed by the hour, and customers can
choose to pay either on-demand or with reserved instances.
While on-demand instances provide the greatest degree of
flexibility, they may incur higher costs for continuous-running
duties. Reserved instances provide a substantial reduction in
cost for operations that continue for a minimum of one year.
Customers have the option to commence their operations
promptly by selecting from a diverse range of Amazon
Machine Images (AMIs) that come preconfigured on EC2
instances. AMIs are templates containing the configuration
and software data required to initiate an instance.
Additionally, customers can generate their own AMIs in order
to modify the configuration and software of their instances.
Instances of EC2 can be utilized to operate an extensive
range of applications, databases, web servers, and more.

Additionally, operating machine learning workloads and high-
performance computing (HPC) applications on EC2 is a
common practice.
Here are some of the key features of Amazon EC2:
  Scalability: EC2 instances can be scaled up or down as
needed to meet the changing demands of your workload.
  Reliability: EC2 instances are backed by the Amazon global
infrastructure, which is highly reliable and secure.
  Security: EC2 instances are protected by a variety of
security features, including firewalls, intrusion detection
systems, and encryption.
  Flexibility: EC2 instances offer a wide range of instance
types to choose from, so you can find the right instance for
your specific workload.
  Cost-effectiveness: EC2 instances are billed by the hour, so
you only pay for the resources you use.
Amazon EC2 is a powerful and flexible computing platform
that can be used to run a wide variety of workloads. It is a
popular choice for businesses of all sizes, from startups to
large enterprises.

2.1.  DIFFERENT TYPE OF
AMAZON EC2 INSTANCES
There are five different types of Amazon EC2 instances:
  General purpose: General purpose instances are the most
common type of EC2 instance. They offer a balance of
computing, memory, and networking resources, making them
suitable for a wide range of workloads, such as web servers,
application servers, and databases. M5, T3, T4g
  Compute optimized: Compute optimized instances are
designed to provide high performance for compute-intensive
workloads, such as batch processing, high performance
computing (HPC), and machine learning. C5, C5n, C5gd
  Memory optimized: Memory optimized instances are
designed to provide large amounts of memory for workloads
that need to process large datasets in memory, such as in-
memory databases and data analytics applications. R5, R5n,
R5ad
  Storage optimized: Storage optimized instances are
designed to provide high-performance storage for workloads
that need to read and write large amounts of data quickly,
such as databases and big data applications. H1, H1e, H1ad

  Accelerated computing: Accelerated computing instances
are designed to provide hardware acceleration for specific
types of workloads, such as graphics processing unit (GPU)
accelerated machine learning and video transcoding. P4d, P4,
G5
Each type of EC2 instance is available in a variety of
different instance families, each with its own unique set of
characteristics. For example, the M5 instance family is a
general-purpose instance family that offers a wide range of
instance types with different computing, memory, and
networking resources.
When choosing an EC2 instance, it is important to consider
the specific needs of your workload. For example, if you are

running a web server, you will need an instance with enough
computing resources to handle the expected traffic load. If
you are running a database, you will need an instance with
enough memory and storage to store your data.
Amazon EC2 instances with Families for reference:

2.2 DIFFERENT PRICING MODELS
FOR AMAZON EC2 INSTANCES
There are four different pricing models for Amazon EC2
instances:
  On-Demand: On-demand instances are the most flexible
type of EC2 instance. They can be launched and terminated
at any time, and you are only charged for the time that you
use them. However, on-demand instances are also the most
expensive type of EC2 instance.
  Reserved Instances: Reserved instances are a great way
to save money on your EC2 costs if you can commit to a
certain amount of usage. Reserved instances offer a
significant discount over on-demand instance. However, you
are required to commit to a one-year or three-year term when
you purchase a reserved instance.
  Spot Instances: Spot instances are unused EC2 capacity
that is available at a discounted price. Spot instances can be
a great way to save money on your EC2 costs, but they are
not always available and the price can fluctuate. You can use
Amazon EC2 Spot Fleet to request spot instances and
automatically launch and terminate them based on your
workload needs and budget.

  Dedicated Hosts: Dedicated hosts are physical EC2
servers that are dedicated to your use. Dedicated hosts can
help you to reduce costs by allowing you to use your existing
server-bound software licenses. However, dedicated hosts are
also the least flexible type of EC2 instance, and they can be
more expensive than other types of instances.
Which pricing model is right for you depends on your
specific needs and requirements. If you need the most
flexibility, then on-demand instances are the best option. If
you can commit to a certain amount of usage, then reserved
instances can save you a significant amount of money. If you
are looking for the lowest possible price, then spot instances
may be a good option for you. And if you need a dedicated
physical server, then dedicated hosts are the best option.

2.3 AMAZON MACHINE IMAGE
(AMI)
An Amazon Machine Image (AMI) is a template that contains
the information required to launch an instance. An AMI
includes the operating system, applications, and settings that
will be applied to the instance when it is launched. Some of
the key points for AMI are:
  AMIs are the basic unit of deployment for Amazon Elastic
Compute Cloud (Amazon EC2) instances.
  AMIs can be created and shared by Amazon Web Services
(AWS) users.
  AMIs can be used to launch new instances, to replace
existing instances, and to create backups of instances.
AMIs come in three main categories:
  
Community AMIs: These
AMIs are free to use and are generally packaged with the

operating system of your choice.
  AWS Marketplace AMIs: These AMIs are pay-to-use and
generally come packaged with additional, licensed software.
  My AMIs: These AMIs are created by you and are only
available to your AWS account.
Example:
To launch an instance from an AMI, you select the AMI that
you want to use and then specify the instance type, the
number of instances that you want to launch, and the other
configuration options for the instance. Once you have
launched the instance, it will be running with the same
operating system, applications, and settings as the AMI.
Amazon Instance user data and
Instance Metadata
Amazon Instance user data is text data that you can
specify when launching an Amazon Elastic Compute Cloud
(EC2) instance. User data can be used to customize the
configuration of your instance, install software, or run scripts.
User data is limited to 16 KB in size and must be base64-
encoded.
Amazon Instance Metadata is dynamic data about your
EC2 instance that is provided by the AWS Instance Metadata

Service (IMDS). Instance metadata includes information about
the instance itself, such as its type, instance ID, and
availability zone. It also includes information about the
instance's environment, such as the region and AMI that it
was launched from.
Example:
You can use instance user data to install the Apache web
server on an EC2 instance. The following user data script will
install Apache and start the web server:
#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
To specify this user data when launching your instance,
you can use the AWS CLI or the Amazon EC2 console.
Once your instance is launched, you can retrieve instance
metadata using the IMDS. For example, the following
command will retrieve the instance ID:
curl http://169.254.169.254/latest/meta-data/instance-id
You can also use instance metadata to access user data
that you specified when launching your instance. To do this,
you can use the following URI:
http://169.254.169.254/latest/user-data
The following example shows how to retrieve user data
from within a running instance:
curl http://169.254.169.254/latest/user-data > user-data

This will save the user data to a file called user-data. You can
then view the user data using a text editor.
Instance metadata and user data can be useful for a
variety of tasks, such as:
  Automating the configuration of EC2 instances
  Installing and configuring software on EC2 instances
  Running scripts on EC2 instances
  Troubleshooting EC2 instances
  Collecting information about EC2 instances
However, it is important to note that instance metadata
and user data are not encrypted, so you should not store
sensitive data in them.
169.254.169.254/latest/user-data > user-data
This will save the user data to a file called user-data. You can
then view the user data using a text editor.
Instance metadata and user data can be useful for a
variety of tasks, such as:
  Automating the configuration of EC2 instances
  Installing and configuring software on EC2 instances
  Running scripts on EC2 instances
  Troubleshooting EC2 instances
  Collecting information about EC2 instances

However, it is important to note that instance metadata
and user data are not encrypted, so you should not store
sensitive data in them.

2.4 CLASSICAL PORT IN
WINDOWS AND LINUX
In both Linux and Windows servers, a classical port is a well-
known port number that is used by a specific application or
service. These ports are often used for common tasks such as
logging in to a server, transferring files, or accessing
websites.
The classic ports that you listed are all important ports to
know, especially if you are working with Linux or Windows
servers. Here is a brief summary of some of the important
classical port:
  Port 22: SSH (Secure Shell) is a network protocol that
provides a secure way to log into a remote server. It uses
encryption to protect your data from being intercepted.
  Port 21: FTP (File Transfer Protocol) is a network protocol
that allows you to transfer files between a computer and a
remote server. It is not secure, so it is important to only use
FTP over a trusted network.
  Port 22: SFTP (Secure File Transfer Protocol) is a secure
version of FTP that uses SSH to encrypt your data. It is the
preferred way to transfer files between computers and
remote servers.

  Port 80: HTTP (Hypertext Transfer Protocol) is the protocol
used to transfer web pages and other resources over the
internet. It is not secure, so it is important to only visit
websites that use HTTPS.
  Port 443: HTTPS (Hypertext Transfer Protocol Secure) is a
secure version of HTTP that uses SSL/TLS to encrypt your
data. It is the preferred way to access websites and other
resources over the internet.
  Port 3389: RDP (Remote Desktop Protocol) is a protocol that
allows you to control a remote computer as if you were sitting
in front of it. It is not secure, so it is important to only use
RDP over a trusted network.
The classic ports that you listed are all important ports to
know, especially if you are working with Linux or Windows
servers. In addition to these classic ports, there are many
other ports that are used by different applications and
services. It is important to be aware of which ports are open
on your network and to take steps to secure them.
Here are some tips for securing your network ports:
  Use a firewall to block unused ports.
  Use strong passwords for all services that use ports.
  Keep your software up to date.

  Monitor your network traffic for suspicious activity.
By following these tips, you can help to protect your
network from unauthorized access and attacks.
What is SSH
SSH stands for Secure Shell. It is a network protocol that
provides a secure way to log into a remote computer. SSH
uses encryption to protect your data from being intercepted.
SSH is used for a variety of tasks, including:
  Logging into a remote server
  Transferring files between computers
  Running commands on a remote server
  Creating a secure tunnel between two computers
SSH is a very versatile tool, and it is used by system
administrators, developers, and other IT professionals around
the world.
To connect to an EC2 instance using SSH, you will need to
have an SSH client installed on your local computer.
On macOS or Linux:
1.  Open a terminal window.
2.  Navigate to the directory where your EC2 private key is
stored.

3.  Run the following command:
ssh -i [private_key_file] [ec2_username]@[ec2_public_ip_address]
Replace the following values:
  [private_key_file]: The path to your EC2 private key file.
  [ec2_username]: The username for your EC2 instance.
  [ec2_public_ip_address]: The public IP address of your EC2
instance.
On Windows:
1.  Install an SSH client such as PuTTY.
2.  Open PuTTY and enter the following information:
  Host Name (or IP address): The public IP address of your
EC2 instance.
  Port: 22
  Connection type: SSH
3.  Click Open.
4.  When prompted, enter the username for your EC2
instance.
5.  When prompted, enter the password for your EC2
instance.
Once you have successfully connected to your EC2
instance, you will be able to run commands on the instance

as if you were sitting in front of it.
Here is an example of how to connect to an EC2 instance
using SSH on macOS or Linux:
ssh -i ~/.ssh/my_key.pem ubuntu@10.10.10.10
Where:
  ~/.ssh/my_key.pem is the path to the EC2 private key file.
  ubuntu is the username for the EC2 instance.
  10.10.10.10 is the public IP address of the EC2 instance.
Here is an example of how to connect to an EC2 instance
using SSH on Windows using PuTTY:
1.  Open PuTTY.
2.  Enter the public IP address of your EC2 instance in
the Host Name (or IP address) field.
3.  Click Open.
4.  When prompted, enter the username for your EC2
instance.
5.  When prompted, enter the password for your EC2
instance.

Once you have successfully connected to your EC2
instance, you can start running commands on it.

What is Scalability:
Scalability in cloud computing is the ability to increase or
decrease IT resources as needed to meet changing demand.
It is one of the key benefits of cloud computing, as it allows
businesses to avoid overprovisioning resources and save
money when demand is low.
There are two main types of scalabilities in cloud
computing:
  Vertical scalability involves adding or removing resources to
a single instance, such as increasing the amount of CPU,
memory, or storage. This is similar to scaling up or down a
physical server.
  Horizontal scalability involves adding or removing instances
to a distributed network. This is similar to adding or removing
physical servers to a cluster.

Vertical scalability is often the simplest and most cost-
effective way to scale a system in the cloud, but it has its
limits. Eventually, there will come a point where a single
instance cannot be scaled vertically any further.
Horizontal scalability is the preferred approach for scaling
large systems in the cloud, such as web applications and data
warehouses. Horizontal scalability allows systems to scale
almost infinitely, but it can be more complex and expensive
to implement than vertical scalability.
Cloud computing providers offer a variety of tools and
services that make it easy to scale applications and
infrastructure vertically and horizontally. For example, most
cloud providers offer load balancing services that can
distribute traffic across multiple instances of an application.
Cloud providers also offer auto-scaling features that can

automatically scale applications and infrastructure up or
down based on demand.
Scalability is an important consideration for any business
that is using cloud computing. By designing applications and
infrastructure to be scalable, businesses can ensure that they
will be able to meet the needs of their users and customers,
even as those needs change.
Here are some examples of scalability in cloud computing:
  A web application that can handle more traffic without
slowing down by adding more instances.
  A database that can store more data without crashing by
adding more instances.
  A machine learning model that can train on more data
faster by adding more GPUs.
  A data warehouse that can process more data faster by
adding more nodes.
Scalability is one of the key benefits that cloud computing
offers businesses. By designing applications and
infrastructure to be scalable, businesses can avoid
overprovisioning resources and save money.

2.5 CREATE SETUP TO USE
AMAZON EC2
First of all, we need to have proper understanding in the below
given subjects and associate topics to complete the exercise
“creation of Virtual Instance EC2 in AWS” Successfully.
  EC2 Instances:
o  Understanding instance types and families
o  On-demand, Reserved, Spot, and Dedicated instances
o  Pricing models
  Amazon Machine Images (AMIs):
o  Understanding AMIs and their role in launching instances
  Security:
o  Security groups and their role in controlling access to
instances
o  Key pairs and secure login
  Storage:
o  Elastic Block Store (EBS) volumes and their types
o  Attaching and detaching EBS volumes
o  Creating and managing EBS snapshots
  Networking:

o  VPCs, subnets, and security groups
o  Public and private IP addresses
o  Elastic IPs and their uses
  Monitoring and Management:
o  CloudWatch for monitoring instance performance and health
o  Auto Scaling for automatically scaling instances based on
demand
You can refer subsequent section of this book to gain the
knowledge of mentioned area.
Some of the terminology discussed here for quick reference.
Key Pair: AWS uses public-key cryptography to secure the
login information for your instance. A Linux instance has no
password; you use a key pair to log in to your instance
securely. You specify the name of the key pair when you launch
your instance, then provide the private key when you log in
using SSH.
Security Group: Security groups act as a firewall for
associated instances, controlling both inbound and outbound
traffic at the instance level. You must add rules to a security
group that enable you to connect to your instance from your IP
address using SSH. You can also add rules that allow inbound
and outbound HTTP and HTTPS access from anywhere.
When you launch your instance, you secure it by specifying a
key pair (to prove your identity) and a security group (which acts
as a virtual firewall to control ingoing and outgoing traffic).

When you connect to your instance, you must provide the
private key of the key pair that you specified when you launched
your instance.
Root Volume: Each EC2 instance has a single root
volume that contains the operating system and boot files. It's
essentially the primary storage device for the instance. By
default, the root volume is created using Amazon Elastic Block
Store (EBS) and persists even after restarting the instance. You
can configure different types of EBS storage for the root volume
based on performance and cost requirements.
Additional Volumes: Besides the root volume, you can
attach additional EBS volumes to your EC2 instance for extra
storage space. These volumes appear as separate devices
within the instance and can be used for storing databases,
application data, logs, etc. You can manage these volumes like
the root volume, resizing, detaching, and creating snapshots.

Volume: An EBS volume is a durable, block-level storage
device that you can attach to your Amazon Elastic Compute
Cloud (EC2) instances. It's like a virtual hard drive that persists
independently from the life of the instance it's attached to. You
can use EBS volumes to store data that needs to be persistent,
such as databases, application files, and operating systems.

Lab 1: Creation of EC2 instance in AWS.
We can create the EC2 instance based on the multiple steps
as follows
1.  Sign up for an AWS Account
2.  Create a key pair
3.  Create a Security Group
4.  Launch an instance
5.  Get Information about newly created instance
6.  Connect to newly created instance
Step 1: Sign up for an AWS account
Open the Amazon EC2 console at
https://console.aws.amazon.com/ec2/.
Step 2: Create a key pair
  In the navigation pane, choose Key Pairs.
  Choose Create key pair.

  For Name, enter a descriptive name for the key pair.
  For Key pair type, choose either RSA or ED25519. (Not
supported for Windows)
  For Private key file format, choose the format in which to
save the private key.
  for OpenSSH you can select pem, whereas for PuTTY choose
ppk extension
  Choose Create key pair.

  The private key file- EC2-demo24-pk.pem is automatically
downloaded by your browser. Save the private key file in a safe
place.

  If you plan to use an SSH client on a macOS or Linux
computer to connect to your Linux instance, use the following
command to set the permissions of your private key file so that
only you can read it.
Chmod 400 EC2-demo24-pk.pem
Create a security group
1.  In the left navigation pane, choose Security Groups.
2.  Choose Create security group.
3.  For Basic details, do the following:
a.  Enter a name for the new security group EC2-demo24-SG.
b.  In the VPC list, select your default VPC for the Region.
4.  For Inbound rules, create rules that allow specific traffic to
reach your instance. For example, use the following rules for a
web server that accepts HTTP and HTTPS and ssh traffic.

a.  Choose Add rule. For Type, choose HTTP. For Source,
choose Anywhere.
b.  Choose Add rule. For Type, choose HTTPS. For Source,
choose Anywhere.
c.  Choose Add rule. For Type, choose SSH. For Source, do one
of the following:
Click on create security group.

Launch an instance
1.  From the EC2 console dashboard, in the Launch
instance box, choose Launch instance.
2.  Under Name and tags, for Name, enter name EC2-demo24.
3.  Under Application and OS Images (Amazon Machine
Image),
Select Free Tier eligible.  Amazon Linux Machine Image
Under Instance type, Choose the t2.micro instance type, which
is selected by default.
4.  Under Key pair (login), for Key pair name,
5.  Next to Network settings, choose Edit. For Security
group name, you'll see that the wizard created and selected a
security group for you. You can use this security group, or

alternatively you can select the security group that you created
when getting set up using the following steps:
a.  Choose Select existing security group.
b.  From Common security groups, choose your security
group from the list of existing security groups.
6.  Keep the default selections for the other configuration
settings for your instance.
7.  Review a summary of your instance configuration in
the Summary panel, and when you're ready, choose Launch
instance.

8.  A confirmation page lets you know that your instance is
launching. Choose View all instances to close the confirmation
page and return to the console.
9.  On the Instances screen, you can view the status of the
launch. It takes a short time for an instance to launch. When you
launch an instance, its initial state is pending. After the instance
starts, its state changes to running and it receives a public DNS

name. If the Public IPv4 DNS column is hidden, choose the
settings icon ( 
 ) in the top-right corner, toggle on Public IPv4
DNS, and choose Confirm.

10.  It can take a few minutes for the instance to be ready for
you to connect to it.
11.  that your instance has passed its status checks; you can
view this information in the Status check column.

3.GET INFORMATION ABOUT YOUR
INSTANCE
1.  Navigate to the EC2 Dashboard in the AWS console.
2.  Select the Instances tab from the left-hand menu.
3.  Click on the instance ID of the newly created instance.
4.  In the Details tab, you will find information such as:
o  Public DNS Name: (If the instance has a public IP assigned)
o  Private IP DNS Name: Resolves to the private IPv4 address of
the instance.
o  Private Resource DNS Name: Resolves to the DNS records
selected for the instance.
o  Instance ID: Unique identifier for the instance.
o  Instance Type: The type of hardware the instance is running
on.
o  Launch Time: When the instance was launched.
o  Security Groups: The security groups associated with the
instance.
o  Tags: Any tags that have been applied to the instance.


4. CONNECT NEWLEY CREATED EC2
INSTANCE
There are several ways to connect EC2 instances via the AWS
Management Console, depending on your specific needs and the
operating system of your instances:
1. EC2 Instance Connect (recommended for public instances):
  This method is the simplest and most secure for connecting to
public instances with a web browser.
  Prerequisites:
o  Instance must have a public IPv4 address.
o  You need a supported web browser like Chrome, Firefox, or
Safari.
  Steps:
0.  Go to the EC2 Dashboard in the AWS console.
1.  Select the Instances tab.
2.  Click on the instance ID of the instance you want to connect
to.
3.  Click on Connect.
4.  Choose EC2 Instance Connect.

5.  Verify the username and click Connect.
This will open a web terminal in your browser where you can
interact with the instance.
7.  Press Connect button in below

8.  Final Screen will appear as below

Session Manager (recommended for both public and
private instances):
  This method allows connecting to both public and private
instances with a web browser or SSH client.
  Prerequisites:
o  IAM user needs the SSM:StartSession permission.
  Steps:
0.  Go to the EC2 Dashboard in the AWS console.
1.  Select the Instances tab.
2.  Click on the instance ID of the instance you want to connect
to.
3.  Click on Connect.
4.  Choose Session Manager.
5.  Choose your preferred connection method (web browser or
SSH client).
6.  Click Connect.
This will open a terminal session where you can interact with
the instance.

3. RDP (Windows instances only):
  This method allows remote desktop connection to Windows
instances.
  Prerequisites:
o  Instance must have a public IP address or be in a VPC with a
NAT gateway.
o  You need an RDP client like Microsoft Remote Desktop.
  Steps:
0.  Go to the EC2 Dashboard in the AWS console.
1.  Select the Instances tab.
2.  Click on the instance ID of the instance you want to connect
to.
3.  Click on Connect.
4.  Choose Connect using Remote Desktop.
5.  Enter the public IP address or DNS name of the instance.
6.  Click Connect.

This will open an RDP session where you can interact with the
Windows desktop environment of your instance.
4. SSH (Linux and Windows instances with SSH enabled):
  This method allows secure shell connection to both Linux and
Windows instances with SSH enabled.
  Prerequisites:
o  Instance must have a public IP address or be in a VPC with a
NAT gateway.
o  You need an SSH client like PuTTY.
  Steps:
0.  Go to the EC2 Dashboard in the AWS console.
1.  Select the Instances tab.
2.  Click on the instance ID of the instance you want to connect
to.
3.  Click on Connect.
4.  Choose Connect using SSH client.
5.  Enter the public IP address or DNS name of the instance.
6.  Enter your username and key pair information.
7.  Click Connect.

This will open an SSH session where you can interact with the
command line interface of your instance.

5.  CONNECT TO AN EC2 INSTANCE
USING PUTTY
It is one of the old popular methods to connect with EC2
instance using PuTTY. We also have one another software name
by PuttyGen used to convert pem extension based private key to
ppk extension based private key.
P.S. Earlier we do have only pem based file but now AWS
providing both the option i.e. pem as well as ppk.
1. Download and Install PuTTY:
  Visit the PuTTY website (https://www.putty.org/) and download
the appropriate installer for your Windows operating system.
  Run the installer to complete the installation process.
2. Prepare Your Key Pair:

  If you don't have a key pair already, create one in the AWS
Management Console when launching your EC2 instance.
  Download the PEM key file and store it securely.
3. Convert PEM to PPK (Optional):
  PuTTY requires a PPK (PuTTY Private Key) file format. If you
have a PEM file, use PuTTYgen to convert it:
o  Open PuTTYgen.
o  Click "Load" and select your PEM file.
o  Click "Save private key" and choose a location to save the PPK
file
4. Open PuTTY: 
Launch the PuTTY application.
5. Configure Connection Settings:
Host Name (or IP address): Enter the public IP address or
hostname of your EC2 instance. You can find this information
in the AWS Management Console.
Port: Set the port to 22 (default for SSH).
Connection type: Select "SSH."

One need to select Auth Option inside the SSH and select Private key file for
authentication: Browse and select your PPK file

6. Start the Connection:
  Click "Open" to initiate the connection.
7. Log In:
  If prompted, accept the security certificate.
  Enter the username for your EC2 instance (usually "ec2-user"
for Amazon Linux or "ubuntu" for Ubuntu).
  You won't be prompted for a password if you've configured the
key pair correctly.
8. Interact with Your Instance:

  Once connected, you'll see a terminal window where you can
interact with your EC2 instance using command-line commands.

6. MULTI-CHOICE QUESTIONS AND
ANSWER
1. What does EC2 stand for?
(a) Elastic Cloud Compute
(b) Elastic Compute Cloud
(c) Elastic Container Cluster
(d) Efficient Cloud Configuration
Answer: (b)
Explanation: EC2 stands for Elastic Compute Cloud, a
service offering on-demand, scalable virtual servers in the
cloud.
2. What is the main advantage of using EC2 compared to
physical servers?
(a) Lower overall cost
(b) Increased security and compliance
(c) Improved user experience with faster loading times
(d) All of the above
Answer: (d)
Explanation: EC2 offers all of these benefits: it's cost-
effective due to pay-as-you-go pricing, secures resources in

the cloud, and scales instantly to meet changing needs,
leading to faster loading times.
3. What is an AMI (Amazon Machine Image)?
(a) A template defining the configuration of an EC2 instance.
(b) A snapshot of the data stored on an EC2 instance.
(c) A virtual disk attached to an EC2 instance.
(d) A security group assigned to an EC2 instance.
Answer: (a)
Explanation: An AMI serves as a blueprint for launching
new EC2 instances, containing the operating system,
software, and configurations.
4. What is user data in EC2?
(a) Information about the user who launched the instance.
(b) A script executed during the first boot of an EC2 instance.
(c) Login credentials for accessing the instance.
(d) Monitoring data collected from the instance.
Answer: (b)
Explanation: User data allows configuring an EC2 instance
automatically upon launch, often used for installing software,
setting up applications, or running scripts.

5. What pricing model does EC2 use?
(a) Fixed monthly cost per instance
(b) Pay-as-you-go based on usage
(c) Subscription-based model with tiers
(d) One-time fee for each instance
Answer: (b)
Explanation: EC2 utilizes a pay-as-you-go pricing model,
charging based on the instance type, running time, and other
resources used, optimizing cost based on actual needs.
6. What is a port in EC2 and how is it used?
(a) A physical connection point on the instance
(b) A logical channel for network communication
(c) A storage location for user data
(d) A security group rule definition
Answer: (b)
Explanation: Ports are virtual communication channels,
identified by numbers, used for specific protocols like HTTP
(port 80) or SSH (port 22). Security groups control inbound
and outbound traffic through specific ports.
7. What is a security group in EC2 and its purpose?
(a) A group of users accessing the instance

(b) A firewall controlling inbound and outbound traffic
(c) A monitoring tool for instance health
(d) A configuration for user data scripts
Answer: (b)
Explanation: Security groups act as virtual firewalls,
defining rules that specify which network traffic is allowed to
reach or leave an instance, enhancing security.
8. What is an Availability Zone (AZ) in EC2 and why is it
important?
(a) A location within a region where instances are physically
stored
(b) A pricing zone with different costs
(c) A group of instances with shared resources
(d) A monitoring zone for tracking instance performance
Answer: (a)
Explanation: AZs are physically separate data centers
within a region, providing redundancy and fault tolerance.
Distributing instances across AZs ensures high availability if
one AZ experiences an outage.
9. What is an EBS volume in EC2 and its benefit?

(a) A temporary storage space attached to an instance
(b) A network interface for connecting to the internet
(c) A persistent storage volume independent of the instance
(d) A snapshot capturing the state of an instance
Answer: (c)
Explanation: EBS volumes provide persistent block storage
that survives instance reboots or terminations, ensuring data
persists even when instances are replaced.
10. What is an Elastic IP address in EC2 and its benefits?
(a) A temporary IP address assigned to the VM for a limited
duration.
(b) A static IP address associated with the instance, even
across restarts.
(c) An IP address used for accessing the VM's management
console.
(d) An IP address shared by multiple instances for cost
optimization.
Answer: (b)
Explanation: An Elastic IP address is a static IP address
that remains associated with the instance even after stopping
and restarting it, offering consistent reachability.

CHAPTER 5  AWS LOAD BALANCER
AND AUTOSCALING

1.  OVERVIEW OF LOAD
BALANCER
A load balancer is a network device that distributes incoming
network traffic across a group of servers. This can improve
performance and reliability by preventing any one server
from becoming overloaded. Load balancers are also used to
improve security by providing a single point of entry for all
incoming traffic.
Load balancer with picture
Here are some of the benefits of using a load balancer:
  Improved performance: Load balancers can improve
performance by distributing traffic across multiple servers.
This can help to reduce latency and improve response times.
  Increased reliability: Load balancers can increase reliability
by ensuring that traffic is always routed to available servers.
This can help to prevent outages and downtime.
  Improved security: Load balancers can improve security by
providing a single point of entry for all incoming traffic. This
makes it easier to implement security measures, such as
firewalls and intrusion detection systems.
There are two main types of load balancers:

  Hardware load balancers: Hardware load balancers are
dedicated devices that are designed to distribute network
traffic. They are typically used in large enterprise networks.
  Software load balancers: Software load balancers are
software programs that can be installed on any server. They
are typically used in smaller networks and cloud
environments.
Load balancers can be configured to distribute traffic in a
variety of ways, including:
  Round robin: Round robin load balancers distribute traffic
evenly across all servers in the group.
  Weighted round robin: Weighted round robin load balancers
distribute traffic to servers based on their weight. This can be
used to give more or less traffic to certain servers, depending
on their capacity or importance.
  Least connections: Least connections load balancers
distribute traffic to the server with the fewest connections.
This helps to ensure that all servers are evenly loaded.
  IP hash: IP hash load balancers distribute traffic to the same
server for each client IP address. This can be used to improve
performance and reliability for long-lived connections.

Load balancers are an essential tool for any network that
needs to improve performance, reliability, or security.
Elastic Load Balancer:
An Elastic Load Balancer (ELB) is a managed load balancer
that automatically distributes incoming application traffic
across multiple targets, such as EC2 instances, containers,
and IP addresses, in one or more Availability Zones (AZs).
ELBs can be configured to distribute traffic based on various
factors, such as the performance of the targets, the
geographic location of the clients, and the type of traffic.
Elastic Load Balancing works with the following services to
improve the availability and scalability of your applications.
  Amazon EC2 — Virtual servers that run your applications in
the cloud.
  Amazon EC2 Auto Scaling — Amazon EC2 Elastic Service
  AWS Certificate Manager — When you create an HTTPS
listener, you can specify certificates provided by Aws
Certification Manager.
  Amazon CloudWatch — Enables you to monitor your load
balancer and to take action as needed.

  Amazon ECS — Enables you to run, stop, and manage
Docker containers on a cluster of EC2 instances.
  AWS Global Accelerator — Improves the availability and
performance of your application. Use an accelerator to
distribute traffic across multiple load balancers in one or
more AWS Regions.
  Route 53 — AWS DNS Service
  AWS WAF — You can use AWS WAF with your Application
Load Balancer to allow or block requests based on the rules in
a web access control list (web ACL)

2.  DIFFERENT TYPE OF LOAD
BALANCER
AWS offers four types of Elastic Load Balancers (ELBs):
›  Application Load Balancers (ALBs)
›  Network Load balancer
›  Gateway Load Balancer (GLB)
›  Classical Load Balancer (GLB)

2.1 APPLICATION LOAD BALANCER
(ALB)
ALB is a Layer 7 load balancer that distributes traffic based on
application-layer information, such as the HTTP request path or
the HTTP headers. ALBs are ideal for web applications and other
applications that use HTTP.
Application load balancer works on round-robin algorithm,
which is the simplest. It cycles through your targets in order, so
each target should receive an equal share of requests. Due to its
simplicity, all load balancers support the round-robin algorithm.
Above diagram is showing how the round-robin algorithm
directs requests. It shows the service scaling up from 5 tasks to
6, after which the new task starts receiving an equal share of the
traffic — 1/6th.

Lab 1: Creation of Application Load
Balancer
To create a load balancer using the AWS Management
Console, complete the following tasks.
Step 1: Configure a target group
Step 2: Register targets
Step 3: Launch and Select Application Load Balancer
Step 4: Configure a load balancer and a listener
Step 5: Test the load balancer
Step 1: Configure a target group
  Access the Target Groups section in the EC2 or Elastic Load
Balancing (ELB) console.
  Create a new target group, specifying:
o  Name
o  Target type (e.g., instances, IP addresses, Lambda functions)
o  Protocol and port (e.g., HTTP/80, HTTPS/443)
o  VPC
o  Health checks (to monitor target health)

Step 2: Register targets
  Add the instances, IP addresses, or Lambda functions that will
receive traffic to the target group.
Step 3: Launch and Select Application Load Balancer.
Open the Amazon EC2 console
at https://console.aws.amazon.com/ec2/.
On the navigation bar, choose a Region for your load
balancer. Be sure to choose the same Region that you used for
your EC2 instances.
In the navigation pane, under Load Balancing, choose Load
Balancers.
Choose Create load balancer.
For Application Load Balancer, choose Create.

Step 3: Configure a load balancer and a listener.
  Create a load balancer by using different options as per added
picture:
o  Basic configuration

o  Network mapping
o  Security groups
o  Listeners and routing
o  Add-on services – optional
In Basic Configuration Section add values:
Load Balancer Name: Application-LB-Demo24
Scheme: Internet Facing
IP Address Type: IPv4

As per Network mapping section: We need to Select at least
two Availability Zones and one subnet per zone. The load
balancer routes traffic to targets in these Availability Zones only.
Availability Zones that are not supported by the load balancer or
the VPC are not available for selection.

Select Security group and protocol/port: HTTP/80 or
HTTPS/443 from dropdown and give the earlier created target
group name.
This target groups are collection of EC2 instance applicable
for Application Load Balancer.
Once done you need to click the Create load balancer button.

Step 4: Test the load balancer
1.  After the load balancer is created, choose Close.
2.  In the navigation pane, choose Target Groups.
3.  Select the newly created target group.
4.  Choose Targets and verify that your instances are ready. If
the status of an instance is initial, it's typically because the
instance is still in the process of being registered. You need to
wait unit status of instance is become health.
5.  In the navigation pane, choose Load Balancers.
6.  Select the newly created load balancer.

7.  Choose Description and copy the DNS name of the internet
facing or internal load balancer (for example, my-load-balancer-
1234567890abcdef.elb.us-east-2.amazonaws.com).
8.  Now you can use this endpoint to check the loadbalancer
working.
Opens in a new

2.2  NETWORK LOAD BALANCER
Network Load balancer distributes traffic based on network-layer
information, such as the IP address or the port number. NLBs are
ideal for high-performance applications, such as gaming
applications and video streaming applications.
Some of other key properties of Network Load Balancer are
  Supports static IP per availability zone (You only need to
whitelist single IP)
  Ideal for applications with long-running connections.
  Extremely low latencies
  Preserves source IP as it’s a pass-through load balancer.

  Uses flow hash of 5-tuple(Source IP, Destination IP, Protocol,
Source Port, and Destination Port) and seq id as routing
algorithm
  Same API as Application Load Balancer.
Lab 2: Creation of Network Load Balancer
To create a load balancer using the AWS Management
Console, complete the following tasks.
Step 1: Configure a target group
Step 2: Register targets
Step 3: Launch and Select Application Load Balancer
Step 4: Configure a Network load balancer and a listener
Step 5: Test the load balancer
Step 1: Configure a target group
  Access the Target Groups section in the EC2 or Elastic Load
Balancing (ELB) console.
  Create a new target group, specifying:
o  Name
o  Target type (e.g., instances, IP addresses, Lambda functions)
o  Protocol and port (e.g., TCP/80)

o  VPC
o  Health checks (to monitor target health)
Step 2: Register targets
  Add the instances, IP addresses, or Lambda functions that will
receive traffic to the target group.
Step 3: Launch and Select Application Load Balancer.
Open the Amazon EC2 console
at https://console.aws.amazon.com/ec2/.
On the navigation bar, choose a Region for your load
balancer. Be sure to choose the same Region that you used for
your EC2 instances.
In the navigation pane, under Load Balancing, choose Load
Balancers.
Choose Create load balancer.
For Network Load Balancer, choose Create.

Step 4: Configure a Network load balancer and a listener
Create a load balancer by using different options as per
added picture:

o  Basic configuration
o  Network mapping
o  Security groups
o  Listeners and routing
o  Add-on services – optional
In Basic Configuration Section add values:
Load Balancer Name: Network-LB-Demo24
Scheme: Internet Facing
IP Address Type: IPv4
As per mapping section: We need to Select at least two
Availability Zones and one subnet per zone. The load balancer

routes traffic to targets in these Availability Zones only.
Availability Zones that are not supported by the load balancer or
the VPC are not available for selection.
Select Security group and protocol/port: TCP/80 from dropdown
and give the earlier created target group name.
This target groups are collection of EC2 instance applicable
for Network Load Balancer.

Once done you need to click the Create load balancer button.

etwork Load Balancer (NLB)

2.3  GATEWAY LOAD BALANCER
(GLB)
GLB is a Layer 3 load balancer that distributes traffic across
multiple VPCs or across multiple on-premises networks. GLBs are
ideal for routing traffic between different networks.
Here's a breakdown of how it works, complete with visuals:
Overview:
  GWLB is designed specifically for third-party virtual
appliances like firewalls, intrusion detection systems, and deep
packet inspection tools.
  It acts as a central gateway for all traffic entering and exiting
your virtual appliances, distributing it across a fleet of instances
for scalability and high availability.
Key components:

1.  Gateway Load Balancer: The central component that receives
all traffic and forwards it to target groups.
2.  Target Groups: Collections of virtual appliance instances that
participate in load balancing.
3.  Listener: Monitors for incoming traffic using the GENEVE
protocol on port 6081.
4.  Virtual Appliances: Third-party software instances deployed in
your VPC.
5.  Route Tables: Direct traffic to the GWLB for processing.
Workflow:
1.  Client request: Traffic arrives at the GWLB through a route
table configuration.
2.  Listener: The listener receives the traffic and forwards it to
the target group based on predefined rules.

3.  Target selection: GWLB chooses a healthy virtual appliance
instance from the target group.
4.  GENEVE encapsulation: The traffic is encapsulated with a
GENEVE header for secure communication.
5.  Forwarding to appliance: The encapsulated traffic is
forwarded to the chosen appliance instance.
6.  Appliance processing: The virtual appliance processes the
traffic and sends a response back.
7.  Decryption and forwarding: GWLB receives the response,
decrypts it, and forwards it back to the client.
Benefits of GWLB:
Scalability: Easily add or remove virtual appliance instances
to handle changing traffic demands.
  High availability: Routes traffic around unhealthy appliances,
ensuring service continuity.
  Centralized management: Simplifies deployment and
management of virtual appliances.
  Private connectivity: Securely connects virtual appliances
across VPCs using GWLB endpoints.

  Integration with AWS services: Works seamlessly with Auto
Scaling and other AWS services.

Gateway Load Balancer (GLB)

2.4  CLASSIC LOAD BALANCER
(CLB)
CLB is an older type of load balancer that is no longer
recommended for new applications. CLBs are a Layer 4 and
Layer 7 load balancer that can distribute traffic based on both
network-layer and application-layer information.

Which type of Elastic Load Balancer
should I use?
The type of ELB you should use depends on your specific
needs. If you need to distribute HTTP traffic to web
applications, you should use an ALB. If you need to distribute
TCP traffic to high-performance applications, you should use
an NLB. If you need to distribute traffic between different
networks, you should use a GLB.
If you are not sure which type of ELB to use, you can
contact AWS support for assistance.
Additional information:
  All four types of ELBs are highly available and scalable.
  ELBs can be used to distribute traffic to EC2 instances,
containers, and IP addresses.
  ELBs can be configured to use different load balancing
algorithms, such as round robin, weighted round robin, and
least connections.
  ELBs can be used to implement security features, such as
SSL termination and DDoS protection.
Here are some examples of when you might use each type
of ELB:

  ALB:
o  A web application that serves dynamic content, such as a
news website or an e-commerce website.
o  A mobile application backend.
o  A load balancer for a microservices architecture.
  NLB:
o  A gaming application backend.
o  A video streaming application backend.
o  A load balancer for a high-performance computing (HPC)
cluster.
  GLB:
o  A load balancer to distribute traffic between different VPCs.
o  A load balancer to distribute traffic between an AWS VPC
and an on-premises network.
o  A load balancer to distribute traffic between multiple
regions.

3.WHAT’S AN AUTO SCALING
GROUP
An Auto Scaling group in AWS is a collection of EC2 instances
that are treated as a logical grouping for the purposes of
automatic scaling and management. An Auto Scaling group can
launch On-Demand Instances, Spot Instances, or both.
The goal of an Auto Scaling Group (ASG) is to:
• Scale out (add EC2 instances) to match an increased load
• Scale in (remove EC2 instances) to match a decreased load
• Ensure we have a minimum and a maximum number of
machines running
• Automatically register new instances to a load balancer
• Replace unhealthy instances.
For example, the following Auto Scaling group has a minimum
size of one instance, a desired capacity of two instances, and a
maximum size of four instances. The scaling policies that you
define adjust the number of instances, within your minimum and
maximum number of instances, based on the criteria that you
specify.


3.1  AUTO SCALING COMPONENTS
There are three important components, while working with
autoscaling:
Groups
Launch Configuration/Template
Scaling Plans
Groups
Groups are the logical groups which contain the collection of
EC2 instances with similar characteristics for scaling and
management purposes. Using the auto scaling groups you can
increase the number of instances to improve your application
performance and you can decrease the number of instances
depending on the load to reduce your cost. The auto-scaling
group also maintains a fixed number of instances even if an
instance becomes unhealthy.
To meet the desired capacity the auto scaling group launches
many EC2 instances, and auto scaling group maintains these
EC2 instances by performing a periodic health check on the
instances in the group. If any instance becomes unhealthy, the
auto-scaling group terminates the unhealthy instance and
launches another instance to replace it. Using scaling policies,
you can increase or decrease the number of running EC2
instances in the group automatically to meet the changing
conditions.
Launch Configuration

The launch configuration is a template used by the auto
scaling group to launch EC2 instances. You can specify the
Amazon Machine Image (AMI), instances type, key pair, and
security groups etc. while creating the launch configuration. You
can also modify the launch configuration after creation. Launch
configuration can be used for multiple auto scaling groups.
Scaling Plans
Scaling plans tells Auto Scaling when and how to scale.
Amazon EC2 auto-scaling provides several ways for you to scale
the auto scaling group.
Maintaining Current instance level at all time: - You can
configure and maintain a specified number of running instances
at all the time in the auto scaling group. To achieve this Amazon
EC2 auto-scaling performs a periodic health check on running
EC2 instances within an auto scaling group. If any unhealthy
instance occurs, auto-scaling terminates that instance and
launches new instances to replace it.
Manual Scaling: - In Manual scaling, you specify only the
changes in maximum, minimum, or desired capacity of your auto
scaling groups. Auto-scaling maintains the instances with
updated capacity.
Scale based on Schedule: - In some cases, you know
exactly when your application traffic becomes high. For example,
on the time of limited offer or some particular day in peak loads,
in such cases, you can scale your application based on
scheduled scaling. You can create a scheduled action which tells

Amazon EC2 auto-scaling to perform the scaling action based on
the specific time.
Scale based on demand: - This is the most advanced
scaling model, resources scales by using a scaling policy. Based
on specific parameters you can scale in or scale out your
resources. You can create a policy by defining the parameters
such as CPU utilization, Memory, Network In and Out etc. For
Example, you can dynamically scale your EC2 instances which
exceeds the CPU utilization beyond 70%. If CPU utilization
crosses this threshold value, the auto scaling launches new
instances using the launch configuration. You should specify two
scaling policies, one for scaling in (terminating instances) and
one for scaling out (launching instances).
Types of Scaling polices: -
  Target tracking scaling: - Based on the target value for a
specific metric, Increase or decrease the current capacity of the
auto scaling group.
  Step scaling: - Based on a set of scaling adjustments, increase
or decrease the current capacity of the group that vary based on
the size of the alarm breach.
  Simple scaling: - Increase or decrease the current capacity of
the group based on a single scaling adjustment.
Lab 3: Create a Autoscaling group using
Launch Template.

Here's a visual guide to creating an Autoscaling Group (ASG)
on AWS:
1. Define Your Requirements:
Application: What application will the ASG support? This will
determine the instance type and configuration.
Scalability: What are your expected traffic fluctuations?
Define minimum, maximum, and desired number of
instances.
Scaling Metrics: What metrics will trigger scaling? CPU
utilization, network traffic, or custom metrics?
2. Choose a Launch Template or Configuration:
Launch Template: Pre-defined configuration for your instances, including AMI,
instance type, security groups, etc. Ideal for reusable configurations.
  Navigate to the EC2 console.
  Click on "Launch Templates" in the left-hand navigation pane.
Now let’s do all this exercise one by one as per given below:
1. Create an EC2 Launch template my-LT-Demo and provide
Template version description.

2. Select Quick Start and Amazon Linux 2023 AMI with
64-bit Architecture.

3. Select existing Security Group launch-wizard-1 having
SSH, http and https port enable from anywhere (0.0.0.0/0)


4. Keep default setting as per above picture related to
Storage(volume) and Resource tags. Enable Advance Details
section and keep all relevant section values are at default level
except user-data (Optional)

5. Add below script in user-data section and click on crate
launch template.
#!/bin/bash
# Use this for your user data (script from top to bottom)
# install httpd
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html

6.  Create a my-TG-Demo Target group, which is available in
Load Balancing Section and select the different available
option for given section:
Basic Configuration: Instances
IP Address Type: IP4
Protocol Version: HTTP1 and click on Create Target Group.
This will create my-TG-Demo target group.



7:  Create an autoscaling group my-ASG-Demo using
t2.micro instances. Some of important option

Launch Template: my-LT-Demo
vCPU: 1(Min), 1(Max)
Memory (GiB): 1(Min), 2(Max)
VPC: Default
Availability Zone: us-east-1a, us-east-1b and us-east-1c


8:  Some other options while creating ASG are:
Load Balancing: No Load Balancer
Enable group matrices collection within cloud watch

9. Configure group size and scaling

Desired capacity: 2
Min desired capacity: 2
Max desired capacity: 4
10: Choose whether to use a target tracking policy: Target
tracking scaling policy
Scaling policy name: Target Tracking Policy
Metric type: Average CPU Utilization
Target Value: 50
Instance maintenance policy: No Policy
Add notifications: None
Add tags: None




11:  Finaly Review will come on screen. After examine the
review, once you find everything is ok then You can click on
Create Auto Scaling Group.





12: Once you click on Create Auto Scaling group. It will create
my-ASG-Demo autoscaling group.
Step 13: You can find all the details for the newly create auto
scaling group my-ASG-Demo once you click on my-ASG-Demo
mentioned in red square.

14: All instances’ details are available in Instance
Management section.

4.  MULTI-CHOICE QUESTIONS
AND ANSWER
1. What is the primary function of a load balancer in AWS?
(a) To encrypt and decrypt data traffic for increased security.
(b) To distribute incoming traffic evenly across multiple
resources.
(c) To provide network storage for your applications.
(d) To manage user authentication and access control.
Answer: (b)
Explanation: Load balancers act as traffic directors,
distributing incoming requests across multiple servers or
instances, preventing any single server from being
overwhelmed and ensuring high availability and
responsiveness.
2. Which type of load balancer is best suited for Layer 7
(HTTP/HTTPS) traffic manipulation and path-based routing?
(a) Network Load Balancer (NLB)
(b) Application Load Balancer (ALB)
(c) Gateway Load Balancer (GLB)
(d) Classic Load Balancer (CLB)
Answer: (b)

Explanation: ALB excels at handling application-layer
traffic, allowing content-based routing, path rewriting, and
other advanced traffic management features.
3. What is a key difference between an NLB and an ALB?
(a) NLB supports higher connection throughput but offers
fewer routing options.
(b) ALB prioritizes security features over raw performance.
(c) NLB exclusively works with TCP/UDP protocols, while ALB
focuses on HTTP/HTTPS.
(d) NLB is better suited for internal traffic routing, while ALB
is ideal for public-facing applications.
Answer: (a)
Explanation: NLB prioritizes high-speed, low-latency
connections for Layer 4 (TCP/UDP) traffic, while ALB sacrifices
some raw performance for more granular application-layer
control.
4. What is an Auto Scaling Group in AWS?
(a) A group of users with shared access permissions.
(b) A collection of instances automatically scaled based on
defined rules.
(c) A storage bucket for managing backups and snapshots.

(d) A network security group controlling inbound and
outbound traffic.
Answer: (b)
Explanation: An Auto Scaling Group automatically adjusts
the number of running instances based on defined metrics
like CPU utilization, ensuring your application has the
resources it needs while optimizing costs.
5. Which component of an Auto Scaling Group determines
when to scale up or down?
(a) The instance type and size
(b) The security group configuration
(c) The launch template used for provisioning instances
(d) The scaling policy with defined metrics and thresholds
Answer: (d)
Explanation: The scaling policy dictates when to scale
based on metrics like CPU usage or network traffic. It defines
thresholds for scaling up (adding instances) and scaling down
(terminating instances), ensuring efficient resource
utilization.

CHAPTER 6  AMAZON STORAGE
SERVICE


1.  AWS STORAGE SYSTEM
A storage system in AWS is a service that provides you with a
place to store your data. AWS offers a variety of storage
services, each with its own unique features and benefits. You
can choose the storage service that best meets your specific
needs, such as the
Type of data you need to store,
How often you need to access it, and
How much you are willing to pay.
AWS storage services can be classified into three main
categories:
  Object storage
  Block storage
  
File storage

Object Storage: Object storage is a type
of storage that stores data as objects, which are self-
contained units of data that include the data itself, metadata
about the data, and a unique identifier. Object storage is
highly scalable and durable, making it ideal for storing large
amounts of data, such as data lakes, websites, and mobile
applications. The best example of Object Storage is Amazon
Simple Storage Service (S3). S3 is an object storage service
that offers industry-leading scalability, data availability,
security, and performance. S3 is designed to store and
retrieve any amount of data, at any time, from anywhere on
the web.
Block Storage: Block storage is a type of
storage that stores data in blocks, which are fixed-size units
of data. Block storage is ideal for storing data that needs to
be frequently accessed and updated, such as database files
and application logs. The classical example of AWS block
storage is Amazon Elastic Block Store (EBS). EBS is a block
storage service that provides durable, block-level storage
volumes that you can attach to a running instance. EBS is

ideal for data that requires frequent and granular updates,
such as database files and application logs.
File Storage: File storage is a type of
storage that stores data in files, which are logical constructs
that contain related data. File storage is ideal for storing data
that needs to be accessed and shared in a traditional file-
based manner, such as user directories and application
development files. Amazon Elastic File System (EFS) is
example of File system type EWS Storage. EFS is a fully
managed, scalable file storage service for use with AWS
Cloud. EFS provides a scalable, elastic file share that you can
use to share files between EC2 instances, on-premises
servers, and other devices.
AWS also offers a number of other storage services, such
as Amazon Glacier (for long-term archival storage) and
Amazon Elastic Container Registry (for container images),
Amazon Storage Gateway and AWS Snow Family.
Amazon Storage Gateway: Storage Gateway is a hybrid
cloud storage service that enables you to store data on-
premises and in the AWS cloud. Storage Gateway provides
two different types of gateways: File Gateway and Volume
Gateway. File Gateway connects an on-premises file server to
AWS S3, enabling you to cache files locally and archive them

to the cloud. Volume Gateway connects an on-premises
storage volume to AWS EBS, enabling you to create cached or
stored EBS volumes.
AWS Snow Family: The AWS Snow Family is a family of
portable storage and edge computing devices that enable
you to transfer large amounts of data to and from AWS. The
Snow Family includes three devices: Snowcone, Snowball, and
Snowmobile.
Amazon Glacier: It is a low-cost, secure, and durable
storage service for long-term archiving and backup of cold
data. Cold data is data that is rarely or never accessed, but
must be retained for compliance, regulatory, or other
reasons. Amazon Glacier is ideal for storing data such as
medical images, financial records, and customer data.
Amazon Glacier is designed to provide 99.999999999% (11
nines) of data durability. Once your data is in Amazon Glacier,
it is not immediately accessible. You must first initiate a
retrieval request. Retrieval requests can take several hours to
complete, depending on the size of the data and the retrieval
class you choose.
Amazon Glacier offers three retrieval classes:
  Expedited: Expedited retrievals are typically completed
within 1-5 minutes. However, expedited retrievals are more
expensive than standard retrievals.

  Standard: Standard retrievals are typically completed within
3-5 hours.
  Bulk: Bulk retrievals are typically completed within 5-12
hours. Bulk retrievals are the least expensive retrieval option.
Amazon Glacier is compliant with a variety of industry
standards, including HIPAA, PCI DSS, and GDPR. Finally, we
can say that Amazon Glacier is one of the most affordable
storage services available.
Amazon Elastic Container Registry (Amazon ECR): It
is a fully managed container registry that makes it easy for
developers to store, share, and deploy container images and
artifacts. Amazon ECR is integrated with Amazon Elastic
Container Service (Amazon ECS) and Amazon Elastic
Kubernetes Service (Amazon EKS), simplifying your
development to production workflow. Amazon ECR eliminates
the need to operate your own container repositories or worry
about scaling the underlying infrastructure.
ECR is highly reliable and available. Your images are stored
in multiple Availability Zones, so they are always
accessible, even in the event of a failure in one Availability
Zone. It is also highly secure (You can use AWS Identity and
Access Management (IAM) to control access) and scalable in
nature.
Use cases for Amazon ECR:

  Developing and deploying containerized
applications: Amazon ECR is a good choice for developing
and deploying containerized applications. It is easy to
use, integrated with Amazon ECS and Amazon EKS, and
secure and reliable.
  Storing and sharing container images: Amazon ECR is a
good choice for storing and sharing container images. It is
scalable and reliable, and it offers a variety of features for
managing your images.
  Building and maintaining container registries: Amazon ECR
is a good choice for building and maintaining container
registries. It is easy to use and manage, and it offers a variety
of features for managing your registries.

2.  SIMPLE STORAGE SERVICE(S3)
Amazon Simple Storage Service (S3) is an object storage
service that offers industry-leading scalability, data
availability, security, and performance. You can store any
type of file in S3.S3 is designed to deliver 99.999999999%
durability and 99.99% Availability. Customers of all sizes and
industries can use S3 to store and protect any amount of data
for a wide range of use cases, such as:
  Websites and web applications: S3 is a popular choice for
storing website and web application content, such as images,
videos, and HTML files. S3 is highly scalable and can handle
even the most demanding traffic loads.
  Mobile applications: S3 can be used to store mobile
application data, such as user profiles, game assets, and
media files. S3 is also a good choice for hosting mobile
application backends.
  Data lakes: S3 is a popular choice for storing data lakes,
which are large repositories of raw data that can be used for
analytics and machine learning. S3 is highly scalable and can
handle even the largest data lakes.
  Backup and restore: S3 can be used to back up data from
on-premises servers and applications. S3 is also a good
choice for restoring data from backups.

  Archive: S3 can be used to archive data that needs to be
retained for long periods of time, but is not accessed
frequently. S3 is highly durable and can protect your data
from corruption and loss.
In addition to these general use cases, S3 is also used for
a variety of more specific applications, such as:
  Streaming video and audio: S3 can be used to store and
deliver streaming video and audio content. S3 is highly
scalable and can handle even the most demanding streaming
workloads.
  Internet of Things (IoT): S3 can be used to store and
manage data from IoT devices. S3 is scalable and can handle
even the largest IoT datasets.
  Machine learning: S3 can be used to store and manage data
for machine learning applications. S3 is highly scalable and
can handle even the largest machine learning datasets.
  Gaming: S3 can be used to store and manage game assets,
such as textures, models, and code. S3 is highly scalable and
can handle even the most demanding gaming workloads.
Overall, S3 is a versatile and powerful object storage
service that can be used for a wide range of use cases. S3 is

highly scalable, secure, and durable, and it offers a variety of
features for managing your data.

2.1  S3 – WHAT IS BUCKET?
A bucket in S3 is a logical container for your
objects. It is similar to a folder on your computer, but it is
designed to store objects of any size, type, and access
pattern. You can store any number of objects in a bucket, and
you can have up to 100 buckets in your account.
In essence, Files are stored in buckets and Buckets are
root level folders. The size of stored Files can be anywhere
from 0 bytes to 5 TB. There is unlimited storage available.
S3 is a universal namespace so bucket names must be
unique globally. However, you create your buckets within a
REGION. It is a best practice to create buckets in regions that
are physically closest to your users to reduce latency.
The stored Objects in bucket consist of:
›  Key (name of the object)
›  Value (data made up of a sequence of bytes)
›  Version ID (used for versioning)
›  Metadata (data about the data that is stored)

2.2  DIFFERENT FUTURES IN
SIMPLE STORAGE SERVICE(S3)
AWS Simple Storage Service(S3) has the following futures:
  Different storage classes
  Data Security using Access Control Lists and Bucket Policies
  Lifecycle Management of different storage classes
  Versioning
  Encryption

2.2.1  S3 - Different Storage Classes
Amazon S3 offers multiple storages classes depending on your
use case scenario and performance access requirements. These
are classified as:
  Storage classes for frequently accessed objects
  Storage class for automatically optimizing data
  Storage classes for infrequently accessed objects
  Storage classes for archiving objects
Storage classes for frequently accessed
objects

  S3 Standard: S3 Standard is the default storage class for
S3. It is designed for frequently accessed data, such as website
and web application content, mobile application data, and
streaming video and audio content. S3 Standard offers high
availability and performance.
   
  S3 Express One Zone – Specific-built to provide consistent,
single-digit millisecond data access for your most latency-
sensitive application and it is high-performance, single-zone
Amazon S3 storage class. It is having data access speeds that
are up to ten times faster and request costs that are fifty
percent lower than S3 Standard.
  Reduced Redundancy – Designed for reproducible, non-
critical data, the Reduced Redundancy Storage (RRS) storage
class provides less redundancy than the S3 Standard storage
class.
Storage class for automatically optimizing data
  S3 Intelligent-Tiering: Amazon S3 storage class S3
Intelligent-Tiering is specifically engineered to optimize storage
expenses through the automated movement of data to the most
economical access tier, while ensuring that there is no adverse
influence on performance or operational burden. S3 Intelligent-
Tiering is the only cloud storage class that automatically reduces

costs when access patterns change by transferring data
between access tiers on a granular object level.
Storage classes for infrequently accessed objects
  S3 Standard-IA: S3 Standard-IA is designed for infrequently
accessed data, such as backups, archives, and media files. S3
Standard-IA offers lower storage costs than S3 Standard, but it
may take longer to access data stored in S3 Standard-IA.
  S3 One Zone-IA: S3 One Zone-IA is a lower-cost alternative to
S3 Standard-IA for infrequently accessed data. S3 One Zone-IA
stores data in a single Availability Zone, which makes it more
cost-effective, but it also makes it less resilient to failures in that
Availability Zone.
Storage classes for archiving objects
  S3 Glacier Instant Retrieval: S3 Glacier Instant Retrieval is
designed for archive data that needs to be accessed
quickly, such as medical images and financial records. S3
Glacier Instant Retrieval offers high availability and
performance, but it is more expensive than other S3 storage
classes.
  S3 Glacier Flexible Retrieval: S3 Glacier Flexible Retrieval is
designed for archive data that does not need to be accessed
immediately, such as historical data and compliance records. S3
Glacier Flexible Retrieval offers lower storage costs than S3

Glacier Instant Retrieval, but it may take longer to access data
stored in S3 Glacier Flexible Retrieval.
  S3 Glacier Deep Archive: S3 Glacier Deep Archive is
designed for long-term archive data, such as digital preservation
and disaster recovery. S3 Glacier Deep Archive offers the lowest
storage costs of all S3 storage classes, but it may take several
hours to access data stored in S3 Glacier Deep Archive.

You can choose the S3 storage class that is best suited for
your needs based on the frequency of access to the data and
the required durability and availability.
Comparison between different S3
Storage type:
2.2.2 Data Security using Access Control Lists and Bucket
Policies
1) Access Control Lists (ACLs)
Using Access Control Lists (ACLs) in Amazon S3 can provide
an additional layer of data security at the object level within a
bucket. However, it's important to understand that ACLs are not
the primary recommended method for managing access control

in S3. AWS suggests using S3 bucket policies or IAM policies for
broader and more granular control.
Here's how you can achieve data security using ACLs in S3:


1. Grant granular permissions: ACLs allow you to grant
specific permissions (read, write, delete, full control) to
individual AWS accounts, predefined groups, or the public. This
can be helpful for cases where you need to share access to
specific objects with different levels of access.
2. Limit public access: By default, S3 buckets are private,
meaning only the bucket owner has access. However, you can
accidentally make objects publicly accessible through ACLs. To
prevent this, ensure you avoid using the "public-read" or "public-
read-write" canned ACLs. Additionally, consider using S3 Block
Public Access to prevent public access altogether.

3. Monitor ACL changes: ACLs can be modified by anyone
with write access to the object. To maintain control and detect
potential security issues, enable CloudTrail logging for S3 and
monitor for ACL changes.
4. Use ACLs cautiously: While ACLs can be useful in specific
scenarios, they have limitations. They can become complex to
manage for a large number of objects, and they lack features
like centralized control and condition-based access. For broader
and more secure access control, consider using S3 bucket
policies or IAM policies.
2) S3 Bucket Policy
An S3 bucket policy is a JSON document that defines the
permissions that users and applications have to access your S3
bucket and its objects. Bucket policies can be used to grant or
deny access to specific users or groups, to specific objects or
prefixes, and to specific operations (such as reading, writing, or
deleting objects).
Here is an example of a simple bucket policy:
S3 Lifecycle Policy
JSON
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": "*",
"Action": "s3:GetObject",

"Resource": "arn:aws:s3:::my-bucket/*"
}
]
This policy allows any user to read any object in the my-
bucket bucket.
Here is an example of a more complex bucket policy:
JSON
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": "arn:aws:iam::123456789012:user/my-user",
"Action": "s3:PutObject",
"Resource": "arn:aws:s3:::my-bucket/private/*"
},
{
"Effect": "Deny",
"Principal": "*",
"Action": "s3:GetObject",
"Resource": "arn:aws:s3:::my-bucket/private/*"
}
]
}
An S3 bucket policy is a JSON document that defines the
permissions that users and applications have to access your S3
bucket and its objects. Bucket policies can be used to grant or
deny access to specific users or groups, to specific objects or
prefixes, and to specific operations (such as reading, writing, or
deleting objects).

Here is an example of a simple bucket policy:
JSON
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": "*",
"Action": "s3:GetObject",
"Resource": "arn:aws:s3:::my-bucket/*"
}
]
}
This policy allows any user to read any object in the my-
bucket bucket.
Here is an example of a more complex bucket policy:
JSON
{
"Version": "2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": "arn:aws:iam::123456789012:user/my-user",
"Action": "s3:PutObject",
"Resource": "arn:aws:s3:::my-bucket/private/*"
},
{
"Effect": "Deny",
"Principal": "*",
"Action": "s3:GetObject",

"Resource": "arn:aws:s3:::my-bucket/private/*"
}
]
}
This policy allows the user my-user to upload objects to the
my-bucket/private prefix, but it denies all users from reading
objects from that prefix.
You can create and manage bucket policies using the AWS
Management Console, the AWS CLI, or the AWS SDKs.
How bucket policies work:
The below diagram shows how to use an IAM role and an S3
bucket policy to allow users to access an S3 bucket. This is a
common way to grant access to S3 buckets in a secure and
controlled manner.
As you can see in the diagram, bucket policies are applied
before IAM policies. This means that even if a user has
permissions granted by an IAM policy, they will still be denied
access if the bucket policy denies them access.

Here are some reasons why you might want to grant public
access to your S3 bucket:
  To host a static website.
  To share files with the public.
  To use S3 as a content delivery network (CDN).
It is important to note that granting public access to your S3
bucket can be a security risk. Anyone on the internet will be able
to access your bucket and its objects, even if they do not have
an AWS account. You should only grant public access to your
bucket if you are sure that you want to do so.
Here are some tips for granting public access to your S3
bucket securely:
  Use bucket policies to restrict access to specific objects or
prefixes.
  Use encryption to protect your data.
  Use versioning to recover from accidental deletions or
overwrites.
  Monitor your bucket activity for suspicious activity
2.2.3 Amazon S3 -Versioning
Amazon S3 Versioning is a feature that allows you to keep
multiple versions of an object in the same bucket. This can be
useful for a variety of reasons, such as:

  To recover from accidental deletions or overwrites.
  To track changes to objects over time.
  To create and maintain backups of objects.
When you enable versioning for a bucket, S3 will
automatically create a new version of an object every time you
modify it. Each version of an object has a unique version ID. You
can access any version of an object by specifying its version ID.
S3 Versioning is a very powerful feature, but it is important to
understand how it works and how to use it properly. Here are
some important things to keep in mind:
  S3 Versioning does not protect your data from unauthorized
access. You still need to use bucket policies to control who has
access to your buckets and objects.
  S3 Versioning can increase your storage costs. Each version of
an object counts as a separate object, so storing multiple
versions of an object can increase your storage usage.

  S3 Versioning can be used to recover from accidental deletions
or overwrites, but it is not a backup solution. You should still
have a separate backup solution in place in case of a major
disaster.
How to enable S3 Versioning:
You can enable S3 Versioning for a bucket using the AWS
Management Console, the AWS CLI, or the AWS SDKs.
To enable S3 Versioning using the AWS Management Console:
1.  Go to the Amazon S3 console and select the bucket for which
you want to enable versioning.
2.  Click the Versioning tab.
3.  Click the Enable button.
4.  Click the Save button.
How to disable S3 Versioning:
You can disable S3 Versioning for a bucket using the AWS
Management Console, the AWS CLI, or the AWS SDKs.
To disable S3 Versioning using the AWS Management Console:
1.  Go to the Amazon S3 console and select the bucket for which
you want to disable versioning.
2.  Click the Versioning tab.

3.  Click the Disable button.
4.  Click the Save button.
How to retrieve a specific version of an object:
To retrieve a specific version of an object, you can use the
AWS Management Console, the AWS CLI, or the AWS SDKs.
To retrieve a specific version of an object using the AWS
Management Console:
1.  Go to the Amazon S3 console and select the bucket that
contains the object.
2.  Click the Versions tab.
3.  Select the version of the object that you want to retrieve.
4.  Click the Download button.
How to restore a previous version of an object:
To restore a previous version of an object, you can use the
AWS Management Console, the AWS CLI, or the AWS SDKs.
To restore a previous version of an object using the AWS
Management Console:
1.  Go to the Amazon S3 console and select the bucket that
contains the object.
2.  Click the Versions tab.

3.  Select the version of the object that you want to restore.
4.  Click the Restore button.
2.2.4 S3 Lifecycle Policy
An S3 Lifecycle Policy is a set of rules that define actions that
Amazon S3 applies to a group of objects. These actions can be:
  Transitioning objects to a different storage class
  Archiving objects
  Deleting objects
S3 Lifecycle Policies can help you manage your storage costs
and ensure that your objects are stored in the most appropriate
storage class for their lifecycle.
S3 lifecycle policies are a powerful tool that can help you to
manage your S3 objects more cost-effectively and efficiently. By
automating the management of your S3 objects, you can save
time and money, and improve the security and compliance of
your data.
Here is a diagram of how an S3 Lifecycle Policy works:

The diagram shows the following steps:
1.  Objects are uploaded to S3.
2.  The S3 Lifecycle Policy is applied to the objects.
3.  Objects that match the criteria of a rule in the lifecycle policy
are transitioned to the specified storage class, archived, or
deleted.
Examples of S3 Lifecycle Policy:
  Example 1: Transition objects to S3 Standard-IA after 90 days
and delete them after 1 year.
  Example 2: Archive objects to S3 Glacier after 1 year.

  Example 3: Delete objects after 30 days if they have not been
accessed.
How to create an S3 Lifecycle Policy:
You can create an S3 lifecycle policy using the AWS
Management Console, AWS CLI, or AWS SDKs.
To create an S3 lifecycle policy using the AWS Management
Console:
1.  Go to the Amazon S3 console and select the bucket for which
you want to create the lifecycle policy.
2.  Click the Management tab.
3.  Click Create lifecycle rule.
4.  Enter a name for the rule and specify the scope of the rule.
5.  Click Add condition.
6.  Select the condition that you want to use to filter the objects
that the lifecycle rule will apply to.
7.  Click Add action.
8.  Select the action that you want Amazon S3 to take on the
objects that match the condition.
9.  Click Save.

Refer some sample lifecycle policies for S3, covering both all
objects in a bucket and individual objects:
1. Lifecycle Policy for All Objects in a Bucket:
JSON
{
"Rules": [
{
"ID": "Transition to Standard-IA after 30 days",
"Prefix": "",  // Applies to all objects because of empty prefix
"Status": "Enabled",
"Transition": {
"Days": 30,
"StorageClass": "STANDARD_IA"
}
},
{
"ID": "Expire objects after 1 year",
"Prefix": "",  // Applies to all objects
"Status": "Enabled",
"Expiration": {
"Days": 365
}
}
]
}
Explanation:
  This policy has two rules:

o  The first rule transitions all objects to the Standard-IA storage
class after 30 days, reducing storage costs for less frequently
accessed data.
o  The second rule expires all objects after 1 year, automatically
deleting them to save space and costs.
2. Lifecycle Policy for Individual Objects:
JSON
{
"Rules": [
{
"ID": "Transition images to Glacier after 90 days",
"Prefix": "images/",  // Applies only to objects with the "images/" prefix
"Status": "Enabled",
"Transition": {
"Days": 90,
"StorageClass": "GLACIER"
}
},
{
"ID": "Expire logs after 30 days",
"Prefix": "logs/",  // Applies only to objects with the "logs/" prefix
"Status": "Enabled",
"Expiration": {
"Days": 30
}
}
]
}

In this exercise, we will perform the following exercise:
1)  Bucket Creation
2)  Viewing the properties of an S3 Bucket
3)  Upload the object in bucket
4)  Emptying the bucket
5)  Delete bucket

Lab 1: Amazon S3 – Bucket Creation
We can use the Amazon S3 console, Amazon S3 APIs, AWS
CLI, or AWS SDKs to create a bucket.
Here we are considering only Amazon S3 console for bucket
creation.
A)  Bucket Creation using S3 console:
1.  Sign in to the AWS Management Console and open the
Amazon S3 console at https://console.aws.amazon.com/s3/.
2.  In the left navigation pane, choose Buckets.
3.  Choose Create bucket. The Create bucket page opens.

4.  For Bucket name, enter a name- “project-s3bucket-
demo24” for your bucket.
5.  For Region, choose the AWS Region- US-EAST-1 where you
want the bucket to reside.
6.  Under Object Ownership, to disable or enable ACLs and
control ownership of objects uploaded in your bucket, choose
one of the following settings:

ACLs disabled
  Bucket owner enforced (default) ACLs enabled
  Bucket owner preferred – The bucket owner owns and has
full control over new objects that other accounts write to the
bucket with the bucket-owner-full-control canned ACL.
7.  Under Block Public Access settings for this bucket,
choose the Block Public Access settings that you want to apply
to the bucket. By default, all four Block Public Access settings
are enabled.

8.  (Optional) Under Bucket Versioning, you can choose if you
wish to keep variants of objects in your bucket. To disable or
enable versioning on your bucket, choose
either Disable or Enable.
9.  (Optional) Under Tags, you can choose to add tags to your
bucket. Tags are key-value pairs used to categorize storage.
To add a bucket tag, enter a Key and optionally a Value and
choose Add Tag.
10.  Under Default encryption, choose Edit.
11.  To configure default encryption, under Encryption type,
choose one of the following:
  Amazon S3 managed key (SSE-S3)
  AWS Key Management Service key (SSE-KMS)
If you chose AWS Key Management Service key (SSE-KMS),
do the following:
12.  Under AWS KMS key, specify your KMS key

  To create a new customer managed key in the AWS KMS
console, choose Create a KMS key.
   
13.  (Optional) If you want to enable S3 Object Lock, do the
following:
Choose Advanced settings.
If you want to enable Object Lock, choose Enable, read the
warning that appears, and acknowledge it.


14.  Choose Create bucket 
15.  Bucket name project-s3bucket-demo24 has been created
and it is available in
Amazon S3 bucket section.
Key Points:
Buckets naming rules
The following naming rules apply for general purpose
buckets.
  Bucket names must be between 3 (min) and 63 (max)
characters long.
  Bucket names can consist only of lowercase letters, numbers,
dots (.), and hyphens (-).

  Bucket names must begin and end with a letter or number.
  Bucket names must not contain two adjacent periods.
  Bucket names must not be formatted as an IP address (for
example, 192.168.5.4).
  Bucket names must be unique across all AWS accounts in all
the AWS Regions within a partition. A partition is a grouping of
Regions. AWS currently has three partitions: aws (Standard
Regions), aws-cn (China Regions), and aws-us-gov (AWS
GovCloud (US)).
  A bucket name cannot be used by another AWS account in the
same partition until the bucket is deleted.
B) Viewing the properties of an S3 Bucket
1.  Sign in to the AWS Management Console and open the
Amazon S3 console at https://console.aws.amazon.com/s3/.
2.  In the Buckets list, choose the name of the bucket –
“project-s3bucket-demo24” that you want to view the properties
for.
3.  Choose Properties.

You can find Bucket versioning and Tags option in above
diagram. Once you scroll down the page, you can find other
option also.
4.  On the Properties page, you can configure the following
properties for the bucket.
  Bucket Versioning 
  Tags 
  Default encryption 
  Server access logging 
  AWS CloudTrail data events 

  Event notifications 
  Transfer acceleration 
  Object Lock 
  Requester Pays 
  Static website hosting

C) Upload folders and files to an S3 bucket
When you upload a file to Amazon S3, it is stored as an
S3 object. Objects consist of the file data and metadata that
describes the object. You can have an unlimited number of
objects in a bucket. Before you can upload files to an Amazon S3
bucket, you need write permissions for the bucket. You can
upload any file type—images, backups, data, movies, and so on
—into an S3 bucket. The maximum size of a file that you can
upload by using the Amazon S3 console is 160 GB.
1.  Sign in to the AWS Management Console and open the
Amazon S3 console at https://console.aws.amazon.com/s3/.
2.  In the left navigation pane, choose Buckets.
3.  In the Buckets list, choose the name of the bucket- “project-
s3bucket-demo24” that you want to upload your folders or files
to.

4.  You can click on it and you will find the screen given below.
5.  Choose Upload. In the Upload window, do one of the
following:
  Drag and drop files and folders to the Upload window.
  Choose Add file or Add folder, choose the files or folders to
upload, and choose Open.

6.  We have selected the file name by “AWS Logo” available in
my desktop/Aws Beginners folder.

7.  Once we select the file. We can click upload button to upload
the logo in given bucket “project-s3bucket-demo24”.

8.  To enable versioning, under Destination, choose Enable
Bucket Versioning.
9.  To upload the listed files and folders without configuring
additional upload options, at the bottom of the page,
choose Upload.
Amazon S3 uploads your objects and folders. When the upload is
finished, you see a success message on the Upload:

status page.
D) Emptying a bucket
1.  Sign in to the AWS Management Console and open the
Amazon S3 console at https://console.aws.amazon.com/s3/.
2.  In the Bucket name list, select the option next to the name
of the bucket that you want to empty, and then choose Empty.

3.  On the Empty bucket page, confirm that you want to empty
the bucket by entering the bucket name into the text field, and
then choose Empty.
4.  Monitor the progress of the bucket emptying process on
the Empty bucket: Status page.
E) Delete an S3 bucket
1.  Sign in to the AWS Management Console and open the
Amazon S3 console at https://console.aws.amazon.com/s3/.
2.  In the Buckets list, select the option next to the name of the
bucket that you want to delete, and then choose Delete at the
top of the page.

3.  On the Delete bucket page, confirm that you want to delete
the bucket by entering the bucket name into the text field, and
then choose Delete bucket.
Important Point for deletion:
1.  Make sure that your given bucket is empty otherwise you will
get the above warning saying that to empty the bucket.
2.  Delete any access points that are attached to the bucket,
before deleting the bucket.
3.  A service control policy can deny the delete permission on a
bucket.

3.  AMAZON S3 – STATIC WEBSITE
HOSTING
Amazon S3 Static Website Hosting is a feature that allows you
to host a static website on Amazon S3. Static websites are
websites that do not contain server-side scripting or dynamic
content. Instead, static websites are made up of HTML, CSS,
and JavaScript files that are served directly to the user's
browser.
To host a static website on S3, you will need to:
1.  Create an S3 bucket.
2.  Upload your website files to the S3 bucket.
3.  Create your index.html and error.html file and place it in
your root directory.
4.  Configure the S3 bucket for static website hosting.
5.  Update your DNS records to point to the S3 bucket.
Here are some tips for hosting a static website on S3:
  Use a content delivery network (CDN) to serve your website
files from multiple locations around the world. This will help
to improve the performance of your website for global users.

  Use a custom domain name for your website. This will make
your website more professional and easier to remember.
  Use S3 versioning to protect your website files from
accidental overwrites and deletions.
  Monitor your website traffic and usage to ensure that your
website is meeting your needs.

Lab 2: Create a Static Website Hosting
using S3
1.  Sign in to the AWS Management Console and open the
Amazon S3 console at https://console.aws.amazon.com/s3/.
2.  Create two files index.html and error.html(optional) having
contents as given:
Index.html -> <html><h1>Welcome to Demo Static Website
on Amazon S3</h1></html>
Error.html -> <html><h1> Error Message Page generated on
Amazon S3</h1></html>
3.  In the Buckets list, choose the name of the bucket-
project-s3bucket-demo24 that you want to enable static
website hosting and uploads both the files name by
index.html and error.html one by one enabling “Grant public
read access” for both the files.
You are not able to access index and error pages without
giving them public access.

4.  Choose Properties.

5.  Scroll down to this page and go to Under Static website
hosting option & choose Edit.
6.  Under Static website hosting, choose Enable.
7.  In Index document, enter the name index.html.
8.  In Error document enter error.html(optional)

9.  (Optional) If you want to specify advanced redirection
rules, in Redirection rules, enter JSON to describe the rules.

10.  Choose Save changes.
Amazon S3 enables static website hosting for your bucket. At
the bottom of the page, under Static website hosting, you
see the website endpoint for your bucket.
11.  Under Static website hosting, note the Endpoint.
http://project-s3bucket-demo24.s3-website-us-east-
1.amazonaws.com 
The Endpoint is the Amazon S3 website endpoint for your
bucket. After you finish configuring your bucket as a static
website, you can use this endpoint to test your website.
12.  Use this endpoint to launch your static website:
http://project-s3bucket-demo24.s3-website-us-east-
1.amazonaws.com 


4.  AMAZON S3 REPLICATION
S3 Replication: It is a powerful feature in Amazon Simple
Storage Service (S3) that allows you to automatically
replicate objects from one S3 bucket to one or more
destination buckets. This can be incredibly useful for various
purposes, including:
  Data protection and disaster recovery: By replicating your
data to another region or Availability Zone (AZ), you can
ensure its availability even in case of regional outages or
disasters. This can significantly improve your data resiliency
and reduce downtime risks.
  Performance optimization: Replicating data to
geographically closer regions can reduce latency for users
accessing the data. This is especially beneficial for
applications used by geographically dispersed users.
  Compliance with regulations: Certain regulations might
require you to store data in specific regions or countries. S3
Replication can help you meet these requirements by
automatically replicating your data to the required locations.
  Data migration: You can use S3 Replication to migrate data
from one bucket to another, either within the same region or
across regions. This can be helpful for reorganizing your data
or moving it to a different storage class.

There are two main types of S3 Replication:
1. Cross-Region Replication (CRR): This replicates objects
to a bucket in a different AWS Region. This is ideal for disaster
recovery and compliance purposes.
2. Same-Region Replication (SRR): This replicates objects
to a bucket in a different Availability Zone within the same
region. This is best for improving performance and
redundancy within a region.

Lab 3: S3 Replication Setup
S3 Replication is a powerful tool for keeping your data
safe, accessible, and performant. Here's a step-by-step guide
with pictures to help you set it up:
1. Choose your source and destination buckets:
  Source bucket: This is the bucket where your original data
resides.
  Destination bucket: This is the bucket where you want your
data replicated. You can choose a bucket in the same region
for performance or a different region for disaster recovery.
2. Create a replication rule:
  Go to the S3 Management Console.
  Select your source bucket.
  Click on the Replication tab.
  Click on Create rule.
3. Configure the rule:
  Rule name: Give your rule a descriptive name.
  Source selection: Choose the objects you want to replicate.
You can use filters based on object keys, prefixes, or tags.

  Destination selection: Choose the destination bucket.
  Status: Enable the rule.
4. (Optional) Configure additional settings:
  Replication schedule: You can choose to replicate objects
immediately or on a specific schedule.
  Retention: You can set how long to keep replicated objects
in the destination bucket, even if they're deleted from the
source.
  Encryption: You can choose to encrypt replicated objects
using server-side encryption.
5. Monitor your replication:
  You can monitor the status of your replication rule in the S3
Management Console.
  You can also use CloudWatch to create alarms for
replication failures.
Here are some additional tips for setting up S3 Replication:
  Test your replication rule: Before relying on S3 Replication
for critical data, test it with a small set of objects to make

sure it's working correctly.
  Consider using versioning: If you enable versioning in your
source bucket, replicated objects will also be versioned in the
destination bucket.
  Use IAM roles for access control: Use IAM roles to grant the
S3 service the necessary permissions to replicate your data.
By following these steps, you can easily set up S3
Replication and keep your data safe, accessible, and
performant.

5.  AMAZON S3 TRANSFER
ACCELERATION
Amazon S3 Transfer Acceleration is a powerful feature that can
significantly speed up your data transfers to and from Amazon
S3 buckets, especially for long distances. Here's how it works:
Problem: When transferring data over long distances, you
often encounter issues like:
  High latency: Distance increases network latency, impacting
upload and download speeds.
  Varying internet speeds: Unpredictable internet routing and
congestion can further slowdown transfers.
  Distance to S3: Applications far from the S3 bucket's region
experience slower transfers.
Solution: S3 Transfer Acceleration tackles these challenges
by:
  Routing through CloudFront: Your data is routed through
strategically placed Amazon CloudFront edge locations closer to
your users or applications. This "logically shortens" the distance
to S3, reducing latency.
  Optimized network paths: S3 Transfer Acceleration leverages
AWS's high-performance backbone network for faster data
transfers.

  Network protocol optimizations: S3 Transfer Acceleration
utilizes advanced protocols like UDP for faster data bursts and
error correction mechanisms for reliable transfers.
  S3 Transfer Acceleration is a cost-effective solution for
accelerating long-distance data transfers to and from S3.
  It can significantly improve transfer speeds, reduce
variability, and boost application performance.
  Getting started is easy and you only pay for what you use.
You can compare upload still using below given link provided
by AWS.
https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-
speed-comparsion.html
you can obtain 163% performance improvement related to
USA Virginia region after enabling Amazon S3 Transfer
Acceleration.

Lab 4: enable Amazon S3 Transfer
Acceleration for given bucket
Step 1: One need to select source target and go to Properties
of source bucket. You will find multiple options in the Source
Backet properties.
Step 2: Scroll down the page and you can find Transfer
acceleration option. Click on Edit.

Step 3: Select Enable and Save the change. You can now find
the source endpoint:
project-s3bucket-demo24.s3-accelerate.amazonaws.com.
Step 4: Use the S3 Transfer Acceleration endpoint for
transfers: Replace the regular S3 endpoint with the S3 Transfer
Acceleration endpoint for your bucket.
Step 5: Test and monitor: Use the provided speed comparison
tool to assess the performance improvement and track transfer
costs.

6.  MULTI-CHOICE QUESTIONS
AND ANSWERS
1. What is the primary function of Amazon S3?
(a) To manage virtual machines and containers in the cloud.
(b) To provide a scalable object storage service for various
data types.
(c) To host and run web applications within AWS.
(d) To establish secure connections between on-premises
networks and the cloud.
Answer: (b)
Explanation: Amazon S3 offers a simple, scalable, and
cost-effective object storage solution for diverse data,
including images, videos, documents, backups, and more.
2. What is the most important factor influencing the cost
of storing data in S3?
(a) The file size and number of objects.
(b) The data transfer speed and throughput.
(c) The specific AWS region where the data is stored.
(d) The type of encryption used to secure the data.

Answer: (a)
Explanation: S3 charges based on the amount of data
stored and the number of requests made to access that data.
3. What are the different storage classes available in S3?
(a) Only a single standard storage class is offered.
(b) Standard, Standard-IA, and Glacier for varying access
frequencies and costs.
(c) S3 offers custom storage classes based on user needs.
(d) The storage class is automatically determined by the file
type.
Answer: (b)
Explanation: S3 provides different storage classes
(Standard, Standard-IA, and Glacier) to optimize cost based
on data access frequency and retrieval times.
4. What is the purpose of a bucket policy in S3?
(a) To organize and categorize objects within a bucket.
(b) To configure access permissions for users and groups to
specific objects or the entire bucket.
(c) To encrypt data at rest and in transit within S3.

(d) To set lifecycle rules for automatically managing data
based on age or access patterns.
Answer: (b)
Explanation: Bucket policies control who can access
specific objects or actions within a bucket, offering granular
security control.
5. What is Amazon CloudFront and how does it relate to
S3?
(a) An independent service unrelated to S3.
(b) A content delivery network (CDN) that can accelerate
delivery of static content stored in S3.
(c) A tool for migrating data between different S3 buckets.
(d) A security service for encrypting data before storing it in
S3.
Answer: (b)
Explanation: CloudFront acts as a CDN, caching static
content from S3 in globally distributed edge locations,
reducing latency and improving website performance.
6. What are the benefits of using Amazon Route 53 with
S3?

(a) No specific benefits; they serve entirely different
purposes.
(b) Route 53 can directly host static websites from S3
buckets.
(c) Route 53 can create aliases and CNAME records to point
domains to S3 buckets for static content.
(d) Route 53 integrates with S3 for disaster recovery and high
availability.
Answer: (c)
Explanation: Route 53 allows mapping custom domain
names to S3 buckets, making static content accessible
through user-friendly URLs.
7. What is Amazon S3 Transfer Acceleration and its
purpose?
(a) A service for migrating large datasets into S3 buckets
efficiently.
(b) A feature optimizing data transfers to and from S3 for
specific user groups.
(c) A security solution for adding additional encryption during
data transfers.

(d) A management tool for setting lifecycle rules for S3
objects.
Answer: (a)
Explanation: S3 Transfer Acceleration utilizes multiple
network paths and optimizes connections to accelerate large
data transfers to and from S3 buckets.
8. What is Amazon Glacier and its use case compared to
other S3 storage classes?
(a) An alternative storage class with faster access times than
standard storage.
(b) An archival storage class for infrequently accessed data
with lower retrieval costs. (c) A temporary storage class for
data before permanent archiving in Glacier.
(d) A secure storage class offering additional encryption
features.
Answer: (b)
Explanation: Glacier is a low-cost storage option for rarely
accessed data, requiring retrieval requests in advance and
incurring retrieval fees.

CHAPTER 7  AMAZON VIRTUAL
PRIVATE CLOUD (VPC)

1.  OVERVIEW OF AMAZON
VIRTUAL PRIVATE CLOUD
An Amazon VPC is a type of virtual network that can be
created within AWS. It is similar to having a private network
that is isolated from other networks within AWS. A logically
isolated section of AWS can be provisioned, which is
equivalent to establishing and operating a distinct, self-
contained network within an on-premises data center.
VPCs span all of the Availability Zones in a region, and you
have full control over who has access to your VPC resources.
By default, you can create up to 5 VPCs per region. Each
region is assigned a default VPC, which includes a subnet in
every AZ.
You must specify a range of IPv4 addresses for the VPC in
the form of a Classless Inter-Domain Routing (CIDR) block

when creating the VPC; for instance, 10.0.0.0/16.
You can control everything about your VPC, including the
IP address range, subnets, and how traffic is routed. You can
also launch your AWS resources, such as EC2 instances, into
your VPC. We will discuss Subnets and Routing in subsequent
section. In essence, VPC is logical data center in Amazon Web
Services.

2.  CIDR BLOCK – CLASSIC INTER-
DOMAIN ROUTING BLOCK
A CIDR block, or Classless Inter-Domain Routing block, is a
range of IP addresses that can be assigned to devices on a
network. VPCs use CIDR blocks to define the range of IP
addresses that can be assigned to resources in the VPC.
CIDR blocks are expressed in a format like this:
10.0.0.0/16
This CIDR block represents the range of IP addresses from
10.0.0.0 to 10.255.255.255. The /16 at the end of the CIDR
block represents the number of bits in the network prefix. In
this case, the network prefix is 10.0.0.0, so the CIDR block
represents all IP addresses that start with 10.0.0.0.
When you create a VPC, you must specify a CIDR block for
the VPC. The CIDR block must be large enough to
accommodate all of the resources that you plan to launch in
the VPC. For example, if you plan to launch 100 EC2
instances in your VPC, you will need to choose a CIDR block
that can accommodate at least 100 IP addresses.
Here is an example of how to define a CIDR block for a
VPC:
# Create a VPC with a CIDR block of 10.0.0.0/16
aws ec2 create-vpc—cidr-block 10.0.0.0/16
This will create a VPC with the CIDR block 10.0.0.0/16. You
can then launch EC2 instances and other resources into the

VPC, and they will be assigned IP addresses from the CIDR
block.
It is important to note that VPC CIDR blocks must be
unique within your AWS account. You cannot create two VPCs
with the same CIDR block.
Available IP Address Calculation:
As per number of bits in the network prefix like /16, we
can identify the number of
IP address available for given subnet with this formula
=2 power 32 – 2 power n = 2 ^ 32 - 2 ^ n so, for
/16 = (2^32) – (2 ^16) = 2^16 = 65536 available address
large
/17 = 2^17= 131072
/20 = 2^20 = 4096
/21 = 2 ^21= 2048
/24 = 2^24 = 256
/28 =16 available address small
Hence, we can consider /16 to /28 range because after /28
not sufficient IP addresses are available.
In AWS VPC first four IP addresses and last IP address are
not available for use and cannot be assigned to instances. In
the range of /10 these addresses are:
  10.0.0.0: Network address

  10.0.0.1: Reserved by AWS for the VPC router
  10.0.0.2: Reserved by AWS
  10.0.0.3: Reserved for broadcast
  10.255.255.255: Broadcast address
Hence valid number of IP address available to assigned in
/16 = (2^32) – (2 ^16) = 2^15 = 65536-5= 65531
Hence, we need to design our subnet IP Address as per
needed
IP address so this needs careful attention and
identification of correct CIDR block accordingly.

3.  AMAZON VIRTUAL PRIVATE
CLOUD (AMAZON VPC)
COMPONENTS
Amazon Virtual Private Cloud (Amazon VPC) components are the
building blocks that you use to create and manage your VPC.
The below diagram is showing VPC without any resources:
The following are some of the most common components:
  Subnets: Subnets are logically isolated partitions of your
VPC. You can use subnets to group resources together based on
function or security requirements. For example, you might
create a subnet for your web servers and a subnet for your
database servers.
  Route tables: Route tables control how traffic flows within
your VPC and to the internet. You can associate one or more
route tables with each subnet.

  Security groups: Security groups define the network traffic
that is allowed to and from your instances. You can associate
one or more security groups with each instance.
  Network access control lists (ACLs): ACLs filter traffic at
the subnet level. You can associate one or more ACLs with each
subnet.
  Internet gateways: Internet gateways allow your instances
to access the internet. You can attach one or more internet
gateways to your VPC.
  Elastic IP (EIP) addresses: EIP addresses are static public IP
addresses that you can assign to your instances. You can use EIP
addresses to access your instances from the internet.
  Elastic network interfaces (ENIs): ENIs allow you to attach
multiple IP addresses to a single instance. You can use ENIs to
isolate your instances' traffic or to provide redundancy.
   
In addition to the components listed above, there are a number
of other VPC components that you can use to manage your
network.
  Endpoints: Endpoints allow your instances to access AWS
services without going through the internet. You can create

endpoints for services such as Amazon S3 and Amazon
DynamoDB.
  Peering connections: Peering connections allow you to
connect two VPCs together. You can use peering connections to
share resources between VPCs or to create a private network
across multiple regions.
  NAT instances and NAT gateways: NAT instances and NAT
gateways allow your instances to access the internet without
publicly exposing their IP addresses. You can use NAT instances
and NAT gateways to improve security and performance.
  Virtual private gateways (VPGs): VPGs allow you to
connect your VPC to an on-premises network using a VPN. You
can use VPGs to create a hybrid cloud environment.

We will describe some of the important components known as
a building stone for VPN in more details for better
understandings:

3.1  SUBNET
A subnet is a segment of an Amazon VPC’s IP address range
where you can launch Amazon EC2 instances, Amazon Relational
Database Service (Amazon RDS) databases, and other AWS
resources. As an illustration, CIDR blocks delineate subnets
(10.0.1.0/24 and 192.168.0.0/24).
❖  You can create one or more subnets in each VPC for one
Availability zone.
❖  one subnet equals one Availability Zone. You can, however,
have multiple subnets in one Availability Zone.
❖  The smallest subnet that you can create is a /28 (16 IP
addresses). AWS reserves the first four IP addresses and the last
IP address of every subnet for internal networking purposes. For
example, a subnet defined as a /28 has 16 available IP
addresses; subtract the 5 IPs needed by AWS to yield 11 IP
addresses for your use within the subnet.

Classification of Subnet:
Subnets can be classified as public, private, or VPN-only.
  A public subnet is one in which the associated route table
directs the subnet’s traffic to the Amazon VPC’s IGW.
  A private subnet is one in which the associated route table
does not direct the subnet’s traffic to the Amazon VPC’s IGW.
  A VPN-only subnet is one in which the associated route table
directs the subnet’s traffic to the Amazon VPC’s VPG and does
not have a route to the IGW.
  Regardless of the type of subnet, the internal IP address range
of the subnet is always private (that is, non-routable on the
Internet).
  Default Amazon VPCs contain one public subnet in every
Availability Zone within the region, with a net mask of /20.

3.2 ROUTE TABLE
A route table is a logical construct within an Amazon VPC that
contains a set of rules (called routes) that are applied to the
subnet and used to determine where network traffic is
directed.
  A route table’s route is what permit Amazon EC2 instances
within different subnets within an Amazon VPC to
communicate with each other. You can modify route tables
and add your own custom routes.
  You can also use route tables to specify which subnets are
public (by directing Internet traffic to the IGW) and which
subnets are private (by not having a route that directs traffic
to the IGW).
  Your VPC has an implicit router.
  Your VPC automatically comes with a main route table that
you can modify.
  You can create additional custom route tables for your VPC.
  Each subnet must be associated with a route table, which
controls the routing for the subnet. If you don’t explicitly
associate a subnet with a particular route table, the subnet
uses the main route table.

  You can replace the main route table with a custom table
that you’ve created so that each new subnet is automatically
associated with it.
  Each route in a table specifies a destination CIDR and a
target; for example, traffic destined for 172.16.0.0/ 12 is
targeted for the VPG. AWS uses the most specific route that
matches the traffic to determine how to route the traffic.
  The following diagram shows the routing for a VPC with an
internet gateway, a virtual private gateway, a public subnet,
and a VPN-only subnet.

3.3  SECURITY GROUPS
❍  A security group is a virtual stateful firewall that controls
inbound and outbound network traffic to AWS resources and
Amazon EC2 instances.
❍  All Amazon EC2 instances must be launched into a
security group. If a security group is not specified at launch,
then the instance will be launched into the default security
group for the Amazon VPC.
❍  The default security group allows communication between
all resources within the security group, allows all outbound
traffic, and denies all other traffic. You may change the rules
for the default security group, but you may not delete the
default security group.
❍  You can create up to 500 security groups in Amazon VPC.
❍  You can specify allow rules, but not deny rules. This is an
important difference between security groups and ACLs.
❍  By default, no inbound traffic is allowed until you add
inbound rules to the security group.
❍  Security groups are stateful. This means that responses to
allowed inbound traffic are allowed to flow outbound
regardless of outbound rules and vice versa. This is an

important difference between security groups and network
ACLs.

3.4  NETWORK ACCESS CONTROL
LISTS (ACLS)
❍  A network access control list (ACL) is another layer of
security that acts as a stateless firewall on a subnet level.
❍  A network ACL is a numbered list of rules that AWS
evaluates in order, starting with the lowest numbered rule, to
determine whether traffic is allowed in or out of any subnet
associated with the network ACL.
❍  Amazon VPCs are created with a modifiable default
network ACL associated with every subnet that allows all
inbound and outbound traffic.
❍  When you create a custom network ACL, its initial
configuration will deny all inbound and outbound traffic until
you create rules that allow otherwise.
Internet gateways
An internet gateway (IGW) is a highly available device that
enables communication between your VPC and the internet.
It allows resources in your VPC to initiate and receive
connections from the internet.
IGWs are horizontally scaled, redundant, and highly
available. They are also stateless, meaning that they do not

keep track of the state of connections. This makes them very
reliable and scalable.
You can attach one or more IGWs to your VPC. However,
you can only have one default IGW. The default IGW is used
for all outbound traffic to the internet that does not match a
rule in a route table.
IGWs are typically used for the following purposes:
  Allowing resources in your VPC to access the internet
  Allowing resources on the internet to access resources in
your VPC
  Connecting your VPC to on-premises networks using a VPN
To use an IGW, you must first create it and then attach it
to your VPC. You can then create route table entries that
direct traffic to the IGW.
Here is an example of how to use an IGW to allow
resources in your VPC to access the internet:
1.  Create an IGW.
2.  Attach the IGW to your VPC.
3.  Create a route table entry that directs all outbound traffic
to the IGW.

4.  Associate the route table with the subnet that contains the
resources that you want to access the internet.
Once you have completed these steps, resources in the
subnet will be able to access the internet.
IGWs are a critical component of Amazon VPC. They allow
you to connect your VPC to the internet and to other
networks. By understanding how IGWs work, you can design
and manage your VPC network more effectively.
3.5 Virtual private gateways
A virtual private gateway (VGW) is a network appliance
that allows you to create a VPN connection between your VPC
and your on-premises network. VGPs are highly available and
scalable, and they can be used to connect multiple VPCs to
the same on-premises network.
To create a VPN connection using a VGW, you must first
create a customer gateway (CGW) on your on-premises
network. A CGW is a physical or virtual device that provides a
secure connection to AWS. Once you have created a CGW,
you can create a VPN connection between the VGW and the
CGW.
Once you have created a VPN connection, resources in
your VPC will be able to communicate with resources on your
on-premises network as if they were on the same network.
This allows you to create a hybrid cloud environment, where

you can run applications and store data in both AWS and your
on-premises network.
VGWs are typically used for the following purposes:
  Connecting a VPC to an on-premises network using a VPN
  Creating a hybrid cloud environment
  Extending your on-premises network to AWS
  Connecting multiple VPCs to the same on-premises network
VGWs are a powerful tool that can help you to connect
your VPC to your on-premises network and to create a hybrid
cloud environment.
If you need to connect your VPC to your on-premises
network, or if you want to create a hybrid cloud environment,
then you should consider using VGWs.

3.5  NAT INSTANCE
A NAT instance is a virtual machine that provides network
address translation (NAT) for resources in a private subnet.
NAT allows resources in a private subnet to communicate
with the internet and other VPCs without publicly exposing
their IP addresses.
NAT instances are typically used for the following
purposes:
  Allowing resources in a private subnet to access the
internet
  Allowing resources in a private subnet to communicate with
resources in other VPCs
  Hiding the IP addresses of resources in a private subnet
from the internet
To use a NAT instance, you must first create it and then
associate it with a public subnet. You can then create route
table entries that direct traffic to the NAT instance.
Here is an example of how to use a NAT instance to allow
resources in a private subnet to access the internet:
1.  Create a NAT instance.
2.  Associate the NAT instance with a public subnet.

3.  Create a route table entry that directs all outbound traffic
to the NAT instance.
4.  Associate the route table with the subnet that contains the
resources that you want to access the internet.
Once you have completed these steps, resources in the
subnet will be able to access the internet.

3.6  NAT GATEWAY
A NAT gateway is a managed network address translation
(NAT) service that allows instances in a private subnet to
connect to the internet without exposing their private IP
addresses. NAT gateways are highly available and scalable,
and they can be used to implement a variety of network
designs.
NAT gateways work by translating the private IP addresses
of instances in a private subnet to a public IP address. This
allows the instances to communicate with the internet
without exposing their private IP addresses. NAT gateways
also support dynamic SNAT port functionality, which
automatically scales outbound connectivity and reduces the
risk of SNAT port exhaustion.
NAT gateways are typically used for the following
purposes:
  Allowing instances in a private subnet to access the
internet
  Implementing a three-tier architecture
  Creating a DMZ
  Connecting to AWS services that require a public IP
address, such as Amazon Simple Email Service (Amazon SES)

and Amazon Simple Notification Service (Amazon SNS)
To use a NAT gateway, you must first create it and then
associate it with a private subnet. You can then create route
table entries that direct traffic to the NAT gateway.
Here is an example of how to use a NAT gateway to allow
instances in a private subnet to access the internet:
1.  Create a NAT gateway.
2.  Associate the NAT gateway with a private subnet.
3.  Create a route table entry that directs all outbound traffic
to the NAT gateway.
4.  Associate the route table with the subnet that contains the
instances that you want to access the internet.
Once you have completed these steps, instances in the
subnet will be able to access the internet.
Why We need NAT instance and NAT Gateway?
❍  By default, any instance that you launch into a private
subnet in an Amazon VPC is not able to communicate with
the Internet through the IGW. This is problematic if the
instances within private subnets need direct access to the
Internet from the Amazon VPC in order to apply security
updates, download patches, or update application software.

❍  we recommend that you use a NAT gateway instead of a
NAT instance. The NAT gateway provides better availability
and higher bandwidth, and requires less administrative effort
than NAT instances.

4.  VPC WITH ALL COMPONENTS:
You can easily customize the network configuration for your
Amazon Virtual Private Cloud. For example, you can create a
public-facing subnet for your web servers that has access to the
Internet, and place your backend systems such as databases or
application servers in a private facing subnet with no Internet
access. You can leverage multiple layers of security, including
security groups and NACL-Network Access Control List, to help
control access to Amazon EC2 instances in each subnet.
When you start using Amazon VPC, you have a default VPC in
each AWS Region. A default VPC comes with a public subnet in
each Availability Zone, an internet gateway, and settings to
enable DNS resolution. Therefore, you can immediately start
launching Amazon EC2 instances into a default VPC. You can
also use services such as Elastic Load Balancing, Amazon RDS,
and Amazon EMR in your default VPC.
A default VPC is suitable for getting started quickly and for
launching public instances such as a blog or simple website. You
can modify the components of your default VPC as needed.
Below diagram shows the VPC having all feasible components
for better understanding purpose. In here Webserver is placed in
public subnet and having internet access. Meanwhile, database
server is placed in private subnet as per standard practice. You
also have two layers of security known by Security Group and
NACL.
Please remember security group is applicable at Instance
level while NACL is applicable at subnet level.

What can you do with VPC?
❍  Launches instances into a subnet of your choosing
❍  Assign custom IP address range in each subnet
❍  Configure route tables between subnet
❍  Create internet gateway and attach it to our VPC.
❍  Much better security control over your AWS resource
❍  Instance security Groups
❍  Subnet network access control list (ACLS)
Lab 1: Create a VPC with no
additional VPC resources using the
console.
Use the following procedure to create a VPC with no
additional VPC resources using the Amazon VPC console.

1.  Open the Amazon VPC console
at https://console.aws.amazon.com/vpc/.
2.  On the VPC dashboard, choose Create VPC.
3.  choose VPC only For Resources to create,
4.  (Optional) For Name tag, enter a name for your VPC.
5.  For IPv4 CIDR block, do one of the following: Select Manual
option and enter 10.0.0.0/24 CIDR Block
6.  (Optional) Choose a Tenancy option. you must
use Default tenancy.

7.  (Optional) To add a tag to your VPC, choose Add new
tag and enter a tag key and a tag value.
8.  Choose Create VPC.
9.  After you create a VPC, you can add subnets
This will create only VPC and no other additional resources.
Lab 2: Create a VPC with other VPC
resources using the console.
When selecting "Create a VPC with other additional VPC
resources" options, the resources created by default are as using
the AWS console:
  VPC: The virtual private cloud itself, acting as an isolated
network within AWS.
  Subnets: One public subnet in each Availability Zone (AZ) in
the region you choose. These subnets allow instances to have
public IP addresses and access the internet.
  Route tables: One main route table, with a route to the
internet gateway for the public subnets.
  Internet gateway: A gateway allowing communication between
the VPC and the internet.

Use the following procedure to create a VPC plus the
additional VPC resources that you need to run your application,
such as subnets, route tables, internet gateways, and NAT
gateways. For example, VPC configurations, see VPC examples.
To create a VPC, subnets, and other VPC resources using the
console
1.  Open the Amazon VPC console
at https://console.aws.amazon.com/vpc/.
2.  On the VPC dashboard, choose Create VPC.
3.  For Resources to create, choose VPC and more.
4.  Keep Name tag auto-generation selected to create Name
tags for the VPC
5.  For IPv4 CIDR block, enter an IPv4 address range for the
VPC.
6.  (Optional) To support IPv6 traffic, choose IPv6 CIDR block
7.  Choose a Tenancy option.; you must use Default tenancy.
8.  For Number of Availability Zones (AZs), we recommend
atleast two Az’s for production environment. To choose the AZs
for your subnets, expand Customize AZs. Otherwise, let AWS
choose them for you.

9.  To configure your subnets, choose values for Number of
public subnets and Number of private subnets.
10.  (Optional) If resources in a private subnet need access to
the public internet over IPv4, for NAT gateways, choose the
number of AZs in which to create NAT gateways
11.  (Optional) If resources in a private subnet need access to
the public internet over IPv6, for Egress only internet
gateway, choose Yes.
12.  (Optional) For DNS options, both options for domain name
resolution are enabled by default.
13.  (Optional) To add a tag to your VPC, expand Additional
tags, choose Add new tag, and enter a tag key and a tag
value.
14.  In the Preview pane, you can visualize the relationships
between the VPC resources that you've configured.
15.  When you are finished configuring your VPC, choose Create
VPC.

Preview pane is one of the important features in here, where
you can visualize the relationships between the VPC resources
that you've configured.
Here you can find nine different subnets, seven routing tables
and three different network connection based on your different
inputs mentioned in red box.

As of now, we have fully understood about VPC and different
components associated with it.
Let us create one fully functional VPC having the following
attributes.
›  Create VPC in us-east-1A Region of AWS.
›  VPC with a public subnet and a private subnet. The public
subnet has a CIDR block of 10.0.1.0/24 and the private subnet
has a CIDR block of 10.0.2.0/24.
›  The public subnet has a security group called "Public Subnet
Security Group" and an EC2 instance called "Public Instance".
The public instance has an IP address of 10.0.1.2/24.
›  The private subnet has a security group called "Private Subnet
Security Group" and an EC2 instance called "Private Instance".
The private instance has an IP address of 10.0.2.2/24.
›  The VPC has an internet gateway (IGW) and a virtual private
gateway (VGW). The IGW is attached to the public subnet and
the VGW is attached to the private subnet.

:
Public subnet: 10.1.0.2/24 Security group: Web Server Security
Group EC2 Instance: Web Server IP Address: 10.1.0.2
Private subnet: 10.1.1.0/24 Security group: Database Server
Security Group EC2 Instance: Database Server IP Address:
10.1.1.2
As mentioned, Public and Private subnet are classical
example of how the VPC could be used:
  The public instance could be a web server that hosts a
website.
  The private instance could be a database server that stores
the website's data.
The web server would be able to communicate with the
database server over the private subnet. The web server would
also be able to communicate with clients on the internet over
the IGW.
Internet gateway: Yes

Virtual private gateway: Yes
The public subnet has a route to the internet gateway, which
means that the web server can communicate with the internet.
The private subnet does not have a route to the internet
gateway, but it does have a route to the NAT gateway. This
means that the database server can access the internet through
the NAT gateway, but its IP address is not exposed to the
internet.
The web server security group allows inbound traffic on port
80, so the web server can be accessed from the internet. The
database server security group only allows inbound traffic from
the web server security group, so the database server can only
be accessed by the web server.
The VPC also has a virtual private gateway, which allows you
to connect your VPC to an on-premises network using a VPN.

5.  VPC ENDPOINTS
VPC Endpoints allow you to connect your VPC to AWS services
without going through the internet. There are two types of
VPC Endpoints:
  VPC Endpoint Gateway: This type of endpoint is only
available for Amazon S3 and Amazon DynamoDB. It provides
a simple and easy-to-use way to connect your VPC to these
services.
  VPC Endpoint Interface: This type of endpoint can be used
to connect your VPC to any AWS service that supports VPC
Endpoints. It is more flexible than VPC Endpoint Gateway, but
it is also more complex to configure.
Which type of VPC Endpoint should you use?
If you are connecting your VPC to Amazon S3 or Amazon
DynamoDB, then you should use VPC Endpoint Gateway. It is
the simplest and easiest-to-use option.

If you are connecting your VPC to any other AWS service,
then you should use VPC Endpoint Interface. It is more
flexible, but it is also more complex to configure.
Here are some examples of when you might want to use
VPC Endpoints:
  To connect your web servers to Amazon S3 to store static
website content.
  To connect your application servers to Amazon DynamoDB
to store application data.
  To connect your data warehouse to Amazon Redshift to
store and analyze data.
  To connect your machine learning models to Amazon
SageMaker to train and deploy models.

6.  VPC CONNECTIVITY METHODS
  To connect your VPCs to each other, you can use Amazon
VPC Peering.
  To connect your VPCs to your own network, you can use
AWS Site-to-Site VPN.
  To connect your VPCs to each other and to your own
network you can use Amazon VPC Transit Gateways.

6.1 VPC TRANSITION PEERING
VPC transition peering is a networking feature that allows you
to connect your VPC to another VPC in a different AWS
Region. This allows you to share resources between the two
VPCs, such as EC2 instances, EBS volumes, and RDS
databases.
VPC transition peering is a one-way connection, which
means that traffic can flow from the source VPC to the
destination VPC, but not the other way around. This is
because VPC transition peering uses a Network Address
Translation (NAT) device to translate the IP addresses of the
source VPC to the IP addresses of the destination VPC.
To set up VPC transition peering, you must first create a
peering connection between the two VPCs. You can then
configure the NAT device to translate the IP addresses of the
source VPC to the IP addresses of the destination VPC.
Once you have set up VPC transition peering, you can
start routing traffic between the two VPCs. You can do this by
creating route table entries in the source VPC and destination
VPC. Also, as per the diagram, two VPC’s cannot perform
peering connect by using other intermediate VPC. They need
to connect one to one mode.

VPC transition peering can be used for a variety of
purposes, such as:
  To share resources between two VPCs in different AWS
Regions.
  To create a disaster recovery solution.
  To connect your VPC to an on-premises network.

6.2  VPC TRANSIT GATEWAYS
A transit gateway is a transit hub that you can use to
interconnect your VPCs and your on-premises networks. Transit
Gateway can help you simplify your network topology and avoid
complex peering relationships between large numbers of VPCs.
You can create a Site-to-Site VPN connection as an attachment
on a transit gateway.
Your Site-to-Site VPN connection on a transit gateway can
support either IPv4 traffic or IPv6 traffic inside the VPN tunnels. 
The following architecture diagram shows a simplified
representation of using Transit Gateway to connect your VPCs to
those of a third-party provider. Each VPC connects to the transit
gateway, and the gateway supports transitive routing between
all of the attached VPCs.


6.3 AWS SITE-TO-SITE VPN
AWS Site-to-Site VPN is a managed service that establishes
secure connections between your on-premises networks and
your Amazon Virtual Private Clouds (VPCs) or AWS Transit
Gateways over the internet. It uses IPsec tunnels for secure data
transmission.
Key components:
1.  Customer Gateway Device: A physical or software-based VPN
device on your on-premises network that establishes and
maintains the VPN connection.
2.  Virtual Private Gateway (VGW) or Transit Gateway: The AWS-
managed VPN endpoint on the Amazon side of the connection.
3.  VPN Connection: Represents the secure connection between
your customer gateway device and the VGW or Transit Gateway.
4.  VPN Tunnels: Each VPN connection consists of two redundant
tunnels for high availability.
How it works:
1.  Traffic from your on-premises network is encrypted and sent
through the VPN tunnels to the VGW or Transit Gateway.
2.  The VGW or Transit Gateway decrypts the traffic and routes it
to the appropriate VPC or resources within AWS.

3.  Traffic from AWS to your on-premises network flows in the
reverse direction.

7.  MULTI-CHOICE QUESTIONS
AND ANSWER
1. What is the primary purpose of an Amazon VPC?
(a) To allow direct internet access to all resources within your
AWS account.
(b) To provide a logically isolated virtual network within the
AWS Cloud for your resources.
(c) To create a public cloud environment for hosting static
websites.
(d) To manage security groups and network access control
lists across your entire AWS infrastructure.
Answer: (b)
Explanation: An Amazon VPC allows you to create a
private network within the AWS Cloud, separate from the
public internet, giving you more control over security and
network configuration for your resources.
2. What are the two main components of a Subnet within a
VPC?
(a) CIDR block and Availability Zone
(b) Security group and route table

(c) NAT gateway and internet gateway
(d) Instance type and storage size
Answer: (a)
Explanation: A Subnet is defined by a CIDR block (network
address range) and belongs to a specific Availability Zone
within your VPC.
3. What is the function of a Route Table in a VPC?
(a) To define inbound and outbound security rules for network
traffic.
(b) To assign specific IP addresses to individual instances
within the VPC.
(c) To determine the path that network traffic takes to reach
its destination.
(d) To monitor network traffic flow and identify potential
security issues.
Answer: (c)
Explanation: A Route Table dictates how network traffic
within a Subnet is routed, specifying how to reach resources
inside the VPC, on-premises networks, or the internet.
4. How does a Security Group control traffic to and from an
instance in a VPC?

(a) By directly filtering specific IP addresses and ports.
(b) By defining ingress and egress rules based on protocols,
ports, and IP addresses.
(c) By automatically blocking all incoming traffic unless
explicitly allowed.
(d) By restricting access based on the instance type and
operating system.
Answer: (b)
Explanation: Security Groups act as firewalls, defining
rules that specify which incoming and outgoing traffic is
allowed for each instance based on protocols, ports, and IP
addresses.
5. What is the difference between a NAT Gateway and a
NAT Instance in a VPC?
(a) There is no difference; they both serve the same purpose.
(b) NAT Gateway is managed by AWS and offers higher
availability, while NAT Instance requires manual
configuration.
(c) NAT Gateway allows access to the internet only, while NAT
Instance can access both internet and other VPCs.

(d) NAT Gateway is less expensive, while NAT Instance offers
more granular control over outbound traffic.
Answer: (b)
Explanation: Both NAT Gateway and NAT Instance provide
internet access for resources within a private subnet.
However, NAT Gateway is a fully managed service, offering
higher availability and scalability, while NAT Instance requires
manual configuration and maintenance.

CHAPTER 8  AMAZON ROUTE53

1.  AMAZON ROUTE 53
Amazon Route 53 is a highly available and scalable cloud
Domain Name System (DNS) web service. It is designed to
route end users to internet applications running on AWS or
on-premises. Route 53 provides a robust and reliable way to
route traffic to your websites and web applications, and it can
be used to create and manage DNS records for any domain
name, regardless of where it is registered.
You can use Route 53 to perform three main functions in
any combination:
›  Domain registration
›  DNS routing  
›  Health checking
If you choose to use Route 53 for all three functions, be
sure to follow the order below:
1. Register domain names

Your website needs a name, such as example.com. Route 53
lets you register a name for your website or web application,
known as a domain name.
2. Route internet traffic to the resources for your domain
When a user opens a web browser and enters your domain
name (example.com) or subdomain name
(acme.example.com) in the address bar, Route 53 helps
connect the browser with your website or web application.
3. Check the health of your resources
Route 53 sends automated requests over the internet to a
resource, such as a web server, to verify that it's reachable,
available, and functional. You also can choose to receive
notifications when a resource becomes unavailable and
choose to route internet traffic away from unhealthy
resources.

2.  BUILDING BLOCK OF
ROUTE53: ROUTE, ALIAS AND
CNAME
Record: Records in Route 53 are the basic building blocks of
the service. They map domain names to resources, such as
websites, web applications, and email servers.
Records can be of different types, such as
›  A record (map a domain name to an IPv4 address),
›  AAAA records (map a domain name to an IPv6 address),
›  CNAME records (map a domain name to another domain
name), and
›  MX records (map a domain name to a mail server).
You can create a record to map the domain
name example.com to your website, which is hosted on
Amazon S3.
Alias: An alias is a special type of record that maps a
domain name or subdomain to another domain name or
subdomain. Aliases are useful for redirecting traffic to a
different location, such as a new website or web application.
  You can create an alias to map the domain
name www.example.com to the domain name example.com.

This would redirect traffic
from www.example.com to example.com.
CNAME: A CNAME (canonical name) record maps a domain
name or subdomain to another domain name or subdomain.
CNAME records are useful for pointing to the canonical
version of a domain name or subdomain.
  You can create a CNAME record to map the domain
name blog.example.com to the domain
name example.com/blog. This would point visitors to the blog
section of your website.
Which record type should you use?
  If you need to create an alias for the root domain, you must
use an alias record. If you need to create an alias for a
subdomain and you need to route traffic to a Route 53
resource, you should use an alias record. If you need to
create an alias for a subdomain and you do not need to route
traffic to a Route 53 resource, you can use either an alias
record or a CNAME record.

3.  DIFFERENCES BETWEEN ALIAS
AND CNAME RECORDS
  Alias records can be used to create aliases for both the root
domain (e.g., example.com) and subdomains (e.g.,
www.example.com). CNAME records can only be used to
create aliases for subdomains.
  Alias records are more efficient than CNAME records
because they do not require an additional DNS lookup.
  Alias records can be used to route traffic to Route 53
resources, such as CloudFront distributions and Amazon S3
buckets. CNAME records can only be used to route traffic to
domain names.

4.  HOW ROUTE 53 WORKS
When a user visits a website or web application, their web
browser sends a DNS query to a DNS server. The DNS server
looks up the DNS record for the domain name and returns the
IP address of the resource that the domain name is mapped
to. The web browser then connects to the resource at that IP
address.
Route 53 acts as a DNS server for your domain names.
When a user visits a website or web application that is hosted
on AWS, their web browser sends a DNS query to Route 53.
Route 53 looks up the DNS record for the domain name and
returns the IP address of the resource that the domain name
is mapped to. The web browser then connects to the resource
at that IP address.

5.  ROUTE53 - ROUTING POLICY
Routing policies in Route 53 allow you to control the flow of
traffic to your resources. You can use routing policies to route
traffic to specific resources based on a variety of factors, such as
the location of the user, the time of day, or the health of the
resource.
Here are some of the different routing policies that you can
use in Route 53:

5.1  SIMPLE ROUTING POLICY
Use for a single resource that performs a given function for your
domain, for example, a web server that serves content for the
example.com website.
How it works:
  Enables standard DNS record creation without intricate Route
53 routing mechanisms like weighted routing or latency-based
routing.
  Typically routes traffic to a single resource, often a web server
hosting your website.
  Supports routing to public and private hosted zones.
Key characteristics:
  Single record with multiple values: You can't create multiple
records with the same name and type under simple
routing. However, you can specify multiple IP addresses or Alias
records within the same record.

  Random selection: When multiple values are present, Route 53
randomly distributes them to the recursive DNS resolver, which
then presents them to the client (e.g., web browser). The client
picks one value and re-submits the query.
  No health checks: Unlike other policies, simple routing doesn't
include health checks. So, it's not ideal for scenarios where
resource health monitoring is crucial.
Use cases:
  Single web server: If you have a single web server handling
your website traffic, simple routing efficiently directs all requests
to that server.
  Static content hosting: For distributing static content like
images or videos from a single source, simple routing offers a
basic yet effective solution.
  Simple load balancing: With multiple IP addresses in a
record, you achieve rudimentary load balancing, distributing
traffic among those IP addresses in a randomized manner.

5.2  WEIGHTED ROUTING POLICY 
Use to route traffic to multiple resources in proportions that you
specify. It offers more control than the simple routing policy in
traffic distribution.
How it works:
  Directs traffic to multiple resources with user-defined
weights, determining the percentage of traffic each receives.
  Supports public and private hosted zones.
Key characteristics:
  Multiple records with weights: Create multiple resource records
with the same name and type, assigning a weight (integer
value) to each.
  Weighted traffic distribution: Route 53 calculates the total
weight of all records and directs traffic based on each record's
weight relative to the total. Higher weights increase the
likelihood of selecting that resource.
  Health checks (optional): You can configure health checks to
monitor resource health. Unhealthy resources receive no traffic
until they recover.
Use cases:

  Load balancing: Distribute traffic across multiple web
servers, application instances, or database replicas based on
desired load distribution.
  A/B testing: Roll out new versions of applications or features
gradually by assigning lower weights to the new
version, gradually increasing traffic as you gather feedback.
  Disaster recovery: Route traffic to a secondary region or
failover site with a higher weight in case of primary resource
failure.
  Geographic redundancy: Serve users from geographically
closer resources by assigning higher weights to resources in
their region, improving latency and performance.
Latency routing policy — The latency-based routing policy
in Route 53 takes traffic routing a step further by dynamically
directing users to the resource that offers the lowest latency,
significantly improving response times and user experience.
Let's delve into its details and use cases:

How it works:
  Routes traffic to the resource in a specified AWS Region that
provides the lowest response time for the user's location.
  Utilizes internal AWS measurements and algorithms to
determine latency.
  Supports public and private hosted zones.
Key characteristics:
  Multiple resource records required: You need to create
resource records for each Region where you have resources.
  Dynamic routing based on latency: Route 53 continuously
measures latency between the user's location and each
resource's Region. The user is directed to the Region with the
shortest measured latency.
  Health checks (optional): You can configure health checks to
ensure traffic only reaches healthy resources.
Use cases:
  Global applications: Deliver content and services to users
worldwide with optimal response times, regardless of their
location.

  Performance-critical applications: Enhance user experience for
applications where low latency is essential, such as
gaming, real-time data, and financial trading platforms.
  Geo-targeting with low latency: Combine geolocation routing
with latency-based routing to target users in specific regions and
provide them with the fastest possible connection.

5.3  FAILOVER ROUTING POLICY
The failover routing policy in Route 53 ensures high availability
for your services by automatically redirecting traffic away from
unhealthy resources to designated backups. Here's a breakdown
of its key features and valuable applications:
How it works:
  Routes traffic to a primary resource when healthy and
seamlessly switches to a secondary resource if the primary
becomes unavailable.
  Supports public and private hosted zones.
Key characteristics:
  Primary and secondary records: You define a primary record
pointing to your main resource and a secondary record for
backup.
  Health checks: Mandatory health checks monitor the primary
resource's health. If unhealthy, traffic automatically routes to the
secondary.
  Active-passive or active-active configurations: Choose
between having the secondary resource remain inactive until
needed (active-passive) or actively serving traffic alongside the
primary (active-active, with failover triggered only when
unhealthy).

Use cases:
  Disaster recovery: Ensure continuous service availability by
redirecting traffic to a failover infrastructure in case of primary
resource outage in a different region.
  Planned maintenance: Seamlessly transition traffic to a
secondary resource during planned maintenance of the primary
one, minimizing downtime.
  Resource redundancy: Improve fault tolerance by having a
backup ready to take over if the primary resource experiences
issues.
  Geographically dispersed resources: Enhance regional service
availability by routing traffic to the closest healthy
resource, reducing latency.


5.4  GEOLOCATION ROUTING
POLICY 
Geolocation routing in Route 53 allows you to direct users to
resources based on their geographic location, enabling you to
deliver localized content, improve response times, and comply
with regional regulations. Let's explore its features and potential
use cases:
Hot it works:
  Routes traffic to different resources based on the user's
geographic location, determined by their DNS query origin.
  Supports public and private hosted zones.
Key characteristics:
  Multiple resource records: Create separate records for each
target geographic region, specifying the desired resource
(e.g., web server) for that region.
  Geographic targeting: Define regions by
continent, country, state, or even custom location groups.
  Failover option: Optionally configure a default record to handle
queries from unsupported locations or regions without specific
records.

  Health checks (optional): You can configure health checks for
each resource record to ensure traffic reaches healthy resources
within each region.
Use cases:
  Content localization: Deliver region-specific content, language
variations, or product offerings based on user location.
  Compliance with regulations: Restrict access to content only in
permitted regions based on local laws or regulations.
  Improved latency: Serve users from geographically closer
resources, reducing latency and improving website performance.
  Traffic management: Balance load across geographically
dispersed resources, optimizing resource utilization and cost
efficiency.


6.  MULTI-CHOICE QUESTIONS
AND ANSWER
1. What is the main function of an A record in Amazon Route
53?
(a) Redirect a domain name to another website.
(b) Map a domain name to an IP address for direct routing.
(c) Point a subdomain to another resource within the same
domain.
(d) Configure failover between multiple resources based on
health checks.
Answer: (b)
Explanation: An A record in Route 53 associates a domain
name with an IP address, directing web traffic directly to that
address.
2. What is the key difference between an Alias and a
CNAME record in Route 53?
(a) An Alias can only be used with CloudFront distributions,
while a CNAME can be used with any resource.
(b) An Alias creates a direct connection, while a CNAME acts
as a pointer without direct routing.

(c) An Alias is always used for root domain mapping, while
CNAMEs are for subdomains.
(d) There is no significant difference; they both achieve the
same outcome.
Answer: (b)
Explanation: An Alias acts as a direct routing record,
essentially creating a copy of the resource (e.g., CloudFront
distribution) within Route 53, while a CNAME simply points to
the existing location of the resource (e.g., another domain
name).
3. How does Route 53 translate a domain name into an IP
address for a website visitor?
(a) It searches its records based on the visitor's location and
selects the closest server. (b) It queries other DNS servers
around the world to find the authoritative answer.
(c) It analyzes the visitor's device and directs them to the
most optimized server.
(d) It uses artificial intelligence to predict the visitor's intent
and redirect them accordingly.
Answer: (b)

Explanation: Route 53 acts as a recursive resolver, initially
querying other DNS servers responsible for the top-level
domain (e.g., .com) and ultimately reaching the authoritative
nameserver for the specific domain, which provides the final
IP address.
4. What is the primary purpose of using a routing policy in
Route 53?
(a) Control access to specific resources based on IP address
restrictions.
(b) Implement failover between multiple resources in case of
an outage.
(c) Balance traffic evenly across several resource records.
(d) Set different TTL values for different records based on
content expiration.
Answer: (b)
Explanation: Routing policies allow you to configure
failover behavior, directing traffic to a healthy standby
resource when the primary resource becomes unavailable.

CHAPTER 9  AMAZON DATABASE
SERVICES

1.  OVERVIEW OF AWS DATABASES
Storing data on disk (EFS, EBS, EC2 Instance Store, S3) can have
its limits. Hence, Sometimes, you want to store data in a
database. AWS offers a wide range of database services that are
designed to meet the needs of a variety of workloads. These
services can be broadly categorized into four types:
  Relational databases store data in tables with rows and
columns. They are well-suited for storing structured data that
needs to be accessed and manipulated in a predictable way.
Examples of relational databases on AWS include Amazon
Relational Database Service (RDS), Amazon Aurora.
  Non-relational databases store data in a variety of formats,
such as key-value pairs, documents, or graphs. They are well-
suited for storing unstructured data that needs to be accessed
and manipulated in a flexible way. Examples of non-relational
databases on AWS include Amazon DynamoDB, Amazon
DocumentDB, and Amazon Neptune.

  Datawarehouse database: Redshift
  Memory based database: Elasticache

1.1  RELATIONAL DATABASE
A relational database is a type of database that stores data in
tables with rows and columns. The data in each row is related
to the data in other rows through the use of keys. Keys are
unique identifiers that are used to link related data together.
The Customer table contains data about the customer: 
  Customer ID (primary key)
  Customer name
  Billing address
  Shipping address 
  In the Customer table, the customer ID is a primary key
that uniquely identifies who the customer is in the relational
database. No other customer would have the same Customer
ID. 
  The Order table contains transactional information about an
order: 
  Order ID (primary key)
  Customer ID (foreign key)

  Order date 
  Shipping date
  Order status
  Here, the primary key to identify a specific order is the
Order ID. You can connect a customer with an order by using
a foreign key to link the customer ID from
the Customer table. 

1.2  NON-RELATIONAL DATABASE
A non-relational database, also known as a NoSQL database, is a
type of database that stores data in a non-tabular format. This
means that data is not stored in rows and columns like in a
relational database. Instead, data is stored in a variety of
formats, such as key-value pairs, documents, or graphs. Overall
non-relational databases are categorized into four different
types:
  Key-value stores
  Document stores
  Column-family stores
  Graph based
Key-value stores: Key-value stores store data as a collection
of key-value pairs. The key is a unique identifier for the data,
and the value is the data itself. Key-value stores are simple and
fast, and they are well-suited for storing small amounts of data.

Example: Redis, Riak, Amazon DynamoDB.We can consider
the Phone directory and MAC table as an example having key
and corresponding value.
Document stores: Document stores store data as documents.
A document is a collection of key-value pairs, but it can also
include other types of data, such as arrays and objects.
Document stores are flexible and can be used to store a variety
of different types of data. Documents can be stored in formats
like JSON and XML.
Example: Amazon DocumentDB, MongoDB & Azure Cosmos
DB

Below is a JSON document that stores information about a
user named John Doe.
Column-family stores: Column-family stores store data as
columns. A column family is a collection of columns that are
related to each other. Column-family stores are well-suited for
storing large amounts of data that is frequently accessed by
column.
Here’s a closer look at a column family:
A column family containing 3 rows. Each row contains its own
set of columns.

As the above diagram shows:
  A column family consists of multiple rows.
  Each row can contain a different number of columns to the
other rows. And the columns don’t have to match the columns in
the other rows (i.e. they can have different column names, data
types, etc).
  Each column is contained to its row. It doesn’t span all rows
like in a relational database. Each column contains a name/value
pair, along with a timestamp. Note that this example uses
Unix/Epoch time for the timestamp.
Here’s how each row is constructed:
Here’s a breakdown of each element in the row:
  Row Key. Each row has a unique key, which is a unique
identifier for that row.
  Column. Each column contains a name, a value, and
timestamp.

  Name. This is the name of the name/value pair.
  Value. This is the value of the name/value pair.
  Timestamp. This provides the date and time that the data was
inserted. This can be used to determine the most recent version
of data.
Examples of Column Store DBMSs: Bigtable, Cassandra and
HBase
Graph databases: Graph databases store data as a graph. A
graph is a collection of nodes and edges. The nodes represent
entities, and the edges represent relationships between entities.
Graph databases are well-suited for storing data that is highly
interconnected.
Key elements:
  Nodes: Represent entities like people, products, places, or
events.
  Edges: Represent relationships between nodes, indicating how
they are connected. Edges can have directions (directed edges)
or be bidirectional (undirected edges).
  Properties: Hold additional information about nodes and edges,
like names, attributes, or timestamps.
In a graph database, both nodes and the edges can store
data.

The above diagram is a graphical representation of a graph
database with three nodes, Bob, Alice, and James. The entity
type of the nodes is Person. The properties of Person entity
include Name, Sex, Born, College, and Employer. The connecting
lines (edges) represent the relationship between two nodes. The
most popular graph databases are Neo4j, OrientDB, and
AangoDB. Microsoft Azure CosmosDB also support graph model. 

2.  DIFFERENT AVAILABLE AWS
DATABASES:
Amazon Relational Database Service (RDS): A fully managed service
that makes it easy to set up, operate, and scale relational databases in the cloud.
Amazon Aurora: A MySQL- and PostgreSQL-compatible relational
database that combines the performance and availability of traditional enterprise
databases with the simplicity and cost-effectiveness of open-source databases.
Amazon Redshift: A fully managed, petabyte-scale data warehouse service that
makes it simple and cost-effective to analyze large datasets.
Amazon DynamoDB: A fully managed, document database service
that is compatible with MongoDB.
Amazon DocumentDB: A fully managed, document database
service that is compatible with MongoDB.
Amazon Neptune: A fully managed graph database service that makes it easy
to build and run applications that work with highly connected datasets.
Amazon ElastiCache: A fully managed in-memory cache service that makes it
easy to deploy, operate, and scale in-memory data stores.

Amazon MemoryDB for Redis: A fully managed, in-memory database service
that delivers ultra-fast performance for real-time applications.
Amazon Quantum Ledger Database (QLDB): A fully managed
ledger database that provides a transparent, tamper-proof, and auditable record
of transactions.

3.  CHOOSING THE RIGHT AWS
DATABASE
The best AWS database for your needs will depend on the
specific requirements of your The best AWS database for your
needs will depend on the specific requirements of your
workload. We need to Consider the following aspects when
selecting a database:
• Data structure: What is the structure of your data? Does
it demonstrate a structured, unstructured, or hybrid nature?
• Utilization patterns: In what manner will your data be
accessed? Will the work involve executing complicated
queries, or will it be limited to data viewing and writing?
• Performance requirements: What are your performance
requirements? High throughput or minimal latency is what
you require.
• Scalability prerequisites: To what extent must your
database contain scalability?
• Cost: What is your database expenditure budget?
Amazon Web Services employs two distinct methodologies
when it comes to utilizing databases.
1) EC2 hosted databases
2) Databases administered by AWS

3.1 EC2 HOSTED DATABASES
EC2 hosted databases are databases that are deployed and
managed on Amazon Elastic Compute Cloud (EC2) instances.
You have complete authority over the fundamental
infrastructure, comprising the database software, operating
system, and database configuration, when utilizing EC2 hosted
databases. This gives you the ability to tailor your database
environment to your particular requirements. Nevertheless, this
involves the responsibility of managing the database, which
includes backups, security updates, and performance
optimization, among other responsibilities.
Examples of EC2 hosted databases
  MySQL on Amazon EC2
  PostgreSQL on Amazon EC2
  Oracle Database on Amazon EC2
  Microsoft SQL Server on Amazon EC2

3.2 AWS MANAGED DATABASES
AWS managed databases are cloud-based services that handle
the entire lifecycle of your database, from provisioning and
setup to patching, upgrades, and backups. You choose the
database engine you need (like MySQL, PostgreSQL, or Aurora)
and let AWS handle the rest.
Hence end to end responsibility lies with Amazon Web
Services.
Examples of AWS managed databases
  Amazon Relational Database Service (RDS)
  Amazon Aurora
  Amazon Redshift
  Amazon DynamoDB
  Amazon DocumentDB
AWS managed databases are databases that are managed by
AWS. With AWS managed databases, you do not need to worry
about managing the underlying infrastructure. AWS takes care of

all of the database administration tasks, such as backups,
security patches, and performance tuning. This allows you to
focus on developing and deploying your applications.
Deciding when to use EC2 hosted databases or AWS
managed databases
The best type of database for you will depend on your
specific needs. If you need full control over your database
environment, then EC2 hosted databases may be the right
choice for you. However, if you want to avoid the hassle of
managing a database, then AWS managed databases may be a
better option. Refer below given table:

4.  AMAZON RELATIONAL
DATABASE SERVICE (RDS)
The Amazon Relational Database Service (RDS) is a cloud-
based web service that simplifies the configuration,
management, and expansion of relational databases. It
provides resizable capacity at a reasonable cost for an
industry-standard relational database and handles routine
database administration responsibilities.
Features of Amazon RDS
  RDS stands for Relational Database Service
  The database is a managed service that employs SQL as its
query language.  
  It facilitates you to create databases in the cloud that are
managed by AWS
›  Postgres
›  MySQL
›  MariaDB
›  Oracle

›  Microsoft SQL Server
›  Aurora (AWS Proprietary database)
  Multi-AZ deployments: Multi-AZ deployments are
supported by RDS, ensuring durability and high availability.
RDS generates a primary database instance in one
Availability Zone and a standby instance in another
Availability Zone when a multi-AZ deployment is created. In
the event of a failure in the primary instance, the reserve
instance is promoted automatically to the status of the
primary instance.
  Read replicas: Employing the read replicas that RDS
provides can enhance the efficiency of applications that rely
heavily on reading. Read replicas function as duplicates of the
principal database instance, providing access to fulfill read
requests.
  Point-in-time recovery: RDS supports point-in-time
recovery, which allows you to restore your database to a
specific point in time. This can be useful if you need to
recover from a data corruption or other problem.
  Automated backups: RDS automatically backs up your
database instances on a daily basis. These backups can be
used to restore your database to a previous point in time.

  Performance insights: RDS offers performance insights that
can be utilized to detect and resolve performance issues.
Metrics including CPU utilization, memory usage, and
database connections are encompassed within these insights.

Advantages of using Amazon Relational Database
Service (RDS) over deploying a database on Amazon
Elastic Compute Cloud (EC2)
Amazon Relational Database Service (RDS) is a managed
database service that provides a cost-effective, reliable, and
secure way to set up, operate, and scale relational databases
in the cloud. Deploying a database on Amazon Elastic
Compute Cloud (EC2) gives you more control over the
underlying infrastructure, but it also means that you are
responsible for managing the database, including tasks such
as backups, security patches, and performance tuning.
Advantages of using RDS over deploying a database on EC2
  Managed service: Since RDS is a managed service, AWS
takes care of all the database management jobs, like making
backups, installing security patches, and fine-tuning
performance. This gives you more time to work on making
and launching your applications.
  Automated provisioning, OS patching, and continuous
backups: RDS automatically provisions database instances,
patches operating systems, and performs continuous
backups. This helps to ensure that your databases are always
up to date and protected from data loss.
  Point-in-time restore: RDS supports point-in-time restore,
which allows you to restore your database to a specific point

in time. This can be useful if you need to recover from a data
corruption or other problem.
  Monitoring dashboards: RDS provides monitoring
dashboards that give you insights into the performance of
your databases. This information can be used to identify and
troubleshoot performance problems.
  Read replicas for improved read performance: RDS supports
read replicas, which can be used to improve the performance
of read-heavy applications. Read replicas are copies of the
primary database instance that can be used to serve read
requests.
  Multi-AZ setup for disaster recovery: RDS supports multi-AZ
deployments, which provide high availability and durability.
When you create a multi-AZ deployment, RDS creates a
primary database instance in one Availability Zone and a
standby instance in another Availability Zone. If the primary
instance fails, the standby instance is automatically promoted
to become the primary instance.
  Maintenance windows for upgrades: RDS provides
maintenance windows for upgrades. This allows you to
schedule upgrades for a time when they will have the least
impact on your applications.

  Scaling capability (vertical and horizontal): RDS supports
both vertical and horizontal scaling. Vertical scaling allows
you to increase the resources allocated to a database
instance. Horizontal scaling allows you to add additional read
replicas to a database instance.
  Storage backed by EBS (gp2 or io1): RDS instances are
backed by Amazon Elastic Block Store (EBS) volumes. EBS
volumes provide high performance and durability.

4.1  AMAZON AURORA
The Amazon Aurora relational database service is a fully
managed relational database that combines the ease of use and
cost-effectiveness of open-source databases with the
performance and dependability of high-end commercial
databases. Due to the fact that Aurora is compatible with both
MySQL and PostgreSQL, you will be able to migrate your existing
applications and databases without having to make any
modifications.
Amazon Aurora is equipped with a storage system that is
fault-tolerant, self-healing, and distributed, and it can
automatically grow up to 64 terabytes for each database
instance.
RDS Solution Architecture
There are a number of different RDS solution architectures
that can be used to meet the needs of different applications.
Some of the most common RDS solution architectures include:
4.1.1 Single-AZ RDS Deployment
In a Single-AZ RDS deployment, your database instance is
deployed in a single Availability Zone (AZ). This is the simplest
and most cost-effective RDS deployment option. However,
Single-AZ deployments do not provide high availability. If the AZ
in which your database instance is deployed becomes
unavailable, your database will also be unavailable.


4.1.2 Multi-AZ RDS Deployment
In a multi-AZ RDS deployment, your database instance is
deployed in two or more AZs. This provides high availability. If
the AZ in which your primary database instance is deployed
becomes unavailable, your database will automatically failover
to a replica in another AZ.
4.1.3 RDS with Read Replicas
In an RDS deployment with read replicas, you have one primary
database instance and one or more read replicas. Read replicas
are copies of the primary database instance that can be used for
read-only operations. Using read replicas can improve the
performance of your database by offloading read-only
operations from the primary database instance.

4.1.4 Aurora Multi-Master Deployment
In an Aurora Multi-Master deployment, you have two or more
Aurora database clusters that are configured as masters. This
allows you to write to any of the masters, and changes will be
replicated to all of the other masters. The architecture of multi-
master clusters differs from those of other Aurora cluster types.
All DB instances in multi-master clusters can read and write
data. While all other DB instances in other types of Aurora
clusters are read-only and only capable of
handling SELECT queries, one dedicated DB instance handles all
write activities. Multi-master clusters do not contain primary

instances or read-only Aurora Replicas.

4.2  AURORA GLOBAL DATABASE
An Aurora Global Database is a single database that is spread
across multiple regions. This allows you to read and write to your
database from anywhere in the world.
  The primary AWS regions consist of primary Aurora instance
for read-write purpose and multiple secondary instances for
read-only workloads
  The Secondary replica has only read-only Aurora replicas. You
cannot perform any read-write operations here
  In the secondary region, we can have up to 16 read replicas in
comparison to the primary replica where 1 primary read-write
and 15 read replicas are allowed
  You can use the secondary region for offloading read-only
workloads and disaster recovery purposes

Lab 1: Create the RDS DB instance
1.  Open the Amazon RDS console at
https://console.aws.amazon.com/rds/. and Click Create
database to start the configuration process. We also have
Restore Multi-AZ DB Cluster from Snapshot Which we can ignore
now.
2.  In the Choose a database creation method section, ensure
the Standard Create option is selected. Next, in the Engine
options section, choose the MySQL engine type and
the MySQL 8.0.35 Version.

3.  In the Templates section, select Production. In
the Availability and Durability section, select Single DB
instance option. We also have two other options:

Multi-AZ DB Instance: Used for Read Replica to improve
performance
Multi-AZ DB Instance: Used for Multi-AZ support for DR You can
select
Free Tier and Dev/Test Option as per your business need. In case
of Free Tier, you will not get different Availability and durability
Options.


4.  In the Settings section, set the DB instance
identifier to database-demo-24. Configure the name and
password of the master database user, with the most elevated
permissions in the database.

5.  In the DB instance size section, select Burstable classes,
and choose t3.medium in the size drop-down. In the Storage
Section, select Provisioned IOPS under Storage Type, and

set 100 as Allocated Storage.
Select 1000 under Provisioned IOPS. Uncheck the Enable
storage autoscaling box.


6.  In the Database Authentication section, select Password
and IAM database authentication.

7.  Finally click on Create Database.

8.  While database creation is in progress, you will receive the
below screen having status like “Creating”
9.  Finally, you will receive the below screen once database is
available. The MySQL database instance may take several
minutes to provision. In order to connect to the DB instance and
start using it, you need to retrieve the DB instance endpoint.
10.  The Endpoint & port section in the Connectivity and
security tab of the details page displays the endpoint. Note this
value down, as you will use them later.


Lab 2: Deletion of RDS database
Here are the steps to delete an RDS database in AWS:
1. Access the RDS Console:
  Log in to the AWS Management Console and navigate to the
Amazon RDS service.
2. Locate the DB Instance:
  Find the target database instance you want to delete from the
list of instances.
  If needed, utilize filters or the search bar to locate it.
3. Initiate Deletion:
  Click on the Actions button associated with the instance.
  Select "Delete" from the dropdown menu.
4. Confirm Deletion:
  A pop-up window will appear requesting confirmation.
  Type "delete me" in the text box to verify your intent.
  Click on the "Delete" button to proceed.
5. Monitor Progress:
  The instance status will change to "deleting."

  Track deletion progress in the RDS console.
  Deletion typically takes a few minutes, but it may vary
depending on database size and complexity.
Important Considerations:
  Data Loss: Deleting a database instance permanently erases
all its data, including automated backups.
  Snapshot Retention: If you desire to preserve a snapshot of the
database before deletion, manually create one.
  Associated Resources: Deletion terminates the database
instance but not other related resources like
snapshots, parameter groups, or DB security groups. Manage
those separately.
  Final Snapshot: AWS automatically creates a final snapshot
before deletion. You can choose to create manual snapshots as
well.
  Deletion Protection: Ensure the instance doesn't have deletion
protection enabled, as it would prevent deletion.
  Additional Costs: Snapshots incur storage costs. Monitor and
manage them accordingly.

Caution: Exercise care when deleting RDS instances as the
process is irreversible. Ensure you have backups or snapshots if
data preservation is crucial.

Lab 3: Modification of RDS database
While you cannot directly modify an existing RDS database
instance, you can achieve modifications through these methods:
1. Modifying Instance Class:
  To adjust compute resources:
o  Navigate to the RDS console.
o  Select the target instance.
o  Click "Modify" under Instance Actions.
o  Choose the desired DB instance class.
o  Apply modifications.
  Impact: Brief outage for the instance.
2. Scaling Storage:
  To increase storage capacity:
o  Follow similar steps as above.
o  Modify the allocated storage size.

  Impact: Typically, no downtime, but I/O might be affected
during resizing.
3. Updating Parameters:
  To change database configuration settings:
o  Edit the associated DB parameter group.
o  Changes usually take effect within a few minutes.
  Caution: Some parameters require instance reboot.
4. Restoring from a Snapshot:
  To revert to a previous state or create a modified copy:
o  Restore a snapshot to a new instance.
o  Configure desired changes during the restoration process.
Other Modifications:
Changing DB engine version: Create a new instance with the
desired version and migrate data.
  Modifying security group rules: Edit the associated DB security
group.
  Enabling/disabling deletion protection: Toggle the setting in
the instance configuration.

5.  AMAZON DYNAMODB
DynamoDB is Fully managed, NoSQL database service with fast
and predictable performance which provides Seamless
scalability with push button scaling and no downtime. It is
having High availability with synchronous replication across 3
Availability Zones (AZs) in a region.
DynamoDB is a key-value and document database and offers
single-digit millisecond latency for both reads and writes. Apart
from DynamoDB is a pay-as-you-go service and it is fully
managed service, so you don't have to worry about managing
the underlying infrastructure
Some other key points related to DynamoDB:
  It consists Global tables for deploying a multi-region, multi-
master database
  DynamoDB Accelerator (DAX) for a 10x performance
improvement
  Scalable to massive workloads, distributed "serverless"
database
  Integrated with IAM for security, authorization, and
administration
  Low cost and auto scaling capabilities

  Standard and Infrequent Access (IA) Table Class for storing
data that is accessed less frequently
DynamoDB – Key/Value Database
DynamoDB is a key-value and document database. In a key-
value database, each item is stored as a key-value pair. The key
is a unique identifier for the item, and the value is the data that
is associated with the item.
Please refer the below DynamoDB table shown in the image,
based on the key-value pairs:
  The table has a primary key, which is a combination of the
partition key and the sort key.
  The partition key is used to evenly distribute data across
partitions. In the example, the partition key is the product ID.
  The sort key is used to order items within a partition. In the
example, the sort key could be the book ID, album ID, or movie
ID.
  Each item in the table has a set of attributes. These attributes
can be any type of data, such as strings, numbers, or Booleans.

For example, in a DynamoDB table that stores product
information, the key could be the product ID, and the value
could be a JSON document that contains information about the
product, such as the product name, price, and description.
DynamoDB is a highly scalable database that can be used to
store large amounts of data. DynamoDB automatically scales up
or down to handle changes in demand, so you never have to
worry about provisioning or managing capacity.
DynamoDB is also a highly available database. Your data is
automatically replicated across multiple Availability Zones in an
AWS Region, and is backed by durable storage. This means that
your data is always available, even in the event of a hardware
failure.
However, DynamoDB does support a number of features that
make it a powerful and versatile database. These features

include:
  Secondary indexes: Secondary indexes allow you to query the
database based on the value of an attribute. For example, you
could create a secondary index on the price attribute in the
product table. This would allow you to query the table for all
products that are priced between $10 and $20.
  Global tables: Global tables allow you to replicate your data
across multiple AWS Regions. This can be useful for applications
that need to be available in multiple regions.
  DynamoDB Streams: DynamoDB Streams is a service that
allows you to capture changes to data in DynamoDB tables. This
can be useful for applications that need to be notified of
changes to data.
DynamoDB is a good choice for a wide range of applications,
such as:
  Web and mobile applications
  Real-time applications
  Big data applications

5.1 AMAZON DYNAMODB
ACCELERATOR (DAX)
Amazon DynamoDB Accelerator (DAX) is designed to run within
an Amazon Virtual Private Cloud (Amazon VPC) environment.
The Amazon VPC service defines a virtual network that closely
resembles a traditional data center. With a VPC, you have
control over its IP address range, subnets, routing tables,
network gateways, and security settings. You can launch a DAX
cluster in your virtual network and control access to the cluster
by using Amazon VPC security groups.
To create a DAX cluster, you use the AWS Management
Console. Unless you specify otherwise, your DAX cluster runs
within your default VPC. To run your application, you launch an
Amazon EC2 instance into your Amazon VPC. You then deploy
your application (with the DAX client) on the EC2 instance.
At runtime, the DAX client directs all of your application's
DynamoDB API requests to the DAX cluster. If DAX can process

one of these API requests directly, it does so. Otherwise, it
passes the request through to DynamoDB.
Finally, the DAX cluster returns the results to your application.

5.2 DYNAMODB – GLOBAL TABLES
DynamoDB Global Tables
is a fully managed service that makes it easy to deploy and
manage multi-region, multi-master DynamoDB tables. Global
tables provide a single logical table that is replicated across
multiple AWS Regions. This allows you to read and write data
from any region, and changes are automatically replicated to
all other regions.
Global tables offer a number of benefits, including:
  Global availability: Global tables make your data
available in multiple regions, which can improve performance
and reduce latency for users around the world.
  Multi-master replication: Global tables allow you to read
and write data from any region. This can improve availability
and reduce latency for write-heavy applications.

  Global consistency: Global tables provide eventual
consistency across all regions. This means that changes
made in one region will eventually be reflected in all other
regions.
  Ease of use: Global tables are easy to set up and manage.
You can create a global table with just a few clicks in the AWS
Management Console.

6.  AMAZON REDSHIFT
Amazon Redshift is a fully managed, petabyte-scale data
warehouse service in the cloud that makes it simple and cost-
effective to analyze all your data using standard SQL and your
existing business intelligence (BI) tools.
With Redshift, you can:
  Run complex analytic queries against petabytes of data using
sophisticated query optimization, columnar storage on high-
performance local disks, and massively parallel query execution.
  Get fast insights into your data with sub-second response
times for common analytical queries.
  Easily load and manage your data with a variety of built-in
data loading and management tools.
  Scale your data warehouse to meet your changing needs with
just a few clicks.
  Pay only for the resources you use with a variety of pricing
options.
Redshift is a powerful tool that can help you get more value
from your data.
Amazon Redshift uses SQL to analyze structured and semi-
structured data across data warehouses, operational databases,

and data lakes, using AWS-designed hardware and machine
learning to deliver the best price performance at any scale.

7.  AWS MEMORY-BASED
DATABASES:
AWS memory-based databases are purpose-built databases
that store data in memory instead of on disk. This provides
ultra-fast performance for applications that require
microsecond response times or have large spikes in traffic.
Some of the benefits of using AWS memory-based
databases include:
  Microsecond latency: Memory-based databases can
provide sub-millisecond latency for read and write operations.
This makes them ideal for applications that require real-time
or near-real-time data access.
  High throughput: Memory-based databases can handle
millions of requests per second. This makes them well-suited
for applications that need to scale to handle large volumes of
data.
  Predictable performance: Memory-based databases
provide predictable performance because they are not
affected by disk latency. This makes them ideal for
applications that require consistent response times.
Some of the AWS memory-based databases include:

  Amazon MemoryDB for Redis: A fully managed, in-
memory database service that is compatible with Redis.
  Amazon ElastiCache: A fully managed, in-memory
caching service that supports Memcached and Redis.
  Amazon DynamoDB Accelerator (DAX): A fully managed,
in-memory cache for Amazon DynamoDB.

7.1 AMAZON ELASTICACHE
Amazon ElastiCache is a fully managed, in-memory data
store and cache service that delivers
real-time, cost-optimized performance for modern
applications. ElastiCache supports two open-source engines:
Memcached and Redis.
›  Memcached is a high-performance, distributed memory
caching system that is designed to speed up dynamic web
applications by caching frequently accessed data in memory.
›  Redis is an in-memory data structure store, used as a
database, cache, and message broker. It supports data
structures such as strings, hashes, lists, sets, sorted sets with
range queries, bitmaps, hyper logs, geospatial indexes, and
streams.

ElastiCache Solution Architecture:
Here is high-level overview of the architecture, with the
following components:
  ElastiCache: This is the core of the architecture, and it's an
in-memory data store or cache. It stores frequently accessed
data in memory, which makes it much faster to access than
data stored on disk. ElastiCache supports two popular
caching protocols: Memcached and Redis.
  EC2 Instances: These are virtual servers that run in the
cloud. You can choose to run your ElastiCache cluster on a
single EC2 instance, or you can distribute it across multiple
instances for improved performance and scalability.
  Elastic Load Balancer: This is a service that distributes
traffic across your ElastiCache cluster. This ensures that no

single instance is overloaded, and it also helps to improve the
availability of your cache.
  Amazon RDS: This is a managed database service offered
by AWS. You can use Amazon RDS to store data that is not
accessed as frequently as the data stored in ElastiCache.
Here's how the architecture works:
1.  The application interacts with an Amazon Elastic Load
Balancer, which distributes traffic across multiple EC2
instances in an Auto Scaling Group.
2.  Your application reads data from the ElastiCache cluster.
Which is having faster read and write due to access from
cache.
3.  If the data is not found in the ElastiCache cluster, the
application reads it from the Amazon RDS database.
4.  The data is then written back to the ElastiCache cluster.
This architecture can significantly improve the
performance of your applications by reducing the number of
times that your application needs to access the slower
Amazon RDS database.
Apart from improvement in Application perform some
other benefits using Elasticache are:

  Reduced database load: ElastiCache can help you offload
some of the data requests from your database, improving
database performance and scalability.
  Simplified cache management: ElastiCache takes care of all
the operational tasks associated with managing a cache,
freeing up your developers to focus on building and
maintaining your applications.
  High availability and durability: ElastiCache offers high
availability and durability, ensuring that your data is always
accessible and protected.
  Scalability: ElastiCache can be easily scaled up or down to
meet the needs of your applications.
  Cost-effective: ElastiCache is a cost-effective way to
improve the performance of your applications.

8.  MULTI-CHOICE QUESTIONS
AND ANSWER
1. Relational databases in AWS offer:
(a) Schemas with predefined data structures
(b) Flexible data models without fixed schemas
(c) Primarily NoSQL storage options
(d) Limited scalability and availability
Answer: (a)
Explanation: Relational databases like Amazon RDS follow
a structured schema, offering strong consistency and query
capabilities.
2. What is the primary difference between a relational and
a non-relational database?
(a) Relational databases store data in structured tables, while
non-relational databases do not.
(b) Relational databases are slower than non-relational
databases.
(c) Relational databases are only suitable for enterprise
applications, while non-relational databases are better for
web applications.

(d) Non-relational databases offer higher data consistency
than relational databases.
Answer: (a)
Explanation: Relational databases organize data in tables
with rows and columns, enforcing relationships between data
points. Non-relational databases offer more flexible data
structures without strict relationships.
3. Which of the following is NOT a key advantage of non-
relational databases like DynamoDB?
(a) High scalability and performance
(b) Simple key-value data model
(c) Limited support for complex queries
(d) Cost-effectiveness for large datasets
Answer: (c)
Explanation: While NoSQL databases excel in scalability
and performance, they often require specialized techniques
for complex queries compared to relational options.
4. When choosing the right AWS database, you should
consider:
(a) Only cost and ease of use
(b) Data model requirements and access patterns
(c) Brand recognition and popularity

(d) Vendor lock-in and migration complexity
Answer: (b)
Explanation: Selecting the best database depends on your
specific data characteristics, access patterns, and desired
performance and scalability.
5. EC2-hosted databases offer:
(a) Managed service benefits like automated backups and
patching
(b) Full control over configuration and instance management
(c) Limited scalability compared to managed database
services
(d) All of the above
Answer: (b)
Explanation: While providing flexibility, EC2-hosted
databases require manual management, unlike managed
services like RDS.
6. Shared responsibility in AWS databases means:
(a) AWS manages everything, and you have no responsibility.
(b) You are solely responsible for data security and backups.
(c) AWS manages the underlying infrastructure, and you
manage data and configuration.

(d) Security is entirely a shared responsibility between AWS
and the user.
Answer: (c)
Explanation: AWS handles the physical infrastructure, but
you are responsible for securing your data, managing access,
and performing backups.
7. Which of the following is NOT an advantage of using a
managed database service like Amazon RDS?
(a) Automatic scaling and patching
(b) Reduced administrative overhead
(c) Increased data security risks
(d) Improved performance and availability
Answer: (c)
Explanation: Managed services like RDS handle tasks like
scaling, patching, and backups, freeing up your time and
potentially reducing risks.
8. What is the benefit of using Multi-AZ deployment for
RDS instances?
(a) Improved write performance
(b) Enhanced data security
(c) Increased availability and disaster recovery

(d) Reduced storage costs
Answer: (c)
Explanation: Multi-AZ deployment distributes your
database across multiple Availability Zones, ensuring high
availability and failover in case of an outage in one zone.
9. Which AWS database service offers in-memory caching
for faster data access?
(a) Amazon RDS
(b) Amazon Aurora
(c) Amazon DynamoDB
(d) Amazon ElastiCache
Answer: (d)
Explanation: Amazon ElastiCache provides in-memory
caching for various database engines, including Redis and
Memcached, significantly improving read performance.

CHAPTER 10  MISCELLANEOUS
SERVICES
Amazon CloudWatch
Amazon CloudFront
Amazon CloudTrail


1.  AMAZON CLOUDFRONT
Amazon CloudFront is a content delivery network (CDN)
service from Amazon Web Services (AWS) that accelerates
the delivery of your static and dynamic web content to users
around the world. It does this by caching your content at
edge locations in the AWS global network, which are
geographically distributed data centers that are closer to
your users than your origin server.
A Content Delivery Network (CDN) is a globally distributed
network of servers that work together to deliver internet
content, such as web pages, images, videos, and other static
assets, to users more quickly and reliably.

1.1 HOW DOES CLOUDFRONT
WORK?
When a user requests content that is served by CloudFront, the
request is routed to the edge location that is closest to the user.
If the content is already cached at that edge location, it is
delivered to the user immediately. If the content is not cached,
CloudFront fetches it from your origin server (which could be an
Amazon S3 bucket, an EC2 instance, or another web server) and
then caches it at the edge location for future requests. It also
gets checked in regional edge cache. Regional Edge Caches are
located between origin web servers and global edge.
Benefits of using CloudFront
There are many benefits to using CloudFront, including:
  Reduced latency: By caching your content at edge locations
that are closer to your users, CloudFront can significantly reduce
the latency of your website or application.

  Improved availability and reliability: CloudFront's global
network is highly redundant and resilient, so your content will be
available to users even if your origin server is unavailable.
  Reduced costs: CloudFront can help you reduce your
bandwidth costs by offloading traffic from your origin server.
  Improved security: CloudFront offers a number of security
features, such as field-level encryption and access controls,
which can help you protect your content.
  It Integrates with many AWS services (S3, EC2, ELB, Route 53,
Lambda)
Origins and Distributions: An origin is the origin of the files
that the CDN will distribute. Origins can be either an S3 bucket,
an EC2 instance, an Elastic Load Balancer, or Route 53 – can
also be external (non-AWS). To distribute content with
CloudFront you need to create a distribution.
CloudFront uses Edge Locations and Regional Edge Caches:
An edge location is the location where content is cached
(separate to AWS regions/AZs) Requests are automatically
routed to the nearest edge location
Regional Edge Caches are located between origin web
servers and global edge locations and have a larger cache
Regional Edge caches aim to get content closer to users
In essence, the process involves establishing a CloudFront
distribution, which specifies the desired origin of content

delivery and the necessary parameters for managing and
tracking content delivery. From there, CloudFront efficiently
delivers the requested content to users or viewers by utilizing
edge servers, which are computers located in close proximity to
the viewers.
Lab 1: Creation of simple CloudFront
distribution
This involve below given steps:
  Step 1: Create an accessible bucket
  Step 2: Upload the content to the bucket
  Step 3: Create a CloudFront distribution
  Step 4: Access your content through CloudFront
Step 1: Create an accessible bucket

1.  Sign in to the AWS Management Console and open the
Amazon S3 console at 
https://console.aws.amazon.com/s3/.
2.  Choose Create bucket.
3.  Complete Bucket name.
4.  For Region, choose an AWS Region that is geographically
close to you, to reduce latency and costs.
5.  Complete the Object Ownership section as follows:
  Choose ACLs enabled
  In Object Ownership, choose Bucket owner preferred.
  Unselect Block all public access.
  In the warning section, select the check box for I
acknowledge that the current settings might result in
this bucket and the objects within becoming public.
We recommend that you read all the information on this page
about ACLs and public access, so that you can start to learn
about the security that you want to apply to your distributions,
when you go into production.

6.  Leave all other settings at their defaults, and then
choose Create bucket with the name project-s3bucket-
demo24.
Step 2: Upload the content to the bucket
1.  In the Buckets section, choose your new bucket. The page
for the bucket appears. Choose Upload.
2.  On the Upload page, drag the three hello world files into the
drop area.
3.  Leave all other settings at their defaults, and then
choose Upload.
4.  After the upload is complete, you can enter the Amazon S3
URL in a web browser, to verify that the content is publicly
accessible.
5.  Enable static website option for the given bucket and it will
give below URL for static website: http://project-s3bucket-
demo24.s3-website-us-east-1.amazonaws.com
6.   

Step 3: Create a CloudFront distribution
1.  Open the CloudFront console
at https://console.aws.amazon.com/cloudfront/v4/home.
2.  Choose Create distribution. Complete the settings as
shown in the table and select the option as mentioned.
Origin: Origin domain: you can select S3 bucket endpoint name
or AWS Region-specific website endpoint of the bucket, in case if
Static website hosting option is enabled. Other option you
can select as default.
Origin: Origin domain:

http://project-s3bucket-demo24.s3-website-us-east-
1.amazonaws.com/
3.  Choose Create distribution. Look at the Details section for
the new distribution. After a few minutes, the Last
modified field will change from Deploying to a date and time.
4.  Record the domain name - d200ofg00qqjz0.cloudfront.net
that CloudFront assigns to your distribution.
5.  Step 4: Access your content through CloudFront
To access your content through CloudFront, combine the
domain name for your CloudFront distribution with the main
page for your content.

  Your distribution domain name might look like
this: d200ofg00qqjz0.cloudfront.net
  The path to the main page of a website is typically /index.html.
Therefore, the URL to access your content through CloudFront
might look like this:
https://d200ofg00qqjz0.cloudfront.net/index.html
You will find the webpage given below:

2.  AMAZON CLOUDWATCH
Amazon CloudWatch is a monitoring service for AWS cloud
resources and the applications you run on AWS. CloudWatch
is for performance monitoring (CloudTrail is for auditing) and
it is regional service.
CloudWatch Alarms can be set to react to changes in your
resources. CloudWatch Events generates events when
resource states change and delivers them to targets for
processing
Here's a breakdown of what CloudWatch does:
  Collects Data
  Analyzes and Visualizes
  Automates Actions
Let us discuss all this in one by one
Collects Data
  Metrics: These are numerical values that measure things
like CPU usage, disk space, and API calls. CloudWatch
automatically collects metrics from most AWS services, and
you can also define custom metrics for your own applications.
  Logs: These are textual records of events that happen
within your resources and applications. CloudWatch can

collect logs from Amazon CloudTrail, S3, EC2, and other
services, as well as your own custom logs.
  Events: These are notifications about specific events that
occur, such as an EC2 instance launching or an S3 bucket
being created. CloudWatch can integrate with various AWS
services and applications to capture these events.
Analyzes and Visualizes:
  Dashboards: You can create custom dashboards to
visualize the data CloudWatch collects. These dashboards can
include charts, graphs, and tables, allowing you to quickly
identify trends and patterns.
  Alarms: You can set alarms on metrics, logs, and events to
trigger notifications when certain thresholds are met. This
allows you to proactively address potential issues before they
become major problems.
  Insights: CloudWatch provides built-in analytics that can
help you identify anomalies, uncover root causes of issues,
and optimize your resource utilization.
Automates Actions:
  Auto Scaling: You can use CloudWatch alarms to
automatically trigger auto scaling actions, such as launching

new EC2 instances or scaling down an ECS cluster. This helps
you maintain performance and optimize costs based on real-
time demand.
  CloudWatch Events: You can use CloudWatch to trigger
Lambda functions or send notifications to other AWS services
based on alarms or events. This allows you to automate
responses to changes in your environment.

2.1  AMAZON CLOUDWATCH
IMPORTANT METRICS
Amazon CloudWatch Important Metrics are Key Performance
Indicators (KPIs) for Monitoring Critical Resource. It provides a
comprehensive monitoring service for various AWS resources,
enabling you to gain insights into their performance, health,
and utilization. Here's a formal breakdown of the key metrics
you should consider tracking:
EC2 Instances:
  Monitored Metrics:
o  CPU Utilization: Measures the percentage of CPU capacity in
use.
o  Status Checks: Indicates the instance's overall health and
functionality.
o  Network In/Out (excluding RAM): Tracks incoming and
outgoing network traffic.
  Default Monitoring: Metrics collected every 5 minutes at no
additional cost.
  Detailed Monitoring (Enhanced): Offers 1-minute granularity
for deeper insights, incurring additional charges.

EBS Volumes:
  Monitored Metrics:
o  Disk Read/Write Bytes: Measures the volume of data read
from or written to the disk.
o  Disk Read/Write Operations: Tracks the number of
read/write operations performed on the disk.
S3 Buckets:
  Monitored Metrics:
o  BucketSizeBytes: Tracks the total size of all objects stored
in the bucket.
o  NumberOfObjects: Indicates the total number of objects
within the bucket.
o  AllRequests: Monitors the total number of requests made to
the bucket.
Billing:
  Monitored Metrics:

o  Total Estimated Charge (us-east-1 only): Provides an
estimate of your current AWS charges within the us-east-1
region.
Service Limits:
  Monitored Metrics: Tracks your API usage for individual AWS
services, helping you identify potential throttling issues.
Custom Metrics:
  Provision: CloudWatch allows you to define and push your
own custom metrics for specific applications or resources.
Additional Notes:
  Enhanced monitoring for EC2 instances incurs additional
charges.
  Total Estimated Charge metrics are currently limited to the
us-east-1 region.

2.2  AMAZON CLOUDWATCH
ALARMS
Amazon CloudWatch Alarms are Proactive Response
Mechanisms for Automated Actions. It serves as a vital tool
for triggering automated responses to events or conditions
detected within your AWS environment. Here's a formal
overview of their key features and functionality:
Key Features:
  Trigger Notifications: Alarms can be configured to initiate
notifications for any monitored metric, ensuring timely
awareness of potential issues or anomalies.
  Automated Actions: CloudWatch Alarms support a range of
automated actions to streamline response and remediation
efforts:
o  Auto Scaling: Dynamically adjust the "desired" count of EC2
instances based on alarm thresholds, optimizing resource
utilization and availability.
o  EC2 Actions: Directly initiate actions on EC2
instances, including stopping, terminating, rebooting, or
recovering instances, as needed.

o  SNS Notifications: Send notifications to designated SNS
topics, enabling integration with other services or alerting
mechanisms.
Configuration Options:
  Customizable Thresholds and Sampling: Fine-tune alarm
triggers using various configuration options, including
sampling rates, percentages, maximum and minimum
values, and statistical functions.
  Evaluation Periods: Select specific time periods over which
to evaluate alarm conditions, providing flexibility for short-
term or long-term analysis.
Example:
  Billing Alarm: Create an alarm on the CloudWatch Billing
metric to receive notifications when estimated charges
exceed a predefined threshold, aiding in proactive cost
management.
Benefits of CloudWatch Alarms:
  Proactive Response: Automate actions to address potential
issues or optimize resource usage before they impact system
performance or incur excessive costs.

  Enhanced Visibility: Gain timely insights into the state of
your AWS resources through notifications and automated
actions.
  Streamlined Operations: Reduce manual intervention and
improve operational efficiency through automated actions.
By effectively utilizing Amazon CloudWatch Alarms, you
can create a more responsive and cost-effective AWS
environment, ensuring optimal performance and resource
utilization.

2.3  AMAZON CLOUDWATCH LOGS
Amazon CloudWatch Logs are Efficient Log Aggregation and
Monitoring for Cloud-Based Resources, It serves as a
centralized logging solution within the AWS ecosystem,
enabling comprehensive collection, storage, and analysis of log
data from diverse sources. Its capabilities encompass:
  Extensive Log Ingestion: CloudWatch Logs seamlessly
collects logs from a multitude of AWS services, including:
o  Elastic Beanstalk: Monitor application behavior within
managed deployments.
o  Amazon ECS: Gain insights into containerized workloads
across your ECS clusters.
o  AWS Lambda: Capture and analyze function invocation logs
for troubleshooting and optimization.
o  CloudTrail (with filtering): Maintain granular control over
event history tracking.
o  CloudWatch Logs agents: Stream logs from EC2 instances
and on-premises servers.
o  Route 53: Analyze DNS query records for improved domain
management.

  Real-time Monitoring: Leverage real-time log streams for
immediate identification of issues and proactive problem
resolution.
  Flexible Retention: Tailor log retention periods to your
specific compliance and auditing requirements. This ensures
data availability as needed while optimizing storage costs.
CloudWatch Logs for EC2
•  By default, no logs from your EC2 instance will go to
CloudWatch
•  You need to run a CloudWatch agent on EC2 to push the log
files you want
•  Make sure IAM permissions are correct
•  The CloudWatch log agent can be setup on-premises too
Monitoring with Amazon CloudWatch:


Lab 2: Working with cloudwatch
Metrics, Logs and dashboard.
1.  Open the CloudWatch console
at https://console.aws.amazon.com/cloudwatch/.
2.  In the navigation pane, choose Metrics, All metrics.
3.  Select a metric namespace (for example, EC2 or Lambda).
4.  Select a metric dimension (for example, Per-Instance
Metrics or By Function Name).
5.  The Browse tab displays all metrics for that dimension in
the namespace. By each metric name is an information button
you can choose to see a popup with the metric definition.

6.  You can create alarm, play with dashboard and can view
logs and events related too AWS CloudWatch

3.  AWS CLOUDTRAIL
AWS CloudTrail is a managed
service by Amazon Web Services (AWS) that acts as an
Auditing and Monitoring tool. It keeps track of all user activity
and API calls happening within your AWS account. This
includes actions taken via the AWS Management Console,
CLI, SDKs, and APIs. Essentially, it records every "who",
"what", "when", and "where" of an action taken in your
account.
In AWS environment Cloud Trail is enabled by default. One
Can put logs from CloudTrail into CloudWatch Logs or S3. A
trail can be applied to All Regions (default) or a single Region
Meanwhile, in case of resource is deleted in AWS, we can
investigate it in CloudTrail first

3.1  CLOUDTRAIL WORKING
METHODOLOGY
CloudTrail works into four different activities:
Capture: Record activity in AWS services as AWS
CloudTrail events
Store: AWS CloudTrail delivers events to the AWS
CloudTrail console, Amazon S3 buckets, and optionally
Amazon CloudWatch Logs
Act: Use Amazon CloudWatch Alarms and Events to take
action when important events are detected.
Review: View recent events in the AWS CloudTrail console,
or analyze log files with Amazon Athena

3.2 AUDITING WITH AWS
CLOUDTRAIL, AWS AUDIT
MANAGER, AND AWS CONFIG
AWS Audit Manager is a service that helps you to audit your
AWS account for compliance with best practices and security
standards. Audit Manager provides a variety of pre-defined
audit rules that you can use to assess your AWS environment
for compliance with specific standards, such as PCI DSS and
HIPAA.
AWS Config is a service that provides a record of the
configuration of your AWS resources. Config records all
changes to the configuration of your AWS resources,
including the date and time of the change, the source of the
change, and the resources that were affected.
AWS CloudTrail, AWS Audit Manager, and AWS Config can
be used together to provide a comprehensive auditing
solution for your AWS environment. You can use CloudTrail to
collect a record of all activity in your AWS account, use Audit
Manager to assess your AWS environment for compliance
with best practices and security standards, and use Config to
track changes to the configuration of your AWS resources.

4.  MULTI-CHOICE QUESTIONS
AND ANSWER
1. What is the primary function of Amazon CloudWatch?
(a) To manage and configure virtual machines in the Amazon
EC2 service.
(b) To provide a secure and scalable object storage service.
(c) To monitor and track metrics, logs, and events from AWS
resources.
(d) To distribute static content across geographically diverse
locations.
Answer: (c)
Explanation: CloudWatch is a monitoring service that
collects and processes data like metrics, logs, and events
from various AWS resources, allowing you to track
performance, analyze trends, and set alarms for potential
issues.
2. What is the difference between CloudWatch metrics and
logs?
(a) Metrics are numerical values like CPU usage, while logs
are detailed textual information.
(b) There is no significant difference; they serve the same
purpose.

(c) Metrics represent historical data, while logs capture real-
time information.
(d) Metrics are only available for specific services, while logs
are universal.
Answer: (a)
Explanation: CloudWatch metrics provide numerical data
points collected at regular intervals, while logs offer detailed
event information with timestamps and textual descriptions.
3. What are CloudWatch alarms and how are they used?
(a) To schedule automated tasks based on specific events.
(b) To trigger notifications based on defined thresholds for
metrics or logs.
(c) To filter and analyze log data for specific patterns.
(d) To create custom dashboards for visualizing monitoring
data.
Answer: (b)
Explanation: CloudWatch alarms enable you to define
thresholds for metrics or logs and trigger notifications (e.g.,
emails, SNS messages) when those thresholds are breached,
alerting you to potential issues.

4. What is Amazon CloudFront and its benefit for web
applications?
(a) To provide managed database solutions for various
applications.
(b) To host and run virtual machines (VMs) in the cloud.
(c) To distribute static content from an S3 bucket across edge
locations for faster delivery.
(d) To monitor and track resource utilization and costs across
your AWS account.
Answer: (c)
Explanation: CloudFront acts as a content delivery network
(CDN), caching static content (images, videos, etc.) from your
S3 bucket in globally distributed edge locations, reducing
latency and improving website performance for users
worldwide.
5. What are CloudFront distributions and how are they
configured?
(a) Automatically created upon uploading content to an S3
bucket.
(b) Defined with settings like origin (S3 bucket), edge
locations, and security options.

(c) Managed by AWS and require no configuration from the
user.
(d) Used solely for distributing dynamic content generated by
web applications.
Answer: (b)
Explanation: CloudFront distributions require configuration,
specifying the origin (S3 bucket), desired edge locations,
security settings, and other options to optimize content
delivery.
6. What is the purpose of Amazon CloudTrail and what
information does it capture?
(a) To provide secure storage for sensitive data within the
AWS cloud.
(b) To monitor and optimize resource utilization for cost
savings.
(c) To record API calls made to AWS services, providing an
audit trail of user activity.
(d) To manage and automate the patching process for various
operating systems.
Answer: (c)

Explanation: CloudTrail acts as an audit log, capturing
information about API calls made to AWS services within your
account, including user identity, timestamps, resources
accessed, and API actions performed.
7. What are the benefits of using CloudTrail for security
and compliance?
(a) No direct security benefit; it primarily serves for resource
tracking.
(b) It helps detect unauthorized access and identify potential
security threats.
(c) It offers no compliance benefits, as data is stored within
the AWS cloud.
(d) It simplifies access control management by granting
granular permissions.
Answer: (b)
Explanation: By recording user activity and API calls,
CloudTrail provides valuable information for forensic analysis,
helping detect suspicious activity and potential security
breaches. It can also contribute to compliance efforts by
demonstrating auditability and adherence to specific
regulations.

8. How does CloudWatch integrate with CloudFront and
CloudTrail?
(a) CloudWatch can directly monitor CloudFront metrics and
CloudTrail logs.
(b) Integration requires custom scripting and manual
configuration.
(c) Integration is not possible; they serve entirely different
purposes.
(d) Integration requires additional paid services like Amazon
Kinesis.
Answer: (a)
Explanation: CloudWatch can collect and process metrics
from CloudFront distributions, such as cache hit rates and
request counts, and can also

AWS PRICING PLANS
The AWS pricing model is based on the principle of pay-as-
you-go. This means that you only pay for the resources you
use, for as long as you use them. There are no upfront
commitments or long-term contracts required.
AWS offers a wide range of resources, including compute,
storage, networking, databases, analytics, machine learning,
and artificial intelligence. All of these resources are available
on a pay-as-you-go basis.
Here are some of the most popular AWS resources and
their pricing models:
  Compute: Amazon Elastic Compute Cloud (EC2) instances
are the most popular compute resource in AWS. EC2
instances are available in a variety of sizes and instance
types, each with its own pricing model. For example, on-
demand EC2 instances are priced per hour, while reserved
instances are priced per year and offer significant discounts.
  Storage: Amazon Simple Storage Service (S3) is the most
popular storage resource in AWS. S3 is a highly scalable and
durable object storage service that is priced per GB per
month.
  Networking: Amazon Virtual Private Cloud (VPC) is the most
popular networking resource in AWS. VPC allows you to

create a secure and isolated network for your AWS resources.
VPC is priced per hour, with additional charges for certain
features, such as elastic IP addresses and load balancers.
  Databases: Amazon Relational Database Service (RDS) is
the most popular database resource in AWS. RDS is a
managed database service that supports a variety of
database engines, including MySQL, PostgreSQL, Oracle, and
SQL Server. RDS is priced per hour, with additional charges
for certain features, such as storage and backup.
  Analytics: Amazon Redshift is the most popular analytics
resource in AWS. Redshift is a fully managed data warehouse
service that is designed for large-scale data analysis. Redshift
is priced per hour, with additional charges for storage and
data transfer.
  Machine learning: Amazon SageMaker is the most popular
machine learning resource in AWS. SageMaker is a managed
platform for machine learning model development, training,
and deployment. SageMaker is priced per hour, with
additional charges for certain features, such as training time
and storage.
  Artificial intelligence: Amazon Rekognition is the most
popular artificial intelligence resource in AWS. Rekognition is
a service that can identify objects, people, scenes, and

events in images and videos. Rekognition is priced per image
or video, with additional charges for certain features, such as
custom model training.
AWS also offers a variety of pricing options for its
resources, such as on-demand, reserved instances, spot
instances, and savings plans. The best pricing option for you
will depend on your specific needs and usage patterns.

1.1.  COMPUTE PURCHASING
OPTION IN AWS
The various Compute purchasing options available in AWS are
following:
On-Demand Instances: On-demand instances are the most
flexible pricing option. You pay for the on-demand instances
you use, per hour or second. There is no upfront commitment
or long-term contract required. On-demand instances are a
good option for workloads that vary in usage or that you need
to be able to scale up or down quickly.
Reserved Instances: Reserved instances offer a significant
discount on the hourly rate of on-demand instances.
However, you must commit to a one- or three-year term
when you purchase a reserved instance. Reserved instances
are a good option for workloads with predictable usage
patterns.
Spot Instances: Spot instances are the least expensive
pricing option, but they are also the least predictable. Spot
instances are available when AWS has excess capacity. You
bid on the hourly rate you are willing to pay for a spot
instance, and if your bid is accepted, you are allocated an
instance. Spot instances can be terminated at any time if
AWS needs the capacity for other customers. Spot instances
are a good option for workloads that can tolerate
interruptions and can be preempted at any time.

1.2  SAVINGS PLANS
Savings plans offer a discount on the hourly rate of on-
demand instances, reserved instances, and spot instances.
However, you must commit to a monthly spend when you
purchase a savings plan. Savings plans are a good option for
workloads with predictable usage patterns and that you want
to commit to a monthly spend.
Dedicated Hosts: Dedicated hosts are physical servers that
are dedicated to your use. This gives you more control over
your hardware and environment, but it's also the most
expensive pricing option. Dedicated hosts are a good option
for workloads that require high performance, isolation, or
control.
Dedicated Instances: Dedicated instances are Amazon EC2
instances that are isolated from the general EC2 compute
pool. This provides greater security and isolation for your
workloads, but it's also more expensive than on-demand
instances. Dedicated instances are a good option for
workloads that require isolation or control.

1.3  CAPACITY RESERVATIONS
Capacity reservations are a way to reserve capacity for Amazon
EC2 instances in a specific Availability Zone. This guarantees
that you will have the capacity you need when you need it, even
if demand is high. Capacity reservations are a good option for
workloads that require capacity guarantees.
To choose the right compute purchasing option for you, you
should consider your specific needs and usage patterns. If you
need a flexible pricing option, then on-demand instances or
savings plans may be a good option for you. If you are looking
for the least expensive pricing option, then spot instances may
be a good option for you. If you need high performance,
isolation, or control, then dedicated hosts or dedicated instances
may be a good option for you. And if you need capacity
guarantees, then capacity reservations may be a good option for
you.
High availability in AWS
High availability in AWS is the ability of your application or
system to remain up and running, even if there is a failure in one
or more components. This is achieved by running your
application or system in multiple Availability Zones (AZs). AZs
are isolated from each other, so a failure in one AZ will not affect
the other AZs.
To achieve high availability in AWS, you can use a variety of
services and features, including:

  Load balancers: Load balancers distribute traffic across
multiple instances of your application or system. This ensures
that if one instance fails, the other instances will continue to
serve traffic.
  Auto Scaling: Auto Scaling automatically scales your
application or system up or down based on demand. This can
help to ensure that your application or system is always
available, even if there is a sudden spike in traffic.
  Route 53: Route 53 is a highly available DNS service that can
be used to route traffic to your application or system. Route 53
can automatically route traffic away from AZs that are
experiencing problems.
In addition to using AWS services and features, you can also
design your application or system to be highly available. This
includes using stateless components, avoiding single points of
failure, and implementing health checks.
Here is an example of a high availability architecture in AWS:
  You have a web application that is running in two AZs.
  You are using a load balancer to distribute traffic across the
two AZs.
  You are using Auto Scaling to ensure that there are always
enough instances of your application running to handle the
current traffic load.

  You are using Route 53 to route traffic to your load balancer.
If there is a failure in one AZ, the load balancer will continue
to route traffic to the other AZ. This ensures that your web
application will remain available to users.

CASE STUDY

AWS HIGHLY SCALABLE
APPLICATION ARCHITECTURE

Cloud computing offers several advantages over traditional on-
premises IT infrastructure, including high availability, scalability,
fault tolerance, and improved security. This case study delves
into how these key characteristics can be achieved for Amazon
Elastic Compute Cloud (EC2) instances through various AWS
services.
High Availability (HA):
  Redundancy Design: Eliminating single points of failure
(SPOFs) is crucial for achieving HA. Distribute critical
components across multiple Availability Zones (AZs) within a
region to ensure continuous service in case of localized outages.
  Load Balancers: Distribute incoming traffic evenly across
healthy EC2 instances within an Auto Scaling group using Load
Balancers. Application Load Balancers (ALBs) provide advanced
routing and health checks for more complex applications.
Scalability:
  Auto Scaling: Automatically adjust the number of EC2
instances based on predefined metrics (CPU, memory, network
traffic) using Auto Scaling. This ensures your resources adapt to
fluctuating demand, optimizing both cost and performance.
  Scaling Policies: Define thresholds and scaling actions (scaling
up or down) for different scenarios. Balance responsiveness to

traffic changes with cost-effectiveness. Consider step scaling for
gradual adjustments to minimize disruptions.
Fault Tolerance:
  Health Checks: Implement health checks at both the load
balancer and Auto Scaling levels. This allows for prompt
identification and termination of unhealthy instances, ensuring
service continuity.
  Instance Replacement: Configure Auto Scaling to automatically
launch new instances to replace unhealthy ones, minimizing
downtime and improving overall system resilience.
Performance Improvement:
  Optimized Instance Types: Choose EC2 instance types that
align with your application's specific resource requirements
(CPU, memory, network). This ensures optimal performance
without overpaying for resources you don't need.
Additional Strategies:
  Monitoring and Logging: Continuously monitor key metrics
(CPU, memory, network) and log application events. This
proactive approach helps identify potential bottlenecks and
issues before they impact users.

Auto Scaling groups and load balancers can be used together
to create a highly scalable and reliable application architecture.
When used together, Auto Scaling groups can automatically
scale the number of EC2 instances in a load balancer based on
demand. This helps to ensure that your applications are always
available and performant, even when traffic spikes.
Here is a diagram of how Auto Scaling groups and load
balancers work together:
1.  The load balancer distributes traffic across all of the EC2
instances in the Auto Scaling group.
2.  The Auto Scaling group monitors the health of all of the EC2
instances.

3.  If an EC2 instance fails, the Auto Scaling group launches a
new EC2 instance to replace it.
4.  The Auto Scaling group can also scale the number of EC2
instances in the load balancer up or down based on demand. For
example, if traffic to your application increases, the Auto Scaling
group can launch new EC2 instances to handle the additional
load. If traffic to your application decreases, the Auto Scaling
group can terminate EC2 instances to save costs.
To use Auto Scaling groups and load balancers together, you
need to:
1.  Create an Auto Scaling group.
2.  Create a load balancer.
3.  Attach the Auto Scaling group to the load balancer.
Once you have attached the Auto Scaling group to the load
balancer, the Auto Scaling group will
automatically manage the number of EC2 instances in the load
balancer based on demand.
Now in this exercise, I will be creating an autoscaling group to
launch instances that will have webserver installed on them into
three different public subnets using a load balancer to distribute
the traffic to the autoscaling group.

I this exercise, we are creating following services one by one
in sequential manner
1)  Create Launch Template – my-LT-Demo
2)  Create Target Group – my-TG-Demo
3)  Create Application Load Balancer – my-LB-Demo
4)  Create Autoscaling group – my-ASG-Demo
The user-data script used during the excise are given below
with explanation for better understanding.
#!/bin/bash
# Use this for your user data (script from top to bottom)
# install httpd
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html
Here's a breakdown or explanation of the user-data script,
line by line:
1. #!/bin/bash
  This line specifies that the script should be executed using the
Bash shell. It's a common way to indicate the interpreter for
scripts on Linux systems.
2. # Use this for your user data (script from top to bottom)

  This is a comment line, intended for human understanding. It
doesn't affect the script's execution, but it provides a helpful
note about the script's purpose.
3. # install httpd
  Another comment line, specifically indicating that the following
commands will install the Apache HTTP server software (httpd).
4. yum update -y
  This command updates the package lists and upgrades any
installed packages to their latest available versions. The -
y option automatically answers "yes" to any prompts during the
update process.
5. yum install -y httpd
  This command downloads and installs the Apache HTTP server
package using the yum package manager. The -y option again
automates any prompts.
6. systemctl start httpd
  This command starts the Apache HTTP server service, making
it active and ready to serve web pages.
7. systemctl enable httpd

  This command configures the Apache HTTP server service to
start automatically when the system boots up, ensuring it's
always running.
8. echo "<h1>Hello World from $(hostname -f)</h1>" >
/var/www/html/index.html
  This line creates a simple HTML file named index.html in the
default web document root (/var/www/html). The file contains
the text "Hello World from" followed by the server's fully
qualified hostname, enclosed in an H1 heading tag. This will be
the content displayed when a user accesses the web server's
root URL.
In summary, this user-data script performs the following
actions:
1.  Updates the system's packages.
2.  Installs the Apache HTTP server.
3.  Starts the Apache HTTP server.
4.  Enables the Apache HTTP server to start automatically at
boot.
5.  Creates a basic "Hello World" web page that displays the
server's hostname.

Step 1. Create an EC2 Launch template my-LT-Demo and
provide Template version description.
Step 2. Select Quick Start and Amazon Linux 2023 AMI with
64-bit Architecture.

Step 3. Select existing Security Group launch-wizard-1
having SSH, http and https port enable from anywhere
(0.0.0.0/0)


Step 4. Keep default setting as per above picture related to
Storage(volume) and Resource tags. Enable Advance Details
section and keep all relevant section values are at default level
except user-data (Optional)

Step 5. Add below script in user-data section and click on
crate launch template.
#!/bin/bash
# Use this for your user data (script from top to bottom)
# install httpd
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html

Step 6.  Create a my-TG-Demo Target group and select the
different available option for given section:
Basic Configuration: Instances
IP Address Type: IP4
Protocol Version: HTTP1 and click on Create Target Group.
This will create my-TG-Demo target group.




Step 7.  Create a my-LB-Demo Load Balancer and select the
different available option for given section:
Schem: Internet Facing
IP Address Type: IP4
Step 8:  Create an Application Load Balancer my-LB-Demo
to distribute traffic to the autoscaling group. and select the
different available option for given section:
Schem: Internet Facing
IP Address Type: IP4
VPC: Default
Mapping: Three Availability Zone ap-southeast-1a, ap-
southeast-1b and ap-southeast-1c

Step 9:  Select existing security Group Demo-LB-SG and
target type my-TG-Demo and click on Create Load Balancer.
This Load Balancer security group Demo-LB-SG, we need to
Create in such a way that it allows inbound traffic from HTTP
from 0.0.0.0/0.



Snapshot of Load Balancer
Step 10:  Create an autoscaling group my-ASG-Demo using
t2.micro instances. Some of important option
Launch Template: my-LT-Demo
vCPU: 1(Min), 1(Max)
Memory (GiB): 1(Min), 2(Max)
VPC: Default
Availability Zone: ap-southeast-1a, ap-southeast-1b and
ap-southeast-1c




Step 11:  Some other options while creating ASG are:
Load Balancing: Attach to an existing load balancer
Existing Load Balancer Target Group: my-TG-Demo:
HTTP
Turn On Elastic Load Balancing Health Checks
Health Check Grace Period: 300 seconds
Enable group matrices collection within cloud watch


Step 12:  Configure group size and scaling
Desired capacity: 2
Min desired capacity: 2
Max desired capacity: 4

Step 13:  Configure Automatic scaling – optional
Choose whether to use a target tracking policy: Target
tracking scaling policy
Scaling policy name: Target Tracking Policy
Metric type: Average CPU Utilization
Target Value: 75
Instance maintenance policy: No Policy
Add notifications: None
Add tags: None



Step 14:  Final Review will come on screen. You can click on
Create Auto Scaling Group.





This will create auto scaling group with the name of my-ASG-
Demo.
Snapshot of AutoScaling Group

Step 15: Now we are having our final test and running status. Go
to load balancer section
You can find your load balancer DNS name marked in red:
http://my-LB-Demo-1272863994.ap-southeast-
1.elb.amazonaws.com
You can use given DNS name in your web browser and find
that final message related to “Hello World” toggle between two
different values due to existence of load balancer. Load Balancer
is distributing the work into two different instances as per
workload.
Hello World from ip-172-31-3-82.ap-southeast-
1.compute.internal

Hello World from ip-172-31-21-111.ap-southeast-
1.compute.internal

Cloud Watch Monitoring Screen For reference

QUICK REFERENCES
  Access control list (ACL): A firewall/security layer on the
subnet level of VPC
   Auto scaling:  Automates the process of adding or
removing EC2 instances based on traffic demand for your
application
 
 
Buckets: 
Root-level 
"folders" 
in 
Simple 
Storage
Service(S3)
  CloudFront:  Content delivery network (CDN) that allows
you to store your content at "edge locations" located all
around the world, allowing customers to access your content
more quickly
   CloudTrail:  Audit Log. Allows you to monitor all actions
taken by IAM users
  CloudWatch: Service that allows you to monitor various
elements of your AWS account
  Consolidated billing:  Allows you to view, manage, and
pay bills for multiple AWS accounts in one user interface
   DNS server:  A database of website domains and their
corresponding IP addresses

   DynamoDB:  NoSQL database service that does not
provide other NoSQL software options
 Edge Locations: Edge locations in AWS are geographically
distributed mini data centers    that bring services closer to
users for lower latency and faster content delivery.
   Elastic Block Store (EBS):  Provides persistent block
storage volumes for use of EC2 instances.
  Elastic Compute Cloud (EC2): A virtual computer, very
similar to a desktop/laptop computer
  Elastic Load Balancing (ELB): Evenly distributes traffic
between EC2 instances that are associated with it
  ElastiCache: Data caching service used to help improve
the speed/performance of web applications running on AWS
   Elasticity:  The ability of a system to increase and
decrease in size
   Fault tolerance:  Property that enables a system to
continue operating properly in the event of the failure of one
or more components
  Firewall:  A type of software that either allows or blocks
certain kinds of internet traffic to pass through it

  High availability: Refers to systems that are durable and
likely to operate continuously without failure for a long time
  IAM users: Individuals who have been granted access to
an AWS account
   Identity and Access Management (IAM):  Service
where AWS user accounts and their access to various AWS
services are managed
   Lambda:  Serverless computing that will replace EC2
instances, for the most part
  Object availability: Percent over a one-year time period
that a file stored in S3 will be accessible
   Object durability:  Percent over a one-year time period
that a file stored in S3 will not be lost
   Object lifecycle:  Set rules to automatically transfer
objects between storage classes at defined time intervals
   Object sharing:  Ability to make any object publicly
available via a URL link
  Object versioning: Automatically keep multiple versions
of an object (when enabled)

   Relational Database Service (RDS):  SQL database
service that provides a wide range of SQL database options to
select from
  Regional Edge Cache: Regional Edge Cache in AWS are
supersized storage caches closer to users, keeping less
popular content available longer for improved performance
and reduced origin load.
  RedShift: Data warehouse database service designed to
handle petabytes of data for analysis
  Roles: How different AWS services are granted permission
to communicate and share data
  Route 53: Where you configure and manage web domains
for websites or applications you host on AWS
  Scalability: The ability of a system to easily increase in
size and capacity in a cost-effective way
   Security group (SG):  Firewall/security layer on the
server/instance level
   Shared responsibility model:  Defines what you and
AWS are responsible for when it comes to security and
compliance

   Simple Storage Service (S3):  Online bulk storage
service you can access from almost any device
   Storage class:  Represents "classification" assigned to
each object in S3 (standard, RRS, S3-IA, Glacier)
  Subnet: A subsection of a network and generally includes
all the computers in a specific location
  User credentials: IAM user's username and password for
logging in to AWS
   Virtual Private Cloud (VPC):  A private subsection of
AWS you control and in which you can place AWS resources


IMPORTANT AWS LINKS
Getting
Started
Amazon
Web Services
Website
https://aws.amazon.com/
AWS
Documentation
https://docs.aws.amazon.com/
AWS Free
Tier
https://aws.amazon.com/free/
AWS
Management
Console
https://aws.amazon.com/console/
Compute
Amazon EC2
(Virtual
Machines)
https://aws.amazon.com/ec2/
Amazon
Lambda
(Serverless
Functions)
https://aws.amazon.com/lambda/
Amazon EKS
(Kubernetes
Service)
https://aws.amazon.com/eks/
Storage
Amazon S3
(Object
Storage)
https://aws.amazon.com/s3/
Amazon EBS
(Block
Storage)
https://aws.amazon.com/ebs/
Amazon FSx
https://aws.amazon.com/efs/

(File Storage)
Databases
Amazon RDS
(Relational
Database
Service)
https://aws.amazon.com/rds/
Amazon
DynamoDB
(NoSQL
Database)
https://aws.amazon.com/dynamodb/
Amazon
Aurora (MySQL
& PostgreSQL
Compatible)
https://aws.amazon.com/rds/aurora/
Networking
Amazon VPC
(Virtual Private
Cloud)
https://aws.amazon.com/vpc/
Amazon
Route 53
(Domain Name
System)
https://aws.amazon.com/route53/
Amazon
CloudFront
(Content
Delivery
Network)
https://aws.amazon.com/cloudfront/
Security
AWS
Identity and
Access
Management
(IAM)
https://aws.amazon.com/iam/

AWS
CloudWatch
(Monitoring &
Logging)
https://aws.amazon.com/cloudwatch/
Management
& Monitoring
AWS Cost
Explorer (Cost
Management)
https://docs.aws.amazon.com/cost-
management/latest/userguide/ce-what-
is.html
Additional
Resources
AWS Blog
https://aws.amazon.com/blogs/
AWS
Training &
Certification
https://aws.amazon.com/training/

DIFFERENT AWS SERVICE LOGO
FOR REFERENCE
Demystifying AWS: Unleash the Power of the Cloud    Page|



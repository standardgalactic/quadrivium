
3D Printing:
Breakthroughs in Research and 
Practice
Information Resources Management Association
USA

Published in the United States of America by
IGI Global
Engineering Science Reference (an imprint of IGI Global)
701 E. Chocolate Avenue
Hershey PA, USA 17033
Tel: 717-533-8845
Fax:  717-533-8661 
E-mail: cust@igi-global.com
Web site: http://www.igi-global.com
Copyright © 2017 by IGI Global.  All rights reserved. No part of this publication may be reproduced, stored or distributed in 
any form or by any means, electronic or mechanical, including photocopying, without written permission from the publisher.
Product or company names used in this set are for identification purposes only. Inclusion of the names of the products or 
companies does not indicate a claim of ownership by IGI Global of the trademark or registered trademark.
	
	
	
Library of Congress Cataloging-in-Publication Data
British Cataloguing in Publication Data
A Cataloguing in Publication record for this book is available from the British Library.
All work contributed to this book is new, previously-unpublished material. The views expressed in this book are those of the 
authors, but not necessarily of the publisher.
For electronic access to this publication, please contact: eresources@igi-global.com.
Names: Information Resources Management Association, editor.
Title: 3D printing : breakthroughs in research and practice / Information  
   Resources Management Association, editor. 
Description: Hershey, PA : Engineering Science Reference, [2017] | Includes  
   bibliographical references and index. 
Identifiers: LCCN 2016037613| ISBN 9781522516774 (hardcover) | ISBN  
   9781522516781 (ebook) 
Subjects:  LCSH: Three-dimensional printing. 
Classification: LCC TS171.95 .A16 2017 | DDC 621.9/88--dc23 LC record available at https://lccn.loc.gov/2016037613 

﻿
List of Contributors
﻿
Akinlabi, Esther T. / University of Johannesburg, South Africa....................................................... 154
Altintas, Pelin / Yıldız Technical University, Turkey........................................................................... 241
Andreani, Stefano / Harvard University, USA................................................................................... 361
Bianconi, Fabio / Università degli Studi di Perugia, Italy.................................................................. 361
Billen, Roland / University of Liège, Belgium.................................................................................... 296
Blum, Rainer / Hochschule Fulda – University of Applied Sciences, Germany................................ 396
Bomsdorf, Birgit / Hochschule Fulda – University of Applied Sciences, Germany........................... 396
Boschetto, Alberto / University of Rome “La Sapienza”, Italy............................................................ 22
Bottini, Luana / University of Rome “La Sapienza”, Italy................................................................... 22
Brusaporci, Stefano / L’Aquila University, Italy................................................................................ 333
Cak, Tugba / Yıldız Technical University, Turkey............................................................................... 241
Cutting-Decelle, Anne-Françoise / Lille University of Science and Technology, France & 
University of Geneva, Switzerland................................................................................................ 296
da Silva Borda, Adriane Almeida / Universidade Federal de Pelotas, Brazil.................................. 204
Da Silva, Matthieu Perreira / University of Nantes, France............................................................... 75
Damianakis, Adam / Conceptum S.A., Greece.................................................................................... 57
El-Mekawy, Mohamed Sobaih Aly / Stockholm University, Sweden................................................ 119
Erisir, Zehra / Yıldız Technical University, Turkey............................................................................ 241
Falquet, Gilles / University of Geneva, Switzerland........................................................................... 296
Filippucci, Marco / Università degli Studi di Perugia, Italy.............................................................. 361
Goldberg, Moshe / Deakin University, Australia............................................................................... 228
Grubbs, W. Tandy / Stetson University, USA.................................................................................... 306
Harman, Graham / American University in Cairo, Egypt................................................................... 48
Kallonis, Pavlos / Conceptum S.A., Greece.......................................................................................... 57
Kotthaus, Christoph / University of Siegen, Germany...................................................................... 282
Künkel, Daniel / Hochschule Fulda – University of Applied Sciences, Germany.............................. 396
Le Callet, Patrick / University of Nantes, France................................................................................ 75
Littlefair, Guy / Deakin University, Australia.................................................................................... 228
Loudos, George / TEI of Athens, Greece.............................................................................................. 57
Ludwig, Thomas / University of Siegen, Germany............................................................................. 282
Mahamood, Rasheedat M. / University of Johannesburg, South Africa & University of Ilorin, 
Nigeria............................................................................................................................. 154,172,183
Marina, Ognen / Saints Cyril and Methodius University of Skopje, Macedonia............................... 296
Métral, Claudine / University of Geneva, Switzerland....................................................................... 296
Mitev, Tihomir / Plovdiv University “Paisii Hilendarski”, Bulgaria................................................ 145

﻿
Narayana, K. L. / KLEF University, India............................................................................................. 1
Paasch, Jesper M. / Lantmäteriet, Sweden......................................................................................... 119
Paulsson, Jenny / KTH Royal Institute of Technology, Sweden.......................................................... 119
Pipek, Volkmar / University of Siegen, Germany............................................................................... 282
Pires, Janice de Freitas / Universidade Federal de Pelotas, Brazil................................................... 204
Pityana, Sisa / National Laser Centre, South Africa.......................................................................... 183
Polishetty, Ashwin / Deakin University, Australia............................................................................. 228
Ricordel, Vincent / University of Nantes, France................................................................................. 75
Ross, Frances / London College of Fashion, UK................................................................................ 251
Ryan, Susan M. / Stetson University, USA......................................................................................... 306
Shukla, Mukul / University of Johannesburg – Doornfontein, South Africa & MNNIT Allahabad, 
India............................................................................................................................................... 183
Shunmugavel, Manikandakumar / Deakin University, Australia.................................................... 228
Singh, Rajkumar Prasad / Kalyani Center for Technology and Innovation, India........................... 228
Suresh, Ganzi / KLEF University, India................................................................................................. 1
Tsatsos, Dimitris / Conceptum S.A., Greece......................................................................................... 57
Tsoukalis, Antonios / TEI of Athens, Greece....................................................................................... 57
Vecchia, Luisa Dalla / Universidade Federal de Pelotas, Brazil........................................................ 204
Wang, Junle / University of Nantes, France......................................................................................... 75
Yastikli, Naci / Yıldız Technical University, Turkey............................................................................ 241
Zlatanova, Sisi / Delft University of Technology, The Netherlands.................................................... 296

﻿
Table of Contents
﻿
Preface..................................................................................................................................................... x
Section 1
Fundamental Concepts and Design Methodologies
This section serves as a foundation for this exhaustive reference tool by addressing underlying principles essential to 
the understanding of 3D Printing and provides in-depth coverage of conceptual architecture frameworks to provide 
the reader with a comprehensive understanding of the emerging developments within the field of 3D Printing. With 
7 chapters comprising this foundational section, the reader can learn and chose from a compendium of expert 
research on the elemental theories underscoring the 3D Printing discipline. Research fundamentals imperative to the 
understanding of developmental processes within 3D Printing are offered. Chapters found within these pages provide 
an excellent framework in which to position 3D Printing within the field of information science and technology. 
From broad examinations to specific discussions on methodology, the research found within this section spans the 
discipline while offering detailed, specific discussions. From basic designs to abstract development, these chapters 
serve to expand the reaches of development and design technologies within the 3D Printing community. Insight 
regarding the critical incorporation of global measures into 3D Printing is addressed, while crucial stumbling 
blocks of this field are explored.
Chapter 1
A Review on Fabricating Procedures in Rapid Prototyping.................................................................... 1
Ganzi Suresh, KLEF University, India
K. L. Narayana, KLEF University, India
Chapter 2
Surface Characterization in Fused Deposition Modeling...................................................................... 22
Alberto Boschetto, University of Rome “La Sapienza”, Italy
Luana Bottini, University of Rome “La Sapienza”, Italy
Chapter 3
3D Printing and Actor-Network Theory................................................................................................ 48
Graham Harman, American University in Cairo, Egypt

﻿
Chapter 4
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic 
Courses: Design Methodology and Implementation in a Small Scale National Project........................ 57
Adam Damianakis, Conceptum S.A., Greece
Pavlos Kallonis, Conceptum S.A., Greece
George Loudos, TEI of Athens, Greece
Dimitris Tsatsos, Conceptum S.A., Greece
Antonios Tsoukalis, TEI of Athens, Greece
Chapter 5
2D and 3D Visual Attention for Computer Vision: Concepts, Measurement, and Modeling............... 75
Vincent Ricordel, University of Nantes, France
Junle Wang, University of Nantes, France
Matthieu Perreira Da Silva, University of Nantes, France
Patrick Le Callet, University of Nantes, France
Chapter 6
Integration of Legal Aspects in 3D Cadastral Systems........................................................................ 119
Mohamed Sobaih Aly El-Mekawy, Stockholm University, Sweden
Jesper M. Paasch, Lantmäteriet, Sweden
Jenny Paulsson, KTH Royal Institute of Technology, Sweden
Chapter 7
Where Is the Missing Matter? A Comment on “The Essence” of Additive Manufacturing............... 145
Tihomir Mitev, Plovdiv University “Paisii Hilendarski”, Bulgaria
Section 2
Technologies and Applications
This section presents an extensive coverage of various tools and technologies available in the field of 3D Printing 
and discusses a variety of applications and opportunities available that can be considered by practitioners in 
developing viable and effective 3D Printing programs and processes. With 7 chapters, this section offers a broad 
treatment of some of the many tools and technologies within the 3D Printing field. These chapters enlighten 
readers about fundamental research on the many tools facilitating the burgeoning field of 3D Printing and review 
topics from case studies to best practices and ongoing research. Further chapters discuss 3D Printing in a variety 
of settings. Contributions included in this section provide excellent coverage of today’s IT community and how 
research into 3D Printing is impacting the social fabric of our present-day global village. It is through these 
rigorously researched chapters that the reader is provided with countless examples of the up-and-coming tools 
and technologies emerging from the field of 3D Printing.
Chapter 8
Laser Additive Manufacturing............................................................................................................. 154
Rasheedat M. Mahamood, University of Johannesburg, South Africa & University of Ilorin, 
Nigeria
Esther T. Akinlabi, University of Johannesburg, South Africa

﻿
Chapter 9
Laser Metal Deposition Process........................................................................................................... 172
Rasheedat M. Mahamood, University of Johannesburg, South Africa & University of Ilorin, 
Nigeria
Chapter 10
Laser Additive Manufacturing in Surface Modification of Metals...................................................... 183
Rasheedat M. Mahamood, University of Johannesburg, South Africa & University of Ilorin, 
Nigeria
Mukul Shukla, University of Johannesburg – Doornfontein, South Africa & MNNIT 
Allahabad, India
Sisa Pityana, National Laser Centre, South Africa
Chapter 11
Transiting between Representation Technologies and Teaching/Learning Descriptive Geometry: 
Reflections in an Architectural Context............................................................................................... 204
Janice de Freitas Pires, Universidade Federal de Pelotas, Brazil
Luisa Dalla Vecchia, Universidade Federal de Pelotas, Brazil
Adriane Almeida da Silva Borda, Universidade Federal de Pelotas, Brazil
Chapter 12
Tool Wear and Surface Integrity Analysis of Machined Heat Treated Selective Laser Melted Ti-
6Al-4V................................................................................................................................................. 228
Manikandakumar Shunmugavel, Deakin University, Australia
Ashwin Polishetty, Deakin University, Australia
Moshe Goldberg, Deakin University, Australia
Rajkumar Prasad Singh, Kalyani Center for Technology and Innovation, India
Guy Littlefair, Deakin University, Australia
Chapter 13
The Use of the Terrestrial Photogrammetry in Reverse Engineering Applications............................ 241
Naci Yastikli, Yıldız Technical University, Turkey
Zehra Erisir, Yıldız Technical University, Turkey
Pelin Altintas, Yıldız Technical University, Turkey
Tugba Cak, Yıldız Technical University, Turkey
Chapter 14
Fashion-Technology and Change in Product Development and Consumption for the High-End 
Menswear Sector: A Study Utilizing a 3D-4C’s Process Model......................................................... 251
Frances Ross, London College of Fashion, UK

﻿
Section 3
Critical Issues and Emerging Trends
This section contains 6 chapters, giving a wide variety of perspectives and advancement of the discipline on 3D 
Printing. Within the chapters, the reader is presented with an in-depth analysis of the most current and relevant 
issues within this growing field of study. Crucial questions are addressed and alternatives offered along with 
theoretical approaches discussed. The section also introduces future research directions and topical suggestions 
for continued debate, centering on the new venues and forums for discussion. In all, this text will serve as a vital 
resource to practitioners and academics interested in the best practices and applications of the burgeoning field 
of 3D Printing.
Chapter 15
Should I Try Turning It Off and On Again? Outlining HCI Challenges for Cyber-Physical 
Production Systems.............................................................................................................................. 282
Thomas Ludwig, University of Siegen, Germany
Christoph Kotthaus, University of Siegen, Germany
Volkmar Pipek, University of Siegen, Germany
Chapter 16
Challenges of Semantic 3D City Models: A Contribution of the COST Research Action TU0801... 296
Roland Billen, University of Liège, Belgium
Anne-Françoise Cutting-Decelle, Lille University of Science and Technology, France & 
University of Geneva, Switzerland
Claudine Métral, University of Geneva, Switzerland
Gilles Falquet, University of Geneva, Switzerland
Sisi Zlatanova, Delft University of Technology, The Netherlands
Ognen Marina, Saints Cyril and Methodius University of Skopje, Macedonia
Chapter 17
Curricular Collaborations: Using Emerging Technologies to Foster Innovative Partnerships............ 306
Susan M. Ryan, Stetson University, USA
W. Tandy Grubbs, Stetson University, USA
Chapter 18
The Importance of Being Honest: Issues of Transparency in Digital Visualization of Architectural 
Heritage................................................................................................................................................ 333
Stefano Brusaporci, L’Aquila University, Italy
Chapter 19
Computational Design and Built Environments: The Quest for an Alternative Role of the Digital 
in Architecture..................................................................................................................................... 361
Marco Filippucci, Università degli Studi di Perugia, Italy
Fabio Bianconi, Università degli Studi di Perugia, Italy
Stefano Andreani, Harvard University, USA

﻿
Chapter 20
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction...................... 396
Birgit Bomsdorf, Hochschule Fulda – University of Applied Sciences, Germany
Rainer Blum, Hochschule Fulda – University of Applied Sciences, Germany
Daniel Künkel, Hochschule Fulda – University of Applied Sciences, Germany
Index.................................................................................................................................................... 414

﻿
Preface
﻿
The constantly changing landscape of 3D Printing makes it challenging for experts and practitioners 
to stay informed of the field’s most up-to-date research. That is why Engineering Science Reference is 
pleased to offer this single-volume reference publication that will empower students, researchers, and 
academicians with a strong understanding of critical issues within 3D Printing by providing both broad 
and detailed perspectives on cutting-edge theories and developments. This publication is designed to 
act as a reference source on conceptual, methodological, and technical, as well as provide insight into 
emerging trends and future opportunities within the discipline.
3D Printing: Breakthroughs in Research and Practice is organized into three sections that provide 
comprehensive coverage of important topics. The sections are:
1. 	
Fundamental Concepts and Design Methodologies;
2. 	
Technologies and Applications;
3. 	
Critical Issues and Emerging Trends;
The following paragraphs provide a summary of what to expect from this invaluable reference tool.
Section 1, “Fundamental Concepts and Design Methodologies,” serves as a foundation for this ex-
tensive reference tool by addressing crucial theories and presents in-depth coverage of the conceptual 
design and architecture essential to the understanding of 3D Printing. Introducing the book is A Review 
on Fabricating Procedures in Rapid Prototyping by Ganzi Suresh and K. L. Narayana; a great founda-
tion laying the groundwork for the basic concepts and theories that will be discussed throughout the rest 
of the book. Through case studies, this section lays excellent groundwork for later sections that will get 
into present and future applications for 3D Printing. The section concludes, and leads into the follow-
ing portion of the book with a nice segue chapter, Where is the Missing Matter? A Comment on “The 
Essence” of Additive Manufacturing by Tihomir Mitev.
Section 2, “Technologies and Applications,” presents extensive coverage of the various tools and 
technologies used along with describing how the broad range of 3D Printing efforts has been utilized 
and offers insight on and important lessons for their applications and impact. The first chapter Laser Ad-
ditive Manufacturing by Rasheedat Modupe Mahamood and Esther Titilayo Akinlabi, lays a framework 
for the types of works that can be found in this section. This section includes the widest range of topics 
because it describes tools at place in the modeling, planning, and applications along with case studies, 
research, methodologies, frameworks, architectures, theory, analysis, and guides for implementation of 
3D Printing. We conclude the section with Fashion-Technology and Change in Product Development 
and Consumption for the High-end Menswear Sector: A Study Utilizing a 3D-4C’s Process Model by 
Frances Ross a well-rounded transitional chapter into the next section.
x

Preface
Section 3, “Critical Issues and Emerging Trends,” discusses coverage of academic and research per-
spectives on 3D Printing tools and applications and highlights areas for future research within the field 
of 3D Printing, opening with Should I Try Turning It Off and On Again? Outlining HCI Challenges for 
Cyber-Physical Production Systems by Thomas Ludwig, Christoph Kotthaus, and Volkmar Pipek. This 
section contains chapters that look into theoretical approaches and offer alternatives to crucial questions 
on the subject of 3D Printing and look at what might happen in the coming years that can extend the 
already staggering amount of applications for 3D Printing. The final chapter of the book looks at an 
emerging field within 3D Printing, in the excellent contribution, Towards ProGesture, a Tool Support-
ing Early Prototyping of 3D-Gesture Interaction by Birgit Bomsdorf, Rainer Blum, and Daniel Künkel.
Although the primary organization of the contents in this work is based on its three sections, offer-
ing a progression of coverage of the important concepts, methodologies, technologies, applications, 
and emerging trends, the reader can also identify specific contents by utilizing the extensive indexing 
system listed at the end. As a comprehensive collection of research on the latest findings related to us-
ing technology to providing various services, 3D Printing: Breakthroughs in Research and Practice, 
provides researchers, administrators and all audiences with a complete understanding of the development 
of applications and concepts in 3D Printing. Given the vast number of issues concerning usage, failure, 
success, policies, strategies, and applications of 3D Printing in countries around the world, 3D Printing: 
Breakthroughs in Research and Practice addresses the demand for a resource that encompasses the most 
pertinent research in technologies being employed to globally bolster the knowledge and applications 
of 3D Printing.
xi

Section 1
This section serves as a foundation for this exhaustive reference tool by addressing underlying principles 
essential to the understanding of 3D Printing and provides in-depth coverage of conceptual architecture 
frameworks to provide the reader with a comprehensive understanding of the emerging developments 
within the field of 3D Printing. With 7 chapters comprising this foundational section, the reader can learn 
and chose from a compendium of expert research on the elemental theories underscoring the 3D Printing 
discipline. Research fundamentals imperative to the understanding of developmental processes within 
3D Printing are offered. Chapters found within these pages provide an excellent framework in which to 
position 3D Printing within the field of information science and technology. From broad examinations 
to specific discussions on methodology, the research found within this section spans the discipline while 
offering detailed, specific discussions. From basic designs to abstract development, these chapters serve 
to expand the reaches of development and design technologies within the 3D Printing community. Insight 
regarding the critical incorporation of global measures into 3D Printing is addressed, while crucial 
stumbling blocks of this field are explored.
Fundamental Concepts and 
Design Methodologies

1
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  1
DOI: 10.4018/978-1-5225-1677-4.ch001
ABSTRACT
Rapid prototyping (RP) advancements are in light of the rule of making three-dimensional geometries 
straightforwardly from computer aided design (CAD) by stacking two-dimensional profiles on top of 
one another. Rapid manufacturing (RM) is the utilization of rapid prototyping advancements to make 
end-utilize or completed items. Aside from the ordinary assembling methods which are utilized for quite 
a while assembling of an item, added substance assembling methodologies have picked up force in 
the late years. The explanation for this is that these techniques don’t oblige extraordinary tooling and 
don’t evacuate material which is exceptionally advantageous really taking shape of a segment. Rapid 
manufacturing is the developing innovation in assembling commercial ventures with a specific end goal 
to create the model inside the less time and expense effective. In this paper we talked about a portion 
of the fast assembling advancements in light of the sort of crude material is utilized for the procedures, 
applications, preferences and limits.
INTRODUCTION
The rapid prototyping (RP) industry is comprised of a progression of innovations containing computerized 
methods that can rapidly manufacture any given three-dimensional article with the end goal of testing 
the structure, fit, and capacity of a configuration (Billiet, Vandenhaute et al. 2012). As a layer-based 
added substance producing system, Rapid Prototyping gives an architect the ability to manufacture any 
possible geometry.
Rapid prototyping forms, when all is said in done, start with a three-dimensional computer aided 
model of the part to be made. This computerized representation of the part is cut into virtual layers by 
computer aided design (CAD). Every layer, speaking to a cross-segment of the sought part, is sent to 
A Review on Fabricating 
Procedures in Rapid Prototyping
Ganzi Suresh
KLEF University, India
K. L. Narayana
KLEF University, India

2
A Review on Fabricating Procedures in Rapid Prototyping
﻿
the rapid prototyping machine where it is based upon the past layer (Peltola). This methodology, as-
sembling the part layer-by-layer starting from the earliest stage, is rehashed until the part is finished as 
indicated in Figure 1.
Rapid prototyping frameworks can deliver models from 3D CAD information, CT and MRI checks, 
and 3D digitizing frameworks (Trevor Boehm). Utilizing an added substance approach, rapid prototyping 
frameworks join fluid, powder or sheet materials to shape physical protests on a layer by layer premise. 
Rapid prototyping machines process plastic, paper, artistic, metal and composite materials from flimsy, 
level cross areas of computer aided models (Lan, 2009).
This report gives a far reaching diagram of rapid prototyping techniques grouped into three principle 
gatherings in light of the kind of material used to manufacture a 3D model (Table 1):
•	
Liquid based procedure;
•	
Powder based procedure;
•	
Solid/ fibre/ foil based methodology.
Prototyping frameworks normally work untended, and upon consummation, the created models can 
oblige some post-operations (Billiet & Vandenhaute et al., 2012). These post transforming operations 
incorporates surface completing and bolster evacuation. Altogether, notwithstanding, the expense of 
model demonstrating is extraordinarily diminished from more routine model shop manufactures. Albeit 
fast prototyping has gotten to be imbued into the item advancement transform far and wide (Mahendru, 
Figure 1. Additive fabrication process of rapid prototyping
Table 1. Classifications of rapid manufacturing processes
Rapid Manufacturing Technologies
Liquid Based Process
Powder Based Process
Solid/Foil Based Process
Stereolithography 
Jetting System 
Direct Light Processing Technology 
High Viscosity Jetting 
The MAPLE Process
Selective laser sintering 
--Polymers 
--Ceramics & metals 
Direct Metal Laser Sintering 
Three-Dimensional Printing 
Fused Metal Deposition System 
Electron Beam Melting 
Selective Laser Melting 
Selective Masking Sintering 
Selective Inhibitation Sintering 
Electro-Photographic Layered Manufacturing 
High Speed Sintering
Fused Deposition Modeling Laminated 
Object Manufacturing or Sheet Stacking 
Technology

3
A Review on Fabricating Procedures in Rapid Prototyping
﻿
2013), it appears that development is more probable than coming to full development. The business 
discovers itself deluged in new methods, materials, and frameworks that will inevitably prompt open up 
new markets, described by new clients and novel applications.
LIQUID BASED PROCESS
Stereo-Lithography (SLA)
Stereolithography is an enclosed substance amassing or 3 dimensional printing developments used for 
convincing models, models(Nagarjan Tukuru* 2008), illustrations and creation components up one layer 
instantly by action a photograph responsive leave with an ultraviolet light optical device or another equiva-
lent power supply. The expression “stereolithography” was begin in 1986 by Charles (Chuck) W. Body.
Figure 2 represents stereolithography machine, Stereolithography is an included substance collecting 
system which uses a vat of liquid brilliant reparable photopolymer “tar” and a splendid laser to develop 
parts’ layers every one thusly. For every layer, the optical device bar takes once a transversally of the 
0.5 found out on the surface of the fluid pitch. Introduction to the wealthy optical optical device light-
weight cures and sets the case took once on the tar and obliges it to the layer beneath. After the case 
has been taken once, the stereolithography convey stage dives by a division similar to the thickness of 
a specific layer, faithfully 0.5 millimetres to 0.15 mm (D.T. Pham, 1997). By then, a sap stuffed harm 
edge clears over the cross scene of the re-covering it with late material. On this new fluid surface (Wong 
& Hernandez, 2012), the related to layer case is taken when; adjustment of reputability the past layer. 
An end three-dimensional half is formed by this technique (Vai 2010). Within the wake of being made, 
components area unit inundated during a substance shower keeping in mind the tip goal to be clean of 
abundance pitch and area unit during this means cured during a bright stove.
Figure 2. Schematic of stereolithography process

4
A Review on Fabricating Procedures in Rapid Prototyping
﻿
Stereolithography obliges the employment of supporting structures that serve to affix the half to the 
elevate stage, avoid avoidance because of gravity and hold the cross segments set up with the goal that 
they oppose horizontal weight from the re-coater edge (C. K. Chua, 1998, Melchels & Feijen et al., 2010). 
Backings are produced naturally amid the arrangement of 3Dimensional computer aided design models 
for utilization on the stereolithography machine, despite the fact that they may be controlled physically. 
Underpins must be expelled from the completed item physically (Kumar & Kruth, 2010), not at all like 
in other, less extreme, quick prototyping developments. Figure 2 adapted from (Neil Hopkinson 2005).
Jetting System
One representative mix based completely fluid framework is Poly-Jet, a 0.5 kind of fabric spouting or 
printing. As coordinator’s scene unit unpleasantly aware of the desktop shaping, the methodology by 
item Geometries uses printing advancement to store bolster and gather material coagulated with picture 
or brilliant feasible materials. Not at interims the tiniest amount like around 3D printing machines (Jin, 
Li et al., 2013), it’s prepared for transference results like those from stereolithography outlines.
As shown in Figure 3, in this method, the spilling head slides forward and in reverse on the X-centre 
purpose, stinting one super skinny layer of photopolymer onto the build plat. Promptly among the wake 
of building every layer, light-weight ultraviolet illumination radiation accessible the streaming extension 
discharge magnetic force wave light (Nastase-Dan, Doru Dumitru et al., 2011), instantly set and activity 
each layer. This step forgoes the requirement for any post showing set, as needed by entirely stunning 
progressions. The internal flying plate moves down with beaming exactitude and consequently the plane 
head begins developing the ensuing layer. This framework is reiterated until the model is completed. The 
methodology programming deals with the procedure that utilizes eight streaming heads.
Figure 3. Schematic representation of jetting process

5
A Review on Fabricating Procedures in Rapid Prototyping
﻿
Every head is severally supplanted by the customer. The stock gadgets enable the eight heads to 
figure in parallel, to synchronously stream undefined measures of pitch on the plate. These outcomes 
during a truthful and swish surface. Two separate materials square measure utilized for building; one is 
employed for the real model, though’ another gel like photopolymer material is employed for support 
(Rochus, Plesseria et al., 2007).
Like all utterly completely different speedy prototyping forms, the arithmetic of the bolster structure 
is planned to consent to confused geometries, as an example, cavities, shades and undercuts, or fragile 
highlights and skinny walled regions. It depends on upon supporter accumulate compose that moves down 
within the Z-bearing (vertical) (Zhao, Zhang et al., 2011), with moving ‘’print heads’’ that carelessness 
the stage and print each the model material what’s additional the reinforce material to develop each 
layer. Once technique plant created, each layer is then cured and powerful by introduction to property 
shaft lighting. The going with layer is then maintained prime of that then forward. At the purpose once 
the build is completed, a water-fly viably empties the gel like support material.
Direct Light ProcessingTM Technology
Digital mirror devices became by Texas instruments have found associate in nursing arrangement of 
employments heading off from learning projectors to the gathering of electronic things. Regarding layer 
accumulation innovation (Gong, Rafi et al., 2014). The going before technique developed by Envision 
Tec of Deutschland is that the primary machine was at first advanced by Envision Tec in walk 2003 is 
a particularly enthralling advancement from the point of view of brisk making. Plainly, the name of the 
machine (a truncation of individual procedure plant) proposes that it’s wanted to make things (mechanical 
office) which these things are powerless to be modified to the main individual (Vijayaragavan, Kurian 
et al., 2014). Past the name, the philosophy to boot has various perceiving perspectives beginning with 
the implies that the system makes a region that get to be downwardly as opposed to upwards.
Similarly like the shot and InVision the procedure assemble components from an acryl Greek deity 
based mostly icon curable leave nonetheless it will intrinsically by using a two-dimensional system of 
mirrors instead of a one-dimensional show of reason heads to explicitly cure the fabric. Therefore on 
specifically cure a layer; the procedure build utilization of doctorate innovation grew by National Instru-
ments to explicitly actuate and off mirrors that mirror lightweight ultraviolet illumination actinic radia-
tion or actinic ray light from a supply on to the build zone. With a create pace of 10 -15 seconds every 
layer the system is suitable to collecting parts quickly, nonetheless the use of a solitary doctorate with 
a restricted network of pixels constrains the procedure to very little components if a fine determination 
is well-kept. Given the quality to make little parts, it’s of little stun that the portable gear business has 
incontestable critical energy for this development with changed machines gave to US-based makers.
High Viscosity Jetting Process
The rule incorporates consistent change in a layers sample (negative photo of the layer) as showed by a 
shaky cut of the article to be printed. This uses a framework considering removing a little drop of printable 
materials (powder stuffed compound glue) to a required zone on a substrate. The key unit incorporates 
a solitary plane that is controlled through plane heft. The distribution from the substrate and therefore 
the length of the gushing pulse as in Figure.4.

6
A Review on Fabricating Procedures in Rapid Prototyping
﻿
A take a look at program on single plane is being done the results are exhibiting the varied shapes 
and sizes of affirmation that may be finished. This thought is scaled to a sq. of multi-planes controlled 
in parallel to store a layer of a stunning delineation (Arenas, Alía et al., 2012). The last strategy can give 
answers to can provide answers for various problems and confinements identified in common printing 
and existing rapid prototyping machines. It in like manner has flexibility within the level of exactness 
relying upon the crevice size getting used for the plane. Associate degree era rate like existing high 
volume creation systems are doable and therefore the paste is stacked with any powder.
The MAPLE Process
MAPLE DW (Matrix Assisted Pulsed Laser Evaporation: Direct write) was created through examiners 
at the military administration science research facility Washington. It uses a high repetition rate, 355μm 
thick layer of amass material on the viewpoint (Neil Hopkinson 2005). As a consequence of the opti-
cal gadget imperativeness is facilitated to trim the develop material trade to the getting substrate. This 
can be for all purposes and capacities a twin of a structure creator strip. Figure 5 adapted from (Neil 
Hopkinson 2005)
Figure 4. The high-viscosity jetting principle

7
A Review on Fabricating Procedures in Rapid Prototyping
﻿
LIQUID BASED PROCESS
Selective Laser Sintering (SLS)
Selective laser sintering is an encased substance conveying procedure that uses an optical laser in light of 
the fact that the force supply to fashion beat material, making the optical device afterwards at collects in 
house portray by a 3D model (Wankhade Nitesh Prakash, 2014), ligature the material toward production 
a strong structure. Specific laser mollify jobs accomplice proportional thought, of course specifically 
laser dissolving the fabric is totally thick as opposed to structure(Alan J. Dutson and Bourell, 2003), 
permitting separated properties. Unequivocal optical device Sintering may be a gigantic new development 
that so far has basically been utilized for speedy Prototyping for low-volume making of portion parts. 
Creation segments zone unit connecting on the grounds that the commercialisation of added substance 
delivering advancement propels.
An enclosed substance amassing layer development, selective optical laser sintering joins the employ-
ment of a high power optical laser (for example, a carbon-dioxide laser) to breaker almost no particles 
of mouldable materials into a mass that joins an obliged three-dimensional structure. The optical laser 
in a far-reaching way circuits fine material by auditing cross-territories produced using a 3-D impelled 
portrayal of the half (for instance from a CAD file or compass information) on the surface of a powder 
bed. After each particle is inspected, the grain spot is assigned to bed by lamina thickness, addition of 
material is joined on high, and the strategy is pushed until the part is finished. Figure 6 adapted from 
(Neil Hopkinson 2005).
Since finished half thickness depends on upon prime optical mechanism, rather than optical contrap-
tion compass a selective laser sintering machine commonly uses a laser. The selective optical device 
sintering machinery preheats the mass powder material within the powder bed to a degree beneath its 
softening purpose, to create it less requesting for the optical device to lift the temperature of the picked 
territories the straggly leftovers of the course to the dissolving purpose.
Direct Metal Laser Sintering (DMLS)
Direct metal laser sintering (DMLS) is process enclosed substance delivering procedure that uses a op-
tical laser because the power supply to sinter powdery material (routinely metal), entwining the fabric 
toward build a solid structure guiding the optical optical laser thus at concentrates in house delineate by 
a 3D model, binding the fabric toward manufacture a vigorous structure. It’s like selective laser sintering 
the two are instantiations of a similar plan but vary in specialised delicate parts. Selective laser melting 
Figure 5. Matrix assisted pulsed laser evaporation

8
A Review on Fabricating Procedures in Rapid Prototyping
﻿
utilizes a much identical plan, but in selective optical laser melting the fabric is totally liquefied as op-
position form (Mika Salmi, 2012), allowing distinctive properties.
The direct metal laser sintering system incorporates usage of a 3D CAD model whereby a.stl record 
is formed and sent to the machine’s item. A specialist lives up to expectations with this 3D model to 
appropriately organize the maths for fabricating and incorporates support structure as fitting (Leary, 
Merli et al., 2014). At the aim once this “production record” has been done, it’s delve the layer thick-
ness the machine will develop in and downloaded to the direct metal laser sintering structure machine 
permitting the accumulate to start. The direct metal optical laser sintering machine uses a high-fuelled 
two hundred watt Yb-fiber optical laser.
Inside the develop chest change, there’s a fabric meeting stage and an assembling stage on board a 
re-coater extremely sharp edge usual move new powder over the amass stage. The event circuits metal 
powder into a solid half by melting it primarily victimisation the jogged laser bar. Parts are made ad-
ditively layer by layer (Jin, Li et al. 2013); usually victimisation layers twenty micrometers thick. This 
philosophy thinks seriously regarding terribly capricious geometries to be created specifically from the 
3D CAD data, utterly thus, in hours and with no machining. Direct metal optical laser sintering is like-
wise a net-shape method, making parts with high precision and purpose behind interest determination, 
wonderful surface quality and stunning mechanical properties.
Three-Dimensional Printing (3 DP)
Three dimensional printing or included substance social occasion is a strategy of making a three-dimen-
sional strong object of all around that truly matters any shape from a computerized model. 3D printing 
is refined utilizing an included substance process (Seyeon Hwang, 2015), where element layers of mate-
rial are arranged down in grouped shapes. 3D printing is moreover seen as unmistakable from standard 
Figure 6. Schematic representation of selective laser sintering

9
A Review on Fabricating Procedures in Rapid Prototyping
﻿
machining strategies, which for the most part depend on upon the take-off of material by systems, for 
occurrence (L. M. Galantuccia, 2014), cutting or penetrating.
The 3D printing improvement is utilized for both prototyping and streamed gathering with applications 
being developed illustrating, headway current plan, auto, flight, military, arranging, essential building, 
dental and helpful business meanders, biotech (human tissue substitution), footwear, gems, eyewear, 
planning, geographic data structures, support, and different unmistakable fields. Figure 7 adapted from 
(Kamrani, 2006).
In 3D printing a model to be created by added to a layer on the double. A layer of powder is thusly 
spared in the model plate. The print head then applies pitch perfectly healthy of the model. The layer dries 
solid rapidly. The model plate then moves down the detachment of a layer and another layer of power is 
kept in position, in the model plate. The print head again applies tar alive and well of the model, binds it 
to the first layer. This course of action happens one layer on the double until the model is done. Starting 
late Engineers at the University of Southampton in the UK have arranged, printed, and sent skyward the 
world’s first carrier created absolutely through 3-D printing development.
Making complete models in a singular strategy using 3D printing has amazing points of interest. 
This creative advancement has been exhibited to extra association’s time, work and money. Associations 
giving 3D printing courses of action have breathed life into a gainful and prepared imaginative thing.
Fused Metal Deposition Systems
Various techniques are made that utilization the rule of processing metal pulverulance into a metal pool 
created by an optical device. Among the engineers of those advancements were Sandia National Labs 
United Nations agency utilized the representation Laser Engineered Net Shaping (LENSTM) and joint 
Figure 7. A typical 3D printer

10
A Review on Fabricating Procedures in Rapid Prototyping
﻿
work between John Hopking University, Penn State School. Distinctive adaptations of the innovation 
have been marketed by various associations including POM, Optimec and Aeromet. For the most part 
these techniques have generally moderate completion (Vamsi Krishna, 2009), yet they are doing pro-
vide the likelihood to method much reviewed materials in high dissolve temperature metals as well as 
metallic element.
These methodology have in addition complete up being particularly consummate at subsiding thought 
outside the box devices by as well as material were essential (Igual Muñoz & Mischler, 2011). This may 
form a rapid manufacturing specialty for these strategies within the equally high enclosed price region 
of item repair or support. Figure 8 adapted from (Chattopadhyaya, 2011).
Electron Beam Melting (EBM)
Electron beam melting is a sort of included added substance assembling for metal parts, electron beam 
melting as a powder bed combination system which likewise incorporates specific laser softening. The 
principle distinction is that electron beam melting utilizes an electron pillar as its vitality source, rather 
than a laser. Electron shaft dissolving advancement manufactures parts by melting metal powder layer 
by layer with an electron bar in a high vacuum (R.S. Kircher, 2008). Rather than sintering techniques, 
both electron beam melting and selective laser melting accomplish full liquefying of the mineral powder. 
The laser sintering is for the most part recorded and is at times used to depict full softening or plastic 
procedures.
This strong free shape manufacture technique creates completely thick metal parts specifically from 
metal powder with attributes of the objective material. The electron beam melting apparatus peruses 
Figure 8. Laser engineered net shaping

11
A Review on Fabricating Procedures in Rapid Prototyping
﻿
information from a 3D computer aided design model and sets down progressive layers of powdered in-
got. These layers are melted together utilizing a computer controlled electron shaft. Along these lines it 
develops the parts. The methodology happens beneath vacuum that’s suited to make part in responsive 
materials with a high warm-heartedness for chemical element, e.g. titanium. The procedure is understood 
to figure at higher temperatures (up to 1000 °C), which might prompt contrasts in stage development 
however cementing and strong state stage change.
Figure 9 adapted from (Jones, 2009). The powder feedstock is commonly pre-alloyed, rather than a 
mixture. That viewpoint permits characterization of electron beam melting with specific laser dissolv-
ing where contending advancements like selective laser sintering and direct metal laser sintering oblige 
warm treatment subsequent production. Differentiated with selective laser melting and direct metal laser 
sintering, electron beam melting has a for the most part prevalent form rate in light of its higher vitality 
thickness and examining strategy.
Selective Laser Melting (SLM)
Selective laser melting is partner encased substance accumulating technique that uses 3D CAD informa-
tion as a pushed data supply and vitality as a high-control optical laser column to make three-dimensional 
metal components by association fine brilliant powders along.
The framework begins by cutting the 3D CAD file information into layers, for the most part from 
twenty to a hundred micrometers thick, this record affiliation is that the business typical.stl report utilized 
on most layer-based 3D printing or stereolithography headways (Hopkinson, 2003). This record is then 
stacked into a document readiness programming bundle that doles out parameters, qualities and physi-
Figure 9. Schematic representation of electron beam melting

12
A Review on Fabricating Procedures in Rapid Prototyping
﻿
cal backings that permit the document to be deciphered and constructed by distinctive sorts of added 
substance assembling machines. Figure 10 adapted from (R. H. Neil Hopkinson, 2005).
With specific optical gadget liquefying thin layers of atomized fine metal powder are consistently 
spread using a covering instrument onto a substrate plate, normally metal, that is joined with a compart-
mentalization table that moves inside the vertical (Z) turn. This happens inside a chamber containing a 
solidly controlled climate of torpid gas, either concoction component or substance component at nuclear 
number eight levels beneath five hundred areas every million. When every layer has been dispersed 
every second cut of the half immaculate science is joined by particularly applying the optical gadget 
imperativeness to the powder surface, by coordinative the ran optical contraption section utilizing two 
high excess examining mirrors inside the X and Y axis’s. The optical device centrality is sufficiently 
exceptional to permit full dissolving (welding) of the particles to edge effective metal. The framework 
is rehashed layer when layer till the half is finished.
Selective Masking Sintering (SMS)
The selective masking sintering strategy incorporates printing a shroud of infrared considering materials 
to a glass sheet and swings the sheet over a powder bed. Infrared is then associated with the glass sheet 
and reasonable to explicitly experience the duvet subsequently on near the powder foursquare to a lower 
spot. Figure 11 adapted from (R. H. Neil Hopkinson, 2005).
This methodology dispenses with the conditions for an optical laser and just in case wherever a large 
a part of the surface must be mould this have to be compelled to considerably diminish handling time 
once contrasted and optical laser sintering. Speed half assert that each layer may be utterly handled in 
10-20 sec which the employment of a canopy started of a layer guarantees that assemble. Times Square 
measures something however tough to foresee and freed from half volume.
Afterwards, this system has to be compelled to have soap benefits whereas being utilised for fast 
producing as a section of high cubic measure to fabricate 300x210x500 metric linear unit may be utilised 
to deliver components. The approach was at first tried for conveying vacuum surrounding devices, creat-
ing utilization of the process in warming consistence, nevertheless new materials might build this one 
among the up and coming back era of rapid manufacturing machines.
Figure 10. Schematic representation of selective laser melting process

13
A Review on Fabricating Procedures in Rapid Prototyping
﻿
Selective Inhibition Sintering (SIS)
The selective inhibition sintering methodology is being created by the University of Southern American 
state in light-weight of speedy manufacturing instead of like specific covering sintering the methodology 
tries to hitch the benefits of selective optical device sintering and flying procedures. Be that as it may, 
selective inhibition sintering is prone to attain to preferred determination and definition over the velocity 
part handle as the incivility material is written squarely on to the powder and uses no cowl which will take 
under consideration lightweight dissemination. The procedure was represented at the robust race creation 
conference in Austin, Texas in august 2002 and utilizes a print head to stream liquid to utmost sintering 
on to picked extents of the assembling volume. This is frequently trailed by utilizing a sending warmth 
supply to explore the gathering area and shape any powder that has not had the substance carved onto it.
Starting work utilized one spout to print round the edge of half yet the framework may while not an 
uncertainty be made to at steady time print the smothering material in an exceptionally one-dimensional 
network. Late analysis has thought-about the utilization of smorgasbord of restraining materials running 
from business improvement specialists to halide. Another half materials that has been looked into is 
that the powder material of the half themselves. With achievement reportable once sintering a buffet of 
compound powder at the part of polycarbonate, and polyester.
Unlike most of the option powder sintering approach, Selective Inhibition Sintering doesn’t oblige that 
the fabric entirely surprising the 0.5 be upraised with an additional sturdy temperature than the fabric to 
not be work. This could make alteration of warmth slant over the surface which can provoke advantages, 
for occurrence, lessened turn age. This can be in different respects a mirror of different procedures, for 
occasion, 3Dimensional procurement in that the fabric that is engraved onto the surface is utilized as 
half a neighbourhood that won’t contribute the last part. As a rule the real volume of an expansive bit of 
that is updated separated and therefore the volume that is incorporates is little. Thus, selective inhibition 
sintering will get to use the preventative material to the dominant a locality of every layer with elements 
Figure 11. Representation of SMS process

14
A Review on Fabricating Procedures in Rapid Prototyping
﻿
merely embodying a lion’s share of every layer. From the reason for read of high-volume fabricate this 
has all the reserves of being illogical approach. Having aforementioned this, the target of high-volume 
fabricate by rapid manufacturing will often be to pack 0.5 beds as thickly as are often allowed. Thus this 
plain nonsensical methodology is susceptible to be less of a difficulty than it might be Rapid Prototyp-
ing, wherever thickly stuffed half beds square measure from time to time utilised.
Electro-Photographic Layered Manufacturing
This methodology utilizes a fascinating blend of thoughts that have been utilized for laser sintering. 
This system utilizes electro-photographic method to store a district powder and around then a bolster 
powder for each layer. starting work focused round the idea about transport of title an unpractised half by 
golf shot away separate half strengthen powders and thus utilizing a radiator operation to form the half 
material in another step, this compelled by a solemn obligation that the bolster material had a superior 
mellow reason than the half material. Notwithstanding, any work has attempted various things with the 
idea about sintering each layer before the chaperon layer is place away, similar to option powder basi-
cally based layer get-together strategy.
One of the issues that need to be overcome is in storing material electro-photographically to create 
elements with an outsized Z stature. It creates the impression that the methodology has to be suited to 
high generation rate however forced to littler elements, for example, electrical segments.
High Speed Sintering (HSS)
Rapid sintering strove for misapplying the mechanical properties given by selective laser sintering while 
attaining to a develop machine through spot and diminished machine worth by shedding the requirement 
for an optical gadget. It portrays the immaculate number-crunching of every layer by printing an antiquity 
that advances considerable approach of radiation on to the powder bed surface, rather like negative of 
selective inhibition sintering. The best thanks to affect high speed sintering is capability to affect the 
speed of sintering over the manufacture surface. Examination has exhibited that a high sintering rate 
attains to immaterial shrinkage and impossible edge definition however poor mechanical properties, 
though’ direct sintering satisfies higher mechanical properties however to a hurt definition and accuracy. 
By predominant sintering rates through ways in which,.for example, the use of faint scale and material 
that acclimatize importance at all completely different rates the goal of achieving impossible mechanical 
properties with extraordinary truth and surface consummation is sought for once.
SOLID BASED PROCESS
Fused Deposition Modeling (FDM)
Fused Deposition Modeling (FDM) is an added substance fabricating innovation normally utilized for 
demonstrating, prototyping, and creation applications. It is one of the procedures utilized for 3D print-
ing. Fused deposition modeling takes a shot at an “included substance” rule by setting down material in 
layers (Jain & Kuthe, 2013); a plastic fiber or metal wire is loose up from a turn and supplies material 
to make a part.

15
A Review on Fabricating Procedures in Rapid Prototyping
﻿
Fused Deposition Modeling starts with a product process which forms a stereolithography record (STL 
document design) (Narayan 2014, Samir Kumar PANDA 2009), experimentally cutting and arranging 
the model for the manufacture process. On the off chance that obliged, fortify structures may be made. 
The machine may allot distinctive materials to accomplish unmistakable destinations: for event, one 
may utilize one material to add to the model and utilization another as a dissolvable bolster structure (B. 
Huang, 2012), or one could utilize different shades of the same sort of thermoplastic on the same model.
The model or part is passed on by evacuating little globules of thermoplastic material to shape layers 
as the material solidifies quickly after expulsion from the spout. A plastic fiber or metal wire is loose 
up from a turn and supplies material to an evacuation gush which can butcher the stream on and. There 
is routinely a worm-drive that pushes the fiber into the spout at a controlled rate. The spout is warmed 
to unwind the material. The thermoplastics are warmed past their glass move temperature and are then 
saved by an evacuation head. Figure 12 adapted from (R. H. Neil Hopkinson, 2005).
The spout can be moved in both even and vertical orientation by a numerically controlled framework. 
The spout takes after an apparatus way controlled by a PC supported assembling (CAM) programming 
bundle, and the part is developed from the base(Ahn, Montero et al., 2002), one layer at once. Stepper 
engines or servo engines are regularly utilized to move the expulsion head. Albeit as a printing innova-
tion fused deposition modeling is extremely adaptable, and it is fit for managing little shades by the 
backing from lower layers fused deposition modeling overall has a couple of restrictions on the slope of 
the shade, and can’t make unsupported stalactites.
Horde materials are accessible, for example, Acrylonitrile Butadiene Styrene ABS, Polylactic cor-
rosive PLA, Polycarbonate PC, Polyamide PA, Polystyrene PS, lignin, elastic, among numerous others, 
with distinctive exchange offs in the middle of quality and temperature properties.
Figure 12. Shematic representation of fused deposition modelling

16
A Review on Fabricating Procedures in Rapid Prototyping
﻿
Laminated Object Manufacturing (LOM)/ Sheet Stacking Technology
Laminated object manufacturing included layer-by-layer overlay of paper material sheet, cut utilizing 
a CO2 laser, every sheet speaking to one cross-sectional layer of the CAD model of the part (Neil Hop-
kinson 2005). In laminated object manufacturing the segment of the paper sheet which is not contained 
inside the last part is cut into 3D shapes of materials utilizing a cross-lid cutting operation.
This procedure has been created taking into account sheet cover including other form materials and 
cutting procedures. In view of the development guideline, just the external shape of the parts is cut and 
the sheet can be either cut and after that stacked or stacked and afterward cut. Figure 13 adapted from 
(Mahendru, 2013).
The procedure is executed as takes after:
•	
Sheet is command quick to a substrate with a warm roller;
•	
Laser follows sought measurements of model;
•	
Laser cross brings forth non-part region to encourage waste evacuation;
•	
Platform with finished layer moves down off the crushed path;
•	
Fresh sheet of fabric is stirred into position;
•	
Platform downs into new position to induce next layer;
•	
The methodology is rehashed.
Materials Used in Rapid Prototyping
Classification of materials presented in Table 2.
Figure 13. Schematic representation of LOM

17
A Review on Fabricating Procedures in Rapid Prototyping
﻿
APPLICATIONS
Regardless of the way that Rapid Prototyping can be joined in every industry, some common Rapid 
Prototyping applications are recorded underneath:
1. 	
More Exact: As Rapid Prototyping machines are getting to be fit for making parts with higher 
determination, the example is to consolidate different systems to convey quality parts, and numer-
ous Rapid Prototyping methods will have the capacity to create parts with great exactness and 
accuracy;
2. 	
Medicinal Applications: Many therapeutic applications oblige specially designed Rapid Prototyping 
parts. At the point when Rapid Prototyping methods can create a mixture of materials for restorative 
application at a sensible expense, Fast Prototyping headways will be a trademark match for helpful 
applications;
3. 	
Repair and Reuse: As some included substance methodologies can adequately lace two materi-
als, including high-temperature materials, use of Rapid Prototyping headways for part repair and 
reuse will be remarkably financially watchful. The applications will be in repairing and reusing 
medium- and high-respect parts. A valid example, one wonderful application will be to repair a 
ragged out mold or to change and reuse a present mold for unmistakable applications;
4. 	
Gradient Materials: Rapid Prototyping will be able to pass on a part that “”successfully”” joins 
two or more materials together, metal to metal, metal to innovative, optical material and metal, and 
so forth. There will be innumerable to make parts with new material properties, and the applica-
tions are past imaginative imperativeness.
ADVANTAGES AND DISADVANTAGES
Assembling completed parts direct from advanced data is a leap forward mechanical headway. Still in 
its initial stages, quick assembling is coming into progressively boundless use in specific innovation, 
some of the pros and cons are listed in Table 3.
Table 2. Classification of materials
Materials Used in Rapid Manufacturing
Metallic
Polymers
Ceramics
Organic
Tool steel 
Aluminium 
Titanium 
Inconel 
Cobalt chrome 
Copper 
Stainless steel 
Gold / Platinum 
Hastelloy
ABS (Acrylonite Butadiene 
Styrene) 
Poly-Amide (nylon) 
Filled PA 
PEEK 
Thermo setting epoxies 
PMMA 
Polycarbonate 
ULTEM 
Aluminium loaded polyamide
Alumina 
Mullite 
Zirconia 
Silicon carbide 
Beta-tri calcium phosphate 
Ceramics loaded epoxies 
Silica (sand) 
Plaster 
Graphite
Wax 
Tissue/cells

18
A Review on Fabricating Procedures in Rapid Prototyping
﻿
CONCLUSION
There are different Rapid prototyping methods, and more are creating. As Rapid prototyping is a rising 
range, every methodology is enhancing quickly and getting to be more aggressive. This paper gives an 
outline of Rapid prototyping innovation in short and accentuates on their capacity to abbreviate the item 
plan and advancement process. Here we exhibited diverse quick prototyping courses of action assembled 
into three classes in view of the crude material used to create the model.
REFERENCES
Ahn, S. H., Montero, M., Odell, D., Roundy, S., & Wright, P. K. (2002). Anisotropic mate-
rial properties of fused deposition modeling ABS. Rapid Prototyping Journal, 8(4), 248–257. 
doi:10.1108/13552540210441166
Alan J. Dutson, K. L. W., Joseph J. Beaman, Richard H. Crawford, and D. L. Bourell (2003). “Applica-
tion of similitude techniques to functional testing of rapid prototypes.” Emerald Rapid Prototyping9(1).
Arenas, J. M., Alía, C., Blaya, F., & Sanz, A. (2012). Multi-criteria selection of structural adhesives to 
bond ABS parts obtained by rapid prototyping. International Journal of Adhesion and Adhesives, 33, 
67–74. doi:10.1016/j.ijadhadh.2011.11.005
Billiet, T., Vandenhaute, M., Schelfhout, J., Van Vlierberghe, S., & Dubruel, P. (2012). A review of trends 
and limitations in hydrogel-rapid prototyping for tissue engineering. Biomaterials, 33(26), 6020–6041. 
doi:10.1016/j.biomaterials.2012.04.050 PMID:22681979
Chaennakesava, P., & Yeole, S. (2014). Fused Deposition Modeling - Insights. Proceedings of the In-
ternational Conference on Advances in Design and Manufacturing ICAD&M’14, Tiruchirappalli, India 
(pp. 1345-1350.
Chattopadhyaya, S. (2011). Journey from Rapid Prototyping to Rapid Manufacturing: Some Introductory 
Description of Existing Rapid Prototyping Processes and Their Interfaces with Rapid Manufacturing, 
ambert Academic Publishing.
Chua, C. K., S. M. C. a. T. S. W. (1998). A Study of the State-of-the-Art Rapid Prototyping Technolo-
gies. International Journal of Advanced Manufacturing Technology, 14.
Table 3. Advantages and disadvantages of rapid manufacturing
Advantages
Disadvantages
•      Flexibility to plan and improve without penalties 
•      Fast cycles through outline stages 
•      Phenomenal for mass customization 
•      End of tooling 
•      Green assembling 
•      Negligible material waste 
•      Vitality proficient 
•      Empowers customized assembling
•      Startling pre and post-handling 
•      High process cost 
•      Absence of industry models 
•      Low seed, not suitable for large scale manufacturing 
•      In steady materials 
•      Predetermined number of materials 
•      High hardware cost for top of the line producing 
•      Poor surface completion and dimensional exactness

19
A Review on Fabricating Procedures in Rapid Prototyping
﻿
Galantuccia, L. M., Kacanib, I. B. J., & Lavecchiaa, F. (2014). Analysis of dimensional performance 
for a 3D open-source printer based on fused deposition modeling technique. Proceedings of the Inter-
national Scientific Committee of the 3rd CIRP Global Web Conference. Elsevier B.V. doi:10.1016/j.
procir.2015.04.014
Gong, H., Rafi, K., Gu, H., Starr, T., & Stucker, B. (2014). Analysis of defect generation in Ti–6Al–4V 
parts made using powder bed fusion additive manufacturing processes. Additive Manufacturing, 1-4, 
87–98. doi:10.1016/j.addma.2014.08.002
Hopkinson, N., and Dickens, P.M. (2003). Analysis of rapid manufacturing—using layer manufacturing 
processes for production. Journal of Mechanical Engineering Science, 217(C).
Hopkinson, Hague, R.J.M., & Dickens, P.M. (2005). Rapid Manufacturing: An Industrial Revolution 
for the Digital Age. Wiley.
Huang, B., S. S. (2012). Alternate slicing and deposition strategies for fused deposition modelling of 
light curved parts. Journal of Achievements in Materials and Manufacturing Engineering, 55(2).
Hwang, S., Reyes, E.I., Moon, K.-s., Rumpf, R.C., & Kim, N.S. (2015). Thermo-mechanical Charac-
terization of Metal/Polymer Composite Filaments and Printing Parameter Study for Fused Deposition 
Modeling in the 3D Printing Process. Journal of Electronic Materials, 44(3), 771-777.
Igual Muñoz, A., & Mischler, S. (2011). Effect of the environment on wear ranking and corrosion of 
biomedical CoCrMo alloys. Journal of Materials Science. Materials in Medicine, 22(3), 437–450. 
doi:10.1007/s10856-010-4224-0 PMID:21221728
Jain, P., & Kuthe, A. M. (2013). Feasibility Study of Manufacturing Using Rapid Prototyping: FDM 
Approach. Procedia Engineering, 63, 4–11. doi:10.1016/j.proeng.2013.08.275
Jin, G. Q., Li, W. D., & Gao, L. (2013). An adaptive process planning approach of rapid prototyping 
and manufacturing. Robotics and Computer-integrated Manufacturing, 29(1), 23–38. doi:10.1016/j.
rcim.2012.07.001
Jin, G. Q., Li, W. D., Gao, L., & Popplewell, K. (2013). A hybrid and adaptive tool-path generation ap-
proach of rapid prototyping and manufacturing for biomedical models. Computers in Industry, 64(3), 
336–349. doi:10.1016/j.compind.2012.12.003
Jones, R. (2009). Manufacturing and Product Design. Crimson Publishing, Limited.
Kamrani, A. K. (2006). Nasr, Emad Abouel. Springer, US: Rapid Prototyping Theory and Practice.
Kircher, R. S., Christensen, A.M., & Wurth, K.W. (2008). Electron Beam Melted (EBM) Co-Cr-Mo 
Alloy for Orthopaedic Implant Applications. Medical Modeling, Inc., Golden, CO, USA.
Kumar, S., & Kruth, J. P. (2010). Composites by rapid prototyping technology. Materials & Design, 
31(2), 850–856. doi:10.1016/j.matdes.2009.07.045
Lan, H. (2009). Web-based rapid prototyping and manufacturing systems: A review. Computers in In-
dustry, 60(9), 643–656. doi:10.1016/j.compind.2009.05.003

20
A Review on Fabricating Procedures in Rapid Prototyping
﻿
Leary, M., Merli, L., Torti, F., Mazur, M., & Brandt, M. (2014). Optimal topology for additive manu-
facture: A method for enabling additive manufacture of support-free optimal structures. Materials & 
Design, 63, 678–690. doi:10.1016/j.matdes.2014.06.015
Mahindru, D.VMahendru, P. (2013). Review of Rapid Prototyping-Technology for the Future. Global 
Journal of Computer Science and Technology Graphics & Vision, 13(4).
Melchels, F. P., Feijen, J., & Grijpma, D. W. (2010). A review on stereolithography and its applications 
in biomedical engineering. Biomaterials, 31(24), 6121–6130. doi:10.1016/j.biomaterials.2010.04.050 
PMID:20478613
Nastase-Dan, C., Doru Dumitru, P., Gheorghe Ion, G., & Sanda, P. (2011). Innovative technology 
through selective laser sintering in mechatronics, biomedical engineering and industry. Incas Bulletin, 
3(1), 31–37. doi:10.13111/2066-8201.2011.3.1.5
Nguyen, H., & Vai, M. (2010). RAPID Prototyping Technology. Lincoln Laboratory Journal, 18(2), 17-27.
Panda, S.K., Padhee, S., Sood, A.K., & Mahapatra, S.S. (2009). Optimization of Fused Deposition 
Modelling (FDM) Process Parameters Using Bacterial Foraging Technique. Intelligent Information 
Management, 2009(1), 89-97.
Peltola, S. M., Melchels, F. P. W., Grijpma, D. W., & Kellomäki, M. (2008). A review of rapid 
prototyping techniques for tissue engineering purposes. Annals of Medicine, 40(4), 268–280. 
doi:10.1080/07853890701881788 PMID:18428020
Pham, D.T., & Gault, R.S. (1997). A Comparison of Rapid Prototyping Technologies. International 
Journal of Machine Tools & Manufacturing, 38(10-11), 1257-1287.
Prakash, W.N., Sridhar, V.G., & Annamalai, K. (2014). New product development by DFMA and rapid 
prototyping. Journal of Engineering and Applied Sciences, 9(3), 274-279.
Rochus, P., Plesseria, J. Y., Van Elsen, M., Kruth, J. P., Carrus, R., & Dormal, T. (2007). New applica-
tions of rapid prototyping and rapid manufacturing (RP/RM) technologies for space instrumentation. 
Acta Astronautica, 61(1-6), 352–359. doi:10.1016/j.actaastro.2007.01.004
Salmi, M., Tuomi, J., Paloheimo, K.-S., Björkstrand, R., Paloheimo, M., Salo, J., & Mäkitie, A. A. et al. 
(2012). Patient-specific reconstruction with 3D modeling and DMLS additive manufacturing. Rapid 
Prototyping Journal, 18(3), 209–214.
Trevor Boehm, M.-I. C., & Elizabeth Douglas, M. (2004). Marco Gero Fernández, Christopher Williams. 
Rapid Prototyping Industry Analysis.
Tukuru, N., Shivalinge Gowda, K.P., Ahmed, S.M., & Badami, S. (2008). Rapid Prototype Technique 
in Medical Field. Research J. Pharm. and Tech, 1(4), 341-344.
Vamsi Krishna, B. B., & Bandyopadhyay, A. (2009). Surface modification of AISI 410 stainless steel 
using laser engineered net shaping (LENSTM). Materials & Design, 30(5), 1490–1496. doi:10.1016/j.
matdes.2008.08.003

21
A Review on Fabricating Procedures in Rapid Prototyping
﻿
Vijayaragavan, E., Kurian, L. M., Sulayman, H., & Gopal, T. V. (2014). Application of Rapid Prototyp-
ing in the Treatment of Clubfoot in Children. Procedia Engineering, 97, 2298–2305. doi:10.1016/j.
proeng.2014.12.474
Wong, K. V., & Hernandez, A. (2012). A Review of Additive Manufacturing. ISRN Mechanical Engi-
neering, 2012, 1–10. doi:10.5402/2012/208760
Zhao, H., Zhang, G., Yin, Z., & Wu, L. (2011). A 3D dynamic analysis of thermal behavior during 
single-pass multi-layer weld-based rapid prototyping. Journal of Materials Processing Technology, 
211(3), 488–495. doi:10.1016/j.jmatprotec.2010.11.002
This work was previously published in the International Journal of Manufacturing, Materials, and Mechanical Engineering 
(IJMMME), 6(2); edited by J. Paulo Davim, pages 23-40, copyright year 2016 by IGI Publishing (an imprint of IGI Global).

22
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  2
DOI: 10.4018/978-1-5225-1677-4.ch002
ABSTRACT
Fused deposition modeling is a proven technology, widely diffused in industry, born for the fabrica-
tion of aesthetic and functional prototypes. Recently used for small and medium series of parts and for 
tooling, it received particular attention in order to integrate prototyping systems within production. A 
limiting aspect of this technology is the obtainable roughness and above all its prediction: no machine 
software and Computer-Aided Manufacturing implements a relationship between process parameters 
and surface quality of components. The prediction of the surface properties is an essential tool that 
allows it to comply with design specifications and, in process planning, to determine manufacturing 
strategies. Recently, great effort has been spent to develop a characterization of such surfaces. In this 
chapter, prediction models are presented and a new characterization approach is detailed. It is based 
on the theoretical prediction of the geometrical roughness profile, thus allowing it to obtain, in advance, 
all roughness parameters.
INTRODUCTION
The competition, in the recent years, increased the identified need of time to market. A successful 
product development depends on managing resources and circumstances with conflicting objectives. 
The products must have highest quality, lowest cost and must be produced in a flexible way and in the 
shortest time (Toriya, 2010; Bramley, Brissaud, Coutellier, & McMahon, 2005). The introduction of 
new technologies increases the number of options at disposal of industries, but the management of such 
a manufacturing landscape becomes complex. In such a way, the improvement of the productivity must 
be accomplished by maintaining, if not improving, product quality. Several factors make this aim very 
difficult. We can only sign: fast changing in customer desires, significance of aesthetic, environmental 
Surface Characterization in 
Fused Deposition Modeling
Alberto Boschetto
University of Rome “La Sapienza”, Italy
Luana Bottini
University of Rome “La Sapienza”, Italy

23
Surface Characterization in Fused Deposition Modeling
﻿
requirements, decreasing in lifetime, decreasing prices, decreasing budgets, limitation from regulations 
and standards (Gebhardt, 2003). Thus, the knowledge of the optimum machining parameters is vital, 
and the industry must acquire as quickly as possible this process parameters for each specific opera-
tion. A product requirement, that research has extensively investigated, is the surface quality. It plays an 
important role on product properties, such as wear resistance, coating and light reflection (Whitehouse, 
2011). It is well known that a reliable indicator is surface roughness expressed by several parameters. 
The intrinsic complexity, arising from surface fabrication, makes its modeling very difficult leading to 
significant area of interest. In the optics of fabricating a high surface-quality product, the resources neces-
sary to improve the manufacturing chain are numerous and affect several process stages: product design, 
test, fabrication, market. The efficient use of such resources calls for new technologies. As a result, in 
the last two decades, a powerful tool for rapid product development has been introduced, namely, rapid 
prototyping (RP). This term is used in a variety of industries to describe a process for rapidly creating 
components before final release or commercialization. In a product development context, the term RP 
was used widely to describe a host of technologies which creates physical part directly from digital data. 
Respect to traditional technologies, RP builds the parts adding or bonding material, layer by layer: this idea 
offers advantages in many applications because parts can be fabricated with any geometric complexity 
without the need for elaborate machine setup or final assembly. Other manufacturing processes require 
a careful and detailed analysis of the part geometry to determine the sequence of the operations, the 
needed tools, the process parameters, the machines setup, the fixing systems, and the semi-manufactured 
products. Conversely, RP needs only part specifications, and process parameters do not depend upon 
geometric complexity, employing the same semi-manufactured product. These advantages, combined 
with the recent introduction of new materials and the improvements of part quality and machines ac-
curacy, make these technologies reliable to produce final products and assist the process tooling stage 
(Hopkinson, Hague, & Dickens, 2005). These aspects determined the wide diffusion in the industry: 
significant application fields are aerospace, automotive, biomedical, customer product industry, design 
and tooling (Chua, 1994; Chua, Leong, & Lim, 2010; Ingole, Kuthe, Thakare, & Talankar, 2009; Iva-
nova, Williams, & Campbell, 2013).
These new evolutions of RP modified the scenarios requiring a new nomenclature. The Technical 
Committee within ASTM International agreed that the RP acronym was obsolete and it adopted the term 
Additive Manufacturing (AM) (Wohler, 2008). Now AM is an increasingly important tool for product 
development which can reduce the fabrication of complex objects to a manageable, straightforward and 
relatively fast process.
AM involves a number of stages that move from the virtual model to the finished parts (Figure 1). The 
first step is related to the 3D model generation, typically in CAD environment or by Reverse Engineering 
techniques. The second step is the conversion to the interchange file: Standard Triangulation Language 
(STL) file format encloses tessellated surfaces and it has become the standard de facto. In the third step 
the file is transferred to the prototyping system and process parameters are chosen: part orientation, 
layer thickness, model and support strategies. The geometry is then sliced into layers and the generated 
curves are verified. The fifth step regards the support creation. Then, the model, the support and the 
transition moves toolpaths are generated and saved. The system is ready for an automatic fabrication 
of physical parts. The last stage is the post processing operations, which consist of the support removal 
and additional cleaning up, if required.

24
Surface Characterization in Fused Deposition Modeling
﻿
Several problems can arise from the above mentioned steps. STL conversion is affected by the well-
known problem highlighted by Leong, Chua, and Ng (1996a, 1996b): commercial CAD modelers, in 
fact, are not robust enough to generate tessellated surfaces and frequently produce errors. The slicing 
step is an approximation of the original model and, depending upon geometry positioning, produces a 
physical object characterized by the staircase effect (Pandey, Reddy, & Dhande, 2003a; Thrimurthulu, 
Pandey, & Reddy, 2004). The CNC system causes problems related to position and speed control which, 
combined with physical and chemical aspects of the specific technology, lay in accuracy and surface 
quality losses. Also the post-processing operations can provoke surface quality modifications because 
they are based on manual or semi-automatic mechanical and chemical actions.
There is a wide range of solution in the field of AM technologies (Kaufui & Hernandez, 2012). They 
are typically classified by the initial form of input material: liquid-based, solid-based and powder-based. 
Stereolithography apparatus (SLA), Fused Deposition Modeling (FDM), Selective Laser Sintering (SLS), 
Selective Laser Melting (SLM), Laminated Object Manufacturing (LOM), Three Dimensional Printing 
(3DP) are representative AM technologies but more than 60 techniques are currently employed in the 
industry.
FDM is the most diffused AM technique in the world (Wohler, 2012). The system fabricates, layer by 
layer, components by depositing the material (typically thermoplastic) in filament form. A temperature-
controlled extrusion head is fed with material that is heated to a semiliquid state. The head extrudes, 
directs and crushes the filament in thin layers onto a fixtureless base. The air, which surrounds the head, 
is maintained at a temperature below the material melting point, and makes the exiting material quickly 
Figure 1. Typical additive manufacturing stages

25
Surface Characterization in Fused Deposition Modeling
﻿
solidify. In the x-y plane a CNC system moves the head following the toolpath of the desired layer. At 
least two materials must be extruded: the model, i.e. the material for final component, and the support 
necessary to sustain the overhanging part. The support structures are eliminated in the post-processing 
stage by hand operation or dissolution. A number of parameters that affect performance and functionalities 
of the system are: material strength, material flexural modulus, material viscosity, positioning accuracy, 
road width, deposition speed, volumetric flow rate, tip diameter, layer thickness, geometry orientation, 
toolpath manufacturing strategy, envelope and extrusion temperatures (Chua, Leong, & Lim, 2010).
The main strengths of FDM are:
•	
Fabrication of Functional Part: components have 85% of strength of the molded one in the same 
material;
•	
Easiness of Implementation: it is suitable both for industrial and office environments;
•	
Low Material Wastage: FDM process material are kept to a minimum because only model and 
support material are needed;
•	
Easy of Support Material Removal: the use of specific support generation strategies allows to 
easily break support structure and soluble systems can be washed away;
•	
Easy of Material Change: the material is supplied in cartridge leading to a simple change;
•	
Large Build Volume: FDM offers larger build volume than most AM technologies.
Despite of its diffusion, this technology presents limitations common to AM technologies. A typical 
drawback lies in the list of the material at disposal to produce part. Commercial ones are thermoplas-
tics such as ABS, Nylon, polyethylene, polypropylene, polycarbonate, a variety of blends, as well as 
engineered thermoplastics, characterized by additives in polymer production. These additives include 
fillers (powders and fibers that can improve properties), plasticizers (organic chemicals that improve 
flexibility), stabilizers (substances that help reduce degradation), and lubricants (chemicals that reduce 
friction and improve material flow in heads). These materials do not cover the world of polymers and a 
proper part design must be performed just starting from this limitation. As concern surface aspects, the 
staircase effect markedly affects FDM as it employs thick filament: at present 0.127 mm is the smallest 
layer thickness, higher than other AM technologies. Other problems are related to thermo-mechanical 
aspects: the polymeric material is rapidly cooled introducing parts distortions and stresses. In this condi-
tion the shrinkage is quite unpredictable and the process needs a compensation by adjusting parameters. 
These problems plague the part surface finishing and lead to an direct impact on post-processing costs, 
aesthetics and functionality of parts (Vasudevarao, Natarajan, & Henderson, 2000). Nevertheless the 
most limiting problem is the lack, in industry, of accurate prediction of the surface output. In fact no 
machine software and no Computer Aided Manufacturing (CAM) environment provide a relationship 
between process parameters and surface quality of components. This leads engineers and process man-
agers towards manufacturing subsidiaries, or forces technicians to replace the FDM with subtractive 
techniques. Furthermore a typical use of the FDM technique is based on trial and error methods with a 
significant increase of manufacturing costs. At present this lack of knowledge limits a truly integration of 
FDM within manufacturing processes. Industrial management requires to know in advance an accurate 
prediction of attainable surface quality: more specifically it is needed, in a manual and Computer Aided 
Process Planning (CAPP) (Radhakrishnan, Subramanyam, & Raju, 2008). In this context, an employ-
ment of FDM, likewise for the traditional technologies, depends upon the prediction of its performance. 
This chapter covers the characterization of surface quality attainable by FDM through prediction models.

26
Surface Characterization in Fused Deposition Modeling
﻿
BACKGROUND
In literature a number of works have been addressed to the problem of FDM surface quality. Several ex-
perimental investigations focused on the determination of the significant process parameters. Vasudevarao, 
Natarajan, and Henderson (2000) found that layer thickness and part orientation have significant effect 
on the roughness of FDM parts while air gap and road width do not: they proposed empirical equation 
for calculating the average surface roughness Ra. Experimental data and statistical analysis showed that 
further investigation must be provided in order to determine interactions between parameters especially 
related to the envelope temperature and the input material. Anitha, Arunachalam, and Radhakrishnan 
(2001) attempted to obtain optimum process conditions by using Taguchi method. The results showed, in 
some cases, the dependency upon the layer thickness, road width and deposition speed. In the considered 
process conditions, the most influencing factor affecting surface roughness was the layer thickness. Perez, 
Vivancos, and Sebastian (2001) suggested a model considering layer thickness and horizontal distance 
between layers as main factors. Among this research field Armillotta (2006) found similar results by 
experimental investigation conducted on textured specimens.
Other theoretical works introduced models for the characterization of the surface quality. These mod-
els are addressed to the development of a relationship between process parameters and average surface 
roughness. Typically the formulation is based on geometrical assumptions on the deposited filament 
sections. Pandey, Reddy, and Dhande (2003a) developed a semi-empirical model of Ra considering layer 
thickness and build orientation as variables; they approximated the layer profile with a parabola assum-
ing the mean surface to be in the middle. Ahn, Kim, and Lee (2009) and Ahn, Kweon, Kwon, Song, 
and Lee (2009) proposed a model to express average roughness by assuming the filament profile as an 
elliptical curve; cross-sectional shape, surface angle, layer thickness and overlap between adjacent layers 
are considered the main factors affecting surface quality. Ahn, Kim, and Lee (2007) propose a model to 
express surface roughness distribution in FDM, according to changes in surface angle and introducing 
the stair stepping effect. Campbell, Martorelli, and Lee (2002) used empirical surface roughness values 
to represent the actual surface roughness in their visualized results: discontinuous roughness distribu-
tions, over several partial regions of the test model, were however found.
These models acceptably predict roughness in a surface slope ranging between 45° and 135°, but some 
gaps between expected and empirical data are present. Outside this range they markedly diverge: this 
highlights the difficulty to theoretically understand process parameters’ effects on surface roughness Ra.
It is well known that Ra cannot give information about peak shape and wavelength: no distinction can 
be made between peaks and valleys. A number of surface parameters have been developed to characterize 
surfaces in order to identify specific requirements coming from particular applications. More than 100 
parameters have been defined for industrial use and many of them appear in national and international 
standard (ISO 4287, 1997; ASME B46.1, 1995; AFNOR XP E05-030-1, 2003; JIS B 0601, 2013; BS 
1134, 2010; DIN 4768, 1990). They can be classified into four categories: amplitude, shape, spatial and 
hybrid parameters. The root mean square roughness Rq is the second most common amplitude param-
eter: it is more sensitive to occasional high peak or deep valley. Similarly the total height of the profile 
is affected by each local damage. Among shape parameters some application requires the indication 
of skewness and kurtosis. They can be calculated as the third and fourth central moment of the pro-
file amplitude density function: they are indicators of symmetry and flatness of the amplitude density 
distribution, respectively. The spatial parameters allow to have a measure of peak width, which gives 
information about surface functionality such as waterproof and airproof capacity. Hybrid parameters 

27
Surface Characterization in Fused Deposition Modeling
﻿
are useful indicators in a number of areas such as tribology, elastic contact, thermal conductance and 
reflexivity material properties (Whitehouse, 2011). They are analytical defined by the absolute mean 
and the root mean square slope of the profile (ISO 4287, 1997).
Table 1 shows some representative parameters. According to the international standard, the numerical 
evaluation of roughness measurement, e.g. contact stylus ones, is always preceded by removal of waviness 
and shape from the measured profile. Analog filters have been ousted by digital ones, such as RC-PC, 
Gaussian and corrected Gaussian filters (ISO/DTS 16610-22, 2006). The separation between waviness 
and roughness is obtained by an accurate selection of the cutoff. According to (ISO 4287, 1997), five 
cutoff lengths are used for the assessment, typically depending upon the profile spacing. An uncorrected 
value of cutoff leads to an uncompleted or waviness-added roughness profile. FDM measurements are 
unusual respect to those of the other technologies: as it will be shown in the next Section, the spacing 
varies of one order of magnitude in the same object. Hence particular attention must be paid to FDM 
Table 1. Definition formulas of roughness parameters
Roughness Parameter
Definition
Average roughness Ra
Arithmetic mean of the absolute height of the profile 
1
0
L
y dx
L
∫
Root mean square roughness Rq
Root mean square of the height of the profile 
1
2
0
L
y dx
L
∫
Amplitude and shape 
parameters 
Maximum height of the profile Rt
Total height of the assessed profile 
maxi yi - mini yi
Skewness Rsk
Third central moment of profile amplitude probability density 
function 
1
3
3
0
LR
y dx
q
L
∫
Kurtosis Rku
Fourth central moment of profile amplitude probability density 
function 
1
4
4
0
LR
y dx
q
L
∫
Spacing parameters 
Mean spacing between peaks S
Mean width of profile elements of the assessed profile 
1
1
n
Si
i
n
=∑
Hybrid parameters 
Mean slope of the profile Δa
Average of the absolute value of the slope of the roughness 
profile 
1
0
L
dy
dx dx
L
∫
Root mean square slope of the profile Δq
Average of the absolute value of the slope of the roughness 
profile 
1
0
2
L
dy
dx dx
L





∫

28
Surface Characterization in Fused Deposition Modeling
﻿
profile in order to obtain accurate measure over intricate 3D surfaces. Limitations arise also from the 
measurement length which can reach more than 10 mm for 2.5 mm cutoff: due to the resulting high peak 
to valley value, many profilometer systems cannot measure such a length.
Characterization of FDM surface finish has received considerable attention in recent years but the 
focus has been limited to average roughness. Also the modeling has been addressed only to this parameter.
A different way to predict surface roughness characterization of FDM specimen has been recently 
adopted. It is based on the mathematical formulation of the roughness profile itself: this methodology 
opens the possibility to achieve every roughness parameters by simply calculating it on the basis of the 
predicted profile. In this chapter this new characterization approach is presented.
SURFACE CHARACTERIZATION
FDM Theoretical Profile Model
The new methodology consists in the development of a geometrical representation of the roughness 
profile. In Figure 2 a schematization of FDM filament is reported.
The versor bis the direction of stratification: onto the plane Γ, parallel to the working plate, the lay-
ers deposited along this direction. The filament is deposited following the toolpath, which has an instan-
taneous direction d , namely deposition direction. The envelope of the filament is the resulting surface 
which has a normal ˆn . The angle between band ˆn is the deposition angle α.
The profilometer measurements are typically taken in a plane perpendicular to the deposition direc-
tion d , that is parallel to Ω plane. It must be highlighted that the amplitude parameters such as Ra, Rq, 
Figure 2. Geometrical representation of roughness profile

29
Surface Characterization in Fused Deposition Modeling
﻿
Rsk do not depend upon the measurement direction except for measures taken along the filament itself. 
As a rule an anisotropic texture is measured perpendicular to the direction of this anisotropy. For spac-
ing and hybrid parameters the direction of the measurement takes an important role and then it must be 
considered in a model formulation. A measurement angle γ is thus introduced as the angle between the 
direction of the profilometer measure and Ω plane.
Let’s consider the measures with a measurement direction γ equal to zero. A first assumption is to 
consider profile peaks as a sequence of circumference’s arcs of radius r spaced by f width, according 
to Equation 1.
x
y
r
x
f
f
y
2
2
2
2
2
0
+
=
∈−
>
,
[
/ ,
/ ]
,
	
(1)
The spacing f is known only for vertical wall, i.e. α equal to 90°. For such an angle the spacing is 
forced to be equal to the layer thickness. In this case the radius r is supposed to be one half of this layer 
thickness.
For surfaces inclined by a different α angle we assume that the Equation 1 still describes this behaviour 
with the width and the radius projected by α angle itself according to Equation 2 and 3.
r
L
=
⋅
/
csc
2
α 	
(2)
f
L
=
⋅cscα 	
(3)
These formulations and assumptions were verified in (Boschetto, Giordano, & Veniali, 2011a; 
Boschetto, Giordano, & Veniali, 2011b) by experimental investigations. A tube-shaped specimen has 
been designed to investigate the effect of deposition angle and layer thickness. The samples have been 
fabricated in ABS (with tensile strength of 22 MPa, tensile modulus of 1.6 GPa, glass transition tem-
perature at 104°C) by a Stratasys Dimension BST 768. The chamber and head temperatures have been 
fixed at 74 °C and 270 °C respectively. The roughness measurements have been achieved by a contact 
profilometer Taylor Hobson Form Talyprofile Plus instrumented with a prolonged stylus in order to cover 
the huge FDM peak to valley. The deposition angle has been measured in the range 0-180° with a very 
little step of 3° while the layer thickness levels have been 0.254 mm and 0.331 mm. Measured profile 
has been segmented obtaining a sub-profile per each filament. Each sub-profile has been measured to 
achieve f and regressed by means of a circular regression to test the hypothesis. The results, reported in 
Figure 3a, confirmed very well the circular assumption but Equation 2 and 3 showed a validity limited 
to about 30-150°: outside this range the model diverges. The range has been found applying the Akaike 
Information Criterion (AIC) (Burnham & Anderson, 2002), a tool for model selection, i.e. it can rank 
different models by giving a relative measure of goodness of fit. In this case it has been employed by 
varying the domain and considering the same model equation.
Now let’s consider a measurement direction not parallel to Ω plane and defined by γ angle. In this 
case the circumference’s arc is stretched along measurement direction and results in an ellipse’s arc. The 
new geometry can be defined by Equation 4.
y
r
x
r
x
f
Cos
f





+





=
∈−
⋅
2
2
1
2
2
/ cos
,
[
/ (
),
/ (
γ
γ
⋅⋅
>
Cos
y
γ)]
,
0	
(4)

30
Surface Characterization in Fused Deposition Modeling
﻿
Solving Equation 4 with respect to y variable and computing a second-order Taylor series expansion 
limited to the second order (this is dictated by the need to symbolical integrate the profile equation), 
Equation 5 can be obtained.
y
r
x
r
≅
−
⋅
(
)
cos γ
2
2
	
(5)
According to ISO Standard (ISO 4287, 1997), the reference mean line y  is calculated as the least 
squares mean line, as reported in Equation 6.
∂
−
∂
=
−∫(
)
/
/
y
y dx
y
f
f
2
2
0	
(6)
The mean value, i.e. the solution of this equation, in the case of generic α and γ, is:
y
f
ydx
f
r
r
L csc
f
f
=
= −
+
=
⋅
−∫
1
24
5
12
2
2
2
/ cos
/( cos )
/( cos )
γ
α
γ
γ
	
(7)
The compensated profile can now be obtained:
Figure 3. Experimental data and circular fitted model for different value of the stratification angle (a), 
experimental and predicted values of radius (b), and spacing (c) and model domain (dashed lines)

31
Surface Characterization in Fused Deposition Modeling
﻿
y
y
y
f
x cos
r
c =
−
=
−
2
2
2
12
24
γ 	
(8)
Replacing Equation 2 and 3 in Equation 8, the compensated height of the profile can be expressed 
as a function of process parameters.
y
L
sin
cos
sin
L
x
x
S
S
c =
−
⋅
∈−
12
2
2
2
2
α
γ
α
(
)
,
[
/ ,
/ ]	
(9)
where S is the spacing of the profile defined by:
S
f
L
=
=
⋅
/ cos
/ (sin
cos )
γ
α
γ 	
(10)
A sequence of shifted peaks can be considered as full profile.
y
L
sin
cos
sin
L
x
x
S S
nS
n
n
c =
−
−
(
)
∈−
=
∈
∈
12
2 2
0
2
2
α
γ
α
ξ
ξ
(
)
,
[
,
] ,
,
,
[ ,int
N
+ 





L
S
m ]	
(11)
Equation 11 provides an analytic representation of roughness profile as a function of process param-
eters and measurement direction. By this way it is possible to have a prevision of the entire surface of 
a prototype before the fabrication and the managing the parameters. In CAM environment the operator 
can choose in advance layer thickness and, above all, determine the part orientation in order to match 
the best solution within roughness requirements. Some examples, in which the orientation of the part 
is a central key of this technology, are reported in the following Section. The modeled profile is shown 
in Figure 4: the superimposition with actual profiles highlights the agreement in different geometry 
conditions and measurement directions.
As mentioned in this paragraph, the Equation 11 permits to calculate every roughness parameter 
coming from profilometer measurements. In Table 2 the symbolic calculus and the FDM prediction 
formula, according to (Gadelmawla, Koura, Maksoud, Elewa & Soliman, 2002), are reported for some 
parameters. The formulation of geometrical profile allows to achieve light analytical expressions also 
for roughness parameters defined by complicate formula.
As expected for surfaces of the same shapes, Rsk and Rku do not depend upon process parameters or 
measurement direction. Other amplitude parameters depend only upon L and α values; on the contrary 
hybrid parameters depend only upon the γ angle, because they are calculated as a ratio between two 
spatial terms.
In (Boschetto, Giordano, & Veniali, 2013a) an experimental campaign has been conducted by vary-
ing L, α and γ. A suitable geometry has been designed considering a twisted pillar composed by tilted 
cubes15 mm in length, angled by by a constant increment of 3° in a range from 0° to 180°. This allowed 
to achieve 3D measures over the face of each cube by profilometer. The instrument has been a Taylor 
Hobson Talysurf Profile Plus: a cut-off of 2.5 mm and a measurement length of 12.5 mm have been 
chosen. Data have been filtered according to ISO/DTS 16610-22 (2006). In Figure 5 some roughness pa-
rameters, taken from very different areas, are reported together the models obtained by theoretical profile.

32
Surface Characterization in Fused Deposition Modeling
﻿
Figure 4. Roughness profile for different layer thickness and deposition angle: experimental data (con-
tinuous line) and modelled data (dashed line)
Table 2. Symbolic calculus and FDM prediction of roughness parameters
Roughness Parameter
Symbolic Calculus
FDM Prediction
Average roughness Ra
 
f
r
2
18 3
 
L cscα
9 3
Root mean square roughness Rq
 
f
r
2
12 5
 
L cscα
6 5
Maximum height of the profile Rt
 f
r
2
8
 
L sinα
4
Skewness Rsk
 −2 5
7
 
−2 5
7
Kurtosis Rku
 15
7  
15
7
Mean spacing between peaks S
 S
f
=
/ cos γ  
S
L
= sin
cos
α
γ
Mean slope of the profile Δa
 cos γf
r
4
 
cos γ
2
Root mean square slope of the profile Δq
 
f
r
2
2
2 3
cos γ
 
cos γ
3

33
Surface Characterization in Fused Deposition Modeling
﻿
The experimental data claim that models give an accurate prediction for a wide range of α and γ. It 
is interesting to notice that experimental shape parameters confirm the assumption of same shape of the 
profile. Some problems are encountered for near horizontal walls and a measurement direction along 
the filament axis. The latter one is an expected consequence of FDM surface anisotropy since, along this 
direction, the profilometer cannot acquire an adequate number of peaks and valleys. When the deposition 
angle decreases several phenomena must be considered. In the paragraph model limitations these aspect 
will be discussed and analysed. Nevertheless the model is very reliable in the range of deposition angle 
Figure 5. Measured and predicted roughness parameters in comparison: Ra vs α (a), Ra vs γ (b), Ra (c), 
Rt (d), Rsk (e), Rku (f), S (g), Δa (h) as a function of α and γ

34
Surface Characterization in Fused Deposition Modeling
﻿
of [25°, 155°] and for measurement angle smaller than 80°. In this domain the amplitude parameters 
can be used as a tool for accomplishing design requirements. Furthermore hybrid parameters are finding 
interest in quantifying part properties, such as reflectivity and aesthetics. The related models show a very 
good accordance confirming that this new methodology is a generalization of previous works and allows 
to extend the capability to have, in advance, information about the effects of manufacturing choices.
Part Orientation Methods
In this paragraph some examples of part orientation based on this theoretical approach are reported. The 
main advantage of this methodology is the possibility to develop new formulations, coming from the 
theoretical profile, in order to obtain customized optimizations or specific evaluations.
As a first method, let’s suppose that a component specification requires a maximum roughness param-
eter value on a indicated zone. In Figure 6 a human head, acquired by a reverse engineering technique, 
is presented. The required specification over the face is an Rt < 100 μm on the evidenced area.
The source data are in STL file format. We need to know the normal direction nin each point of the 
surface in order to obtain the deposition angle. The STL file is a tessellated surface composed by tri-
angles (Figure 6a). In this case the normal direction of each triangle can be calculated and αi can be 
obtained by the following vector cross product:
αi
i
i
i
i
i
i
i
i
d
p
p
p
p
p
p
p
p
=
×
−
∧
−
−
∧
−




arccos
(
)
(
)
(
)
(
)

1
2
3
2
1
2
3
2


	
(12)
where pki is the triplet of points (k=1,2,3) of the i-th triangle.
By applying Rt model to each triangle αi it is possible to achieve the fraction of point complying with 
the specification. An optimization can be now performed. The Figure 6b show several part orientation 
of the component: the maximum is obtained at 104° respect to the initial positioning.
Moreover an interesting tool to investigate the theoretical surface quality is a graphical representation 
of the predictive results. Some attempts have been made to plot the roughness attainable. In (Campbell, 
Martorelli, & Lee, 2002) a visualization system has been employed to allow the user to compare the 
effect of different part built orientations. This visualization is only qualitative and gives an aesthetic 
acceptability. A different approach can be now considered by mapping the quantitative results onto the 
virtual object. The Figure 6c and Figure 6d show the quantitative representation of the Rt prediction 
for original and optimized orientations, respectively. This method allows the operator to determine, at 
a glance, critical and unacceptable zones in a so difficult 3D object. In Figure 6e and Figure 6f some 
details of the mouth are reported: it is well evident the improvement obtained by this method.
Let’s now suppose that, in a product development stage, the designers possess the sets of the equa-
tions that describe a 3D surface. This is typical in a parametric formulation of the object to fabricate.
As an example in Figure 7 surface defined by the following equation:
x u v
u
v
y u v
u
v
z u v
u
v
( , )
cos
( , )
sin
( , )
(
) /
=
⋅
⋅
=
⋅
⋅
=
−





10
10
100
4

∈−
∈
,
[
, ] ,
[ ,
]
u
v
π π
π
π
2
	
(13)

35
Surface Characterization in Fused Deposition Modeling
﻿
The normal direction can be found by the normalized vector cross product is a function of u, v co-
ordinates:
n
u v
Pu u v
Pv u v
Pu u v
Pv u v
Pu
p

 
 
 
 
 
( , )
( , )
( , )
( , )
( , )
,
(
=
∧
∧
u v
x u v
y u v
z u v
Pv u v
x u v
y
u
u
u
v
v
, )
{
( , ),
( , ),
( , )}
( , )
{
( , ),
= ∂
∂
∂
= ∂
∂
 
( , ),
( , )}
u v
z u v
v
∂





	
(14)
Figure 6. STL file of human head (a), fraction of points complying with specification as a function of 
part orientation (b), representation of modeled Rt and corresponding details for original (c, e), and 
optimized orientation (d, f)

36
Surface Characterization in Fused Deposition Modeling
﻿
The deposition angle α can be calculated as the scalar product between the deposition direction d
and the normal direction n p
 .
α( , )
arccos
( , )
u v
d
n
u v
p
=
×
(
)


	
(15)
The surface visualization of predicted values of Ra can be now developed. It is evident the effect 
produced by a different orientation of the part. The physical object (this surface is not a solid) must 
be obtained by the thickening of such surfaces, e.g. by an adequate mesh choose (Boschetto &Veniali, 
2010) and an error-free offset (Boschetto & Bordoni, 2012). Also in this case part orientation markedly 
modifies surface textures (Figure 7c and Figure 7d).
The method can be applied also when there are multiple specifications. In the door handle reported in 
Figure 8a three design aspects must be accomplished: a cylindrical part must have an Ra< 50 μm for the 
functionality of the joint pin; ergonomics requirements call for an Rq< 60 μm in such zones where the 
hand grabs the object; aesthetics purpose requires Δq< 2 in the front side. In this case a joined optimiza-
tion of three roughness models, with the given constraints, leads to the orientation reported (Figure 8b).
Model Limitation
In Section Introduction several materials have been listed but the proposed model does not take into 
account this aspect. Literature has not still investigated the effect of the material on surface roughness. 
In Boschetto, Giordano, and Veniali (2013a) an experimental on deeply different materials, by means 
of their properties, have been performed: ABS, ABSPlus, Polycarbonate, ULTEM 9085. In Table 3 the 
mechanical and thermal properties of these materials are reported. ABS has been used in the previous 
analysis and modelling. ABSPlus, a commercial variation of ABS, shows an increasing of mechanical 
properties, such as tensile and flexural strength and tensile modulus, 50% more than standard ABS 
but same glass transition. The polycarbonate has been chosen for its different flexural strength, heat 
Figure 7. Representation of modeled Ra and corresponding details for original (a, c) and modified 
orientation (b, d)

37
Surface Characterization in Fused Deposition Modeling
﻿
deflection and glass transition. Finally the Ultem 9085 has been used for its thermal properties (it is 
flame retardant). The process parameters to fabricate these materials are also different: the chamber 
temperature varies from 74 °C for ABS and ABSPlus to 110°C for ULTEM; for the same materials the 
head temperature is 270°C and 380°C.
The ABS specimens have been fabricated by a Dimension Bst 768, the ABSPlus and polycarbonate 
ones by a Fortus 360mc, a Fortus 400mc has been used for ULTEM9085. This scenario has been analysed 
by means of roughness measurements: materials showed a variation of the order of 10% for amplitude 
parameters which can be considered negligible for surface measurements (Whitehouse, 2011); a wider 
scattering can be noticed for deposition angle greater than 160°, especially for polycarbonate. Spac-
ing and shape parameters show variations smaller than 8% while hybrid parameters do not point out a 
dependency from the material. The field of materials currently used for FDM is almost unexplored by 
means of surface characterization. At present, experienced effects do not play a decisive role on rough-
ness parameters modifications.
A limitation for presented model lie in applicability to the entire domain of measurement and deposi-
tion angles. As mentioned before the theoretical profile model is reliable in the α domain [25°, 155°]. 
From a mathematical point of view, this is due to the assumptions made in Equation 2 and 3, as the model 
tends to infinity in correspondence of α = 0° and α = 180°. Moreover in correspondence of horizontal 
and near horizontal walls, several phenomena take place. A deeper investigation can be performed by 
microscopic analysis. Some studies have been focused on specific aspects related to the deposited fila-
ment sections. In this type of research some problems arose. Firstly, the measurements of the sections 
are limited by the difficulty to cut the filaments without damaging the structure: a typical problem is 
Figure 8. Representation of modeled Ra for original (a) and optimized orientation (b)
Table 3. Materials datasheet
Mechanical and 
thermal properties
ABS
ABSPlus
Polycatbonate 
PC-ISO
ULTEM 9085
Tensile strength
22 Mpa
36 Mpa
57 Mpa
72 Mpa
Tensile modulus
1627 MPa
2400 MPa
2000 MPa
2200 MPa
Flexural strength
41 MPa
61 MPa
90 MPa
115 MPa
Flexural modulus
1834 MPa
2300 MPa
2100 MPa
2500 MPa
Glass transition
104°
108°
161°
186°

38
Surface Characterization in Fused Deposition Modeling
﻿
the delamination and the deconfiguration due to the low resistance of interlaminar elements respect to 
the filament itself. In the past, the investigations have been rarely performed considering several values 
of deposition angle. It plays an important role on section aspect ratio. The geometry of the deposited 
filament section is the result of a plastic deformation induced by the moving head that spreads the ma-
terial onto the previous layer. The positions of underlying filaments affect the geometry of the section. 
These aspects have been observed by Boschetto, Giordano, and Veniali (2011a): a very slow cut has 
been performed in order to avoid material delamination. The samples have been incorporated in resin 
and cut, perpendicularly to the deposition direction, by a metallographic precision cutoff machine. The 
section have been analyzed and compared with the profiles acquired by contact measurements for differ-
ent process conditions and deposition angles. In Figure 9 some of these experimental sets are reported 
and the following observations can be made:
•	
A markedly variation of section height to width ratio is present and varies with deposition angle 
(Figure 9a-e), thus confirming a relevant effect induced by this parameter;
Figure 9. Roughness profiles and filament section macrographs for different fabrication conditions (a-g) 
and SEM photos (h, i) of specimen surfaces

39
Surface Characterization in Fused Deposition Modeling
﻿
•	
Also the spacing varies with the deposition angle (Figure 9a-g); it is well evident the increasing 
of the width between peaks as the slope reduces, according to the previous theoretical and experi-
mental data;
•	
Overhanging surfaces, which have been in contact with support structures, present sharpened 
profiles (Figure 9e and Figure 9g);
•	
Horizontal surfaces have flat profiles (Figure 9f and Figure 9g).
Scanning Electron Microscope (SEM) analysis can explain some aspects relative to the last two ob-
servations. Overhanging surfaces are supported by structures previously deposited: the output texture 
depends upon the geometry of the part to be supported and upon the support generation criteria. The 
supporting structures touch the part surfaces in some points: this explains the local deformations of the 
supported filaments, as shown in Figure 9h. Since this damage is spatially occasional, a profilometer 
measurement can achieve different profiles depending upon the stylus path. The profile reported in Fig-
ure 9e is, in this case, sharpened and it relative to deformed filaments. Moreover, this resulting surface 
presents different properties in terms of mechanical coupling and tribology aspects. The model does 
not take into account this problem.
Figure 9i shows surfaces with low values of deposition angle. For these near horizontal walls, the 
upper model contour does not cover the underlying one, revealing the inner model. For deposition angle 
of 0° and 180° the surface is entirely composed by interior filling.
These gaps are not considered in the model, but in CAM of FDM the operator needs to know the 
surface quality output in an early stage, i.e. when he has only chosen part orientation and layer thickness 
and support and model strategies are not clear yet. The proposed methodology is thus a tool to charac-
terize in advance the surface quality by means of all kind of roughness parameters. Furthermore it is 
possible to achieve new customized parameters in order to comply with a specific industrial problem.
Anyway some improvements have been made to the mentioned models. The described difficulties to 
theoretical understand the phenomena outside the model domain, addressed the research toward expert 
systems, such as neural network. Typically used to find the surface roughness relationship with process 
parameters in many technologies (Pontes, Ferreira, Silva, Paiva, & Balestrassi, 2010), they present many 
attractive properties such as resistance to noisy or missing data, accommodation of multiple variables 
and their unknown interactions.
In (Boschetto, Giordano, & Veniali, 2013b; Boschetto, Bottini, Lettina, & Veniali, 2013) Feed For-
ward Neural Network (FFNN) have been employed to extend the model all over the range of the deposi-
tion angle. Tailored samples have been designed in a systematic procedure allowing to supply adequate 
data sets for neural network training and validation stages. The FFNN architectures has been developed 
choosing network parameters on the basis of an evaluation function. The number of neurons in the hid-
den layer and the activation function have been so determined. For example, Ra has been modeled by an 
architecture with hyperbolic tangent and eight neurons in the hidden layer.
Ra = −
+
+
−

−
6118
1766 Tanh 15 67
16 
49 L
2
4 Tanh 11
.
.
.
.
0 0
0 0
00
α
.
.
.
.
.
.
.
54
80 
45 L
24 87 Tanh 17 13
44 
42 
−
−

+
−
−
−
0 0
0 0
0 0
0 0
α
α
L
1131 Tanh 1 33
19 
34 L
55 1 Tanh 2

−
+
−

+
0
0 0
0 0
0
0
.
.
.
.
α
84
56 
2  L
195 5 Tanh 4 559
62 
18 L
−
−

−
+
−

0 0
0 0 0
0 0
0 0
.
.
.
.
.
.
α
α

+
−
−
−

+
1 13 Tanh 2
9
88 
18 L
116 1 Tanh 21
0
0 0
0 0
0 0
.
.
.
.
.
α
.
.
.
66
1 5 
13 L
−
−


0 0
0 0
α
	
(16)

40
Surface Characterization in Fused Deposition Modeling
﻿
Equation 16 shows the model: it must be pointed out that this equation presents high sensitivity to 
input variable variation and thus it is less robust than one obtained by previous theoretical approach. 
Notwithstanding the critical zones in the ranges 0-30° and 150-180° of the deposition angle were modeled 
in a satisfactory way, finding several formulations for roughness parameters. Moreover the validation 
in this work shows an important extensibility of model employed. Several conditions have been consid-
ered: different materials, different prototyping systems and different geometry complexity. The models 
confirmed their reliability for all these combinations. It has been concluded that, in critical zones, many 
phenomenon are significant in the surface quality output but their effects are not prominent.
In conclusion the models achieved by neural network are a completion of the models found by theo-
retical profile approach. Nevertheless this method is limited to parameters already modeled and does 
not allow to develop new ones. Moreover the characterization of surface topography, by the theoretical 
model, permits to investigate research fields related to tribological aspects, mechanical coupling, fluid 
dynamics proprieties. In the next paragraph, an implementation of this emerging methodology, applied 
to a hybrid operation is reported.
FUTURE RESEARCH DIRECTIONS
As discussed before, the theoretical profile approach, presented in this chapter, can be usefully employed 
to design a customized characterization of surface output as a function of process parameters. Thus it can 
be used to investigate secondary operations such as finishing ones. A variety of methods are currently 
under study for FDM part surface roughness reduction. They can be divided in three categories: Computer 
Numerical Control (CNC) machining, chemical method, mass finishing. CNC milling (Pandey, Reddy, 
& Dhande, 2003b) can improve surface roughness but it is limited by tool size (some features could be 
inaccessible) and, when complex objects have to be realized, this approach is time-consuming because 
it needs a machine setup and CNC code generation. Other machining operations such as abrasive flow 
machining (Williams & Melton, 1998) and abrasive jet deburring (Leong, Chua, & Chua, 1998) presents 
the same problems. Stratasys, the producer of FDM apparatus, has developed a semi-automated finishing 
system for ABS parts (Espalin, Medina, & Wicker, 2009): this method needs human intervention and 
a post curing phase. Chemical finishing has recently received great effort: Galantucci, Lavecchia, and 
Percoco (2009, 2010) employed a solution of dimethylketone to attack the external filaments without 
affect object shape. At present there is no operative knowledge about achievable surfaces.
The last category is the mass finishing. These operations are currently used in industry and in AM 
services. A recent work (Boschetto, Bottini, Monarca, & Veniali, 2013) developed a theoretical profile 
geometry, based on FDM theoretical formulation, to determine the surface quality output achievable by 
the hybridization of FDM operation and barrel finishing. Moreover by the presented methodology it has 
been possible to predict tolerance attainable by the hybrid operation. By this way a geometrical profile 
evolution as a function of working has been provided: this allows to determine when finishing operation 
must be stopped to accomplish the requirements still saving time and cost.

41
Surface Characterization in Fused Deposition Modeling
﻿
CONCLUSION
FDM is an AM technology which permits to fabricate complex functional components, but its indus-
trial diffusion is limited because the prediction of attainable roughness is not a clear aspect. Previous 
investigation focused on the development of a model for average surface roughness parameter. This 
chapter presented a new methodology based on the mathematical prevision of the roughness profile as 
a function of process parameters and part geometry. Two dimensional formulation has been validated 
by specific profilomenter analysis which confirmed the assumptions the profile is a sequence of circular 
arcs spaced by a particular width: both these elements have been related to deposition angle and layer 
thickness in a wide range. A three dimensional formulation has been introduced to extend the geometrical 
description to spatial and hybrid roughness parameters, which depend upon the measurement direction. 
By symbolic calculus light analytical expression of several roughness parameters have been achieved. 
The validation by a number of 3D data showed a great accordance in a wide range of process param-
eters and measurement direction. As a consequence all kind of surface definition can be implemented: 
furthermore, specific aspects, coming from industrial needs, can call for customized parameters which 
can be now modeled. This flexibility opens to a wide range of applications: the process managers can 
have, in advance, all the possible scenarios to produce a specific component with precise requirements 
to be satisfied. This eliminates current trial and error methods. A key aspect of FDM, but also of AM, 
is the determining of part orientation. Several problems arise from this CAM operation, such as the 
resulting part anisotropy (it is sliced and built in layer) and support structures. Thus a tool to character-
ize, in advance, the surface attainable is essential. Three case studies have been considered to highlight 
the possibility to determine part orientation accomplishing specific requirements. In the first case, the 
performed optimization allowed to find the stratification direction which better improves the surface 
quality of the desired zone. A suitable graphical representation permits to investigate, at a glance, the 
critical areas and direct specific operations. The second case shows how the method can be adopted 
when the components is defined by mathematical formulation. The third case points out the possibility 
to optimize multiple requirements, simultaneously.
The limitations of the profile model have been analyzed. Microscopic, profilometer and SEM mea-
surements have been used to identify several phenomena which explain the discrepancy in the critical 
zones. The use of FFNN demonstrates that, outside the model domain, improved formulations of some 
roughness parameters can be found, but the flexibility of the approach is lost.
The theoretical profile can also be employed to study the finishing operations which could be necessary 
after the prototype fabrication. Future research opportunities regard the implementation of this approach 
in paradigms which comprehends the integration of such a technology with other ones or, furthermore, 
their hybridization. An attempt has been already made with barrel finishing: the FDM theoretical profile 
has been used as a basis to predict the working time needed to reach a specified surface requirements in 
terms not only of roughness but also tolerances.
REFERENCES
AFNOR XP E05-030-1. (2003). Spécification géométrique des produits (GPS) - État de surface: surfaci-
que - Partie 1: Termes et définitions et paramètres surfaciques. Association Française de Normalisation.

42
Surface Characterization in Fused Deposition Modeling
﻿
Ahn, D. K., Kim, H., & Lee, S. (2007). Fabrication direction optimization to minimize post-machining 
in layered manufacturing. International Journal of Machine Tools & Manufacture, 47(3-4), 593–606. 
doi:10.1016/j.ijmachtools.2006.05.004
Ahn, D. K., Kim, H., & Lee, S. (2009). Surface roughness prediction using measured data and inter-
polation in layered manufacturing. Journal of Materials Processing Technology, 209(2), 664–671. 
doi:10.1016/j.jmatprotec.2008.02.050
Ahn, D. K., Kweon, J. H., Kwon, S., Song, J., & Lee, S. (2009). Representation of surface roughness 
in fused deposition modelling. Journal of Materials Processing Technology, 209(15-16), 5593–5600. 
doi:10.1016/j.jmatprotec.2009.05.016
Anitha, R., Arunachalam, S., & Radhakrishnan, P. (2001). Critical parameters influencing the quality of 
prototypes in fused deposition modelling. Journal of Materials Processing Technology, 118, 385–388. 
doi:10.1016/S0924-0136(01)00980-3
Armillotta, A. (2006). Assessment of surface quality on textured FDM prototypes. Rapid Prototyping 
Journal, 12(1), 35–41. doi:10.1108/13552540610637255
ASME B46.1. (1995). Surface texture, surface roughness waviness and lay. American Society of Me-
chanical Engineers.
Boschetto, A., & Bordoni, M. (2012). Thickening of surfaces for direct additive manufacturing fabrica-
tion. Rapid Prototyping Journal, 18(4), 308–318. doi:10.1108/13552541211231734
Boschetto, A., Bottini, L., Lettina, F., & Veniali, F. (2013). Neural network application to FDM surface 
roughness prediction. In Proceedings of Enhancing the Science of Manufacturing proceedings of the XI 
A.I.Te.M. Conference (pp.79-92). San Benedetto del Tronto, Italy: AITeM.
Boschetto, A., Bottini, L., Monarca, F., & Veniali, F. (2013). Barrel finishing of FDM parts. In Proceed-
ings of Enhancing the Science of Manufacturing proceedings of the XI A.I.Te.M. Conference (pp.29-40). 
San Benedetto del Tronto, Italy: AITeM.
Boschetto, A., Giordano, V., & Veniali, F. (2011a). Modelling micro geometrical profile in fused de-
position process. International Journal of Advanced Manufacturing Technology, 61(9-12), 945–956. 
doi:10.1007/s00170-011-3744-1
Boschetto, A., Giordano, V., & Veniali, F. (2011b). Roughness prediction in fused deposition modelling. 
In Proceedings of Enhancing the Science of Manufacturing proceedings of the X A.I.Te.M. Conference 
(pp.183-187). Naples, Italy: AITeM.
Boschetto, A., Giordano, V., & Veniali, F. (2013a). 3D roughness profile model in fused deposition 
modelling. Rapid Prototyping Journal, 19(4), 240–252. doi:10.1108/13552541311323254
Boschetto, A., Giordano, V., & Veniali, F. (2013b). Surface roughness prediction in fused deposition 
modelling by neural networks. International Journal of Advanced Manufacturing Technology, 67(9-12), 
2727–2742. doi:10.1007/s00170-012-4687-x
Boschetto, A., & Veniali, F. (2010). Intricate shape prototypes obtained by FDM. International Journal 
of Material Forming, 3(1), 1099–1102. doi:10.1007/s12289-010-0963-1

43
Surface Characterization in Fused Deposition Modeling
﻿
Bramley, A., Brissaud, D., Coutellier, D., & McMahon, C. (2005). Advances in integrated design and 
manufacturing in mechanical engineering. New York: Springer Verlag. doi:10.1007/1-4020-3482-2
BS 1134. (2010). Assessment of surface texture: Guidance and general information. British Standards 
Institution.
Burnham, K. P., & Anderson, D. R. (2002). Model selection and multimodel inference: A practical 
information-theoretic-approach. New York: Springer Verlag.
Campbell, R. I., Martorelli, M., & Lee, H. S. (2002). Surface roughness visualization for rapid prototyp-
ing models. Computer Aided Design, 34(10), 717–725. doi:10.1016/S0010-4485(01)00201-9
Chua, C. K. (1994). Three-dimensional rapid prototyping technologies and key development area. Com-
puting & Control Engineering Journal, 5(4), 200–206. doi:10.1049/cce:19940407
Chua, C. K., Leong, K. F., & Lim, C. S. (2010). Rapid prototyping: principles and applications. River 
Edge, NJ: World Scientific. doi:10.1142/6665
DIN 4768. (1990). Determination of roughness parameters Ra, Rz, Rmax by means of stylus instruments, 
terms, measuring conditions. Deutsches Institut für Normung.
Espalin, D., Medina, F., & Wicker, R. (2009). Vapor smoothing: A method for improving FDM-manufactured 
part surface finish. Internal Report of the W.M. Keck Center for 3D Innovation. University of Texas.
Gadelmawla, E. S., Koura, M. M., Maksoud, T. M. A., Elewa, I. M., & Soliman, H. H. (2002). Rough-
ness parameters. Journal of Materials Processing Technology, 123(1), 133–145. doi:10.1016/S0924-
0136(02)00060-2
Galantucci, L.M., Lavecchia, F., & Percoco, G. (2009). Experimental study aiming to enhance the surface 
finish of fused deposition modeled parts. CIRP Annals - Manufacturing Technology, 58(1), 189-192.
Galantucci, L.M., Lavecchia, F., & Percoco, G. (2010). Quantitative analysis of a chemical treatment to 
reduce roughness of parts fabricated using fused deposition modeling. CIRP Annals - Manufacturing 
Technology, 59, 247-250.
Gebhardt, A. (2003). Rapid prototyping. Munich, Germany: Hanser. doi:10.3139/9783446402690
Hopkinson, N., Hague, R., & Dickens, P. (2005). Rapid manufacturing: An industrial revolution for the 
digital age. Chichester, UK: John Wiley & Sons. doi:10.1002/0470033991
Ingole, D. S., Kuthe, A. M., Thakare, S. B., & Talankar, A. S. (2009). Rapid prototyping - A technol-
ogy transfer approach for development of rapid tooling. Rapid Prototyping Journal, 15(4), 280–290. 
doi:10.1108/13552540910979794
ISO 4287. (1997). Geometrical product specification (GPS) – Surface texture: Profile method – Terms, 
definition and surface texture parameters. Geneva: International Organization for Standardization (ISO).
ISO/DTS 16610-22. (2006). Geometrical product specifications (GPS)–Filtration–linear profile filters: 
Spline filters. Geneva: International Organization for Standardization (ISO).

44
Surface Characterization in Fused Deposition Modeling
﻿
Ivanova, O., Williams, C., & Campbell, T. (2013). Additive manufacturing (AM) and nanotechnology: 
Promises and challenges. Rapid Prototyping Journal, 19(5), 353–364. doi:10.1108/RPJ-12-2011-0127
JIS B 0601. (2013). Geometrical product specifications (GPS) -- Surface texture: Profile method -- Terms, 
definitions and surface texture parameters. Japanese Industrial Standards.
Kaufui, V. W., & Hernandez, A. (2012). A review of additive manufacturing. ISRN Mechanical Engi-
neering, 1–12.
Leong, K. F., Chua, C. K., Chua, G. S., & Tan, C. H. (1998). Abrasive jet deburring of jewellery models 
built by stereolithography apparatus. Journal of Materials Processing Technology, 83, 36–47. doi:10.1016/
S0924-0136(98)00041-7
Leong, K. F., Chua, C. K., & Ng, Y. M. (1996a). A study of stereolithography file errors and repair part 
1 – Generic solutions. International Journal of Advanced Manufacturing Technology, 12(6), 407–414. 
doi:10.1007/BF01186929
Leong, K. F., Chua, C. K., & Ng, Y. M. (1996b). A study of stereolithography file errors and repair 
part 2 – Special cases. International Journal of Advanced Manufacturing Technology, 12(6), 415–422. 
doi:10.1007/BF01186930
Pandey, P. M., Reddy, N. V., & Dhande, S. G. (2003a). Real time adaptive slicing for fused deposition 
modelling. International Journal of Machine Tools & Manufacture, 43(1), 61–71. doi:10.1016/S0890-
6955(02)00164-5
Pandey, P. M., Reddy, N. V., & Dhande, S. G. (2003b). Improvement of surface finish by staircase ma-
chining in fused deposition modelling. Journal of Materials Processing Technology, 1(1-3), 323–331. 
doi:10.1016/S0924-0136(02)00953-6
Perez, C. J. L., Vivancos, J., & Sebastiàn, M. A. (2001). Surface roughness analysis in layered forming 
processes. Precision Engineering, 25(1), 1–12. doi:10.1016/S0141-6359(00)00049-0
Pontes, F. J., Ferreira, J. R., Silva, M. B., Paiva, A. P., & Balestrassi, P. P. (2010). Artificial neural net-
works for machining processes surface roughness modeling. International Journal of Manufacturing 
Technology, 49, 879–902. doi:10.1007/s00170-009-2456-2
Radhakrishnan, P., Subramanyam, S., & Raju, V. (2008). CAD/CAM/CIM. New Delhi: New Age Inter-
national.
Thrimurthulu, K., Pandey, P. M., & Reddy, N. V. (2004). Optimum part deposition orientation in 
fused deposition modelling. International Journal of Machine Tools & Manufacture, 44(6), 585–594. 
doi:10.1016/j.ijmachtools.2003.12.004
Toriya, H. (2010). 3D manufacturing innovation springer: Revolutionary change in Japanese manufac-
turing with digital data. New York: Springer Verlag.
Vasudevarao, B., Natarajan, D. P., & Henderson, M. (2000). Sensitivity of RP surface finish to process 
parameter variation. In Proceedings of Solid Free Form Fabrication (pp. 252–258). Austin, TX: Aca-
demic Press.

45
Surface Characterization in Fused Deposition Modeling
﻿
Whitehouse, D. J. (2011). Handbook of surface and nanometrology. New York: CRC Press. doi:10.1201/
b10415
Williams, R. E., & Melton, V. L. (1998). Abrasive flow finishing of stereolithography prototypes. Rapid 
Prototyping Journal, 4(2), 56–67. doi:10.1108/13552549810207279
Wohlers, T. (2008). Wohlers report 2008: Executive summary. Fort Collins, CO: Wohlers Associates, Inc.
Wohlers, T. (2012). Wohlers report 2012: Executive summary. Fort Collins, CO: Wohlers Associates, Inc.
ADDITIONAL READING
Bellehumeur, C., Li, L., Sun, Q., Gu, P. (2004). Modeling of Bond Formation Between Polymer Fila-
ments in the Fused Deposition Modeling Process. Journal of Manufacturing processes, 6(2), 169-178.
Bordegoni, M., & Rizzi, C. (2011). Innovation in Product Design, From CAD to Virtual Prototyping. 
Springer.
Brajlih, T., Valentan, B., Balic, J., & Drstvensek, I. (2011). Speed and accuracy evaluation of additive 
manufacturing machines. Rapid Prototyping Journal, 17(1), 64–75. doi:10.1108/13552541111098644
Byun, H. S., & Lee, K. H. (2006). Determination of the optimal build direction for different rapid prototyp-
ing processes using multi-criterion decision making. Robotics and Computer-integrated Manufacturing, 
22, 69–80. doi:10.1016/j.rcim.2005.03.001
Cooper, K. C. (2001). Rapid Prototyping Technology. Selection and Applications. New York: Marcel 
Drekker, Inc. doi:10.1201/9780203910795
Galantucci, L.M., Lavecchia, F., Percoco G. (2008). Study of compression properties of topologically 
optimized FDM made structured parts. CIRP Annals - Manufacturing Technology, 57, 243–246.
Gibson, I. (2002). Software Solution for Rapid Prototyping. Professional Engineering Publishing.
Gibson, I., Rosen, D. W., & Stucker, B. (2009). Additive Manufacturing Technologies, Rapid Prototyping 
to Direct Digital Manufacturing. New York: Springer Verlag.
Grimm, T. (2004). User’s Guide to Rapid Prototyping. Society of Manufacturing Engineers.
Huang, X., Ye, C., Wu, S., Guo, K., & Mo, J. (2009). Sloping wall structure support generation for fused 
deposition modeling. International Journal of Manufacturing Technology, 42, 1074–1081. doi:10.1007/
s00170-008-1675-2
ISO 13565/1-2-3 (1996–1998). Geometrical product specifications (GPS) – Surface texture: Profile method, 
Surfaces having stratified functional properties - Part 1: Filtering and general measurement conditions 
- Part 2: Height characterization using the linear material ratio curve - Part 3: Height characterization 
using the material probability curve. Geneva: International Organization for Standardization (ISO).

46
Surface Characterization in Fused Deposition Modeling
﻿
ISO 25178-2 (2012). Geometrical product specifications (GPS) -- Surface texture: Areal -- Part 2: Terms, 
definitions and surface texture parameters. Geneva: International Organization for Standardization (ISO). 
Huang, X., Ye, C., Mo, J., Liu, H. (2009). Slice Data Based Support Generation Algorithm for Fused 
Deposition Modeling. Tsinghua Science and Technology, 14(S1), 223-228.
ISO/TS 16610-1 (2006). Geometric Product Specifications (GPS), Filtration – Part 1: Overview and 
basic concepts. Geneva: International Organization for Standardization (ISO).
Jin, Y., Zhang, J., Wang, Y., & Zhu, Z. (2009). Filament geometrical model and nozzle trajectory analysis 
in the fused deposition modeling process. Journal of Zhejiang University - Science A, 10(3), 370-376.
Kamrani, A. K., & Nasr, E. A. (2006). Rapid Prototyping. Theory and Practice. New York: Springer 
Verlag. doi:10.1007/b101140
Kohonen, T. (1988). An introduction to neural computing. Neural Networks, 1, 3–16. doi:10.1016/0893-
6080(88)90020-2
Leach, R. K. (2009). Fundamental Principles of Engineering Nanometrology. Amsterdam: Elsevier.
Lee, C. S., Kim, S. G., Kim, H. J., & Ahn, S. H. (2007). Measurement of anisotropic compressive 
strength of rapid prototyping parts. Journal of Materials Processing Technology, 187-188, 627–630. 
doi:10.1016/j.jmatprotec.2006.11.095
Liou, F. W. (2007). Rapid prototyping and engineering applications. A Toolbox for Prototype Develop-
ment. New York: CRC Press.
Mahesh, M., Wong, Y. S., Fuh, J. Y. H., & Loh, H. T. (2004). Benchmarking for comparative evaluation of 
RP systems and processes. Rapid Prototyping Journal, 10(2), 123–135. doi:10.1108/13552540410526999
Masood, S. H., & Song, W. Q. (2004). Development of new metal/polymer materials for rapid tooling 
using Fused deposition modeling. Materials & Design, 25, 587–594. doi:10.1016/j.matdes.2004.02.009
Muralikrishnan, B., & Raja, J. (2008). Computational Surface and Roundness Metrology. London: Springer.
Noorani, R. (2006). Rapid Prototyping - Principles and Applications. Chichester, West Sussex: John 
Wiley & Sons.
Principe, J. C., Euliano, N. R., & Lefebvre, W. C. (2000). Neural and Adaptive Systems: Fundamentals 
Through Simulations. Chichester, West Sussex: John Wiley & Sons.
Rodriquez, J. F., Thomas, J. P., & Renaud, J. E. (2000). Characterization of the mesostructure of 
fused-deposition acrylonitrile–butadienestyrene materials. Rapid Prototyping Journal, 6(3), 175–185. 
doi:10.1108/13552540010337056
Sun, Q., Rizvi, G. M., Bellehumeur, C. T., & Gu, P. (2008). Effect of processing conditions on the bonding qual-
ity of FDM polymer filaments. Rapid Prototyping Journal, 14(2), 72–80. doi:10.1108/13552540810862028
Tellis, B. C., Szivek, J. A., Bliss, C. L., Margolis, D. S., Vaidyanathan, R. K., & Calvert, P. D. (2008). 
Trabecular scaffolds created using micro CT guided fused deposition modeling. Materials Science and 
Engineering C. Biomimetic and Supramolecular Systems, 28(1), 171–178.

47
Surface Characterization in Fused Deposition Modeling
﻿
Ulrich, K. T., & Eppinger, S. D. (2000). Product Design and Development. Boston: McGraw-Hill.
Whitehouse, D. (2004). Surfaces and their measurement. London: Kogan Page Science.
Zhang, L. C., Han, M., & Huang, S. H. (2002). An effective error-tolerance slicing algorithm for STL 
files. International Journal of Manufacturing Technology, 20, 363–373. doi:10.1007/s001700200164
KEY TERMS AND DEFINITIONS
Computer Aided Manufacturing (CAM): A computer-based system which generates toolpath and 
machinery control starting from process parameters and algorithms chosen by the operator.
Deposition Angle: The angle between the stratification direction and the normal to the surface en-
veloping external filaments.
Fused Deposition Modeling (FDM): A solid-based RP technology that fabricates components using 
input material (typically thermoplastic) in filament form.
Rapid Prototyping (RP): A set of technologies that builds the parts adding or bonding material 
layer by layer, without geometry limitations.
Roughness Parameters: The values associated to particular characteristic of surface calculated on 
roughness profile data.
Stratification Direction: The direction the layers are stacked over each other, it is perpendicular to 
working plate.
Surface Roughness: The micro-geometrical variation of the profile above its actual shape and waviness.
This work was previously published in Surface Engineering Techniques and Applications edited by Loredana Santo and J. Paulo 
Davim, pages 249-280, copyright year 2014 by Engineering Science Reference (an imprint of IGI Global).

48
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  3
DOI: 10.4018/978-1-5225-1677-4.ch003
ABSTRACT
Some futuristic technologies belong to a distant time that is hard for most to imagine in palpable terms. 
In the case of 3D printing, however, any citizen can easily grasp the stakes by simply watching a brief 
video (Global News 2013). The 3D printer, whose ancestry can be traced to factory innovations of the 
1980s, provides a method of assembling objects piece by piece: a kind of less precise nanotechnology 
for the macro-level. Within a decade humans might be able to print functional body parts, bringing 
an end to the grisly waits for donated organs and the even more grisly international organ trafficking 
rings. Some believe that the “killer app” for 3D printing will be found in the printing of food, perhaps 
replacing single-site restaurants with a home library of Platonic forms of gourmet cuisine. Still others 
celebrate or fear the printing of assault rifles on a desktop, whether by right-wing citizens, convicted 
felons, or the mentally ill. One can expect the easy availability, in residential privacy, of all manner of 
benign and malignant objects, transforming economic structures, social life, and domestic security in 
roughly fifteen to twenty years.
1. ARMSTRONG ON MATERIALITY
In the past several years, nearly every conversation I have had about philosophy and technology has 
quickly turned into a discussion of the coming impact of 3D printing. Clearly, the topic is on everyone’s 
mind. Thus it is strange that as of late 2014, there is still relatively little published academic literature 
on the topic. Let’s begin our reflections with a recent article by Rachel Armstrong, one of today’s most 
diligent observers of the intersection between science, art, and architecture. The title of Armstrong’s 
article could hardly be more candid: “3D Printing will Destroy the World Unless it Tackles the Issue of 
Materiality.” (Armstrong 2014) Two questions can be asked on the basis of this title alone. First, how 
might 3D printing destroy the world? And second, what does Armstrong mean by “materiality”? Hav-
ing clarified these preliminary themes, we can ask more broadly about what lessons 3-D printing might 
offer present-day philosophy in general, and actor-network theory in particular.
3D Printing and Actor-
Network Theory
Graham Harman
American University in Cairo, Egypt

49
3D Printing and Actor-Network Theory
﻿
Armstrong certainly does not come off as a luddite. She is willing to concede the appeal of “[archi-
tect] Norman Foster [planning] to print moon bases using an array of mobile printing nozzles on a 6 
metre frame to squirt out sequential layers of lunar soil that will be set with a binding solution.” She is 
perfectly impressed by “experimental technology [that] may one day design entire ports to withstand 
future earthquakes that devastate places like Haiti, at a fraction of the cost of a traditional construction 
company.” Armstrong is also willing to appreciate both the efficiency and the science-fiction potential 
of the coming technologies: “Stratasys has just announced its new revolutionary 3-D printer that can 
produce multiple material types in a single print run, reducing the price of complex prototypes by around 
50 per cent, while Skylar Tibbits promises us a phase of 4-D printing where geometries become even 
weirder when they encounter activating solutions.” Finally, she is well aware of the likely practical upside 
of 3-D printing: “[it] can also process locally sourced materials, reducing the expense of transport and 
distribution systems and has even been proposed to improve employment conditions.”
Yet Armstrong holds that all of these benefits are outweighed by the potential garbage-related ca-
tastrophe of the new technology. As she puts it, “urgent thinking is required to avoid the revolutionary 
potential of 3D printing being lost in a sea of pointless plastic products.” She rues the coming day when 
“the unit cost of printers falls and hobbyists make legions of white elephants out of toxic plastics and 
when our landfills are chock-a-block with yesterday’s badly made fashionable shapes,” most of which 
“will simply clutter up our rubbish dumps and precipitate our plastic marine continents as indestructible 
rubbish icebergs.” And just as poetically:
Climate change may be evidenced empirically in specific events —such as rising sea levels and escalating 
concentrations of atmospheric carbon dioxide— but it is also experienced through bizarre encounters 
with matter such as the covert continents of particulate plastics causing the painful death of marine 
wildlife and entering our own food chain.
Armstrong remains skeptical even of the energy-saving benefits of the technology, since she has 
“yet to see a full supply chain analysis on the energy and resource requirements of 3-D printing,” and 
notes as well that it is “extraordinary for a practice whose material platform is largely based on plastics, 
compounds that do not do well in ecosystems, to propose to be ‘ecological.’”
3-D printing must escape the imaginative constraints of familiar pre-existing industry, and “[become] 
the champion of research into dynamic systems and lifelike materials —which may not yet have a mature 
market— so that we can produce objects that in themselves forge positive environmental relationships 
such as carbon recycling or soil generating systems.” Armstrong concludes that “perhaps the most vexing 
aspect of 3-D printing is that squirting plastics into funny digital shapes says absolutely nothing about 
matter— which as Timothy Morton reminds us, is the essence of Nature.” (Morton 2013) To summarize, 
“in the 21st century, matter is lively, strange and unpredictable— and is a force to be reckoned with… 3-D 
printing is not a revolution in making until it addresses the fundamental issue of 21st-century materiality.”
Though Armstrong only stresses the word “materiality,” it is not a risky step to describe her position 
with the related word “materialism,” a term that has grown increasingly popular among theorists in recent 
years. Since ancient Greece the usual opposite term for matter is “form,” and even today materialism is 
often introduced to counter some type of formalism: especially in art, architecture, and literary criticism. 
It is easy to see how a materialist critique of possible formalism in 3-D printing might also hit home. 
Whatever forms of sculpture, consumer products, and vulgar amusement we might choose to generate 

50
3D Printing and Actor-Network Theory
﻿
with our eventually low-cost home printers (the ridiculous things that people choose to photocopy are 
widely known) these objects are not freestanding entities without consequences for their human and 
natural environment. Armstrong cites exactly three “material” aspects that lead 3-D printed objects to 
become entangled with their neighbors. Though Armstrong never mentions it, we note that these three 
aspects happen to correlate nicely with the three tenses of time:
1. 	
Past: Armstrong is curious about the supply chains through which the base materials of 3-D print-
ing become available. She is rightly suspicious of the assumption that these materials come to us 
in raw, virginal, angelic form, untainted by environmental or political cost.
2. 	
Future: She expresses the more-than-plausible worry that 3-D printing will quickly flood the earth 
with preposterous trinkets of predictably low lifespan. The items produced by the machine will 
not simply vanish into the ether when no longer wanted, but will worsen an already demoralizing 
world garbage crisis.
3. 	
Present: Quite apart from their physical origin and ultimate disposal site, it would be irresponsible 
to view 3-D printed objects as freestanding products-without-context. Instead, we should ask 3-D 
printing to “[become] the champion of research into dynamic systems and lifelike materials… so 
that we can produce objects that in themselves forge positive environmental relationships such as 
carbon recycling or soil generating systems.”
What all three uses of “materialism” have in common is their commitment to a relational view of 
the world, for which things are not or should not be free-floating singularities. Instead, things must be 
conceived in dynamic interplay with their supply sources, their effects, and their present contexts. This 
relational standpoint is characteristic of most of today’s new materialisms, and distinguishes them clearly 
from the old materialism of autonomous chips of matter that are what they are in no matter what “acci-
dental” context they may be found. (Coole & Frost, 2010; Barad 2007) For Armstrong too, materialism 
and relationism go hand in hand. This makes the work of Bruno Latour a good point of comparison and 
contrast with Armstrong, since Latour explicitly drives a wedge between these two terms: accepting 
relationism while rejecting materialism outright.
2. LATOUR ON MATERIALISM
There can no question that Latour’s philosophy and social theory are utterly relationist in character. (See 
Harman 2009) After all, there are no substances or objects for Latour, only actors— and by definition, 
an actor is nothing more or less than its sum total of actions. As he puts it in his important early work 
“Irreductions,” “everything happens only once, and at one place.” (Latour 1988, 162) A generation 
later, in the 1999 book Pandora’s Hope, he tells us that an actor is nothing more than whatever it modi-
fies, transforms, perturbs, or creates. (Latour 1999, 122) If we remove an actor from one network and 
install it in another, it is no longer the same actor. The equivalence of the two can be established only 
by some observer who does the work of demonstrating a chain of translations linking one to the other. 
This means that Aristotle’s conception of individual substances, which remain the same thing across 
time even as their accidents and relations change, is erased from the Latourian picture of the world, 
which thereby inherits the metaphysics of Whitehead, the semiotics of Greimas, and the pragmatism of 

51
3D Printing and Actor-Network Theory
﻿
William James. As Latour says of his most famous human topic: “[Louis] Pasteur is a good pragmatist: 
for him essence is existence and existence is action.” (Latour 1999, 123) A bit later in Latour’s Pasteur 
book we read as follows:
The word ‘substance’ does not designate ‘what remains beneath,’ impervious to history, but what gathers 
together a multiplicity of agents into a stable and coherent whole. A substance is more like the thread 
that holds the pearls of a necklace together than the rock bed that remains the same no matter what 
is built on it… Substance is a name that designates the stability of an assemblage. (Latour 1999, 151)
This manner of interpreting actor-network theory should not be controversial, since the relational 
approach is precisely where the strengths of the theory are to be found. It is enough for many people 
to treat actor-network theory as a variant of materialism. After all, ANT grants no primacy to human 
consciousness or the workings of human language, but is equally concerned with the contributions of 
non-human entities to any situation. Worms, canoes, apricots, trains, and documents all find their way 
into Latour’s analyses of ostensibly social phenomena. Yet it is interesting that, unlike Rachel Armstrong 
and other contemporary adherents of a relational approach to the world, Latour emphatically rejects the 
term “materialism.”
The topic receives explicit attention in Latour’s still under-read article “Can We Get Our Material-
ism Back, Please?” (Latour 2007) During the heyday of traditional materialism, Latour notes, “it was 
possible to explain conceptual superstructures by means of material infrastructures. Thus an appeal to 
a sound, table-thumping materialism seemed an ideal way to shatter the pretensions of those who tried 
to hide their brutal interests behind notions like morality, culture, religion, politics, or art.” (Latour 
2007, 138) What is wrong with such a materialism? Latour goes straight to the point: “Materialism, 
in the short period in which it could be used as a discussion-closing trope, implied what now appears 
in retrospect as a rather idealist definition of matter and its various agencies.” (Latour 2007, 138) He 
expands this thought on the following page: “This is why the materialism of the recent past now looks 
so idealistic: it takes the idea of what things in themselves should be —that is, primary qualities— and 
then never stops gawking at the miracle that makes them ‘resemble’ their geometrical reproduction in 
drawings…” (Latour 2007, 139) In other words, though humans can never be entirely sure what things 
are, materialists of the olden style short-circuit the mystery of things with an idealized conception of 
what they ought to be: hard material particles moving through time and space, and serving as the bedrock 
for every larger-scale entity.
For further evidence of the idealist overtones of materialism, Latour appeals to the authority of 
his friend Isabelle Stengers, the formidable Belgian philosopher of science. He refers in particular to 
Stengers’s well-known book Thinking with Whitehead, (Stengers 2014) though the following summary 
is Latour’s own:
Under the rubric of “matter,” two totally different types of movement had been conflated: first, the way 
we move knowledge forward in order to access things that are far away or otherwise inaccessible; and, 
second, the way things move to keep themselves in existence. We can identify matter with one or the 
other, but not with the two together without absurdity. (Latour 2007, 139)

52
3D Printing and Actor-Network Theory
﻿
Anyone who has so much as glanced at Latour’s recent book An Enquiry Into Modes of Existence 
will immediately recognize here two of his fifteen modes: reference [REF] and reproduction [REP], 
whose conflation Latour regards as the source of modern scientific materialism. (Latour 2013) For 
Latour, [REF] thinks it is referring directly and unproblematically to an enduring geometrical/material 
substrate of primary qualities in the things. But in fact [REF] is no better than a translation, one that 
caricatures matter as nothing more than a hard material substance, which is nothing more than a feeble 
exaggeration of what actors really are. Meanwhile, Latour also joins Whitehead in spotting a second 
and very different sort of translation: the mode of [REP], which indicates that things have no existential 
inertia as traditional materialism believed, but must be reproduced again and again in each instant of 
time. For Latour all relation is translation, even if it be the simple relation of a thing in time T1 to its 
own close successor at time T2.
For Latour, the non-reality of inertial substance also implies the non-reality of the simplicity that 
Leibniz granted to his monads. This becomes clear from Latour’s decisive rejection of Heidegger:
This is why I always find it baffling that people would take Heidegger’s “philosophy of technology” 
seriously. Not only would Heidegger see no difference whatsoever between an atomic bomb, a dam, a lie 
detector, and a staple —all being mere examples of the same “enframing”— but when he finally gives 
some respect to a shoe or a hammer it is only to see it as the assembly of four elements— his “fourfold.” 
To be sure, such tools may be beautifully made, and it is much better to call on the gods and the mortals, 
heaven and earth, to account for their emergence than to dismiss them as the thinnest of “mere” objects. 
But look again at the VW Beetle: just four elements, really? That’s a very small list indeed… Any techni-
cal imbroglio forces us to count way beyond four. (Latour 2007, 140)
Though Latour is basically right about the relative poverty of Heidegger’s philosophy of technology 
when compared with those of Marshall McLuhan, Gilbert Simondon, and others, he nonetheless misreads 
the fourness of Heidegger’s fourfold. Das Geviert in Heidegger is an ontology of four aspects of every 
thing, not a bizarre assembly-line claim that four pieces make up each thing. (See Harman 2007) Thus 
Heidegger cannot be refuted by the overwhelming number of elements belonging to any technology, 
which is precisely what Latour tries to do when he notes that “in 2003… after the explosion of the shuttle 
Columbia, hundreds of hitherto unknown actors had to be drawn into the discussion— a legal dispute, 
a ‘thing’ in the etymological sense.” (Latour 2007, 141) More important, however, is Latour’s urging us 
to look away from the “‘thin description’ of an entity’s idealized material aspects” (Latour 2007, 142) 
in favor of a “post hoc narrative thick description of what should have been visible in the gathering 
that brings a thing together…” (Latour 2007, 142) This latter materialism is the sort that Latour hopes 
to “have back” while he is busy getting rid of the old kind, which idealized things as primary physical 
qualities while dismissing their other features as accidental or derivative.
In the end, it might seem that there is not much of a difference between Latour’s position on material-
ism and Armstrong’s own. While Latour uses “materialism” as a polemical term for modern scientistic 
philosophy, that is obviously not the kind of materialism that Armstrong wishes to advocate. In fact, 
both authors defend a thoroughly relational conception of beings, and both wish to trace things back 
to the vast army of confederate actors that help make them what they are. More to the point, if we had 
an essay by Latour himself on 3-D printing, it is likely that he would stress the same sorts of concerns 
that Armstrong addresses. Latour would surely critique the notion of 3-D printing conceived as a useful 

53
3D Printing and Actor-Network Theory
﻿
“black box” producing useful end products, and would try instead to shed light on its numerous internal 
components and outward effects. But this is just what Armstrong already does in her article: asking us 
not to leave 3-D printing as a sleek black box, but to open the box and examine its internal and external 
entanglements.
3. THE NON-RELATIONAL SENSE OF 3-D PRINTING
Unlike contemporary materialism, object-oriented ontology asks us to focus instead on the non-relational 
aspects of new technologies. (See Harman 2011; Bryant 2011) Let’s return for a moment to the three 
aspects of materiality that were emphasized by Armstrong and would surely be endorsed by Latour as 
well. Armstrong opens up the black box of 3-D printing by asking about (a) the supply chain that pro-
vides it with materials, (b) the material debris it generates by way of garbage dumps filled with ludicrous 
trifles, and (c) how it interacts with other entities in its present functional context.
But by the same token, there are three parallel problems with this focus on the material/relational 
aspects of entities: (a) It often overstates their dependence on relations. Sometimes, rather than being 
dependent on its supply chain, an entity retroactively affects or dominates it: witness the recent reversal 
in which amazon.com is now in a position to dictate financial and distribution terms to major publishers. 
We might even say that an entity becomes more real the more it becomes a force to reckon with for the 
entities that originally gave it life. Every parent knows this well. (b) To focus on the unwanted side-effects 
of a technology assumes that the world has no choice but to look on in horror as tragic heaps of white 
plastic junk pile up, simply because the world has done this so far in the case of the Pacific garbage patch. 
In principle, such effects can always be addressed. The more challenging side-effects are often slower, 
less expected, and less permanently bound to the entity that generates them than one might expect. (c) 
An entity is not slavishly defined by its context, and often transforms that context in turn. This happens 
to be precisely the argument that some architects use against the site-specific or contextualized approach 
to building whose time has not yet passed.
In short, if we are looking for an analysis of any object that will not be prematurely contaminated 
by whatever we happen to notice of its current situation in the world, we need to zero in on the thing 
in its formality rather than its materiality. This admittedly commits us to a certain formalism, a type of 
thinking currently held in low esteem amidst the materialist/relationist wave of recent intellectual life. 
Yet the usual critique of formalism that it fails to grasp a thing’s entanglement with its environmental 
relations neglects the fact that an entity can (within certain limits) be shifted into different contexts, and 
endure the replacement of a good number of its parts, without turning into a different entity. As we have 
just seen, each thing has only a loose relationship with any given supply chain or environmental effect, 
and often modifies or overpowers these relations through its own internal capacities. The real problem 
with most formalisms is different. First, there is formalism’s excessive tendency towards holism; second, 
there is its over-emphasis on the outward visible look of a thing. Obviously, the only way to address these 
difficulties would be to argue for a technological formalism that is neither holistic nor surface-oriented.
We are fortunate that the outlines of just such a theory of technology are already available in the 
works of McLuhan. (See esp. M. McLuhan 1994) The great Canadian intellectual is most certainly a 
formalist, as can be seen in his explicit championing of formal causation over Aristotle’s other three 
causes: material, efficient, and final. (M. McLuhan 2011) And instead of equating formal cause with 

54
3D Printing and Actor-Network Theory
﻿
the outward morphological “look” of a thing, McLuhan pushes in the opposite direction, driving the 
form underground while keeping it fully operative. What we encounter when using any technology is 
its content, focused as we are on dissecting good or bad television shows while ignoring the formal 
properties of the television medium itself. But the true power of the television medium inhabits the 
background, deadening us to its silent power as we remain distracted by its relatively trivial surface. In 
this way, McLuhan clearly distances himself from any formalism that would reduce things to their easily 
accessible aspects. Nor does McLuhan commit the other formalist sin of holism, since for him there is 
always a gap or interval between the background medium and its surface-effects.
While it has become popular in recent years to claim that what a thing does is more important than 
what it is, there is good reason to assert the opposite principle. Since we can never summarize or predict 
all the possible uses or actions of a thing, this seems clearly the wrong way to approach the nature of a 
new technology. The only way to attain a sufficiently flexible concept of any given thing is to ask about 
its never fully manifested form, or “what it is” rather than “what it does.” In the posthumously published 
Laws of Media, McLuhan and his son Eric zero in on the relevance of formal causation for media theory, 
even if they seem to do this at first by means of what looks like a list of the current and future effects of 
the thing. The list to which I refer is the McLuhans’ “tetrad”: a fourfold structure like Heidegger’s, and 
just as immune to being refuted by any enumeration of the hundreds of pieces that actually compose 
each thing. (M. & E. McLuhan 1992) But while the terms listed with each tetrad may look like a list 
of “what a thing does,” they actually give us indirect insight into what the things are, just as collisions 
in a particle accelerator draw our attention less to “events” than to the particles that make those events 
possible. The first two elements of the tetrad run as follows:
•	
Enhancement: Every new artifact extends or amplifies some feature of experience.
•	
Obsolescence: By the same stroke, every new artifact cuts off or closes some previous feature of 
experience.
Before moving on to the other two media laws, we should note that the McLuhans often give a strangely 
backwards explanation of them. While it may seem as if enhancement should refer to the more visible 
features of a technology and obsolescence to less visible ones, only the opposite can be the case, for 
reasons argued forcefully by McLuhan himself. Namely, the potency of an artifact for McLuhan comes 
from its way of receding from view, dominating consciousness silently from behind the scenes. That 
which is obtrusively visible is ipso facto trivial for McLuhan, who famously claimed that the content 
of any medium is no more important than the graffiti painted on the first atomic bomb. (M. McLuhan 
1969) To summarize, what is enhanced by a new technology is that which remains initially hidden in 
it, and what is obsolesced is that which becomes conspicuously present as debris from the extinguished 
media of yesteryear.
We have just covered enhancement and obsolescence, which McLuhan assigns to the morphology 
or static structure of an artifact. His other two media laws refer instead to metamorphosis, or how any 
medium eventually gives way to another. Here as well, there are two distinct moments:
•	
Retrieval. Every medium has an old medium as its explicit content. Another way of putting it is 
that retrieval occurs when someone (described by McLuhan as an “artist” in the widest sense of 
the term) takes the obsolescent rubbish of a past medium and restores it once more to an effective 
background position.

55
3D Printing and Actor-Network Theory
﻿
•	
Reversal. When a medium “overheats,” generating too much content, it ultimately flips into its op-
posite. We can see this for example when cars reverse from convenience into inconvenience once 
a certain number of them begin to clutter the available geography.
Let’s now try to generate a McLuhan “tetrad” for the 3-D printer:
•	
Enhances: economy, customizability, cottage industries, democratization of means of produc-
tion, importance of formal models, highly complicated or “organic” shapes, parametric design 
principles, contraband.
•	
Obsolesces: Centralized mass production, superstores, one-size-fits-all, national customs regula-
tions, proprietary products, simple geometric shapes, skilled carpentry.
•	
Retrieves: Stonemasonry, tailoring, home as production site, local variation.
•	
Reverses Into: Vulgar mass taste, snobbery of superior models, aristocracy of latest hardware, 
monotony of complex shapes, lewd cult of obscene shapes.
By calling our attention to the internal/formal structure of any given technology, tetrads help make 
us aware that entities may not be as vulnerable to their happenstance surroundings as relationist theories 
might assume. The thing is a “deep background” that tends to run a historical course loosely pre-ordained 
by the thing’s own properties, while its relational or contextual features turn out to be relatively peripheral 
surface incidents. If opening the black box of the 3-D printer is meant to show us the material supply 
chain and the catastrophic garbage-production of the device, this does not prove that the 3-D printer 
can be wholly identified with these grim side-effects. Instead, the 3-D printer is a partially autonomous 
technology whose likely desirability could motivate efforts to dampen the sorts of impacts against which 
Armstrong justifiably warns. More generally, if plastic products so far have produced horrific waste on 
land and at sea, we cannot simply extrapolate and assume that a soon-to-be-widespread device that utilizes 
a plastic medium will aggravate the problem even more. To overheat a medium in this way sometimes 
leads to reversals in its uses and its results.
But the same could be said about the tetrad diagrams of pretty much any artifact, since any quadruple 
consideration of a new medium will tend to draw our attention to the formal rather than relational aspects 
of any given thing. In that respect, the tetrad and actor-network theory already make for uneasy bedfellows. 
Can we go further than this and ask if the 3-D printer poses any particular challenge to actor-network 
theory’s conception of the world, to a greater degree than other artifacts we might consider? The answer 
seems to be yes. By eventually allowing us simply to posit an object and decree its rapid creation in the 
confines of our homes, the 3-D printer will allow us to bypass the usual painstaking process of assembly. 
Once such machines reach an affordable price for consumers, the shapes it produces will be constrained, 
not by the scarce supplies and skills needed to create a complex form, but only by the imaginations of 
users and template suppliers. In that respect, rather than forcing us to attend to 21st century materiality as 
Armstrong holds, the 3-D printer may bring us instead to an era of 21st century formalism. Rather than 
seeming like black boxes that demand to be opened, the 3D printer’s forms-by-decree could be flashing 
red boxes that alert us more than ever to the autonomy of things from their inner and outer relations alike.

56
3D Printing and Actor-Network Theory
﻿
REFERENCES
Armstrong, R. (2014). 3D printing will destroy the world unless it tackles the issue of materiality. The 
Architectural Review, January 31, 2014. http://www.architectural-review.com/home/products/3d-printing-
will-destroy-the-world/8658346.article
Barad, K. (2007). Meeting the universe halfway: Quantum physics and the entanglement of matter and 
meaning. Durham, NC: Duke University Press. doi:10.1215/9780822388128
Bryant, L. (2011). The democracy of objects. Ann Arbor, MI: Open Humanities Press. doi:10.3998/
ohp.9750134.0001.001
Coole, D., & Frost, S. (Eds.) (2010). New materialism: Ontology, agency, and politics. Durham, NC: 
Duke University Press.
Global News. (2013). “3D printing: Make anything you want,” YouTube video, January 25, 2013. https://
www.youtube.com/watch?v=G0EJmBoLq-g
Harman, G. (2007). Heidegger explained: From phenomenon to thing. Chicago: Open Court.
Harman, G. (2009). Prince of networks: Bruno Latour and metaphysics. Melbourne: re.press.
Harman, G. (2011). The quadruple object. Winchester, UK: Zero Books.
Latour, B. (1988). The pasteurization of France (A. Sheridan & J. Law, Trans.). Cambridge, MA: Har-
vard University Press.
Latour, B. (1999). Pandora’s hope: Essays on the reality of science studies. Cambridge, MA: Harvard 
University Press.
Latour, B. (2007). Can We Get Our Materialism Back, Please? Isis, 98(1), 138–142. doi:10.1086/512837
Latour, B. (2013). An enquiry into modes of existence: An anthropology of the moderns (C. Porter, 
Trans.). Cambridge, MA: Harvard University Press.
Marshall, M., & McLuhan, E. (1992). Laws of Media: The New Science. Toronto: University of Toronto 
Press.
McLuhan, M. (1969). “The Playboy Interview: Marshall McLuhan,” Playboy, March, pp. 26-27, 45, 
55-56, 61, 63.
McLuhan, M. (1994). Understanding media: The extensions of man. Cambridge, MA: MIT Press.
McLuhan, M. (2011). Media and formal cause. Vancouver: NeoPoiesis.
Morton, T. (2013). Hyperobjects: Philosophy and ecology after the end of the world. Minneapolis: 
University of Minnesota Press.
Stengers, I. (2014). Thinking with Whitehead: A free and wild creation of concepts (M. Chase, Trans.). 
Cambridge, MA: Harvard University Press.
This work was previously published in the International Journal of Actor-Network Theory and Technological Innovation (IJAN-
TTI), 7(1); edited by Ivan Tchalakov, pages 1-9, copyright year 2015 by IGI Publishing (an imprint of IGI Global).

57
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  4
DOI: 10.4018/978-1-5225-1677-4.ch004
Antonios Tsoukalis
TEI of Athens, Greece
ABSTRACT
Biomedical Imaging is a rapidly evolving area, where new versions of advanced medical equipment/new 
methodologies based on complex physical phenomena are developed. The cost for the effective training 
of both academic students/company employees in real training environments is high and in many cases 
impossible. Most of the times, it is not possible to effectively transfer knowledge due to the limitations 
of these environments, which has a strong effect both on the theoretical understanding and the practical 
skills of students. On the other hand, the exploitation of 3D simulations enables for better knowledge 
acquisition by learners, although not efficiently explored yet. Several studies support that this can be 
achieved by maximizing the interaction with simulations. To this end, interactive 3D Medical Equipment 
Simulations have been developed, by exploiting open source software and delivering them through the 
Web and eLearning Environments. The proposed chapter aims to present the need for such Environments, 
the development tools, challenges, solutions and possible applications.
Exploiting 3D Medical 
Equipment Simulations to 
Support Biomedical Engineering 
Academic Courses:
Design Methodology and Implementation 
in a Small Scale National Project
Adam Damianakis
Conceptum S.A., Greece
Pavlos Kallonis
Conceptum S.A., Greece
George Loudos
TEI of Athens, Greece
Dimitris Tsatsos
Conceptum S.A., Greece

58
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
INTRODUCTION
Biomedical Imaging is a rapidly evolving area, where new advanced medical equipment and new meth-
odologies based on complex physical phenomena are continuously being developed. A growing number 
of academic students or trainees in corporate environments require continuous inservice training. The 
authors’ experience from Greece shows that there is a wide variety of both undergraduate and postgradu-
ate academic programmes in this domain and new programs are being designed. The learners/trainees 
should understand concepts and acquire skills by interacting with complex phenomena and processes 
on which the function of medical equipment is based (Shyam et al., 2011); however, this cannot be eas-
ily covered using conventional educational means and as a consequence, the exploitation of advanced 
equipment in clinical practice is not optimal. By achieving this type of interaction with state-of-the-art 
equipment in a hands-on setting, the gap between the general theoretical concepts and the efficient us-
age of medical equipment can be bridged. In this way it would be possible to educate a new generation 
of learners/trainees, who can acquire competences (knowledge, skills and values/beliefs) regarding the 
latest and innovative technologies and subject matters (Hofer et al., 2013; Holzinger et al., 2009).
Student/trainee access to actual, state-of-the-art systems is often difficult due to practical restric-
tions. The daily workload of medical staff, safety issues and other complications apply limitations to 
the learning/training experience. Usually, training is limited to a small number of visits, demonstration 
without hands-on practice and only, in a few cases, interaction with the external parts of the medical 
equipment. Furthermore, a most of the latest and innovative medical equipment is only available to a 
selected number of hospitals, making them inaccessible to most of the learners/trainees. These problems 
are acknowledged in the international literature and existing studies define the basic reasons that make 
the design, development, and implementation of Integrated Virtual Labs e-Training Environments a ne-
cessity (Bedia et al., 2011). Other studies support the argument that the exploitation of 3D Technologies 
is flourishing and presents advantages (Boulos et al., 2007), including the use of simulations for other 
similar areas, such as medicine, nursing, physics, chemistry and others (Wiecha et al., 2011).
Specific studies (Dieckmann, 2009) (Table 1) acknowledge that the need for the design, development 
and implementation of Integrated Virtual Labs eLearning Environments due to:
1. 	
The high cost of acquiring the most recent and innovative medical equipment along with the cost 
of the educators, which cannot be omitted, as this area is growing and changing rapidly
Table 1. Needs and objectives
Need
Type of Need
General Objectives
Reduce the high cost of buying equipment 
for training purposes
Economic
Cost effective eLearning Environment platform using 
open and widely available tools and services
Reduced need for large space to interact 
with medical equipment
Spatial
Design, Develop and Implement 3D Simulations of 
Medical Equipment providing high interaction
Provide multidisciplinary training
Training/Education
Provide Integrated 3D Simulations of Medical Equipment 
and Phenomena along with training material
Relate Medical Equipment with physical 
phenomena
Training/Education
Design and implement by combining the Medical 
Equipment with the corresponding physical phenomena

59
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
2. 	
The lack of space, because many medical equipment units are as large as a whole room and usually 
are operated continuously
3. 	
The lack of multidisciplinary experiments, because most of the times learners/trainees interact only 
with diagrams and results from a specific machine, and
4. 	
The need to integrate study of the medical equipment with the phenomena that occur while it is 
being used.
To this end, the authors proposed the design and development of highly interactive 3D Medical 
Equipment Simulations, which have been implemented and partially tested in real environments. To 
build these simulations, available open source tools were used. The final achievement of this work is an 
integrated eLearning Environment that can be used by for both Health Industry and Academic Techno-
logical Institutions and which is unique on international level.
The general objective of this research is to present and describe the design, development and imple-
mentation of highly interactive 3D Medical Equipment Simulations to support the Biomedical Engineer-
ing Academic Course of TEI Athens, as part of a small scale National Project. This main objective can 
be split into five sub-objectives
Objective 1 - Literature Review and Analysis: Initially,
1. 	
Determine the different types of needs and difficulties related to Biomedical Engineering 
learning/training,
2. 	
Describe ways to fulfil those needs and bypass those difficulties and
3. 	
Discuss the possible exploitation of 3D Simulations as a means to provide high quality inter-
active learning/training activities.
Objective 2 - Create Robust General Design Specifications: In order to develop and implement suit-
able 3D Simulations of medical equipment, it is first necessary to analyse the needs of the trainees/
learners and teachers/trainers. The objective of this analysis is to present the design specifications 
that were used as a guide for the implementation. See below for a list of the design specifications.
Objective 3 - Select the Most Appropriate Medical Equipment: In order to provide the most appropri-
ate 3D Simulations of the medical systems it was necessary to visit several sites and interact with 
the equipment. In our first implementation 10 different systems along with 10 different physical 
phenomena which are related to them were chosen. See below for these choices and their selection 
criteria.
Objective 4 - Develop 3D Medical Equipment Simulations for Learning/Training: To develop the 
10 different systems and their corresponding physical phenomena, we will:
1. 	
Described the open source technology used,
2. 	
Implement 10 selected Medical Simulations and 10 selected corresponding physical phenom-
ena and
3. 	
Assess the interactivity provided within those 3D Medical Simulations.
Objective 5 - Provide a Solid E-Learning Environment on Medical Equipment: Finally, the authors 
presented the implementation of the 3D Medical Equipment Simulations in Learning Management 
System they had developed and it is currently being used in TEI Athens. The authors conducted a 
pilot test study with the cooperation of students and tutors.

60
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
BACKGROUND
Introduction in Medical Imaging
Biomedical Engineering is a rapidly evolving area, where new versions of advanced medical equipment 
and new methodologies based on complex physical phenomena are continuously developed (Figure 1). 
The development of diagnostic technologies, based on medical imaging, is a multidisciplinary field 
where expertise from engineering, physics, medicine and computer science meet. Medical imaging 
is based on technologies that mainly use energy and, through interaction with matter, it is possible to 
non-invasively obtain information of the interior of the patient, either on anatomical or functional levels 
(Figure 2). The main anatomical imaging technologies are Computerized Tomography (CT), Magnetic 
Resonance Imaging (MRI), and Ultrasound (US), while the major functional imaging methods are 
Single Photon Emission Computed Tomography (SPECT) and Positron Emission Tomography (PET). 
Recently, combined modalities can simultaneously offer both anatomical and functional information i.e 
PET/CT, SPECT/CT and PET/MRI. Those technologies are considered mature and cover a large share 
of the diagnostic market. A large number of such clinical systems are installed worldwide with millions 
of diagnostic exams taking place every day. Apart from the diagnostic value, these systems are used to 
guide therapy and/or assess therapeutic effects; thus their optimal use by the technologists is necessary, 
in order to provide to the physicians with the most accurate information. However, this optimal level of 
use requires extensive training and, in most cases, deep understanding of the physical phenomena on 
which their function is based.
•	
The Need: A continuously growing number of students and trainees in academic and corporate 
environments request training in the field of medical imaging, both at undergraduate and post-
graduate levels. In every case, the learners should understand concepts and acquire skills by inter-
acting with the complex phenomena and processes (Figure 3) on which the function of this medi-
cal equipment is based (Shyam et al., 2011). This cannot be easily achieved using conventional 
Figure 1. Representation of brain based on different imaging techniques

61
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
educational means and the exploitation of advanced equipment in clinical practice has not reached 
optimal levels. By achieving this type of interaction for learners/trainees, it would be possible to 
bridge the gap between general theoretical concepts and the efficient usage of medical equipment. 
In this way it would be possible to educate a new generation of learners/trainees, who should be 
able to acquire competences (knowledge, skills and values/beliefs) regarding the latest/innovative 
technologies and subject matter (Hofer et al., 2013; Holzinger et al., 2009).
•	
The Current Limitations: The access to actual, state of art systems is frequently difficult due 
to practical restrictions. Most of the time, the daily workload of medical staff, safety concerns 
and other reasons apply limitations to the opportunities for students’ learning/training experi-
ence. Training is limited to a small number of visits, demonstration without hands-on experi-
ence and only, in a few cases, interaction with the external parts of the medical imaging sys-
tems. Furthermore, most of the latest and most innovative medical equipment is only available 
to a selected number of hospitals, making the equipment inaccessible to most of the learners/
trainees. Moreover, learners/trainees with disabilities face additional difficulties as most training 
programs do not take into account the specific learning needs that this category of learners has. 
Furthermore, although it is generally acknowledged that more women are needed in the fields of 
science and engineering (Hunt, 2010), the actual number of women participating in these fields 
is low. Vetri et al. (2010) state that the reasons that may have led to this can be grouped into three 
main categories: differences in aptitude, discrimination, and personal choice (Rosenbloom, Ash, 
Dupont, & Coder, 2008). Impaired access to formal, technical, and vocational education and train-
ing, stereotyping, cultural barriers and gender imbalance in research (European Commission She 
Figures 2012) all contribute to the underrepresentation of women in science and technology. This 
project will therefore have a special focus on ensuring equal access and opportunity.
While the need for such Learning Environments is obvious, following a detailed search very few 
existing solutions were found. Those that were found wererather incomplete and passive, compared with 
the project objectives. More specifically:
Figure 2. Interior of a clinical CT system; its complexity demonstrates the need for the development of 
advanced educational tools

62
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
•	
Dosch Design (Dosch Design, 2015) provides some 3D models of medical tools, that are limited 
to either simple instruments, more advanced equipment or patient rooms and hospital facilities 
and are passive. The completely textured 3D-models are provided in multiple file formats: 3DS, 
3dsmax, Lightwave, Maya, OBJ, VRML and Cinema 4D, but the user cannot explore the equip-
ment’s functionality.
•	
ARCreative (ARCreative, 2015) offers models of some medical systems, mainly for visualization 
purposes. The emphasis of these implementations is on accuracy from an aesthetic point of view.
•	
An Initiative of the Ministry of Human Resource Development (MHRD) from India (MHRD, 
2015) includes some labs related to Medical Instrumentation, but with no interactive and visual-
ization tools and simple technologies are used.
•	
Epic Systems is a company that is creating 3D Virtual Reality representations of medical fa-
cilities for training (EPIC, 2015), without providing any instrumentation simulation or functional 
visualization.
•	
Virtlab: A Virtual Laboratory (Virtlab, 2015) is a series of hands-on experiments using a simu-
lated chemistry laboratory, which has low skill requirements compared to the use of complex 
medical systems.
•	
Some implementations (i.e. a Scanner) are available in Archive 3D (Archive3D, 2015) but, with 
no additional medical systems (e.g., MRI, PET).
Those and many more examples show that there is no existing integrated product/solution, that is 
designed specifically for educational purposes and includes both the visualization of medical systems 
and the physical phenomena on which their function is based. For this reason, in terms of a small na-
tional project the authors implemented a Biomedical Engineering Virtual Lab, using limited resources. 
The lab describes on a basic level, 10 basic Medical Systems and 10 major Physical Phenomena, with 
advanced visualization tools and accompanying exercises. At the moment this lab is restricted to Greek 
students (the program language is Greek) and the prototype will need to be upgraded in order to include 
additional simulations of systems components and phenomena details.
Building on the experience acquired during this prototype implementation, the authors understand the 
need for re-design and to build it on a global platform to be internationally accessible and transformed 
Figure 3. Phenomena in γ-camera collimator

63
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
into a unique product that meets users’ needs. Using the existing development as a starting point, it is 
necessary to further exploit existing software tools, carry out original research to combine them with 
novel teaching and training methodsin order to derive a software platform, which can be used to provide 
efficient training in the field of medical imaging systems and can be further extended to other scientific 
domains, not limited to medical imaging.
DESIGN, DEVELOPMENT AND IMPLEMENTATION 
OF 3D MEDICAL SIMULATIONS
Design Specifications
As noted above, the authors have identified the most important design specifications for the design and 
development of the 3D Medical Simulations. The following list describes each one of them:
Design Specification 1: Each one of the 3D Medical Equipment Simulations (3DMES) should be related 
with its corresponding physical phenomena.
Design Specification 2: The 3DMES should provide extensive features for customizing the phenomena/
equipment and functions for creating exercises, with parameterization for specific users.
Design Specification 3: The 3DMES should include modification features of the phenomena/equipment 
and functions, to further upgrade/improve the exercises or for their scientific update.
Design Specification 4: The 3DMES should allow the implementation of new phenomena/equipment 
and functions in the virtual environment for the future expansion of the lab.
Design Specification 5: The 3DMES should implement an interactive user interface with mouse enabled 
navigation.
Design Specification 6: The 3DMES should implement authoring tools for the design and development 
of educational activities/exercises.
Design Specification 7: The 3DMES should be accessible through the web and be compatible with 
web-based learning management systems.
Design Specification 8: The 3DMES should be multilingual.
Selected Systems and Corresponding Physical Phenomena
In the first version of this virtual lab the authors have selected the most common full medical imaging 
systems, plus two cardiac monitoring devices. The authors have also selected some basic physical phe-
nomena, which are characteristic for the presented imaging systems.
The selected devices can be grouped in two categories:
Imaging Systems
•	
X-Ray: Which is the standard system that provides anatomical images using radiation, with very 
good resolution and tissue to bone contrast.
•	
Computerized Tomography (CT): Which is based on the X-ray and, by using rotation, provides 
anatomical tomographic images of patients’ internal structures with very high spatial resolution.

64
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
•	
Magnetic Resonance Imaging (MRI): Which also provides tomographic images with high spa-
tial resolution and good soft tissue contrast, without the use of radiation.
•	
Ultrasound: Which provides high resolution anatomical images of human tissue and has the ad-
vantage of no radiation use.
•	
γ-Camera: Which provides functional information of a radiotracer distribution and can monitor 
the existence of tumors and/or the function of several organs including heart, kidneys, and thyroid.
•	
Single Photon Emission Computed Tomography (SPECT): Which is based on the rotation of 
one or more γ-cameras and can provide similar tomographic information.
•	
Positron Emission Tomography (PET): Which is based on a similar principle as SPECT, but 
offers higher sensitivity and resolution and is mainly used for whole body cancer screening.
•	
Optical Imaging: Which is mainly used as a research tool for imaging cells, small animals and/
or thin human tissues.
Cardiac Monitoring Devices
•	
Electrocardiograph: Which is used to monitor cardiac signals and can identify different heart 
malfunctions.
•	
Phonocardiograph: Which is based on recording heart sounds and is a simple way to detect car-
diac rate and arrhythmias.
The selected basic physical phenomena are:
•	
Production of X-Rays from a Source: Which is a typical phenomenon in X-ray and CT imaging.
•	
Matter and X-Ray Interaction: Which is the main principle on which the formulation of images 
in X-ray and CT imaging is based.
•	
Magnetic Nuclear Resonance: Which is the basic phenomenon on which MRI imaging is based.
•	
Interaction between Matter and Ultrasound: Which highlights how ultrasound imaging works.
•	
Positron Annihilation and Pair Production: On which PET imaging is based.
•	
Biofluorescence: on which optical imaging is based.
•	
Collimator in γ-Camera: Which shows the role of the main module of γ-camera and SPECT 
system.
•	
Absorption of Photons in a Scintillator: Which is one of the most important phenomena in 
γ-camera, SPECT and PET.
•	
Photoelectric Phenomenon: Which is present in X-ray, CT, γ-camera, SPECT and PET.
•	
Principle of Operation of a Conventional and a Position Sensitive Photomultipler: Which are 
present in γ-camera, SPECT and PET.
The last 4 phenomena are connected and they can be incorporated in the corresponding imaging 
systems, in order to add more detail for their functionality. This list was a proof of concept effort to test 
the level of detail for the description of imaging systems. More levels of detail can be added by includ-
ing other phenomena.

65
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
Development Methodology
Based on the above Design specifications, the authors have devised a development methodology that 
consisted of multiple steps and the incorporation of different programming tools andplatforms. The main 
criteria for the selection of the programming tools/platforms was that they were to be open source and 
their exports compatible with the web and web based learning management systems. The following list 
describes the steps of the authors’ development methodology:
Step 1: The creation of a detailed description and design of every equipment based on available material 
or on site visits to hospitals and definition of the most relevant to the equipment physical phenomena
Step 2: The description of each phenomena in the form of an algorithm, based on the relevant physics 
theory
Step 3: The development of the 3D model simulating the equipment and the physical phenomena with 
appropriate 3D Programming tool/platform.
Step 4: The processing of the 3D representation models so that they were able to be imported in a variety 
of programming tools.
Step 5: The import of the 3D representation models into a selected programming tool/platform in order 
to develop the algorithm
Step 6: The development of all the configurable variables based on the designed algorithm. This step 
enables the different scenarios of the 3DMES behavior.
Step 7: The export of the final 3DMES and integration with the developed corresponding physical 
phenomena.
Step 8: The integration of the 3DMES within existing e-classes offered by TEI ATHENS.
To support the above-mentioned steps, the authors selected and exploited a mix of open source pro-
gramming tools and platforms that provided the necessary features. The following list describes each 
one of the tools/platforms used and the steps supported with them (see Table 2).
•	
Blender 2.69 (Blender, 2015 ): Blender is a designing/programming platform that enables 3D 
Graphics design/development. It is a free program and is distributed under the GNU General 
Public License v3. It is mainly used for modeling, rigging, simulations, animation, rendering and 
the development of interactive 3D Applications, such as videogames. It is available to be used 
Table 2. Exploitation of open source tools to support the development methodology
Tool/Platform
Supported Steps
How They Were Supported
Blender 2.68
3 and 4
Blender has been used to support the development of 3D Models of both the systems and their 
corresponding physical phenomena. The models were extracted in .3ds format in order to be 
imported to other programming tools (PreFab)
PreFab 2.1
4 and 5
Prefab has been used to edit the 3D Models and export them in a format suitable for adding 
high interaction with FlashDevelop. The exported format that the authors have used is the awd 
format, utilizing the Away 3D library.
FlashDevelop 
4.6.1
6,7 and 8
FlashDevelop has been used in order to import the .awd format of the 3D Models, develop their 
available interactions and configurable variables. Then, the authors have exploited Blender in 
order to integrate the 3D Models with their corresponding physical phenomena and finally they 
have exported the 3DMES in suitable format for TEI Athens e-class (SCORM format).

66
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
with most of the current Operating Systems, such as Windows, Linux and MAC OS. Blender 
enables the user to save the 3D data (models) in .blend format (this file format is only supported 
by Blender) and in other formats such as .3ds which is compatible with other programming tools/
platforms. Blender has been used in our application for steps 3 and 4.
•	
PreFab 2.1 (Prefab, 2015 ): PreFab is a programming platform based on the Flash (Actionscript 
3) programming language and it utilizes the Adobe Air technology. This enables the user to design 
and develop applications that can be compatible with Windows and Mac OS systems, using an in-
tegrated user friendly programming environment. The main aspect that made PreFab usable is this 
initiative is its ability to import and interact with the most known types of 3D files, such as Collada 
(.dae), Wavefront (.obj) and 3D Studio MAX (.3ds). Once the model is imported, the user can edit 
every aspect of it. The tools provided by PreFab are less than the tools provided by Blender, but 
PreFab enables the user to export the 3D models in .awd and Actionstript 3 formats, so that they 
can be imported into Flash based tools. The import of those files to Flash-based tools is done with 
the Away 3D library. PreFab has been used in our application for steps 4 and 5.
•	
FlashDevelop 4.6.1 (FlashDevelop, 2015 ): FlashDevelop is an open source programming plat-
form distributed under the open source MIT License. Its development was started in 2005 and 
is still an on-going project with frequent updates. Its main purpose is to provide an open source 
programming environment for Flash based programming languages, such as Actionscript 3. As 
Flash became more and more flexible in developing web applications, the Flash Develop program-
ming platform evolved into a tool for developing applications for multiple platforms and operating 
systems. The main aspect of FlashDevelop, that made the authors exploit it in this initiative, is its 
ability to develop highly interactive applications available through the Web. Flash Develop has 
been used in our application for steps 6, 7 and 8
Presentation of 3D Medical Simulations
Collimator of γ-Camera
The γ-camera (see Figure 3) is one of the most common medical systems, which is used for scintigraphic 
studies of the heart, thyroid, bones etc. The person to be examined receives a radiopharmaceutical, which 
is concentrated in the organ/tissue to be imaged and emits photons isotropically, which are detected by 
the γ-camera. The collimator is its most important component and its role is to allow only the photons 
that travel perpendicular to the γ-camera to be detected. There are four basic phenomena that take place 
in a collimator. The first is when a photon travels parallel to the collimator hole and is correctly detected 
by the system. The second is when the photon is absorbed by the collimator walls and is never detected. 
The third is when the photon is scattered on the collimator walls and finally is detected, which means 
that the position is not accurately recorded. Finally, if a photon has high energy, it can pass through the 
collimator walls and be detected at a completely wrong position, thus adding to image noise.
For this reason those four scenarios are visualized and four photons have been simulated, which follow 
predefined orbits to demonstrate the abovementioned phenomena In Figure 1 a square hole collimator 
and four photons are drawn. Each photon simulates one of these phenomena and the student can see 
them through different viewing points, in order to fully understand them. There is provision to develop 
different exercises, based on this geometry and the student will have to guess what will happen when 
the geometric parameters of the collimator or photons energy change.

67
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
Absorption of γ-Rays in Scintillator
The second, basic phenomenon on which the operation of a γ-camera is based is the radiation detection 
in the scintillator, which is placed right after the collimator (see Figure 4). The photons which reach the 
scintillator are absorbed and emit other photons of lower energy. The main phenomenon that takes place 
is the photoelectric effect. Here a γ-photon interacts with an electron of an atom, transfers its energy to 
it and, as a result, the γ-photon disappears, while the electron starts moving freely with kinetic energy 
described asEK=hv-Εβ, where h is Plank constant, ν is the frequency of the γ-photon and EB is the ion-
izing energy of the electron. This means that part of the photon energy is used for moving the electron 
from its normal state and the rest is transferred as kinetic energy to the electron.
To make the procedure clear, the interaction between a γ-photon and an electron, as well as electron 
release are simulated. In Figure 4 the first implementation of the photoelectric effect is shown. The photon 
kinetic energy can be parameterized, in order to allow the implementation of different exercises for the 
students. In addition, it is possible to include other physical phenomena, such as Compton scattering, 
where there is no photon absorption.
Photomultipler Tube (PMT)
The PMT is the main detector component of a gamma camera, SPECT and PET. The object that is im-
aged i.e. patient, is absorbed by the scintillator and converted to optical photons. These optical photons 
enter the entrance window of the PMT (brown left part) and they produce a photo-electron at the photo-
cathode (gray part in the middle). This electron is attracted by a high voltage what? and enters a dynode 
Figure 4. Absorption of γ-radiation in a scintillator

68
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
stage (brown components in the middle). When an electron hits a dynode, more electrons are produced, 
which are attracted by the next dynode, until a large electron cloud reaches the anode (gray part on the 
right in the figure) and are converted to current. The implementation shows the entire procedure from 
the moment that an electron enters the PMT, until the electrons are absorbed by the anode. The user can 
navigate inside the PMT and see the procedure of electrons multiplication (see Figure 5).
Biofluorescence
The fourth phenomenon, on which optical imaging is based, is biofluorescence. Optical imaging methods 
depict the distribution of a tracer in a target area and is based on tracers that emit optical photons, which 
are detected usually by a CCD system. In biofluorescnce, the optical signal comes from a fluorescent 
molecule that can be either endogenous (collagen, hemoglobin) or injected (green fluorescent protein 
and other optical contrast agents). An external light source is used to excite the fluorophore and a CCD 
detects the lower energy emission light. In Figure 6 a screenshot of this process is shown, when a light 
sources triggers a chromophore (marked as white) and optical photons are emitted (marked as red). In 
the implementation the user can see how the light source on the right triggers the chromophore, which 
absorbs this energy, starts to rotate and emits the optical photons. When the extra energy is released the 
procedure stops (See Figure 6).
MRI and SPECT
In Figure 5 the internal part of an MRI scanner is shown. In this implementation the user has the option 
to make different components of the MRI scanner transparent, so the learner/trainee could understand 
its structure in detail. It is possible to add text on these components and/or the tutor can add exercises 
in order to educate the trainees, as well as set up exams, which can assess their level of understanding. 
Figure 5. Photomultiplier principle of operation

69
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
Although linking with the relative physical phenomena has not been implemented yet, it is possible to 
link the different components of the MRI with the phenomena that they exploit (See Figure 7).
In Figure 8 a realistic representation of a SPECT camera is shown. The system includes the patient 
bed and the two camera heads that rotate around it. By initiating the simulation, the user can see a typi-
cal scenario of an exam, with the bed moving inside the field of view and the 2 camera heads rotating 
around it. Each head consists of a γ-camera, thus it is possible to link the implementation of a γ-camera 
with the SPECT system. As the four different phenomena, on which the function of a γ-camera is based 
have already been simulated, it is possible to also link them, thus the description of the SPECT system 
can start from the level of full system, then to γ-camera and finally to the corresponding phenomena. 
This is the ultimate goal of the entire Virtual Laboratory, although the authors recognize the need to 
attract additional resources for such an implementation.
Implementation TEI of ATHENS Case
Currently this Virtual Laboratory is hosted at the e-class of the Department of Biomedical Engineering 
at Technological Educational Institute of Athens (TEI ATHENS, 2015). At the moment, it is accessible 
only upon request, but the last stage of the project includes its open release to the public. Currently the 
platform is available only in the Greek language and a screenshot of the exercises that are available are 
shown in Figure 9 and Figure 10.
Taking advantage of the e-class tools, the tutor can add more exercises, set up questionnaires and 
evaluate the students on line. It is also possible to also add educational material to the existing virtual 
representations of the systems and the phenomena, and to upgrade the existing models, by adding new 
simulations.
Figure 6. Biofluorescence

70
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
FUTURE RESEARCH DIRECTIONS
Based on the authors’ work, we have devised a table with current issues and proposed solutions as future 
research directions of this initiative. Table 3 presents each indentified issue with a proposed research 
directions (see Table 3).
Figure 7. MRI scanner (internal)
Figure 8. SPECT

71
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
Figure 9. Screenshot of the virtual laboratory at the 
Department of Biomedical Engineering at Tech-
nological Educational Institute of Athens (1/2)
Figure 10. Screenshot of the virtual laboratory 
at the Department of Biomedical Engineering 
at Technological Educational Institute of Athens 
(2/2)
Table 3. Issues and related research directions
Issue
Research Directions (RD)
Limited/no 
access to train on 
real systems for 
students, engineers, 
health practitioners, 
and sales 
representatives
RD1: The description of the major medical imaging systems (e.g CT, MRI, SPECT, PET, Ultrasounds etc). The 
user will be able to navigate outside and inside the systems (as he could do if he were in front of the real system); 
view and interact with their components and see examples of their principles of operation; set up test scenarios of 
standard calibration procedures, quality assurance tests and common problems 
RD2: Link with open source simulation tools (GATE toolkit, 2015), which would allow the user to not only 
visualize the physical phenomena, but also to run realistic simulations and produce original data by exploiting 
existing and well validated physics libraries. At least one such implementation (clinical g-camera) will be part 
of the initial product, with detailed description of the procedures to add similar implementations for all imaging 
systems. 
RD3: Additional tools, which allow for continuous scale up, including the design and implementation of more 
detailed components of the medical systems or descriptions of physical phenomena. In the architecture provided, 
each component of a medical system will be an independent item, initially static, which can be split in smaller 
compartments, which will be linked with the corresponding physical phenomena. The platforms that have been 
used for the initial implementation follow this philosophy.
Limited educational 
materials to 
efficiently describe 
the physical 
phenomena on 
which medical 
imaging systems are 
based
RD4: The description and visualization of the major physical phenomena (magnetic resonance, photoelectric 
effect, radiation absorption, etc). The user will be able to visualize those complex phenomena, and change their 
parameters, in order to fully understand them and overcome the limitations that are present in conventional 
educational materials. 
RD5: Test examples with answers and self-evaluation questions, which will be combined with the provided 
material, in order to evaluate the learner/trainee. Additional tools will be provided, in order to allow tutors/trainers 
to create new questions and/or examples.
continued on following page

72
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
CONCLUSION
This chapter has discussed the design and development of interactive 3D medical equipment simulations 
using open source software and delivering them through the web and e-learning environments. The pre-
sentation included the identification of the need for such environments, a proposal for their development 
tools, the description of the challenges, the solutions and their possible applications.
The authors believe that, although this effort could be the first step for providing integrated and 
meaningful technology supported learning/training to both academic students and industrial trainees, 
there are a lot of aspects that should be considered for future research. Some of those aspects are the 
design/development of educational methodologies and the provision of specifically tailored tools that 
will enable teachers and trainers to utilize the provided 3D Medical Simulations in their own educational/
training practices.
REFERENCES
Archive3D. (2015). Retrieved June 3, 2015, from http://archive3d.net/?a=download&id=a3cfcbf4
ARCreative. (2015). Retrieved June 3, 2015, from http://www.arcreativemedical.com/
Bedia, A., Zohra, F. B., & Benyoucef, B. (2011). 2D Device Modeling and Simulation / FET for Bio-
medical Applications. Physics Procedia, 21, 35–41. doi:10.1016/j.phpro.2011.10.006
Blender. (2015). Retrieved June 3, 2015, from https://www.blender.org/
Boulos, M., Kamel, N., Hetherington, L., & Wheeler, S. (2007). Second Life: An overview of the poten-
tial of 3-D virtual worlds in medical and health education. Health Information and Libraries Journal, 
24(4), 233–245. doi:10.1111/j.1471-1842.2007.00733.x PMID:18005298
Issue
Research Directions (RD)
Limited and/or 
hidden common 
reference points 
• Lack of 
a common 
competences 
framework 
• Hidden 
methodologies 
within existing 
educational/training 
practices 
• Lack of a valid 
Certification
RD6: Proposed common framework describing the knowledge (what do I need to know?), the skills (how can I 
perform this action?) and the values/belief (are there ethical or other dilemmas related to my actions?), for three 
levels of expertise (basic, advanced and expert). Such a framework is not available at the moment and it could 
be most useful for situations such as the design of a job description by industry, the selection of appropriate 
educational/training programs by the learners/trainees and the design and development of appropriate educational 
activities by the teachers/trainers. 
RD7: Provision of generic teaching/training templates that could be exploited by teachers/trainers along with the 
provided educational material (which, in our case, are the 3D models of Medical Imaging Systems) in order to 
design and develop their own lessons. This type of tools is available for other specific subjects such as physics 
(COSMOS project) or as generic design tools such as LAMS, but such a tool is not available as an integrated 
solution for medical equipment education/training. 
RD8: The existence of an acknowledged service, which provides learners/trainees with a valid certification, is not 
available in the market. The proposed accreditation service would give a) the learners/trainees a way to certify 
on new technologies (e.g. the latest models of Medical Imaging Systems) as part of their continuing education/
training (lifelong learning), b) the industry a reference to evaluate its potential and current employees and c) the 
vocational training providers a new and innovative mechanism of certification provision.
Table 3. Continued

73
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
Dieckmann, P. (Ed.). (2009). Using Simulations for Education, Training and Research. Lengerich, 
Germany: Pabst.
Dosh Design. (2015). Retrieved June 3, 2015, from http://www.doschdesign.com/productlist2.
php?line=4&subrubrik=41
EPIC. (2015). Retrieved June 3, 2015, from https://www.youtube.com/watch?v=zzhTlCO3JS8
FlashDevelop. (2015). Retrieved June 3, 2015, from http://www.flashdevelop.org/
GATE Toolkit. (2015). Retrieved June 3, 2015, from http://www.opengatecollaboration.org/
Hofer, E., Wiener, T., Arnold, R., & Ahammer, H. (2013). Evaluation of Computer Simulation and 
Computer Animation used as eLearning Tools in Biophysics Lectures and in the self-learning Process of 
Medical Students In Proceedings of in the proc. of the WCMPBE. doi:10.1007/978-3-642-29305-4_450
Holzinger, A., Kickmeier-Rust, M. D., Wassertheurer, S., & Hessinger, M. (2009). Learning performance 
with interactive simulations in medical education: Lessons learned from results of learning complex 
physiological models with the HAEMOdynamics SIMulator. Computers & Education, 52(2), 292–301. 
doi:10.1016/j.compedu.2008.08.008
Hunt, J. (2010). Why Do Women Leave Science and Engineering? NBER Working Paper No. 15853.
MHRD. (2015). Retrieved June 3, 2015, from http://www.vlab.co.in/ba_labs_all.php?id=6
Prefab. (2015). Retrieved June 3, 2015, from http://www.closier.nl/prefab/
Rosenbloom, J. L., Ash, R. A., Dupont, B., & Coder, L. A. (2008). Why are there so few women in 
information technology? Assessing the role of personality in career choices. Journal of Economic Psy-
chology, 29(4), 543–554. doi:10.1016/j.joep.2007.09.005
She Figures. (2012). Retrieved June 3, 2015, from http://ec.europa.eu/research/science-society/docu-
ment_library/pdf_06/she-figures-2012_en.pdf
Shyam, D., Krishnashree, A., Prema, N., & Bipin, N. (2011). Enhanced Facilitation of Biotechnology 
Education in Developing Nations via Virtual Labs: Analysis, Implementation and Case-studies. Inter-
national Journal of Computer Theory and Engineering, 3(1), 1793–8201.
TEI ATHENS eclass. (2015). Retrieved June 3, 2015, from http://medisp.bme.teiath.gr/eclass/modules/
exercice/exercice.php?course=TIO222
Veltri, N. F., Webb, H. W., & Papp, R. (2010). GETSMART: An Academic-Industry Partnership to En-
courage Female Participation in Science, Technology, Engineering and Math Careers. In A. Cater-Steel 
& E. Cater (Eds.), Women in Engineering, Science and Technology: Education and Career Challenges 
(pp. 56–77). doi:10.4018/978-1-61520-657-5.ch003
Virtlab. (2015). Retrieved June 3, 2015, from http://www.virtlab.com
Wiecha, J., Heyden, R., Sternthal, E., & Merialdi, M. (2011). Learning in a virtual world: Experience with 
using second life for medical education. Journal of Medical Internet Research, 12(1). PMID:20097652

74
Exploiting 3D Medical Equipment Simulations to Support Biomedical Engineering Academic Courses
﻿
KEY TERMS AND DEFINITIONS
Blender: Blender is a design/programming platform that enables 3d graphics design/development.
Competence: A competence describes the knowledge, skills and values/beliefs, which characterize 
a person. It could be used as a measure to identify a person’s ability to do something and how well and 
efficiently he/she can do it.
Computerized Tomography (CT): CT is based on the X-ray and, by using rotation, provides ana-
tomical tomographic images of patients’ interior structures with very high spatial resolution.
FlashDevelop: FlashDevelop is an open source programming platform distributed under the open 
source MIT License.
Magnetic Resonance Imaging (MRI): Provides tomographic images with high spatial resolution 
and good soft tissue contrast, without use of radiation.
PreFab: PreFab is a programming platform based on the Flash (Actionscript 3) programming lan-
guage and it utilizes the Adobe Air technology.
STEM: It is the field of education that includes Science, Technology, Engineering and Mathematics.
Ultrasound: Provides high resolution anatomical images of human and has the advantage of no 
radiation use.
X-Ray: The standard system that provides anatomical images using radiation, with very good resolu-
tion and tissue to bone contrast.
This work was previously published in Revolutionizing Modern Education through Meaningful E-Learning Implementation 
edited by Badrul H. Khan, pages 277-295, copyright year 2016 by Information Science Reference (an imprint of IGI Global).

75
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  5
DOI: 10.4018/978-1-5225-1677-4.ch005
ABSTRACT
Visual attention is one of the most important mechanisms deployed in the human visual system (HVS) to 
reduce the amount of information that our brain needs to process. An increasing amount of efforts has 
been dedicated to the study of visual attention, and this chapter proposes to clarify the advances achieved 
in computational modeling of visual attention. First the concepts of visual attention, including the links 
between visual salience and visual importance, are detailed. The main characteristics of the HVS involved 
in the process of visual perception are also explained. Next we focus on eye-tracking, because of its role 
in the evaluation of the performance of the models. A complete state of the art in computational modeling 
of visual attention is then presented. The research works that extend some visual attention models to 3D 
by taking into account of the impact of depth perception are finally explained and compared.
INTRODUCTION
In everyday life, we are constantly receiving an abundant amount of information through various senses. 
Among the senses, sight is considered to be the most dominant one (Wandell, 1995). However, our sensory 
system for vision, the human visual system (HVS), continually receives a really large amount of visual 
data and it is beyond our brain’s capability to process all of them (Borji & Itti, 2013). To cope with this 
large amount of information, visual attention is one of the most important mechanisms deployed in the 
HVS to reduce the complexity of the analysis of visual scene (Wolfe, 2000). Driven by visual attention, 
viewers can selectively focus their attention on specific areas of interest in the scene.
2D and 3D Visual Attention 
for Computer Vision:
Concepts, Measurement, and Modeling
Vincent Ricordel
University of Nantes, France
Junle Wang
University of Nantes, France
Matthieu Perreira Da Silva
University of Nantes, France
Patrick Le Callet
University of Nantes, France

76
2D and 3D Visual Attention for Computer Vision
﻿
In the last decades, extensive efforts have been dedicated to the study of visual attention. Neurologists, 
psychologists, vision scientists, and computer scientists have taken part in, and contributed to various 
aspects of visual attention. These efforts from different disciplines made the research on visual attention 
become a highly interdisciplinary field; different relevant disciplines deal with the research on visual 
attention from different points of view, and profit from each other.
In recent years, the use of visual attention mechanisms in image processing systems has found increas-
ing interest by computer scientists. Taking into account visual attention information becomes an effective 
way for improving various existing algorithms in image processing. A variety of areas, including com-
pression (Parkhurst, Law, & Niebur, 2002), retargeting (D. Wang, Li, Jia, & Luo, 2011), image retrieval 
(Vu, Hua, & Tavanapong, 2003), quality assessment (H. Liu & Heynderickx, 2011), have been benefiting 
of being provided information about the locations that attracts viewer’s attention in the visual scene.
When visual attention is taken into account by the signal-processing community, the two terms, “sa-
lience” and “importance”, have traditionally been considered synonymous. It is true that both of visual 
salience and visual importance denote the most visually “relevant” parts of the scene. However, from 
the vision scientist’s point of view, they are two different concepts, since they come from two different 
mechanisms of visual attention: bottom-up and top-down. The two mechanisms are driven by different 
types of stimuli, and are formed in different visual pathways that go through different areas of the brain. 
Therefore, it would be worth identifying the two terms in the context of image processing.
In recent years, another problem faced by researchers in the field of visual attention is the impact 
of 3D. During the viewing of 3D content, depth perception of the scene is enhanced. This change of 
depth perception also largely changes human viewing behavior (Hakkinen, Kawai, Takatalo, Mitsuya, 
& Nyman, 2010; Huynh-Thu, Barkowsky, Le Callet, & others, 2011). Because of the emergence of 3D 
content and recent availability of 3D-capable display equipments, studies related to 3D visual attention 
have been gaining an increasing amount of attention in the last few years.
In this chapter we propose to clarify the last advancements in computational modeling of visual at-
tention.
The first section details the concepts of visual attention including the latest research results linking 
visual salience and visual importance. The main characteristics of the human visual system involved 
in the process of visual perception are explained in section 2. Because of their substantial role in the 
evaluation of the performances of the models, eye-tracking systems are presented in section 3. A com-
plete state of the art in computational modeling of visual attention is then presented in section 4. The 
research works that extend some visual attention models to 3D by taking into account of the impact of 
depth perception are finally explained in section 5.
1. VISUAL ATTENTION
It would be difficult to go directly into specific studies without a general introduction of some background 
knowledge on visual attention. So in this section, we first introduce concepts of visual attention as well 
as various mechanisms of attention. Secondly, we present a brief introduction of the HVS and of dif-
ferent types of eye movements as well as the technique for measuring eye movements, i.e. eye-tracking. 
Finally, we introduce some typical state-of-the-art computational models of visual attention.
The oldest and most famous definition of attention, which is provided by the psychologist William 
James (James, Burkhardt, & Skrupskelis, 1980), dates back to year 1890: “Everyone knows what atten-

77
2D and 3D Visual Attention for Computer Vision
﻿
tion is. It is the taking possession by the mind, in clear and vivid form, of one out of what seem several 
simultaneously possible objects or trains of thought. Focalization, concentration, consciousness are of 
its essence. It implies withdrawal from some things in order to deal effectively with others”.
In the HVS, attention plays an important role in visual processing by keeping only the essential vi-
sual information. Tsotsos et al. (1995) proposed that visual attention is a mechanism having at least the 
following basic components:
1. 	
The selection of a region of interest in the visual field;
2. 	
The selection of feature dimensions and values of interest;
3. 	
The control of information flow through the network of neurons that constitutes the visual system; 
and
4. 	
The shifting from one selected region to the next in time. Driven by visual attention, viewers can 
therefore selectively focus their attention on specific areas of interest in the scene.
1.1 Overt Attention and Covert Attention
There are two types of attention, namely overt attention and covert attention. These two types of atten-
tion are differentiated based on their relation with eye movements.
Overt attention is usually associated with eye movements. This type of attention is easy to observe: 
when we focus our attention to an object, our eyes move to fixate this object. One of the earliest studies 
of overt attention came from Yarbus (1967). In particular, he studied the correlation between visual at-
tention and eye movements during the viewing of human faces.
In addition to overt attention, William James et al. (1980) found that human are able to attend to 
peripheral locations of interest without moving the eyes; this type of attention is named as covert atten-
tion. An advantage of covert attention is its independence to motor commands (Frintrop, 2006). Since 
the eyes do not need to be moved to focus attention on a certain region, covert attention is much faster 
as compared to overt attention. An example of covert attention is driving, where a driver keeps his eyes 
on the road while simultaneously covertly monitoring the status of signs and lights (Borji & Itti, 2013).
Overt attention and covert attention are not independent. Humans cannot attend to one location while 
moving their eyes to a different location (Deubel & Schneider, 1996). The covert shift of attention to a 
location is linked to eye movement by setting up a saccade to that location (Peterson, Kramer, & Irwin, 
2004).
Most of current studies, especially the studies of computational modeling of visual attention, are 
with respect to overt attention, since overt attention can be measured in a straightforward way by using 
eye-tracking. However, it is difficult to measure covert attention. A computational framework for covert 
attention is also still lacking.
1.2 Bottom-Up Attention and Top-Down Attention
A shift of attention can be caused by two categories of cues: one is referred to as bottom-up cue, and 
the other one is referred to as top-down cue.
Bottom-up attention is driven by the characteristics of a visual scene, i.e. the bottom-up cues. Bottom-
up attention is hence also referred to as stimulus-driven attention or exogenous attention. Bottom-up 
attention is fast, involuntary, and most likely feed-forward (Borji & Itti, 2013). Since bottom-up attention 

78
2D and 3D Visual Attention for Computer Vision
﻿
is usually driven by low-level features (e.g. intensity, color, and orientation), in order to attract human’s 
bottom-up attention, an area must be sufficiently distinctive compared to the surrounding area with 
respect to these low-level visual features.
On the other hand, top-down attention is based on “higher level” information, such as knowledge, 
expectations and current goals (Desimone & Duncan, 1995). Top-down attention is thus also referred to 
as concept-driven attention, goal-driven or endogenous attention. As compared to bottom-up attention, 
top-down attention is slow, voluntary and driven by the task demands. A famous illustration of top-down 
attention comes from Yarbus’s work in 1967 (Yarbus, 1967). He demonstrated how eye movements varied 
depending on the question asked during the observation of the same scene (see Figure 1).
1.3 The Feature Integration Theory
One of the best-known and most accepted theories of visual attention is the “Feature Integration Theory”, 
which was proposed by Treisman and Gelade (1980). This theory has been the basis of many computa-
tional models of visual attention.
Treisman and Gelade (1980) claimed that “different features are registered early, automatically and 
in parallel across the visual field, while objects are identified separately and only at a later stage, which 
requires focused attention”. According to the Feature Integration Theory (FIT), the different features of 
Figure 1. An example that eye movements depend on observer’s viewing task
(Yarbus, 1967). (Image from Lucs-kho at en.wikipedia [Public domain] via Wikimedia Commons.).

79
2D and 3D Visual Attention for Computer Vision
﻿
stimuli are firstly encoded in areas partially independent. In addition, our hierarchical cortical structures 
are organized in order to make the detection of these features relatively independently of their positions 
in the visual scene.
The FIT introduced a concept of “feature maps”, which are topographical maps that highlight salience 
according to the respective feature. Information of the feature maps is then collected in a “master map 
of location”. This map indicates the location of the objects, but does not provide information about what 
the objects are.
Finally, to construct a coherent representation of the scene, selective attention is used. The scene is 
scanned by an attentional beam of variable size (see Figure 2). This beam blocks the information that 
is not located within its radius. It is thus possible to match all the features found in this area in order to 
build a coherent representation. By moving the beam over time, our brain gradually constructs a global 
perception of the scene.
1.4 Linking Visual Salience and Visual Importance
Visual salience (Koch & Ullman, 1987; Itti, Koch, & Niebur, 1998) and visual importance (Osberger 
& Maeder, 1998; Maeder, 1995; Etz & Luo, 2000; Kadiyala, Pinneli, Larson, & Chandler, 2008) come 
from the two different mechanisms of visual attention, the bottom-up mechanism and the top-down 
mechanism, respectively. Both visual salience and visual importance can provide important insights 
into how the human visual system addresses the image-analysis problem. Both of them are also believed 
Figure 2. Illustration of the Feature Integration Theory
(Perreira Da Silva, 2010).

80
2D and 3D Visual Attention for Computer Vision
﻿
to denote the most visually “relevant” parts of the scene. However, despite the differences in the way 
(bottom-up) visual salience and (top-down) visual importance are determined in terms of human visual 
processing, both salience and importance have traditionally been considered synonymous in the signal-
processing community.
Experimental Study
A study measuring the similarities and differences between visual salience and visual importance has 
been conducted. We present the results of these two psychophysical experiments and the associated 
computational analyses designed to quantify the relationship (and its evolution over time) between visual 
salience and visual importance:
•	
A first experiment was performed to obtain visual importance maps for a large database of im-
ages. A visual importance map is an object-level map that specifies the visual importance of each 
object in an image relative to the other objects in the image (including what would normally be 
considered as the background). The object(s) that receive(s) the greatest visual importance is/are 
traditionally considered as the image’s main subject. By using images from the Berkeley Image 
Segmentation Dataset, importance ratings were collected for each object in the 300 database im-
ages. Such importance ratings are generally believed to result from top-down visual processing, 
since the decisions used to rate each object typically involve scene interpretation, object recogni-
tion, and often consideration of artistic intent.
•	
In a second experiment, visual gaze patterns were measured for 80 of the images from the same 
Berkeley Image Segmentation Dataset. Using an eye-tracker, visual gaze locations were recorded 
under task-free viewing. Whereas importance maps are driven primarily by top-down processing, 
visual gaze patterns are generally believed to be driven by bottom-up, signal-based attributes, at 
least for early gaze locations. Bottom-up saliency (Koch & Ullman, 1987) is one particular signal-
based attribute, which has been shown to correlate well with early gaze locations. An image re-
gion is considered visually salient if it “stands out” from its background in terms of one or more 
attributes (e.g., contrast, color, orientation). When visual gaze patterns are measured in task-free 
viewing, one can consider the locations to denote the salient regions in the image. Thus, from the 
gaze patterns, one can construct an experimental saliency map.
Results and Analysis
Qualitative Observations of Importance Maps and Saliency Maps
A qualitative comparison of the saliency maps and importance maps reveals some distinct similarities 
and differences between the two. Figure 3 depicts some representative examples. 
First of all, one can notice that both importance maps and saliency maps are centrally biased, because 
source content is itself centrally biased by the photographer (the region of interest typically tends to be 
in the center of the photos that people take).
Tatler (2007) has proposed a method to correct data in very specific cases by using a centrally biased 
sampling distribution (from fixations from same observers of other images) when sampling non-fixated 
points for ROC computation. This helps evaluating the real values of a saliency prediction model without 

81
2D and 3D Visual Attention for Computer Vision
﻿
the effect of central bias. In our case, this method in not applicable since we don’t evaluate the perfor-
mance of a visual attention prediction model, but we compare using a fair approach, visual salience 
(namely here, salience and central bias) and visual importance maps.
The importance maps suggest that object category plays a bigger role than most other factors in de-
termining subjective importance. In general, we found that observers tended to rate objects containing 
human faces and/or animals (for instance, consider in Figure 3, the image at the left second line, or the 
one at the right first line) to be of greatest importance. Background objects such as sky and grass were 
generally rated to be of least importance (for instance, consider in Figure 3, the image at the left 5th line, 
or the one at the right 4th line). Occlusion, whether an object is in the foreground vs. the background 
(for instance, consider in Figure 3, the image at the right 7th line), also seems to be an important factor 
for perceived importance.
The saliency maps generally suggest that regions that possess a distinguished shape, color, contrast, 
or other local spatial features attract attention. However, observers always gazed upon the image’s main 
subject(s): Gaze position samples tended to occur on objects which belong to animal faces, human faces, 
or other subjects, which represent the region of interest in the image. The background, such as sky and 
ground, always attracted the least attention.
Yet, despite these similarities, the saliency maps and importance maps do not always agree. Although 
we employed a relatively long viewing period, the saliency maps never yielded an object-level segregation 
that is enforced in the importance maps. For example, whenever a face occurred in an image, whether 
an animal face or a human face, the observers’ gaze positions always occurred on the face.
Figure 3. Representative results from the experiments

82
2D and 3D Visual Attention for Computer Vision
﻿
Predicting the Main Subject, Secondary Subject, and the Background
The results of the qualitative analysis suggest a relationship between saliency maps and importance maps. 
One way to quantify this relationship is to attempt to predict the importance maps from the saliency 
maps using the object-level segmentations as side-information. To predict the importance maps from 
the saliency maps (given the segmentations), the following two approaches were tested:
1. 	
Mean Saliency: For each object, we summed those values of the saliency map that occurred within 
the object, and then we divided this value by the total number of pixels in the object. For each im-
age, the resulting set of per-object saliency values was then normalized to span the range [0, 1].
2. 	
Coverage Saliency: For each object, we summed those values of the saliency map which occurred 
within the object, and then we divided this value by the number of pixels in the object that were 
gazed upon (specifically, the number of pixels that were covered by the fovea). For each image, 
the resulting set of per-object coverage saliency values was then normalized to span the range [0, 
1].
To facilitate the prediction, each importance map was quantized into three classes based on the 
importance values:
•	
Main Subject: Consisted of objects that received an importance value ranging from 2/3 to 1;
•	
Secondary Subject: Consisted of objects that received an importance value ranging from 1/3 to 
2/3;
•	
Background: Consisted of objects that received an importance value ranging from 0 to 1/3.
The results of the prediction by the two approaches, namely Mean Saliency and Coverage Saliency 
are provided in Table 1 and Table 2, respectively. The prediction accuracy is presented in the form of 
confusion matrices. Each row of each matrix represents the actual (importance) class, and each column 
represents the predicted class. An ideal prediction would yield a diagonal matrix with 100% values, which 
means that all the objects belonging to each actual class are successfully predicted. As shown in Table 
1, Mean Saliency can successfully predict the main subject 80.5% of the time. In other words, 80.5% of 
the objects predicted as main subject are really the main subject of the scene; so 19.5% of the objects 
predicted as main subject actually belong to secondary subject or background. Similarly, the background 
is successfully predicted approximately 47% of the time. We also found that, Coverage Saliency (shown in 
Table 2) yields lower prediction accuracy for main subject, but slightly higher accuracy for background.
Table 1. Confusion matrices for predicting each subject’s importance from gaze data, and using the 
Mean Saliency approach
 
 
Predicted
Main Subject
Secondary Subject
Background
Actual 
Main Subject 
80.5% 
29.8%
12.6%
Secondary Subject 
12.5%
42.6% 
40.7%
Background 
7.1%
27.6%
46.7% 

83
2D and 3D Visual Attention for Computer Vision
﻿
Temporal Analysis
During normal viewing, because visual attention shifts from one object to another, the number of gaze 
position samples that occur on each subject varies over time. For each of the three levels of importance 
(main subject, secondary subject, background), we analyzed this time dependence. Specifically, we 
computed the number of gaze position samples per importance class which occurred within each 100-
ms interval during the 15-second viewing time. The resulting three time curves, summed across all 
observers, are shown in Figure 4. 
The plots in Figure 4 clearly indicate that, on average, subjects from different importance classes at-
tract considerably different amounts of visual attention. Specifically, throughout the 15-second viewing 
time, the main subject always received the greatest number of gaze position samples, approximately 7-8 
times greater than the number of samples for secondary subject and background.
Within 0-500 ms, the number of gaze position samples for the main subject (see Figure 4 (a)) was 
already 4-6 times greater than the number of samples for secondary subject (see Figure 4 (b)) and back-
ground (see Figure 4 (c)). This observation suggests bottom-up mechanisms can be effective at locating 
the main subject in these images; this might result from the fact that photographers tend to increase 
the saliency of the main subject via retouching, selective focusing, or other photographic techniques. 
Between 500-2000 ms, there was a pronounced increase in the number of gaze position samples for 
the main subject (see Figure 4 (a)), while the number for the other two importance classes decreased 
Table 2. Confusion matrices for predicting each subject’s importance from gaze data, and using the 
Coverage Saliency approach
 
 
Predicted
Main Subject
Secondary Subject
Background
Actual 
Main Subject 
56.5% 
38.6%
8.2%
Secondary Subject 
13.0%
40.4% 
24.7%
Background 
30.5%
21.1%
67.1% 
Figure 4. Total number of gaze position samples in (a) main subjects, (b) secondary subjects, and (c) 
background computed in each 100-ms interval of the 15-second viewing time; note that the scale for the 
vertical axis in the first graph is 10x that of the other two graphs.

84
2D and 3D Visual Attention for Computer Vision
﻿
in this period (see Figures 4 (b) and (c)). These changes potentially indicate the influence of top-down 
mechanisms that might force observers to attend to the main subject. After this process, the number of 
gaze position samples for the main subject slightly decreased, and those for the other two classes slightly 
increased. This latter change may imply that the observers attempt to explore the whole image, but their 
attention is still held by the main subject.
These three time curves suggest that the relationship between visual salience and visual importance 
may be time dependent. In particular, the fact that the main subject attracts the most attention within 
0-2000ms suggests that these early gaze position samples might be a better predictor of visual impor-
tance for the main subject than previously achieved using all samples. Accordingly, we predicted the 
importance maps by using the samples taken from only the first 0-2000ms. Table 3 lists the resulting 
confusion matrix computed (using Mean Saliency approach) based on gaze data of the first 2 seconds. 
Figure 5 depicts representative importance maps predicted from the data taken from all 15 seconds (see 
in Figure 5 the maps of the 3rd column) and from only the first two seconds (see in Figure 5 the maps of 
the 4th column). By using only these early gaze data, better prediction is achieved for the main subject.
This section presented the results of two psychophysical experiments and an associated computational 
analysis designed to quantify the relationship between visual salience (namely here, visual salience and 
central bias) and visual importance. We found that saliency maps and importance maps are related, but 
perhaps less than one might expect. The saliency maps were shown to be effective at predicting the 
main subjects. However, the saliency maps were less effective at predicting the subject of secondary 
importance and the unimportant one. We also found that the vast majority of early gaze position samples 
(0-2000 ms) were made on the main subject. This suggests that a possible strategy of the human visual 
system is to quickly locate the main subject in the scene.
The implications of the findings presented in this chapter are quite important for image processing. 
Several algorithms have been published which can successfully predict gaze patterns, e.g. (Itti et al., 1998; 
Le Meur, Le Callet, Barba, & Thoreau, 2006). These results suggest that these predicted patterns can 
be used to predict importance maps when coupled with a segmentation scheme. In turn, the importance 
maps can then be used to perform importance-based processing such as auto-cropping, enhancement, 
compression, unequal error protection, and quality assessment.
Removing the central bias in the analysis of the relationship between visual salience and visual impor-
tance is not a simple task, and it was not done in this initial work (J. Wang, Chandler, & Le Callet, 2010) 
on which this part of this chapter is based. This issue is for the moment left for further investigations.
Table 3. Confusion matrix (using Mean Saliency approach) for predicting importance from the first 2 
seconds of gaze samples
 
 
Predicted
Main Subject
Secondary Subject
Background
Actual 
Main Subject 
89.0% 
43.5%
12.4%
Secondary Subject 
3.3%
43.5% 
27.2%
Background 
7.7%
13.0%
60.5% 

85
2D and 3D Visual Attention for Computer Vision
﻿
2. VISUAL ATTENTION AND THE HUMAN VISUAL SYSTEM
Since visual attention is a mechanism involved in the process of visual perception, it is of importance 
to introduce also the knowledge regarding how visual information is processed in the human visual 
system. While being far from an exhaustive explanation of the HVS and the mechanisms involved in 
the processing of visual information, we briefly present in this section an introduction of the retina and 
different areas of the visual cortex (Figure 6) that allow determining the main characteristics of the HVS. 
Figure 5. Representative results of using all gaze samples vs. only those from the first two seconds to 
predict the importance maps

86
2D and 3D Visual Attention for Computer Vision
﻿
2.1 The Retina
The retina is a light-sensitive surface, which has over 100 million photoreceptor cells (Mather, 2009). 
The photoreceptor cells are responsible for transducing light energy into neural signals. Note that the 
retina is not of uniform spatial resolution. The density of photoreceptor cells is higher at the center, 
which enables vision to be more accurate at the center (i.e. the fovea) than at the periphery. There are 
two types of photoreceptor cells: rods and cones, which are sensitive to light and color, respectively. 
Cone photoreceptors can be divided into three classes based on their spectral sensitivity: “Blue” or short 
wavelength (S); “Green” or medium wavelength (M); and “Red” or long wavelength (L).
The photoreceptor cells are connected to Ganglion cells, which provide the output signal from the 
retina. The receptive field of ganglion cell is circular and separated into two areas: a center area and a 
surround area. Two types of ganglion cells exist: the on-center cells, which respond excitatorily to light 
at the center and off-center cells, which respond inhibitorily to light at the center (Frintrop, 2006). The 
center area and the surround area always have opposite characteristics. This is why center-surround 
filtering is used as a mechanism for processing visual information in many computational models of 
visual attention.
2.2 Visual Pathway
From the retina the optic nerve transmits visual information to the optic chiasm. From the optic chiasm, 
two visual pathways go to different areas of brain (see Figure 6). In primates, the majority (approximately 
90%) of the visual information is transmitted by the retino-geniculate pathway to the Lateral Geniculate 
Nuclei (LGN); the remaining 10% goes to the superior colliculi. LGN cell fibers then transmit visual 
information to the cortical receiving area for vision, known as primary visual cortex or V1, which is 
located at the very back of the brain. From the primary visual cortex, the visual information is sent to 
higher brain areas, namely extrastriate cortex. The extrastriate cortex includes: V2, V3, V4, the infero-
temporal cortex, the middle temporal area and the posterior-parietal cortex (Frintrop, 2006).
There is evidence that the connections between extrastriate areas segregate into two pathways after 
area V2: a ventral path way and a dorsal pathway. The dorsal pathway, which concerns the motion and 
depth information, runs via V3 to the middle temporal area (MT), then to the medial superior temporal 
Figure 6. The human visual system: from the retina to different areas of the visual cortex
Adapted from Kandel et al. (2000).

87
2D and 3D Visual Attention for Computer Vision
﻿
area (MST) and the parieto occipale area (PO) and finally to the posterior-parietal cortex (PP). The dor-
sal pathway is also named as the “where pathway”, since it mainly deals with the question of “where” 
something is in a scene. The ventral pathway, which processes color and form information, runs to V4 
and finally in infero-temporal cortex (IT). Since the area IT responds to the recognition of objects, this 
pathway is also named as the “what pathway”.
2.3 Attentional Mechanisms in the Brain
So far, it is believed that visual attention is not guided by any single brain area. Several areas have been 
found to be involved in the attentional process, but the accurate task and behavior of each area, as well 
as the interplay among these areas, still remain an open questions (Frintrop, 2006). Nevertheless, several 
findings have been claimed. It was proposed that the posterior-parietal cortex responds to disengaging 
the focus of attention from its present location (inhibition of return); the superior colliculus (SC) is re-
sponsible for shifting the attention to a new location (Posner & Petersen, 1990). The Frontal Eye Field 
area of the prefrontal cortex is found to be involved in guiding the eye movements. Additionally, this 
area is also the place where a kind of saliency map is located, which is affected by both bottom-up and 
top-down information (Bichot, 2001).
3. EYE MOVEMENTS AND EYE-TRACKING
Eye tracking is a technique which records the eye movements so that researchers can obtain precise 
information about
1. 	
Where an observer is looking at any given time, and
2. 	
The sequence in which his eyes are shifting from one location to another.
Eye tracking plays a substantial role in the research of psychology, biology, computer vision, and 
especially the computational modeling of visual attention. Given the strong link between overt visual 
attention and eye movements (Itti & Koch, 2001; Wolfe & Horowitz, 2004), eye movements data col-
lected by means of eye tracking experiment are used as the ground truth to evaluate the performance of 
computational models.
3.1 Measures of Eye-Movements
Just and Carpenter (1976) assumed that what a person is looking at indicates what is at the “top of the 
stack” in cognitive processes. This “eye-mind” hypothesis implies that the eye movements provide a trace 
about where a person’s (overt) attention is being directed. There exist various types of eye movements. 
Two basic ones are “fixation” and “saccade”. From these two basic eye movements, another measure-
ment, “scanpath”, is stemmed. Moreover, pupil size and blink rate are also two types of eye movements 
usually studied. Introduction of each type of eye movement as well as metrics based on these basic types 
of eye movement are presented.

88
2D and 3D Visual Attention for Computer Vision
﻿
Fixations
A fixation means that the visual gaze is approximately stationary on a single location. Fixations last for 
218 milliseconds on average, with a range of 66 to 416 milliseconds (Poole & Ball, 2006). Based on 
fixations, several metrics can be derived:
•	
Fixations per Area of Interest: Experiments show that more fixations on a particular area indi-
cate a greater interest or importance of a target (J. Wang, Chandler, & Le Callet, 2010). And it may 
also mean that the target is complex in some way and difficult to encode (Just & Carpenter, 1976). 
Jacob and Karn (2003) suggest that, in a search task, a higher number of fixations often means a 
greater uncertainty in recognizing a target item.
•	
Fixations Duration: A longer fixation can be interpreted in two ways, it may imply that informa-
tion is difficult to extract, or the object is more engaging in some way (Just & Carpenter, 1976).
•	
Fixation Spatial Distribution: Cowen et al. (2002) suggested that highly concentrated fixations 
in a small area mean a focused and efficient searching, and evenly spread fixations indicate a wide-
spread and inefficient searching. It was also found that if an object contains an area with highly 
concentrated fixations, the object tends to be considered as of high importance (J. Wang et al., 
2010).
•	
Repeat Fixations or “Post-Target Fixations”: A higher number of off-target fixations after the 
target has been fixated (i.e., a lower number of repeat fixations) means that the target lacks mean-
ingfulness or visibility (Goldberg & Kotval, 1999).
•	
Time to First Fixation On-Target: A shorter time to first-fixation on an object or area indi-
cates that the object or area has better attention-getting properties (Byrne, Anderson, Douglass, 
& Matessa, 1999).
Note that in the studies of computational modeling of visual attention, fixation spatial density is the 
metric mostly used, by means of computing a so-called “fixation density map”.
Saccades
Saccades are those quick, simultaneous movements of both eyes in the same direction (Cassin, Solomon, 
& Rubin, 1990). They are fast movements of eyes occurring between fixations. It is generally believed 
that no encoding takes place in the human visual system during saccades, so vision is suppressed and 
it is difficult for us to get any clues about the complexity or salience of an object from the saccades. 
However, information about visual perception can be still extracted from several saccade metrics:
•	
Number of Saccades: A larger number of saccades indicates that more searching takes place dur-
ing the observation (Goldberg & Kotval, 1999).
•	
Saccade Amplitude: Saccade amplitude is computed by measuring the distance between one 
saccade’s start point (a fixation) and its end point (another fixation). Larger amplitude indicates 
the existence of more meaningful cues, since the attention is drawn from a distance (Goldberg, 
Stimson, Lewenstein, Scott, & Wichansky, 2002).

89
2D and 3D Visual Attention for Computer Vision
﻿
Scanpaths
Scanpath is a metric derived from the measurement of both fixations and saccades. A scanpath means a 
complete saccade-fixate-saccade sequence. The area covered by scanpath indicates the area observed. A 
longer scanpath means a less efficient searching (Goldberg et al., 2002). Additionally, we can compare 
the time spent for searching (saccades) to the time spent for processing (fixation) in a scanpath. A higher 
saccade/fixation ratio means more searching or less processing.
Blink Rate and Pupil Size
The blinking of eyes and the variation of pupil size are two eye movements that could also be recorded 
during eye tracking experiments. They can be considered as a cue that indicates cognitive workload. A 
lower blink rate is assumed to indicate a higher cognitive workload (Bruneau, Sasse, & McCarthy, 2002), 
and a higher blink rate may indicate visual fatigue (Brookings, Wilson, & Swain, 1996). The changing 
of pupil size also indicates some kinds of cognitive effort (Marshall, 2000). However, the blink rate 
and the pupil size can be easily affected by many factors during the observation, e.g. the luminance of 
environment. Due to this reason, blink rate and pupil size are not widely used in the researches related 
to visual attention.
3.2 Eye-Tracking
Eye tracking is a technique which records eye movements so that the researchers can obtain precise 
information about
1. 	
Where an observer is looking at any given time, and
2. 	
The sequence in which his eyes are shifting from one location to another.
Eye tracking has thus been deployed in a variety of disciplines to capture and analyze overt visual 
attention of human observers, including neuroscience, psychology, medicine, human factors, marketing, 
and computer science (Duchowski, 2002).
The common goal amongst all these disciplines is to capture human viewing behavior when performing 
specific visual tasks in a given context. For instance, in marketing research it is of interest to determine 
what products customers attend to in order to maximize profit (Wedel & Pieters, 2007). In medical 
research it is of interest to identify the search patterns of radiologists when investigating mammograms 
for improved breast cancer detection (Wedel & Pieters, 2007). In image and video quality assessment, 
taking into account the attention of viewers to artifacts may lead to enhanced quality prediction models 
(Engelke, Kaprykowsky, Zepernick, & Ndjiki-Nya, 2011). In the context of computational modeling 
of visual attention, eye-tracking results are usually post-processed into scanpaths or so-called fixation 
density maps (FDM), which are considered to be a reliable ground truth for developing computational 
models of visual attention.

90
2D and 3D Visual Attention for Computer Vision
﻿
Background of Eye Tracking Technique
The technology of eye tracking appeared more than 100 years ago in reading research (Rayner, Pollatsek, 
Ashby & Clifton, 2012). Since, different techniques have been applied to eye tracking. For instance, 
the “electro-oculographic techniques” needs to put electrodes on the skin around the eye so that eye 
movements can be detected by measuring the differences in electric potential. Some other methods rely 
on wearing large contact lenses. The lenses cover the cornea (the transparent front part of the eye) and 
sclera (the white part of the eye), while a metal coil is embedded around the lens so it moves along with 
the eye. The eye movements can thus be measured by fluctuations in an electromagnetic field when the 
eye is moving (Duchowski, 2007). However, these historical methods affect observers’ eye-movement 
and are inconvenient to implement.
Video-based techniques are used by modern eye-trackers to determine where a person is looking 
at (i.e., the so-called “gaze point” or “point-of-regard”). These eye-trackers achieve the detection of 
point-of-regard based on the eye’s features extracted from video images of the eye, such as corneal re-
flections (i.e. Purkinje images), iris-sclera boundary, and the apparent pupil shape (Poole & Ball, 2006; 
Duchowski, 2007).
Most state-of-the-art commercial eye trackers use the “corneal-reflection/pupil-centre” method to 
measure the point-of-regard. The corneal reflection is also known as (first) Purkinje image. During the 
eye-tracking recording, a camera focuses on one or both eyes to get images. Contrast is then used to 
get the location of the pupil, and infrared light is used to create a corneal reflection. By measuring the 
movements of corneal reflection relative to the pupil, it is then possible to know the head movement, 
eye rotation, the direction of gaze and consequently the point-of-regard.
Algorithms for Eye-Movement Identification
Given the information about eye-movement type (e.g. fixations, saccades) and their characteristics (e.g. 
duration, spatial distribution), various subsequent analyses can then be performed depending on the 
particular context and application of the research. However, the raw eye-movement data output from 
eye-tracking experiments are usually presented by means of a stream of sampled gaze points. Post-
processings need to be performed to identify different types of eye movements from the gaze points.
Fixation detection algorithms extract and label fixations and saccades from raw eye-tracking data 
(i.e. sample points). These algorithms can identify the fixations, the saccades taking place between two 
successive fixations, and those smaller eye movements occurring during fixations, such as tremors, drifts, 
and flicks (Salvucci & Goldberg, 2000). Note that the fixation identification is a critical aspect of eye-
movement data analysis, since its result can significantly affect later analyses. Evidences have showed 
that different identification algorithms could produce different interpretations even when analyzing the 
same eye-tracking data (Karsh & Breitenbach, 1983).
Salvucci and Goldberg (2000) suggested that most fixation identification algorithms took advantage 
of the following spatial or temporal features:
•	
Velocity: Some algorithms take advantage of the fact that fixation points have much lower veloci-
ties compared to the saccades. Generally, the sampling rate of an eye-tracker is constant, so the 
velocity equals to the distance between sample points.

91
2D and 3D Visual Attention for Computer Vision
﻿
•	
Dispersion: Some algorithms emphasize the spread distance (i.e. dispersion) of fixation points. It 
assumes that the sample points belonging to a fixation generally occur near one another, but sac-
cades are far away from others.
•	
Duration Information: This criterion is based on the fact that fixations are rarely less than 100 
ms and usually in the range of 200-400 ms.
•	
Local Adaptivity: This criterion means that the interpretation of a given point is influenced by the 
interpretation of temporally adjacent points.
Based on the different features selected, various fixation identification algorithms have been proposed. 
The two main types of fixation identification algorithms are introduced below.
Velocity-Based Algorithms
The velocity information of eye movements shows two distributions of velocities: low velocities for fixa-
tions, and high velocities for saccades. This velocity-based discrimination is straightforward and robust.
Among various velocity-based algorithms, Velocity-Threshold Identification (I-VT) is the simplest 
one to implement (Salvucci & Goldberg, 2000). I-VT calculates firstly point-to-point velocities for each 
point. Each velocity is computed as the distance between the current point and the next (or previous) 
point. Each point is then classified as a saccade point or fixation point based on a velocity threshold: if 
the velocity is higher than the threshold, it becomes a saccade, otherwise it becomes a fixation point. 
Finally, I-VT translate each fixation group into a <x, y, t, d> representation. <x, y> represent the cen-
troid of the points, t and d means the time of the first point and the duration of the points respectively.
A more sophisticated type of velocity-based algorithm is Hidden Markov Model fixation Identification 
(I-HMM) (Salvucci, 1999; Salvucci & Anderson, 1998). I-HMM applies a two-state HMM in which the 
two states represent the velocity distributions for saccade and fixation points, respectively. Generally, 
I-HMM can perform more robust identification than fixed-threshold methods (e.g. I-VT) (Salvucci & 
Goldberg, 2000).
Dispersion-Based Algorithms
Dispersion-based Algorithms use the fact that fixation points tend to cluster closely together because of 
their low velocity. Dispersion-Threshold Identification (I-DT) is a typical type of the dispersion-based 
algorithms. I-DT identifies fixations as groups of consecutive points within a particular dispersion. A 
dispersion threshold is thus essential for I-DT algorithms. Moreover, a minimum duration threshold is 
also required, which is used to help alleviate equipment variability. The minimum duration threshold 
normally ranges from 100 ms to 200 ms (Widdel, 1984).
An implementation of I-DT algorithm is proposed by Widdel et al. (1984). They use a moving win-
dow to cover consecutive data points. The moving window begins at the start of the protocol. It initially 
contains a minimum number of points which is determined by a given duration threshold. The I-DT then 
computes the dispersion of the points in the window by summing the differences between the points’ 
maximum and minimum x and y: D=[max(x) - min(x)] + [max(y) - min(y)] .

92
2D and 3D Visual Attention for Computer Vision
﻿
If the dispersion is above a dispersion threshold, the window moves to the following point. If the dis-
persion is below the threshold, the window represents a fixation and will be expended until the window’s 
dispersion is above the threshold. The final window is marked as a fixation that centers at the centroid 
of the points and has a given onset time and duration.
4. COMPUTATIONAL MODELING OF VISUAL ATTENTION
Eye-tracking experiments can be considered as a reliable way to acquire the distribution of human’s 
attention on a specific scene. However, conducting eye-tracking experiments is usually cumbersome, 
time consuming, and hence, expensive. In order to automatically predict the distribution of human’s 
attention, extensive research efforts have been dedicated to computational modeling of visual attention. 
In our study, we particularly focus on the models that compute saliency maps. The results of this type 
of model, the saliency maps, indicate where the most visually interesting regions are located.
In the past years, a body of models using various mathematical tools has been proposed. According 
to the taxonomy introduced by Le Meur and Le Callet (2009), most of the computational models can be 
grouped into three main categories: hierarchical model, statistical model, and Bayesian model.
4.1 Main Computational Models
Hierarchical Models
This kind of model is characterized by the use of a hierarchical decomposition, whether it involves a 
Gaussian, a Fourier-based or wavelet decomposition. Various feature maps are then computed. Different 
strategies are then used to integrate information across sub-bands to create a final saliency map.
The Model of Itti
One of the most famous models of this category is the model proposed by Laurent Itti et al. (1998). It is 
the first computational and biologically plausible model of bottom-up visual attention, and it serves as 
a basis in many studies. The architecture of this model (see Figure 7) is based on the following principle 
steps. The original image is firstly decomposed into three different perceptual channels: intensity, color 
and orientation. A multi-scale representation is constructed from the various channels. A set of linear 
center-surround operations akin to visual receptive fields is performed to obtain feature maps. These 
feature maps are
1. 	
Normalized by an operator N , which enhances the feature maps containing a small number of 
peaks of saliency, and
2. 	
Then summed to create the conspicuity maps (intensity, color, and orientation).
These three conspicuity maps are also normalized and summed to obtain a unique saliency map. In 
this model, the selection of focuses of attention (FOA) is achieved by a “winner-take-all” network, which 
selects the most salient area in the saliency map and contains an “inhibition of return” mechanism to 
temporarily prevent the FOA return immediately to the areas already visited. 

93
2D and 3D Visual Attention for Computer Vision
﻿
The Model of Le Meur
Another representative model comes from Le Meur et al. (2006). It is also a bottom-up model based 
on Treisman’s Feature Integration Theory (Treisman & Gelade, 1980) and the biologically plausible 
architecture proposed by Koch and Ullman (1987). This model was first described by Le Meur et al. 
(2006) and then modified in (Le Meur, Le Callet, & Barba, 2007), in order to take into account motion. 
We introduce here the original version of the model. 
Le Meur’s model (see Figure 8) builds on a coherent psychovisual space. Three aspects of the vision 
process are tackled: visibility, perception, and perceptual grouping. The “visibility” process simulates 
the limited sensitivity of the human visual system. For an input image, RGB luminance is first trans-
formed into the Krauskopf’s color space (A, Cr1 and Cr2 ), which simulates the three channels used by 
retina for visual information encoding. The first channel, A, transforms achromatic perceptual signals; 
the second channel, Cr1, transforms chromatic perceptual signals of the opponent colors of red-green; 
and the third channel, Cr2 , transforms chromatic perceptual signals of the opponent colors of blue-
yellow. A contrast sensitivity function is then applied to each of the three channels. These contrast 
Figure 7. Architecture of Itti’s model
Adapted from Itti et al. (1998).

94
2D and 3D Visual Attention for Computer Vision
﻿
sensitivity functions show how the sensitivity of human eye varies as a function of spatial frequency and 
orientation. A hierarchical decomposition is then applied to each of the three channels. The decomposi-
tion consists in splitting the 2D spatial frequency domain both in spatial radial frequency and in orienta-
tion. In this model, each channel is considered as the feature map corresponding to a specific set of 
neurons. A “perception” process is then applied, in which a center-surround mechanism is performed 
to simulate the HVS for selecting relevant areas and reducing the redundant incoming visual information. 
The third process of the model is “perceptual grouping”. It refers to the human visual ability which 
groups and binds visual features to organize a meaningful higher-level structure. Finally, this computa-
tional model sums the output of the different channels to obtain a two-dimensional spatial saliency map. 
Note that Le Meur et al. (2007) proposed a computational model for video, in which motion is considered 
as an additional visual channel. The result of the motion channel is a temporal saliency map, which is 
finally combined with the spatial saliency map to get a final saliency map.
Figure 8. Architecture of Le Meur’s model
(Le Meur et al., 2006).

95
2D and 3D Visual Attention for Computer Vision
﻿
Statistical Models
This kind of model utilizes probabilistic methods to compute the saliency. The probabilistic framework 
is deduced from the content of the current image. The measure of saliency of each location is based on 
various features, and is defined as the deviation of these features between the current location and its 
surrounding region. Note that
1. 	
The features used in statistical models refer not only to the low level visual features (e.g. color, 
intensity or orientation) but also some features derived by Independent Component Analysis (ICA) 
or Principal Component Analysis (PCA) algorithms; and
2. 	
Even the whole image might be considered as the “surrounding region” in these models.
The Model of Bruce and Tsotsos
Bruce and Tsotsos (2009) proposed a model of saliency computation based on the premise that localized 
saliency computation serves to maximize information sampled from one’s environment. The framework 
of this model is depicted in Figure 9. 
The first step of this model is the independent feature extraction. For each location (i, j) in the image, 
the response of various learned filters that simulate V1 cortical cells are computed. This operation is 
considered as measuring the response of various cortical cells coding for content at each individual 
spatial location. Gabor-like cells that respond to orientation structure within a specific spatial frequency 
band and cells that respond to color opponency are taken into account. This step yields a group of coef-
ficients for each local neighborhood of the scene Ci j, .
The second stage is density estimation. The content of each local neighborhood Ci j,  of the image is 
characterized by several coefficients ak . These coefficients, ak , correspond to the various basis filters 
coding for that location. At one spatial location and in the surrounding regions of that location, there is 
Figure 9. Architecture of the model of Bruce and Tsotsos
Adapted from Bruce & Tsotsos (2009).

96
2D and 3D Visual Attention for Computer Vision
﻿
a set of coefficients for a same filter type. Based on a non-parametric or histogram density estimate, the 
coefficients in the surround form a distribution that can be used to predict the likelihood of the coeffi-
cients of Ci j, . Any given coefficient can be then converted to a probability by looking up its likelihood 
from the probability distribution derived from the surround. Based on the probabilities, joint likelihood 
of each location can be computed, which is then translated into Shannon’s measure of Self-information. 
The resulting information map serves as the output of the model, the spatial saliency map.
The Model of Gao et al.
The model proposed by Gao et al. (2008) computes a so-called “discriminant center-surround saliency” 
by combining
1. 	
The classical assumption that bottom-up saliency is a center-surround process, and
2. 	
A discriminant saliency hypothesis.
The computation of saliency is formulated as a binary classification problem. For each location l in 
the input image, the saliency is defined with respect to two classes of stimuli: stimuli of interest and null 
hypothesis. Stimuli of interest refer to the observations within a neighborhood of l, (i.e. Wl
1, which is 
referred to as the center); null hypothesis refers to the observations within a surrounding window, (i.e. 
Wl
0 , which is referred to as the surround). The saliency of each location is thus equal to the discriminant 
power, which is quantified by mutual information, for the classification of the observed features that 
comes from the center area and the surround.
Bayesian Models
In addition to information from the current image, the Bayesian framework is applied in this type of 
models to also take into account prior knowledge. This prior knowledge concerns, for instance, the sta-
tistic of visual features in natural scenes, including its distribution or its spectral signature. Since prior 
knowledge from the perceptual learning process would help the human visual system to understand the 
visual environment, the integration of prior knowledge into computational models could be compared 
to a visual priming effect that would facilitate the scene perception (Le Meur & Le Callet, 2009).
The Model of Zhang et al.
The model proposed by Zhang et al. (2008) is based on the assumption that one goal of human visual 
system is to find potential targets by estimating the probability of a target at every location given the 
visual features. The proposed model relies on a Bayesian probabilistic framework, in which bottom-up 
saliency is regarded as the self-information of visual features; when searching for a target, the overall 
saliency is considered as the point-wise mutual information between the features and the target. By
1. 	
Letting the binary variable C denote whether a point belongs to a target class,
2. 	
Letting the random variable L denotes the location,
3. 	
Letting the random variable F denote the visual features, the computation of saliency of specific 
location z (e.g. a pixel) is formulated by:

97
2D and 3D Visual Attention for Computer Vision
﻿
S
p C
F
f L
l
z
z
z
=
=
=
=
(
)
1 |
,
	
where fz represents the feature observed at z, and l represents the location (i.e. pixel coordinates) of z.
Compared to other bottom-up saliency measures, which are defined solely in terms of the image cur-
rently being viewed, this model is defined based on natural statistics collected from a set of images of 
natural scenes. And this is the reason why it is named SUN. Besides, compared to the others, it involves 
only local computation on images, without calculation of global image statistics or saliency normaliza-
tion or winner-take-all competition.
Due to the assumptions that
1. 	
Features and location are independent and conditionally independent given C = 1, and
2. 	
The distribution of a feature does not change with location, the formulation is given by:
log
log (
)
log (
,
)
log (
,
)
S
p F
f
p F
f C
p F
f L
l
z
z
z
z
z
= −
=
+
=
=
+
=
=
1
.	
The first term on the right side of this equation, −
=
log (
)
p F
fz , is the self-information. The rarer 
the visual features are, the more informative they are. The second term, log (
,
)
p F
f C
z
=
= 1 , is a log-
likelihood term which favors feature values consistent with our knowledge of the target. It corresponds 
to the top-down effect when searching for a known target. The third term in the equation, 
log (
,
)
p F
f L
l
z
z
=
=
, is independent of visual features and represents any prior knowledge of where 
the target is likely to appear. In the free-viewing condition, both the location prior knowledge and the 
log-likelihood term are unknown, so the bottom-up saliency is equal to the self-information, −
=
log (
)
p F
fz .
4.2 Features for Visual Saliency Detection
The selection of visual features is of great importance in the computational modeling of visual attention. 
According to the feature integration theory (Treisman & Gelade, 1980), three features have been widely 
used in existing computational models of visual attention: intensity, color and orientation (Borji & Itti, 
2013). Intensity is usually processed by a center-surround process, which is inspired by neural responses 
in lateral geniculate nucleus (LGN) and V1 cortex. To extract this feature, two types of filter are used to 
simulate the response of visual cells that have a center ON (resp. OFF) and a surround OFF (resp. ON). 
Color is usually taken into account by means of the red/green and the blue/yellow color pairs, which is 
inspired by color-opponent neurons in V1 cortex. Orientation is usually implemented as a convolution 
with oriented Gabor filters or by the application of oriented masks. Motion is also used in the models for 
video (in the primate brain motion is derived by the neurons at MT an MST regions which are selective 
to direction of motion (Borji & Itti, 2013). In addition to the basic visual features introduced previously, 
some other specific features that direct human’s attention have been used in the modeling of visual atten-
tion (Borji & Itti, 2013), including: faces (Cerf, Harel, Einhäuser, & Koch, 2008), horizontal lines (Oliva 
& Torralba, 2001), wavelets (Li, Tian, Huang, & Gao, 2010), gist (Torralba, 2003), center-bias (Tatler, 
2007), spatial resolution (Hamker, 2005), optical flow (Vijayakumar, Conradt, Shibata, & Schaal, 2001), 
flicker (Itti, Dhavale, & Pighin, 2004), crosses or corners (Privitera & Stark, 2000), entropy (Kadir & 

98
2D and 3D Visual Attention for Computer Vision
﻿
Brady, 2001), ellipses (Lee, Buxton, & Feng, 2005), symmetry (Kootstra, Nederveen, & De Boer, 2008), 
texture contrast (Parkhurst et al., 2002), depth (Maki, Nordlund, & Eklundh, 2000), components derived 
by ICA or PCA algorithms (L. Zhang et al., 2008; Bruce & Tsotsos, 2009).
5. EXTENSION TO THE COMPUTATIONAL MODELING 
OF STEREOSCOPIC 3D VISUAL ATTENTION
After introducing the studies regarding the ground truth of 3D visual attention and the impact of depth 
perception on visual attention, we particularly focus on the modeling of 3D visual attention. A new 3D 
visual attention model relying on both 2D visual features and features extracted from depth information 
is proposed and evaluated in this section.
We first introduce and summarize previous works on 3D visual attention. A taxonomy of computational 
models of 3D visual attention is proposed. After that, we introduce a depth-saliency-based model of 3D 
visual attention. To get benefit from psychophysical studies, we propose to apply Bayes’s theorem on 
the result of an eye-tracking experiment using synthetic stimuli to model the correlation between depth 
features and the level of depth saliency. We also introduce and discuss two ways of combining depth 
saliency map with 2D saliency map.
During the viewing of stereoscopic 3D content, disparity information is used by the brain to retrieve 
the 3D layout of the environment, it leads to a stereoscopic perception of depth. This change of depth 
perception also largely modifies human’s visual attention when watching stereoscopic 3D images/videos 
(Hakkinen, Kawai, Takatalo, Mitsuya, & Nyman, 2010b; Quan Huynh-Thu, Barkowsky, & Le Callet, 
2011). Predicting the salient areas of a 3D scene becomes thus a challenging task due to the additional 
depth information.
Several challenges, importance and new applications of visual attention for 3D content viewing were 
introduced by Huynh-Thu et al. (2011). They described the conflicts that the human visual system has to 
deal with during watching 3D-TV. They also explained how these conflicts might be limited, and how 
visual comfort could be improved by knowing how visual attention is deployed. Several new applica-
tion areas that can benefit from being provided the location (including depth) of salient areas were also 
introduced. These candidate applications exist in the different steps of a typical 3D-TV delivery chain, 
e.g. 3D video capture, 2D to 3D conversion, reframing and depth adaptation, and subtitling in 3D movie.
The increasing demand of visual-attention-based applications for 3D content highlights the impor-
tance of computationally modeling 3D visual attention. However, two questions need to be figured out 
for developing a 3D visual attention model:
•	
The influence of 2D visual features (e.g. color, intensity, orientation, and center-bias) in 3D view-
ing condition.
•	
The influence of depth on visual attention deployment in 3D viewing condition. For instance, it is 
necessary to figure out how the bias of fixations according to depth (i.e. the depth-bias), and the 
visual features based on depth information (e.g., the orientation of surface, the contrast of depth) 
affect the deployment of human’s visual attention.

99
2D and 3D Visual Attention for Computer Vision
﻿
In the last decade, a large number of 2D visual attention models have been created. Therefore, the 
first question concerns the possibility of adapting this large amount of existing 2D models into the 3D 
case. On the other hand, the second question concerns the means by which the additional information, 
depth, can be taken into account.
The research on 3D visual attention modeling is also facing another problem: the lack of published 
eye-tracking database of 3D images. In addition to the lack of quantitative evaluation of performance, 
another consequence of the lack of ground truth is that most of the existing 3D visual attention models 
only take into account, in a qualitative way, the results of psychophysical experiments about depth’s 
influence or the variation of 2D features’ effects. Any model that quantitatively integrates experimental 
observation results is still missing so far. Moreover, there is still no strong conclusion on the means by 
which depth information should be used in 3D visual attention modeling: whether the depth should be 
used to weight 2D saliency map; or alternatively it should be considered as an additional visual dimen-
sion to extract depth features and create depth saliency map.
5.1 How 3D Visual Attention Is Affected by Various Visual Features
Based on observations from psychophysical experiments, several studies have started to examine both 
qualitatively and quantitatively how visual attention may be influenced by the 2D visual features and 
additional binocular depth cues.
One of the early works was done by Jansen et al. (2009) who investigated the influence of disparity 
on viewing behavior in the observation of 2D and 3D still images. They conducted a free-viewing task 
on the 2D and 3D versions of the same set of images. They found that additional depth information led 
to an increased number of fixations, shorter and faster saccades, and increased spatial extent of explora-
tion. However, no significant difference was found between the viewing of 2D and 3D stimuli in terms 
of saliency of several 2D visual features including mean luminance, luminance contrast, and texture 
contrast. This consistence of the influence of 2D low-level visual features implied:
1. 	
The importance of 2D visual feature detection in the design of 3D visual attention model, and
2. 	
The possibility of adapting existing 2D visual attention models in the modeling of 3D visual 
attention.
Liu, Cormack and Bovik (2010) examined visual features at fixated positions for stereo images with 
natural content. They focused on comparing visual features extracted from fixations and random loca-
tions in the viewing of 3D still images. On one hand, they demonstrated that some 2D visual features 
including luminance contrast and luminance gradient were generally higher at fixated areas. On the 
other hand, their results also indicate that disparity contrast and disparity gradient of fixated locations 
are lower than randomly selected locations. This result is inconsistent with the result from Jansen et al 
(2009) who found that observers consistently look more at depth discontinuities (high disparity contrast 
areas) than at planar surfaces. One limitation of Liu et al.’s study might rely on the quality of ground 
truth disparity map. The disparity maps they used came from a simple correspondence algorithm rather 
than any depth range sensing systems or any sophisticated depth estimation algorithms. The final results 
might thus be affected by a considerable amount of noise in the estimated disparity maps.

100
2D and 3D Visual Attention for Computer Vision
﻿
Hakkinen et al. (2010) examined the difference in eye movement patterns between the viewing of 2D 
and 3D versions of the same video content. They found that eye movements are more widely distributed 
for 3D content. Compared to the viewing of 2D content, viewers did not only look at the main actors but 
also looked at some other targets on typical movie content. Their result shows that depth information 
from the binocular depth cue provides viewers additional information, and thus creates new salient areas 
in a scene. This result suggests the existence of a saliency map from depth, and a potential “summa-
tion” operation during the integration of 2D and depth saliency information. In opposite, Ramasamy et 
al.’s study (Ramasamy, House, Duchowski, & Daugherty, 2009), which is related to stereo-filmmaking, 
showed that observers’ gaze points could be more concentrated when viewing the 3D version of some 
content (e.g. the scenes containing long deep hallway).
In terms of the depth plane where fixations tend to be located, Wang, Le Callet, Ricordel and Tour-
ancheau (2011) examined a so-called “depth-bias” in task-free viewing of still stereoscopic synthetic 
stimuli. They found that objects closest to the observer always attract most fixations. The number of 
fixations on each object decreases as the depth order of the object increases, except that the furthest 
object receives a few more fixations than the one or two objects in front of it. The number of fixations 
on objects at different depth planes was also found to be time dependent. This result is consistent with 
the result of Jansen et al. (2009). Considering the influence of center-bias in 2D visual attention, these 
results indicate the existence of a location prior according to depth in the viewing of 3D content. This 
location prior indicates the possibility of integrating depth information by means of doing a weighting.
Wismeijer, Erkelens, van Ee and Wexler (2010) examined if saccades were aligned with individual 
depth cues or with a combination of depth cues by presenting stimuli in which monocular perspective 
cues and binocular disparity cues conflicted. Their results indicate a weighted linear combination of 
cues when the conflicts are small, and a cue dominance when the conflicts are large. They also found 
that vergence is dominated only by binocular disparity. Their result implies that the interocular distance 
recorded by binocular eye-tracking experiment for 3D content should be compensated by taking into 
account the local disparity value.
5.2 Previous Works on 3D Visual Attention Modeling
As introduced previously, great efforts have been put into the study of viewing behavior of 3D content. 
However, only a few computational models of 3D visual attention have been proposed as compared to 
the body of 2D visual attention models. Experimental results have demonstrated strong influences of 
2D visual features, in the viewing of 3D content. However, due to the addition of new depth cues, depth 
features, and their combination or conflicts with other monocular cues (Hoffman, Girshick, Akeley, & 
Banks, 2008; Okada et al., 2006), a direct use of 2D visual attention model for 3D content is neither 
biologically plausible nor effective.
Furthermore, the disparity between two views can raise serious challenges on collecting 3D gaze 
points and creating fixation density maps, which are used as ground-truth, since the gaze data need to be 
extrapolated or processed to provide a notion of depth in relation with gaze direction or location (Quan 
Huynh-Thu et al., 2011).
In the literature, a few computational models of 3D visual attention have been proposed. All of these 
models contain a stage in which 2D visual features are extracted and used to compute 2D saliency maps. 
According to the ways they use depth information, these models can be classified into three different 
categories: depth-weighting model, depth-saliency model, and stereovision model.

101
2D and 3D Visual Attention for Computer Vision
﻿
Depth-Weighting Models
This type of model (Maki, Nordlund, & Eklundh, 1996; Y. Zhang, Jiang, Yu, & Chen, 2010; Chamaret, 
Godeffroy, Lopez, & Le Meur, 2010) do not contain any depth-map-based feature-extraction processes. 
Apart from detecting salient areas by 2D visual features, these models share a same step in which depth 
information is used as the weighting factor of 2D saliency. The saliency of each location (e.g. pixel, 
target or depth plane) in the scene is directly related to its depth. Both 2D scene and depth map are 
taken as input. Note that depth maps used in these models can be a ground truth depth map provided by 
depth detection equipment, or come from depth estimation algorithms which use two or multiple views.
Depth-Saliency Models
The models (Ouerhani & Hugli, 2000; Potapova, Zillich, & Vincze, 2011) in this category take depth 
saliency as additional information. This type of model relies on the existence of “depth saliency maps”. 
Depth features are first extracted from depth map to create additional feature maps, which are used to 
generate the depth saliency maps. These depth saliency maps are finally combined with 2D saliency maps 
(e.g. from 2D visual attention models using color, orientation or intensity) by using saliency map pooling 
strategy to obtain a final 3D saliency map. This type of model takes as input 2D scene and depth map.
Stereovision Models
Instead of directly using depth map, this type of model takes into account the mechanisms of stereoscopic 
perception in the human visual system. Bruce and Tsotsos (2005) extend the 2D models that use a visual 
pyramid processing architecture by adding neuronal units for modeling stereo vision. Images from both 
views are taken as input, from which 2D visual features can be considered. In addition, the model takes 
into account the conflicts between two eyes resulting from occlusions or large disparities.
Summary of the Previous Studies
Table 4 introduces the main properties of the models belonging to each of the three categories. So far, 
most of the existing computational models of 3D visual attention belong to the first or the second cat-
egory. Figure 10 summarizes the two different ways by which depth information is taken into account 
in these two types of model.
5.3 Recent Works: Depth-Saliency-Based 
Computational Model of 3D Visual Attention
Depth features have demonstrated their contribution in predicting saliency map of 3D images. Several 
depth features (e.g. surface curvature, depth gradient, relative surface orientation) have been proposed 
and used in previous 3D models. In this section, the way of creating depth map from which depth fea-
tures can be extracted is first introduced. In a second step, we introduce a new method for generating 
a so-call “depth saliency map”. The depth saliency map is computed based only on depth features (i.e. 
depth contrast) by using a Bayesian framework.

102
2D and 3D Visual Attention for Computer Vision
﻿
Depth Map Creation
We propose that a depth map providing the information of a scene’s perceived depth needs to be computed 
at the first step of modeling 3D visual attention. In the literature, the disparity map is usually directly 
adopted as depth information (Chamaret et al., 2010). We propose that a transformation from disparity 
map to depth map which represents perceived depth in unit of length should be added in the chain of 3D 
Table 4. Main features of computational models of 3D visual attention
Depth Weighting
Depth Information 
Operation 
Validation 
Maki, Nordlund and 
Eklundh (2000) 
Relative depth
Assigned the target closer to observers with 
highest priority
Qualitative assessment; no quantitative 
comparison to eye-tracking data
Zhang et al. (2010) 
Perceived depth, 
pop-out effect
Irregular space conversion. Pixels closer 
to observers and in front of screen is 
considered to be higher salient
Qualitative assessment; no quantitative 
comparison to eye-tracking data
Chamaret et al. 
(2010) 
Relative depth
Weight each pixel in 2D saliency map by 
its depth value
Qualitative assessment; no quantitative 
comparison to eye-tracking data
Depth Saliency 
Depth Information 
Operation 
Validation 
Ouerhani and Hugli 
(2000) 
Absolute depth 
(distance), surface 
curvature, depth 
gradient
Extract depth features from depth map. 
Compute additional conspicuity maps 
based on depth features. Pool all the 
conspicuity maps (from 2D features and 
depth features)
Qualitative assessment; no quantitative 
comparison to eye-tracking data
Potapova et al. 
(2011) 
Surface height, relative 
surface orientation, 
occluded edges
Compute one saliency map for each 
(2D and depth) feature, then sum all the 
saliency maps
Qualitative assessment and quantitative 
comparison to labeled ROIs
Stereovision 
Depth Information 
Operation 
Validation 
Bruce and Tsotsos 
(2005) 
Disparity
Take two views as input. Add interpretive 
neuronal units for stereo-vision modeling 
into 2D computational model which use 
visual pyramid processing architecture [34].
Qualitative assessment; no quantitative 
comparison to eye-tracking data
Figure 10. Two different ways of using depth in the depth-weighting models (left), and the depth-saliency 
models (right); note that the main difference between these two types of model is the existence of a stage 
for extracting depth features and creating depth saliency map.

103
2D and 3D Visual Attention for Computer Vision
﻿
visual attention modeling, since even the same disparity value corresponds to different perceived depth 
depending on the viewing condition.
From the view of display system, disparity is measured in unit of pixels. The relationship between 
disparity (in pixel) and perceived depth can be modeled by the following equation (see also Figure 11):
D
V
I R
P W
x
=
+
⋅
⋅






1
, 	
(1)
where D represents the perceived depth, V represents the viewing distance between observer and screen 
plane, I represents the interocular distance, P is the disparity in pixels, W and Rx represent, respectively, 
the width (in cm) and the horizontal resolution of the screen.
According to this Equation 1, perceived depth is not only a function of disparity but also influenced by 
the viewing conditions (the viewing distance and the properties of the display). For instance, an 8-pixel 
negative disparity can create a perceived depth of about 3.5 cm behind the screen when it is presented 
on a 24-inch full-HD stereoscopic display with 1-meter viewing distance (3 times of the screen’s height). 
However, the same disparity corresponds to a perceived depth of infinite on a 2k-cinema screen with 
8-meter screen height and 8-meter viewing distance. When the viewing condition varies, the change 
of perceived depth from even a same disparity value might make some areas of a 3D scene impossible 
to fuse. Consequently, the saliency distribution can be different. In this study, we adopt Equation 1 to 
compute the depth map for each image, the interocular distance is set to 6.3 cm, while the screen prop-
erty parameters are set according to the setup of the eye-tracking experiment conducted for building the 
Bayesian based model presented).
A Bayesian Approach of Depth Saliency Map Generation
In the area of saliency map creation, Bayes’s theorem has been widely applied in various ways (L. Zhang, 
Tong, Marks, Shan, & Cottrell, 2008; Chikkerur, Serre, Tan, & Poggio, 2010; Pinneli & Chandler, 2008). 
Figure 11. The relationship between disparity and perceived depth (top view)

104
2D and 3D Visual Attention for Computer Vision
﻿
In this chapter, we propose a new approach of Bayes’s theorem for computing a depth saliency map based 
on features extracted from a depth map. The proposed approach correlates depth features with the level 
of depth saliency, by using data from a psychophysical experiment.
We firstly introduce the proposed definition of depth saliency: the depth saliency (S) of each location 
(a pixel) equals to the probability of this point being gazed at, given depth features observed at this point 
and the spatial location of this point:
S
P C
f
l
dep
z
=
=
(
)
1 |
,
, 	
(2)
where C is a binary random variable denoting whether or not a point is gazed at. The random variable 
vector fdep  denotes depth features observed at this point, lz denotes its location in depth. Note that the 
term about ‘features’, fdep , stands for not only the local visual features such as relative depth (i.e. dispar-
ity) and absolute depth (i.e. distance to observer), but also some higher order features considering the 
information from neighborhood, such as the result of applying Difference of Gaussian kernel (DoG) on 
feature maps.
Regarding to the right side of equation 2,P C
f
l
dep
z
=
(
)
1 |
,
, we make the assumptions that 1) the 
depth features of each point are independent of its distance to the viewer, and 2) P(C=1) is simply a 
constant.
By using Bayes’ rule, this probability can be thus transformed to:
S
P C
f
P C
l
const
dep
z
=
=
(
)⋅
=
(
)⋅
1
1
|
|
.	
(3)
The first term in equation 3, P C
fdep
=
(
)
1 |
, represents the probability of a point to be gazed at, 
given only the features extracted from depth information at this point. By computing this probability, 
the saliency map from depth channel can be obtained. The second term in this equation, P C
lz
=
(
)
1 |
, 
represents the probability of a point to be gazed at given its distance to the viewer. This probability re-
flects observers’ viewing strategy, the bias of eyes position, or the prior knowledge about at which 
distance potential targets are likely to appear. Compared to the well known ‘center-bias’ regarding to the 
location prior in the viewing of 2D image (Tatler, 2007; Tseng, Carmi, Cameron, Munoz, & Itti, 2009), 
relatively little of this preference of observation in depth is known and studied. Recently, this preference 
was quantified and named as ‘depth-bias’ by Wang et al. (2011). Therefore, based on the proposed 
model of depth saliency, the saliency value of each point in a three dimensional scene can be considered 
as a combination of visual saliency from depth features and depth prior. However, studying depth-bias 
is not in the scope of this chapter. In the following part, we focus on the introduction of modeling 
P C
fdep
=
(
)
1 |
, omitting the depth prior part.
By using Bayes’ rule, we can get:

105
2D and 3D Visual Attention for Computer Vision
﻿
P C
f
P f
C
P f
dep
dep
dep
=
(
) =
⋅
=
(
)
(
)
1
1
|
|
α
, 	
(4)
where α is a constant value representing the probability P(C=1). The function P C
f dep
=
(
)
1 |
 repre-
sents how depth features observed at a point, influence the probability of the human visual system of 
deciding whether to fixate this point or not. This probability is proportional to the feature distribution 
at a gaze point, normalized by the rarity of features in the context (see equation 4). Note that the use of 
the likelihood, P f
C
dep |
=
(
)
1  in the proposed approach differs from the way in which it is usually 
used by many models in the literature applying also Bayes’s theory. We are not doing any binary clas-
sification to make a decision that a point is a fixation or not. Instead, we define the result (i.e. depth 
saliency map) as a distribution of probability of the points being gazed at as a function of depth features.
To achieve the computation of depth saliency map, the proposed approach consists of two stages:
1. 	
Depth feature extraction, and
2. 	
Probability distribution modeling.
Depth Feature Extraction
The proposed model uses depth contrast as feature for depth saliency map prediction. In most situations, 
depth contrast can be an efficient indicator of interesting targets. For example, the HVS might consider 
a region protruding above a flat plane as a potential target (Potapova et al., 2011); or might consider a 
hole as a place where potential target might exist.
Difference of Gaussians (DoG) filter was applied to the depth map for feature extraction (i.e. depth 
contrast). DoG filter has been widely used by computational models in the literature due to its resemblance 
to the receptive fields of neurons, and its capability of simulating the center-surround mechanism in the 
human visual system. The DoG filters used in the proposed model were generated by:
f x y
x
y
K
x
y
K
( , )
exp
exp
=
−
+





−
−
+
1
2
2
1
2
2
2
2
2
2
2
2
2
2
2
πσ
σ
π
σ
σ2





, 	
(5)
where (x,y) is the location in the filter. σ  and K were used to control the scales of DoG and the ratio 
between the ‘center’ area and ‘surround’ area. In this chapter, we selected a scale as σ =32 pixels (ap-
proximately corresponding to 1 degree of visual angle in our experiment) and a center/surround ratio of 
1/1.6 (the same value as the one used in (L. Zhang et al., 2008)).
Probability Distribution Modeling
In the proposed model, the function P C
f contrast
=
(
)
1 |
 models the relationship between the depth 
contrast of each position in a scene and the probability that this position is gazed. We propose to model 
this function based on a probability-learning of eye movement data collected from an eye-tracking ex-
periment.

106
2D and 3D Visual Attention for Computer Vision
﻿
An important factor that can affect modeling is the stimuli used in the eye-tracking experiment. In 
this study, eye movement data are obtained from an eye-tracking experiment using synthetic stimuli. 
These stimuli consisted of 3D scenes in which a background and some similar objects were deliberately 
displayed at different depth positions. The details of this experiment have been introduced in our previ-
ous study (J. Wang et al., 2011).
The probability distribution P(fcontrast) can be obtained based on the depth contrast maps of the syn-
thetic stimuli. By considering the probability distribution of depth contrast at gaze points recorded dur-
ing the viewing, P f
C
contrast |
=
(
)
1  can be then obtained. Therefore, the likelihood P C
f contrast
=
(
)
1 |
 
which models the relationship between depth contrast and the probability of being fixated, can be ob-
tained by Equation 4. In Figure 12, we illustrate the resulting likelihood distribution P C
f contrast
=
(
)
1 |
. 
For the implementation of the proposed model, the modeled P C
f contrast
=
(
)
1 |
 is applied on the depth 
feature map. By taking the depth contrast value at each pixel as input, the saliency value of each pixel 
in an image can be thus computed.
A Framework of Computational Modeling of 3D Visual Attention
In this section, we introduce the framework which integrates the depth saliency map with the saliency 
maps computed based on 2D visual features, and achieves the prediction of the final 3D saliency map. 
The general architecture of the proposed framework is presented in Figure 13.
Figure 12.  The distribution P C
f contrast
=
(
)
1 |
 resulting from the eye-tracking experiment using syn-
thetic stimuli

107
2D and 3D Visual Attention for Computer Vision
﻿
2D Saliency Map Generation
Since developing a completely new computational model of 2D visual attention is not in the scope of 
the present chapter, we leave the work of 2D visual features detection and 2D saliency map creation to 
existing models. Three bottom-up visual attention models using quite different mechanisms were used 
to perform the 2D saliency prediction, and involved in the final performance evaluation:
1. 	
Itti’s model (Itti, Koch, & Niebur, 1998) (obtained from http://www.saliencytoolbox.net/),
2. 	
Bruce’s AIM model (Bruce & Tsotsos, 2009) (obtained from http://www-sop.inria.fr/members/
Neil.Bruce/), and
3. 	
Hou’s model (Hou & Zhang, 2007) (obtained from http://www.klab.caltech.edu/~xhou/).
In the proposed framework, 2D saliency computation is only performed based on the image from 
the left view which is selected arbitrarily, since the images from the two views are quite similar, and 
the difference in 2D features between the two views’ images has thus only marginal influence on visual 
attention deployment. Computing a 2D saliency map based on only one view instead of both views can 
be beneficial to decrease the computational complexity of the model.
Saliency Maps Combination
In the literature, although several approaches combining conspicuity maps of 2D visual features have 
been proposed, there are still not any specific and standardized approach to combine saliency maps from 
depth and 2D visual features. In the proposed model, we adopt a straightforward approach which is the 
Figure 13. Overview diagram of the proposed framework

108
2D and 3D Visual Attention for Computer Vision
﻿
same as the one used in the work of Potapova et al. (2011) to merge the depth saliency map SMdep  and 
2D saliency map SM D
2 : the final saliency map SMS  is equal to the sum of both maps (see Equation 
6):
SM
i j
SM
SM
S
dep
D
( , ) =
+
ω
ω
1
2
2 , 	
(6)
where ω1 = ω2 =0.5.
5.4 Performance Assessment
Both qualitative and quantitative comparisons between the fixation density map and the output of the 
proposed model are performed in this section. All the results are obtained and based on our 3D image 
eye-tracking database which is introduced in (J. Wang, Da Silva, Le Callet, & Ricordel, 2013).
Figure 14. Examples of the performance of different models including from left to right: (a) original 
image, (b) human saliency map, (c) the depth saliency maps created by the proposed model, and the 
saliency maps created by (d) Itti’s model, (e) Bruce’s model, (f) Hou’s model

109
2D and 3D Visual Attention for Computer Vision
﻿
Qualitative Assessment
Figure 14 gives some examples of the performance of depth saliency maps (the predicted saliency maps 
created based on only depth map) and predicted saliency maps based on only 2D visual features (from 
the three 2D visual attention models introduced previously). The fixation density maps generated by 
eye-tracking data are also provided to be the ground-truth for the qualitative assessment. In each saliency 
map, brighter areas correspond to areas with higher saliency.
Qualitatively speaking, the proposed approach creates depth saliency maps that well predict salient 
areas in the 3D images. All the potential salient areas are depicted to be salient in the depth saliency 
maps. Compared to the depth saliency maps, the contribution of 2D visual features in predicting saliency 
of a 3D image largely depends on the model selected: Itti’s model usually predicts some most salient 
areas, while it misses many areas of middle level saliency; Bruce’s model significantly highlights the 
edges; Hou’s model (as well as Bruce’s model) is largely affected by the appearance of the large amount 
of texture in the background. For instance, in the image on the last row, the trees in the background are 
assigned high saliency by both these models.
Quantitative Metrics of Assessment
The goal of quantitative assessment is to quantify how well the proposed computational model of vi-
sual attention predicts fixation density maps coming from eye-tracking experiments. So far, there are 
no specific and standardized measures to compare the similarity between fixation density maps and 
the saliency maps created by computational models in 3D situation. Nevertheless, there exists a range 
of different measures that are widely used to perform the comparison between saliency maps for 2D 
content. The most common ones include:
1. 	
Correlation coefficient (Le Meur, Le Callet, Barba, & Thoreau, 2006; Engelke, Maeder, & Zepernick, 
2010),
2. 	
Kullback-Leibler divergence (Le Meur et al., 2006; Bruce & Tsotsos, 2009), and
3. 	
Area under the receiver operating characteristics (ROC) curve (L. Zhang et al., 2008; Zhao & Koch, 
2011).
The former two are directly applicable to a comparison between a fixation density map and a predicted 
saliency map, whereas the area under the ROC curve (AUC) is usually used to compare the actual fixa-
tion (or gaze) points to a predicted saliency map. Since the disparity compensation for binocular eye-
tracking data has been done during the process of fixation density map creation, the two fixation density 
maps from both views have been merged to one. We therefore adopt these three similarity measures 
to quantitatively compare a merged fixation density map and a predicted saliency map from one view.
Performance of Depth Saliency Map
The creation of depth saliency map and the 2D saliency map are the two main parts of the proposed 
model. In order to assess the extent to which these two sources of saliency maps can predict the salient 
areas in a scene, the performance of the depth saliency map only is compared with the performance of 
the 2D saliency maps that come from three state-of-the-art 2D visual attention models, respectively.

110
2D and 3D Visual Attention for Computer Vision
﻿
The results (see Table 5) from all the three objective metrics show that the depth saliency map has 
a significantly higher performance than Itti’s model. Compared to Bruce’s model and Hou’s model, the 
performance of the depth saliency map is still higher, but without significant difference (except that the 
KL divergence value shows that the depth saliency map significantly outperforms Hou’s model). These 
results demonstrate a great influence of the depth contrast on the distribution of visual attention in the 
viewing of 3D content.
Added Value of Depth Saliency Map
The proposed model in the present chapter belongs to the category “depth-saliency model”, which high-
lights the existence of a depth saliency map. To compare the two different ways of taking advantage of 
depth information, the performance of the following methods was measured and compared:
•	
No-Depth Method: This is a direct deployment of 2D computational model, no depth information 
is taken into account.
•	
Depth-Weighting (DW) Method: We adopt Chamaret’s method (Chamaret et al., 2010), which 
weights each pixel in the 2D saliency map by multiplying it with the depth value of the corre-
sponding pixel in the depth map. Since we do not have the code to apply exactly the same 2D 
computational model used in their paper, the 2D saliency map creation part is replaced here by 
the models of Itti, Bruce, or Hou.
•	
Depth-Saliency (DS) Method: i.e. the proposed computational model in this section. It creates a 
depth saliency map and a 2D saliency map respectively, then combines the resulting saliency map 
from both paths to get the final result.
The performance of these three methods is shown in Table 6. Large added values of the depth saliency 
map are demonstrated for all the three 2D visual attention models. The proposed model outperforms both 
the DW method and the 2D models in predicting salient areas of 3D images. As it is difficult to have 
an idea of what a good performance is, we remind here the performance of these three state-of-the-art 
2D models which has been validated on different 2D-image databases: Itti’s model has a PLCC value 
ranging from 0.27 to 0.31 (Perreira Da Silva, 2010); Bruce’s model has a PLCC value ranging from 0.40 
to 0.45 (Perreira Da Silva, 2010); and Hou’s model has an AUC value staying around 0.69 (Le Meur 
& Chevet, 2010). Compared to the performance of these state-of-the-art 2D models on 2D content, the 
proposed model (DS method) is demonstrated to have a comparable level of performance on 3D content.
Table 5. Performance of depth saliency map (noted as DSM in the table) only and three state-of-the-art 
2D models; note that a smaller KLD score means a better performance.
PLCC
KLD
AUC
DSM only
0.368
0.708
0.656
Itti’s model
0.141*
2.809*
0.540*
Bruce’s model
0.325
0.735
0.638
Hou’s model
0.291
0.802*
0.630
* Means that it is significantly different from the performance of the DSM (paired t-test, p < 0.1).

111
2D and 3D Visual Attention for Computer Vision
﻿
The proposed model contains a depth saliency map creation part that is based on a probability-learning 
from the experimental data. By integrating the depth saliency map with the results of 2D visual fea-
ture detection, the proposed model provides a good prediction of salient areas. On the other hand, it is 
demonstrated that the performance of depth saliency map and its added value to 2D model vary across 
different types of scene. When too many areas (or no area) can be indicated as salient based on 2D visual 
features, depth becomes the dominant cue that directs viewer’s attention. But the depth saliency map 
does not perform well in showing the salient areas that are caused by only 2D visual features.
Two different ways of applying depth information in 3D visual attention models are compared in our 
study. Our results show that, creating a depth saliency map based on depth contrast achieves a higher 
performance than a simple depth-weighting method (a multiplication of 2D saliency map and depth map). 
This result indicates the importance of a depth saliency map in modeling 3D visual attention. Neverthe-
less, this result should not lead to a strong conclusion that a depth-saliency model is definitely better or 
worse than a depth-weighting model, since depth-weighting models also have various advantages, such 
as a low computational complexity, or comparable performances for some types of scenes. On the other 
hand, it would be reasonable to suggest that an efficient 3D visual attention model can be an integration 
of both types of models: depth information is treated as an additional visual dimension from which depth 
features are extracted to create depth saliency maps; as a possible extension, the location information 
(e.g. center-bias or depth-bias) is also used as weighting information to relate the distribution of atten-
tion and the distance between the observer and each object in the scene.
6. CONCLUSION
In this chapter, we detail advancements in computational modeling of visual attention by presenting 
studies focusing on several aspects of the research of visual attention.
We quantitatively identify the similarities and differences between visual saliency maps and impor-
tance maps, which are two widely used ground truths for attention-related applications. By comparing 
Table 6. Contribution of the depth information on 2D models; note that a smaller KLD score means a 
better performance
PLCC
KLD
AUC
Itti’s model
2D model only
0.141
2.809
0.540
Chamaret’s method
0.140
2.892
0.540
Proposed method
0.356*
0.701*
0.656*
Bruce’s model
2D model only
0.325
0.735
0.638
Chamaret’s method
0.311
0.810
0.639
Proposed method
0.423*
0.615
0.674
Hou’s model
2D model only
0.291
0.802
0.630
Chamaret’s method
0.290
0.878
0.633
Proposed method
0.409
0.603*
0.669
* Means that it is significantly different from the performance of the corresponding 2D model (paired t-test, p < 0.1).

112
2D and 3D Visual Attention for Computer Vision
﻿
the importance maps with the saliency maps, we show that the two types of maps are related, and dem-
onstrate that saliency maps can be effective at predicting the main subject of a scene.
The main characteristics of the human visual system involved in the process of visual perception 
are also explained. A focus on eye-tracking systems is done because of their role in the research for 
evaluating the performances of the models, and a complete state of the art in computational modeling 
of visual attention is presented.
Lastly, a depth-saliency-based model of 3D visual attention is described where Bayes’s theorem is 
applied for learning from previous eye-tracking data in order to compute the depth saliency map. The 
results demonstrate a large added value of depth saliency map and a good performance of the proposed 
depth-saliency model. Two different ways of applying depth information in 3D visual attention model 
are compared. Results show that, creating a depth saliency map based on depth feature achieves a higher 
performance than a simple depth-weighting method. But the proposed model still has limitations: for 
instance it takes into account only one depth feature, or a simple pooling strategy is used to combine 
the depth saliency map and 2D saliency map. In future works, one might consider including more depth 
features (e.g. surface curvature or depth gradient) and to use a more sophisticated pooling strategy, to 
improve the performance of such model. This chapter also does not present any practical application of 
the proposed 3D stereoscopic visual attention model. However, as mentioned for example in (Q. Huynh-
Thu, Barkowsky, Le Callet, & others, 2011), potential applications of 3D models are numerous, not only 
for 3DTV but also for a wider range of domains (e.g. robotics). As a consequence, future works might 
focus on applications of our model.
REFERENCES
Bichot, N. P. (2001). Attention, eye movements, and neurons: Linking physiology and behavior. Vision 
and Attention, 209–232.
Borji, A., & Itti, L. (2013). State-of-the-art in Visual Attention Modeling. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 35(1), 185–207. doi:10.1109/TPAMI.2012.89 PMID:22487985
Brookings, J. B., Wilson, G. F., & Swain, C. R. (1996). Psychophysiological responses to changes in 
workload during simulated air traffic control. Biological Psychology, 42(3), 361–377. doi:10.1016/0301-
0511(95)05167-8 PMID:8652753
Bruce, N. D. B., & Tsotsos, J. K. (2005). An attentional framework for stereo vision. In Computer and 
Robot Vision, 2005. Proceedings. The 2nd Canadian Conference on, (pp. 88–95). IEEE. doi:10.1109/
CRV.2005.13
Bruce, N. D. B., & Tsotsos, J. K. (2009). Saliency, attention, and visual search: An information theoretic 
approach. Journal of Vision, 9(3).
Bruneau, D., Sasse, M. A., & McCarthy, J. (2002). The eyes never lie: The use of eye tracking data in 
HCI research. In Proceedings of the CHI, (vol. 2, p. 25). ACM.
Byrne, M. D., Anderson, J. R., Douglass, S., & Matessa, M. (1999). Eye tracking the visual search of 
click-down menus. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems, 
(pp. 402–409). ACM. doi:10.1145/302979.303118

113
2D and 3D Visual Attention for Computer Vision
﻿
Cassin, B., Solomon, S., & Rubin, M. L. (1990). Dictionary of eye terminology. Wiley Online Library.
Cerf, M., Harel, J., Einhäuser, W., & Koch, C. (2008). Predicting human gaze using low-level saliency 
combined with face detection. Advances in Neural Information Processing Systems, 241–248.
Chamaret, C., Godeffroy, S., Lopez, P., & Le Meur, O. (2010). Adaptive 3D rendering based on region-
of-interest. In IS&T/SPIE Electronic Imaging (p. 75240V). SPIE.
Chikkerur, S., Serre, T., Tan, C., & Poggio, T. (2010). What and where: A Bayesian inference theory 
of attention. Vision Research, 50(22), 2233–2247. doi:10.1016/j.visres.2010.05.013 PMID:20493206
Cowen, L., Ball, L. J., & Delin, J. (2002). An eye movement analysis of web page usability. In People 
and Computers XVI-Memorable Yet Invisible, (pp. 317–335). Springer.
Desimone, R., & Duncan, J. (1995). Neural mechanisms of selective visual attention. Annual Review of 
Neuroscience, 18(1), 193–222. doi:10.1146/annurev.ne.18.030195.001205 PMID:7605061
Deubel, H., & Schneider, W. X. (1996). Saccade target selection and object recognition: Evidence for a 
common attentional mechanism. Vision Research, 36(12), 1827–1837. doi:10.1016/0042-6989(95)00294-
4 PMID:8759451
Duchowski, A. T. (2002). A breadth-first survey of eye-tracking applications. Behavior Research Methods, 
Instruments, & Computers, 34(4), 455–470. doi:10.3758/BF03195475 PMID:12564550
Duchowski, A. T. (2007). Eye tracking methodology: Theory and practice. Springer Science & Business 
Media New York.
Engelke, U., Kaprykowsky, H., Zepernick, H.-J., & Ndjiki-Nya, P. (2011). Visual Attention in Quality 
Assessment. Signal Processing Magazine, IEEE, 28(6), 50–59. doi:10.1109/MSP.2011.942473
Engelke, U., Maeder, A., & Zepernick, H. J. (2010). Analysing inter-observer saliency variations in task-
free viewing of natural images. In Image Processing (ICIP), 2010 17th IEEE International Conference 
on, (pp. 1085–1088). IEEE.
Etz, S. P., & Luo, J. (2000). Ground truth for training and evaluation of automatic main subject detec-
tion. Electronic Imaging, 434–442.
Frintrop, S. (2006). VOCUS: A visual attention system for object detection and goal-directed search 
(Vol. 3899). Springer-Verlag New York. doi:10.1007/11682110
Gao, D., Mahadevan, V., & Vasconcelos, N. (2008). On the plausibility of the discriminant center-
surround hypothesis for visual saliency. Journal of Vision, 8(7).
Goldberg, J. H., & Kotval, X. P. (1999). Computer interface evaluation using eye movements: Methods 
and constructs. International Journal of Industrial Ergonomics, 24(6), 631–645. doi:10.1016/S0169-
8141(98)00068-7
Goldberg, J. H., Stimson, M. J., Lewenstein, M., Scott, N., & Wichansky, A. M. (2002). Eye tracking in 
web search tasks: design implications. In Proceedings of the 2002 symposium on Eye tracking research 
& applications, (pp. 51–58). ACM. doi:10.1145/507072.507082

114
2D and 3D Visual Attention for Computer Vision
﻿
Hakkinen, J., Kawai, T., Takatalo, J., Mitsuya, R., & Nyman, G. (2010). What do people look at when 
they watch stereoscopic movies?. Electronic Imaging, 7524, 75240E.
Hamker, F. H. (2005). The emergence of attention by population-based inference and its role in distrib-
uted processing and cognitive control of vision. Computer Vision and Image Understanding, 100(1), 
64–106. doi:10.1016/j.cviu.2004.09.005
Hoffman, D. M., Girshick, A. R., Akeley, K., & Banks, M. S. (2008). Vergence–accommodation conflicts 
hinder visual performance and cause visual fatigue. Journal of Vision (Charlottesville, Va.), 8(3), 33. 
doi:10.1167/8.3.33 PMID:18484839
Hou, X., & Zhang, L. (2007). Saliency detection: A spectral residual approach. In Computer Vision and 
Pattern Recognition, 2007. CVPR’07. IEEE Conference on, (pp. 1–8). IEEE.
Huynh-Thu, Q., Barkowsky, M., & Le Callet, P. et al. (2011). The Importance of Visual Attention in 
Improving the 3D-TV Viewing Experience: Overview and New Perspectives. Broadcasting. IEEE 
Transactions on, 57(2), 421–431.
Itti, L., Dhavale, N., & Pighin, F. (2004). Realistic avatar eye and head animation using a neurobiologi-
cal model of visual attention. In Optical Science and Technology, SPIE’s 48th Annual Meeting, (pp. 
64–78). SPIE.
Itti, L., & Koch, C. (2001). Computational modeling of visual attention. Nature Reviews. Neuroscience, 
2(3), 194–203. doi:10.1038/35058500 PMID:11256080
Itti, L., Koch, C., & Niebur, E. (1998). A model of saliency-based visual attention for rapid scene analysis. 
Pattern Analysis and Machine Intelligence. IEEE Transactions on, 20(11), 1254–1259.
Jacob, R. J. K., & Karn, K. S. (2003). Eye tracking in human-computer interaction and usability research: 
Ready to deliver the promises. Mind, 2(3), 4.
James, W., Burkhardt, F., & Skrupskelis, I. K. (1980). The principles of psychology (Vol. 1). Harvard 
Univ Press.
Jansen, L., Onat, S., & König, P. (2009). Influence of disparity on fixation and saccades in free viewing 
of natural scenes. Journal of Vision, 9(1).
Just, M. A., & Carpenter, P. A. (1976). Eye fixations and cognitive processes. Cognitive Psychology, 
8(4), 441–480. doi:10.1016/0010-0285(76)90015-3
Kadir, T., & Brady, M. (2001). Saliency, scale and image description. International Journal of Computer 
Vision, 45(2), 83–105. doi:10.1023/A:1012460413855
Kadiyala, V., Pinneli, S., Larson, E. C., & Chandler, D. M. (2008). Quantifying the perceived interest of 
objects in images: effects of size, location, blur, and contrast. In Electronic Imaging (p. 68060S). SPIE.
Kandel, E. R., Schwartz, J. H., & Jessell, T. M. (2000). Principles of Neural Science (Vol. 4). McGraw-
Hill New York.
Karsh, R., & Breitenbach, F. (1983). Looking at looking: The amorphous fixation measure. Eye Move-
ments and Psychological Functions: International Views, 53–64.

115
2D and 3D Visual Attention for Computer Vision
﻿
Koch, C., & Ullman, S. (1987). Shifts in selective visual attention: towards the underlying neural circuitry. 
In Matters of intelligence, (pp. 115–141). Springer.
Kootstra, G., Nederveen, A., & De Boer, B. (2008). Paying attention to symmetry. In Proceedings of the 
British Machine Vision Conference (BMVC2008), (pp. 1115–1125). BMVC.
Le Meur, O., & Chevet, J. C. (2010). Relevance of a feed-forward model of visual attention for goal-
oriented and free-viewing tasks. Image Processing. IEEE Transactions on, 19(11), 2801–2813.
Le Meur, O., & Le Callet, P. (2009). What we see is most likely to be what matters: visual attention and 
applications. In Image Processing (ICIP), 2009 16th IEEE International Conference on, (pp. 3085–3088). 
IEEE.
Le Meur, O., Le Callet, P., & Barba, D. (2007). Predicting visual fixations on video based on low-level 
visual features. Vision Research, 47(19), 2483–2498. doi:10.1016/j.visres.2007.06.015 PMID:17688904
Le Meur, O., Le Callet, P., Barba, D., & Thoreau, D. (2006). A coherent computational approach to 
model bottom-up visual attention. Pattern Analysis and Machine Intelligence. IEEE Transactions on, 
28(5), 802–817.
Lee, K. W., Buxton, H., & Feng, J. (2005). Cue-guided search: A computational model of selective at-
tention. Neural Networks. IEEE Transactions on, 16(4), 910–924.
Li, J., Tian, Y., Huang, T., & Gao, W. (2010). Probabilistic multi-task learning for visual saliency estima-
tion in video. International Journal of Computer Vision, 90(2), 150–165. doi:10.1007/s11263-010-0354-6
Liu, H., & Heynderickx, I. (2011). Visual Attention in Objective Image Quality Assessment: Based on 
Eye-Tracking Data. Circuits and Systems for Video Technology. IEEE Transactions on, 21(7), 971–982.
Liu, Y., Cormack, L. K., & Bovik, A. C. (2010). Natural scene statistics at stereo fixations.Proceedings of the 
2010 Symposium on Eye-Tracking Research & Applications, 161–164. ACM. doi:10.1145/1743666.1743706
Maeder, A. J. (1995). Importance maps for adaptive information reduction in visual scenes. In Intelligent 
Information Systems, 1995. ANZIIS-95.Proceedings of the Third Australian and New Zealand Confer-
ence on, (pp. 24–29). IEEE. doi:10.1109/ANZIIS.1995.705709
Maki, A., Nordlund, P., & Eklundh, J. O. (1996). A computational model of depth-based attention. Pattern 
Recognition, 1996, Proceedings of the 13th International Conference on, (vol. 4, pp. 734–739). IEEE.
Maki, A., Nordlund, P., & Eklundh, J. O. (2000). Attentional scene segmentation: Integrating depth 
and motion. Computer Vision and Image Understanding, 78(3), 351–373. doi:10.1006/cviu.2000.0840
Marshall, S. P. (2000). Method and apparatus for eye tracking and monitoring pupil dilation to evaluate 
cognitive activity. Google US Patents.
Mather, G. (2009). Foundations of sensation and perception (Vol. 10). Psychology Press.
Okada, Y., Ukai, K., Wolffsohn, J. S., Gilmartin, B., Iijima, A., & Bando, T. (2006). Target spatial fre-
quency determines the response to conflicting defocus-and convergence-driven accommodative stimuli. 
Vision Research, 46(4), 475–484. doi:10.1016/j.visres.2005.07.014 PMID:16198392

116
2D and 3D Visual Attention for Computer Vision
﻿
Oliva, A., & Torralba, A. (2001). Modeling the shape of the scene: A holistic representation of the spatial 
envelope. International Journal of Computer Vision, 42(3), 145–175. doi:10.1023/A:1011139631724
Osberger, W., & Maeder, A. J. (1998). Automatic identification of perceptually important regions in an 
image. In Pattern Recognition, 1998. Proceedings. Fourteenth International Conference on, (vol. 1, pp. 
701–704). IEEE.
Ouerhani, N., & Hugli, H. (2000). Computing visual attention from scene depth. In Pattern Recognition, 
2000. Proceedings. 15th International Conference on, (vol. 1, pp. 375–378). IEEE.
Parkhurst, D., Law, K., & Niebur, E. (2002). Modeling the role of salience in the allocation of overt vi-
sual attention. Vision Research, 42(1), 107–123. doi:10.1016/S0042-6989(01)00250-4 PMID:11804636
Perreira Da Silva, M. (2010). Modèle computationnel d’attention pour la vision adaptative. Université 
de La Rochelle.
Peterson, M. S., Kramer, A. F., & Irwin, D. E. (2004). Covert shifts of attention precede involuntary 
eye movements. Attention, Perception & Psychophysics, 66(3), 398–405. doi:10.3758/BF03194888 
PMID:15283065
Pinneli, S., & Chandler, D. M. (2008). A Bayesian approach to predicting the perceived interest of objects. 
In Image Processing, 2008. ICIP 2008. 15th IEEE International Conference on, (pp. 2584–2587). IEEE.
Poole, A., & Ball, L. J. (2006). Eye tracking in HCI and usability research. In Encyclopedia of Human 
Computer Interaction (Vol. 1, pp. 211–219). IGR. doi:10.4018/978-1-59140-562-7.ch034
Posner, M., & Petersen, S. (1990). The attention system of the human brain. Annual Review of Neurosci-
ence, 13(1), 25–42. doi:10.1146/annurev.ne.13.030190.000325 PMID:2183676
Potapova, E., Zillich, M., & Vincze, M. (2011). Learning What Matters: Combining Probabilistic Models 
of 2D and 3D Saliency Cues. In Computer Vision Systems, (pp. 132–142). Springer.
Privitera, C. M., & Stark, L. W. (2000). Algorithms for defining visual regions-of-interest: Comparison 
with eye fixations. Pattern Analysis and Machine Intelligence. IEEE Transactions on, 22(9), 970–982.
Ramasamy, C., House, D. H., Duchowski, A. T., & Daugherty, B. (2009). Using eye tracking to analyze 
stereoscopic filmmaking. In SIGGRAPH’09: Posters (p. 28). ACM.
Rayner, K., Pollatsek, A., Ashby, J., & Clifton, C. Jr. (2012). Psychology of Reading. Psychology Press.
Salvucci, D. D. (1999). Mapping eye movements to cognitive processes. Carnegie Mellon University.
Salvucci, D. D., & Anderson, J. R. (1998). Tracing eye movement protocols with cognitive process 
models. Lawrence Erlbaum Associates, Inc.
Salvucci, D. D., & Goldberg, J. H. (2000). Identifying fixations and saccades in eye-tracking protocols. 
In Proceedings of the 2000 symposium on Eye tracking research & applications, (pp. 71–78). ACM. 
doi:10.1145/355017.355028
Tatler, B. W. (2007). The central fixation bias in scene viewing: Selecting an optimal viewing position 
independently of motor biases and image feature distributions. Journal of Vision, 7(14).

117
2D and 3D Visual Attention for Computer Vision
﻿
Torralba, A. (2003). Modeling global scene factors in attention. JOSA A, 20(7), 1407–1418. doi:10.1364/
JOSAA.20.001407 PMID:12868645
Treisman, A. M., & Gelade, G. (1980). A feature-integration theory of attention. Cognitive Psychology, 
12(1), 97–136. doi:10.1016/0010-0285(80)90005-5 PMID:7351125
Tseng, P.-H., Carmi, R., Cameron, I. G. M., Munoz, D. P., & Itti, L. (2009). Quantifying center bias of 
observers in free viewing of dynamic natural scenes. Journal of Vision, 9(7).
Tsotsos, J. K., Culhane, S. M., Kei Wai, W. Y., Lai, Y., Davis, N., & Nuflo, F. (1995). Modeling visual 
attention via selective tuning. Artificial Intelligence, 78(1), 507–545. doi:10.1016/0004-3702(95)00025-9
Vijayakumar, S., Conradt, J., Shibata, T., & Schaal, S. (2001). Overt visual attention for a humanoid 
robot. In Intelligent Robots and Systems, 2001. Proceedings. 2001 IEEE/RSJ International Conference 
on, (vol. 4, pp. 2332–2337). IEEE.
Vu, K., Hua, K. A., & Tavanapong, W. (2003). Image retrieval based on regions of interest. Knowledge 
and Data Engineering. IEEE Transactions on, 15(4), 1045–1049.
Wandell, B. A. (1995). Foundations of vision. Sinauer Associates.
Wang, D., Li, G., Jia, W., & Luo, X. (2011). Saliency-driven scaling optimization for image retargeting. 
The Visual Computer, 27(9), 853–860. doi:10.1007/s00371-011-0559-x
Wang, J., Chandler, D. M., & Le Callet, P. (2010). Quantifying the relationship between visual salience 
and visual importance. In IS&T-SPIE Electronic Imaging (p. 75270K). SPIE.
Wang, J., Da Silva, M. P., Le Callet, P., & Ricordel, V. (2013). A Computational Model of Stereoscopic 
3D Visual Saliency. Image Processing. IEEE Transactions on, 22(6), 2151–2165.
Wang, J., Le Callet, P., Ricordel, V., & Tourancheau, S. (2011). Quantifying depth bias in free viewing 
of still stereoscopic synthetic stimuli. In Proceedings of16th European Conference on Eye Movements.
Wedel, M., & Pieters, R. (2007). A review of eye-tracking research in marketing. Review of Marketing 
Research, 4, 123–147. doi:10.1108/S1548-6435(2008)0000004009
Widdel, H. (1984). Operational problems in analysing eye movements. Advances in Psychology, 22, 
21–29. doi:10.1016/S0166-4115(08)61814-2
Wismeijer, D., Erkelens, C., van Ee, R., & Wexler, M. (2010). Depth cue combination in spontaneous 
eye movements. Journal of Vision (Charlottesville, Va.), 10(6), 25. doi:10.1167/10.6.25 PMID:20884574
Wolfe, J. M. (2000). Visual attention. Seeing, 2, 335–386. doi:10.1016/B978-012443760-9/50010-6
Wolfe, J. M., & Horowitz, T. S. (2004). What attributes guide the deployment of visual attention and how 
do they do it?. Nature Reviews. Neuroscience, 5(6), 495–501. doi:10.1038/nrn1411 PMID:15152199
Yarbus, A. L. (1967). Eye movements and vision. New York: Plenum. doi:10.1007/978-1-4899-5379-7
Zhang, L., Tong, M. H., Marks, T. K., Shan, H., & Cottrell, G. W. (2008). SUN: A Bayesian framework 
for saliency using natural statistics. Journal of Vision (Charlottesville, Va.), 8(7), 32. doi:10.1167/8.7.32 
PMID:19146264

118
2D and 3D Visual Attention for Computer Vision
﻿
Zhang, Y., Jiang, G., Yu, M., & Chen, K. (2010). Stereoscopic Visual Attention Model for 3D Video. 
In Advances in Multimedia Modeling, (pp. 314–324). Springer Berlin.
Zhao, Q., & Koch, C. (2011). Learning a saliency map using fixated locations in natural scenes. Journal 
of Vision (Charlottesville, Va.), 11(3), 9. doi:10.1167/11.3.9 PMID:21393388
This work was previously published in Innovative Research in Attention Modeling and Computer Vision Applications edited by 
Rajarshi Pal, pages 1-44, copyright year 2016 by Information Science Reference (an imprint of IGI Global).

119
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  6
DOI: 10.4018/978-1-5225-1677-4.ch006
ABSTRACT
This article continues a research on the feasibility of BIM for 3D cadastre in unified building models, 
presented in El-Mekawy & Östman (2012). It describes problems and solutions concerning interaction 
between BIM and the registration and visualisation of legal 3D property information. BIM and legal 
3D property are two seemingly different domains, and there is a lack of BIM-3D property research in 
relation to technical and registration issues. The article therefore focuses on possibilities and difficul-
ties of addressing legal interests (i.e. rights, restrictions and responsibilities) in land. A Swedish case 
study is used to describe the use of 3D property formation and registration in Sweden, and how it might 
be possible to achieve a more integrated, standard based registration of legal boundaries and physical 
buildings. The results emphasize how BIM and 3D property domains can interact to serve the needs for 
effective information handling by e.g. importing 3D cadastral boundaries into BIM as basis for decision-
making or to use BIM as input in the 3D cadastral formation process.
1. INTRODUCTION
The increased use of computer aided design (CAD) and building information modelling (BIM) has 
had a huge impact on the building industry in the recent decades and BIM has become a standard ap-
proach in large scale building projects worldwide (IAI, 2008-2014). To support this increased use of 
digital information in the building process different standards have been developed, such as CityGML 
and IFC. The building process is related to land use, which is increasing, especially in urban areas. The 
Integration of Legal Aspects 
in 3D Cadastral Systems
Mohamed Sobaih Aly El-Mekawy
Stockholm University, Sweden
Jesper M. Paasch
Lantmäteriet, Sweden
Jenny Paulsson
KTH Royal Institute of Technology, Sweden

120
Integration of Legal Aspects in 3D Cadastral Systems
﻿
extended use of land, water and air has resulted in complex legal constellations of e.g. ownership and 
other rights. A tool to describe these rights, restrictions and responsibilities in a standardized framework 
is the international standard for land administration, the Land Administration Domain Model, LADM.
However, there has not been enough focus on how to implement the benefits of the increased modelling 
approach on the cadastral domain, which has very close relations to the building and construction domain 
by determining the legal boundaries and use of constructions. This issue was addressed by researchers 
in the last decades (e.g. Osskó, 2001; Sandberg, 2001) and still appears in recent studies (e.g. Ledoux & 
Meijers, 2011). Moreover, the use of models in the real property formation process and registration of 
real property information is not yet always a fully digital production line since analogue elements such 
as “printed maps and drawings” still may occur in the property formation process.
Although, the development of both domains, the construction modelling and the cadastral domain, has 
had unmatched speed, it is not seen only negative. Instead, it has caused a clear movement in research. 
A clear focus of research on cadastral issues and the need for the 3D cadastre was a driver for the first 
workshop of 3D cadastre in 2001. Three main topics were firstly defined as themes for 3D cadastre, 
namely ‘legal’, ‘institutional’ and ‘technical’ aspects. Among these three themes, the legal aspects were 
defined as the main key players of the design and requirements for developing real 3D cadastre informa-
tion systems and applications. In addition to that, legal aspects received the highest initial support from 
the research community (Fendel, 2002). However, the focus of studies and projects in the last decade 
has been in contrast to what was aimed. The work has progressed more on developing prototypes of 3D 
cadastre system and data models resulting in the ISO standard 2012, whereas the non-technical areas of 
research (i.e. mostly the legal and organizational issues) have been receiving little attention (Paulsson & 
Paasch, 2011; Ho, Rajabifard, Stoter, & Kalantari, 2013). According to Paulsson & Paasch (2013) there 
is a need for more work on the legal aspects of 3D cadastre. Following their results on recent research, 
even though the focus on technical applications and solutions is apparent, there are still several legal 
aspects not solved or approached by these applications.
1.1. Purpose of the Study
The study illustrates and discusses proposals for how to represent real and complex 3D properties by 
the four surface types “Building Elements Surfaces”, “Digging Surfaces”, “Protecting Area Surfaces” 
and “Real Estate Boundary Surfaces” (El-Mekawy & Östman, 2012).
The aim is not to create a combined 3D property and BIM model or to implement the findings in 
existing or experimental 3D cadastral systems. The aim is to research how these domains can interact to 
serve the needs for effective information handling by e.g. importing 3D cadastral boundaries into BIM 
models as a basis for decision-making or to use BIM building models as input in the 3D cadastral forma-
tion process. Some of the areas to be researched are how to define and describe boundaries, investigate 
to which extent standards can be applied, describe 3D cadastral properties, etc.
An example of Swedish three-dimensional (3D) property formation is used as a case study to illustrate 
the current method of 3D property registration in Sweden, which is then used as basis for a discussion 
of how to apply CityGML and IFC standards to achieve a more integrated, standard-based registration 
of legal boundaries and physical buildings than possible today, at least in Sweden.
It is the intention that the research presented here can act as a theoretical framework and input to 
how to further an interaction of 3D legal real property concepts with the CityGML, IFC and LADM 
standards in the development of 3D cadastral systems.

121
Integration of Legal Aspects in 3D Cadastral Systems
﻿
1.2. Method and Structure
The article compares the content of CityGML, IFC and LADM standards in regard to how they are 
capable of handling 3D real property information, such as boundaries.
The article starts with an introduction to the concept of 3D cadastre followed by a description of the 
3D capabilities of the CityGML, IFC and LADM standards in section 2. A case study in section 3 illus-
trates the use of 3D property formation and registration in Sweden. This is followed by an introduction 
to legal aspects on boundaries, rights/restrictions/responsibilities, areas and the content of construction 
environments in section 4. In section 5 the concept of “Building Elements Surfaces”, “Digging Surfaces”, 
“Protecting Area Surfaces” and “Real Estate Boundary Surfaces” is introduced. Section 6, finally, con-
tains a discussion on the concepts described in the previous sections, and conclusions.
2. 3D CADASTRE AND EXISTING STANDARDS
2.1. 3D Cadastre
Today, when the density of cities is high, there is a need to use and subdivide space into separate properties 
with different use. This divided use of space can include multi-use on the ground level, multi-use above 
surface, multi-use sub-surface and multi-use in time (van der Molen, 2003, p. 389). In order to provide 
secure and lasting rights for the divided use of such space, 3D property rights have become a common 
feature (see e.g. Paulsson, 2007) in order to combine different types of activities within the same space.
The use of land, water and air is regulated in most parts of the world. 3D property formation is a 
method to increase the use of 3D property as the result of the increased competition for space, especially 
in the cities, with increasing population and more advanced space-demanding activities that will have 
to share space within the same traditional two-dimensional (2D) property unit, i.e. parcel boundaries. 
Complex situations where there is a need to separate the ownership within an existing parcel and its 
space can be found (Stoter & Ploeger, 2002, p. I.2).
Internationally, there are different types of 3D property rights, depending on the legal system in a 
specific country. The independent 3D property type refers to a volume of space that is subdivided and 
separated from the rest of the property. Often it is a larger unit, which may include several apartments 
or offices, or be used for facilities and infrastructure objects, such as tunnels. Apartment (condominium) 
ownership is another form of 3D property that exists in many countries around the world (see e.g. Pauls-
son, 2007; van der Merwe, 1994).
Cadastre is a type of land information system that records land parcels and may include a register of 
ownership, register of land value or other interests in land use. A cadastre is not limited to one, single 
use but can consist of information covering a multitude of interests in land. It is originally defined as a 
register of real estates in a country. It may be defined as a parcel based and up-to-date land information 
system that includes important records on land such as rights, ownerships and restrictions (FIG, 1995). 
In such a context, cadastre refers to public inventory of data concerning properties that are methodically 
arranged within a certain country or district, based on a survey of their boundaries. Such properties 
are systematically identified by means of outlines of the property and a parcel identifier. The outlines 
of the property, identifiers and the additional descriptive data may show information on each property 

122
Integration of Legal Aspects in 3D Cadastral Systems
﻿
such as the nature, size, value and legal rights or restrictions associated with the land object associated 
with the parcel.
In the same way as for other traditional properties, 3D properties of course have to be registered and 
mapped (van der Molen, 2003, p. 388). The 3D property unit is recorded in the real property register, 
and its boundaries are usually defined by x, y and z co-ordinates or other types of indication of its extent 
in the vertical dimension (Eriksson, 2005, p. 7).
2.2. CityGML and 3D Cadastre
CityGML was originally initiated as a common semantic information model representing different 
3D urban and geographical objects that can be shared among different applications (Gröger, Kolbe, & 
Czerwinski, 2007). It has been developed as an open data model with XML-based format that can be 
used for storing and exchanging virtual 3D objects and city models among applications. Serving this 
purpose, it was divided into different thematic models representing a city like building, city furniture, 
land use, relief (i.e. digital terrain model), transportation, vegetation, water body, etc. In CityGML, there 
are five consecutive levels of detail (LoDs) that represent objects in city models (LoD0 to LoD4). An 
object becomes more detailed with increasing the LoD regarding its geometry and thematic (semantic) 
representation. Differentiation between LoDs is mainly referred to the process of data collection, its 
techniques and the purpose of applications. For 3D cadastre (i.e. the focus in this article), the building 
model in CityGML (Figure 1) is our target in studying CityGML. This is related to the appearance of 
cadastral aspects from LoD1 to LoD4. The CityGML Building Model (Figure 1) has been produced in 
the CityGML standards based on the ISO 19107 (ISO 19107, 2011) and ISO 19109 (ISO 19109, 2011) 
(Kolbe, 2009). The building model (Figure 1) is an excerpted version from the CityGML standard in 
which BuildingFurniture and IntBuildingInstallation are not represented.
Figure 1. The CityGML building model (Kolbe, 2009)

123
Integration of Legal Aspects in 3D Cadastral Systems
﻿
The development of CityGML has greatly contributed to the development of city models. This is 
mainly attributed to its semantic capabilities. Since its establishment as a standard with its first release 
in 2007, CityGML has witnessed a lot of changes in its semantic capabilities and building model. How-
ever, the latest and richest version (2.0.0) was released in 2012. Different classes and objects have been 
developed making CityGML capable of building 3D city models at different LoDs, especially at LoD4 
with details on complex surfaces and objects.
Following this discussion, the focus of examining CityGML capabilities is on, and is limited to, the 
level of details of LoD4. This can be clearly referred to as the appearance of building details and cadastral 
information on LoD4. At LoD4, the class _AbstractBuilding is responsible for representing a building 
and, therefore, it carries all information that is needed for the construction of spatial components of a 
building. Further representation of objects in a city model is carried out by the class CityObjectMember 
which can be any object in the city like a building, road, landscape, etc. However, the detailed cadastral 
information in LoD4 is represented in the attributes in Figure 2. For construction, a 3D model for each 
object from these attributes’ geometry is defined as a spatial composition of different surfaces of the 
object. These surfaces are represented by the tag bldg:Building. The information on vertices of each 
surface is stored individually in the tag gml:pos or collectively as coordinates of each corner in the tag 
gml:posList.
Considering the attributes in Figure 2, it can be concluded that the building class _AbstractBuilding 
does not provide sufficient information for 3D cadastre. It can be argued that it has the capabilities to 
construct a building or spatial objects. However, the legal aspects including rights, responsibilities and 
restrictions cannot be represented or stored.
As an open standard, CityGML can be extended by developers to design/add their own classes and 
objects, and use them in different applications. In such a way, the CityGML data model will be extended 
to link the additional classes and objects to the original data model of CityGML at a specific module 
(e.g. the building model in Figure 1). In general, there are two methods for extending the CityGML data 
model and, therefore, the two methods are discussed below in highlight of 3D cadastre for how to model 
the needed legal and cadastral information of 3D properties.
•	
The first method is to use generic city objects and attributes on the existing schema. New at-
tributes in accordance with their names and data types can be attached to any city object 
Figure 2. Attributes in the _AbstractBuilding Class (El-Mekawy & Östman, 2012, p. 44)

124
Integration of Legal Aspects in 3D Cadastral Systems
﻿
(CityObjectMember) during the application runtime. In the cadastre case, this can be done by 
adding legal rights and cadastral information to be called and used by the CityGML data model 
on the 3D properties. Although this approach is relatively easy and straightforward in adding func-
tionalities, it has clear limitations. The limited number of predefined data types (i.e. string, integer, 
double, date and URI) for the generic attributes need to be adjusted in different applications. For 
example, for cadastral purpose, other types should be added such as BuildingType (commercial 
building, private building, residential building, etc.) and OwnershipType (partnership, private, 
others). Another limitation is seen as the capabilities of XML parsers. It cannot automatically 
validate the layout and occurrences of generic objects and attributes in some cases (e.g. when two 
generic objects have the same variables). A third limitation related to the second one might occur 
in naming conflicts of generic attributes or objects.
•	
The second method for extending the CityGML data model is the use of application domain 
extensions (ADEs). In this method, an extra XML schema file should be created having its own 
namespace and file definition. In an ADE, new properties can be added to existing CityGML 
classes (e.g. parcel number) and new object types can be introduced (e.g. Cadastre Apartment). 
To activate and validate the ADE, the XML schema definition of the extended CityGML modules 
have to be explicitly imported into the ADE’s schema definition file. This method has different ad-
vantages over the use of generic city objects and attributes. One of the most important advantages 
here is that ADEs can be formally specified and standardized by communities that are interested 
in specific application domains, i.e. within the CityGML standard. The extended instance docu-
ments can also be validated against the CityGML schema and its ADE schema as well. In addition 
to that, more than one ADE can be used in the same dataset. Moreover, for increasing flexibility, 
ADEs may be defined for one or more CityGML modules. The important question here is ‘con-
sidering all these advantages over the use of generic city objects and attributes, do ADEs provide 
a complete and ready solution for representing 3D cadastre?’. The answer is simply ‘No’. It can be 
strongly argued that extending the CityGML schema for representing cadastre details results in a 
too large CityGML file size that limits the usability of different 3D city applications. In addition 
to that, CityGML was originally developed for the city modelling community which is different 
from a detailed application of cadastre having other goals and requirements. Therefore, extending 
CityGML to cover all the needs for cadastre is not feasible if both communities and target groups 
(including public and private stakeholders) did not yet come to a meeting point on their view of 
legal information and aspects.
2.3. IFC and 3D Cadastre
Industry Foundation Classes (IFC) was originally developed in 1996 by the International Alliance for 
Interoperability (IAI) and it is still under ongoing process of development by different stakeholders (IAI, 
2008-2014). According to the IAI, IFC is defined as an object oriented specification for exchanging, 
sharing and reusing information throughout the building industry’s life-cycle. The aim of IFC develop-
ment was to facilitate interoperability in the building industry and sharing information among different 
participants and stakeholders. It is therefore used to assemble computer readable models that contain 
data elements and represent parts of buildings with their relevant information. Currently, a wide area 
of the engineering domains including architecture, engineering and construction (AEC) and facilities 
management (FM) domains (such as architectural design, engineering, building analysis, control sys-

125
Integration of Legal Aspects in 3D Cadastral Systems
﻿
tems of HVAC (i.e. Heating, Ventilation and Air Conditioning), energy simulation, thermal analysis, 
maintenance and cost estimation) are covered by IFC compliance software vendors (IAI, 2008-2014; 
Lapierre & Cote, 2008).
It is believed today that IFC is the data model that has the widest scope for enabling interoperability 
within the AEC/FM industry (IAI, 2008-2014). Additionally, with its extensible representation, it is being 
rapidly growing for specific applications in construction, manufacturing and operation tasks within the 
AEC/FM industry. However, prior to 2011 there was no universally accepted building model for IFC. 
Instead, it has been uniquely developed by different actors (researchers or practitioners) different projects 
according to their interpretations of the IFC standard. (Kolbe, Nagel, & Stadler, 2008; Kiziltas, Leite, 
Akinci, & Lipman, 2010). Therefore, an IFC building model is presented in Figure 3, primarily based 
on the work done by the IAI and ISO (i.e. in form of IFC standard documentation (IAI, 2008), the ISO 
16739 standard (ISO 16739, 2011)), and the work of Benner, Geiger, & Leinemann (2005). Important 
concepts from those research efforts and standards were highlighted and relationships between them 
were built. Figure 3 shows the proposed IFC building model based on the UML standard notations.
The IFC, as the most prominent semantic model for representation of BIM (Isikdag & Zlatanova, 
2009), provides a data model for 3D BIM which enables all actors in a planning, construction and man-
agement process to exchange information in an integrated data pool. The model does not exchange the 
drawings of a building as in drawing exchange format (DXF), but deals with building components as 
they are designed or as exist in reality in their spatial relationships (i.e. objects oriented approach). BIM, 
generally, and IFC models, specifically, are very detailed models, keeping the semantics attached with 
geometries. The IFC building model generally brings more detailed information on building elements 
than what CityGML or any other building models have. This information is attached to the activities 
and environment inside the building and the surrounding area (e.g. materials, maintenance, temperature, 
etc.). In IFC, there are two main entities for defining and storing cadastral information, the IfcSite and 
IfcBuilding. The hierarchy of IFC model starts by a project (IfcProject), which is a container of under-
taken design, engineering, construction or maintenance activities. It is used for defining the root of the 
Figure 3. IFC Building Model (El-Mekawy, Östman, & Shahzad, 2011)

126
Integration of Legal Aspects in 3D Cadastral Systems
﻿
spatial structure for the contained spatial objects. Therefore, it is decomposed into spatial structures of 
type: IfcSite, IfcBuilding, IfcBuildingStorey and IfcSpace. Considering the representation of cadastral 
information, the following limitations of IFC have been observed (El-Mekawy & Östman, 2012):
•	
Although IFC has a very detailed semantic representation, the entities (especially the cadastre 
related ones, IfcProject, IfcSite, IfcBuilding, IfcBuildingStorey and IfcSpace) have no direct re-
lationships (link or pointer) in the schema. They refer to each other by objectified relationship 
entities that hold the spatial coordinates of different vertices of a building element. To make this 
clear, for example, we know that a building might have one or more storeys and, consequently, 
each storey has different space/s. In the IFC model, there is neither direct relationship between 
the IfcBuilding and the IfcBuildingStorey nor between the IfcBuildingStorey and the IfcSpace. 
However, the relationship entity IfcRelAggregates is responsible for aggregating all the horizontal 
connected spaces (in most cases) to form an IfcBuildingStorey object. The relationship objects 
have important information for defining how the building objects are spatially represented (see 
El-Mekawy, Östman, & Hijazi, 2012a). Considering this concept in IFC, extracted 3D cadas-
tre information in a multi ownership or a complex building from the entities IfcProject, IfcSite, 
IfcBuilding, IfcBuildingStorey and IfcSpace is a complex process. It requires robust understanding 
and management of the model.
•	
Unlike most of the building models today (such as CityGML), different level of details at different 
map scales are not explicitly defined in IFC. This makes an IFC model of a building or a project 
overloaded with different types of information, and more detailed than what is needed in simple 
projects such as boundaries of 3D cadastre. Extra information can be linked in separated files.
•	
In the latest version of IFC, IFC 2x4, external objects of a building or a parcel have been consid-
ered to be modelled. Such objects might be external building installations (e.g. emergency stairs, 
shaded area, etc.) or urban elements surrounding a parcel such as roads, paved zones, trees, etc. 
For such purpose, a new entity, IfcGeographicElement, was introduced. Although these spatial el-
ements add urban and cadastral modelling capabilities to the IFC standard, they are still not tested 
in real applications. An important question, as it is current today, will remain in testing the file size 
of IFC models in these applications.
•	
In general, IFC does not cover the information on parcels. Although it has rich semantics and 
detailed information on building elements, it has no objects that represent what is not built or con-
structed. The terrain model of topography is also not supported by IFC. By its wide definition, the 
terrain model does not only represent above- and underground-cadastre units, but it is important 
for representing the relationships between the building and the parcel (i.e. needed for excavation 
activities and calculations), and between the parcel and surrounding land (i.e. needed for related 
activities such as designing skylines of a city, and for utility networks).
•	
In addition to that, topologies in terms of neighbourhood relationships are not represented in IFC 
and they must be derived from the maintained containment relations of spaces. This is an impor-
tant issue for cadastre applications related to the land use and urban management. This makes 
the automation process of creating 3D cadastral objects from IFC data challenging as well as a 
problem that is worth investigating.

127
Integration of Legal Aspects in 3D Cadastral Systems
﻿
2.4. LADM and 3D Cadastre
The newly developed international standard for land administration, the Land Administration Domain 
Model, LADM, ISO 19152:2012 (ISO 19152, 2012) features the description of rights, responsibilities 
and restrictions affecting land (or water) and the geometrical (geospatial) components thereof. The 
standard uses the concept of “spatial unit”, which is defined as a single area (or multiple areas) of land 
and/or water, or a single volume (or multiple volumes) of space (ISO, 2012, 4.1.23). Spatial units can 
be specialized into building units (6.5.3) or utility networks (6.5.4). Note that the standard is an instru-
ment for describing the legal space covered by the right, restriction or responsibility, not the physical 
space. However, it is possible to relate the legal space to physical objects, such as buildings or part of 
buildings. A (non-mandatory) attribute describing a building unit is, among others, a reference to the 
identifier of the physical building unit (ISO 19152, 2012, 6.5.3). A (non-mandatory) attribute to the 
legal utility network can have a reference to the physical (technical) description of the utility network 
(ISO 19152, 2012, 6.5.4), see Figure 4.
Three-dimensional elements are described by using the topological concepts of “faces”, “boundary 
faces and “boundary face strings”. A face is a 2-dimensional topological primitive (ISO 19107, 2003, 
4.38). The geometric realization of a face is a surface. Boundary face is used in the three-dimensional 
representation of a boundary of a spatial unit. The volumes represent the legal space located - in whole 
or in part - in e.g. a building, not the physical space (ISO 19152, 2012, 4.1.4). Boundary face strings are 
used to represent the boundaries of spatial units by means of line strings in 2D. In a 3D land administra-
tion system it represents a series of vertical boundary faces where an unbounded volume is assumed, 
surrounded by boundary faces which intersect the Earth’s surface, e.g. such as traditionally depicted in 
the cadastral map (ISO 19152, 2012, 4.1.5). The volume is closed in the “top” and “bottom” by boundary 
faces. See Figures 5-6, illustrating the concepts of boundary face and boundary face strings.
Figure 4. LADM spatial unit with (legal) building units and (legal) utility networks (ISO, 2012)

128
Integration of Legal Aspects in 3D Cadastral Systems
﻿
3. CASE STUDY
The New Karolinska Solna (NKS) hospital is the new university hospital currently under construction in 
Solna, just outside Stockholm City, replacing the existing Karolinska University Hospital (in Swedish: 
Karolinska Universitetssjukhuset). The project started in summer 2010 and will continue – as planned – 
until October 2017 as the first Swedish Public Private Partnership (PPP) project and the world’s largest 
PPP project in healthcare (New Karolinska Solna, 2012; NKS-bygg, 2012).
As the world’s first and largest Public private Partnership hospital (SKANSKA, 2015) different legal 
aspects have emerged from the first day of the NKS project. One of the first issues is the ownership 
of the BIM model which is the base of construction process and life-cycle management of the project. 
Figure 5. The concept of spatial units defined by boundary face strings (ISO 19152, 2012, annex B)
Figure 6. Representation of vertical boundaries in ISO 19152 using boundary face (horizontally) and 
boundary face strings (vertically) (ISO 19152, 2012, annex B)

129
Integration of Legal Aspects in 3D Cadastral Systems
﻿
The partners involved in the project might feel entitled to own the BIM model as a way for controlling 
the process. However, these issues should be changed after the year 2040 when the Stockholm County 
Council will be the main owner of the project. In addition to that, during the period until the year 2040, 
different actors provide property information such as the ‘Coor Service Management’ organization 
which is responsible for facility management. The ownership should then be solved unequally between 
stakeholders because of security of data, different roles, management controls, types of data and the 
learning process from one actor to another.
Another issue in the NKS project is related to the different types of buildings with their requirements 
and facilities. In total, there are seven different buildings in the NKS project. They are as follows (New 
Karolinska Solna, 2012):
•	
Technology Center: A coordination centre for technology and for providing energy to the entire 
hospital buildings. This is also where all shipments and deliveries are handled.
•	
Parking Multi-Storey Garage: For 818 parking spaces from which 75 are equipped with charg-
ing stations for electric cars.
•	
Research Building: Includes all the lab areas, training facilities, and lecture and conference halls.
•	
Radiation Building: For all types of radiations.
•	
Hospital Part 1: The first part of the hospital.
•	
Hospital Part 2: The second part of the hospital.
•	
Patient Hotel: Accommodation for patients who prefer or need to stay closer to the healthcare.
Of special interest from a 3D legal point of view is the new NKS research and conference center which, 
when built, will extend over a public road, which is located on another property. The road is located 
in the lower, right side of the map in Figure 7 above. Regardless of the purpose of this extension, it is 
needed to be represented, registered, and calculated in volumes and spaces. However, these functions 
cannot completely be fulfilled in the 2D-based cadastre systems.
The left part of the building, shown with a grey shadow in Figure 8, is registered as a separate, legal 
entity in the Swedish real property register. This allows a creative solution to hold the property together, 
even if it reaches across another property.
Figure 9 shows the vertical boundary cutting through the physical building. The overhang suspends 
over a public road (E4/E20).
Figure 10 shows the overhanging part as described in the legal survey document.
In 2D-based cadastral systems, the spatial relationships are only described in text documents which 
have no spatial representation in 3D, which is also the case in Sweden.
The Swedish real property register consists of two parts: the cadastral index map and the land register. 
3D property is not registered as volumes in a separate cadastre. The property´s physical 2D extension on 
the ground is registered in 2D in the digital cadastral index map, whereas 3D information is registered 
in the land register. The text in the land register specifies the type of 3D property, x and y coordinates. 
The vertical extension is registered by adding a brief description of the location in height, e.g. between 
level “CA” +31.2 meters and level “CA” +55 meters on the construction drawing, which is part of the 
legal documents as illustrated in Figure 11, showing the 3D registration of the NKS building in the land 
register.

130
Integration of Legal Aspects in 3D Cadastral Systems
﻿
Figure 7. NKS construction site and surroundings. Reference: New Karolinska Solna (2012)
Figure 8. The NKS research and conference building. The shadowed left side reaching over a public 
road. Side view (Lantmäteriet, 2014)

131
Integration of Legal Aspects in 3D Cadastral Systems
﻿
The presented case study shows that it is possible to use standards in order to achieve a more inte-
grated, standard-based registration of legal boundaries and physical buildings. Although it is not tested 
in practice here, the examples show that it is theoretically possible and could be tested in future projects 
like this. The legal aspects then have to be investigated and developed in order to provide solutions for 
combining 3D property with a BIM model. This is further discussed and developed in the section below.
Figure 9. Vertical property boundary passing through the NKS building. (Lantmäteriet, 2014)
Figure 10. The NKS overhang described in the legal property formation map (Lantmäteriet, 2014)

132
Integration of Legal Aspects in 3D Cadastral Systems
﻿
4. LEGAL ASPECTS
Different types of rights, restrictions and responsibilities (collectively referred to as “RRR”s in the ISO 
19152 standard) are attached to land, water and/or air, covering an area or filling a 3D volume of (legal) 
space. RRRs are subject to national land use legislation, but examples are the right to use a road on a 
property, the restriction not to erect buildings without permission and the obligation to keep your house 
in good condition. That space is delimited by boundaries. These boundaries are handled differently in 
GityGML and IFC. Furthermore, RRRs affecting the use of property and its surroundings have to be 
registered.
4.1. Boundaries
In traditional cadastral systems boundaries were defined as the dividing lines between physical spheres 
(Kaufmann & Steudler, 1998). We then consider the definition of boundaries as the dividing lines be-
tween physical as well as abstract spheres. These spheres may define the type of people’s behaviours, 
use, jurisdictions, virtual and abstract activities.
Consideration of Legal Boundaries: the legal boundaries in BIM, as discussed above about the IFC, 
are based on building elements which are considered as solid geometries (Figure 12a). In CityGML, 
representation of building elements is, however, based on a composition of different surfaces of a certain 
building element (Figure 12b and 12c). This method also does not fulfil a clear definition of legal bound-
aries of a property since questions like ‘which surface (the internal/external) should be considered?’, 
‘what is the difference in consideration as a boundary between the ceiling and the roof (i.e. ceiling refers 
to the closing level that covers a space from the top side, whereas the roof refers to the top covering of a 
building or the top storey which gives the external shape of a building from above)?’ cannot be generally 
answered or defined in a building model.
Figure 11. Example of textual 3D information (in Swedish) in the land register (3D-utrymme = 3D 
space), Source: Extract from the Swedish Real Property Register

133
Integration of Legal Aspects in 3D Cadastral Systems
﻿
In addition to that, the construction method of walls is another problematic area in 3D properties. 
Nagel, Stadler, & Kolbe (2009) discussed the problem of different representation of walls as a conceptual 
problem in constructing building information models from different data sources (e.g. 2D photogram-
metry, 3D registration as laser scanning, 3D CAD design). However, this problem clearly appears with 
other dimensions in this research for representing legal boundaries of a property in 3D cadastre. Figure 
13 (a and b) shows different compositions of walls and surfaces in IFC and CityGML respectively. 
Although they might appear the same after finishing the building, they greatly vary in calculations 
regarding construction cost, pricing, volume and spaces, and maintenance when considering separated 
houses versus apartments with joint building elements or when building row-houses. This problem is 
not seen as only technical for how to represent or to visualize 3D properties. However, the main prob-
lem is the lack of a unified system or method for modelling such cases. Each case is different from the 
other according to the registration conditions. This makes it extremely difficult to accept either IFC or 
CityGML for such purposes.
On top of the above discussed problems, Figure 14 shows a part of the boundaries’ problem that 
challenges the implementation of 3D cadastre systems. The consideration of legal boundaries might be 
different from one building to another, especially if we have condominium (apartment ownership). The 
boundary can be defined in the middle of a joint wall or to one of its surfaces. The differences from 
one case to another makes it difficult to unify a system or method to be applied, and hence, it makes it 
difficult for IFC, CityGML and LADM to represent cadastre boundaries as they represent a building 
in building elements. The same problem of course applies for the RRRs in the coming section (Section 
Figure 12. Wall as boundaries in IFC and CityGML (W = Wall, S = Surface)

134
Integration of Legal Aspects in 3D Cadastral Systems
﻿
4.2) in which rights, restrictions and responsibilities differ based on the boundaries with other functions 
related to, for example, pricing, maintenance cost, energy simulation, thermal analysis, etc.
Construction Changes: changes during the construction are common in all types of buildings, es-
pecially in complex buildings that have several floors with special utility networks, multi purposes or 
different owners. Changes can be classified in two types:
1. 	
External changes that are linked to the land as a property or the exterior façade/s of buildings. 
Hence, they are directly related to boundaries of parcels or buildings. These changes are usually 
handled by BIM applications as the building elements and BIM objects are automatically updated. 
The property registration, ownerships and legislations should also be updated, and re-requested 
when substantial changes are made.
2. 	
internal changes that appear in the construction drawings or utility networks’ design. THESE 
changes often appear after the property registration and do not require changes in the registration 
as a condominium property. However, these changes require changes in ownerships, rights and 
other legal aspects. Different examples of these types of changes can be as follows:
◦◦
The design of ducts for utility networks’ cables or pipes. This type of changes usually comes 
in later stages after the architecture design and may appear in the phases of construction 
design, HVAC (Heating, Ventilation and Air Conditioning), utility networks’ design or dif-
ferent phases of construction processes. The ducts in this research refer to the built-up ducts 
that penetrate a building from bottom to the top, part of buildings or horizontal ducts in the 
ceilings or roofs of different floors. The ducts are usually isolated, owned not by a specific 
property (i.e. joint ownership), and can be internal (between properties) or external (on the 
facades of a building). In addition to that, they vary in size from tens of cubic centimetres to 
several cubic meters according to its purposes. As explained above, it is then expected that 
both types of ducts, internal and external, require changes in aspects like boundaries, owner-
ships, rights, use, etc. (Figure 15).
◦◦
Repartitioning of a space. During the design phase, a multipurpose building might not be 
finalized in the distribution of its usage or partitioning of its open spaces. An example of this 
case might be represented in a university-hospital building such as NKS (i.e. when a hospital 
is used for research and academic activities) or a university-business building (i.e. when a 
compound building is owned by different organizations, companies and a university). In the 
Figure 13. Different connections of internal and external walls in IFC (a) and CityGML (b)

135
Integration of Legal Aspects in 3D Cadastral Systems
﻿
Figure 14. Different surfaces might define the boundaries differently. Based on El-Mekawy & Östman 
(2012, p. 47)
Figure 15. Examples of internal and external ducts in buildings

136
Integration of Legal Aspects in 3D Cadastral Systems
﻿
two examples, the use of reception, use of spaces such as seminar and meeting rooms and 
the partitioning of open space cannot be defined in a 2D map or in registration of property 
drawings. These changes do not appear in BIM or require their updates of building elements. 
However, they cause substantial changes in legal aspects of the cadastre registration.
4.2. Rights, Restrictions and Responsibilities, RRR
RRRs affect the whole parcel such as ownerships, uses, leases and mortgages. Considering the defini-
tions and delimitations of RRRs needed for a 3D cadastral system, the following types of RRRs may 
be observed:
1. 	
RRRs that are related to the parcel itself of a property. These rights do not require a 3D representa-
tion and they are included in the cadastral surveying in 2D maps. These rights may be attached in 
a legal document file to the parcel or the property and stored in the cadastre. This type of RRRs 
can easily be represented in BIM, CityGML or LADM.
2. 	
RRRs that are not related to the parcel itself, however, are affected by the type of property to be 
constructed on the parcel. These RRRs are normally not included in the cadastral survey and can 
only be represented in 3D cadastral information, spaces or volumes. An example is the condominium 
(apartment ownership) (Figure 16). It has different rights, responsibilities and restrictions on, for 
example, areas of spaces, heights, openings, and space usages that are poorly defined on 2D maps 
or documents. An example of 2D visualization of 3D on a cadastral index map can be seen in El-
Mekawy, Paasch, & Paulsson (2014). For a full image of these rights, they require a 3D cadastral 
system.
3. 	
RRRs that are legally not clearly defined or delimited. These rights concern any objects or activi-
ties in the parcel property with the 3rd dimension. An example of these rights is the maximum 
underground digging level when constructing on bedrock or the existence of a public utility net-
work underground. These types of RRRs can be seen in the hospital project (NKS) where some 
construction operations are attached to the main road (E4/E20), in addition to the construction of 
the multi-storey garage. In these cases, the underground operations were determined not as 3D 
cadastral legal aspects. Another example for this type of RRRs is the temporary TV dishes or 
antennas on balconies or the roof of buildings. This type of rights requires clear 3D definitions 
and space volumes of above and underground surfaces. Figure 17 shows an example of this type 
of RRRs where residential or office buildings are constructed on the top of a mountain or rocky 
land under which a car/train tunnel is constructed. Even though the RRRs for the parcel should 
be extended underground through the rock, they are changed because of two aspects. Firstly, they 
are affected by the rock itself as a volume of digging not to cause a rockslide. Secondly, they are 
affected by the tunnel so as not to cause danger on its surfaces or to be collapsed.
4. 	
RRRs that are based on easements and other agreements between real properties. These RRRs are 
legally not clearly defined or captured easily. The easement as a relationship between two proper-
ties can be defined as the right for a property to use another property for a specific use. The two 
properties might be adjacent to or separated from each other. An easement of a utility pipe to the 
public network is an example of this type of right. Another example is ownership of tunnels for cars, 
trains or subways under parcels or buildings expanding over legal boundaries in complex cities, 
such as the 3D solution in the NKS building reaching over a main public road as described in the 

137
Integration of Legal Aspects in 3D Cadastral Systems
﻿
case study in section 3. These rights require 3D cadastral systems as well as space volumes of the 
underground surfaces. Underground easements are different from the previous type of RRRs that 
legally is not clearly defined or delimited in the sense that in the previous type the digging level 
is defined and measured naturally, as there are no needs for artificial installations to support the 
earth or mountain from probable collapse. In the underground easement, a large part of the RRRs 
is taken by other parcels with the need in complex and crowded cities in which tunnels or under-
ground parking are constructed directly under buildings or roads. In the latter situation, artificial 
supports are needed to support the sand and earth under buildings or roads. 
4.3. Area
The legal areas should be attached in the official legal document of a cadastre. It is not always the same 
as the one calculated from boundaries or from the spatial cadastral boundary vertices. According to the 
determined rights, it can be increased or decreased. However, it should always define the area where 
certain activities are permitted or prohibited under certain conditions defined in the legal rights’ section. 
Figure 16. Cross section (right) of ownership apartments and their cartographic representation in the 
cadastral index map (left) (Lantmäteriet, 2004)
Figure 17. RRRs that limit the underground digging level. Based on El-Mekawy & Östman (2012, p. 43)

138
Integration of Legal Aspects in 3D Cadastral Systems
﻿
It is an important factor affecting the boundary concept and consequently the rights in the area sur-
rounded by the boundaries. It is important to understand that if the boundaries of a parcel are approved 
and determined, the calculated area and its changes are also to be accepted. 
In the NKS project, the legal area of the project is not the same as the calculated area boundaries 
or from the spatial cadastral boundary vertices of the different buildings. In Figure 7, the area that is 
defined for the project activities is huge and includes public properties or properties such as roads that 
are owned by other stakeholders.
4.4. Content
Following the determination and calculations of a parcel’s area, the content of this area may not be af-
fected only by its private and public rights, but also by the demarcation of where, when and how a right 
or a restriction starts, begins and the content of each right. In addition to that, the content should provide 
information on buildings, vegetables and mines and be controlled by the public-law and private-law 
property restrictions. In the case of the project NKS, the RRRs will be changed according to the change 
in RRRs before the year 2040 and after. These changes might appear in terms of different ownerships 
and different changes in the areas and in case of extensions of buildings or activities.
5. SURFACES
By analysing the legal aspects of 3D property (Boundaries, RRRs, Area and Content), it can be con-
cluded that the availability of information on ‘Area’ and ‘Content’ are dependent on the information and 
its accuracy on ‘Boundaries’ and ‘RRRs’. In addition to that, they can be provided in descriptive docu-
ments without a need for 3D representation. Therefore, the focus in this article is oriented more towards 
’Boundaries’ and ‘RRRs’. In addition to that, all problematic aspects in representing 3D properties are 
related to ‘Boundaries’ and ‘RRRs’, whereas ‘Area’ and ‘Content’ can follow the same representation 
or even be attached as text description files. However, as it is discussed in Section 4 the information on 
representation of ‘Boundaries’ and ‘RRRs’ cannot be extracted only from physical or building elements 
that have clear x, y and z coordinates from which they can be spatially represented and constructed. 
Instead, different virtual surfaces must be created for representing ‘Boundaries’ and ‘RRRs’ in 3D, 
especially for underground aspects. 
As shown in the NKS case study the registration of 3D legal boundaries can be quite rudimentary, 
e.g. stating that the boundary “follows the façade”, and the spatial extension is shown on an (analogue) 
construction plan. We think that such descriptions, even if being legally valid, do not provide sufficient 
information for 3D cadastral and BIM digital portfolio. However, following the insufficient capabilities 
of IFC, CityGML for representing real and complete 3D properties (discussed in details in Section 2), 
this section represents our proposal for fulfilling this purpose. The proposal is based on investigating the 
feasibility of the original proposal in El-Mekawy & Östman (2012) for four types of boundary surfaces. 
These four types originally were proposed as a part of extending the unified building model (UBM) that 
was developed in El-Mekawy, Östman, & Shahzad (2011) and El-Mekawy, Östman, & Hijazi (2012b) 
for unifying and converting between IFC and CityGML standards. However, in this article, we extend 
the discussion on these four boundary surfaces for modelling the legal aspects of cadastre, mainly 
‘Boundaries’ and ‘RRRs’, in 3D. These surfaces are proposed to be constructed as spatial volumes or 

139
Integration of Legal Aspects in 3D Cadastral Systems
﻿
spaces and to be saved in the 3D cadastre system whether it is based on CAD or geospatial information 
systems. Figure 18 shows these four surfaces and the relationships between them relating to a building 
and its surrounding land.
The aim of these types of surfaces is not to present or to form any kind of building elements. Therefore 
the interest is to only consider the outer/external surfaces of all external building elements to define the 
volume in which the building is allowed to be built or building elements to be constructed. 
1. 	
“Building Elements Surfaces”. This type of surface represents the outer lines of all surfaces and 
building elements. Building elements like stairs and walls have different internal and external 
surfaces, but a building element surface can be formed by combining the external surfaces of the 
external building elements. In most of the condominium (apartment ownership) ‘Boundaries’ and 
‘RRRs’ are defined by this type of surfaces where the ownership is limited by the external walls 
of the building or the apartment.
2. 	
“Digging Surfaces”. This type of surface is used for modelling the sub-terrain 3D legal usage (i.e. 
rights, restrictions and boundaries) of a property. It is important not only for constructions like 
tunnels and multi-storey car parking, but even for underground objects such as foundations and 
representation of different use and maintenance rights for utility networks. These virtual surfaces 
cannot be physically seen or recognized by most of the users, but they can define the contents, area 
and even the legal boundaries of underground parts of a property. 
3. 	
“Protecting Area Surfaces”. This type of surface has relationships to the “digging surfaces” but 
has another purpose. They define the area around a 3D property in which some legal aspects are 
applied. For example, restriction distance can be applied around underground buildings such as 
Figure 18. The four proposed boundary surfaces for 3D property representation. Based on El-Mekawy 
& Östman (2012, p. 47)

140
Integration of Legal Aspects in 3D Cadastral Systems
﻿
tunnels, to come closer to a water area (e.g. sea, river or channel), or around an underground utility 
network. In general, this type of surface determines the underground legal relationships and directly 
affects the ”Boundaries” and “RRRs”.
4. 	
“Real Estate Boundary Surfaces”. These surfaces define the real legal boundaries of a 3D property 
after applying the previous three types of surfaces. It is introduced for two main reasons;
a. 	
Firstly, in cadastre systems in 2D maps, a cadastre unit is defined by points on its borders with 
lines connecting these points to construct the final polygon of legal boundaries. However, 
in 3D, these boundaries are extended downwards into the earth and upwards into the sky. 
Defining the highest and lowest levels should be stored somehow to serve different applica-
tions such as the skyline development of a city, designing an airport or calculating densities 
of urban areas, etc. 
b. 	
Secondly, these surfaces have different relationships to other surfaces. They can be; identical 
to any one of the surfaces as follows; identical to the “Protecting Area Surfaces” in cases 
where constructions such as tunnels need restrictions in land use, or identical to the “Building 
Elements Surfaces” in cases where the external building elements also constitute the legal 
boundary such as in row-houses or compact cities. In other examples, the legal boundaries of 
the 3D real estate property should be created separately, combined or extracted from one or 
more of the other surfaces. An example of this case is the joint ownership in apartments when 
two apartments share the same building element such as a wall, a slab, a stair, an elevator, etc. 
It should then be extracted from different surfaces of the building elements or defined virtu-
ally in the centre of their walls. The ‘Boundaries’ and ‘RRRs’ are usually, in normal cases of 
properties, defined as this type of surfaces as they represent the volume and space that can 
be utilised by an owner.
It is important here to notice that Figure 18 represents the four types of surfaces as they are to be 
represented in the 3D cadastre system. They represent the volumes in which legal aspects are defined. 
They always represent the maximum dimensions (outer extents) as containers (i.e. they are not to be 
positioned inside, in the center, or outside the building components). This is different from what is 
presented in Figure 14 in which the legal aspects are firstly defined by the designers or shared as in 
condominium property.
6. DISCUSSION AND CONCLUSION
This article has focused on studying the 3D cadastre as an ongoing interest in many countries today 
represented in different research forums, seminars and conferences. In spite of the existence of a large 
number of research efforts, still no single complete/real 3D cadastre system is seen today. One of the 
reasons for this phenomenon is attributed to the focus in most of these studies on technical aspects, with 
less attention to the importance of legal aspects. This article has therefore discussed the legal aspects as 
a key point towards bridging the gap of the completeness of a 3D cadastre system.
A case study has been conducted on a part of the New Karolinska Hospital (NKS) in Sweden illustrat-
ing that the Swedish registration procedures of 3D property does not yet include the use of CityGML, 
IFC or other standards in the national real property register, which is limited to textual description of 
3D property.

141
Integration of Legal Aspects in 3D Cadastral Systems
﻿
The article provided an overview of problems concerning the 3D property information process in 
the case of a large construction project, such as NKS, using analogue data to a large extent in the real 
property formation registration process. The study showed possibilities for potential use of 3D cadastre 
for presenting the legal part of it in several stages of the construction process. 
The current most recognised standards that have 3D cadastre focus have been reviewed. They are 
the IFC, CityGML and LADM. They have great contributions to support in constructing 3D properties. 
However, the CityGML and IFC standards focus more on solving technical aspects on how to define 
and store cadastral boundaries in 3D instead of only 2D maps. These standards have added several steps 
towards a 3D cadastre which was not available in the last decades. We also agree that defining boundaries 
in 3D is overlapped between technical and legal aspects of cadastre. However, much more of the legal 
aspects that have been discussed in this article (i.e. Boundaries, RRRs, Area and Content) cannot be 
represented in these standards, except from the LADM which provides a legal framework for describing 
3D real property, but does not concern itself with how to connect the legal and physical boundaries and 
spaces. Therefore, the complete real 3D cadastre approach incorporating legal and physical boundaries 
and space remains a question and is not fully answered.
In this article, we have investigated and added to the proposed extension of a pre-developed unified 
building model (UBM) by El-Mekawy, Östman, & Shahzad (2011) and implemented in El-Mekawy, Öst-
man, & Hijazi (2012b). The extension was implemented from a pure technical perspective in El-Mekawy 
& Östman (2012) without any consideration of legal aspects. Therefore, the focus in this article is purely 
oriented to the legal aspects. It has been shown in this article that adding the four surfaces, namely “Build-
ing Elements Surfaces”, “Digging Surfaces”, “Protecting Area Surfaces” and “Real Estate Boundary 
Surfaces”, has great possible contribution to the representation of the legal aspect of 3D properties, e.g. 
in a large construction as NKS, but also related to all 3D property. The four surfaces make it possible 
to represent the virtual, above- and under-ground, surfaces that are needed for defining legal aspects.
The aim of this article has been to present a theoretical framework for how to further an interaction 
between 3D legal real property concepts with CityGML, IFC and LADM standards and the “surfaces” 
described above. However, future research should implement the findings presented here in existing or 
experimental 3D cadastral systems, thus furthering the development of 3D cadastral systems.
REFERENCES
Benner, J., Geiger, A., & Leinemann, K. (2005). Flexible Generation of Semantic 3D Building Models. 
In G. Gröger, & T. Kolbe (Eds.), 1st International Workshop on Next Generation 3D City Models (pp. 
17–22). Bonn, Germany, 21–22 June 2005.
El-Mekawy, M., & Östman, A. (2012). Feasibility of building information models for 3D cadastre in 
unified city models.[IJEPR]. International Journal of E-Planning Research, 1(4), 35–58. doi:10.4018/
ijepr.2012100103
El-Mekawy, M., Östman, A., & Hijazi, I. (2012a). An Evaluation of IFC-CityGML Unidirectional 
Conversion.[IJACSA]. International Journal of Advanced Computer Science and Applications, 3(5), 
159–171. doi:10.14569/IJACSA.2012.030525

142
Integration of Legal Aspects in 3D Cadastral Systems
﻿
El-Mekawy, M., Östman, A., & Hijazi, I. (2012b). A Unified Building Model for 3D Urban GIS. ISPRS-. 
International Journal of Geo-Information, 1(2), 120–145. doi:10.3390/ijgi1020120
El-Mekawy, M., Östman, A., & Shahzad, Kh. (2011). Towards Interoperating CityGML and IFC Building 
Models: A Unified Model-Based Approach. In Th. Kolbe, G. König & C. Nagel (Eds.), 5th 3D GeoInfo 
Conference. Lecture Notes in Geoinformation and Cartography Series, Springer Lecture Notes in Geoin-
formation and Cartography (LNG&C). Heidelberg: Springer-Verlag. doi:10.1007/978-3-642-12670-3_5
El-Mekawy, M., Paasch, J. M., & Paulsson, J. (2014). 3D Cadastre, 3D property formation and BIM in 
Sweden. In Oosterom (Ed.) 4th International FIG 3D Cadastre Workshop, 9-11 November 2014, Dubai, 
United Arab Emirates.
Eriksson, G. (2005). A New Multi-dimensional Information System Introduced in Sweden. Paper pre-
sented during FIG Working Week 2005 and GSDI-8 “From Pharaohs to Geoinformatics”, 16-21 April 
2005, Cairo, Egypt.
Fendel, E. M. (2002). Registration of Properties in Strata: report on the Working Sessions. Digital report, 
last accessed on 03 January 2015 from: http://www.gdmc.nl/3DCadastres/literature/3Dcad_2001_05.pdf
FIG (1995). The FIG Statement on the Cadastre. Technical Report Publication No. 11, International 
Federation of Surveyors, Commission 7.
Gröger, G., Kolbe, TH., & Czerwinski, A. (2007). Candidate OpenGIS CityGML Implementation Speci-
fication (City Geography Markup Language), OGC project document: OGC 07-062.
Ho, S., Rajabifard, A., Stoter, J., & Kalantari, M. (2013). Legal barriers to 3D cadastre implementation: 
What is the issue? Land Use Policy, 35, 379–387. doi:10.1016/j.landusepol.2013.06.010
IAI (2008-2014). BuildingSMART. http://www.iai-tech.org/ Retrieved October-2014.
Isikdag, S., & Zlatanova, S. (2009). Towards defining a framework for automatic generation of build-
ings in CityGML using BIM. In Lee and Zlatanova (eds.), 3D geo-information sciences, LNG & C. (pp. 
79-96). Heidelberg: Springer Verlag.
ISO 16739 (2011). ISO 16739:2011, Industry Foundation Classes, Release 2x, Platform Specification. 
Geneva: International Organization for Standardization.
ISO 19107 (2011). ISO 19107:2011, Geographic information – Spatial schema. Geneva: International 
Organization for Standardization.
ISO 19109 (2011). ISO 19109:2011, Geographic information – Rules for application schema. Geneva: 
International Organization for Standardization.
ISO 19152 (2012). ISO 19152:2012, Geographic Information – Land Administration Domain Model. 
Geneva: International Organization for Standardization.
Karki, S., McDougall, K., & Thompson, R. (2010). An overview of 3D Cadastre from a physical land 
parcel and a legal property object perspective.FIG In (Ed.), XXIV FIG International Congress 2010, 
April 2010, Sydney.

143
Integration of Legal Aspects in 3D Cadastral Systems
﻿
Kaufmann, J., & Steudler, D. (1998). Cadastre 2014 – A Vision for a Future Cadastral System. Switzer-
land: Rüdlingen & Bern, July 1998.
Kiziltas, S., Leite, F., Akinci, B., & Lipman, R. (2009). Interoperable Methodologies and Techniques 
in CAD, CAD and GIS Integration. Auerbach Publications.
Kolbe, T. (2009). Representing and Exchanging 3D City Models with CityGML. In J. Lee & S. Zlatanova 
(Eds.), 3D Geo-Information Sciences (pp. 15–31). Berlin, Germany: Springer-Verlag.
Kolbe, T., & Gröger, G. (2003). Towards unified 3D city models. In J. Schiewe, M. Hahn, M. Madden 
& M. Sester (Eds.) Challenges in Geospatial Analysis, Integration and Visualization II.Joint ISPRS 
Workshop, Stuttgart.
Kolbe, T., & Gröger, G. (2004). Unified Representation of 3D City Models. Geoinformation Science 
Journal, 4(1).
Kolbe, T., Nagel, C., & Stadler, A. (2008). CityGML – A Framework for the Representation of 3D City 
Models from Geometry Acquisition to full Semantic Qualification. In Proceedings of ISPRS Congress 
2008 in Bejing, China.
Lantmäteriet (2004). Handbok registerkarta. (In Swedish). [Digital Cadastral Index Map]. Lantmäteriet, 
the Swedish mapping, cadastral and land registration authority. Report no. LMV-Rapport 2004:6. With 
later amendments. Version 2014-06-04.
Lantmäteriet (2014). Property Formation Documents. (In Swedish). Achieve document no. 0184-1022. 
Lantmäteriet, the Swedish mapping, cadastral and land registration authority.
Lapierre, A., & Cote, P. (2008). Using Open Web Services for urban data management: a testbed resulting 
from an OGC initiative offering standard CAD/GIS/BIM services. In Coors, Rumor, Fendel & Zlatanova 
(eds.), Urban and Regional Data Management (pp. 381-393). London: Taylor & Francis, Proceedings 
of the 26th UDMS, October 10-12, Stuttgart.
Ledoux, H., & Meijers, M. (2011). Topologically consistent 3D city models obtained by extrusion. Inter-
national Journal of Geographical Information Science, 25(4), 557–574. doi:10.1080/13658811003623277
Nagel, C., Stadler, A., & Kolbe, T. (2009). Conceptual Requirements for the Automatic Reconstruc-
tion of Building Information Models from Uninterpreted 3D Models. Academic Track of Geoweb 2009 
Conference, Vancouver.
New Karolinska Solna (2012). Last accessed on 2015-02-09 from: http://www.nyakarolinskasolna.se/en/
NKS-bygg (2012). ”Fakta om projektavtalet för Nya Karolinska Solna”, in English ”Facts about the 
project contracting for New Karolinska Solna”, last accessed on 2015-02-09 from: http://www.nyakaro-
linskasolna.se/PageFiles/556/Faktablad_om_projektavtalet_NKS_feb2012%5b1%5d.pdf
Osskó, A. (2001) Problems in registration in the third vertical dimension in the unified Land Registry 
in Hungary, and possible solution.Proceedings of the International Workshop on 3D Cadastres, 2001, 
Delft, pp. 305-314

144
Integration of Legal Aspects in 3D Cadastral Systems
﻿
Paulsson, J. (2007). 3D Property Rights – An Analysis of Key Factors Based on International Experience. 
Doctoral Thesis, Report 4:99. Section of Real Estate Planning and Land Law, KTH Royal Institute of 
Technology, Stockholm, Sweden.
Paulsson, J., & Paasch, J. (2013). 3D property research from a legal perspective. Computers, Environ-
ment and Urban Systems, 40, 7–13. doi:10.1016/j.compenvurbsys.2012.11.004
Sandberg, H. (2001). Three-dimensional division and registration of title to land: Legal aspects. Proceed-
ings of the International Workshop on 3D Cadastres, 2001, Delft, pp. 201-209.
SKANSKA. (2015). New Karolinska Solna University Hospital. Last accessed on 2015-02-09 from:http://
www.group.skanska.com/en/campaigns/125/new-karolinska-solna-university-hospital/
Steudler, D. (2014). CADASTRE 2014 and Beyond. International Federation of Surveyors (FIG), 2014. 
Helsinki, Finland: Hakapaino.
Stoter, J. (2004). 3D Cadastre. PhD Thesis, NCG Nederlandse Commissie voor Geodesie (Netherlands 
Geodetic Commission), Publications on Geodesy 57. Delft: Delft University of Technology, the Neth-
erlands.
Stoter, J., & Ploeger, H. D. (2002). Multiple Use of Space: Current Practice of Registration and Develop-
ment of a 3D Cadastre.Proceedings of UDMS 2002, October 2002, Prague, Czech Republic.
UN-GGIM (2013). Future trends in geospatial information management: the five to ten year vision. 
United Nations Committee of Experts on Global Geospatial Information Management, July 2013.
van der Merwe, C. G. (1994). Apartment ownership. In Yiannopoulos, A. N. (Ed.), International ency-
clopedia of comparative law. Vol. 6, Chapter 5, Property and trust. Tübingen: Mohr.
van der Molen, P. (2003). Institutional aspects of 3D cadastres. Computers, Environment and Urban 
Systems, 27(4), 383–394. doi:10.1016/S0198-9715(02)00038-8
This work was previously published in the International Journal of E-Planning Research (IJEPR), 4(3); edited by Carlos Nunes 
Silva, pages 47-71, copyright year 2015 by IGI Publishing (an imprint of IGI Global).

145
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  7
DOI: 10.4018/978-1-5225-1677-4.ch007
ABSTRACT
The additive manufacturing (or the popular 3D printing) is relatively new technology which opens new 
spaces for entrepreneurial imagination and promises next stage of the industrial revolution. It is creating 
three dimensional solid objects from a digital file. The printer transforms the file into a material object 
layer by layer, using different raw materials. Today, the additive manufacturing is successfully used in 
architecture, medicine and healthcare, light and heavy industries, education, etc. The paper analyses the 
roles of actors in manufacturing the objects. It starts with the Heideggerian questioning of technology 
(Heidegger 1977), searching for the causes of bringing into appearance of the 3D model. According to 
Heideggerian analysis the technology is represented as an ‘unveiling of the truth’. The paper suggests 
that the old understanding of matter as a thing-in-itself should be replaced by a new, flexible, fluid, 
concept of matter, which is more or less manipulable. The matter is no more an occasion for object’s 
taking place. On the other hand, it seems 3D printing technology is reduced to mere means; a simple 
intermediary, a copier of ideas. From that perspective the paper questioning the problem of action in 
ANT and search how action and interaction is distributed and how actors constitutes themselves as well 
as their actor-world.
1. INTRODUCTION: WHAT IS ADDITIVE MANUFACTURING?
The precursors of the additive technology could be found in the XIX-the century, when the researchers 
have been searching for different ways of three-dimensional copying of solid objects. Yet, the additive 
manufacturing (or as it is more popular three-dimensional printing), as a substantive technology, is 
relatively new technology that has been developed radically with the promoting of the computer-aided 
design (CAD), computer numerical control (CNC) machining, and lasers. It really opens new spaces for 
Where Is the Missing Matter?
A Comment on “The Essence” 
of Additive Manufacturing
Tihomir Mitev
Plovdiv University “Paisii Hilendarski”, Bulgaria

146
Where Is the Missing Matter?
﻿
entrepreneurial imagination and promises a next stage of the industrial revolution. (Koleva, A. 2015) 
This is a technology known to the world for over 30 years. What is it; how does it work?
The 3D printing is based on the development of digital technologies.
Additive manufacturing (AM), also referred to as solid freeform fabrication (SFF) or three dimensional 
(3D) printing, is a set of layer-by-layer processes for producing 3D objects directly from a digital model. 
(Weber, C. L., Peña, V., and al. 2013: 1)
The additive technology is an encounter and confluence of numerous of fields like photonics, com-
puter science and modelling, materials science, control theory and computer numerically controlled 
machining, and machine design. It is a creating of three dimensional solid objects from a digital file, 
and the printer transforms the file into a material object, using different raw materials. A range of dif-
ferent fibres of metals, plastics and composite materials may be used. Among the most commonly used 
materials in three-dimensional printing are acrylonitile butadiene styrene (ABS), polylactic acid (PLA), 
steel, titanium, gold, silver, as well as nylon, nylon, glass-filled polyamide, epoxy resins, wax, and 
photopolymers; еven bio-material. Instead of carving or milling of detail from an existing solid object, 
the additive manufacturing fabricates the product, using materials that have been transformed in fine 
powder. Since there is no removing of material and the object is built “bottom-up”, that technology is 
acknowledged as a completely opposite to the classical ones. 
The stages of production process in 3D printing are performed in three steps - designing (usually 
with CAD software), printing (structuring of the material layers) and finishing (polishing and colouring). 
They are mainly three specific methods/techniques of manufacturing: Selective laser sintering (SLS); 
Fused deposition modeling (FDM); Stereolithography (SLA).
The additive manufacturing is considered to be one of the fastest growing markets today. Since its 
active development in 1980-s till 2013 it has grown to above $ 3 billion (Wohlers 2013). It is applied 
both to the designing of prototypes and to the regular production of small product series. It was initially 
applied for industrial purposes, then gradually it was being commercialized, and today some promotes 
the role of 3D printing in everyday life. There are lots of advantages of that technology: local production, 
reduced consumption of resources, rapid prototyping, development and adjustment of patterns, rapid 
production of tools and spare parts, freedom of design and personalized production. Today, the additive 
manufacturing is successfully used in architecture, medicine and healthcare, light and heavy industries, 
education, culture industries, etc.
It seems that it is a revolutionary technology with vast scope and great promises. Is there something 
brand new with that technology? How the production process has been challenged? How the interaction 
between men and nature in the fabricating of artefacts take place? These issues will be discussed below 
from the perspective of philosophy of Martin Heidegger.
2. ТHE ADDITIVE MANUFACTURING: THE CHALLENGING OF THE “STOCK”?
Reflections on the nature and the role of technology are old enough. They start with two initial assump-
tions that pretend to outline exhaustively the essence of technology. As Heidegger resumes, according 
to the “old” philosophical doctrine, technology is: 1) means for the achieving of goals and 2) a certain 
kind of human activity. These two viewpoints could be defined as instrumental and anthropological 

147
Where Is the Missing Matter?
﻿
definitions of technology. In modern times, as consequence of modernization and industrialization of 
Western societies the theories on technology (and particularly the philosophy of technology) take its 
momentum trying to understand its key role.
There exists two main positions on the issues concerning the nature of modern technology and its 
role in society: technology is determining/technology is neutral (Feenberg 2010; 5-29). The first stance 
has Marxist roots, which have been developed by sociologist like Max Webber and the thinkers from 
the Frankfurt school. They argue that modern technology is emancipated and neutralized techniques, 
that is determined of an autonomous functional logic. That view naturally leads to dystopian arguments 
like these of Heidegger (Heidegger 1977) On the other side, social constructivists (like Bloor, Pinch, 
and Bijker for example) demonstrate that technology is absolutely neutral and fully depends on social 
context in which it functions. Still, what in its essence is technology?
In his well-known analysis “The Question Concerning Technology” (Heidegger, M. 1977), searching 
for the essence of technology, Martin Heidegger takes a specific perspective on its nature, regarding it 
in its actuality, as a mode of unveiling. Technology is first and foremost a way of bringing something 
into appearance; it is a way of disclosing Beings. Referring to the four causes for the existing formulated 
by Aristotle in his Metaphysics (Aristotle 2008), Heidegger approaches to the question of technology, 
seeking its essence beyond its pure instrumentality:
(1) the causa materialis, the material, the matter out of which, for example, a silver chalice is made; (2) 
the causa formalis, the form, the shape into which the material enters; (3) the causa finalis, the end, for 
example, the sacrificial rite in relation to which the chalice required is determined as to its form and 
matter; (4) the causa efficiens, which brings about the effect that is the finished, actual chalice, in this 
instance, the silversmith. (Heidegger 1997: 4)
Let us scrutinize the principles of the additive manufacturing through Aristotle’s philosophical teaching.
First of all, where is the matter? What is it? Obviously it is not the classic or modern object. The 
matter is no more the well-known texture of the Being, it is not ‘thing-in-itself’, it is dissolved, fluid, 
un-significant. In the additive manufacturing the material is dissolved in a powder form in advance. 
Matter as a substance, as already-available, is not a reality, but a construction. Matter does not guarantee 
the resistance of the world; it does not bring the stability of the reality, as it does not determine what is 
possible. Matter is fabricated; it is rather a product, than a ground of the existed. In that sense, it is not 
a cause of the things, as it does not define them. Matter is created and produced.
On the other hand, form is loosed from the bondage of the matter. It looks for and appeals the proper 
matter. Form questions the matter that is subjected to putting into reality. Form is responsible for its matter.
The third, the causa finalis, that is what determines the things. It determines the proper form and 
appeal for the requisite matter. In the case of the additive manufacturing, there is not an unveiling of the 
Being, but affirmation of the “stock”, i.e. multiplying, cloning of the visible, or more precisely, of what 
exists. The “stock” is what is already here and subjected to deal with; it is the known, transparent, and 
unfolded. It is the storage of the pure instrumentality.
In respect to the causa efficiens, it seems that in the technology of additive manufacturing the interac-
tion fades. It is not a game between man and technology, as the process is not a ‘bringing-forth’, it is not 
unveiling (of truth), ‘techné’ (Heidegger 1977). The interaction is not a game, testing of the possible; it 
is an affirmation of the existing stock.

148
Where Is the Missing Matter?
﻿
The additive manufacturing is not an action which makes truth to come into being. It seems that 
‘the how’ of the things withdraws, and the pure functionality, instrumentality, usefulness, excels. ‘The 
what’ of the things is already here, it is not questioned; it is given and, in that sense, defined. From that 
perspective the additive manufacturing is a doubling of the “standing-reserve” that is a non-material 
object. The causa finalis masters matter, form, and action, which does not meet resistance.
Тhe additive manufacturing is not a bringing-forth in the sense of techné and poiesis. It is rather a 
challenging [Herausfordern] of the standing-reserve, or the “stock”. Here is what Heidegger concludes:
Modern technology as an ordering revealing is, then, no merely human doing. Therefore we must take 
that challenging that sets upon man to order the real as standing-reserve in accordance with the way 
in which it shows itself. That challenging gathers man into ordering. This gathering concentrates man 
upon ordering the real as standing-reserve. (Heidegger 1977: 10)
Does the additive manufacturing a modern technology? In his comment on the Heidegger’s theses from 
the ‘The Question Concerning Technology’ Bernhard Waldenfels portrays the expansion of a neutralized 
technology, outlining that process as modern loosen of technology. That is a process of reversing of the 
roles of the goals and means, in which the mean becomes ‘potentia’, which generates its own goals. 
(Waldenfels 1995). It seems that in the case with the additive manufacturing there is pure instrumen-
talisation of the technology, in which its essence is no more ‘unveiling of the truth’, but a routine work 
for reproduction of the standing-reserve. It is a mode of ‘Enframing’, in terms of Heidegger, in which 
the intention, the regulating, and the ordering, are responsible for the co-operation of the matter, form, 
and activity. From a critical theory perspective, the additive manufacturing could be considered as an 
ultramodern technology process, which is determined by a single motive power, demiurge – that of the 
human reason and imagination. The world is already here, it is unfolded and known, and one could only 
clones and stores it.
Now, we will change the optics of analysis. Trying something like ‘downshifting’ from the ‘high-
ness’ of the critical theory, we will examine technology according to its ‘pragmatic’ essence and role. 
Below, we will get together the Actor-Network Theory with Heidegger’s philosophy of technology and 
will sketch some points for dialog between the two.
3. PHILOSOPHY OF TECHNOLOGY AND ACTOR-NETWORK THEORY
As Waldenfels points out, technology is a model of organization, in which artefacts take place (Waldenfels 
1995). It is rather a way of interaction between human and nature. That is why it is important to remind 
that Heidegger’s philosophy of technology reveals the specific dynamic essence of technology, which 
oscillates between the technique-as-an-emancipated-demon and the technique-as-pure-means. We must 
closely monitor the dosing and distribution of forces without privilege none of the poles as essence of 
the technique. The last has a synergetic nature (Waldenfels 1995: 113).
There is at least two important, and paradoxical at first glance, consequences: first, the technique does 
not have its own, independent from the interaction nature; second, the technique has (or may have) its own 
identity. The misunderstanding, or the trapping in this paradox, often leads to labelling of Heidegger’s 
philosophy as unfree by modernist dogma. Besides, it is more a critique of modernity.

149
Where Is the Missing Matter?
﻿
Let us see what ANT tells about technology. Bruno Latour insist that one could not comprehend the 
“nature” of technology relying on materialistic or sociological accounts, since any of these repertoires 
represent a purified, practically non-existent technology. He tries to identify technology in its own 
stance, getting out of the trap of the deterministic/constructivist debate and revealing its ontological 
status. Latour insists,
…essence is existence and existence is action. (Latour 1994: 33) 
Nothing is, by itself, either reducible or irreducible to anything else. (…) There are only trials of strength, 
of weakness. Or more simply, there are only trials. This is my point of departure: a verb, “to try.” (La-
tour, B. 1988: 158)
Every entelechy makes a whole world for itself. It locates itself and all the others; it decides which forces 
it is composed of; it generates its own time; it designates those who will be its principle of reality. It 
translates all the other forces on its own behalf, and it seeks to make them accept the version of itself 
that it would like them to translate. (Latour, 1988: 166)
In order to escape from the unfruitful realist-constructivist dichotomy, Bruno Latour suggests the 
concept of technical mediation (Latour 1994). As he suggests, technology that is a domain of transla-
tion, it is a field of interference between actants, where humans’ and non-humans’ exchange features and 
capacities, where goals and intentions take place, invent, enforce, embed, drift, and shift. Although the 
mediation implies a relative autonomy of technology, standing and being blackboxed as actant, Latour’s 
standpoint is far from the apprehensions of critical theorists or determinists. Technical mediation is a 
way of varying the boundaries between signs and things and crossing the imagined line between them 
(Latour: 1994; 1999). Eventually, there are no pure human and non-human activities, and if there are 
any, they are consequences of ‘trials of strength’ (Latour) and translations, purified by thinkers (mainly 
humanitarian ones). Technology cannot master humans as well as humans cannot overpower technology, 
since it is impossible to separate one another. These are just two, even though widespread, myths – the 
myth of neutral tool and the myth of the autonomous destiny.
Indeed, there are so many differences between Heidegger and Latour (Latour 1999). Nevertheless, 
there exists a fundamental similarity in the analyses of Martin Heidegger and Bruno Latour (Khong, 
L. 2003; Rijs 2008; Harman 2009). These authors show that despite of the fact that Latour has been 
acknowledged as an extremely non-modern thinker and researcher, and despite of the explicit Latour’s 
criticism on Heidegger’s philosophy of technology (Latour 1994; 1999), there are some very important 
similarities between the two. First of all, the two of them struggle with the subject-object distinction in 
the knowledge structure and appeal to “…treat all beings as substitutable with artifacts” (Rijs, S. 2008: 
296). In relation with technology, they accept its ambivalence, insisting on its processing “nature” and 
actuality. They agree that technology reveals the folding of both humans and non-humans and the mo-
bilizing of the world in the form it appears to us.
As Andrew Feenberg notes, Heidegger rejects the causal interpretations on interactions between 
humans and nature and insists on the actual experience as an “irreducible ontological basis” (Feenberg 
2010: 186) On the other side, Latour elaborates his theory focusing on the action as a irreducible to any 
essence event, which is the only gradient of reality. According to the two of them, the world, the reality, 
gains its meaning and constitution only in the actual encounter and interaction between actors. And the 

150
Where Is the Missing Matter?
﻿
difference is in the way they see the building of that reality. For Haidegger it happens in more or less in 
passive terms – as opening, revealing, bringing-forth; since for Latour reality is a battlefield of ‘trials of 
strength, where forces express themselves only through action, and translation.1
Nevertheless, the most important similarity between Heidegger and Latour is that both of them work 
on ontologies, i.e. they frame the principles of existence and interaction of all of the entities, inhabit-
ing the world. Both of them agree that the classic Cartesian schemes do not deal with reality, but with 
a theoretical construction. Though in its researches Latour appears as par excellence sociologist, in its 
theoretically oriented works he deals with ontology. Though Heidegger’s theses are often recognized 
as sharp social critics, he aims to reveal the essence of technology, according to its ontological status. 
Proceeding from that, what consequences for the analysis of the additive technologies can be drawn?
4. ADDITIVE MANUFACTURING: WHERE IS THE MISSING MATTER?
In his well-known article ‘’Where Are the Missing Masses? The Sociology of a Few Mundane Artifacts’’ 
Bruno Latour reveals the narrow perspective of modern sociologists on those elements which often attach 
the ingredients of the social. Asking, ironically in a way, where are the missing masses, he called out the 
hordes of unrepresented in social theories actual participants in the social collectives (technology and 
natural forces). He argues: “we should do now to find a place in a new social theory for the nonhuman 
masses that beg us for understanding” (Latour 1992: 153).
As we saw, in the process of additive manufacturing there is no pure wild non-human, since matter 
as thing-in-itself, does not exist. The researcher is belated, since the powder form of the material shows 
that the translation has been already fulfilled. The ‘trials of strengths’ (Latour) have already taken place. 
There are no essences. It works with material that is a construction, not an essence. Some would say that 
the material has only been dissolved and transformed, but it is still the same matter. However, matter as 
a substance, as already-available, is a construction, since its “drift” of experience (potential or aim to 
express itself) diverges. From that perspective the additive manufacturing lies on such a stripping and 
depersonalization of objects (reducing them to mere raw materials), which does not have their own iden-
tity. We can say it overrides the realism and the objectivism of things-in-itself, as it decomposes them. 
The additive manufacturing privileges the freedom of designing and the power to vary and manipulate 
“the real”, regardless of the limitations of the matter.
There is something ambivalent in the concept of ‘non-human’. From the one hand, Latour insists that 
there are only forces, actants, but on the other hand, he talks about humans and non-humans, as the main 
actors in social life, without framing what exactly constitutes them. In fact, pure ‘non-human’ is a kind 
of ’ideal type’, used by Latour in two ways. In the first way it is just a language form through which one 
could escape subject-object distinction and in the same time comprehend entity which does not have 
reason, but acts. The second way presents the scientists’ and engineers’ expressions of their objects dur-
ing the trials in which they are involved in. From that perspective, we could conclude that ‘human’ and 
‘non-human’ do not have their own ontological or theoretical status. In order to keep the principles of 
Irredustions2 program (which is, in my opinion, the most important program document, or, I would say, 
the Constitution, of ANT), I think that in the reality, in the practice, the ‘non-human’ appears as a kind of 
‘inverted form’ which comes to signify an actant who was translated as a force which resists to human. 
If we would like to keep the use of the concept of “non-human” in ANT, it is very important to bear 
in mind some distinct features of that notion. Non-human is not thing-in-itself. Moreover, non-human is 

151
Where Is the Missing Matter?
﻿
not an essence, inherent to some of the actors. It is neither an autonomous force, independent from other 
participants, nor a closed in itself entity, passively waiting to be presented. It is just a force, an ingredient 
of the reality that exists. Its activity (or passivity) is not their “own” feature, but a consequence of ‘trials 
of strength’ (Latour). But let us go back to the additive manufacturing. 
5. CONCLUSION: WHERE IS THE MISSING MASTER AFTER ALL?
The analysis of the additive manufacturing shows us one more interesting paradox. From one hand, it is 
a simple process of doubling, multiplying, or cloning of the reality. There is nothing brand new and it 
could not be defined as bringing-forth. On the other hand, the additive manufacturing (particularly the 
designing and the printing in itself) creates product out of nothing, since the process is not determined 
from given limits and essences. It seems that the potential of that technology really could generate goals 
out of nothing. It seems that it is a pure, neutralized technology, whose autonomy is unlimited in itself. 
Is it an example of an emancipated ultra-modern technology? One more time, it seems paradoxical: who 
masters who? The burden of the matter is missing; technology as a craft, as bringing-forth, is missing; 
Being is unfolded and transparent. Who is the responsible one? It is not the mind, neither the Being. 
Everything that we could examine is a secondary product of the ‘trials of strength’ (Latour). Everything 
is at stake. Everything belongs on the domain of ‘trials of strength’ (Latour). 
Since there is no presupposed actor(s) who is responsible in the technological process, or who has 
the attribution for it, we can conclude that the case of the additive manufacturing shows us the actor-
network ontology in action. Through it one could examine the forming of reality as a negotiating process 
between flexible entities, and sketch the space of ‘parliament of thing” (Latour 1993) The case of the 
additive manufacturing demonstrates a way of democratizing the technology process, as it opens space 
for testing the possibilities of all of the forces, translated in more or less defined actors – in that case 
these are reason, creativity, matter, form, goal, action. 
REFERENCES
Aristotle, . (2008). The Metaphysics. New York: Cosimo Classics.
Feenberg, A. (2010). Between Reason and Experience. Essays in Techology and Modernity. MIT Press.
Harman, G. 2009. Prince of Networks: Bruno Latour and Metaphysics. Re.press Melbourne 2009. (http://
www.re-press.org/book-files/OA_Version_780980544060_Prince_of_Networks.pdf)
Heidegger, M. (1977). The Question Concerning Technology. In D. F. Krell (Ed.), Basic Writings (pp. 
307–343). New York: Harper & Row.
Kapriev, G., & Tchalakov, I. 2009. Actor-Network Theory and Byzantine Interpretation of Aristotle’s 
Theory of Action: Three Points of Possible Dialogue, in: 2008 Yearbook of the Institute for Advanced 
Studies on Science, Technologies & Society, Edited by Arno Bammé, Günter Getzinger, and Bernhard 
Wieser, Graz (Austria) – München (Germany), Profil

152
Where Is the Missing Matter?
﻿
Khong, L. 2003. Actants and enframing: Heidegger and Latour on technology. In: Studies in History 
and Philosophy of Science Part A Volume 34, Issue 4, December 2003, Pages 693–704
Koleva, A. 2015. The additive technologies (3 D printers) – world social and and economic revolution. 
(unpublished MA thesis in Plovdiv University)
Latour, B. 1988. The Pasteurization of France. Harvard University Press.1988
Latour, B. (1992). ‘Where Are the Missing Masses? The Sociology of a Few Mundane Artifacts. In W. 
E. Bijker & J. Law (Eds.), Shaping Technology/Building Society: Studies in Sociotechnical Change (pp. 
225–258). Cambridge, Mass.: MIT Press.
Latour, B. (1994). On technical mediation. Common Knowledge, 3(2), 29–64.
Latour, B. (1999). Pandora’s Hope: Essays on the Reality of Science Studies. Cambridge, MA: Harvard 
University Press.
Riis, S. (2008). The Symmetry Between Bruno Latour and Martin Heidegger: The Technique of Turning a Po-
lice Officer into a Speed Bump. Social Studies of Science, 38(2), 285–301. doi:10.1177/0306312707081379
Waldenfels, B. (1995). The scope of technology. In Sociological problems (Vol. 2, pp. 105–115). Quar-
terly. (in Bulgarian)
Weber, C. L., Peña, V., and al. 2013. The Role of the National Science Foundation in the Origin and 
Evolution of Additive Manufacturing in the United States. Washington DC: Science & Technology 
Policy Institute.
Wohlers, T. (2012). Wohlers Report 2012: Additive Manufacturing and 3d Printing, State of the Industry. 
CO: Wohlers Associates.
ENDNOTES
1 	
Ivan Tchalakov and Georgi Kapriev propose an interesting critique on the activism of ANT and 
suggest a revision of its core principles. (Kapriev, G. and Tchalakov, I. 2009)
2 	
The Irreductions is a part of the famous book The Pasteurization of France. It is one of the very 
few firmly philosophical texts by Bruno Latour. (Latour 1988)
This work was previously published in the International Journal of Actor-Network Theory and Technological Innovation 
(IJANTTI), 7(1); edited by Ivan Tchalakov, pages 10-17, copyright year 2015 by IGI Publishing (an imprint of IGI Global).

Section 2
This section presents an extensive coverage of various tools and technologies available in the field of 3D 
Printing and discusses a variety of applications and opportunities available that can be considered by 
practitioners in developing viable and effective 3D Printing programs and processes. With 7 chapters, 
this section offers a broad treatment of some of the many tools and technologies within the 3D Printing 
field. These chapters enlighten readers about fundamental research on the many tools facilitating the 
burgeoning field of 3D Printing and review topics from case studies to best practices and ongoing re-
search. Further chapters discuss 3D Printing in a variety of settings. Contributions included in this section 
provide excellent coverage of today’s IT community and how research into 3D Printing is impacting the 
social fabric of our present-day global village. It is through these rigorously researched chapters that 
the reader is provided with countless examples of the up-and-coming tools and technologies emerging 
from the field of 3D Printing.
Technologies and Applications

154
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  8
DOI: 10.4018/978-1-5225-1677-4.ch008
ABSTRACT
Laser additive manufacturing is an advanced manufacturing process for making prototypes as well as 
functional parts directly from the three dimensional (3D) Computer-Aided Design (CAD) model of the 
part and the parts are built up adding materials layer after layer, until the part is competed. Of all the 
additive manufacturing process, laser additive manufacturing is more favoured because of the advantages 
that laser offers. Laser is characterized by collimated linear beam that can be accurately controlled. 
This chapter brings to light, the various laser additive manufacturing technologies such as: - selective 
laser sintering and melting, stereolithography and laser metal deposition. Each of these laser additive 
manufacturing technologies are described with their merits and demerits as well as their areas of applica-
tions. Properties of some of the parts produced through these processes are also reviewed in this chapter.
1. INTRODUCTION
Laser is an important technology with exciting properties that makes it highly valued in most human 
endeavor. The importance of laser in engineering in general cannot be over emphasized and in mate-
rial processing in particular is phenomenal. Some important characteristics and types of lasers used in 
material processing are discussed in this chapter. The use of laser in material processing and in additive 
manufacturing is revolutionary. Additive manufacturing is an advance manufacturing process that is 
used to fabricate three dimensional (3D) parts directly from the 3D computer aided design (CAD) model 
of the part to be produced simply by adding materials layer after layer until the building of the part is 
completed (Scott et al., 2012; Mahamood et al., 2014a). A number of additive manufacturing technolo-
gies use laser as their energy source and they are called laser additive manufacturing (LAM) processes. 
The laser additive manufacturing uses the energy from the laser to process materials due to the exciting 
properties of the laser that enables the laser beam to be effectively controlled for the intended manufac-
Laser Additive Manufacturing
Rasheedat M. Mahamood
University of Johannesburg, South Africa & University of Ilorin, Nigeria
Esther T. Akinlabi
University of Johannesburg, South Africa

155
Laser Additive Manufacturing
﻿
turing operation. Some of the laser additive manufacturing technologies are discussed in this chapter. 
There merits and demerits are also highlighted, and their area of applications are mentioned. The chapter 
ends with summary and the future research directions of the laser additive manufacturing processes.
1.1. Lasers in Material Processing
Laser is an acronym that is used to describe the technology of Laser. LASER stands for Light Amplification 
by Simulated Emission of Radiation (Haken, 1983). The laser is generated from the light source which 
is then amplified in such a way that is similar to the way microphone amplifies sound. The amplification 
of the light is achieved by a process that is known as simulated emission; it is also referred to as optical 
amplification (Yamashita et al., 2007). The rays of light that are emitted from a single light source are 
used to create an excitation in the atoms that are present in the lasing medium or the gain amplification 
medium which could be in form of solid, liquid or gas (e.g. Co2). The atoms in these lasing media get 
excited and they emit a coherent type of light rays. The amplification is achieved through the arrangement 
of mirrors in the gain chamber. The excited atoms bounce back and forth between these mirrors thereby 
resulting in a powerful amplified coherent beam of light rays that is called ‘Laser’ (Haken, 1983; Silfvast, 
1996). Imagine placing an object in between two parallel mirrors, the image of the object will bounce 
back and forth and the numbers of images that is produced as seen in the mirrors becomes uncountable 
(see Figure 1a.). This is what is referred to as an optical amplification that has generated countless im-
ages of a single object placed between two mirrors. This same principle happens when a single light 
source is placed in between two parallel mirrors. The Laser light is characterized by a single wavelength 
that is known as monochromaticity; the light from laser usually comes from one atomic transition with 
a single precise wavelength which gives the laser light a single spectral color and it is almost the purest 
monochromatic light available (Ambroseo, 2001). Coherency, also known as same phase position and 
low divergence (they spread out in parallel lines) are other important characteristics of laser (Haken, 
1983). All these characteristics contribute to the higher intensity of the laser beam as compared to other 
light sources and thereby allowing the concentration of all the intensity at a particular point of interest.
1.2. Classification of Lasers
Laser can be classified based on the mode of operation. It can either be operated in a continuous wave 
(CW) mode or in a pulsed mode (Paschotta, 2008). Continuous wave mode operated laser are operated 
at a constant power output over a length period of time. The Laser can also be operated in a pulsed 
mode such that the output power appears in pulses for a certain length of time and also repeatedly over 
a period of time. The pulsed laser may be used where a large power is required for a very short length 
of time. An example of an application where the pulse laser is desirable is in the laser ablation process 
(Powell, 1993). In laser ablation process, the high power is required to melt and evaporate a small por-
tion of material in a very short length of time without transferring the heat generated during the process 
from the point of ablation to the bulk material. On the other hand, the continuous wave laser is used in 
applications that required a constant power over a period of time. There are different types of lasers and 
they are discussed in the next sub section.

156
Laser Additive Manufacturing
﻿
1.3. Types of Laser
There are different types of lasers depending on the lazing medium used. They include the gas laser 
(such as Helium-neon, argon, Nitrogen, Co2 and excimer laser), the chemical laser (such as hydrogen 
fluoride, and Deuterium fluoride), the solid-state lasers (Ruby, Nd: YAG, Nd: Glass), the Dye laser, the 
metal-vapour lasers, and the semiconductor lasers (Csele, 2004). Out of all the numerous lasers avail-
able, only a few of them are used for processing metallic materials. This is because of the low power 
delivered by most of them (Wikipedia, 2013). Some of these lasers are presented in Table 1 with their 
wavelength and areas of application.
From Table 1, the different lasers presented have different wavelength. The laser absorption into the 
material that is being processed is a function of the wavelength of the laser used (Berkmanns and Faerber, 
2010). Only a few lasers are used in material processing for this reason and also for low powers. The 
most commonly used lasers in material processing are the Co2 and the Nd-YAG lasers. The difference 
between Nd-YAG and Co2 laser are presented in the next sub section where the effect of the wavelength 
is seen on the absorption of different materials.
Figure 1. (a) Demonstration of image amplification by two parallel mirrors (b) Absorption rate of laser 
radiations in cold metal (Berkmanns and Faerber, 2010)

157
Laser Additive Manufacturing
﻿
1.4. Difference between Co2 Laser and Nd: YAG Laser
There is a relationship between the rate of absorption of laser in materials and the laser wavelength as 
reported by Berkmanns and Faerber, (2010). The difference between the Co2 laser and the Nd: YAG 
laser is that, the wavelength of Co2 is ten times the wavelength of the Nd: YAG laser. The influence of 
these wavelengths is seen in their behaviour when used to process materials. For example, cold metallic 
materials will reflect most part of the Co2 radiations. As the material temperature is increased, the absorp-
tion rate will also increase. Whereas, the Nd: YAG laser is better absorbed by cold metallic materials. 
Also, the laser absorption differs from one material to the other based on the wavelength of the laser. 
For example, Co2 laser is very well absorbed in plastics and plywood whereas, the Nd: YAG is poorly 
absorbed in these same materials. The Nd: YAG has good absorption in steel and non-ferrous metals 
but the Co2 laser is poorly absorbed in some non-ferrous metals (Berkmanns and Faerber, 2010). Some 
metals and their absorptivity in different lasers are shown in Figure 1b
It can be seen from Figure 1b that the CO2 laser with the wavelength of 10.6 µm is poorly absorbed 
in Aluminium, Silver and Copper and well absorbed in Iron and Steel. Most of the Co2 laser beam is 
reflected in these materials and the physical appearance of these materials could be responsible for this 
behaviour. Brightly coloured metals tend to reflect most of the long wavelength of the CO2 laser rather 
than the short wavelength laser-Nd: YAG. The shining Silver also poorly absorbs the Nd: YAG laser 
and it is seen to absorb the Diode laser with the shorter wavelength of less than 1.0 µm. The physical 
appearance of the material has a higher influence on the laser’s wavelength it is going to absorb. The 
next sub section focuses on the laser-material interaction.
1.5. Laser Material Interactions
The coherent and highly focused energy density provided by the laser has made it possible to process 
material through localized heating of the materials. The Laser is a tool-less machine and it does not pollute 
the workpiece through wear or tear which is common in the traditional material processing techniques. 
The laser processes the materials by interacting with the surface of the material depending on the laser 
beam parameters and the properties of the material that is being processed (Bauerele, 2011). When the 
Table 1. Types of laser, their wavelength and areas of application
Type of Laser
Wavelength
Areas of Application
Carbon monoxide (Co)
2.4-4 µm
Material processing (engraving, welding etc.)
Hydrogen fluoride
2.7-2.9 µm
Laser weapon
Carbon dioxide (Co2)
10.6 µm
Material processing, surgery etc.
Nd: YAG
1.064 µm
Material processing
Nd: Glass
1.062 µm
Velocity and length measurement
Dye laser
390-640 nm
Medicine, Birth mark removal
Excimer
193 nm
Laser surgery
Ruby
694.3 nm
Tattoo removal, Holography
Helium-neon
632.8 nm
Holography, spectroscopy
Argon
454.6 nm
Lithography, spectroscopy

158
Laser Additive Manufacturing
﻿
laser beam hits the surface of the material being processed, it excites the free electrons within a metal 
for example; this excitation causes the atoms to release energy by colliding with other atoms as they are 
being excited (Siegman, 1986; Haken, 1983). The energy that is released by the colliding atoms is then 
dissipated as heat to the surrounding lattice (Paschotta, 2008). The heat that is dissipated causes a rise 
in the temperature of the material. The temperature distribution in the material will depend largely on 
the properties (such as reflectivity and thermal conductivity) of that material (Bauerele, 2011). Also, the 
temperature distribution in the material under laser processing will depend on whether the material is 
close to a phase change, for example solid to liquid, that is, near the melting point (Bauerele, 2011). The 
functionality that is required from laser may be to melt and vaporize, or it may be needed to just heat the 
material but not to melt the material or just to melt and not to vaporize the material. Depending on the 
required application of laser in material processing, drilling and cutting, may requires to remove material 
inform of liquid, vapour, or plasma. Plasma is formed as a cloud of vapour from the material at certain 
laser intensity (Liseykina and Bauer, 2012). It is important to control the laser beam intensity or the laser 
material interaction time during the material processing depending on the desired objective. The Co2 
laser and the Nd: YAG lasers are the most commonly used lasers for processing metallic materials. The 
next sub section is focused on the laser energy absorption during the laser-material interaction process.
1.6. Laser Energy Absorption during Laser Material Interaction
There are two stages involved when the laser comes into contact with a material being processed with 
laser. The first thing that happens is the coupling of the laser and the material particles (Hügel and 
Dausinger, 1996). When a laser beam falls on the substrate material during the laser metal deposition 
process for example, part of this laser beam is absorbed while some are reflected. The material property 
and the surface condition of the material being processed are responsible for the degree of absorption and 
reflection that is taking place during this interaction phase. The ratio of laser absorptivity and reflectivity 
is called the laser coupling or the energy coupling as described by Hügel and Dausinger 1996. The laser 
coupling is the measure of the quantity of the laser energy that can be transferred from the laser beam 
into the material under processing; and these variables are related mathematically, according to Hügel 
and Dausinger (1996); and this ratio is given in equation 1.
P
P
P
P
P
R
A
=
+
=
+
+
=
α
β
α
β
;
1 	
(1)
where P is the laser power, PA is the partially absorbed laser power; PR is the partially reflected laser 
power. The α and β are the absorptivity and reflectivity ratios, respectively.
The powder material’s absorptivity is greater than that of a single solid mass of material (Hügel et 
al., 1994). This is because when the laser beam falls on the surface of the substrate material, the beam 
is partially absorbed and partially reflected. The partially reflected beams are lost, as they are reflected 
back to space: whereas, when the laser beam falls on the powders, a higher proportion of the energy 
from the laser beam is absorbed. This is because, the reflected beams would penetrate the surrounding 
powder; and this absorption, reflection, and further absorption of the powder particles would help to 
increase the overall laser absorptivity (Hügel et al., 1994). Gas atomized powders are of great importance 
in the laser material interaction because they help to increase the laser power absorptivity (Niu & Chang, 
2000). On the other hand, the surface oxidation is detrimental to the laser energy absorptivity, because 

159
Laser Additive Manufacturing
﻿
the oxidized surfaces will reduce the energy coupling process (Hügel et al., 1994). This is why it is im-
portant to perform the laser processing of most materials in an inert atmosphere. The second stage after 
the coupling process is the penetration of the absorbed energy. The depth of the laser energy absorption 
will depend on the quantity of energy absorbed by the material being processed, and the wavelength of 
the laser that is being used (Wang & Kruth, 2000). The Solidification Mechanisms and Microstructural 
Formation in the Laser Material Processing are discussed in the next sub section.
1.7. Solidification Mechanisms and Microstructural 
Formation in the Laser Material Processing
The importance of microstructure in the laser material processing cannot be overemphasized, because the 
microstructure has a direct relationship with the properties of the processed materials. Also, the process 
of solidification has direct influence on the microstructural formation. The melt pool that is created on 
the surface of the substrate by the laser beam during the laser material interaction begins to solidify the 
moment the laser leaves the melt pool site. The solidification rate of this melt pool would depend on a 
number of factors. The factors include: the laser power that was applied, the scanning velocity, and the 
initial temperature of the substrate before the laser energy was applied. A high laser power combined 
with a low scanning speed would result in a large volume of melt pool and hence a low solidification 
rate (Wu et al., 2004). A lower solidification rate would also result in some melting of the substrate or 
the preceding layer (Brody, 1986). This is because a low solidification rate causes the melt pool to stay 
longer on the surface of the substrate. When the melt pool stays longer on the substrate or the preceding 
layer, it softens and further melts the substrate or the preceding layer. If the substrate is cold, on the other 
hand, the solidification would be quite rapid because the substrate would act as a heat sink. That is, the 
heat in the melt pool is absorbed by the cold substrate, thereby causing the solidification to be very fast. 
Furthermore, if the scanning speed is high, the laser material interaction time will be low; and this also 
causes the melt pool to solidify rapidly. In laser deposition process, solidification process begins from 
the solid-liquid interface region, (that is, the interface between the melt pool and the substrate or the 
preceding layer) (Brody, 1986). This interface is the nucleation site for the crystal growth. The crystal 
that is nucleated on the substrate or the preceding layer will act as a seed crystal, upon which the other 
crystals grow (Brody, 1986). This type of crystal growth that follows a crystallographic orientation 
with respect to the substrate crystal is known as “epitaxial grains” (Brody, 1986; Kurz, 1992). Epitaxial 
grains are shown in Figure 2. As the solidification process progresses, the grains will continue to grow 
in the perpendicular direction towards the substrate – and in the opposite direction to the direction of the 
heat flow (Kobryn et al, 2000). The heat flow direction is towards the substrate; and the grain growth 
is towards the surface where the melt pool is located. This grain growth shows a characteristic grain 
structure that is referred to as columnar grain (Kobryn et al, 2000; Brandl et al., 2011). The columnar 
grain structures are also indicated in the Figure 2. The heat affected zone in laser metal deposited Tita-
nium is characterized by the globular grain structures. The globular grains are formed as a result of heat 
transferred from the melt pool to the substrate material. The grains very close to the melt pool region 
gain enough heat, which results in the growth of the surrounding grains. This grain growth in the heat-
affected zone results in a globular grain structure. The size of the globular grain depends on the relative 
position of the grains to the melt pool region. The closer the grain to the melt pool, the bigger the size 

160
Laser Additive Manufacturing
﻿
of the globular grain produced. The solidification rate is related to the scanning velocity, as described 
by Steen (1998), and it is given in equation 2.
R= v sin θ 	
(2)
where R is the solidification rate, v is the scanning velocity, and θ is the angle between the tangent of 
the growth vector and the scanning direction.
The solidification rate is directly proportional to the scanning velocity. Such that the higher the 
scanning velocity, the higher the cooling rate. At a very high solidification rate, the microstructure 
formed in titanium-metal- composite, for example, tends to become more dendritic (Chen & Wang, 
2003). This in turn results in the higher hardness property of the material. The coarser or the finer the 
evolving microstructure is directly caused by the cooling rate. The higher the cooling rate, the finer the 
microstructure and the lower the cooling rate the coarser microstructure. At a very high cooling rate, 
the microstructure becomes equiaxed (Steen, 1998). This shows that the processing parameters play an 
important role in achieving the desired cooling rate in order to obtain the desired microstructure and as 
well the desired properties.
The various laser additive manufacturing processes are discussed in the next sub- section.
2. LASER ADDITIVE MANUFACTURING (LAM) PROCESSES
The laser additive manufacturing processes use laser energy to melt or fuse materials layer after layer in 
order to produce a 3D component directly from the 3D CAD model of the component being produced. 
The laser additive manufacturing technologies that are discussed in this section are: stereolithography 
Figure 2. Epitaxial and columnar grain structure in laser deposited Ti6Al4V

161
Laser Additive Manufacturing
﻿
apparatus (SLA), selective laser sintering (SLS), selective laser melting (SLM), laser metal deposition 
(LMD) and laminated object manufacturing (LAM). The laser provides a high intensity and a highly col-
limated beam of energy that can be quickly moved in a controlled manner with the help of the directional 
mirrors or lenses in the laser system. This is what makes the laser to be useful as an energy source in 
the laser additive manufacturing. The use of the laser energy in the additive manufacturing technologies 
is basically for curing, cutting, fusing or melting. In the photopolymer resins application, for example, 
what is required from the laser energy is to cause the liquid resin to solidify or “cure” (as is the case in 
stereolithography (SLA)). Heating is required for cutting through a solid material as used in laminated 
object manufacturing or for melting powdered material as it is in selective laser melting or fusing powder 
as in selective laser sintering and sheet materials. Laser is expected to carry sufficient thermal energy to 
be able to achieve the above mentioned applications. In laser metal deposition process for example, the 
basic requirement from laser is to melt the powder in a controlled manner without creating heat build-up. 
When the laser energy is removed, the molten material will rapidly solidify. Some of the commercially 
available LAM technologies are discussed in the following sub-sections.
2.1. Selective Laser Melting
The Selective laser melting started in 1995 at the Fraunhofer Institute ILT in Aachen, Germany. The se-
lective laser melting (SLM) is an additive manufacturing process that can produce complex components 
directly from CAD model of the components using powdered materials. Selective laser melting belongs 
to the ‘laser sintering’ according to the ASTM International F42 standards committee (Scott et al., 2012). 
The processing speed is about 5-20 cm3/h depending on the surface area of the component being made. 
Three steps are involved in the SLM process: during the first step the substrate is lowered by one layer 
thickness level. The second step involves applying of the new layer of the powder material on the substrate 
that was initially lowered one layer thickness. The layer is applied using the coater or roller. The third step 
involves scanning of the path dictated by the CAD file with the laser. As the laser moves on the surface 
of the powder, it melts the powder along its path as a result of the energy absorbed by the powder. These 
steps are repeated until the building of the component is completed. The building of part takes place inside 
a build chamber containing an inert gas, in order to keep the nitrogen and oxygen levels below 500 ppm. 
The laser energy is high enough to cause the full melting of the metal powder particles to form the 3D solid 
metallic component. The schematic diagram of the SLM process is shown in Figure 3.
2.1.1. Advantages of SLM
Components with hollows and undercuts are easily built using the selective laser melting process; this 
is because components are made by building of components layer by layer. Components can be built 
in any orientation that is so convenient without any restrictions like it is the case with the traditional 
manufacturing processes.
2.1.2. Disadvantages of SLM
Though nearly 100% fully dense part can be produced using the SLM process, but the low degree of 
porosity is still an issue as this impact the mechanical property of the component produce. Dimensional 
stability is also of great concern in the SLM, and warping of the part as a result of residual stress. The 

162
Laser Additive Manufacturing
﻿
traditional manufacturing techniques have a relatively high set-up cost (for example high cost for creat-
ing a mould) but the SLM has a high cost per part (mostly because it is time-intensive process), it is 
economical only if few parts are to be produced.
2.1.3. Areas of Applications
SLM are used to produce parts with a high degree of complexity and structures with thin walls and hidden 
voids and for low volume of work. Example includes lightweight parts for the aerospace industry. This 
laser additive manufacturing process can be used to manufacture tools for the plastic injection molding 
and the die casting. It can also be used to produce structures for medical implants. SLM is mostly used 
in the area of rapid prototyping, rapid tooling and rapid manufacturing.
2.2. Laser Metal Deposition (LMD)
The laser metal deposition (LMD) process is also known as the direct metal laser deposition (DMLD), 
direct laser deposition (DLD), direct metal deposition (DMD) or laser powder deposition (LPD). LMD 
belongs to the ‘Directed Energy Deposition’ (DED) class of additive manufacturing process that is 
based on the laser cladding process. The major difference between the laser cladding and the laser metal 
deposition process is that, the laser cladding process involved the pacing of powder on the substrate 
after which the laser beam is scanned on the powder bed while in the LMD process, the laser beam is 
used to create a melt pool on the surface of the substrate, then the powder or wire is fed into the melt 
pool which upon solidification leaves the tracks of solid material on its path. The schematic of the LMD 
process is shown in Figure 4.
Figure 3. Schematic of the selective laser melting (adapted from Sidambe 2014)

163
Laser Additive Manufacturing
﻿
The LMD process is unique because it can be used to repair high valued component parts that were 
not possible or difficult to repair in the past (Bergan, 2011). More than one material can be used simul-
taneously in the LMD process that makes it possible for the production of functionally graded materials. 
LMD can be used to produce better coating to improve the surface property of material with minimum 
distortion, better surface quality, near net shape and fully dense. The components that are produced using 
LMD have good grain structure which in turns determines the mechanical properties of the component. 
The parts that were usually discarded in the past as a result of being not weldable can now readily be 
repaired using the LMD technology (Gasser et al., 2010). It enhances the thermal control in the component 
being built with its well-controlled heat affected zone (HAZ). For the repair of parts, LMD is a safe repair 
technology especially on critical contacting surfaces when compared to the traditional repair methods, 
such as welding, which are destructive in nature. An example of LMD process is Laser engineered net 
shaping (LENS). LENS was developed by the Sandia National Laboratory in the mid-1990s (Kumar and 
Stucker, 2005) and it uses high power laser to melt the metal powder particles by focusing the laser beam 
on a small spot at a time through one or more lenses (Hedges and Keicher, 2002). The molten material 
solidifies very quickly and thus results in fully dense component thereby eliminating the need for any 
heat-treatment after the processing (Mazumder at al., 1999). The LENS is a very important technology 
that is used to produce parts with better property control with the help of an inert gas to shield the melt 
pool which protects it from atmospheric oxygen. Hence, better surface wetting is achieved and with 
better layer adhesion. It is also possible to dynamically change material composition leading to produc-
tion of functionally graded parts (Liu and DuPont, 2003; Mahamood and Akinlabi, 2015a). Laser metal 
deposition process has also been used to deposit difficult to machine materials such as titanium and its 
alloy and was reported in the literature (Mahamood et al 2015b- Mahamood et al 2015f.)
Figure 4. The schematic diagram of the laser metal deposition process (Mahamood et al., 2014b)

164
Laser Additive Manufacturing
﻿
2.2.1. The Advantages of Laser Metal Deposition
The laser metal deposition process offers many advantages; and some of them are summarized as follows:
Laser metal deposition process allows a new part to be built on old or an existing parts (Song et al., 
2006), that are metallurgically bonded together. This provides a lot of flexibility for part designers; and 
modification can also be done on any existing design – without having to start from the scratch, thereby 
saving time, materials, and the overall cost of production. LMD can handle more than one material simul-
taneously; hence it can be used to produce parts with functionally graded composition (Qin et al., 2010). 
Also LMD can be used to repair worn-out parts, which were prohibitive in the past (Graf et al., 2012).
2.2.2. Limitations of the Laser Metal Deposition
Laser metal deposition is a relatively new technology like other additive manufacturing processes and 
the underlying physics is yet to be fully understood. For the process to be fully acceptable especially 
for manufacturing critical parts, the characteristics of the produced part must be predictable and as well 
controllable. More research is needed to fully establish this promising technology in order to be able to 
predict the properties of the part produced and also to be able to control the evolving properties.
2.2.3. Areas of Application of LMD Process
LMD is used for producing 3D complex part. It is also used for cladding and repair applications. Part 
made of functionally graded material can readily be made with LMD because of its flexibility in han-
dling multiple materials
2.3. Selective Laser Sintering (SLS)
The SLS process was first developed and patented in 1989 by Dr. Carl Deckaid at the University of 
Texas (Deckard, 1989). The SLS process was commercialized by DTM Corporation. The Selective 
Laser Sintering process is a laser additive manufacturing technique that uses a laser as the power source 
to fuse powdered material, and binding the material together to develop a 3D solid component directly 
from the 3D CAD model of the component. It is similar to selective laser melting process. The two are 
technology selectively fuse or melt powder in order to create the solid component. The selective laser 
melting fully melts the powder material instead of just sintering the powder material in the selective 
laser sintering process. The SLS uses the energy from laser to fuse or sinter the powdered material that 
has been spread on the substrate by following the laser path that is dictated by the 3D model of the 
component being produced to create a 3D solid component (Kruth et al., 2003). The laser selectively 
fuses the powdered material (polymer, metal or ceramic powder) by scanning the two dimensional (2D) 
cross-sections of the part that has been generated by the 3D geometrical description of the part on the 
surface of a powder bed, spread on the surface of the substrate. After each cross-section is scanned, the 
powder bed is lowered by one layer thickness, and a new layer of powder material is applied on top of 
the previously scanned layer, and the process is repeated until the building of the part is completed. The 
schematic of the selective laser sintering is shown in Figure 5.

165
Laser Additive Manufacturing
﻿
2.3.1. Advantages of SLS
One of the major advantages of SLS process is that it does not require any support structures. This is 
because the surrounding powder materials that are not scanned and fused provide the needed support for 
the part being built. This important characteristic makes it possible to make parts no matter the complex-
ity of these parts. The selective laser sintering process does not require support structures because part 
being built is surrounded by unsintered powder particles at all times and this allows for the construction 
of previously impossible geometries.
2.3.2 Disadvantages of SLS
The component made with selective laser sintering is porous and the part must be infiltrated to improve 
the property of the part through a secondary operation.
2.3.2.1. Areas of applications of SLS
SLS are used in the production of prototypes and functional end use parts. SLS can be used to produce 
functionally graded material because two materials can be used simultaneously on some SLS machines. 
The SLS can produce parts from a relatively wide range of commercially available powder materials. 
More than one part can be built simultaneously within the powder bed, resulting in a very high produc-
tivity. The application of SLS process is rapidly growing in art.
2.4. Stereolithography (SLA)
SLA was the first commercially available rapid prototyping machine that was developed and patented 
by Charles Hull of 3D Systems, Inc. in the USA (Hull, 1986). Initially this process was inaccurate and 
the choice of material was limited, so only prototypes are made with the process (Boboulos, 2011). The 
SLA uses laser energy to selectively scan and cure the liquid photo-sensitive polymer. The laser follows 
the geometry generated by the CAD model of the component being made. The process is similar to 
SLS and SLM in that the building platform is lowered by one layer thickness inside the photo-sensitive 
Figure 5. Schematics of selective laser sintering (Baourell 2011)

166
Laser Additive Manufacturing
﻿
polymer thank each time the laser scans the platform. The platform is lowered again and the laser scans 
and cures the polymer to trace the 2D cross-section of the part being built based on the CAD model. 
The step is repeated each time until the building of the part is completed. The schematic diagram of the 
SLA is shown in Figure 6. The only difference between the SLA and the SLS is that the material used 
in SLA is liquid while powder is used in the SLS process. After the building of the part is complete, the 
parts are immersed in a chemical bath in order to remove all the excess resins, and then the parts are 
then subsequently cured in an ultraviolet oven. The SLA process requires the use of support structures 
that serve to prevent deflection of the part being built as a result of gravity. The support structure also 
helps to hold the cross sections of the part in place so that they are balanced and are not disturbed by the 
pressure of the re-coater blade. The supports structures are normally generated automatically during the 
3D CAD models preparation. The supports are removed from the finished part manually.
2.4.1 Advantages of SLA Process
One of the major advantages of the SLA is that it is relatively faster than other laser additive manufac-
turing processes, so high productivity can be achieved with this process.
2.4.1.1. Disadvantages of SLA Process
The major disadvantage of SLA is the high cost of the stereolithography machines. The price is now 
coming down as a result of many manufacturers now producing several consumer model machines.
Figure 6. The schematic diagram of stereolithography (Lee 2001)

167
Laser Additive Manufacturing
﻿
2.4.2 Areas of Application of SLA
Prototypes that are produced by SLA can be used as patterns for injection molding and molds for other 
metal casting processes.
2.5. Laminated Object Manufacturing
The LOM process was developed by Helisys, Inc. in the USA in 1986 (Boboulos, 2011). The objects are 
formed by bonding adhesive coated sheet material together, the shape are then traced by a laser optics 
system to cut out the traced shape. At the invention of the technology, the material used was paper, but 
now different materials are now being used and more are being developed also by the Helisys, Inc. the 
building of part is achieved by feeding the materials into the machine from a feed roller. The materials 
are fed on to the stacks that were previously produced during the process and the new fed materials are 
bonded to the previous layer with the aid of a heated roller which melts the plastic coating on the mate-
rial. The laser beam is then used to trace out the desired part as dictated by the 3D CAD model profile 
of the part being built. The take-up roll removes the excess material from the building platform. The 
steps are repeated until the building of the part is completed. There is no need of any support structure 
for overhang and under cut. The schematic diagram of the LOM is shown in Figure 7.
2.5.1. Advantages of LOM Process
The main advantages of the LOM process are as follows: - The raw materials are cheap and are read-
ily available. The models produced from paper material are very strong and looks like wood. It can be 
worked and finished as desired.
2.5.2. Disadvantages of LOM Process
The dimensional accuracy of part produce using LOM is poor when compared to other LAM processes.
Figure 7. Schematics of Laminated object manufacturing

168
Laser Additive Manufacturing
﻿
2.5.3. Areas of Application of LOM
LOM is to make pattern for sand casting, architectural Modelling, etc.
3. CONCLUSION
One of the driving forces in laser additive manufacturing process is the aerospace industry. Produc-
ing highly complex part using the traditional manufacturing route is material wasting. The ratio of the 
percentage of raw material to the percentage of the finish product is very high when complex parts are 
produced through the traditional manufacturing processes. This is often referred to as buy-to-fly ratio 
in the aerospace industry. Laser additive manufacturing is a promising technology for the production 
of aerospace parts that will help to reduce the buy to fly ratio because, as against material removal for 
shaping objects in the traditional manufacturing processes, laser additive manufacturing produce material 
simply by adding materials layer after layer. Some of the commercialized laser additive manufacturing 
processes are presented in this chapter. Their processes are described their advantages, disadvantages, 
and areas of applications are also presented.
4. FUTURE RESEARCH DIRECTIONS
Laser additive manufacturing is an evolutionary manufacturing technology that allows product to be 
design for functionality as against design for manufacturing which is the case with the traditional manu-
facturing processes. This technology is still fairly new and some of the underlying principles of these 
technologies are yet to be fully understood. The technology is a highly promising technology and it is 
capable of reducing the carbon foot print in all our transportation industries by reducing the net weight 
of all the moving parts. There is need for more research in this area to better understand the physics 
of this technology so as to be able to effectively control the achievable properties of the parts that are 
produced. There is need to further develop the raw materials used by this process so as to reduce the 
overall cost of production of the processes.
ACKNOWLEDGMENT
This work is supported by the Rental Pool Programme of National Laser Centre, Council of Scientific 
and Industrial Research Pretoria, South Africa and L’Oreal-UNESCO for Women in Science.
REFERENCES
Ambroseo, J. (2001). Coherent Laser. In The Photonics Design and Applications Handbook. Academic 
Press.

169
Laser Additive Manufacturing
﻿
Baourell, D. (2011). Webpage. Retrieved from http://www.me.utexas.edu/news/2011/1011_bourell_fame.
php
Bauerele, D. (2011). Laser processing and chemistry. Springer. doi:10.1007/978-3-642-17613-5
Bergan, P. Implementation of laser repair processes for navy aluminum components. Proceeding of 
Diminishing Manufacturing Sources and Material Shortages Conference.
Berkmanns, J., & Faerber, M. (2010). Laser basics, BOC. Available from: https://boc.com.au/boc_sp/
downloads/gas_brochures/BOC_216121_Laser%20Basics_v7.pdf
Boboulos, M. A. (2011). CAD-CAM & rapid prototyping application evaluation. Retrieved 21st August 
2014, from www.bookBoom.com
Brandl, E., Michailov, V., Viehweger, B., & Leyens, C. (2011). Deposition of Ti–Al–4V using laser 
and wire, part I: Microstructural properties of single beads. Surface and Coatings Technology, 206(6), 
1120–1129. doi:10.1016/j.surfcoat.2011.07.095
Brody, H. D. (1986). Segregation and structure in the weld zone.Proceedings of Advances in Welding 
Science and Technology.
Chen, Y., & Wang, H. M. (2003). Growth morphology and mechanism of primary TiC carbide in laser clad 
TiC/FeAl composite coating. Materials Letters, 57(5-6), 1233–1238. doi:10.1016/S0167-577X(02)00964-3
Deckard, C. (1989). Methods and Apparatus for Producing Parts by Selective Laser Sintering. US Pat-
ent 4863538.
Gasser, A., Backes, G., Kelbassa, I., Weisheit, A., & Wissenbach, K. (2010). Laser additive manufac-
turing: Laser metal deposition (LMD) and selective laser melting (SLM) in turbo-engine applications. 
Laser Technik Journal, 7(2), 58–63. doi:10.1002/latj.201090029
Graf, B., Gumenyuk, A., & Rethmeier, M. (2012). Laser metal deposition as repair technology for 
stainless steel and Titanium alloys. Physics Procedia, 39, 376–381. doi:10.1016/j.phpro.2012.10.051
Haken, H. (1983). Laser Theory. Springer Berlin Heidelberg. doi:10.1007/978-3-642-45556-8
Hedges, M., & Keicher, D. M. (2002). Laser engineered net shaping - Technology and applications. In 
3rd National Conference on Rapid Prototyping. Rapid Tooling and Rapid Manufacturing.
Hügel, H., & Dausinger, F. (1996). Interaction phenomena and energy coupling in laser surface treat-
ment. University of Stuttgart.
Hügel, H., Dausinger, F., Bloehs, B., & Grunenwald, B. (1994). Basic coupling mechanisms in laser 
surface treatment.Proceedings of the NATO Advanced Study Institute on Laser Processing: Surface 
Treatment and Film Deposition. Kluwer Academic Press.
Hull, C. W. (1986). Apparatus for Prodtiction of Three-dimensional Objects by Stereolithography. US 
Patent 4575330.
Kobryn, P. A., Moore, E. H., & Semiatin, S. L. (2000). The Effect of Laser Power and Traverse Speed 
on Microstructure, Porosity and Build Height in Laser-Deposited Ti-6Al-4V. Scripta Materialia, 43(4), 
299–305. doi:10.1016/S1359-6462(00)00408-5

170
Laser Additive Manufacturing
﻿
Kruth, J.-R. Mercelis P. Vaerenbergh J.V., Froyen L. & Mrombouts M. Advances in selective laser sin-
tering. In International Conference on Advanced Research in Virtual and Rapid Prototyping.
Kumar, S., & Stucker, B. Development of a Co-Ci-Mo to tantalum transition using LENS for orthopaedic 
applications. Solid Freeform Fabrication Symposium.
Lee, J. H. (2001). Ceramic/polymer Composite Materials through Stereolithography. Ceramic Materi-
als Laboratory. Retrieved from https://www.princeton.edu/~cml/html/research/stereolithography.html
Liseykina, T. V., & Bauer, D. (2012). Plasma formation dynamics in intense laser-droplet interaction. 
Available from: http://arxiv.org/pdf/1209.5948v3.pdf
Liu, W., & DuPont, J. N. (2003). Fabrication of functionally graded TiC/Ti composites by laser engineered 
net shaping. Scripta Materialia, 48(9), 1337–1342. doi:10.1016/S1359-6462(03)00020-4
Mahamood, R. M., & Akinlabi, E. T. (2015a), Laser metal deposition of functionally graded Ti6Al4V/
TiC. Materials & Design, 84, 402-410. 10.1016/j.matdes.2015.06.135
Mahamood, R. M., & Akinlabi, E. T. (2015b). Effect of Processing Parameters on Wear Resistance Prop-
erty of Laser Material Deposited Titanium -Alloy Composite. Journal of Optoelectronics and Advanced 
Materials, 17(9-10), 1348–1360.
Mahamood, R. M., & Akinlabi, E. T. (2015c). Process Parameters Optimization for Material Deposition 
Efficiency in Laser Metal Deposited Titanium Alloy. Lasers in Manufacturing and Materials Processing. 
doi:10.1007/s40516-015-0020-5
Mahamood, R. M., & Akinlabi, E. T. (2015d). Effect of laser power and powder flow rate on the wear 
resistance behaviour of laser metal deposited TiC/Ti6Al4V composites. Materials Today: Proceedings, 
2(4–5), 2679–2686. doi:10.1016/j.matpr.2015.07.233
Mahamood, R. M., & Akinlabi, E. T. (2015e). Influence on degree of porosity in laser metal deposition 
process. Transactions on Engineering Technologies, 31-42.
Mahamood, R. M., & Akinlabi, E. T. (2015f). Functionally graded material produced using laser metal 
deposition process. In Additive manufacturing: Emerging Technologies, Applications, and Economic 
Implications. Thornton.
Mahamood, R. M., Akinlabi, E. T., Shukla, M., & Pityana, S. (2014a). Evolutionary additive manufac-
turing: An overview. Lasers in Engineering, 27, 161–178.
Mahamood, R. M., Akinlabi, E. T., Shukla, M., & Pityana, S. (2014b). Characterization of Laser De-
posited Ti6A4V/TiC Composite. Lasers in Engineering, 29(3-4), 197–213.
Mazumder, J., Schifferer, A., & Choi, J. (1999). Direct materials deposition: Designed macro and mi-
crostructure. Materials Research Innovations, 3(3), 118–13L. doi:10.1007/s100190050137
Niu, H. J., & Chang, I. T. H. (2000). Selective laser sintering of gas atomized M2 high speed steel 
powder. Journal of Materials Science, 35(1), 31–38. doi:10.1023/A:1004720011671 PMID:15348096
Paschotta, R. (2008). Encyclopedia of Laser Physics and Technology. Berlin: Wiley-VCH.

171
Laser Additive Manufacturing
﻿
Powell, J. (1993). CO2 laser cutting. Berlin: Springer. doi:10.1007/978-1-4471-3384-1
Qin, L. Y., Yang, G., Wang, W., Tong, M., & Wang, X. L. (2010). Application of Laser Metal Deposi-
tion for Fabrication of Titanium Matrix Wear-Resistant Coating and its Wearing Performance. Applied 
Mechanics and Materials, 44-47, 316–320. doi:10.4028/www.scientific.net/AMM.44-47.316
Scott, J., Gupta, N., Wember, C., Newsom, S., Wohlers, T., & Caffrey, T. (2012). Additive manufactur-
ing: status and opportunities, Science and Technology Policy Institute. Available from: https://www.ida.
org/stpi/occasionalpapers/papers/AM3D_33012_Final.pdf
Sidambe, A. T. (2014). Biocompatibility of Advanced Manufactured Titanium Implants—A Review. . 
. Materials (Basel), 7(12), 8168–8188. doi:10.3390/ma7128168
Siegman, A. E. (1986). Lasers. University Science Books.
Silfvast, W. T. (1996). Laser Fundamentals. Cambridge University Press.
Song, J., Deng, Q., Chen, C., Hu, D., & Li, Y. (2006). Rebuilding of metal components with laser clad-
ding forming. Applied Surface Science, 252(22), 7934–7940. doi:10.1016/j.apsusc.2005.10.025
Steen, W. M. (1998). Laser material processing (2nd ed.). London: Springer- Verlag. doi:10.1007/978-
1-4471-3609-5
Wang, X., & Kruth, J.-P. (2000). Energy absorption and penetration in selective laser sintering: a ray 
tracing model. Proceedings of International Conference on Mathematical Modelling and Simulation of 
Metal Technologies. MMT.
Wikipedia. (2013). List of laser types. Available from: http://en.wikipedia.org/wiki/List_of_laser_types
Wu, X., Liang, J., Mei, J., Mitchell, C., Goodwin, P. S., & Voice, W. (2004). Microstructures of laser-
deposited Ti–6Al–4V. Materials & Design, 25(2), 137–144. doi:10.1016/j.matdes.2003.09.009
Yamashita, K., Taniguchi, H., Yuyama, S., Oe, K., Sun, J., & Mataki, H. (2007). Continuous-wave 
simulated emission and optical amplification in europium (III)-aluminum nanocluster-doped polymeric 
waveguide. Applied Physics Letters, 91(8), 081115–081117. doi:10.1063/1.2773933
This work was previously published in Advanced Manufacturing Techniques Using Laser Material Processing edited by Esther 
Titilayo Akinlabi, Rasheedat Modupe Mahamood, and Stephen Akinwale Akinlabi, pages 1-23, copyright year 2016 by Engi-
neering Science Reference (an imprint of IGI Global).

172
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  9
DOI: 10.4018/978-1-5225-1677-4.ch009
ABSTRACT
Laser metal deposition process belongs to the directed energy deposition class of additive manufactur-
ing process that is capable of producing highly complex part directly from the three dimensional (3D) 
computer aided design file of the component by adding materials layer after layers. Laser metal depo-
sition process is a very important additive manufacturing process and it is the only class of additive 
manufacturing process that can be used to repair valued component parts which were not repairable 
in the past. Also because this additive manufacturing process can handle multiple materials simulta-
neously, it is used to produce part with functionally graded material. Some of the features of the laser 
metal deposition process are described in this chapter. Some experimental studies on the laser metal 
deposition of Titanium alloy- composite are also presented.
1. INTRODUCTION
Laser Metal Deposition (LMD) is an additive manufacturing process that uses laser beam to create a melt 
pool on the surface of a metallic substrate and powder particle or wire is fed into the melt pool created. 
The powder or wire melts in the process to form a deposit that is metallurgically bonded to the substrate. 
The required shape is built up layer after layer according to the geometry of the two dimensional (2D) 
cross section of the part from the three dimensional computer aided design (CAD) model of the part 
(Scott et al., 2012). Laser metal deposition process is an important additive manufacturing process that 
was grouped into the class of Directed Energy Deposition by the F42 committee on additive manufac-
turing standards (Scott et al., 2012). Laser metal deposition process has a unique characteristics such as 
reduction in material wastage during the manufacturing process, repair of high valued parts which were 
in the past costly to replace or difficult to repair, and deposition of functionally graded material (Zang 
et al., 2008). Difficult to machine engineering materials such as titanium and its alloys are easily formed 
using the laser metal deposition process.
Titanium alloy Ti6Al4V is an important aerospace alloy and it is the most widely used titanium alloy 
that is referred to as the workhorse of the industry (Ramesh et al., 2008; Cui et al., 2012). Ti6Al4V pos-
Laser Metal Deposition Process
Rasheedat M. Mahamood
University of Johannesburg, South Africa & University of Ilorin, Nigeria

173
Laser Metal Deposition Process
﻿
sess some exciting properties such as high strength to weight ratio, good corrosion resistance, retaining 
of properties even at elevated temperature and bio-compatibility which makes them to be more favoured 
in most field of human endeavor (Ribeiro et al., 2003; Lütjering and Williams, 2003). Despite all these 
exciting properties, titanium and its alloys are difficult to machine because they chemically react with 
the cutting tool material thereby causing high temperature and galling of the cutting tool (Arrazola et 
al., 2009). In the aerospace industry, manufacturing of complex part is material wasting when produce 
through the traditional manufacturing process which results in the typical high buy-to-fly ratio of the 
aerospace parts (Brandl et al. 2011). All of these challenges can be overcome if the complex aerospace 
parts are manufactured through the laser metal deposition process. The development of the part us-
ing the laser metal deposition process is achieved by adding materials layer by layer directly from the 
CAD model of the part no matter the complexity resulting in improvement of the buy-to-fly ratio. Also 
producing part made of titanium and its alloy through laser metal deposition process will overcome the 
problem of tool reacting with the workpiece since the LMD process is a tool-less process. The features 
of the laser metal deposition process are described in this chapter and how the processing parameters 
influence the properties of the deposited part. Also some experimental studies on the laser metal depo-
sition of titanium alloy composite are presented in this chapter. The laser metal deposition process is 
described in the following sub-section.
2. THE LASER METAL DEPOSITION PROCESS
Typical laser metal deposition process equipment consists of a laser system with optical laser beam for 
focusing, a powder feeding system (nozzles) and a control system. The Laser engineering net shaping 
(LENS) is an example of machine that is based on the laser metal deposition process. There are five (5) 
basic steps involved in the laser metal deposition process like any additive manufacturing process; the 
steps are explained as follows:
Firstly, the component to be made must be drawn using 3D software such as AutoCAD, solid work, 
Uni-graphics and Pro Engineer. This CAD model is sent into the LENS. The CAD model file received 
is converted into a standard triangulation language (STL) which has now been termed as an Additive 
Manufacturing File (AMF) according to the F42 committee on additive manufacturing standards (Scott 
et al., 2012). The old file format –STL is not capable of defining some characteristics that are now 
present in the new AMF format. The AMF is based on an open standard Extension Mark-up Language 
(XML) (Scott et al., 2012). The AMF format is capable of describing in detail, the texture, the colour, 
the curve triangles, and the lattice structure, as well as the functionally graded materials. The AMF 
format represents the 3-D surface assembly of planar and curved triangles containing the co-ordinates 
of the vertices of these triangles. The third step after the conversion process is the slicing of the AMF 
into two dimensional (2-D) profile sections that is defined by the geometry of the CAD model and the 
chosen build orientation. The building orientation will affect the way the AMF file is sliced. The building 
orientation is the direction with which the building process will follow, for example, from the bottom 
to the top, from one side to another side etc. The software may choose the building orientation, or the 
operator may choose the building orientation. Support structures may are also generated automatically 
if necessary (Boboulus, 2010). The slicing process is a very critical part of the whole process because 
it determines the dimensional accuracy of the product. After the slicing is completed, the fourth step is 
the building of the part. The building process in LMD is achieved by creating a melt pool on the surface 

174
Laser Metal Deposition Process
﻿
of the substrate by the focusing laser beam and the materials are delivered into the melt pool. The laser 
beam follows the direction that was generated by the sliced layers to create a solid mass representing 
the 2-D section of the 3-D model. The step is repeated layer by layer until the building of the part is 
completed. The schematic diagram of the laser-material deposition process is shown in Figure 1.
The last step after the building process is completed is the removal of the part from the machine and 
the cleaning up of the part produced. The support structures are removed and any required finishing 
operations are performed. Heat treatment can also be performed, depending on the service requirement 
of the part. The flow chart of the steps in the laser metal deposition process is shown in Figure 2.
The whole process takes place inside an enclosed chamber that is filled with inert gas so as to minimize 
oxidation of both the powdered material as well as the deposited part. A number of research activities 
has taken place on the laser metal deposition process in the literature and it has been established that 
the processing parameters are of great influence on properties of deposited parts materials (Kobryn 
et al, 2000; Bontha, 2006; Brandl et al., 2011; Brandl et al. 2012; Wu et al., 2004). These processing 
parameters are discussed in the next sub-section.
3. EFFECT OF PROCESSING PARAMETERS IN THE 
LASER METAL DEPOSITED MATERIALS
Researches have shown that the processing parameters have a very strong influence on the resulting 
properties of the laser metal deposited material or part and there are strong interactions among these 
processing parameters according to Choi and Chang, (2005). Some of the key processing parameters in 
the laser material deposition process are explained in this section. They are: the laser power, the scanning 
speed, the powder or wire flow rate, the gas flow rate, the laser beam diameter, and the overlap percentage.
Figure 1. Schematic diagram of LMD process (Mahamood et al., 2013a)

175
Laser Metal Deposition Process
﻿
3.1. Laser Power
Laser power is one of the important processing parameters in the laser metal deposition process. The 
amount of laser energy that is available on the surface of material being processed depends on the maxi-
mum laser power of the machine. The Laser power has been found to influence the material properties 
ranging from the physical properties to the microstructural characteristics (Brandl et al., 2011; Kobryn 
et al., 2000; Mahamood et al., 2013b). If the laser power is high, it can result in a high dilution rate be-
tween the deposited materials and the substrate materials or the preceding layer. Too high a laser power 
can even cause the material to evaporate. These are not desirable in the LMD process. It will affect 
the dimensional accuracy of the part being built amongst other things. Low laser power could result in 
improper melting of the materials thereby resulting in porosity, lack of fusion of the deposited material 
with the substrate, or it can even result in no melting of the material at all. It is important to establish 
the optimum laser power for the intended application amidst other processing parameters because these 
processing parameters interact greatly.
Figure 2. Flow chart of steps in AM (based on Mahamood et al., 2014)

176
Laser Metal Deposition Process
﻿
3.2. Scanning Velocity
The scanning velocity is the speed at which the laser beam is made to interact with the materials being 
processed. The scanning velocity is achieved by either moving the laser head against the fixed substrate, 
or by moving the substrate against the fixed laser head. Whichever way this movement is achieved, it 
determines the length of time that the materials interact with the laser energy. The scanning velocity 
has also been reported to affect the microstructural properties of the laser deposited materials (Akinlabi 
et al., 2012; Kobryn et al, 2000b; Mahamood et al., 2013a). If the scanning velocity is low, the materi-
als interact longer with the laser beam. Too low a scanning velocity, depending on the available laser 
power, could result in high dilution rate or even evaporation of the materials being processed. Also if 
the scanning velocity is too high, the laser material interaction time will be too small and this could lead 
to incomplete melting of the materials or no melting of the material. It can be seen that there is a strong 
relationship between the laser power and the scanning velocity, although the relationship has an inverse 
effect on the property of the deposited part. This interaction leads to the laser energy density, which is 
given in equation 1, according to Sentikumara, (2009) as:
E (J/mm2) = p/dv 	
(1)
where: E is the laser energy density; p is the laser power (W); v is the scanning velocity (mm/s); and d 
is the laser-beam diameter (mm).
It may be seen from Equation 1 that, the energy density is directly proportional to the laser power 
and it is inversely proportional to the scanning velocity and the beam diameter. The energy density can 
be increased either by increasing the laser power, or by reducing the scanning velocity, or by reducing 
the laser beam diameter.
3.3. Laser Beam Diameter
The laser beam diameter is also referred to as the laser spot size. It is the width of the laser beam measured 
at a given focal distance. It is measured at a plane that is perpendicular to the laser beam axis and it is 
measured in millimeters. The laser beam diameter is inversely proportional to the laser energy density. 
Meaning that the smaller the laser beam diameter, the larger the laser energy density (Sentikumara, 2009).
3.4. Powder or Wire Flow Rate
The powder or the wire flow rate is the amount of material in grams that is leaving the nozzle or the wire 
feeder in a unit time. The material flow rate also has a large effect on the physical, the metallurgical, the 
chemical and the mechanical properties of the final deposited sample or part, and it has a great influence 
on the overall economy of the LMD process (Brandl et al., 2011; Wu et al., 2004; Shukla et al., 2012). If 
the material flow rate is too high, depending on the quantity of the available energy density, then most 
of the material may not be melted because the available energy density may not be sufficient to be able 
to fully melt the material. This may in turn result in material wastage. It is important to establish the 
optimal material flow rate in LMD, in order to achieve a deposit with the required properties, and with 
high material efficiency utilization.

177
Laser Metal Deposition Process
﻿
3.5. Gas Flow Rate
The gas flow rate is the powder carrier gas flow rate. The powder is delivered through the carrier gas 
and it is also responsible for protecting the powder from environmental degradation. The gas flow rate 
has also been found to affect the properties of the deposited part from some studies conducted in the 
literature (Bi et al., 2006; Pityana et al., 2013). The gas flow rate should not be too low, in order to 
provide better protection for the powder; and neither should it be too high, so as not to blow the powder 
away from the melt pool.
3.6. Overlap Percentage
The overlap percentage is the percentage of the preceding track that is covered by the succeeding track. 
There is need for the tracks to overlap because the shape of a single track is dome-shaped and a certain 
overlap percentage is necessary to prevent porosity in the part being built, (Schneider, 1998). It is impor-
tant to have the right combination of parameters in order to achieve the desired part properties. The next 
section presents characterization of laser metal deposition of Titanium alloy composite for improving 
the wear resistance properties of titanium alloy.
4. LASER METAL DEPOSITION OF TITANIUM ALLOY 
AND TITANIUM ALLOY COMPOSITE
Titanium and its alloys are of great importance in engineering applications because of their exciting 
properties that include: high corrosion resistance, high strength-to-weight ratio, and their ability to retain 
these properties at elevated temperature (Ramesh et al., 2008). Of all the titanium and titanium alloys, 
Ti6Al4V is the most widely produced and the most widely used Titanium alloy because of its unique 
light weight with the high strength and they are structurally efficient and useful in critical and high 
performance applications such as jet engine parts and air frame components (Lu et al., 2012). Despite 
all these exciting properties of the Ti6Al4V, they are difficult to machine because of the way they reacts 
chemically with the cutting tool materials during the cutting operation. Which leads to generation of heat 
that often results in eventual galling of the cutting tool (Wang and Ezugwu 1997). Laser metal deposition 
process is an ideal alternative manufacturing process that can be used to produce parts with difficult 
to machine materials such as titanium and its alloys because it is a tool-less manufacturing process. A 
number of research works on laser metal deposition of titanium alloy have appeared in the literature such 
as Brandl et al., 2011; Brandl et al., 2012; Lu et al 2012; Mahamood and Akinlabi, 2015b-Mahamood 
and Akinlabi 2015f; Mahamood et al., 2014a and Mahamood et al., 2014b. Wu et al.,
Brandl et al., 2011, studied the effects of the laser power, the scanning speed, and the wire-feed rate 
on the resulting microstructures. The microstructure was correlated with these process parameters and 
revealed fundamental microstructure of the laser deposited Ti6-Al-4V. Lu et al., 2012, studied the effect 
of the annealing temperature and the annealing time on the microstructure of the deposited Ti–6Al– 4V. 
A unique bi-modal microstructure consisting of coarse primary alpha and fine lamellar transformed beta 
was observed in this study. Wu et al., 2004 investigated the effects of laser power, scan speed, and powder 
feed rate on the microstructure of the deposited Ti–6Al–4V and it was discovered that the deposited Ti-
6Al-4V has a long columnar grain structures that dominate the microstructures especially at high laser 

178
Laser Metal Deposition Process
﻿
power settings used in the study. They also found that the degree of the columnar grain structure also 
increases with reducing scanning speed with other parameters kept constant.
Some of the studies conducted on the laser metal deposition process are discussed in the next sub-
section.
4.1. Characterization of Laser Metal Deposited Titanium Alloy Composites
Despite the exciting properties of titanium alloys, the wear resistance performance is very poor as a 
result of the chemical behaviour of titanium which makes it to react with any surface it comes in con-
tact with. A number of researches on the laser metal deposition of Ti6Al4V have been reported in the 
literature. Some of these works include those of: Obiolodan and Strucker, (2012). The authors used the 
laser metal deposition process to produce composites of 10 and 5w% TiC/Ti6Al4V composite. The study 
revealed that the surface property of the Ti6Al4V was improved with the addition of the TiC/Ti6Al4V 
composites. Popoola et al. (2013) studied the effect of TiC addition on the TiC/Ti6Al4V composite us-
ing different TiC compositional ratio. They were able to establish that the TiC/Ti6Al4V composite was 
able to improve the wear resistance behaviour of the Ti6Al4V. Wang et al. (2007a) also deposited TiC/
Ti6Al4V composite at different TiC compositions to establish the optimum TiC percentage that resulted 
in improved properties of the titanium alloy. Ochonogor et al., 2012 studied the effect of the TiC ratio 
on the wear resistance performance of Ti/TiC composite, using laser metal deposition process. Some 
authors have also studied the production of functionally graded Ti6Al4V/TiC composite using the laser 
metal deposition process (Mahamood and Akinlabi, 2015a; Obiolodan and Strucker, 2012; Wang et 
al., 2007b; Zang et al., 2008). The importance of producing functionally graded parts directly from the 
3-D CAD model in one single step in laser metal deposition process has been the driving force for the 
research interest in functionally graded materials (Foroozmehr et al., 2009). The earlier research works 
were basically on trying to establish the feasibility of making the functionally graded materials through 
the LMD process (Balla et al., 2009; Thivillon et al., 2009). In some other works, the functionally graded 
materials were built using the LMD process, and then characterized the samples produced. In a study 
conducted by Zang et al. (2008), they deposited functionally graded Ti/TiC on a Ti6Al4V substrate. 
They first established the processing parameters for various volume fractions of the Ti/TiC composite 
in their preliminary works. They used the results from their preliminary work to successfully deposit a 
thin wall of functionally graded material by adjusting the processing parameters during the deposition 
process. They showed that the wear resistance performance of the Ti6Al4V substrate was greatly im-
proved with the addition of the TiC. They also showed that functionally graded materials be produced 
with the LMD without a discrete interface. In another study performed by Wang et al. (2007b), a func-
tionally graded material of Ti6Al4V/TiC was deposited using the LMD process. They used Ti6Al4V 
wire and TiC powder; and the two materials were fed simultaneously. They achieved the compositional 
grading by keeping the wire feed rate of Ti6Al4V constant; while they varied the TiC powder feed rate. 
They kept other processing parameters constant. Liu and DuPont (2003) also successfully deposited 
functionally graded material of Ti/TiC composite using the LMD. They relied on the controller in the 
LENS to monitors the melt pool area and control the laser power to achieve a constant melt pool area, 
for the deposition of their functionally graded material. The melt pool area control in LENS is intended 
to control the dimensional accuracy in the deposited part. Shah, (2011) deposited functionally graded 
material of Inconel 718 Nickel alloy and Ti6Al4V using the laser metal deposition process. The effect 
of the laser pulse parameters and the powder flow rate on the residual stress was studied. The study 

179
Laser Metal Deposition Process
﻿
found that the layer thickness plays an important role in the crack behaviour of the functionally graded 
material produced. The effect of the powder flow rate on the melt pool size was also studied; and the 
study showed that by increasing the powder flow rate, the melt pool size was found to increase. Lin et al. 
(2005) used the laser metal deposition process to produce functionally graded material of stainless steel-
SS316L/super alloy-Rene88DT. They investigated the solidification behaviour and the microstructural 
evolution of the functionally graded material they produced. Epitaxial growth and columnar dendrites 
microstructure were also observed. Qin et al. (2011) produced in situ functionally graded TiC reinforced 
titanium matrix from Ti and Cr3C2 powder using the laser metal deposition process. The functionally 
graded material was achieved by changing the powder flow rate of the Ti and the Cr3C2 powder. The 
microhardness and the wear-resistance properties of the deposited samples were studied. The micro-
hardness and the wear resistance were found to be greatly improved with the addition of the Cr3C2. The 
problem with the functionally graded material produced in situ is that the magnitude of the reinforcement 
achieved will largely depend on the reactions taking place during the deposition and cooling process. It 
would be very difficult to achieve a desired percentage ratio of the reinforcement and the matrix. Also, 
there is no doubt that different Ti6Al4V/TiC ratios would have different optimal process parameters.
5. CONCLUSION
The laser metal deposition process has been described in this chapter. The capabilities of the technology 
in the production of functionally graded materials were also highlighted. The laser metal deposition pro-
cess is an important additive manufacturing process that is capable of repairing high valued component 
parts which were not repairable or prohibitive to repair in the past. This capability has helped to extent 
the service life of many machine parts. Also the ability to produce part that is made of functionally 
graded material using the laser metal deposition process is another important capability of the laser 
metal deposition process which was also presented in this chapter. A lot is needed to be done in term 
of research to further understand this process in order to position the technology for the production of 
critical parts in the aerospace industry which will further help to reduce the buy-to-fly ratio and help 
to reduce the carbon foot print of this industry. This can be made possible if critical complex parts are 
produced using the laser metal deposition process, thereby reducing the overall weight of the aircraft 
through the elimination of some joining processes by producing complex parts as a single piece part.
ACKNOWLEDGMENT
This work is supported by the Rental Pool Programme of National Laser Centre, Council of Scientific 
and Industrial Research Pretoria, South Africa and L’Oreal-UNESCO for Women in Science.
REFERENCES
Akinlabi, E. T., Mahamood, R. M., Shukla, M., & Pityana, S. (2012). Effect of Scanning Speed on 
Material Efficiency of Laser Metal Deposited Ti6Al4V. World Academy of Science and Technology, 
Paris, 6, 58–62.

180
Laser Metal Deposition Process
﻿
Arrazola, P. J., Garay, A., Iriarte, L. M., Armendia, M., Marya, S., & Le Maître, F. (2009). Machin-
ability of Titanium alloys (Ti6Al4V and Ti555.3). Journal of Materials Processing Technology, 209(5), 
2223–2230. doi:10.1016/j.jmatprotec.2008.06.020
Bi, G., Gasser, A., Wissenbach, K., Drenker, A., & Poprawe, R. (2006). Characterization of the process 
control for the direct laser metallic powder deposition. Surface and Coatings Technology, 6(6), 2676–2683. 
doi:10.1016/j.surfcoat.2006.05.006
Boboulos, M. A. (2010). CAD-CAM & rapid prototyping application evaluation. PhD & Ventus publish-
ing Aps, Available from: www.bookBoom.com
Bontha, S. (2006). The effect of process variables on microstructure in laser-deposited materials. (PhD 
thesis). Mechanical Engineering, Wright State University.
Brandl, E., Michailov, V., Viehweger, B., & Leyens, C. (2011). Deposition of Ti–Al–4V using laser 
and wire, part I: Microstructural properties of single beads. Surface and Coatings Technology, 206(6), 
1120–1129. doi:10.1016/j.surfcoat.2011.07.095
Brandl, E., Schoberth, A., & Leyens, C. (2012). Morphology, microstructure, and hardness of Titanium 
(Ti-6Al-4V) blocks deposited by wire-feed additive layer manufacturing (ALM). Materials Science and 
Engineering A, 532, 295–307. doi:10.1016/j.msea.2011.10.095
Choi, J., & Chang, Y. (2005). Characteristics of laser aided direct metal/material deposition process for 
tool steel. International Journal of Machine Tools & Manufacture, 45(4-5), 597–607. doi:10.1016/j.
ijmachtools.2004.08.014
Cui, Z. D., Zhu, S. L., Man, H. C., & Yang, X. J. (2005). Microstructure and wear performance of gradi-
ent Ti/TiN metal matrix composite coating synthesized using a gas nitriding technology. Surface and 
Coatings Technology, 190(2-3), 309–313. doi:10.1016/j.surfcoat.2004.02.012
Donachie, M. J. (2000). Titanium—A technical guide (2nd ed.). Academic Press.
Kobryn, P. A., Moore, E. H., & Semiatin, S. L. (2000). The Effect of Laser Power and Traverse Speed 
on Microstructure, Porosity and Build Height in Laser-Deposited Ti-6Al-4V. Scripta Materialia, 43(4), 
299–305. doi:10.1016/S1359-6462(00)00408-5
Lin, X., Yue, T. M., Yang, H. O., & Huang, W. D. (2005). Laser rapid forming of SS316L/Rene88DT graded 
material. Materials Science and Engineering A, 391(1–2), 325–336. doi:10.1016/j.msea.2004.08.072
Liu, W., & DuPont, J. N. (2003). Fabrication of functionally graded TiC/Ti composite by laser engineered 
net shaping. Scripta Materialia, 48(9), 1337–1342. doi:10.1016/S1359-6462(03)00020-4
Lu, Y., Tang, H. B., Fang, Y. L., Liu, D., & Wang, H. M. (2012). Microstructure evolution of sub-
critical annealed laser deposited Ti–6Al–4V alloy. Materials & Design, 37, 56–63. doi:10.1016/j.
matdes.2011.12.016
Lütjering, G., & Williams, J. C. (2003). Titanium. Berlin, Germany: Springer. doi:10.1007/978-3-540-
71398-2

181
Laser Metal Deposition Process
﻿
Mahamood, R. M., & Akinlabi, E. T. (2015a), Laser metal deposition of functionally graded Ti6Al4V/
TiC. Materials & Design, 84, 402-410. 10.1016/j.matdes.2015.06.135
Mahamood, R. M., & Akinlabi, E. T. (2015b). Effect of Processing Parameters on Wear Resistance Prop-
erty of Laser Material Deposited Titanium -Alloy Composite. Journal of Optoelectronics and Advanced 
Materials, 17(9-10), 1348–1360.
Mahamood, R. M., & Akinlabi, E. T. (2015c). Process Parameters Optimization for Material Deposition 
Efficiency in Laser Metal Deposited Titanium Alloy. Lasers in Manufacturing and Materials Processing. 
doi:10.1007/s40516-015-0020-5
Mahamood, R. M., & Akinlabi, E. T. (2015d). Effect of laser power and powder flow rate on the wear 
resistance behaviour of laser metal deposited TiC/Ti6Al4V composites. Materials Today: Proceedings, 
2(4–5), 2679–2686. doi:10.1016/j.matpr.2015.07.233
Mahamood, R. M., & Akinlabi, E. T. (2015e). Influence on degree of porosity in laser metal deposition 
process. Transactions on Engineering Technologies, 31-42.
Mahamood, R. M., & Akinlabi, E. T. (2015f). Functionally graded material produced using laser metal 
deposition process. In Additive manufacturing: Emerging Technologies, Applications, and Economic 
Implications. Thornton.
Mahamood, R. M., Akinlabi, E. T., Shukla, M., & Pityana, S. (2013a). Scanning velocity influence on 
microstructure, microhardness and wear resistance performance on laser deposited Ti6Al4V/TiC com-
posite. Materials & Design, 50, 656–666. doi:10.1016/j.matdes.2013.03.049
Mahamood, R. M., Akinlabi, E. T., Shukla, M., & Pityana, S. (2013b). Laser metal deposition of Ti6Al4V: 
A study on the effect of laser power on microstructure and microhardness.International Multi conference 
of Engineering and Computer Science (IMECS 2013).
Mahamood, R. M., Akinlabi, E. T., Shukla, M., & Pityana, S. (2014a). Improving Surface Integrity using 
Laser Metal Deposition Process. In L. Santo & J. P. Davim (Eds.), Surface Engineering Techniques and 
Applications: Research Advancements (pp. 146–176). IGI Global; doi:10.4018/978-1-4666-5141-8.ch005
Mahamood, R. M., Shukla, M., & Pityana, S. (2014b). Laser Additive Manufacturing in Surface Modi-
fication of Metals. In L. Santo & J. P. Davim (Eds.), Surface Engineering Techniques and Applications: 
Research Advancements (pp. 222–248). IGI Global; doi:10.4018/978-1-4666-5141-8.ch007
Obielodan, J., & Stucker, B. (2013). Characterization of LENS-fabricated Ti6Al4V and Ti6Al4V/TiC 
dual-material transition joints. International Journal of Advanced Manufacturing Technology, 66(9-12), 
2053–2061. doi:10.1007/s00170-012-4481-9
Pityana, S., Mahamood, R. M., Akinlabi, E. T., & Shukla, M. (2013).Gas flow rate and powder flow 
rate effect on properties of laser metal deposited Ti6Al4V.International Multi conference of Engineering 
and Computer Science (IMECS 2013).
Popoola, A. P. I., Ochonogor, O. F., & Abdulwahab, M. (2013). Corrosion and Hardness Characteristics 
of Laser Surface Modified Ti6Al4V/Zr+TiC and Ti6Al4V/Ti+TiC Composites. International Journal 
of Electrochemical Science, 8, 2449–2458.

182
Laser Metal Deposition Process
﻿
Qin, L. Y., Yang, G., Wang, W., Tong, M., & Wang, X. L. (2010). Application of Laser Metal Deposi-
tion for Fabrication of Titanium Matrix Wear-Resistant Coating and its Wearing Performance. Applied 
Mechanics and Materials, 44-47, 316–320. doi:10.4028/www.scientific.net/AMM.44-47.316
Ramesh, S., Karunamoorthy, L., & Palanikumar, K. (2008). Surface Roughness Analysis in Machining of Ti-
tanium Alloy. Materials and Manufacturing Processes, 23(2), 174–181. doi:10.1080/10426910701774700
Ribeiro, M. V., Moreira, M. R. V., & Ferreira, J. R. (2003). Optimization of Titanium alloy (6Al–4V) 
machining. Journal of Materials Processing Technology, 143–144(20), 458–463. doi:10.1016/S0924-
0136(03)00457-6
Schneider, M. F. (1998). Laser cladding with powder, effect of some machining parameters on clad 
properties. (Ph.D. Thesis). University of Twente.
Scott, J., Gupta, N., Wember, C., Newsom, S., Wohlers, T., & Caffrey, T. (2012). Additive manufactur-
ing: status and opportunities, Science and Technology Policy Institute. Available from: https://www.ida.
org/stpi/occasionalpapers/papers/AM3D_33012_Final.pdf
Senthilkumaran, K., Pandey, P. M., & Rao, P. V. M. (2009). Influence of building strategies on the 
accuracy of parts in selective laser sintering. Materials & Design, 30(8), 2946–2954. doi:10.1016/j.
matdes.2009.01.009
Shah, K. (2011). Laser Direct Metal Deposition of Dissimilar and Functionally graded alloys. (PhD 
Thesis). The University of Manchester.
Shukla, M., Mahamood, R. M., Akinlabi, E. T., & Pityana, S. (2012). Effect of laser power and powder 
flow rate on properties of laser metal deposited Ti6Al4V. Engineering and Technology, 71, 1268–1272.
Wang, F., Mei, J., Jiang, H., & Wu, X. (2007a). Laser fabrication of Ti6Al4V/TiC composites us-
ing simultaneous powder and wire feed. Materials Science and Engineering A, 445–446, 461–466. 
doi:10.1016/j.msea.2006.09.093
Wang, F., Mei, J., & Wu, X. (2007b). Compositionally graded Ti6Al4V + TiC made by direct laser fabri-
cation using powder and wire. Materials & Design, 28(7), 2040–2046. doi:10.1016/j.matdes.2006.06.010
Wang, Z. M., & Ezugwu, E. O. (1997). Titanium Alloys and Their Machinability a Review. Journal of 
Materials Processing Technology, 68(3), 262–270. doi:10.1016/S0924-0136(96)00030-1
Wu, X., Liang, J., Mei, J., Mitchell, C., Goodwin, P. S., & Voice, W. (2004). Microstructures of laser-
deposited Ti–6Al–4V. Materials & Design, 25(2), 137–144. doi:10.1016/j.matdes.2003.09.009
Zhang, Y., Wei, Z., Shi, L., & Xi, M. (2008). Characterization of laser powder deposited Ti–TiC com-
posites and functional gradient materials. Journal of Materials Processing Technology, 438–444.
Zhou, W., & Chew, K. G. (2003). Effect of welding on impact toughness of butt-joints in a titanium 
alloy. Materials Science and Engineering A, 347(1-2), 180–185. doi:10.1016/S0921-5093(02)00596-8
This work was previously published in Advanced Manufacturing Techniques Using Laser Material Processing edited by Es-
ther Titilayo Akinlabi, Rasheedat Modupe Mahamood, and Stephen Akinwale Akinlabi, pages 46-59, copyright year 2016 by 
Engineering Science Reference (an imprint of IGI Global).

183
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  10
DOI: 10.4018/978-1-5225-1677-4.ch010
ABSTRACT
Additive Manufacturing (AM) offers lots of advantages when compared to other manufacturing processes, 
such as high flexibility and ability to produce complex parts directly from the Three Dimensional (3D) 
Computer-Aided Design (CAD) model. Producing highly complex parts using traditional manufactur-
ing processes is difficult, and it requires it to be broken down into smaller parts, which consumes lots 
of materials and time. If this part needs to have a surface with improved property or a surface made of 
composite materials, it has to be done by employing another manufacturing process after the parts are 
completed. AM, on the other hand, has the ability to produce parts with the required surface property 
in a single manufacturing run. Out of all the AM technologies, Laser Additive Manufacturing (LAM) 
is the most commonly used technique, especially for metal processing. LAM uses the coherent and col-
limated properties of the laser beam to fuse, melt, or cut materials according to the profile generated 
from the CAD image of the part being made. Some of the LAM techniques and their mode of operations 
are highlighted in this chapter. The capabilities of using LAM for surface modification of metals are 
also presented in this chapter. A specific example is given as a case study for the surface modification 
of titanium alloy (Ti6Al4V) with Ti6Al4V/TiC composite using laser material deposition process – an 
important LAM technology. Ti6Al4V is an important aerospace alloy, and it is also used as medical 
implants because of its corrosion resistance property and its biocompatibility.
Laser Additive Manufacturing in 
Surface Modification of Metals
Rasheedat M. Mahamood
University of Johannesburg, South Africa & University of Ilorin, Nigeria
Mukul Shukla
University of Johannesburg – Doornfontein, South Africa & MNNIT Allahabad, India
Sisa Pityana
National Laser Centre, South Africa

184
Laser Additive Manufacturing in Surface Modification of Metals
﻿
INTRODUCTION
Additive Manufacturing (AM) is also referred to as 3D printing technology according to the international 
standard organization (ISO) F42 committee on AM standard (Scott et al., 2012). AM is defined as a layer 
by layer fabrication method of producing parts directly from the CAD model of the part being produced. 
Additive manufacturing is a revolutionary (Mahamood et al., 2013a) and a promising technology es-
pecially for the aerospace industry because of it flexibility and its capability of reducing the buy-to-fly 
ratio (Allen, 2006; Richter et al., 2004). Buy-to-fly ratio is the ratio of weight of the raw material bought 
for the manufacturing of a part, to the weight of the final component produced. The difference in these 
sizes has gone into scrap. In some aerospace parts, the buy-to-fly ratio can be as high as 80% (Bryant et 
al., 2012; Mahamood et al., 2013b), that means, only about 20% of the material invested made it to the 
final part produced which is responsible for the high cost of the airplanes.
For a technology that is able to reverse this trend by reducing the buy-to-fly ratio to about 20% or 
less is very promising and AM technology can do just that. AM can achieve this objective in two differ-
ent ways, namely 1) The ability to produce complex part in one single step by adding materials layer by 
layer, as against removing material that later become scrap in the traditional manufacturing methods. 2) 
Less materials are used up during the material processing. This is because, assembly of smaller parts is 
eliminated in additive manufacturing method and the overall weight of the component becomes lighter 
as a result of the extra weight from bolt, nut, rivets etc. that are eliminated. This weight saving comes 
with two advantages: reduced material cost and reduced running cost in terms of routine maintenance, 
energy saving (especially in the aerospace industry) and reduction of carbon foot print. AM technology 
can be referred to as a ‘green’ technology because it uses less energy intensive processes (Lewis, 2012).
Additive manufacturing technologies are classified based on the energy system used, which are laser 
based system or non-laser based system. Laser based additive manufacturing system is the most popular 
additive manufacturing technologies (Wohlers, 2012). They took the advantage of the highly coherent 
characteristics of laser beam that makes it possible to produce a very fine detail part with intricate shapes, 
and with minimum dilution as well as with minimum heat affected zone. LAM technologies are capable 
of producing composite materials which are useful in various applications such as aerospace, defense, 
and medicine. In traditional manufacturing process or processes, when parts that required improved 
surface properties need to be produced, the parts are first made in a separate manufacturing process, and 
the surface modifications are done in another manufacturing process. The case is different with LAM 
technology because, part with the required surface properties can be produced in one manufacturing run 
which can be programmed at the onset of production process.
This chapter presents a brief review of additive manufacturing. Some laser additive manufacturing 
technologies that are used in the production of metallic parts with improved surface and surface made 
of composite material to improve the surface property of the parts are explained. Ti6Al4V, an impor-
tant aerospace material, is briefly reviewed and why it requires surface modification for applications 
demanding better wear performance behaviour is also explained. A section is devoted to a research work 
conducted on surface modification performed on a Ti6Al4V substrate with Ti6Al4V/TiC composite 
using laser material deposition (LMD) process, a laser additive manufacturing technique. The chapter 
ends with concluding remarks.

185
Laser Additive Manufacturing in Surface Modification of Metals
﻿
AN OVERVIEW OF ADDITIVE MANUFACTURING (AM) TECHNOLOGY
Additive Manufacturing (AM) or 3-Dimensional (3-D) printing (Scott et al., 2012), is a layer-by-layer 
fabrication method. Objects are built directly from 3-Dimensional (3-D) digital model that are produced 
using AutoCAD, Solid Edge, Unigraphics, Pro Engineer, etc. The CAD model is converted to Additive 
Manufacturing File (AMF) format (Scott et al., 2012) formally known as Stereolitography or standard 
triangulation language (STL) file format. The AFM format is the new file format with improved capa-
bilities as compared to the old STL file with limited capabilities. The AMF format is sliced and used to 
build the component layer after layer. Figure 1 shows the flow chart of AM technique. It involves the 
following steps: receiving of the CAD model file, converting of the CAD model into the AMF format, 
slicing of the AMF into two dimensional profile determined from the CAD model, building of the part 
following the sliced data information and when the building process is completed, the part is removed 
and the finishing operations are performed (Boboulos, 2010) based on the service requirement from the 
part. Finishing operation could be in the form of heat treatment or in the form of non-traditional finishing 
operations such as water jet cutting, laser beam machining and so on depending on the properties desired.
Additive manufacturing, as the name implies, fabricates components by adding materials layer by 
layer as against the traditional fabrication method (e.g. turning and cutting) that are subtractive in nature. 
The turning operation for example involves removing material in order to shape a component. Traditional 
fabrication method produces lots of scraps especially when fabricating complex parts. Also, in traditional 
manufacturing methods, complex parts need to be broken down into various parts because it cannot be 
Figure 1. Flow chat of Additive Manufacturing (AM) process

186
Laser Additive Manufacturing in Surface Modification of Metals
﻿
fabricated directly as a single piece. These parts are assembled at a later stage using bolt, nut, rivet etc. 
which makes the final component to be heavy. The complex component produced through the traditional 
manufacturing processes is more expensive as a result of additional materials used up during assembly, 
it produces a large quantity of scrap and it consumes a lot of man hour. Additive manufacturing on the 
other hand, eliminate most of these steps required in the traditional fabrication techniques. AM technol-
ogy produces a very complex component in one single step making it lighter and cheaper, because less 
material is used up, less scrap is produced, and manufacturing time is greatly reduced. An example of 
part produced through traditional fabrication method and AM technology are compared and are shown 
in Figure 2. Figure 2a shows parts produced from traditional manufacturing method while the same part 
produced from AM technology is shown in Figure (2b).
To use the traditional manufacturing methods to produce the environmental control ducting, the 
component has to be broken down into eleven various parts as shown in Figure 2a. These parts are later 
assembled using additional materials from welding, bolt, nuts etc. The same component was produced 
using AM technology as a single component shown in Figure 2b. This shows the flexibility and capability 
of the AM technology. Additive manufacturing technology helps to reduce design error, manufacturing 
lead time and it has a less negative environmental impact (Lewis, 2012). Some of the advantages of AM 
techniques are summarized as follows.
Advantages of Additive Manufacturing
•	
Complex parts that would be impractical or difficult to manufacture through traditional method 
can easily be produced using AM technology.
•	
Additive manufacturing are used to produce high value customised and limited edition products 
at no extra cost.
•	
It has the potential to reduce the carbon foot print as it eliminates the use of energy intensive 
manufacturing processes. Additive manufacturing is a green technology.
Figure 2. Environmental control ducting (a) using traditional method (b) using additive manufacturing 
(Wohlers, 2011)

187
Laser Additive Manufacturing in Surface Modification of Metals
﻿
•	
Components produced by AM technology are always lighter than the ones produced through tradi-
tional methods because part can be produced in a single piece thereby eliminating the use of nuts, 
screws, rivets, etc. for assembly purposes.
Areas of Application of AM Technology
AM technologies are used to produce prototypes, tooling and functional parts. Aerospace and Biomedical 
industries have the lion market share of additive manufacturing (Wohlers, 2012) because of the need for 
lighter parts in the aerospace industry and highly customized parts required for the medical implants. 
Automobile parts are also now being produced using additive manufacturing technologies because AM 
reduces weight, manufacturing time, lead time and the overall manufacturing cost. Any part that can be 
drawn or modelled digitally can be fabricated using additive manufacturing (Wohlers, 2012). Additive 
manufacturing can also be used to consolidate parts by metallurgically bonding them together without the 
need for part assembly; this is against mechanical bonding used in assembly of traditionally fabricated 
parts. This is possible because parts can be built directly on an existing part.
Additive manufacturing techniques are now grouped into seven and are explained in the next section.
Classes of Additive Manufacturing
There are over thirty commercialized AM technologies which has recently been classified into seven 
groups by the American Society for Testing Materials (ASTM) international committees F42 (committee 
set up by the ASTM International and International Standard Organization (ISO)) on Additive Manu-
facturing technologies (Scott et al., 2012). They are: Vat Photo polymerization (VP), Material Jetting 
(MJ), Material Extrusion (ME), Binder Jetting (BJ), Powder Bed Fusion (PBF), Sheet Lamination (SL) 
and Directed Energy Deposition (DED). The first three groups, uses photopolymers, polymers, waxes 
and foundry sand as their working materials while the last four can use metals.
Of all the classes of AM, only the directed energy deposition (DED) class can be used to repair high 
valued component parts, with improved surface property, that are prohibitive in the past using traditional 
manufacturing techniques. The energy in AM technology is used to fuse, melt, consolidate or cut con-
solidated materials in the AM processes. Laser beam, electron beam, and plasma welding are the major 
energy sources used in additive manufacturing processes (Kobryn et al., 2006). Laser beam is the most 
popular and the most commonly used energy source in the additive manufacturing technology especially 
for metallic parts (Wohlers, 2012). The unique characteristics and advantages of laser energy over other 
energy sources make laser to be more favoured in AM technology. The laser additive manufacturing is 
the focus of the next section.
Laser Additive Manufacturing (LAM)
Laser additive manufacturing has gained a lot of attentions from researchers over the last decade. The 
competitive advantage of LAM is in the area of production of end-used parts and in the use of metallic 
materials. The Light Amplification by Stimulated Emission of Radiation (Laser) is capable of producing 
a very high intensity and a highly collimated beam of energy that can be quickly moved in a controlled 
manner using directional mirrors or lenses. Laser is the most commonly used energy source in AM 
technologies because of the highly collimated nature of the beam that helps to concentrate the energy 

188
Laser Additive Manufacturing in Surface Modification of Metals
﻿
only to the needed area. The use of laser in additive manufacturing is basically for curing, cutting or 
heating (fusing or melting) purposes. In Vat photopolymerisation (VP) (a class of AM technology) for 
example, the requirement from laser energy is to cause the liquid resin to solidify, or “cure.” Heating is 
required for cutting through a solid material in Sheet Lamination (SL), another class of AM technology, 
or for melting powdered material in Directed Energy Deposition (DED) or fusing powder in Powder 
Bed Fusion (PBF).
Laser is required to deliver sufficient thermal energy to be able to achieve the above mentioned ap-
plications. In Directed Energy Deposition process for example, laser is required to melt the material 
in a controlled manner without creating heat build-up and as the laser energy is removed, the molten 
material rapidly solidifies. A lot of laser additive manufacturing processes applied to metals have been 
reported in the literature (Cai et al., 2007; Mahamood et al., 2012; Zhang et al., 2011). The laser additive 
manufacturing of small and medium-sized Ti6Al4V parts for aerospace industry is an important research 
area because of its capability of reducing buy-to-fly ratio. Production of a small complex aerospace part 
can produce up to 80% scrap, that is, only 20% of the raw material finds its way to the final component 
when traditional manufacturing methods are employed. That is a very high buy-to-fly ratio that can be 
reduced to as low as 20% when LAM technology is employed. Laser additive manufacturing produces 
parts with better dimensional accuracy when compared to other AM technologies (Martina et al., 2012) 
and it can also produce part with an improved surface property and also with surface made of composite 
material. This is responsible for the high interest of researches in this area. Some important laser additive 
manufacturing technologies are discussed in the next section.
Laser Additive Manufacturing (LAM) Technologies
Selective Laser Sintering (SLS), Selective Laser Melting (SLM) and Laser Material Deposition (LMD) 
are regarded as the three most versatile laser additive manufacturing processes (Gu et al., 2012). Of all 
the LAM technologies, only a few are capable of producing composite from metal (Kumar & Kruth, 
2009). The above mentioned three LAM process are among those that can produce composite from 
metals and can be used for surface modification of metallic parts. These processes are fully described 
in the following sub sections.
Selective Laser Sintering (SLS)
The selective laser sintering (SLS) process was first developed and patented by Dr. Carl Deckard for 
his master’s thesis at the University of Texas (Deckard, 1989). SLS process was commercialized by 
DTM Corporation and subsequently bought by 3D Systems incorporation (Banerjee, 2009). SLS uses 
the energy from laser to consolidate the powder by fusing the powder particle to form a solid mass ac-
cording to the path defined by the geometry of the part being produced (Kruth et al., 2003; Pham et al., 
2003). SLS belongs to the Powder Bed Fusion group of AM technologies, in which powder is spread 
layer by layer as the object is built up and the laser beam scans the required geometrical information on 
the spread powder. As the laser beam scans this geometry, it fuses the powder particles on its path. The 
schematic of the SLS process is shown in Figure 3.
Object is built on the build stage or platform as shown in Figure 3, by spreading the powder on the 
stage by a roller or sweeper. The spread powder is heated up by the laser beam that is scanned on the 
powder according to the geometry of the CAD data. The surface of the powder particle is melted causing 

189
Laser Additive Manufacturing in Surface Modification of Metals
﻿
each particle to be fused with one another on the laser path. The piston is lowered at a predetermined 
distance based on the CAD data information to allow the next layer of the powder to be spread and 
the next layer geometry of the part to be built. As the piston is lowered, the roller or sweeper spread 
another layer of powder on the previous layer and the laser is scanned over the powder bed following 
the direction provided by the CAD data information. These steps are repeated until the building process 
is completed. The powder particles that are off the laser path will serve as a support for the build up 
part and are removed after the build is completed. The materials used in SLS process include plastic, 
nylon, metals and ceramics. SLS process is capable of producing part with an improve surface as well 
as part made of composite material (Kumar & Kruth, 2009). The major disadvantage of SLS is the poor 
dimensional accuracy (Singh et al., 2012) which is dependent on the powder particle size. The finer the 
powder particle size, the better the dimensional accuracy. When coarse powder particles are used, the 
dimensional accuracy is poor due to the shrinkage in the part produced during solidification process. The 
surface finish achievable in SLS is determined by the layer thickness (Srivastava et al., 2010). The layer 
thickness is different from the piston movement distance; it is defined by the amount of powder that is 
spread on the building stage by the roller at a time. The smaller the layer thickness, the better the surface 
finish produced (Srivastava et al., 2010). SLS parts are highly susceptible to porosity (Singh et al., 2012).
Selective Laser Melting (SLM)
Selective laser melting (SLM) is similar to the selective laser sintering process and differs in the way 
powder is processed. In the SLS process, only the surface of the powder is melted to allow them to fuse 
together while in SLM process the metallic powder particles are completely melted and high density part 
is produced. The SLS and SLM belong to Powder Bed Fusion (PBF) class of the additive manufacturing 
by the ASM F42 standards committee on additive manufacturing (Wohlers, 2012). Selective laser melt-
ing process fully melts the metal into a solid homogeneous mass based on the CAD data information 
of the part to be produced. The schematic of the SLM process is shown in Figure 4. The part is built by 
spreading the powder particle by the roller, and then the laser selectively scans the powder bed based 
on the information from the CAD data. Once each layer is distributed, the 2D slice of the part geometry 
is formed by applying the laser energy that traces the 2D shape on the surface of the powder and fully 
Figure 3. Schematics of Selective Laser Sintering (SLS)

190
Laser Additive Manufacturing in Surface Modification of Metals
﻿
melts the powder on its path to form the solid part upon solidification. The two laser beam is focused 
using the two high frequency scanning mirrors in the X and Y axes as shown in Figure 4. Combination 
of the two mirrors allows the generation of high intensity laser energy that is high enough to fully melt 
the powder particles to form a fully dense solid part. The process is also repeated like the SLS process 
layer after layer until building of the part is completed.
It is useful in producing single parts with solid and partially formed or lattice type geometries such 
as a hip stem or other orthopedic implant. SLM processes also find their application in lightweight parts 
for aerospace industry. SLM can also produce part with improved surface and part that is made up of 
composite materials. Though SLM can produce high density parts but it still has some porosity of about 
1 to 2% which could be detrimental in some applications (Yasa & Kruth, 2011).
Laser Material Deposition (LMD)
Laser material deposition (LMD) belongs to the Directed Energy Deposition (DED) class of additive 
manufacturing technologies (Wohlers, 2012) that is useful in repair (Pinkerton et al., 2008) of an ex-
isting high valued part as well as building new parts. Examples of the parts repaired by DED process 
include engine blisks and gas turbine blade (Hedges & Calder, 2006). LMD can be used to manufacture 
part made of composite materials and part made of functionally graded material with improved sur-
face properties because of its ability to handle more than one material simultaneously. The deposition 
process is achieved by the laser first creating a melt pool on the substrate, and then the material(s) to 
be deposited are delivered into the melt pool. The materials are delivered through the powder delivery 
nozzles or the wire feeder system or both depending on the number and form of materials involved. The 
deposited materials get melted in the melt pool and upon solidification forms track of solid mass of the 
materials. The schematics of the LMD process are shown in Figure 5. Laser material deposition is an 
important AM technology that offers unique advantages to many industries such as aerospace, medicine 
and defense. It is also used to repair high valued component parts, with improved surface property, which 
are discarded in the past because they were not repairable (Capello et al., 2005; Leunda et al., 2011; 
Pinkerton et al., 2008). With proper process control, the heat affected zone can be made minimal thereby 
not causing damage to the surrounding or the substrate material. Laser material deposition process has 
Figure 4. Schematic of Selective Laser Melting process

191
Laser Additive Manufacturing in Surface Modification of Metals
﻿
the capability to handle various materials at the same time making it a suitable process for producing 
compositional gradient materials.
LMD is a promising manufacturing system because of the aforementioned advantages. It can be 
used to process difficult to machine materials like titanium and its alloys (Shokrani et al., 2012) thereby 
reducing manufacturing cost as well as material utilization and the overall weight of the part. One of 
the current problems of the LMD process is that, the underlying physics of the process is not fully un-
derstood. The properties of the fabricated part need to be fully understood in order to be able to control 
the process in order to achieve the desired properties based on the service requirement of the part. Since 
this process is highly promising for manufacturing of difficult to machine materials like titanium for 
aerospace industry, there is need to be able to predict properties of the part produced. Titanium alloy, 
Ti6Al4V, the most widely used titanium alloy in the aerospace industry, is the focus of the next section.
Titanium Alloy (Ti6Al4V): An Aerospace Alloy
Ti6Al4V is the most widely used titanium alloy in the aerospace industry (Lütjering, 2007) owning to 
their excellent combination of properties which include high strength to weight ratio, and its excellent 
corrosion resistance property (Henriques, 2009). Ti6Al4V is also favoured in other industries such as 
defense, medicine and petrochemical industries (Casalono et al., 2005; Wang et al., 2003). The wear 
behaviour of Ti6Al4V is very poor (Boyer, 1994) when used in an application involving it coming in 
contact itself or with other materials. Ti6Al4V is very chemically reactive making it to wear greatly when 
involved in rubbing action with itself or with other materials. This is one of the reasons why titanium 
and its alloys are termed difficult to machine material using traditional manufacturing process. Laser 
additive manufacturing is more advantageous for processing such materials. The poor wear behaviour of 
titanium and its alloy necessitates the need for surface modification of this material in any application 
involving contact with another surface. Surface modification of Ti6Al4V is the focus of the next section.
Figure 5. Schematic of Laser Metal Deposition process (Mahamood et al., 2013c)

192
Laser Additive Manufacturing in Surface Modification of Metals
﻿
Surface Modification of Ti6Al4V
Surface modification is also referred to as surface engineering. There are lots of surface modifications 
performed on Ti6Al4V in the literature to improve its wear behaviour (Popoola et al., 2012; Popoola et 
al., 2013; Singh et al., 2006; Guleryuz & Cimenoglu, 2005). Some of the surface modification techniques 
include surface alloying (Jiang et al., 2000; Tang et al., 2004) achieved by adding alloying materials and 
melting on the surface to be modified. Surface heating or melting (Singh et al., 2006) are done to refine 
the microstructure of the surface. And surface cladding (Cai, 2007) done by metallurgically bonding 
another material on the surface of the material being modified with minimum mixing or dilution of the 
added material and the substrate material. Surface cladding can be achieved through processes such as 
physical vapour deposition, chemical vapour deposition, laser cladding and laser additive manufactur-
ing. LAM offers lots of advantages in surface modification of metals when compared to traditional 
manufacturing processes in terms of flexibility and production of part with desired surface properties in 
one single step. Unlike in traditional manufacturing processes, the part needs to be fabricated first, after 
which the surface modification is performed as a secondary operation. The uses of LAM for surface 
modification of parts are explained in the next section.
Laser Additive Manufacturing for Surface Modification of Metallic Parts
The surface properties of laser additive manufactured Ti6Al4V part has been reported to have an improved 
surface properties when compared to the property of the material before the additive manufacturing 
process (Kobryn & Semiatin, 2001; Santos et al., 2006). The rapid solidification of the melt pool in the 
laser additive manufacturing process is responsible for the improved surface properties observed. The 
rapid solidification results in martensitic microstructure and in some cases an equiaxed microstructure. 
The surface property of laser additive manufactured part can also be improved by controlling the cool-
ing rate during the manufacturing process in order to achieve a controlled microstructure. Controlling 
the microstructure means controlling the mechanical properties of the material. When the surface of 
the laser additive manufactured material are compared with their parent materials, the surface of the 
processed materials is found to be of better microstructure and an improved mechanical properties. Kruth 
et al. (2010) reported controllable microstructure achievable in selective laser melting of metals which 
resulted in improved mechanical properties. Thijs et al. (2010) reported an improved surface property 
in selective laser melting of Ti6Al4V. Miranda et al. (2008) reported an improved surface property of 
laser metal deposited Ti6Al4V.
The possibility of using more than one material simultaneously makes it possible to produce part 
made of composite surface using laser additive manufacturing techniques which is achievable in one 
manufacturing run. Some of the LAM processes used to produce surface modification of metallic parts 
that have been reported in the literature include the work of Popoola et al. (2013) that investigated the 
corrosion property of Ti6Al4V modified by Ti6Al4V/Zr+TiC and Ti6Al4V/Ti+TiC using laser metal 
deposition process. The study reveals that the corrosion resistance property of the Ti6Al4V was greatly 
improved. In a similar study conducted by Popoola et al. (2012), investigated the wear resistance behav-
iour of surface modified Ti6Al4V/Zr-TiC also using laser metal deposition process. The study showed 
that the wear resistance property of the surface material was improved. The wear resistance behaviour 
of Ti6Al4V was found to have been improved by depositing Ti6Al4V/TiC on the surface of Ti6Al4V 
using laser metal deposition process (Mahamood et al., 2013c). Traini et al. (2008) investigated the 

193
Laser Additive Manufacturing in Surface Modification of Metals
﻿
properties of titanium alloy implants obtained by selective laser sintering process. The surface appear-
ance, microstructure, composition, mechanical properties were investigated in the study. An improved 
mechanical property was achieved in the study. Wild et al. (2009) conducted a similar study on the 
property of selective laser melting of Ti6Al4V. An improved surface property was also achieved with 
appropriate thermal post processing of the part. A number of literatures are also available on the use 
of laser additive manufacturing for surface modification of metallic parts. The readers are referred to 
(Brandl et al., 2010; Brandl et al., 2011) for further reading.
In the next section, a typical research work conducted on surface modification of Ti6Al4V by deposit-
ing Ti6Al4V/TiC composite on the surface of Ti6Al4V substrate using laser material deposition process. 
The case study of the wear resistance behaviour of the surface produced is presented and discussed.
Surface modification of Ti6Al4V with Ti6A4V/TiC Composite 
Using Laser Material Deposition (LMD) Process
Laser Metal Deposition has been successfully used in the literature to produce composite on the surface 
of Ti6Al4V substrate to improve the wear performance property of this aerospace alloy. These include the 
work done by Sampedro et al. (2011); they deposited Ti6Al4V/TiC composite on Ti6Al4V substrate to 
improve the surface properties of the substrate using laser metal deposition process. Their results showed 
an improvement in the wear resistance performance. Cai et al. (2007) improved the surface properties of 
Ti6Al4V substrate by producing TiB/TiC composite in-situ. They demonstrated that TiC can be formed 
through chemical reaction on the Ti6Al4V using laser deposition to improve wear resistance perfor-
mance. Wang et al. (2007) deposited a composite material of Ti6Al4V/TiC on Ti6Al4V substrate using 
TiC powder and Ti6Al4V wire simultaneously in laser material deposition process. They also recorded 
improved wear resistance behaviour when the volume fraction of TiC reinforcements was larger than 24%.
A research work that characterizes a Ti6Al4V/TiC composite deposited to modify the surface of a 
Ti6Al4V substrate is demonstrated in the following sub sections. The Ti6Al4V/TiC composite deposited 
is in the ratio of 25% by weight of Ti6Al4V and 75% by weight of TiC powders. The effect of varying 
the scanning speed on the evolving wear resistance property was studied. The materials and methods are 
described in the next section, followed by the results, the discussion of results, further research direction 
and the general conclusions of the chapter.
Materials and Experimental Methods
Ti6Al4V powder of particle size ranging between 150 to 200 µm and TiC powder of particle size below 
60 µm were used in this experiment. The two powders were placed in two separate hoppers and set to 
deliver the weights of Ti6Al4V and TiC powders at a ratio of 1 to 4 at the same time. These settings 
result in 25W% Ti6Al4V and 75W% TiC. The powder flow rate for the two hoppers is a total of 2.88 
g/min. The powder carrier gas flow rate was set at a 0.5 l/m for Ti6Al4V and 1.5 l/min for TiC powder 
making a total 2 l/min. The laser power was set at 3.6 kW while the scanning speed was varied between 
0.001 and 0.01 m/s. The substrate used is 5 mm thick Ti6Al4V plate of dimension 72 x 72 mm. The 
substrate was sand blasted and cleaned with acetone before the deposition.
The laser material deposition was achieved by first creating a melt pool on the surface of the sub-
strate by the laser beam of approximately 2 mm in diameter and at a focal distance of 195 mm. The 
two powders were then delivered into the melt pool by the coaxial nozzles. The shield mechanism was 

194
Laser Additive Manufacturing in Surface Modification of Metals
﻿
achieved by improvising a glove box with plastic cover. The shielding is necessary to prevent atmospheric 
contamination of the deposit because titanium picks up oxygen at high temperature (Kahraman et al., 
2007). The glove box is filled with argon gas to keep the oxygen level at below 10 ppm. Multiple tracks 
deposition was achieved at 50% overlap.
Samples for microstructural examinations were cut, mount, ground and polished according to stan-
dard metallographical technique for titanium and its alloy sample preparation and the samples were 
etched with Kroll’s reagent. The etched samples were observed under optical microscope and scanning 
electron microscopy (SEM) by Tescan. The SEM is equipped with Oxford energy dispersion spectrom-
etry (EDS). The dry sliding wear test was carried out with CERT tribotester at a nominal load of 25N, 
at a sliding distance of 2000 m, a stroke of 2 mm, with a tungsten carbide ball of 10 mm diameter as a 
counter face. The set up was the ball on disk arrangement. The change in weight as a result of wear was 
determined by measuring the weight prior to and after wear test. The tribotester was used to access the 
wear resistance property of the surface of the laser material deposited part which is compared to that 
of the parent material.
Results and Discussion
From the microstructural observation, the micrograph of the composite samples shows three distinct 
regions, namely: the composite, the dilution and the heat affected zone. The schematic of the cross sec-
tion of the sample showing the three zones mentioned above is presented in Figure 6a.
The composite layer region consists of the Ti6Al4V and the TiC that has been melted or partly 
melted. The dilution region is the interface between the substrate and the composite deposited. This 
region is responsible for the bonding of the deposit and the substrate. There is intermixing of the de-
posited materials and the substrate material in the dilution region and this region is required to be small 
enough to produce sound metallurgical bonding between the substrate and the deposited composite. It 
should however not too small to prevent porosity or improper bonding and it should not be too large to 
prevent swallowing of the supposed surface coating materials thereby producing little or no effect on 
the surface being modified. Through proper control of the processing parameters, this can be achieved 
and it is demonstrated in this work. If the coated surface is buried into the substrate, the objective of 
surface modification will be forfeited and we do not want that to happen. Figure 6b shows the bar chat 
of dilution thicknesses against the scanning speeds employed. The sample at the scanning speed of 0.001 
m/s, the lowest scanning speed studied, produced the highest dilution thickness when compared to the 
other samples.
The highest dilution thickness observed in the sample at 0.001 m/s scanning speed, is as a result of 
high laser material interaction time. At low scanning speed, the laser spend more time interacting with 
the material and creating a larger melt pool that does not solidifies quickly. The slow solidification 
causes more and more of the substrate material to be melted and mixing with the deposited materials. 
After solidification, the deposited material is seen buried more inside the substrate because of the long 
interaction time and mixing processes that had occurred. Also, to be able to actually improve the wear 
performance of the surface, it is necessary to retain some Unmelted Carbides (UMC) in the final com-
posite produced.
The wear test results show that the substrate has the highest coefficient of friction when compared 
to all the samples and this is because of the reactive nature of Ti6Al4V to contacting surfaces. Other 
samples show improved lower coefficient of friction but it initially decreased as the scanning speed 

195
Laser Additive Manufacturing in Surface Modification of Metals
﻿
was increased and started increasing as the scanning speed was increased above 0.007 m/s. Figure 7a 
shows the plot of average coefficient of friction against scanning speed. The coefficient of friction of 
the substrate is represented at zero scanning speed on the plot.
The bar chat of weight loss is shown in Figure 7b. The weight loss also follows the trend observed in 
the coefficient of friction, the weight loss initially decreases as the scanning speed increases and started 
to increase as the scanning speed was increased beyond the scanning speed of 0.006 m/s.
At low scanning speed of 0.001 m/s, most of the carbides are fully melted. As the scanning speed was 
increased, the quantity and size of the unmelted carbide increases until the scanning speed of 0.006 m/s 
after which less deposited materials are being delivered and melted, resulting in a decline in the wear 
resistance performances. This result is consistent with the work of Mahamood et al. (2013c). Since the 
powder flow rate is fixed, at higher scanning speed, there is little time available for the powder delievery as 
well as the laser material interaction time and these are responsible for the poor wear resistance behaviour 
Figure 6. (a) Schematic of the sample showing the cross section and the three zones, (b) bar chat of 
dilution thicknesses against the scanning speed, from 0.001 to 0.01 m/s

196
Laser Additive Manufacturing in Surface Modification of Metals
﻿
observed at a very high scanning speeds. The optimum processing parameters in this study are; 3.6 kW 
laser power, 0.006 m/s scanning speed, 2.88 g/min and 2 l/min powder and gas flow rate respectively.
Figure 8a shows the SEM micrograph of the substrate. The microstructure of the Ti6Al4V substrate 
is characterised by alpha and beta microstructure. At a very low scanning speed, the length of time 
involved for the deposited materials and the substrate to interact with the laser causes some of the TiC 
powder to be completely melted and can be seen as dendritic TiC as shown in Figure 8b. The SEM 
micrograph of the surface modified sample at laser power and a scanning speed of 3.6 kW and 0.006 
m/s is shown in Figure 8b.
The wear mechanism of the substrate material is characterised by abrasive, adhesive and plastic 
deformation (Mahamood et al., 2013c). The wear track of the substrate is shown in Figure 8c. The 
ploughing grooves are seen on the wear track with some of the loose debris on the track. The abrasion 
wear mechanism happened at the beginning of the rubbing action between the two surfaces in contact. 
As the sliding action continues, the temperature of the sliding surfaces becomes very high. This high 
Figure 7. (a) Plot of the average coefficient of friction against the scanning speed. (b) bar chat of weight 
loss against scanning speed

197
Laser Additive Manufacturing in Surface Modification of Metals
﻿
temperature results in the formation of debris from the rubbing surfaces. The loose debris caused the 
wear mechanism to change. The loose debris are trapped in between the two lidding surfaces causing 
the wear mechanism to change from the two body to a three body wear mechanism (Mahamood et al., 
2013c). As the sliding action continued, loose debris became work hardened as the temperature was 
further increase. The work hardened debris further cut the ploughing grooves deeper and deeper. The 
surface of the loose debris became melted as a result of high temperature generated by the rubbing ac-
tion, which later weld themself to the surface of the wear track.
The wear track of the sample at a scanning speed of 0.006 is shown in Figure 8d. By comparing the 
wear track of the substrate (see Figure 8c) and that of the processed sample shown in Figure 8d, it can be 
seen that the wear action is less severed in Figure 8d. This is because the surface modification provided a 
kind of lubricating action during the sliding process. The Unmelted carbide (UMC) particles are reduces 
to a fine powder particle as a result of grinding of the UMC particles during the rubbing actions. The 
UMC are ground against each other as well as the surface to form a fine powder that coat the surface of 
the material under the sliding wear action. Initially the UMC cut slightly into the surface but the action 
was not prolonged because the UMC becomes fine powder as the sliding action was continued. This is 
different from the wear action observed in the substrate material where the debris become work hardened 
and cut deeper in to the rubbing surface. The ground powder prevent further cutting into the surfaces 
because the powder now act as a lubricant for the surfaces. This is responsible for the improved wear 
resistance behaviour of the modified surface.
Figure 8. SEM micrograph of (a) substrate, (b) sample at the scanning speed of 0.006 m/s showing 
dendritic TiC and Unmelted carbide, (c) wear track of the substrate, (d) wear track of the sample at a 
at the scanning speed of 0.006 m/s

198
Laser Additive Manufacturing in Surface Modification of Metals
﻿
FUTURE RESEARCH DIRECTIONS
The future research will focus on the full design of experiment to study the effect of various processing 
parameters associated with Laser additive manufacturing processes for surface modification. This will 
help to fully understand how these processing parameters affect the properties of the surface modifica-
tion achievable using the Ti6Al4V/TiC composite. Also, interactions among processing parameters 
will also be understood during this study. With a proper design of experiment, a power model can be 
developed. To effectively control the properties evolving from any surface modification process, it is 
very important to fully understand the effects of various processing parameters on the final properties of 
the modified surface. An effective functionally graded surface can also be achieved using the results that 
will be obtained. Functionally graded materials or surfaces are useful when the material or the surface 
will be exposed to extreme working conditions.
CONCLUSION
In this chapter, an overview of additive manufacturing technology and surface modification using laser 
additive manufacturing processes were presented. The application of laser additive manufacturing in the 
surface modification of titanium and its alloys has been highlighted. The capability of using laser additive 
manufacturing processes to produce composite surfaces is also presented. The results of a research work 
conducted using laser material deposition process; a laser additive manufacturing technique, for surface 
modification of Ti6Al4V substrate was presented and discussed as a case study. The results from this 
work showed that at any of the processing parameter setting used in this study, even if not optimum, the 
wear resistance performance surpassed that of the substrate showing that the surface modification was 
actually achieved. This chapter brings to lime light the ability to produce part directly from the CAD 
model of the part as well as achieving the desired surface properties in one single manufacturing run. 
This is not possible with the traditional manufacturing methods. The capability of using LAM techniques 
as a manufacturing process to reduce buy-fly-ratio in the aerospace industry was also highlighted. LAM 
can be used to produce highly complex parts with desired surface properties and at no extra cost but with 
lots of savings on material, weight, lead time and the overall cost of the part. In traditional manufacturing 
method, complex part needs to be broken down in to smaller parts that are manufactured in a number of 
manufacturing processes. The parts are later assembled and additional operation need to be carried out 
for surface modification. The traditional manufacturing method involves the use of energy intensive pro-
cesses, generate a lot of scrap and require a lot of man hour. Producing the same part using laser additive 
manufacturing technique is achievable in a single manufacturing process. This makes the LAM process 
to be termed a green manufacturing process because it is a less energy intensive manufacturing process.
ACKNOWLEDGMENT
This work is supported by the Council of Scientific and Industrial Research (CSIR) National Laser Centre 
(NLC), Rental Pool Programme, Pretoria South Africa, and The Schlumberger Foundation Faculty for 
the Future (FFTF).

199
Laser Additive Manufacturing in Surface Modification of Metals
﻿
REFERENCES
Allen, J. (2006). An investigation into the comparative costs of additive manufacture vs. machine from 
solid for aero engine parts in cost effective manufacture via net-shape processing. In Proceedings RTO-
MP-AVT-139. RTO-MP-AVT.
Banerjee, S. (2009). Development of a novel toner for electrophotography based additive manufacturing 
process. (PhD Thesis). De Montfort University, Leicester, UK.
Boboulos, M. A. (2010). CAD-CAM & rapid prototyping application evaluation. PhD and Ventus Pub-
lishing Aps. Retrieved from www.bookboom.com
Boyer, I. R. (1994). Material properties handbook: Titanium alloys. Pittsburgh, PA: ASM.
Brandl, E., Baufeld, B., Leyens, C., & Gault, R. (2010). Additive manufactured Ti-6Al-4V using welding 
wire: Comparison of laser and arc beam deposition and evaluation with respect to aerospace material 
specifications. Physics Procedia, 5(B), 595-606.
Brandl, E., Palm, F., Michailov, V., Viehweger, B., & Leyens, C. (2011). Mechanical properties of ad-
ditive manufactured titanium (Ti–6Al–4V) blocks deposited by a solid-state laser and wire. Materials 
& Design, 32(10), 4665–4675. doi:10.1016/j.matdes.2011.06.062
Bryant, P., Rao, S., & Rabenau, B. (2012). Understanding the manufacturing landscape of the future 
and how the ecosystem, production process, and business models will evolve. Retrieved April 13, 2013 
from http://clareopartners.com/pages/wp-content/uploads/PDFs/Clareo-Manufacturing-2020.pdf
Cai, L. F., Zhang, Y. Z., & Shi, L. K. (2007). Microstructure and formation mechanism of titanium 
matrix composites coating on Ti-6Al-4V by laser cladding. Rare Metals, 26(4), 342–346. doi:10.1016/
S1001-0521(07)60226-5
Capello, E., Colombo, D., & Previtali, B. (2005). Repairing of sintered tools using laser cladding by wire. 
Journal of Materials Processing Technology, 164-165, 990–1000. doi:10.1016/j.jmatprotec.2005.02.075
Casalino, G., Curcio, F., Memola, F., & Minutolo, C. (2005). Investigation on Ti6Al4V laser welding 
using statistical and Taguchi approaches. Journal of Materials Processing Technology, 167, 422–428. 
doi:10.1016/j.jmatprotec.2005.05.031
de Wild, M., Schumacher, R., Fabbri, S., Yildiz, A., & Schkommodau, E. (2009). Analysis and surface 
modification of rapid prototyped titanium structures. European Cells & Materials, 17, 1–6. PMID:19579210
Deckard, C. (1989). Methods and apparatus for producing parts by selective laser sintering (US Patent 
4863538). Washington, DC: US Patent Office.
Gu, D. D., Meiners, W., Wissenbach, K., & Poprawe, R. (2012). Laser additive manufacturing of metallic 
components: Materials, processes and mechanisms. International Materials Reviews, 57(3), 133–164. 
doi:10.1179/1743280411Y.0000000014
Guleryuz, H., & Cimenoglu, H. (2005). Surface modification of a Ti–6Al–4V alloy by thermal oxidation. 
Surface and Coatings Technology, 192, 164–170. doi:10.1016/j.surfcoat.2004.05.018

200
Laser Additive Manufacturing in Surface Modification of Metals
﻿
Hedges, M., & Calder, N. (2006). Near net shape rapid manufacture and repair by LENS. Neotech 
Services. Retrieved February 26, 2013 from http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA524690
Henriques, V. A. R. (2009). Titanium production for aerospace applications. Journal of Aerospace 
Technology and Management, 1(1), 7–17. doi:10.5028/jatm.2009.01010717
Kahraman, N., Gulenc, B., & Findik, F. (2007). Corrosion and mechanical–microstructural aspects of 
dissimilar joints of Ti6Al4V and Al plates. International Journal of Impact Engineering, 34, 1423–1432. 
doi:10.1016/j.ijimpeng.2006.08.003
Kobryn, P. A., Ontko, N. R., Perkins, L. P., & Tiley, J. S. (2006). Additive manufacturing of aerospace 
alloys for aircraft structures in cost effective manufacturing via net-shape processing. In Proceedings of 
RTO-Mp-AVT-139, (vol. 3, pp. 1-14). Retrieved March 12, 2013 from http://www.rto.nato.int/abstracts.asp
Kobryn, P. A., & Semiatin, S. L. (2001). The laser additive manufacture of Ti-6Al-4V. Springer-Verlag, 
53(9), 40-42.
Kruth, J.-P., Badrossamay, M., Yasa, E. D., Jan, T. L., & Van Humbeeck, J. (2010). Part and material 
properties in selective laser melting of metals. In Proceedings of the 16th International Symposium on 
Electromachining. Shanghai, China: Academic Press.
Kruth, J.-P., Mercelis, P., Vaerenbergh, J. V., Froyen, L., & Mrombouts, M. (2003). Advances in selec-
tive laser sintering. In Proceedings of International Conference on Advanced Research in Virtual and 
Rapid Prototyping, (pp. 59-69). Academic Press.
Kumar, S., & Kruth, J.-P. (2009). Composites by rapid prototyping technology. Materials & Design, 1–23.
Leunda, J., Soriano, C., Sanz, C., & Garcia Navas, V. (2011). Laser cladding of vanadium-carbide tool 
steels for die repair. Physics Procedia, 12, 345–352. doi:10.1016/j.phpro.2011.03.044
Lewis, M. (2012). Additive manufacturing puts its clean-tech face forward in federally funded develop-
ment program. IMT Green and Clean Journal. Retrieved June 12, 2013 from http://news.thomasnet.
com/green_clean/2012/08/29/additive-manufacturing-puts-its-clean-tech-face-forward-in-federally-
funded-development-program/
Lütjering, G. (2007). Titanium. Berlin: Springer.
Mahamood, R. M., Akinlabi, E. T., Shukla, M., & Pityana, S. (2012). Evolving properties of laser metal 
deposited Ti6Al4V. In Proceedings of 2012 International Conference of Rapid Product Development 
Association of South Africa (RAPDASA). RAPDASA.
Mahamood, R. M., Akinlabi, E. T., Shukla, M., & Pityana, S. (2013a). Revolutionary additive manu-
facturing: An overview. Journal of Laser in Engineering.
Mahamood, R. M., Akinlabi, E. T., Shukla, M., & Pityana, S. (2013b). Material efficiency of laser metal 
depositedTi6Al4V: Effect of laser power. Engineering Letters, 21(1). Retrieved February 25 2013 from 
http://www.engineeringletters.com/issues_v21/issue_1/EL_21_1_03.pdf
Mahamood, R. M., Akinlabi, E. T., Shukla, M., & Pityana, S. (2013c). Scanning velocity influence on 
microstructure, microhardness and wear resistance performance on laser deposited Ti64/TiC composite. 
Materials & Design, 50, 656–666. doi:10.1016/j.matdes.2013.03.049

201
Laser Additive Manufacturing in Surface Modification of Metals
﻿
Martina, F., Mehnen, J., Williams, S. W., Colegrove, P., & Wang, F. (2012). Investigation of the benefits 
of plasma deposition for the additive layer manufacture of Ti6Al-4V. Journal of Materials Processing 
Technology, 212, 1377–1386. doi:10.1016/j.jmatprotec.2012.02.002
Miranda, R. M., Lopes, G., Quintino, L., Rodrigues, J. P., & Williams, S. (2008). Rapid prototyping with 
high power fiber lasers. Materials & Design, 29(10), 2072–2075. doi:10.1016/j.matdes.2008.03.030
Pham, D. T., Dimov, S. S., Ji, C., & Gault, R. S. (2003). Layer manufacturing processes: Technology 
advances and research challenges. In Proceedings of 1st International Conference on Advanced Research 
in Virtual and Rapid Prototyping, (pp. 107-113). Leiria, Portugal: Academic Press.
Pinkerton, A. J., Wang, W., & Li, L. (2008). Component repair using laser direct metal deposition. Pro-
ceedings of IMechE Part B: J. Engineering M, 222, 827–836. doi:10.1243/09544054JEM1008
Popoola, A. P. I., Ochonogor, O. F., & Abdulwahab, M. (2013). Corrosion and hardness characteristics 
of laser surface-modified Ti6Al4V/Zr+TiC and Ti6Al4V/Ti+TiC composites. Int. J. Electrochem. Sci., 
8, 2449–2458.
Popoola, A. P. I., Ochonogor, O. F., Abdulwahab, M., Pityana, S., & Meacock, C. (2012). Microhardness 
and wear behaviour of surface modified Ti6Al4V/Zr-TiC metal matrix composite for advanced material. 
Journal of Optoelectronics and Advanced Materials, 14(11-12), 991–997.
Richter, K. H., Orban, S., & Nowotny, S. (2004). Laser cladding of the titanium alloy ti6242 to restore 
damaged blades. In Proceedings of the 23rd International Congress on Applications of Lasers and 
Electro-Optics. ICALEO.
Sampedro, J., Pérez, I., Carcel, B., Ramos, J.A., & Amigó, V. (2011). Laser cladding of TiC for better 
titanium components. Physics Procedia, 12(A), 313-322.
Santos, E. C., Shiomi, M., Osakada, K., & Laoui, T. (2006). Rapid manufacturing of metal components 
by laser forming. International Journal of Machine Tools & Manufacture, 46(12–13), 1459–1468. 
doi:10.1016/j.ijmachtools.2005.09.005
Scott, J., Gupta, N., Wember, C., Newsom, S., Wohlers, T., & Caffrey, T. (2012). Additive manufactur-
ing: Status and opportunity. Science and Technology Policy Institute.
Shokrani, A., Dhokia, V., & Newman, S. T. (2012). Environmentally conscious machining of difficult-to-
machine materials with regard to cutting fluids. International Journal of Machine Tools & Manufacture, 
57, 83–101. doi:10.1016/j.ijmachtools.2012.02.002
Singh, R., Kurella, A., & Dahotre, N. B. (2006). Laser surface modification of Ti–6Al–4V: Wear and 
corrosion characterization in simulated biofluid. Journal of Biomaterials Applications, 21(1), 49–73. 
doi:10.1177/0885328206055998 PMID:16443617
Singh, S., Sachdeva, A., & Sharma, V. S. (2012). Investigation of dimensional accuracy/mechanical 
properties of part produced by selective laser sintering. International Journal of Applied Science and 
Engineering, 10(1), 59–68.

202
Laser Additive Manufacturing in Surface Modification of Metals
﻿
Srivastava, V., Parida, S. K., & Pandey, P. S. (2010). Surface roughness studies in selective laser sinter-
ing of glass filled polyamide. In Proceedings of the 36th International MATADOR Conference, (pp. 
495-499). MATADOR.
Thijs, L., Verhaeghe, F., & Craeghs, T., Van Humbeeck, & Kruth, J-P. (2010). A study of the micro-
structural evolution during selective laser melting of Ti–6Al–4V. Acta Materialia, 58(9), 3303–3312. 
doi:10.1016/j.actamat.2010.02.004
Traini, T., Mangano, C., Sammons, R. L., Mangano, F., Macchi, A., & Piattelli, A. (2008). Direct la-
ser metal sintering as a new approach to fabrication of an isoelastic functionally graded material for 
manufacture of porous titanium dental implants. Dental Materials, 24(11), 1525–1533. doi:10.1016/j.
dental.2008.03.029 PMID:18502498
Wang, F., Mei, J., Jiang, H., & Wu, X. (2007). Laser fabrication of Ti6Al4V/TiC composites using si-
multaneous powder and wire feed. Materials Science and Engineering a-Structural Materials Properties 
Microstructure and Processing, 445, 461-466.
Wang, S. H., Wei, M. D., & Tsay, L. W. (2003). Tensile properties of LBW welds in Ti–6Al–4V al-
loy at evaluated temperatures below 450◦C. Materials Letters, 57, 1815–1823. doi:10.1016/S0167-
577X(02)01074-1
Wohlers, T. (2011). Wohlers report. In Additive manufacturing technology roadmap for Australia. 
Wohlers Associates, Inc. Retrieved on April 23, 2013 from http://3dprintingexpo.org/wp-content/uploads/
Additive-Manufacturing-Technology-Roadmap-CSIRO-2011.pdf
Wohlers, T. (2012). Additive manufacturing advances. Manufacturing Engineering Magazine, 55-63. 
Retrieved on June 3, 2013 from www.sme.org/manufacturingengineering
Yasa, E., & Kruth, J-P. (2011). Application of laser re-melting on selective laser melting parts. Advances 
in Production Engineering and management, 6(4), 259-270.
Zhang, Y., Huang, C., & Vilar, R. (2011). Microstructure and properties of laser direct deposited 
CuNi17Al3Fe1.5Cr alloy. International Journal of Minerals. Metallurgy and Materials, 18(3), 325–328. 
doi:10.1007/s12613-011-0442-9
ADDITIONAL READING
Grylls, R. (2003). Laser Engineered Net Shapes. Advanced Materials and Processes, 161(6), 46–57.
Hedges, M., & Keicher, D. M. (2002), Laser Engineered Net Shaping - Technology and Applications, 
3rd National Conference on Rapid Prototyping, Rapid Tooling, and Rapid Manufacturing, Buckingham-
shire, UK, 2002, pp. 17-23.
Kuvella, A., & Dahotre, N.B. (2005). Review paper: surface modification for bio implant: the role of 
laser surface engineering, Journal of Biomaterial application, 20(1), 5-50.

203
Laser Additive Manufacturing in Surface Modification of Metals
﻿
Mahamood, R. M., Akinlabi, E. T., Shukla, M., & Pityana, S. (2012), functionally graded material: An 
overview, Proceedings of the World Congress on Engineering 2012 vol. III WCE2012, July 4 - 6, 2012, 
London, UK.
Paschotta, R. (2008). Encyclopaedia of Laser Physics and Technology. Wiley-VCH.
KEY TERMS AND DEFINITIONS
Additive Manufacturing (AM): It is manufacturing process whereby materials are added layer by 
layer to produce a component, directly from the drawings or image of the component.
Buy-to-Fly Ratio: Is the ratio of the weight of stock material to the weight of the final part produces 
from this stock. The balance in this weight is the weight if scrap produced during the manufacturing 
process of the part.
Functionally Graded Material: Materials with different properties across the cross section of the 
material. This belongs to a class of advanced materials that can withstand some extreme working condi-
tions where old materials will fail.
Laser Additive Manufacturing (LAM): An additive manufacturing process where the energy 
source is laser.
Laser Material Deposition (LMD): It is also a LAM process that uses powder or wire or both to 
fabricate components in layer wise manner. The laser first create a melt pool on the surface of the sub-
strate and the power or wire or both materials are delivered into the melt pool which upon solidification, 
the solid mass produce forms a track of the deposited materials. This process is repeated layer by layer 
until the part if completed.
Selective Laser Melting (SLM): A LAM process similar to SLS but completely melt the powder 
with laser instead of just fusing the powder as was done in SLS process.
Selective Laser Sintering (SLS): It is a laser additive manufacturing process that uses powder and 
laser energy to manufacture components. The powder spread on the build platform is fused by the laser 
scanning the platform according to the geometry of the part from the image of the part.
Surface Modification: Is a process of changing the surface properties of the bulk material in order 
to be able to withstand the service condition the surface will be subjected to.
Ti6Al4VTiC Composite: Is an important composite material that is used to improve wear resistance 
performance of a titanium alloy -Ti6Al4V-surface. Ti6Al4V is an important aerospace alloy but has poor 
wear resistance behaviour.
This work was previously published in Surface Engineering Techniques and Applications edited by Loredana Santo and J. Paulo 
Davim, pages 222-248, copyright year 2014 by Engineering Science Reference (an imprint of IGI Global).

204
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  11
DOI: 10.4018/978-1-5225-1677-4.ch011
ABSTRACT
Teaching descriptive geometry, in the context of this study, is characterized by the continuous investment 
in recognizing digital representation technologies which can enhance the didactic activities in architec-
tural training. This study describes this trajectory which includes the use of virtual reality, augmented 
reality and parametric modelling, as well as freehand drawing and the production of physical models 
both by automating the unfolding process and by digital fabrication processes of 3D printing and laser 
cutting. In addition to questioning the relevance and sustainability of the infrastructure needed to ensure 
the continuation of this trajectory, the potentialities identified in each of the learning activities that have 
been structure, are shown. Although these potentialities are specific to this context, it is considered that 
this type of record contributes to understand the issues being faced in teaching practices.
Transiting between 
Representation Technologies 
and Teaching/Learning 
Descriptive Geometry:
Reflections in an Architectural Context
Janice de Freitas Pires
Universidade Federal de Pelotas, Brazil
Luisa Dalla Vecchia
Universidade Federal de Pelotas, Brazil
Adriane Almeida da Silva Borda
Universidade Federal de Pelotas, Brazil

205
Transiting between Representation Technologies
﻿
INTRODUCTION
The history of teaching descriptive geometry certainly differs very little in different contexts of archi-
tectural training. Aiming specifically at developing spatial reasoning and skills to control and represent 
geometric form, this history was initially established with, perhaps, differences in the study strategy, 
whether or not it was applied directly to the architectural form. The bibliographic references of this ge-
ometry present the fundamental concepts of representation of point, line and plane. The teachers, geom-
eters with different professional backgrounds, declared insistently the comprehension of the projections 
of these geometric entities as a solution to any kind of problem regarding the representation of form in 
two or three dimensional space. This discipline is taught in the beginning of the architectural training 
and throughout its history registers a strong investment in the development of didactic strategies to ac-
celerate the abstraction abilities required by the intrinsic characteristics of the knowledge in question. 
The students come from different backgrounds and were often not stimulated to develop such abilities. 
In Brazil, this problem is registered specially in the GRAPHICA forums promoted by the Brazilian 
Association of Graphic Expression (ABEG), initially founded as Brazilian Association of Teachers of 
Descriptive Geometry and Technical Drawing. The reoccurrence of this problem, intensified even after 
the possibilities offered by graphics computing, is attributed to the changes in the structure of basic 
education on a national level. Kopke (1996) observed that since the Law of National Education Basis 
and Guidelines (LDB) from 1971, law n° 5.692, drawing started being addressed in the discipline of 
Artistic Education and lost its importance. Freshmen students, because of this lack of previous training 
in abstract geometric thinking, find it hard to understand.
In the context of this study, teaching descriptive geometry in the course of Architecture and Urban 
Design in the Federal University of Pelotas (UFPel), Brazil, the scenery reflects exactly the situation 
described above. Until 2011 the disciplines that taught descriptive geometry were offered by the Depart-
ment of Technical Drawing and Computational Graphics. Professors with different backgrounds such 
as architecture, art and engineering were responsible for all the representation disciplines offered to all 
the courses that needed them in the university. Due to the tradition of the department itself, the content 
taught in these disciplines did not give priority to applying it to the professional context. However, in 
some moments when the background of the professor was the same as the course for which the disci-
pline was being taught, some initiatives of applying directly to the professional context were registered.
The disciplines offered to Architecture students were called Descriptive Geometry III and IV, and 
they dealt with graphic-projective procedures aimed at the representation of polyhedral and curved 
surfaces respectively. Studies of two dimensional geometry were not included being offered only for 
the undergraduates in Arts. This is made worst by the guidelines for Brazilian basic education because 
the architecture students were studying spacial geometry without having the basis of 2D geometry. In 
Descriptive Geometry III these skills were developed from the methods of orthogonal projection with 
a single plane and from Monge projection and in Descriptive Geometry IV from Monge projection. In 
both, until 2010 the didactic activities were mainly based on drawing with traditional drawing instru-
ments. Thus, exploring form was limited, considering the time it took to build the representation of a 
surface on a single point of view, especially when it involved more complex surfaces.
Since 2010 these disciplines started including activities developed through digital graphic represen-
tation. In this way, taking advantage of automated methods of projection, accelerating form exploring 
and transforming in a dynamic and precise way. Furthermore, the disciplines advanced in the use of 

206
Transiting between Representation Technologies
﻿
interactive and digital systems of visualization, allowing the investment in variating parameters. With 
this, developing the comprehension of the logic of more complex surfaces in a playful way.
At this moment, as registered in Pires (2010), the presence of didactic activities applied specifically 
to the professional context of architecture, was made more intense. The activities using digital means 
were directed to the study and representation of example architectural buildings; this was configured as 
a strategy for acquisition of a vocabulary and geometric repertoire for the design practice. Such approach 
was based mainly in Potmann et al (2007), who from architectural references demonstrate that the digital 
representation technologies require a deep understanding of the geometric concepts and procedures to 
cover different levels of formal complexity.
In 2011, the School of Architecture carried out a curriculum change and the disciplines mentioned 
earlier were extinct and its contents were moved to two new disciplines which are part of a sequence of 
four. The contents from Descriptive Geometry III, centered in the study of polyhedral surfaces, were 
transposed to the discipline Graphic and Digital Geometry 2 (GGD 2) and the contents from Descriptive 
Geometry IV, centered in the study of curved surfaces, were transposed to GGD 3. Both in the same 
semesters in which they use to take place in the old curriculum, first and second semester respectively. 
However, GGD 2 is now offered in parallel to GGD 1 which was created specially to include the contents 
of planar geometry in the curriculum. GGD 4 received the contents form Perspective and Shadows and 
is now in the same semester as GGD 3, in the old curriculum these contents were taught in the third 
semester. Thus, in the current curriculum, the proposal is that in the first year of studying Architecture, 
the students build up a conceptual and procedural repertoire in planar and spacial geometry as a base 
for representing form.
In this context, teaching descriptive geometry has been characterized by the constant investment in 
the process of recognizing digital representation technologies capable of enhancing the didactic activi-
ties in architectural training.
This paper describes the process of restructuring the educational activities in the context of the 
discipline Graphic and Digital Geometry 3, covering, thus, the study of Descriptive Geometry in Ar-
chitecture in the period between the second semester of 2010 (still as Descriptive Geometry IV) and the 
first semester of 2015.
The didactic activities, in this trajectory of representing curved surfaces, include the use of virtual 
reality, augmented reality, parametric modeling, as well as free hand drawing and incrementing the pro-
duction of physical models either through automating unfolding processes or through digital fabrication 
techniques of laser cutting and 3D printing.
As well as this description, this study aims to include the problematic of the relevance and sustainability 
of the infrastructure needed to ensure the continuation of this trajectory, and to show the potentialities 
identified in each of the learning activities that have been structured. Although these potentialities are 
specific to this context, it is considered that this type of record contributes to understand the issues be-
ing faced in teaching practices in different educational institutions in the area of architectural training.
Materials and Methods
The study is of descriptive and reflective character and comprised the following stages: 1) description 
of the didactic practices involved in the disciplines of Descriptive Geometry IV and GGD 3, from the 
period immediately prior to the insertion of digital representation technologies until the first semester 
of 2015. The description is focused on the proposal of exploring the representation technologies as a 

207
Transiting between Representation Technologies
﻿
repertoire for the architecture students, specially addressing the physical and conceptual infrastructure 
as well as the instructor’s background needed for such approach; 2) discussion about the pertinence and 
viability to continue investing in exploring the technologies involved, facing the infrastructure available 
and the expectations in terms of acquired competences towards the design action in architecture.
The materials used for the description of the didactic practices are those registered in didactic materials 
in digital format, and accessible through the institution’s virtual learning environment (http://ava.ufpel.
edu.br/pre/course/view.php?id=423). Previous published works by the professors of the discipline are 
also used as well as complementary information about the development of the teaching practice given 
by the first two authors of this study since either one or the other was responsible for the discipline 
throughout the period of insertion of the digital technologies.
Description of the Didactic Practices
The didactic practices prior to the discipline’s reformulation, until 2010, did not include the use of digi-
tal graphic representation technologies. These practices, regarding the study of curved surfaces, were 
registered in Kremer (2008). They were based mainly in Rodrigues (1960), promoting exercises from 
traditional technologies using manual drawing instruments. The record i made by presenting a sequence 
of representation exercises using the Monge projection method through a printed material called exercise 
notebook. In this notebook, the exercises are proposed through classic descriptive geometry statements 
requiring that the students solve problems such as identifying points on surfaces, sections on surfaces, 
intersections and unfolding. The material also presents the final solution of each problem with a single 
image, without a description of the graphic procedures, considering, thus, that such image was used in 
support to the face to face classes. Overall, these representations were not associated to an architectural 
object. Figure 1 shows examples of how the solution to the proposed problems is presented in this 
material. The image on the left in Figure 1 shows the solution of an exercise in which a section is to be 
obtained from a semi-sphere with a vertical plane. The image on the right, in the same figure, shows 
Figure 1. Traditional exercises of representation of curved surfaces, Kremer (2008).

208
Transiting between Representation Technologies
﻿
the solution for unfolding a cone surface onto which a section had been previously carried out, thus the 
resulting curve is also unfolded.
Supported by the studies presented in Pottmann et al (2007), a posture of investigation regarding 
the implicit geometry in works of architecture was inserted in the discipline. Thus, from 2010, the 
representation exercises were preceded by the analysis of reference buildings exemplifying the use of 
the curved surfaces to be studied. The sequence and types of surfaces to be addressed were kept from 
that registered in Kremer (2008). The didactic design of the activities was always conditioned by the 
physical structure available for the use of computer laboratories and the professor’s familiarity with the 
digital graphic representation technologies. However, the purpose of integrating traditional and digital 
methods was always present.
The didactic activities structured to propose the transit between the representation technologies as a 
repertoire for the design practice in architecture are shown below.
The Moment of Inserting the Activities of Digital Graphic Representation
With the insertion of the activities using digital means, in the second semester of 2010, we sought to 
emphasize the importance of adopting an investigative posture. The didactic material meant to go from a 
passive attitude of representing, for example, any ellipsoid without considering the relationship between 
the parameters to determine the major or the minor axis of its generating ellipse, to an investigative attitude 
of what must these parameters be to generate the specific ellipsoid employed in the building analyzed.
Associating buildings to the types of curved surfaces a systemizing them in conceptual maps, works 
as provocative examples to motivate the students to seek for buildings with the purpose of illustrating 
the forms studied. This activity established itself as an investigation activity, of searching for informa-
tion, searching for registers made by the architects to explain their buildings to support this process of 
understanding form.
The maps in Figure 2 show examples of buildings with properly curved surfaces and ruled surfaces 
(Nunes et al, 2010; Pires et al, 2011).
Figure 3 illustrates one of the materials which supports the moments of exposing the traditional 
methods of representation, with the purpose of demonstrating the deduction of the drawings directly 
Figure 2. Associating architectural buildings to the curved surfaces according to the division made by 
Rodrigues (1960).

209
Transiting between Representation Technologies
﻿
on three dimensional digital models (Pires et al, 2011). These didactic materials were made using 3D 
Studio Max, which was used in class, both by professors and students, to demonstrate the operations.
In such materials it is possible to reproduce the sequence of drawing accompanied by a description 
which explains each procedure.
The use of conceptual free-hand drawing is highly encouraged during the process of geometric mod-
elling of the surfaces studied, at the same time emphasizing the potential of digital means for obtaining 
precision and speed in the transformations and changes in point of view.
Structuring didactic material to support the geometric modeling process of architectural buildings 
started with exercises developed in the context of scientific initiation (Nunes et al, 2010). At that mo-
ment it sought to establish methods for connecting the manual and digital procedures, based on the 
use of digital tools already used by the students, especially those with free access. Figure 4 illustrates 
this first material, which shows the modeling process of a building by Félix Candela, configured by a 
hyperbolic paraboloid, this surface is classified as ruled non-developable surface (Rodrigues, 1960). 
It is important to point out that this material was used in class for the first time with the support of the 
scientific initiation student who made it and who helped the students and instructors. From the first mo-
ments of applying the material an environment of reflection was established about the pertinence of the 
procedures and the logic of transposing the procedures from traditional methods to those employed by 
using a digital tool, in this case the software Sketch Up.
The sequence of images in Figure 5 show examples of modelling processes of architectural building, 
which appear in the didactic materials, to motivate the development of the activities of digital graphic 
representation.
Implicit to this activity is the proposal of the students building their own knowledge. They represent 
other buildings which they select themselves, seeking to reproduce the same logic demonstrated in the 
didactic materials regarding the use of the concepts, techniques and technologies studied. Thus, this 
activity counts on stages of identifying the buildings, geometric analysis and graphic representation 
using manual and digital means, investing in the development of different skills alongside the study of 
curved surfaces, as shown in Figure 6.
Figure 3. Didactic material about identifying the main elements in a revolved surface and its plane sec-
tions in the discipline of Descriptive Geometry IV/Faurb/UFPel, second semester of 2010.

210
Transiting between Representation Technologies
﻿
Figure 4. On the left, a photograph and the digital model of the building Capela Lomas de Cuernavaca, 
by Félix Candela; in the middle and on the right, stages of modeling the hyperbolic paraboloid of the 
building. Developed by Nunes et al, 2010. Source of the photograph:http://mcis2.princeton.edu/candela/
cuernavaca.html#
Figure 5. Digital models of architectural buildings proposed for the activity of geometric modeling of 
curved surfaces. Developed by Nunes et al, 2010 and Pires et al, 2011.

211
Transiting between Representation Technologies
﻿
Figure 6. Activities developed by the students of the disciplines of Descriptive Geometry IV and GGD3/
Faurb/UFPel, from 2010 to 2012. Source: Pires et al 2011; 2013

212
Transiting between Representation Technologies
﻿
These activities, shown in Figure 6, were developed from 2010 to 2012, and demonstrate how much, 
even in early stages of training, the students use both the conceptual repertoire and the digital technol-
ogy for their representations. This kind of activity has established itself and is still used without any 
interruptions until now.
It is considered that in these activities the forms studied from the buildings were effectively identi-
fied and learned, making the representation of these buildings into an autonomous activity done by the 
students, through the geometric modeling of the forms involved.
The Moment of Experimenting Augmented Reality Techniques
In the first semester of 2012 there was an intention to broaden the means of representation by insert-
ing advanced visualization technologies in augmented reality (AR). Starting from a proposal idealized 
by the professors of the discipline, this interest rose from the opportunity to experiment with students 
of scientific initiation and also with the support of a masters student taking teaching training in this 
discipline. AR was applied to make the visualization of sections on curved surfaces more agile. For 
each relative position between the represented surface and the section plane, due to the digital means 
being used at the time, there was an excessive amount of work for manipulating and positioning these 
elements. It involved several movements of translation and rotation of the section plane, as well as the 
need to determine adequate points of view for each visualization. Augmented reality made it possible 
to visualize several positions of the section plane and the curves generated from them, in a fast and 
dynamic way and from different points of view. This was possible by just moving the hand over a card 
which encodes the interaction between the physical means and the digital models.
This activity was developed by the master degree student and guided by the undergraduate students 
involved in the discipline. Figure 7 shows parts of the didactic material structured specifically for this 
activity and some images which register this technique being applied to view sections on curved surfaces.
Due to the limits of the technology available in the laboratory at FAUrb, and specially the limits 
of the students’ equipment, this activity only took place in the two semesters of 2012 and was not ap-
plied again until now due to the technological problems faced. The main problem was the compatibility 
between the cameras and the graphics card of the computers in the laboratory. Such situation will only 
be solved when there is a technological infrastructure which allows the inclusion of this activity in the 
discipline on a regular basis.
The Moment of Experimenting with Parametric Modeling
The experiment of using parametric modeling was carried out in the two semesters of 2012. Once again, 
the experience was promoted by a research study: a master degree student investigating the potential of 
inserting parametric modeling techniques in early stages of architectural training. The use of this technique 
was unprecedented in the School of Architecture (Faurb) and also to the professors of the discipline. 
Thus, the activity was designed in an initial stage of learning the concept itself. The hyperbolic parabo-
loid was chosen as the object to be represented due to the possibility of achieving such form starting 
with a planar form of a square or rectangle and parametrizing only a few elements (Brod et al, 2012).
As shown in Figure 8, following the same development logic as the previous activities, the build-
ing used as reference was the London Velopark. This representation required the understanding of the 

213
Transiting between Representation Technologies
﻿
relative positions of the generatrices of the hyperbolic paraboloid as well as the type of section made in 
order to obtain the final form of the building (Brod et al, 2012; Brod, 2014).
In the second experiment, developed in the second semester of 2012, modeling a generic hyperbolic 
paraboloid was proposed, and from this form the students were to explore the specific configuration of 
buildings, previously selected, by manipulating the parameterized geometric elements. Figure 7 also 
shows examples of the supporting material of this second activity.
The Moment of Insertion of Digital Unfolding Tools 
and Building Physical Models with Paper
The unfolding techniques of curved surfaces, part of the discipline´s program, were studied until 2011 
through traditional techniques of representation, restricted, thus, to the manual drawing of the develop-
ment of the surfaces. Unfolding techniques were studied for developable surfaces such as the cone and 
cylinder, and also for non-developable surfaces by approximation (Rodrigues, 1960). In 2012 these 
activities were associated to the representation of buildings, requiring paper models to be made from 
the development of the surfaces, still by manual techniques. At this moment, regarding the unfolding 
process mainly of conic and cylindrical sectioned surfaces. The advancement to building paper models 
of more complex forms, such as ellipsoids, paraboloids, and hyperboloids, was limited due to the time 
it took for the development of such representations.
From the first semester of 2013, seeking to include activities with more complex geometric surfaces, 
we started investing in unfolding activities supported by digital technology. The models produced using 
Figure 7. In the first line, didactic material developed to support the activities of viewing sections on 
curved surfaces using AR; in the second line, experiments of viewing sections on curved surfaces using 
AR. Developed by Sopeña et al, 2012.

214
Transiting between Representation Technologies
﻿
Figure 8. In the first line, development scheme of the parametric modeling activity of the London Ve-
lopark; from the second line, supporting didactic material, including the example of other buildings 
configured by hyperbolic paraboloid. Developed by Brod et al, 2012

215
Transiting between Representation Technologies
﻿
Sketch Up were used with a tool for automating the unfolding of the surfaces, the plugin Unfoldtool 
(http://sketchuptips.blogspot.com.br/2007/08/plugin-unfoldrb.html). With this, it was possible to advance 
to the representation of the other surfaces to include all those classified as curved surfaces. Didactic 
materials were structured to support the development of the unfolding activities and the construction of 
the paper models of such surfaces (Figure 9).
As mentioned in Pires et al (2014), the method was refined from one semester to the next; the prob-
lems detected in the first version of the activity were categorized, involving the adjustment of modeling 
parameters of the generatrix and directrix, as well as identifying strategies for using the structural ele-
ments of the model to guarantee the exact curvature desired in the paper models as shown in the second 
last image in Figure 9.
Figure 10 shows the models developed when this activity was first applied in the first semester of 
2013, and Figure 11 shows the models developed after using some strategies to improve the process of 
assembly of the physical models, in the second semester of the same year.
The Moment of Insertion of Parametric Modeling
In the second semester of 2014 parametric modeling of curved surfaces was inserted in the discipline to be 
developed simultaneously to the other activities. For this, didactic materials were developed which gave 
examples of the structure of the parametric models of all the curved surfaces studied in the discipline. 
As each surface is studied in the discipline, the student is asked to model that surface using parametric 
modeling software, in this case Grasshopper. The study starts with simple surfaces such as cylinders, 
cones and spheres, attributing a small amount of parameters. The amount of parameters increase as the 
surfaces became more complex throughout the semester. Although some examples are shown in the 
didactic material, the students are encouraged to carry out the parametric model in their own way ac-
Figure 9. Stages proposed for the representation of the unfolding process of curved surfaces. Source: 
Pires et al, 2014.

216
Transiting between Representation Technologies
﻿
cording to which parameters they want to vary. Figure 12 shows the materials regarding the parametric 
representation of some of the surfaces.
The materials structured in the first moment of experimentation are also used and are being incremented 
every semester. However, the proposed activity and developed in the second half of 2013, to explore 
the specific configuration of the surfaces of works of architecture previously selected by the manipula-
Figure 10. Activities developed by the students in the discipline GGD3/Faurb/UFPel, first semester of 
2013. Source: Pires et al, 2014

217
Transiting between Representation Technologies
﻿
tion of parameterized geometric elements of a generic hyperbolic paraboloid, did not have continuity 
of application. It is also important to point out that the current professors of the discipline are directly 
involved with research regarding the insertion of parametric design in early stages of architectural train-
ing (VASCONSELOS et al, 2014) and the discipline also counts on scientific initiation students who 
assist in the development of the didactic materials.
Figure 13 shows the activities of parametric modeling carried out by the students. Although the 
activities were developed with the support of a “modeling script”, the students were encouraged to use 
their own solution and, when they felt secure enough, propose alternatives to the models
The Moment of Experimenting with Digital Fabrication
The experiments with digital fabrication in the context of this discipline are still initial. So far it has 
occurred in three ways. The first was in parallel to the moment of experimenting with techniques of 
parametric modeling in 2012. It happened by bringing to the context of the classroom a demonstration 
of the processes of 3D printing and laser cutting of a hyperbolic paraboloid. The 3D printing technique 
used was by the additive method of plastic layer deposition. The second, in the second half of 2014 
consisted in making 3D printed models available as didactic materials, by bringing to the physical the 
exercise carried out with augmented reality, illustrated in Figure 6. 3D models of the sectioned torus 
were produced to show the different kinds of curves produced by such sections, like a 3D puzzle. The 
third consists in the automation of the cutting process of the unfolded surfaces using laser cutting. Such 
experiments are shown in Figure 14.
Figure 11. Activities developed by the students in the discipline GGD3/Faurb/UFPel, second semester 
of 2013 and first semester of 2014. Source: Pires et al, 2014

218
Transiting between Representation Technologies
﻿
Reflections
The description of the different activities developed throughout the configuration trajectory of the dis-
cipline GGD 3 shows the goal of transiting between representation technologies. What has this transit 
represented to the context in question? What has it represented specifically to the students involved? What 
has it represented to the scientific initiation students, and to the professors? What is the pertinence and 
Figure 12. Didactic materials to support the parametric modeling of curved surfaces

219
Transiting between Representation Technologies
﻿
sustainability of each of the technologies used when facing the discipline´s objectives? This report was 
provoked by formulating these and so many other questions which cannot all be answered immediately.
Considering the records gathered to this moment, of the didactic materials, the students’ activities and 
the perception of the professors involved, some issues that were outlined with this report are addressed. 
Although, initially, the development of broader questions are highlighted, such as the consequences of 
this kind of training on the didactic practices in the design disciplines which is being studied by a master 
degree student. The discussion is centered on the developments perceived by the professors exclusively 
in the context of the discipline of GGD3.
As outline in the methods, a reflection is made regarding the pertinence and viability of continuing 
the investment in exploring the technologies involved, considering the physical and conceptual infra-
structure available and the background of the professors. Also, and specially, considering the outcome 
in terms of skills learned by the students towards the action of architectural design.
Conceptual and Physical Infrastructure and Professors’ Background
With the description of the activities the kind of physical infrastructure available was also highlighted. In 
this context, a classroom environment which guarantees the possibility of working with both traditional 
and digital means is not yet available. The space available is either configured for manual drawing, 
with large tables, or computer labs where there is only space for the computers. However, this scenario 
is changing due to the students’ attitude who, when faced with the lack of infrastructure, take on the 
Figure 14. Models of a hyperbolic paraboloid using digital fabrication techniques: on the left and 
center, model of the London Velopark by fused deposition modelling; on the right, generic hyperbolic 
paraboloid. Source: Brod, 2014
Figure 13. Activities developed by the students in the discipline GGD3/Faurb/UFPel, second semester 
of 2014

220
Transiting between Representation Technologies
﻿
responsibility of bringing their own computers whenever possible. Furthermore, they share amongst 
them to guarantee the development of the activities regardless of the availability of the institution’s 
equipment. Thus, the activities which involve only virtual reality are daily present in the discipline. In 
Figure 15 it is possible to observe the kind of representation technologies which were being inserted in 
Figure 15. Outlining of the gradual addition of the representation techniques in the discipline GGD3/
FAUrb/Ufpel, from 2010 to 1015

221
Transiting between Representation Technologies
﻿
the discipline and the activities linked to them, the activity using only virtual reality, specially using the 
free software Sketch Up, appears more.
The same figure shows that the activity involving the use of augmented reality was discontinued due 
to the problems with the equipment available. However, given the current condition of being able to 
count on the student’s equipment and on the mobile devices which have also incorporated AR, such as 
mobile phones and tablets, there is a perspective of resuming this activity.
The infrastructure for the development of activities using digital fabrication techniques is still being 
weighed. 3D printing is used only for demonstrations and in the production of models to support research 
projects. The infrastructure available is still not enough for the production on an undergraduate level. 
There is only one machine with a small printing area. However, for the production of models using laser 
cutting the infrastructure is adequate, depending only on working out the management of the equipment 
and resources in the context of a public institution.
Thus, it is considered that in terms of equipment there is the perspective of guaranteeing the condi-
tions for the development of the didactic activities described.
Regarding the conceptual bases, to support the activities, the strategies to guarantee the didactic ac-
tion were also shown in the descriptions. These actions depend not only on the professor’s background 
but also on creating a team in which each acts as an active member in the learning process. From the 
first moments and for most of the activities, due to the training differences, a partnership was established 
between the professors and students, in order to combine conceptual knowledge, characteristic of the 
professors, to the speed of learning digital technologies characteristic to the current generation of students.
An investigative dynamic for the selection of appropriate buildings to study was also highlighted. 
Every semester new challenges are imposed, especially due to the diversity of forms and new concepts 
present in contemporary architecture, stimulating the constant updating of the professors. Considering 
this, the sustainability of the investigative proposal depends on the professors’ attitude of being open to 
the unexpected, constantly broadening the conceptual bases in geometry to understand and represent form.
This attitude is also relevant in regard to training in order to have criteria in selecting the digital 
technologies to be used. This trajectory of continuous transit between representation technologies has 
allowed us to identify the specific potential of each tool and the adequate didactic moment to use them. 
In this direction, the recent insertion of parametric modeling in parallel to the traditional representation 
methods (freehand drawing) and other digital methods, has required a great conceptual knowledge form 
the professors, provoking an advancement even in the hand drawing techniques. The logic of making 
the parameters explicit has been incorporated in the didactic discourse of all the other methods of rep-
resentation. This seems to be, in the context of this experience, the biggest differential in the didactic 
action and, therefore, in the training of the professors involved.
Another fundamental aspect when evaluating the daily classroom routine regards the continuous 
education environment in actual architecture which has become established in contrast to when the 
geometric forms were studied without being applied to architecture. The problems of representing 
complex forms selected by the students has motivated the understanding of the architectural discourses, 
introducing the students in history and theory of architecture, and requiring of the geometry professors 
such interdisciplinary approach.
With this dynamic it was possible to build a repertoire of buildings associated to the types of surfaces 
studied. This repertoire is broadened and many times the discussions are resumed regarding the perti-
nence of the discourses which associate a certain building to a determined curved surface, deepening the 
knowledge about the buildings and consequently about the formal strategies of the architects involved.

222
Transiting between Representation Technologies
﻿
Simultaneously, didactic materials were structured, registering a theoretic discourse and the traditional 
representation techniques based on the analysis of architectural buildings.
Workshops of digital graphic representation of the buildings were also structured and offered by 
scientific initiation students and master degree students. Such workshops promoted the education of 
the professors considering that each workshop established a moment of dialog between techniques and 
concepts, scientific and technological languages.
The gradual insertion of different representation techniques reached the purpose of promoting the 
recognition of different resources which help to understand and create form, without substituting one 
for the other throughout the activities.
It is important to point out that it was not possible to guarantee an uniform trajectory from one semester 
to another regarding the use of some representation technologies, partially due to matters of infrastruc-
ture, as pointed out before, and partially due to the dynamic of the professors, which is affected by the 
difficulty to guarantee experienced professors who can work with all the technologies experimented. 
Examples of this are the augmented reality which was no longer used due to the difficulties faced with 
the technological devices, and parametric modeling which had an interruption for three semesters until 
the moment in which a professor with transit in such technology took over such activities in the second 
semester of 2014.
A collaborative process of production of didactic materials was also established, highlighting the 
importance of the university being in the middle of developing a repository of learning objects. Such 
materials, like those developed by the professors, on one hand, promote the digital graphic representa-
tion of architectural buildings and can support both face to face and distance didactic moments since 
they include the didactic discourse to promote autonomous learning; on the other hand, the materials 
made by the students, broaden the collection of buildings which illustrate the use of the forms studied.
The Skills for the Action of Design in Architecture
The insertion of the techniques and technologies mentioned made it possible for the students to develop 
different skills, which go from the knowledge about forms in architecture to the representation of complex 
forms from techniques of geometric generation and transformation of curved surfaces.
An investigative attitude has established itself in the discipline, by means of learning trajectories 
which involve representing architectural buildings and not only the explanation of generation laws and 
projective procedures. Such trajectories add a component of understanding architectural design strategies.
The students learned the digital technologies through the techniques of representing the buildings 
associated to the geometric forms studied; an understanding of the traditional representation procedures 
also developed by being constantly associated to the insertion of digital representation techniques.
The goal of enabling the students to acquire an analysis attitude from the recognition of a geometric 
repertoire was reached. At the same time, architectural knowledge was acquired.
Even in cases in which we could not guarantee that the students learned adequately the concepts 
and the technology addressed, such as the case with parametric modeling or even augmented reality, it 
is considered pertinent that, in such an initial stage of architectural training, the student recognizes the 
tools which, in a future moment, may be adequate to solving design problems.

223
Transiting between Representation Technologies
﻿
It can be said that just by having used the technologies, even if it was just as a didactic strategy, as 
was the case of demonstrating the sections on a torus, such strategy will be useful for the student of 
architecture to have this logic of studying form to understand it as a repertoire, a resource which the 
architect needs to expose ideas.
Final Considerations
This reformulation included a set of theories, technologies and techniques of graphic representation, with 
the specific purpose of training students to solve the architectural problems which involve the activity 
of graphic representation.
From this technological bias along with the report of the didactic activities of a descriptive geometry 
discipline, aimed at the study of curved surfaces in architectural training, the teaching strategies adopted 
were questioned.
The contextual opportunities allowed the transit between technologies from hand drawings, trans-
posing the drawings to digital means, obtaining three dimensional models to the study of virtual and 
augmented reality, the investment in physical models by 3D printing and laser cutting, to the use of 
parametric modeling.
The moments of experimenting and investigating established themselves in the classroom context. The 
unbreakable association shown in this trajectory, between teaching and research, in its different levels, 
undergraduate, post-graduation, masters and Phd, is what attributes the sustainability to these didactic 
practices. This has guaranteed the update of the professors to continue these actions.
Acting in urgency and deciding in uncertainty (Perrenoud, 1991), characteristic of the teaching ac-
tion, certainly was present during the moments described. On one side the appeal of the technological 
evolution and the students’ curiosity about it. On another, the motivation in recognizing new resources 
which can help in learning what is being addressed.
Each experience added a differential both to the act of teaching and of learning.
Noteworthy is also that parametric modeling and techniques related may be a future direction of 
research.
ACKNOWLEDGMENT
To the professors Felipe Etchegaray Heidrich (UFPel), especially for the consultancies on augmented 
reality and Ana Paula de Andrea Dametto (UFPel), regarding the consultancies to improve the produc-
tion of physical models.
To the scientific initiation students who helped during this trajectory: Cristiane Nunes (2010-13); Tás-
sia Borges de Vasconselos (2010-13); Valentina Toaldo Brum (2014-15); Larissa Perdezolli (2014-14).
To the volunteer students and undergraduate and outreach scholarship students: Gabriela Gonzales 
Peronti (2013-2014); Mariana Reami Osmaré (2013-14).
To the master degree students who took teaching training in this discipline: Sirlene Sopeña, Gustavo 
Brod, Tássia Vasconselos and Cristiane Nunes.
To the research support organs: CNPQ, CAPES and FAPERGS.
To the university’s support organs: undergraduation, outreach, culture and research pro-rectories, for 
the scholarships awarded between 2010 and 2015.

224
Transiting between Representation Technologies
﻿
To the ALFA program from the European Comunity (ALFA GAVIOTA Project), to FINEP (MORAR.
TS Project) for the financial support of technological infrastructure and of the digital fabrication labora-
tory, and to the research project PROBARQ, in which the didactic materials regarding the insertion of 
digital graphic representation techniques in architecture were structured.
To the all architecture students, FAUrb / UFPel, which passed through the discipline among the 
second half of 2010 and the first of 2015, contributed to the work shown here.
REFERENCES
Brod, G. A. (2014). Parametric design in the early stages of training in architecture: technological, 
conceptual, methodological issues. (Master Dissertation). Federal University of Pelotas.
Brod, G. A., Pires, J. F., & Borda, A. (2012). An experiment for introducing the concept of generative 
processes in early stages of architectural education. In Proceedings of the 16th Iberoamerican Congress 
of Digital Graphics.
Kopke, R. C. M. (n.d.). Desenho e escola. In Proceedings of I International Graphic Engineering Congress 
in Arts and Design and XII National Symposium of Descriptive Geometry and Technical Drawing. UFSC.
Kremer, R. T. (2008). Exercícios de Geometria Descritiva. Curvas e Superfícies. Editora e Gráfica 
Universitária UFPel.
Nunes, C. S., Pires, J. F., & Borda, A. (2010). Aquisição de repertório geométrico a partir de proces-
sos de modelagem de obras de Candela e Calatrava. In: XIX Congresso de Iniciação Científica Ufpel, 
Pelotas (Brasil).
Nunes, C. S., Portella, A., & Borda, A. (2014). Intenção ou Intuição: Geometria como Repertório para a 
Estruturação de Método de Projeto. In: XVI ENPOS Encontro de Pós-graduação UFPel. PRPPG UFPel, 
Pelotas (Brasil).
Perrenoud, P. (1991). Ensinar: agir na urgência e decidir na incerteza. Porto Alegre, Brasil: Artes Médicas.
Pires, J. F. (2010). The Construction of Geometric Vocabulary and Repertoire for Architectural Design. 
(Master Dissertation). Federal University of Pelotas.
Pires, J. F., Nunes, C. S., Vasconselos, T. B., & Borda, A. (2011). Trajetórias de Geometria na Arquitetura. 
In: IX International Graphic Engineering Congress in Arts and Design and XX National Symposium of 
Descriptive Geometry and Technical Drawing. Escola de Belas Artes, Rio de Janeiro (Brasil).
Pires, J. F., Nunes, C. S., Vasconselos, T. B., Borda, A., & Heidrich, F. E. (2013). Processos de Ensino e 
Aprendizagem da Geometria de Superfícies Curvas em Arquitetura e Design. In: X International Graphic 
Engineering Congress in Arts and Design and XXI National Symposium of Descriptive Geometry and 
Technical Drawing. Universidade Federal de Santa Catarina, UFSC, Florianópolis (Brasil).
Pires, J. F., Peronti, G. G., Borda, A., & Osmaré, M. R. (2014). Parameterized flattening and Kirigami: 
Possible Approximations to the Study and the Representation of Curved Surfaces. In Proceedings SIG-
raDi2014 “Design in Freedom”. Montevideo, Uruguay: Facultad de Arquitectura. Universidad de la 
República/UDELAR.

225
Transiting between Representation Technologies
﻿
Pottmann, H., Asperl, A., Hofer, M., & Kilian, A. (2007). Architectural Geometry. Bentley Institute Press.
Rodrigues, A. (1960). Geometria Descritiva: Projetividades, Curvas e Superfícies. Ao Livro Técnico 
Ltda 1a ed. Rio de Janeiro (Brasil).
Sopeña, S. M., Pires, J. F., Heidrich, F., & Borda, A. (2012). A Realidade Aumentada como Estratégia 
de Visualização Dinâmica de Seções em Superfícies Curvas. In: IV Congreso Internacional de Expresión 
Gráfica e IX Congreso Nacional de Profesores de Expresión Gráfica, 2012, La Plata. Gráfica: Gráfica 
del Diseño: tradición e innovaciones. La Plata.
Vasconcelos, T. B., Vecchia, L. F. D., & Borda, A. (2014). Aparametrização como experiência prévia 
para a estruturação de métodos projetuais em arquitetura. In Proceedings SIGraDi2014 “Design in 
Freedom”. Montevideo, Uruguay: Facultad de Arquitectura. Universidad de la República/UDELAR.
ADDITIONAL READING
Borda, A., Félix, N., & Pires, J. (2008). Digital Construction and Reconstruction as a Methodology for 
the Teaching/Learning of Geometric Form Representation. In: 13th International Conference on Geom-
etry and Graphics, Dresden, Germany. Disponível em: <http://icgg2008.math.tu-dresden.de/abstracts/
Borda.pdf > Acesso em: 23 set 2008.
Burry, J., & Burry, M. (2010). The New Mathematics of Architecture. London: Thames & Hudson.
Burry, M., & Murray, Z. (1997). Architectural Design Based on Parametric Variation and Associative 
Geometry.Challenges of the Future [15th eCAADe Conference Proceedings]. Vienna (Austria).
Pereira, D. C. VAZ, C. E. V., (2013). Parametrismo e ensino de geometria– as superfícies de Felix 
Candela. In: Simpósio Nacional de Geometria Descritiva e Desenho Técnico e International Conference 
on Graphics for Arts and Design, 2013, Florianópolis. Tecnologia e Arte para Inovação. Florianópolis. 
01-09. Acessado em: http://wright.ava.ufsc.br/~grupohipermidia/ graphica2013/anais.html
Reig, C. (1999). La geometría en la obra de Eduardo Torroja. Revista de Obras Públicas. Dez.1999, n. 3393, 
p. 15-31. Disponível em: <http://ropdigital.ciccp.es/public/detalle_articulo.php?registro=18306http://
ropdigital.ciccp.es/pdf/publico/1999/1999_diciembre_3393_02.pdf > Acesso em: dez 09
Woodbury, R. (2010). Elements of parametric design. New York: Routledge.
KEY TERMS AND DEFINITIONS
3D Printing: Also known as additive manufacturing is a process used to make a three-dimensional 
object without the use of dies, molds or machining. In 3D printing additive processes are used, in which 
successive layers of material are laid down under computer control. These objects can often be of virtu-
ally any shape or geometry, and are produced from a 3D model or another electronic data source. 3d 
printing thus enables to production of parts which would not be possible with traditional manufacturing 
techniques.

226
Transiting between Representation Technologies
﻿
Augmented Reality: Augmented Reality is an advanced display technology, defined by Tori, Kirner 
and Ciscouto (2006, p. 10) as “[...] enrich the real environment with virtual objects using a technological 
device, working in real time.”
Complex Geometries: In mathematics, complex geometry is the study of complex manifolds and 
functions of many complex variables. Application of transcendental methods to algebraic geometry 
falls in this category, together with more geometric chapters of complex analysis (Huybrechts, 2005). 
Geometry lies at the core of the architectural design process. It is omnipresent, from the initial form-
finding stages to the actual construction. Modern constructive geometry provides a variety of tools for 
the efficient design, analysis, and manufacture of complex shapes. This results in new challenges for 
architecture. However, the architectural application also poses new problems to geometry. Architectural 
geometry is therefore an entire research area, currently emerging at the border between applied geom-
etry and architecture. Complex geometries in architecture include central concepts on freeform curves 
and surfaces, differential geometry, kinematic geometry, mesh processing, digital reconstruction, and 
optimization of shapes (Pottman et al, 2007).
Descriptive Geometry: The descriptive geometry is a branch of geometry that aims to accurately 
represent objects of three dimensions on a two dimensional plane. For that draws on the projection of 
the objects, which consists of passing the projecting straight object, which intersect the projection plane, 
where it creates a projection of the object. This projection can be made according to several projection 
methods, which differ according to the number of projection planes used and the different characteristics 
of projecting straight.
Developable Surfaces: Forms are called ‘developable’ or ‘single curved’ when they can be cre-
ated through ordinary bending of a planar surface without stretching, cutting or wrinkling the material. 
These surfaces are characterized by only bending in one direction at a time, like the cylinder or the cone. 
Developable surfaces are useful because they allow round forms to be made out of flat materials like 
plywood, sheet metal or cloth (Kolher, 2010). For non-developable surfaces is studied your approximate 
planning, and one of the processes used is approximated by spherical zones. The zones that make up the 
spherical surface are not developable hence these zones are developed by means of a sufficiently rough 
flat surface (Kremer, 2008).
Digital Fabrication: Digital fabrication is a type of manufacturing process where the machine used is 
controlled by a computer. Are classed according to the processes acting upon the physical and/or chemical 
properties of the materials used. These are: additive procedures, subtractive procedures and formative 
procedures (according Brankolevic and Malkawi, 2003), and joining procedures (Schodek, 2005).
Laser Cutting: Is a non-contact process which utilizes a laser to cut materials, resulting in high 
quality, dimensionally accurate cuts. The process works by directing the laser beam through a nozzle 
to the work piece. A combination of heat and pressure creates the cutting action. The material melts, 
burns, vaporizes, or is blown away by a jet of gas, leaving an edge with a high-quality surface finish.
Parametric Design: For parametric design is meant the generation process forms whose constitu-
ent elements are not geometrically determined in a static manner, but using variables whose values are 
specified in each case. In parametric design systems elements can be changed later without that overall 
consistency is changed. This requires the specification of the restrictive properties that affect selectively 
some of the elements of the form or the relationship that some have with each other (Monedero, 2000).

227
Transiting between Representation Technologies
﻿
Virtual Reality: It is an advanced interface technology between a user and a computer system. The 
goal of this technology is to recreate the most of the sense of reality for an individual, leading him to 
adopt this interaction as one of its temporal realities. For this, this interaction takes place in real time, 
using computational techniques and equipment to assist in expanding the user presence feeling.
This work was previously published in the Handbook of Research on Visual Computing and Emerging Geometrical Design Tools 
edited by Giuseppe Amoruso, pages 250-273, copyright year 2016 by Information Science Reference (an imprint of IGI Global).

228
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  12
DOI: 10.4018/978-1-5225-1677-4.ch012
Guy Littlefair
Deakin University, Australia
ABSTRACT
In this study, the tool wear and surface integrity during machining of wrought and Selective Laser Melted 
(SLM) titanium alloy (after heat treatment) are studied. Face turning trails were carried out on both the 
materials at different cutting speeds of 60,120 and 180 m/min. Cutting tools and machined specimens 
collected are characterized using scanning electron microscope, surface profiler and optical microscope 
to study the tool wear, machined surface quality and machining induced microstructural alterations. It 
was found that high cutting speeds lead to rapid tool wear during machining of SLM Ti-6Al-4V materials. 
Rapid tool wear observed at high cutting speeds in machining SLM Ti-6Al-4V resulted in damaging the 
surface integrity by 1) Deposition of chip/work material on the machined surface giving rise to higher 
surface roughness and 2) Increasing the depth of plastic deformation on the machined sub surface.
1. INTRODUCTION
Additive manufacturing(AM) is gaining huge attention in the manufacturing industries due to its high 
productivity and freedom of design capabilities (Gibson, Rosen, & Stucker, 2010). Currently, titanium 
components are preferred to be manufactured using AM technology rather the conventional technology 
Tool Wear and Surface 
Integrity Analysis of Machined 
Heat Treated Selective 
Laser Melted Ti-6Al-4V
Manikandakumar Shunmugavel
Deakin University, Australia
Ashwin Polishetty
Deakin University, Australia
Moshe Goldberg
Deakin University, Australia
Rajkumar Prasad Singh
Kalyani Center for Technology and Innovation, 
India

229
Tool Wear and Surface Integrity Analysis 
﻿
which is energy intensive and time consuming. However, AM components often requiring post machining 
operations for superior surface finish and improved product quality (Horn & Harrysson, 2012).
Speaking about the titanium alloys, it is regarded as one of the “difficult to machine” materials due 
to its several inherent properties. Low thermal conductivity and high chemical reactivity of this material 
often leads to high tool wear due to concentration of the heat in the deformation zones during machin-
ing. In addition, the high strength at elevated temperature, low modulus of elasticity and shear instabil-
ity in chip formation further impairs its machinability (Davim, 2014). As titanium alloys are used for 
components requiring highest reliability, it is necessary to maintain the surface integrity. The surface 
integrity of the titanium alloys is easily affected because of the poor machinability of the titanium al-
loys. Heat generated during titanium machining acts as a major source of damage and results in surface 
and sub-surface alterations including plastic deformation, micro cracking, phase transformations and 
residual stress effects (Che-Haron, 2001; Che-Haron & Jawaid, 2005; Ginting & Nouari, 2009; Mantle 
& Aspinwall, 2001; Che-Haron, 2001; Mantle & Aspinwall, 2001). Having said these, there are not 
many literatures available, that discusses the machining of AM titanium components. However, some 
few literatures discuss the machinability and tool wear during machining of additive manufactured tita-
nium alloys. A. a. b. u. i. Bordin, Bruschi, Ghiotti, Bucciotti, and Facchini (2014) (Bordin et al., 2014) 
compared the machinability characteristics of wrought and Electron Beam Melted (EBM) titanium al-
loys. They also studied and analyzed the tool wear in cryogenic machining of EBM Ti-6Al-4V (Bordin, 
Bruschi, Ghiotti, & Bariani, 2015). From their studies, it was found that adhesion was the main wear in 
semi finish turning of EBM Ti-6Al-4V using carbide tool.
This research has been undertaken by realizing the importance of tool wear, surface integrity and 
the lack of works related to effect of machining on surface integrity of additive manufactured titanium 
alloys. Face turning trails were carried out at different cutting speeds for specific number of machining 
passes and the tool wear, machined surface and machined sub-surface are analysed to study to the effect 
of machining on selective laser melted (SLM) Ti-6Al-4V.
2. EXPERIMENTAL PROCEDURE
Hollow cylindrical samples of wrought and additive manufactured Ti-6Al-4V were used as work materi-
als in this research. A hollow cylindrical specimen of height 60 mm, outer diameter 50 mm and inner 
diameter 25 mm was fabricated using a SLM 125 metal additive manufacturing machine (as shown in 
Figure 1). Important process parameters such as meander build style, 0.2 mm laser focus diameter and 
425 mm/s laser scan speed were used in fabricating this hollow cylinder. The sample was then heat treated 
in a vacuum furnace to 730oC and cooled slowly to 1) relieve the residual stresses and 2) soften and 
improve the materials ductility. Face turning machining operation was then performed on the material 
in a Nakumara tome CNC machine. TiAlN+TiN PVD coated rhombic carbide tools with positive rake 
angle and a tool nose radius of 0.8 mm was used for the machining tests. The designation of the cutting 
tool used is DNMG 150608-TF.The material was machined at different cutting speeds of 60,120 and 
180 m/min using a constant feed rate of 0.1 mm/rev and depth of cut of 0.5 mm respectively. The tool 
specimens and the machined surface were collected and preserved after specific number of machining 
passes to study the tool wear and surface integrity. The tool wear and surface characteristics was measured 
and characterized using optical and scanning electron microscope. The surface roughness was measured 
using Alicona Infinite Focus 3D optical surface profilometer.

230
Tool Wear and Surface Integrity Analysis 
﻿
2.1. Presentation of the Work Materials
Microstructure of the selective laser melted titanium alloys consists of columnar prior β grains as a result 
of thermal history experienced by the layers (Sercombe, Jones, Day, & Kop, 2008; Thijs, Verhaeghe, 
Craeghs, Humbeeck, & Kruth, 2010; Thijs et al., 2010). It was found that the columnar grains transform 
in to a fully martensite α’phase upon cooling to room temperature (Vilaro, Colin, & Bartout, 2011). 
Peculiar microstructure of the SLM titanium alloys and its martensite phase regimes gives rise to its high 
strength and hardness compared to conventionally produced wrought alloys. However, the martensite 
microstructure limit the ductility of SLM Ti-6Al-4V as compared to the conventionally made components 
(Shunmugavel, Polishetty, & Littlefair, 2015). In order to the remove the residual stresses and improve 
the materials ductility, the components are often heat treated. Vilaro et al. (2011) and Vrancken, Thijs, 
Kruth, and Van Humbeeck (2012) found that heat treatment of SLM titanium alloys have improved 
ductility significantly as compared to “as built” specimens. Microstructure of the wrought Ti-6Al-4V 
alloy and SLM Ti-6Al-4V after heat treatment are shown in Figure 2 and Figure 3 respectively. Wrought 
Ti-6Al-4V exhibited an equiaxed microstructure. SLM Ti-6Al-4V after heat treatment exhibited fully 
lamellar microstructure. The corresponding mechanical properties of the work materials used in this 
research such as yield strength (σ0.2), ultimate tensile strength (U.T.S), percentage of elongation (ε), 
hardness (H.V) are listed in Table 1.
3. RESULTS AND DISCUSSIONS
Face turning operations are carried out for a number of machining passes on wrought and SLM Ti-6Al-
4V at various cutting speeds. At high cutting speeds of 180 m/min, the cutting tests were stopped due to 
the catastrophic failure of the cutting tool. The cutting tools and the machined specimens collected after 
the specified number of passes (as shown in Table 2) are used to study the tool wear, surface roughness 
and microstructural alterations of the machined sub surface.
Figure 1. Hollow Ti-6Al-4V specimen fabricated using selective laser melting technology

231
Tool Wear and Surface Integrity Analysis 
﻿
3.1. Tool Wear
Cutting tools used in machining SLM Ti-6Al-4V exhibited higher wear compared to the tools used for 
machining the wrought alloy. Maximum flank (VB max) wear observed in machining these materials 
at various cutting speeds are shown in Figure 4. It can be seen from the graph that tool wear increased 
rapidly with the cutting speeds during machining of SLM Ti-6Al-4V. At 180 m/min, the maximum flank 
wear was found to be highest leading to the catastrophic failure of the cutting tool. Tool wear was almost 
three times higher for machining SLM Ti-6Al-4V at 180 m/min as compared to the wrought titanium 
alloy. The rapid wear of the cutting tools at high cutting speeds might be due to the high cutting and 
high cutting pressures combined with the poor thermal conductivity of titanium alloys that accelerates 
tool wear.
Figure 2. Microstructure of mill annealed wrought 
Ti-6Al-4V (“as received”) used in this study
Figure 3. Microstructure of SLM Ti-6Al-4V after 
annealing heat treatment (730oC/2hrs/furnace 
cooling)
Table 1. Mechanical properties of work materials used in this research
Materials
Heat treatment conditions
σ0.2
(MPa)
U.T.S 
(MPa)
ɛ 
(%)
Hardness 
(H.V)
Wrought 
Ti-6Al-4V
730/2hrs/air cooling
948
994
21
324
SLM Ti-6Al-4V
730/2hrs/furnace cooling
964
1041
7
390
Table 2. Details of the machining experiments
Material
60 m/min
120 m/min
180 m/min
Wrought Ti-6Al-4V
18
9
18
SLM Ti-6Al-4V
18
9
2*
*Machining tests were stopped due to the tool failure.

232
Tool Wear and Surface Integrity Analysis 
﻿
Figure 4 shows the progression of wear on the rake face of the cutting tool with respect to cutting 
speed for machining SLM Ti-6Al-4V. It can be seen that wear regions on the rake face increased with 
cutting speeds. At low cutting speeds of 60 and 120 m/min, adhesion and abrasion was found to be the 
major operating wear mechanism. Adhesion on the rake face of the cutting is evident from the small 
lumps of work material/chip material sticking to the cutting edge as shown in Figure 5. Evidence of 
adhesion, abrasion and attrition was observed in all the cutting tools due to the high chemical reactivity 
of titanium alloys. Cutting tools used for machining at 180 m/min, exhibited rapid cratering, abrasion 
and attrition on the rake face of the cutting tool. In addition to that, heavy smearing was also observed 
on the cutting tools during machining of SLM Ti-6Al-4V at high cutting speeds as shown in Figure 5 
c). The severity of the tool wear at high cutting speeds might be due to the high friction, poor thermal 
conductivity and hardness of SLM Ti-6Al-4V. The buildup of work material and chipping of the cutting 
edge in the tools employed for machining SLM Ti-6Al-4V are shown in Figure 6.
3.2. Surface Roughness
Surface roughness of the wrought and SLM Ti-6Al-4V after machining at various cutting speeds are 
shown in Figure 7.SLM Ti-6Al-4V offered better surface finish at low cutting speeds as compared to 
wrought alloy. This might be due to the hardness and brittleness of the material which would result in 
less materials plastic side flow(Chen, 2000). Measured Ra of the machined surface was about 0.6083 μm 
at 60 m/min, this almost reduced drastically to 0.4545 at a speed of 120 m/min. This strongly suggests 
that higher machining speeds results in better surface finish. However, the high surface roughness was 
observed at 180m/min due to catastrophic failure of the cutting tool that leads to entangling and stick-
ing of chip material on the tool which eventually gets removed and deposited in the machined surface 
(as illustrated in Figure 8). This redeposited material on the machined surface leads to higher surface 
roughness and poor surface finish. Evidence of chips/work material deposited on the machined surface 
Figure 4. Maximum flank (VB max) wear during machining of SLM Ti-6Al-4V at various cutting speeds

233
Tool Wear and Surface Integrity Analysis 
﻿
is shown in Figure 9. Higher surface roughness of machined surface of the SLM Ti-6Al-4V at higher 
cutting speeds can also be due to the high buildup of the work material on the cutting tool as shown in 
the Figure 6. The buildup of chip and work material on the tool flank face can push the tool off from its 
original route to increase the roughness (Mantle & Aspinwall, 2001).
3.3. Microstructural Alterations
Cross section of the machined surfaces is examined under an optical microscope to study the micro-
structural alterations and amount of plastic deformation induced beneath the machined surface. Figures 
10 to 15 shows the microstructural alterations induced by machining for various cutting conditions. The 
movement and direction of the cutting tool are illustrated in the figures with the help of arrow marks. 
A layer of disturbed or plastically deformed layer was found underneath the machined surface for all 
cutting conditions. This plastic deformation can be visualized clearly by the deformation and bending of 
the grains along the direction of the tool movement. The microstructural plastic deformation was found 
to increase with cutting speed and tool wear due to the combined effect of high thermal and mechanical 
loads during machining of these materials. SLM Ti-6Al-4V exhibited higher plastic deformation beneath 
the machined surface as compared to wrought Ti-6Al-4V. Figure 16 illustrates the depth of sub-surface 
microstructural alterations after machining these materials. It can be seen that, SLM Ti-6Al-4V exhibits 
almost twice the amount of deformation observed in wrought Ti-6Al-4V for all the machining conditions. 
Highest depth of plastic deformation of about 10 μm was observed for SLM Ti-6Al-4V machined at a 
cutting speed of 180 m/min. However, wrought Ti-6Al-4V did not exhibit severe plastic deformation 
at high cutting speeds like SLM Ti-6Al-4V. There was no evidence of white layer formation on the sub 
Figure 5. Cutting tool after machining SLM Ti-6Al-4V at a) 60 m/min, b) 120 m/min and c) 180 m/min

234
Tool Wear and Surface Integrity Analysis 
﻿
Figure 6. a) Rake face and b) Flank face of the cutting tool after machining SLM Ti-6Al-4V at 180 m/min
Figure 7. Surface roughness after machining SLM Ti-6Al-4V at various cutting speeds

235
Tool Wear and Surface Integrity Analysis 
﻿
surface during machining of these materials unlike other nickel based aerospace alloys (Ulutan & Ozel, 
2011). Overall, the difference in microstructural alterations during machining of these materials can 
be attributed due to the difference in microstructure and mechanical properties, cutting forces, cutting 
temperatures and tool wear. From the above investigations, all the evidences clearly suggest that SLM 
Ti-6Al-4V is more difficult to machine as compared to wrought Ti-6Al-4V.
Figure 8. Mechanism leading to redeposition of chip/work material on the machined surface
Figure 9. Evidence of the deposition of the chip/work material on the machined surface of SLM Ti-6Al-
4V at 180 m/min

236
Tool Wear and Surface Integrity Analysis 
﻿
Figure 10. Sub-surface microstructure of 
wrought Ti-6Al-4V after machining at a cutting 
speed of 60m/min with a cutting tool wear of VB 
max=101μm
Figure 11. Sub-surface microstructure of wrought 
Ti-6Al-4V after machining at a cutting speed of 
120m/min with a cutting tool wear of VB max=140 
μm
Figure 12. Sub-surface microstructure of wrought Ti-6Al-4V after machining at a cutting speed of 180m/
min with a cutting tool wear of VB max=125 μm

237
Tool Wear and Surface Integrity Analysis 
﻿
Figure 13. Sub-surface microstructure of SLM Ti-
6Al-4V after machining at a cutting speed of 60 m/
min with a cutting tool wear of VB max=173 μm
Figure 14. Sub-surface microstructure of SLM Ti-
6Al-4V after machining at a cutting speed of 120 
m/min with a cutting tool wear of VB max=178 μm
Figure 15. Sub-surface microstructure of SLM Ti-6Al-4V after machining at a cutting speed of 120 m/
min with a cutting tool wear of VB max=352 μm

238
Tool Wear and Surface Integrity Analysis 
﻿
4. CONCLUSION
From the face turning trails carried out on the wrought and heat treated SLM Ti-6Al-4V at various cut-
ting speeds, it can be concluded that:
•	
Cutting tool exhibits severe wear in machining SLM Ti-6Al-4V as compared to wrought alloy. 
The maximum flank wear increased with cutting speed and results in catastrophic failure of the 
cutting tool at high cutting speed of 180 m/min during SLM machining.
•	
High welding action between the cutting tools, work materials and chip materials results in rapid 
crater and deformation of the cutting tool during machining of SLM Ti-6Al-4V.
•	
High tool wear, chip welding and buildup of work material on the cutting tool observed during 
machining SLM Ti-6Al-4V results in the redeposition of the chip material on the machined sur-
face and adversely affects the machined surface.
•	
Microstructural alterations were found to be dependent on the cutting speeds and tool wear. 
Increase in cutting speeds and tool wear increased the depth of plastic deformation layer beneath 
the machined surface.
•	
SLM Ti-6Al-4V machined surface was significantly affected due to severe tool wear at high cut-
ting speeds.
REFERENCES
Bordin, A., Bruschi, S., Ghiotti, A., & Bariani, P. F. (2015). Analysis of tool wear in cryogenic machin-
ing of additive manufactured Ti6Al4V alloy. Wear, 328–329, 89–99. doi:10.1016/j.wear.2015.01.030
Figure 16. Depth of sub-surface microstructural alterations after machining of wrought and SLM Ti-
6Al-4V at various cutting speeds

239
Tool Wear and Surface Integrity Analysis 
﻿
Bordin, A., Bruschi, S., Ghiotti, A., Bucciotti, F., & Facchini, L. (2014). Comparison between wrought 
and EBM Ti6AI4V machinability characteristics. Key Engineering Materials, 611-612, 1186-1193. 
doi:10.4028/www.scientific.net/KEM.611-612.1186
Che-Haron, C. H. (2001). Tool life and surface integrity in turning titanium alloy. Journal of Materials 
Processing Technology, 118(1–3), 231–237. doi:10.1016/S0924-0136(01)00926-8
Che-Haron, C. H., & Jawaid, A. (2005). The effect of machining on surface integrity of titanium alloy 
Ti–6% Al–4% V. Journal of Materials Processing Technology, 166(2), 188–192. doi:10.1016/j.jmat-
protec.2004.08.012
Chen, W. (2000). Cutting forces and surface finish when machining medium hardness steel using CBN 
tools. International Journal of Machine Tools & Manufacture, 40(3), 455–466. doi:10.1016/S0890-
6955(99)00011-5
Davim, J. P. (2014). Machining of Titanium Alloys. Springer. doi:10.1007/978-3-662-43902-9
Gibson, I., Rosen, D. W., & Stucker, B. (2010). Additive manufacturing technologies. Springer. 
doi:10.1007/978-1-4419-1120-9
Ginting, A., & Nouari, M. (2009). Surface integrity of dry machined titanium alloys. International 
Journal of Machine Tools & Manufacture, 49(3–4), 325–332. doi:10.1016/j.ijmachtools.2008.10.011
Horn, T. J., & Harrysson, O. L. (2012). Overview of current additive manufacturing technologies and 
selected applications. Science progress, 95(3), 255–282. doi:10.3184/003685012X13420984463047 
PMID:23094325
Mantle, A. L., & Aspinwall, D. K. (2001). Surface integrity of a high speed milled gamma titanium 
aluminide. Journal of Materials Processing Technology, 118(1–3), 143–150. doi:10.1016/S0924-
0136(01)00914-1
Sercombe, T., Jones, N., Day, R., & Kop, A. (2008). Heat treatment of Ti-6Al-7Nb components produced 
by selective laser melting. Rapid Prototyping Journal, 14(5), 300–304. doi:10.1108/13552540810907974
Shunmugavel, M., Polishetty, A., & Littlefair, G. (2015). Microstructure and Mechanical Properties of 
Wrought and Additive Manufactured Ti-6Al-4V Cylindrical Bars. Procedia Technology, 20, 231–236. 
doi:10.1016/j.protcy.2015.07.037
Thijs, L., Verhaeghe, F., Craeghs, T., Humbeeck, J. V., & Kruth, J.-P. (2010). A study of the micro-
structural evolution during selective laser melting of Ti–6Al–4V. Acta Materialia, 58(9), 3303–3312. 
doi:10.1016/j.actamat.2010.02.004
Ulutan, D., & Ozel, T. (2011). Machining induced surface integrity in titanium and nickel alloys: A 
review. International Journal of Machine Tools & Manufacture, 51(3), 250–280. doi:10.1016/j.ijmach-
tools.2010.11.003

240
Tool Wear and Surface Integrity Analysis 
﻿
Vilaro, T., Colin, C., & Bartout, J.-D. (2011). As-fabricated and heat-treated microstructures of the Ti-
6Al-4V alloy processed by selective laser melting. Metallurgical and Materials Transactions. A, Physical 
Metallurgy and Materials Science, 42(10), 3190–3199. doi:10.1007/s11661-011-0731-y
Vrancken, B., Thijs, L., Kruth, J.-P., & Van Humbeeck, J. (2012). Heat treatment of Ti6Al4V produced 
by Selective Laser Melting: Microstructure and mechanical properties. Journal of Alloys and Compounds, 
541(0), 177–185. doi:10.1016/j.jallcom.2012.07.022
This work was previously published in the International Journal of Materials Forming and Machining Processes (IJMFMP), 
3(2); edited by J. Paulo Davim, pages 50-63, copyright year 2016 by IGI Publishing (an imprint of IGI Global).

241
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  13
DOI: 10.4018/978-1-5225-1677-4.ch013
ABSTRACT
The reverse engineering applications has gained great momentum in industrial production with develop-
ments in the fields of computer vision and computer-aided design (CAD). The reproduction of an existing 
product or a spare part, reproduction of an existing surface, elimination of the defect or improvement 
of the available product are the goals of industrial reverse engineering applications. The first and the 
most important step in reverse engineering applications is the generation of the three dimensional (3D) 
metric model of an existing product in computer environment. After this stage, many operations such as 
the preparation of molds for mass production, the performance testing, the comparison of the existing 
product with other products and prototypes which are available on the market are performed by using 
the generated 3D models. In reverse engineering applications, the laser scanner system or digital ter-
restrial photogrammetry methods, also called contactless method, are preferred for the generation of the 
3D models. In particular, terrestrial photogrammetry has become a popular method since require only 
photographs for the 3-dimensional drawing, the generation of the dense point cloud using the image 
matching algorithms and the orthoimage generation as well as its low cost. In this paper, an industrial 
application of 3D information modelling is presented which concerns the measurement and 3D metric 
modelling of the ship model. The possible usage of terrestrial photogrammetry in reverse engineering 
application is investigated based on low cost photogrammetric system. The main aim was the generation 
of the dense point cloud and 3D line drawing of the ship model by using terrestrial photogrammetry, for 
the production of the ship in real size as a reverse engineering application. For this purpose, the images 
were recorded with digital SLR camera and orientations have been performed. Then 3D line drawing 
operations, point cloud and orthoimage generations have been accomplished by using PhotoModeler 
software. As a result of the proposed terrestrial photogrammetric steps, 0.5 mm spaced dense point cloud 
and orthoimage have been generated. The obtained results from experimental study were discussed and 
possible use of proposed methods was evaluated for reverse engineering application.
The Use of the Terrestrial 
Photogrammetry in Reverse 
Engineering Applications
Naci Yastikli
Yıldız Technical University, Turkey
Zehra Erisir
Yıldız Technical University, Turkey
Pelin Altintas
Yıldız Technical University, Turkey
Tugba Cak
Yıldız Technical University, Turkey

242
The Use of the Terrestrial Photogrammetry in Reverse Engineering Applications
﻿
1. INTRODUCTION
In recent years, 3D information modelling has made great progress with the help of the computer visu-
alization. Today, computer-aided design has been widely used in 3D city models, building information 
modeling, 3D geographical information system (GIS), and integration of 3D information in the urban 
built environment and industrial production. In modern industrial production, the 3D model generation 
is the first step for mass production of an object. In this step, the object transferred to the computer 
environment and then 3D CAD model is created.
The reverse engineering is comprehended if the object is exist or produced before and recreation of 
existing product or its part is required. The reverse engineering includes the fast production steps of an 
existing product or its part. Nowadays, reverse engineering has been used successfully in many different 
applications such as; mold manufacturing for production, reproduction for machine or its part, adding 
new functions or upgrading functions for existing product, maintenance, performance analysis, and re-
vamping of an industrial installation, production of historical ship models, remodeling and restoration 
of historical and cultural old buildings, modeling and restoration of archaeological sites and remains 
etc. (Cheng & Jin, 2006; Fernández-Hernandez et al., 2012; Gerbino et al., 2004a; Górski et al., 2010; 
Koelman, 2010; Menna et al., 2011; Menna & Troisi, 2010; Tangelder et al., 2003). Especially in recent 
years, advances in 3D printer technology are contributed to production of prototypes of the objects which 
have 3D CAD model (Cavagnini et al., 2008; Górski et al., 2010).
Digital terrestrial photogrammetry has been used successfully for many years in the documentation 
of historical and cultural heritage property and 3D modeling (Emem et al., 2002; Emem et al., 2004; 
Yastikli, 2007; Yastikli & Alkis, 2003; Yastikli et al., 2007; Yastikli & Guler, 2013). As mentioned earlier, 
the first step in reverse engineering application is the generation of object metric 3D model in computer 
environment. In this step, digital terrestrial photogrammetry is successfully used for the creation of 3D 
models of the existing objects. The terrestrial laser scanner systems is the another system which is used 
in reverse engineering for 3D modelling (Opitz et al., 2012). Most of the time, terrestrial photogrammetry 
is preferred because of image based system which includes low cost digital cameras and software. Espe-
cially recent image matching approaches such as dense image matching algorithms (Gehrig et al., 2009; 
Haala, 2013; Hirschmüller, 2008; Hirschmüller et al., 2012; Hosseininaveh Ahmadabadian et al., 2013; 
Remondino et al., 2013; Remondino et al., 2014), produces more dense point cloud in comparison to the 
terrestrial laser scanners. The terrestrial photogrammetry became more common in reverse engineering 
applications with the help of the recent improvement in image matching approach.
In this study, we aimed to generate dense point cloud and 3D line drawing of the ship model at 1/15 
scale by terrestrial photogrammetry method for the production of the ship in real size as a reverse en-
gineering application. For this purpose, an overview of the reverse engineering application, processing 
steps and procedures are presented in the sections to follow. The experimental study which is presented 
in Section 3 provides the technical information about proposed dense point cloud and 3D line drawing 
process with terrestrial photogrammetry. The result of the experimental study is discussed, and possible 
use of terrestrial photogrammetry for reverse engineering application is assessed.

243
The Use of the Terrestrial Photogrammetry in Reverse Engineering Applications
﻿
2. REVERSE ENGINEERING
Reverse engineering, aims to collection of the information about the size, properties and working principles 
of an object, provide an opportunity to usage in researches and applications in multiple disciplines. The 
medical applications (reconstruction of the face, prosthesis etc.), terrestrial and geodetic issues (digital 
terrain models (DTM), GIS applications etc.), documentation and 3D modeling of historical and cultural 
heritage building, aviation, mechanic, automotive and ship building can be given as an example of the 
reverse engineering applications (Menna & Troisi, 2010).
Cavagnini et al. (2008) investigated the usage of reverse engineering method for collecting data for 
research and analysis of evidence in crime scene investigation. Górski et al. (2010) revealed practical use 
of terrestrial photogrammetry based reverse engineering applications to generate 3D models of desired 
mechanical object for the reproduction. Similarly, Menna & Troisi (2010) investigated the integrated use 
of terrestrial photogrammetry and the laser scanning method for 3D modeling of a small ship propel-
ler for reverse engineering applications. Menna et al. (2011) conducted research for 3D modeling and 
reverse engineering applications of a 3 meter long ship called ‘Indomito’ for the protection, restoration 
and reconstruction of marine heritage.
The reverse engineering application involves the following processing steps (Górski et al., 2010):
•	
Point cloud generation of the existing object with the terrestrial photogrammetry or laser scanning 
method;
•	
Generation of 3D CAD model of the object from the generated point cloud;
•	
Prototyping;
•	
Silicone mold generation;
•	
Final product.
The contact and contactless methods are used for generation of 3D model in high precision reverse 
engineering applications. The summary of these two methods are given in following.
2.1. Contact Method
In this method, the system is working by physical contact with the help of the probe placed end of a 
mechanical arm. With these systems, 3D coordinates of the object surface are measured precisely by 
touching the probe over the object surface. This contact method is reliable 3D coordinate measurement 
technology, but their usage is limited to specialized laboratories and small objects (Menna & Troisi, 
2010). Other important limitation of this method is the usage of the probe for coordinate measuring steps 
which is time consuming and high cost of the system (Gerbino et al., 2004b).
2.2. Contactless Method
In this method, measurement and 3D modeling are carried out using an active object detection system or 
digital photos. The most common contactless methods used in reverse engineering applications are the 
laser scanning system and digital photogrammetry. Laser scanning systems are often used for 3D model-
ing step because of contactless distance based measurement principles and additional color information 
in reverse engineering applications (Górski et al., 2010; Menna & Troisi, 2010). The high system cost 

244
The Use of the Terrestrial Photogrammetry in Reverse Engineering Applications
﻿
and scanning from multiple stations for precise point cloud generation of an object are the limitations 
of the laser scanning systems. Because of that, the digital terrestrial photogrammetry method becomes 
an indispensable part in the reverse engineering applications with their low hardware and software cost. 
Today, the new algorithms developed for digital image analysis enable fast and accurate semi-automatic 
or automatic measurements and manufacturing of dense point cloud (Menna & Troisi, 2010).
3. EXPERIMENTAL SECTION
The experimental section focuses on the generation dense point cloud and 3D line drawing of the ship 
model using terrestrial photogrammetry in reverse engineering applications. The small ship model called 
“Princess” on a scale of 1:15 with 1 m long and 20 cm width have been used for the experiment (Figure 
1). The proposed 3D modelling approach with terrestrial photogrammetry method for generation of the 
dense point cloud and 3D line drawing of ship model “Princess” is shown in Figure 2 (Yastikli, 2005). 
The follow diagram of the proposed strategy as shown in Figure 2 includes the control point measure-
ment, image acquisition with digital cameras, orientation process, creation stereo models, 3D point cloud 
generation and 3D line drawing steps.
The image acquisition of the ship model called “Princess” was performed with Nikon D3X digital 
Single Lens Reflex (SLR) camera which has 6 µ pixel size. The geometric calibration of the camera was 
performed within the framework of the research project for deformation analyses of historical build-
ings (Yastikli & Guler, 2013). The determined focal length (c) was 19.7582 mm and the position of the 
principal point (x0, y0) was -0.0293 mm, -0.0955 mm respectively.
In order to produce a correct point cloud and 3D line drawing of the ship model with terrestrial 
photogrammetry, image acquisition were performed from different sides top, right, left, bottom, front 
and back. Before image acquisition, the ship model was placed in 3D test object which has 77 control 
points (Figure 3). The control points on the surface of the ship model were not preferred to produce 
real surface model of the ship. The coordinates of the control points placed on the 3D test object were 
measured with a non-prism total station with an accuracy of 1 mm.
Figure 1. Different views of the ship model called “Princess”: (a) top view; (b) right view; and (c) bot-
tom view

245
The Use of the Terrestrial Photogrammetry in Reverse Engineering Applications
﻿
Figure 2. The follow diagram of dense point cloud and 3D line drawing approach with terrestrial pho-
togrammetry
Figure 3. The image of the 3D test object and placed ship model

246
The Use of the Terrestrial Photogrammetry in Reverse Engineering Applications
﻿
For each side, at least the three overlapping images were recorded. For stereo model generation, the 
distance between the ship model and camera was 2 meters, base to height ratio (B/H) was 1/8 and base 
distance was 0.25 meters. The approximate image scale was 1/100.
The PhotoModeler software was used for image coordinate measurements, orientation process, point 
cloud generation and 3D line drawing. The points on the ship model, which was called tie points, were 
also measured in addition to the control points on the 3D test object as shown in Figure 4. The number of 
recorded images, measured control and tie points for the top, right and bottom sides were given at Table 
1. The orientation of the images was performed with the help of measured tie points and control points 
using the PhotoModeler software. The orientation parameters of each images and the 3D coordinate of 
the measured tie points were computed.
The all measured tie points, 3D line drawings and generated point clouds will be in reference co-
ordinate system (X, Y, Z) with the help of the control points at the end of the orientation process. The 
3D line drawings of the ship model were performed in digital environment for each side. The Figure 5 
shows the 3D line drawings of top side of the ship model.
In addition to the 3D line drawing, the point clouds of the ship model were generated using the 
generated stereo models using the recorded images from different sides. The image masks were created 
for each stereo model area to define region for point cloud generation. The dense point clouds with the 
grid size of 0.5 mm were generated. The achieved mean square error for matching of targeted area was 
0.91 pixels. The outlier points with the small correlation coefficient were eliminated then new points 
were interpolated using neighboring points where the image matching was not performed successfully. 
Figure 4. Measured control and tie points for top part of the ship

247
The Use of the Terrestrial Photogrammetry in Reverse Engineering Applications
﻿
After these steps, the holes can be still seen in generated point clouds because of the poor textures as 
can be seen in Figure 6. The big holes in the point clouds can be seen in white colored areas which mean 
that the gray values of the conjugate pixels were the same and the correlation coefficient was very low. 
Because of that, 3D coordinates of the points in such areas were not computed.
After point cloud generation, the orthoimages with the ground sample distance (GSD) of 10 mm were 
generated using the recorded images, orientation parameters and generated point clouds. The bicubic 
interpolation method was used as a resampling method for generation of the orthoimages. The orthoim-
age of the ship model for the top side can be seen at Figure 7 with the GSD of 10 mm. The areas where 
the point cloud generation failed can be seen in black because of the lack of the 3D coordinates of ship 
model surface. The possible solution for such areas can be the projection of colored light to the ship 
model surface to create different gray value of the neighbored pixel.
4. CONCLUSION
In this paper, a low cost procedure based on terrestrial photogrammetry for 3D modelling of the ship model 
has been presented. The proposed approach for dense point cloud and 3D line drawing with terrestrial 
photogrammetry allows obtaining digital representation of studied ship model in straightforward way, 
without using equipment other than digital camera and computer with appropriate software packages. 
The dense point clouds of ship model with 0.5 mm grid size have been produced in addition to the 3D 
line drawing after photogrammetric processing steps. The produced orthoimages can be used also 2D 
information extraction from different views.
Table 1. Control and tie points numbers
Sides
Number of Images
Number of Control Points
Number of Tie Points
Top
4 images
17
268
Right
4 images
17
72
Bottom
4 images
17
15
Figure 5. The 3D line drawings of top side of the ship model

248
The Use of the Terrestrial Photogrammetry in Reverse Engineering Applications
﻿
The proposed low cost procedure based on terrestrial photogrammetry can be used for generation 
of the 3D model for physical replicas or technical analyses (symmetry, hydrostatics, lines drawing) of 
objects in reverse engineering applications. The complete solid model or replicate the original object 
can be easily produced with generated point cloud using rapid prototyping techniques. For complex 3D 
model generation of complex object, multiple point clouds and 3D line drawing produced from different 
views have to be integrated. The more accurate and dense point clouds can be generated with the help 
of the progress in image matching algorithm. The obtained result from experimental section approve 
the reliability and effectiveness of the proposed dense point cloud and 3D line drawing procedure using 
terrestrial photogrammetry for reverse engineering applications.
Figure 6. The point cloud of the ship model for (a) top side; (b) right site; and (c) bottom side
Figure 7. The orthoimage of the ship model for top side

249
The Use of the Terrestrial Photogrammetry in Reverse Engineering Applications
﻿
REFERENCES
Cavagnini, G. M., Scalvenzi, M., Trebeschi, M., & Sansoni, G. (2008). Reverse Engineering from 3D 
Optical Acquisition: Application to Crime Scene Investigation. In  Bartolo et al. (Eds.), Virtual and 
Rapid Manufacturing: Advanced Research in Virtual and Rapid Prototyping (pp. 195–202). London: 
Taylor & Francis Group.
Cheng, X. J., & Jin, W. (2006). Study on Reverse Engineering of Historical Architecture based on 3D 
Laser Scanner. Journal of Physics: Conference Series, 48, 843–849. doi:10.1088/1742-6596/48/1/160
Emem, O., Batuk, G., Yastikli, N., & Balik, F. (2004, October). 3D Modelling and Visualization Using 
Photogrammetric Data. Paper presented at UDMS 2004, Chioggia, Italy.
Emem, O., Yastıklı, N., Balik, F., & Alkis, Z. (2002, June). Creating 3D Photo Realistic Models for 
Visualization of Historical Buildings. Paper presented at Proc. 3rd International Symposium Remote 
Sensing of Urban Areas, Istanbul, Turkey.
Fernández-Hernandez, J., González-Aguilera, D., Rodríguez-Gonzálvez, P., & Mancera-Taboada, J. (2012). 
A new trend for reverse engineering: Robotized aerial system for spatial information management. Applied 
Mechanics and Materials, 152-154, 1785–1790. doi:10.4028/www.scientific.net/AMM.152-154.1785
Gehrig, S. K., Eberli, F., & Meyer, T. (2009). A real-time low-power stereo vision engine using semi-
global matching. Lecture Notes in Computer Science, 5815, 134–143. doi:10.1007/978-3-642-04667-4_14
Gerbino, S., Martorelli, M., Renno, F., & Speranze, D. (2004b, May). Cheap Photogrammetry Versus 
Expensive Reverse Engineering Tecniques in 3D Model Acquisition and Shape Reconstruction. Paper 
presented at International Design Conference – Design 2004, Dubrovnik, Croatia.
Gerbino, S., Renno, F., & Papa, S. (2004a, May). Two Reverse Engineering Methods For The Recon-
struction Of An High Speed Craft Surface: A Comparison. Paper presented at International Design 
Conference – Design 2004, Dubrovnik, Croatia.
Górski, F., Kuczko, W., Wichniarek, R., & Zawadzki, P. (2010, April). Application of Close-range Pho-
togrammetry in Reverse Engineering. Paper presented at 7th International DAAAM Baltic Conference, 
INDUSTRIAL ENGINEERING, Tallinn, Estonia.
Haala, N. (2013). The Landscape of Dense Image Matching Algorithms. In Fritsch, D. (Ed.), Photogram-
metric Week ’13 (pp.271-284).Wichmann, Berlin/Offenbach.
Hirschmüller, H. (2008). Stereo processing by semiglobal matching and mutual information. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 30(2), 328–342. doi:10.1109/TPAMI.2007.1166 
PMID:18084062
Hirschmüller, H., Buder, M., & Ernst, I. (2012). Memory efficient semi-global matching. ISPRS Annals 
of the Photogrammetry. Remote Sensing and Spatial Information Sciences, 1(3), 371–376.
Hosseininaveh Ahmadabadian, A., Robson, S., Boehm, J., Shortis, M., Wenzel, K., & Fritsch, D. (2013). 
A comparison of dense matching algorithms for scaled surface reconstruction using stereo camera rigs. 
ISPRS Journal of Photogrammetry and Remote Sensing, 78, 157–167. doi:10.1016/j.isprsjprs.2013.01.015

250
The Use of the Terrestrial Photogrammetry in Reverse Engineering Applications
﻿
Koelman, H. J. (2010). Application of a photogrammetry-based system to measure and re-engineer ship 
hulls and ship parts: An industrial practices-based report. Computer Aided Design, 42(8), 731–743. 
doi:10.1016/j.cad.2010.02.005
Menna, F., Nocerino, E., & Scamardella, A. (2011). Reverse Engineering and 3D Modelling for Digital 
Documentation of Maritime Heritage. International Archives of the Photogrammetry, Remote Sensing 
and Spatial Information Sciences, Volume XXXVIII-5/W16, 2011 ISPRS Trento 2011 Workshop, 2-4 
March, Trento, Italy.
Menna, F., & Troisi, S. (2010). Low Cost Reverse Engineering Techniques for 3D Modelling of Propel-
lers. International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, Vol. 
XXXVIII, Part 5 Commission V Symposium, Newcastle upon Tyne, UK.
Opitz, R., Simon, K., Barnes, A., Fisher, K., & Lippiello, L. (2012, March). Close-range photogramme-
try vs 3D scanning: comparing data capture, processing and model generation in the field and the lab. 
Paper presented at Computer Applications and Quantitative Methods in Archaeology, Southampton, UK.
Remondino, F., Spera, M. G., Nocerino, E., Menna, F., Nex, F., & Gonizzi-Barsanti, S. (2013). Dense 
image matching: comparisons and analyses.Proc. IEEE Conference “Digital Heritage 2013”, Vol. 1, 
pp. 47-54, Marseille, France.
Remondino, F., Spera, M. G., Nocerino, E., Menna, F., & Nez, F. (2014). State of the art in high density 
image matching. The Photogrammetric Record, 29(146), 144–166. doi:10.1111/phor.12063
Tangelder, J. W. H., Ermes, P., Vosselman, G., & Heuvel, F. A. (2003). CAD-Based Photogrammetry for 
Reverse Engineering of Industrial Installations. Computer-Aided Civil and Infrastructure Engineering, 
18(4), 264–274. doi:10.1111/1467-8667.00316
Yastikli, N. (2005, March). The Documentation and 3D modelling with Digital Photogrammetry and 
Terrestrial Laser Scanner (In Turkish). Paper presented at TMMOB Harita ve Kadastro Mühendisleri 
Odası, 10. Türkiye Harita Bilimsel ve Teknik Kurultayı, Ankara, Turkey.
Yastikli, N. (2007). Documentation of Cultural Heritage Using Digital Photogrammetry and Laser scan-
ning. Journal of Cultural Heritage, ISSN: 1296-2074, Volume 8, Issue 4, pp 423-427.
Yastikli, N., & Alkis, Z. (2003, October). Documentation of Cultural Heritage by Using Digital Close 
Range Photogrammetry. Paper presented at Proc. XIXth International Symposium CIPA 2003, New 
Perspective to Save Cultural Heritage, Antalya, Turkey.
Yastikli, N., Emem, O., & Alkis, Z. (2003, October). 3D Model Generation and Visualization of Cultural 
Heritage. Paper presented at Proc. XIXth International Symposium CIPA 2003, New Perspective to Save 
Cultural Heritage, Antalya, Turkey.
Yastikli, N., & Guler, E. (2013). Performance Evaluation Of Thermographic Cameras For Photogram-
metric Documentation Of Historical Buildings. Boletim De Ciencias Geodesicas, Volume, 19(Issue: 4), 
711–728. doi:10.1590/S1982-217020130004000012
This work was previously published in the International Journal of 3-D Information Modeling (IJ3DIM), 4(2); edited by Jason 
Underwood, Sisi Zlatanova, and Umit Isikdag, pages 48-57, copyright year 2015 by IGI Publishing (an imprint of IGI Global).

251
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  14
DOI: 10.4018/978-1-5225-1677-4.ch014
ABSTRACT
This updates a longitudinal study of Bespoke/ High End tailoring (Ross, 2007) with a secondary focus 
on mid-market menswear. The discussion commences with a review of key literature and practices from 
digital menswear studies to-date and evaluates how in the last two decades a deeply entrenched tra-
ditional sector such as tailoring has embraced numerous fashion digital-technologies from the design 
process to multi-channel consumption in-store, e-commerce and apps. The author utilizes a 3D-4C’s 
process model as a structural template. The research methodology is interpretive, qualitative, online 
structured-observation of tailors currently utilizing digital practices plus in-depth interviews with 
industry experts. Those interviewed include Bespoke/Designer tailors with current 3D- platforms and 
International researchers providing insight into the latest developments in digital fashion including virtual 
design, scanning, sizing, avatars, 3D-fabric-simulations, 3D-printing, virtual-try-on and customization 
to establish best practices and future forecasts for proactive fashion management.
INTRODUCTION
Until the 1990’s design and manufacturing processes in menswear bespoke and high end sectors of the 
fashion industry had hardly changed since the 19th century, despite the availability of new digital tech-
nology and changing consumer attitudes to the old tailor-client relationship. This is partially because 
tailoring, as we perceive the term, has a heritage that dates back to the thirteenth century when the first 
Fashion-Technology and 
Change in Product Development 
and Consumption for the 
High-End Menswear Sector:
A Study Utilizing a 3D-4C’s Process Model
Frances Ross
London College of Fashion, UK

252
Fashion-Technology and Change in Product Development and Consumption
﻿
coats were made and considered fashionable. According to Almond (2011) at that period coats were 
made by skilled laborers sewing the cut cloth by hand. The laborer became known by the name Tailor 
from the French Tailler which means to cut. The term Bespoke comes from the description of the tailor 
and customer bespoking the exact specification of the cloth, style and fit. From the early19th century 
mass-manufacturing of menswear suiting became the norm for all but the very rich, or men with sartorial 
style whose preference was for the art of bespoke created by the master tailor. A recent study by Ross 
(2012a) identified a new Semi-Bespoke descriptor which utilizes both the hand crafts of the traditional 
tailor combined with digital technology to accommodate the contemporary male consumer who values 
luxury and craft but wants a more accessible, cost-effective and faster suit. This is because a typical 
bespoke suit from Savile Row London can take up to forty hours to produce and 3-4 customer fittings. In 
between the bespoke and mass-manufacturing sits Mass Customization which has been well documented 
by many key writers (Gilmore & Pine, 2000; Westbrook & Williamson, 1993) since Toffler (1980) first 
predicted the paradigm shift from mass manufacturing to mass customization when customers demand 
for more choice and a better fit would be personalized in apparel (Davis, 1987; Pine, 1993).
Advances in digital technology allow reductions in the intensity of labor, cost and the need for custom-
ers to attend time-scarcity fittings in person while keeping the quality and service. Studies show that the 
technology is now providing a younger generation of male consumers with more opportunity to choose 
a well-fitting semi-bespoke suit. (Ross, 2007; Bougourd & Delamore, 2007) As many consumers are 
now better educated, discerning and more demanding in choice of style, quality, service and price value 
this is a fast growing niche market that needs to be addressed for the future. However, this market also 
requires the willingness to accept new technological processes and methods by both the tailor and the 
customer in what is recognized as a conservative sector. The perceived ‘usefulness’ and ‘ease of use,’ 
‘acceptance of technology’ was first considered worthy of study by Davis (1987) when he developed a 
valid measurement scales system for predicting user acceptance of computers. Since then this generic 
model has been adapted and reused regularly by many academics and practitioners to evaluate the open-
mindedness of innovation by consumers and the fashion industry and is considered relevant to this study.
Research has identified a number of social-cultural, technological and economic factors arising 
globally in the past two decades of the 20- 21st century as indicators of a paradigm shift driving those 
who have relied on the traditionally entrenched practices of the menswear industry and their consum-
ers to reconsider their conservative and conventional practices and modes of consumption. Around the 
same time as the first published study on change in the bespoke menswear market Mark Tungate (2008) 
described in his book The Branded Male the untapped potential of men’s internet shopping sites, citing 
in particular Net-a-porter luxury brands for women as a potential model. On the 22nd February 2011 Mr 
Porter the male equivalent was launched and has been followed by many other successful menswear 
shopping sites as well as a number of innovative Semi-bespoke e-tailors trekking the same profitable 
path and resulting in more male customers having the confidence to use bespoke services on the internet.
New digital technology has made great advances in design and manufacturing within the middle 
to high end menswear market as well as mass manufacturing through the availability and more com-
mon use of methods such as digital printing, 3D pattern-cutting, 3D tailoring, co-creation via web 3.0; 
styling; fabric simulation; body-scanning and semi-bespoke manufacturing processes. One of the main 
leaps forward that enables both the tailor and the customer to visualize the apparel on himself without 
actually trying on or even being in a retail environment is known as ‘Virtual-try-on’ which simulates 
the behavior of textiles on the human digitized body. Cloth/textile items can be seen to behave (drape 

253
Fashion-Technology and Change in Product Development and Consumption
﻿
and move) on a digital representation of your type of body shape, or from actual measurements and the 
additional uploading of a photograph of your face creates a ‘virtual-you’ with which you can visualize 
yourself wearing a choice of styles (Ross, 2012; D’Apuzzo, 2007). This complete process of sizing and 
co-designing a bespoke suit, shirt, tie and shoes can be achieved via 3D digital fashion technology using 
an app on a smart phone or tablet providing online semi-bespoke consumption via one-stop shopping 
and changing the customer-facing business models of future tailors and menswear retailers.
This chapter will take a meta-view of several previous qualitative research studies conducted in the 
last ten years by the author (Ross, 2007, 2009, 2012a, 2012b) and include other relevant recent surveys 
and research case-studies to map the shift from’ traditional’ fashion management to new methods of 
product innovation utilizing fashion-technology and enabling the fashion consumer to receive a more 
fitted, customized/co-created man’s apparel from the comfort of his home or while on the move. The 
chapter will aim to answer the following questions:
1. 	
How has new fashion digital technology enabled men to co-create and consume their own styled 
apparel?
2. 	
To identify from case-studies successful contemporary best practices utilizing digital technology 
and new business models.
3. 	
To test the usefulness of the 3D-4C’s Process Model to identify and classify the best practices of 
3D-fashion technology for Bespoke/High end menswear.
4. 	
Identify and address the key management challenges of menswear design, manufacture and con-
sumption for the future
The next section contextualizes the last ten years of evolutionary, and in some instances revolutionary, 
change to the Bespoke/High end menswear industry sector. This will include definitions of key new 3D 
fashion technology descriptors and business platforms/models utilized for the selection of styles, fabrics, 
design, cutting, manufacturing, promotion and selling channels. Any issues, or gaps in the literature on 
the topic will be identified and various solutions (where relevant) will be discussed. This is followed 
by the research methodology and analysis of best practice case-studies which includes an update of the 
38 semi-bespoke tailors studied by Ross (2012a) plus additional monitoring of new e-tailors; followed 
by a critical discourse on the empirical research case-studies of Designer/Tailors utilizing online/of-
fline business models to provide customized suits and apparel creating a hybrid of hand-crafts and 3D 
manufacturing.
The final sets of case-studies are from industry including leading International researchers in the 
field of digital fashion; Philip Delamore Head of Digital Innovation at Change of Paradigm, Jennifer 
Bougourd who took part in the first body metrics sizing survey in the UK and Andrew Crawford Chief 
Executive of Sizemic. The explanatory and descriptive discourse in this chapter will enable evaluation of 
an original model that enables the complete 3D process of sizing, design, visualization and manufacture 
through to retail selling channels to draw conclusions that l support a newly created process model for 
semi-bespoke/ high end menswear sector. This is entitled the 3D-4C’s Process Model denoting ‘3-Di-
mensional Capture-Create-Construct-Consume.’ (Mnemonic descriptor is 3D-4C’s. Process flow as 
shown in Figure 1). This represents a potential paradigm shift in the fashion industry.

254
Fashion-Technology and Change in Product Development and Consumption
﻿
BACKGROUND REVIEW
This section starts with a contextualization of Mass-customization/Personal-Customization and the 
updating of the paradigm shift that was first identified by Toffler in 1980. This will continue with a 
discussion of the main 3D digital technologies currently utilized in the contemporary menswear industry 
for the complete design process and their contribution and relationships within the 3D-4C’s Process 
Model ‘Capture-create-construct-consume.’ as shown in Figure 1.
Mass Production vs. Craft Production
In 1980 Toffler first identified a trend towards “de-massification” (Toffler, 1981, p. 192 as cited in 
Squire et al, 2006, p. 11) away from mass-manufacturing, moving towards a form of customization for 
the individual consumer, this was followed by Davis (1987) who coined the phrase ‘mass-customization’ 
and has continued to grow in popularity linked with the acceptance of technology in general and 3D 
fashion technology in particular. Many scholars since that time have developed new perspectives on 
Mass Customization and Personal Customization (MC/PC) the most famous and prolific being Professor 
Piller (Piller, 2000; Piller & Muller, 2004; Piller, 2007; Piller & Diener, 2009). Mass customization has 
been considered to combine the best of craft with individualized products (which were high cost) with 
the best of mass production at a more accessible and standardized cost.
Figure 1. 3D-4C’s process model ‘capture-create-construct-consume’
(Ross, 2015)

255
Fashion-Technology and Change in Product Development and Consumption
﻿
Squire et al (2006) developed a diagrammatic positioning chart that shows the position of mass 
customization in relation to the scale of production and degree of customer input. Craft is placed in the 
bottom left hand corner pointing up to mass customization; giving an implied trade-off between the 
degree of product customization and consumer input and the effectiveness of the production process. 
However, this concept is now outdated as digital fashion technologies are rapidly changing to one-to-
one co-creation as discussed in Markets of One (Gilmore & Pine, 2000) so that they may be viewed as 
sustainable commercial design technologies. Tailoring is a field where the balance between customer 
co-creation and tailor’s input via technology enabling choice to be configured has to be economically 
viable while avoiding customer confusion with too many choices. The concept ‘Mass Confusion’ covering 
this state was coined by Tseng & Piller (2003). The amount of hand-craft or finishing on the garment is 
a symbol of quality and price value meaning that most customers are willing to pay a higher price for 
individualized work, as identified and discussed by Gilmore & Pine (2000) in ‘Markets of One: Creat-
ing Customer-Unique Value through Mass Customization’. The limitation identified up to this point was 
accurate individual-customer sizing, design and finishing making it much closer to mass customization 
than co-creation, despite Gilmore and Pine (2000) titling their publication ‘Markets of One.’
From a high end menswear perspective Almond (2011) describes the tailor as using his craft and 
experience to sculpt the medium of cloth for the human form, therefore aligning tailoring in general to 
craft production. However, the gap identified between the hand-crafted bespoke tailor and computerized 
mass market manufacturing is blurring, so SME menswear companies can now offer crafted high perfor-
mance garments with client customization together with high performance in delivery at attractive prices.
Anthropometry and 3D Body Scanning
3D Body scanning in its many technological formats has been well documented by many key researchers 
and writers including (Simmons & Istook, 2003; D’Apuzzo, 2007; Apeagyei, 2010; Delamore & Sweeney 
2010). However, a clear gap in the literature relates to how all these are linked together to provide a holistic 
process supporting the paradigm shift in fashion manufacturing. Therefore, the discussion starts with the 
first C of the 3D-4C’s Process Model where C = Capture of body measurements in a tailoring context.
As no two people are ever the same size or shape even if they are wearing the same sized clothes, 
accurate measurements of the average person’s body has always been vital information to obtain good 
fitting clothes; the study of which is known as Anthropometry (Simmons & Istook, 2003). However, 
it was not until the 1950’s that Anthropometry became a recognized discipline of study, whether via 
the tape measure used by the traditional tailor, or the e-tape, an electronic device that combines human 
body measurements with digital technology. Although the measurement process is similar to a classical 
tape-measure it calculates and records each individual measurements electronically and E-Measurement 
Solutions can deliver the data to a PC via a wireless connection which is faster and easier but neverthe-
less still relies on essentially traditional measurements (D’Apuzzo, 2007). The e-tape does not have the 
limitations of traditional measurement error inherent in chalked markings and can therefore improve 
the data quality which master tailors have previously claimed only comes through years of training and 
experience (D’Apuzzo, 2007). This approach also allows tailors and other ecommerce sites to operate 
internationally with accurate data potentially coming directly from any country worldwide.

256
Fashion-Technology and Change in Product Development and Consumption
﻿
3D body scanning is now the main method for capturing measurements for mass market menswear. 
Anthropometry has been used for national sizing surveys including Body Metric’s UK sizing survey 
(Bougourd, 2007) both for clothing gradients for the mass market and the health status of the nation 
(Simmons & Istook, 2003).
3D Body Scanning
Systems for digitizing the shape of the body have existed for over 20 years and have been utilized in 
several different industries that have commonalities with the clothing and fashion industry in requiring 
the capture of accurate sizing of the body or body parts. (Reilly, 2014) The film industry (Hollywood 
originally but now many film industries commercial and corporate) has utilized the technology to replace 
real actors with virtual ones (D’Apuzzo, 2007). Creative experimental usage of 3D scanners by fashion 
designers and photographers aided by technological improvements in the visualization of game-anima-
tion have made our mirror image hyper- realistic and helped push the technology to its limits enabling 
business platforms and other commercially-minded retailers to adapt or re-appropriate the potential for 
multi-channel fashion consumption (Delamore & Sweeney, 2010).
The second industry utilizing 3D body scanners was the military who wanted to be able to mass-
customize uniforms and equipment but with the advantage of better fit, less alterations and a more cost 
effective process of measurements for all of their staff (D’Apuzzo, 2007).
The trickle effect from these two industries leads to a ‘sweet spot’ being identified for the fashion 
industry which shifts from being brand-centric to consumer-centric consumption (Pratt et al, 2012). This 
is linked to various changes in society, mass-customization created for the co-designer, communication 
and purchase through e-commerce, online social media, apps and the mobile phone. These have led to 
the world of the ‘selfie’ being embedded in culture and shared with friends driving both pleasure and 
consumption (Delamore, 2015).
To fit with the shift to a ‘selfie’ representation and consumption concept, visual representation of 
the human body as an avatar has evolved rapidly over the recent period appearing progressively more 
realistic. This is witnessed in the movies, television broadcasts, gaming and the fashion industry, particu-
larly for the high end market. Here the end experience consists of measuring followed by visualization 
in a reflective mirror whether in store by the tailor, through a 3D scanner or from the virtual-try-on at 
home as required to match the quality of the products. According to D’Apuzzo (2007) state of the art 
digital technology divides into 5 groups. The first is Laser-scanning, the second utilizes the Projection of 
white light patterns, the third a combination of modeling and image processing. The fourth takes digital 
manual measurements and the fifth is technology based on other active sensors. According to Delamore 
and Sweeney (2010) a sixth technological group for body scanning is also available and growing in 
popularity with both consumers and fashion designers/retailers. This is ‘Time of Flight cameras,’ known 
sometimes as ‘depth-cams.’ This method utilizes a similar lens to a regular web-cam with a CCF sensor. 
The simple advantage of this technology is that the data can be read directly from the camera feed. Both 
mid-market and high end menswear are embracing more of the 3D scanning measurement methods. 
Delamore and Sweeney (2010) state that whatever combination of specialized sensors and equipment 
are utilized the democratization and fuller usage of 3D scanners for sizing is dependent on the reduced 
price entry-level as well as their complexity of use, especially with SME (Small, Medium Enterprise) 
tailoring businesses which run from sole-trader to a maximum of 250 employees (1-10 employees is 
defined as a small business, up to 250 is the upper range for a medium enterprise). It is possible, as the 

257
Fashion-Technology and Change in Product Development and Consumption
﻿
digital equipment becomes less expensive to purchase and easier to use, more tailors and menswear 
designers will invest in the process equipment either for their own business or in a group co-operative 
arrangement much like the early days of digital printing. However, there is a lack of published texts 
covering this aspect other than Delamore and Sweeney (2010).
An effective solution to costly and time consuming fittings is a Skype fitting utilizing a camera. This 
has been identified as a good way of carrying out 2nd and 3rd fittings for customers not on site by Andrew 
Ramroop, Bespoke tailor of Savile Row interviewed by Ross (2012a). This technique enables tailors 
and designers to increase their International business from their home base, for example Ramroop has 
customers in the USA, China, India and Europe where the initial consultation takes place face-to-face 
either in a pre-booked city location or at Savile Row but further fitting can be conducted remotely.
This now could be taken a step further and a camera could stand for the C= Capture of the sizing 
of the client before the fitting. However, this raises the issue of technology acceptance at the very top 
end of the Bespoke market where persisting client expectations are for a personal master tailor taking 
measurements and discussing personal preferences on shape for their body-type, ultimately reflecting 
the high cost invested in the garment (Ross, 2012a).
Issues of Consumer Acceptance and Usefulness of 3D Scanner Technology
Studies have consistently identified dislike of having your body-scanned in your underwear or special 
suiting in what appears to be a medical-looking chamber (Ross, 2007; Ross, 2012a; Apeagyei, 2010; 
Delamore & Sweeney, 2010). This does not sit well with the ‘experiential retailing’ environment which 
has been created as part of the concept of the ‘retail theatre’ a term coined in the Experience Economy by 
Pine & Gilmore (1999). It is clear that the successful usage of 3D body scanners in the fashion industry, 
especially high end, requires appreciation of the holistic role it plays within the retail experience for the 
customer. Issues to be taken into account include fear of technology through to the rather impersonal 
service and fitting experience that does not compare to the service provided by a tailor or shop assistant. 
According to Apeagyei (2010) some customers were concerned about the ethical issue of who owns 
the IP for the use of the body scan data and how it may be used or passed on to other retailers without 
their permission. Other concerns are specific to the claustrophobic nature of being enclosed in a small 
space for the scanning even if it is only 2 minutes in duration. General lack of a positive retail experi-
ence and fears regarding IP and security of the 3D scanned data all need to be addressed as valid issues 
of technology acceptance or TAM (Davis, 1987). The next section addresses the issues in generating a 
positive retail experience.
‘Create’ a Representation/Design Visualization/Virtual-Try-On for Consumption
Augmented reality (AR) and VTO (Virtual-Try-On) can enhance and enrich the experiential consumption 
process as AR and VTO give the opportunity to view a live physical presence for the simulation of the 
garment, or body modifications to improve the fit (Delamore & Sweeney, 2010). Also visual represen-
tation of how cloth drapes and moves with the body has been vastly improved by 3D textile simulation 
engines which are used to show how a 2D textile pattern as seen in CAD software can demonstrate the 
characteristics of the textile for example how stretchy, thick, fine, or flowing the fabric is on the body 
(D’Apuzzo, 2007). The tailor or Stylist, real or virtual, often used in online shopping platforms or kiosks 
can advise the customer on fit, color and form of the garment (Ross, 2009, 2012a). This technological 

258
Fashion-Technology and Change in Product Development and Consumption
﻿
simulation process is now also being utilized on virtual mannequins. OptiTex was one of the original 
widely implemented systems many other companies now provide similar systems. All can modify the 
existing mannequin to model real anthropometric data which is then transformed to a virtual representa-
tion of the real human form (D’Apuzzo, 2007). Integration with CAD design system body shapes and 
sizes from the 3D scanned visualization can create avatars that simulate a competent fit on screen which 
can be assessed without travelling to the retail site.
In summary virtual try-on technology through manual or digital measurements cuts down on fitting 
time undertaken by the customer, increases accuracy of sizing and fit and reduces costs (Apeagyei, 2010). 
This couples the C = Create with the C=Consume encouraging new business models and revenue sources 
to be developed by entrepreneurs and innovative fashion management. This aspect will be discussed and 
developed under new business models for the fashion industry.
Augmented Reality (AR) is for augmenting the real world with digital information and helps the 
retail experience by mixing reality and fantasy which adds new layers to the retail experience. This has 
so far been used to good effect by luxury brands and clearly has a place in bespoke/high end menswear 
as another way of visualizing the garment on the customer without them having to be available for the 
fitting. This approach may address the issue of fear of the scanning equipment identified by Delamore 
and Sweeney (2010) but this was not explicitly explored in their paper.
3D Scan Shoes Made-To-Measure for Bespoke Consumers
The traditional approach to measuring the body or foot has generally involved use of the tape measure. 
However, in footwear 3D scanning of the foot currently allows an individual last to be created (Bae & 
May-Plumlee, 2005). For example, The Left Shoe Company founded in 1998 in Finland now has loca-
tions worldwide including Dubai, Naples, Germany, London, Kobe (Japan) and Los Angeles (The Left 
Shoe Company, 2015). This now enables the investment in the first pair of bespoke shoes to reduce the 
cost of further orders by re-using the stored sizing data and showing the customer simulations of dif-
ferent styles, colors and materials through virtual-try-on from home or while on the move. This process 
has already proved successful in high end menswear outlets and tailoring establishments enabling more 
men to have a really good fitting shoe customized to their own individual feet better than even an ex-
pensive luxury brand could ever hope to achieve (Straub & Kerlin, 2014). This C= Create now merges 
with the C = Construct moving away from traditional engineering and manufacturing to faster and more 
economic production.
Futuristic 3D Scanner and 3D-Printing for Bespoke and High End Tailoring
3D printing combined with 3D scanning may significantly help to develop mass-customization and 
personal customization in the bespoke and high end menswear sectors. According to Ross (2009) and 
more recently Tseng et al (2014) both of whom describe the customer’s style requirements fulfilling 
the role of the co-designer. The product configurator is the most commonly used toolkit to extract cus-
tomer’s requirements often via images or a set of questions on their preference of style or role model. 
The toolkit consists of a set of predefined component attributes that offers predetermined choices to 
suit the co-designer’s size, form, color, style and price band. The co-design system is enabled by an 
open-product architecture and on-demand manufacturing system via the user’s participation in design, 
product simulation, manufacturing and assembly. Brooks Brothers in the USA has successfully utilized 

259
Fashion-Technology and Change in Product Development and Consumption
﻿
this for bespoke suit and shirt tailoring (Straub & Kerlin, 2014). 3D printing is also known as ‘additive 
manufacturing’ the process is faster than weaving textiles or creating accessories such as ties, sunglasses 
and cufflinks and according to Pratt et al (2012, p. 366) has ‘enormous possibilities for bespoke (custom 
made to order) clothing: using laser beams to fuse layers of recycled plastic powder into shape….to make 
seamless, perfect-fitting clothing from 3D laser printers with virtually no waste’. This has important 
additional sustainability and ethical fashion implications not pursued further in this chapter.
Ratto and Ree (2012) describe 3D printing involving slicing digital models into horizontal layers 
and then building the wearable up a layer at a time similar to Rapid-Prototyping machines being used 
by in-house designers and SME manufacturers for research and testing rather than engaging with a large 
outside source of engineering that is costly and time consuming. 3D printers can print almost anything 
that can be created as a CAD file and real bodies and materials can be digitized by software, therefore 
3D scanning/printing can be utilized to design custom clothes (Reilly, 2014). Futuristic versions of this 
are already in development so that customers can now create customized wearables from 3D displays 
such as holograms or viewing googles that show either still representations or videos (Straub & Kerlin, 
2014). This moves into the area of C= Construct which although having currently limited research or 
acceptance, except with Couture and Independent designers, has the potential to become mass market in 
the future thereby changing traditional manufacturing forever (Kuhn, de Fatima & Minuzzi, 2015; Reilly, 
2014). Other digital processes with increasing use and having potential to change traditional craftwork 
and create innovative design detailing include Laser Cutting.
Laser Cutting
Laser cutting in the fashion industry from the 1960s has had an impact (mainly) on the made-to-measure 
tailoring sector. The cutting systems are mostly focused on the mass-customization sectors of the menswear 
industry rather than traditional tailors. Numerical cutting systems exist to draft patterns and cut directly 
onto fabric using laser, water jet, and mechanical (vertical and circular) cutting heads. These cutting 
methods are often used and combined with traditional methods by the larger tailoring manufacturers or 
even SME’s. The linkage with 3D body scanning and CAD/CAM systems extends into cutting systems 
and interfaces with sewing systems for the assembly of custom garments (Cannon-Jones, 2015). As 
with all new technologies laser cutting techniques also offer innovation and creativity for the menswear 
designers in accessories such as ties, belts and shoes but also in suits and jackets where for example 
designers can create lace-like effects on different textiles and skins.
The Change of Consumption Channels
For the final C=Consume discussion of the 3D-4C’s process model ways of consuming from omni-chan-
nels will be explored. In terms of new online environments such as web 2.0 and 3.0 tools which enable 
the consumer to receive style advice and participate in the co-creation of their suit design (Ross, 2009) 
studies the possibilities through the ‘pink market’ for civil/wedding- partnership suiting and other gay 
styling that could provide a growth niche-market for online-mass-market tailors. Consumption can now 
be enabled through an app on your mobile, tablet, or more recently wearables being currently launched 
by Apple and Samsung worldwide. These ‘personalised’ portable technologies enable male consumers 
to browse and purchase anywhere in the world at any time especially if the sizing data has already been 
established with the company leading to repeat orders being easier to manage online.

260
Fashion-Technology and Change in Product Development and Consumption
﻿
Social Media Platforms Providing New Channels of Consumption
There is a buzz about social media platforms and how they change fashion business and consumers 
behaviour. Ross (2009), Nguyen (2011) discuss the importance of social media in the menswear market 
with focus on Facebook, U Tube and Twitter as premium social platforms for fashion, more recently 
Pratt et al (2012) in his research paper on the technological evolution of the fashion industry discusses 
social media from the perspective of communicating experiential web-atmospherics. There is a body 
of knowledge which includes Kurniawan (2000), Schenkman and Jonsson, (2000) and Mahlke, (2008). 
Manganari (2009) reviewed the ‘Store atmosphere in web retailing’. A conceptual model of consumer 
responses to the online store environment was tested which paralleled with the physical store environment 
in terms of sensory perception, accuracy of product information and virtual social presence, enabling 
consumers to choose how they want to purchase in different contexts. According to Pratt et al (2012) the 
technological infrastructural evolution in e-commerce is leading to the development of different online 
business models such as Personal subscription a monthly club that identifies style and price preferences 
from a pre-set questionnaire. The system can select products from the consumer’s perspective or remind 
the customer that they last bought an item at this time of the year and new lines are currently available 
creating a value-add personal shopper offering for regular customers.
Social merchandising and crowd-producing are business or revenue models based on user-generated 
content, reviews posts, blogs or articles by non-expert contributors (Pratt et al, 2012); also known as 
Digital Influencers (Spicer, 2015). User-generated content is a key part of the consumer decision pro-
cess and purchase and online/offline shopping can be used as feedback via social merchandise and for 
crowd-producing (Pratt et al, 2012). Facebook and Instagram, which is more popular with the younger 
demographic, can influence prospective customers and their friends. Payment for this is required but 
less is now spent on traditional advertising budgets.
Purchase platform model via social media is also becoming popular in western economies and in 
emerging markets such as China where bespoke/semi-bespoke tailors and high end International brands 
are keen to enter. The most recent survey of tailors (Ross, 2012a) showed a general uptake of social 
media/blogs to purchase or advertise menswear. Many have managed to successfully balance traditional 
bespoke quality and luxury service with new experiential online sites that would appeal to the younger 
and less traditional consumer.
RESEARCH METHODOLOGY
The background to the topic will be reviewed and compared with the collection of data and case-studies 
following a discussion on the research methodology being utilized.
The methodology used throughout the longitudinal study on the subject of bespoke/semi-bespoke and 
high end tailors has been qualitative in nature in line with an Interpretative Research Philosophy from 
which the research questions have been developed and will need to be interpreted via observation includ-
ing the online and offline study of tailors and interviews with the industry and academic professionals.
The research approach is deductive in terms of the review of existing papers on 3D technology and 
management strategies but inductive in the process of conducting empirical research through structured 
online observations of e-tailors and multi-channel tailors and industry/academic expert interviews.

261
Fashion-Technology and Change in Product Development and Consumption
﻿
Sampling
Sampling was carried out via an updated Google search of the 38 tailors who had previously described 
themselves as either utilizing 3D digital technology or using the descriptor ‘Semi-Bespoke’ which 
indicates a positioning and open-ness to innovative new technologies other than that of a traditional 
bespoke tailor. Additional new or established tailors that are currently marketing themselves with the 
above descriptors are included in Table 1, with the first three entries being newly added to the list fol-
lowing the online observation study conducted in July 2015.
Industry/academic experts are key to the study in order to understand the technological advances made 
in body-scanning etc. over the period since the last study was conducted (Ross, 2012a). Key informants 
are those with specialist knowledge and position on processes that add extensive detailed knowledge 
to the research. Payne and Payne (2004) consider them particularly valuable sources of information. 
Therefore in-depth interviews were recorded where possible when conducted between March-July 2015. 
For recorded interviews, observations of tailors and digital fashion professionals made in-store or online, 
discussions and note-taking content analysis is applied to either transcribed interviews, or paraphrased 
summary of the responses in line with the analysis and discussion that follows in the next section. Table 
2 shows who, when and what were the criteria for their selection.
Research Strategy
The research strategy has been developed to fulfill the preset research questions with additional sub-
questions that were considered relevant to fully answer the focus of the chapter. These are listed below 
and were either considered as secondary or primary research or a combination of both. Research questions 
1 & 2 are discussed in the literature review and will also be explored further through the case-studies. 
Research question 3 has been utilized as a structural template for both secondary and primary discus-
sion. The following g four Research questions will be discussed thematically in the recommendations 
and conclusions.
1. 	
How has new fashion digital technology enabled men to co-create and consume their own styled 
apparel?
2. 	
To identify from case-studies successful contemporary best practices utilizing digital technology 
and new business models.
3. 	
To test the usefulness of the 3D-4C’s Process Model to identify and classify the best practices of 
3D-fashion technology for Bespoke/High end menswear.
4. 	
Identify and address the key management challenges of menswear design, manufacture and con-
sumption for the future
Amongst the most common qualitative research methods used are ‘structured observations’ and ‘content 
analysis’ these are again selected as in Ross (2012a) and endorsed by Silverman (2003) to review all the 
e-tailoring websites to establish who markets their use of digital technology as an open innovation and 
those that either prefer to market themselves as ‘traditional’ or actually adhere to the traditional Savile 
Row tailoring methods. As Content analysis is also used for either text-based or image-based analysis 
from visual material such as websites this was considered the most viable methodology combined with 

262
Fashion-Technology and Change in Product Development and Consumption
﻿
structured observations that Rose (2003, p. 55) writes ‘will not be considered woolly but rigorous, reli-
able and objective.’ Clearly the researcher fits into the category of subjective as her prior knowledge and 
studies have informed her perception on the topic, but it is therefore important that the strategies used 
for the research are as objective in their presentation of the data as possible.
Table 1. Survey of online/multi channel tailors utilizing digital technology/e platforms
Name of Tailor/Company
Market Position and Description of Digital Technology
Dates of Structured 
Online Survey
The Lanieri Pop-Up @ 
Old Street Station*
Italian Tailoring made to measure younger mid-market manufactured in Italy 
online & popup shop 3D body scanners
2015
Arden Reed*
USA Mid-Market tailor with 3D Body-scanner travelling tailor truck
2015
Tailor Made London *
21st century bespoke tailoring mid-premium end of market 3D Body scanning
2015
Savile Row Co London
Bespoke & Ready to wear mid-market with emphasis on digital use only on the 
ready to wear and some accessories
2011 & 2015
Richard James
Bespoke, Semi Bespoke & Ready to wear; Savile Row Contemporary
2011 & 2015
Kilgour
Bespoke Ready to Wear Savile Row traditional and Contemporary
2011 & 2015
Dege & Skinner
Bespoke & Ready to Wear Savile Row traditional
2011 & 2015
Marc Wallace
Made to measure & Ready to wear young to mid- market
2011 & 2015
Steed Savile Row London
Bespoke, Semi Bespoke Savile Row traditional and young semi-bespoke digital 
pattern manipulation
2011 & 2015
Henry Bailey
Bespoke & Semi Bespoke Savile Row traditional
2011 & 2015
Souster & Hicks*
Bespoke, Semi Bespoke & Ready to Wear traditional 3D computer cutting systems 
used since late 1980’s
2011 & 2015
Cad & the Dandy*
Bespoke, Semi Bespoke & Made to Measure Savile Row style Online design 
visualization tools
2011 & 2015
Max Hence
Bespoke & Semi Bespoke English & Italian London Contemporary
2011 & 2015
Richard Anderson
Bespoke & Ready to Wear Savile Row Traditional
2011 & 2015
A suit that fits*
Semi Bespoke & Ready to wear Young low-mid market 3-D body scanner for 
custom-made suit
2011 & 2015
Jasper Littman**
Bespoke & Semi Bespoke mid-to high market traditional
2011 & 2015
Anderson & Sheppard
Bespoke, Made to Measure & Ready to Wear Savile Row high end traditional
2011 & 2015
Huality tailoring
Bespoke & Semi Bespoke Young mid market
2011 & 2015
Dress2kill*
Bespoke London low-mid market Online ordering and sizing system
2011 & 2015
TheMastertailoruk.com*
Bespoke & made to measure young mid market computerized cutting equipment
2011 & 2015
King and Allen**
Bespoke mid-high market
2011 & 2015
Henry Herbert
Bespoke Savile Row mid-market
2011 & 2015
Suitsupply.com
Made to measure & Ready to wear Italian style mid market
2011 & 2015
Terencetrout.net
Bespoke, Made to measure and Ready to Wear London mid market
2011 & 2015
Nutters of Savile Row
Bespoke & Semi Bespoke Savile Row traditional contemporary Mid-High market
2011
(Ross, 2011, 2015)
*Openly state they utilize digital technology in one form or another the rest have online e-tailoring platforms
** Do not openly state they utilize digital technology on their online e-tailoring platforms but do use either CAD/CAM or 3D cutting 
systems.

263
Fashion-Technology and Change in Product Development and Consumption
﻿
CASE-STUDY PRACTICES AND RECOMMENDATIONS
The analysis of the interviews with industry/academic experts and three case studies of Tailors, a sole-
trader, an SME company plus Change of Paradigm all will be thematically structured around the 3D-4C’s 
Process Model, concluding with a discussion on the future of bespoke and high end menswear and the 
challenges for fashion management. The exposition of the ‘4Cs’ C= Capture, C= Create, C= Construct 
and C= Consume follows.
Table 2. Industry professionals and academics in-depth interviews
Date/Place
Industry/Academic Criteria
Respondent
25/3/15 in-depth Mobile 
telephone conversation & 
access to a copy supplied of the 
presentation for a new launch 
online e-tailor business model
Alexander Ross is a young up and coming tailor/designer – a sole 
trader hoping to expand into the semi-bespoke suit/jacket market 
utilizing online measurements and possibly body-scanning in the 
future. Studying for a BA Fashion Management part time to help 
develop his e-business model. (Now graduated)
Ross Alexander Company 
owner of Alfie Bond 
Designer/Tailor
24/3/15 An in-depth Skype 
telephone conversation to 
discuss new business models 
utilizing 3-D technology 
including the launch of his 
own online business Change of 
Paradigm
Philip Delamore has 25 years extensive experience in Fashion and 
Technology was a Research Fellow at LCF prior to becoming Co-
Founder & Head of Digital Innovation at ‘Change of Paradigm’’ the 
company explores new frontiers in digital fashion through its 3D 
fashion e-commerce platform and its Digital Haute Couture project. 
His industry relevance covers sitting on the BSI committee for new 
ISO standards for Digital Fitting and A research project funded by 
the Technology Strategy Board exploring ‘Fashioning Metadata 
Production Tools in 2012.
Philip Delamore Co-Founder 
& Head of Digital Innovation 
at Change of Paradigm
16/6/15 Meeting at St Georges 
Hotel London to discuss current 
Body scanning and 3d digital 
technology
Jennifer has 20+-years extensive experience in fashion & technology 
research and lecturing at LCF and UCL and De Montfort University. 
She has published many conference papers, journal articles and a 
book chapter on ‘Sizing systems, fit models and target markets’ in 
Sizing in Clothing 2007. She currently is an industry consultant and 
runs a special interest group committee on Fashion and Technology 
for the International Textile Institute.
Jennifer Bougourd 
Consultant
25/6/15 Meeting at UCL to 
discuss his body-scanning 
company Sizemic and industry 
best practice for use of 3D 
digital technology for fashion 
ready to wear and bespoke 
tailoring
Andrew Crawford is the founder of Sizemic a fashion technology 
company that provides a one-stop sizing and garment fit solution for 
the fashion industry based on 3D body data from scanners. Utilizing 
Size UK data for consultancy, size chart development, customized/
generic fit mannequins and 3D pattern development. All critical to 
contemporary use in e-commerce, platforms and multiple channel 
tailors.
Andrew Crawford Chief 
Executive of Sizemic
25/6/15 John Buni tour and 
interview of Tailor Made 
London Shoreditch, London
John Buni is the co-founder of Tailor Made London a successful 
multi-channel tailoring company that uses body-scanning. He plans 
to open a new London branch soon and is researching the viability of 
an app as well as the e-platform consumption method. He has been 
involved in industry technology studies and has been funded for two 
recent research project funded by the Technology Strategy Board 
exploring ‘Fashioning Metadata Production Tools in 2012 and the 
latest ‘Fashioning Digital Production Tools’ just completed in March 
2015.
John Buni co-owner of Tailor 
Made London
19/6/15 Does Size Matter 
The Future of Online fit LCF 
ESRCSI Industry feedback 
Symposium with discussion 
panel at LCF John Princes 
Street, London
Invite from LCF to attend dissemination presentations from industry/
retailers and academic research perspectives on latest fashion sizing 
technology for HC; B2C mid-market fashion; B2B and High end.
Philip Delamore, Fanke Peng 
University of Canberra & 
Mouhannad Al Sayegh LCF 
and panel discussion Lynne 
Murry LCF, Asaf Moses Fit 
Analytics Lara Mazzoni Bodi.
me Jim Rudall from Fits.me
(Ross, 2015)

264
Fashion-Technology and Change in Product Development and Consumption
﻿
C= Capture Sizing for Consumer
Lara Mazzoni co-founder of Bodi.Me,, a panel member at the LCF Does Size Matter The Future of 
Online Fit Symposium 2015, discussed how utilizing body-scanning for B2B such as military uniforms 
mass-customization could be very cost effective as it is more accurate and quick to carry out on behalf 
of the armed forces, taking a fraction of the time of traditional measurements, with the consequence 
of saving resources and providing a better fit but not concerned about individualized fashionable style 
(Mazzoni, 2015).
Andrew Crawford in an in-depth discussion agreed and shared his experience of over 10 years in 
this field ‘Sizemic demerged out of Bodymetric had two focuses the consumer led and the other was 
very much B2B for the clothing industry using Size UK and fit data for corporate work which had more 
mileage - in that the client has a lot closer contact with the customer.’ Only having one design and mak-
ing it fit B2B is a more commercially viable prospect. This could also be applied to other areas of B2B 
successfully such as the leisure industry, sportswear and corporate uniforms. The downside of poor retail 
experience is not a critical factor for B2B, or the issue of IP of the body-scanned data. This is congruent 
with the literature on 3d bodyscanning for the armed forces (D’Apuzzo, 2007).
Conversely, Asaf Moses the founder of Fit Analytics, a panel member at the LCF Does Size Matter 
The Future of Online Fit Symposium 2015, focused on the issues around business and technology in 
order to improve the fashion shopping experience and help determine the correct fit for the customer with 
their chosen brands (Moses, 2015). However his attitude is that all consumers have a different concept 
of fit which is not directly related to sizing measurements but links to individual taste and fashion so for 
example do you like a ‘tight fitting’ shirt, ‘tailored’, ‘relaxed’ or ‘loose’ in specific styles or brands. His 
view is that he recommends size is an individual concept and more about shape than just measurements, 
although measurements are taken by 3D technology. This is supported by the literature on Anthropometry
(Simmons & Istook, 2003) identifying the fact that no two people are ever exactly the same size or 
shape even if they are wearing the same sized clothes, Jim Rudall at Fits.me, also a panel member at the 
LCF Does Size Matter The Future of Online Fit Symposium 2015, is in agreement about shape being 
paramount but this raises the contentious issue of both the tailor and the fashion industry re-thinking 
all of their sizing strategy and investing time and money to research Anthropometry from a contempo-
rary commercial perspective (Rudall, 2015). However, this is not currently practical until both cost and 
simplicity of use has become more acceptable within the fashion industry as discussed by Delamore & 
Sweeney (2010).
Andrew Crawford in the recorded interview discusses how fashion sizing has evolved to date:
The industry has improved in some areas…to standardize fit and improve the accuracy and consistency 
of fit, so that some of the variables of sizing or fit are limited if not removed. Retailers depend on user 
fit mannequins which is real base-standard, their bible, or they would have to input a lot of subjective 
changes of new data from fit-models and mannequins that are not the same in their measurements or 
shape… the key point of reference should (in my opinion) be a fit-model…tweaks and adjustments may 
be made but it should still fit that fit-model. (Crawford interview, 25th June 2015) 
This aligns with the literature in that 3D sizing is the main use for clothing gradients in the mass market 
(Simmons & Istook, 2003).

265
Fashion-Technology and Change in Product Development and Consumption
﻿
Issues Surrounding In-Store Experiential Consumption
Andrew Crawford continued to reflect on the in-store experience of setting up body-scanning for jeans 
at “Selfridges which was effectively selling a very personalized service, so the quality of the staff and 
their ability to operate the systems and give good advice was important as they were offering a service 
and a product that is very fit sensitive and so you raise expectations of the technology,” (Crawford in-
terview, 25th June 2015). This supports the view that in-store body-scanning does not sit well with the 
‘experiential retailing’ environment (Ross, 2007; Ross, 2012a; Apeagyei, 2010; Delamore & Sweeney, 
2010) created as part of the concept of the ‘retail theatre’ identified by Pine & Gilmore (1999).
Issues of Acceptance and Accurate Fit for the Consumer and Fashion Industry
The issue of measurements taken via body-scanners, electronic tape measure or traditional tape measure 
is considered a starting point and the shape of the person and their individual style preferences are key 
to a personalized/customized/bespoke fit. This is affirmed by Andrew Crawford, Jenny Bougourd and 
Philip Delamore in their interviews and presentations at the Symposium. Jenny went as far as to question 
‘what does measurements between two points of the measuring tool hold for the future?’ in her view old 
ways that need to be fundamentally changed in the future fashion industry. SERVIVE 3 year EU HRC 
Funded Research Project. ‘SERVice Orientated Intelligent Value Adding nEtwork for Clothing SMEs 
embarking in Mass-Customisation’ (Lead researcher 2011-12 Jenkyn-Jones) found similar psychological 
barriers to implementing shape language or symbols such as triangle or square, vs pear or vase-shape 
to represent and capture differing body sizes.
Lynne Murray Director of the Digital Fashion Studio at LCF, and on the panel for 2015, explained 
how some customers were concerned about viewing their own shape on either a body scan, virtual 
simulation or augmented reality because of imperfections such as belly-shape or size. A there is no ac-
cepted norms in shape, size, or body type, viewing your own body-shape from the scanner is an issue the 
fashion industry has to address in relation to technology acceptance models (TAM). Andrew Crawford 
concurs with this issue ‘it is important to understand that some body-scanners create unnatural postures 
when taking shape data so some stand in odd postures others suck in their tummy knowing scanners 
can pick out this area.’
Andrew Crawford views sizing and shape in two distinct ways, capturing the accurate measurements 
of a consumer and visualizing these on a simulated mannequin based on Size UK data software adapta-
tions, then translating those into commercial grading sizes appropriate for different fashion brands and the 
Retail Psychology of sizing. What is meant by the latter is connected to the retail experience previously 
discussed as the concept of ‘retail theatre’ a term coined as part of the Experience Economy by Pine & 
Gilmore (1999) and part of the relationship bond between tailor and client identified as an experiential 
service ritual by Ross (2007, 2012a).
C= Create and Consume
Accurate measurements are captured and stored as a ‘cloud’ of measurement point data which can then 
be used to create a visualized mannequin from the data series which is translated into a virtual 3D de-
sign of block/patterns directly for the pattern gradients design in the mid-market menswear sector, or 
for the bespoke/semi bespoke tailor a customized avatar/mannequin of the individual client. This can be 

266
Fashion-Technology and Change in Product Development and Consumption
﻿
viewed in store by virtual-try-on magic mirrors or online/tablet/mobile and in the future with technol-
ogy such as smart watches. The self- image enables the customer to visualize himself in various styles, 
colorways and fits as well as with different accessories. How the customer views and purchases the ap-
parel is somewhat down to preference of channels, service expectations and price-band all connected to 
Retail Psychology, Consumption through e-commerce, online social media, apps and the mobile phone 
capable of being shared with friends driving both pleasure and consumption was previously discussed 
by Delamore (2015). This is discussed further in the two SME tailor case-studies offering semi-bespoke 
services but as Andrew states ‘Certainly if you are going into something semi-bespoke the consumer 
expectation immediately increases and rightly so’.
C= Construct
Although direct digital transference to CAM/CAD of the body measurements for pattern cutting is 
becoming embedded in the industry, in the main the more traditional manufacturing methods are still 
being employed in the menswear and tailoring sectors. Many more British factories are again being 
used for production as the Far East and Eastern Europe becomes less competitive in price and distance. 
This notion of ‘British is Best’ in terms of quality, service and interpretation of styles is affirmed in the 
case-study discussion.
3D printing is being researched and utilized in the Haute Couture and Independent designer end of the 
fashion market and this was discussed with Philip Delamore and Jennifer Bougourd. Delamore consid-
ers that we have technology for intermediate 3D knit and weave which has evolved and some solutions 
for leather-like materials and in the future 3D printed fabrics that can be worn by more customers than 
just one-off designer clients.
According to both Jennifer Bougourd and Philip Delamore from the customization side of things 
footwear has been the most successful for some time both at the Premium and mid-market level with 
examples such as Heron & Co shoe makers of Savile Row who use ‘Quick socks technology’ that takes 
a 3D mould of the customer’s feet for a bespoke high quality leather shoe or Nike who use electronic 
foot-scanners for customizing the trainer. Delamore considers ‘this is the most successful area of prod-
ucts as it is more akin to product design. 3D scanning and printing for a shoe is smaller and more cost 
effective. It is also a relatively rigid product’ (as opposed to a dress which needs to flow and drape with 
the body) ‘so there are lots of things you can do with it’. International expansion of the footwear market 
has previously been cited through The Left Shoe Company (2015).
Wearables can also be a key market accessory especially for the menswear sector where monitoring 
and communicating is more important than just fashion design. For example, The Intel bracelet claims 
in the online publicity that the device provides alarms and notifications, including SMS messages, 
calendar reminders and reminders for meetings sent to the wrist display. Also, two-way wireless data 
communication through a 3G cellular radio is available without the need to pair with a smartphone. The 
device charges wirelessly or through a USB cable’. The wearable at the moment is more quantifiable 
for men (although designed for women) who want to monitor sports activities from a health and leisure 
perspective while still staying in touch with the office.
Another fashion product that is radically changing in design and construction is knitwear according 
to Delamore who cited the Knyttan company. This is known on the promotional material as the ‘Fac-
tory of the Future’ and has an interesting new retail concept at Somerset House – very in tune with the 
demographic profile of the visitors and tourists visiting the revamped culture and art center. The con-

267
Fashion-Technology and Change in Product Development and Consumption
﻿
sumer can interact and create their own mass customization knitwear design which is made in front of 
you in minutes on the industry Stoll knitting machines. They will then sew them up and you can collect 
later that day. If you do not want to co-create they also have a selection of pre-existing models including 
designs by well-known artists, designers and musicians that can be customized into your style and fit.
Shifts in production-consumption will continue to require changes in the 3D-4C’s Process Model. 
Potentially little will have to be made traditionally with woven textiles and manufactured in factories as 
they can be printed directly from the customer’s body measurements and style/color shape preferences.
Delamore’s 3D fashion e-commerce platform is launching a Digital Haute Couture Project with in-
dependent designers who decide which of their designs they want to be hyper-realistically visualized on 
the commercial platform and when the (pre-agreed) minimal order has been reached and customer sizing 
captured they then (and only then) manufacture. The implications of this business model are radical as 
no expensive retail storage or showrooms of garments are needed, reducing the risk in price reductions 
and excess garments produced. All that is required is to Create the simulation, Capture the order and 
Construct- to-Consume. According to Pratt et al (2012, p. 366) in the future ‘additive manufacturing’ 
processes will be faster than weaving textiles and have ‘enormous possibilities for bespoke (custom 
made to order) clothing’, which may be something Delamore and other innovative fashion businesses 
will exploit.
C= Consume
Andrew Crawford considers that online consumption is more appropriate in the mid-market than the 
bespoke area of menswear as ‘retail theatre - the experience and the ritual between tailor and client is 
key… the consumer buys into that whole service and a personal relationship they can go and talk to 
their tailor like a psychologist. So introducing body scanners and technology in is not what is wanted.’ 
This aligns with an earlier study by Ross (2007) but by 2012 Ross (Ross 2012a) found positive shifts in 
attitude to introduction of technology being reflected by both the consumer and tailor.
With the online tailoring version the technology is about
giving access to bespoke some of it is about price, some of the reasons are they don’t have the time 
and they do not have the inclination for the retail theatre…but expect more in terms of fit and the tailor 
experience…they are after a better fitting product and personalization, whether it be the design, the cut 
of the suit, or fabrication, buttons and trim can make a big difference. Some people once they get to a 
fit or a style they like they may refabricate the style…they become designers in their own right. Some 
customers are quite confident and know what they are after but others want to be led because they are 
not confident, but they still want an element of uniqueness and personalization. They have bought enough 
product to know what they have bought before doesn’t fit them very well and they realize they want 
something that does fit them well, or they have personal eccentricities of body shape or size. (Crawford 
interview, 25th June 2015)
This is a myth as everyone is different in that sense and Crawford concludes ‘but this might lead to 
making it very hard to buy off the peg.’ Here the concepts of open-innovation, co-design and co-creation 
via mass-customization are all supported by the background literature including Ross (2009), Tseng et 
al (2014), Straub & Kerlin (2014) and Pratt et al (2012).

268
Fashion-Technology and Change in Product Development and Consumption
﻿
Changes in Consumption Channels and Preferences
Consumption channels and preferences have changed radically in the last decade and are set to change 
even more in the future. Pre-research before purchase on style/quality/price recommendations is more 
than ever relevant whether from professionals online through look books, style advice or viewing of 
your own avatar of visualized self-body-shape. This may take place in a retail environment with a tailor/
shop assistant, or via shop window with a mobile/tablet or wearable, on your home online computer/
tablet or while on the move. In addition to professional advice digital influencers of a non-professional 
nature may help complete the sale directly on business platforms, links to brands or by promoting and 
driving the consumption at a later date. According to Philip Delamore ‘the next generation of products 
enables consumers to scan things yourself with your mobile phone or tablet’ (Delamore interview, 24th 
March 2015) bringing sizing and co-design increasingly into the home. Consumption, customization 
or bespoke models are available currently at either end of the market. Because of ‘economies of scale’ 
the mass market can afford to implement this or Premium priced products so both high and low-end 
markets are embedding the every-day technology utilized by people to research, recommend, create 
and consume products and services. This includes Fashion bloggers viewed through a website or social 
media such as Facebook, Twitter or Instagram. Additionally, friends may also be influencers and can 
see the virtual you at the same time. The choice to consume in store or via other channels may differ 
with time preference and social mobility, making multi-channel or omni-channel consumption key for 
the future of the fashion industry.
FUTURE RESEARCH DIRECTIONS
The following three case studies represent current and future research directions from the perspective 
of Industry Practice for fashion technology-enabled product innovation and development for Bespoke, 
High End and Mid-Market menswear consumption.
Case Study 1: Small Medium Enterprise (SME) Sole-Trader Tailor/Designer
Background
Ross Alexander trained at Oswald Boateng on made to measure and bespoke from 2003-2007/9. His own 
business Alfie Bond http://alfiebond.co.uk/ - was launched online in 2012 as a visiting tailor. Company 
Mission was to provide quality British Bespoke menswear at a lower-mid price point. £500-£1000.
Ross Alexander is the sole trader of Alfie Bond, describing himself as a ‘suit designer’ and has re-
searched the market opportunity for ‘online self-measuring.’ He defines Bespoke as requiring 12 body 
measurements and special requirements for posture detail and Semi-bespoke requiring the same but is 
made from a block pattern rather than one that is made especially for the Bespoke client. He describes 
his market segment as between 25-45 and is aiming at a professional/creative industries man who likes 
fit and style. He considers the e-market to be a trend that is set to continue and as he wants his business 
in the early stages to be ‘lean and mean’ opted for online pure play only. His competitive analysis is 
shown as follows in Table 3 and Table 4.

269
Fashion-Technology and Change in Product Development and Consumption
﻿
This competitive analysis of a saturated tailoring market gave Alfie Bond the USP of offering high 
quality British Bespoke visiting tailoring services from an online website and the choice of ‘self-
measuring’ items such as a blazer or waistcoat or suit. He starts with the ‘blazer style jacket as it only 
requires three key measurements taken by the client’. Ready to measure sits in between ready to wear 
and bespoke. The reasons are that it is an alternative to high end ready to wear but with the option of 
a personal fit and a higher quality. The ‘ready’ stands for the fact that the garments are pre designed 
which fits perfectly into men’s fast shopping behavior. It has a small element of ‘bespoke’ as the garment 
is made based on your 3 body measurements and choice of fit. The blazer will be of the same quality 
and construction as the bespoke offering. But the same price point as high end ready to wear. As the 
concept is still made to order it means that the company can operate with no stock holding, which like 
the bespoke side of the company means it is a profit only business and cannot accrue debt. His research 
showed that consumer’s perceived barriers to purchasing online were as follows. Firstly they do not have 
the time to self-measure. Secondly they would not feel confident in measuring themselves for a garment 
valued over £300. Thirdly they would struggle to visualize the finished garment. Fourthly they would 
like to feel the cloth before purchase and finally they are concerned about returns and refund policy of 
the garment if the fit is not satisfactory.
Table 3. Industry Competition for ‘self- measuring’ menswear online
Company Name
Online Offering
Itailor
Entry level self measuring
Indochino
Mid range self measuring
Tailor store
Entry level self tailoring
Suitopia
Entry level self tailoring
Institchu
Low/mid range self measuring
Tailor4less
Entry level self measuring
(Alexander 2015)
Table 4. Industry competition British Bespoke visiting tailors
Name of Company/Tailor
Market Position Description
Neil McCann Bespoke
Mid range British visiting tailor
Apsley Tailors
Mid range Hong Kong tailor
Cad and the dandy
Mid/ high range Savile row based
Alfie Bond
Mid range visiting Tailor
Anthony Leslie
Contemporary visiting tailor
Tom James
Contemporary city based tailor
Truth and Tradition
Mid range British visiting tailor
Shipton and White
Mid range British visiting tailor
Clement Church
Mid range visiting tailor
(Alexander 2015)

270
Fashion-Technology and Change in Product Development and Consumption
﻿
Addressing Issues of Consumer Acceptance of Bespoke Online Purchase
To understand and overcome these issues he has interviewed Neil McCann Director of McCann Be-
spoke who said ‘If you show and tell people how to measure themselves and you just input the data to 
your supplier it devalues your skill and value and may affect your profit, if people believed measuring 
is a simple process then why would you employ (tailors) to measure.’ Matthew Norton from Norton 
Townsend agreed and added ‘it is the theatre of measuring that people pay for…the romance of bespoke 
tailoring’ this returns to experience being pleasurable and adding to the retail theatre.
Alexander dismisses avatars and virtual models as gimmicky and they are not included in his 10-year 
business plan. However, he realizes that the creation and consumption has to fit with the retail experience 
in quality of fabric and color match. He uses, alongside the 3 key measurements for the blazer jacket, 
a fashion style – fit language of Slim/Classic and a box for additional comments/requirements about 
posture or preferences. This is congruent with the discussion on whether shape is more key than size by 
Asaf Moses and Jenny Bougard. Ross Alexander plans to utilize a drop box ‘self-help’ video on taking 
your own measurements. Alternatively, the client can request a consultation on the phone to give him 
confidence, or a personal fitting or viewing of swatches of the cloth to touch. For a product that is a bet-
ter fit than an off the peg jacket at approximately £500 it is worth the gamble. When questioned about 
the inevitable returns of ill-fitting blazers if the client has got it wrong. Alexander stated his insurance 
of 80%+ correction rate is in the making with sufficient fabric allowance of 3 inches there is a lee-way 
to correct.”
The construction used to be in Portugal but he did not like the way they cut from standard block pat-
tern manufacturing and now only uses British manufacturing in Leeds. The company uses CAM/CAD 
digital technology for sizing patterns but manufactures traditionally in the factory. Interestingly when the 
company website was observed there were at least two other traditional Bespoke tailors listed in Table 
1 Jasper Littman* and Allen & King* who stated on their websites as taking measurements and cutting 
their own patterns in the Savile Row way as listed as B2B customers of the Leeds factory.
Ross Alexander currently uses W-O-M promotion for recommendations on the website but is also 
a social media fan on Twitter, Facebook and Google+ Figure 2 is self-promotion at Ascot on Twitter 
where he models his own suit and those of his friends, with viewings of fabric and links to the website 
Alfie Bond. Ross Alexander’s 10-year plan is to build a reputation and enter the international market 
probably with a partner who will specialize in the website requirements.
Case Study 2: Small Medium Enterprise (SME) - Small Enterprise Tailors
Background
Tailor Made London was established in 2008. Their mission was to bring elite tailoring to the mass 
audience. They offer high quality bespoke tailoring at affordable prices according to their website http://
www.tailormadelondon.com.
Their uniqueness is traditional tailoring with the fusion of the latest cutting edge 3D body scanning 
technology. This takes 10 seconds for the laser to take the required measurements that enables them to 
create a perfectly fitted garment. Price bands start at £660 for a two-piece suit but they position them-
selves more and more at the Premium end of the market. John Buni the Co-founder plans to open another 
store in the West End of London based around the notion that ‘British is Best’ it will be called made in 

271
Fashion-Technology and Change in Product Development and Consumption
﻿
England. The site for the Shoreditch Studio is in the trendier end of the city near Old Street. His main 
clientele are aged 25-45, professional, Creative Industry, Advertising but all have a non-conservative 
approach to their dress.
Addressing Consumer Acceptance in 3D Scanning and Retail Experience
The invitation to tour the studio included a look at the body-scanner which is in large space that would 
not evoke issues of claustrophobia or alienation of the technology, despite having to stand in a set posi-
tion and keep still for the 10 seconds. In a bid to blend the traditional tailoring environment with the 
contemporary technology John has a digital print design on the side of the body-scanner viewed from 
reception of a library of books which is a backdrop to a comfy and well used chesterfield leather couch 
in the client-tailor consultation area. This post-modern ironic take on traditional tailoring with the fu-
sion of the latest 3D body scanning technology pays homage to the concept of the importance of the 
‘retail experience.’ The ritual bond between tailor and client is further fulfilled in the studio site visit 
with the personal consultation of either the tailor or style consultant who will explain the three levels of 
tailored suits and show style cards of fit and cut before or after you choose from a wide range of fabrics 
Figure 2. Ross Alexander modelling a suit at Ascot
(Alexander, 2015)

272
Fashion-Technology and Change in Product Development and Consumption
﻿
and finishes, with personalization of embroidery and buttons. A free full hour consultation is offered 
that includes fabrics selection as well as all manner of customization, body scanner measurements and 
fitting to gauge personal preferences. An overview consultation that takes approximately 30 minutes is 
offered for those prospective future clients.
The style cards illustrated in Figure 3 show the difference in make between traditional and semi-
traditional jackets in the inner facing of the jacket. The second image illustrates traditional front canvas 
(FC) make and the handmade make with extra details such as button holes. The final image illustrates the 
unstructured (less fitted style) often associated with the Italian suit. The company 3D scanner transfers 
the data taken from the client directly to their manufacturer/s for pattern cutting via CAD/CAM and 
construction in the UK/Italy.
The website guides you through the process and gives style and cut examples of jackets or suits on a 
professional tailor’s dummy (no faces or bodies) only your shape and fit from the laser scanner will be 
visualized on a virtual mannequin with the suit. There is a style blog for those that want inspiration or 
guidance. Tailor Made London is linked to Pinterest for promotional ideas that can be ‘pinned’, Facebook, 
Google+ and Twitter which can be ‘liked’ were being used at the time to promote the correct dress for 
Wimbledon Tennis fortnight. Facebook is most popular with the older customer, whereas Twitter and 
Instagram are the first go-to for ideas and research for the younger demographic. John talked about his 
Figure 3. Tailor made London style sheet descriptor for traditional and handmade make

273
Fashion-Technology and Change in Product Development and Consumption
﻿
unconventional clients and these were represented online by images of a same-sex wedding showing 
complementing suits in different blues and patterns and an editorial opportunity in Style magazine. This 
demonstrates the importance of digital influencers leading to recommendations and sales.
Tailor Made London have been involved in two research projects to improve the speed, efficiency 
and experience of the customer through the design stage, production chain from customer to manufac-
turer with metadata tools. The actual content of the reports are still confidential to date. What can be 
said is that John’s business is a successful blend of traditional tailoring ritual and 3D digital tools which 
he hopes to keep improving with the changes and buying behavior of his customers so that an app for 
viewing and purchasing his products is planned for the near future.
Case Study 3: Small Medium Enterprise (SME) - 
E-Platform New Retail Business Model
Background
Philip Delamore and his business partner plan to launch via the company ‘Change of Paradigm’ a cutting-
edge new retail business later this year. The research and development has taken a little over 2 years to 
come to a working concept that is worthy of investment.
The new business model is different from Net-a-Porter/Mr Porter who as a pure-play retailer shows 
high end designer products as photographs or video content. Philip explains how the technology process 
differs for his e-platform ‘these (photographs) will be replaced with 3D simulations that are physically 
accurate and photographically real’. The platform will sell pre-ordered products that have not yet been 
made. ‘The business model is similar to Made.com - who sell premium furniture and homeware designs 
that are produced to specifications by customers and only manufactured when the order is completed. 
‘As a direct retailer who cuts out the middle man they can afford to produce products for up 50% less 
than other retailers. The customer’s incentive to purchase through their e-platform and wait 6-8 weeks for 
delivery is that they are paying 50% less that they would expect to pay in a standard format retail store’. 
The pre-order system of this direct retail model would be made available like Moda Operandi whose 
offer is the opportunity to pre-order designer collections straight off the catwalk for high end consumers.
Philip explained how the mannequin design works ‘The designers give them a sketch or a pattern 
and they produce a simulation. For them it is very low risk and low cost as they do not have to produce 
a prototype and only manufacture when the order is confirmed’. A minimum order requirement is estab-
lished before the simulation is created. If the order falls below this minimum the manufacture does not 
go ahead and the customer is informed. Their market is High-end designers initially for women’s wear 
but they hope to go into menswear soon after the launch. Philip said ‘The designers are well established 
high end and produce exclusive designs for their e-platform.’ Many designers have been contacted di-
rectly by Philip or his partner at London or Paris Fashion week. The response he assured me ‘was very 
positive’ but ‘although some of them don’t want to frighten their existing retailers.’ This is common 
when new business models and operations start up as designers will keep one foot in each camp until 
they are totally assured that the e-platform will work.

274
Fashion-Technology and Change in Product Development and Consumption
﻿
Addressing Issues of Visualization and Representation for the Customer
At the current time the products available will be standardized and made to fit the customer through any 
form of sizing and measurements that is available to them. When discussing the creation of the virtual 
model Philip told me they had looked into Avatars and decided for the high end fashion model it was 
important for them ‘not to look too Avatary or Gamey for this market’. The head/face when shown will 
be generic as most e-platforms cut off the head completely as this enables the consumer to imagine 
themselves in the outfit.
The company has been working on a funded research project with physical modelling by companies 
such as Dreamworks with animation for films in trying to improve the realism particularly for high end. 
The sizing had been researched and identified a top quality avatar model technology company would be 
the best solution for their market based on population scan data. Philip said they
… were fairly agnostic as to how they input the data as long as the body-scan can receive reasonable 
measurements. But clearly a body-scan model would ‘give a very accurate fit, while an Xbox (used for 
gaming and sports workouts) connection would be accurate and manual model measurements would be 
less so. (Delamore interview, 24th March 2015) 
When asked about the manufacturing side of the business Philip responded ‘Originally we considered 
taking on the manufacturing but decided against it…and pushed it back to the designers to deal with 
but when we go for the customization model we may consider it again.’ Sizing and the fashion fit were 
part of the standardized product model but as soon as they convert to a customization body model with 
multiple style preferences they may need to look at manufacture again ‘As dedicated lines will require 
factories that can cope with this sort of thing…. as many manufacturing processes are still catching up 
with digital technology.’
CONCLUSION
In summary and addressing the research questions and issues posed at the start of the chapter, 3D fashion 
technology is enabling men to have more creative input and choice in menswear design and fit. Although 
3D technology is not currently being used throughout the complete supply chain for the Bespoke/Semi-
Bespoke and high end menswear fashion market there is evidence of increased acceptance by both the 
tailors and the new breed of customer who have shown considerable technological uptake since the initial 
surveys were conducted in 2007 and 2012 (Ross, 2007, 2012a) especially in body-scanning. Addition-
ally consumption is now not just at the end of the supply-chain but comes in many different channels 
from on/off line mobile, tablet, apps and social media. This is enabling men to purchase apparel on the 
move from any country in the world especially when the initial sizing data has been created, received 
and stored for use by the tailor, manufacturer or online company. As Wearables are the next generational 
mode to drive consumption, this will be an additional channel for fashion managers to understand and 
utilize for promotion and consumption, indicating that fashion marketers should be currently researching 
usability and future best for this channel. As customers become more accustomed to taking their own 
measurements, or having them taken digitally, their expectations of quality of fit will increase and no 
longer just be associated with the premium market. Fashion management must progressively embrace 

275
Fashion-Technology and Change in Product Development and Consumption
﻿
change in consumption and production techniques as the paradigm shift of co-creation and bespoke is 
now established in all fashion markets. Embracing the new digital technology also enables innovative 
fashion businesses to expand internationally faster and with less risk and cost.
Visualization of the consumer’s ‘self’ through sophisticated avatars and realistic mannequins have 
made body-scanners or self-measuring important parts of the retail theatre experience, in parallel to 
in-store magic-mirrors and window displays with augmented reality. As these retail technologies will 
continue to evolve and grow, fashion managers should consider the full range of consumer technology 
that has the potential for augmented product display and viewing.
From the construction perspective Laser cutting is being utilized on fashion accessories and some 
limited premium menswear detailing, this and 3D printing will in the future hold the key to non-woven 
textiles and trimmings and may radically change the production-consumption traditional supply chain 
model. We are already seeing consumers operating as co-creators and tailoring is clearly a market where 
the balance between customer co-creation choices and the tailor’s input via the technological ability to 
configure size, shape and other preference choices needs to be become economically viable if companies 
are to remain commercially competitive.
The digital technology good practice case-studies identified in this chapter Tailor Made London, 
Alfie Bond and Change of Paradigm all are exploring new business and revenue models which have 
evolutionary, or in the case of the latter, revolutionary concepts in line with changes in society and ac-
ceptance of technology being part of everyday life. This has been enabled by the research and develop-
ment of scientists, engineers and designers who market software and equipment for the retail fashion 
industry such as Fit Analytics, Bodi-me, Fits.me and Sizemic. Many of these companies have invested 
entrepreneurially in change in sizing and the supply chain without making profits in the short term but 
with the knowledge and conviction that change must and will be accepted in the fashion industry in the 
near future. The time for capitalizing on their investment and foresight is here.
The 3D-4C’s Process Model ‘Capture-create-construct-consume’ (mnemonic descriptor 3D-4C’s, 
process flow shown in Figure 1) has proved useful as a template to structure and analyse research on 
the current retail environment and consumption channels. In the fashion industry context this could 
prove in the future to have similar conceptual utility as the 4P’s general marketing model developed by 
Philip Kotler in the 1960’s. However, it must be considered as flexible in order to change and evolve 
as technology and consumer attitudes and behavior shift rapidly and inevitably in favor of new fashion 
processes and technology.
LIMITATIONS AND MANAGERIAL IMPLICATIONS
With regard to the challenges and limitations facing the menswear market, management must urgently 
face up to the limitations of the traditional sizing practices within fashion and style which have to be 
updated to take account of the individual preference for fit. The IP and security of customer size data 
needs specialist legal consideration within the retail fashion context as it is clear it will be utilized more 
frequently in the future for consumption.

276
Fashion-Technology and Change in Product Development and Consumption
﻿
FURTHER RESEARCH
Continual development and revision of strategy and organization is required by senior fashion manage-
ment to cope with the rapid increase in forms, variety and complexity of ‘channels of consumption’ 
driving the need for more research to be carried out on these areas in the future.
On the positive side new innovative business models and multiple revenue streams are opening up 
for those brave enough to look outside of traditional product development and consumption. This is 
an opportunity sweet-spot in time when technological advances in all areas will radically change the 
menswear fashion industry. In summary this is best expressed by Philip Delamore who identifies that
A paradigm shift is almost now embedded moving to the next (consumer) generation. Usability is re-
ally key as the technology now being put into consumer products … makes them adept at using this 
and therefore much more likely to use it without having to be educated or consult a store. (Delamore 
interview, 24th March 2015)
If this is true of consumers then the fashion industry has to be at least as adept and forward-thinking 
in their use of digital technology within and across all of the 3D-4C’s processes of the menswear sec-
tor. The aim of this chapter has been to give an up-to-date perspective on the literature regarding digital 
fashion in menswear and an insight into contemporary practices and explore the paradigm shift driving 
product development and acceptance of fashion technology.
REFERENCES
Almond, K. (2011, March 25-27). Bespoke Tailoring: The luxury and heritage we can afford. Proceed-
ings of the International Conference of Technology, Knowledge and Society, University of the Basque 
Country, Spain. Retrieved from http://eprints.hud.ac.uk/10817/
Apeagyei, P. (2010). Application of 3D body scanning technology to human measurement for clothing 
Fit. International Journal of Digital Content Technology and its Applications, 4(7.6), 58-68.
Bae, J.-H., & May-Plumlee, T. (2005). Customer focused textile and apparel manufacturing systems: 
Toward an effective e-Commerce model. Journal of Textile and Apparel. Technology and Management., 
4(4), 1–19.
Bougourd, J. (2007). Sizing systems, fit models and target markets in Textiles. In S. Ashdown (Ed.), 
Sizing in Clothing: Developing effective sizing systems for ready-to-wear clothing (pp. 108–151). Wood-
head Publishing in Textiles.
Bougourd, J., & Delamore, P. (2007). A case study in personalized digitally printed jeans.Proceedings 
of 9th IFFTI Annual Conference ‘Extreme Fashion: Pushing the Boundaries of design, technology and 
business. (pp. 234-247). Ryerson University School of Fashion Toronto, Ontario, Canada.
Cannon-Jones, A. (2015) Cutting Fabric for Fashion: Pattern Technology. In. Beauty & Fashion. Retrieved 
from fashion-history.lovetoknow.com

277
Fashion-Technology and Change in Product Development and Consumption
﻿
D’Apuzzo, N. (2007). 3D body scanning technology for fashion and apparel industry Proceedings of 
Electronic Imaging 2007.
Davis, F. D. (1987). Perceived usefulness, perceived ease of use, and user acceptance of information 
technology MIS Quarterly, 13(3), 319-340.
Delamore, P. (2015). Keynote Speech. Presented at the Industry feedback Symposium “Does Size Mat-
ter”: The Future of Online Fit LCF ESRCSI ‘15, London College of Fashion, London.
Delamore, P., & Sweeney, D. (2010). Everything in 3D: Developing the Fashion Digital Studio. www.
fashion.arts.ac.uk
Gilmore, J.H. & Pine, B.J., II. (1999). The Experience Economy. Harvard Business School Press, USA.
Gilmore, J.H. & Pine, B.J., II. (2000). Markets of One: Creating Customer-Unique Value Through Mass 
Customization. USA: Harvard Business Review.
Jenkyns-Jones, S. (2012). SERVice Orientated Intelligent Value Adding nEtwork for Clothing SMEs 
embarking in Mass-Customisation. SERVIVE.
Kuhn, R., de Fatima, R., & Minuzzi, B. (2015). The 3D Printing’s Panorama in Fashion Design. Moda 
Documenta: Museu, Memoria e Design, 11(1).
Kurniawan, S. H. (2000, April 18-20). Modelling Online Retailer Customer Preference and Stickiness: 
A Mediated Structurual Equation Model.Proceedings Conference 4th Pacific Asia Conference.
Mahlke, S. (2008) Visual aesthetics and the user experience. Proceedings of Schloss Dagstuhl Seminar.
Manganari, E, Slomkos & Vrechopoulos, G. (2009). A Store Atmosphere in web retailing. European 
Journal of Marketing, 53(9-10), 1140–1153.
Mazzoni, L. (2015). Panel Discussion. Presented at the Industry feedback Symposium “Does Size Mat-
ter”: The Future of Online Fit LCF ESRCSI ‘15, London College of Fashion, London.
Moses, A. (2015). Panel Discussion. Presented at the Industry feedback Symposium “Does Size Matter”: 
The Future of Online Fit LCF ESRCSI ‘15, London College of Fashion, London.
Nguyen, H. (2011). Facebook Marketing in the Fashion Industry. Arcadia. Retrieved from publicationd.
theseus.fi
Payne, G., & Payne, J. (2004). Key Concepts in Social Research. London: Sage.
Piller, F. T. (2000). Mass Customization (1st ed.). Handbuch Produktmanagement. doi:10.1007/978-3-
663-08187-6
Piller, F. T. (2007). Mass Customization (2nd ed., pp. 941–968). Handbuch Productmanagement.
Piller, F.T. & Diener, K. (2009-10). The Market for Open Innovation: Increasing the efficiency and ef-
fectiveness of the innovation process. Open Innovation Accelerator Survey.
Piller, F. T., & Muller, M. (2004). A new Marketing approach to mass customization. International 
Journal of Computer Integrated Manufacturing, 17(7), 583–593. doi:10.1080/0951192042000273140

278
Fashion-Technology and Change in Product Development and Consumption
﻿
Pine, B. J. (1993). Mass customization. Boston, USA: Harvard Business School Press.
Pratt, A., Borrine, P., Lavanga, M., & D’Ovidio, M. (2012) International Change and technological 
evolution in the Fashion Industry. In M. Angoetti, A. Carandini, W. Santagata, W. (Eds), Essays and 
Research: International Biennial of Culture and Environmental Heritage (pp. 359-376). Florence, Italy: 
Badecchi and Vivaldi.
Ratto, M., & Ree, R. (2012). Materializing information: 3D printing and social change. Firstmonday 
Peer-Reviewed Journal on the Internet, 17(7), 2.
Reilly, L. (2014, April 14-16). The shift from 3D body scanned data to the physical world. Proceedings 
of theShapeshifting Conference, Auckland, University of Technology
Rose, G. (2003). Visual Methodologies. London: Sage Publications.
Ross, F. (2007). Refashioning London’s Bespoke and Demi-Bespoke tailors; new textiles, technology 
and design in contemporary menswear The Journal of the Textiles Institute, August/September, 281-288.
Ross, F. (2009, September) Managing Fashion and the Internet: design and the Co-online experience. 
Proceedings of theSymposium track on Design Management British Academy of Management Confer-
ence, Brighton.
Ross, F. (2012a). A study of how Small and Medium-sized Enterprises tailors Utilize e-Commerce, 
Social Media, and New 3D Technological Practices Fashion Practice: The Journal of Design, Creative 
Process & the Fashion Industry, 4(2), 197–219. doi:10.2752/175693812X13403765252343
Ross, F. (2012b, January 13). Contemporary Men’s Fashion and New Technology; Shifting Perceptions 
of Masculinity, Menswear Aesthetics and Consumption. Fashion Colloquia.
Ross, F., & Jenkyn-Jones, S. (2009). Same-Sex Suits; Co-design, Style Advice and the identification of 
the Gay Aesthetic Experience. Proceedings MC/PC World Conference.
Ross, R. (2009) Same-Sex Suits; Co-design, Style Advice and the identification of the Gay Aesthetic 
Experience. Proceedings of theMCPC Conference
Rudall, J. (2015). Panel Discussion. Presented at the Industry feedback Symposium “Does Size Matter”: 
The Future of Online Fit LCF ESRCSI ‘15, London College of Fashion, London.
Schenkman, B. O. N., & Jönsson, F. U. (2000). Aesthetics and preferences of web pages. Behaviour & 
Information Technology, 9(5), 367–377. doi:10.1080/014492900750000063
Sen, A. (2007). The U.S. fashion industry: a supply chain review. International Journal of Production 
Economics, 114(2), 571-593.
Silverman, D. (2001). Interpreting Qualitative Data. London: Sage Publishers.
Simmons, K. P., & Istook, C. L. (2003). Body measurement techniques. Journal of Fashion Marketing 
and Management: An International Journal, 7(3), 306–332. doi:10.1108/13612020310484852
Spicer, K. (2015, July 5). Why this British Blogger Gets £800 for Posting Pictures like this on Instagram. 
Style Sunday Times, 34-37.

279
Fashion-Technology and Change in Product Development and Consumption
﻿
Squire, B., Brown, S., Readman, J., & Bessant, J. (2006). The Impact of Mass Customization on Manufactur-
ing Trade-offs. Production and Operations Management, 15(1), pp10–pp21. doi:10.1111/j.1937-5956.2006.
tb00032.x
Straub, J., & Kerlin, S. (2014). Development of a Large, Low-Cost, Instant 3D Scanner. Technologies, 
2(2), 76–95. doi:10.3390/technologies2020076
The Left Shoe Company. (2015) Retrieved from http://www.leftshoecompany.com
Toffler, A. (1980). The Third Wave. USA: Bantom Books.
Tseng, M. M., Hu, S. J., & Wang, Y. (2014). Mass Customization. In Encyclopedia of Production En-
gineering (pp. 72-87).
Tseng, M. M., & Piller, F. T. (2003). The Customer Centric Enterprise. Berlin, Germany: Springer 
Heidelberg. doi:10.1007/978-3-642-55460-5
Tungate, M. (2008). Branded Male; Marketing to Men. London, United Kingdom: Kogan Page.
Vilumsone, A., & Dabolina, I. (2012) Virtual Garment Creation. Institute of Textile Material Technolo-
gies and Design, Riga, Latvia.
Westbrook, R., & Williamson, P. (1993, March). Mass Customization: Japan’s New frontier. European 
Management Journal, 11(1), 38–45. doi:10.1016/0263-2373(93)90022-A
KEY TERMS AND DEFINITIONS
3D Body Scanning: Fully-automated 3D body scan producing digitized measurements of the human 
body by a selection of different processes (e.g. Scanning the profile of the body with Laser beams) to 
create a highly detailed 3D virtual model.
3D Printing: Is known as ‘additive manufacturing’ the process is faster than weaving traditional textiles 
or creating accessories; 3D printing involves slicing digital models into horizontal layers and then build-
ing the wearable up a layer at a time similar to how earlier design Rapid-Prototyping machines operate.
Augmented Reality: Is the integration of additional digital information via live video or photographs 
into the consumer’s visual environment in real time.
Avatar: Is an icon or figure representing a person, or some of the physical characteristics of a person 
on a display screen.
Bespoke: The term comes from the description of the tailor and customer bespoking the exact speci-
fication of the cloth, style and fit.
Digital Fashion: The application of digital technology in its many forms for the production and 
consumption of fashion apparel and services.
Haute Couture: (HC) is French. Haute means ‘high’ or elegant and ‘Couture’ means sewing but in 
contemporary terms indicates designer’s custom-made high fashion.

280
Fashion-Technology and Change in Product Development and Consumption
﻿
Semi-Bespoke: A recently coined descriptor. Semi-Bespoke, identified in 2007, utilizes the hand 
crafts of the traditional tailor combined with digital technology to accommodate the contemporary male 
consumer who values luxury and craft but wants a more accessible, cost-effective and faster suit.
Web 3.0: The term used to describe the projected evolution of the Web as an extension of Web 2.0 
which provides connective intelligence connecting data, concepts, applications and people through rec-
ognizing commonalities and relationships between initially independent data entities.
This work was previously published in the Handbook of Research on Global Fashion Management and Merchandising edited 
by Alessandra Vecchi and Chitra Buckley, pages 231-260, copyright year 2016 by Business Science Reference (an imprint of 
IGI Global).

Section 3
This section contains 6 chapters, giving a wide variety of perspectives and advancement of the discipline 
on 3D Printing. Within the chapters, the reader is presented with an in-depth analysis of the most current 
and relevant issues within this growing field of study. Crucial questions are addressed and alternatives 
offered along with theoretical approaches discussed. The section also introduces future research direc-
tions and topical suggestions for continued debate, centering on the new venues and forums for discus-
sion. In all, this text will serve as a vital resource to practitioners and academics interested in the best 
practices and applications of the burgeoning field of 3D Printing.
Critical Issues and Emerging 
Trends

282
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  15
DOI: 10.4018/978-1-5225-1677-4.ch015
ABSTRACT
The flexible production and process designs of complex and automated manufacturing systems – called 
Cyber-Physical Production Systems (CPPS) – lead to enormous challenges for the machine operator 
with regard to understanding their “behavior” and therefore their technical controllability. One way 
to face these challenges is to foster the operator’s appropriation of highly complex hardware-centered 
ICT-systems. Based on the historical development of CPPS and a short excursion into a study about the 
appropriation of 3D printers, the authors will adapt the concept of sociable technologies, as hardware-
centered appropriation infrastructures, to CPPS.
1. INTRODUCTION
Globalization and the resulting larger markets, better purchase conditions and cheaper production pos-
sibilities could offer a lot of potential to industrial companies, but it is often accompanied by a number 
of challenges too. A global market means global competitors. For long-term survival on international 
markets, industrial companies need to adapt their products to market trends at even shorter intervals 
Should I Try Turning It 
Off and On Again?
Outlining HCI Challenges for Cyber-
Physical Production Systems
Thomas Ludwig
University of Siegen, Germany
Christoph Kotthaus
University of Siegen, Germany
Volkmar Pipek
University of Siegen, Germany

283
Should I Try Turning It Off and On Again?
﻿
and market positions must be kept or even expanded with new, technologically more advanced products 
offering higher quality at competitive prices. Companies find themselves in an area of conflict between 
customer-driven cost pressure, quality demands and features expected of the products as well as services 
that customers request; customers who, if left dissatisfied, might otherwise shift to other companies of-
fering similar products. The steadily increasing demand for individual and customized manufacturing 
products is leading to an increased number of product variations, which generally means higher set-up 
times. This in turn leads to lower quantities and the result is higher costs per piece compared to tradi-
tional mass production.
Complex manufacturing systems offer a solution to these expectations and challenges. Modern 
manufacturing processes of shaping or cutting, separating or joining operations consist of a large variety 
of process parameters and resources needed to complete a product. Various mechanical functions of a 
machine and a range of ICT tools are brought together in the manufacturing process, resulting in a wide 
range of machine states and dependent process parameters. In the light of this complexity, the implicit 
pressure exerted by customers through their demands for increased variety, smaller batch sizes and more 
product complexity makes the monitoring and control of fully-automated manufacturing machines in-
creasingly confusing and opaque. The rise of complexity with problems of operating such systems could 
also be found in other application fields like crisis management (Comes et al., 2011).
Despite of a more flexible production and process design, these complex and automated manufactur-
ing systems still present enormous challenges for the machine operator with regard to their availability 
and technical controllability (Munir et al., 2013). A particular challenge is presented by the intra- and 
inter-organizational gathering and analyzing of relevant factors of the machine with regard to the real-
time-based complex production processes. This is especially critical if errors or incidents within the 
highly complex automated production process occur as the machine operator is often not fully aware of 
a hardware-related machine failure (Ludwig et al., 2014). The capability of the production machines to 
report internal and external critical situations systematically and to present the production process to 
the machine operators is often limited; thus machine operators are often hindered in appropriating the 
complex production process. In other words: If an error in the machine – which is usually automated – 
occurs, the machine operator does not always know why it fails nor how to fix it, because the state of 
the machine depends on too many parameters both inside, but also outside the machine itself (Ludwig et 
al., 2015). This has become especially critical since modern complex production systems often operate 
within an entire value-added chain; so, if an error occurs within a previous company, it can have fatal 
consequences within one’s own systems.
Within this paper, we outline current challenges in the field of Human-Computer-Interaction (HCI) and 
Computer-Supported Cooperative Work (CSCW) that deal with the question of how machine operators 
could be supported in appropriating the modern complex production machines known as cyber-physical 
production systems (CPPS). Based on a historical perspective on the development of CPPS (section 2), 
we will provide an excursion into a study, in which we empirically examined the appropriation of 3D 
printers as modern and complex machines in the field of additive production (section 3). Based on this 
study, we will outline design challenges for the appropriation of CPPS in the field of industrial produc-
tion (section 4).

284
Should I Try Turning It Off and On Again?
﻿
2. HISTORICAL PERSPECTIVE ON THE DEVELOPMENT OF 
CYBER-PHYSICAL PRODUCTION SYSTEMS (CPPS)
Based on their focus, we can divide current research and industrial approaches in the field of industrial 
production design into three areas: (1) data-centered, (2) technology-centered and (3) process-oriented 
(Birkhahn, 2007). We will present the development of CPPS as a combined approach of all three areas.
2.1. Data-Centered Approaches
Data-centered approaches mainly encompass Computer Integrated Manufacturing (CIM). Beginning with 
initial digital support systems in the design (CAE, CAD), work process planning (CAP) and manufacturing 
(CAM, CAQ), then adding the operation of production planning and control systems, CIM encompasses 
an integrated approach for information processing in industrial companies in the manufacturing sector. 
More comprehensive approaches of CIM – under the concept of Computer Integrated Business (CIB) – 
include the inter-organizational information flows within the production networks (Becker & Rosemann, 
1993; Geitner, 1990). The basic idea of CIM is to improve the consistency, timeliness and quality of 
enterprise data via computer-supported technologies to contribute to the process improvement within 
the company (Birkhahn, 2007). The concept of ‘digital factory’ describes a comprehensive network of 
digital models, methods and tools through the assimilation of integrated data management. The aim is 
for all the actors involved in the planning process to be able to access a digital model and the stored data 
base with the support of software tools (Westkämper & Jendoubi, 2003).
Pure data-oriented approaches such as the introduction of CIM tools or the design of a digital fac-
tory have however not led to the success anticipated (Birkhahn, 2007). Goals such as the shortening of 
run-through times, increasing product quality or reducing personnel in the production and management 
area were not achieved satisfactorily due to the often user-unfriendly handling and the resulting high 
training costs implied (Becker & Rosemann, 1993; Büring, 1997).
2.2. Technology-Centered Approaches
The development and advancement of technologies has resulted in a number of technology-oriented 
approaches such as ‘Smart Factory’ or ‘Smart Object’ for use in industrial production systems. Smart 
objects are hybrid products comprising a physical and a data processing component (Fleisch, 2001). 
The data processing of a ‘smart thing’ is hidden in the background and cannot be directly perceived by 
the user. Examples of applications for smart objects can be found, for example, in logistics as part of the 
equipment of goods with wireless radio chips (e.g. RFID) which transfer data directly to ICT systems. 
In the industrial context, the ‘Smart Factory’ tries to close the gap between digital planning and the 
actual process (Westkämper & Jendoubi, 2003), creating a transparent, optimized production resource 
management system, in which highly dynamic sensor data is integrated within the contextual environ-
ment model (Bauer et al., 2003).
2.3. Process-Centered Approaches
The flexibility with which companies must react to global market changes leads to process-centered 
approaches, which can be found in early concepts of ‘Lean Production’. One characteristic of ‘Lean 

285
Should I Try Turning It Off and On Again?
﻿
Production’ is that production planning does not primarily optimize machine capacity but is geared to-
wards customer demands (Womack et al., 1990). ‘Lean Production’ offers new options for the physical 
process design. The changes brought about by ‘Lean Production’ are so far-reaching that they signify a 
paradigm shift – especially in European and American production companies. More recently, the con-
cept of ‘Lean Production’ has been expanded towards a socio-technical system that integrates the entire 
company, the customers, suppliers, maintenance and process control as well as appropriate feedback 
loops (Shah & Ward, 2007).
2.4. Combined Approaches: Cyber-Physicial Production Systems
The concept of a ‘Smart Production System’ is an approach which aims to meet the challenges currently 
facing production, in which the gap between data, technology and process-centered production design 
approaches is closed. The basic idea is to use ‘knowledge-integrated objects’ within the production 
process to make the processes transparent at all times, thus ensuring the efficiency and flexibility of the 
associated production system. The ‘knowledge-integrated objects’ represent production items. These, 
in addition to their actual function, also possess information technology functions by storing as well 
as providing data (Birkhahn, 2007). Today the ‘knowledge-integrated objects’ are mainly described 
under the comprehensive concept of cyber-physical systems (CPS) – and, when used within industrial 
production companies, as cyber-physical production systems (CPPS). CPPS are distributed, networked 
smart objects that connect embedded systems with internet-based technologies and capture sensor data 
by means of which they regulate materials, goods, and information flows (Rajkumar et al., 2010).
Manufacturing companies in countries with a high wage level compete with Asian manufacturers, and 
especially with the Chinese (Eisenhut et al., 2011). For the middle and upper quality and price segments 
however, CPPS provide a good opportunity for the producers to provide competition-related value; i.e. by 
offering additional services to their customers (Geisberger & Broy, 2012). Voluntary commitments such 
as ISO 9000 or legal liability regarding products and processes force European companies in particular 
to control and document their production processes themselves (Bundesministeriums der Justiz, 2004). 
However, the complexity of CPPS, the rapid technical progress and the close link between hardware and 
software in the field of industrial production means the staff and end-users of such complex production 
technologies are faced with major challenges when dealing with them. New user interfaces and support 
tools are therefore required which would enable users to keep pace and to work both effectively and 
efficiently.
2.5. Appropriation and Appropriation Infrastructures
Given the complexity and the socio-technical orientation of such new forms of production systems, is-
sues from the field of HCI are of particular importance (Balka & Wagner, 2006; Dourish, 2003; Mackay, 
1990; G Stevens et al., 2010). Within the field of HCI, the concept of appropriation is emphasized (Pi-
pek, 2005), as well as further research being conducted on appropriation within the areas of HCI and 
CSCW. Following Pipek (2005), appropriation is the discovery and sense-making of an artifact while 
using it in practice. This understanding has its roots in established CSCW and HCI literature, where 
appropriation is associated with the process of fitting new technologies in users’ practices in situ by 
adoption as well as adaptation of those technologies (Balka & Wagner, 2006; Dourish, 2003; Mackay, 
1990; Gunnar Stevens et al., 2009). The concept of appropriation goes deeper than that of customiza-

286
Should I Try Turning It Off and On Again?
﻿
tion or tailoring of software in that it can encompass fundamental changes in practice and embraces the 
possibility of users adopting and using the technology in ways not anticipated by the designer (Pipek, 
2005). Furthermore, it should be noted that appropriation is associated with processes of exchange and 
interaction in networks of co-users where experiences and stories are shared between actors involved in 
the appropriation process (Gantt & Nardi, 1992; Mackay, 1990; Pipek & Kahler, 2006; Pipek, 2005).
Pipek (2005) focused on developing appropriation support functionalities for connecting users of 
one tool, while both Star & Bowker, (2002) and Stevens et al. (2009) looked at ensembles of tools and 
suggested an appropriation framework that would also address the developer’s interest in improving the 
technology. Aside from the core features of an interactive system (e.g. a CAD tool, an IDE or a printer), 
this framework adds second-level-functionality to support all associated appropriation activities. Examples 
for appropriation frameworks for software-centered domains include the development and integration 
of a participative feedback tool into the Eclipse IDE (Draxler et al., 2012; Draxler & Stevens, 2011) 
or attempts at formalizing a theoretical appropriation model focused on software (Belin & Prié, 2012). 
The integration of appropriation frameworks directly into the IS which it is intended to support has been 
thoroughly investigated and tested, and seems to have merit (Pipek, 2005; Gunnar Stevens et al., 2009; 
Yetim et al., 2012). Traditionally, whenever hardware is involved in such studies, it is mostly considered 
in a systemic way in conjunction with software (Dalton et al., 2012; March et al., 2005). Generally speak-
ing, this makes sense since complex hardware usually also has software components. However, given 
the discrepancy between the scientific approaches with regard to software vs. hardware appropriation, 
we argue it is necessary to focus on hardware aspects in order to bring both sides to an even synthesis.
3. HARDWARE-CENTERED APPROPRIATION INFRASTRUCTURES
Summarized under the concept of sociable technology as ‘highly hardware-centered appropriation infra-
structures’, Ludwig et al. (2014) have outlined several design implications for deploying an appropriation 
infrastructure to the machine itself. As their application field, they used 3D printing. Unlike traditional 
“plug-and-play”-printers from the “2D world”, there are only a handful of previous practices which users 
can relate to while handling 3D printers. Unexpected errors and breakdowns as well as complex con-
figurations lead to difficulties in understanding and appropriating these machines for existing practices 
of ‘making’. The same could be interesting for modern CPPS because technology changes the current 
work practices of industrial manufacturing.
Ludwig et al. (2014) pointed out different design ideas for functionalities which support infrastructur-
ing activities (Table 1). They build on previous work for software infrastructures (Dalton et al., 2012; 
Draxler & Stevens, 2011; Lazar & Preece, 2002; Mackay, 1990). However, the physical/material condi-
tions in relation to the machine, printing material and artifacts also suggest enhancing those activities 
with additional sensors and visualizations of printing processes, printer environments and the tasks/
structures/workflows the printing is embedded in, as well as by incorporating in-situ and remote tools 
for those purposes. They have shown that most infrastructuring measures were communicative or col-
laborative activities involving not only technology manipulations and combinations of various kinds, 
but also articulations of usages or breakdowns that referred to the technologies at hand and their context.
Studies on these kinds of communication are not completely new, but current practices deserve a 
second look. The practices of communication evolve along with the communication infrastructures users 
change with the developments of miniaturization, mobile and ubiquitous computing. The argument now 

287
Should I Try Turning It Off and On Again?
﻿
is to use this ubiquitous interconnectedness of devices to make more technologies ‘sociable’, meaning that 
the support now available to users outside the technology itself – e.g. at hand in the internet in general, 
in community forums or in neighboring offices – should be integrated into the technologies themselves. 
Conceptually, these activities have been referred to as ‘appropriation activities’ if they consist of user-user 
interaction, or as ‘infrastructuring activities’ if they also relate to other actors of technology production 
and use, or to the further development of technologies and their foundations. A functionality to support 
all these practices within the existing infrastructure needs to address not only interoperability issues as 
they arise, e.g. by defining standardized interfaces between various infrastructural layers; it must also 
allow for – and actively support – the negotiation of the socio-material aspects of emerging new practices 
connected to these technologies.
Ludwig et al. (2014) showed that the ability to articulate and discuss usage and configuration issues 
would benefit from sociable technologies that describe themselves on three context levels, potentially 
using additional sensors, and providing visualizations and tools in three dimensions: (1) Internal context. 
At this level, information about the inner workings of a machine, including its current state as well as 
behavioral structure, should be provided; (2) Socio-material context, providing e.g. location and sur-
roundings, environmental data such as room temperature, maintenance and user/usage data; and (3) 
Task/process context, affording information on e.g. the technologies used to build or prepare models, 
the position in a production chain or process, purpose and goal of machine usage.
Table 1. Empirical study-based design implications for 3D printers as sociable technologies (Ludwig 
et al., 2014) 
Empirical Findings
Challenges and Design Implications
Current practices and usage behaviors during 3D printing 
1. The 3D printer itself is a black box for the users and lacks in 
methods to see and grasp how it really works
Sensor based capturing and visualizing of context and printer 
information to support a better understanding of the machine
2. General orientation of learning-by-doing and experimenting 
instead of an extensive literature research
Context-related ambient learning through software- and sensor-
based hints / tutorials / best practices
3. Identifying and locating problems is an problem itself due to the 
high context-dependence of those problems
Sensor based capturing of environmental variables and mapping 
them to possible printing problems. Detailed visual presentation
4. The entire 3D printing process is very time-consuming because the 
users must always be close to the printer
Providing web-based and in-situ options for monitoring and 
managing the printer as well as communicating with it remotely
Documentation and knowledge sharing 
5. The printer settings with regard to the model, material and 
individual print are forgotten regularly
Integrated print history with printer settings, material data, 
errors, etc. Recommendations for current prints based on the 
history
6. Individual documentation and sharing of 3D printing experiences 
is very cumbersome and involves multiple systems which keeps users 
from doing it
Print history from 5. should be presented to the user together 
with easy to use tools to add more information
7. Difficulties in asking for help because often, not enough contextual 
information can be provided.
Data from steps 5 and 6 should easily be postable to 3D Printing 
communities and social networks
8. Knowledge, tips and hints in sharing communities are scattered 
and not really searchable / indexed
Establishing an orderly, searchable data structure (format) for the 
data from the previous steps
9. Community-specific terminology hampers appropriation
Community-maintained dictionary and automated “translator”
10. Technical validation (manifoldness, etc.) has to be done manually 
by the users and is not standardized.
Integrated validation tool checking model specifications and 
matching them to the printer’s characteristics.

288
Should I Try Turning It Off and On Again?
﻿
4. DESIGN CHALLENGES FOR THE APPROPRIATION OF CPPS
Ludwig et al. (2014) have shown that most of the activities constituted around the appropriation of 3D 
printers were collaborative activities involving the configuration of a technology as well as the articula-
tion of usages and their context. However, the complexity of CPPS does not only result in its ability to 
capture and process environmental data and to adapt its current state to those parameters in real-time in 
just one production process (vertically). Instead, CPPS aims to connect customers and suppliers along 
the entire value-added chain by exchanging information or even influencing business processes within 
the company itself (horizontally). This characteristic distinguishes CPPS from 3D printers or similar 
complex machines, which are used mainly in private contexts or within the maker community and thus 
demand a reconsideration of appropriation requirements. Manufacturing and delivery time, cost effi-
ciency, quality standards, legal liability of products and processes or protection of business secrets are 
much more relevant to commercial than to private manufacturers. CPPS is highly dependent on the basic 
(network) infrastructure. These aspects, in combination with increasing process automation, hamper the 
operators’ understanding of the machine. This leads to an enormous complexity which in turn demands 
design implications of CPPS to support users along the entire value-added chain from an HCI as well 
as CSCW perspective.
Based on the example of 3D printers, Ludwig et al. (2014) outlined ten design implications for 
hardware-centered appropriation, which might possibly also be adapted to CPPS. To examine how the 
design implications could be adapted, we conducted a workshop with several experts from the field 
of HCI with a proper understanding of both 3D printers as well as of manufacturing processes within 
enterprises. We discussed each design implication outlined by Ludwig et al. (2014) with regard to its 
potential adaption for CPPS. Then we tried to either outline its transferability or to sketch amendments. 
Our examination of the transferability of the design implications of 3D printers towards CPPS was 
aimed neither towards general validity nor comprehensive results. Instead, based on a discussion about 
the concept of sociable technologies by Ludwig et al. (2014), we wish to outline new thoughts on the 
appropriation of CPPS that could be taken up by HCI designers or system developers.
4.1. Sensor Based Capturing and Visualizing of Context and Printer 
Information to Support Better Understanding of the Machine
CPPS are based on sensor data to perform their tasks (Rajkumar et al., 2010). The sensors deliver all 
the relevant data processed within the machine itself or transmitted to other nodes for further process-
ing. These processes and dependencies are hard to comprehend for machine operators; thus it is almost 
impossible for them to solve or even understand problems regarding the machines (Munir et al., 2013). 
The horizontal dimension of CPPS compounds this issue, as information might also influence the ma-
chine state that does not originate from within the company. Capturing and visualizing machine data and 
context information in-situ, as proposed by Ludwig et al. (2014), is therefore an essential design issue 
for CPPS. Access to this kind of information is promoted by the functionality of CPPS and is thus easy 
to acquire, given the necessary interfaces. A dashboard visualizing all relevant data about the product, 
machine and process should be presented to the machine operator on a display individually suitable 
for each operator, machine or process respectively. In addition to the proposed high-resolution display 
mounted on the machine, other displays need to be considered in the CPPS context. Machine operators 
monitoring a number of distributed machines during a shift benefit from portable displays, such as tablets, 

289
Should I Try Turning It Off and On Again?
﻿
smartphones, data glasses and augmented reality glasses. These devices should render access to all the 
relevant information from every machine for which the operator is responsible during his or her shift. 
The dashboard should emphasize information related to the current operator’s position. If s/he works at 
one specific machine, all the relevant information regarding this machine, the product(s) inside and the 
process should be displayed and all other information should be minimized and condensed to relevant 
figures or warnings, otherwise the operator could be overloaded with information.
4.2. Context-Related Ambient Learning through Software- and 
Sensor-Based Hints, Tutorials, and Best Practices
Experimental learning approaches such as learning-by-doing, as occur when appropriating 3D printers, 
can – in the best case – only supplement formal learning methods such as professional courses or profes-
sional instructions in production companies. Training sessions are often organized only once following the 
acquisition of a machine, and are often included in the purchase price (even if new employees are taken 
on). This does not apply to safety-critical machines such as x-ray apparatus, where periodical training 
sessions have to be absolved by the operators. From a business point of view, CPPS can be considered 
critical, as incorrect operation might cause a chain reaction of errors possibly leading to economic dam-
age. However, a mixture of context-related ambient learning such as additional hints or tutorials as well 
as formal learning, as proposed by Ludwig et al. (2014), could be applied to CPPS appropriation too, 
especially if formal training sessions only take place once. Due to the horizontal dimension of CPPS, 
these measures have to focus strongly on intra- and inter-organizational process information. First steps 
towards the use of augmented reality for supporting the manufacturing processes based on complex 
machines are already in place (e.g. Nee et al., 2012; Perey et al., 2014).
4.3. Sensor Based Capturing of Environmental Variables and Mapping 
Them to Possible Printing Problems: Detailed Visual Presentation
This design implication strongly correlates with the first issue (capturing and visualizing). CPPS are 
equipped with all sensors necessary for performing their tasks and should therefore have detailed in-
formation on their actual state as well as relevant environmental data, including data provided by either 
suppliers or customers. However, this might not always be the case as circumstances not anticipated 
by the CPPS designers might affect functionality. Thus the integration of additional sensor information 
should be provided to assist the machine operator. Visualization should be integrated in the situations 
and devices mentioned in 1.
4.4. Providing Web-Based and In-Situ Options for Monitoring and 
Managing the Printer as well as Communicating with It Remotely
As mentioned above, it is feasible that machine operators might have to watch several distributed CPPS 
during their shift, or the operators themselves might not even be located at the production site (miscel-
laneous sites, various time zones, working from home etc.). To provide support in these scenarios, both, 
monitoring as well as remote control functionality, are necessary, as Ludwig et al. (2014) proposed. 
Operators should have access to the relevant information regarding the portable devices mentioned 
above; or to a computer or monitor in the office or at home, respectively. They should be in a position to 

290
Should I Try Turning It Off and On Again?
﻿
perform non-physical tasks remotely (machine restart, change of parameters, etc.). Although one might 
think that nowadays all machines can be operated remotely, especially in the manufacturing sector, this 
is not true: machines are often isolated applications. Of course, the suggested approach also presents 
a number of obstacles, e.g. establishing a secure connection or lowering the level of binding between 
operator and machine which in turn probably lowers the feeling of responsibility.
4.5. Integrated Print History with Printer Settings, Material Data, Errors, 
etc: Recommendations for Current Prints Based on History
The vision of CPPS is that it can set up automatically for each related product and process, retrieving all 
the necessary information from the ‘smart product’, CAD or ERP. Thus, the issues pertaining to forgotten 
settings, or materials used for private 3D printing, do not apply to professional CPPS. However, logging 
data relevant to the machine, product and process as well as additional environmental data could definitely 
benefit companies, in terms of quality control and liability matters, as well as machine operators by 
supporting them in-situ. Particularly when errors occur, data history could be indispensable in speeding 
up the recovery process. Thus the proposed logging and visualization of the above information should 
be displayed optionally on the mentioned displays when requested; or automatically, in the case of an 
error, to help speed up the recovery process. This might be especially helpful if the error was caused by 
process data provided by parties in the value-added chain rather than by the machine.
4.6. Print History from Step 5 Should be Presented to the User 
Together with Easy to Use Tools to Add More Information
Ludwig et al. (2014) propose this feature further to support the user’s memory regarding knowledge 
capturing and sharing. As mentioned in 5, CPPS retrieve all the necessary data from distinct sources. 
There would seem to be no need to add information to the data logged during a manufacturing process 
and relate it to the machine operator. However, if an error occurs, this functionality could be useful for 
adding information that might not have been captured by sensors or other data sources. It could also 
be used to help point out relevant events that obviously occurred but would have to be elaborately ex-
tracted from the raw or even aggregated data. These annotations could support the process quality and 
also the product ex post. Referring to the operator’s working context, it should be easy to complete the 
annotations using the portable or stationary devices either onsite or remote. Due to the almost real-time 
character of CPPS processes, audio or video notices should be supported in addition to written notices.
4.7. Data from Steps 5 and 6 Should Easily be Postable to 
3D Printing Communities and Social Networks
This design implication that focuses on sharing process information with online communities or social 
networks has to be discussed ambivalently. Sharing the information from 5 and 6 is mainly a matter of 
data protection. This information may contain company secrets and cannot be shared to open access on-
line communities, nor oftentimes even to closed online groups. Usually, companies conclude a contract 
with a CPPS producer which includes maintenance and support or even the machine’s functionality as a 
service – one of the CPPS visions. Such business partners have to clearly negotiate issues regarding law 
and data protection by at least concluding a non-disclosure agreement. Based on these formal as well 

291
Should I Try Turning It Off and On Again?
﻿
as technical perquisites, sharing this kind of information, e.g. to rectify an error cooperatively, might 
be a responsible scenario. Such information sharing could be established within closed communities of 
practices – people that share a common practice – within one company.
This does not apply to open communities or social media the same way, as obviously such contracts 
cannot be conducted with e.g. Facebook users in a legally acceptable way. However, not all CPPS are 
forced to be so complex; or professional support might not be available, or too expensive. In order to 
motivate the formation of an online community at all, another requirement is that the CPPS are nearly 
of the same kind. Given this context, sharing this kind of information in open online communities – as 
Microsoft administrators do to resolve Windows Server or Exchange issues, for example – could be 
helpful for machine operators as well to enable them to access help when malfunctions arise. To sup-
port this feature, help should be available to machine operators, allowing them to revise the information 
before sharing. Business secrets must be removed or made unrecognizable but in a way that does not 
surrender too much information. There should be the opportunity to set up an approval process before 
finally sending data to online communities. This way, machine operators are not solely responsible and 
thus business secrets will unlikely leave the company accidently. But there is still the matter of insur-
ance and quality issues which must be addressed in future. One such issue might be the question of 
who is responsible for damages to the machine due to bad advice. This has to be considered selectively. 
On the one hand, machine operators would be responsible for any advice followed from (anonymous) 
open online communities. On the other hand, if damage were caused by the advice given by providers 
or machine manufacturers, the responsibility would lie with these contractors.
4.8. Establishing an Orderly, Searchable Data Structure 
(Format) for the Data from the Previous Steps
As mentioned above, industrial companies are not inclined to share detailed process or product informa-
tion in open online groups as such data generally incorporates critical business secrets. A searchable data 
structure of this kind of information is not very feasible and will not be regarded further. However, it 
would be desirable if such a data structure were internally accessible within the company to enable the 
inclusion of knowledge from processes implemented in the past. In view of the requirements mentioned 
in 7, an error-related data structure would be valuable for both CPPS producers and open online com-
munities. Taking the possibly large extent of context information regarding an error into consideration, 
the need for “[…] a standardized, indexed and especially searchable data structure for the enriched 
information […]” (Ludwig et al., 2014) for the machine operators is necessary. As CPPS usually work 
under nearly real-time circumstances, the fast retrieval of valuable information on errors saves time and 
can help prevent such consequences as missing a delivery date or even bringing a whole production 
chain to a full stop. Based on this data format, an API should therefore be provided by the data base to 
enable suggestions based on certain error parameters. This should help the machine operator to find a 
solution quickly or at least be able to work around the issue.
4.9. Community-Maintained Dictionary and Automated ‘Translator’
CPPS are often much more complex than 3D printers. It is essential that machine operators have an 
adequate clarity of expression at their command in order to articulate questions or describe errors pre-
cisely. The proposed integrated dictionary and translator would support this in both open and closed 

292
Should I Try Turning It Off and On Again?
﻿
online communities, as discussed before. The dictionary and translator would provide correct terms for 
the different machine components, thus providing valuable assistance to operators on the one hand and 
supporting a data-base pre-search by CPPS on the other.
This type of dictionary could also prove useful to service providers or producers, especially of 
customer-specific CPPS, who should be equipped with corresponding data bases to enable issues to be 
resolved quickly.
In both cases, due to the ability of CPPS to determine internal data, states and other information, the 
machine operator or support hotline should have at their disposal a meaningful error expression (like 3), 
but including the correct terminology. Additionally, the CPPS should present its components or processes 
and it should be included into and be easily accessible directly within the CPPS. This could be provided 
by a 3D model of the CPPS including all relevant terms and descriptions.
4.10. Integrated Validation Tool Checking Model Specifications 
and Matching Them to the Characteristics of the Printer
This design implication is very specific to 3D printers and cannot easily be transferred to CPPS in the 
context of commercial companies. Issues regarding specific products and processes to produce them ef-
ficiently are business secrets and as such will most likely not be shared and discussed publicly. Affected 
companies are more likely to consult the CPPS producer or provider should any physical limitations arise.
5. CONCLUSION
For long term survival on the international market, industrial companies must adapt their products to 
changes in market trends at even shorter intervals, maintaining and expanding market positions with 
new, technologically more advanced, higher quality products at competitive prices. One suggested 
technical approach to respond to these challenges is the concept of cyber-physical production systems 
as distributed, networked smart objects to connect embedded systems with internet-based technologies 
(Rajkumar et al., 2010). Taking the complexity of CPPS and their dependency on several parameters into 
account, a number of obstacles appear when putting the technology into practice – especially from the 
machine operator’s perspective (Munir et al., 2013). The concept of sociable technologies might be one 
approach to resolve these issues as they encompass the machine operator’s appropriation of a machine 
directly into the hardware itself.
Ludwig et al. (2014) outlined ten design challenges when trying to design 3D printers as sociable 
technologies. We have adapted these design challenges to CPPS and, based on an expert workshop, 
we have outlined new design directions that need to be addressed when trying to apply the concept of 
sociable technologies to CPPS. We assume that, in comparison to 3D printers, the horizontal perspec-
tive towards the entire manufacturing process and the related supply chain, deserve particular attention. 
Most of the above listed design implications to support the appropriation of CPPS address vertical issues 
mainly regarding individual companies and do not focus on networked machines. CPPS are, however, not 
only smart production systems within the production line of a single company, but also span over entire 
value-added supply chains consisting of different companies to add even more efficiency to production 
processes. Ludwig et al. (2014) do not explicitly focus on this aspect and thus further research is needed 
to determine design implications for sociable technologies in the horizontal perspective of CPPS.

293
Should I Try Turning It Off and On Again?
﻿
We are currently in touch with several small and medium enterprises that are interested in applying 
the concept of sociable technologies to their manufacturing process. These enterprises in particular often 
rely on the knowledge of their employees; integrating fully-automated systems to their practice will not 
always lead the companies towards successful production. As future work, we aim to examine how such 
companies might be able to use sociable technologies by applying human-centered approaches towards 
the adaption of CPPS and by including their employees in the entire process of implementation. During 
this deployment across the supply chain, we also will try to address the legal perspective of CPPS. For 
example, the field of CPPS does not focus on questions concerning who is responsible for (data) errors, 
or how errors can be determined in such a complex system comprised of many legally independent 
actuators. We will further move the concept of sociable technologies to other application fields like the 
design of medical technology.
REFERENCES
Balka, E., & Wagner, I. (2006). Making things work: dimensions of configurability as appropriation 
work. Proceedings of the 20th anniversary conference on Computer supported cooperative work (pp. 
229–238). ACM New York. doi:10.1145/1180875.1180912
Bauer, M., Jendoubi, L., & Siemoneit, O. (2003). Grundlagen ubiquitärer Systeme und deren Anwendung 
in der “Smart Factory”. Industrie Management – Zeitschrift fur den industriellen Geschaftsprozess, 19(6).
Becker, J., & Rosemann, M. (1993). CIM und Logistik: Die effiziente Material- und Informationsflussge-
staltung im Industrieunternehmen. Springer-Verlag Berlin Heidelberg. doi:10.1007/978-3-642-61697-6
Belin, A., & Prié, Y. (2012). DIAM: Towards a model for describing appropriation processes through the 
evolution of digital artifacts. Proceedings of the Designing Interactive Systems Conference, Newcastle, 
UK (pp. 645–654). New York: ACM. doi:10.1145/2317956.2318053
Birkhahn, C. (2007). Smart Production Systems - intelligente Konzepte zur Gestaltung von Produktions-
systemen. Kaiserslautern. doi:10.1007/978-3-662-45302-5
Bundesministeriums der Justiz. (2004). Gesetz über technische Arbeitsmittel und Verbraucherprodukte. 
Geräte- und Produktsicherheitsgesetz - GPSG.
Büring, E. (1997). Anspruch und Realität computerintegrierter Produktionssysteme und Fertigungss-
teuerungskonzepte. Duncker & Humblot.
Comes, T., Wijngaards, N., Hiete, M., Conrado, C., & Schultmann, F. (2011). A Distributed Scenario-
Based Decision Support System for Robust Decision-Making in Complex Situations.[IJISCRAM]. 
International Journal of Information Systems for Crisis Response and Management, 3(4), 17–35. 
doi:10.4018/jiscrm.2011100102
Dalton, N., MacKay, G., & Holland, S. (2012). Kolab: Appropriation & Improvisation in Mobile Tangible 
Collaborative Interaction. Proceedings of the Designing Interactive Systems Conference, Newcastle, UK 
(pp. 21–24). New York: ACM. doi:10.1145/2317956.2317960

294
Should I Try Turning It Off and On Again?
﻿
Dourish, P. (2003). The Appropriation of Interactive Technologies: Some Lessons from Placeless Docu-
ments. Computer Supported Cooperative Work, 12(4), 465–490. doi:10.1023/A:1026149119426
Draxler, S., & Stevens, G. (2011). Supporting the Collaborative Appropriation of an Open Software 
Ecosystem. Computer Supported Cooperative Work, 20(4-5), 403–448. doi:10.1007/s10606-011-9148-9
Draxler, S., Stevens, G., & Stein, M. (2012). Supporting the social context of technology appropriation: on 
a synthesis of sharing tools and tool knowledge. Proceedings of the SIGCHI Conference on Human Factors 
in Computing Systems, Austin, Texas (pp. 2835–2844). New York: ACM. doi:10.1145/2207676.2208687
Eisenhut, M., Lässig, R., & Liedl, J. (2011). Production Systems 2020. Roland Berger Strategy Consultants.
Fleisch, E. (2001). Betriebswirtschaftliche Perspektiven des Ubiquitous Computing. In H. U. Buhl, 
A. Huther., & B. Reitwiesener (Eds.), Information Age Economy. Heidelberg: Physica-Verlag. 
doi:10.1007/978-3-642-57547-1_17
Gantt, M., & Nardi, B. (1992). Gardeners and gurus: patterns of cooperation among CAD users. Proceed-
ings of the SIGCHI Conference on Human Factors in Computing Systems. Monterey, CA (pp. 107–117). 
New York: ACM. doi:10.1145/142750.142767
Geisberger, E., & Broy, M. (2012). agenda CPS - Integrierte Forschungsagenda Cyber-Physical Systems. 
acatech STUDIE. doi:10.1007/978-3-642-29099-2
Geitner, U. H. (1990). Die CIM-Konzeption. In U. H. Geitner (Ed.), CIM-Handbuch (pp. 3–41). Braun-
schweig.
Lazar, J., & Preece, J. (2002). Social considerations in online communities: Usability, sociability, and 
success factors. In H. Van Oostendorp (Ed.), Cognition in the Digital World (pp. 1–46). Lawrence Erl-
baum Associates Inc. Publishers.
Ludwig, T., Stickel, O., Boden, A., & Pipek, V. (2014). Towards Sociable Technologies: An Empirical 
Study on Designing Appropriation Infrastructures for 3D Printing. In Designing Interactive Systems 
(pp. 835–844). Vancouver, Canada. doi:10.1145/2598510.2598528
Ludwig, T., Stickel, O., Boden, A., & Pipek, V. (2015). Appropriating Digital Fabrication Technolo-
gies – A comparative study of two 3D Printing Communities. Proceedings of the iConference 2015, 
Newport Beach, CA. USA.
Mackay, W. E. (1990). Patterns of Sharing Customizable Software. Proceedings of the 1990 ACM con-
ference on Computer-supported cooperative work, Los Angeles, CA (pp. 209–221). New York: ACM. 
doi:10.1145/99332.99356
March, W., Jacobs, M., & Salvador, T. (2005). Designing technology for community appropriation. 
Proceedings of CHI 05 extended abstracts on Human factors in computing systems CHI 05, Montreal 
(pp. 2126–2127). New York: ACM. doi:10.1145/1056808.1057120
Munir, S., Stankovic, J. a., Liang, C.-J. M., & Lin, S. (2013). Cyber Physical System Challenges for 
Human-in-the-Loop Control. Proceedings of the 8th International Workshop on Feedback Computing, 
San Jose, California, USA.

295
Should I Try Turning It Off and On Again?
﻿
Nee, A. Y. C., Ong, S. K., Chryssolouris, G., & Mourtzis, D. (2012). Augmented reality applications in 
design and manufacturing. CIRP Annals - Manufacturing Technology, 61(2), 657–679. doi:10.1016/j.
cirp.2012.05.010
Perey, O. C., Community, a R., Alliance, E., & Wild, F. (2014). Advanced Manufacturing with Aug-
mented Reality. Proceedings of theIEEE International Symposium on Mixed and Augmented Reality 
(ISMAR). Munich, Germany.
Pipek, V. (2005). From Tailoring to Appropriation Support: Negotiating Groupware Usage. Processing. 
University of Oulu.
Pipek, V., & Kahler, H. (2006). Supporting Collaborative Tailoring. In End-User Development (pp. 
315–345). doi:10.1007/1-4020-5386-X_15
Rajkumar, R., Lee, I. L. I., Sha, L. S. L., & Stankovic, J. (2010). Cyber-physical systems: The 
next computing revolution. Proceedings of the2010 47th Design Automation Conference (DAC). 
doi:10.1145/1837274.1837461
Shah, R., & Ward, P. T. (2007). Defining and developing measures of lean production. Journal of Op-
erations Management, 25(4), 785–805. doi:10.1016/j.jom.2007.01.019
Star, S. L., & Bowker, G. C. (2002). How to Infrastructure. In L. A. Lievrouw & S. Livingstone (Eds.), 
Handbook of New Media: Social Shaping and Social Consequences of ICTs, Updated Student Edition 
(pp. 151–162). London, UK: SAGE Pub.
Stevens, G., Pipek, V., & Wulf, V. (2009). Appropriation Infrastructure: Supporting the Design of 
Usages.Proceedings of the 2nd International Symposium on End-User Development (pp. 50–69). 
doi:10.1007/978-3-642-00427-8_4
Stevens, G., Pipek, V., & Wulf, V. (2010). Appropriation infrastructure: Mediating appropriation and 
production work. Journal of Organizational and End User Computing, 22(2), 58–81. doi:10.4018/
joeuc.2010040104
Westkämper, E., & Jendoubi, L. (2003). Smart Factories - Manufacturing Environments and Systems of 
the Future.Proceedings of the 36th CIRP International Seminar on Manufacturing Systems. Saarbruecken.
Womack, J. P., Jones, D. T., & Roos, D. (1990). The Machine that Changed the World: The Story of 
Lean Production. WORLD (Oakland, Calif.). doi:10.1016/0024-6301(92)90400-V
Yetim, F., Draxler, S., Stevens, G., & Wulf, V. (2012). Fostering Continuous User Participation by 
Embedding a Communication Support Tool in User Interfaces. AIS Transactions on Human-Computer 
Interaction, 4(2), 153–168.
This work was previously published in the International Journal of Information Systems for Crisis Response and Management 
(IJISCRAM), 7(3); edited by Murray E. Jennex and Víctor Amadeo Bañuls Silvera, pages 55-68, copyright year 2015 by IGI 
Publishing (an imprint of IGI Global).

296
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  16
DOI: 10.4018/978-1-5225-1677-4.ch016
ABSTRACT
This technical paper is a contribution to the identification of current challenges of semantic 3D city 
models. They are presented in four parts, namely 3D enriched city models and their connection with 
urban information models and smartcities, urban models integration, urban analyses and data. This 
work is an output of the COST Action TU0801 “Semantic Enrichment of 3D city models for sustainable 
urban development”.
1. INTRODUCTION
Cities are complex systems constituted of physical elements interrelated into elaborated spatial relations, 
with a complexity increasing as the shape and the structure are changing and evolving. To try to under-
stand the dynamics and the processes shaping our cities, we must coherently make models according 
to the dynamic and complex nature of cities, but also models should remain understandable and simple 
enough to be operationally useful. Hence, any attempt to model the spatial system and the dynamics of 
the cities should involve this non-determination and instability of the cities and this theoretical frame-
Challenges of Semantic 
3D City Models:
A Contribution of the COST 
Research Action TU0801
Roland Billen
University of Liège, Belgium
Anne-Françoise Cutting-Decelle
Lille University of Science and Technology, 
France & University of Geneva, Switzerland
Claudine Métral
University of Geneva, Switzerland
Gilles Falquet
University of Geneva, Switzerland
Sisi Zlatanova
Delft University of Technology, The 
Netherlands
Ognen Marina
Saints Cyril and Methodius University of 
Skopje, Macedonia

297
Challenges of Semantic 3D City Models 
﻿
work as one of its basic features. In the same time it should be followed with a strong knowledge and 
data management providing a necessary coherence with the nature of the city. To do all that, we have 
to search for a new paradigm in the way of modelling urban form, thus allowing to free the process of 
representation from fixed types and pre-determined shapes of elements of the model and to introduce a 
concept that will be generative as much as it is analytical.
The future of cities based on the idea of a creation of a society of knowledge lies in the creation of 
semantically enriched 3D city models as powerful tools for gathering, storing, evaluating and using urban 
information through a comprehensive open and accessible system coherent with the nature of the cities 
and the way we see and understand them. Urban Information Models can emerge from the horizontal and 
vertical integration of different information sources with an active contribution of institutions, companies 
and individuals creating a complex network of urban knowledge. This in return will enable cities to act 
as facilitators for the exchange of urban information with a high level of details and usability. This would 
also help to become more effective and efficient in the way of providing policies for city management 
and urban planning suited to the best interests of their citizen.
3D city models must be seen as powerful tools for the integration of information coming from a wide 
range of levels of detail and backgrounds, from buildings and activities representations to ground and 
underground infrastructures. A high level of integration and interoperability between existing data and GIS 
related information and simulation capacities of the model can be achieved. 3D city models can become 
an urban analytical tool capable of harvesting information from different fields of activities. They can 
also become tools to generate new information and understanding through a complex urban knowledge 
about the conditions of the city, revealing the future in a most comprehensive way for future users. This 
will be considered as the true beginning of the concept of smart cities and the society of knowledge.
This paper presents some of the outputs of the COST Action TU0801 called: “Semantic Enrichment 
of 3D city models for sustainable urban development” which started in November 2008 and ended in 
November 2012 (www.semcity.net). COST- the acronym for European Cooperation in Science and 
Technology - is the oldest and widest European intergovernmental network for cooperation in research 
(www.cost.eu). This action was part of the Transport and Urban Development (TU) domain (www.
cost.eu/domains_actions/tud). The main motivation of the action was to explore ways to semantically 
enrich 3D models with urban knowledge and models, so as to extend their functionality and usability in 
a perspective of sustainable development. It was an active research network of 75 permanent members 
gathering people from academia, industry, administration and regulation bodies from 23 participating 
countries. More details about the action can be found in the final report (Billen et al., 2014).
The paper is structured as follows: section 2 presents the challenges we faced at the beginning and 
during the Research Action about 3D city models, in terms of their use, the context of their use and the 
related benefits/drawbacks of their use, thus leading to the elaboration of research agendas organised 
according to the main axes that can be built from the figure below. The paper ends with the description 
of some of the most important issues and perspectives as identified by the Action.
2. CHALLENGES OF SEMANTIC 3D CITY MODELS
The role of 3D city models as integrators of the urban and environmental knowledge has been strongly 
acknowledged all along the activities of COST TU0801 (Billen et al., 2012; Leduc et al., 2012; Billen 
et al., 2014). This emerging vision will have major impacts on the one hand on standardisation bodies, 

298
Challenges of Semantic 3D City Models 
﻿
decision makers, data producers and consequently on the industry, and, on the other hand on urban experts 
and scientists. Indeed, considering enriched 3D city models as urban knowledge integration platforms 
implies a (re-)definition of geospatial standards, new data production/integration strategies, develop-
ment of new applications… Enriched 3D city models are the foundation bricks of future Smart Cities.
Beyond these considerations, some conceptual and technical issues still remain open:
•	
Considerations about the types of semantic enrichment;
•	
Procedures for enriching 3D city models;
•	
Ways of visualizing enrichment information and knowledge.
Besides, even 3D city models definitions can be considered as being still debatable.
Indeed, the consideration of a modelling approach of the urban landscape based on 3D city models 
leads to the integration of a lot of disciplines and applications, which are diverse and not traditionally 
integrated as such. In other words, 3D city models can be seen as the basic ground for integrating ur-
ban knowledge, but such integration is complex and new. It is not just an extension of traditional 2D 
maps, or geometrical 3D city models, but the creation of a spatial model of complex urban networks of 
relationships between elements, attributes and configurations. This implies also to integrate time in the 
modelling approach; it is mandatory to capture the dynamic nature of urban knowledge.
A global view of the issues and challenges of the use of 3D city models for urban knowledge integra-
tion is presented in Figure 1 below; this schema highlights the four main components:
Figure 1. A global view on 3D enriched city models: issues and challenges

299
Challenges of Semantic 3D City Models 
﻿
•	
3D enriched city models;
•	
Urban models integration;
•	
Urban analyses;
•	
Urban data.
The central component gathers questions about use, usage and usability of 3D enriched city models 
and their possible evolution or connection with urban information models and smartcities. The whole 
issue of 3D enrichment and the role of 3D models as urban knowledge integrator can be addressed 
through urban models integration, urban analyses and data. This division in four components makes it 
possible to present open questions and recommendations in a flexible way.
To identify and to organise the different challenges, in order to facilitate their presentation in a clear 
way, we will make the analysis according to four axes built from the categories of the schema, namely: 
vocabulary and concepts (for 3D enriched city models), urban models integration, urban analyses and 
urban data.
For each of the different axes, we will first recall the context, and then we will propose challenges 
in terms of a research agenda.
2.1. Challenges in Terms of Vocabulary and Concepts
2.1.1. Context
Definition, usage and use of 3D enriched city models are terms far to be the subject of a common un-
derstanding amongst the actors of the planning activity in the urbanism and construction sectors; there 
is still work to do for outlining the nature and the scope of such models. This lack of consensus is due to 
the high diversity of usages that such models offer and the high diversity of disciplines involved. Indeed, 
this integration role goes far beyond what is known in 2D (through 2D GIS). This is a major scale issue 
which implies new paradigms. Up to now, we had a rather clear division between applications deeply 
rooted in mixing (2D) urban information at city level and applications using fragmented 3D urban in-
formation. Moving towards 3D city models brings together people that were not considering the same 
scale and extension of the urban phenomena. This leads to new communication issues between actors 
and consequently interoperability issues between systems.
It is becoming clear for most of the researchers and urban specialists that 3D city models should go 
beyond a simple geometric representation of the City (buildings, roads, vegetation) and progressively 
evolve towards 3D dynamic representation of above and underground, physical and abstract objects, al-
lowing combining multiple sources of information. This evolution leads to what one could call an urban 
information model. Such model will be necessary for the future management of cities; from that respect 
it is closely related to the concept of smartcities.
Semantic enrichment is a rather broad concept which corresponds to different realities depending 
on domains and applications. Although we have identified several types of semantic enrichment and 
identified ways to enrich models, namely annotations, ontologies, model extensions… there is still work 
to do for providing a comprehensive and usable inventory of methods.

300
Challenges of Semantic 3D City Models 
﻿
2.1.2. Research Agenda
In terms of challenges, we can consider that the following points still need further developments:
•	
Inventory of uses and usage of enriched 3D city models;
•	
Inventory of types of semantic enrichment of 3D city models;
•	
Inventory of objects to be included in enriched 3D city models;
•	
Study on evolution of paradigms (3D city model / urban information model / smartcities);
•	
Methods for the evaluation of 3D city models usability.
2.2. Challenges in Terms of Urban Models Integration
2.2.1. Context
As presented in the final report of the Action (Billen et al., 2014), there are various strategies for extend-
ing models, interconnecting models and integrating models. We are talking about integrating simulation 
models (e.g. air quality models) with 3D city models, models coming from the BIM domain (e.g. IFC) 
with 3D city models… and more generally any kind of semantic models (e.g. historical model) with 
3D city models. It seems also crucial to ensure the integration of 2D city models allowing combining 
the best of both worlds. For all these cases, there is no universal solution; model’s extension, models 
interconnection, models integration through ontologies… all of these present pros and cons.
On the other side, there is a clear need to continue to develop modelling and meta-modelling meth-
odologies and ontological approaches.
Enriching 3D City models is strongly related to interoperability issues. On the other hand, standar-
disation is a good way to improve interoperability. This means that there is a strong need to support and 
to influence existing standardization initiatives, through:
•	
Promoting communication between models (ex: CityGML and IFC);
•	
Identifying 3D standardisation priorities (ex: make distinction between core and application 
models);
•	
Promoting the development of standard compliant systems.
This initiative requires to keep working at a high level on 3D urban paradigms (ex: relevant rela-
tions and properties) with a focus on spatial integration of urban knowledge through 3D City models. It 
represents a huge impact on standardization bodies (e.g. OGC, INSPIRE). Furthermore, time concept 
cannot be discarded: 3D city models and their applications require most of the time to consider dynamic 
aspects and temporality issues. It implies also a deep conceptual thinking and standardization efforts.
2.2.2. Research Agenda
In terms of challenges, the following points still need further developments:
•	
Make investigations on the ways of interconnecting 3D models;
•	
Make investigations on the ways of extending 3D models;

301
Challenges of Semantic 3D City Models 
﻿
•	
Make investigations on the ways of integrating 3D models;
•	
Improvement of conceptualisation and formalization of 3D urban models;
•	
Development / improvement of standards for 3D / 4D urban models.
2.3. Challenges in Terms of Urban Simulation and Analysis
2.3.1. Context
The fundamental reason which motivates the use of 3D city models as a platform for the integration 
of urban knowledge is to improve urban analyses and urban management. In other words, the already 
identified and prospective needs for a better management of cities and understanding of urban phenom-
ena impose the use of enriched 3D city models and the underlying development of urban information 
systems which are the foundation of future smart cities.
Performing 3D simulations significantly improves the quality of the output in most of the applica-
tions studied all along the action TU0801. It implies 3D simulation models which can take benefit of 
existing 3D city models (instead of developing non-standard ad-hoc solutions) and associated 3D data 
(instead of acquiring expensive one-shot data). However, there is still need to assess the benefit of such 
3D approaches in all aspects of urban management (economy, environment, social, cultural, etc.).
In the perspective of a standardized and integrated 3D urban information system, there is a need to 
develop an urban analysis toolkit providing analyses, simulation and management functionalities which 
could be combined and shared. If we can think about performing and combining various queries and 
processes, it represents several challenges in terms of processes standardisations technical and semantic 
interoperability, etc. The relevance of such combined analyses is also debatable, and at least assessment 
methods should be developed.
Since the beginning of the Action, the question of the enrichment of 3D city models by simulations 
and spatial analyses was raised. Indeed, up-to-now, the interaction between 3D data models and simu-
lation models (such as an AQM for example) is one directional; the simulation is performed using 3D 
data and simulation output is not permanently linked with the 3D data. We discussed the opportunity 
to keep simulation results in a 3D city model either by a change of 3D city model object attributes, or 
by the creation of new objects within the 3D city models. It has been identified as a possible kind of 
enrichment. It raises once again standardisation and integration issues and also a question about the as-
sessment of the quality of the information that has been produced.
The Action also focused on visualization. Indeed, the growing variety of 3D information (new types 
of data, simulation outputs, etc.) implies the adoption of the most efficient visualisation techniques. 
This is a core issue knowing that visualisation is the most important communication media according 
to the users; it depends on application, data and processes. Work is needed to categorize and to provide 
formalized visualisation techniques, anticipating mobile devices and technological evolution.
2.3.2. Research Agenda
The following points can be considered as important elements of a research program for the future:
•	
Inventory of 3D simulation / analysis models;
•	
Study of existing and prospective needs of 3D simulation / analysis in the urban context;

302
Challenges of Semantic 3D City Models 
﻿
•	
Development of new 3D simulation / analysis models possible within an enriched 3D city model 
context;
•	
Development of a 3D tool kit for urban management;
•	
Categorization and formalization of visualization techniques;
•	
Study of the enrichment of 3D city models with simulation / analyses outputs.
2.4. Challenges in Terms of Data
2.4.1. Context
Over the past 20 years, data management was considered as the main limitation of the development of 
3D city models or more generally for 3D GIS. It is changing. First, photogrammetry and land survey-
ing techniques have evolved and thanks to automation and sensors evolution, 3D acquisition becomes 
easier. Furthermore, a lot of other technologies have emerged such as global navigation satellite systems 
(GNSS), satellite photogrammetry, laser scanning and combined solutions such as mobile mapping, 
drones, etc. The amount of 3D geometric data is exploding. Data related to underground utilities or soil 
structures have to be taken into account as well. The data production context has changed too; we came 
from a world led by local or national mapping agencies (NMAs) and some big companies to a world 
with multiple specialised data producers, emerging collaborative communities and some powerful web 
solutions developers which tend to impose their business model. This evolution impacts the way data 
should be produced; as analysed throughout this Action, there is a trend to adopt collaborative solutions 
and data co-production. It is no longer possible to ignore the open data and big data streams which hold 
on 3D data like on any other kinds of data.
3D data face the same issue than other types of data: privacy and quality. 3D makes more critical 
the privacy issues as the 3D data is closer to real world and therefore usually well understood by com-
mon users. Data quality and production of standardized metadata is also an important matter. There is a 
need to clarify and to formalize ways to describe data geometry such as the concept of level-of-details, 
resolution, precision and accuracy. There is also an increasing pressure for providing up-to-date data to 
users; as mentioned above, they are more sensitive to data obsolescence since they can associate them 
more easily to real world features. There is a huge challenge about temporality and dynamic informa-
tion management.
However 3D data are not only limited to 3D geometry but they also imply to consider semantic data 
and environmental data. 3D analyses, 3D models and consequently 3D urban management systems require 
performing crossed analyses on various types of data, and not only geometric features. There is a need to 
organize the production and collection of semantic data which can be integrated into a 3D environment 
(census data, cadastral data, etc.). They could also come from simulation or analyses. Environmental 
sensors provide also huge amounts of data about air quality, noise, etc. It is essential to consider these 
data to feed 3D integration platforms and to move towards smart cities.
2.4.2. Research Agenda
The following points can be considered as important elements of a research program for the future:

303
Challenges of Semantic 3D City Models 
﻿
•	
Inventory and assessment of 3D acquisition techniques;
•	
Development of coproduction strategies including data updating;
•	
Identification and formalization of semantic and environmental data collection techniques;
•	
Further studies on 3D data privacy issues, and creation/development of ethical rules.
2.5. Prioritising Challenges
The identified elements in the previous sections are summarised in Table 1 and are organised following 
their research priority levels. A high priority level means that the item is currently underestimated or 
needs to reach a higher level of maturity. A medium priority level means that the item is clearly identi-
fied by the community and studies or solutions exist or are in development.
3. ISSUES AND PERSPECTIVES OF THE WORK
The motivation behind the development of semantically enriched 3D city models is the belief that effec-
tive and useful urban models cannot be accomplished without a proper knowledge base that integrates 
knowledge in a variety of formats and in a meaningful way. As a basis for urban models, an integrated 
knowledge base has been proposed as a container of a variety of traceable information needed to exhaus-
tively describe the urban context. By organizing theoretically- and empirically-sound knowledge for a 
semantically enriched representation of urban spaces into a coherent framework, we have attempted to 
improve the methodology of the modelling process.
There is an urgent need of a converging knowledge representation that requires unity of models, 
strategies and perspectives, contributed by the relevant disciplines and experts. The knowledge base 
should provide a means for integrating and interconnecting heterogeneous data such as urban maps, pho-
tographs, cadastre data, and various unstructured data, as well as empirical studies, social surveys. This 
effort needs access to data, solicited and gathered by experts coming from diverse fields (e.g., architects, 
city planners, social science experts, geographers, computer scientists, etc.), using various solicitations 
and analytical methods. Semantic heterogeneity, terminology differences, inconsistency, redundant data 
and interoperability are some of the problems encountered. Neither the technological aspects nor the 
Table 1. Summary of research agenda items organised following their level of priority and their research 
axes
Priority Level
Concepts
Models Integration
Simulation / Analyses
Data
High
• Evolution of paradigms 
• Evaluation of 3D city 
models usability
• Interconnection methods 
• Integration methods 
• Conceptualisation and 
formalization methods
• 3D tool kit 
• Visualization techniques 
• Enrichment of 3D city 
models with simulation 
outputs
• Semantic and 
environmental data 
collection 
• Privacy issues and ethical 
rules
Medium
• Uses and usage 
• Types of semantic 
enrichment 
• Objects to be included
• Extension methods 
• Standards for 3D / 4D 
urban models
• Inventory of 3D S/A 
models 
• Existing and prospective 
needs of 3D S/A 
• Development of new 
3D S/A
• 3D data acquisition 
techniques 
• Coproduction strategies

304
Challenges of Semantic 3D City Models 
﻿
social aspects are to be given supremacy in the knowledge construction and pattern explanation. The 
knowledge categories should be derived from both, theoretical and empirical work in various disciplines. 
The knowledge space is expected to provide a starting point with enough possibilities to account for the 
complexity and relationships existing between urban dynamics of varying nature with the same depth 
as it was investigated in the past.
Context representation and use of sound inferring techniques to understand and to anticipate human 
activities, interactions and behaviour in the urban environment is an imperative. The new directions in 
information technologies directed toward pervasiveness and intelligence have increased the amount of 
raw data collection with a potential to increase our knowledge of the different aspects of social urban 
life. Individual pieces of knowledge or a collection of categories do not constitute a model. The effort to 
find out how those pieces of information are related to one another (e.g., causal and associative relation-
ships) is far more challenging. Employment of a number of tools and intelligent techniques could support 
the process of capturing and visualizing the observable manifestation of behaviour trends and patterns 
i.e., the urban dynamics. Establishing the relevance of these patterns and their proper inclusion in the 
models of urban dynamics is as complex as any reasoning approach in an uncertain, non-deterministic, 
dynamic system.
Urban context knowledge is crucial in disambiguating the meaning and the relevance of the exhibited 
patterns in comparison to the model of urban dynamics. The main focus of our research to date has been 
to correlate discovered patterns with other urban-related knowledge that can help us make sense of the 
underlying complex systems. Extracting qualitative knowledge from large quantities of data is just the 
beginning in our search for the meaning and plausible explanation of urban dynamics. New platforms 
that combine urban informatics with the more traditional urban-related knowledge are yet to be devel-
oped and deployed.
This new enrichment and integration of information, structure and process, of course poses the big-
gest problem, the most important challenge and raises many technical issues that are yet to be resolved. 
Integrating interoperability between many different aspects of the spatial systems of the cities raises the 
questions of the unifying code that will provide us with a platform able to perform such an operation. 
This also includes the agility of city models to shift between different scales and the level of detail of 
the urban elements and urban systems and further on towards the exploration of integrated spatial data, 
allowing us to become aware of what are the mutual influences that affect spatial developments. The 
process of the urban development and the nature of the city itself are still at the brink of a new under-
standing and development in theory and practice. The new and enriched understanding of the cities 
and their structure, partial through the use of city models will also affect the way we comprehend and 
represent the process of their creation and the models of the urban reality.
ACKNOWLEDGMENT
We kindly thank the COST office of the ESF and the TU domain commission for their support and 
funding of our COST Action TU0801. We also wish to thank all the members of the Action TU0801 
with specific focus on the main contributors of the final report namely José-Paulo de Almeida, Matteo 
Caglioni, Thomas Leduc, Guillaume Moreau, Julien Perret, Giovanni Rabino, Roberto San Jose and 
Irina Yatskiv (Jackiva). Finally, we wish to point out that our Action took birth within the COST Action 
C21 Towntology.

305
Challenges of Semantic 3D City Models 
﻿
REFERENCES
Billen, R., Caglioni, M., Marina, O., Rabino, G., & San José, R. (Eds.). (2012). 3D issues in urban and 
environmental systems. Bologna, Italy: Società Editrice Esculapio Srl.
Billen, R., Cutting-Decelle, A.-F., Marina, O., de Almeida, J.-P., Caglioni, M., Falquet, G., Leduc, T., 
Métral, C., Moreau, G., Perret, J., Rabino, G., San Jose, R., Yatskiv, I., & Zlatanova, S. (2014). 3D City 
Models and urban information: current issues and perspectives. Les Ulis, France: edpsciences
Leduc, T., Moreau, G., & Billen, R. (Eds.). (2012). Usage, Usability, and Utility of 3D City models. 
Les Ulis, France: edp
This work was previously published in the International Journal of 3-D Information Modeling (IJ3DIM), 4(2); edited by Jason 
Underwood, Sisi Zlatanova, and Umit Isikdag, pages 68-76, copyright year 2015 by IGI Publishing (an imprint of IGI Global).

306
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  17
DOI: 10.4018/978-1-5225-1677-4.ch017
ABSTRACT
As libraries make the transition from information repositories to centers of learning, librarians are under 
pressure to collaborate more effectively with other academic units and departments. At the same time, 
classroom faculty feel pressure to experiment with innovative teaching methods, to provide experiential 
learning opportunities, to be more interdisciplinary and collaborative, and to engage their students more 
proactively. A 3D printing curricular collaboration between a library and an academic department is 
presented that illustrates the importance of collaboration and innovation, the changing mission of li-
braries, the learning styles of millennial students, and the benefits of experiential learning. The chapter 
explores in-depth both the opportunities presented by curricular collaborations and the challenges to 
providing technologies in a curricular context.
INTRODUCTION
Academic libraries are experiencing transformational change - rather than just housing and disseminating 
information, libraries are transitioning into multi-purpose learning centers. To meet the contemporary 
academic library mission of active involvement in teaching and learning, librarians must explore ways in 
which today’s students learn and how we can best support their academic development. Rapidly evolving 
and emerging technologies offer new opportunities for libraries to partner with academic departments 
to design technology-infused curricula that enhance teaching and learning. Librarians should feel chal-
lenged to ensure that their students have the information and technology literacy skills to deal with a 
rapidly changing information world.
Curricular Collaborations:
Using Emerging Technologies to 
Foster Innovative Partnerships
Susan M. Ryan
Stetson University, USA
W. Tandy Grubbs
Stetson University, USA

307
Curricular Collaborations
﻿
As libraries make the transition from information repositories to centers of learning, librarians are 
under pressure to broaden their missions and collaborate more effectively with other academic units and 
departments. At the same time, classroom faculty feel pressure to experiment with innovative teaching 
methods, to provide experiential learning opportunities, to be more interdisciplinary and collaborative, and 
to engage their students more proactively. This chapter opens with a discussion of the changing mission 
of libraries, and the importance of collaboration and innovation. The authors will then explore research 
on the learning styles of millennial students and the benefits of experiential learning, and detail potential 
collaborations with academic departments in developing innovative uses of library-housed technology 
to support curriculum development and faculty and student research. The chapter will describe both the 
opportunities presented by curricular collaborations (support of mission; library relevancy and visibility; 
marketing and promotion; fundraising; student research and learning; and librarian, faculty, and student 
publication and presentation possibilities) and the challenges to providing technologies in a curricular 
context (collaboration barriers; management of technology; costs; hardware and software issues; time 
commitment; staffing; and level of mediation required). To ground the discussion with an illustrative 
example, a 3D printing curricular collaboration between a library and an academic department will be 
featured. The chapter will close with measures of success and a discussion of how libraries can develop 
their own curricular collaborations.
BACKGROUND AND LITERATURE REVIEW
Discussions of 3D printing in libraries began to appear in the literature in 2012, with public libraries as 
early adopters - mostly as part of the “MakerSpace” movement. The University of Nevada Reno advertised 
itself as the first academic library in the United States to offer 3D printing and scanning to all students as 
a library service, but the printers were located in the specialized Science & Engineering Library (News 
from the field, 2012). Similarly, as an early 3D printing adopter, the University of Alabama housed its 
3D printing studio in its Library for Science and Engineering (Scalfani & Sahib, 2013). Only in 2014 did 
articles begin to appear detailing 3D printing start-ups in several academic libraries (Gonzalez & Bennett, 
2014; Norton & Gonzalez, 2014; Pryor, 2014), with Steven Pryor noting that “little has been written so 
far regarding the use of 3D printing technology in academic libraries” (Pryor, 2014, p. 2). Very little has 
been published on academic libraries collaborating with academic departments on any direct curricular 
applications for 3D printing. While Gonzalez & Bennett (2014) stress the importance of identifying 
“internal community advocates” such as instructors of design classes in engineering and the fine arts, 
they offered no evidence of direct collaboration. Likewise, although Scalfani & Sahib (2013) state that 
“two Art & Art History Professors have begun to incorporate the 3D printing workshops and services 
directly into their design classes,” they do not detail any curricular collaborations with the library other 
than using the library’s equipment. Librarians in two libraries - the University of Alabama Science and 
Engineering Library (Scalfani & Vaid, 2014) and the University of Florida Science Library (Gonzalez 
& Bennett, 2014) - assisted instructors by printing models used in science and mathematics classes, but 
neither library indicated any substantial collaboration with teaching faculty. The academic literature offers 
no examples of academic 3D printing curricula that have been developed. Two commercial companies, 
however, have designed 3D printing curricula for educators; NVBOTS offers lesson plans that integrate 
3D printing into the classroom that are in line with the Common Core State Standards and Next Genera-

308
Curricular Collaborations
﻿
tion Science Standards (NVBOTS unveils 3D printable curricula, 2015) and Stratasys provides colleges 
and universities with a lengthy curriculum around which courses can be designed (3DTectonix, 2015).
Transformational Change
As access to information has become ubiquitous, academic libraries have grappled with rapid change. 
Examples of libraries undergoing transformational change abound in the library and higher education 
literature, including changing missions, the demise of print material, the addition of non-traditional 
services, and the influence of emerging technologies. Academic libraries are now often referred to as 
the “learning commons” or the “learning center” (or have become one part of a larger “learning center” 
that houses other academic support services as well as the library). Whatever the vocabulary used, the 
message is important – libraries have an increasingly broad mission to support learning in a variety of 
ways. Learning support has traditionally meant housing information (in some kind of tangible or online 
format) that library users can access with or without assistance. With the changing mission of librar-
ies and the emergence of so many learning technology options, however, learning support often means 
hosting student support services, tutoring, instructional technologies, MakerSpaces, innovation centers, 
3D printing studios, robotics laboratories, and more.
Bunnett believes that librarians, as experts in their field, have an obligation to drive technological 
change (Bengston & Bunnett, 2012, p. 702). The idea of providing multiple avenues to achieve learning 
seems completely compatible with what libraries have done for decades. In 1995, Gorman offered a 
modern twist to Ranganathan’s (1963) fundamental Five Laws of Library Science. Gorman essentially 
rearticulated the essential truths of the laws that originally pertained to books, readers, and libraries and 
expanded them to encompass the information needs of modern society. He outlined the new laws as:
•	
Libraries Serve Humanity,
•	
Respect All Forms by Which Knowledge is Communicated,
•	
Use Technology Intelligently to Enhance Service,
•	
Protect Free Access to Knowledge,
•	
Honor the Past and Create the Future (Gorman, 1995).
Librarians have always had rather noble ambitions to serve and libraries have taken a leading role 
in promoting literacy in many forms – reading literacy, information literacy, technological literacy, etc. 
Gorman suggests that the best method for assessing how well libraries are meeting the first law, Librar-
ies Serve Humanity, is to ask, “How will this change make the service that this library gives better?” 
He asserts that service in libraries should exceed the expectations of its users. It is the librarian’s job to 
explore how emerging technologies might benefit students in ways they themselves may not have con-
sidered (or that their professors may not have considered). The second law, Respect All Forms by Which 
Knowledge is Communicated, urges us to acknowledge that “the library of the future will use all kinds 
of carriers of knowledge and information…” and that librarians must study the “realities of each means 
of communication in the light of the history of innovation…” (Gorman, 1995, p. 784). How the transfer 
of knowledge is defined is critical – librarians cannot allow themselves to be bound by definitions of 
knowledge that are confined to words on paper or on a screen. Experiential or applied learning imparts 
knowledge in ways that may be particularly relevant to today’s generation of students.

309
Curricular Collaborations
﻿
The third law is perhaps the most relevant to this discussion – Use Technology Intelligently to En-
hance Service. While Gorman undoubtedly was not envisioning 3D printing, the dominance of mobile 
technologies, and other emerging technologies in 1995, he understood that “the history of progress in 
librarianship has been a story of the successful integration of new technologies and new means of com-
munications into existing programs and services” (Gorman, 1995, p. 785). He rightfully asserts that 
intelligent use of technology means building problem-solving skills rather than just seeking applications 
for a particular technological tool. Protect Free Access to Knowledge, the fourth law, tells librarians to 
take responsibility for their libraries as institutions. Gorman is referring to the archival safeguarding of 
knowledge over time, but in the broader construct, this law tells us to protect the continued existence 
of the library by staying relevant to the library user of today. Finally, the fifth law, Honor the Past and 
Create the Future, should be a rallying cry to all librarians: “The library of tomorrow must be one that 
retains not only the best of the past but…there must be the acceptance of the challenge of innovation” 
(Gorman, 1995, p. 785).
If we are to accept Gorman’s marching orders, libraries must find avenues to expand effectively 
their campus role in student learning. One of the best ways to do this is to develop collaborations with 
faculty members who are already well positioned to bring new technologies to students in ways that 
are meaningful in a particular discipline. Just as librarians are feeling pressure to up their game and 
expand into new territories in order to remain relevant, classroom faculty are inundated with messages 
that they need to innovate their teaching (flipped classrooms, new pedagogies), provide experiential and 
applied learning opportunities (hands-on learning, fieldwork, internships), offer undergraduate student 
research programs (developing research projects that can be shared locally, as well as at conferences, and 
in publications), and keep up with their own research and publication agendas. Developing curricular 
applications for emerging technologies in cooperation with libraries may appeal to faculty members’ 
needs to address all of these pressures.
The Drive to Innovate
Finding willing faculty collaborators is just one part of creating fruitful partnerships. Just as important 
is the need to find innovative projects that will fuel the interest of all parties involved. The term “innova-
tion” abounds in today’s library and higher education literature – perhaps to the point where it has lost 
significance or any real impact. A pervasive fear exists that if a library is not “innovating” it is falling 
behind and heading toward obsolescence. The most basic definition of innovation is something new – “a 
new idea, device, or method; the act or process of introducing new ideas or methods” (Merriam-Webster 
Dictionary, 2015). Libraries regularly offer new services or try new tools or processes to accomplish 
tasks, and most libraries likely think of their operations as innovative in some way. But to be truly 
innovative, librarians must create an organizational culture that encourages and supports innovation 
that goes far beyond offering a new method of doing business. True innovation means risk-taking and 
tolerating failure. It means being an early adopter and being the first to do something well. Librarians 
must be willing to persuade university administrators, faculty members, and students (and sometimes 
their own staffs) that a particular innovation offers enough potential rewards for the institution that it is 
worth the risks and costs.
Bengston and Bunnett (2012) offer an interesting point-counter point view of managing technologi-
cal innovation. Bengston, an emerging technologies librarian, argues that when innovation is fostered, 
it “can become an object of pride and a constant reminder of an organization’s vitality, relevance, and 

310
Curricular Collaborations
﻿
utility” (Bengston & Bunnet, 2012, p. 701). Bunnett, a library administrator, contends that institutions 
have fiscal responsibilities where risks must be carefully managed (Bengston & Bunnett, 2012, p.702). 
So what is the balance between the freedom to innovate and the need to limit failures and maximize 
success? Below, in the discussion of “collaboration challenges,” questions are proposed that encourage 
a library to plan thoughtfully for potential technology innovation partnerships - careful consideration of 
the probability of success may mitigate risks significantly.
Millennial Students and Today’s Employers
The millennial generation has been inundated with information since birth, the flow of data delivered 
across an expanding number of media, Internet, and other digital communication outlets. Millennials 
are more commonly accessing information in a wireless fashion - smartphone use for texting, social 
media, and other forms of wireless communication has been commonplace for years. Together with 
growing tablet use, wireless connectivity is now accessible virtually every waking hour of the day1. 
Armed with the latest gadgetry, teens and young adults have played a large role in ushering in a new 
wireless culture, where graphically enriched social exchanges can take place anywhere and at any time. 
While on occasion millennials have had the luxury of soliciting their digital information (e.g., Google 
searches, iTunes downloads, social media exchanges, or simply enjoying a DVD movie), far too much 
of the information stream has consisted of unsolicited advertising generated from a highly aggressive, 
global, consumer-driven economy. Years from now, a historian tasked with capturing the essence of the 
millennial age might very well characterize it as “one, never-ending sales pitch.”
How has constant connectivity, the resultant flood of information, changes in the nature of social 
interactions, and the excesses of consumerism impacted and shaped the millennial learner? Undesirable 
characteristics that have been attributed to this upbringing include an increased prevalence of stress and 
anxiety, early exposure to unhealthy adult activities, and difficulties in forming healthy social pacts 
(Considine, Horton, & Moorman, 2009; McMahon & Pospisil, 2005; Murphy, 2010; Prensky, 2010). 
The deluge of information has forced millennials to adopt more effective screening mechanisms. This 
adaptation to better filter may come with a price - shorter attention spans and less fortitude for exploring 
any one topic with sufficient depth. Taken together, these attributes are clearly detractors for learners 
and represent a challenge for 21st century educators.
Conversely, not all attributes of the millennial generation represent a negative. Many have argued that 
millennials are better multitaskers, more effective participants in a team work environment, and more 
accepting of constructive criticism, this latter characteristic reinforced from years of receiving constant 
feedback in school and during childhood/early teen activities. Perhaps most notable, the technological 
proficiency of most millennial students is high and often exceeds that of the instructors (Murphy, 2010; 
Prensky, 2010). As enthusiastic supporters of new technologies and possessing a genuine desire to better 
understand the latest digital gadgetry, millennials are both motivated and better positioned to become the 
future innovators who will drive forward the technological components of our knowledge-based economy.
Educators, including librarians, should certainly keep in mind the positive attributes of the millennial 
generation as we design future learning strategies. Higher education institutions are inundated, however, 
with admonitions that they are not preparing students for the realities of the 21st century labor market. 
An Association of American Colleges & Universities study that queried employers on what they valued 
in college graduates determined that:

311
Curricular Collaborations
﻿
Employers value the ability to apply learning in real-world settings and broadly endorse an emphasis 
on applied, hands-on learning experiences in college today, such as an internship, a senior project, a 
collaborative research project, a field-based project in a diverse community setting with people from 
different backgrounds, or a community-based project.
•	
88% think that it is important for colleges and universities to ensure that all students are prepared 
with the skills and knowledge needed to complete an applied learning project. 
•	
73% think that requiring college students to complete a significant applied learning project before 
graduation would improve the quality of their preparation for careers. 
•	
60% think that all students should be expected to complete a significant applied learning project 
before graduating ( Hart Research Associates, 2015 , p. 2). 
It is imperative that both faculty members and librarians work to ensure that graduates not only have 
the distinct disciplinary skills that they need to succeed in their field of choice, but that they also have 
the applied learning skills that employers seek.
Gartner, a large information technology research and advisory company, produces an annual “hype 
cycle” graphic for emerging technologies. The chart graphs “expectations” for a particular technology 
against its advancement over time - with the technologies passing through the stages of “Innovation 
Trigger,” “Peak of Inflated Expectations,” “Trough of Disillusionment,” “Slope of Enlightenment,” and 
“Plateau of Productivity” (Gartner, 2014). In 2009, 3D printing (which will be discussed in detail below 
as an example of a library-academic department curricular collaboration initiative) showed up on the 
hype cycle as an “innovation trigger” with rising expectations (Gartner, 2009). At that point, early adopt-
ers showed interest in the technology, but libraries had not in large part taken notice. Five years later, in 
2014, consumer 3D printing was at the “peak of high expectations” (Gartner, 2014) and the technology 
could be found in many academic, public, and special libraries (although still not in the majority of librar-
ies). With a predicted five years before reaching the “plateau of productivity,” 3D printing is probably 
a good investment for libraries. While the hype cycles are speculative, they are also informative. Before 
choosing to buy into an emerging technology, particularly one that libraries want to “sell” to faculty as 
a worthy collaborative tool, librarians must do their homework not only on what relevant technologies 
are emerging, but also which technologies best fit their curriculum and constituencies.
Emerging Technologies: 3D Printing
Many emerging or new technologies might be suitable additions to a library, depending on the cur-
riculum and mission of a particular institution. To illustrate both collaboration opportunities and chal-
lenges, a case study is offered in which Stetson University2 librarians actively sought collaborators to 
develop learning and curricular applications for 3D printing. (Keep in mind, however, that the kinds of 
issues discussed pertaining to 3D printing are the same issues that libraries would want to consider for 
any new technology.) Libraries have long provided technology (computers, scanners, printers, etc.) in 
support of academic programs. The acquisition of 3D printers is one more technology resource to add 
to the mix. What Stetson librarians discovered when initially considering the potential addition of 3D 
printers, however, is that 3D printing in libraries has been primarily implemented for “fun” or creative 
initiatives, but has not yet been well-developed as a true learning technology integrated into curricular 
assignments. A number of libraries with 3D printers do not let students have any hands-on experience 

312
Curricular Collaborations
﻿
with the printers at all - rather, students drop off files to be printed by library staff. While giving students 
a form of access to 3D printing, mediated printing becomes a passive activity for the student with a large 
part of the learning component removed.
The Stetson Library wanted to implement 3D printing as a curricular tool that would support faculty-
developed classroom and/or laboratory assignments. At the same time, the chemistry faculty began to 
consider how students might create 3D chemical models as a lab activity in the chemistry curriculum. 
The faculty in both the library and the Chemistry Department wanted to explore the question, “How 
might educators take advantage of 3D printing to enhance curricula and promote student learning of 
important principles?” A project was undertaken involving the library and several chemistry students, 
in collaboration with chemistry faculty, to answer that question. The timing seemed ripe - the costs, 
compactness, lower maintenance, and overall reliability of 3D printers have trended favorably in recent 
years to an extent that it is now possible to implement this technology within an academic setting.
What exactly is 3D printing? References to the technology abound in all types of media - the news, 
business and popular literature, and television programs all tout 3D printing as an impressive inven-
tion with an expansive range of uses. It is now common to hear about 3D printed vehicles, parts for the 
Space Station, artificial limbs for humans and animals, and much more. Applications are limited only 
by imagination and both businesses and researchers are quickly finding new ways to apply 3D printing 
to their work in education, art, design, architecture, healthcare, computer science, and across a wide 
variety of industries. Although the technology has been developed since the mid-1980s and has long 
been used by manufacturers to develop prototypes (referred to as “rapid prototyping”), only in recent 
years has it emerged as a viable commercial option suitable for library use.
3D printing, or additive manufacturing, is the process of making three-dimensional objects from 
a digital file. “Additive” refers to the process of laying down hundreds (or thousands) of successive 
thinly-sliced horizontal layers of some type of material (generally some type of plastic filament in library 
applications) until the object is created. Several different methods of building the layers may be used: 
selective laser sintering (SLS) and fused deposition modeling (FDM) are the most common additive 
technologies. It should be noted that even though it is used generically, FDM is a term trademarked by 
one of the major 3D printing companies, Stratasys®. The non-trademarked equivalent term is “fused 
filament fabrication” (FFF), but is not as commonly used (3Dprinting.com, [2015], “Fused Deposition 
Modeling,” para. 3).
Before considering the addition of 3D printing to library services, the pros and cons of the two most 
common filaments (printing material) should be investigated. ABS (Acrylonitrile Butadiene Styrene) 
is a fairly strong plastic that is often used in printers with heated build chambers. ABS tends to extrude 
well, but once it is extruded and starts to cool, it will shrink. Shrinkage can cause issues if the print job 
is paused and restarted, may make it more difficult to get the part off the print baseplate, and can tend 
to split or crack if an object is tall. Generally, however, ABS is a reliable print filament. Libraries will 
want to investigate carefully the placement and ventilation of the printer if ABS is used; a robust discus-
sion is taking place on library and MakerSpace listservs about the potential health hazards from emitted 
fumes3. A number of scientific studies have concluded that significant exposure to ABS particles may 
be hazardous when inhaled (Rutkowksi & Levin, 1986; Schaper, Thompson & Detwiler-Okabayashi, 
1994; Stephens, Azimi, El Orch, & Ramos, 2013; Zitting & Savolainen, 1980). While the jury is still 
out on potential environmental hazards, at the very least ABS will produce some level of “plastic” odor 
that may bother some people. PLA (Polylactic Acid), on the other hand, is biodegradable, is generally 
considered non-toxic, and should not pose any environmental issues. It is softer than ABS, however, so 

313
Curricular Collaborations
﻿
it is not suitable for printing objects that need to be very strong – objects printed with PLA can also be a 
little brittle, so more possibility for chipping or breakage exists. PLA is used as the dissolvable support 
structure filament in conjunction with ABS modeling material in some printers, and this combination 
works well. Libraries offering 3D printing, especially with ABS filament, should plan for work space in 
which fumes can be controlled or ventilated. Printers may be enclosed in some sort of container - often 
a plastic “topper” to contain fumes or a more sophisticated ventilation hood (as found in many univer-
sity science laboratories). Options can range from relatively inexpensive (some libraries have even built 
their own printer covers) to very expensive commercial hoods that vent to the outside of the building.
Three-dimensional printable files can be created from scratch using CAD (Computer Aided Design) 
programs, such as AutoCAD® (commercial software) or TinkerCAD (a freely available web-based 
application). The rapid development of open source file websites (many fueled by the early adopter 
hobbyists) such as Thingiverse, Cubify, AutoDesk 123D®, Pinshape, the NIH (National Institutes of 
Health) 3D Print Exchange4, and others allow users to quickly find downloadable 3D files that have 
already been created. A Google search will bring up many more freely available print files (although as 
with any Internet source, caution must be used in downloading files from unknown sources). A number 
of 3D graphical file formats exist (.stl, .vrml, .x3d, .obj, .iges, .dae)5, but the .stl (Standard Tessellation 
Language) format is used by most commercial printers found in libraries. Alternatively, a physical object 
can be 3D-scanned using a 3D scanner or freely-available applications that convert multiple photographs 
of an object into a file that can be read by 3D printer software.
Additive manufacturing technology has evolved to the point where the consumer market, including 
the library market, can maintain and afford 3D printers. Low-end printers meant for home and hobbyist 
use can sell for as little as $250 or less, although at this time, quality, reliable 3D printing requires a 
printer at about the $20,000 level. These costs are sure to decrease, however, as more vendors enter the 
market and the technology continues to improve. Libraries now have access to affordable 3D printers, 
the technology is quickly improving, and students and faculty have easy access to files or can create their 
own. What other rationale exists for a library to provide this technology? The technology fits well with 
Gorman’s re-envisioned laws of library science which is reinforced by Griffey’s assertion that 3D printing 
belongs in libraries because they “have adopted the role of providing universal access to technology over 
the last couple of decades” (Griffey, 2012, p. 23). Gonzalez and Bennett (2014) argue that the library 
provides “an environment that traditionally supports creative thinking and collaboration” and that the 
library’s long open hours and service to all disciplines makes it the ideal campus location for 3D printing.
As discussed above, employers seek potential hires who have well-developed applied learning skills. 
Some additive manufacturing experts argue that 3D printing technology will actually change the nature 
of commerce (3Dprinting.com, [2015], “Future,” para. 1). Imagine a day, likely not too far in the future, 
in which we print our dishes, our children’s toys, replacement parts for our appliances and cars. Not only 
will end-users be able to create their own consumer products, but industries will need designers, engineers, 
and material scientists to develop and adapt ever-changing manufacturing methods. Demand for workers 
with 3D printing skills increased by 1,834% between 2010 and 2014 and a 2014 study showed that 35% 
of all advertisements posted for engineering jobs over a 30-day period prioritized 3D printing skills and 
additive manufacturing as the most sought-after skill (Rowe, 2014, para. 1). Emerging trends in industry 
suggest that additive manufacturing will continue to affect professional and consumer markets and will 
cause manufacturers to “rethink their approach to new product development” (McCue, 2012, para. 3). 
At the time this chapter was written, however, only two higher education institutions, one in the United 
States and one in Singapore, could be identified that offered full courses in 3D printing (3DTectonix, 

314
Curricular Collaborations
﻿
2015, para. 1). (For more information, the website 3Dprinting.com offers an accessible overview of 3D 
printing while Gross, Erkal, Lockwood, Chen, and Spence (2014) provide a more detailed explanation 
of 3D printing as it relates to chemistry and biotechnology.)
3D Printing Curricular Collaboration
Readily available market trend data convinced Stetson librarians that teaching 3D printing skills would 
be beneficial not only as a learning tool, but would also bolster students’ appeal to potential employers. 
Stetson chemistry faculty agreed that 3D printing would not only provide curricular advantages, but would 
also enhance the set of marketable technology skills that their graduates possessed. In addition, the 3D 
printing collaborative initiative would draw upon the technological prowess of millennial learners. As 
discussed above, the technological proficiency of many millennials, along with an innate enthusiasm for 
new technologies, makes today’s students a built-in user base for this type of library service. Armed with 
enough evidence that the library could justify the purchase and support of 3D printers as a true learning 
technology, the Stetson library initially bought two MakerBot® 3D printers at a cost of about $2,500 
each. The printers purchased use FDM printing technology, can print in two colors with a 100 micron 
layer resolution, and use a “rafting” support system made out of the same plastic as the 3D objects that 
are printed. Demand was so heavy, and the use of the printers in innovative ways to promote learning 
was so impressive, that less than a year later the library added a $20,000 printer that could handle far 
more sophisticated print jobs. The added Stratasys® printer also uses FDM technology and prints in 
one color at the same 100 micron layer resolution as the MakerBot® printers, but the differences are 
notable (see Figure 1). The more expensive printer uses a heated build chamber, self-calibrates, cleans 
its own extruder print heads to alleviate clogging, and uses dissolvable PLA filament as the support 
structure. The biggest advantage to a more expensive printer, however, is reliability. Failed print jobs are 
common in hobbyist printers, an outcome which is frustrating to both the library staff who must support 
Figure 1. The Stetson Library’s Innovation Laboratory (top), the MakerBot® printer (left) and the 
Stratasys® printer (right)

315
Curricular Collaborations
﻿
the printers, and to the users, especially if the library is charging for use of the printers. The higher-end 
printers have a remarkable success rate - close to 100% - that is well worth the additional cost. The ease 
of use of the high-end printers and the greatly increased sophistication of the printing that can be done 
also makes them attractive. The rule of thumb for any library wanting to purchase 3D printers is to get 
the best printer possible within budget limitations.
Once the printers were in place, the library began its collaboration with the Chemistry Department. 
What does 3D printing bring to the field of chemical education? Certainly an accurate three-dimensional 
visualization of how atoms are arranged within a chemical structure is often required before one can 
truly understand that chemical’s function. The value of hand-held, physical models in support of student 
learning is well documented in the education literature (Abraham, Varghese & Tang, 2010; Gabel & 
Sherwood, 1980; Small & Morton, 1983; Talley, 1973). While commercial student model building kits 
have long been a great asset for those learning organic chemistry, most of these kits have limitations 
with regard to how accurately they can be used to depict the true bond lengths and angles in a molecular 
structure. As will be described below, there are comparatively few limits on how accurately molecular 
structures can be depicted when they are computer designed for 3D printing. Consequently, 3D printing 
represents a powerful new tool that can be utilized by chemists to create more realistic, tangible models 
of molecular structures. 3D printed models can range from simple ball-and-stick renderings of common 
chemical structures to more realistic, space-filling models of organic compounds, proteins, and other 
molecular complexes that show exactly how electron density is distributed around atomic locations. 3D 
printed models can do far more than simply illustrate how atoms are bonded and configured in space; 
they can be used to show how molecular entities chemically bind and interact in a three-dimensional 
fashion (interactions that can be difficult to visualize using traditional student model building kits or 
two-dimensional computer-generated representations).
How does one generate a 3D printable molecular model? While it is possible to use a CAD software 
environment to piece together a series of computer-generated spheres and cylinders with a goal of creating 
a ball-and-stick rendering of a molecule, one quickly learns that such an approach is overly labor inten-
sive and ultimately not conducive to generating an accurate structure. Fortuitously, present day chemists 
already have at their disposal a range of molecular editing and visualization software applications for 
drawing accurate 3D representations of molecules. Different atoms and common molecular units can be 
picked from templates within these applications, inserted into the graphical user interface, and pieced 
together through a sequence of simple click-and-drag mouse maneuvers. Many of these applications also 
include iterative computational chemistry algorithms that will vary the atomic bond lengths and bond 
angles within chemical structures until the energetically optimum configuration of atoms is identified.
Several standard computer text file formats have emerged over the last two decades for storing struc-
tural information about molecules, such as the Research Collaboratory for Structural Bioinformatics 
- Protein Data Bank (RCSB-PDB) format (.pdb), Tripos Mol2 format (.mol2), and Crystallographic 
Information File format (.cif). These formats commonly list the identity and numerical x-y-z coordinates 
of all atoms, and additional information about the types of bonding and connectivity that may exist 
between the different atoms and molecular chains. The primary challenge associated with 3D printing 
a chemical structure lies in finding a way to convert the commonly encountered chemical file formats 
mentioned above into a 3D graphical file format that is recognizable by commercial 3D printers. For-
tunately, avenues for achieving these file conversions are possible. Several step-by-step examples about 
how to convert common chemical structure file formats into the .stl (Standard Tessellation Language) 
format that is recognizable by most commercial printers have previously been described by Violante, 

316
Curricular Collaborations
﻿
Nunez, Ryan, and Grubbs (2015, pp. 129-143). With few exceptions, these conversions can be realized 
through the use of software and web-based applications that can be downloaded or accessed for free.
Stetson’s Chemistry Department has incorporated student use of 3D printing at several points in its 
curriculum. Students have been presented with opportunities to create several types of chemical mod-
els, either as part of established laboratory exercises or as part of an independent study/senior research 
experience. Chemistry students, faculty, and librarians have collaborated in using 3D printing to create 
models that reinforce student learning in the classroom – models that can be passed around among stu-
dents when certain geometrically intensive topics are discussed. A number of 3D printing projects that 
have been completed as part of the library-Chemistry Department collaboration, including examples of 
the specific curricular context of several projects, are described below:
•	
Students and faculty have used molecular editing and visualization freeware (e.g., Avogadro or 
IQmol) to design simple chemical compounds for 3D printing. This approach has been used to 
design chemical models in support of general and organic chemistry instruction. As an example, 
a chemical structure called 2,2-dimethypropane that is encountered in the organic chemistry cur-
riculum is illustrated in Figure 2 as both a .stl file visualization and in final 3D printed form.
•	
Students taking biochemistry, as well as senior research students, have been presented with op-
portunities to download and 3D print biologically relevant chemical structures from web-based 
file sharing resources such as the National Institute of Health (NIH) 3D Print Exchange or the 
Research Collaboratory for Structural Bioinformatics – Protein Data Bank (RCSB-PDB). For ex-
ample, a biochemistry student downloaded, performed the correct file conversion, and ultimately 
3D printed the protein plastocyanin as shown in Figure 3.
•	
Students and faculty have made use of quantum computational chemistry software (e.g., Gaussian) 
to generate the energetically optimum structure of chemical compounds for subsequent 3D print-
ing. The C40 and C60 structures shown in Figure 4 were created in this fashion by a student as part 
of an independent study of fullerene compounds. As part of a laboratory assignment, students 
enrolled in Stetson’s spring 2015 Advanced Physical Chemistry course were each challenged to 
pick a chemical structure that was personally meaningful, to create and optimize the structure us-
Figure 2. An organic hydrocarbon called 2,2-dimethypropane illustrated in 2D computer graphical .stl 
format (left) and as a 3D printed object (right)

317
Curricular Collaborations
﻿
ing computational chemistry software, and to subsequently 3D print the structure. Students from 
this class and their 3D printed molecules are shown in Figure 5.
•	
As part of a student-faculty research project, quantum computational chemistry software (e.g., 
Gaussian) was used to create and optimize a chiral recognition host-guest complex which was 
subsequently 3D printed. The host-guest complex associated with this work is illustrated at vari-
ous stages of construction in Figure 6 and as a 3D printed ball-and-stick model in Figure 7. The 
results of this work have shed new light on seminal Nobel Prize winning research undertaken by 
Dr. Donald Cram (UCLA) in the 1970s and 1980s. In particular, the 3D printed model of the chiral 
host-guest complex suggests that the highly selective binding that occurs between this particular 
molecular host and guest is restricted to only one of two possible binding geometries that were 
proposed in Cram’s original work (Cram et al., 1975).
Figure 3. The biochemical protein plastocyanin, displayed as a 2D computer graphic ribbon structure 
(left) and as an active 3D print job (right)
Figure 4. 3D printed models of two carbon fullerene structures, C40 (left) and C60 (right); the C60 struc-
ture shown on the right is more commonly referred to as a Buckminsterfullerene (Bucky Ball) molecule.

318
Curricular Collaborations
﻿
•	
Students and faculty have used a free web-based CAD drawing application (e.g., Tinkercad) to 
create models for chemical instruction. In particular, a senior research student designed 3D print-
ed models of atomic orbitals which were subsequently used in the classroom in support of general 
chemistry instruction. A student-faculty team also designed and 3D printed a model that illustrates 
surface catalysis at the atomic level. This model was subsequently used by the student-faculty 
team in support of a K-12 STEM educational outreach activity in central Florida.
•	
Students and faculty made use of a chemical crystallographic software resource (e.g., Crystalmaker) 
to create cubic crystal lattice models that were 3D printed and used in support of classroom in-
struction in general chemistry (shown in Figure 8).
•	
As part of another research project, students and faculty utilized computational chemistry soft-
ware (e.g., Gaussian) to create a series of space-filling molecular models that can be used by stu-
dents taking organic chemistry to visualize and better understand the effects that steric interactions 
Figure 5. Students from the Stetson University spring 2015 Advanced Physical Chemistry class, with 
their chemistry professor and the librarian who assisted (top), pose with molecules that were created 
as part of a lab exercise (bottom)

319
Curricular Collaborations
﻿
have on the reactivity between molecules. The outcomes of this particular project were presented 
by the student and faculty member at the 2015 National American Chemical Society Meeting 
(Diaz-Allen & Sibbald, 2015).
Presently, the authors are aware of only a few other examples where the use of 3D printing in sup-
port of chemical education has been reported in the peer-reviewed literature. Readers are encouraged to 
consult a recent article in the Journal of Chemical Education that details how 3D printing can be used 
to create models of different chemical structures that illustrate important spatial symmetry classes that 
Figure 6. Step-by-step creation (from top-left to bottom-right) of a 3D model of a chiral recognition 
host complex that selectively binds one chiral isomer of the amino acid structure (phenylglycine methyl 
ester ammonium cation)
Figure 7. 3D printed ball-and-stick model of the host-guest chiral recognition complex

320
Curricular Collaborations
﻿
are encountered by students of chemistry (Scalfani & Vaid, 2014). A separate article in the same volume 
of the journal describes how educators can 3D print molecular potential energy surface models that help 
students better understand how molecular structure impacts reactivity (Lolur & Dawes, 2014). At the 
2015 National American Chemical Society Meeting, faculty from Davidson College reported on two 
applications of 3D printing, one involving the creation of a surface that promotes student understand-
ing of the relationship between potential energy and conformational geometry in a molecule (Carroll 
& Blauch, 2015), and the other involving the creation of a model that illustrates the phase relationship 
between pressure, volume, and temperature in a real gas (Striplin, Carroll, & Blauch, 2015).
OUTCOMES OF COLLABORATION
As much as academic libraries provide essential support to academic programs, it is relatively rare to 
have a direct curricular collaboration of this type. Without the library’s interest in housing and support-
ing 3D printing, the Chemistry Department may not have developed this line of student research and 
development of laboratory experiments. Without the Chemistry Department’s willingness to collaborate 
on curriculum development, the library would have been reluctant to provide 3D printing as a technology 
it could justifiably support. Based on what was learned from the library-chemistry collaboration, the 
library created a 3D Printing Innovation Laboratory and has subsequently worked with other academic 
departments to develop 3D printing research projects.
The collaboration, which has led to some curricular innovations in chemistry, has proven successful 
for both the department and the library. None of the current faculty in the Stetson chemistry program can 
recall a more fruitful partnership with a neighboring campus unit. Over the first four month period, all 
five tenured/tenure-track chemistry faculty pursued different 3D printing projects with students. While 
the chemistry faculty have been involved in developing 3D printing ideas for classroom/laboratory use, 
the most impressive outcome is the enthusiasm of the students involved in the project. Innovative un-
dergraduate research and publication are hallmarks of a successful liberal arts education. To date, many 
chemistry students have developed interesting research projects as part of the collaboration, and one 
Figure 8. 3D printed unit cell representations of simple cubic, body-centered cubic, and face-centered 
cubic crystal lattices that are encountered by students taking general chemistry

321
Curricular Collaborations
﻿
student devoted his senior research thesis to the use of 3D printers in developing innovative representa-
tions of molecular structures. Two students had the opportunity to publish in the chemistry literature 
on their involvement with the 3D printing project and another presented her 3D printing research at a 
national chemistry conference.
The success of this project for the library cannot be overstated. While the Stetson Library has en-
gaged in many worthy initiatives over the years, the 3D printing program has led to more attention, both 
within the university and externally, than anything in recent memory. Librarians worked directly with 
chemistry faculty and chemistry students to determine what software was needed to design the most ef-
fective research and laboratory assignments. In the case of the Advanced Physical Chemistry lab exercise 
described above, both the chemistry professor and a librarian worked with students to complete their 3D 
projects. In spring 2016, a semester-long rapid prototyping interdisciplinary course will be offered with 
team teaching done by a chemist and a political scientist with significant assistance from a librarian and 
a library staff member.
The library dean, with the chair of the Chemistry Department, has presented on the collaboration 
at a state library conference and at four national conferences – two library conferences and two chem-
istry conferences. The opportunity for a librarian to share a collaboration success story with a national 
chemistry audience, and a chemist to share the same story with a librarian audience, is invaluable public 
relations. Two book chapters detailing various aspects of the collaboration have been published (this 
one and one in a book on teaching chemistry to the millennial generation), as well as a peer-reviewed 
article. In less than two years after the project launched, the Stetson Library has won two competitive 
innovation awards for its collaboration with the Chemistry Department. The collaboration has stimulated 
donor interest, and captured the attention of the Board of Trustees of the University. It is not the 3D 
printing technology itself that has garnered the successful outcomes, but rather the collaborative use of 
an emerging technology to develop pedagogical and learning innovations.
Beyond Chemistry
Chemistry student and faculty enthusiasm quickly spread to other disciplines. Librarians have assisted 
biologists in printing scanned replicas of items in their Natural History Museum (human and animal 
skulls, for example). The Museum has long wanted a manatee skull for its collection, but could not acquire 
one. As an alternative, Stetson was able to obtain a manatee skull on loan that librarians subsequently 
scanned and 3D printed, creating a replica skull for the Museum. Most of the collaborative work done 
with the Chemistry Department has involved using various software packages to convert chemical struc-
ture files from some known chemistry database into a 3D-printable format - with the resulting 3D image 
primarily used to illustrate chemistry concepts. Biologists, however, have wanted to scan existing physi-
cal objects from the Natural History Museum. Affordable 3D scanning technology has not necessarily 
kept pace with the rapid development of 3D printing, although that is starting to change. While realistic 
commercial scanning (including full-body scans of humans) can perform well, the cost is prohibitive 
for the library market. The hobbyist-level scanners, like the hobbyist 3D printers, have high degrees 
of failure. The low-end scanner offered from MakerBot® (one of the early leaders in 3D printers) at a 
cost of about $700, for example, has very limited capabilities of what can be scanned successfully. The 
scanner is only capable of 3D capturing relatively small objects that must be placed on a rotating plate in 
various positions (upright, sideways, and upside down) for the scanner to get enough data points. Many 
objects, because of their shape, cannot be placed upside down on the scanner, and thus the file created 

322
Curricular Collaborations
﻿
is incomplete and misshapen. Likewise, the scanner has trouble scanning items that are shiny or glossy, 
are too big, or that cast too many shadows. Even the ambient light around the scanner and the color on 
the walls near the scanner (causing reflection) may affect the success of the scan.
Recently, however, free web-based scanning applications have improved tremendously and can 
scan most items very accurately from one position. One that works particularly well is AutoDesk 123D 
Catch®, available as an iPhone or Android application. The object is scanned by circling it 360 degrees 
while taking multiple photographs at the same height at regular intervals around the circle (perhaps 15 
photos or so). An additional lap or two around the object while taking more photographs from slightly 
different heights results in anywhere from 30 to 45 photographs of the object from every angle. (Pho-
tographs may be reviewed and those that are not desirable may be discarded.) Excellent scans can be 
made by hand-holding the phone camera and estimating equal distances around the circled object – a 
tripod and exact measurements are not necessary. One of the best ways to get good shots is to place 
the object in the center of a circular table so that the photographer is always the same distance from 
the object when moving around the circle. Once the photographs have been taken and accepted, the 
application runs an algorithm that creates a 3D-printable file – the process can take anywhere from a 
few minutes to overnight. Once completed, the file is ready to print. Success rates for this application 
are very good and the replications can be quite detailed and realistic. The biggest obstacle in scanning 
real physical objects, however, is not accuracy, but the ability to 3D-replicate the object to scale. Even 
with an expensive high-end printer, scanning something the size of a human skull could take 30 hours 
or more. Most libraries are not equipped to handle 24-hour printing, so many items must be replicated 
at a much smaller scale – even producing a replica human skull that is about 4 x 4 x 5 inches may take 
10 hours or so. Nevertheless, the ability to create replica physical objects can be tremendously useful 
to museums of all types. Original items that are fragile and cannot be handled can be replicated for use 
as durable and accurate teaching and learning tools. A good example is a human skull from Stetson’s 
Natural History Museum that was scanned using the AutoDesk 123D Catch® iPhone app and subse-
quently 3D printed (see Figure 9).
In a different twist on applying 3D printing technology, a Stetson senior biology student has devel-
oped a CAD-designed artificial flower that will be 3D printed and used in research on insects and how 
they relate to scent. Designing original files is yet another way to produce 3D-printable objects that 
makes the possibilities for research and learning projects virtually limitless. A computer science major 
is designing a workable drone in which the structure is made entirely of 3D-printed parts (Figure 10). 
During his senior research presentation, the student stressed that his project would not be possible had 
the library not invested in a high-end, reliable, and accurate 3D printer. Both the biology student and 
the computer science student have gained significant new computer design skills by working with the 
library’s 3D printers. They have not only learned about scale and precision, but have learned patience 
and diligence as their models go through many prototypes and design adjustments.
Mathematics represents another field in which 3D printing can be utilized to transform abstract con-
cepts into tactile forms. As far back as the ancient Greeks, mathematical philosophers have known that 
geometric shapes can be described using algebraic equations (e.g., the equation x2 + y2 = 1 describes a 
2D unit circle centered at the origin in the Cartesian plane). In principle, equations can be formulated 
that describe any type of geometric shape, including 3D shapes. As a demonstration of this fact, a Stetson 
student and faculty member recently collaborated to 3D print several mathematically inspired geomet-
ric forms, including an algebraic heart shape (Figure 11), a mathematical star fruit (Figure 12), and a 

323
Curricular Collaborations
﻿
Figure 9. A 3D graphical file of a human skull is generated using the AutoDesk 123D Catch® iPhone 
app. The file is subsequently edited and cropped using a free/open-source software called MeshLab 
before 3D printing.
Figure 10. Drone made from 3D printed components

324
Curricular Collaborations
﻿
Sierpiński fractal pyramid (Figure 13); the latter object is built from a repeated trigonal structure named 
after Polish mathematician Wacław Sierpiński who contributed to the understanding of mathematical 
fractal numbers. To 3D print a mathematical function, the computer software application Mathematica 
can be used to create a 3D surface plot from the function, which subsequently can be exported directly 
as an .stl file for 3D printing. The ability to take mathematically defined surfaces and easily generate 
a 3D printed model represents a big leap forward in the visualization of systems in physical science 
(chemistry and physics) and engineering, where multivariable algebraic equations are frequently used 
to describe and represent real world phenomena.
Students from other disciplines have engaged in innovative printing applications as well. Music 
students, for example, have printed working mouthpieces and replica instruments (Figure 14), while 
art students have engaged in numerous projects of their own design. Perhaps the most telling indicator 
of success is that the three printers are in almost constant use and booked weeks in advance. Given the 
enthusiasm of everyone who works with 3D printing, library administrators have found it relatively easy 
to find funding for the Innovation Laboratory and a fourth printer has been added to help meet demand.
Collaboration Challenges
Despite the success of the Stetson collaboration, barriers often exist that can make such technology 
partnerships difficult. Faculty members are busy with ever-increasing demands on their time. Unless 
librarians can convince their faculty colleagues that working with a particular new technology directly 
benefits them and their students, cooperation may not be readily forthcoming. Resistance to change, 
reluctance to make significant changes to long-standing courses, or fear of innovation failure may also 
lead to lack of enthusiasm from faculty. The library, too, will likely have some barriers to overcome in 
order to offer an innovative new technology. Cost is always a primary concern. Administrators must 
calculate up-front the costs-benefits involved in adding any new technology to their services. Librar-
Figure 11. An x-y-z mathematical equation for a heart (top) rendered as a 3D surface plot within Math-
ematica (bottom left) and subsequently 3D printed (bottom right)

325
Curricular Collaborations
﻿
ies have decades of experience in managing various traditional technologies, so the obvious issues of 
equipment failure and hardware/software maintenance will not be new. But issues relating to supporting 
a specific technology must be addressed before launch. Sometimes the reluctance to innovate does not 
come from the faculty or library administration, but from university administrators. Increased fiscal pres-
sures and tight budgets may make it harder to defend expenditures on experimentation and innovation. 
If a proposal is presented well to university administrators, however, with emphasis on how a particular 
innovative technology supports the library and academic missions, as well as how it can be collaborative 
with teaching faculty, the probability of support increases significantly.
Once you have identified an emerging technology that is a good match for your institution’s curricu-
lum, addressing the following questions positively should encourage a library to proceed:
Figure 12. A spherical coordinate mathematical equation for a star fruit (top) rendered as a 3D contour 
plot within Mathematica (bottom left) and subsequently 3D printed (bottom right). In this example, the 
raft support structure was left on the 3D printed object so that it would have a base.
Figure 13. 3D printed model of a Sierpiński 
fractal pyramid
Figure 14. 3D printed replica musical instruments

326
Curricular Collaborations
﻿
•	
Does this technology support the institutional and/or library missions?
•	
Does this technology have the potential to promote learning?
•	
Can we identify faculty and students who want to work with this technology?
•	
Can this technology add a new twist to something that is already being taught?
•	
Is there a demand for the skills related to this technology?
•	
Would our students benefit from learning this technology?
•	
Would our students find concepts in a particular discipline more accessible through this technology?
•	
Would student motivation/enthusiasm be positively impacted by learning this technology?
•	
Do we have the budget for up-front costs of hardware, software, and/or equipment?
•	
Do we have the ongoing funds to support the technology?
•	
Do we have the proper space (size, environmental, location) to offer this service?
•	
Can we support the level of mediation required to offer this service?
•	
Do we have the staff (in sufficient quantity and sufficiently skilled) and staff time to support this 
service?
•	
Do we see positive benefits for the library, librarians, faculty, and students if we implement this 
service?
In the case of 3D printing, some of these questions take on more importance than others. Print times, 
for example, can be lengthy – it is often the case that only one student per day can print on each printer. 
Any 3D printing initiative of any significance, therefore, is probably best supported by multiple printers 
(so careful consideration must be made to initial and ongoing costs). An additional question to ask is, 
“Can the anticipated demand for this service be reasonably met?” In one of the chemistry lab exercises 
outlined above, 12 students each needed to print two molecules. To complete that exercise, the high-end 
printer had to be reserved for that class for 12 days straight. Student enthusiasm was so high, however, 
that a donor agreed to fund a second $20,000 printer so that the exercise could be repeated with a larger 
class during the next semester. Staff support of 3D printing may also be much more labor-intensive than 
implementing some other learning technologies. As discussed, low-end printers fail at a notoriously high 
rate and require quite a bit of staff intervention. If a library is offering the service on a cost-recovery 
basis, failure rates complicate charging the end-user.
None of these issues, however, are insurmountable. Despite the challenges and barriers, both major 
and minor, libraries should not be dissuaded from offering new technology services if the up-front re-
search shows it to be a potentially good fit for both the library and those in the classroom. Librarians 
can overcome barriers by educating themselves on the positive outcomes for both the library and the 
faculty member (and by extension, of course, the students).
Collaboration Opportunities: What’s in It for Librarians
Librarians have long-standing relationships with faculty members – we provide information for their 
research, collaborate with them on developing the library collections in their fields, work with their 
students on assignments, go into their classrooms to offer research instruction, and often serve with 
them on various university committees. It is less common, however, to develop collaborative techno-
logical applications directly related to the curriculum as described above. Emerging technologies offer 
opportunities for both librarians and faculty members. But, as is the case in every collaborative effort, 
both parties must benefit from the arrangement. Before deciding to acquire and support an emerging 

327
Curricular Collaborations
﻿
technology, librarians must ask themselves, “What’s in it for library?” and, essentially, “What’s in it for 
me?” Libraries and librarians have many reasons for seeking out such collaborations. First and foremost, 
collaborating on emerging technology projects very likely supports both the institutional and library 
missions. As discussed earlier, the focus of higher education today is not only to give students a specific 
skill set in one discipline, but to ensure that students graduate with a holistic set of skills that will make 
them marketable in a competitive economy; the ability to understand and problem-solve using technol-
ogy is one of those skill sets that they should possess. Technologies will emerge, peak, and be replaced 
by something better, but students must be comfortable with adapting to rapid technological change. The 
more they work with emerging technologies, the more their comfort level will increase. Academic library 
missions have always revolved around supporting research and learning. Offering access to hands-on 
emerging technologies, especially in the form of collaborations with professors, is a new twist that is 
completely in sync with a mission that promotes learning.
In this age of rapid change, when some university administrators question the value of libraries, 
librarians must also be vigilant about marketing and promoting themselves and their libraries. It is im-
perative that libraries define new roles for themselves. Emerging technologies are exciting, they generate 
interest, and they promote library relevancy and visibility. As described above in the 3D printing case 
study, the Stetson library has garnered more attention for its innovative uses of emerging technologies 
than anything else it has done in recent memory. Not only has the project generated student and faculty 
interest, but it has captured the attention of donors who understand the need for students to have access to 
the most current technologies and to be developing applied technology skills. Major donors often come 
from the business community; emerging technologies with applications to business and industry make 
sense to them and can be a lucrative new avenue for library administrators in their fundraising efforts.
Collaboration Opportunities: What’s in It for Faculty?
Pressures on faculty have been mentioned above. If, indeed, faculty members feel the pressure to be 
innovative in their teaching and research, and if they feel the need to provide applied learning and re-
search opportunities for their students, then they may be likely to embrace the opportunity to work with 
an emerging technology funded by and housed in the library. Librarians can sell faculty members by 
answering the same “What’s in it for me?” question for faculty that they asked themselves. The oppor-
tunities are many: curricular innovation; experiential, hands-on learning; student research possibilities; 
interdisciplinary collaboration; cost-sharing; and potential publication and presentation opportunities. 
When faculty members realize that they would have a proactive library partner who will do much of the 
work of managing the technology, their enthusiasm may increase.
Be prepared to present specific potential opportunities to faculty members in a particular discipline. 
Do your homework and find out what’s already been done with the technology and what has yet to be 
explored. Know the curriculum of the discipline of the faculty member(s) with whom you want to col-
laborate. While you may not know the faculty member’s discipline well, librarians are expert researchers 
and can approach a faculty member with enough background information so as to not have the collabora-
tion idea immediately dismissed. Share success stories of what worked with other departments or at other 
institutions. Most importantly, let the faculty members know why the collaboration would be beneficial 
to both them and their students.

328
Curricular Collaborations
﻿
CONCLUSION
The Stetson Library experience left little doubt that the benefits of developing curricular/learning col-
laborations with faculty far outweigh the costs. Libraries can add technologies without these partnerships, 
of course, but they run the risk of being technology-rich and innovation-poor. Think back to Gorman’s 
third law - Use Technology Intelligently to Enhance Services. Rather than passively provide access to a 
technology with no input into how it might be used as a teaching and learning tool, libraries can take a 
proactive stance to ensure that technologies are used, in Gorman’s words, intelligently to build students’ 
problem-solving skill sets (Gorman, 1995, p. 785). In a university setting, librarian-faculty-student col-
laborations lead to learning opportunities that are likely not possible if the library goes it alone. Likewise, 
faculty and students benefit from technologies that are housed and supported by the library - technologies 
that an academic department alone might not be staffed or funded to support.
The Stetson University case study convinced librarians and university administrators that the li-
brary caught the 3D printing wave just in time to get ahead of the curve in developing some innovative 
pedagogical applications that have been well-received in both the library and chemistry communities. 
The library has run into surprisingly few pitfalls adding 3D printing into the services it offers. Little 
faculty resistance has been encountered to requests to collaborate on developing academic applications 
for the library’s 3D printers and scanners. The most pressing issue has become how to expand (both 
in square footage and equipment inventory) to meet the demands for 3D printing. Unlike other library 
service initiatives for which it is difficult to find funding, donors have been eager to help the Innovation 
Lab grow. The successful fundraising is attributed heavily to the librarians’ insistence that 3D printers 
should be used as a true learning technology and not as exciting new toys. Without cooperation from 
enthusiastic teaching faculty members, however, it would have been difficult to “sell” the printers as 
tools for teaching and learning.
That initial success has carried over into new emerging technology arenas. In 2014, the library acquired 
two pairs of Google Glass – the hype was huge and the Glass was expensive and hard to find which made 
them very appealing to both faculty and students. Instead of just housing them for checkout, however, 
the Glass was offered to faculty and students based on competitive research proposals. Students and 
faculty were asked, “What would you do if we gave you a $2,000 pair of Google Glass for a semester?” 
The only catch for the winners was the requirement to present the results of the project in some public 
forum – in other words, Glass recipients had to share what they learned. The proposals were innovative 
and exciting and both initial winning proposals have resulted in publications and/or presentations. Based 
on the experiences with 3D printing and Google Glass, the Stetson Library has committed to acquiring 
emerging technologies only if a learning component is attached.
How do you choose the right technology to make a difference? Institute for the Future Distinguished 
Fellow Bob Johansen has said, “When tracking technological innovation, pay attention not so much to 
what is new as to what is ready to take off…” (“Introduction,” 2011, para. 1). What’s at the beginning 
of the “hype cycle” now? Identify some emerging technologies that may be right for your library and 
then ask the questions outlined above to ensure that acquiring the technology is feasible. Not only can 
the library take a leading role in supporting learning technologies, but librarians may find themselves 
in a position to be true innovators.

329
Curricular Collaborations
﻿
REFERENCES
3Dprinting.com. (2015). What is 3D Printing? Retrieved April 5, 2015 from http://3dprinting.com/
what-is-3d-printing/
3DTectonix. (2015). 3D Printing Curriculum for Colleges and University Students. Retrieved April 
5, 2015 from http://3dtectonix.com/3D%20Printing%20Curriculum%20For%20College%20and%20
University%20Students
Abraham, M., Varghese, V., & Tang, H. (2010). Using molecular representations to aid student under-
standing of stereochemical concepts. Journal of Chemical Education, 87(12), 1425–1429. doi:10.1021/
ed100497f
Bengston, J., & Bunnett, B. (2012). Across the table: Competing perspectives for managing technology in 
a library setting. Journal of Library Administration, 52(8), 699–715. doi:10.1080/01930826.2012.746877
Carroll, F., & Blauch, D. (2015, March). Conformational analysis discovery activity using 3D potential 
energy surface models. Poster session presented at the National American Chemical Society Meeting, 
Denver, CO.
Considine, D., Horton, J., & Moorman, G. (2009). Teaching and reaching the millennial generation 
through media literacy. Journal of Adolescent & Adult Literacy, 52(6), 471–481. doi:10.1598/JAAL.52.6.2
Cram, D. J., Helgeson, R. C., Sousa, L. R., Timko, J. M., Newcomb, M., & Moreau, P. et al. (1975). 
Chiral recognition in complexation of guests by designed host molecules. Pure and Applied Chemistry, 
43(3-4), 327–349. doi:10.1351/pac197543030327
Diaz-Allen, C., & Sibbald, P. (2015). Using 3D printing to model steric interactions. Poster session 
presented at the National American Chemical Society Meeting, Denver, CO.
Gabel, D., & Sherwood, R. (1980). The effect of student manipulation of molecular models on chem-
istry achievement according to Piagetian level. Journal of Research in Science Teaching, 17(1), 75–81. 
doi:10.1002/tea.3660170112
Gartner. (2009). Hype cycle for emerging technologies, 2009. Retrieved April 15, 2015 from https://
www.gartner.com/doc/1085912?ref=ddisp
Gartner. (2014). Gartner’s 2014 hype cycle for emerging technologies maps the journey to digital busi-
ness. Retrieved April 15, 2015 from http://www.gartner.com/newsroom/id/2819918
Gonzalez, S.R. & Bennett, D.B. (2014). Planning and Implementing a 3D Printing Service in an Aca-
demic Library. Issues in Science and Technology Librarianship, 78. doi: 10.5062/F4M043CC
Gorman, M. (1995, September). Five new laws of librarianship. American Libraries, 784–785.
Griffey, J. (2012). Absolutely Fab-ulous. Library Technology Reports, 48, 21–24.
Gross, B. C., Erkal, J. L., Lockwood, S. Y., Chen, C., & Spence, D. M. (2014). Evaluation of 3D print-
ing and its potential impact on biotechnology and the chemical sciences. Analytical Chemistry, 86(7), 
3240–3252. doi:10.1021/ac403397r PMID:24432804

330
Curricular Collaborations
﻿
Hart Research Associates. (2015). Falling short? College learning and career success: selected find-
ings from online surveys of employers and college students conducted on behalf of the Association of 
American Colleges & Universities. Retrieved April 5, 2015 from https://www.aacu.org/sites/default/
files/files/LEAP/2015employerstudentsurvey.pdf
Introduction. (2011). The future of open fabrication. Retrieved April 15, 2015 from http://www.open-
fabrication.org/
Lolur, P., & Dawes, R. (2014). 3D printing of molecular potential energy surface models. Journal of 
Chemical Education, 91(8), 1181–1184. doi:10.1021/ed500199m
McCue, T. J. (2012). Additive manufacturing will change in the next 5-10 years. Retrieved April 15, 
2015 from http://www.forbes.com/sites/tjmccue/2012/05/02/additive-manufacturing-will-change-in-
the-next-5-10-years/
McMahon, M., & Pospisil, R. (2005). Laptops for a digital lifestyle: Millennial students and wireless 
mobile technologies. Proceedings of the Australasian Society for Computers in Learning in Tertiary 
Education, (pp. 421-443).
Merriam-Webster Dictionary. (2015). Innovation. Retrieved April 5, 2015 from http://www.merriam-
webster.com/dictionary/innovation
Murphy, M. K. (2010). An electromagnetic spectrum for millennial students: Teaching light, color, energy, 
and frequency using the electronic devices of our time. Journal of College Science Teaching, 39, 52–58.
News from the field: University of Nevada-Reno library offers 3-D printing. (2012). College & Research 
Library News, 73, 455.
Norton, H. F., & Gonzalez, S. R. (2014). Building for innovation with library-hosted 3D-printing and 
scanning. Journal of the Medical Library Association: JMLA, 102, 236–237.
Prensky, R. R. (2010). Teaching Digital Natives: Partnering for Real Learning. Newbury Park, CA: 
Corwin.
Pryor, S. (2014). Implementing a 3D printing service in an academic library. Journal of Library Admin-
istration, 54(1), 1–10. doi:10.1080/01930826.2014.893110
Ranganathan, S. R. (1963). The Five Laws of Library Science. New York: Asia Publishing House.
Rowe, A. Z. (2014). Demand for 3D printing skills soars. Retrieved April 15, 2015 from https://www.
wantedanalytics.com/analysis/posts/demand-for-3d-printing-skills-soars
Rutkowski, J. V., & Levin, B. C. (1986). Acrylonitrile-butadiene-styrene copolymers (ABS): Pyroly-
sis and combustion products and their toxicity-a review of the literature. Fire and Materials, 10(3-4), 
93–105. doi:10.1002/fam.810100303
Scalfani, V.F. & Sahib, J. (2013). A model for managing 3D printing services in academic libraries. 
Issues in Science and Technology Librarianship, 72. doi: 10.5062/F4XS5SB9
Scalfani, V. F., & Vaid, T. P. (2014). 3D printed molecules and extended solid models for teaching sym-
metry and point groups. Journal of Chemical Education, 91(8), 1174–1180. doi:10.1021/ed400887t

331
Curricular Collaborations
﻿
Schaper, M. M., Thompson, R. D., & Detwiler-Okabayashi, K. A. (1994). Respiratory responses of mice ex-
posed to thermal decomposition products from polymers heated at above workplace processing temperatures. 
American Industrial Hygiene Association Journal, 55(10), 924–934. doi:10.1080/15428119491018420 
PMID:7977032
Small, M.Y. & Morton, M.E. (1983, September/October). Spatial visualization training improves per-
formance in organic chemistry. Research in College Science Teaching, 41–43.
Stephens, B., Parham, A., El Orch, Z., & Ramos, T. (2013). Ultrafine particle emissions from desktop 
3D printers. Atmospheric Environment, 79, 334–339. doi:10.1016/j.atmosenv.2013.06.050
Striplin, D., Carroll, F., & Blauch, D. (2015, March). Discovering pressure-volume-temperature phase 
relationships with 3D models. Poster session presented at the National American Chemical Society 
Meeting, Denver, CO.
Talley, L. (1973). The use of three-dimensional visualization as a moderator in the higher cognitive 
learning of concepts in college level chemistry. Journal of Research in Science Teaching, 10(3), 263–269. 
doi:10.1002/tea.3660100311
Violante, L. E. H., Nunez, D. A., Ryan, S. M., & Grubbs, W. T. (2015). 3D printing in the chemistry 
curriculum: Inspiring millennial students to be creative innovators. In G. Potts & C. Dockery (Eds.), 
Addressing the millennial student: New pedagogy and approaches to improve student learning outcomes 
in undergraduate chemistry (pp. 125–146). Washington, DC: American Chemical Society.
Zitting, A., & Savolainen, H. (1980). Effects of single and repeated exposures to thermo-oxidative deg-
radation products of poly(acrylonitrile-butadiene-styrene) (ABS) on rat lung, liver, kidney, and brain. 
Archives of Toxicology, 46(3-4), 295–304. doi:10.1007/BF00310447 PMID:7236007
KEY TERMS AND DEFINITIONS
3D Printing: The process of making three-dimensional objects from a digital file.
3D Scanning: The use of an algorithm (run through either a 3D physical scanner or a 3D scanning 
application) to create a 3D-printable digital file from multiple photographs of an object.
Acrylonitrile Butadiene Styrene (ABS): A fairly strong plastic often used as filament in 3D print-
ers to create 3D objects.
Additive Manufacturing: Umbrella term for processes, including 3D printing, that lay down hundreds 
or thousands of layers of various materials to create a physical object from a digital file.
Filament: Material (often plastic or metal) used in 3D printers to create 3D objects.
Fused Deposition Modeling (FDM): A term trademarked by Stratasys® to describe the 3D printing 
process of feeding some type of filament (plastic, for example) through a heated extruder nozzle that 
will harden into a 3D object on a build plate.
Fused Filament Fabrication (FFF): A non-trademarked term to describe the 3D printing process of 
feeding some type of filament (plastic, for example) through a heated extruder nozzle that will harden 
into a 3D object on a build plate.

332
Curricular Collaborations
﻿
Polylactic Acid (PLA): A biodegradable plastic often used as filament in 3D printers to create 3D 
objects.
ENDNOTES
1 	
According to the 2013-2018 Cisco Visual Networking Index forecast, global Wi-Fi and mobile 
wireless traffic has dramatically expanded over the last few years, is on track to exceed hardwired 
traffic by early 2015, and will account for 61% of total Internet protocol (IP) traffic by 2018. 
Retrieved April 1, 2015 from http://www.cisco.com/c/en/us/solutions/collateral/service-provider/
ip-ngn-ip-next-generation-network/white_paper_c11-481360.html
2 	
Stetson University is a private comprehensive liberal arts university in the central Florida city 
of DeLand. Eight librarians, two instructional technologists, and 11 support staff serve a student 
population of approximately 3,200 primarily undergraduate students and 300 faculty.
3 	
The Library MakerSpace Listserv (LIBRARYMAKERSPACE-L@LISTS.UFL.EDU) is a good 
place to start with questions about libraries experiences with all aspects of offering 3D printing 
services; discussion is ongoing on the potential health hazards of ABS fumes and possible solu-
tions.
4 	
Thingiverse (http://www.thingiverse.com/); Cubify (http://cubify.com/); AutoDesk 123D® (http://
www.123dapp.com/Gallery/catch/content/all); Pinshape (https://pinshape.com/); NIH 3D Print 
Exchange (http://3dprint.nih.gov/).
5 	
.stl (Standard Tesellation Language); .vrml (Virtual Reality Modeling Language); .x3d (Extensible 
3D); .obj (Object file); .iges (Initial Graphics Exchange Specification); .dae (Collaborative Design 
Activity – COLLADA).
This work was previously published in Technology-Centered Academic Library Partnerships and Collaborations edited by Brian 
Doherty, pages 89-125, copyright year 2016 by Information Science Reference (an imprint of IGI Global).

333
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  18
DOI: 10.4018/978-1-5225-1677-4.ch018
ABSTRACT
The chapter presents a reflection on the concept of transparency in digital modeling and visualization of 
Architectural Heritage. Moving from topics of transparency and from the experiences in using paradata 
in different fields to state model’s source, the degree of reliability of virtual re-constructions, and to made 
the digital model testable by other professionals, transparency and paradata are studied and declined 
for a dedicated application to historical buildings. In fact paradata is useful for model’s design, use, 
management, diffusion, archiving, and interoperability. This according to an aim of model’s intellectual 
transparency, and scientific computing and visualization of historic buildings. Follows issues about: the 
relationship between physical and digital heritage, the design of the digital 3D model and the database, 
the communication of transparency through spatial visualizations and multiple windowed representa-
tions, the transparency as possible methodological workflow for scientific analysis.
INTRODUCTION
Etymologically, transparency derives from the Latin “Trans” & “Pareo”, i.e. being able to show what 
lies behind. This arises two questions: What there is behind (the reality, the scholar, the digital system, 
etc.) and how an interpretative model can be transparent.
In the Digital Age, the ICT growth, the decreasing cost and the ease of use of digital tools have made 
the most advanced technologies available to a large number of users, thus favoring their practice and 
experimentation. Digital representations and information systems have become powerful tools for the 
study and communication of tangible heritage, in particular allowing the interrelation of information, 
the scientific representation of research’s hypothesis, the simulation of complex systems. At the same 
The Importance of 
Being Honest:
Issues of Transparency in Digital 
Visualization of Architectural Heritage
Stefano Brusaporci
L’Aquila University, Italy

334
The Importance of Being Honest
﻿
time, the multiplication of data and the diffusion of participatory cultural approaches raised questions 
of analysis, computing, sharing, and management of data and information. Follows issues on metadata 
and paradata.
In particular – considering the fields that deal with tangible heritage – archaeology first looked 
to computer based visualization as scientific methodology and, thus, posed questions of philological 
interpretation of digital reconstructions of the past. It could be a reference for virtual reconstruction of 
buildings that no longer exist or have been only designed: All these studies are based on the critique 
interpretation of heterogeneous archival materials (old drawings, historical photographs, descriptions, 
yard or competition documents, etc.). Similarly occurs in the definition of past configurations of exist-
ing buildings where, however, the archival apparatus have to be reflected on the reality, i.e. the physical 
architectural document.
But the study, surveying, modeling, and visualization of architectural heritage highlight new issues 
that are related to the whole process of surveying and critical historical analysis, to the visualization 
of past configurations, to the re-construction of constructive systems, also with the aim to declare the 
degree of reliability. Transparency allows the scholars to retrace critical decisions and test conclusions 
on colleagues’ work of and on own research.
LOOKING THROUGH: THE BACKGROUND
Why Transparency?
The title of the well know book “The Transparent Society” (Vattimo, 1989) evokes the critical topic of 
“interpretation”, typical of the postmodern culture, where a leading role is played by technology and 
media. Day by day digital tools have changed and are still changing media, according to a dimension of 
pervasive and continuous interrelation between reality and “digitality” (wording coined by Negroponte 
in 1995). In our current “on line” life (The Onlife Manifesto, 2015), the concepts of “digital natives” and 
“digital immigrants” (Prensky, 2001) blurs (Jenkins, 2007). “Hyperreality” (Baudrillard, 1976) has grown, 
becoming simply a component of reality, and people have become shrewder in their relation with the sort 
of state of “augmented reality” that involves us. The relationship with technology renews: Its outcomes 
are constitutive of reality and of our culture, and therefore they requires knowledge, understanding, and 
assessment. The claim for digital heritage preservation (UNESCO, 2003), and the statement of digital 
heritage as common heritage ratifies the value and cultural importance of this new kind of artifacts, and 
consequently their significance in our post-postmodern condition.
The philosophical line of new realism (Ferraris, 2012) is not unrelated to this context: New realism 
roots on postmodern lesson and hermeneutics, but at the same time reckon with reality and perception; 
the characteristic of perception of being “opaque”, requires the need to be represented, i.e. interpreted.
In this context the claim for transparency follows. And a request for transparency underlies also the 
“European Charter of Rights of Citizens in the Knowledge Society” (aka “The Charter of eRights”) of 
2005.

335
The Importance of Being Honest
﻿
Metadata and Paradata
With the sprawl of digital tools and applications, metadata and paradata have become very important issues 
in model’s design, use, management, diffusion, archiving. Last but not least for model’s transparency.
As well known, from a literal point of view, “metadata” means “data about data”. We can say that 
metadata are data that gives meaning to the raw content of data. «Until the mid-1990s, metadata was a 
term used primarily by communities involved with the management and interoperability of geospatial 
data and with data management and systems design and maintenance in general. […] Perhaps a more 
useful, “big picture” way of thinking about metadata is as the sum total of what one can say about any 
information object at any level of aggregation. In this context, an information object is anything that can 
be addressed and manipulated as a discrete entity by a human being or an information system. The object 
may comprise a single item, it may be an aggregate of many items, or it may be the entire database or 
record-keeping system» (Gilliland, 2008, 1-2). In short we could say that a metadata is useful to describe, 
manage, preserve, and use data, its content, its context, and its structure. There is a lot of interest in the 
topic of metadata for cultural heritage, and many are metadata schemas: Dublin Core Metadata Element 
Set (DCMES), Metadata Object Description Schema (MODS), etc. (Gul, Tramboo, Ahangar, 2015).
Related to metadata is the concept of paradata that can be generically referred to the process by which 
the survey data are collected – for example like in a statistic survey – (Kreuter, 2013).
In particular, paradata is a kind of metadata focused on the use of data, «but the big difference to 
keep in mind is that paradata involves “action” and traditional metadata involves “classification”» (US 
Department of Education, 2011, 1). Therefore, for example, metadata of a book describes the title, the 
author, the language and so on; while paradata indicates how people used the book and its sources. Thus 
paradata records: The “actor” (and its description) that use metadata; The “action” (and its description 
and “measure”) that the actor does; The “object” (and its description) that is acted. It’s evident how 
paradata is very important in open-ended transformation of data.
In 2010, The National Science Digital Library (NSDL), developing an initiative called “STEM 
Exchange” on Learning Objects, has given an interesting operational definition of paradata: «[…] we 
needed to distinguish between traditional, relatively static metadata that describes a digital learning object 
and the dynamic information about digital learning objects that is generated as they are used, reused, 
adapted, contextualized, favorited, tweeted, retweeted, shared, and all the other social media style ways 
in which educational users interact with resources. In this context, paradata captures the user activity 
related to the resource that helps to elucidate its potential educational utility. We first presented paradata 
at a March 3, 2010 presentation as:
Paradata: - a complement to metadata, not a replacement; - separate layer of information from 
metadata; - a means to automate information generation about resource use by using social networking 
tools; - a means to create an open source and open access data space around resources; - emphasizes 
dissemination rather than description; - accommodates expert and user-generated knowledge.
Paradata opens opportunities to: - visualize a resource at the center of its own social-style network 
of activity; - explicates usage patterns and inferred utility of resources; - map network connections as 
teacher communities add multidirectional flows of information; - create persistent, short references to 
resources; - aggregate activity around the resource through user-friendly practices, such as hashtagging; 
- power feedback loops with teacher communities» (https://wiki.ucar.edu/display/nsdldocs/Paradata ac-
cessed on June, 15 2015). In this way paradata is not only about the use of metadata, but also describes 

336
The Importance of Being Honest
﻿
the transformation of data during their use and “inter-use” in participatory systems. At the same time 
paradata could be a useful way to portray the whole cultural context.
The Archaeological Legacy
Pointing out the attention on the archaeological field – interested in digital heritage derived from 
physical contents – from the very beginning of the Seventies, it has immediately looked carefully to the 
possibilities offered by computer modeling: It is a research path that, beginning from the purpose of 
digital representations as an effective tool for three-dimensional representation (Forte & Siliotti, 1996), 
developed towards the use of computer graphics for scientific research (Frischer, 2008).
In the book “The Virtual Representation of the Past” (Greengrass & Hughes, 2008), transparency is 
one of the fundamental issues, and in the different chapters the problem related to the representation of 
“when”, “where”, and “what” recurs. In particular Beacham (2008) focuses on transparency and para-
data, as crucial topic “if 3D visualization is to ‘mature’ as a research method and acquire widespread 
acceptance within subject communities” (p. 172).
Of particular interest are the “Guides to Good Practice” by Archaeology Data Service and Digital 
Antiquity (last version published in 2013), that highlights main contents in archiving digital works on 
archeology.
From these research lines grows a methodological reflection, in particular focused on the philological 
transparency of digital visualization (Huvila, 2013).
On ‘transparency’, cornerstones are The London Charter and the Principles of Seville, that deal 
with issues related to “Research Sources”, “Documentation”, “Authenticity”, and “Historical Rigor” 
of computer-based visualization of cultural and archaeological heritage. And they find in paradata an 
important tool to reach transparency.
The London Charter (2009) defines paradata for cultural heritage as: «Information about human 
processes of understanding and interpretation of data objects. Examples of paradata include descriptions 
stored within a structured database of how evidence was used to interpret an artefact, or a comment on 
methodological premises within a research publication. It is closely related, but somewhat different in 
emphasis, to “contextual metadata”, which tend to communicate interpretations of an artefact or col-
lection, rather than the process through which one or more artefacts were processed or interpreted» (p. 
13). Paradata configures as a sort of metadata useful to the philological reconstruction of the modeling 
process (Denard, 2012). In this way The London Charter defines “Intellectual transparency” as: «The 
provision of information, presented in any medium or format, to allow users to understand the nature and 
scope of “knowledge claim” made by a computer-based visualization outcome» (p.12). The concept of 
paradata accompanies the one of “scientific transparency”, following the ambition to shun the seduction 
of some photorealistic computer based visualizations, and to declare the scientific fundaments, reasons, 
and reflections from which the digital representation derives.
The Principles of Seville (2012) say: «All computer-based visualization must be essentially trans-
parent, i.e. testable by other researchers or professionals, since the validity, and therefore the scope, of 
the conclusions produced by such visualization will depend largely on the ability of others to confirm 
or refute the results obtained» (p.8). The principle of transparency has been widely developed in the 
archaeological field, where the digital reconstruction of artifacts (in their ancient configuration) is largely 
based on indirect information, comparative analysis, interpretative hypothesis.

337
The Importance of Being Honest
﻿
In 2012 a publication titled “Paradata and Trasparency in Virtual Heritage” renovates the debate on 
transparency (Bentkowska-Kafel, Denard, Baker 2012). This is necessary requisite because: «Digital 
technologies offer flexible analytical tools, both sensory and semantic, for the study and representation 
of the past, but the digital techniques – it is argued here – are only useful and valid if interpretative 
frameworks and processes are published», therefore the need to «emphasize the importance of reliably, 
documenting the process of interpretation of historical materials and hypotheses that arise in the course of 
the research». From here descend the use of paradata: «[…] the term borrowed from other disciplines that 
rely on recording information processes. Paradata document the process of interpretation so that the aims, 
contexts and reliability of visualization methods and their outcomes can be properly understood» (p.1).
And «Paradata may be seen as a digital equivalent to “scholia”, as well as an addition to the traditional 
critical apparatus for describing the process of reasoning in scholarly research» (p.2).
In this way paradata is a sort of “comments” and “explanations” that refer the choices made in visu-
alizing raw-data (i.e. the “documents”) and describe the critical interpretations. According to this use, 
we remember the archaeologist’s’ consciousness that “The Past is a Foreign Country” – citing the title 
of a famous book of Lowenthal (1985) –. Therefore «Virtual reconstruction allows comparison between 
“what is possible” according to the reconstructed model and the “facts”. A possible discrepancy requires 
re-evaluation both of the model and the archeological data» (Hermon, 2012, p.19).
Therefore the need to define the degree of reliability of the reconstructions becomes very important. 
For example Hermon & Nicolucci (2006) propose to assign a numeric value of reliability to archaeo-
logical reconstruction models, defined on fuzzy logic. The aim is to allow a transparent analysis of the 
visualization and a scientific comparison between different hypothesis. But this methodology doesn’t 
seem so easy to be used because practically the circumstances are very complex and there are too many 
degrees of uncertainty that cannot be easily solved with an equation. Nicolucci (2012) suggests the 
representing of the modeling process as a decision tree, a graph where «terminal nodes represent final 
(alternative) stages, that is alternative reconstructions» (p.30).
Experiences in Architectural Heritage Visualization
Historical buildings present peculiarities compared to other kinds of built heritage. They are the synthesis 
of materials, surfaces, aesthetic characteristics, constructive systems, spaces, old and current uses. The 
building system plays a static rule and it is the witnesses of architectural and constructive cultures oc-
curred in the past. Last but not least the spatial context – and how it has been modified during centuries 
– is important, because it is an essential relational element of each specific building.
Scholars of architecture have different kinds of archival documents, ancient and recent: Graphical and 
textual information on the yard, the building, and its transformations, on the different players (design-
ers, builders, contractors, etc.); bibliographies and biographies; various kinds of drawings, as previous 
surveys, paintings, etc.; photographs; surveying data (preliminary surveying sketches, field notebooks, 
measures, point clouds, etc.); 2D and – recently – 3D interpretative models; comparative, metrological, 
proportional, historical, structural, energetic analysis, and on degradation and materials; reports, calcula-
tions, and project elaborations. Moreover the architectural analysis requires the study of other works of 
the same author and of the same workers, and of buildings of the same age and of the same architectural 
culture. It requires also the architectural theories of the past – preserved by essays, treatises and manuals 
–, and recent and past critical studies (Brusaporci, 2015a).

338
The Importance of Being Honest
﻿
Follows that the act of representation is an interpretative action, influenced by: measurements, tools, 
procedures, and methodologies; historical and present documents; the own characteristics of the heritage; 
the aims of the visualization; the culture and study of surveyors and modelers (Docci & Maestri, 2009). 
Therefore representation has to consider building’s surfaces and metric aspects, but inevitably it must 
also include understanding and communication of historical and aesthetic characteristics, interpreted in 
the broadest sense of the terms (Fig. 1).
Every representation – both of the present building and of past or non realized configurations – re-
quires an interpretative model, and consequently issues of transparency derive. Also the surveying and 
understanding of the building’s system need a critical reflection, because usually it is not directly and 
completely analyzable in ancient architectures (Fig. 2).
Recently computer based representation and visual computing are growing in the architectural field, 
and studies involving transparency, temporality and reliability are ever more diffused. These allow 
representing the confidence level of geometric 3D modeling – especially for ancient temporal states – 
and the type of uncertainly, in particular related to surveying data, archival documents, and scholar’s 
interpretations.
Figure 1. Palace of Margaret of Austria in L’Aquila; Aerial view and virtual reconstruction of the build-
ing and of the context at the XVIII century
Figure 2. Reconstruction of Palace of Margaret of Austria in L’Aquila at the XVIII century. The different 
levels of detail between the palace and the urban context.

339
The Importance of Being Honest
﻿
Recalling archaeological studies, Borra (2004) reflects on the “Truth of the 3D model”; he presents 
a way to communicate the degree of coherence of virtual analstylosis and suggesting how the 3D model 
could be not only a visualizing tool but also an interface to communicate the work methodology. He 
highlights how in a critical reconstruction the distinction between true and false is often “liquid”, and he 
develops the idea that the “virtual model” is a synonymous of “reconstructive hypothesis” and therefore 
it requires to be validated. This kind of model configures as medium of data and metadata and become a 
database of data, information, alternative hypothesis, procedures and methodologies. Borra suggests four 
levels of coherence of the model’s shape: The reliability according to “objective” data (also derived from 
documental information); The reliability according constructive analysis; The coherence to architectural 
styles; Interpretative hypothesis. Another question is about equivalent hypothesis that can be visualized 
to compare them, and are useful to underline how the 3D model is an interpretative representation.
Other experiences in investigating the issues of transparency follows, ever related to model’s semanti-
zation. Semantization allows the management of data, metadata, and paradata, different representations, 
and modeling. In fact it is very important to built, manage and visualize complex models with architectural 
configurations changing over time, avoiding “static” and non-critical renderings.
Stefani, De Luca, Véron and Florenzano (2010) present a modeling approach of buildings repre-
sentation focused on issues of visualizing transformations and related temporal indetermination. It’s a 
scientific methodology based on the relation between historic graphs – useful to a conceptual definition 
of transformations – and 3D models. The semantization of the 3D model favors the spatial structuring 
of the buildings and implementation. Important is the concept of “granularity” that, with its variation, 
allow the representation of spatial and temporal uncertainly. The system interface is based on simultane-
ous windows with 3D views, graphs and datasets.
De Luca, Busayarat, Stefani, Véron and Florenzano (2011) propose a platform for architectural heri-
tage analysis, made by the association of multiple visualization and information through the semantic 
description of the model. In particular geometric entities have attributes concerning the type of uncer-
tainly: Spatial uncertainly (shape or position) and Temporal uncertainly (dating). They are declared by 
a web application and visualized using color tones. About building transformations, authors describe 
changes with graphic notations (creation, destruction, alteration, union, division, reconstruction), related 
to historical visualizations. About geometric restitutions, four degree levels of reconstruction are defined:
1. 	
Based on surveying data;
2. 	
Based on images in conic or cylindrical projection;
3. 	
Based on images in pseudo-perspective, pseudo-axonometry or sketches;
4. 	
Without iconographic support or survey data.
It’s interesting the fact that in this paper the uncertainly level is not related to the quality of the inter-
pretative hypothesis (because it is assured by the critical work of qualified scholars) but it is characterized 
by the kind of source. This approach is different from the one usually used in archaeology, and made 
possible by the fact that buildings have many archival documents.
Apollonio, Gaiani and Sun (2013), developing the theme of “uncertainly” of 3D models of non-
more existing or never realized building, moves from archival data and 2D drawings «implemented 
with the corresponding text metadata (catalog, commentary and bibliography) that allow transparency 
of information to user, in a manner similar to what happens with notes, comments, and the bibliography 
in traditional publishing.» But they add: «A second fundamental requirement is that 3D models will be 

340
The Importance of Being Honest
﻿
demonstrative of the solutions adopted to meet the uncertainties and the lacks» (p.863). That is authors 
focuses on the necessity of the parallel use of two different ways to communicate transparency: The 
textual one and the visual one. This consideration is very interesting from an operational point of view. 
Then they define a uncertainty gradient color code, referred to the kind of information source: Original 
drawing; Original sketch; Coeval design reference; Architect’s treaties or books; Treaties references; 
Architectural styles; Construction system; Failing references. It’s evident that this approach is related to 
the characteristics of the case study: i.e. only planned villas, not existing.
The previous work is a reference for Apollonio and Giovannini (Apollonio & Giovannini, 2015), but 
they study different kinds of architectural heritage. They present a methodological approach to display 
data-processing, aiming to validate 3D modeling practice, to facilitate the exchange of information 
and collaboration. In particular it favors the visualization of the uncertainty level of the reconstruction 
process. Recalling the concept of paradata, as presented in The London Charter and in The Seville 
Principles, in this work reliability is defined through a gradient color scale related to the different kind 
of sources: Laser scanning survey; Original drawings; Original sketches and uncomplete survey draw-
ings; Coeval design reference (same architect and way of representation); Data deducted from previous 
levels; Failing references. Authors decide to privilege not textual paradata but visual communication to 
document paradata management: They define a “conceptual model” to «create a simplified representa-
tion of relationship between references and virtual reconstruction of architectural elements at different 
Level of Detail» (p.11). And this interesting idea to refer to LODs is based on the conceptual organiza-
tion of architectural orders and its parts. Therefore the architecture of the work is strictly related to the 
semantization of the model (about these issues see also Apollonio, 2012).
Computer based visualization is ever more diffused also in the urban history field. Often GIS (and 
HGIS) is used to visualize town models and reconstructions, and to record, organize, manage, compare, 
and analyze current and historical data (Ferrighi, 2015).
Geremia (2014) presents some virtual reconstructions of urban environments of Rome at different 
ages. She declares the level of authenticity of 3D models of old buildings with different colors: Complete 
(reconstruction of elevations from archival drawings and historical photos; Partial (reconstruction from 
archival planimetries and current elevations); Typological (reconstruction according to a typological 
analysis). It’s interesting the last one, that suggest a useful (but when declared) approach to “fill” lacunas 
in virtual reconstruction.
The collaboration between architects and archeologists can produce interesting results, as showed by 
Ippolito who points out the importance of an all-comprehensive multi-scalar approach (Ippolito, 2015). 
About issues of transparency, he highlights that «one has to tackle them form the cultural point of view 
rather than for the technical one in such a way that operations and choices undertaken with the view to 
achieving the level of knowledge – more profound and structured – of the object of study will always be 
guided by scientific rigor» (p.73). According to principles of scientific rigor and reliability, in this work 
three kinds of elements are distinguished: certain (remains of the archaeological site), ie extracted from 
surveying; derived from prior reproductions (with probable errors or misinterpretations); deduced. A 
database records the whole processes, methodologies, data, etc.
Very often is difficult to separate archaeological heritage from architectural one and, moreover, meth-
odologies of different fields can be usefully interrelated. In particular digital tools create a sort of digital 
“agora” (Perry, 2003) here different scholars meet and collaborate, sharing experiences, methodologies, 
instruments, approaches, and field of researches.

341
The Importance of Being Honest
﻿
THE DIGITAL GLASS HOUSE
On the Nature of the Architectural Heritage
The study of architecture requires the analysis of spaces, surfaces, volumes, materials, constructive 
systems, physical and historical contexts, last but not least archival and bibliographical documents.
An historic building is the synthesis of transformation and modification processes occurred over 
time, expression of architectural cultures and interventions that occurred since its construction to the 
present day. The fact is that an architectural heritage bases its ontology precisely on the fact of being 
equipped with a story. At the same time its aesthetic, historical and functional characteristics are made 
immanent by the constructive system, i.e. its materiality. Materials and constructive systems play an 
important role, as physical reason of existence of the artifact, and as witnesses of the architectural and 
constructive cultures occurred in the past.
In this way each building has a own quality of “individual”, and for this reason it is often used the 
term “handmade” (or “artifact”), to highlight how a historic building can be seen as a product of human 
activities.
Therefore a digital representation cannot be limited to architectural surfaces or to problems of metric 
accuracy – as far as necessary –, but inevitably it has to include issues of understanding, representation 
and communication of historical, aesthetic, and architectural characteristics.
An Architectural Heritage is a sort of “living being”, every day used and experienced by people, 
that – using and experiencing it – modifies it. This concept is true not only considering its internal space 
and own physicality but also in relation to the whole urban and social context. As already recommended 
from the first “Charter of Restoration” of 1931, the use of the monument assures its conservation and 
safeguard, avoiding its abandonment. And, at the same time, it suggests regular and permanent mainte-
nances, instead of invasive interventions. Brandi (1963) points out that Restoration is the methodological 
moment of the acknowledgment of the artifact as work of art, in its physical, historical, and aesthetical 
consistence, with the aim of the conservation of its values. In this way knowledge is the first act of res-
toration and every project has to root on this knowledge. Follows a strict relationship between surveying 
representation and project design in architectural restoration: The first one has to be the basis to develop 
the restoration solutions, according to a specularity between the restoration model and the design model 
as an invariant (Centofanti & Brusaporci, 2012).
Unlike in the design of a new building, during restoration or maintenance, new findings should re-
quire changes of the project. The lifecycle of the building cannot be completely planned, and it requires 
a different approach, because the architectural heritage and its values live before the project, the tools, 
and the technologies.
The ever changing state of the architectural heritage – influenced by the passage of time, by its use, 
by the changes of its urban context, by restoration projects, by maintenances, etc. – configures a dynamic 
nature. It requires open-ended interpretative model able to witness phases, analysis, projects, and changes.
The “critical role” of the scholar is essential: Every model of an architectural heritage moves from a 
critical interpretative act, and the model origins from a comprehension of its values and then from a wise 
visual documentation. It requires the understanding of the cultural and constructive characteristics and 
context of the building, and a correct project of the research. We could say that an interpretative model 
born from a careful and constant meeting between the building and the surveyor. Consequently the con-
tents of the model are influenced not only by tools and procedures, but also by: Building characteristics 

342
The Importance of Being Honest
﻿
(for example a ruin of classical age, a medieval castle, a renaissance palace, a baroque church); Specific 
representation aims (such as geometrical-dimensional surveying, degradation analysis, communication 
addressed to non-expert users, etc.); Level of definition (scale / levels of details). In this way a model 
represent a critical interpretation of the scholar(s) (Docci & Maestri, 2009).
About this critical item, it is particularly important to focus on the historical dimension of the archi-
tectural heritage, because an heritage roots its ontology specifically on being historical. Therefore its 
study, and then its computing and visualization, cannot elude the study of its history. Spagnesi (1984) 
starts from a disciplinary reflection on the study of history in the architectural field: «we must reaffirm 
the autonomy of architectural history […] especially the identification of a own field of investigation, 
studied with own tools, that cannot be confused with ones of other historical disciplines. […] If history 
is always equivalent to knowledge, the History of Architecture has to be the knowledge of the physical 
built space made by men, i.e. the current reality» (p.7) 1. Compared to archaeology, the viewpoint is 
reversed: The aim is not the re-construction of no more existing artifacts, but the analysis of the temporal 
sequence of events that led to the current state. And Spagnesi highlights: «only the “current reality” can 
be known, the contemporary one, through the analysis of the causes that determined it» (p.7). Spagnesi 
concludes: «The knowledge of reality is the critical synthesis of the judgments of values referred to the 
singles historical moments» that conduced to the current situation (p.10). That is the study of the past 
configurations aims to the understanding of the values of the current artifact. The re-discovering of 
values no more existing is important for the knowledge of the existing building. Conceptually the issues 
in visualizing transparency of ancient configurations are similar to the ones in the archaeological field. 
But also when we deal with the re-construction of non realized projects, no more existing building, or 
ancient configurations, there are some differences with archeology: architects have not only findings or 
narrative documents, but also other kinds of documents: Drawings, paintings, projects, reports, images, 
etc. Therefore in architecture the study of history is a path toward the present, and therefore to the proj-
ect: A project of restoration that involves the “image” of the building, its systems, its use, etc (Figure 3).
Figure 3. Marine Hospice in Giulianova. Computer-based reconstructions of heritage transformations
(1896, 1914, 1922, 1949)

343
The Importance of Being Honest
﻿
The seduction of amazing computer generated images finished, the glance of the user is more cynic, 
scholars and designers are ever and ever wiser. Therefore the quest for transparency – endogenous in 
our culture and technology – becomes increasingly important also in the architectural field, where the 
computer and digital technologies, associated with a higher diffusion and development of Information 
and Communication Technologies, have led to profound changes in the field of architectural survey and 
representation, and the use of representative digital 3D models and informative systems have acquired 
an inescapable role (Fig. 4).
Physical Born DH
In 1999, on UNESCO’s “World Heritage Magazine”, Stone defined Virtual Heritage as: «the utilization 
of technology for interpretation, conservation and preservation of Natural, Cultural and World Heritage» 
(Stone 1999). The “Charter on the Preservation of the Digital Heritage” published by UNESCO in 2003, 
defines the “Digital Heritage” as “Common Heritage”, made by: «cultural, educational, scientific and 
administrative resources, as well as technical, medical and other kinds of information created digitally, 
or converted into digital form from existing analogue resources». It includes different kinds of products 
such as texts, databases, images, audio, graphies, software and web pages». In particular the Charter 
claims for maintenance, management, access, and guarding against loss of digital heritage, implying a 
deep reflection on measures and responsibilities.
Consequently to the diffusion of digital visualization, derives the interest in this subject by many 
disciplines. Humanities – in particular ones involved in museography (Parry, 2010. Cameron & Kend-
erdine, 2010) –, archaeologists (Frischer, 2008), and architects (Docci, 2005. Gaiani, 2006) have began 
to work and reflect on digital heritage derived from tangible artifacts.
Reflecting on digital heritage, Fiona Cameron (2010) points out the different nature between digital 
born contents and ones derived from physical references. The surveyor chooses what and how to digitize, 
model, and visualize. Therefore she takes an active role in decoding and interpreting the work, according 
Figure 4. Marine Hospice in Giulianova. Visualization of building steps of transformations and strati-
fication (1896, 1914, 1922, 1949) 
Red colored: What is realized in the corresponding period; Grey colored: What already exists. Every historical step is referred 
to the current document: i.e. the present building

344
The Importance of Being Honest
﻿
to a “process of selective canonization” (p.57). The new digital object assumes a new meaning of “real”, 
and this meaning derives from the active relationship with the physical content, from which it derives. 
In this kind of digital heritage, there is not visualization without a prior reality and, in a philological 
study of a digital model, we cannot forget its real reference from whom it was born: The architectural 
artifact. Therefore, considering digital heritage derived from tangible heritage, the issues related to data 
and information grow to include the relationship with the real content, its history and materiality (Fig. 6).
Topics of architectural digital surveying become very important: The measurements tools, meth-
odologies and data (point clouds); the modeling process (noise reduction, uniforming, smoothing, 
decimating point clouds, interpolating meshes, filling holes, texturing, CSG / B-Rep / Mesh / Image 
Based representations, the semantization of the model, LODs, etc.); the kind of software; the database 
organization; the visualization tools, etc. And we cannot forget: The historical research; the metrologi-
cal, proportional, comparative and typological studies; The analysis of constructive components, etc. 
The architectural surveying process is as a critical, iterative, and articulate workflow, where the aim is 
the wise representation of architectural characteristics.
Consequently a first consideration on transparency follows: the digitalizing and computing processes 
and tools have to be declared, in relation to the real building and its documents.
Also when we deal with models of never built buildings or not more existing ones, we have to consider 
the relation with documents: All these studies are based on the critique interpretation of heterogeneous 
archival materials (old drawings, historical photographs, descriptions, yard or competition documents, 
but also other historical publications and studies) (Fig. 7). Similarly occurs in the definition of past 
configurations of an existing building where, however, the archival apparatus have to be reflected on the 
reality of the existing building, i.e. the architectural document that can be experienced (Fig. 8).
Last but not least, we have not to fail to remember the constructive system. In an historic building 
often it is not visible, and critically deducible from indirect analysis, historical documents, and compara-
tive studies (Fig. 9).
In conclusion, the whole historical critical process of surveying, analysis, and representation must 
be transparent.
The Intrinsic Nature of DH
Being a critical product of a creative-interpretative act, the digital model is a knowledge contribution 
and, therefore, it becomes a new document. Centofanti (2010) writes, “The representative and restitu-
Figure 5. The conceptual relationship between architectural heritage and digital heritage

345
The Importance of Being Honest
﻿
Figure 6. Church of S. Giustino in Paganica. Comparison between the plans of the current building 
and before the restoration of 1946; longitudinal section with the overlay of the previous configuration. 
Figure 7. Church of S. Giustino in Paganica. Historical photos of the left elevation before and soon 
after the transformation. 
Figure 8. Church of S. Giustino in Paganica. 3D models of the current building and reconstruction of 
the state before 1946

346
The Importance of Being Honest
﻿
tive model of an architectural heritage is an integral part of historical knowledge and an autonomous 
text that can be subjected to further analysis and interpretations. Indeed the model provides a series of 
useful information on the architectural significant [...]. In turn, the model lends itself to be historicized 
and studied no more, and not only for its relationship with the presented object, but as a document itself 
in relation to the historical and cultural context that produced it” (p.47).
A double condition of the digital model of architectural heritage descends: The relation with the 
reality (historical building and documents); the digital nature that is the architecture of the model and of 
its data and information. The wording “heritage” implies that the digital resources presents own values, 
but indirectly related to the physical content. And being equipped with values, digital heritage has to be 
safeguarded. From here the recommendations of the “Charter for the Preservation of the Digital Heritage” 
about the guarding against loss of heritage, the need for action, in particular related to the lifecycle of 
information, and the development of strategies and policies (UNESCO, 2003).
Moreover we have to consider the characteristics that digital models are taking over time: No more 
only an “aesthetic” simulation of reality but interactive, manipulable, interrogable, and navigable models, 
allowing new ways of visualization and access to data and information (Brusaporci, 2015c).
Data, metadata, paradata, and information are the core of a model: An architectural model isn’t only 
made by spatial and geometric data, because 3D models and databases are two sides, and the modeling 
process has to involve both of them. The 3D model’s semantization and the granulosity of its components 
are important aspects for the correlation with the informative model (Murray, 2012. Apollonio, 2012. 
Gaiani, 2012). This complex informative model offers new non-linear narratives, favoring the knowledge, 
communication and design (Brusaporci, 2015b).
Conceptually these aspects requires a need for transparency intrinsically related to the digital archi-
tecture of the tools, hardware, software, and systems that make the digital heritage alive (Manovich, 
2013) (Figure 5).
Figure 9. Church of S. Giustino in Paganica. Transparency in constructive systems analysis: hypothetical 
reconstruction of masonries’ nucleolus according to the characteristics of the wall’s curtains. 

347
The Importance of Being Honest
﻿
DIKW Hierarchy, Paradata, and the Path for Transparency
Ackoff in 1989 defined the well known DIKW hierarchy, i.e. Data – Information – Knowledge – Wisdom 
hierarchy (Ackoff, 1989). This is usually represented by a triangle with “data” on the base and “wisdom” 
on the top. In our society of information, the triangle suggests the necessity for a wise understanding in 
moving from the base to the top, according to a process of filtration, reduction, and transformation. A lot 
has been written about – also to criticize it – (Rowley, 2007. Frické, 2009), but this concept – important 
in information science and management – is useful to reflect on what should be transparent and how it 
could be transparent.
Considering the architectural heritage, examples of data are the list of the spatial coordinates of a 
point cloud (exportable as a text file made by lines and columns of numbers) or the raster file deriving 
by the scanning of an archival document. But when we interpret a point cloud as a 3D discrete cast of an 
architecture (perhaps with indications on colors) or a paper with pencil traces as a drawing of a building, 
they become information. Data can be accompanied by many different kind of metadata:
•	
Building metadata about its author(s), history, context, state of conservation, use, transformations, 
maintenance, bibliographical and archival references, comments and analysis, studies on its con-
structive system, old surveying, photographs, drawings, etc.
•	
Model metadata on tools, software, procedures, workflow, ways of modeling, semantization, da-
tabase organization, etc.
Paradata helps in climbing the DIKH pyramid: paradata is metadata on the use of data, i.e. on its 
computing for Knowledge. And Knowledge could be interpreted as the critical outcome of modeling, 
where the complex model – made by 3D elements and informative system – is the critical interpretation 
and representation of the architectural phenomenon produced by the scholar(s). Finally Wisdom, as 
evaluated understanding, prodrome of communication and/or restoration design.
Transparency can reflect the whole process and paradata is considered the way to declare sources 
and reasons of choices. In this way paradara could be a reference between choices and documents, and 
between choices and comments of the author(s). Paradata describes the decisions and actions of the 
Demiurge(s): The scholars and/or the operators, who creates, uses, computes, and modifies data and info.
When we realize virtual reconstruction of not more existing buildings or of deeply modified ar-
chitectural configurations, paradata works in the same way as in the archaeological field. Moreover, 
in architectural surveying, paradata can take a new and more extended value: first they can be used to 
state the precision’s degree of architectural reconstruction; secondly to underline all elements deriving 
from critical and/or indirect analysis. An example is given by the modeling of the constructive system, 
whose understanding is based on documental information, analysis, or on the experiences and critical 
skill of the surveyor.
Issues of transparency could be related to the recoding of paradata into the database of the complex 
interpretative model of the architectural heritage, and therefore to an aware design of the building’s in-
formative system. There are many examples of architectural informative systems, but they rarely include 
explicit sets for paradata. However, when they are accurate and exhaustive, a guarantee of transparency 
is assured by the completeness of data and information (Brusaporci, 2010). And paradata could be added 
to the database inserting sets of commented links.

348
The Importance of Being Honest
﻿
Remembering that paradata are nothing else but data, the issue of using paradata relates to the link 
between 3D model and database. In this way, the 3D model itself is a “map” for the navigation between 
information. Therefore, it is important the structuring of the model (of both database and building), ac-
cording to a tree structure of elements. Consequently, the relation between model and paradata arises 
issues of semantization of the model.
The design of metadata and paradata has to be related to the characteristics of the two subjects (the 
real one and the digital one). We have to take into account the purposes: “When we are labeling a seg-
ment for browsing we should remember that the goal is to provide intellectual access-retrieval by subject 
labels. It is much more useful to know that a particular artifact refers to the Szechuan region of China, 
than to know whether it is a video segment, a jpg map, or a text file” (Murray 2012, 225). Therefore 
there must be a strict relation between metadata, paradata and model’s semantization: This allows a clear 
and smart navigation through model and information, and a transparent link between 3D components, 
data, information, and reliability.
Issues of transparency are necessarily related to semantization and granularity of the model, because 
they are the keynote for the structuring of the complex informative model, made by 3D and database 
(Figure 11).
Considering digital heritage transparency, we could say that there are two main topics: Transparency 
of the model toward the existing physical reality; Transparency of virtual re-constructions, in particular 
when we don’t have an immediate physical reference.
Focusing on digital heritage, we could indicate two kinds of paradata:
1. 	
Intrinsic Paradata: To declare choices about instruments and tools, applications, and their use, 
computing workflow, surveying pipeline and tools, 3D modeling and rendering, database model-
ing, etc. that is the architecture of the model and of the process. This paradata is similar to the 
traditional one in surveying process.
2. 	
Extrinsic Paradata: About the relation of the computer visualization with: Heritage’s nature and 
characteristics, Archival documents, Scholar’s experiences, skills, and decisions. The paradata has 
to describe the critical interpretation of sources by the scholar and, therefore, presets a degree of 
reliability.
The model is affected by: Kind of source; Source completeness; Source reliability; Level of inter-
pretation of sources. Thus the model has a degree of Objectivity/Believability.
The scholar(s) has to define critical degrees of reliability of the 3D model for the following items:
•	
Geometry;
•	
Location/Position;
•	
Date/Age;
•	
Colour/Texture;
•	
Material/Constructive system;
•	
Context (urban – rural – natural) /Landscape.
Obviously intrinsic and extrinsic paradata are related, and both of them derives from scholar’s criti-
cal choices.

349
The Importance of Being Honest
﻿
Follows reflection on context reliability. From a computing point of view, its simplified representation 
is necessary (for example using impostor billboards); in territorial application different LOD visualization 
are upload according to an interactive relation with the operator. Intending as “context” the surround-
ing background of architectural heritage, a simplified representation is conceptually acceptable, also to 
let a better perception of the studied heritage. However, remembering that every building exist also in 
relation to its context, its representation and the declaration of the critical choices made in its modeling 
and visualization are not less important.
Last but not least we have to consider that, in architectural heritage visualization, models are used to 
re-present also the future: that is the projects of restoration and management. Therefore also the degree 
of reliability and paradata must be extended to the future, i.e. to the design.
Visualizing Transparency
Transparency could be interpreted as an issue of communication of a particular medium: The complex 
informative model, made by 3D elements and data (and metadata, and paradata). From this point of view, 
the communication of transparency becomes a problem of mediating data, information, and knowledge.
Let’s consider the double logic of immediacy and of hypermediacy of media in our culture, according 
to the reflection on “mediation” made by Bolter and Grusin (1999). They don’t deal with transparency 
but their lesson could suggest some approaches to define strategies for communicating transparency.
On one hand, virtual reality, 3D graphics, and graphical interfaces favor the logic of transparent 
immediacy, that is «[…] the user is no longer aware of confronting a medium, but instead stands in an 
immediate relationship to the contents of that medium» (p. 24).
On the other hand, media multiplies itself and «The multiple representations inside the windows 
(text, graphies, video) create a heterogeneous space, as they compete for the view’s attention. Icons, 
menus, and toolbars add further layers of visual and verbal meaning. […] Unlike a perspective painting 
or three-dimensional computer graphic, this windowed interface does not attempt to unify the space 
around any one point of view. Instead, each text window defines its own verbal, each graphic window its 
own visual, point of view […] The multiplicity of windows and the heterogeneity of their contents mean 
that the user is repeatedly brought back into contact with the interface, which she learns to read just as 
she would read any hypertext. She oscillates between manipulating the windows and examining their 
contents, just as she oscillates between looking at a hypertext as a texture of links and looking through 
the links to the textual units as language» (p. 32-33).
The digital representation of a model requires the visualization of 3D elements and data together. 
The firsts – according to their architectural nature – can be experienced by spatial images, while the 
second by multiple media windows. Consequently, in computer based visualization, there are two ways 
to show transparency:
•	
Spatial visualization, with graphical strategies to declare reliability;
•	
Multiple windowed mediation of data and information (like scholia of humanities).
They are not alternative but integrated, aiming to a critical and efficient communication. But we have 
to use them in a wise way, avoiding to lose transparency into a huge quantity of data.
A lesson of visual perception and communication may be derived from the field of restoration: the 
so called “rigatino” is a technique to fill painting lacunas, reintegrating the overall unity of the image; 

350
The Importance of Being Honest
﻿
but, at the same time, looking closely, we percept the intervention. This concept could be useful when 
we have to clearly explain the spatial re-constructions, but also the reliability. In this way we could have 
visualizations with different levels of detail, where from afar we percept a homogeneous image, but up 
close we understand degrees of reliability indicated by shades, transparency, colors, simplified textures 
and geometries, but also pop-out callouts, labels or tags.
Multiple windowed strategy declares sources, without any intermediation. But if we bury people with 
a non-critical huge quantity of information, we risk to lose sight of transparency. Windows re-mediates 
the model and «Where immediacy suggests a unified visual model space, contemporary hypermediacy 
offers a heterogeneous space, in which representation is conceived of not as window on to the world, but 
rather as “windowed” itself – with windows that open on to other representations or other media» (Bolter, 
Grusin, 1999, 34). Tree structure should help the understanding of the database, but also the 3D model 
– being linked to data – could be the guide to knowledge of data, metadata, paradata, and information.
In conclusion, different kinds of solution are useful to visualize transparency: 3D visualizations, texts, 
graphs, windows with related sources, etc (Figure 10).
OPEN-ENDED SYSTEMS AND PARTICIPATORY 
CULTURE: FUTURE RESEARCH LINES
Until now, paradata is used to describe a defined model that is the outcome of an interpretative process 
that, at the end, produces computer visualization. Although this representation could be navigated and 
interrogated, it has a “static” architecture. In this way, paradata are about the procedure by which data and 
Figure 10. Church of S. Giustino in Paganica. Integrated informative system, realized with the impor-
tation of 3D models in GIS. Correlation of the database with the models of the current and historical 
building. Spatial visualizations are linked to windowed representation of data. 

351
The Importance of Being Honest
﻿
information are collected, computed and showed. It is a witnesses of how a model is defined. Because 
the scholar(s) is well defined, and the user(s) plays a well defined role.
In our participatory culture, the distinction between scholar(s), modeler(s) and user(s) blends, ac-
cording to new kind of media.
Murray, in her famous book “Hamlet on the Holodeck” (1997), pointed out four essential proprieties 
of digital environments: They are Procedural, Participatory, Spatial and Encyclopedic. And focusing on 
the aesthetics of the medium, she analyses the main issues of immersion, agency, and transformation.
Manovich in “The Language of New Media” (2001) stated five basic principles of new media: Nu-
merical representation, Modularity, Automation, Variability, Transcoding.
Jenkins, reflecting on our digital participatory culture (2009), highlighted the new kind of skills in 
using new media: Play, Performance, Simulation, Appropriation, Multitasking, Distributed cognition, 
Collective intelligence, Judgment, Transmedia navigation, Networking, Negotiation.
We are not interested in deeply analyzing and commenting these aspects, but we cite them with 
the aim to suggest how – considering these new kinds of systems – new issues about what should be 
transparent arise.
Nevertheless, paradata is defined and used not only to describe static systems, but – remembering 
the definition quoted at the beginning of this paper – «paradata involves “action” and traditional meta-
data involves “classification”» (US Department of Education, 2011, 1): Paradata is the tool to describe 
metadata usage and transformation. This issue is fundamental in open-ended and participatory systems. 
And a wise use of paradata is important also recalling the claim for measures against the loss of heritage, 
advocated by the Charter for the Preservation of the Digital Heritage.
Although in archeology there are examples of dynamic and interactive systems, based on eco-systemic 
processes of data computing, like in the “Cyber-Archeology” field (Forte, 2000). It is referred to knowl-
edge model based on the interactions in a virtual reality environment. It is a methodological instrument 
for scientific and cultural contents validation, where, according to a cybernetic system, the user(s) and 
the model interact in a dynamic way in the same environment (Forte, 2008). And Agent-based Modeling 
is a scientific methodology ever more diffused (Wurzel, Kowarik, & Reschreiter, 2015).
In web systems transparency’s protocols are important to declare the following items: The role of 
scholar(s) in computing data; The architecture of the model; The transformations made by agent-based 
modeling; The transformations realized by networked users, etc.
There are many digital system that simulates geographic and spatial phenomena, and open web GIS 
should introduce this new paradigms.
Figure 11. Transparency as methodological line for best-practices

352
The Importance of Being Honest
﻿
Actually one kind of dynamic system is diffused and is ever more important in the architectural 
heritage field: Building Information Systems root on the concept on “interoperability” between differ-
ent professionals and users, that interact interfacing, computing, and sharing the same model, made by 
3D elements and information.
BIM was born to plan and manage new buildings, but currently the studies on its use dedicated to 
built heritage is growing (Osello et alii, 2015. Garagnani, 2015. Dore & Murphy, 2015. Garagnani, 
2013. Apollonio, Gaiani & Zheng, 2012. Centofanti & Brusaporci, 2012. Murphy, McGovern, & Pavia, 
2011). For example, considering BIM applied to the restoration process of historical buildings, dynamic 
paradata may be used to record model’s modifications and updating according to: New findings in the 
yard; Actions of multiple and different professionals, administrators, and users; Maintenance; (open-
ended)-life-cycle of an heritage.
BIM is not provided for paradata, but a “cloud system” should be useful. Moreover the information 
system of BIM was design for the project of new buildings, and it’s not adequate to the architectural 
heritage complexity and for surveying data and information. In this direction, a development of BIM 
should be really interesting and useful.
Last but not least a future research line could be the study of standards and procedures for transpar-
ency communication, but for a semantic modeling according to transparency protocols.
CONCLUSION: TRANSPARECY AS DIGITAL ETHIC
Beside the DIKW hierarchy, we may consider a problem of “truth”, related to a conscious elaboration of 
data and information. There is different kinds of information, and the consideration that we could have 
bad information arises a question of IQ Information Quality, and therefore of knowledge quality. Floridi 
(2013) highlights how there isn’t a clear understanding of IQ properties and he points out the follow-
ing IQ categories: Intrinsic IQ (accuracy, objectivity, believability), Accessibility IQ (access, security), 
Contextual IQ (relevancy, value added, timeliness, completeness, amount of data), Representational IQ 
(interpretability, ease of understanding, concise representation, consistent representation). But these 
items recall the issues of transparency we analyzed before.
And obviously the reliability degree knowledge is – clearly – an issue of information quality.
Consequently we could make a link between IQ and transparency, and this parallel poses an ethical 
question.
Considering ethic as the reason of a behavior of responsibility toward colleagues, students, and every 
people interested or involved by studies, we don’t intend transparency as synonymous of ethics, but the 
practice of surveying, modeling, and visualization based on transparency is an ethical warranty.
And the ethical matter is ever more evident if we consider that transparency isn’t absence of interme-
diation but a clear and wise intermediation. This requires a great responsibility of the scholar.
Pomilio (2012) points out that in a communication we have a two-direction of information flow, where 
equally important is the role of the “writer” and of the “reader”. In fact in our participatory culture the 
user often becomes who produces information and, even if he plays only as reader, he filters informa-
tion in according to an active role. Then we have two filters of information: The first one made by the 
scholar, the second defined by the user. In this way transparency becomes a playing field where all the 
participants interact, and filter information. Both of them aim to reach knowledge.

353
The Importance of Being Honest
﻿
In conclusion transparency is important not only for a subjective ethic need of the scholar or for sci-
entific rigor for its own sake (and this may already be enough), but moreover it’s ever more important 
for the two following reasons: As methodological tool for communication; As methodology of research, 
to aspire to IQ, knowledge, and wisdom. In fact transparency favor to go back over research’s steps and 
validate the results; at the same time transparency could be the compass of the scholar to reflect on his 
own paradigms, procedures, and outcomes. In this way transparency could be seen as possible architec-
ture of the research’s workflow, and therefore transparency could be interpreted as a scientific approach. 
Therefore the definition of standards could be important, but in this way transparency configures as a 
best practice.
Like in Oscar Wilde’s comedy – where we cannot be sure if the characters are earnest – we cannot 
ensure an absolute “transparency”. But, to be honest, we should try to pursue clear and rigorous processes 
and visualization modalities, to declare the way from data and information to knowledge and wisdom. 
Perhaps Transparency is only a myth; but like all the myths it is necessary to bring out our purposes, 
clarify our points of view, and (at least) understand ourselves and our work.
REFERENCES
Ackoff, R. L. (1989). From Data to Wisdom. Journal of Applied Systems Analysis, 16, 3–9.
Apollonio, F. I. (2012). Architettura in 3D. Milano: Bruno Mondadori.
Apollonio, F. I., Gaiani, M., & Sun, Z. (2013). Characterization of Uncertainty and Approximation in 
Digital Reconstruction of CH Artifacts. In Proceeding of Le Vie dei Mercanti XI Forum Internazionale 
di Studi (pp.860-869). Napoli: La Scuola di Pitagora.
Apollonio, F.I., Gaiani, M., & Zheng, S. (2012). BIM-based modeling and data enrichment of classical 
architectural buildings. SCIRES-IT, 2(2).
Apollonio, F. I., & Giovannini, E. C. (2015). A paradata documentation methodology for the Uncertainty 
Visualization in digital reconstruction of CH artifacts. SCIRES-IT, 5(1), 1–24.
Archaeology Data Service & Digital Antiquity. (2013). Caring for Digital Data in Archaeology: A Guide 
to Good Practice. Oxford, UK: Oxbow Books.
Baudrillard, J. (1976). L’echange symbolique et la mort. Paris: Gallimard.
Beacham, R. (2008). ‘Oh, to Make Boards to Speak! There is a Task!’ Toward a Poetics of Paradata. 
In M. Greengrass & L. Hughes (Eds.), The Virtual Representation of the Past (pp. 171-178). Farnham, 
MD: Ashgate.
Bentkowska-Kafel, A., Denard, H., & Baker, D. (Eds.). (2012). Paradata and Transparency in Virtual 
Heritage. Farnham, MD: Ashgate Publishing.
Bolter, J. D., & Grusin, R. (1999). Remediation – Understanding New Media. Cambridge, MA: MIT Press.
Borra, D. (2004). Sulla verità del modello 3D. Un metodo per comunicare la validità dell’anastilosi 
virtuale. In E. S. Malinverni (Ed.), Proceeding of eArcom04 Tecnologie per comunicare l’architettura 
(pp. 132–137). Ancona: CLUA.

354
The Importance of Being Honest
﻿
Brandi, C. (1963). Teoria del restauro di Cesare Brandi. Lezioni raccolte da L. Vlad Borrelli, J. Raspi 
Serra e G.Urbani. Roma: Edizioni di Storia e Letteratura.
Brusaporci, S. (Ed.). (2010). Sistemi Informativi integrati per la tutela la conservazione e la valoriz-
zazione del patrimonio architettonico e urbano. Roma: Gangemi.
Brusaporci, S. (2015a). The Representation of Architectural Heritage in the Digital Age. In M. Khos-
row-Pour (Ed.), Encyclopedia of Information Science and Technology, Third Edition (pp. 4195-4205). 
Hershey, PA: IGI Global. doi:10.4018/978-1-4666-5888-2.ch412
Brusaporci, S. (2015b). Handbook of Research on Emerging Digital Tools for Architectural Surveying, 
Modeling, and Representation. Hershey, PA: IGI Global. doi:10.4018/978-1-4666-8379-2
Brusaporci, S. (2015c). On Visual Computing for Architectural Heritage. In S. Brusaporci (Ed.), Hand-
book of Research on Emerging Digital Tools for Architectural Surveying, Modeling, and Representation. 
Hershey, PA: IGI Global. doi:10.4018/978-1-4666-8379-2.ch003
Cameron, F. (2010). Beyond the Cult of the Replicant. In F. Cameron & S. Kenderdine (Eds.), Theoriz-
ing digital cultural heritage: a critical discourse (pp. 49-76). Cambridge, MA: MIT Press.
Cameron, F., & Kenderdine, S. (Eds.). (2010). Theorizing digital cultural heritage: a critical discourse. 
Cambridge, MA: MIT Press.
Centofanti, M. (2010). Della natura del modello architettonico. In S. Brusaporci (Ed.), Sistemi informativi 
integrati per la tutela, la conservazione e la valorizzazione del patrimonio architettonico e urbano (pp. 
43–54). Roma: Gangemi.
Centofanti, M., & Brusaporci, S. (2012). Architectural 3D modeling in historical buildings knowledge 
and restoration processes. In C. Gambardella (Ed.), Less More architecture design landscape. Naples: 
La Scuola di Pitagora.
Charter on the Preservation of Digital Heritage . (2003). Retrieved from http://portal.unesco.org/en/
ev.php-URL_ID=17721&URL_DO=DO_TOPIC&URL_SECTION=201.html
De Luca, L., Bussayarat, C., Stefani, C., Véron, P., & Florenzano, M. (2011). A semantic-based platform 
for the digital analysis of architectural heritage. Computers & Graphics, 2(35), 227–241. doi:10.1016/j.
cag.2010.11.009
Denard, H. (2012). A New Introduction to The London Charter. In Paradata and Transparency in Virtual 
Heritage (pp. 57–72). Farnham, MD: Ashgate Publishing.
Docci, M. (Ed.). (2005). Metodologie innovative integrate per il rilievo dell’architettura e dell’ambiente. 
Roma: Gangemi.
Docci, M., & Maestri, D. (2009). Manuale di rilevamento architettonico e urbano. Rome: Laterza.
Dore, C., & Murphy, M. (2015). Historic Building Information Modelling (HBIM). In S. Brusaporci 
(Ed.), Handbook of Research on Emerging Digital Tools for Architectural Surveying, Modeling, and 
Representation (pp. 233–273). Hershey, PA: IGI Global. doi:10.4018/978-1-4666-8379-2.ch007
Ferraris, M. (2012). Manifesto del nuovo realismo. Roma, Bari: Laterza.

355
The Importance of Being Honest
﻿
Ferrighi, A. (2015). Cities over Space and Time: Historical GIS for Urban History. In S. Brusaporci 
(Ed.), Handbook of Research on Emerging Digital Tools for Architectural Surveying, Modeling, and 
Representation (pp. 425–445). Hershey, PA: IGI Global. doi:10.4018/978-1-4666-8379-2.ch015
Floridi, L. (2013). Information Quality. Philosophy & Technology, 26(1), 1–6. doi:10.1007/s13347-013-
0101-3 PMID:22657387
Forte, M. (2000). About Virtual Archaeology: Disorders, Cognitive Interactions and Virtuality. In Virtual 
Reality in Archaeology (pp. 247-263). Oxford, UK: BAR International Series S843 (Archaeopress).
Forte, M. (2008). Introduzione. In M. Forte (Ed.), La Villa di Livia. Un percorso di realtà virtuale (pp. 
1-36). Roma: «L’Erma» di Bretschneider.
Forte, M., & Siliotti, A. (Eds.). (1996). Virtual Archaeology. Re-creating Ancient Words. New York: 
Harry N. Abrams.
Frické, M. (2009). The knowledge pyramid: A critique of the DIKW hierarchy. Journal of Information 
Science, 35(2), 131–142. doi:10.1177/0165551508094050
Frischer, B. (2008). From digital illustration to digital heuristic. In B. D. Frischer (Ed.), Beyond il-
lustration: 2d and 3d Digital Technologies As Tool for Discovery in Archaeology. Oxford, UK: British 
Archaeological Reports.
Gaiani, M. (2006). Un viaggio attraverso gli strumenti e i metodi di produzione del progetto di disegno 
industriale nell’epoca della progettazione digitale totale. In M. Gaiani (Ed.), La rappresentazione ricon-
figurata (pp. 23–57). Milano: Poli.Design.
Gaiani, M. (2012). Trattamento, tutela e comunicazione dei giacimenti documentalidell’architetturaant
ica. In P. Clini (Ed.), Vitruvio e il disegno di architettura (pp. 177–219). Venezia: Marsilio.
Garagnani, S. (2013). Building Information Modeling and real world knowledge: A methodological ap-
proach to accurate semantic documentation for the built environment. Proceedings of Digital Heritage, 
1, 489–496.
Garagnani, S. (2015). Semantic Representation of Accurate Surveys for the Cultural Heritage: BIM Ap-
plied on the Existing Domain. In S. Brusaporci (Ed.), Handbook of Research on Emerging Digital Tools 
for Architectural Surveying, Modeling, and Representation (pp. 292–310). Hershey, PA: IGI Global. 
doi:10.4018/978-1-4666-8379-2.ch009
Geremia, F. (2014). Building on our losses: Principles and methodologies of virtual restoration applied 
to Rome’s historic centre. Città & Storia, IX(1), 33–60.
Gilliland, A. J. (2008). Setting the Stage. In M. Baca (Ed.), Introduction to Metadata (pp. 1–19). Los 
Angeles: Getty Research Institute.
Greengrass, M., & Hughes, L. (Eds.). (2008). The Virtual Representation of the Past. Farnham: Ashgate.
Gul, S., Tramboo, S. R., & Ahangar, H. (2015). Metadata Diversity in the Cultural Heritage Reposi-
tories. In M. Khosrow-Pour (Ed.), Encyclopedia of Information Science and Technology (3rd ed.; pp. 
1843–1854). Hershey, PA. doi:10.4018/978-1-4666-5888-2.ch178

356
The Importance of Being Honest
﻿
Hermon, S. (2012). Scientific Method, chaîne opératoire and Visualization: 3D Modelling as a Research 
Tool in Archaeology. In Paradata and Transparency in Virtual Heritage (pp. 13–22). Farnham: Ashgate 
Publishing.
Hermon, S., & Nicolucci, F. (2006). A Fuzzy Logic Approach to Realiability in Archaeological Virtual 
Reconstruction. In S. Hermon & F. Nicolucci (Eds.), Beyond the Artefact. Digital Interpretation of the 
Past (pp. 28–35). Budapest: Archaeolingua.
Huvila, I. (2013). The Unbearable Complexity of Documenting Intellectual Processes: Paradata and 
Virtual Cultural Heritage Visualisation. Human IT, 12(1), 97–110.
Ippolito, A. (2015). Digital documentation for archaeology. Case studies on etruscan and roman heritage. 
SCIRES-IT, 5(2), 71–90.
Jenkins, H. (2007). Reconsidering Digital Immigrants. Retrieved from www.henryjenkins.org/2007/12/
reconsidering_digital_immigran.html
Jenkins, H. (2009). Confronting the Challenges of Participatory Culture. New York: The Free Press.
Kreuter, F. (Ed.). (2013). Improving Surveys with Paradata. Hoboken, NJ: Wiley. doi:10.1002/9781118596869
Lowenthal, D. (1985). The Past is a Foreign Country. Cambridge, UK: Cambridge University Press.
Manovich, L. (2001). The Language of New Media. Cambridge, MA: The MIT Press.
Manovich, L. (2013). Software Takes Command. New York: Bloomsbury Academic.
Murphy, M., McGovern, E., & Pavia, S. (2011). Historic building information modelling - adding intel-
ligence to laser and image based surveys. International Archives of the Photogrammetry, Remote Sensing 
and Spatial Information Sciences, XXXVIII-5/W16.
Murray, J. (1997). Hamlet on the Holodeck. The Future of Narrative in Cyberspace. Cambridge, MA: 
The MIT Press.
Murray, J. H. (2012). Inventing the Medium. Principles of Interaction Design as a Cultural Practice. 
Cambridge, MA: The MIT Press.
Negroponte, N. (1995). Being Digital. New York: Alfred A. Knopf.
Nicolucci, F. (2012). Setting Standards for 3D Visualization of Cultural heritage in Europe and Beyond. 
In Paradata and Transparency in Virtual Heritage (pp. 23–36). Farnham: Ashgate Publishing.
Osello, A., Acquaviva, A., Dalmasso, D., Erba, D., Del Giudice, M., Macii, E., & Patti, E. (2015). In S. 
Brusaporci (Ed.), Handbook of Research on Emerging Digital Tools for Architectural Surveying, Model-
ing, and Representation (pp. 274–291). Hershey, PA: IGI Global. doi:10.4018/978-1-4666-8379-2.ch008
Parry, R. (Ed.). (2010). Museums in a Digital Age. Abingdon: Routledge.
Perry, R. (2003). Foreword. Digital Heritage: Agora and Agility. In E. Ch’ng, V. Gaffney, & H. Chapman 
(Eds.), Visual Heritage in the Digital Age (pp. v–vii). New York: Springer-Verlag.

357
The Importance of Being Honest
﻿
Pomilio, F. (2012). La comunicazione come facilitatore etico. In F. Pomilio & D. Pasonetti (Eds.), Co-
municare la trasparenza (pp. 21–53). Bologna: Fausto Lupetti.
Prensky, M. (2001). Digital Natives, Digital Immigrants Part 1. On the Horizon, 9(5), 1–6. 
doi:10.1108/10748120110424816
Principles of Seville . (2012). Retrieved from http://www.arqueologiavirtual.com/carta/?page_id=12
Rowley, J. (2007). The wisdom hierarchy: Representations of the DIKW hierarchy. Journal of Informa-
tion Science, 33(2), 163–180. doi:10.1177/0165551506070706
Spagnesi, G. (1984). Autonomia della Storia dell’architettura. In G. Spagnesi (Ed.), Storia e restauro 
dell’architettura (pp. 7–10). Roma: Istituto della Enciclopedia Italiana Treccani.
Stefani, C., De Luca, L., Véron, P., & Florenzano, M. (2010). Time indeterminacy and spatio-temporal 
building transformations: An approach for architectural heritage understanding. International Journal 
on Interactive Design and Manufacturing, 4(1), 61–74. doi:10.1007/s12008-009-0085-5
Stone, R. J. (1999, November). Virtual heritage. UNESCO World Heritage Magazine, 18-20.
The London Charter . (2009). Retrieved from: http://www.londoncharter.org/
The Onlife Manifesto . (2015). Retrieved from: https://ec.europa.eu/digital-agenda/en/onlife-manifesto
US Department of Education. (2011). Paradata in 20 Minutes or Less. Retrieved from (https://docs.
google.com/document/d/1QG0lAmJ0ztHJq5DbiTGQj9DnQ8hP0Co0x0fB1QmoBco/edit?pli=1
Vattimo, G. (1989). La società trasparente. Milano: Garzanti.
Wurzel, G., Kowarik, K., & Reschreiter, H. (Eds.). (2015). Agent-based Modeling and Simulation in 
Archeology. Cham: Springer.
ADDITIONAL READING
Albisinni, P., & De Carlo, L. (Eds.). (2011). Architettura. Disegno. Modello. Roma: Gangemi.
Benedetti, B., Gaiani, M., & Remondino, F. (2010). Modelli digitali 3D in archeologia: il caso di Pompei. 
Pisa: Scuola Normale Superiore Pisa.
Bianchini, C. (2014). Survey 2.0: new technologies, new equipment, new surveyors? In P. Giandebiaggi 
& C. Vernizzi (Eds.), Italian Survey & International Experiences (pp. 763–768). Roma: Gangemi.
Bianchini, C., Borgogni, F., & Ippolito, A. (2015). Advantages and disadvantages of digital approach in 
archaeological fieldwork. In F. Giligny, F. Djindjian, L. Costa, P. Moscati, & R. Show (Ed.). Proceed-
ings of the 42nd annual conference on computer applications and quantitative methods in archaeology 
(pp. 95-106). Oxford: Oxuniprint.

358
The Importance of Being Honest
﻿
Bianchini, C., Ippolito, A., & Bartolomei, C. (2015). The surveying and representation process applied 
to architecture: non contact-methods for the documentation of Cultural Heritage. In S. Brusaporci (Ed.), 
Handbook of Research on Emerging Digital Tools for Architectural Surveying, Modeling, and Represen-
tation (pp. 45–93). Hershey, PA, USA: IGI Global. doi:10.4018/978-1-4666-8379-2.ch002
Brusaporci, S. (2011). Modelli digitali per la rappresentazione dell’architettura. DisegnareCon, 8, 107–115.
Brusaporci, S. (2013). Modellazione e rappresentazione digitale per i beni architettonici. In S. Brusaporci 
(Ed.), Modelli complessi per il patrimonio architettonico-urbano (pp. 18–24). Roma: Gangemi Editore.
Brusaporci, S. (2014). Issues of Historic Town Surveying: Visualizing Urban Values. SCIRES.it 4(2), 
63-80.
Centofanti, M., Brusaporci, S., Continenza, R., & Trizio, I. (2012). Sistemi Informativi Architettonici 
per la gestione, tutela e fruizione dell’edilizia storica. In Atti della 16ª Conferenza Nazionale ASITA (pp. 
315–322). Vicenza: ASITA Society.
Centofanti, M., Brusaporci, S., & Lucchese, V. (2014). Architectural Heritage and 3D Models. In P. Di 
Giamberardino, D. Iacoviello, R. Natal Jorge & J. M. R. S. Tavares, Computational Modeling of Objects 
Presented in Images (pp. 31–49). Geneva: Springer. doi:10.1007/978-3-319-04039-4_2
Ch’ng, E., Gaffney, V., & Chapman, H. (Eds.). (2013). Visual Heritage in the Digital Age. New York: 
Springer-Verlag. doi:10.1007/978-1-4471-5535-5
Chiavoni, E., & Filippa, M. (Eds.). (2011). Metodologie integrate per il rilievo, il disegno, la model-
lazione dell’architettura e della città. Rome: Gangemi.
Docci, M. (2007). The unbuilt Vatican Basilica. The project by Antonio da Sangallo. Disegnare idee 
immagini, 34, 24-35.
Dylla, K., Frischer, B., Mueller, P., Ulmer, A., & Haegler, S. (2010). Rome Reborn 2.0: A Case Study 
of Virtual City Reconstruction Using Procedural Modeling Techniques. In CAA 2009. Making History 
Interactive (pp. 62–66). Oxford: Archaeopress.
Forte, M., Pescarin, S., & Pierantoni, E. (2006). Transparency, interaction, communication and open 
source in Virtual Archaeology. In M. Forte & S. Campana (Eds.), From Space to Place. BAR Interna-
tional Series 1568 (pp. 535–540). Oxford: Archeopress.
Gere, C. (2002). Digital Culture. London: Reaktion Books.
Gold, M. K. (Ed.). (2012). Debates in the Digital Humanities. Minneapolis. MN: University of Min-
nesota Press. doi:10.5749/minnesota/9780816677948.001.0001
Hoel, A. S., & Webmoor, T. (Eds.). (2014). Visualization in the Age of Computerization. Oxford: Routledge.
Ioannides, M., Addison, A., Georgopoulos, A., Kalisperis, L., Brown, A., & Pitzalis, D. (Eds.). (2010). 
Heritage in the Digital Era. Brentwood: Multi-science publishing. doi:10.1007/978-3-642-16873-4
Ippoliti, E. (Ed.). (2013). Shedding light on the cultural heritage. Roma: Aracne.

359
The Importance of Being Honest
﻿
Ippolito, A., Senatore, L. J., Belelli Marchesini, B., & Ceroli, G. (2015). From survey to representation 
of the model. A documentation of typological and chronological sequences of archaeological artefacts: 
traditional and innovative approach. In F. Giligny, F. Djindjian, L. Costa, P. Moscati, R. Show (Ed.). 
Proceedings of the 42nd annual conference on computer applications and quantitative methods in ar-
chaeology (pp. 107-114). Oxford: Oxuniprint.
Kalay, Y., Kvan, T., & Affleck, J. (Eds.). (2007). New Heritage: New Media and Cultural Heritage. 
Abingdon: Routledge.
Maggio, F., & Villa, M. (2008). Architettura demolita. Palermo: Edizioni Caracol.
Sacchi, L. (2015). La fine del disegno? Op. Cit., 123, 5–15.
Scheer, D. R. (2014). The Death of Drawing: Architecture in the Age of Simulation. Abington: Routledge.
Spallone, R. (2015). Digital Reconstruction of Demolished Architectural Masterpieces, 3D Modeling 
and Animation: The Case Study of Turin Horse-Racing by Mollino. In S. Brusaporci (Ed.), Handbook 
of Research on Emerging Digital Tools for Architectural Surveying, Modeling, and Representation (pp. 
476–509). Hershey, PA: IGI Global. doi:10.4018/978-1-4666-8379-2.ch017
Terras, M., Nyhan, J., & Vanhoutte, E. (Eds.). (2013). Defining Digital Humanities: A Reader. Farnham: 
Ashgate.
KEY TERMS AND DEFINITIONS
Architectural Artifact: This definition, referred to an architectural heritage, is used to highlight that 
a historic building can be seen as a product of human activity. This becomes even more important for an 
historic building, built with artisan technologies and stratified during of his life. Therefore, its material 
consistency takes on a specific testimonial value of the cultures and transformations that have gener-
ated and modified it over time. Additionally, the material characteristics can be used as chronological 
indicators for dating parts of the building. Hence it follows that it is necessary to put special attention 
in the study of construction system also in digital modeling.
Architectural Digital Model: Complex Model made by the synthesis of geo-referenced architec-
tural models and correlated databases. The Informative Model has to be able to visualize and computize 
synchronically and diachronically architectural information. The model is made by two kinds of digital 
models: 3D model able to simulate the architectural characteristics of a building (geometries, spaces, 
materials, historical and aesthetical values, etc.); database model collecting documents, studies, and 
analysis on the historical building.
Architectural Heritage: The “UNESCO 1972 World Heritage Convention”, indicates as cultural 
heritage monuments, group of buildings and sites, outstanding universal value from the point of view 
of history, art or science. In the modern theory of restoration, the qualities of a cultural heritage are 
the historical and the aesthetic values. An historical building is a complex system of spaces, volumes, 
materials, surfaces, constructive aspects, actual and past functions and configurations, degradation, etc. 
The whole is the result of a continuous historical process of modification and transformation. An ar-
chitectural heritage can be interpreted as an “artifact”, where its elements are witnesses of constructive 

360
The Importance of Being Honest
﻿
cultures and of events occurred during the life of the building. In the study of architectural heritage is 
fundamental the surveying and archival analysis.
Digital Heritage: In 1999, on UNESCO’s “World Heritage Magazine”, Stone defined Virtual Heritage 
as: “the utilization of technology for interpretation, conservation and preservation of Natural, Cultural 
and World Heritage.” The “Charter on the Preservation of the Digital Heritage” published by UNESCO 
in 2003, defines the “Digital Heritage” as “Common Heritage”, made by: “cultural, educational, scien-
tific and administrative resources, as well as technical, medical and other kinds of information created 
digitally, or converted into digital form from existing analogue resources.” It includes different kinds of 
products such as texts, databases, images, audio, graphics, software and web pages.
Paradata: It can be generically referred to the process by which the survey data are collected. In 
particular, paradata is a kind of metadata focused on the use of data, and moreover it describes the trans-
formation of data during their “inter-use” in participatory systems. According to the definition presented 
by The London Charter (2009) – focused on its use in 3D computer based visualizations – paradata is: 
“Information about human processes of understanding and interpretation of data objects. Examples of 
paradata include descriptions stored within a structured dataset of how evidence was used to interpret an 
artefact, or a comment on methodological premises within a research publication. It is closely related, 
but somewhat different in emphasis, to ‘contextual metadata’, which tend to communicate interpretations 
of an artefact or collection, rather than the process through which one or more artefacts were processed 
or interpreted.”
Semantization: Logical division of digital model’s components according to their meaning. In archi-
tectural heritage models, the semantization can be realized “a-posteriori” (reverse modeling processes) 
or “a-priori” (for example Constructive Solid Geometry processes). The term “granularity” is related to 
semantization and is about the levels of articulation of the model’s elements.
Transparency: In computer-based visualization, it consists in the statement of sources and of the 
degree of reliability of the virtual re-constructions. From a methodological point of view it makes the 
digital model testable by other professionals. The concept has been developed in the archaeological field 
where often 3D models are realized to show the configuration of ancient artifacts. In The London Char-
ter’s Glossary (2009), there is a definition of “Intellectual transparency”: “The provision of information, 
presented in any medium or format, to allow users to understand the nature and scope of “knowledge 
claim” made by a computer-based visualisation outcome”.
ENDNOTE
1	
 The original text is in Italian. The translation was made by the author of the chapter.
This work was previously published in the Handbook of Research on Emerging Technologies for Architectural and Archaeo-
logical Heritage edited by Alfonso Ippolito, pages 66-93, copyright year 2017 by Information Science Reference (an imprint 
of IGI Global).

361
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  19
DOI: 10.4018/978-1-5225-1677-4.ch019
ABSTRACT
Drawing has always been the most powerful instrument for the conceptualization, interpretation and 
representation of spaces and forms. Today, the computer screen complements the eye-brain telescope 
with an additional lens that increases the ability to understand, visualize and ultimately design the built 
environment. Computational design is dramatically shifting not only established drawing and modeling 
practices, but also ‒ and perhaps most importantly ‒ design thinking processes in the very conception 
and morphogenesis of forms and of their complex relationships in space. Specifically parametric mod-
eling allows to understand geometry and manipulate shapes in dynamic, articulated and yet intuitive 
ways, opening up unprecedented design opportunities but also diminishing the importance of the design 
process for the sake of formal complexity. This chapters offers some insights on the incredible design 
opportunities offered by new computational instruments, as well as highlighting circumstances in which 
the act of ‘modeling’ takes over the ‘design.’
Computational Design 
and Built Environments:
The Quest for an Alternative Role 
of the Digital in Architecture
Marco Filippucci
Università degli Studi di Perugia, Italy
Fabio Bianconi
Università degli Studi di Perugia, Italy
Stefano Andreani
Harvard University, USA

362
Computational Design and Built Environments
﻿
INTRODUCTION
Drawing can be considered as the “logical instrument” that humans have always used for trying to 
understand and envision their surrounding reality (de Rubertis, 1994). Another “logical instrument” 
geometry articulates precisely the rules of drawing, acting as a fundamental support of scientific repre-
sentation (Migliari, 2000). In this field the digital revolution has fostered the development of rigorous 
representation methods. The syntactic and purely mathematical alphabet of the computer is now able to 
create synthetic elements and morphological patterns, eventually providing models of an either existing 
or envisioned physical environment. Computational design indeed has a tremendous intrinsic power of 
formal modeling, with potentially infinite spatial possibilities and configurations. For quite some time 
this ability was in the hands of skillful programmers who were able to manipulate scripts and algorithms 
for 3D modeling. Over the last few years though, parametric software have evolved in such a way that 
mastering coding is not a necessary prerequisite anymore. In fact, relatively intuitive visual interfaces of 
parametric software such as Rhinoceros Grasshopper, Catia Digital Project or Autodesk Dynamo make 
clear to the designer not only the geometrical proprieties of forms, but also the relationships between them 
to create complex compositions. These environments also facilitate and make easier the use of scripts 
to integrate their modeling limitations. Today’s open source and sharing culture in fact pushes towards 
that direction, allowing to borrow codes from the web and manipulating them for specific design needs. 
Basic coding skills so become sufficient for designers to embrace the potentials of computational design.
The digital language of parametric modeling can be thus expressed by a visual interface translating 
software procedural logic that would otherwise be formulated through strings of codes that are usually 
Figure 1. Models of Le Corbusier’s Philips Pavilion
(Student’s work, University of Perugia)

363
Computational Design and Built Environments
﻿
hard to understand by the average designer. In the modeling process then the attention focuses on the 
represented form, and the interface allows to understand its morphogenesis and the geometric rules 
that structure its digital development. In this merging between formal representation and geometric 
rules, digital modeling becomes more than ever a generative process that shifts the focus from shapes 
themselves to operations. In fact, several parametric software today embed visual algorithm editors that 
allow to easily manipulate intrinsic geometric parameters of the model that is being designed, as a sort 
of ‘representation of representation.’ Such a parametric process thus empowers the designer with an 
ability to clearly build relationships between the different components of a model, and modify every 
single geometric parameter at any given moment of the whole modeling chain. If this modeling chain is 
well designed, then the new configurations of the model will be dynamically and seamlessly updated.
Over time the relationship between designers, representation and the built environment has deeply 
evolved, and the recent digital revolution has dramatically changed the way in which we study and employ 
geometry in architecture: new software, media and interaction technologies allow the user to dynamically 
shape the ‘model’ center of the design language since Galileo’s scientific method. The combination of 
digital representation and mathematical techniques sparks a profound renewal of descriptive geometry 
research, too often reduced in between the two extremes of an anachronistic classicism and an innovative 
computerization without historical background.
Computational design then becomes an instrument able to reinvigorate the union of graphical repre-
sentation and digital space (Migliari, 2009). This process highlights what a form is made of, rather than 
what kind of form it is (Gay, 2008). If the digital revolution shifted the attention from the model to the 
visualization, then parametric design allows to regain possession of the infinite potential and dynamic 
heuristic of the model, making explicit its generative path and understanding the singular elements of 
the digital syntax. As a result, we are then able to assess a “critical analysis of the digital representation’s 
performance,” as the Italian school of descriptive geometry claimed in its Manifesto (Monge, 1798).
Back in time, Monge wrote about a descriptive geometry that enabled a shift from “the known to 
the unknown.” In parametric design the modeling process is useful to understand forms and their mor-
phogenesis, highlighting the logic of the model for a better interaction, validation and iteration. The 
representation’s action in parametric modeling is fundamentally defined by geometrical and mathemati-
Figure 2. Models of Nervi’s St. Mary Cathedral 
(Student’s work, University of Perugia)

364
Computational Design and Built Environments
﻿
cal parameters and algorithms. Values are required as input and the output information can be used as 
yet other input data for further operations, so forming a network of nodes and connections perhaps an 
ideogrammatic morpheme of contemporary hypertext communication. Therefore the parametric model 
changes the way shapes and objects can be studied and developed, combining complex drawing and 
spatial understanding and questioning “the medium is the message” assumption (McLuhan, 1964).
Through parametric modeling, employed as an analytical tool, geometry is described in a unexplored 
dynamicity that fosters experimental processes of formal generation. With an analogy with some studies 
about shapes of the natural world, models arise and are structured around a morphogenetic analysis of 
growth. The representation of relationships between elements highlight their structure and provide op-
portunities to maximize the generative procedure. Making this process explicit then becomes very useful 
Figure 3. Parametric model of Gensler’s Shanghai Tower 
(Stefano Andreani, MIT, 2012)

365
Computational Design and Built Environments
﻿
at different levels: “To disclose mathematics means to disclose a tremendous source of knowledge, a 
wonderful source of inspiration and a powerful tool of creativity” (Maertterer, 2007). The data structure 
then represents a sort of DNA of the model, which can be dynamically altered to create an almost infinite 
number of design solutions. By understanding this ‘analogical’ representation of ‘logical space,’ we 
can then reveal the geometry of geometry. As Leon Battista Alberti once wrote, “potrai tu senza pena, 
liberamente aggiungere, diminuire, tramutare, rinnovare, e rivoltare finalmente ogni cosa sotto sopra, 
insino a tanto che ogni e qualunque cosa stia come tu vuoi e sia da lodare”, translated from the authors: 
“May you easily and freely add, reduce, transform, renew, and finally turn upside down everything, until 
each end every single thing becomes as you want and should be praised for.” (Alberti, 1450).
BACKGROUND
If the normal progress is to make a quantity growing from 1 to 10, to 100, or to 1000, innovation is instead 
jumping from 0 to 1. That is giving birth to something that did not exist before and that we were not even 
able to imagine. This is how Peter Thiel, a Silicon Valley entrepreneur, defines the value of innovation. 
It is difficult to innovate, because the future is hidden where nobody is supposed to look at, and predic-
tions are usually always wrong. The “Blade Runner” movie was set in a future city where people were 
driving flying cars, and yet nobody would have thought of using mobile phones to talk to each other.
The digital representation then makes us ask a fundamental question: is it progress or innovation? 
What is the role of technology to evolve or transform contents? What is the influence of the medium to 
our knowledge?
We can understand each other, but each one of us can only interpret himself. Herman Hesse (1972) 
in the Demian introduction states that “the life of each person is a way towards himself, only an attempt, 
a sign for a path.” He goes on by saying that “no one has ever been entirely himself, yet each one tries 
to become someone, there is who does it underhandedly, who clearly, according to different possibili-
ties. Each one [...] embeds traces from the birth moment, feelings and eggshells of a primordial world. 
Some will never become men, remaining frogs, lizards or even ants. Others can be half man and half 
Figure 4. Analysis of Michelucci’s Autostrada Church 
(Student’s work, University of Perugia)

366
Computational Design and Built Environments
﻿
Figure 5. Computational experiments on circle packing techniques 
(Stefano Andreani, Aurgho Jioty, Harvard Graduate School of Design, 2011)

367
Computational Design and Built Environments
﻿
fish, but everyone is a run-up towards being a man, we all have common origins as well as mothers, we 
all come from the same darkness; but each one, in an attempt and a run-up of depth, tends towards is 
own destination.”
From these words, full of feelings, of history and of literature from Ovid to Kafka, it can be argued 
that the human being is constantly evolving towards himself. It is a never ending process that can be 
changed and adapted over time. Its opposite is represented by immunity to change, which in architecture 
is expressed and takes physical form through the idea of symmetry. On this subject Mario Livio, in his 
interesting book The impossible equation (2006), highlights the ancient roots of the word symmetry, 
with its meaning of “same measure.” At its first use, being symmetric represented the possibility for 
an architectural work to contain a small portion of it such that the size of all other parts would present 
that fragment for an exact number of times. This definition is probably more akin to our current idea 
of proportion than to the one of symmetry though. It was then Plato and Aristotle that associated the 
concept of symmetry to the classical idea of beauty, disrupted only after two millennia by the modern 
movement with the introduction of the concept of time in space.
The current concept of symmetry, which dates back to the 18th century, abandons the geometric ideal 
of proportion and at the same time assumes a purely mathematical sense of “immunity to any possible 
change.” De Rubertis in his The Mutant City (2008) argues that symmetry and uniformity, seen as a 
predisposition to a morphological regularity, correspond to the conceptual categories of simplicity, pre-
dictability and order. They are all oriented towards a controlled architecture which is nonetheless stable, 
durable and reliable. This tendency marked the whole classical architecture, only reeling with the organic 
forms of the modern times and eventually being destroyed in contemporary architecture. Those are two 
important moments: the first one is marked by the industrial revolution when the plane becomes a curved 
surface and the other one by the digital revolution when the curve is discretized through a hybrid surface.
In that sense, the shift from the old to the new concept of symmetry is extraordinary and gets estab-
lished in the times of the mechanistic consciousness. At the same time, the Euclidean geometry sees its 
interpretative liability being undermined by the not-well-structured theories on the “parallels’ axiom,” 
eventually giving its way to the introduction of curved geometries.
That is the consequence of a scientific and cultural revolution started in 300 BC with Euclid of 
Alexandria when his The Elements began to circulate. A monumental work that put the basis of the 
so-called “Euclidean geometry” and that for about two millennia was the only type of geometry to be 
known. He started off by analyzing five postulates and, by using logical deductions, attempted to prove 
Figure 6. Models of Toyo Ito’s La Concha Negra Pavilion 
(Student’s work, University of Perugia)

368
Computational Design and Built Environments
﻿
all the other ones on the basis of the already proved first four. The uncertainties on the fifth postulate 
eventually fostered the development of “non Euclidean” geometries.
Euclid’s Elements defines the planar geometry upon five postulates, but it is the fifth “the parallel 
axiom” that sparked for two thousand of years the interest of scholars. This axiom actually seemed to be 
depended upon the other postulates. It was only during the XIX century (between 1830-1860) that the 
problem found a permanent solution, thanks to the work of Bolyai, Gauss and Lobačeviskij. Lobačeviskij’s 
geometry in fact shows that it is possible to construct geometries in which the uniqueness is no longer 
valid: Through a given point P on a straight line r, more than one line can be drawn parallel to a given 
line (Klein’s model). In the geometrical construction proposed by Riemann (spherical geometry) this 
is not valid anymore: Through a given point P on a straight line r, no line can be drawn parallel to a 
given line. Both models meet all the Euclid’s axioms but the fifth. If we don’t want Euclidean geometry 
to be contradictory, then “parallel axiom” can not be dependent upon the first four (exhaustion). From 
these new models it is clear that there is no longer a single geometry, but multiple geometries whose 
appropriateness lies in its consistent internal logic more than in its ability to describe the physical reality.
From the cultural transformation expressed by non-Euclidean geometry, the modern thought has 
been animated by eminent researchers such as Saccheri, Gauss, Riemann, Lobacevskij. Thanks to those 
prominent figures it has been understood that different phenomena related to the physical world and 
reaching different spaces need to respond to the laws of those spaces themselves. At the end of the 19th 
century Felix Klain analyzed the work of his predecessors and pinned down three types of geometry: 1. 
the Euclidean geometry, the geometry of surfaces with a zero curvature; 2. the spherical geometry, the 
geometry of surfaces with positive curvature (Riemann); 3. the hyperbolic geometry, the geometry of 
surfaces with negative curvature (Lobacevskij).
Pavel Florenskij in his masterpiece Space and Time in Arts (1923) claims that “the things themselves 
are nothing more than spatial wrinkles, places with a particular curvature” and therefore “the properties 
of reality, for a rational knowledge, must be placed somewhere in the model, that is in the space, in the 
thing or in the environment.”
Thanks to computational geometry and the digital revolution, the description and generation of forms 
is finally amplified. After more than 100 years from the theorization of curved spaces, the hybrid geom-
etry, the definition of advanced algorithms for modeling, the parametric surfaces in triangular lattices and 
the definition of procedures for manipulating complex surfaces, allow for an architecture that replaces 
Vitruvius’s static (static equilibrium) with dynamic equilibrium. Equilhybrids, hybrid equilibrium, kinetic 
equilibrium that abandon the concept of eternity in favor of immediate fixity, as if they were generated 
Figure 7. Models of Calatrava’s Turning Torso 
(Student’s work, University of Perugia)

369
Computational Design and Built Environments
﻿
out of a photograph. The movement captured by a photo is fixed on an image and, at the same time, the 
form of movement is revealed in its representation. The shape emerges from its representation.
In the contemporary research of new forms in an increasingly more complex and unconventional 
architectures, in the pursuit of their interpretative model, it becomes necessary to break the form into 
simpler, known and easily recognizable parts. Therefore in the revelation of the shape itself, it is our 
senses, our culture and our intuition that will interpret and thus reveal the unknown. Knowing what can 
be recognizable is a simple yet useful tool of meta-design.
The presented research is a path that, through the analysis of complex surfaces, recognizes the archi-
tectural forms, studies and interprets them, and ultimately gives materiality to them. The morphogenesis 
process, starting from the object decomposition into simpler pieces and combining unknown morphemes 
with hybrid shapes or defined patterns, then reveals apparently unknown and yet easy to create geom-
etries. This is a process that unveils forms through a deductive path that is full of representative potential. 
An almost didactic journey that marks the shift from the Euclidean geometry to the NURBS geometry.
Figure 8. Geometric studies of Gubbio’s candles 
(Student’s work, University of Perugia)
Figure 9. Punctual variations in design process
(Student’s work, University of Perugia)

370
Computational Design and Built Environments
﻿
The focus is the concept of interpretation, as a sort of morphogenesis place. Taking the reconstruc-
tion of a simple shape like a rugby ball as an example, it is clear that by starting from different shape 
interpretations different modeling solutions can be achieved. Two emblematic geometric interpretations 
can be compared: the first one takes the axis and a longitudinal section and defines a surface of rota-
tion; the second one instead identifies a series of cross-sections and generates an interpolation surface. 
Perhaps amongst those two what changes is the accuracy level or the complexity of their construction 
process. What is certain though is that the two shapes are different because their genesis is different.
The eighteenth-century image of the “Manus oculata” (hand-with-eye) can be taken as a hybrid para-
digm that perfectly embodies our interpretation of reality. It would be difficult to represent the shape of 
a three-dimensional object on a two-dimensional surface if it was not possible to see it, to understand 
the sensorial aspects, to read it through touch. If this translation to the two dimensions results in a loss 
of plastic and volumetric information that can be perceived by only moving around the real object, then 
it is only by blending in a unitary image the different views that we are able to understand forms and 
spaces. Also, our visual memory allows us to see much more than a single point of view would show. 
Without the contribution of stored data, our eye would have a reduced cognitive function.
The hand-with-eye makes explicit the continuous relationship between the eye and the hand itself, and 
by extension what the relationship between the visual perception and the tactile perception defining the 
very design and representation act. The ‘digital,’ from digitus, finger, makes those relationships virtual. 
This paradigm makes it clear the link between the theoretical science based on speculative observation 
represented by the eye and the techniques of material production represented by the hand. The human 
being knows what he is knowledgeable of, according to the operationalism. The hand then reminds to 
Galileo’s sense experiences, whereas the eye refers to the ability of evaluation and of rational discrimi-
nation of phenomena, therefore to the necessary demonstrations.
Digital design and parametric modeling are new medium that open up new visions. The hand allows 
to understand new shapes. The tool has changed, the message has changed. However, the images in our 
mind always refer to our Euclidean understanding. It is not possible to think of curved spaces without 
Figure 10. “Manus oculata” speculative approach in the design process

371
Computational Design and Built Environments
﻿
referencing them to a triad. The rules of our mind can not be changed. It is instead possible to see with 
the hand, with the medium. New shapes can thus be found. And digital design is yet one more step 
towards the unveiling of new horizons.
COMPUTATIONAL DESIGN, PARAMETRIC 
MODELING AND DIGITAL FABRICATION
At its most basic, parametric design represents a deep commitment to a geometrical problem as the 
centerpiece for architectural evolution. (Jason Payne)
Parametricism?
Computational design and parametric modeling go well beyond being powerful instruments that support 
today’s quest for complexity, precision and speed of contemporary architecture and design research. For 
Patrick Schumacher, they even form a ‘style’ (Schumacher, 2012).
Contemporary avant-garde architecture is addressing the demand for an increased level of articulated 
complexity by means of retooling its methods on the basis of parametric design systems. The contem-
porary architectural style that has achieved pervasive hegemony within the contemporary architectural 
avant-garde can be best understood as a research programme based upon the parametric paradigm. We 
propose to call this style: Parametricism (Schumacher, 2008).
According to Schumacher then, parametricism can only exist via sophisticated parametric techniques 
achieved through scripting and parametric modeling that he believes are becoming a pervasive reality. 
In his view complexity aims for a maximal emphasis on formal differentiation, creating compositions 
in which forms and spaces are highly integrated into a whole system.
This idea of the role of computation as an efficient tool for creating differentiation is also supported 
by Jason Payne, who argues that “Scripting is a very simple thing: an efficient way to produce differenti-
Figure 11. Architectural and engineering design process
(Marco Filippucci, 2013)

372
Computational Design and Built Environments
﻿
ated repetition in digital modeling that would otherwise require a great deal of time and effort” (Kwinter, 
2008). The concept of multiple variations actually goes back to the nineties, when it emerged to define 
the new idea of the digitally-made object, what Deleuze would refer to as “objectile” a function that 
contains an infinite number of objects (Deleuze, 1993). The objectile is not an object but an algorithm 
a parametric function which may determine an infinite variety of objects, all different (one for each set 
of parameters) yet all similar (as the underlying function is the same for all).
As it will be discussed later in this chapter, instead of focusing on one instance from a virtual series 
of many, the new technological paradigm is increasingly dealing with variations that can all be designed 
and fabricated sequentially. Mathematical continuity in this case is set in a manufacturing series, not in 
a diachronic sequence, and used to mass produce the infinite variants of the same objectile at the same 
unit cost as its identical copies (Carpo, 2011).
This idea of the use of the computer to investigate the topological variations and complexities of forms 
has been acknowledged by Antoine Picon as well, who in fact argues that “”Blobs”, folded surfaces, 
topological singularities have flourished, often giving the impression that architecture was entering a 
new baroque condition. But this morphological complexity is not the only dimension to be taken into 
consideration. [...] Confronted with a series of [contemporary] technological innovations, the only cer-
titude we have is that the change they are bringing is profound. It might prove as radical and enduring 
as the transformation that gave birth to the architectural discipline at the beginning of the Renaissance” 
(Picon, 2010).
This positive take on the role of the digital in radically shaping the evolution of architecture is how-
ever counteracted by Sanford Kwinter, who discusses of the formal consequences of the inappropriate 
use of computational instruments.
Parametric tools are doubtless among the most powerful computational devices yet to be put on the 
service of architectural production. But parametric software is not design software and its use as such 
has failed to produce objects or worlds of interest, novelty or depth. The charm of the many automatic 
processes embedded in it is undeniable but this has led many designers into the production of debilitating 
clichés. [...] The observation that it “all looks the same” has become universal (Kwinter, 2008).
Kwinter then goes on by leaning on an extreme position that nevertheless highlights a crucial aspect 
of today’s digital culture: computational tools can deeply affect the mindset of designers and their ap-
proach to design challenges. “The techniques are not solely to blame for the ugliness and clumsiness of 
contemporary architectural production, it is the minds and worldviews of those who are developing and 
using them that need to be explained” (Kwinter, 2008).
Pros and Cons of Digital Techniques
Computational and parametric techniques thus offer to designers enormous creative possibilities, but 
the fact that they are becoming ubiquitous and easy-to-use instruments can turn them into ‘dangerous’ 
modeling tools. What follows is an analysis of some of the advantages and disadvantages of the use of 
parametric software today in the realm of architecture.
Pros
1. 	
Relatively Easy Representation of Digital Spaces and Generation of Complex Forms: Parametric 
software today come with built-in libraries and tools that allow to model complex geometries in 

373
Computational Design and Built Environments
﻿
literally just a few seconds, without the need to necessarily be aware of the geometric principles 
that underline the form generation process.
2. 	
Modeling Speed for Multiple Design Variations: Parametric modeling is based on the construc-
tion of a master model that is the result of explicit relationships and connections between forms 
and elements, each controlled by geometric and mathematical parameters. If this modeling chain 
is well built, then the master model can be dynamically altered at any given point resulting in the 
quick display of design variations.
3. 	
Understanding of Complex Relationships between System Components: That fact that a para-
metric model is built upon the definition of geometric relationships between different forms or 
component allows to get a clear understanding of how each individual part relates to the whole 
system and of its role in the composition dynamics.
4. 	
Precise Control of Geometric Parameters: In parametric modeling each shape is the result of 
the definition of its geometric and mathematical parameters, which are explicitly visualized by the 
interface and adjustable by manipulating slides of numerical ranges or other quantifiable properties. 
This aspect becomes particularly useful when the structural needs sometimes require to adjust the 
3d model accordingly.
5. 	
Testing Different Design Solutions Almost in Real-Time: The possibility to dynamically change 
the values of the geometric parameters that define the parametric model allows to easily change the 
morphology of the model itself. In this way, multiple design variations can be evaluated without 
building a new model each time as it would happen in conventional modeling software.
Cons
1. 	
Modeling Takes over the Design: The main consequence of a quick and relatively easy generation 
of parametric models is that the designer is inevitably persuaded to explore multiple options for 
her project, shifting the attention from the design process to the model making. In this sense, the 
computer itself might end up driving the project development rather than the other way around, with 
the extreme result that “Many designers think forms come out of computers or movies” (Kwinter, 
2008).
2. 	
Questioning the Role of the Designer: Parametric software usually come with built-in libraries 
of geometrical shapes and structural configurations that can be manipulated and customized. From 
an outsider’s perspective these tools greatly simplify the modeling process, with the unfortunate 
consequence that not enough value is given to the design act.
3. 	
Focus on Complex Shapes Only: The incredible modeling power of computational software pushes 
for a constant quest for complexity, challenging the limits of formal articulation. As a consequence, 
the attention is mostly focused on the design of complex shapes, and unfortunately we see in fact 
just a few applications in more conventional practices. Parametric techniques can instead provide 
enormous advantages in the design of everyday buildings as well.
4. 	
Implicit Assumption That More Complex = Better: This quest for complexity brings with it an 
implicit assumption in parametric design, that is complexity is usually seen as a good quality almost 
a necessity for a project to be successful. The widespread increase of extravagant design projects 
in academia over the last few years can be regarded as the natural consequence of that assumption.
5. 	
Other Aspects of the Project Get Overlooked: In parametric modeling it is the geometric articula-
tion of the model that drives the development of the model itself. Usually though, that model is part 

374
Computational Design and Built Environments
﻿
of a design project that needs to account for other design aspects. If the project is an architectural 
intervention, then sometimes the modeling potentials make the designer less aware of, for instance, 
the project’s impact on the built environment, its relationship with the context, the user experience, 
etc.
A paradox then arises: on the one hand, computational tools allow to easily shape forms and spaces; on 
the other hand, this simplicity somehow diminishes the importance that it should instead be given to 
the design process. The exponential increase of ‘complex’ models and designs that can be seen not only 
in speculative academic environments but also in contemporary architectural practices worldwide is a 
good example of the consequences of this paradox.
Theoretical and Design Research
In this context, the authors argue that computational design and parametric modeling can be employed 
in research strategies that provide an alternative understanding of the origins of parametric design and 
of the work of great masters of the past, shed new light on representation theories, and advance research 
in formal innovation coupled with digital fabrication techniques.
Luigi Moretti’s “Architettura Parametrica”
In the investigation process on the origins of parametric design, it has emerged that, with surprising 
intuition, the Italian architect Luigi Moretti (1906-1973) had already defined in the 1940s the principles 
of what he named as “Architettura Parametrica.” Moretti in fact studied possible relations between math-
ematical, geometrical and physical parameters, and architectural forms, applying his theories in some 
innovative experiments. These studies concerned the design of three stadiums and a concert hall that 
made use of curves and surfaces of “equivisibility” as design principles. The result was the fabrication 
of working prototypes that somehow anticipated contemporary parametric approaches (Andreani, 2010).
In order to demonstrate Moretti’s research, to find correspondences with current principles of 
parametric design, and to explore innovative techniques to further evolve the architect’s projects, this 
research digitally and physically reinterpreted Moretti’s case studies. The entire design process has been 
implemented using parametric tools, computing a series of algorithms for linking mathematical equations 
and geometric constructions. Besides digital models, a physical prototype was also fabricated with 3d 
printing techniques. Once ‘computed the history,’ the research stepped ‘towards the future’ exploring 
speculative design methods for developing possible evolutions of Moretti’s stadium project.
Pier Luigi Nervi’s Columns: Flow of Lines and Forces
This research, developed by Paolo Perugini and Stefano Andreani, offers a detailed study of the columns 
in Pier Luigi Nervi’s architecture. Geometrical analyses highlight the columns’ complex formal ges-
tures, such as the recurring use of ruled surfaces, as key in meeting the essential principles of structural 
architecture. The study links geometry articulations to functional, static, technological and economic 
considerations in six projects. Nervi’s possible relationship with contemporary technological advance-
ments is also reflected upon to enhance understanding of his design approach.

375
Computational Design and Built Environments
﻿
Figure 12. Luigi Moretti’s parametric experiments and speculative design evolutions 
(Stefano Andreani, UniPg, 2011)
Figure 13. Pier Luigi Nervi (1891-1979)

376
Computational Design and Built Environments
﻿
This research ultimately shows how the dichotomy of complex geometry and structural efficiency 
finds great expression in Pier Luigi Nervi’s columns. His work researched economies of construction 
and material investigations. The study analysed the columns of six projects exploring and reflecting on 
how structural architecture principles were applied to these elements. Nervi resolved simultaneously 
geometrical, structural and constructional problems with the use of complex yet rational forms. The 
following points were found significant: 1. All columns share a strategic design logic that exploited the 
formal advantages of ruled surfaces (frequently hyperbolic paraboloids) in combination with simpler 
planar surfaces; 2. The volumes were defined with the selective use of two geometrical expedients: varia-
tion of cross-sectional shape and revolution of the principal inertia directions; 3. Nervi’s static intuitions 
led to close duality between column forms and structural behaviour at both global and local levels; 4. 
Column designs were ultimately expression of construction pragmatism, articulated through constant 
refinement of formwork technology and research of finished surfaces. The ruled geometries provided 
forming advantages and required the use of only straight planks for moulds assembly.
The elegance of the complexity given by the smooth shapes finds formal rigour in Nervi’s columns, in 
which curves had deep meanings that went beyond the mere creation of appealing architectural artifacts, 
as it is often the case in contemporary avant-garde. In this constant quest for aesthetic and efficiency, 
“form must be the necessary result, and not the initial basis, of structure,” a principle still valid in con-
temporary architecture, perhaps now more than ever (Perugini, 2013).
Ames Room
The proposed example is the well-known Ames room, constructed from its virtual appearance so that it 
appears to be, from the front, an ordinary parallelepiped-shaped room, but distorted in reality, so as to 
create an optical illusion on measure. There are two illusions associated with the Ames Room. First the 
room appears parallelepiped when viewed monocularly from a special viewing point (the true shape of 
the room is trapezoidal). Second, within an Ames Room people or objects can appear to grow or shrink 
when moving from one corner to the other.
The Ames room experiment, developed by Marco Filippucci, starts from a regular space and an 
observer. Eye and brain work as drawing, in fact particular lines are projected on a screen. This virtual 
representation, linked to the point of view, defines a visual pyramid. From this virtual representation 
the wall room is distorted holding new walls to the same virtual representation, but in the same position 
of the viewer’s eye we don’t notice any difference. So we can intersect the visual pyramid with another 
painting, rotated also with two axis, and we can find a new room shape, distorted. This experiment shows 
that the brain doesn’t notice irregular geometry, but it always judges the space as regular; so if we put 
inside this room two identical objects, in a different distance from the painting, the brain doesn’t estimate 
this condition of distance, but it notices a big one and a small one. So we can investigate on the mind’s 
capacity to abstract and its linkage with experience.
It is possible to analyze this case study with an experimental approach, exploiting the developments 
of parametric modeling that allow to clearly see the logical process and simply ensure dynamic solutions. 
Once designed the logic of the experiment, it is possible to vary the parameters so as to test the range of 
validity of the basic hypotheses. It could be interesting to describe the experiment, as well as understand 
the logical process of this geometry application, generative modeling value and flaw, parametric potential.
Practically, we can start from a simple rectangular shape, positioned in XY plane at origin of axis for 
the sake of simplicity, parametric in two side’s length (l1x,l1y). We extrude it in Z direction (l1z) and 

377
Computational Design and Built Environments
﻿
Figure 14. Columns geometrical articulations in the six case-study projects 
(Perugini, 2013)

378
Computational Design and Built Environments
﻿
Figure 15. Geometrical studies of column formwork, and site photos of the six case-study projects 
(Perugini, 2013)

379
Computational Design and Built Environments
﻿
through sliders we can control those parameters interactively. From this shape is possible to select the 
vertexes and to redraw a plane in one of those faces -to make is simple the surface is described by the Y 
vector as a normal vector, in discord sense. It’s a bit laborious this generative modeling step, because we 
want to offer the best variability of Ames room. So we have to divide uniformly the horizontal edges of 
selected face, in n-point (ex. n=10), select with item list command the same in two divisions (ex. n/2): 
this is then the construction of observer’s X-position (P_Ox). Z parameter results from the intersection 
between this connection line that defines P_Ox and XY plane in dynamic position that defines P_Oz. 
From this point it is possible to move in Y axis, so this movement defines P_Oy parameter.
Once defined the principal elements and the generated virtual image, we can draw distorted room 
by section and projection operations. From the visual pyramid it is possible to intersect it with a rotated 
generic plane. We have to define two rotation axis, vertical and horizontal, and we identify them in 
horizontal line (LO) and vertical line (LV) in P_Ox position. This line is moved from its origin to other 
point of division, so we can have the best possibility of parametrization. From the intersection between 
the Pyramid and the plane, we have vertexes of Ames room’s principal face. We have to translate this face 
behind the observer position in Y direction: it’s not important to define a particular point and also we can 
scale this surface in dynamic mode; it’s only important to not change this shape and its correspondence 
with the first face because if it diverges, it will be a paraboloid and by looking at its curvature, it shows 
signs of distorted construction.
In this way we can define the space with elements useful to help the perception of an ideal model: 
windows, distorted and parametrized in position and dimensions, whit their depth; putting inside two 
identical shapes, in the example two spheres, parametrized in dimension, positions, colors and able to 
define optical illusion: if we put the camera in the observer position, a change of the spheres’ position 
appears to us as a dimensional change. Because of this propriety, the Ames room principle has been 
widely used in TV and movie productions for special effects when it was necessary to show actors in 
giant size next to actors in small size.
Figure 16. Ames room’s digital model 
(Marco Filippucci, 2010)

380
Computational Design and Built Environments
﻿
Medial Surfaces
This study presents an innovative research on an interesting typology of complex geometries: medial 
surfaces. The models, developed by Stefano Andreani and Aurgho Jyoti at the Harvard Graduate School 
of Design, investigate the relationship between freedom of design and logic of geometry. In particular, 
they use automated topological, mathematical, and geometric systems to engage architectural issues 
such as designed topology, material efficiency, and structural rigidity through geometrical methods. The 
forms are generalizations of hyperbolic paraboloids called medial or skeletal surfaces. From a diagram 
of circulation, the surfaces that induce that circulation are calculated automatically.
By mathematical definition, all surfaces are subdivided into flat quadrilateral and triangular panels, 
easily buildable with standard sheet material. Like hyperbolic shells, these surfaces are mathematically 
defined for anticlastic curvature, with a remarkable structural rigidity as natural consequence. This project 
was particularly an attempt to generate a taxonomy of medial surfaces, starting from generatrix curves 
with increasingly complexity. The aim was to define rules for envisioning the resulting spatial forms 
before the algorithm’s application. Such experiments thus demonstrate the possibility of how multiple 
constraints of design might be exploited in the context of mathematical frameworks with provocative 
spatial results.
Innovations in Parametric Design and Digital Fabrication Workflows
The history of architecture features a combination of different technological timelines. As Mario Carpo 
points out, we can identify three technical ages: the age of hand-making, the age of mechanical-making, 
and the age of digital-making (Carpo, 2011). This distinction is of particular relevance if we think of the 
intrinsic dichotomy of architectural design: on the one hand, the built environment is based on the produc-
tion of material objects, which in turn depend on the evolution of manufacturing technologies; whereas 
Figure 17. Medial surfaces’ digital and physical models 
(Stefano Andreani and Aurgho Jyoti, Harvard GSD, 2011)

381
Computational Design and Built Environments
﻿
on the other hand, architectural design translates into abstract operations that are very much influenced 
by cultural environments and medium of representation (Andreani, 2014). The world of hand-making 
that preceded the machine-made environment was characterized by unique pieces, mainly conceived 
and produced through a process of imitation rather than replication. The first break, the transition as 
Carpo defines it “from artisanal variability to mechanical identicality,” occurred with the industrial 
revolution. But the second break in this sequence, the passage from mechanically-made identical cop-
ies to digitally-generated differential variations, is happening now. Hand-making creates variations, as 
does digital making; but the capacity to design and mass-produce serial variations (or differentiality) 
is specific to the present digital environment. Unlimited variability, however, may result in a loss of 
relevance and ultimately in a loss of meaning.
Back to the nineties, the digital revolution began to narrow down the gap between design and fabrica-
tion. Computers in fact not only could easily deliver tools for the ideation and manipulation of complex 
mathematical forms, but could also be directly applied to the production of objects. Back then Bernard 
Cache stated that “mathematics has effectively become an object of manufacture,” (Cache, 1999) and 
Greg Lynn remarked that computer-aided design had “allowed architects to explore calculus-based forms 
for the first time” (Lynn, 1999). The concept of multiple variations then emerged to define this new idea 
of the digitally-made object, and its implications on the construction realm.
This technological shift in fact defines the basic principles of a nonstandard series i.e., a set in which 
each item has some features in common with all others. It thus follows the notion of mass-customization, 
which was born as a marketing strategy well before the rise of CAD-CAM technologies (Davis, 1987). 
The term first appeared in 1987, with Stanley M. Davis’ Future Perfect book, supplying both a name 
and a conceptual framework for processes initially from the clothing industry and recognizing that mass 
customization simply extended the capabilities latent in CAD/CAM processes (Willis, 2010). In 1993 B. 
Figure 18. “Protosurfaces” exhibition at the Harvard Graduate School of Design, 2012

382
Computational Design and Built Environments
﻿
Figure 19. Taxonomical studies on medial surfaces 
(Stefano Andreani and Aurgho Jyoti, Harvard GSD, 2011)

383
Computational Design and Built Environments
﻿
Joseph Pine II expanded on Davis’ ideas articulating the production systems into three main categories: 
craft production, mass production, and mass customization – which combined elements of the first two 
(Pine, 1993).
However, in the realm of architecture, the increasing power of today’s computational tools might lead 
to discrepancies, rather than proximities, between the digital and physical worlds. As John Frazer argues 
when referring to the evolution of computational design, “We went to all this effort in order to solve real 
social, environmental and technical problems where we believed a computer could significantly assist. 
But now that there is a massive computer power and software cheaply available, most scripting has be-
come nothing more than an onanistic self indulgence in a cozy graphic environment. Endless repetition 
and variation on elaborate geometrical schema with no apparent social, environmental and technical 
purpose whatsoever” (Burry, 2011). A true paradigmatic shift in architectural design and fabrication may 
lie instead on the role of, and push for, innovation in the sense of “an activity that generates vitality,” 
as proposed by Mark Burry (Burry, 2013). A conceptual approach to innovation that highlights the role 
of the environment in affecting an idea, supporting its development, and eventually its implementation 
and dissemination,
Innovation is what actually drives the projects presented here, as an example of how research can 
promote the understanding, development and deployment of new strategies and techniques in design as 
an agent of change in the quest for a better built environment.
SmartGeometry “Ceramics 2.0”
An example of the implementation of the design-oriented research pursued at the Harvard Graduate 
School of Design is the “Ceramics 2.0” Workshop Cluster at the 2012 SmartGeometry event, where 
ceramic material systems were explored through a combination of computational design methods and a 
six-axis robotic manipulator equipped with a wire-cutting tool. Run by Prof. Martin Bechthold, Stefano 
Andreani, Jose Luis Garcia del Castillo, Aurgho Jyoti and Nathan King, the workshop built upon the 
‘Flowing Matter’ project developed at Harvard GSD (Andreani, 2012).
Going back and forth between manual and robotic clay manipulation, workshop participants developed 
design intuitions that expanded beyond what would have been feasible when limited to either physical 
or computational methods. Exploring a material system in this open-ended manner generated a host 
of powerful ideas and much discussion. The workshop demonstrated that design robotics has matured 
to a point that brainstorming and sketching are now possible in newly hybridized modes that combine 
robotics and computation with exploratory hands-on experiments (Bechthold, 2013).
Robotic Casting
Complex geometry and material system research pursued at Harvard GSD thus embraces a variety of 
technologies and material. A further investigation is the “Robotic Casting” workshop at the 2012 Robot-
ics in Architecture in Vienna, run by Prof. Martin Bechthold, Nathan King and Stefano Andreani. The 
workshop explored novel approaches to serially-customized casting processes enabled through the strategic 
deployment of 6-axis robotic manipulators. Traditional casting techniques currently limit designers to 
repetitive use of identical elements in order to distribute the cost of the molds. Serialized customization, 
mass customization or related techniques that produce highly variable, individualized design expressions 
are rarely possible, yet increasingly demanded in pursuit of contemporary architectural forms.

384
Computational Design and Built Environments
﻿
Figure 20. “Ceramics 2.0” workshop cluster at 2012 SmartGeometry, Troy (NY)

385
Computational Design and Built Environments
﻿
Robotic casting is a new casting method developed at a Harvard GSD that uses strategically designed 
molds that are oriented robotically and, when filled with variable material volumes, produce families of 
varied yet similar shapes. Studies performed during the workshop proposed modular assemblies driven 
by acoustic, lighting, views, or assembly techniques of interlocking and staggering. Robotic code was 
created with a custom Grasshopper component that outputs angle rotations and volume measurements 
for each piece. Following the digital design process, the workshop team produced, in just 14 hours, a 
doubly-curved, perforated wall consisting of 40 individual elements.
Figure 21. Floating Ceramic Shell, 2014 Cevisama, Valencia (Harvard GSD + TU Graz)
Figure 22. Floating Ceramic Shell, 2014 Cevisama, Valencia (Harvard GSD + TU Graz)

386
Computational Design and Built Environments
﻿
Figure 23. Robotic Casting, 2012 Robotics in Architecture, Vienna
Figure 24. Robotic Casting, 2012 Robotics in Architecture, Vienna

387
Computational Design and Built Environments
﻿
FUTURE RESEARCH DIRECTIONS
An architect must be a craftsman. Of course any tools will do. These days the tools might include a com-
puter, an experimental model and mathematics. However, it is still craftsmanship the work of someone 
who does not separate the work of the mind from the work of the hand. It involves a circular process that 
draws you from an idea to a drawing, from a drawing to an experiment, and from a construction back 
to an idea again. For me this cycle is fundamental to creative work. (Renzo Piano)
As highlighted in this chapter, over the last few years there has been an exponential increase of research 
efforts and innovations on computational design and parametric modeling techniques. It can be argued 
that, in terms of the generation of complex geometric forms, the potentials of computational tools have 
been largely exploited, if not exhausted.
Digital fabrication technologies give new life to those tools, and offer unprecedented opportunities 
to give physical form to virtual artifacts, challenging material constraints and structural performances. 
And perhaps the combination of parametric design with advanced fabrication meets the hopes of Sanford 
Kwinter, who back in 2008 stated that “I have faith that advanced digital design in general and parametrics 
in particular will find its way out of the bramble of technical savantism toward a more healthy, robust, 
and pragmatic deployment” (Kwinter, 2008).
But there is still much that can be done to bring computational design to the next level. Here the 
authors argue that future academic research and architectural practice development will focus on two 
main research directions.
1. Integration with Other Building Parameters in the Design Process
In the near future designers will have the chance to seamlessly integrate in the design process the whole 
spectrum of parameters that define the proprieties of an architectural project. For instance, parametric 
modeling will be deeply tight with structural optimization and material efficiency. In fact, we will be 
able to use more than one material in the same component, each responding to specific performance 
requirements. Environmental analysis will be expanded to take into account more ‘hidden’ qualities of 
both buildings and environments, and deeper connections with nature will be established.
In that sense, the work of Achim Menges clearly offers a good example of how that future might be 
closer than imagined. In the words of Kwinter, Menges “derives all his experiments from the concrete 
world of materials loaded with forces and uses the software environment to capture the geometries that 
the real world produces. [...] He naturally speaks about a ‘physiology’ of forms in the same way as a 
biologist. [...] His forms invariably begin in the world, get processed in the parametric environment and 
are redelivered and tested in the world.” (Kwinter, 2008)
Other interesting examples of future trajectories can be found in the experimental projects developed 
each year during design technology workshops promoted by the SmartGeometry organization (Peters, 
2013). The 2012 “Material Intensities” event in Troy, for instance, challenged participants with the quest 
to imagine that “the design space of architecture was no longer at the scale of rooms, walls and atria, 
but that of cells, grains and vapour droplets. Rather than the flow of people, services, or construction 
schedules, the focus becomes the flow of light, vapour, molecular vibrations and growth schedules: de-
sign from the inside out.” The goal was in fact “to dissolve our notion of the built environment as inert 
constructions enclosing physically sealed spaces. Spaces and boundaries are abundant with vibration, 

388
Computational Design and Built Environments
﻿
fluctuating intensities, shifting gradients and flows. The materials that define them are in a continual 
state of becoming: a dance of energy and information” (SG 2012).
Parametric design, material proprieties, environmental performance, and fabrication constraints 
are tightly integrated in the “[R]evolving Brick” project, developed at Harvard GSD by Prof. Martin 
Bechthold and Stefano Andreani. This study introduces the concept of strategic customization in the 
industrial fabrication context (Andreani, 2014). The project in fact integrates robotic technology on the 
production side, so that the age-old rectangular form of the brick can be successfully overcome, while 
maintaining the efficiency of tried and true mass-production methods.
The resulting mass customization of brick forms opens up a new design space in brick construction. 
In order to combine ornamental effects with sustainable design in architectural ceramic systems, this 
work also developed strategies to improve the energy efficiency of brick envelopes. In particular, by 
combining material proprieties and geometric parameters, the research shows that it is possible to optimize 
the material configuration to generate solar-selective thermal mass systems that include self-shading. 
Exploiting the advantages of the geometric complexity available through the proposed shaping process, 
the new material system merges aesthetics and environmental performance by creating design pattern 
articulations that respond to variable climatic and diurnal cycles. The resulting integrated workflow 
would eventually let both architects and manufacturers re-think the way brick building systems can be 
used, and let designers re-create novel and unexpected relationships with this traditional material.
Figure 25. [R]evolving Brick: Facade configuration, ceramic prototype and its construction document
(Stefano Andreani, Harvard GSD, 2013)

389
Computational Design and Built Environments
﻿
2. Integration with Situated Technologies for Responsive Environments
Today, more than ever, we feel the technological presence as part of our everyday life. The all-pervasive 
nature of digital information and technological interaction affects all scales from our bodies to the larger 
urban contexts we occupy and the infrastructures that support them (Sayegh, 2015). And the built en-
vironment is one of the most fascinating yet enigmatic artifacts of the human being. We perceive it as 
a complex entity resulting from the juxtaposition of spaces, flows, experiences, objects, and events.
The combination of objective, sensed data and subjective, observed data will soon allow us to observe 
the built environment as both scientists and designers, fusing quantitative and qualitative analysis into 
a hybrid form of design research. In this sense, design plays a crucial role in shaping the way in which 
people experience, engage with, and move through places, and what we don’t see is as important as we 
don’t. We should thus design not only for the ‘visible’ but also, and most importantly, for the ‘invisible.’
This is the research that the Responsive Environments and Artifacts Lab (REAL) at the Harvard 
Graduate School of Design, led by Professor Allen Sayegh with Stefano Andreani, has been pursuing. 
This research unit basically looks at how new technologies can have a profound impact on the built 
environment. Pursuing the design of digital, virtual and physical worlds as an indivisible whole, REAL 
investigates the all-pervasive nature of digital information and interaction at scales ranging from the 
individual to the city.
Figure 26. [R]evolving Brick: Strategically customised ruled-surface unit typologies 
(Stefano Andreani, Harvard GSD, 2013)

390
Computational Design and Built Environments
﻿
Figure 27. [R]evolving Brick: Principles of the solar-selective thermal mass system and heat flux results 
of physical test
(Stefano Andreani, Harvard GSD, 2013)
Figure 28. [R]evolving Brick: Ceramic prototype and robotic wire cutting simulation and fabrication 
(Stefano Andreani, Harvard GSD, 2013)

391
Computational Design and Built Environments
﻿
In the near future new situated technologies will allow to blur the boundaries between the digital 
and the physical worlds. As Antoine Oicon points out, “In connection with the rise of digital culture, 
[the architects’] main contribution may very well lie in the domain of augmented reality, that is, dealing 
with the interface between the physical and the virtual, rather than focusing almost exclusively on the 
latter.” (Picon, 2010)
The future dynamics of contemporary cities will thus likely encompass a seamless integration of smart 
objects, interactive environments, and digitally-augmented fabrication machines with the everyday life 
of citizens. As designers, it is then our responsibility to develop ecological models of implementation 
for ensuring the creation of high-quality environments in our cities.
CONCLUSION
This chapter discussed the role of computational design and parametric modeling in the realm of geo-
metric representation and architectural design. It offered an overview of current digital techniques of 
3d modeling by means of parametric software that allow articulated geometric compositions through 
relatively easy-to-use visual interfaces. With those, the designer is empowered with sophisticated tools 
Figure 29. Responsive Environments and Artifacts Lab (REAL), Harvard GSD

392
Computational Design and Built Environments
﻿
to control geometrical and mathematical parameters of forms and spaces. The study highlighted not only 
the benefits of such a modeling potential, but also the limits and dangers of a modeling act that takes 
over the design process.
A few case studies and experimental projects drawn from research pursued by the authors presented 
some innovative design strategies that combine parametric modeling and digital fabrication, as mani-
festations of the potential impact of computational techniques on the built environment. Finally, the 
authors elaborated on further evolutions of the ‘digital environment’ towards a deeper integration with 
the ‘physical world’ for more intelligent cities.
ACKNOWLEDGMENT
In a great collaboration between all the authors, it is possible attribute to Marco Filippucci the intro-
duction and Ames room description, to Fabio Bianconi the background analysis, and the other parts to 
Stefano Andreani.
REFERENCES
Alberti, L.B. (1450). De re ædificatoria. Florence (Italy),: II, 34.
Andreani, S. (2010). Computing the History toward the Future. In N. Ando et al. (Eds.), Proceedings of 
ICGG 2010 14th International Conference on Geometry and Graphics. Kyoto.
Andreani, S. (2012). Flowing Matter: Robotic fabrication of complex ceramic systems. In Proceedings of 
ISARC 2012: The 29th International Symposium on Automation and Robotics in Construction. Eindhoven.
Andreani, S. (2014). Over the Material, Past the Digital: Back to Cities. In A. Tedeschi (Ed.), AAD, 
Algorithms-aided design: parametric strategies using Grasshopper. Edizioni Le Penseur.
Andreani, S., & Bechthold, M. (2014). Revolving Brick: Informed Design and Robotic Fabrication 
Workflow for Strategic and Sustainable Mass Customization of Complex Ceramic Building Systems. 
In F. Gramazio et al. (Eds.), Fabricate: Negotiating Design and Making. Zurich: Gta Publishers.
Bechthold, M. (2013). Design Robotics: New Strategies for Material System Research. In B. Peters et al. 
(Eds.), Inside Smartgeometry: Expanding the Architectural Possibilities of Computational Design (pp. 
254–265). London: John Wiley & Sons. doi:10.1002/9781118653074.ch22
Bellini, F. (2004). Cupole di Borromini. Milano: Electa.
Burry, M. (2011). Scripting cultures: architectural design and programming. Chichester, UK: Wiley.
Burry, M., Ednie-Brown, P., & Burrow, A. (2013). The innovation imperative: architectures of vitality. 
London: John Wiley & Sons, Inc.
Cache, B. (1999). Objectile: The Pursuit of Philosophy by Other Means. In S. Perrella (Ed.), Hypersur-
face Architecture II, Architectural Design 69. London: Wiley.
Carpo, M. (2011). The alphabet and the algorithm. Cambridge, MA: MIT Press.

393
Computational Design and Built Environments
﻿
Davis, S. M. (1987). Future perfect. Reading, MA: Addison-Wesley.
de Rubertis, R. (1993). Fondamenti e applicazioni di geometria descrittiva. Roma, Italia: Kappa.
de Rubertis, R. (1994). Il disegno dell’architettura. Roma, Italy: NIS.
de Rubertis, R. (2008). La città mutante. Indizi di evoluzionismo in architettura. Milano, Italia: Franco 
Angeli.
Deleuze, G. (1993). The fold: Leibniz and the Baroque. Minneapolis, MN: University of Minnesota Press.
Di Napoli, G. (2004). Disegnare e conoscere. La mano, l’occhio, il segno. Torino, Italia: Einaudi.
Euclide, & Heath, T. L. (1956). The thirteen books of Euclid’s Elements. New York: Dover Publication.
Filippucci, M. (2010). Virtual in virtual, discretization in discretization: shape and perception in para-
metric modelling for renewing descriptive geometry. In Proceedings of ICGG 2010 14th International 
Conference on Geometry and Graphics (pp. 129-130). Kyoto.
Florenskij, P. (2005). Lo spazio e il tempo nell’arte. Milano, Italia: Adelphi.
Gay, F. (2008). Modelli geometrici per le arti prima e dopo la geometria descrittiva. In B. Aterini, & 
R. Corazzi, (Eds.), La geometria fra didattica e ricerca. Firenze, Italia: Dipartimento di Progettazione 
dell’Architettura.
Hermann, H. (1919). Demian. Die Geschichte einer Jugend von Emil Sinclair. Frankfurt am Main, 
Germany: Fisher.
Hitos. (2014). Domus. Retrieved July 13, 2015, from http://www.domusweb.it/en/news/2014/02/20/
trans_hitos_2014.html
Kwinter, S., & Payne, J. (2008). A conversation between Sanford Kwinter and Jason Payen. In T. Saka-
moto & A. Ferré (Eds.), From control to design: parametric/algorithmic architecture (pp. 219–239). 
Barcelona: Actar-D.
Livio, M. (2006). L’equazione impossibile. Milano, Italia: BUR.
Lynn, G. (1999). Animate form. New York: Princeton Architectural Press.
Maertterer, J. (2007). Math of aesthetics. In L. De Carlo (Ed.), Informatica e fondamenti scientifici della 
rappresentazione. Roma, Italia: Gangemi.
McLuhan, M. (1964). Understanding Media: The Extensions of Man. New York: McGraw-Hill.
Migliari, R. (2000). La rappresentazione e il controllo dello spazio: morte e trasfigurazione della geo-
metria descrittiva. In Disegnare idee immagini, 20-21, 9.
Migliari, R. (2009). Geometria descrittiva – Tecniche e applicazioni. Novara, Italia: Città Studi Edizioni.
Migliari, R. (2009). Disegnare nello spazio. Disegnare idee immagini, 38, 23.
Monge, G. (1798). Gèomètrie Descriptive. Paris, France: Boudouin.

394
Computational Design and Built Environments
﻿
Perugini, P., & Andreani, S. (2013). Pier Luigi Nervi’s Columns: Flows of Lined and Forces. Journal of 
the International Association for Shell and Spatial Structures, 54(176-177).
Peters, B. (2013). Inside Smartgeometry: expanding the architectural possibilities of computational 
design. Chichester, UK: Wiley. doi:10.1002/9781118653074
Picon, A. (2010). Digital culture in architecture: an introduction for the design professions. Basel: 
Birkhauser.
Pine, B. J. (1993). Mass customization: the new frontier in business competition. Boston, MA: Harvard 
Business School Press.
Regalado, A. (2013). You Must Make the New Machines. In A business report on the next wave of 
manufacturing. Cambridge, MA: MIT Technology Review.
SG2012 Material Intensities. (n.d.). Retrieved July 16, 2015, from http://smartgeometry.org/index.
php?option=com_content&view=article&id=108&Itemid=137
Sayegh, A., & Andreani, S. (in press). Embracing the glitch in search of the (extra-ordinary): Strategies 
for post-smart cities. In Proceedings of the Association for Computer-Aided Design in Architecture-
2015International Conference.
Schumacher, P. (2008). Parametricism as Style - Parametricist Manifesto. Retrieved July 13, 2015, from 
http://www.patrikschumacher.com/texts/parametricism as style.htm
Schumacher, P. (2012). Autopoiesis of architecture. West Sussex: John Wiley & Sons.
Tzonis, A. (2004). Calatrava. Opera Completa. Milano, Italia: Rizzoli.
Willis, D., & Woodward, T. (2010). Diminishing Difficulty: Mass Customization and the Digital Pro-
duction of Architecture. In R. Corser (Ed.), Fabricating architecture: selected readings in digital design 
and manufacturing. New York: Princeton Architectural Press.
ADDITIONAL READING
Bachelard, G., Jolas, M., & Stilgoe, J. R. (1994). The poetics of space. Beacon Press.
Bechthold, M. (2008). Innovative surface structures: technology and applications. Abingdon: Taylor 
& Francis.
Carpo, M. (2013). The digital turn in architecture 1992-2012. Chichester: Wiley. doi:10.1002/9781118795811
Gleiter, J. H. (Ed.). (2012). Ornament today: digital material structural. Bolzano: Bolzano University 
Press.
Gramazio, F., Kohler, M., & Langenberg, S. (2014). Fabricate: negotiating design and making. Zurich: 
Gta-Verlag.

395
Computational Design and Built Environments
﻿
Kolarevic, B. (2003). Architecture in the digital age: design and manufacturing. New York, NY: Spon 
Press.
Kolarevic, B., & Klinger, K. R. (2008). Manufacturing material effects: rethinking design and making 
in architecture. New York: Routledge.
Leatherbarrow, D., & Mostafavi, M. (2007). Surface Architecture. In W. W. Braham et al. (Eds.), Re-
thinking Technology: A Reader in Architectural Theory (pp. 394–401). London, New York: Routledge.
Menges, A., & Ahlquist, S. (2011). Computational design thinking. Chichester, UK: John Wiley & Sons.
Oxman, R., & Oxman, R. (2010). The new structuralism: design, engineering and architectural tech-
nologies. Hoboken, NJ: Wiley.
Oxman, R., & Oxman, R. (2013). Theories of the digital in architecture. New York: Routledge.
Pallasmaa, J. (2005). The eyes of the skin: architecture and the senses. Chichester: Wiley-Academy.
Picon, A. (2013). Ornament: the politics of architecture and subjectivity. Chichester: Wiley.
Picon, A. (2013). Architecture, Innovation and Tradition. In M. Burry et al. (Eds.), The Innovation Im-
perative: Architectures of Vitality (pp. 128–133). London: Wiley.
Pottmann, H., & Bentley, D. (2007). Architectural geometry. Exton, PA: Bentley Institute Press.
Zaera Polo, A. (2008). Politics of the Envelope: A Political Critique of Materialism. In C. Davidson 
(Ed.), Log 13/14 (pp. 76-105). S.I.: Anyonr Corp.
This work was previously published in the Handbook of Research on Visual Computing and Emerging Geometrical Design Tools 
edited by Giuseppe Amoruso, pages 790-824, copyright year 2016 by Information Science Reference (an imprint of IGI Global).

396
Copyright © 2017, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.
Chapter  20
DOI: 10.4018/978-1-5225-1677-4.ch020
ABSTRACT
Development of gesture interaction requires a combination of three design matters: gesture, presenta-
tion, and dialog. However, in current work on rapid prototyping the focus is on gestures taking into ac-
count only the presentation. Model-based development incorporating gestures, in contrast, supports the 
gesture and dialog dimensions. The work on ProGesture aims at a rapid prototyping tool supporting a 
coherent development within the whole gesture-presentation-dialog design space. In this contribution, 
a first version of ProGesture is introduced. Here, gestures are specified by demonstrating the movements 
or they are composed of other gestures. The tool also provides a dialog editor, which allows gestures to 
be assigned to dialog models. Based on its executable runtime system the models and gestures can be 
tested and evaluated. In addition, gestures can be bound to first presentations or existing applications 
and evaluated in their context.
1. INTRODUCTION
3D-gestures, such as touchless hand gestures and body movements are more and more used in human-
computer interaction. Although gesture controlled user interfaces have been investigated for several years, 
developing systematically intuitive and ergonomic 3D-gesture interactions is still challenging. Work in 
this field does not only aim at appropriate gestures taking into account the physiology of the human 
Towards ProGesture, a Tool 
Supporting Early Prototyping 
of 3D-Gesture Interaction
Birgit Bomsdorf
Hochschule Fulda – University of Applied Sciences, Germany
Rainer Blum
Hochschule Fulda – University of Applied Sciences, Germany
Daniel Künkel
Hochschule Fulda – University of Applied Sciences, Germany

397
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
body and the users’ goals, but also includes investigation of suitable UI widgets and presentations as a 
whole, as well as of the development process.
A gesture, according to Hummels & Stappers (1998) and Saffer (2008), is a coordinated movement or 
position of a body or parts of a body with the intent to interact with a system. This definition comprises 
dynamic gestures, that are characterized by their movements, and static gestures (also named poses), 
which are specific postures that are shown for a short while (Mitra & Acharya, 2007). Furthermore, a 
gesture may be discrete, i.e. the respective reaction of an associated object is triggered after completing 
the gesture (e.g. a thumb-up gesture to move to the next page of a form). Whereas a continuous gesture 
provides the user with simultaneous reactions, i.e. the system’s reaction is started and completed with 
the gesture performance, e.g. “pinch” or “zoom” for map interactions (Ruiz, Li, & Lank 2011). Pointer-
based interactions are typical for the continuous category. Nevertheless, from a usability point of view, 
it may be rewarding, to provide intermediate feedback for all types of gestures, independent of the suc-
cessful completion of a discrete gesture, e.g. to indicate for the user that the start of a dynamic gesture 
was recognized.
Besides deciding on gestures, feedback and their relations it has to be ascertained how to communicate 
the gestures to the user. The presentation, more precisely the perceived affordance highly impacts the 
gestures users will perform. The concept of affordances was introduced by Norman (1998) and later on 
clarified as perceived affordances. It describes a desirable property of a UI, which leads users to perform 
the correct actions to reach their goals. Hence, designing gesture interaction requires not only to develop 
the gesture set and the presentation but also to consider their dependencies thoroughly. A modification of 
a gesture set often necessitates altering the presentation, in order to indicate the new gestures to the user.
Additionally, the dialog structure determines the order of gesture execution and therefore affects 
the design of gesture interaction and vice versa. The dialog may have to be modified, for example, if a 
sequence of gestures is exhausting the user or if the planned presentation leads the user to perform the 
gestures in a different sequence.
Three, mutually dependent design dimensions can describe these different perspectives: gesture, pre-
sentation and dialog. At the same time, procedures and tools supporting a rapid prototyping of gesture 
interactions are still under investigation. Currently two approaches exist with respect to these design 
dimensions: On the one hand, work concentrating on the gesture dimension while taking into account the 
presentation, and on the other hand work, mostly model-based design approaches, focusing on gestures 
as a new, possibly additional interaction modality of a dialog. Hence, support for typical development 
activities is reduced to only two dimensions and their relationships, as yet.
In this paper our work towards ProGesture, a tool supporting rapid Prototyping of 3D-Gesture interac-
tions is introduced. The objective of its development is to cope with the resulting gesture-presentation-
dialog design space as a whole in a flexible way. It aims at the early development phases, i.e. at rapid 
prototyping of 3D-gestures in combination with first UI sketches, such as mockups. In addition, it focuses 
on dialog modeling and on testing based on executable models. Currently, different main functional 
modules are implemented as proof-of-concept tools to gain experiences and to refine the requirements. 
These modules are presented in the following by means of their central features and scenarios of use.
Prior to this, the next section takes a look at the design space spanned by the design dimensions gesture, 
presentation and dialog. Here, a coffee maker is taken as a simple example of developing gesture-based 
interactions according to the 1-, 2- or 3-dimension design (sub-)space. This is followed by a section on 
related work. Approaches from the literature on concrete procedures for gesture interaction development 
within the early phases are presented as well as tools related to ProGesture. Afterwards a short overview 

398
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
of the main functional modules of the ProGesture tool is given. Next, usage scenarios are described re-
visiting the coffee maker examples. The paper concludes with a summary of main results and an outlook
2. GESTURE INTERACTION DESIGN SPACE
The development space of gesture-based interactions is impacted by different aspects such as visualiza-
tion techniques (e.g. 2D vs. 3D), sensor technique (e.g. Microsoft™ Kinect®1 vs. Leap Motion™2) and 
gesture recognition algorithms (e.g. body movements vs. finger gestures). The axes in Figure 1 repre-
sent the design matters gesture, presentation and dialog, whereby the axis values determine the scope 
for development in each case. The more scope for design exists on each dimension the more space for 
finding a solution is available. If on the contrary the presentation, the gestures and the dialog are fixed, 
the design space collapses to one point, to the “point of no design options” (“PnD” in Figure 1a). For 
example, this nearly holds true if Kineed for Speed3 and Kinesic Mouse4 are utilized. The two tools 
enable the developer to map already implemented gestures to existing applications in order to emulate 
keyboard, mouse and joystick events. Here, a design space nearly does not exist since only the concrete 
gesture-to-event mapping can be specified but no gestures and no presentation properties.
Example 1: Figure 1b exemplifies a case in which the design space collapses to one dimension, here 
the gesture dimension. This is true if a gesture set is to be developed for an unchangeable application, 
e.g. for an existing coffee maker in a public area. The extent of the gesture design scope (distance of 
G and PnD in Figure 1b) is given by the applied sensor technology, the decision on gesture types such 
as dynamic and/ or static gestures, commitment to standard gestures (which do not exist at the moment 
of writing) etc. Since presentation and dialog are fixed providing no design space, they constrain the 
development of gestures and gesture sets, respectively.
Example 2: If the presentation is given, e.g. as a conceptual mockup by a designer, developers deal with 
a 2-axes design space (Figure 1c). The coffee maker mockup, for example, which is partially shown in 
Figure 2a, sets the presentation for selecting a coffee in one dialog step. The next step would be choosing 
the strength of the coffee (denoted in the figure by the second screenshot positioned in the background).
Eliciting gestures for the task “select coffee type” may now result in different gestures affecting the 
dialog structure. The appearance and the alignment may suggest a slide gesture to put the virtual selec-
tion focus on the coffee of choice (perceived affordance). The respective dialog is shown in Figure 2b 
Figure 1. 3D-gesture interaction design space

399
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
by means of a state transition diagram. The presentation, however, may also suggest a pointing gesture 
(deictic gesture) by which users can directly select a type of coffee. Deciding for this gesture results in 
a modified dialog structure (Figure 2c), now with transitions between each of the three states. This may 
correspond with the users’ expectation not to have to make a detour to cappuccino if they want to select 
a coffee instead of an espresso and vice versa, when using pointing gestures.
Gestures developed at first may be modified during the development because, for example, if the 
gestures are exhausting the user. The presentation may have to be modified as well due to the demand 
for perceived affordances. All in all, designing gesture interaction requires a combination of the three 
design matters.
Example 3: All of the three mentioned axes span the design space if a gesture-controlled application 
is developed from scratch (Figure 1a). Figure 3 shows ideas of alternative designs for a coffee maker. 
In contrast to the example above the user is able to select coffee type, sugar and milk in a single dialog 
step, i.e. within a single presentation view.
It is open which composition of the sugar and milk fields would be favored. Small changes to the layout 
could impact eliciting gestures from users, and possibly the dialog. The proposal on the left in Figure 3 
suggests selecting the coffee type first, afterwards sugar, followed by milk and at last to confirm these 
Figure 2. Given presentation mockup and related dialog models

400
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
choices. Even if no sugar is wanted, the user has to pass over the sugar field (comparable to the dialog in 
Figure 2b). This is not the case in the presentation on the right hand resulting in a modified dialog model.
In the case of these two presentation designs, one possible interaction concept may be to hold the 
gesture performing hand in front of the body while making all of the selections, one after the other, from 
the first step up to the confirmation. This could be a sequence of, e.g. wipe gestures, each in the direction 
of the desired product option. But then, additional gestures may be rewarding, e.g. a pause gesture or a 
set gesture to fix a selection so that the user is able to relax the hand. Such a decision may also affect 
the UI presentation as well as the UI dialog.
Developers, while prototyping 3D-gesture controlled UI, have to “move” within the gesture-presen-
tation-dialog design space in a flexible way. They may, for example, draft the presentation as well as 
the dialog based on which they develop gestures in a subsequent design step. In the next moment, they 
may decide for a gesture set and redesign the presentation and dialog to conform better to these gestures. 
The development requires small iteration steps while following a user-centered design approach taking 
into account the mutual dependencies of presentation, gesture and dialog.
3. RELATED WORK
This section presents work related to rapid prototyping of gesture-based UI. First of all, concrete devel-
opment procedures are referenced and roughly classified. Additionally, tools for rapid development and 
prototyping of 3D-gesture interactions are reflected.
Figure 3. Two versions of a design with only one dialog step, i.e. within a single presentation view

401
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
3.1. Procedures
Concrete procedures for developing gestures can be classified based on different criteria. To begin 
with, the approaches referenced in this contribution can be distinguished according to the gestures’ 
origin, which is basically the user or the designer. In the case of user-elicited gestures, users are asked 
to demonstrate gestures (Hummels & Stappers, 1998; Rapp & Strube, 2002) that are, e.g. videotaped 
and analyzed afterwards. If the origin is the design team (designer-determined gestures), e.g. (Gerling, 
Livingston, Nacke, & Mandryk, 2002), gestures are based on the designers’ knowledge and experiences 
as well as on literature and guidelines. However, currently available guidelines are mostly restricted to 
specific technology, e.g. the Microsoft™ Human Interface Guidelines5 (HIG) or Intel® HIG6. The two 
approaches, eliciting and determination, may also be combined, such as in (Rupprecht, Blum, & Boms-
dorf, 2013). Both the user-elicited gestures and the designer-determined gestures are usually evaluated 
with user tests.
A well-known technique applied to gesture elicitation and evaluation is the Wizard of Oz (WOZ) 
experiment (Jonker, 2008; Beringer, 2002; Höysniemi, Hämäläinen, & Turkki, 2004; Rupprecht et al., 
2013), originally developed by Kelley (1983). The advantage is that gestures do not have to be imple-
mented beforehand because a human behind a curtain, the so-called wizard, emulates the functionality 
of the system providing users with feedback.
In addition to the gestures’ origin the development procedures can be differentiated in terms of techni-
cal limitations that impact the resulting gestures. Nielsen, Störring, Moeslund, & Granum (2004) contrast 
technology-based with human-based approaches. In the first approach, gestures are implemented before 
being evaluated (in contrast to a WOZ-experiment). Identified gestures are therefore constrained by cur-
rent technology resulting in solutions that may be undesired from the user perspective. Modifications, 
however, are costly if at all practicable. In human-based approaches gestures are user-elicited, i.e. users 
are asked to demonstrate gestures that should be implemented (e.g. Rapp & Strube, 2002). This time, 
however, users may want gestures that are not realizable by up-to-date technology.
ProGesture, the tool presented in this paper, supports rapid prototyping of 3D-gesture interactions 
allowing a combination of the two approaches. Thus, we call this mixed approach human- & technology-
based. The user demonstrates gestures in front of a Kinect® sensor, taking into account the constraints 
given by the employed gesture recognition algorithms. Hence, technical limitations are considered but 
gestures are still user-elicited – and currently realizable at the same time.
Another mixed approach, but this time combining elicitation and determination of gestures, is given 
by the theater-system technique by Schieben, Heesen, Schindler, Kelsch, & Flemisch (2009), affirmed by 
Mahr, Endres, Müller, & Schneeberger (2011). It is a WOZ-experiment in which gestures are predefined 
but single parameters (e.g. the radius of circle gestures, the time of motions for dynamic gestures) are 
left open and are adjusted by users performing the gestures. The advantage is that predefined gestures 
are evaluated and selected parameters for subsequent gesture implementation are collected within the 
same step.
A further differentiation of gesture development approaches is based on the given input. In the work 
referenced above it can be distinguished whether only functions of the target system or prototypes are 
taken into account. If only functions are given, according to the above-defined gesture design space, no 
dialog and no presentation are considered. When working with prototypes the presentation is given, but 
no dialog is regarded (explicitly).

402
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
However, none of the mentioned concrete approaches addresses the mutual dependencies of gestures 
and presentation (perceived affordance). Even though their focus is on the gesture-presentation design 
space, an iterative development that thoroughly considers the dependencies is still missing. ProGesture, 
by contrast, meets this need: Gestures are mapped to events, and the presentation reacts to these events. 
However, the connection of the presentation is established on a rather technical level, yet, via socket 
communication (WebSocket protocol). ProGesture does not provide means to generate a presentation, 
but, instead, to link UI sketches or elaborated UI created outside of ProGesture to the dialog model 
inside ProGesture. Arbitrary UI tools can be used to build the presentation, as long as WebSocket com-
munication is possible. With this concept, modifications in the gesture-presentation design space can 
be undertaken easily.
Model-based UI development, like the works above, follows a user-centered approach in engineering 
interactive systems. State-of-the-art works often favor the Cameleon framework7. It defines the modeling 
layers Task & Concepts, Abstract User Interface (AUI), Concrete UI (CUI), and Final UI (FUI). The 
AUI is assumed to be independent of any modality of interaction while the CUI copes with modality. 
The idea is to systematically transform AUI into CUI. This is valuable once there is a common agree-
ment on interaction objects and related interactions. In the area of 3D-interactions this commitment does 
not exist yet. Furthermore, as the analysis of concrete procedures has shown, finding gestures is mostly 
not related to abstract UI models but to concrete presentations, e.g. in the form of first UI sketches. In 
model-based design an explicit dialog specification is developed in general before the presentation. 
Thus, gestures are systematically tested in the context of formal dialog models. Nevertheless, different 
tools have demonstrated the advantages of testing and prototyping, respectively, by means of executable 
models (see next section). ProGesture provides concepts to support testing and prototyping based on 
both, on models and on concrete, specific UI sketches.
3.2. Tools
Model-based design tools more and more support early testing and prototyping based on executable 
models. In tools such as presented in (Biere et al., 2002; Mori et al., 1999; Reichart et al., 2004) user 
actions are simulated by activating buttons, while animation of the model diagrams visualizes system 
reactions in place of UI representations. This allows developers to evaluate a design while concentrating 
on the AUI independently of concrete interactions and modalities.
Evaluating real gesture performance and sequencing is of vital importance once gestures are identi-
fied. Therefore, ProGesture enables both, indicating gesture actions by means of buttons and, similar 
to (Feuerstack, Anjo & Pizzolato, 2011), by executing real gestures to control the executable model. In 
(Feuerstack et al., 2011) gestures (though in contrast to ProGesture only poses, no movements, e.g. for 
dynamic gestures) and dialog models are specified separately. The gesture recognition is subsequently 
linked to the executable dialog model in an explicit modeling step. In ProGesture first steps towards an 
integrated tool are implemented aiming at specifying gestures during a test run and using them in its 
next step as user action. All in all, the tool presented in (Feuerstack et al., 2011) as well as ProGesture 
support designers to cope with the gesture-dialog design space. Furthermore, ProGesture enables to test 
elicited gestures in the context of first UI sketches, as proposed in former work of the authors (Bomsdorf 
& Szwillus, 1996).

403
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
Another approach of early testing is revolutionary (also called throw-away) prototyping, e.g. mockups. 
Respective tools are extended more and more to include touch gestures, e.g. proTACT8 and proto.io9. 
The focus is on presentation and interactions, i.e. on the gesture-presentation design space. The dialog 
is specified implicitly by means of connected, interactive areas by which the user can move within and 
between presentation units. ProGesture, in contrast, enables to test real-time gestures based on executable 
dialog models in the context of presentations to realize the gesture-presentation-dialog design space.
Explicit, formal dialog models are open to verification of specific properties. The same holds true 
for formal descriptions of 3D-gestures. Different gesture formalisms exist for the specification of poses 
and body movements (e.g. Loke, Larssen, & Robertson, 2005; Vilhjálmsson et al., 2007), which facili-
tate integration of gesture specification into the model-based approach. In GestIT (Spano, Cisternino, 
Paternò, & Fenu, 2013) basic gesture fragments, typically made up of gesture events of the employed 
sensor device, are incorporated as given, as so-called “ground terms”. With GestIT’s notation, these 
ground terms are composed to higher-level gestures and gesture sets, using temporal and composition 
operators, and connected to a formal specification of a UI. For this purpose, dialog feedback is bound 
to the leafs and nodes of a modelled decision tree. ProGesture uses the well-known Harel State Charts, 
though not all required concepts are implemented yet, e.g. parallel states. With this approach, as in Ges-
tIT, also intermediate states during the gesture performance can be handled (if delivered by the gesture 
recognizer), in order to provide relevant feedback that adapts accordingly: Each intermediate state can 
be modelled as a separate state.
Detailed formal gesture specifications enable animation of the gestures (Wilke, Calvert, Ryman, & 
Fox, 2005) supporting evaluation from the perspective of ergonomics. Current animation tools, however, 
do not incorporate the dialog or the presentation. Also the work presented in (Feuerstack et al., 2011) 
necessitates video recording of users performing a gesture to analyze the movements afterwards. Pro-
Gesture implements the by-demonstration concept. Hereby, gestures are recorded and are immediately 
available for evaluation purposes – via real gestures in action as well as via animations.
Furthermore, in ProGesture, recorded gestures can be assigned to both dialog and presentation. The 
tool presented in (Lü & Li, 2012) follows a similar approach but considers touch gestures. There are, to 
the authors’ knowledge, only a few tools supporting the specification of 3D-gestures by-demonstration. 
With the Kinetic Space Tool10 a gesture is performed only one time and can be used afterwards by dif-
ferent persons. The tool can be linked to another application by a given communication protocol. With 
this mechanism, it would be possible to utilize the gestures in combination with executable models. 
Meanwhile, an integration that enables to switch directly between, e.g. gesture specification and dialog 
modeling is not possible, because the linkage of gestures and executable model would require program-
ming (also for every modification) and, besides would only function in one direction, from gesture de-
sign space to presentation. In addition, the tool’s gesture recognition failed too often in our tests. Omek 
GAT11, which is no longer available, has realized the by-demonstration concept as well. Specifying 
gestures demanded several repetitions, around 30 performances were recommended. Such an approach 
is cumbersome particularly when it comes to rapid prototyping in small iterations. LeapTrainer.js12 also 
implements the by-demonstration concept. It allows developing gestures to interact with websites using 
the Leap Motion™ Controller. Mapping of the gestures to functions and to the dialog design space or 
even developing within this space is not supported. For this purpose additional tools (e.g. programming 
tools) are necessary. However, in contrast to Omek GAT the gesture training demanded only one, or 
only a few more repetitions for the sake of better recognition, in our tests.

404
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
The Gesture Recognition Toolkit13 (GRT) claims to support any type of sensor or data input. It provides 
different algorithms for the training of user-specific gestures. Thus, unlike ProGesture, it is possible to 
test gestures for multiple sensors and with different algorithms and to select the most appropriate solution.
With the Flexible Action and Articulated Skeleton Toolkit (FAAST) by Suma et al. (2013) gestures 
are specified via building blocks. Each block represents a gesture or parts of a gesture from which new 
gestures can be composed. Afterwards the gestures can be used to emulate keyboard and mouse events 
using the Kinect® sensor. ProGesture, in addition to demonstrating gestures, also supports such a speci-
fication of gestures by means of building blocks.
4. OVERVIEW OF PROGESTURE
This first version of ProGesture is intended as a proof-of-concept tool. In order to identify relevant re-
quirements, practitioners interested in gesture interaction were involved from the beginning. It became 
apparent that it is not possible to raise a complete and thorough set of requirements at the moment due to 
the novelty of the topic and related uncertainties concerning good practice and design methods. There-
fore, the tool’s current version serves also as an experimental demonstrator to collect more precise and 
detailed requirements on how to develop ProGesture further as well as to investigate methodic aspects 
of the user-centered design process.
The tool is basically structured into three modules that are presented below: (1) gesture editor, (2) 
dialog editor and (3) model simulator. The gesture editor and the dialog editor are related to the gesture 
dimension and to the dialog dimension, respectively. The presentation dimension, by contrast, is cov-
ered by creating UI sketches or elaborated UI outside of ProGesture and linking them within it to dialog 
models and thus to gesture specifications.
The model simulator allows testing and analyzing the dialog model in three different ways: at its 
own, in conjunction with gestures as well as in conjunction with gestures and presentation (involving 
all three dimensions).
A further module (4) implements functionalities for controlling third party software by body gestures. 
We named it emulation module as its primary purpose is to emulate keyboard, mouse and joystick events. 
In the context of this paper it is interesting to mention, that by this module gestures can also be directly 
linked to presentations and used immediately afterwards. Hereby it enables iterative development within 
the gesture-presentation design space, i.e. without taking into account a dialog model. Furthermore, 
gestures may be specified in two ways: First, the module contains some kind of own gesture recognizer, 
that supports gestures composed of specific given gesture building blocks, as well as pointer control 
(continuous gesture-interaction). Second, gestures can be specified module-externally by means of the 
ProGesture gesture editor and imported into this module. Basically, the emulation module contains 
functionalities to specify gestures, to assign them to an application and to interact with the application 
by means of these gestures. Such an application may be, for example, a mockup. Therefore, the emula-
tion module enables rapid prototyping within the gesture-presentation design space. It works similarly 
to FAAST (Suma et al., 2013) and covers additional features that are also included in the gesture editor, 
dialog editor and model simulator. Therefore, the emulation module is not detailed by the subsequent 
presentation.

405
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
From the functional and software architectural points of view the four modules are loosely coupled 
but work together closely. We strive to design generic, non-specific module interfaces, in order to enable 
replacement of each of the modules by a similar software package. The aim is to be able to integrate 
future developments, also from third parties.
4.1. Gesture Editor
The gesture editor provides functionalities for recording and editing gestures and organizing them in 
gesture sets. A gesture is specified by demonstrating it in front of a sensor. Hence, gestures are not 
described explicitly using a specific human-readable notation, but as frame sequences comparable to 
video clips. The system currently employs the Microsoft™ Kinect® sensor and works with the skeleton 
data delivered by the Kinect® SDK. Consequently, each frame contains the separate positions of all the 
skeleton joints that are of relevance for the respective gesture. This frame sequence is accompanied by 
specific parameters for its interpretation, e.g. the tolerable deviation from the “ideal” pose or movement. 
Therefore, gestures are not explicitly modelled in ProGesture. 
Integral part of the gesture editor is a gesture recognizer implemented by our team. It is based upon 
the Dynamic Time Warping (DTW) algorithm (Reyes, Dominguez, & Escalera, 2011) that is able to 
eliminate temporal variations when comparing two gesture sequences. With this approach, the ProGes-
ture gesture recognizer is able to identify static and dynamic gestures, both in the “discrete gesture” 
category. Any sequence of these gestures can be combined, defined and detected as a specific distinct 
gesture (aggregation of sub-gestures). Continuous gestures are not supported directly, but, provisionally, 
may be substituted by a series of discrete dynamic gestures.
The gesture recognizer is realized as an independent library and can therefore be employed to use 
recorded gestures in other applications. Actually, all ProGesture modules use it.
The gesture editor provides first features for editing and testing of gestures supporting iterative de-
velopment within the gesture design space. When the user repeats a movement, the editor reports the 
recognized gesture (Figure 4a). Furthermore, recorded gestures can be played in a so-called gesture 
player (Figure 4b) in order to analyze the skeleton movements and cut the frame sequence as needed. 
Additionally, the skeleton joints that are of relevance for the respective gesture can be selected, at that 
time or earlier before starting the recording (Figure 4c). They are highlighted in the gesture player. For 
example, for a typical wipe gesture of one hand the positions and movements of head, legs and feet and 
even of the other arm are not of relevance and should be excluded from the gesture recognition process.
4.2. Dialog Editor
The dialog editor supports the specification of dialog models by means of the well-known Harel State 
Charts, though not all concepts are implemented yet. Recorded, discrete gestures are assigned to state 
transitions, together with additional information relevant for the dialog sequence, i.e. constraints and 
feedback such as highlighting of a chosen option. The gestures to be assigned to transitions can be se-
lected from a gesture set. In the case a gesture is not recorded yet and to be specified later on it can be 
added to the dialog model by a representative name. In doing so, the dialog model is already executable. 
Therefore, gestures may be specified before, during or after the dialog modeling allowing the developer 
to shift the design focus arbitrarily within the gesture-dialog space.

406
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
From the view of the dialog model, the following gesture types can be used: Discrete gestures are 
supported in both their forms, poses and dynamic gestures, analogous to the capabilities of the ProGes-
ture recognizer. Continuous gestures, as a workaround, can be realized with movement segmentation, 
i.e. series of discrete dynamic gestures. Each of them would be bound to a distinct transition. One idea, 
though not implemented in ProGesture yet, is to encapsulate the involved series of states as some special 
“groups of intermediate states”. The same mechanism is able to provide intermediate feedback for all 
types of gestures, during gesture performance.
4.3. Model Simulator
The model simulator is based on an executable model that is derived from the dialog model. It allows 
the simulation of the dialog to analyze dialog paths, gesture sequences and system reactions in differ-
ent scenarios and in different situations. In a specific state of the model, all those gestures are allowed 
that are attached to its out-going transitions. If one of these gestures occurs, the respective transition 
is triggered. The execution of the transition involves processing of any constraints and initiation of the 
presentation feedback via WebSocket method invocation.
Dialogs can be tested in conjunction with or without the assigned presentation. Additionally, model 
execution can be triggered either by mouse clicks or by real gestures.
Thus, the model simulator provides various options for the test and evaluation of 3D-gesture applica-
tions, based on combinations of its diverse features, such as:
• 	
Gesture actions can be simulated with mouse clicks on the graphical representation of the gesture 
within the diagram (Figure 5a). All outgoing transitions of an active state are listed in a separate 
Figure 4. Screenshot of the gesture editor’s main window

407
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
window panel (Figure 5b). Within this panel, gesture actions can be triggered by mouse clicks on 
buttons labeled with gestures names (e.g. “increase”, “next” etc. in Figure 5).
• 	
The gesture player introduced above is also integrated into the simulator (Figure 5c). Developers 
can use it to recall gestures as well as to analyze and discuss the defined body movements in the 
context of complete interaction and gesture sequences, respectively.
• 	
The dialog model can alternatively be traversed by executing the gestures in reality, e.g. for evalu-
ating physical effort. Feedback is given with a live skeleton view (Figure 5d) while the gesture 
recognizer works in the background.
• 	
The presentation can be connected, currently via simple network communication (WebSocket 
protocol). This enables interaction with the presentation, e.g. based on real gestures, to evaluate 
the whole user experience, also taking into account the presentation feedback.
• Switching to the gesture editor existing gestures may be modified or replaced by a new version and 
used instantly inside the model simulation.
• A history function stores model simulation sequences, i.e. states traversed and constraints changed 
(Figure 5e). For subsequent analysis, they can be played back together with the involved gestures.
5. EXAMPLE SCENARIOS OF USING THE PROGESTURE TOOL
The previous section outlined prototyping features of the simulator, the editors and also of the emula-
tion module that was shortly mentioned. This section provides insights into the usage of ProGesture by 
means of three scenarios revisiting the coffee maker examples (cf. section 2).
Figure 5. Dialog editor in model simulation mode

408
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
5.1. Scenario 1: Developing Gesture Interaction for an Existing Application
In scenario 1 a gesture set is to be developed for an existing application, e.g. for the coffee maker of 
example 1. This may be the case if we want to augment an existing maker in a public area with a ges-
ture recognition module, but want to avoid effort for a new GUI (or if modifications of the UI are not 
possible for some reasons). By consequence, the presentation and dialog are fixed resulting in a design 
space with only one dimension, which is here the gestures.
Let us assume that in this scenario a developer’s first step is to analyze the given UI with respect to 
the presentation’s units and transitions between them as well as to the system feedback. As a result, the 
corresponding dialog is documented with the ProGesture dialog editor, as far as required for gesture test 
purposes. In addition, the model is linked with the presentation units, the latter in the form of a simple 
mockup including photos of the real coffee maker’s presentation. In this scenario, we assume that the 
existing UI is composed of a text display listing the available options with hardware buttons close-by 
to make the desired selections.
Now, the developer specifies gestures with the ProGesture gesture editor, taking into account the 
existing coffee machine UI. As in this scenario, he has to replace pushing hardware buttons by gestures, 
he decides for semaphoric gestures. A semaphoric gesture is precisely designed to designate one specific 
symbol within a given alphabet (Quek et al., 2002), e.g. in this scenario “chose option 1” (showing one 
finger), “… option 2” (two fingers) and “… option 3” (three fingers). Here, this gesture type is used to 
express and confirm the coffee type selection in one step.
After assigning these gestures to the dialog model, the setting can be tested interactively based on the 
ProGesture executable model – using the real gestures. Problems may be identified, e.g. in the sequenc-
ing of the gestures and prospective solutions can be devised. Spontaneously, gestures can be modified or 
replaced by newly recorded ones using the gesture editor, and then used instantly within a follow-up test.
5.2. Scenario 2: Developing Gesture Interaction for a Given Presentation
In scenario 2, which refers to example 2, only the presentation is given, as a mockup by a designer: This 
coffee maker mockup, of which a selected part is shown in Figure 2, constrains the presentation for 
choosing a coffee type. Here, developers deal with a 2-axes design space, requiring a design solution 
concerning gestures and the corresponding dialog.
Let us assume the developer again starts with analyzing the given UI, though this time represented 
by sketches without included concrete dialog behavior. Then, with the ProGesture gesture editor a set 
of gestures is compiled, whereby the developer can choose gestures from existing sets or record new 
ones. The mockup, as pointed out for example 2, may suggest a wipe gesture to make a selection. In this 
scenario, as a start, the designer decides for this option. The corresponding dialog model (cf. Figure 
2b) can be specified with the dialog editor either previous to the gesture specification, in parallel or 
afterwards. Then, the presentation is connected to the dialog model and an interactive test of the “look 
and feel” of the chosen 3D-gesture interaction is conducted instantly.
Possibly, as already mentioned in section 2, it would be worth comparing an alternative: The mockup 
may also suggest a pointing gesture by which users select a type of coffee directly. This implies other 
gestures and a modification of the dialog structure (cf. Figure 2c) – both of which is effectively done with 
the gesture and dialog editor. Switching between both design versions to compare them is easy and fast.

409
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
5.3. Scenario 3: Developing from Scratch
The last scenario exemplifies the development from scratch of a 3D-gesture controlled application (cf. 
example 3 in section 2). As no design presets or limitations are given, the design space spans all of the 
three axes. This time our developer starts to investigate gestures and corresponding presentation clues, 
e.g. widgets, based on the user tasks involved when using the respective coffee maker.
The developer, with a specific gesture concept in mind, generates one or several presentation mockups 
using an arbitrary tool, e.g. a UI prototyping software. Figure 3 shows two versions of possible designs. 
Here, the central idea is, in contrast to the example in scenario 2, to let the user select coffee type, sugar 
and milk in a single dialog step. Since the two drafts differ in the sequence of selecting sugar and milk, 
two slightly different dialog models have to be specified. This scenario does not imply a specific order of 
dealing with the three design dimensions – on the contrary, the designer can switch his focus arbitrarily.
At last, having connected the dialog model to the mockups, both versions are ready to be tested. As 
argued for example 3, the tests may unfold, that additional gestures may be needed to improve the us-
ability. The resulting modifications for the dialog model and the gesture set as well as the subsequent 
evaluation of the new design can be realized quickly within ProGesture.
6. SUMMARY AND OUTLOOK
Working on ProGesture, we investigated existing tools and concrete procedures applied to rapid proto-
typing of 3D-gesture interaction to gather requirements for appropriate tool support. It became apparent 
that such work primarily focuses on the development within the gesture design space only, within the 
gesture-presentation design space, or within the gesture-dialog design space. Each of these three sub-
design spaces is challenging. An iterative development within the whole gesture-presentation-dialog 
design space taking into account the mutual dependencies is even more challenging.
Developers need to develop from different dimensions, arbitrarily combining dimensions and changing 
perspectives in a flexible way since it is too complex to develop gesture interaction taking into account 
all three dimensions at the same time. They may start with a prototype of a UI and then develop gestures 
(design space is given by the gesture dimension). While testing they may encounter problems with single 
gestures. In ProGesture a person just has to demonstrate alternative gestures and shortly afterwards can 
use them to interact with the UI draft. In the case these gestures do not match the presentation (notice-
able e.g. from issues with perceived affordances) and/or dialog behavior, the developer can alter the UI 
design and assign the gestures to it (acting in the presentation-dialog design space).
Although we argue for tools that enable such a coherent development, we are aware that a lot of 
questions are still open. For example, future experiences must show when and how designers change 
perspectives appropriately and which support is necessary in different development steps. Thus, we ap-
proach ProGesture by means of different proof-of-concept developments to gain more experiences and 
to get feedback from industrial partners of the project (within which the tool was developed), as well 
as from an involved ergonomics expert. One of the main insights so far is the importance of including 
mockups in eliciting and testing gestures, particularly if the development should result in a novel, in-
novative design solution with high usability. In such situations, mockups are vital in enabling users to 
imagine and understand the new concepts in question. In order to get further feedback for ProGesture, 
we plan to continue with systematic user evaluations.

410
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
We investigate two directions of assigning gestures to UI drafts (prototypes or mockups): The gesture 
can be assigned to a dialog model that is interlinked with a UI or gestures are linked directly with a UI 
without incorporating an explicit dialog specification. At the same time, ProGesture does not only support 
evaluation of gestures in the context of first UI drafts but up to a final UI. The tool can be used within 
Wizard of Oz experiments and additionally supports a model-based development approach. It enables 
to test real as well as simulated 3D-gesture interactions based on executable dialog models.
The concept of executable models was implemented to support the transformation from early models 
and prototypes to final code of target applications. Currently, the dialog editor and the model simulator 
are fully integrated into a coherent tool. This tool, the gesture editor and the additional emulation module, 
that covers the gesture-presentation design space, are coupled by shared components. Subsequent work 
will aim at the integration of all of them.
The gesture recognizer used in ProGesture can also be utilized by a target application. In such a case 
only those user-elicited gestures are added to a gesture set that can readily be utilized within an imple-
mentation, strengthening our human- & technology-based approach of gesture interaction development.
The emulation module (gesture-presentation design space: direct connection of gestures and presenta-
tion without dialog model) is currently the only one that supports continuous gesturing for the provision 
of a virtual UI pointer (e.g. mouse emulation). By contrast, in the other three modules, gesture editor, 
dialog editor and model simulator, the exact coordinates within the interaction space are not taken into 
account, yet. Therefore, deictic or manipulative gestures, “that require a tight relationship between the 
actual movements of the gesturing hand/arm with the entity being manipulated” (Quek et al., 2002, p. 
172) are not considered there.
But, one of the next steps in our work on ProGesture is to extend the range of gesture types, most of 
all continuous gestures. However, it is open to which extent a prototype should simulate intermediate 
feedback for gestures. As already mentioned, in such a case the gesture’s course and the system feedback 
have to be coordinated closely. Realization of this may be costly and laborious to specify, especially 
on the part of UI prototypes, and thus contradicting the idea of rapid prototyping. A possible solution 
in the context of early rapid prototyping could be the above-mentioned movement segmentation and 
introduction of intermediate states.
A further direction of our work is to integrate existing usability suites and annotation tools offering 
possibilities for collecting test data and analysis. Options to annotate certain areas in media data (Kipp, 
2010), for example, are valuable features to indicate gesture interactions for later in-depth analysis.
We also investigate methodic aspects of the development process as well as further applications of 
ProGesture. One objective is to refine the underlying requirement specification of tool support for early 
prototyping of 3D-gesture interaction and to realize additional requirements in a follow-up version of 
ProGesture.
ACKNOWLEDGMENT
We thank Sebastian Hesse, Patrick Heinz and Dmitry Orlov for the implementation of main parts of the 
ProGesture tool.

411
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
REFERENCES
Beringer, N. (2002). Evoking gestures in SmartKom-Design of the graphical user interface. In Ges-
ture and Sign Language in Human-Computer Interaction (pp. 228–240). Springer Berlin Heidelberg. 
doi:10.1007/3-540-47873-6_25
Biere, M., Bomsdorf, B., & Szwillus, G. (1999). The Visual Task Model Builder.Proceedings of the third 
international conference on Computer-aided design of user interfaces (pp. 245-256). Kluwer Academic 
Publishers. doi:10.1007/978-94-011-4295-3_20
Bomsdorf, B., & Szwillus, G. (1996). Early Prototyping based on executable task models. Proceed-
ings of the Conference Companion on Human Factors in Computing Systems (pp. 254–255). ACM. 
doi:10.1145/257089.257308
Dorau, R. (2011). Emotionales Interaktionsdesign. Emotionales Interaktionsdesign: Gesten und Mimik 
interaktiver Systeme. Springer-Verlag Berlin Heidelberg. doi:10.1007/978-3-642-03101-4
Feuerstack, S., Anjo, M. D. S., & Pizzolato, E. B. (2011). Model-based design and generation of a 
gesture-based user interface navigation control.Proceedings of the 10th Brazilian Symposium on on 
Human Factors in Computing Systems and the 5th Latin American Conference on Human-Computer 
Interaction (pp. 227-231). Brazilian Computer Society.
Gerling, K., Livingston, I., Nacke, L., & Mandryk, R. (2012). Full-body motion-based game interaction 
for older adults.Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 
1873-1882). ACM. doi:10.1145/2207676.2208324
Höysniemi, J., Hämäläinen, P., & Turkki, L. (2004). Wizard of Oz prototyping of computer vision based 
action games for children.Proceedings of the 2004 conference on Interaction design and children: build-
ing a community (pp. 27-34). ACM. doi:10.1145/1017833.1017837
Hummels, C., & Stappers, P. J. (1998). Meaningful gestures for human computer interaction: beyond 
hand postures. Proceedings of theThird IEEE International Conference onAutomatic Face and Gesture 
Recognition (pp. 591-596). IEEE. doi:10.1109/AFGR.1998.671012
Jonker, J. (2008). Wizard of Oz for gesture prototyping [Unpublished master’s thesis]. University of 
Twente, Netherlands.
Kelley, J. F. (1983). An empirical methodology for writing user-friendly natural language computer 
applications.Proceedings of the SIGCHI conference on Human Factors in Computing Systems (pp. 193-
196). ACM. doi:10.1145/800045.801609
Kipp, M. (2010). Multimedia annotation, querying and analysis in ANVIL. Multimedia information 
extraction, 19.
Loke, L., Larssen, A. T., & Robertson, T. (2005). Labanotation for design of movement-based interac-
tion.Proceedings of the second Australasian conference on Interactive entertainment (pp. 113-120). 
Creativity & Cognition Studios Press.

412
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
Lü, H., & Li, Y. (2012). Gesture coder: a tool for programming multi-touch gestures by demonstration.
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 2875-2884). ACM.
Mahr, A., Endres, C., Müller, C., & Schneeberger, T. (2011). Determining human-centered parameters 
of ergonomic micro-gesture interaction for drivers using the theater approach.Proceedings of the 3rd 
International Conference on Automotive User Interfaces and Interactive Vehicular Applications (pp. 
151-158). ACM. doi:10.1145/2381416.2381441
Mitra, S., & Acharya, T. (2007). Gesture recognition: A survey. IEEE Transactions on Systems, Man, 
and Cybernetics, Part C: Applications and Reviews, 37(3), 311–324.
Mori, G., Paternò, F., & Santoro, C. (2002). CTTE: Support for developing and analyzing task models 
for interactive system design. IEEE Transactions on Software Engineering, 28(8), 797–813.
Nielsen, M., Störring, M., Moeslund, T. B., & Granum, E. (2004). A procedure for developing intuitive 
and ergonomic gesture interfaces for HCI. Proceedings of the5th International Gesture Workshopon 
Gesture-Based Communication in Human-Computer Interaction (pp. 409-420). Springer Berlin Hei-
delberg. doi:10.1007/978-3-540-24598-8_38
Norman, D. A. (1998). The Psychology of Everyday Things. New York: Basic Books.
Quek, F., McNeill, D., Bryll, R., Duncan, S., Ma, X., Kirbas, C., & Ansari, R. et al. (2002). Multimodal 
human discourse: Gesture and speech. ACM Transactions on Computer-Human Interaction, 9(3), 171–193. 
doi:10.1145/568513.568514
Rapp, S., & Strube, M. (2002). An Iterative Data Collection Approach for Multimodal Dialogue Sys-
tems.Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC) 
(pp. 661-665).
Reichart, D., Forbrig, P., & Dittmar, A. (2004). Task models as basis for requirements engineering and 
software execution.Proceedings of the 3rd annual conference on Task models and diagrams (pp. 51-58). 
ACM. doi:10.1145/1045446.1045457
Reyes, M., Dominguez, G., & Escalera, S. (2011). Featureweighting in dynamic timewarping for gesture 
recognition in depth data. Proceedings of the2011 IEEE International Conference onComputer Vision 
Workshops (ICCV Workshops), (pp. 1182-1188). IEEE. doi:10.1109/ICCVW.2011.6130384
Ruiz, J., Li, Y., & Lank, E. (2011). User-defined motion gestures for mobile interaction.Proceedings of 
the SIGCHI Conference on Human Factors in Computing Systems (pp. 197-206). ACM.
Rupprecht, D., Blum, R., & Bomsdorf, B. (2013). Towards a Gesture Set for a Virtual Try-On. Proceed-
ings of IADIS International Conference Interfaces and Human Computer Interaction. IADIS Press.
Saffer, D. (2008). Designing gestural interfaces: Touchscreens and interactive devices. O’Reilly Media, Inc.
Schieben, A., Heesen, M., Schindler, J., Kelsch, J., & Flemisch, F. (2009). The theater-system technique: 
Agile designing and testing of system behavior and interaction, applied to highly automated vehicles.
Proceedings of the 1st International Conference on Automotive User Interfaces and Interactive Vehicular 
Applications (pp. 43-46). ACM. doi:10.1145/1620509.1620517

413
Towards ProGesture, a Tool Supporting Early Prototyping of 3D-Gesture Interaction
﻿
Spano, L. D., Cisternino, A., Paternò, F., & Fenu, G. (2013). GestIT: a declarative and compositional 
framework for multiplatform gesture definition.Proceedings of the 5th ACM SIGCHI symposium on 
Engineering interactive computing systems (pp. 187-196). ACM. doi:10.1145/2494603.2480307
Sturman, D. J., & Zeltzer, D. (1993). A design method for “whole-hand” human-computer interaction. 
ACM Transactions on Information Systems, 11(3), 219–238. doi:10.1145/159161.159159
Suma, E. A., Krum, D. M., Lange, B., Koenig, S., Rizzo, A., & Bolas, M. (2013). Adapting user interfaces 
for gestural interaction with the flexible action and articulated skeleton toolkit. Computers & Graphics, 
37(3), 193–201. doi:10.1016/j.cag.2012.11.004
Vilhjálmsson, H., Cantelmo, N., Cassell, J., Chafai, N. E., Kipp, M., Kopp, S., & Van Der Werf, R. J. 
et al. (2007). The behavior markup language: Recent developments and challenges. In Intelligent virtual 
agents (pp. 99–111). Springer Berlin Heidelberg. doi:10.1007/978-3-540-74997-4_10
Wilke, L., Calvert, T., Ryman, R., & Fox, I. (2005). From dance notation to human animation: The 
LabanDancer project. Computer Animation and Virtual Worlds, 16(3‐4), 201–211. doi:10.1002/cav.90
ENDNOTES
1 	
http://www.xbox.com/en-US/xbox-360/accessories/kinect 
2 	
https://www.leapmotion.com 
3 	
http://tangochen.com/blog/?p=564 
4 	
http://kinesicmouse.com 
5 	
http://msdn.microsoft.com/en-us/library/jj663791.aspx 
6 	
https://software.intel.com/sites/default/files/article/401008/ 
	
perc-humaninterfaceguidelines.pdf
7 	
http://giove.isti.cnr.it/projects/cameleon/pdf/ 
	
CAMELEON%20D1.1RefFramework.pdf
8 	
http://www.pro-tact.de/ 
9 	
http://proto.io/ 
10	
 https://code.google.com/p/kineticspace 
11 	
https://web.archive.org/web/20140519202648/http://www.omekinteractive.com/ 
	
products/beckon-usability-framework/
12 	
https://github.com/roboleary/LeapTrainer.js/tree/master 
13 	
http://www.nickgillian.com/wiki/pmwiki.php/GRT/FAQ 
This work was previously published in the International Journal of People-Oriented Programming (IJPOP), 4(2); edited by 
Steve Goschnick and Leon Sterling, pages 54-70, copyright year 2015 by IGI Publishing (an imprint of IGI Global).

414
Index
3D body scanning 255-256, 259, 270-271, 279
3D cadastre 119-124, 127, 133, 139-141
3D city models 123, 242, 296-303
3D modelling 242, 244, 247
3D scanning 256, 258-259, 266, 271, 321, 331
3D-4C’s process model 251, 253-255, 259, 263, 267, 
275
3D-gesture interactions 396-397, 400-401, 410
A
Acrylonitrile Butadiene Styrene (ABS) 15, 312, 331
Actor-Network Theory 48, 51, 55, 148
Additive Manufacturing (AM) 23-24, 145-148, 150-
151, 154-155, 160-162, 164, 166, 168, 172-173, 
179, 183-193, 198, 203, 225, 228-229, 259, 267, 
279, 312-313, 331
aerospace industry 162, 168, 173, 179, 184, 187-188, 
190-191, 198
architectural artifact 344, 359
Architectural Digital Model (ADM) 359
architectural heritage 333-334, 337, 339-342, 344, 
346-347, 349, 352, 359-360
augmented reality 204, 206, 212, 217, 221-223, 226, 
257-258, 265, 275, 279, 289, 334, 391
avatars 251, 258, 270, 274-275
B
bespoke 251-253, 255, 257, 259-261, 263, 265-270, 
274-275, 279
buy-to-fly ratio 168, 173, 179, 184, 188, 203
by-demonstration 403
C
cadastral systems 119-120, 129, 132, 141
chemical models 312, 316
chemical reactivity 229, 232
CityGML 119-123, 125, 132-134, 138, 140-141
competences 58, 207
complex geometries 226, 380
complex geometry 226, 376, 383
computational design 361-363, 371, 374, 383, 387, 391
computational model 94, 101, 107, 109
Computer-Aided Design (CAD) 1, 4, 11, 119, 145, 
154, 172, 183, 241-242, 313, 381
Computer-Aided Manufacturing (CAM) 22, 25, 47
Computerized Tomography (CT) 60, 74
curriculum development 307, 320
Cyber-Physical Production Systems (CPPS) 282-285, 
292
D
deposition angle 28-29, 32-34, 36-41, 47
depth-saliency-based model 98, 112
descriptive geometry 204-207, 209, 211, 223, 226, 363
developable surfaces 213, 226
digital fabrication 204, 206, 217, 219, 221, 224, 226, 
371, 374, 380, 387, 392
digital fashion 251, 253, 255, 261, 265, 276, 279
digital heritage 333-334, 336, 343-344, 346, 348, 
351, 360
digital revolution 362-363, 367-368, 381
E
eliciting gestures 398-399
engineering applications. 241, 243-244, 248
enriched city models 296, 298-299
Euclidean geometry 367-369
executable models 397, 402-403, 410
eye-tracking 75-77, 87, 89-90, 92, 98-100, 103, 105-
106, 108-109, 112
F
fabrication method 184-186
filament 24-26, 28-29, 33, 37-38, 47, 312-314, 331-332

Index
Index
  415
FlashDevelop 74
Functionally Graded Material (FGM) 164-165, 172, 
178-179, 190, 203
Fused Deposition Modeling (FDM) 14-15, 22, 24, 47, 
146, 312, 331
Fused Filament Fabrication (FFF) 312, 331
G
gesture development 401
H
Haute Couture 266-267, 279
Heidegger’s theses 148, 150
Human-Computer-Interaction (HCI) 283
I
image acquisition 244
image matching 241-242, 246, 248
L
Laminated Object Manufacturing (LOM) 16, 24, 
161, 167
laser additive 154-155, 160-162, 164, 166, 168, 183-
184, 187-188, 191-193, 198, 203
Laser Additive Manufacturing (LAM) 154-155, 160-
162, 164, 166, 168, 183-184, 187-188, 191-193, 
198, 203
laser additive manufacturing process 162, 168, 192, 203
laser cutting 204, 206, 217, 221, 223, 226, 259, 275
Laser Material Deposition (LMD) 174, 183-184, 188, 
190, 193, 198, 203
Laser Metal Deposition (LMD) 154, 158, 161-164, 
172-175, 177-179, 191-193
laser metal deposition process 158, 161-164, 172-175, 
177-179, 191-193
laser power 158-159, 174-178, 193, 196
layered manufacturing 14
learning technologies 326, 328
legal liability 285, 288
M
machined surface 228-229, 232-233, 235
machining 9, 23, 40, 145-146, 185, 225, 228-238
Magnetic Resonance Imaging (MRI) 60, 74
material processing 154-159, 184
materiality 48-49, 53, 55, 341, 344, 369
mathematical parameters 363, 392
mechanical properties 8, 14, 36, 163, 176, 192-193, 
230, 235
medical equipment 57-60, 72
medical simulations 63, 66, 72
methodic aspects 404, 410
millennial students 306-307, 310
model-based development 396, 410
morphogenesis 361, 363, 369-370
N
neutralized technology 148, 151
O
ontological status 149-150
open source 57, 59, 65, 72, 74, 313, 335, 362
orientation process 244, 246
P
paper models 213, 215
paradata 333-337, 339-340, 346-352, 360
parametric design 217, 226, 363, 371, 374, 380, 387-388
parametric modeling 206, 212, 214-215, 217-218, 221-
223, 361-364, 370-371, 374, 376, 387, 391-392
philosophy 5, 8, 48, 50, 52, 146-149, 260
photogrammetry 133, 241-245, 247-248, 302
point cloud 241-242, 244-248, 347
Polylactic Acid (PLA) 146, 312, 332
pooling strategy 101, 112
PreFab 74
Principles of Seville 336
property formation 119-121, 131, 141
R
rapid manufacturing 1, 10, 12, 14, 162
Rapid Prototyping (RP) 1-2, 6, 14, 16-18, 23, 47, 146, 
162, 165, 248, 312, 321, 396-397, 400-401, 403-
404, 409-410
rapid tooling 162
reverse engineering 23, 34, 241-244, 248
roughness parameters 22, 28, 31, 33, 37, 39-41, 47
S
scanning velocity 159-160, 176
Selective Laser Melting (SLM) 7, 10-12, 24, 161-162, 
164, 188-190, 192-193, 203, 230

Index
Index
416  
Selective Laser Sintering (SLS) 7-8, 11, 14, 24, 146, 
154, 161, 164-165, 188-189, 193, 203, 312
semantization 339-340, 344, 346, 348, 360
semi-bespoke 252-253, 260-261, 266, 268, 274, 280
smart objects 284-285, 292, 391
sociable technologies 282, 287-288, 292-293
STEM 74, 190, 335
stereolithography 3-4, 11, 15, 24, 146, 154, 160-161, 
165-166
stratification direction 41, 47
subject-object distinction 149-150
surface integrity 228-229
surface modification 183-184, 188, 191-194, 197-
198, 203
surface roughness 23, 26, 28, 36, 39-41, 47, 228-230, 
232-234
T
thermal conductivity 158, 229, 231-232
Ti6Al4V substrate 178, 184, 193, 196, 198
Ti6Al4VTiC composite 203
titanium alloy 172-173, 177-178, 183, 191, 193, 203, 
228, 231
tool support 409-410
tool wear 228-233, 235-237
U
ultrasound 60, 74
Unified Building Model (UBM) 138, 141
urban models 296, 299-300, 303
V
virtual reality 204, 206, 220-221, 227, 349, 351
virtual-try-on 251-252, 256-258, 266
visual attention 75-79, 81, 83, 85-89, 92, 97-101, 103, 
106-107, 109-112
visual importance 75-76, 79-81, 84
visual saliency 97, 104, 111
W
web 3.0 252, 280
X
x-ray 74, 289



PROCEEDINGS OF THE TENTH
WORKSHOP ON ALGORITHM
ENGINEERING AND EXPERIMENTS
AND THE FIFTH WORKSHOP 
ON ANALYTIC ALGORITHMICS 
AND COMBINATORICS  

SIAM PROCEEDINGS SERIES LIST
Computational Information Retrieval (2001), Michael Berry, editor
Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms (2004), J. Ian Munro,
editor
Applied Mathematics Entering the 21st Century: Invited Talks from the ICIAM 2003 Congress (2004), James 
M. Hill and Ross Moore, editors
Proceedings of the Fourth SIAM International Conference on Data Mining (2004), Michael W. Berry,
Umeshwar Dayal, Chandrika Kamath, and David Skillicorn, editors
Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms (2005), Adam
Buchsbaum, editor
Mathematics for Industry: Challenges and Frontiers. A Process View: Practice and Theory (2005), David R.
Ferguson and Thomas J. Peters, editors
Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms (2006), Cliff Stein,
editor
Proceedings of the Sixth SIAM International Conference on Data Mining (2006), Joydeep Ghosh, Diane
Lambert, David Skillicorn, and Jaideep Srivastava, editors
Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms (2007), Hal Gabow,
editor
Proceedings of the Ninth Workshop on Algorithm Engineering and Experiments and the Fourth Workshop on
Analytic Algorithmics and Combinatorics (2007),  David Applegate, Gerth Stølting Brodal, Daniel Panario,
and Robert Sedgewick, editors
Proceedings of the Seventh SIAM International Conference on Data Mining (2007), Chid Apte, Bing Liu,
Srinivasan Parthasarathy, and David Skillicorn, editors
Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms (2008), Shang-Hua
Teng, editor
Proceedings of the Tenth Workshop on Algorithm Engineering and Experiments and the Fifth Workshop on
Analytic Algorithmics and Combinatorics (2008), J. Ian Munro, Robert Sedgewick, Wojciech Szpankowski,
and Dorothea Wagner, editors

PROCEEDINGS OF THE TENTH
WORKSHOP ON ALGORITHM
ENGINEERING AND EXPERIMENTS
AND THE FIFTH WORKSHOP 
ON ANALYTIC ALGORITHMICS 
AND COMBINATORICS  
Society for Industrial and Applied Mathematics
Philadelphia
Edited by J. Ian Munro, Robert Sedgewick, 
Wojciech Szpankowski, and Dorothea Wagner

Proceedings of the Tenth Workshop on Algorithm Engineering and Experiments, San Francisco, CA,
January 19, 2008. 
Proceedings of the Fifth Workshop on Analytic Algorithmics and Combinatorics, San Francisco, CA,
January 19, 2008. 
The Workshop on Algorithm Engineering and Experiments was supported by the ACM Special Interest
Group on Algorithms and Computation Theory and the Society for Industrial and Applied
Mathematics.
Copyright  © 2008 by the Society for Industrial and Applied Mathematics.
10 9 8 7 6 5 4 3 2 1
All rights reserved. Printed in the United States of America. No part of this book may be reproduced,
stored, or transmitted in any manner without the written permission of the publisher. For information,
write to the Association for Computing Machinery, 1515 Broadway, New York, NY 10036 and the
Society for Industrial and Applied Mathematics, 3600 Market Street, 6th Floor, Philadelphia, PA 
19104-2688 USA.
Library of Congress Control Number:  2008923320
ISBN 978-0-898716-53-5
PROCEEDINGS OF THE TENTH WORKSHOP 
ON ALGORITHM ENGINEERING AND EXPERIMENTS 
AND THE FIFTH WORKSHOP ON ANALYTIC 
ALGORITHMICS AND COMBINATORICS  
is a registered trademark.

v
vii 
Preface to the Workshop on Algorithm Engineering and Experiments
ix  
Preface to the Workshop on Analytic Algorithmics and Combinatorics
Workshop on Algorithm Engineering and Experiments
3
Compressed Inverted Indexes for In-Memory Search Engines
Frederik Transier and Peter Sanders
13
SHARC: Fast and Robust Unidirectional Routing
Reinhard Bauer and Daniel Delling
27
Obtaining Optimal k-Cardinality Trees Fast
Markus Chimani, Maria Kandyba, Ivana Ljubic, and Petra Mutzel
37
Implementing Partial Persistence in Object-Oriented Languages
Frédéric Pluquet, Stefan Langerman, Antoine Marot, and Roel Wuyts
49
Comparing Online Learning Algorithms to Stochastic Approaches for the Multi-period
Newsvendor Problem
Shawn O’Neil and Amitabh Chaudhary
64
Routing in Graphs with Applications to Material Flow Problems
Rolf H. Möhring
65
How Much Geometry It Takes to Reconstruct a 2-Manifold in R3
Daniel Dumitriu, Stefan Funke, Martin Kutz, and Nikola Milosavljevic
75
Geometric Algorithms for Optimal Airspace Design and Air Traffic Controller Workload
Balancing
Amitabh Basu, Joseph S. B. Mitchell, and Girishkumar Sabhnani
90
Better Approximation of Betweenness Centrality
Robert Geisberger, Peter Sanders, and Dominik Schultes
101
Decoupling the CGAL 3D Triangulations from the Underlying Space
Manuel Caroli, Nico Kruithof, and Monique Teillaud
109
Consensus Clustering Algorithms: Comparison and Refinement
Andrey Goder and Vladimir Filkov
118
Shortest Path Feasibility Algorithms: An Experimental Evaluation
Boris V. Cherkassky, Loukas Georgiadis, Andrew V. Goldberg, Robert E. Tarjan, and Renato F.
Werneck
133
Ranking Tournaments: Local Search and a New Algorithm
Tom Coleman and Anthony Wirth
142
An Experimental Study of Recent Hotlink Assignment Algorithms
Tobias Jacobs
152
Empirical Study on Branchwidth and Branch Decomposition of Planar Graphs
Zhengbing Bian, Qian-Ping Gu, Marjan Marzban, Hisao Tamaki, and Yumi Yoshitake
CONTENTS
´

Workshop on Analytic Algorithmics and Combinatorics
169
On the Convergence of Upper Bound Techniques for the Average Length of Longest
Common Subsequences
George S. Lueker
183
Markovian Embeddings of General Random Strings
Manuel E. Lladser
191
Nearly Tight Bounds on the Encoding Length of the Burrows-Wheeler Transform
Ankur Gupta, Roberto Grossi, and Jeffrey Scott Vitter
203
Bloom Maps
David Talbot and John Talbot
213
Augmented Graph Models for Small-World Analysis with Geographical Factors
Van Nguyen and Chip Martel
228
Exact Analysis of the Recurrence Relations Generalized from the Tower of Hanoi
Akihiro Matsuura
234
Generating Random Derangements
Conrado Martínez, Alois Panholzer, and Helmut Prodinger
241
On the Number of Hamilton Cycles in Bounded Degree Graphs
Heidi Gebauer
249
Analysis of the Expected Number of Bit Comparisons Required by Quickselect
James Allen Fill and Takéhiko Nakama
257
Author Index
CONTENTS
vi

ALENEX WORKSHOP PREFACE
vii
The annual Workshop on Algorithm Engineering and Experiments (ALENEX) provides a forum for the
presentation of original research in all aspects of algorithm engineering, including the implementation,
tuning, and experimental evaluation of algorithms and data structures. ALENEX 2008, the tenth
workshop in this series, was held in San Francisco, California on January 19, 2008. The workshop was
sponsored by SIAM, the Society for Industrial and Applied Mathematics, and SIGACT, the ACM Special
Interest Group on Algorithms and Computation Theory. 
These proceedings contain 14 contributed papers presented at the workshop as well as the abstract
of the invited talk by Rolf Möhring. The contributed papers were selected from a total of 40
submissions based on originality, technical contribution, and relevance. Considerable effort was
devoted to the evaluation of the submissions with three reviews or more per paper.  It is nonetheless
expected that most of the papers in these proceedings will eventually appear in finished form in
scientific journals.
The workshop took place in conjunction with the Fifth Workshop on Analytic Algorithmics and
Combinatorics (ANALCO 2008), and papers from that workshop also appear in these proceedings.
Both workshops are concerned with looking beyond the big-oh asymptotic analysis of algorithms to
more precise measures of efficiency, albeit using very different approaches. The communities are
distinct, but the size of the intersection is increasing as is the flow between the two sessions. We hope
that others in the ALENEX community, not only those who attended the meeting, will find the ANALCO
papers of interest.
We would like to express our gratitude to all the people who contributed to the success of the
workshop. In particular, we would like thank the authors of submitted papers, the ALENEX Program
Committee members, and the external reviewers. Special thanks go to Kirsten Wilden, for all of her
valuable help in the many aspects of organizing this workshop, and to Sara Murphy, for coordinating
the production of these proceedings. 
J. Ian Munro and Dorothea Wagner
ALENEX 2008 Program Committee
J. Ian Munro (co-chair), University of Waterloo
Dorothea Wagner (co-chair), Universität Karlsruhe
Michael Bender, SUNY Stony Brook
Joachim Gudmundsson, NICTA 
David Johnson, AT&T Labs––Research
Stefano Leonardi, Universita di Roma “La Sapienza” 
Christian Liebchen, Technische Universität Berlin 
Alex Lopez-Ortiz, University of Waterloo
Madhav Marathe, Virginia Polytechnic Institute and State University
Catherine McGeoch, Amherst College
Seth Pettie, University of Michigan at Ann Arbor
Robert Sedgewick, Princeton University
Michiel Smid, Carleton University 
Norbert Zeh, Dalhousie University 
ALENEX 2008 Steering Committee
David Applegate, AT&T Labs––Research 
Lars Arge, University of Aarhus
Roberto Battiti, University of Trento
Gerth Brodal, University of Aarhus 
Adam Buchsbaum, AT&T Labs––Research
Camil Demetrescu, University of Rome “La Sapienza”

Andrew V. Goldberg, Microsoft Research
Michael T. Goodrich, University of California, Irvine
Giuseppe F. Italiano, University of Rome “Tor Vergata”
David S. Johnson, AT&T Labs––Research
Richard E. Ladner, University of Washington
Catherine C. McGeoch, Amherst College
Bernard M.E. Moret, University of New Mexico
David Mount, University of Maryland, College Park
Rajeev Raman, University of Leicester, United Kingdom 
Jack Snoeyink, University of North Carolina, Chapel Hill
Matt Stallmann, North Carolina State University 
Clifford Stein, Columbia University
Roberto Tamassia, Brown University
ALENEX 2008 External Reviewers
Reinhard Bauer
Michael Baur
Marc Benkert
Christian Blum
Ilaria Bordino
Ulrik Brandes
Jiangzhou Chen
Bojan Djordjevic
Frederic Dorn
John Eblen
Martin Ehmsen
Jeff Erickson
Arash Farzan
Mahmoud Fouz
Paolo Franciosa
Markus Geyer
Robert Görke
Meng He
Riko Jacob
Maleq Khan
Marcus Krug 
Giuseppe Liotta
Hans van Maaren
Steffen Mecke
Damian Merrick
Matthias Mueller-Hannemann
Alantha Newman
Rajeev Raman
S.S. Ravi
Adi Rosen
Ignaz Rutter
Piotr Sankowski
Matthew Skala
Jan Vahrenhold
Anil Vullikanti
Thomas Wolle
Katharina Zweig
ALENEX WORKSHOP PREFACE
viii

The aim of ANALCO is to provide a forum for original research in the analysis of algorithms and
associated combinatorial structures. The papers study properties of fundamental combinatorial
structures that arise in practical computational applications (such as trees, permutations, strings, tries,
and graphs) and address the precise analysis of algorithms for processing such structures, including
average-case analysis; analysis of moments, extrema, and distributions; and probabilistic analysis of
randomized algorithms. Some of the papers present significant new information about classic
algorithms; others present analyses of new algorithms that present unique analytic challenges, or
address tools and techniques for the analysis of algorithms and combinatorial structures, both
mathematical and computational.
The papers in these proceedings were presented in San Francisco on January 19, 2008, at the Fifth
Workshop on Analytic Algorithmics and Combinatorics (ANALCO’08). We selected 9 papers out of a
total of 20 submissions. An invited lecture by Don Knuth on “Some Puzzling Problems” was the highlight
of the workshop.
The workshop took place on the same day as the Tenth Workshop on Algorithm Engineering and
Experiments (ALENEX’08). The papers from that workshop are also published in this volume. Since
researchers in both fields are approaching the problem of learning detailed information about the
performance of particular algorithms, we expect that interesting synergies will develop. People in the
ANALCO community are encouraged to look over the ALENEX papers for problems where the
analysis of algorithms might play a role; people in the ALENEX community are encouraged to look
over these ANALCO papers for problems where experimentation might play a role.
Robert Sedgewick and Wojciech Szpankowski
ANALCO 2008 Program Committee
Robert Sedgewick (co-chair), Princeton University
Wojciech Szpankowski (co-chair), Purdue University
Mordecai Golin (SODA Program Committee Liaison), 
Hong Kong University of Science & Technology, Hong Kong
Luc Devroye, McGill University, Canada
James Fill, Johns Hopkins University
Eric Fusy, Inria, France
Andrew Goldberg, Microsoft Research
Mike Molloy, University of Toronto, Canada
Alois Panholzer, Technische Universität Wien, Austria
Robin Pemantle, University of Pennsylvania
Alfredo Viola, Republica University, Uruguay
ANALCO WORKSHOP PREFACE
ix














SHARC: Fast and Robust Unidirectional Routing ∗
Reinhard Bauer †
Daniel Delling†
Abstract
During the last years, impressive speed-up techniques for
Dijkstra’s algorithm have been developed. Unfortunately,
the most advanced techniques use bidirectional search which
makes it hard to use them in scenarios where a back-
ward search is prohibited. Even worse, such scenarios are
widely spread, e.g., timetable-information systems or time-
dependent networks.
In this work, we present a unidirectional speed-up tech-
nique which competes with bidirectional approaches. More-
over, we show how to exploit the advantage of unidirectional
routing for fast exact queries in timetable information sys-
tems and for fast approximative queries in time-dependent
scenarios. By running experiments on several inputs other
than road networks, we show that our approach is very ro-
bust to the input.
1
Introduction
Computing shortest paths in graphs is used in many
real-world applications like route planning in road net-
works, timetable information for railways, or schedul-
ing for airplanes. In general, Dijkstra’s algorithm [10]
ﬁnds a shortest path between a given source s and tar-
get t. Unfortunately, the algorithm is far too slow to
be used on huge datasets. Thus, several speed-up tech-
niques have been developed (see [33, 29] for an overview)
yielding faster query times for typical instances, e.g.,
road or railway networks. Due to the availability of huge
road networks, recent research on shortest paths speed-
up techniques solely concentrated on those networks [9].
The fastest known techniques [5, 1] were developed for
road networks and use speciﬁc properties of those net-
works in order to gain their enormous speed-ups.
However, these techniques perform a bidirectional
query or at least need to know the exact target node of a
query. In general, these hierarchical techniques step up
a hierarchy—built during preprocessing—starting both
from source and target and perform a fast query on a
very small graph. Unfortunately, in certain scenarios a
backward search is prohibited, e.g. in timetable infor-
∗Partially supported by the Future and Emerging Technologies
Unit of EC (IST priority – 6th FP), under contract no.
FP6-
021235-2 (project ARRIVAL).
†Universit¨at Karlsruhe (TH), 76128 Karlsruhe,
Germany,
{rbauer,delling}@ira.uka.de.
mation systems and time-dependent graphs the time of
arrival is unknown. One option would be to guess the
arrival time and then to adjust the arrival time after for-
ward and backward search have met. Another option is
to develop a fast unidirectional algorithm.
In this work, we introduce SHARC-Routing, a fast
and robust approach for unidirectional routing in large
networks.
The central idea of SHARC (Shortcuts +
Arc-Flags) is the adaptation of techniques developed for
Highway Hierarchies [28] to Arc-Flags [21, 22, 23, 18].
In general, SHARC-Routing iteratively constructs a
contraction-based hierarchy during preprocessing and
automatically sets arc-ﬂags for edges removed during
contraction. More precisely, arc-ﬂags are set in such a
way that a unidirectional query considers these removed
component-edges only at the beginning and the end of a
query. As a result, we are able to route very eﬃciently
in scenarios where other techniques fail due to their
bidirectional nature. By using approximative arc-ﬂags
we are able to route very eﬃciently in time-dependent
networks, increasing performance by one order of mag-
nitude over previous time-dependent approaches. Fur-
thermore, SHARC allows to perform very fast queries—
without updating the preprocessing—in scenarios where
metrics are changed frequently, e.g. diﬀerent speed pro-
ﬁles for fast and slow cars. In case a user needs even
faster query times, our approach can also be used as
a bidirectional algorithm that outperforms the most
prominent techniques (see Figure 1 for an example on a
typical search space of uni- and bidirectional SHARC).
Only Transit-Node Routing is faster than this variant of
SHARC, but SHARC needs considerably less space. A
side-eﬀect of SHARC is that preprocessing takes much
less time than for pure Arc-Flags.
Related Work. To our best knowledge, three ap-
proaches exist that iteratively contract and prune the
graph during preprocessing. This idea was introduced
in [27]. First, the graph is contracted and afterwards
partial trees are built in order to determine highway
edges. Non-highway edges are removed from the graph.
The contraction was signiﬁcantly enhanced in [28] re-
ducing preprocessing and query times drastically. The
RE algorithm, introduced in [14, 15], also uses the con-
traction from [28] but pruning is based on reach values
13

Figure 1: Search space of a typical uni-(left) and bidirectional(right) SHARC-query. The source of the query is
the upper ﬂag, the target the lower one. Relaxed edges are drawn in black. The shortest path is drawn thicker.
Note that the bidirectional query only relaxes shortest-path edges.
for edges. A technique relying on contraction as well
is Highway-Node Routing [31], which combines several
ideas from other speed-up techniques. All those tech-
niques build a hierarchy during the preprocessing and
the query exploits this hierarchy. Moreover, these tech-
niques gain their impressive speed-ups from using a bidi-
rectional query, which—among other problems—makes
it hard to use them in time-dependent graphs. Up to
now, solely pure ALT [13] has been proven to work in
such graphs [7]. Moreover, REAL [14, 15]—a combina-
tion of RE and ALT—can be used in a unidirectional
sense but still, the exact target node has to be known
for ALT, which is unknown in timetable information
systems (cf. [26] for details).
Similar to Arc-Flags [21, 22, 23, 18], Geometric
Containers [34] attaches a label to each edge indicating
whether this edge is important for the current query.
However, Geometric Containers has a worse perfor-
mance than Arc-Flags and preprocessing is based on
computing a full shortest path tree from every node
within the graph. For more details on classic Arc-Flags,
see Section 2.
Overview. This paper is organized as follows.
Sec-
tion 2 introduces basic deﬁnitions and reviews the clas-
sic Arc-Flag approach. Preprocessing and the query al-
gorithm of our SHARC approach are presented in Sec-
tion 3, while Section 4 shows how SHARC can be used
in time-dependent scenarios. Our experimental study
on real-world and synthetic datasets is located in Sec-
tion 5 showing the excellent performance of SHARC on
various instances. Our work is concluded by a summary
and possible future work in Section 6.
2
Preliminaries
Throughout the whole work we restrict ourselves to
simple, directed graphs G = (V, E) with positive length
function len : E →
+. The reverse graph G = (V, E)
is the graph obtained from G by substituting each
(u, v) ∈E by (v, u). Given a set of edges H, source(H)
/ target(H) denotes the set of all source / target nodes
of edges in H. With degin(v) / degout(v) we denote the
number of edges whose target / source node is v. The 2-
core of an undirected graph is the maximal node induced
subgraph of minimum node degree 2. The 2-core of a
directed graph is the 2-core of the corresponding simple,
unweighted, undirected graph. A tree on a graph for
which exactly the root lies in the 2-core is called an
attached tree.
A partition of V is a family C = {C0, C1, . . . , Ck} of
sets Ci ⊆V such that each node v ∈V is contained
in exactly one set Ci.
An element of a partition is
14

called a cell. A multilevel partition of V is a family of
partitions {C0, C1, . . . , Cl} such that for each i < l and
each Ci
n ∈Ci a cell Ci+1
m
∈Ci+1 exists with Ci
n ⊆Ci+1
m .
In that case the cell Ci+1
m
is called the supercell of Ci
n.
The supercell of a level-l cell is V . The boundary nodes
BC of a cell C are all nodes u ∈C for which at least one
node v ∈V \C exists such that (v, u) ∈E or (u, v) ∈E.
The distance according to len between two nodes u and
v we denote by d(u, v).
Classic Arc-Flags. The classic Arc-Flag approach,
introduced in [21, 22], ﬁrst computes a partition C of
the graph and then attaches a label to each edge e.
A label contains, for each cell Ci ∈C, a ﬂag AF Ci(e)
which is true iﬀa shortest path to a node in Ci starts
with e. A modiﬁed Dijkstra then only considers those
edges for which the ﬂag of the target node’s cell is true.
The big advantage of this approach is its easy query
algorithm. Furthermore an Arc-Flags Dijkstra often
is optimal in the sense that it only visits those edges
that are on the shortest path. However, preprocessing
is very extensive, either regarding preprocessing time or
memory consumption. The original approach grows a
full shortest path tree from each boundary node yielding
preprocessing times of several weeks for instances like
the Western European road network. Recently, a new
centralized approach has been introduced [17]. It grows
a centralized tree from each cell keeping the distances
to all boundary nodes of this cell in memory.
This
approach allows to preprocess the Western European
road network within one day but for the price of high
memory consumption during preprocessing.
Note that AF Ci(e) is true for almost all edges
e ∈Ci (we call this ﬂags the own-cell-ﬂag). Due to these
own-cell-ﬂags an Arc-Flags Dijkstra yields no speed-
up for queries within the same cell. Even worse, using
a unidirectional query, more and more edges become
important when approaching the target cell (the coning
eﬀect) and ﬁnally, all edges are considered as soon as the
search enters the target cell.
While the coning eﬀect
can be weakened by a bidirectional query, the former
also holds for such queries. Thus, a two-level approach
is introduced in [23] which weakens these drawbacks
as cells become quite small on the lower level.
It is
obvious that this approach can be extended to a multi-
level approach.
3
Static SHARC
In this section, we explain SHARC-Routing in static sce-
narios, i.e., the graph remains untouched between two
queries.
In general, the SHARC query is a standard
multi-level Arc-Flags Dijkstra, while the preprocess-
ing incorporates ideas from hierarchical approaches.
3.1
Preprocessing of SHARC is similar to Highway
Hierarchies and REAL. During the initialization phase,
we extract the 2-core of the graph and perform a multi-
level partition of G according to an input parameter P.
The number of levels L is an input parameter as well.
Then, an iterative process starts.
At each step i we
ﬁrst contract the graph by bypassing unimportant nodes
and set the arc-ﬂags automatically for each removed
edge.
On the contracted graph we compute the arc-
ﬂags of level i by growing a partial centralized shortest-
path tree from each cell Ci
j.
At the end of each
step we prune the input by detecting those edges
that already have their ﬁnal arc-ﬂags assigned.
In
the ﬁnalization phase, we assemble the output-graph,
reﬁne arc-ﬂags of edges removed during contraction
and ﬁnally reattach the 1-shell nodes removed at the
beginning.
Figure 2 shows a scheme of the SHARC-
preprocessing. In the following we explain each phase
separately.
We hereby restrict ourselves to arc-ﬂags
for the unidirectional variant of SHARC. However,
the extension to computing bidirectional arc-ﬂags is
straight-forward.
3.1.1
1-Shell Nodes. First of all, we extract the
2-core of the graph as we can directly assign correct
arc-ﬂags to attached trees that are fully contained in a
cell: Each edge targeting the core gets all ﬂags assigned
true while those directing away from the core only
get their own-cell ﬂag set true.
By removing 1-shell
nodes before computing the partition we ensure the
“fully contained” property by assigning all nodes in an
attached tree to the cell of its root. After the last step
of our preprocessing we simply reattach the nodes and
edges of the 1-shell to the output graph.
3.1.2
Multi-Level Partition. As shown in [23], the
classic Arc-Flag method heavily depends on the par-
tition used. The same holds for SHARC. In order to
achieve good speed-ups, several requirements have to
be fulﬁlled: cells should be connected, the size of cells
should be balanced, and the number of boundary nodes
has to be low. In this work, we use a locally optimized
partition obtained from SCOTCH [25]. For details, see
Section 5. The number of levels L and the number of
cells per level are tuning-parameters.
3.1.3
Contraction. The graph is contracted by it-
eratively bypassing nodes until no node is bypassable
any more. To bypass a node n we ﬁrst remove n, its
incoming edges I and its outgoing edges O from the
graph.
Then, for each u ∈source(I) and for each
v ∈target(I) \ {u} we introduce a new edge of the
length len(u, n) + len(n, v). If there already is an edge
15

multi-level
partitioning
contraction
component arc-ﬂags
core arc-ﬂags
pruning
++i==L?
construct
output-graph
remove 1-shell nodes
reﬁnement
reattach 1-shell nodes
if(i==L −1):
boundary-shortcuts
i=0
P,L,c
YES
NO
initialization
iteration
ﬁnalization
Figure 2: Schematic representation of the preprocess-
ing. Input parameters are the partition parameters P,
the number of levels L, and the contraction parame-
ter c. During initialization, we remove the 1-shell nodes
and partition the graph. Afterwards, an iterative pro-
cess starts which contracts the graph, sets arc-ﬂags, and
prunes the graph. Moreover, during the last iteration
step, boundary shortcuts are added to the graph. Dur-
ing the ﬁnalization, we construct the output-graph, re-
ﬁne arc-ﬂags and reattach the 1-shell nodes to the graph.
connecting u and v in the graph, we only keep the one
with smaller length.
We call the number of edges of
the path that a shortcut represents on the graph at the
beginning of the current iteration step the hop number
of the shortcut. To check whether a node is bypassable
we ﬁrst determine the number #shortcut of new edges
that would be inserted into the graph if n is bypassed,
i.e., existing edges connecting nodes in source(I) with
nodes in target(O) do not contribute to #shortcut.
Then we say a node is bypassable iﬀthe bypass criterion
#shortcut ≤c·(degin(n)+degout(n)) is fulﬁlled, where
c is a tunable contraction parameter.
A node being bypassed inﬂuences the degree of their
neighbors and thus, their bypassability. Therefore, the
order in which nodes are bypassed changes the resulting
contracted graph. We use a heap to determine the next
bypassable node. The key of a node n within the heap
is h · #shortcut/(degin(n) + degout(n)) where h is the
hop number of the hop-maximal shortcut that would
be added if n was bypassed, smaller keys have higher
priority. To keep the length of shortcuts limited we do
not bypass a node if that results in adding a shortcut
with hop number greater than 10. We say that the nodes
that have been bypassed belong to the component, while
the remaining nodes are called core-nodes.
In order
to guarantee correctness, we use cell-aware contraction,
i.e., a node n is never marked bypassable if any of its
neighboring nodes is not in the same cell as n.
Our contraction routine mainly follows the ideas
introduced in [28].
The idea to control the order, in
which the nodes are bypassed using a heap is due to
[14].
In addition, we slightly altered the bypassing
criterion, leading to signiﬁcantely better results, e.g.
on the road network of Western Europe, our routine
bypasses twice the number of nodes with the same
contraction parameter. The main diﬀerence to [28] is
that we do not count existing edges for determining
#shortcut. Finally, the idea to bound the hop number
of a shortcut is due to [6].
3.1.4
Boundary-Shortcuts. During our study, we
observed that—at least for long-range queries on road
networks—a classic bidirected Arc-Flags Dijkstra of-
ten is optimal in the sense that it visits only the edges
on the shortest path between two nodes. However, such
shortest paths may become quite long in road networks.
One advantage of SHARC over classic Arc-Flags is that
the contraction routine reduces the number of hops of
shortest paths in the network yielding smaller search
spaces. In order to further reduce this hop number we
enrich the graph by additional shortcuts.
In general
we could try any shortcuts as our preprocessing favors
paths with less hops over those with more hops, and
16

thus, added shortcuts are used for long range queries.
However, adding shortcuts crossing cell-borders can in-
crease the number of boundary nodes, and hence, in-
crease preprocessing time.
Therefore, we use the fol-
lowing heuristic to determine good shortcuts: we add
boundary shortcuts between some boundary nodes be-
longing to the same cell C at level L −1.
In order
to keep the number of added edges small we compute
the betweenness [4] values cB of the boundary nodes on
the remaining core-graph. Each boundary node with a
betweenness value higher than half the maximum gets
3 ·

|BC| additional outgoing edges. The targets are
those boundary nodes with highest cB · h values, where
h is the number of hops of the added shortcut.
3.1.5
Arc-Flags. Our query algorithm is executed
on the original graph enhanced by shortcuts added
during the contraction phase. Thus, we have to assign
arc-ﬂags to each edge we remove during the contraction
phase. One option would be to set every ﬂag to true.
However, we can do better. First of all, we keep all arc-
ﬂags that already have been computed for lower levels.
We set the arc-ﬂags of the current and all higher levels
depending on the source node s of the deleted edge. If
s is a core node, we only set the own-cell ﬂag to true
(and others to false) because this edge can only be
relevant for a query targeting a node in this cell. If s
belongs to the component, all arc-ﬂags are set to true as
a query has to leave the component in order to reach a
node outside this cell. Finally, shortcuts get their own-
--0-
--0-
--0-
1111
1111
0010
1111
0010
1
2
3
4
5
Figure 3: Example for assigning arc-ﬂags during con-
traction for a partition having four cells. All nodes are
in cell 3.
The red nodes (4 and 5) are removed, the
dashed shortcuts are added by the contraction.
Arc-
ﬂags are indicated by a 1 for true and 0 for false. The
edges directing into the component get only their own-
cell ﬂag set true. All edges in and out of the component
get full ﬂags. The added shortcuts get their own-cell
ﬂags ﬁxed to false.
cell ﬂag ﬁxed to false as relaxing shortcuts when the
target cell is reached yields no speed-up. See Figure 3
for an example. As a result, an Arc-Flags query only
considers components at the beginning and the end of
a query. Moreover, we reduce the search space.
Assigning Arc-Flags to Core-Edges. After the
contraction phase and assigning arc-ﬂags to removed
edges, we compute the arc-ﬂags of the core-edges of
the current level i.
As described in [17], we grow,
for each cell C, one centralized shortest path tree on
the reverse graph starting from every boundary node
n ∈BC of C. We stop growing the tree as soon as all
nodes of C’s supercell have a distance to each b ∈BC
greater than the smallest key in the priority queue used
by the centralized shortest path tree algorithm (see [17]
for details). For any edge e that is in the supercell of C
and that lies on a shortest path to at least one b ∈BC,
we set AF i
C(e) = true.
Note that the centralized approach sets arc-ﬂags to
true for all possible shortest paths between two nodes.
In order to favor boundary shortcuts, we extend the
centralized approach by introducing a second matrix
that stores the number of hops to every boundary
node. With the help of this second matrix we are able
to assign true arc-ﬂags only to hop-minimal shortest
paths.
However, using a second matrix increases the
high memory consumption of the centralized approach
even further. Thus, we use this extension only during
the last iteration step where the core is small.
3.1.6
Pruning. After computing arc-ﬂags at the cur-
rent level, we prune the input. We remove unimportant
edges from the graph by running two steps. First, we
identify prunable cells. A cell C is called prunable if
all neighboring cells are assigned to the same supercell.
Then we remove all edges from a prunable cell that have
at most their own-cell bit set. For those edges no ﬂag
can be assigned true in higher levels as then at least
one ﬂag for the surrounding cells must have been set
before.
3.1.7
Reﬁnement of Arc-Flags. Our contraction
routine described above sets all ﬂags to true for almost
all edges removed by our contraction routine. However,
we can do better: we are able to reﬁne arc-ﬂags by
propagation of arc-ﬂags from higher to lower levels.
Before explaining our propagation routine we need the
notion of level. The level l(u) of a node u is determined
by the iteration step it is removed in from the graph. All
nodes removed during iteration step i belong to level i.
Those nodes which are part of the core-graph after the
last iteration step belong to level L. In the following,
we explain our propagation routine for a given node u.
17

0001
1101
1100
1111
1111
0010
1111
0010
1
2
3
4
5
1100
0011
0010
0010
0010
0001
1101
1100
1111
0010
0010
1111
0010
1
2
3
4
5
1100
0011
0010
0010
0010
Figure 4: Example for reﬁning the arc-ﬂags of outgoing edges from node 4. The ﬁgure in the left shows the graph
from Figure 3 after the last iteration step. The ﬁgure on the right shows the result of our reﬁnement routine
starting at node 4.
First, we build a partial shortest-path tree T start-
ing at u, not relaxing edges that target nodes on a level
smaller than l(u). We stop the growth as soon as all
nodes in the priority queue are covered. A node v is
called covered as soon as a node between u and v—with
respect to T—belongs to a level > l(u). After the termi-
nation of the growth we remove all covered nodes from
T resulting in a tree rooted at u and with leaves either
in l(u) or in a level higher than l(u). Those leaves of
the built tree belonging to a level higher than l(u) we
call entry nodes ⃗N(u) of u.
With this information we reﬁne the arc-ﬂags of all
edges outgoing from u. First, we set all ﬂags—except
the own-cell ﬂags—of all levels ≥l(u) for all outgoing
edges from u to false. Next, we assign entry nodes to
outgoing edges from u. Starting at an entry node nE
we follow the predecessor in T until we ﬁnally end up in
a node x whose predecessor is u. The edge (u, x) now
inherits the ﬂags from nE. Every edge outgoing from
nE whose target t is not an entry node of u and not in a
level < l(u) propagates all true ﬂags of all levels ≥l(u)
to (u, x).
In order to propagate ﬂags from higher to lower
levels we perform our propagation-routine in L −1 re-
ﬁnement steps, starting at level L−1 and in descending
order. Figure 4 gives an example. Note that during re-
ﬁnement step i we only reﬁne arc-ﬂags of edges outgoing
from nodes belonging to level i.
3.1.8
Output Graph. The output graph of the pre-
processing consists of the original graph enhanced by all
shortcuts that are in the contracted graph at the end of
at least one iteration step.
Note that an edge (u, v)
may be contained in no shortest path because a shorter
path from u to v already exists. This especially holds
for the shortcuts we added to the graph. As a conse-
quence, such edges have no ﬂag set true after the last
step. Thus, we can remove all edges from the output
graph with no ﬂag set true. Furthermore the multi-
level partition and the computed arc-ﬂags are given.
3.2
Query. Basically, our query is a multi-level Arc-
Flags Dijkstra adapted from the two-level Arc-Flags
Dijkstra presented in [23]. The query is a modiﬁed
Dijkstra that operates on the output graph.
The
modiﬁcations are as follows: When settling a node n,
we compute the lowest level i on which n and the target
node t are in the same supercell. When relaxing the
edges outgoing from n, we consider only those edges
having a set arc-ﬂag on level i for the corresponding
cell of t. It is proven that Arc-Flags performs correct
queries. However, as our preprocessing is diﬀerent, we
have to prove Theorem 3.1.
Theorem 3.1. The distances computed by SHARC are
correct with respect to the original graph.
The proof can be found in Appendix A.
We want
to point out that the SHARC query, compared to
plain Dijkstra, only needs to additionally compute the
common level of the current node and the target. Thus,
our query is very eﬃcient with a much smaller overhead
compared to other hierarchical approaches. Note that
SHARC uses shortcuts which have to be unpacked for
determining the shortest path (if not only the distance
is queried). However, we can directly use the methods
from [6], as our contraction works similar to Highway
Hierarchies.
Multi-Metric Query. In [3], we observed that the
shortest path structure of a graph—as long as edge
weights somehow correspond to travel times—hardly
changes when we switch from one metric to another.
Thus, one might expect that arc-ﬂags are similar to each
other for these metrics. We exploit this observation for
our multi-metric variant of SHARC. During preprocess-
18

ing, we compute arc-ﬂags for all metrics and at the end
we store only one arc-ﬂag per edge by setting a ﬂag
true as soon as the ﬂag is true for at least one metric.
An important precondition for multi-metric SHARC is
that we use the same partition for each metric. Note
that the structure of the core computed by our contrac-
tion routine is independent of the applied metric.
Optimizations. In order to improve both perfor-
mance and space eﬃciency, we use three optimizations.
Firstly, we increase locality by reordering nodes accord-
ing to the level they have been removed at from the
graph. As a consequence, the number of cache misses is
reduced yielding lower query times. Secondly, we check
before running a query, whether the target is in the
1-shell of the graph. If this check holds we do not re-
lax edges that target 1-shell nodes whenever we settle
a node being part of the 2-core. Finally, we store each
diﬀerent arc-ﬂag only once in a separate array. We as-
sign an additional pointer to each edge indicating the
correct arc-ﬂags. This yields a lower space overhead.
4
Time-Dependent SHARC
Up to this point, we have shown how preprocessing
works in a static scenario. As our query is unidirectional
it seems promising to use SHARC in a time-dependent
scenario.
The fastest known technique for such a
scenario is ALT yielding only mild speed-ups of factor
3-5. In this section we present how to perform queries
in time-dependent graphs with SHARC. In general, we
assume that a time-dependent network −→
G = (V, −→
E )
derives from an independent network G = (V, E) by
increasing edge weights at certain times of the day. For
road networks these increases represent rush hours.
The idea is to compute approximative arc-ﬂags
in G and to use these ﬂags for routing in −→
G.
In
order to compute approximative arc-ﬂags, we relax our
criterion for setting arc-ﬂags. Recall that for exact ﬂags,
AF C((u, v)) is set true if d(u, b) + len(u, v) = d(v, b)
holds for at least one b ∈BC.
For γ-approximate
ﬂags (indicated by AF), we set AF C((u, v)) = true if
equation d(u, b)+len(u, v) ≤γ ·d(v, b) holds for at least
one b ∈BC.
Note that we only have to change this
criterion in order to compute approximative arc-ﬂags
instead of exact ones by our preprocessing. However, we
do not add boundary shortcuts as this relaxed criterion
does not favor those shortcuts.
It is easy to see that there exists a trade-oﬀbetween
performance and quality. Low γ-values yield low query
times but the error-rate may increase, while a large γ
reduces the error rate of γ-SHARC but yields worse
query performance, as much more edges are relaxed
during the query than necessary.
5
Experiments
In this section, we present an extensive experimental
evaluation of our SHARC-Routing approach. To this
end, we evaluate the performance of SHARC in various
scenarios and inputs. Our tests were executed on one
core of an AMD Opteron 2218 running SUSE Linux
10.1. The machine is clocked at 2.6 GHz, has 16 GB
of RAM and 2 x 1 MB of L2 cache. The program was
compiled with GCC 4.1, using optimization level 3.
Implementation Details. Our implementation is
written in C++ using solely the STL. As priority queue
we use a binary heap.
Our graph is represented as
forward star implementation. As described in [30], we
have to store each edge twice if we want to iterate
eﬃciently over incoming and outgoing edges. Thus, the
authors propose to compress edges if target and length
of incoming and outgoing edges are equal.
However,
SHARC allows an even simpler implementation. During
preprocessing we only operate on the reverse graph and
thus do not iterate over outgoing edges while during
the query we only iterate over outgoing edges.
As
a consequence, we only have to store each edge once
(for preprocessing at its target, for the query at its
source). Thus, another advantage of our unidirectional
SHARC approach is that we can reduce the memory
consumption of the graph.
Note that this does not
hold for our bidirectional SHARC variant which needs
considerably more space (cf. Tab. 1).
Multi-Level Partition. As already mentioned,
the performance of SHARC highly depends on the par-
tition of the graph. Up to now [2], we used METIS [20]
for partitioning a given graph. However, in our experi-
mental study we observed two downsides of METIS: On
the one hand, cells are sometimes disconnected and the
number of boundary nodes is quite high. Thus, we also
tested PARTY [24] and SCOTCH [25] for partitioning.
The former produces connected cells but for the price of
an even higher number of boundary nodes. SCOTCH
has the lowest number of boundary cells, but connec-
tivity of cells cannot be guaranteed. Due to this low
number of boundary nodes, we used SCOTCH and im-
prove the obtained partitioning by adding smaller pieces
of disconnected cells to neighbor cells. As a result, con-
structing and optimizing a partition can be done in less
than 3 minutes for all inputs used.
Default Setting. Unless otherwise stated, we use
a unidirectional variant of SHARC with a 3-level parti-
tion with 16 cells per supercell on level 0 and 1 and 96
cells on level 2. Moreover, we use a value of c = 2.5 as
contraction parameter. When performing random s-t
queries, the source s and target t are picked uniformly
at random and results are based on 10 000 queries.
19

Table 1: Performance of SHARC and the most prominent speed-up techniques on the European and US road
network with travel times. Prepro shows the computation time of the preprocessing in hours and minutes and
the eventual additional bytes per node needed for the preprocessed data. For queries, the search space is given
in the number of settled nodes, execution times are given in milliseconds. Note that other techniques have been
evaluated on slightly diﬀerent computers. The results for Highway Hierarchies and Highway-Node Routing derive
from [30]. Results for Arc-Flags are based on 200 PARTY cells and are taken from [17].
Europe
USA
Prepro
Query
Prepro
Query
[h:m]
[B/n]
#settled
[ms]
[h:m]
[B/n]
#settled
[ms]
SHARC
2:17
13
1 114
0.39
1:57
16
1 770
0.68
bidirectional SHARC
3:12
20
145
0.091
2:38
21
350
0.18
Highway Hierarchies
0:19
48
709
0.61
0:17
34
925
0.67
Highway-Node
0:15
8
1 017
0.88
0:16
8
760
0.50
REAL-(64,16)
2:21
32
679
1.10
2:01
43
540
1.05
Arc-Flags
17:08
19
2 369
1.60
10:10
10
8 180
4.30
Grid-based Transit-Node
–
–
–
–
20:00
21
NA
0.063
HH-based Transit-Node
2:44
251
NA
0.006
3:25
244
NA
0.005
5.1
Static Environment. We start our experimen-
tal evaluation with various tests for the static scenario.
We hereby focus on road networks but also evaluate
graphs derived from timetable information systems and
synthetic datasets that have been evaluated in [2].
5.1.1
Road Networks. As inputs we use the largest
strongly connected component of the road networks of
Western Europe, provided by PTV AG for scientiﬁc use,
and of the US which is taken from the DIMACS home-
page [9]. The former graph has approximately 18 mil-
lion nodes and 42.6 million edges and edge lengths cor-
respond to travel times. The corresponding ﬁgures for
the USA are 23.9 million and 58.3 million, respectively.
Random Queries. Tab. 1 reports the results of
SHARC with default settings compared to the most
prominent speed-up techniques. In addition, we report
the results of a variant of SHARC which uses bidirec-
tional search in connection with a 2-level partition (16
cells per supercell at level 0, 112 at level 1).
We observe excellent query times for SHARC in
general. Interestingly, SHARC has a lower preprocess-
ing time for the US than for Europe but for the price
of worse query performance. On the one hand, this is
due to the bigger size of the input yielding bigger cell
sizes and on the other hand, the average hop number of
shortest paths are bigger for the US than for Europe.
However, the number of boundary nodes is smaller for
the US yielding lower preprocessing eﬀort. The bidirec-
tional variant of SHARC has a more extensive prepro-
cessing: both time and additional space increase, which
is due to computing and storing forward and backward
arc-ﬂags. However, preprocessing does not take twice
the time than for default SHARC as we use a 2-level
setup for the bidirectional variant and preprocessing the
third level for default SHARC is quite expensive (around
40% of the total preprocessing time). Comparing query
performance, bidirectional SHARC is clearly superior
to the unidirectional variant. This is due to the known
disadvantages of uni-directional classic Arc-Flags: the
coning eﬀect and no arc-ﬂag information as soon as the
search enters the target cell (cf. Section 2 for details).
Comparing SHARC with other techniques, we ob-
serve that SHARC can compete with any other tech-
nique except HH-based Transit Node Routing, which
requires much more space than SHARC. Stunningly, for
Europe, SHARC settles more nodes than Highway Node
Routing or REAL, but query times are smaller. This is
due to the very low computational overhead of SHARC.
Regarding preprocessing, SHARC uses less space than
REAL or Highway Hierarchies. The computation time
of the preprocessing is similar to REAL but longer than
for Highway-Node Routing. The bidirectional variant
uses more space and has longer preprocessing times,
but the performance of the query is very good.
The
number of nodes settled is smaller than for any other
technique and due to the low computational overhead
query times are clearly lower than for Highway Hier-
archies, Highway-Node Routing or REAL. Compared
to the classic Arc-Flags, SHARC signiﬁcantely reduces
preprocessing time and query performance is better.
Local Queries. Figure 5 reports the query times
of uni- and bidirectional SHARC with respect to the
Dijkstra rank. For an s-t query, the Dijkstra rank of
node v is the number of nodes inserted in the priority
queue before v is reached. Thus, it is a kind of distance
measure.
As input we again use the European road
network instance.
20

●
●●●●●●●●●
●
●●
●
●
●
●
●●
●●●●
●●●●●●●●●●●
●
●●
●●●●
●●
●
●●●●●
●●●●●●●●
●
●●
●
●●
●●
●
●●●●●
●●●●●
●
●
●●
●
●
●
●
●●●●●●●
●
●●●
●
●
●●●●●●
●●●●●●
●●●
●
●●●●●●
●●●●●●●●●●●●
●●
●●●●●●
●●●●●●●●●●●●●●●●●
●●●●●
●
●●●●●●
●
●●●
●●
●
●
●●●●●●●●
●●
●●●
●●
●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●
●●●
●●
●●●
●●●●●●●●●●●
●●●●
SHARC (Europe)
●●●●●●●●●●
●
●
●
●●
●●
●●●●●●●●●
●●●
●●●●●●●
●
●●●●
●●●
●●●●
●
●●●●●●●●
●●
●
●
●
●
●●
●●●●
●●●●
●
●●
●
●●●●●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●●
●
●
●
●
●
●●
●
●
●
●●●
●
●●
●●
●●●
●●●
●
●●
●
●
●
●●●
●
●
●●●
●●
●
●
●
●
●●
●●
●
●●●
●●
●
●
●
●
●
●
●
●●
●●
●
●●
●●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●●
●
●
●●
●
●●●
●
●●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●●●●●
●
●
●●
●
●
●●
●
●
●
●
●
●●●●●
●
●●
●
●
●
●
●
●
●●●●
●
●
●
●
●●
●
●
●
●●●●●●
●
●●
●
●●
●
●●
●
●
●
●●
●●
●
●
●●
●●
●
●
●
●●●
●
●
●●●●●●●●●
●●
●●
●●
●
●
●
●●●
●●●●
●●●●●●●
●●●●●●●
●●●●
●
●●
●
●●
●
●
●●
●
●
●
●
●●●●
●
●
●
●●
●●●●
●
●
●
●
●●●●●●●●●
●●●●
●●●
●●●
●
●
●
●●●
●
●
●
●
●●
●
●●
●●●
●●
●●
●●
●
●●
●●
●●●●●
●
●●
●●
●
●●●
●
●●●●●
●
●●●●●●●
●●●●●
●●●
●
●●
●
●●
●
●●●●●●●●●
●
●
●●●
●●
●
●●●
●
●
●●●●●●●
●
●
●●
●●
●
●●●●●●●●●
●●
●●
●
●●●
●
●●
●●●
●
●
●●
●
●
●
●●●●●●●●●●
●●●●●●●
●
●●●●●
●●
●●●●●●●
●●●●●●●
●●●
●●●●●●●●●●●●●●●●●
●●●
●●
●●
●●●●
●●
Dijkstra Rank
Query Time [μs]
28
29
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
10
100
1000
SHARC
bidirectional SHARC
Figure 5: Comparison of uni- and bidirectional SHARC using the Dijkstra rank methodology [27]. The results are
represented as box-and-whisker plot [32]: each box spreads from the lower to the upper quartile and contains the
median, the whiskers extend to the minimum and maximum value omitting outliers, which are plotted individually.
Note that we use a logarithmic scale due to outliers.
Unidirectional SHARC gets slower with increasing rank
but the median stays below 0.6 ms while for bidirec-
tional SHARC the median of the queries stays below
0.2 ms. However, for the latter, query times increase up
to ranks of 213 which is roughly the size of cells at the
lowest level. Above this rank query times decrease and
increase again till the size of cells at level 1 is reached.
Interestingly, this eﬀect deriving from the partition can-
not be observed for the unidirectional variant.
Com-
paring uni- and bidirectional SHARC we observe more
outliers for the latter which is mainly due to less levels.
Still, all outliers are below 3 ms.
Table 2: Performance of SHARC on diﬀerent metrics
using the European road instance. Multi-metric refers
to the variant with one arc-ﬂag and three edge weights
(one weight per metric) per edge, while single refers to
running SHARC on the applied metric.
Prepro
Query
proﬁle
metric
[h:m]
[B/n]
#settled
[ms]
linear
single
2:17
13
1 114
0.39
multi
6:51
16
1 392
0.51
slow car
single
1:56
14
1 146
0.41
multi
6:51
16
1 372
0.50
fast car
single
2:24
13
1 063
0.37
multi
6:51
16
1 348
0.49
Multi-Metric Queries. The original dataset of
Western Europe contains 13 diﬀerent road categories.
By applying diﬀerent speed proﬁles to the categories
we obtain diﬀerent metrics. Tab. 2 gives an overview
of the performance of SHARC when applied to metrics
representing typical average speeds of slow/fast cars.
Moreover, we report results for the linear proﬁle which
is most often used in other publications and is obtained
by assigning average speeds of 10, 20, . . . , 130 to the
13 categories. Finally, results are given for multi-metric
SHARC, which stores only one arc-ﬂag for each edge.
As expected, SHARC performs very well on other
metrics based on travel times. Stunningly, the loss in
performance is only very little when storing only one
arc-ﬂag for all three metrics.
However, the overhead
increases due to storing more edge weights for shortcuts
and the size of the arc-ﬂags vector increases slightly.
Due to the fact that we have to compute arc-ﬂags for all
metrics during preprocessing, the computational eﬀort
increases.
5.1.2
Timetable Information Networks. Unlike
bidirectional approaches, SHARC-Routing can be used
for timetable information. In general, two approaches
exist to model timetable information as graphs: time-
dependent and time-expanded networks (cf. [26] for
details). In such networks timetable information can be
obtained by running a shortest path query. However, in
21

Table 3: Performance of plain Dijkstra and SHARC
on a local and long-distance time-expanded timetable
networks, unit disk graphs (udg) with average degree
5 and 7, and grid graphs with 2 and 3 number of
dimensions. Due to the smaller size of the input, we
use a 2-level partition with 16,112 cells.
Prepro
Query
graph
tech.
[h:m] [B/n]
#sett
[ms]
rail
Dijkstra
0:00
0 1 299 830
406.2
local
SHARC
10:02
9
11 006
3.8
rail
Dijkstra
0:00
0
609 352
221.2
long
SHARC
3:29
15
7 519
2.2
udg
Dijkstra
0:00
0
487 818
257.3
deg.5
SHARC
0:01
16
568
0.3
udg
Dijkstra
0:00
0
521 874
330.1
deg.7
SHARC
0:10
42
1 835
1.0
grid
Dijkstra
0:00
0
125 675
36.7
2–dim
SHARC
0:32
60
1 089
0.4
grid
Dijkstra
0:00
0
125 398
78.6
3–dim
SHARC
1:02
97
5 839
1.9
both models a backward search is prohibited as the time
of arrival is unknown in advance. Tab. 3 reports the
results of SHARC on 2 time-expanded networks: The
ﬁrst represents the local traﬃc of Berlin/Brandenburg,
has 2 599 953 nodes and 3 899 807 edges, the other graph
depicts long distance connections of Europe (1 192 736
nodes, 1 789 088 edges). For comparison, we also report
results for plain Dijkstra.
For time-expanded railway graphs we observe an in-
crease in performance of factor 100 over plain Dijkstra
but preprocessing is still quite high which is mainly due
to the partition. The number of boundary nodes is very
high yielding high preprocessing times. However, com-
pared to other techniques (see [2]) SHARC (clearly) out-
performs any other technique when applied to timetable
information system.
5.1.3
Other inputs. In order to show the robustness
of SHARC-Routing we also present results on synthetic
data.
On the one hand, 2- and 3-dimensional grids
are evaluated. The number of nodes is set to 250 000,
and thus, the number of edges is 1 and 1.5 million,
respectively.
Edge weights are picked uniformly at
random from 1 to 1000. On the other hand, we evaluate
random geometric graphs—so called unit disk graphs—
which are widely used for experimental evaluations in
the ﬁeld of sensor networks (see e.g. [19]). Such graphs
are obtained by arranging nodes uniformly at random
on the plane and connecting nodes with a distance
below a given threshold. By applying diﬀerent threshold
values we vary the density of the graph. In our setup, we
use graphs with about 1 000 000 nodes and an average
degree of 5 and 7, respectively. As metric, we use the
distance between nodes according to their embedding.
The results can be found in Tab. 3.
We observe that SHARC provides very good results
for all inputs. For unit disk graphs, performance gets
worse with increasing degree as the graph gets denser.
The same holds for grid graphs when increasing the
number of dimensions.
5.2
Time-Dependency. Our ﬁnal testset is per-
formed on a time-dependent variant of the European
road network instance. We interpret the initial values
as empty roads and add transit times according to rush
hours. Due to the lack of data we increase all motor-
ways by a factor of two and all national roads by a
factor of 1.5 during rush hours. Our model is inspired
by [11]. Our time-dependent implementation assigns 24
diﬀerent weights to edges, each representing the edge
weight at one hour of the day. Between two full hours,
we interpolate the real edge weight linearly. An easy
approach would be to store 24 edge weights separately.
As this consumes a lot of memory, we reduce this over-
head by storing factors for each hour between 5:00 and
22:00 of the day and the edge weight representing the
empty road. Then we compute the travel time of the
day by multiplying the initial edge weight with the fac-
tor (afterwards, we still have to interpolate). For each
factor at the day, we store 7 bits resulting in 128 addi-
tional bits for each time-dependent edge. Note that we
assume that roads are empty between 23:00 and 4:00.
Another problem for time-dependency is shortcut-
ting time-dependent edges.
We avoid this problem
by not bypassing nodes which are incident to a time-
dependent edge which has the advantage that the space-
overhead for additional shortcuts stay small.
Tab. 4
shows the performance of γ-SHARC for diﬀerent ap-
proximation values. Like in the static scenario we use
our default settings. For comparison, the values of time-
dependent Dijkstra and ALT are also given. As we
perform approximative SHARC-queries, we report three
types of errors: By error-rate we denote the percentage
of inaccurate queries. Besides the number of inaccurate
queries it is also important to know the quality of a
found path. Thus, we report the maximum and average
relative error of all queries, computed by 1 −μs/μD,
where μs and μD depict the lengths of the paths found
by SHARC and plain Dijkstra, respectively.
We observe that using γ values higher than 1.0
drastically reduces query performance.
While error-
rates are quite high for low γ values, the relative error is
still quite low. Thus, the quality of the computed paths
22

Table 4: Performance of the time-dependent versions of Dijkstra, ALT, and SHARC on the Western European
road network with time-dependent edge weights. For ALT, we use 16 avoid landmarks [16].
error
Prepro
Query
γ
rate
rel. avg.
rel. max
[h:m]
[B/n]
#settled
[ms]
Dijkstra
-
0.0%
0.000%
0.00%
0:00
0
9 016 965
8 890.1
ALT
-
0.0%
0.000%
0.00%
0:16
128
2 763 861
2 270.7
SHARC
1.000
61.5%
0.242%
15.90%
2:51
13
9 804
3.8
1.005
39.9%
0.096%
15.90%
2:53
13
113 993
61.2
1.010
32.9%
0.046%
15.90%
2:51
13
221 074
131.3
1.020
29.5%
0.024%
14.37%
2:50
13
285 971
182.7
1.050
27.4%
0.013%
2.19%
2:51
13
312 593
210.9
1.100
26.5%
0.009%
0.56%
2:52
12
321 501
220.8
is good, although in the worst-case the found path is
15.9% longer than the shortest. However, by increasing
γ we are able to reduce the error-rate and the relative
error signiﬁcantely: The error-rate drops below 27%,
the average error is below 0.01%, and in worst case the
found path is only 0.56% longer than optimal. Generally
speaking, SHARC routing allows a trade-oﬀbetween
quality and performance. Allowing moderate errors, we
are able to perform queries 2 000 times faster than plain
Dijkstra, while queries are still 40 times faster when
allowing only very small errors.
Comparing SHARC (with γ = 1.1) and ALT, we
observe that SHARC queries are one order of magnitude
faster but for the price of correctness.
In addition,
the overhead is much smaller than for ALT. Note that
we do not have to store time-dependent edge weights
for shortcuts due to our weaker bypassing criterion.
Summarizing, SHARC allows to perform fast queries
in time-dependent networks with moderate error-rates
and small average relative errors.
6
Conclusion
In this work, we introduced SHARC-Routing which
combines several ideas from Highway Hierarchies, Arc-
Flags, and the REAL-algorithm. More precisely, our ap-
proach can be interpreted as a unidirectional hierarchi-
cal approach: SHARC steps up the hierarchy at the be-
ginning of the query, runs a strongly goal-directed query
on the highest level and automatically steps down the
hierarchy as soon as the search is approaching the target
cell. As a result we are able to perform queries as fast
as bidirectional approaches but SHARC can be used in
scenarios where former techniques fail due to their bidi-
rectional nature. Moreover, a bidirectional variant of
SHARC clearly outperforms existing techniques except
Transit Node Routing which needs much more space
than SHARC.
Regarding future work, we are very optimistic that
SHARC is very helpful when running multi-criteria
queries due to the performance in multi-metric scenar-
ios. In [15], an algorithm is introduced for computing
exact reach values which is based on partitioning the
graph. As our pruning rule would also hold for reach
values, we are optimistic that we can compute exact
reach values for our output graph with our SHARC pre-
processing. For the time-dependent scenario one could
think of other ways to determine good approximation
values. Moreover, it would be interesting how to per-
form correct time-dependent SHARC queries.
SHARC-Routing itself also leaves room for improve-
ment. The pruning rule could be enhanced in such a
way that we can prune all cells. Moreover, it would be
interesting to ﬁnd better additional shortcuts, maybe
by adapting the algorithms from [12] to approximate
betweenness better. Another interesting question aris-
ing is whether we can further improve the contraction
routine.
And ﬁnally, ﬁnding partitions optimized for
SHARC is an interesting question as well.
Summarizing, SHARC-Routing is a powerful, easy,
fast and robust unidirectional technique for performing
shortest-path queries in large networks.
Acknowledgments. We would like to thank Peter
Sanders and Dominik Schultes for interesting discus-
sions on contraction and arc-ﬂags. We also thank Daniel
Karch for implementing classic Arc-Flags. Finally, we
thank Moritz Hilger for running a preliminary experi-
ment with his new centralized approach.
References
[1] H. Bast, S. Funke, P. Sanders, and D. Schultes. Fast
Routing in Road Networks with Transit Nodes. Sci-
ence, 316(5824):566, 2007.
23

[2] R. Bauer, D. Delling, and D. Wagner. Experimental
Study on Speed-Up Techniques for Timetable Infor-
mation Systems.
In C. Liebchen, R. K. Ahuja, and
J. A. Mesa, editors, Proceedings of the 7th Workshop
on Algorithmic Approaches for Transportation Model-
ing, Optimization, and Systems (ATMOS’07). Schloss
Dagstuhl, Germany, 2007.
[3] R. Bauer, D. Delling, and D. Wagner. Shortest-Path
Indices: Establishing a Methodology for Shortest-Path
Problems.
Technical Report 2007-14, ITI Wagner,
Faculty of Informatics, Universit¨at Karlsruhe (TH),
2007.
[4] U. Brandes. A Faster Algorithm for Betweenness Cen-
trality. Journal of Mathematical Sociology, 25(2):163–
177, 2001.
[5] D. Delling, M. Holzer, K. M¨uller, F. Schulz, and
D. Wagner. High-Performance Multi-Level Graphs. In
Demetrescu et al. [9].
[6] D. Delling, P. Sanders, D. Schultes, and D. Wagner.
Highway Hierarchies Star. In Demetrescu et al. [9].
[7] D. Delling and D. Wagner. Landmark-Based Routing
in Dynamic Graphs. In Demetrescu [8], pages 52–65.
[8] C. Demetrescu, editor. Proceedings of the 6th Work-
shop on Experimental Algorithms (WEA’07), volume
4525 of Lecture Notes in Computer Science. Springer,
June 2007.
[9] C. Demetrescu, A. V. Goldberg, and D. S. Johnson,
editors.
9th DIMACS Implementation Challenge -
Shortest Paths, November 2006.
[10] E. W. Dijkstra. A Note on Two Problems in Connexion
with Graphs.
Numerische Mathematik, 1:269–271,
1959.
[11] I. C. Flinsenberg.
Route Planning Algorithms for
Car Navigation. PhD thesis, Technische Universiteit
Eindhoven, 2004.
[12] R. Geisberger, P. Sanders, and D. Schultes. Better Ap-
proximation of Betweenness Centrality. In Proceedings
of the 10th Workshop on Algorithm Engineering and
Experiments (ALENEX’08). SIAM, 2008. to appear.
[13] A. V. Goldberg and C. Harrelson.
Computing the
Shortest Path: A* Search Meets Graph Theory.
In
Proceedings of the 16th Annual ACM–SIAM Sympo-
sium on Discrete Algorithms (SODA’05), pages 156–
165, 2005.
[14] A. V. Goldberg, H. Kaplan, and R. F. Werneck. Reach
for A*: Eﬃcient Point-to-Point Shortest Path Algo-
rithms.
In Proceedings of the 8th Workshop on Al-
gorithm Engineering and Experiments (ALENEX’06),
pages 129–143. SIAM, 2006.
[15] A. V. Goldberg, H. Kaplan, and R. F. Werneck. Better
Landmarks Within Reach.
In Demetrescu [8], pages
38–51.
[16] A. V. Goldberg and R. F. Werneck.
Computing
Point-to-Point Shortest Paths from External Memory.
In Proceedings of the 7th Workshop on Algorithm
Engineering and Experiments (ALENEX’05), pages
26–40. SIAM, 2005.
[17] M. Hilger. Accelerating Point-to-Point Shortest Path
Computations in Large Scale Networks.
Master’s
thesis, Technische Universit¨at Berlin, 2007.
[18] M. Hilger, E. K¨ohler, R. M¨ohring, and H. Schilling.
Fast Point-to-Point Shortest Path Computations with
Arc-Flags. In Demetrescu et al. [9].
[19] F. Kuhn, R. Wattenhofer, and A. Zollinger.
Worst-
Case Optimal and Average-Case Eﬃcient Geometric
Ad-Hoc Routing. In Proceedings of the 4th ACM In-
ternational Symposium on Mobile Ad Hoc Networking
and Computing (MOBIHOC’03), 2003.
[20] K. Lab.
METIS - Family of Multilevel Partitioning
Algorithms, 2007.
[21] U. Lauther.
Slow Preprocessing of Graphs for Ex-
tremely Fast Shortest Path Calculations, 1997. Lecture
at the Workshop on Computational Integer Program-
ming at ZIB.
[22] U. Lauther.
An Extremely Fast, Exact Algorithm
for Finding Shortest Paths in Static Networks with
Geographical Background. volume 22, pages 219–230.
IfGI prints, 2004.
[23] R. M¨ohring, H. Schilling, B. Sch¨utz, D. Wagner,
and T. Willhalm.
Partitioning Graphs to Speedup
Dijkstra’s Algorithm. ACM Journal of Experimental
Algorithmics, 11:2.8, 2006.
[24] B. Monien and S. Schamberger.
Graph Partition-
ing with the Party Library:
Helpful-Sets in Prac-
tice. In Proceedings of the 16th Symposium on Com-
puter Architecture and High Performance Computing
(SBAC-PAD’04), pages 198–205. IEEE Computer So-
ciety, 2004.
[25] F. Pellegrini.
SCOTCH: Static Mapping, Graph,
Mesh and Hypergraph Partitioning, and Parallel and
Sequential Sparse Matrix Ordering Package, 2007.
[26] E. Pyrga, F. Schulz, D. Wagner, and C. Zaroliagis.
Eﬃcient Models for Timetable Information in Public
Transportation Systems. ACM Journal of Experimen-
tal Algorithmics, 12:Article 2.4, 2007.
[27] P. Sanders and D. Schultes.
Highway Hierarchies
Hasten Exact Shortest Path Queries. In Proceedings of
the 13th Annual European Symposium on Algorithms
(ESA’05), volume 3669 of Lecture Notes in Computer
Science, pages 568–579. Springer, 2005.
[28] P. Sanders and D. Schultes. Engineering Highway Hi-
erarchies. In Proceedings of the 14th Annual European
Symposium on Algorithms (ESA’06), volume 4168 of
Lecture Notes in Computer Science, pages 804–816.
Springer, 2006.
[29] P. Sanders and D. Schultes. Engineering Fast Route
Planning Algorithms. In Demetrescu [8], pages 23–36.
[30] P. Sanders and D. Schultes.
Engineering Highway
Hierarchies.
submitted for publication, preliminary
version at http://algo2.iti.uka.de/schultes/hwy/,
2007.
[31] D. Schultes and P. Sanders. Dynamic Highway-Node
Routing. In Demetrescu [8], pages 66–79.
[32] R. D. Team.
R: A Language and Environment for
Statistical Computing, 2004.
[33] D. Wagner and T. Willhalm.
Speed-Up Techniques
24

for Shortest-Path Computations. In Proceedings of the
24th International Symposium on Theoretical Aspects
of Computer Science (STACS’07), Lecture Notes in
Computer Science, pages 23–36. Springer, February
2007.
[34] D. Wagner, T. Willhalm, and C. Zaroliagis. Geometric
Containers for Eﬃcient Shortest-Path Computation.
ACM Journal of Experimental Algorithmics, 10:1.3,
2005.
A
Proof of Correctness
We here present a proof of correctness for SHARC-
Routing. SHARC directly adapts the query from classic
Arc-Flags, which is proved to be correct. Hence, we only
have to show the correctness for all techniques that are
used for SHARC-Routing but not for classic Arc-Flags.
The proof is logically split into two parts.
First,
we prove the correctness of the preprocessing without
the reﬁnement phase.
Afterwards, we show that the
reﬁnement phase is correct as well.
A.1
Initialization and Main Phase. We denote by
Gi the graph after iteration step i, i = 1, . . . , L −1. By
G0 we denote the graph directly before iteration step 1
starts. The level l(u) of a node u is deﬁned to be the
integer i such that u is contained in Gi−1 but not in Gi.
We further deﬁne the level of a node contained in GL−1
to be L.
The correctness of the multi-level arc-ﬂag approach
is known.
The correctness of the handling of the 1-
shell nodes is due to the fact that a shortest path
starting from or ending at a 1-shell node u is either
completely included in the attached tree T in which
also u is contained, or has to leave or enter T via the
corresponding core-node.
We want to stress that, when computing arc-ﬂags,
shortest paths do not have to be unique. We remember
how SHARC handles that: In each level l < L −1
all shortest paths are considered, i.e., a shortest path
directed acyclic graph is grown instead of a shortest
paths tree and a ﬂag for a cell C and an edge (u, v) is set
true, if at least one shortest path to C containing (u, v)
exists. In level L −1, all shortest paths are considered,
that are hop minimal for given source and target, i.e., a
ﬂag for a cell C and an edge (u, v) is set true, if at least
one shortest path to C containing (u, v) exists that is
hop minimal among all shortest paths with same source
and target.
We observe that the distances between two arbi-
trary nodes u and v are the same in the graph G0 and
i
k=0 Gk for any i = 1, . . . , L −1.
Hence, to proof the correctness of unidirectional
SHARC-Routing without the reﬁnement phase and
without 1-shell nodes we additionally have to proof the
following lemma:
Lemma A.1. Given arbitrary nodes s and t in G0, for
which there is a path from s to t in G0.
At each
step i of the SHARC-preprocessing there exists a short-
est s-t-path P = (v1, . . . , vj1; u1, . . . , uj2; w1, . . . , wj3),
j1, j2, j3 ∈
0, in i
k=0 Gk, such that
• the nodes v1, . . . , vj1 and w1, . . . , wj3 have level of
at most i,
• the nodes u1, . . . , uj2 have level of at least i + 1
• uj2 and t are in the same cell at level i
• for each edge e of P, the arc-ﬂags assigned to e
until step i allow the path P to t.
We use the convention that jk = 0, k ∈{1, 2, 3} means
that the according subpath is void.
The lemma guarantees that, at each iteration step,
arc-ﬂags are set properly.
The correctness of the
bidirectional variant follows from the observation that
a hop-minimal shortest path on a graph is also a hop-
minimal shortest path on the reverse graph.
Proof. We show the claim by induction on the iteration
steps.
The claim holds trivially for i = 0.
The
inductive step works as follows: Assume the claim holds
for step i.
Given arbitrary nodes s and t, for which
there is a path from s to t in G0.
We denote by
P = (v1, . . . , vj1; u1, . . . , uj2; w1, . . . , wj3) the s-t-path
according to the lemma for step i.
The iteration step i + 1 consists of the contraction
phase, the insertion of boundary shortcuts in case i+1 =
L −1, the arc-ﬂag computation and the pruning phase.
We consider the phases one after another:
After the Contraction Phase. There exists a maxi-
mal path (uℓ1, uℓ2, . . . , uℓd) with 1 ≤ℓ1, ≤. . . ≤ℓd ≤k
for which
• for each f = 1, . . . , d−1 either ℓf +1 = ℓf+1 or the
subpaths (uℓf , uℓf +1, . . . uℓf+1) have been replaced
by a shortcut,
• the nodes u1, . . . , uℓ1−1 have been deleted, if ℓ1 ̸= 1
and
• the nodes uℓd+1, . . . , uk have been deleted, if ℓd ̸=
k.
By the construction of the contraction routine we know
• (uℓ1, uℓ2, . . . , uℓd) is also a shortest path
25

• uℓd is in the same component as uk in all levels
greater than i (because of cell aware contraction)
• the deleted edges in (u1, . . . , uℓ1−1) either already
have their arc-ﬂags for the path P assigned. Then
the arc-ﬂags are correct because of the inductive
hypothesis.
Otherwise, We know that the nodes
u1, . . . , uℓ1−1 are in the component. Hence, all arc-
ﬂags for all higher levels are assigned true.
• the deleted edges in (uℓd+1, . . . , uk) either already
have their arc-ﬂags for the path P assigned, then
arc-ﬂags are correct because of the inductive hy-
pothesis. Otherwise, by cell-aware contraction we
know that uℓd+1, . . . , uk are in the same component
as t for all levels at least i. As the own-cell ﬂag al-
ways is set true for deleted edges the path stays
valid.
As distances do not change during preprocessing
we know that, for arbitrary i, 0 ≤i ≤L −1 a
shortest path in Gi is also a shortest path in L−1
k=0 Gk.
Concluding, the path ˆP = (v1, . . . , vj1, u1, . . . , uℓ1−1;
uℓ1, uℓ2, . . . , uℓd; uℓd+1, . . . , uk, w1, . . . , wj3) fullﬁlls all
claims of the lemma for iteration step i + 1.
After Insertion of Boundary Shortcuts. Here, the
claim holds trivially.
After Arc-Flags Computation. Here, the claim also
holds trivially.
After Pruning. We consider the path ˆP obtained from
the contraction step. Let (ulr, ulr+1) be an edge of ˆP
deleted in the pruning step, for which ulr is not in the
same cell as uld at level i + 1. As there exists a shortest
path to uld not only the own-cell ﬂag of (ulr, ulr+1) is
set, which is a contradiction to the assumption that
(ulr, ulr+1) has been deleted in the pruning step.
Furthermore, let (ulz, ulz+1) be an edge of P deleted
in the pruning step.
Then, all edges on P after
(ulz, ulz+1) are also deleted in that step.
Summariz-
ing, if no edge on ˆP is deleted in the pruning step,
then ˆP fullﬁlls all claims of the lemma for iteration step
i + 1.
Otherwise, the path (v1, . . . , vj1, u1, . . . , uℓ1−1;
uℓ1, uℓ2, . . . ; ulk, . . . , uℓd, uℓd+1, . . . , uk, w1, . . . , wj3) full-
ﬁlls all claims of the lemma for iteration step i+1 where
ulk, ulk+1 is the ﬁrst edge on P that has been deleted in
the pruning step.
Summarizing, Lemma A.1 holds during all phases
of all iteration steps of SHARC-preprocessing. So, the
preprocessing algorithm (without the reﬁnement phase)
is correct.
□
A.2
Reﬁnement phase. Recall that the own-cell
ﬂag does not get altered by the reﬁnement routine.
Hence, we only have to consider ﬂags for other cells.
Assume we perform the propagation routine at a level l
to a level l node s.
A path P from s to a node t in another cell on
level ≥l needs to contain a level > l node that is in
the same cell as u because of the cell-aware contraction.
Moreover, with iterated application of Lemma A.1 we
know that there must be an (arc-ﬂag valid) shortest s-t-
path P for which the sequence of the levels of the nodes
ﬁrst is monotonically ascending and then monotonically
descending. In fact, to cross a border of the current cell
at level l, at least two level > l nodes are on P. We
consider the ﬁrst level > l node u1 on P. This must
be an entry node of s. The node u2 after u1 on P is
covered and therefore no entry node.
Furthermore it
is of level > l.
Hence, the ﬂags of the edge (u1, u2)
are propagated to the ﬁrst edge on P and the claim
holds which proves that the reﬁnement phase is correct.
Together with Lemma A.1 and the correctness of the
multi-level Arc-Flags query, SHARC-Routing is correct.
26

Obtaining Optimal k-Cardinality Trees Fast
Markus Chimani∗
Maria Kandyba∗†
Ivana Ljubi´c‡§
Petra Mutzel∗
Abstract
Given an undirected graph G = (V, E) with edge weights and
a positive integer number k, the k-Cardinality Tree problem
consists of ﬁnding a subtree T of G with exactly k edges and
the minimum possible weight. Many algorithms have been
proposed to solve this NP-hard problem, resulting in mainly
heuristic and metaheuristic approaches.
In this paper we present an exact ILP-based algo-
rithm using directed cuts. We mathematically compare the
strength of our formulation to the previously known ILP
formulations of this problem, and give an extensive study
on the algorithm’s practical performance compared to the
state-of-the-art metaheuristics.
In contrast to the widespread assumption that such a
problem cannot be eﬃciently tackled by exact algorithms for
medium and large graphs (between 200 and 5000 nodes), our
results show that our algorithm not only has the advantage
of proving the optimality of the computed solution, but also
often outperforms the metaheuristic approaches in terms of
running time.
1
Introduction
We consider the k-Cardinality Tree problem (KCT):
given an undirected graph G = (V, E), an edge weight
function w : E →R, and a positive integer number k,
ﬁnd a subgraph T of G which is a minimum weight tree
with exactly k edges. This problem has been extensively
studied in literature as it has various applications, e.g.,
in oil-ﬁeld leasing, facility layout, open pit mining, ma-
trix decomposition, quorum-cast routing, telecommuni-
cations, etc [9]. A large amount of research was devoted
to the development of heuristic [5, 14] and, in particular,
metaheuristic methods [4, 8, 11, 7, 25]. An often used
argument for heuristic approaches is that exact methods
for this NP-hard problem would require too much com-
putation time and could only be applied to very small
graphs [9, 10].
The problem also received a lot of attention in the
∗Technical
University
of
Dortmund;
{markus.chimani,
maria.kandyba, petra.mutzel}@cs.uni-dortmund.de
†Supported by the German Research Foundation (DFG)
through the Collaborative Research Center “Computational In-
telligence” (SFB 531)
‡University of Vienna; ivana.ljubic@univie.ac.at
§Supported by the Hertha-Firnberg Fellowship of the Austrian
Science Foundation (FWF)
approximation algorithm community [1, 3, 17, 18]: a
central idea thereby is the primal-dual scheme, based
on integer linear programs (ILPs), which was pro-
posed by Goemans and Williamson [19] for the prize-
collecting Steiner tree problem. An exact approach was
presented by Fischetti et al. [15], by formulating an
ILP based on general subtour elimination constraints
(Gsec). This formulation was implemented by Ehrgott
and Freitag [13] using a Branch-and-Cut approach. The
resulting algorithm was only able to solve graphs with
up to 30 nodes, which may be mainly due to the com-
parably weak computers in 1996.
In this paper we show that the traditional argu-
ment for metaheuristics over exact algorithms is de-
ceptive on this and related problems.
We propose
a novel exact ILP-based algorithm which can indeed
be used to solve all known benchmark instances of
KCTLIB [6]—containing graphs of up to 5000 nodes—
to provable optimality. Furthermore, our algorithm of-
ten, in particular on mostly all graphs with up to 1000
nodes, is faster than the state-of-the-art metaheuristic
approaches, which can neither guarantee nor assess the
quality of their solution.
To
achieve
these
results,
we
present
Branch-
and-Cut algorithms for KCT and NKCT—the node-
weighted variant of KCT. Therefore, we transform
both KCT and NKCT into a similar directed and
rooted problem called k-Cardinality Arborescence prob-
lem (KCA), and formulate an ILP for the latter, see Sec-
tion 2. In the section thereafter, we provide polyhedral
and algorithmic comparison to the known Gsec formu-
lation. In Section 4, we describe the resulting Branch-
and-Cut algorithm in order to deal with the exponential
ILP size. We conclude the paper with the extensive ex-
perimental study in Section 5, where we compare our
algorithm with the state-of-the-art metaheuristics for
the KCT.
2
Directed Cut Approach
2.1
Transformation into the k-Cardinality Ar-
borescence Problem. Let D = (VD, AD) be a di-
rected graph with a distinguished root vertex r ∈
VD and arc costs ca for all arcs a ∈AD.
The k-
Cardinality Arborescence problem (KCA) consists of
ﬁnding a weight minimum rooted tree TD with k arcs
27

which is directed from the root outwards.
More for-
mally, TD has to satisfy the following properties:
(P1) TD contains exactly k arcs,
(P2) for all v ∈V (TD) \ {r}, there exists a directed
path r →v in TD, and
(P3) for all v ∈V (TD) \ {r}, v has in-degree 1 in TD.
We
transform
any
given
KCT
instance
(G
=
(V, E), w, k)
into
a
corresponding
KCA
instance
(Gr, r, c, k + 1) as follows: we replace each edge {i, j}
of G by two arcs (i, j) and (j, i), introduce an artiﬁ-
cial root vertex r and connect r to every node in V .
Hence we obtain a digraph Gr = (V ∪{r}, A ∪Ar) with
A = {(i, j), (j, i) | {i, j} ∈E} and Ar = {(r, j) | j ∈V }.
For each arc a = (i, j) we deﬁne the cost function
c(a) := 0 if i = r, and c(a) := w({i, j}) otherwise.
To be able to interpret each feasible solution TGr of
this resulting KCA instance as a solution of the original
KCT instance, we impose an additional constraint
(P4) TGr contains only a single arc of Ar.
If this property is satisﬁed, it is easy to see that a
feasible KCT solution with the same objective value can
be obtained by removing r from TGr and interpreting
the directed arcs as undirected edges.
2.2
The
Node-weighted
k-Cardinality
Tree
Problem. The
Node-weighted
k-Cardinality
Tree
problem (NKCT) is deﬁned analogously to KCT but
its weight function w′ : V →R uses the nodes as its
basic set, instead of the edges (see, e.g., [10] for the list
of references).
We can also consider the general All-
weighted k-Cardinality Tree problem (AKCT), where a
weight-function w for the edges, and a weight-function
w′ for the nodes are given.
We can transform any NKCT and AKCT instance
into a corresponding KCA instance using the ideas
of [24]: the solution of KCA is a rooted, directed tree
where each vertex (except for the unweighted root)
has in-degree 1.
Thereby, a one-to-one relationship
between each selected arc and its target node allows
us to precompute the node-weights into the arc-weights
of KCA: for all (i, j) ∈A ∪Ar we have c((i, j)) := w′(j)
for NKCT, and c((i, j)) := w({i, j}) + w′(j) for AKCT.
2.3
ILP for the KCA. In the following let the
graphs be deﬁned as described in Section 2.1.
To
model KCA as an ILP, we introduce two sets of binary
variables:
xa, yv ∈{0, 1}
∀a ∈A ∪Ar, ∀v ∈V
Thereby, the variables are 1, if the corresponding vertex
or arc is in the solution and 0 otherwise.
Let S ⊆V . The sets E(S) and A(S) are the edges
and arcs of the subgraphs of G and Gr, respectively,
induced by S.
Furthermore, we denote by δ+(S) =
{(i, j) ∈A∪Ar | i ∈S, j ∈V \S} and δ−(S) = {(i, j) ∈
A ∪Ar | i ∈V \ S, j ∈S} the outgoing and ingoing
edges of a set S, respectively. We can give the following
ILP formulation, using x(B) := 
b∈B xb, with B ⊆A,
as a shorthand:
(2.1)
DCut :
min

a∈A
c(a) · xa
x(δ−(S)) ≥yv
∀S ⊆V \ {r}, ∀v ∈S
(2.2)
x(δ−(v)) = yv
∀v ∈V
(2.3)
x(A) = k
(2.4)
x(δ+(r)) = 1
(2.5)
xa, yv ∈{0, 1}
∀a ∈A ∪Ar, ∀v ∈V
(2.6)
The dcut-constraints (2.2) ensure property (P2) via
directed cuts, while property (P3) is ensured by the
in-degree constraints (2.3).
Constraint (2.4) ensures
the k-cardinality requirement (P1) and property (P4)
is modeled by (2.5).
Lemma 2.1. By
replacing
all
in-degree
constraints
(2.3) by a single node-cardinality constraint
(2.7)
y(V ) = k + 1,
we obtain an equivalent ILP and an equivalent LP-
relaxation.
Proof. The node-cardinality constraint can be gener-
ated directly from (2.3) and (2.4), (2.5).
Vice versa,
we can generate (2.3) from (2.7), using the dcut-
constraints (2.2).
Although the formulation using (2.7) requires less
constraints, the ILP using in-degree constraints has
certain advantages in practice, see Section 4.
3
Polyhedral Comparison
In [15], Fischetti et al. give an ILP formulation for
the undirected KCT problem based on general subtour
elimination constraints (Gsec).
We reformulate this
approach and show that both Gsec and DCut are
equivalent from the polyhedral point of view.
In order to distinguish between undirected edges
and directed arcs we introduce the binary variables
ze ∈{0, 1} for every edge e ∈E, which are 1 if e ∈T
and 0 otherwise. For representing the selection of the
28

nodes we use the y-variables as in the previous section.
The constraints (3.9) are called the gsec-constraints.
(3.8)
Gsec :
min

e∈E
c(e) · ze
z(E(S)) ≤y(S \ {t})
∀S ⊆V, |S| ≥2, ∀t ∈S
(3.9)
z(E) = k
(3.10)
y(V ) = k + 1
(3.11)
ze, yv ∈{0, 1}
∀e ∈E, ∀v ∈V
(3.12)
Let PD and PG be the polyhedra corresponding to
the DCut and Gsec LP-relaxations, respectively. I.e.,
PD := {
(x, y) ∈R|A∪Ar|+|V | | 0 ≤xe, yv ≤1
and (x, y) satisﬁes (2.2)–(2.5)
}
PG := {
(z, y) ∈R|E|+|V | | 0 ≤ze, yv ≤1
and (z, y) satisﬁes (3.9)–(3.11)
}
Theorem 3.1. The Gsec and the DCut formulations
have equally strong LP-relaxations, i.e.,
PG = projz(PD),
whereby projz(PD) is the projection of PD onto the
(z, y) variable space with z{i,j} = x(i,j) + x(j,i) for all
{i, j} ∈E.
Proof. We prove equality by showing mutual inclusion:
• projz(PD) ⊆PG: Any (¯z, ¯y) ∈projz(PD) satisﬁes
(3.10) by deﬁnition, and (3.11) by (2.3) and Lemma
2.1. Let ¯x be the vector from which we projected
the vector ¯z, and consider some S ⊆V with |S| ≥2
and some vertex t ∈S. We show that (¯z, ¯y) also
satisﬁes the corresponding gsec-constraint (3.9):
¯z(E(S)) = ¯x(A(S)) = 
v∈S ¯x(δ−(v)) −¯x(δ−(S))
(2.3)
= ¯y(S) −¯x(δ−(S))
(2.2)
≤¯y(S) −¯yt.
• PG ⊆projz(PD): Consider any (¯z, ¯y) ∈PG and a set
X := {
x ∈R|A∪Ar|
≥0
| x satisﬁes (2.5)
and xij + xji = ¯z{ij} ∀(i, j) ∈A
}.
Every such projective vector ¯x ∈X clearly satisﬁes
(2.4).
In order to generate the dcut-inequalities
(2.2) for the corresponding (¯x, ¯y), it is suﬃcient to
show that we can always ﬁnd an ˆx ∈X, which
together with ¯y satisﬁes the indegree-constraints
(2.3). Since then, for any S ⊆V and t ∈S:
ˆx(δ−(S)) = 
v∈S ˆx(δ−(v)) −ˆx(A(S))
(2.3)
= ¯y(S) −¯z(E(S))
(3.9)
≥¯yt.
We show the existence of such an ˆx using a proof
technique similar to [20, proof of Claim 2], where
it was used for the Steiner tree problem.
An ˆx ∈X satisfying (2.3) can be interpreted as the
set of feasible ﬂows in a bipartite transportation
network (N, L), with N := (E ∪{r}) ∪V . For each
undirected edge e = (u, w) ∈E in G, our network
contains exactly two outgoing arcs (e, u), (e, w) ∈
L. Furthermore, L contains all arcs of Ar. For all
nodes e ∈E in N we deﬁne a supply s(e) := ¯ze; for
the root r we set s(r) := 1. For all nodes v ∈V in
N we deﬁne a demand d(v) := ¯yv.
Finding a feasible ﬂow for this network can be
viewed as a capacitated transportation problem on
a complete bipartite network with capacities either
zero (if the corresponding edge does not exist in L)
or inﬁnity. Note that in our network the sum of all
supplies is equal to the sum of all demands, due to
(3.10) and (3.11). Hence, each feasible ﬂow in such
a network will lead to a feasible ˆx ∈X. Such a
ﬂow exists if and only if for every set M ⊆N with
δ+
(N,L)(M) = ∅the condition
(3.13)
s(M) ≤d(M)
is satisﬁed, whereby s(M) and d(M) are the total
supply and the total demand in M, respectively,
cf. [16, 20]. In order to show that this condition
holds for (N, L), we distinguish between two cases;
let U := E ∩M:
r ∈M: Since r has an outgoing arc for every v ∈
V and δ+
(N,L)(M) = ∅, we have V
⊂M.
Condition (3.13) is satisﬁed, since s(r) = 1
and therefore:
s(M) = s(r) + ¯z(U) ≤s(r) + ¯z(E)
= ¯z(E) + 1
(3.10),(3.11)
=
¯y(V ) = d(M).
r /∈M: Let S := V ∩M. We then have U ⊆E(S).
If |S| ≤1 we have U = ∅and therefore (3.13)
is automatically satisﬁed.
For |S| ≥2, the
condition is also satisﬁed, since for every t ∈S
we have:
s(M) = ¯z(U) ≤¯z(E(S))
(3.9)
≤¯y(S) −¯yt
≤¯y(S) = d(M).
29

3.1
Other approaches.
3.1.1
Multi-Commodity Flow. One can formulate
a multi-commodity-ﬂow based ILP for KCA (Mcf)
as it was done for the prize-collecting Steiner tree
problem (PCST) [22], and augment it with cardinality
inequalities.
Analogously to the proof in [22], which
shows the equivalence of DCut and Mcf for PCST, we
can obtain:
Lemma 3.1. The LP-relaxation of Mcf for KCA is
equivalent to Gsec and DCut.
Nonetheless, we know from similar problems [12, 23]
that directed-cut based approaches are usually more
eﬃcient than multi-commodity ﬂows in practice.
3.1.2
Undirected Cuts for Approximation Al-
gorithms. In [17], Garg presents an approximation al-
gorithm for KCT, using an ILP for lower bounds (GU-
Cut).
It is based on undirected cuts and has to be
solved |V | times, once for all possible choices of a root
node r.
Lemma 3.2. DCut is stronger than GUCut.
Proof. Clearly, each feasible point in PD is feasible
in the LP-relaxation of GUCut using the projection
projz. On the other hand, using a traditional argument,
assume a complete graph on 3 nodes is given, where each
vertex variable is set to 1, and each edge variable is set
to 0.5. This solution is feasible for the LP-relaxation of
GUCut, but infeasible for DCut.
4
Branch-and-Cut Algorithm
Based on our DCut formulation, we developed and im-
plemented a Branch-and-Cut algorithm. For a general
description of the Branch-and-Cut scheme see, e.g., [27]:
Such algorithms start with solving an LP relaxation,
i.e., the ILP without the integrality properties, only
considering a certain subset of all constraints. Given
the fractional solution of this partial LP, we perform a
separation routine, i.e., identify constraints of the full
constraint set which the current solution violates. We
then add these constraints to our current LP and reit-
erate these steps. If at some point we cannot ﬁnd any
violated constraints, we have to resort to branching, i.e.,
we generate two disjoint subproblems, e.g., by ﬁxing a
variable to 0 or 1. By using the LP relaxation as a lower
bound, and some heuristic solution as an upper bound,
we can prune irrelevant subproblems.
In [13], a Branch-and-Cut algorithm based on the
Gsec formulation has been developed. Note that the
dcut-constraints are sparser than the gsec-constraints,
which in general often leads to a faster optimization
in practice.
This conjecture was experimentally con-
ﬁrmed, e.g., for the similar prize-collecting Steiner tree
problem [23], where a directed-cut based formulation
was compared to a Gsec formulation. The former was
both faster in overall running time and required less
iterations, by an order of 1–2 magnitudes. Hence we
can expect our DCut approach to have advantages over
Gsec in practice. In Section 4.2 we will discuss the for-
mal diﬀerences in the performances between the DCut
and the Gsec separation algorithms.
4.1
Initialization. Our algorithm starts with the
constraints (2.3), (2.4), and (2.5).
We prefer the in-
degree constrains (2.3) over the node-cardinality con-
straint (2.7), as they strengthen the initial LP and we
do not require to separate dcut-constraints with |S| = 1
later.
For the same reason, we add the orientation-
constraints
(4.14)
xij + xji ≤yi
∀i ∈V, ∀{i, j} ∈E
to our initial ILP. Intuitively, these constraints ensure a
unique orientation for each edge, and require for each se-
lected arc that both incident nodes are selected as well.
These constraints do not actually strengthen the DCut
formulation as they represent the gsec-constraints for
all two-element sets S = {i, j} ⊂V . From the proof
of Theorem 3.1, we know that these inequalities can be
generated with the help of (2.3) and (2.2). Nonethe-
less, as experimentally shown in [22] for PCST and is
also conﬁrmed by our own experiments, the addition of
(4.14) speeds up the algorithm tremendously, as they
do not have to be separated explicitly by the Branch-
and-Cut algorithm.
We also tried asymmetry constraints [22] to reduce
the search space by excluding symmetric solutions:
(4.15)
xrj ≤1 −yi
∀i, j ∈V, i < j.
They assure that for each KCA solution, the vertex
adjacent to the root is the one with the smallest possible
index.
Anyhow, we will see in our experiments that
the quadratic number of these constraints becomes a
hindrance for large graphs and/or small k in practice.
4.2
Separation. The dcut-constraints (2.2) can be
separated
in
polynomial
time
via
the
traditional
maximum-ﬂow separation scheme:
we compute the
maximum-ﬂow from r to each v ∈V using the edge
values of the current solution as capacities. If the ﬂow
is less than yv, we extract one or more of the induced
minimum (r, v)-cuts and add the corresponding con-
straints to our model.
In order to obtain more cuts
30

with a single separation step we also use nested- and
back-cuts [21, 23]. Indeed, using these additional cuts
signiﬁcantly speeds up the computation.
Recall that in a general separation procedure we
search for the most violated inequality of the current
LP-relaxation. In order to ﬁnd the most violated in-
equality of the DCut formulation, or to show that no
such exists, we construct the ﬂow network only once and
perform at most |V | maximum-ﬂow calculations on it.
This is a main reason why the DCut formulation per-
forms better than Gsec in practice: a single separation
step for Gsec requires 2|V | −2 maximum-ﬂow calcu-
lations, as already shown by Fischetti et al. [15]. Fur-
thermore, the corresponding ﬂow network is not static
over all those calculations, but has to be adapted prior
to each call of the maximum-ﬂow algorithm.
Our test sets, as described in Section 5, also contain
grid graphs. In such graphs, it is easy to detect and
enumerate all 4-cycles by embedding the grids into the
plane and traversing all faces except for the single large
one. Note that due to our transformation, all 4-cycles
are bidirected.
Let C4 be the set of all bidirected 4-
cycles; a cycle C ∈C4 then consists of 8 arcs and V [C]
gives the vertices on C. We use a separation routine for
gsec-constraints on these cycles:
(4.16)

a∈C
xa ≤

i∈V [C]\{v}
yi
∀C ∈C4, ∀v ∈V [C].
4.3
Upper Bounds and Proving Optimality. In
the last decade, several heuristics and metaheuristics
have been developed for KCT. See, e.g., [4, 5, 8, 11]
for an extensive comparison. Traditional Branch-and-
Cut algorithms allow to use such algorithms as primal
heuristics, giving upper bounds which the Branch-and-
Cut algorithm can use for bounding purposes when
branching. The use of such heuristics is two-fold: (a)
they can be used as start-heuristics, giving a good initial
upper bound before starting the actual Branch-and-
Cut algorithm, and (b) they can be run multiple times
during the exact algorithm, using the current fractional
solutions as an additional input, or hint, in order to
generate new and tighter upper bounds on the ﬂy.
Let h be a primal bound obtained by such a
heuristic.
Mathematically, we can add this bound to
our LP as

a∈A
c(a) · xa ≤h −Δ.
Thereby, Δ := min{c(a) −c(b) | c(a) > c(b), a, b ∈A}
denotes the minimal diﬀerence between any two cost
values. If the resulting ILP is found to be infeasible,
we have a proof that h was optimal, i.e., the heuristic
solution was optimal.
As our experiments reveal, our algorithm is already
very successful without the use of any such heuristic.
Hence we compared our heuristic-less Branch-and-Cut
algorithm (DC−) with one using a perfect heuristic: a
(hypothetical) algorithm that requires no running time
and gives the optimal solution. We can simulate such a
perfect heuristic by using the optimal solution obtained
by a prior run of DC−.
We can then measure how
long the algorithm takes to discover the infeasibility of
the ILP. We call this algorithm variant DC+.
If the
runtime performance of DC−and DC+ are similar, we
can conclude that using any heuristic for bounding is
not necessary.
5
Experimental results
We implemented our algorithm in C++ using CPLEX
9.0 and LEDA 5.0.1. The experiments were performed
on 2.4 GHz AMD Opteron with 2GB RAM per pro-
cess. We tested our algorithm on all instances of the
KCTLIB [6] which consists of the following benchmark
sets:
(BX) The set by Blesa and Xhafa [2] contains 35 4-
regular graphs with 25–1000 nodes. The value of k
is ﬁxed to 20. The results of [8] have already shown
that these instances are easy, which was conﬁrmed
by our experiments:
our algorithm needed on
average 1.47 seconds per instance to solve them to
optimality, the median was 0.09 seconds.
(BB) The set by Blesa and Blum [8] is divided into four
subsets of dense, sparse, grid and 4-regular graphs,
respectively, with diﬀerent sizes of up to 2500
nodes. Each instance has to be solved for diﬀerent
values of k, speciﬁed in the benchmark set: these
are krel of n = |V |, for krel = {10%, . . . , 90%}1, and
additionally k = 2 and k = n −2. Note that the
latter two settings are rather insigniﬁcant for our
analysis, as they can be solved optimally via trivial
algorithms in quadratic time.
The most successful known metaheuristics for
(BB)
are
the
hybrid
evolutionary
algorithm
(HyEA) [4] and the ant colony optimization algo-
rithm (ACO) [11].
(UBM) The set by Uroˇsevi´c et al. [25] consists of large
20-regular graphs with 500–5000 nodes which were
originally generated randomly. The values for k are
deﬁned as for (BB) by using krel = {10%, . . . , 50%}.
In [25] a variable neighborhood decomposition
search (VNDS) was presented, which is still the
best known metaheuristic for this benchmark set.
1For the grid instances, the values krel diﬀer slightly.
31

# of nodes
500
1000
1500
2000
3000
4000
5000
avg. time in sec.
7.5
48.3
107.4
310.7
1972
5549
15372.2
avg. gap of BKS
1.5%
0.1%
0.1%
0.2%
0.2%
0.3%
0.3%
Table 1: Average running times and average gap to the BKS provided in [5, 7, 25] for (UBM).
0,01
0,1
1
10
100
2
10% 20% 30% 40% 50% 60% 70% 80% 90%
n-2
k, krel
dense
regular
sparse
speed-up
Figure 1: Speed-up factors for dense, regular and sparse
graphs with |V | < 2000 obtained when asymmetry
constraints (4.15) are included in the initial LP
Our computational experiments on (UBM) show
that all instances with up to 3000 nodes can be solved
to optimality within two hours.
We are also able to
solve the graphs with 4000 and 5000 nodes to optimal-
ity, although only about 50% of them in less than two
hours. Note that for these large instances the VNDS
metaheuristic of [25] is faster than our algorithm, how-
ever they thereby could not reach optimal solutions. Ta-
ble 1 gives the average running times and the diﬀerences
between the optimal solutions and the previously best
known solutions (BKS).
In the following we will concentrate on the more
common and diversiﬁed benchmark set (BB), and com-
pare our results to those of HyEA and ACO. Unless
speciﬁed otherwise, we always report on the DC−algo-
rithm, i.e., the Branch-and-Cut algorithm without using
any heuristic for upper bounds.
Algorithmic Behaviour. Figure 1 illustrates the
eﬀectiveness of the asymmetry constraints (4.15) de-
pending on increasing relative cardinality krel. There-
fore we measured the speed-up by the quotient
t∅
tasy ,
whereby tasy and t∅denote the running time with and
without using (4.15), resprectively. The constraints al-
low a speed-up by more than an order of magnitude for
sparse, dense and regular graphs, but only for large car-
dinality k > n
2 . Our experiments show that for smaller
k, a variable xri, for some i ∈V , is quickly set to 1
and stays at this value until the ﬁnal result. In these
cases the constraints cannot help and only slow down
0,01
0,1
1
10
100
0%
10%
20%
30%
40%
50%
60%
70%
80%
90% 100%
krel
speed-up
Figure 2:
Speed-up factors for the grid instances of
(BB) when gsec-constraints (4.16) are separated. For
each instance and krel value there is a diamond-shaped
datapoint; the short horizontal bars denote the average
speed-up per krel.
the algorithm. Interestingly, the constraints were never
proﬁtable for the grid instances. For graphs with more
than 2000 nodes using (4.15) is not possible due to mem-
ory restrictions, as the O(|V |2) many asymmetry con-
straints are too much to handle. Hence, we ommitted
these graphs in our ﬁgure.
We also report on the experiments with the special
gsec-constraints (4.16) within the separation routine
for the grid graphs.
The clear advantage of these
constraints is shown in Figure 2, which shows the
obtained speed-up factor
t∅
tgsec
by the use of these
constraints.
Based on these results we choose to include the
assymmetry constraints for all non-grid instances with
less then 2000 nodes and k > n
2 , in all the remaining
experiments. For the grid instances we always separate
the gsec-constraints (4.16).
In Table 2, we show that the computation time is
not only dependent on the graph size, but also on the
density of the graph.
Generally, we leave table cells
empty if there is no problem instance with according
properties.
As described in Section 4.3, we also investigate the
inﬂuence of primal heuristics on our Branch-and-Cut
algorithm.
For the tested instances with 1000 nodes
the comparison of the running times of DC+ and DC−
is shown in Figure 3. In general, our experiments show
32

-20%
0%
20%
40%
60%
80%
10%
20%
30%
40%
50%
60%
70%
80%
90%
k, krel
dense
regular
sparse
rel. speed-up
Figure 3: Relative speed-up
(tDC−−tDC+)
tDC−
(in percent)
of DC+ compared to DC−for the instances with 1000
nodes.
avg. deg
set
500 nodes
1000 nodes
2.5
(BB)
1
8.1
4
(BB)
0.9
15.7
10
(BB)
2.6
25
20
(UBM)
7.5
48.4
36.3
(BB)
10.7
—
Table 2: Average CPU time (in seconds) over krel values
of 10%, 20%, . . . , 50%, sorted by the average degree of
the graphs.
that DC+ is only 10–30% percent faster than DC−
on average, even for the large graphs. Hence, we can
conclude that a bounding heuristic is not crucial for the
success of our algorithm.
Runtime Comparison. Table 3 summarizes the
average and median computation times of our algo-
rithm, sorted by size and categorized according to the
special properties of the underlying graphs. We can ob-
serve that performance does not diﬀer signiﬁcantly be-
tween the sparse, regular and dense graphs, but that the
grid instances are more diﬃcult and require more com-
putational power. This was also noticed in [9, 10, 14].
The behaviour of DC−also has a clear dependency
on k, see Figures 5(a), 5(c) and 5(d): for the sparse,
dense and regular instances the running time increases
with increasing k. In contrast to this, solving the grid
instances (cf.
Figure 5(b)) is more diﬃcult for the
relatively small k-values.
The original experiments for HyEA and ACO were
performed on an Intel Pentium IV, 3.06 GHz with 1GB
RAM and a Pentium IV 2.4 GHz with 512MB RAM,
respectively. Using the well-known SPEC performance
evaluation [26], we computed scaling factors of both ma-
chines to our computer: for the running time compar-
ison we divided the times given in [4] and [11] by 1.5
0%
25%
50%
75%
100%
0%
10%
20%
30%
40%
50%
60%
70%
80%
90% 100%
krel
grid (225)
regular (all)
sparse (2500)
eq.
Figure 4:
Dependancy of the BKS quality on krel,
for selected instances.
The vertical axis gives the
percentage of the tested instances for which the BKS
provided in [6] are optimal.
and 2, respectively.
Anyhow, note that these factors
are elaborate guesses and are only meant to help the
reader to better evaluate the relative performance.
Table 3 additionally gives the average factor of
tHyEA
tDC−, i.e., the running time of our algorithm compared
to (scaled) running time of HyEA. Analogously, Figure 5
shows the CPU time in (scaled) seconds of HyEA, ACO
and our algorithm.
We observe that our DC−algorithm performs bet-
ter than the best metaheuristics in particular for the
medium values of k, i.e., 40−70% of |V |, on all instances
with up to 1089 nodes, except for the very dense graph
le450 15a.g with 450 nodes and 8168 edges, where
HyEA was slightly faster.
Interestingly, the gap be-
tween the heuristic and the optimal solution tended to
be larger especially for medium values of k (cf. next
paragraph and Figure 4 for details).
Solution Quality. For each instance of the sets
(BX) and (BB) we compared the previously best known
solutions, see [6], with the optimal solution obtained by
our algorithm, in order to assess their quality. Most of
the BKS were found by HyEA, followed by ACO. Note
that these solutions where obtained by taking the best
solutions over 20 independent runs per instance. In Ta-
ble 4 we show the number of instances for which we
proved that BKS was in fact not optimal, and give the
corresponding average gap gapbks := BKS−OPT
OPT
(in per-
cent), where OPT denotes the optimal objective value
obtained by DC−and BKS denotes the best known so-
lution obtained by either ACO or HyEA. Analogously,
we give the average gaps gapavg := AVG−OPT
OPT
(in per-
cent), AVG denotes the average solution obtained by
a metaheuristic. We observe that—concerning the so-
lution quality—metaheuristics work quite well on in-
stances with up to 1000 nodes and relatively small k.
33

# nodes
500
1000–1089
2500
group
avg/med
tHyEA
tDC−
avg/med
tHyEA
tDC−
avg/med
tHyEA
tDC−
sparse
1.7/2.0
2.2
15.2/20.2
2.6
923.2/391.5
0.1
regular
1.7/1.5
3.1
22.2/21.4
5.7
—
—
dense
7.5/7.9
2.2
25.5/27.9
2.7
—
—
grid
11.7/1.2
0.1
124.8/98.7
1.1
3704.1/2800.1
0.1
Table 3: Average/median CPU time (in seconds) and the average speed-up factor of DC−to HyEA for the
instance set (BB). Cells are left empty if there exists no instance matching the given criteria.
0,01
0,1
1
10
100
1000
2
10%
20%
30%
40%
50%
60%
70%
80%
90%
n-2
k, krel
sec.
HyEA
ACO
DC-
(a) sparse (1000 nodes, 1250 edges)
0,01
0,1
1
10
100
1000
2
9%
18%
28%
37%
46%
55%
64%
73%
83%
92%
n-2
2
9%
18%
28%
37%
46%
55%
64%
73%
83%
92%
n-2
k, krel
HyEA
ACO
DC-
sec.
(b) grid 33x33 (1089 nodes, 2112 edges)
0,01
0,1
1
10
100
1000
2
10%
20%
30%
40%
50%
60%
70%
80%
90%
n-2
k, krel
HyEA
ACO
DC-
sec.
(c) dense (1000 nodes, 1250 edges)
0,01
0,1
1
10
100
1000
2
10%
20%
30%
40%
50%
60%
70%
80%
90%
n-2
2
10%
20%
30%
40%
50%
60%
70%
80%
90%
n-2
k, krel
HyEA
ACO
DC-
sec.
(d) 4-regular (1000 nodes, 2000 edges)
Figure 5: Running times of DC−, HyEA, and ACO (in seconds) for instances of (BB) with ∼1000 nodes, depending
on k. The ﬁgures for the grid and regular instances show the times for two diﬀerent instances of the same type,
respectively.
34

instance
(|V |,|E|)
eq.
gapbks
gapavg ACO
gapavg HyEA
regular g400-4-1.g
(400,800)
10/11
0.09
0.07
0.04
regular g400-4-5.g
(400,800)
8/11
0.19
0.31
0.35
regular g1000-4-1.g
(1000,2000)
7/11
0.07
0.65
0.12
regular g1000-4-5.g
(1000,2000)
3/11
0.08
0.45
0.35
sparse steinc5.g
(500,625)
11/11
–
0.97
0.06
sparse steind5.g
(1000,1250)
11/11
–
0.48
0.11
sparse steine5.g
(2500,3125)
3/11
0.13
n/a
0.23
dense le450a.g
(450,8168)
11/11
–
n/a
0.04
dense steinc15.g
(500,2500)
11/11
–
0.36
0.02
dense steind15.g
(1000,5000)
10/11
0.22
0.38
0.04
grid 15x15-1
(225,400)
13/13
–
1.27
0.18
grid 15x15-2
(225,400)
13/13
–
2.04
0.12
grid 45x5-1
(225,400)
4/13
0.54
n/a
1.22
grid 45x5-2
(225,400)
10/13
0.08
n/a
0.13
grid 33x33-1
(1089,2112)
3/12
0.31
1.70
0.57
grid 33x33-2
(1089,2112)
3/12
0.39
2.48
0.49
grid 50x50-1
(2500,4900)
2/11
0.95
n/a
1.27
grid 50x50-2
(2500,4900)
2/11
0.55
n/a
0.82
Table 4: Quality of previously best known solutions (BKS) provided in [6] for selected instances. “eq.” denotes
the number of instances for which the BKS was optimal. For the other instances where BKS was not optimal, we
give the average relative gap (gapbks) between OPT and BKS. For all instances we also give the average relative
gap (gapavg) between the average solution of the metaheuristic and OPT. All gaps are given in percent. Cells
marked as “n/a” cannot be computed as the necessary data for ACO is not not available.
In particular, for k = 2 and k = n −2 they always
found an optimal solution.
Acknowledgements. We would like to thank Chris-
tian Blum and Dragan Uroˇsevi´c for kindly providing us
with test instances and sharing their experience on this
topic.
References
[1] S. Arora and G. Karakostas. A (2 + ϵ)-approximation
algorithm for the k-MST problem.
In Proc. of the
eleventh annual ACM-SIAM symposium on Discrete
algorithms (SODA’00), pages 754–759, 2000.
[2] M. J. Blesa and F. Xhafa.
A C++ Implementa-
tion of Tabu Search for k-Cardinality Tree Problem
based on Generic Programming and Component Reuse.
In c/o tranSIT GmbH, editor, Net.ObjectDsays 2000
Tagungsband, pages 648–652, Erfurt, Germany, 2000.
Net.ObjectDays-Forum.
[3] A. Blum, R. Ravi, and S. Vempala. A constant-factor
approximation algorithm for the k-MST problem. In
ACM Symposium on Theory of Computing, pages 442–
448, 1996.
[4] C. Blum.
A new hybrid evolutionary algorithm for
the huge k-cardinality tree problem.
In Proc. of
Genetic and Evolutionary Computation Conference
(GECCO’06), pages 515–522, New York, NY, USA,
2006. ACM Press.
[5] C. Blum. Revisiting dynamic programming for ﬁnding
optimal subtrees in trees. European Journal of Opera-
tional Research, 177(1):102–115, 2007.
[6] C. Blum and M. Blesa.
KCTLIB – a library
for
the
edge-weighted
k-cardinality
tree
problem.
http://iridia.ulb.ac.be/ cblum/kctlib/.
[7] C. Blum and M. Blesa.
Combining ant colony opti-
mization with dynamic programming for solving the
k-cardinality tree problem.
In Proc. of Computa-
tional Intelligence and Bioinspired Systems, 8th In-
ternational Work-Conference on Artiﬁcial Neural Net-
works, (IWANN’05), volume 3512 of Lecture Notes in
Computer Science, Barcelona, Spain, 2005. Springer.
[8] C. Blum and M. J. Blesa.
New metaheuristic ap-
proaches for the edge-weighted k-cardinality tree prob-
lem. Computers & OR, 32:1355–1377, 2005.
[9] C. Blum and M. Ehrgott.
Local search algorithms
for the k-cardinality tree problem.
Discrete Applied
Mathematics, 128(2–3):511–540, 2003.
[10] J. Brimberg, D. Uroˇsevi´c, and N. Mladenovi´c. Vari-
able neighborhood search for the vertex weighted k-
cardinality tree problem. European Journal of Opera-
tional Research, 171(1):74–84, 2006.
[11] T. N. Bui and G. Sundarraj.
Ant system for the
k-cardinality tree problem.
In Proc. of Genetic and
Evolutionary Computation Conference (GECCO’04),
volume 3102 of Lecture Notes in Computer Science,
pages 36–47, Seattle, WA, USA, 2004. Springer.
[12] M. Chimani, M. Kandyba, and P. Mutzel.
A new
ILP formulation for 2-root-connected prize-collecting
35

Steiner networks. In Proc. of 15th Annual European
Symposium on Algorithms (ESA’07), volume 4698 of
Lecture Notes in Computer Science, pages 681–692,
Eilat, Israel, 2007. Springer.
[13] M. Ehrgott and J. Freitag. K TREE/K SUBGRAPH:
a program package for minimal weighted k-cardinality
tree subgraph problem.
European Journal of Opera-
tional Research, 1(93):214–225, 1996.
[14] M. Ehrgott, J. Freitag, H.W. Hamacher, and F. Maﬃ-
oli. Heuristics for the k-cardinality tree and subgraph
problem.
Asia Paciﬁc J. Oper. Res., 14(1):87–114,
1997.
[15] M. Fischetti, W. Hamacher, K. Jornsten, and F. Maf-
ﬁoli.
Weighted k-cardinality trees: Complexity and
polyhedral structure. Networks, 24:11–21, 1994.
[16] D. Gale. A theorem on ﬂows in networks. Paciﬁc J.
Math, 7:1073–1082, 1957.
[17] N. Garg.
A 3-approximation for the minimum tree
spanning k vertices.
In Proc. of the 37th An-
nual Symposium on Foundations of Computer Science
(FOCS’96), pages 302–309, Washington, DC, USA,
1996. IEEE Computer Society.
[18] N. Garg.
Saving an epsilon: a 2-approximation for
the k-MST problem in graphs. In Proc. of the thirty-
seventh annual ACM symposium on Theory of comput-
ing (STOC’05), pages 396–402. ACM Press, 2005.
[19] M. X. Goemans and D. P. Williamson. A general ap-
proximation technique for constrained forest problems.
SIAM Journal on Computing, 24(2):296–317, 1995.
[20] M.X. Goemans and Y. Myung. A catalog of Steiner
tree formulations. Networks, 23:19–28, 1993.
[21] T. Koch and A. Martin. Solving Steiner tree problems
in graphs to optimality. Networks, 32:207–232, 1998.
[22] I. Ljubi´c.
Exact and Memetic Algorithms for Two
Network Design Problems.
PhD thesis, Technische
Universit¨at Wien, 2004.
[23] I. Ljubi´c,
R. Weiskircher,
U. Pferschy,
G. Klau,
P. Mutzel, and M. Fischetti. An algorithmic framework
for the exact solution of the prize-collecting Steiner tree
problem. Mathematical Programming, Series B, 105(2–
3):427–449, 2006.
[24] A. Segev.
The node-weighted Steiner tree problem.
Networks, 17(1):1–17, 1987.
[25] D. Uroˇsevi´c, J. Brimberg, and N. Mladenovi´c. Vari-
able neighborhood decomposition search for the edge
weighted k-cardinality tree problem. Computers & OR,
31(8):1205–1213, 2004.
[26] Standard
performance
evaluation
corporation.
http://www.spec.org/.
[27] L.
A.
Wolsey.
Integer
Programming.
Wiley-
Interscience, 1998.
36













Comparing Online Learning Algorithms to Stochastic Approaches for the
Multi-Period Newsvendor Problem
Shawn O’Neil
Amitabh Chaudhary
Abstract
The
multi-period
newsvendor
problem
describes
the
dilemma of a newspaper salesman—how many papers should
he purchase each day to resell, when he doesn’t know the
demand? We develop approaches for this well known prob-
lem based on two machine learning algorithms: Weighted
Majority of Warmuth and Littlestone, and Follow the Per-
turbed Leader of Kalai and Vempala. With some modiﬁed
analysis, it isn’t hard to show theoretical bounds for our
modiﬁed versions of these algorithms. More importantly, we
test the algorithms in a variety of simulated conditions, and
compare the results to those given by traditional stochastic
approaches which assume more information about the de-
mands than is typically known. Our tests indicate that such
online learning algorithms can perform well in comparison to
stochastic approaches, even when the stochastic approaches
are given perfect information.
1
Introduction
On each morning of some sequence of days, a newspaper
salesman needs to decide how many newspapers to order
at a cost of c per paper, so that he can resell them for
an income of r per paper. Unfortunately, every day it
is unknown how many papers d will be demanded. If
too many are ordered, some proﬁts are lost on unused
stock. If too few are ordered, some proﬁts are lost due
to unmet demand. The actual proﬁt seen by a vendor
who orders x items on a day with demand d is given by
r min{d, x} −xc. The papers ordered for a single day
are of course only useful for that day; leftover papers
cannot be sold in any later period.
This model describes a wide variety of products in
industry. Fashion items and the trends they rely on are
typically short lived, inducing many manufacturers to
introduce new product lines every season[16]. Consumer
electronics also have a short selling season due to their
continuously evolving nature; cellular phones can have a
lifecycle as short as six months[2]. Some vaccines such as
those for inﬂuenza are only useful for a single season[6].
For many such products, due to required minimum
manufacturing or processing times, the vendor must ﬁ-
nalize his order before any demand is seen.
Further,
because properties of the products themselves can vary
markedly between selling periods, so too can the de-
mand seen each period. This demand uncertainty is the
most challenging hallmark of the newsvendor model.
A common approach taken to resolve the demand
uncertainty issue is using a stochastic model for the
demands; assuming, for example that for each period
the demand is drawn independently from some known
distribution. In using such an approach, the goal is then
to choose an order amount which maximizes expected
proﬁt (see, e.g., [11]).
However, such approaches are
commonly inadequate, as the quality of the ﬁnal result
depends heavily on the quality of the assumptions made
about the distribution.
Given the strong uncertainty
inherent in many newsvendor items, such quality is
usually low. (See [21] for a lengthier discussion on the
shortcomings of this approach.)
Alternate approaches to the newsvendor problem
are more “adversarial” in nature. In these models, very
little is assumed about the nature of the demands, and
worst-case analysis is used.
Typically, only a lower
bound m and upper bound M on the range of possible
demand values are assumed. One solution in this area
develops a strategy to minimize the maximum regret:
max
demand values(OPT −ALG),
where OPT denotes the proﬁt of the oﬄine optimal
algorithm which knows the demand values, and ALG
is the proﬁt of the strategy used (see [17, 21, 22]).
Another method used to evaluate and design online
algorithms for such problems is competitive ratio, where
the goal is to minimize the ratio OPT/ALG in the worst
case. However, one can show, using Yao’s technique, a
lower bound of Ω(M/(mk)) for this ratio in the single
period case when r = kc.
This bound is tight, as a
simple balancing algorithm can guarantee proﬁts of this
form.
Similarly restrictive results can be found for the
worst case approach to regret with respect to OPT seen
above. The single period minimax regret solution results
in a maximum regret of c(M −m)(r −c)/r[21], which
implies that for t periods of a newsvendor game it is
possible to suﬀer a regret of tc(M −m)(r −c)/r, even
for the best possible deterministic algorithm.
49

For these reasons, we turn away from evaluating
the performance of algorithms in terms of the dynamic
oﬄine optimal, and consider a more realistic target:
the static oﬄine optimal, which we denote here by
STOPT.
STOPT is a weaker version of OPT which
makes an optimal decision based on perfect knowledge
of the demands, but is required to choose one single
order quantity to use for all periods.
Comparing the performance of algorithms with
the performance of STOPT has practical signiﬁcance,
because any bounds for an algorithm with respect
to STOPT also hold with respect to an algorithm
which makes decisions based on stationary stochastic
assumptions. Much of the inventory theory literature
deals with algorithms of this type[15, 11].
We look at adaptations of two Expert Advice al-
gorithms: Weighted Majority, developed by Littlestone
and Warmuth[14], and Follow the Perturbed Leader, de-
veloped by Kalai and Vempala[12].
In the expert advice problem, the algorithm de-
signer is given access to n experts, each of whom make
a prediction for each period, and suﬀer some cost for in-
correct predictions. The goal is to design an algorithm
that makes its own predictions based on the experts’ ad-
vice, and yet does not suﬀer much more cost than the
best performing expert in hindsight.
In our setting, we use naive experts which make
ﬁxed predictions in the range [m, M], and the cost they
suﬀer in each period is the regret (diﬀerence in proﬁt)
from the dynamic oﬄine OPT. Adapting the Weighted
Majority algorithm to the non linear proﬁt function of
the newsvendor problem requires some careful attention
if one wants to show theoretical performance bounds,
whereas Follow the Perturbed Leader is a more straight-
forward implementation. Details of the algorithms’ op-
eration and theoretical performance bounds in this set-
ting can be found in the appendices.
2
Goals of This Paper
In Section 4, we’ll give overviews of the operation
of three algorithms, two based on Weighted Majority
variants which we call WMN and WMNS, and one based
on Follow the Perturbed Leader which we call FPL.
Each of these algorithms takes parameters which are
chosen by the experimenter as input, which aﬀect their
operation and the performance bounds they achieve.
The primary interest of this paper, then, is to em-
pirically evaluate the performance of these algorithms
and compare the results to those generated by STOPT
as well as more traditional stochastic approaches. Each
of the stochastic solutions takes as input the assump-
tions made by the experimenter about the mean and
standard deviation of the input distribution.
Further, the speciﬁcs of the problem instance itself
may lead to interesting observations about all of the
solutions speciﬁed.
For instance, we know that the
relationship of r and c can make a large diﬀerence on the
performance of the minimax regret solution; does this
ratio also aﬀect the performance of other approaches we
are going to test? Do certain types of input distributions
favor one approach over the other?
Given such a large number of possible experimental
variables, we are forced to select those which we believe
will be most interesting, and design experiments using
simulated data which are most likely to highlight the
advantages and deﬁciencies of the diﬀerent approaches.
3
Related Work
The
Newsvendor
Problem The
origins
of
the
newsvendor problem can be traced as far back as Edge-
worth’s 1888 paper[10] in which the author considers
how much money a bank should keep in reserve to sat-
isfy customer withdrawal demands, with high proba-
bility.
If the demand distribution and the ﬁrst two
moments are assumed known (normal, log-normal, and
Poisson are common), then it can be shown that the ex-
pected proﬁt is maximized at x, where φ(x) = (r −c)/r
and φ(·) is the cumulative probability density func-
tion for the distribution. Gallego’s lecture notes[11] as
well as the book by Porteus[15] have useful overviews.
When only the mean and standard deviation are known,
Scarf’s results[18] give the optimal stocking quantity
which maximizes the expected proﬁt assuming the worst
case distribution with those two moments (a maxi-min
approach). In some situations this solution prescribes
ordering no items at all.
Among worst-case analyses, one of the earliest uses
of the minimax regret criterion for decision making un-
der uncertainty was introduced by Savage[17]. Apply-
ing the techniques to the newsvendor problem, Vairak-
tarakis describes adversarial solutions for several per-
formance criteria in the setting of multiple item types
per period and a budget constraint[21]. Bertsimas and
Thiele give solutions for several variants of the newsven-
dor problem which optimize the order quantity based on
historical data[3]. The solutions discussed take into ac-
count risk preferences by “trimming,” or ignoring, his-
torical data which leads to overly optimistic predictions.
Learning from Experts Weighted Majority is a very
adaptable machine learning algorithm developed by Lit-
tlestone and Warmuth[14]. There are several versions
of the weighted majority algorithm, including discrete,
continuous, and randomized. Each consults the predic-
tions of experts, and seeks to minimize the regret (in
terms of prediction mistakes) with respect to the best
50

expert in the pool.
Weighted Majority and variations thereof have been
applied to a wide variety of areas including online
portfolio selection[8, 7] and robust option pricing[9].
Other variants include the WINNOW algorithm also
developed by Littlestone[13], which has been applied to
such areas as predicting user actions on the world wide
web[1].
Follow the Perturbed leader is a general algorithm
for online decision making which is also applicable
to the learning from experts problem.
It’s creators,
Kalai and Vempala[12], apply the algorithm to such
problems as online shortest paths[20] and the tree
update problem[19].
4
Algorithms
For these experiments, we impliment the following
algorithms as described:
STOPT This approach is given perfect information
about the demand sequence, and chooses the single or-
der quantity to use for all periods which maximizes the
overall proﬁt (and thus also minimizes the total regret).
As Bertsimas and Thiele discuss[3], the static oﬄine op-
timal choice is the ⌈t −t(c/r)⌉th order statistic of the
demand sequence.
NORMAL This stochastic solution assumes the de-
mands will be drawn from a known normal distribution,
and maximizes the expected proﬁt. This approach pre-
scribes ordering the amount μ+σφ−1
(r −c)/r

, where
φ−1(·) is the inverse of the standard normal cumulative
distribution function[11].
SCARF This stochastic solution is described in Scarf’s
original paper[18] as well as in [11]. The solution max-
imizes the expected proﬁt for the worst case distribu-
tion (a maximin approach in the stochastic sense) with
ﬁrst and second moments μ and σ. The order quantity
is prescribed to be μ + σ
2

(r −c)/c −

c/(r −c)

if
c(1 + σ2/μ2) < r, and 0 otherwise.
MINIMAX This is the minimax regret approach men-
tioned in Section 1. Described by [21], the algorithm
orders the quantity

M(r −c)+mc

/r for every period,
which minimizes the maximum possible regret from the
optimal for each period. As such, it also minimizes the
maximum possible regret for the whole sequence.
The solution works by balancing the regret suﬀered
by the two worst case possibilities: the demand being
m or M. Because of this, its order never changes (as
long as the range [m, M] doesn’t change), and is very
pessimistic in nature.
WMN We develop this algorithm (Weighted Majority
Newsvendor) as an adaptation of the Weighted Major-
ity algorithm of Littlestone and Warmuth[14]. The al-
gorithm takes two parameters: n, the number of “ex-
perts” to consult, and β ∈(0, 1], the weight adjustment
parameter. Essentially, we divide up the range [m, M]
into n buckets, and have expert i predict the minimax
regret order quantity for the ith bucket. Buckets and
experts are set up so that each bucket/expert pair has
the same minimax regret.
As per the standard operation of Weighted Major-
ity, each expert is given an initial weight of 1. After
each round, we decrease each expert i’s weight by some
factor F, where F depends on β and the regret that
expert would have suﬀered on the demand seen using
its prediction. If an expert is often wrong, its weight
will be decreased faster than others. This punishment
happens faster overall with smaller β’s.
The amount ordered by WMN in a given period is
the weighted average of all experts.
The intuition is
that wherever the static optimal choice is, it must fall
in one of the n buckets, and thus one of our experts will
be close to this static optimal choice. Further, because
experts’ weights are decreased according to how poorly
they do, the algorithm is able to learn where the static
optimal choice is after a few periods, and even adapt to
changing inputs over time.
Adapting the analysis of Weighted Majority to the
non linear newsvendor proﬁt function requires special
care to ensure bounds similar to that of Weighted
Majority can still be given. In Appendix A, we give a
detailed description of WMN and a proof of the following
theorem:
Theorem 4.1. The total regret experienced by WMN
for a t period newsvendor game with per item cost c, per
item revenue r, and all demands within [m, M] satisﬁes
WMNT otalRegret
≤C ln(n)
1 −β +
ln

1
β
	
c(M −m)(r −c)t
nr(1 −β)
+
ln

1
β
	
STOPTT otalRegret
1 −β
where C = max{(M −m)(r −c), (M −m)c} is the
maximum possible single period regret, n is the number
of buckets used by WMN, and β is the update parameter
used.
WMNS WMNS, for Weighted Majority Newsvendor
Shifting, is based on the “shifting target” version of the
standard Weighted Majority algorithm. Here, if the in-
put sequence can be decomposed into subsequences such
51

that for each subsequence a particular expert does very
well, then WMNS will do nearly as well for that subse-
quence. WMNS needs no information about how many
shifts there will be, or when they will be. For exam-
ple, if for the ﬁrst third of the sequence all demands
are near m, WMNS will initially adjust the weights of
the experts so that it is ordering near m as well. If the
sequence shifts so that demands are then drawn from
near M, WMNS will adjust the weights quickly (quicker
than WMN) so that the order quantities will match.
This ability comes from WMNS’s use of a weight
limiting factor δ ∈(0, 1], so that no expert’s weight
will be less than δ times the average weight.
When
a new expert starts doing signiﬁcantly better, the old
best expert’s weight is decreased to below the new
expert’s weight more rapidly, as the new expert’s weight
is guaranteed not to be too low in relation.
Theorem 4.2. The total regret experienced by WMNS
for a t period newsvendor game with per item cost c, per
item revenue r, and all demands within [m, M] satisﬁes
WMNST otalRegret
≤
kC ln

n
βδ
	
(1 −β)(1 −δ) +
ln

1
β
	
c(M −m)(r −c)t
nr(1 −β)(1 −δ)
+
ln

1
β
	
SSTOPTT otalRegret
(1 −β)(1 −δ)
where C = max{(M −m)(r −c), (M −m)c} is the
maximum possible single period regret, n is the number
of buckets used by WMNS, β is the update parameter
used, and δ is the weight limiting parameter used.
SSTOPT is allowed to use a static optimal choice for
k subsequences (i.e., is allowed to change order values
k −1 times, see below).
Details of WMNS’s operation and proof of the above
bounds are given in Appendix B.
SSTOPT This “optimal,” which makes its decisions
based on the entire sequence, is a slightly stronger
version of STOPT, which is allowed to change its order
quantity exactly k −1 times during the sequence.
FPL Similar to WMN, FPL is a randomized algorithm
based upon the Follow the Perturbed Leader approach
developed by Kalai and Vempala[12].
As a general
algorithm it is well suited to making decisions a number
of times, when one wants to minimize the total cost
in relation to the best single decision for all periods.
Here, decisions will be of the form “use expert i’s
prediction,” where the experts again predict minimax
values in buckets which divide the [m, M] range. FPL
as we use it takes two parameters, n, for the number
of experts/buckets, and ϵ, which aﬀects the ﬁnal cost
bound in relation to the best static decision.
Theorem 4.3. The total regret experienced by FPL for
a t period newsvendor game with per item cost c, per
item revenue r, and all demands within [m, M] satisﬁes
E[FPLT otalRegret]
≤4C(1 + ln(n))
ϵ
+ (1 + ϵ)c(M −m)(r −c)t
nr
+ (1 + ϵ)STOPTT otalRegret
where C = max{(M −m)(r −c), (M −m)c} is the max-
imum possible single period regret, n is the number of
buckets used by FPL, and ϵ is the randomness parameter
used.
Details of the algorithm and proof of the bounds it gives
in our application appear in Appendix C.
5
Experiments
In order to evaluate the online learning algorithms for
the newsvendor problem, we run them on simulated
demand sequences comparing the total regret suﬀered
by each approach to the regret suﬀered by the stochastic
algorithms SCARF and NORMAL, as well as MINIMAX
and STOPT.
Unless otherwise noted, all experiments consist of
100 demand newsvendor sequences, and each data point
represents the average of 100 such trials. Thus, data
points in the following ﬁgures typically represent the
average total regret of various approaches on newsven-
dor sequences of length 100. Also, due to space limi-
tations, we won’t experiment with the aﬀect of the up-
per and lower demand bounds [m, M]; we’ll instead ﬁx
these bounds to [10, 100] for all tests. Whenever a nor-
mal distribution is used, we restrict it to this range by
resampling if a demand falls outside the range, and we
further restrict all demands to be integers.
5.1
Algorithm Parameters
5.1.1
β, ϵ, and μ For this ﬁrst batch of tests, we in-
vestigate the performance of our three machine learning
approaches while varying some of the parameters they
accept as input. WMN and WMNS use β as a weight
adjustment parameter: the smaller β is, the quicker ex-
pert weights are adjusted downward. WMNS also uses a
“weight limiting” parameter δ, which we hold constant
at 0.3 for these tests.
FPL uses the parameter ϵ, which aﬀects the amount
of “randomness” used in deciding which expert to
52

follow. Smaller ϵ values lead to more randomness being
used. Even though the bounds discussed for FPL are
only valid for ϵ ∈(0, 1], the algorithm is still operable
for larger values, so we test ϵ ∈(0, 5]. While we test
the eﬀects of varying β and ϵ, we hold the number of
experts, n, at 32.
For Figures 1, 2, and 3, the distribution is nor-
mal with mean demand of 25 and standard deviation
15. Note that because the distribution is bounded to
[10, 100] via resampling, the actual mean of the distri-
bution used is about 29.3. The per item cost c is held
at 1, and the per item proﬁt r is 4.
Figure 1 plots the average total regret of WMN,
WMNS, and FPL as we vary β and ϵ. We also show
the average total regret of STOPT as a baseline for
comparison. One thing to notice in this ﬁgure is that
while WMNS is adapted to be useful in situations where
the distribution makes drastic changes over time, it does
very nearly as well as WMN in this case.
On the other hand, even though FPL suﬀers a re-
spectably low amount of regret, it’s performance is only
comparable to the other two approaches when rather
large ϵ’s are used which aren’t valid for theoretical anal-
ysis.
 0
 1000
 2000
 3000
 4000
 5000
Average Regret (Demands from N(25,15))
Regret - Varying Algorithm Parameters - β,ε
WMN, WMNS β
FPL ε
0
0
.25
1.25
.75
3.75
1
5
STOPT
WMN
WMNS
FPL
Figure 1:
Average regret suﬀered on a 100 period
newsvendor sequence, varying algorithm parameters.
Even though ϵ must be less than or equal to 1 for FPL’s
theoretical bounds to hold, we see that in this situation
it performs well with larger values also.
Figure 2 shows the regret of NORMAL and SCARF
on the same test, varying the mean assumed about
the demand distribution. Both approaches assume the
correct standard deviation of 15. As this ﬁgure shows,
the consequences of assuming incorrect information can
be quite drastic for such stochastic algorithms.
In fact, it is interesting to look at the range of
μ values used by NORMAL for which it suﬀers less
regret than WMN.
When WMN uses a β of 0.5, a
rather naive choice, the average regret suﬀered is about
1856. NORMAL suﬀers less regret than this only when
it assumes μ ∈[21.7, 37], or within about 7.6 units on
either side of the actual mean.
In Figure 2 we also plot the rather large regret suf-
fered by the pessimistic worst case algorithm MINIMAX.
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
100
77.5
55
32.5
10
Average Regret (Demands from N(25,15))
μ Assumed by SCARF/NORMAL
Regret - Varying Algorithm Parameters - μ
Actual μ = 29.3
STOPT
SCARF
NORMAL
MINIMAX
WMN β=.5
Figure 2:
Average regret suﬀered on a 100 period
newsvendor sequence, varying the mean assumed by
NORMAL and SCARF.
This plot shows the conse-
quences to the stochastic approaches of assuming incor-
rect information. For comparison, we also plot WMN’s
regret of 1856 when WMN uses a β = 0.5.
In Figure 3 we indicate what the theoretical bounds
for WMN, WMNS, and FPL would be in the previous ex-
periment. That is, given the values used for r,c,m,M,t,
as well as the parameters used by the algorithms, we
use the actual regret suﬀered by STOPT to compute
the worst case regret for the algorithms no matter the
input sequence. We see that the theoretical bounds are
much higher than the actual empirical performance seen
in ﬁgure 1, by as much as an order of magnitude.
As is reﬂected in Figure 3, the theoretical bounds
given in Theorems 4.1 and 4.2 increase without bound
as β is reduced to 0, because of the ln(1/β) term.
Because of this, we begin to notice the trade oﬀ
between minimizing the theoretical bounds and getting
good performance in actual simulation.
(Later, in
Figures 9 and 10, we’ll see the same phenomenon.)
Similar to MINIMAX, the three experts algorithms
give theoretical worst case bounds (though in relation
to STOPT, rather than OPT), and as such have a
pessimistic nature to them as well.
Using a β which
decreases weights rapidly will quickly ﬁnd the correct
amount to order, however may be more susceptible to
53

poor performance with very adversarial sequences.
 0
 10000
 20000
 30000
 40000
 50000
Average Posterior Regret Bound
Bounds - Varying Algorithm Parameters - β,ε
WMN, WMNS β
FPL ε
0
0
.25
1.25
.75
3.75
1
5
ε > 1 Not Valid for FPL Bound
WMN Bound
WMNSBound
FPL Bound
Figure 3: Average worst case theoretical bounds com-
puted using algorithm parameters, problem parameters,
and the actual regret suﬀered by STOPT. Comparing
to Figure 1, we see a trade oﬀbetween low worst case
bounds and better empirical performance.
5.1.2
Number of Experts: n Having looked at the
eﬀects of varying β and ϵ, we now turn our attention to
the other main parameter of WMN, WMNS, and FPL:
the number of experts/buckets used. Intuitively, using a
larger value for n means that we are more likely to have
an expert close to STOPT’s order value. On the other
hand, all of the theoretical bounds grow as n becomes
very large.
For Figures 4 and 5 we run the same test as section
5.1.1, with all demands drawn from N(25,15) bounded
to [10, 100].
Here, WMN uses β = 0.5, WMNS uses
β = 0.5, δ = 0.3, and FPL uses ϵ = 0.75.
Figure 4 plots the average total regret of the three
algorithms varying the number of experts used from 1
to 100. Like the last test, WMN and WMNS perform
remarkably similar, and FPL performs somewhat worse.
(Had we used a larger ϵ, this diﬀerence would probably
not be as striking.) In this plot, it appears that above a
certain point, around 5 or 10, increasing the number of
experts is ineﬀective. One possible reason for this is that
because WMN and WMNS use the weighted average of
experts, it is possible for them to settle upon an order
quantity between two experts, making the number of
experts somewhat less important. This cannot be the
case for FPL, however, as FPL always goes with a single
expert’s choice.
Figure 5 shows the computed theoretical bounds
given by varying the number of experts for this test.
Again, we see that a modest number of experts appears
 0
 1000
 2000
 3000
 4000
 5000
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
Average Regret (Demands from N(25,15))
Number of Experts (n) Used by WMN,WMNS,FPL
Regret - Varying Algorithm Parameters - n
1
STOPT
WMN
WMNS
FPL
Figure 4: Average regret suﬀered while holding β, δ, and
ϵ constant and varying the number of experts/buckets
used n.
While small values of n result in poor per-
formance, past a certain point increasing the number
doesn’t help.
to be best, and increasing beyond this point has no
beneﬁt. Though it is diﬃcult to see, there is a slight
upcurve for WMN and WMNS toward the right side
of the graph; theoretically, there will always be a
minimizing value of n given a value for STOPT’s regret.
 0
 5000
 10000
 15000
 20000
 25000
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
Average Posterior Regret Bound
Number of Experts (n) Used by WMN/WMNS,FPL
Bounds - Varying Algorithm Parameters - n
1
WMN Bound
WMNSBound
FPL Bound
Figure 5: Theoretical bounds while varying the number
of experts used, computed using the actual average reget
suﬀered by STOPT.
5.2
Problem Parameters Now we turn our atten-
tion to how the various approaches perform under dif-
ferent problem conditions. For all of the tests in this
section, WMN uses a β = 0.5, WMNS uses β = 0.5,δ =
0.3, and FPL uses ϵ = 0.75.
Then number of ex-
perts/buckets, n, used by all three is 32.
Given the
54

results so far, these seem to be typical naive choices
which one might use in practice if no information about
the problem is given.
In contrast, for all the tests in this section, we give
SCARF and NORMAL the actual mean and standard de-
viation of the sequence to be used. Giving such perfect
information about the input distribution represents a
best case for these; comparing with typical naive imple-
mentations of the experts algorithms should give some
insight as to their real-world applicability.
5.2.1
Per Item Proﬁt: r Since we know that the
worst case regret that can be suﬀered by MINIMAX
depends on r and c, it will be interesting to look at
a situation where we hold c constant to 1, and vary the
per item proﬁt r.
As in Section 5.1.1, all demands are drawn from the
bounded normal N(25,15). Figure 6 shows the average
regret for the various algorithms as we increase the
value of r from 1 to 10. Notice that when r = 1, the
correct order quantity is 0, as no net proﬁt is possible
in this situation. STOPT, MINIMAX, and the stochastic
approaches all take this into account, and as such suﬀer
no regret. The experts algorithms on the other hand
aren’t given information about r and c, and must adjust
their operation over time as they normally do.
Overall, as r increases with respect to c, the cost
of poor decisions is ampliﬁed in comparison with OPT
(which is how regret is measured). Thus, we see that
all regret curves increase as r increases, with NORMAL
and SCARF tracking STOPT most closely, followed by
WMN and WMNS, whose plots nearly overlap.
 0
 1000
 2000
 3000
 4000
 5000
 6000
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
Average Regret (Demands from N(25,15))
Return r
Regret - Varying Problem Parameters - r
STOPT
WMN
WMNS
FPL
SCARF
NORMAL
MINIMAX
Figure 6: Average regret on sequences with demands
drawn from N(25,15) bounded to [10, 100], varying r
and holding c at 1.
SCARF and NORMAL are given
perfect information, while WMN, WMNS, and FPL use
relatively naive operating parameters.
5.2.2
Distribution:
m,M Mix Perhaps the most
important consideration for a multi-period newsvendor
algorithm is how well it deals with the inherent demand
uncertainty.
We’ve already looked at the eﬀects of
various algorithm parameters, but there we used a fairly
“tame” distribution for demand values based on the
normal. Here, we’ll look at a somewhat more diﬃcult
distribution: all 100 demand values will either be the
minimum value m or the maximum value M. (Recall
that these are 10 and 100, respectively.) Further, the
sequences will be randomized so that where each type
occurs is unknown. We still ﬁx r to be 4 and c to be 1.
Figure 7 plots the regret of the WMN,WMNS, and
FPL as we vary the number of minimum value m’s which
appear in the sequence. Thus, at 0 on the left half of
the graph, all demands in the sequence are M’s. In the
middle at 50, each sequence is a random mixture of 50
m’s and 50 M’s.
In this ﬁgure we see a performance diﬀerence be-
tween WMN and WMNS, though this diﬀerence is still
fairly small. All three algorithms do fairly well despite
the large variance in demand values, tracking STOPT’s
regret in a somewhat linear fashion throughout the mix
range.
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
 0
 20
 40
 60
 80
 100
Average Regret (All Demands either m or M)
Number of m’s in Mix
Regret - Varying Problem Parameters - m,M Mix
STOPT
WMN
WMNS
FPL
Figure 7:
Average regret for m,M mix.
Here, all
demands in the 100 period sequence are either m = 10
or M = 100, and we vary how many of the demands
are m’s.
The order the demands are presented in is
randomized.
Figure 8, which plots the regret of the stochastic
approaches and MINIMAX, shows several interesting
characteristics.
There are a few points in the curve
where SCARF and NORMAL perform as well as STOPT,
but for much of the range they suﬀer a signiﬁcant
amount of regret in spite of the fact that they are
working with perfect information about the actual mean
55

and standard deviation.
In comparison, the other
algorithms perform similarly, if not better in some
areas, given no a-priori information about the demand
sequence.
MINIMAX manages the same regret for the entire
range because whether a period demand is m or M,
MINIMAX is designed to suﬀer the same regret. STOPT
experiences the most regret when there are ⌈t−t(c/r)⌉=
75 minimum demands and 25 maximum demands. Note
than in this case, STOPT’s perfect knowledge of the
demand sequence doesn’t help it fare any better than
MINIMAX, which operates completely blind. (Both of
these features can also be shown algebraically, for any
values of r and c.)
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
 0
 20
 40
 60
 80
 100
Average Regret (All Demands either m or M)
Number of m’s in Mix
Regret - Varying Problem Parameters - m,M Mix
STOPT
SCARF
NORMAL
WMN
MINIMAX
Figure 8: Average regret for m,M mix for the stochastic
approaches as well as MINIMAX. Despite SCARF and
NORMAL being given perfect information about the
mean and standard deviation, they still suﬀer regret
comparable to the other algorithms.
(WMN’s regret
using β = 0.5 is also shown for comparison.)
5.2.3
Distribution:
Shifting Normals,
Algo-
rithm Parameters Revisited Finally, for this last
set of tests, we explore if WMNS can perform better
than WMN when the input is characterized by dramatic
“shifts” in the demand sequence. Because WMNS em-
ploys a weight limiting factor δ, it is theoretically able to
adjust the relative weights of the experts more quickly,
and thus change decisions more rapidly.
Here, our sequence length is now 400 periods. The
ﬁrst 100 demands are drawn from N(25,15), the second
100 are drawn from N(75,15), the third 100 are again
from N(25,15), and the last 100 demands are drawn
from N(75,15).
As usual, all demands are bounded
to [10, 100], so the true means of each subsequence are
about 29.3 and 73.4, respectively.
Figure 9 plots WMN’s regret when WMN uses β =
0.5, FPL’s regret when ϵ = 0.75, and WMNS’s regret
varying the δ used from 0 to .99. WMNS also used a
constant β of 0.5. As we can see, up to a point increasing
δ leads to less regret, such that WMNS can outperform
STOPT and achieve regret closer to that of SSTOPT.
If δ is too high, however, we see an increase in regret,
as fairly little weight adjustment is happening at all,
limiting WMNS’s learning ability.
 0
 5000
 10000
 15000
 20000
 0
 0.2
 0.4
 0.6
 0.8
 1
Average Regret (Demands from N(25,15) and N(75,15))
δ Used by WMNS
Regret - Varying Algorithm Parameters - δ
STOPT
SSTOPT
WMN
WMNS
FPL
Figure 9:
Regret of WMNS varying δ, the weight
limiting parameter.
For this plot, demand sequences
were 400 periods long, with the ﬁrst and third sets
of 100 demands being drawn from N(25,15), and the
second and fourth being drawn from N(75,15). SCARF
and NORMAL, not shown in this plot, do approximately
as well as STOPT given perfect information.
The increase in performance, however, comes at a
steep price in terms of the theoretical regret bound,
shown in Figure 10. bound for WMNS is computed from
the actual average regret of SSTOPT which used the
single best static order value on each of the four subse-
quences. This increase in the computed bound happens
primarily because of the (1−δ) term in the denominator
of the bound (in Theorem 4.2), as SSTOPTT otalRegret
will generally be a fairly large number.
6
Conclusion
Looking at all of the ﬁgures and discussion in aggregate,
we see that overall WMN and WMNS perform compara-
bly to the traditional stochastic approaches SCARF and
NORMAL, even when those approaches are given perfect
information about the demand distribution. When the
stochastic methods assume incorrect information, they
suﬀer as expected.
Though the bounds given for FPL are comparable
to the bounds for WMN, the actual performance for
56

 0
 20000
 40000
 60000
 80000
 100000
 0
 0.2
 0.4
 0.6
 0.8
 1
Average Regret (Demands from N(25,15) and N(75,15))
δ Used by WMNS
Bounds - Varying Algorithm Parameters - δ
WMN Bound
FPL Bound
WMNS Bound
Figure 10: Regret bound of WMNS varying δ.
The
bound was computed in terms of SSTOPT’s regret,
where SSTOPT was allowed to use the static optimal
decision for each 100 period subsequence.
this problem wasn’t as good, except for in the more
diﬃcult [m, M] distribution mix scenario. Nevertheless,
all algorithms signiﬁcantly outperformed the bounds
given for all tests that we ran.
We believe part of the reason for this is that in
the general experts problem, it is possible to have a
single expert perform well in a period while all other
experts simultaneously suﬀer maximum regret. In the
newsvendor setting, this is not possible, as the optimal
order for a single period suﬀers no regret, and the
regret suﬀered increases linearly as one looks at order
quantities on either side. Designing an algorithm which
exploits this fact will be the focus of future research.
Philosophically
speaking,
designing
approaches
which successfully balance the competing goals of good
worst case performance and acceptable average case per-
formance is one of the most interesting and challenging
areas of online algorithms research. Sometimes, it seems
to be necessary to further restrict the input criteria to
achieve good average case results. Other times, simple
extensions to an algorithm can improve average case re-
sults without sacriﬁcing worst case performance, such
as the THREAT algorithm discussed in [4].
Of course, a solution isn’t worth much if no one
uses it. Brown and Tang surveyed 250 MBA students
and 6 professional buyers, supplying them with sim-
ple newsvendor problems[5]. Very few of the subjects
used the classical newsvendor solution as prescribed by
NORMAL, though the approach was known to almost
all. One possible explanation given is that the classi-
cal solution doesn’t take into account risk preferences—
buyers may be more comfortable underestimating de-
mand to have a stronger guarantee on a particular proﬁt
rather than shoot for a higher proﬁt with less certainty.
References
[1] R.
Armstrong,
D.
Freitag,
T.
Joachims,
and
T. Mitchell. Webwatcher: A learning apprentice for the
world wide web. In 1995 AAAI Spring Symposium on
Information Gathering from Heterogeneous Distributed
Environments, 1995.
[2] E. Barnes, J. Dai, S. Deng, D. Down, M. Goh, H. C.
Lau, and M. Sharafali. Electronics manufacturing ser-
vice industry.
The Logistics Institute–Asia Paciﬁc,
Georgia Tech and The National University of Singa-
pore, Singapore, 2000.
[3] D. Bertsimas and A. Thiele. A data driven approach to
newsvendor problems. Technical report, Massechusetts
Institute of Technology, Cambridge, MA, 2005.
[4] A. Borodin and R. El-Yaniv. Online Computation and
Competitive Analysis.
Cambridge University Press,
1998.
[5] A. O. Brown and C. S. Tang.
The single-period
inventory problem:
A new perspective.
Decisions,
Operations, and Technology Management, 2000.
[6] S.E. Chick, H. Mamani, and D. Simchi-Levi. Supply
chain coordination and the inﬂuenza vaccination. In
Manufacturing and Service Operations Management.
Institute for Operations Research and the Management
Sciences, 2006.
[7] T. M. Cover and E. Ordentlich.
On-line portfolio
selection. In Proceedings of the ninth annual conference
on Computational Learning Theory, pages 310–313,
1996.
[8] T. M. Cover and E. Ordentlich. Universal portfolios
with side information.
IEEE Transactions on Infor-
mation Theory, 42(2), 1996.
[9] P. Demarzo, I. Kremer, and Y. Mansour. Online trad-
ing algorithms and robust option pricing. In Proceed-
ings of the thirty-eighth annual AMC Symposium on
Theory of Computing, pages 477–486, 2006.
[10] F. Y. Edgeworth. The mathematical theory of bank-
ing. Journal of the Royal Statistical Society, 1888.
[11] G. Gallego. Ieor 4000: Production management lecture
notes. Available as http://www.columbia.edu/~gmg2/
4000/pdf/lect_07.pdf.
[12] A. Kalai and S. Vempala.
Eﬃcient algorithms for
online decision problems.
J. Comput. Syst. Sci.,
71(3):291–307, 2005.
[13] N. Littlestone.
Learning quickly when irrelevant at-
tributes abound:
A new linear-threshold algorithm.
Machine Learning, pages 285–318, 1988.
[14] N. Littlestone and M. K. Warmuth.
The weighted
majority algorithm.
Information and Computation,
pages 212–261, 1994.
[15] E. L. Porteus.
Foundations of Stochastic Inventory
Theory.
Stanford University Press, Stanford, CA,
2002.
57

[16] A. Raman and M. Fisher. Reducing the cost of demand
uncertainty through accurate response to early sales.
Operations Research, 44(4):87–99, January 1996.
[17] L. J. Savage.
The theory of statistical decisions.
Journal of the American Statistical Association, 46:55–
67, 1951.
[18] H. E. Scarf.
A min-max solution of an inventory
problem. In Stanford University Press, 1958.
[19] Daniel Dominic Sleator and Robert Endre Tarjan. Self-
adjusting binary search trees. J. ACM, 32(3):652–686,
1985.
[20] Eiji Takimoto and Manfred K. Warmuth. Path kernels
and multiplicative updates.
J. Mach. Learn. Res.,
4:773–818, 2003.
[21] C. L. Vairaktarakis. Robust multi-item newsboy mod-
els with a budget constraint. International Journal of
Production Economics, pages 213–226, 2000.
[22] G. Yu.
Robust economic order quantity models.
European Journal of Operations Research, 100(3):482–
493, 1997.
A
WMN Operation and Bounds
Deﬁnitions In this section, we suppose we are going
to play t periods of the newsvendor problem, with item
cost c and item revenue r. We have access to n experts,
each of which makes a prediction of the demand each
period. (In Section A.1, we’ll discuss how WMN actually
chooses experts’ predictions and give the ﬁnal bounds.)
In period j, each expert i predicts a demand of x(j)
i
and the true demand is revealed to be d(j) at the
end of the period. The experts are allowed to change
their predictions any way they wish between periods;
the only restriction is that x(j)
i
and d(j) are within
the interval [m, M] for all i and j.
WMN (Weighted
Majority Newsvendor) will aggregate the predictions of
the experts, and order an amount γ(j) for the jth period.
Clearly, the optimal choice for period j would be
d(j), so the true dynamic oﬄine optimal’s proﬁt for
period j, which we’ll denote as OPT(j), is d(j)(r −c).
WMN’s proﬁt is WMN(j) = min{d(j), γ(j)}r −γ(j)c, and
the proﬁt each expert i would have made is EX(j)
i
=
min{d(j), x(j)
i }r −x(j)
i c.
Algorithm WMN Algorithm WMN operates as fol-
lows: each expert i is assigned an initial weight w(1)
i
= 1.
Also, a weight adjustment parameter β ∈(0, 1] is cho-
sen.
In each period j, WMN orders an amount γ(j)
which is the weighted average of the predictions of the
experts: γ(j) = n
i=1 w(j)
i x(j)
i / n
i=1 w(j)
i .
In every period, after d(j) is revealed, we update
each expert i’s weight by some factor F:
w(j+1)
i
=
w(j)
i F, where F satisﬁes
βf(d(j),x(j)
i
) ≤F ≤1 −(1 −β)f(d(j), x(j)
i ) .
We use f(d(j), x(j)
i ) =

d(j)(r −c) −min{d(j), x(j)
i }r +
x(j)
i c

/C, where C = max{(M −m)(r −c), (M −m)c}
is the maximum possible regret any prediction can
suﬀer. Choosing C in this fashion guarantees that 0 ≤
f(d(j), x(j)
i ) ≤1 for any valid d(j) and x(j)
i , which allows
us to ensure that such an update factor F exists[14].
Intuitively, f(d(j), x(j)
i ) gives a sense of the regret expert
i would have suﬀered.
In practice, we use the upper
bound on F as the update factor.
Analysis For clarity, we deﬁne s(j) = n
i=1 w(j)
i
to be
the total sum of weights over the experts in period j.
To prove bounds on the total regret of WMN, we begin
as in [14] by showing a bound on ln(s(t+1)/s(1)). (s(t+1)
is the sum of weights at the end of the game, s(1) is
the sum of weights at the start.) Because of the upper
bound on each update factor F, we have that s(j+1) is
less than or equal to:
n

i=1
w(j)
i

1 −(1 −β)f(d(j), x(j)
i )

= s(j) −(1 −β)
n

i=1
w(j)
i f(d(j), x(j)
i )
= s(j) −(1 −β)
 n

i=1
w(j)
i d(j)(r −c)
C
−
n

i=1
w(j)
i
min{d(j), x(j)
i }r
C
+
n

i=1
w(j)
i x(j)
i c
C

= s(j) −(1 −β)

s(j)d(j)(r −c)
C
−r
C
n

i=1
min{w(j)
i d(j), w(j)
i x(j)
i } + γ(j)s(j)c
C

.
We arrive at the last line by the deﬁnition of s(j) and
γ(j). (Also, it must be noted that d(j), x(j)
i , w(j)
i
≥0.)
Now, by virtue of the fact that the summation over a
minimum is less than or equal to the minimum of two
58

summations, the above is less then or equal to:
s(j) −(1 −β)

s(j)d(j)(r −c)
C
−r
C min
 n

i=1
w(j)
i d(j),
n

i=1
w(j)
i x(j)
i

+ γ(j)s(j)c
C

= s(j) −s(j)(1 −β) 1
C

d(j)(r −c)
−min{d(j), γ(j)}r + γ(j)c

= s(j) 
1 −(1 −β)f(d(j), γ(j))

.
So, over the entire sequence,
s(t+1) ≤s(1)
t
j=1

1 −(1 −β)f(γ(j), d(j))

,
ln(s(t+1)/s(1)) ≤
t

j=1
ln

1 −(1 −β)f(d(j), γ(j))

≤
t

j=1
−(1 −β)f(d(j), γ(j)) .
Going a step further, we have the beginnings of a bound
on the total regret of WMN:
t

j=1
f(d(j), γ(j)) ≤ln(s(1)/s(t+1))
1 −β
.
Now we’ll bound s(t+1). We let mi = t
j=1 f(d(j), x(j)
i )
be the total “adjusted regret” for expert i.
By the
lower bound on our update factor F, and the fact that
w(1)
i
= 1 for all i:
s(t+1) ≥
n

i=1
w(1)
i
βmi
≥βmi, ∀i ,
ln(s(1)/s(t+1)) ≥ln(n) −mi ln(β), ∀i .
Combining this with the above, we ﬁnally get, for any
expert i,
t

j=1
f(d(j), γ(j))
≤
ln(n) + ln

1
β
	 t
j=1 f(d(j), x(j)
i )
1 −β
,
t

j=1

OPT(j) −WMN(j)	
≤C ln(n)
1 −β +
ln

1
β
	 t
j=1

OPT(j) −EX(j)
i
	
1 −β
.
Thus, we have our total regret for WMN bounded in
terms of the total regret for our best performing expert
(since the bound holds for all experts).
A.1
Placing Experts in Buckets So far, we have
a bound on the regret of WMN in terms of the regret
of the best expert. Now, we’re interested in deriving a
similar bound in terms of the regret of the oﬄine static
optimal, STOPT.
The approach we take is to let each of our n
experts consistently predict a unique demand for all
t newsvendor periods.
We divide the overall range
[m, M] into n “buckets,” such that each bucket has the
same minimax regret should the demand fall in that
bucket.
There are n + 1 bucket endpoints, {q0, q1,
. . . , qn}. As Vairaktarakis shows[21], for a given bucket
i (with endpoints qi−1 and qi) the minimax regret
order quantity is

qi(r −c) + cqi−1

/r, which results in
a maximum regret of

c(qi −qi−1)(r −c)

/r when the
demand is at either endpoint.
To achieve our “many buckets, same regret” goal,
we simply need to choose the endpoints according to:
qi = i(M −m)
n
+ m .
We then let expert i consistently predict the optimal
order quantity for the ith bucket:
x(j)
i
= qi(r −c) + cqi−1
r
= (ir −c)(M −m)
rn
+ m, ∀j .
Claim A.1. For a t-period newsvendor game, there
exists an expert i such that the diﬀerence in i’s proﬁt
and any given static oﬄine algorithm is at most
c(M −m)(r −c)t
rn
.
Proof. Suppose the static oﬄine algorithm chooses a
value which lies in the ith bucket.
The expert who
59

minimizes his diﬀerence in proﬁt is the ith expert,
since regret increases as demand moves further from
the expert’s prediction, and each expert has the same
regret at his bucket boundaries. For a single period, the
true demand could fall in one of three places: below the
bucket, in the bucket, or above the bucket.
If the demand d falls below the bucket (d < qi−1),
the maximum diﬀerence in proﬁt occurs if the static
algorithm has chosen the lowest point in the bucket at
qi−1. The diﬀerence in proﬁt is then
dr −qi−1c −(dr −x(j)
i c) = c(M −m)(r −c)
nr
.
If the demand falls in the ith bucket, we know from
above that the maximal diﬀerence in proﬁt (which is
now equivalent to regret within this bucket, since the
static algorithm can now predict the demand exactly) is
the same thing. Similarly, if the demand falls above the
ith bucket, the worst case is when the static algorithm
is at the top of the bucket at qi, and the diﬀerence in
proﬁt can again be shown to be the same.
All three cases give identical worst case proﬁt
diﬀerence.
Summing over all t periods, we have the
claim.
Since the claim holds for any static oﬄine algorithm,
it also holds for the static oﬄine optimal algorithm,
STOPT. Using the notation from Section A, the claim
implies that there exists an expert i such that
(1.1)
t

j=1

STOPT(j) −EX(j)
i
	
≤c(M −m)(r −c)t
nr
.
Using the substitution
t

j=1
(OPT(j) −EX(j)
i ) =
t

j=1
(OPT(j) −STOPT(j))
+
t

j=1
(STOPT(j) −EX(j)
i ) ,
in the bound shown at the end of Section A, and the
bound implied by Equation 1.1, we have the following
theorem:
Theorem A.1. The total regret experienced by WMN
for a t period newsvendor game with per item cost c, per
item revenue r, and all demands within [m, M] satisﬁes
WMNT otalRegret =
t

j=1
(OPT(j) −WMN(j))
≤C ln(n)
1 −β +
ln

1
β
	
c(M −m)(r −c)t
nr(1 −β)
+
ln

1
β
	 t
j=1(OPT(j) −STOPT(j))
1 −β
where C = max{(M −m)(r −c), (M −m)c} is the
maximum possible single period regret, n is the number
of experts used by WMN, and β is the update parameter
used.
The ﬁrst term, which depends on the maximum
possible single period regret, is independent of the
number of periods. This gives the following corollary:
Corollary A.1. The average per period regret of
WMN approaches
ln

1
β
	
c(M −m)(r −c)
nr(1 −β)
+
ln

1
β
	 t
j=1(OPT(j) −STOPT(j))
t(1 −β)
as the number of periods becomes arbitrarily large.
B
WMNS Operation and Bounds
In this section, we’ll look at an extension of the results
and provide an algorithm which does well in the face
of an input sequence which can be decomposed into
subsequences, where for each subsequence a diﬀerent
expert does well.
This algorithm is analogous to the
“shifting target” version of Littlestone and Warmuth’s
Weighted Majority algorithm. Using the same method
of selecting experts as in Section A.1, we show a bound
on the regret of the algorithm in terms of the “semi-
static oﬄine optimal” algorithm SSTOPT. SSTOPT is
a version of STOPT described above which is allowed to
change its choice k −1 times during the sequence.
B.1
Doing as Well as the Best Expert on any
Subsequence
Deﬁnitions We again consider playing t periods of
a newsvendor game, with all demands drawn from
[m, M], per item cost c, and per item revenue r. The
algorithm described here, WMNS (Weighted Majority
Newsvendor Shifting), will however operate slightly
diﬀerently compared to WMN.
These diﬀerences will
60

allow us to partition the sequence of periods into
subsequences, and show that WMNS will perform as
well as the best expert for each subsequence, despite the
fact that nothing is known about when the subsequences
start or end or how many there are.
Aside from the deﬁnitions mentioned in Section A,
we need to deﬁne two subsets of the n experts: UPD,
those which are “updatable”, and !UPD, those which
are not. As we will see, WMNS uses a weight limiting
factor δ.
For any period j, UPD is deﬁned as those
experts whose weights satisfy w(j)
i
> (δ n
i=1 w(j)
i )/n.
!UPD contains all other experts.
Algorithm WMNS WMNS operates as WMN, with a
couple of notable diﬀerences. First, a weight limiting
parameter δ
∈(0, 1] is chosen in addition to the
weight update parameter β
∈
(0, 1].
Initially all
experts’ weights are set to 1. In each period j, WMNS
orders the amount γ(j) which is the weighted average
of the predictions of the updatable experts:
γ(j) =

i∈UPD w(j)
i x(j)
i / 
i∈UPD w(j)
i .
In every period, after the actual demand d(j) is
revealed, we update only the experts in UPD by the
same factor F described in Section A: w(j+1)
i
= w(j)
i F
where
βf(d(j),x(j)
i
) ≤F ≤1 −(1 −β)f(d(j), x(j)
i ) .
Again,
we use f(d(j), x(j)
i )
=

d(j)(r −c) −
min{d(j), x(j)
i }r + x(j)
i c

/C, where C = max{(M −
m)(r −c), (M −m)c}.
Analysis We start by showing that for any subse-
quence, WMNS performs nearly as well as the best ex-
pert for that subsequence. This means that if an ex-
pert does quite well for some subsequence, and then for
another (later) subsequence another expert does quite
well, WMNS will track the change quickly.
We let s(j) = n
i=1 w(j)
i
be the total sum of weights
of all experts in period j, s(j)
UPD = 
i∈UPD w(j)
i
be
the sum of weights of updatable experts, and s(j)
!UPD =

i∈!UPD w(j)
i
be the sum of weights of not updatable
experts. We deﬁne init to be the index of the ﬁrst period
of the subsequence, and fin to be the index of the last
period of the subsequence.
We
are
interested
in
ﬁnding
a
bound
for
ln(s(fin+1)/s(init)).
First we note that, by the oper-
ation of WMNS, for any period j and any expert i,
w(j)
i
≥βδs(j)/n. This is also true for the ﬁrst period,
because 1 ≥βδs(1)/n = βδ.
By the bound on the update factor F and the
mechanism of WMNS, we have that s(j+1) is less than
or equal to:
X
i∈UPD
w(j)
i
h
1 −(1 −β)f(d(j), x(j)
i )
i
+
X
i∈!UPD
w(j)
i
= s(j) −(1 −β)
X
i∈UPD
w(j)
i
f(d(j), x(j)
i )
= s(j) −(1 −β)
" X
i∈UPD
w(j)
i
d(j)(r −c)
C
−
X
i∈UPD
w(j)
i
min{d(j), x(j)
i }r
C
+
X
i∈UPD
w(j)
i
x(j)
i c
C
#
≤s(j) −(1 −β) 1
C
"
s(j)
UPDd(j)(r −c)
−min{s(j)
UPDγ(j), s(j)
UPDd(j)}r + s(j)
UPDγ(j)c
#
.
We arrive at the last line by the deﬁnition of s(j)
UPD
and γ(j), as well as moving the summation inside of the
min expression as in Section A. Next we need a lower
bound for s(j)
UPD:
s(j)
UPD = s(j) −

i∈!UPD
w(j)
i
≥s(j) −

i∈!UPD
δs(j)/n
≥s(j)(1 −δ) .
So, we have that
s(j+1) ≤s(j) −(1 −β)s(j)
UPDf(d(j), γ(j))
≤s(j) 
1 −(1 −β)(1 −δ)f(d(j), γ(j))

.
Over all periods in this subsequence,
s(fin+1) ≤s(init)
fin
Y
j=init
h
1 −(1 −β)(1 −δ)f(d(j), γ(j))
i
,
ln
„s(fin+1)
s(init)
«
≤
fin
X
j=init
−(1 −β)(1 −δ)f(d(j), γ(j)) ,
fin
X
j=init
f(d(j), γ(j)) ≤ln(s(init)/s(fin+1))
(1 −β)(1 −δ)
.
In any period j,
because WMNS doesn’t up-
date weights below βδs(j)/n, we know that w(init)
i
>
βδs(init)/n. If we let mi = fin
j=init f(d(j), x(j)
i ), we have
by the lower bound on the update factor F:
s(fin+1) ≥w(fin+1)
i
≥w(init)
i
βmi, ∀i
≥βδs(init)
n
βmi, ∀i
61

Consequently, for all experts i:
fin

j=init
f(d(j), γ(j)) ≤
ln

s(init)
s(fin+1)
	
(1 −β)(1 −δ)
≤
ln

s(init)
βδs(init)βmi/n
	
(1 −β)(1 −δ)
=
ln

n
βδ
	
+ mi ln

1
β
	
(1 −β)(1 −δ)
.
By substitution and rearrangement similar to that
in Section A, we arrive at the following theorem:
Theorem B.1. For any subsequence of newsvendor pe-
riods indexed from init to ﬁn and any expert i, WMNS’s
regret satisﬁes
WMNSSubseqRegret =
fin

j=init
(OPT(j) −WMNS(j))
≤
C ln

n
βδ
	
(1 −β)(1 −δ) +
ln

1
β
	 fin
j=init(OPT(j) −EX(j)
i )
(1 −β)(1 −δ)
B.2
Doing
as
Well
as
SSTOPT SSTOPT,
the
“Semi-Static Oﬄine Optimal” algorithm is a slightly
stronger version of STOPT, which is allowed to change
its order choice k −1 times for the whole t period
newsvendor game. Consider subsequence l, (1 ≤l ≤k),
which is the subsequence where SSTOPT is using it’s
lth choice.
For this subsequence, SSTOPT acts as a
static oﬄine optimal for periods from initl to finl,
the beginning and ending indices of l. We deﬁne tl =
finl+1−initl; the number of periods in subsequence l.
(Thus, k
l=1 tl = t.) In essense, we are now considering
k individual newsvendor games against diﬀerent static
optimal algorithms.
By deﬁning experts according to the same construc-
tion of Section A.1, we can show that for any subse-
quence l,
finl

j=initl
(OPT(j) −WMNS(j))
≤
C ln

n
βδ
	
(1 −β)(1 −δ) +
ln

1
β
	
c(M −m)(r −c)tl
nr(1 −β)(1 −δ)
+
ln

1
β
	 finl
j=initl(OPT(j) −SSTOPT(j))
(1 −β)(1 −δ)
.
Summing over all k subsequences (again, WMNS
requires no knowledge of how long subsequences are,
or even how many there are), we ultimately reach the
following theorem:
Theorem B.2. The total regret experienced by WMNS
for a t period newsvendor game with per item cost c, per
item revenue r, and all demands within [m, M] satisﬁes
WMNST otalRegret =
t

j=1
(OPT(j) −WMNS(j))
≤
kC ln

n
βδ
	
(1 −β)(1 −δ) +
ln

1
β
	
c(M −m)(r −c)t
nr(1 −β)(1 −δ)
+
ln

1
β
	 t
j=1(OPT(j) −SSTOPT(j))
(1 −β)(1 −δ)
where C = max{(M −m)(r −c), (M −m)c} is the
maximum possible single period regret, n is the number
of experts used by WMNS, β is the update parameter
used, and δ is the weight limiting parameter used.
When k = t, then SSTOPT is equivalent to OPT,
though in this case the bound becomes useless because
of the kC factor in the ﬁrst term. When k is constant,
however, we can note the following corollary:
Corollary B.1. When
the
number
of
changes
k
SSTOPT is allowed is constant, the average per period
regret of WMNS approaches
ln

1
β
	
c(M −m)(r −c)
nr(1 −β)(1 −δ)
+
ln

1
β
	 t
j=1(OPT(j) −SSTOPT(j))
t(1 −β)(1 −δ)
as the number of periods becomes arbitrarily large.
C
FPL Operation and Bounds
FPL, for Follow the Perturbed Leader, was developed
by Kalai and Vempala in [12]. (In this paper, they give
two versions of the algorithm, FPL and FPL *. We use
the latter for our problem.) In the experts setting, the
algorithm keeps track of the total regret suﬀered by each
expert. When a decision needs to be made, a random
cost is added to each expert’s sum regret so far, and
FPL chooses the expert with the lowest overall regret.
Deﬁnitions FPL takes as input a “randomness” pa-
rameter ϵ. For the bounds given to hold, ϵ must be in
the range (0, 1], however the algorithm will still operate
with larger values.
Two other values are used by FPL and the bounds
given in [12], A and D. A is deﬁned as the maximum
of the sum of all experts’ regret for a single period,
and D is the maximum “diameter diﬀerence” between
62

two decisions.
(Because FPL is applicable to general
decision making settings, these values have a more
precise meaning which we won’t go into.) However, in
Section 2 of [12], the authors note that for the experts
problem, D is 1, and A is the maximum regret of a
single expert for one period. In our case, this is then
A = C = max{(M −m)(r −c), (M −m)c}.
Algorithm FPL For each period, let si be the total
regret suﬀered by expert i so far. For each expert i,
choose a perturbation factor pi from the exponential
distribution with rate ϵ/2A. The best perturbed expert
so far then is argmaxi{si + pi}. Use the prediction of
this expert.
Note that this algorithm is a speciﬁc, limited version
of the general algorithm FPL *.
Bounds of FPL Kalai and Vempala give the following
theorem, which bounds the regret of FPL in terms of
the regret of the best performing expert:
Theorem C.1. (Due to Kalai and Vempala.)
The
expected regret of FPL satisﬁes
E[FPLT otalRegret] ≤(1 + ϵ)MinT otalRegret
+ 4AD(1 + ln(n))
ϵ
Where MinT otalRegret is the regret of the best per-
forming expert.
If we place experts in n buckets accoring to Section
A.1, we know that the best expert won’t suﬀer more
than
c(M −m)(r −c)t
nr
extra regret from the true static optimal on any t period
newsvendor sequence. Using the notation of Appendix
A, we can now give a theorem similar to Theorem A.1:
Theorem C.2. The total regret experienced by FPL for
a t period newsvendor game with per item cost c, per
item revenue r, and all demands within [m, M] satisﬁes
E [FPLTotalRegret]
≤4C(1 + ln(n))
ϵ
+ (1 + ϵ)c(M −m)(r −c)t
nr
+ (1 + ϵ)
t

j=1
(OPT(j) −STOPT(j))
where C = max{(M −m)(r −c), (M −m)c} is the max-
imum possible single period regret, n is the number of
experts used by FPL, and ϵ is the randomness parameter
used.
OPT(j) and STOPT(j) are the proﬁts of OPT and
STOPT, respectively, in period j.
D
Newsvendor Extensions
One
frequently
discussed
extension
to
the
classic
newsvendor problem considers, in addition, per item
overstock costs co and per item understock costs cu.
Understock costs can be used to express customer ill
will due to unmet demand, or perhaps in the vaccine
ordering setting to express costs to the economy due to
unvaccinated portions of the workforce. Overstock costs
may represent extra storage costs or disposal costs for
outdated products such as unwanted consumer electron-
ics. Vairaktarakis[21] and Bertsimas and Thiele[3] also
discusses these extensions.
The proﬁt function for a prediction x in a period is
given by min{d, x}r −xc −max{(d −x)cu, (x −d)co}.
Because of the negative max term in the expression, we
can use this model and the bounds will follow through
a proof similar to that of Section A using the trick
of moving the summation inside the max expression.
Of course, we’ll also need to adjust C and f(d(j), x(j)
i )
accordingly.
Using the same bucket endpoints as in
Section A.1 and letting expert i consistently choose the
minimax regret order quantity
x(j)
i
= qi(r −c + cu) + qi−1(c + co)
r + co + cu
= i(M −m)
n
−(M −m)(c + co)
n(r + co + cu)
+ m, ∀j,
we can get the following bound for WMN:
WMNT otalRegret
≤C2 ln(n)
1 −β
+
ln

1
β
	
(M −m)(c + co)(r −c + cu)t
n(1 −β)(r + co + cu)
+
ln

1
β
	 t
j=1(OPT(j) −STOPT(j))
1 −β
where C2 = max{(M −m)(r−c+cu), (M −m)(c+co)}.
Similar bounds can be had for WMNS and FPL.
Another commonly discussed extension considers
per item salvage proﬁt s (usually s < c), wherein unused
items can be sold for a guaranteed smaller proﬁt. In
this version, the proﬁt function for a prediction x is
min{d, x}r−xc+max{0, (x−d)s}. Here, the max term is
positive, so the expression won’t follow through a proof
technique similar to the one used for WMN and WMNS.
This indicates that, unfortunately, salvage proﬁts are
incompatible with these approaches, though a bound
can still be had for FPL.
63

Routing in Graphs with Applications
to Material Flow Problems
Rolf H. M¨ohring∗
Material ﬂow problems are complex logistic optimization problems. We want
to utilize the available logistic network in such a way that the load is minimized
or the throughput is maximized.
This lecture deals with these optimization
problems from the viewpoint of network ﬂow theory and reports on two indus-
trial applications: (1) contolling material ﬂow with automated guided vehicles
in a container terminal (cooperation with HHLA), and (2) timetabling in public
transport (cooperation with Deutsche Bahn and Berlin Public Transport). The
key ingredient for (1) is a very fast real-time algorithm which avoids collisions,
deadlocks, and other conﬂicts already at route computation, while for (2) it is
the use of integer programs based on special bases of the cycle space of the
routing graph.
References
[1] E.
Gawrilow,
E.
K¨ohler,
R.
H.
M¨ohring,
and
B.
Sten-
zel,
Dynamic
routing
of
automated
guided
vehicles
in
real-time,
Tech.
Rep.
039-2007,
TU
Berlin,
2007,
http://www.math.tu-
berlin.de/coga/publications/techreports/2007/Report-039-2007.html
[2] C. Liebchen and R. H. M¨ohring, The modeling power of the periodic
event scheduling problem: Railway timetables - and beyond, in Algorith-
mic Methods for Railway Optimization, F. Geraets, L. Kroon, A. Sch¨obel,
D. Wagner, and C. D. Zaroliagis, eds., vol. 4359 of Lecture Notes in Com-
puter Science, Springer, Berlin/Heidelberg, 2007, pp. 3–40.
∗Institut f¨ur Mathematik, Technische Universit¨at Berlin, Berlin, Germany. Research sup-
ported by DFG Research Center Matheon
64

























































































































































































Analysis of the Expected Number of Bit Comparisons
Required by Quickselect∗
James Allen Fill†
Tak´ehiko Nakama‡
Abstract
When algorithms for sorting and searching are applied
to keys that are represented as bit strings, we can quan-
tify the performance of the algorithms not only in terms
of the number of key comparisons required by the algo-
rithms but also in terms of the number of bit compar-
isons. Some of the standard sorting and searching algo-
rithms have been analyzed with respect to key compar-
isons but not with respect to bit comparisons. In this ex-
tended abstract, we investigate the expected number of
bit comparisons required by Quickselect (also known
as Find). We develop exact and asymptotic formulae
for the expected number of bit comparisons required to
ﬁnd the smallest or largest key by Quickselect and
show that the expectation is asymptotically linear with
respect to the number of keys. Similar results are ob-
tained for the average case. For ﬁnding keys of arbitrary
rank, we derive an exact formula for the expected num-
ber of bit comparisons that (using rational arithmetic)
requires only ﬁnite summation (rather than such oper-
ations as numerical integration) and use it to compute
the expectation for each target rank.
1
Introduction and Summary
When an algorithm for sorting or searching is analyzed,
the algorithm is usually regarded either as comparing
keys pairwise irrespective of the keys’ internal structure
or as operating on representations (such as bit strings)
of keys. In the former case, analyses often quantify the
performance of the algorithm in terms of the number
of key comparisons required to accomplish the task;
Quickselect (also known as Find) is an example of
those algorithms that have been studied from this point
of view. In the latter case, if keys are represented as bit
strings, then analyses quantify the performance of the
algorithm in terms of the number of bits compared until
∗Supported by NSF grant DMS–0406104, and by The Johns
Hopkins University’s Acheson J. Duncan Fund for the Advance-
ment of Research in Statistics.
†Department of Applied Mathematics and Statistics at The
Johns Hopkins University.
‡Department of Applied Mathematics and Statistics at The
Johns Hopkins University.
it completes its task. Digital search trees, for example,
have been examined from this perspective.
In order to fully quantify the performance of a sort-
ing or searching algorithm and enable comparison be-
tween key-based and digital algorithms, it is ideal to
analyze the algorithm from both points of view. How-
ever, to date, only Quicksort has been analyzed with
both approaches; see Fill and Janson [3]. Before their
study, Quicksort had been extensively examined with
regard to the number of key comparisons performed by
the algorithm (e.g., Knuth [11], R´egnier [16], R¨osler
[17], Knessl and Szpankowski [9], Fill and Janson [2],
Neininger and R¨uschendorf [15]), but it had not been
examined with regard to the number of bit comparisons
in sorting keys represented as bit strings. In their study,
Fill and Janson assumed that keys are independently
and uniformly distributed over (0,1) and that the keys
are represented as bit strings. [They also conducted the
analysis for a general absolutely continuous distribution
over (0,1).] They showed that the expected number of
bit comparisons required to sort n keys is asymptotically
equivalent to n(ln n)(lg n) as compared to the lead-order
term of the expected number of key comparisons, which
is asymptotically 2n ln n. We use ln and lg to denote
natural and binary logarithms, respectively, and use log
when the base does not matter (for example, in remain-
der estimates).
In this extended abstract,
we investigate the
expected
number
of
bit
comparisons
required
by
Quickselect.
Hoare [7] introduced this search algo-
rithm, which is treated in most textbooks on algorithms
and data structures.
Quickselect selects the m-th
smallest key (we call it the rank-m key) from a set of
n distinct keys. (The keys are typically assumed to be
distinct, but the algorithm still works—with a minor
adjustment—even if they are not distinct.) The algo-
rithm ﬁnds the target key in a recursive and random
fashion. First, it selects a pivot uniformly at random
from n keys.
Let k denote the rank of the pivot.
If
k = m, then the algorithm returns the pivot. If k > m,
then the algorithm recursively operates on the set of
keys smaller than the pivot and returns the rank-m key.
Similarly, if k < m, then the algorithm recursively oper-
249

ates on the set of keys larger than the pivot and returns
the (k −m)-th smallest key from the subset. Although
previous studies (e.g., Knuth [10], Mahmoud et al. [13],
Gr¨ubel and U. R¨osler [6], Lent and Mahmoud [12], Mah-
moud and Smythe [14], Devroye [1], Hwang and Tsai [8])
examined Quickselect with regard to key comparisons,
this study is the ﬁrst to analyze the bit complexity of
the algorithm.
We suppose that the algorithm is applied to n dis-
tinct keys that are represented as bit strings and that
the algorithm operates on individual bits in order to
ﬁnd a target key.
We also assume that the n keys
are uniformly and independently distributed in (0, 1).
For instance, consider applying Quickselect to ﬁnd the
smallest key among three keys k1, k2, and k3 whose bi-
nary representations are .01001100..., .00110101..., and
.00101010..., respectively. If the algorithm selects k3 as
a pivot, then it compares each of k1 and k2 to k3 in
order to determine the rank of k3. When k1 and k3 are
compared, the algorithm requires 2 bit comparisons to
determine that k3 is smaller than k1 because the two
keys have the same ﬁrst digit and diﬀer at the second
digit. Similarly, when k2 and k3 are compared, the al-
gorithm requires 4 bit comparisons to determine that
k3 is smaller than k2. After these comparisons, key k3
has been identiﬁed as smallest.
Hence the search for
the smallest key requires a total of 6 bit comparisons
(resulting from the two key comparisons).
We let µ(m, n) denote the expected number of bit
comparisons required to ﬁnd the rank-m key in a ﬁle
of n keys by Quickselect. By symmetry, µ(m, n) =
µ(n+1−m, n). First, we develop exact and asymptotic
formulae for µ(1, n) = µ(n, n), the expected number
of bit comparisons required to ﬁnd the smallest key by
Quickselect, as summarized in the following theorem.
Theorem 1.1. The expected number µ(1, n) of bit com-
parisons required by Quickselect to ﬁnd the smallest
key in a ﬁle of n keys that are independently and uni-
formly distributed in (0, 1) has the following exact and
asymptotic expressions:
µ(1, n)
=
2n(Hn −1) + 2
n−1
X
j=2
Bj
n −j + 1 −
 n
j

j(j −1)(1 −2−j)
=
cn −
1
ln 2(ln n)2 −
 2
ln 2 + 1

ln n + O(1),
where Hn and Bj denote harmonic and Bernoulli num-
bers, respectively, and, with χk := 2πik
ln 2 and γ := Euler’s
constant .= 0.57722, we deﬁne
c
:=
28
9 + 17 −6γ
9 ln 2
−4
ln 2
X
k∈Z\{0}
ζ(1 −χk)Γ(1 −χk)
Γ(4 −χk)(1 −χk)
(1.1)
.=
5.27938.
The constant c can alternatively be expressed as
(1.2)
c = 2
∞
X
k=0

1 + 2−k
2k
X
j=1
ln j
2k

.
It is easily seen that the expression (1.1) is real, even
though it involves the imaginary numbers χk.
The
asymptotic formula shows that the expected num-
ber of bit comparisons is asymptotically linear in n
with the lead-order coeﬃcient approximately equal to
5.27938. Hence the expected number of bit comparisons
is asymptotically diﬀerent from that of key comparisons
required to ﬁnd the smallest key only by a constant
factor (the expectation for key comparisons is asymp-
totically 2n).
Complex-analytic methods are utilized
to obtain the asymptotic formula; in a future paper,
it will be shown how the linear lead-order asymptotics
µ(1, n) ∼cn [with c given in the form (1.2)] can be ob-
tained without resort to complex analysis. An outline
of the proof of Theorem 1.1 is provided in Section 3.
We also derive exact and asymptotic expressions
for the expected number of bit comparisons for the av-
erage case.
We denote this expectation by µ( ¯m, n).
In the average case, the parameter m in µ(m, n) is
considered a discrete uniform random variable; hence
µ( ¯m, n) =
1
n
Pn
m=1 µ(m, n). The derived asymptotic
formula shows that µ( ¯m, n) is also asymptotically linear
in n; see (4.11). More detailed results for µ( ¯m, n) are
described in Section 4.
Lastly, in Section 5, we derive an exact expression
of µ(m, n) for each ﬁxed m that is suited for computa-
tions. Our preliminary exact formula for µ(m, n) [shown
in (2.7)] entails inﬁnite summation and integration. As
a result, it is not a desirable form for numerically com-
puting the expected number of bit comparisons. Hence
we establish another exact formula that only requires
ﬁnite summation and use it to compute µ(m, n) for
m = 1, . . . , n, n = 2, . . . , 25. The computation leads to
the following conjectures: (i) for ﬁxed n, µ(m, n) [which
of course is symmetric about (n + 1)/2] increases in m
for m ≤(n+1)/2; and (ii) for ﬁxed m, µ(m, n) increases
in n (asymptotically linearly).
Space limitations on this extended abstract force
us to omit a substantial portion of the details of our
study. We refer the interested reader to our full-length
paper [4].
250

2
Preliminaries
To investigate the bit complexity of Quickselect, we
follow the general approach developed by Fill and
Janson [3]. Let U1, . . . , Un denote the n keys uniformly
and independently distributed on (0, 1), and let U(i)
denote the rank-i key. Then, for 1 ≤i < j ≤n (assume
n ≥2),
P{U(i) and U(j) are compared}
=

















2
j −m + 1
if m ≤i
2
j −i + 1
if i < m < j
2
m −i + 1
if j ≤m.
(2.1)
To determine the ﬁrst probability in (2.1), note that
U(m), . . . , U(j) remain in the same subset until the ﬁrst
time that one of them is chosen as a pivot. Therefore,
U(i) and U(j) are compared if and only if the ﬁrst
pivot chosen from U(m), . . . , U(j) is either U(i) or U(j).
Analogous arguments establish the other two cases.
For 0 < s < t < 1, it is well known that the
joint density function of U(i) and U(j) is given by
fU(i),U(j)(s, t)
:=

n
i −1, 1, j −i −1, 1, n −j

×si−1(t −s)j−i−1(1 −t)n−j.
(2.2)
Clearly, the event that U(i) and U(j) are compared
is independent of the random variables U(i) and U(j).
Hence, deﬁning
P1(s, t, m, n)
:=
X
m≤i<j≤n
2
j −m + 1fU(i),U(j)(s, t),
(2.3)
P2(s, t, m, n)
:=
X
1≤i<m<j≤n
2
j −i + 1fU(i),U(j)(s, t),
(2.4)
P3(s, t, m, n)
:=
X
1≤i<j≤m
2
m −i + 1fU(i),U(j)(s, t),
(2.5)
P(s, t, m, n)
:=
P1(s, t, m, n) + P2(s, t, m, n)
+P3(s, t, m, n)
(2.6)
[the sums in (2.3)–(2.5) are double sums over i and
j], and letting β(s, t) denote the index of the ﬁrst bit
at which the keys s and t diﬀer, we can write the
expectation µ(m, n) of the number of bit comparisons
required to ﬁnd the rank-m key in a ﬁle of n keys as
µ(m, n)
=
Z 1
0
Z 1
s
β(s, t)P(s, t, m, n) dt ds
=
∞
X
k=0
2k
X
l=1
Z (l−1
2 )2−k
(l−1)2−k
Z l2−k
(l−1
2 )2−k(k + 1)
(2.7)
×P(s, t, m, n) dt ds;
in this expression, note that k represents the last bit at
which s and t agree.
3
Analysis of µ(1, n)
In Section 3.1, we outline a derivation of the exact
expression for µ(1, n) shown in Theorem 1.1; see the
full paper [4] for the numerous suppressed details of
the various computations. In Section 3.2, we prove the
asymptotic result asserted in Theorem 1.1.
3.1
Exact Computation of µ(1, n) Since the con-
tribution of P2(s, t, m, n) or P3(s, t, m, n) to P(s, t, m, n)
is zero for m = 1, we have P(s, t, 1, n) = P1(s, t, 1, n)
[see (2.4) through (2.6)]. Let x := s, y := t −s, z :=
1 −t. Then
P1(s, t, 1, n)
=
zn
X
1≤i<j≤n
2
j

n
i −1, 1, j −i −1, 1, n −j

×xi−1yj−i−1z−j
=
2
n
X
j=2
(−1)j
n
j

tj−2.
(3.1)
From (2.7) and (3.1),
µ(1, n)
=
n
X
j=2
(−1)j n
j

j −1
∞
X
k=0
(k + 1)2−kj
2k
X
l=1
[lj−1 −(l −1
2)j−1].
(3.2)
To further transform (3.2), deﬁne
aj,r =













Br
r
j −1
r −1

if r ≥2
1
2
if r = 1
1
j
if r = 0,
(3.3)
251

where Br denotes the r-th Bernoulli number.
Let
Sn,j := Pn
l=1 lj−1.
Then Sn,j = Pj−1
r=0 aj,rnj−r (see
Knuth [11]), and
µ(1, n)
=
2
n
X
j=2
(−1)j n
j

j −1
∞
X
k=0
(k + 1)2−kj
×
j−1
X
r=1
aj,r2k(j−r)(1 −2−r)
=
2n(Hn −1) + 2tn,
(3.4)
where Hn denotes the n-th harmonic number and
tn :=
n−1
X
j=2
Bj
j(1 −2−j)
"
n −
 n
j

j −1
−1
#
.
(3.5)
3.2
Asymptotic Analysis of µ(1, n) In order to
obtain an asymptotic expression for µ(1, n), we analyze
tn in (3.4)–(3.5).
The following lemma provides an
exact expression for tn that easily leads to an asymptotic
expression for µ(1, n):
Lemma 3.1. Let
γ
denote
Euler’s
constant
( .= 0.57722), and deﬁne χk := 2πik
ln 2 . Then
tn
=
−(nHn −n −1) + a(n −2)
−
1
2 ln 2

H2
n + H(2)
n
−7
2

+
γ −1
ln 2 −1
2
 
Hn −3
2

+b −Σn,
where
a
:=
14
9 + 17 −6γ
18 ln 2
−2
ln 2
X
k∈Z\{0}
ζ(1 −χk)Γ(1 −χk)
Γ(4 −χk)(1 −χk) ,
b
:=
X
k∈Z\{0}
2ζ(1 −χk)Γ(−χk)
(ln 2)(1 −χk)Γ(3 −χk),
Σn
:=
X
k∈Z\{0}
ζ(1 −χk)Γ(−χk)Γ(n + 1)
(ln 2)(1 −χk)Γ(n + 1 −χk),
and H(2)
n
denotes the n-th Harmonic number of order 2,
i.e., H(2)
n
:= Pn
i=1
1
i2 .
The proof of the lemma involves complex-analytic tech-
niques and is rather lengthy, so it is omitted in this
extended abstract; see our full-length paper [4]. From
(3.4), the exact expression for tn also provides an alter-
native exact expression for µ(1, n).
Using Lemma 3.1, we complete the proof of
Theorem 1.1. We know
Hn = ln n + γ + 1
2n −
1
12n2 + O(n−4),
(3.6)
H(2)
n = π2
6 −1
n +
1
2n2 + O(n−3).
(3.7)
Combining (3.6)–(3.7) with (3.4) and Lemma 3.1, we
obtain an asymptotic expression for µ(1, n):
µ(1, n)
=
2an −
1
ln 2(ln n)2 −
 2
ln 2 + 1

ln n + O(1).
(3.8)
The term O(1) in (3.8) has ﬂuctuations of small magni-
tude due to Σn, which is periodic in log n with ampli-
tude smaller than 0.00110. Thus, as asserted in Theo-
rem 1.1, the asymptotic slope in (3.8) is
c
=
2a
=
28
9 + 17 −6γ
9 ln 2
−
4
ln 2
X
k∈Z\{0}
ζ(1 −χk)Γ(1 −χk)
Γ(4 −χk)(1 −χk) .
(3.9)
The alternative expression (1.2) for c is established in
a forthcoming revision to our full-length paper [4]; this
was also done independently by Grabner and Prodinger
[5]. As described in their paper, suitable use of Stirling’s
formula with bounds allows one to compute c very
rapidly to many decimal places.
4
Analysis of the Average Case: µ( ¯m, n)
4.1
Exact Computation of µ( ¯m, n) Here we con-
sider the parameter m in µ(m, n) as a discrete ran-
dom variable with uniform probability mass function
P{m = i} = 1/n, i = 1, 2, . . . , n, and average over m
while the parameter n is ﬁxed. Thus, using the notation
deﬁned in Section 2,
µ( ¯m, n) = µ1( ¯m, n) + µ2( ¯m, n) + µ3( ¯m, n),
where, for l = 1, 2, 3,
µl( ¯m, n)
=
Z 1
0
Z 1
s
β(s, t) 1
n
n
X
m=1
Pl(s, t, m, n) dt ds.
(4.1)
Here µ1( ¯m, n) = µ3( ¯m, n) by an easy symmetric argu-
ment we omit, and so
µ( ¯m, n) = 2µ1( ¯m, n) + µ2( ¯m, n);
(4.2)
we will compute µ1( ¯m, n) and µ2( ¯m, n) exactly in
Section 4.1.1.
252

4.1.1
Exact Computation of µ( ¯m, n) We use the
following lemma in order to compute µ1( ¯m, n) exactly:
Lemma 4.1.
Z 1
0
Z 1
s
β(s, t) 1
n
n
X
m=2
P1(s, t, m, n) dt ds
=
2
n−1
X
j=2
(−1)j n−1
j

j(j −1)
+ 2
9
n−1
X
j=2
(−1)j n−1
j

j −1
−2
n−1
X
j=3
Bj
n −j + 1 −
 n−1
j−1

j(j −1)(j −2)(1 −2−j)
−2
n−1
X
j=2
(−1)j n−1
j

(j + 1)j(j −1)(1 −2−j).
Space limitations on this extended abstract do not allow
us to prove this lemma here; we give the proof in our
full-length paper [4]. Since
µ1( ¯m, n)
= 1
nµ(1, n)
+
Z 1
0
Z 1
s
β(s, t) 1
n
n
X
m=2
P1(s, t, m, n) dt ds,
it follows from (3.4) and Lemma 4.1 that
µ1( ¯m, n)
=
n −1 −4
n
X
j=3
(−1)j n−1
j−1

j(j −1)(j −2)
+ 2
n
n−1
X
j=2
Bj
n −j + 1 −
 n
j

j(j −1)(1 −2−j)
+2
9
n−1
X
j=2
(−1)j n−1
j

j −1
−2
n−1
X
j=3
Bj
n −j + 1 −
 n−1
j−1

j(j −1)(j −2)(1 −2−j)
−2
n−1
X
j=2
(−1)j n−1
j

(j + 1)j(j −1)(1 −2−j).
(4.3)
Similarly, after laborious calculations, one can show that
µ2( ¯m, n) = −4
n
n
X
j=2
(−1)j n
j

j(j −1)[1 −2−(j−1)] + 2(n −1).
(4.4)
From (4.2)–(4.4), we obtain
µ( ¯m, n)
=
2(n −1) −8
n
X
j=3
(−1)j n−1
j−1

j(j −1)(j −2)
+ 4
n
n−1
X
j=2
Bj
n −j + 1 −
 n
j

j(j −1)(1 −2−j)
+4
9
n−1
X
j=2
(−1)j n−1
j

j −1
−4
n−1
X
j=3
Bj
n −j + 1 −
 n−1
j−1

j(j −1)(j −2)(1 −2−j)
−4
n−1
X
j=2
(−1)j n−1
j

(j + 1)j(j −1)(1 −2−j)
−4
n
n
X
j=2
(−1)j n
j

j(j −1)[1 −2−(j−1)] + 2(n −1).
(4.5)
We rewrite or combine some of the terms in (4.5) for
the asymptotic analysis of µ( ¯m, n) described in the next
section. We deﬁne
F1(n)
:=
n
X
j=3
(−1)j n
j

(j −1)(j −2),
F2(n)
:=
n−1
X
j=2
Bj
j(1 −2−j)
"
n −
 n
j

j −1
−1
#
,
F3(n)
:=
n−1
X
j=2
(−1)j n−1
j

j −1
F4(n)
:=
n−1
X
j=3
Bj
j(j −1)(1 −2−j)
"
n −1 −
 n−1
j−1

j −2
−1
#
,
F5(n)
:=
n
X
j=3
(−1)j n
j

j(j −1)(j −2)[1 −2−(j−1)].
Then
µ( ¯m, n)
=
2(n −1) −8
nF1(n) + 4
nF2(n) + 4
9F3(n)
−4F4(n) + 8
nF5(n).
(4.6)
4.2
Asymptotic Analysis of µ( ¯m, n) We derive
an asymptotic expression for µ( ¯m, n) shown in (4.6).
253

Routine arguments show that
F1(n)
=
−1
2n2 ln n +
5
4 −γ
2

n2
−n ln n +
n2
2(n −1) −(γ + 1)n + O(1),
(4.7)
F3(n)
=
n ln n + (γ −1)n −ln n + O(1),
(4.8)
F4(n)
=
1
9n ln n +

˜a + 1
9γ −1
9

n + 8
9 ln n + O(1),
(4.9)
F5(n)
=
−1
2n2 ln n + 3 + ln 2 −γ
2
n2 −
1
2 ln 2n(ln n)2
+
 1
ln 2 −1
2

n ln n + O(n),
(4.10)
where
˜a
:=
7
36 ln 2 −41
72 −
γ
12 ln 2
−
X
k∈Z\{0}
ζ(1 −χk)Γ(1 −χk)
(ln 2)(2 −χk)Γ(4 −χk).
Since F2(n) is equal to tn, which is deﬁned at (3.5) and
analyzed in Section 3.2, we already have an asymptotic
expression for F2(n). Therefore, from (4.6)–(4.10), we
obtain the following asymptotic formula for µ( ¯m, n):
µ( ¯m, n)
=
4(1 + ln 2 −˜a)n −
4
ln 2(ln n)2
+4
 2
ln 2 −1

ln n + O(1).
(4.11)
The asymptotic slope 4(1 + ln 2 −˜a) is approximately
8.20731. We have not (yet) sought an alternative form
for ˜a like that for c in (1.2).
5
Derivation of a Closed Formula for µ(m, n)
The exact expression for µ(m, n) obtained in Section 2
[see (2.7)] involves inﬁnite summation and integration.
Hence it is not a preferable form for numerically com-
puting the expectation.
In this section, we establish
another exact expression for µ(m, n) that only involves
ﬁnite summation. We also use the formula to compute
µ(m, n) for m = 1, . . . , n, n = 2, . . . , 20.
As described in Section 2, it follows from equa-
tions (2.6)–(2.7) that
µ(m, n)
=
µ1(m, n) + µ2(m, n) + µ3(m, n),
where, for q = 1, 2, 3,
µq(m, n)
:=
∞
X
k=0
2k
X
l=1
Z (l−1
2 )2−k
s=(l−1)2−k
Z l2−k
t=(l−1
2 )2−k(k + 1)
×Pq(s, t, m, n) dt ds.
(5.1)
The same technique can be applied to eliminate the inﬁ-
nite summation and integration from each µq(m, n). We
describe the technique for obtaining a closed expression
of µ1(m, n).
First, we transform P1(s, t, m, n) shown in (2.3)
so that we can eliminate the integration in µ1(m, n).
Deﬁne
C1(i, j)
:=
I{1 ≤m ≤i < j ≤n}
×
2
j −m + 1

n
i −1, 1, j −i −1, 1, n −j

,
(5.2)
where I{1 ≤m ≤i < j
≤n} is an indicator
function that equals 1 if the event in braces holds and
0 otherwise. Then
P1(s, t, m, n) =
n−2
X
f=m−1
n−f−2
X
h=0
sfthC2(f, h),
(5.3)
where
C2(f, h)
:=
f+1
X
i=m
f+h+2
X
j=f+2
C1(i, j)
j −i −1
f −i + 1

×

n −j
h −j + f + 2

(−1)h−i−j+1.
Thus, from (5.1) and (5.3), we can eliminate the inte-
gration in µ1(m, n) and express it using polynomials in
l:
µ1(m, n)
=
n−2
X
f=m−1
n−f−2
X
h=0
C3(f, h)
∞
X
k=0
(k + 1)
×
2k
X
l=1
2−k(f+h+2)[lh+1 −(l −1
2)h+1]
×[(l −1
2)f+1 −(l −1)f+1],
(5.4)
where
C3(f, h) :=
1
(n + 1)(f + 1)C2(f, h).
254

One can show that
"
lh+1 −

l −1
2
h+1# "
l −1
2
f+1
−(l −1)f+1
#
=
f+h+1
X
j=1
C4(f, h, j)lj−1,
(5.5)
where
C4(f, h, j)
:=
(−1)f+h−j+1
1
2
h−j+2
×
(j−1) V f
X
j′=0 W(j−1−h)
f + 1
j′

h + 1
j −1 −j′

×
"
1 −
1
2
f+1−j′# 1
2
j′
.
From (5.4)–(5.5), we obtain
µ1(m, n)
=
n−2
X
f=m−1
n−f−2
X
h=0
f+h+1
X
j=1
C5(f, h, j)
×
∞
X
k=0
(k + 1)2−k(f+h+2)
2k
X
l=1
lj−1,
where
C5(f, h, j) := C3(f, h) · C4(f, h, j).
Here, as described in Section 3.1,
2k
X
l=1
lj−1 =
j−1
X
r=0
aj,r2k(j−r),
where aj,r is deﬁned by (3.3). Now deﬁne
C6(f, h, j, r) := aj,r C5(f, h, j).
Then
µ1(m, n) =
n−1
X
a=1
C7(a)(1 −2−a)−2,
(5.6)
where
C7(a) :=
n−2
X
f=m−1
n−f−2
X
h=α
f+h+1
X
j=β
C6(f, h, j, a + j −(f + h + 2)),
in which α := 0 W(a−f −1) and β := 1 W(f +h+2−a).
The procedure described above can be applied
to derive analogous exact formulae for µ2(m, n) and
µ3(m, n). In order to derive the analogous exact formula
for µ2(m, n), one need only start the derivation by
changing the indicator function in C1(i, j) [see (5.2)]
to I{1 ≤i < m < j ≤n} and follow each step of
the procedure; similarly, for µ3(m, n), one need only
start the derivation by changing the indicator function
to I{1 ≤i < j ≤m ≤n}.
Using the closed exact formulae of µ1(m, n),
µ2(m, n), and µ3(m, n), we computed µ(m, n) for n =
2, 3, . . . , 20 and m = 1, 2, . . . , n.
Figure 1 shows the
results, which suggest the following: (i) for ﬁxed n,
µ(m, n) [which of course is symmetric about (n + 1)/2]
increases in m for m ≤(n + 1)/2; and (ii) for ﬁxed m,
µ(m, n) increases in n (asymptotically linearly).
5
10
15
20
0
5
10
15
20
0
20
40
60
80
100
120
Expectation of bit comparisons
m
μ(m,n)
n
Figure
1:
Expected
number
of
bit
compar-
isons for Quickselect.
The closed formulae for
µ1(m, n), µ2(m, n), and µ3(m, n) were used to compute
µ(m, n) for n = 1, 2, . . . , 20 (n represents the number
of keys) and m = 1, 2, . . . , n (m represents the rank of
the target key).
6
Discussion
Our investigation of the bit complexity of Quickselect
revealed that the expected number of bit comparisons
required by Quickselect to ﬁnd the smallest or largest
key from a set of n keys is asymptotically linear in
n with the asymptotic slope approximately equal to
255

5.27938. Hence asymptotically it diﬀers from the ex-
pected number of key comparisons to achieve the same
task only by a constant factor. (The expectation for key
comparisons is asymptotically 2n; see Knuth [10] and
Mahmoud et al. [13]). This result is rather contrastive
to the Quicksort case in which the expected number of
bit comparisons is asymptotically n(ln n)(lg n) whereas
the expected number of key comparisons is asymptoti-
cally 2n ln n (see Fill and Janson [3]). Our analysis also
showed that the expected number of bit comparisons
for the average case remains asymptotically linear in n
with the lead-order coeﬃcient approximately equal to
8.20731. Again, the expected number is asymptotically
diﬀerent from that of key comparisons for the average
case only by a constant factor. (The expected number
of key comparisons for the average case is asymptoti-
cally 3n; see Mahmoud et al. [13]).
Although we have yet to establish a formula
analogous to (3.4) and (4.6) for the expected number
of bit comparisons to ﬁnd the m-th key for ﬁxed m, we
established an exact expression that only requires ﬁnite
summation and used it to obtain the results shown in
Figure 1. However, the formula remains computation-
ally complex. Written as a single expression, µ(m, n) is
a seven-fold sum of rather elementary terms with each
sum having order n terms (in the worst case); in this
sense, the running time of the algorithm for computing
µ(m, n) is of order n7. The expression for µ(m, n) does
not allow us to derive an asymptotic formula for it or to
prove the two intuitively obvious observations described
at the end of Section 5. The situation is substantially
better for the expected number of key comparisons
to ﬁnd the m-th key from a set of n keys; Knuth
[10] showed that the expectation can be written as
2[n+3+(n+1)Hn −(m+2)Hm −(n+3−m)Hn+1−m].
In
this
extended
abstract,
we
considered
independent and uniformly distributed keys in (0,1).
In this case, each bit in a bit-string key is 1 with
probability 0.5.
In ongoing research, we generalize
the model and suppose that each bit results from an
independent Bernoulli trial with success probability p.
The more general results of that research will further
elucidate the bit complexity of Quickselect and other
algorithms.
Acknowledgment.
We thank Philippe Fla-
jolet, Svante Janson, and Helmut Prodinger for helpful
discussions.
References
[1] L. Devroye.
On the probablistic worst-case time of
“Find”. Algorithmica, 31:291–303, 2001.
[2] J. A. Fill and S. Janson.
Quicksort asymptotics.
Journal of Algorithms, 44:4–28, 2002.
[3] J. A. Fill and S. Janson.
The number of bit com-
parisons used by Quicksort: An average-case analysis.
Proceedings of the ACM-SIAM Symposium on Discrete
Algorithms, pages 293–300, 2004.
[4] J. A. Fill and T. Nakama. Analysis of the expected
number of bit comparisons required by Quickselect.
http://front.math.ucdavis.edu/0706.2437, 2007.
[5] P. J. Grabner and H. Prodinger. On a constant arising
in the analysis of bit comparisons in Quickselect.
Preprint, 2007.
[6] R. Gr¨ubel and U. R¨osler.
Asymptotic distribution
theory for Hoare’s selection algorithm.
Advances in
Applied Probability, 28:252–269, 1996.
[7] C. R. Hoare. Find (algorithm 65). Communications of
the ACM, 4:321–322, 1961.
[8] H. Hwang and T. Tsai. Quickselect and the Dickman
function. Combinatorics, Probability and Computing,
11:353–371, 2002.
[9] C. Knessl and W. Szpankowski. Quicksort algorithm
again revisited. Discrete Mathematics and Theoretical
Computer Science, 3:43–64, 1999.
[10] D. E. Knuth.
Mathematical analysis of algorithms.
In Information Processing 71 (Proceedings of IFIP
Congress,
Ljubljana,
1971),
pages
19–27.
North-
Holland, Amsterdam, 1972.
[11] D. E. Knuth.
The Art of Computer Programming.
Volume 3:
Sorting and Searching.
Addison-Wesley,
Reading, Massachusetts, 1998.
[12] J. Lent and H. M. Mahmoud. Average-case analysis
of multiple Quickselect: An algorithm for ﬁnding order
statistics.
Statistics and Probability Letters, 28:299–
310, 1996.
[13] H. M. Mahmoud, R. Modarres, and R. T. Smythe.
Analysis of Quickselect: An algorithm for order statis-
tics. RAIRO Informatique Th´eorique et Applications,
29:255–276, 1995.
[14] H. M. Mahmoud and R. T. Smythe.
Probabilistic
analysis of multiple Quickselect. Algorithmica, 22:569–
584, 1998.
[15] R. Neininger and L. R¨uschendorf. Rates of convergence
for Quickselect. Journal of Algorithm, 44:51–62, 2002.
[16] M.
R´egnier.
A
limiting
distribution
of
Quick-
sort. RAIRO Informatique Th´eorique et Applications,
23:335–343, 1989.
[17] U. R¨osler.
A limit theorem for Quicksort.
RAIRO
Informatique Th´eorique et Applications, 25:85–100,
1991.
256

ALENEX/ANALCO AUTHOR INDEX 
Ba.su, A., 75 
Bauer, R., 13 
Bian.Z., 152 
Caroli. M., 101 
Chaudhary. A..49 
Cherkassky, B. V., 118 
Chimani, M.,27 
C o l e m a n , ! , 133 
Delling, D., 13 
Dumitriu, D., 65 
Nakama,T.,249 
Nguyen, V, 213 
O'Neil.S.,49 
Panholzer, A.,234 
Pluguet, F.,37 
Prodinger. H.,234 
Sabhnani, G., 75 
Sanders, P., 3. 90 
Schultes, D.,90 
Filkov, V., 109 
Fill, J. A., 249 
Funke, S., 65 
Gebauer, H..241 
Geisberger, R.,90 
Georgiadis, L., 118 
Goder, A., 109 
Goldberg, A. V., 118 
Grossi. R., 191 
Gu, Q.-P., 152 
Gupta, A., 191 
Jacobs,!. 142 
Talbot, D., 203 
Talbot, J., 203 
Tamaki.H., 152 
Tarjan, R. E., 118 
Teillaud, M,, 101 
Transier, F., 3 
Vitter, J. S., 191 
Werneck, R. F, 
Wirth, A., 133 
Wuyts, R., 37 
118 
Yoshitake.Y., 152 
Kandyba, M.,27 
Kruithof, N., 101 
Kutz. M.,65 
Langerman, S.. 37 
Ljubic. I..27 
Lladser, M. E.. 183 
Lueker, G. S. 169 
Marot, A.. 37 
Martel, C.,213 
Martinez, C , 234 
Marzban, M., 152 
Matsuura, A.,228 
Milosavljevic, N , 65 
Mitchell, J. S. B., 75 
Mohring, R. H.,64 
Mutzel, P.,27 
257 



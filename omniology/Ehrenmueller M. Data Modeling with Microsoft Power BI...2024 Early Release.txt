
Data Modeling with Microsoft Power
BI
Self-Service and Enterprise Data Warehouses with Power BI
With Early Release ebooks, you get books in their earliest
form—the author’s raw and unedited content as they write
—so you can take advantage of these technologies long
before the official release of these titles.
Markus Ehrenmueller-Jensen

Data Modeling with Microsoft Power
BI
by Markus Ehrenmueller-Jensen
Copyright © 2024 O’Reilly Media. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc.
, 1005 Gravenstein Highway
North, Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or
sales
promotional use. Online editions are also available for
most titles (http://oreilly.com). For more information, contact
our corporate/institutional sales
department: 800-998-9938 or
corporate@oreilly.com.
Acquisitions Editor:
Michelle Smith
Development Editor:
Shira Evans
Production Editor:
Katherine Tozer
Copyeditor:
Liz Wheeler
Proofreader:
FILL IN PROOFREADER

Indexer:
FILL IN INDEXER
Interior Designer:
David Futato
Cover Designer:
Karen Montgomery
Illustrator:
Kate Dullea
October 2024:
First Edition
Revision History for the First Edition
2023-04-14: First Release
2023-06-01: Second Release
2023-07-14: Third Release
2023-09-13: Fourth Release
2023-11-08: Fifth Release
2024-03-12: Sixth Release
See http://oreilly.com/catalog/errata.csp?isbn=9781098148553
for release details.
The O’Reilly logo is a registered trademark of O’Reilly Media,
Inc. Data
Modeling with Microsoft Power BI, the cover image,
and related trade dress
are trademarks of O’Reilly Media, Inc.

The views expressed in this work are those of the author and do
not
represent the publisher’s views. While the publisher and
the
author have used good faith efforts to ensure that the
information and
instructions contained in this work are
accurate, the publisher and the
author disclaim all
responsibility for errors or omissions, including
without
limitation responsibility for damages resulting from the use of
or
reliance on this work. Use of the information and
instructions contained
in this work is at your own risk. If any
code samples or other technology
this work contains or
describes is subject to open source licenses or the
intellectual
property rights of others, it is your responsibility to
ensure that
your use thereof complies with such licenses and/or rights.
978-1-098-14849-2

Dedication
I want to dedicate this book to the best that ever happened to
me: my lovely children Clara (Alex) and Victor.

Preface
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles
Welcome to your journey into data modeling concepts and
practical examples for Power BI, including DAX, Power Query
and T-SQL. This book is your companion to gain a
comprehensive understanding about what steps you need to
follow to make building reports in Power BI Desktop and Power
BI Report Builder and creating measures in DAX easier.
Power BI supports a wide variety of data sources (covering
databases from different vendors, flat files, like CSV, Text or
Excel, webs servicec, etc.). Data sources in Power BI Desktop
contains the current list. The only way getting data into Power
BI is through Power Query. It is best practice to add calculations
as (explicit) measures in DAX (as opposed to calculated columns
in DAX or as columns in Power Query or in the data source).
Creating calculated tables in DAX should be an exception,
depending on your skills and preferences, you will implement

transformations to shape the data model either in Power Query
(in the user interface or by writing code in the M language) or
in the data source, for example in T-SQL in the case of a
relational Data warehouse implemented in Microsoft’s
relational database engines, as laid out in Figure P-1. The first
part of this book will introduce you to the necessary concepts in
a general way, which is written in an agnostic way: You can
apply this to any analytical system. In the second part you will
learn about the properties of a data model in Power BI. The rest
of this book works its way over DAX, Power Query to SQL. The
book is designed for you, the reader, to have an individual
experience, based on your knowledge. You may not know DAX,
Power Query and SQL. But it is up to your choice, to pick and
choose to fill in gaps in your knowledge. Maybe you need a
refresher on which parts a data model consists of? Part I has
you covered. Maybe you struggle with a bunch of Excel files on
which you need to create reports? The part on Power Query
will be your starting point. Maybe your task is to build a data
warehouse to which other people connect with Power BI
Desktop? Then the part about SQL will present you with
solutions to typical problems.

Figure P-1. Power BI data shaping architecture
Data modeling is definitely the single most underestimated task
when working with Power BI Desktop. It is a crucial part in
your steps from data to business intelligence and analytics.
Decisions made during this step are influencing on how much
detail your reports can show, how user-friendly the database or
semantic model is to create reports and analysis, and how easy
it is to add more data and implement calculations on existing
data. Wrong decisions in the start are very expensive to fix
later, as changes to the data model will break existing reports. I,
the author, speak of experience: I had to learn this the hard way
– and I see also other people struggling with that almost every

day in my work as a trainer and consultant. This book is your
guide, to get data modeling right from the beginning.
You will learn that it is less important, how complex the steps
done in the “back end” (DAX, Power Query, or T-SQL) are, as
long as it results in an easier-to-understand and easier-to-use
data model for the user who creates reports and does analytics
based on this data model: report authors, business analysts,
data scientists, etc. These steps can be as simple as changing
technical names (e.g., CSTNM4711 ) into user friendly names
(e.g., Customer Name ) or as complicated as combining or
splitting tables into a whole new structure. You will learn how
to add calculations to the data model and enrich plain data with
meta-data (hierarchies, translations, etc.). This book is full of
practical examples the author faced in more than 25 years of
experience in the field. Keep in mind: The essential goal is to
remove burden from the report-creator.
A common analogy for data transformation is a restaurant. As
the restaurant’s customer you expect the dishes served
attractively arranged on plates so you can enjoy the meal with
only common tools (spoon, knife, fork, chopsticks, or maybe
your fingers). To make this happen, the restaurant not only
reserves significant space and resources in the kitchen, but also
owns expensive tools (convection oven, broiler, sous-vide

cooker, blender, etc.) and employs formally trained and skilled
people (cooks) to control those devices in order to transform the
raw ingredients into the dishes. Think of the people using a data
model to create reports and do analytics as restaurant guests:
they will prefer to have all information presented in an easy-to-
digest (pun intended) form, which they can consume with
common tools (Power BI, Excel, etc.). Set yourself into the role
of a cook who puts all her experience together to create a data
model which makes appetite to consume it.
Are you a data cook? Then read on!

Who Is This Book For?
Are you accessing data in Power BI Desktop and interacting
with it via visuals? Did you gather the most important data into
one Power BI Desktop file so others can build reports on it? Are
you in charge of a data warehouse and want to make sure that
the data model is optimized for usage in Power BI Desktop? If
you answer either of these questions with “Yes, I am!”, then this
book is for you.
The primary audience is the enthusiastic Power
BI report creator who wants to apply best practices in the data
model for performant reports and easy DAX calculations. The
secondary audience is the IT-Pro who wants to support report
creators with a connect-and-go data source (a data model
created in Power BI Desktop). You should be comfortable with
creating reports in Power BI Desktop and have a basic
understanding of at least either DAX, Power Query/M, or SQL,
so you can follow the code examples provided in this book.
Throughout the book you will not only learn about the data
modeling options in Power BI, but also about modeling options
in tools outside of Power BI, to create a data model which is
optimal for Power BI. The whole part about SQL is covering the
latter point of view.

Why is it worth to read (and write!) a whole book about data
modeling? The next section delivers the answer.
What Is Data Modeling?
The challenges of storing data in different logical formats is as
old as data itself. Before electronic computers have been
invented, data was put into different physical files and
structured in physical folders, filling the shelves of big cabinets
or even whole rooms or basements. Optional indexes
(alphabetically ordered small cardboards with important terms
and a reference on which shelf in which folder and file the
information can be found) allowed to scan the data not only by
its physical order, but by different tags. Later this terminology
was kept, and data stored in files and folders on hard disk, with
additional indexes speeding up reading access.
Different approaches were invented and discussed over history
to make for easy storage (e.g., avoiding redundancy and
standardization of physical data storage) and easy read-access
(e.g., indexing and re-introducing of some redundancy to speed
up access to data). The concept of relational databases (e.g., SQL
Server and Azure SQL Database) goes back to the year 1970.

Dimensional modeling is even older – but nevertheless still very
useful today.
To master Power BI you need to master data modeling, because
Power BI is a model-oriented analytics tool (as opposed to some
other tools on the market). The next section gives you an
overview about what parts Power BI is built of and where you
define the data model.
What Is Power BI?
Power BI is not a tool, but a whole suite of tools, which went
into general availability in 2015. In case you are new to Power
BI, I will give you a brief overview of the different tools, which
contain the following:
Power BI Desktop
Power BI Desktop is the full-client with which you achieve
a lot of tasks (s. Figure P-2): You connect to data sources,
clean and transform the data (with Power Query), develop
a data model (in the Model View) and create reports (in
the Report View). This is the tool, with which you will
spend a lot of time when re-creating the examples in this
book. All examples and screenshots in this book covering

Power BI data modeling, DAX, and Power Query are based
on Power BI Desktop. Files created with Power BI Desktop
have the extension .pbix  or .pbip . You need to
download and install Power BI Desktop from
PowerBI.com. Power BI Desktop is kept up-to-data via
monthly releases – make sure to download and install the
MSI file regularly, or install Power BI Desktop from the
Microsoft Store, which will update automatically. Power BI
Desktop is free of charge - you need no sort of license and
you can skip the sign-in step, when the tool asks you.
Figure P-2. The start screen in Power BI Desktop
Power BI Service

The Power BI Service is where you host files you have
created with Power BI Desktop, so that others can
consume a report or create a new report based on your
data model. Power BI Service offers additional features
(like Metric, Dashboard, Power BI App, Analyze in Excel,
Export to Excel, etc.) not available in Power BI Desktop.
Power BI Service is hosted in Microsoft’s cloud data
centers and is available with an internet browser of your
choice via Power BI Service (s. Figure P-3). You can also
edit reports directly in the Power BI Service and Microsoft
is working hard to enable editing data models available in
the Power BI Service as well. At the time of writing this
lacks important features (like version control and
collaborative editing) – so I would currently recommend
relying on Power BI Desktop instead. Power BI Service
comes with different licenses (Licenses and subscriptions
for business users). Find more about Power BI Service at
What is the Power BI service?.

Figure P-3. The Power BI Service says hello
Power BI Report Server
Power BI Report Server (s. Figure P-4) is an alternative to
the Power BI Service, which you can install on your own
premises. Power BI Report Server comes with a limited
feature set (s. Compare Power BI Report Server and the
Power BI service for a comparison to the Power BI
Service) and new versions are released (only) three times
a year. You need to use a matching version of Power BI
Desktop (called: Power BI Desktop for Report Server which
shows the month name and year in the title of the
application) when you intend to publish a Power BI
Desktop report on a Power BI Report Server, as the

monthly released version of Power BI Desktop might
contain artifacts not compatible (yet) with the Power BI
Server you are using.
Figure P-4. The Power BI Server says hello
Power BI Report Builder
When you need to create pixel-perfect reports, with lists
of data covering several pages or you need to export in
file-formats not available for reports created with Power
BI Desktop, then Power BI Report Builder is the tool for
you (s. Figure P-5). Power BI Report Builder is free of
charge. Find more about Power BI Report builder at
Power BI Report Builder.

Figure P-5. Create paginated reports with Power BI Report Builder
NOTE
If you intend to publish paginated reports on a Power BI Report Server (as
opposed to the Power BI Service) you need to use SQL Server Report Builder
instead of Power BI Report Builder.
You cannot create data models in (any version) of Report
Builder.
Analysis Services Tabular
You can think of Analysis Services Tabular (which is
available as Azure Analysis Services and as SQL Server

Analysis Services Tabular) as Power BI Desktop stripped
from the report creation feature and reduced to the table
view, the model view and Power Query. It shares the same
storage engine (VertiPaq) and modeling capabilities as
Power BI Desktop and you can connect any reporting tool
(including Power BI Desktop and Power BI Report
Builder) to Analysis Services to create reports and
analytics. You develop and deploy such a database with
Visual Studio (s. Figure P-6). Some of my customers use
Azure Analysis Service instead of Power BI Service to host
the data because they can scale the costs for data storage
more granular (compared to the Power BI licensing costs);
others use SQL Server Analysis Services because they
cannot or do not want to host their data in the cloud.
Microsoft’s vision is to make Power BI a super-set of
Analysis Services Tabular; and with the announcement of
Fabric (Microsoft Fabric) at the Build conference in May
2023 Microsoft made a big step forward to it. Fabric will
also allow to scale costs more granular, compared to
Power BI’s licensing.

Figure P-6. Analysis Services Tabular database definition in Visual Studio
Let’s talk about, why you should care so much about the data
model you build in Power BI Desktop.
What Is So Special About a Power BI
Data Model?
Power BI is very versatile, when it comes to the possible shapes
of data models you can use (= how you spread information
between the tables or if you combine everything into a single
table). But don’t be seduced and step into a trap here: the
storage engine behind Power BI (called VertiPaq) and the

language to define formulas for measure (DAX) are optimized
for a certain shape, called a Star schema. Maybe you are
wondering what a Star schema is and how this is different from
other data modeling approaches. Rest, assured, this book is
your guide to understand what a Star schema is, why it is so
important to take the time to transform the table(s) of your data
source(s) into a Star schema and, most importantly, how to
actually implement these steps.
In a nutshell, Star schema is a term used for a data model,
where you have a fact table in the middle (forming the center of
the star) surrounded by dimension tables (Figure P-7). I know,
you need a lot of imagination to see such a Star - but still, this
concept is very important and useful, when building analytical
systems in general, and especially when it comes to Power BI.

Figure P-7. A Star Schema
In a perfect world, the tables in the data source are already in
the shape of a Star schema. If this is true for all your data
sources, then I consider you as really lucky – and you can stop
reading this book and ask for a refund. The mere mortals, who
are not as lucky, have several options, which I discuss in this
book. You can build a data warehouse layer (e.g., in form of a
relational database or as a lakehouse). The SQL language will be
the tool of your choice. Or you can use Power BI’s Power Query
to reshape the tables. The third option is DAX, which is
discussed in the next section.

What Is DAX?
Data Analysis Expressions (DAX) are used to create calculated
columns, measures, calculated tables, and row level security
and to write queries. As you will learn in this book, it is
important to master DAX, as certain types of calculations can
only be done in DAX (and not Power BI’s data source or in
Power Query).
In order to reshape any data model into a Star schema you can
move information from one table into another via calculated
columns or create new tables as calculated tables. You will also
learn how to use DAX in cases where you hit limits of Power
BI’s data modeling capabilities.
Overall, DAX is not my first choice as a data-shaping tool if I can
solve a problem with Power Query. Whenever possible I would
push transformations into Power Query or, if available, into the
data warehouse layer. The reason is that it is best practice to
push transformations as far “upstream” in the data processing
pipeline as possible, to increase re-usability of a
transformation. A transformation in Power Query can be
pushed into a Power BI dataflow to re-use it in different data
models. Transformation in DAX are tight to the data model they

reside in. Another reason is that in Power Query and SQL I can
shape the data before I actually load it into Power BI, while in
DAX I can only add calculated columns and tables on top of a
model; any sub-optimal part still occupies resources in my data
model.
If you don’t feel ready for Power Query (or the M language) or if
you don’t have a data warehouse at hand, then modeling the
data in DAX is way better than not modeling your data at all.
And remember, some problems can only be solved with a DAX
measure.
What Is Power Query?
As the name suggests, Power Query is a tool to create queries
you send against your data source(s). At the time of writing
Power Query has connectors to over 120 data sources, from flat
files like CSV, XLS, or JSON to relational (SQL Server, Oracle,
DB2, Teradata, …​) and analytical databases (Analysis Service).
Via a graphical user interface you can clean and transform the
original tables: renaming tables and columns to give them a
more user-friendly name, removing unnecessary columns and
adding new tables and columns based on the existing
information to shape a better data model.

All steps you apply in the graphical user interface are added as
lines of code to a Power Query script (short: M), similar to the
macro-recorder in Excel. Through the course of this book, you
will learn how to use the graphical user interface and when to
edit the resulted query. Any time you refresh the content of a
table, this script is executed and therefore all transformations
steps are applied to the new data as well. That’s why I ask
people to hand me over “raw” CSV or Excel files (in the case that
files are the preferred data source). There is no need that they
put effort in shaping the content manually every time before
the send me the new data, if I can implement transformations
once and re-apply them during the refresh of my data model.
Power Query (and therefore scripts in its mashup language M)
is not only available in Power BI Desktop, but as Power BI
dataflows and mashup task in Azure Data Factory as well.
But maybe, you want (and if you ask me, you should) push
transformation further up the data stream. That’s what all the
fuzz in this book about Azure SQL DB and T-SQL is.
What Is SQL?

SQL is an acronym for Structured Query Language on the one
hand, and Microsoft’s offering of relational databases on the
other hand: Azure SQL DB is Microsoft’s cloud offering for
relational databases, which is available for on-premises’ usage
under the name Microsoft SQL Server Database Engine.
In this book, Azure SQL DB is used as an example of how to
model your data outside of Power BI. I still consider this as best
practice: model your data in a database and model your data as
early as possible, or in other words: in a data warehouse layer.
It is less important, if this layer is physically hosted in a
relational database or in a data lake(house) or any other data
store, just somewhere in Microsoft’s Fabric.
The reason is, pushing the transformations as far as possible
towards the data source makes it easier for tools and people
down the data stream to re-use it. If you wait until the last
moment (this is: the tool you use to show your data, e.g., Power
BI Desktop, Power BI Report Builder, Excel or any other tool
your end-users might use to access the data and do their
analytics) then you have cumulated a technical debt all the way.
The poor end-users are then responsible for cleaning the data
and bringing it into a useful shape. Due to lack of education,
they might fail – leading to over-complicated reports and
possibly wrong numbers (you will learn about this problem in

Chapter 5). On top of that, all the effort is then re-done in or
copied over into the next file – adding the problem of
duplicated versions of these overcomplicated reports. Soon, you
could end up in so-called Excel-hell where nobody has a
complete picture of the different versions of logics applied to
the data. On the opposite, a data warehouse layer guarantees a
single version of the truth.
The book’s part about SQL will concentrate on solutions built
with SELECT statements and it’s procedural extension T-SQL. I
will use the SQL dialect available in Azure SQL DB and SQL
Server Database Engine. There is some chance that simple
SELECT statements provided in this book will also run on other
relational databases, or even NO-SQL databases. There is almost
no chance that the procedural extensions (loops, functions,
procedures, etc.) will work without any change on other
database management system. In this case you have to find a
way to migrate the code to your destination system, if you are
not using Microsoft’s SQL-based databases.
T-SQL as a language is quite stable in terms of how rare new
extensions are made, or what part are becoming deprecated.
Power BI is a different beast - new versions are released every
month.

A New Release Every Few Weeks
The team behind Power BI and its tools and services at
Microsoft is very busy delivering new versions of Power BI
Desktop every month and rolling out changes in the cloud-
based services weekly. This is a challenge for everybody: the
user interface changes; icons are redesigned, and buttons
moved to different places. This is also a challenge for every
book project: Most probably some of the screenshots are
outdated when you read this book. Therefore, I included only
portions of the screen, when sufficient. On many places I also
included a link to Microsoft’s official documentation – which is
kept up-to-date by Microsoft and the community and you can
use to double-check in case “your” Power BI Desktop looks
different then “mine”.
The general concepts on how to create an optimal data model
for Power BI did not change in the past, and therefore, there is
hope that this knowledge is here to stay and will help you in the
future as well. Read on, to learn how I divided all the necessary
knowledge and skills into digestable portions.
How to Read This Book

The book is organized in five parts. Basically, you have got five
books for the price of one:
Data Modeling 101 (Part I)
The first part gently introduces all the theory and
concepts and teaches you why data modeling matters. It
covers all the content in a practical, but tool-agnostic way.
Look at this part as a “readers-digest” version of Ralph
Kimball’s (The Kimball Group) and Christopher
Adamson’s Star Schema: The Complete Reference.
Practical Examples in Power BI/AAS data models (Part II)
The second part covers data modeling features of Power
BI Desktop, where I will show you how to apply all the
theory (from the first part) in concrete examples. I will
guide you around the menu and ribbon and into the data
model’s properties.
Practical Examples in DAX (Part III)
This part shows how you can bring the information from
the data source(s) into the necessary shape (discussed in
the part about Power BI) with the help of DAX. The
advantage of using DAX is that you need to learn DAX
anyway, when you want to master Power BI (as there is no

way around writing calculated measures in DAX for
anything but very simple data models). The advantage is
also that changes in the formula will be calculated
immediately after you press OK on the keyboard, without
accessing the data source. The disadvantage is that the
result of calculated columns and calculated tables will
occupy disk space, when you save the file, and memory,
when you open the file. Another disadvantage of using
DAX to re-shape a data model is that your data model will
contain both, the original shape and the re-shaped version
of the data model (and therfore occupying an unnecessary
amount of space). DAX’s main purpose is to define
measures and not data shaping - some of the examples
will still be insightful about what you can achieve with
DAX. Learning the full capabilities of the DAX language
will also help you in cases where you need to reach out to
complex DAX measures, when you for some reason are
not able to solve the problem in the model itself.
Practical Examples in Power Query (Part IV)
This part shows how you can bring the information from
the data source(s) into the necessary shape (discussed in
the part about Power BI) with the help of Power Query’s
user interface and the M language. You will see that you

can come a very long way with the user interface alone,
before you need to handle code in M. Unfortunately, for
any learner, the M language is very different from the
DAX language (e.g., in M you apply transformations based
on the the result of a previous step, while in DAX you M is
case sensitive for the keywords and the content of
columns, while DAX is case insensitive; M is similar to F#,
while DAX is similar to Excel formulas) - they can hardly
be compared to each other. As you will see in this part, M
as a language is much better suited for the task of data
shaping, than DAX is. In cases where I cannot bring the
data model into the right shape already in the data source,
Power Query and the M language are the tools of my
choice. Only the final result of transformations made in
Power Query are loaded into the Power BI data model -
avoiding unnecessary tables and columns. While working
with Power Query you need active access to the data
source.
Practical Examples in T-SQL (Part V)
This part shows how you can bring the information from
the data source(s) into the necessary shape (discussed in
the part about Power BI) with the help of SQL and T-SQL.
Despite trends to gather data in lakes I still have the

strong opinion that an enterprise organisation needs a
central place where someone takes care about the data. A
place where natural (and dirty) data is cleaned and
brought into the right shape. If this place is a physical
database or “just” views on some data store is not so
important. But it is important that you have such a (data
warehouse) layer instead of putting the burden onto the
end-user and the tools she uses. If you are eager to learn
about relational databases or if you are in charge of the
data warehouse layer in your organisation and want to
make the life of Power BI users easier, then this part of the
book is especially for you. In Microsoft Fabric you can
access your data residing in a Data Lake
Each part is divided into four chapters, covering the following
topics:
Understanding a data model
These chapters are about the basic terms and concepts. If
you are new to data modeling, make sure that you read
through this chapter and understand the content.
Otherwise, it might be hard to follow the rest of the book.
If you already have some understanding in data
modeling, you might quickly check the chapter for terms

you might have forgotten and give yourself a refresh.
Look out for the key takeaways at the end of each chapter.
Building a data model
These are all about the meat-and-potatoes in data
modeling. It discusses problems and solutions to very
common problems. I am convinced that you will face at
least some of those on your journey of refining your data
into usable information.
Use cases
For these chapters I called out for advanced challenges.
For simple data models none of the problems discussed
here might be an issue. You might enjoy a break from
reading this book here, and first return later, when you
are really facing one of the issues. These chapters are
diving deeper into data modeling, DAX, Power Query, and
SQL and reach out to not-so-common features to solve a
certain problem. These chapters are aimed for the
advanced data cook.
Performance Tuning
Every part closes with a chapter on performance tuning.
It is most often the last step in the whole journey of data

modeling, as well. If you follow all best practices pointed
out in this book you will come a far way, before report
performance will be an issue. For really large
implementations (I am talking about billions of rows of
data) even following these best practices will not be
enough. Then it’s time to dive into the technical layer of
data modeling and learn how to tune the available storage
modes (Import, DirectQuery, Live connection, and
DirectLake) to your advantage.
NOTE
The whole book, and therefore also the chapters about performance tuning
concentrates on data modeling and not so much on how to write performant
code in DAX, Power Query or SQL.
All the parts and topics are brought together in 20 chapters, as
shown in Figure P-8.

Figure P-8. Chapter overview
The order of the chapters is chosen in a way that the level of
complexity/difficulty increases with each chapter. This allows
you to read the book cover-to-cover. On the other hand, it allows
you to easily jump over DAX or ignore SQL, for example, if you
intend to use Power Query to solve your challenges, instead of
scanning inside a chapter for the portions relevant to you.
For example, in the first part you will find the general thinking
of possible solutions to assign values into buckets (“binning”) in
“Binning” (Chapter 3). In “Binning” (Chapter 7) this solution is
brought into the world of Power BI, with a description of the
necessary tables, all the settings and demonstrated with
screenshots. To learn, how you can create the necessary tables
(and/or measures) you lookup “Binning” (Chapter 11), “Binning”

(Chapter 15) and/or “Binning” (Chapter 19), depending on with
which language you want to implement the solution (see
Figure P-9).
Figure P-9. Example navigation
Ready to start with the examples? First you need to make sure
that you have all necessary software installed on your device.
Installing Necessary Software

You need to install the following software in order to open the
provided demo-files and replicate exercises described in this
book:
Power BI Desktop
To open the PBIX-files you need to either install the
Windows Store version from Power BI Desktop Store or
download the installation files from Power BI Desktop
Installer.
SQL Client
For the exercises in SQL I used SQL Server Management
Studio (SSMS) which you can download and install from
Download SQL Server Management Studio (SSMS).
Alternatively you can use Azure Data Studio, which you
get from Download and install Azure Data Studio, or you
use your preferred SQL client.
SQL Server or Azure SQL DB
For the exercises in SQL you need access to one of the
relational databases available from Microsoft: either SQL
Server (installed on your premises) or Azure SQL DB
(software-as-a-services in Microsoft’s cloud).

You can download SQL Server from Try SQL Server on-
premises or in the cloud. Use either the Express Edition or
the Developer Edition. Both are free of charge and
sufficient for the exercises.
Alternatively, you can sign-up for an Azure SQL DB. For
the exercises a free trial-access will be sufficient. Register
here: Create a managed cloud database with your Azure
free account.
Write-access to database “AdventureWorksDW”
Most of the examples are based on a data warehouse
schema of the fictitious company “Adventure Works”, a
sport retailer, which earns the majority of its revenue
through selling bikes on three continents either via
resellers or directly over its web shop. To create the
necessary database objects (tables, views, procedures,
functions, schemas), you need write-access to the
database “AdventureWorksDW” (the one with the “DW” in
the postfix of the name; not: “AdventureWorks” or
“AdventureWorksLT” or “Adventure Works OLTP”). You
find the backup-file and a description of how to install it
on SQL Server here: AdventureWorks sample databases.

To install database “AdventureWorksDW” as an Azure SQL
DB, you can use the “Data-tier Application” (also: BACPAC
file), available at AdventureWorksDW.bacpac.
Install this BACPAC file via SQL Server Management
Studio (SSMS). Right-click Databases and chose Import
Data-tier Application, as shown in Figure P-10.
Figure P-10. Import Data-tier Application in SQL Server management Studio
Provide the name of the folder and the file, where you
downloaded it Figure P-11.

Figure P-11. Provide the name of the folder and file.
The smallest/cheapest edition will be sufficient (Figure P-
12).

Figure P-12. Choose the smallest available database setting.
If you chose to run the database serverless, you can
further reduce costs, as the database (and all costs) will
hibernate, if you do not use it for 1h. Next time you access
the database, it will be woken up for you, which can lead
to a time-out. Just re-connect at once again, and the
connection will be established successfully.

Finally run the script 001 Preparation.sql  when
connected to database AdventureWorksDW to create all
artifacts for the demo environment.
Installing Necessary Software
The data community around Power BI is great. I really admire
such smart people who combine a deep knowledge of the
technology behind Power BI with understanding the needs of
professional Power BI developers and the skillset to develop
useful tools. Some of those people even develop their tools open
source and share it with the community. There are many such
tools out there. In this book I will refer to two of them:
Tabular Editor V2 (Tabular Editor), the open-source version
of Tabular Editor V3, by Daniel Otykier (CTO at Tabular
Editor ApS)
DAX Studio (Download DAX Studio), an open-source tool by
Darren Gosbell (Senior Program Manager at Microsoft)
Demo Files
For every problem (= section within the chapters) I created one
single Power BI Desktop file (“.pbix”), which contains the whole

data model and all the solution in DAX and in Power Query / M.
Also a connection to the tables in Azure SQL. This allows you to
easily compare the different technical solutions with each other.
The majority of the examples is of eductional purpose: For
instance, in file Data multiple.pbix  you will find the table
Order Date  three times with the exact same content. Order
Date (DAX)  is a version completly created in DAX, Order
Date (PQ)  is a version completly created in Power Query, and
Order Date (SQL) , as you might guess, is a version completly
created in SQL and just loaded as-is into Power BI. To avoid any
misconceptions: In a practical scenario it does neither make
sense to create a table with the same content several times in
your data model, nor does it make sense to do some
transformations in DAX, while other in Power Query and/or
SQL. Stick to one tool, to make it easier to find any
transformation.
You can find all files at Repository for “Data Modeling with
Power BI Desktop”. Here is the list of all the files, ordered by
appearance in the book:
Understanding a data model
Relationship.pbix, Auto-Exist.pbix

103 SET and JOINS.sql, 104 ETL.sql
Building a data model
Normalize Facts DAX.pbix, Normalize Facts
PQ.pbix, Denormalize Dimensions DAX.pbix,
Denormalize Dimensions PQ.pbix, Financials
Dimensional Model Surrogate Key.pbix,
Financials Dimensional Model.pbix, Financials
Filter Dimension Surrogate Key Measures.pbix,
Financials Filter Dimension Surrogate
Key.pbix, Financials Filter Dimension.pbix,
Financials OBT Measures.pbix, Financials
OBT.pbix, Flag.pbix, Auto-date.pbix, Auto-
exist.pbix, Date.pbix, Date Role-playing.pbix,
Date Multiple.pbix, Slowly Changing
Dimensions.pbix, Hierarchies.pbix
201 Normalizing.sql, 202 Denormalizing.sql,
203 Calculations.sql, 204 Flags and
Indicators.sql, 205 Time and Date.sql, 206
Role Playing Dimensions.sql, 207 Slowly
Changing Dimensions.sql, 208 Hierarchies.sql
Real World Examples

Binning.pbix, Budget TREATAS.pbix, Budget
BRIDGE.pbix, Multi-language.pbix, Key
Value.pbix, Composite Model.pbix
301 Binning.sql, 302 Budget.sql, 303
Localized model.sql, 304 Key Value.sql
Performance tuning
Performance tuning.pbix
401 Data vs Query.sql, 402 Aggregated
Facts.sql, 403 Partitioning.sql
Now you are set for the first chapter, which will give you a basic
understanding what a data model is and what it consists of.
Conventions Used in This Book
The following typographical conventions are used in this book:
Italic
Indicates new terms, URLs, email addresses, filenames,
and file extensions.
Constant width

Used for program listings, as well as within paragraphs to
refer to program elements such as variable or function
names, databases, data types, environment variables,
statements, and keywords.
Constant width bold
Shows commands or other text that should be typed
literally by the user.
Constant width italic
Shows text that should be replaced with user-supplied
values or by values determined by context.
TIP
This element signifies a tip or suggestion.
NOTE
This element signifies a general note.
WARNING
This element indicates a warning or caution.

Using Code Examples
Supplemental material (code examples, exercises, etc.) is
available for download at
https://github.com/oreillymedia/title_title.
If you have a technical question or a problem using the code
examples, please send email to bookquestions@oreilly.com.
This book is here to help you get your job done. In general, if
example code is offered with this book, you may use it in your
programs and documentation. You do not need to contact us for
permission unless you’re reproducing a significant portion of
the code. For example, writing a program that uses several
chunks of code from this book does not require permission.
Selling or distributing examples from O’Reilly books does
require permission. Answering a question by citing this book
and quoting example code does not require permission.
Incorporating a significant amount of example code from this
book into your product’s documentation does require
permission.
We appreciate, but generally do not require, attribution. An
attribution usually includes the title, author, publisher, and

ISBN. For example: “Book Title by Some Author (O’Reilly).
Copyright 2012 Some Copyright Holder, 978-0-596-xxxx-x.”
If you feel your use of code examples falls outside fair use or
the permission given above, feel free to contact us at
permissions@oreilly.com.
O’Reilly Online Learning
NOTE
For more than 40 years, O’Reilly Media has provided technology and business
training, knowledge, and insight to help companies succeed.
Our unique network of experts and innovators share their
knowledge and expertise through books, articles, and our
online learning platform. O’Reilly’s online learning platform
gives you on-demand access to live training courses, in-depth
learning paths, interactive coding environments, and a vast
collection of text and video from O’Reilly and 200+ other
publishers. For more information, visit https://oreilly.com.
How to Contact Us

Please address comments and questions concerning this book to
the publisher:
O’Reilly Media, Inc.
1005 Gravenstein Highway North
Sebastopol, CA 95472
800-889-8969 (in the United States or Canada)
707-827-7019 (international or local)
707-829-0104 (fax)
support@oreilly.com
https://www.oreilly.com/about/contact.html
We have a web page for this book, where we list errata,
examples, and any additional information. You can access this
page at https://www.oreilly.com/catalog/catalog_page.
For news and information about our books and courses, visit
https://oreilly.com.
Find us on LinkedIn: https://linkedin.com/company/oreilly-media.
Watch us on YouTube: https://youtube.com/oreillymedia.

Acknowledgments
Writing a book looks like a lonely work in the beginning:
Writing down a concept and draft, going through my own
material and doing research, developing and improving the
examples, creating the screenshots and diagrams, transforming
the thoughts and ideas into words, etc. But it turns out fast that
it is a collective teamwork with very engaged and motivated
people I want to bring in front of the curtain here:
Michelle Smith as the responsible acquisition editor who
challenged my book proposal and helped it becoming a useful
draft to start from and Shira Evans as the responsible
development editor. I had countless hours of fruitful discussions
with Shira where she challenged the structure of the book,
made sure I aligned with the publishers guidelines, asked the
right questions to make the book better readable and kept me
motivated during the whole process until publication with her
knowledge, experience and happy mood.
I am also very happy that I could convince renowned top
experts Shabnam Watson, Matt Allington, and Nikola Ilic as
technical reviewers for this project. I appreciate the time, they

dedicated to this book. Their comments and findings made the
book so much better.
Furthermore, I want to thank all my clients who keep me busy
in projects and attendees of my seminars, workshops and
webinars through the past years: Solving challenging problems,
preparing material and answering questions lead finally to the
idea of bringing everything together for this book.

Part I. Data Modeling 101
Figure I-1. Example navigation
The goal of this first part of the book is to bring everybody onto
the same page when it comes to the topic of data modeling. The
chapters in this part are written agnosticly: The content,
problems and solutions are not tight to any specific type of
database management system. You will be able to apply the
knowledge you gain from this part to any relational or
analytical database. This includes, of coures, Power BI, Analysis
Services Tabular und Azure SQL DB, but is not limited to them
and you can apply all statements, information and conclusion
to a database management system from other vendors than
Microsoft, to classical cubes, to data lakehouses, and so on.

These concepts already exist since decades and are so mature
that I bet they will be around for decades to come. Make sure to
learn of all them - so you gain the understanding, why I insist
on applying one transformation or another, when it comes to
data modeling in Power BI in the later parts of this book.
In Chapter 1 will introduce you to the basic terms and concepts:
Entities and Tables
Relations and their Cardinality
Primary and Foreign Keys
You will learn, how you can combine information spread out
into different tables with the help of Set Operators and Join
Operators, including problems that you can face, which could
resulat in missing data or duplication of data.
I will talk about the three core possibilites of modeling your
data (combining everything into one single table, splitting the
information in a way which avoids duplicates under all
circumstances, and a compromise in the form of a so-called
Dimensional Model) and you will learn to decide to use which
under what conditions.
Based on the learnings of Chapter 1, Chapter 2 will teach you
how to transform the data model of your data source into a

data model of the intended shape. This can be done via
transformation steps like:
Normalizing and denormalizing tables
Adding calculations
Transforming flags and indicators into meaningful text
Adding a dedicated table to contain all dates and/or
timestamps
Modeling dimensions, which can play different roles (e.g., a
person can be in the role of an employee or in the role of a
customer)
Modeling dimensions, where we need to track changes of
their attributes over time
Implementing hierarchies
In my experience, the challenges presented in Chapter 2 are
very common in most data models. Chapter 3 will talk about
rarer challenges, which you will not see in every data model. All
of them are real-world problems I had to tackle for my
customers. For their solutions you will need to combine
different techniques, which makes them more advanced.
Finally in this part’s Chapter 4 I will you introduce to what role
a data model plays, when it comes to guarantee a fast
performance of the reports and queries based on the data

model. Basically you need to decided, if you want to persist data
in the shape you need it for your analytics or only store the
code for a query, which transforms the original data into the
necessery shape on-the-fly, instead.

Chapter 1. What Is a Data Model?
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 1st chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
This chapter will teach you (or refresh your memory on) the
basics of data modeling. It starts with the very basic terms, but
please bear with me, because it will help you to understand the
reasoning behind why you should take so much care of the data
model. Creating reports and doing analysis based on a data
model which is optimized for these tasks is much easier, than
compared to trying this with a data model optimized for other

purposes (for example, a data model optimized to store data for
an application or data collected in a spreadsheet). When it
comes to analytical databases and data warehouses, you have
more than one option. Throughout this book you will learn that
the goal is to create the data model as a Star Schema. At the end
of this chapter, you will know which characteristics a Star
Schema will differentiate it from other modeling approaches.
With each and every chapter throughout this book you will
learn more and more why a Star Schema is so important when
it comes to analytical databases in general and Power BI and
Analysis Services Tabular in particular. And I will teach you,
how you can transform any data model into a Star Schema.
Transforming the information of your data source(s) into a Star
Schema is usually not an easy task. On the opposite: It can be
hard. It might take several iterations. It might take discussions
with the people who you build the data model for. And the
people using the reports. And the people who are responsible
for the data sources. You might face doubts (from others and
yourself) and might ask yourself if it is really worth all the
effort instead of simply pulling the data in as it is, to avoid all
the struggle. At such a point it is very important that you take a
deep breath and evaluate, if a transformation would make the
report creator’s life easier. Because, if it will make the report
creator’s life easier, than it’s worth the effort. Therefore, repeat

the data modelers mantra together with me: Make the report
creators life easier.
Before I actually come to talk about transformations, I will to
introduce you to the basic terms and concepts in this chapter:
What is a data model at all?
What is an entity? What has an entity to do with a table?
Why should you care about relations?
I will then introduce the concept of different keys (primary
key, foreign key, surrogate key) and explain the general
meaning of cardinality.
Next topic is how to combine tables (set operators and
joins).
And finally, I will compare the different data modeling
options and their use cases.
First stop in our journey towards the Star Schema is describing
what a Data Model is in general terms.
Data Model
A model is something which represents the real-world. It is not
replicating the real world. Think of a map. A map replicating 1:1
the real world would be very unpractical: It would cover the

whole planet. Instead, a map scales down the distances. And a
map is created for a special purpose. A hiking map will contain
different information (and omit others) compared to a road
map. And a nautical chart would look completely different. All
of them are maps – but with different purposes.
The same applies to a data model. A data model represents a
certain business logic. As with the map example, a data model
for different use cases will look differently. Therefore, models
for different industries will not be the same. And even
organizations within the same industry will need different data
models (even for basically the identical business processes), as
they will concentrate on different requirements. Throughout
the book you will see several challenges and their solutions and
I am confident that they will help you in overcoming the
challenges you will face when building a data model for your
organization.
So, the bad news are: There is not one data model, which rules
them all. And: With just technical knowledge, but no domain-
knowledge, it is impossible to create a useful data model. But
the good news are: This book will guide you through the
technical knowledge necessary to successfully build data
models for Power BI and/or Analysis Services Tabular. But don’t
forget to collect all requirements from the business (or

explicitly write them down, if you are the domain expert on
your own) before you start with creating the data model. You
can collect the requirements by writing down sentences in
natural language, like “We sell goods to customers and need to
know the day and the SKU of the product we sold.” or “We need
to analyze the 12 month rolling average of the sold quantity for
each product.” or “Up to ten employees form a project team and
we need to report the working hours per project task.” These
requirements will help you to know which information you
need to store in which combinations and in which level of
detail. There might be more than one option for the design of a
data model for a certain use case.
And very importantly: You need the create the correct data
model right from the beginning. As soon as the first reports
created on a data model, every change in the data model bears
the risk of breaking those reports. The later you discover
inconsistences and mistakes in your data model, the more
expensive it will be to correct them (in terms of time to check
and correct all depending reports). This cost hits everybody
who created those reports: Yourself, but also other users who
built reports based upon your data model.
The design of the data model has a huge impact on the
performance of your reports, which query the data from the

data model to feed the visualizations, as well. A well-designed
data model lessens the need for query tuning later. And a well-
designed data model can be used intuitively by the report
creators - saving them time and effort (and therefore saving
your organization money). In a different point of view:
Problems with the performance of a report or report-creators
unsure of which tables and columns they need to use for to gain
certain insights are a sure sign for a data model which can be
improved by a better choice of design.
Later, in section “Entity-Relationship Diagrams” I will describe
graphical ways of how to document the shape of a data model.
Immediately in the next section, you will learn that a data
model consists of entities.
Basic Components
Before you dive in, there are a few key components of a data
model that you will need to understand. In the next section, I
will explain the basic parts of a data model. Later I will walk
you through different ways of combining tables with the help of
set operators and joins, and which kind of problems you can
face and how to solve them. Remember, this part of the book is
not specific to Power BI - some concepts may only apply when

you prepare data before you even connect Power Query to it
(e.g., when writing a SQL statement in a relational database).
Entity
An entity is someone or something which can be individually
identified. In natural language entities are nouns. Think of a
real person (your favorite teacher, for example), a product you
bought recently (ice cream, anybody?) or a term (e.g., “entity”).
Entities can be both, real and fictious. And most entities have
attributes: a name, a value, a category, point in time of creation,
etc. These attributes are the information we are after. These
attributes are displayed in reports to help the reader of the
reports to provide context, gain insights, and get to decisions.
They are used to filter displayed information to narrow down
an analysis, too.
Maybe you wonder, how such entities make it into a data
model? They are stored in tables, as you find out in the next
section.
Tables

Tables are the base of a data model. They are part of data
models at least since around 1970, when Edgar F. Codd
developed the relational data model for his employer, IBM. But I
am convinced that collecting information as lists or tables was
already done way before the invention of computers, as you
can see when looking old books. Tables are hosting entities:
Every entity is represented by a row in a table. Every attribute
of an entity is represented by a column in a table. A column in a
table has a name (e.g., birthday) and a data type (e.g., date). All
rows of a single column must conform to this data type (it is not
possible to store the place of birth in the column birthday for
any row). This is an important difference between a (database’s)
table and a spreadsheet’s worksheet (e.g., in Excel). A single
column in a single table contains content.

Table 1-1. A table containing the name of
doctors
Doctor’s Name
Hire Date
Smith
1993-06-03
Grey
2005-03-27
Young
2004-12-01
Stevens
2000-06-07
Karev
1998-09-19
O’Malley
2003-02-14
Entities do not exist just on their own, but are related to each
other. This is what I describe in the upcoming section.
Relationships
Relationships connect (in most cases) only two entities. In
natural language the relationship is represented by a verb (e.g.,
bought). Between the same two entities there might exist more
than one single relationship. For example, a customer might
have first ordered a certain product, which we later shipped.

It’s the same customer and the same product, but different
relations (ordered vs. shipped).
Some relationships can be self-referencing. That means that
there can be a relationship between one entity (= one row in
this table) and another entity of the same type (= a different
row in the same table). Organizational hierarchies a are a
typical example here. Every employee (except maybe the CEO)
needs to report to her boss. The reference to the boss is
therefore an attribute. One column contains the identifier of the
employee (e.g., [Employee ID] ) and a different column
contains the identifier of who this employee reports to (e.g.,
[Manager ID] ). The content of [Manager ID]  of one row
can be found as the [Employee ID]  of a different row in the
same table.
Examples for relations expressed in natural language:
Dr. Smith treats Mr. Jones
Michael attended (course) “Data Modeling”
Mr. Gates owns Microsoft
Mr. Nadella is CEO
When you start collecting the requirements for a report (and
therefore for the necessary content of your data model) it

makes sense to write everything down in sentences in natural
language, as in the examples here. This is the first step. Later
you will learn that you can also draw tables and their
relationships as an “Entity-Relationship Diagrams”.
Sometimes the existence of a relationships alone is enough
information to collect and satisfy analysis. But some
relationships might have attributes, which we collect for more
in-depts analysis:
Dr. Smith treats Mr. Jones against flue
Michael attended (course) “Data Modeling” with grade A
Mr. Gates owns Microsoft to 50%
Mr. Nadella is CEO since February 4 2014
You learned that entities are represented as rows in tables. The
question remains, how you can then connect these rows with
each other to represent their relationship. The first step to the
solution is, to find a (combination of) columns, which will
uniquely identify a row. Such a unique identifier is called a
Primary Key and you will learn all about it in the next section.
Primary Keys

Both, in the real world and in a data model, it is important that
you can uniquely identify a certain entity (= row in a table).
People, for example, are identified via their names in the real
world. When you know two people of the same (first) name, you
might add something to their name (e.g., their last name) or
invent a call name (which is usually shorter than the first name
and last name combined), so that you can make clear, who you
are referring to (but don’t spend too much time). If somebody
doesn’t pay attention, they might end up with a confusing
conversation, where one is referring to one person and the
other to a different person (“Ah, you are talking about the other
John!”).
In a table it is very similar: You can mark one column (or a
combined set of columns, which is called a composite key) as the
primary key of the table. If you do not do that, you might end
up with confusing reports because of duplicates (as the
combined sales of both Johns are shown for every John). Best
practice is, to have a single column as a primary key (as
opposed to composite keys) for the same reason I use call
names for people: because it is shorter and therefore easier to
use. You can only define one single primary key.
Explicitly defining a primary key (short: PK) on a table has
several consequences: It puts a unique constraint on the

column(s) (which guarantees that no other row can have the
same value(s) in the primary key; rejecting both, inserts and
updates, which would violate this rule). Every relational
database management system I know, also puts an index on the
primary key (to speed up the lookup for, if an insert or update
would violate the primary key). And all columns used as a
primary key must contain a value (nullability is disabled). I
strongly believe that you should have a primary key constraint
on every table, ot avoid ending up with duplicate rows.
In a table you can make sure that every row is uniquely
identifiable, by marking one or the combinations of several
rows as the primary key. To make the example with [Employee
ID]  and [Manager ID]  work, it is crucial that the content of
column [Employee ID]  is unique for the whole table.
Typically, in a data warehouse (= a database built for the sole
purpose of making reporting easier) you would not use one of
the columns of the source system as a primary key (e.g., first
name & last name or social security number), but introduce a
new artificial ID, which only exists inside the data warehouse: a
surrogate key, which is explained in the very next section.
Surrogate Keys

A Surrogate Key is an artificial value, only created in the data
warehouse or analytical database. It is neither an entity’s
natural key nor the business key of the source system, and
definitely not a composite key (but a single value). It is created
solely for the purpose of having one single key column, which is
independent of any source system. Think of it as a (bit weird
and secret) call name, which identifies an entity uniquely and
will only be used to join tables, but never used to filter the
content of a table. The Surrogate Key is never exposed to the
report consumers and users of the analytical system.
Typically, the columns have “Key”, “ID”, “SID”, etc. as part of
their names (like ProductKey or Customer_ID or just SID). The
common relational database management systems are able to
automatically find a value for this column for you. Best practice
is to use an integer value, starting at 1. Depending on the
number of rows you expect inside the table, you should find the
appropriate type of integer, which usually can cover something
between 1 Byte (= 8 bit = 2  = 256 values) and 8 Bytes (= 8 * 8 bits
= 64 Bits = 2
 = 18 446 744 073 709 551 616 values).
Sometimes
global unique identifiers are used. They have their use case in
scale-out scenarios, where processes need to run independently
from each other, but still generate surrogate keys for a common
table. They require more space to store (16 bytes), compared to
8
64

an integer value (max. 8 bytes). That’s why I would only use
them in cases when integer values can absolutely not be used.
The goal is to make the data warehouse independent from
changes in the source system (e.g., when an ERP system is
changed, when the ERP system re-uses IDs for new entities or
when the data type of the ERP’s business key changes).
Surrogate Keys are also necessary, when you want to implement
Slowly Changing Dimension Type 2, which I will talk about in
“Slowly Changing Dimensions”.
I still did not explain, how you can represent relations of
entities. In the next section it is time for that. The solution to
relate entities to each other is not very complicated: You just
store the Primary Key of an entity as column in an entity who
has a relationship to it. This way you reference another entity.
You reference the Primary Key of a foreign entity. That’s why
this column is called a Foreign Key in the referencing table, as
laid out in the next section.
Foreign Keys
Foreign keys are simply referencing a primary key (of a foreign
entity). The primary key is hosted in a different column, either

in a different table, or the same table. For example, the sales
table will contain a column [Product ID]  to identify, which
product was sold. The [Product ID]  is the primary key of the
Product  table. On the other hand, the [Manger ID]  column
of the Employee  table refers to column [Employee ID]  in
the very same table ( Employee ).
When you explicitly define a foreign key constraint on the table,
the database management system will make sure that the value
of the foreign key column for every single row can be found as
a value in the referred primary key. It will guarantee that no
insert or update in the referring table can change the value of
the foreign key to something invalid. And it will also guarantee
that a referred primary key can not be updated to something
different or deleted in the referred table.
Best practice is to disable nullability for a foreign key column. If
the foreign key value is (yet) not known or does not make sense
in the current context, than a replacement value should be used
(typically surrogate key -1). To make this work, you need to
explicitely add a row with -1 as it’s primary key to the referred
table. This gives you better control of what to show in case of a
missing value (instead of just showing an empty value in the
report). It also allows for inner joins, which are more
performant compared to outer joins (which are necessary when

a foreign key contains null to not loose those rows in the result
set; read more about “Joins”).
While creating primary key constraints will automatically put
an index on the key, creating foreign key constraints is not
implemented this way in e.g., Azure SQL DB or SQL Server. To
speed up joins between the table containing the foreign key and
the table containing the primary key, indexing the foreign key
column is strongly recommended.
A next question might arise in your mind: How do I know, in
which of two entities involved in a relationship do I store the
Foreign Key? Before I can answer this question, you need first to
understand that there are different types of relationships. These
types are described as the Cardinality of a relationship.
Cardinality
The term cardinality has two meanings. It is either a number to
describe how many distinct values one column (or
combinations of values for several columns) you can find in a
table. If you store a binary value in a column, e.g., either “yes”
or “no”, then the cardinality of the column will be two. The
cardinality of the primary key of a table will always be identical
to the number of rows in a table, as every row of the table will

have a different value. In a columnar database (as Power BI or
Analysis Services Tabular are), the compression factor is
dependent on the cardinality of a column (the less distinct
values, the better the compression will be).
In the rest of the book I will mostly referring to the other
meaning, which describes cardinality as how many rows can
(maximally) be found in a related table for a given row in the
referencing table. For two given tables, the cardinality can be
either of the following:
One-to-many (1:m, 1-*)
For example: One customer may have many orders. One
order is from exactly one customer.
One-to-one (1:1, 1-1)
For example: This person is married with this other person.
Many-to-many (m:m, *-*)
For example: One employee works for many different
projects. One project has many employees.
The cardinality is defined by the business rules. Maybe in your
organisation a single order can be assigned to two customers
simulatnously. Then the above assumption (one-to-many)
would be wrong and you need to model this relationship as
many-to-many. Finding the correct cardinalities is a crucial task

when designing a data model. Make sure that you fully
understand the business rules and not make wrong
assumptions.
If you want to be more specific, you can also describe, if a
relationship can be conditional. As all relationships on the
“many” side are conditional (e.g., a specific customer might not
have ordered yet) this is usually not explicitly mentioned.
Relationships on the “one” side could be conditional (e.g., not
every person is married). You might then change the
relationship description from 1:1 to c:c in our documentation.
Combining Tables
So far you learned that information (= entities and their
relationships) is stored in tables in a data model. Before I
introduce you to rules, when to split information into different
table or keep it together in on single table, I want to discuss how
you can combine information spread into different tables in the
upcoming section.
Set Operators
You can imagine a “set” as the result of a query, or as rows of
data in a tabular shape. Set operators allow us to combine two

(or more) query results, by adding or removing rows. It’s
important to keep in mind that the number of columns in the
queries involved must be the same. And the data types of the
columns must be identical or the data type conversion rules of
the database management you are using are able to (implicitly)
convert to the data type of the column of the first query. The
first query sets both the data types and the names of the
columns of the overall result. A set operator does not change the
number or type of columns, only the number of rows. “Set
Operators” is a graphical representation of the following
explanation:
Union
Adds the rows from the second set to the rows of the first
set. Depending on the database management system you
are using, duplicates may appear in the result or be
removed by the operator.
For example, you want a combined list of both, customers,
and suppliers.
Intersect
Looks for rows, which appear in both sets. Only rows
appearing in both sets are kept, all other rows are
omitted.

For example, you want to find out, who appears to be
both, a customer, and a supplier, in your system.
Except (or minus)
Looks for rows, which appear in both sets. Only rows
from the first set, which are not appearing in the second
set are returned. You “subtract” the rows of the second
table from the rows of the first table (hence, this operator
is also called minus).
For example, you want to get a list of customers, limited to
those, who are not also a supplier.
WARNING
The comparison, if a row is identical or not is done by evaluating and comparing the
content of all columns of the row of the query. Pay attention here, as while the
primary keys listed in the result set might be identical, the names or description
might be different. Rows with identical keys but different descriptions will not be
recognized as identical by a set operator.


Figure 1-1. Set operators
As you learned, set operators are combining tables in a vertical
fashion: They basically append the content of one table to the
content of another table. The number of columns can not
change with a set operator. If you need to combine tables in a
way where you add columns of one table to the columns of
another table, you need to work with join operators.
Joins
Joins are like the set operators in the sense that they are also
combining two (or more) queries (or tables). Depending on the
type of the join operator, you might end up with the same
number of rows of the first table, or more or less rows. With
joins you can also add columns to a query (which you can not
do with a set operator).
While set operators compare all columns, joins are done on
only a selected (sub-)set of columns, which you need to specify
(in the so-called join predicate). For the join predicate usually
you will use an equality comparison between the primary key
of one table and the foreign key in the other table (equi join).
For example, you want to show the name of a customer for a
certain order (and specify an equality comparison between the

order table’s foreign key [Customer Key]  and the customer
table’s primary key [Customer Key]  in the join predicate).
In only special cases you would compare other (non-key)
columns with each other to join two tables. You will see
examples for such joins in the chapters Chapter 7, Chapter 11,
Chapter 15, and Chapter 19, where I will pick advanced
problems and demonstrate their solution. There you will also
use comparisons done not on the equality of two columns, but
use comparison operators like between, greater equal, not
equal, etc. (such joins are called non-equi join). One of the
examples is about grouping values (= binning). This is about
finding the group in which a certain value falls into by joining
the table containing the groups with a condition asking for that
the value is greater or equal than the lower range of the bin
and lower than the upper range of the bin. While the range
values form the composite primary key of the table containing
the groups, the lookup value is not a foreign key: It is an
arbitrary value, possible not found as a value in the lookup
table, as the lookup table only contains a start and end value
per bin, but not all the values within the bin.
Natural joins are a special case of equi-joins. In such a join you
do not specify the columns to compare. The columns to use for
the equi-joins are automatically chosen for you: columns with

the same name in the two joined tables are used. As you might
guess, this only works, if you stick to a naming convention
(which is a very good idea anyways) to support these joins. If
the primary key and the foreign key columns have different
names a natural join will not work properly (e.g., when the
primar key in the customer table is column ID , while in the
order table the foreign key is named CustomerID ). The same is
true, in the opposite case, when columns in both tables have the
same name, but no relationship (e.g., because both the
Product  table and the Product Category  table have a
Name  column, which represents the name of the Product  and
the name of the Product Category  respectively, but cannot
meaningfully used for the equi join).
Earlier in this chapter I used Employee  as an typical example,
where the foreign key ( [Manager Key] ) references a row in
the same table (via primary key [Employee Key] ). If you
actually join the Employee  table with itself, to find e.g., the
manager’s name for an employee, you are implementing a self
join.

TIP
The important difference between set operators and joins is that joins are adding
columns to the first table, while set operators are adding rows. Joins allow you to add
a category column to your products which can be found in a lookup table. Set
operators allow you to combine e.g., tables containing sales from different data
sources into one unified sales table. So, I imagine set operators as a combination of
tables in a vertical manner (putting two tables underneath each other). And join
operators as a combination of tables in a horizontal manner (putting two table side-
by-side). This mental concept is not exact in all regards (set operators INTERSECT
and EXCECPT  will remove rows and joins will also add or remove rows depending
on the cardinality of the relationship or the join type) but it is, as I think, a good
starting point to differentiate both.
You can join two tables in the following manners:
Inner join
Looks for rows, which appear in both tables. Only rows
appearing in both tables are kept, all other rows are
omitted.
For example, you want to get a list of customers for which
you can find an order. You can see a graphical
representation in Figure 1-2.

Figure 1-2. Inner join
This is similar to the INTERSECT  set operator. But the
result can contain the same, more or less rows than the
first table contains. It will contain the same number of
rows, if for every row of the first table exact one single
row in the second table exists (e.g., when every customer
has placed exactly one single order). It will contain more
rows, if there is more than one matching row in the
second table (e.g., when every customer has placed at
least one order or some customers have so many orders
that they over-compensate make for other customers who
didn’t place any order). It will contain less rows, if some
rows of the first table can’t be matched to rows in the
second table (e.g., when not all customers have placed
orders and these missing orders are not compensated by
other customers).

WARNING
The latter is the “danger” of inner joins: The result may skip some rows of
one of the tables (e.g., the result will not list customers without orders).
Outer join
Returns all the rows from one table and values for the
columns of the other table from matching rows. If no
matching row can be found, the value for the columns of
the other table are null (and the row of the first table is
still kept). This is shown in a graphical manner in
Figure 1-3.

Figure 1-3. Outer join

You can ask for either all rows of the first table in a chain
of join operators (left join), making the values of the
second table optional. Or the other way around (right
join). A full outer join makes sure to return all rows from
both tables (with optional values from the other table).
For example, you want a list of all customers with their
order sales from the current year, even when the
customer did not order anything in the current year (and
then display null or 0 as their order sales). To achieve this
you would select the rows from the Customer  table and
left join the Orders  table. The first table ( Customer ) is
considered the left table, the joined table ( Orders ) is the
right table in such a query.
TIP
In SQL this is easy to understand, when you write all the tables in one single
line, e.g., …​ FROM Customer LEFT OUTER JOIN Order …​. The Customer
table is literally written left of the Order  table, alas the left table. The Order
table is literally right of the Customer  table, alas the right table.
There is no similar set operator to achieve this. An outer
join will have at least so many rows as an inner join. It’s
not possible that an outer join (with the identical join

predicated) returns less rows than an inner join.
Depending on the cardinality it might return the same
number of rows (if there is a matching row in the second
table for every row in the first table) or more (if some
rows of the first table cannot be matched with rows of the
second table, which are omitted by an inner join).
Anti-join
An anti-join is based on an outer join, where you only
keep the rows not existing in the other table.
The same
ideas for left, right and full apply here, as you can see in
Figure 1-4.

Figure 1-4. Anti join
Anti-joins have a very practical use case. For example, you
want a list of customers, who did not order anything in

the current year (to send them an offer they can’t refuse).
There is no similar set operator to achieve this. The anti-
join delivers the difference of an inner join compared to
an outer join.
Cross Join
Creates a so-called cartesian product. Every single row of
the first table is combined with each and every row from
the second table. In many scenarios this does not make
sense (e.g., combining every row of the sales table with
every customer, independent, if the row of the sales table
is for the customer or a different one). Practically you can
create queries, which show thinkable combinations. For
example, by applying a cross join on the sizes of clothes
with all the colors, you get a list of all thinkable
combinations of sizes and colors (independent, if a
product really is available in this combination of size and
color). A cross join can be a basis for a left join or anti-
join, to show explicitly point out combinations with no
values available. You can see an example of the result of a
crosss join in Figure 1-5.

Figure 1-5. Cross join
Do you feel dizzy because of all the different join options?
Unfortunately, I need to add one layer of complexity in the next
section. As you just have learned, when joining two tables, the
number of rows in the result set might be smaller, equal or
higher than the number of rows of a single table involved in the
operation. The exact number depends on both, the type of the
join and the cardinality of the tables. In a chain of joins
involving several tables, the combined result might lead to
undesired results, as you will learn in the next section.
Join Path Problems
Example 1-1.
The Power BI data model has a failsafe to avoid the join path
problems described here. The problems can though easily show

up, when you combine tables in Power Query, in SQL or your
data source.
When you join the rows of one table to the rows of another
table you can face several problems, resulting in unwanted
query results. The possible problems are:
Loop
Fan Trap
Chasm Trap
Let’s take a closer look onto them:
Loop
You face this problem in a data model if there is more
than one single path between two tables. It does not have
to be a literal loop in your entity-relationship diagram,
where you can “walk” a join path in a manner where you
return to the first table. You speak already of a loop, when
a data model is ambiguous. And this cannot only exist in
very complex data models, but also in the very simple
setting of having just more than one direct relationship
between the same two tables. Think of a sales table
containing a due date, an order date and a ship date
column (Figure 1-6). All three date columns of table

FactResellerSales  ( DueDateKey , OrderDateKey
and SalesDateKey ) have a relationship to the date
column of the date table.
Figure 1-6. Join path problem: loop
The tables contain the following rows:

Table 1-2. DimDate
DateKey
2023-08-01
2023-08-02
2023-08-03
Table 1-3. FactResellerSales
DueDateKey
OrderDateKey
ShipDateKey
SalesAm
2023-08-01
2023-08-01
2023-08-02
10
2023-08-01
2023-08-02
2023-08-02
20
2023-08-01
2023-08-02
2023-08-03
30
2023-08-03
2023-08-03
2023-08-03
40
If you join the DimDate  table with the
FactResellerSales  simulatounusly on all three
datekey-columns (by writing a join predicate like
DimDate.DateKey =
FactResellerSales.DueDateKey AND
DimDate.DateKey =

FactResellerSales.OrderDateKey AND
DimDate.DateKey =
FactResellerSales.ShipDateKey) the result
would only show a single row in this example
(namely the row, which - by chance - was due,
ordered and shiped on the same day, +2023-08-
03 ). We might safely assume that many business orders
are not due or shiped on the day of the order. Such sales
rows would not be part of the result. This might be an
unexpected behavior, returning too few rows.
Table 1-4. Query Result
DateKey
DueDateKey
OrderDateKey
ShipDateK
2023-08-03
2023-08-03
2023-08-03
2023-08-03
The solution for a loop is (physically or logically)
duplicating the date table and joining one date table on
the order date and the other date table on the ship date.
Chasm trap
The chasm trap (s. Figure 1-7) describes a situation in a
data model, where you have a converging many-to-one-to-
many relationship. For example, you could store the sales

you are making over the internet in a different table than
the sales you are making through resellers. Both tables
can be filtered over a common table, let’s say, a date table.
The date table has a one-to-many relationship to each of
the two sales tables – creating a many-to-one-to-many
relationship between the two sales tables.
Figure 1-7. Join path problem: chasm trap
Here is an example for the three tables:

Table 1-5. DimDate
DateKey
2023-08-01
2023-08-02
2023-08-03
Table 1-6. FactResellerSales
OrderDateKey
SalesAmount
2023-08-01
10
2023-08-02
20
2023-08-02
30
2023-08-03
40
Table 1-7. FactInternetSales
OrderDateKey
SalesAmount
2023-08-01
100
2023-08-02
200
2023-08-03
300

Joining DimDate  and FactResellerSales  on
DimDate.OrderDateKey =
FactResellerSales.OrderDateKey  would result in
four rows, where the 2023-08-02  row of DateKey  will
be duplicated (due to the fact that on this day there are
two reseller sales). So far so good. The (chasm trap)
problem comes, when you join the FactInternetSales
table to this result (on DimDate.OrderDateKey =
FactResellerSales.OrderDateKey ). As the result of
the previous join duplicated the 2023-08-02  row of
DimDate , the second join will also duplicate all rows of
FactInternetSales  for this day. In the example the
row with SalesAmount  200 will appear twice in the
result. If you sum the numbers up, you will wrongly
report an internet sales amount of 400 for 2023-08-02
in the combined query result. This problem appears
independently of using an inner or outer join. (In the
following table I abbreviated FactResellerSales  with
FRS and FactInternetSales  with FIS.)

Table 1-8. Query Result
DateKey
FRS.OrderDateKey
FRS.SalesAmount
2023-08-01
2023-08-01
10
2023-08-02
2023-08-02
20
2023-08-02
2023-08-02
30
2023-08-03
2023-08-03
40
The solution for the chasm trap problem depends on the
tool you are using. Jump to the chapters in the other parts
of this book to read how you solve this in Power Query/M
and SQL.
Fan trap
You can step into a fan trap (Figure 1-8) in situations
where you want to aggregate on a value on the one-side of
a relationship, while joining a table on the many side of
the same relationship. For example, you could store the
freight cost in a sales header table which holds
information per order. When you join this table with the
sales detail table which holds information per ordered
item of the order (which could be multiple per order), you

are duplicating the rows from the header table in the
query result, therefore duplicating the amount of freight.
Figure 1-8. Join Path Problem: Fan trap
Here is a sample table:

Table 1-9. SalesOrderHeader
SalesOrderID
Freight
1
100
2
200
3
300
Table 1-10. SalesOrderDetail
SalesOrderID
SalesOrderLineID
OrderQty
1
1
10
1
2
20
2
1
30
3
1
40
Joining the tables SalesOrderHeader  and
SalesOrderDetail  on the SalesOrderID  leads to
duplicated rows of the SalesOrderHeader  table: The
row for SalesOrderID  1 has two order details and will
be duplicated. When you naively sum up the Freight
you would falsely report 200, instead of the correct
number of 100.

Table 1-11. Query Result
SalesOrderID
Freight
SalesOrderLineID
Orde
1
100
1
10
1
100
2
20
2
200
1
30
3
300
1
40
Similar to the Chasm problem, the solution for the Fan
problem depends on the tool you are using. Jump to the
chapters in the other parts of this book to read how you
solve this in DAX, Power Query/M and SQL.
As you saw in the screenshots, drawing the tables and the
cardinality of their relationships can help in getting an
overview about potential problems. The saying “A picture says
more than a thousand words.” applies to data models as well. I
introduce such Entity-Relationship Diagrams in the next section.
Entity-Relationship Diagrams
An Entity-Relationship Diagram (ERD) is a graphical
representation of entities and the cardinality of their

relationships. When a relationship contains an attribute, it
might be shown as a property of the relationship as well. Over
the years different notations have been developed (Lucid chart
has a nice overview about the most common notations). In my
point of view, it is not so important which notation you are
using – it’s more important to have an ERD at hand for your
whole data model. If the data model is very complex (= contains
a lot of tables) it is common to split it into sections, sub-ERDs.
Deciding on the cardinality of a relationship and documenting
it (e.g., in the form of an ERD) will help to find out, in which
table you need to create the foreign key. Look at the following
examples:
The cardinality of the relationship between customers and their
orders should be a one-to-many relationship. One customer can
possibly have many orders (even when some customers only
have a single order or others don’t have any order yet). On the
other hand, a particular order is associated with one single
customer only. This knowledge helps us to decide, if you need to
create a foreign key in the customer table to refer the primary
key of the order table, or the other way around. If the customer
table contains the [Order Key] , it will allow each customer to
refer to a single order only. And, any order, could be referenced
by multiple customers. So, plainly, this approach would not

reflect the reality in a correct manner. That’s why you need a
[Customer Key]  (as a foreign key) in the order table instead,
as shown in Figure 1-9. Then, every row in the order table, can
only refer a single customer. And a customer can be referenced
by many orders.
Figure 1-9. Entity-Relationship Diagramm for tables customer and orders
In case that an order could be associated with more than a
single customer, you would face a many-to-many relationship.
(As a customer could still have more than one order.) Many-to-
many relationships are typical if you want to find a data model
to represent employees and in which projects they are engaged.
Or collecting the reasons for a sale from your customers. The
same reason will be given for more than one sale. And a
customer would tell you several reasons, why she made the
sale.
Typically, you would add a foreign key to neither the sales table
nor the sales reason table, but create a new table on its own,

consisting of a composite primary key: the primary key of the
sales table ( SalesOrderNumber  and
SalesOrderLineNumber  in our example, shown in Figure 1-
10) and the primary key of the sales reason table
( SalesReasonKey ). This new table has a many-to-one
relationship to the sales table (over the sales’ table primary key)
and a many-to-one relationship to the sales reason table table
(over the sales reason’s table primary key). It’s therefore called
a bridge table, as it is bridging the many-to-many relationship
between the two tables and converting it into two one-to-many
relationships.

Figure 1-10. Entity-Relationship Diagramm for tables sales and sales reason
In the other parts of this book, you will learn about practical
ways of creating ERD for your (existing) data models.
Data Modeling Options
By now you should have a good understanding of the moving
parts of a data model. Therefore, it is about time to talk about

the different options of how to spread information over tables
and relationships in a data model. This is, what the next
sections will teach you.
Types of Tables
Basically, you can assign each table in your data model to either
of three types:
Entity table
Rows in such a table represent events in the real world.
These tables are also referred to as business entity, data,
detail or fact tables.
Examples include orders, invoices, etc.
Lookup table
They are used to store more detailed information, which
you do not want to repeat in every row of the entity table.
These tables are also referred to as master or main data
or dimension.
Examples include customer, product, etc.
Bridge table

A bridge table changes a single many-to-many relation
ship into two one-to-many relationships. In many
database systems, two one-to-many relationships can be
handled more gracefully than one many-to-many
relationship.
For example, to link a table containing all employees and
a table containing all projects.
Maybe you do not want to split your data into tables but keep it
in one single table. In the next section I will describe the pros
and cons of such an idea.
A Single Table to Store it All
Having all necessary information in one single table has its
advantages: It is easy to read by humans; therefore, it seems to
be a natural way of storing and providing information. If you
take a random Excel file, it will probably contain one table (or
more) and all relevant information is listed as columns per
single table. Excel even provides you with functions (e.g.,
VLOOKUP ) to fetch data from a different table to make all
necessary information available at one glance. Some tools (e.g.,
Power BI Report Builder, with which you create paginated
reports) require you to collect all information into one single

query, before you can start building a report. If you have a table
containing all the necessary information, writing this query is
easy, as no joins are involved.
Power BI Desktop and Analysis Services Tabular are not those
tools. They require you to create a proper data model. A proper
data model needs always to consist of more than one single
table, if you do not want to step into troubles (as pointed out in
“A Single Table To Store It All”). In the next section you will
learn rules, how to split columns from one table into several
tables to achieve the goal of a redundancy-free data model.
Normal Forms
The term normalizing in the context of databases was
introduced by Edgar F. Codd, the inventor of relational
databases. Personally, I don’t like the term very much (as I think
it is a confusing term, as it’s hard to tell, what’s normal and
what’s not, if you think about your real life in general, and
database in particular). But I like the idea and the concept
behind this term very much: The ultimate goal of normalizing a
database is to remove redundancy, which is a good idea in
many situations (but not all, as you will learn by reading on).

If you would store the name, address, email, phone, etc. of the
customer in each and every order, you would store this
information redundant. On one hand, you would use more
storage space than necessary, due to the duplicated
information.
On the other hand, this makes changes to the data over-
complicated. You would need to touch not just a single row for
the customer, but many (in the combined order table) if the
content of an attribute changes. If the address of a customer
changes, you would need to make sure to change all
occurrences of this information over multiple rows. If you want
to insert a new customer, who just registered in your system,
but didn’t order something yet, you have to think of which
placeholder value to store in the columns which contain the
order information (order number, amount, price, etc.) until an
order is placed. If you delete an order, you have to pay
attention, to not accidently also remove the customer
information, in case this was the customer’s only order in the
table.
To normalize a database, you apply a set of rules, to bring it
from one state, to the other. Here I will show you the rules to
bring a database into third normalform, which is the most
common normalform. If you want to dig deeper, you will find

books explaining the Boyce-Codd-Normalform, fourth and fifth
normalform, which I consider mainly as academic, and less
practical relevant.
First Normalform (1NF)
You need to define a primary key and remove repeating
column values.
Second Normalform (2NF)
Non-key columns are fully dependent on the primary key.
Third Normalform (3NF)
All attributes are directly dependent on the primary key.
The following sentence helps me to memorize the different
rules: Each attribute is placed in an entity where it is dependent
on the key, the whole key, and nothing but the key …​ so help me,
Codd. (origin unknown)
Let’s apply those rules on a concrete example:

Table 1-12. A table violating the rules of normalization
StudentNr
Mentor
MentorRoom
Course1
Cou
1022
Jones
412
101-07
143
4123
Smith
216
201-01
211
This is one single table, containing all the necessary
information. In some situations, such a table is very useful, as
laid out in “A Single Table to Store it All”. But this is not a good
data model, when it comes to Power BI and it clearly violates all
three rules of normalization.
In this example you see that there are repeating columns for
the courses a student attends (columns Course1 , Course2 ,
and Course3 ). Such a schema limits the amounts of courses to
three - and creating a report on how many students visited a
certain course is over-complicated, as you need to look in three
different columns. Sometimes information is not split into
several columns, but all information is stored in a single
column, separated by commas, or stored as JSON or XML in a
single text column would (e.g., think of a list of phone
numbers). Again, querying will be extra hard, as the format of
the input cannot be forced. Some might delimet the list using a
comma, others might use a semicolon, etc. These examples

violate the rule of the first normalform, as well. You need to
deserialize the information, and split the information into rows
instead, so that you get one single column with the content split
out into separate rows. This transforms the table towards the
first normal form (1NF).
In the given example, somebody might accidentily assign a
student to the identical course more than once. The database
could not prohibit such a mistake. Yes, you could add a check
constraint to enforce that the three columns must have
different content. But somebody could add a second row for
student number 1022, and add course 143-01.
Here, the defintion of a primary key comes into play. A primary
key uniquely identifies every row of this new table. In this first
step I do not introduce a new (surrogate) key but can live with a
composite primary key. The column headers, which make up
the primary key are printed underlined ( StudentNr  and
Course ).

Table 1-13. A table in first normalform (with a composite primary key consisting
of StudentNr  and Course )
StudentNr+#
Mentor
MentorRoom
Course
1022
Jones
412
101-07
1022
Jones
412
143-01
1022
Jones
412
159-02
4123
Smith
216
201-01
4123
Smith
216
211-02
4123
Smith
216
214-01
To transform this table into second normalform (2NF), you start
at a table in first normalform (1NF) and have to guarantee that
all columns are functional dependent on (all columns of) the
primary key. A column is functional dependent on the primary
key if a change in the content of the primary key also requires a
change in the content of the column. A look on the table makes
it clear that the column Mentor  is functional dependent on the
column StudentNr , but apparently not on the column
Course . No matter which courses a student attends, his or her
mentor stays the same. Mentors are assigned to students in
general, not on a by-course basis and the same applies to the

column MentorRoom . So, you can safely state that columns
Mentor  and MentorRoom  are functional dependent on only
the StudentNr , but not on Course . Therefore, the current
design violates the rules for second normalform.
Keeping it like this, would allow to introduce rows, with the
same student number, but different mentors or mentor rooms,
which is not possible from a business logic perspective.
To achieve the second normalform, you have to split the table
into two tables. One containing columns StudentNr , Mentor ,
and MentorRoom  (with StudentNr  as its single primary key).
A second one, containing StudentNr  and Course , only. Both
columns form the primary key of this table.
Table 1-14. Table Student  in second normalform (with
primary key StudentNr )
StudentNr
Mentor
MentorRoom
1022
Jones
412
4123
Smith
216

Table 1-15. Table StudentCourse  in
second normalform (with a composite
primary key consisting of StudentNr
and Course )
StudentNr
Course
1022
101-07
1022
143-01
1022
159-02
4123
201-01
4123
211-02
4123
214-01
The rules for the third normalform (3NF) require that there is
no functional dependency on non-key columns. In our example,
the column MentorRoom  is functional dependent on column
Mentor  (which is not the primary key), but not on StudentNr
(which is the primary key). A mentor keeps using the same
room, independent from the mentee student. In the current
version of the data model it would be possible to insert rows
with wrong combinations of mentor and mentor room.

Therefore, you have to split the data model into three tables,
carving out columns Mentor  and MentorRoom  into a separate
table (with Mentor  as the primary key). The second table
contains StudentNr  (primary key) and Mentor  (foreign key
to the newly created table). And finally, the third, unchanged
table, contains StudentNr  (foreign key) and Course  (which
both form the primary key of this table).
Table 1-16. Table Student  in third
normalform (with primary key Stude
ntNr )
StudentNr
Mentor
1022
Jones
4123
Smith
Table 1-17. Table Mentor  in second
normalform (with primary key Mentor )
Mentor
MentorRoom
Jones
412
Smith
216

Table 1-18. Table StudentCourse
was already in third normalform as
well (with a composite primary key
consisting of StudentNr  and
Course )
StudentNr
Course
1022
101-07
1022
143-01
1022
159-02
4123
201-01
4123
211-02
4123
214-01
This final version is free of any redundancy. Every single piece
of information is only stored once. No anomalies or violations
to the business logic can happen. This is a perfect data model to
store information collected by an application.
The data model is though rather complex. This complexity
comes with a price: It is hard to understand. It is hard to query
(because of many necessary joins). And queries might be slow
(because of many necessary joins). These are characteristics,

which does not make this data model ideal for analytical
purposes. Therefore, I will introduce you to Dimensional
Modeling in the next section.
Dimensional Modeling
Data models in third normalform (= fully normalized) avoid
any redundancy, which makes them perfect for storing
information for applications. Data maintained by applications,
can rapidly change. Normalization guarantees that a change
has only to happen in one single place (= content of one single
column in one single row in on single table).
Unfortunately, normalized data models are hard to understand.
If you look on the ERD of a model for an even simple
application, you will be easily overwhelmed by the number of
tables and relationships between them. It’s not rare that the
printout will cover the whole wall of an office and that
application developers who use this data model are only
confident about a certain part of the data model. If a data model
is hard to understand for IT folks, how hard will it then for
domain experts to understand?
Such data models are also hard to query. As in the process of
normalizing multiple tables get created, querying the

information in a normalized data models requires to join
multiple tables together. Joining tables is expensive. It requires
a lengthy query to be written (the lengthier, the higher the
chance for making mistakes; if you don’t believe me, re-read the
chapters about “Joins” and “Join Path Problems”) and it
requires to physically join the outspread information from
different tables by the database management system. The more
joins, the slower the query.
Therefore, let’s introduce Dimensional Modeling. You can look at
this approach as a (very good) compromise between a single
table and a fully normalized data model. Dimensional models
are sometimes referred to as denormalized models. As less as I
like the term normalized, as much do I dislike the term
denormalized. Denormalizing could be easily misunderstood as
the process to fully reverse all steps done during normalizing.
That’s wrong. A Dimensional Model reintroduces redundancy
for some tables, but does not undo all the efforts of bringing a
data model into third normal form.
Remember, the ultimate goal is to create a model, which is easy
to understand and use (by the report creators) and which
allows for fast query performance. A Dimensional model is very
common for data warehouses (DWH), OLAP systems (Online

Analytical Processing), also called cubes, and is the optimal
model for Power BI and Analysis Services Tabular.
In a dimensional model, most of the attributes (or tables) can be
either seen as a dimension (hence the name Dimensional
Modeling) or as a fact. (Later I will introduce you to other types
of tables, as well.):
Dimension
A dimension table contains answers to questions like:
How? What? When? Where? Who? Why? Those answers
are used to filter and group information in a report. This
kind of table can and will be wide (= it can contain loads
of columns). Compared to facts, dimension tables will be
relatively small in terms of the number of rows (“short”).
Dimension tables are on the “one” side of a relationship.
They have a mandatory primary key (so they can be
referenced by a fact table) and contain columns of all
sorts of data types. In a pure Star Schema, dimension
tables do not contain foreign keys, but are fully
denormalized. Think of the number of articles
(dimension) a retailer sells, compared to the number of
sales transactions (fact).
Fact

A fact table tracks real world events, sometimes called
transactions, details, or measurements. It is the core of a
data model, and its content is used for counting and
aggregating in a report. You should pay attention that you
keep a fact table narrow (only add columns if really
necessary), as compared to dimensions, fact tables can be
relatively big in terms of the number of rows (“long”). You
want fact tables to be fully normalized. Fact tables are on
the “many” side of a relationship. If there is not a special
reason, then a fact table will not contain a primary key,
because a fact table is not – and never should be –
referenced by another table and every bit you save in
each row, sums up to a lot of space, when multiplied by
the number of rows. Typically, you will find foreign keys
and (mostly) numeric columns. The latter can be of
additive, semi-additive or non-additive nature. Some fact
tables contain transactional data, others snapshots or
aggregated information.
Depending on how much you denormalize the dimension
tables, you will end up with a Star Schema or a Snowflake
Schema. In a Star Schema dimensional tables do not contain
foreign keys. All relevant information is already stored in the
table in a fully denormalized fashion. That’s what Power BI
(and a ColumnStore index in Microsoft’s relational databases) is

optimized for. Only if you have certain reasons, you might keep
a dimension table (partly) normalized and split information
over more than one table. Then some of the dimension tables
contain a foreign key.
Star Schema is preferred over a Snowflake
Schema, because in comparison a Snowflake Schema …​
has more tables (due to normalization)
takes longer to load (because of the bigger amount of
tables)
makes filter slower (due to necessary additional joins)
makes the model less intuitive (instead of having all
information for a single entity in a single table)
impede the creation of hierarchies (in Power BI/Analysis
Services Tabular)
Of course, a dimension may contain redundant data, due to
denormalizing. In a data warehouse scenario this is not a big
issue, as there are not several processes who add and change
rows to the dimension table, but only one single one (as
explained below in section “Extract, Transform, Load”).
The number of rows and columns for a fact table will be given
by the level of granularity of the information you want or need
to store within the data model. It will also give the number of

rows of your dimension tables. The next section talks about this
important part.
Granularity
Granularity means the level of detail of a table. On the one
hand, you can define the level of detail of a fact table by the
foreign keys it contains. A fact table could track sales per day, or
it could track sales per day and product, or by day, product and
customer. This would be three different levels of granularity.
On the other hand, you can also look on the granularity in the
following terms:
Transactional fact
The level of granularity is an event. All the details of the
event are stored (not aggregated values).
Aggregated fact
In an aggregated fact table some foreign keys are either
left out, you use a foreign key to a dimension table of
different granularity (e.g., dimension table on month level
instead of day level) or you pick a placeholder value for
the existing foreign key (e.g., the first day of the month of
the dimension table on the day level) and the rows are

grouped and aggregated on the remaining foreign keys.
This can make sense when you want to save storage space
and/or make queries faster. An aggregated fact table can
be part of a data model additionally to the transactional
fact table, when the storage space is not so important, but
query performance is. In the chapters about performance
tuning you will learn more about how to improve query
time with the help of aggregation tables.
Periodic snapshot fact
When you do not reduce the number of foreign keys, but
reduce the granularity of the foreign key on the date
table, than you have created a periodic snapshot fact
table. For example, you keep the foreign key to the date
table, but instead of storing events for every day (or
multiple events per day) you reference only the (first day
of the) month to create a periodic snapshot on month
level. This is common with stock levels (and other
measures from a balance sheet): Queries are much faster,
when you have the correct number of available products
on stock per day or month, instead of adding up the initial
stock and all transactions until the point in time you need
to report.
Accumulated snapshot fact

In an accumulated snapshot table aggregations are done
for a whole process. Instead of storing a row for every
step of a process (and storing e.g., the duration of this
process) you store only one single row, covering all steps
of a process (and aggregating all related measures, like
the duration).
No matter, which kind of granularity you choose, it’s important
that the granularity of a table stays constant for all rows of a
table. For example, you should not store aggregated sales per
day in a table, which is already on the granularity of day and
product. Instead, you would create two separate fact tables. One
with the granularity of only the day, and a second one with
granularity of day and product. It would be complicated to
query a table, in which some rows are on transactional level,
but other rows are aggregated. This would make the life of the
report creator hard, and not easy.
Keep also in mind that the granularity of a fact table and the
referenced dimension table must match. If you store
information by product group in a fact table, it is advised to
have a dimension table with the product group as the primary
key.

Now that you know how the data model should look like, it is
time to talk about how you can get the information of your data
source into the right shape. The process is called “Extract,
Transform and Load” and I introduce it in the next section. In
later chapters I will show you concrete tips, tricks and scripts of
how to use Power BI, DAX, Power Query and SQL to implement
transformations.
Extract, Transform, Load
By now I have hopefully made it clear that a data model which
is optimized for an application looks very different from a data
model for the same data, which is optimized for analytics. The
process of converting the data model from one type to another
is called Extract, Transform and Load (ETL):
Extract
“Extract” means to get the data out of the data source.
Sometimes the data source offers an API, sometimes it is
extracted as files, sometimes you can query tables in the
application’s database.
Transform

Transforming the source data starts with easy tasks as
giving tables and columns user friendly names (as nobody
wants to see “EK4711” as the name of a column in a
report) and covers data cleaning, filtering, enriching, etc.
This is where converting the shapes of the tables into a
dimensional model happens. In the chapters about
“Building a Data Model” you will learn concepts and
techniques to achieve this.
Load
As the source system might not be available 24/7 for
analytical queries (or ready for such queries at all) and
transformation can be complex as well, it is
recommended to store the extracted and transformed
data in a way, where it can be queried easily and fast, e.g.,
in a Power BI semantic model in the Power BI Service or
in an Analysis Services database. Storing it in a relational
data warehouse (before making it available to Power BI or
Analysis Services) makes sense in most enterprise
environments.
The ETL process is sometimes also described as the tasks in a
kitchen of a restaurant. The cooks have dedicated tools to
process the food and put in all their knowledge and skills to
make the food both, good-looking and tasteful, when served on

a plate to the restaurant’s customer. This is a great analogy to
what happens during ETL: You use tools and all our knowledge
and skills to transform raw data into savory data which makes
appetite for insights (hence the name of this book’s author’s
company). Such data can then easily be consumed to create
reports and dashboards.
As the challenge of extracting, transforming, and loading the
data from one system to another is widespread, there are plenty
of tools available. Common tools in Microsoft’s Data Platform
family are SQL Server Integration Services, Azure Data Factory,
Power Query, and Power BI dataflows. You should have one
single ETL job (e.g., one SQL Server Integration Services package,
one Azure Data Factory pipeline, one Power Query query or one
Power BI dataflow) per entity in your data warehouse. Then it is
straighforward to adopt the job in case the table changes. These
jobs are then put into the correct order by one additional
orchestration job.
Sometimes people refer not to ETL, but to ELT or ELTLT, as the
data might be first loaded into a staging area and then
transformed. I personally don’t think it so important if you first
load the data and then transform it, or the other way around.
This is mostly given as a fact by which tool you are using (if you
need or should first persist data before you transform it, or if

you can transform it “on-the-fly” when loading the data). The
only importance is that the final result of the whole process
must be accessible easily and fast by the report users, to make
their life easier (as postulated in the introduction to this
chapter).
Implementing all transformations, before users query the data
is crucial. And I think it is crucial as well to apply
transformations as early as possible. If you possess a data
warehouse, then implement the transformations there (via SQL
Server Integration Services, Azure Data Factory, or simply
views). If you don’t have (access to) a data warehouse, then
implement and share the transformations as Power BI dataflow
or use Power Query (inside Power BI Desktop) and share the
result as a Power BI semantic model. Only implement the
transformations in the report layer as the last resort (better to
implement it there instead of not implementing it at all). The
reason for this rule is that, the “earlier” in your architecture
you implement the transformation, the more tools and users
can use them. Something implemented in the report only, is
only available to the users of the report. If you need the same
logic in an additional report, you need to re-create the
transformation in the additional report (and face all
consequences of code-duplication, like higher maintenance
effort for code-changes and the risk of different

implementations of the same transformation, leading to
different results). If you do the transformation in Power Query
(in Power BI or in Analysis Services), then only users and tools
with access to the Power BI semantic model or Analysis
Services Tabular database benefit from them. While, when you
already implement everything in the data warehouse layer
(which might be a relational database but could be a data lake
or delta lake as well or anything else which can hold all the
necessary data and allows for your transformations), then a
more widespread population of your organization have access
to clean and transformed information, without repeating
transformation again (and you connect Power BI to those tables
and don’t need to apply any transformations).
Every concept and idea I introduced so far is based on the great
work of two giants of data warehousing: Ralph Kimball and Bill
Inmon. It is time that I introduce you to them.
Ralph Kimball and Bill Inmon
A book about data modeling would not be complete, without
mentioning (and referencing to) Ralph Kimball and Bill Inmon.
Both are the godfathers of data warehousing. They invented
many concepts and solutions for different problems you will
face when creating an analytical database. Their approaches

have some things in common but show also huge differences.
On their differences they never found compromises and they
“fought” about them (and against each other) in their articles
and books.
For both, dimensional modeling (facts and dimensions) play an
important role as the access layer for the users and tools. Both
call this layer a Data Mart. But they describe the workflow and
the architecture to achieve this quite differently.
For Ralph Kimball the data mart comes first. A data mart
contains only what is needed for a certain problem, project,
workflow, etc. A data warehouse does not exist on its own but is
just the collection of all available data marts in your
organization. The data marts are shaped in a Star Schema
fashion. Even when “agile project management” was not (yet) a
thing, when Ralph Kimball described his concept, they clearly
match easily. Concentrating in smaller problems and creating
data marts for them allows for quick wins. Of course, there is a
risk that you do not always keep the big picture in mind and
end up with a less consistent data warehouse, as dimensions
are not as conformed as they should be over the different data
marts. The Enterprise Data Bus’s task is to make all dimensions
conformed. Ralph Kimball retired in 2015, but you can find
useful information at The Kimball Group and his books are still

worth a read. Their references and examples to SQL are still
valid. He didn’t mention Power BI or Analysis Services Tabular,
as this was only emerging then.
On the opposite, Bill Inmon favors a top-down approach: You
need to create a consistent data warehouse in first place. This
central database is called the Corporate Information Factory and
it is fully normalized. Data marts are then derived from the
Corporate Information Factory where needed (by denormalizing
the dimensions into a Star Schema). While this will guarantee a
consistent database and data model, it surely will lead to a
longer project duration while you collect all requirements and
implement them in a then consistent fashion. His ideas are
collected in “Building a Data Warehouse” (2005, Wiley) and are
worth read as well. Bill Inmon also supports the Data Vault
modeling approach (“Data Vault and Other Anti-Patterns”) and
is an active publisher of books around data lake architecture.
If you want to dig deeper into the concept of a Star Schema
(which you should!) I strongly recommend to read Chris
Adamson’s masterpiece Star Schema: The Complete Reference.
Over the years many different data modeling concepts have
been developed and many different tools to build reports and
support ad-hoc analysis have been created. In the next section I

will describe them as anti-patterns. Not because they are bad in
general, but because Power BI and Analysis Services Tabular
are optimized for the Star Schema instead.
Data Vault and Other Anti-Patterns
I will not go into many details of how you can implement a Data
Vault architecture. Its though important to lay out that a Data
Vault is merely a data modeling approach which makes your
ETL flexible and robust against changes in the structure of the
data source. Data Vault’s philosophy is to postpone cleaning of
data to the business layer. As easy this approach makes the live
of the data warehouse/ETL developers, as hard it will make the
life of the business users. But remember: The idea of this book
is to describe how you can create a data model which makes the
end-users life easier.
A Data Vault model is somewhere between third normal form
and a star schema. Proponents of the Data Vault claim rightfully
that such a data model can also be loaded into Power BI or
Analysis Service Tabular. The problem is though: You can load
any data model into Power BI and Analysis Services Tabular –
but you will pay a price when it comes to query performance
(this happened to me with the first data model I implemented
with Power BI; even when the tables contained just a few

hundred rows, the reports I built where really slow). You will
sooner or later suffer from overcomplex DAX calculation, too.
That’s why I try to strongly convince you, not using any of the
following data model approaches for Power BI and Analysis
Services Tabular:
Single table
I already described my reasoning in
<UnderstandingADataModel_<ASingleTableToStoreItAll>>.
A table for every source file
This is a trap non-IT users easily step into. A table should
contain attributes of one single entity only. Often, a flat
file or an Excel spreadsheet contains a report and not
information limited to one single entity. Chance are high
that when you create a data model with a table per file,
the same information is spread out over different tables
and many of your relationships show a many-to-many
cardinality due to a lack of primary keys. Applying filters
on those attributes and writing more than just simple
calculations can quickly start to be a nightmare.
Sometimes this “model” is referred to as OBT (one big
table).

Fully normalized schema
Such a schema is optimized for writing, not optimized for
querying. The number of tables and necessary joins
makes it hard to use and leads to slow query response
time. Chances are high that query performance is less
than optimal and that you will suffer from “Join Path
Problems”.
Header – Detail
Separating e.g., the order information and the order line
information into two tables requires to join two relatively
big tables (as you will have loads of orders – and loads of
order lines, representing the different goods, per order).
This additional join will make queries slow and DAX more
complex than necessary, compared to combining the
header and detail table into one single fact table. The
joined table will contain as many rows as the detail table
already has and as many columns as the two tables
combined, except for the join key column, but will save
the database management system from executing joins
over two big tables.
Key-Value

A Key-Value table is a table with basically just two
columns: a key column (containing e.g., the string “Sales”)
and a value column (containing e.g., “100”). Such a table is
very flexible to maintain (for new information you just
add a new row with a new key, e.g., “Quantity”), but it is
very hard to query. In Chapter 3 I write in length about
the challenges key-value-pair tables bring, and how to
overcome them in order to transform them into a
meaningful table.
The reason why I describe these as anti-patterns is not that
these modeling approaches, in an objective point of view, per se
are worse than star schema. The only reason is that many
reporting tools benefit from a star schema so much that it is
worth to transform your data model into one. The only
exceptions are tools like Power BI Paginated Reports, which
benefit from (physical or virtual) single tables containing all the
necessary information.
The VertiPaq engine (which is the storage engine behind Power
BI, Analysis Services Tabular, Excel’s PowerPivot and SQL
Server’s Column Store index) is fully optimized for star schema
with every single fiber. You should not ignore this fact.

While you can write a letter in Excel and do some simple
calculations in a table in a Word document, there are good
reasons why you would write a letter with Word and create the
table and its calculations in Excel. You would not start
complaining how hard it is to write a letter in Excel, or that are
many features to do your table calculations are missing in
Word. Your mindset towards Power BI should by similar: You
can use any data model in Power BI, but you should not start
complaining about the product unless you have your data
model as star schema.
Key Takeaways
Congratulations on finishing the first chapter of this book. I am
convinced that all the described concepts are crucial for your
understanding of data models in general, and for all the
transformations and advanced concepts I will talk about in the
rest of the book. Here is a short refresher of what you learned
so far:
You learned about the basic parts of a data model: tables,
columns, relationships, primary keys, and foreign keys.
I talked about different ways of combining tables with the
help of set operators and joins, and which kind of problems

you can face when joining tables.
Normalized data models are optimized for write operations
that’s why they are the preferred data model for
application databases. Dimensional modeling re-introduces
some redundancy to make them easier to understand and
to allow for faster queries (as there are less joins
necessary).
Transforming of the data model (and much more) is done
during the ETL, which extracts, transforms and loads from
data sources into the data warehouse.
I gave you a rough overview about the contrary ideas of the
two godfathers of data warehouses, Ralph Kimball and Bill
Inmon.
At the end I pointed out, why it is so important to stick to a
star schema, when it comes to Power BI and Analysis
Services Tabular. Other approaches have their value – but
they or not optimal for the VertiPaq engine, which handles
all the data queried in Power BI, Analysis Services Tabular,
Excel’s PowerPivot and SQL Server’s Column Store index.

Chapter 2. Building a Data Model
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 2nd chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
Traditionally we speak of OLTP (Online Transactional
Processing) databases on the one hand and OLAP (Online
Analytical Processing) databases on the other hand. The term
“online” is not related to the internet here, but means that you
query a database directly instead of triggering and waiting for
an asynchronous batch job, which runs in the background –
something you might only have seen in your career when you

are about my age (or even older). “Transactional” means that
the purpose of the database is to store real-world events
(transactions). This is typical for databases behind any
application your mind can come up with: the software your
bank uses to track the movement of money or the retailer
which keeps track of your orders and their delivery. Databases
for such use cases should avoid redundancy under all
circumstances: A change of your name should not end up in a
complicated query to persist the new name through several
tables in the database, but only in one single place.
This book concentrates on analytical queries in general and on
Power BI and Analysis Services in particular. Therefore, when I
speak of a data model in this and all following chapters, I mean
data models built for analytical purposes, OLAP databases. For
Power BI and Analysis Services the optimal shape of the data
model is the dimensional model. Such databases hold the data
for the sole purpose of making analytical queries and reports
easy, convenient and fast (Make the report creators life easier.).
Building an analytical database (and transforming data from
data sources which were built with other goals in mind into a
dimensional model) is mostly not easy and can be a challenge.
This chapter will help you to understand those challenges and
how to overcome them. As you already learned in “Dimensional

Modeling”, you need to normalize the fact tables and
denormalize the dimension tables, which is what I start
describing in the next section. You should also add calculations,
transform flags and indicators into meaningful information to
make the data model ready to use for reports. I recommend that
you build a dedicated date (and maybe an additional time)
dimension so that the report creator does not need to fumble
with a date column and extract the year, month etc. for filters
and groupings. Some dimensions may play more than one role
within a data model, and you will learn how to model such
cases. We will discuss the concepts of slowly changing
dimensions and how to bring hierarchies into the right shape,
so they can be used in Power BI.
Remember from “Dimensional Modeling”: Normalizing and
denormalizing are terms to describe to remove or to add
redundancy to a data model. A database for the purpose of
storing information for an application should be fully
normalized. Analytical databases, on the other side, should
contain redundant information, where appropriate. In the next
two sections you will learn where redundancy should be
avoided in an analytical database as well and where you should
explicitly make information redundant.

Normalizing
Normalizing means to apply rules to the data model with the
ultimate goal to avoid redundancy. In “Dimensional Modeling”
you learned about the importance of normalizing a data model
and why this is so important for OLTP (Online Transactional
Processing) databases.
Normalizing is also necessary for fact tables in a Dimensional
Model. As you learned in “Dimensional Modeling”, fact tables
are the biggest tables in a data warehouse in terms of number
of rows, and they constantly get more rows added. Every bit
and byte we can save within a single row by optimizing the
amount and type of columns we have, is more than welcomed.
Think about the following: If you save a single byte per row in a
table containing one million rows, you save one megabyte of
data. If you save ten bytes in a table containing one billion
rows, you save 10 gigabytes of data for this table. Less data will
put less pressure on the given infrastructure. And scanning less
data will also lead to faster reports.
Typically, if your data source is a flat file (e.g., an Excel
spreadsheet someone created or extracted from a database
system) chances are high that a model created as one table per

Excel worksheet will be too denormalized, hence the
worksheets need to be normalized. The extreme case is that of a
data model consisting of one big table (OBT), where all the
information resides in one single table. You should avoid this,
as you do not want to have any tables in the model, which are
long (= many rows) and wide (= many columns) simultaneously.
You will also face situations where details for an entity are
spread over different sources, tables or files. That’s where you
need to denormalize.
Denormalizing
Denormalizing means that you intentionally introduce
redundancy into a table: The same piece of information is
repeated over several rows within a table, because several rows
share the same information (e.g., as several customers reside in
the same country or several products are part of the same
product category). This happens every time you add a column
to a table, which contains information not unique per primary
key of the table.
When you model a natural hierarchy within one single table,
you will face redundancy. For example, one product category

can consist of more than one single product. If you store the
product name together with the category name in a single table,
the name of a category will appear in several rows.
Or think of a table containing a row for each day of a year with
a column containing the date. Adding a column for the year or
the month will introduce redundancy as a given year or a given
month will appear in several rows. On top of that, storing the
year and month additionally to the date is redundant from the
point of view that the year and the month can always be
calculated by applying a function on the date column. In an
OLTP database such a redundancy is unwanted and should be
avoided under all circumstances. In an analytical database this
type of redundancy is wanted and recommended, for several
reasons:
Having all information about an entity in one single place
(table) is user friendly. Alternatively, e.g., product related
information would be spread out over several tables like,
Product , Product Subcategory , and Product
Category .
Additionally, having all information pre-calculated at hand
as needed is more user-friendly (instead of putting the
burden onto the report-user and the used report-tool to
calculate the year from a date, for example).

Joining information over several tables is expensive (in
terms of query performance and pressure on the
resources). Reducing the number of joins to satisfy a query
will improve the performance of the report.
The relatively small size of dimensions allows for added
columns without a huge impact onto overall size of the
model. (In the case of Power BI and Analysis Services this
problem is even smaller, as the storage engine’s automatic
compression algorithm is optimized for such scenarios).
Therefore, the backdraft of denormalizing is not as huge of
a problem when it comes to storage space, as you might
think.
Power BI Desktop and Analysis Services Tabular are
optimized for a fully denormalized star-schema.
Long story, short: All dimension tables should be fully
denormalized, to form a star-schema.
Furthermore, you should enrich the source’s data by adding all
sorts of calculations (again, to remove the burden of creating
these from the report-user). That’s what I will discuss in the
next section.
Calculations

It’s a good idea to add a calculation as early as possible in your
stream of data. Keep in mind though that only additive
calculations can be (pre-)calculated in the data source. Semi-
and non-additive calculations must be calculated as measures
(in case of Power BI and Analysis Services this means: in the
DAX language):
Additive
Many calculations can be calculated on top of results of
lower granularity. The given quantity sold in a day can be
added up to monthly and yearly results. The sales amount
(calculated as the quantity multiplied by the appropriate
price) can be added over several products.
Semi-additive
The result of a semi-additive calculation can be
aggregated over all dimensions, except the date
dimension. A typical example is stock levels. Stock levels
are stored as the number of products available on a
certain day. If you look at a certain day you can add the
stock levels over several warehouses for a product: You
can safely say that we have 5 kg of vanilla ice cream if
there is 3kg in one freezer and another 2kg in a second
freezer. But it does not make sense to add up the

individual stock level of different days: When we had 5kg
yesterday and today only 1kg is left, then adding these two
numbers up to 6kg gives a meaningless number. Thus, the
calculation formula needs to make sure, to use only the
data from the most current day within the selected
timeframe.
Non-additive
Some calculations cannot be aggregated at all. This covers
distinct counts and all calculations containing a division
operator in its formula (e.g., average, percentage, ratio).
Adding up results of such a calculation does not make any
sense: Instead of aggregating the results, the formula must
be executed upon the aggregated data: counting the
distinct customers over all days of the month (instead of
adding up the number of distinct customers per day) or
dividing the sum of margin by the sum of sales amount
(instead of dividing the margin by the sales amount of
each individual sales row and then summing up those
results).
Formulas can also be applied to non-numeric values. In the
next chapter you will find out why and how this should be
done.

Flags and Indicators
In most cases, reports showing rows of “Yes” and “No” values or
abbreviations like “S” or “M” are hard to read. To avoid this,
you need to convert all flags and indicators delivered by the
source system into meaningful text. For example:
FinishedGoodFlag with content 0 or 1 should be
transformed accordingly into text “no salable” or “salable”.
Productlines “R”, “M”, “T”, or “S” should be transformed
accordingly into text “Road”, “Mountain”, “Touring”, or
“Standard”.
Column Class with values “H”, “M”, or “L” should be
transformed accordingly into “High”, “Medium”, or “Low”.
Styles containing “W”, “M”, or “U” should be transformed
accordingly into “Women’s”, “Men’s”, or “Unisex”.
In general blank (or null) should be avoided for texts, but
replaced by a meaningful text: “unknown”, “N/A”, “other”,
etc. Depending on the context, a blank value could be
transformed to different texts (to distinguish an “unknown”
value from “other”) within the same column.
Do you create reports on data which is not related to any point
in time? Writing this book, I thought hard about it and could

not remember a single report I created which did not either
filter or aggregate on dates, or even both. Of course, this does
not mean that such report does not exist. But it makes me
confident that such reports are not so common. Therefore, you
should prepare your data model to make handling date and
time easy for the end-user. The next section is exactly about
this.
Time and Date
It’s very rare to build a data model upon data, which does not
bear any relation to a point in time. Therefore, a date
dimension is very common in the majority of data models. The
idea of a date dimension is two-folded:
Create columns for all variants of a date information which
will be later used in the reports. Year, month number,
month name, name of the weekday, week number, etc. are
common examples. The report tool shall not cover this, but
just show the pre-calculated columns. Therefore, add a
column for every variation needed in the report (e.g.,
“December 2023”, “2023-12”, “Dec”, …​). Sometimes a
numeric column is used to reference the date table is used,
instead of a column of data type date. This can be derived

by a simple formula: Year * 10000 + Month Number * 100 +
Day Number.
Having a table with one row per day of a calendar year.
This allows you to calculate a duration in days and is
mandatory if you want to use the built-in time intelligence
functions in DAX (which we will cover in “Time and Date”).
TIP
There is no year 0000 in the Gregorian calendar, but the first year is year 0001 (AD =
Anno Domini or CE = Common Era). Directly before year 0001 comes year -0001 (or:
0001 BC = Before Christ or AC = Ante Christum) This makes calculations into the past
easy (as the same rules apply to AD and AC), but sometimes confuses people that the
second millenia ended first in year 2001 (as it started in year 0001) and not in year
2000 (as it did not start in year 0000).
Calendar week numbers can be tricky, by the way. Apart from
that about half of the population of this planet start their weeks
on Sundays, while the others start on Mondays, there are
basically two definitions of how to calculate the calendar week
numbers. These definitions deviate from each other only in
certain years, which could be the reason that you do not
discover a possible mistake. If you do not pay attention to the
right calculation of the calendar week, you might end up with a
sudden surprise in one year. Wikipedia got you covered in case

you need to find out which definition is the one the report users
expect (ISO Week Date).
A time dimension (having rows for hours and minutes within a
day) on the other hand is in fact very rare in my experience. It’s
important that you separate the time dimension from the date
dimension, so both can be filtered independently from each
other. Furthermore, splitting a timestamp into a date and a time
portion, minimizes the number of distinct rows: To cover a full
calendar year, you need 365 (or 366 for leap years) rows in the
date dimension, and 1440 (= 24 hours multiplied by 60 minutes)
rows for a time dimension to cover every minute. For every
new year, you add another 365 (or 366) rows in the date table. If
you stored this information together in one single datetime
table, you would end up with 525600 (365 days times 24 hours
times 60 minutes) rows. For every year, you would add another
525600 rows in the datetime table.
Talk to your end-users to find out, on which granularity level of
the time they need to filter and group information. If the lowest
granularity is e.g., only by hour, make sure to round (or trim)
the timestamp in your fact table to the hour and create a time
dimension with only 24 rows.

Role-Playing Dimensions
Sometimes one single entity can play different roles in a data
model. A person could simultaneously be an employee and a
customer as well. A year could have the meaning of an order
year and/or the shipping year. These are examples of role-
playing dimensions.
Assigning different roles can be achieved with the following
approaches:
Load the table only once, and then assign different roles by
creating several relationships between the dimension table
and the fact table, according to its roles. For example, you
create two filter relations between the Date  dimension
and the Sales  fact table. One where you connect the
Date ’s date column first with the Sales ’ order date and
second with the Sales ’ ship date. The report creator needs
then a way to specify, which role the dimension should play
in a visualisation.
Load the table twice into the data model, under two
different names. For example, you would load the Date
table first as Order Date  and second as Ship Date
table. (Make sure that the column names are unique

throughout the data model, by e.g., adding the Order  or
Ship  prefix to the column names as well: Year  becomes
Order Year  etc.) You would then create filter
relationships between the Sales  fact and those two tables.
The report creator chooses either the Order Date  or the
Ship Date  table according to the needs.
Slowly Changing Dimensions
The value for a column of a row in a dimension table may not
be carved in stone, but could change over time. The question
we need to clarify with the business users is if it is important to
keep track of changes – or if we can just overwrite the old
information with the new information. A decision needs to be
made per column of a dimension table (maybe the business
wants to overwrite any changes of the customer’s name but
keep a historic track of the changes of the customer’s address).
We talk about slowly changing dimensions when chances of the
attributes are happening only once in a while. If the
information for a dimension changes often (e.g., every day) you
might capture the changes of this attribute not in the dimension
table, but in a fact table instead. Unfortunately, there is not a

clear line here on how to distinguish slowly changing
dimensions from rapid changing dimensions.
While Ralph Kimball was very creative with creating new terms
for the challenges of analytical databases, he came up with a
rather boring way of naming the different types of slowly
changing dimensions, he just numbered them: Type 0, Type 1,
etc.
Type 0: Retain Original
Usually only a small set of dimensions (and their columns) shall
never change. For example, August 1 2023 will always be a
Tuesday and will always be part of month of August and year
2023. This will never change – it’s not necessary to implement a
way for updating this information.
Type 1: Overwrite
When the name of a customer changes, we want to make sure
to correct it and display the new name in all reports – even in
reports for that past (where an old version of the report may
show the old name; re-creating the same report now will show
the new name). Maybe we want to store some additional
columns in the table, like, when the change happened and who

(or which ETL process) did the change. In Table 2-1, you see a
table containing three rows, enriched with a ChangedAt  and a
DeletedAt  column, which represent the day of modification
(or creation) and invalidation, respectively.
Table 2-1. SCD Type 1 before the change
AlternateKey
Region
ChangedAt
DeletedAt
0
NA
2023-01-01
1
Northwest
2023-01-01
10
United
Kingdom
2023-01-01
Let’s assume that we get the new data, as laid out in Table 2-2:
the row for region “NA” was removed from the data source, the
name of region “Northwest” was translated to German
language “Nordwest”, the row for “United Kingdom” stayed
unchanged and a new row for “Austria” was added.

Table 2-2. SCD Type 1 changed rows
AlternateKey
Region
1
Nordwest
10
United Kingdom
11
Austria
As you can see in Table 2-3, in a Type 1 solution, the row for
“NA” will not be removed, but marked as deleted by setting the
DeletedAt  column to the timestamp of removal. The row for
“Northwest” will be changed to “Nordwest” and the
ChangedAt  timestamp will be updated. “United Kingdom” will
stay unchanged. And the new row for “Austria” is added, with a
ChangedAt  set to the current day.

Table 2-3. SCD Type 1 after the changes
AlternateKey
Region
ChangedAt
DeletedAt
0
NA
2023-01-01
2023-08-15
1
Nordwest
2023-08-15
10
United
Kingdom
2023-01-01
11
Austria
2023-08-15
This is a very common type of slowly changing dimension.
Type 2: Add New Row
If you want to update a column, but need to guarantee that a
report made for the past does not reflect the change (but stays
the same, even if created today), then we need to store two
versions. One version, reflecting the status before the change,
and a new version, reflecting the status after the change. An
example could be the address (and region) of a customer. If the
customer moves, maybe we only want to assign sales made
after the customer moved to the new region but want to keep
all previous sales in the old region.

Slowly Changing Dimension Type 2 achieves this by creating a
new row in the dimension table for every change we want to
keep track. It is important to mention that for this solution we
need to have a surrogate key as the primary key in place, as the
business key will not be unique after the first change. Customer
“John Dow” will have two rows in the customer table. One row
before the change, one row after the change (and several
further rows after more changes happened). All sales before the
change use the old versions surrogate key as the foreign key. All
sales after the change use the new versions surrogate key as the
foreign key. Querying is therefore not big of an issue (as long as
the report users do not need to select a certain version of the
customer to be used for a report for any point in time; “Slowly
Changing Dimensions” will have you covered to implement this
request). In Table 2-4 you see a table which contains the
additional columns to describe the timespan, when the row
is/was valid ( ValidFrom  and ValidUntil ). This looks
somehow similar to the Type 1 solution. In the example I kept
ValidUntil empty for rows without a invalidation. Alternatively,
you could also use a timestamp far in the future (e.g., December
31 9999).

Table 2-4. SCD Type 2 before the change
AlternateKey
Region
ValidFrom
ValidUntil
0
NA
2023-01-01
1
Northwest
2023-01-01
10
United
Kingdom
2023-01-01
Let’s assume that we get the same new data, as laid out in
Table 2-5: the row for region “NA” was removed from the data
source, the name of region “Northwest” was translated to
German language “Nordwest”, the row for “United Kingdom”
stayed unchanged and a new row for “Austria” was added.
Table 2-5. SCD Type 2 changed rows
AlternateKey
Region
1
Nordwest
10
United Kingdom
11
Austria
As you can see in Table 2-6, in a Type 2 solution as well, the row
for “NA” will not be removed, but marked as deleted by setting

the ValidUntilAt  column to the timestamp of removal. For
the row, containing “Northwest” the ValidUntil  timestamp
will be updated and a new version for the same
AlternateKey , but region “Nordwest” will be inserted into
the row. “United Kingdom” will stay unchanged. And the new
row for “Austria” is added, with a ValidFrom  set to the current
day.
Table 2-6. SCD Type 1 after the changes
AlternateKey
Region
ValidFrom
ValidUntil
0
NA
2023-01-01
2023-08-15
1
Northwest
2023-01-01
2023-08-15
10
United
Kingdom
2023-01-01
1
Nordwest
2023-08-15
11
Austria
2023-08-15
WARNING
You need to keep an eye on how many changes are to be expected for the dimension
on average in a certain period of time, as this approach will let the dimension table
grow in terms of rows.

This is a very common type of slowly changing dimension as
well.
Type 3: Add New Attribute
Instead of creating a new row for every change, Type 3 keeps a
dedicated column per version. Obviously, you need to decide
upfront of how many versions you want to keep track of, as you
need to provide one column per version.
New versions will therefore not let the table grow, but the
number of versions you can keep per entity is limited. Querying
can be a bit of an issue, as you need to query different columns,
depending on if you want to display the most current value of
an attribute or one of the previous versions.
I have never implemented this type of slowly changing
dimension for one of my customers. But it may still be a useful
approach for your use case.
Type 4: Add Mini-Dimension
This approach keeps track of the changes in new rows, but in a
separate table. The original table shows the most current
version (as Type 1 does) and the older versions are archived in

a separate table (which can hold as many older versions as you
need). Querying the most current version is easy. Showing older
versions involves a more complex query for joining a fact table
to the correct row of the archive table. Or you would store both
the foreign key to the original table and a second foreign key to
the matching rows in the mini-dimension. New versions do not
change the number of rows in the original table but will
certainly do in the extra table.
Again, I have never implemented this type of slowly changing
dimension for one of my customers. But it may still be a useful
approach for your use case.
Type 5, 6, and 7
The rest of the types are more or less combinations of the
previous versions. I am sure they have their use cases – but I
never had to implement them, as Type 1 and Type 2 were
sufficient for my client’s needs so far. That’s why I just give you
a short overview here instead of a in-depth description:
Type 5: Add Mini-Dimension and Type 1 Outrigger
Type 6: Add Type 1 Attributes to Type 2 Dimension
Type 7: Dual Type 1 and Type 2 Dimensions

At Slowly Changing Dimensions you find more about these
types.
Hierarchies
Hierarchical relationships can be found in real-life in many
situations:
Product categories (and their main- and subcategories)
Geographical information (like continents, regions,
countries, districts, cities, etc.)
Time and date (like year, month, day, hour, minute, second,
etc.)
Organisation tree (every employee reports to another
employee with the CEO at the top)
I am convinced that you will have some hierarchical structures
in your data model(s) as well. From a technical perspective, the
latter example (organisation tree) is different from the other
examples. Typically, you store year, month, day, etc. in separate
columns of a dimension table to represent such a natural
hierarchy. This doesn’t necessarily apply to an organisation
tree, which is a so-called parent-child hierarchy. There are
plenty of ways of storing the parent-child relationships in a

table. One way is that an employee references another
employee (which is a different row within the same table) over
a simple foreign key relationship. This is called a self-
referencing relationship because the Employee  table contains
both the primary key and the foreign key used in this
relationship. The employee’s Manager ID  references another
employee’s Employee ID. This is a very efficient way of storing
this information, but it’s hard to query because you need to
somehow traverse the organisation tree from one row to the
other.
You can either use a recursive Common Table Expression (CTE)
written in SQL to collect information from different levels or
you could write a recursive function in T-SQL (I will
demonstrate both in (s. “Hierarchies”). You can also solve this in
DAX (“Hierarchies”) and Power Query (“Hierarchies”). Any way,
Power BI asks you to create a so-called materialized path per
row in the employees table. Figure 2-1 shows an example, what
the materialized path could look like for a bunch of nodes in a
hierarchy.

Figure 2-1. MaterializedPath
The materialized path is simply a string containing a reference
to the current node and all its parent nodes. This example uses
the names of the employees, concatenated with a pipe (|) as the
delimiter. I used the full names for better readabilty, in reality
you should use the primary keys of the nodes (e.g.,
EmployeeKey ) instead, of course. The delimiter is necessary,
otherwise a materialized path of “123” could be interpreted as
node 1 and 23 or as node 12 and 3. Make sure that the delimiter
will never be used in the actual values.
A materialized path is a rather convenient way to query. This
string can be split into separated columns containing e.g., the

name of the managers as one column per level. In this flattened
table the name of the CEO will then appear in the column
representing level one of all employees. Level two contains the
next management level, and so on. You can easily count the
number of keys in the materialized path (by counting the
number of separators and adding one) to know on which level
the employee is within the hierarchy. See an example for
employee “Amy Alberts” in Table 2-7
Table 2-7. Materialized path and columns per level
EmployeeKey
ParentEmployeeKey
FullName
PathKey
290
277
Amy Alberts
112|277
You made it through Chapter 2 and now it is time to wrap it up.
Key Takeaways
This chapter guided you through typical tasks when building a
data model:
You need to denormalize your fact tables and fully
normalize your dimension tables to form a star schema.

You should push transformations up stream as far as
possible and delay calculations as long as possible,
preferably at runtime if performance allows, and still in a
centralized semantic model (and not duplicated in every
report). The more up stream you put definitions, the more
people and tools can use it, which avoids having
calculations defined on many places.
You should avoid keeping flags and indicators as they are
but transform them into meaningful texts instead.
Time and Date play a crucial role in many data models.
Both should be created in the granularity needed in your
reports (e.g., a Date dimension with a row for every single
day of the year or a Time dimension for every single hour
of a day).
A single dimension may play more than one role within the
data model. You can either create the table several times in
your data model (once per role) or create several
relationships from the dimension to the fact table and
activate the relationship as needed.
Modeling of Slowly Changing Dimensions is needed when
you want to keep track of changes in the attributes of a
dimension. The most common type is the one, where you
create a new row for every new version of the entity.

There are two different types of hierarchies. One, where
you have one column per level (either within one table in
the case of a denormalized table or every column in a
different table in the case of a normalized model; called a
natural hierarchy). The other type is where a child row
references its parent row, both stored in the same table (=
parent-child hieararchy).

Chapter 3. Real-World Examples
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 3rd chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
In Chapters 1 and 2, you learned to understand the basics of a
data model and which steps you need to take to build a data
model optimized for analytics. This chapter builds upon these
steps. Please make sure that your data model is built upon these
steps before you dive into the use cases described in this
chapter.

WARNING
If you did not proceed as described in Chapters 1 and 2, applying the concepts of the
current chapter might be full of frustrations – because the best basis for advanced
concept is a star schema. There is no shortcut into advanced topics.
In this chapter I will describe five uses cases, which I see at
many of my customers – and therefore assume that there is a
high likelihood of you facing them as well sooner or later. The
list is, of course, a personal selection – every new project comes
with its own challenges. Binning is the challenge of not showing
the actual value, but a category the value falls into, instead. I
will then use the case of Budget values to introduce a data
model, containing more than one single fact table (which still
conforms with the rules of a star schema). I am very excited
(and proud of) the section Multi-language model. It describes
the problem of giving the report-user full control over the
display language of the report (read: headlines and content)
and the solution I came up with. It is similar with the section on
Key-value pair tables: A data source for a data model of one of
my customers contained all information in a single, unpivoted
table, for which I describe the problems and (semi-)automatic
solutions to pivot the table, which I could not find a pre-defined
solution. Finally, the section on Combining self-service and
enterprise business intelligence will describe the basis for what

can be done in Power BI and Analysis Services in a Composite
model.
In the first use case, Binning, I will describe different
approaches of solving the same problem – all of them forcing
you to think a bit out-of-the-box.
Binning
The term “binning” in this context means that you do not want
to show the actual values (like a quantity of four), but to show a
category instead (like “medium” or “between one and five”).
Basically, we have the following options to model this
requirement, all of them with different disadvantages and
advantages, as described:
Adding a Column to the Fact Table
Adding a new column to the table containing the value and
making sure to fill it with the correct description of the bin as in
Table 3-1. This is the simplest of the options, but not
recommended, as you would make the fact table (containing the
value to be binned) wider. Also, in case the range of the bin or
its descriptive text changes, you would need to update the fact
table. In some implementations this will not be possible,

because due to the size of the fact table and the run-time of the
update statement.
Table 3-1. Adding a column to the fact table
Date
Product
Quantity
Bin
2023-08-01
A
3
Middle
2023-08-01
B
1
Low
2023-08-02
B
4
Middle
2023-08-03
C
5
High
Creating a Lookup Table
Creating a lookup table like in Table 3-2, which consists of two
columns: One contains a distinct list of all possible values. The
second column contains the value to show for the bin. This is
identical to the approach with the lookup table to transform a
flag or code into a meaningful text, described in the previous
chapter.

Table 3-2. Adding a lookup table
containing distinct quanties and their
bin
Quantity
Bin
1
Low
2
Low
3
Middle
4
Middle
5
High
6
High
You then create an equi-join between the table containing the
values to-be-binned and this lookup table. This looks a bit
unusual, as we are used to join primary keys and foreign keys
and not e.g., a quantity. But this solution is easy to implement
and has a very good performance.
Another advantage is that such a table is usually easy to create.
Maintaining the table is easy, in principle, as well. The catch is
only if somebody needs to maintain the table by hand and typos
happen in some of rows (then a value of three would be
assigned to “medum” instead of “medium” and would be shown

as a category for itself). Or, if the categories get mixed up by
accident (that a value of four is “medium”, but a value of five is
set to “small”). Usually such a problem is easy to spot and fix
manually. Alternatively, you can use a script to create and
maintain the table.
A real backdraft is though that this idea only works, if we can
generate a distinct list of all possible values. Yes, you can add
some extra lines for outliers (quantities beyond a thousand,
maybe), but if we are not talking about pieces, but about
pounds or kilograms, then an unknown amount of decimal
digits can be involved as well. Rounding (to the nearest whole
number or thousand or million) could though help to overcome
this problem.
Describing the Ranges of the Bins
The other option is to create a table, containing three columns:
One, defining the lower value per category, another one to
define the upper value per category, and finally the value to
show, when a value falls in between the lower and upper range.
You can see an example in Table 3-3.

Table 3-3. Adding a lookup table containing ranges of
quantities and their bin
Bin
Low (incl.)
High (excl.)
Low
3
Middle
3
5
High
5
Such a table is even easier to create. It’s less prone to mistakes,
but it involves some extra logic, when assigning the actual
values to the category.
NOTE
I also want to point out that I strongly recommend making one range value (e.g., the
lower value) inclusive, and the other one (e.g., the upper value) exclusive. That
means that a value falls into a category if it is greater or equal the lower bound, but
lower than the upper bound. This has the advantage that you can use the exact same
number as the upper bound for one category and as the lower bound for the next
category. There will be no gaps, as a value is either lower than the upper bound (and
therefore falls into this category), or it is greater or equal than the upper bound
(which matches the lower bound of the next category) and therefore falls into the
next category. Makes sense?
Another challenge I see often in models I build for my
customers is that of combining information of different

granularity in one single data model. This is e.g., the case when
you combine actual values and their budget, as discussed in the
next section.
Budget
I called this section “Budget”, but budget is only one of plenty
use cases with the exact same problem. The problem I am
talking about is the problem of “multi-fact” models. A “multi-
fact” model is a data model, containing more than one single
fact table. Such models are sometimes called “galaxies” or
“universes”, as they contain more than a single “star”. This
makes only sense if those stars have at least one common
dimension. If not, I would recommend creating two
independent data models instead.
The definitive goal in a star schema is to add new information
to the existing tables only, if possible, and not to create a new
table for every extra piece of information. The reason is that
joining tables is an expensive operation in terms of
report/query runtime. With that said, you should first evaluate
if the granularity of the new information matches the
granularity of an existing table.

Let’s first look at cases, where we can just add the information
without further changes.
Maybe you want to add the information about a the product’s
category to the reports (and therefore to the data model). If you
already have a table which is on the same or a lower
granularity than the product category, e.g., a dimension table
Product  which contains information about individual
products, you can simply add a Product Category  column to
this dimension table. The granularity of the Product  table will
not change, as you can see in Table 3-4
Table 3-4. Product table with main product category
Product Key
Product Name
Product Category
100
A
Group 1
110
B
Group 1
120
C
Group 2
130
C
Group 3
If the new information, you want to add to a fact table, is on the
same granularity, you can simply add this as a new column. For
example, in a table containing Sales  amounts in EUR you can

simply add a new column containing the Quantity  in pieces.
As long as both the amount in EUR and the quantity in pieces
are on the same granularity this is no problem. The granularity
of a fact table is given by the foreign keys in the table (e.g., date,
product, customer, etc), which did not change in the example
shown in Table 3-5
Table 3-5. Adding quantity to a fact table
Date
Product
Sales
Quantity
2023-08-01
A
30
3
2023-08-01
B
20
1
2023-08-02
B
120
4
2023-08-03
C
500
5
The more challenging cases are coming now: If the table to start
with is on the granularity of product category (e.g., with
Product Category Key  as its primary key as shown in
Table 3-6), then adding the product’s key would change the
granularity of the table. Product Category Key  would not
be the primary key anymore, as it is expected that there are
several products (with individual rows) per Product
Category Key . The cardinality of relationships from (fact)

tables to the dimension table would suddenly change from one-
to-many to many-to-many: per row in the fact table there will
be several rows in the dimension table. This is something you
should avoid, as described in “Relationships”. Instead you
would keep the existing dimension table on its granularity and
introduce a new dimension table with the different granularity.
Table 3-6. Table for Product Categories
Product Category Key
Product Category
10
Group 1
20
Group 2
Something similar happens if you want to add facts on a
“higher” granularity. While we collect actual sales information
on the granularity of day, product, customer, etc., values for a
budget are typically only available on a coarser level: per
month, per product group, not per customer, etc. One solution is
to find an algorithm to split the budget value down to the
smaller granularity (e.g., dividing the month’s budget over the
days of the month). Another solution is to create a fact table of
its own for the budget (s. Table 11-5), hence creating a “multi-
fact” data model. Then the relationship between the Budget
fact table and the Product  dimension table can only be

created on Product Group  level, which has a cardinality of
many-to-many (in neither table the Product Group  is the
primary key). In later chapters I will introduce solutions to
overcome this problem.
Table 3-7. A budget it typically on a different granularity than
the actual values
Month
Product Group
Budget
2023-08
Group 2
20000
2023-08
Group 3
7000
2023-09
Group 2
25000
2023-09
Group 3
8000
No matter what the reason for a many-to-many cardinality is, it
is best practice to introduce a table in between to bridge the
many-to-many cardinality and creating two one-to-many
relationships instead. For example, you create a table consisting
of the distinct product groups. The product group’s names (or
their keys) would be the primary key of this new table. The
relationship from this table to the Budget  table has then a one-
to-many relationship. Likewise, the relationship from this table
to the Product  table is a one-to-many relationship, as well
(Figure 3-1). In the end, the relationship between tables

Budget  and Product  has still a cardinality of many-to-many,
but is split into two one-to-many relationships, which makes
handling in many tools easier.
Figure 3-1. Bridge Table
Now to something completely different: In a global world your
users might expect to get the reports shown in a language of
their choice. In the next section I describe a data model which
allows for such.
Multi-language Model
A requirement for reports/data models to support several
languages can be seen on different levels:
Textual content (e.g., product names)
In my opinion, the most robust solution is, to introduce
translations of dimensional values as additional columns
to the dimension table, as laid out in Table 3-8 New

languages can then be introduced by adding rows to the
tables – no change to the data model or report is
necessary. The challenge is that the tables’ primary key is
then not unique anymore, as for e.g., Dim1 ID  of value
11 we have now several rows in the table (with different
content for the description, and an additional Language
ID ). The primary key becomes a composite key ( Dim1 ID
and Language ID ), which comes with several
consequences we will discuss in the other parts of this
book.
Table 3-8. A table containing every dimensional entity per
language
Language ID
Dim1 ID
Dim1 Desc
EN
11
house
EN
12
chair
DE
11
Hause
DE
12
Stuhl
Visual elements (e.g., report headline)
As I don’t want to create and maintain several versions of
the same report (one for each and every languages) I store

all the text for visual elements in the database as well (in
a table like in Table 3-9). This can be done via a very
simple table, containing three columns: the Language
ID , a text identifier (which is independent of the
language) and the display text (which is different per
language and text identifier). The user’s selection of the
language will also be applied as a filter on this table.
Instead of just typing the headline, I show the
DisplayText  for a specific text identifier.
Table 3-9. A table containing display texts for different parts of the
report
Language ID
Textcomponent
DisplayText
EN
SalesOverview
Sales Overview
EN
SalesDetails
Sales Details
DE
SalesOverview
Verkaufsübersicht
DE
SalesDetails
Verkaufsdetails
Numerical content (e.g., values in different currencies)
Numbers are not strictly translated, but localizing in a
broader sense also means that numbers have to be
converted between currencies. There are a wide variety

of solutions when it comes to finding the correct exchange
rate. In a simple model you would have one exchange rate
per currency (s. Table 3-10). In more complex scenarios
you would have different exchange rates over time and an
algorithm to select the correct exchange rate.
Table 3-10. A table containing exchange rates
Currency Code
Currency
Spot
EUR
Euro
1.0000
USD
US Dollar
1.1224
JPY
Japanese yen
120.6800
Data model’s metadata (e.g., the names of tables and columns)
Analytical database allow to translate the names of all
artefacts of a data model (names of tables, columns,
measures, hierarchies, etc.). When a user connects to the
data model the preferred language can be specified in the
connection string. Usually, only power users, who create
queries and reports from the data model, care about this
meta data. And usually they understand terms in English
(or in the language the data model is created). Mere report
consumers will not directly see any part of the data

model, but only what the report exposes to them. And a
report can expose text via translated visual elements.
Therefore, in my experience, the use case for meta data
translation is only narrow.
User interface (e.g., Power BI Desktop)
You need to check the user documentation on how to
change the user interface’s language. In “Multi-Language
Model” I will describe the settings for Power BI Desktop,
Power BI Service and Power BI Report Server.
Some data sources expose their information in a way which
looks like a table on first sight, but which – after a closer look –
turn out to be not a classical table with information spread out
over different columns. You will learn how to handle such
tables in the next section.
Key-Value Pair Tables
You can see an example for a key-value pair table in Table 3-11.
Such a table basically consists of only a key-column and a value-
column (alas the name):
Key

This is the attribute. For example, “city”.
Value
This is the attribute’s value. For example, “Seattle”.
Typically, you will find two extra columns:
ID
Common rows share the same ID. For example, for ID=1
there would be two rows, one for key =”name” and
another one for key =”city”.
Type
This column contains a data type descriptor for the value
column. As column Value  must be of a string data type
(as a string is the common denominator for all data types;
a value of any data type can be converted into a string),
Type  tells us what kind of content to await in the Value
column.

Table 3-11. A table containing key-value pairs of rows
ID
Key
Value
Type
1
name
Bill
text
1
city
Seattle
text
1
revenue
20000
integer
1
firstPurchase
1980-01-01
date
2
name
Jeff
text
2
city
Seattle
text
2
revenue
19000
integer
2
firstPurchase
2000-01-01
date
3
name
Markus
text
3
city
Alkoven
text
3
revenue
5
integer
3
firstPurchase
2021-01-01
date
Such a table is extremely flexible when it comes to adding new
information. New information is simply added via an additional

row (containing a new value for Key  and its Value ). No need
to change the actual schema (column definition of such a table).
This makes it very likable for application developers. Its
flexibility is like storing information in flat files (JSON, XML,
CSV, …​).
On the other hand, it is very hard to build reports on top of such
a table. Usually, you need to pivot the content of the Key
column and explicitly specify the correct data type (e.g., to
allow for calculation on numeric values).
Although there is one use case, where the table in its original
state can make for very flexible reports. If the goal is to count
the IDs on aggregations on different combination of keys to look
for correlations, you can self-join the key-value pair table on the
ID  column. Then you filter the two Key  columns individually
(e.g., one on “name” and another on the “city”). Showing one
Value  on the rows and the other on the columns of a pivot
table (or a Matrix visual in Power BI for that matter) and the
count of the ID  in the value’s section. You get a quick insight
into the existing combinations (e.g., that we have people of
three different names living in two different cities and in which
city how many people of each name live.). If you allow the
report user to change the values for the two Key  columns she

can easily grasp the correlation of combinations of any
attribute. You will see this in action in “Key-Value Pair Tables”.
Most reports you need to build are probably of a different
nature: You need to group and filter some of the attributes and
aggregate others. Therefore, you need to pivot all the keys and
assign them to dedicated columns (with a proper data type), as
shown in Table 3-12. Some reporting tools/visuals can do that
for you. Most prominently Excel’s pivot table or Power BI’s
matrix visual. They can pivot the key column for you, but they
are not capable of changing the data type of the Value  column.
Aggregations will not be done at all or at least not in the proper
way. Therefore, the best solution is one, where you prepare the
pivoted table in the data model.
Table 3-12. The key-value pairs table pivoted on the key column
ID
name
city
revenue
first
1
Bill
Seattle
20000
1980
2
Jeff
Seattle
19000
2000
3
Markus
Alkoven
5
2021

Who typically builds the data models in your organization: the
domain experts or a dedicated (IT-)department? Both concepts
have their advantages and disadvantages. The next section is
dedicated to lay out their possibilities.
Combining Self-Service and
Enterprise BI
We speak of Self Service BI when a domain expert (with no or
little IT background) solves a data related problem on her own.
This includes connecting to the data source(s), cleaning and
transforming the data as necessary and building the data
model, with no or just little code. The advantage is that
involvement of IT is not necessary, which usually speeds up the
whole process: All the requirements are clear to the person who
implements the solution on her own. No infrastructure needs to
be installed (everything runs on the client machine or uses no-
code/low-code services in the cloud).
Everything available in an Enterprise BI solution, on the other
hand, is built with heavy involvement of an IT department.
Servers are set up. Services are deployed or configured. Code is
developed. Automation is key. The advantage is that such a
solution is ready to be scaled up and scaled out. All the

requirements are implemented in one single place (on one of
the servers running the necessary services). But this takes time
to build. Sometimes collecting all the requirements and writing
down the user stories for the engineers to implement can or
will take longer than it would take for the domain expert to
build a solution on her own.
No serious organization will trust business intelligence to be
run on a client machine (Self Service BI), only. No serious
domain expert is always patient enough to set up a project to
implement a database and the reports (Enterprise BI).
Therefore, the solution is to play both cards to the benefit of
everybody.
Data needed for the daily tasks of information workers to be
transformed into reports and ad hoc analysis should be
available in a centralized data warehouse. Only here, one
version of the truth can be made available. But there will always
be extra data, which has not made it into the data warehouse
(yet). That’s where Self Service BI has its place.
The question is, how to combine both worlds, so that the
centralized data can be enriched with the extra data by the
domain experts themselves. “Key-Value Pair Tables” will
describe how this can be done in Power BI in a convenient way.

Key Takeaways
In this chapter I described real-world use cases. You learned
about business problems and different concept of how to solve
them:
Binning of values can be done either with a simple lookup
table (which contains all possible values and their bin) and
a physical relationship between the values and the lookup
table. Or you can describe the ranges per bin and apply a
non-equi join between the values and the lookup table.
New tables should only be added to a data model if the
information cannot be added to an existing table. As a
budget is usually on a different granularity than the actual
data is, I took this as use case for a “multi-fact” data model.
There are many aspects you must cover, when you want to
implement localized reports: content of textual columns,
text on the report, currency exchange rates, the names in
the data model and the language of the user interface of the
application you are working with.
Both, Self Service BI and Enterprise BI will always exist side-
by-side. In this chapter you learned about the challenges of
how to combine both worlds. In Chapter 8 you will see how
both worlds can live together in Power BI.

In later chapters you will learn how to implement the solutions
in DAX, Power Query and SQL. But first, I introduce you to ideas
and concepts which allow you to optimize a data model for
performance.

Chapter 4. Performance Tuning
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 4th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
You are very blessed if performance tuning was never a topic in
a report you built. Usually, it is not a question if, but only when
the performance of a business solution becomes a priority.
Generally, if you took everything we discussed in this book so
far seriously, and transformed all data into a star schema data
model, then you made an important step forward towards good
performing reports. The shape of the data model plays an

important role when it comes to performant reports. But, of
course, there are many more pieces, which play a role in how
fast reports will react to show the first data or react to filters. As
this book is about data modeling, I will limit myself to only
discuss performance tuning topics around data modeling.
My first computer had a Turbo button on the front part of its
case, just next to the power button. I only used it rarely in the
first weeks, but sooner or later I asked myself why I should run
everything at a lower speed? The same applies to the data
model you build. You should always build it with thoughts
about good performance in the back of your mind, because,
why should you build a model, where everything runs at a
lower speed?
Unfortunately, there is no Turbo button in Power BI you can just
hit after you powered on the application. But there are some
concepts you can apply. Read on to learn about them.
If you are about my age, you maybe, in your youth, had a piece
of paper with the telephone numbers of your close family,
friends, and neighbors listed. Mine had the most important
people first, and later I added more and more people to it.
When the list got a decent length scanning through it every
single time when I needed a number bored me. So, I started a

new list and split the names and numbers on different papers:
One paper per letter in the alphabet, with the numbers of all
people with the same first letter of their first name together and
the papers in alphabetical order as well.
This principle applies to databases as well: You can either
create simple tables, where new data is (chronologically) added
at the end of the table (or in between rows, after a row was
deleted). It is very fast to add data to the table, because there is
no need to find the right place, but just use the next empty
space. But you pay a penalty when you read from the table, as
the full content of the table has to be scanned every time you
query it. Filters will only reduce the result set, but not the
necessity of reading and evaluating every single row (to find
out if it satisfies the filter condition or not).
The alternative is to store all the rows in a table in a certain
order. As long as your filter is about the order key, finding the
right rows can be faster: you ignore all the rows which do not
fulfill the search condition, return all matching rows and stop
as soon as non-matching rows appear in the table. In reality this
can be even faster, as databases store extra information (“meta
data”) about the data just for the sake of making queries faster.
But writing data into such a table will be a bit slower: The right
position for the new rows has to be found. Maybe new space

must made available at this position. And the meta data has to
be maintained.
The examples I’ve just mentioned should make clear the very
important principle: You can exchange query speed with space
on disk or in memory. Speeding up queries this way will likely
slow down the write operations. In analytics it is most often the
case that reading the data is done often and should go fast,
while refreshes (= write operations) can be done on only
scheduled points in time, and are fewer in numbers. Optimizing
for read-operations is therefore a good idea, even when it slows
down the write-operations.
You can choose one of the following options as a way to store
data in tables:
Only storing queries
Storing query results
Adding meta data about the data
Adding data of different granularity to the data model
Only Storing Queries
You could opt for not physically storing (duplicating) data but
keep the data in the original tables. Instead, you store only the

query which will return the data in the desired shape. The
advantage is that no extra space is needed to store the (shaped)
data and no steps to update the data has to be scheduled. The
query result will always be fresh. Depending on the type of
transformations and the way the source tables are stored, the
query will need some time to run.
Storing Query Results
Instead of running the queries against the data source every
single time you need the data, you could store the result in a
table and schedule a refresh. This will occupy space on disk or
in memory, but speed up the queries, as the result of the
transformations is already persisted. The challenge is to
schedule the refresh often enough, so that the reports do not
show stale data.
Adding Meta Data About the Data
Here we can distinguish between meta data automatically
added by the database system (like descriptive statistics about
the data) and meta data explicitly added (like indexes). A
database index is the same as the index at the end of this book.

Instead of scanning (= reading) the whole book and looking for
the term “foreign key”, you can jump to the index pages, where
the important terms are ordered alphabetically. You will
quickly find out, if the book talks about this term and find
references to the book’s pages where you find more
information about the term “foreign key”. While the book itself
is “ordered” by its chapters, the index is ordered by an
additional key. For tables it is not uncommon to have more than
one index.
Adding Data of Different Granularity
to the Data Model
Querying a table by its sort order or over an additional index
will be faster compared to not having such. But still, a query
needs to collect the necessary information and calculate the
aggregated values for the report (which typically does not show
single transactions but data grouped by dimensions). Of course,
it would be faster if those aggregated values are already stored
(= persisted) in a table. This is what aggregation tables are
about: They store the identical information as the base table,
but on different levels of granularity. For the report the table
with the right level of aggregation will be used.

No matter which solution you want to implement, all of them
are a strategy to exchange disk space (and therefore increasing
the durtion of time to process the transformation and refresh
the user-facing data model) with query runtime.
Key Takeaways
A good data model takes query performance into account. By
following the principles of the earlier chapters, you already
created a good performing data model. No matter which data
model you design or which tools you are using, you have a wide
variety of possibilities to control the performance, by applying
a taste of three options:
Directly querying the data source will always return the
freshest information. But query time might not be
acceptable (due to complex transformation or a data source
not designed for these ad-hoc queries).
We can speed up queries by pre-computing all or parts of
the data needed. Transformations can be persisted;
statistics and indexes will help to find information faster
and we can pre-aggregate data on different levels of
granularity. This takes up extra space in the database and

needs to be maintained regularly, so it does not contain
stale data.
By cleverly trading off query time and space used for the
persisted data, you can achieve a balanced system, which
satisfies the needs for fast reports and available storage
resources.
With the next chapter we leave the world of concepts and dive
into Power BI Desktop and its possibilities to create the data
model, which will make the live of your report-creators easier.

Part II. Data Modeling in Power BI
Figure II-1. Example navigation
This second part of the book concentrates on the specialities of
Power BI’s features when it comes to data modeling. I will walk
you through the basic concepts, and different problems and
their solution. In this part I will keep DAX, Power Query and
SQL out of the game, as far as possible - as their specialities will
be discussed in the other parts of this book.
First, I will introduce you to the Model view and its parts in
Chapter 5:
Tables
Columns

Relationships
You will learn that Power BI does not use the terms Primary key
Foreign key but they still play an important role when it comes
to relationships and their Cardinality.
In the Model view you will not combine tables, but only define
their relationships. Still it is important to understand how to
build a data model, which is easy to understand and allows for
performant queries later. This part’s Chapter 6 is all about
building a data model, which works optimally in Power BI:
Finding the right way of normalizing and denormalizing
tables.
Telling Power BI the formulas of your calculations.
Providing Power BI with a a date (and optionally a time)
table.
Implementing a solution for tables, which play more than
one role inside the data model
Taking care of Slowly Changing Dimensions
Combining columns into a hierarchy
I will pick up the real-world use cases discussed in Chapter 3
and show you which buttons you need to press in Power BI’s
Model view (Chapter 7):

Binning values
Multi-fact data model (a data model that contains more
than one fact table)
Multi-language data models
Key-Value pair tables
The last chapter in this part (Chapter 8) will talk about the
Model view’s options to achieve a good performing data model:
Basically you can decide if you want to store a copy of all the
data in Power BI and refresh it regularily, or if you want to
query the data source every time a visual is shown. You will
learn about the advantages and disadvanteges of these Storage
modes.

Chapter 5. Understanding a Power BI
Data Model
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 5th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
In this chapter you will learn how to create a useful data model
in Power BI (and Analysis Services Tabular). This chapter will
concentrate on the features of the Model view. The following
parts of this book discuss options of bringing data of any shape
into the desired shape in Power BI (which has been described in
general in Chapter 1).
You will learn that Power BI needs a data

model to work. I will go into details about the properties tables
can have and how you can put them into relationships with
each other. As you will find out, there is no need of explicitly
marking primary and foreign keys, but you still need to be able
to identify them to create appropriate relationships. The
cardinality of the relationships play an important role in Power
BI. Lucky enough, you do not need to think about the joins and
join path problems too much. You only need to create
relationships for your data model. Power BI will automatically
use these realtionships to join the tables appropriatly when the
data is queried for a visual. Power BI will also make sure to
execute requests against the data in a way that the join path
problems (described in “Join Path Problems”) do not occure.
In this chapter I will, again, make clear why a single table is not
a data model fit for Power BI and that a dimensional model is
the go-to solution. Remember: The ultimate goal is to create a
data model, which makes the report creator’s life easy.
Data Model
To get a visual overview and most of the options needed to
create and modify the data model, you need to select the Model
view in Power BI (or the Diagram View in Visual Studio in case

of Analysis Services Tabular). This view looks much like an
Entity-Relationship-Diagram (ERD), but has subtle differences,
we will discuss during this chapter.
The tab named All tables
shows each and every table in the data model, as shown in
Figure 5-1. For bigger data models (read: data models with a lot
of tables) it makes sense to create separate layouts for only
selected tables of your data model. This will give you a more
manageable view for different parts of your data model. For
example, if your data model contains several fact tables, it
might be a good idea to create a separated view per fact table
(and all the connected dimension tables), as an alternative to
the All tables view. While in the All tables view not all tables
might fit on your screen (or only, if you zoom-out so much that
you can’t read the names of the tables and columns anymore), a
separated layout with less content can be helpful.
You can create such a separate layout by clicking on the +-
symbol, just right of All tables. Then, you can add individual
tables from the fields pane (on the very right of the screen) via
drag-and-drop. Alternatively, you can right click a table’s name
(either in the model’s canvas or in the fields pane) to add not
only the table itself, but all the tables with a relationship to it, as
well (Add related tables).

Figure 5-1. Model view
The Model view has three properties:
You can decide to Show the database in the header when
applicable. This is applicable in data models in DirectQuery
mode and Dual. You will learn about the different storage
modes in Chapter 8. This setting is turned off by default.
Each table can be either expanded to show every column of
the table or be collapsed. When a table is collapsed, no
columns are shown, unless you ask to Show related fields
when card is collapsed. Related fields are columns, which
are used in relationships. This property is enabled by
default.

As you might guess, if you Pin related fields to top of card,
columns being part of a relationship are shown on top of
the field list. By default, this setting is disabled, and all fields
are shown in alphabetical order (measures are listed after
the columns, again in alphabetical order).
Tables play an important part of the data model. Let’s look on
their properties in the next section.
Basic Concepts
This section will introduce you to the basic concepts of a data
model, which are:
Tables and columns
Relations
Primary Keys and Foreign Keys
Cardinality
Tables
Every rectangle in the Model view represents one table in the
data model. In the header you can read the table’s name. Below,
the columns (= fields) are listed, in alphabetical order.

A table offers a list of functionalities, which you can access by
either right-clicking the table name or by left-clicking the
ellipses (…​), as I did in Figure 5-2.
Figure 5-2. Model View Context Menu
You have many options, when it comes to :

Add related tables will add all tables which have a filter
relatinship with the chosen table to the model view. This
option is only available, when the table has relatinships to
other columns and only in a layout view (not in All tables).
You can create a New measure or a New column within the
selected table. In “Calculations” you will learn more about
these two options.
You can choose Refresh data to refresh the content of the
whole table. The data source has to be available at this
point in time.
Edit query will open the Power Query window. In
Chapter 13 I will introduce to the capabilities of Power
Query.
In “Relationships” you will learn everything about the
options to Manage relationships.
Chapter 8 covers all the details of Incremental refresh and
Manage aggregations.
With Select columns you can select all columns in this table.
You can then change the properties for all of them in one
go.
On the other hand, Select measures will select all measures
in this table. You can then change the properties for all of
them in one go.

If you choose Delete from model, the whole table will be
removed not only from the layout view, but from the whole
file (incl. Power Query).
WARNING
Be careful, this step cannot be undone. Make sure to save intermediate versions
of your changes to have a backup.
With Hide in report view the table (and all its columns) are
hidden. The goal is to not overwhelm the report creators
with tables (or columns) holding intermediate results, not
intended to be shown in a report.
WARNING
A hidden table (or column) can still be seen by the user if the user enables View
hidden. Therefore, this is not a security feature: Don’t use it to hide tables with
sensitive content. Use it to hide tables, which are only helping to build the data
model, but which do not contain any content, which should be used in a report
(e.g., to show the value, to filter on it, etc.).
The option Remove from diagram is only available in
layouts, but not in tab All tables. This does not remove the
table from the whole file, but just from the current layout.
With Unhide all you can disable the property Hidden for all
elements within this table. Again, this step cannot be

undone. In case of a mistake, you need to hide the
individual columns again.
Collapse all collapses the height of the table to only show
column which are used in relationships (or no column at
all, depending on the overall settings).
Expand all expands the height of the table to its original
size.
In Power BI and Analysis Services Tabular a table can contain
not only columns, but measure and hierarchies as well.
Measures are written in the DAX language. In Chapter 9
through Chapter 12 I will show you many capabilities of the
DAX language and in Chapter 10 you will find more information
about measures in the section about calculations. Hierarchies
group several columns into one entity. There is a whole section
dedicated to hierarchies in Chapter 6.
Tables have properties in Power BI, as you can see in Figure 5-3:


Figure 5-3. Model view table properties
Name
This is the name of the table. You can change the name
either here or by renaming the Power Query associated
with this table in the Power Query windows (which I
discuss in “Tables or Queries”).
Description
The description is shown in a tooltip when you (or the
report creatore) move the mouse cursor over the table’s
name in the Data list. Typically I use this field to include a
few sentences to describe the content of the table.
Synonyms
This field is automatically propagated by Power BI as a
comma separated list. You can add your own synonyms as
well. The list of synonyms helps the Q&A (question and
answer) visual to answer your questions. You should type
in alternate terms used in your organization (e.g., some
people may use revenue as an alternate name for table
sales; you would then enter revenue as a synonym for
sales). Visuals are out of scope for this book. You will find

an in-depth description of the Q&A visual in my book
“Self-Service AI with Power BI Desktop”, though.
Row label
A correct content of this field helps the Q&A visual when
you reference a table. It will then show the content of the
column you specify here. Select the column containing the
name of the entity as the Row label of the table. It helps in
Excel in a similar way, too (Set featured tables in Power BI
Desktop to appear in Excel)
Key column
Again, this feature helps Excel. Select the column
containing the primary key as the Key column of a table.
You can only select a column which does not contain any
duplicate values.
Is hidden
Enabled means that the table is hidden by default, disabled
means that the table is shown by default. You should hide
all tables which do not contain content relevant for the
reports, but which are needed to create the data model in
the right shape, e.g., bridge tables.

WARNING
The user can still View hidden elements. Therefore, keep in mind that this is
not a security feature.
NOTE
Technically, tables cannot be hidden in Power BI or Analysis Services Tabular,
but columns only. If a table contains hidden columns only, then the table is
hidden as well. So, hiding a table, changes the Is hidden property of all
columns within the table. If you choose to unhide a table, all columns will be
visible (no matter if they have been hidden or not before you changed this
setting on the table level).
Is featured table
Makes the table easier to find in Excel as it will show up in
the Organization Data Types Gallery (Set featured tables in
Power BI Desktop to appear in Excel). Tables where this
setting is disabled can still be found via Analyze in Excel
(Create Excel workbooks with refreshable Power BI data).
Storage Mode (Advanced)
The storage mode of a table can be either Import,
DirectQuery or Dual. You will learn more about using the
storage mode to your advantage in Chapter 8.

Columns reside in tables and have properties as well, as you
can see in Figure 5-4. In the Model view you set the following
properties:


Figure 5-4. Model view column properties
Name
This is the name of the column. You can change the name
either here or in the Power Query window. You will learn
about Power Query in “Tables or Queries”.
Description
The description is shown as a tooltip, when you hover
over the column in the fields list in the Data pane.
Typically I add a few sentences to describe the content of
the column or the formula of a DAX measure.
Synonyms
The provided synonyms are used by the Q&A visual to
find the columns not directly referenced by their
(technical) name, but by alternative names and versions
as well (e.g., a user might ask Q&A about revenue, but the
column is called Sales Amount ; you would then add
revenue  in the list of synonyms for the column Sales
Amount ).
Display folder

In tables with a lot of elements (measures, columns,
hierarchies, etc.), browsing through the list can be tedious.
Putting a column (or a measure) into a folder provides
some structure. To put the column Sales Amount  into a
folder called Sales  you would just enter “Sales” into the
Display folder. You can even create subfolder by using the
backslash (“\”) in this field. Etc. “KPI\Sales” puts the
column into a folder KPI  and there into a subfolder
Sales .
Is hidden
Hides the column. You should hide all columns which we
need for the data model only (e.g., all keys), but
should/will never be shown in a report.
WARNING
The user can still View hidden elements. Therefore, keep in mind that this is
not a security feature.
Data type (Formatting)
Every column has a dedicated data type. I discuss the
available data types later in this section.
Format (Formatting)

You can choose a format, in which the value of the column
is shown. The available formatting options are dependent
on the data type, of course: Percentage format, Thousands
separator, Decimal places, Currency format, etc.
NOTE
Changes to the format do not change the data type (= internal storage) of the
column. For example, if you want to get rid of the time portion of a
timestamp, you could change the format to only showing the date portion.
But then the time portion is still stored in the data model (which is expensive
in terms of storage space, as you learned in “Tables” and might lead to non-
working filters, if the dimension table does not contain the time portion, but
the fact table does, for example). If nobody will ever report on the time
portion, then it is a good idea to change the data type to Date instead.
Sort by column (Advanced)
By default every column is sorted by the value it contains.
In some situations, this can be unpractical. Typically, you
do not want a list of month names sorted alphabetically,
but by their month number. This is what this property is
for. You would select column Month Number  as the Sort
by column of Month Name . You can also use this option to
show names of categories or countries in a specific order.
For every value of the column, only a single value of the
Sort by column must be available; you cannot sort the

Month Name  by the Date  or by a month key, which
contains both, the year and the month number. In other
terms, the relationship between the column and the Sort
by column must be of a one-to-one or a many-to-one
cardinality but can’t be a one-to-many or many-to-many.
Data category (Advanced)
Assigning a Data category to a column allows Power BI to
default to a certain visual, when this column is used. e.g.,
if you mark a column as a Place, Power BI will suggest
showing the content on a Map visual. It also gives Power
BI the information, what the content means (like if a two-
letter code is identifieng an US state or the the ISO country
code).
Summarize by (Advanced)
I consider this setting to be aimed for newcomers to
Power BI’s data modeling capabilities. It allows you to
specify an default aggregation function which should be
applied when you add a numerical column into a visual.
For example, if you add the SalesAmount  column to a
visual, you usually do not want to get a (long) list of all the
rows of the Sales  table showing the individual
SalesAmount  value, but you want to sum up the

numbers to a total. You can override the data model’s
default aggregation setting per visual (e.g., showing the
average of the SalesAmount ). Summarize by allows you
to specify, how the values are aggregated: Sum, Average,
Min, Max, Count, and Distinct Count. For numeric values,
which you do not want to aggregate (e.g., the year or the
month number), you must specify None.
Any setting different from None will create a so-called
implicit measure in the data model, containing the chosen
aggregate function. Unfortunately, implicit measures do
not work with all client tools (they work in Excel first
since 2023). Implicit measures are automatically disabled
as soon as you create Calculation groups (you will learn
about them in Chapter 10 in the section about calculations
in DAX), breaking existing reports, which depend on those
implicit measures.
My strong recommendation therefore is, to explicitly
creating measures for columns, where you need
aggregations to be applied, and set Is hidden to Yes for
such columns. (You will learn more about explicit
measures in “Calculations” as well.)
Is nullable (Advanced)

Specifies, if the column may contain blank (or null, as it is
called in relational databases). If you consider blank in
this column as a data quality issue, then you should turn
this setting to No. This will lead to an error during the
refresh in case a row actually contains blank for this
column.
Every row of a column must conform to the
column’s datatype in Power BI. Let’s take a closer look
onto the different data types, Power BI allows to choose
from:
Binary
This data type is not supported and exists for legacy
reasons only. You should remove columns of this datatype
before loading into Power BI and Analysis Services or just
delete them from the model in the Model view.
True/false
A column of this type can contain Boolean values: True, or
false. But this data type is no exception in the sense that it
additionally can also contain blank. Blank represents an
unknown value. In a note below I will provide you more
information about blank.
Fixed decimal number

This datatype can contain numbers with up to four
decimals and up to 19 digits of significance. You can store
values between -922,337,203,685,477.5807 and
+922,337,203,685,477.5807. These 19 digits are identical to
the Whole number, as a Fixed decimal number is stored in
the same internal format, but with the extra information
that the last four digits are decimals. For example, the
Whole number 12345 and the Fixed decimal number 1.2345
are stored in the exact same internal representation, with
the only difference that the Fixed decimal number is
automatically divided by 1000 before it is shown. Due to
the limit to four decimal places, you can face rounding
error, when values are aggregated.
Decimal number
Is 64bit floating point number which can handle very
small and very big numbers, both in the positive and
negative spectrum. As it is only precise to up to 15 digits,
you may face rounding error, when values are aggregated.
Date/time
Represents a point in time, precise to 3.33 milliseconds.
Internally in a database all date and time related data is
stored as a decimal number counting the days since an

specific point in time. (In case of Power BI, this point in
time is midnight of December 30, 1899). The decimal
portion represents the parts of the day (e.g., 0.5 represents
12 PM). I am pointing this out to make sure that you do
not make the mistake of thinking that a date/time is stored
in a certain format (e.g., “August 01 2023 01:15:00 PM” or
“2023-08-01 13:15:00”). As already pointed out, the Format
properties task is to put a value into a user-friendly
format humans can read, but does not change the internal
representation (which is 45,139.55 in Power BI for the
given example – and would obviously be not very user-
friendly to show in a report).
Date
Represents a point in time, without the time portion.
Everything mentioned for data type Date/time also applies
here. Internally this data type is represented as a whole
number (e.g., 45,139 to represent August 1, 2023).
Time
Represents a point in time, without the date portion.
Everything mentioned for data type Date/time also applies
here. Internally this data type is represented as a decimal

number with only the decimal portion (e.g., 0.55 to
represent 1:15 PM).
Text
Holds unicode character strings, which can hold up to
268,435,456 characters.
Whole number
Values of this data type are stored as an 64bit integer
value. This data type does not have decimal places and
allows for 19 digits. It covers the spectrum between
9,223,372,036,854,775,807 and + 9,223,372,036,854,775,806.
NOTE
For databases it is typical that every data type supports also an “unknown” value. In
relational databases and in Power Query this is represented by null, in DAX by blank.
It is important to understand that this unknown value is different from an empty
string, the numeric value zero (0) or a date value of January 1 1900. The information
that something is unknown might be an important fact (and should not be set equal
to some default value). In a user-friendly data model, an unknown value should be
replaced by something which explicitly tells that the value is unknown (e.g., string
“N/A” or “Not available”), as you already learned in “Tables”.
In many data models, tables don’t live just by themselves, but
contain information which is somehow in relation to

information in other tables. Read on to learn more about this
kind of relationships.
Relationships
Relationships in Power BI connect tables with each other and
look like foreign key constraints in a relational database, but
work differently. While foreign key constraints limit possibilities
(they prohibit having a value in a foreign key column, which
cannot be found in the related primary key column),
relationships in Power BI exists solely to propagate a filter from
one table to another. If you filter the Date table on a certain
year, the filter relationship will propagate this filter to the Sales
table, so queries only show sales for the specified year. Their
effect enables what we usually perceive as something very
natural. But for this natural thing to happen, we must help
Power BI by setting the relationships right.
Creating filter relationships is rather easy. Power BI offers three
methods, which all lead to the same result:
Automatic creation
Power BI can automatically create and maintain filter
relationships for you, when loading new tables. Under File
– Options and Settings – Options – Current File – Data Load

you can find three options related to Relationships (s.
Figure 5-5). You can Power BI let import the relationships
from a data source on first load (when the data source is a
relational database and the foreign key constraints are
available). You can let Power BI maintain those
relationships when refreshing data. And Power BI can
also autodetect new relationships after data is loaded (by
applying a set of rules: the column names must be the
same, the data types of these columns must be the same,
and in at least one of the two tables the column’s value
must be unique).
WARNING
If your source system follows the rule to name all primary keys e.g., “ID”, then
the automatic detection of relationship will end in a relationship chaos, as
Power BI will most probably start creating relationships between all those
columns. Either turn this feature off or change the naming convention to add
the table’s name to the key fields (“ProductID” instead of just “ID”, or similar).

Figure 5-5. Options – Curent File – Data Load
Drag and drop
A very simple way of creating a relationship is to drag one
column over another (from a different table) in the Model
view. In theory, it does not matter which of the two
columns you drag over the other one. The cardinality and
filter direction (s. below) are automatically set for you. It
is though always a good idea to double-check if all

properties of the created relationship are as they should
be. I’ve seen it more than once that Power BI created a
many-to-many (due to unintended duplicates in a table) or
a one-to-one relationship (due to uncomplete test data
with only e.g., one order per customer), where it should
have been a one-to-many cardinality instead.
Dialog box
Via the ribbon you can choose Home – Manage
relationship to open a dialog box from which you can
create a New relationship, start Autodetect (as described in
the paragraph before), Edit or Delete an existing
relationship (s. Figure 5-6). By clicking on the checkbox
Active you can (de-)activate a relationship. I explain this
feature below (Make this relationship active).
TIP
The order the relationships are shown looks unpredictable to me: A
relationship between the Date and the Sales tables might show up with the
Date table first (and ordered by it) or with the Sales tables first (and ordered
by that). If you cannot find the relationship you are looking for, double-check
if can find it listed with the other table first in this dialog box.

Figure 5-6. Modeling – Manage Relationships
TIP
As filter relationships are so important, I would not rely (only) on automatic creation.
Even if you let Power BI create the relationships for you in first place, I would make
sure to review every single one, to assure that no relationship is defined in a wrong
way (read: check the cardinality) that no relationship is missing and that no
unnecessary relationship was created.
Figure 5-7 shows you the property pane in the model view and
the Edit relationship dialog for the same relationship.

Figure 5-7. Model view relationship properties
A filter relationship in Power BI consists of the following
properties:
First table
Sometimes also described as the left table. It is one of the
two tables, for which you create a relationship. Which
table the first/left table is, is not important, as any one-to-
many relationship can also be seen as a many-to-one
relationship – they are identical.
Column in the first table

You can click on the column name shown for the first
table to select one or choose from the listbox. The Edit
relationship dialog windows shows a preview of the
values of the first three rows. Selecting more than one
column is not possible, as Power BI does not allow to use
composite keys. In cases where you must work with
composite keys, you need to simply concatenate the
values of those columns (as a DAX calculated column, in
Power Query, SQL or the data source) before you can
create the relationship. I strongly recommend adding a
separator character in-between the column values to
avoid “false twins”. For example, if you concatenate “12”
and “34” for one row, and “1” and “234” for a different
row the resulting concatenated string would be “1234” for
both examples. If you put a separator character in-
between, you will get “12|34” in one case, and “1|234” in
the other.
Second table
Sometimes also described as the right table. It is one of the
two tables, for which you create a relationship.
Column in the second table

You can click on the column name shown for the second
table to select one. The Edit relationship dialog window
shows a preview of the values of the first three rows.
Selecting more than one column is not possible.
Cardinality
Cardinality describes, how many rows in the other table
can maximally be found for a single row of one table.
“Cardinality” keeps you fully covered on this topic.
Cross filter direction
As explained, the sole purpose of a relationship in Power
BI is to propagate a filter from one table to another. A
filter can go in either direction, or even in both directions.
I strongly advise sticking to the best practice of only using
single-directed filters. These filters are propagated from
the one-side of relationship to the many-side of a
relationship. Other filter directions (especially the bi-
directional filter) might lead to ambiguous data models,
which Power BI will prohibit you to create, and/or poor
report performance.

WARNING
Bi-directional filters are sometimes used to create cascading filters (where a
selection of a year limits the list of products in another filter to only those
where there have been sales in the selected year). I strongly advise you to
solve this problem through a filter in the slicer visual instead: just add e.g.,
the Sales Amount measure as a filter to the slicer visual and set the filter to Is
not blank. Now any filter’s result will cascade into this slicer. Repeat this for
every slicer visual.
Make this relationship active
A maximum of one single active relationship can exist
between the same set of two tables in Power BI. The first
created relationship between two tables is active by
default. If you create additional relationships between
these two tables, they can only be marked as inactive. In
the model view you can distinguish active and inactive
relationships by how the line is drawn: Active
relationships are represented by a continuous line, while
inactive relationships are drawn as a dashed line.
In
Chapter 9 you will learn how you can make use of
inactive relationships with the help of DAX. In Chapter 10
in the section about “Role-Playing Dimensions” I will show
you alternatives to having more than one relationship
between two tables.

Apply security filter in both directions
This setting is only relevant, if you have implemented
Row Level Security (RLS) in the data model and use
bidirectional filters (which is not recommended; s. above
“Cross filter direction”). Propagation of Row Level Security
is always single-directed (from the table on the one-side to
the table on the many-side), unless you activate this
setting. Learn more about RLS in Microsoft’s online
documentation (Row-level security (RLS) with Power BI).
Assume referential integrity
This property is only available when using DirectQuery
(you learn about DirectQuery in Chapter 8). Activating this
checkbox will let Power BI assume that the columns used
for this relationship have a foreign key constraint in the
relational database. Therefore, Power BI can use inner
joins (instead of outer joins) when querying the two tables
in one single query. Inner joins have a performance
benefit over outer joins. But with inner joins, rows could
be unintentionally filtered out, when referential integrity
is violated by some rows, as you learned in “Joins”.
Independent of these relationship settings, joins in Power BI are
always outer joins (except for DirectQuery, when Assume

referential integrity is enabled). This guarantees under all
circumstances that no rows are unintentionally lost (even when
referential integrity in the data is not guaranteed). Missing
values are represented as Blank.
The Power BI data model does not allow for non-equi joins. In
“Binning” I will show you ways of implementing non-equi joins
with the help of DAX.
Many, but not all, relationship are built on a primary key in one
table and a foreign key in the other table. Let’s start how Power
BI handles primary key in the next section.
Primary Keys
In Power BI you do not explicitly mark primary keys (except
when using DirectQuery to benefit from a better report
performance). Implicitly, any column used in relationships on
the one-side is a primary key. If the column on the one-side
contains duplicated values, then the refresh will fail. Empty or
blank values for a column on the one-side are not allowed. I
strongly encourage you to make sure in the ETL to have no
blank values anywhere, neither in key columns nor other
columns, but to replace them with a meaningful value or
description (like “not available”). In “Flags and Indicators”,

“Flags and Indicators”, and “Flags and Indicators” you learn
different ideas of how to achieve this.
Power BI’s data model does not allow composite keys. In case
you decided for a composite key you need to concatenate all the
columns participating in the key into one single column
(usually of type Text). Make sure that you add a separator
character between the values. Look for a (special) character
which will never be part of any of the column’s values (e.g., a
pipe symbol | when concatenating names). This makes sure
that the result of concatenating “ABC” and “XYZ” is different
from concatenating “ABCX” and “YZ”. With the separator you
get “ABC|XYZ” in one case and “ABCX|YZ” in the other. Without
the separator you would end up with the identical primary key
“ABCXYZ” for both rows, which is problematic as Power BI can
then not distinguish those different two rows from each other.
Surrogate Keys
In Power BI a relationship can only be created on two single
columns in two separate tables, Power BI does not allow to use
composite keys. I strongly advise to use columns of type Whole
number for the relationships, as they can be stored more
efficiently (compared to the other data types, e.g., texts) and will
therefore make filter propagation happen faster (which leads to

faster response time in the reports). While the key of the source
system could be of any type, a surrogate key is usually a Whole
number. This makes them perfect keys for Power BI. Learn
more about creating surrogate keys in “Surrogate Keys”.
An important reason to have primary keys is, to reference a
single row in this table. The column in the referencing table is
called a foreign key. Learn more about it in the next section.
Foreign Keys
You do not explicitly define foreign keys in Power BI. They are
implicitly defined when you create relationships. A column on
the many-side of a relationship is the foreign key.
In case you decided for a composite key as a primary key you
need to concatenate all the columns participating in the foreign
key as well. Make sure that you concatenate the columns in the
very same order as you did for the primary key and to use the
same separator character.
When it comes to primary and foreign keys, you should be
prepared to know how many rows in the table containing the
primary key are a available for a single foreign key, and the

other way around. This is called Cardinality and covered in the
next section.
Cardinality
For every relationship you also need to specify its cardinality.
Power BI offers three types of cardinalities:
One-to-many (1:m, 1 - *)
One-to-one (1:1, 1 - 1)
Many-to-many (m:m, * - *)
All relationships in Power BI are automatically conditional.
That means that it is allowed that a corresponding row in the
other table is not available: For Power BI it is OK, when there is
no row in the Sales  table for a specific Customer . This is also
OK in the real-world, as a brand-new customer might not have
ordered yet. But it is also OK for Power BI if no customer can be
found for a CustomerID  in the Sales  table. In the real-world
this would be an issue in most cases: Either no CustomerID
was stored for a sale. Then you need to clarify with the business
if this is indeed possible (for edge cases). Or the CustomerID
provided in the Sales  table is invalid. That would be a data
quality issue you would need to dig deeper into. Because if the
CustomerID  is invalid for a row, who knows if the

CustomerID ’s for the other rows are just valid by random, but
contain the wrong information?
WARNING
Keep in mind that Power BI will create a many-to-many relationship not only for the
classical many-to-many relationships (e.g., one employee works for many different
projects, one project has many employees), but in all cases, where none of the two
columns used for creating the relationship only contains unique values. In case of
data quality issues (like if there is duplicated customer row or you have more than
one row with a blank CustomerID ), Power BI will not let you to change the
relationship to a one-to-many.
Relationships of many-to-many cardinality are called weak or
limited relationships, as they come with two special effects,
which are reasons why you should avoid this type realtionships
(and using a bridge table instead, as discussed in “Types of
Tables”):
In case of missing values in the dimension table or wrong
foreign keys in the fact table, no blank rows are shown in
the reports to represent the values for the missing/wrong
keys. Instead, these values are not shown at all. Reports and
the totals might show incomplete numbers. This effect only
hits you in case of data quality issues. Making sure that
there are no missing rows in the dimension table and that

there are no invalid foreign keys in the fact tables is a good
idea anyways.
Calculations in DAX which use function ALL  (or
REMOVEFILTERS ) to remove filters will not remove filters
on tables connected over a limited relationship. This can be
a trap when you ask to Show value as - Percent of grand
total or when creating more complex measures in DAX. As
report creators can create measures (containing function
ALL  in their expression), this problem can appear anytime
and can only be avoided by avoiding many-to-many
relationships. That’s why I try to avoid many-to-many
relationships. Find explanations about the specialties of
many-to-many relations ships at Apply many-to-many
relationships in Power BI Desktop. In case of very large
tables, many-to-many relations might have a better
performance compared to the solution with a bridge table,
though.
In the Model view you can easily spot such problematic
relationships: They are represented with paranthesis-like
marks after the cardinality indicators. (as you can see in
Figure 5-8). The solution to avoid all these effects is to model a
many-to-many relationship via a “bridge” table. You will learn
this technique later in “Types of Tables”.

Figure 5-8. Many-to-many relationships are limited relationships, drawn with gaps at
both ends.
TIP
Always make sure that you understand how the relationship between two entities
are in the real world. In cases you can’t set the configuration for a relationship in
Power BI accordingly, double-check the reason for that. Don’t just carry on but clarify
the business’ requirement and the data quality.
Combining Tables
A data model usually contains more then one table. This
sections discusses, how you can combine the information of
several tables back again.
Set Operators
Set operators are not available directly in the data model of
Power BI, but when querying data only. Jump to the other parts
of this book to learn about if and how to implement set

operators in DAX (“Set Operators”), Power Query/M (“Set
Operators”), and SQL (“Set Operators”).
On the other hand, joins
are a regular thing when working with data models in Power
BI. Read on, to learn more about that.
Joins
Joins are implemented implicitly over the data model’s
relationships. The filter relationships stored in the data model,
are implemented automatically by the Power BI’s storage
engine to perform necessary joins. That means, if you create
reports or write queries in DAX, there is no need to (explicitly)
specify a join operator, as this is implicitly done by the storage
engine for you (and the report user). The filter relationship
defines the join predicate (= which two columns are involved).
The predicate is always done as an equi-join (= the values of the
two columns must match). You cannot define non-equi joins in
Power BI’s data model. In Chapter 9 you will learn how to
implement queries for non-equi joins with DAX. You can also
perform non-equi joins in Power Query/M and SQL, to join
queries and load the result as one single table into Power BI.
The natural join is simulated by Power BI’s ability to create the
relationships in the data model automatically for you. If the

filter relationship is not explicitly defined in the data model,
then no natural join will happen, when creating reports.
Unfortunately, you cannot create self-joins at all. In the section
about hierarchies (in chapters 10, 14, and 18), you will learn
how to flatten parent-child hierarchies, so you are able to
report on such hierarchies.
By default, all joins in Power BI are implemented as outer joins.
This guarantees that no rows are lost, even when referential
integrity of the model is not guaranteed. In relational databases
outer joins come with a performance penalty (compared to
inner joins). The storage engine behind Power BI was always
built with outer joins in mind – so there is no performance
penalty to be expected. And there is also no way of comparing,
as you can’t execute inner joins on data imported into Power BI.
When you don’t import data, but use DirectQuery (on relational
data sources), then it is recommended that you first, guarantee
that referential integrity is in place in the data source and
second, you can tell Power BI so (with the table’s property in the
model view). Then the storage engine will use inner joins
instead of outer joins when querying the data source (and
making use of the performance advantage).

Joins are necessary to bring information, spread over several
tables, back into the result of a single query. Combining the
tables in a query can be tricky – but Power BI covers the usual
problems for you, as you will see in the next section.
Join Path Problems
NOTE
The file used for the demonstrations is Relationship.pbix
No worries, Power BI got you covered on all join path problems:
None of the three problems discussed in “Join Path Problems”
(loop, chasm trap, and fan trap) are an issue in Power BI. But
you can see this for yourself in the following list:
You cannot create a loop, neither directly nor indirectly (via
intermediate tables), as Power BI will not allow you to
create multiple active paths. Power BI will force you to
declare such a relationship as inactive.
Power BI has implemented a logic to avoid the negative
effects of a Chasm trap. The example in Figure 5-9 shows a
report with three table visuals. The table on top left shows
the reseller sales per day. Just below you see the internet
sales per day. On the right, the results of both tables are

combined, per day. The two tables have each a one-to-many
relationship to the Date  table (and therefore a many-to-
many relationship between themselves). The DateKey
column is always taken from the Date  table. As you can
see, none of the sales amount for a day (or for the total) is
wrongly duplicated, but matches the numbers shown for
the individual results.
Figure 5-9. Chasm Trap is not a problem in Power BI
Similarly, Power BI is fail-safe against the Fan trap problem.
In Figure 5-10 you see the Freight  per day (stored in the
SalesOrderHeader  table) and the OrderQty  per day
(stored in the SalesOrderDetail  table, which has a
many-to-one relationship to the SalesOrderHeader

table). In the table visual on the right you see that the
Freight  is not wrongly duplicated per day, but shows the
same values as in the SalesOrderHeader  visual.
Figure 5-10. Fan Trap is not a problem in Power BI
A good way to document the relationship, and therefore the
possible join paths, is to show the data model as an Entity-
Relationship Diagram, as you will learn in the next section.
Entity-Relationship Diagrams (ERD)
The model view in Power BI (and the diagram view for Analysis
Services projects in Visual Studio) are exactly what you would
draw in an Entity-Relationship diagram if you want to

document the tables and their relations. In the model view you
see “1” for the one-side and “*” to represent the many-side of a
relation.
As the model view is not about foreign keys, but about filters, it
additionally shows the direction of a filter, which can go either
into one single direction or into both directions, represented by
a small triangle (for single-directed filters) and two small
triangles (for bi-directional filters).
In Figure 5-11 you can see a single-directed, many-to-one
relationship between AccountCustomer  and Customer  and a
bi-directional, one-to-one relationship between Customer  and
CustomerDetail  shown in Power BI’s model view.

Figure 5-11. Power BI’s Model view
Data Modeling Options
Types of Tables
There is no explicit property for a table to indicate the usage of
the table (e.g., entity table, lookup table, bridge table). I also
have the strong opinion that the type of a (non-hidden) table
should not be indicated by hints in its name (e.g., Fact_Sales or
Dim_Customer). I instead recommend that the names should be
user-friendly for the report creators (who usually don’t care
about what role a table plays in the data model, as long its
returning the expected results).

The role of a table is just given by its relation to other tables.
Fact tables are always on the many-side of a filter relationship.
Dimension tables are always on the one-side in a star schema.
In a snowflake schema they might as well be on the many-side
in a relation to another dimension table. For example, the
Product  table will be on the one-side of the relationship to the
Sales  table (which is on the many-side, as each order line
contains one single product, but the same product will be
ordered several times over time). The Product  table will be on
the many-side in a relationship to the Product Subcategory
table, as many products might share the same subcategory.
In Figure 5-8 you already saw a many-to-many relationship
between tables Budget  and Product . The reason, why this
relationship has a many-to-many cardinality is because the
budget was not created on the Product ’s table granularity, but
per Product Group  instead. The Budget ’s table foreign key
Product Group  is not referencing the Product  tables
primary key ( Product Key ). In table Product  column
Product Group  is not unique, the same Product Group  will
be found in several rows. As the join key is not unique in either
table, Power BI restricts a direct relationship to cardinality
many-to-many.

Table 5-1. Product table with main product category
Product Key
Product Name
Product Category
100
A
Group 1
110
B
Group 1
120
C
Group 2
130
C
Group 3
Table 5-2. A budget it typically on a different granularity than
the actual values
Month
Product Group
Budget
2023-08
Group 2
20000
2023-08
Group 3
7000
2023-09
Group 2
25000
2023-09
Group 3
8000
Relationships of cardinality many-to-many have some
disadvantages in Power BI, as laid out in “Cardinality”. A bridge
table resolves a many-to-many relationship and is put between
two tables, which are logically connected by a many-to-many
relationship. The bridge table is replacing a many-to-many

relationship with two one-to-many relationships. It is always on
the one-side of the two relationships. The content of the bridge
table is a distinct list of key(s) used to join the two original
tables. As the content is only relevant for creating the
relationship, but not for building a report, the bridge table
should be hidden from the user. I usually put the postfix
“BRIDGE” in the name of a bridge table. It makes it easier for
me to spot the bridge tables, and therefore many-to-many
relationships in my data model.
Table 5-3. Bridge table for product categories
Product Category Key
Product Category
10
Group 1
20
Group 2
Figure 5-12 shows the model view of three tables of different
type. The table on the very left is a fact table ( Budget ) and is
on the many-side of the relationship. Right of this table you see
a “bridge” table ( Product Group BRIDGE ), which bridges the
many-to-many relationship between the Budget  and
Product  table. The “bridge” table is on the one-side of both
relationships. The table on the very left is a dimension table
( Product ). It is on the many-side of the relationship, as the

Budget  table is not referencing the Product ’s table primary
key ( Product Desc ), but the non-unique column Product
Group . You will learn more about the specifics of this data
model in “Budget”. Here I just used it to demonstrate different
table types.
Figure 5-12. Tables of different types
Maybe you think, why should you bother with different (kind
of) tables when you can just just store everything into one singe
table? In the next section you will learn, why this is a bad idea,
when it comes to Power BI and Analysis Services Tabular.
A Single Table To Store It All
NOTE
The file used for the demonstrations is Auto-Exist.pbix
While putting all information into a single table is common in
many use cases or tools, and even Power BI allows you to build

such a model, I would strongly discourage you from doing that.
If you think in terms of a star schema, a single table means that
the information of all dimension tables is stored inside the fact
table. There are plenty of reasons, why you should split the
information in at least two separate tables:
Size of model
Because Power BI’s storage engine stores imported data in
memory, Microsoft made sure to compress the data. The
compression algorithm works very well for a star schema,
but replicated dimensional information in every single
row of a fact table does not compress very good. In a
scenario I built to try this out, the single table used almost
three times the space of a star schema. That means that
you can only store a third of the data in a single table,
compared to a star schema on a given infrastructure. And
the more memory the storage engine has to scan, the
longer it will take and the more pressure is on your CPU
as well. Transforming the single table into a star schema
will help to fully benefit from the storage engine. The
model size will be smaller, the reports will be faster.
Adding new information may be difficult

If you want to extend the data model later by adding
additional information, you would either need to
implement a transformation to add the data to the
existing table – which can be dreadful (by aligning the
different granularities of the existing and the new
information) and you would just increase the problems
you are already facing with the single table. Or you would
add the new information as a separated table. This would
only work, if you need to join the two tables on one single
dimensional column, as you can’t have more than one
active relationship. Joining two fact table directly is not
recommended due to the size of the tables. Transforming
the single table into a star schema will make it easier to
extend the model. You would just add new dimension
tables, re-use the already existing dimension tables and
connect new fact tables to the existing dimensions.
Wrong calculations
I guess, everybody wants to make sure that the numbers
reported are the correct ones. Due to some optimization
in the storage engine, queries on a single table might
though result to wrong results, as laid out in the following
example.
Table 5-4 shows a simple and small table,
containing Date , ProductID , Price , and Quantity .

Table 5-4. A simple table containing some sales
Date
ProductID
Price
Quantity
2023-02-01
100
10
3
2023-02-01
110
20
1
2023-02-02
110
30
4
2023-03-03
120
100
5
I then created three measures: One to count all rows of the
Sales table ( # Sales = COUNTROWS('Sales') ) and two
others, where I assume that I want to count the rows of the
Sales  table independently of any filter on the Date  colum.
One version removes all filters from the 'Date'[Date]
column and the other removes the filter from the
'Sales'[Date]  column (by applying function
REMOVEFILTERS() ).
Figure 5-13 shows the formula of [# Sales] and a card visual
next to it, which shows the value of 1. This number is correct:
there was only one single sale for the filtered date of February 2
2023.

Figure 5-13. A report showing the count of rows of the Sales  table
Figure 5-14 shows the formula and the content of [# Sales
ALL('Date'[Date])] , which shows a value of 3. There is one
slicer per dimension: Date  (with the second of the month
selected) and Product  (with ProductID  100 and 110
selected). Measure [# Sales ALL('Date'[Date])]
calculates the expected value of 3, because, if we remove the
filter on the Date  (for the second of the month) we are left
with only a filter on ProductID . For the two selected products
(100 and 110) there are three rows available in the Sales
table.
Figure 5-14. A report showing the count of rows of the Sales  table for all dates
The third section in the report shows a very similar content, but
for measure [# Sales ALL('Sales'[Date])]  and filters on
two columns of the Sales  table ( Date  and Product ID )
with the identical selection as on the dimensions. Unfortunately
[# Sales ALL('Sales;'[Date])]  shows an unexpected

value of 2. Removing the filter from the ‘Sales'[Date] column
should lead to a result of three.
Figure 5-15. A report showing the wrong count of rows of the Sales  table for all dates
The reason why this measure shows an unexpected value is due
to an optimization in the storage engine of Power BI/Analysis
Services Tabular. At the point in time, when the
REMOVEFILTERS()  function kicks in, the Sales  rows with
ProductID  of 100 were already filtered out to speed up the
queries (as there are no Sales  for this ProductID  on the
second of the month), leaving only rows for ProductID  110.
Removing the filter on the 'Sales'[Date]  column does not
recover the rows for ProductID  100. That’s why this measure
only counts the rows for ProductID  110. This effect is not
limited to counting, but affects all kind of measures, as soon as
you start manipulating the filter context, which is very common
even for rather simple calculations. This optimization (and the
negative effect) only happens when filters directly on the fact
table are in place; filters via dimension table are not affected by
this optimization.

That was the long explanation why you should not create a
single table data model in Power BI or Analysis Services
Tabular, if you care about performance, an easy-to-understand
data model and correct numbers. The short version is: Always
create a dimensional model, never put all columns into a single
table. And never filter directly on the fact table (but hide those
columns from the users).
And don’t forget: If you now start with the single table, and first
later discover that you want to change it to a star schema, this
step will break all your existing reports based on this model.
On the other extreme of a data model, you do not put
everything into one single table, but fully normalize the data
model to avoid redundancy. This is not such a great idea for a
data model inside of Power BI, as you will learn in the next
section.
Normal Forms
Power BI is rather flexible about what kind of data models it
allows you to build. Unfortunately, a normalized data model
comes with a lot of tables and relationships. Such a model will
be hard to understand for the report users (as information even
from the same entity is spread out over multiple tables). And all

the different tables need to be joined, according to their filter
relationship. Joins are expensive, leading to slower reports.
Normalized data models are optimal for application databases,
but not for analytical databases.
Again: If you now start with a normalized data model, and first
later discover that you want to change it to a star schema, this
step will break all your existing reports based on this model.
Better start with a dimensional model up front. The next section
will remind you once again.
Dimensional Modeling
You do not need to believe that dimensional data modeling is so
much better than all the other data modeling approaches. But,
please, trust me that Power BI and Analysis Services Tabular (to
be more precise: their storage engine, called VertiPaq) is
optimized for dimensional data models through all its fibers.
That’s why it is the goal not to just load the data as it is into
Power BI, but to transform the tables you get from the data
source into a dimensional data model.
There is no need to actually store the data already in a
dimensional model in the data source. For example, Bill Inmon
(mentioned in Chapter 1), recommends to store all analytical

data in a fully normalized schema (which he calls the Corporate
Information Factory). Only the data mart layer is a dimensional
model. This layer on the other side, can either be derived from
the normalized schema with the help of DAX, Power Query or
SQL. In this book I will teach you all necessary steps in all three
languages – so no excuses anymore!
An important question you have to ask the report users (or
yourself): How much of detail is really necessary in the reports?
Does the report need to cover every single transaction or are
aggregates enough? Is it necessary to store the time portion, or
will the report only be generated on a month basis? The
answers to these question define the Granularity of the data
model. Read on, to learn more.
Granularity
It is important that the granularity of your fact table matches
the primary keys of your dimension tables, so you can create a
filter relationship between them with a one-to-many
cardinality. In “Budget” you will see an example for a case,
where new information needs to be added to an existing data
model (the budget) which has a different level of detail: The
budget is only available per product group, but not per product.
The actual sales data, on the other hand, is on the product level.

The solution is to add the budget as a fact table on its own.
(“Budget” will also explain how you can create a filter
relationship between the product table and the budget table,
despite the different granularity).
No matter which kind of data source you need to do analytics
on, the shape of it will most probably not directly fit into a
dimensional model. Luckily, we have tools available to extract,
transform and first then load the data into Power BI. The next
section got you covered on these challenges.
Extract, Transform, Load
The process to extract, transform, and load the data (short:
ETL), is not done via the model view, but you can use either
DAX, Power Query/M, or SQL to achieve this. Beginning with
Chapter 10, you will dive into those languages and make them
your tool to extract and transform the data as needed.
Read on to learn that there is a special kind of transformation
necessary to implement Slowly Changing Dimensions, which is
not done in the Model view.
Key Takeaways

In this chapter I took the basic concepts of data modeling and
matched it with features available in Power BI and Analysis
Services Tabular. You learned:
The Model view (in Power BI) and the Diagram view (in
Visual Studio) gives you a graphical representation of the
tables and their relationship in the data model and allow
you to set plenty of properties, for both the tables and their
columns.
The purpose of the filter relationship is to propagate filters
from one table to another. The propagation works only via
one single column and is implemented as an equi-join.
The filter relationship between two tables is represented by
a continuous line (for active relationships) or a dashed line
(for inactive relationship). The latter can be activated in a
DAX measure, which will be discussed in Chapter 10.
The cardinality of a relationship is represented by “1” or “*”
(= many). You can create filter relationships of type one-to-
many (which is the most common), one-to-one and many-
to-many. Many-to-many relationships can be created
unintentionally when both columns contain duplicates by
mistake. One-to-one relationships can be created
unintentionally when both columns contain only unique
values. Double-check those cases.

A filter relationship has a direction. A filter can be either
propagated from one table to another or in both directions.
Bi-directional filters bear the risk of making a data model
slow. There can be situations where you cannot add
another table with a bi-directional filter, when it would lead
to an ambiguous model.
You now have an understanding of how important a data model
in Power BI is. The next chapter will teach you practical
knowledge of how to shape it into a dimensional model.

Chapter 6. Building a Data Model in
Power BI
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 6th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
Because Power BI is a data model driven tool, it is important to
ensure the information you display and interact with is
modelled correctly. In this chapter you will learn about the bad
and the good things, when it comes to building a data model in
Power BI. You will learn techniques on “how-to” in DAX, Power
Query and SQL in later chapters of the book. Here I will talk

about the principles and options in the model view of Power BI
Desktop.
I will give you a short re-cap on normalizing and denormalizing
before we jump into the topic of writing calculations. As you
will learn, there are certain type of calculations, which cannot
be done before loading the data into Power BI, but only by
defining the formula inside of Power BI. You will learn that
Power BI can do common calculations for you, without the need
to specify a formula. I will also explain, why it is not a good idea
to depend on this behavior, but to always explicitly write even
simple formulas. I will recap the importance of having a
dedicated Date  (and maybe Time ) dimension in your Power
BI data model. You will learn two ways of modeling role-playing
dimensions and that slowly changing dimensions need to be
modeled outside of Power BI (in a physical data warehouse
layer). I will finish this chapter with a description of how to
define hierarchies and how to use them.
Let’s start with the most important part: Normalizing and
denormalizing.
Normalizing and Denormalizing

In “Data Model” I already talked about the concept of
normalizing fact tables and denormalizing dimension tables to
transform any given data model into a star schema. In
“Normalizing”, “Denormalizing”, “Normalizing”,
“Denormalizing”, “Normalizing”, and “Denormalizing” you will
learn actual techniques of how to achieve this task in DAX,
Power Query/M, and SQL. But in this chapter I want first to
demonstrate different modeling approaches and their effects in
Power BI.
NOTE
I want to introduce you to the fictitous company Adventure Works, a sport article
retailer, which makes the majority of its revenue through selling bikes on three
continents either via resellers or directly over its web shop. Many examples in this
book are based on this company’s data warhouse (database AdventureWorksDW ),
mainly tables FactResellerSales  (sales made via resellers), DimDate  (a table
containing one row per day of a calendar), DimSalesTerritory  (a table containg
the sales regions, grouped into countries and continents), DimProduct  (a table
containing the goods, which rows are assigned to DimProductSubcategory  and
DimProductCategory , to shape a product category hierarchy).
NOTE
The files used for the demonstrations are Single Table.pbix, Snowflake.pbix
, and
Star.pbix

For the following demonstration, I inflated the
FactResellerSales  table of the database
AdventureWorksDW  so it contains 18m rows (instead of the
original 60k rows) and then I added this table and DimDate ,
DimProduct , DimSubcategory , DimProductCategory , and
DimSalesTerritory  and created three different models:
The model where I merge all information into one single
table containing all the information has a size of 200MB
(one big table, OBT).
If I keep the tables as they are to form a snowflake schema,
the model size is 84MB, including 704KB for relationships.
When I denormalize the three product-related tables into
one, but keep all the other tables, to form a star schema, the
model size is, again, 84MB with only 656KB spent for
relationships.
You might think that 200MB is not a size to worry about and I
agree. Any laptop (and server, of course, as well) will easily
handle a 200MB database. Any report on such a model will be
fast enough. But this is not the point. The point is that by
bringing the Adventure Works data model into a proper shape,
you can reduce the size by half or two-third, without losing any
information. That means that you can use your existing
hardware for hosting double or three-fold of information.

Opening the PBIX file on your local machine will be faster. You
can keep your premium subscription almost three times as long
before you need to upgrade to the next bigger one. And so forth.
The single table model does not spend any bytes on storing
information about relationships, because there are no
relationships. The difference between the star and the
snowflake schema is only a few kilo bytes. While this difference
is not impressive in this example, it again shows that reports
built on a snowflake schema tend to be slower than reports
built upon a star schema, as some filters will need to travers a
longer way, over more tables.
Long story, short: It is important
that you find the optimal compromise between normalizing
and denormalizing your data model. The good news is that you
do not have to invent something new, but that you can rely on a
concept which has proofed its usefulness over the past decades:
a star schema, where you normalize all your fact tables and
denormalize all your dimension tables.
The other example I like to use is that of a single Excel file,
which is provided by Power BI as a demo file: Financials. It
contains the following columns:
Segment
Country

Product & Manufacturing Price
Discount Band
Units Sold, Sale Price, Gross Sales, Discounts, Sales, COGS,
Profit
Date, Month Number, Month Name, Year
NOTE
The files used for the demonstrations are Financials Dimensional Model Surrogate
Key.pbix, Financials Dimensional Model.pbix,
Financials Filter Dimension Surrogate
Key Measures.pbix,
Financials Filter Dimension Surrogate Key.pbix,
Financials Filter
Dimension.pbix,
Financials OBT Measures.pbix, and
Financials OBT.pbix.
This is the classic example of one big table (OBT), where all
information is joined together into a single table - which is less
than optimal when it comes to Power BI (“A Single Table To
Store It All”). Again, I took the time to transform this table into
different models:
Keeping the single table as it is, results in 5.5MB of total
model size.
Adding dimension tables (for Segment, Country, Product,
Discount Band and Date). These additional tables occupy
additional 5MB, resulting in a 10MB total model size.

Adding just one table, which contains all combinations of
dimensional values. I also had to add a combined business
key (where I concatenated all business keys), but could
remove the single business keys from the original table.
This two-table version of the data model occopies only
319KB! Many thanks to Ana María Bisbé York for bringing
my attention to this solution.
The latter concept is what Ralph Kimball names a Junk
Dimension. As so many times, I like Mr. Kimball’s ideas, but not
all their names. I don’t think that report users will be eager to
add a column to the report, which comes from a table named
Junk Dimension, or just Junk for that matter. I therefore name a
table which contains the combination of dimensions just Filter
instead. This concept works very well for dimension tables,
which do not really have a lot attributes (like in the case of the
Financials example, where every dimension, except for the
product table, which also contains the Manufacturing Price,
only has one single attribute, which is simultanously the table’s
key).
Again, look at this approach as another tool, when it comes to
finding the optimal data model. Build different proof-of-
concepts, before you head into one direction. The milage for
every concrete data model may vary, due to the characteristics

of the database engine behind Power BI. What works well in
one situation does not necessarily work well with different
data.
Calculations
Calculations can be done in either DAX, Power Query, SQL or
any data source. If the result of a calculation is not additive (s.
below), then the calculation must be done as a explicit measure
in DAX in order to achieve a meaningful result.
There are several possible steps, where you (or your colleagues)
can add calculations:
Adding a column in the data source: using a formula in an
Excel file, adding a column to a table or view in a database,
etc.; you will learn how to add calculations in relational
database in “Calculations”. The result of such a calculation
is available to everybody with access to the data source.
Adding a Custom Column in Power Query: If I need to
persist the result of a calculation and cannot do so in the
data source, then Power Query is my next best option. The
result will only be available inside this Power BI data model

(and everybody with access to the data model) and you will
learn how to in “Calculations”.
Adding a Calculated Column in DAX: If you feel more
confident in creating a column in DAX instead of Power
Query, than you will choose this option.
Creating a measure in DAX: For semi-additive and non-
additive calculations, a calculated measure, written in DAX
is the only option. I will elaborate more about this later in
the current section.
It’s a good idea to add a calculation as early as possible in your
stream of data (as far as upstream as possible). If you create a
calculation in Power Query or DAX, the result can only be used
within the report (or reports built on top of this Power BI
semantic model). If you add the calculation already in the data
warehouse, other reports, models and tools (and therefore a
broader range of users) will benefit from the calculation as
well.
Keep in mind though that only additive calculations can be
calculated in the data source, Power Query, or as a calculated
column. Semi- and non-additive must be calculated as measures
(and therefore in DAX as laid out in “Calculations”).

But there is an additional option: Numeric columns in a data
model have a property called default summarization, as shown
in Figure 6-1. There you choose one of the following options:
Don’t summarize
Sum
Average
Minimum
Maximum
Count
Count (Distinct)
Figure 6-1. Default summarization

A similar list of options is available within visuals, as well (s.
Figure 6-2). This allows you to keep the default or define a
different calculation for the scope of this visual. It allows the
same options, but additionally:
Standard deviation
Variance
Median

Figure 6-2. Visual summarization

This property is clearly aimed towards beginners in Power BI:
Even without learning DAX, Power Query or SQL you can create
(simple) calculations. Through this setting Power BI creates an
(implicit) measure for you, by applying the chosen calculation
on top of the column. This implicit measure is not visible
anywhere – you can neither view nor edit it.
But first, you, dear humble reader of this book, are not a
beginner anymore. And second, this feature has certain
backdrafts:
Default summarization does not work in all tools consuming
a data model created in Power BI (in Excel this was first
added in 2023, for exampel). In general, tools accessing the
semantic model with the MDX query language, will ignore
this setting. As a consequence you will not be able to show
the aggregated value of a numeric column, even when with
Default summarization switched to something different
than “Don’t summarize”.
If you start using Calculation Groups, all implicit measures
are voided. Visuals, where you used an implicit measure,
will show an error message. You need to manually fix those
visuals by first explicitly creating the measures and then
exchanging the columns with the measures.

As you never can be sure, if users of your data model will use
Excel (or any other client which uses MDX instead of DAX to
query your data model) as the tool of their choice or if you want
to introduce Calculation Groups (or any future feature which
might prohibit implicit measures) in a later point in time, it’s
best to hide numeric columns at all and define explicit
measures instead. “Calculations” will get you covered.
On top of that, Power BI automatically sets Default
summarization for every numeric column: Either by setting it to
Default (which sums the value) or Count (for the primary key
and foreign key columns used in filter relationships). While the
latter can be useful (even when I would prefer to create an
explicit measure with a good name, like Count of
Customers ), summing up the values of the Year  column, does
not make sense at all.
Therefore, it is important that you take the time to scan through
all numeric columns in your data model:
Columns which values should be aggregated should be
hidden and instead an explicit measure should be created.
In some cases, it might make sense to create more than one
explicit measure (one for the sum, another for the average,
for example).

For columns which values should not be aggregated, Default
summarization must be set to Don’t summarize.
While you are scanning through your columns, you should use
the opportunity to take a look at other properties as well. They
are not directly connected to calculations, but influence the
behavior of the column’s values in a visual:
Data type defines what kind of values can be stored in this
column (and influences therefore the storage format).
Format sets, how the value is shown. You can choose one of
the plenty options or use a custom format string (Use
custom format strings in Power BI Desktop).
WARNING
Be reminded that the Format does not change the Data type. This can be a
problem for columns of any data type, but I see this often especially with
columns of data type Date/time: If you change the (display) format to only show
the date portion but keep the data type as Date/time (instead of Date), then the
time portion will still be stored in the model. The column will unnecessarily
occupy additional space in memory and on disk. You might get into trouble
when you create a filter relationship between columns of Date/time and Date
when the former indeed contains timestamps. When the filtering is propagated,
the Date column’s time will be midnight, while the Date/time column’s time will
be the actual timestamp – and no related columns will be found.

Some visuals will react to the Data category property and
show the content in a certain way. For example, if you
categorize something as an Address or City, then Power BI
will choose a map visualization by default.
Typically you want the month names not ordered
alphabetically (which would show “April” first and
“September” last), but ordered by month number. The
option _ Sort by column_ enables exactly this: By choosing
column Month Number  as the Sort by column of Month
Name , all requests to order by Month Name  will
automatically be changed to order by Month Number
instead – and “January” will be the first and “December”
the last month, as usually desired.
Time and Date
Having a Date  dimension is crucial for many data models,
therfore, you need to take extra care of this dimension table.
Turning off Auto Date/time
NOTE
The file used for the demonstrations is Auto date-time.zip.

Power BI can automatically create a date table for you. It will
contain four columns: Year , Quarter , Month , and Day ,
which are grouped nicely together in a hierarchy. In Figure 6-3
you can see that I added a Date  column to a visual, which the
option Date Hierarchy enabled. You can choose Date instead, to
remove the hierarchy and show a single column containing the
date instead. If you do not want to show the Quarter  for
example, you can just remove it by clicking on the X right to
Quarter .
Figure 6-3. Auto-generated date hierarchy
In Figure 6-4 you see the content of the four hierarchy levels of
the auto-generated date table (and column Amount).

Figure 6-4. Auto-generated date hierarchy shown in a table visual
Unfortunately, this auto-generated date table comes with a
bunch of disadvantages:
You cannot add, change, or remove columns from the
hierarchy.
Power BI will create such a table for every column of data
type Date or Date/time in your data model. This sounds like
an advantage, but is not, as the memory footprint of your
data model will increase unnecessarily.
If you later turn this feature off or create a relationship
between this column and your date table, it will change
your visuals or even break them.
If your report creators will never use the Quarter  column in
reports, they need to add the hierarchy and remove column
Quarter  from the visual every single time. If they need the

Month  in a different format (like “Sep” or “09” or “September
2023”) or expect that the Day  column contains the full date or
includes the weekday, you cannot use the auto-generated date
table, but need to create your own date table (which you will
learn in “Time and Date”, “Time and Date”, and “Time and
Date”).
Power BI will automatically detect the date range by looking at
the earliest and latest entry and will then create a (hidden) date
table, covering the beginning (January 1) of the first year until
the end (December 31) of the last year for each and every
column of datatype Date or Date/time, even when you are not
intending to use this hierarchy for this column. This can lead to
multiple huge tables in your data model, covering a widespread
range of dates (e.g., if you load the birthdate of people into your
data model, the date table created for just this column will
easily cover many decades; or if your data source uses
placeholders like January 1 1900, or December 31 9999 or
January 1 0000 – the automatically created date tables will
easily occupy a remarkable amount of space in your Power BI
file and in memory). This auto-generated date tables are hidden
in Power BI Desktop - you need third-party tools to see them.
Figure 6-5 was created with DAX Studio and shows a list of these
tables (with prefix LocalDateTable).

Figure 6-5. Auto-generated date tables shown in DAX Studio

TIP
According to the official documentation, Power BI supports date ranges from January
1 1900 to December 31 9999 (Date/time types). I could though successfully create
calculated columns in DAX starting in the year 0100 (not that I ever needed it, but I
was curios about it). Values with a year lower then 0100 (or subtracting days from
0100-01-01) are automatically moved into the 20th century: 0000 is interpreted as
1900 and 0099 as 1999. You cannot create negative years (to represent years BC =
before christ).
VertiPaq Analyzer, avalable e.g., in DAX Studio, can also show
you the size of these tables. As you can see in Figure 6-6, the five
tables with prefix LocalDateTable  contain between 365 and
3.615.900 (!) rows, occupying something between 60KB and
269MB. The biggest table was created for a column containing
dates between January 1 of year 100 and December 31 9999.
This might be an extreme example, but demonstrates what can
happen below your radar when you do not turn the Auto
date/time setting off.

Figure 6-6. VertiPaq Analyzer exposes the size of the auto-generated date tables
If you turn the Auto date/time setting off after somebody has
included the hierarchy in a visual, the visual will change.
Figure 6-7 shows how Figure 6-4 changed.
Figure 6-7. The same visual, but with Auto date/time turned off

On top of that, referencing one of the four mentioned parts of
the auto-generated hierarchy ( Year , Quarter , Month , and
Day ) in a DAX calculatoin has a special syntax, like
Sales[OrderDate].Year . The Year  is not referenced as a
column, but as a variation of the Date  column. If you later
change your mind and start creating your own Date  table this
will void all measures, which use this special syntax, which can
easily happen if you implement Time Intelligence calculations
(explained in “Calculations”). Fine working measures are
suddenly voided and you see a error message like this: “Column
Reference to ‘Date’ in table ‘Sales’ cannot be used with a
variation ‘Year’ because it does not have any.”
Users of your data model will see a gray box with an error
messages instead of their visual (Figure 6-8), announcing a
problem with a measure. This might not only be the case with
measures defined in the data model (which you have under
control and can fix), but also hit measure definitions inside
reports and queries, outside your data model, created by users
or their tools. Please do not click on Fix this, as it will “fix” the
visual by removing all void columns or measures. Yes, this will
get you rid of the error message, but this will make it hard for
you to find out which measure to add back again, after it was
corrected. Correcting the measure is not too hard: you need to
change parts of the code from something like

Sales[OrderDate].Year  (which references the variation of
the fact table’s date column) to something like Date[Year]
(which references the Year  column of your Date  table).
Figure 6-8. The same visual, but with Auto date/time turned off, containing a now
voided measure

WARNING
Therefore, I consider it dangerous to not disable this option already from the
beginning when you start creating a data model. You turn this feature off for
individual files via File - Options - Options and Settings - Current File - Data Load -
Time Intelligence, as shown in Figure 6-9.
Figure 6-9. Current file’s data load options for time intelligence

If you want to make sure that this feature is turned off by default on your computer
(which I strongly recommend), then you can do so via File - Options - Options and
Settings - Global - Data Load - Time Intelligence. Figure 6-10 shows this setting.
Figure 6-10. General data load options for time intelligence

In the Chapter 10, Chapter 18, and Chapter 18 you will find
scripts to create your own date table. These scripts give you full
control over the columns and their format.
Marking the Date table
Another thing you should not overlook is to explicitly mark
your date table as a date table. This is not necessary in all cases
– but it doesn’t do any harm to set it in all cases (and it’s way
easier to remember to always mark your date table, instead of
remembering the cases when it is really necessary). When both
of the following conditions are met, then you must mark the
date table:
You (or your report creators) want to use the built-in time
intelligence functions of DAX.
You are not using a column of data type Date or Date/time to
create the filter relationship. (In data warehouses it is best
practice to use a column of data type Integer as the primary
key of the date table, for example.)
In case both conditions are met, then the DAX’s time
intelligence calculations will not work properly, unless you
explicitly mark your data table. Luckily, this is very easy to do:
Select the date table and then choose Table tools - Mark as data

table in the ribbon. In the dialog box (Figure 6-11) you select the
date column from the list box. Only columns of data type Date
or Date/time are listed.
Figure 6-11. Marking the data column of your data table
Now you have made sure that your data model has an explicit
date table. In some data models, there is though more than one
meaning possible for the shown or selected date: The report
consumer might filter on the dates, when the goods have been
ordered. Or the report consumer wants to filter on the dates
when the goods have been shipped. In this case, the Date  table
might play more than one role. The next sections covers two
different solutions for this problem: role-playing dimensions
and slowly changing dimensions.

Role-Playing Dimensions
NOTE
The files used for the demonstrations are Date role-playing.pbix, and
Date
multiple.pbix.
A dimension in a data model could play more than one role. It’s
most typical but not limited to the date dimension: Let’s assume
that the Sales  table stores both an Order Date  and a Ship
Date  column. You have two options to make this work in the
data model:
Create two relationships between the Date  table and the
Sales  table. One filter relationship uses the Ship Date
column of the Sales  table, the second one uses the Order
Date  column. This is shown in Figure 6-12. As only one of
the two relationships can be marked as active, you will
need to use DAX to create dedicated measures per
relationship in which you activate the inactive relationship.
In “Role-Playing Dimensions” you will learn about all these
details. You end up with only one dimension table, but
several measures.

Duplicate the Date  table (name one copy of this table
Order Date  and the other Ship Date ) and create one
relationship between the Order Date  table and the
Sales  table and another between the Ship Date  table
and the Sales  table (shown in Figure 6-14). As you have
only active relationships there is no need to build
additional versions of measures. In the end you have
several versions of the dimension table, but only single
versions of the measures.
Let’s elaborate about the differences of the two options to learn
that both options cover slightly different use cases and
requirement:
Figure 6-12 shows you a model with two tables Date  and
Sales , and with two relationships between them. One is based
on the Sales  table’s Ship Date  column and one is based on
the Order Date  column. If you look closely, you will discover
that one of the two lines representing the filter relationship is
continuous, the other is dashed. The continuous one is an active
relationship, the dashed lined represents an inactive
relationship. Only one relationship (filter path) between two
tables can be active. You can create additional relationships,
which can only be inactive. This is due to the fact that if you
would have two active relationships, the model would be

ambiguous: Should a filter on the Date  table’s Year  column
filter all the Sales  which were ordered in this year or which
were shipped in this year? Or should the filter be applied to
both, so we only get to see sales which were ordered and
shipped in the same year? To avoid this ambiguous situation,
Power BI will only respect the active relationship – and you
need to chose, which relationship should be the active one. If
you want to make a different relationship active, then you first
have to deactivate the active one. At one point in time, only one
single relationship path can be active.

Figure 6-12. The Date table and the Sales table have two relationships
Maybe you ask yourself: Why would you create additional
relationships in the data model, if they cannot be active? The
answer is first that creating the inactive relatinships will make

it transparent that this is a valid way of propagating a filter
from one table to the other. Second, the storage engine inside
Power BI will add extra (hidden) structures/information to the
data model to make filter propagation based on this
relationship faster (compared to, when you do not define a
realtionship at all). And last, you can activate an inactive
realationtship inside a calculation done in DAX. Here I will give
you a brief overview of the function USERELATIONSHIP  so you
can see how it’s used to activate an inactive relationship, but
you’ll find more in-depth information about it in “Role-Playing
Dimensions”. Following this approach, you add only one
generic Date  table and decide inside the formula of a measure
if one or the other relationship should be used for the
calculation of this measure. USERELATIONSHIP  will
temporarily active the chosen relationship (and temporarily
inactivate the existing active relationship). For example, you
can create one measure for the default relationship and name it
[Order Quantity]  and then you create another measure
with the same formula, but where you activate the relationship
on the Ship Date  column to calculate the [Ship
Quantity] .
The advantage of this approach is that you only load one single
version of the dimension table (e.g., Date ) and that you can
use this dimension in visuals where you can then compare the

same value/measure side-by-side based on each of the two role
playing options. For instance, in the previous example, you can
compare the [Order Quantity]  against the [Ship
Quantity]  side-by-side based on dates, as shown in Figure 6-
13. I intentionally used USERELATIONSHIP  in both of the
measures, while it would be sufficiant to use it only to activate
the inactive relatinship (the one on [Ship Date]  in my
example). I make this as a failsafe, in case somebody changes
which of the relationships are (in)active.
Figure 6-13. A single Date table and two measures
Alternatively, you can implement a solution, which works
without DAX. Then you need to duplicate the Date  table under
different names (how, you will learn in Chapters 10, 14, and 18).

In Figure 6-14 you see this implemented for the previous
example: You will end up with several dimension tables (one
per role, e.g., Order Date  and Ship Date ) which have each
an active relationship the Sales  table. For example, the
Order Date  table’s Order Date  column will have a filter
relatinship to the Sales  table’s Order Date  column. Similar,
the Ship Date  table’s Ship Date  column will have a filter
relatinship to the Sales  table’s Ship Date  column.
Figure 6-14. A model with two dedicated Date tables
Following this approach, the report user can choose from two
different filters for each, the Order Date  and Ship Date
and use one single measure. You can also create a matrix visual
showing when goods have been ordered and compare them to
when goods have been shipped. Both dimensions filter the
identical measure, as shown in Figure 6-15.

Figure 6-15. Two independent dimensions/filters and one single measure
Now you learned that the difference is not only in the use of
DAX or not, but the two solutions give you different options in
the reports. If you want to cover all use cases, you can of course
build a model with three Date tables ( Date , Order Date , and
Ship Date ), four filter relationships to the Sales  table (two
from the Date  table and one each from tables Order Date
and Ship Date ) and three measures to calculate the different
versions of quantity.
In this section we talked about when dimensions play different
roles within a data model. In the next section you will learn
how to keep track of changes for a dimension.
Slowly Changing Dimensions

NOTE
The file used for the demonstrations is Slowly Changing Dimensions.pbix.
To implement a solution for slowly changing dimensions you
need the possibility to discover changes and selectively update
information in your analytical database. There is no way to
compare the existing data with the new data during a data
refresh in Power BI. You can only refresh a whole table, but not
selectively certain rows in the table. Therefore, you need to
implement slowly changing dimensions in a (physical) data
warehouse.
If Slowly Changing Dimensions was implemented correctly in
the data source, there is nothing special to do in the Model view,
except for creating the filter relationships. All the dimension
tables will already track the historic changes. And the fact
tables will refer to the primary key of the dimension’s row
matching the point in time of the fact table.
Your task in Power BI to just create a filter relationship between
the fact table and the dimension table on the dimension table’s
(surrogate) key column (like in Figure 6-16 where the Product
table and the Reseller Sales  table are connected simply via
the Product Key  column). All rows in the fact table will

always contain the surrogate key of the dimension’s row
version which was active at the point in time when the fact
occurred. So, there is no need for a complicated logic to find the
right version of the dimension table’s row when joining with
the fact table. An ordinary filter relationship will be sufficient.
Figure 6-16. Filter relationship between table Product and table Reseller Sales
If your report users ask you to give them a choice of which
version of the dimension is used, then I have a solution for you.
By default, the numbers for the actual year, are displayed
according to the dimension’s current version, and the numbers
for the previous year are displayed for the dimension’s last
year’s version. By introducing a detached table (s. Product
Version  in Figure 6-16) containing dates or version numbers

and some logic in a DAX measure you can implement a model,
where the user can choose a version and then override the
relationship between the fact table and the dimension table.
You will learn to implement this case in “Slowly Changing
Dimensions”.
Many data models contain some sort of hierarchical data. Not
so much for Power BI, but very much for the report user it can
be very convenient, if they can find pre-defined hierarchies in
the data model.
Hierarchies
NOTE
The file used for the demonstrations is Hierarchies.pbix.
In multi-dimensional cubes, defining hierarchies was (and still
is) a very crucial step. Analysis Services Multidimensional (the
predecessor of Tabular models, like Power BI) is using this
information to (automatically) calculating and storing
aggregations on the fact table’s data to speed up the queries.
Lack of definitions (or mistakes) can be the reason why queries
are slow.

In Power BI and Analysis Services Tabular, the definition of
hierarchies is playing a more secondary role. They make
creating reports easier, as a user can add several columns with
just one click to a visual – but the (lack of) definitions of
hierarchies does not have any influence on the size of the data
model or the speed of the reports (at least not at time of writing
this book).
Nevertheless, the goal of data modeling is indeed making report
creating easier, therefore you should still check your data model
for natural and non-natural hierarchies. Natural hierarchies
are such, where something is part of something bigger: A day is
part of a month, which is part of a year. Upper Austria is part of
Austria, which is part of the European Union. These are
examples for natural hierarchies. Organigrams show who is
reporting to who within an organization (and is an example for
a parent child hierarchy). Sometimes, reports show e.g., the
available colors per t-shirt size – this is a non-natural hierarchy
(as most of the colors are available in all t-shirt sizes), but if this
is a common request, I would also define a hierarchy within
Power BI for this case.
It’s important to point out that Power BI and Analysis Services
Tabular do not directly support parent-child hierarchies. You
can add columns from within one single table as a part of a

hierarchy – but Power BI cannot automatically find its way
through the e.g., Manager ID  column of one row to the
Employee ID  column of another row and grab the
information. The way around here is that we, as the data
modelers, have to dissolve the parent-child-hierarchy by adding
all information of the ascendant’s levels to every single row.
That means that we need to add a column with the CEO and a
column for the VP-level and so forth to the employee table. In
“Hierarchies”, “Hierarchies”, and “Hierarchies” I will
demonstrate how you can create the content for those columns.
To make the report users’ life easier (which, remember, is the
ultimate goal of a data model) you can group columns together
to form a hierarchy. This saves the user from finding every
single column to add to a visual – the user can instead add the
hierarchy (and thus all columns that make up the hierarchy) at
once. This feature is independent of the type of the hierarchy.
The important part is that all level which should form the
hierarchy must be a separate column within the same table. If
the columns are spread out over several tables, then you cannot
define a hierarchy, but must first denormalize the columns into
a single table. If you decide against denormalizing, then the
user can still add the individual columns to a single visual to
show the hierarchical information.
You define a hierarchy in
the Model view. Start by right-clicking on a column to Create

hierarchy. Then you Select a column to add level, as shown in
Figure 6-17. Don’t forget to click Apply Level Changes after you
are finished to not losing the whole definition.
Figure 6-17. Definition of hierarchy “Product Hierarchy”
This chapter is a wrap – see what you learned so far.
Key Takeaways
In this chapter I laid out typical tasks which I need to complete
in almost every single Power BI data model I build:
The best practice in Power BI is to bring your data model
into a star schema, by denormalizing all dimensions and
normalizing all your fact tables. This is the most optimal
way to leverage compression of the data and performance
of your reports.

Relying on implicit measures (via Default summarization
property of numeric columns) is considered bad practice
and limits the capabilities of your data model in some tools
and might compromise reports when you later introduce
Calculation groups.
Taking care of the properties of your numeric columns is
important (e.g., setting Default summarization to Don’t
summarize or hide numeric columns for which you created
an explicit measure). You also want to take care of the Sort
by column for some text columns (like a month’s name).
Auto date/time is turned on by default in Power BI Desktop
and is considered bad practice because of the inflexibility
and the potential size of the date dimensions created for
you. Turning this setting off later might break the syntax of
your measures or change the look and feel of existing
visuals. That’s why you should turn it off from the
beginning.
The two solutions to implement role-playing dimensions
come with different features attached. Make sure that you
understand both and apply one or the other (or both
concepts) in your data model.
Usually, you do not need to pay attention to Slowly
Changing Dimensions in your data model, as all the logic is
implemented in a persisted data warehouse. Only in rare

situations you will face the requirement that the report
user needs to choose the version of the dimension to show
data based on that version.
Hierarchies are of great importance in many data models.
Natural hierarchies are easy to implement by fully
denormalizing a dimension (which you should do
anyways). Parent-child hierarchies need to be persisted into
one column per level. As soon as you have the different
levels as dedicated columns within one single table, it’s easy
to add them to a hierarchy.
Now that you know how to build the basic parts of a data model
in Power BI, it is time to dig into real-world use cases and how
to solve them via the data model.

Chapter 7. Real-World Examples
Using Power BI
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 7th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
In Chapter 5 and Chapter 6 I introduced the basic data
modeling features of Power BI and typical tasks to transform
your data into a useful, performant, and easy-to-understand
data model. Now it is time to take the next step and look at
challenges that more advanced data models must face. This
chapter will make it clear why Microsoft’s decision to make

Power BI a data model driven tool (as opposed to a report driven
tool) was a brilliant decision. Instead of building
transformations specific to the report you need to create, Power
BI enables you to build a more generic solution, which can
solve very complex challenges. All problems discussed in this
chapter would be harder to solve in a report driven tool.
The selected use cases are independent from each other. All of
them demonstrate different advanced functionalities in Power
BI. Mastering them will allow you to solve others problems as
well. The first use case (Binning) will demonstrate how you can
show a higher level grouping of a value (e.g., small, medium,
large) instead of the actual value, which is sometimes more
helpful. Then you will learn about multi-fact data models and
their challenges in the section about Budget. I took this as a
typical example, where your data model will contain more than
one single fact table, as a budget is usually created on a
different granularity than the actual values. The available
solutions to implement multi-language models did not satisfy
the needs of one of my customers. Therefore, I designed and
implemented one myself – and will describe it in this chapter.
Again, I was faced with the problem of key-value pair tables at
one of my projects and developed a solution to handle them
gracefully. After everybody had waited for the general
availability of DirectQuery for Power BI semantic models and

Analysis Services it finally was the time in April 2023. Basically,
DirectQuery means that you do not import data into Power BI,
but Power BI is directly querying the data source, whenever a
report page is shown or a filter was changed by the user. This
feature will tremendously help in combining Self-Service BI and
Enterprise BI, as you will learn in the last section of this chapter.
Keep in mind that nevertheless, a star schema is the basis of
more challenging uses cases as well.
Binning
NOTE
The file used for the demonstrations is Binning.pbix.
In this section I will talk about two options, already discussed in
general in “Binning”:
A lookup table with all possible values
A lookup table describing the value ranges of a bin
Lookup table

If you have a lookup table, containing a row for every possible
row of the value plus the value to show as the bin, you only
need to create a single-directed, one-to-many filter relationship
to the value of the fact table. In Figure 7-1 the Bin Table ’s
primary key Quantity  has a filter relationship with the
Quantity  column of the Sales  table. Quantity  in here is in
the double-role of the actual quantity and the foreign key to the
Bin Table . The latter role may look a bit unusual but has a
good performance and is therefore more than acceptable.
Figure 7-1. Relationship between the fact and the bin table
Range table
If your lookup table contains not all distinct values, but the
lower and upper range of the bin instead, you cannot create the
proper filter relationship in the Model view as Power BI only
allows for equi-joins. Figure 7-2 shows two unrelated tables.

You can (and need to) solve this with the help of DAX instead.
Please lookup “Binning” to learn about this.
Figure 7-2. Relationship between the fact and the bin range table
In both cases, make sure to add a column which contains the
sort order to the table containing the bins. Usually, people
expect the bins to be ordered in a certain way (which might not
be alphabetical). For example, “low”, “average”, “high” (instead
of in alphabetical order “average”, “high”, “low”). Click on the
bin’s text column and select Column tools - Sort by column from
the ribbon and select the column which contains the values for
the proper order.
The next section talks about solutions when you need more
than one single fact table in your data model. This is a generic

use case, and I will demonstrate it on the basis of a budget.
Budget
NOTE
The file used for the demonstrations is Budget.pbix.
Power BI as a model-driven tool allows for as many tables of
any type to be added to the data model as you need to solve
your business problem. Therefore, creating a multi-fact data
model (with e.g., one fact table for the actual values and
another fact table for the budget) is no problem at all. The
challenge is to find the best way to bring the budget table
(which has a different granularity than the table for the actual
values) into a relationship with the rest of the tables. So, the
Bugdet use case is only a model for how to implement any
multi-fact data model.
To solve this challenge, you have to chose between three
potential solutions available in Power BI, which have their own
advantages and disadvanteges:
As the budget is e.g., on product category level, but not per
individual product you do not create a relationship

between the Budget table and the product table at all (s.
Figure 7-3). You keep the Budget table and Product table
disconnected. The solutions for binning (“Binning”) can be
applied here, too: using DAX to create the relationship on-
the-fly, where need (s. “Binning”).
Figure 7-3. No relationship between tables Budget and Product
Alternatively, you can create a relationship between tables
Budget and Product (s. Figure 7-4).

Figure 7-4. A many-to-many relationship between tables Budget and Product
Doing so, you will receive the following message (s.
Figure 7-5): This relationship has cardinality Many-to-Many.
This should only be used if it is expected that neither column
(Product Group and Product Group) contains unique values,
and that the significantly different behavior of Many-to-Many
relationships is understood. The message contains a link to
Apply many-to-many relationships in Power BI Desktop. As
it is only a warning, you can successfully create such a
relationship by clicking OK. It’s important though that you
fully understand (and accept) the consequences of such a
limited relationship (as discussed in “Relationships”). One is
that no blank row is shown to represent missing values,
they are just skipped. When you follow all best practices
then there should be no missing values anyways. Another is

that the function ALL  in DAX will not remove a filter from
related tables. That means in this example that removing
filters from the Budget table will not remove the filter on
the Product Group column of the Budget table.
Make sure to make this relationship single-directed
(Product filters Budget), to avoid an unnecessary bi-
directional filter.

Figure 7-5. Create a relationship between tables Budget and Product
Using a bridge table (later sections of this book “Budget”,
“Budget”, and “Budget” got you covered how to build such a
table) and create two one-to-many relationships between
the bridge table and the two tables (as shown in Figure 7-6.
If Power BI should propagate a filter from the Product table
to the Budget table, you need to set the relationship

between the Product table and the bridge table to Both. This
is one of the rare cases where it is good that there is the
option of a bi-directional filter (which you should not use as
your default filter direction). The two one-to-many
relationships over the bridge table will not have the
disadvantages described for the many-to-many
relationship. That’s why a bridge table is usually my
preferred solution.
Figure 7-6. Using a bridge table between tables Budget and Product
The next section changes the topic towards localization. I will
demonstrate the solution I built for one of my customers to give
control over the display language to the report user.
Multi-Language Model
NOTE
The file used for the demonstrations is Multilanguage.pbix.

To build a data-driven multilingual data model, as described in
“Multi-language Model”, you need to take care of the following
things in Power BI:
Dimension table for the available languages
You need to create a table, which contains a key and a
display name for the available languages. I would
recommend making sure to set the slicer/filter to single-
select on all reports, as it does not make sense to select
more than one single language.
In the following example I created such a table for two
languages: English and Klingon. You can use any value for
column Language ID . I decided to use the short names
used by Azure Cognitive Service, as I will demonstrate its
usage later in “Multi-language Model” to automatically
translate texts into different languages (e.g., from English
to Klingon).
Table 7-1. A language table
Language ID
Language Desc
en
English
thl-Latin
Klingon

Visual elements (e.g., report headline)
You create a plain vanilla relationship between the
Language  table and the table containing the texts for the
visual elements for all languages (s. Figure 7-7).
Figure 7-7. The Language table has a filter relationship with the
Textcomponent  table
I prefer the table with the texts for the visual elements to
be pivoted. Then I have one column per visual element,
with rows per language. As then there is only one row per
language in both, the Language  table and the
Textcomponent  table, the cardinality is one-to-one (s.
Figure 7-9). Here is an example of what this table could
look like:

Table 7-2. A (pivoted) table containing the texts for the visual
elements
Language ID
SalesOverview
SalesDetails
en
Sales Overview
Sales Details
thl-Latin
QI’yaH
qeylIS belHa’
If you do not pivot this table, the cardinalitiy of the
relationship between tables Language  and
Textcomponent  would be one-to-many instead and you
need to add an extra filter for the textbox where you show
the display name, to filter on the right text identifier (s.
Figure 7-8).

Figure 7-8. Filter on unpivoted table for visual elements
Textual content (e.g., product names)
Now comes the complex part, which is the heart of my
data driven solution. In my solution all dimension tables
end up having a composite key (composed of the
dimension’s key plus the language key). In order to
identify a single row of a dimension table, it must be
filtered on both the dimension key and the language key.
The fact tables reference the dimension table only on the

dimension key through a regular one-to-many
relationship.
The language (key), on the other hand, is chosen by the
report user via a slicer. To model this relationship is
tricky: Of course, you can add a one-to-many relationship
between the language table and the dimension table. But
it turns out that this only works for the first dimension
table. As soon as you add the relationship from the
language table to the next dimension table, Power BI will
complain about ambiguity: Adding another relationship
of this kind would create more than one path from the
language table to the fact table (one path is over the first
dimension, the other path over the next dimension).
Power BI will not allow us to create an ambiguous model.
We are forced to apply the tricks we learned in the section
on binning: leaving the dimension tables and the
language table disconnected (as shown in Figure 7-9) and
using DAX to create the relationship via function
TREATAS , as you will learn in “Multi-Language Model”.

Figure 7-9. The Language table is disconnected from the dimension tables
Numerical content (e.g., values in different currencies)
The table containing the exchange rates is usually
disconnected (shown in Figure 7-10). Finding the correct
row in this table is the matter of a non-equi join (e.g., most
current available exchange rate on or before the date the
fact occurred). Again, we move the complexity over to
DAX.

Figure 7-10. Tables CurrencyExchangeRate and Fact are disconnected
Data model’s metadata (e.g., the names of tables and columns)
To add translations of the metadata, you need to export a
JSON file, which will contain all artefacts of the data
model. In this file you add the translations and import it
again.
Exporting the file for an Analysis Services Tabular
database is done via Visual Studio (which is Microsoft’s
offering as a tool for creating and altering the definitions
for such databases). At the time of the editorial deadline
for this book, Power BI Desktop had no feature to export
the JSON file via the user interface. The recommended
tool to create the JSON file is Tabular Editor. This tool is
not directly supported by Microsoft, but strongly

recommended (you will find references to Tabular Editor
in all of Microsoft’s official documentation, certification
exams, etc.).
NOTE
Tabular Editor is developed by Data Platform MVP Daniel Otykier and comes
in two tastes: Version 2 is open-source and free. For version 3 you need to buy
a license. Find more about both versions at Tabular Editor. Meta-data export
for Power BI (and Analysis Services Tabular definition, for that matter) does
work in both versions.
In Tabular Editor’s TOM Explorer you will find
Translations on the bottom of the list (s. Figure 7-11. By
right-clicking you can export, and later re-import the
JSON definition containing translations for all the data
model’s meta data.

Figure 7-11. Tabular Editor’s TOM Explorer lets you maintain translations
You can either edit the JSON file with a plain text editor, or
you can use TabularTranslator, an open-source and free
tool by Kasper de Jonge (Principal Program Manager at
Microsoft), you can download from Tabular Translator.
In the JSON file you can ignore the first half of the content
and directly scroll down the section “cultures”. Tag
“name” defines the culture you translate to. Afterwards
you look for pair of lines containing “name” (the name the

artefact has in the model) and “tranlatedCaption” (the
name of the artefact in the new culture):
"cultures": [
    {
      "name": "de-AT",
      "translations": {
        "model": {
          "name": "Model",
          "translatedCaption": "Modell",
          "tables": [
            {
              "name": "Language",
              "translatedCaption": "Sprache"
              "columns": [
                {
                  "name": "Language ID",
                  "translatedCaption": "Spra
                },
... (cut off for brevity)
TabularTranslator makes it a bit more convenient to enter
the translations, as you can see in Figure 7-12. On top left
you see the culture. The first two columns of the grid in
the middle show the table (e.g., Language , Fact , Dim1 ,

etc.) and object type (e.g., Model, Table, Column, Measure,
etc.). Then the original name, description and display
folder are displayed. Only the last three columns are
editable. There you (or the translator) enter the translated
version of the name, description and display folder. If you
leave something empty, it will fallback to the original
language of the model (English in my case).
Figure 7-12. Tabular Translator is a convenient tool to maintain translations of
meta data
User interface of Power BI Desktop (Standalone)
Not matter in which language you initially have installed
Power BI Desktop, you can select from a long list of
supported languages via File – Options – Options and
Settings – Global – Regional Settings (s. Figure 7-13).

Application language is the Power BI Desktop displays the
menu or messages. Model language is the language the
data model (in the Data pane on the very right of the
screen) is displayed. After the change you need to close
Power BI Desktop and start it again before this change
will be activated.
Figure 7-13. You can change the Application Language and the Model language
separately
User interface of Power BI Desktop (Windows Store)

If Power BI Desktop is not installed as a full application,
but via the windows store (which has the advantage that
it updates to the newest version automatically), then the
setting from the operating system is used by the app and
you cannot change it separately. Look for Time &
Language – Region & Language in the Windows settings if
you need to change the display language (s. Figure 7-14).
Figure 7-14. The Windows Store version of Power BI Desktop respects the
operating system’s display language
User interface of Power BI Service
Via the gear icon on the top right corner of the Power BI
Service you will find General – Language (s. Figure 7-15).

Figure 7-15. Selecting the display language in the Power BI Service
User interface of Power BI Report Server
The on-premises Power BI Server respects the settings of
your internet browser to choose the language of the user
interface. In the case of Edge you will find this setting via
Settings – Languages – Preferred language). Figure 7-16
shows the setting in the Edge browser.

Figure 7-16. Selecting the display language in Edge
Next I will show you an interesting way of how to model key-
value pair models to reports where the report-user has a
maximum of flexibility to analyze correlations in the data.
Key-Value Pair Tables
NOTE
The file used for the demonstrations is KeyValue.pbix.
In the model view of Power BI, a key-value pair table is just a
single table, without any special relationships. To transform it

into a proper model you need to pivot the table (which you will
learn in sections “Key-Value Pair Tables”, “Key-Value Pair
Tables”, and “Key-Value Pair Tables”) and then split it into fact
tables and dimension tables, as discussed in Chapter 6.
But there is one interesting use case for a very flexible kind of
report, where you not only keep the key-value pair table in its
unpivoted state but load it twice into Power BI. Figure 7-17
shows the key-value table Source  twice in Power BI’s data
model. The filter relationship is created on the ID  column. The
cardinality is many-to-many (as the same id can appear
multiple times in the key-value pair table) and the filter
direction is singe directed from Source 2  to Source .
Figure 7-17. The key-value pair table loaded twice into the data model
Modeling the key-value pair in this way enables you to create a
report like shown in Figure 7-18. The two slicer visuals filter

once the Key  column of table Source  and once of table
Source2 . The matrix in the center shows the Value  column
of table Source  in the rows and of table Source2  in the
columns. In the Values section of the matrix visual I put a
measure counting the rows of the Source  table ( [Count of
rows] := COUNTROWS(Source) ). Now the report user has full
flexibility on what (key) to display on the rows and on the
columns of the matrix visual. As a result, you see how many
entries you can find in the key-value pair table for the different
combinations of values. For example, there is one Bill, one Jeff
and one Markus in the table. Bill and Jeff live both in Seattle,
while Markus lives in Alkoven. In total we are talking about
three different IDs in the key-value pair table.
Figure 7-18. A flexible report build on top of the duplicated key-value pair table
Independent of the shape of your data model you will face
situations where you want to enrich an existing data model

with local data or combine two models with each other. This
problem is usually described as the combination of self-service
and enterprise business intelligence. Power BI comes with an
very interesting approach, as you will learn in the next section.
Combining Self-Service and
Enterprise BI
Power BI Desktop was born as a self-service BI tool (in 2015, by
the way) and comes with plenty of features to make the life of
information workers easy. Most important, you can import data
from a wide variety of data sources into Power BI Desktop. But
it also allows you to connect to a relational data warehouse or
an analytic database (e.g., hosted on an Analysis Services
Tabular or a Power BI semantic model, hosted in the Power BI
Service). These data sources are usually created or at least
curated by IT departments.
The catch with a connection to a Power BI semantic model or
Analysis Services is though that this is considered a live
connection. And a live connection comes with a set of
limitations. For example, you can only create a connection to
one single data source. That means to e.g., only one single
Power BI semantic model or one single Analysis Services

database. It does not allow you to add data from any other data
source (neither from an Excel spreadsheet, nor from a
relational data warehouse or any other source). As you can see
in Figure 7-19 you can only create a New measure but cannot
add new columns.
Figure 7-19. Limited functionalities in Live Connection mode
Since April 2023 an alternative is generally available: A
DirectQuery connection to a Power BI semantic model or
Analysis Services Tabular (not multi-dimensional, though).
Don’t misinterpret live connection and DirectQuery as
something similar. Despite the fact that the names share a
common meaning (in both, the data is not loaded into Power BI
Desktop, but only queried as needed by the report consumer)
under the hood these are very different technologies.
The most important difference is that you can create a data
model which mixes the two storage modes Import and
DirectQuery. You can load the data for some of the tables but

keep other tables in DirectQuery mode. This is called a
composite model (as two different storage modes are combined
in the model). Both, the imported tables and the DirectQuery
tables can come from different data sources. (A live connection
cannot be part of a composite model.) This option allows you, to
create a model, which re-uses all the centralized and predefined
logic of your enterprise model but adds additional information
(e.g., an Excel spreadsheet containing the first draft of your
budget or a CSV file containing a custom definition of sales
regions or data from a different enterprise data model) into the
same model. Based on this (composite) model you can then
create a report containing both, self-service and enterprise
information.
The only thing you have to keep in mind is to find the right
relationships between tables from the different data sources
(because e.g., the products might have different key values in
the data warehouse than they have in the Excel spreadsheet). If
you add fact tables from different sources, then you will face
the problems of multi-fact models and you can apply the
solutions discussed in the section on “Budget”.
Key Takeaways

With this chapter you made a deep dive into advanced data
modeling concepts. Basically, all of these advanced challenges
can only be solved with Power BI, because Power BI is a model-
driven tool. I do not want to imagine solving these problems in
one big table (in Excel) or write the SQL code for a fully
normalized data model in my reporting tool. I hope that these
challenges (and their solutions) finally made it clear, why
having a model-driven tool is an advantage, even if it might
have looked over-complex in the beginning of your journey into
data modeling with Power BI.
A lookup table for the bins must be connected as a regular
one-to-many relationship to the fact table.
To connect a fact table which is on a higher granularity
than the primary key of the dimension table, you have
three options: use DAX, create a many-to-many relationship
or introduce a bridge table to the model.
Setting the language for text on the report, the names in the
data model and the language of the user interface of the
application you are working with is relatively straight
forward.
The lookup tables for the language must stay disconnected
from the dimension tables, because Power BI does not
allow us to create an ambiguous data model.

The lookup table for the currency exchange rates stays
disconnected for a different reason: Finding the correct
date of the exchange rate involves an non-equi join (which
only can be solved with DAX).
Power BI allows to create composite models where tables
can have independent data sources and can be in different
storage modes (Import or DirectQuery). This bridges the gap
between pure self-service and pure enterprise BI solutions.
The next chapter will elaborate more about different storage
modes: Import, Live connection, and DirectQuery.

Chapter 8. Performance Tuning in the
Power BI Data Model
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 8th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
Performance tuning in Power BI is a complex topic, as there are
several components involved: Every storage mode has different
performance implications, and optimizing the report run-time
must be done with approaches specific to the storage mode. You
have more than one option in implementing the same
calculation in DAX. You can pre-calculate values and physically

store them in the data model, or first let calculations be done ad
hoc via explicit measures. The in-memory compression is
highly dependent on the cardinality of a column – and can
therefore not be predicted in a general way. Performance
tuning in Power BI can easily fill a book on its own. All the
optimizations have in common though that scanning less data
will speed up query time.
As this book is about data modeling, I limit myself to discuss the
storage modes and how the variants can be combined in a data
model to speed up query time.
NOTE
The file used for the demonstrations is Performance Tuning.pbix.
Storage Mode
The VertiPaq engine offers different storage modes. Some of
them can be set under the Advanced settings in the Properties
pane for a table (Figure 8-1):

Figure 8-1. Storage mode of tables
Import mode
When you choose to import data, the complete table will
be physically loaded into the data model. The data will be
compressed and stored in an in-memory columnar
database. This offers the best query performance by far.
Therefore, this should be the mode of choice in many
scenarios. This mode is possible for all data sources (from
flat files over excel worksheets to all kinds of databases).
A regular refresh needs to be scheduled to keep the stored
data up-to-date.

DirectQuery mode
In this mode, only data connection information (like
server name and database name) and metadata (like
names of tables and columns) are stored in the data
model. Every query against the VertiPaq engine (like a
DAX statement generated by a visual inside Power BI) will
be sent to the data source, in lack of any data stored
locally. If necessary, the DAX query will automatically be
converted into a SQL query. This mode is only available
for some data sources (namely relational databases,
which can be queried in SQL and analytical databases like
Power BI semantic models hosted in the Power BI Services
or Analysis Services Tabular, which can be queried in
DAX). Performance, especially for relational databases,
will be slower compared to Import mode: There is some
overhead involved when running a query which
combines information from different data sources. The
SQL statement (derived from the DAX query) will be less
than optimal; and the performance of the query will be
fully dependent on the data source. It will be a challenge
for any data source to compete with query performance
of the VertiPaq engine, even when tuned properly. If you
don’t have the knowledge or the possibilities to invest in
performance tuning of the data source, then DirectQuery

mode is not for you. Some limitations apply to DirectQuery
mode, as laid out in Microsoft’s documentation at
DirectQuery limitations. No data refresh needs to be
scheduled in this mode, as no data is stored inside of
Power BI. When your data model is in DirectQuery mode,
you will see a hint in the status bar on the bottom of the
window, as shown in Figure 8-2.
Figure 8-2. DirectQuery mode
Dual mode
A table in Dual covers both features: It is _Import_ed into
the data model and all connection meta-data are kept in
the data model, so it can be used in _DirectQuery_s as
well. In “Dual Mode” I will explain in which scenarios it
makes sense to set a table to this storage mode.
Live connection mode
Live connection is similar to DirectQuery in the means that
no data is loaded into the VertiPaq engine. But it is
completely different if you look at the technology under
the hoods. You cannot set storage mode to Live connection
on a table-by-table basis (as you can do with the other

storage modes, shown in Figure 8-1). Live connection
mode is only possible for analytical databases, which can
be queried in DAX (as a Power BI semantic model, Azure
Analysis Services or SQL Server Analysis Services
Tabular). The whole data model is then in live connection
mode (unless you choose to Import all necessary tables or
choose DirectQuery mode when you make the first
connection to the data source database). In Live
connection mode you can create explicit measures, but
many other data modeling features are not available (like
using the Power Query Editor to transform the date or
adding calculated columns or calculated tables). If you
need such features, you need to change into DirectQuery
mode instead.
Live connection is the default mode, when connecting to a
Power BI semantic model or Analysis Services Tabular
databases. You can convert such a Live connection with just a
few clicks into a DirectQuery connection, by clicking on Make
changes to this data model on the bottom of the screen, as
shown in Figure 8-3.
Figure 8-3. Live connection mode

Power BI asks you for confirmation before it executed the
conversion (Figure 8-4). You do confirm by clicking Add local
model.
Figure 8-4. Converting to a DirectQuery connection
In the next step you need to decide which parts of the remote
data model should be made available into the current data
model (Figure 8-5). Under Settings you can decide if Power BI
should add tables, which were added later, automatically to the
model (which is the default) or not.

Figure 8-5. Chosing the tables
Then, press Submit to actually converting the data model and
loading data model’s meta data (s. Figure 8-6).

Figure 8-6. Loading the (meta) data
The status bar on the bottom of the window will change to
Storage mode: DirectQuery as you saw in Figure 8-2).
WARNING
Converting from Live connection to DirectQuery cannot be undone, if you should later
change your mind. Therefore, I recommend making a backup of the PBIX file, before
you convert the type of connection, just in case.
While your data model is in DirectQuery mode now, you can
still decide to import additional tables into the Power BI data
model or add another DirectQuery data source. If you do so,
Power BI will warn you about a Potential security risk (Figure 8-
7).

Figure 8-7. Warning about a potential security risk
This potential security risk becomes a real security risk under
two circumstances:
You or any consumer of the data model combines
information from both data sources in one single visual.
Power BI does not even temporarily load the data from
both DirectQuery data sources to combine it into a single
query result. Instead, it pushes the join key’s values as
hardcoded values into the query sent to one of both
DirectQuery data sources.
The join key for both tables contains sensitive information,
which must not be seen by anybody with access to the
query code. An administrator of a database, for example,
has access to the queries running against the database she
is responsible for. In a man-in-the-middle attack someone
would be able to capture the code of the queries sent to a
database.

În the following example you’ll see the portion of a SQL query
generated by Power BI in order to join information from a date
table in Import mode (with alias semijoin1) with a fact table in
DirectQuery mode (with alias basetable0). The whole list of
values of the date table’s date column becomes part of the
query sent to the DirectQuery data source. This code can be seen
by the database administrators or attackers who can capture
the query code. As I do not consider the list of dates as sensitive,
I don’t see a problem for this concrete query:
...
INNER JOIN
(
(SELECT 368 AS [c42],CAST('20230101 00:00:00' AS 
(SELECT 369 AS [c42],CAST('20230102 00:00:00' AS 
(SELECT 370 AS [c42],CAST('20230103 00:00:00' AS 
...
(SELECT 730 AS [c42],CAST('20231229 00:00:00' AS 
(SELECT 731 AS [c42],CAST('20231230 00:00:00' AS 
(SELECT 732 AS [c42],CAST('20231231 00:00:00' AS 
)
 AS [semijoin1] on
(
([semijoin1].[c16] = [basetable0].[c16])
...

As soon as you combine tables with different storage modes in
one single data model, we speak of a Composite model and the
status bar shows Storage mode: mixed (s. Figure 8-8).
Figure 8-8. A composite model has a mixed storage mode
It is common to speak of islands or source groups when
someone refers to data in different storage modes and/or from
different DirectQuery data sources. In this sense, imported data
always belongs to the same island, no matter from which data
source it was imported from. Data in DirectQuery mode belongs
to the same island only if the tables are from the same data
source. This concept is of importance, when it comes to
performance: The potential security risk for cross-island
queries is also a potential performance risk. Executing queries
with long code (due to injection of filters as hardcoded filter
values into the query code) will put more pressure on the
source system, as if the filter could have been applied as an
inner join between two physical (and indexed) tables inside one
datatabase.

WARNING
All relationships across different islands, independent of their cardinality (like one-
to-many, one-to-one or many-to-many), are limited relationships. You already learned
about this term, in “Relationships” or Limited Relationships). In the model view such
relationships are represented with paranthesis-like marks after the cardinality
indicators. Tables are joined with an inner join (opposed to an outer join for regular
relationships), no blank rows are shown for rows violating the referential integrity.
Maybe you are asking yourself now, when to choose which
storage mode? I would suggest the run through the following
checklist before you make your decision:
When your data source is a Power BI semantic model or an
Analysis Services database, choose Live connection.
Performance for reports will be very good.
When you need to enrich the data model provided as Power
BI semantic model or an Analysis Services database,
migrate to DirectQuery mode and import the additional
information. Pay special attention to the performance of
cross-island queries.
In the other cases you should consider importing all the
data into the data model. It will give you a superior report
performance.
Only when at least one of the following criteria is met,
evaluate if you can live with all the limitations of

DirectQuery and a less than optimal query performance:
Use DirectQuery when the amount of data is too big to
load it into Power BI. “Too big” in this case means that
it hits either the physical limitations or your budget.
Keep in mind that Power BI Premium and Microsoft
Fabric always store data in a compressed format and
that a dataset can contain up to 100 TB. 100 TB of
compressed data can easily hold uncompressed
information of beyond 1 Petaybytes (= 1000 TB).
The data source has a complex row level security in
place, which cannot be replicated in Power BI or
Analysis Service Tabular. Keep in mind that row level
security can be either implemented over (Azure Active
Directory) roles or in a dynamic fashion inside the data
model (Row-level security (RLS) with Power BI).
Another reason could be that refreshing the data model
takes too long. “Too long” means that the refresh time
would put too much pressure on the data source for a
too long period of time or that after the refresh finally
has finished, the data is already stale again. In a
(near-)real-time scenario the latter can be an even very
short period of time (like, a couple of minutes or
seconds). Keep in mind that with incremental refresh
or your own partition strategy you can speed up data

refresh, so that the imported data becomes not stale (s.
“Partitioning”). Make sure to compare the query-
runtime between a model over the same data in Import
mode and in DirectQuery mode; not that the
DirectQuery model sabotages your real-time goals with
too long runtimes (which can be minutes or even run
into a timeout).
PERFORMANCE ANALYZER
Chose View - Performance Analyzer to collect information about the runtime of
visuals built in Power BI Desktop. These collection will tell you how much time was
spent on running the DAX query (executed locally) and/or Direct query (executed on a
remote database), how long it took Power BI Desktop to “draw” the visuals (Visual
display) or how much time a visual waited for Other visuals to finish. Via Copy query
you get access to the actual DAX (or SQL) queries executed. You can paste this to
either DAX Studio (Download DAX Studio), an open-source tool by Darren Gosbell
(Senior Program Manager at Microsoft) or SQL Server Management Studio
(Download SQL Server Management Studio (SSMS)) to further investigate the query
plans or possible optimizations. Optimization of DAX queries and SQL queries is
beyond the scope of this book.
Independently from the available storage modes, you can
decide to split a table into easier-to-manage sub-entities. This is
what you learn in the next section.

Partitioning
NOTE
At time of writing creating partitions is only available through the XMLA endpoint,
which is a a premium feature in Power BI/Fabric.
Partitioning is a way to split a table into smaller parts (=
partitions). Instead of one big storage entity covering all rows
you end up with several smaller storage entities with less rows
each. This can improve query time, because scanning for data
can be limited to the partitions where metadata tells that they
might contain necessary information. But more importantly,
you can schedule a refresh on partition-level. Instead of always
triggering a full refresh (for the full content of a table), you can
trigger to only update certain partitions; for example, those
containing data from the last few days, where a change in the
data source could have happened. With that said, it makes
sense to partition your big tables by a time stamp related to a
transaction.

CHOOSING THE PARTITION KEY
Do not partition data on metadata like creation date or
modification date. First, such a timestamp will most probably

not be used for filtering in the majority of your reports. Only a
minority of users might be interested when a fact was created;
instead, most of the people will be interested in, to which day a
fact is related (= when the fact happened). When the fact is
created, this day might be in the past or in the future. For the
best performance, the partition key must be the first column of
every index (so that the index is aligned with the partitions). If
the partition key is not part of the filter, then the index will not
be used.
Secondly, the modification date of a row can be changed (with
every modification) and will then move a row between
partitions. If not both (the old and the new partition) are
refreshed, you end up with duplicates of this row in your data
model (as the row will loaded into the news partition but still
will be in the old partition as well, as long as the old partition is
not refreshed). If you do not know what the previous value of
the modification date was, you need to update all partitions.
This would defeat the whole purpose of creating partitions.
TIP
Use a fact-related timestamp instead: the booking date or the day of
order, which will not change in a later point in time.

In a project I was confronted with the requirement that a
change in the data source had to be available in a Power BI
report within ten minutes. Turned out that even with heavy
performance tuning in the data source these complex queries
would take longer than 10min on average in a DirectQuery
setup to finish. (So much about real-time and DirectQuery.) In
the final solution, I partitioned the table. As a partition key I
used a special column, which was created only for the purpose
of partitioning. Every time a change to the source table
happened, the row’s partition key was logged in a separate
table. Every five minutes a job was triggered, to read this
separate logging table and trigger a data refresh for those
partitions, where changes had happened since the last run of
this job. After we found the right number of partitions (with the
goal of not having them containing too many rows, so the
refresh is fast enough, but also not to have so many partitions
to refresh all the time), the regular refreshes took only a couple
of minutes. And the report response time with the imported
data was sub-second (compared to 10+ minutes in the
DirectQuery approach).
Power BI Desktop only allows to define partitions over the
feature Incremental refresh. As the name suggests, this enables
to refresh data of a table in increments. You need to follow
three steps if you want to enable this feature for a table. First,

you need to define two Power Query parameters with the
mandatory name RangeStart and RangeEnd. In Power Query
select Home - Manage Parameters and create them via New. As
you can see in Figure 8-9, these parameters must be of Type
Date/Time.

Figure 8-9. Power Query Parameter for the start of the time range of an incremental
refresh
Second, you need to use both parameters in the Power Query of
the fact table (for which you want to turn incremental refresh
on) as a filter. Figure 8-10 shows how you create a Between filter

on the OrderDate  column, which will be the partition key in
this example.
Figure 8-10. Creating a filter on the partition key
The filter itself references the two parameters, making sure that
only rows where the OrderDate  is after or equal to
RangeStart  and is before RangeEnd  are loaded from the data
source, as shown in Figure 8-11.

Figure 8-11. Filtering the partition key with the parameters
In the Model view you need then to right-click the table and
choose Incremental refresh. Then you proceed in four steps (as
shown in Figure 8-12):
1. Select table
The table is already pre-selected, but you can choose any
table of the data model.
2. Set import and refresh ranges
Turn on Incrementelly refresh this table. This is only
possible if you already have defined the RangeStart and
RangeEnd parameters in Power Query. You then
decide,how many periods of data you want to keep in the
table. I chose to discard anything older than 5 years in the

example. Additionally, I set Incrementally refresh data
starting to 1 Months before data refresh.
3. Chose optional parameters
I kept Get the latest data in real time with _DirectQuery
(Premium only)_ disabled for this example. In “Hybrid
Tables” you will learn about this option. Only refresh
complete months I have enabled. If you choose to
Incrementally refresh data starting days or year, this
setting will offer accordingly complete days or years as
well. If you enable Detect data changes a refresh will not
run by a fixed cadence, but only when data changed.
Refer to Incremental refresh and real-time data for
datasets to learn more about this feature.
4. Refresh and apply
Here you see graphical representation of the settings
done.

Figure 8-12. The dialog box guides you through the steps necessary to turn incremental
refresh on for a table

Partitioning splits existing data into smaller parts. In the next
section you will learn how you can optimize query-runtime be
intentionally introducing duplication of data in an pre-
aggregated form.
WARNING
At the time of writing, enabling incremental refresh is one of the reasons which
disables the option to download a Power BI file from the Power BI Service.
Limitations of downloading a .pbix file.
Pre-Aggregating
There is no reason why you could not load the same piece of
information several times into the data model. For example,
you can load the transaction table on the lowest necessary
granularity (so that even very detailed analytic requests are
satisfied), and load it once again, on an aggregated level (e.g.,
one row per day and product). This enables your data model to
satisfy detailed information (from the transaction table) and do
fast calculations (from the aggregation table) as well. This
concept works for both imported data or tables in DirectQuery
mode. Maybe the detailed data is only needed very rarely. Then
you could keep the transaction table in DirectQuery mode to

safe storage and refresh time and only import the aggregated
version of this table to enable fast reports for the majority of
analysis. Reports on the detailed data will take longer to
execute, but this may be acceptable.
For rather simple requirements, Power BI does fully support
such a performance optimization. You can specify, on which
granularity the aggregation table does aggregate data, which is
available in the detailed table. The detailed table must be in
DirectQuery storage mode. In Chapter 12, Chapter 16, and
Chapter 20 you will learn how to create a aggregation table. Her
I show you, how to tell Power BI in which cases it should use
the aggregation table instead of the detailed table to satisfy
queries.
As you’ll see in Figure 8-13 that I have two tables: Reseller
Sales (Direct Query – Agg)  which contains all rows in the
most detailed granularity (date and product) and Reseller
Sales (Agg Table PQ)  which contains the same
information, but aggregated by date only (with the product key
omitted).

Figure 8-13. Managing aggregations in your data model
There is no filter relationship between these two tables. If you
right-click Reseller Sales (Agg Table PQ)  and choose
Manage aggregations (Figure 8-13) the selected table is already
pre-selected (but you could chose a different on via the list box
Aggregation table). If more than one aggregation table could
satisfy a query, then the Precedence is used to decide which one
to choose. The aggregation table with the higher value in
Precedence is preferred.

The aggregation table in Figure 8-13 contains three columns.
For each you need to tell Power BI how it relates to the detailed
table or the rest of the model. In my example, column
OrderDate  is used to GroupBy column Date  of table Date
(DirectQuery) . Column SalesAmount  is the Sum of my
detail table’s ( Reseller Sales (Direct Query – Agg) )
column SalesAmount . And column SalesCount  is calculates
a Count table rows again over the detail table ( Reseller
Sales (Direct Query – Agg) ). Based on this information,
Power BI will make informed decisions when an aggregation on
column SalesAmount  of the detail table ( Reseller Sales
(Direct Query – Agg) ) can be satisfied with an aggregation
on the aggregation table ( Reseller Sales (Agg Table PQ) )
instead.
To prove if Power BI uses the aggregation table correctly, I
created a report with two table visuals: both contain the same
measure Sales Amount (DirectQuery - Agg)  (which is
defined as the sum over column SalesAmount  of the detail
table: SUM(Reseller Sales (DirectQuery - Agg)
[SalesAmount]) ) and the CalendarYear  column (from table
Date (DirectQuery) ). One additionally contains column
Style  from the Product  table. As the aggregation table does
not contain product specific information, but is aggregated on
the day only, it is expected that Power BI uses the aggregation

table to calculate the measure in the first visual only. Only the
detailed table contains product specific information; therefore,
it is expected that is used in the second visual.
In the Performance Analyzer pane on the right part of Figure 8-
14 you can see that for the first visual (named Date only) only a
DAX query is available. Obviously, the aggregation table was
use, which is in Import mode. For the second visual (named
Product & Date) DirectQuery is mentioned. That is the sign that
the detailed table (which is in DirectQuery mode) was used to
satisfy the query.
Figure 8-14. Analyzing the performance of report of different granularity
In more advanced scenarios (e.g., when the detailed table is in
Import mode) you cannot use Power BI’s Manage aggregations
but you need to add logic to your measures instead: The code in
the measure decides on from which table it can satisfy a

calculation in order to achieve the best possible performance.
Chapter 12 explains how you can create such special measures
in DAX.
In “Storage Mode” I already introduced the term Composite
model. In the next section I will explain use cases for such data
models.
Composite Models
As described in “Storage Mode”, Power BI allows you to set the
storage mode per table. You can build a data model, where
some of the tables are imported, while others are in DirectQuery
mode. (Composite models are not possible when you are in Live
connection mode.)
This feature helps to design performant data models: For
example, you can import all the (small) dimension tables into
the data model, but keep the big dimension tables and the fact
tables in DirectQuery mode. You can also create aggregation
table (as described in “Pre-Aggregating”) for those big tables,
which you either import (to give queries on the granularity of
the aggregation table the best possible performance) or keep in
DirectQuery mode (to save storage space and refresh time and

still give a better performance, if the query to get the
aggregated data is properly tuned for performance on the data
source’s side).
Composite models are common, when you build reports based
on analytical databases (like a Power BI semantic model or an
Analysis Services Tabular database). These databases already
contain an optimized model, calculations in the form of
measures and all sorts of meta-data (like hierarchies or
translations), which you do not want to rebuild from scratch.
Instead you can connect to such a data source in DirectQuery
mode and enrich it by importing other important data sources -
and thereby creating a Composite model.
Not only a data model can have a mixed storage mode, but also
a table. Read on to learn more about Dual mode tables.
Dual Mode
When a single query references data from both, a table in
Import mode and a table in DirectQuery mode, then there are
two possible ways to resolve this. Either, for the sake of the
query, all the necessary rows from the table in DirectQuery
mode are temporarily loaded into the current model and the

joins are resolved inside the model. If the reason to choose
DirectQuery mode for a table was its size, than transferring a
big amount of rows from this table into the data model might
be not a good idea (as it might take some time and it will put
pressure on the resources on Power BI’s side that means your
computer or the Power BI Service). Therefore, VertiPaq choses
the other option: It pushes all filters to the data source by
adding all necessary filters in the query text. In “Storage Mode”
you have already seen that this leads to long (and therefore
probably less performant) queries and potential security risk.
A special storage mode I have not mentioned so far, is a solution
here. A table can be set to Dual mode. A table in Dual mode is
refreshed together with all other tables which are in__Import
mode_ and takes up physical space in the data model.
Combining information from this table with other tables
imported into the data model, makes full advantage of the in-
memory engine. On the other hand, all necessary meta
information is stored in the data model to allow to reference it
in DirectQuery queries. When columns from this table are
combined with columns from DirectQuery tables of the same
island the generated SQL statement can make use of ordinary
joins, instead of injecting filters into the query code. It can be
expected that the query runs faster as all necessary data to

satisfy the query is stored in the data source and access to it can
be fully optimized (e.g., via indexes).
You change the storage mode in the model view in the table’s
properties pane.
Hybrid Tables
Combining imported data and DirectQuery is not only possible
within one data model, but also within one single table. The
idea is, to import all the “old” data, which will not change
anymore, but to keep all the recent data in DirectQuery mode so
it will be shown reports as it is imported or into the data source.
Basically, you change the mode per partition of the table. At the
time of writing in Power BI Desktop you can activate this setting
only in the Model view via Incremental refresh by enabling Get
the latest data in real-time with _DirectQuery (Premium only), as
shown in Figure 8-12. It will automatically keep the latest
partition in _DirectQuery mode, and the “older” partitions
in__Import mode_.
The concept could also be turned around: Only loading the data
for the current reporting period into the data model, but
keeping the old data, which is just rarely queried, in

DirectQuery. Reports covering the standard periods will be fast,
but the data model will not be inflated in terms of size by
loading older periods. In the rare case, someone needs to report
on the old data, the report will work as usual, but will simply
take more time to execute. This can be a good compromise, as
people will appreciate the fast query response for the standard
use cases and tend to accept that the data for special periods of
time will not be cached. To implement such a flexible concept
you need to run XMLA scripts against the Power BI Service.
Announcing Public Preview of Hybrid Tables in Power BI
Premium shows how to achieve this.
NOTE
At the time of writing, hybrid tables are only available in a premium workspaces. As
soon as you
Key Takeaways
In this chapter you have learned that there are many options in
terms of storage mode to store either the data or only metadata
inside Power BI. Combining these options in a smart way can
increase the overal performance of the data model by
optimizing the trade-off between refresh time and query time.

Power BI offers three basic storage modes: Import,
DirectQuery and Live connection. Imported data offers the
best report response time, but the data needs to be
refreshed periodically. DirectQuery and Live connection
have in common that no refresh of the data model is
necessary, but the queries might run a longer period of
time. Live connection is only available for Power BI
semantic models and Analysis Services databases. They
come with a lot restrictions in term of data modeling
options. DirectQuery is only available for relational
databases, Power BI semantic models and Analysis Services
databases. Tables in DirectQuery mode can be combined
with tables in Import mode in one single data model,
forming a composite model.
Partitioning allows you to split a table into smaller parts.
These parts can be refreshed independently. By only
refreshing partitions where a change in data has happens,
you will speed up the overall refresh time for your data
model.
Tables with pre-aggregated contents are exchanging storage
for query run time, as some queries can be satisfied with
the (smaller) table containing the pre-aggregated values.
Only if data from a more granular level is needed, the
bigger table is queried.

Composite model is the term which describes a data model,
in which storage modes are mixed. Some tables are in
Import mode, while others are in DirectQuery mode. (Live
connection does not allow for a composite model.) Such a
model can be a trade-off between storage size and query
runtime.
Dual tables store both the imported data and the meta data
to allow for DirectQuery. Such tables allow Power BI to
facilitate the full power of the in-memory engine when
combined with other imported tables; or injecting all logic
and join operators into the SQL query when combined with
other DirectQuery tables.
Hybrid tables apply the idea of a composite model to a
single table: partitions of a table can be in different storage
mode. When real-time is the goal, then you import old data,
but keep the most recent partitions in DirectQuery mode. If
you need to optimize model size, then you only load the
most recent data, but keep the old partitions in DirectQuery
mode.
Equipped with all the theoretical concepts and their use in
Power BI it is now time to get you to actively transforming your
data to fit into the desired data model. The next chapter will
introduce you to DAX as a data transformation tool. In later

chapters you learn how to achieve the same results with the
help of Power Query/M and SQL.

Part III. Data Modeling for Power BI
with the Help of DAX
Figure III-1. Example navigation
Welcome to the DAX part of this book! DAX stands for “Data
Analysis Expression” and is the language to create calculated
tables, calculated columns and measures, define row-level
security and is used to query the data inside of Power BI. The
latter is done by Power BI’s visuals for you - so there is no need
to write a DAX query for most of the people.
If you have read this book from the start, you should now have
a good understanding of the importance of the shape of a data

model and that it will be necessary in many situations (if not
all!) to re-shape what you get from your data source.
This part is not an introduction to the DAX language, but you
will learn that the data model and DAX are going hand-in-hand
very tightly and I will introduce you to the moving parts of the
language (Chapter 9):
Calculated Tables
Actively changing relationships for the sake of a calculation
Creating a single Primary key
Combining tables in DAX
In the next chapter (Chapter 10) I will show you, how you can
use DAX to re-shape your data model:
Normalizing
Denarmalizing
Adding calculated columns and measures
Transforming flags and indicators into meaningful text
Building your own date and time table
Implementing role-playing dimensions
Making the best out of Slowly Changing Dimensions
Flattening parent-child hierarchies

DAX plays an important role in the real-world use cases,
discussed in the first two parts of this book, as well. Chapter 11
will introduce you to DAX solutions for:
Binning
Multi-fact data models
Multi-lingual data models
Key-Value pair tables
Finally, Chapter 12 points out, what you can do with DAX to find
a good trade-off between loading data into Power BI or directly
querying the data source instead.

Chapter 9. Understanding a Data
Model from DAX Point-of-View
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 9th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
When it comes to Power BI, not all data modeling challenges
can be solved in Power Query or the data source. Some data
modeling solutions you can only be implement inside a DAX
measure. I will point out such solutions in detail where
appropriate. And it’s important to understand that the data
model and DAX go together very tightly. That means that in

some situations you can exchange complexity in the Model view
against complexity in a DAX measure, or the other way around.
You have already seen such situations in Chapter 7. I will point
out such situations in this and the following three chapters as
well.
But before I dive into the complex examples, I want you to first
understand how DAX “sees” the data model.
Data Model
The relationship between DAX and the data model is two
folded: You can use DAX to add calculated columns to a table
and even create whole tables. On the other hand, DAX uses the
information provided by the data model (read: filter
relationships) to navigate through the model and access the
data needed for the calculation.
To be more precise: Whenever you write a formula (or a query)
in DAX it is passed forward to the formula engine, which takes
the appropriate steps (requesting values from the storage
engine and doing its own calculations in case the storage engine
cannot execute the calculations due to complexity). Both the
formula engine and the storage engine use all the information
available from the data model to do their job.

Look at a simple formula like SUM(Sales[SalesAmount]) . It
will aggregate the SalesAmount  column of table Sales  by
adding up all available values. In an unfiltered context (like
during evaluation inside a calculated column or in a report,
where no filter is provided), this will result in the grand total
over the content of the whole table. The same calculation will
show a different (= smaller) number, when you provide a filter
context, e.g., in the by adding a slicer on the report and selecting
a certain year or product. This will be no surprise because this
is the expected behavior and quiete intuitive.
In “Data Model” you already learned that you should not put a
filter directly on a fact table (the Sales  table, for example), but
on the dimension tables, like Date  or Product . If you have
correctly defined the filter relationships between the dimension
tables and the fact table, these filters are automatically
propagated from the dimension table to the fact table for you.
The information defined in the model helps to write rather
simple DAX formulas, as we do not need to repeatedly specify
that the Sales  table needs to be filtered by the other tables in
every single formula, but define this only once in the data
model in the Model view.
A data model consists of tables and relations between them.
Let’s first look at how you can use and create tables in DAX.

Basic Components
The following basic components are important to understand as
you learn more about DAX: tables, relationships, and primary
keys.
Tables
NOTE
The file used for the demonstrations is DAX table.pbix
In DAX you can refer to a table by just mentioning its name. If
the table’s name is a reserved keyword, matches a DAX
function’s name, or contains special characters (like a space or
an umlaut), then you must specify the table’s name under single
quotes. Common table names where you must specify the single
quotes are Date  and Product . I tend to specify single quotes
every time I remember it, to make my code more conform. That
means I refer to Sales  (including the single-quotes, even
when optional in this case) instead of just Sales  (which would
syntactically be no problem). Look up DAX syntax in case you
are in doubt about the need for single-quotes or other syntax
related question.

TIP
By looking at a piece of DAX code it can sometimes be hard to recognize if it is the
definition of a calculated column, a measure or a calculated table. Therefore, I use
the following convention:
[Measure Name] :=
   <definition of a measure>

'Table Name'[Column Name] =
   <definition of a calculated column>

[Table Name] =
   /* calculated table */
   <definition of a calculated table>
To create a table in DAX, you first choose Modeling - New table
and then you have several options to specify a DAX code, which
must be a table expression. Here is a list of simple examples, to
give you an idea:
Example 1
Use braces (“{}”) to specify a table expression, returning
one single row.
My 1st table = /* calculated table */
{
    1,

    "one"
}
Table 9-1. My 1st
table
Value
1
one
Example 2
Use parenthesis (“()”) inside of braces (“{}”) to specify a
table expression, containing several rows.
My 2nd table =  /* calculated table */
{
    (20, "twenty"),
    (21, "twentyone")
}

Table 9-2. My 2nd table
Value1
Value2
20
twenty
21
twentyone
Example 3
Function ROW()  allows you to specify pairs of column
names and expressions. This creates one single row.
My 3rd table =  /* calculated table */
ROW(
    "My first column", 3,
    "My second column", "three"
    )
Table 9-3. My 3rd table
My first column
My second column
3
three
Example 4
For function DATATABLE()  you can pass parameters to
specify both the datatype and the name of the columns in

one go.
My 4th table =  /* calculated table */
DATATABLE(
    "MyNumber", INTEGER,
    "MyText", STRING,
    {
        {40,"fourty"},
        {41,"fourtyone"}
    }
)
Table 9-4. My 4th table
MyNumber
MyText
40
fourty
41
fourtyone
Example 5
The simplest table expression is to type the name of
another existing table. This way you duplicate the full
content of the table under a new name.
Referenced table = 'My 1st table'

Table 9-5.
Referenced table
Value
1
one
Example 9-1.
I use table expression Measure = {BLANK()}  as a shortcut to
create a dedicated (calculated) table which contains all my
measures (if report-creators requested to have measures
collected into one place in the data model, as opposed to be
spread them out into the data model’s different tables). This
expression creates a table with one single column (with name
Value ) and one single row (containing blank). Don’t forget to
hide the only column of this table after you created your first
measure. When a table only contains measures and all columns
are hidden, then the table becomes listed on top of Power BI’s
field list in the Data pane section of Power BI Desktop.
References to columns should always mention the table name
where the column resides in. The column’s name must be
enclosed in brackets ([]), independently if the name contains
special characters or not. The following example passes the

reference to column SalesAmount  of table Sales  as the
parameter for function SUM .
SUM(Sales[SalesAmount])
Tables don’t exist just for themselves but are in relation with
other tables (as you already learned in “Tables”). Let’s look at
how relations affect DAX expression in the next section.
Relationships
NOTE
The file used for the demonstrations is Relationship.pbix
In most cases, you will not specify any relationships in DAX, but
just rely on what you already configured in the model view of
Power BI. This is because you should create all relationships in
the Model view, so that Power BI can create sort-of indexes on
these relationships. In Chapter 10 and Chapter 11 I will
demonstrate situations where it is necessary to create a
relationship inside a DAX measure instead.
One of these use cases is, when you have inactive relationships
(discussed in “Relationships”). DAX allows you to activate an

inactive relationship, just for the sake of the current measure.
To achieve that, you need to wrap the expression inside the
CALCULATE  function and add USERELATIONSHIP  (speak as:
use relationship) as the second parameter. USERELATIONSHIP
itself is a function as well. Via the two parameters you specify
the name of the columns used in the inactive relationship.
'Sales'[Ship Quantity] :=
   CALCULATE(
      SUM('Sales'[Quantity]),
      USERELATIONSHIP('Sales'[Ship Date], 'Date'
DAX is smart enough to then deactivate the active relationship
implicitly, so only one single relationship is active during the
evaluation of the calculation.
With function TREATAS  (speak as: treat as) you can create a
relationship inside a DAX formula which is not available in the
model view. Such a relationship is called a virtual relationship.
This can be very helpful in certain use cases, but can bear a
performance penalty, as the engine cannot rely on pre-
calculated information about how to join the tables (as it can do
with explicitly defined relationships in the Model view).
Therefore, use TREATAS  with care, and instead, rely as much

as possible on (physical) relationships in the data model. I will
show examples, where I used TREATAS  in real world scenarios
in “Budget”.
The only way to avoid USERELATIONSHIP  and TREATAS  is to
build your data model in a different way, so DAX can rely on
active physical relationships. In situations, where you decided
against such active physical relationships, one or the other
function can be very helpful. Both functions are an example for
situations, where you need to write your DAX measures in a
certain way; this cannot be substituted with Power Query or
SQL (unless you use Power Query or SQL to create a data model
which can fully rely on active physical relationships).
Relationships in a data model are based on primary and foreign
keys. DAX is no exception.
Primary Keys
In DAX there is no explicit concept of primary keys. As
described in the previous section, you should preferably define
all relationships in the Model view and therefore describe the
relationships between the tables (which itself preferably have a
one-to-many cardinality and therefore describe implicitly a
relationship between a primary key on the one side and a

foreign key on the many side). These relationships can only be
created on a single column; composite keys are not supported
in Power BI.
Composite primary (and foreign) keys must be converted into
one single key with the help of the CONCATENATE  function or
the &  operator. As CONCATENATE  only allows for pair of
parameters, you need to wrap multiple calls to this function to
achieve, what you can do with a shorter piece of code with the
&  operator:
CustomerKey =
    CONATENATE(
            Customer[Firstname],
            CONCATENATE(
                    "|",
                    Customer[LastName]
            )
    )
CustomerKey = Customer[Firstname] & "|" & Custome
One way to combine tables is, to append one table to another.
This is the basic idea of set operators, as discussed in the next
section.

Combining Queries
NOTE
The file used for the demonstrations is Functions Relational.pbix
Combining quers can be done is basically two different ways:
Either you use set operators or you use join operators.
Set Operators
DAX offers functions for the usual set operations (discussed in
“Set Operators”). For the examples in this section, I use the
Product  table, which contains a List Price  column and the
Sales  table, which contains a Price  column, as shown in
Table 9-6 and Table 9-7.

Table 9-6. Product
Product ID
List Price
100
10
110
30
120
110
130
200
Table 9-7. Sales
Date
Product ID
Price
2023-08-01
100
10
2023-08-01
110
20
2023-08-02
110
30
2023-08-03
120
100
UNION
This function allows for multiple parameters. Duplicated
rows are not removed. If you only want to keep unique
rows, you need to wrap UNION  into DISTINCT .

In Table 9-8 you can see the result of an example where I
apply the function to the available VALUE+s of the
product’s +List Price  and the sales’ Price  column
inside the definition of a calculated table. The List
Price  of 10  appears twice in the result:
UNION = /* calculated table */
UNION(
   VALUES('Product'[List Price]),
   VALUES(Sales[Price])
)

Table 9-8. UNION
List Price
10
30
110
200
10
20
30
100
INTERSECT
Returns only rows, which appear in both tables. The
following example results in the rows shown in Table 9-9.
INTERSECT = /* calculated table */
INTERSECT(
   VALUES('Product'[List Price]),
   VALUES(Sales[Price])
)

Table 9-9.
INTERSECT
List Price
10
30
EXCEPT
Returns only rows, which appear in the first table, but not
in the second. The result of the following code is shown in
Table 9-10.
EXCEPT = /* calculated table */
EXCEPT(
   VALUES('Product'[List Price]),
   VALUES(Sales[Price])
)
Table 9-10. EXCEPT
List Price
110
200

A different way of combining tables is to synchronize their
rows and create a query result, which consists of the combined
list of columns for both of the tables. Learn how to implement
such joins in the next section.
Joins
In most scenarios you will just rely on the existing tables (and
their relationships) in the data model. In certain use cases it
could make sense to create a calculated table (to persist the
result in the data model in order to speed up queries) or use a
table expression inside a measure (to solve advanced use cases
by explicitly joining tables and iterating over the result).
For the examples I use the following two tables Product  and
+Sales* from the previous section (Table 9-6 and Table 9-7)).
That’s the DAX functions you can use:
NATURALLEFTOUTERJOIN
This function applies a left outer join on the specified two
tables. This function comes in handy in cases, where you
decided against creating an active relationship in the
Model view. Usually, as mentioned previously, you would
explicitly create an active relationship between those two

tables in the data model. If a relationship does exist, you
can still join the two tables with
NATURALLEFTOUTERJOIN , but must first break the data
lineage of the tables, by manipulating the key columns
(e.g., by adding 0 or by concatenating an empty string to
the key columns). Keep in mind that in a natural join you
cannot specify the join predicate (= the names of the
columns to use for the join operation) – therefore, the key
columns have to have the exact same name in both tables.
Left outer means that all the rows from the first (= left)
table are kept, and information from matching rows of
the second table are added. This join is implemented as an
equi-join.
In the first example I put the Sales  table as the first (=
left) table. I need to wrap the reference to the Product
table inside ALLEXCEPT  to get rid of its Product ID
column, as the result would otherwise contain two
Product ID  columns, which is not allowed.
NATURALLEFTOUTERJOIN Sales = /* calculated t
NATURALLEFTOUTERJOIN(
    'Sales',
    ALLEXCEPT('Product', 'Product'[Product I
)

And here is the result:
Table 9-11. NATURALLEFTOUTERJOIN Sales
Date
Product ID
Price
List Price
2023-08-01
100
10
10
2023-08-01
110
20
30
2023-08-02
110
30
30
2023-08-03
120
100
200
If I exchange the order of the two tables inside
NATURALLEFTOUTERJOIN , the result will be slightly
different. First the orders of the columns in the result are
exchanged. And second, as the Product  table contains a
Product ID  for which there are no entries in the
Sales  table the number of rows change. When the
Product  table is on the right side of a left outer join, this
row is omitted from the result. When the Product  table
is on the left side of an left outer join, then this row is kept
and shown in the final result:

NATURALLEFTOUTERJOIN Product = /* calculated
NATURALLEFTOUTERJOIN(
    ALLEXCEPT('Product', 'Product'[Product I
    'Sales'
)
And here is the result:
Table 9-12. NATURALLEFTOUTERJOIN Product
List Price
Date
Product ID
Price
10
2023-08-01
100
10
30
2023-08-01
110
20
30
2023-08-02
110
30
200
2023-08-03
120
100
NATURALINNERJOIN
This applies a natural inner join on the specified two
tables. All the rules about relationships and data lineage
apply here as well. As this is an inner join, only rows, with
matching keys in both tables are kept. This join is
implemented as an equi-join.

In our example, the result of the inner join matches the
result of the left outer join in the case the sales table was
on the left side, as you can see for yourself:
NATURALINNERJOIN = /* calculated table */
NATURALINNERJOIN(
    'Sales',
    ALLEXCEPT('Product', 'Product'[Product I
)
And here is the result:
Table 9-13. NATURALINNERJOIN
Date
Product ID
Price
List Price
2023-08-01
100
10
10
2023-08-01
110
20
30
2023-08-02
110
30
30
2023-08-03
120
100
200
CROSSJOIN

Creates the cartesian product of the two specified tables.
This works also in a table (expression) where no physical
relationship was created in the data model.
CROSSJOIN = /* calculated table */
CROSSJOIN(
    DISTINCT('Sales'[Date]),
    DISTINCT('Product'[Product ID])
)

Table 9-14. CROSSJOIN
Date
Product ID
2023-08-01
100
2023-08-02
100
2023-08-03
100
2023-08-01
110
2023-08-02
110
2023-08-03
110
2023-08-01
120
2023-08-02
120
2023-08-03
120
2023-08-01
130
2023-08-02
130
2023-08-03
130
I will return to these functions in “Denormalizing” and show
you, how you can use them to denormalize a data model.

The whole idea of combining tables in this book is to bring
them into a different shape to create the perfect data model
(which is a Star schema). But there is much more you will need
to do. These tasks are called Extract, Transform and Load. The
next section explains the role of DAX in this task.
Extract, Transform, Load
All the necessary steps to extract, transform, and load data are
ideally done before the data is loaded into Power BI (s. “Extract,
Transform, Load” and “Extract, Transform, Load”). DAX is less
ideal for implementing the ETL, as it can only builds on top of
tables which are already loaded into the data model. If you use
DAX to model the data, then your model will contain both, the
un-modeled data and the modeled data, thus unnecessarily
increasing the size of your data model. The size of a model is
visible through the size of the .pbix file, but also how much the
data model will occupy in memory, when the .pbix file is
opened in Power BI Desktop or an Analysis Services Tabular
database is refreshed.
But cleaning, transforming, and modeling your data in DAX (as
shown in Chapter 10) is better than not to model the data at all.

Key Takeaways
In this chapter you learned about the important moving parts
in the DAX language, when it comes to creating a data model.
Specifically you now know the following things:
DAX can handle tables as expressions and parameters. You
can create a calculated table in DAX if needed.
With DAX you can create not only calculated tables, but
calculated columns as well.
Relations are usually not explicitly maintained in a DAX
expression, but the definition of the filter relationships
(created in the _Model view) is implicitly in effect in every
DAX expression. Functions, like USERELATIONSHIP or
TREATAS allow you to explicitly change relationships inside
a DAX expression.
Set operators UNION, INTERSECT, and EXCEPT are available
as DAX functions.
DAX offers functions to implement a natural join
(NATURALINNERJOIN and NATURALLEFTOUTERJOIN) and
a cross join (CROSSJOIN).
You should push the ETL to earlier stages in your data
platform architecture. DAX is only the last resort to clean
and transform data (as it cannot substitute the uncleaned

and untransformed data, but only add the cleaned and
transformed version to the data model). The only thing you
can’t do in the previous stages are non-additive calculations
– that’s where we need DAX and where it shows its true
power.
Now that you have a basic understanding of how the data
model and DAX interact with each other, it is time to learn how
you can actively shape the data model with the help of the DAX
language in the next chapter of this book.

Chapter 10. Building a Data Model
with DAX
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 10th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
With DAX you are writing Data Analytic Expressions – which
allow you to create calculated tables, calculated columns and,
most important, measures (and row level security and queries,
which are not in the scope of this book). All you can achieve
with calculated tables and columns you can also achieve with
solutions in Power Query/M and with SQL. If you just started

with Power BI, then you need to learn DAX anyways, as some
problems can only be solved with measures, written in DAX –
and you might also implement the transformation to build your
data model in DAX as well.
Normalizing
NOTE
The file used for the demonstrations is Normalize Facts DAX.pbix.
As you learned back in Chapter 1 and Chapter 2, normalizing is
important for your fact tables, and means that you strip the
table from replicating information. You only keep foreign keys
to one or more different tables, which contains DISTINCT  lists
of the otherwise redundant information. These other table are
the dimension tables.
With that said, normalizing is as easy as just removing all
column with repeating information, which are not the
(primary) key of the information and putting them into a
different table of its own. To find out, which columns contain
repeating information, I just create a table visual in Power BI
with only the single column or a combination of columns which

might have a one-to-one relationship with each other, which
you can see in Figure 10-1. Power BI will automatically only
show the distinct values. In the given example, the following
combinations of columns are candidates for building
dimensions:
Country
Discount Band
Product, Manufacturing Price
Date, Month Name, Month Number, Year
Segment
Figure 10-1. Dimension candidates

Always also use your domain knowledge and discuss with the
domain experts to decide, if choosing these candidates are
indeed a good decision. Especially if you only work with demo
or test data (and not production data), the true relationship
between the columns might not be clear from just looking at the
data available.
When we can agree on that the candidates above are the true
dimensions, you can create a calculated table and use
DISTINCT  with either a single column reference (which will
work for Country , Discount Band  and Segment  in our
example) or in combination with SELECTCOLUMNS  (for
columns Product  and Manufacturing Price ). Maybe you
ask yourself now: Is it really worth to create such dimensions?
The clear answer is: Yes! Remember, when we talked about all
the disadvantages and problems you get with a single-table
model back in Chapter 5? You must avoid direct filters on the
fact table under all circumstances.

NOTE
There are situations, where it is recommended to add attributes directly into the fact
table: An order number is a typical example here, where it is not a good idea to
create a dedicated dimension table. The reason is that the cardinality of the
dimension table would be close to the fact table. And the size of the fact table will not
be reduced, because instead of the order number you need to add the foreign key to
the fact table. But be aware that adding such information to the data model comes at
a price: The size of the Power BI semantic model will increase dramatically as a
column of a high cardinality compresses badly. Propagating a slicer with the column
values will take a while and applying the filter in a visual will not be fast. On top, you
could suffer from the problem pointed out in “A Single Table To Store It All”.
You should overcome the temptation to create the Date  table
in the same manner (by applying DISTINCT  over the fact
table’s columns), as the date values in the fact table usually
have gaps (e.g., weekend, bank holiday, etc.), which you can
clearly see in Figure 10-1. I will show you a way of how to
create a full-functional date table later in section “Time and
Date”.
We can though not physically remove any of the columns above
from the fact table, but only hide those columns (so that the
report creators are not unintentionally using them) – otherwise
the creation of the calculated tables (referencing those
columns) would fail.

WARNING
And that’s the big disadvantage of using DAX to model your data: You can only add
columns and tables to form a star schema, but you still need to keep all the
information in its original (un-modeled, non-star) shape.
You cannot truly transform the data into the intended model.
But this should not keep your from applying these best
practices. It is better that you shape your data with DAX then
not shaping it at all. Just remember that when the size of the
data model (in memory – but you can also just take a look on
the size of the PIBIX file on disk to get a rough impression how
small or big your data model is) is starting to become a problem
for your resources, then it’s time to re-factor your DAX into
Power Query (or SQL).
Denormalizing
NOTE
The file used for the demonstrations is Denormalize Dimension DAX.pbix.
I am sure, by now, you are already familiar that we need to
denormalize the dimension tables. To denormalize, we need to
add columns with information from a related table into the

main dimension table. The DAX function which achieves this is
called RELATED . It can traverse from the many side of a
relationship to the one side, even over several tables, and fetch
the content of a column.
In the example, a product’s
information is split into three different tables:+DimProduct+,
DimProductSubcategory , and+DimProductCategory+. Simply
create two new calculated columns in table DimProduct :
DimProduct[Subcategory] = RELATED(DimProductSubca
DimProduct[Category] = RELATED(DimProductCategory
As there is a direct relationship in the data model between
DimProduct and DimProductSubcategory , it makes sense
that we can reference a value from there. But DAX is smart
enough to traverse also from DimProduct over
DimProductSubcategory  to DimProductCategory .
Therefore, the second example works as expected. Be reminded
that RELATED  can only reference from a table on the many side
to a table on the one side. (To traverse the other direction, you
can use RELATEDTABLE , which returns a table with all the
values from the many side.)
Again, we can (and definitely
should) hide the two tables DimProductSubcategory and
DimProductCategory , to avoid that report creators will use
any of the columns unintentionally, but we cannot actually

delete the two tables from the model (because, then the newly
created calculated columns would through an error).
Calculations
NOTE
The file used for the demonstrations is Financials Filter Dimension Surrogate Key
Measures.pbix.
Calculations is the home game for DAX. DAX stands for Data
Analysis Expressions and is therefore built for creating
formulas for even very complex challenges. And when I speak
of calculations I mostly mean (explicit) measures that’s what the
core competence of a Data Analytic Expression is. Creating
calculated tables and calculated columns is possible as well, but
I consider them more as a workaround in the early days of
Excel’s Power Pivot, when Power Query was not available yet.
In many scenarios you are better off with explicit measures, as
opposed to calculated columns or adding columns in Power
Query or SQL.
Before I dive into different kind of calculations, I want to
demonstrate how much resources you can save, by replacing

redundant columns (= columns whose value can be calculated
by using other existing columns) with a DAX measure. In the
financials example you will find the following three columns,
which are redundant:
Gross Sales  can be calculated from the Sale Price
and Units Sold , as follows:
Gross Sales :=
SUMX(
    financials,
    financials[Sale Price] * financials[Units
)
Sales can be calculated from the Gross Sales  and
Discounts , as follows:
Sales = [Gross Sales] - SUM(financials[Discou
Profit  can be derived from Sales  and COGS  (“Cost of
Goods”), as shown here:
Profit = [Sales] - SUM(financials[COGS])

NOTE
In DAX, every use of function SUM  can be rewritten into us of SUMX  - and that is
what the storage engine indeed does. There is no difference in performance, only in
the syntax. If you simply want to add up the value of a column use SUM  (e.g.,
SUM(financials[Discounts]) ). If you need to add up the result of an expression,
you need to use SUMX  (e.g., SUMX(financials, financials[Sale Price] * financials[Units
Sold]) ), as +SUM  does not allow to provide an expression, but a column reference
only.
If you replace the existing columns in table financials with the
explicit DAX measures, you will recognize a remarkable
difference in size of the data models:
A model with the financials table as is, occupies 5,5MB of
RAM (Financials OBT.pbix).
A model, where I replaced the three columns Gross
Sales , Sales  and Profit  with the forementioned DAX
measures, only occupies 253KB of RAM (Financials OBT
Measures.pbix).
Getting rid of the columns by replacing them with
mathematically identical DAX measures reduced the size of the
model to a 20th - without losing any information or feature in
the data model. Instead of persisting the result of these
calculations (and occupiying space), they are replaced with
their formula, which will be calculated as needed when

queried. But what about query performance?, you might ask.
Turns out that the query plan for the two different models is
not so different, as you might expect. To measure it, I create a
visual with the three columns/measures by date in each of the
two Power BI files. The two reports look very similar: they show
the same numbers, I kept though the default names for the
headers (“Sum of …​”) to make it easy to distinguish the two
reports (Figure 10-2 and Figure 10-3).
Figure 10-2. Report based on persisted columns

Figure 10-3. Report based on measures to calculate the values on the fly
For each of the two files, I then switched on View - Performance
Analyzer, hit Start recording and Refresh visuals. If you then
expand Table you can click on Copy query. Next, I start DAX
Studio (via External tools), paste the query and switch on Query
plan, Server timings and Clear on Run via the ribbon. Then, run
the query.
As both data models are very small, the queries will finish in no
time. What I want to point out here, is that the storage engine
query, is almost identical. Here you see the xmSQL
representation (chose Server Timings below the query and click
on the only line to display the query) of what the storage engine
needs to do, to deliver the rows for the report with the data

model where all columns are persisted. It simply sums up the
stored values for the columns Profit , Gross Sales , and
Sales :
SET DC_KIND="AUTO";
SELECT
    'LocalDateTable'[Year],
    'LocalDateTable'[MonthNo],
    'LocalDateTable'[Month],
    'LocalDateTable'[QuarterNo],
    'LocalDateTable'[Quarter],
    'LocalDateTable'[Day],
    SUM ( 'financials'[Profit] ),
    SUM ( 'financials'[Gross Sales] ),
    SUM ( 'financials'[ Sales] )
FROM 'financials'
    LEFT OUTER JOIN 'LocalDateTable'
        ON 'financials'[Date]='LocalDateTable'[Da

Estimated size: rows = 16  bytes = 576
In the case, where I removed those three columns and added
explicit measures to the data model, the xmSQL query is slightly
changed: The result for the three KPIs ( Gross Sales , Sales
and Profit ) cannot be obtained from the data. Instead, the
values for columns Discount  and COGS  are summed up and

an expression to multiply the Sales Price  column with the
Units sold  column is added, and it’s result is summed up as
well (to satisfy the calculations for the three measures, which is
done by the formula engine).
SET DC_KIND="AUTO";
WITH
    $Expr0 := ( CAST ( PFCAST ( 'financials'[Sale
SELECT
    'LocalDateTable'[Year],
    'LocalDateTable'[MonthNo],
    'LocalDateTable'[Month],
    'LocalDateTable'[QuarterNo],
    'LocalDateTable'[Quarter],
    'LocalDateTable'[Day],
    SUM ( 'financials'[Discounts] ),
    SUM ( 'financials'[COGS] ),
    SUM ( @$Expr0 )
FROM 'financials'
    LEFT OUTER JOIN 'LocalDateTable'
        ON 'financials'[Date]='LocalDateTable'[Da

Estimated size: rows = 16  bytes = 576
In the end the query performance is identical (and the random
differences with every execution is bigger then the differences

between the two queries measured with only one run). Your
milage may vary with more complex calculations.
The difference in storage space is not so much about the three
decimal values, but it is about the cardinality of these three
columns:
Gross Sales  contains 550 distinct values.
Sales  contains 545 distinct values.
Profit  contains 557 distinct values.
Power BI’s in-memory compression algorithm is highly
dependent on the cardinality (= number of distinct values) of a
column. As the whole fact table has 700 rows, we can derive
that almost every row has a different Gross Sales , Sales ,
and Profit . The high cardinality of these columns leads to the
high amount of space these columns occupy in RAM, even after
compression was applied.
On the other hand, refresh time might improve in the model,
where you do not add the three columns to the data model (as it
has not be calculated in the data source and less data has to be
moved when loaded into the data model).
Overall, you should consider explicitly creating measure for all
type of calculations (including the forementioned columns

Discount  and COGS , which I wrapped in DAX’s SUM
function):
Simple aggregations for additive calculations, which could
also be calculated through Default Summarization . In
the general part about Power BI I already laid out why you
should explicitly create DAX measures instead of relying on
Default Summarization . I usually rename the numeric
column (e.g., add an underscore “_” as a prefix), hide the
column and then create a simple measure by applying the
SUM  function (or whatever aggregation is making sense).
When the calculation is more complex (e.g., because you
need to multiply the quantity with a price) you need the
SUMX function (or a comparable iterator function), where
you can provide the formula for the multiplication. SUMX is
calculating this formula for each and every row of the table
you provided as the first parameter of the function and
sums these results up.
[Units Sold] :=
    SUM(Financials[Units Sold])

[Gross Sales] :=
    SUMX(
        'Financials',
        'Financials'[Units Sold] * Financials

    )
Semi-additive calculations require you to specify for which
date the value should be calculated. Usually, it is the first or
the last date of the current time range.
[First Value] :=
/* based on a blog post by Alberto Ferrari
   https://www.sqlbi.com/articles/semi-additi
 */
VAR FirstDatesPerProduct =
    ADDCOLUMNS (
        VALUES ( 'Product'[Product ID] ),
        "MyDay", CALCULATE ( MIN ( 'Sales'[Da
        )
    )
VAR FirstDatesPerProductApplied =
    TREATAS (
        FirstDatesPerProduct,
        'Product'[Product ID],
        'Date'[Date]
    )
VAR Result =
    CALCULATE (
        SUM ( 'Sales'[Quantity] ),
        FirstDatesPerProductApplied
)

    )
RETURN Result

[Last Value] :=
/* based on a blog post by Alberto Ferrari
   https://www.sqlbi.com/articles/semi-additi
 */
VAR LastDateInContext = MAX ( 'Date'[Date] )
VAR LastDatesPerProduct =
    ADDCOLUMNS (
        CALCULATETABLE (
            VALUES ( 'Product'[Product ID] ),
            ALL ( 'Date' )
        ),
        "MyDate", CALCULATE (
            MAX ( 'Sales'[Date] ),
            ALL ( 'Date' ),
            'Date'[Date] <= LastDateInContext
        )
    )
VAR LastDatesPerProductApplied =
    TREATAS (
        LastDatesPerProduct,
        'Product'[Product ID],
        'Date'[Date]
    )
VAR Result =
    CALCULATE (
SUM ( 'S l
'[Q
tit ] )

        SUM ( 'Sales'[Quantity] ),
        LastDatesPerProductApplied
    )
RETURN Result
TIP
Non-additive calculations must be done in the form of a DAX measure. You cannot
achieve the correct results with any other technique (e.g., calculated column, Power
Query, SQL, etc.)
Results of non-additive calculations cannot just be
aggregated in a meaningful sense. Therefore, you need re-
create the calculation as a DAX measure based on the
aggregated parts of the formula. You need to sum up the
elements of the formula (instead of summing up the
results). The Margin in Percentage of the Sales  is
calculated by dividing the margin by the sales amount,
which works perfectly on the level of one single row in the
sales table. But a report barely shows the individual sales
rows, but aggregated values. Calculating the sum or even
the average of the result of the division would show the
wrong value. Therefore, it needs to be calculated as shown.
[Margin %] := DIVIDE(SUM('Sales'[Margin]), SU

This measure will work on the level of individual sales,
where only a single sale event is available, as well (as the
sum of the margin of a single row in the sales table is just
the margin of the row).
Counts over DISTINCT  entities are another example for
non-additive calculations. The DISTINCT  count of
customers who bought something in the first quarter of a
year is not the sum of the DISTINCT  counts of customers
in January plus the DISTINCT  counts of customers in
February plus the DISTINCT  customers in March, as some
customers might have bought something in more than one
month. Those customers may not be counted twice when
calculating the DISTINCT  count for the quarter. But
creating such a measure is not a big deal:
[DISTINCT Count of Products] := DISTINCTCOUNT
You can see in Figure 10-4 that two products were sold on
the first of the month (A and B), and a single product each
on the second (B) and third ©. But in total it has been only
three different products (A, B, and C), which were sold
during those three days (as product B was sold on both, the
first and the second of the month). The column Count of

Products  adds up to 4 products, while Distinct Count
of Products  shows the correct total of 3 (different)
products. Sometimes I see people complaining on social
media that the table visual in Power BI is buggy, as it does
not always add up the individual numbers in the total. I
don’t really get these discussions, as it clearly depends on
the context of a calculation, if the individual numbers need
to be aggregated or if the calculation has to be done on the
aggregated level, as pointed out in the discussion of
additive and non-additive measures back in the first part of
this book.
Figure 10-4. Visual showing measures Count of Products  and Distinct
Count of Products
TIP
Non-additive calculations, like a distinct count, must be done in the form of a
DAX measure. You cannot achieve the correct results with any other technique
(e.g., calculated column, Power Query, SQL, etc.)
Time Intelligence calculations are another use case, which
can only be solved with DAX measures. The trick is

basically to use CALCULATE  to change the time period
accordingly (e.g., from the current day to all days since the
beginning of the year to calculate the year-to-date value),
similar to the logic for the semi-additive measures. DAX
comes with built-in functions to either directly calculate the
value (e.g., TOTALYTD) or functions which you can use as a
filter parameter for CALCULATE  (e.g., DATESYTD ). Those
functions are just hiding some complexity from you, but
you can always come up with a formula which is achieving
the same result (even with the same performance) by e.g.,
calculating the first day of the year and then changing the
filter context accordingly. See three implementations of a
year-to-date calculation for Sales Amount  in the
following code snippets:
[TOTALYTD Sales Amount] :=
TOTALYTD(
    [Sales Amount],
    'Date'[Date]
)

[TOTALYTD Sales Amount 2] :=
CALCULATE(
    [Sales Amount],
    DATESYTD('Date'[Date])
)



[TOTALYTD Sales Amount 3] :=
CALCULATE(
    [Sales Amount],
    DATESBETWEEN(
        'Date'[Date],
        STARTOFYEAR(LASTDATE('Date'[Date])),
        LASTDATE('Date'[Date])
    )
)
All three have the same semantic and their different syntax
generates the identical execution plan. Therefore, their
performance is identical. They are just using more or less
syntax sugar to write the code.
Figure 10-5 shows the identical result of the three different
approaches in DAX to calculate the YTD.
Figure 10-5. Visual showing the result of the three different measures to calculate
the year-to-date value for Sales Amount

TIP
Time Intelligence calculations must be done in the form of a DAX measure. You
cannot achieve the correct results with any other technique (e.g., calculated column,
Power Query, SQL, etc.)
Especially requirements for time intelligence can easily lead to
many variations of a single measure (e.g., year-to-date, previous
month, previous year, differences in absolute numbers,
differences in percentage, comparison to budget, etc.). It can be
very tedious to create (and maintain) all the variations for each
measure. Here, Calculation Groups come in very handy.
Calculation Groups add a layer above all measures and are
explicitly activated as filters within visuals or via CALCULATE
within other measures. The advantage is that you only need to
specify the logic of how to calculate e.g., a year-to-date for a
measure as one single item in the calculation group. When you
create a calculation item, you can simply copy and paste an
existing definition of a measure, but replace the base measures
name (e.g., [Sales Amount] ) with function
SELECTEDMEASURE . This logic can then be activated for every
measure when you need it. If the logic changes, you need only
change it in a single place (the calculation item), instead of
changing it per measure. Calculation Groups are fully supported
in Power BI - but at time of writing the user interface of Power

BI Desktop does not expose their definitions. Therefore, you
need to use a third-party tool to create and maintain Calculation
Groups in your PBIX file. If you are working with Analysis
Services Tabular you have full access to the definition of
Calculation Groups in, for example, Visual Studio.
[Actual] := SELECTEDMEASURE()

[YTD] := TOTALYTD(SELECTEDMEASURE(), 'Date'[Date
Figure 10-6. Defining a Calculation Group in Tabular Editor 3
In the screenshot I used Tabular Editor 3, but you can also use
the free version of Tabular Editor (Version 2) to maintain
Calculation Groups. In the first step you need to create a new
Calculation Group by right-clicking Tables  inside TOM

Explorer . I renamed both, the table and column “Name” to
“Time Intelligence”. Second, you add Calculation Items  per
variance. Here I added one for Actual  and one for YTD  as just
described.
Flags and Indicators
NOTE
The file used for the demonstrations is Flag.pbix.
Replacing abbreviations and technical identifiers with
meaningful text can easily be achieved with DAX. I intentionally
used a different syntax for each of the following examples, to
demonstrate different possibilities:
IF Function
Every replacement logic can be implemented by writing a
bunch of nested IF functions. Always make sure to use a new
line for each of the parameters and indent the parameters.
Otherwise, a formula, especially one with nested functions can
be really hard to read. If the first parameter of IF evaluates to
TRUE, then the second parameter is returned. Otherwise, the

third parameter. Calculated column Class Description
shows three nested IF functions.
+
'DimProduct'[Class Description] =
IF(
    DimProduct[Class] = "H",
    "High",
    IF(
        DimProduct[Class] = "M",
        "Medium",
        IF (
            DimProduct[Class] = "L",
            "Low",
            "other"
        )
    )
)
SWITCH Function
SWITCH  can be used with just a simple list of values, which I
prefer over nested `IF`s due to better readability. For calculated
column Product Line Description  I did provide a column
name as the first parameter ( DimProduct[ProductLine] )

and different literals for the even (=second, fourth, etc.)
parameters ( "R" , "M" , "T" , and "S" ). If the first parameter
matches one of these literals, then one of the uneven (third or
fifth or etc.) parameter values is returned (either "Road" ,
"Mountain" , "Touring" , or "Standard" ). I provided a last
parameter ( "other" ) for cases where a new ProductLine
was added after I wrote the formula. If I omit the last
parameter, then for such a new ProductLine  a blank value
would be shown as the Product Line Description . I prefer
“other” (or something similar) over a blank text, though.
'DimProduct'[Product Line Description] =
SWITCH(
    DimProduct[ProductLine],
	
"R", "Road",
	
"M", "Mountain",
	
"T", "Touring",
	
"S", "Standard",
	
"other"
)
SWITCH TRUE Function
Finished Good Description  works with the SWITCH
function, but in a different way. For the first parameter I used

TRUE  and the even parameters contain each a condition
(which evaluates to TRUE or not) instead of a literal value. If the
first parameter and the second parameter are equal (that
means that the condition provided in the second parameter
evaluates to TRUE ) then the third parameter is returned. If
that’s not the case, then the first parameter is compared with
the fourth parameter and so forth. You should provide a last
parameter which is returned when all the comparisons failed.
+
'DimProduct'[Finished Goods Description] =
SWITCH(
    TRUE(),
    DimProduct[FinishedGoodsFlag] = 0, "not salab
    DimProduct[FinishedGoodsFlag] = 1, "salable"
    "unknown"
)
Lookup Table
Generally, I prefer to have a lookup table for the replacement
values. I find it easier to just maintain the content of a table
instead of rewriting a formula when new values need to be
added or existing replacements have to be updated. If you need

the replacements in more than one language, than a lookup has
its benefit as well (as we will discuss in Chapter 3 when we talk
about multi-lingual reports). Creating the lookup-table in DAX is
clearly not my favorite (because changing the content of the
table means to change the formula of the calculated table), but
it can be done with the DATATABLE  function. The following
code shows how to use function DATATABLE  to create a table
called Styles (DAX) .
[Styles (DAX)] = /* calculated table */
DATATABLE(
    "Style", STRING,
    "Style Description", STRING,
    {
        {"W", "Womens"},
        {"M", "Mens"},
        {"U", "Universal"}
    }
)
Then you create a filter relationship between the table’s
Styles (DAX)  column Style  and and table’s DimProduct
column Style . This enables you to use RELATED  to lookup the
values. In case a value for Style  is present in DimProduct

which is not (yet) available in table Styles (DAX)  I check for
BLANK  and return “unknown”.
'DimProduct'[Style Description] =
VAR StyleDescription = RELATED('Styles (DAX)'[Sty
VAR Result =
IF(
    ISBLANK(StyleDescription),
    "unknown",
    StyleDescription
)
RETURN Result
Treating BLANK values
Sometimes you do not need to develop complex transformation,
but only make sure to replace empty strings. DAX distinguishes
two kinds of empty strings. A string can indeed contain just an
empty string. This can be checked by comparing an expression
against two double-quotes “”. Additionally, a string (or a column
or expression of any data type) can also be blank. Blank means
that the string is not just an empty string, but that there was no
value provided at all. Relational database call those missing
values NULL . You can either compare an expression against
BLANK()  or you can explicitly check if an expression is blank

by passing the expression into function ISBLANK . In calculated
column WeightUnitMeasureCode  I replaced empty and blank
values with “N/A”.
+
'DimProduct'[WeightUnitMeasureCode cleaned] =
IF(
    ISBLANK(DimProduct[WeightUnitMeasureCode]) |
    "N/A",
    DimProduct[WeightUnitMeasureCode]
)
Time and Date
NOTE
The file used for the demonstrations is Date.pbix.
As pointed out in Chapter 6, you should create your own time-
related table(s) when it comes to Power BI and Analysis
Services Tabular. You can use the DAX code in this section as a

template, which you then change and adopt to the needs of
your report users. The number of rows in a Date  or Time
table is usually negligible – so you do not have to limit yourself
in the amount and variations of columns you want to add.
First, let’s create a Date  table. The starting point is to create a
list of dates for the time range your fact tables contain.
Basically, you do have two options: CALENDARAUTO  and
CALENDAR .
CALENDARAUTO
Function CALENDARAUTO  scans all your tables for
columns of data type Date  and will then create a list of
dates for January 1 of the earliest year until December 31
for the latest year. This will work as long as you do not
import columns with “exotic” dates (like birthdates or
placeholders like January 1 1900 or December 31 9999). In
case of fiscal years (which do not start with January 1) you
can pass in an optional parameter to CALENDARAUTO  to
move the start month by x months).
[Date (CALENDARAUTO)] = CALENDARAUTO() /* ca
CALENDAR

Function CALENDAR  gives you more control, as you have
to provide two parameters: The first date and the last date
of your Date  table. These parameters can either be
hardcoded (e.g., DATE(2023, 01, 01) ), which is not
very flexible and you need to set a reminder in your
calendar to change the value once a year to add the dates
for the new year) or you can write an expression where
you calculate the two dates from your fact table’s date
column. Unless your fact table is huge, the calculation will
be fast enough and will give you rest-of-mind that the date
table will always contain all necessary entries with every
refresh.
[Date (CALENDAR)] = /* calculated table */
    CALENDAR(
        DATE(
            YEAR(MIN('Fact Reseller Sales'[O
            01, /* January */
            01  /* 1st */
        ),
        DATE(
            YEAR(MAX('Fact Reseller Sales'[O
            12, /* December */
            31  /* 31st */
        )
    )

After you created the calculated table you can add new columns
over the user interface of Power BI Desktop. Although, I
recommend to nest CALENDARAUTO  or CALENDAR  into
ADDCOLUMNS  and then specify pairs of name and expression
for the additional columns. With that approach you have
everything in one single place (the expression for the calculated
table) and not spread out over separated calculated columns.
This allows you also to easily copy and paste this full definition
of the calculated table to the next data model.
[Date (CALENDAR)] = /* calculated table */
ADDCOLUMNS(
    CALENDAR(
        DATE(
            YEAR(MIN('Fact Reseller Sales'[OrderD
            01, /* January */
            01  /* 1st */
        ),
        DATE(
            YEAR(MAX('Fact Reseller Sales'[OrderD
            12, /* December */
            31  /* 31st */
        )
    ),
    "Year", YEAR([Date]),
"MonthKey", YEAR([Date]) * 12 + MONTH([Date]

    MonthKey , YEAR([Date])  12  MONTH([Date]
    "Month Number", MONTH([Date]),
    "Month", FORMAT([Date], "MMMM"),
    "YYYY-MM", FORMAT([Date], "YYYY-MM"),
    "Weeknumber (ISO)", WEEKNUM([Date], 21),
    "Current Year", IF(YEAR([Date])=YEAR(TODAY()
)
Typical additional columns for a date table are:
DateKey  as a whole number representing the date in the
format YYYYMMDD . You can calculate this whole number by
extracting the year from the date, which you multiple with
10000, add the number of the month multiplied by 100 and
then add the day. In a data warehouse it is best practice to
also have the keys for dates in the form of a whole number.
In Power BI and Analysis Services Tabular this is not so
important.
Year  as the year portion of the date. DAX function YEAR
got you covered here.
Variations of the month, like the month number of the year,
the month name, the year and the month combined in
different formats. Most of the variations can be calculated
by using function FORMAT  and passing in a format string.

You pass the date as the first parameter for function
WEEKNUM. The second parameter allows you to specify, if
your week starts on Sundays or on Mondays or if the week
number should be calculated according to the ISO standard.
Users expect that a report shows the most recent data.
Preselecting the right year and month can be a challenging
task unless you have a column containing “Current Year” or
“Current Month”, which dissolves to the right year or
month.
There are no functions similar to CALENDARAUTO  or
CALENDAR  to get the range for a time table. But we can use
GENERATESERIES  to request a table containing a list of values
for the specified range of integers. To create a table for every
minute of the day, we need to CROSSJOIN  a table containing
values 0 to 23 (for the hours of a day) and a second table
containing values 0 to 59 (representing the minutes of an hour).
Again, by using ADDCOLUMNS  we can add additional columns to
this expression, so we have the full definition of this calculated
table in one single place:
* Function TIME  can convert the pairs
of hours and minutes into a proper column of datatype Time  .
* Function FORMAT  can do its wonders also with time related
content.
[ i
(
)]
l
l
d
bl

[Time (DAX)] = /* calculated table */
VAR Hours = SELECTCOLUMNS(GENERATESERIES(0, 23), 
VAR Minutes = SELECTCOLUMNS(GENERATESERIES(0, 59
VAR HoursMinutes = CROSSJOIN(Hours, Minutes)
RETURN
    ADDCOLUMNS(
        HoursMinutes,
        "Time", TIME([Hour], [Minute], 0),
        "Time Description", FORMAT(TIME([Hour], 
    )
Role-Playing Dimensions
NOTE
The file used for the demonstrations is Date role-playing.pbix.
If you opt to load a role-playing dimension only once into the
data model, you need to make sure that you add as many
relationships as foreign keys from one single table to the role
playing dimension exist. A maximum of one of those
relationships can be active, the others only can be inactive, but
can be activated in measures. That means that you need to
create one variation of each measure per role. Instead of having
just a Quantity  measure, you would create individual

measures like Order Quantity , Sales Quantity , etc. Each
of these measure use CALCULATE  and USERELATIONSHIP  to
explicitly activate one of the relationships. DAX is smart enough
to (implicitly) deactivate the active relationship for the sake of
the context inside CALCULATE , so that still only one
relationship is active at one point in time.
[Order Quantity] :=
CALCULATE(
    SUM(Sales[Quantity]),
    USERELATIONSHIP(Sales[OrderDate], 'Date'[Date
)

[Ship Quantity] :=
CALCULATE(
    SUM(Sales[Quantity]),
    USERELATIONSHIP(Sales[ShipDate], 'Date'[Date
)
TIP
Again, Calculation Groups can be of help here. You can create Calculation Group items
per role of the dimension, instead of duplicating all your measures as many times as
you have roles.

The alternative approach is to physically load the role playing
dimensions several times. Instead of living with just one single
Date  table you will create calculated tables in DAX to duplicate
the table (with all its content). This has the disadvantage of
increasing the size of your model, but as long as the size of the
role-playing dimension is not huge, this is usually negligible.
The advantage is that you do not need to create variations of
your measures (by applying CALCULATE  and
USERELATIONSHIP ), but the report creater choses one copy of
the dimension table over the other – or can even combine both.
Creating a copy of a table in DAX is rather easy. You just create a
calculated table and use solely the name of the other table as
the expression. I, though, strongly recommend renaming all
columns to add e.g., the table name as the prefix, so it is clear
which e.g., Date  column is referred to (the one from the newly
created Order Date  or the Sales Date ). You can do this by
either manually renaming all columns or by changing the
expression of just referring to the base table and use
SELECTCOLUMNS  which allows you to specify which columns
(or expressions) you want to return under which column name.
This approach allows you again to have all the logic (renaming)
in one single place (namely the expression for the calculated
table). In the parts about Power Query/M and SQL I will show

you, how you can automatically rename all columns, without
specifying each and every column, as we need to do in DAX.
[Order Date (DAX)] = /* calculated table */
SELECTCOLUMNS(
    'Date',
    "Order Date", [Date],
    "Order Year", [Year],
    "Order Month", [Month]
)

[Sales Date (DAX)] = /* calculated table */
SELECTCOLUMNS(
    'Date',
    "Sales Date", [Date],
    "Sales Year", [Year],
    "Sales Month", [Month]
)
Slowly Changing Dimensions
NOTE
The file used for the demonstrations is Slowly Changing Dimensions.pbix.

If you want to implement Slowly Changing Dimensions, you
have to do this in a physically implemented data warehouse. In
DAX you cannot update rows to keep track of changes and
different versions, only load the results of such updates.
Usually, Slowly Changing Dimension means not extra effort in
the world of DAX, as the rows in the fact table are already
referencing the right version of the dimension table. Only, if
your report user need to override the default version (= the
version which was valid at the point in time the fact was
collected), then you need to reach out to DAX and implement
the logic via CALCULATE  in your measures.
Figure 10-7 shows a report page with the following content:
A slicer to choose the product.
A slicer to choose the year, month, or day when the product
had to be valid. If a time range is selected (e.g., a year) then
the version valid at the end of this period will be taken into
account.
Two Card visuals showing the last day of the period chosen
( Selected Product Version ) and the Standard Cost
valid for this day.
A Table visual showing columns
Date
Product name

StartDate , EndDate  and StandardCost  of the
version of the product valid at the Date ,
Quantity  sold on that date
Cost  as the result of the shown StandardCost  times
the shown Quantity
Cost (Product Version)  calculated as Product’s
version Standard Cost  as shown at the top of the
screen times Quantity  sold on that day
Figure 10-7. A report page which gives the choice of the Standard Cost  of which
product’s version should be used to calculate the Cost (Product Version)
For Product Sport-100 Helmet, Black  a StandardCost  of
12.0278 was valid in years 2020 and 2021 ( StartDate  2020-01-
01 and EndDate  2022-01-01). With beginning of 2022 the

StandardCost  rose to 13.7882. In the individual lines of the
Table visual the Cost  is calculated by multiplying Quantity
with either of those two values (e.g., 27 * 12.0278 = 324.7506). In
contrast, the value in column Cost (Product Version)  is
calculated as the individual Quantity  by 13.7882 in all rows,
as this is the standard cost valid for the selected version of the
product (e.g., 27 * 13.7882 = 374.7114).
To implement this, we need the following parts:
1. A table containing the versions, where the report user
selects from. As new versions can be created in any point in
time, probably it is a good idea to use the date dimension
(and possibly the time dimension) here.
[Product Version] = 'Date' /* calculated tabl
Alternatively, you could also create a table containing all
distinct EndDate  values from the fact table. I decided
against it here, as in a real-world scenario there could be a
along list of those version, which will be possibly spread
very unevenly over time, which makes scrolling down the
list a bit awkward. But it’s totally up to you to exchange the

reference to the Date  table with
DISTINCT(Product[EndDate]) .
Resist to create a relationship from this table to e.g., the
StartDate  or EndDate  of the Product  table. Such a
filter would not work as expected, as someone could select
a date not existing as a StartDate  or EndDate .
Therefore, we will apply the filter over DAX in the measure
where we are calculating Cost (Product Version) .
2. A measure for the selected product version:
[Selected Product Version] := MAX('Product Ve
3. A measure to find the standard cost for the selected version,
independent from the selected date. All the versions of the
same product have the identical business key
( ProductAlternateKey ). Therefore, you need to remove
any filter on the product table (as a filter e.g., on the name
would be problematic, if the name changed over the
versions) and add a filter on the ProductAlternateKey
and find the product, for which the selected product
version falls into the timespan of StartDate  and
EndDate . We need also take into account that StartDate
or EndDate  could be empty, as the product’s version is
valid since ever or still valid.

[Standard Cost (Product Version)] :=
VAR ProductVersion = [Selected Product Versio
RETURN
SUMX(
    'Product',
    VAR AlternateKey = 'Product'[ProductAlter
    VAR Result =
    CALCULATE(
        MIN('Product'[StandardCost]),
        ALL('Product'),
        'Product'[ProductAlternateKey] = Alte
        ProductVersion >= 'Product'[StartDate
        ProductVersion <= 'Product'[EndDate] 
    )
    RETURN Result
)

[Cost (Product Version)] :=
SUMX(
    'Product',
    [Order Quantity] * [Standard Cost (Produc
)
Hierarchies

NOTE
The file used for the demonstrations is Hierarchies.pbix.
If you followed all best practices described in this book so far,
then you already have denormalized all natural hierarchies in
the dimension tables, as described in “Denormalizing”. With the
natural hierarchy denormalized you have all levels of the
hierarchy as columns in one single table. Adding them to a
hierarchy is very easy.
Here we concentrate on parent-child hierarchies. They are very
common, and we also need to store the names of all parents in
dedicated columns. Read on if you want to learn how you can
achieve this with DAX.
First, we create the materialized path. Luckily there is function
DAX available:
'Employee (DAX)'[Path] = PATH('Employee (DAX)'[Em
Then we need to dissect the Path  and create a calculated
column per (expected) level. Please add calculated columns for
some extra levels in case the depth of the organigram (and
therefore the path length of some of the employees) will

increase in the future. To make creating these columns as
convenient as possible, I put the level number (which should
correspond with the name of the calculated column) into
variable. Then you can just copy & paste this definition for each
level and only change the name and the content of variable
LevelNumber . LevelNumber  is used as a parameter for
PATHITEM  to find the n  entry in the path. The found string
represents the key of the employee and is stored in variable
LevelKey . This key is then passed into LOOKUPVALUE  to
extract the full name of this employee and stored in variable
LevelName . The latter is returned.
'Employee (DAX)'[Level 1] =
VAR LevelNumber = 1
VAR LevelKey = PATHITEM ( 'Employee (DAX)'[Path]
VAR LevelName = LOOKUPVALUE ( 'Employee (DAX)'[Fu
RETURN LevelName
You can already add all Level  columns to a common
hierarchy if you want. I created a Matrix visual, shown in
Figure 10-8, with the hierarchy on the rows and measure
Sales Amount  in the value section. So far so good. As soon as
you start expanding the upper levels you will see that the
Sales Measure  is available for all (in my case seven) levels of
th

the hierarchy, even when there is no employee related to the
shown level. The result for the last available level is repeated
for all sub-levels when they do not have their “own” value.
Figure 10-8. The hierarchy expands to unnecessary levels with empty names and
repeating Sales Amount
In a good data model, you should take care of this problem. You
need to add another column (to calculate on which level an
employee is) and a measure to aggregate this column with MAX ,
another measure (to calculate on which level the measure is
actually displayed) and tweak the existing measures to return
blank in case an employee is shown in an unnecessary level (by
returning blank in case the level the measure is displayed on is

higher than the employee). The unnecessary level will not be
displayed, if all measures only return blank.
We can calculate the level of an employee by counting the levels
the path contains (by basically counting the separator character
plus one). This gives us the position of an employee within the
organigram. The lower the path length, the higher the position
in the organigram, with the CEO having a path length of 1.
Calculating this is much easier, than you might think, thanks to
function PATHLENGTH . Calculating the maximum as a measure
is then no real challenge, I guess:
'Employee (DAX)'[PathLength] = PATHLENGTH('Employ

[MaxPathLength] := MAX('Employee (DAX)'[PathLengt
We need also to count at which level the measure is. Here we
need to check, if the column, representing a certain level, is in
the current scope of the visual or not. If it is, INSCOPE  will
return TRUE , which is implicitly converted to 1 in an
arithmetic calculation. If it is not in scope, then INSCOPE  will
return FALSE , which is implicitly converted to 0 in an
arithmetic calculation. In case you add columns for additional

level, please remember to add them in the calculation of this
measure as well.
[CurrentLevel (DAX)] :=
ISINSCOPE('Employee (DAX)'[Level 1]) +
ISINSCOPE('Employee (DAX)'[Level 2]) +
ISINSCOPE('Employee (DAX)'[Level 3]) +
ISINSCOPE('Employee (DAX)'[Level 4]) +
ISINSCOPE('Employee (DAX)'[Level 5]) +
ISINSCOPE('Employee (DAX)'[Level 6]) +
ISINSCOPE('Employee (DAX)'[Level 7])
Finally, we need to add a measure, in which we decide if a value
has to be displayed or not.
Sales Amount (DAX) =
VAR Val = [Sales Amount]
VAR ShowVal = [CurrentLevel (DAX)] <= [MaxPathLen
VAR Result =
    IF ( ShowVal, Val )
RETURN
    Result

TIP
Again, Calculation Groups can be of help here. You can create two Calculation Group
items. One to just return the plain measure (” SELECTEDMEASURE() “), another where
you copy and paste the code from measure Sales Amount (DAX)  and replace "
[Sales Amount] " with " SELECTEDMEASURE() “.
If you now exchange the Sales Amount  measure with the
newly created Sales Amount (DAX)  measure, you get rid of
the unnecessary empty levels of the hierarchy, as you can see in
Figure 10-9.
Figure 10-9. The hierarchy expands to unnecessary levels with empty names and
repeating Sales Amount

Key Takeaways
In this chapter you learned that DAX is a very powerful
language, and that you can do every transformation in it.
Especially, when it comes to semi- and non-additive
calculations, as they cannot implemented outside of DAX
(neither with Power Query nor in the data source). Take a look
on what you learned in this chapter:
Normalizing your fact tables involves steps to find
candidates for dimensions, creating dimension tables as
calculated tables via SELECTCOLUMNS  and hide those
colum	ns in the fact table. Unfortunately, we cannot
actually remove those columns from the fact table, because
it would break the DAX code of the dimension tables.
Denormalizing in DAX means to use RELATED  to move
columns over to the main dimension. Again, we cannot
remove the referenced tables without breaking the DAX
code. Therefore, we just hide these tables.
I would recommend creating all calculations as DAX
measures, as a starting point (and not as DAX calculated
columns or as columns in the Power Query or in SQL).
Carefully analyze the formula if it involves multiplication,

because then you may need to use an iterator function to
achieve the correct result.
We can solve the problem of role-playing dimensions in two
ways: Either adding (inactive) relationships to the data
model and activating them via USERELATIONSHIP  in the
measures where we do not want to use the active
relationship. Or we can add the dimension several times
under different names and create standard relationships.
Then no special treatment of your DAX code is necessary.
Natural hierarchies are denormalized anyways in a star
schema.
Parent-child hierarchies need some extra love before we
can use them conveniently in reports. You need to create
some extra columns and measures.
Now it’s time to learn about more advanced challenges, derived
from real-world use cases, and how to solve them in DAX.

Chapter 11. Real-World Examples
Using DAX
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 11th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
DAX and the data model go very close hand-in-hand. Some
challenging use cases you can solve directly in the model so that
there is no need to write a single line of DAX formula (this
applies to most one-to-many relationships – but there are
exceptions, as you will learn in this chapter). Other solutions
involve a cooperation between the data model and DAX (like

activating an inactive relationship). In rare cases, tables have
no relationship defined at all in the data model, but you create
a relationship with the TREATAS  function in DAX, which will
only exist during evaluation of the DAX expression.
This chapter covers the following use cases:
Binning is the idea, to not showing the actual values, but the
bucket a value falls into (like small, medium or large). In
this chapter you will learn, how DAX can help you to
implement this based on a table which defines the ranges of
each bucket.
I use Budget as an example for a data model with more
than one fact table. Her you need to overcome the challenge
that the granularity of one of the fact tables might not be on
the primary key level of a dimension table. This leads to a
many-to-many cardinality between the fact table and the
dimension table. DAX can be a solution here.
As there is no single button to press in Power BI Desktop to
create multi-language reports, you need to find
workarounds for several problems. I prefer a workaround
which is solely data-driven and does not need any change
in the data model if new translations or new languages are
added. To achieve this, I need to add logic to all of my
measures.

Key-value pair tables are tables where attributes are not
stored in dedicated columns, but as individual rows. To
make such a table useful for typical reports, you need to
pivot the table (so that you make transform the information
from rows per attribute into columns per attribute). In this
chapter I teach you several approaches of how to achieve
this in DAX.
Binning
NOTE
The file used for the demonstrations is Binning.pbix.
In Chapter 3 I introduced you to three different solutions to
assign values into bins, where I only recommended two:
Creating a lookup table with a row per distinct value you want
to bin or creating a lookup table containing one row per bin
and its lower and upper range value.
Lookup Table
To create the lookup table with the distinct values, you can
create a Calculated Table. The following code starts with a
variable _Bins  containing the distinct values of the sales’s

table quantity, adds a column containing the bin (with the help
of SWITCH  to check for the given quantity). The second
variable _BinsWithSortOrder  builds on the previous one
and adds another column, using the minimum quantity for a
given bin as the _SortOrder  column.
Bin Table (DAX) = /* calculated table */
VAR _Bins =
    ADDCOLUMNS(
        DISTINCT('Sales'[Quantity]),
        "Bin",
            SWITCH(
                TRUE(),
                Sales[Quantity] < 3, "Low",
                Sales[Quantity] < 5, "Middle",
                "High"
            )
    )
VAR _BinsWithSortOrder=
ADDCOLUMNS(
    _Bins,
    "_SortOrder",
        VAR _CurrentBin = [Bin]
        RETURN
        MINX(
            FILTER(
                _Bins,
[Bin]= CurrentBin

                [Bin]=_CurrentBin
            ),
            [Quantity]
        )
)
RETURN
    _BinsWithSortOrder
You can see the content of the resulting table in Table 11-1. The
content of the table appears un-ordered. This is because the
formula derives the content from the Sales  table; therefore,
the rows in the Bin Table  are in the same order as in the
Sales  table.
Table 11-1. Bin table (DAX)
Quantity
Bin
_SortOrder
3
Middle
3
1
Low
1
4
Middle
3
5
High
5
Don’t forget to sort column Bin  by column _SortOrder . You
can achieve this easily by selecting the Bin  column the fields

list of Power BI Desktop and then choose Column tools - Sort by
column and select the _SortOrder  column, as shown in
Figure 11-1.
Figure 11-1. Sort the bin column by the _SorterOrder column
Range Table
The other solution, which was discussed in “Binning” is
implemented via non-equi joins from the fact table, which
contains the value to-be-binned, to the lookup table, which
contains the lower and the upper value of the bin. This table
can also be created as a Calculated Table in DAX. The first set of

parameters of function DATATABLE  expects pairs of column
names and their datatype (as you already learned in “Tables”).
The second set of parameters must be the data for this table. As
you learned in “Tables” you have several options to specify a
table in the DAX language. I prefer the {}-operator here,
because of brevity. The outer {} defines the whole table. The
inner {} define the content for the individual (three) rows. Here
you do not need an explicit column to order the Bin  column.
You can just use the existing column Low (incl.)  (or High
(excl.)  for that matter) to achieve this instead. See the code
to create the Bin Range  table here:
Bin Range (DAX) = /* calculated table */
DATATABLE(
    // Schema defintion
    "Bin",          STRING,
    "Low (incl.)",  INTEGER,
    "High (excl.)", INTEGER,
    // Rows of data
    {
    {"Low",    BLANK(), 3      },
    {"Medium", 3,       5      },
    {"High",   5,       BLANK()}
    }
)

The result of the DAX formula is shown in Table 11-2. Pay
attention to the blank entries for column Low (incl.)  in row
Low and for column High (excl.)  in row High (created by
the BLANK()  function in the code).
Table 11-2. Bin Range (DAX)
Bin
Low (incl.)
High (excl.)
Low
3
Medium
3
5
High
5
The data model in Power BI does not allow to define non-equi
joins, but in DAX we can implement them. The following is
important to point out here:
The bin for the lowest value has an empty Low value. This
indicates that we don’t care how low the value might be, as
long as the value is lower than the specified High value, it
will fall into the first bin.
Similarly, the bin for the highest value has an empty High
value. This indicates that we don’t care how high the value
might be, as long as the value is greater or equal to the
specified Low value, it will fall into the last bin.

I used the exact same value for the High value of one bin
and the Low value of the next bin. This makes the table
usually easier to maintain and guarantees that there are no
gaps between the bins. Later you will implement the lookup
so that a value must be greater or equal than the Low value
to fall into a bin, but lower than the High value.
To make use of the Bin Range table, you need to create a DAX
measure which implements the lookup between the sales’
quantity and the Bin Range table, like this:
[Bin Count] :=
SUMX (
    FILTER(
        'Sales',
        VAR Quantity = 'Sales'[Quantity]
        VAR Bin =
                FILTER(
                    'Bin Range',
                    NOT ISBLANK(Quantity) &&
                    (Quantity >= 'Bin Range'[Low 
ISBLANK('Bin Range'[Low (incl.)] )) &&
                    (Quantity <  'Bin Range'[High
ISBLANK('Bin Range'[High (excl.)]))
                )
        VAR IsInBin = NOT ISEMPTY( Bin )
        RETURN IsInBin


    ),
    1
)
The DAX measure consists of three important parts:
1. A SUMX  over the value of 1. This could be easily replaced
by a COUNTROWS . I used a SUMX  in this example for two
reasons: First, to point out that you could also aggregate
facts like quantity, sales amount etc. instead of just counting
rows. Second, as a reminder that sometimes a
SUMX(<table>, 1)  has a better performance compared
to a COUNTROWS(<table>) . Always stay flexible in terms
of achieving the same result with different DAX formulas,
to have something in your sleeve when performance is not
up to expectations.
2. An outer FILTER , which iterates over the fact table
( Sales ) and only returns a row, if it falls into the bin
available in the current filter context.
3. An inner FILTER , which iterates over the lookup table
( Bin Range ) and only returns a row, when the quantity
falls in between the lower and upper range of a bin
available in the current filter context. Blank quantities are
ignored ( NOT ISBLANK(Quantity) ), as they fall into none
of the bins. The quantity must either fulfill the condition of

being greater or equal the lower bound or lower than the
higher bound, or the boundary is blank (e.g., ISBLANK(Bin
Range[Low (incl.)]) ).
TIP
If you like the solution with the bin range table, remember that the non-equi join
implemented in DAX with the two filtered iterations can only be realized in DAX, but
not directly in the data model or in Power Query or SQL.
Let’s turn towards the next use case: multi-fact data models,
explained on the example of adding a budget table to a model
already containing actual values.
Budget
NOTE
The file used for the demonstrations is Budget.pbix.
In “Budget” I laid out that Budget works as a typical example in
this book, where you will end up with a data model with more
than one fact table. And when you create the filter relationship
between the Budget  table and some of the dimensions, you
will discover that it has a many-to-many relationship, as the

relationship is based on neither of the primary keys of the two
tables
One of the three solutions discussed in “Budget” involves DAX
and its function TREATAS . This function applies a list of values
as a filter on a column – and therefore taking over the task you
would usually achieve with a filter relationship.
WARNING
Be aware that creating a relationship in the data model (active or inactive) will create
sort of an index on the two columns involved in the filter relationship. The size of the
model will increase a bit due to this index, but joining the two tables will be sped up.
Function TREATAS  cannot benefit from such an index and is therefore potentially
slower. That’s why it is important that TREATAS  should not be your first go, but you
should only use it, when a solution with relationships cannot be implemented for
good reasons.
In the following code I use TREATAS  to manipulate the filter
context with the help of CALCULATE . I pass in the current filter
context’s distinct list of VALUES  of the product’s product group
as the first parameter and ask to apply this as a filter onto the
budget’s product group.
[Budget TREATAS] :=
CALCULATE (
    SUM ( Budget[Budget] ),

    TREATAS (
        VALUES ( 'Product'[Product Group] ),
        Budget[Product Group]
    )
)
I created this measure in a Power BI Desktop file containing the
following tables:
A Date  table with a row for every day for the years of data
of the fact tables.
Table Product  with four rows:
Table 11-3. Product
Product ID
Product Desc
Product Group
100
A
Group 1
110
B
Group 1
120
C
Group 2
130
D
Group 2
The Sales  table, again with four rows (with sales for three
different days), on the granularity of a single day and a
single product:

Table 11-4. Sales<
Date
Product ID
Amount
2023-08-01
100
3000
2023-08-01
110
20000
2023-08-02
110
10000
2023-08-03
120
15000
And a Budget  table with the planned sales amount per
day (the first of the month for two different months) and
per product group (not: product):
Table 11-5. Budget
Date
Product Group
Budget
2023-08-01
Group 2
20000
2023-08-01
Group 3
7000
2023-09-01
Group 2
25000
2023-09-01
Group 3
8000
In Figure 11-2 you can see that the measure Budget TREATAS
shows the correct budget per month and product group. The

total is calculated as the sum over all product groups available
in table Product  (and matches the values displayed in the
table visual). Unfortunately, in the example we also have a
budget for product groups, which are not available in the
product’s table (as no product’s for this groups are defined yet).
Figure 11-2. Budget TREATAS per day and product group
If you want to solve the problem of foreign keys in the fact
table, which are not available in the dimension table, then you
need a different approach. You need to create a table, which
contains all product groups, by combining the product groups
from both the product and the budget table. No duplicated
product groups are allowed in this table. Afterwards you can
use this table as a bridge table, as described in “Budget”, to
create two one-to-many relationships from this table to the
product table and to the budget table.

Find the DAX code for the bridge table created as calculated
table here:
Product Group BRIDGE = /* calculated table */
DISTINCT (
    UNION (
        DISTINCT ( 'Budget'[Product Group] ),
        DISTINCT ( 'Product'[Product Group] )
    )
)
Next challenge is to create the necessary logic in DAX to supply
the multi-language model.
Multi-Language Model
NOTE
The file used for the demonstrations is Multilanguage.pbix.
In this section I will show you how you can use DAX to apply
the selected language (row in the Language  table) to the
(other) dimension tables. In “Multi-Language Model” you
already learned that Power BI does not allow you to create
active filter relationships between the language table and more

than one dimension table (and complains about an ambigous
model instead).
Therefore, we need to find a way of applying the filter for the
language within the DAX measure which calculates e.g., the
sales amount. You need to make shure the a non-blank value is
only available for dimension rows with the correct language.
For the other languages it should return BLANK . Power BI’s
default behavior will make sure, to not display such blank
values and therefore omit dimension rows in the “wrong” (=
not selected) language.
TIP
As a fail-safe always make sure to use function SELECTEDVALUE  when accessing the
selected language to display it or use it to filter the dimension table. This function
will return the selected value, as you might guess from its name. In case, no or
multiple selection have been done, you can provide a fallback value (e.g., English). In
a perfect world this would not be necessary as every report-creator will make sure to
set the properties of the slicer or filter for the language selection to force single-
selection only. But as a model creator we have no control over this and I like to be
better safe than sorry.
Before I show you my preferred solution, I want to contemplate
about different options:
Option 1

Power BI allows us to create the relationships between the
language table and the dimension tables as inactive
relationships. An inactive relationship is not part of the
filter context, unless it is explicitly activated in a measure.
Activating is done via function USERELATIONSHIP , as
you learned already in “Relationships” (in the section
about role-playing dimensions):
Sales (USERELATIONSHIP) =
CALCULATE(
    SUM('Fact'[Amount]),
    USERELATIONSHIP('Language'[Language ID],
)
This works very well, when you only activate a single non-
active relationship, because DAX will automatically de-
activate the active relationship (to avoid that more than a
single relationship is active).
Unfortunately, this does not
work at all in the use case of the Language  table and
relationships to two dimension tables, as the proper
solution needs to activate more than one of the inactive
relationships (all relationships between the Language
table and the dimension tables), to propagate the filter
from the Language  table to the dimension tables.

Sales (USERELATIONSHIP) =
CALCULATE(
    SUM('Fact'[Amount])
    ,USERELATIONSHIP('Language'[Language ID]
    ,USERELATIONSHIP('Language'[Language ID]
)
The above formula leads to the confusing error message
USERELATIONSHIP function can only use the two columns
references participating in relationship and just means that
you cannot active both inactive relationships. This idea
cannot be implemented successfully.
Option 2
Another solution could ignore the filter on the language
table for the case of displaying the dimension’s names and
therefore avoid all problems of ambiguous relationships
in the data model. You could use function USERCULTURE
to access the user’s language. USERCULTURE  returns a
string in the format “<language>-<CULTURE>”. For
example, “en-US” for language English and US-American
formatting (make sure to use lower-case and upper-case
as in the example, as this string is case sensitive). A user
can set the preferred language in Power BI Desktop (File -

Options and settings - Options - GLOBAL - Regional
Settings) for reports opened in Power BI Desktop and in
the browser’s settings in case of Power BI Service. You can
always explicitly override the browser’s settings by
adding a parameter to the URL. e.g., add ?language=en-
US  to set the language explicitly to language English and
all formatting options to culture US. The following
example uses TREATAS  to set the dimension’s Language
ID  gotten from these settings. In the DAX formula I first
set the content of variable LanguageID  to the first two
characters of the result of the result of USERCULTURE
(which represents the language). I wrapped the
expression into {} to convert the scalar value into a table
(which contains only one column and one row), as
TREATAS  expects a table expression as its first parameter.
Like in the previous solution (with USERELATIONSHIP ) I
then use CALCULATE  to change the filter context for the
expression; this time I apply variable LanguageID  as
filters to the dimension’s Language ID  columns:
[Sales (USERCULTURE)] =
VAR _LanguageID = {LEFT(USERCULTURE(), 2)}
RETURN
CALCULATE(
    SUM('Fact'[Amount])
TREATAS( L
ID
Di 1[L
ID])

    ,TREATAS(_LanguageID, Dim1[Language ID])
    ,TREATAS(_LanguageID, Dim2[Language ID])
)
This can be benefitable when you want to tie the display
language to the browser’s settings (instead of offering a
slicer to change the language ad-hoc in the report). The
backdraft is that you need to apply the logic in
CALCULATE  to each and every measure. To make
maintenance easily, you could create one Calculation
Groups, which takes care of the filtering, as shown here:
VAR _LanguageID = {LEFT(USERCULTURE(), 2)}
RETURN
CALCULATE(
    SELECTEDMEASURE()
    ,TREATAS(_LanguageID, Dim1[Language ID])
    ,TREATAS(_LanguageID, Dim2[Language ID])
)
Option 3
A different approach to make sure that all dimensions are
filtered by the selected language is to create one slicer per
dimension on the dimension’s language key (not on the

Language  table). These slicers can be synchronized with
each other, by choosing View – Sync slicers – Advanced
options from the ribbon. Now you need to make sure to
input the same string for all to-be-synced slicers under
Enter a group name to sync selection to any other visuals
with that group name (s. Figure 11-3).
Figure 11-3. Advanced options allows to keep slicers on different fields in sync
There are backdrafts though. First, filters on the lookup
table ( Language ) might be more performant than filters
created on the foreign keys (= the Language ID  columns
of the dimension tables). Second, the content of foreign
key might not be very user friendly (eg. “en” instead of

“English”) – so you need to store the full language’s name
in every dimension if you want to keep the slicers’s
content user friendly. Lastly, which I think is the biggest
problem, this solution must be implemented in the report
tool (and is not covered by the data model). If somebody
else is using your reports dataset as a data source, they
need to reimplement the synchronization of the slicers (to
not suffer from duplicated dimension rows and
duplicated values). Some tools (like Excel) may not allow
synchronizing filters.
Option 4
The problem with propagating the language’s filter on
several dimensions can also be solved via Row Level
Security. A filter based on Row Level Security is activated
on a different layer, so to speak, than the usual filter
context. If a security role filters multiple tables with the
same filter, you do not receive the “ambiguous model”
error message (as in the case when I tried to have several
active relationships from the Language  table to the
dimension tables). You can either statically assign users to
a role matching their preferred language or you can
implement dynamic Row Level Security through a lookup
table that provides the user’s universal principal name

(UPN) and the language. I implemented the latter in the
following example:
First you need the lookup table, which provides the
security context per user. For example, user
James@savorydata.com  has Language ID  “EN”
assigned:
Table 11-6. A table containing users and their preferred
languages.
User Name
Language ID
James@savorydata.com
EN
Fritz@savorydata.com
DE
Jens@savorydata.com
DA
Koloth@savorydata.com
tlh-Latn
I recommend making sure that an active filter
relationship between table Users  and Language  is
created on basis column Language ID . The relationship
must have a one-to-many cardinality (with table
Languages  on the one side). In case of our example,
Power BI will suggest a one-to-one cardinality, due to the
fact that the Users  table only contains one row per

language. But in reality, of course there could be many
rows for the same language in table Users , as multiple
users will want to consume the report in the same
language. As a filter direction choose Both, as you want
the Users  table filter the Language  table (and not the
other way around), as shown in Figure 11-4.
Figure 11-4. One-to-many bi-directional relationship between tables Language
and Users
Then you choose Modeling - Manage roles - Create to
create a new role. I named the role USERNAME  as it
facilitates the USERNAME  function to create the right
filters. In this role you must create a filter for every
dimension table (e.g., Dim1  and Dim2 ) where you let
Power BI lookup the Language ID  column from table
Users , where the User Name  column contains the

name of the currently signed in user (returned by
function USERNAME ).
[Language ID] = LOOKUPVALUE('Users'[Language
I recommend creating a filter on the Users  table as well:
[User Name] = USERNAME()
You can test, if the role assignment does successfully work
by clicking Modeling - View as and then selecting first
Other user and entering the user for who you want to
simulate the experience. And second, the newly created
role (USERNAME in my case). Figure 11-5 shows the dialog
box in the foreground, and the resulting filtered report
and the background.

Figure 11-5. User Fritz@savorydata.com will be presented with the German
version of the report
I though see backdrafts in using Row Level Security to
filter the right language. First, the user cannot change the
language dynamically, but need either write permission
on the Users  table (to change the content of the
Language ID  for her entry) or someone to change their
role assignment there. Row Level Security only works for
users in a Reader role in Power BI Service. If you are a
member, contributor, or admin in the workspace (or the
admin of the Analysis Services database) any role-
assignment is ignored. Therefore, no filter on the
language is in place for you and you see instead all the
values for all dimensions multiple times, each per existing
language. Every report author would need a second read-

only user so he can develop and test reports. Lastly, I
personally find using Row Level Security as a workaround
for language-selection like taking a sledgehammer to
crack a nut.
I for myself would prefer to let the user decide on the display
language. Instead of USERCULTURE  we can, with the exact
same effort, also apply the selected language (of the Language
table) to the dimension tables. The only difference is the
expression for variable _LanguageID . It uses
SELECTEDVALUE  to return the selected Language ID . In case
no selection was made, or more than one single value is
selected, the function will return the content of the optional
second parameter (“EN” in this case):
[Sales] :=
VAR _LanguageID = {SELECTEDVALUE('Language'[Langu
RETURN
CALCULATE(
    SUM('Fact'[Amount])
    ,TREATAS(_LanguageID, Dim1[Language ID])
    ,TREATAS(_LanguageID, Dim2[Language ID])
)

Again, you need to apply this to each and every measure – or
create a Calculation Group, as shown here:
VAR _LanguageID = {SELECTEDVALUE('Language'[Langu
RETURN
CALCULATE(
    SELECTEDMEASURE()
    ,TREATAS(_LanguageID, Dim1[Language ID])
    ,TREATAS(_LanguageID, Dim2[Language ID])
)
In an ideal world we could create a filter, which can be changed
by the user interactively, but can be applied to all (dimension)
tables like what Row Level Security does and without the
annoying hint about ambiguity of the data model. I proposed
this as an idea a couple of years ago. Maybe you want to
support it by providing the needed votes? Here you go: Support
multi-language reports in Power BI Desktop & Power BI Service.
Now it’s time to face the next challenge: Pivoting key-value pair
tables with the help of DAX.
Key-Value Pair Tables

NOTE
The file used for the demonstrations is KeyValue.pbix.
Remember (from “Key-Value Pair Tables”) that key-value pair
tables are tables, where attributes are stored row-by-row. Many
reporting tools (Power BI included) benefit, if such a table is
pivoted and each key-value pair (row) is transformed into an
explicit column. For the solution you need first to distinguish
between aggregable (typically numeric) columns, for which you
want to calculate and show a total (e.g., a customer’s revenue),
and the others (where showing information in the total does
not make any sense; e.g., the city of residence of a customer).
Let’s start with the non-aggregable columns.
The very inner part of the following measure is calculating the
minimum value for column Value . In the context of a
measure, you always need to apply an aggregate function when
referencing a table’s column. Practically, there will be only one
value available at the point in time MIN  is executed, as it is
embedded into an IF  with a condition that only one single ID
is available in the current filter context ( HASONEVALUE ). This is
the safeguard to keep the total (= a filter context where more
than a single ID  is available) blank. Theoretically, there could
be multiple cities for one ID , and then we would select the

(alphabetically) first value. The whole expression is wrapped
inside a CALCULATE  which manipulates the filter context to
only a Key  of “city”. That’s because the measure should only
return the value for a city.
[City] :=
CALCULATE(
    IF(
        HASONEVALUE('Source'[ID]),
        MIN('Source'[Value])
   ),
    KEEPFILTERS('Source'[Key] = "city")
)
For aggregable values, you use one of the fitting iterator
functions (in the example below it is SUMX ) to iterate over the
key-value pair table where the Key  is “revenue” (function
FILTER ) and aggregate the content of Value . As column
Value  is of data type string (so it can contain data of any data
type), I use function VALUE  to convert its content explicitly to a
numeric value.
[Revenue] :=
SUMX(
    FILTER(
        'Source',

        [Key] = "revenue"
    ),
    VALUE([Value])
    )
In this static approach you need to take one or the other pattern
(for aggregable or for non-aggregable columns) and create
measures for every single key of the key-value pair table. By
now, you have learned that I am not a huge fan of such static
solutions. I don’t think it is a good idea to have a solution in
place, where you need to involve a developer, every time
something in the data changes. New keys can be added at any
time to the key value pair table. That’s why I developed the
following generic measure. It contains two variables, one for
each approach ( NumericValue  for the aggregable cases,
NonNumericValue  when the column should be treated as a
string). For NonNumericValue  I removed the CALCULATE  and
filter, for NumericValue  I still need the filter to avoid error
messages that some Value  (e.g., “Seattle”) could not be
successfully converted to a numeric value. At the very
beginning of the code I create another variable
( NumericColumns ), which is a table containing all those keys,
which should be treated as numeric (aggregable). This table
variable is used in variable NumericValue  (instead of the
static value discussed in the static solution) and in variable

GenericValue to decide, which of the two variables
( NumericValue  or NonNumericValue ) should be returned as
the result of this measure. Instead of creating a new measure
for every key, you only need to maintain the list of numeric
columns within one single measure. What an improvement in
maintenance!
[Generic Value] :=
VAR NumericColumns = {"revenue"}
VAR NumericValue =
    SUMX(
        FILTER(
            'Source',
            [Key] IN NumericColumns
        ),
        VALUE([Value])
        )
VAR NonNumericValue =
    IF(
        HASONEVALUE('Source'[ID]),
        MIN('Source'[Value])
    )
VAR GenericValue =
    IF (
        ISBLANK(NumericValue),
        NonNumericValue,
        NumericValue

        )
RETURN GenericValue
But there is more! If you are as lucky as I am with the given
example, the key-value pair table contains a column to specify
the key’s data type. In the code example, I trust that the names
of Power Query’s numeric data types are used to describe the
actual data type of a key’s value ( "Number", "Currency",
"Percentage", "Int64.Type" ). But you can change the code
easily to look for “1” or for “integer” or any value that the
creator of the key-value pair table used to identify numeric
keys. With this version of the generic measure, you don’t need
to change anything when new data arrives, as long as the Type
column delivers the correct content. Maintenance went down to
zero!
[Generic Value (type)] :=
VAR NumericColumns =
DISTINCT(
    SELECTCOLUMNS(
        FILTER('Source', 'Source'[Type] IN
           {"Number", "Currency", "Percentage", 
        "Key", [Key]
    )
)
VAR NumericValue =


    SUMX(
        FILTER(
            'Source',
            [Key] IN NumericColumns
        ),
        VALUE([Value])
        )
VAR NonNumericValue =
    IF(
        HASONEVALUE('Source'[ID]),
        MIN('Source'[Value])
    )
VAR GenericValue =
    IF (
        ISBLANK(NumericValue),
        NonNumericValue,
        NumericValue
        )
RETURN GenericValue
As a bonus I added another version of the generic measure. The
following code will concatenate the content of
NonNumericValues  instead of picking the alphabetical first
row:
[Generic Value (type, concatenated)] :=
VAR N
i C l

VAR NumericColumns =
DISTINCT(
    SELECTCOLUMNS(
        FILTER('Source', 'Source'[Type] IN
           {"Number", "Currency", "Percentage", 
        "Key", [Key]
    )
)
VAR NumericValue =
    SUMX(
        FILTER(
            'Source',
            [Key] IN NumericColumns
        ),
        VALUE([Value])
        )
VAR NonNumericValue =
IF(
    HASONEVALUE('Source'[ID]),
    CONCATENATEX(
        VALUES('Source'[Value]),
        [Value]
        )
)
VAR GenericValue =
    IF (
        ISBLANK(NumericValue),
        NonNumericValue,


        NumericValue
        )
RETURN GenericValue
Combining Self-Service and
Enterprise BI
Creation of Calculated Columns and Calculated Tables is
prohibited in Live connection mode. If you need to add such
artifacts, you need to convert a Live connection into DirectQuery
mode. Independent of the data source and the storage mode
(Import, DirectQuery or Live connection) you can always add
measures written in DAX to the data model.
I strongly recommend that you only store calculations for ad-
hoc analysis in a local data model. Before you start publishing
the report (including the calculation) you should make sure that
the calculation is moved into the central (enterprise) data
model, so everybody can benefit from it. You only need to copy
the DAX formula and paste it into an email to the data engineer
responsible for the centralized analytic database.
Key Takeaways

This was the last chapter about how to model data with DAX.
You learned how to use DAX in the following use cases:
In DAX you can create the lookup tables containing the
definitions of the buckets of each value for the binning
problem. You can also use DAX to create a measure
implementing a non-equi join if the bins are specified via
ranges of values instead.
Filters can be propagated in DAX with function TREATAS.
This comes in handy in cases where you cannot or do not
want to create a relationship in the _Model view. Examples
in this chapter covered the case of non-equi-joins, an
alternative to implement many-to-many relationships and
avoiding ambiguous data models.
Propagating the user’s language into the dimension tables is
a bit more complex than it should be in my humble
opinion. My preferred workaround to add TREATAS  to
either every measure or to create a Calculation Group item
for this purpose.
By applying the appropriate filter, you can create a DAX
measure per Key  of a key-value pair table, and thereby
pivoting the content. If you want to avoid creating (over-
and-over again) a DAX measure per (new) key, you can use
the generic approach described in this chapter. A column

describing the true data type of the content of the value
column is very useful.
DAX can be used in both, Self Service BI and Enterprise BI to
enrich the data available. Keep in mind though that the best
place for standard calculations is always the centralized
data store and not the report layer. Therefore, move
calculations from a Power BI Desktop file to the centralized
data model, as soon as you have verified its correctness.
As pointed out in this chapter: When it comes to non-additive
and semi-additive calculations, there is no way around explicit
DAX measures and for additive calculations I prefer DAX
measures as well. For general transformations of a data model,
Power Query and SQL are the better choice. The next part of
this book is introducing you what role Power Query plays,
when you modeling your data for Power BI.

Chapter 12. Performance Tuning with
DAX
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 12th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
In this chapter I will show how you can create the necessary
tables in DAX to support the performance tuning concepts from
Chapter 8. The idea is to create additional tables, which contain
the data in an aggregated way. These additional tables increase
the memory consumption of your semantic model in exchange
for faster reports (as less data has to be read to create the result

of a query). For simpler aggregation tables, you can configure
Power BI to automatically make use of the aggregation tables.
For more complex scenarios you need to add logic to your
measures, so they use the detailed or the aggregated table
instead for their calculations.
NOTE
The file used for the demonstrations is Performance Tuning.pbix.
Storage Mode
Analog to calculated columns, calculated tables are always
persisted in the data model and re-calculated during the refresh
of the data model. With that said, calculated tables (written in
DAX) are always in Import mode – independent if the source of
the table expression is in DirectQuery or Import mode. You
cannot add calculated tables in Live connection mode.
Pre-Aggregating
You can group by one or more dimension columns and create
aggregations on this aggregation level with the help of function
SUMMARIZECOLUMNS . In the first parameter(s) you provide

column names for the dimensional values you want to group
on. For the combination of the values of these columns, the
aggregation table is generated. These columns specify the
granularity of your aggregation table. Then you provide pairs of
parameters. The first one of these pairs is a string and specifies
the name of the resulting column. The second member of this
pair is a DAX expression (to calculate the aggregated value).
TIP
Keep in mind that SUMMARIZECOLUMNS  iterates over the virtual table generated
from the first parameters. Therefore, the DAX expression for the value is calculated
in a row context. Make sure to wrap the expression into CALCULATE when you need
a filter context instead.
The table generated from the following code is on the
OrderDate  column only. For every available OrderDate  of
the Reseller Sales (DirectQuer – Agg)  table the Sales
Count  (= number of transaction) and the Sales Amount  is
calculated:
Reseller Sales (Agg Table DAX) = /* calculated ta
SUMMARIZECOLUMNS(
    'Reseller Sales (DirectQuery - Agg)'[OrderDat
    "Sales Count", CALCULATE(COUNT('Reseller Sale
"Sales Amount"
CALCULATE(SUM('Reseller Sales

    Sales Amount , CALCULATE(SUM( Reseller Sales
    )
Reports requesting any of the two numbers calculated per day,
can be directly satisfied (as one single row) from this table.
Reports requesting these numbers on e.g., a monthly basis can
also be satisfied, only about 30 rows have to be aggregated.
Queries which ask for numbers per product (or day and
product) can’t be satisfied from this table. You need to make
sure that either Power BI is aware of how to use this
aggregation table (as explained in “Pre-Aggregating”) or that
you add logic to your measures accordingly. The latter is
explained in the next section.
Aggregation-Aware Measures
In cases the out-of-the box features of Power BI’s Manage
aggregations (as discussed in Chapter 8) does not match all the
necessary requirements, you can make any measure
“aggregation aware” by adding a condition to return either an
expression based on the original transaction table or based on
the aggregation table. The task of the condition is to find out if
the current filter context contains only dimension tables

referenced by the aggregation table. If this is the case, then the
calculation can be safely based on the aggregation table. If the
current filter context also contains other dimension tables, then
we need to base the calculation on the transaction table. A
shoutout to Marco Russo, who helped me to write this condition
in a way, where I list the dimension tables used in the
aggregation table (instead of checking all the other dimensions).
The aggregation table Reseller Sales (Agg Table DAX) ,
created in the previous section is aggregated on Date  level.
Therefore, this table can be used to satisfy calculations, which
have no filter at all or filter on the date dimension. It can’t be
used, if a filter on Product  table is present in the current filter
context. In this case, the transaction table Reseller Sales
(DirectQuery - Agg)  has to be used. You see an example of
such an expression in the following code:
[Sales Amount] :=
IF(
    CALCULATE ( NOT ISCROSSFILTERED ( 'Reseller S
    [Sales Amount (Agg Table)],
    [Sales Amount (DirectQuery)]
)

The first parameter of the IF  function is a condition, to check
if the (non-aggregated) transaction table is (still) cross-filtered,
after the filter on table Date (Dual)  was removed. If there
are other filters in the current context, then this condition
returns false and the expression needs to return the sales
amount calculated over the transaction table ( [Sales Amount
(DirectQuery)] ). If the transaction table is only filtered by
the Date (Dual) table or is not filtered at all, the expression can
safely calculate the sales amount from the aggregated table
( [Sales Amount (Agg Table)] ).
Example 12-1.
You could also write the if-condition the other way around: not
checking for filters on the dimensions which the aggregation
table is based on, but on the dimensions the aggregation table is
not based on. If ISCROSSFILTERED( Product )  is true, then
the sales amount cannot be safely calculated from the
aggregation table, but must be calculated from the transaction
table. The problem I see is though that if you in a later point in
time add another dimension (referenced by the transaction
table) to the data model, then you need to change the measure
and add ISCROSSFILTERED( <New Dimension> ) , as you
need to continue to check for filters on dimensions on which
the aggregation table is not based on. Probably there is a chance

that you will oversee a measure and then end up in wrong
calculations. Therefore, I prefer checking on the dimensions
used in the aggregation table instead.
Key Takeaways
The performance tuning concept in the data model is to
exchange storage for query-runtime, where the query-runtime
is longer than expected by the report users. DAX can support
this concept in two ways: One, you can create aggregated tables
as calculated tables and second, you can make any measure
aware of such aggregations in order to perform a calculation in
the most efficient way. You learned also other things in this
chapter:
Calculated tables are always in Import mode (independent
from the mode of the base table).
Table expressions in DAX allow you to create calculated
tables. They can have any content. You can create tables
with aggregated content from another table with the help
of the SUMMARIZECOLUMNS  function.
If the capabilities of the Manage aggregations feature are
not enough, you can implement logic of any complexity in a
measure: via IF  or SWITCH  you decide if the calculation

should be done based on the transaction table or based on
any of the pre-aggregated tables.
This chapter concluded the part about DAX. As you learned,
DAX is very powerful in creating calculated columns and
calculated tables in a data model. These artefacts are always
persisted in the data model and occupy memory. If you want to
aggregation tables in DirectQuery mode you can do so with the
help of Power Query and SQL. The next chapter kicks you off in
Power Query.

Part IV. Data Modeling for Power BI
with the Help of Power Query
Figure IV-1. Example navigation
The fourth part of this book is all about Power Query. Power
Query’s purpose is to bring the information avaiable in any
data source into the right shape. Even when the data is already
in the correct shape, it runs “through” Power Query. You will
come very far by clicking the right buttons in Power Query’s
user interface. Every transformation you are applying to the
data is “recorded” as a step in a Power Query query, written in
the M language. Only in more advanced cases, you need to lay
your hands on such a script.

This part starts with giving an overview about the basic terms
and concept (in Chapter 13):
Tables and Queries
Merging columns to form a Primary key
Creating a Surrogate key
Combining queries
Chapter 14 will show you the steps you will most probably need
to apply to all your data sources:
Normalizing and Denormalizing
Adding calculations
Transforming flags and indicators into meaningful text
Adding a date or time table
Duplicating dimension tables per role
Treating Slowly Changing Dimensions
Flattening parent-child hierarchies
This part teaches you, how to solve the real-world examples,
introduced in Chapter 3 with the help of Power Query
(Chapter 15):
Binning
Multi-fact data models
Multi-language data models

Key-Value pair tables
There are some very intersting features in Power Query, which
will allow you to optimize and find the best trade-off between
the available storage modes, in order to increase the speed of
reports and queries, based on a data model. Chapter 16 has
covered you here.

Chapter 13. Understanding a Data
Model from Power Query Point-of-
View
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 13th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
Power BI’s tool to create the tables for and maintain the shape
of the data model is Power Query. Power Query is directly built
into the product and can be accessed via Home – Transform
data. It opens in a separate window, which is convenient in case

you have more than one screen. Then you can do changes to the
transformations in Power Query, refresh the data model, and
test in the reports, without closing the Power Query window.
All changes you are making in the user interface in the Power
Query window are “recorded” as steps, which are applied to the
source data. You can re-access every step (which is convenient,
when you need to debug the transformations). For many of the
steps you can also click on the gear icon to conveniently change
a step through a dialog box (which, in the most cases, matches
the dialog box you used when you created a step in the first
place). Throughout this book I will also show you, how to make
changes directly to the steps or edit a script in the Advanced
editor. The language of the script is the Power Query Mashup
Language, or M for short. Unfortunately, it has not very much in
common with the DAX language.
Power BI dataflows are sometimes called “Power Query online”.
They share most of the functionality of Power Query.
Unfortunately, they are not completly identical in terms of
features, but so close that you can copy most M scripts between
the two tools. Azure Data Factory offers a similar experience via
Power Query Data Wrangling. Again, there are different
limitations compared to Power Query in Power BI Desktop.

Data Model
All the data landing in Power BI’s data model comes via Power
Query. There is no way around it. It plays a very important role,
but Power Query is not part of the data model itself.
I prefer implementing transformations in Power Query over
implementing them in DAX, for several reasons. First, Power
Query offers a rich graphical interface where you can achieve
even complex tasks without writing a single line of code by
hand. This makes the task of creating transformations fast and
easy. Every transformation is recorded as an Applied step. Many
types of steps offer a gear icon (for example, step Pivot shown
in Figure 13-1), which brings back a dialog box, in case you
need to change the properties of the step. You can directly edit
the code as well: Either you turn on View - Formula Bar and edit
the code for a single step in the formula bar, or you use View -
Advanced Editor to edit the whole script behind a Power Query
query. This is necessary, for example, when a feature of the M
language did not make it into the graphical users interface yet,
like some parameters of functions or if you need to solve more
complex algorithms as programming a loop to iterate over
items or reference intermediate steps of the Power Query code.

Figure 13-1. Applied steps of a Power Query
Second, you can decide per Power Query query if the result
should be loaded into Power BI’s data model. In Figure 13-2 you
can see that the option Enable load does not show a check-mark
and that the name of query SQL  is displayed in italic font.
Disabling this option is useful when a certain query should not
become a table in the Power BI data model. If you would
postpone transformations to DAX instead, you would need to
load the “half-baked” tables and then combine them in the
correct manner into calculated tables and/or calculated
columns to shape the data model. You can hide those “half-
baked” tables, but they still will take up memory and will still
be updated with every refresh (and slow down the overall
refresh time of your data model).

Figure 13-2. Applied steps of a Power Query
For some data sources (s. Query folding on native queries)
Power Query will translate your transformations into the data
source’s query language and hence push the hard work to the
data source. This saves Power Query (and your machine or
your resources in the Power BI Service) from downloading all
the raw data before your transformations are applied step-by-
step. You can imagine that a database server is much better in
applying filters, selecting columns and completing
transformations than your local hardware will ever be. This
feature is called Query folding and leads to a faster refresh of
the data model (and query response time in case of DirectQuery
storage mode).

NOTE
When you plan to enable DirectQuery mode or Dual for a table, you need to make
sure that the transformations applied are only so complex that query folding is
possible for the table (s. Power Query query folding). Otherwise, the query response
time of reports and visuals will not be satisfying.
Lastly, compression of the data model tends to be better, if you
import it through Power Query as opposed to adding calculated
columns via DAX. This has something to do with the fact that
the order of the rows directly influences the compression ratio
for your data. When loading data from Power Query, Power BI
tries to find an optimal sort order of the rows of a table. When
you later add calculated columns, the sort order of the rows will
not be changed – resulting in a potential less than optimal sort
order and therefore less than optimal overall compression of
the column’s data.
The backdraft of adding columns via Power Query is that you
need to refresh the whole query/table afterwards. When you
add or change the definition of a calculated column in DAX, the
new column is calculated based on the already imported data –
no need for any data refresh. For a quick proof-of-concept a
DAX calculated column can be the better choice. But I would
recommend to move the definition to Power Query before you
make the model available to other people.

Tables are the basis of every data model. Learn how Power
Query queries are related to Power BI’s tables in the next
section.
Basic Components
A data model consists of the following parts, which can be
provided by Power Query:
Tables or Queries
Relations
Primary Keys
Tables or Queries
Power Query creates and maintains queries (hence the name
Power Query), not tables. But it is important to know that a
query created in Power Query ultimately becomes a table in the
data model (unless you disable the Enable laod option for a
query). Therefore, the result of a Power Query transformation
is sometimes also called a table. All transformations (including
choice of data type, names of tables and columns, etc.) are
matched into the data model. The collaboration between the
two is very strong. If you, for example, change the name of a
column in Power BI in the data pane, you will discover that

there is a step “Renamed columns” added in Power Query
reflecting this change.
The syntax of Power Query/M scripts is unfortunately
completely different from DAX. While DAX is similar to Excel’s
formulas, M is similar to (and based on) the F# language. Most
importantly, M is case sensitive (that means it distinguishes
between lower- and upper-case characters in both, keywords
and string comparison), while DAX is case insensitive.
WARNING
The fact that Power Query is case senstive and the Power BI data model (or DAX) is
not, can sometimes bite you. For example, if you remove duplicates in Power Query,
it will identify the texts “ABC123” and “Abc123” or “abc123” in a coluomn
ProductID  in the Product  table as different values, and therefore, keep all three
values. If you then want to use this ProductID  column in the data model to create a
relationship, the Product  table’s ProductID  column can not be on the one-side of
the relationship. Power BI will force you to end up in a many-to-many relationship
between the Product  table and the Sales  table, for example. The solution here is
to transform the content of the ProductID  column into uppercase or lowercae
before you let Power Query remove the duplicates.
I use Power Query/M to prepare the data but will not use it to
create any non-additive calculations, as they can only be
correctly provided in DAX.

Power BI allows you to define on a query-per-query basis if you
want to load the result into the Power BI data model. This
allows for queries, which contain only intermediate steps, like
normalized tables, which you will merge with other tables to
achieve a unified dimension table.
In Power Query a column can be any of the following data types
(Figure 13-3):
Decimal Number
A Decimal Number is stored as a 64bit floating point
number. Such numbers have a range from –1.79E +308
through –2.23E –308, 0, and positive values from 2.23E –
308 through 1.79E + 308, but only with a maximum
precision of 15 digits.
WARNING
Power Query suggests Decimal Number when you let it detect the data type. In
general, you should avoid floating point numbers, as they can’t represent all
numbers within their supported range with 100% accuracy (in accordance
with the IEEE 754 Standard as pointed out at Accuracy of number type
calculations). Only use them, if Fixed decimal number will not work for you.
Fixed decimal number

Despite the $-icon suggesting this data type is built for
currency amounts, a Fixed decimal number is made for all
types of numbers, as long as a precision of 19 digits, from
which 4 fixed digits are reserved for the decimals, are
good enough (which is the case for most of the data
models I built so far). This data type can cover numbers
ranging from -922,337,203,685,477.5807 through
+922,337,203,685,477.5807. Fixed decimal number should
be your default choice for all numbers which are not
whole numbers, as full accuracy is guaranteed.
Whole Number
A Whole Number is identical to Fixed decimal number,
except that no digits are reserved for the decimals. It
covers numbers ranging from - 9,223,372,036,854,775,807
(–2^63+1) and 9,223,372,036,854,775,806 (2^63–2).
Percentage
Percentage is identical to Decimal number, except that the
value is automatically multiplied by 100 and a %-sign is
added when displayed.
Date/Time

A column of data type Date/Time is internally stored as a
Decimal number with the whole number portion
representing the number of dates since December 30 1899
(which itself is stored as a value of 0) and the decimal
portion representing the parts of the day (with 0.5
representing 12 pm, for example). The precision is 1/300
of a second.
WARNING
Merging a column of type Date/Time which contains non-zero timestamps
with a usual Date dimension will not succeed. The reason is that Date value is
converted into a Date/Time with a time value of midnight (which will not
match a non-zero timestamp of a column of data type Date/Time).
Date
Date store the data portion only (and not time portion). It
is best practices to only store dates, or, if you need the
time portion as well, the time portion as a separate
column.
Time
Time stores only the time portion (and no date portion).
Date/Time/Timezone

Date/Time/Timezone is a Date/Time including the
timezone. Power BI does not offer such a data type, so the
column will be automatically converted into a Date/Time
when loaded into the data model.
Duration
Duration represents the length of time. Power BI does not
offer such a data type, so the column will be automatically
converted into a Decimal Number when loaded into the
data model.
Text
Text is stored as Unicode (which allows for all sorts of
special characters including Emojis) and can have a
maximum length of 268,435,456 characters.
True/False
True/False can contain a Boolean value (or null).
Binary
A column of data type Binary can contain any data. It is
best practice to either remove such columns or extract
data out of this column into one of the other available

data types. A column of this type will not be loaded into
the Power BI data model anyways.
Figure 13-3. Data types in Power Query
TIP
If Power Query could not detect the data type and you did not set the data type
explicitly, then a column will be of type Any. Such columns will be loaded as Text into
the data model. It is though best practice to being explicit about the data type of a
column instead.

On top of these column types, the content of a column could
also contain one of the following structures:
A List of values.
Record is a grouping of a single row of data with individual
fields.
A Table contains multiple records.
An Error.
Typically, you see such columns when you load data from a
JSON file or merge rows from a different query. All three types
have in common that Power Query shows an Expand icon right
to the header of the column, with which you can extract
individual values from the structures. If you decide against
extracting the values, the content will not be loaded into Power
BI’s data model.
In the next section I will explain which role relations play in
Power Query.
Relationships
As Power Query does not have the concept of tables, it does not
have the concept of defining relationships either. Still, a Power
Query’s result set ends up as a table in the data model. And

therefore, one query can have a relationship to the content of a
different query.
That’s why Power Query lets you merge (“Joins”) and append
(“Set Operators”) queries on the one hand and allows query
references and adding and removing columns on the other
hand. The goal is to be able to combine individual queries into
one or split a single query into several queries. If you split a
query, you will need to create a relationship later in the data
model (to be able to combine the information back again). If
you combine queries, you will need less relationships.
Keep in mind that there is an optimal way of how to split and
combine queries, called Star schema. The number of
relationships is not so important, but the general design of the
data model is. Use Power Query and M to shape the source data
into a Star schema. Chapter 14 will show you different
techniques on how to achieve this in Power Query.
Primary Keys
If you load data into the data model with Power Query, it is not
important for Power Query (and not possible) that you identify
the primary key column(s). Having a single column as the
primary key is though important later in the data model. If you

can not identify one single column as the primary key, but only
a combination of columns (= composite key), then you need to
simply concatenate those columns into a new column, which
forms then the single-key primary key.
Concatenating individual column values of a composite key into
one single column can be done via the user interface: Control
click the column headers in the desired order, then right-click
and select Merge Columns (which will replace the existing
columns; via the ribbon you can choose Merge columns from
either the Transform or the Add Column section, to either
replace the existing columns with the merged result or add
another column to the query). You can then choose a Separator
and give the resulting column a New column name (Figure 13-4).
Figure 13-4. Merging several columns into one

Surrogate Keys
NOTE
The files used for the demonstrations are Financials Dimensional Model Surrogate
Key.pbix, Financials Dimensional Model.pbix,
Financials Filter Dimension Surrogate
Key Measures.pbix,
Financials Filter Dimension Surrogate Key.pbix,
Financials Filter
Dimension.pbix,
Financials OBT Measures.pbix, and
Financials OBT.pbix.
I do prefer surrogate keys as primary keys very much and they
are common best practices (as laid out in “Surrogate Keys”). The
question though remains if it does payoff to add extra steps in
Power Query to create surrogate keys (which will increase the
time used to refresh the data model) to replace non-numerical
keys from your data source. By choosing Add Column – Index
column you can easily introduce a surrogate key in your
dimension tables. That’s simple. But you need the surrogate key
in the (referencing) fact table as well. Therefore, you need to
implement lookups (by chosing Merge and Expand in the
ribbon, as I will demonstrate in detail in “Normalizing”).
You can safely state that if a data model uses less memory the
report performance will be faster (as less memory has to be
scanned to statisfy a query). That’s why I implemented different

versions of the same data model for Power BI’s demo data
“Financials”:
Dimensional model, based on the business keys
Dimensional model, based on surrogate keys
With the help of DAX Studio you have access to the
VertiPaqAnalyzer functionality, which tells you how much space
a table and/or a column occupies in the data model. Turns out
that the difference in size for the Financials dataset is huge:
Creating the model based on non-numerical business keys
occupies roughly 10MB of storage, while the same model, just
based on surrogate keys, only occupies 277KB. That’s less then
3%! Or in other words: You can store 39 times so much
information on any given infrastructure (your PC or your
Power BI Premium capacity) with the version of the model
based on surrogate keys compared to the model based on
business keys.
Taking a look at the details, you will see that the size of the data
itself is very small and almost the same in both approaches. The
difference lies in the size of the dictionary, as shown in
Table 13-1. The use of dictionaries is one of two ways of storing
(and compressing) the value of a column (also called HASH
encoding and is typical for texts; the other is VALUE encoding

and typical for numbers). The dictionary for table Discount
Band  occupies 1MB, while the data occupies only 264 bytes.
This is, of course, an extreme case. The size of the dictionary is
not dependent on the number of rows in the table, but
dependent on the number of distinct values in a column their
size (and has a minumum size of 1MB). In a situation where you
store not 700 rows in the fact table, but 700 billion rows for
these four Discount Band  values, the size of the dictionary
will stay the same, and its size will most probably be
neglectable compared to the size of the data.

Table 13-1. Model size comparison: Business Keys (BK) vs. Surrogate Keys (SK)
Table
Cardinality
Size BK
Data BK
Dicti
BK
Financials
700
5430256
13272
5396
Date
730
1130140
2976
1108
Product
6
1066496
400
1066
Country
5
1066168
264
1065
Segment
5
1066168
264
1065
Discount
Band
4
1066160
264
1065
TOTAL
10825388
17440
1076
How much the refresh time increases due to introducing the
index column in the dimension tables and looking up the
results in the fact table is hard to predict as well. It depends on
the number of rows in the dimension tables and the number of
rows in the fact table compared to the available resources
(memory & CPU). In the concrete example, which only has 700
rows in the fact table, the difference is neglectable on my
machine.

TIP
I encourage you to try out both approaches (keeping business keys vs. introducing
surrogate keys in Power Query) against your own data before you decide for or
against one solution. The size of the resulting data model (and the duration of the
refresh time) is close to unpredictability. This is due to the nature of the compression
algorithm used in Power BI’s VertiPaq engine, which is first and foremost optimized
for fast queries and is not optimized for predictibility of how much time it will take
to compress the data and how much space the compressed data will occupy.
I will return to this topic in “Normalizing”, when I demonstrate
how you can further decrease the size of the data model by
implementing a single filter dimension. Keep in mind that my
suggestion is to consider replacing the business keys in the fact
table with a surrogate key, but to keep the business key’s
content as a column in the dimension table, if you need it in the
reports.
Surrogate keys or not – you will face situations where you need
to combine several Power Query queries into one single. Either
you merge two queries in order to enrich the rows of the first
one with additional columns or you can just append one query
to another. I will talk about the latter in the next section.
Combining Queries

Queries (or better: their results) can be combined in two
different fashions: with Set Operatores (which basically add or
remove rows) or with Join Operators (which basically add
columns). Power Query allows you to show dependencies of
such queries in a dedicated view.
Set Operators
The only set operator available in Power Query is UNION and
can be implemented via Home – Append Queries. In the dialog
box (Figure 13-5) you first select via radio buttons if you just
want to append two queries to each other or more and then,
from a dropbox, which queries you want to append.
Figure 13-5. Append another query to the selected on
This step keeps duplicates, so you need to explicitly remove
duplicates in an additional step if they are not desired (e.g.,

when creating dimension tables). You will see examples of
appended queries later in this book, for example, when you
learn how to create a bridge table (“Budget”).
The next section talks about joining tables, which is called
Merge in Power Query.
Joins
You can join two Power Query queries via Home – Merge
Queries. A dialog box will appear and will display you all the
columns and the first four rows of the selected query (later,
when choosing the join kind, this query is referred to as the left
and first query). Then you choose the second table (referred to
as the right and second query) via a dropbox. In Figure 13-6 I
did merge query DimProduct with DimProductSubcategory.

Figure 13-6. Merging two queries
Selecting the join predicate (= the columns which should be
used to join the two tables) is done by clicking on the columns.
In case the join predicate consists of more than one single
column, you can Ctrl-click the columns. Pay attention to
selecting all the columns in the very same order for both the
first and the second query, as this configures which columns are

compared to each other. On the bottom of Figure 13-6 you see
how many matches the join predicate makes: The selection
matches 397 of 606 rows from the first table.
The dropbox Join Kind (Figure 13-7) offers different kind of
joins with a good description of which result to expect from
each. All joins are equi-joins.
Figure 13-7. Different join kinds are available in Power Query
One join kind is though missing in Figure 13-7: a cross join.
These joins are rare, but in Chapter 15 I will demonstrate a use
case for such a join. To implement a cross join in Power Query,
you simply select Add column - Custom from the ribbon and just
type in the name of the query you want to cross join with the
current.

The condition of the equi-join can be loosend, when you choose
settings from Use fuzzy matching (bottom of screenshot
Figure 13-6) to perform the merge. Fuzzy matching allows you
to fit two rows together on a column value, which is similar to
each other, but not identical. You need to find out for yourself if
such an imprecise match will achieve what you are looking for,
or if it would be better to clean up the data in the source system
to allow for precise joins.
After you merged the queries, you need to expand the result
and select the columns from the second query you want to add
as columns to the first query.
When you combine a lot of queries with each other, it is easy to
lose the oversight of which query is referenced in which other. I
talk about a way to keep an overview in the next section.
Query Dependencies
In Power Query you can visualize the dependencies between
queries. Every query is dependent on at least one source.
Queries can also be dependent on other queries. In complex
scenarios _Query dependencies _can give you a good overview.

In Figure 13-8 I clicked on DimProduct which is therefore
highlighted. Also, all queries and sources, on which query
DimProduct is directly or indirectly dependent are highlighted
as well:
DimProductSubcategory is referenced directly in
DimProduct. Therfore, a direct arrow is drawn between
theese two queries to visualize this direct dependency.
DimProduct is indirectyl dependend on
DimProductSubcategory’s source (c:\users\mehre...). This
path is not directly added somewhere in DimProduct’s
query, but is part of the DimProductSubcategory, which is
directly referenced in DimProduct.
Direct dependency on its own source (c:\users\mehre...)

Figure 13-8. Displaying the dependencies between queries
NOTE
Keep in mind that this is not about entities and their real world relationships, but
queries and their technical relationship to each other. Therefore, this diagram is not
an Entity-Relationship Diagram (ERD).
Types of Queries
Strictly speaking, Power Query has no tables (only queries),
hence no type of table can be determined. But the artifacts
listed under Queries can be of different types:

Query
Every query that is enabled will load data into a table in
Power BI’s data model. It has at least one single step (to
reference the data source) and possible a wide variety of
additional steps to transform the data source into the
desired shape.
Query (Enable load disabled)
Queries created only as an intermediate step (or for
testing and debugging reasons only) should not be loaded
into the data model. Therefore, you can right click a
query’s name and disable the option Enable load. The
name of a disabled query is printed in italic font.
Parameter
Parameters come in handy, if you need certain values
either easily be accessed and changed, or if you need a
certain value be repeated in several queries or on several
steps. Choose Home – Manage Parameters to manage the
options of all parameters in a single dialog box, edit the
value of the selected parameter or create a brand new
parameter. By default, a parameter’s value is not loaded
into Power BI as a table (if you choose so, it would appear

as a table with one single row and one single column,
containing the parameter’s value).
Parameters are exposed in Power BI Service, which makes
them a handy tool for specifying data source’s properties,
like the server-name or the file path. The parameter’s
default value can be overridden in the Power BI service
accordingly, to point it to the data sources in the
production environment, for example. For parameters the
same data types apply as for columns.
WARNING
Parameters of data type Any cannot be changed in Power BI service –avoid
them and choose an appropriate data type instead.
Function
As in other programming languages, functions allow for
code-reuse. You specify the code only once (inside the
function) and then call the function, whenever the code
should be executed (again). You can right-click any query
and select Create function to create a function based on
the selected query (the base query remains as it is).
Extract, Transform, Load

Power Query is the built-in self-service tool in Power BI to
implement all steps of extracting, transforming and loading
(ETL) data into the data model. It’s easily accessible (as its part
of Power BI Desktop) and via the user interface you can clean
and transform the data and achieve even complex steps
without typing any code. For more advanced scenarios (e.g.,
because some features are not exposed over the user interface)
you can also edit and write scripts in the Power Query/M
language.
In Chapter 15 I will show you how I implemented real-world
use cases with the help of the user interface and/or the scripting
language to apply different techniques to transform your data
in an efficient way. Next I will talk about options to keep track
of changes in dimensional data.
Key Takeaways
Power Query is an integral part of Power BI – all the data in the
data model must pass through Power Query. In general, Power
Query is the better place for transformations compared to DAX:
Power Query is the self-service tool inside Power BI to clean
data and to bring it into the desired shape. I recommend

using Power Query over DAX for those kinds of tasks, as it
allows you to only load what is needed into the data model.
Every Power Query that has load enabled query becomes a
table in the data model. The column’s data types are the
same in Power Query and Power BI.
You cannot create relationships in Power Query, but you
can merge, append and also split queries so the resulting
tables form the desired data model shape.
Power Query has a powerful user interface, which allows
for complex transformations by only using the user
interface (also referred to as a low-code/no-code
environment). Only in special cases you will need to write
code in the Power Query Mashup Language, called M. In the
next chapter (Chapter 14) you will see examples in both, the
user interface and in M.
Due to the fact that during a refresh in Power BI you cannot
neither read nor update the already existing rows of data,
you cannot implement Slowly Changing Dimensions of any
type in Power Query. If you need to implement Slowly
Changing Dimensions, you need a data warehouse.
Chapter 17 covers examples.
With the foundational knowledge about Power Query learned
in this chapter it is now time to apply it to build data model.

Chapter 14. Building a Data Model
with Power Query and M
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 14th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
After you have learned about all the moving parts of Power
Query and the M language in Chapter 13, it is now time to see
what this tool has in hand for you to bring a data model into
shape. You can achieve many tasks by finding your way around
in the user interface. No matter what kind of transformation
you are applying, every step is “recorded” as a step in a script in

language M. Think of this feature as something similar like
Excel’s macro recorder. Some steps will show a gear icon. If you
press this button a dialog box will open and guide you again
through the options you have to change the steps logic. You can
also directly edit the scripts, if you want. During this chapter
you will learn how and why to touch some of the scripts
directly. Editing the script is sometimes just faster, compared to
navigating through the user interface with the mouse. In other
cases, it might be that a feature of the powerful language M is
just not available via the user interface. Especially when it
comes to making the transformation either flexible or resilient
against changes in the data source, M can do some very useful
magic.
In this chapter you will learn how to normalize fact tables and
denormalize dimension tables (to form a Star schema) using
Power Query. I will introduce you to calculations in M and show
you how you can transform flags and indicators into user-
friendly text. A whole section is dedicated to creating a date
table, which is mandatory for Time Intelligence calculations in
DAX and increases the useability of a data model by offering a
variety of data and/or time related attributes (like year, month,
weekday, etc.). Especially the date table (but also other
dimensions) may play different roles in your data model (e.g., to
filter and aggregate by order date vs. ship date). Creating these

separate tables is rather easy in Power Query, as you will see.
This chapter will close with a description of how to transform
hierarchies in a way so that they can be analyzed in Power BI.
Let’s start with Normalizing.
Normalizing
NOTE
The file used for the demonstrations is Normalize Facts PQ.pbix.
You can normalize a table (and derive the necessary dimension
tables) with just a few mouse clicks in Power Query. But before
you start you need to find candidates for columns which should
be normalized into one or more dimension tables.
Power Query can assist you in finding these candidates, even
before you load the data into Power BI (and create a report) as
it can calculate descriptive statistics on the query’s result.

NOTE
Pay attention that the default setting is: Column profiling based on top 1000 rows. This
is a good idea performance-wise as it speeds up calculating the necessary statistics.
Only in cases where you have doubts that the first thousand rows might not be
representative for the full dataset you should change this setting to Column profiling
based on entire dataset. Then Power Query will load the whole result set of the query
into memory, which can take a while, of course. You can change the setting by
directly clicking on the text on the bottom left of the Power Query window, as shown
in Figure 14-1.
Figure 14-1. By default column profiling is based on the top 1,000 rows
There are three different statistics available:
Column Quality
Column Distribution
Column Profile
Column Quality
The setting View - Column quality activates the display of three
indicators just underneath the column name, telling you which
percentage of rows are Valid, have an Error or are Empty. You

hopefully remember from “Tables” that primary keys must not
be empty. It seems logical that a primary key must not evaluate
to an error as well. This leaves your choice for a primary key
with columns, which shows 100% for the Valid indicator. In case
you are confident that a column, containing error or empty
values is indeed the primary key then you must first clean up
the column to remove the errors and empty values. You could
replace the erroneous or empty values or filter the rows out.
The best option is to achieve this in the data source. Second best
option is to transform the column in Power Query by replacing
invalid values or by filtering those rows out (in case there are
not needed). Column Segment  shows 100% valid rows in
Figure 14-2.
Figure 14-2. The quality of column Segment shows 100% valid rows
If you want to know how the valid values look like, you can
turn on Column Distribution, which I will introduce next.
Column Distribution

The option View - Column distribution activates a column chart
(to be more precise: a histogram) just underneath the column
name. This chart illustrates you the distribution of the values in
the column and two additional indicators: the numbers for
distinct and unique values. A column can only be a candidate
for a primary key if both distinct (number of different values in
this column) and unique (number of values which only appear
in one single row for this column) are showing the total number
of rows of the table. Column Segment  in my example
(Figure 14-3) shows five distinct values, none of which are
unique. In the current table, Segment  cannot be the primary
key.
Figure 14-3. The distribution of column Segment shows five distinct and zero unique
columns

TIP
The number for unique values in the Column distribution shows how many of the
values are only appearing once in this table. If unique is lower than distinct it means
that some of the distinct values are duplicates. Or in other words: If distinct is lower
than the number of rows, than not all rows can be unique.
For example, if a table only has three rows and one column, which contains values
11, 12, and 13, then it has three distinct values (11, 12, and 13) and all three are
unique (each value only appears once). If the column would contain values 11, 11,
and 12, then it would only have two distinct values (11 and 12) and only one (12)
would be unique.
A column containing the primary key must show the same number for distinct and
unique, namely the number of rows in the table.
If you need even more information about the distribution of the
values for a column, take a look at the next section.
Column Profile
First enable setting View - Column profile and then click on a
column. In Figure 14-4 I have selected column Segment . The
Column Profile displays the histogram from Figure 14-3 as a bar
chart and includes the distinct values of column Segment .
Additionally it gives you descriptive statistics for the columns
content on the left section. The kind of statistics shown are
dependent on the column’s data type.

Figure 14-4. The column profile shows descriptive statistics and a histogram
Via the ellipses (“…​”) of Column statistics you can copy the
whole information into the clipboard. The same applies to the
ellipses (“…​”) of Value distribution. The latter allows you to
Group by the histogram by different options. For example, you
can group the Segment  by text length instead of the actual
values, as shown in Figure 14-5.
Figure 14-5. The histogram can be grouped by text length
The descriptive statistics of a column can give you a hint about,
if the column should be normalized or not and will support you

in identifying if a column is transitive dependent on another
column, as laid out in the next section.
Identifying the columns to normalize
In the process of normalizing a fact table you can use these
statistical indicators, shown by Column Quality, Column
Distribution and Column Profile. Neither a primary key nor a
foreign key may be empty or contain an error. The cardinality
of such a column is low compared to the overall number of
rows in a fact table.
NOTE
In a Star Schema a fact table is not (and should not be) referenced by any other table.
Therefore, you should omit the fact table’s primary key, if not needed in any report.
Keep in mind that columns with high cardinality compress badly – keeping such a
column is costly in terms of memory consumption. The rule here is: When in doubt,
omit a column. You can later decide to include the column, if really needed.
The columns you identify as a foreign keys will become the
primary keys for the dimension tables you “carve out” of the
fact table during the process of normalizing. Foreign keys are
not required to be either distinct nor unique, as they are on the
many side of the relationship. Though, foreign keys, typically
have a low cardinality. For example, depending in your

organisation, your sales table might contain millions of rows,
but only contain a couple of hundreds of products. As the
product is the answer to the question “What?”, it is a
dimensional attribute and needs to be stored separated from
the fact table, in a (dimension) table of its own. You need to
walk through all columns which content is not summable.
Summable columns (like quantity or sales amount) stay in the
fact table.
For numeric non-summable columns like a price, you have two
options: You can store the price in the fact table and/or you
move it into one of the dimension tables. If the actual price for a
product can be different with every sale (because different
discounts will be applied), then keeping them in the fact table
will be a good idea. If the (list) price for a product never
changes, I would move it into the dimension table to save the
space in the fact table. If you need to keep track of price
changes over time in the dimension table, you need to
implement the concept of Slowly Changing Dimensions, as
described in “Slowly Changing Dimensions”. In case the goal is
to store both the applied price and the list price, nothing will
prevent you from storing the applied price in the fact table and
still keep track of the list price in the dimension table.

Let’s apply these ideas. For the following example I used the
built-in example file of Power BI Desktop. If you open Power BI
Desktop the following screen (Figure 14-6) appears.
Figure 14-6. Power BI Desktop’s splash screen
Choose Report in the New section of the screen to get the choice
of Try a sample semantic model as the right-most option on the
canvas (Figure 14-7).

Figure 14-7. Try a sample semantic model
From the Two ways to use sample data you chose button Load
sample data. (Figure 14-8).
Figure 14-8. Two ways to use sample data

Then a Navigator opens (Figure 14-9). If you ever have imported
data from an Excel file, this screen will look familiar to you, as
it is exactly the same. Please activate the checkbox left to
financials and click Transform Data.
Figure 14-9. Choosing the needed artefacts from the Excel file in the Navigator

TIP
When you have the choice between Load and Transform data please always choose
Transform data (despite the fact that the Load button is highlighted). Transform data
will open the Power Query window and you can filter and transform the data, which
is always necessary in a real world scenario. I have never experienced a situation,
where e.g., an Excel file contains the data exactly as it is optimal for Power BI.
Therefore, I could never just Load it, but always had to Transform data contained in
the file.
The example file contains 700 rows – so the default of value of
top 1000 rows will evaluate all rows for column profiling. In
Power Query you get the following results for the Column
distribution:
Segment has 5 distinct rows, 0 unique
Country has 5 distinct rows, 0 unique
Product has 6 distinct rows, 0 unique
Manufacturing Price has 6 distinct rows, 0 unique
Discount Band has 4 distinct rows, 0 unique
Sales Price has 7 distinct rows, 0 unique
Units Sold has 510 distinct rows, 350 unique
Gross Sales has 550 distinct rows, 406 unique
Discounts has 515 distinct rows, 384 unique
Sales has 559 distinct rows, 418 unique
COGS has 545 distinct rows, 398 unique

Profit has 557 distinct rows, 417 unique
Date has 16 distinct rows, 0 unique
Month Number has 12 distinct rows, 0 unique
Month Name has 12 distinct rows, 0 unique
Year has 2 distinct rows, 0 unique
Some columns of this list are clearly candidates for dimensions,
as their number of distinct rows is only a fraction of the total
number of rows (700). The Sales Price has a low cardiniality as
well, but I will keep it in the fact table, as it represents the price
used for an individual sales event (opposed to Manufacturing
Price which is tied to a product).
Before I start creating the queries for the dimensions, I want to
make sure that the financials query only contains cleaned data
(e.g., naming is done properly, data types are the correct ones,
etc.). Then we have two options: We can either Duplicate the
query or we could Reference it. Duplicate creates a full copy of
the query, including all steps. In this case I would rather not
duplicate those steps, because if we later discover a mistake
(e.g., Column names are not correct or we have chosen the
wrong data type) we would have to correct the mistake several
times, once in each copy of the query. That’s why I strongly
recommend to Reference the query instead. This creates a new
Power Query with the base query’s name as the source step of

the query (Figure 14-10). I give the referencing query then the
name of the dimension table. (e.g., Segment).
Figure 14-10. The Source step references query financials
Next, I need to remove all columns, which should not be part of
the dimension. You can do so, by selecting Home - Choose
Columns_ - Choose Columns from the ribbon. I prefer to scroll
through the query result in the middle of the screen instead, as
this shows me the values and the column profile. I Ctrl-click the
columns I want to keep. Then right-click one of the columns and
select Remove Other Columns. In case, in a later point in time,
the table contains more details for a dimension table, you can
always edit this step. Either choose Advanced Editor from the
View section of the ribbon, or turn on View - Formula Bar. Then
you can directly edit the step and add or remove columns from
the Table.SelectColumns  function’s parameter.
Alternatively, you can click on the gear icon, just next to the
step Removed Other Columns in the APPLIED STEPS section and
change your selection in a dialog box. Figure 14-11 shows this.

Figure 14-11. Table.SelectColumn
Finally, we need to reduce the query to only distinct values, so
that every unique dimension row only appears once in the
dimension query. Just chose Remove Duplicates from the query’s
options (Figure 14-12).

Figure 14-12. Removing duplicated rows from a query

Then you repeat all steps (referencing the base query in the
Source step, selecting the dimensional columns and removing
the duplicates) for all dimension candidates. The list of the
applied steps will look the same for all those queries (Figure 14-
13).
Figure 14-13. Creating dimensions in three steps
You can also directly copy and edit the M code generated from
Power Query’s graphical user interface. Here is the script for
the three steps:
let
    Source = financials,
    #"Removed Other Columns" = Table.SelectColumn
    #"Removed Duplicates" = Table.Distinct(#"Remo
in
    #"Removed Duplicates"

The Source  step just references query financials. To remove
the unnecessary columns, column Segment is selected via
Table.SelectColumns  in the next step. Finally,
Table.Distinct  removes all duplicates.
There are only a few exceptions:
One is the date table. You cannot successfully derive the
Date table from the fact table, as there are not sales for
every single day of a year (e.g., the weekends, bank holiday,
etc.), which is mandatory for a date table in Power BI. In
“Time and Date” I will show you, how you can create such a
table in Power Query.
In this example it makes sense to store the Manufacturing
price only once per product. When you select the columns
for query Product make sure that you not only select
column Product, but column Manufacturing Price as well. In
general, for columns with the same statistical properties
(distinct rows and unique rows) you should check if they
can be part of the same dimension. Domain knowledge will
guide you here. And you can create a temporarily Power
Query containing all those columns and see if you discover
a functional dependency between these columns as soon as
you ask to Remove Duplicates.

The base query financials is not appropriate as the fact
table, because it contains unnecessary columns
(Manufacturing Price, Month Number, Month Name, and
Year). Therefore, you need to reference the base query once
more and remove these four columns. You only keep the
references to the dimension’s primary keys, and all factual
attributes, of course. Make sure to not remove duplicates, as
a fact table may contain duplicated rows when the same
product was sold twice on the same day to the same
customer.

As two Power Query queries (and tables in Power BI) cannot
have the same name, you need either to rename the base query
(financials) or find a good name for the fact table. In my
opinion, all tables (including the fact table) should have a user-
friendly name, which makes sense for the people who use the
data model to get answers to their questions. Therefore, I do not
prefer having prefixes like “Dim” or “Fact” in the table’s names
(nor do I like camelCase, PascalCase or hungarian_notation in
such cases).
I prefer to put an underscore “_” as a prefix for names of objects
(both, tables and columns) which I will hide from the users
anyways. This means that I rename financials to _financials and
I am free to keep financials as the name for the fact table (or
any other name, which makes sense for the report-creators).
Alternatively, you can put all dimensional values together into
one single Filter dimension (as discussed in “Normalizing and
Denormalizing”). To achieve this, I applied the following steps:
Creating a composite business key
In query _financials I concatenated the dimensions’s
business keys into a new column (which will form a

composite key). Just Ctrl-click the columns and then right-
click and chose Merge columns. Then choose a separator
character, for which you can guarantee that it will never
be part of the individual columns’s content. I also replaced
the default name Merged wit _FilterKey (Figure 14-14). The
M code looks like this: = Table.AddColumn(#"Changed
Type", "_FilterKey", each
Text.Combine({[Segment], [Country], [Product],
[Discount Band]}, "|"), type text)
Figure 14-14. Merge Columns
Creating one dimensional table for all dimensional attributes
Here I just kept all dimensional attributes intead of just a
single one (as in the solution above), including the
business key column (_FilterKey).

Normalizing the fact table
In the fact table you can remove all dimensional
attributes, but only keep the business key (_FilterKey).
Independent from the aproach: As you do not want to load
both, the original table and the newly created star schema, you
should disable the Enable load option for the original table
(_financials in the example).
During normalization you remove columns containing
duplicated information from the fact table. In the next section
you will learn how to denormalize tables in Power Query. This
is the process of intentionally introducing duplicated
information into a dimension table.
Denormalizing
NOTE
The file used for the demonstrations is Denormalize Dimension PQ.pbix.
Denormalizing means that you remove references from one
dimension table and add the referenced attributes directly to
the dimension table instead. In Power Query you can achieve

this by merging the referenced query with the referencing
query. Select Home - Merge Queries - Merge Queries to add
columns to the current query or Home - Merge Queries - Merge
Queries as New to create an additional query (which I don’t
think is necessary in this case).
NOTE
In Power Query Merge means that you add columns from one or more queries. You
need to specify the merge key(s), to tell Power Query how it knows which rows shall
be synchronized. You can specify a composite join key by holden the Ctrl key on the
keyboard while selecting the columns with your mouse. Of course, the order of the
keys matter - so you should click the columns in both tables in the same order. You
can imagine merging as putting both queries side-by-side. In “Joins” I did call this a
join.
Append on the other hand puts the results of queries under each other. So, you add
rows (and not columns). In “Set Operators” I did call this a union set operator.
Columns with the same name in the source queries end up in the same column in the
result. Columns available in one query, but not the other, will show empty values in
the other queries.
In the example shown in Figure 14-15 I merge DimProduct with
DimProductSubcategory on column ProductSubcategoryKey with
a Join Kind of Left Outer to guarantee that all rows from
DimProduct get loaded, even if there is no matching
subcategory.

Figure 14-15. Merging ProductSubcategory into Product on ProductSubcategoryKey

If your data source is a table from a relational database, which
has foreign key constraints defined, you do not need to Merge
Queries before you can expand the columns from the other
table, but Power Query offers this directly to you.
I want to emphasize though that best practice is that you should
not directly access tables, but exclusively use views. Views give
you an important extra layer. In case the structure of the tables
changes, the views can be rewritten so that the schema of their
result stays the same. This gives stability to your reports and
takes out the pressure that changes to the tables and
modifications in Power Query need to be rolled out to
production at the same point in time. Therefore, I see views as
an API between the database and Power BI. In a well
implemented data warehouse, not breaking changes are done
to the views (that means changes to existing columns like
renaming, change of datatype, removing of columns). If
breaking changes must occure, they have to be implemented as
a new view together with a grace period to give the Power BI
data modelers (you and me) time to implement the changes to
the existing data models.

Then you expand the column via the icon just right of the
column name (Figure 14-16).
Figure 14-16. Column DimProductSubcategory can be expanded
Expanding allows you to choose which columns you want to
expand and if you want to have a prefix inf front of the original
column names. In the example, I chose columns
EnglishProductSubcategoryName (which is the name of the
subcategory I want to show in my reports) and
ProductCategoryKey (which I need to denormalize the
category’s name into the product table). As I want to keep the

column names as they are, I chose to not have a Default name
prefix (Figure 14-17).


Figure 14-17. A choice of columns to expand
Don’t forget to disable Enable load for the referenced queries, as
all necessary columns are now part of a different query and it
does not make sense to load information twice into the data
model. Abstain from actually deleting those queries, as this
would break the Merge step we just created.
The important steps in the M script are the following two:
    #"Merged Queries" = Table.NestedJoin(#"Change
    #"Expanded DimProductSubcategory" = Table.Exp
Function Table.NestedJoin  joins the two queries in an left
outer fashion. Table.ExpandTableColumn  extracts the two
columns (and keeps their original name).
Not many people are eager to build reports, which show long
lists of data. Every report I build so fare contains at least one
measure, which in the simplest form aggregates individual
values. In the next section I introduce you to calculations in
Power Query.
Calculations

In Power Query you can implement numerical calculations of
high complexity. Remember though that it only makes sense to
add calculations for which the result is fully additive in a
report. A visual rarely shows individual rows of a table, but
aggregated values in most of the cases. For example, the count
of rows is an additive calculation, the distinct count definitely
not. While I, in general, do not recommend adding
transformations in DAX, I fully recommend adding calculations
(in the form of DAX measures). As even additive calculations
can be done in a DAX measure, I would not create calculations
in Power Query, but start first in DAX. Only if you see
performance problems you can move the formula to Power
Query and hereby persist (intermediate) results. The size of
your data model will grow when you add new columns, the
speed of the report might increase.
You can use a calculation to replace the value of an existing
column (Transform) or by adding a new column (Add columns).
Both, the Transform and the Add columns sections in the ribbon
of Power Query offer a wide range of transformations,
including calculations on numeric values (Figure 14-18):
Statistics: Sum, Minimum, Maximum, Median, Average,
Standard Deviation, Count Values, Count Distinct Values

Standard: Add, Multiply, Subtract, Divide, Integer Divide,
Modulo, Percentage, Percentage of
Scientific: Absolute Value, Power, Square Root, Exponent,
Logarithm, Factorial
Trigometry: Sine, Cosine, Tangent, Arcsine, Arccosine,
Arctangent
Rounding: Round up, Round down, Round specific decimal
places
Information: Is Even, Is Odd, Sign
Figure 14-18. The Number Column section in Power Query’s ribbon
As you learned, calculations in Power Query are
transformations of numeric columns. You can apply
transformations to non-numerical columns as well, as I show in
the next section.
Flags and Indicators

NOTE
The file used for the demonstrations is Flag.pbix.
Transforming and adding columns is not only possible with
numeric columns (as described in the previous section but is an
easy task for any type of column with Power Query. In the
following examples you will learn about the strength of Power
Query when it comes to transformations of any kind. I will
show you which button to click in the user interface, but we
will talk about the generated M code as well.
NOTE
Power Query’s user interface is very user friendly, and you can achieve many
powerful transformations without writing a single line of code. That means that you
will come very far in Power Query without touching (or understanding) M. Only in
special occasions I turn towards the M code: Fixing a typo in a text I provided,
duplicating logic (e.g., the rules of a conditional column) by copying & pasting
existing parts of the code, or if the user interface does not provide a functionality
(yet). Chapter 7 is about advanced solutions and I will show you some of the latter. In
the current chapter I want to open your mind to M by demonstrating both, the UI and
the M code behind.
In the following example I will apply a (slightly) different
method of transformation on each column:

FinishedGoods
To transform column FinishedGoods  (which contains 0
or 1) to a descriptive text column I selected Add Column -
Conditional Column from the ribbon. Then I typed in
Finished Goods as the New column name and provided the
rules to transform a value of 0 to “not salable” and a value
of 1 to “salable”. In the Else field I provided “unknown”, as
you can see from screenshot , as you can see in Figure 14-
19. If you make sure that View - Formula Bar is enabled,
then you can read the generated M code, when you click
on step “Added Finished Goods” under Applied steps:
Figure 14-19. You can provide rules to create a new column based on the values
of an existing column
= Table.AddColumn(Source, "Finished Goods", 
         if [FinishedGoodsFlag] = 0 then "no

    else if [FinishedGoodsFlag] = 1 then "sa
    else "unknown"
)
Style
In general, I try to avoid solutions with conditional
columns/(nested) if statements. The reason is that I (or
some poor other person) need to dig into the code to find
the place where the condition is hidden, in case the logic
has to be changed (e.g., there is an additional value
available now or the descriptive text has to be adapted).
Therefore, I prefer to have a lookup table instead. In this
example I created table MyStyles via Home - Enter Data
and provided a column named Style with the style’s code
(“W”, “M”, “U”, and an empty value) and a column
StyleDescription with the descriptive text, as shown in
Figure 14-20.

Figure 14-20. A table containing each style and its descriptive text
Editing this table is usually faster and easier than
changing code containing if-statements or filling out the
Add Conditional Column form, offered in a dialog via the
gear icon of the Added Finished Goods step. If you want to
change the content of the table, just click the gear icon
right of the step in APPLIED STEPS. In a perfect world
table MyStyles would be provided from outside Power BI
though, where the responsible users (who have the
business’s authoritiy to decide which descriptive text to
show) have editing rights. This could by anything: an

Excel file on a shared OneDrive, a SharePoint List or a
table in a database, which the users can edit via an
application.
After you have created the lookup table, you need to
merge it into the existing query (Home - Merge Queries -
Merge Queries) and expand the newly added column to
add StyleDescription (Figure 14-21).
Figure 14-21. Merging the created table into DimProduct

As column Style in table DimProduct contains blanks and
Power Query will respect those blanks while merging
tables, I added an extra step Trimmed Style just before
step Merged MyStyles. You just right-click the column’s
header and choose Transform - Trim. And here is the M
code that was generated via the graphical user interface:
#"Trimmed Style" = Table.TransformColumns(#"
#"Merged MyStyles" = Table.NestedJoin(#"Trim
#"Expanded Styles" = Table.ExpandTableColumn
WeightUnitMeasureCode
Replacing a value in a column can be done by right-
clicking on a column’s value and chose Replace Values.
Value To Find will already contain the selected value. You
just fill out Replace With accordingly. That’s how I
replaced an empty WeightUnitMeasureCode with “N/A”.
Column WeightUnitMeasureCode in table DimProduct is
sometimes empty (which is a blank string in the UI and
two double-quotes “” in M), as shown in Figure 14-22. In
other situations, a value, independent of the datatype,
could also be null. And this is exactly, what you would
type into the field in the UI and/or in M code if you want
to replace a null or replace something with null.

Figure 14-22. Replacing empty values with “N/A”
= Table.ReplaceValue(#"Expanded Styles","","
In BuildingADataModelPowerQuery_Normalizing>> I promised
to show you how to create a date table in Power Query. Now the
time has come!
Time and Date
NOTE
The file used for the demonstrations is Date.pbix.
To generate a time and a date table from scratch in Power
Query, you need to reach out to the M language - and can not

completely rely on the user interface. In the first step, you need
to generate a list of dates (or timestamps), which the dimension
table should cover.
Start with Home - New Query - Blank query (Figure 14-23).


Figure 14-23. Starting with a Blank query in order to provide M code
In the following code example, I start with Power Query’s list
indexer operator ({}). Inside the curly braces I pass in the start
value, then two dots (..), and then the end value. In this case the
operator only accepts numeric indexes, but not a date or
timestamp. Therefore, I use function Number.From to convert
the two dates into a number (representing days since the
December 30 1899, btw). I resisted to provide a hard-coded time
date, but lookup column OrderDate from query Fact Reseller
Sales. This gives me peace of mind, in the case data for a new
year is available or old data was removed. As I refer to the fact
table’s content I can be sure that the date table will always
cover the full time range. For the start date I apply
Date.StartOfYear and Table.Min. For the end date,
Date.EndOfYear and Table.Max respectively.
= {Number.From(Date.StartOfYear(Table.Min(#"Fact 
If you then right-click the header (List) you can convert To
Table, as shown in Figure 14-24.

Figure 14-24. Converting a list to a table
Here I have inserted the M code, which was generated by the To
Table step:
= Table.FromList(Source, Splitter.SplitByNothing
Left of Column1 you can click the button to select the correct
data type, Date for the column (Figure 14-25).

Figure 14-25. Setting the right data type for the column
Table.TransformColumnTypes  is the corresponding function
in M.
= Table.TransformColumnTypes(#"Converted to Table
Column1 is not so practicable as the column’s name. Double-
click the header and type in Date. The resulting code looks like

the following:
= Table.RenameColumns(#"Changed Type",{{"Column1
Then I add informative columns to this query. Select the Date
column and choose from available transformation under Add
Column. I chose the following:
Date - Year - Year
For example: 2023
= Table.AddColumn(#"Renamed Columns", "Year",
each Date.Year([Date]), Int64.Type)
Date - Month - Month
For example: 01
= Table.AddColumn(#"Inserted Year", "Month",
each Date.Month([Date]), Int64.Type)
Date - Month - Name of Month
For example: January
= Table.AddColumn(#"Inserted Month", "Month
Name", each Date.MonthName([Date]), type

text)
Date - Day - Day
For example: 31
= Table.AddColumn(#"Inserted Month Name",
"Day", each Date.Day([Date]), Int64.Type)
Add as many versions and combinations, as will be usefull for
the report users. This table will contain a maximum of 366 rows
per year. If your report covers 10 years, this table will contain
less then 4000 rows. Therefore, you can easily afford to enrich it
with columns to support every need of the report creators.
A numeric key for the table can be useful in some situations.
For the date table I prefer a numeric key, which represents the
date in a YYYYMMDD fashion (the value 20230801 would be the
key for August 1 2023). I added the following step to my query:
= Table.AddColumn(#"Inserted Day", "DateKey", eac
= Table.TransformColumnTypes(#"Added Custom Colum
The solution for generating the Time dimension from scratch in
Power Query involves similar steps. You start again with Home -
New Source - Blank query. Then you use List.Times to generate a

list of timestamps. As the first parameter I pass in #time(0,
0, 0)  which represents a timestamp for midnight (0 hours, 0
minutes and 0 seconds). As the second parameter I pass in the
expression “24 * 60”, which gives the number of rows I want the
query to have (for 24 hours of the day and 60 minutes per hour
to cover a time table for every minute of the day). Of course,
you can just write the number 1440 (which is the result of the
expression), as well. I just find it more readable to provide a
calculation, where I multiple the number hours a day (24) by
the number of minutes per hour (60). The third parameter
provides the steps, the final result should contain. I used
#duration(0, 0, 1, 0) , as I want a table for every minute
(0 days, 0 hours, 1 minute, 0 seconds). Here you see the line of
code for the Source step of the query:
= List.Times(#time(0, 0, 0), 24 * 60, #duration(0
The next steps include the conversion of the list into a table,
changing the data type of the column to time and renaming the
first column to Time. I will not repeat the description for the
user interface here, as they are very similar to what we did
when creating the date table. Here you can see the M code:
= Table.FromList(Source, Splitter.SplitByNothing
bl
f
l
(
d
bl

= Table.TransformColumnTypes(#"Converted to Table
= Table.RenameColumns(#"Changed Type",{{"Column1
You can add variations of the timestamp via Power Query’s user
interface. Choose Add Column in the ribbon and then:
Time - Hour- Hour
For example: 11
= Table.AddColumn(#"Renamed Columns", "Hour",
each Time.Hour([Time]), Int64.Type)
Time - Minute
For example: 59
= Table.AddColumn(#"Inserted Hour", "Minute",
each Time.Minute([Time]), Int64.Type)
I also wanted a column, containing the time as a string in the
format “HH:MM”. Let’s take this as a chance to introduce you to
Power Query’s feature Column From Examples. First, Ctrl-click
on both, the Hour column and Minute. Then choose Add Column
- Column From Examples - From Selection. Now you need to
“teach” Power Query by giving examples of the results you
want to achieve by a Power Query expression. Double-click in

column Column1 on the line for Time “00:01:00” and type in
“00:01”. Power Query will fill out the rest of the rows. From the
results, we see that Power Query did not fully understand now
(non-intended leading zeros are the problem here). So, we
provide another example and correct the wrong results.
Double-click on “00:010” and change it to “00:10”. There is
another wrong value “10:00”, which we correct to “01:00”.
When using this feature, always make sure to look out for
“weird” results (and correct them) and make sure that you
understand the code generated and shown on top of the screen.
In the current example I get:
= Table.AddColumn(#"Added Custom Column", "Custom
The formula is concatenating the hour with a colon (:) and the
minute. For both, the hour and the minute, it makes sure to add
a leading “0” and then choose the two right-most characters.
This looks perfect to me. You can keep the formula. Before you
press enter, double-click the header and provide “Time
Description” as the column’s name.
It is not unusual for a dimension to play several roles inside a
data model. And its very typical for the date table. The next
section describes how to do this in Power Query.

Role-Playing Dimensions
A dimension in a data model can play different roles in
different context: You may have both a order date and a sales
date in your fact table. As pointed out in “Role-Playing
Dimensions”, you can either create several filter relationships
between the fact table and the date dimension, or you can add
the date dimension several times into the data model under
different names (e.g., Order Date and Sales Date).
You can easily achieve the latter in Power Query. Simply right-
click the query which has multiple roles in the data model and
choose either Duplicate or Reference. If you duplicate a query,
the new query will contain duplications of all the steps of the
original query. Referencing means that the new query only
contains one single step, which references the other query.
Immediately, this seems not much of a difference. First, if you
later need to change some of the steps, you will face the
difference. If the changes should be applied to all queries (the
original one and the duplicates) you need to repeat the change
for all queries. Referenced queries will automatically receive
the changes. The decision between Duplicate and Reference is a
bet on the future: How high is the probability that future
changes need to be applied to all copies of the query? If the

chances are high, then Reference is the better choice. How high
is the probability that changes should only be applied to
individual copies of the query? If these chances are high, then
you are better of with duplicating all steps, generating an
independent query.
When it comes to role-playing dimensions, I usually decide to
apply all necessary steps (= transformations like renaming of
columns, setting the correct data type, etc.) to the original
query, and then reference the query as many times I have roles
for this dimension (similar to the workflow for creating
dimension tables out of a fact table in “Normalizing”). Make
sure to rename the queries accordingly (e.g., I renamed the
copies of the Date query to Order Date and Ship Date).
It is important that you pay attention to the fact that the two
queries are now fully identical - except for their name. When
you load these queries as they are, then you would end up with
columns with identical names appearing in multiple tables.
This is very confusing if you search for something in the data
model (and therefore, undermines our ultimate goal of making
the report-creator’s life easy). It is also confusing for the report
consumer, because the column names become the standard
header for the visuals. If the report creator does not take time
to change those default headers, it’s up to the report consumer

to guess if “Year” means the year from the Order Date or from
the Ship Date table. Before you take a deep breath and start
manually renaming all columns, take another deep breath and
take a look on the following M code:
= Table.TransformColumnNames(Source, (columnName 
This small line of code iterates over all columns (not the rows!)
of the existing query and adds text “Order " in front of the
column name. The code is fully independent of the number of
columns or the actual name of the existing columns - and
makes it therefore resilient to changes of the table. I added the
same piece of code to the “Ship Date (PQ)” as well (with “Ship "
instead of “Order “, of course). This shows the beauty of M code:
you can make transformations dynamic, instead of statically
renaming every single column.
Dimensions can not only play different roles, but their
attributes can change. If you want to track those changes you
need to learn about the concept of Slowly Changing Dimensions.
Slowly Changing Dimensions

NOTE
The file used for the demonstrations is Slowly Changing Dimensions.pbix.
The idea of the different types of slowly changing dimensions is
to keep track of changes in the data compared to what was
already loaded in a previous point in time. Depending on the
desired type you need then to update existing rows or insert
additional rows to the existing tables. None of that is possible
with Power Query. The way how Power BI (and the storage
engine behind) works, is that you can only refresh a whole
table (or partitions of a table, which you will learn in
Chapter 16). A refresh operation triggers a full load of the whole
content of the table and the refresh operation does not allow
you to access the previously stored data. Therefore, you can’t
implement slowly changing dimensions with Power Query, but
need a data warehouse layer (where you can not only store the
versions permanently, but where you are also able to update
existing rows). In the part about SQL you will learn about the
concept of a data warehouse layer in general and in “Slowly
Changing Dimensions” you will learn how to implement
different types of slowly changing dimensions in a relational
database.

Usually, an implementation of Slowly Changing Dimension does
not mean any extra effort in the world of Power Query, as the
rows in the data warehouse’s fact table(s) are already
referencing the right version of the dimension tables.
Power BI is special when it comes to hierarchies, as you need to
deserialize hierarchies in a way that each level of the hierarchy
is represented by one column in the table. The next section
keeps you covered here.
Hierarchies
NOTE
The file used for the demonstrations is Hierarchies.pbix.
If you followed all best practices described in this book so far,
then you already have denormalized all natural hierarchies in
the dimension tables, as described in “Denormalizing” in this
part of the book. With the natural hierarchy denormalized you
have all levels of the hierarchy as columns in one single table.
Adding them to a hierarchy in Power BI’s data model is very
easy.

In this section I want to concentrate on parent-child
hierarchies. They are very common, and you also need to store
the names of all parents in dedicated columns, one for each
level of the hierarchy. Read on, if you want to learn how you
can achieve this with Power Query.
The solution you need for Power BI is a materialized path of the
hierarchy. Before you can create the materialized path you have
to merge and expand the query (in my chosen example it is
query DimEmployee) as many times as levels exist in this
hierarchy. In a parent-child hierarchy the number of levels is
never fixed (basically that’s the idea of modeling parent-child
hiearchies as a self-joining table, first hand). So, unfortunately,
you need some guessing here: How many levels are currently
used? Then you add a buffer amount (and a note in you todo-list
or calendar to regularly check the table, if the amount of levels
increased over time), to avoid negative surprises or calls from
the users about missing information. You need to repeat similar
(but not the exact same) steps per level:
Home - Merge Queries - Merge Queries to merge column
ParentEmployeeKey of the current query (Employee (PQ
2)) with column EmployeeKey of (again) Employee (PQ 2).
Make sure to choose Left Outer as the Join Kind so you are

not losing any rows, in case there are no child-nodes
available (Figure 14-26).
Figure 14-26. Merging query Employee (PQ 2) with itself. Make sure to choose the
correct columns
The resulting code looks like this:
= Table.NestedJoin(Source,  {"ParentEmployeeK

Expand the newly created column and select at least
ParentEmployeeKey and FullName. Make sure to add "Level
-1" (or a similar hint) to the column name (Figure 14-27).


Figure 14-27. Expand the next level
The resulting code looks like this:
= Table.ExpandTableColumn(#"Merged Queries", 
Home - Merge Queries - Merge Queries to merge column
Level-1.ParentEmployeeKey of the current query
(Employee (PQ 2)) with column EmployeeKey of (again)
Employee (PQ 2). Make sure to choose Left Outer as the Join
Kind so you are not losing any rows, in case there are not
child-nodes available.
= Table.NestedJoin(#"Expanded Level -1", {"Le
Expand the newly created column and select at least
ParentEmployeeKey and FullName. Make sure to add "Level
-2" (or a similar hint) to the column name.
= Table.ExpandTableColumn(#"Merged Queries -2
…​ (repeat similarly for all necessary levels)

After you have got the Employee Key  and Full Name  for
every level as individual columns for every row, you can create
the materialized path. Remember, the materialized path is a
concatenated list of all the keys of all levels above a node. In the
user interface of Power Query, you simply Ctrl-click all columns
containing the keys (in the current example: Level
-5.ParentEmployeeKey, Level -4.ParentEmployeeKey, Level
-3.ParentEmployeeKey, Level -2.ParentEmployeeKey, Level
-1.ParentEmployeeKey, ParentEmployeeKey, EmployeeKey). As
the order of the keys inside the materialized path is crucial, pay
attention to keep the order of the columns exactly as described.
Then choose Add Column - Merge Columns from the ribbon
(Figure 14-28).

Figure 14-28. Build the Path as a merged columns
Which character you choose as the Seperator is not so
important, as long as you make sure that the separator is not
and will never be part of the content of the employee key. I
prefer to use the pipe (|) - which is not available in the list.

Therefore, I choose --Custom-- and type in the pipe symbol. As a
New column name (optional) I provide Path in the dialog box
shown in Figure 14-29.
Figure 14-29. Choosing the seperator
The resulting code looks like this:
= Table.AddColumn(#"Expanded Level -5", "Path", e
With the (materialized) Path at hand, it’s now easy to calculate
on which level a certain node in the hierarchy is. You just count
the separators (|) and add one. Choose Add Column - Custom
Column from the ribbon and fill in “PathLength” for the New
column name and List.Count(Text.PositionOf([Path],

"|", Occurrence.All)) + 1  for the Custom column formula
in the appearing text box (Figure 14-30).
Figure 14-30. Calculating the PathLength
It is common to speak of the leaves of the hierarchy when you
speak of the nodes on the very bottom of the hierarchy. Leaf
nodes are nodes, which are not parents. To find out if a node is
a leaf you need to find out, if you can find the node’s
EmployeeKey inside the list of all ParentEmployeeKey. If we find
it, it is not a leaf. If we do not find it, it is a leaf. Choose Add
Column - Custom Column from the ribbon once more and fill in
“IsLeaf” for the New column name and not

List.Contains(Table.Column(#"Changed Type",
"ParentEmployeeKey") , [EmployeeKey])  for the Custom
column formula in the appearing text box (Figure 14-31).
Figure 14-31. Finding out if an employee is on the leaf level of the hierarchy
Finally, its time for the most important part: Creating a column
for every level, showing all the names of all the parents of a
node. Unfortunately, the Level -X.FullName columns are just in
the wrong order (that’s why I named them “minus X”). As the
hierarchy can be ragged (= not every part of the hierarchy has
the same amount of level), you need to start from the top node
(e.g., the CEO) and work towards the leaves. The first step is

very similar to the creation of column Path, but it is based on
FullName (and not ParentEmployeeKey). Again, double-check
the sequence order of the columns (starting with the column
with “-5” in the name and ending with column named
FullName), as shown in Figure 14-32.
Figure 14-32. Merging the name colums into one

Then you specify the separator and a new name (Figure 14-33).
Figure 14-33. Specifying the Separator and a New column name
And here is the resulting M code:
= Table.AddColumn(#"Changed Type1", "Level", each
This step was just an intermediate step. By selecting Transform -
Split Column - By Delimiter and providing the separator from
the step before (|), you can easily split the content of the Level
column into individual columns per level (Figure 14-34).

Figure 14-34. Spliting the merged column
Here I have the M code for you:
= Table.SplitColumn(#"Added [Level]", "Level", Sp
This code is not the one, which was created by the user
interface. I removed the dots (.) in the list of names for the level
(e.g., I changed “Level.1” to “Level 1” for the sake of better
readability).

If you think, all these steps are a bit tideous and that guessing
(about the number of levels) is not a good approach for a
resilient data model, I fully agree. And we are not alone. Imke
Feldmann developed a function (written in M code), to
dynamically dissolve the levels of the parent-child hierarchy
and flatten them out into individual columns (as you did in a
manual process above). You find the code and blog post
describing the content and reasoning (which is more or less
identical to the steps we did in this section) here: Dynamically
flatten Parent-Child Hierarchies in DAX and PowerBI. I tweaked
Imke’s code in the example file just a bit, to make it conform to
the naming of my columns and tables and my data model.
Now it is time to wrap up what is so important to understand
about Power Query’s role when it comes to data modeling.
Key Takeaways
You learned that Power Query is a powerful tool to achieve all
sorts of transformations. I demonstrated many functionalities
in the user interface and also made sure that you get yourself
familiar with the M language, as it can help to make the steps
resilient against changes in the data source. I limited myself to

what I consider the most important features, which are listed
below:
Normalizing your fact tables involves steps to find
candidates for dimensions. Power Query’s Column quality
and Column distribution gives you an idea of the cardinality
of a column, even before you load it into the data model.
Always double-check the transitive dependencies to make
the right decision.
You create a dimension table by referencing it, selecting all
necessary columns (and removing all others) and remove
all duplicates.
To denormalize a table, you add the information from the
related table by merging it into the existing one and expand
the result to add the columns.
It’s important to disable Enable load for intermediate
queries, which should not be part of the final data model.
Don’t spend too much time on calculations in Power Query.
DAX is usually the better place.
Physically adding variations for a table (for role-playing
purposes) is very easy in Power Query. You just reference
the original table and add a postfix to all column names,
indicating the role.
Slowly changing dimensions must be solved in a data
warehouse layer, which allows to compare the existing

rows with the newly delivered rows in order to insert these
rows or update the already existing rows. This is not
possible in Power Query.
You can flatten parent-child-hierarchies by applying several
steps. Via a function you can apply these steps in a dynamic
way.
In the next chapter you will learn about more advanced
challenges and how to solve them in Power Query.

Chapter 15. Real-world Examples
Using Power Query and M
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 15th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
Beside the standard tasks to transform a given data model into
a star schema, there is also a lot we can do in Power Query to
prepare the data for advanced challenges. In many cases,
Power Query is the better place compared to DAX, when it
comes to shaping the data model. Keep in mind though that
some solutions require DAX measures to be written, which

cannot be replaced by even sophisticated Power Query or M. I
will also utilize scripts in the mashup language M to make a
solution more dynamic, lessening the effort to maintain the
solution in case of changes in the data source.
The uses cases I prepared for this chapter are:
How to group values into bins or buckets in order to show
the name of the bucket instead of the actual value.
How you can support multi-fact data models by bridging
the many-to-many relationship, which will appear between
some of the relationships between a fact and a dimension
table. I will demonstrate the solution on the example of a
budget.
In my multi-language solution Power Query has the part of
collecting the translations together. I will show you how you
can use Azure Cognitive Services to get the texts translated.
Key-value pair tables must be pivoted to be able to satisfy
common reporting requirements. You will learn which
buttons to click in the user interface to pivot the table so
that every key becomes a column of its own. I will also
show you how you can implement a dynamic solution
which will take automatically care of new keys.
I start with binning.

Binning
NOTE
The file used for the demonstrations is Binning.pbix.
Let’s take a look on how we can create either of the two lookup
tables discussed in the section about binning in “Binning”. One
of the lookup tables described there was the full list of possible
values including a column with the correct bin.
Create a Bin Table by Hand
Of course, you can always create a new table by using Home -
Enter Data and manually insert the necessary values (or copy
and paste from an excel speadsheet). You can see such a table in
Figure 15-1.

Figure 15-1. A table containing a row for each quantity
Example 15-1.
The M script for for the content of the table is impossible to
maintain directly, as the table’s content is Base64 encoded:
= Table.FromRows(Json.Document(Binary.Decompress

Instead of the values you entered into the table you see a step
containing a long chain of letters and numbers, which seem to
have nothing to do with your input, due to the fact that the
values are Base64 encoded. Luckily you can edit the data of
such a table by clicking on the gear icon of the Source step.
Resist to add Replace values steps to correct typos in the data
created via Enter Data, but edit the content of the table directly.
I like to use this feature (directly entering the data for a table)
for quick demos or proof-of-concepts. But I do not like it for
solutions rolled out into production. Instead, I find it way better
to create the tables with a script (instead of maintaining the
tables themselves).
Deriving the Bin Table from the Facts
As an alternative to the hardcoded table you can use two other
approaches to define the list of values (which are then assigned
to a bin). Either you reference the fact table (right click the fact
table in the Queries list and choose Reference) and then remove
all columns except the one for which you want to create the
bins (e.g., Quantity), by right-clicking the column header as
shown in Figure 15-2.

Figure 15-2. Removing all columns except the one that should be binned
Then you remove all duplicates, again via the context menue of
the column’s header (Figure 15-3)

Figure 15-3. Removing duplicates
The generated code looks like this:
let
Source = Sales

    Source = Sales,
    #"Removed Other Columns" = Table.SelectColumn
    #"Removed Duplicates" = Table.Distinct(#"Remo
...
The result will maybe not have a row for each quantity, but will
always contain all necessary lookup values used in the fact
table. The order of the rows of this table looks a bit unusual, as
it is not ordered by quantity, but order of appearance of a
quantity in the fact table - this is not a problem. Before I show
you the next step to complete the table, I want to show you how
to create such a table starting with M code.
Create a Bin Table in M
Depending on the size of your fact table, the time to build the
table can take a while. The alternative is, to use function
List.Numbers  and provide a start and an end value (for
which I use Power Query parameters MinBin  and MaxBin ; I
will explain how to create and maintain parameters in a
second). I then make sure to convert this list into a table, give
column Column1 a more descriptive name (e.g., Quantity) and
the proper data type. This cannot be started off via the UI, but
only by creating a new empty query, where you call the
function and pass in the two parameters:

let
    Source = List.Numbers(MinBin, MaxBin),
...
Then you need to convert the list (returned by function
List.Numbers ) into a table via Transform - To Table
(Figure 15-4).
Figure 15-4. Transforming a list into a table
Then I renamed Column 1 to Quantity and chose Whole number
as the data type in the user interface. Here is the first part of
this script:
let
    Source = List.Numbers(MinBin, MaxBin),
    #"Converted to Table" = Table.FromList(Source
    #"Renamed Columns" = Table.RenameColumns(#"Co
    #"Changed Type" = Table.TransformColumnTypes
...
In the next step I add a custom column ( Bin ) containing the
bin name per quantity. To decide in which bin a value fall into, I

provide parameters (which are easy to spot and understand)
and use the parameter values. In case I need to change the
borders of the bins, I do not need to scan through all applied
steps of all queries, but simply change the parameter value.
Figure 15-5 shows the parameters I have created for the
purpose of this demonstration. MediumBin  is selected. Via
Home - Manage Parameters - Manage Parameters you can create
New parameters, change the settings for an existing parameter
or remove it via the x-icon next to the parameter’s name.

Figure 15-5. Managing the properties of a Power Query parameter
A Power Query parameter has the following properties:
A mandatory Name.

A optional Description, which is shown as a tooltip if you
mouse-over the parameter name in the list of queries.
A checkbox to define if a value for this parameter is
Required.
You need to choose a data Type from the list of available
values. The available data types match the data types
available for a column. You find a complete list an
explanation in “Tables or Queries”.
WARNING
Make sure to choose the data type of each parameter wisely. A common mistake
is to leave the data type at the default value Any. This will prohibit changing the
parameter’s value in the Power BI service. A different kind of problem can
occur when you do not choose the right datetime-related data type, as e.g., a
filter based on the parameter might not behave like expected.
Through Suggested Values you decide, if the parameters’
value can be Any value and can be changed via an simple
input field, or if one needs to choose from a hardcoded List
of values or a value listed in a Power Query Query.
Current Value is the value to which the parameter currently
is set. This value is also saved in the .PBIX-file and will be
used for all future refreshes, unless changed here or in
Power BI service.

Let’s use these parameters in a script to add the name of the
bin. Choose Add Column - Custom Column from the ribbon
(Figure 15-6).
Figure 15-6. Adding a custom column
In the dialog box you can then add the expression to return
either “Low”, “Middle”, or “High” accordingly to the bin’s range:
Figure 15-7. Specifying the condition to return either Low, Middle, or High as the bin’s
name
Here is the full M code:
= Table.AddColumn(#"Changed Type", "Bin", each
if [Quantity] < MediumBin then "Low"

[Q
y]
else if [Quantity] < HighBin then "Middle"
else "High")
The code for the column Bin  uses if then – else if then chains
(due to the lack of SWITCH or CASE keyword in Power Query). I
did not hard code the borders of the bins, but reference Power
Query parameters MediumBin and HighBin to decide into which
bin a value fall into. If these boundaries are changing over time,
nobody needs to touch the code, but only change the content of
the parameters. Only if the number of bins increases, we need
to add parameters and a new else if then line to the step where
we define the custom column.
I added another column ( _SortOrder ) to be able to order the
bin names in a custom order (Low first, then Middle and High
last) instead of alphabetical order (which would be High, Low,
and Middle and confuse most report users). For this column I
implemented a logic to pick the smallest quantity of per bin. For
this to happen, I first group the result of the previous step
( Added Custom ) by bin name. Choose Transform - Group by in
the ribbon (Figure 15-8).
Figure 15-8. Grouping a query

Specify the Bin  column in the drop box, enter "SortOrder” for
the _New Column Name, chose Min as the Operation (to get the
minimal quantity value per bin as its sorting value) and choose
Quantity from the Column drop box (Figure 15-9).
Figure 15-9. Specifiying the grouping
I then merge the result of the Grouped rows  step with with
Added Custom  step. Here you need to right-click the last step
and choose Insert Step After (Figure 15-10).

Figure 15-10. Insert a step after Grouped Rows
Insert the following code:
= Table.NestedJoin(#"Added Custom", {"Bin"}, #"G
Finally you need to expand the _SortOrder  column. Click on
the Expand icon left of the column name in the column header
(Figure 15-11).

Figure 15-11. Expanding the _SortOrder  column
Here is the code for the last three steps:
...
#"Grouped Rows" = Table.Group(#"Added Custom", {
#"Merged Queries" = Table.NestedJoin(#"Added Cust
#"Expanded Grouped Rows" = Table.ExpandTableColum

TIP
Don’t forget to provide column _SortOrder  as the Sort by column for column Bin
Power BI’s Column tools.
In Table 15-1 you see the final result for the approach based on
the fact table, which lists only quantities existing in the fact
table in order of appearance of a quantity in the fact table:
Table 15-1. A bin table derived from the values available
in the fact table.
Quantity
Bin
_SortOrder
3
Middle
3
4
Middle
3
1
Low
1
5
High
5
In Table 15-2 you see the final result for the approach based on
function List.Numbers , which is a complete list of values,
independent from if a value appears in the fact table:

Table 15-2. A bin table derived from a list of values
Quantity
Bin
_SortOrder
1
Low
1
2
Low
1
3
Middle
3
4
Middle
3
5
High
5
6
High
5
7
High
5
8
High
5
9
High
5
10
High
5
Create a Bin Range Table in M
A completely different approach to model binning in Power BI,
laid out in “Binning”, is to provide a table containing the ranges
per bin. You can easily create a table in M via function #table :

let
    Source = #table(
type table [#"Low (incl.)" = number, #"High (excl
{
{null, MediumBin, "Low"},
{MediumBin, HighBin, "Medium"},
{HighBin, null, "High"}
}
)
in
    Source
The first parameter is optional, but I recommend to specify the
names and data types of the columns of the table (to have
everything in one place, instead of adding additional steps
afterwards). The second parameter specifies then the content of
the table, using the {} syntax. You need an outer {} for the whole
table, and then inner {} per row. Inside the row you provide a
comma separated list of values. Again, I do not provide the
ranges per bin in a hard coded fashion but reference the
parameters. In the code you can see the advantage of having
lower range inclusive and the higher range exclusive: I can
provide parameter MediumRange for both, the high range of the
“Low” bin and the low range of the “Medium” bin. In “Binning”

I demonstrated how you can use such a table as a lookup table
for the bins in your DAX measure.
In my example such a table looks like this:
Table 15-3. A bin table derived from a list of values.
Low (incl.)
High (excl.)
Bin name
null
3
Low
3
5
Medium
5
null
High
In the following example I will show you how to create a bridge
table in Power Query.
Budget
NOTE
The file used for the demonstrations is Budget.pbix.
The “Budget problem” is a classic example for a multi-fact data
model. You may need more than one single fact table if their
content is on different granularity; usually the budget is on a

different granularity than the actual values. My preferred
solution to connect a fact table of different granularity to the
same dimension tables is over a bridge table. Such a bridge
table is simply a distinct list of the dimensional values on the
fact’s granularity level. The fact’s granularity might be either on
the dimension’s primary key or a different column. In “Budget”
I introduced an example, where the fact table (Budget) is not on
granularity of the product table, but on the product’s product
group. The bridge table must be on the product group in this
case. This table is then “inserted” in the data model between the
dimension table and the fact table with the higher granularity,
as pointed out in “Budget”.
Creating a bridge table involves the same steps as normalizing a
table and creating dimension tables. There are a few
differences though:
You need to create a distinct list of the common values (e.g.,
the product group’s name) from both, the fact table (Budget)
and the dimension table (Product).
You keep all tables: the base fact table, the base dimension
table and the resulting bridge table.
You will find a detailed description of the necessary steps to
normalize a table in “Normalizing”. Here I describe the

necessary steps to create the bridge table between the Budget
and the Product table of my example.
First, I referenced query Budget  (by right-clicking the query
Budget  in the query list and selecting Reference) as shown in
Figure 15-12 and renamed it to Budget Product Group
(PQ) .

Figure 15-12. Referencing the Budget  table
Right-click the newly created query and make sure, to disable
the Enable load option (Figure 15-13), as this query will only be
an intermediate query which you should not load into the data
model.

Figure 15-13. Disabling the load for Budget Product Group (PQ)
Then, I right-clicked column Product Group  and chose
Remove other columns to only keep this column, as shown in
Figure 15-14.

Figure 15-14. Removing other columns than Product Group

The code of these steps looks like the following:
let
    Source = Budget,
    #"Removed Other Columns" = Table.SelectColumn
in
    #"Removed Other Columns"
Second, I created the query for the bridge table, which I will
load into the data model. The first steps are similar to the above
steps: I referenced query Product  (by right-clicking the query
Product  in the query list and selecting Reference) and
renamed it to Product Group (PQ) . Then, I right-clicked
column Product Group  and chose Remove other columns to
only keep this column.
I continued with two additional steps: Choosing Append queries
from the Home ribbon and selecting the intermediate query
from the above steps ( Budget Product Group (PQ) ) to the
current one (Figure 13-5).

Figure 15-15. Appending query Budget Product Group (PQ)  to query Product
Group (PQ)
Then right-click the column header and choose Remove
Duplicates (Figure 14-12). This will give you a distinct list of
product groups, derived from the Product  and the Budget
table.

Figure 15-16. Removing duplicate product groups from the query
The code of these steps looks like the following:
let
Source = Product,

    Source  Product,
    #"Removed Other Columns" = Table.SelectColumn
    #"Appended Query" = Table.Combine({#"Removed 
    #"Removed Duplicates" = Table.Distinct(#"Appe
in
    #"Removed Duplicates"
The final result looks like this:
Table 15-4. A distinct
list of product groups.
Product Group
Group 1
Group 2
Group 3
In the next section I will show you the steps necessary to
implement my concept of a multi-language model in Power
Query.
Multi-language Model

NOTE
The file used for the demonstrations is Multilanguage.pbix.
In this section I want to concentrate on the TextComponent table
described in “Multi-Language Model”, which contains all the
descriptive text for the report (headlines, buttons, etc.). I will
show you how to solve two challenges with the help of Power
Query:
Translating texts into the different languages
Pivoting the table to get one column per text and one row
per language
Let’s assume that you need to create the report for different
languages – including some languages which you do not
understand yourself. Of course, you could hire a translator to
do that job. As this chapter is about Power Query, I want to
show you how you can use Azure Cognitive Services to translate
texts for you. Azure Cognitive Services offer several services,
including a translations API, which you can call in a step in
Power Query. You send the text to the API and tell it into which
language you need this text to be translated to. The service will
return the text in the chosen language. At time of writing the
API supports translations from and to over 100 languages,

including Klingon (now you know the secret of how I translated
the description fields for the sample dimensions into Klingon).
Find a detailed documentation here: Azure AI Translator
documentation
Before you can start with using the API you need to set up an
Azure account (if you not already have one) and add Translator
to a subscription. If you just want to play around with this
feature, a Azure free account will be sufficient. After you
created the Translator service in your subscription you will
receive an API key. Please treat this key as secure as you treat
your passwords. Everybody who knows this key will be able to
use the services at your cost. That’s why I made sure to not
expose the full key in the screenshots shown in this section. If
you think the key was leaked, then you should immediately
change the key in the service (and at every place where you use
the key to connect to the API).
Powers Query’s user interface does not have a button to
connect to the Translator API. (Some Cognitive Services are
exposed via the AI Insights ribbon. You need a Premium
subscription if you want to use them. Translator API is not part
of the AI Insights offering at time of writing.) It makes sense to
encapsulate the steps to call the API in a Power Query function.
Fortunately, the M code is not too heavy, as you can see here:

(text, language) =>
let
    body =  "[{""Text"":""" & text & """}]",
    jsonContent = Text.ToBinary(body, TextEncodin
    //language = if isempty(language) then "en" e
    source= Web.Contents(
        "https://api.cognitive.microsofttranslato
        [
        Headers=
        [#"Ocp-Apim-Subscription-Key" = apikey_T
        #"Content-Type"="application/json",
        #"Accept"="application/json"],
        Content=jsonContent
        ]
    ),
    json = Json.Document(source),
    json1 = json{0},
    translations = json1[translations],
    translations1 = translations{0}
in
    translations1
The function has two parameters: The text  (which should be
translated) and the language  (in which the text should be
translated to). It then embeds the text into a string, representing
the body  for the API call. The body  is converted into a binary

format ( Text.ToBinary +). The call to the API is
done via function +Web.Contents . The language
parameter is appended to the APIs URL. Inside the Headers
the Power Query parameter apikey_Translation  is
referenced (this is the Power Query parameter where you
should paste the API key from the the Translator service). The
result is stored in step source , which is then treated as a JSON
document, from the first row ( {0} ) column [translations]
is referenced and again the first row returned.
Before you can set this function into action, you need to do a
cross join between the Textcomponent  table and the
Language  table. You learned about how to cross join in Power
Query in “Multi-language Model”: Add a new column which
references the Language table. Then expand the Language ID ,
as shown in the following piece of M code:
    #"Added Language" = Table.AddColumn(#"Changed
    #"Expanded Language" = Table.ExpandTableColum
In my example, the texts are only available in English. The cross
join will duplicate the existing English texts per language
available in the Language  table. Before I call the Translate API
function, I added a check: I only want to call the API if the

language is not English. If the language is English, I add an
empty record instead (“[]”), which avoids calling the API for
nothing (and saving money, as you pay per call). Expanding the
new column will contain the translations for all languages,
except for English it will be empty. In step Replace empty
translations  I add a column Custom  containing either the
translated text or the original English text. This is the column I
later rename to DisplayText .
    #"Translate API" = Table.AddColumn(#"Expanded
    #"Expanded Translate API" = Table.ExpandReco
    #"Replace empty translations" = Table.AddColu
Now you have gotten a table with three columns: a technical
identifier ( Textcomponent ), a Language ID  and the
DisplayText . I pivot this result on the Textcomponent
column, providing DisplayText  as the Values Column and
Minimum as the Aggregate Value Function (hidden under the
Advanced options in the dialog window). Voilá, you have now a
table with one single row per language and one column per
text. When a language is selected the column will only show
one single value. Don’t forget to create an active relationship
between this table and the Language  table in the data model.

The cardinality of this relationship will be one-to-one (as the
newly created table only contains one single column per
language).
Of course, you can also easily create the bridge tables for all
dimension tables with Power Query, as laid out in Table 11-5.
Power Query has also nice capabilities to dynamically pivot a
key-value pair table, as you will learn in the next sections.
Key-Value Pair Tables
NOTE
The file used for the demonstrations is KeyValue.pbix.
In some situations you will face a table, structed like this:

Table 15-5. A table containing key-value pairs of rows
ID
Key
Value
Type
1
name
Bill
text
1
city
Seattle
text
1
revenue
20000
integer
1
firstPurchase
1980-01-01
date
2
name
Jeff
text
2
city
Seattle
text
2
revenue
19000
integer
2
firstPurchase
2000-01-01
date
3
name
Markus
text
3
city
Alkoven
text
3
revenue
5
integer
3
firstPurchase
2021-01-01
date
In many situations you want (or need) to transform a key-value
pair table in a way that you end up with one column per key

(instead of one of one row per key), as shown here:
Table 15-6. The key-value pairs table pivoted on the key column
ID
name
city
revenue
first
1
Bill
Seattle
20000
1980
2
Jeff
Seattle
19000
2000
3
Markus
Alkoven
5
2021
In this section I present three different solutions to the same
problem – each solution is building upon the previous one to
make it more resilient to sudden changes in the content of the
key-value pair table. Such changes can easily happen, as the
idea of such a table is exactly, to have data in a format which
stores information in a flexible fashion:
Using the graphical user interface (over and over again)
Using M code (and not touching the query again)
Writing a M function (which can find the data type itself)
Using the Graphical User Interface

I will walk you through all the steps you need to do in the user
interface to pivot a table.
First, I remove column Type , as it’s content is not needed in
the final result and would lead to an unwanted result in the
next step. (Figure 15-17).


Figure 15-17. Removing the Type column
Then you need to select the header of column Key  and select
Transform - Pivot Column from the ribbon (Figure 15-18).
Figure 15-18. Pivoting the Key column
In the appearing dialog box you chose column Value  for
Values Column (easy to remember, right). Very importantly, you
need also to expand the Advanced options and select either
Maximum or Minimum as the Aggregate Value Function. This is
necessary in order to have only one single value per key. If you
choose the alphabetically last or first value is of lesser
importance. In theory there should be only one value per Key
(and ID ) anyways, but you still need to tell Power Query what
to do if the table contains than one value for a combination of
ID  and Key  (Figure 15-19).

Figure 15-19. Pivoting the Key column
NOTE
If you choose Don’t Aggregate as the Aggregate Value Function then it will only work
as long as there is only one value per key. If you duplicate one of the entries in the
example, you will see that you receive an error “Expression.Error: There weren’t
enough elements in the enumeration to complete the operation.”. I do not find the
message very helpful, but rather confusing. You can easily fix it, by making sure to
choose either Maximum or Minimum as the Aggregate Value Function.
The steps so far create the following script:
let
    Source = Source,
    #"Removed Columns" = Table.RemoveColumns(Sou
    #"Changed Type" = Table.TransformColumnTypes
    #"Pivoted Column" = Table.Pivot(#"Changed Typ

All the newly created columns are now of data type text,
because their data type is inherited from column Value , which
was of data type text. And column Value  is of data type text,
because that’s the common denominator to store information of
any data type. Therefore, as a last step you need to scroll
through all the columns and decide if this column is indeed of
data type text or if you need to change it accordingly. Here is
the according code example:
#"Changed Type2" = Table.TransformColumnTypes(#"P
As a one-time effort, this would be OK. But remember, the
reason why someone decides to create a key-value pair table by
firsthand is, because schema changes (= adding new rows with
new keys) are very easy. When I face such a table, I will assume
that I have to regularly check the Change Type step and set the
data type for columns where I assumed the wrong data type
and set the correct type for new columns as well.
Using M Code
Wouldn’t it be better, if the key-value pair table would contain a
hint about the data type of a key? Then we would be able to use

this information to let Power Query set the correct data type
accordingly automatically. Guess what, this is what the column
Type  is about. Many key-value pair tables contain such a
column (because the creator of the table also needs a way to
track the true data type, e.g., to show a date picker in the user
interface for a column of data type date). It is important that
you get a list of all possible values for the Type  column. You
can create a distinct list of the column, but you never can be
totally sure that it will be complete. Therefore, you need to talk
to the owner of the table and get an explanation what values to
expect and how to interpret them. For example, in one of my
project “1” means text and “2” means decimal number. In the
example file I used values, which match 1:1 the Power Query
data types.
In this improved version I replace the step Changed Type2 with
the following logic: I add is step which groups over the Key
column and finds the minimal Type . In theory, there will be
only one single type per key. Practically, the table contains
several rows per key and could therefore contain – by mistake –
different types. In such a scenario I would pick the alphabetical
first type assigned to the key.
#"Column Types" = Table.Group(Source, {"Key"}, {{

The result of this step is not used in the next step but referred to
several times over the course of the remaining script.
Then I created blocks of four steps each per data type. There is
one block per data type available in Power Query: Text,
Int64.Type, Number, Currency, Percentage, Datetime, Date, Time,
Datetimezone, Duration, Logical, and Binary. Each of the blocks
contains these four steps:
<Datatype> Rows
This selects all rows from step Column types, whose data
type should be changed to <Datatype>. The condition puts
a filter on column Type  according to what the
documentation for the key-value pair table tells you. If a
type of “1” means it’s a string, then you have to set the
condition for step Text Rows to filter where Type equals
“1”. Remember that Power Query is case sensitive. A
check for equality to “text” (lower case t) will not find
rows, which contains “Text” (capital T).
<Datatype> Rows Keys
This step removes all columns from the previous step,
except for column Key .

<Datatype> Column List
This step converts the content of the previous step into a
List.
<Datatype> Changed Type
Now, finally, the appropriate data type is set. A nested
Table.TransformColumnTypes and List.Transform allows to
change the data type for a list of column names. This list is
not hard coded, but references the list generated in the
previous step.
I included the block for integers here:
    #"Int64.Type Rows" = Table.SelectRows(#"Colum
    #"Int64.Type Rows Keys" = Table.SelectColumns
    #"Int64.Type Column List" = #"Int64.Type Rows
    #"Int64.Type Changed Type" = Table.TransformC
You can take this script and re-use it. The only thing you need to
take care are the “<Datatype> Rows” steps update the filter
condition accordingly to the content of the Type column.
Writing a M Function

To make everything even easier to use and maintain, I built an
even more dynamic solution, which is based on the works of
Imke Feldmann and Daniil Maslyuk. First, the matching
between the data types mentioned in the key-value pair table
and the actual Power Query data types is done via a table.
Second, in a case, where the type cannot be found in this
matching table, the solution finds a fitting data type itself.
Third, everything is packaged into a bunch of functions, so that
you only need to call a function and not
Table SourceType2PowerQueryType  which contains one
row per Type  delivered from the data source and matches
it with the appropriate data type in Power Query. For this
example, I used the feature Enter Data to create the pairs
(Figure 15-20). In the last step (Type From Text) I call Power
Query function TypeFromText  to convert the text in
column PowerQueryType  into a Power Query data type
from. This will later allow, to convert a Value  column into
the appropriate data type.

Figure 15-20. A table containing the mapping between the source table’s Type
and the matching Power Query data type

Power Query function TypeFromText , which takes a text
as a parameter and converts it into a Power Query type. It is
used in Power Query SourceType2PowerQueryType
let
    /* Based on a script from Imke Feldmann/D
     * https://www.thebiccountant.com/2019/11
     */
    func = (TypeAsText as text) =>
    Record.Field(
    [
        type null = type null,
        type number = type number,
        Currency.Type = Currency.Type,
        Percentage.Type = Percentage.Type,
        Int64.Type = Int64.Type,
        type datetime = type datetime,
        type date = type date,
        type time = type time,
        type datetimezone = type datetimezone
        type duration = type duration,
        type text = type text,
        type logical = type logical,
        type binary = type binary,
        type type = type type,
        type list = type list,
        type record = type record,
type table = type table

        type table = type table,
        type function = type function,
        type anynonnull = type anynonnull
    ],
    TypeAsText)
in
Value.ReplaceType(func, Value.ReplaceMetadata
Function TypeFromAny  which accepts parameters
TableName  (the name of a Power Query result to process),
Key  for the name of the key column of the mentioned
table, Value  for the value column and Type  for the type
column. The function first converts all the values explicitly
to data type Any ( Changed to Any ). Then it removes the
Type  column ( Remove Type ) and then pivots the result
( Pivoted Column ), like with the other two solutions for
the key-value pair problem, presented in the two previous
sections. Now comes the secret sauce of my solution: In step
Added custom  I try  (and catch ) to convert the values
of a Key  into into different datatypes: First logical . If
this fails, then datetime , if this fails currency  and as a
final fail over I assign data type text . This solution is not
perfect, as e.g., a conversion to date , time  or datetime
will succeed for any date/time related data and therefore
not be distinguished. In this case I chose datetime  as it

can host date and time as well. Similar is true for numbers:
Any number can be successfully converted into a
decimal , currency  or whole number , where I chose
currency  as the common denominator. The code looks
like the following:
(TableName as table, Key as text, Value as te
let
    Source = TableName,
    #"Changed to Any" = Table.TransformColumn
    #"Remove Type" = Table.RemoveColumns(#"Ch
    #"Pivoted Column" = Table.Pivot(#"Remove 
    #"Added Custom" = Table.AddColumn(#"Pivot
        if (try Logical.From(Record.Field(_, 
        if (try DateTime.From(Record.Field(_,
        if (try Number.From(Record.Field(_, R
        "text"),
    Custom = #"Added Custom"{0}[Custom]

in
    Custom
Function TablePivotDynamic  with parameter
TableName  and the names for the mandatory columns in
this table: IDColumn , KeyColumn , ValueColumn , and
TypeColumn . This is the function which you should call in

your solution, because it takes care of all necessary steps to
pivot your key-value pair table and gracefully assign the
data types to the key-columns. This code is again based on
the works of Imke and Daniil. I incorporated the call to
function TypeFromAny  in cases where I could not find a
match in table SourceType2PowerQueryType . It achieves
the work of the blocks-per-data-type in the previous
section’s solution in a dynamic way.
(TableName as table, IDColumn as text, KeyCol
let
    Source = TableName,
    #"Removed Type" = Table.RemoveColumns(Sou
    #"Changed to Text" = Table.TransformColum
    #"Pivoted Column" = Table.Pivot(#"Changed

    /*
    inspried by Imke Feldmann
    Dynamic & bulk type transformation in Pow
    https://www.thebiccountant.com/2017/01/09
    */

    #"Column Types" = Table.Group(Source, {Ke
    #"Find missing Type" = Table.AddColumn(#"
        if Record.Field(_, TypeColumn) = null
        then TypeFromAny(Source, KeyColumn, V
        else Record.Field(_, TypeColumn)),

    #"Removed Type 2" = Table.RemoveColumns(#
    #"Renamed Type" = Table.RenameColumns(#"R
    #"Changed Type to Text 2" = Table.Transfo
    #"Lowercased Type" = Table.TransformColum
    #"Trimmed Type" = Table.TransformColumns(
    #"Merged Queries" = Table.NestedJoin(#"Tr
    #"Expanded SourceType2PowerQueryType" = T
    #"List Key & Type" = List.Zip({#"Expanded
    #"Set Data Type" = Table.TransformColumnT
in
    #"Set Data Type"
The last section in this chapter is dedicated to which role Power
Query plays when it comes to compare self-service and
enterprise BI.
Combining Self-Service and
Enterprise BI
Only if you are using Power BI as a Self-Service BI tool, you will
create transformations in Power Query yourself. In an
Enterprise BI environment, all the necessary transformations
are already done by data engineers in the end user’s data
source: Either in a data warehouse layer, in an Analysis Services

database, in an Power BI dataflow, in an Power BI semantic
model, etc. When you discover yourself applying steps in Power
BI, while you are connected to the enterprise data warehouse
layer you should stop and re-consider. Ask yourself why (if?)
this transformation is really necessary and why it was not
already done in the data source. If it is necessary, then talk to
the owner of the data source and agree on a solution to add the
transformation there. Only add it in Power Query as an
intermediate solution and only if the timeline for implementing
the transformation in the data source is too long (as a “quick
fix”). Set a reminder in your calendar to review the steps in
Power Query in a later point time to adapt it to use the
transformed data from the data source.
The reason for this recommendation is, to aim for one version
of the truth. Every time you transform data it is only available
for others if they connect to this dataset. Not all datasets (and
the transformations they contain) can be re-used equally well.
Transformation made with Power Query are available in the
Power BI semantic model if published to the Power BI Service.
Users with access to it can connect via Power BI Desktop or
Excel. Your milage with other tools may vary, as other tools (or
users) might prefer a relational database. To satisfy
requirements from a broader range of tools (and users) it’s a
good idea to apply all transformations in a relational data

warehouse as a common denominator from which you load the
data (without any additional transformations) into a Power BI
semantic model or Analysis Services Tabular.
Key Takeaways
This chapter presented solutions to practical use cases. It
demonstrated that you can go some extra steps in Power Query
(mostly by using the power of the M language) to create a
dynamic and resilient solution, which does not break (so easily)
with changes in the data source.
You can create a bin table as either a distinct list of the
fact’s or by generating a list of values in Power Query. A
table containing simple the ranges can be created via the M
language’s table operator. In both cases, the bin’s borders
should be provided in Power Query parameters for easy
maintenance.
Just apply the concept of denormalizing to create a bridge
table, in case a fact table has not the same granularity as
the dimension table.
The Textcomponent  needs to be pivoted for easy usage in
the report. This can be done as a simple transformation
step.

I discussed how you can use Azure Cognitive Services to
translate the Textcomponent  table for you.
A key-value pair table needs to be pivoted in most cases, so
that common reports can be built on top of it. This is not a
real challenge in Power Query and can be solved with the
user interface. It is very important though, to set the right
data type per key. I showed you a script which will use the
information of the Type  column to apply the data type
automatically and another script which can even find a
fitting data type per Key  automatically.
Power Query is the go-to-tool to apply transformations in a
Self-Service BI scenario. In an Enterprise BI setting consider
moveing transformations into the data warehouse layer
(and just load the tables then 1:1 into Power BI without
further transformations).
This was already the third of four chapters about Power Query.
In the next chapter you will learn how to support the
performance tuning concepts discussed in Chapter 8 in Power
Query.

Chapter 16. Performance Tuning the
Data Model with Power Query
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 16th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
In this chapter you will learn how to improve the performance
of a data model with different strategies in Power Query.
You
will learn that Power Query is not available in all storage modes
– its choice is therefore important. Depending on the storage
mode Partitioning can speed up refresh time and/or query time

tremendously. I will show you how can support your
partitioning strategy in Power Query.
Finally, this chapter talks about another strategy to speed up
query time: how to pre-aggregate the content of a table.
Remember, aggregation tables are tables with a different
granularity than the base (transaction) table. This can speed up
calculations. For example, you can create a table, which
aggregates calculations by day. Then the value for a year does
not need to be calculated based on millions of rows but based
on only 365 pre-aggregated values.
NOTE
The file used for the demonstrations is Performance Tuning.pbix.
Storage Mode
The aggregation table implemented with Power Query can be in
any of the available storage modes (Import, DirectQuery, or
Dual), except for Live connection, which prohibits the use of
Power Query at all.
That means that the aggregation table does not necessarily need
be imported into the data model but can be “virtualized” in

DirectQuery mode. This is feasible, if you need not only to keep
the transaction table in DirectQuery mode (please re-read
Chapter 8 to refresh your memory on when DirectQuery makes
sense), but the aggregation table as well. In a perfect world,
Power Query’s feature Query folding will help to apply
aggregations directly to the query sent to the data source, so
that no explicit aggregation table should be necessary. If query
folding does not happen, the whole content of the Source step in
Power Query is first transferred from the data source and then
all transformations listed in Applied steps are executed locally
on your machine. This is not very efficient, especially when you
work with a “smart” data source, like a database system, which
could apply transformations on the server’s side. In real world
scenarios Query folding might generate a less-than-optimal
query, though. Then an aggregation table created in Power
Query can be of help.
NOTE
In Chapter 20 I will show you how you can create an aggregation table in a relational
database. In combination with DirectQuery this might be the better solution
compared to Power Query. I would use Power Query in combination with
DirectQuery for aggregation tables only in cases where adding a database object to
the data source is not an option.

At the time of writing Power BI’s feature Manage aggregation
can only be applied to a base table which is in DirectQuery
mode. The aggregation table must be in Import mode. In
Chapter 12 you learned that you can manage which aggregation
to use in DAX, too. Therefore, any combination of storage modes
(except for Live connection) can be made for the base table and
“its” aggregation table(s).
Independent from the storage mode, a table contains one or
more partitions – sub-parts which can be managed and
refreshed independently from each other.
Partitioning
By default, every refresh in Power BI triggers a full load of the
table. To implement a delta load, you need either to partition
your table in Power BI (usually by date; s. “Partitioning”) and
implement a logic to only refresh those partitions where you
expect that changes have happened.
Natively, Power BI Desktop does not allow you to specify these
partitions. And even with external tools you cannot specify
partitions in your Power BI Desktop file. Instead, you need to
publish your .PBIX file to the Power BI Service first. At the time

of writing, you need a workspace with Premium capabilities in
order to create partitions (as read/write access to the XMLA
endpoint is needed). Then you need an external tool, like
Tabular Editor to define the partitions.
Custom partitions give you great flexibility in creating the
definitions and great control over when you want to trigger a
refresh for which partition. Think of a partition as a sub-
element of a table. Indeed: When you did not explicitly
partition a table yet, the table consists of one single partition.
Every partition has its “own” copy of the Power Query/M script
to refresh it, with the exception that at least one filter is
different.
WARNING
Please make sure that the filters are written (and tested!) in a way that no row of the
table gets lost (because then you load too few rows from the source) or can be part of
more than one single partition (because then you would load this row into multiple
partitions of the same table). Both would falsify the content of the table and must be
avoided under all circumstances. You are responsible totally on your own for the
partitions and their filters – I am not aware of any tool which will check the
partitions for the mentioned mistakes for you.
Alternatively, you can use a feature of Power BI Service called
incremental refresh. Every time a refresh is triggered, Power BI

Service will take care to refresh (only) the necessary partitions.
If you want to use this feature, you need to create two
mandatoryPower Query parameters of data type Date/Time. The
names of these two parameters are “RangeStart” and
“RangeEnd”. You need to create filters with the help of these
two parameters in all queries, you intend to refresh
incrementally. For example, you add a filter to a query and
specify that column OrderDate  must be equal or after the
RangeStart’s value and before the RangeEnd’s value. After that
you can activate the incremental refresh settings in Power BI’s
Model view under the table’s properties. You can find more
about this feature in Microsoft’s documentation at Incremental
refresh and real-time data for datasets.
NOTE
No matter how you define partitions in Power BI make sure to ¬align the partitions
with the partition strategy in the data source. If the data source has different or no
partitions, the danger is that a refresh for each partition in Power BI triggers a full
scan of the whole table in the data source. This will make the overall refresh of the
data model slower compared to no Power BI partitions. This would be the opposite of
what you want to achieve with partitioning.
Partitioning makes the refresh faster – when the filters of a
query match the partition key, it can also make a query faster.

However, a query becomes even faster when it can build on top
of pre-aggregated data.
Pre-Aggregating
Pre-aggregating means that you create a Power Query, which
applies grouping on some of the columns of the Power Query
and applies aggregation functions on others. You can achieve
this via Home - Group By in the Power Query window. The Basic
mode of the dialog window (Figure 16-1) allows you to specify
one single column to group on and have one single Column
created based on an aggregation Operation. You can give this
column a name of your choice (New column name).
Figure 16-1. Grouping by OrderDate  and aggregating the SalesAmount

You can apply any of the following operations:
Sum
Average
Median
Min
Max
Count Rows
Count Distinct Rows
All Rows
For the majority of operators, you need to specify a column on
which the aggregations will be applied (e.g., the content of
which column you want to sum up). The latter three operators
do not allow you to specify a column, but they are calculated
over all columns: Either counting all the rows (Count Rows),
counting all the rows with a unique combination of values
(Count Distinct Rows) or collapsing everything into a column of
type Table (All Rows).
In Advanced mode (Figure 16-2) you can add several columns
on which you want the resulting query to be grouped by and
add more aggregations. In the example I kept the aggregation to
a single column ( OrderDate ) but added two aggregations: One
Sum of the SalesAmount  which I name SalesAmount  and

another named SalesCount  which just counts the rows per
OrderDate .
Figure 16-2. Grouping by OrderDate , aggregating the SalesAmount , and calculating
the number of rows
The result is a query with three rows: OrderDate ,
SalesAmount , and SalesCount  which has one single row per
OrderDate . If there is no filter applied or only OrderDate  is
filtered, visuals based on this aggregated query will be way
faster compared to applying the same calculations on the
Reseller Sales table.

NOTE
Lookup “Pre-Aggregating” to find out how you can make Power BI aware of the
available aggregations. If the logic is more complex, then “Pre-Aggregating” teaches
you how to change your measures in order to calculate in the most optimal way.
Key Takeaways
This chapter talked about how steps in Power Query can
support the performance tuning strategy for your data model.
Tables created in Power Query can be in either Import,
DirectQuery or Dual storage mode. Therefore, an
aggregation table created with Power Query can be
imported (for best performance) or stay in DirectQuery
mode (or Dual for that matter) if you have reasons to do so.
Live connections do not allow for any transformations in
Power Query.
Power BI’s Incremental refresh builds on top of two Power
Query parameters and filters you apply to a Power Query.
Partitions are then automatically created for you.
You can create your own custom partitions as well. You are
fully responsible to set the filters so that every row appears
exactly in one partition.

Partitions in Power BI should always be aligned with the
partitions in the data source to gain performance
advantages (otherwise the performance can degrade
instead of improving).
Power Query offers a limited list of aggregation functions
you can apply on existing columns: sum, average, median,
min, max. You can also count all or the distinct rows or
collapse everything into a column of type Table.
This chapter closed this book’s topics about Power Query. The
next part of the book is dedicated to the SQL language. You will
learn what you can do in a relational data warehouse (layer) in
terms of supporting the perfect data model for Power BI.

Part V. Data Modeling for Power BI
with the Help of SQL
Figure V-1. Example navigation
The part about SQL is especially aimed towards data engineers
(usually part of the IT department) and the dedicated domain
expert. SQL stands for “Standard Query Language”, a deceiving
name, as every database management system comes with its
own dialect. As this book is about Microsoft Power BI, I will
concentrate on “T-SQL” (= Transact-SQL), which is available for
all SQL interfaces in Microsoft’s data platform. This dialect
comes with a procedural extension, which allows to create
variables, implement conditionsal executionso of code or loops.

Such a code can be stored in the form of procedures and
functions in a database.
Chapter 17 starts with an introduction to the parts of a data
model in a SQL-based database:
Tables
Primary and Foreign Keys
Relationships
Combining the content of tables and possible traps
Chapter 18 shows you all steps, typically when building a data
warehouse (layer) from any data source:
Normalizing and Denormalizing
Adding calculations
Transforming flags and indicators into meaningful text
Creating your own date and time tables
Duplicating tables in case they play more than one role in a
data model
Implementing Slowly Changing Dimensions of different
types
Flattening parent-child hierarchies
The challenges of the real-world can be manyfolded. Also in this
part, I will show you how to solve the four introduced in

Chapter 3 by demonstrating the power of the SQL language:
Binnng
Multi-fact data models
Multi-language data models
Key-Value pair tables
This part concludes with solutions in SQL to support a good
performing data model in Power BI (Chapter 20). On the one
hand, you can decide to “virtualize” transformations in SQL or
persist the data in the right shape in the relational database. On
the other hand, you need to decide if you can import the data
provided by SQL into Power BI or only query it when needed in
a Power BI visual.

Chapter 17. Understanding a
Relational Data Model
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 17th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
Relational databases have existed since 1970 and introduced a
lot, then, new concepts: tables, relationships, constraints,
normalization, etc. The concept, and its implementation by
various vendors (e.g., SQL Server, Azure SQL DB, or Azure SQL
Managed Instance by Microsoft) is still successful and allows for
a variety of use cases. That’s why you find both application

databases (also called OLTP = Online Transactional Processing)
and analytical databases (also called OLAP = Online Analytical
Processing) implemented as relational databases.
This Chapter will guide you on how relational databases are
different from Power BI and Analysis Services Tabular. You will
learn that exactly due to these differences a relational data
warehouse is the perfect addition to your analytical
infrastructure in an an enterprise environment. You will
explore techniques and I will describe use cases and how you
can implement them in a relational database (managed and
updated by SQL = standard query language) to make the
experience in Power BI and Analysis Services Tabular great.
In this chapter I will introduce all basic concepts: That a data
model consists of tables; that columns of a table can have
different purposes (key or attribute); how you can combine
information which is spread out into different tables.
Data Model
A data model implemented in Power BI/Analysis Services
Tabular or in a relational database have many things in
common: You store both data and metadata in it. The data is
hosted in tables. Metadata explains how the tables form the

data model. Let’s introduce the basic parts of a relational data
model.
Basic Components
This section introduces you to the basic components of a
relational data model:
Tables
Relationships
Primary Keys
Surrogate Keys
Foreign Keys
Tables
Tables are the core of a relational database. That’s where the
data is stored. A table is part of a database schema, which is
part of a database, which is stored on a database server. The
database schema works well as a security barrier. I put all the
content I want to expose to Power BI into its own schema and
give the users and applications read-only access to (only) this
schema. Typically I name this scheme PowerBI , Reporting ,
or something similar. [PowerBI].[Sales]  would refer the
Sales  table within the PowerBI  schema.

You have influence over the physical storage of a table by
creating a clustered index on it. As a rule of thumb, all fact
tables and dimension tables with a size over one million rows
should be stored in the columnstore format. All other table
should have clustered index on their primary key and page
compression enabled.
To further improve performance for different scenarios you
might create non-clustered indices. As long as you plan to fully
load the data from the data warehouse into the Power
BI/Analysis Services data model (which is the recommendation,
unless you have to specific reasons to not do so), and nobody
and no application is directly querying the tables there is no
need to create such additional indices.
A table consists of columns. The columns can store data of
different types. In case of SQL Server/Azure SQL DB you have
the choice between:
Exact numerics: bit, tinyint, smallint, int, bigint, numeric,
decimal, smallmoney, and money
Can be stored in Power BI’s Whole number, Fixed decimal
number, or Decimal number

A value of SQL’s type bit is usually stored as True/false in
Power BI. smallmoney and money are legacy data types
and should not be used anymore, choose one of the other
data types instead. numeric and decimal are synonyms
for each other. You need to provide scale and precision.
e.g., decimal(5,2) can fit values between -999.99 and
+999.99 – a precision of up to five digits from which are
two decimals. The default precision is 18, the default scale
is 0.
Approximate numerics: float, real
Can be stored in Power BI’s Decimal number
You should avoid these approximate data types at all, as
they are not exact and unexpected rounding effects can
bite; use one of the exact numerics instead.
Date and time: datetime, smalldatetime, datetime2,
datetimeoffset, date, time
Can be stored in Power BI’s Date/time, Date or Time data
type
datetime and smalldatetime are legacy data types you
should not use anymore, but one of the other three data
types instead. datetime2, datetimeoffset and time can

have a precision of up to 7 fractions of the second (=100
nanoseconds). This is also the default if you are not
providing a precision.
Character strings: char, varchar, text, nchar, nvarchar, ntext
Can be stored in Power BI’s Text data type
text and ntext are a legacy data types and should not be
used anymore but should be replaced by varchar(max) or
nvarchar(max) respectively.
For all text columns you need to specify a size (otherwise
it defaults to 1). The maximum is 8000 bytes. When the
string size might exceed 8000 bytes use “max” as the
precision. The character string data types with a
preceding “n” (nchar and nvarchar) are stored in the
Unicode format. One character occupies two bytes; you
can only fit half of the characters per byte compared to
the ordinary character string data types.
Binary strings: binary, image, varbinary
Can be stored in Power BI’s Binary data type, but it’s
better to remove a column of this data type before loading
into Power BI, as its not supported.

Other data types: cursor, hierarchyid, sql_variant, spatial
geometry and geography types, table, rowversion,
uniqueidentifier, xml
These datatypes are not supported in Power BI. You need
to extract the necessary information into one of the
supported data types, before importing into Power BI (e.g.,
the relevant parts of the xml, or resolving the hierarchical
information from a hierarchyid column).
Best practice is, to not directly expose tables of a data
warehouse, but views instead. Simply put, views are stored
SELECT  statements, which expose the content of one or more
tables. From the outside perspective, queries against tables and
queries against views cannot be distinguished from each other.
The only disadvantage is that you cannot create foreign key
constraints on views – and therefore Power BI (or Analysis
Services Tabular) cannot create filter relationship based on this
information (but needs to apply other rules to discover them for
you, if you want so). The advantage is that the views work as an
additional layer between the physical relational data model and
Power BI. In case of changes to the physical relational data
model the views can be changed accordingly so that they are
still returning the same content. This eases the rollout of
changes, as Power BI (or any other tool which uses the

relational database as a data source) doesn’t have to change
how it accesses the information. I consider views as an API. If I
really need to implement (breaking) changes, I create a new
database schema, and create the new set of views there. This
gives all report creators a grace periode to migrate their reports
to the new schema/data model before I turn off the “old”
schema.
Alternatively, you could also expose data in the form of a stored
procedure or a table-valued function. Stored procedures are
called via the EXEC  keyword. Table-valued functions are called
as part of query (either in the FROM  or the JOIN  part). Both
can be useful in edge cases, as you can provide parameters to a
stored procedure or a function, making for maybe more
efficient code inside the procedure or function. I prefer not to
use them, because they mean that I have to write (a small piece)
of SQL as the data source in Power BI which needs to be
maintained or which may include some logic. I think I am
better off with “ordinary” views and tables to which I can
simply connect in Power BI/Power Query.
Relationships
Queries in SQL do not depend on any up-front definition of a
relationship. You must define the necessary relationship via the

JOIN  operator in each and every query. This gives you the
freedom to apply any JOIN  as you need it in the specific query
(including non-equi joins).
You can define foreign key constraints (about which we talk
later in this chapter). But they give only suggestions to the
query author and do not limit the kind of queries you can write.
With foreign key constraints given, you must still define the
(kind of joins) in each and every query.
Primary Keys
Primary keys can be explicitly defined as a constraint on a table
( PRIMARY KEY ). This constraint is a combination of a UNIQUE
constraint and a CHECK  constraint under the hood of the
database system. The UNIQUE  constraint will limit the content
of the column to only unique values (prohibiting duplicates
from being created via INSERT  or UPDATE ). And a check
constraint to dis-allow NULL  values for the column is created
on top ( NOT NULL ). This is strongly recommended, to discover
data quality issues already when the table is manipulated.
Here I included the definition of the product’s table primary
key:
ALTER TABLE [dbo] [DimProduct]

ALTER TABLE [dbo].[DimProduct]
ADD CONSTRAINT [PK_DimProduct_ProductKey] PRIMARY
(
	
[ProductKey] ASC
) ON [PRIMARY]
In a data warehouse the primary key should be a surrogate key
(read on to learn how you create a surrogate key in a SQL
database). Additionally, you should create a unique
constraint/unique index on the business key of a table. This will
speed up queries which filter on the business key (as it will be
necessary to lookup the primary key for a business key in a
dimension table during the ETL).
Surrogate Keys
In “Surrogate Keys” I already emphasized that having a single
integer surrogate key as the primary key of a table is best
practices. In Azure SQL you can define a surrogate key quite
easily by using keyword IDENTITY(1, 1)  for the primary key
column when you create a table. This lets the database
automatically maintain unique numbers for your primary key.
The two parameters in the keyword mean that the first row will
get 1 as the content and the second parameter means all the
subsequent rows will get a number which is 1 higher than the

previous one. (To be honest, I don’t see any reason to start at a
different number than 1 or to let the database intentionally
generate key numbers with gaps; so “1,1” is simply best
practice.) Due to performance optimization, it can happen that
there actually will be gaps in the numbers (e.g., that after
number 2 the next number is not 3 but 4 or 5). But this is not big
of an issue, as the surrogate key by itself is meaningless and
should therefore not be used in any filter or grouping. A signed
integer can hold values up to 2 billion – this is usually sufficient
for most of the dimension tables, even if there are gaps in the
surrogate key. If you can foresee that a table will contain more
than 2 billion rows you should create the primary key as a
BIGINT , which holds numbers up to 263 (which is over 9
quintillion, a number with 19 digits).
The following example shows, how to create a table Product
in schema PowerBI  with a PRIMARY KEY  called ProductID
of type INT  with the forementioned IDENTITY  definition:
CREATE TABLE PowerBI.Product (
   ProductID    INT IDENTITY(1, 1) PRIMARY KEY,
   Product      NVARCHAR(50),
   Subcategory  NVARCHAR(50),

   Category     NVARCHAR(50)
)
Foreign Keys
A foreign key references the primary key of another table – the
foreign key is a primary key of a different table. If you create a
foreign key constraint it will guarantee that the foreign key will
always match a row in the table containing the referenced
primary key. You will not be able to change the foreign key
column to a value, which can’t be found as the primary key in
the referenced table. And it will prohibit you from deleting a
row in the table containing the primary, which is currently
referenced as a foreign key. This is strongly recommended, to
discover data quality issues already when the tables are
manipulated.
Creating foreign key constraints is the only way to fully
guarantee that there is referential integrity between the tables.
Together with disabled nullability it will allow for inner joins
(instead of outer joins), which are faster.
Here you see how you can add a foreign key constraint to an
existing table, DimProduct  in this example. The constraint’s

name ( FK_DimProduct_DimProductSubcategory ) is
mentioned directly after key words ADD CONSTRAINT +.
After keywords +FOREIGN KEY  you need to mention the
referencing column in parathesis ( ProductSubcategoryKey ).
The referenced table and column in parathesis ( [dbo].
[DimProductSubcategory] ([ProductSubcategoryKey]) )
are following the REFERENCES  keyword.
ALTER TABLE [dbo].[DimProduct]  WITH CHECK
ADD CONSTRAINT [FK_DimProduct_DimProductSubcatego
FOREIGN KEY([ProductSubcategoryKey])
REFERENCES [dbo].[DimProductSubcategory] ([Produc
GO
Alternatively, you can create the foreign key constraint during
the CREATE TABLE  statement, as shown in the following code:
CREATE TABLE [dbo].[DimProduct](
	
[ProductKey] [int] IDENTITY(1,1) NOT NULL
	
[ProductAlternateKey] [nvarchar](25) NULL
	
[ProductSubcategoryKey] [int] NULL REFERE
,
	
...
);

NOTE
It’s best practice to use prefix FK  for foreign keys and to give them a meaningful
name. In case of a violation of a constraint during insert, update or delete, the name
of the constraints is shown. If the name already gives you a hint what the constraint
is about, its easier to find out what violated the constraint.
I strongly recommend creating the foreign key constraints in
the development environment to quickly discover data quality
issues in the ETL processes, which would violate the constraint.
In cases where I need to performance tune the database to the
last bits, I keep those constraints, but disable them in the
production environment. This will allow tools and people to
recognize the relationships, but DML operations (insert, update,
and delete) will not be slowed down, as they will ignore a
disabled constraint. Here is the code to disable and enable a
foreign key constraint:
ALTER TABLE [dbo].[DimProduct]
NOCHECK CONSTRAINT [FK_DimProduct_DimProductSubca
After you learned how to create tables and their relationships
in the table’s definition it is time to talk about, how you can
combine data which resides in different tables.

Combining Queries
No data model consists of just a single table, but several tables.
To extract the necessary informatioon from a data model you
need to combine tables in your queries. This can be either done
with Set Operatores or with Joins.
NOTE
The file used for the demonstrations is 103 SET and JOINS.sql.
Set Operators
SQL allows for all three set operators, which were all discussed
in “Set Operators”. Find examples for all the different set
operators below. All of them build upon the following two
queries:
SELECT SalesTerritoryRegion FROM dbo.DimSalesTer

Table 17-1. The 11 rows of the D
imSalesTerritory  table.
SalesTerritoryRegion
Northwest
Northeast
Central
Southwest
Southeast
Canada
France
Germany
Australia
United Kingdom
NA
SELECT DISTINCT EnglishCountryRegionName FROM dbo

Table 17-2. The 6 rows of the DimGeograp
hy  table.
EnglishCountryRegionName
Australia
Canada
France
Germany
United Kingdom
United States
For the next example it is important that you do understand
that the two queries results have both, identical rows (Australia,
Canada, France, Germany, and United Kingdom), and rows
which appear only in one of the results. The first query contains
Northwest, Northeast, Central, Southwest, Southeast, and NA
exclusively. The second query contains United States
exclusively. I put the rows which are specific for one table in
italic.
UNION

The union operator puts the two queries just underneath
each other to form one single list of items. If you want to
keep duplicates (or if you are sure that there can’t
duplicates under no circumnstances) you can save the
database from an (expensive) sort operation, which is
needed to look for duplicates by using UNION ALL
instead of UNION . The first query ( UNION ) only returns
twelve rows. The five duplicate regions (Australia,
Canada, France, Germany, and United Kingdom) only
appear once in the result. The second query ( UNION ALL )
returns 17 rows (eleven rows from the first query plus six
rows from the second, showing five rows twice).
-- UNION
SELECT SalesTerritoryRegion FROM dbo.DimSale
UNION
SELECT DISTINCT EnglishCountryRegionName FRO

Table 17-3. Twelve rows as the
result of the union’ed queries.
SalesTerritoryRegion
Australia
Canada
Central
France
Germany
NA
Northeast
Northwest
Southeast
Southwest
United Kingdom
United States

-- UNION ALL
SELECT SalesTerritoryRegion FROM dbo.DimSale
UNION ALL
SELECT DISTINCT EnglishCountryRegionName FRO

Table 17-4. Seventeen rows as
the result of the union all’ed
queries.
SalesTerritoryRegion
Northwest
Northeast
Central
Southwest
Southeast
Canada
France
Germany
Australia
United Kingdom
NA
Australia
Canada

SalesTerritoryRegion
France
Germany
United Kingdom
United States
INTERSECT
This operator looks for rows, which appear in both
queries and filters the other rows out. The rows only
appearing in the first (Northwest, Northeast, Central,
Southwest, Southeast, and NA) or in the second query
(United States) are not shown in the following example:
-- INTERSECT
SELECT SalesTerritoryRegion FROM dbo.DimSale
INTERSECT
SELECT DISTINCT EnglishCountryRegionName FRO

Table 17-5. Five rows as the
result of the intersect’ed queries.
SalesTerritoryRegion
Australia
Canada
France
Germany
United Kingdom
EXCEPT
This operator looks for rows, which appear in either pf
both queries. But it filters these rows out and only returns
rows from the first query result, which do not appear in
the result of the second query. Only the “extra” rows from
the first query are shown (Northwest, Northeast, Central,
Southwest, Southeast, and NA). The “extra” rows from the
second query are not returned.
-- EXCEPT
SELECT SalesTerritoryRegion FROM dbo.DimSale
EXCEPT
SELECT DISTINCT EnglishCountryRegionName FRO

Table 17-6. Five rows as the
result of the intersect’ed queries.
SalesTerritoryRegion
Central
NA
Northeast
Northwest
Southeast
Southwest
Joins
All joins discussed in “Joins” can be implemented in SQL. Again,
I built all examples on the same two tables I used to
demonstrate the set operators.
INNER JOIN
For an inner join you need to specify a join predicate (a
Boolean condition to specify if two rows are related or
not). In the following example, only rows with regions

( EnglishCountryRegionName  respectively +
SalesTerritoryRegion+) available in both queries are
shown in an INNER JOIN  (Australia, Canada, France,
Germany, United Kingdom, and United States). The
INNER  keyword is optional optional and does not change
the behavior of the join operation. If you specify INNER
JOIN  or just JOIN  – you will get the exact same result.
-- INNER JOIN
SELECT DISTINCT dst.SalesTerritoryRegion, dg
FROM dbo.DimSalesTerritory dst
INNER JOIN dbo.DimGeography dg ON dg.English
Table 17-7. Five rows as the result of the inner join’ed queries.
SalesTerritoryRegion
EnglishCountryRegionName
Australia
Australia
Canada
Canada
France
France
Germany
Germany
United Kingdom
United Kingdom

OUTER JOIN
Is available as LEFT , RIGHT  and FULL OUTER JOIN .
The OUTER  keyword is optional and does not change the
behavior of the join operation. If you specify LEFT OUTER
JOIN  or only LEFT JOIN  it will work and return the
exact same result in both cases.
The left outer join returns all rows of the first query. For
regions, not available in the second query, the
EnglishCountryRegionName  is NULL .
-- LEFT OUTER JOIN
SELECT DISTINCT dst.SalesTerritoryRegion, dg
FROM dbo.DimSalesTerritory dst
LEFT OUTER JOIN dbo.DimGeography dg ON dg.En

Table 17-8. Eleven rows as the result of the left outer join’ed queries.
SalesTerritoryRegion
EnglishCountryRegionName
Australia
Australia
Canada
Canada
Central
NULL
France
France
Germany
Germany
NA
NULL
Northeast
NULL
Northwest
NULL
Southeast
NULL
Southwest
NULL
United Kingdom
United Kingdom
The right outer join returns all rows of the second query.
For regions not available in the first query, the
SalesTerritoryRegion  is NULL.

-- RIGHT OUTER JOIN
SELECT DISTINCT dst.SalesTerritoryRegion, dg
FROM dbo.DimSalesTerritory dst
RIGHT OUTER JOIN dbo.DimGeography dg ON dg.E
Table 17-9. Six rows as the result of the right outer join’ed queries.
SalesTerritoryRegion
EnglishCountryRegionName
NULL
United States
Australia
Australia
Canada
Canada
France
France
Germany
Germany
United Kingdom
United Kingdom
The full outer join returns all rows of both queries, no
matter if there is a matching row in the other table. For
regions not available in the other query, you will get a
NULL.

-- FULL OUTER JOIN
SELECT DISTINCT dst.SalesTerritoryRegion, dg
FROM dbo.DimSalesTerritory dst
FULL OUTER JOIN dbo.DimGeography dg ON dg.En

Table 17-10. Twelve rows as the result of the full outer join’ed queries.
SalesTerritoryRegion
EnglishCountryRegionName
NULL
United States
Australia
Australia
Canada
Canada
Central
NULL
France
France
Germany
Germany
NA
NULL
Northeast
NULL
Northwest
NULL
Southeast
NULL
Southwest
NULL
United Kingdom
United Kingdom
Anti-join

There is no keyword for an anti-join in SQL. You must
implement it by combining an OUTER JOIN  with a
WHERE  clause, which filters only those rows where the
join key on the outer rows is null. As this is an outer join,
you can write it as left, right or full anti-join. The left anti-
join returns rows, only appearing in the first query
(Northwest, Northeast, Central, Southwest, Southeast, and
NA), which means that the EnglishCountryRegionName
is NULL :
-- LEFT ANTI-JOIN
SELECT DISTINCT dst.SalesTerritoryRegion, dg
FROM dbo.DimSalesTerritory dst
LEFT JOIN dbo.DimGeography dg ON dg.EnglishC
WHERE dg.EnglishCountryRegionName IS NULL

Table 17-11. Six rows as the result of the left anti-join’ed queries.
SalesTerritoryRegion
EnglishCountryRegionName
Central
NULL
NA
NULL
Northeast
NULL
Northwest
NULL
Southeast
NULL
Southwest
NULL
The right anti-join returns rows, only appearing in the
second query (United States), showing NULL for the
SalesTerritoryRegion :
-- RIGHT ANTI-JOIN
SELECT DISTINCT dst.SalesTerritoryRegion, dg
FROM dbo.DimSalesTerritory dst
RIGHT JOIN dbo.DimGeography dg ON dg.English
WHERE dst.SalesTerritoryRegion IS NULL

Table 17-12. One row as the result of the right anti-join’ed queries.
SalesTerritoryRegion
EnglishCountryRegionName
NULL
United States
The full anti-join returns rows, only appearing exclusively
in either the first or second query (Australia, Canada,
France, Germany, and United Kingdom vs. United States).
Every row shows NULL in either
SalesTerritoryRegion  or
EnglishCountryRegionName .
-- FULL ANTI-JOIN
SELECT DISTINCT dst.SalesTerritoryRegion, dg
FROM dbo.DimSalesTerritory dst
FULL JOIN dbo.DimGeography dg ON dg.EnglishC
WHERE dst.SalesTerritoryRegion IS NULL or dg

Table 17-13. Seven rows as the result of the full anti-join’ed queries.
SalesTerritoryRegion
EnglishCountryRegionName
NULL
United States
Central
NULL
NA
NULL
Northeast
NULL
Northwest
NULL
Southeast
NULL
Southwest
NULL
CROSS JOIN
A cross join returns the so-called cartesian product, a
combination of all rows of the first query with all rows
from the second query. This long list has only certain,
narrow use cases:
-- CROSS JOIN
SELECT DISTINCT dst.SalesTerritoryRegion, dg
FROM dbo.DimSalesTerritory dst
CROSS JOIN dbo.DimGeography dg

Table 17-14. Some of the 66 rows as the result of the cross join’ed queries.
SalesTerritoryRegion
EnglishCountryRegionName
Australia
Australia
Australia
Canada
Australia
France
Australia
Germany
Australia
United Kingdom
Australia
United States
…​
…​
United Kingdom
Australia
United Kingdom
Canada
United Kingdom
France
United Kingdom
Germany
United Kingdom
United Kingdom
United Kingdom
United States

Self join
The following example is not based on the two region
tables, but on the employee table. It contains a primary
key ( EmployeeKey ) and a foreign key
( ParentEmployeeKey ), which refers the primary key of
the same table (EmployeeKey). As discussed in “Joins”, this
allows for implementing a hierarchical structure (like an
organigram). The same table ( DimEmployee ) appears
twice in a query to implement a self join.
-- SELF JOIN
SELECT employee.EmployeeKey, employee.Parent
FROM dbo.DimEmployee employee
JOIN dbo.DimEmployee parent on parent.Employ

Table 17-15. Some of the 66 rows as the result of the cross join’ed queries.
EmployeeKey
ParentEmployeeKey
FirstName
Em
4
3
Rob
3
5
3
Rob
3
11
3
Gail
3
13
3
Jossef
3
162
3
Dylan
3
267
3
Ovidiu
3
271
3
Michael
3
274
3
Sharon
3
…​
…​
…​
…​
295
290
Rachel
290
291
290
Jae
290
292
290
Ranjit
290
296
294
Lynn
294

Equi join
Commonly you will write most of the joins as equi joins:
You want to find rows where the primary key of one table
matches the foreign key of another table. All of the
previous examples demonstrating the different kinds of
the JOIN keyword were equi joins, as the condition after
the ON  keyword contained solely the equal sign (“=”).
That’s why I don’t list an example here.
Non-equi join
Only in rare use-cases you will write non-equi joins:
Finding rows matching a range of values, for example.
The following example only shows combinations of rows,
where the region names are not the same. Basically, it
looks like the full join, but with the matching rows (were
both SalesTerritoryRegion  and
EnglishCountryRegionName  have the same content)
not listed. This is more of an educational example. Later,
when we talk about binning (in “Binning”), you will see a
more relevant example of an non-equi join.
SELECT DISTINCT dst.SalesTerritoryRegion, dg
FROM dbo.DimSalesTerritory dst
JOIN dbo.DimGeography dg ON dg.EnglishCountr

Table 17-16. Some of the 61 rows as the result of a non-equi join’ed queries.
SalesTerritoryRegion
EnglishCountryRegionName
Australia
Canada
Australia
France
Australia
Germany
Australia
United Kingdom
Australia
United States
…​
…​
United Kingdom
Australia
United Kingdom
Canada
United Kingdom
France
United Kingdom
Germany
United Kingdom
United States
Natural joins
This type of joins are not supported in Microsoft’s SQL
dialect. That means, you must always specify a join

predicate (= the columns you want to be matched during
the join).
Join Path Problems
When you join a row of one table to a row of another table you
can face several problems, resulting in unwanted query results.
The possible problems are:
Loop
Between the table FactResellerSales  and the table
DimDate  exists more than one foreign key constraint.
Some users (and unfortunately many reporting tools) will
therefore create a combined join predicate, asking for
only rows where both the FactResellerSales ’
OrderDateKey  and the ShipDateKey  are equal to the
DimDate ’s DateKey . This leads to an empty query, as in
the example there was no sale, which was ordered and
shipped on the exact same day.
SELECT
	
SUM(SalesAmount) SalesAmount
FROM
	
dbo.FactResellerSales frs
JOIN dbo.DimDate dd ON dd.DateKey = frs.Orde

Table 17-17. This
simple loop leads to
an empty query
result.
SalesAmount
NULL
In SQL you need to explicitly solve this problem by
rewriting this query as follows. You need to join the
DimDate  table twice. Once with a join predicated on the
FactResellerSales ’ OrderDateKey , once more with a
join predicated on the the FactResellerSales ’
ShipDateKey. In this case specifying an alias (like od  or
sd  as in the example) is mandatory, to distinguish the
two references to the same table ( DimDate ). This is also
called a role-playing dimension, as DimDate  plays the
role of the order date dimension in one join, and the role
of the ship data dimension in the other:
SELECT
	
SUM(SalesAmount) SalesAmount
FROM
	
dbo.FactResellerSales frs
JOIN dbo.DimDate od ON od.DateKey = frs.Orde
JOIN dbo.DimDate sd ON sd.DateKey = frs.Ship

Table 17-18. This
simple loop leads to an
empty query result.
SalesAmount
80,450,596.9823
Chasm trap
A join between FactResellerSales  and DimDate
leads to duplicate rows of the DimDate  table, as there is
more than one row in FactResellerSales  per row in
DimDate  (remember that the relationship between
DimDate  to FactResellerSales  has a one-to-many
cardinality). This is expected and intended. The exact
same thing happens if you join FactInternetSales
with DimDate .
But when you join all three tables ( FactResellerSales ,
DimDate , and FactInternetSales ) in the same query,
you will get the rows of FactInternetSales
+duplicated (per duplicate row in +DimDate ).
But also into the other direction, as the relationship
between DimDate  and FactInternetSales  is, again, a
one-to-many relationship. The result is a very inflated

sum of SalesAmount (which results from the true sales
amount multiplied by the number of rows in the “other”
table). Let’s first query the two fact tables separatedly:
SELECT
	
dd.DateKey, SUM(frs.SalesAmount) Res
FROM
	
dbo.FactResellerSales frs
JOIN dbo.DimDate dd ON dd.DateKey = frs.Orde
WHERE dd.DateKey=20110101
GROUP BY dd.DateKey
Table 17-19. For January 1 2011 there are 786 reseller sales worth 1,538,508.31
DateKey
ResellerSalesAmount
ResellerSalesCou
20110101
1,538,408.3122
785
SELECT
	
dd.DateKey, SUM(fis.SalesAmount) Int
FROM
	
dbo.FactInternetSales fis
JOIN dbo.DimDate dd ON dd.DateKey = fis.Orde
WHERE dd.DateKey=20110101
GROUP BY dd.DateKey

Table 17-20. For January 1 2011 there are 2 internet sales worth 7,156.54
DateKey
ResellerSalesAmount
ResellerSalesCou
20110101
7,156.54
2
So far so good. If you simply combine the two queries and
join all three tables in one single query, the results are
inflated:
SELECT
	
dd.DateKey, SUM(frs.SalesAmount) Res
FROM
	
dbo.FactResellerSales frs
JOIN dbo.DimDate dd ON dd.DateKey = frs.Orde
JOIN dbo.FactInternetSales fis ON fis.OrderD
WHERE dd.DateKey=20110101
GROUP BY dd.DateKey
Table 17-21. A chasm trap inflates the true values.
DateKey
ResellerSalesAmount
InternetSalesAm
20110101
3,076,816.6244
5,617,883.90

Both the ResellerSalesAmount  and
InternetSalesAmount  in the last query are higher than
in the first two queries – and the results in the last query
are wrong. The amount of 3,076,816.6244 for the
ResellerSalesAmount  is double the true amount, as the
true amount was multiplied by the number of rows in the
FactInternetSales  table. The shown amount of
5,617,883.90 results from the true amount of 7,156.54
multiplied by 785 (the number of rows from the
FactResellerSales  table).
To overcome this problem, you need to make sure that
you split the necessary joins into separate queries. Later
you can then either UNION  the two queries (to two rows
per date, one for the FactResellerSales  and one for
the FactInternetSales ) or JOIN  the two query results
(on the now unique DateKey ). The first piece of code
shows the solution with UNION ALL :
SELECT
	
DateKey, SUM(SalesAmount) SalesAmoun
FROM
(
	
SELECT
	
	
dd.DateKey, SUM(frs.SalesAmo

	
FROM
	
	
dbo.FactResellerSales frs
	
JOIN dbo.DimDate dd ON dd.DateKey = 
	
WHERE dd.DateKey=20110101
	
GROUP BY dd.DateKey

	
UNION ALL

	
SELECT
	
	
dd.DateKey, SUM(fis.SalesAmo
	
FROM
	
	
dbo.FactInternetSales fis
	
JOIN dbo.DimDate dd ON dd.DateKey = 
	
WHERE dd.DateKey=20110101
	
GROUP BY dd.DateKey
) x
GROUP BY DateKey
Table 17-22. Union’ing the two separate queries overcomes
the problem.
DateKey
SalesAmount
SalesCount
20110101
1,545,564.8522
787
Alternatively, you can just join the two queries:

SELECT
	
frs.DateKey, frs.ResellerSalesAmount
FROM
(
	
SELECT
	
	
dd.DateKey, SUM(frs.SalesAmo
	
FROM
	
	
dbo.FactResellerSales frs
	
JOIN dbo.DimDate dd ON dd.DateKey = 
	
WHERE dd.DateKey=20110101
	
GROUP BY dd.DateKey
) frs
JOIN
(
	
SELECT
	
	
dd.DateKey, SUM(fis.SalesAmo
	
FROM
	
	
dbo.FactInternetSales fis
	
JOIN dbo.DimDate dd ON dd.DateKey = 
	
WHERE dd.DateKey=20110101
	
GROUP BY dd.DateKey
) irs ON irs.DateKey = frs.DateKey

Table 17-23. Joining the two separate queries overcomes the
problem.
DateKey
SalesAmount
SalesCount
20110101
1,545,564.8522
787
In my demo system the execution plans for both versions
are slightly different, but the from a query cost
perspective they are identical.
Fan trap
In the following query the freight cost (stored in in table
SalesOrderHeader ) “fans-out” to every row of the
SalesOrderDetail  table. Adding the order’s freight up
per order line, does lead to a wrong (= too high) amount of
freight (as it is the freight times the number of rows in the
SalesOrderDetail  table for that order). Let’s look at the
true values first, for order with SalesOrderID  of 43659:
SELECT
	
soh.SalesOrderID, SUM(soh.Freight) F
FROM
	
Sales.SalesOrderHeader soh
WHERE soh.SalesOrderID = 43659
GROUP BY soh.SalesOrderID

Table 17-24. The total freight costs of an order.
SalesOrderID
Freight
HeaderCount
43659
616,0984
1
SELECT
	
soh.SalesOrderID, SUM(sod.OrderQty) 
FROM
	
Sales.SalesOrderHeader soh
JOIN Sales.SalesOrderDetail sod ON sod.Sales
WHERE soh.SalesOrderID = 43659
GROUP BY soh.SalesOrderID
Table 17-25. The order has 12 line items.
SalesOrderID
OrderQty
DetailCount
43659
26
12
When I combine the two queries naively, I am bitten by
the fan trap, and the true freight costs of 616.0984 for the
particular order is multiplied by the number of order
detail rows (12) and a wrong freight amount of 7,393.1808
is returned by the query:

SELECT
	
soh.SalesOrderID, SUM(soh.Freight) F
FROM
	
Sales.SalesOrderHeader soh
JOIN Sales.SalesOrderDetail sod ON sod.Sales
WHERE soh.SalesOrderID = 43659
GROUP BY soh.SalesOrderID
Table 17-26. The order header rows are “fanned out” over
the order detail rows.
SalesOrderID
Freight
OrderQty
43659
7,393.1808
26
Again, you need to split the single query into two, to avoid
this effect and get the correct value instead. The first
query calculates the freight (without the problematic join
to the SalesOrderDetail  table). The second query
calculates only the OrderQty  by joining the two tables:
SELECT
	
soh.SalesOrderID, SUM(soh.Freight) F
FROM
	
Sales.SalesOrderHeader soh
JOIN (
SELECT

	
soh.SalesOrderID, SUM(sod.OrderQty) 
FROM
	
Sales.SalesOrderHeader soh
JOIN Sales.SalesOrderDetail sod ON sod.Sales
GROUP BY soh.SalesOrderID
) sod ON sod.SalesOrderID = soh.SalesOrderID
WHERE soh.SalesOrderID = 43659
GROUP BY soh.SalesOrderID
Table 17-27. The separated queries are joined into one.
SalesOrderID
Freight
OrderQty
43659
7,393.1808
26
Before you start writing a query you should always look out for
join-path problems. One way to find out is to look at the entity-
relationship diagram. In the next section I will show you, how
you can create one in SQL Server Management Studio.
Entity-Relationship Diagrams (ERD)
SQL Server Management Studio lets you create an ERD for your
existing tables (and foreign key constraints). If you expand the
database name in the Object explorer, you will find Database
Diagrams. There you can create a new diagram by adding and

arranging tables or view an existing one. In Figure 17-1 you see
an example I created for the AdventureWorksDW database. It
illustrates the many-to-one relationships between the
FactResellerSales table and the date, sales territory, and product
dimensions. You can clearly see that this is a snowflake schema,
as the DimProduct table has a relationship to
DimProductSubcategory, which has a relationship to
DimProductCategory.
Figure 17-1. A database diagram in SQL Server Management Studio
You can save as many database diagrams as you want. Just hit
File - Save and give it a name.
Extract, Transform, Load

NOTE
The file used for the demonstrations is 104 ETL.sql.
If the amount of data loaded into a data warehouse is small
enough (in relation to the available hardware resources), a
regular full load might be sufficient to complete in the
maintenance window: First you truncate the table. Then you
insert all the rows from the data source, as in the following
example:
-- Full Load
TRUNCATE TABLE demo.Sales;
INSERT INTO demo.Sales (ProductID, SalesAmount)
SELECT
	
ISNULL(p.ID, -1) ProductID,
	
ps.SalesAmount
FROM
	
demo.ProductSales ps
LEFT JOIN demo.Product p ON p.Product=ps.Product

TIP
The outcome of TRUNCATE TABLE demo.Sales  is identical to DELETE FROM
demo.Sales . There is a difference under the hood though: TRUNCATE TABLE  is
considered a meta data operation. This operation will not touch a single row of the
table, but just change the information for the whole table to inform everybody that
the table is empty. As you can imagine, this operation is much faster compared to
actually deleting every row in a table (as the DELETE  keyword does). That’s why I
prefer TRUNCATE TABLE  over DELETE .
There are two things you need to consider though: First, TRUNCATE TABLE  is only
possible, if the table is not part of any foreign key constraint. If so, you need first to
DROP  the constraint, TRUNCATE  the table and then CREATE  the constraint again.
Second, TRUNCATE TABLE  does not accept any filter conditions as DELETE  does via
the WHERE  clause. You can only empty a full table (or partition, to be precise;
“Partitioning” will talk about partitions).
The bigger the amount of data gets (as the data grows in size) or
the smaller the maintenance windows become (as there are
more and more processes running in the maintenance window
or because you need to load the data more often, for example
multiple times a day), the more critical it is to update your data
warehouse more quickly. That’s the time where you should
think about changing your ETL from a full to a delta load. A
delta load is more complex in terms of that it needs to identify
the rows which are …​

new in the data source and therefore needs to be inserted
into the data warehouse.
changed since the last load and therefore needs to be
updated in the data warehouse.
deleted in the data source and therefore needs to be either
removed from the data warehouse (hard delete) or marked
as deleted (soft delete) in the data warehouse.
The following example inserts only rows for new products
(products which didn’t have a row in table Sales  yet). The
WHERE  clause filters to rows which NOT EXISTS  in the data
warehouse:
-- Delta Load
INSERT INTO demo.Sales (ProductID, SalesAmount)
SELECT
	
ISNULL(p.ID, -1) ProductID,
	
ps.SalesAmount
FROM
	
demo.ProductSales ps
LEFT JOIN demo.Product p ON p.Product=ps.Product
WHERE NOT EXISTS (SELECT TOP 1 1 FROM demo.Sales 
In case already loaded data can change as well, you need to add
an UPDATE  statement. In case already loaded data can be

deleted in the data source you need to a DELETE  statement (for
hard delete) or UPDATE  statement (to implement soft delete) as
well. More often than not, the business users want to keep track
of such changes: when a row was inserted, updated or modified
and how did the row look like before the modification. In
“Slowly Changing Dimensions” I show you how you can add
such information by implementing Slowly Changing
Dimensions.
Keep in mind that a delta load will only be faster, if the physical
organization (index strategy, table partitioning) of the tables in
the data source and in the data warehouse support the JOIN
predicates and WHERE  conditions. If not, the delta load might
take as long as a full load (or even longer due to added filter
conditions). In Chapter 20 you will learn which indexes you
should create and how you can manage partitions in a
relational table.
If you want to further refresh your data model in Power BI or
Analysis Services more quickly, you need to create partitions
there and in the data warehouse. Usually you partition your
tables timewise (e.g., one partition per day). A partition stores
rows of a table together. In a relational database you can
quickly truncate the content of a partition before you reload the
data for it. In Power BI and Analysis Services you can specify to

refresh only a partition instead of the whole table. This will
speed up the refresh time tremendously, if you do not refresh
all the (certainly unchanged) old data from years ago over and
over again. It is important that the partitions are aligned
between the relational database and Power BI/Analysis
Services, to get the best performance.
With the section about ETL the first chapter about SQL finished.
Key Takeaways
In this chapter I introduced the moving parts of a relational
database, which is queried and maintained via SQL.
You learned about the basic parts of a relational database:
tables, columns, relationships, primary keys, and foreign
keys.
It is recommended to create a dedicated schema for all
object you want to expose to Power BI, Analysis Services (or
any other reporting layer) to which you give read-only
access to all tools and people who need access.
The content of the data warehouse can be exposed as
tables, views, stored procedures, or functions. Views are the
most common way, and a good starting point. In case of

performance issues when querying the content, the
information can be persisted as a table. Implementing the
queries as stored procedures or as functions makes it less
user friendly for the data consumers, but allows for
parameters and optimized T-SQL code, where necessary.
There are tools to visualize the tables and their foreign key
constraints as an entity-relationship diagram. Foreign key
constraints restrict manipulations of the content of the
tables ( INSERT , UPDATE , and DELETE ), but have no direct
influence on SELECT .
You must define the kind of relationship in each and every
query ( JOIN  operator in SELECT  statement) – they are not
automatically added behind the scenes for you. On the one
hand, this gives you full control and flexibility about how
you want to combine the content of tables. But be aware of
traps in your data model, which might lead to too few or
too many rows in the query result (or too low or too high
numbers in aggregated values).
In SQL Server Management Studio you can easily create an
Entity-relationship diagram and get a quick overview about
tables and their relationship, which will allow you to
discover potential join path problems.
If you need keep track of changes to dimensions, you can
implement Slowly Changing Dimensions in a relational data

warehouse during the ETL.
Now after you have met your new best friends in the world of
SQL, it’s about time to learn what these friends can do you for
you in terms of building a data model that will help in your
adventures in Power BI.

Chapter 18. Building a Data Model
with SQL
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 18th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
SQL as a language is quite mature and - if you master this
language – you can make transformations of your tables very
easy. Similar to some applied steps in Power Query, dynamic
SQL can make our life even easier as it can make your code
resilient to changes. Instead of manually maintaining code, you

can write code to maintain the solution for you in an
(semi-)automatic way.
After many years working with analytic solutions, I still believe
that a data warehouse is the power house in a business
intelligence architecture. Putting transformations in a
relational layer (which does not necessarily persist all the
content) opens your solution to many tools. Of course, Power BI
(via Power Query) can consume data from a relational
database. But also, plenty of other tools (and users) will be able
to connect to a relational database. In a scenario, where you or
your colleagues have - God forbid! - BI and reporting tools other
than Power BI in place, a relational data warehouse (layer) can
be the common ground and important puzzle piece to achieve
the single-truth. Instead of re-implementing transformations in
all those tools, you can do them in the data warehouse.
All solutions around SQL can be done different ways:
Persisting the content into a table. This is practical, if tools
or users query content of the data warehouse regularly
(e.g., if you use Power BI Report Builder on top of the
relational database, instead of a Power BI semantic model
or if you run Power BI in DirectQuery mode). You can create

indexes and partitions on this table to speed up the queries
even more.
Persisting a query only, as a view. This is practical if the sole
purpose of the relational database is to provide all
transformations, but the data is cached in a different place
anyways (e.g., if you load the data into Power BI and run it
in Import mode). If the data is only read once per refresh,
persisting the data usually does not pay off.
Many relational database management systems have a
feature called indexed view or materialized view. Through
this, the result of a view is persisted, and offers similar
performance like a indexed table. The advantage though is
that you do not need setup schedules to refresh the content
of the indexed view - the database management system will
make sure to keep it updated, whenever the underlying
table(s) are maniupulated. The disadvantage is that it will
slow down maniuplation of the underlying tables (due to
this extra steps of updating the indexed view).
Create functions and/or stored procedures. You can pass
parameters into functions and stored procedures, which
can make handling them easier for users (and you can hide
complex logic, which reacts in different ways to the
parameters, from them).

TIP
I strongly recommend, to not giving the report creators access to the data warehouse
tables, but on views (and maybe functions or stored procedures) only. Put all those
artifacts into dedicated database schema and give read access to this schema only
(and no other schema). In Chapter 20 I will discuss the different options from a
performance perspective.
Normalizing
NOTE
The file used for the demonstrations is 201 Normalizing.sql.
Normalizing in SQL means, to just list the columns you want in
the projection (= to write them just right after the SELECT
keyword). The question is again, how to decide on which
columns you want to keep in a result set. In SQL, you can find
dimension candidates by using COUNT  and GROUP BY . By
counting the number of rows per distinct value of a column you
can easily discover redundant information. The goal of
normalizing is to move redundant information into a different
table, and only keep a foreign key to reference the row in the
(then) different table.

The table demo.financials  in my example contains several
columns: Segment , Country , Product , Discount Band ,
Units Sold , Manufacturing Price , Sale Price , Gross
Sales , Discounts , Sales , COGS , Profit , Date , Month
Number , Month Name , Year . A quick glance will identify
Units Sold , Sale Price , Gross Sales , Discounts ,
Sales , COGS , Profit  as the facts: they are numeric and
aggregable (except for the Sale Price ). The other columns
(including all date related columns like month and year) are
candidates for normalization into their own table.
From the following query you learn that there are 700 rows in
total in the table, but only five different segments:
SELECT COUNT(*) CountAll, COUNT(DISTINCT Segment
CountAll
CountSegment
700
5
The Segment  is a redundant column in the financials  table.
Therefore, I remove it from financials  and create a separate
Segment  table:
CREATE TABLE normalization Segment (

CREATE TABLE normalization.Segment (
	
SegmentID int IDENTITY(1,1) PRIMARY KEY,
	
Segment nvarchar(50)
)
Column SegementID  is the PRIMARY KEY  of this new table.
The content for each row of this column is automatically
determined by the database ( IDENTITY(1,1) ). The only “real”
content (which will later be exposed to the report creators) is
column Segment .
During the INSERT  statement you need to make sure to specify
the DISTINCT  keyword (so each unique Segment  is only
inserted once). A mistake here would later duplicate the rows in
the fact table.
INSERT INTO normalization.Segment
SELECT
	
DISTINCT
	
Segment
FROM
	
demo.financials;
Additionally, I strongly recommend to insert a placeholder
value, in case a fact row contains an invalid segment or when
the segment should contain NULL . It’s best practice to have row

with value -1 for the ID  column in a dimension table (which
will be later referred when the rows are inserted into the fact
table):
SET IDENTITY_INSERT normalization.Segment ON;
INSERT INTO normalization.Segment (SegmentID, Seg
SET IDENTITY_INSERT normalization.Segment OFF;
Before you can directly set the value for an identity column you
need to set the IDENTITY_INSERT  property of the table to ON .
Don’t forget to set it to OFF  right afterwards, to avoid the
identity column is used in other INSERT  or UPDATE
statements by mistake. The provided value for the Segment
column (“unknown” in the example above) should be
something that is meaningful to report consumers. I will talk
about your options again in “Flags and Indicators”.
When you count the distinct values of the Product  and the
[Manufacturing Price]  column you will discover that they
have the same count:
SELECT COUNT(*) CountAll, COUNT(DISTINCT Product

CountAll
CountProduct
CountManufactoringPrice
700
6
6
As it turns out, this is not by chance, but because there is a
transitive dependency between the two columns: There is
exactly one single [Manufactoring Price]  per Product :
SELECT
	
DISTINCT
	
Product,
	
[Manufacturing Price]
FROM
	
demo.financials;

Product
Manufactoring Price
Amarilla
260.00
Carretera
3.00
Montana
5.00
Paseo
10.00
Velo
120.00
VTT
250.00
That’s why I will put them together into the same dimension
table.
You need to repeat these steps for all dimensional attributes of
table financials :
Segment ( Segment )
Country ( Country )
Product ( Product , [Manufactoring Price] )
Discount Band ( [Discount Band]
The date-related columns are special. They are definitely a
candidate for a dimension table, but you cannot derive its

content from table financials . The reason is that Power BI
will need a table which contains a row for every day of a
calendar year. The rows in table financials  violate this
condition, as you can see here:
SELECT
    COUNT(*) CountAll,
    MIN([Date]) MinDate,
    MAX([Date]) MaxDate,
    COUNT(DISTINCT [Date]) CountDay
FROM
    demo.financials;
CountAll
MinDate
MaxDate
CountDay
700
2013-09-01
2014-12-01
16
The range of dates is between September 1 2013 and December
1 2014, and does therefore not cover full calendar years. On top
of that it only contains 16 different dates. This is a clear
violation of the rules for a proper date table (at least when it
comes to Power BI). This is not very untypical for a fact table, as
transactions will not happen every day or because the fact table
is already aggregated (like in this case, where the facts are
aggregated to the first of the month). That’s why you should

never derive the content for the date dimension from the fact
able. In “Time and Date” I will show you how you can create a
date table properly.
After every dimension has its own table, I can finally normalize
the fact table (in a schema named normaliziation ). This
table will only contain numerical columns: Integers for the
foreign keys and decimals for the facts:
CREATE TABLE normalization.financials (
	
SegmentID int NOT NULL
	
	
CONSTRAINT FK_financials_Segment
	
	
REFERENCES normalization.Segment
	
CountryID int NOT NULL
	
	
CONSTRAINT FK_financials_Country
	
	
REFERENCES normalization.Country
	
ProductID int NOT NULL
	
	
CONSTRAINT FK_financials_Product
	
	
REFERENCES normalization.Product
	
DiscountBandID int NOT NULL
	
	
CONSTRAINT FK_financials_Discount
	
	
REFERENCES normalization.[Discoun
	
DateKey int,
	
[Units Sold] decimal(18,2),
	
[Sales Price] decimal(18,2),
	
[Gross Sales] decimal(18,2),
	
[Discounts] decimal(18,2),

	
[Sales] decimal(18,2),
	
[COGS] decimal(18,2),
	
[Profit] decimal(18,2)
)
I do not allow NULL  as a value for a foreign key. To tell the
database system that these columns are indeed foreign key
columns, I added the CONSTRAINT  keyword to give the
constraint a proper name (which will make it easier to drop or
disable the constraint later, if needed) and specify the
dimension table and its primary key column after
REFERENCES . As I have not created the Date  table, I omit the
foreign key constraint here.
I then insert all rows from financials  into the newly created
table:
Example 18-1.
SELECT
	
ISNULL(s.SegmentID, -1)	as SegmentID,
	
ISNULL(c.CountryID, -1)	as CountryID,
	
ISNULL(p.ProductID, -1)	as ProductID,
	
ISNULL(d.DiscountBandID, -1)	
as Discou
	
YEAR(f.[Date]) * 10000 + MONTH(f.[Date]) 
        as DateKey,

y,
	
f.[Units Sold],
	
f.[Sale Price],
	
f.[Gross Sales],
	
f.[Discounts],
	
f.[Sales],
	
f.[COGS],
	
f.[Profit]
FROM
	
demo.financials f
LEFT JOIN normalization.Segment s ON
    s.Segment = f.Segment
LEFT JOIN normalization.Country c ON
    c.Country = f.Country
LEFT JOIN normalization.Product p ON
    p.Product= f.Product
LEFT JOIN normalization.[Discount Band] d ON
    d.[Discount Band]= f.[Discount Band]
For the values of the foreign key columns, I added a safe guard
in cases where the fact table contains a business key (e.g.,
Segment ) which is not (yet) inserted into the dimension table.
If you run the queries in the exact order as discussed here (first
the insert statements for the dimension, and in the end the
insert stamen for the fact table) then this will not be necessary. I
tend to add this logic anyways, as I want to be better safe, then

sorry, for cases where the order was mixed up for some reason.
The safeguard is two-folded:
I use intentionally a LEFT JOIN  (instead of an inner join).
I replace NULL  values (via function ISNULL ) with a value
of -1.
Without this safeguard, either rows from the fact table could be
filtered out (by an inner join) or a value of NULL  would be
inserted into the foreign key columns. Both is bad: If you
unintentionally filter out rows from the fact table, the content
of your data warehouse would be wrong. If you add fact rows
with NULL  as their foreign key, every report would need to use
(slower) LEFT JOIN  than (faster) INNER JOIN . I prefer the
latter. The previously inserted extra row in the dimension
tables (with -1 as the primary key) and the safe guard here will
allow to always do INNER JOIN  between the tables in the data
warehouse. And even missing or wrong dimensions do not lead
to empty descriptions shown in the report, but meaningful texts
(e.g., “unknown”).
The value for the DateKey  foreign key column is derived from
the fact’s actual date: The date’s year is multiplied by 10,000
(e.g., 20230000), then the month number is multiplied by 100
and added (e.g., 202308) and finally the day number of the

month is added as well (20230801). Against the rule that a
surrogate key in a data warehouse is always meaningless, it is
best practice to allow the date’s key an exception. This key is
still readable. Nevertheless, in Power BI you should never use
the fact’s foreign key to filter the rows, but always join the date
dimension and put filters there, as I explained in “Normalizing
and Denormalizing”.
I would not recommend directly using the query Example 18-1
when connecting Power BI to the data source but provide the
result in the database. As mentioned in “Normalizing”, you
have several options to do so:
Persisting into a table
For the dimension tables there is no way around, if you
want to use the IDENTITY  feature. This is only available
for rows physically persisted in a table. As a first step, you
need to drop all foreign key constraints, referencing the
table. Then you drop the table if it does already exists.
Finally, you (re-)create the table (in the right schema) and
insert the rows. Don’t forget to insert the extra row with
primary key -1. For the Segment  table this looks like this:
-- Segment
IF EXISTS (


    SELECT TOP 1 1
    FROM sys.objects
    WHERE OBJECT_NAME(object_id) = 'FK_finan
    )
    ALTER TABLE normalization.financials
DROP CONSTRAINT FK_financials_Segment;

DROP TABLE IF EXISTS normalization.Segment;

CREATE TABLE normalization.Segment (
	
SegmentID int IDENTITY(1,1) PRIMARY 
	
Segment nvarchar(50)
)
GO

INSERT INTO normalization.Segment
SELECT
	
DISTINCT
	
Segment
FROM
	
demo.financials;

SET IDENTITY_INSERT normalization.Segment ON
INSERT INTO normalization.Segment (SegmentID
VALUES (-1, 'unknown');
SET IDENTITY_INSERT normalization.Segment OF

For the fact table(s) the steps are simpler: You only need to
drop the table (if it exists) and then insert all the rows. No
need to take extra care of constraints or the extra row (-1):
-- Financials
DROP TABLE IF EXISTS normalization.Financial
CREATE TABLE normalization.Financials (
	
SegmentID int NOT NULL
	
	
CONSTRAINT FK_Financials_Seg
	
	
REFERENCES normalization.Seg
	
CountryID int NOT NULL
	
	
CONSTRAINT FK_Financials_Cou
	
	
REFERENCES normalization.Cou
	
ProductID int NOT NULL
	
	
CONSTRAINT FK_Financials_Pro
	
	
REFERENCES normalization.Pro
	
DiscountBandID int NOT NULL
	
	
CONSTRAINT FK_Financials_Dis
	
	
REFERENCES normalization.[Di
            (DiscountBandID),
	
DateKey int,
	
[Units Sold] decimal(18,2),
	
[Sales Price] decimal(18,2),
	
[Gross Sales] decimal(18,2),
	
[Discounts] decimal(18,2),
	
[Sales] decimal(18,2),
	
[COGS] decimal(18,2),
	

	
[Profit] decimal(18,2)
)
GO
INSERT INTO normalization.Financials
SELECT
	
ISNULL(s.SegmentID, -1)	as SegmentID
	
ISNULL(c.CountryID, -1)	as CountryID
	
ISNULL(p.ProductID, -1)	as ProductID
	
ISNULL(d.DiscountBandID, -1)	
as D
	
YEAR(f.[Date]) * 10000 + MONTH(f.[Da
        as DateKey,
	
f.[Units Sold],
	
f.[Sale Price],
	
f.[Gross Sales],
	
f.[Discounts],
	
f.[Sales],
	
f.[COGS],
	
f.[Profit]
FROM
	
demo.financials f
LEFT JOIN normalization.Segment s ON
    s.Segment = f.Segment
LEFT JOIN normalization.Country c ON
    c.Country = f.Country
LEFT JOIN normalization.Product p ON
    p.Product= f.Product
LEFT JOIN normalization.[Discount Band] d ON
    d.[Discount Band]= f.[Discount Band]

Creating a VIEW
Instead of duplicating all the data and persisting it into a
table, on many occasions a view will be enough (e.g.,
when you do not need to create a surrogate key or when
the query is not used in queries, but is only used to
refresh a Power BI semantic model or an Analysis
Services database). A view can be described as a “virtual”
table. No data is duplicated, only the SELECT  statement is
stored in the VIEW ’s definition. As I already created a
table with name Financials  in the normalization
schema, I decided to add “vw_” as a prefix to the view’s
name. The datatypes for the columns of the VIEW  are
automatically derived from the columns referenced in the
view.
CREATE OR ALTER VIEW normalization.vw_Financ
SELECT
	
ISNULL(s.SegmentID, -1)	as SegmentID
	
ISNULL(c.CountryID, -1)	as CountryID
	
ISNULL(p.ProductID, -1)	as ProductID
	
ISNULL(d.DiscountBandID, -1)	
as D
	
YEAR(f.[Date]) * 10000 + MONTH(f.[Da
        as DateKey,
	
f.[Units Sold],
	

	
f.[Sale Price],
	
f.[Gross Sales],
	
f.[Discounts],
	
f.[Sales],
	
f.[COGS],
	
f.[Profit]
FROM
	
demo.financials f
LEFT JOIN normalization.Segment s ON
    s.Segment = f.Segment
LEFT JOIN normalization.Country c ON
    c.Country = f.Country
LEFT JOIN normalization.Product p ON
    p.Product= f.Product
LEFT JOIN normalization.[Discount Band] d ON
    d.[Discount Band]= f.[Discount Band]
)
The VIEW  can be queried all the same way, as you would
query a table. Either with a SELECT  statement, or simply
in Power Query by selecting its name in the list of “tables”
(which also expose the views, just with a slightly different
icon in front of its name):
SELECT * FROM normalization.vw_Financials;

Creating a FUNCTION
The definition of following function is very similar to the
view. Typically, you would create a function (instead of a
view) because you want to add parameters, which you
pass into the query. I therefore call (table-valued)
functions sometimes “parametrized views”. To
demonstrate this, I added parameter DateKey  which is
used as a filter in the WHERE  clause of the query. Again, I
used a prefix (“fn_”) to distinguish it as a database object
from the table and the view.
-- FUNCTION
CREATE OR ALTER FUNCTION normalization.fn_Fi
	
@Date date
)
RETURNS TABLE
AS
RETURN
SELECT
	
ISNULL(s.SegmentID, -1)	as SegmentID
	
ISNULL(c.CountryID, -1)	as CountryID
	
ISNULL(p.ProductID, -1)	as ProductID
	
ISNULL(d.DiscountBandID, -1)	
as D
	
YEAR(f.[Date]) * 10000 + MONTH(f.[Da
        as DateKey,
	
f.[Units Sold],

	
f.[Sale Price],
	
f.[Gross Sales],
	
f.[Discounts],
	
f.[Sales],
	
f.[COGS],
	
f.[Profit]
FROM
	
demo.financials f
LEFT JOIN normalization.Segment s ON
    s.Segment = f.Segment
LEFT JOIN normalization.Country c ON
    c.Country = f.Country
LEFT JOIN normalization.Product p ON
    p.Product= f.Product
LEFT JOIN normalization.[Discount Band] d ON
    d.[Discount Band]= f.[Discount Band]
WHERE [Date] = ISNULL(@Date, [Date]);
The only difference to the VIEW  (beyond the differences
in the keywords you need to use to define a view versus a
function) is the added line for the WHERE  clause. I use
function ISNULL  to fail over to the row’s [Date]  value.
This allows you to use NULL  for the @Date  parameter.
When this is the case, then all fact rows are returned
(instead of none). This makes the parameter optional, as
you can see in the following two examples.

Unfortunately, Power Query will not expose functions to
you. Instead providing a SQL query as the data source is
mandatory. You can use this function similar to a table or
a view in a SELECT  statement. Don’t forget to add an
open and closes parathesis after the function’s name,
though. In the first example, the parameter is respected
and 35 rows which matches the parameter value are
returned:
SELECT * FROM normalization.fn_Financials ({
On the other hand, if I provide NULL  as the parameter
value, all 700 rows are returned:
SELECT * FROM normalization.fn_Financials (null)
Creating a PROCEDURE
To make this examples complete, I added a definition for a
stored procedure here. Again, the example contains the
query and a parameter for the date column. Typically, you
would create a stored procedure because you want or
need to add more logic. A procedure can use all of the
good stuff the procedural extension (called T-SQL, for

transactional SQL) offers, including parameters. Again, I
used a prefix (“usp_”) for the procedure’s name.
-- PROCEDURE
CREATE OR ALTER PROCEDURE normalization.usp_
	
@Date date = null
)
AS
SELECT
	
ISNULL(s.SegmentID, -1)	as SegmentID
	
ISNULL(c.CountryID, -1)	as CountryID
	
ISNULL(p.ProductID, -1)	as ProductID
	
ISNULL(d.DiscountBandID, -1)	
as D
	
YEAR(f.[Date]) * 10000 + MONTH(f.[Da
        as DateKey,
	
f.[Units Sold],
	
f.[Sale Price],
	
f.[Gross Sales],
	
f.[Discounts],
	
f.[Sales],
	
f.[COGS],
	
f.[Profit]
FROM
	
demo.financials f
LEFT JOIN normalization.Segment s ON
    s.Segment = f.Segment
LEFT JOIN normalization.Country c ON
c Country = f Country

    c.Country = f.Country
LEFT JOIN normalization.Product p ON
    p.Product= f.Product
LEFT JOIN normalization.[Discount Band] d ON
    d.[Discount Band]= f.[Discount Band]
WHERE [Date] = ISNULL(@Date, [Date]);
Unfortunately, procedures cannot be used as part of a the
FROM  or JOIN  clause, as you are using tables, views and
functions. You need to call a procedure via the EXEC
command instead. This makes stored procedures a less
common way of exposing information from the data
warehouse. Before you decide to create procedures make
sure that the reporting tool you want to use can handle
procedures. In Power BI you would connect to the
database and specify a query as the data source.
EXEC normalization.usp_Financials {d'2014-01
As I defined the procedure’s parameter with a default
value ( @Date date = null ) you can also omit the
parameter value if you want to have it contain the value
NULL  :

EXEC normalization.usp_Financials null;

EXEC normalization.usp_Financials;
In the rest of the sections I only will show the query, but will
not repeat the code to persist the query as a table, or transform
it into a VIEW , FUNCTION , or PROCEDURE  for the sake of
brevity.
I also want to demonstrate the solution of a Junk or Filter
dimension in SQL (as described in “Normalizing and
Denormalizing”). Especially, when the dimensions do not have
many attributes (or just one, as it is the case for Segment,
Country or Discount Amount), this aproach can reduce storage
space, as the fact table only needs to contain a single foreign
key (the composite business key), instead of plenty independent
foreign keys.
First you create the Filter table:
CREATE TABLE normalization_filter.[Filter]
(
	
_FilterKey [int] IDENTITY(1, 1) PRIMARY K
	
[Segment] [nvarchar](50) NULL,
	
[Country] [nvarchar](50) NULL,
	
[Product] [nvarchar](50) NULL,

	
[Discount Band] [nvarchar](50) NULL,
	
[Manufacturing Price] [decimal](18, 2) NU
) ON [PRIMARY];
When I physically create a table (instead of just creating a view)
I will definitely add a surrogate key (_FilterKey).
The table will be populated with a distinct list of possible
combinations of the dimensional values, derived from the
financials table. Column _FilterKey is not part of the list, as it
will be auto-populated due to the IDENTITY(1, 1)  clause
used during the CREATE TABLE  statement.
INSERT INTO normalization_filter.[Filter]
SELECT
	
DISTINCT
	
[Segment],
	
[Country],
	
[Product],
	
[Discount Band],
	
[Manufacturing Price]
FROM
	
demo.financials;
There is nothing special with the fact table. It has columns for
the _FilterKey and all facts:

CREATE TABLE normalization_filter.[Financials]
(
	
[_FilterKey] [int] NULL REFERENCES normal
	
[Units Sold] [decimal](18, 2) NULL,
	
[Sale Price] [decimal](18, 2) NULL,
	
[Gross Sales] [decimal](18, 2) NULL,
	
[Discounts] [decimal](18, 2) NULL,
	
[Sales] [decimal](18, 2) NULL,
	
[COGS] [decimal](18, 2) NULL,
	
[Profit] [decimal](18, 2) NULL,
	
[Date] [date] NULL
) ON [PRIMARY];
The SELECT  statement for the INSERT  operation is querying
all rows from table financials . In order to insert the correct
_FilterKey value, a lookup to the newly created dimension table
( Filter ) is necessary. The JOIN  predicate includes all column
from the Filter  table. In theory, you could omit the
Manufacturing Price , as this column is transitive
dependent on the Product  column. In practice I want to make
sure that my code covers also situations, where a product
suddenly comes with different prices. The same is true for the
LEFT JOIN . As I just have inserted a disticnt list of possible
combinations into the Filter  table, I do not want to risk
loosing any rows from the fact table, in case there was a

problem when populating the Filter  table. The LEFT JOIN
guarantees that not row from the fact table can be lost. Here is
the code:
INSERT INTO normalization_filter.[Financials]
SELECT
	
d._FilterKey
	
,f.[Units Sold]
	
,f.[Sale Price]
	
,f.[Gross Sales]
	
,f.[Discounts]
	
,f.[Sales]
	
,f.[COGS]
	
,f.[Profit]
	
,f.[Date]
FROM
	
[demo].[financials] f
LEFT JOIN
	
normalization_filter.[Filter] d ON
	
	
d.[Segment] = f.[Segment] AND
	
	
d.[Country] = f.[Country] AND
	
	
d.[Product] = f.[Product] AND
	
	
d.[Discount Band] = f.[Discount B
	
	
d.[Manufacturing Price]  = f.[Man
Denormalizing

NOTE
The file used for the demonstrations is 202 Denormalizing.sql.
Denormalizing can be seen as the opposite of normalizing.
Instead of storing a reference, the goal is to have the actual
value(s) in the current table, even when these values are
redundant and transitive dependent. If you have a background
in IT and databases, this step might be counterintuitive for you.
At least, I had to overcome an inner resistance to introduce
redundancy to a table, after I have learned (and practiced) to
avoid redundancies, when I started to build data warehouses
about two decades ago.
You can access the columns of the referenced table in a query
by joining the referencing and the referenced table together.
Remember: You need to denormalize dimension tables to
achieve a Star schema. And while you are at transforming a
table in SQL, you should add an integer surrogate key to the
dimension table. Therefore, I would recommend persisting
every dimension as a table, as shown in the following code
example for table Product . Only the primary key
( ProductID ) is of data type integer (with an IDENTITY
specification), the rest of the columns is defined as unicode

strings of variable length (and a maxmimum of 50 characters,
which should fit the expected texts).
CREATE TABLE denormalization.Product (
	
ProductID int IDENTITY(1,1) PRIMARY KEY,
	
Product nvarchar(50),
	
Subcategory nvarchar(50),
	
Category nvarchar(50)
);
The content for this table is created via joins from three tables
( DimProduct , DimProductSubcategory , and
DimCategory ):
INSERT INTO denormalization.Product
SELECT
	
dp.EnglishProductName Product,
	
ISNULL(dps.EnglishProductSubcategoryName
	
ISNULL(dpc.EnglishProductCategoryName, 'u
FROM
	
dbo.DimProduct dp
LEFT JOIN dbo.DimProductSubcategory dps ON
    dps.ProductSubcategoryKey=dp.ProductSubcatego
LEFT JOIN dbo.DimProductCategory dpc ON
    dpc.ProductCategoryKey=dps.ProductCategoryKey

NOTE
I intentionally use LEFT JOIN+s here, in case referential integrity is
not guaranteed (= in case a product’s +ProductSubcategoryKey  cannot
be found in DimProductSubcategory). On top of that I would recommend finding a
good replacement value for the subcategory’s name stored in the Product  table, like
“Unknown”. In the section about “Flags and Indicators” I show ways how to assign
replacement values gracefully.
If referential integrity is guaranteed (= there are active foreign key constraint in
place between the tables used in the join) you should use INNER JOINs instead, as
they are more performant. No treatment for replacement values is necessary, then.
After you have not the perfect start schema, as you normalized
all fact tables and denormalized all dimension tables, it is time
to think about adding information in the form of calculations.
Calculations
NOTE
The file used for the demonstrations is 203 Calculations.sql.
SQL offers standard operators and a wide variety of
mathematical and statistical function. I will not go into any
detail here. You can find all the operators (Arithmetic Operators
(Transact-SQL)) and functions documented very well in

Microsoft’s official online documentation (Mathematical
Functions (Transact-SQL)) for free.
Before you start creating calculations in SQL evaluate them, if
the result is indeed additive (that means that aggregating the
result leads to a meaningful result). A rule of thumb is that
calculations involving a division operator (e.g., to calculate an
average, a percentage or a ratio) are not additive. Those
calculations need to be done on report level (read: on
aggregated values and not per individual row of the table) and
therefore be defined as a measure in the data model. You find
typical examples for such calculations in DAX in “Calculations”.
To demonstrate the problem with non-aggregable calculations, I
created the following view, which aggregates and calculates
numbers based on the available facts in the
FactResellerSales  table:
CREATE OR ALTER VIEW calc.SalesAggregation AS (
SELECT
	
frs.ProductKey,
	
SUM(frs.OrderQuantity) as OrderQuantity,
	
AVG(frs.UnitPrice) as UnitPrice, -- dange
	
SUM(frs.TotalProductCost) as TotalProduct
	
SUM(frs.DiscountAmount) as DiscountAmount
	
SUM(SalesAmount) - SUM(frs.TotalProductCo	
(SUM(S l
A
t)
SUM(f
T t lP
d
tC

	
(SUM(SalesAmount) - SUM(frs.TotalProductC
        as MarginPct,
	
SUM(frs.SalesAmount) as SalesAmount,
	
COUNT(frs.SalesAmount) as SalesAmountCoun
	
AVG(frs.SalesAmount) as SalesAmountAvg -
FROM
	
dbo.FactResellerSales frs
GROUP BY frs.ProductKey
);
Based on this view, I calculate the grand margin (= single
number calculated over all rows) once as difference of
SalesAmount  and TotalProductCost  (named Margin1 )
and once as sum of the view’s Margin  column (named
Margin2 ):
-- Margin
SELECT
	
FORMAT(SUM(SalesAmount) - SUM(TotalProduc
	
FORMAT(SUM(Margin), '#,###') Margin2
FROM
	
calc.SalesAggregation;

Margin1
Margin2
470,483
470,483
The result is the exact same. And the reason is that it makes no
difference if you first add up the values for SalesAmount  and
TotalProductCost  and then subtract the two numbers (as
done in Margin1 ) or if you first subtract the two values per
product and then add up the results (as done in Margin2 ).
Such a calculation (result of a subtraction) is aggregable and
can be implemented either way.
Next, let’s look at ways to calculate the sales amount. I
implemented five different versions. The first version just sums
up the column from the view ( SalesAmount1 ). The second
version multiplies the UnitPrice  with the OrderQuantity
and subtracts the +DiscountAmount  from the result
( SalesAmount2 ). Alternatively, I calculate the average of the
UnitPrice , multiply this with the sum of the
OrderQuantity  and then subtract the DiscountAmount
( SalesAmount3 ). The next two versions ignore the view and
query directly the base table ( FactResellerSales ) to avoid
the aggregations done in the view. Once, the sum of the
SalesAmount  is calculated ( SalesAmount4 ). For
SalesAmount5  I implement the formula per row of

FactResellerSales  and add up the values. The expectation
is that the last two versions are identical and correct. But what
about the other versions? Here you are:
-- SalesAmount
SELECT
	
FORMAT(SUM(SalesAmount), '#,###')  SalesA
	
FORMAT(SUM((UnitPrice * OrderQuantity) - 
	
FORMAT((AVG(UnitPrice) * SUM(OrderQuantit
FROM
	
calc.SalesAggregation

SELECT
	
FORMAT(SUM(SalesAmount4), '#,###') SalesA
	
FORMAT(SUM(SalesAmount5), '#,###') SalesA
FROM
	
(SELECT
	
	
SalesAmount as SalesAmount4,
	
	
(UnitPrice*OrderQuantity)-Discoun
	
 FROM dbo.FactResellerSales
	
 ) x
SalesAmount1
SalesAmount2
SalesAmount3
SalesAmou
80,450,597
80,722,815
101,291,559
80,450,597

SalesAmount1 , SalesAmount4  and SalesAmount5  are
identical and trust me, are correct. SalesAmount2  is (slightly)
off, and therefore wrong. SalesAmount3  is completely off.
These two versions have in common that they are based on the
(aggregated) UnitPrice  from view SalesAggregation ,
which is calculated as the average of the UnitPrice  per
ProductKey . Function AVG  calculates only an arithmetic
average. That means, it sums up all values of UnitPrice  (per
ProductKey ) and then divides it by the number of rows (per
ProductKey ). This would only work correctly if only a single
product was sold. The correct calculation must take the
OrderQuantity  into account. Imagine, you bought apples one
day for the price of USD 1 per pound and for USD 2 per pound
the other day. If you bought a pound each day, then the average
price is USD 1.5. But imagine, you bought 1 pound for USD 1 and
10 pounds for USD 2, then you can not safely pretend that the
average price was USD 1.5 (as the AVG  function will calculate),
but apparently was much higher than that. The correct formula
has to take the quantity into account: 1 pound * USD 1 plus 10
pounds * USD 2 = USD 21. Divided by 11 pounds gives an correct
average price of USD 1.9.
The problem with SalesAmount2  is that it multiplies the
wrongly calculated UnitPrice  from the view with the
OrderQuantity . The problem with SalesAmount3  is that it

averages the wrongly calculated UnitPrice . The calculation
formula of SalesAmount5  works, because it is not based on
the (aggregated values of the) view but calculated on the
granularity of table FactResellerSales .
The correct calculation for the UnitPrice  on any aggregation
level as sum of SalesAmount  divided by the sum of
OrderQuantity . This, you can only achieve as a calculation on
the report level, which means in DAX, when it comes to Power
BI.
MarginPct  is the Margin  in percent of the SalesAmount . To
calculate “in percent” means that the Margin  has to be divided
by the SalesAmount . The following query shows the Margin
and the SalesAmount  and then two versions for the
calculation of the margin in percent: One averages the
MarginPct  from the view (which calculates the margin in
percent per ProductKey ) and the other calculates it as the
division of the sum of Margin  and sum of SalesAmount .
-- MarginPct
SELECT
	
FORMAT(SUM(Margin), '#,###')  Margin,
	
FORMAT(SUM(SalesAmount), '#,###')  SalesA
	
FORMAT(AVG(MarginPct), '0.00%') MarginPct

	
FORMAT(SUM(Margin)/SUM(SalesAmount), '0.0
FROM
	
calc.SalesAggregation;
Margin
SalesAmount
MarginPct1
MarginPct2
470,483
80,450,597
8.34%
0.58%
The numbers for the two versions are very different. Averaging
the MarginPct , as in the calculation for MarginPct1 , does
not lead to the correct number. The only way is to divide the
two aggregated (and shown values): 470,483 divided by
80,450,597 leads to the correct value of 0.58%.
WARNING
It is not enough that the numbers in the tables and views provided in the data
warehouse are correct. You need to think beyond the data warehouse and decide if a
calculation’s result is aggregable, if the result is additive. If it is not, then pre-
calculating the value in the data warehouse layer makes no sense. On the opposite it
could lead to wrongly reported numbers, if somebody aggregates the values in
visuals and reports.
Flags and Indicators

NOTE
The file used for the demonstrations is 204 Flags and Indicators.sql.
Remember: The goal of all the exercises in transforming source
data is to make the life of report creators easier. Usually,
showing codes or abbreviations in a report is not what the
report consumer wants. SQL is good central place to transform
such flags and indicators into meaningful text:
To transform column FinishedGoods  (which contains 0
or 1) to a descriptive text column I use the CASE  operator
and provide different conditions to convert the flag into
“not salable”, “salable”, or “unknown”:
SELECT
	
FinishedGoodsFlag as _FinishedGoodsFl
	
CASE
	
WHEN FinishedGoodsFlag=0 THEN 'not sa
	
WHEN FinishedGoodsFlag=1 THEN 'salabl
	
ELSE                          'unknow
	
END [Finished Goods Flag]
	
,*
FROM dbo.DimProduct;

_FinishedGoodsFlag
Finished
Goods Flag
ProductKey
Produ
…​
…​
…​
…​
0
not salable
208
TP-092
0
not salable
209
RC-029
1
salable
210
FR-R92
1
salable
211
FR-R92
…​
…​
…​
…​
Instead of providing individual conditions, you can use
CASE  also to compare an expression to different values.
You specify the expression between CASE  and WHEN  and
for each WHEN  you specify the value, the expression should
be compared to:
SELECT
	
ProductLine as _ProductLine,
	
CASE ProductLine
	
WHEN 'R' THEN 'Road'
	
WHEN 'M' THEN 'Mountain'
	
WHEN 'T' THEN 'Touring'

	
WHEN 'S' THEN 'Standard'
	
ELSE          'other'
	
END [Product Line]
	
,*
FROM dbo.DimProduct;
_ProductLine
Product
Line
ProductKey
ProductAlter
…​
…​
…​
…​
NULL
other
208
TP-0923
NULL
other
209
RC-0291
R
Road
210
FR-R92B-58
R
Road
211
FR-R92R-58
S
Standard
212
HL-U509-R
S
Standard
213
HL-U509-R
…​
…​
…​
…​

Converting values is a classical use case for common table
epressions (CTE) in SQL, as well. In the following example I
specify common table expression Class  as a unioned list
of SELECT-statements, which deliver a query result of three
rows and two columns: One row per Class  and an
additional ClassDescription . I then join table
DimProduct  with the common table expression ( Class ),
as it would do with a table:
WITH Class AS (
SELECT 'H' Class, 'High'    ClassDescription 
SELECT 'M' Class, 'Medium'	
ClassDescript
SELECT 'L' Class, 'Low'     ClassDescription
)
SELECT
	
dp.Class as _Class,
	
c.ClassDescription as [Class Descript
	
, *
FROM dbo.DimProduct dp
JOIN Class c ON c.Class=dp.Class;

_Class
Class
Class
Description
ProductKey
P
…​
…​
…​
…​​
Medium
194
SA-T612
…​
L
196
SD-2342
…​
L
L
SH-4562
…​
H
High
2
…​
…​
…​
…​​
An improved version of this query would also take care of
classes not listed in the CTE and show e.g., “Unknown” as
the ClassDescription . In the next example I show you
how can catch missing values and replace them with a
meaningful text.
I am not a too big fan of implementing business logic within
a query (or report), as shown in the previous examples. In a
perfect world the report-user should oversee defining what
text should be shown instead of a flag or an indicator. It
should not be necessary to change the query code (by IT) to
implement a change. That’s why my preferred way of
transforming flags and indicators is to create a physical
table to store the lookup values. This table could sit inside

an Excel workbook or a SharePoint list, but optimally its a
table in the database, exposed to the responsible users via
an application, so they can maintain changes to the content
of this table. This is table is then simply joined in a query, as
you can see from the following code example (which
contains the code to re-create the table and fill it with
(initial) values):
DROP TABLE IF EXISTS flag.Styles;
CREATE TABLE flag.Styles (
	
Style char(1),
	
StyleDescription nvarchar(50)
	
)
INSERT INTO flag.Styles
SELECT 'W' Class, 'Womens'    StyleDescriptio
SELECT 'M' Class, 'Mens'      StyleDescriptio
SELECT 'U' Class, 'Universal' StyleDescriptio
GO
CREATE OR ALTER VIEW flag.Style AS (
SELECT
	
s.Style as _Style,
	
ISNULL(s.StyleDescription, 'Unkown') 
	
, *
FROM dbo.DimProduct dp

JOIN flag.Styles s ON s.Style=dp.Style
);
_Style
Style
Description
ProductKey
ProductAltern
…​
…​
…​
…​
NULL
Unkown
208
TP-0923
NULL
Unkown
209
RC-0291
U
Universal
210
FR-R92B-58
U
Universal
211
FR-R92R-58
…​
…​
…​
…​
Unknown values are coded as NULL  in relational
databases. Power BI treats a NULL  as zero for numerical
columns, as False for True/False  columns and as empty
for all other data types (text or date/time). Its best practice

to avoid empty values in reports, though. Therefore, you
need to replace NULL  with meaningful text. If you want to
write this as a condition in CASE  statement, remember that
a comparison like <expression> = NULL  never evaluates
to true, but you need to formulate the condition as
<expression> IS NULL . In the following example I use
function ISNULL . This function checks, if the first value IS
NULL . If it is the case, then the second parameter is
returned. If it is not the case, then the first parameter is
returned. In cases where the logic is chained (“take the
value from column c1, but if c1 is null, take c2, but if c2 is
null take, …​”) you can use function COALESCE .
SELECT
	
EnglishProductName,
	
WeightUnitMeasureCode as WeightUnitMe
	
ISNULL(WeightUnitMeasureCode, 'N/A') 
FROM dbo.DimProduct dp

EnglishProductName
WeightUnitMeasureCode
Weigh
Measu
Code
…​
…​
…​
Cone-Shaped Race
NULL
N/A
Reflector
NULL
N/A
LL Mountain Rim
G
G
ML Mountain Rim
G
G
…​
…​
…​
So far, I have shown you how can split or combine existing
tables or add calculations or meaningful text. Now it is time to
talk about, how you can create a new table from scratch.
Time and Date
NOTE
The file used for the demonstrations is 205 Time and Date.sql.

If someone would present a data warehouse to me, in which
there is not a Date  table, I would be very curios. Every single
data warehouse I built so far, had a date table for good reasons.
Creating such a table is not a big issue. And keeping it
automatically up-to-date (pun intended) can be done, as well. To
create the number of rows I need for the table, I use Itzik Ben-
Gan’s function GetNumsItzikBatch . It has two parameters (a
start value and a maximum value) and will return a row for
every value in between as a query result. I add then these
numeric value via function DATEADD  as number of days to my
start date. The start date I derive as January 1 for the year of the
earliest ( MIN ) order date of my fact table. The maximum value
for the GetNumsItzikBatch  function I derive as the
difference in days between the start date and the last day of the
year of the latest ( MAX ) order date of my fact table. All these
components I created as common table expressions, as you can
see in the first portion of the code listed here:
WITH
MinYear     AS
    ( SELECT YEAR(MIN(OrderDate)) MinYear FROM Po
MinDate     AS
    ( SELECT DATEFROMPARTS(MinYear, 01, 01) MinDa
MaxYear     AS
    ( SELECT YEAR(MAX(OrderDate)) MaxYear FROM Po
MaxDate
AS

MaxDate     AS
    ( SELECT DATEFROMPARTS(MaxYear+1, 12, 31) Max
MaxNumber   AS
    ( SELECT CONVERT(bigint, DATEDIFF(day, MinDat
NumberTable AS
    ( SELECT N as Number
      FROM demo.GetNumsItzikBatch(0, (SELECT MaxN
Date AS (
SELECT
	
DATEADD(
	
	
day,
	
	
n.Number,
	
	
d.MinDate
	
	
) Date
FROM
	
NumberTable n
CROSS JOIN MinDate d
)
SELECT
	
Date                                   as
	
CONVERT(int, FORMAT(Date, 'yyyyMMdd')) as
	
YEAR(Date)                             as
	
MONTH(Date)                            as
	
FORMAT(Date, 'MMMM')                   as
	
FORMAT(Date, 'yyyy-MM')                as
	
DATEPART(ww, Date)                     as
	
DATEPART(iso_week, Date)               as
FROM
	
Date;

The SELECT takes then the generated Date  and applies
different formulas to derive the columns I need in my Date
dimension:
DateKey  is an integer representing the date in the format
YYYYMMDD (e.g., 20231231).
Year  is the year (e.g., 2023).
Month Number  contains the month number of the year
(e.g., 12)
In Month  is usually store the month’s name (e.g.,
December)
Depending on the needs in the report, I add different
format of the month (e.g., 2023-12). This is easy to do with
the FORMAT  function.
Sometimes reports are based on weeks. Function
DATEPART  can deliver both, the number of the week and
the number of the ISO week.
The story for a Time  table, if needed, is similar. This time I
generate a list of values between 1 and 1440 (minutes a day). I
find the code better readable, when I specify 24 * 60 , instead
of 1440, but that’s, of course, totally up to you. I add these
numbers then as minutes to time “00:00:00”, as you can see
from the following code snippet:

WITH
NumberTable AS ( SELECT N as Number FROM demo.Get
Time AS (
SELECT
	
DATEADD(minute, Number, '00:00:00') Time
FROM
	
NumberTable
)
SELECT
	
CONVERT(Time, Time)	
	
	
			
	
FORMAT(Time, 'HH')	
	
	
			
	
FORMAT(Time, 'mm')	
	
	
			
	
FORMAT(Time, 'HH:mm')	
	
	
		
FROM
	
Time;
Again, I use function FORMAT to generate the additional
columns for my Time  table, upon what is needed in the
reports. A Time  table for every minute of a day contains only
1440 rows - so we do not have to be greedy in terms of columns
the report users ask us for.
Role-Playing Dimensions

NOTE
The file used for the demonstrations is 206 Role Playing Dimensions.sql.
In the world of SQL, role-playing dimensions can easily be
solved, as you can join a table with a join predicate of your full
choice. The same tables could be joined with different join
predicates multiple times in a query. One solution for Power BI
and Analysis Services Tabular is, to add the same table multiple
times under different names to the data model. As long as the
dimension table is not too big (no, there is no clear number,
what too big means, but it depends solely on the resources you
are using), this is easily tolerable.
Remember that best practice is anyways to not directly access
the tables from the data warehouse, but to use VIEW+s as a
layer in between. So, the solution is, to just
create more than one single +VIEW  for the identical
table from the data warehouse. Instead of creating on single
view Date , you create variations per role: [Order Date] ,
[Ship Date] , etc. No need to actually duplicate the content of
the dimension table in the data warehouse.
It’s important to remember that it is best practice to chose
names for your columns, which are unique through the whole

data model. Nothing is more frustrating than discussions about
“Which year you are showing in this visual?” or “Which date
the filter should be applied on?”. That’s what the alias definition
(keyword AS ) in the SELECT  projection is for:
CREATE OR ALTER VIEW roleplaying.vw_OrderDate AS 
SELECT
	
DateKey		
	
as OrderDateKey,
	
Date	
	
	
as OrderDate,
	
CalendarYear	
as OrderYear
FROM
	
PowerBI.DimDate
)
GO
CREATE OR ALTER VIEW roleplaying.vw_ShipDate AS 
SELECT
	
DateKey		
	
as ShipDateKey,
	
Date	
	
	
as ShipDate,
	
CalendarYear	
as ShipYear
FROM
	
PowerBI.DimDate
)
As long as the number of columns and the number of rows is
small, creating these views and copy and pasting the prefix in

front of all columns is doable. The brave one’s among us will
still choose a solution built on dynamic SQL, as it’s really
comfortable to use, once you have built it. And, as all automatic
solutions, will guarantee that the prefixes are spelled identically
for all columns (which is not always the case when you do
changes manually in an editor).
The following code first defines a bunch of variables, to identify
the database object the VIEW  should be generated on (variables
@SchemaName  and @TableName ), to specify the VIEW  which
should be generated (variables @SchemaNameTarget  and
@TableNameTarge ) and the prefix, which will be applied to
the name of the VIEW  and all columns. Another set of helper
variables is defined, to host the list of the names of the columns
( @ColumnList ) and the separator ( @Separator ).
Then a SELECT  statement aggregates via function
STRING_AGG  a string containing the column names of the
source object in the format we need it when creating the view
(including brackets and the alias definition). e.g., for a query list
list containing only “DateKey” and “Date”, the aggregation
creates this string: [DateKey] as [OrderDateKey], [Date] as
[OrderDate]. This string is then printed to the console output
with the rest of text to form a valid CREATE OR ALTER VIEW
statement:

-- Automate renaming
DECLARE
	
@SchemaName sysname = 'PowerBI',
	
@TableName sysname = 'DimDate',
	
@SchemaNameTarget sysname = 'roleplaying
	
@TableNameTarget sysname = 'Date',
	
@Prefix nvarchar(50) = 'Ship';

DECLARE
	
@ColumnList nvarchar(max),
	
@Separator nvarchar(50) = N',' + char(10
	
;

SELECT
	
@ColumnList = STRING_AGG(QUOTENAME(c.name
FROM
	
(SELECT t.object_id, t.schema_id FROM sys
	
 SELECT v.object_id, v.schema_id FROM sys
JOIN sys.columns c ON c.object_id=t.object_id
WHERE
	
OBJECT_NAME(t.object_id) = @TableName AND
	
SCHEMA_NAME(t.schema_id) = @SchemaName;
print '
CREATE OR ALTER VIEW ' + QUOTENAME(@SchemaNameTa	
SELECT
	
' + @ColumnList + '
FROM
	
' + QUOTENAME(@SchemaName) + ' ' + QUOTEN

	
 + QUOTENAME(@SchemaName) + .  + QUOTEN
)
'
To generate a view for a role-playing dimension, make sure to
set the first five parameters right, execute the code and then
copy, paste & execute the generated code from the console. Feel
free to transform this code into a procedure, run the procedure
in an automatic manner, etc.
TIP
I usually also add a comment in the code inside the generated +VIEWs definition to
warn people not to change this definition by hand, as their changes might be
overwritten the next time I generate the code again. I, myself, was thankful for such a
hint several times, when I, in the heat of a problem, was almost starting to directly
changing the content of such a view.
Next, I introduce you how to implement Slowly Changing
Dimensions with the help of SQL.
Slowly Changing Dimensions
NOTE
The file used for the demonstrations is 207 Slowly Changing Dimensions.sql.

Implementing Slowly Changing Dimensions (SCD) is only
possible if you physically store the data in a data warehouse (a
logical data warehouse layer with views on e.g., the data source
does not allow for SCD). If you want (or need?) to implement
Slowly Changing Dimension then there is no way around a data
warehouse, where you persist the dimension’s data. Only with
the persisted data you can compare newly arriving data with
the already existing data to track the changes. In this section I
will show you how to implement Slowly Changing Dimensions of
Type 1 (last change wins) and Type 2 (creating separated rows
for each version) in SQL.
I chose the following simple table to illustrate the use cases:
Example 18-2.
DROP TABLE IF EXISTS scd.SCDSource;
CREATE TABLE scd.SCDSource (
	
AlternateKey int,
	
Region nvarchar(50)
);
INSERT INTO scd.SCDSource
SELECT  0, 'NA'		
	
	
UNION ALL
SELECT  1, 'Northwest'	 	
UNION ALL
SELECT 10, 'United Kingdom'

Table 18-1. The separated queries are joined
into one.
AlternateKey
Region
0
NA
1
Northwest
10
United Kingdom
Type 0: Retain Original
Type 0 means that the loaded data must not change.
Implementation of the ETL task is limited to inserting new
rows. The NOT EXISTS  clause makes sure to identify such
rows via the source’s business key (column AlternateKey ).
When rows get deleted in the data source, nothing needs to be
done in the data warehouse’s table. The same applies to
updated rows in the data source: The change is just be ignored.
I store the creation date in an additional column ( CreatedAt ).
-- SCD Type 0
DROP TABLE IF EXISTS scd.SCD0;
CREATE TABLE scd.SCD0 (
	
AlternateKey int,
	
Region nvarchar(50),
	
CreatedAt datetime2


);

-- INSERT
INSERT INTO scd.SCD0
SELECT AlternateKey, Region, SYSDATETIME()
FROM scd.SCDSource stage
WHERE NOT EXISTS (SELECT TOP 1 1 FROM scd.SCD0 dw
AlternateKey
Region
CreatedAt
0
NA
2023-07-08
07:46:16.9373128
1
Northwest
2023-07-08
07:46:16.9373128
10
United
Kingdom
2023-07-08
07:46:16.9373128
Type 1: Overwrite
I implement type 1 in a way where it not just overwrites
existing rows of data but keeps track of changes (e.g., timestamp
of the change, which process changed the data, etc.). Checking
changes for columns which can be NULL  involves some extra
logic, to find out if the value changed to or from NULL . Instead

of deleting rows, I only mark them as deleted (soft delete) by
updating a column with the current timestamp. That’s why
rows which were deleted in the data source, trigger an UPDATE
(and not a DELETE ) statement in the following code snippet.
In a first step, I load this table into the data warehouse (= into
another table). Let’s create this table as a Type 1. This table
contains all columns of the source table, plus some additional to
store metadata: a timestamp, when the row was firstly created,
or the latest changes happened and a timestamp to mark it as
deleted.
DROP TABLE IF EXISTS scd.SCD1
CREATE TABLE scd.SCD1 (
	
AlternateKey int,
	
Region nvarchar(50),
	
ChangedAt datetime2,
	
DeletedAt datetime2
);
A usual plain INSERT  needs to be extended with a check, to
only insert rows into the data warehouse, which are not
already exist. The check for existence is done via the business
key AlternateKey . ChangedAt  is propagated with the
current time and data, DeletedAt  is explicitely set to NULL .

Example 18-3.
INSERT INTO scd.SCD1
SELECT AlternateKey, Region, SYSDATETIME(), null
FROM scd.SCDSource stage
WHERE NOT EXISTS (SELECT TOP 1 1 FROM scd.SCD1 dw
The result is that we’ve got the three rows in the data
warehouse, as you can see in table Table 18-1.
AlternateKey
Region
ChangedAt
DeletedAt
0
NA
2023-07-08
08:06:16.9373128
NULL
1
Northwest
2023-07-08
08:06:16.9373128
NULL
10
United
Kingdom
2023-07-08
08:06:16.9373128
NULL
Let’s introduce now some changes to the source table (laid out
in Example 18-4). We cover the following cases:

A new row appears in the source system ( AlternateKey  =
11). It must be inserted into the data warehouse. (That’s
already covered with the statement described in
Example 18-3. Make sure to re-run this statement.)
A row could be removed from the source table
( AlternateKey  = 0). In this case we expect the row’s
DeletedAt  to be filled with the current timestamp.
An attribute could be changed in the source
( AlternateKey  = 1, where I changed the region from
“Northwest” to “Nordwest”). Then we expect the update to
be reflected in the data warehouse and the ChangedAt  to
be set to the current point in time.
A row could exist totally unchanged in the source system
( AlternateKey  = 10). We have to make sure to not update
any column for such rows.
Example 18-4. Simulating changed to the data source
INSERT INTO scd.SCDSource SELECT 11, 'Austria'
DELETE FROM scd.SCDSource WHERE AlternateKey=0;
UPDATE scd.SCDSource SET Region='Nordwest' WHERE 
In case of the removed row, I do not want to remove the row
from the data warehouse, but only mark it as removed. That’s

why a deletion is implemented as an UPDATE  which sets the
DeletedAt  column to the current timestamp.
Example 18-5.
UPDATE dwh
SET
	
dwh.[DeletedAt] = SYSDATETIME()
FROM [scd].[SCD1] dwh
WHERE NOT EXISTS (SELECT TOP 1 1 FROM scd.SCDSou
An update is only justifiable, if a change to one of the attributes
of the row happened. This is important, otherwise I would
update column ChangedAt  under all circumstances (even
when no change has happened). Someone reading this column
would then wrongly get the impression that a change
happened. And any delta-load logic depending on the
ChangedAt  column would wrongly re-load all rows instead,
ending up in an unwanted full-load.
If an attribute is nullable, the check for a change is a bit more
complex. It is not enough to just compare the columns from the
source and from the data warehouse table with each other. If
either would be NULL  the comparison would evaluate to NULL
as well and no update would happen. Instead, you need to

additionally check if the source IS NULL and the target IS NOT
NULL or if the source is IS NOT NULL, but the target IS NULL. In
all these cases we need to trigger the update as well. Also, we
need to set DeletedAt  to NULL , to resurrect a row, in case the
row was previously (soft) deleted but happens to show up in the
source again.
Example 18-6.
UPDATE [dwh]
SET
	
 [dwh].[Region] = [stage].[Region]
	
,[dwh].[ChangedAt] = SYSDATETIME()
	
,[dwh].[DeletedAt] = null
FROM [scd].[SCD1] [dwh]
INNER JOIN [scd].[SCDSource] [stage] on [dwh].Alt
WHERE
([dwh].[Region] <> [stage].[Region] OR ([dwh].[Re

NOTE
SQL Server 2022 (compatibility level 160+) introduced a new comparison operator IS
DISTINCT FROM, which is available in all Azure tastes of SQL Server as well (Azure
SQL Database, Azure SQL Managed Instance, SQL Endpoint in Microsoft Fabric, and
Warehouse in Microsoft Fabric). With the help of this operator, you can replace
([dwh].[Region] <> [stage].[Region] OR ([dwh].[Region] IS NOT NULL
AND [stage].[Region] IS NULL) OR ([dwh].[Region] IS NULL AND
[stage].[Region] IS NOT NULL))
with
[dwh].[Region] IS DISTINCT FROM [stage].[Region]
This makes the code shorter and better readable. Learn more about this operator at
IS DISTINCT FROM (Transact-SQL)
If you have executed all snippets as explained, the data
warehouse table will now look like the following:

AlternateKey
Region
ChangedAt
DeletedAt
0
NA
2023-07-08
08:06:16.9373128
2023-07-08
08:39:58.6840
1
Northwest
2023-07-08
08:06:16.9373128
NULL
10
United
Kingdom
2023-07-08
08:06:16.9373128
NULL
11
Austria
2023-07-08
08:31:41.4968427
NULL
In a final step I test, if resurrection of previously deleted rows
works as well. Therefore, please execute the script to initialize
the source table Example 18-2 and then re-run the load scripts
Example 18-3, Example 18-5, and Example 18-6. This will reset
the data warehouse, to a state, where the rows with
DeletedAt  of NULL  matches the initial state and ChangedAt
changed several times (except for the row with AlternateKey
= 10, which never changed). A table of Type 1 will keep the extra
row ( AlternateKey =11) but marked as deleted. Compared to
a table without Slowly Changing Dimensions, the number of
rows might be slightly bigger.

AlternateKey
Region
ChangedAt
DeletedAt
 0
NA
2023-07-08
08:46:37.0275009
NULL
 1
Northwest
2023-07-08
08:46:37.0275009
NULL
10
United
Kingdom
2023-07-08
08:06:16.9373128
NULL
11
Austria
2023-07-08
08:31:41.4968427
2023-07-08
08:46:37.0275
When you need not to only keep track that a change happened,
but what a row looked like before the change, then you need to
implement Slowly Changing Dimensions of Type 2.
Type 2: Add New Row
Slowly Changing Dimension Type 2 (SCD2) is similar to Type 1 for
new and for deleted rows, but different for rows changed in the
data source: In the case of a change of an attribute you do not
simply overwrite the row in the data warehouse but keep the

old row (and mark it as old) and insert an additional row (with
the changed attributes).
Before you can do the initial load, you need to create the table
in the data warehouse. Instead of ChangedAt  and DeletedAt
it contains ValidFrom  and ValidUntil  to specify a range of
time, when this version of a row was or is active. It’s important
to point out that a surrogate key ( SID ) is mandatory in this
scenario. The business key ( AlternateKey ) will not be unique,
as there will (soon) be multiple versions of this key stored in the
table.
DROP TABLE IF EXISTS scd.SCD2
CREATE TABLE scd.SCD2 (
	
SID int identity(1,1),
	
AlternateKey int,
	
Region nvarchar(50),
	
ValidFrom datetime2,
	
ValidUntil datetime2
);
INSERT  into a Type 2 table is very similar to inserting into a
Type 1 table. You need to make sure that an active row for this
business key ( AlternateKey ) doesn’t already exist. A row is
active, if ValidUntil  is NULL . ValidFrom  is propagated

with the current timestamp, ValidUntil  is explicitly set to
NULL  for this new row. (Alternatively, you could also set it to a
date in the far future, like December 31 in year 9999. This has
though the potential to create huge date tables in Power BI in
case users don’t follow best practices and either did not turn off
the Auto date/time setting, as described in “Time and Date”, or
use function CALENDARAUTO, as described in “Time and
Date”).
Example 18-7.
INSERT INTO [scd].[SCD2] ([AlternateKey], [Region
SELECT [stage].[AlternateKey], [stage].[Region], 
FROM [scd].[SCDSource] [stage]
WHERE NOT EXISTS (
	
SELECT TOP 1 1
	
FROM [scd].[SCD2] [dwh]
	
WHERE [dwh].AlternateKey=[stage].Alternat
	
	
AND [dwh].[ValidUntil] IS NULL
	
)
The result is that you’ve got the three rows in the data
warehouse, as you can see in table Table 18-1.

SID
AlternateKey
Region
ValidFrom
1
0
NA
2023-07-08
10:00:06.9957244
2
1
Northwest
2023-07-08
10:00:06.9957244
3
10
United
Kingdom
2023-07-08
10:00:06.9957244
Let’s now introduce the same changes to the source table as in
the Type 1 scenario (laid out in Example 18-4). After applying
the changes to the source table, make sure to re-run
Example 18-7.
In case of the removed row, I do not want to remove the row
from the data warehouse, but only mark it as removed. That’s
why a delete is implemented as an UPDATE  statement which
sets the ValidUntil  column to the current timestamp.
Example 18-8.
UPDATE dwh
SET d h [V lidU til]
SYSDATETIME()

SET dwh.[ValidUntil] = SYSDATETIME()
FROM [scd].[SCD2] dwh
WHERE
	
dwh.SID >= 1 AND
	
ISNULL(dwh.[ValidUntil], SYSDATETIME()) >
	
NOT EXISTS (SELECT TOP 1 1 FROM [scd].[SC
An update is only justifiable, if a change to one of the attributes
of the row happened. This is important, otherwise we would
update column ValidUntil  for all existing rows and re-create
new versions for the AlternateKey . The dimension table
would unnecessarily insert a row for every AlternateKey
with every single load: In the case of a daily load, after a year
365 versions of every region would exist, even when no column
changed it values. Therefore, make sure to test this case
thoroughly.
Nullable attributes need a special treatment, as well (and
described in the section about Type 1). First, I soft-delete the old
version via UPDATE . Then, I insert the new version of the row,
as you can see in the snippet in Example 18-9.
Example 18-9.  
-- INACTIVATE OLD VERSION
UPDATE d h

UPDATE dwh
SET dwh.[ValidUntil] = SYSDATETIME()
FROM [scd].[SCD2] dwh
INNER JOIN [scd].[SCDSource] stage on dwh.Alterna
WHERE
	
dwh.SID >= 1 AND
	
ISNULL(dwh.[ValidUntil], SYSDATETIME()) >
	
([dwh].[Region] <> [stage].[Region] OR (
-- INSERT NEW VERSION
INSERT INTO [scd].[SCD2] ([AlternateKey], [Region
SELECT [stage].[AlternateKey], [stage].[Region], 
FROM [scd].[SCDSource] stage
WHERE
	
EXISTS (SELECT TOP 1 1 FROM [scd].[SCD2] 
	
AND ([dwh].[Region] <> [stage].[Region] O
	
AND dwh.SID >= 1
	
)

NOTE
SQL Server 2022 (compatibility level 160+) introduced a new comparison operator IS
DISTINCT FROM, which is available in all Azure tastes of SQL Server as well (Azure
SQL Database, Azure SQL Managed Instance, SQL Endpoint in Microsoft Fabric, and
Warehouse in Microsoft Fabric). With the help of this operator, you can replace
([dwh].[Region] <> [stage].[Region] OR ([dwh].[Region] IS NOT NULL
AND [stage].[Region] IS NULL) OR ([dwh].[Region] IS NULL AND
[stage].[Region] IS NOT NULL))
with
[dwh].[Region] IS DISTINCT FROM [stage].[Region]
This makes the code shorter and better readable. Learn more about this operator at
IS DISTINCT FROM (Transact-SQL)
If you have executed all snippets as described, the data
warehouse table will now look like the following:

SID
AlternateKey
Region
ValidFrom
1
0
NA
2023-07-08
10:00:06.9957244
2
1
Northwest
2023-07-08
10:00:06.9957244
3
10
United
Kingdom
2023-07-08
10:00:06.9957244
4
11
Austria
2023-07-08
10:04:56.1362259
5
1
Nordwest
2023-07-08
10:14:11.3859165
In a final step I want to test, if resurrection of previously
deleted rows works as well. Therefore, please execute the script
to initialize the source table Example 18-2 and then re-run the
load scripts Example 18-7, Example 18-8, and Example 18-9.
This will reset the data warehouse, to a state, where the active
rows match the initial state. You can clearly see that a table
with Slowly Changing Dimensions Type 2 can grow very fast in
number of rows, as every change (and re-change) creates its
own row:

SID
AlternateKey
Region
ValidFrom
1
0
NA
2023-07-08
10:00:06.9957244
2
1
Northwest
2023-07-08
10:00:06.9957244
3
10
United
Kingdom
2023-07-08
10:00:06.9957244
4
11
Austria
2023-07-08
10:04:56.1362259
5
1
Nordwest
2023-07-08
10:14:11.3859165
6
0
NA
2023-07-08
10:18:46.0263334
7
1
Northwest
2023-07-08
10:18:46.0263334

Another challenge which can happen with every dimension is
that it can store hierarchical information. Continue to read if
you want to learn how you can use SQL to transform this
information into a shape, which can be consumed easily by
Power BI.
Hierarchies
NOTE
The file used for the demonstrations is 208 Hierarchies.sql.
If you followed all best practices described in this book so far,
then you already have denormalized all natural hierarchies in
the dimension tables, as described in “Denormalizing”. With the
natural hierarchy denormalized you have all levels of the
hierarchy as columns in one single table. Adding them to a
hierarchy is very easy.
Here I want to concentrate on parent-child hierarchies. They
are very common, and for the sake of Power BI you need to
store the names of all parent levels in dedicated columns.
Therefore, you need to bring the parent-child hierarchy into a
materialized path.

Before you can create the materialized path for the hierarchy
you have to merge and expand the query as many times as
levels you have. You could do this by writing a JOIN per
(expected) level of the parent-child hierarchy - with the
backdraft of duplicated code (similar to the first, static, solution
in Power Query, described in “Hierarchies”). While in Power
Query you need a function to solve this problem in a dynamic
way, you can use a Common Table Expression (CTE) to solve this
within a query. The result delivers then one row per level. To
turn those rows into columns, you need to PIVOT  the result.
The query in Example 18-10 shows the full code.
NOTE
Common Table Expressions (CTE) can divide the code of a query in smaller parts,
which are easier to digest for the developer and the database management system. I
call CTE sometimes also “named subqueries”: you define a query, give it a name, and
then join this query in the main portion of your SELECT  statement. But there is
more: You can refer to a CTE within its own definition. This will start iterations over
the result content of the CTE. Such a (recursive) CTE must start with a query, which
does not reference itself. This is called the anchor query of the recursive CTE. Then
you UNION  this SELECT  with a second SELECT  statement, which references the
CTE. When the CTE is referenced during the first iteration, its content is simply, what
the first (anchor) query returns. During the second iteration, the CTE contains, what
the first iteration contained. And so forth. Until an iteration does not return any
rows. Or until a maximal number of iterations (which can be set via options,
including 0 for no limit) is reached.

Example 18-10.  
SELECT
	
EmployeeKey, ParentEmployeeKey,
	
convert(varchar(max), FirstName + ' ' + L
	
1 as Lvl,
	
convert(varchar(max), FirstName + ' ' + L
	
CONVERT(bit, CASE WHEN EXISTS (SELECT 1 F
FROM dbo.DimEmployee
WHERE ParentEmployeeKey IS NULL
UNION ALL
SELECT
	
child.EmployeeKey, child.ParentEmployeeKe
	
convert(varchar(max), FirstName + ' ' + L
	
parent.Lvl + 1 as Lvl,
	
convert(varchar(max), parent.[Path] + '|
	
CONVERT(bit, CASE WHEN EXISTS (SELECT 1 F
FROM dbo.DimEmployee child
JOIN PCCTE parent ON parent.EmployeeKey= child.Pa
)
SELECT *
FROM (
	
SELECT c.*, [split].[Value], 'Level ' +  
	
FROM   PCCTE AS c
	
CROSS APPLY STRING_SPLIT([Path], '|') AS 
	
) AS t
PIVOT(
	
MAX([Value])

([
])
	
FOR ColumnName
	
IN([Level 1], [Level 2], [Level 3], [Leve
	
) p
Don’t forget to add some logic to your measures, as described
“Hierarchies”, to avoid that this hierarchy can be expanded to
unnecessary levels.
Key Takeaways
In this chapter I showed you how you can apply the power SQL
to bring a data model into a star schema and solve typical use
cases. Specifically, you learned the following:
Normalizing your fact tables involves steps to find
candidates for dimensions. GROUP BY  and COUNT(*)  are
helpful to get an idea of the cardinality of a column, even
before you load it into the data model.
You create a (denormalized) dimension table by joining all
necessary tables and creating a DISTINCT  list of columns.
Don’t spend too much time on calculations in SQL. DAX is
usually the better place.
Physically adding variations for a table (for role-playing
purposes) is very easy: you just create several views with

renamed columns, based on the same query.
You can flatten parent-child-hierarchies by applying several
steps. You learned about recursive Common Table
Expressions and the PIVOT keyword in SQL.
Continue with the next chapter to learn how to support
advanced data modeling use cases in SQL.

Chapter 19. Real-world Examples
Using SQL
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 19th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
Chapter 3 introduced real-world use cases I found during
projects for my customers and how I solved them in Power BI.
Being able to “attack” a problem with different “weapons” is of
help when it comes to those challenges. You always need to be
flexible and think sometimes out-of-the box to find the right

approach. In this chapter I will demonstrate how you can
generate and shape tables in SQL for these use cases:
You will learn how you can generate both, the bin table and
the bin range table in SQL.
In order to resolve a many-to-many relationship (like for a
budget which is on different granularity than the actual
values) you need to create a bridge table. This can be done
easily in SQL.
For the demonstrated solution for multi-language reports
you need a table containing the texts for headlines, buttons,
etc. I will show you how you can pivot the table, so it
perfectly fits into the solution.
Key-value pair tables are hard to query in the shape they
are natively. Therefore, you will learn how you can pivot
the table to bring it in an analytics-friendly shape.
I will show you what role a data warehouse plays in the
concept of self-service BI vs. enterprise BI.
Binning
NOTE
The file used for the demonstrations is 301 Binning.sql.

In “Binning” I described three solutions. All of the needed tables
can be created in SQL. Remember that I would not recommend
adding the bin information into the fact table, as a change of
the bin ranges would mean updating the whole fact table. That’s
why you should look at the two other solutions only.
Deriving the Lookup Table from the Facts
Creating a DISTINCT  list of values for the lookup table (which
contains then every possible value and its bin) in SQL is easy, as
there is a dedicated keyword for that. The name of the bin can
be created with the help of the CASE  keyword. CASE  is similar
to DAX’s SWITCH  function, which I introduced in “Binning”.
CREATE OR ALTER VIEW bin.vw_QuantityBin AS
SELECT
	
DISTINCT
	
OrderQuantity,
	
CASE
	
WHEN OrderQuantity <=  5 THEN 'Small'
	
WHEN OrderQuantity <= 10 THEN 'Medium'
	
ELSE                          'Large'
	
END QuantityBin
FROM
	
PowerBI.FactResellerSales

OrderQuantity
QuantityBin
33
Large
1
Small
6
Medium
38
Large
12
Large
…​
…​
To use this bin table in a query, you join view
vw_QuantityBin  with the fact table on the OrderQuantity
column. This looks a bit unusual, as the OrderQuantity
column in the fact table is not a classic foreign key, but we use it
in such a way. This frees you from the burden to create an
additional foreign key in the fact table (which would occupy
space and would need to be maintained if the ranges of the bins
should change).
SELECT
	
frs.OrderQuantity,
	
qb.QuantityBin,

	
frs.*
FROM
	
PowerBI.FactResellerSales frs
JOIN bin.vw_QuantityBin qb ON
qb.OrderQuantity = frs.OrderQuantity
Due to a high number of distinct values, the bin table could be
huge. This is the case for the SalesAmount  column. To reduce
the number of rows, you could create this bin list not on the fact
table’s value, but e.g., on the thousandths of the value. You
simple take the code snippet from above and replace
OrderQuantity  with SalesAmount/1000 , which a aliased
SalesAmountK . The changed code is listed here:
CREATE OR ALTER VIEW bin.vw_SalesAmountBin AS
SELECT
	
DISTINCT
	
CONVERT(int, SalesAmount/1000) SalesAmoun
	
CASE
	
WHEN SalesAmount/1000 <=  5 THEN 'Small'
	
WHEN SalesAmount/1000 <= 10 THEN 'Medium
	
ELSE                             'Large'
	
END SalesAmountBin
FROM
	
PowerBI.FactResellerSales

SalesAmountK
SalesAmountBin
1
Small
6
Medium
12
Large
26
Large
…​
…​
As the goal is again to not adding columns to the fact table you
need to implement the division by 1000 in the JOIN  predicate:
SELECT
	
frs.SalesAmount,
	
sb.SalesAmountBin,
	
frs.*
FROM
	
PowerBI.FactResellerSales frs
JOIN bin.vw_SalesAmountBin sb ON
sb.SalesAmountK = CONVERT(int, frs.SalesAmount/10
Generating a Lookup Table

Depending on the size of the fact table, querying the distinct
values of the fact table to create the bin table can have a bad
performance and put pressure onto your resources, leading to a
long query duration. The following codes queries the fact table
too, but asks for the smallest and largest OrderQuantity
which can sometimes be calculated more efficient. That’s why I
included this version here as well:
CREATE OR ALTER VIEW bin.vw_QuantityBin AS
WITH
MinNumber AS (SELECT MIN(OrderQuantity) MinNumbe
MaxNumber AS (SELECT MAX(OrderQuantity) MaxNumbe
NumberTable AS (
SELECT N as Number
FROM demo.GetNumsItzikBatch(0, (SELECT MaxNumber 
SELECT
	
Number as OrderQuantity,
	
CASE
	
WHEN Number <=  5 THEN 'Small'
	
WHEN Number <= 10 THEN 'Medium'
	
ELSE                   'Large'
	
END QuantityBin
FROM
	
NumberTable

OrderQuantity
QuantityBin
0
Small
1
Small
2
Small
3
Small
4
Small
5
Small
6
Medium
7
Medium
…​
…​
Range Table
A completely different way of looking at the problem is not to
create a table containing all possible values and their bins, but
a table which specifies the ranges per bin, which a lower and an
upper value. You can create such a lookup table if you UNION
simple SELECT  statement which provide the bin’s name and
the ranges. But I guess it would be better if not a SQL developer

has the only control over the names and ranges, but some
domain user. Anyways, here is the code:
CREATE OR ALTER VIEW bin.[vw_QuantityBin Range] A
SELECT 'Small'  [QuantityBin], null [Low (incl.)
SELECT 'Medium' [SalesAmountBin],    5 [Low (incl
SELECT 'Large'  [QuantityBin],   10 [Low (incl.)
)
QuantityBin
Low (incl.)
High (excl.)
Small
NULL
5
Medium
5
10
Large
10
NULL
The created table is much smaller, but using the table in a join is
slightly more complex: You need to get the non-equi joins (>=
and <) correct and treat the NULL  in the ranges correctly:
SELECT
	
frs.OrderQuantity,

	
sb.QuantityBin,
	
frs.*
FROM
	
PowerBI.FactResellerSales frs
JOIN bin.[vw_QuantityBin Range] sb ON
	
(frs.OrderQuantity >= sb.[Low (incl.)]  O
	
(frs.OrderQuantity <  sb.[High (excl.)] O
OrderQuantity
QuantityBin
OrderDateKey
…​
…​
…​
…​
…​
3
Small
20231101
…​
3
Small
20231101
…​
6
Medium
20201201
…​
5
Medium
20201201
…​
…​
…​
…​
…​
Next, I will show you how to create the bridge table discussed
in “Budget” as a solution to avoid many-to-many relationships.

Budget
NOTE
The file used for the demonstrations is 302 Budget.sql.
Long story short in “Budget” was that you need a bridge table
between the Budget  table and the Product  table, as in none
of the two tables the Product Group  column is unique. Power
BI will only allow creating a relationship of a many-to-many
cardinality based on Product Group . Such relationships have
some side-effects in Power BI compared to one-to-many
relationships. The bridge table allows to transform the many-to-
many relationship into two one-to-many relationships.
Creating the bridge table is a very simple SELECT  statement
with DISTINCT  over the Product Group column of each table.
Then you combine these two queries with a UNION .
CREATE OR ALTER VIEW budget.ProductGroup AS
SELECT DISTINCT ProductGroup from budget.Product
UNION
SELECT DISTINCT ProductGroup from budget.Budget

ProductGroup
Group 1
Group 2
Group 3
TIP
In this case you need indeed to write a UNION  and not a UNION ALL . UNION  will
scan the result for duplicates and remove them. UNION ALL  would keep _Product
Group_s which appear in both the Product  table and the Budget  table (e.g.,
“Group 2”). The bridge table must not contain duplicates.
In the next section I take a look on how to implement pivoting
with SQL.
Multi-language Model
NOTE
The file used for the demonstrations is 303 Localized model.sql.
In this section I will concentrate on the Textcomponent  table,
which contains the parts of a report (headlines, buttons, etc.)

which are usually static. Such a table for two texts (Sales
Overview and Sales Details) and languages English (“EN”) and
Klingon (“tlh-Latn”) could look like the following:
CREATE OR ALTER VIEW [language].[TextComponent] A
(
SELECT 'EN' [LanguageID], 'SalesOverview' [TextCo
UNION ALL
SELECT 'EN' [LanguageID], 'SalesDetails' [TextCom
UNION ALL
SELECT 'tlh-Latn' [LanguageID], 'SalesOverview' 
UNION ALL
SELECT 'tlh-Latn' [LanguageID], 'SalesDetails' [T
)
LanguageID
TextComponent
DisplayText
EN
SalesOverview
Sales Overview
EN
SalesDetails
Sales Details
tlh-Latn
SalesOverview
QI’yaH
tlh-Latn
SalesDetails
qeylIS belHa’

This table can be used very easily after it was pivoted (= one
column per text, one row per language). As it will be in an
active one-to-many relationship with the Language  table, only
one row (= one language) will be available at query time.
Fortunately, SQL provides the PIVOT  keyword to transform
rows into columns. You need to provide the following:
An aggregate function and the name of the column which
content you want pivot into different columns. For text
columns you choose either the MIN  or the MAX  function,
which will return the alphabetically first or last text, in case
there is more than one row available for a text. Even if you
are convinced that this is not the case, to use PIVOT  it is
mandatory to provide such an aggregate function.
The name of the column whose content should be
transformed into column names, after the FOR  keyword.
A list of the new column names (= a list of the expected
content of the column name, whose content should be
transformed into column names) provided after the IN
keyword.
The code looks straightforward:
CREATE OR ALTER VIEW [language].[vw_TextComponent
SELECT

	
*
FROM
	
[language].[TextComponent] tc
PIVOT
(
	
MIN([DisplayText])
	
FOR [TextComponent]
	
IN (
	
	
[SalesOverview],
	
	
[SalesDetails]
	
)
) p
LanguageID
SalesOverview
SalesDetails
EN
Sales Overview
Sales Details
tlh-Latn
QI’yaH
qeylIS belHa’
On top of my wish list for the SQL syntax is to be able to provide
the list of column names for the IN  keyword as a subquery.
Unfortunately, this is not supported by the language. Therefore,
you need to either manually update the code every single time a
new value for TextComponent  was added to the table. Or you
reach out to dynamic SQL to generate the whole SELECT

statement including the list of column names, as you can see
here:
DECLARE
	
@CRLF nvarchar(MAX) = CHAR(13)+CHAR(10),
	
@cmd nvarchar(MAX),
	
@ColumnNameList nvarchar(max)
SELECT
	
@ColumnNameList   = STRING_AGG([ColumnNam
FROM
	
(SELECT
	
	
DISTINCT
	
	
CONVERT(nvarchar(max), [TextCompo
	
FROM
	
	
(
	
	
SELECT
	
	
	
DISTINCT
	
	
	
QUOTENAME(TRIM(tc.[TextCo
	
	
FROM [language].[TextComponent] t
	
	
) k
	
) x;

-- VIEW
SET @cmd=N'
CREATE OR ALTER VIEW [language].[vw_TextComponent
AS
(
/*** DO NOT MAKE ANY CHANGES DIRECTLY ***/

/*** This code was generated ***/
SELECT *
FROM [language].[TextComponent] tc
PIVOT
(
	
MIN([DisplayText])
	
FOR [TextComponent] IN (
'	
+ @ColumnNameList + N'
	
)
) as p
'
exec sp_executesql @stmt = @cmd
WARNING
When using dynamic SQL always pay attention that SQL injection is prohibited. You
find a good guide for this topic at SQL Injection.
In the code examples shown in this book the problem of SQL injection is rather
neglectable, as the injected text is not an input from an (possible untrustful) user, but
from a key-value pair table, delivered from an application and from a lookup table
maintained by you. On top of that, function QUOTENAME will wrap every key inside
of brackets. In case the name of a key would contain harmful code, it would just not
be executed, but treated as part of the key’s name only.
I first declared the variables: one to contain the two characters
for carriage return and line feed ( @CRLF ), so that list of column

names can be put one per line. Variable @cmd  hosts the whole
SQL statement to create the view. And @ColumnNameKey  will
contain the list of column names, which is used for the IN
clause for PIVOT .
The @ColumnNameList  is propagated in the inner SELECT
with DISTINCT  values of the TextComponent  column. The
values are trimmed (function TRIM ) to cutoff leading and
trailing whitespace. Function QUOTENAME  wraps the content
inside brackets ([]) and will add escape characters in case the
TextComponent  contains any ]-character itself. I then append
@CRLF  and convert the content explicitly to a varchar(max) so
no column name is cutoff. In the outmost SELECT  I aggregate
all the column names with function STRING_AGG  and a comma
(“,”) as a separator in alphabetical order.
The @cmd  variable is then assigned to a string containing the
full CREATE OR ALTER VIEW  command, where I replace a
hardcoded list for IN  with the content of @ColumnNameList .
Here is the generated command:
CREATE OR ALTER VIEW [language].[vw_TextComponent
AS
(

--/*** DO NOT MAKE ANY CHANGES DIRECTLY ***/
--/*** This code was generated ***/
SELECT	 *
FROM [language].[TextComponent] tc
PIVOT
(
	
MIN([DisplayText])
	
FOR [TextComponent] IN (
[SalesDetails]
, [SalesOverview]

	
)
) as p
In the next section you will face a similar challenge: pivoting a
table. Additionally it will be necessary to find the correct data
type for the columns, which adds a bit of complexity to the
problem.
Key-Value Pair Tables
NOTE
The file used for the demonstrations is 304 Key Value.sql.

The problem for the key-value pair table is similar to the
problem of the TextComponent  table – it needs to be pivoted.
There is an additional challenge, though. While all columns in
the pivoted TextComponent  table are of a string data type
( varchar ), the types for the columns of the pivoted key-value
pair table can be of any data type.
Here is the table:
CREATE TABLE [dwh].[KeyValue] (
	
[_Source] varchar(20),
	
[ID] int,
	
[Key] nvarchar(3000),
	
[Value] nvarchar(3000),
	
[Type] nvarchar(125)
)
;

INSERT INTO [dwh].[KeyValue] VALUES
('[dwh].[KeyValue]', 1,	'name'	 	
				
('[dwh].[KeyValue]', 1,	'city'	 	
			
('[dwh].[KeyValue]', 2,	'name'	 	
				
('[dwh].[KeyValue]', 2,	'city'	 	
			
('[dwh].[KeyValue]', 3,	'name'	 	
			
('[dwh].[KeyValue]', 3,	'city'	 	
			
('[dwh].[KeyValue]', 1,	'revenue'	
			

('[dwh].[KeyValue]', 2,	'revenue'	
			
('[dwh].[KeyValue]', 3,	'revenue'	
				
('[dwh].[KeyValue]', 1,	'firstPurchase'	,'1980-0	
('[dwh].[KeyValue]', 2,	'firstPurchase'	,'2000-0	
('[dwh].[KeyValue]', 3,	'firstPurchase'	,'2021-0	
('[dwh].[KeyValue]', 1,	'zip'	
	
				
('[dwh].[KeyValue]', 2,	'zip'	
	
				
('[dwh].[KeyValue]', 3,	'zip'	
	
				

_Source
ID
Key
Value
Typ
[dwh].
[KeyValue]
1
name
Bill
text
[dwh].
[KeyValue]
1
city
Seattle
text
[dwh].
[KeyValue]
2
name
Jeff
text
[dwh].
[KeyValue]
2
city
Seattle	text
[dw
[Key
3
name
Markus
text
[dw
[Key
3
city
Alkoven
text
[dw
[Key
1
revenue
20000
Int64.Type
[dw
[Key
2
revenue
19000
Int64.Type
[dw
[Key

_Source
ID
Key
Value
Typ
3
revenue
5
Int64.Type
[dw
[Key
1
firstPurchase
1980-01-01
date
[dw
[Key
2
firstPurchase
2000-01-01
date
[dw
[Key
3
firstPurchase
2021-01-01
date
[dw
[Key
1
zip
0100
text
[dw
[Key
2
zip
0200
text
[dw
[Key
I will first teach you how to write the static code before I jump
to dynamic SQL here as well. When you apply the PIVOT
keyword in the same way as in the previous section (Multi-
lingual Model) you will be disappointed as you will get more
rows than expected with many column values being NULL .

-- PIVOT
SELECT
	
*
FROM
	
[dwh].[KeyValue] kv
PIVOT
(
	
MIN([Value])
	
FOR [Key]
	
IN (
	
	
[name],
	
	
[city],
	
	
[revenue],
	
	
[firstPurchase],
	
	
[zip]
	
)
) p

_Source
ID
Type
name
city
[dwh].
[KeyValue]
1
date
NULL
NUL
[dwh].
[KeyValue]
1
Int64.Type
NULL
NUL
[dwh].
[KeyValue]
1
text
Bill
Seatt
[dwh].
[KeyValue]
2
date
NULL
NUL
[dwh].
[KeyValue]
2
Int64.Type
NULL
NUL
[dwh].
[KeyValue]
2
text
Jeff
Seatt
[dwh].
[KeyValue]
3
date
NULL
NUL
[dwh].
[KeyValue]
3
Int64.Type
NULL
NUL

_Source
ID
Type
name
city
[dwh].
[KeyValue]
3
text
Markus
Alko
You could GROUP BY  column ID  and apply function MIN  on
all pivoted columns. Before you do that, let’s think about why
you receive these extra rows. The reason for this behavior can
be found in the Type  column. There are several Type  s per
ID . Removing column Type  from the query will also remove
the extra rows. For the sake of pivoting you need to tell SQL to
ignore this column. That’s why I rewrote the query with a sub-
select in the first FROM  clause, replacing the simple reference
to the table. The sub-select queries only the necessary columns
( ID , Key , and Value ):
-- PIVOT without Type column
SELECT
	
*
FROM
	
(SELECT [ID], [Key], [Value] FROM [dwh].
PIVOT
(
	
MIN([Value])
	
FOR [Key]
	
IN (

	
IN (
	
	
[name],
	
	
[city],
	
	
[revenue],
	
	
[firstPurchase],
	
	
[zip]
	
)
) p
ID
name
city
revenue
first
1
Bill
Seattle
20000
1980
2
Jeff
Seattle
19000
2000
3
Markus
Alkoven
5
2021
The next improvement is that you should get the data types
right. PIVOT  will keep the data type of the Value  column and
will make all columns in the result set a VARCHAR(3000) . As
there is simply no guarantee that the Value  for a Key  really
sticks to the expected data type (described in the Type
column), I prefer to use function TRY_CONVERT . In case a value
cannot be converted to the desired data type, this function does
not throw an error (as function CONVERT  would do), but simply

returns NULL  as the result after conversion. You need to decide
on your own, if you prefer an error or NULL .
Here is the improved code for the VIEW :
CREATE OR ALTER VIEW [PowerBI].[KeyValue]
AS
(
SELECT
	
p.ID [ID]
	
, TRY_CONVERT(NVARCHAR(3000),	
[city])				
	
, TRY_CONVERT(DATE,	
	
	
		
	
, TRY_CONVERT(NVARCHAR(3000),	
[name])				
	
, TRY_CONVERT(BIGINT,	
	
				
	
, TRY_CONVERT(NVARCHAR(3000),	
[zip])	 			
FROM (SELECT [ID], [Key], [Value] FROM [dwh].[Key
PIVOT
	
(MIN([Value]) FOR [Key] IN (
	
  [city]
	
, [firstPurchase]
	
, [name]
	
, [revenue]
	
, [zip]
	
)
) as p
)

I guess you already see the backdraft of this static code: If new
keys are added, you always need to adopt the definition of the
VIEW . You need to maintain the column list in the projection
( SELECT ) and in the PIVOT  list. You also need to choose the
right data type per column. If over time many new keys are
added, maintaining the definition of the VIEW  can take up a lot
of your time. That’s where dynamic SQL can come in. First, I
create a table, which matches the content of the Type  column
to a data type available in SQL:
-- KeyType
DROP TABLE IF EXISTS [KeyValue].[KeyType];
CREATE TABLE [KeyValue].[KeyType] (
	
KeyType nvarchar(128),
	
KeyDescription nvarchar(128),
	
DataType nvarchar(128)
)
INSERT INTO [KeyValue].[KeyType] VALUES
(N'text',	
	
	
N'Text',	
				
(N'Int64.Type',		
N'Whole Number',	
		
(N'number',	
	
	
N'Decimal Number			
(N'currency',	
	
N'Fixed Decimal Number',	
(N'Percentage',		
N'Percentage',	 	
		
(N'datetime',	
	
N'Date/Time',	
	
		
(N'date',	
	
	
N'Date',	
				
(N'time',	
	
	
N'Time',	
				
(N'datetimezone',	
N'Date/Time/Timezone',	 	

(N'duration',	
	
N'Duration',	
	
		
(N'logical',	
	
N'True/False',	 	
		
(N'binary',	
	
	
N'Binary',	
				
KeyType
KeyDescription
DataType
text
Text
NVARCHAR(3000)
Int64.Type
Whole Number
BIGINT
number
Decimal Number
DOUBLE
currency
Fixed Decimal Number
DECIMAL(19,4)
Percentage
Percentage
DECIMAL(19,4)
datetime
Date/Time
DATETIME2
date
Date
DATE
time
Time
TIME
datetimezone
Date/Time/Timezone
DATETIMEOFFSET
duration
Duration
DOUBLE
logical
True/False
BIT
binary
Binary
VARCHAR(max)

TIP
As a side effect, this table gives also a good overview about which data type in Power
Query is compatible with which data type in SQL.
Second, I create a stored procedure which creates the VIEW  for
me. The first step in the procedure is to LEFT JOIN  the key-
value pair table ( KeyValue ) and the lookup table for the data
types ( KeyType ). In case a data type cannot be found, I default
to NVARCHAR(3000) . With function STRING_AGG  I
concatenate the list of keys to a comma-separated list. I do this
twice: Once a plain list, which is later used for the IN  clause of
the PIVOT keyword, and a second one, with TRY_CONVERT
added for the projection. The variables containing the result of
the aggregation are then injected inside the CREATE OR ALTER
VIEW  definition:
CREATE or ALTER PROC [KeyValue].[CreateViewKeyVal
	
@_Source varchar(50),
	
@debug bit = 0
)
AS
BEGIN

SET NOCOUNT ON;

DECLARE

DECLARE
	
@CRLF nvarchar(MAX) = CHAR(13)+CHAR(10),
	
@cmd nvarchar(MAX),
	
@ColumnNameKey nvarchar(max),
	
@ColumnNamePivot nvarchar(max)
SELECT
	
@ColumnNameKey   = STRING_AGG([ColumnName	
	
@ColumnNamePivot = STRING_AGG([ColumnName	
FROM
	
(SELECT
	
	
DISTINCT
	
	
CONVERT(varchar(max), [Key] + @CR					
	
	
CONVERT(varchar(max),
	
	
	
N'TRY_CONVERT(' + [DataTy
	
	
	
+ N') AS ' + [Key] + @CRL						
	
FROM
	
	
(
	
	
SELECT DISTINCT
	
	
	
kv.[_source]
	
	
	
,QUOTENAME(TRIM(kv.[key]
	
	
	
,ISNULL(kt.[DataType], 'N
	
	
FROM [dwh].[KeyValue] kv
	
	
LEFT JOIN [KeyValue].[KeyType] kt
	
	
) k
	
) x;

-- VIEW
SET @cmd=N'
CREATE OR ALTER VIEW [PowerBI].[KeyValue]

AS
(
/*** DO NOT MAKE ANY CHANGES DIRECTLY ***/
/*** This code was generated ***/
SELECT
	
p.ID [ID],
'	
+ @ColumnNamePivot + N'
FROM (SELECT [ID], [Key], [Value] FROM [dwh].[Key
PIVOT
	
(MIN([Value]) FOR [Key] IN (
'	
+ @ColumnNameKey + N'
)
	
) as p
)
'
if @debug = 1 exec [KeyValue].[Print] @cmd;
exec sp_executesql @stmt = @cmd

END
Now you mastered a rather complex topic in SQL! This chapter
concludes with discussing how to combine the world of self-
service and enterprises, when it comes to a data warehouse.

Combining Self-Service and
Enterprise BI
I assume that the idea of many programming languages in the
1960s and 1970s was that they should enable anyone, who can
write a natural language, to control a computer. SQL is no
exception here. The language is derived from the English
language including the grammar. That’s why it starts with the
SELECT  key word: SELECT  Book FROM  Library. (Intellisense
would appreciate if we would start with the FROM  key word
instead, as e.g., it is implemented in .NET’s LINQ, because it
would make it easier for Intellisense to support you with
helpful suggestions). A couple of decades later we can conclude
that this plan did not work out. Only (some?) people from IT
and a few power users are capable of writing SQL statements to
receive useful answers from a relational database.
With that said, implementations in SQL are clearly part of an
Enterprise BI environment. This chapter had though
demonstrated that you can utilize the power of SQL to model
and transform a data model so it makes the end-users life easy,
when working with tools like Power BI.

Key Takeaways
The chosen use cases for this book demonstrated that SQL is a
very powerful language. Combined with the possibility to
generate dynamic SQL you can write resilient code and
(semi-)automate a lot of processes. Therefore, I prefer a
relational data warehouse (layer) for these tasks, if available for
my customer. Specifically you learned the following features of
the language:
You can use a combination of DISTINCT  and CASE  to
build a static lookup table for the desired bins. This table
you can directly join with the fact table. Alternatively, you
can UNION  a couple of SELECT  statements to build a table
containing the ranges for each bin. In SQL you can write
non-equi joins to combine this information with the fact
table. In Power BI you can re-use this table, but must write
a measure in DAX to implement the non-equi join (as
described in Chapter 11)
For the Budget problem I prefer to create a bridge table,
which contains simply the distinct values of the Product
Group column. This table has then two one-to-many
relationships, replacing to original many-to-many
relationship.

For the TextColumn you learned that you can use PIVOT .
This keyword needs a static list of the pivoted column’s list,
though. Therefore, I introduced you to dynamic SQL where
you can dynamically inject the list of columns into the
SELECT  statement.
In the key-value pair scenario applying PIVOT  is necessary,
too. Additionally, you need to take care of assigning the
appropriate data types to the columns. In case a data type
per key is available, you learned how you can extend the
dynamic SQL solution and inject the right data type
conversions as well.
When the discussion comes to SQL and relational
databases, it is clearly about a Enterprise BI environment.
The goal should not be to teach domain users the secrets of
SQL, but to build (with SQL) a data model which can easily
consumed by the domain experts in tools like Power BI
Desktop, Power BI Report Builder, or Excel.
Last, but not least, I will show you how you can improve the
performance of your data model with features in SQL.

Chapter 20. Performance Tuning and
Data Model with SQL
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form
—the author’s raw and unedited content as they write—so you
can take advantage of these technologies long before the official
release of these titles.
This will be the 20th chapter of the final book. Please note that
the GitHub repo will be made active later on.
If you have comments about how we might improve the content
and/or examples in this book, or if you notice missing material
within this chapter, please reach out to the editor at
sevans@oreilly.com.
In Chapter 4 and Chapter 8 you have already learned that you
have several options to optimize query response time through
decisions on the storage mode of the tables in your Power BI
data model. Physics mandate that you can exchange faster
query response time by using more disk space – and the other
way around. This is not only true for Power BI and Analysis

Services, but for all database systems, including relational
databases. In this chapter I will describe the options you have
in relational databases to exchange storage space against query
response time and what you can do in the relational world to
support query speed inside Power BI and Analysis Services.
Storage Mode
NOTE
The file used for the demonstrations is 401 Data vs Query.sql.
I kept Power BI’s term “storage mode” as a title for this section,
even when it is typically not used when talking about relational
databases. In relational databases people will talk about either
persisting data in the database or not. Persisting means to
actually use disk space to store information in a certain form
and shape, as opposed to only storing a query string. If the data
is already stored in the right shape on disk, it only has to be
transferred from the disk into memory and then sent over the
network to the client. If only a query string is stored, all the
mentioned has to be done as well, but on top, resources (CPU,
memory, disk IO) are needed fetch the data from different
tables, join it and apply the one or another transformation on it.

In a relational database you have the following options:
Table
You can create a table in a relational database and then
INSERT , UPDATE  and DELETE  rows in it. By storing
information in a table you can apply complex
transformations already when you insert a row into the
table. Such a transformation will make updating the
content of the table slower but will improve the time it
will take to query the information later. Only when the
content of the table is read multiple times, it is worth
persisting the data into a table. If the sole purpose of the
information is to feed an analytical system (like Power BI
or Analysis Services) it is usually not worth the effort of
triggering a job to persist the data and spend disk space
on storing the information. Persisting the data will
increase the overall time to transfer the data from the
data source into the analytical system. If certain users
(e.g., data scientists) or tools (which can send requests in
SQL, but not the analytical systems’s query language) are
supposed to read regularly from the relational system it is
worth to persist the data and implement an index strategy
on the tables. Another use case to persist the data, if you
are planning to implement a delta load based on

partitioning. Only when the partitions between Power BI
and the relational database are aligned, you can expect a
faster refresh of the Power BI semantic model. The same
is true, if you decided against importing the content of a
table into Power BI, but keep the table in DirectQuery
mode.
Index
An index on a table is like the index at the end of this
book: It is extra space reserved to an ordered list of items
to speed up the time it takes to find a certain information
in this book. While a book usually only contains a single
index, a table can have several indexes. Microsoft’s SQL
databases (Azure SQL DB, Azure SQL Managed Instance,
SQL Server, etc.) are offering the following types of
indexes:
You can have one single CLUSTERED INDEX  per table.
The specialty of a clustered index is that it does not use
extra storage space, but puts the rows of the table itself
in the order of the index column(s). This is like the
order of the chapters in this book. The table of content
at the beginning of this book points out on which page
you are going to find the information for a certain
chapter or a section. As the rows of a table can only be

put into one single order, there can only be one single
CLUSTERED INDEX  per table. Also, a book can only be
physical ordered after one single criteria. If you
further need to improve queries (based on filters and
groupings of other columns), then you can add
+NONCLUSTERED INDEX+es to the table. These use
extra storage space (like the index at the end of this
book) for the index columns and a row identifier. The
row identifier is like the page number in the index of
this book. When using +NONCLUSTERED INDEX+es the
database management system needs first to find the
entry in the index and then lookup the row in the table
in case the query asks for non-indexed columns. You
can avoid the lookup by including further columns in
the index. The index is not sorted by those included
columns. Operations which manipulate the content of
the table will maintain the index automatically but will
take extra effort to keep the index ordered and the
content of the included columns up to date. Again, you
are exchanging storage space (and slower write
operations) for improved query time.
When it comes to a physical implementation of a
CLUSTERED  or a NONCLUSTERED  index, the most
common form in relational databases is a so-called B-

Tree. You can imagine a B-Tree as a tree put upside-
down, with the individual rows of the table on the
bottom (called leaves). If I ask you to guess which
number between 1 and 100 I have in mind, you
probably will start by asking if it is greater or lower
than 50 (which is half the way between 1 and 100). If
my answer is “bigger”, than you will most probably ask
if the number is greater or lower than 75 (which is
halve the way between 50 and 100) and so on. This
search algorithm is called a binary search. You can
imagine the structure of a B-Tree index that it is
created in a way to support exactly this algorithm. As a
rule of thumb you should create at least one single
index on a table (that would be on the primary key of a
table and its foreign keys), when it comes to analytical
databases.
Technically the Columnstore index is not an index, but
a completely different storage engine inside Microsoft’s
relational databases. As opposed to “normal” tables
and indexes, which store the information of all
columns for a single row physically together, a
columnstore splits up rows into their columns and
store the content of the columns physically together (as
you might have guessed from the name). These blocks

of similar data can be compressed very efficient. You
can create both, a CLUSTERED  or an NONCLUSTERED
columnstore index on a table. As a rule of thumb, you
should add a clustered columnstore index on all fact
tables and big dimension tables in an analytical
database. The high compression rate of this type of
index (which typically squeezes the data to a tenths of
its size without losing any information) is very
welcomed on these big tables.
NOTE
Under the hood, a Columnstore index is the same as the storage engine
behind Power BI and Analysis Services. Any whitepaper written on the
Columnstore index will explain to you well, how Power BI’s VertiPaq is
implemented. In theory, a DirectQuery connection on a data model hosted
in a table with a clustered Columnstore index should be as fast as having
the same data available in Import mode. Practically, there are small but
crucial differences, which will make queries against a Columnstore index
slower: a Columnstore index does not need to fit into memory (as it is the
case with a data model in Power BI and Analysis Services) but can be
paged to the disk. And you can update, insert and delete individual rows
of Columnstore index, while you only can refresh a full partition of a
Power BI and Analysis Services data model. This makes the Power BI
implementation of this storage engine superior over the columnstore
index of a relational database. Therefore, when you need to use
DirectQuery, make sure that you apply the right index strategy. But
import everything into a Power BI semantic model if it is possible, to
achieve the best query performance for your data model.

Compression
For all tables, where a clustered ColumnStore index is not
feasible (for example, if it contains less than one million
rows) you should turn on PAGE compression. This will not
only save you disk space, but usually decreases query
time as well. The reason is that for most servers the disk
transfer is the bottleneck and CPU tends to be
underutilized. Compression will increase overhead to the
CPU (for compressing and uncompressing the data), but
less data will be transferred from and to the disk, thus the
overall query performance will improve.
View
A VIEW  is simply a SELECT  statement stored under a
certain name in the database. It can be queried in the
same way a table can. If you do not look at the meta-data
of the database object, you will not recognize any
difference in syntax. Only query time might be slower, if
the SELECT  statement of the definitioon of the VIEW
contains complex queries, complex transformation and/or
the underlying tables are not indexed well enough.
Especially, when the sole purpose of the data warehouse
(layer) is to provide a data model for Power BI or Analysis
Services, there is no need to persist the data, as it will only

be read once per refresh. Not even an index is necessary,
if you do full refreshes only. In such a case, putting all
transformations into a VIEW  is more than sufficient. Only
when the data will be read more often (from different BI
tools or data analysts, data scientist or similar) it might
make sense to persist the data), as pointed out when I
explained the advantages and disadvantages of a table.
NOTE
Azure Synapse Serverless offers you to create a table as a SQL query (CTAS).
The result of the query will be persisted in a parquet file on the data lake.
Storage costs on the data lake are very cheap compared to the storage costs in
a relational database. Azure Synapse Serverless lets you utilize these cheap
costs in exchange for increased computing costs on the database side to
collect (and transform) all the data from text files. The costs for the data in
rest may be close to be neglectable, but every query on such a table (e.g., to
refresh your Power BI/Azure Analysis Services data model) will cost you.
Function
There are different types of functions available in T-SQL.
Table-valued functions can be queried similar to tables
and views, with the difference that you need to write
mandatory parentheses after the name. Inside the
parentheses you specify parameters, if applicable. I
therefore tend to call table-valued functions

“parametrized views”. The provided parameter(s) can
help to optimize the query statement inside the definition
of the function. Functions are not listed, when you
connect to the database from Power BI, but you need to
choose to provide a SQL statement for the data source,
which then contains the function’s name in the FROM  or
JOIN  portions.
Stored Procedure
A stored procedure can contain even very complex T-SQL
code. Again, a procedure might have parameters, which
might allow to optimize the code inside. A procedure
cannot be part of a query, but you need to execute it.
Stored procedures are not listed when you connect to the
database from Power BI, but you need to choose to
provide a SQL statement for the data source and then
provide the name of the procedure plus its parameters
(and no SELECT  statement).
In the next section I introduce you to a different technique,
which can speed up queries and your ETL process.
Partitioning

NOTE
The file used for the demonstrations is 403 Partitioning.sql.
In case the refresh of the Power BI/Analysis Services data model
takes too much time, you should invest time in a good
partitioning concept (either on the table you are directly
importing or on the underlying table of a view, function or
stored procedure). It is important that the partition strategy
between the relational table and the table in Power BI/Analysis
Services is aligned. “Aligned” means that both use the same
partition key. If you only partition the latter (or use a partition
key which is different) then the refresh time might not improve
at all, because still the full source table must be scanned to
provide the rows needed for the refreshed partition. It might
even take longer, as the refresh of every partition in Power BI
may trigger a full scan of the whole table of Power BI’s data
source.
Tables in DirectQuery mode will also benefit from partitioning,
when the partition key is also part of the queries filter, as whole
partitions can be ruled out and less data needs to be scanned. If
the partition key is not part of the filter, then you should not
expect any backdrafts. A full scan over a partitioned table or a
non-partitioned table will take the same time. So, implementing

the right index is crucial to avoid full table scans also in the
case of partitioned tables.
Partitioning will also help in scenarios, where you persist data
in a relational data warehouse. The recommendation is to use
the timestamp of creation of the row in the data source as the
partition key, as it will not change. If you need to reload data for
a certain period of time, you can easily switch out the partition
in question into a table of its own, truncate this table, insert the
new version of these rows into this table and switch the
partition back into the original table. All operations, except for
the insert, are so-called “meta data operations” which can be
done very quickly (and the duration is independent from the
number of rows involved). Microsoft’s online documentation
will provide you with the necessary details to implement
partitioning (Create partitioned tables and indexes).
From a technical perspective it is important to understand that
you need to create a PARTITION FUNCTION  which contains a
list of values for the partition key and is used to assign a row of
the table to a certain partition. Each value provided during the
definition of the PARTITION FUNCTION  is then either the left
or the right border of the partition: RANGE LEFT  means that
the first partition will contain values lower or equal the first
value of the PARTITION FUNCTION  (= the value is part of the

partition left of it); RANGE RIGHT  means that the first partition
will contain values lower the first value of the partition
function (and the value mentioned in the PARTITION
FUNCTION  is part of the partition right to it). So, the type
specifies if the border’s value are part of the partition left or
right of the border. You have always one partition more, than
values listed in the PARTITION FUNCTION .
Then you need to create a PARTITION SCHEMA  which utilizes
the PARTITION FUNCTION . To make a table partitioned you
need to create it ON  this PARTITION SCHEMA . Here you see
these commands in action.
First I create a PARTITION FUNCTION  of name pfOrderDate
which accepts partition keys of data type datetime2(0) ,
which means dates including timestamps with a precision to
the second. It is important that this data type matches the data
type of the column you will use as a partition key. The function
has three values. All partition keys before September 1st 2023
will be assigned to the first partition (due to RANGE RIGHT
September 1st 2023 is part of the partition right of it).
CREATE PARTITION FUNCTION pfOrderDate (datetime2
     AS RANGE RIGHT FOR VALUES
     ('2023-09-01', '2023-10-01', '2023-11-01');

As the function defines three borders, there will be four
partitions. The RANGE RIGHT  or RANGE LEFT  option only
defines if the value of the border itself is part of the partition
right or left of the border (Figure 20-1).
Figure 20-1. Partition RANGE RIGHT vs. RANGE LEFT
TIP
Technically it makes no difference if you choose RANGE RIGHT  or RANGE LEFT . But
if your partition key is somehow related to a point in time (date or datetime2) I
would strongly recommend opting for RANGE RIGHT , as you want to have all rows
for a day or month in the same partition. With RANGE LEFT  rows for midnight/first
day of the month would be in a different partition than rows for the same day, but
with a later point in time, or later in the month as you can see in Figure 20-1.

Next, I create a PARTITION SCHEME  named psOrderDate ,
which references the newly created PARTITION FUNCTION  of
name pfOrderDate . In the example, all partitions will be
hosted in the PRIMARY  file group of the database. Alternatively,
you could provide a list of FILEGROUP . The first partition
would be then hosted in the first FILEGROUP , the second
partition in the second FILEGROUP , etc. In the definition of a
FILEGROUP  you specify in which physical file on which
physical drive the data is stored. This enables you to put e.g., the
partitions containing the newest data, which are updated and
queried often, on a fast SSD drive, while the partitions for older
years, which are never updated and queried very rarely, may
reside on cheaper storage. You find more information about
managing filegroups at Database Files and Filegroups.
CREATE PARTITION SCHEME psOrderDate
    AS PARTITION pfOrderDate
    ALL TO ('PRIMARY') ;
Last, but not least, I create a table dbo.PartitionTable
which I put ON  the newly created PARTITION SCHEMA  of
name psOrderDate . As the parameter for the PARTITION
SCHEMA  I pass OrderDate . The content of OrderDate  will
decide for each row, in which partition the row will be stored -

it is my partition key. Having this column as the first (or only)
column of the PRIMARY KEY  and any index will further speed
up queries, as they are then aligned with the partition:
CREATE TABLE partitioning.FactResellerSales (
     [OrderDate] datetime2(0),
     [SalesAmount] decimal(19, 2)
)
    ON psOrderDate (OrderDate) ;
Let’s insert some rows and find then out, in which partitions
they landed:
INSERT INTO partitioning.FactResellerSales
SELECT OrderDate, SalesAmount
FROM PowerBI.FactResellerSales
I group the rows of this table by OrderDate  and count the
rows:
SELECT
     OrderDate,
     COUNT(*) [RowCount]
FROM
     PowerBI.FactResellerSales
GROUP BY

     OrderDate
ORDER BY
     OrderDate;
OrderDate
RowCount
…​
…​
2023-07-01 00:00:00.000
2076
2023-08-01 00:00:00.000
2177
2023-09-01 00:00:00.000
1797
2023-10-01 00:00:00.000
2847
2023-11-01 00:00:00.000
3004
The expectation is that a lot of rows were added to the first
partition (which contains everything before September 1st
2023), but only 1797 rows are put into the second partition (on
or after September 1st 2023), 2847 rows into the third partition
(on or after October 1st 2023) and 3004 rows into the fourth
partition (on or after November 1st 2023). The following query
proofs this successfully. It joins metadata tables from the
database’s sys  schema ( schemas , tables , indexes ,
partitions , filegroups , destination_data_space  and

filegroups ) and then shows the schema’s name, the table’s
name, the partition number, the filegroup and the row count:
Example 20-1. Count rows per partition
-- Count rows per partition
SELECT
	
s.[name] AS SchemaName
	
, t.[name] AS TableName
	
, ds.[name] AS PartitionScheme
	
, p.partition_number AS PartitionNumber
	
, COALESCE(f.[name], d.[name]) AS [FileG
	
, p.[rows] AS [RowCount]
FROM
    sys.schemas AS s
INNER JOIN sys.tables AS t ON
	
t.schema_id = s.schema_id
INNER JOIN sys.partitions AS p ON
	
p.object_id = t.object_id
INNER JOIN sys.indexes AS i ON
	
i.[object_id] = p.[object_id] AND
	
i.index_id = p.index_id
LEFT JOIN sys.index_columns AS ic ON
	
ic.[object_id] = i.[object_id] AND
	
ic.index_id = i.index_id
LEFT JOIN sys.columns AS c ON
	
c.[object_id] = ic.[object_id] AND
    c.column id = ic.column id

_
_
LEFT JOIN sys.data_spaces AS ds ON
	
ds.data_space_id = i.data_space_id
LEFT JOIN sys.data_spaces AS ds ON
	
ds.data_space_id = i.data_space_id
LEFT JOIN sys.partition_schemes AS ps ON
	
ps.data_space_id = ds.data_space_id
LEFT JOIN sys.partition_functions AS pf ON
	
pf.function_id = ps.function_id
LEFT JOIN sys.filegroups AS f ON
	
f.data_space_id = i.data_space_id
LEFT JOIN sys.destination_data_spaces AS dds ON
	
dds.partition_scheme_id = i.data_space_id
	
dds.destination_id = p.partition_number
LEFT JOIN sys.filegroups AS d ON
	
d.data_space_id = dds.data_space_id
LEFT JOIN sys.partition_range_values AS prv_left 
	
ps.function_id = prv_left.function_id AND
	
prv_left.boundary_id = p.partition_numbe
LEFT JOIN sys.partition_range_values AS prv_right
	
ps.function_id = prv_right.function_id AN
	
prv_right.boundary_id = p.partition_numbe
WHERE
	
s.[name] = 'partitioning' AND
	
t.[name] IN ( 'FactResellerSales', 'FactR
	
t.[type] = 'U' AND
	
i.index_id IN (0, 1)
ORDER BY
      s.[name]

[
]
	
, t.[name]
    , p.index_id
    , p.partition_number;
SchemaName
TableName
PartitionScheme
Partitio
partitioning
FactResellerSales
psOrderDate
1
partitioning
FactResellerSales
psOrderDate
2
partitioning
FactResellerSales
psOrderDate
3
partitioning
FactResellerSales
psOrderDate
4
As already explained, a partitioned table can be queried faster,
if the partition key is part of the filter. Even if no index is
available to satisfy the search condition, only the partition
needs to be scanned for the rows, instead of the full table.
Partitioning is also helpful, when you need to reload data into
the table for a specific day. Without partitions per day, you
would need to either DELETE  and INSERT  all the rows for this
day (= full load) or implement a logic to remove outdated rows,
insert new rows and update changed rows (delta load). All this
operations can take a while to complete, when executed on a
large table.

With partitioning you can tremendously speed up the process
of reloading the data for one day (assuming that you use a date
as the partition key of your table). First you create an empty
copy of the table, which will host the rows which should be
updated. In the following code I use the INTO  clause to create
the new table (with postfix _STAGE ) and select the TOP 0
rows (= no data) for all columns ( * ). You need to make sure
that the newly created table is put on the same file group as the
partition scheme.
DROP TABLE IF EXISTS partitioning.FactResellerSal
SELECT TOP 0 *
INTO partitioning.FactResellerSales_STAGE ON [PRI
FROM partitioning.FactResellerSales;
Then I SWITCH  a specific partition out of the original table
( partitioning.FactResellerSales ) and into the stage
table. This operation is incredible fast, as no data is moved, just
the meta data for the partition’s rows is changed. The partition
information for the rows is just changed to tell that they are
now part of the stage table. In the following example I specified
to SWITCH  partition 3, which are the 2847 rows from October
1st.

ALTER TABLE partitioning.FactResellerSales
SWITCH PARTITION 3
TO partitioning.FactResellerSales_STAGE;
If you re-run the query to count the rows per partition
(Table 20-0), you will see that partition 3 of
FactResellerSales  has a row count of 0, but the only
partition available in FactResellerSales_STAGE  has a row
count of 2847:
SchemaName
TableName
PartitionScheme
partitioning
FactResellerSales
psOrderDate
partitioning
FactResellerSales
psOrderDate
partitioning
FactResellerSales
psOrderDate
partitioning
FactResellerSales
psOrderDate
partitioning
FactResellerSales_STAGE
PRIMARY
Now it is easy to work with the data for October 1st, because all
the rows are sitting in a much smaller table. For a full load, you

can just execute a TRUNCATE TABLE
partitioning.FactResellerSales  to erase the content (as
a, again, fast meta-data operation) and insert the source’s data
into it again.
If you are done with bringing the rows for October 1st up-to-
date, you can SWITCH  the full table back into
FactResellerSales :
ALTER TABLE partitioning.FactResellerSales_STAGE 
Unfortunately, you will receive an error message asking for a
mandatory CHECK CONSTRAINT  which must be present on the
stage table:
ALTER TABLE SWITCH statement failed. Check const
'AdventureWorksDW.partitioning.FactResellerSales_
are not allowed by range defined by partition 3 o
'AdventureWorksDW.partitioning.FactResellerSales
The mandatory check constraint must reflect the rule which
allows a row to be part of partition 3, which is that the
OrderDate  must be greater or equal October 1st and lower
November 1st:

ALTER TABLE partitioning.FactResellerSales_STAGE
WITH CHECK
ADD CONSTRAINT CK_FactResellerSales_STAGE_OrderDa
CHECK (
     OrderDate IS NOT NULL AND
     OrderDate >= {d'2023-10-01'} AND
     OrderDate < {d'2023-11-01'}
     )
With the following addition to the projection of the query in
Example 20-1 you get the text for the CHECK  constraint listed in
the query result, derived from the partition’s definition:
	
, QUOTENAME(c.name) + ' IS NOT NULL AND 
	
	
CASE pf.boundary_value_on_right W
	
	
--'RIGHT'
	
	
ISNULL(QUOTENAME(c.name) + ' >= 
	
	
convert(varchar, prv_left.[value
	
	
case when p.partition_number NOT 
	
	
then ' AND '
	
	
else ''
	
	
end +
	
	
ISNULL(QUOTENAME(c.name) + ' < '
	
	
convert(varchar, prv_right.[value
	
	
ELSE
	
	
--'LEFT'
		
ISNULL('[PartitionKey] > ''' +

	
	
ISNULL( [PartitionKey]  
 
	
	
convert(varchar, prv_left.[value
	
	
case when p.partition_number NOT 
	
	
then ' AND '
	
	
else ''
	
	
end + ISNULL(QUOTENAME(c.name) + 
	
	
convert(varchar, prv_right.[value
	
	
END as CheckConstraint
Here is the result for this additional column, which you can just
copy & paste into the ALTER TABLE  statement.

…​
TableName
PartitionNumber
…​
…​
FactResellerSales
1
…​
…​
FactResellerSales
2
…​
…​
FactResellerSales
3
…​
…​
FactResellerSales
4
…​

If you now re-try to switch the partition back into
FactResellerSales  this will succeed. If you did not change
the content of the stage table, the content of
FactResellerSales  should be back to the original state:
SchemaName
TableName
PartitionNumber
partitioning
FactResellerSales
1
partitioning
FactResellerSales
2
partitioning
FactResellerSales
3
partitioning
FactResellerSales
4
partitioning
FactResellerSales_STAGE
1
Instead of speeding up the access to the data itself, you can
calculate intermediate results on an aggregated level and take
te data from there, as explained in the next section.
Pre-Aggregating
NOTE
The file used for the demonstrations is 402 Aggregated Facts.sql.

Providing Power BI with pre-aggregated data can be done with
any of the relational “storage modes” discussed in an earlier
section: (indexed) table, view, function, or stored procedure. All
of them you can either Import or access in DirectQuery mode
from Power BI and Analysis Services Tabular.
To create an aggregated version of a table, all you need to do is
to specify the granularity (basically the dimension keys) and
aggregation functions (typically COUNT  or SUM ) on the
numeric values. Watch out for the GROUP BY  clause, in which
you need to put all the columns which are not wrapped inside
an aggregation function:
CREATE OR ALTER VIEW agg.FactResellerSalesAgg AS
SELECT
	
OrderDate,
	
COUNT(*) SalesCount,
	
SUM(SalesAmount) SalesAmount
FROM
	
PowerBI.FactResellerSales
GROUP BY OrderDate
After you added this view as an additional table to your data
model, do not forget to tell Power BI that this table is an
aggregated version of the already existing table

FactResellerSales  (“Pre-Aggregating”) or to include logic
into your measures to use the aggregated table, where
appropriate (“Pre-Aggregating”).
WARNING
Remember that I explained in “Calculations” that the result of not all operations are
aggregable. Aggregations like a distinct count or a percentage are non-additive and
should therefore not added to an aggregation table.
This concluded the last chapter about SQL.
Key Takeaways
This chapter talked about both how to increase speed of
operations in a relational database and how to support speedy
operations in Power BI through a well-thought design of your
database objects. In detail, you learned the following:
In a relational database, information can be provided in
different forms: Either persisted in a table or as a query
stored inside a view, function, or store procedure.
Information stored in a table should definitely be indexed.
You learned about the principles of how an index works.
And you learned that a columnstore index is not truly an

index, but supports a query engine inside a relational
database, which is basically identical to the query engine of
Power BI and Analysis Services Tabular.
Partitioning is crucial for both big tables in a relational
database and big tables in Power BI and Analysis Services.
If you decide to use partitioning, make sure to use the
identical column to partition in the relational world and in
Power BI/Analysis Services, to align the partitions.
You can pre-aggregate information in the relational layer
with the help of the GROUP BY clause and aggregation
functions.

Epilogue
Congratulations! You made it through to the last page!
Thank you so much for spending your precious time reading
this book. I hope you enjoyed reading it as much as I enjoyed
writing it.
Before I let you go, remember the most important things when
it comes to data modeling for Power BI:
You should sit down and understand your data and the
business’ needs before you connect to the first data source.
Data Profiling in Power Query can help you to understand
the data, as it shows descriptive statistics for each column.
Bringing the source’s information into a Dimensional model
is crucial, when it comes to Power BI. Any short cut taken
here will come back at you in a later stage. (I speak from
experience!) Avoid many-to-many relationships and bi-
directional filters as much as possible.
Make sure to connect that every table is connected to at
least one other table. Unrelated tables are only useful in
special cases (like when you need a non-equi join).
Push all necessary transformations as early as possible up
the data chain. Use Power Query over DAX for this task. If

you can, convince the people providing you with an excel
sheet to bring it into the necessary shape already. Even
better, talk to the right people in your organisation and set
up a data warehouse layer, where all the transformation
“magic” should happen. This way you make all the work
better re-usable through your organiation.
When it comes to all sorts of calculations (especially semi-
additive and non-additive ones) you need to develop DAX
measures. Don’t create calculated columns. Don’t add the
results of such calculations as columns during the data
transformation process.
When you discover obstacles, like slow performance or
very complicated DAX calculations, take a step back and re-
evaluate, if your data model really is in a Star schema.
You will get the best possible report speed by importing the
data to Power BI. Spend some time to allow importing data,
in case you discover problems in terms of data size or
refresh time. DirectQuery is only the second-best option.
Mastering data modeling takes time and effort. Exercise is
king. Failing is part of the journey!
And last but not least the most important thing to remember:
The goal of the data model is to make the report-creator’s life
easy!

Index

About the Author
Markus Ehrenmueller-Jensen has been involved in data
modeling for more than three decades. He teaches best
practices at international conferences, webinars and
workshops and implements them for clients in various
industries. Since 2006 he has been specialized in building
Business Intelligence and Data Warehouse solutions with
Microsoft’s Data Platform (relational database engine on- and
off-premises, Analysis Services Multidimensional and Tabular,
Reporting Services, Integration Services, Azure Data Factory,
Power BI dataflows, and – of course – Power BI Desktop and the
Fabric platform). He is the founder of Savory
Data (www.savorydata.com) and a professor for Databases and
Information Systems at HTL Leonding (technical college,
www.htl-leonding.ac.at) and holds several Microsoft
certifications. Markus is the (co)-author of several books. He has
been repeatedly awarded as a Microsoft Data Platform MVP
since 2017. You can contact him via markus@savorydata.com.
When Markus does not beat data into shape, he holds the beat
behind the drum set in various musical formations.
Find Markus on LinkedIn

Follow Markus on Twitter
Follow Markus on Blue Sky
Find Markus on Git Hub

Colophon
The animal on the cover of FILL IN TITLE is FILL IN
DESCRIPTION.
Many of the animals on O’Reilly covers are endangered; all of
them are important to the world.
The cover illustration is by Karen Montgomery, based on a
black and white engraving from FILL IN CREDITS. The cover
fonts are Gilroy Semibold and Guardian Sans. The text font is
Adobe Minion Pro; the heading font is Adobe Myriad
Condensed; and the code font is Dalton Maag’s Ubuntu Mono.


Statistics for Industry, Technology, and Engineering
Ron S. Kenett
Shelemyahu Zacks
Peter Gedeck
Industrial 
Statistics
A Computer-Based Approach 
with Python


Statistics for Industry, Technology, and 
Engineering 
Series Editor 
David Steinberg, Tel Aviv University, Tel Aviv, Israel 
Editorial Board Members 
V. Roshan Joseph, Georgia Institute of Technology, Atlanta, GA, USA 
Ron S. Kenett, Neaman Institute, Haifa, Israel 
Christine M. Anderson-Cook, Los Alamos National Laboratory, Los Alamos, USA 
Bradley Jones, SAS Institute, JMP Division, Cary, USA 
Fugee Tsung, Hong Kong University of Science and Technology, Hong Kong, 
Hong Kong 
The Statistics for Industry, Technology, and Engineering series will present up-to-
date statistical ideas and methods that are relevant to researchers and accessible 
to an interdisciplinary audience: carefully organized authoritative presentations, 
numerous illustrative examples based on current practice, reliable methods, realistic 
data sets, and discussions of select new emerging methods and their application 
potential. Publications will appeal to a broad interdisciplinary readership including 
both researchers and practitioners in applied statistics, data science, industrial 
statistics, engineering statistics, quality control, manufacturing, applied reliability, 
and general quality improvement methods. 
Principal Topic Areas: 
* Quality Monitoring * Engineering Statistics * Data Analytics * Data Science * 
Time Series with Applications * Systems Analytics and Control * Stochastics and 
Simulation * Reliability * Risk Analysis * Uncertainty Quantiﬁcation * Decision 
Theory * Survival Analysis * Prediction and Tolerance Analysis * Multivariate 
Statistical Methods * Nondestructive Testing * Accelerated Testing * Signal 
Processing * Experimental Design * Software Reliability * Neural Networks * 
The series will include professional expository monographs, advanced textbooks, 
handbooks, general references, thematic compilations of applications/case studies, 
and carefully edited survey books.

Ron S. Kenett • Shelemyahu Zacks • Peter Gedeck 
Industrial Statistics 
A Computer-Based Approach with Python

Ron S. Kenett 
KPA Ltd. 
Ra’anana, Israel 
Shelemyahu Zacks 
Binghamton University 
Mc Lean, VA, USA 
Peter Gedeck 
University of Virginia 
Falls Church, VA, USA 
ISSN 2662-5555
ISSN 2662-5563 
(electronic) 
Statistics for Industry, Technology, and Engineering 
ISBN 978-3-031-28481-6
ISBN 978-3-031-28482-3 
(eBook) 
https://doi.org/10.1007/978-3-031-28482-3 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland 
AG 2023 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse 
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional afﬁliations. 
This book is published under the imprint Birkhäuser, www.birkhauser-science.com by the registered 
company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

To my wife Sima, our children and their 
children: Yonatan, Alma, Tomer, Yadin, Aviv, 
Gili, Matan, Eden, and Ethan. RSK 
To my wife Hanna, our sons Yuval and David, 
and their families with love. SZ 
To Janet with love. PG 

Preface 
Knowledge and information are critical assets for any industrial enterprise. It 
enables businesses to differentiate themselves from competitors and compete 
efﬁciently and effectively to the best of their abilities. At present, information 
technology, telecommunications, and manufacturing are merging as the means of 
production are becoming increasingly autonomous. Advanced manufacturing, or 
Industry 4.0, is based on three interconnected pillars: (1) Computerized Product 
Design and Smart Technology; (2) Smart Sensors, Internet of Things, and Data 
Collectors integrated in Manufacturing Lines; and (3) Analytics, Control Theory, 
and Data Science. Advanced manufacturing requires analytics and operational 
capabilities to interface with devices in real time. Software development has become 
agile in DevOps operations focused on providing continuous delivery, as opposed 
to the traditional waterfall versioning approach (Kenett et al. 2018a). Moreover, 
processing and analytic models have evolved in order to provide a high level of 
ﬂexibility. The emergence of agile processing models enables the same instance of 
data to support batch analytics, interactive analytics, global messaging, database, 
and ﬁle-based models. The result is an application platform that supports the 
broadest range of processing and analytic models. The ability to understand data 
within a context, and assume the right business action, is a source of competitive 
advantage (Olavsrud 2017; Kenett and Redman 2019; Kang et al. 2021b). 
This book is about industrial statistics. It reﬂects many years of experience of the 
authors in doing research, teaching and applying statistics in science, healthcare, 
business, defense, and industry domains. The book invokes over 40 case studies 
and provides comprehensive Python applications. In 2020, there were 10 million 
developers in the world who code using Python which is considered the fastest-
growing programming language. A special Python package, mistat, and additional 
Python code are available for download at https://gedeck.github.io/mistat-code-
solutions/IndustrialStatistics. Everything in the book can be reproduced with mistat. 
We therefore provide, in this book, an integration of needs, methods, and delivery 
platform for a large audience and a wide range of applications. 
Industrial Statistics: A Computer-Based Approach with Python is a companion 
volume to Modern Statistics: A Computer-Based Approach with Python. Both books 
vii 

viii
Preface 
include mutual cross-references but are considered as stand-alone publications. 
Industrial Statistics: A Computer-Based Approach with Python can be used as 
textbook in a one-semester or two-semester course on industrial statistics. Every 
chapter includes exercises, data sets, and Python applications. These can be used in 
regular classroom setups, ﬂipped classroom setups and on-line or hybrid education 
programs. This book is focused on industrial statistics with chapters on advanced 
process monitoring methods, computer experiments, and Bayesian reliability. Mod-
ern Statistics: A Computer-Based Approach with Python is a foundational text and 
can be combined with any program requiring data analysis in its curriculum. This, 
for example, can be courses in data science, industrial statistics, physics, biology, 
chemistry, economics, psychology, social sciences, or any engineering discipline. 
Modern Statistics: A Computer-Based Approach with Python includes eight 
chapters. Chapter 1 is on analyzing variability with descriptive statistics. Chapter 2 
is on probability models and distribution functions. Chapter 3 introduces statistical 
inference and bootstrapping. Chapter 4 is on variability in several dimensions and 
regression models. Chapter 5 covers sampling for estimation of ﬁnite population 
quantities, a common situation when one wants to infer on a population from a 
sample. Chapter 6 is dedicated to time series analysis and prediction. Chapters 7 
and 8 are about modern data analytic methods. The Python code used in this book 
and the solutions to the exercises of this book are available from https://gedeck. 
github.io/mistat-code-solutions/ModernStatistics/. 
Industrial Statistics: A Computer-Based Approach with Python, this volume, 
contains 11 chapters: Chap. 1—Introduction to Industrial Statistics, Chap. 2— 
Basic Tools and Principles of Process Control, Chap. 3—Advanced Methods of 
Statistical Process Control, Chap. 4—Multivariate Statistical Process Control, 
Chap. 5—Classical Design and Analysis of Experiments, Chap. 6—Quality by 
Design, Chap. 7—Computer Experiments, Chap. 8—Cybermanufacturing and 
Digital Twins, Chap. 9—Reliability Analysis, Chap. 10—Bayesian Reliability 
Estimation and Prediction, and Chap. 11—Sampling Plans for Batch and Sequential 
Inspection. The Python code used in these chapters and the solutions to the 
exercises of this book are available from https://gedeck.github.io/mistat-code-
solutions/IndustrialStatistics/. The book covers (i) statistical process monitoring in 
Chaps. 2–4, (ii) the design of experiments in Chaps. 5–7, (iii) reliability analysis 
in Chaps. 9–10, and (iv) sampling testing and sequential inspection in Chap. 11. 
These four topics can be the topic of one trimester courses or covered in a full-year 
academic course. In addition, they can be used in focused workshops combining 
theory, applications, and Python implementations. Practitioners and researchers will 
ﬁnd the book topics comprehensive and detailed enough to provide a solid source 
of reference. 
Some of the material in these books builds on a book by the ﬁrst two authors 
titled Modern Industrial Statistics with Applications in R, MINITAB and JMP, 3rd  
edition, Wiley. Modern Statistics: A Computer-Based Approach with Python and 
Industrial Statistics: A Computer-Based Approach with Python include new and 
updated chapters on modern analytics and are dedicated to Python and refer to a 
specially developed Python application titled mistat that implements many tools and 

Preface
ix 
methods described in the books. A solution manual is also available, with Python 
examples, so that self-learners and instructors can assess the level of knowledge 
achieved by studying the book. Industrial Statistics: A Computer-Based Approach 
with Python, this volume, introduces several topics such as cybermanufacturing and 
computation pipelines. We made every possible effort to ensure the calculations 
are correct. However, should errors have skipped to the printed version, we would 
appreciate feedback from readers noticing these. In general, any feedback will be 
much appreciated. 
We thank Bill Meeker, Murat Testik, Alessandro di Bucchianico, Bart De Kete-
laere, Nam-Ky Nguyen, and Fabrizio Ruggeri for providing insightful comments 
on an early draft that helped improve the text. Finally, we would like to thank 
Christopher Tominich and the team at Springer Birkhäuser. They made everything 
in the publication process look easy and simple. 
Ra’anana, Israel
Ron S. Kenett 
Mc Lean, VA, USA
Shelemyahu Zacks 
Falls Church, VA, USA
Peter Gedeck 
January 2023 

Contents 
1 
The Role of Statistical Methods in Modern Industry...................
1 
1.1
Evolution of Industry ...............................................
1 
1.2
Evolution of Quality ................................................
3 
1.3
Industry 4.0 Characteristics ........................................
5 
1.4
Digital Twin .........................................................
6 
1.5
Chapter Highlights ..................................................
8 
1.6
Exercises ............................................................
9 
2 
Basic Tools and Principles of Process Control .......................... 
11 
2.1
Basic Concepts of Statistical Process Control ..................... 
11 
2.2
Driving a Process with Control Charts............................. 
22 
2.3
Setting Up a Control Chart: Process Capability Studies .......... 
25 
2.4
Process Capability Indices.......................................... 
28 
2.5
Seven Tools for Process Control and Process Improvement ...... 
32 
2.6
Statistical Analysis of Pareto Charts ............................... 
35 
2.7
The Shewhart Control Charts ...................................... 
39 
2.7.1 
Control Charts for Attributes ............................. 
40 
2.7.2 
Control Charts for Variables .............................. 
43 
2.8
Process Analysis with Data Segments ............................. 
49 
2.8.1 
Data Segments Based on Decision Trees ................ 
50 
2.8.2 
Data Segments Based on Functional Data Analysis ..... 
52 
2.9
Chapter Highlights .................................................. 
54 
2.10 
Exercises ............................................................ 
54 
3 
Advanced Methods of Statistical Process Control ...................... 
59 
3.1
Tests of Randomness ............................................... 
59 
3.1.1 
Testing the Number of Runs .............................. 
60 
3.1.2 
Runs Above and Below a Speciﬁed Level ............... 
62 
3.1.3 
Runs Up and Down ....................................... 
63 
3.1.4 
Testing the Length of Runs Up and Down ............... 
66 
3.2
Modiﬁed Shewhart Control Charts for ¯X .......................... 
67 
xi 

xii
Contents 
3.3
The Size and Frequency of Sampling for Shewhart 
Control Charts....................................................... 
68 
3.3.1 
The Economic Design for ¯X-charts ...................... 
68 
3.3.2 
Increasing the Sensitivity of p-charts .................... 
70 
3.4
Cumulative Sum Control Charts ................................... 
73 
3.4.1 
Upper Page’s Scheme ..................................... 
73 
3.4.2 
Some Theoretical Background ........................... 
76 
3.4.3 
Lower and Two-Sided Page’s Scheme ................... 
78 
3.4.4 
Average Run Length, Probability of False 
Alarm, and Conditional Expected Delay................. 
83 
3.5
Bayesian Detection ................................................. 
86 
3.6
Process Tracking .................................................... 
90 
3.6.1 
The EWMA Procedure.................................... 
91 
3.6.2 
The BECM Procedure..................................... 
94 
3.6.3 
The Kalman Filter ......................................... 
95 
3.6.4 
The QMP Tracking Method .............................. 
98 
3.7
Automatic Process Control ......................................... 102 
3.8
Chapter Highlights .................................................. 105 
3.9
Exercises ............................................................ 106 
4 
Multivariate Statistical Process Control ................................. 113 
4.1
Introduction ......................................................... 113 
4.2
A Review Multivariate Data Analysis ............................. 116 
4.3
Multivariate Process Capability Indices ........................... 120 
4.4
Advanced Applications of Multivariate Control Charts........... 124 
4.4.1 
Multivariate Control Charts Scenarios ................... 124 
4.4.2 
Internally Derived Target ................................. 125 
4.4.3 
External Reference Sample ............................... 127 
4.4.4 
Externally Assigned Target ............................... 128 
4.4.5 
Measurement Units Considered as Batches.............. 129 
4.4.6 
Variable Decomposition and Monitoring Indices........ 129 
4.5
Multivariate Tolerance Speciﬁcations.............................. 130 
4.6
Tracking Structural Changes ....................................... 133 
4.6.1 
The Synthetic Control Method ........................... 134 
4.7
Chapter Highlights .................................................. 139 
4.8
Exercises ............................................................ 140 
5 
Classical Design and Analysis of Experiments .......................... 141 
5.1
Basic Steps and Guiding Principles ................................ 141 
5.2
Blocking and Randomization....................................... 146 
5.3
Additive and Non-additive Linear Models......................... 147 
5.4
The Analysis of Randomized Complete Block Designs .......... 149 
5.4.1 
Several Blocks, Two Treatments per Block: 
Paired Comparison ........................................ 149 
5.4.2 
Several Blocks, t Treatments per Block.................. 153 
5.5
Balanced Incomplete Block Designs............................... 157 

Contents
xiii 
5.6
Latin Square Design ................................................ 160 
5.7
Full Factorial Experiments ......................................... 166 
5.7.1 
The Structure of Factorial Experiments .................. 166 
5.7.2 
The ANOVA for Full Factorial Designs.................. 166 
5.7.3 
Estimating Main Effects and Interactions ................ 173 
5.7.4 
2m Factorial Designs ...................................... 174 
5.7.5 
3m Factorial Designs ...................................... 185 
5.8
Blocking and Fractional Replications of 2m Factorial Designs... 193 
5.9
Exploration of Response Surfaces ................................. 201 
5.9.1 
Second Order Designs .................................... 202 
5.9.2 
Some Speciﬁc Second Order Designs.................... 205 
5.9.3 
Approaching the Region of the Optimal Yield........... 210 
5.9.4 
Canonical Representation................................. 211 
5.10 
Evaluating Designed Experiments ................................. 215 
5.11 
Chapter Highlights .................................................. 218 
5.12 
Exercises ............................................................ 221 
6 
Quality by Design .......................................................... 225 
6.1
Off-Line Quality Control, Parameter Design, and the 
Taguchi Method ..................................................... 226 
6.1.1 
Product and Process Optimization Using Loss 
Functions .................................................. 227 
6.1.2 
Major Stages in Product and Process Design ............ 229 
6.1.3 
Design Parameters and Noise Factors .................... 230 
6.1.4 
Parameter Design Experiments ........................... 231 
6.1.5 
Performance Statistics .................................... 233 
6.2
The Effects of Non-linearity........................................ 235 
6.3
Taguchi’s Designs................................................... 239 
6.4
Quality by Design in the Pharmaceutical Industry ................ 242 
6.4.1 
Introduction to Quality by Design........................ 242 
6.4.2 
A Quality by Design Case Study: The Full 
Factorial Design ........................................... 243 
6.4.3 
A Quality by Design Case Study: The 
Desirability Function...................................... 247 
6.4.4 
A Quality by Design Case Study: The Design Space ... 251 
6.5
Tolerance Designs .................................................. 253 
6.6
Case Studies......................................................... 255 
6.6.1 
The Quinlan Experiment.................................. 255 
6.6.2 
Computer Response Time Optimization ................. 258 
6.7
Chapter Highlights .................................................. 262 
6.8
Exercises ............................................................ 262 
7 
Computer Experiments .................................................... 265 
7.1
Introduction to Computer Experiments ............................ 265 
7.2
Designing Computer Experiments ................................. 270 
7.3
Analyzing Computer Experiments ................................. 272 

xiv
Contents 
7.4
Stochastic Emulators................................................ 277 
7.5
Integrating Physical and Computer Experiments.................. 279 
7.6
Simulation of Random Variables................................... 280 
7.6.1 
Basic Procedures .......................................... 280 
7.6.2 
Generating Random Vectors .............................. 282 
7.6.3 
Approximating Integrals .................................. 283 
7.7
Chapter Highlights .................................................. 284 
7.8
Exercises ............................................................ 285 
8 
Cybermanufacturing and Digital Twins ................................. 287 
8.1
Introduction to Cybermanufacturing ............................... 287 
8.2
Cybermanufacturing Analytics ..................................... 288 
8.3
Information Quality in Cybermanufacturing ...................... 290 
8.4
Modeling in Cybermanufacturing.................................. 300 
8.5
Computational Pipelines............................................ 303 
8.6
Digital Twins ........................................................ 307 
8.7
Chapter Highlights .................................................. 314 
8.8
Exercises ............................................................ 315 
9 
Reliability Analysis ........................................................ 319 
9.1
Basic Notions ....................................................... 321 
9.1.1 
Time Categories ........................................... 321 
9.1.2 
Reliability and Related Functions ........................ 323 
9.2
System Reliability .................................................. 324 
9.3
Availability of Repairable Systems ................................ 328 
9.4
Types of Observations on T T F  .................................... 335 
9.5
Graphical Analysis of Life Data ................................... 336 
9.6
Nonparametric Estimation of Reliability .......................... 342 
9.7
Estimation of Life Characteristics.................................. 343 
9.7.1 
Maximum Likelihood Estimators for 
Exponential TTF Distribution ............................ 343 
9.7.2 
Maximum Likelihood Estimation of the 
Weibull Parameters........................................ 349 
9.8
Reliability Demonstration .......................................... 351 
9.8.1 
Binomial Testing .......................................... 352 
9.8.2 
Exponential Distributions ................................. 353 
9.9
Accelerated Life Testing............................................ 362 
9.9.1 
The Arrhenius Temperature Model....................... 363 
9.9.2 
Other Models .............................................. 363 
9.10 
Burn-In Procedures ................................................. 364 
9.11 
Chapter Highlights .................................................. 366 
9.12 
Exercises ............................................................ 366 
10 
Bayesian Reliability Estimation and Prediction ........................ 371 
10.1 
Prior and Posterior Distributions ................................... 371 
10.2 
Loss Functions and Bayes Estimators ............................. 375 

Contents
xv 
10.2.1 
Distribution-Free Bayes Estimator of Reliability ........ 376 
10.2.2 
Bayes Estimator of Reliability for Exponential 
Life Distributions.......................................... 377 
10.3 
Bayesian Credibility and Prediction Intervals ..................... 378 
10.3.1 
Distribution-Free Reliability Estimation ................. 379 
10.3.2 
Exponential Reliability Estimation ....................... 380 
10.3.3 
Prediction Intervals........................................ 380 
10.3.4 
Applications with Python: Lifelines and pymc .......... 382 
10.4 
Credibility Intervals for the Asymptotic Availability of 
Repairable Systems: The Exponential Case ....................... 390 
10.5 
Empirical Bayes Method ........................................... 392 
10.6 
Chapter Highlights .................................................. 394 
10.7 
Exercises ............................................................ 395 
11 
Sampling Plans for Batch and Sequential Inspection .................. 397 
11.1 
General Discussion ................................................. 398 
11.2 
Single-Stage Sampling Plans for Attributes ....................... 400 
11.3 
Approximate Determination of the Sampling Plan................ 403 
11.4 
Double Sampling Plans for Attributes ............................. 406 
11.5 
Sequential Sampling and A/B Testing ............................. 410 
11.5.1 
The One-Armed Bernoulli Bandits....................... 410 
11.5.2 
Two-Armed Bernoulli Bandits............................ 415 
11.6 
Acceptance Sampling Plans for Variables ......................... 416 
11.7 
Rectifying Inspection of Lots ...................................... 418 
11.8 
National and International Standards .............................. 420 
11.9 
Skip-Lot Sampling Plans for Attributes ........................... 423 
11.9.1 
The ISO 2859 Skip-Lot Sampling Procedures........... 423 
11.10 
The Deming Inspection Criterion .................................. 426 
11.11 
Published Tables for Acceptance Sampling ....................... 427 
11.12 
Sequential Reliability Testing ...................................... 429 
11.13 
Chapter Highlights .................................................. 439 
11.14 
Exercises ............................................................ 440 
A 
Introduction to Python .................................................... 443 
A.1
List, Set, and Dictionary Comprehensions......................... 443 
A.2
Scientiﬁc Computing Using numpy and scipy ................. 444 
A.3
Pandas Data Frames ................................................ 445 
A.4
Data Visualization Using pandas and matplotlib .......... 446 
B 
List of Python Packages ................................................... 449 
C 
Code Repository and Solution Manual................................... 451 
Bibliography ...................................................................... 453 
Index............................................................................... 465 

Modern Statistics: A Computer-Based 
Approach with Python (Companion 
Volume) 
1 
Analyzing Variability: Descriptive Statistics 
1.1
Random Phenomena and the Structure of Observations 
1.2
Accuracy and Precision of Measurements 
1.3
The Population and the Sample 
1.4
Descriptive Analysis of Sample Values 
1.5
Prediction Intervals 
1.6
Additional Techniques of Exploratory Data Analysis 
1.7
Chapter Highlights 
1.8
Exercises 
2 
Probability Models and Distribution Functions 
2.1
Basic Probability 
2.2
Random Variables and Their Distributions 
2.3
Families of Discrete Distribution 
2.4
Continuous Distributions 
2.5
Joint, Marginal and Conditional Distributions 
2.6
Some Multivariate Distributions 
2.7
Distribution of Order Statistics 
2.8
Linear Combinations of Random Variables 
2.9
Large Sample Approximations 
2.10 
Additional Distributions of Statistics of Normal Samples 
2.11 
Chapter Highlights 
2.12 
Exercises 
3 
Statistical Inference and Bootstrapping 
3.1
Sampling Characteristics of Estimators 
3.2
Some Methods of Point Estimation 
3.3
Comparison of Sample Estimates 
3.4
Conﬁdence Intervals 
3.5
Tolerance Intervals 
3.6
Testing for Normality with Probability Plots 
3.7
Tests of Goodness of Fit 
xvii 

xviii
Modern Statistics: A Computer-Based Approach with Python (Companion Volume) 
3.8
Bayesian Decision Procedures 
3.9
Random Sampling From Reference Distributions 
3.10 
Bootstrap Sampling 
3.11 
Bootstrap Testing of Hypotheses 
3.12 
Bootstrap Tolerance Intervals 
3.13 
Non-Parametric Tests 
3.14 
Chapter Highlights 
3.15 
Exercises 
4 
Variability in Several Dimensions and Regression Models 
4.1
Graphical Display and Analysis 
4.2
Frequency Distributions in Several Dimensions 
4.3
Correlation and Regression Analysis 
4.4
Multiple Regression 
4.5
Quantal Response Analysis: Logistic Regression 
4.6
The Analysis of Variance: The Comparison of Means 
4.7
Simultaneous Conﬁdence Intervals: Multiple Comparisons 
4.8
Contingency Tables 
4.9
Categorical Data Analysis 
4.10 
Chapter Highlights 
4.11 
Exercises 
5 
Sampling for Estimation of Finite Population Quantities 
5.1
Sampling and the Estimation Problem 
5.2
Estimation with Simple Random Samples 
5.3
Estimating the Mean with Stratiﬁed RSWOR 
5.4
Proportional and Optimal Allocation 
5.5
Prediction Models with Known Covariates 
5.6
Chapter Highlights 
5.7
Exercises 
6 
Time Series Analysis and Prediction 
6.1
The Components of a Time Series 
6.2
Covariance Stationary Time Series 
6.3
Linear Predictors for Covariance Stationary Time Series 
6.4
Predictors for Non-stationary Time Series 
6.5
Dynamic Linear Models 
6.6
Chapter Highlights 
6.7
Exercises 
7 
Modern Analytic Methods: Part I 
7.1
Introduction to Computer Age Statistics 
7.2
Data Preparation 
7.3
The Information Quality Framework 
7.4
Determining Model Performance 
7.5
Decision Trees 
7.6
Ensemble Models 

Modern Statistics: A Computer-Based Approach with Python (Companion Volume)
xix 
7.7
Naïve Bayes Classiﬁer 
7.8
Neural Networks 
7.9
Clustering Methods 
7.10 
Chapter Highlights 
7.11 
Exercises 
8 
Modern Analytic Methods: Part II 
8.1
Functional Data Analysis 
8.2
Text Analytics 
8.3
Bayesian Networks 
8.4
Causality Models 
8.5
Chapter Highlights 
8.6
Exercises 
A 
Introduction to Python 
B 
List of Python Packages 
C 
Code Repository and Solution Manual 
D 
Bibliography 
Index 

List of Abbreviations 
AIC
Akaike Information Criteria 
ANOVA 
Analysis of Variance 
ANSI
American National Standard Institute 
AOQ
Average Outgoing Quality 
AOQL
Average Outgoing Quality Limit 
AQL
Acceptable Quality Level 
ARIMA 
Autoregressive Integrated Moving Average 
ARL
Average Run Length 
ASN
Average Sample Number 
ASQ
American Society for Quality 
ATE
Average Treatment Effect 
ATI
Average Total Inspection 
BECM
Bayes Estimation of the Current Mean 
BI
Business Intelligence 
BIBD
Balanced Incomplete Block Design 
BIC
Bayesian Information Criteria 
BLUP
Best Linear Unbiased Prediction 
BN
Bayesian Network 
BP
Bootstrap Population 
CAD
Computer-Aided Design 
CADD
Computer-Aided Drawing and Drafting 
CAM
Computer-Aided Manufacturing 
CART
Classiﬁcation And Regression Trees 
CBD
Complete Block Design 
c.d.f.
cumulative distribution function 
CED
Conditional Expected Delay 
cGMP
Current Good Manufacturing Practices 
CHAID 
Chi-square Automatic Interaction Detector 
CI
Condition Indicator 
CIM
Computer-Integrated Manufacturing 
CLT
Central Limit Theorem 
xxi 

xxii
List of Abbreviations 
CMM
Coordinate Measurement Machines 
CMMI
Capability Maturity Model Integrated 
CNC
Computerized Numerically Controlled 
CPA
Circuit Pack Assemblies 
CQA
Critical Quality Attribute 
CUSUM 
Cumulative Sum 
DACE
Design and Analysis of Computer Experiments 
DAG
Directed Acyclic Graph 
DFIT
Difference in Fits distance 
DLM
Dynamic Linear Model 
DoE
Design of Experiments 
DTM
Document Term Matrix 
EBD
Empirical Bootstrap Distribution 
ETL
Extract-Transform-Load 
EWMA 
Exponentially Weighted Moving Average 
FDA
Food and Drug Administration 
FDA
Functional Data Analysis 
FPCA
Functional Principal Component Analysis 
FPM
Failures Per Million 
GFS
Google File System 
GRR
Gage Repeatability and Reproducibility 
HPD
Highest Posterior Density 
HPLC
High-Performance Liquid Chromatography 
IDF
Inverse Document Frequency 
i.i.d.
independent and identically distributed 
InfoQ
Information Quality 
IPO
Initial Public Offering 
IPS
Inline Process Control 
IQR
InterQuartile Range 
ISC
Short-Circuit Current of Solar Cells (in Ampere) 
KS
Kolmogorov–Smirnov Test 
LCL
Lower Control Limit 
LLN
Law of Large Numbers 
LQL
Limiting Quality Level 
LSA
Latent Semantic Analysis 
LSL
Lower Speciﬁcation Limit 
LTPD
Lot Tolerance Percent Defective 
LWL
Lower Warning Limit 
MAE
Mean Absolute Error 
m.g.f.
moment generating function 
MLE
Maximum Likelihood Estimator 
MSD
Mean Squared Deviation 
MSE
Mean Squared Error 
MTBF
Mean Time Between Failures 
MTTF
Mean Time To Failure 

List of Abbreviations
xxiii 
NID
Normal Independently Distributed 
OAB
One-Armed Bandit 
OC
Operating Characteristic 
PCA
Principal Component Analysis 
p.d.f.
probability density function 
PERT
Project Evaluation and Review Technique 
PFA
Probability of False Alarm 
PL
Product Limit Estimator 
PPM
Defects in Parts Per Million 
PSE
Practical Statistical Efﬁciency 
QbD
Quality by Design 
QMP
Quality Measurement Plan 
QQ-Plot 
Quantile vs. Quantile Plot 
RCBD
Randomized Complete Block Design 
Regex
Regularized Expression 
RMSE
Root Mean Squared Error 
RSWOR 
Random Sample Without Replacement 
RSWR
Random Sample With Replacement 
SE
Standard Error 
SL
Skip Lot 
SLOC
Source Lines of Code 
SLSP
Skip Lot Sampling Plans 
SPC
Statistical Process Control 
SPRT
Sequential Probability Ratio Test 
SR
Shiryaev Roberts 
SSE
Sum of Squares of Errors 
SSR
Sum of Squares around the Regression Model 
SST
Total Sum of Squares 
STD
Standard Deviation 
SVD
Singular Value Decomposition 
TAB
Two-Armed Bandit 
TF
Term Frequency 
TTC
Time Till Censoring 
TTF
Time Till Failure 
TTR
Time Till Repair 
TTT
Total Time on Test 
UCL
Upper Control Limit 
USL
Upper Speciﬁcation Limit 
UWL
Upper Warning Limit 
WSP
Wave Soldering Process 

Chapter 1 
The Role of Statistical Methods in 
Modern Industry 
Preview Industrial statistics is a discipline that needs to be adapted and provide 
enhanced capabilities to modern industrial systems. This chapter presents the 
evolution of industry and quality in the last 300 years. The transition between the 
four industrial revolutions is reviewed as well as the evolution of quality from 
product quality to process, service, management, design, and information quality. 
To handle the new opportunities and challenges of big data, a current perspective of 
information quality is presented, including a comprehensive InfoQ framework. The 
chapter concludes with a presentation of digital twins which are used in industry 
as a platform for monitoring, diagnostic, prognostic and prescriptive analytics. The 
Python code used in this and the following chapters is available from https://gedeck. 
github.io/mistat-code-solutions/IndustrialStatistics. 
1.1 
Evolution of Industry 
In medieval Europe, most families and social groups made their own goods such 
as cloth, utensils, and other household items. The only saleable cloth was woven 
by peasants who paid their taxes in kind to their feudal lords. Barons afﬁxed their 
marks to the fabric, which came to stand for their levels of quality. While some 
details differ, the textile industry all over Europe and China was similar and was 
apparently the ﬁrst industry to analyze data. Simple production ﬁgures, including 
percentages of defective products, were compiled in British cotton mills early in the 
nineteenth century. Quality control activities generated data that was aggregated in 
ledgers for accounting and planning purposes (Juran 1995). 
The industrial revolution started in England. Richard Arkwright (1732–1792) 
was an English inventor and a leading entrepreneur who became known as the 
“father of the modern industrial factory system”. He invented the spinning frame 
and a rotary carding engine that transformed raw cotton into cotton lap. Arkwright’s 
Supplementary Information The online version contains supplementary material available at 
(https://doi.org/10.1007/978-3-031-28482-3_1). 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3_1
1

2
1
The Role of Statistical Methods in Modern Industry
achievement was to combine power, machinery, semi-skilled labor and a new raw 
material, cotton, to create mass-produced yarn. In 10 years, he became the richest 
man in England. 
During the early twentieth century, a constellation of technologies and man-
agement techniques expanded mass production. The internal combustion engine 
(and the oil and gas needed to fuel it) and electricity, powered the way. The 
production line formalized the division of labor, and huge factories were built. 
The Taylor System, featuring time and motion studies, drove production tasks 
and productivity quotas. Companies learned how to manage enormous factories 
(Chandler 1993). This was the second industrial revolution. As an example, Western 
Electrics’ Hawthorne Works, on the outskirts of Chicago, employed up to 45,000 
workers and produced unheard of quantities of telephone equipment and a wide 
variety of consumer products. It was in this environment that Shewhart realized 
that manufacturing processes can be controlled using control charts (Shewhart 
1926). Control charts minimized the need for inspection, saving time and money 
and delivering higher quality. W. Edwards Deming and Joseph M. Juran, who 
both worked for Western Electric in the 1920s, were instrumental in bringing 
this approach to Japan in the 1950s. Deming emphasized the use of statistical 
methods (Deming 1982), and Juran developed a comprehensive management system 
featuring the “quality trilogy” (Godfrey and Kenett 2007). From a data analysis 
perspective, attention shifted from inspection of ﬁnal products to production process 
and the need to understand variation in key parameters. Statistical models and 
probability played a key role in this shift. 
In the third industrial revolution, computers changed manufacturing in several 
ways. First, computers enabled “mass customization” (Davis 1997). Essentially 
mass customization combines the scale of large, continuous ﬂow production 
systems, with the ﬂexibility of a job shop. This allows a massive effort, with batches 
of size one. A call center that employs screening to route calls to specialized support 
experts is a good example. Second is automation of so-called back-ofﬁce functions, 
such as inventory management and product design. As an example, take the 
development of an automobile suspension system designed using Computer-Aided 
Design (CAD). The new suspension must meet customer and testing requirements 
under a range of speciﬁc road conditions. After coming up with an initial design 
concept, design engineers use computer simulation to show the damping effects 
of the new suspension design under various road conditions. The design is then 
iteratively improved. Third is integration. Thus, in parallel to the design of the 
suspension system, purchasing specialists and industrial engineers proceed with 
specifying and ordering the necessary raw materials, setting up the manufacturing 
processes, and scheduling production using computer-aided manufacturing tools 
(CAM). Then, throughout manufacturing, tests provide the necessary production 
controls. Finally, computer-integrated manufacturing pulls everything together. 
Ultimately, the objective is to minimize the impact of failures to products delivered 
to customers. The application of computer simulations, with no experimental error, 
required new experimental design methods, such as Latin Hypercubes and Kriging 
models. In addition, modern advances in optimization of statistically designed

1.2
Evolution of Quality
3
experiments led to designs that better address constraints and exploit optimality 
properties. These methods are introduced in Chap. 7. 
The current fourth industrial revolution is fueled by data from sensors and IoT 
devices and powered by ﬂexible manufacturing systems like added manufacturing 
and 3D printing. Futurists talk of machines that organize themselves, delivery chains 
that automatically assemble themselves, and applications that feed customer orders 
directly into production. This evolution in industrial processes is matched by an 
evolution in quality methods and approaches. We present this in the next section. 
1.2 
Evolution of Quality 
In tracking the evolution of quality, we highlight several milestones over time. A 
ﬁrst step, on this journey, can be found in the old testament. On the sixth day, the 
Creator completed his work and invoked inspection to determine if further action 
was needed. The thirty ﬁrst verse of Genesis reads: 
And God saw everything that he had made, and, behold, it was very good (Genesis I, 31). 
Inspection was indeed the leading quality model for many centuries. A vivid picture 
of inspection in action is depicted in Syndics of the Drapers’ Guild, a 1662 oil 
painting by Rembrandt one can admire in the Rijksmuseum in Amsterdam. A 
second important milestone, where speciﬁcation of parts got set before ﬁnal assem-
bly, is attributed to Eli Whitney (1765–1825), an American inventor, mechanical 
engineer, and manufacturer. Whitney is known in the USA as the inventor of the 
concept of mass production of interchangeable parts. In 1797, the US government 
threatened by war with France, solicited 40,000 muskets from private contractors 
because the two national armories had produced only 1000 muskets in 3 years. 
Whitney offered to supply 10,000 muskets in 2 years. He designed machine tools 
enabling unskilled workman to make parts that were checked against speciﬁcation. 
The integration of such parts made a musket. Any part would ﬁt in any musket 
of similar design. The workforce was now split into production and inspection 
teams. A third milestone, 120 years later, was the introduction of statistical process 
control charts by Shewhart (1926). Following this third milestone, attention shifted 
from quality of product, and inspection, to process quality, and statistical process 
control. Sixty years later, on the basis of experience gained at Western Electric 
Joseph Juran formulated the Quality Trilogy, as a universal approach for managing 
quality. This marked the start of quality management and was a precursor to total 
quality management and six sigma. A key contributor to this movement was W. 
Edwards Deming who, together with Juran, had huge success in implementing 
quality management principles in devastated post world War II Japan (Deming 1982, 
1991; Juran 1986, 1995). In a further development in the 1960s a Japanese engineer, 
Genichi Taguchi, introduced to industry methods for designing statistically designed 
experiments aimed at improving products and processes by achieving design-based 
robustness properties (Godfrey 1986; Taguchi 1987). These methods were originally

4
1
The Role of Statistical Methods in Modern Industry
suggested by R. A. Fisher, the founder of modern statistics in agriculture and greatly 
developed in the chemical industry, by his student and son in law, G.E.P Box (Fisher 
1935; Box et al. 2005). In 1981 Taguchi came to Bell Laboratories, the research 
arm of Western Electric, to share his experience in robust design methodologies. 
His seminars at Holmdel, New Jersey, were attended by only a dozen people. 
His English was poor and his ideas so new that it took time to understand his 
methods. At that time, industry was mostly collecting data on ﬁnished product 
quality with only some data on processes. Thirty years later, industry started facing 
a big data phenomenon. Sensors and modern data analysis systems offered new 
options for process and product control. This led to considerations of integrated 
models combining data from different sources (Godfrey and Kenett 2007). With 
data analytics and manufacturing execution systems (MES), the business of quality 
started shifting to information quality (Kenett 2008). To handle this, Kenett and 
Shmueli (2014) introduced a framework labeled “InfoQ”. Technically, the deﬁnition 
of InfoQ is the derived utility (U) from an application of a statistical or data analytic 
model (f ), to a dataset (X), given the research goal (g); InfoQ equals upper U left parenthesis f left parenthesis upper X vertical bar g right parenthesis right parenthesisInfoQ = U(f (X | g)). On  
this basis, data scientists can help organizations generate information quality from 
their data lakes. 
To assess the level of InfoQ in a speciﬁc study, Kenett and Shmueli (2014) 
proposed eight dimensions of InfoQ: 
1. Data resolution: The measurement scale and level of aggregation of the data 
relative to the task at hand must be adequate for the study. For example, consider 
data on daily purchases of over-the-counter medications at a large pharmacy. 
If the goal of the analysis is to forecast future inventory levels of different 
medications when re-stocking is done on a weekly basis, then, weekly aggregated 
data is preferred to daily aggregate data. 
2. Data structure: The data can combine structured quantitative data with unstruc-
tured, semantic based data. For example, in assessing the reputation of an 
organization one might combine data derived from the stock exchange with data 
mined from text such as newspaper archives or press reports. Doing it enhances 
information quality. 
3. Data integration: Data is often spread out across multiple data sources. Hence, 
properly identifying the different relevant sources, collecting the relevant data, 
and integrating the data, directly affect information quality. 
4. Temporal relevance: A dataset contains information collected during a certain 
time framework. The degree of relevance of the data in that time framework to 
the current goal at hand must be assessed. For instance, in learning about current 
online shopping behaviors, a dataset with last year’s records of online purchasing 
behavior might be irrelevant. 
5. Chronology of data and goal: Depending on the nature of the goal, the chronol-
ogy of the data can support the goal to different degrees. For example, in 
process control applications of discrete parts, we might collect data from previous 
processes that is relevant to a speciﬁc part. If the goal is to quantify the effect of 
previous manufacturing steps on the speciﬁc part’ quality, then the chronology is

1.3
Industry 4.0 Characteristics
5
ﬁne. However, if the goal is to predict the ﬁnal quality of a part, then the required 
information builds on data collected in future manufacturing steps and, hence, 
the chronology of data and goal is not met. 
6. Generalizability: There are two types of generalizability: statistical and scientiﬁc 
generalizability. Statistical generalizability refers to inferring from a sample 
to a target population. Scientiﬁc generalizability refers to applying a model 
based on a particular target population to other populations. It may imply either 
generalizing an estimated population pattern/model to other populations or else 
applying it from one population to predict individual observations in other 
populations. 
7. Operationalization: Observable data are an operationalization of underlying 
concepts. “Customer Satisfaction” may be measured via a questionnaire or by 
evaluating the impact of attributes that were assessed via conjoint analysis. 
Constructs play a key role in causal models and raise the question of what to 
measure and how. The sensitivity to what is measured versus the construct of 
interest depends on the study goal. Action operationalization is about deriving 
concrete actions from the information provided by a study. 
8. Communication: If the information does not reach the right person at the right 
time in a clear and understandable way, the quality of information becomes poor. 
Data visualization is crucial for good communications and it is therefore directly 
related to the quality of information. Poor visualization of ﬁndings may lead to 
degradation of the information quality contained in the analysis performed on the 
data. Dashboards are about Communication. 
For more on InfoQ see Kenett and Shmueli (2016). An application of the informa-
tion quality framework to chemical process engineering is presented in Reis and 
Kenett (2018). 
After 2010s, organizations started hiring data scientists to leverage the poten-
tial in their data and data scientists started getting involved in organizational 
infrastructures and data quality (Kenett and Redman 2019). Systems Engineering 
in the Fourth Industrial Revolution: Big data, Novel Technologies, and Modern 
Systems Engineering is discussed in Kenett et al. (2021a). In summary, quality 
models evolved through the following milestones: (1) Product quality, (2) Process 
quality, (3) Management quality, (4) Design quality, and (5) Information quality. The 
chapters in this book cover the tools and methods of industrial analytics supporting 
this quality evolution. 
1.3 
Industry 4.0 Characteristics 
Industry 4.0, the so-called fourth industrial revolution, relies on three basic ele-
ments: 
• 
Sensor technology that can extensively measure products and processes online.

6
1
The Role of Statistical Methods in Modern Industry
• 
Flexible manufacturing capabilities—such as 3D printing—that can efﬁciently 
produce batches of varying size. 
• 
Analytics that power the industrial engine with the capability to monitor, 
diagnose, predict, and optimize decisions. 
One signiﬁcant analytic challenge is data integration. Sensors may collect data with 
different time cycles. Dynamic time warping (DTW) and Bayesian Networks (BN) 
can fuse the collected data into an integrated picture (Kenett 2019). In analytic 
work done in industry, data is collected either actively or passively and models are 
developed with empirical methods, ﬁrst principles or hybrid models. The industrial 
cycle provides opportunities to try out new products or new process set-ups and, 
based on the results, determine follow-up actions. It is, however, important to 
make sure that analytic work is reviewed properly to avoid deriving misleading 
conclusions, which could be very costly and/or time-consuming. For example, 
a lithium battery manufacturer discovered it had uncalibrated test equipment 
evaluating end-of-the-line products. The company was able to avoid a major recall 
by using the plant’s control charts to precisely identify the problematic batches. To 
avoid shipping immature products, or defective batches, good diagnostic capabilities 
are vital for monitoring and identifying the cause of any reported problems. 
Analytic challenges in systems engineering and industrial applications include 
(Kenett et al. 2021b): 
• 
Engineering design 
• 
Manufacturing systems 
• 
Decision-support systems 
• 
Shop-ﬂoor control and layout 
• 
Fault detection and quality improvement 
• 
Condition-based maintenance 
• 
Customer and supplier relationship management 
• 
Energy and infrastructure management 
• 
Cybersecurity and security 
These challenges require monitoring products and process; designing new products 
and processes; and improving products and processes. The next section presents an 
approach designed to attain all these objectives; the digital twin, also known as a 
surrogate model. 
1.4 
Digital Twin 
The term “digital twin” is deﬁned in different forms including as a high-ﬁdelity 
simulation, a virtual organization, a virtual reality representation, and an emulation 
facility. Its uses are in deploying optimization, monitoring, diagnostic, prognostic, 
and prescriptive capabilities (Kenett et al. 2018b; Kenett and Bortman 2021). The 
digital twin originated with the concept of a digital factory (Jain and Shao 2014) and

1.4
Digital Twin
7
is the digital representation of a physical asset or system, across its life-cycle, using 
operational real-time data and other sources, adopted to drive business outcomes. 
The digital twin concept has been implemented by leading manufacturing com-
panies. Ford Motor Company enhanced assembly line performance by evaluating 
and optimizing the designs using digital twins (IMT 2013). Volvo Group Global 
(2017) showed how to validate changes using a digital twin. General Electric 
developed digital twins of aircraft engines. Major commercial software vendors 
support development of virtual factories via integrated solutions for product, 
process and system design, simulation, and visualization (Tolio et al. 2013). A 
standardization of process control technologies is provided by ANSI (2010). On the 
other hand, Jain and Shao (2014) attempted to implement a multi-resolution digital 
twin but found it highly challenging due technology limitations and information 
availability. 
Virtual data management, automatic model generation, static and dynamic 
simulation, and integration and communication are paramount to realizing a digital 
twin (Choi et al. 2015). However, most software tools are, in general, not supplied 
with these capabilities making it a challenge to develop a digital twin. There 
are efforts addressing different aspects of the challenge. To enhance conventional 
simulations for a digital twin, Bal and Hashemipour (2009) use Product-Resource-
Order-Staff Architecture for modeling controls while the Quest simulation tool 
models the physical elements. To integrate models and enhance communication, 
Hints et al. (2011) developed a software tool named Design Synthesis Module. 
Debevec et al. (2014) use a simulative model to test and improve schedules before 
implementation in factories of small and medium size. For production planning, 
Terkaj et al. (2015) present an ontology of a virtual factory or digital twin, in order 
to aid planning decisions. 
The recent concept of “Industry 4.0”, or the fourth industrial revolution, includes 
Cyber-Physical Systems (CPS) as a key component. The function of CPS is the 
monitoring of physical processes and creating a virtual copy of the physical world 
to support decentralized decision-making (Hermann et al. 2015). In Industry 4.0 
applications, one sees a growing role of twinning a physical plant with simulation-
based surrogates. By means of sensors, real-time data about physical items are 
collected and used to duplicate the physical state of the item and assess the impact of 
ongoing changes (Kenett et al. 2021a). Digital twins include ﬁve main components: 
physical part, virtual part, connection, data, and service. The virtual and physical 
parts exchange information collected through the connection part. The interaction 
between the human and the digital twin is provided by the service part. Digital 
twins are traditionally used to improve the performance of engineering devices, 
like wind turbines or jet engines. In this context, they also serve to model systems 
of devices, to collect and analyze information about processes and people, and to 
help solve complex problems. Such digital twins provide powerful planning and 
troubleshooting capabilities and statistical methods play a signiﬁcant role in both 
the design and analysis of simulations and computer experiments on digital twin 
platforms. Digital twins provide a platform that enables a life cycle perspective on

8
1
The Role of Statistical Methods in Modern Industry
products and systems. This emphasizes a transition from engineering the design to 
engineering the performance. 
Simulation models of systems and processes are used to shorten time to market, 
reduce design, operations and maintenance costs while improving quality. An 
example is the PENSIM simulation software used in modelling penicillin production 
in a fed-batch fermentor. The model includes variables such as pH, temperature, 
aeration rate, agitation power, and feed ﬂow rate of the substrate. It is used in 
monitoring and process trouble shooting activities. Such simulators are used in fault 
diagnosis of semiconductors, biotechnological, and chemical production processes. 
They are also used for research and educational purposes (Reis and Kenett 2018). 
Digital twins complement or substitute physical experiments with software-
based simulation experiments (Santner et al. 2003). They permit building knowledge 
of a physical system and supporting decision-making in the design and monitoring 
of such systems (Kenett and Coleman 2021). A physical experiment consists of 
executing real life experiments where modiﬁcations to input variables are taking 
place. Similarly, a digital twin can be used to run experiments by a number of runs 
of a simulation code where factors are parametrized as a subset of the code’s inputs. 
An important application of digital twins is condition-based maintenance (CBM). 
This approach is based on the idea that maintenance operations should be done only 
when necessary (Gruber et al. 2021). The purpose of CBM is to prevent a reduction 
in the effectiveness of a system which can evolve to a total failure of the system. It 
aims to reduce maintenance costs by enabling planning of maintenance operations 
in advance and indicate necessary replacement of damaged component. In CBM, 
prediction of the remaining useful life (RUL) 
of each component is required. 
Evaluation of the components’ RUL requires not only diagnostics of the fault but 
also the estimation of the fault location and severity. 
Predictive analytics and operational Business Intelligence systems identify 
potential failures and allow for customer churn prevention initiatives. The 
application of industrial analytics, within such computerized environments, allows 
practitioners to concentrate on statistical analysis as opposed to repetitive numerical 
computations. 
1.5 
Chapter Highlights 
The main concepts and deﬁnitions introduced in this chapter include: 
• 
The ﬁrst industrial revolution 
• 
The second industrial revolution 
• 
The third industrial revolution 
• 
The fourth industrial revolution (Industry 4.0) 
• 
Inspection as a management approach 
• 
Process control and improvement 
• 
Quality by design

1.6
Exercises
9
• 
Information quality 
• 
Computer simulations 
• 
Digital twins 
1.6 
Exercises 
Exercise 1.1 Describe three work environments where quality is assured by 100% 
inspection of outputs (as opposed to process control). 
Exercise 1.2 Search periodicals, such as Business Week, Fortune, Time and 
Newsweek and newspapers such as the New York Times and Wall Street Journal for 
information on quality initiatives in service, healthcare, governmental and industrial 
organizations. Summarize three such initiatives indicating what was done and what 
were the concrete outcomes of these initiatives. 
Exercise 1.3 Provide examples of the three types of production systems. 
(a) Continuous ﬂow production 
(b) Discrete mass production 
(c) Industry 4 production 
Exercise 1.4 What management approach cannot work with continuous ﬂow pro-
duction? 
Exercise 1.5 What management approach characterizes 
(a) A school system? 
(b) A group of scouts? 
(c) A football team? 
Exercise 1.6 Provide examples of how you, personally, apply inspection, process 
control or quality by design approaches. 
(a) As a student 
(b) In your parents’ house 
(c) With your friends 
Exercise 1.7 Evaluate the information quality of a case study provided by your 
instructor.

Chapter 2 
Basic Tools and Principles of Process 
Control 
Preview Competitive pressures are forcing many management teams to focus on 
process control and process improvement, as an alternative to screening and inspec-
tion. This chapter discusses techniques used effectively in industrial organizations 
that have adopted such ideas as concepts. Classical control charts, quality control, 
and quality planning tools are presented along with modern statistical process 
control procedures including new statistical techniques for constructing conﬁdence 
intervals of process capability indices and analyzing Pareto charts. Throughout the 
chapter, a software piston simulator is used to demonstrate how control charts are 
set up and used in real-life applications. 
2.1 
Basic Concepts of Statistical Process Control 
In this chapter, we present the basics of statistical process control (SPC). The 
general approach is prescriptive and descriptive rather than analytical. With SPC, we 
do not aim at modeling the distribution of data collected from a given process. Our 
goal is to control the process with the aid of decision rules for signaling signiﬁcant 
discrepancies between the observed data and the standards of a process under 
control. We demonstrate the application of SPC to various processes by referring to 
the examples of piston cycle time and strength of ﬁbers, which have been discussed 
in Chapter 1 in Modern Statistics (Kenett et al. 2022b). Other examples used include 
data on power failures in a computer center and ofﬁce procedures for scheduling 
appointments of a university dean. The data on the piston cycle time are generated 
by the piston simulator function pistonSimulation available in the mistat package. 
In order to study the causes for variability in the piston cycle time, we present, in 
Supplementary Information The online version contains supplementary material available at 
(https://doi.org/10.1007/978-3-031-28482-3_2). 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3_2
11

12
2
Basic Tools and Principles of Process Control
Fig. 2.1 A sketch of the  
piston 
exhaust valve
inlet valve 
cylinder head 
cooling water 
piston 
connecting rod 
crankshaft 
spark plug 
valve spring 
Table 2.1 Operating factors of the piston simulator 
Factor
Units
Minimum
Maximum 
Piston weight
M [Kg]
30
60 
Piston surface area
S [msquared2]
0.005
0.020 
Initial gas volume
VSubscript 00 [mcubed3]
0.002
0.010 
Spring coeff.
K [N/m]
1000
5000 
Atmosph. pressure
PSubscript 00 [N/msquared2]
90,000
110,000 
Ambient temperat.
T [Superscript ring◦K]
290
296 
Filling gas temperat.
TSubscript 00 [Superscript ring◦K]
340
360 
Fig. 2.1, a sketch of a piston and, in Table 2.1, seven factors that can be controlled 
to change the cycle time of a piston. 
Figure 2.2 is a run chart (also called “connected line plot”), and Fig. 2.3 is a 
histogram, of 50 piston cycle times (seconds) measured under stable operating 
conditions. Throughout the measurement time frame, the piston operating fac-
tors remained ﬁxed at their maximum levels. The data can be found in dataset 
OTURB1.csv.
The average cycle time of the 50 cycles is 0.392 [sec] with a standard deviation 
of 0.114 [sec]. 
Even though no changes occurred in the operating conditions of the piston, we 
observe variability in the cycle times. From Fig. 2.2, we note that cycle times vary 
between 0.22 and 0.69 s. The histogram in Fig. 2.3 indicates some skewness in the 
data. The normal probability plot of the 50 cycle times (Fig. 2.4) also leads to the 
conclusion that the cycle time distribution is skewed.
Another example of variability is provided by the yarn strength data presented in 
Chapter 1 in Modern Statistics (Kenett et al. 2022b). The yarn strength test results 
indicate that there is variability in the properties of the product. High yarn strength 
indicates good spinning and weaving performance. Yarn strength is considered a 
function of the ﬁber length, ﬁber ﬁneness, and ﬁber tensile strength. As a general 
rule, longer cottons are ﬁne-ﬁbered and shorter cottons coarse-ﬁbered. Very ﬁne 
ﬁbers, however, tend to reduce the rate of processing, so that the degree of ﬁber

2.1
Basic Concepts of Statistical Process Control
13
Fig. 2.2 Run chart or connected line plot of 50 piston cycle times [sec] 
Fig. 2.3 Histogram of 50 piston cycle times
ﬁneness depends upon the speciﬁc end product use. Variability in ﬁber ﬁneness is a 
major cause of variability in yarn strength and processing time. 
In general, a production process has many sources or causes of variation. These 
can be further subdivided as process inputs and process operational characteristics 
including equipment, procedures, and environmental conditions. Environmental 
conditions consist of factors such as temperature and humidity or work tools. 
Visual guides, for instance, might not allow operators to precisely position parts on 
ﬁxtures. The complex interactions between material, tools, machine, work methods, 
operators, and the environment combine to create variability in the process. Factors 
that are permanent, as a natural part of the process, are causing chronic problems

14
2
Basic Tools and Principles of Process Control
Fig. 2.4 Normal probability plot of 50 piston cycle times
and are called common causes of variation. The combined effect of common 
causes can be described using probability distributions. Such distributions were 
introduced in Chap. 1 and their theoretical properties presented in Chapter 2 in 
Modern Statistics (Kenett et al. 2022b). It is important to recognize that recurring 
causes of variability affect every work process and that even under a stable process 
there are differences in performance over time. Failure to recognize variation 
leads to wasteful actions and detrimental overcontrol. The only way to reduce 
the negative effects of chronic, common causes of variability is to modify the 
process. This modiﬁcation can occur at the level of the process inputs, the process 
technology, the process controls, or the process design. Some of these changes are 
technical (e.g., different process settings), some are strategic (e.g., different product 
speciﬁcations), and some are related to human resources management (e.g., training 
of operators). Special causes, assignable causes, or  sporadic spikes arise from 
external temporary sources that are not inherent to the process. These terms are 
used here interchangeably. For example, an increase in temperature can potentially 
affect the piston’s performance. The impact can be in terms of changes in both the 
average cycle times and the variability in cycle times. 
In order to signal the occurrence of special causes, we need a control mechanism. 
Speciﬁcally in the case of the piston, such a mechanism can consist of taking 
samples or subgroups of 5 consecutive piston cycle times. Within each subgroup, 
we compute the subgroup average and standard deviation. 
Figures 2.5 and 2.6 display charts of the average and standard deviations of 20 
samples of 5 cycle time measurements. To generate these charts with Python, we 
use:

2.1
Basic Concepts of Statistical Process Control
15
Fig. 2.5 X-bar chart of cycle times under stable operating conditions 
Fig. 2.6 S-chart of cycle times under stable operating conditions

16
2
Basic Tools and Principles of Process Control
simulator = mistat.PistonSimulator(n_simulation=20, n_replicate=5, seed=1) 
Ps = simulator.simulate() 
Ps['seconds'].groupby(Ps['group']).apply(np.mean) 
group 
1
0.044902 
2
0.042374 
3
0.043812 
4
0.048865 
5
0.047265 
6
0.043910 
7
0.048345 
8
0.041833 
9
0.041135 
10
0.045080 
11
0.044307 
12
0.047490 
13
0.045008 
14
0.045684 
15
0.046281 
16
0.044656 
17
0.044445 
18
0.044227 
19
0.041077 
20
0.044947 
Name: seconds, dtype: float64 
The chart of averages is called an X-bar chart, and the chart of standard deviations 
is called an S-chart. All 100 measurements were taken under ﬁxed operating 
conditions of the piston (all factors set at the maximum levels). We note that the 
average of cycle time averages is 0.045 s and that the average of the standard 
deviations of the 20 subgroups is 0.0048 s. All these numbers were generated by 
the piston computer simulation model that allows us to change the factors affecting 
the operating conditions of the piston. Again we know that no changes were made 
to the control factors. The observed variability is due to common causes only such 
as variability in atmospheric pressure or ﬁlling gas temperature. 
We now rerun the piston simulator introducing a forced change in the piston 
ambient temperature. At the beginning of the 8th sample, temperature begins to rise 
at a rate of 20% per group. Can we ﬂag this special cause? The X-bar chart of 
these new simulated data is presented in Fig. 2.7. Up to the 7th sample, the chart is 
identical to that of Fig. 2.5. At the 8th sample, we note a small increase in cycle time. 
As of the 11th sample, the subgroup averages are consistently above 0.05 s. This run 
persists until the 21st sample when we stopped the simulation. To have 10 points in 
a row above the average is unlikely to occur by chance alone. The probability of 
such an event is left parenthesis 1 divided by 2 right parenthesis Superscript 10 Baseline equals 0.00098(1/2)10 = 0.00098. The implication of the 10 points run is that 
common causes are no longer the only causes of variation and that a special factor 
has begun affecting the piston’s performance. In this particular case, we know that it 
is an increase in ambient temperature. The S-chart of the same data (Fig. 2.8) shows  
a downward trend with several points falling below the average of 0.004 beginning 
at the 8th sample. This indication occurs earlier than that in the X-bar chart. The 
information obtained from both charts indicates that a special cause has been in

2.1
Basic Concepts of Statistical Process Control
17
Fig. 2.7 X-bar chart of cycle times with a trend in ambient temperature 
Fig. 2.8 S-chart of cycle times with a trend in ambient temperature

18
2
Basic Tools and Principles of Process Control
Fig. 2.9 X-bar chart of cycle times with a trend in spring coefﬁcient precision
effect from the 8th sample onward. Its effect has been to increase cycle times and 
reduce variability. The new average cycle time appears to be around 0.052 s. 
The piston simulator allows us to try other types of changes in the operational 
parameters of the piston. For example, we can change the spring that controls the 
intake valve in the piston gas chamber. In the next simulation, the standard deviation 
of the spring coefﬁcient is increasing at a 15% rate past the 8th sample. Figures 2.9 
and 2.10 are X-bar and S-charts corresponding to this scenario. Until the 8th sample, 
these charts are identical to those in Figs. 2.5 and 2.6. After the 8th sample, changes 
appear in the chart. In particular, we see a large drop in cycle times at the end. We 
also see changes in the S-chart in Fig. 2.10. We see that after the 13th sample, the 
standard deviation trends upward. 
Control charts have wide applicability throughout an organization. Top managers 
can use a control chart to study variation in sales and decide on new marketing 
strategies. Operators can use the same tool to determine if and when to adjust a 
manufacturing process. An example with universal applicability comes from the 
scheduling process of daily appointments in a university dean’s ofﬁce. At the end of 
each working day, the various meetings and appointment coordinated by the ofﬁce 
of the Dean were classiﬁed as being “on time” or with a problem such as “late 
beginning,”  “did  not  end  on  time,”  “was  interrupted,”  etc.. . . The  ratio  of  problem  
appointments to the total number of daily appointments was tracked and control 
limits computed. Figure 2.11 is the Dean’s control chart (see Kelly et al. 1991). 

2.1
Basic Concepts of Statistical Process Control
19
Fig. 2.10 S-chart of cycle times with a trend in spring coefﬁcient precision 
Fig. 2.11 Control chart for proportion of appointments with scheduling problems (based on a 
chart prepared by Dean of the School of Management at SUNY Binghamton)

20
2
Basic Tools and Principles of Process Control
Another example of a special cause is the miscalibration of spinning equipment 
and yarn strength mentioned above. Miscalibration can be identiﬁed by ongoing 
monitoring of yarn strength. Process operators analyzing X-bar and S-charts can 
stop and adjust the process as trends develop or sporadic spikes appear. Timely 
indication of a sporadic spike is crucial to the effectiveness of process control 
mechanisms. Ongoing chronic problems, however, cannot be resolved by using 
local operator adjustments. The statistical approach to process control allows 
us to distinguish between chronic problems and sporadic spikes. This is crucial 
since these two different types of problems require different approaches. Process 
control ensures that a process performs at a level determined “doable” by a process 
capability study. Section 2.3 discusses how to conduct such studies and how to set 
control limits. 
So far we focused on the analysis of data for process control. Another essential 
component of process control is the generation and routing of relevant and timely 
data through proper feedback loops. We distinguish between two types of feedback 
loops: External feedback loops and internal feedback loops. An external feedback 
loop consists of information gathered at a subsequent downstream process or by 
direct inspection of the process outputs. 
To illustrate these concepts and ideas, let us look at the process of driving to 
work. The time it takes you to get to work is a variable that depends on various 
factors such as how many other cars are on the road, how you happen to catch the 
trafﬁc lights, your mood that morning, and so on. These are factors that are part of 
the process, and you have little or no control over them. Such common causes create 
variation in the time it takes you to reach work. One day it may take you 15 min and 
the next day 12 min. If you are particularly unlucky and had to stop at all the red 
lights, it might take you 18 min. Suppose, however, that on one particular day it took 
you 45 min to reach work. Such a long trip is outside the normal range of variation 
and is probably associated with a special cause such as a ﬂat tire, a trafﬁc jam, or 
road constructions. 
External feedback loops rely on measurements of the process outcome. They 
provide information like looking at a rear view mirror. The previous example 
consisted of monitoring time after you reached work. In most cases, identifying 
a special cause at that point in time is too late. Suppose that we had a local 
radio station that provided its listeners live coverage of the trafﬁc conditions. If we 
monitor, on a daily basis, the volume of trafﬁc reported by the radio, we can avoid 
trafﬁc jams, road constructions, and other unexpected delays. Such information will 
help us eliminate certain special causes of variation. Moreover, if we institute a 
predictive maintenance program for our car, we can eliminate many types of engine 
problems, further reducing the impact of special causes. To eliminate the occasional 
ﬂat tire would involve improvements in road maintenance—a much larger task. The 
radio station is a source of internal feedback that provides information that can be 
used to correct your route and thus arrive at work on time almost every day. This is 
equivalent to driving the process while looking ahead. Most drivers are able to avoid 
getting off the road, even when obstacles present themselves unexpectedly. We now 
proceed to describe how control charts are used for “staying on course.”

2.1
Basic Concepts of Statistical Process Control
21
Fig. 2.12 The supplier–process–customer structure and its feedback loops
Manufacturing examples consist of physical dimensions of holes drilled by a 
numerically controlled CNC machine, piston cycle times, or yarn strength. The 
ﬁnished part leaving a CNC machine can be inspected immediately after the drilling 
operation or later, when the part is assembled into another part. Piston cycle times 
can be recorded online or stored for off-line analysis. Another example is the 
testing of electrical parameters at ﬁnal assembly of an electronic product. The test 
data reﬂect, among other things, the performance of the components’ assembly 
process. Information on defects such as missing components, wrong or misaligned 
components should be fed back, through an external feedback loop, to the assembly 
operators. Data collected on process variables, measured internally to the process, 
are the basis of an internal feedback loop information ﬂow. An example of such 
data is the air pressure in the hydraulic system of a CNC machine. Air pressure can 
be measured so that trends or deviations in pressure are detected early enough to 
allow for corrective action to take place. Another example consists of the tracking 
of temperature in the surroundings of a piston. Such information will directly point 
out the trend in temperature that was indirectly observed in Figs. 2.7 and 2.8. 
Moreover, routine direct measurements of the precision of the spring coefﬁcient 
will exhibit the trend that went unnoticed in Figs. 2.9 and 2.10. The relationship 
between a process, its suppliers, and its customers is presented in Fig. 2.12. Internal 
and external feedback loops depend on a coherent structure of suppliers, processes, 
and customers. It is in this context that one can achieve effective statistical process 
control. 
We discussed in this section the concepts of feedback loops, chronic problems 
(common causes), and sporadic spikes (special causes). Data funneled through 
feedback loops are used to indicate what are the types of forces affecting the 
measured process. Statistical process control is “a rule of behavior that will strike 
a balance for the net economic loss from two sources of mistake: (1) looking for 
special causes too often, or overadjusting; (2) not looking often enough” (excerpt 
from Deming 1967). In the implementation of statistical process control, one 
distinguishes between two phases: (1) achieving control and (2) maintaining control. 
Achieving control consists of a study of the causes of variation followed by an 
effort to eliminate the special causes and a thorough understanding of the remaining 

22
2
Basic Tools and Principles of Process Control
permanent factors affecting the process, the common causes. Tools such as graphic 
displays (Chaps. 1 and 4 in the Modern Statistics companion volume, Kenett et al. 
2022b), correlation and regression analysis (Section 4.3, also in Modern Statistics), 
control charts (Chaps. 2–4), and designed experiments (Chaps. 5 and 6) are typically 
used in a process capability study whose objective is to achieve control. Section 2.3 
will discuss the major steps of a process capability study and the determination of 
control limits on the control charts. Once control is achieved, one has to maintain it. 
The next section describes how control is maintained with the help of control limits. 
2.2 
Driving a Process with Control Charts 
Control charts allow us to determine when to take action in order to adjust a process 
that has been affected by a special cause. Control charts also tell us when to leave a 
process alone and not misinterpret variations due to common causes. Special causes 
need to be addressed by corrective action. Common causes are the focus of ongoing 
efforts aimed at improving the process. 
We distinguish between control charts for variable data and control charts for 
attribute data. Attribute data require an operational deﬁnition of what constitutes 
a problem or defect. When the observation unit is classiﬁed into one of the two 
categories (e.g., “pass” vs. “fail” or conforming vs. nonconforming), we can track 
the proportion of nonconforming units in the observation sample. Such a chart is 
called a p-chart. If the size of the observation sample is ﬁxed, we can simply track 
the number of nonconforming units and derive an np-chart. When an observation 
consists of the number of nonconformities per unit of observation, we track either 
the number of nonconformities (c-charts) or rates of nonconformities (u-charts). 
Rates are computed by dividing the number of nonconformities by the number 
of opportunities for errors or problems. For variable data, we distinguish between 
processes that can be repeatedly sampled under uniform conditions and processes 
where measurements are derived one at a time (e.g., monthly sales). In the latter 
case, we will use control charts for individual data also called moving range charts. 
When data can be grouped, we can use a variety of charts such as the X-bar chart 
or the median chart discussed in detail in Chap. 3. We proceed to demonstrate how 
X-bar control charts actually work using the piston cycle times discussed earlier. 
An X-bar control chart for the piston’s cycle time is constructed by ﬁrst grouping 
observations by time period and then summarizing the location and variability 
statistics in these subgroups. An example of this was provided in Figs. 2.5 and 
2.6 where the average and standard deviations of 5 consecutive cycle times were 
tracked over 20 such subgroups. The three lines that are added to the simple run 
charts are the center line, positioned at the grand average, the lower control 
limits (LCL), and the upper control limits (UCL). The UCL and LCL indicate 
the range of variability we expect to observe around the center line, under stable 
operating conditions. Figure 2.5 shows averages of 20 subgroups of 5 consecutive 
cycle times each. The center line and control limits are computed from the average

2.2
Driving a Process with Control Charts
23
of the 20 subgroup averages and the estimated standard deviation for averages of 
samples of size 5. The center line is at 0.394 s. When using the classical 3-sigma 
charts developed by Shewhart, the control limits are positioned at three standard 
deviations of upper X overbar¯X, namely 3 sigma divided by StartRoot n EndRoot3σ/√n, away from the center line. In this example, we 
ﬁnd that UCL equals= 0.562 s and LCL equals= 0.227 s. Under stable operating conditions, with 
only common causes affecting performance, the chart will typically have all points 
within the control limits. Speciﬁcally with 3-sigma control limits, we expect to 
have, on the average, only one out of 370 points (1/.0027), outside these limits, 
a rather rare event. Therefore, when a point falls beyond the control limits, we can 
safely question the stability of the process. The risk that such an alarm will turn 
to be false is 0.0027. A false alarm occurs when the sample mean falls outside 
the control limits, and we suspect an assignable cause, but only common causes 
are operating. Moreover, stable random variation does not exhibit patterns such as 
upward or downward trends, or consecutive runs of points above or below the center 
line. We saw earlier how a control chart was used to detect an increase in ambient 
temperature of a piston from the cycle times. The X-bar chart (Fig. 2.7) indicates a 
run of six or more points above the center line. Figure 2.13 shows several patterns 
that indicate non-randomness. These are:
1. A single point outside the control limits 
2. A run of nine or more points in a row above (or below) the center line 
3. Six consecutive points increasing (trend up) or decreasing (trend down) 
4. Two out of three points in a region between mu plus or minus 2 sigma divided by StartRoot n EndRootμ ± 2σ/√n and mu plus or minus 3 sigma divided by StartRoot n EndRootμ ± 3σ/√n
A comprehensive discussion of detection rules and properties of the classical 3-
sigma control charts and of other modern control chart techniques is presented in 
Chap. 3. 
As we saw earlier, there are many types of control charts. Selection of the control 
chart to use in a particular application primarily depends on the type of data that 
will ﬂow through the feedback loops. The piston provides us with an example of 
variable data, and we used an X-bar and S-chart to monitor the piston’s performance. 
Examples of attribute data are blemishes on a given surface, wave solder defects, 
below standard service level at the bank, and missed shipping dates. Each type of 
data leads to a different type of control chart. All control charts have a center line 
and upper and lower control limits (UCL and LCL). In general, the rules for ﬂagging 
special causes are the same in every type of control chart. Figure 2.14 presents a 
classiﬁcation of the various control charts. Properties of the different types of charts, 
including the more advanced EWMA and CUSUM charts, are presented in Chap. 3. 
We discussed earlier several examples of control charts and introduced different 
types of control charts. The block diagram in Fig. 2.14 organizes control charts by 
the type of data ﬂowing through feedback loops. External feedback loops typically 
rely on properties of the process’ products and lead to control charts based on counts 
or classiﬁcation. If products are classiﬁed using “pass” versus “fail” criteria, one 
will use np-charts or p-charts depending on whether the products are tested in ﬁxed 
or variable subgroups. The advantage of such charts is that several criteria can be 
combined to produce a deﬁnition of what constitutes a “fail” or defective product.

24
2
Basic Tools and Principles of Process Control
Fig. 2.13 Patterns to detect special causes

2.3
Setting Up a Control Chart: Process Capability Studies
25
Fig. 2.14 Classiﬁcation of control charts 
When counting nonconformities or incidences of a certain event or phenomenon, 
one is directed to use c-charts or u-charts. These charts provide more information 
than p-charts or np-charts since the actual number of nonconformities in a product 
is accounted for. The drawback is that several criteria cannot be combined without 
weighing the different types of nonconformities. C-charts assume a ﬁxed likelihood 
of incidence, and u-charts are used in cases of varying likelihood levels. For large 
subgroups (subgroup sizes larger than 1000), the number of incidences, incidences 
per unit, the number of defectives or percent defectives can be considered as 
individual measurements, and an X -chart for subgroups of size 1 can be used. 
Internal feedback loops and, in some cases, also external feedback loops rely 
on variable data derived from measuring product or process characteristics. If 
measurements are grouped in samples, one can combine X-bar charts with R-charts 
or S-charts. Such combinations provide a mechanism to control stability of a process 
with respect to both location and variability. X-bar charts track the sample averages, 
R-charts track sample ranges (maximum–minimum), and S-charts are based on 
sample standard deviations. For samples larger than 10, S-charts are recommended 
over R-charts. For small samples and manual maintenance of control charts, R-
charts are preferred. When sample sizes vary, only S-charts should be used to track 
variability. 
2.3 
Setting Up a Control Chart: Process Capability Studies 
Setting up control limits of a control chart requires a detailed study of process 
variability and of the causes creating this variability. Control charts are used to detect

26
2
Basic Tools and Principles of Process Control
occurrence of special, sporadic causes while minimizing the risk of misinterpreting 
special causes as common causes. In order to achieve this objective, one needs to 
assess the effect of chronic, common causes and then set up control limits that reﬂect 
the variability resulting from such common causes. The study of process variability 
that precedes the setting up of control charts is called a process capability study. 
We distinguish between attribute process capability studies and variable process 
capability studies. 
Attribute process capability studies determine a process capability in terms 
of fraction of defective or nonconforming output. Such studies begin with data 
collected over several time periods. A rule of thumb is to use three time periods 
with 20 to 25 samples of size 50 to 100 units each. For each sample, the control 
chart statistic is computed and a control chart is drawn. This will lead to a p-, np-, 
c-, or u-chart and investigation patterns ﬂagging special causes such as those in 
Fig. 2.13. Special causes are then investigated and possibly removed. This requires 
changes to the process that justify removal of the measurements corresponding to 
the time periods when those special causes were active. The new control charts, 
computed without these points, indicate the capability of the process. Its center line 
is typically used as a measure of process capability. For example in Fig. 2.11, one 
can see that the process capability of the scheduling of appointments at the Dean’s 
ofﬁce improved from 25% of appointments with problems to 15% after introducing 
a change in the process. The change consisted of acknowledging appointments with 
a conﬁrmation note spelling out, time, date and topic of appointment, a brief agenda, 
and a scheduled ending time. On the 25th working day, there was one sporadic spike 
caused by illness. The Dean had to end early that day and several appointments got 
canceled. When sample sizes are large (over 1000 units), control charts for attribute 
data become ineffective because of very narrow control limits and X-charts for 
individual measurements are used. 
Variable process capability studies determine a process capability in terms of
the distribution of measurements on product or process characteristics. Setting up
of control charts for variable data requires far less data than attribute data control
charts. Data are collected in samples, called rational subgroups, selected from a
time frame so that relatively homogeneous conditions exist within each subgroup.
The design strategy of rational subgroups is aimed at measuring variability due to
common causes only. Control limits are then determined from measures of location
and variability in each rational subgroup. The control limits are set to account for
variability due to these common causes. Any deviation from stable patterns relative
to the control limits (see Fig. 2.13) indicates a special cause. For example, in the 
piston case study, a rational subgroup consists of 5 consecutive cycle times. The 
statistics used are the average and standard deviation of the subgroups. The 3-sigma 
control limits are computed to be UCL equals 0.052 and LCL equals 0.038. From an analysis 
of Fig. 2.5, we conclude that the X-bar chart, based on a connected time plot of 
20 consecutive averages, exhibits a pattern that is consistent with a stable process. 
We can now determine the process capability of the piston movement within the 
cylinder.
=
=

2.3
Setting Up a Control Chart: Process Capability Studies
27
LCL
UCL 
0 
0.037736 
0.051829 
Process Capability Analysis 
Number of obs = 100
Target = 0.04 
Center = 0.04
LSL = 0.03 
StdDev = 0.005252
USL = 0.05 
Capability indices: 
Value
2.5%
97.5% 
Cp
0.6347 
0.5463 
0.7228 
Cp_l
0.9382 
0.8156 
1.0608 
Cp_u
0.3312 
0.2640 
0.3983 
Cp_k
0.3312 
0.2512 
0.4111 
Cpm
0.4693 
0.3910 
0.5474 
Exp<LSL
0%
Obs<LSL
0% 
Exp>USL 
16%
Obs>USL 
17% 
z_target: -0.910 
z_usl: 0.994 
p_above_usl: 0.160 
Process capability for variable data is a characteristic that reﬂects the probability 
of the individual outcomes of a process to be within the engineering speciﬁcation 
limits. Assume that the piston engineering speciﬁcations stipulate a nominal value of 
0.04 s and maximum and minimum values of 0.05 and 0.03 s, respectively. Table 2.2 
shows the output from the process capability analysis included in the mistat 
package.

28
2
Basic Tools and Principles of Process Control
Table 2.2 Process capability analysis of piston cycle time 
simulator = mistat.PistonSimulator(n_simulation=20, n_replicate=5, seed=1) 
Ps = simulator.simulate() 
cycleTime = mistat.qcc_groups(Ps['seconds'], Ps['group']) 
qcc = mistat.QualityControlChart(cycleTime) 
print(qcc.limits) 
pc = mistat.ProcessCapability(qcc, spec_limits = [0.03, 0.05]) 
pc.plot() 
plt.show() 
pc.summary() 
z_target = (0.04 - 0.04478) / 0.0052521 
z_usl = (0.05 - 0.04478) / 0.0052521 
p_above_usl = 1-stats.norm.cdf((0.05 - 0.04478) / 0.0052521) 
print(f'z_target: {z_target:.3f}') 
print(f'z_usl: {z_usl:.3f}') 
print(f'p_above_usl: {p_above_usl:.3f}') 
The 50 measurements that were produced under stable conditions have a mean 
(average) of 0.045 s and a standard deviation of 0.005 s. The predicted proportion of 
cycle times beyond the speciﬁcation limits is computed using the normal distribution 
as an approximation. The computations yield that, under stable operating conditions, 
an estimated 16% of future cycle times will be above 0.05 s, and that 0.24% will 
be below 0.03 s. We clearly see that the nominal value of 0.04 s is slightly lower 
than the process average, having a Z-score of negative 0.91−0.91, and that the upper limit, or 
maximum speciﬁcation limit, is 0.99 standard deviations above the average. The 
probability that a standard normal random variable is larger than 0.99 is 0.16. This 
is an estimate of the future percentage of cycle times above the upper limit of 0.5 s, 
provided stable conditions prevail. It is obvious from this analysis that the piston 
process is incapable of complying with the engineering speciﬁcations. 
2.4 
Process Capability Indices 
In assessing the process capability for variable data, two indices are used: upper C Subscript pCp and 
upper C Subscript p kCpk. The ﬁrst index is an indicator of the potential of a process to meet two-sided 
speciﬁcations with as few defects as possible. For symmetric speciﬁcation limits, 
the full potential is actually achieved when the process is centered at the mid-point 
between the speciﬁcation limits. In order to compute upper C Subscript pCp, one simply divides the 
process tolerance by six standard deviations, i.e., 
upper C Subscript p Baseline equals StartFraction Upper Specification Limit minus Lower Specification Limit Over 6 times Standard Deviation EndFraction periodCp = Upper Speciﬁcation Limit −Lower Speciﬁcation Limit
6 × Standard Deviation
.
(2.4.1)

2.4
Process Capability Indices
29
The numerator indicates how wide the speciﬁcations are, the denominator measures 
the width of the process. Under normal assumptions, the denominator is a range 
of values that accounts for 99.73% of the observations from a centered process, 
operating under stable conditions with variability only due to common causes. When 
upper C Subscript p Baseline equals 1Cp = 1, we expect 0.27% of the observations to fall outside the speciﬁcation limits. 
A target for many modern industries is to reach, on every process, a level of upper C Subscript p Baseline equals 2Cp = 2, 
which practically guarantees that under stable conditions, and for processes kept 
under control around the process nominal values, there will be no defective products 
(“zero defects”). With upper C Subscript p Baseline equals 2Cp = 2, the theoretical estimate under normal assumptions, 
allowing for a possible shift in the location of the process mean by as much as 
1.5 standard deviations, is 3.4 cases per million observations outside speciﬁcation 
limits. 
Another measure of process capability is 
upper C Subscript p k Baseline equals minimum left parenthesis upper C Subscript p u Baseline comma upper C Subscript p l Baseline right parenthesis commaCpk = minimum(Cpu, Cpl),
(2.4.2) 
where 
upper C Subscript p u Baseline equals StartFraction Upper Specification Limit minus Process Mean Over 3 times Standard Deviation EndFractionCpu = Upper Speciﬁcation Limit −Process Mean
3 × Standard Deviation
(2.4.3) 
and 
upper C Subscript p l Baseline equals StartFraction Process Mean minus Lower Specification Limit Over 3 times Standard Deviation EndFraction periodCpl = Process Mean −Lower Speciﬁcation Limit
3 × Standard Deviation
.
When the process mean is not centered midway between the speciﬁcation limits, 
upper C Subscript p kCpk is different from upper C Subscript pCp. Non-centered processes have their potential capability 
measured by upper C Subscript pCp, and their actual capability measured by upper C Subscript p kCpk. As shown  in  
Table 2.2, for the piston data, estimates of upper C Subscript pCp and upper C Subscript p kCpk are ModifyingAbove upper C With caret Subscript p Baseline equals 0.63 ˆCp = 0.63 and 
ModifyingAbove upper C With caret Subscript p k Baseline equals 0.33 ˆCpk = 0.33. This indicates that something could be gained by centering the piston 
cycle times around 0.04 s. Even if this is possible to achieve, there will still be 
observations outside the upper and lower limits, since the standard deviation is too 
large. 
The validity of the upper C Subscript pCp and upper C Subscript p kCpk indices is questionable in cases where the 
measurements on X are not normally distributed but have skewed distributions. The 
proper form of a capability index under non-normal conditions can be treated with 
bootstrapping (Chapter 3, Modern Statistics, Kenett et al. 2022b). Some authors 
offered partial analytic solutions (Kotz and Johnson 1993). 
It is common practice to estimate upper C Subscript pCp or upper C Subscript p kCpk, by substituting the sample mean, 
upper X overbar¯X, and the sample standard deviation S, for the process mean, muμ, and the process 
standard deviation sigmaσ, i.e., 
ModifyingAbove upper C With caret Subscript p u Baseline equals StartFraction xi Subscript upper U Baseline minus upper X overbar Over 3 upper S EndFraction comma ModifyingAbove upper C With caret Subscript p l Baseline equals StartFraction upper X overbar minus xi Subscript upper L Baseline Over 3 upper S EndFraction ˆCpu = ξU −¯X
3S
,
ˆCpl =
¯X −ξL
3S
(2.4.4)

30
2
Basic Tools and Principles of Process Control
and ModifyingAbove upper C With caret Subscript p k Baseline equals min left parenthesis ModifyingAbove upper C With caret Subscript p u Baseline comma ModifyingAbove upper C With caret Subscript p l Baseline right parenthesis ˆCpk = min( ˆCpu, ˆCpl), where xi Subscript upper LξL and xi Subscript upper UξU are the lower and upper speciﬁcation 
limits. The question is how close is ModifyingAbove upper C With caret Subscript p k ˆCpk to the true process capability value? We 
develop below conﬁdence intervals for upper C Subscript p kCpk, which have conﬁdence levels close to 
the nominal left parenthesis 1 minus alpha right parenthesis(1 −α) in large samples. The derivation of these intervals depends 
on the following results that invoke mathematical derivations and can be skipped 
without loss of continuity: 
1. In a large size random sample from a normal distribution, the sampling distribu-
tion of S is approximately normal, with mean sigmaσ and variance sigma squared divided by 2 nσ 2/2n. 
2. In a random sample from a normal distribution, the sample mean, upper X overbar¯X, and the 
sample standard deviation S are independent. 
3. If A and B are events such that PrStartSet upper A EndSet equals 1 minus alpha divided by 2{A} = 1 −α/2 and PrStartSet upper B EndSet equals 1 minus alpha divided by 2{B} = 1 −α/2, then 
Prleft brace upper A intersection upper B right brace greater than or equals 1 minus alpha{A ∩B} ≥1 −α. (This inequality is called the Bonferroni inequality.) 
In order to simplify notation, let 
rho 1 equals upper C Subscript p l Baseline comma rho 2 equals upper C Subscript p u Baseline and omega equals upper C Subscript p k Baseline periodρ1 = Cpl,
ρ2 = Cpu
and
ω = Cpk.
Notice that since upper X overbar¯X is distributed like upper N left parenthesis mu comma StartFraction sigma squared Over n EndFraction right parenthesisN

μ, σ 2
n

, upper X overbar minus xi Subscript upper L ¯X −ξL is distributed like 
upper N left parenthesis mu minus xi Subscript upper L Baseline comma StartFraction sigma squared Over n EndFraction right parenthesisN

μ −ξL, σ 2
n

. Furthermore, by the above results 1 and 2, the distribution of 
upper X overbar minus xi Subscript upper L Baseline minus 3 upper S rho 1 ¯X −ξL −3Sρ1 in large samples is like that of 
upper N left parenthesis 0 comma StartFraction sigma squared Over n EndFraction left parenthesis 1 plus nine halves rho 1 squared right parenthesis right parenthesis periodN

0, σ 2
n

1 + 9
2ρ2
1

.
It follows that, in large samples, 
StartStartFraction left parenthesis upper X overbar minus xi Subscript upper L Baseline minus 3 upper S rho 1 right parenthesis squared OverOver StartFraction upper S squared Over n EndFraction left parenthesis 1 plus nine halves rho 1 squared right parenthesis EndEndFraction( ¯X −ξL −3Sρ1)2
S2
n

1 + 9
2ρ2
1

is distributed like upper F left bracket 1 comma n minus 1 right bracketF[1, n −1]. Or,  
probability left brace StartStartFraction left parenthesis upper X overbar minus xi Subscript upper L Baseline minus 3 upper S rho 1 right parenthesis squared OverOver StartFraction upper S squared Over n EndFraction left parenthesis 1 plus nine halves rho 1 squared right parenthesis EndEndFraction less than or equals upper F Subscript 1 minus alpha divided by 2 Baseline left bracket 1 comma n minus 1 right bracket right brace equals 1 minus alpha divided by 2 periodPr
⎧
⎨
⎩
( ¯X −ξL −3Sρ1)2
S2
n

1 + 9
2ρ2
1

≤F1−α/2[1, n −1]
⎫
⎬
⎭= 1 −α/2.
(2.4.5) 
Thus, let rho Subscript 1 comma alpha Superscript left parenthesis upper L right parenthesisρ(L)
1,α and rho Subscript 1 comma alpha Superscript left parenthesis upper U right parenthesisρ(U)
1,α be the two real roots (if they exist) of the quadratic equation 
in rho 1ρ1
left parenthesis upper X overbar minus xi Subscript upper L Baseline right parenthesis squared minus 6 upper S rho 1 left parenthesis upper X overbar minus xi Subscript upper L Baseline right parenthesis plus 9 upper S squared rho 1 squared equals upper F Subscript 1 minus alpha divided by 2 Baseline left bracket 1 comma n minus 1 right bracket StartFraction upper S squared Over n EndFraction left parenthesis 1 plus nine halves rho 1 squared right parenthesis period( ¯X−ξL)2 −6Sρ1( ¯X−ξL)+9S2ρ2
1 = F1−α/2[1, n−1]S2
n

1 + 9
2ρ2
1

.
(2.4.6)

2.4
Process Capability Indices
31
Equivalently, rho Subscript 1 comma alpha Superscript left parenthesis upper L right parenthesisρ(L)
1,α and rho Subscript 1 comma alpha Superscript left parenthesis upper U right parenthesisρ(U)
1,α are the two real roots left parenthesis rho Subscript 1 comma alpha Superscript left parenthesis upper L right parenthesis Baseline less than or equals rho Subscript 1 comma alpha Superscript left parenthesis upper U right parenthesis Baseline right parenthesis(ρ(L)
1,α ≤ρ(U)
1,α ) of the quadratic 
equation 
StartLayout 1st Row 9 upper S squared left parenthesis 1 minus StartFraction upper F Subscript 1 minus alpha divided by 2 Baseline left bracket 1 comma n minus 1 right bracket Over 2 n EndFraction right parenthesis rho 1 squared minus 6 upper S left parenthesis upper X overbar minus xi Subscript upper L Baseline right parenthesis rho 1 2nd Row plus left parenthesis left parenthesis upper X overbar minus xi Subscript upper L Baseline right parenthesis squared minus StartFraction upper F Subscript 1 minus alpha divided by 2 Baseline left bracket 1 comma n minus 1 right bracket Over upper S squared n EndFraction right parenthesis equals 0 period EndLayout9S2

1 −F1−α/2[1, n −1]
2n

ρ2
1 −6S( ¯X −ξL)ρ1
+

( ¯X −ξL)2 −F1−α/2[1, n −1]
S2n

= 0.
(2.4.7) 
Substituting in this equation left parenthesis upper X overbar minus xi Subscript upper L Baseline right parenthesis equals 3 upper S ModifyingAbove upper C With caret Subscript p l( ¯X −ξL) = 3S ˆCpl, we obtain the equation 
StartLayout 1st Row left parenthesis 1 minus StartFraction upper F Subscript 1 minus alpha divided by 2 Baseline left bracket 1 comma n minus 1 right bracket Over 2 n EndFraction right parenthesis rho 1 squared minus 2 ModifyingAbove upper C With caret Subscript p l Baseline rho 1 2nd Row plus left parenthesis ModifyingAbove upper C With caret Subscript p l Superscript 2 Baseline minus StartFraction upper F Subscript 1 minus alpha divided by 2 Baseline left bracket 1 comma n minus 1 right bracket Over 9 n EndFraction right parenthesis equals 0 period EndLayout

1 −F1−α/2[1, n −1]
2n

ρ2
1 −2 ˆCplρ1
+

ˆC2
pl −F1−α/2[1, n −1]
9n

= 0.
(2.4.8) 
We assume that n satisﬁes n greater than StartFraction upper F Subscript 1 minus alpha Baseline left bracket 1 comma n minus 1 right bracket Over 2 EndFractionn > F1−α[1,n−1]
2
. Under this condition, 
1 minus StartFraction upper F Subscript 1 minus alpha divided by 2 Baseline left bracket 1 comma n minus 1 right bracket Over 2 n EndFraction greater than 0 comma1 −F1−α/2[1, n −1]
2n
> 0,
and the two real roots of the quadratic equation are 
rho Subscript 1 comma alpha Superscript left parenthesis upper U comma upper L right parenthesis Baseline equals StartStartFraction ModifyingAbove upper C With caret Subscript p l Baseline plus or minus StartRoot StartFraction upper F Subscript 1 minus alpha divided by 2 Baseline left bracket 1 comma n minus 1 right bracket Over n EndFraction EndRoot left parenthesis StartFraction ModifyingAbove upper C With caret Subscript p l Superscript 2 Baseline Over 2 EndFraction plus one ninth left parenthesis 1 minus StartFraction upper F Subscript 1 minus alpha divided by 2 Baseline left bracket 1 comma n minus 1 right bracket Over 2 n EndFraction right parenthesis right parenthesis Superscript 1 divided by 2 Baseline OverOver 1 minus StartFraction upper F Subscript 1 minus alpha divided by 2 Baseline left bracket 1 comma n minus 1 right bracket Over 2 n EndFraction EndEndFraction periodρ(U,L)
1,α
=
ˆCpl ±

F1−α/2[1,n−1]
n
 ˆC2
pl
2 + 1
9

1 −F1−α/2[1,n−1]
2n
1/2
1 −F1−α/2[1,n−1]
2n
.
(2.4.9) 
From the above inequalities, it follows that left parenthesis rho Subscript 1 comma alpha Superscript left parenthesis upper L right parenthesis Baseline comma rho Subscript 1 comma alpha Superscript left parenthesis upper U right parenthesis Baseline right parenthesis(ρ(L)
1,α, ρ(U)
1,α ) is a conﬁdence interval for 
rho 1ρ1 at conﬁdence level 1 minus alpha divided by 21 −α/2. 
Similarly, left parenthesis rho Subscript 2 comma alpha Superscript left parenthesis upper L right parenthesis Baseline comma rho Subscript 2 comma alpha Superscript left parenthesis upper U right parenthesis Baseline right parenthesis(ρ(L)
2,α, ρ(U)
2,α ) is a conﬁdence interval for rho 2ρ2, at conﬁdence level 1 minus alpha divided by 21−α/2, 
where rho Subscript 2 comma alpha Superscript left parenthesis upper U comma upper L right parenthesisρ(U,L)
2,α
are obtained by replacing ModifyingAbove upper C With caret Subscript p l ˆCpl by ModifyingAbove upper C With caret Subscript p u ˆCpu in the above formula of rho Subscript 1 comma alpha Superscript left parenthesis upper U comma upper L right parenthesisρ(U,L)
1,α
. 
Finally, from the Bonferroni inequality and the fact that upper C Subscript p k Baseline equals min left brace upper C Subscript p l Baseline comma upper C Subscript p u Baseline right braceCpk = min{Cpl, Cpu}, we  
obtain that conﬁdence limits for upper C Subscript p kCpk, at level of conﬁdence left parenthesis 1 minus alpha right parenthesis(1 −α), are  given by  
StartLayout 1st Row 1st Column upper C Subscript p k Superscript left parenthesis upper L right parenthesis 2nd Column equals min left parenthesis rho Subscript 1 comma alpha Superscript left parenthesis upper L right parenthesis Baseline comma rho Subscript 2 comma alpha Superscript left parenthesis upper L right parenthesis Baseline right parenthesis 2nd Row 1st Column upper C Subscript p k Superscript left parenthesis upper U right parenthesis 2nd Column equals min left parenthesis rho Subscript 1 comma alpha Superscript left parenthesis upper U right parenthesis Baseline comma rho Subscript 2 comma alpha Superscript left parenthesis upper U right parenthesis Baseline right parenthesis period EndLayoutC(L)
pk = min

ρ(L)
1,α, ρ(L)
2,α

C(U)
pk = min

ρ(U)
1,α , ρ(U)
2,α

.
Example 2.1 In the present example, we illustrate the computation of the conﬁ-
dence interval for upper C Subscript p kCpk. Suppose that the speciﬁcation limits are xi Subscript upper L Baseline equals negative 1ξL = −1 and 
xi Subscript upper U Baseline equals 1ξU = 1. Suppose that mu equals 0μ = 0 and sigma equals 1 divided by 3σ = 1/3. In this case, upper C Subscript p k Baseline equals 1Cpk = 1. We simulate

32
2
Basic Tools and Principles of Process Control
now, using Python, a sample of size n equals 20n = 20, from a normal distribution with mean 
mu equals 0μ = 0 and standard deviation sigma equals 1 divided by 3σ = 1/3. 
np.random.seed(seed=1) # fix random seed for reproducibility 
X = stats.norm.rvs(size=20, scale=1/3) 
Xbar = np.mean(X) 
S = np.std(X) 
We obtain a random sample with upper X overbar equals negative 0.04445 ¯X = −0.04445 and standard deviation upper S equals 0.3666S =
0.3666. 
Cpl = (Xbar - (-1)) / (3 * S) 
Cpu = (1 - Xbar) / (3 * S) 
Cpk = min(Cpu, Cpl) 
F = stats.f.ppf(0.975, 1, 19) 
For this sample, ModifyingAbove upper C With caret Subscript p l Baseline equals ˆCpl = 0.8688 and ModifyingAbove upper C With caret Subscript p u Baseline equals ˆCpu = 0.9497. Thus, the estimate of upper C Subscript p kCpk
is ModifyingAbove upper C With caret Subscript p k Baseline equals ˆCpk = 0.9497. For alpha equals 0.05α = 0.05, upper F 0.975 left bracket 1 comma 19 right bracket equalsF0.975[1, 19] = 5.9216. Obviously, n equals 20 greater than StartFraction upper F 0.975 left bracket 1 comma 19 right bracket Over 2 EndFraction equalsn = 20 >
F0.975[1,19]
2
= 2.9608. According to the formula, 
rho Subscript 1 comma 0.05 Superscript left parenthesis upper U comma upper L right parenthesis Baseline equals StartStartFraction 0.8688 plus or minus StartRoot StartFraction 5.9216 Over 20 EndFraction EndRoot left parenthesis StartFraction left parenthesis 0.8688 right parenthesis squared Over 2 EndFraction plus StartStartFraction 1 minus StartFraction 5.9216 Over 40 EndFraction OverOver 9 EndEndFraction right parenthesis Superscript 1 divided by 2 Baseline OverOver 1 minus StartFraction 5.9216 Over 40 EndFraction EndEndFraction periodρ(U,L)
1,0.05 =
0.8688 ±

5.9216
20

(0.8688)2
2
+ 1−5.9216
40
9
1/2
1 −5.9216
40
.
b = 1 - F/40 
a = np.sqrt(F/20) * np.sqrt( Cpl**2/2 + b/9) 
rho_1U = (Cpl + a) / b 
rho_1L = (Cpl - a) / b 
a = np.sqrt(F/20) * np.sqrt( Cpu**2/2 + b/9) 
rho_2U = (Cpu + a) / b 
rho_2L = (Cpu - a) / b 
Thus, rho Subscript 1 comma 0.05 Superscript left parenthesis upper L right parenthesis Baseline equalsρ(L)
1,0.05 = 0.581 and rho Subscript 1 comma 0.05 Superscript left parenthesis upper U right parenthesis Baseline equalsρ(U)
1,0.05 = 1.4586. Similarly, rho Subscript 2 comma 0.05 Superscript left parenthesis upper L right parenthesis Baseline equalsρ(L)
2,0.05 = 0.6429 and 
rho Subscript 2 comma 0.05 Superscript left parenthesis upper U right parenthesis Baseline equalsρ(U)
2,0.05 = 1.5865. Therefore, the conﬁdence interval, at level 0.95, for upper C Subscript p kCpk is (0.581, 
1.4586) left parenthesis 0.5859 comma 1.4687 right parenthesis(0.5859, 1.4687).
black medium square■
2.5 
Seven Tools for Process Control and Process 
Improvement 
In this section, we review seven tools that have proven extremely effective in helping 
organizations control processes and implement process improvement projects. Some 
of these tools were already presented. For completeness, all the tools are brieﬂy 
reviewed here. 
The preface to the English edition of the famous text by Ishikawa (1986) on  
Quality Control states: “the book was written to introduce quality control practices 
in Japan which contributed tremendously to the country’s economic and industrial

2.5
Seven Tools for Process Control and Process Improvement
33
Fig. 2.15 A typical check sheet
development.” The Japanese work force did indeed master an elementary set of 
tools that helped them improve processes. Seven of the tools were nicknamed the 
“magniﬁcent seven,” and they are: The ﬂow chart, the check sheet, the run chart, the 
histogram, the Pareto chart, the scatterplot, and the cause and effect diagram. 
Flow Charts 
Flow charts are used to describe a process being studied or to 
describe a desired sequence of a new, improved process. Often this is the ﬁrst step 
taken by a team looking for ways to improve a process. The differences between 
how a process could work and how it actually does work expose redundancies, 
misunderstandings, and general inefﬁciencies. 
Check Sheets 
Check sheets are basic manual data collection mechanisms. They 
consist of forms designed to tally the total number of occurrences of certain 
events by category. They are usually the starting point of data collection efforts. 
In setting up a check sheet, one needs to agree on the categories deﬁnitions, the 
data collection time frame, and the actual data collection method. An example of 
a check sheet is provided in Fig. 2.15. 
Run Charts 
Run charts are employed to visually represent data collected over 
time. They are also called connected time plots. Trends and consistent patterns 
are easily identiﬁed on run charts. Example of a run chart is given in Fig. 2.2. 
Histograms 
The histogram was presented in Section 1.4 of Modern Statistics 
(Kenett et al. 2022b) as a graphical display of the distribution of measurements 
collected as a sample. It shows the frequency or number of observations of a 
particular value or within a speciﬁed group. Histograms are used extensively in 
process capability studies to provide clues about the characteristics of the process 
generating the data. However, as we saw in Sect. 2.3, they ignore information on 
the order by which the data were collected. 

34
2
Basic Tools and Principles of Process Control
Fig. 2.16 Pareto chart of software errors
Pareto Charts 
Pareto charts are used extensively in modern organizations. These 
charts help to focus on the important few causes for trouble. When observations 
are collected and classiﬁed into different categories using valid and clear criteria, 
one can construct a Pareto chart. The Pareto chart is a display, using bar graphs 
sorted in descending order, of the relative importance of events such as errors, 
by category. The importance can be determined by the frequency of occurrence 
or weighted, for example, by considering the product of occurrence and cost. 
Superimposed on the bars is a cumulative curve that helps point out the important 
few categories that contain most of cases. Pareto charts are used to choose the 
starting point for problem solving, monitor changes, or identify the basic cause of 
a problem. Their usefulness stems from the Pareto principle that states that in any 
group of factors contributing to a common effect, a relative few (20%) account 
for most of the effect (80%). A Pareto chart of software errors found in testing 
a PBX electronic switch is presented in Fig. 2.16. Errors are labeled according 
to the software unit where they occurred. For example, the “EKT” (Electronic 
Key Telephone) category makes up 6.5% of the errors. What can we learn from 
this about the software development process? The “GEN,” “VHS,” and “HI” 
categories account for over 80% of the errors. These are the causes of problems 
on which major improvement efforts should initially concentrate. Section 2.6 
discusses a statistical test for comparing Pareto charts. Such tests are necessary 
if one wants to distinguish between differences that can be attributed to random 
noise and signiﬁcant differences that should be investigated for identifying an 
assignable cause. 
Scatterplots 
Scatterplots are used to exhibit what happens to one variable, when 
another variable changes. Such information is needed in order to test a theory 

2.6
Statistical Analysis of Pareto Charts
35
Patient conditions 
Muscoloskletal disorder 
Patients anatomy 
Drop of sugar 
Awareness to disability 
Opens another patients restrainst
Opens restraints 
Orientation difficulty 
Alone in dinning room 
Alone in loby 
Alone in yard
Alone in smoking area 
Staff distracted during treatment 
Staff 
Equipment 
Clothing 
Medication
Environement 
Falls of Patients in Hospital 
Staff unaware of produre 
Staff unaware of disability 
Cannot hold patient 
Not cooperating with staff 
In bad mental state 
Neorological disorder 
Sensory depravation 
Lighting 
Obstacles 
Shoes wrong size 
Walking on socks 
Usafe shoes 
Patient is sleepy 
Medication effect Wetfloor 
Unfit clothes 
Open shoes 
Patient is dizzy 
Drop of blood pressure 
Bell not working 
Damaged wheelchair 
Unfit wheelchair 
Furniture not suitable 
Unsuitable walking aides 
Falls from lift 
Wrong use of equipment 
Damaged walking aides 
Fig. 2.17 A cause and effect diagram of patient falls during hospitalization 
or make forecasts. For example, one might want to verify the theory that the 
relative number of errors found in engineering drawings declines with increasing 
drawing sizes. 
Cause and Effect Diagrams 
Cause and effect diagrams (also called ﬁshbone 
charts or Ishikawa diagrams) are used to identify, explore, and display all 
the possible causes of a problem or event. The diagram is usually completed 
in a meeting of individuals who have ﬁrsthand knowledge of the problem 
investigated. Figure 2.17 shows a cause and effect diagram listing causes for 
falls of hospitalized patients. A typical group to convene for completing such 
a diagram consists of nurses, physicians, administrative staff, housekeeping 
personnel, and physiotherapists. It is standard practice to weight the causes 
by impact on the problem investigated and then initiate projects to reduce the 
harmful effects of the main causes. Cause and effect diagrams can be derived 
after data were collected and presented, for example, using a Pareto chart or be 
entirely based on collective experience without supporting data. 
The successful efforts of previous improvement are data-driven. In attempting 
to reduce levels of defects, or output variability, a team will typically begin by 
collecting data and charting the process. Flow charts and check sheets are used in 
these early stages. Run charts, histograms, and Pareto charts can then be prepared 
from the data collected on check sheets or otherwise. Diagnosing the current process 
is carried out, using in addition scatterplots and cause and effect diagrams. Once 
solutions for improvement are implemented, their impact can be assessed using run 
charts, histograms, and Pareto charts. A statistical test for comparing Pareto charts 
is presented next. 
2.6 
Statistical Analysis of Pareto Charts 
Pareto charts are often compared over time or across processes. In such compar-
isons, one needs to know whether differences between two Pareto charts should 
be attributed to random variation or to special signiﬁcant causes. In this section, 
we present a statistical test that is used to ﬂag statistically signiﬁcant differences 
between two Pareto charts (Kenett 1991). Once the classiﬁcation of observations

36
2
Basic Tools and Principles of Process Control
into different categories is completed, we have the actual number of observations, 
per category. The reference Pareto chart is a Pareto chart constructed in an earlier 
time period or on a different, but comparable, process. Other terms for the reference 
Pareto chart are the benchmark or standard Pareto chart. The proportion of obser-
vations in each category of the reference Pareto chart is the expected proportion. 
We expect to ﬁnd these proportions in Pareto charts of data collected under the 
conditions of the reference Pareto chart. The expected number of observations in 
the different categories of the Pareto chart is computed by multiplying the total 
number of observations in a Pareto chart by the corresponding expected proportion. 
The standardized residuals are assessing the signiﬁcance of the deviations between 
the new Pareto chart and the reference Pareto chart. The statistical test relies on 
computation of standardized residuals: 
upper Z Subscript i Baseline equals StartFraction n Subscript i Baseline minus upper N p Subscript i Baseline Over left bracket upper N p Subscript i Baseline left parenthesis 1 minus p Subscript i Baseline right parenthesis right bracket Superscript 1 divided by 2 Baseline EndFraction comma i equals 1 comma midline horizontal ellipsis comma upper K commaZi =
ni −Npi
[Npi(1 −pi)]1/2 ,
i = 1, · · · , K,
(2.6.1) 
where: 
upper N equalsN = the total number of observations in Pareto chart 
p Subscript i Baseline equalspi = the proportion of observations in category i, in reference Pareto chart 
upper N p Subscript i Baseline equalsNpi = the expected number of observations in category i, given a total of N 
observations 
n Subscript i Baseline equalsni = the actual number of observations in category i 
In performing the statistical test, one assumes that observations are independently 
classiﬁed into distinct categories. The actual classiﬁcation into categories might 
depend on the data gathering protocol. Typically, the classiﬁcation relies on the 
ﬁrst error cause encountered. A different test procedure could therefore produce 
different data. The statistical test presented here is more powerful than the standard 
chi-squared test. It will therefore recognize differences between a reference Pareto 
chart and a current Pareto chart that will not be determined as signiﬁcant by the 
chi-squared test. 
In order to perform the statistical analysis, we ﬁrst list the error categories in 
a ﬁxed order. A natural order to use is the alphabetic order of the categories’ 
names. This organization of the data is necessary in order to permit meaningful 
comparisons. The test itself consists of seven steps. The last step is being an 
interpretation of the results. To demonstrate these steps, we use data on timecard 
errors presented in Table 2.3.
Example 2.2 The data come from a monitoring system of timecard entries in a 
medium size company with 15 departments. During a management meeting, the 
human resources manager of the company was asked to initiate an improvement 
project aimed at reducing timecard errors. The manager asked to see a reference 
Pareto chart of last months’ timecard errors by department. Departments # 6, 
7, 8, and 12 were responsible for 46% of the timecard errors. The manager 
appointed a special improvement team to learn the causes for these errors. The team

2.6
Statistical Analysis of Pareto Charts
37
Table 2.3 Timecard errors 
data in 15 departments 
Department # 
Reference Pareto 
Current Pareto 
1
23
14 
2
42
7 
3
37
85 
4
36
19 
5
17
23 
6
50
13 
7
60
48 
8
74
59 
9
30
2 
10
25
0 
11
10
12 
12
54
14 
13
23
30 
14
24
20 
15
11
0
recommended to change the input format of the time card. The new format was 
implemented throughout the company. Three weeks later, a new Pareto chart was 
prepared from 346 newly reported timecard errors. A statistical analysis of the new 
Pareto chart was performed in order to determine what department had a signiﬁcant 
change in its relative contribution of timecard errors. 
The steps in applying the statistical test are: 
1. Compute for each department its proportion of observations in the reference 
Pareto chart: 
StartLayout 1st Row StartLayout 1st Row 1st Column p 1 2nd Column equals 23 divided by 516 equals 0.04457 2nd Row 1st Column Blank 2nd Column vertical ellipsis 3rd Row 1st Column p 15 2nd Column equals 11 divided by 516 equals 0.0213 period EndLayout EndLayout
p1 = 23/516 = 0.04457
...
p15 = 11/516 = 0.0213.
2. Compute the total number of observations in the new Pareto chart: 
upper N equals 14 plus 7 plus 85 plus midline horizontal ellipsis plus 20 equals 346 periodN = 14 + 7 + 85 + · · · + 20 = 346.
3. Compute the expected number of observations in department # i, upper E Subscript i Baseline equals upper N p Subscript iEi = Npi, 
i equals 1 comma midline horizontal ellipsis comma 15i = 1, · · · , 15. 
StartLayout 1st Row StartLayout 1st Row 1st Column upper E 1 2nd Column equals 346 times 0.04457 equals 15.42 2nd Row 1st Column Blank 2nd Column vertical ellipsis 3rd Row 1st Column upper E 15 2nd Column equals 346 times 0.0213 equals 7.38 period EndLayout EndLayout
E1 = 346 × 0.04457 = 15.42
...
E15 = 346 × 0.0213 = 7.38.

38
2
Basic Tools and Principles of Process Control
Table 2.4 Critical values for 
standardized residuals 
Signiﬁcance level 
K 
10% 
5% 
1% 
4 
1.95 
2.24 
2.81 
5 
2.05 
2.32 
2.88 
6 
2.12 
2.39 
2.93 
7 
2.18 
2.44 
2.99 
8 
2.23 
2.49 
3.04 
9 
2.28 
2.53 
3.07 
10 
2.32 
2.57 
3.10 
20 
2.67 
2.81 
3.30 
30 
2.71 
2.94 
3.46 
Table 2.5 Table of 
standardized residuals for the 
timecards error 
Department # 
Pareto 
Ei 
Zi 
1
14
15 
−0.26 
2
7
28 
−4.13 
* 
3
85
25 
12.50 
* 
4
19
24 
−1.06 
5
23
11
3.61 
* 
6
13
34 
−3.82 
* 
7
48
40
1.34 
8
59
50
1.38 
9
2
20 
−4.14 
* 
10
0
17 
−4.26 
* 
11
12
7
1.95 
12
14
36 
−3.86 
* 
13
30
15
3.91 
* 
14
20
16
1.02 
15
0
7 
−2.61
4. Compute the standardized residuals: upper Z Subscript i Baseline equals left parenthesis upper N Subscript i Baseline minus upper N p Subscript i Baseline right parenthesis divided by left parenthesis upper N p Subscript i Baseline left parenthesis 1 minus p Subscript i Baseline right parenthesis right parenthesis Superscript 1 divided by 2Zi = (Ni −Npi)/(Npi(1 −pi))1/2, i equals 1 comma ellipsis comma 15i =
1, . . . , 15. 
StartLayout 1st Row StartLayout 1st Row 1st Column upper Z 1 2nd Column equals left parenthesis 14 minus 15.42 right parenthesis divided by left bracket 15.42 left parenthesis 1 minus 0.04457 right parenthesis right bracket Superscript 1 divided by 2 Baseline equals negative 0.37 2nd Row 1st Column Blank 2nd Column vertical ellipsis 3rd Row 1st Column upper Z 15 2nd Column equals left parenthesis 0 minus 7.38 right parenthesis divided by left bracket 7.38 left parenthesis 1 minus 0.0213 right parenthesis right bracket Superscript 1 divided by 2 Baseline equals negative 2.75 period EndLayout EndLayout
Z1 = (14 −15.42)/[15.42(1 −0.04457)]1/2 = −0.37
...
Z15 = (0 −7.38)/[7.38(1 −0.0213)]1/2 = −2.75.
5. Look up Table 2.4 for upper K equals 15K = 15. Interpolate between upper K equals 10K = 10 and upper K equals 20K = 20. For  
alpha equals 0.01α = 0.01 signiﬁcance level, the critical value is approximately left parenthesis 3.10 plus 3.30 right parenthesis divided by 2 equals 3.20(3.10+3.30)/2 =
3.20. 
6. Identify categories with standardized residuals larger, in absolute value, than 
3.20. Table 2.5 indicates with a star the departments where the proportion of 
errors was signiﬁcantly different from that in the reference Pareto. 

2.7
The Shewhart Control Charts
39
7. Departments # 2, 3, 5, 6, 9, 10, 12, and 13 are ﬂagged with a asterisk∗that indicates 
signiﬁcant changes between the new Pareto data from the reference Pareto chart. 
In category 3, we expected 25 occurrences, a much smaller number than the 
actual 85. 
black medium square■
The statistical test enables us to systematically compare two Pareto charts with the 
same categories. Focusing on the differences between Pareto charts complements 
the analysis of trends and changes in overall process error levels. Increases or 
decreases in such error levels may result from changes across all error categories. 
On the other hand, there may be no changes in error levels but signiﬁcant changes 
in the mix of errors across categories. The statistical analysis reveals such changes. 
Another advantage of the statistical procedure is that it can apply to different time 
frames. For example, the reference Pareto can cover a period of one year, and the 
current Pareto can span a period of three weeks. 
The critical values are computed on the basis of the Bonferroni Inequality 
approximation. This inequality states that, since we are examining simultaneously 
K standardized residuals, the overall signiﬁcance level is not more than K times 
the signiﬁcance level of an individual comparison. Dividing the overall signiﬁcance 
level of choice by K, and using the normal approximation, produces the critical 
values in Table 2.4. For more details on this procedure, see Kenett (1991). 
2.7 
The Shewhart Control Charts 
The Shewhart control charts is a detection procedure in which every h units of 
time a sample of size n is drawn from the process. Let thetaθ denote a parameter of the 
distribution of the observed random sample x 1 comma midline horizontal ellipsis comma x Subscript n Baselinex1, · · · , xn. Let ModifyingAbove theta With caret Subscript nˆθn denote an appropriate 
estimate of thetaθ. If theta 0θ0 is a desired operation level for the process, we construct around 
theta 0θ0 two limits UCL and LCL. As long as LCL less than or equals ModifyingAbove theta With caret Subscript n Baseline less than or equals≤ˆθn ≤UCL, we say that the process 
is under statistical control. 
More speciﬁcally, suppose that x 1 comma x 2 comma midline horizontal ellipsisx1, x2, · · · are normally distributed and indepen-
dent. Every h hours (time units) a sample of n observations is taken. 
Suppose that when the process is under control x Subscript i Baseline tilde upper N left parenthesis theta 0 comma sigma squared right parenthesisxi ∼N(θ0, σ 2) and that sigma squaredσ 2 is 
known. We set ModifyingAbove theta With caret Subscript n Baseline identical to x overbar Subscript n Baseline equals StartFraction 1 Over n EndFraction sigma summation Underscript j equals 1 Overscript n Endscripts x Subscript j ˆθn ≡¯xn = 1
n
n
j=1 xj. The control limits are 
StartLayout 1st Row 1st Column UCL 2nd Column equals theta 0 plus 3 StartFraction sigma Over StartRoot n EndRoot EndFraction 2nd Row 1st Column LCL 2nd Column equals theta 0 minus 3 StartFraction sigma Over StartRoot n EndRoot EndFraction period EndLayout
UCL = θ0 + 3 σ
√n
LCL = θ0 −3 σ
√n.
(2.7.1) 
The warning limits are set at

40
2
Basic Tools and Principles of Process Control
StartLayout 1st Row 1st Column UWL 2nd Column equals theta 0 plus 2 StartFraction sigma Over StartRoot n EndRoot EndFraction 2nd Row 1st Column LWL 2nd Column equals theta 0 minus 2 StartFraction sigma Over StartRoot n EndRoot EndFraction period EndLayout
UWL = θ0 + 2 σ
√n
LWL = θ0 −2 σ
√n.
(2.7.2) 
Notice that: 
(i) The samples are independent. 
(ii) All x overbar Subscript n¯xn are distributed as upper N left parenthesis theta 0 comma StartFraction sigma squared Over n EndFraction right parenthesisN

θ0, σ 2
n

as long as the process is under control. 
(iii) If alphaα is the probability of observing x overbar Subscript n¯xn outside the control limits, when theta equals theta 0θ = θ0, 
then alpha equals 0.0027α = 0.0027. We expect one every upper N equals 370N = 370 samples to yield a value of 
x overbar Subscript n¯xn outside the control limits. 
(iv) We expect about 5% of the x overbar Subscript n¯xn points to lie outside the warning limits, when 
the process is under control. Thus, testing the null hypothesis upper H 0 colon theta equals theta 0H0 : θ = θ0
against upper H 1 colon theta not equals theta 0H1 : θ ̸= θ0, we may choose a level of signiﬁcance alpha equals 0.05α = 0.05 and use 
the limits UWL, LWL as rejection limits. In the control case, the situation is 
equivalent to that of simultaneously (or repeatedly) testing many hypotheses. 
For this reason, we consider a much smaller alphaα, like alpha equals 0.0027α = 0.0027, derived from 
the use of the 3-sigma limits. 
(v) In many applications of the Shewhart 3-sigma control charts, the samples taken 
are of small size, n equals 4n = 4 or n equals 5n = 5, and the frequency of samples is high 
(h small). Shewhart recommended such small samples in order to reduce the 
possibility that a shift in thetaθ will happen during sampling. On the other hand, 
if the samples are picked very frequently, there is a higher chance to detect 
a shift early. The question of how frequently to sample and what should be 
the sample size is related to the idea of rational subgroups discussed earlier 
in Sect. 2.3. An economic approach to the determination of rational subgroups 
will be presented in Sect. 3.3.1. 
Figure 2.18 shows a Shewhart control chart of a process where all samples are 
falling inside the control limits. We provide now formulae for the control limits of 
various Shewhart type control charts.
2.7.1 
Control Charts for Attributes 
We consider here control charts when the control statistic is the sample fraction 
defectives ModifyingAbove p With caret Subscript i Baseline equals StartFraction x Subscript i Baseline Over n Subscript i Baseline EndFraction ˆpi = xi
ni i equals 1 comma midline horizontal ellipsis comma upper Ni = 1, · · · , N. Here n Subscript ini is the size of the ith sample, and x Subscript ixi is the 
number of defective items in the ith sample. 
Given N samples, we estimate the common parameter thetaθ by ModifyingAbove theta With caret equals StartFraction sigma summation Underscript i equals 1 Overscript upper N Endscripts x Subscript i Baseline Over sigma summation Underscript i equals 1 Overscript upper N Endscripts n Subscript i Baseline EndFraction ˆθ =
N
i=1 xi
N
i=1 ni . The  
upper and lower control limits are

2.7
The Shewhart Control Charts
41
Fig. 2.18 Shewhart control chart
StartLayout 1st Row 1st Column LCL 2nd Column equals ModifyingAbove theta With caret minus 3 StartRoot StartFraction ModifyingAbove theta With caret left parenthesis 1 minus ModifyingAbove theta With caret right parenthesis Over n EndFraction EndRoot 2nd Row 1st Column UCL 2nd Column equals ModifyingAbove theta With caret plus 3 StartRoot StartFraction ModifyingAbove theta With caret left parenthesis 1 minus ModifyingAbove theta With caret right parenthesis Over n EndFraction EndRoot period EndLayout
LCL = ˆθ −3

ˆθ(1 −ˆθ)
n
UCL = ˆθ + 3

ˆθ(1 −ˆθ)
n
.
(2.7.3) 
In Table 2.6, we present, for example, the number of defective items found in 
random samples of size n equals 100n = 100, drawn daily from a production line.
In Fig. 2.19, we present the control chart for the data of Table 2.6. We see that 
there is indication that the fraction defectives in two days were signiﬁcantly high, 
but the process on the whole remained under control during the month. Deleting 
these two days, we can revise the control chart, by computing a modiﬁed estimate 
of thetaθ. We obtain a new value of ModifyingAbove theta With caret equals 139 divided by 2900 equals 0.048 ˆθ = 139/2900 = 0.048. This new estimator yields a 
revised upper control limit
UCL prime equals 0.112 periodUCL′ = 0.112.

42
2
Basic Tools and Principles of Process Control
Table 2.6 The number of 
defects in daily samples 
(sample size is n equals 100n = 100) 
Sample/day 
# of Defects 
Sample/day 
# of Defects 
i
x Subscript ixi
i
x Subscript ixi
1
6
16
6 
2
8
17
4 
3
8
18
6 
4
13
19
8 
5
6
20
2 
6
6
21
7 
7
9
22
4 
8
7
23
4 
9
1
24
2 
10
8
25
1 
11
5
26
5 
12
2
27
15 
13
4
28
1 
14
5
29
4 
15
4
30
1 
31
5
Fig. 2.19 p-chart for January data

2.7
The Shewhart Control Charts
43
2.7.2 
Control Charts for Variables 
2.7.2.1 
¯X-Charts 
After the process has been observed for k sampling periods, we can compute 
estimates of the process mean and standard deviation. The estimate of the process 
mean is 
upper X overbar overbar equals StartFraction 1 Over k EndFraction sigma summation Underscript i equals 1 Overscript k Endscripts upper X overbar Subscript i Baseline period¯¯X = 1
k
k

i=1
¯Xi.
This will be the center line for the control chart. The process standard deviation can 
be estimated using either the average sample standard deviation 
ModifyingAbove sigma With caret equals upper S overbar divided by c left parenthesis n right parenthesis commaˆσ = ¯S/c(n),
(2.7.4) 
where 
upper S overbar equals StartFraction 1 Over k EndFraction sigma summation Underscript i equals 1 Overscript k Endscripts upper S Subscript i Baseline comma¯S = 1
k
k

i=1
Si,
or the average sample range 
ModifyingAbove Above ModifyingAbove sigma With caret With caret equals upper R overbar divided by d left parenthesis n right parenthesis commaˆˆσ = ¯R/d(n),
(2.7.5) 
where 
upper R overbar equals StartFraction 1 Over k EndFraction sigma summation Underscript i equals 1 Overscript k Endscripts upper R Subscript i Baseline period¯R = 1
k
k

i=1
Ri.
The factors c left parenthesis n right parenthesisc(n) and d left parenthesis n right parenthesisd(n) guarantee that we obtain unbiased estimates of sigmaσ. We can 
show, for example, that upper E left parenthesis upper S overbar right parenthesis equals sigma c left parenthesis n right parenthesisE( ¯S) = σc(n), where 
c left parenthesis n right parenthesis equals left bracket upper Gamma left parenthesis n divided by 2 right parenthesis divided by upper Gamma left parenthesis StartFraction n minus 1 Over 2 EndFraction right parenthesis right bracket StartRoot 2 divided by left parenthesis n minus 1 right parenthesis EndRoot periodc(n) =

(n/2)/
n −1
2
 
2/(n −1).
(2.7.6) 
Moreover, 
upper E left brace upper R Subscript n Baseline right brace equals sigma d left parenthesis n right parenthesisE{Rn}
=
σd(n), where from the theory of order statistics (see 
Section 2.7 in Modern Statistics, Kenett et al. 2022b), we obtain that 
d left parenthesis n right parenthesis equals StartFraction n left parenthesis n minus 1 right parenthesis Over 2 pi EndFraction integral Subscript 0 Superscript normal infinity Baseline y integral Subscript negative normal infinity Superscript normal infinity Baseline exp left brace minus StartFraction x squared plus left parenthesis y plus x right parenthesis squared Over 2 EndFraction right brace left bracket upper Phi left parenthesis x plus y right parenthesis minus upper Phi left parenthesis x right parenthesis right bracket Superscript n minus 2 Baseline d x d y periodd(n) = n(n −1)
2π
 ∞
0
y
 ∞
−∞
exp

−x2 + (y + x)2
2

[
(x+y)−
(x)]n−2 dx dy.
(2.7.7)

44
2
Basic Tools and Principles of Process Control
Table 2.7 Factors c left parenthesis n right parenthesisc(n) and 
d left parenthesis n right parenthesisd(n) for estimating sigmaσ
n
c left parenthesis n right parenthesisc(n)
d left parenthesis n right parenthesisd(n)
2 
0.7979 
1.2838 
3 
0.8862 
1.6926 
4 
0.9213 
2.0587 
5 
0.9400 
2.3259 
6 
0.9515 
2.5343 
7 
0.9594 
2.7044 
8 
0.9650 
2.8471 
9 
0.9693 
2.9699 
10 
0.9727 
3.0774 
In Table 2.7, we present the factors c left parenthesis n right parenthesisc(n) and d left parenthesis n right parenthesisd(n) for n equals 2 comma 3 comma midline horizontal ellipsis comma 10n = 2, 3, · · · , 10. 
The control limits are now computed as 
UCL equals upper X overbar overbar plus 3 ModifyingAbove sigma With caret divided by StartRoot n EndRootUCL = ¯¯X + 3ˆσ/√n
(2.7.8) 
and 
LCL equals upper X overbar overbar minus 3 ModifyingAbove sigma With caret divided by StartRoot n EndRoot periodLCL = ¯¯X −3ˆσ/√n.
Despite the wide use of the sample ranges for estimating the process standard 
deviation, this method is neither very efﬁcient nor robust. It is popular only 
because the sample range is easier to compute than the sample standard deviation. 
However, since many hand calculators now have built-in programs for computing 
the sample standard deviation, the computational advantage of the range should not 
be considered. In any case, the sample ranges should not be used when the sample 
size is greater than 10. 
Example 2.3 We illustrate the construction of an upper X overbar ¯X-chart for the data in Table 2.8. 
These measurements represent the length (in cm) of the electrical contacts of 
relays in samples of size ﬁve, taken hourly from the running process. Both the 
sample standard deviation and the sample range are computed for each sample, for 
the purposes of illustration. The center line for the control chart is upper X overbar overbar equals 2.005 ¯¯X = 2.005. From  
Table 2.7, we ﬁnd for n equals 5n = 5, c left parenthesis 5 right parenthesis equals 0.9400c(5) = 0.9400. Let
upper A 1 equals 3 divided by left parenthesis c left parenthesis 5 right parenthesis StartRoot n EndRoot right parenthesis equals 1.427 periodA1 = 3/(c(5)√n) = 1.427.
The control limits are given by 
UCL equals upper X overbar overbar plus upper A 1 upper S overbar equals 2.186UCL = ¯¯X + A1 ¯S = 2.186

2.7
The Shewhart Control Charts
45
Table 2.8 20 samples of 5 electric contact lengths 
Hour 
i
x 1x1
x 2x2
x 3x3
x 4x4
x 5x5
upper X overbar ¯X
S
R 
1
1.9890 
2.1080 
2.0590 
2.0110 
2.0070
2.0348 
0.04843 
0.11900 
2
1.8410 
1.8900 
2.0590 
1.9160 
1.9800
1.9372 
0.08456 
0.21800 
3
2.0070 
2.0970 
2.0440 
2.0810 
2.0510
2.0560 
0.03491 
0.09000 
4
2.0940 
2.2690 
2.0910 
2.0970 
1.9670
2.1036 
0.10760 
0.30200 
5
1.9970 
1.8140 
1.9780 
1.9960 
1.9830
1.9536 
0.07847 
0.18300 
6
2.0540 
1.9700 
2.1780 
2.1010 
1.9150
2.0436 
0.10419 
0.26300 
7
2.0920 
2.0300 
1.8560 
1.9060 
1.9750
1.9718 
0.09432 
0.23600 
8
2.0330 
1.8500 
2.1680 
2.0850 
2.0230
2.0318 
0.11674 
0.31800 
9
2.0960 
2.0960 
1.8840 
1.7800 
2.0050
1.9722 
0.13825 
0.31600 
10 
2.0510 
2.0380 
1.7390 
1.9530 
1.9170
1.9396 
0.12552 
0.31200 
11 
1.9520 
1.7930 
1.8780 
2.2310 
1.9850
1.9678 
0.16465 
0.43800 
12 
2.0060 
2.1410 
1.9000 
1.9430 
1.8410
1.9662 
0.11482 
0.30000 
13 
2.1480 
2.0130 
2.0660 
2.0050 
2.0100
2.0484 
0.06091 
0.14300 
14 
1.8910 
2.0890 
2.0920 
2.0230 
1.9750
2.0140 
0.08432 
0.20100 
15 
2.0930 
1.9230 
1.9750 
2.0140 
2.0020
2.0014 
0.06203 
0.17000 
16 
2.2300 
2.0580 
2.0660 
2.1990 
2.1720
2.1450 
0.07855 
0.17200 
17 
1.8620 
2.1710 
1.9210 
1.9800 
1.7900
1.9448 
0.14473 
0.38100 
18 
2.0560 
2.1250 
1.9210 
1.9200 
1.9340
1.9912 
0.09404 
0.20500 
19 
1.8980 
2.0000 
2.0890 
1.9020 
2.0820
1.9942 
0.09285 
0.19100 
20 
2.0490 
1.8790 
2.0540 
1.9260 
2.0080
1.9832 
0.07760 
0.17500 
Average 
2.0050 
0.09537 
0.23665 
upper X overbar overbar ¯¯X
upper S overbar ¯S
upper R overbar ¯R
and 
LCL equals upper X overbar overbar minus upper A 1 upper S overbar equals 1.824 periodLCL = ¯¯X −A1 ¯S = 1.824.
The resulting control chart is shown in Fig. 2.20. If we use the  sample ranges to 
determine the control limits, we ﬁrst ﬁnd that d left parenthesis 5 right parenthesis equals 2.326d(5) = 2.326 and
upper A 2 equals 3 divided by left parenthesis d left parenthesis 5 right parenthesis StartRoot n EndRoot right parenthesis equals 0.577 periodA2 = 3/(d(5)√n) = 0.577.
This gives us control limits of 
StartLayout 1st Row 1st Column UCL prime 2nd Column equals upper X overbar plus upper A 2 upper R overbar equals 2.142 2nd Row 1st Column LCL prime 2nd Column equals upper X overbar minus upper A 2 upper R overbar equals 1.868 period EndLayout
UCL′ = ¯X + A2 ¯R = 2.142
LCL′ = ¯X −A2 ¯R = 1.868.
black medium square■

46
2
Basic Tools and Principles of Process Control
Fig. 2.20 
upper X overbar ¯X-control chart for contact data
2.7.2.2 
S-Charts and R-Charts 
As discussed earlier, control of the process variability can be as important as control 
of the process mean. Two types of control charts are commonly used for this 
purpose: an R-chart, based on sample ranges, and an S-chart, based on sample 
standard deviations. Since ranges are easier to compute than standard deviations, 
R-charts are probably more common in practice. The R-chart is not very efﬁcient. 
In fact, its efﬁciency declines rapidly as the sample size increases, and the sample 
range should not be used for a sample size greater than 5. However, we shall discuss 
both types of charts. 
To construct control limits for the S-chart, we will use a normal approximation 
to the sampling distribution of the sample standard deviation, S. This means that we 
will use control limits 
LCL equals upper S overbar minus 3 ModifyingAbove sigma With caret Subscript s BaselineLCL = ¯S −3ˆσs
(2.7.9) 
and 
UCL equals upper S overbar plus 3 ModifyingAbove sigma With caret Subscript s Baseline commaUCL = ¯S + 3ˆσs,

2.7
The Shewhart Control Charts
47
where ModifyingAbove sigma With caret Subscript sˆσs represents an estimate of the standard deviation of S. This standard 
deviation is 
sigma Subscript s Baseline equals sigma divided by StartRoot 2 left parenthesis n minus 1 right parenthesis EndRoot periodσs = σ/

2(n −1).
(2.7.10) 
Using the unbiased estimate ModifyingAbove sigma With caret equals upper S overbar divided by c left parenthesis n right parenthesisˆσ = ¯S/c(n), we obtain 
ModifyingAbove sigma With caret Subscript s Baseline equals upper S overbar divided by left parenthesis c left parenthesis n right parenthesis StartRoot 2 left parenthesis n minus 1 right parenthesis EndRoot right parenthesis commaˆσs = ¯S/(c(n)

2(n −1)),
(2.7.11) 
and hence, the control limits 
LCL equals upper S overbar minus 3 upper S overbar divided by left parenthesis c left parenthesis n right parenthesis StartRoot 2 left parenthesis n minus 1 right parenthesis EndRoot right parenthesis equals upper B 3 upper S overbarLCL = ¯S −3 ¯S/(c(n)

2(n −1)) = B3 ¯S
and 
UCL equals upper S overbar plus 3 upper S overbar divided by left parenthesis c left parenthesis n right parenthesis StartRoot 2 left parenthesis n minus 1 right parenthesis EndRoot right parenthesis equals upper B 4 upper S overbar periodUCL = ¯S + 3 ¯S/(c(n)

2(n −1)) = B4 ¯S.
(2.7.12) 
The factors upper B 3B3 and upper B 4B4 can be determined from Table 2.7. 
Example 2.4 Using the electrical contact data in Table 2.8, we ﬁnd 
centerline equals upper S overbar equals 0.095 commacenterline = ¯S = 0.095,
LCL equals upper B 3 upper S overbar equals 0 commaLCL = B3 ¯S = 0,
and 
UCL equals upper B 4 upper S overbar equals 2.089 left parenthesis 0.095 right parenthesis equals 0.199 periodUCL = B4 ¯S = 2.089(0.095) = 0.199.
The S-chart is given in Fig. 2.21. 
An R-chart is constructed using similar techniques, with a center line equals upper R overbar= ¯R, and 
control limits:
LCL equals upper D 3 upper R overbar commaLCL = D3 ¯R,
and 
UCL equals upper D 4 upper R overbar commaUCL = D4 ¯R,
(2.7.13) 
where 
StartLayout 1st Row upper D 3 equals left parenthesis 1 minus StartFraction 3 Over d left parenthesis n right parenthesis StartRoot 2 left parenthesis n minus 1 right parenthesis EndRoot EndFraction right parenthesis Superscript plus Baseline and upper D 4 equals left parenthesis 1 plus StartFraction 3 Over d left parenthesis n right parenthesis StartRoot 2 left parenthesis n minus 1 right parenthesis EndRoot EndFraction right parenthesis period EndLayoutD3 =

1 −
3
d(n)√2(n −1)
+
and
D4 =

1 +
3
d(n)√2(n −1)

.
(2.7.14)

48
2
Basic Tools and Principles of Process Control
Fig. 2.21 S-chart for contact data
Using the data of Table 2.8, we ﬁnd 
centerline equals upper R overbar equals 0.237 commacenterline = ¯R = 0.237,
LCL equals upper D 3 upper R overbar equals 0 commaLCL = D3 ¯R = 0,
and 
UCL equals upper D 4 upper R overbar equals left parenthesis 2.114 right parenthesis left parenthesis 0.237 right parenthesis equals 0.501 periodUCL = D4 ¯R = (2.114)(0.237) = 0.501.
The R-chart is shown in Fig. 2.22.
black medium square■
The decision of whether to use an R-chart or S-chart to control variability 
ultimately depends on which method works best in a given situation. Both methods 
are based on several approximations. There is, however, one additional point that 
should be considered. The average value of the range of n variables depends to 
a great extent on the sample size n. As  n increases, the range increases. The R-
chart based on 5 observations per sample will look quite different from an R-chart 
based on 10 observations. For this reason, it is difﬁcult to visualize the variability 
characteristics of the process directly from the data. On the other hand, the sample 
standard deviation, S, used in the  S-chart is a good estimate of the process standard 
deviation sigmaσ. As the sample size increases, S will tend to be even closer to the true

2.8
Process Analysis with Data Segments
49
Fig. 2.22 R-chart for contact data
value of sigmaσ. The process standard deviation is the key to understanding the variability 
of the process. An alternative approach, originally proposed by Fuchs and Kenett 
(1987), is to determine control limits on the basis of tolerance intervals derived 
either analytically (Section 3.5, Modern Statistics, Kenett et al. 2022b) or through 
nonparametric bootstrapping (Section 3.10, Modern Statistics, Kenett et al. 2022b). 
We discuss this approach, in the multivariate case, in Sect. 4.5. 
2.8 
Process Analysis with Data Segments 
In previous sections, we introduced the basic elements of statistical process 
control. Section 2.2 is about using a control chart for running a process under 
statistical control, and Sect. 2.3 is about process capability analysis that is conducted 
retrospectively on data to characterize a process under control and set up the control 
limits of a control chart. In this section, we expand on the process capability analysis 
by considering time-related patterns. Speciﬁcally, we focus on data segments that 
can show up in the data collected during process capability analysis. If such 
segments are detected, and investigations reveal the causes for such segments, the 
analyst might decide to focus on a data segment representing a steady state stable 
process, as opposed to the full data collected for process capability analysis. In this

50
2
Basic Tools and Principles of Process Control
Fig. 2.23 Run charts of sensors  X and Z over time  
context, one will aim at identifying transient periods and not include them in the 
computation of control limits. 
We provide here two methods for identifying such segments. One method is 
based on decision trees introduced in Section 7.6 of Modern Statistics (Kenett 
et al. 2022b). The second method is applying functional data analysis introduced 
in Section 8.1 of Modern Statistics. Additional options include ﬁtting step functions 
to time series data using least squares, see Chapter 6 in Modern Statistics. 
2.8.1 
Data Segments Based on Decision Trees 
In Fig. 2.23, we show run charts of sensors X and Z over time. The sensors are 
tracking a tank ﬁlling process with an impeller operating at variable speed. The 
process is operating at various time segments. To identify the time segments, 
we apply a decision tree with 4 splits with the time index as covariate. For an 
introduction to decision trees, see Section 7.6 of Modern Statistics (Kenett et al. 
2022b). The decision tree identiﬁes the time instances for the various optimal splits 
corresponding to a maximized difference in average level and minimum variance. 
We want to identify the starting and ending points of the central data segment. 
data = mistat.load_data('PROCESS_SEGMENT') 
# Load and prepare data for analysis 
def sensorData(data, label): 
series = data[label] 
return pd.DataFrame({ 
'Time': np.arange(len(series)), 
'values': series, 
}) 
sensorX = sensorData(data, 'X') 
sensorZ = sensorData(data, 'Z') 
# Build decision trees for segment analysis 
def addSegmentAnalysis(sensor, max_leaf_nodes): 
model = tree.DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes) 
model.fit(sensor[['Time']], sensor['values'])

2.8
Process Analysis with Data Segments
51
Range
Count
Mean
SEM
[0.025
0.975] 
Level 1
upper T i m e less than 72Time < 72
72
1125.0
33.2
1059.9
1190.0 
Level 2
72 less than or equals upper T i m e less than 89972 ≤Time < 899
827
4566.8
9.8
4547.6
4586.0 
Level 3
899 less than or equals upper T i m e less than 1443899 ≤Time < 1443
544
5027.0
12.1
5003.3
5050.6 
Level 4
1443 less than or equals upper T i m e less than 14671443 ≤Time < 1467
24
3147.5
57.4
3034.8
3260.2 
Level 5
1467 less than or equals upper T i m e1467 ≤Time
430
170.0
13.6
143.4
196.6 
sensor['predicted'] = model.predict(sensor[['Time']]) 
return sensor, model 
sensorX, modelX = addSegmentAnalysis(sensorX, 5) 
sensorZ, modelZ = addSegmentAnalysis(sensorZ, 5) 
# Plot data 
def plotSegmentAnalysis(sensor, label, ax): 
sensor.plot.scatter(x='Time', y='values', ax=ax, color='grey') 
ax.plot(sensor['Time'], sensor['predicted'], color='black') 
ax.set_xlabel('Time') 
ax.set_ylabel(label) 
data = mistat.load_data('PROCESS_SEGMENT') 
fig, axes = plt.subplots(ncols=2, figsize=(10, 4)) 
plotSegmentAnalysis(sensorX, 'X', axes[0]) 
plotSegmentAnalysis(sensorZ, 'Z', axes[1]) 
plt.tight_layout() 
Figure 2.24 presents the decision trees for X and Z. The ﬁrst split for the X series 
is at t equals 72t = 72. The ﬁrst split for the Z series is at t equals 17t = 17. 
The identiﬁed levels for sensor X are: 
and the levels identiﬁed for sensor Z are:
Range
Count
Mean
SEM
[0.025
0.975] 
Level 1
upper T i m e less than 17Time < 17
17
527.8
28.5
471.9
583.6 
Level 2
17 less than or equals upper T i m e less than 7117 ≤Time < 71
54
1641.5
16.0
1610.1
1672.8 
Level 3
71 less than or equals upper T i m e less than 10271 ≤Time < 102
31
3715.7
21.1
3674.3
3757.1 
Level 4
102 less than or equals upper T i m e less than 891102 ≤Time < 891
789
4784.5
4.2
4776.3
4792.7 
Level 5
891 less than or equals upper T i m e891 ≤Time
111
5163.8
11.1
5141.9
5185.6 
In considering the variability of sensors X and Z, we can distinguish transient 
states, at the beginning and end of the series, from steady states, at the central part 
of the series. We can then set up control limits based on mean levels and variability 
in steady states. For sensor X, the steady state is achieved at Level 2 (72 less than or equals upper T i m e less than 89972 ≤Time <
899). For sensor Z, it corresponds to Level 4 (102 less than or equals upper T i m e less than 891102 ≤Time < 891). 
As shown, the decision tree provides an easy and interpretable approach to data 
segmentation. One should note, however, that the splits in a decision tree are done 
using a greedy algorithm that considers one split at a time, without accounting for 
follow-up steps. In that sense, decision trees reach local solutions and do not offer 
globally optimized solutions. This technical nuance has, at worst, little impact on 
data segmentation applications.

52
2
Basic Tools and Principles of Process Control
> 
>
≤ 
≤ 
Fig. 2.24 Decision trees for sensors X (left) and Z (right) by time index
2.8.2 
Data Segments Based on Functional Data Analysis 
A second approach for identifying data segments is to ﬁt functional models to the 
data. Speciﬁcally, we ﬁt step functions at knots whose location was determined to 
minimize the residual sum of squares (SSR) using the Python package pwlf. 
For sensor X, we ﬁt a step function using 8 knots (9 segments). For sensor Z, we 
get a reasonable ﬁt using only 5 knots (6 segments). 
def fitPiecewiseLinearFit(sensor, segments): 
model = pwlf.PiecewiseLinFit(sensor['Time'], sensor['values'], degree=0) 
model.fit(segments) 
return model 
modelX = fitPiecewiseLinearFit(sensorX, 9) 
modelX = fitPiecewiseLinearFit(sensorZ, 6) 
In Fig. 2.25, we show ﬁts of step functions for X and Z using 8 and 5 knots, 
respectively. For sensor X, the step function is:

2.8
Process Analysis with Data Segments
53
Fig. 2.25 Step function ﬁts for sensors X (left) and Z (right) by time index. The gray vertical lines 
indicate the position of the knots 
Range
Prediction 
Level 1
upper T i m e less than 69.4Time < 69.4
275.0 
Level 2
69.4 less than or equals upper T i m e less than 131.669.4 ≤Time < 131.6
1400.1 
Level 3
131.6 less than or equals upper T i m e less than 902.2131.6 ≤Time < 902.2
3751.9 
Level 4
902.2 less than or equals upper T i m e less than 1377.3902.2 ≤Time < 1377.3
4628.8 
Level 5
1377.3 less than or equals upper T i m e less than 1444.81377.3 ≤Time < 1444.8
5086.7 
Level 6
1444.8 less than or equals upper T i m e less than 1467.81444.8 ≤Time < 1467.8
4580.9 
Level 7
1467.8 less than or equals upper T i m e less than 1490.21467.8 ≤Time < 1490.2
3049.1 
Level 8
1490.2 less than or equals upper T i m e less than 1896.01490.2 ≤Time < 1896.0
1556.1 
Level 9
1896.0 less than or equals upper T i m e1896.0 ≤Time
86.0 
For sensor Z, we get the following result: 
Range
Prediction 
Level 1
upper T i m e less than 63.5Time < 63.5
460.2 
Level 2
63.5 less than or equals upper T i m e less than 70.663.5 ≤Time < 70.6
1486.1 
Level 3
70.6 less than or equals upper T i m e less than 101.170.6 ≤Time < 101.1
2554.4 
Level 4
101.1 less than or equals upper T i m e less than 890.1101.1 ≤Time < 890.1
3715.5 
Level 5
890.1 less than or equals upper T i m e less than 1001.0890.1 ≤Time < 1001.0
4784.3 
Level 6
1001.0 less than or equals upper T i m e1001.0 ≤Time
5163.5 
The segments that can be used to characterize a process under control are 131.6 less than or equals upper T i m e less than 902131.6 ≤
Time < 902 for X and 101.1 less than or equals upper T i m e less than 890.1101.1 ≤Time < 890.1. These segments are shorter for 
sensor X than the segments identiﬁed by a decision tree (Sect. 2.8.1). Functional 
data analysis provides, however, more degrees of freedom in terms of the functional 
model being used and the number of knots applied in a speciﬁc ﬁt.

54
2
Basic Tools and Principles of Process Control
2.9 
Chapter Highlights 
The main concepts and deﬁnitions introduced in this chapter include: 
• 
Statistical process control 
• 
Chronic problems 
• 
Common causes 
• 
Special causes 
• 
Assignable causes 
• 
Sporadic spikes 
• 
External feedback loops 
• 
Internal feedback loops 
• 
Control charts 
• 
Lower control limit (LCL) 
• 
Upper control limit (UCL) 
• 
Upper warning limit (UWL) 
• 
Lower warning limit (LWL) 
• 
Process capability study 
• 
Rational subgroups 
• 
Process capability indexes 
• 
Flow charts 
• 
Check sheets 
• 
Run charts 
• 
Histograms 
• 
Pareto charts 
• 
Scatterplots 
• 
Cause and effect diagrams 
• 
Control charts for attributes 
• 
Control charts for variables 
• 
Data segments 
• 
Decision trees 
• 
Functional data analysis 
2.10 
Exercises 
Exercise 2.1 Use Python and dataset OELECT.csv to chart the individual electri-
cal outputs of the 99 circuits. Do you observe any trend or non-random pattern in 
the data? Create a ¯X-chart with Python. 
Exercise 2.2 Chart the individual variability of the length of steel rods, in dataset 
STEELROD.csv. Is there any perceived assignable cause of non-randomness?

2.10
Exercises
55
Exercise 2.3 Examine the chart of the previous exercise for possible patterns of 
non-randomness. 
Exercise 2.4 Test the data in dataset OTURB2.csv for lack of randomness. In this 
dataset, we have three columns. In the ﬁrst, we have the sample size. In the second 
and third, we have the sample means and standard deviation. Chart the individual 
means. For the historical mean, use the mean of column xbar. For historical 
standard deviation, use (ˆσ 2/5)1/2, where ˆσ 2 is the pooled sample variance. 
Exercise 2.5 A sudden change in a process lowers the process mean by one 
standard deviation. It has been determined that the quality characteristic being 
measured is approximately normally distributed and that the change had no effect 
on the process variance: 
(a) What percentage of points are expected to fall outside the control limits on the 
¯X-chart if the subgroup size is 4? 
(b) Answer the same question for subgroups of size 6. 
(c) Answer the same question for subgroups of size 9. 
Exercise 2.6 Make capability analysis of the electric output (volts) of 99 circuits in 
dataset OELECT.csv, with target value of μ0 = 220 and LSL = 210, USL = 230. 
Exercise 2.7 Estimate the capability index Cpk for the output of the electronic 
circuits, based on dataset OELECT.csv when LSL = 210 and USL = 230. 
Determine the point estimate as well as its conﬁdence interval, with conﬁdence level 
0.95. 
Exercise 2.8 Estimate the capability index for the steel rods, given in dataset 
STEELROD.csv, when the length speciﬁcations are ξL = 19 and ξU = 21 [cm] 
and the level of conﬁdence is 1 − α = 0.95. 
Exercise 2.9 The speciﬁcation limits of the piston cycle times are 0.05 ± 0.01 s. 
Generate 20 cycle times at the lower level of the 7 control parameters: 
(a) Compute Cp and Cpk. 
(b) Compute a 95% conﬁdence interval for Cpk. 
Generate 20 cycle times at the upper level of the 7 control factors: 
(c) Recompute Cp and Cpk. 
(d) Recompute a 95% conﬁdence interval for Cpk. 
(e) Is there a signiﬁcant difference in process capability between lower and upper 
operating levels in the piston simulator? 
Exercise 2.10 A ﬁber manufacturer has a large contract that stipulates that its ﬁber, 
among other properties, has tensile strength greater than 1.800 [grams/ﬁber] in 95% 
of the ﬁbers used. The manufacturer states the standard deviation of the process is 
0.015 grams. 
(a) Assuming a process under statistical control, what is the smallest nominal value 
of the mean that will assure compliance with the contract?

56
2
Basic Tools and Principles of Process Control
(b) Given the nominal value in part a), what are the control limits of ¯X and S-charts 
for subgroups of size 6? 
(c) What is the process capability, if the process mean is μ = 1.82? 
Exercise 2.11 The output voltage of a power supply is speciﬁed as 350 ± 5 volts 
DC. Subgroups of four units are drawn from every batch and submitted to special 
quality control tests. The data from 30 subgroups on output voltage produced
30 
i=1 ¯X = 10, 950.00 and 30 
i=1 Ri = 77.32: 
(a) Compute the control limits for ¯X and R. 
(b) Assuming statistical control and a normal distribution of output voltage, what 
properties of defective product are being made? 
(c) If the power supplies are set to a nominal value of 350 volts, what is now the 
proportion of defective products? 
(d) Compute the new control limits for ¯X and R. 
(e) If these new control limits are used, but the adjustment to 350 volts is not 
carried out, what is the probability that this fact will not be detected on the 
ﬁrst subgroup? 
(f) What is the process capability before and after the adjustment of the nominal 
value to 350 volts? Compute both Cp and Cpk. 
Exercise 2.12 The following data were collected in a circuit pack production plant 
during October: 
Number of nonconformities 
Missing component 
293 
Wrong component 
431 
Too much solder
120 
Insufﬁcient solder
132 
Failed component
183 
An improvement team recommended several changes that were implemented in 
the ﬁrst week of November. The following data were collected in the second week 
of November. 
Number of nonconformities 
Missing component 
34 
Wrong component 
52 
Too much solder
25 
Insufﬁcient solder
34 
Failed component
18

2.10
Exercises
57
(a) Construct Pareto charts of the nonconformities in October and the second week 
of November. 
(b) Has the improvement team produced signiﬁcant differences in the type of 
nonconformities? 
Exercise 2.13 Control charts for ¯X and R are maintained on total soluble solids 
produced at 20 ◦C in parts per million (ppm). Samples are drawn from production 
containers every hour and tested in a special test device. The test results are orga-
nized into subgroups of n = 5 measurements, corresponding to 5 h of production. 
After 125 h of production, we ﬁnd that 25 
i=1 ¯Xi = 390.8 and 25 
i=1 Ri = 84. The 
speciﬁcation on the process states that containers with more than 18 ppm of total 
soluble solids should be reprocessed. 
(a) Compute an appropriate capability index. 
(b) Assuming a normal distribution and statistical control, what proportion of the 
sample measurements are expected to be out of spec? 
(c) Compute the control limits for ¯X and R. 
Exercise 2.14 Part I: Run the piston simulator at the lower levels of the 7 piston 
parameters and generate 100 cycle times. Add 0.02 to the last 50 cycle times. 
(a) Compute control limits of ¯X and R by constructing subgroups of size 5, and 
analyze the control charts. 
Part II: Randomly shufﬂe the cycle times using the Python function ran-
dom.sample. 
(b) Recompute the control limits of ¯X and R and reanalyze the control charts. 
(c) Explain the differences between (a) and (b). 
Exercise 2.15 Part I: Run the piston simulator by specifying the 7 piston param-
eters within their acceptable range. Record the 7 operating levels you used and 
generate 20 subgroups of size 5. 
1. Compute the control limits for ¯X and S. 
Part II: Rerun the piston simulator at the same operating conditions and generate 
20 subgroups of size 10. 
2. Recompute the control limits for ¯X and S. 
3. Explain the differences between (a) and (b). 
Exercise 2.16 Repeat the data segment analysis from Sect. 2.8.2 with a piecewise  
linear regression ﬁt. This can be done using pwlf.PiecewiseLinFit initialized with 
the keyword argument degree=1. 
(a) Prepare models with a variety of knots. 
(b) Compare the results to the step function ﬁts. 
(c) When would you use a piecewise linear ﬁt compared to a step function ﬁt?

Chapter 3 
Advanced Methods of Statistical Process 
Control 
Preview Following Chap. 2, we present in this chapter more advanced methods 
of statistical process control. We start with testing whether data collected over 
time are randomly distributed around a mean level, or whether there is a trend 
or a shift in the data. The tests which we consider are nonparametric run tests. 
These tests are required as a ﬁrst step in checking the statistical stability of 
a process. This is followed with a section on modiﬁed Shewhart type control 
charts for the mean. Modiﬁcations of Shewhart charts were introduced as SPC 
tools, in order to increase the power of sequential procedures to detect change. 
Section 3.3 is devoted to the problem of determining the size and frequency of 
samples for proper statistical control of processes by Shewhart control charts. 
In Sect. 3.4, we introduce an alternative control tool, based on cumulative sums 
we develop and study the famous CUSUM procedures based on Page’s control 
schemes. Special computer programs are given in Sect. 3.4 for the estimation of 
the probability of false alarm, conditional expected delay, and expected run length; 
an example is cusumPfaCed. The chapter concludes with sections on modern topics 
introducing the readers to non-standard techniques and applications. In Sect. 3.5, 
Bayesian detection procedures are presented. Section 3.6 is devoted to procedures 
of process control that track the process level. The last section introduces tools from 
engineering control theory, which are useful in automatically controlled processes. 
These include dynamic linear models (DLMs), stochastic control, proportional 
rules, and dynamic programming. 
3.1 
Tests of Randomness 
In performing process capability analysis (see Chap. 2) or analyzing retroactively 
data for constructing a control chart, the ﬁrst thing we would like to test is whether 
these data are randomly distributed around their mean. This means that the process 
Supplementary Information The online version contains supplementary material available at 
(https://doi.org/10.1007/978-3-031-28482-3_3). 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3_3
59

60
3
Advanced Methods of Statistical Process Control
is statistically stable and only common causes affect the variability. In this section, 
we discuss such tests of randomness. 
Consider a sample x 1 comma x 2 comma midline horizontal ellipsis comma x Subscript n Baselinex1, x2, · · · , xn, where the index of the values of x indicates 
some kind of ordering. For example, x 1x1 is the ﬁrst observed value of X, x 2x2 is 
the second observed value, etc., while x Subscript nxn is the value observed last. If the sample 
is indeed random, there should be no signiﬁcant relationship between the values 
of X and their position in the sample. Thus, tests of randomness usually test 
the hypothesis that all possible conﬁgurations of the x’s are equally probable, 
against the alternative hypothesis that some signiﬁcant clustering of members 
takes place. For example, suppose that we have a sequence of 5 0’s and 5 1’s. 
The ordering 0 comma 1 comma 1 comma 0 comma 0 comma 0 comma 1 comma 1 comma 0 comma 10, 1, 1, 0, 0, 0, 1, 1, 0, 1 seems to be random, while the ordering 
0 comma 0 comma 0 comma 0 comma 0 comma 1 comma 1 comma 1 comma 1 comma 10, 0, 0, 0, 0, 1, 1, 1, 1, 1 seems, conspicuously, not to be random. 
3.1.1 
Testing the Number of Runs 
In a sequence of m 1m1 0’s and m 2m2 1’s, we distinguish between runs of 0’s, i.e., 
an uninterrupted string of 0’s, and runs of 1’s. Accordingly, in the sequence 
0 1 1 1 0 0 1 0 1 10 1 1 1 0 0 1 0 1 1, there are 4 0’s and 6 1’s, and there are 3 runs of 0’s and 
3 runs of 1’s, i.e., a total of 6 runs. We denote the total number of runs by R. 
The probability distribution of the total number of runs, R, is determined under 
the model of randomness. It can be shown that if there are m 1m1 0’s and m 2m2 1’s, then 
probability left brace upper R equals 2 k right brace equals StartFraction 2 StartBinomialOrMatrix m 1 minus 1 Choose k minus 1 EndBinomialOrMatrix StartBinomialOrMatrix m 2 minus 1 Choose k minus 1 EndBinomialOrMatrix Over StartBinomialOrMatrix n Choose m 2 EndBinomialOrMatrix EndFractionPr{R = 2k} =
2
m1−1
k−1
m2−1
k−1

 n
m2

(3.1.1) 
and 
probability left brace upper R equals 2 k plus 1 right brace equals StartFraction StartBinomialOrMatrix m 1 minus 1 Choose k minus 1 EndBinomialOrMatrix StartBinomialOrMatrix m 2 minus 1 Choose k EndBinomialOrMatrix plus StartBinomialOrMatrix m 1 minus 1 Choose k EndBinomialOrMatrix StartBinomialOrMatrix m 2 minus 1 Choose k minus 1 EndBinomialOrMatrix Over StartBinomialOrMatrix n Choose m 2 EndBinomialOrMatrix EndFraction periodPr{R = 2k + 1} =
m1−1
k−1
m2−1
k

+
m1−1
k
m2−1
k−1

 n
m2

.
(3.1.2) 
Here, n is the sample size, m 1 plus m 2 equals nm1 + m2 = n. 
One alternative to the hypothesis of randomness is that there is a tendency for 
clustering of the 0’s (or 1’s). In such a case, we expect to observe longer runs of 0’s 
(or 1’s) and, consequently, a smaller number of total runs. In this case, the hypothesis 
of randomness is rejected if the total number of runs, R, is too small. On the other 
hand, there could be an alternative to randomness that is the reverse of clustering. 
This alternative is called “mixing.” For example, the following sequence of 10 0’s 
and 10 1’s is completely mixed and is obviously not random: 
0 comma 1 comma 0 comma 1 comma 0 comma 1 comma 0 comma 1 comma 0 comma 1 comma 0 comma 1 comma 0 comma 1 comma 0 comma 1 comma 0 comma 1 comma 0 comma 1 period0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1.
The total number of runs here is upper R equals 20R = 20. Thus, if there are too many runs, one 
should also reject the hypothesis of randomness. Consequently, if we consider the

3.1
Tests of Randomness
61
null hypothesis upper H 0H0 of randomness against the alternative upper H 1H1 of clustering, the lower 
(left) tail of the distribution should be used for the rejection region. If the alternative, 
upper H 1H1, is the hypothesis of mixing, then the upper (right) tail of the distribution should 
be used. If the alternative is either clustering or mixing, the test should be two-sided. 
We test the hypothesis of randomness by using the test statistic R, which is the 
total number of runs. The critical region for the one-sided alternative that there is 
clustering is of the form: 
upper R less than or equals upper R Subscript alpha Baseline commaR ≤Rα,
where upper R Subscript pRp is the pth quantile of the null distribution of R. For the one-sided 
alternative of mixing, we reject upper H 0H0 if upper R greater than or equals upper R Subscript 1 minus alphaR ≥R1−α. In cases of large samples, we 
can use the normal approximations 
upper R Subscript alpha Baseline equals mu Subscript upper R Baseline minus z Subscript 1 minus alpha Baseline sigma Subscript upper RRα = μR −z1−ασR
and 
upper R Subscript 1 minus alpha Baseline equals mu Subscript upper R Baseline plus z Subscript 1 minus alpha Baseline sigma Subscript upper R Baseline commaR1−α = μR + z1−ασR,
where 
mu Subscript upper R Baseline equals 1 plus 2 m 1 m 2 divided by nμR = 1 + 2m1m2/n
(3.1.3) 
and 
sigma Subscript upper R Baseline equals left bracket StartFraction 2 m 1 m 2 left parenthesis 2 m 1 m 2 minus n right parenthesis Over n squared left parenthesis n minus 1 right parenthesis EndFraction right bracket Superscript 1 divided by 2 Baseline commaσR =
2m1m2(2m1m2 −n)
n2(n −1)
1/2
,
(3.1.4) 
are the mean and standard deviation, respectively, of R under the hypothesis of 
randomness. We can also use the normal distribution to approximate the P-value of 
the test. For one-sided tests, we have 
alpha Subscript upper L Baseline equals probability left brace upper R less than or equals r right brace approximately equals upper Phi left parenthesis left parenthesis r minus mu Subscript upper R Baseline right parenthesis divided by sigma Subscript upper R Baseline right parenthesisαL = Pr{R ≤r} ∼= ((r −μR)/σR)
(3.1.5) 
and 
alpha Subscript upper U Baseline equals probability left brace upper R greater than or equals r right brace approximately equals 1 minus upper Phi left parenthesis left parenthesis r minus mu Subscript upper R Baseline right parenthesis divided by sigma Subscript upper R Baseline right parenthesis commaαU = Pr{R ≥r} ∼= 1 −((r −μR)/σR),
(3.1.6) 
where r is the observed number of runs. For the two-sided alternative, the P-value 
of the test is approximated by 
alpha prime equals StartLayout Enlarged left brace 1st Row 1st Column 2 alpha Subscript upper L Baseline comma 2nd Column if upper R less than mu Subscript upper R Baseline 2nd Row 1st Column 2 alpha Subscript upper U Baseline comma 2nd Column if upper R greater than mu Subscript upper R Baseline period EndLayoutα′ =

2αL,
if R < μR
2αU,
if R > μR.
(3.1.7)

62
3
Advanced Methods of Statistical Process Control
3.1.2 
Runs Above and Below a Speciﬁed Level 
The runs test for the randomness of a sequence of 0’s and 1’s can be applied to test 
whether the values in a sequence, which are continuous in nature, are randomly 
distributed. We can consider whether the values are above or below the sample 
average or the sample median. In such a case, every value above the speciﬁed level 
will be assigned the value 1, while all the others will be assigned the value 0. Once 
this is done, the previous runs test can be applied. 
For example, suppose that we are given a sequence of n equals 30n = 30 observations, and 
we wish to test for randomness using the number of runs, R, above and below the 
median, upper M Subscript eMe. There are 15 observations below and 15 above the median. In this case, 
we take m 1 equals 15m1 = 15, m 2 equals 15m2 = 15, and n equals 30n = 30. In Table 3.1, we present the p.d.f. and 
c.d.f. of the number of runs, R, below and above the median, of a random sample of 
size n equals 30n = 30.
For a level of signiﬁcance of alpha equals 0.05α = 0.05 if upper R less than or equals 10R ≤10 or upper R greater than or equals 21R ≥21, the two-sided test 
rejects the hypothesis of randomness. Critical values for a two-sided runs test, above 
and below the median, can be obtained also by the large sample approximation 
StartLayout 1st Row StartLayout 1st Row 1st Column upper R Subscript alpha divided by 2 2nd Column equals mu Subscript upper R Baseline minus z Subscript 1 minus alpha divided by 2 Baseline sigma Subscript upper R Baseline 2nd Row 1st Column upper R Subscript 1 minus alpha divided by 2 2nd Column equals mu Subscript upper R Baseline plus z Subscript 1 minus alpha divided by 2 Baseline sigma Subscript upper R Baseline period EndLayout EndLayout
Rα/2 = μR −z1−α/2σR
R1−α/2 = μR + z1−α/2σR.
(3.1.8) 
Substituting m equals m 1 equals m 2 equals 15m = m1 = m2 = 15 and alpha equals 0.05α = 0.05, we have  mu Subscript upper R Baseline equals 16μR = 16, sigma Subscript upper R Baseline equals 2.69σR = 2.69, 
z 0.975 equals 1.96z0.975 = 1.96. Hence, upper R Subscript alpha divided by 2 Baseline equals 10.7Rα/2 = 10.7 and upper R Subscript 1 minus alpha divided by 2 Baseline equals 21.3R1−α/2 = 21.3. Thus, according to the 
large sample approximation, if upper R less than or equals 10R ≤10 or upper R greater than or equals 22R ≥22, the hypothesis of randomness is 
rejected. 
This test of the total number of runs, R, above and below a given level (e.g., the 
mean or the median of a sequence) can be performed by using Python. 
Example 3.1 In the present example, we have used Python to perform a run test on 
a simulated random sample of size n equals 28n = 28 from the normal distribution upper N left parenthesis 10 comma 1 right parenthesisN(10, 1). 
The test is of runs above and below the distribution mean 10. We obtain a total 
of upper R equals 14R = 14 with m 1 equals 13m1 = 13 values below and m 2 equals 15m2 = 15 values above the mean. In 
Fig. 3.1, we present this random sequence. 
In Python, we use the runsTest command provided in the mistat package. 
rnorm10 = mistat.load_data('RNORM10') 
x = [0 if xi <= 10 else 1 for xi in rnorm10] 
_ = mistat.runsTest(x, alternative='less', verbose=True) 
Runs Test 
Standard Normal = -0.3596, p-value = 0.3596 
alternative hypothesis: less 
black medium square■

3.1
Tests of Randomness
63
Table 3.1 Distribution of R, 
in a random sample of size 
n equals 30n = 30, m 1 equals 15m1 = 15
R 
p.d.f.
c.d.f. 
2 
0.00000 
0.00000 
3 
0.00000 
0.00000 
4 
0.00000 
0.00000 
5 
0.00002 
0.00002 
6 
0.00011 
0.00013 
7 
0.00043 
0.00055 
8 
0.00171 
0.00226 
9 
0.00470 
0.00696 
10 
0.01292 
0.01988 
11 
0.02584 
0.04572 
12 
0.05168 
0.09739 
13 
0.07752 
0.17491 
14 
0.11627 
0.29118 
15 
0.13288 
0.42407 
16 
0.15187 
0.57593 
17 
0.13288 
0.70882 
18 
0.11627 
0.82509 
19 
0.07752 
0.90261 
20 
0.05168 
0.95428 
21 
0.02584 
0.98012 
22 
0.01292 
0.99304 
23 
0.00470 
0.99774 
24 
0.00171 
0.99945 
25 
0.00043 
0.99987 
26 
0.00011 
0.99998 
27 
0.00002 
1.00000 
28 
0.00000 
1.00000 
29 
0.00000 
1.00000 
30 
0.00000 
1.00000
3.1.3 
Runs Up and Down 
Tests of the total number of runs above or below a speciﬁed level may not 
be sufﬁcient in cases where the alternative hypothesis to randomness is cyclical 
ﬂuctuations in the level of the process. For example, a sequence may show consistent 
ﬂuctuations up and down, as in the following example: 
negative 1 comma negative 0.75 comma negative 0.50 comma negative 0.25 comma 0 comma 0.5 comma 1 comma 0.5 comma 0.25 comma negative 0.75 comma midline horizontal ellipsis period −1, −0.75, −0.50, −0.25, 0, 0.5, 1, 0.5, 0.25, −0.75, · · · .
Here we see a steady increase from negative 1−1 to 1 and then a steady decrease. This 
sequence is obviously not random, and even the previous test of the total number of 
runs above and below 0 will reject the hypothesis of randomness. If the development

64
3
Advanced Methods of Statistical Process Control
Fig. 3.1 Random normal sequence and the runs above and below the level 10
of the sequence is not as conspicuous as that above, as, for example, in the sequence 
negative 1 comma negative 0.75 comma negative 0.50 comma 1 comma 0.5 comma negative 0.25 comma 0 comma 0.25 comma negative 0.25 comma 1 comma 0.5 comma 0.25 comma negative 0.75 comma midline horizontal ellipsis comma −1, −0.75, −0.50, 1, 0.5, −0.25, 0, 0.25, −0.25, 1, 0.5, 0.25, −0.75, · · · ,
the runs test above and below 0 may not reject the hypothesis of randomness. Indeed, 
in the present case, if we replace every negative number by 0 and every non-negative 
number by plus 1+1, we ﬁnd that m 1 equals 6m1 = 6, m 2 equals 7m2 = 7, and upper R equals 7R = 7. In this case, the exact 
value of alpha Subscript upper LαL is 0.5, and the hypothesis of randomness is not rejected. 
To test for possible cyclical effects, we use a test of runs up and down. Let  
x 1 comma x 2 comma midline horizontal ellipsis comma x Subscript n Baselinex1, x2, · · · , xn be a given sequence, and let us deﬁne 
y Subscript i Baseline equals StartLayout Enlarged left brace 1st Row 1st Column plus 1 comma 2nd Column if x Subscript i Baseline less than x Subscript i plus 1 Baseline 2nd Row 1st Column negative 1 comma 2nd Column if x Subscript i Baseline greater than or equals x Subscript i plus 1 Baseline EndLayoutyi =

+1,
if xi < xi+1
−1,
if xi ≥xi+1
for i equals 1 comma midline horizontal ellipsis comma n minus 1i = 1, · · · , n −1. We count, then, the total number of runs, R, up and down. 
A run up is a string of plus 1+1’s, while a run down is a string of negative 1−1’s. In the previous 
sequence, we have the following values of x Subscript ixi and y Subscript iyi: 
x Subscript ixi
negative 1.00−1.00
negative 0.75−0.75
negative 0.50−0.50 1.00 0.050 
negative 0.25−0.25 0.00 0.25 
negative 0.25−0.25 1.00 0.50 0.25 
negative 0.75−0.75
y Subscript iyi
1
1
1
minus−1 
minus−1
1
1
minus−1
1
minus−1 
minus−1 
minus−1 
We thus have a total of upper R Superscript asterisk Baseline equals 6R∗= 6 runs, 3 up and 3 down, with n equals 13n = 13. 
To test the hypothesis of randomness based on the number of runs up and down, 
we need the null distribution of upper R Superscript asteriskR∗.

3.1
Tests of Randomness
65
When the sample size is sufﬁciently large, we can use the normal approximation 
upper R Subscript alpha Superscript asterisk Baseline equals mu Subscript upper R Sub Superscript asterisk Baseline minus z Subscript 1 minus alpha Baseline sigma Subscript upper R Sub Superscript asteriskR∗
α = μR∗−z1−ασR∗
and 
upper R Subscript 1 minus alpha Superscript asterisk Baseline equals mu Subscript upper R Sub Superscript asterisk Subscript Baseline plus z Subscript 1 minus alpha Baseline sigma Subscript upper R Sub Superscript asterisk Subscript Baseline commaR∗
1−α = μR∗+ z1−ασR∗,
where 
mu Subscript upper R Sub Superscript asterisk Baseline equals left parenthesis 2 n minus 1 right parenthesis divided by 3μR∗= (2n −1)/3
(3.1.9) 
and 
sigma Subscript upper R Sub Superscript asterisk Subscript Baseline equals left bracket left parenthesis 16 n minus 29 right parenthesis divided by 90 right bracket Superscript 1 divided by 2 Baseline periodσR∗= [(16n −29)/90]1/2.
(3.1.10) 
The attained signiﬁcance levels are approximated by 
alpha Subscript upper L Superscript asterisk Baseline equals upper Phi left parenthesis left parenthesis r Superscript asterisk Baseline minus mu Subscript upper R Sub Superscript asterisk Subscript Baseline right parenthesis divided by sigma Subscript upper R Sub Superscript asterisk Subscript Baseline right parenthesisα∗
L = ((r∗−μR∗)/σR∗)
(3.1.11) 
and 
alpha Subscript upper U Superscript asterisk Baseline equals 1 minus upper Phi left parenthesis left parenthesis r Superscript asterisk Baseline minus mu Subscript upper R Sub Superscript asterisk Subscript Baseline right parenthesis divided by sigma Subscript upper R Sub Superscript asterisk Subscript Baseline right parenthesis periodα∗
U = 1 −((r∗−μR∗)/σR∗).
(3.1.12) 
Example 3.2 The sample in dataset YARNSTRG.csv contains 100 values of log-
yarn strength. In this sample, there are upper R Superscript asterisk Baseline equals 64R∗= 64 runs up or down, 32 runs up and 32 
runs down. The expected value of upper R Superscript asteriskR∗is 
mu Subscript upper R Sub Superscript asterisk Subscript Baseline equals StartFraction 199 Over 3 EndFraction equals 66.33 commaμR∗= 199
3
= 66.33,
and its standard deviation is 
sigma Subscript upper R Sub Superscript asterisk Subscript Baseline equals left parenthesis StartFraction 1600 minus 29 Over 90 EndFraction right parenthesis Superscript 1 divided by 2 Baseline equals 4.178 periodσR∗=
1600 −29
90
1/2
= 4.178.
The attained level of signiﬁcance is 
alpha Subscript upper L Baseline equals upper Phi left parenthesis StartFraction 64 minus 66.33 Over 4.178 EndFraction right parenthesis equals 0.289 periodαL = 
64 −66.33
4.178

= 0.289.
The hypothesis of randomness is not rejected.
black medium square■

66
3
Advanced Methods of Statistical Process Control
Table 3.2 Expected values 
of upper R Subscript kRk as a function of n 
k 
upper E left brace upper R Subscript k Baseline right braceE{Rk}
1 
left parenthesis 2 n minus 1 right parenthesis divided by 3(2n −1)/3
2 
left parenthesis 3 n minus 5 right parenthesis divided by 12(3n −5)/12
3 
left parenthesis 4 n minus 11 right parenthesis divided by 60(4n −11)/60
4 
left parenthesis 5 n minus 19 right parenthesis divided by 360(5n −19)/360
5 
left parenthesis 6 n minus 29 right parenthesis divided by 2520(6n −29)/2520
6 
left parenthesis 7 n minus 41 right parenthesis divided by 20160(7n −41)/20160
7 
left parenthesis 8 n minus 55 right parenthesis divided by 181440(8n −55)/181440
3.1.4 
Testing the Length of Runs Up and Down 
In the previous sections, we have considered tests of randomness based on the total 
number of runs. If the number of runs is small relative to the size of the sequence, 
n, we obviously expect some of the runs to be rather long. We shall now consider 
the question of just how long the runs can be under a state of randomness. 
Consider runs up and down, and let upper R Subscript kRk left parenthesis k equals 1 comma 2 comma midline horizontal ellipsis right parenthesis(k = 1, 2, · · · ) be the total number of 
runs, up or down, of length greater than or equal to k. Thus, upper R 1 equals upper R Superscript asteriskR1 = R∗, upper R 2R2 is the total 
number of runs, up or down, of length 2 or more, etc. The following are formulas for 
the expected values of each upper R Subscript kRk, i.e., upper E left brace upper R Subscript k Baseline right braceE{Rk}, under the assumption of randomness. 
Each expected value is expressed as a function of the size n of the sequence (Table 
3.2). 
In general, we have 
upper E left brace upper R Subscript k Baseline right brace equals StartFraction 2 left bracket n left parenthesis k plus 1 right parenthesis minus k squared minus k plus 1 right bracket Over left parenthesis k plus 2 right parenthesis factorial EndFraction comma 1 less than or equals k less than or equals n minus 1 periodE{Rk} = 2[n(k + 1) −k2 −k + 1]
(k + 2)!
,
1 ≤k ≤n −1.
(3.1.13) 
If k greater than or equals 5k ≥5, we have  upper E left brace upper R Subscript k Baseline right brace approaches the limit upper V left brace upper R Subscript k Baseline right braceE{Rk}
.= V {Rk}, and the Poisson approximation to the 
probability distribution of upper R Subscript kRk is considered good, provided n greater than 20n > 20. Thus, if k greater than or equals 5k ≥5, 
according to the Poisson approximation, we ﬁnd 
probability left brace upper R Subscript k Baseline greater than or equals 1 right brace approaches the limit 1 minus exp left parenthesis minus upper E left brace upper R Subscript k Baseline right brace right parenthesis periodPr{Rk ≥1} .= 1 −exp(−E{Rk}).
(3.1.14) 
For example, if n equals 50n = 50, we present upper E left brace upper R Subscript k Baseline right braceE{Rk} and Prleft brace upper R Subscript k Baseline greater than or equals 1 right brace{Rk ≥1} in the following table. 
k 
upper E left brace upper R Subscript k Baseline right braceE{Rk}
Prleft brace upper R Subscript k Baseline greater than or equals 1 right brace{Rk ≥1}
5 
0.1075 
0.1020 
6 
0.0153 
0.0152 
7 
0.0019 
0.0019 
We see in the above table that the probability to observe even 1 run, up or down, 
of length 6 or more is quite small. This is the reason for the rule of thumb, to reject 
the hypothesis of randomness if a run is of length 6 or more. This and other rules 
of thumb were presented in Chap. 2 for ongoing process control.

3.2
Modiﬁed Shewhart Control Charts for ¯X
67
3.2 
Modiﬁed Shewhart Control Charts for ¯X 
The modiﬁed Shewhart control chart for upper X overbar ¯X, to detect possible shifts in the means of 
the parent distributions, gives a signal to stop, whenever the sample means upper X overbar ¯X fall 
outside the control limits theta 0 plus or minus a StartFraction sigma Over StartRoot n EndRoot EndFractionθ0 ± a σ
√n, or whenever a run of r sample means falls 
outside the warning limits (all on the same side) theta 0 plus or minus w StartFraction sigma Over StartRoot n EndRoot EndFractionθ0 ± w σ
√n. 
We denote the modiﬁed scheme by left parenthesis a comma w comma r right parenthesis(a, w, r). For example, 3-sigmaσ control charts, 
with warning lines at 2-sigmaσ and a run of r equals 4r = 4 is denoted by left parenthesis 3 comma 2 comma 4 right parenthesis(3, 2, 4). If  r equals normal infinityr = ∞, 
the scheme left parenthesis 3 comma 0 comma normal infinity right parenthesis(3, 0, ∞) is reduced to the common Shewhart 3-sigmaσ procedure. Similarly, 
the scheme left parenthesis a comma 3 comma 1 right parenthesis(a, 3, 1) for a greater than 3a > 3 is equivalent to the Shewhart 3-sigmaσ control charts. 
A control chart for a left parenthesis 3 comma 1.5 comma 2 right parenthesis(3, 1.5, 2) procedure is shown in Fig. 3.2. The means are of 
samples of size 5. There is no run of length 2 or more between the warning and 
action limits. 
The run length, of a control chart, is the number of samples taken until an “out 
of control” alarm is given. The average run length, ARL, of an left parenthesis a comma w comma r right parenthesis(a, w, r) plan is 
smaller than that of the simple Shewhart a-sigmaσ procedure. We denote the average run 
length of an left parenthesis a comma w comma r right parenthesis(a, w, r) procedure by ARL left parenthesis a comma w comma r right parenthesis(a, w, r). Obviously, if w and r are small, 
we will tend to stop too soon, even when the process is under control. For example, if 
r equals 1r = 1, w equals 2w = 2, then any procedure left parenthesis a comma 2 comma 1 right parenthesis(a, 2, 1) is equivalent to Shewhart 2-sigmaσ procedure, 
which stops on the average every 20 samples, when the process is under control.
Fig. 3.2 A modiﬁed Shewhart upper X overbar ¯X-chart 

68
3
Advanced Methods of Statistical Process Control
Weindling (1967) and Page (1962) derived the formula for the average run length 
ARLleft parenthesis a comma w comma r right parenthesis(a, w, r). Page used the theory of runs, while Weindling used another theory 
(Markov chains theory). An excellent expository paper discussing the results was 
published by Weindling et al. (1970). 
The basic formula for the determination of the average run length is 
StartLayout 1st Row ARL Subscript theta Baseline left parenthesis a comma w comma r right parenthesis equals 2nd Row left bracket upper P Subscript theta Baseline left parenthesis a right parenthesis plus upper H Subscript theta Superscript r Baseline left parenthesis a comma w right parenthesis StartFraction 1 minus upper H Subscript theta Baseline left parenthesis a comma w right parenthesis Over 1 minus upper H Subscript theta Superscript r Baseline left parenthesis a comma w right parenthesis EndFraction plus upper L Subscript theta Superscript r Baseline left parenthesis a comma w right parenthesis StartFraction 1 minus upper L Subscript theta Baseline left parenthesis a comma w right parenthesis Over 1 minus upper L Subscript theta Superscript r Baseline left parenthesis a comma w right parenthesis EndFraction right bracket Superscript negative 1 Baseline comma EndLayoutARLθ(a, w, r) =

Pθ(a) + H r
θ (a, w) 1 −Hθ(a, w)
1 −H r
θ (a, w) + Lr
θ(a, w)1 −Lθ(a, w)
1 −Lr
θ(a, w)
−1
,
(3.2.1) 
where 
StartLayout 1st Row 1st Column upper P Subscript theta Baseline left parenthesis a right parenthesis 2nd Column equals upper P Subscript theta Baseline left brace upper X overbar less than or equals theta 0 minus a StartFraction sigma Over StartRoot n EndRoot EndFraction right brace plus upper P Subscript theta Baseline left brace upper X overbar greater than or equals theta 0 plus a StartFraction sigma Over StartRoot n EndRoot EndFraction right brace 2nd Row 1st Column upper H Subscript theta Baseline left parenthesis a comma w right parenthesis 2nd Column equals upper P Subscript theta Baseline left brace theta 0 plus w StartFraction sigma Over StartRoot n EndRoot EndFraction less than or equals upper X overbar less than or equals theta 0 plus a StartFraction sigma Over StartRoot n EndRoot EndFraction right brace 3rd Row 1st Column upper L Subscript theta Baseline left parenthesis a comma w right parenthesis 2nd Column equals upper P Subscript theta Baseline left brace theta 0 minus a StartFraction sigma Over StartRoot n EndRoot EndFraction less than or equals upper X overbar less than or equals theta 0 minus w StartFraction sigma Over StartRoot n EndRoot EndFraction right brace period EndLayout
Pθ(a) = Pθ
	
¯X ≤θ0 −a σ
√n

+ Pθ
	
¯X ≥θ0 + a σ
√n

Hθ(a, w) = Pθ
	
θ0 + w σ
√n ≤¯X ≤θ0 + a σ
√n

Lθ(a, w) = Pθ
	
θ0 −a σ
√n ≤¯X ≤θ0 −w σ
√n

.
(3.2.2) 
In Table 3.3, we present some values of ARLleft parenthesis a comma w comma r right parenthesis(a, w, r) for a equals 3a = 3, w equals 1 left parenthesis .5 right parenthesis 2.5w = 1(.5)2.5, 
r equals 2 left parenthesis 1 right parenthesis 7r = 2(1)7, when the samples are of size n equals 5n = 5 from a normal distribution, and the 
shift in the mean is of size theta minus theta 0 equals delta sigmaθ −θ0 = δσ. We see in the table that the procedures 
left parenthesis 3 comma 1 comma 7 right parenthesis(3, 1, 7), left parenthesis 3 comma 1.5 comma 5 right parenthesis(3, 1.5, 5), left parenthesis 3 comma 2 comma 3 right parenthesis(3, 2, 3) and left parenthesis 3 comma 2.5 comma 2 right parenthesis(3, 2.5, 2) yield similar ARL functions. However, 
these modiﬁed procedures are more efﬁcient than the Shewhart 3-sigmaσ procedure. They 
all have close ARL values when delta equals 0δ = 0, but when delta greater than 0δ > 0 their ARL values are 
considerably smaller than the Shewhart’s procedure.
3.3 
The Size and Frequency of Sampling for Shewhart 
Control Charts 
In the present section, we discuss the importance of designing the sampling 
procedure for Shewhart control charts. We start with the problem of the economic 
design of sampling for upper X overbar ¯X-charts. 
3.3.1 
The Economic Design for ¯X-charts 
Duncan (1956, 1971, 1978) studied the question of optimally designing the ¯X 
control charts. We show here, in a somewhat simpler fashion, how this problem can

3.3
The Size and Frequency of Sampling for Shewhart Control Charts
69
Table 3.3 Values of 
ARLleft parenthesis a comma w comma r right parenthesis(a, w, r), a equals 3.00a = 3.00
against delta equals left parenthesis mu 1 minus mu 0 right parenthesis divided by sigmaδ = (μ1 −μ0)/σ, 
n equals 5n = 5
r 
w
deltaδ
2
3
4
5
6
7 
1.00 
0.00 
22.0 
107.7 
267.9 
349.4 
366.9 
369.8 
0.25 
11.2 
32.1 
67.1 
101.3 
120.9 
129.0 
0.50
4.8
9.3 
14.9 
20.6 
25.4 
28.8 
0.75
2.8
4.4
5.8
7.1
8.1
8.9 
1.00
2.0
2.7
3.3
3.7
3.9
4.1 
1.25
1.6
2.0
2.2
2.3
2.3
2.4 
1.50
1.4
1.5
1.5
1.6
1.6
1.6 
1.75
1.2
1.2
1.2
1.2
1.2
1.2 
2.00
1.1
1.1
1.1
1.1
1.1
1.1 
2.25
1.0
1.0
1.0
1.0
1.0
1.0 
1.50 
0.00 
93.1 
310.2 
365.7 
370.1 
370.4 
370.4 
0.25 
31.7 
88.1 
122.8 
131.3 
132.9 
133.1 
0.50
9.3 
18.8 
26.8 
31.0 
32.6 
33.1 
0.75
4.0
6.4
8.2
9.4 
10.1 
10.4 
1.00
2.4
3.2
3.7
4.1
4.3
4.4 
1.25
1.7
2.1
2.2
2.3
2.4
2.4 
1.50
1.4
1.5
1.5
1.6
1.6
1.6 
1.75
1.2
1.2
1.2
1.2
1.2
1.2 
2.00
1.1
1.1
1.1
1.1
1.1
1.1 
2.25
1.0
1.0
1.0
1.0
1.0
1.0 
2.00 
0.00 
278.0 
367.8 
370.3 
370.4 
370.4 
370.4 
0.25 
84.7 
128.3 
132.8 
133.1 
133.2 
133.2 
0.50 
19.3 
30.0 
32.8 
33.3 
33.4 
33.4 
0.75
6.5
9.2 
10.3 
10.6 
10.7 
10.7 
1.00
3.1
3.9
4.3
4.4
4.5
4.5 
1.25
1.9
2.2
2.3
2.4
2.4
2.4 
1.50
1.4
1.5
1.6
1.6
1.6
1.6 
1.75
1.2
1.2
1.2
1.2
1.2
1.2 
2.00
1.1
1.1
1.1
1.1
1.1
1.1 
2.25
1.0
1.0
1.0
1.0
1.0
1.0 
2.50 
0.00 
364.1 
370.4 
370.4 
370.4 
370.4 
370.4 
0.25 
127.3 
133.0 
133.2 
133.2 
133.2 
133.2 
0.50 
30.6 
33.2 
33.4 
33.4 
33.4 
33.4 
0.75
9.6 
10.6 
10.7 
10.8 
10.8 
10.8 
1.00
4.0
4.4
4.5
4.5
4.5
4.5 
1.25
2.2
2.4
2.4
2.4
2.4
2.4 
1.50
1.5
1.6
1.6
1.6
1.6
1.6 
1.75
1.2
1.2
1.2
1.2
1.2
1.2 
2.00
1.1
1.1
1.1
1.1
1.1
1.1 
2.25
1.0
1.0
1.0
1.0
1.0
1.0

70
3
Advanced Methods of Statistical Process Control
be approached. More speciﬁcally, assume that we sample from a normal population 
and that σ 2 is known. A shift of size δ = (θ1 − θ0)/σ or larger should be detected 
with high probability. 
Let c [$/hour] be the hourly cost of a shift in the mean of size δ. Let  d[$] be the 
cost of sampling (and testing the items). Assuming that the time of shift from θ0 to 
θ1 = θ0 + δσ is exponentially distributed with mean 1/λ [hr] and that a penalty of 
1[$] is incurred for every unneeded inspection, the total expected cost is 
upper K left parenthesis h comma n right parenthesis approaches the limit StartFraction c h plus d n Over 1 minus upper Phi left parenthesis 3 minus delta StartRoot n EndRoot right parenthesis EndFraction plus StartFraction 1 plus d n Over lamda h EndFraction periodK(h, n) .=
ch + dn
1 −(3 −δ√n) + 1 + dn
λh
.
(3.3.1) 
This function can be minimized with respect to h and n, to determine the optimal 
sample size and frequency of sampling. Differentiating partially with respect to h 
and equating to zero, we obtain the formula of the optimal h, for a given n, namely 
h Superscript 0 Baseline equals left parenthesis StartFraction 1 plus d dot n Over c lamda EndFraction right parenthesis Superscript 1 divided by 2 Baseline left parenthesis 1 minus upper Phi left parenthesis 3 minus delta StartRoot n EndRoot right parenthesis right parenthesis Superscript 1 divided by 2 Baseline periodh0 =
1 + d · n
cλ
1/2
(1 −(3 −δ√n))1/2.
(3.3.2) 
However, the function K(h, n) is increasing with n, due to the contribution of the 
second term on the RHS. Thus, for this expected cost function, we take every h0 
hours a sample of size n = 4. Some values of h0 are: 
δ 
d
c
λ
h0 
2 
0.5 
3.0 
0.0027 
17.6 
1 
0.1 
30.0 
0.0027 
1.7 
For additional reading on this subject, see Gibra (1971). 
3.3.2 
Increasing the Sensitivity of p-charts 
The operating characteristic function for a Shewhart p-chart is the probability, as 
a function of p, that the statistic ModifyingAbove p With caret Subscript n ˆpn falls between the lower and upper control limits. 
Thus, the operating characteristic of a p-chart, with control limits p 0 plus or minus 3 StartRoot StartFraction p 0 left parenthesis 1 minus p 0 right parenthesis Over n EndFraction EndRootp0 ±3

p0(1−p0)
n
, 
is 
upper O upper C left parenthesis p right parenthesis equals probability Subscript theta Baseline left brace p 0 minus 3 StartRoot StartFraction p 0 left parenthesis 1 minus p 0 right parenthesis Over n EndFraction EndRoot less than ModifyingAbove p With caret Subscript n Baseline less than p 0 plus 3 StartRoot StartFraction p 0 left parenthesis 1 minus p 0 right parenthesis Over n EndFraction EndRoot right brace commaOC(p) = Prθ

p0 −3

p0(1 −p0)
n
< ˆpn < p0 + 3

p0(1 −p0)
n

,
(3.3.3) 
where ModifyingAbove p With caret Subscript n ˆpn is the proportion of defective items in the sample. n times ModifyingAbove p With caret Subscript nn× ˆpn has the binomial 
distribution, with c.d.f. upper B left parenthesis j semicolon n comma p right parenthesisB(j; n, p). Accordingly,

3.3
The Size and Frequency of Sampling for Shewhart Control Charts
71
Fig. 3.3 Typical OC curve for a p-chart 
StartLayout 1st Row 1st Column upper O upper C left parenthesis p right parenthesis equals 2nd Column upper B left parenthesis n p 0 plus 3 StartRoot n p 0 left parenthesis 1 minus p 0 right parenthesis EndRoot semicolon n comma p right parenthesis 2nd Row 1st Column minus 2nd Column upper B left parenthesis n p 0 minus 3 StartRoot n p 0 left parenthesis 1 minus p 0 right parenthesis EndRoot semicolon n comma p right parenthesis period EndLayout
OC(p) = B

np0 + 3

np0(1 −p0); n, p

−B

np0 −3

np0(1 −p0); n, p

.
(3.3.4) 
For large samples, we can use the normal approximation to upper B left parenthesis j semicolon n comma p right parenthesisB(j; n, p) and obtain 
OC left parenthesis p right parenthesis approximately equals upper Phi left parenthesis StartFraction left parenthesis UCL negative p right parenthesis StartRoot n EndRoot Over StartRoot p left parenthesis 1 minus p right parenthesis EndRoot EndFraction right parenthesis minus upper Phi left parenthesis StartFraction left parenthesis LCL negative p right parenthesis StartRoot n EndRoot Over StartRoot p left parenthesis 1 minus p right parenthesis EndRoot EndFraction right parenthesis periodOC(p) ∼= 
(UCL −p)√n
√p(1 −p)

−
(LCL −p)√n
√p(1 −p)

.
(3.3.5) 
The value of the OCleft parenthesis p right parenthesis(p) at p equals p 0p = p0 is 2 upper Phi left parenthesis 3 right parenthesis minus 1 equals 0.9972(3) −1 = 0.997. The values of OCleft parenthesis p right parenthesis(p) for 
p not equals p 0p ̸= p0 are smaller. A typical OCleft parenthesis p right parenthesis(p) function looks as in Fig. 3.3. 
When the process is in control with process fraction defective p 0p0, we have  
OCleft parenthesis p 0 right parenthesis equals 0.997(p0) = 0.997; otherwise, OCleft parenthesis p right parenthesis less than 0.997(p) < 0.997. The probability that we will detect 
a change in quality to level p 1p1, with a single point outside the control limits, is 
1 minus OC left parenthesis p 1 right parenthesis1 −OC(p1). As an example, suppose we have estimated p 0p0 as p overbar equals 0.15 ¯p = 0.15 from past 
data. With a sample of size n equals 100n = 100, our control limits are 
UCL equals 0.15 plus 3 left parenthesis left parenthesis 0.15 right parenthesis left parenthesis 0.85 right parenthesis divided by 100 right parenthesis Superscript 1 divided by 2 Baseline equals 0.257UCL = 0.15 + 3((0.15)(0.85)/100)1/2 = 0.257
and 
LCL equals 0.15 minus 3 left parenthesis left parenthesis 0.15 right parenthesis left parenthesis 0.85 right parenthesis divided by 100 right parenthesis Superscript 1 divided by 2 Baseline equals 0.043 periodLCL = 0.15 −3((0.15)(0.85)/100)1/2 = 0.043.
In Table 3.4, we see that it is almost certain that a single point will fall outside 
the control limits when p equals 0.40p = 0.40, but it is unlikely that it will fall there when

72
3
Advanced Methods of Statistical Process Control
Table 3.4 Operating 
characteristic values for 
p-chart with p overbar equals 0.15 ¯p = 0.15 and 
n equals 100n = 100
p
OCleft parenthesis p right parenthesis(p)
1 minus OC left parenthesis p right parenthesis1 −OC(p)
1 minus left bracket OC left parenthesis p right parenthesis right bracket Superscript 51 −[OC(p)]5
0.05 
0.6255 
0.3745
0.9043 
0.10 
0.9713 
0.0287
0.1355 
0.15 
0.9974 
0.0026
0.0130 
0.20 
0.9236 
0.0764
0.3280 
0.25 
0.5636 
0.4364
0.9432 
0.30 
0.1736 
0.8264
0.9998 
0.40 
0.0018 
0.9982
1.0000 
p equals 0.20p = 0.20. However, if the process fraction defective remains at the p equals 0.20p = 0.20 level 
for several measurement periods, the probability of detecting the shift increases. The 
probability that at least one point falls outside the control limits when p equals 0.20p = 0.20 for 
5 consecutive periods is 
1 minus left bracket OC left parenthesis 0.20 right parenthesis right bracket Superscript 5 Baseline equals 0.3279 period1 −[OC(0.20)]5 = 0.3279.
The probability of detecting shifts in the fraction defective is even greater than 0.33 
if we apply run tests on the data. 
The OC curve can also be useful for determining the required sample size for 
detecting, with high probability, a change in the process fraction defective in a 
single measurement period. To see this, suppose that the system is in control at level 
p 0p0, and we wish to detect a shift to level p Subscript tpt with speciﬁed probability, 1 minus beta1 −β. For  
example, to be 90% conﬁdent that the sample proportion will be outside the control 
limits immediately after the process fraction defective changes to p Subscript tpt, we require that 
1 minus OC left parenthesis p Subscript t Baseline right parenthesis equals 0.90 period1 −OC(pt) = 0.90.
We can solve this equation to ﬁnd that the required sample size is 
n approaches the limit StartFraction left parenthesis 3 StartRoot p 0 left parenthesis 1 minus p 0 right parenthesis EndRoot plus z Subscript 1 minus beta Baseline StartRoot p Subscript t Baseline left parenthesis 1 minus p Subscript t Baseline right parenthesis EndRoot right parenthesis squared Over left parenthesis p Subscript t Baseline minus p 0 right parenthesis squared EndFraction periodn .= (3√p0(1 −p0) + z1−β
√pt(1 −pt))2
(pt −p0)2
.
(3.3.6) 
If we wish that with probability left parenthesis 1 minus beta right parenthesis(1 −β) the sample proportion will be outside the 
limits at least once within k sampling periods, when the precise fraction defective is 
p Subscript tpt, the required sample size is 
n approaches the limit StartFraction left parenthesis 3 StartRoot p 0 left parenthesis 1 minus p 0 right parenthesis EndRoot plus z Subscript 1 minus b Baseline StartRoot left parenthesis p Subscript t Baseline left parenthesis 1 minus p Subscript t Baseline right parenthesis EndRoot right parenthesis squared Over left parenthesis p Subscript t Baseline minus p 0 right parenthesis squared EndFraction comman .= (3√p0(1 −p0) + z1−b
√(pt(1 −pt))2
(pt −p0)2
,
(3.3.7) 
where b equals beta Superscript 1 divided by kb = β1/k. 
These results are illustrated in Table 3.5 for a process with p 0 equals 0.15p0 = 0.15. 
It is practical to take at each period a small sample of n equals 5n = 5. We see in Table 3.5 
that in this case, a change from 0.15 to 0.40 would be detected within 5 periods with 
a probability of 0.9. To detect smaller changes requires larger samples.

3.4
Cumulative Sum Control Charts
73
Table 3.5 Sample size 
required for probability 0.9 of 
detecting a shift to level p Subscript tpt
from level p 0 equals 0.15p0 = 0.15 (in one 
period and within ﬁve 
periods) 
p Subscript tpt
One period 
5 periods 
0.05 
183
69 
0.10 
847
217 
0.20 
1003
156 
0.25 
265
35 
0.30 
122
14 
0.40 
46
5 
Fig. 3.4 A plot of cumulative sums with drift after t equals 20t = 20
3.4 
Cumulative Sum Control Charts 
3.4.1 
Upper Page’s Scheme 
When the process level changes from a past or speciﬁed level, we expect that a 
control procedure will trigger an “alarm.” Depending on the size of the change and 
the size of the sample, it may take several sampling periods before the alarm occurs. 
A method that has a smaller ARL than the standard Shewhart control charts, for 
detecting certain types of changes, is the cumulative sum (or CUSUM) control 
chart that was introduced by Barnard (1959) and Page (1954). 
CUSUM charts differ from the common Shewhart control chart in several 
respects. The main difference is that instead of plotting the individual value of the 
statistic of interest, such as X, upper X overbar ¯X, S, R, p, or  c, a statistic based on the cumulative 
sums is computed and tracked. By summing deviations of the individual statistic 
from a target value, T, we get a consistent increase, or decrease, of the cumulative 
sum when the process is above, or below, the target. In Fig. 3.4, we show the 
behavior of the cumulative sums 
upper S Subscript t Baseline equals sigma summation Underscript i equals 1 Overscript t Endscripts left parenthesis upper X Subscript i Baseline minus 10 right parenthesisSt =
t
i=1
(Xi −10)
(3.4.1)

74
3
Advanced Methods of Statistical Process Control
Table 3.6 The number of 
monthly computer crashes 
due to power failures 
t
upper X Subscript tXt
t
upper X Subscript tXt
t
upper X Subscript tXt
1 
0
11 
0
21 
0 
2 
2
12 
0
22 
1 
3 
0
13 
0
23 
3 
4 
0
14 
0
24 
2 
5 
3
15 
0
25 
1 
6 
3
16 
2
26 
1 
7 
0
17 
2
27 
3 
8 
0
18 
1
28 
5 
9 
2
19 
0 
10 
1
20 
0
of data simulated from a normal distribution with mean 
mu Subscript t Baseline equals StartLayout Enlarged left brace 1st Row 1st Column 10 comma 2nd Column if t less than or equals 20 2nd Row 1st Column 13 comma 2nd Column if t greater than 20 EndLayoutμt =

10,
if t ≤20
13,
if t > 20
and sigma Subscript t Baseline equals 1σt = 1 for all t. 
We see that as soon as the shift in the mean of the data occurred, a pronounced 
drift in upper S Subscript tSt started. Page (1954) suggested to detect an upward shift in the mean by 
considering the sequence 
upper S Subscript t Superscript plus Baseline equals max left brace upper S Subscript t minus 1 Superscript plus Baseline plus left parenthesis upper X Subscript k Baseline minus upper K Superscript plus Baseline right parenthesis comma 0 right brace comma t equals 1 comma 2 comma midline horizontal ellipsis commaS+
t = max{S+
t−1 + (Xk −K+), 0},
t = 1, 2, · · · ,
(3.4.2) 
where upper S 0 Superscript plus Baseline identical to 0S+
0
≡0, and decide that a shift has occurred, as soon as upper S Subscript t Superscript plus Baseline greater than h Superscript plusS+
t
> h+. 
The statistics upper X Subscript tXt, t equals 1 comma 2 comma midline horizontal ellipsist = 1, 2, · · · , upon which the (truncated) cumulative sums are 
constructed, could be means of samples of n observations, standard deviations, 
sample proportions, or individual observations. In the following section, we will see 
how the parameters upper K Superscript plusK+ and h Superscript plush+ are determined. We will see that if upper X Subscript tXt are means of 
samples of size n, with process variance sigma squaredσ 2, and if the desired process mean is theta 0θ0, 
while the maximal tolerated process mean is theta 1θ1, theta 1 minus theta 0 greater than 0θ1 −θ0 > 0, then 
upper K Superscript plus Baseline equals StartFraction theta 0 plus theta 1 Over 2 EndFraction and h Superscript plus Baseline equals minus StartFraction sigma squared log alpha Over n left parenthesis theta 1 minus theta 0 right parenthesis EndFraction periodK+ = θ0 + θ1
2
and
h+ = −σ 2 log α
n(θ1 −θ0).
(3.4.3) 
0 less than alpha less than 10 < α < 1. 
Example 3.3 The above procedure of Page is now illustrated. The data in Table 3.6 
represent the number of computer crashes per month, due to power failures 
experienced at a computer center, over a period of 28 months. After a crash, the 
computers are made operational with an “Initial Program Load.” We refer to the 
data as the IPL dataset. 
Power failures are potentially very harmful. A computer center might be able to 
tolerate such failures when they are far enough apart. If they become too frequent, 

3.4
Cumulative Sum Control Charts
75
Table 3.7 The upper S Subscript t Superscript plusS+
t statistics 
for the IPL data 
t
upper X Subscript tXt
upper X Subscript t Baseline minus 1.07Xt −1.07
upper S Subscript t Superscript plusS+
t
1 
0 
minus−1.07
0 
2 
2
0.93
0.93 
3 
0 
minus−1.07
0 
4 
0 
minus−1.07
0 
5 
3
1.93
1.93 
6 
3
1.93
3.86 
7 
0 
minus−1.07
2.79 
8 
0 
minus−1.07
1.72 
9 
2
0.93
2.65 
10 
1 
minus−0.07
2.58 
11 
0 
minus−1.07
1.51 
12 
0 
minus−1.07
0.44 
13 
0 
minus−1.07
0 
14 
0 
minus−1.07
0 
15 
0 
minus−1.07
0 
16 
2
0.93
0.93 
17 
2
0.93
1.86 
18 
1 
minus−0.07
1.79 
19 
0 
minus−1.07
0.72 
20 
0 
minus−1.07
0 
21 
0 
minus−1.07
0 
22 
1 
minus−0.07
0 
23 
3
1.93
1.93 
24 
2
0.93
2.86 
25 
1 
minus−0.07
2.79 
26 
1 
minus−0.07
2.72 
27 
3
1.93
4.65 
28 
5
3.93
8.58
one might decide to invest in an uninterruptable power supply. It seems intuitively 
clear from Table 3.6 that computer crashes due to power failures become more 
frequent. Is the variability in failure rates due to chance alone (common causes) 
or can it be attributed to special causes that should be investigated? Suppose that 
the computer center can tolerate, at the most, an average of one power failure in 3 
weeks (21 days) or 30/21 = 1.43 crashes per month. It is desirable that there will 
be less than 1 failure per 6 weeks, or 0.71 per month. In Table 3.7, we show the 
computation of Page’s statistics upper S Subscript t Superscript plusS+
t , with upper K Superscript plus Baseline equals one half left parenthesis 0.71 plus 1.43 right parenthesis equals 1.07K+ = 1
2(0.71 + 1.43) = 1.07. 
For alpha equals 0.05α = 0.05, sigma equals 1σ = 1, n equals 1n = 1, we obtain the critical level h Superscript plus Baseline equals 4.16h+ = 4.16. Thus we 
see that the ﬁrst time an alarm is triggered is after the 27th month. In Fig. 3.5, we  
present the graph of upper S Subscript t Superscript plusS+
t versus t. This graph is called a CUSUM Chart. 
We see in Fig. 3.5 that although upper S 6 Superscript plusS+
6 is close to 4, the graph falls back toward zero, 
and there is no alarm triggered until the 27th month.
black medium square■

76
3
Advanced Methods of Statistical Process Control
Fig. 3.5 Page’s CUSUM chart of IPL data 
3.4.2 
Some Theoretical Background 
Generally, if upper X 1 comma upper X 2 comma midline horizontal ellipsisX1, X2, · · · is a sequence of i.i.d. random variables (continuous or 
discrete), having a p.d.f. f left parenthesis x semicolon theta right parenthesisf (x; θ), and we wish to test two simple hypotheses: 
upper H 0 colon theta equals theta 0H0 : θ = θ0 versus upper H 1 colon theta equals theta 1H1 : θ = θ1, with Types I and II error probabilities alphaα and 
betaβ, respectively, the Wald Sequential Probability Ratio Test (SPRT) is a sequential 
procedure that, after t observations, t greater than or equals 1t ≥1, considers the likelihood ratio 
upper Lamda left parenthesis upper X 1 comma midline horizontal ellipsis comma upper X Subscript t Baseline right parenthesis equals product Underscript i equals 1 Overscript t Endscripts StartFraction f left parenthesis upper X Subscript i Baseline semicolon theta 1 right parenthesis Over f left parenthesis upper X Subscript i Baseline semicolon theta 0 right parenthesis EndFraction period	(X1, · · · , Xt) =
t
i=1
f (Xi; θ1)
f (Xi; θ0).
(3.4.4) 
If StartFraction beta Over 1 minus alpha EndFraction less than upper Lamda left parenthesis upper X 1 comma midline horizontal ellipsis comma upper X Subscript t Baseline right parenthesis less than StartFraction 1 minus beta Over alpha EndFraction β
1−α < 	(X1, · · · , Xt) <
1−β
α , then another observation is taken; otherwise, 
sampling terminates. If upper Lamda left parenthesis upper X 1 comma midline horizontal ellipsis comma upper X Subscript t Baseline right parenthesis less than StartFraction beta Over 1 minus alpha EndFraction	(X1, · · · , Xt) <
β
1−α, then upper H 0H0 is accepted. upper H 0H0 is rejected, 
if upper Lamda left parenthesis upper X 1 comma midline horizontal ellipsis comma upper X Subscript t Baseline right parenthesis greater than StartFraction 1 minus beta Over alpha EndFraction	(X1, · · · , Xt) > 1−β
α . 
In an upper control scheme, we can consider only the upper boundary, by setting 
beta equals 0β = 0. Thus, we can decide that the true hypothesis is upper H 1H1, as soon as 
sigma summation Underscript i equals 1 Overscript t Endscripts log StartFraction f left parenthesis upper X Subscript i Baseline semicolon theta 1 right parenthesis Over f left parenthesis upper X Subscript i Baseline semicolon theta 0 right parenthesis EndFraction greater than or equals minus log alpha period
t
i=1
log f (Xi; θ1)
f (Xi; θ0) ≥−log α.
We will examine now the structure of this testing rule in a few special cases.

3.4
Cumulative Sum Control Charts
77
A. Normal Distribution 
We consider upper X Subscript iXi to be normally distributed with known variance sigma squaredσ 2 and mean theta 0θ0 or 
theta 1θ1. In this case, 
StartLayout 1st Row 1st Column log StartFraction f left parenthesis upper X Subscript i Baseline semicolon theta 1 right parenthesis Over f left parenthesis upper X Subscript i Baseline semicolon theta 0 right parenthesis EndFraction 2nd Column equals minus StartFraction 1 Over 2 sigma squared EndFraction left brace left parenthesis upper X Subscript i Baseline minus theta 1 right parenthesis squared minus left parenthesis upper X Subscript i Baseline minus theta 0 right parenthesis squared right brace 2nd Row 1st Column Blank 2nd Column equals StartFraction theta 1 minus theta 0 Over sigma squared EndFraction left parenthesis upper X Subscript i Baseline minus StartFraction theta 0 plus theta 1 Over 2 EndFraction right parenthesis period EndLayout
log f (Xi; θ1)
f (Xi; θ0) = −1
2σ 2 {(Xi −θ1)2 −(Xi −θ0)2}
= θ1 −θ0
σ 2

Xi −θ0 + θ1
2

.
(3.4.5) 
Thus, the criterion 
sigma summation Underscript i equals 1 Overscript t Endscripts log StartFraction f left parenthesis upper X Subscript i Baseline semicolon theta 1 right parenthesis Over f left parenthesis upper X Subscript i Baseline semicolon theta 0 right parenthesis EndFraction greater than or equals minus log alpha
t
i=1
log f (Xi; θ1)
f (Xi; θ0) ≥−log α
is equivalent to 
sigma summation Underscript i equals 1 Overscript t Endscripts left parenthesis upper X Subscript i Baseline minus StartFraction theta 0 plus theta 1 Over 2 EndFraction right parenthesis greater than or equals minus StartFraction sigma squared log alpha Over theta 1 minus theta 0 EndFraction period
t
i=1

Xi −θ0 + θ1
2

≥−σ 2 log α
θ1 −θ0
.
For this reason, we use in the upper Page control scheme 
upper K Superscript plus Baseline equals StartFraction theta 0 plus theta 1 Over 2 EndFraction comma and h Superscript plus Baseline equals minus StartFraction sigma squared log alpha Over theta 1 minus theta 0 EndFraction periodK+ = θ0 + θ1
2
,
and
h+ = −σ 2 log α
θ1 −θ0
.
If upper X Subscript tXt is an average of n independent observations, then we replace sigma squaredσ 2 by sigma squared divided by nσ 2/n. 
B. Binomial Distributions 
Suppose that upper X Subscript tXt has a binomial distribution upper B left parenthesis n comma theta right parenthesisB(n, θ). If  theta less than or equals theta 0θ ≤θ0, the process level is 
under control. If theta greater than or equals theta 1θ ≥θ1, the process level is out of control left parenthesis theta 1 greater than theta 0 right parenthesis(θ1 > θ0). Since 
StartLayout 1st Row f left parenthesis x semicolon theta right parenthesis equals StartBinomialOrMatrix n Choose x EndBinomialOrMatrix left parenthesis StartFraction theta Over 1 minus theta EndFraction right parenthesis Superscript x Baseline left parenthesis 1 minus theta right parenthesis Superscript n Baseline comma 2nd Row sigma summation Underscript i equals 1 Overscript t Endscripts log StartFraction f left parenthesis upper X Subscript i Baseline semicolon theta 1 right parenthesis Over f left parenthesis upper X Subscript i Baseline semicolon theta 0 right parenthesis EndFraction greater than or equals minus log alpha if comma 3rd Row sigma summation Underscript i equals 1 Overscript t Endscripts left parenthesis upper X Subscript i Baseline minus StartStartFraction n log left parenthesis StartFraction 1 minus theta 0 Over 1 minus theta 1 EndFraction right parenthesis OverOver log left parenthesis StartFraction theta 1 Over 1 minus theta 1 EndFraction dot StartFraction 1 minus theta 0 Over theta 0 EndFraction right parenthesis EndEndFraction right parenthesis greater than or equals minus StartStartFraction log alpha OverOver log left parenthesis StartFraction theta 1 Over 1 minus theta 1 EndFraction dot StartFraction 1 minus theta 0 Over theta 0 EndFraction right parenthesis EndEndFraction period EndLayout
f (x; θ) =
n
x
 
θ
1 −θ
x
(1 −θ)n,
t
i=1
log f (Xi; θ1)
f (Xi; θ0) ≥−log α if,
t
i=1
⎛
⎝Xi −
n log

1−θ0
1−θ1

log

θ1
1−θ1 · 1−θ0
θ0

⎞
⎠≥−
log α
log

θ1
1−θ1 · 1−θ0
θ0
.
(3.4.6)

78
3
Advanced Methods of Statistical Process Control
Accordingly, in an upper Page’s control scheme, with binomial data, we use 
upper K Superscript plus Baseline equals StartStartFraction n log left parenthesis StartFraction 1 minus theta 0 Over 1 minus theta 1 EndFraction right parenthesis OverOver log left parenthesis StartFraction theta 1 Over 1 minus theta 1 EndFraction dot StartFraction 1 minus theta 0 Over theta 0 EndFraction right parenthesis EndEndFractionK+ =
n log

1−θ0
1−θ1

log

θ1
1−θ1 · 1−θ0
θ0

(3.4.7) 
and 
h Superscript plus Baseline equals minus StartStartFraction log alpha OverOver log left parenthesis StartFraction theta 1 Over 1 minus theta 1 EndFraction dot StartFraction 1 minus theta 0 Over theta 0 EndFraction right parenthesis EndEndFraction periodh+ = −
log α
log

θ1
1−θ1 · 1−θ0
θ0
.
(3.4.8) 
C. Poisson Distributions 
When the statistics upper X Subscript tXt have Poisson distribution with mean lamdaλ, then for speciﬁed 
levels lamda 0λ0 and lamda 1λ1, 0 less than lamda 0 less than lamda 1 less than normal infinity0 < λ0 < λ1 < ∞, 
sigma summation Underscript i equals 1 Overscript t Endscripts log StartFraction f left parenthesis upper X Subscript i Baseline semicolon lamda 1 right parenthesis Over f left parenthesis upper X Subscript i Baseline semicolon lamda 0 right parenthesis EndFraction equals log left parenthesis StartFraction lamda 1 Over lamda 0 EndFraction right parenthesis sigma summation Underscript i equals 1 Overscript t Endscripts upper X Subscript i Baseline minus t left parenthesis lamda 1 minus lamda 0 right parenthesis period
t
i=1
log f (Xi; λ1)
f (Xi; λ0) = log
λ1
λ0

t
i=1
Xi −t(λ1 −λ0).
(3.4.9) 
It follows that the control parameters are 
upper K Superscript plus Baseline equals StartFraction lamda 1 minus lamda 0 Over log left parenthesis lamda 1 divided by lamda 0 right parenthesis EndFractionK+ =
λ1 −λ0
log(λ1/λ0)
(3.4.10) 
and 
h Superscript plus Baseline equals minus StartFraction log alpha Over log left parenthesis lamda 1 divided by lamda 0 right parenthesis EndFraction periodh+ = −
log α
log(λ1/λ0).
(3.4.11) 
3.4.3 
Lower and Two-Sided Page’s Scheme 
In order to test whether a signiﬁcant drop occurred in the process level (mean), we 
can use a lower page scheme. According to this scheme, we set upper S 0 Superscript minus Baseline identical to 0S−
0 ≡0 and 
upper S Subscript t Superscript minus Baseline equals min left brace upper S Subscript t minus 1 Superscript minus Baseline plus left parenthesis upper X Subscript t Baseline minus upper K Superscript minus Baseline right parenthesis comma 0 right brace comma t equals 1 comma 2 comma midline horizontal ellipsis periodS−
t = min{S−
t−1 + (Xt −K−), 0},
t = 1, 2, · · · .
(3.4.12) 
Here the CUSUM values upper S Subscript t Superscript minusS−
t
are either zero or negative. We decide that a shift 
down in the process level, from theta 0θ0 to theta 1θ1, theta 1 less than theta 0θ1 < θ0, occurred as soon as upper S Subscript t Superscript minus Baseline less than h Superscript minusS−
t
< h−. 
The control parameters upper K Superscript minusK−and h Superscript minush−are determined by the formula of the previous 
section by setting theta 1 less than theta 0θ1 < θ0.

3.4
Cumulative Sum Control Charts
79
Fig. 3.6 The number of yearly coal mine disasters in England 
Example 3.4 In dataset COAL.csv, one can ﬁnd data on the number of coal mine 
disasters (explosions) in England, per year, for the period 1850 to 1961. These data 
are plotted in Fig. 3.6. It seems that the average number of disasters per year dropped 
after 40 years from 3 to 2 and later settled around an average of one per year. We 
apply here the lower Page’s scheme to see when do we detect this change for the 
ﬁrst time. It is plausible to assume that the number of disasters per year, upper X Subscript tXt, is a  
random variable having a Poisson distribution. We therefore set lamda 0 equals 3λ0 = 3 and lamda 1 equals 1λ1 = 1. 
The formulae of the previous section, with upper K Superscript plusK+ and h Superscript plush+ replaced by upper K Superscript minusK−and h Superscript minush−, 
yield, for alpha equals 0.01α = 0.01, 
upper K Superscript minus Baseline equals StartFraction lamda 1 minus lamda 0 Over log left parenthesis lamda 1 divided by lamda 0 right parenthesis EndFraction equals 1.82 and h Superscript minus Baseline equals minus StartFraction log left parenthesis 0.01 right parenthesis Over log left parenthesis 1 divided by 3 right parenthesis EndFraction equals negative 4.19 periodK−=
λ1 −λ0
log(λ1/λ0) = 1.82
and
h−= −log(0.01)
log(1/3) = −4.19.
In Table 3.8, we ﬁnd the values of upper X Subscript tXt upper X Subscript t Baseline minus upper K Superscript minusXt −K−and upper S Subscript t Superscript minusS−
t for t equals 1 comma midline horizontal ellipsis comma 50t = 1, · · · , 50. We  
see that upper S Subscript t Superscript minus Baseline less than h Superscript minusS−
t < h−for the ﬁrst time at t equals 47t = 47. The graph of upper S Subscript t Superscript minusS−
t versus t is plotted in 
Fig. 3.7. 
black medium square■
If we wish to control simultaneously against changes in the process level in either 
upward or downward directions, we use an upper and lower Page’s schemes together 
and trigger an alarm as soon as either upper S Subscript t Superscript plus Baseline greater than h Superscript plusS+
t
> h+ or upper S Subscript t Superscript minus Baseline less than h Superscript minusS−
t
< h−. Such a two-sided 
scheme is denoted by the four control parameters left parenthesis upper K Superscript plus Baseline comma h Superscript plus Baseline comma upper K Superscript minus Baseline comma h Superscript minus Baseline right parenthesis(K+, h+, K−, h−). 
Example 3.5 Yashchin (1991) illustrates the use of a two-sided Page’s control 
scheme on data, which are the difference between the thickness of the grown silicon 
layer and its target value. He applied the control scheme left parenthesis upper K Superscript plus Baseline equals 3(K+ = 3, h Superscript plus Baseline equals 9h+ = 9,

80
3
Advanced Methods of Statistical Process Control
Table 3.8 Page’s lower 
control scheme for the coal 
mine disasters data 
t
upper X Subscript tXt
upper X Subscript t Baseline minus upper K Superscript minusXt −K−
upper S Subscript t Superscript minusS−
t
1 
3
1.179
0 
2 
6
4.179
0 
3 
4
2.179
0 
4 
0 
minus−1.820
minus−1.820 
5 
0 
minus−1.820
minus−3.640 
6 
5
3.179
minus−0.461 
7 
4
2.179
0 
8 
2
0.179
0 
9 
2
0.179
0 
10 
5
3.179
0 
11 
3
1.179
0 
12 
3
1.179
0 
13 
3
1.179
0 
14 
0 
minus−1.820
minus−1.820 
15 
3
1.179
minus−0.640 
16 
5
3.179
0 
17 
3
1.179
0 
18 
3
1.179
0 
19 
6
4.179
0 
20 
6
4.179
0 
21 
3
1.179
0 
22 
3
1.179
0 
23 
0 
minus−1.820
minus−1.820 
24 
4
2.179
0 
25 
4
2.179
0 
26 
3
1.179
0 
27 
3
1.179
0 
28 
7
5.179
0 
29 
2
0.179
0 
30 
4
2.179
0 
31 
2
0.179
0 
32 
4
2.179
0 
33 
3
1.179
0 
34 
2
0.179
0 
35 
2
0.179
0 
36 
5
3.179
0 
37 
1 
minus−0.820
minus−0.820 
38 
2
0.179
minus−0.640 
39 
3
1.179
0 
40 
1 
minus−0.820
minus−0.820
(continued)

3.4
Cumulative Sum Control Charts
81
Table 3.8 (continued)
t
upper X Subscript tXt
upper X Subscript t Baseline minus upper K Superscript minusXt −K−
upper S Subscript t Superscript minusS−
t
41 
2
0.179
minus−0.640 
42 
1 
minus−0.820
minus−1.461 
43 
1 
minus−0.820
minus−2.281 
44 
1 
minus−0.820
minus−3.102 
45 
2
0.179
minus−2.922 
46 
2
0.179
minus−2.743 
47 
0 
minus−1.820
minus−4.563 
48 
0 
minus−1.820
minus−6.384 
49 
1 
minus−0.820
minus−7.204 
50 
0 
minus−1.820
minus−9.025 
Fig. 3.7 Page’s lower CUSUM control chart
upper K Superscript minus Baseline equals negative 2K−= −2, h Superscript minus Baseline equals negative 5 right parenthesish−= −5). We present the values of upper X Subscript tXt, upper S Subscript t Superscript plusS+
t , and upper S Subscript t Superscript minusS−
t
in Table 3.9. 
We see in this table that upper S Subscript t Superscript plus Baseline greater than h Superscript plusS+
t > h+ for the ﬁrst time at t equals 40t = 40. There is an indication 
that a signiﬁcant drift upward in the level of thickness occurred. 
In Fig. 3.8, we present the two-sided control chart for the data of Table 3.9. 
black medium square■
The two-sided Page’s control scheme can be boosted by changing the values of 
upper S 0 Superscript plusS+
0 and upper S 0 Superscript minusS−
0 to non-zero. These are called headstart values. The introduction of 
non-zero headstarts was suggested by Lucas and Crosier (1982) in order to bring 
the history of the process into consideration and accelerate the initial response of 
the scheme. Lucas (1982) suggested also to combine the CUSUM scheme with the 
Shewhart Control Chart. If any upper X Subscript tXt value exceeds an upper limit UCL, or falls below 
a lower limit LCL, an alarm should be triggered.

82
3
Advanced Methods of Statistical Process Control
Table 3.9 Computation of 
left parenthesis upper S Subscript t Superscript plus Baseline comma upper S Subscript t Superscript minus Baseline right parenthesis(S+
t , S−
t ) in a two-sided 
control scheme 
t
upper X Subscript tXt
upper X Subscript t Baseline minus upper K Superscript plusXt −K+
upper X Subscript t Baseline minus upper K Superscript minusXt −K−
upper S Subscript t Superscript plusS+
t
upper S Subscript t Superscript minusS−
t
1 
minus−4
minus−7
minus−2
0
minus−2 
2 
minus−1
minus−4
1
0
minus−1 
3 
3
0
5
0
0 
4 
minus−2
minus−5
0
0
0 
5 
minus−2.5 
minus−5.5
minus−0.5
0
minus−0.5 
6 
minus−0.5 
minus−3.5
1.5
0
0 
7 
1.5
minus−1.5
3.5
0
0 
8 
minus−3
minus−6
minus−1
0
minus−1 
9 
4
1
6
1
0 
10 
3.5
0.5
5.5
1.5 
0 
11 
minus−2.5 
minus−5.5
minus−0.5
0
minus−0.5 
12 
minus−3
minus−6
minus−1
0
minus−1.5 
13 
minus−3
minus−6
minus−1
0
minus−2.5 
14 
minus−0.5 
minus−3.5
1.5
0
minus−1 
15 
minus−2.5 
minus−5.5
minus−0.5
0
minus−1.5 
16 
1
minus−2
3
0
0 
17 
minus−1
minus−4
1
0
0 
18 
minus−3
minus−6
minus−1
0
minus−1 
19 
1
minus−2
3
0
0 
20 
4.5
minus−2
6.5
1.5 
0 
21 
minus−3.5 
minus−6.5
minus−1.5
0
minus−1.5 
22 
minus−3
minus−6
minus−1
0
minus−2.5 
23 
minus−1
minus−4
1
0
minus−1.5 
24 
4
1
6
1
0 
25 
minus−0.5 
minus−3.5
1.5
0
0 
26 
minus−2.5 
minus−5.5
minus−0.5
0
minus−0.5 
27 
4
1
6
1
0 
28 
minus−2
minus−5
0
0
0 
29 
minus−3
minus−6
minus−1
0
minus−1 
30 
minus−1.5 
minus−4.5
0.5
0
minus−0.5 
31 
4
1
6
1
0 
32 
2.5
minus−0.5
4.5
0.5 
0 
33 
minus−0.5 
minus−3.5
1.5
0
0 
34 
7
4
9
4
0 
35 
5
2
7
6
0 
36 
4
1
6
7
0 
37 
4.5
1.5
6.5
8.5 
0 
38 
2.5
minus−0.5
4.5
8
0 
39 
2.5
minus−0.5
4.5
7.5 
0 
40 
5
2
3
9.5 
0

3.4
Cumulative Sum Control Charts
83
Fig. 3.8 CUSUM two-sided control chart for thickness difference, control parameters (upper K Superscript plus Baseline equals 3K+ = 3, 
h Superscript plus Baseline equals 9h+ = 9, upper K Superscript minus Baseline equals negative 2K−= −2, h Superscript minus Baseline equals negative 5h−= −5)
3.4.4 
Average Run Length, Probability of False Alarm, and 
Conditional Expected Delay 
The run length (RL) is deﬁned as the number of time units until either upper S Subscript t Superscript plus Baseline greater than h Subscript t Superscript plusS+
t
> h+
t
or upper S Subscript t Superscript minus Baseline less than h Superscript minusS−
t
< h−, for the ﬁrst time. We have seen already that the average run length 
(ARL) is an important characteristic of a control procedure, when there is either no 
change in the mean level (ARL(0)), or the mean level has shifted to mu 1 equals mu 0 plus delta sigmaμ1 = μ0 + δσ, 
before the control procedure started (ARL(deltaδ)). When the shift from mu 0μ0 to mu 1μ1 occurs 
at some change-point tauτ, tau greater than 0τ > 0, then we would like to know what is the probability of 
false alarm, i.e., that the run length is smaller than tauτ, and the conditional expected 
run length, given that RL greater than tauRL > τ. It is difﬁcult to compute these characteristics of 
the Page control scheme analytically. The theory required for such an analysis is 
quite complicated (see Yashchin 1985). We provide Python methods in the mistat 
package that approximate these characteristics numerically by simulation. 
The method cusumArl computes the average run length, ARL, and cusumPfaCed 
returns the probability of false alarm, FPA, and conditional expected delay, CED, 
for a given distribution, e.g., normal, binomial, or Poisson. 
In Table 3.10, we present estimates of the ARLleft parenthesis delta right parenthesis(δ) for the normal distribution, 
with NR = 100 runs. S.E. = standard deviation(RL)/StartRoot NR EndRoot
√
NR.

84
3
Advanced Methods of Statistical Process Control
Table 3.10 ARLleft parenthesis delta right parenthesis(δ)
estimates for the normal 
distribution, mu equals deltaμ = δ, sigma equals 1σ = 1
NR = 100, left parenthesis upper K Superscript plus Baseline equals 1(K+ = 1, h Superscript plus Baseline equals 3h+ = 3, 
upper K Superscript minus Baseline equals negative 1K−= −1, h Superscript minus Baseline equals negative 3h−= −3) 
deltaδ
ARL
2 asterisk2∗S.E. 
0
1225.0 
230.875 
0.5 
108.0 
22.460 
1.0
18.7
3.393 
1.5
7.1
0.748 
results = [] 
for loc in (0, 0.5, 1.0, 1.5): 
arl = mistat.cusumArl(randFunc=stats.norm(loc=loc), N=100, 
limit=10_000, seed=100, verbose=False) 
results.append({ 
'theta': loc, 
'ARL': arl['statistic']['ARL'], 
'2 S.E.': 2 * arl['statistic']['Std. Error'], 
}) 
print(pd.DataFrame(results)) 
theta
ARL
2 S.E. 
0
0.0 
978.71 
294.600536 
1
0.5 
126.66
36.962435 
2
1.0
16.54
4.265489 
3
1.5
5.86
1.386506 
Program cusumArl can also be used to determine the values of the control 
parameters h Superscript plush+ and h Superscript minush−so that a certain ARL(0) is attained. For example, if we 
use the Shewhart 3-sigma control charts for the sample means in the normal case, 
the probability that, under no shift in the process level, a point will fall outside the 
control limits is 0.0026, and ARL(0) = 385. Suppose we wish to devise a two-sided 
CUSUM control scheme, when mu 0 equals 10μ0 = 10, sigma equals 5σ = 5, mu 1 Superscript plus Baseline equals 14μ+
1 = 14, and mu 1 Superscript minus Baseline equals 6μ−
1 = 6. We obtain 
upper K Superscript plus Baseline equals 12K+ = 12 and upper K Superscript minus Baseline equals 8K−= 8. If we take alpha equals 0.01α = 0.01, we obtain h Superscript plus Baseline equals StartFraction negative 25 times log left parenthesis 0.01 right parenthesis Over 4 EndFraction equals 28.78h+ = −25×log(0.01)
4
= 28.78. 
Program cusumArl yields, for the parameters mu equals 10μ = 10, sigma equals 5σ = 5, upper K Superscript plus Baseline equals 12K+ = 12, h Superscript plus Baseline equals 29h+ = 29, 
upper K Superscript minus Baseline equals 8K−= 8, h Superscript minus Baseline equals negative 29h−= −29, the estimate ARL(0) = 411 plus or minus± 33.6. If we use alpha equals 0.05α = 0.05, we  
obtain h Superscript plus Baseline equals 18.72h+ = 18.72. Under the control parameters (12, 18.7, 8, negative 18.7−18.7), we obtain 
ARL(0) = 70.7 plus or minus± 5.5. We can now run the program for several h Superscript plus Baseline equals minus h Superscript minush+ = −h−values 
to obtain an ARL(0) estimate close to 385. The value in Fig. 3.9 ARL is 411.4 with 
an SE of 33.6.
for h in (18.7, 28, 28.5, 28.6, 28.7, 29, 30): 
arl = mistat.cusumArl(randFunc=stats.norm(loc=10, scale=5), 
N=300, limit=7000, seed=1, kp=12, km=8, hp=h, hm=-h, 
verbose=False) 
print(f"h {h:5.1f}: ARL(0) {arl['statistic']['ARL']:5.1f} ", 
f"+/- {arl['statistic']['Std. Error']:4.1f}") 
h 
18.7: ARL(0) 
70.7 
+/-
5.5 
h 
28.0: ARL(0) 363.2 
+/- 30.0 
h 
28.5: ARL(0) 387.7 
+/- 31.5 
h 
28.6: ARL(0) 394.7 
+/- 32.4 
h 
28.7: ARL(0) 397.3 
+/- 32.5 
h 
29.0: ARL(0) 411.4 
+/- 33.6 
h 
30.0: ARL(0) 484.0 
+/- 42.0 
Thus, h Superscript plus Baseline equals 29h+ = 29 would yield a control scheme having an ARL(0) close to that of 
a Shewhart 3 sigma3σ scheme.

3.4
Cumulative Sum Control Charts
85
Fig. 3.9 Histogram of RL for mu equals 10μ = 10, sigma equals 5σ = 5, upper K Superscript plus Baseline equals 12K+ = 12, h Superscript plus Baseline equals 29h+ = 29, upper K Superscript minus Baseline equals 8K−= 8, h Superscript minus Baseline equals negative 29h−= −29
The function cusumArl can also compute the estimates of the ARLleft parenthesis delta right parenthesis(δ) for the 
binomial distribution. To illustrate, consider the case of the binomial distribution 
upper B left parenthesis n comma theta right parenthesisB(n, θ) with n equals 100n = 100, theta equals 0.05θ = 0.05. A two-sided Page’s control scheme, protecting 
against a shift above theta 1 Superscript plus Baseline equals 0.07θ+
1 = 0.07 or below theta 1 Superscript minus Baseline equals 0.03θ−
1 = 0.03, can use the control parameters 
upper K Superscript plus Baseline equals 5.95K+ = 5.95, h Superscript plus Baseline equals 12.87h+ = 12.87, upper K Superscript minus Baseline equals 3.92K−= 3.92, and h Superscript minus Baseline equals negative 8.66h−= −8.66. 
results = [] 
for p in (0.05, 0.06, 0.07): 
arl = mistat.cusumArl(randFunc=stats.binom(n=100, p=p), N=100, limit=2000, 
seed=1, kp=5.95, km=3.92, hp=12.87, hm=-8.66) 
results.append({ 
'p': p, 
'delta': p/0.05, 
'ARL': arl['statistic']['ARL'], 
'2 S.E.': 2 * arl['statistic']['Std. Error'], 
}) 
print(pd.DataFrame(results)) 
p 
delta
ARL
2 S.E. 
0 
0.05
1.0 
291.71 
78.261710 
1 
0.06
1.2
41.00 
10.528704 
2 
0.07
1.4
11.78
2.636513 
The program cusumArl yields, for NR = 100 runs, the estimate ARL(0) = 291.7 
plus or minus± 39.13. Furthermore, for delta equals theta 1 divided by theta 0δ = θ1/θ0, we obtain for the same control scheme 
ARL left parenthesis six fifths right parenthesis equals 41.0 plus or minus 10.53 comma ARL left parenthesis seven fifths right parenthesis equals 11.8 plus or minus 2.64 periodARL
6
5

= 41.0 ± 10.53,
ARL
7
5

= 11.8 ± 2.64.
Similarly, program cusumArl can be used to estimate the ARLleft parenthesis delta right parenthesis(δ) in the Poisson 
case. For example, suppose that upper X Subscript tXt has a Poisson distribution with mean lamda 0 equals 10λ0 = 10. 
We wish to control the process against shifts in lamdaλ greater than lamda 1 Superscript plus Baseline equals 15λ+
1 = 15 or smaller

86
3
Advanced Methods of Statistical Process Control
Table 3.11 Estimates of PFA and CED, normal distribution mu 0 equals 0μ0 = 0, sigma equals 1σ = 1, control parameters 
left parenthesis upper K Superscript plus Baseline equals 1(K+ = 1, h Superscript plus Baseline equals 3h+ = 3, upper K Superscript minus Baseline equals negative 1K−= −1, h Superscript minus Baseline equals negative 3 right parenthesish−= −3), tau equals 100τ = 100, NR equals 500= 500
deltaδ
PFA
CED 
0.5
0.08
109.2 plus or minus± 21.7 
1
0.08
15.66 plus or minus± 12.0 
1.5
0.08
4.87 plus or minus± 10.9 
than lamda 1 Superscript minus Baseline equals 7λ−
1 = 7. We use the control parameters upper K Superscript plus Baseline equals 12.33K+ = 12.33, h Superscript plus Baseline equals 11.36h+ = 11.36, upper K Superscript minus Baseline equals 8.41K−= 8.41, 
and h Superscript minus Baseline equals negative 12.91h−= −12.91. 
arl = mistat.cusumArl(randFunc=stats.poisson(mu=10), N=100, limit=2000, seed=1, 
kp=12.33, km=8.41, hp=11.36, hm=-12.91) 
arl['statistic'] 
{'ARL': 289.6363636363636, 'Std. Error': 43.223677106723144} 
The obtained estimate is ARL(0) = 289.6 plus or minus± 43.224. 
We can use now program cusumPfaCed to estimate the probability of false 
alarm, PFA, and the conditional expected delay if a change in the mean of magnitude 
delta sigmaδσ occurs at time tauτ. In Table 3.11, we present some estimates obtained from this 
method. 
results = [] 
for loc in (0.5, 1.0, 1.5): 
pfaced = mistat.cusumPfaCed(randFunc1=stats.norm(), 
randFunc2=stats.norm(loc=loc), 
tau=100, N=100, limit=1_000, seed=1, 
verbose=False) 
results.append({ 
'theta': loc, 
'PFA': pfaced['statistic']['PFA'], 
'CED': pfaced['statistic']['CED'], 
'S.E.': pfaced['statistic']['Std. Error'], 
}) 
3.5 
Bayesian Detection 
The Bayesian approach to the problem of detecting changes in distributions can be 
described in the following terms. Suppose that we decide to monitor the stability 
of a process with a statistic T , having a distribution with p.d.f. f Subscript upper T Baseline left parenthesis t semicolon bold italic theta right parenthesisfT (t; θ), where 
bold italic thetaθ designates the parameters on which the distribution depends (process mean, 
variance, etc.). The statistic T could be the mean, upper X overbar ¯X, of a random sample of size 
n, the sample standard deviation, S, or the proportion defectives in the sample. 
A sample of size n is drawn from the process at predetermined epochs. Let upper T Subscript iTi
left parenthesis i equals 1 comma 2 comma midline horizontal ellipsis right parenthesis(i = 1, 2, · · · ) denote the monitoring statistic at the ith epoch. Suppose that m 
such samples were drawn and that the statistics upper T 1 comma upper T 2 comma midline horizontal ellipsis comma upper T Subscript m BaselineT1, T2, · · · , Tm are independent. Let

3.5
Bayesian Detection
87
tau equals 0 comma 1 comma 2 comma midline horizontal ellipsisτ = 0, 1, 2, · · · denote the location of the point of change in the process parameter 
bold italic theta 0θ0, to bold italic theta 1 equals bold italic theta 0 plus bold italic upper Deltaθ1 = θ0 + . tauτ is called the change-point of bold italic theta 0θ0. The event left brace tau equals 0 right brace{τ = 0} signiﬁes 
that all the n samples have been drawn after the change-point. The event left brace tau equals i right brace{τ = i}, 
for i equals 1 comma midline horizontal ellipsis comma m minus 1i = 1, · · · , m −1, signiﬁes that the change-point occurred between the ith and 
left parenthesis i plus 1 right parenthesis(i+1)st sampling epoch. Finally, the event left brace tau equals m Superscript plus Baseline right brace{τ = m+} signiﬁes that the change-point 
has not occurred before the ﬁrst m sampling epochs. 
Given upper T 1 comma midline horizontal ellipsis comma upper T Subscript m BaselineT1, · · · , Tm, the  likelihood function of tauτ, for speciﬁed values of bold italic theta 0θ0 and 
bold italic theta 1θ1, is deﬁned as 
upper L Subscript m Baseline left parenthesis tau semicolon upper T 1 comma midline horizontal ellipsis comma upper T Subscript m Baseline right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column product Underscript i equals 1 Overscript m Endscripts f left parenthesis upper T Subscript i Baseline semicolon bold italic theta 1 right parenthesis comma 2nd Column if tau equals 0 2nd Row 1st Column Blank 3rd Row 1st Column product Underscript i equals 1 Overscript tau Endscripts f left parenthesis upper T Subscript i Baseline comma bold italic theta 0 right parenthesis product Underscript j equals tau plus 1 Overscript m Endscripts f left parenthesis upper T Subscript j Baseline semicolon bold italic theta 1 right parenthesis comma 2nd Column if 1 less than or equals tau less than or equals m minus 1 comma 4th Row 1st Column Blank 5th Row 1st Column product Underscript i equals 1 Overscript m Endscripts f left parenthesis upper T Subscript i Baseline semicolon bold italic theta 0 right parenthesis comma 2nd Column if tau equals m Superscript plus Baseline period EndLayoutLm(τ; T1, · · · , Tm) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
m
i=1 f (Ti; θ1),
if τ = 0
τ
i=1 f (Ti, θ0) m
j=τ+1 f (Tj; θ1),
if 1 ≤τ ≤m −1,
m
i=1 f (Ti; θ0),
if τ = m+.
(3.5.1) 
A maximum likelihood estimator of tauτ, given  upper T 1 comma midline horizontal ellipsis comma upper T Subscript m BaselineT1, · · · , Tm, is the argument maxi-
mizing upper L Subscript m Baseline left parenthesis tau semicolon upper T 1 comma midline horizontal ellipsis comma upper T Subscript m Baseline right parenthesisLm(τ; T1, · · · , Tm). 
In the Bayesian framework, the statistician gives the various possible values of tauτ
non-negative weights, which reﬂect his belief where the change-point could occur. 
High weight expresses higher conﬁdence. In order to standardize the approach, we 
will assume that the sum of all weights is one, and we call these weights, the prior 
probabilities of tauτ. Let  pi left parenthesis tau right parenthesisπ(τ), tau equals 0 comma 1 comma 2 comma midline horizontal ellipsisτ = 0, 1, 2, · · · , denote the prior probabilities of tauτ. 
If the occurrence of the change-point is a realization of some random process, the 
following modiﬁed-geometric prior distribution could be used 
pi Subscript m Baseline left parenthesis tau right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column pi comma 2nd Column if tau equals 0 2nd Row 1st Column Blank 3rd Row 1st Column left parenthesis 1 minus pi right parenthesis p left parenthesis 1 minus p right parenthesis Superscript i minus 1 Baseline comma 2nd Column if 1 less than or equals tau less than or equals m minus 1 4th Row 1st Column Blank 5th Row 1st Column left parenthesis 1 minus pi right parenthesis left parenthesis 1 minus p right parenthesis Superscript m minus 1 Baseline comma 2nd Column if tau equals m Superscript plus Baseline comma EndLayoutπm(τ) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
π,
if τ = 0
(1 −π)p(1 −p)i−1,
if 1 ≤τ ≤m −1
(1 −π)(1 −p)m−1,
if τ = m+,
(3.5.2) 
where 0 less than pi less than 10 < π < 1, 0 less than p less than 10 < p < 1 are prior parameters. Applying Bayes formula, 
we convert the prior probabilities pi left parenthesis t right parenthesisπ(t) after observing upper T 1 comma midline horizontal ellipsis comma upper T Subscript m BaselineT1, · · · , Tm into posterior 
probabilities. Let  pi Subscript mπm denote the posterior probability of the event left brace tau less than or equals m right brace{τ ≤m}, given  
upper T 1 comma midline horizontal ellipsis comma upper T Subscript m BaselineT1, · · · , Tm. Using the above modiﬁed-geometric prior distribution, and employing 
Bayes theorem, we obtain the formula 
pi Subscript m Baseline equals StartStartFraction StartFraction pi Over left parenthesis 1 minus pi right parenthesis left parenthesis 1 minus p right parenthesis Superscript m minus 1 Baseline EndFraction product Underscript j equals 1 Overscript m Endscripts upper R Subscript j Baseline plus StartFraction p Over left parenthesis 1 minus p right parenthesis Superscript m minus 1 Baseline EndFraction sigma summation Underscript i equals 1 Overscript m minus 1 Endscripts left parenthesis 1 minus p right parenthesis Superscript i minus 1 Baseline product Underscript j equals i plus 1 Overscript m Endscripts upper R Subscript j Baseline OverOver StartFraction pi Over left parenthesis 1 minus pi right parenthesis left parenthesis 1 minus p right parenthesis Superscript m minus 1 Baseline EndFraction product Underscript j equals 1 Overscript m Endscripts upper R Subscript j Baseline plus StartFraction p Over left parenthesis 1 minus p right parenthesis Superscript m minus 1 Baseline EndFraction sigma summation Underscript i equals 1 Overscript m minus 1 Endscripts left parenthesis 1 minus p right parenthesis Superscript i minus 1 Baseline product Underscript j equals i plus 1 Overscript m Endscripts upper R Subscript j Baseline plus 1 EndEndFraction commaπm =
π
(1−π)(1−p)m−1
m
j=1 Rj +
p
(1−p)m−1
m−1
i=1 (1 −p)i−1 m
j=i+1 Rj
π
(1−π)(1−p)m−1
m
j=1 Rj +
p
(1−p)m−1
m−1
i=1 (1 −p)i−1 m
j=i+1 Rj + 1
,
(3.5.3) 
where

88
3
Advanced Methods of Statistical Process Control
upper R Subscript j Baseline equals StartFraction f left parenthesis upper T Subscript j Baseline semicolon bold italic theta 1 right parenthesis Over f left parenthesis upper T Subscript j Baseline semicolon bold italic theta 0 right parenthesis EndFraction comma j equals 1 comma 2 comma midline horizontal ellipsisRj = f (Tj; θ1)
f (Tj; θ0),
j = 1, 2, · · ·
(3.5.4) 
A Bayesian detection of a change-point is a procedure that detects a change as soon 
as pi Subscript m Baseline greater than or equals pi Superscript asteriskπm ≥π∗, where pi Superscript asteriskπ∗is a value in left parenthesis 0 comma 1 right parenthesis(0, 1), close to 1. 
The above procedure can be simpliﬁed, if we believe that the monitoring starts 
when bold italic theta equals bold italic theta 0θ = θ0 (i.e., pi equals 0 right parenthesisπ = 0) and p is very small, we can represent pi Subscript mπm then, 
approximately, by 
pi overTilde Subscript m Baseline equals StartFraction sigma summation Underscript i equals 1 Overscript m minus 1 Endscripts product Underscript j equals i plus 1 Overscript m Endscripts upper R Subscript j Baseline Over sigma summation Underscript i equals 1 Overscript m minus 1 Endscripts product Underscript j equals i plus 1 Overscript m Endscripts upper R Subscript j Baseline plus 1 EndFraction period ˜πm =
m−1
i=1
m
j=i+1 Rj
m−1
i=1
m
j=i+1 Rj + 1
.
(3.5.5) 
The statistic 
upper W Subscript m Baseline equals sigma summation Underscript i equals 1 Overscript m minus 1 Endscripts product Underscript j equals i plus 1 Overscript m Endscripts upper R Subscript jWm =
m−1

i=1
m

j=i+1
Rj
(3.5.6) 
is called the Shiryaev–Roberts (S.R.) statistic. Notice that pi overTilde Subscript m Baseline greater than or equals pi Superscript asterisk ˜πm ≥π∗if upper W Subscript m Baseline greater than or equals StartFraction pi Superscript asterisk Baseline Over 1 minus pi Superscript asterisk Baseline EndFractionWm ≥
π∗
1−π∗. 
StartFraction pi Superscript asterisk Baseline Over 1 minus pi Superscript asterisk Baseline EndFraction π∗
1−π∗is called the stopping threshold. Thus, for example, if the Bayes procedure 
is to “ﬂag” a change as soon as pi overTilde Subscript m Baseline greater than or equals 0.95 ˜πm ≥0.95, the procedure that “ﬂags” as soon as 
upper W Subscript m Baseline greater than or equals 19Wm ≥19 is equivalent. 
We illustrate now the use of the S.R. statistic in the special case of monitoring 
the mean theta 0θ0 of a process. The statistic T is the sample mean, upper X overbar Subscript n ¯Xn, based on a sample 
of n observations. We will assume that upper X overbar Subscript n ¯Xn has a normal distribution upper N left parenthesis theta 0 comma StartFraction sigma Over StartRoot n EndRoot EndFraction right parenthesisN

θ0, σ
√n

, 
and at the change-point, theta 0θ0 shifts to theta 1 equals theta 0 plus delta sigmaθ1 = θ0 +δσ. It is straightforward to verify that 
the likelihood ratio is 
upper R Subscript j Baseline equals exp left brace minus StartFraction n delta squared Over 2 sigma squared EndFraction plus StartFraction n delta Over sigma squared EndFraction left parenthesis upper X overbar Subscript j Baseline minus theta 0 right parenthesis right brace comma j equals 1 comma 2 comma midline horizontal ellipsis periodRj = exp
	
−nδ2
2σ 2 + nδ
σ 2 ( ¯Xj −θ0)

,
j = 1, 2, · · · .
(3.5.7) 
Accordingly, the S.R. statistic is 
upper W Subscript m Baseline equals sigma summation Underscript i equals 1 Overscript m minus 1 Endscripts exp left brace StartFraction n delta Over sigma squared EndFraction sigma summation Underscript j equals i plus 1 Overscript m Endscripts left parenthesis upper X overbar Subscript j Baseline minus theta 0 right parenthesis minus StartFraction n delta squared left parenthesis m minus i right parenthesis Over 2 sigma squared EndFraction right brace periodWm =
m−1

i=1
exp
⎧
⎨
⎩
nδ
σ 2
m

j=i+1
( ¯Xj −θ0) −nδ2(m −i)
2σ 2
⎫
⎬
⎭.
(3.5.8) 
Example 3.6 We illustrate the procedure numerically. Suppose that theta 0 equals 10θ0 = 10, n equals 5n = 5, 
delta equals 2δ = 2, pi Superscript asterisk Baseline equals 0.95π∗= 0.95, and sigma equals 3σ = 3. The stopping threshold is 19. Suppose that tau equals 10τ = 10. 
The values of upper X overbar Subscript j ¯Xj have the normal distribution upper N left parenthesis 10 comma StartFraction 3 Over StartRoot 5 EndRoot EndFraction right parenthesisN

10,
3
√
5

for j equals 1 comma midline horizontal ellipsis comma 10j = 1, · · · , 10 and 
upper N left parenthesis 10 plus delta sigma comma StartFraction 3 Over StartRoot 5 EndRoot EndFraction right parenthesisN

10 + δσ,
3
√
5

for j equals 11 comma 12 comma midline horizontal ellipsisj = 11, 12, · · · . In Table 3.12, we present the values of upper W Subscript mWm. 
We see that the S.R. statistic detects the change-point quickly if deltaδ is large.
black medium square■

3.5
Bayesian Detection
89
Table 3.12 Values of upper W Subscript mWm for 
delta equals 0.5 left parenthesis 0.5 right parenthesis 2.0δ = 0.5(0.5)2.0, n equals 5n = 5, 
tau equals 10τ = 10, sigma equals 3σ = 3, pi Superscript asterisk Baseline equals 0.95π∗= 0.95
m 
delta equals 0.5δ = 0.5
delta equals 1.0δ = 1.0
delta equals 1.5δ = 1.5
delta equals 2.0δ = 2.0
2 
0.3649 
0.0773 
0.0361
0.0112 
3 
3.1106 
0.1311 
0.0006
0.0002 
4 
3.2748 
0.0144 
0.0562
0.0000 
5 
1.1788 
0.0069 
0.0020
0.0000 
6 
10.1346 
0.2046 
0.0000
0.0291 
7 
14.4176 
0.0021 
0.0527
0.0000 
8 
2.5980 
0.0021 
0.0015
0.0000 
9 
0.5953 
0.6909 
0.0167
0.0000 
10 
0.4752 
0.0616 
0.0007
0.0001 
11 
1.7219 
5.6838 
848.6259 
1538.0943 
12 
2.2177 
73.8345 
13 
16.3432 
14 
74.9618 
Table 3.13 Estimates of 
PFA and CED for mu 0 equals 10μ0 = 10, 
sigma equals 3σ = 3, n equals 5n = 5, tau equals 10τ = 10, and  
stopping threshold = 99 
delta equals 0.5δ = 0.5
delta equals 1.0δ = 1.0
delta equals 1.5δ = 1.5
delta equals 2.0δ = 2.0
PFA 
0.00
0.01
0.01
0.01 
CED 
7.17
2.50
1.61
1.09
The larger is the critical level w Superscript asterisk Baseline equals pi Superscript asterisk Baseline divided by left parenthesis 1 minus pi Superscript asterisk Baseline right parenthesisw∗= π∗/(1 −π∗), the smaller will be the 
frequency of detecting the change-point before it happens (false alarm). Two 
characteristics of the procedure are of interest: 
(i) The probability of false alarm (PFA) 
(ii) The conditional expected delay (CED), given that the alarm is given after the 
change-point 
The 
functions 
shroArlPfaCedNorm 
and 
shroArlPfaCedPois 
estimate 
the ARL(0) of the procedure for the normal and Poisson cases. Functions 
shroArlPfaCedNorm and shroArlPfaCedPois estimate the PFA and CED of 
these procedures. In Table 3.13, we present simulation estimates of the PFA and 
CED for several values of deltaδ. The estimates are based on 100 simulation runs. 
common = {'mean0': 10, 'sd': 3, 'n': 5, 'tau': 10, 'w': 99, 'seed': 1, 
'verbose': False} 
pd.DataFrame([ 
mistat.shroArlPfaCedNorm(delta=0.5, **common)['statistic'], 
mistat.shroArlPfaCedNorm(delta=1.0, **common)['statistic'], 
mistat.shroArlPfaCedNorm(delta=1.5, **common)['statistic'], 
mistat.shroArlPfaCedNorm(delta=2.0, **common)['statistic'], 
], index=[0.5, 1.0, 1.5, 2.0]) 
ARL 
Std. Error
PFA
CED 
CED-Std. Error 
0.5 
17.17
0.250621 
0.00 
7.170000
1.580130 
1.0 
12.45
0.106184 
0.01 
2.505051
1.234753 
1.5 
11.55
0.080467 
0.01 
1.606061
1.156715 
2.0 
11.04
0.059867 
0.01 
1.090909
1.109741 

90
3
Advanced Methods of Statistical Process Control
Table 3.14 Average run 
length of Shiryaev–Roberts 
procedure, mu 0 equals 10μ0 = 10, delta equals 2δ = 2, 
sigma equals 3σ = 3, n equals 5n = 5
Stopping 
Threshold 
ARL(0) 
19
48.81 plus or minus± 4.41 
50
106.32 plus or minus± 10.87 
99
186.49 plus or minus± 18.29
We see that if the amount of shift deltaδ is large left parenthesis delta greater than 1 right parenthesis(δ > 1), then the conditional 
expected delay (CED) is small. The estimates of PFA are small due to the large 
threshold value. Another question of interest is, what is the average run length 
(ARL) when there is no change in the mean. We estimated the ARL(0), for the same 
example of normally distributed sample means using function shroArlPfaCedNorm. 
100 independent simulation runs were performed. In Table 3.14, we present the 
estimated values of ARL(0), as a function of the stopping threshold. 
common = {'mean0': 10, 'sd': 3, 'n': 5, 'delta': 2.0, 'seed': 1, 
'verbose': False} 
pd.DataFrame([ 
mistat.shroArlPfaCedNorm(w=19, **common)['statistic'], 
mistat.shroArlPfaCedNorm(w=50, **common)['statistic'], 
mistat.shroArlPfaCedNorm(w=99, **common)['statistic'], 
], index=[19, 50, 99]) 
ARL 
Std. Error 
19
48.81
4.411115 
50 
106.32
10.868872 
99 
186.49
18.288885 
Thus, the procedure based on the Shiryaev–Roberts detection is sensitive to 
changes, while in a stable situation (no changes), it is expected to run long till 
an alarm is given. Figure 3.10 shows a box plot of the run length with stopping 
threshold of 99, when there is no change. For more details on data analytic aspects 
of the Shiryaev–Roberts procedure, see Kenett and Pollak (1996). 
3.6 
Process Tracking 
Process tracking is a procedure that repeatedly estimates certain characteristics 
of the process that is being monitored. The CUSUM detection procedure, as 
well as that of Shiryaev–Roberts, is designed to provide warning quickly after 
changes occur. However, at times of stopping, these procedures do not provide 
direct information on the current location of the process mean (or the process 
variance). In the Shewhart upper X overbar ¯X-bar control chart, each point provides an estimate of 
the process mean at that speciﬁc time. The precision of these estimates is generally 
low, since they are based on small samples. One may suggest that, as long as there 
is no evidence that a change in the process mean has occurred, an average of all 
previous sample means can serve as an estimator of the current value of the process 
mean. Indeed, if after observing m samples, each of size n, the grand average

3.6
Process Tracking
91
Fig. 3.10 Box and whisker plots of 100 run lengths of the Shiryaev–Roberts procedure, normal 
distribution mu 0 equals 10μ0 = 10, delta equals 2δ = 2, sigma equals 3σ = 3, n equals 5n = 5, stopping threshold 99
upper X overbar overbar Subscript m Baseline equals StartFraction 1 Over m EndFraction left parenthesis upper X overbar Subscript 1 Baseline plus midline horizontal ellipsis plus upper X overbar Subscript m Baseline right parenthesis ¯¯Xm =
1
m( ¯X1 + · · · + ¯Xm) has the standard error sigma divided by StartRoot n m EndRootσ/√nm, while the standard 
error of the last mean, upper X overbar Subscript m ¯Xm, is only sigma divided by StartRoot n EndRootσ/√n. It is well -established by statistical 
estimation theory that, as long as the process mean mu 0μ0 does not change, upper X overbar overbar Subscript m ¯¯Xm is the 
best (minimum variance) unbiased estimator of mu Subscript m Baseline equals mu 0μm = μ0. On the other hand, if mu 0μ0
has changed to mu 1 equals mu 0 plus delta sigmaμ1 = μ0 + δσ, between the tauτth and the left parenthesis tau plus 1 right parenthesis(τ + 1)st sample, where 
tau less than mτ < m, the grand mean upper X overbar overbar Subscript m ¯¯Xm is a biased estimator of mu 1μ1 (the current mean). The 
expected value of upper X overbar overbar Subscript m ¯¯Xm is 
StartFraction 1 Over m EndFraction left parenthesis tau mu 0 plus left parenthesis m minus tau right parenthesis mu 1 right parenthesis equals mu 1 minus StartFraction tau Over m EndFraction delta sigma period 1
m (τμ0 + (m −τ)μ1) = μ1 −τ
mδσ.
Thus, if the change-point, tauτ, is close to m, the bias of upper X overbar overbar ¯¯X can be considerable. The 
bias of the estimator of the current mean, when 1 less than tau less than m1 < τ < m, can be reduced by 
considering different types of estimators. In the present chapter, we focus attention 
on four procedures for tracking and monitoring the process mean: the exponentially 
weighted moving average procedure (EWMA), the Bayes estimation of the current 
mean (BECM), the Kalman ﬁlter, and the quality measurement plan (QMP). 
3.6.1 
The EWMA Procedure 
The exponentially weighted moving average (EWMA) chart is a control chart for 
the process mean that at time t left parenthesis t equals 1 comma 2 comma midline horizontal ellipsis right parenthesis(t = 1, 2, · · · ) plots the statistic 
ModifyingAbove mu With caret Subscript t Baseline equals left parenthesis 1 minus lamda right parenthesis ModifyingAbove mu With caret Subscript t minus 1 Baseline plus lamda upper X overbar Subscript t Baseline comma ˆμt = (1 −λ) ˆμt−1 + λ ¯Xt,
(3.6.1)

92
3
Advanced Methods of Statistical Process Control
where 0 less than lamda less than 10 < λ < 1, and ModifyingAbove mu With caret Subscript 0 Baseline equals mu 0 ˆμ0 = μ0 is the initial process mean. The Shewhart upper X overbar ¯X-chart 
is the limiting case of lamda equals 1λ = 1. Small values of lamdaλ give high weight to the past data. It 
is customary to use the values of lamda equals 0.2λ = 0.2 or lamda equals 0.3λ = 0.3. 
By repeated application of the recursive formula, we obtain 
StartLayout 1st Row 1st Column ModifyingAbove mu With caret Subscript t 2nd Column equals left parenthesis 1 minus lamda right parenthesis squared ModifyingAbove mu With caret Subscript t minus 2 Baseline plus lamda left parenthesis 1 minus lamda right parenthesis upper X overbar Subscript t minus 1 Baseline plus lamda upper X overbar Subscript t Baseline 2nd Row 1st Column Blank 2nd Column equals midline horizontal ellipsis 3rd Row 1st Column Blank 2nd Column equals left parenthesis 1 minus lamda right parenthesis Superscript t Baseline mu 0 plus lamda sigma summation Underscript i equals 1 Overscript t Endscripts left parenthesis 1 minus lamda right parenthesis Superscript t minus i Baseline upper X overbar Subscript i Baseline period EndLayout
ˆμt = (1 −λ)2 ˆμt−2 + λ(1 −λ) ¯Xt−1 + λ ¯Xt
= · · ·
= (1 −λ)tμ0 + λ
t
i=1
(1 −λ)t−i ¯Xi.
(3.6.2) 
We see in this formula that ModifyingAbove mu With caret Subscript t ˆμt is a weighted average of the ﬁrst t means upper X overbar Subscript 1 Baseline comma midline horizontal ellipsis comma upper X overbar Subscript t Baseline ¯X1, · · · , ¯Xt
and mu 0μ0, with weights that decrease geometrically, as t minus it −i grows. 
Let tauτ denote the epoch of change from mu 0μ0 to mu 1 equals mu 0 plus delta sigmaμ1 = μ0 + δσ. As in the previous 
section, left brace tau equals i right brace{τ = i} implies that 
upper E left brace upper X overbar Subscript j Baseline right brace equals StartLayout Enlarged left brace 1st Row 1st Column mu 0 comma 2nd Column for j equals 1 comma midline horizontal ellipsis comma i 2nd Row 1st Column Blank 3rd Row 1st Column mu 1 comma 2nd Column for j equals i plus 1 comma i plus 2 comma midline horizontal ellipsis EndLayoutE{ ¯Xj} =
⎧
⎪⎪⎨
⎪⎪⎩
μ0,
for j = 1, · · · , i
μ1,
for j = i + 1, i + 2, · · ·
(3.6.3) 
Accordingly, the expected value of the statistic ModifyingAbove mu With caret Subscript t ˆμt (an estimator of the current mean 
mu Subscript tμt) is  
upper E left brace ModifyingAbove mu With caret Subscript t Baseline right brace equals StartLayout Enlarged left brace 1st Row 1st Column mu 0 comma 2nd Column if t less than or equals tau 2nd Row 1st Column Blank 3rd Row 1st Column mu 1 minus delta sigma left parenthesis 1 minus lamda right parenthesis Superscript t minus tau Baseline comma 2nd Column if t greater than tau period EndLayoutE{ ˆμt} =
⎧
⎪⎪⎨
⎪⎪⎩
μ0,
if t ≤τ
μ1 −δσ(1 −λ)t−τ,
if t > τ.
(3.6.4) 
We  see that  the bias of  ModifyingAbove mu With caret Subscript t ˆμt, minus delta sigma left parenthesis 1 minus lamda right parenthesis Superscript t minus tau−δσ(1 −λ)t−τ, decreases to zero geometrically fast as 
t grows above tauτ. This is a faster decrease in bias than that of the grand mean, upper X overbar overbar Subscript t ¯¯Xt, 
which was discussed earlier. 
The variance of ModifyingAbove mu With caret Subscript t ˆμt can be easily determined, since upper X overbar Subscript 1 Baseline comma upper X overbar Subscript 2 Baseline comma midline horizontal ellipsis comma upper X overbar Subscript t Baseline ¯X1, ¯X2, · · · , ¯Xt are indepen-
dent and VarStartSet upper X overbar Subscript j Baseline EndSet equals StartFraction sigma squared Over n EndFraction{ ¯Xj} = σ 2
n , j equals 1 comma 2 comma midline horizontal ellipsisj = 1, 2, · · · . Hence, 
StartLayout 1st Row 1st Column Var StartSet ModifyingAbove mu With caret Subscript t Baseline EndSet 2nd Column equals StartFraction sigma squared Over n EndFraction lamda squared sigma summation Underscript i equals 1 Overscript t Endscripts left parenthesis 1 minus lamda right parenthesis Superscript 2 left parenthesis t minus i right parenthesis Baseline 2nd Row 1st Column Blank 2nd Column equals StartFraction sigma squared Over n EndFraction lamda squared StartFraction 1 minus left parenthesis 1 minus lamda right parenthesis Superscript 2 t Baseline Over 1 minus left parenthesis 1 minus lamda right parenthesis squared EndFraction period EndLayout
Var{ ˆμt} = σ 2
n λ2
t
i=1
(1 −λ)2(t−i)
= σ 2
n λ2 1 −(1 −λ)2t
1 −(1 −λ)2 .
(3.6.5)

3.6
Process Tracking
93
This variance converges to 
Avar StartSet ModifyingAbove mu With caret Subscript t Baseline EndSet equals StartFraction sigma squared Over n EndFraction StartFraction lamda Over 2 minus lamda EndFraction commaAvar{ ˆμt} = σ 2
n
λ
2 −λ,
(3.6.6) 
as t right arrow normal infinityt
→∞. An  EWMA control chart for monitoring shifts in the mean is 
constructed in the following manner. Starting at ModifyingAbove mu With caret Subscript 0 Baseline equals mu 0 ˆμ0 = μ0, the points left parenthesis t comma ModifyingAbove mu With caret Subscript t Baseline right parenthesis(t, ˆμt), 
t equals 1 comma 2 comma midline horizontal ellipsist = 1, 2, · · · , are plotted. As soon as these points cross either one of the control 
limits 
CL equals mu 0 plus or minus upper L StartFraction sigma Over StartRoot n EndRoot EndFraction StartRoot StartFraction lamda Over 2 minus lamda EndFraction EndRoot commaCL = μ0 ± L σ
√n

λ
2 −λ,
(3.6.7) 
an alarm is given that the process mean has shifted. 
In Fig. 3.11, we present an EWMA chart with mu 0 equals 10μ0 = 10, sigma equals 3σ = 3, n equals 5n = 5, lamda equals 0.2λ = 0.2, 
and upper L equals 2L = 2. The values of ModifyingAbove mu With caret Subscript t ˆμt indicate that a shift in the mean took place after the 
eleventh sampling epoch. An alarm for change is given after the fourteenth sample. 
As in the previous sections, we have to characterize the efﬁcacy of the EWMA 
chart in terms of PFA and CED when a shift occurs, and the ARL when there is no 
shift. In Table 3.15, we present estimates of PFA and CED based on 1000 simulation 
runs. The simulations were from normal distributions, with mu 0 equals 10μ0 = 10, sigma equals 3σ = 3, and
Fig. 3.11 EWMA chart, mu 0 equals 10μ0 = 10, sigma equals 3σ = 3, delta equals 0δ = 0, n equals 5n = 5, lamda equals 0.2λ = 0.2 (smooth), and upper L equals 2L = 2

94
3
Advanced Methods of Statistical Process Control
Table 3.15 Simulation 
estimates of PFA and CED of 
an EWMA chart 
CED 
L
PFA 
delta equals 0.5δ = 0.5
delta equals 1.0δ = 1.0
delta equals 1.5δ = 1.5
delta equals 2.0δ = 2.0
2
0.168 
3.93
2.21
1.20
1.00 
2.5 
0.043 
4.35
2.67
1.41
1.03 
3
0.002 
4.13
3.36
1.63
1.06 
n equals 5n = 5. The change-point was at tau equals 10τ = 10. The shift was from mu 0μ0 to mu 1 equals mu 0 plus delta sigmaμ1 = μ0 + δσ. 
The estimates ModifyingAbove mu With caret Subscript t ˆμt were determined with lamda equals 0.2λ = 0.2. We see in this table that if we 
construct the control limits with the value of upper L equals 3L = 3, then the PFA is very small, and 
the CED is not large. 
The estimated ARL values for this example are 
L
2
2.5
3.0 
ARL 
48.7 
151.36 
660.9 
3.6.2 
The BECM Procedure 
In this section, we present a Bayesian procedure for estimating the current mean 
mu Subscript tμt left parenthesis t equals 1 comma 2 comma midline horizontal ellipsis right parenthesis(t = 1, 2, · · · ). Let  upper X overbar Subscript 1 Baseline comma upper X overbar Subscript 2 Baseline comma midline horizontal ellipsis comma upper X overbar Subscript t Baseline ¯X1, ¯X2, · · · , ¯Xt, t equals 1 comma 2 comma midline horizontal ellipsist = 1, 2, · · · , be means of samples of size 
n. The distribution of upper X overbar Subscript i ¯Xi is upper N left parenthesis mu Subscript i Baseline comma StartFraction sigma Over StartRoot n EndRoot EndFraction right parenthesisN

μi, σ
√n

, where sigmaσ is the process standard deviation. 
We will assume here that sigmaσ is known and ﬁxed throughout all sampling epochs. This 
assumption is made in order to simplify the exposition. In actual cases, one has to 
monitor also whether sigmaσ changes with time. 
If the process mean stays stable throughout the sampling periods, then 
mu 1 equals mu 2 equals midline horizontal ellipsis equals mu Subscript t Baseline equals mu 0 periodμ1 = μ2 = · · · = μt = μ0.
Let us consider this case ﬁrst and present the Bayes estimator of mu 0μ0. In the Bayesian 
approach, the model assumes that mu 0μ0 itself is random, with some prior distribution. 
If we assume that the prior distribution of mu 0μ0 is normal, say upper N left parenthesis mu Superscript asterisk Baseline comma tau right parenthesisN(μ∗, τ), then using 
Bayes theorem one can show that the posterior distribution of mu 0μ0, given  the  t sample 
means, is normal with mean 
ModifyingAbove mu With caret Subscript upper B comma t Baseline equals left parenthesis 1 minus StartFraction n t tau squared Over sigma squared plus n t tau squared EndFraction right parenthesis mu Superscript asterisk Baseline plus StartFraction n t tau squared Over sigma squared plus n t tau squared EndFraction equals upper X overbar Subscript t Baseline equals StartFraction 1 Over t EndFraction sigma summation Underscript i equals 1 Overscript t Endscripts upper X overbar Subscript i ˆμB,t =

1 −
ntτ 2
σ 2 + ntτ 2

μ∗+
ntτ 2
σ 2 + ntτ 2 = ¯Xt = 1
t
t
i=1
¯Xi
(3.6.8) 
and variance

3.6
Process Tracking
95
w Subscript t Superscript 2 Baseline equals tau squared left parenthesis 1 minus StartFraction n t tau squared Over sigma squared plus n t tau squared EndFraction right parenthesis periodw2
t = τ 2

1 −
ntτ 2
σ 2 + ntτ 2

.
(3.6.9) 
The mean ModifyingAbove mu With caret Subscript upper B comma t ˆμB,t of the posterior distribution is commonly taken as the Bayes 
estimator of mu 0μ0 (see Chapter 3, Modern Statistics, Kenett et al. 2022b). 
It is interesting to notice that ModifyingAbove mu With caret Subscript upper B comma t ˆμB,t, t equals 1 comma 2 comma midline horizontal ellipsist = 1, 2, · · · , can be determined recursively 
by the formula 
ModifyingAbove mu With caret Subscript upper B comma t Baseline equals left parenthesis 1 minus StartFraction n w Subscript t minus 1 Superscript 2 Baseline Over sigma squared plus n w Subscript t minus 1 Superscript 2 Baseline EndFraction right parenthesis ModifyingAbove mu With caret Subscript upper B comma t minus 1 Baseline plus StartFraction n w Subscript t minus 1 Superscript 2 Baseline Over sigma squared plus n w Subscript t minus 1 Superscript 2 Baseline EndFraction upper X overbar Subscript t Baseline comma ˆμB,t =
 
1 −
nw2
t−1
σ 2 + nw2
t−1
!
ˆμB,t−1 +
nw2
t−1
σ 2 + nw2
t−1
¯Xt,
(3.6.10) 
where ModifyingAbove mu With caret Subscript upper B comma 0 Baseline equals mu Superscript asterisk ˆμB,0 = μ∗, w 0 squared equals tau squaredw2
0 = τ 2, and 
w Subscript t Superscript 2 Baseline equals StartFraction sigma squared w Subscript t minus 1 Superscript 2 Baseline Over sigma squared plus n w Subscript t minus 1 Superscript 2 Baseline EndFraction periodw2
t =
σ 2w2
t−1
σ 2 + nw2
t−1
.
(3.6.11) 
This recursive formula resembles that of the EWMA estimator. The difference here 
is that the weight lamdaλ is a function of time, i.e., 
lamda Subscript t Baseline equals StartFraction n w Subscript t minus 1 Superscript 2 Baseline Over sigma squared plus n w Subscript t minus 1 Superscript 2 Baseline EndFraction periodλt =
nw2
t−1
σ 2 + nw2
t−1
.
(3.6.12) 
From the above recursive formula for w Subscript t Superscript 2w2
t , we obtain that w Subscript t Superscript 2 Baseline equals lamda Subscript t Baseline StartFraction sigma squared Over n EndFractionw2
t = λt σ 2
n , or  lamda Subscript t Baseline equals lamda Subscript t minus 1 Baseline divided by left parenthesis 1 plus lamda Subscript t minus 1 Baseline right parenthesisλt =
λt−1/(1+λt−1), t equals 2 comma 3 comma midline horizontal ellipsist = 2, 3, · · · where lamda 1 equals n tau squared divided by left parenthesis sigma squared plus n tau squared right parenthesisλ1 = nτ 2/(σ 2+nτ 2). The procedures become 
more complicated if change-points are introduced. We discuss in the following 
section a dynamic model of change. 
3.6.3 
The Kalman Filter 
In the present section, we present a model of dynamic changes in the observed 
sequence of random variables and a Bayesian estimator of the current mean, called 
the Kalman ﬁlter. 
At time t, let  upper Y Subscript tYt denote an observable random variable, having mean mu Subscript tμt. We  
assume that mu Subscript tμt may change at random from one time epoch to another, according to 
the model 
mu Subscript t Baseline equals mu Subscript t minus 1 Baseline plus upper Delta Subscript t Baseline comma t equals 1 comma 2 comma midline horizontal ellipsisμt = μt−1 + t,
t = 1, 2, · · ·
where upper Delta Subscript tt, t equals 1 comma 2 comma midline horizontal ellipsist = 1, 2, · · · , is a sequence of i.i.d. random variables having a normal 
distribution upper N left parenthesis delta comma sigma 2 right parenthesisN(δ, σ2). Furthermore, we assume that mu 0 tilde upper N left parenthesis mu 0 Superscript asterisk Baseline comma w 0 right parenthesisμ0 ∼N(μ∗
0, w0), and the 
observation equation is

96
3
Advanced Methods of Statistical Process Control
upper Y Subscript t Baseline equals mu Subscript t Baseline plus epsilon Subscript t Baseline comma t equals 1 comma 2 comma midline horizontal ellipsis commaYt = μt + ϵt,
t = 1, 2, · · · ,
where epsilon Subscript tϵt are i.i.d. upper N left parenthesis 0 comma sigma Subscript epsilon Baseline right parenthesisN(0, σϵ). According to this dynamic model, the mean at time t 
(the current mean) is normally distributed with mean 
ModifyingAbove mu With caret Subscript t Baseline equals upper B Subscript t Baseline left parenthesis ModifyingAbove mu With caret Subscript t minus 1 Baseline plus delta right parenthesis plus left parenthesis 1 minus upper B Subscript t Baseline right parenthesis upper Y Subscript t Baseline comma ˆμt = Bt( ˆμt−1 + δ) + (1 −Bt)Yt,
(3.6.13) 
where 
StartLayout 1st Row upper B Subscript t Baseline equals StartFraction sigma Subscript epsilon Superscript 2 Baseline Over sigma Subscript epsilon Superscript 2 Baseline plus sigma 2 squared plus w Subscript t minus 1 Superscript 2 Baseline EndFraction comma EndLayout StartLayout 2nd Row w Subscript t Superscript 2 Baseline equals upper B Subscript t Baseline left parenthesis sigma 2 squared plus w Subscript t minus 1 Superscript 2 Baseline right parenthesis period EndLayoutBt =
σ 2
ϵ
σ 2ϵ + σ 2
2 + w2
t−1
,
(3.6.14) 
w2 
t = Bt(σ 2 
2 + w2 
t−1).
(3.6.15) 
The posterior variance of mu Subscript tμt is w Subscript t Superscript 2w2
t . ModifyingAbove mu With caret Subscript t ˆμt is the Kalman ﬁlter. 
If the prior parameters sigma Subscript epsilon Superscript 2σ 2
ϵ , sigma 2 squaredσ 2
2 , and deltaδ are unknown, we could use a small portion 
of the data to estimate these parameters. 
According to the dynamic model, we can write 
y Subscript t Baseline equals mu 0 plus delta t plus epsilon Subscript t Superscript asterisk Baseline comma t equals 1 comma 2 comma midline horizontal ellipsis commayt = μ0 + δt + ϵ∗
t ,
t = 1, 2, · · · ,
where epsilon Subscript t Superscript asterisk Baseline equals sigma summation Underscript i equals 1 Overscript t Endscripts left bracket left parenthesis upper Delta Subscript i Baseline minus delta right parenthesis plus epsilon Subscript i Baseline right bracketϵ∗
t = t
i=1[(i −δ) + ϵi]. Notice that upper E left brace epsilon Subscript t Superscript asterisk Baseline right brace equals 0E{ϵ∗
t } = 0 for all t and upper V left brace epsilon Subscript t Superscript asterisk Baseline right brace equals t left parenthesis sigma 2 squared plus sigma Subscript epsilon Superscript 2 Baseline right parenthesisV {ϵ∗
t } =
t(σ 2
2 + σ 2
ϵ ). Let upper U Subscript t Baseline equals y Subscript t Baseline divided by StartRoot t EndRootUt = yt/√t, t equals 1 comma 2 comma midline horizontal ellipsist = 1, 2, · · · ; then we can write the regression model 
upper U Subscript t Baseline equals mu 0 x Subscript 1 t Baseline plus delta x Subscript 2 t Baseline plus eta Subscript t Baseline comma t equals 1 comma 2 comma midline horizontal ellipsis commaUt = μ0x1t + δx2t + ηt,
t = 1, 2, · · · ,
where x Subscript 1 t Baseline equals 1 divided by StartRoot t EndRootx1t = 1/√t and x Subscript 2 t Baseline equals StartRoot t EndRootx2t = √t and eta Subscript tηt, t equals 1 comma 2 midline horizontal ellipsist = 1, 2 · · · , are independent random 
variables, with upper E left brace eta Subscript t Baseline right brace equals 0E{ηt} = 0 and upper V left brace eta Subscript t Baseline right brace equals left parenthesis sigma 2 squared plus sigma Subscript e Superscript 2 Baseline right parenthesisV {ηt} = (σ 2
2 + σ 2
e ). 
Using the ﬁrst m points of left parenthesis t comma y Subscript t Baseline right parenthesis(t, yt) and ﬁtting, by the method of least squares (see 
Chapter 3, Modern Statistics, Kenett et al. 2022b), the regression equation of upper U Subscript tUt
against left parenthesis x Subscript 1 t Baseline comma x Subscript 2 t Baseline right parenthesis(x1t, x2t), we obtain estimates of mu 0μ0, deltaδ and of left parenthesis sigma 2 squared plus sigma Subscript epsilon Superscript 2 Baseline right parenthesis(σ 2
2 +σ 2
ϵ ). Estimate of sigma Subscript e Superscript 2σ 2
e can 
be obtained, if y Subscript tyt are group means, by estimating within groups variance; otherwise, 
we assume a value for sigma Subscript epsilon Superscript 2σ 2
ϵ , smaller than the least squares estimate of sigma 2 squared plus sigma Subscript epsilon Superscript 2σ 2
2 + σ 2
ϵ . We  
illustrate this now by example. 
Example 3.7 In Fig. 3.12, we present the Dow–Jones ﬁnancial index for the 300 
business days of 1935 (dataset DOJO1935.csv). The Kalman ﬁlter estimates of the 
current means are plotted in this ﬁgure too. These estimates were determined by the 
formula
ModifyingAbove mu With caret Subscript t Baseline equals upper B Subscript t Baseline left parenthesis ModifyingAbove mu With caret Subscript t minus 1 Baseline plus delta right parenthesis plus left parenthesis 1 minus upper B Subscript t Baseline right parenthesis y Subscript t Baseline comma ˆμt = Bt( ˆμt−1 + δ) + (1 −Bt)yt,
(3.6.16) 
where the prior parameters were computed as suggested above, on the basis of the 
ﬁrst m equals 20m = 20 data points.

3.6
Process Tracking
97
Fig. 3.12 The daily Dow–Jones ﬁnancial index for 1935
dojo1935 = mistat.load_data('DOJO1935') 
# solve the regression equation 
m = 20  
sqrt_t = np.sqrt(range(1, m + 1)) 
df = pd.DataFrame({ 
'Ut': dojo1935[:m]/sqrt_t, 
'x1t': 1 / sqrt_t, 
'x2t': sqrt_t, 
}) 
model = smf.ols(formula='Ut ~ x1t + x2t - 1', data=df).fit() 
mu0, delta = model.params 
var_eta = np.var(model.resid, ddof=2) 
pd.Series({'mu0': mu0, 'delta': delta, 'Var(eta)': var_eta}) 
mu0
127.484294 
delta
0.655591 
Var(eta)
0.073094 
dtype: float64 
The least squares estimates of mu 0μ0, deltaδ, and sigma 2 squared plus sigma Subscript epsilon Superscript 2σ 2
2 +σ 2
ϵ are, respectively, ModifyingAbove mu With caret Subscript 0 Baseline equals 127.484 ˆμ0 = 127.484, 
ModifyingAbove delta With caret equals 0.656ˆδ = 0.656, and ModifyingAbove sigma With caret Subscript 2 Superscript 2 Baseline plus ModifyingAbove sigma With caret Subscript epsilon Superscript 2 Baseline equals 0.0731ˆσ 2
2 + ˆσ 2
ϵ = 0.0731. For ModifyingAbove sigma With caret Subscript epsilon Superscript 2ˆσ 2
ϵ , we have chosen the value 0.0597, and for 
w 0 squaredw2
0, the value 0.0015. Using these starting values, we can apply the Kalman ﬁlter. 
# choose sig2e and w20 
sig2e = 0.0597 
w20 = 0.0015 
# apply the filter 
results = [] 
mu_tm1 = mu0 
w2_tm1 = w20 
y_tm1 = mu0 
for i in range(0, len(dojo1935)): 
y_t = dojo1935[i] 
B_t = sig2e / (var_eta + w2_tm1) 
mu_t = B_t * (mu_tm1 + delta) + (1 - B_t) * y_t 
results.append({

98
3
Advanced Methods of Statistical Process Control
't': i + 1, # adjust for Python indexing starting at 0 
'y_t': y_t, 
'mu_t': mu_t, 
}) 
w2_tm1 = B_t * (var_eta - sig2e + w2_tm1) 
mu_tm1 = mu_t 
y_tm1 = y_t 
results = pd.DataFrame(results) 
The ﬁrst 50 values of the data, y Subscript tyt, and the estimate ModifyingAbove mu With caret Subscript t ˆμt, are given in Table 3.16. black medium square■
3.6.4 
The QMP Tracking Method 
Hoadley (1981) introduced at Bell Laboratories a quality measurement plan 
(QMP), which employs Bayesian methods of estimating the current mean of a
Table 3.16 The Dow–Jones index for the ﬁrst 50 days of 1935, and the Kalman ﬁlter 
estimates 
t
y Subscript tyt
mu Subscript tμt
1 
128.06 
128.12 
2 
129.05 
128.86 
3 
129.76 
129.60 
4 
130.35 
130.29 
5 
130.77 
130.88 
6 
130.06 
130.99 
7 
130.59 
131.25 
8 
132.99 
132.31 
9 
133.56 
133.19 
10 
135.03 
134.29 
11 
136.26 
135.44 
12 
135.68 
135.94 
13 
135.57 
136.21 
14 
135.13 
136.22 
15 
137.09 
136.95 
16 
138.96 
138.11 
17 
138.77 
138.77 
18 
139.58 
139.48 
19 
139.42 
139.87 
20 
140.68 
140.58 
21 
141.47 
141.33 
22 
140.78 
141.53 
23 
140.49 
141.55 
24 
139.35 
141.14 
25 
139.74 
141.02 
t
y Subscript tyt
mu Subscript tμt
26 
141.31 
141.54 
27 
141.20 
141.82 
28 
141.07 
141.95 
29 
142.90 
142.72 
30 
143.40 
143.38 
31 
144.25 
144.12 
32 
144.36 
144.62 
33 
142.56 
144.26 
34 
143.59 
144.42 
35 
145.59 
145.27 
36 
146.32 
146.07 
37 
147.31 
146.95 
38 
147.06 
147.40 
39 
148.44 
148.20 
40 
146.65 
148.03 
41 
147.37 
148.19 
42 
144.61 
147.26 
43 
146.12 
147.24 
44 
144.72 
146.71 
45 
142.59 
145.58 
46 
143.38 
145.16 
47 
142.34 
144.52 
48 
142.35 
144.11 
49 
140.72 
143.25 
50 
143.58 
143.79 

3.6
Process Tracking
99
process. This QMP provides reporting capabilities of large datasets and, in a certain 
sense, is an improvement over the Shewhart 3-sigma control. These plans were 
implemented throughout Western Electric Co. in the late 1980s. The main idea is 
that the process mean does not remain at a constant level but changes at random 
every time period according to some distribution. This framework is similar to 
that of the Kalman ﬁlter but was developed for observations Xt having Poisson 
distributions with means λt (t = 1, 2, · · ·  ), and where λ1, λ2, · · ·  are independent 
random variables having a common gamma distribution G(ν, 	). The parameters ν 
and 	 are unknown and are estimated from the data. At the end of each period, a 
box plot is put on a chart. The center line of the box plot represents the posterior 
mean of λt, given past observations. The lower and upper sides of the box represent 
the 0.05th and 0.95th quantiles of the posterior distribution of λt. The lower whisker 
starts at the 0.01th quantile of the posterior distribution, and the upper whisker ends 
at the 0.99th quantile of that distribution. These box plots are compared to a desired 
quality level. 
QMP is a mixed model with random effects, in contrast with ﬁxed effects. It is 
a best linear unbiased prediction (BLUP). BLUP estimates of realized values of a 
random variable are linear in the sense that they are linear functions of the data. They 
are unbiased in the sense that the average value of the estimate is equal to the average 
value of the quantity being estimated and best in the sense that they have minimum 
sum of squared errors within the class of linear unbiased estimators. Estimators of 
random effects are called predictors, to distinguish them from estimators of ﬁxed 
effects called estimators. BLUP estimates are solutions to mixed model equations 
and are usually different from generalized linear regression estimates used for ﬁxed 
effects. 
It is interesting to consider random versus ﬁxed effects in the context of 
the analytic studies versus enumerative studies dichotomy introduced by Deming 
(1982). Enumerative studies are focused on estimation used to explain existing 
conditions. Acceptance sampling covered in Chap. 11 is enumerative. Analytic 
studies focus on predictions of performance in new circumstances. The QMP model 
is analytic in that it predicts the impact of data measured over time. The design of 
experiments covered in Chaps. 5–7 can be considered as both enumerative, when 
focused on explaining the effects of a list of factors on responses, or analytic, 
in predictive future responses. This distinction is reﬂected by optimality criteria 
discussed in Sect. 5.10. We proceed with an analysis of the QMP model. 
We show in Sect. 3.8.3 of Modern Statistics (Kenett et al. 2022b) that if Xt has 
a Poisson distribution P(λt), and λt has a gamma distribution G(ν, 	), then the 
posterior distribution of λt, given  Xt, is the gamma distribution G

ν + Xt,
	
1+	

. 
Thus, the Bayes estimate of λt, for a squared error loss, is the posterior expectation 
ModifyingAbove lamda With caret Subscript t Baseline equals left parenthesis nu plus upper X Subscript t Baseline right parenthesis StartFraction upper Lamda Over 1 plus upper Lamda EndFraction periodˆλt = (ν + Xt)
	
1 + 	.
(3.6.17) 
Similarly, the pth quantile of the posterior distribution is

100
3
Advanced Methods of Statistical Process Control
lamda Subscript t comma p Baseline equals StartFraction upper Lamda Over 1 plus upper Lamda EndFraction upper G Subscript p Baseline left parenthesis nu plus upper X Subscript t Baseline comma 1 right parenthesis commaλt,p =
	
1 + 	Gp(ν + Xt, 1),
(3.6.18) 
where Gp(ν + Xt, 1) is the pth quantile of the standard gamma distribution G(ν + 
Xt, 1). We remark that if ν is an integer, then 
upper G Subscript p Baseline left parenthesis nu plus upper X Subscript t Baseline comma 1 right parenthesis equals one half chi Subscript p Superscript 2 Baseline left bracket 2 left parenthesis nu plus upper X Subscript t Baseline right parenthesis right bracket periodGp(ν + Xt, 1) = 1
2χ2
p[2(ν + Xt)].
(3.6.19) 
We assumed that λ1, λ2, · · ·  are independent and identically distributed. This 
implies that X1, X2, · · ·  are independent, having the same negative binomial 
predictive distribution, with predictive expectation 
upper E left brace upper X Subscript t Baseline right brace equals nu upper LamdaE{Xt} = ν	
(3.6.20) 
and predictive variance 
upper V left brace upper X Subscript t Baseline right brace equals nu upper Lamda left parenthesis 1 plus upper Lamda right parenthesis periodV {Xt} = ν	(1 + 	).
(3.6.21) 
We therefore can estimate the prior parameters ν and 	 by the consistent estimators 
ModifyingAbove upper Lamda With caret Subscript upper T Baseline equals left parenthesis StartFraction upper S Subscript upper T Superscript 2 Baseline Over upper X overbar Subscript upper T Baseline EndFraction minus 1 right parenthesis Superscript plus ˆ	T =
 
S2
T
¯XT
−1
!+
(3.6.22) 
and 
ModifyingAbove nu With caret Subscript upper T Baseline equals StartFraction upper X overbar Subscript upper T Baseline Over ModifyingAbove upper Lamda With caret Subscript upper T Baseline EndFraction commaˆνT =
¯XT
ˆ	T
,
(3.6.23) 
where ¯XT and S2 
T are the sample mean and sample variance of X1, X2, · · ·  , XT . 
For determining ˆλt and λt,p, we can substitute ˆ	T and ˆνT in the above equations, 
with T = t − 1. We illustrate this estimation method, called parametric empirical 
Bayes method, in the following example. 
Example 3.8 In dataset SOLDEF.csv, we present results of testing batches of 
circuit boards for defects in solder points, after wave soldering. The batches include 
boards of similar design. There were close to 1000 solder points on each board. The 
results Xt are the number of defects per 106 points (PPM). The quality standard 
is λ0 = 100 (PPM). λt values below λ0 represent high-quality soldering. In this 
dataset, there are N = 380 test results. Only 78 batches had an Xt value greater 
than λ0 = 100. If we take UCL = λ0 + 3
√
λ0 = 130, we see that only 56 batches 
had Xt values greater than the UCL. All runs of consecutive Xt values greater than 
130 are of length not greater than 3. We conclude therefore that the occurrence 
of low-quality batches is sporadic, caused by common causes. These batches are 
excluded from the analysis.

3.6
Process Tracking
101
soldef = mistat.load_data('SOLDEF') 
print('Batches above quality standard: ', sum(soldef > 100)) 
print('Batches above UCL: ', sum(soldef > 130)) 
xbar = np.cumsum(soldef) / np.arange(1, len(soldef)+1) 
results = [] 
for i in range(2, len(soldef)): 
xbar_tm1 = np.mean(xbar[i-1]) 
S2_tm1 = np.var(soldef[:i]) 
gamma_tm1 = S2_tm1/xbar_tm1 - 1 
nu_tm1 = xbar_tm1 / gamma_tm1 
result = { 
't': i + 1, 
'Xt': soldef[i], 
'xbar_tm1': xbar_tm1, 
'S2_tm1': S2_tm1, 
'Gamma_tm1': gamma_tm1, 
'nu_tm1': nu_tm1, 
} 
f = gamma_tm1 / (gamma_tm1 + 1) 
shape = nu_tm1 + soldef[i] 
result['lambda_t'] = f * shape 
result.update(((f'lambda({p})', f * stats.gamma.ppf(p, a=shape, scale=1)) 
for p in (0.01, 0.05, 0.95, 0.99))) 
results.append(result) 
results = pd.DataFrame(results) 
Batches above quality standard: 
78 
Batches above UCL: 
56 
In Table 3.17, we present the Xt values and the associated values of ¯Xt−1, 
S2 
t−1, ˆ	t−1, and ˆνt−1, associated with t = 10, · · ·  , 20. The statistics ¯Xt−1, etc., 
are functions of X1, · · ·  , Xt−1. In Table 3.18, we present the values of ˆλt and the 
quantiles λt,p for p = 0.01, 0.05, 0.95, and 0.99.
■
Table 3.17 The number of 
defects (PPM) and associated 
statistics for the SOLDEF 
data 
t
Xt 
¯Xt−1 
S2 
t−1 
ˆ	t−1 
ˆνt−1 
10 
29 
23.67 
75.56 
2.19 
10.79 
11 
16 
24.20 
70.56 
1.92 
12.63 
12 
31 
23.45 
69.70 
1.97 
11.89 
13 
19 
24.08 
68.24 
1.83 
13.13 
14 
18 
23.69 
64.83 
1.74 
13.65 
15 
20 
23.29 
62.35 
1.68 
13.88 
16 
103 
23.07 
58.86 
1.55 
14.86 
17 
31 
28.06 
429.56 
14.31 
1.96 
18 
33 
28.24 
404.77 
13.34 
2.12 
19 
12 
28.50 
383.47 
12.46 
2.29 
20 
46 
27.63 
376.86 
12.64 
2.19

102
3
Advanced Methods of Statistical Process Control
Table 3.18 Empirical Bayes 
estimates of λt and λt,p, 
p = 0.01, 0.05, 0.95, 0.99 
t
λt
λt,0.01 
λt,0.05 
λt,0.95 
λt,0.99 
10 
27.33 
18.27 
20.61 
34.82 
38.40 
11 
18.81 
11.61 
13.43 
24.94 
27.94 
12 
28.46 
19.34 
21.71 
35.97 
39.53 
13 
20.79 
13.22 
15.15 
27.17 
30.26 
14 
20.08 
12.72 
14.59 
26.29 
29.30 
15 
21.23 
13.67 
15.61 
27.56 
30.62 
16 
71.68 
57.22 
61.17 
82.87 
87.92 
17 
30.81 
19.71 
22.54 
40.13 
44.65 
18 
32.67 
21.22 
24.16 
42.23 
46.84 
19 
13.23 
6.46
8.04
19.46 
22.69 
20 
44.65 
31.06 
34.62 
55.73 
60.96 
3.7 
Automatic Process Control 
Certain production lines are fully automated, like in chemical industries, paper 
industries, automobile industry, etc. In such production lines, it is often possible 
to build in feedback and control mechanism, so that if there is indication that the 
process mean or standard deviation changes signiﬁcantly, then a correction is made 
automatically via the control mechanism. If mu Subscript tμt denotes the level of the process mean 
at time t, and u Subscript tut denotes the control level at time t, the  dynamic linear model 
(DLM) of the process mean is 
mu Subscript t Baseline equals mu Subscript t minus 1 Baseline plus upper Delta Subscript t Baseline plus b u Subscript t minus 1 Baseline comma t equals 1 comma 2 comma midline horizontal ellipsis commaμt = μt−1 + t + but−1,
t = 1, 2, · · · ,
(3.7.1) 
the observations equation is as before 
upper Y Subscript t Baseline equals mu Subscript t Baseline plus epsilon Subscript t Baseline comma t equals 1 comma 2 comma midline horizontal ellipsis commaYt = μt + ϵt,
t = 1, 2, · · · ,
(3.7.2) 
and upper Delta Subscript tt is a random disturbance in the process evolution. The recursive equation of 
the DLM is linear, in the sense that the effect on mu Subscript tμt of u Subscript t minus 1ut−1 is proportional to u Subscript t minus 1ut−1. 
The control could be on a vector of several variables, whose level at time t is given 
by a vector bold u Subscript tut. The question is how to determine the levels of the control variables? 
This question of optimal control of systems, when the true level mu Subscript tμt of the process 
mean is not known exactly, but only estimated from the observed values of upper Y Subscript tYt, is a  
subject of studies in the ﬁeld of stochastic control. We refer the reader to the book 
of Aoki (1989). The reader is referred also to the paper by Box and Kramer (1992). 
It is common practice, in many industries, to use the proportional rule for 
control. That is, if the process level (mean) is targeted at mu 0μ0, and the estimated 
level at time t is ModifyingAbove mu With caret Subscript t ˆμt, then 
u Subscript t Baseline equals minus p left parenthesis ModifyingAbove mu With caret Subscript t Baseline minus mu 0 right parenthesis commaut = −p( ˆμt −μ0),
(3.7.3)

3.7
Automatic Process Control
103
where p is some factor, which is determined by the DLM, by cost factors, etc. This 
rule is not necessarily optimal. It depends on the objectives of the optimization. For 
example, suppose that the DLM with control is 
mu Subscript t Baseline equals mu Subscript t minus 1 Baseline plus b u Subscript t minus 1 Baseline plus upper Delta Subscript t Baseline comma t equals 1 comma 2 comma midline horizontal ellipsis commaμt = μt−1 + but−1 + t,
t = 1, 2, · · · ,
(3.7.4) 
where the process mean is set at mu 0μ0 at time t equals 0t = 0. upper Delta Subscript upper TT is a random disturbance, 
having a normal distribution upper N left parenthesis delta comma sigma right parenthesisN(δ, σ). The process level mu Subscript tμt is estimated by the 
Kalman ﬁlter, which was described in the previous section. We have the option to 
adjust the mean, at each time period, at a cost of c Subscript upper A Baseline u squaredcAu2 [$]. On the other hand, at the 
end of T periods, we pay a penalty of $ c Subscript d Baseline left parenthesis mu Subscript upper T Baseline minus mu 0 right parenthesis squaredcd(μT −μ0)2, for the deviation of mu Subscript upper TμT from 
the target level. In this example, the optimal levels of u Subscript tut, for t equals 0 comma midline horizontal ellipsis comma upper T minus 1t = 0, · · · , T −1, are  
given by 
u Subscript t Superscript 0 Baseline equals minus StartFraction b q Subscript t plus 1 Baseline Over c Subscript upper A Baseline plus q Subscript t plus 1 Baseline b squared EndFraction left parenthesis ModifyingAbove mu With caret Subscript t Baseline minus mu 0 right parenthesis commau0
t = −
bqt+1
cA + qt+1b2 ( ˆμt −μ0),
(3.7.5) 
where 
q Subscript upper T Baseline equals c Subscript dqT = cd
and, for t equals 0 comma midline horizontal ellipsis comma upper T minus 1t = 0, · · · , T −1, 
q Subscript t Baseline equals StartFraction c Subscript upper A Baseline q Subscript t plus 1 Baseline Over c Subscript upper A Baseline plus q Subscript t plus 1 Baseline b squared EndFraction periodqt =
cAqt+1
cA + qt+1b2 .
(3.7.6) 
These formulae are obtained as special cases from general result given in Aoki 
(1989, pp. 128). Thus, we see that the values that u Subscript tut obtains, under the optimal 
scheme, are proportional to minus left parenthesis ModifyingAbove mu With caret Subscript t Baseline minus mu 0 right parenthesis−( ˆμt −μ0), but with varying factor of proportionality, 
p Subscript t Baseline equals b q Subscript t plus 1 Baseline divided by left parenthesis c Subscript upper A Baseline plus q Subscript t plus 1 Baseline b squared right parenthesis periodpt = bqt+1/(cA + qt+1b2).
(3.7.7) 
In Table 3.19, we present the optimal values of p Subscript tpt for the case of c Subscript upper A Baseline equals 100cA = 100, c Subscript d Baseline equals 1000cd =
1000, b equals 1b = 1, and upper T equals 15T = 15. 
c_A = 100 
c_d = 1000 
b = 1  
q_tp1 = c_d 
data = [] 
for t in range(14, 0, -1): 
q_t = c_A * q_tp1 / (c_A + q_tp1 * b**2) 
p_t = b * q_tp1 / (c_A + q_tp1 * b**2) 
data.append({'t': t, 'q_t': q_t, 'p_t': p_t}) 
q_tp1 = q_t 
result = pd.DataFrame(data) 
If the penalty for deviation from the target is cumulative, we wish to minimize 
the total expected penalty function, namely

104
3
Advanced Methods of Statistical Process Control
Table 3.19 Factors of 
proportionality in optimal 
control 
t
q Subscript tqt
p Subscript tpt
15 
–
– 
14 
90.909 
0.909 
13 
47.619 
0.476 
12 
32.258 
0.323 
11 
24.390 
0.244 
10 
19.608 
0.196 
9 
16.393 
0.164 
8 
14.085 
0.141 
7 
12.346 
0.123 
6 
10.989 
0.110 
5 
9.901 
0.099 
4 
9.009 
0.090 
3 
8.264 
0.083 
2 
7.634 
0.076 
1 
7.092 
0.071 
upper J Subscript t Baseline equals c Subscript d Baseline sigma summation Underscript t equals 1 Overscript upper T Endscripts upper E left brace left parenthesis mu Subscript t Baseline minus mu 0 right parenthesis squared right brace plus c Subscript upper A Baseline sigma summation Underscript t equals 0 Overscript upper T minus 1 Endscripts u Subscript t Superscript 2 Baseline periodJt = cd
T

t=1
E{(μt −μ0)2} + cA
T −1

t=0
u2
t .
(3.7.8) 
The optimal solution in this case is somewhat more complicated than the above 
rule, and it is also not one with ﬁxed factor of proportionality p. The method of 
obtaining this solution is called dynamic programming. We do not present here 
this optimization procedure. The interested reader is referred to Aoki (1989). We 
just mention that the optimal solution using this method yields for example that the 
last control is at the level (when b equals 1b = 1) of  
u Subscript upper T minus 1 Superscript 0 Baseline equals minus StartFraction c d Over c Subscript upper A Baseline plus c Subscript d Baseline EndFraction left parenthesis ModifyingAbove mu With caret Subscript upper T minus 1 Baseline minus mu 0 right parenthesis periodu0
T −1 = −
cd
cA + cd
( ˆμT −1 −μ0).
(3.7.9) 
The optimal control at t equals upper T minus 2t = T −2 is 
u Subscript upper T minus 2 Superscript 0 Baseline equals minus StartFraction c d Over c Subscript upper A Baseline plus 2 c Subscript d Baseline EndFraction left parenthesis ModifyingAbove mu With caret Subscript upper T minus 2 Baseline minus mu 0 right parenthesis commau0
T −2 = −
cd
cA + 2cd
( ˆμT −2 −μ0),
(3.7.10) 
and so on. 
We conclude this section mentioning that a simple but reasonable method of 
automatic process control is to use the EWMA chart, and whenever the trend 
estimates, ModifyingAbove mu With caret Subscript t ˆμt, are above or below the upper or lower control limits, then a control is 
applied of size 
u equals minus left parenthesis ModifyingAbove mu With caret Subscript t Baseline minus mu 0 right parenthesis periodu = −( ˆμt −μ0).
(3.7.11)

3.8
Chapter Highlights
105
Fig. 3.13 EWMA chart for average ﬁlm speed in subgroups of n equals 5n = 5 ﬁlm rolls, mu 0 equals 105μ0 = 105, 
sigma equals 6.53σ = 6.53, lamda equals 0.2λ = 0.2
In Fig. 3.13, we present the results of such a control procedure on the ﬁlm speed 
(dataset FILMSP.csv) in a production process of coating ﬁlm rolls. This EWMA 
chart was constructed with mu 0 equals 105μ0 = 105, sigma equals 6.53σ = 6.53, lamda equals 0.2λ = 0.2, upper L equals 2L = 2, and n equals 5n = 5. 
Notice that at the beginning the process was out of control. After a remedial action, 
the process returned to a state of control. At time 30, it drifted downward but was 
corrected again. 
3.8 
Chapter Highlights 
The main concepts and deﬁnitions introduced in this chapter include: 
• 
Run tests 
• 
Average run length 
• 
Operating characteristic functions 
• 
Multivariate control charts 
• 
Cumulative sum control charts 
• 
Bayesian detection 
• 
Shiryaev–Roberts statistic 
• 
Probability of false alarm 
• 
Conditional expected delay

106
3
Advanced Methods of Statistical Process Control
• 
Process tracking 
• 
Exponentially weighted moving average (EWMA) 
• 
Kalman ﬁlter 
• 
Quality measurement plan 
• 
Automatic process control 
• 
Dynamic programming 
3.9 
Exercises 
Exercise 3.1 Generate the distribution of the number of runs in a sample of size 
n = 25, if the number of elements above the sample mean is m2 = 10: 
(i) What are Q1, Me, and Q3 of this distribution? 
(ii) Compute the expected value, μR, and the standard deviation σR. 
(iii) What is Pr{10 ≤R ≤ 16}? 
(iv) Determine the normal approximation to Pr{10 ≤ R ≤ 16}. 
Exercise 3.2 Use Python to perform a run test on the simulated cycle times from 
the pistons, which are in dataset CYCLT.csv. Is the number of runs above the mean 
cycle time signiﬁcantly different than its expected value? 
Exercise 3.3 
(i) What is the expected number of runs up or down, in a sample of size 50? 
(ii) Compute the number of runs up or down in the cycle time data (CYCLT.csv). 
(iii) Is this number signiﬁcantly different than expected? 
(iv) What is the probability that a random sample of size 50 will have at least one 
run of size greater or equal to 5? 
Exercise 3.4 Analyze the observations in YARNSTRG.csv for runs. 
Exercise 3.5 Run the piston simulator at the upper level of the seven control 
parameters and generate 50 samples of size 5. Analyze the output for runs in both 
¯X- and S-charts. 
Exercise 3.6 
(i) Run the piston simulator at the upper level of the seven control parameters, and 
generate 50 samples of size 5 (both ¯X- and S-charts) 
(ii) Repeat the exercise allowing T to change over time (provide a list of T which 
specify the changing ambient temperatur over time) 
(iii) Compare the results in (i) and (ii). with those of Exercise 3.3 
Exercise 3.7 Construct a p-chart for the fraction of defective substrates received 
at a particular point in the production line. One thousand (n = 1000) substrates 
are sampled each week. Remove data for any week for which the process is not 
in control. Be sure to check for runs as well as points outside the control limits.

3.9
Exercises
107
Table 3.20 Dataset for 
Exercise 3.7 
Week 
No. Def. 
Week 
No. Def. 
1
18
16
38 
2
14
17
29 
3
9
18
35 
4
25
19
24 
5
27
20
20 
6
18
21
23 
7
21
22
17 
8
16
23
20 
9
18
24
19 
10
24
25
17 
11
20
26
16 
12
19
27
10 
13
22
28
8 
14
22
29
10 
15
20
30
9 
Construct the revised p-chart and be sure to check for runs again. The data are in 
Table 3.20. 
Exercise 3.8 Substrates were inspected for defects on a weekly basis, on two 
different production lines. The weekly sample sizes and the number of defectives 
are indicated below in the dataset in Table 3.21. Plot the data and indicate which 
of the lines is not in a state of statistical control. On what basis do you make your 
decision? Use Python to construct control charts for the two production lines.
Note When the sample size is not the same for each sampling period, we use 
variable control limits. If X(i) and n(i) represent the number of defects and sample 
size, respectively, for sampling period i, then the upper and lower control limits for 
the ith period are 
upper U upper C upper L Subscript i Baseline equals p overbar plus 3 left parenthesis p overbar left parenthesis 1 minus p overbar right parenthesis divided by n Subscript i Baseline right parenthesis Superscript 1 divided by 2UCLi = ¯p + 3( ¯p(1 −¯p)/ni)1/2
and 
upper L upper C upper L Subscript i Baseline equals p overbar minus 3 left parenthesis p overbar left parenthesis 1 minus p overbar right parenthesis divided by n Subscript i Baseline right parenthesis Superscript 1 divided by 2 Baseline commaLCLi = ¯p −3( ¯p(1 −¯p)/ni)1/2,
where 
p overbar equals sigma summation upper X left parenthesis i right parenthesis divided by sigma summation n left parenthesis i right parenthesis ¯p =

X(i)/

n(i)
is the center line for the control chart.

108
3
Advanced Methods of Statistical Process Control
Table 3.21 Dataset for 
Exercise 3.8 
Line 1
Line 2 
Week 
Xi 
ni
Xi
ni 
1
45 
7920 
135 
2640 
2
72 
6660 
142 
2160 
3
25 
6480 
16 
240 
4
25 
4500
5 
120 
5
33 
5840 
150 
2760 
6
35 
7020 
156 
2640 
7
42 
6840 
140 
2760 
8
35 
8460 
160 
2980 
9
50 
7020 
195 
2880 
10
55 
9900 
132 
2160 
11
26 
9180 
76 
1560 
12
22 
7200 
85 
1680
Exercise 3.9 In designing a control chart for the fraction defectives p, a random 
sample of size n is drawn from the productions of each day (very large lot). How 
large should n be so that the probability of detecting a shift from p0 = 0.01 to 
pt = 0.05, within a 5-day period, will not be smaller than 0.8? 
Exercise 3.10 The data in Table 3.22 represent dock-to-stock cycle times for a 
certain type of shipment (class D). Incoming shipments are classiﬁed according to 
their “type,” which is determined by the size of the item and the shipment, the type 
of handling required, and the destination of the shipment. Samples of ﬁve shipments 
per day are tracked from their initial arrival to their ﬁnal destination, and the time it 
takes for this cycle to be complete is noted. The samples are selected as follows: at 
ﬁve preselected times during the day, the next class D shipment to arrive is tagged, 
and the arrival time and identity of the shipment are recorded. When the shipment 
reaches its ﬁnal destination, the time is again recorded. The difference between these 
times is the cycle time. The cycle time is always recorded for the day of arrival:
(i) Construct ¯X and S-charts from the data. Are any points out of control? Are 
there any trends in the data? If there are points beyond the control limits, 
assume that we can determine special causes for the points, and recalculate 
the control limits, excluding those points that are outside the control limits. 
(ii) Use a t-test to decide whether the mean cycle time for days 21 and 22 was 
signiﬁcantly greater than 45. 
(iii) Make some conjectures about possible causes of unusually long cycle times. 
Can you think of other appropriate data that might have been collected, such 
as the times at which the shipments reached intermediate points in the cycle? 
Why would such data be useful?

3.9
Exercises
109
Table 3.22 Dock-to-stock 
cycle times 
Day 
Times 
1
27 
43 
49 
32 
36 
2
34 
29 
34 
31 
41 
3
36 
32 
48 
35 
33 
4
31 
41 
51 
51 
34 
5
43 
35 
30 
32 
31 
6
28 
42 
35 
40 
37 
7
38 
37 
41 
34 
44 
8
28 
44 
44 
34 
50 
9
44 
36 
38 
44 
35 
10
30 
43 
37 
29 
32 
11
36 
40 
50 
37 
43 
12
35 
36 
44 
34 
32 
13
48 
49 
44 
27 
32 
14
45 
46 
40 
35 
33 
15
38 
36 
43 
38 
34 
16
42 
37 
40 
42 
42 
17
44 
31 
36 
42 
39 
18
32 
28 
42 
39 
27 
19
41 
41 
35 
41 
44 
20
44 
34 
39 
30 
37 
21
51 
43 
36 
50 
54 
22
52 
50 
50 
44 
49 
23
52 
34 
38 
41 
37 
24
40 
41 
40 
23 
30 
25
34 
38 
39 
35 
33
Exercise 3.11 Consider the modiﬁed Shewhart control chart for sample means, 
with a = 3, w = 2, and r = 4. What is the ARL of this procedure when δ = 0, 1, 
2, and the sample size is n = 10? 
Exercise 3.12 Repeat the previous exercise for a = 3, w = 1, r = 15, when n = 5 
and δ = 0.5. 
Exercise 3.13 Write a Python application to simulate ARL and compare the results 
from the simulation to Exercises 3.11 and 3.12. 
Exercise 3.14 Suppose that a shift in the mean is occurring at random, according 
to an exponential distribution with mean of 1 h. The hourly cost is $100 per shift 
of size δ = μ1−μ0 
σ 
. The cost of sampling and testing is d = $10 per item. How 
often should samples of size n = 5 be taken, when shifts of size δ ≥ 1.5 should be 
detected? 
Exercise 3.15 Compute the OC(p) function, for a Shewhart 3-sigma control chart 
for p, based on samples of size n = 20, when p0 = 0.10. (Use the formula for exact 
computations.)

110
3
Advanced Methods of Statistical Process Control
Exercise 3.16 How large should the sample size n be, for a 3-sigma control chart 
for p, if we wish that the probability of detecting a shift from p0 = 0.01 to pt = 
0.05 be 1 − β = 0.90? 
Exercise 3.17 Suppose that a measurement X, of hardness of brackets after heat 
treatment, has a normal distribution. Every hour a sample of n units is drawn and a 
¯X-chart with control limits μ0 ± 3σ/√n is used. Here, μ0 and σ are the assumed 
process mean and standard deviation. The OC function is 
upper O upper C left parenthesis delta right parenthesis equals upper Phi left parenthesis 3 minus delta StartRoot n EndRoot right parenthesis plus upper Phi left parenthesis 3 plus delta StartRoot n EndRoot right parenthesis minus 1 commaOC(δ) = (3 −δ√n) + (3 + δ√n) −1,
where δ = (μ − μ0)/σ is the standardized deviation of the true process mean from 
the assumed one: 
(i) How many hours, on the average, would it take to detect a shift in the process 
mean of size δ = 1, when n = 5? 
(ii) What should be the smallest sample size, n, so that a shift in the mean of size 
δ = 1 would be on the average detected in less than 3 h? 
(iii) One has two options: to sample n1 = 5 elements every hour or to sample 
n2 = 10 elements every 2 h. Which one would you choose? State your criterion 
for choosing between the two options and make the necessary computations. 
Exercise 3.18 Electric circuits are designed to have an output of 220 (volts, DC). 
If the mean output is above 222 (volts DC), you wish to detect such a shift as soon 
as possible. Examine the sample of dataset OELECT.csv for such a shift. For this 
purpose, construct a CUSUM upward scheme with K+ and h+ properly designed 
(consider for h+ the value α = 0.001). Each observation is of sample of size n = 1. 
Is there an indication of a shift in the mean? 
Exercise 3.19 Estimate the probability of false alarm and the conditional expected 
delay in the Poisson case, with a CUSUM scheme. The parameters are λ0 = 15, 
λ+ 
1 = 25 and λ− 
1 = 7. Use α = 0.001, τ = 30. 
Exercise 3.20 A CUSUM control scheme is based on sample means: 
(i) Determine the control parameters K+, h+, K−, h−, when μ0 = 100, μ+ 
1 = 
110, μ− 
1 = 90, σ = 20, n = 5, α = 0.001. 
(ii) Estimate the PFA and CED, when the change-point is at τ = 10, 20, 30. 
(iii) How would the properties of the CUSUM change if each sample size is 
increased from 5 to 20. 
Exercise 3.21 Show that the Shiryaev–Roberts statistic Wn, for detecting a shift in 
a Poisson distribution from a mean λ0 to a mean λ1 = λ0 + δ, is  
upper W Subscript m Baseline equals left parenthesis 1 plus upper W Subscript m minus 1 Baseline right parenthesis upper R Subscript m Baseline commaWm = (1 + Wm−1)Rm,
where W0 ≡0, Rm = exp{−δ + xm log(ρ)}, and ρ = λ1/λ0.

3.9
Exercises
111
Exercise 3.22 Analyze the data in data OELECT, with an EWMA control chart 
with λ = 0.2. 
Exercise 3.23 Analyze the variable diameters in the dataset ALMPIN with an 
EWMA control chart with λ = 0.2. Explain how you would apply the automatic 
process control technique described at the end of Sect. 3.7. 
Exercise 3.24 Construct the Kalman ﬁlter for the Dow–Jones daily index, which is 
given in the dataset DOW1941.

Chapter 4 
Multivariate Statistical Process Control 
Preview As was discussed in Chap. 4 in Modern Statistics (Kenett et al. (Modern 
statistics: a computer-based approach with Python, 1st edn. Springer, Birkhäuser, 
2022)), multivariate observations require special techniques for visualization and 
analysis. This chapter presents techniques for multivariate statistical process control 
(MSPC) based on the Mahalanobis upper T squaredT 2 chart. Like in previous chapters, examples of 
MSPC using Python are provided. Section 4.3 introduces the reader to multivariate 
extensions of process capability indices. These are expansions of the capability 
indices presented in Chap. 2. A special role is played in this context by the concept of 
multivariate tolerance regions (TR). Section 4.4 considers four scenarios for setting 
up and running MSPC: (1) internally derived targets, (2) using an external reference 
sample, (3) externally assigned targets and (4) measurements units considered 
as batches. These four cases cover most practical applications of MSPC. Two 
subsections cover the special cases of measurement units considered as batches 
and a variable decomposition of indices used for process monitoring. Section 4.5 
is a special application of MSPC to the monitoring of bioequivalence of drug 
product dissolution proﬁles. In this application tablets manufactured by a generic 
drug company are compared to the original product at several dissolution times. 
The Food and Drug Administration allows for a gap of at most 15%, a requirement 
that deﬁne multivariate speciﬁcation limits. We show how TR are used in such cases. 
More on multivariate applications in pharmaceuticals will be discussed in Chap. 6 
on Quality by Design. 
4.1 
Introduction 
Univariate control charts track observations on one dimension. Multivariate data is 
much more informative than a collection of one dimensional variables. Simultane-
ously accounting for variation in several variables requires both an overall measure 
Supplementary Information The online version contains supplementary material available at 
(https://doi.org/10.1007/978-3-031-28482-3_4). 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3_4
113

114
4
Multivariate Statistical Process Control
of departure of the observation from the targets as well as an assessment of the data 
covariance structure. Multivariate control charts were developed for that purpose. 
We present here the construction of multivariate control charts with the multivariate 
data on aluminum pins, which were introduced in Chapter 4 of Modern Statistics 
(Kenett et al. 2022b), dataset ALMPIN.csv. 
The following is the methodology for constructing a multivariate control chart. 
We use the ﬁrst 30 cases of the dataset as a base sample. The other 40 observations 
will be used as observations from a production process which we wish to control. 
The observations in the base sample provide estimates of the means, variance and 
covariances of the six variables being measured. Let upper X overbar Subscript i¯Xi denote the mean of variable 
upper X Subscript iXi left parenthesis i equals 1 comma midline horizontal ellipsis comma p right parenthesis(i = 1, · · · , p) in the base sample. 
Let upper S Subscript i jSij denote the covariance between upper X Subscript iXi and upper X Subscript jXj left parenthesis i comma j equals 1 comma midline horizontal ellipsis comma p right parenthesis(i, j = 1, · · · , p), namely 
upper S Subscript i j Baseline equals StartFraction 1 Over n minus 1 EndFraction sigma summation Underscript l equals 1 Overscript n Endscripts left parenthesis upper X Subscript i l Baseline minus upper X overbar Subscript i dot Baseline right parenthesis left parenthesis upper X Subscript j l Baseline minus upper X overbar Subscript j dot Baseline right parenthesis periodSij =
1
n −1
n

l=1
(Xil −¯Xi·)(Xjl −¯Xj·).
(4.1.1) 
Notice that upper S Subscript i iSii is the sample variance of upper X Subscript iXi left parenthesis i equals 1 comma midline horizontal ellipsis comma p right parenthesis(i = 1, · · · , p). Let bold upper SS denote the p times pp × p
covariance matrix, i.e., 
bold upper S equals Start 4 By 4 Matrix 1st Row 1st Column upper S 11 2nd Column upper S 12 3rd Column midline horizontal ellipsis 4th Column upper S Subscript 1 p Baseline 2nd Row 1st Column upper S 21 2nd Column upper S 22 3rd Column midline horizontal ellipsis 4th Column upper S Subscript 2 p Baseline 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column Blank 4th Column vertical ellipsis 4th Row 1st Column upper S Subscript p Baseline 1 Baseline 2nd Column upper S Subscript p Baseline 2 Baseline 3rd Column midline horizontal ellipsis 4th Column upper S Subscript p p Baseline EndMatrix periodS =
⎡
⎢⎢⎢⎣
S11 S12 · · · S1p
S21 S22 · · · S2p
...
...
...
Sp1 Sp2 · · · Spp
⎤
⎥⎥⎥⎦.
(4.1.2) 
Notice that upper S Subscript i j Baseline equals upper S Subscript j iSij = Sji for every i comma ji, j. Thus, bold upper SS is a symmetric and positive deﬁnite 
matrix. 
Let bold upper MM denote the left parenthesis p times 1 right parenthesis(p × 1) vector of sample means, whose transpose is 
bold upper M prime equals left parenthesis upper X overbar Subscript 1 dot Baseline comma midline horizontal ellipsis comma upper X overbar Subscript p dot Baseline right parenthesis periodM′ = ( ¯X1·, · · · , ¯Xp·).
Finally, we compute the inverse of bold upper SS, namely bold upper S Superscript negative 1S−1. This inverse exists, unless one 
(or some) of the variable(s) is (are) linear combinations of the others. Such variables 
should be excluded. 
Suppose now that every time unit we draw a sample of size m left parenthesis m greater than or equals 1 right parenthesis(m ≥1) from the 
production process, and observe on each element the p variables of interest. In order 
to distinguish between the sample means from the production process to those of the 
base sample, we will denote by ModifyingAbove upper Y With bar Subscript i dot Baseline left parenthesis t right parenthesis ¯Yi·(t), t equals 1 comma 2 comma midline horizontal ellipsist = 1, 2, · · · the sample mean of variable upper X Subscript iXi
from the sample at time t. Let bold upper Y left parenthesis t right parenthesisY(t) be the vector of these p means, i.e., bold upper Y prime left parenthesis t right parenthesis equals left parenthesis ModifyingAbove upper Y With bar Subscript 1 dot Baseline left parenthesis t right parenthesis comma midline horizontal ellipsis comma ModifyingAbove upper Y With bar Subscript p dot Baseline left parenthesis t right parenthesis right parenthesisY′(t) =
( ¯Y1·(t), · · · , ¯Yp·(t)). We construct now a control chart, called the bold upper T squaredT2-Chart. The  
objective is to monitor the means bold upper Y left parenthesis t right parenthesisY(t), of the samples from the production process, 
to detect when a signiﬁcant change from bold upper MM occurs. We assume that the covariances 
do not change in the production process. Thus, for every time period t, t equals 1 comma 2 comma midline horizontal ellipsist = 1, 2, · · ·
we compute the upper T squaredT 2 statistics

4.1
Introduction
115
bold upper T Subscript t Superscript 2 Baseline equals left parenthesis bold upper Y left parenthesis t right parenthesis minus bold upper M right parenthesis prime bold upper S Superscript negative 1 Baseline left parenthesis bold upper Y left parenthesis t right parenthesis minus bold upper M right parenthesis periodT2
t = (Y(t) −M)′S−1(Y(t) −M).
(4.1.3) 
It can be shown that as long as the process mean and covariance matrix are the same 
as those of the base sample, 
upper T squared tilde StartFraction left parenthesis n minus 1 right parenthesis p Over n minus p EndFraction upper F left bracket p comma n minus p right bracket periodT 2 ∼(n −1)p
n −p F[p, n −p].
(4.1.4) 
Accordingly, we set up the (upper) control limit for upper T squaredT 2 at 
UCL equals StartFraction left parenthesis n minus 1 right parenthesis p Over n minus p EndFraction upper F 0.997 left bracket p comma n minus p right bracket periodUCL = (n −1)p
n −p F0.997[p, n −p].
(4.1.5) 
If a point upper T left parenthesis t right parenthesisT (t) falls above this control limit, there is an indication of a signiﬁcant 
change in the mean vector in the baseline data and, after investigations, we might 
decide to remove such points. 
After establishing the baseline control limits, the UCL used in follow up 
monitoring is computed so as to account for the number of observations in the 
baseline phase. The UCL for monitoring is: 
UCL equals StartFraction left parenthesis n minus 1 right parenthesis left parenthesis n plus 1 right parenthesis p Over n left parenthesis n minus p right parenthesis EndFraction upper F 0.997 left bracket p comma n minus p right bracket periodUCL = (n −1)(n + 1)p
n(n −p)
F0.997[p, n −p].
(4.1.6) 
Example 4.1 The base sample consists of the ﬁrst 30 rows of dataset ALMPIN.csv. 
The mean vector of the base sample is 
bold upper M prime equals left parenthesis 9.99 comma 9.98 comma 9.97 comma 14.98 comma 49.91 comma 60.05 right parenthesis periodM′ = (9.99, 9.98, 9.97, 14.98, 49.91, 60.05).
The covariance matrix of the base sample is bold upper SS, where 10 cubed bold upper S103S is 
Start 6 By 6 Matrix 1st Row 1st Column 0.1826 2nd Column 0.1708 3rd Column 0.1820 4th Column 0.1826 5th Column negative 0.0756 6th Column negative 0.0054 2nd Row 1st Column Blank 2nd Column 0.1844 3rd Column 0.1853 4th Column 0.1846 5th Column negative 0.1002 6th Column negative 0.0377 3rd Row 1st Column Blank 2nd Column Blank 3rd Column 0.2116 4th Column 0.1957 5th Column negative 0.0846 6th Column 0.0001 4th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column 0.2309 5th Column negative 0.0687 6th Column negative 0.0054 5th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column 1.3179 6th Column 1.0039 6th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column 1.4047 EndMatrix
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0.1826 0.1708 0.1820 0.1826 −0.0756 −0.0054
0.1844 0.1853 0.1846 −0.1002 −0.0377
0.2116 0.1957 −0.0846 0.0001
0.2309 −0.0687 −0.0054
1.3179
1.0039
1.4047
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
(Since bold upper SS is symmetric we show only the upper matrix). The inverse of bold upper SS is 
bold upper S Superscript negative 1 Baseline equals Start 6 By 6 Matrix 1st Row 1st Column 53191.3 2nd Column negative 22791.0 3rd Column negative 17079.7 4th Column negative 9343.4 5th Column 145.0 6th Column negative 545.3 2nd Row 1st Column Blank 2nd Column 66324.2 3rd Column negative 28342.7 4th Column negative 10877.9 5th Column 182.0 6th Column 1522.8 3rd Row 1st Column Blank 2nd Column Blank 3rd Column 50553.9 4th Column negative 6467.9 5th Column 853.1 6th Column negative 1465.1 4th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column 25745.6 5th Column negative 527.5 6th Column 148.6 5th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column 1622.3 6th Column negative 1156.1 6th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column 1577.6 EndMatrix periodS−1 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
53191.3 −22791.0 −17079.7 −9343.4
145.0
−545.3
66324.2 −28342.7 −10877.9 182.0
1522.8
50553.9
−6467.9
853.1 −1465.1
25745.6 −527.5
148.6
1622.3 −1156.1
1577.6
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.

116
4
Multivariate Statistical Process Control
We compute now for the last 40 rows of this dataset the upper T Subscript t Superscript 2T 2
t values. We consider as 
though each one of these rows is a vector of a sample of size one taken every 10 
min. In Table 4.1, we present these 40 vectors and their corresponding upper T squaredT 2 values. black medium square■
For example, upper T 1 squaredT 2
1 of the table is computed according to the formula 
bold upper T 1 squared equals left parenthesis bold upper Y left parenthesis 1 right parenthesis minus bold upper M right parenthesis prime bold upper S Superscript negative 1 Baseline left parenthesis bold upper Y left parenthesis 1 right parenthesis minus bold upper M right parenthesis equals 3.523 periodT2
1 = (Y(1) −M)′S−1(Y(1) −M) = 3.523.
The 40 values of upper T Subscript t Superscript 2T 2
t of Table 4.1 are plotted in Fig. 4.1. The UCL in this chart is 
UCL equals= 34.56. 
We remark here that the computations can be performed by the following Python 
program. Note that MultivariateQualityControlChart labels the control limit for the 
base (the ﬁrst 30 observations), UCL, and for the ongoing monitoring (the following 
40 observations), UPL. 
almpin = mistat.load_data('ALMPIN') 
base = almpin.iloc[:30,] 
newdata = almpin.iloc[30:,] 
mqcc = mistat.MultivariateQualityControlChart(base, qcc_type='T2single', 
confidence_level=0.997, newdata=newdata) 
mqcc.plot() 
plt.show()
4.2 
A Review Multivariate Data Analysis 
Chapters 2 and 3 present applications of statistical process control (SPC) to mea-
surements in one dimension. To extend the approach to measurements consisting 
of several dimensions lead us to multivariate statistical process control (MSPC), 
as introduced in Sect. 4.1. MSPC requires applications of methods and tools of 
multivariate data analysis presented in Chapter 4 of Modern Statistics (Kenett et al. 
2022b). In this section we expand on the material presented so far and use the 
components placement data of Examples 4.1 and 4.2. The case study consists of 
displacement co-ordinates of 16 components placed by a robot on a printed circuit 
board. Overall, there are 26 printed circuit boards and therefore a total of 416 placed 
components (see PLACE.csv). The components’ co-ordinates are measured in three 
dimensions, representing deviations with respect to the target in the horizontal, 
vertical and angular dimensions. The measured variables are labeled xDev, yDev, 
and tDev. The placement of components on the 26 boards was part of a validation 
test designed to ﬁne tune the placement software in order to minimize the placement 
deviations. 
Figure 4.2 presents a scatterplot matrix of xDev, yDev, and tDev with non-
parametric densities providing a visual display of the two dimensional distribution 
densities. On the yDev xDev scatterplot there are clearly three groups of boards. In

4.2
A Review Multivariate Data Analysis
117
Table 4.1 Dimensions of 
aluminum pins in a 
production process and their 
upper T squaredT 2 value 
upper X 1X1
upper X 2X2
upper X 3X3
upper X 4X4
upper X 5X5
upper X 6X6
upper T squaredT 2
10.00
9.99
9.99 
14.99 
49.92 
60.03
3.523 
10.00
9.99
9.99 
15.00 
49.93 
60.03
6.983 
10.00 
10.00
9.99 
14.99 
49.91 
60.02
6.411 
10.00
9.99
9.99 
14.99 
49.92 
60.02
4.754 
10.00
9.99
9.99 
14.99 
49.92 
60.00
8.161 
10.00 
10.00
9.99 
15.00 
49.94 
60.05
7.605 
10.00
9.99
9.99 
15.00 
49.89 
59.98 
10.299 
10.00 
10.00
9.99 
14.99 
49.93 
60.01 
10.465 
10.00 
10.00
9.99 
14.99 
49.94 
60.02 
10.771 
10.00 
10.00
9.99 
15.00 
49.86 
59.96 
10.119 
10.00
9.99
9.99 
14.99 
49.90 
59.97 
11.465 
10.00 
10.00 
10.00 
14.99 
49.92 
60.00 
14.317 
10.00 
10.00
9.99 
14.98 
49.91 
60.00 
13.675 
10.00 
10.00 
10.00 
15.00 
49.93 
59.98 
20.168 
10.00
9.99
9.98 
14.98 
49.90 
59.98
8.985 
9.99
9.99
9.99 
14.99 
49.88 
59.98
9.901 
10.01 
10.01 
10.01 
15.01 
49.87 
59.97 
14.420 
10.00 
10.00
9.99 
14.99 
49.81 
59.91 
15.998 
10.01 
10.00 
10.00 
15.01 
50.07 
60.13 
30.204 
10.01 
10.00 
10.00 
15.00 
49.93 
60.00 
12.648 
10.00 
10.00 
10.00 
14.99 
49.90 
59.96 
19.822 
10.01 
10.01 
10.01 
15.00 
49.85 
59.93 
21.884 
10.00
9.99
9.99 
15.00 
49.83 
59.98
9.535 
10.01 
10.01 
10.00 
14.99 
49.90 
59.98 
18.901 
10.01 
10.01 
10.00 
15.00 
49.87 
59.96 
13.342 
10.00
9.99
9.99 
15.00 
49.87 
60.02
5.413 
9.99
9.99
9.99 
14.98 
49.92 
60.03
8.047 
9.99
9.98
9.98 
14.99 
49.93 
60.03
5.969 
9.99
9.99
9.98 
14.99 
49.89 
60.01
4.645 
10.00 
10.00
9.99 
14.99 
49.89 
60.01
5.674 
9.99
9.99
9.99 
15.00 
50.04 
60.15 
23.639 
10.00 
10.00 
10.00 
14.99 
49.84 
60.03 
10.253 
10.00 
10.00
9.99 
14.99 
49.89 
60.01
5.674 
10.00
9.99
9.99 
15.00 
49.88 
60.01
5.694 
10.00 
10.00
9.99 
14.99 
49.90 
60.04
4.995 
9.90
9.89
9.91 
14.88 
49.99 
60.14 
82.628 
10.00
9.99
9.99 
15.00 
49.91 
60.04
4.493 
9.99
9.99
9.99 
14.98 
49.92 
60.04
7.211 
10.01 
10.01 
10.00 
15.00 
49.88 
60.00
8.737 
10.00
9.99
9.99 
14.99 
49.95 
60.10
3.421

118
4
Multivariate Statistical Process Control
Fig. 4.1 
upper T squaredT 2-chart for aluminum pins
Section 4.1.2 of Modern Statistics (Kenett et al. 2022b) we identiﬁed them with box 
plots and conﬁrmed the classiﬁcation with coding and redrawing the scatterplot. 
Figure 4.3 presents the histograms of xDev, yDev and tDev with the low 
values of xDev highlighted. Through dynamic linking, the ﬁgure highlights, the 
corresponding values for yDev and tDev. We can see that components position on 
the left of the target tend to be also placed below the target, with some components 
in this group being positioned on target in the vertical direction (the group of yDev 
between negative 0.001−0.001 and 0.0020.002).
To further characterize the components placed on the left of the target (negative 
xDev) we draw a plot of xDev and yDev versus the board number (Fig. 4.4). On each 
board we get 16 measurements of xDev (circle) and yDev (cross). The highlighted 
points correspond to the highlighted values in Fig. 4.3. One can see from Fig. 4.4 that 
up to board number 9 we have components placed to the right and below the target. 
In boards 10, 11 and 12 there has been a correction in component placement which 
resulted in components being placed above the target in the vertical direction and 
on target in the horizontal direction. These are the components in the histogram of 
yDev in Fig. 4.3 with the high trailing values between negative 0.001−0.001 and 0.0020.002 mentioned 
above. For these ﬁrst 12 circuit boards we do not notice any speciﬁc pattern in 
tDev. We will use the placement data example again in Sect. 4.4 to demonstrate the 
application of a multivariate control chart.
Software such as Python or R provide visualization and exploration technologies 
that complement the application of MSPC. Specialized packages like plotly or 
Dash allow the development of dashboards for interactive data analysis.

4.2
A Review Multivariate Data Analysis
119
Fig. 4.2 Scatterplot matrix of placement data with nonparametric densities
Fig. 4.3 Histograms of xDev, yDev and tDev, with conditional linking

120
4
Multivariate Statistical Process Control
Fig. 4.4 Plot of xDev and yDev versus circuit board number
4.3 
Multivariate Process Capability Indices 
Chapter 4.4 introduced process capability studies that are a prerequisite to the 
setup of control limits in control charts. Section 2.4 presented several univariate 
process capability indices such as upper C Subscript pCp and upper C Subscript p kCpk that are used to characterize the 
performance of a process by comparing the quality attributes speciﬁcations to the 
process variability. These indices map processes in terms of their ability to deliver 
high critical quality parameters. In this section we focus on multivariate data and 
develop several multivariate process capability indices. 
As in the univariate case, the multivariate capability indices are based on the 
multivariate normal distribution. Chapter 2 of Modern Statistics (Kenett et al. 
2022b) introduces the bivariate normal distribution. We introduce here the m-
variate normal distribution as a joint distribution of a vector bold upper X prime equals left parenthesis upper X 1 comma ellipsis comma upper X Subscript m Baseline right parenthesisX′ = (X1, . . . , Xm)
of m random variable. The expected value of such a vector is the vector bold italic mu prime equals left parenthesis upper E left brace upper X 1 right brace comma ellipsis comma upper E left brace upper X Subscript m Baseline right brace right parenthesisμ′ =
(E{X1}, . . . , E{Xm}). The covariance matrix of this vector is an m times mm×m, symmetric, 
positive deﬁnite matrix Bar Summation left parenthesis sigma Subscript i j Baseline semicolon i comma j equals 1 comma ellipsis comma m right parenthesis | =
	
σij; i, j = 1, . . . , m

, where sigma Subscript i j Baseline equals cov left parenthesis upper X Subscript i Baseline comma upper X Subscript j Baseline right parenthesisσij = cov(Xi, Xj). The  
multivariate normal vector is denoted by upper N left parenthesis bold italic mu comma Bar SummationN(μ, |). The joint p.d.f. of bold upper XX is 
f left parenthesis bold x semicolon bold italic mu comma Bar Summation equals StartFraction 1 Over left parenthesis 2 pi right parenthesis Superscript m divided by 2 Baseline vertical bar Bar Summation Superscript 1 divided by 2 Baseline EndFraction exp left brace minus one half left parenthesis bold x minus bold italic mu right parenthesis Superscript prime Baseline Bar Summation negative 1 left parenthesis bold x minus bold italic mu right parenthesis right brace periodf (x; μ, |) =
1
(2π)m/2| ||1/2 exp

−1
2(x −μ)′ |−1(x −μ)

.
(4.3.1) 
In the multivariate normal distribution, the marginal distribution of upper X Subscript iXi is normal 
upper N left parenthesis mu Subscript i Baseline comma sigma Subscript i Superscript 2 Baseline right parenthesisN(μi, σ 2
i ), i equals 1 comma ellipsis comma mi = 1, . . . , m. 
We describe now some of the multivariate capability indices. The reader is 
referred to papers of Chen (1994), Haridy et al. (2011) and Jalili et al. (2012). 
A tolerance region, TR,

4.3
Multivariate Process Capability Indices
121
is a region around a target point, bold upper TT. We will consider here two possible regions: 
(i) a hyper-rectangular, RTR, and (ii) a sphere, CTR. In case (i) the region is 
speciﬁed by parameters left parenthesis delta 1 comma ellipsis comma delta Subscript m Baseline right parenthesis(δ1, . . . , δm) and is the set 
RTR equals left brace bold x colon StartAbsoluteValue x Subscript i Baseline minus upper T Subscript i Baseline EndAbsoluteValue less than or equals delta Subscript i Baseline comma i equals 1 comma ellipsis comma m right brace periodRTR = {x : |xi −Ti| ≤δi, i = 1, . . . , m}.
(4.3.2) 
In case (ii) the tolerance region is 
CTR equals StartSet bold x colon StartAbsoluteValue bold upper X minus bold upper T EndAbsoluteValue less than or equals r EndSet commaCTR = {x : |X −T| ≤r},
(4.3.3) 
where r is the radius of a sphere centered at bold upper TT. 
In the paper of Jalili et al. (2012) a tolerance region, which is the largest 
ellipsoidal region in the RTR (4.3.2), is considered. In the present section we 
will conﬁne attention to (4.3.2) and (4.3.3). Chen (1994) suggested the following 
multivariate capability index (MCSubscript pp). Suppose that the TR is CTR, with a speciﬁed 
radius r. Let r Subscript alpharα be the value of r, for which upper P left brace StartAbsoluteValue bold upper X minus upper T EndAbsoluteValue less than or equals r Subscript alpha Baseline right brace equals 1 minus alphaP{|X −T | ≤rα} = 1 −α, with 
alpha equals 0.0027α = 0.0027. Then the MCSubscript pp index is 
MC Subscript p Baseline equals StartFraction r Over r Subscript alpha Baseline EndFraction periodMCp = r
rα
.
(4.3.4) 
In the case of a RTR, Chen suggested the index 
MC Subscript p Baseline equals StartFraction delta Subscript s Baseline Over delta Subscript alpha Baseline EndFraction commaMCp = δs
δα
,
(4.3.5) 
where delta Subscript alphaδα is the value for which 
upper P left brace max left brace StartFraction StartAbsoluteValue upper X Subscript i Baseline minus upper T Subscript i Baseline EndAbsoluteValue Over delta Subscript i Baseline EndFraction comma i equals 1 comma ellipsis comma m right brace less than or equals delta Subscript alpha Baseline right brace equals 1 minus alpha periodP

max
|Xi −Ti|
δi
, i = 1, . . . , m

≤δα

= 1 −α.
bold italic delta Subscript s Baseline equals max left brace delta Subscript i Baseline comma i equals 1 comma ellipsis comma m right braceδs = max{δi, i = 1, . . . , m}. We will show later how to compute these indices in the 
case of m equals 2m = 2. In the special case where bold upper X tilde upper N left parenthesis bold upper T comma sigma squared upper I right parenthesisX ∼N(T, σ 2I), i.e., all the m components 
of bold upper XX are independent with mu Subscript i Baseline equals upper T Subscript iμi = Ti and sigma Subscript i Superscript 2 Baseline equals sigma squaredσ 2
i = σ 2 we have a simple solution. If 
delta Subscript i Baseline equals deltaδi = δ for all i, then 
max left brace StartFraction StartAbsoluteValue upper X Subscript i Baseline minus upper T Subscript i Baseline EndAbsoluteValue Over delta EndFraction comma i equals 1 comma ellipsis comma m right brace tilde left parenthesis StartFraction sigma Over delta EndFraction right parenthesis StartAbsoluteValue upper Z EndAbsoluteValue Subscript left parenthesis m right parenthesis Baseline commamax
|Xi −Ti|
δ
, i = 1, . . . , m

∼
σ
δ

|Z|(m),
where StartAbsoluteValue upper Z EndAbsoluteValue Subscript left parenthesis m right parenthesis Baseline equals max left brace StartAbsoluteValue upper Z Subscript i Baseline EndAbsoluteValue comma i equals 1 comma ellipsis comma m right brace|Z|(m) = max{|Zi|, i = 1, . . . , m} and upper Z tilde upper N left parenthesis 0 comma 1 right parenthesisZ ∼N(0, 1). Thus 
StartLayout 1st Row 1st Column upper P left brace StartFraction sigma Over delta EndFraction StartAbsoluteValue upper Z EndAbsoluteValue Subscript left parenthesis m right parenthesis Baseline less than or equals y right brace 2nd Column equals upper P left brace StartAbsoluteValue upper Z EndAbsoluteValue less than or equals y delta divided by sigma right brace Superscript m Baseline 2nd Row 1st Column Blank 2nd Column equals left parenthesis 2 upper Phi left parenthesis StartFraction y delta Over sigma EndFraction right parenthesis minus 1 right parenthesis Superscript m Baseline period EndLayout
P
σ
δ |Z|(m) ≤y

= P{|Z| ≤yδ/σ}m
=

2
yδ
σ

−1
m
.

122
4
Multivariate Statistical Process Control
Hence, delta Subscript alphaδα is a solution of left parenthesis 2 upper Phi left parenthesis StartFraction delta Subscript alpha Baseline delta Over sigma EndFraction right parenthesis minus 1 right parenthesis Superscript m Baseline equals 1 minus alpha

2

δαδ
σ

−1
m
= 1 −α, or  
delta Subscript alpha Baseline equals StartFraction sigma Over delta EndFraction upper Phi Superscript negative 1 Baseline left parenthesis one half plus one half left parenthesis 1 minus alpha right parenthesis Superscript 1 divided by m Baseline right parenthesis periodδα = σ
δ −1
1
2 + 1
2(1 −α)1/m

.
(4.3.6) 
Haridy et al. (2011) suggested a different type of index, based on the principal 
components of Bar Summation|. 
Let upper H primeH ′ be an orthogonal matrix, whose column vectors are the orthogonal 
eigenvectors of Bar Summation|. Let lamda 1 greater than or equals lamda 2 greater than or equals midline horizontal ellipsis greater than or equals lamda Subscript m Baseline greater than 0λ1 ≥λ2 ≥· · · ≥λm > 0 be the corresponding eigenvalues. 
Recall that 
upper H Bar Summation Superscript prime Baseline equals Start 3 By 3 Matrix 1st Row 1st Column lamda 1 2nd Column Blank 3rd Column 0 2nd Row 1st Column Blank 2nd Column down right diagonal ellipsis 3rd Column Blank 3rd Row 1st Column 0 2nd Column Blank 3rd Column lamda Subscript m Baseline EndMatrix periodH |H ′ =
⎛
⎜⎝
λ1
0
...
0
λm
⎞
⎟⎠.
(4.3.7) 
The transformed vector 
bold upper Y equals upper H left parenthesis bold upper X minus bold italic mu right parenthesisY = H(X −μ)
(4.3.8) 
is called the principal component vector. 
The distribution of bold upper YY is that of upper N left parenthesis bold 0 comma upper Lamda right parenthesisN(0, ), where upper Lamda equals diag StartSet lamda Subscript i Baseline comma i equals 1 comma ellipsis comma m EndSet = diag{λi, i = 1, . . . , m}. 
Here lamda Subscript iλi is the variance of upper Y Subscript iYi. Also, upper Y 1 comma ellipsis comma upper Y Subscript m BaselineY1, . . . , Ym are independent. 
The vector of upper speciﬁcation limits in the RTR is bold upper U equals bold upper T plus bold italic deltaU = T + δ. The  
corresponding vector of lower speciﬁcation limits is bold upper L equals bold upper T minus bold italic deltaL = T −δ. These vectors 
are transformed into bold upper U Superscript asterisk Baseline equals upper H bold italic deltaU∗= Hδ and bold upper L Superscript asterisk Baseline equals minus upper H bold italic deltaL∗= −Hδ. 
Suppose that bold upper X 1 comma ellipsis comma bold upper X Subscript n BaselineX1, . . . , Xn is a random sample from the process. These n vectors 
are independent and identically distributed. The maximum likelihood estimator of 
bold italic muμ is bold upper M equals StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts bold upper X Subscript iM =
1
n
n
i=1 Xi. An estimator of Bar Summation| is the sample covariance matrix S 
(see (4.1.1)). Let ModifyingAbove upper H With caret prime ˆH ′ and ModifyingAbove upper Lamda With caretˆ be the corresponding matrices of eigenvectors and 
eigenvalues of S. The estimated vectors of the principal components are 
ModifyingAbove bold upper Y With caret Subscript i Baseline equals ModifyingAbove upper H With caret left parenthesis bold upper X Subscript i Baseline minus bold upper M right parenthesis comma i equals 1 comma ellipsis comma n period ˆYi = ˆH(Xi −M),
i = 1, . . . , n.
(4.3.9) 
Let StartSet ModifyingAbove upper Y With caret Subscript 1 j Baseline comma midline horizontal ellipsis comma ModifyingAbove upper Y With caret Subscript n comma j Baseline EndSet{ ˆY1j, · · · , ˆYn,j} be the sample of the j-th left parenthesis j equals 1 comma ellipsis comma m right parenthesis(j = 1, . . . , m) principal components. 
Let 
upper C Subscript p comma p c Sub Subscript j Subscript Baseline equals StartFraction upper U Superscript asterisk Baseline minus upper L Superscript asterisk Baseline Over 6 ModifyingAbove sigma With caret Subscript y Sub Subscript j Subscript Baseline EndFraction comma j equals 1 comma ellipsis comma m commaCp,pcj = U∗−L∗
6ˆσyj
,
j = 1, . . . , m,
(4.3.10) 
where ModifyingAbove sigma With caret Subscript y Sub Subscript j Superscript 2 Baseline equals StartFraction 1 Over n minus 1 EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis ModifyingAbove upper Y With caret Subscript i j Baseline minus upper Y overbar Subscript j Baseline right parenthesis squaredˆσ 2
yj
=
1
n−1
n
i=1( ˆYij −¯Yj)2. The MCP index, based on the principal 
components is 
MCP equals left parenthesis product Underscript j equals 1 Overscript m Endscripts upper C Subscript p comma p c Sub Subscript j Subscript Baseline right parenthesis Superscript 1 divided by m Baseline periodMCP =
⎛
⎝
m

j=1
Cp,pcj
⎞
⎠
1/m
.
(4.3.11)

4.3
Multivariate Process Capability Indices
123
We derive now explicit formula for a RTR, when m equals 2m = 2 (bivariate normal 
distribution). The distribution of bold upper XX is 
upper N left parenthesis StartBinomialOrMatrix xi Choose q u a d eta EndBinomialOrMatrix comma Start 2 By 2 Matrix 1st Row 1st Column sigma 1 squared 2nd Column rho sigma 1 sigma 2 2nd Row 1st Column bullet 2nd Column sigma 2 squared EndMatrix right parenthesis periodN
ξ
η

,
σ 2
1 ρσ1σ2
•
σ 2
2

.
The conditional distribution of upper X 2X2, given upper X 1X1, is upper N left parenthesis eta plus rho StartFraction sigma 2 Over sigma 1 EndFraction left parenthesis upper X 1 minus xi right parenthesis comma sigma 2 squared left parenthesis 1 minus rho squared right parenthesis right parenthesisN

η + ρ σ2
σ1 (X1 −ξ), σ 2
2 (1 −ρ2)

. 
Accordingly, the probability that bold upper XX belongs to the rectangular tolerance region, 
RTR, with speciﬁed bold italic delta equals left parenthesis delta 1 comma delta 2 right parenthesis primeδ = (δ1, δ2)′, is  
StartLayout 1st Row 1st Column Blank 2nd Column upper P left brace bold upper X element of RTR left parenthesis bold italic delta right parenthesis right brace 2nd Row 1st Column Blank 2nd Column equals integral Subscript upper T 1 minus delta 1 minus xi 1 divided by sigma 1 Superscript left parenthesis upper T 1 plus delta 1 minus xi 1 right parenthesis divided by sigma 1 Baseline phi left parenthesis z right parenthesis left bracket upper Phi left parenthesis StartStartFraction upper T 2 plus delta 2 minus left parenthesis eta plus rho StartFraction sigma 2 Over sigma 1 EndFraction left parenthesis z minus left parenthesis xi minus upper T 1 right parenthesis right parenthesis right parenthesis OverOver sigma 2 left parenthesis 1 minus rho squared right parenthesis Superscript 1 divided by 2 Baseline EndEndFraction right parenthesis 3rd Row 1st Column Blank 2nd Column minus upper Phi left parenthesis StartStartFraction upper T 2 minus delta 2 minus left parenthesis eta plus rho StartFraction sigma 2 Over sigma 1 EndFraction left parenthesis z minus left parenthesis xi minus upper T 1 right parenthesis right parenthesis OverOver sigma 2 left parenthesis 1 minus rho squared right parenthesis Superscript 1 divided by 2 Baseline EndEndFraction right parenthesis right bracket normal d z period EndLayout
P{X ∈RTR(δ)}
=
 (T1+δ1−ξ1)/σ1
T1−δ1−ξ1/σ1
φ(z)



T2 + δ2 −(η + ρ σ2
σ1 (z −(ξ −T1)))
σ2(1 −ρ2)1/2
 
−

T2 −δ2 −(η + ρ σ2
σ1 (z −(ξ −T1))
σ2(1 −ρ2)1/2
 !
dz.
(4.3.12) 
In particular, if upper T 1 equals xiT1 = ξ and upper T 2 equals etaT2 = η then 
StartLayout 1st Row upper P left brace bold upper X element of RTR left parenthesis bold italic delta right parenthesis right brace equals integral Subscript minus StartFraction delta 1 Over sigma 1 EndFraction Superscript StartFraction delta 1 Over sigma 1 EndFraction Baseline phi left parenthesis z right parenthesis left bracket upper Phi left parenthesis StartFraction sigma 1 delta 2 minus rho sigma 2 z Over sigma 1 sigma 2 left parenthesis 1 minus rho squared right parenthesis Superscript 1 divided by 2 Baseline EndFraction right parenthesis 2nd Row minus upper Phi left parenthesis StartFraction minus sigma 1 delta 2 minus rho sigma 2 z Over sigma 1 sigma 2 left parenthesis 1 minus rho squared right parenthesis Superscript 1 divided by 2 Baseline EndFraction right parenthesis right bracket normal d z period EndLayoutP{X ∈RTR(δ)} =

δ1
σ1
−δ1
σ1
φ(z)



σ1δ2 −ρσ2z
σ1σ2(1 −ρ2)1/2

−
 −σ1δ2 −ρσ2z
σ1σ2(1 −ρ2)1/2

dz.
(4.3.13) 
phi left parenthesis z right parenthesis equals StartFraction 1 Over StartRoot 2 pi EndRoot EndFraction exp left parenthesis minus one half z squared right parenthesisφ(z) =
1
√
2π exp(−1
2z2) is the standard normal density. 
If the tolerance region is circular, CRCleft parenthesis r right parenthesis(r), with radius r, left parenthesis xi comma eta right parenthesis equals left parenthesis upper T 1 comma upper T 2 right parenthesis equals bold 0(ξ, η) = (T1, T2) = 0, 
then 
upper P left brace bold upper X element of CRC left parenthesis r right parenthesis right brace equals upper P left brace lamda 1 Superscript negative 1 Baseline upper Y 1 squared plus lamda 2 Superscript negative 1 Baseline upper Y 2 squared less than or equals r right brace commaP{X ∈CRC(r)} = P{λ−1
1 Y 2
1 + λ−1
2 Y 2
2 ≤r},
(4.3.14) 
where lamda 1λ1 and lamda 2λ2 are the eigenvalues of Bar Summation Start 2 By 2 Matrix 1st Row 1st Column sigma 1 squared 2nd Column rho sigma 1 sigma 2 2nd Row 1st Column bullet 2nd Column sigma 2 squared EndMatrix | =
σ 2
1 ρσ1σ2
•
σ 2
2

, and upper Y 1 squaredY 2
1 , upper Y 2 squaredY 2
2 are 
independent, having a chi squared left bracket 1 right bracketχ2[1] distribution. Thus, 
upper P left brace bold upper X element of CRC left parenthesis r right parenthesis right brace equals StartFraction 1 Over StartRoot 2 pi EndRoot EndFraction integral Subscript 0 Superscript StartRoot lamda r EndRoot Baseline x Superscript negative 1 divided by 2 Baseline e Superscript negative x divided by 2 Baseline dot left bracket 2 upper Phi left parenthesis left parenthesis lamda 1 comma lamda 2 r squared minus lamda 2 x right parenthesis Superscript 1 divided by 2 Baseline right parenthesis minus 1 right bracket normal d x periodP{X ∈CRC(r)} =
1
√
2π
 √
λr
0
x−1/2e−x/2 · [2((λ1, λ2r2 −λ2x)1/2) −1] dx.
(4.3.15) 
Example 4.2 We compute now the capability index MCSubscript pp according to (4.3.5), for  
the ALMPIN.csv dataset. We restrict attention to the last two variables in the 
dataset, i.e., LengthNocp and LengthWcp. The sample consists of the ﬁrst

124
4
Multivariate Statistical Process Control
30 data vectors, described in Sect. 4.1. We use Eq. (4.3.13) with the r equals 0.7377r = 0.7377, 
sigma 1 squared equals 1.3179σ 2
1 = 1.3179, sigma 2 squared equals 1.4047σ 2
2 = 1.4047. We assume that upper T 1 equals xi equals 49.91T1 = ξ = 49.91 and upper T 2 equals eta equals 60.05T2 = η = 60.05. 
From Eq. (4.3.13) we ﬁnd that for bold italic delta equals left parenthesis 3.5 comma 3.5 right parenthesis primeδ = (3.5, 3.5)′ upper P left brace bold upper X element of RTR left parenthesis bold italic delta right parenthesis right brace equals 0.9964P{X ∈RTR(δ)} = 0.9964. If the  
tolerance region is speciﬁed by bold italic delta Subscript s Baseline equals left parenthesis 1.5 comma 1.5 right parenthesis primeδs = (1.5, 1.5)′ then we get index upper M upper C Subscript p Baseline equals StartFraction 1.5 Over 3.5 EndFraction equals 0.4286MCp = 1.5
3.5 =
0.4286. 
If the tolerance region is circular, we get according to (4.3.15) r Subscript alpha Baseline equals 4.5rα = 4.5. Thus, 
upper M upper C Subscript p Baseline equals StartFraction 1.5 Over 4.5 EndFraction equals 0.333MCp =
1.5
4.5 = 0.333. We compute now the upper M upper C Subscript upper PMCP index (4.3.11), for the 6-
dimensional vector of the ALMPIN.csv dataset. We base the estimation on the last 
37 vectors of the dataset. Notice that the expected value of the principal components 
bold upper YY is zero. Hence, we consider the upper tolerance limit for Y to be upper U upper S upper L equals 0.015USL = 0.015
and the lower tolerance limit to be upper L upper S upper L equals negative 0.015LSL = −0.015. For these speciﬁcations 
we get upper M upper C Subscript upper P Baseline equals 0.4078MCP = 0.4078. If we increase the tolerance limits to plus or minus 0.03±0.03 we obtain 
upper M upper C Subscript upper P Baseline equals 0.8137MCP = 0.8137.
black medium square■
4.4 
Advanced Applications of Multivariate Control Charts 
4.4.1 
Multivariate Control Charts Scenarios 
The Hotelling upper T squaredT 2 chart introduced in Sect. 4.2 plots the upper T squaredT 2 statistic, which is the 
squared standardized distance of a vector from a target point (see Eq. (4.1.3)). Values 
of upper T squaredT 2 represent equidistant vectors along a multidimensional ellipse centered at 
the target vector point. The chart has an upper control limit left parenthesis upper U upper C upper L right parenthesis(UCL) determined by 
the F distribution (see Eq. (4.1.5)). Points exceeding UCL are regarded as an out-
of-control signal. The charted upper T squaredT 2 statistic is a function that reduces multivariate 
observations into a single value while accounting for the covariance matrix. Out-of-
control signals on the upper T squaredT 2 chart trigger an investigation to uncover the causes for the 
signal. 
The setup of an MSPC chart is performed by a process capability study. The 
process capability study period is sometimes referred to as phase I. The ongoing 
control using control limits determined in phase I is then called phase II. The  
distinction between these two phases is important. 
In setting MSPC charts, one meets several alternative scenarios derived from the 
characteristics of the reference sample and the appropriate control procedure. These 
include: 
1. Internally derived target 
2. Using an external reference sample 
3. Externally assigned target 
4. Measurements units considered as batches 
We proceed to discuss these four scenarios.

4.4
Advanced Applications of Multivariate Control Charts
125
4.4.2 
Internally Derived Target 
Internally derived targets are a typical scenario for process capability studies. 
The parameters to be estimated include the vector of process means, the process 
covariance matrix, and the control limit for the control chart. Consider a process 
capability study with a base sample of size n of p-dimensional observations, 
upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript n BaselineX1, X2, . . . , Xn. When the data are grouped and k subgroups of observations of 
size m are being monitored, n equals k mn = km, the covariance matrix estimator, upper S Subscript pSp can be 
calculated as the pooled covariances of the subgroups. In that case, for the j-th 
subgroup, the Hotelling upper T squaredT 2 statistic is then given by: 
upper T squared equals bold m left parenthesis upper X overbar Subscript j Baseline minus upper X overbar overbar right parenthesis prime upper S Subscript p Superscript negative 1 Baseline left parenthesis upper X overbar Subscript j Baseline minus upper X overbar overbar right parenthesis commaT 2 = m( ¯Xj −¯¯X)′S−1
p ( ¯Xj −¯¯X),
(4.4.1) 
where upper X overbar Subscript j ¯Xj is the mean of the j-th subgroup, upper X overbar overbar¯¯X is the overall mean, and upper S Subscript p Superscript negative 1S−1
p
is the 
inverse of the pooled estimated covariance matrix. The UCL for this case is 
upper U upper C upper L equals StartFraction p left parenthesis k minus 1 right parenthesis left parenthesis m minus 1 right parenthesis Over k left parenthesis m minus 1 right parenthesis minus p plus 1 EndFraction upper F Subscript 1 minus alpha Baseline left bracket p comma k left parenthesis m minus 1 right parenthesis minus p plus 1 right bracket periodUCL = p(k −1)(m −1)
k(m −1) −p + 1F1−α[p, k(m −1) −p + 1].
(4.4.2) 
When the data is ungrouped, and individual observations are analyzed, the estima-
tion of the proper covariance matrix and control limits requires further consider-
ation. Typically in this case, the covariance matrix is estimated from the pooled 
individual observations as upper S equals StartFraction 1 Over n minus 1 EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis upper X Subscript i Baseline minus upper X overbar right parenthesis left parenthesis upper X Subscript i Baseline minus upper X overbar right parenthesis primeS =
1
n−1
n
i=1(Xi −¯X)(Xi −¯X)′, where upper X overbar¯X is the 
mean of the n observations. The corresponding upper T squaredT 2 statistic for the i-th observation, 
i equals 1 comma ellipsis comma ni = 1, . . . , n is given by upper T squared equals left parenthesis upper X Subscript i Baseline minus upper X overbar right parenthesis prime upper S Superscript negative 1 Baseline left parenthesis upper X Subscript i Baseline minus upper X overbar right parenthesisT 2 = (Xi −¯X)′S−1(Xi −¯X). In this case, left parenthesis upper X Subscript i Baseline minus upper X overbar right parenthesis(Xi −¯X) and S 
are not independently distributed and the appropriate upper control for upper T squaredT 2 is based 
on the Beta distribution with 
upper U upper C upper L equals StartFraction left parenthesis n minus 1 right parenthesis squared Over n EndFraction upper B Subscript 1 minus alpha divided by 2 Baseline left parenthesis StartFraction p Over 2 EndFraction comma StartFraction n minus p minus 1 Over 2 EndFraction right parenthesisUCL = (n −1)2
n
B1−α/2
p
2 , n −p −1
2

(4.4.3) 
and 
upper L upper C upper L equals StartFraction left parenthesis n minus 1 right parenthesis squared Over n EndFraction upper B Subscript alpha divided by 2 Baseline left parenthesis StartFraction p Over 2 EndFraction comma StartFraction n minus p minus 1 Over 2 EndFraction right parenthesis commaLCL = (n −1)2
n
Bα/2
p
2 , n −p −1
2

,
(4.4.4) 
where upper B Subscript alpha Baseline left parenthesis nu 1 comma nu 2 right parenthesisBα(ν1, ν2) is the left parenthesis 1 minus alpha right parenthesis(1 −α)-th quantile of the Beta distribution with nu 1ν1 and nu 2ν2
as parameters. While theoretically the lower control limit left parenthesis upper L upper C upper L right parenthesis(LCL) can be calculated 
as above, in most circumstances LCL is set to zero. 
Example 4.3 Figure 4.5 presents the upper T squaredT 2 Hotelling Control Chart for the placement 
data used in Fig. 4.2. 
In Fig. 4.5, phase I was conducted over the ﬁrst 9 printed circuit boards. The 
implication is that the ﬁrst 144 observations are used to derive estimates of the 
means and covariances of xDev, yDev, and tDev and, with these estimates, the Upper 
Control Limit is determined.

126
4
Multivariate Statistical Process Control
Fig. 4.5 
upper T squaredT 2 control chart of xDev, yDev and tDev with control limits and correlation structure set 
up with data from ﬁrst 9 printed circuit boards 
place = mistat.load_data('PLACE') 
columns = ['xDev', 'yDev', 'tDev'] 
calibration = place[place.crcBrd <= 9][columns] 
newdata = place[place.crcBrd > 9][columns] 
mqcc = mistat.MultivariateQualityControlChart(calibration, 
qcc_type='T2single', newdata=newdata, 
confidence_level=(1-0.0000152837)**3) 
mqcc.plot() 
plt.show() 
The chart in Fig. 4.5 indicates an out-of-control point at observations 55 from 
board 4 due to very low horizontal and vertical deviations (xDev equals negative 0.0005200= −0.0005200, 
yDev equals 0.0002500= 0.0002500) and an extreme deviation in angular placement, t (tDev equals 0.129810=
0.129810). In the components inserted on the ﬁrst 9 boards, the average and standard 
deviations (in bracket) of xDev, yDev and tDev are, respectively: negative 0.001062−0.001062
(0.000602), negative 0.001816−0.001816 (0.000573), plus 0.01392+0.01392 (0.02665). 
Referring again to Fig. 4.5, we see a deviation in performance after board 9 with 
a signiﬁcant jump after board 12. We already studied what happened on boards 10-
12 and know that the shift is due to a correction in the vertical direction, increasing 
the values of yDev to be around zero (see Sect. 4.2).
black medium square■

4.4
Advanced Applications of Multivariate Control Charts
127
4.4.3 
External Reference Sample 
Consider again a process yielding independent observations upper X 1 comma upper X 2 comma ellipsisX1, X2, . . . of a 
p-dimensional random variable bold upper XX, such as the quality characteristics of a manu-
factured item or process measurements. Initially, when the process is “in control,” 
the observations follow a distribution F, with density f . We now assume that we 
have a “reference” sample upper X 1 comma ellipsis comma upper X Subscript n BaselineX1, . . . , Xn of F from an in-control period. To control 
the quality of the produced items, multivariate data is monitored for potential change 
in the distribution of bold upper XX, by sequentially collecting and analyzing the observations 
upper X Subscript iXi. At some time t equals n plus kt = n + k, k time units after n, the process may run out of 
control and the distribution of the upper X Subscript iXi’s changes to G. Our aim is to detect, in 
phase II, the change in the distribution of subsequent observations upper X Subscript n plus kXn+k, k greater than or equals 1k ≥1, 
as quickly as possible, subject to a bound alpha element of left parenthesis 0 comma 1 right parenthesisα ∈(0, 1) on the probability of raising 
a false alarm at each time point t equals n plus kt = n + k (that is, the probability of erroneously 
deciding that the distribution of upper X Subscript n plus kXn+k is not F). The reference sample upper X 1 comma ellipsis comma upper X Subscript n BaselineX1, . . . , Xn
does not incorporate the observations upper X Subscript n plus kXn+k taken after the “reference” stage, even 
if no alarm is raised, so that the rule is conditional only on the reference sample. 
When the data in phase II is grouped, and the reference sample from historical 
data includes k subgroups of observations of size m, n equals k mn = km, with the covariance 
matrix estimator upper S Subscript pSp calculated as the pooled covariances of the subgroups, the upper T squaredT 2
for a new subgroup of size m with mean upper Y overbar¯Y is given by 
upper T squared equals bold m left parenthesis upper Y overbar minus upper X overbar overbar right parenthesis prime upper S Subscript p Superscript negative 1 Baseline left parenthesis upper Y overbar minus upper X overbar overbar right parenthesis commaT 2 = m( ¯Y −¯¯X)′S−1
p ( ¯Y −¯¯X),
and the UCL is given by 
upper U upper C upper L equals StartFraction p left parenthesis k plus 1 right parenthesis left parenthesis m minus 1 right parenthesis Over k left parenthesis m minus 1 right parenthesis minus p plus 1 EndFraction upper F Subscript 1 minus alpha Baseline left bracket p comma k left parenthesis m minus 1 right parenthesis minus p plus 1 right bracket periodUCL = p(k + 1)(m −1)
k(m −1) −p + 1F1−α[p, k(m −1) −p + 1].
Furthermore, if in phase I, l subgroups were outside the control limits and assignable 
causes were determined, those subgroups are omitted from the computation of upper X overbar overbar¯¯X
and upper S Subscript p Superscript negative 1S−1
p , and the control limits for this case are 
upper U upper C upper L equals StartFraction p left parenthesis k minus l plus 1 right parenthesis left parenthesis m minus 1 right parenthesis Over left parenthesis k minus l right parenthesis m minus 1 right parenthesis negative p plus 1 EndFraction upper F Subscript 1 minus alpha Baseline left bracket p comma left parenthesis k minus l divided by left parenthesis m minus 1 right parenthesis minus p plus 1 right bracket periodUCL =
p(k −l + 1)(m −1)
(k −l)m −1) −p + 1F1−α[p, (k −l/(m −1) −p + 1].
The upper T squaredT 2 Control Charts constructed in phase I, and used both in phase I and in phase 
II, are the multivariate equivalent of the Shewhart Control Chart. Those charts, as 
well as some more advanced ones, simplify the calculations down to single-number 
criteria and produce a desired Type I error or in-control run length. 
While we focused on the reference sample provided by phase I of the multivariate 
process control, other possibilities can occur as well. In principle, the reference in-
control sample can also originate from historical data. In this case, the statistical

128
4
Multivariate Statistical Process Control
analysis will be the same but this situation has to be treated with precaution since 
both the control limits and the possible correlations between observations may shift. 
4.4.4 
Externally Assigned Target 
If all parameters of the underlying multivariate distribution are known and externally 
assigned, the upper T squaredT 2 value for a single multivariate observation of dimension p is 
computed as 
upper T squared equals left parenthesis bold upper Y minus bold italic mu right parenthesis prime upper Sigma Superscript negative 1 Baseline left parenthesis bold upper Y minus bold italic mu right parenthesis commaT 2 = (Y −μ)′−1(Y −μ),
(4.4.5) 
where bold italic muμ and bold italic upper Sigma are the expected value and covariance matrix, respectively. 
The probability distribution of the upper T squaredT 2 statistic is a chi squaredχ2 distribution with p degrees 
of freedom. Accordingly, the 0.95 UCL for upper T squaredT 2 is upper U upper C upper L equals chi Subscript nu comma 0.95 Superscript 2UCL = χ2
ν,0.95. When the data 
are grouped in subgroups of size m, and both bold italic muμ and bold italic upper Sigma are known, the upper T squaredT 2 value of 
the mean vector upper Y overbar¯Y is upper T squared equals m left parenthesis upper Y overbar minus bold italic mu right parenthesis prime bold italic upper Sigma Superscript negative 1 Baseline left parenthesis upper Y overbar minus bold italic mu right parenthesisT 2 = m( ¯Y −μ)′−1( ¯Y −μ) with the same UCL as above. 
If only the expected value of the underlying multivariate distribution, bold italic muμ, is  
known and externally assigned, the covariance matrix has to be estimated from the 
tested sample. The upper T squaredT 2 value for a single multivariate observation of dimension p is 
computed as upper T squared equals left parenthesis bold upper Y minus bold italic mu right parenthesis prime upper S Superscript negative 1 Baseline left parenthesis bold upper Y minus bold italic mu right parenthesisT 2 = (Y −μ)′S−1(Y −μ), where bold italic muμ is the expected value and S is the 
estimate of the covariance matrix bold italic upper Sigma, estimated either as the pooled contribution of 
the individual observations, i.e., upper S equals StartFraction 1 Over n minus 1 EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis upper X Subscript i Baseline minus upper X overbar right parenthesis left parenthesis upper X Subscript i Baseline minus upper X overbar right parenthesis primeS =
1
n−1
n
i=1(Xi −¯X)(Xi −¯X)′, or by a method 
which accounts for possible lack of independence between observations. 
In this case, the 0.95 UCL for upper T squaredT 2 is 
upper U upper C upper L equals StartFraction p left parenthesis n minus 1 right parenthesis Over n left parenthesis n minus p right parenthesis EndFraction upper F 0.95 left bracket p comma n minus p right bracket periodUCL = p(n −1)
n(n −p)F0.95[p, n −p].
When the tested observations are grouped, the mean vector of a subgroup with m 
observations (a rational sample) will have the same expected value as the individual 
observations, bold italic muμ, and a covariance matrix bold italic upper Sigma divided by m/m. The covariance matrix bold italic upper Sigma can be 
estimated by S or as upper S Subscript pSp obtained by pooling the covariances of the k subgroups. 
When bold italic upper Sigma is estimated by S, the upper T squaredT 2 value of the mean vector upper Y overbar¯Y of m tested 
observations is upper T squared equals m left parenthesis upper Y overbar minus bold italic mu right parenthesis prime upper S Superscript negative 1 Baseline left parenthesis upper Y overbar minus bold italic mu right parenthesisT 2 = m( ¯Y −μ)′S−1( ¯Y −μ) and the 0.95 UCL is 
upper U upper C upper L equals StartFraction p left parenthesis m minus 1 right parenthesis Over m minus p EndFraction upper F 0.95 left bracket p comma m minus p right bracket periodUCL = p(m −1)
m −p F0.95[p, m −p].
(4.4.6) 
When bold italic upper Sigma is estimated by upper S Subscript pSp, the 0.95 UCL of upper T squared equals m left parenthesis upper Y overbar minus bold italic mu right parenthesis prime upper S Subscript p Superscript negative 1 Baseline left parenthesis upper Y overbar minus bold italic mu right parenthesisT 2 = m( ¯Y −μ)′S−1
p ( ¯Y −μ) is 
upper U upper C upper L equals StartFraction p k left parenthesis m minus 1 right parenthesis Over k left parenthesis m minus 1 right parenthesis minus p plus 1 EndFraction upper F 0.95 left bracket p comma k left parenthesis m minus 1 right parenthesis minus p plus 1 right bracket periodUCL =
pk(m −1)
k(m −1) −p + 1F0.95[p, k(m −1) −p + 1].
(4.4.7)

4.4
Advanced Applications of Multivariate Control Charts
129
4.4.5 
Measurement Units Considered as Batches 
In the semiconductor industry, production is typically organized in batches or 
production lots. In such cases, the quality control process can be performed either 
at the completion of the batch or sequentially, in a curtailed inspection, aiming at 
reaching a decision as soon as possible. When the quality control method used is 
reaching a decision at the completion of the process, the possible outcomes are (a) 
determine the production process to be in statistical control and accept the batch or 
(b) stop the production ﬂow because of a signal that the process is out of control. 
On the other hand, in a curtailed inspection, based on a statistical stopping rule, the 
results from the ﬁrst few items tested may sufﬁce to stop the process prior to the 
batch completion. 
Consider a batch of size n, with the tested items bold upper Y 1 comma ellipsis comma bold upper Y Subscript n BaselineY1, . . . , Yn. The curtailed 
inspection tests the items sequentially. Assume that the targets are speciﬁed, either 
externally assigned or from a reference sample or batch. With respect to those 
targets, let upper V Subscript i Baseline equals 1Vi = 1 if the upper T squaredT 2 of the ordered i-th observation exceeds the critical 
value kappaκ and upper V Subscript i Baseline equals 0Vi = 0, otherwise. For the i-th observation, the process is considered 
to be in control if for a prespeciﬁed P, say upper P equals 0.95P = 0.95, upper P r left parenthesis upper V Subscript i Baseline equals 0 right parenthesis greater than or equals upper PPr(Vi = 0) ≥P. Obviously, 
the inspection will be curtailed only at an observation i for which upper V Subscript i Baseline equals 1Vi = 1 (not 
necessarily the ﬁrst). 
Let upper N left parenthesis g right parenthesis equals sigma summation Underscript i equals 1 Overscript g Endscripts upper V Subscript iN(g) = g
i=1 Vi be the number of rejections up to the g-th tested item. 
For each number of individual rejections U (out of n), upper R left parenthesis upper U right parenthesisR(U) denotes the minimal 
number of observations allowed up to the U-th rejection, without rejecting the 
overall null hypothesis. Thus, for each U, upper R left parenthesis upper U right parenthesisR(U) is the minimal integer value 
such that under the null hypothesis, upper P r left parenthesis sigma summation Underscript i equals 1 Overscript upper R left parenthesis upper U right parenthesis Endscripts upper V Subscript i Baseline less than or equals upper U right parenthesis greater than or equals alphaPr
R(U)
i=1 Vi ≤U

≥α. For ﬁxed U, the  
random variable sigma summation Underscript i equals 1 Overscript upper U Endscripts upper V Subscript iU
i=1 Vi has a negative binomial distribution, and we can compute 
upper R left parenthesis upper N left parenthesis g right parenthesis right parenthesisR(N(g)) from the inverse of the negative binomial distribution. For example, when 
n equals 13n = 13, upper P equals 0.95P = 0.95, and alpha equals 0.01α = 0.01, the null hypothesis is rejected if the second 
rejection occurred at or before the third observation, or if the third rejection occurred 
at or before the ninth observation, and so on. 
4.4.6 
Variable Decomposition and Monitoring Indices 
Data in batches is naturally grouped, but even if quality control is performed on 
individual items, grouping the data into rational consequent subgroups may yield 
relevant information on within subgroups variability, in addition to deviations from 
targets. In the j-th subgroup (or batch) of size n Subscript jnj, the individual upper T Subscript i j Superscript 2T 2
ij values, i equals 1 comma ellipsis comma n Subscript j Baselinei =
1, . . . , nj are given by upper T Subscript i j Superscript 2 Baseline equals left parenthesis bold upper Y Subscript i j Baseline minus bold italic theta right parenthesis prime upper G Superscript negative 1 Baseline left parenthesis bold upper Y Subscript i j Baseline minus bold italic theta right parenthesisT 2
ij = (Yij −θ)′G−1(Yij −θ). When the targets are externally 
assigned then bold italic theta equals bold italic muθ = μ. If the covariance matrix is also externally assigned then upper G equals upper SigmaG =
, otherwise G is the covariance matrix estimated either from the tested or from the 
reference sample. In the case of targets derived from an external reference sample

130
4
Multivariate Statistical Process Control
bold italic theta equals bold mθ = m and upper G equals upper SG = S, where bold mm and S are the mean and the covariance matrix of a 
reference sample of size n. Within the j-th subgroup, let us denote the mean of the 
subgroup observations by upper Y overbar Subscript j¯Yj and the mean of the target values in the j-th subgroup 
by bold italic theta Subscript jθj. 
The sum of the individual upper T Subscript i j Superscript 2T 2
ij values, upper T Subscript 0 j Superscript 2 Baseline equals sigma summation Underscript i equals 1 Overscript n Endscripts upper T Subscript i j Superscript 2T 2
0j = n
i=1 T 2
ij can be decomposed into 
two measurements of variability, one representing the deviation of the subgroup 
mean from the multivariate target denoted by upper T Subscript upper M j Superscript 2T 2
Mj, and the other measuring the inter-
nal variability within the subgroup, denoted by upper T Subscript upper D j Superscript 2T 2
Dj. The deviation of the subgroup 
mean from the multivariate target is estimated by upper T Subscript upper M j Superscript 2 Baseline equals left parenthesis upper Y overbar Subscript j Baseline minus bold italic theta Subscript j Baseline right parenthesis prime upper G Superscript negative 1 Baseline left parenthesis upper Y overbar Subscript j Baseline minus bold italic theta Subscript j Baseline right parenthesisT 2
Mj = ( ¯Yj −θj)′G−1( ¯Yj −θj), 
while the internal variability within the subgroup is estimated by 
upper T Subscript upper D j Superscript 2 Baseline equals left parenthesis bold upper Y Subscript i j Baseline minus upper Y overbar Subscript j Baseline right parenthesis prime upper G Superscript negative 1 Baseline left parenthesis bold upper Y Subscript i j Baseline minus upper Y overbar Subscript j Baseline right parenthesis comma with upper T Subscript 0 j Superscript 2 Baseline equals left parenthesis n minus 1 right parenthesis upper T Subscript upper M j Superscript 2 Baseline plus upper T Subscript upper D j Superscript 2 Baseline periodT 2
Dj = (Yij −¯Yj)′G−1(Yij −¯Yj),
with
T 2
0j = (n −1)T 2
Mj + T 2
Dj.
(4.4.8) 
Since asymptotically, upper T Subscript upper M j Superscript 2T 2
Mj and upper T Subscript upper D j Superscript 2T 2
Dj have a chi squaredχ2 distribution with p and left parenthesis n minus 1 right parenthesis p(n −1)p
degrees of freedom, respectively, one can further compute two indices, upper I 1I1 and upper I 2I2, 
to determine whether the overall variability is mainly due to the distances between 
the means of the tested subgroup from targets or to the within subgroup variability. 
The indices are relative ratios of the normalized versions of the two components 
of upper T Subscript 0 j Superscript 2T 2
0j, i.e., upper I 1 equals upper I 1 Superscript asterisk Baseline divided by left parenthesis upper I 1 Superscript asterisk Baseline plus upper I 2 Superscript asterisk Baseline right parenthesisI1 = I ∗
1 /(I ∗
1 + I ∗
2 ), and upper I 2 equals upper I 2 Superscript asterisk Baseline divided by left parenthesis upper I 1 Superscript asterisk Baseline plus upper I 2 Superscript asterisk Baseline right parenthesisI2 = I ∗
2 /(I ∗
1 + I ∗
2 ), where upper I 1 Superscript asterisk Baseline equals upper T Subscript upper M j divided by p Superscript 2I ∗
1 = T 2
Mj/p and 
upper I 2 Superscript asterisk Baseline equals upper T Subscript upper D j Superscript 2 Baseline divided by left bracket left parenthesis n minus 1 right parenthesis p right bracketI ∗
2 = T 2
Dj/[(n−1)p]. We can express the indices in terms of the original upper T squaredT 2 statistics 
as, upper I 1 equals left parenthesis n minus 1 right parenthesis upper T Subscript upper M j Superscript 2 Baseline divided by left bracket left parenthesis n minus 1 right parenthesis upper T Subscript upper M j Superscript 2 Baseline plus upper T Subscript upper D j Superscript 2 Baseline right bracketI1 = (n−1)T 2
Mj/[(n−1)T 2
Mj +T 2
Dj] and upper I 2 equals upper T Subscript upper D j Superscript 2 Baseline divided by left bracket left parenthesis n minus 1 right parenthesis upper T Subscript upper M j Superscript 2 Baseline plus upper T Subscript upper D j Superscript 2 Baseline right bracketI2 = T 2
Dj/[(n−1)T 2
Mj +T 2
Dj]. Tracking 
these indices provides powerful monitoring capabilities. 
4.5 
Multivariate Tolerance Speciﬁcations 
Multivariate tolerance regions are based on estimates of quantiles from a multi-
variate distribution with parameters either known or estimated from the data (John 
1963). Setting up a process control scheme, on the basis of tolerance regions, 
involves estimating the level set left brace f greater than or equals c right brace{f ≥c} of the density f which generates the 
data, with a prespeciﬁed probability content 1 minus bold italic alpha1 −α. With this approach, originally 
proposed in Fuchs and Kenett (1987), the rejecting region is upper X Subscript n plus 1 Baseline element of left brace f greater than or equals c right braceXn+1 ∈{f ≥c}. This  
method provides an exact false alarm probability of bold italic alphaα. Since f is usually unknown, 
the population tolerance region left brace f greater than or equals c right brace{f ≥c} needs to be estimated by an estimator of f . 
A similar approach was adopted by the Food and Drug Administration to determine 
equivalence of a drug product tablet before and after a change in manufacturing 
processes such as introduction of new equipment, a transfer of operations to another 
site or the scaling up of production to larger vessels. The equivalence is evaluated by 
comparing tablet dissolution proﬁles of a batch under test with dissolution proﬁles 
of tablets from a reference batch and allowing for at most a 15% difference. We 
expand on this example using the procedure proposed by Tsong et al. (1996).

4.5
Multivariate Tolerance Speciﬁcations
131
When comparing the dissolution data of a new product and a reference approved 
product, the goal is to assess the similarity between the mean dissolution values 
at several observed sample time points. The decision of accepting or rejecting 
the hypothesis that the two batches have similar dissolution proﬁles, i.e., are 
bioequivalent, is based on determining if the difference in mean dissolution values 
between the test and reference products is no larger than the maximum expected 
difference between any two batches of the approval product. When dissolution value 
is measured at a single time point, the conﬁdence interval of the true difference 
between the two batches is compared with prespeciﬁed similarity limits. When 
dissolution values are measured at several time points, the Mahalanobis upper D Subscript upper M Superscript 2D2
M deﬁned 
below can be used to compare the overall dissolution proﬁles. 
The important property of the Mahalanobis upper D squaredD2, is that differences at points with 
low variability are given a higher weight than differences at points with higher 
variability. This ensures that the experimental noise is properly addressed. 
Let upper X 1 equals left parenthesis x 11 x 12 comma ellipsis comma x Subscript 1 p Baseline right parenthesisX1 = (x11x12, . . . , x1p) and upper X 2 equals left parenthesis x 21 comma x 22 comma ellipsis comma x Subscript 2 p Baseline right parenthesisX2 = (x21, x22, . . . , x2p) represent the mean 
dissolution values at p time instances of the reference and the batch under test, 
respectively. These means can correspond to a different number of replicates, say n 
and m. 
The Mahalanobis distance between any two vectors bold upper X 1X1 and bold upper X 2X2, having the same  
dispersion matrix Bar Summation|, is  
upper D Subscript upper M Baseline left parenthesis bold upper X 1 comma bold upper X 2 right parenthesis equals left parenthesis left parenthesis bold upper X 1 minus bold upper X 2 right parenthesis prime Bar Summation negative 1 left parenthesis bold upper X 1 minus bold upper X 2 right parenthesis right parenthesis Superscript 1 divided by 2 Baseline periodDM(X1, X2) =

(X1 −X2)′ |−1(X1 −X2)
1/2
.
(4.5.1) 
If we estimate Bar Summation| by covariance matrices upper S 1S1 and upper S 2S2, we substitute for Bar Summation| in (4.5.1) 
the pooled estimator, upper S Subscript pooledSpooled. A conﬁdence region for the difference bold italic upper Delta equals bold italic mu 1 minus bold italic mu 2 = μ1 −μ2, 
between the expected value of the batch and the reference populations, at conﬁdence 
level 1 minus alpha1 −α, is  
upper C upper R equals left brace bold upper Y colon left parenthesis bold upper Y minus left parenthesis bold upper X 1 minus bold upper X 2 right parenthesis right parenthesis prime upper S Subscript pooled Superscript negative 1 Baseline left parenthesis bold upper Y minus left parenthesis bold upper X 1 minus bold upper X 2 right parenthesis Superscript prime Baseline right parenthesis less than or equals upper K upper F Subscript 1 minus alpha Baseline left bracket p comma 2 n minus p minus 1 right bracket comma right braceCR =

Y : (Y−(X1−X2))′S−1
pooled(Y−(X1 −X2)′) ≤KF1−α[p, 2n −p −1],

(4.5.2) 
where p is the dimension of the vectors, and 
upper K equals StartFraction 4 left parenthesis n minus 1 right parenthesis p Over n left parenthesis 2 n minus p minus 1 right parenthesis EndFraction periodK =
4(n −1)p
n(2n −p −1).
(4.5.3) 
Example 4.4 To demonstrate the procedure we use an example where Y is the 
percent dissolution of a tablet, measured at two time instances, 15 min and 90 min 
(see Table 4.2 and dataset DISS.csv). Calculations with Python are implemented in 
the MahalanobisT2 function of the mistat package.

132
4
Multivariate Statistical Process Control
Table 4.2 Dissolution data 
of reference and batch under 
test 
Batch
Tablet 
15
90 
1
REF
1
65.58 
93.14 
2
REF
2
67.17 
88.01 
3
REF
3
65.56 
86.83 
4
REF
4
66.51 
88.00 
5
REF
5
69.06 
89.70 
6
REF
6
69.77 
88.88 
7
TEST 
1
47.77 
92.39 
8
TEST 
2
49.46 
89.93 
9
TEST 
3
47.76 
90.19 
10 
TEST 
4
49.72 
94.12 
11 
TEST 
5
52.68 
93.80 
12 
TEST 
6
51.01 
94.45 
diss = mistat.load_data('DISS') 
columns = ['batch', 'min15', 'min90'] 
mahalanobisT2 = mistat.MahalanobisT2(diss[columns], 'batch', 
compare_to=[15,15], conf_level=0.95) 
mahalanobisT2.summary() 
Coordinates 
min15
min90 
LCR
14.558418 -2.810708 
Center 
17.541667 -3.386667 
UCR
20.524915 -3.962625 
Mahalanobis 
LCR
8.664879 
Center
10.440449 
UCR
12.216019 
dtype: float64 
comparison: 9.6308 
A scatterplot of the data shows the difference between test and reference. At 15 
min dissolution is lower in the tested batch than the reference, at 90 min this is 
reversed (see Fig. 4.6). Our tested material therefore starts dissolving slower than 
the reference but then things change and it reaches high dissolution levels faster 
than the reference. 
def to_coord_rep(coord): 
return f'({coord[0]:.2f}, {coord[1]:.2f})' 
center_s = to_coord_rep(mahalanobisT2.coord.loc['Center', :]) 
lcr_s = to_coord_rep(mahalanobisT2.coord.loc['LCR', :]) 
ucr_s = to_coord_rep(mahalanobisT2.coord.loc['UCR', :]) 
For this data, n equals 6n = 6, p equals 2p = 2, upper K equals 1.35K = 1.35, upper F Subscript 2 comma 19 comma 0.95 Baseline equals 4.26F2,19,0.95 = 4.26, left parenthesis upper X 2 minus upper X 1 right parenthesis equals left parenthesis 17.54 comma negative 3.39 right parenthesis(X2 −X1) =
(17.54, −3.39) and upper D Subscript upper M Baseline equals 10.44DM = 10.44. A contour plot with the limits of CR set at 4.26 is 
presented in Fig. 4.7. 
The center of the ellipsoid is set at left parenthesis 17.54 comma negative 3.39 right parenthesis(17.54, −3.39) and, as mentioned above, 
at that point, upper D Subscript upper M Baseline equals 10.44DM = 10.44. The line from the origin connecting to this point is

4.6
Tracking Structural Changes
133
Fig. 4.6 Scatterplot of reference and batch under test
upper Y equals minus 0.193 upper XY = −0.193X. It crosses the ellipse ﬁrst at left parenthesis 14.56 comma negative 2.81 right parenthesis(14.56, −2.81) labeled as “1” on Fig. 4.7 
and then at left parenthesis 20.52 comma negative 3.96 right parenthesis(20.52, −3.96) labeled as “2” with upper D Subscript upper MDM values of upper D Subscript upper M Superscript l Baseline equals 8.66Dl
M = 8.66 and 
upper D Subscript upper M Superscript u Baseline equals 12.22Du
M = 12.22, respectively. 
To determine equivalence, with a 15% buffer, we consider the contour cor-
responding to results within this buffer. The upper D Subscript upper MDM value for these point upper R upper D equals upper S q r t left bracket left parenthesis 15 comma 15 right parenthesis prime upper S Subscript pooled Superscript negative 1 Baseline left parenthesis 15 comma 15 right parenthesis right bracket equals 9.63RD =
Sqrt[(15, 15)′S−1
pooled(15, 15)] = 9.63. 
Since upper D Subscript upper M Superscript u Baseline greater than upper R upper DDu
M > RD we have a conﬁdence region for the true difference in mean 
dissolution that exceeds the 15% buffer. We therefore declare the batch under test 
not to be equivalent to the reference. 
An index that can be used to assess process capability in terms of equivalence 
between reference and batch under test is upper C e q equals upper R upper D divided by upper D Subscript upper M Superscript uCeq = RD/Du
M. To determine the batch 
under test equivalent to the reference we need to show that upper C e q greater than 1Ceq > 1.
black medium square■
4.6 
Tracking Structural Changes 
In many industrial processes, consecutive measurements are auto and cross-
correlated. This is due to inertial elements such as raw materials, storage tanks, 
reactors, reﬂuxes, environmental conditions, etc. with dynamics larger than the 
sampling frequency. Classical linear regression models typically assume that the 
relationships between the inputs and the outputs in a system are instantaneous. 
However, as mentioned, dynamic processes often show inertias and delayed

134
4
Multivariate Statistical Process Control
Fig. 4.7 Difference between dissolution of batch under test and reference at 15 and 90 min
responses. In this section we focus on tracking structural changes over time. It 
expands on Chapter 6 of Modern Statistics (Kenett et al. 2022b) (“Time Series 
Analysis and Prediction”). A comprehensive treatment of univariate time series 
models is available in Box et al. (2015). 
4.6.1 
The Synthetic Control Method 
Comparative case studies are often applied to the evaluation of interventions or 
the impact of events such as occurrence of faults or other interventions. The 
synthetic control method (SCM) is based on the idea that a combination of affected 
and unaffected time units provides a comparison that can establish causality. It 
formalizes the weighting of the comparison time units using a data driven procedure, 
see Abadie et al. (2015) and Ben-Michael et al. (2021a), Ben-Michael et al. (2021b). 
Suppose we obtain data left parenthesis y comma upper X right parenthesis(y, X) from m plus 1m + 1 systems at n time intervals: t equals t 1 comma t 2 comma ellipsis comma t Subscript n Baselinet =
t1, t2, . . . , tn. The n times 1n × 1 vector y from a system where at time t 0t0 an intervention,

4.6
Tracking Structural Changes
135
Table 4.3 Data structure for synthetic control method (SCM) 
Treatment 
Time t
1
2
...
t 0t0
t 0 plus 1t0 + 1
...
t Subscript ntn
Treated y
y 1y1
y 2y2
...
y Subscript t 0yt0
y Subscript t 0 plus 1yt0+1
...
y Subscript t Sub Subscript nytn
Untreated X 
healthy 
upper X Subscript 1 comma 1X1,1
upper X Subscript 2 comma 1X2,1
...
upper X Subscript t 0 comma 1Xt0,1
upper X Subscript t 0 plus 1 comma 1Xt0+1,1
...
upper X Subscript t Sub Subscript n Subscript comma 1Xtn,1
. . . 
. . . 
. . . 
. . . 
. . . 
upper X Subscript 1 comma mX1,m
upper X Subscript 2 comma mX2,m
...
upper X Subscript t 0 comma mXt0,m
upper X Subscript t 0 plus 1 comma mXt0+1,m
...
upper X Subscript t Sub Subscript n Subscript comma mXtn,m
Additional 
covariates C 
upper C Subscript 1 comma 1C1,1
upper C Subscript 2 comma 1C2,1
...
upper C Subscript t 0 comma kCt0,k
upper C Subscript t 0 plus 1 comma 1Ct0+1,1
...
upper C Subscript t Sub Subscript n Subscript comma 1Ctn,1
. . . 
. . . 
. . . 
. . . 
. . . 
upper C Subscript 1 comma kC1,k
upper C Subscript 2 comma kC2,k
...
upper C Subscript t 0 comma kCt0,k
upper C Subscript t 0 plus 1 comma kCt0+1,k
...
upper C Subscript t Sub Subscript n Subscript comma kCtn,k
such as a fault or a change, occurred. This intervention affects the time interval 
left parenthesis t 0 plus 1 comma t Subscript n Baseline right parenthesis(t0 + 1, tn). The n times mn × m matrix X are similar data from untreated, healthy systems. 
The data structure is shown in Table 4.3. This table also includes optional covariates 
C (n times kn × k) that if available can be combined with the matrix X. In the following, we 
will not explicitly include the covariates in the description of the method. 
We ﬁrst split the dataset into pre-intervention data left parenthesis upper X Superscript upper P Baseline comma y Superscript upper P Baseline right parenthesis(XP , yP ) and post-
intervention, treatment data left parenthesis upper X Superscript upper T Baseline comma y Superscript upper T Baseline right parenthesis(XT , yT ). 
StartLayout 1st Row 1st Column y Superscript upper P Baseline equals left parenthesis y Subscript i Baseline right parenthesis Superscript prime Baseline 2nd Column i equals 1 comma ellipsis comma t 0 2nd Row 1st Column upper X Superscript upper P Baseline equals left parenthesis upper X Subscript i comma j Baseline right parenthesis 2nd Column i equals 1 comma ellipsis comma t 0 and j equals 1 comma ellipsis comma m 3rd Row 1st Column y Superscript upper T Baseline equals left parenthesis y Subscript i Baseline right parenthesis Superscript prime Baseline 2nd Column i equals t 0 plus 1 comma ellipsis comma n 4th Row 1st Column upper X Superscript upper T Baseline equals left parenthesis upper X Subscript i comma j Baseline right parenthesis 2nd Column i equals t 0 plus 1 comma ellipsis comma n and j equals 1 comma ellipsis comma m period EndLayoutyP = (yi)′
i = 1, . . . , t0
XP = (Xi,j)
i = 1, . . . , t0 and j = 1, . . . , m
yT = (yi)′
i = t0 + 1, . . . , n
XT = (Xi,j)
i = t0 + 1, . . . , n and j = 1, . . . , m.
The idea of the synthetic control method is to estimate y using a model f based on 
the pre-intervention data using the following model. 
ModifyingAbove y With caret Subscript i Superscript upper P Baseline equals f left parenthesis upper Y Superscript upper P Baseline right parenthesis equals sigma summation Underscript j equals 1 Overscript m Endscripts w Subscript j Baseline upper Y Subscript i comma j Baseline periodˆyP
i = f (Y P ) =
m

j=1
wjYi,j.
The matrix of weights is found by choosing to minimize upper W Superscript asteriskW ∗to minimize StartAbsoluteValue EndAbsoluteValue y Superscript upper N minus upper W upper X Superscript upper N Baseline StartAbsoluteValue EndAbsoluteValue||yN −
WXN||. In the original publication, Abadie et al. (2015) estimated the weights 
using W 
g g t Baseline 0ggt0 and sigma summation Underscript 1 Overscript m Endscripts w Subscript j Baseline equals 1m
1 wj = 1 as an additional constraint. This means, the weights form a 
convex combination of the untreated data. In our example, we use a different form 
or regularization by adding the upper L 1L1 penalty, i.e., adding lamda sigma summation Underscript j equals 1 Overscript m Endscripts StartAbsoluteValue w Subscript j Baseline EndAbsoluteValueλ m
j=1 |wj|. This is also  
known as Lasso regression. 
Once we have a model, we can construct an estimated synthetic control for all 
time periods. In particular, we can estimate the treatment effect using

136
4
Multivariate Statistical Process Control
Fig. 4.8 Amplitude of accelerometer sensor of railway vehicle suspension over time with healthy 
and faulty system data shown as a two-dimensional density plot 
Fig. 4.9 Amplitude of accelerometer data after angular transformation
ModifyingAbove tau With caret Subscript t Baseline equals y Subscript t Superscript upper T Baseline minus ModifyingAbove upper Y With caret Subscript t Superscript upper T Baseline periodˆτt = yT
t −ˆY T
t .
Because the synthetic control is constructed from untreated units, when the interven-
tion occurs at time t 0t0, the difference between the synthetic control and the treated 
unit is the estimated treatment effect. 
Example 4.5 Davidyan et al. (2021) provide an example of time series tracking 
engine vibrations of railway vehicle suspension systems. These suspensions are 
affected by several potential fault including wheel ﬂats. Such faults can have 
signiﬁcant impact on system performance and safety. Figure 4.8 presents vibration 
sensor data amplitude over time, in a healthy system and with wheel ﬂats of 10 
and 20 mm. With increasing asymmetry of the wheel, the vibration sensor detects 
increased vibrations. 
Figure 4.9 presents the same data after angular transformation. 
The dataset SCM_WHEELS.csv contains vibrational amplitude data for 40 
wheels. For each wheel, we have a time series of 100 steps. The ﬁrst wheel is 
damaged at step 60. We use SCM to identify this event. 
We load and preprocess the data for further analysis. 

4.6
Tracking Structural Changes
137
Fig. 4.10 Change in vibration amplitude over 100 time steps for ﬂat wheel. The shaded area shows 
the mean and plus+/minus−one standard deviation of all 40 wheels
data = mistat.load_data('SCM_WHEELS.csv') 
nr_wheels = 40 
scm_data = defaultdict(list) 
n = len(data) 
for i in range(1, nr_wheels+1): 
scm_data['wheel'].extend([i] * n) 
scm_data['time'].extend(range(1, n+1)) 
scm_data['vibrations'].extend(data[f'Wheel-{i}']) 
scm_data['status'].extend([True if i == 1 else False] * n) 
scm_data['after_event'].extend([False] * 59) 
scm_data['after_event'].extend([True] * 41) 
scm_data = pd.DataFrame(scm_data).sort_values(by=['time', 'wheel']) 
Figure 4.10 shows the change of vibration amplitude over time. The effect of the 
wheel damage at time point 60 is clearly visible. 
The method train_predict_SCM_model implements the SCM method. It is 
deﬁned as a function that takes the data, the number of a selected wheel, and the 
event date. The SCM model requires the data in a matrix where the wheels are in 
columns and the vibration and sensor data in rows. The model is then trained to 
learn the sensor data of the selected wheel using information from the other wheels. 
We use Lasso, a L1-regularized linear regression model, from the scikit-learn 
package. The function returns a data frame that contains the actual and synthetic 
data and their residual. 

138
4
Multivariate Statistical Process Control
Fig. 4.11 SCM estimated effect of ﬂat wheel on the vibration sensor data. Actual (orange) and 
predicted (blue)
def train_predict_SCM_model(scm_data, wheel, event): 
# convert data into a table with vibration and sensor data in rows and 
# wheels in columns 
features = ['vibrations'] 
full_data = scm_data.pivot(index='wheel', columns='time')[features].T 
# filter pre-damage event period (make a slice on the multi-index) 
pre_event = full_data.loc[('vibrations', 1):('vibrations', event)] 
# train regularized regression model 
X = pre_event.drop(columns=wheel).values 
# other wheels 
y = pre_event[wheel].values # selected wheel 
model = Lasso(fit_intercept=False, max_iter=10_000, alpha=2, 
selection='random', random_state=1) 
model.fit(X, y) 
vibrations = full_data.loc['vibrations'] 
pred_y = model.predict(vibrations.drop(columns=wheel)) 
return pd.DataFrame({ 
'time': scm_data.query(f'wheel == {wheel}')['time'], 
'vibrations': scm_data.query(f'wheel == {wheel}')['vibrations'], 
'synthetic': pred_y, 
'residual': scm_data.query(f'wheel == {wheel}')['vibrations'] - pred_y, 
}) 
The function can be used to predict the expected vibration amplitude for the 
damaged wheel. Figure 4.11 contains the resulting graph. 
scm_faulty = train_predict_SCM_model(scm_data, 1, 60) 
ax = scm_faulty.plot(x='time', y='synthetic', label='Synthetic data') 
scm_faulty.plot(x='time', y='vibrations', label='Vibration amplitude', ax=ax) 
plt.show() 
We can repeat this model building and prediction for each of the 40 wheels. 
Figure 4.12 summarizes the results graphically. Unsurprisingly, the residuals before 

4.7
Chapter Highlights
139
Fig. 4.12 Comparison of the residual of the SCM estimated effect for healthy (brown) and ﬂat 
(blue) wheel on the vibration sensor data. The shaded area shows mean and plus+/minus−one, two, and 
three standard deviations of all residuals 
the damage occurred ﬂuctuate less compared to the residuals after. Only the data 
between 1 and 60 time steps are used in the model training. We can also see that 
the damaged wheel has residuals that are more than three standard deviations away 
from the mean. The SCM method reveals that the change is unexpected.
black medium square■
4.7 
Chapter Highlights 
The main concepts and tools introduced in this chapter include:
•
Mean vector
•
Covariance matrix
•
Mahalanobis T2
•
Multivariate statistical process control
•
Multivariate process capability indices
•
Multivariate tolerance region
•
Hyper-rectangular tolerance regions
•
Circular tolerance regions
•
Principal components
•
Internal targets
•
Reference sample
•
External targets
•
Multivariable control of data in samples
•
Synthetic control method (SCM)

140
4
Multivariate Statistical Process Control
4.8 
Exercises 
Exercise 4.1 In dataset TSQ we ﬁnd 368 T 2 values corresponding to the vectors 
(x, y, θ)  in the PLACE dataset. The ﬁrst n = 48 vectors in PLACE dataset were 
used as a base sample, to compute the vector of means m and the covariance matrix 
S. The  T 2 values are for the other individual vectors (m = 1). Plot the T 2 values in 
the dataset TSQ.csv. Compute the UCL and describe from the plot what might have 
happened in the placement process generating the (x, y, θ)  values. 
Exercise 4.2 Prove that if X has a multivariate normal distribution, Nv(μ, σ), then 
(X −μ)′−1(X − μ) has a χ2 distribution with v degrees of freedom where R = 
χ2 
1−p[v] is the corresponding (1 − p) quantile of the χ2 distribution with v degrees 
of freedom. 
Exercise 4.3 Sort the dataset CAR by variable cyl, indicating the number of 
cylinders in a car, and run a T 2 chart with internally derived targets for the variables 
turn, hp, mpg, with separate computations for cars with 4, 6 and 8 cylinders. How 
is the number of cylinders affecting the overall performance of the cars? 
Exercise 4.4 Sort the dataset CAR.csv by variable origin, indicating the country 
of origin, and run a T 2 chart with internally derived targets for the variables turn, 
hp, mpg, with separate computations for cars from 1 = US; 2 = Europe; 3 = Asia. 
How is the country of origin affecting the overall performance of the cars? 
Exercise 4.5 Load the dataset GASOL.csv and compute a T 2 chart for x1, x2, 
astm, endPt, yield. Design the chart with an external assigned target based on 
observations 12–24. Compare the charts. Explain the differences. 
Exercise 4.6 Repeat Exercise 4.5, but this time design the chart with an externally 
assigned target based on observations 25–32. Explain the computational difﬁculty. 
Exercise 4.7 Calculate control limits for grouped data with 20 subgroups of size 5 
and 6 dimensions, with internally derived targets (Eq. (4.4.2)). How will the control 
limits change if you start monitoring a process with similar data? 
Exercise 4.8 Let X1 = (x11, x12, . . . , x1p) and X2 = (x21, x22, . . . , x2p) represent 
the mean dissolution values of tablets at p time instances of a reference product 
and a batch under test, respectively. The Mahalanobis distance T 2, between X1 
and X2, is deﬁned here as DM =
"
(X2 − X1)′S−1 
pooled(X2 −X1), where Spooled = 
(Sreference + Stest)/2, is the pooled covariance matrix of the reference and test 
samples. The conﬁdence region, CR, of the difference between batch and reference 
consists of all vectors Y satisfying: [(Y − (X2 − X1)′S−1 
pooled(Y − (X2 −X1)] ≤ 
KF0.90[p, 2n − p − 1] where F0.90[p, 2n − p − 1] is the 90th quantile of the F-
distribution with degrees of freedom p and (2n−p−1). Prove that for measurements 
conducted at one time instance (p = 1) these formulae correspond to the conﬁdence 
intervals presented in Chapter 3 of Modern Statistics (Kenett et al. 2022b).

Chapter 5 
Classical Design and Analysis of 
Experiments 
Preview Experiments are used in industry to improve productivity, reduce vari-
ability, enhance quality and obtain robust products and manufacturing processes. In 
this chapter we study how to design and analyze experiments which are aimed at 
testing scientiﬁc or technological hypotheses. These hypotheses are concerned with 
the effects of procedures or treatments on quality and productivity; or the general 
relationship between variables. Designed experiments help determine the conditions 
under which a production process yields maximum output or other optimum results, 
etc. The chapter presents the classical methods of design of experiments. It starts 
with an introductory section with examples and discusses guiding principles in 
designing experiments. The chapter covers the range of classical experimental 
designs including complete block designs, Latin squares, full and fractional factorial 
designs with factors at two and three levels. The basic approach to the analysis is 
through modeling the response variable and computing ANOVA tables. Particular 
attention is given to the generation of designs using Python. 
5.1 
Basic Steps and Guiding Principles 
The following are guiding principles for statistically designed experiments. They 
ensure high information quality (InfoQ) of a study, as introduced in Chap. 1. 
1. The objectives of a study should be well stated, and criteria established to test 
whether these objectives have been met. 
2. The response variable(s) should be clearly deﬁned so that the study objectives 
are properly translated to measurable variables. At this stage measurement 
uncertainty should be established. 
3. All factors which might affect the response variable(s) should be listed and 
speciﬁed. We call these the controllable factors. This requires interactive 
brainstorming with content experts. 
Supplementary Information The online version contains supplementary material available at 
(https://doi.org/10.1007/978-3-031-28482-3_5). 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3_5
141

142
5
Classical Design and Analysis of Experiments
4. The type of measurements or observations on all variables should be speciﬁed. 
5. The levels of the controllable factors to be tested should be determined. 
6. A statistical model should be formulated concerning the relationship between 
the pertinent variables, and their error distributions. This can rely on prior 
knowledge or literature search. 
7. An experimental layout or experimental array should be designed so that the 
inference from the gathered data will be: 
a. valid 
b. precise 
c. generalizable 
d. easy to obtain 
8. The trials should be performed, if possible, in a random order, to avoid bias 
by factors which are not taken into consideration. 
9. A protocol of execution should be prepared, including the method of analysis. 
The method of analysis and data collection depends on the design. 
10. The execution of the experiment should carefully follow the protocol with 
proper documentation. 
11. The results of the experiments should be carefully analyzed and reported 
ensuring proper documentation and traceability. Modern technology can ensure 
that data, analysis and conclusions are fully integrated and reproducible. 
12. Conﬁrmatory experiments should be conducted, to validate the inference 
(conclusions) of the experiments. 
We illustrate the above principles with two examples. 
Example 5.1 The ﬁrst example deals with a problem of determining with exper-
iments the weights of four objects. It illustrates what is an experimental layout 
(design) and why an optimal one should be chosen. 
Step 1: Formulation of Objectives 
The objective is to devise a measurement plan that will yield weight estimates of 
chemicals with maximal precision with four weighing operations. 
Step 2: Description of Response 
The weight measurement device is a chemical balance, with right and left pans. 
One or more objects can be put on either pan. The response variable Y, is the  
measurement read on the scale of the chemical balance. This is equal to the total 
weight of objects on the right pan left parenthesis plus right parenthesis(+) minus the total weight of objects on the left 
pan left parenthesis minus right parenthesis(−), plus a measurement error. 
Step 3: Controllable Variables 
We have four objects upper O 1O1, upper O 2O2, upper O 3O3, upper O 4O4, with unknown weights w 1w1, w 2w2, w 3w3, w 4w4. The  
controllable (inﬂuencing) variables are

5.1
Basic Steps and Guiding Principles
143
upper X Subscript i j Baseline equals StartLayout Enlarged left brace 1st Row 1st Column 1 comma 2nd Column if j hyphen th object is put on plus pan 2nd Row 1st Column Blank 2nd Column in the i hyphen th measurement 3rd Row 1st Column Blank 4th Row 1st Column negative 1 comma 2nd Column if j hyphen th object is put on minus pan 5th Row 1st Column Blank 2nd Column in the i hyphen th measurement period EndLayoutXij =
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
1,
if j-th object is put on + pan
in the i-th measurement
−1,
if j-th object is put on −pan
in the i-th measurement.
i comma j equals 1 comma 2 comma 3 comma 4i, j = 1, 2, 3, 4. 
Step 4: Type of Measurements 
The response Y is measured on a continuous scale in an interval left parenthesis y Superscript asterisk Baseline comma y Superscript asterisk asterisk Baseline right parenthesis(y∗, y∗∗). The  
observations are a realization of continuous random variables. 
Step 5: Levels of Controllable Variables 
upper X Subscript i j Baseline equals plus or minus 1Xij = ±1, as above. 
Step 6: A Statistical Model 
The measurement model is linear, i.e., 
upper Y Subscript i Baseline equals w 1 upper X Subscript i Baseline 1 Baseline plus w 2 upper X Subscript i Baseline 2 Baseline plus w 3 upper X Subscript i Baseline 3 Baseline plus w 4 upper X Subscript i Baseline 4 Baseline plus e Subscript iYi = w1Xi1 + w2Xi2 + w3Xi3 + w4Xi4 + ei
i equals 1 comma midline horizontal ellipsis comma 4i = 1, · · · , 4, where e 1e1, e 2e2, e 3e3, e 4e4 are independent random variables, with upper E left brace e Subscript i Baseline right brace equals 0E{ei} = 0
and upper V left brace e Subscript i Baseline right brace equals sigma squaredV {ei} = σ 2, i equals 1 comma 2 comma midline horizontal ellipsis comma 4i = 1, 2, · · · , 4. 
Step 7: Experimental Layout 
An experimental layout is represented by a 4 times 44 × 4 matrix 
left parenthesis upper X right parenthesis equals left parenthesis upper X Subscript i j Baseline semicolon i comma j equals 1 comma midline horizontal ellipsis comma 4 right parenthesis period(X) = (Xij; i, j = 1, · · · , 4).
Such a matrix is called a design matrix. 
Given a design matrix left parenthesis upper X right parenthesis(X), and a vector of measurements bold upper Y equals left parenthesis upper Y 1 comma midline horizontal ellipsis comma upper Y 4 right parenthesis primeY = (Y1, · · · , Y4)′, 
we estimate bold w equals left parenthesis w 1 comma midline horizontal ellipsis comma w 4 right parenthesis primew = (w1, · · · , w4)′ by 
ModifyingAbove bold upper W With caret equals left parenthesis upper L right parenthesis bold upper Y comma ˆW = (L)Y,
where left parenthesis upper L right parenthesis(L) is a 4 times 44 × 4 matrix. We say that the design is valid, if there exists a matrix 
L such that upper E left brace ModifyingAbove bold upper W With caret right brace equals bold wE{ ˆW} = w. Any  non-singular design matrix left parenthesis upper X right parenthesis(X) represents a valid 
design with left parenthesis upper L right parenthesis equals left parenthesis upper X right parenthesis Superscript negative 1(L) = (X)−1. 
Indeed, upper E left brace bold upper Y right brace equals left parenthesis upper X right parenthesis bold wE{Y} = (X)w. Hence 
upper E left brace ModifyingAbove bold upper W With caret right brace equals left parenthesis upper X right parenthesis Superscript negative 1 Baseline upper E left brace bold upper Y right brace equals bold w periodE{ ˆW} = (X)−1E{Y} = w.
The precision of the design matrix left parenthesis upper X right parenthesis(X) is measured by left parenthesis sigma summation Underscript i equals 1 Overscript 4 Endscripts upper V left brace ModifyingAbove upper W With caret Subscript i Baseline right brace right parenthesis Superscript negative 1
4
i=1 V { ˆWi}
−1
. The  
problem is to ﬁnd a design matrix left parenthesis upper X right parenthesis Superscript 0(X)0 which maximizes the precision.

144
5
Classical Design and Analysis of Experiments
It can be shown that an optimal design is given by the orthogonal array 
left parenthesis upper X right parenthesis Superscript 0 Baseline equals Start 1 By 1 Matrix 1st Row StartLayout 1st Row 1st Column 1 2nd Column negative 1 3rd Column negative 1 4th Column 1 2nd Row 1st Column 1 2nd Column 1 3rd Column negative 1 4th Column negative 1 3rd Row 1st Column 1 2nd Column negative 1 3rd Column 1 4th Column negative 1 4th Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 1 EndLayout EndMatrix(X)0 =
⎡
⎢⎢⎣
1 −1 −1
1
1
1 −1 −1
1 −1
1 −1
1
1
1
1
⎤
⎥⎥⎦
or any row (or column) permutation of this matrix. Notice that in this design, in each 
one of the ﬁrst three weighing operation (row) two objects are put on the left pan 
left parenthesis minus right parenthesis(−) and two on the right. Also, each object, excluding the ﬁrst, is put twice on left parenthesis minus right parenthesis(−)
and twice on left parenthesis plus right parenthesis(+). The weight estimates under this design as 
ModifyingAbove bold upper W With caret equals one fourth Start 1 By 1 Matrix 1st Row StartLayout 1st Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 1 2nd Row 1st Column negative 1 2nd Column 1 3rd Column negative 1 4th Column 1 3rd Row 1st Column negative 1 2nd Column negative 1 3rd Column 1 4th Column 1 4th Row 1st Column 1 2nd Column negative 1 3rd Column negative 1 4th Column 1 EndLayout EndMatrix Start 4 By 1 Matrix 1st Row upper Y 1 2nd Row q u a d upper Y 2 3rd Row q u a d upper Y 3 4th Row q u a d upper Y 4 EndMatrix period ˆW = 1
4
⎡
⎢⎢⎣
1
1
1
1
−1
1 −1
1
−1 −1
1
1
1 −1 −1
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
Y1
Y2
Y3
Y4
⎤
⎥⎥⎦.
Moreover, 
sigma summation Underscript i equals 1 Overscript 4 Endscripts upper V left brace ModifyingAbove upper W With caret Subscript i Baseline right brace equals sigma squared period
4

i=1
V { ˆWi} = σ 2.
The order of measurements is random.
black medium square■
Example 5.2 The second example illustrates a complex process, with a large 
number of factors which may affect yield variables. 
Wave soldering of circuit pack assemblies (CPA) is an automated process of 
soldering which, if done in an optimal fashion, can raise quality and productivity. 
The process, however, involves three phases and many variables. We analyze the 
various steps required for designing an experiment to learn the effects of the various 
factors on the process. We follow the process description of Lin and Kacker (2012). 
If the soldering process yields good results, the CPA’s can proceed directly 
to automatic testing. This is a big savings in direct labor cost and increase in 
productivity. The wave soldering process (WSP) is in three phases. In Phase I, 
called ﬂuxing, the solder joint surfaces are cleaned by the soldering ﬂux, which 
also protects it against reoxidation. The ﬂuxing lowers the surface tension for better 
solder wetting and solder joint formation. 
Phase II of the WSP is the soldering assembly. This is performed in a cascade of 
wave soldering machine. After preheating the solution, the non-component side of 
the assembly is immersed in a solder wave for 1–2 s. All solder points are completed 
when the CPA exits the wave. Preheating must be gradual. The correct heating 
is essential to effective soldering. Also important is the conveyor speed and the 
conveyor’s angle. The last phase, Phase III, of the process is that of detergent 
cleaning. The assembly is ﬁrst washed in detergent solution, then rinsed in water

5.1
Basic Steps and Guiding Principles
145
and ﬁnally dried with hot air. The temperature of the detergent solution is raised to 
achieve effective cleaning and prevent excessive foaming. The rinse water is heated 
to obtain effective rinsing. 
We list now the design steps: 
1. Objectives. To ﬁnd the effects of the various factors on the quality of wave 
soldering and optimize the process. 
2. Response Variables. There are four yield variables 
a. Insulation resistance 
b. Cleaning characterization 
c. Soldering efﬁciency 
d. Solder mask cracking 
3. Controllable Variables. There are 17 variables (factors) associated with the 
three phases of the process. 
I. Flux formulation
II. Wave Soldering
III. Detergent cleaning 
A. Type of activator
H. Amount of Flux
N. Detergent concentration 
B. Amount of activator
I. Preheat time
O. Detergent temperature 
C. Type of surfactant
J. Solder temperature
P. Cleaning conveyor speed 
D. Amount of surfactant
K. Conveyor speed
Q. Rinse water temperature 
E. Amount of antioxidant
L. Conveyor angle 
F. Type of solvent
M. Wave height setting 
G. Amount of solvent 
4. Measurements 
a. Insulation resistance test at 30 min, 1 and 4 days after soldering at 
• 
.−35C, 90% RN, no bias voltage 
• 
.−65C, 90% RH, no bias voltage 
• 
(continuous variable). 
b. Cleaning characterization: The amounts of residues on the board (continuous 
variable). 
c. Soldering efﬁciency: Visual inspection of no solder, insufﬁcient solder, good 
solder, excess solder and other defects (discrete variables). 
d. Solder mask cracking: Visual inspection of cracked spots on the solder mask 
(discrete variables). 
5. Levels of Controllable Factors 
Factor 
# levels  
Factor 
# levels  
Factor 
# levels  
A
2
H
3
N
2 
B
3
I
3
O
2 
C
2
J
3
P
3 
D
3
K
3
Q
2 
E
3
L
3 
F
2
M
2 
G
3

146
5
Classical Design and Analysis of Experiments
6. The Statistical Model. The response variables are related to the controllable 
variables by linear models having “main effects” and “interaction” parameters, 
as will be explained in Sect. 5.3. 
7. The Experiment Layout. A fractional factorial experiment, as explained in 
Sect. 5.8, is designed. Such a design is needed, because a full factorial design 
contains 3 Superscript 10 Baseline 2 Superscript 7 Baseline equals 7,558,27231027 = 7,558,272 possible combinations of factor levels. A fractional 
replication design chooses a manageable fraction of the full factorial in a manner 
that allows valid inference, and precise estimates of the parameters of interest. 
8. Protocol of Execution. Suppose that it is decided to perform a fraction of 3 cubed 2 squared equals 1083322 =
108 trials at certain levels of the 17 factors. However, the set-up of the factors 
takes time and one cannot perform more than 4 trials a day. The experiment 
will last 27 days. It is important to construct the design so that the important 
effects, to be estimated, will not be confounded with possible differences between 
days (blocks). The order of the trials within each day is randomized as well as, 
the trials which are assigned to different days. Randomization is an important 
component of the design, which comes to enhance its validity. The execution 
protocol should clearly specify the order of execution of the trials. 
black medium square■
5.2 
Blocking and Randomization 
Blocking and randomization are used in planning of experiments, in order to 
increase the precision of the outcome and ensure the validity of the inference. 
Blocking is used to reduce errors. A block is a portion of the experimental material 
that is expected to be more homogeneous than the whole aggregate. For example, if 
the experiment is designed to test the effect of polyester coating of electronic circuits 
on their current output, the variability between circuits could be considerably 
bigger than the effect of the coating on the current output. In order to reduce this 
component of variance, one can block by circuit. Each circuit will be tested under 
two treatments: no-coating and coating. We ﬁrst test the current output of a circuit 
without coating. Later we coat the circuit, and test again. Such a comparison of 
before and after a treatment, of the same units, is called paired comparison. 
Another example of blocking is the boy’s shoes examples of Box et al. (2005). 
Two kinds of shoe soles’ materials are to be tested by ﬁxing the soles on n pairs of 
boys’ shoes, and measuring the amount of wear of the soles after a period of actively 
wearing the shoes. Since there is high variability between activity of boys, if m pairs 
will be with soles of one type and the rest of the other, it will not be clear whether 
any difference that might be observed in the degree of wearout is due to differences 
between the characteristics of the sole material or to the differences between the 
boys. By blocking by pair of shoes, we can reduce much of the variability. Each pair

5.3
Additive and Non-additive Linear Models
147
of shoes is assigned the two types of soles. The comparison within each block is free 
of the variability between boys. Furthermore, since boys use their right or left foot 
differently, one should assign the type of soles to the left or right shoes at random. 
Thus, the treatments (two types of soles) are assigned within each block at random. 
Other examples of blocks could be machines, shifts of production, days of the 
week, operators, etc. 
Generally, if there are t treatments to compare, and b blocks, and if all t 
treatments can be performed within a single block, we assign all the t treatments 
to each block. The order of applying the treatments within each block should be 
randomized. Such a design is called a randomized complete block design. We  
will see later how a proper analysis of the yield can validly test for the effects of 
the treatments. 
If not all treatments can be applied within each block it is desirable to assign 
treatments to blocks in some balanced fashion. Such designs, to be discussed later, 
are called balanced incomplete block designs (BIBD). 
Randomization within each block is important also to validate the assumption 
that the error components in the statistical model are independent. This assumption 
may not be valid if treatments are not assigned at random to the experimental units 
within each block. 
5.3 
Additive and Non-additive Linear Models 
Seventeen factors which might inﬂuence the outcome in WSP are listed in Exam-
ple 5.2. Some of these factors, like type of activator left parenthesis upper A right parenthesis(A), or type of surfactant left parenthesis upper C right parenthesis(C) are 
categorical variables. The number of levels listed for these factors was 2. That is, the 
study compares the effects of two types of activators and two types of surfactants. 
If the variables are continuous, like amount of activator left parenthesis upper B right parenthesis(B), we can use a 
regression linear model to represent the effects of the factors on the yield variables. 
Such models will be discussed later (Sect. 5.7). In the present section linear models 
which are valid for both categorical or continuous variables are presented. 
For the sake of explanation, let us start ﬁrst with a simple case, in which the 
response depends on one factor only. Thus, let A designate some factor, which is 
applied at different levels, upper A 1 comma midline horizontal ellipsis comma upper A Subscript a BaselineA1, · · · , Aa. These could be a categories. The levels of 
A are also called “treatments.” 
Suppose that at each level of A we make n independent repetitions (replicas) of 
the experiment. Let upper Y Subscript i jYij, i equals 1 comma midline horizontal ellipsis comma ai = 1, · · · , a and j equals 1 comma midline horizontal ellipsis comma nj = 1, · · · , n denote the observed yield 
at the j-th replication of level upper A Subscript iAi. We model the random variables upper Y Subscript i jYij as 
upper Y Subscript i j Baseline equals mu plus tau Subscript i Superscript upper A Baseline plus e Subscript i j Baseline comma i equals 1 comma midline horizontal ellipsis comma a comma j equals 1 comma midline horizontal ellipsis comma n commaYij = μ + τ A
i + eij,
i = 1, · · · , a,
j = 1, · · · , n,
(5.3.1)

148
5
Classical Design and Analysis of Experiments
where muμ and tau 1 Superscript upper A Baseline comma midline horizontal ellipsis comma tau Subscript a Superscript upper Aτ A
1 , · · · , τ A
a are unknown parameters, satisfying 
sigma summation Underscript i equals 1 Overscript a Endscripts tau Subscript i Superscript upper A Baseline equals 0 period
a

i=1
τ A
i = 0.
(5.3.2) 
e Subscript i jeij, i equals 1 comma midline horizontal ellipsis comma ai = 1, · · · , a, j equals 1 comma midline horizontal ellipsis comma nj = 1, · · · , n, are independent random variables such that, 
upper E left brace e Subscript i j Baseline right brace equals 0 and upper V left brace e Subscript i j Baseline right brace equals sigma squared commaE{eij} = 0 and V {eij} = σ 2,
(5.3.3) 
for all i equals 1 comma midline horizontal ellipsis comma ai = 1, · · · , a; j equals 1 comma midline horizontal ellipsis comma nj = 1, · · · , n. 
Let 
upper Y overbar Subscript i Baseline equals StartFraction 1 Over n EndFraction sigma summation Underscript j equals 1 Overscript n Endscripts upper Y Subscript i j Baseline comma i equals 1 comma midline horizontal ellipsis comma a period¯Yi = 1
n
n

j=1
Yij,
i = 1, · · · , a.
The expected values of these means are 
upper E left brace upper Y overbar Subscript i Baseline right brace equals mu plus tau Subscript i Superscript upper A Baseline comma i equals 1 comma midline horizontal ellipsis comma k periodE{ ¯Yi} = μ + τ A
i ,
i = 1, · · · , k.
(5.3.4) 
Let 
upper Y overbar overbar equals StartFraction 1 Over k EndFraction sigma summation Underscript i equals 1 Overscript k Endscripts comma upper Y overbar Subscript i Baseline period ¯¯Y = 1
k
k

i=1
, ¯Yi.
(5.3.5) 
This is the mean of all upper N equals k times nN = k × n observations (the grand mean), since sigma summation Underscript i equals 1 Overscript a Endscripts tau Subscript i Superscript upper A Baseline equals 0a
i=1 τ A
i =
0, we obtain that 
upper E left brace upper Y overbar overbar right brace equals mu periodE{ ¯¯Y} = μ.
(5.3.6) 
The parameter tau Subscript i Superscript upper Aτ A
i is called the main effect of A at level i. 
If there are two factors, A and B, at  a and b levels respectively, there are a times ba × b
treatment combinations left parenthesis upper A Subscript i Baseline comma upper B Subscript j Baseline right parenthesis(Ai, Bj), i equals 1 comma midline horizontal ellipsis comma ai = 1, · · · , a, j equals 1 comma midline horizontal ellipsis comma bj = 1, · · · , b. Suppose also that 
n independent replicas are made at each one of the treatment combinations. The 
yield at the k-th replication of treatment combination left parenthesis upper A Subscript i Baseline comma upper B Subscript j Baseline right parenthesis(Ai, Bj) is given by 
upper Y Subscript i j k Baseline equals mu plus tau Subscript i Superscript upper A Baseline plus tau Subscript j Superscript upper B Baseline plus tau Subscript i j Superscript upper A upper B Baseline plus e Subscript i j k Baseline periodYijk = μ + τ A
i + τ B
j + τ AB
ij
+ eijk.
(5.3.7) 
The error terms e Subscript i j keijk are independent random variables satisfying 
upper E left brace e Subscript i j l Baseline right brace equals 0 comma upper V left brace e Subscript i j l Baseline right brace equals sigma squared commaE{eijl} = 0, V {eijl} = σ 2,
(5.3.8) 
for all i equals 1 comma midline horizontal ellipsis comma ai = 1, · · · , a, j equals 1 comma midline horizontal ellipsis comma bj = 1, · · · , b, k equals 1 comma midline horizontal ellipsis comma nk = 1, · · · , n. 
We further assume that

5.4
The Analysis of Randomized Complete Block Designs
149
StartLayout 1st Row 1st Column sigma summation Underscript j equals 1 Overscript b Endscripts tau Subscript i j Superscript upper A upper B 2nd Column equals 0 comma i equals 1 comma midline horizontal ellipsis comma a 2nd Row 1st Column sigma summation Underscript i equals 1 Overscript a Endscripts tau Subscript i j Superscript upper A upper B 2nd Column equals 0 comma j equals 1 comma midline horizontal ellipsis comma b period 3rd Row 1st Column sigma summation Underscript i equals 1 Overscript a Endscripts tau Subscript i Superscript upper A 2nd Column equals 0 comma 4th Row 1st Column sigma summation Underscript j equals 1 Overscript b Endscripts tau Subscript j Superscript upper B 2nd Column equals 0 period EndLayout
b

j=1
τ AB
ij
= 0,
i = 1, · · · , a
a

i=1
τ AB
ij
= 0,
j = 1, · · · , b.
a

i=1
τ A
i = 0,
b

j=1
τ B
j = 0.
(5.3.9) 
tau Subscript i Superscript upper Aτ A
i is the main effect of A at level i, tau Subscript j Superscript upper Bτ B
j is the main effect of B at level j, and tau Subscript i j Superscript upper A upper Bτ AB
ij
is the interaction effect at left parenthesis upper A Subscript i Baseline comma upper B Subscript j Baseline right parenthesis(Ai, Bj). 
If all the interaction effects are zero then the model reduces to 
upper Y Subscript i j k Baseline equals mu plus tau Subscript i Superscript upper A Baseline plus tau Subscript j Superscript upper B Baseline plus e Subscript i j k Baseline periodYijk = μ + τ A
i + τ B
j + eijk.
(5.3.10) 
Such a model is called additive. If not all the interaction components are zero then 
the model is called non-additive. 
This model is generalized in a straightforward manner to include a larger number 
of factors. Thus, for three factors, there are three types of main effect terms, tau Subscript i Superscript upper Aτ A
i , 
tau Subscript j Superscript upper Bτ B
j and tau Subscript k Superscript upper Cτ C
k ; three types of interaction terms tau Subscript i j Superscript upper A upper Bτ AB
ij , tau Subscript i k Superscript upper A upper Cτ AC
ik
and tau Subscript j k Superscript upper B upper Cτ BC
jk ; and one type of 
interaction tau Subscript i j k Superscript upper A upper B upper Cτ ABC
ijk
. 
Generally, if there are p factors, there are 2 Superscript p2p types of parameters, 
mu comma tau Subscript i Superscript upper A Baseline comma tau Subscript j Superscript upper B Baseline comma midline horizontal ellipsis comma tau Subscript i j Superscript upper A upper B Baseline comma tau Subscript i k Superscript upper A upper C Baseline comma midline horizontal ellipsis comma tau Subscript i j k Superscript upper A upper B upper C Baseline comma midline horizontal ellipsisμ, τ A
i , τ B
j , · · · , τ AB
ij , τ AC
ik , · · · , τ ABC
ijk
, · · ·
etc. Interaction parameters between two factors are called 1st order interactions. 
Interaction parameters between three factors are called 2-nd order interactions, and 
so on. In particular modelling it is often assumed that all interaction parameters of 
higher than 1st order are zero. 
5.4 
The Analysis of Randomized Complete Block Designs 
5.4.1 
Several Blocks, Two Treatments per Block: Paired 
Comparison 
As in the shoe soles example, or the example of the effect of polyester coating on 
circuits output, there are two treatments applied in each one of n blocks. The linear 
model can be written as

150
5
Classical Design and Analysis of Experiments
upper Y Subscript i j Baseline equals mu plus tau Subscript i Baseline plus beta Subscript j Baseline plus e Subscript i j Baseline comma i equals 1 comma 2 semicolon j equals 1 comma midline horizontal ellipsis comma n commaYij = μ + τi + βj + eij,
i = 1, 2; j = 1, · · · , n,
(5.4.1) 
where tau Subscript iτi is the effect of the i-th treatment and beta Subscript jβj is the effect of the j-th block. e Subscript i jeij
is an independent random variable, representing the experimental random error or 
deviation. It is assumed that upper E left brace e Subscript i j Baseline right brace equals 0E{eij} = 0 and upper V left brace e Subscript i j Baseline right brace equals sigma Subscript e Superscript 2V {eij} = σ 2
e . Since we are interested 
in testing whether the two treatments have different effects, the analysis is based on 
the within block differences 
upper D Subscript j Baseline equals upper Y Subscript 2 j Baseline minus upper Y Subscript 1 j Baseline equals tau 2 minus tau 1 plus e Subscript j Superscript asterisk Baseline comma j equals 1 comma midline horizontal ellipsis comma n periodDj = Y2j −Y1j = τ2 −τ1 + e∗
j,
j = 1, · · · , n.
(5.4.2) 
The error terms e Subscript j Superscript asteriske∗
j are independent random variables with upper E left brace e Subscript j Superscript asterisk Baseline right brace equals 0E{e∗
j} = 0 and upper V left brace e Subscript j Superscript asterisk Baseline right brace equals sigma Subscript d Superscript 2V {e∗
j} =
σ 2
d , j equals 1 comma midline horizontal ellipsis comma nj = 1, · · · , n where sigma Subscript d Superscript 2 Baseline equals 2 sigma Subscript e Superscript 2σ 2
d = 2σ 2
e . 
An unbiased estimator of sigma Subscript d Superscript 2σ 2
d is 
upper S Subscript d Superscript 2 Baseline equals StartFraction 1 Over n minus 1 EndFraction sigma summation Underscript j equals 1 Overscript n Endscripts left parenthesis upper D Subscript j Baseline minus upper D overbar Subscript n Baseline right parenthesis squared commaS2
d =
1
n −1
n

j=1
(Dj −¯Dn)2,
(5.4.3) 
where upper D overbar Subscript n Baseline equals StartFraction 1 Over n EndFraction sigma summation Underscript j equals 1 Overscript n Endscripts upper D Subscript j ¯Dn = 1
n
n
j=1 Dj. The hypotheses to be tested are: 
upper H 0 colon delta equals tau 2 minus tau 1 equals 0H0 : δ = τ2 −τ1 = 0
against 
upper H 1 colon delta not equals 0 periodH1 : δ ̸= 0.
5.4.1.1 
The t-Test 
Most commonly used is the t-test, in which upper H 0H0 is tested by computing the test 
statistic 
t equals StartFraction StartRoot n EndRoot upper D overbar Subscript n Baseline Over upper S Subscript d Baseline EndFraction periodt =
√n ¯Dn
Sd
.
(5.4.4) 
If e 1 Superscript asterisk Baseline comma midline horizontal ellipsis comma e Subscript n Superscript asteriske∗
1, · · · , e∗
n are i.i.d., normally distributed then, under the null hypothesis, t has a 
t-distribution with left parenthesis n minus 1 right parenthesis(n −1) degrees of freedom. In this case, upper H 0H0 is rejected if 
StartAbsoluteValue t EndAbsoluteValue greater than t Subscript 1 minus alpha divided by 2 Baseline left bracket n minus 1 right bracket comma|t| > t1−α/2[n −1],
where alphaα is the selected level of signiﬁcance.

5.4
The Analysis of Randomized Complete Block Designs
151
Table 5.1 Sign assignments 
and values of upper Y overbar¯Y
Signs
D 
minus−1 
−1 
−1 
−1 
−0.55 
1 
−1 
−1 
−1
0 
minus−1 
1 
−1 
−1 
−0.4 
1 
1 
−1 
−1
0.15 
minus−1 
−1
1 
−1 
−0.20 
1 
−1
1 
−1
0.35 
minus−1 
1
1 
−1 
−0.05 
1 
1
1 
−1
0.50 
minus−1 
−1 
−1
1 
−0.50 
1 
−1 
−1
1
0.05 
minus−1 
1 
−1
1 
−0.35 
1 
1 
−1
1
0.2 
minus−1 
−1
1
1 
−0.15 
1 
−1
1
1
0.40 
minus−1 
1
1
1
0 
1 
1
1
1
0.55 
5.4.1.2 
Randomization Tests 
A randomization test for paired comparison, constructs a reference distribution of 
all possible averages of the differences that can be obtained by randomly assigning 
the sign plus+ or minus−to the value of upper D Subscript iDi. It computes then an average difference upper D overbar ¯D for 
each one of the 2 Superscript n2n sign assignments. 
The P -value of the test, for the two-sided alternative, is determined according to 
this reference distribution, by 
upper P equals probability left brace upper Y overbar greater than or equals Observed upper D overbar right brace periodP = Pr{ ¯Y ≥Observed ¯D}.
For example, suppose we have four differences, with values 1.1, 0.3, negative 0.7−0.7, negative 0.1−0.1. 
The mean is upper D overbar Subscript 4 Baseline equals 0.15 ¯D4 = 0.15. There are 2 Superscript 4 Baseline equals 1624 = 16 possible ways of assigning a sign to 
StartAbsoluteValue upper D Subscript i Baseline EndAbsoluteValue|Di|. Let upper X Subscript i Baseline equals plus or minus 1Xi = ±1 and upper Y overbar equals one fourth sigma summation Underscript i equals 1 Overscript 4 Endscripts upper X StartAbsoluteValue upper D Subscript i Baseline EndAbsoluteValue ¯Y = 1
4
4
i=1 X|Di|. The possible combinations are listed in 
Table 5.1 
Under the reference distribution, all these possible means are equally probable. 
The P -value associated with the observed upper D overbar equals 0.15 ¯D = 0.15 is upper P equals seven fifteenths equals 0.47P =
7
15 = 0.47. If the  
number of pairs (blocks) n is large the procedure becomes cumbersome, since we 
have to determine all the 2 Superscript n2n sign assignments. If n equals 20n = 20 there are 2 Superscript 20 Baseline equals 1,048,576220 = 1,048,576
such assignments. We can, however, estimate the P-value by taking a RSWR 
from this reference distribution. In Python this is performed with the following 
commands:

152
5
Classical Design and Analysis of Experiments
Fig. 5.1 Stem-and-leaf plot 
of 200 random difference 
averages 
1
-4
3
 
2
-3
7
 
7
-3 
55333 
16
-3 
097766555 
25
-2 
444433111 
38
-2 
0999998776666 
62
-1 
555444433333333333222211 
82 
0 
99999999888887776655 
(34) 
0 
5444333332222111110000112333333444 
84 
0 
5666666666777779999999999 
59 
1 
011133333333555 
44 
1 
555555666666799999 
26 
2 
01112222333 
15 
2 
5677888 
8 
3 
0002 
4
3
 99
 
2
4
 1
 
1
4
 5
 
random.seed(1) 
X = [1.1, 0.3, -0.7, -0.1] 
m = 20000 
Di = pd.DataFrame([random.choices((-1, 1), k=len(X)) for _ in range(m)]) 
DiX = (Di * X) 
np.mean(DiX.mean(axis=1) > np.mean(X)) 
0.31425 
Example 5.3 We analyze here the results of the shoe soles experiment, as reported 
in Box et al. (2005). The observed differences in the wear of the soles, between type 
B and type A, for n equals 10n = 10 children, are: 
0.8 comma 0.6 comma 0.3 comma negative 0.1 comma 1.1 comma negative 0.2 comma 0.3 comma 0.5 comma 0.5 comma 0.3 period0.8, 0.6, 0.3, −0.1, 1.1, −0.2, 0.3, 0.5, 0.5, 0.3.
The average difference is upper D overbar Subscript 10 Baseline equals 0.41 ¯D10 = 0.41. 
A t-test of upper H 0H0, using the observed differences, is obtained using ttest_1samp from 
the scipy package. 
X = [0.8, 0.6, 0.3, -0.1, 1.1, -0.2, 0.3, 0.5, 0.5, 0.3] 
statistic, pvalue = stats.ttest_1samp(X, 0.0) 
print(f't {statistic:.2f}') 
print(f'pvalue {pvalue:.4f}') 
t 3.35 
pvalue 0.0085 
The randomization test is also straightforward in Python. A stem-and-leaf plot of 
the 200 random difference averages is shown in Fig. 5.1.

5.4
The Analysis of Randomized Complete Block Designs
153
random.seed(1) 
X = [0.8, 0.6, 0.3, -0.1, 1.1, -0.2, 0.3, 0.5, 0.5, 0.3] 
m = 200 
Di = pd.DataFrame([random.choices((-1, 1), k=len(X)) for _ in range(m)]) 
DiX = (Di * X) 
means = DiX.mean(axis=1) 
Pestimate = np.mean(DiX.mean(axis=1) > np.mean(X)) 
print(f'P_estimate: {Pestimate}') 
P_estimate: 0.01 
According to this, the P-value is estimated as 
ModifyingAbove upper P With caret equals 0.01 period ˆP = 0.01.
This estimate is almost the same as the P-value of the t-test.
black medium square■
5.4.2 
Several Blocks, t Treatments per Block 
As mentioned earlier, the Randomized Complete Block Designs (RCBD) are those 
in which each block contains all the t treatments. The treatments are assigned to the 
experimental units in each block at random. Let b denote the number of blocks. The 
linear model for these designs is 
upper Y Subscript i j Baseline equals mu plus tau Subscript i Baseline plus beta Subscript j Baseline plus e Subscript i j Baseline comma i equals 1 comma midline horizontal ellipsis comma t j equals 1 comma midline horizontal ellipsis comma b commaYij = μ + τi + βj + eij,
i = 1, · · · , t j = 1, · · · , b,
(5.4.5) 
where upper Y Subscript i jYij is the yield of the i-th treatment in the j-th block. The main effect of the 
i-th treatment is tau Subscript iτi, and the main effect of the j-th block is beta Subscript jβj. It is assumed that  the  
effects are additive (no interaction). Under this assumption, each treatment is tried 
only once in each block. The different blocks serve the role of replicas. However, 
since the blocks may have additive effects, beta Subscript jβj, we have to adjust for the effects of 
blocks in estimating sigma squaredσ 2. This is done as shown in the ANOVA table below. 
Further assume that, e Subscript i jeij are the error random variables with upper E left brace e Subscript i j Baseline right brace equals 0E{eij} = 0 and 
upper V left brace e Subscript i j Baseline right brace equals sigma squaredV {eij} = σ 2 for all left parenthesis i comma j right parenthesis(i, j). The ANOVA for this model is presented in Table 5.2. In  
this table, 
Table 5.2 ANOVA table for RCBD 
Source of variation
DF
SS
MS
upper E left brace upper M upper S right braceE{MS}
Treatments
t minus 1t −1
SST R
MST R
sigma squared plus StartFraction b Over t minus 1 EndFraction sigma summation Underscript i equals 1 Overscript t Endscripts tau Subscript i Superscript 2σ 2 +
b
t−1
t
i=1 τ 2
i
Blocks
b minus 1b −1
SSBL
MSBL
sigma squared plus StartFraction t Over b minus 1 EndFraction sigma summation Underscript j equals 1 Overscript b Endscripts beta Subscript j Superscript 2σ 2 +
t
b−1
b
j=1 β2
j
Error
left parenthesis t minus 1 right parenthesis left parenthesis b minus 1 right parenthesis(t −1)(b −1)
SSE
MSE
sigma squaredσ 2
Total
t b minus 1tb −1
SST
–

154
5
Classical Design and Analysis of Experiments
StartLayout 1st Row 1st Column upper S upper S upper T 2nd Column equals sigma summation Underscript i equals 1 Overscript t Endscripts sigma summation Underscript j equals 1 Overscript b Endscripts left parenthesis upper Y Subscript i j Baseline minus upper Y overbar overbar right parenthesis squared comma EndLayout StartLayout 2nd Row 1st Column upper S upper S upper T upper R 2nd Column equals b sigma summation Underscript i equals 1 Overscript t Endscripts left parenthesis upper Y overbar Subscript i period Baseline minus upper Y overbar overbar right parenthesis squared comma EndLayout StartLayout 3rd Row 1st Column upper S upper S upper B upper L 2nd Column equals t sigma summation Underscript j equals 1 Overscript b Endscripts left parenthesis upper Y overbar Subscript period j Baseline minus upper Y overbar overbar right parenthesis squared comma EndLayoutSST =
t
i=1
b

j=1
(Yij −¯¯Y)2,
(5.4.6) 
SST R = b 
t
i=1 
( ¯Yi. − ¯¯Y)2,
(5.4.7) 
SSBL = t 
b

j=1 
( ¯Y.j − ¯¯Y)2,
(5.4.8) 
and 
StartLayout 1st Row upper S upper S upper E equals upper S upper S upper T minus upper S upper S upper T upper R minus upper S upper S upper B upper L period EndLayoutSSE = SST −SST R −SSBL.
upper Y overbar Subscript i period Baseline equals StartFraction 1 Over b EndFraction sigma summation Underscript j equals 1 Overscript b Endscripts upper Y Subscript i j Baseline comma upper Y overbar Subscript period j Baseline equals StartFraction 1 Over t EndFraction sigma summation Underscript i equals 1 Overscript t Endscripts upper Y Subscript i j Baseline ¯Yi. = 1
b
b

j=1
Yij,
¯Y.j = 1
t
t
i=1
Yij
(5.4.9) 
and upper Y overbar overbar¯¯Y is the grand mean. 
The signiﬁcance of the treatment effects is tested by the F-statistic 
upper F Subscript t Baseline equals StartFraction upper M upper S upper T upper R Over upper M upper S upper E EndFraction periodFt = MST R
MSE .
(5.4.10) 
The signiﬁcance of the block effects is tested by 
upper F Subscript b Baseline equals StartFraction upper M upper S upper B upper L Over upper M upper S upper E EndFraction periodFb = MSBL
MSE .
(5.4.11) 
These statistics are compared with the corresponding left parenthesis 1 minus alpha right parenthesis(1 −α)-th quantile of the 
F-distribution. Under the assumption that sigma summation Underscript i equals 1 Overscript t Endscripts tau Subscript i Baseline equals 0t
i=1 τi = 0, the main effects of the 
treatments are estimated by 
ModifyingAbove tau With caret Subscript i Baseline equals upper Y overbar Subscript i period Baseline minus upper Y overbar overbar comma i equals 1 comma midline horizontal ellipsis comma t periodˆτi = ¯Yi. −¯¯Y,
i = 1, · · · , t.
(5.4.12) 
These are least squares estimates. Each such estimation is a linear contrast 
ModifyingAbove tau With caret Subscript i Baseline equals sigma summation Underscript i prime equals 1 Overscript t Endscripts c Subscript i i Sub Superscript prime Subscript Baseline upper Y overbar Subscript i prime period Baseline commaˆτi =
t
i′=1
cii′ ¯Yi′.,
(5.4.13) 
where 
c Subscript i i prime Baseline equals StartLayout Enlarged left brace 1st Row 1st Column 1 minus StartFraction 1 Over t EndFraction comma 2nd Column if i equals i Superscript prime Baseline 2nd Row 1st Column Blank 3rd Row 1st Column minus StartFraction 1 Over t EndFraction comma 2nd Column if i not equals i Superscript prime Baseline period EndLayoutcii′ =
⎧
⎪⎪⎨
⎪⎪⎩
1 −1
t ,
if i = i′
−1
t ,
if i ̸= i′.
(5.4.14)

5.4
The Analysis of Randomized Complete Block Designs
155
Hence, 
StartLayout 1st Row 1st Column upper V left brace ModifyingAbove tau With caret Subscript i Baseline right brace 2nd Column equals StartFraction sigma squared Over b EndFraction sigma summation Underscript i prime equals 1 Overscript t Endscripts c Subscript i i prime Superscript 2 Baseline 2nd Row 1st Column Blank 2nd Column equals StartFraction sigma squared Over b EndFraction left parenthesis 1 minus StartFraction 1 Over t EndFraction right parenthesis comma i equals 1 comma midline horizontal ellipsis comma t period EndLayout
V {ˆτi} = σ 2
b
t
i′=1
c2
ii′
= σ 2
b

1 −1
t

,
i = 1, · · · , t.
(5.4.15) 
An unbiased estimator of sigma squaredσ 2 is given by MSE. Thus, simultaneous conﬁdence 
intervals for tau Subscript iτi left parenthesis i equals 1 comma midline horizontal ellipsis comma t right parenthesis(i = 1, · · · , t), according to the Scheffé method, are 
ModifyingAbove tau With caret Subscript i Baseline plus or minus upper S Subscript alpha Baseline left parenthesis StartFraction upper M upper S upper E Over b EndFraction left parenthesis 1 minus StartFraction 1 Over t EndFraction right parenthesis right parenthesis Superscript 1 divided by 2 Baseline comma i equals 1 comma midline horizontal ellipsis comma t commaˆτi ± Sα
MSE
b

1 −1
t
1/2
,
i = 1, · · · , t,
(5.4.16) 
where 
upper S Subscript alpha Baseline equals left parenthesis left parenthesis t minus 1 right parenthesis upper F Subscript 1 minus alpha Baseline left bracket t minus 1 comma left parenthesis t minus 1 right parenthesis left parenthesis b minus 1 right parenthesis right bracket right parenthesis Superscript 1 divided by 2 Baseline periodSα = ((t −1)F1−α[t −1, (t −1)(b −1)])1/2 .
Example 5.4 In Example 4.2 we estimated the effects of hybrids on the resistance in 
cards. We have t equals 6t = 6 hybrids (treatments) on a card, and 32 cards. We can test now 
whether there are signiﬁcant differences between the cards, by considering the cards 
as blocks, and using the ANOVA for RCBD. In this case, b equals 32b = 32. Using two-way 
Anova in statsmodels and dataset HADPAS.csv. 
hadpas = mistat.load_data('HADPAS') 
model = smf.ols('res3 ~ C(diska) + C(hyb)', data=hadpas).fit() 
print(anova.anova_lm(model)) 
df
sum_sq
mean_sq
F
PR(>F) 
C(diska)
31.0 
2.804823e+06
90478.160618
26.863283 
1.169678e-47 
C(hyb)
5.0 
1.780741e+06 
356148.170833 
105.741638 
4.017015e-48 
Residual 
155.0 
5.220551e+05
3368.097715
NaN
NaN 
The ANOVA table is shown in Table 5.3. Since upper F 0.99 left bracket 5 comma 155 right bracket equals 2.2725F0.99[5, 155] = 2.2725 and 
upper F 0.99 left bracket 31 comma 155 right bracket equals 1.5255F0.99[31, 155] = 1.5255, both the treatment effects and the card effects are 
signiﬁcant. 
The estimator of sigmaσ, ModifyingAbove sigma With caret Subscript p Baseline equals left parenthesis upper M upper S upper E right parenthesis Superscript 1 divided by 2ˆσp = (MSE)1/2, according to the above ANOVA, is ModifyingAbove sigma With caret Subscript p Baseline equals 58.03ˆσp =
58.03. Notice that this estimator is considerably smaller than the pooled estimator 
of sigmaσ of 133.74 (see Example 4.16, Modern Statistics, Kenett et al. 2022b). This is 
due to the variance reduction effect of the blocking. 
Table 5.3 ANOVA for 
hybrid data 
Source 
DF 
SS
MS
F 
Hybrids
5 
1,780,741 
356,148 
105.7 
Cards
31 
2,804,823 
90,478 
26.9 
Error
155 
522,055
3368
– 
Total
191 
5,107,619 
–
–

156
5
Classical Design and Analysis of Experiments
The simultaneous conﬁdence intervals, at level of conﬁdence 0.95, for the 
treatment effects (the average hybrid measurements minus the grand average of 
1965.2) are: 
StartLayout 1st Row 1st Column Hybrid 1 2nd Column colon 3rd Column 178.21 4th Column plus or minus 28.66 semicolon 2nd Row 1st Column Hybrid 2 2nd Column colon 3rd Column negative 62.39 4th Column plus or minus 28.66 semicolon 3rd Row 1st Column Hybrid 3 2nd Column colon 3rd Column negative 114.86 4th Column plus or minus 28.66 period 4th Row 1st Column Hybrid 4 2nd Column colon 3rd Column negative 64.79 4th Column plus or minus 28.66 semicolon 5th Row 1st Column Hybrid 5 2nd Column colon 3rd Column 15.36 4th Column plus or minus 28.66 semicolon 6th Row 1st Column Hybrid 6 2nd Column colon 3rd Column 48.71 4th Column plus or minus 28.66 semicolon EndLayoutHybrid 1:
178.21 ± 28.66;
Hybrid 2:
−62.39 ± 28.66;
Hybrid 3:
−114.86 ± 28.66.
Hybrid 4:
−64.79 ± 28.66;
Hybrid 5:
15.36 ± 28.66;
Hybrid 6:
48.71 ± 28.66;
Accordingly, the effects of Hybrid 2 and Hybrid 4 are not signiﬁcantly different, 
and that of Hybrid 5 is not signiﬁcantly different from zero. In Python, we get the 
conﬁdence intervals from the model: 
model.conf_int().tail(5) 
0
1
 
C(hyb)[T.2] -269.254303 -211.933197 
C(hyb)[T.3] -321.723053 -264.401947 
C(hyb)[T.4] -271.660553 -214.339447 
C(hyb)[T.5] -191.504303 -134.183197 
C(hyb)[T.6] -158.160553 -100.839447 
ci = model.conf_int().tail(5) 
hyb_mean = hadpas.groupby(by='hyb').mean()['res3'] - hadpas['res3'].mean() 
print(hyb_mean.round(2)) 
(ci.iloc[:,1] - ci.iloc[:,0]) / 2 
hyb 
1
178.17 
2
-62.43 
3
-114.90 
4
-64.83 
5
15.32 
6
48.67 
Name: res3, dtype: float64 
C(hyb)[T.2]
28.660553 
C(hyb)[T.3]
28.660553 
C(hyb)[T.4]
28.660553 
C(hyb)[T.5]
28.660553 
C(hyb)[T.6]
28.660553 
dtype: float64 
black medium square■

5.5
Balanced Incomplete Block Designs
157
5.5 
Balanced Incomplete Block Designs 
As mentioned before, it is often the case that the blocks are not sufﬁciently large to 
accommodate all the t treatments. For example, in testing the wearout of fabric one 
uses a special machine (Martindale wear tester) which can accommodate only four 
pieces of clothes simultaneously. Here the block size is ﬁxed at k equals 4k = 4, while the 
number of treatments t, is the number of types of cloths to be compared. Balanced 
Incomplete Block Designs (BIBD) are designs which assign t treatment to b blocks 
of size k left parenthesis k less than t right parenthesis(k < t) in the following manner. 
1. Each treatment is assigned only once to any one block. 
2. Each treatment appears in r blocks. r is the number of replicas. 
3. Every pair of two different treatments appears in lamdaλ blocks. 
4. The order of treatments within each block is randomized. 
5. The order of blocks is randomized. 
According to these requirements there are, altogether, upper N equals t r equals b kN
= tr
= bk trials. 
Moreover, the following equality should hold 
lamda left parenthesis t minus 1 right parenthesis equals r left parenthesis k minus 1 right parenthesis periodλ(t −1) = r(k −1).
(5.5.1) 
The question is how to design a BIBD, for a given t and k. One can obtain a BIBD 
by the complete combinatorial listing of the StartBinomialOrMatrix t Choose k EndBinomialOrMatrix
t
k

selections without replacements of 
k out of t letters. In this case, the number of blocks is 
b equals StartBinomialOrMatrix t Choose k EndBinomialOrMatrix periodb =
t
k

.
(5.5.2) 
The number of replicas is r equals StartBinomialOrMatrix t minus 1 Choose k minus 1 EndBinomialOrMatrixr =
t−1
k−1

, and lamda equals StartBinomialOrMatrix t minus 2 Choose k minus 2 EndBinomialOrMatrixλ =
t−2
k−2

. The total number trials is 
StartLayout 1st Row 1st Column upper N equals t r 2nd Column equals t StartBinomialOrMatrix t minus 1 Choose k minus 1 EndBinomialOrMatrix equals StartFraction t factorial Over left parenthesis k minus 1 right parenthesis factorial left parenthesis t minus k right parenthesis factorial EndFraction equals k StartBinomialOrMatrix t Choose k EndBinomialOrMatrix 2nd Row 1st Column Blank 2nd Column equals k b period EndLayout
N = tr = t
t −1
k −1

=
t!
(k −1)!(t −k)! = k
t
k

= kb.
(5.5.3) 
Such designs of BIBD are called combinatoric designs. They might be, however, 
too big. For example, if t equals 8t = 8 and k equals 4k = 4 we are required to have StartBinomialOrMatrix 8 Choose 4 EndBinomialOrMatrix equals 70
8
4

= 70 blocks. 
Thus, the total number of trials is upper N equals 70 times 4 equals 280N = 70 × 4 = 280 and r equals StartBinomialOrMatrix 7 Choose 3 EndBinomialOrMatrix equals 35r =
7
3

= 35. Here  
lamda equals StartBinomialOrMatrix 6 Choose 2 EndBinomialOrMatrix equals 15λ =
6
2

= 15. 
There are advanced algebraic methods which can yield smaller designs for t equals 8t = 8
and k equals 4k = 4. Box et al. (2005) list a BIBD of t equals 8t = 8, k equals 4k = 4 in b equals 14b = 14 blocks. Here 
upper N equals 14 times 4 equals 56N = 14 × 4 = 56, r equals 7r = 7, and lamda equals 3λ = 3. 
It is not always possible to have a BIBD smaller in size than a complete 
combinatoric design. Such a case is t equals 8t = 8 and k equals 5k = 5. Here the smallest number 
of blocks possible is StartBinomialOrMatrix 8 Choose 5 EndBinomialOrMatrix equals 56
8
5

= 56, and upper N equals 56 times 5 equals 280N = 56 × 5 = 280.

158
5
Classical Design and Analysis of Experiments
Table 5.4 ANOVA for a BIBD 
Source of variation
DF
SS
MS
upper E left brace upper M upper S right braceE{MS}
Blocks
b minus 1b −1
SSBL
MSBL
sigma squared plus StartFraction t Over b minus 1 EndFraction sigma summation Underscript i equals 1 Overscript b Endscripts beta Subscript i Superscript 2σ 2 +
t
b−1
b
i=1 β2
i
Treatments adjusted
t minus 1t −1
SST R
MST R
sigma squared plus StartFraction b Over t minus 1 EndFraction sigma summation Underscript j equals 1 Overscript t Endscripts tau Subscript j Superscript 2σ 2 +
b
t−1
t
j=1 τ 2
j
Error
upper N minus t minus b plus 1N −t −b + 1
SSE
MSE
sigma squaredσ 2
Total
upper N minus 1N −1
SST
–
– 
The reader is referred to Box et al. (2005) for a list of some useful BIBD’s for 
k equals 2 comma midline horizontal ellipsis comma 6k = 2, · · · , 6, t equals k comma midline horizontal ellipsis comma 10t = k, · · · , 10. Let  upper B Subscript iBi denote the set of treatments in the i-th block. 
For example, if block 1 contains the treatments 1, 2, 3, 4 then upper B 1 equals StartSet 1 comma 2 comma 3 comma 4 EndSetB1 = {1, 2, 3, 4}. Let  
upper Y Subscript i jYij be the yield of treatment j element of upper B Subscript ij ∈Bi. The effects model is 
upper Y Subscript i j Baseline equals mu plus beta Subscript i Baseline plus tau Subscript j Baseline plus e Subscript i j Baseline comma i equals 1 comma midline horizontal ellipsis comma b j element of upper B Subscript i BaselineYij = μ + βi + τj + eij,
i = 1, · · · , b j ∈Bi
(5.5.4) 
StartSet e Subscript i j Baseline EndSet{eij} are random experimental errors, with upper E left brace e Subscript i j Baseline right brace equals 0E{eij} = 0 and upper V left brace e Subscript i j Baseline right brace equals sigma squaredV {eij} = σ 2 all left parenthesis i comma j right parenthesis(i, j). 
The block and treatment effects, beta 1 comma midline horizontal ellipsis comma beta Subscript b Baselineβ1, · · · , βb and tau 1 comma midline horizontal ellipsis comma tau Subscript t Baselineτ1, · · · , τt satisfy the constraints 
sigma summation Underscript j equals 1 Overscript t Endscripts tau Subscript j Baseline equals 0t
j=1 τj = 0 and sigma summation Underscript i equals 1 Overscript b Endscripts beta Subscript i Baseline equals 0b
i=1 βi = 0. 
Let upper T Subscript jTj be the set of all indices of blocks containing the j-th treatment. The least 
squares estimates of the treatment effects are obtained in the following manner. 
Let upper W Subscript j Baseline equals sigma summation Underscript i element of upper T Subscript j Baseline Endscripts upper Y Subscript i jWj = 
i∈Tj Yij be the sum of all Y values under the j-th treatment. Let upper W Subscript j Superscript asteriskW ∗
j
be the sum of the values in all the r blocks which contain the j-th treatment, i.e., 
upper W Subscript j Superscript asterisk Baseline equals sigma summation Underscript i element of upper T Subscript j Baseline Endscripts sigma summation Underscript l element of upper B Subscript i Baseline Endscripts upper Y Subscript i lW ∗
j = 
i∈Tj

l∈Bi Yil. Compute 
upper Q Subscript j Baseline equals k upper W Subscript j Baseline minus upper W Subscript j Superscript asterisk Baseline comma j equals 1 comma midline horizontal ellipsis comma t periodQj = kWj −W ∗
j ,
j = 1, · · · , t.
(5.5.5) 
The least squares error (LSE) of tau Subscript jτj is 
ModifyingAbove tau With caret Subscript j Baseline equals StartFraction upper Q Subscript j Baseline Over t lamda EndFraction comma j equals 1 comma midline horizontal ellipsis comma t periodˆτj = Qj
tλ ,
j = 1, · · · , t.
(5.5.6) 
Notice that sigma summation Underscript j equals 1 Overscript t Endscripts upper Q Subscript j Baseline equals 0t
j=1 Qj = 0. Thus, sigma summation Underscript j equals 1 Overscript t Endscripts ModifyingAbove tau With caret Subscript j Baseline equals 0t
j=1 ˆτj = 0. Let  upper Y overbar overbar equals StartFraction 1 Over upper N EndFraction sigma summation Underscript i equals 1 Overscript b Endscripts sigma summation Underscript l element of upper B Subscript i Baseline Endscripts upper Y Subscript i l ¯¯Y =
1
N
b
i=1

l∈Bi Yil. The  
adjusted treatment average is deﬁned as upper Y overbar Subscript j Superscript asterisk Baseline equals upper Y overbar overbar plus ModifyingAbove tau With caret Subscript j ¯Y ∗
j = ¯¯Y + ˆτj, j equals 1 comma midline horizontal ellipsis comma tj = 1, · · · , t. The ANOVA 
for a BIBD is given in Table 5.4. Here, 
StartLayout 1st Row 1st Column upper S upper S upper T 2nd Column equals sigma summation Underscript i equals 1 Overscript b Endscripts sigma summation Underscript l element of upper B Subscript i Baseline Endscripts upper Y Subscript i l Superscript 2 Baseline minus left parenthesis sigma summation Underscript i equals 1 Overscript b Endscripts sigma summation Underscript l element of upper B Subscript i Baseline Endscripts upper Y Subscript i l Baseline right parenthesis squared divided by upper N semicolon EndLayout StartLayout 2nd Row 1st Column upper S upper S upper B upper L 2nd Column equals StartFraction 1 Over k EndFraction sigma summation Underscript i equals 1 Overscript b Endscripts left parenthesis sigma summation Underscript l element of upper B Subscript i Baseline Endscripts upper Y Subscript i l Baseline right parenthesis squared minus upper N upper Y overbar overbar squared semicolon EndLayout StartLayout 3rd Row 1st Column upper S upper S upper T upper R 2nd Column equals StartFraction 1 Over lamda k t EndFraction sigma summation Underscript j equals 1 Overscript t Endscripts upper Q Subscript j Superscript 2 EndLayoutSST =
b

i=1

l∈Bi
Y 2
il −
⎛
⎝
b

i=1

l∈Bi
Yil
⎞
⎠
2
/N;
(5.5.7) 
SSBL = 1 
k 
b

i=1 
⎛ 
⎝
l∈Bi 
Yil 
⎞ 
⎠ 
2 
− N ¯¯Y 2;
(5.5.8) 
SST R = 1 
λkt 
t
j=1 
Q2 
j
(5.5.9)

5.5
Balanced Incomplete Block Designs
159
Table 5.5 Block sets
i 
upper B Subscript iBi
i
upper B Subscript iBi
i
upper B Subscript iBi
1 
1, 2, 3, 4 
6 
1, 2, 5, 6 
11 
2, 3, 4, 5 
2 
1, 2, 3, 5 
7 
1, 3, 4, 5 
12 
2, 3, 4, 6 
3 
1, 2, 3, 6 
8 
1, 3, 4, 6 
13 
2, 3, 5, 6 
4 
1, 2, 4, 5 
9 
1, 3, 5, 6 
14 
2, 4, 5, 6 
5 
1, 2, 4, 6 
10 
1, 4, 5, 6 
15 
3, 4, 5, 6 
Table 5.6 Values of upper Y Subscript i lYil, l element of upper B Subscript il ∈Bi
i
upper Y Subscript i lYil
i
upper Y Subscript i lYil
i
upper Y Subscript i lYil
1
24.7, 20.8, 29.4, 24.9
6
21.4, 20.1, 30.1, 34.1
11
21.4, 29.6, 24.8, 31.2 
2
24.1, 20.4, 29.8, 30.3
7
23.2, 28.7, 24.9, 31.0
12
21.3, 28.9, 25.3, 35.1 
3
23.4, 20.6, 29.2, 34.4
8
23.1, 29.3, 27.1, 34.4
13
21.6, 29.5, 30.4, 33.6 
4
23.2, 20.7, 26.0, 30.8
9
22.0, 29.8, 31.9, 36.1
14
20.1, 25.1, 32.9, 33.9 
5
21.5, 22.1, 25.3, 35.4
10
22.8, 22.6, 33.2, 34.8
15
30.1, 24.0, 30.8, 36.5 
Table 5.7 The set upper T Subscript jTj and the statistics upper W Subscript jWj, upper W Subscript j Superscript asteriskW ∗
j , upper Q Subscript jQj
j
upper T Subscript jTj
upper W Subscript jWj
upper W Subscript j Superscript asteriskW ∗
j
upper Q Subscript jQj
1
1, 2, 3, 4, 5, 6, 7, 8, 9, 10
229.536
1077.7
−159.56 
2
1, 2, 3, 4, 5, 6, 11, 12, 13, 14
209.023
1067.4
−231.31 
3
1, 2, 3, 7, 8, 9, 11, 12, 13, 15
294.125
1107.6
68.90 
4
1, 4, 5, 7, 8, 10, 11, 12, 14, 15
249.999
1090.9
−90.90 
5
2, 4, 6, 7, 9, 10, 11, 13, 14, 15
312.492
1107.5
142.47 
6
3, 5, 6, 8, 9, 10, 12, 13, 14, 15
348.176
1123.8
268.90 
and 
StartLayout 1st Row upper S upper S upper E equals upper S upper S upper T minus upper S upper S upper B upper L minus upper S upper S upper T upper R period EndLayoutSSE = SST −SSBL −SST R.
(5.5.10) 
The signiﬁcance of the treatments effects is tested by the statistic 
upper F equals StartFraction upper M upper S upper T upper R Over upper M upper S upper E EndFraction periodF = MST R
MSE .
(5.5.11) 
Example 5.5 Six different adhesives left parenthesis t equals 6 right parenthesis(t = 6) are tested for the bond strength in a 
lamination process, under curing pressure of 200 [psi]. Lamination can be done in 
blocks of size k equals 4k = 4. 
A combinatoric design will have StartBinomialOrMatrix 6 Choose 4 EndBinomialOrMatrix equals 15
6
4

= 15 blocks, with r equals StartBinomialOrMatrix 5 Choose 3 EndBinomialOrMatrix equals 10r =
5
3

= 10, lamda equals StartBinomialOrMatrix 4 Choose 2 EndBinomialOrMatrix equals 6λ =
4
2

=
6 and upper N equals 60N = 60. The treatment indices of the 15 blocks are listed in Table 5.5. The  
observed bond strength in these trials are listed in Table 5.6. The grand mean of the 
bond strength is upper Y overbar overbar equals 27.389 ¯¯Y = 27.389. The  sets  upper T Subscript jTj and the sums upper W Subscript jWj, upper W Subscript j Superscript asteriskW ∗
j are summarized 
in Table 5.7. The resulting ANOVA table is Table 5.8. The adjusted mean effects of 
the adhesives are in Table 5.9

160
5
Classical Design and Analysis of Experiments
Table 5.8 ANOVA for 
BIBD 
Source
DF 
SS
MS
F 
Blocks
14
161.78 
11.556
23.99 
Treat. adj. 
5 
1282.76 
256.552
532.54 
Error
40
19.27
0.48175 
– 
Total
59 
1463.81 
Table 5.9 Mean effects and 
their S.E. 
Treatment 
upper Y overbar Subscript i Superscript asterisk ¯Y ∗
i
S.E.StartSet upper Y overbar Subscript i Superscript asterisk Baseline EndSet{ ¯Y ∗
i }
1
22.96 
1.7445 
2
20.96 
1.7445 
3
29.33 
1.7445 
4
24.86 
1.7445 
5
31.35 
1.7445 
6
34.86 
1.7445 
The variance of each adjusted mean effect is 
upper V left brace upper Y overbar Subscript j Superscript asterisk Baseline right brace equals StartFraction k sigma squared Over t lamda EndFraction comma j equals 1 comma midline horizontal ellipsis comma t periodV { ¯Y ∗
j } = kσ 2
tλ ,
j = 1, · · · , t.
(5.5.12) 
Thus, the S.E. of upper Y overbar Subscript i Superscript asterisk¯Y ∗
i is 
upper S period upper E period StartSet upper Y overbar Subscript j Superscript asterisk Baseline EndSet equals left parenthesis StartFraction k upper M upper S upper E Over t lamda EndFraction right parenthesis Superscript 1 divided by 2 Baseline comma j equals 1 comma midline horizontal ellipsis comma t periodS.E.{ ¯Y ∗
j } =
k MSE
tλ
1/2
,
j = 1, · · · , t.
(5.5.13) 
It seems that there are two homogeneous groups of treatments StartSet 1 comma 2 comma 4 EndSet{1, 2, 4} and 
StartSet 3 comma 5 comma 6 EndSet{3, 5, 6}.
black medium square■
5.6 
Latin Square Design 
Latin Square designs are such that we can block for two error inducing factors in a 
balanced fashion, and yet save considerable amount of trials. 
Suppose that we have t treatments to test, and we wish to block for two factors. 
We assign the blocking factors t levels (the number of treatments) in order to obtain 
squared designs. For example, suppose that we wish to study the effects of 4 new 
designs of keyboards for desktop computers. The design of the keyboard might have 
effect on the speed of typing or on the number of typing errors. Noisy factors are 
typist or type of job. Thus we can block by typist and by job. We should pick at 
random 4 typists and 4 different jobs. We construct a square with 4 rows and 4 
columns for the blocking factors (see Table 5.10). 
Let A, B, C, D denote the 4 keyboard designs. We assign the letters to the cells 
of the above square so that

5.6
Latin Square Design
161
Table 5.10 A 4 times 44 × 4 Latin 
square 
Job 1 
Job 2 
Job 3 
Job 4 
Typist 1 
A
B
C
D 
Typist 2 
B
A
D
C 
Typist 3 
C
D
A
B 
Typist 4 
D
C
B
A 
1. Each letter appears exactly once in a row. 
2. Each letter appears exactly once in a column. 
Finally, the order of performing these trials is random. Notice that a design which 
contains all the combinations of typist, job and keyboard spans over 4 times 4 times 4 equals 644 × 4 × 4 =
64 combinations. Thus, the Latin square design saves many trials. However, it is 
based on the assumption of no interactions between the treatments and the blocking 
factors. That is, in order to obtain valid analysis, the model relating the response to 
the factor effects should be additive, i.e., 
upper Y Subscript i j k Baseline equals mu plus beta Subscript i Baseline plus gamma Subscript j Baseline plus tau Subscript k Baseline plus e Subscript i j k Baseline comma i comma j comma k equals 1 comma midline horizontal ellipsis comma t commaYijk = μ + βi + γj + τk + eijk,
i, j, k = 1, · · · , t,
(5.6.1) 
where muμ is the grand mean, beta Subscript iβi are the row effects, gamma Subscript jγj are the column effects and tau Subscript kτk
are the treatment effects. The experimental error variables are StartSet e Subscript i j k Baseline EndSet{eijk}, with upper E left brace e Subscript i j k Baseline right brace equals 0E{eijk} =
0 and upper V left brace e Subscript i j k Baseline right brace equals sigma squaredV {eijk} = σ 2 for all left parenthesis i comma j right parenthesis(i, j). Furthermore, 
sigma summation Underscript i equals 1 Overscript t Endscripts beta Subscript i Baseline equals sigma summation Underscript j equals 1 Overscript t Endscripts gamma Subscript j Baseline equals sigma summation Underscript k equals 1 Overscript t Endscripts tau Subscript k Baseline equals 0 period
t
i=1
βi =
t
j=1
γj =
t
k=1
τk = 0.
(5.6.2) 
The Latin square presented in Table 5.10 is not unique. There are other 4 times 44 × 4 Latin 
squares. For example, 
StartLayout 1st Row 1st Column upper A 2nd Column upper B 3rd Column upper C 4th Column upper D 2nd Row 1st Column q u a d upper D 2nd Column upper C 3rd Column upper B 4th Column upper A 3rd Row 1st Column q u a d upper B 2nd Column upper A 3rd Column upper D 4th Column upper C 4th Row 1st Column q u a d upper C 2nd Column upper D 3rd Column upper A 4th Column upper B EndLayout StartLayout 1st Row 1st Column upper A 2nd Column upper B 3rd Column upper C 4th Column upper D 2nd Row 1st Column q u a d upper C 2nd Column upper D 3rd Column upper A 4th Column upper B 3rd Row 1st Column q u a d upper D 2nd Column upper C 3rd Column upper B 4th Column upper A 4th Row 1st Column q u a d upper B 2nd Column upper A 3rd Column upper D 4th Column upper C EndLayout period
A B C D
D C B A
B A D C
C D A B
A B C D
C D A B
D C B A
B A D C
.
A few Latin square designs, for t equals 3 comma midline horizontal ellipsis comma 9t = 3, · · · , 9 are given in Box et al. (2005). 
If we perform only 1 replication of the Latin square, the ANOVA for testing the 
main effects is shown in Table 5.11 
Formulae for the various SS terms will be given below. At this time we wish to 
emphasize that if t is small, say t equals 3t = 3, then the number of DF for SSE is only 2. 
This is too small. The number of DF for the error SS can be increased by performing 
replicas. One possibility is to perform the same Latin square r times independently, 
and as similarly as possible. However, signiﬁcant differences between replicas may 
emerge. The ANOVA, for r identical replicas is as in Table 5.12.

162
5
Classical Design and Analysis of Experiments
Table 5.11 ANOVA for a Latin square, one replication 
Source
DF
SS
MS
F 
Treatments
t minus 1t −1
SST R
MST R
upper M upper S upper T upper R divided by upper M upper S upper EMST R/MSE
Rows
t minus 1t −1
SSR
MSR
upper M upper S upper R divided by upper M upper S upper EMSR/MSE
Columns
t minus 1t −1
SSC
MSC
upper M upper S upper C divided by upper M upper S upper EMSC/MSE
Error
left parenthesis t minus 1 right parenthesis left parenthesis t minus 2 right parenthesis(t −1)(t −2)
SSE
MSE
– 
Total
t squared minus 1t2 −1
SST
–
– 
Table 5.12 ANOVA for replicated Latin square 
Source
D.F.
SS
MS
F 
Treatments
t minus 1t −1
SST R
MST R
upper M upper S upper T upper R divided by upper M upper S upper EMST R/MSE
Rows
t minus 1t −1
SSR
MSR
– 
Columns
t minus 1t −1
SSC
MSC
– 
Replicas
r minus 1r −1
SSREP
MSREP
– 
Error
left parenthesis t minus 1 right parenthesis left bracket r left parenthesis t plus 1 right parenthesis minus 3 right bracket(t −1)[r(t + 1) −3]
SSE
MSE
– 
Total
r left parenthesis t squared minus 1 right parenthesisr(t2 −1)
SST
–
– 
Notice that now we have r t squaredrt2 observations. Let upper T Subscript ellipsis periodT.... and upper Q Subscript ellipsis periodQ.... be the sum and sum 
of squares of all observations. Then, 
upper S upper S upper T equals upper Q Subscript ellipsis period Baseline minus upper T Subscript ellipsis period Superscript 2 Baseline divided by r t squared periodSST = Q.... −T 2
..../rt2.
(5.6.3) 
Let upper T Subscript i ellipsisTi... denote the sum of rt observations in the i-th row of all r replications. Then 
upper S upper S upper R equals StartFraction 1 Over t r EndFraction sigma summation Underscript i equals 1 Overscript t Endscripts upper T Subscript i ellipsis Superscript 2 Baseline minus StartFraction upper T Subscript ellipsis period Superscript 2 Baseline Over r t squared EndFraction periodSSR = 1
tr
t
i=1
T 2
i... −T 2
....
rt2 .
(5.6.4) 
Similarly, let upper T Subscript period j period periodT.j.. and upper T Subscript period period k periodT..k. be the sums of all rt observations in column j of all 
replicas, and treatment k of all r replicas, then 
upper S upper S upper C equals StartFraction 1 Over r t EndFraction sigma summation Underscript j equals 1 Overscript t Endscripts upper T Subscript period j period period Superscript 2 Baseline minus StartFraction upper T Subscript ellipsis period Superscript 2 Baseline Over r t squared EndFractionSSC = 1
rt
t
j=1
T 2
.j.. −T 2
....
rt2
(5.6.5) 
and 
upper S upper S upper T upper R equals StartFraction 1 Over r t EndFraction sigma summation Underscript k equals 1 Overscript t Endscripts upper T Subscript period period k period Superscript 2 Baseline minus StartFraction upper T Subscript ellipsis period Superscript 2 Baseline Over r t squared EndFraction periodSST R = 1
rt
t
k=1
T 2
..k. −T 2
....
rt2 .
(5.6.6) 
Finally, let upper T Subscript ellipsis lT...l left parenthesis l equals 1 comma midline horizontal ellipsis comma r right parenthesis(l = 1, · · · , r) denote the sum of all t squaredt2 observations in the l-th 
replication. Then,

5.6
Latin Square Design
163
Table 5.13 Latin square 
design, t equals 5t = 5
Job 
Typist 
1 
2 
3 
4 
5 
1
A 
B 
C 
D 
E 
2
B 
C 
D 
E 
A 
3
C 
D 
E 
A 
B 
4
D 
E 
A 
B 
C 
5
E 
A 
B 
C 
D 
upper S upper S upper R upper E upper P equals StartFraction 1 Over t squared EndFraction sigma summation Underscript l equals 1 Overscript r Endscripts t Subscript ellipsis l Superscript 2 Baseline minus StartFraction upper T Subscript ellipsis period Superscript 2 Baseline Over r t squared EndFraction periodSSREP = 1
t2
r

l=1
t2
...l −T 2
....
rt2 .
(5.6.7) 
The pooled sum of squares for error is obtained by 
upper S upper S upper E equals upper S upper S upper D minus upper S upper S upper R minus upper S upper S upper C minus upper S upper S upper T upper R minus upper S upper S upper R upper E upper P periodSSE = SSD −SSR −SSC −SST R −SSREP.
(5.6.8) 
Notice that if t equals 3t = 3 and r equals 3r = 3, the number of DF for SSE increases from 2 (when 
r equals 1r = 1) to 18 (when r equals 3r = 3). 
The most important hypothesis is that connected with the main effects of the 
treatments. This we test with the statistic 
upper F equals upper M upper S upper T upper R divided by upper M upper S upper E periodF = MST R/MSE.
(5.6.9) 
Example 5.6 Five models of keyboards (treatments) were tested in a Latin square 
design in which the blocking factors are typist and job. Five typists were randomly 
selected from a pool of typists of similar capabilities. Five typing jobs were selected. 
Each typing job had 4000 characters. The yield, upper Y Subscript i j kYijk, is the number of typing errors 
found at the i-th typist, j-th job under the k-th keyboard. The Latin square design 
used is presented in Table 5.13. 
The ﬁve keyboards are denoted by the letters A, B, C, D, E. The experiment 
spanned over 5 days. In each day a typist was assigned a job at random (from those 
not yet tried). The keyboard used is the one associated with the job. Only one job 
was tried in a given day. The observed number of typing errors (per 4000 characters) 
are summarized in Table 5.14. 
Figure 5.2 present box plots of the error rates for the three different factors. The 
only inﬂuencing factor is the typist effect. 
The total sum of squares is 
upper Q equals 30636 periodQ = 30636.
Thus, 
upper S upper S upper T equals 30636 minus StartFraction 798 squared Over 25 EndFraction equals 5163.84 periodSST = 30636 −7982
25
= 5163.84.

164
5
Classical Design and Analysis of Experiments
Table 5.14 Number of 
typing errors 
Job 
Typist
1
2
3
4
5
Row sums 
1
A
B
C
D
E
100 
20 
18 
25 
17 
20 
2
B
C
D
E
A
277 
65 
40 
55 
58 
59 
3
C
D
E
A
B
140 
30 
27 
35 
21 
27 
4
D
E
A
B
C
94 
21 
15 
24 
16 
18 
5
E
A
B
C
D
187 
42 
38 
40 
35 
32 
Column sum 
178 
138 
179 
147 
156 
798 
Sums
A
B
C
D
E 
Keyboard 
162 
166 
148 
152 
170 
Similarly, 
StartLayout 1st Row 1st Column upper S upper S upper R 2nd Column equals one fifth left parenthesis 100 squared plus 277 squared plus midline horizontal ellipsis plus 187 squared right parenthesis minus StartFraction 798 squared Over 25 EndFraction 2nd Row 1st Column Blank 2nd Column equals 4554.64 3rd Row 1st Column upper S upper S upper C 2nd Column equals one fifth left parenthesis 178 squared plus 138 squared plus midline horizontal ellipsis plus 156 squared right parenthesis minus StartFraction 798 squared Over 25 EndFraction 4th Row 1st Column Blank 2nd Column equals 270.641 EndLayoutSSR = 1
5(1002 + 2772 + · · · + 1872) −7982
25
= 4554.64
SSC = 1
5(1782 + 1382 + · · · + 1562) −7982
25
= 270.641
and 
StartLayout 1st Row 1st Column upper S upper S upper T upper R 2nd Column equals one fifth left parenthesis 162 squared plus 166 squared plus midline horizontal ellipsis plus 170 squared right parenthesis minus StartFraction 798 squared Over 25 EndFraction 2nd Row 1st Column Blank 2nd Column equals 69.4395 period EndLayoutSST R = 1
5(1622 + 1662 + · · · + 1702) −7982
25
= 69.4395.
The analysis of variance, following Table 5.12, is summarized in Table 5.15. In  
Python, we can perform the analysis of variance as follows: 
keyboards = mistat.load_data('KEYBOARDS.csv') 
model = smf.ols('errors ~ C(keyboard) + C(job) + C(typist)', data=keyboards).fit() 
print(anova.anova_lm(model)) 
df
sum_sq
mean_sq
F
PR(>F) 
C(keyboard)
4.0
69.44
17.360000
0.774078 
5.627148e-01 
C(job)
4.0
270.64
67.660000
3.016944 
6.158117e-02 
C(typist)
4.0 
4554.64 
1138.660000 
50.772592 
2.009919e-07 
Residual
12.0
269.12
22.426667
NaN
NaN

5.6
Latin Square Design
165
Fig. 5.2 Effect of factors on error rate 
The null hypothesis that the main effects of the keyboards are zero cannot be 
rejected. The largest source of variability in this experiment were the typists. The 
different jobs contributed also to the variability. The P-value for the F test of Jobs 
is 0.062.
black medium square■

166
5
Classical Design and Analysis of Experiments
Table 5.15 ANOVA for 
keyboard Latin square 
experiment 
Source
DF 
SS
MS
F 
Typist
4 
4554.640 
1138.66 
50.772 
Job
4
270.641 
67.66
3.017 
Keyboard 
4
69.439 
17.3598 
0.774 
Error
12
269.120 
22.4267 
– 
Total
24 
5163.840 
–
– 
5.7 
Full Factorial Experiments 
5.7.1 
The Structure of Factorial Experiments 
Full factorial experiments are those in which complete trials are performed of all 
the combinations of the various factors at all their levels. For example, if there are 
ﬁve factors, each one tested at three levels, there are altogether 3 Superscript 5 Baseline equals 24335 = 243 treatment 
combinations. All these 243 treatment combinations are tested. The full factorial 
experiment may also be replicated several times. The order of performing the trials 
is random. 
In full factorial experiments, the number of levels of different factors do not have 
to be the same. Some factors might be tested at two levels and others at three or 
four levels. Full factorial, or certain fractional factorials which will be discussed 
later, are necessary, if the statistical model is not additive. In order to estimate or 
test the effects of interactions, one needs to perform factorial experiments, full or 
fractional. In a full factorial experiment, all the main effects and interactions can be 
tested or estimated. Recall that if there are p factors upper A comma upper B comma upper C comma midline horizontal ellipsisA, B, C, · · · there are p types of 
main effects, StartBinomialOrMatrix p Choose 2 EndBinomialOrMatrix
p
2

types of pairwise interactions upper A upper B comma upper A upper C comma upper B upper C comma midline horizontal ellipsis comma StartBinomialOrMatrix p Choose 3 EndBinomialOrMatrixAB, AC, BC, · · · ,
p
3

interactions 
between three factors, upper A upper B upper C comma upper A upper B upper D comma midline horizontal ellipsisABC, ABD, · · · and so on. On the whole there are, together 
with the grand mean muμ, 2 Superscript p2p types of parameters. 
In the following section we discuss the structure of the ANOVA for testing the 
signiﬁcance of main effects and interaction. This is followed by a section on the 
estimation problem. In Sects. 5.7.4 and 5.7.5 we discuss the structure of full factorial 
experiments with 2 and 3 levels per factor, respectively. 
5.7.2 
The ANOVA for Full Factorial Designs 
The analysis of variance for full factorial designs is done for testing the hypotheses 
that main effects or interaction parameters are equal to zero. We present the ANOVA 
for a two factor situation, factor A at a levels and factor B at b levels. The method 
can be generalized to any number of factors. 
The structure of the experiment is such that all a times ba × b treatment combinations are 
tested. Each treatment combination is repeated n times. The model is

5.7
Full Factorial Experiments
167
upper Y Subscript i j k Baseline equals mu plus tau Subscript i Superscript upper A Baseline plus tau Subscript j Superscript upper B Baseline plus tau Subscript i j Superscript upper A upper B Baseline plus e Subscript i j k Baseline commaYijk = μ + τ A
i + τ B
j + τ AB
ij
+ eijk,
(5.7.1) 
i equals 1 comma midline horizontal ellipsis comma ai = 1, · · · , a; j equals 1 comma midline horizontal ellipsis comma bj = 1, · · · , b; k equals 1 comma midline horizontal ellipsis comma nk = 1, · · · , n. e Subscript i j keijk are independent random variables 
upper E left brace e Subscript i j h Baseline right brace equals 0E{eijh} = 0 and upper V left brace e Subscript i j k Baseline right brace equals sigma squaredV {eijk} = σ 2 for all i comma j comma ki, j, k. Let  
StartLayout 1st Row 1st Column upper Y overbar Subscript i j 2nd Column equals StartFraction 1 Over n EndFraction sigma summation Underscript k equals 1 Overscript n Endscripts upper Y Subscript i j k Baseline EndLayout StartLayout 2nd Row 1st Column upper Y overbar Subscript i period 2nd Column equals StartFraction 1 Over b EndFraction sigma summation Underscript j equals 1 Overscript b Endscripts upper Y overbar Subscript i j Baseline comma i equals 1 comma midline horizontal ellipsis comma a EndLayout StartLayout 3rd Row 1st Column upper Y overbar Subscript period j 2nd Column equals StartFraction 1 Over a EndFraction sigma summation Underscript i equals 1 Overscript a Endscripts upper Y Subscript i j Baseline comma j equals 1 comma midline horizontal ellipsis comma b EndLayout ¯Yij = 1
n
n

k=1
Yijk
(5.7.2) 
¯Yi. = 1 
b 
b

j=1 
¯Yij,
i
 
= 1, · · ·  , a
(5.7.3) 
¯Y.j = 1 
a 
a

i=1 
Yij,
j
 
= 1, · · ·  , b
(5.7.4) 
and 
StartLayout 1st Row 1st Column upper Y overbar overbar 2nd Column equals StartFraction 1 Over a b EndFraction sigma summation Underscript i equals 1 Overscript a Endscripts sigma summation Underscript j equals 1 Overscript b Endscripts upper Y overbar Subscript i j Baseline period EndLayout ¯¯Y = 1
ab
a

i=1
b

j=1
¯Yij.
(5.7.5) 
The ANOVA partitions ﬁrst the total sum of squares of deviations from upper Y overbar overbar¯¯Y, i.e., 
upper S upper S upper T equals sigma summation Underscript i equals 1 Overscript a Endscripts sigma summation Underscript j equals 1 Overscript b Endscripts sigma summation Underscript k equals 1 Overscript n Endscripts left parenthesis upper Y Subscript i j k Baseline minus upper Y overbar overbar right parenthesis squaredSST =
a

i=1
b

j=1
n

k=1
(Yijk −¯¯Y)2
(5.7.6) 
to two components 
upper S upper S upper W equals sigma summation Underscript i equals 1 Overscript a Endscripts sigma summation Underscript j equals 1 Overscript b Endscripts sigma summation Underscript k equals 1 Overscript n Endscripts left parenthesis upper Y Subscript i j k Baseline minus upper Y overbar Subscript i j Baseline right parenthesis squaredSSW =
a

i=1
b

j=1
n

k=1
(Yijk −¯Yij)2
(5.7.7) 
and 
upper S upper S upper B equals n sigma summation Underscript i equals 1 Overscript a Endscripts sigma summation Underscript j equals 1 Overscript b Endscripts left parenthesis upper Y overbar Subscript i j Baseline minus upper Y overbar overbar right parenthesis squared periodSSB = n
a

i=1
b

j=1
( ¯Yij −¯¯Y)2.
(5.7.8) 
It is straightforward to show that 
upper S upper S upper T equals upper S upper S upper W plus upper S upper S upper B periodSST = SSW + SSB.
(5.7.9) 
In the second stage, the sum of squares of deviations SSB is partitioned to three 
components SSI, SSMA, SSMB, where

168
5
Classical Design and Analysis of Experiments
Table 5.16 Table of ANOVA for a 2-factor factorial experiment 
Source of variation
DF
SS
MS
F 
A
a minus 1a −1
SSMA
MSA
upper F Subscript upper AFA
B
b minus 1b −1
SSMB
MSB
upper F Subscript upper BFB
AB
left parenthesis a minus 1 right parenthesis left parenthesis b minus 1 right parenthesis(a −1)(b −1)
SSI
MSAB
upper F Subscript upper A upper BFAB
Between
a b minus 1ab −1
SSB
–
– 
Within
a b left parenthesis n minus 1 right parenthesisab(n −1)
SSW
MSW
– 
Total
upper N minus 1N −1
SST
–
– 
StartLayout 1st Row 1st Column upper S upper S upper I 2nd Column equals n sigma summation Underscript i equals 1 Overscript a Endscripts sigma summation Underscript j equals 1 Overscript b Endscripts left parenthesis upper Y overbar Subscript i j Baseline minus upper Y overbar Subscript i period Baseline minus upper Y overbar Subscript period j Baseline plus upper Y overbar overbar right parenthesis squared comma EndLayout StartLayout 2nd Row 1st Column upper S upper S upper M upper A 2nd Column equals n b sigma summation Underscript i equals 1 Overscript a Endscripts left parenthesis upper Y overbar Subscript i period Baseline minus upper Y overbar overbar right parenthesis squared EndLayoutSSI = n
a

i=1
b

j=1

¯Yij −¯Yi. −¯Y.j + ¯¯Y
2
,
(5.7.10) 
SSMA = nb 
a

i=1 
( ¯Yi. − ¯¯Y)2
(5.7.11) 
and 
StartLayout 1st Row 1st Column upper S upper S upper M upper B 2nd Column equals n a sigma summation Underscript j equals 1 Overscript b Endscripts left parenthesis upper Y overbar Subscript period j Baseline minus upper Y overbar overbar right parenthesis squared comma EndLayoutSSMB = na
b

j=1
( ¯Y.j −¯¯Y)2,
(5.7.12) 
i.e., 
StartLayout 1st Row 1st Column upper S upper S upper B 2nd Column equals upper S upper S upper I plus upper S upper S upper M upper A plus upper S upper S upper M upper B period EndLayoutSSB = SSI + SSMA + SSMB.
(5.7.13) 
All these terms are collected in a table of ANOVA (see Table 5.16). 
Thus, 
StartLayout 1st Row 1st Column upper M upper S upper A 2nd Column equals StartFraction upper S upper S upper M upper A Over a minus 1 EndFraction comma EndLayout StartLayout 2nd Row 1st Column upper M upper S upper B 2nd Column equals StartFraction upper S upper S upper M upper B Over b minus 1 EndFraction comma EndLayoutMSA = SSMA
a −1 ,
(5.7.14) 
MSB = SSMB 
b − 1 ,
(5.7.15) 
and 
StartLayout 1st Row 1st Column upper M upper S upper A upper B 2nd Column equals StartFraction upper S upper S upper I Over left parenthesis a minus 1 right parenthesis left parenthesis b minus 1 right parenthesis EndFraction comma EndLayout StartLayout 2nd Row 1st Column upper M upper S upper W 2nd Column equals StartFraction upper S upper S upper W Over a b left parenthesis n minus 1 right parenthesis EndFraction period EndLayoutMSAB =
SSI
(a −1)(b −1),
(5.7.16) 
MSW = 
SSW 
ab(n − 1) .
(5.7.17) 
Finally, we compute the F-statistics

5.7
Full Factorial Experiments
169
StartLayout 1st Row 1st Column upper F Subscript upper A 2nd Column equals StartFraction upper M upper S upper A Over upper M upper S upper W EndFraction comma EndLayout StartLayout 2nd Row 1st Column upper F Subscript upper B 2nd Column equals StartFraction upper M upper S upper B Over upper M upper S upper W EndFraction EndLayoutFA = MSA
MSW ,
(5.7.18) 
FB = MSB 
MSW 
(5.7.19) 
and 
upper F Subscript upper A upper B Baseline equals StartFraction upper M upper S upper A upper B Over upper M upper S upper W EndFraction periodFAB = MSAB
MSW .
(5.7.20) 
upper F Subscript upper AFA, upper F Subscript upper BFB, and upper F Subscript upper A upper BFAB are test statistics to test, respectively, the signiﬁcance of the main 
effects of A, the main effects of B and the interactions AB. 
If upper F Subscript upper A Baseline less than upper F Subscript 1 minus alpha Baseline left bracket a minus 1 comma a b left parenthesis n minus 1 right parenthesis right bracketFA < F1−α[a −1, ab(n −1)] the null hypothesis 
upper H 0 Superscript upper A Baseline colon tau 1 Superscript upper A Baseline equals midline horizontal ellipsis equals tau Subscript a Superscript upper A Baseline equals 0H A
0 : τ A
1 = · · · = τ A
a = 0
cannot be rejected. 
If upper F Subscript upper B Baseline less than upper F Subscript 1 minus alpha Baseline left bracket b minus 1 comma a b left parenthesis n minus 1 right parenthesis right bracketFB < F1−α[b −1, ab(n −1)] the null hypothesis 
upper H 0 Superscript upper B Baseline colon tau 1 Superscript upper B Baseline equals midline horizontal ellipsis equals tau Subscript b Superscript upper B Baseline equals 0H B
0 : τ B
1 = · · · = τ B
b = 0
cannot be rejected. 
Also, if 
upper F Subscript upper A upper B Baseline less than upper F Subscript 1 minus alpha Baseline left bracket left parenthesis a minus 1 right parenthesis left parenthesis b minus 1 right parenthesis comma a b left parenthesis n minus 1 right parenthesis right bracket commaFAB < F1−α[(a −1)(b −1), ab(n −1)],
we cannot reject the null hypothesis 
upper H 0 Superscript upper A upper B Baseline colon tau 11 Superscript upper A upper B Baseline equals midline horizontal ellipsis equals tau Subscript a b Superscript upper A upper B Baseline equals 0 periodH AB
0
: τ AB
11
= · · · = τ AB
ab = 0.
The ANOVA for two factors can be performed using Python. We illustrate this 
estimation and testing in the following example. 
Example 5.7 In Chap. 2 we introduced the piston example. Seven prediction factors 
for the piston cycle time were listed. These are 
A: Piston weight m, 30–60 [Kg] 
B: Piston surface area s, 0.005–0.020 [msquared2] 
C: Spring coefﬁcient k, 1000–5000 [N/m] 
D: Ambient temperature t, 290–296 [Superscript ring◦K] 
E: Atmospheric pressure p0, 90,000–110,000 [N/msquared2] 
F: Initial gas volume v0, 0.002–0.010 [mcubed3] 
G: Filling gas temperature t0, 340–360[Superscript ring◦K] 
We are interested to test the effects of the piston surface area s and the spring 
coefﬁcient k on the cycle times (seconds). For this purpose we designed a factorial 
experiment at three levels of s, and three levels of k. The levels are

170
5
Classical Design and Analysis of Experiments
s 1 equals 0.005 left bracket m squared right bracket comma s 2 equals 0.0125 left bracket m squared right bracket and s 3 equals 0.02 left bracket m squared right bracket periods1 = 0.005 [m2],
s2 = 0.0125 [m2]
and
s3 = 0.02 [m2].
The levels of factor k (spring coefﬁcient) are 
k 1 equals 1500 left bracket upper N slash m right bracket comma k 2 equals 3000 left bracket upper N slash m right bracket and k 3 equals 4500 left bracket upper N slash m right bracket periodk1 = 1500 [N/m],
k2 = 3000 [N/m]
and
k3 = 4500 [N/m].
Five replicas were performed at each treatment combination left parenthesis n equals 5 right parenthesis(n = 5). 
The data can be obtained by using the piston simulator from the mistat 
package. The ﬁve factors which were not under study were kept at the levels m equals 30m = 30
[Kg], t equals 293t = 293 [Superscript ring◦K], p Baseline 0 equals 0.005p0 = 0.005 [mcubed3], v Baseline 0 equals 95,000v0 = 95,000 [N/msquared2], and t Baseline 0 equals 350t0 = 350 [Superscript ring◦K]. 
from mistat.design import doe 
np.random.seed(2) 
# Build design from factors 
FacDesign = doe.full_fact({ 
'k': [1500, 3000, 4500], 
's': [0.005, 0.0125, 0.02], 
}) 
# Randomize design 
FacDesign = FacDesign.sample(frac=1).reset_index(drop=True) 
# Setup and run simulator with five replicates 
# for each combination of factors 
simulator = mistat.PistonSimulator(n_replicate=5, **FacDesign, 
m=30, v0=0.005, p0=95_000, t=293, t0=350) 
result = simulator.simulate() 
model = smf.ols('seconds ~ C(k) * C(s)', data=result).fit() 
print(anova.anova_lm(model).round(4)) 
df 
sum_sq 
mean_sq
F 
PR(>F) 
C(k)
2.0 
0.0037
0.0019
2.0451 
0.1441 
C(s)
2.0 
0.0997
0.0499 
54.5429 
0.0000 
C(k):C(s)
4.0 
0.0057
0.0014
1.5600 
0.2060 
Residual
36.0 
0.0329
0.0009
NaN
NaN 
Figure 5.3 shows the effect of the factors spring coefﬁcient k and piston surface 
area s on cycle time. A spring coefﬁcient at 1500 [N/m] decreases the variability 
of mean cycle time. For the piston surface area, we see a strong effect on the cycle 
time. Increasing the surface area, leads to a decrease of cycle time and at the same 
time to a decrease in variability. Figure 5.4 is an interaction plot showing the effect 
of combinations of the two factors on the mean cycle time. 
anova_result = anova.anova_lm(model) 
not_signif = ['C(k)', 'C(k):C(s)', 'Residual'] 
SS = anova_result['sum_sq'].loc[not_signif].sum() 
DF = anova_result['df'].loc[not_signif].sum() 
sigma2 = SS / DF 
print(SS, DF, sigma2) 
0.042345909157371985 42.0 0.0010082359323183806 
The P -values are computed with the appropriate F-distributions. We see in the 
ANOVA table that only the main effects of the piston surface area left parenthesis s right parenthesis(s) are signiﬁcant.

5.7
Full Factorial Experiments
171
Since the effects of the spring coefﬁcient left parenthesis k right parenthesis(k) and that of the interaction are not 
signiﬁcant, we can estimate sigma squaredσ 2 by a pooled estimator, which is 
StartLayout 1st Row 1st Column ModifyingAbove sigma With caret squared 2nd Column equals StartFraction upper S upper S upper W plus upper S upper S upper I plus upper S upper S upper M upper A Over 36 plus 4 plus 2 EndFraction equals StartFraction 0.0423 Over 42.0 EndFraction 2nd Row 1st Column Blank 2nd Column equals 0.00101 period EndLayoutˆσ 2 = SSW + SSI + SSMA
36 + 4 + 2
= 0.0423
42.0
= 0.00101.
Ymean = result.groupby('s').mean()['seconds'] 
print('Ymean', Ymean) 
print('Grand', Ymean.sum() / 3) 
print('Main effects', Ymean - Ymean.sum() / 3) 
Ymean s 
0.0050
0.126664 
0.0125
0.035420 
0.0200
0.019997 
Name: seconds, dtype: float64 
Grand 0.060693739637674736 
Main effects s 
0.0050
0.065970 
0.0125
-0.025274 
0.0200
-0.040696 
Name: seconds, dtype: float64 
To estimate the main effects of s we pool all data from samples having the same 
level of s together. We obtain pooled samples of size n Subscript p Baseline equals 15np = 15. The means of the 
cycle time for these samples are 
The standard error of these main effects is S.E.StartSet ModifyingAbove tau With caret Subscript j Superscript s Baseline EndSet equals StartRoot 0.00101 divided by left parenthesis 2 times 15 right parenthesis EndRoot equals 0.0058{ˆτ s
j } = √0.00101/(2 × 15) =
0.0058. 
Since we estimate on the basis of the pooled samples, and the main effects ModifyingAbove tau With caret Subscript j Superscript sˆτ s
j
left parenthesis j equals 1 comma 2 comma 3 right parenthesis(j = 1, 2, 3) are contrasts of 3 means, the coefﬁcient upper S Subscript alphaSα for the simultaneous 
Fig. 5.3 Effect of spring coefﬁcient k and piston surface area s on cycle time

172
5
Classical Design and Analysis of Experiments
Fig. 5.4 Interaction plot of piston weight spring coefﬁcient 
s 1s1
s 2s2
s 3s3
Grand 
upper Y overbar ¯Y
0.127 
0.035 
0.02 
0.0607 
Main effects 
0.066 
−0.025 
−0.041 
– 
conﬁdence intervals has the formula 
StartLayout 1st Row 1st Column upper S Subscript alpha 2nd Column equals left parenthesis 2 upper F 0.95 left bracket 2 comma 42 right bracket right parenthesis Superscript 1 divided by 2 Baseline 2nd Row 1st Column Blank 2nd Column equals StartRoot 2 times 3.22 EndRoot equals 2.538 period EndLayout
Sα = (2F0.95[2, 42])1/2
=
√
2 × 3.22 = 2.538.
Salpha = np.sqrt(2 * stats.f.ppf(0.95, 2, 42)) 
The simultaneous conﬁdence intervals for tau Subscript j Superscript sτ s
j , at alpha equals 0.05α = 0.05, are calculated using 
ˆτ s 
j ± Sα · S.E.{ˆτ s 
j } 
Lower
Upper 
Limit
Limit 
τ s 
1 :
0.0513
0.0807 
τ s 
2 :
−0.0400 
− 0.0106 
τ s 
3 :
−0.0554 
− 0.0260 
We see that none of the conﬁdence intervals for tau Subscript i Superscript sτ s
i covers zero. Thus, all main effects 
are signiﬁcant. 
black medium square■

5.7
Full Factorial Experiments
173
5.7.3 
Estimating Main Effects and Interactions 
In this section we discuss the estimation of the main effects and interaction 
parameters. Our presentation is conﬁned to the case of two factors A and B, 
which are at a and b levels, respectively. The number of replicas of each treatment 
combinations is n. We further assume that the errors StartSet e Subscript i j k Baseline EndSet{eijk} are i.i.d., having a normal 
distribution upper N left parenthesis 0 comma sigma squared right parenthesisN(0, σ 2). 
Let 
upper Y overbar Subscript i j Baseline equals StartFraction 1 Over n EndFraction sigma summation Underscript l equals 1 Overscript n Endscripts upper Y Subscript i j l ¯Yij = 1
n
n

l=1
Yijl
(5.7.21) 
and 
upper Q Subscript i j Baseline equals sigma summation Underscript l equals 1 Overscript n Endscripts left parenthesis upper Y Subscript i j l Baseline minus upper Y overbar Subscript i j Baseline right parenthesis squared commaQij =
n

l=1
(Yijl −¯Yij)2,
(5.7.22) 
i equals 1 comma midline horizontal ellipsis comma ai = 1, · · · , a; j equals 1 comma midline horizontal ellipsis comma bj = 1, · · · , b. It can be shown that the least squares estimators of 
tau Subscript i Superscript upper Aτ A
i , tau Subscript j Superscript upper Bτ B
j and tau Subscript i j Superscript upper A upper Bτ AB
ij
are, respectively, 
StartLayout 1st Row 1st Column ModifyingAbove tau With caret Subscript i period Superscript upper A 2nd Column equals upper Y overbar Subscript i period Baseline minus upper Y overbar overbar comma 3rd Column i equals 1 comma midline horizontal ellipsis comma a 2nd Row 1st Column ModifyingAbove tau With caret Subscript period j Superscript upper B 2nd Column equals upper Y overbar Subscript period j Baseline minus upper Y overbar overbar comma 3rd Column j equals 1 comma midline horizontal ellipsis comma b EndLayout
ˆτ A
i. = ¯Yi. −¯¯Y,
i = 1, · · · , a
ˆτ B
.j = ¯Y.j −¯¯Y,
j = 1, · · · , b
(5.7.23) 
and 
ModifyingAbove tau With caret Subscript i j Superscript upper A upper B Baseline equals upper Y overbar Subscript i j Baseline minus upper Y overbar Subscript i period Baseline minus upper Y overbar Subscript period j Baseline plus upper Y overbar overbar commaˆτ AB
ij
= ¯Yij −¯Yi. −¯Y.j + ¯¯Y,
(5.7.24) 
where 
upper Y overbar Subscript i period Baseline equals StartFraction 1 Over m EndFraction sigma summation Underscript j equals 1 Overscript m Endscripts upper Y overbar Subscript i j Baseline comma ¯Yi. = 1
m
m

j=1
¯Yij,
(5.7.25) 
and 
upper Y overbar Subscript period j Baseline equals StartFraction 1 Over k EndFraction sigma summation Underscript i equals 1 Overscript k Endscripts upper Y overbar Subscript i j Baseline period ¯Y.j = 1
k
k

i=1
¯Yij.
(5.7.26) 
Furthermore, an unbiased estimator of sigma squaredσ 2 is 
ModifyingAbove sigma With caret squared equals StartFraction sigma summation Underscript i equals 1 Overscript a Endscripts sigma summation Underscript j equals 1 Overscript b Endscripts upper Q Subscript i j Baseline Over a b left parenthesis n minus 1 right parenthesis EndFraction periodˆσ 2 =
a
i=1
b
j=1 Qij
ab(n −1)
.
(5.7.27)

174
5
Classical Design and Analysis of Experiments
The standard errors of the estimators of the interactions are 
upper S period upper E period StartSet ModifyingAbove tau With caret Subscript i j Superscript upper A upper B Baseline EndSet equals StartFraction ModifyingAbove sigma With caret Over StartRoot n EndRoot EndFraction left parenthesis left parenthesis 1 minus StartFraction 1 Over a EndFraction right parenthesis left parenthesis 1 minus StartFraction 1 Over b EndFraction right parenthesis right parenthesis Superscript 1 divided by 2 Baseline commaS.E.{ˆτ AB
ij } =
ˆσ
√n

1 −1
a
 
1 −1
b
1/2
,
(5.7.28) 
for i equals 1 comma midline horizontal ellipsis comma ai = 1, · · · , a; j equals 1 comma midline horizontal ellipsis comma bj = 1, · · · , b. The standard errors of the estimators of the main 
effects are 
upper S period upper E period StartSet ModifyingAbove tau With caret Subscript i Superscript upper A Baseline EndSet equals StartFraction ModifyingAbove sigma With caret Over StartRoot n b EndRoot EndFraction left parenthesis 1 minus StartFraction 1 Over a EndFraction right parenthesis Superscript 1 divided by 2 Baseline comma i equals 1 comma midline horizontal ellipsis comma aS.E.{ˆτ A
i } =
ˆσ
√
nb

1 −1
a
1/2
,
i = 1, · · · , a
(5.7.29) 
and 
upper S period upper E period StartSet ModifyingAbove tau With caret Subscript j Superscript upper B Baseline EndSet equals StartFraction ModifyingAbove sigma With caret Over StartRoot n a EndRoot EndFraction left parenthesis 1 minus StartFraction 1 Over b EndFraction right parenthesis Superscript 1 divided by 2 Baseline comma j equals 1 comma midline horizontal ellipsis comma b periodS.E.{ˆτ B
j } =
ˆσ
√na

1 −1
b
1/2
,
j = 1, · · · , b.
(5.7.30) 
Conﬁdence limits at level left parenthesis 1 minus alpha right parenthesis(1 −α) for such a parameter are obtained by 
StartLayout 1st Row 1st Column ModifyingAbove tau With caret Subscript i Superscript upper A 2nd Column plus or minus upper S Subscript alpha Baseline dot upper S period upper E period StartSet ModifyingAbove tau With caret Subscript i Superscript upper A Baseline EndSet 2nd Row 1st Column ModifyingAbove tau With caret Subscript j Superscript upper B 2nd Column plus or minus upper S Subscript alpha Baseline dot upper S period upper E period StartSet ModifyingAbove tau With caret Subscript j Superscript upper B Baseline EndSet EndLayout
ˆτ A
i ± Sα · S.E.{ˆτ A
i }
ˆτ B
j ± Sα · S.E.{ˆτ B
j }
(5.7.31) 
and 
ModifyingAbove tau With caret Subscript i j Superscript upper A upper B Baseline plus or minus upper S Subscript alpha Baseline upper S period upper E period StartSet ModifyingAbove tau With caret Subscript i j Superscript upper A upper B Baseline EndSetˆτ AB
ij
± Sα S.E.{ˆτ AB
ij }
(5.7.32) 
where 
upper S Subscript alpha Baseline equals left parenthesis left parenthesis a b minus 1 right parenthesis upper F Subscript 1 minus alpha Baseline left bracket a b minus 1 comma a b left parenthesis n minus 1 right parenthesis right bracket right parenthesis Superscript 1 divided by 2 Baseline periodSα = ((ab −1)F1−α[ab −1, ab(n −1)])1/2.
Multiplying the upper S Subscript alphaSα guarantees that all the conﬁdence intervals are simultaneously 
covering the true parameters with probability left parenthesis 1 minus alpha right parenthesis(1−α). Any conﬁdence interval which 
covers the value zero implies that the corresponding parameter is not signiﬁcantly 
different than zero. 
5.7.4 
2m Factorial Designs 
2m factorial designs are full factorials of m factors, each one at two levels. The 
levels of the factors are labelled as “Low” and “High” or 1 and 2. If the factors are 
categorical then the labelling of the levels is arbitrary and the values of the main 
effects and interaction parameters depend on this arbitrary labeling. We will discuss 
here experiments in which the levels of the factors are measured on a continuous

5.7
Full Factorial Experiments
175
scale, like in the case of the factors effecting the piston cycle time. The levels of the 
i-th factor (i = 1, · · ·  , m)  are ﬁxed at xi1 and xi2, where xi1 < xi2. 
By simple transformation all factor levels can be reduced to 
c Subscript i Baseline equals StartLayout Enlarged left brace 1st Row 1st Column plus 1 comma 2nd Column if x equals x Subscript i Baseline 2 Baseline 2nd Row 1st Column Blank 2nd Column to 1 truein comma i equals 1 comma midline horizontal ellipsis comma m period 3rd Row 1st Column negative 1 comma 2nd Column if x equals x Subscript i Baseline 1 Baseline EndLayoutci =
⎧
⎪⎪⎨
⎪⎪⎩
+1,
if x = xi2
to 1truein, i = 1, · · · , m.
−1,
if x = xi1
In such a factorial experiment there are 2m possible treatment combinations. Let 
(i1, · · ·  , im) denote a treatment combination, where i1, · · ·  , im are indices, such 
that 
i Subscript j Baseline equals StartLayout Enlarged left brace 1st Row 1st Column 0 comma 2nd Column if c Subscript i Baseline equals negative 1 2nd Row 1st Column Blank 3rd Row 1st Column 1 comma 2nd Column if c Subscript i Baseline equals 1 period EndLayoutij =
⎧
⎪⎪⎨
⎪⎪⎩
0,
if ci = −1
1,
if ci = 1.
Thus, if there are m = 3 factors, the number of possible treatment combinations is 
23 = 8. These are given in Table 5.17 
The index ν of the standard order, is given by the formula 
nu equals sigma summation Underscript j equals 1 Overscript m Endscripts i Subscript j Baseline 2 Superscript j minus 1 Baseline periodν =
m

j=1
ij2j−1.
(5.7.33) 
Notice that ν ranges from 0 to 2m − 1. This produces tables of the treatment 
combinations for a 2m factorial design, arranged in a standard order which can 
be useful to compare different design (see Table 5.18). The implementation in the 
mistat package does not generate the rows in standard order. However, as we 
mentioned in Sect. 5.2, it is advisable to randomize the designs prior to use, so the 
initial order is in practice lost anyway. 
A full factorial experiment is a combination of fractional factorial designs. In 
Python we obtain a fraction of a full factorial design with the mistat package. 
Table 5.17 Treatment 
combinations of a 23 
experiment 
ν 
i1 
i2 
i3 
0 
0 
0 
0 
1 
1 
0 
0 
2 
0 
1 
0 
3 
1 
1 
0 
4 
0 
0 
1 
5 
1 
0 
1 
6 
0 
1 
1 
7 
1 
1 
1

176
5
Classical Design and Analysis of Experiments
Table 5.18 The labels in 
standard order for a 25 
factorial design 
ν
l1 
l2 
l3 
l4 
l5 
ν
l1 
l2 
l3 
l4 
l5 
0 
1 
1 
1 
1 
1 
16 
1 
1 
1 
1 
2 
1 
2 
1 
1 
1 
1 
17 
2 
1 
1 
1 
2 
2 
1 
2 
1 
1 
1 
18 
1 
2 
1 
1 
2 
3 
2 
2 
1 
1 
1 
19 
2 
2 
1 
1 
2 
4 
1 
1 
2 
1 
1 
20 
1 
1 
2 
1 
2 
5 
2 
1 
2 
1 
1 
21 
2 
1 
2 
1 
2 
6 
1 
2 
2 
1 
1 
22 
1 
2 
2 
1 
2 
7 
2 
2 
2 
1 
1 
23 
2 
2 
2 
1 
2 
8 
1 
1 
1 
2 
1 
24 
1 
1 
1 
2 
2 
9 
2 
1 
1 
2 
1 
25 
2 
1 
1 
2 
2 
10 
1 
2 
1 
2 
1 
26 
1 
2 
1 
2 
2 
11 
2 
2 
1 
2 
1 
27 
2 
2 
1 
2 
2 
12 
1 
1 
2 
2 
1 
28 
1 
1 
2 
2 
2 
13 
2 
1 
2 
2 
1 
29 
2 
1 
2 
2 
2 
14 
1 
2 
2 
2 
1 
30 
1 
2 
2 
2 
2 
15 
2 
2 
2 
2 
1 
31 
2 
2 
2 
2 
2 
d1 = {  
'A': [-1, 1], 
'B': [-1, 1], 
'C': [-1, 1], 
'D': [-1, 1], 
'E': [-1, 1], 
} 
mistat.addTreatments(doe.frac_fact_res(d1, 4), mainEffects=['A', 'B', 'C', 'D', 
'E']) 
Treatments 
A 
B
C
D
E
 
0
(1) -1 -1 -1 -1 -1 
1
AE 
1 -1 -1 -1 
1 
2
BE -1 
1 -1 -1 
1 
3
AB  1  1 -1 -1 -1  
4
CE -1 -1
 
1 -1
 
1
 
5
AC  1 -1  1 -1 -1  
6
BC -1  1  1 -1 -1  
7
ABCE 
1 
1 
1 -1 
1 
8
D -1 -1 -1 
1 -1 
9
ADE 
1 -1 -1 
1 
1 
10
BDE -1 
1 -1 
1 
1 
11
ABD 
1 
1 -1 
1 -1 
12
CDE -1 -1
1
1
1
 
13
ACD 
1 -1 
1 
1 -1 
14
BCD -1 
1 
1 
1 -1 
15
ABCDE 
1 
1
1
1
1
 
This is a half-fractional replications of a 25 designs as will be explained in 
Sect. 5.8. In Table 5.18 we present the design of a 25 full factorial experiment 
derived using Python. 
d1 = {  
'A': [1, 2], 
'B': [1, 2], 
'C': [1, 2], 
'D': [1, 2],

5.7
Full Factorial Experiments
177
Table 5.19 Treatment means 
in a 22 design 
Factor A 
Factor B
1
2
Row means 
1
¯Y0 
¯Y1 
¯Y1. 
2
¯Y2 
¯Y3 
¯Y2. 
Column means 
¯Y0.1 
¯Y0.2 
¯¯Y 
'E': [1, 2], 
} 
Design = doe.full_fact(d1) 
Design = mistat.addTreatments(Design, mainEffects=['A', 'B', 'C', 'D', 'E']) 
print(Design.head(3).round(0)) 
print(Design.tail(3).round(0)) 
Treatments 
A 
B
C
D
E
 
0
(1)
 1
1
1
1
1
 
1
A
 2
1
1
1
1
 
2
B
 1
2
1
1
1
 
Treatments 
A 
B
C
D
E
 
29
ACDE 
2 
1
2
2
2
 
30
BCDE 
1 
2
2
2
2
 
31
ABCDE 
2 
2
2
2
2
 
Let Yν, ν = 0, 1, · · ·  , 2m−1, denote the yield of the ν-th treatment combination. 
We discuss now the estimation of the main effects and interaction parameters. 
Starting with the simple case of 2 factors, the variables are presented schematically, 
in the Table 5.19 
According to our previous deﬁnition there are four main effects τ A 
1 , τ A 
2 , τ B 
1 , τ B 
2 
and four interaction effects τ AB 
11 , τ AB 
12 , τ AB 
21 , τ AB 
22 . But since τ A 
1 +τ A 
2 = τ B 
1 +τ B 
2 = 0, 
it is sufﬁcient to represent the main effects of A and B by τ A 
2 and τ B 
2 . Similarly, since 
τ AB 
11 + τ AB 
12 
= 0 = τ AB 
11 + τ AB 
21 and τ AB 
12 + τ AB 
22 
= 0 = τ AB 
21 + τ AB 
22 , it is sufﬁcient 
to represent the interaction effects by τ AB 
22 . 
The main effect τ A 
2 is estimated by 
StartLayout 1st Row 1st Column ModifyingAbove tau With caret Subscript 2 Superscript upper A 2nd Column equals upper Y overbar Subscript 0.2 Baseline minus upper Y overbar overbar equals 2nd Row 1st Column Blank 2nd Column equals one half left parenthesis upper Y overbar Subscript 1 Baseline plus upper Y overbar Subscript 3 Baseline right parenthesis minus one fourth left parenthesis upper Y overbar Subscript 0 Baseline plus upper Y overbar Subscript 1 Baseline plus upper Y overbar Subscript 2 Baseline plus upper Y overbar Subscript 3 Baseline right parenthesis 3rd Row 1st Column Blank 2nd Column equals one fourth left parenthesis minus upper Y overbar Subscript 0 Baseline plus upper Y overbar Subscript 1 Baseline minus upper Y overbar Subscript 2 Baseline plus upper Y overbar Subscript 3 Baseline right parenthesis period EndLayout
ˆτ A
2 = ¯Y0.2 −¯¯Y =
= 1
2( ¯Y1 + ¯Y3) −1
4( ¯Y0 + ¯Y1 + ¯Y2 + ¯Y3)
= 1
4(−¯Y0 + ¯Y1 −¯Y2 + ¯Y3).
The estimator of τ B 
2 is 
StartLayout 1st Row 1st Column ModifyingAbove tau With caret Subscript 2 Superscript upper B 2nd Column equals upper Y overbar Subscript 2 period Baseline minus upper Y overbar overbar 2nd Row 1st Column Blank 2nd Column equals one half left parenthesis upper Y overbar Subscript 2 Baseline plus upper Y overbar Subscript 3 Baseline right parenthesis minus one fourth left parenthesis upper Y overbar Subscript 0 Baseline plus upper Y overbar Subscript 1 Baseline plus upper Y overbar Subscript 2 Baseline plus upper Y overbar Subscript 3 Baseline right parenthesis 3rd Row 1st Column Blank 2nd Column equals one fourth left parenthesis minus upper Y overbar Subscript 0 Baseline minus upper Y overbar Subscript 1 Baseline plus upper Y overbar Subscript 2 Baseline plus upper Y overbar Subscript 3 Baseline right parenthesis period EndLayout
ˆτ B
2 = ¯Y2. −¯¯Y
= 1
2( ¯Y2 + ¯Y3) −1
4( ¯Y0 + ¯Y1 + ¯Y2 + ¯Y3)
= 1
4(−¯Y0 −¯Y1 + ¯Y2 + ¯Y3).

178
5
Classical Design and Analysis of Experiments
Finally, the estimator of τ AB 
22 is 
StartLayout 1st Row 1st Column ModifyingAbove tau With caret Subscript 22 Superscript upper A upper B 2nd Column equals upper Y overbar Subscript 3 Baseline minus upper Y overbar Subscript 2.0 Baseline minus upper Y overbar Subscript 0.2 Baseline plus upper Y overbar overbar 2nd Row 1st Column Blank 2nd Column equals upper Y overbar Subscript 3 Baseline minus one half left parenthesis upper Y overbar Subscript 2 Baseline plus upper Y overbar Subscript 3 Baseline right parenthesis minus one half left parenthesis upper Y overbar Subscript 1 Baseline plus upper Y overbar Subscript 3 Baseline right parenthesis 3rd Row 1st Column Blank 2nd Column plus one fourth left parenthesis upper Y overbar Subscript 0 Baseline plus upper Y overbar Subscript 1 Baseline plus upper Y overbar Subscript 2 Baseline plus upper Y overbar Subscript 3 Baseline right parenthesis 4th Row 1st Column Blank 2nd Column equals one fourth left parenthesis upper Y overbar Subscript 0 Baseline minus upper Y overbar Subscript 1 Baseline minus upper Y overbar Subscript 2 Baseline plus upper Y overbar Subscript 3 Baseline right parenthesis period EndLayout
ˆτ AB
22
= ¯Y3 −¯Y2.0 −¯Y0.2 + ¯¯Y
= ¯Y3 −1
2( ¯Y2 + ¯Y3) −1
2( ¯Y1 + ¯Y3)
+ 1
4( ¯Y0 + ¯Y1 + ¯Y2 + ¯Y3)
= 1
4( ¯Y0 −¯Y1 −¯Y2 + ¯Y3).
The parameter μ is estimated by the grand mean ¯¯Y = 1 
4( ¯Y0 + ¯Y1 + ¯Y2 + ¯Y3). All  
these estimators can be presented in a matrix form as 
Start 4 By 1 Matrix 1st Row ModifyingAbove mu With caret 2nd Row q u a d ModifyingAbove tau With caret Subscript 2 Superscript upper A Baseline 3rd Row q u a d ModifyingAbove tau With caret Subscript 2 Superscript upper B Baseline 4th Row q u a d ModifyingAbove tau With caret Subscript 22 Superscript upper A upper B Baseline EndMatrix equals one fourth Start 1 By 1 Matrix 1st Row StartLayout 1st Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 1 2nd Row 1st Column negative 1 2nd Column 1 3rd Column negative 1 4th Column 1 3rd Row 1st Column negative 1 2nd Column negative 1 3rd Column 1 4th Column 1 4th Row 1st Column 1 2nd Column negative 1 3rd Column negative 1 4th Column 1 EndLayout EndMatrix dot Start 4 By 1 Matrix 1st Row upper Y overbar Subscript 0 Baseline 2nd Row q u a d upper Y overbar Subscript 1 Baseline 3rd Row q u a d upper Y overbar Subscript 2 Baseline 4th Row q u a d upper Y overbar Subscript 3 Baseline EndMatrix period
⎡
⎢⎢⎣
ˆμ
ˆτ A
2
ˆτ B
2
ˆτ AB
22
⎤
⎥⎥⎦= 1
4
⎡
⎢⎢⎣
1
1
1
1
−1
1 −1
1
−1 −1
1
1
1 −1 −1
1
⎤
⎥⎥⎦·
⎡
⎢⎢⎣
¯Y0
¯Y1
¯Y2
¯Y3
⎤
⎥⎥⎦.
The indices in a 22 design are given in the following 4 × 2 matrix  
upper D Subscript 2 squared Baseline equals Start 4 By 2 Matrix 1st Row 1st Column 1 2nd Column 1 2nd Row 1st Column 2 2nd Column 1 3rd Row 1st Column 1 2nd Column 2 4th Row 1st Column 2 2nd Column 2 EndMatrix periodD22 =
⎡
⎢⎢⎣
1 1
2 1
1 2
2 2
⎤
⎥⎥⎦.
The corresponding C coefﬁcients are the 2nd and 3rd columns in the matrix 
upper C Subscript 2 squared Baseline equals Start 1 By 1 Matrix 1st Row StartLayout 1st Row 1st Column 1 2nd Column negative 1 3rd Column negative 1 4th Column 1 2nd Row 1st Column 1 2nd Column 1 3rd Column negative 1 4th Column negative 1 3rd Row 1st Column 1 2nd Column negative 1 3rd Column 1 4th Column negative 1 4th Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 1 EndLayout EndMatrix periodC22 =
⎡
⎢⎢⎣
1 −1 −1
1
1
1 −1 −1
1 −1
1 −1
1
1
1
1
⎤
⎥⎥⎦.
The fourth column of this matrix is the product of the elements in the second and 
third columns. Notice also that the linear model for the yield vector is 
Start 4 By 1 Matrix 1st Row upper Y 0 2nd Row q u a d upper Y 1 3rd Row q u a d upper Y 2 4th Row q u a d upper Y 3 EndMatrix equals Start 1 By 1 Matrix 1st Row StartLayout 1st Row 1st Column 1 2nd Column negative 1 3rd Column negative 1 4th Column 1 2nd Row 1st Column 1 2nd Column 1 3rd Column negative 1 4th Column negative 1 3rd Row 1st Column 1 2nd Column negative 1 3rd Column 1 4th Column negative 1 4th Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 1 EndLayout EndMatrix Start 4 By 1 Matrix 1st Row mu 2nd Row q u a d tau 2 Superscript upper A Baseline 3rd Row q u a d tau 2 Superscript upper B Baseline 4th Row q u a d tau 22 Superscript upper A upper B Baseline EndMatrix plus Start 4 By 1 Matrix 1st Row e 1 2nd Row q u a d e 2 3rd Row q u a d e 3 4th Row q u a d e 4 EndMatrix comma
⎡
⎢⎢⎣
Y0
Y1
Y2
Y3
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1 −1 −1
1
1
1 −1 −1
1 −1
1 −1
1
1
1
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
μ
τ A
2
τ B
2
τ AB
22
⎤
⎥⎥⎦+
⎡
⎢⎢⎣
e1
e2
e3
e4
⎤
⎥⎥⎦,
where e1, e2, e3 and e4 are independent random variables, with E{ei} =  0 and 
V {ei} =  σ 2, i = 1, 2, · · ·  , 4.

5.7
Full Factorial Experiments
179
Let Y(4) 
= 
(Y0, Y1, Y2, Y3)′, θ(4) 
= 
(μ, τ A 
2 , τ B 
2 , τ AB 
22 )′ and e(4) 
= 
(e1, e2, e3, e4)′ then the model is 
bold upper Y Superscript left parenthesis 4 right parenthesis Baseline equals upper C Subscript 2 squared Baseline bold italic theta Superscript left parenthesis 4 right parenthesis Baseline plus bold e Superscript left parenthesis 4 right parenthesis Baseline periodY(4) = C22θ(4) + e(4).
This is the usual linear model for multiple regression. The least squares estimator of 
θ(4) is 
ModifyingAbove bold italic theta With caret Superscript left parenthesis 4 right parenthesis Baseline equals left bracket upper C prime Subscript 2 squared Baseline upper C Subscript 2 squared Baseline right bracket Superscript negative 1 Baseline upper C prime Subscript 2 squared Baseline bold upper Y Superscript left parenthesis 4 right parenthesis Baseline periodˆθ
(4) = [C′
22C22]−1C′
22Y(4).
The matrix C22 has orthogonal column (row) vectors and 
upper C prime Subscript 2 squared Baseline upper C Subscript 2 squared Baseline equals 4 upper I 4 commaC′
22C22 = 4I4,
where I4 is the identity matrix of rank 4. Therefore, 
StartLayout 1st Row 1st Column ModifyingAbove bold italic theta With caret Superscript left parenthesis 4 right parenthesis 2nd Column equals one fourth upper C prime Subscript 2 squared Baseline bold upper Y Superscript left parenthesis 4 right parenthesis Baseline 2nd Row 1st Column Blank 2nd Column equals one fourth Start 1 By 1 Matrix 1st Row StartLayout 1st Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 1 2nd Row 1st Column negative 1 2nd Column 1 3rd Column negative 1 4th Column 1 3rd Row 1st Column negative 1 2nd Column negative 1 3rd Column 1 4th Column 1 4th Row 1st Column 1 2nd Column negative 1 3rd Column negative 1 4th Column 1 EndLayout EndMatrix Start 4 By 1 Matrix 1st Row upper Y overbar Subscript 0 Baseline 2nd Row q u a d upper Y overbar Subscript 1 Baseline 3rd Row q u a d upper Y overbar Subscript 2 Baseline 4th Row q u a d upper Y overbar Subscript 3 Baseline EndMatrix period EndLayoutˆθ
(4) = 1
4C′
22Y(4)
= 1
4
⎡
⎢⎢⎣
1
1
1
1
−1
1 −1
1
−1 −1
1
1
1 −1 −1
1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
¯Y0
¯Y1
¯Y2
¯Y3
⎤
⎥⎥⎦.
This is identical with the solution obtained earlier. 
The estimators of the main effects and interactions are the least squares estima-
tors, as has been mentioned before. 
This can now be generalized to the case of m factors. For a model with m factors 
there are 2m parameters. The mean μ, m main effects τ 1, · · ·  , τ m,
m 
2

ﬁrst order 
interactions τ ij, i ̸= j = 1, · · ·  , m,
m 
3

second order interactions τ ijk, i ̸= j ̸= k, 
etc. We can now order the parameters in a standard manner in the following manner. 
Each one of the 2m parameters can be represented by a binary vector (j1, · · ·  , jm), 
where ji = 0, 1 (i = 1, · · ·  , m). The vector (0, 0, · · ·  , 0) represents the grand mean 
μ. A vector (0, 0, · · ·  , 1, 0, · · ·  , 0) where the 1 is the i-th component, represents the 
main effect of the i-th factor (i = 1, · · ·  , m). A vector with two ones, at the i-th 
and j-th component (i = 1, · · ·  , m  − 1; j = i + 1, · · ·  , m)  represent the ﬁrst 
order interaction between factor i and factor j. A vector with three ones, at i, j, k 
components, represents the second order interaction between factors i, j, k, etc.  
Let ω = m 
i=1 ji2i−1 and βω be the parameter represented by the vector with 
index ω. For example, β3 corresponds to (1, 1, 0, · · ·  , 0), which represents the ﬁrst 
order interaction between factors 1 and 2. 
Let Y(2m ) be the yield vector, whose components are arranged in the standard 
order, with index ν = 0, 1, 2, · · ·  , 2m −1. Let C2m be the matrix of coefﬁcients, 
that is obtained recursively by the equations

180
5
Classical Design and Analysis of Experiments
upper C 2 equals Start 1 By 1 Matrix 1st Row StartLayout 1st Row 1st Column 1 2nd Column negative 1 2nd Row 1st Column q u a d Baseline 1 2nd Column 1 EndLayout EndMatrix commaC2 =
1 −1
1
1

,
(5.7.34) 
and 
upper C Subscript 2 Sub Superscript l Subscript Baseline equals Start 1 By 1 Matrix 1st Row StartLayout 1st Row 1st Column upper C Subscript 2 Sub Superscript l minus 1 Subscript Baseline 2nd Column minus upper C Subscript 2 Sub Superscript l minus 1 Subscript Baseline 2nd Row 1st Column upper C Subscript 2 Sub Superscript l minus 1 Subscript Baseline 2nd Column upper C Subscript 2 Sub Superscript l minus 1 Subscript Baseline EndLayout EndMatrix commaC2l =
 C2l−1 −C2l−1
C2l−1
C2l−1

,
(5.7.35) 
l = 2, 3, · · ·  , m. Then, the linear model relating Y(2m) to β(2m) is 
bold upper Y Superscript left parenthesis 2 Super Superscript m Superscript right parenthesis Baseline equals upper C Subscript 2 Sub Superscript m Subscript Baseline dot bold italic beta Superscript left parenthesis 2 Super Superscript m Superscript right parenthesis Baseline plus bold e Superscript left parenthesis 2 Super Superscript m Superscript right parenthesis Baseline commaY(2m) = C2m · β(2m) + e(2m),
(5.7.36) 
where 
bold italic beta Superscript left parenthesis 2 Super Superscript m Superscript right parenthesis Baseline equals left parenthesis beta 0 comma beta 1 comma midline horizontal ellipsis comma beta Subscript 2 Sub Superscript m minus 1 Subscript Baseline right parenthesis Superscript prime Baseline periodβ(2m) = (β0, β1, · · · , β2m−1)′.
Since the column vectors of C2m are orthogonal, (C2m)′C2m = 2m I2m, the least 
squares estimator (LSE) of β(2m ) is 
ModifyingAbove bold italic beta With caret Superscript left parenthesis 2 Super Superscript m Superscript right parenthesis Baseline equals StartFraction 1 Over 2 Superscript m Baseline EndFraction left parenthesis upper C Subscript 2 Sub Superscript m Subscript Baseline right parenthesis prime bold upper Y Superscript left parenthesis 2 Super Superscript m Superscript right parenthesis Baseline period ˆβ
(2m) = 1
2m (C2m)′Y(2m).
(5.7.37) 
Accordingly, the LSE of βω is 
ModifyingAbove beta With caret Subscript omega Baseline equals StartFraction 1 Over 2 Superscript m Baseline EndFraction sigma summation Underscript nu equals 0 Overscript 2 Superscript m Baseline minus 1 Endscripts c Subscript left parenthesis nu plus 1 right parenthesis comma left parenthesis omega plus 1 right parenthesis Superscript left parenthesis 2 Super Superscript m Superscript right parenthesis Baseline bold upper Y Subscript nu Baseline comma ˆβω = 1
2m
2m−1

ν=0
c(2m)
(ν+1),(ω+1)Yν,
(5.7.38) 
where c (2m ) 
ij 
is the i-th row and j-th column element of C2m, i.e., multiply the 
components of Y(2m ) by those of the column of C2m, corresponding to the parameter 
βω, and divide the sum of products by 2m. 
We do not have to estimate all the 2m parameters, but can restrict attention only 
to parameters of interest, as will be shown in the following example. 
Since c (2m ) 
ij 
= ±1, the variance of ˆβω is 
upper V left brace ModifyingAbove beta With caret Subscript omega Baseline right brace equals StartFraction sigma squared Over 2 Superscript m Baseline EndFraction comma for all omega equals 0 comma midline horizontal ellipsis comma 2 Superscript m Baseline minus 1 periodV { ˆβω} = σ 2
2m ,
for all ω = 0, · · · , 2m −1.
(5.7.39) 
Finally, if every treatment combination is repeated n times, the estimation of the 
parameters is based on the means ¯Yν of the n replications. The variance of ˆβω 
becomes 
upper V left brace ModifyingAbove beta With caret Subscript omega Baseline right brace equals StartFraction sigma squared Over n 2 Superscript m Baseline EndFraction periodV { ˆβω} = σ 2
n2m .
(5.7.40)

5.7
Full Factorial Experiments
181
The variance σ 2 can be estimated by the pooled variance estimator, obtained from 
the between replication variance within each treatment combinations. That is, if Yνj, 
j = 1, · · ·  , n, are the observed values at the ν-th treatment combination then 
ModifyingAbove sigma With caret squared equals StartFraction 1 Over left parenthesis n minus 1 right parenthesis 2 Superscript m Baseline EndFraction sigma summation Underscript nu equals 1 Overscript 2 Superscript m Baseline Endscripts sigma summation Underscript j equals 1 Overscript n Endscripts left parenthesis upper Y Subscript nu j Baseline minus upper Y overbar Subscript nu Baseline right parenthesis squared periodˆσ 2 =
1
(n −1)2m
2m

ν=1
n

j=1
(Yνj −¯Yν)2.
(5.7.41) 
Example 5.8 In Example 5.7 we studied the effects of two factors on the cycle time 
of a piston in a gas turbine, keeping all the other ﬁve factors ﬁxed. In the present 
example we perform a 25 experiment with the piston varying factors m, s, v0, k, and 
t at two levels, keeping the atmospheric pressure (factor E) ﬁxed at 90,000 [N/m2] 
and the ﬁlling gas temperature (factor G) at 340 [◦K]. The two levels of each factor 
are those speciﬁed, in Example 5.7, as the limits of the experimental range. Thus, 
for example, the low level of piston weight (factor A) is 30 [Kg] and its high level 
is 60 [Kg]. The treatment combinations are listed in Table 5.20. The table also lists 
the average response ¯Yν. 
np.random.seed(3) 
factors = { 
'm': [30, 60], 
's': [0.005, 0.02], 
'v0': [0.002, 0.01], 
'k': [1000, 5000], 
't': [290, 296], 
} 
Design = doe.full_fact(factors) 
# Randomize design 
Design = Design.sample(frac=1).reset_index(drop=True) 
# Run the simulation with 5 replications for each setting 
simulator = mistat.PistonSimulator(**{k:list(Design[k]) for k in Design}, 
p0=90_000, t0=340, n_replicate=5) 
result = simulator.simulate() 
# Pooled standard deviation 13.7.41 
byFactors = result.groupby(list(factors.keys())) 
groupedStd = byFactors.std()['seconds'] 
pooledVar = np.mean(groupedStd**2) 
Vparam = pooledVar 
/ (5 * len(byFactors)) 
SE = np.sqrt(Vparam) 
The number of replications is n = 5. Denote the means ¯Yν and the standard 
deviations, Sν, of the ﬁve observations in each treatment combination. We obtain 
the value ˆσ 2 = 0.00079 and the estimated variance of all LSE of the parameters is 
ModifyingAbove upper V With caret left brace ModifyingAbove beta With caret Subscript omega Baseline right brace equals StartFraction ModifyingAbove sigma With caret squared Over 5 times 32 EndFraction equals 4.9 e minus 06 commaˆV { ˆβω} =
ˆσ 2
5 × 32 = 4.9e −06,
or standard error of S.E.{ ˆβω} =  0.00222. For example, as an estimate of the main 
effect of m we obtain the value ˆβ1 = 0.001852. In the following tables derived using

182
5
Classical Design and Analysis of Experiments
Table 5.20 Labels of 
treatment combinations and 
average response 
m 
s
v0
k
t
¯Yν
Sν 
30 
0.005 
0.002 
1000 
290 
0.027 
0.016 
30 
0.005 
0.002 
1000 
296 
0.035 
0.019 
30 
0.005 
0.002 
5000 
290 
0.045 
0.017 
30 
0.005 
0.002 
5000 
296 
0.040 
0.023 
30 
0.005 
0.010 
1000 
290 
0.164 
0.047 
30 
0.005 
0.010 
1000 
296 
0.188 
0.013 
30 
0.005 
0.010 
5000 
290 
0.200 
0.034 
30 
0.005 
0.010 
5000 
296 
0.207 
0.036 
30 
0.020 
0.002 
1000 
290 
0.008 
0.002 
30 
0.020 
0.002 
1000 
296 
0.009 
0.001 
30 
0.020 
0.002 
5000 
290 
0.009 
0.002 
30 
0.020 
0.002 
5000 
296 
0.006 
0.002 
30 
0.020 
0.010 
1000 
290 
0.042 
0.002 
30 
0.020 
0.010 
1000 
296 
0.042 
0.003 
30 
0.020 
0.010 
5000 
290 
0.050 
0.003 
30 
0.020 
0.010 
5000 
296 
0.050 
0.005 
60 
0.005 
0.002 
1000 
290 
0.022 
0.007 
60 
0.005 
0.002 
1000 
296 
0.021 
0.009 
60 
0.005 
0.002 
5000 
290 
0.049 
0.008 
60 
0.005 
0.002 
5000 
296 
0.037 
0.019 
60 
0.005 
0.010 
1000 
290 
0.213 
0.079 
60 
0.005 
0.010 
1000 
296 
0.224 
0.072 
60 
0.005 
0.010 
5000 
290 
0.238 
0.050 
60 
0.005 
0.010 
5000 
296 
0.294 
0.065 
60 
0.020 
0.002 
1000 
290 
0.008 
0.001 
60 
0.020 
0.002 
1000 
296 
0.010 
0.003 
60 
0.020 
0.002 
5000 
290 
0.009 
0.002 
60 
0.020 
0.002 
5000 
296 
0.011 
0.003 
60 
0.020 
0.010 
1000 
290 
0.049 
0.003 
60 
0.020 
0.010 
1000 
296 
0.050 
0.004 
60 
0.020 
0.010 
5000 
290 
0.059 
0.006 
60 
0.020 
0.010 
5000 
296 
0.061 
0.006 
Python we present the LSE’s of all the 5 main effects and 10 ﬁrst order interactions. 
The S.E. values in the table are the standard errors of the estimates and the t values 
are t = LSE 
SE . 
# Perform analysis of variance 
Design['response'] = result['seconds'] 
model = smf.ols('seconds ~ (m + s + v0 + k + t)  ** 2', data=result).fit() 
# print(anova.anova_lm(model)) 
print(f'r2={model.rsquared}') 
r2=0.9064681668404627

5.7
Full Factorial Experiments
183
Table 5.21 LSE of main 
effects and interactions 
LSE
S.E.
t 
m 
−0.0054
0.00222 
−2.44 
s
37.9277
0.00222 
17097.52
** 
v0 
−61.9511
0.00222 
−27927.08 
** 
k
−0.0000
0.00222 
−0.00 
t
−0.0001
0.00222 
−0.07 
m:s 
−0.0413
0.00222 
−18.62
** 
m:v0 
0.1332
0.00222 
60.03
** 
m:k 
0.0000
0.00222 
0.00 
m:t
0.0000
0.00222 
0.01 
s:v0 
−1165.8813 
0.00222 
−525569.93 
** 
s:k 
−0.0004
0.00222 
−0.17 
s:t 
−0.1173
0.00222 
−52.86
** 
v0:k 
0.0005
0.00222 
0.21 
v0:t 
0.2834
0.00222 
127.76
** 
k:t
0.0000
0.00222 
0.00 
Fig. 5.5 Main effects plot 
print(np.var(model.predict(result))) 
0.0068959316771731935 
Values of t which are greater in magnitude than 2.6 are signiﬁcant at α = 0.02. If 
we wish, however, that all 15 tests have simultaneously a level of signiﬁcance of α = 
0.05 we should use as critical value the Scheffé coefﬁcient √32 × F0.95[32, 128] =  
7.01, since all the LSE are contrasts of 32 means. In Table 5.21 we marked 
with one * the t values greater in magnitude than 2.6, and with ** those greater 
than 7. When we execute this regression, we obtain R2 = 0.906. The variance 
around the regression surface is s2 
y = 0.00761. This is signiﬁcantly greater than 
ˆσ 2/5 = 0.00016. This means that there might be signiﬁcant high order interactions, 
which have not been estimated. 
Figure 5.5 is a graphical display of the main effects of factors m, s, v0, k and t. 
The left limit of a line shows the average response at a low level and the right limit

184
5
Classical Design and Analysis of Experiments
Fig. 5.6 Two-way interaction plots 
that at a high level. Factors s and v0 seem to have the highest effect, as is shown by 
the t-values in Table 5.21. Figure 5.6 shows the two-way interactions of the various 
factors. Interaction (s ∗ v0) is the most pronounced. 
From Fig. 5.5 we see that s has a big impact on cycle time. In Fig. 5.6 we realize 
that the effect of s at its lowest level (s = 0.005) is observed at the high level of v0 
(v0 = 0.01). If we only look at the main effect plots (Fig. 5.5) we can be mislead 
that the effect of s does not depend on v0. 
In general, main effects and interactions need to be considered simultaneously. 
We should stress that interactions are not to be considered as secondary to main 
effects. To emphasize this point, Kenett and Vogel (1991) suggested graphing main 
effects and interactions on the same plot. To achieve this, main effects are drawn 
vertically next to interaction plots. Figure 5.7 shows such a plot for the same data.
■

5.7
Full Factorial Experiments
185
Fig. 5.7 Combined main effects (left) and interaction plots (right). For the main effects, the lower 
factor levels are identiﬁed as red circles, and the higher factor level as a black square. For the 
interaction part, lower factor levels are identiﬁed as red and higher factor levels as black half 
squares 
5.7.5 
3m Factorial Designs 
We discuss here the estimation and testing of model parameters, when the design 
is full factorial, of m factors each one at p equals 3p = 3 levels. We assume that the levels 
are measured on a continuous scale, and are labelled Low, Medium and High. We 
introduce the indices i Subscript jij left parenthesis j equals 1 comma midline horizontal ellipsis comma m right parenthesis(j = 1, · · · , m), with values 0, 1, 2 for the Low, Medium 
and High levels, correspondingly, of each factor. Thus, we have 3 Superscript m3m treatment 
combinations, represented by vectors of indices left parenthesis i 1 comma i 2 comma midline horizontal ellipsis comma i Subscript m Baseline right parenthesis(i1, i2, · · · , im). The index nuν of the 
standard order of treatment combination is 
nu equals sigma summation Underscript j equals 1 Overscript m Endscripts i Subscript j Baseline 3 Superscript j minus 1 Baseline periodν =
m

j=1
ij3j−1.
(5.7.42) 
This index ranges from 0 to 3 Superscript m Baseline minus 13m −1. Let upper Y overbar Subscript nu¯Yν denote the yield of n replicas of the nuν-th 
treatment combination, n greater than or equals 1n ≥1. 
Since we obtain the yield at three levels of each factor we can, in addition to the 
linear effects estimate also the quadratic effects of each factor. For example, if we 
have m equals 2m = 2 factors, we can use a multiple regression method to ﬁt the model 
StartLayout 1st Row upper Y equals beta 0 plus beta 1 x 1 plus beta 2 x 1 squared plus beta 3 x 2 plus beta 4 x 1 x 2 plus 2nd Row beta 5 x 1 squared x 2 plus beta 6 x 2 squared plus beta 7 x 1 x 2 squared plus beta 8 x 1 squared x 2 squared plus e period EndLayoutY = β0 + β1x1 + β2x2
1 + β3x2 + β4x1x2+
β5x2
1x2 + β6x2
2 + β7x1x2
2 + β8x2
1x2
2 + e.
(5.7.43) 
This is a quadratic model in two variables. beta 1β1 and beta 3β3 represent the linear effects of x 1x1
and x 2x2. beta 2β2 and beta 6β6 represent the quadratic effects of x 1x1 and x 2x2. The other coefﬁcients 
represent interaction effects. beta 4β4 represents the linear times× linear interaction, beta 5β5

186
5
Classical Design and Analysis of Experiments
represents the quadratic times× linear interaction, etc. We have two main effects for each 
factor (linear and quadratic) and 4 interaction effects. 
Generally, if there are m factors we have, in addition to beta 0β0, 2m parameters for 
main effects (linear and quadratic) 2 squared StartBinomialOrMatrix m Choose 2 EndBinomialOrMatrix22m
2

parameters for interactions between 2 
factors, 2 cubed StartBinomialOrMatrix m Choose 3 EndBinomialOrMatrix23m
3

interactions between 3 factors, etc. Generally, we have 3 Superscript m3m parameters, 
where 
3 Superscript m Baseline equals sigma summation Underscript j equals 0 Overscript m Endscripts 2 Superscript j Baseline StartBinomialOrMatrix m Choose j EndBinomialOrMatrix period3m =
m

j=0
2j
m
j

.
As in the case of 2 Superscript m2m models, each parameter in a 3 Superscript m3m model is represented by a vector 
of m indices left parenthesis lamda 1 comma lamda 2 comma midline horizontal ellipsis comma lamda Subscript m Baseline right parenthesis(λ1, λ2, · · · , λm) where lamda Subscript j Baseline equals 0 comma 1 comma 2λj = 0, 1, 2. Thus, for example, the vector 
left parenthesis 0 comma 0 comma midline horizontal ellipsis comma 0 right parenthesis(0, 0, · · · , 0) represent the grand mean mu equals gamma 0μ = γ0. A vector left parenthesis 0 comma midline horizontal ellipsis comma 0 comma 1 comma 0 comma midline horizontal ellipsis comma 0 right parenthesis(0, · · · , 0, 1, 0, · · · , 0)
with 1 at the  i-th component represents the linear effect of the i-th factor. Similarly, 
left parenthesis 0 comma 0 comma midline horizontal ellipsis comma 0 comma 2 comma 0 comma midline horizontal ellipsis comma 0 right parenthesis(0, 0, · · · , 0, 2, 0, · · · , 0) represents the quadratic effect of the i-th factor. Two 
indices equal to 1 and all the rest zero, represent the linear times× linear interaction 
of the i-th and j-th factor, etc. The standard order of the parameters is 
omega equals sigma summation Underscript j equals 1 Overscript m Endscripts lamda Subscript j Baseline 3 Superscript j minus 1 Baseline comma omega equals 0 comma midline horizontal ellipsis comma 3 Superscript m Baseline minus 1 periodω =
m

j=1
λj3j−1,
ω = 0, · · · , 3m −1.
If m is not too large, it is customary to label the factors by the letters upper A comma upper B comma upper C comma midline horizontal ellipsisA, B, C, · · ·
and the parameters by upper A Superscript lamda 1 Baseline upper B Superscript lamda 2 Baseline upper C Superscript lamda 3 Baseline midline horizontal ellipsisAλ1Bλ2Cλ3 · · · . In this notation a letter to the zero power is 
omitted. In Table 5.22 we list the parameters of a 3 cubed33 system. 
Table 5.22 The main effects 
and interactions of a 3 cubed33
factorial 
omegaω
Parameter 
Indices 
omegaω
Parameter 
Indices 
0 
Mean
(0,0,0) 
15 
upper B squared upper CB2C
(0,2,1) 
1 
A
(1,0,0) 
16 
upper A upper B squared upper CAB2C
(1,2,1) 
2 
upper A squaredA2
(2,0,0) 
17 
upper A squared upper B squared upper CA2B2C
(2,2,1) 
3 
B
(0,1,0) 
18 
upper C squaredC2
(0,0,2) 
4 
AB
(1,1,0) 
19 
upper A upper C squaredAC2
(1,0,2) 
5 
upper A squared upper BA2B
(2,1,0) 
20 
upper A squared upper C squaredA2C2
(2,0,2) 
6 
upper B squaredB2
(0,2,0) 
21 
upper B upper C squaredBC2
(0,1,2) 
7 
upper A upper B squaredAB2
(1,2,0) 
22 
upper A upper B upper C squaredABC2
(1,1,2) 
8 
upper A squared upper B squaredA2B2
(2,2,0) 
23 
upper A squared upper B upper C squaredA2BC2
(2,1,2) 
9 
C
(0,0,1) 
24 
upper B squared upper C squaredB2C2
(0,2,2) 
10 
AC
(1,0,1) 
25 
upper A upper B squared upper C squaredAB2C2
(1,2,2) 
11 
upper A squared upper CA2C
(2,0,1) 
26 
upper A squared upper B squared upper C squaredA2B2C2
(2,2,2) 
12 
BC
(0,1,1) 
13 
ABC
(1,1,1) 
14 
upper A squared upper B upper CA2BC
(2,1,1)

5.7
Full Factorial Experiments
187
It is simple to transform the x-values of each factor to 
upper X Subscript j Baseline equals StartLayout Enlarged left brace 1st Row 1st Column negative 1 comma 2nd Column if i Subscript j Baseline equals 0 2nd Row 1st Column Blank 3rd Row 1st Column 0 comma 2nd Column if i Subscript j Baseline equals 1 4th Row 1st Column Blank 5th Row 1st Column 1 comma 2nd Column if i Subscript j Baseline equals 2 period EndLayoutXj =
⎧
⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎩
−1,
if ij = 0
0,
if ij = 1
1,
if ij = 2.
However, the matrix of coefﬁcients X that is obtained, when we have quadratic and 
interaction parameters, is not orthogonal. This requires then the use of the computer 
to obtain the least squares estimators, with the usual multiple regression program. 
Another approach is to redeﬁne the effects so that the statistical model will be linear 
with a matrix having coefﬁcients obtained by the method of orthogonal polynomials 
(see Draper and Smith 1998). Thus, consider the model 
bold upper Y Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis Baseline equals upper Psi Subscript left parenthesis 3 Sub Superscript m Subscript right parenthesis Baseline bold italic gamma Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis Baseline plus bold e Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis Baseline commaY(3m) = (3m)γ (3m) + e(3m),
(5.7.44) 
where 
bold upper Y Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis Baseline equals left parenthesis upper Y overbar Subscript 0 Baseline comma midline horizontal ellipsis comma upper Y overbar Subscript 3 Sub Superscript m Subscript minus 1 Baseline right parenthesis Superscript prime Baseline commaY(3m) = ( ¯Y0, · · · , ¯Y3m−1)′,
and bold e Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis Baseline equals left parenthesis e 0 comma midline horizontal ellipsis comma e Subscript 3 Sub Superscript m Subscript minus 1 Baseline right parenthesis primee(3m) = (e0, · · · , e3m−1)′ is a vector of random variables with 
upper E left brace e Subscript nu Baseline right brace equals 0 comma upper V left brace e Subscript nu Baseline right brace equals sigma squared all nu equals 0 comma midline horizontal ellipsis comma 3 Superscript m Baseline minus 1 periodE{eν} = 0,
V {eν} = σ 2
all ν = 0, · · · , 3m −1.
Moreover for m equals 1m = 1
upper Psi Subscript left parenthesis 3 right parenthesis Baseline equals Start 1 By 1 Matrix 1st Row StartLayout 1st Row 1st Column 1 2nd Column negative 1 3rd Column 1 2nd Row 1st Column 1 2nd Column 0 3rd Column negative 2 3rd Row 1st Column 1 2nd Column 1 3rd Column 1 EndLayout EndMatrix period(3) =
⎡
⎣
1 −1
1
1
0 −2
1
1
1
⎤
⎦.
(5.7.45) 
For m greater than or equals 2m ≥2 it can be calculated iteratively using the Kronecker product of upper Psi Subscript left parenthesis 3 right parenthesis(3) and 
upper Psi Subscript left parenthesis 3 Sub Superscript m minus 1 Subscript right parenthesis(3m−1), 
upper Psi Subscript left parenthesis 3 Sub Superscript m Subscript right parenthesis Baseline equals upper Psi Subscript left parenthesis 3 right parenthesis Baseline circled times upper Psi Subscript left parenthesis 3 Sub Superscript m minus 1 Subscript right parenthesis Baseline equals Start 1 By 1 Matrix 1st Row StartLayout 1st Row 1st Column upper Psi Subscript left parenthesis 3 Sub Superscript m minus 1 Subscript right parenthesis Baseline 2nd Column minus upper Psi Subscript left parenthesis 3 Sub Superscript m minus 1 Subscript right parenthesis Baseline 3rd Column upper Psi Subscript left parenthesis 3 Sub Superscript m minus 1 Subscript right parenthesis Baseline 2nd Row 1st Column upper Psi Subscript left parenthesis 3 Sub Superscript m minus 1 Subscript right parenthesis Baseline 2nd Column 0 3rd Column minus 2 upper Psi Subscript left parenthesis 3 Sub Superscript m minus 1 Subscript right parenthesis Baseline 3rd Row 1st Column upper Psi Subscript left parenthesis 3 Sub Superscript m minus 1 Subscript right parenthesis Baseline 2nd Column upper Psi Subscript left parenthesis 3 Sub Superscript m minus 1 Subscript right parenthesis Baseline 3rd Column upper Psi Subscript left parenthesis 3 Sub Superscript m minus 1 Subscript right parenthesis Baseline EndLayout EndMatrix period(3m) = (3) ⊗(3m−1) =
⎡
⎣
(3m−1) −(3m−1)
(3m−1)
(3m−1)
0
−2(3m−1)
(3m−1)
(3m−1)
(3m−1)
⎤
⎦.
(5.7.46) 
The matrices upper Psi Subscript left parenthesis 3 Sub Superscript m Subscript right parenthesis(3m) have orthogonal column vectors and 
left parenthesis upper Psi Subscript left parenthesis 3 Sub Superscript m Subscript right parenthesis Baseline right parenthesis prime left parenthesis upper Psi Subscript left parenthesis 3 Sub Superscript m Subscript right parenthesis Baseline right parenthesis equals upper Delta Subscript left parenthesis 3 Sub Superscript m Subscript right parenthesis Baseline comma((3m))′((3m)) = (3m),
(5.7.47)

188
5
Classical Design and Analysis of Experiments
where upper Delta Subscript left parenthesis 3 Sub Superscript m Subscript right parenthesis(3m) is a diagonal matrix whose diagonal elements are equal to the sum 
of squares of the elements in the corresponding column of upper Psi Subscript left parenthesis 3 Sub Superscript m Subscript right parenthesis(3m). For example, for 
m equals 1m = 1, 
upper Delta Subscript left parenthesis 3 right parenthesis Baseline equals Start 3 By 3 Matrix 1st Row 1st Column 3 2nd Column 0 3rd Column 0 2nd Row 1st Column q u a d Baseline 0 2nd Column 2 3rd Column 0 3rd Row 1st Column q u a d Baseline 0 2nd Column 0 3rd Column 6 EndMatrix period(3) =
⎛
⎝
3 0 0
0 2 0
0 0 6
⎞
⎠.
For m equals 2m = 2 we obtain 
upper Psi Subscript left parenthesis 9 right parenthesis Baseline equals Start 1 By 1 Matrix 1st Row StartLayout 1st Row 1st Column 1 2nd Column negative 1 3rd Column 1 4th Column negative 1 5th Column 1 6th Column negative 1 7th Column 1 8th Column negative 1 9th Column 1 2nd Row 1st Column 1 2nd Column 0 3rd Column negative 2 4th Column negative 1 5th Column 0 6th Column 2 7th Column 1 8th Column 0 9th Column negative 2 3rd Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column negative 1 5th Column negative 1 6th Column negative 1 7th Column 1 8th Column 1 9th Column 1 4th Row 1st Column 1 2nd Column negative 1 3rd Column 1 4th Column 0 5th Column 0 6th Column 0 7th Column negative 2 8th Column 2 9th Column negative 2 5th Row 1st Column 1 2nd Column 0 3rd Column negative 2 4th Column 0 5th Column 0 6th Column 0 7th Column negative 2 8th Column 0 9th Column 4 6th Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 0 5th Column 0 6th Column 0 7th Column negative 2 8th Column negative 2 9th Column negative 2 7th Row 1st Column 1 2nd Column negative 1 3rd Column 1 4th Column 1 5th Column negative 1 6th Column 1 7th Column 1 8th Column negative 1 9th Column 1 8th Row 1st Column 1 2nd Column 0 3rd Column negative 2 4th Column 1 5th Column 0 6th Column negative 2 7th Column 1 8th Column 0 9th Column negative 2 9th Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 1 5th Column 1 6th Column 1 7th Column 1 8th Column 1 9th Column 1 EndLayout EndMatrix(9) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 −1
1 −1
1 −1
1 −1
1
1
0 −2 −1
0
2
1
0 −2
1
1
1 −1 −1 −1
1
1
1
1 −1
1
0
0
0 −2
2 −2
1
0 −2
0
0
0 −2
0
4
1
1
1
0
0
0 −2 −2 −2
1 −1
1
1 −1
1
1 −1
1
1
0 −2
1
0 −2
1
0 −2
1
1
1
1
1
1
1
1
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
and 
upper Delta Subscript left parenthesis 9 right parenthesis Baseline equals Start 9 By 9 Matrix 1st Row 1st Column 9 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column Blank 9th Column Blank 2nd Row 1st Column Blank 2nd Column 6 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column Blank 9th Column Blank 3rd Row 1st Column Blank 2nd Column Blank 3rd Column 18 4th Column Blank 5th Column Blank 6th Column Blank 7th Column 0 8th Column Blank 9th Column Blank 4th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column 6 5th Column Blank 6th Column Blank 7th Column Blank 8th Column Blank 9th Column Blank 5th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column 4 6th Column Blank 7th Column Blank 8th Column Blank 9th Column Blank 6th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column 12 7th Column Blank 8th Column Blank 9th Column Blank 7th Row 1st Column Blank 2nd Column Blank 3rd Column 0 4th Column Blank 5th Column Blank 6th Column Blank 7th Column 18 8th Column Blank 9th Column Blank 8th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column 12 9th Column Blank 9th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column Blank 9th Column 36 EndMatrix period(9) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
9
6
18
0
6
4
12
0
18
12
36
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Thus, the LSE of bold italic gamma Superscript left parenthesis 3 Super Superscript m Superscript right parenthesisγ (3m) is 
ModifyingAbove bold italic gamma With caret Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis Baseline equals upper Delta Subscript left parenthesis 3 Sub Superscript m Subscript right parenthesis Superscript negative 1 Baseline left parenthesis upper Psi Subscript left parenthesis 3 Sub Superscript m Subscript right parenthesis Baseline right parenthesis prime bold upper Y Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis Baseline period ˆγ (3m) = −1
(3m)((3m))′Y(3m).
(5.7.48) 
These LSE are best linear unbiased estimators and 
upper V left brace ModifyingAbove gamma With caret Subscript omega Baseline right brace equals StartFraction sigma squared Over n sigma summation Underscript i equals 1 Overscript 3 Superscript m Baseline Endscripts left parenthesis upper Psi Subscript i comma omega plus 1 Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis Baseline right parenthesis squared EndFraction periodV { ˆγω} =
σ 2
n 3m
i=1((3m)
i,ω+1)2 .
(5.7.49) 
If the number of replicas, n, is greater than 1 then sigma squaredσ 2 can be estimated by

5.7
Full Factorial Experiments
189
ModifyingAbove sigma With caret squared equals StartFraction 1 Over 3 Superscript m Baseline left parenthesis n minus 1 right parenthesis EndFraction sigma summation Underscript nu equals 0 Overscript 3 Superscript m Baseline minus 1 Endscripts sigma summation Underscript l equals 1 Overscript n Endscripts left parenthesis upper Y Subscript nu l Baseline minus upper Y overbar Subscript nu Baseline right parenthesis squared periodˆσ 2 =
1
3m(n −1)
3m−1

ν=0
n

l=1
(Yνl −¯Yν)2.
(5.7.50) 
If n equals 1n = 1 we can estimate sigma squaredσ 2 if it is known a priori that some parameters gamma Subscript omegaγω are zero. 
Let upper Lamda 00 be the set of all parameters which can be assumed to be negligible. Let upper K 0K0 be 
the number of elements of upper Lamda 00. If  omega element of upper Lamda 0ω ∈0 then ModifyingAbove gamma With caret Subscript omega Superscript 2 Baseline left parenthesis sigma summation Underscript j equals 1 Overscript 3 Superscript m Baseline Endscripts left parenthesis upper Psi Subscript i comma omega plus 1 Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis Baseline right parenthesis squared right parenthesis ˆγ 2
ω
3m
j=1((3m)
i,ω+1)2
is distributed 
like sigma squared chi squared left bracket 1 right bracketσ 2χ2[1]. Therefore, an unbiased estimator of sigma squaredσ 2 is 
ModifyingAbove Above ModifyingAbove sigma squared With caret With caret equals StartFraction 1 Over k 0 EndFraction sigma summation Underscript omega element of upper Lamda 0 Endscripts ModifyingAbove gamma With caret Subscript omega Superscript 2 Baseline left parenthesis sigma summation Underscript j equals 1 Overscript 3 Superscript m Baseline Endscripts left parenthesis upper Psi Subscript j comma omega plus 1 Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis Baseline right parenthesis squared right parenthesis period ˆˆσ 2 = 1
k0

ω∈0
ˆγ 2
ω
⎛
⎝
3m

j=1
((3m)
j,ω+1)2
⎞
⎠.
(5.7.51) 
Example 5.9 Oikawa and Oka (1987) reported the results of a 3 cubed33 experiment to 
investigate the effects of three factors A, B, C on the stress levels of a membrane 
Y. The data is given in dataset STRESS.csv. The ﬁrst three columns of the dataset 
provide the levels of the three factors, and column 4 presents the stress values. 
In order to use the methodology derived in this section, we ﬁrst deﬁne a 
few utility functions. The function getStandardOrder determines the position of a 
combination of levels in the standard order as shown in Table 5.22. 
def getStandardOrder(levels, labels): 
parameter = '' 
omega = 0 
for i, (level, label) in enumerate(zip(levels, labels), 1): 
omega += level * 3**(i-1) 
if level == 1: 
parameter = f'{parameter}{label}' 
elif level == 2: 
parameter = f'{parameter}{label}2' 
if parameter == '': 
parameter = 'Mean' 
return {'omega': omega, 'Parameter': parameter} 
This function can be used to sort the dataset STRESS.csv in standard order. 
stress = mistat.load_data('STRESS') 
standardOrder = pd.DataFrame(getStandardOrder(row[['A','B','C']], 'ABC') 
for _, row in stress.iterrows()) 
# add information to dataframe stress and sort in standard order 
stress.index = standardOrder['omega'] 
stress['Parameter'] = standardOrder['Parameter'] 
stress = stress.sort_index() 
The function get_psi3m calculates the matrix upper Psi Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis(3m) recursively. Here, we use the 
function np.kron to calculate the Kronecker product. 
def get_psi3m(m): 
psi31 = np.array([[1, -1, 1], [1, 0, -2], [1, 1, 1]] ) 
if m == 1:  
return psi31

190
5
Classical Design and Analysis of Experiments
psi3m1 = get_psi3m(m-1) 
return np.kron(psi31, psi3m1) 
This allows us to calculate the LSE estimates ModifyingAbove bold italic gamma With caret Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis ˆγ (3m). 
Y_3m = stress['stress'] 
psi3m = get_psi3m(3) 
delta3m = np.matmul(psi3m.transpose(), psi3m) 
inv_delta3m = np.diag(1/np.diag(delta3m)) 
gamma_3m = np.matmul(inv_delta3m, np.matmul(psi3m.transpose(), Y_3m)) 
estimate = pd.DataFrame({ 
'Parameter': stress['Parameter'], 
'LSE': gamma_3m, 
}) 
Suppose that from technological considerations we decide that all interaction 
parameters involving quadratic components are negligible (zero). In this case we 
can estimate sigma squaredσ 2 by ModifyingAbove Above ModifyingAbove sigma squared With caret With caret ˆˆσ 2. In the present example the set upper Lamda 00 contains 16 parameters, 
i.e., 
StartLayout 1st Row 1st Column upper Lamda 0 2nd Column equals left brace upper A squared upper B comma upper A upper B squared comma upper A squared upper B squared comma upper A squared upper C comma upper A squared upper B upper C comma upper B squared upper C comma upper A upper B squared upper C comma upper A squared upper B squared upper C comma upper A upper C squared comma 2nd Row 1st Column Blank 2nd Column upper A squared upper C squared comma upper B upper C squared comma upper A upper B upper C squared comma upper A squared upper B upper C comma upper B squared upper C squared comma upper A upper B squared upper C squared comma upper A squared upper B squared upper C squared right brace period EndLayout0 = {A2B, AB2, A2B2, A2C, A2BC, B2C, AB2C, A2B2C, AC2,
A2C2, BC2, ABC2, A2BC, B2C2, AB2C2, A2B2C2}.
Thus upper K 0 equals 16K0 = 16 and the estimator ModifyingAbove Above ModifyingAbove sigma squared With caret With caret ˆˆσ 2 has 16 degrees of freedom. The estimate of sigma squaredσ 2
is ModifyingAbove Above ModifyingAbove sigma squared With caret With caret equals 95.95 ˆˆσ 2 = 95.95. It is calculated in Python as follows. 
# determine Lambda0 set as interactions that include quadratic terms 
lambda0 = [term for term in stress['Parameter'] if '2' in term and len(term) > 2] 
print(f'lambda0 : {lambda0}') 
estimate['Significance'] = ['n.s.' if p in lambda0 else '' 
for p in estimate['Parameter']] 
# estimate sigma2 using non-significant terms in lambda0 
sigma2 = 0 
for idx, row in estimate.iterrows(): 
p = row['Parameter'] 
if p not in lambda0: 
continue 
idx = int(idx) 
sigma2 += row['LSE']**2 * np.sum(psi3m[:, idx]**2) 
K0 = len(lambda0) 
sigma2 = sigma2 / K0 
print(f'K0 = {K0}') 
print(f'sigma2 = {sigma2.round(2)}') 
lambda0 : ['A2B', 'AB2', 'A2B2', 'A2C', 'A2BC', 'B2C', 'AB2C', 
'A2B2C', 'AC2', 'A2C2', 'BC2', 'ABC2', 'A2BC2', 'B2C2', 'AB2C2', 
'A2B2C2'] 
K0 = 16 
sigma2 = 95.95 
Using the estimate ModifyingAbove Above ModifyingAbove sigma squared With caret With caret ˆˆσ 2 we can now derive standard errors for our parameter 
estimates. The sum sigma summation Underscript j equals 1 Overscript 3 Superscript m Endscripts left parenthesis upper Psi Subscript j comma omega plus 1 Superscript left parenthesis 3 Super Superscript m Superscript right parenthesis Baseline right parenthesis squared3m
j=1((3m)
j,ω+1)2 is the sum of column omegaω of psi3m**2. Using

5.7
Full Factorial Experiments
191
Table 5.23 The LSE of the 
parameters of the 3 cubed33 system 
Parameter 
LSE
Signiﬁcance 
Mean
223.781 
A
44.917 
Asquared2
−1.843 
B
−42.494 
AB
−16.558 
Asquared2B
−1.897 
n.s. 
Bsquared2
6.557 
ABsquared2
1.942 
n.s. 
Asquared2Bsquared2
−0.171 
n.s. 
C
26.817 
AC
22.617 
Asquared2C
0.067 
n.s. 
BC
−3.908 
ABC
2.012 
Asquared2BC
1.121 
n.s. 
Bsquared2C
−0.708 
n.s. 
ABsquared2C
0.246 
n.s. 
Asquared2Bsquared2C
0.287 
n.s. 
Csquared2
−9.165 
ACsquared2
−4.833 
n.s. 
Asquared2Csquared2
0.209 
n.s. 
BCsquared2
2.803 
n.s. 
ABCsquared2
−0.879 
n.s. 
Asquared2BCsquared2
0.851 
n.s. 
Bsquared2Csquared2
−0.216 
n.s. 
ABsquared2Csquared2
0.288 
n.s. 
Asquared2Bsquared2Csquared2
0.059 
n.s. 
np.sum with axis=0 to calculate this sum, we get the variance and standard error 
as follows. 
n = len(psi3m) 
variance = sigma2 / (n * np.sum(psi3m**2, axis=0)) 
estimate['S.E.'] = np.sqrt(n * variance) 
The estimates of the standard errors (S.E.) in Table 5.23 use this estimate. If other 
parameters are assumed negligible, the standard error estimates will change.
black medium square■
Example 5.10 If not all possible interactions need to be studied, we can also use 
ordinary linear least squares to analyze the data from Example 5.9. 
The main effects and interaction plots in Fig. 5.8 as well as the combined main 
effects and interaction plot from Fig. 5.9 give an indication of the importance of 
quadratic terms and interactions.

192
5
Classical Design and Analysis of Experiments
Fig. 5.8 Main effects and interaction plot for 3 cubed33 design 
Fig. 5.9 Combined main effect and interaction plot for 3 cubed33 design. The grey circle relate to results 
at intermediate factor levels 
The main effects plot shows for A only a small deviation from linearity which 
means that the quadratic term upper A squaredA2 has only little importance. This is also reﬂected 
in the combined main effects plot. Here the grey circle for A is almost centered 
between the red circle and black square. If the position of the grey circle is off 
center like for B and even more so for C, quadratic components may be important. 
For the interaction plots, we analyze the parallelity of the lines. The interactions 
AB and AC demonstrate deviations from it while for BC, the lines deviation from 
parallelity is very small. From this, we expect that the interaction BC will be of less 
importance than AB and AC.

5.8
Blocking and Fractional Replications of 2m Factorial Designs
193
Table 5.24 Parameter 
estimates for STRESS.csv 
using a reduced model 
Coef.
Std.Err. 
t
P>|t| 
[0.025
0.975] 
232.681 
4.988
46.652 
0.000 
222.108 
243.255 
44.917 
2.309
19.454 
0.000 
40.022 
49.811 
−42.494 
2.309 
−18.405 
0.000 
−47.389 
−37.600 
26.817 
2.309
11.615 
0.000 
21.922 
31.711 
−16.558 
2.828
−5.856 
0.000 
−22.553 
−10.564 
22.617 
2.828
7.998 
0.000 
16.622 
28.611 
−3.908 
2.828
−1.382 
0.186 
−9.903
2.086 
2.012 
3.463
0.581 
0.569 
−5.329
9.354 
−5.528 
3.999
−1.382 
0.186 
−14.005
2.950 
19.672 
3.999
4.919 
0.000 
11.195 
28.150 
−27.494 
3.999
−6.875 
0.000 
−35.972 
−19.017 
To analyze this data quantitatively with Python we apply: 
stress = mistat.load_data('STRESS') 
# convert factor levels from (0,1,2) to (-1,0,1) 
stress['A'] = stress['A'] - 1 
stress['B'] = stress['B'] - 1 
stress['C'] = stress['C'] - 1 
# train a model including interactions and quadratic terms 
formula = ('stress ~ A + B + C + A:B  + A:C + B:C  +  A:B:C + ' +  
'I(A**2) + I(B**2) + I(C**2)') 
model = smf.ols(formula, data=stress).fit() 
model.summary2() 
As can be seen in Table 5.24, most components in the model are highly 
signiﬁcant. The interactions BC and ABC, as well as the quadratic component upper A squaredA2
are not signiﬁcant. 
black medium square■
5.8 
Blocking and Fractional Replications of 2m Factorial 
Designs 
Full factorial experiments with large number of factors might be impractical. For 
example, if there are m equals 12m = 12 factors, even at p equals 2p = 2 levels, the total number of 
treatment combinations is 2 Superscript 12 Baseline equals 4096212 = 4096. This size of an experiment is generally 
not necessary, because most of the high order interactions might be negligible and 
there is no need to estimate 4096 parameters. If only main effects and ﬁrst order 
interactions are considered, a priori of importance, while all the rest are believed to 
be negligible, we have to estimate and test only 1 plus 12 plus StartBinomialOrMatrix 12 Choose 2 EndBinomialOrMatrix equals 791 + 12 +
12
2

= 79 parameters. 
A fraction of the experiment, of size 2 Superscript 7 Baseline equals 12827 = 128 would be sufﬁcient. Such a fraction 
can be even replicated several times. The question is, how do we choose the fraction 
of the full factorial in such a way that desirable properties of orthogonality, equal

194
5
Classical Design and Analysis of Experiments
variances of estimators, etc. will be kept, and the parameters of interest will be 
estimable unbiasedly. 
The problem of fractioning the full factorial experiment arises also when the 
full factorial cannot be performed in one block, but several blocks are required 
to accommodate all the treatment conditions. For example, a 2 Superscript 525 experiment is 
designed, but only 8 equals 2 cubed8 = 23 treatment combinations can be performed in any given 
block (day, machine, etc.). We have to design the fractions that will be assigned to 
each block in such a way that, if there are signiﬁcant differences between the blocks, 
the block effects will not confound or obscure parameters of interest. We start with a 
simple illustration of the fractionization procedure, and the properties of the ensuing 
estimators. 
Consider 3 factors A, B, C at 2 levels. We wish to partition the 2 cubed equals 823 = 8 treatment 
combinations to two fractions of size 2 squared equals 422 = 4. Let  lamda Subscript i Baseline equals 0 comma 1λi = 0, 1 left parenthesis i equals 1 comma 2 comma 3 right parenthesis(i = 1, 2, 3) and 
let upper A Superscript lamda 1 Baseline upper B Superscript lamda 2 Baseline upper C Superscript lamda 3Aλ1Bλ2Cλ3 represent the 8 parameters. One way of representing the treatment 
combinations, when the number of factors is not large, is by using low case letters 
a comma b comma c comma midline horizontal ellipsisa, b, c, · · · . The letter a indicates that factor A is at the High level left parenthesis i 1 equals 1 right parenthesis(i1 = 1), similarly 
about other factors. The absence of a letter indicates that the corresponding factor is 
at Low level. The symbol (1) indicates that all levels are Low. Thus, the treatment 
combinations and the associated coefﬁcients c Subscript i j Superscript left parenthesis 2 cubed right parenthesisc(23)
ij
are shown in Table 5.25. 
Suppose now that the treatment combinations should be partitioned to two 
fractional replications (blocks) of size 4. We have to choose a parameter, called 
a deﬁning parameter, according to which the partition will be done. This deﬁning 
parameter is in a sense sacriﬁced. Since its effects will be either confounded with 
the block effects or inestimable if only one block of trials is performed. Thus, 
let us choose the parameter ABC, as a deﬁning parameter. Partition the treatment 
combinations to two blocks, according to the signs of the coefﬁcients corresponding 
to ABC. These are the products of the coefﬁcients in the A, B, and C columns. Thus, 
two blocks are obtained 
StartLayout 1st Row 1st Column upper B Subscript minus 2nd Column equals StartSet left parenthesis 1 right parenthesis comma a b comma a c comma b c EndSet comma 2nd Row 1st Column upper B Subscript plus 2nd Column equals StartSet a comma b comma c comma a b c EndSet period EndLayoutB−= {(1), ab, ac, bc},
B+ = {a, b, c, abc}.
Table 5.25 A 2 squared22 factorial
Main effects
Deﬁning parameter 
Treatments 
A 
B
C
ABC 
(1)
−1 
−1 
−1 
−1 
A
1 
−1 
−1
1 
B
−1
1 
−1
1 
AB
1
1 
−1 
−1 
C
−1 
−1
1
1 
AC
1 
−1
1 
−1 
BC
−1
1
1 
−1 
ABC
1
1
1
1

5.8
Blocking and Fractional Replications of 2m Factorial Designs
195
In Python, we can use the method fracfact from the pyDOE2 package. 
from pyDOE2 import fracfact 
# define the generator 
generator = 'A B C ABC' 
design = pd.DataFrame(fracfact(generator), columns=generator.split()) 
block_n = design[design['ABC'] == -1] 
block_p = design[design['ABC'] == 1] 
The two blocks are: 
Main effects
Deﬁning parameter 
Treatments 
A 
B
C
ABC 
(1)
−1 
−1 
−1 
−1 
AB
1
1 
−1 
−1 
AC
1 
−1
1 
−1 
BC
−1
1
1 
−1 
Main effects
Deﬁning parameter 
Treatments 
A 
B
C
ABC 
A
1 
−1 
−1
1 
B
−1
1 
−1
1 
C
−1 
−1
1
1 
ABC
1
1
1
1 
If 2 Superscript m2m treatment combinations are partitioned to 2 Superscript k Baseline equals 22k = 2 blocks, we say that 
the degree of fractionation is k equals 1k = 1, the fractional replication is of size 2 Superscript m minus k2m−k, 
and the design is 1 divided by 2 Superscript k1/2k fraction of a full factorial. If, for example, m equals 5m = 5 factors 
and we wish to partition to 4 blocks of 8, the degree of fractionization is k equals 2k = 2. 
Select k equals 2k = 2 parameters to serve as deﬁning parameters, e.g., ACE and BDE, and 
partition the treatment combinations according to the signs plus or minus 1±1 of the coefﬁcients in 
the ACE and BDE columns. This becomes very cumbersome if m and k are large. 
Function fac.design performs this partitioning and prints into a ﬁle the block which 
is requested. We will return to this later. It is interesting to check now what are the 
properties of estimators in the 2 Superscript 3 minus 123−1 fractional replication, if only the block upper B Subscript minusB−was 
performed. The deﬁning parameter was ABC. 
Let upper Y left parenthesis 1 right parenthesisY(1) be the response of treatment combination (1), this is upper Y 0Y0 in the standard 
order notation, let upper Y left parenthesis a right parenthesisY(a) be the response of ‘a’, etc. The results of performing upper B Subscript minusB−, 
with the associated coefﬁcients of parameters of interest can be presented as in 
Table 5.26. 
We see that the six columns of coefﬁcients are orthogonal to each other, and each 
column has 2 negative 1−1’s and 2 plus 1+1’s. The LSE of the above parameters are orthogonal 
contrasts, given by

196
5
Classical Design and Analysis of Experiments
Table 5.26 Coefﬁcients and 
response for several treatment 
combinations (t.c.) 
t.c. 
A
B
C
AB 
AC 
BC 
Y 
(1) 
−1 
−1 
−1
1
1
1 
upper Y left parenthesis 1 right parenthesisY(1)
ab
1
1 
−1
1 
−1 
−1 
upper Y left parenthesis a b right parenthesisY(ab)
ac
1 
−1
1 
−1
1 
−1 
upper Y left parenthesis a c right parenthesisY(ac)
bc −1
1
1 
−1 
−1
1 
upper Y left parenthesis b c right parenthesisY(bc)
StartLayout 1st Row 1st Column ModifyingAbove upper A With caret 2nd Column equals one fourth left parenthesis minus upper Y left parenthesis 1 right parenthesis plus upper Y left parenthesis a b right parenthesis plus upper Y left parenthesis a c right parenthesis minus upper Y left parenthesis b c right parenthesis right parenthesis comma 2nd Row 1st Column ModifyingAbove upper B With caret 2nd Column equals one fourth left parenthesis minus upper Y left parenthesis 1 right parenthesis plus upper Y left parenthesis a b right parenthesis minus upper Y left parenthesis a c right parenthesis plus upper Y left parenthesis b c right parenthesis right parenthesis comma EndLayout
ˆA = 1
4(−Y(1) + Y(ab) + Y(ac) −Y(bc)),
ˆB = 1
4(−Y(1) + Y(ab) −Y(ac) + Y(bc)),
etc. The variances of all these estimators, when n equals 1n = 1, are equal to StartFraction sigma squared Over 4 EndFractionσ 2
4 . However, 
the estimators might be biased. The expected value of the ﬁrst estimator is 
upper E left brace ModifyingAbove upper A With caret right brace equals one fourth left parenthesis minus upper E left brace upper Y left parenthesis 1 right parenthesis right brace plus upper E left brace upper Y left parenthesis a b right parenthesis right brace plus upper E left brace upper Y left parenthesis a c right parenthesis right brace minus upper E left brace upper Y left parenthesis b c right parenthesis right brace right parenthesis periodE{ ˆA} = 1
4(−E{Y(1)} + E{Y(ab)} + E{Y(ac)} −E{Y(bc)}).
Now, 
StartLayout 1st Row 1st Column upper E left brace upper Y left parenthesis 1 right parenthesis right brace 2nd Column equals mu minus upper A minus upper B minus upper C plus upper A upper B plus upper A upper C plus upper B upper C minus upper A upper B upper C comma 2nd Row 1st Column upper E left brace upper Y left parenthesis a b right parenthesis right brace 2nd Column equals mu plus upper A plus upper B minus upper C plus upper A upper B minus upper A upper C minus upper B upper C minus upper A upper B upper C comma 3rd Row 1st Column upper E left brace upper Y left parenthesis a c right parenthesis right brace 2nd Column equals mu plus upper A minus upper B plus upper C minus upper A upper B plus upper A upper C minus upper B upper C minus upper A upper B upper C comma EndLayout
E{Y(1)} = μ −A −B −C + AB + AC + BC −ABC,
E{Y(ab)} = μ + A + B −C + AB −AC −BC −ABC,
E{Y(ac)} = μ + A −B + C −AB + AC −BC −ABC,
and 
upper E left brace upper Y left parenthesis b c right parenthesis right brace equals mu minus upper A plus upper B plus upper C minus upper A upper B minus upper A upper C plus upper B upper C minus upper A upper B upper C periodE{Y(bc)} = μ −A + B + C −AB −AC + BC −ABC.
Collecting all these terms, the result is 
upper E left brace ModifyingAbove upper A With caret right brace equals upper A minus upper B upper C periodE{ ˆA} = A −BC.
Similarly, one can show that 
StartLayout 1st Row 1st Column upper E left brace ModifyingAbove upper B With caret right brace 2nd Column equals upper B minus upper A upper C comma 2nd Row 1st Column upper E left brace ModifyingAbove upper C With caret right brace 2nd Column equals upper C minus upper A upper B comma 3rd Row 1st Column upper E left brace ModifyingAbove upper A upper B With caret right brace 2nd Column equals upper A upper B minus upper C comma EndLayout
E{ ˆB} = B −AC,
E{ ˆC} = C −AB,
E{ ˆ
AB} = AB −C,
etc. The LSE of all the parameters are biased, unless upper A upper B equals upper A upper C equals upper B upper C equals 0AB = AC = BC = 0. The  
bias terms are called aliases. The aliases are obtained by multiplying the parameter 
of interest by the deﬁning parameter, when any letter raised to the power 2 is 
eliminated, e.g.,

5.8
Blocking and Fractional Replications of 2m Factorial Designs
197
upper A circled times upper A upper B upper C equals upper A squared upper B upper C equals upper B upper C periodA ⊗ABC = A2BC = BC.
The sign of the alias is the sign of the block. Since we have used the block upper B Subscript minusB−, 
all the aliases appear above with a negative sign. The general rules for ﬁnding the 
aliases in 2 Superscript m minus k2m−k designs is as follows. 
To obtain a 2 Superscript m minus k2m−k fractional replication one needs k deﬁning parameters. The 
multiplication operation of parameters was illustrated above. The k deﬁning param-
eters should be independent, in the sense that none can be obtained as a product 
of the other ones. Such independent deﬁning parameters are called generators. 
For example, to choose 4 deﬁning parameters, when the factors are A, B, C, D, E, 
F, G, H, choose ﬁrst two parameters, like ABCH and ABEFG. The product of 
these two is CEFGH. In the next step choose, for the third deﬁning parameter, any 
one which is different than StartSet upper A upper B upper C upper H comma upper A upper B upper E upper F upper G comma upper C upper E upper F upper G upper H EndSet{ABCH, ABEFG, CEFGH}. Suppose one chooses 
BDEFH. The three independent parameters ABCH, ABEFG, and BDEFH 
generate a subgroup of eight parameters, including the mean muμ. These are: 
StartLayout 1st Row 1st Column mu 2nd Column upper B upper D upper E upper F upper H 2nd Row 1st Column upper A upper B upper C upper H 2nd Column upper A upper C upper D upper E upper F 3rd Row 1st Column upper A upper B upper E upper F upper G 2nd Column upper A upper D upper G upper H 4th Row 1st Column upper C upper E upper F upper G upper H 2nd Column upper B upper C upper D upper G EndLayout
μ
BDEFH
ABCH
ACDEF
ABEFG ADGH
CEFGH
BCDG
The utility function mistat.subgroupOfDeﬁning can be used to enumerate the 
subgroup based on the deﬁning parameters. 
mistat.subgroupOfDefining(['ABCH', 'ABEFG', 'BDEFH']) 
['', 'ABCH', 'ABEFG', 'ACDEF', 'ADGH', 'BCDG', 'BDEFH', 'CEFGH'] 
Finally, to choose a fourth independent deﬁning parameter, one can choose any 
parameter which is not among the eight listed above. Suppose that the parameter 
BCEFH is chosen. Now we obtain a subgroup of 2 Superscript 4 Baseline equals 1624 = 16 deﬁning parameter, by 
adding to the eight listed above their products with BCEFH. Thus, this subgroup 
is 
StartLayout 1st Row 1st Column mu 2nd Column upper B upper C upper E upper F upper H 2nd Row 1st Column upper A upper B upper C upper H 2nd Column upper A upper E upper F 3rd Row 1st Column upper A upper B upper E upper F upper G 2nd Column upper A upper C upper G upper H 4th Row 1st Column upper C upper E upper F upper G upper H 2nd Column upper B upper G 5th Row 1st Column upper B upper D upper E upper F upper H 2nd Column upper C upper D 6th Row 1st Column upper A upper C upper D upper E upper F 2nd Column upper A upper B upper D upper H 7th Row 1st Column upper A upper D upper G upper H 2nd Column upper A upper B upper C upper D upper E upper F upper G 8th Row 1st Column upper B upper C upper D upper G 2nd Column upper D upper E upper F upper G upper H EndLayout
μ
BCEFH
ABCH
AEF
ABEFG
ACGH
CEFGH
BG
BDEFH
CD
ACDEF
ABDH
ADGH
ABCDEFG
BCDG
DEFGH
Notice that this subgroup includes, excluding the mean, two ﬁrst order interactions 
CD and BG. This shows that the choice of deﬁning parameters was not a good one.

198
5
Classical Design and Analysis of Experiments
Table 5.27 The aliases to the main effects in a 2 Superscript 8 minus 428−4 design, the generators are ABCH, 
ABEFG, BDEFH, and  BCEFH 
Main effects 
Aliases 
A
upper A upper B upper C upper D upper G comma upper A upper B upper C upper E upper F upper H comma upper A upper B upper D upper E upper F upper H comma upper A upper B upper G comma upper A upper C upper D comma upper A upper C upper E upper F upper G upper H comma upper A upper D upper E upper F upper G upper H commaABCDG, ABCEFH, ABDEFH, ABG, ACD, ACEFGH, ADEFGH,
upper B upper C upper D upper E upper F upper G comma upper B upper C upper H comma upper B upper D upper H comma upper B upper E upper F upper G comma upper C upper D upper E upper F comma upper C upper G upper H comma upper D upper G upper H comma bold upper E upper FBCDEFG, BCH, BDH, BEFG, CDEF, CGH, DGH, EF
B
upper A upper B upper C upper D upper E upper F comma upper A upper B upper C upper G upper H comma upper A upper B upper D upper G upper H comma upper A upper B upper E upper F comma upper A upper C upper D upper E upper F upper G comma upper A upper C upper H comma upper A upper D upper H commaABCDEF, ABCGH, ABDGH, ABEF, ACDEFG, ACH, ADH,
upper A upper E upper F upper G comma upper B upper C upper D comma upper B upper C upper E upper F upper G upper H comma upper B upper D upper E upper F upper G upper H comma upper C upper D upper G comma upper C upper E upper F upper H comma upper D upper E upper F upper H comma bold upper GAEFG, BCD, BCEFGH, BDEFGH, CDG, CEFH, DEFH, G
C
upper A upper B upper C upper D upper H comma upper A upper B upper C upper E upper F upper G comma upper A upper B upper D upper E upper F upper G comma upper A upper B upper H comma upper A upper C upper D upper G upper H comma upper A upper C upper E upper F comma upper A upper D upper E upper F commaABCDH, ABCEFG, ABDEFG, ABH, ACDGH, ACEF, ADEF,
upper A upper G upper H comma upper B upper C upper D upper E upper F upper H comma upper B upper C upper G comma upper B upper D upper G comma upper B upper E upper F upper H comma upper C upper D upper E upper F upper G upper H comma bold upper D comma upper E upper F upper G upper HAGH, BCDEFH, BCG, BDG, BEFH, CDEFGH, D, EFGH
D
upper A upper B upper C upper D upper H comma upper A upper B upper C upper E upper F upper G comma upper A upper B upper D upper E upper F upper G comma upper A upper B upper H comma upper A upper C upper D upper G upper H comma upper A upper C upper E upper F comma upper A upper D upper E upper F commaABCDH, ABCEFG, ABDEFG, ABH, ACDGH, ACEF, ADEF,
upper A upper G upper H comma upper B upper C upper D upper E upper F upper H comma upper B upper C upper G comma upper B upper D upper G comma upper B upper E upper F upper H comma upper C upper D upper E upper F upper G upper H comma bold upper C comma upper E upper F upper G upper HAGH, BCDEFH, BCG, BDG, BEFH, CDEFGH, C, EFGH
E
upper A upper B upper C upper D upper F upper G comma upper A upper B upper C upper E upper H comma upper A upper B upper D upper E upper H comma upper A upper B upper F upper G comma upper A upper C upper D upper F comma upper A upper C upper E upper G upper H comma upper A upper D upper E upper G upper H commaABCDFG, ABCEH, ABDEH, ABFG, ACDF, ACEGH, ADEGH,
bold upper A upper F comma upper B upper C upper D upper E upper G comma upper B upper C upper F upper H comma upper B upper D upper F upper H comma upper B upper E upper G comma upper C upper D upper E comma upper C upper F upper G upper H comma upper D upper F upper G upper HAF, BCDEG, BCFH, BDFH, BEG, CDE, CFGH, DFGH
F
upper A upper B upper C upper D upper E upper G comma upper A upper B upper C upper F upper H comma upper A upper B upper D upper F upper H comma upper A upper B upper E upper G comma upper A upper C upper D upper E comma upper A upper C upper F upper G upper H comma upper A upper D upper F upper G upper H commaABCDEG, ABCFH, ABDFH, ABEG, ACDE, ACFGH, ADFGH,
bold upper A upper E comma upper B upper C upper D upper F upper G comma upper B upper C upper E upper H comma upper B upper D upper E upper H comma upper B upper F upper G comma upper C upper D upper F comma upper C upper E upper G upper H comma upper D upper E upper G upper HAE, BCDFG, BCEH, BDEH, BFG, CDF, CEGH, DEGH
G
upper A upper B upper C upper D upper E upper F comma upper A upper B upper C upper G upper H comma upper A upper B upper D upper G upper H comma upper A upper B upper E upper F comma upper A upper C upper D upper E upper F upper G comma upper A upper C upper H comma upper A upper D upper H commaABCDEF, ABCGH, ABDGH, ABEF, ACDEFG, ACH, ADH,
upper A upper E upper F upper G comma bold upper B comma upper B upper C upper D comma upper B upper C upper E upper F upper G upper H comma upper B upper D upper E upper F upper G upper H comma upper C upper D upper G comma upper C upper E upper F upper H comma upper D upper E upper F upper HAEFG, B, BCD, BCEFGH, BDEFGH, CDG, CEFH, DEFH
H
upper A upper B upper C comma upper A upper B upper C upper D upper E upper F upper G upper H comma upper A upper B upper D comma upper A upper B upper E upper F upper G upper H comma upper A upper C upper D upper E upper F upper H comma upper A upper C upper G comma upper A upper D upper G commaABC, ABCDEFGH, ABD, ABEFGH, ACDEFH, ACG, ADG,
upper A upper E upper F upper H comma upper B upper C upper D upper G upper H comma upper B upper C upper E upper F comma upper B upper D upper E upper F comma upper B upper G upper H comma upper C upper D upper H comma upper C upper E upper F upper G comma upper D upper E upper F upper GAEFH, BCDGH, BCEF, BDEF, BGH, CDH, CEFG, DEFG
Since the aliases which will be created by these deﬁning parameters will include 
main effects and other low order interactions. 
Given a subgroup of deﬁning parameters, the aliases of a given parameter are 
obtained by multiplying the parameter by the deﬁning parameters. In Table 5.27 
we list the aliases of the eight main effects, with respect to the above subgroup of 
2 Superscript 424 deﬁning parameters. 
We see in this table that most of the aliases to the main effects are high order 
interactions (that are generally negligible). However, among the aliases to A there 
is EF. Among the aliases to B there is the main effect G. Among the aliases to C 
there is D, etc. This design is not good since it may yield strongly biased estimators. 
The resolution of a 2 Superscript m minus k2m−k design is the length of the smallest word (excluding muμ) 
in the subgroup of deﬁning parameters. For example, if in a 2 Superscript 8 minus 428−4 design we use 
the following four generators BCDE, ACDF, ABCG, and ABDH, we obtain the 
16 deﬁning parameters 
StartLayout 1st Row left brace mu comma upper B upper C upper D upper E comma upper A upper C upper D upper F comma upper A upper B upper E upper F comma upper A upper B upper C upper G comma upper A upper D upper E upper G comma upper B upper D upper F upper G comma upper C upper E upper F upper G comma upper A upper B upper D upper H comma 2nd Row upper A upper C upper E upper H comma upper B upper D upper F upper H comma upper D upper E upper F upper H comma upper C upper D upper G upper H comma upper B upper E upper G upper H comma upper A upper F upper G upper H comma upper A upper B upper C upper D upper E upper F upper G upper H right brace period EndLayout{μ, BCDE, ACDF, ABEF, ABCG, ADEG, BDFG, CEFG, ABDH,
ACEH, BDFH, DEFH, CDGH, BEGH, AFGH, ABCDEFGH}.
The length of the smallest word, excluding muμ, among these deﬁning parameters is 
four. Thus the present 2 Superscript 8 minus 428−4 design is a resolution IV design. In this design, all 
aliases of main effects are second order interactions or higher (words of length 
greater or equal to three). Aliases to ﬁrst order interactions are interactions of ﬁrst 
order or higher. The present design is obviously better, in terms of resolution, than

5.8
Blocking and Fractional Replications of 2m Factorial Designs
199
the previous one (which is of resolution II). We should always try to get resolution 
IV or higher. If the degree of fractionation is too high there may not exist resolution 
IV designs. For example, in 2 Superscript 6 minus 326−3 and 2 Superscript 7 minus 427−4, 2 Superscript 9 minus 529−5, 2 Superscript 10 minus 6210−6 and 2 Superscript 11 minus 7211−7 we have only 
resolution III designs. One way to reduce the bias is to choose several fractions at 
random. For example, in a 2 Superscript 11 minus 7211−7 we have 2 Superscript 7 Baseline equals 12827 = 128 blocks of size 2 Superscript 4 Baseline equals 1624 = 16. If we  
execute only one block, the best we can have is resolution III. In this case some main 
effects are biased (confounded) with some ﬁrst order interactions. If one chooses n 
blocks at random (RSWOR) out of the 128 possible ones, and compute the average 
estimate of the effects, the bias is reduced to zero, but the variance of the estimators 
is increased. 
To illustrate this, suppose that we have a 2 Superscript 6 minus 226−2 design with generators ABCE 
and BCDF. This will yield a resolution IV design. There are 4 blocks and the 
corresponding bias terms of the LSE of A are 
StartLayout 1st Row 1st Column Underine block 2nd Column Blank 2nd Row 1st Column 0 2nd Column minus upper B upper C upper E minus upper A upper B upper C upper D upper F plus upper D upper E upper F 3rd Row 1st Column 1 2nd Column upper B upper C upper E minus upper A upper B upper C upper D upper F minus upper D upper E upper F 4th Row 1st Column 2 2nd Column minus upper B upper C upper E plus upper A upper B upper C upper D upper F minus upper D upper E upper F 5th Row 1st Column 3 2nd Column upper B upper C upper E plus upper A upper B upper C upper D upper F plus upper D upper E upper F EndLayout
block
0
−BCE −ABCDF + DEF
1
BCE −ABCDF −DEF
2
−BCE + ABCDF −DEF
3
BCE + ABCDF + DEF
If we choose one block at random, the expected bias is the average of the four terms 
above, which is zero. The total variance of ModifyingAbove upper A With caret ˆA is StartFraction sigma squared Over 16 EndFraction plus σ 2
16 + Variance of conditional bias 
equals StartFraction sigma squared Over 16 EndFraction plus left bracket left parenthesis upper B upper C upper E right parenthesis squared plus left parenthesis upper A upper B upper C upper D upper F right parenthesis squared plus left parenthesis upper D upper E upper F right parenthesis squared right bracket divided by 4= σ 2
16 + [(BCE)2 + (ABCDF)2 + (DEF)2]/4. 
Example 5.11 In the present example we illustrate the construction of fractional 
replications. The case that is illustrated is a 2 Superscript 8 minus 428−4 design. Here we can construct 
16 fractions, each one of size 16. As discussed before, four generating parameters 
should be speciﬁed. Let these be BCDE, ACDF, ABCG, ABDH. These 
parameters generate resolution 4 design where the degree of fractionation, k=4. 
The blocks can be indexed 0 comma 1 comma midline horizontal ellipsis comma 150, 1, · · · , 15. Each index is determined by the signs 
of the four generators, which determine the block. Thus, the signs left parenthesis negative 1 comma negative 1 comma 1 comma 1 right parenthesis(−1, −1, 1, 1)
correspond to left parenthesis 0 comma 0 comma 1 comma 1 right parenthesis(0, 0, 1, 1) which yields the index sigma summation Underscript j equals 1 Overscript 4 Endscripts i Subscript j Baseline 2 Superscript j minus 1 Baseline equals 124
j=1 ij2j−1 = 12. 
The index of generator 1 (upper B upper C upper D upper E equals upper A Superscript 0 Baseline upper B Superscript 1 Baseline upper C Superscript 1 Baseline upper D Superscript 1 Baseline upper E Superscript 1 Baseline upper F Superscript 0 Baseline upper G Superscript 0 Baseline upper H Superscript 0BCDE = A0B1C1D1E1F 0G0H 0) is  0 comma 1 comma 1 comma 1 comma 1 comma0, 1, 1, 1, 1,
0 comma 0 comma 00, 0, 0, for generator 2: 1 comma 0 comma 1 comma 1 comma 0 comma 1 comma 0 comma 01, 0, 1, 1, 0, 1, 0, 0; for generator 3: 1 comma 1 comma 1 comma 0 comma 0 comma 0 comma 1 comma 01, 1, 1, 0, 0, 0, 1, 0 and 
for generator 4: 1 comma 1 comma 0 comma 1 comma 0 comma 0 comma 0 comma 11, 1, 0, 1, 0, 0, 0, 1. 
We can generate the ﬁrst block in Python as follows: 
mainEffects = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] 
defining = ['ABCH', 'ABEFG', 'BDEFH', 'BCEFH'] 
design = pd.DataFrame(fracfact(' '.join(mainEffects)), columns=mainEffects) 
design = mistat.addTreatments(design, mainEffects) 
subgroup = mistat.subgroupOfDefining(defining, noTreatment='(1)') 
block1 = design[design['Treatments'].isin(subgroup)] 
block1 
Treatments
A
B
C
D
E
F
G
H 
0
(1) -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 
12
CD -1.0 -1.0 
1.0 
1.0 -1.0 -1.0 -1.0 -1.0

200
5
Classical Design and Analysis of Experiments
49
AEF 
1.0 -1.0 -1.0 -1.0 
1.0 
1.0 -1.0 -1.0 
61
ACDEF 
1.0 -1.0 
1.0 
1.0 
1.0 
1.0 -1.0 -1.0 
66
BG -1.0 
1.0 -1.0 -1.0 -1.0 -1.0 
1.0 -1.0 
78
BCDG -1.0 
1.0 
1.0 
1.0 -1.0 -1.0 
1.0 -1.0 
115
ABEFG 
1.0 
1.0 -1.0 -1.0 
1.0 
1.0 
1.0 -1.0 
127
ABCDEFG 
1.0 
1.0 
1.0 
1.0 
1.0 
1.0 
1.0 -1.0 
135
ABCH 
1.0 
1.0 
1.0 -1.0 -1.0 -1.0 -1.0 
1.0 
139
ABDH 
1.0 
1.0 -1.0 
1.0 -1.0 -1.0 -1.0 
1.0 
182
BCEFH -1.0 
1.0 
1.0 -1.0 
1.0 
1.0 -1.0 
1.0 
186
BDEFH -1.0 
1.0 -1.0 
1.0 
1.0 
1.0 -1.0 
1.0 
197
ACGH 
1.0 -1.0 
1.0 -1.0 -1.0 -1.0 
1.0 
1.0 
201
ADGH 
1.0 -1.0 -1.0 
1.0 -1.0 -1.0 
1.0 
1.0 
244
CEFGH -1.0 -1.0 
1.0 -1.0 
1.0 
1.0 
1.0 
1.0 
248
DEFGH -1.0 -1.0 -1.0 
1.0 
1.0 
1.0 
1.0 
1.0 
In Table 5.28 two blocks derived with Python are printed. Use the com-
mand fold(block1[mainEffects], columns=[1, 2, 3]) to gener-
ate Block 1. 
black medium square■
In Box et al. (2005) there are recommended generators for 2 Superscript m minus k2m−k designs. Some 
of these generators are given in Table 5.29. 
The LSE of the parameters is performed by writing ﬁrst the columns of 
coefﬁcients c Subscript i comma j Baseline equals plus or minus 1ci,j = ±1 corresponding to the design, multiplying the coefﬁcients 
by the Y values, and dividing by 2 Superscript m minus k2m−k. 
Table 5.28 Blocks of 2 Superscript 8 minus 428−4
designs 
Block 0
Block 1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
2 
2 
2 
1 
1 
1 
1 
1 
2 
2 
2 
2 
1 
1 
1 
1 
1 
1 
1 
2 
1 
1 
1 
2 
1 
2 
2 
1 
2 
1 
1 
2 
2 
1 
1 
1 
2 
1 
1 
2 
2 
1 
1 
2 
2 
1 
1 
2 
1 
2 
2 
2 
2 
1 
1 
2 
2 
2 
1 
1 
1 
2 
1 
2 
1 
1 
2 
1 
1 
2 
1 
2 
1 
1 
2 
2 
1 
2 
1 
2 
2 
2 
1 
2 
1 
2 
1 
1 
2 
1 
2 
1 
2 
2 
1 
1 
1 
2 
1 
1 
2 
2 
1 
1 
1 
2 
1 
2 
2 
2 
1 
1 
2 
1 
2 
2 
2 
2 
1 
2 
2 
1 
2 
1 
1 
1 
2 
2 
1 
2 
1 
1 
1 
1 
2 
2 
1 
2 
1 
2 
1 
1 
2 
2 
2 
1 
2 
2 
1 
1 
2 
1 
2 
2 
1 
1 
2 
1 
2 
1 
1 
1 
2 
1 
2 
1 
2 
1 
1 
1 
2 
2 
2 
1 
2 
1 
2 
2 
1 
2 
2 
1 
2 
1 
1 
2 
2 
1 
1 
2 
2 
1 
2 
1 
1 
1 
1 
2 
2 
1 
2 
1 
1 
2 
1 
2 
2 
1 
1 
2 
2 
2 
1 
2 
2 
2 
1 
1 
1 
1 
2 
2 
2 
2 
2 
2 
2 
1 
2 
2 
2 
2 
2 
2 
2 
2 
2 
2 
2 
2 
1 
1 
1 
2 
2 
2 
2

5.9
Exploration of Response Surfaces
201
Table 5.29 Some generators 
for 2 Superscript m minus k2m−k designs 
m 
k 
5
6
7
8 
1 
ABCDE 
ABCDEF 
ABCDEFG 
ABCDEFGH 
2 
ABD
ABCE
ABCDF
ABCDG 
ACE
BCDF
ABDEG
ABEFH 
3
ABD
ABCE
ABCF 
ACD
BCDF
ABDG 
BCF
ACDG
BCDEH 
4
ABD
BCDE 
ACE
ACDF 
BCF
ABCG 
ABCG
ABDH 
5.9 
Exploration of Response Surfaces 
The functional relationship between the yield variable Y and the experimental 
variables left parenthesis x 1 comma midline horizontal ellipsis comma x Subscript k Baseline right parenthesis(x1, · · · , xk) is modeled as 
upper Y equals f left parenthesis x 1 comma midline horizontal ellipsis comma x Subscript k Baseline right parenthesis plus e commaY = f (x1, · · · , xk) + e,
where e is a random variable with zero mean and a ﬁnite variance, sigma squaredσ 2. The  set  
of points left brace f left parenthesis x 1 comma midline horizontal ellipsis comma x Subscript k Baseline right parenthesis{f (x1, · · · , xk), x Subscript i Baseline element of upper D Subscript ixi ∈Di, i equals 1 comma midline horizontal ellipsis comma k right bracei = 1, · · · , k}, where left parenthesis upper D 1 comma midline horizontal ellipsis comma upper D Subscript k Baseline right parenthesis(D1, · · · , Dk) is the 
experimental domain of the x-variables, is called a response surface. Two types 
of response surfaces were discussed before, the linear 
f left parenthesis x 1 comma midline horizontal ellipsis comma x Subscript k Baseline right parenthesis equals beta 0 plus sigma summation Underscript i equals 1 Overscript k Endscripts beta Subscript i Baseline x Subscript if (x1, · · · , xk) = β0 +
k

i=1
βixi
(5.9.1) 
and the quadratic 
f left parenthesis x 1 comma midline horizontal ellipsis comma x Subscript k Baseline right parenthesis equals beta 0 plus sigma summation Underscript i equals 1 Overscript k Endscripts beta Subscript i Baseline x Subscript i Baseline plus sigma summation Underscript i equals 1 Overscript k Endscripts beta Subscript i i Baseline x Subscript i Superscript 2 Baseline plus sigma summation sigma summation Underscript i not equals j Endscripts beta Subscript i j Baseline x Subscript i Baseline x Subscript j Baseline periodf (x1, · · · , xk) = β0 +
k

i=1
βixi +
k

i=1
βiix2
i +
 
i̸=j
βijxixj.
(5.9.2) 
Response surfaces may be of complicated functional form. We assume here that in 
local domains of interest, they can be approximated by linear or quadratic models. 
Researchers are interested in studying, or exploring, the nature of response 
surfaces, in certain domains of interest, for the purpose of predicting future yield, 
and in particular for optimizing a process, by choosing the x-values to maximize (or 
minimize) the expected yield (or the expected loss). In the present section we present 
special designs for the exploration of quadratic surfaces, and for the determination 
of optimal domains (conditions). Designs for quadratic models are called second

202
5
Classical Design and Analysis of Experiments
order designs. We start with the theory of second order designs, and conclude with 
the optimization process. 
5.9.1 
Second Order Designs 
Second order designs are constructed in order to estimate the parameters of the 
quadratic response function 
upper E left brace upper Y right brace equals beta 0 plus sigma summation Underscript i equals 1 Overscript k Endscripts beta Subscript i Baseline x Subscript i Baseline plus sigma summation Underscript i equals 1 Overscript k Endscripts beta Subscript i i Baseline x Subscript i Superscript 2 Baseline plus sigma summation Underscript i equals 1 Overscript k minus 1 Endscripts sigma summation Underscript j equals i plus 1 Overscript k Endscripts beta Subscript i j Baseline x Subscript i Baseline x Subscript j Baseline periodE{Y} = β0 +
k

i=1
βixi +
k

i=1
βiix2
i +
k−1

i=1
k

j=i+1
βijxixj.
(5.9.3) 
In this case the number of regression coefﬁcients is p equals 1 plus 2 k plus StartBinomialOrMatrix k Choose 2 EndBinomialOrMatrixp = 1 + 2k +
k
2

. We will 
arrange the vector bold italic betaβ in the form 
bold italic beta prime equals left parenthesis beta 0 comma beta 11 comma midline horizontal ellipsis comma beta Subscript k k Baseline comma beta 1 comma midline horizontal ellipsis comma beta Subscript k Baseline comma beta 12 comma midline horizontal ellipsis comma beta Subscript 1 k Baseline comma beta 23 comma midline horizontal ellipsis comma beta Subscript 2 k Baseline comma midline horizontal ellipsis comma beta Subscript n minus 1 comma k Baseline right parenthesis periodβ′ = (β0, β11, · · · , βkk, β1, · · · , βk, β12, · · · , β1k, β23, · · · , β2k, · · · , βn−1,k).
Let N be the number of x-points. The design matrix takes the form 
left parenthesis upper X right parenthesis equals Start 5 By 10 Matrix 1st Row 1st Column 1 2nd Column x 11 squared 3rd Column midline horizontal ellipsis 4th Column x Subscript 1 k Superscript 2 Baseline 5th Column x 11 6th Column midline horizontal ellipsis 7th Column x Subscript 1 k Baseline 8th Column x 11 x 12 9th Column midline horizontal ellipsis 10th Column x Subscript 1 comma k minus 1 Baseline x Subscript 1 comma k Baseline 2nd Row 1st Column 1 2nd Column x 21 squared 3rd Column midline horizontal ellipsis 4th Column x Subscript 2 k Superscript 2 Baseline 5th Column x 21 6th Column midline horizontal ellipsis 7th Column x Subscript 2 k Baseline 8th Column x 21 x 22 9th Column midline horizontal ellipsis 10th Column x Subscript 2 comma k minus 1 Baseline x Subscript 2 comma k Baseline 3rd Row 1st Column 1 2nd Column x 31 squared 3rd Column midline horizontal ellipsis 4th Column x Subscript 3 k Superscript 2 Baseline 5th Column x 31 6th Column midline horizontal ellipsis 7th Column x Subscript 3 k Baseline 8th Column x 31 x 32 9th Column midline horizontal ellipsis 10th Column x Subscript 3 comma k minus 1 Baseline x Subscript 3 comma k Baseline 4th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column Blank 4th Column vertical ellipsis 5th Column vertical ellipsis 6th Column Blank 7th Column vertical ellipsis 8th Column vertical ellipsis 9th Column Blank 10th Column vertical ellipsis 5th Row 1st Column 1 2nd Column x Subscript upper N Baseline 1 Superscript 2 Baseline 3rd Column midline horizontal ellipsis 4th Column x Subscript upper N k Superscript 2 Baseline 5th Column x Subscript upper N Baseline 1 Baseline 6th Column midline horizontal ellipsis 7th Column x Subscript upper N k Baseline 8th Column x Subscript upper N Baseline 1 Baseline x Subscript upper N Baseline 2 Baseline 9th Column midline horizontal ellipsis 10th Column x Subscript upper N comma k minus 1 Baseline x Subscript upper N comma k EndMatrix(X) =
⎡
⎢⎢⎢⎢⎢⎣
1 x2
11 · · · x2
1k x11 · · · x1k x11x12 · · · x1,k−1x1,k
1 x2
21 · · · x2
2k x21 · · · x2k x21x22 · · · x2,k−1x2,k
1 x2
31 · · · x2
3k x31 · · · x3k x31x32 · · · x3,k−1x3,k
...
...
...
...
...
...
...
1 x2
N1 · · · x2
Nk xN1 · · · xNk xN1xN2 · · · xN,k−1xN,k
⎤
⎥⎥⎥⎥⎥⎦
Impose on the x-values the conditions: 
(i) 
.N
j=1 xji = 0, i equals 1 comma midline horizontal ellipsis comma ki = 1, · · · , k
(ii) 
.N
j=1 x3
ji = 0, i equals 1 comma midline horizontal ellipsis comma ki = 1, · · · , k
(iii) 
.N
j=1 x2
jixjl = 0, i not equals li ̸= l
(iv) 
.N
j=1 x2
ji = b, i equals 1 comma midline horizontal ellipsis comma ki = 1, · · · , k
(5.9.4) 
(v) 
.N
j=1 x2
jix2
jl = c, i not equals li ̸= l
(vi) 
.N
j=1 x4
ji = c + d. 
The matrix left parenthesis upper S right parenthesis equals left parenthesis upper X right parenthesis prime left parenthesis upper X right parenthesis(S) = (X)′(X) can be written in the form 
left parenthesis upper S right parenthesis equals Start 2 By 2 Matrix 1st Row 1st Column left parenthesis upper U right parenthesis 2nd Column 0 2nd Row 1st Column 0 2nd Column left parenthesis upper B right parenthesis EndMatrix comma(S) =
(U) 0
0
(B)

,
(5.9.5) 
where left parenthesis upper U right parenthesis(U) is the left parenthesis k plus 1 right parenthesis times left parenthesis k plus 1 right parenthesis(k + 1) × (k + 1) matrix

5.9
Exploration of Response Surfaces
203
left parenthesis upper U right parenthesis equals Start 2 By 2 Matrix 1st Row 1st Column upper N 2nd Column b bold 1 Subscript k Superscript prime Baseline 2nd Row 1st Column bold b bold 1 Subscript k Baseline 2nd Column d bold upper I Subscript k Baseline plus c bold upper J Subscript k EndMatrix(U) =
 N
b1′
k
b1k dIk + cJk

(5.9.6) 
and left parenthesis upper B right parenthesis(B) is a diagonal matrix of order StartFraction k left parenthesis k plus 1 right parenthesis Over 2 EndFractionk(k+1)
2
left parenthesis upper B right parenthesis equals Start 6 By 6 Matrix 1st Row 1st Column b 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 2nd Row 1st Column Blank 2nd Column down right diagonal ellipsis 3rd Column Blank 4th Column Blank 5th Column 0 6th Column Blank 3rd Row 1st Column Blank 2nd Column Blank 3rd Column b 4th Column Blank 5th Column Blank 6th Column Blank 4th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column c 5th Column Blank 6th Column Blank 5th Row 1st Column Blank 2nd Column 0 3rd Column Blank 4th Column Blank 5th Column down right diagonal ellipsis 6th Column Blank 6th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column c EndMatrix period(B) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
b
...
0
b
c
0
...
c
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(5.9.7) 
One can verify that 
left parenthesis upper U right parenthesis Superscript negative 1 Baseline equals Start 2 By 2 Matrix 1st Row 1st Column p 2nd Column q bold 1 Subscript k Superscript prime Baseline 2nd Row 1st Column q u a d q bold 1 Subscript k Baseline 2nd Column t upper I Subscript k Baseline plus s upper J Subscript k Baseline EndMatrix comma(U)−1 =
 p
q1′
k
q1k tIk + sJk

,
(5.9.8) 
where 
StartLayout 1st Row 1st Column p 2nd Column equals StartFraction d plus k c Over upper N left parenthesis d plus k c right parenthesis minus b squared k EndFraction comma 2nd Row 1st Column q 2nd Column equals minus StartFraction b Over upper N left parenthesis d plus k c right parenthesis minus b squared k EndFraction comma 3rd Row 1st Column t 2nd Column equals StartFraction 1 Over d EndFraction 4th Row 1st Column s 2nd Column equals StartFraction b squared minus upper N c Over d left bracket upper N left parenthesis d plus k c right parenthesis minus b squared k right bracket EndFraction period EndLayout
p =
d + kc
N(d + kc) −b2k ,
q = −
b
N(d + kc) −b2k ,
t = 1
d
s =
b2 −Nc
d[N(d + kc) −b2k].
(5.9.9) 
Notice that U is singular if upper N left parenthesis d plus k c right parenthesis equals b squared kN(d + kc) = b2k. We therefore say that the design is 
non-singular if 
upper N not equals StartFraction b squared k Over d plus k c EndFraction periodN ̸=
b2k
d + kc.
Furthermore, if upper N equals b squared divided by cN = b2/c then s equals 0s = 0. In this case the design is called orthogonal. 
Let bold x Superscript bold 0 prime Baseline equals left parenthesis x 1 Superscript 0 Baseline comma midline horizontal ellipsis comma x Subscript k Superscript 0 Baseline right parenthesisx0′ = (x0
1, · · · , x0
k) be a point in the experimental domain, and 
bold italic xi Superscript bold 0 bold italic prime Baseline equals left parenthesis 1 comma left parenthesis x 1 Superscript 0 Baseline right parenthesis squared comma midline horizontal ellipsis comma left parenthesis x Subscript k Superscript 0 Baseline right parenthesis squared comma x 1 Superscript 0 Baseline comma midline horizontal ellipsis comma x Subscript k Superscript 0 Baseline comma x 1 Superscript 0 Baseline x 2 Superscript 0 Baseline comma x 1 Superscript 0 Baseline x 3 Superscript 0 Baseline comma midline horizontal ellipsis comma x Subscript k minus 1 Superscript 0 Baseline x Subscript k Superscript 0 Baseline right parenthesis periodξ0′ = (1, (x0
1)2, · · · , (x0
k)2, x0
1, · · · , x0
k, x0
1x0
2, x0
1x0
3, · · · , x0
k−1x0
k).
The variance of the predicted response at bold x Superscript 0x0 is

204
5
Classical Design and Analysis of Experiments
upper V left brace ModifyingAbove upper Y With caret left parenthesis bold x Superscript 0 Baseline right parenthesis right brace equals sigma squared bold italic xi Superscript bold 0 bold italic prime Baseline Start 7 By 8 Matrix 1st Row 1st Column left parenthesis upper U right parenthesis Superscript negative 1 Baseline 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column bold 0 6th Column Blank 7th Column Blank 8th Column Blank 2nd Row 1st Column q u a d 2nd Column Blank 3rd Column b Superscript negative 1 Baseline 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column Blank 3rd Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column down right diagonal ellipsis 5th Column Blank 6th Column Blank 7th Column 0 8th Column Blank 4th Row 1st Column bold 0 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column b Superscript negative 1 Baseline 6th Column Blank 7th Column Blank 8th Column Blank 5th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column c Superscript negative 1 Baseline 7th Column Blank 8th Column Blank 6th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column 0 5th Column Blank 6th Column Blank 7th Column down right diagonal ellipsis 8th Column Blank 7th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column c Superscript negative 1 Baseline EndMatrix bold italic xi Superscript 0 Baseline periodV { ˆY(x0)} = σ 2ξ0′
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
(U)−1
0
b−1
...
0
0
b−1
c−1
0
...
c−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
ξ0.
(5.9.10) 
This can be expanded as follows. 
StartLayout 1st Row 1st Column upper V left brace ModifyingAbove upper Y With caret left parenthesis bold x Superscript 0 Baseline right parenthesis right brace 2nd Column equals sigma squared left bracket p plus StartFraction 1 Over b EndFraction sigma summation Underscript i equals 1 Overscript k Endscripts left parenthesis x Subscript i Superscript 0 Baseline right parenthesis squared plus left parenthesis t plus s right parenthesis sigma summation Underscript i equals 1 Overscript k Endscripts left parenthesis x Subscript i Superscript 0 Baseline right parenthesis Superscript 4 Baseline 2nd Row 1st Column Blank 2nd Column plus StartFraction 1 Over c EndFraction sigma summation sigma summation Underscript h less than j Endscripts left parenthesis x Subscript h Superscript 0 Baseline right parenthesis squared left parenthesis x Subscript j Superscript 0 Baseline right parenthesis squared plus 2 b sigma summation Underscript i equals 1 Overscript k Endscripts left parenthesis x Subscript i Superscript 0 Baseline right parenthesis squared 3rd Row 1st Column Blank 2nd Column plus 2 s sigma summation sigma summation Underscript h less than j Endscripts left parenthesis x Subscript h Superscript 0 Baseline right parenthesis squared left parenthesis x Subscript j Superscript 0 Baseline right parenthesis squared right bracket 4th Row 1st Column Blank 2nd Column equals sigma squared left bracket p plus rho squared left parenthesis 2 b plus StartFraction 1 Over b EndFraction right parenthesis plus left parenthesis t plus s right parenthesis sigma summation Underscript i equals 1 Overscript k Endscripts left parenthesis x Subscript i Superscript 0 Baseline right parenthesis Superscript 4 Baseline 5th Row 1st Column Blank 2nd Column plus 2 left parenthesis s plus StartFraction 1 Over 2 c EndFraction right parenthesis sigma summation sigma summation Underscript h less than j Endscripts left parenthesis x Subscript n Superscript 0 Baseline right parenthesis squared left parenthesis x Subscript j Superscript 0 Baseline right parenthesis squared right bracket comma EndLayout
V { ˆY(x0)} = σ 2

p + 1
b
k

i=1
(x0
i )2 + (t + s)
k

i=1
(x0
i )4
+ 1
c
 
h<j
(x0
h)2(x0
j )2 + 2b
k

i=1
(x0
i )2
+ 2s
 
h<j
(x0
h)2(x0
j )2
⎤
⎦
= σ 2

p + ρ2

2b + 1
b

+ (t + s)
k

i=1
(x0
i )4
+ 2

s + 1
2c
  
h<j
(x0
n)2(x0
j )2
⎤
⎦,
(5.9.11) 
where rho squared equals sigma summation Underscript i equals 1 Overscript k Endscripts left parenthesis x Subscript i Superscript 0 Baseline right parenthesis squaredρ2 = k
i=1(x0
i )2. Notice that 
rho Superscript 4 Baseline equals left parenthesis sigma summation Underscript i equals 1 Overscript k Endscripts left parenthesis x Subscript i Superscript 0 Baseline right parenthesis squared right parenthesis squared equals sigma summation Underscript i equals 1 Overscript k Endscripts left parenthesis x Subscript i Superscript 0 Baseline right parenthesis Superscript 4 Baseline plus 2 sigma summation sigma summation Underscript h less than j Endscripts left parenthesis x Subscript h Superscript 0 Baseline right parenthesis squared left parenthesis x Subscript j Superscript 0 Baseline right parenthesis squared periodρ4 =
 k

i=1
(x0
i )2
2
=
k

i=1
(x0
i )4 + 2
 
h<j
(x0
h)2(x0
j )2.
Thus, if d equals 2 cd = 2c then t plus s equals s plus StartFraction 1 Over 2 c EndFractiont + s = s + 1
2c and 
upper V left brace ModifyingAbove upper Y With caret left parenthesis bold x Superscript 0 Baseline right parenthesis right brace equals sigma squared left bracket p plus rho squared left parenthesis 2 b plus StartFraction 1 Over b EndFraction right parenthesis plus left parenthesis t plus s right parenthesis rho Superscript 4 Baseline right bracket periodV { ˆY(x0)} = σ 2

p + ρ2

2b + 1
b

+ (t + s)ρ4

.
(5.9.12) 
Such a design left parenthesis d equals 2 c right parenthesis(d = 2c) is called rotatable, since upper V left brace ModifyingAbove upper Y With caret left parenthesis bold x Superscript 0 Baseline right parenthesis right braceV { ˆY(x0)} is constant for all points 
bold x Superscript 0x0 on the circumference of a circle of radius rhoρ, centered at the origin.

5.9
Exploration of Response Surfaces
205
5.9.2 
Some Speciﬁc Second Order Designs 
5.9.2.1 
3k-Designs 
Consider a factorial design of k factors, each one at three levels negative 1 comma 0 comma 1−1, 0, 1. In this  
case the number of points is upper N equals 3 Superscript kN = 3k. Obviously sigma summation Underscript j equals 1 Overscript 3 Superscript k Baseline Endscripts x Subscript j i Baseline equals 03k
j=1 xji = 0 for all i equals 1 comma midline horizontal ellipsis comma ki = 1, · · · , k. 
Also sigma summation Underscript j equals 1 Overscript 3 Endscripts x Subscript j i Superscript 3 Baseline equals 03
j=1 x3
ji = 0 and sigma summation Underscript j equals 1 Overscript 3 Superscript k Baseline Endscripts x Subscript j i Superscript 2 Baseline x Subscript j k Baseline equals 03k
j=1 x2
jixjk = 0, i not equals ki ̸= k. 
StartLayout 1st Row 1st Column b 2nd Column equals sigma summation Underscript j equals 1 Overscript 3 Superscript k Baseline Endscripts x Subscript j i Superscript 2 Baseline equals two thirds 3 Superscript k Baseline equals 2 dot 3 Superscript k minus 1 Baseline period 2nd Row 1st Column c 2nd Column equals sigma summation Underscript j equals 1 Overscript 3 Superscript k Baseline Endscripts x Subscript j i Superscript 2 Baseline x Subscript j l Superscript 2 Baseline equals two thirds b equals 4 dot 3 Superscript k minus 2 Baseline period 3rd Row 1st Column c plus d 2nd Column equals b period EndLayout
b =
3k

j=1
x2
ji = 2
33k = 2 · 3k−1.
c =
3k

j=1
x2
jix2
jl = 2
3b = 4 · 3k−2.
c + d = b.
(5.9.13) 
Hence 
d equals 2 dot 3 Superscript k minus 1 Baseline minus 4 dot 3 Superscript k minus 2 Baseline equals 2 dot 3 Superscript k minus 2 Baseline periodd = 2 · 3k−1 −4 · 3k−2 = 2 · 3k−2.
(5.9.14) 
b squared equals 4 dot 3 Superscript 2 k minus 2b2 = 4 · 32k−2 and upper N dot c equals 3 Superscript k Baseline dot 4 dot 3 Superscript k minus 2 Baseline equals 4 dot 3 Superscript 2 k minus 2N · c = 3k · 4 · 3k−2 = 4 · 32k−2. Thus, upper N c equals b squaredNc = b2. The design is 
orthogonal. However, d not equals 2 dot cd ̸= 2 · c. Thus, the design is not rotatable. 
5.9.2.2 
Central Composite Designs 
A Central Composite Design is one in which we start with n Subscript c Baseline equals 2 Superscript knc = 2k points of a 
factorial design, in which each factor is at levels negative 1−1 and plus 1+1. To these points we add 
n Subscript a Baseline equals 2 kna = 2k axial points which are at a ﬁxed distance alphaα from the origin. These are the 
points 
left parenthesis plus or minus alpha comma 0 comma midline horizontal ellipsis comma 0 right parenthesis comma left parenthesis 0 comma plus or minus alpha comma 0 comma midline horizontal ellipsis comma 0 right parenthesis comma midline horizontal ellipsis comma left parenthesis 0 comma 0 comma midline horizontal ellipsis comma 0 comma plus or minus alpha right parenthesis period(±α, 0, · · · , 0), (0, ±α, 0, · · · , 0), · · · , (0, 0, · · · , 0, ±α).
Finally, put n 0n0 points at the origin. These n 0n0 observations yield an estimate of the 
variance sigma squaredσ 2. Thus, the total number of points is upper N equals 2 Superscript k Baseline plus 2 k plus n 0N = 2k +2k +n0. In such a design, 
StartLayout 1st Row 1st Column b 2nd Column equals 2 Superscript k Baseline plus 2 alpha squared comma 2nd Row 1st Column c 2nd Column equals 2 Superscript k Baseline comma 3rd Row 1st Column c plus d 2nd Column equals 2 Superscript k Baseline plus 2 alpha Superscript 4 Baseline comma or 4th Row 1st Column d 2nd Column equals 2 alpha Superscript 4 Baseline period EndLayout
b = 2k + 2α2,
c = 2k,
c + d = 2k + 2α4,
or
d = 2α4.
(5.9.15)

206
5
Classical Design and Analysis of Experiments
The rotatability condition is d equals 2 cd = 2c. Thus, the design is rotatable if 
alpha Superscript 4 Baseline equals 2 Superscript k Baseline or alpha equals 2 Superscript k divided by 4 Baseline periodα4 = 2k
or
α = 2k/4.
(5.9.16) 
For this reason, in central composite designs, with k equals 2k = 2 factors we use alpha equals StartRoot 2 EndRoot equals 1.414α =
√
2 = 1.414. For  k equals 3k = 3 factors we use alpha equals 2 Superscript 3 divided by 4 Baseline equals 1.6818α = 23/4 = 1.6818. For rotatability and 
orthogonality, the following should be satisﬁed 
n 0 plus 2 Superscript k Baseline equals StartFraction 4 alpha squared left parenthesis 2 Superscript k Baseline plus alpha squared right parenthesis Over 2 Superscript k Baseline EndFraction comman0 + 2k = 4α2(2k + α2)
2k
,
(5.9.17) 
and since alpha squared equals 2 Superscript k divided by 2α2 = 2k/2 (for rotatability) 
n 0 equals 4 left parenthesis 2 Superscript k divided by 2 Baseline plus 1 right parenthesis minus 2 Superscript k Baseline periodn0 = 4(2k/2 + 1) −2k.
(5.9.18) 
Thus, if k equals 2k = 2, the number of points at the origin is n 0 equals 8n0 = 8. For  k equals 4k = 4 we need 
n 0 equals 4n0 = 4 points at the origin. For k equals 3k = 3 there is no rotatability, since 4 left parenthesis 2 Superscript 3 divided by 2 Baseline plus 1 right parenthesis minus 8 equals 7.3134(23/2+1)−8 =
7.313. 
Example 5.12 In Example 5.7 the piston simulation experiment was considered. 
We tested there, via a 3 squared32 experiment, the effects of piston surface area (factor s) and 
the spring coefﬁcient (factor k). It was found that the effects of the piston surface 
area were signiﬁcant, while the spring coefﬁcient had no signiﬁcant effect on the 
cycle time. We will conduct now a Central Composite Design of four factors, in 
order to explore the response surface. The factors chosen are: Piston surface area 
(factor s); Initial gas volume (factor v0); spring coefﬁcient (factor k) and ﬁlling gas 
temperature (factor t0). 
The experiment is performed with the piston simulator designed to attain both 
orthogonality and rotatability. Since k equals 4k = 4, we have  alpha equals 2α = 2 and n 0 equals 4n0 = 4. The  
number of replications is n Subscript r Baseline equals 50nr = 50. We can create the design using the function 
central_composite from the mistat package (Table 5.30). 
factors = { 
's': [0.01, 0.015], 
'v0': [0.00625, 0.00875], 
'k': [2000, 4000], 
't0': [345, 355], 
} 
Design = doe.central_composite(factors, alpha='r', center=[4, 4]) 
Table 5.30 Factors and level in piston simulator experiment 
Factor
Code
Levels 
Piston surface area s
x 1x1
0.0075
0.01
0.0125
0.015
0.0175 
Initial gas volume v 0v0
x 2x2
0.0050
0.00625
0.0075
0.00875
0.0100 
Spring coefﬁcient k
x 3x3
1000
2000
3000
4000
5000 
Filling gas temperature t 0t0
x 4x4
340
345
350
355
360 
Code 
minus−2
minus−1
0
1
2

5.9
Exploration of Response Surfaces
207
Using the Design, run the piston simulation and aggregate the results by group. 
simulator = mistat.PistonSimulator(**Design, m=60, p0=110_000, t=296, 
n_replicate=50, seed=2) 
result = simulator.simulate() 
# calculate mean and std of response by group 
result = result.groupby(by='group') 
result = result.agg({'s': 'mean', 'v0': 'mean', 'k': 'mean', 
't0': 'mean', 'seconds':['mean','std']}) 
result.columns = ['s', 'v0', 'k', 't0', 'Ymean', 'Ystd'] 
We also deﬁne a number of variables that will help transforming between factor and 
code co-ordinates. 
# transformation between factors and code levels 
factor2x = {factor: f'x{i}' for i, factor in enumerate(factors, 1)} 
x2factor = {f'x{i}': factor for i, factor in enumerate(factors, 1)} 
center = {factor: 0.5 * (max(values) + min(values)) 
for factor, values in factors.items()} 
unit = {factor: 0.5 * (max(values) - min(values)) 
for factor, values in factors.items()} 
# define helper function to convert code co-ordinates to factor co-ordinates 
def toFactor(code, codeValue): 
''' convert code to factor co-ordinates ''' 
factor = x2factor[code] 
return center[factor] + codeValue * unit[factor] 
# add code levels to table 
for c in factors: 
result[factor2x[c]] = 
(result[c] - center[c]) / unit[c] 
The experimental design and response outcomes are presented in Table 5.31. 
Figure 5.10 presents the main effects plot for the above four factors. The piston 
surface area (x1) and the initial gas volume (x2) have similar main effects. The 
cycle time average is monotonically changing with increasing levels of these two 
factors. It decreases for the piston surface area and changes in the opposite direction 
for the initial gas volume. 
In the following table we present the results of regression analysis of the mean 
cycle time upper Y overbar¯Y on 14 predictors x 1 squaredx2
1, x 2 squaredx2
2, x 3 squaredx2
3, x 4 squaredx2
4, x 1x1, x 2x2, x 3x3, x 4x4, x 1 x 2x1x2, x 1 x 3x1x3, x 1 x 4x1x4, x 2 x 3x2x3, 
x 2 x 4x2x4, x 3 x 4x3x4, where x 1x1 corresponds to factor s, x 2x2 to factor v 0v0, x 3x3 to factor k and x 4x4
to factor t 0t0. The result of the regression analysis is summarized in Table 5.32. The  
contour lines for the mean cycle time, corresponding to x 2 equals x 4 equals 0x2 = x4 = 0, are shown in 
Fig. 5.11. 
We see in Table 5.32 that only factor s (x 1x1, piston surface area) has a signiﬁcant 
quadratic effect. Factor s (x 1x1), v 0v0 (x 2x2), and k (x 3x3) have signiﬁcant linear effects. The 
interaction effect of s with k (x 1 colon x 3x1 : x3) will also be added. Thus, the response surface 
can be approximated with a reduced model based on x 1x1, x 2x2, x 3x3, x 1 x 2x1x2, x 1 x 3x1x3 and x 1 squaredx2
1. 
regression analysis of the reduced model is shown in Table 5.32.
black medium square■

208
5 
Classical Design and Analysis of Experiments 
Table 5.31 The central 
composite design and the 
mean and standard deviations 
of cycle time 
Group 
x1 
x2 
x3 
x4 
Ymean 
Ystd 
1
minus−1 
minus−1 
minus−1 
minus−1 
0.056
0.0078 
2
0
minus−1 
minus−1 
minus−1 
0.037
0.0045 
3
minus−1 
1
minus−1 
minus−1 
0.083
0.0108 
4
0
1
minus−1 
minus−1 
0.054
0.0052 
5
minus−1 
minus−1 
1
minus−1 
0.066
0.0099 
6
0
minus−1 
1
minus−1 
0.041
0.0043 
7
minus−1 
1
1
minus−1 
0.103
0.0162 
8
0
1
1
minus−1 
0.062
0.0060 
9
minus−1 
minus−1 
minus−1 
1
0.052
0.0059 
10
0
minus−1 
minus−1 
1
0.037
0.0044 
11
minus−1 
1
minus−1 
1
0.084
0.0102 
12
0
1
minus−1 
1
0.055
0.0057 
13
minus−1 
minus−1 
1
1
0.067
0.0116 
14
0
minus−1 
1
1
0.043
0.0050 
15
minus−1 
1
1
1
0.103
0.0141 
16
0
1
1
1
0.064
0.0076 
17
0
0
0
0
0.060
0.0079 
18
0
0
0
0
0.061
0.0078 
19
0
0
0
0
0.060
0.0066 
20
0
0
0
0
0.059
0.0064 
21
minus−2 
0
0
0
0.107
0.0189 
22
1
0
0
0
0.043
0.0030 
23
0
minus−2 
0
0
0.036
0.0052 
24
0
2
0
0
0.083
0.0107 
25
0
0
minus−2 
0
0.050
0.0045 
26
0
0
2
0
0.068
0.0081 
27
0
0
0
minus−2 
0.060
0.0080 
28
0
0
0
2
0.061
0.0077 
29
0
0
0
0
0.059
0.0074 
30
0
0
0
0
0.059
0.0079 
31
0
0
0
0
0.060
0.0081 
32
0
0
0
0
0.061
0.0079 
Fig. 5.10 Main effects plot

Table 5.32 Python regression analysis of the full and reduced model 
formula = ('Ymean ~ (x1+x2+x3+x4)**2 + ' +  
'I(x1**2) + I(x2**2) + I(x3**2) + I(x4**2)') 
model = smf.ols(formula, data=result).fit() 
print(model.summary2()) 
Results: Ordinary least squares 
=================================================================== 
Model:
OLS
Adj. R-squared:
0.989 
Dependent Variable: Ymean
AIC:
-301.2069 
Date:
2022-12-02 21:28 BIC:
-279.2208 
No. Observations:
32
Log-Likelihood:
165.60 
Df Model:
14
F-statistic:
208.0 
Df Residuals:
17
Prob (F-statistic): 3.71e-16 
R-squared:
0.994
Scale:
3.5252e-06
--------------------------------------------------------------------
Coef.
Std.Err.
t
P>|t|
[0.025
0.975]
--------------------------------------------------------------------
Intercept
0.0599
0.0007
90.2640 
0.0000
0.0585
0.0613 
x1
-0.0146
0.0004
-38.1172 
0.0000
-0.0154
-0.0138 
x2
0.0126
0.0004
32.8412 
0.0000
0.0118
0.0134 
x3
0.0052
0.0004
13.4893 
0.0000
0.0044
0.0060 
x4
0.0001
0.0004
0.3034 
0.7653
-0.0007
0.0009 
x1:x2
-0.0033
0.0005
-7.0326 
0.0000
-0.0043
-0.0023 
x1:x3
-0.0023
0.0005
-4.8492 
0.0002
-0.0033
-0.0013 
x1:x4
0.0003
0.0005
0.6054 
0.5529
-0.0007
0.0013 
x2:x3
0.0013
0.0005
2.6971 
0.0153
0.0003
0.0023 
x2:x4
0.0003
0.0005
0.6170 
0.5454
-0.0007
0.0013 
x3:x4
0.0004
0.0005
0.8523 
0.4059
-0.0006
0.0014 
I(x1 ** 2)
0.0037
0.0003
10.5681 
0.0000
0.0029
0.0044 
I(x2 ** 2)
-0.0001
0.0003
-0.3867 
0.7038
-0.0009
0.0006 
I(x3 ** 2)
-0.0003
0.0003
-0.9792 
0.3412
-0.0011
0.0004 
I(x4 ** 2)
0.0001
0.0003
0.1551 
0.8785
-0.0007
0.0008
-------------------------------------------------------------------
Omnibus:
0.162
Durbin-Watson:
3.015 
Prob(Omnibus):
0.922
Jarque-Bera (JB):
0.093 
Skew:
0.108
Prob(JB):
0.954 
Kurtosis:
2.849
Condition No.:
4 
=================================================================== 
formula = ('Ymean ~ x1 + x2 + x3 + x1:x3 + x1:x2 + I(x1**2)') 
reduced_model = smf.ols(formula, data=result).fit() 
print(reduced_model.summary2()) 
Results: Ordinary least squares 
=================================================================== 
Model:
OLS
Adj. R-squared:
0.989 
Dependent Variable: Ymean
AIC:
-302.4490 
Date:
2022-12-02 21:28 BIC:
-292.1888 
No. Observations:
32
Log-Likelihood:
158.22 
Df Model:
6
F-statistic:
448.4 
Df Residuals:
25
Prob (F-statistic): 3.42e-24 
R-squared:
0.991
Scale:
3.8018e-06
--------------------------------------------------------------------
Coef.
Std.Err.
t
P>|t|
[0.025
0.975]
--------------------------------------------------------------------
Intercept
0.0596
0.0004 
136.6637 
0.0000
0.0587
0.0605 
x1
-0.0146
0.0004
-36.7047 
0.0000
-0.0154
-0.0138 
x2
0.0126
0.0004
31.6243 
0.0000
0.0118
0.0134 
x3
0.0052
0.0004
12.9895 
0.0000
0.0044
0.0060 
x1:x3
-0.0023
0.0005
-4.6695 
0.0001
-0.0033
-0.0013 
x1:x2
-0.0033
0.0005
-6.7720 
0.0000
-0.0043
-0.0023 
I(x1 ** 2)
0.0037
0.0004
10.3340 
0.0000
0.0029
0.0044
-------------------------------------------------------------------
Omnibus:
0.768
Durbin-Watson:
2.210 
Prob(Omnibus):
0.681
Jarque-Bera (JB):
0.817 
Skew:
-0.229
Prob(JB):
0.665 
Kurtosis:
2.365
Condition No.:
2 
=================================================================== 

210
5 
Classical Design and Analysis of Experiments 
Fig. 5.11 Contour lines of the response surface setting x 2 equals x 4 equals 0x2 = x4 = 0. Paths show minimization of 
the function using steepest descent (circles) and constrained optimization (squares) 
5.9.3 
Approaching the Region of the Optimal Yield 
Very often, the purpose for ﬁtting a response surface is to locate the levels of the 
factors, which yield optimal results. 
Initially one might be far from the optimal regions. A series of small experiments 
may be performed, in order to move toward the optimal region. Thus, we start with 
simple ﬁrst order experiments, like 2 Superscript k2k factorial, and ﬁt to the results a linear model 
of the form 
ModifyingAbove upper Y With caret equals b 0 plus b 1 x 1 plus midline horizontal ellipsis plus b Subscript k Baseline x Subscript k Baseline periodˆY = b0 + b1x1 + · · · +  bkxk. 
We wish to determine now a new point, bold italic xi Superscript asteriskξ∗ say, whose distance from the center of the 
1st stage experiment (say 0) is R, and with maximal (or minimal) predicted yield. 
The predicted yield at bold italic xi Superscript asteriskξ∗ is 
ModifyingAbove y With caret Superscript asterisk Baseline equals b 0 plus sigma summation Underscript i equals 1 Overscript k Endscripts b Subscript i Baseline xi Subscript i Superscript asterisk Baseline periodˆy∗ = b0 + 
k

i=1 
biξ∗
i . 
To ﬁnd bold italic xi Superscript asteriskξ∗ we differentiate the Lagrangian 
upper L equals b 0 plus sigma summation Underscript i equals 1 Overscript k Endscripts b Subscript i Baseline xi Subscript i Superscript asterisk Baseline plus lamda left parenthesis upper R squared minus sigma summation Underscript i equals 1 Overscript k Endscripts left parenthesis xi Subscript i Superscript asterisk Baseline right parenthesis squared right parenthesisL = b0 + 
k

i=1 
biξ∗
i + λ

R2 − 
k

i=1 
(ξ∗
i )2


5.9 
Exploration of Response Surfaces
211 
with respect to xi Subscript i Superscript asteriskξ∗
i left parenthesis i equals 1 comma midline horizontal ellipsis comma k right parenthesis(i = 1, · · ·  , k)  and lamdaλ. The solution is 
xi Subscript i Superscript asterisk Baseline equals upper R StartFraction b Subscript i Baseline Over StartRoot sigma summation Underscript i equals 1 Overscript k Endscripts b Subscript i Superscript 2 Baseline EndRoot EndFraction comma i equals 1 comma midline horizontal ellipsis comma k periodξ∗
i = R 
bi
k 
i=1 b2 
i 
,
i
 
= 1, · · ·  , k.  
The direction of the steepest accent (descent) is in the direction of the normal 
(perpendicular) to the contours of equal response. 
At the second stage we perform experiments at a few points along the direction 
of the steepest ascent (at upper R 1 comma upper R 2 comma midline horizontal ellipsis right parenthesisR1, R2, · · ·  ) until there is no further increase in the mean 
yield. We then enter the third stage, at which we perform a second order design, 
centered at a new region, where the optimal conditions seem to prevail. 
Example 5.13 Continuing with the full model of the previous example, we iter-
atively minimize the cycle time using steepest descent. At each iteration, we 
determining the gradient at the current position and making a step along its direction 
to get a new position. The mistat package provides the class ResponseSur-
faceMethod that calculates the gradient using the information from a second order 
model. 
# calculate gradient and create steps along gradient to descend 
codes = ['x1', 'x2', 'x3', 'x4'] 
rsm = mistat.ResponseSurfaceMethod(model, codes) 
distance = np.arange(0, 2.6, 0.5) 
direction = {code: np.zeros(len(distance)) for code in codes} 
x = pd.Series(np.zeros(4), index=codes) 
for step in range(len(distance)): 
for code in codes: 
direction[code][step] = x[code] 
gradient = rsm.gradient(x) 
x = x - 0.5  * gradient 
Alternatively, we can use the constrainedOptimization method which in addition 
to steepest descent, makes adjustments at each step to further optimize the function 
at the speciﬁed distance. 
path = rsm.constrainedOptimization((0, 0, 0, 0), distances=(0.5, 1, 1.5, 2, 2.5), 
maximize=False) 
steps = pd.DataFrame({x2factor[code]: toFactor(code, path[code]) 
for code in rsm.codes}) 
Table 5.33 gives the steps for steepest descent in factor co-ordinates and 
the predicted cycle time (yhat). The paths for steepest descent and constrained 
optimization are shown in Fig. 5.11. The additional optimization at each step with 
constrained optimization leads to faster minimization.
black medium square■
5.9.4 
Canonical Representation 
The quadratic response function 

212
5 
Classical Design and Analysis of Experiments 
Table 5.33 Path of steepest descent 
Distance 
x1
x2
x3
x4
s
v0
k
t0
yhat 
0.0
0.00 
0.00
0.00
0.00
0.0125 
0.0075 
3000 
350 
0.060 
0.5
0.37 
minus−0.32 
minus−0.13 
minus−0.00 
0.0134 
0.0071 
2870 
349 
0.051 
1.0
0.72 
minus−0.65 
minus−0.26 
minus−0.01 
0.0143 
0.0067 
2742 
349 
0.044 
1.5
1.05 
minus−1.00 
minus−0.38 
minus−0.01 
0.0151 
0.0062 
2616 
349 
0.039 
2.0
1.35 
minus−1.38 
minus−0.50 
minus−0.01 
0.0159 
0.0058 
2495 
349 
0.035 
2.5
1.62 
minus−1.78 
minus−0.62 
minus−0.01 
0.0166 
0.0053 
2379 
349 
0.033 
ModifyingAbove upper Y With caret equals b 0 plus sigma summation Underscript i equals 1 Overscript k Endscripts b Subscript i Baseline x Subscript i Baseline plus sigma summation Underscript i equals 1 Overscript k Endscripts b Subscript i i Baseline x Subscript i Superscript 2 Baseline plus 2 sigma summation sigma summation Underscript i less than j Endscripts b Subscript i j Baseline x Subscript i Baseline x Subscript j ˆY = b0 + 
k

i=1 
bixi + 
k

i=1 
biix2 
i + 2
 
i<j 
bijxixj
(5.9.19) 
can be written in the matrix form 
ModifyingAbove upper Y With caret equals b 0 plus bold b prime bold x plus bold x prime bold upper B bold x comma ˆY = b0 + b′x + x′Bx,
(5.9.20) 
where bold x prime equals left parenthesis x 1 comma midline horizontal ellipsis comma x Subscript k Baseline right parenthesisx′ = (x1, · · ·  , xk), bold b prime equals left parenthesis b 1 comma midline horizontal ellipsis comma b Subscript k Baseline right parenthesisb′ = (b1, · · ·  , bk) and 
bold upper B equals Start 4 By 4 Matrix 1st Row 1st Column b 11 2nd Column b 12 3rd Column midline horizontal ellipsis 4th Column b Subscript 1 k Baseline 2nd Row 1st Column b 12 2nd Column down right diagonal ellipsis 3rd Column Blank 4th Column vertical ellipsis 3rd Row 1st Column vertical ellipsis 2nd Column Blank 3rd Column down right diagonal ellipsis 4th Column vertical ellipsis 4th Row 1st Column b Subscript 1 k Baseline 2nd Column midline horizontal ellipsis 3rd Column midline horizontal ellipsis 4th Column b Subscript k k Baseline EndMatrix periodB = 
⎡ 
⎢ 
⎢ 
⎢ 
⎢ 
⎣ 
b11 b12 · · ·  b1k 
b12 
... 
... 
... 
... ... 
b1k · · ·  · · ·  bkk 
⎤ 
⎥ 
⎥ 
⎥ 
⎥ 
⎦ 
. 
Let bold nabla ModifyingAbove upper Y With caret∇ˆY be the gradient of ModifyingAbove upper Y With caretˆY, i.e., 
bold nabla ModifyingAbove upper Y With caret equals StartFraction partial differential Over partial differential bold x EndFraction ModifyingAbove upper Y With caret equals bold b plus 2 upper B bold x period∇ˆY = ∂ 
∂x 
ˆY = b + 2Bx. 
Let bold x Superscript 0x0 be a point at which the gradient is zero, namely 
bold x Superscript 0 Baseline equals minus one half bold upper B Superscript negative 1 Baseline bold b commax0 = −1 
2 B−1b,
(5.9.21) 
assuming that the matrix bold upper BB is non-singular. Making the transformation (change of 
origin to bold x Superscript 0 Baseline right parenthesisx0) bold z equals bold x minus bold x Superscript 0z = x − x0, we obtain 
StartLayout 1st Row 1st Column ModifyingAbove upper Y With caret 2nd Column equals b 0 plus left parenthesis bold x Superscript 0 Baseline plus bold z right parenthesis prime bold b plus left parenthesis bold x Superscript 0 Baseline plus bold z right parenthesis prime bold upper B left parenthesis bold x plus bold z right parenthesis 2nd Row 1st Column Blank 2nd Column equals ModifyingAbove upper Y With caret Subscript 0 Baseline plus bold z prime bold upper B bold z comma EndLayout
ˆY = b0 + (x0 + z)′b + (x0 + z)′B(x + z) 
= ˆY0 + z′Bz, 
(5.9.22) 
where ModifyingAbove upper Y With caret Subscript 0 Baseline equals b 0 plus bold b prime bold x Superscript 0 ˆY0 = b0 + b′x0. 

5.9 
Exploration of Response Surfaces
213 
The matrix B is real symmetric. Thus, there exists an orthogonal matrix bold upper HH, which 
consists of the normalized eigenvectors of B, such that 
bold upper H bold upper B bold upper H Superscript prime Baseline equals Start 3 By 3 Matrix 1st Row 1st Column lamda 1 2nd Column Blank 3rd Column Blank 2nd Row 1st Column 0 2nd Column down right diagonal ellipsis 3rd Column 0 3rd Row 1st Column Blank 2nd Column Blank 3rd Column lamda Subscript k Baseline EndMatrix commaHBH′ = 
⎛ 
⎜ 
⎝ 
λ1 
0 ... 0 
λk 
⎞ 
⎟ 
⎠ , 
where 
lamda Subscript iλi left parenthesis i equals 1 comma midline horizontal ellipsis comma k right parenthesis(i = 1, · · ·  , k)  are the eigenvalues of B. We make now a new 
transformation (rotation), namely 
bold w equals bold upper H bold upper Z periodw = HZ. 
Since H is orthogonal, bold z equals bold upper H prime bold wz = H′w and 
StartLayout 1st Row 1st Column bold z prime bold upper B bold z 2nd Column equals bold w prime bold upper H bold upper B bold upper H prime bold w 2nd Row 1st Column Blank 2nd Column equals sigma summation Underscript i equals 1 Overscript k Endscripts lamda Subscript i Baseline w Subscript i Superscript 2 Baseline period EndLayout
z′Bz = w′HBH′w 
= 
k

i=1 
λiw2 
i . 
In these new co-ordinates, 
ModifyingAbove upper Y With caret equals ModifyingAbove upper Y With caret Subscript 0 Baseline plus sigma summation Underscript i equals 1 Overscript k Endscripts lamda Subscript i Baseline w Subscript i Superscript 2 Baseline period ˆY = ˆY0 + 
k

i=1 
λiw2 
i .
(5.9.23) 
This representation of the quadratic surface is called the canonical form. We see  
immediately that if lamda Subscript i Baseline greater than 0λi > 0 for all i equals 1 comma midline horizontal ellipsis comma ki = 1, · · ·  , k, then ModifyingAbove upper Y With caret Subscript 0ˆY0 is a point of minimum. If  
lamda Subscript i Baseline less than 0λi < 0 for all i equals 1 comma midline horizontal ellipsis comma ki = 1, · · ·  , k  then ModifyingAbove upper Y With caret Subscript 0ˆY0 is a maximum. If some eigenvalues are positive 
and some are negative, then ModifyingAbove upper Y With caret Subscript 0ˆY0 is a saddle point. 
The following examples of second order equations are taken from Box et al. 
(2005). They are visualized in Fig. 5.12 using contour lines. 
Fig. 5.12 Contour lines of different response surfaces 

214
5 
Classical Design and Analysis of Experiments 
1. Simple maximum: 
StartLayout 1st Row 1st Column ModifyingAbove upper Y With caret 2nd Column equals 83.57 plus 9.39 x 1 plus 7.12 x 2 minus 7.44 x 1 squared minus 3.71 x 2 squared minus 5.80 x 1 x 2 2nd Row 1st Column Blank 2nd Column equals 87.69 minus 902 w 1 squared minus 2.13 w 2 squared period EndLayout
ˆY = 83.57 + 9.39x1 + 7.12x2 − 7.44x2 
1 − 3.71x2 
2 − 5.80x1x2 
= 87.69 − 902w2 
1 − 2.13w2 
2. 
2. Minimax: 
StartLayout 1st Row 1st Column ModifyingAbove upper Y With caret 2nd Column equals 84.29 plus 11.06 x 1 plus 4.05 x 2 minus 6.46 x 1 squared minus 0.43 x 2 squared minus 9.38 x 1 x 2 2nd Row 1st Column Blank 2nd Column equals 87.69 minus 9.02 w 1 squared plus 2.13 w 2 squared period EndLayout
ˆY = 84.29 + 11.06x1 + 4.05x2 − 6.46x2 
1 − 0.43x2 
2 − 9.38x1x2 
= 87.69 − 9.02w2 
1 + 2.13w2 
2. 
3. Stationary ridge: 
StartLayout 1st Row 1st Column ModifyingAbove upper Y With caret 2nd Column equals 83.93 plus 10.23 x 1 plus 5.59 x 2 minus 6.95 x 1 squared minus 2.07 x 2 squared minus 7.59 x 1 x 2 2nd Row 1st Column Blank 2nd Column equals 87.69 minus 9.02 w 1 squared plus 0.00 w 2 squared period EndLayout
ˆY = 83.93 + 10.23x1 + 5.59x2 − 6.95x2 
1 − 2.07x2 
2 − 7.59x1x2 
= 87.69 − 9.02w2 
1 + 0.00w2 
2. 
4. Rising ridge: 
StartLayout 1st Row 1st Column ModifyingAbove upper Y With caret 2nd Column equals 82.71 plus 8.80 x 1 plus 8.19 x 2 minus 6.95 x 1 squared minus 2.07 x 2 squared minus 7.59 x 1 x 2 2nd Row 1st Column Blank 2nd Column equals 87.69 minus 9.02 w 1 squared plus 2.97 w 2 period EndLayout
ˆY = 82.71 + 8.80x1 + 8.19x2 − 6.95x2 
1 − 2.07x2 
2 − 7.59x1x2 
= 87.69 − 9.02w2 
1 + 2.97w2. 
Example 5.14 Continuing with the model from Example 5.13, we identify the 
stationary point using the ResponseSurfaceMethod. 
codes = ['x1', 'x2', 'x3', 'x4'] 
rsm = mistat.ResponseSurfaceMethod(model, codes) 
stationary = rsm.stationary_point() 
factor_stationary = pd.Series({x2factor[code]: toFactor(code, d) 
for code, d in stationary.items()}) 
factor_stationary 
s
0.018796 
v0
0.009571 
k
1451.172868 
t0
317.713341 
dtype: float64 
Next, starting from the stationary point, we minimize the function in steps using 
constraint optimization. The path can be seen in Fig. 5.13. 
path = rsm.constrainedOptimization(rsm.stationary_point(), maximize=False, 
reverse=True) 
steps = pd.DataFrame({x2factor[code]: toFactor(code, path[code]) 
for code in rsm.codes}) 
steps.head() 

5.10 
Evaluating Designed Experiments
215 
Fig. 5.13 Minimizing the function starting from the stationary point 
s
v0
k
t0
 
0 
0.018796 
0.009571 
1451.172868 
317.713341 
1 
0.018337 
0.008990 
1441.344186 
317.749174 
2 
0.018461 
0.008589 
2055.556297 
317.775331 
3 
0.018293 
0.008099 
2357.674709 
317.806241 
4 
0.018126 
0.007608 
2659.852584 
317.837251 
black medium square■
5.10 
Evaluating Designed Experiments 
The classic approach to the design of experiments (DoE) introduced in this chapter 
includes orthogonal arrays with mixed level designs including full, fractional and 
central composite designs (see also Box et al. 2005; Nguyen and Pham 2016). 
With the availability of enhanced computational power, industrial statistics has seen 
major improvements in experimental design methodology including computer based 
experiments presented in Chap. 7. These developments permit to design optimal 
designs that account for constraints in the design space, for blocking and split 
plots to accommodate hard to change factors and for designing experiments with 
various optimality properties. The pioneers of optimal design include Kiefer (1959) 
who solved an approximate optimal design problem that consists in determining 
a probability measure over the design space, Wynn (1972) proposed to compute 
optimal designs with an exchange-based method and Zacks (1973) introduced 
optimal sequential designs. 
Optimal designs are constructed using several approaches. Their optimality is 
determined by different criteria that lead to different solutions. The D-optimality 
criterion minimizes the determinant of the covariance matrix of the model coef-
ﬁcient estimates, the information matrix upper X prime upper XX′X of the design. D-optimality aims 

216
5 
Classical Design and Analysis of Experiments 
at deriving precise estimates of effects, i.e., main effects, quadratic effects and 
interactions. This assumes a known prespeciﬁed model and is fully determined by 
the experimental design before the experiment is conducted. D-optimal designs 
are used in experiments conducted to test for signiﬁcance of effects in order 
to best interpret the ﬁtted model. Their main application is in designs aimed at 
distinguishing active from inert factors. A design is A-optimal if it minimizes 
the sum of the variances of the regression coefﬁcients, the trace of the inverse of 
the information matrix. This is another approach to obtain precise estimates of 
the effects. I-optimal designs minimize the average variance of prediction over 
the design space. If the primary experimental goal is to predict a response or 
determine regions in the design space where the response falls within an acceptable 
range, the I-optimality criterion is more appropriate than the D-optimality criterion. 
In these cases, precise prediction of the response takes precedence over precise 
estimation of the parameters. A related approach is G-optimal designs, which 
minimize the maximum prediction variance over the design region. The maximum 
entry in the diagonal of the hat matrix upper X Superscript negative 1 Baseline left parenthesis upper X prime upper X right parenthesis Superscript negative 1 Baseline upper XX−1(X′X)−1X. These designs are calculated 
using Monte Carlo experiments of the design space. The minimum G-aberration 
criterion is useful when selecting good, regular two-level fractional factorial designs 
and is used to discriminate among regular fractional factorial designs with the 
same resolution. Minimum aberration compares the frequency of aliases of regular 
designs at different levels. Regular designs with the smallest frequency of worst 
aliases are considered the best. The minimum G-aberration criterion can also handle 
irregular design spaces (Tang and Deng 1999; Goos 2011). 
When considering what method to use to generate an experimental design, 
we are reminded that experiments are conducted to produce information. This 
requires a managed transition from problem elicitation to data collection, data 
analysis, derivation and communication of ﬁndings. Eventually, this roadmap 
produces information (Kenett and Shmueli 2016). Experimental designs are used to 
generate the dataset used in a study used to derive information. Information quality 
(InfoQ) is different from data quality and analysis quality. A key requirement for 
determining InfoQ is the understanding the study goal and whether it is explanatory, 
predictive, or descriptive. An explanatory goal can be based on causal hypotheses 
or can seek causal answers. A predictive goal predicts future or new individual 
observations. A descriptive goal quantiﬁes an observed effect using a statistical 
or other approximation. In designing an experiment, the study goal should be 
accounted for. 
The deﬁnition of InfoQ consists of the utility (U) from the application of a 
statistical or data analytic model (f ) to a dataset (X) given the research goal (g). 
The assessment of InfoQ is performed by considering eight dimensions: 
1. Data resolution 
2. Data structure 
3. Data integration 
4. Temporal relevance 
5. Chronology of data and goal 

5.10 
Evaluating Designed Experiments
217 
6. Generalizability 
7. Operationalization 
8. Communication 
These eight dimensions, introduced in Chap. 1, should be considered when design-
ing an experiment. As an example, consider the piston simulator introduced in 
Chap. 2. The performance of the piston is measured by the cycle time of a full 
revolution, in seconds. The seven factors that potentially affect cycle time are: 
X1 : Piston weight m (30–60 Kg) 
X2 : Piston surface area s (0.005–0.2 msquared2) 
X3 : Initial gas volume v0 (0.002–0.01 mcubed3) 
X4 : Spring coefﬁcient k (1000–5000 N/m) 
X5 : Atmospheric pressure p0 (90,000–110,000 N/msquared2) 
X6 : Ambient temperature t (290–296 Superscript ring◦K) 
X7 : Gas temperature t0 (340–360 Superscript ring◦K) 
The levels of these factors (shown in parentheses) represent extremes on the 
operating range that cannot be exceeded. In designing experiments to learn the 
performance of the piston, we have a wide range of options as presented in this 
chapter and in Chaps. 6 and 7. To compare these experimental set ups we can invoke 
a set of tools including: 
• 
Cost (size of the experiment) 
• 
Correlations between estimates of main effects and interactions 
• 
Prediction precision throughout design space (fraction of design space plot) 
• 
Ability to assess curvature from quadratic terms 
• 
Prediction variance 
• 
Power 
These criteria provide a broad perspective of experimental design capabilities. The 
levels of these factors (shown in parentheses) represent extremes on the operating 
range that cannot be exceeded without affecting the smooth operation of the engine. 
When designing experiments to investigate the performance of the piston, one can 
consider various questions to help set up a goal. For example: 
• 
Do you want to accurately estimate the effect of the seven factors to derive 
engineering insights on the system’s design? 
• 
Do you want a good predictive model of piston performance to design an optimal 
control system? 
• 
Do you want to compare different models of the piston to assess the effect of 
engineering modiﬁcations? 
• 
What can you achieve with an experimental budget of 14 experimental runs? 
What about 16 runs? Is there a difference? 
Speciﬁcally, we compare here three designs of experiments with the piston sim-
ulator. These include experimental arrays with 35, 80, and 169 experimental runs. 

218
5 
Classical Design and Analysis of Experiments 
These designs are available as CUSTOMDESIGN_35, CUSTOMDESIGN_80, 
and CUSTOMDESIGN_169. 
design35 = mistat.load_data('CUSTOMDESIGN_35') 
design80 = mistat.load_data('CUSTOMDESIGN_80') 
design169 = mistat.load_data('CUSTOMDESIGN_169') 
The ﬁrst design with 35 runs was designed using a coordinate exchange algorithm 
with seven factors at two levels each. The second and third designs are central 
composite designs with 80 and 169 runs and seven factors at three levels and ﬁve 
levels, respectively. Table 5.34 shows the ﬁrst design. 
The correlations between estimates of main effects and interactions are deter-
mined by the experimental design. Figure 5.14 shows the correlation plots of the 
three designs with the seven factors listed in Table 5.34. The correlation plots 
show correlations between estimates of main effects and interactions. A correlation 
of one indicates full aliasing so that these paired estimates are indistinguishable. 
Independence of estimates is indicated by a correlation of zero. For the two level 
design with 35 runs we see that the main effects are estimated without confounding. 
The 80 runs central composite designs has confounded quadratic effects, the 169 
run experiments has no confounding and all main effects, two-way interactions and 
quadratic effects are estimable. 
The fraction of design space (FDS) plot shows the proportion of the design 
space for which the scaled prediction variance (SPV) falls below a speciﬁc value. 
FDS plots are constructed by sampling n values from throughout the design space 
and obtaining all corresponding SPVs, which are then ordered and plotted against 
the quantiles (Zahran et al. 2003). The idea is the closer the FDS for an SPV is to 
the minimum, the better the design. In this example, a high FDS produces a low 
prediction variance from models created after collecting data. Moreover, the ﬂatter 
the line, the more stable the SPV distribution for that design. In the example in 
Fig. 5.15, the increase of experimental runs from 35 to 80 and to 169 clearly reduces 
the SPV. 
The SPV is signiﬁcantly smaller for the 169 runs design which has no confound-
ing effects. The 80 runs design can estimate, without confounding main effects, all 
two-way interactions and some quadratic terms. For screening main effects, the 35 
runs design sufﬁces. Experimental design evaluation is based on different criteria, 
including experimental size, correlation plots and Fraction of Design Space plots. 
In this section we presented these criteria with examples. 
5.11 
Chapter Highlights 
The main concepts and deﬁnitions introduced in this chapter include: 
• 
Response variable 
• 
Controllable factor 

5.11 
Chapter Highlights
219 
Table 5.34 The 
experimental arrays with 35 
runs for the piston simulator 
m 
s
v0
k
p0
t0
t 
30 
0.020 
0.010 
1000 
0.0009 
360 
296 
60 
0.020 
0.002 
1000 
0.0011 
340 
296 
30 
0.020 
0.010 
5000 
0.0011 
360 
290 
30 
0.020 
0.010 
5000 
0.0009 
360 
296 
30 
0.005 
0.002 
1000 
0.0009 
340 
290 
60 
0.005 
0.010 
5000 
0.0009 
340 
296 
30 
0.005 
0.002 
1000 
0.0009 
360 
296 
60 
0.020 
0.002 
1000 
0.0011 
360 
290 
30 
0.020 
0.010 
1000 
0.0009 
360 
296 
60 
0.005 
0.010 
5000 
0.0009 
360 
290 
30 
0.005 
0.010 
1000 
0.0011 
340 
296 
60 
0.005 
0.010 
1000 
0.0011 
340 
296 
60 
0.005 
0.010 
5000 
0.0009 
360 
290 
60 
0.020 
0.002 
1000 
0.0009 
360 
290 
30 
0.005 
0.002 
5000 
0.0011 
340 
290 
60 
0.020 
0.002 
1000 
0.0011 
340 
296 
30 
0.005 
0.002 
5000 
0.0011 
340 
290 
30 
0.020 
0.010 
1000 
0.0009 
340 
290 
30 
0.005 
0.002 
5000 
0.0011 
360 
296 
30 
0.005 
0.002 
1000 
0.0009 
360 
296 
60 
0.005 
0.002 
5000 
0.0009 
360 
290 
30 
0.020 
0.010 
1000 
0.0009 
340 
290 
60 
0.005 
0.010 
1000 
0.0011 
360 
290 
60 
0.005 
0.010 
1000 
0.0011 
360 
290 
30 
0.005 
0.002 
1000 
0.0009 
340 
290 
60 
0.020 
0.002 
5000 
0.0009 
340 
290 
30 
0.020 
0.002 
5000 
0.0011 
360 
296 
60 
0.005 
0.010 
1000 
0.0011 
340 
296 
60 
0.020 
0.002 
5000 
0.0009 
340 
296 
60 
0.020 
0.002 
1000 
0.0011 
360 
290 
30 
0.005 
0.002 
5000 
0.0011 
360 
296 
60 
0.005 
0.010 
5000 
0.0009 
340 
296 
60 
0.020 
0.010 
5000 
0.0011 
360 
296 
30 
0.020 
0.010 
5000 
0.0011 
340 
290 
30 
0.020 
0.010 
5000 
0.0011 
340 
290 
• 
Factor level 
• 
Statistical model 
• 
Experimental array 
• 
Blocking 
• 
Randomization 
• 
Block designs 
• 
Main effects 

220
5 
Classical Design and Analysis of Experiments 
Fig. 5.14 Correlation plots of experimental arrays with 35, 80, and 169 experimental runs 
Fig. 5.15 Fraction of design space plot for the three designs 
• 
Interactions 
• 
Analysis of variance 
• 
Latin squares 
• 
Factorial designs 
• 
Fractional factorial designs 
• 
Experimental design evaluation 
• 
Experimental design optimality 
• 
Fraction of design space 
• 
Response surfaces 
• 
D optimality 
• 
I optimality 
• 
Path of steepest ascent 

5.12 
Exercises
221 
5.12 
Exercises 
Exercise 5.1 Describe a production process familiar to you, like baking of cakes, 
or manufacturing concrete. List the pertinent variables. What is (are) the response 
variable(s)? Classify the variables which affect the response to noise variables and 
control variables. How many levels would you consider for each variable? 
Exercise 5.2 Different types of adhesives are used in a lamination process, in 
manufacturing a computer card. The card is tested for bond strength. In addition to 
the type of adhesive, a factor which might inﬂuence the bond strength is the curing 
pressure (currently at 200 psi). Follow the basic steps of experimental design to set 
a possible experiment for testing the effects of adhesives and curing pressure on the 
bond strength. 
Exercise 5.3 Provide an example where blocking can reduce the variability of a 
product. 
Exercise 5.4 Three factors A, B, C are tested in a given experiment, designed to 
assess their effects on the response variable. Each factor is tested at 3 levels. List all 
the main effects and interactions. 
Exercise 5.5 Let x1, x2 be two quantitative factors and Y a response variable. A 
regression model Y = β0 + β1x1 + β2x2 + β12x1x2 + e is ﬁtted to the data. Explain 
why β12 can be used as an interaction parameter. 
Exercise 5.6 Consider the ISC values for times t1, t2 and t3 in dataset 
SOCELL.csv. Make a paired comparison for testing whether the mean ISC in 
time t2 is different from that in time t1, by using  a  t-test. 
Exercise 5.7 Use permutation_test from scipy to perform a randomization test 
for the differences in the ISC values of the solar cells in times t2 and t3 (dataset 
SOCELL.csv). 
Exercise 5.8 Box et al. (2005) give the results of four treatments A, B, C, D in 
penicillin manufacturing in ﬁve different blends (blocks) shown in Table 5.35. 
Perform an ANOVA to test whether there are signiﬁcant differences between the 
treatments or between the blends. 
Table 5.35 Result of four 
treatments in penicillin 
manufacturing 
Treatments 
Blends 
A
B
C
D 
1
89 
88 
97 
94 
2
84 
77 
92 
79 
3
81 
87 
87 
85 
4
87 
92 
89 
84 
5
79 
81 
80 
88 

222
5 
Classical Design and Analysis of Experiments 
Table 5.36 Results of 
treatments A, B, C, · · ·  , H  
for different treatments 
Block 
Treatments
Block 
Treatments 
1
A 
38 
B 
30 
15
D 
11 
G 
24 
2
C 
50 
D 
27 
16
F 
37 
H 
39 
3
E 
33 
F 
28 
17
A 
23 
F
40 
4
G 
62 
H 
30 
18
B 
20 
D 
14 
5
A 
37 
C 
25 
19
C 
18 
H 
10 
6
B 
38 
H 
52 
20
E 
22 
G 
52 
7
D 
89 
E 
89 
21
A 
66 
G 
67 
8
F 
27 
G 
75 
22
B 
23 
F
46 
9
A 
17 
D 
25 
23
C 
22 
E 
28 
10
B 
47 
G 
63 
24
D 
20 
H 
40 
11
C 
32 
F 
39 
25
A 
27 
H 
32 
12
E 
20 
H 
18 
26
B 
10 
E 
40 
13
A 
5
E 
15 
27
C 
32 
G 
33 
14
B 
45 
C 
38 
28
D 
18 
F
23 
Table 5.37 Compressive 
strengths of cement for 
different treatments 
Batches 
Days 
1
2
3
4 
1
A
B
C
D 
312 
299 
315 
290 
2
C
A
D
B 
295 
317 
313 
300 
3
B
D
A
C 
295 
298 
312 
315 
4
D
C
B
A 
313 
314 
299 
300 
Exercise 5.9 Eight treatments A, B, C, · · ·  , H  were tested in a BIBD of 28 blocks, 
k = 2 treatments per block, r = 7 and λ = 1. The results of the experiments are 
shown in Table 5.36 
Make an ANOVA to test the signiﬁcance of the block effects, treatment effects, If 
the treatment effects are signiﬁcant, make multiple comparisons of the treatments. 
Exercise 5.10 Four different methods of preparing concrete mixtures A, B, C, D 
were tested, these methods consisted of two different mixture ratios of cement to 
water and two blending duration). The four methods (treatments) were blocks in four 
batches and 4 days, according to a Latin square design. The concrete was poured 
to cubes and tested for compressive strength [Kg/cm2] after 7 days of storage in 
special rooms with 20 ◦C temperature and 50% relative humidity. The results are in 
Table 5.37 
Are the differences between the strength values of different treatments signiﬁ-
cant? [Perform the ANOVA.] 

5.12 
Exercises
223 
Table 5.38 Results of 32 
factorial experiment with 
n = 3 observations 
A1
A2
A3 
B1 
18.3 
17.9 
19.1 
17.9 
17.6 
19.0 
18.5 
16.2 
18.9 
B2 
20.5 
18.2 
22.1 
21.1 
19.5 
23.5 
20.7 
18.9 
22.9 
B3 
21.5 
20.1 
22.3 
21.7 
19.5 
23.5 
21.9 
18.9 
23.3 
Exercise 5.11 Repeat the experiments described in Example 5.7 at the low levels 
of factors m, v0, p0, t, and t0. Perform the ANOVA for the main effects and 
interaction of spring coefﬁcient k and piston weight m on the cycle time. Use a 
marginal interaction plot to visualize both main effects and interaction. Are your 
results different from those obtained in the example? 
Exercise 5.12 For the data from Exercise 5.11 compute the least squares estimates 
of the main effects on the means and on the standard deviations. 
Exercise 5.13 A 24 factorial experiment gave the following response values, 
arranged in standard order: 72, 60, 90, 80, 65, 60, 85, 80, 60, 50, 88, 82, 58, 50, 
84, 75. 
(i) Estimate all possible main effects. 
(ii) Estimate σ 2 under the assumption that all the interaction parameters are zero. 
(iii) Determine a conﬁdence interval for σ 2 at level of conﬁdence 0.99. 
Exercise 5.14 A 32 factorial experiment, with n = 3 replications, gave the 
observations in Table 5.38. 
Perform an ANOVA to test the main effects and interactions. Break the between 
treatments sum of squares to one degree of freedom components. Use the Scheffé 
Sα coefﬁcient to determine which effects are signiﬁcant. 
Exercise 5.15 Construct a 28−2 fractional replication, using the generators 
ABCDG and ABEFH. What is the resolution of this design? Write the aliases to 
the main effects, and to the ﬁrst order interactions with the factor A. 
Exercise 5.16 Consider a full factorial experiment of 26 = 64 runs. It is required 
to partition the runs to 8 blocks of 8. The parameters in the group of deﬁning 
parameters are confounded with the effects of blocks and are not estimable. Show 
which parameters are not estimable if the blocks are generated by ACE, ABEF, 
and ABCD. 
Exercise 5.17 A 22 factorial design is expanded by using 4 observations at 0. The 
design matrix and the response are in Table 5.39. 

224
5 
Classical Design and Analysis of Experiments 
Table 5.39 Design matrix 
and response of 22 factorial 
design 
X1 
X2 
Y 
−1 
−1 
55.8 
−1 
−1 
54.4 
1 
−1 
60.3 
1 
−1 
60.9 
−1
1 
63.9 
−1
1 
64.4 
1
1 
67.9 
1
1 
68.5 
0
0 
61.5 
0
0 
62.0 
0
0 
61.9 
0
0 
62.4 
Table 5.40 Design matrix 
and the response for a control 
composite design of 
Exercise 5.18 
X1 
X2
Y 
1.0 
0.000 
95.6 
0.5 
0.866 
77.9 
−0.5 
0.866 
76.2 
−1.0 
0
54.5 
−0.5 
−0.866 
63.9 
0.5 
−0.866 
79.1 
0
0
96.8 
0
0
94.8 
0
0
94.4 
(i) Fit a response function of the form: Y = β0 + β1X1 + β2X2 + β12X1X2 + e, 
and plot its contour lines. 
(ii) Estimate the variance σ 2 and test the goodness of ﬁt of this model. 
Exercise 5.18 Table 5.40 represents a design matrix and the response for a control 
composite design. 
(i) Estimate the response function and its stationary point. 
(ii) Plot contours of equal response, in two dimensions. 
(iii) Conduct an ANOVA. 

Chapter 6 
Quality by Design 
Preview Quality is largely determined by decisions made in the early planning 
phases of products and processes. A particularly powerful technique for making 
optimal design decisions is the statistically designed experiment introduced in 
Chap. 5. This chapter covers the basics of experimental designs in the context of 
engineering and economic optimization problems. Taguchi’s loss function, signal-
to-noise ratios, factorial models, and orthogonal arrays are discussed using case 
studies and simple examples. A special section is dedicated to the application 
of quality by design (QbD) in the pharmaceutical industry. QbD is supported 
internationally by the International Conference on Harmonization of Technical 
Requirements for Registration of Pharmaceuticals for Human Use (ICH) and by 
the Food and Drug Administration (FDA). 
Factorial designs discussed in the previous chapter were developed in the 1930s by 
R.A. Fisher and F. Yates in Rothamsted agricultural station in Britain. Fractional 
replications were developed in the 1940s by D. Finney, also in Rothamsted. 
After World War II, these experimental design methods were applied to industrial 
problems in the Imperial Chemical Laboratories (ICL) in Britain. The objective 
of agronomists is to ﬁnd treatment combinations that lead to maximal yield 
in agricultural product growth. The chemical engineer wishes to ﬁnd the right 
combinations of pressure, temperature, and other factors, which lead to a maximal 
amount of the product coming out of a reactor. The objective in manufacturing 
engineering is, on the other hand, to design the process so that the products will be 
as close as possible to some speciﬁed target, without much ﬂuctuations over time. 
Flaws in engineering design can cause severe problems over time, and here 
is an example. A small British electronics company called Encrypta designed 
an ingenious electronic seal for lorries and store rooms. Industrial versions of 
D-size batteries were used to drive the circuit and numeric display. Encrypta 
started to receive defective seals returned by customers. A failure mode analysis 
Supplementary Information The online version contains supplementary material available at 
(https://doi.org/10.1007/978-3-031-28482-3_6). 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3_6
225

226
6
Quality by Design
revealed that, when dropped on a hard surface, the batteries would heat up and 
cause a short circuit. Encrypta won 30,000 pounds in compensations from the 
batteries manufacturer and switched to Vidor batteries made by Fuji who passed 
the test. Encrypta found that the D-batteries failed on dropping because a cloth-like 
separation inside ruptures. This produced an active chemicals mix that discharged 
the battery. Fuji uses a tough, rolled separator that eliminates the problem Fox 
(1988). 
In this chapter, we discuss methods and tools compatible with advanced quality 
and industrial standards mentioned in Chap. 3. It applies to organizations that 
adopted a quality by design approach that requires high organizational maturity. 
An example of quality by design is the comprehensive quality engineering approach 
developed by the Japanese engineer Genichi Taguchi. Taguchi labeled his methodol-
ogy off-line quality control. The basic ideas of off-line quality control originated, 
while Taguchi was working at the Electrical Communications Laboratory (ECL) 
of the Nippon Telephone and Telegraph Company (NTT). Taguchi’s task was 
to help Japanese engineers develop high-quality products with raw materials of 
poor quality, outdated manufacturing equipment, and an acute shortage of skilled 
engineers. Central to his approach is the application of statistically designed 
experiments. Taguchi’s impact on Japan has expanded to a wide range of industries. 
He won the 1960 Deming prize for application of quality as well as three Deming 
prizes for publications on quality in 1951, 1953, and 1984. In 1959, the Japanese 
company NEC followed Taguchi’s methods and ran 402 such experiments. In 1976, 
Nippon Denso, which was a 20,000-employee company producing electrical parts 
for automobiles, reported to have run 2,700 experiments using the Taguchi off-line 
quality control method. Off-line quality control was ﬁrst applied in the West to 
integrated circuit manufacturing (see Phadke et al. 1983). Applications of off-line 
quality control range now from the design of automobiles, copiers, and electronic 
systems to cash-ﬂow optimization in banking, improvements in computer response 
times, and runway utilization in an airport. Another industry that has adopted quality 
by design, to ensure high quality and reduced inefﬁciencies, is the pharmaceutical 
industry. The chapter will cover both the Taguchi methods and the application of 
quality by design in pharmaceutical companies. Chapter 8 is about cybermanufac-
turing and digital twins, methods deployed in Industry 4.0 manufacturing. The next 
chapter, Chap. 7, is an introduction to computer experiments, in general. 
6.1 Off-Line Quality Control, Parameter Design, and the 
Taguchi Method 
Kackar (1985), Dehnad (1989), Phadke (1989), John (1990), Box et al. (1988), and 
others explain the Taguchi methodology for off-line experimentation. We provide 
here a concise summary of this approach. 
The performance of products or processes is typically quantiﬁed by performance 
measures. Examples include measures such as piston cycle time, yield of a produc-
tion process, output voltage of an electronic circuit, noise level of a compressor,

6.1
Off-Line Quality Control, Parameter Design, and the Taguchi Method
227
or response times of a computer system. These performance measures might be 
affected by several factors that have to be set at speciﬁc levels to get desired results. 
For example, the piston simulator introduced in previous chapters has seven factors 
that can be used to control the piston cycle time. The aim of off-line quality control 
is to determine the factor-level combination that gives the least variability to the 
appropriate performance measure, while keeping the mean value of the measure on 
target. The goal is to control both accuracy and variability. In the next section, we 
discuss an optimization strategy that solves this problem by minimizing various loss 
functions. 
6.1.1 
Product and Process Optimization Using Loss Functions 
Optimization problems of products or processes can take many forms that depend on 
the objectives to be reached. These objectives are typically derived from customer 
requirements. Performance parameters such as dimensions, pressure, or velocity 
usually have a target or nominal value. The objective is to reach the target within a 
range bounded by upper and lower speciﬁcation limits. We call such cases “nominal 
is best.” Noise levels, shrinkage factors, and amount of wear and deterioration are 
usually required to be as low as possible. We call such cases “the smaller the  
better.” When we measure strength, efﬁciency, yields, or time to failure, our goal 
is, in most cases, to reach the maximum possible levels. Such cases are called “the 
larger the better.” These three types of cases require different objective (target) 
functions to optimize. Taguchi introduced the concept of loss function to help 
determine the appropriate optimization procedure. 
When “nominal is best,” speciﬁcation limits are typically two-sided with an 
upper speciﬁcation limit (USL) and a lower speciﬁcation limit (LSL). These 
limits are used to differentiate between conforming and nonconforming products. 
Nonconforming products are usually ﬁxed, retested, and sometimes downgraded or 
simply scrapped. In all cases, defective products carry a loss to the manufacturer. 
Taguchi argues that only products on target should carry no loss. Any deviation 
carries a loss that is not always immediately perceived by the customer or production 
personnel. Taguchi proposes a quadratic function as a simple approximation to a 
graduated loss function that measures loss on a continuous scale. A quadratic loss 
function has the form 
upper L left parenthesis y comma upper M right parenthesis equals upper K left parenthesis y minus upper M right parenthesis squared commaL(y, M) = K(y −M)2,
(6.1.1) 
where y is the value of the performance characteristic of a product, M is the target 
value of this characteristic, and K is a positive constant, which yields monetary or 
other utility value to the loss. For example, suppose that left parenthesis upper M minus upper Delta comma upper M plus upper Delta right parenthesis(M −, M + ) is the 
customer’s tolerance interval around the target. When y falls out of this interval, 
the product has to be repaired or replaced at a cost of $A. Then, for this product,

228
6
Quality by Design
upper A equals upper K upper Delta squaredA = K2
(6.1.2) 
or 
upper K equals upper A divided by upper Delta squared periodK = A/2.
(6.1.3) 
The manufacturer’s tolerance interval is generally tighter than that of the 
customer, namely left parenthesis upper M minus delta comma upper M plus delta right parenthesis(M −δ, M + δ), where delta less than upper Deltaδ < . One can obtain the value of deltaδ
in the following manner. Suppose the cost to the manufacturer to repair a product 
that exceeds the customer’s tolerance, before shipping the product, is $B, upper B less than upper AB < A. 
Then 
upper B equals left parenthesis StartFraction upper A Over upper Delta squared EndFraction right parenthesis left parenthesis upper Y minus upper M right parenthesis squared commaB =
 A
2

(Y −M)2,
or 
upper Y equals upper M plus or minus upper Delta left parenthesis StartFraction upper B Over upper A EndFraction right parenthesis Superscript 1 divided by 2 Baseline periodY = M ± 
B
A
1/2
.
(6.1.4) 
Thus, 
delta equals upper Delta left parenthesis StartFraction upper B Over upper A EndFraction right parenthesis Superscript 1 divided by 2 Baseline periodδ = 
B
A
1/2
.
(6.1.5) 
The manufacturer should reduce the variability in the product performance charac-
teristic so that process capability upper C Subscript p kCpk for the tolerance interval left parenthesis upper M minus delta comma upper M plus delta right parenthesis(M −δ, M + δ)
should be high. See Fig. 6.1 for a schematic presentation of these relationships. 
Fig. 6.1 Quadratic loss and tolerance intervals

6.1
Off-Line Quality Control, Parameter Design, and the Taguchi Method
229
Notice that the expected loss is 
upper E left brace upper L left parenthesis upper Y comma upper M right parenthesis right brace equals upper K left parenthesis Bias squared plus Variance right parenthesis commaE{L(Y, M)} = K(Bias2 + Variance),
(6.1.6) 
where Bias equals mu minus upper M= μ −M, mu equals upper E left brace upper Y right braceμ = E{Y} and Variance equals upper E left brace left parenthesis upper Y minus mu right parenthesis squared right brace= E{(Y −μ)2}. Thus, the objective 
is to have a manufacturing process with muμ as close as possible to the target M and 
variance, sigma squaredσ 2, as small as possible left parenthesis sigma less than StartFraction delta Over 3 EndFraction(σ < δ
3 so that upper C Subscript p k Baseline greater than 1Cpk > 1). Recall that Variance + 
Biassquared2 is the mean squared error, MSE. Thus, when “normal is best,” the objective 
should be to minimize the MSE. 
Objective functions for cases of “the bigger the better” or “the smaller the better” 
depend on the case under consideration. In cases where the performance measure is 
the life length of a product, the objective might be to design the product to maximize 
the expected life length. In the literature, we may ﬁnd the objective of minimizing 
StartFraction 1 Over n EndFraction sigma summation StartFraction 1 Over y Superscript j Baseline EndFraction 1
n
 1
yj , which is an estimator of upper E left brace StartFraction 1 Over upper Y EndFraction right braceE

1
Y

. This parameter, however, may not always 
exist (e.g., when Y has an exponential distribution), and this objective function 
might be senseless. 
6.1.2 
Major Stages in Product and Process Design 
A major challenge of industry is to reduce variability in products and processes. 
The previous section dealt with measuring the impact of such variability. In this 
section, we discuss methods for actually reducing variability. Design of products 
or processes involves two main steps: designing the system and setting tolerances. 
System design is the stage where engineering skills, innovation, and technology 
are pooled together to create a basic design. Once the design is ready to go 
into production, one has to specify tolerances of parts and subassemblies so that 
the product or process meets its requirements. Loose tolerances are typically less 
expensive than tight tolerances. Taguchi proposed to change the classical approach 
to the design of products and processes and add an intermediate stage of parameter 
design. Thus, the three major stages in designing a product or a process are: 
I. System Design—This is when the product architecture and technology are 
determined. 
II. Parameter Design—At this stage, a planned optimization program is carried 
out in order to minimize variability and costs. 
III. Tolerance Design—Once the optimum performance is determined, tolerances 
should be speciﬁed, so that the product or process stays within speciﬁcations. 
The setting of optimum values of the tolerance factors is called tolerance 
design. 
Table 6.1 (adapted from Phadke (1989)) shows the relationships between the type 
of problems experienced in industrial products and processes and the various design 
phases.

230
6
Quality by Design
Table 6.1 Noise factors and design phases 
Leverage on noise factors 
Manu-
facturing 
imperf 
Natural 
deterio-
ration
Activity
Design phase External
Comments 
Product design 
(a) System 
design 
High
High
High
Involves innovation to 
reduce sensitivity to all 
noise factors 
(b) 
Parameter 
design 
High
High
High
Most important step for 
reducing sensitivity to all 
noise factors 
(c) Tolerance 
design 
High
High
High
Method for selecting 
most economical grades 
of materials, components, 
and manufacturing 
equipment, and operating 
environment for the 
product 
Manufacturing 
process design 
(a) Concept 
design 
Low
High
Low
Involves innovation to 
reduce the effect of 
manufacturing 
imperfections 
(b) 
Parameter 
design 
Low
High
Low
Important for reducing 
sensitivity of unit-to-unit 
variation to 
manufacturing variations 
(c) Tolerance 
design 
Low
High
Low
Method for determining 
tolerance on 
manufacturing process 
parameters 
Manufacturing 
(a) Concept 
design 
Low
High
Low
Method of detecting 
problems when they 
occur and correcting 
them 
(b) 
Parameter 
design 
Low
High
Low
Method of compensating 
for known problems 
(c) Tolerance 
design 
Low
High
Low
Last alternative, useful 
when process capability 
is poor 
Customer 
usage 
Warranty 
and repair 
Low
Low
Low 
6.1.3 
Design Parameters and Noise Factors 
Taguchi classiﬁes the variables that affect the performance characteristics into 
two categories: design parameters and source of noise. All factors that cause 
variability are included in the source of noise. Sources of noise are classiﬁed into 

6.1 Off-Line Quality Control, Parameter Design, and the Taguchi Method
231 
two categories: external sources and internal sources. External sources are those 
external to the product, like environmental conditions (temperature, humidity, dust, 
etc.), human variations in operating the product, and other similar factors. Internal 
sources of variability are those connected with manufacturing imperfections and 
product degradation or natural deterioration. 
The design parameters, on the other hand, are controllable factors that can be set 
at predetermined values (level). The product designer has to specify the values of the 
design parameters to achieve the objectives. This is done by running an experiment 
that is called parameter design. In manufacturing conditions, the values of these 
parameters may slightly vary from values determined in the parameter design stage 
(the nominal ones). In tolerance designs, we test the effects of such variability and 
determine tolerances that yield the desired results at lower cost. 
Example 6.1 An RL circuit is an electrical circuit of alternating current that obtains 
an input of voltage 100 [V] AC and frequency 55 [Hz]. The output current of the 
circuit is aimed at 10 [A], with tolerances of upper Delta equals plus or minus 4 = ±4 [A]. There are four factors 
that inﬂuence the output y: 
bold upper VV: 
Input voltage [V]. 
bold ff: 
Input frequency [Hz]. 
bold upper RR: 
Resistance [Ohm]. 
bold upper LL: 
Self-Inductance [H]. 
R and L are controllable factors, while V and f are noise factors. Assume that V 
has a distribution between 90 and 110 [V] and f has a distribution between 55 and 
65 [Hz]. R and L are design parameters. What should be values of R [upper Omega] and L [H] 
to obtain an output y distributed around the target of upper M equals 10M = 10 [A], with minimal 
mean squared error and lowest cost? In the next section, we study how one can take 
advantage of the non-linear relationship between the above factors to attain lower 
variability and high accuracy.
black medium square■
6.1.4 
Parameter Design Experiments 
In a parameter design experiment, we test the effects of the controllable factors and 
the noise factors on the performance characteristics of the product, in order to: 
(a) Make the product robust (insensitive) to environmental conditions. 
(b) Make the product insensitive to components variation. 
(c) Minimize the mean squared error about a target value. 
We distinguish between two types of experiments, physical experiments and 
computer based simulation experiments. In Chap. 7, we discuss this type of 
computer based simulation experiments. Let bold italic theta equals left parenthesis theta 1 comma midline horizontal ellipsis comma theta Subscript k Baseline right parenthesisθ = (θ1, · · ·  , θk) be the vector of 
design parameters. The vector of noise variables is denoted by bold x equals left parenthesis x 1 comma midline horizontal ellipsis comma x Subscript m Baseline right parenthesisx = (x1, · · ·  , xm). 

232
6 Quality by Design 
The response function y element of f left parenthesis bold italic theta comma bold x right parenthesisy ∈ f (θ, x) involves in many situations the factors bold italic thetaθ and 
bold xx in a non-linear fashion. The RL circuit described in Example 6.1 involves the four 
factors V , f , R, L and the output y according to the non-linear response function 
y equals StartFraction upper V Over left parenthesis upper R squared plus left parenthesis 2 pi f upper L right parenthesis squared right parenthesis Superscript 1 divided by 2 Baseline EndFraction periody =
V 
(R2 + (2πf L)2)1/2 . 
If the noise factors V and f had no variability, one could determine the values of R 
and L to always obtain a target value y 0 equals upper My0 = M. The variability of V and f around 
their nominal values turns y to be a random variable, Y, with expected value muμ and 
variance sigma squaredσ 2, which depend on the setting of the design parameters R and L and on 
the variances of V and f . The effects of the non-linearity on the distribution of Y 
will be studied in Sect. 6.2. The objective of parameter design experiments is to take 
advantage of the effects of the non-linear relationship. The strategy is to perform a 
factorial experiment to investigate the effects of the design parameters (controllable 
factors). If we learn from the experiments that certain design parameters affect the 
mean of Y but not its variance and, on the other hand, other design factors affect the 
variance but not the mean, we can use the latter group to reduce the variance of Y 
as much as possible and then adjust the levels of the parameters in the ﬁrst group to 
set muμ close to the target M. We illustrate this approach in the following example. 
Example 6.2 The data for the present example are taken from (John 1990, p. 335). 
Three factors upper A comma upper B comma upper CA, B, C (controllable) affect the output, Y, of a system. In order to 
estimate the main effects of upper A comma upper B comma upper CA, B, C, a 2 cubed23 factorial experiment was conducted. Each 
treatment combination was repeated 4 times, at the “low” and “high” levels of two 
noise factors. The results and the mean, upper Y overbar¯Y, and standard deviation, S, of  Y at the 8 
treatment combinations are given in Table 6.2. 
response = pd.DataFrame([ 
[-1, -1, -1, 60.5, 61.7, 60.5, 60.8], 
[ 1, -1, -1, 47.0, 46.3, 46.7, 47.2], 
[-1, 
1, -1, 92.1, 91.0, 92.0, 91.6], 
[ 1, 
1, -1, 71.0, 71.7, 71.1, 70.0], 
[-1, -1, 
1, 65.2, 66.8, 64.3, 65.2], 
[ 1, -1, 
1, 49.5, 50.6, 49.5, 50.5], 
[-1, 
1, 
1, 91.2, 90.5, 91.5, 88.7], 
[ 1, 
1, 
1, 76.0, 76.0, 78.3, 76.4], 
], columns=['A', 'B', 'C', 'y1', 'y2', 'y3', 'y4']) 
response['Ymean'] = response[['y1', 'y2', 'y3', 'y4']].mean(axis=1) 
response['S'] = response[['y1', 'y2', 'y3', 'y4']].std(ddof=0, axis=1) 
model = smf.ols('Ymean ~ A + B + C',  data=response).fit() 
print(f'rsquared: {model.rsquared:.3f}') 
model_S = smf.ols('S ~ A + B + C',  data=response).fit() 
print(f'rsquared: {model_S.rsquared:.3f}') 
rsquared: 0.991 
rsquared: 0.816 

6.1 Off-Line Quality Control, Parameter Design, and the Taguchi Method
233 
Table 6.2 Response and mean, upper Y overbar¯Y, and standard deviation, S, at a 2 cubed23 factorial experiment 
A
B
C
y 1y1
y 2y2
y 3y3
y 4y4
upper Y overbar ¯Y
S 
negative 1−1
negative 1−1
negative 1−1
60.5
61.7
60.5
60.8
60.875
0.4918 
1
negative 1−1
negative 1−1
47.0
46.3
46.7
47.2
46.800
0.3391 
negative 1−1
1
negative 1−1
92.1
91.0
92.0
91.6
91.675
0.4323 
1
1
negative 1−1
71.0
71.7
71.1
70.0
70.950
0.6103 
negative 1−1
negative 1−1
1
65.2
66.8
64.3
65.2
65.375
0.9010 
1
negative 1−1
1
49.5
50.6
49.5
50.5
50.025
0.5262 
negative 1−1
1
1
91.2
90.5
91.5
88.7
90.475
1.0871 
1
1
1
76.0
76.0
78.3
76.4
76.675
0.9523 
Regressing the column upper Y overbar¯Y on the 3 orthogonal columns under upper A comma upper B comma upper CA, B, C in 
Table 6.2, we obtain 
Mean equals 69.1 minus 7.99 upper A plus 13.3 upper B plus 1.53 upper CMean = 69.1 − 7.99A + 13.3B + 1.53C 
with upper R squared equals 0.991R2 = 0.991. Moreover, the coefﬁcient 1.53 of C is not signiﬁcant (a P value 
of 0.103). Thus, the signiﬁcant main effects on the mean yield are the factors A and 
B only. Regressing the column of S on upper A comma upper B comma upper CA, B, C, we obtain the equation 
STD equals 0.668 minus 0.060 upper A plus 0.103 upper B plus 0.199 upper C commaSTD = 0.668 − 0.060A + 0.103B + 0.199C, 
with upper R squared equals 0.816R2 = 0.816. Only the main effect of C is signiﬁcant. Factors A and B have no 
effects on the standard deviation. The strategy is therefore to set the value of C at 
negative 1−1 (as small as possible) and the values of A and B to adjust the mean response to 
be equal to the target value M. If upper M equals 85M = 85, we ﬁnd A and B that solve the equation 
StartLayout 1st Row 85 equals 69.1 minus 7.99 upper A plus 13.3 upper B minus 1.53 2nd Row upper A equals StartFraction negative 17.43 plus 13.3 upper B Over 7.99 EndFraction period EndLayout85 = 69.1 −7.99A + 13.3B − 1.53 
A = −17.43 + 13.3B 
7.99 
. 
Letting upper B equals 0.75B = 0.75, then upper A equals negative 0.933A = −0.933. The optimal setting of the design parameters 
upper A comma upper B comma upper CA, B, C is at upper A equals negative 0.742A = −0.742, upper B equals 0.75B = 0.75, and upper C equals negative 1C = −1.
black medium square■
6.1.5 
Performance Statistics 
As we have seen in the previous example, the performance characteristic y at 
various combinations of the design parameters is represented by the mean, upper Y overbar¯Y, and 
standard deviation, S, of the  y values observed at various combinations of the noise 
factors. We performed the analysis ﬁrst on upper Y overbar¯Y and then on S to detect which design 
parameters inﬂuence upper Y overbar¯Y but not S, and which inﬂuences S but not upper Y overbar¯Y. Let eta left parenthesis bold italic theta right parenthesisη(θ) denote 

234
6 Quality by Design 
the expected value of Y, as a function of the design parameters theta 1 comma midline horizontal ellipsis comma theta Subscript k Baselineθ1, · · ·  , θk. Let  
sigma squared left parenthesis bold italic theta right parenthesisσ 2(θ) denote the variance of Y as a function of bold italic thetaθ. The situation described above 
corresponds to the case that eta left parenthesis bold italic theta right parenthesisη(θ) and sigma squared left parenthesis bold italic theta right parenthesisσ 2(θ) are independent. The objective in 
setting the values of theta 1 comma midline horizontal ellipsis comma theta Subscript k Baselineθ1, · · ·  , θk is to minimize the mean squared error 
upper M upper S upper E left parenthesis bold italic theta right parenthesis equals upper B squared left parenthesis bold italic theta right parenthesis plus sigma squared left parenthesis bold italic theta right parenthesis commaMSE(θ) = B2(θ) + σ 2(θ),
(6.1.7) 
where upper B left parenthesis bold italic theta right parenthesis equals eta left parenthesis bold italic theta right parenthesis minus upper MB(θ) = η(θ) − M. The  performance statistic is an estimator of MSEleft parenthesis bold italic theta right parenthesis(θ), 
namely, 
ModifyingAbove upper M upper S upper E With caret left parenthesis bold italic theta right parenthesis equals left parenthesis ModifyingAbove upper Y With bar left parenthesis bold italic theta right parenthesis minus upper M right parenthesis squared plus upper S squared left parenthesis bold italic theta right parenthesis period
MSE(θ) = ( ¯Y(θ) −M)2 + S2(θ).
(6.1.8) 
If ModifyingAbove upper Y With bar left parenthesis bold italic theta right parenthesis ¯Y(θ) and upper S squared left parenthesis bold italic theta right parenthesisS2(θ) depend on different design parameters, we perform the minimiza-
tion in two steps. First we minimize upper S squared left parenthesis bold italic theta right parenthesisS2(θ) and then ModifyingAbove upper B With caret squared left parenthesis bold italic theta right parenthesis equals left parenthesis ModifyingAbove upper Y With bar left parenthesis theta right parenthesis minus upper M right parenthesis squared ˆB2(θ) = ( ¯Y(θ) −M)2. If eta left parenthesis bold italic theta right parenthesisη(θ) 
and sigma squared left parenthesis bold italic theta right parenthesisσ 2(θ) are not independent, the problem is more complicated. 
Taguchi recommends devising a function of eta left parenthesis bold italic theta right parenthesisη(θ) and sigma left parenthesis bold italic theta right parenthesisσ(θ), which is called 
a signal-to-noise ratio (SN), and maximizing an estimator of this SN function. 
Taguchi devised a large number of such performance statistics. In particular, Taguchi 
recommended to maximize the performance statistic 
eta equals 10 log left parenthesis StartFraction upper Y overbar squared Over upper S squared EndFraction minus StartFraction 1 Over n EndFraction right parenthesis commaη = 10 log
 ¯Y 2 
S2 − 1 
n

,
(6.1.9) 
which is being used in many studies. As can be shown by the method of the next 
section, the variance of log left parenthesis upper Y overbar right parenthesislog( ¯Y) in large samples is approximately 
upper V left brace log left parenthesis upper Y overbar right parenthesis right brace approximately equals StartFraction sigma squared Over n mu squared EndFraction periodV {log( ¯Y)} ∼= σ 2 
nμ2 .
(6.1.10) 
Thus, 
minus log upper V left brace log left parenthesis upper Y overbar right parenthesis right brace equals log left parenthesis StartFraction mu squared Over sigma squared EndFraction right parenthesis plus log n period− log V {log( ¯Y)} =  log
μ2 
σ 2

+ log n. 
In the case of a normal distribution of Y, StartFraction upper Y overbar squared Over upper S squared EndFraction minus StartFraction 1 Over n EndFraction ¯Y 2 
S2 − 1 
n is an unbiased estimator of StartFraction mu squared Over sigma squared EndFractionμ2 
σ 2 . 
Thus, 
10 log left parenthesis StartFraction upper Y overbar squared Over upper S squared EndFraction minus StartFraction 1 Over n EndFraction right parenthesis10 log
 ¯Y 2 
S2 − 1 
n

(6.1.11) 
is an estimator of minus 10 log left parenthesis n upper V left brace log left parenthesis upper Y overbar right parenthesis right brace right parenthesis−10 log(nV {log( ¯Y)}), although not an unbiased one. It is difﬁcult 
to give any other justiﬁcation to the performance statistic etaη(SN ratio). One has to be 
careful, since maximizing this SN might achieve bad results if eta left parenthesis bold italic theta right parenthesisη(θ) is far from the 
target M. Thus, if the objective is to set the design parameters to obtain means close 

6.2 The Effects of Non-linearity
235 
to M and small standard deviations, one should minimize the mean squared error 
and not necessarily maximize the above SN ratio. 
6.2 The Effects of Non-linearity 
As mentioned in the previous section, the response function f left parenthesis bold italic theta comma bold x right parenthesisf (θ, x) might be non-
linear in bold italic thetaθ and bold xx, and an example was given for the case of output current of an RL 
circuit, namely 
upper Y equals StartFraction upper V Over left parenthesis upper R squared plus left parenthesis 2 pi f upper L right parenthesis squared right parenthesis Superscript 1 divided by 2 Baseline EndFraction periodY =
V 
(R2 + (2πf L)2)1/2 . 
This is a non-linear function of the design parameters R and L and the noise factor 
f . We have assumed that  V and f are random variables and R and L are constant 
parameters. The output current is the random variable Y. What is the expected 
value and variance of Y? Generally, one can estimate the expected value of Y 
and its variance by simulation, using the function f left parenthesis bold italic theta comma bold upper X right parenthesisf (θ, X), and the assumed joint 
distribution of bold upper XX. An approximation to the expected value and the variance of Y can 
be obtained by the following method. 
Let the random variables upper X 1 comma midline horizontal ellipsis comma upper X Subscript k BaselineX1, · · ·  , Xk have expected values xi 1 comma midline horizontal ellipsis comma xi Subscript k Baselineξ1, · · ·  , ξk and 
variance–covariance matrix 
upper V equals Start 4 By 4 Matrix 1st Row 1st Column sigma 1 squared 2nd Column sigma 12 3rd Column midline horizontal ellipsis 4th Column sigma Subscript 1 k Baseline 2nd Row 1st Column sigma 21 2nd Column sigma 2 squared 3rd Column Blank 4th Column sigma Subscript 2 k Baseline 3rd Row 1st Column vertical ellipsis 2nd Column Blank 3rd Column down right diagonal ellipsis 4th Column vertical ellipsis 4th Row 1st Column sigma Subscript k Baseline 1 Baseline 2nd Column sigma Subscript k Baseline 2 Baseline 3rd Column midline horizontal ellipsis 4th Column sigma Subscript k Superscript 2 Baseline EndMatrix periodV = 
⎡ 
⎢⎢⎢⎣ 
σ 2 
1 σ12 · · ·  σ1k 
σ21 σ 2 
2
σ2k 
... 
... ... 
σk1 σk2 · · ·  σ 2 
k 
⎤ 
⎥⎥⎥⎦ . 
Assuming that f left parenthesis bold italic theta comma bold upper X right parenthesisf (θ, X) can be expanded into a Taylor series around the means bold italic xi 1 equals left parenthesis xi 1 comma midline horizontal ellipsis comma xi Subscript k Baseline right parenthesisξ1 = 
(ξ1, · · ·  , ξk), we obtain the approximation 
f left parenthesis bold italic theta comma bold upper X right parenthesis approximately equals f left parenthesis bold italic theta comma bold italic xi right parenthesis plus sigma summation Underscript i equals 1 Overscript k Endscripts left parenthesis x Subscript i Baseline minus xi Subscript i Baseline right parenthesis StartFraction partial differential Over partial differential x Subscript i Baseline EndFraction f left parenthesis bold italic theta comma bold italic xi right parenthesis plus one half left parenthesis bold upper X minus bold italic xi right parenthesis prime upper H left parenthesis bold italic theta comma bold italic xi right parenthesis left parenthesis bold upper X minus bold italic xi right parenthesis commaf (θ, X) ∼= f (θ, ξ)+ 
k

i=1 
(xi−ξi) ∂ 
∂xi 
f (θ, ξ)+1 
2 (X−ξ)′H(θ, ξ)(X−ξ),
(6.2.1) 
where upper H left parenthesis bold italic theta comma bold italic xi right parenthesisH(θ, ξ) is a k times kk × k matrix of second order partial derivatives, evaluated at bold italic xi Subscript iξi 
with left parenthesis i comma j right parenthesis(i, j)th element equal to 
upper H Subscript i j Baseline left parenthesis bold italic theta comma bold italic xi right parenthesis equals StartFraction partial differential squared Over partial differential x Subscript i Baseline partial differential x Subscript j Baseline EndFraction f left parenthesis bold italic theta comma bold italic xi right parenthesis comma i comma j equals 1 comma midline horizontal ellipsis comma k periodHij(θ, ξ) =
∂2 
∂xi∂xj 
f (θ, ξ), 
i, j = 1, · · ·  , k.
(6.2.2) 
Thus, the expected value of f left parenthesis bold italic theta comma bold upper X right parenthesisf (θ, X) is approximated by 

236
6 Quality by Design 
upper E left brace f left parenthesis bold italic theta comma bold upper X right parenthesis right brace approximately equals f left parenthesis bold italic theta comma bold italic xi right parenthesis plus one half sigma summation Underscript i equals 1 Overscript k Endscripts sigma summation Underscript j equals 1 Overscript i Endscripts sigma Subscript i j Baseline upper H Subscript i j Baseline left parenthesis bold italic theta comma bold italic xi right parenthesis commaE{f (θ, X)} ∼= f (θ, ξ) + 1 
2 
k

i=1 
i

j=1 
σijHij(θ, ξ),
(6.2.3) 
and the variance of f left parenthesis bold italic theta comma bold upper X right parenthesisf (θ, X) is approximated by 
upper V left brace f left parenthesis bold italic theta comma bold upper X right parenthesis right brace approximately equals sigma summation Underscript i equals 1 Overscript k Endscripts sigma summation Underscript j equals 1 Overscript k Endscripts sigma Subscript i j Baseline StartFraction partial differential Over partial differential x Subscript i Baseline EndFraction f left parenthesis bold italic theta comma bold italic xi right parenthesis StartFraction partial differential Over partial differential x Subscript j Baseline EndFraction f left parenthesis bold italic theta comma bold italic xi right parenthesis periodV {f (θ, X)} ∼= 
k

i=1 
k

j=1 
σij 
∂ 
∂xi 
f (θ, ξ) ∂ 
∂xj 
f (θ, ξ).
(6.2.4) 
As seen in these approximations, if the response variable Y is a non-linear function 
of the random variables upper X 1 comma midline horizontal ellipsis comma upper X Subscript m BaselineX1, · · ·  , Xm, its expected value depends also on the 
variances and covariances of the X’s. This is not the case if Y is a linear function of 
the x’s. Moreover, in the linear case, the formula for upper V left brace upper Y right braceV {Y} is exact. 
Example 6.3 Consider the function 
upper Y equals StartFraction upper V Over left parenthesis upper R squared plus left parenthesis 2 pi f upper L right parenthesis squared right parenthesis Superscript 1 divided by 2 Baseline EndFraction commaY =
V 
(R2 + (2πf L)2)1/2 , 
where upper R equals 5.0R = 5.0 [upper Omega] and upper L equals 0.02L = 0.02 [H]. V and f are independent random variables 
having normal distributions: 
StartLayout 1st Row 1st Column upper V 2nd Column tilde upper N left parenthesis 100 comma 9 right parenthesis 2nd Row 1st Column f 2nd Column tilde upper N left parenthesis 55 comma 25 divided by 9 right parenthesis period EndLayoutV ∼N(100, 9) 
f ∼ N(55, 25/9). 
Notice that 
StartLayout 1st Row 1st Column StartFraction partial differential y Over partial differential v EndFraction 2nd Column equals StartFraction 1 Over left parenthesis upper R squared plus left parenthesis 2 pi f upper L right parenthesis squared right parenthesis Superscript 1 divided by 2 Baseline EndFraction 2nd Row 1st Column StartFraction partial differential y Over partial differential f EndFraction 2nd Column equals minus 4 upper V left parenthesis upper R squared plus left parenthesis 2 pi f upper L right parenthesis squared right parenthesis Superscript negative 3 divided by 2 Baseline pi squared upper L squared f 3rd Row 1st Column StartFraction partial differential squared y Over partial differential v squared EndFraction 2nd Column equals 0 4th Row 1st Column StartFraction partial differential squared y Over partial differential v partial differential f EndFraction 2nd Column equals minus 4 left parenthesis upper R squared plus left parenthesis 2 pi f upper L right parenthesis squared right parenthesis Superscript negative 3 divided by 2 Baseline pi squared upper L squared f period EndLayout∂y 
∂v =
1 
(R2 + (2πf L)2)1/2 
∂y 
∂f = −4V (R2 + (2πf L)2)−3/2π2L2f 
∂2y 
∂v2 = 0 
∂2y 
∂v∂f = −4(R2 + (2πf L)2)−3/2π2L2f. 
Also, 
StartLayout 1st Row 1st Column StartFraction partial differential squared y Over partial differential f squared EndFraction equals 2nd Column minus 4 upper V left parenthesis upper R squared plus left parenthesis 2 pi f upper L right parenthesis squared right parenthesis Superscript negative 3 divided by 2 Baseline pi squared upper L squared 2nd Row 1st Column Blank 2nd Column plus 48 upper V left parenthesis upper R squared plus left parenthesis 2 pi f upper L right parenthesis squared right parenthesis Superscript negative 5 divided by 2 Baseline pi Superscript 4 Baseline upper L Superscript 4 Baseline f squared period EndLayout∂2y 
∂f 2 = − 4V

R2 + (2πf L)2−3/2 
π2L2 
+ 48V

R2 + (2πf L)2−5/2 
π4L4f 2. 

6.2 The Effects of Non-linearity
237 
Substituting in these derivatives the values of R and L and the expected values of V 
and f , we obtain 
StartLayout 1st Row 1st Column StartFraction partial differential y Over partial differential v EndFraction 2nd Column equals StartFraction 1 Over 8.5304681 EndFraction equals 0.11723 2nd Row 1st Column StartFraction partial differential y Over partial differential f EndFraction 2nd Column equals negative 0.13991 3rd Row 1st Column StartFraction partial differential squared y Over partial differential v squared EndFraction 2nd Column equals 0 4th Row 1st Column StartFraction partial differential squared y Over partial differential v partial differential f EndFraction 2nd Column equals negative 0.0013991 5th Row 1st Column StartFraction partial differential squared y Over partial differential f squared EndFraction 2nd Column equals negative 0.0025438 plus 0.0050098 6th Row 1st Column Blank 2nd Column equals 0.002466 period EndLayout∂y 
∂v =
1 
8.5304681 = 0.11723 
∂y 
∂f = −0.13991 
∂2y 
∂v2 = 0 
∂2y 
∂v∂f = −0.0013991 
∂2y 
∂f 2 = −0.0025438 + 0.0050098 
= 0.002466. 
Accordingly, an approximation for upper E left brace upper Y right braceE{Y} is given by 
upper E left brace upper Y right brace approximately equals 11.722686 plus one half left parenthesis 9 times 0 plus 2.7778 times 0.002466 right parenthesis equals 11.7261 periodE{Y} ∼= 11.722686 + 1 
2 (9 × 0 + 2.7778 × 0.002466) = 11.7261. 
The variance of Y is approximated by 
upper V left brace upper Y right brace approximately equals 9 times left parenthesis 0.11723 right parenthesis squared plus StartFraction 25 Over 9 EndFraction times left parenthesis negative 0.13991 right parenthesis squared equals 0.17806 periodV {Y} ∼= 9 × (0.11723)2 + 25 
9 × (−0.13991)2 = 0.17806. 
To check the goodness of these approximations, we do the following simulation 
using Python. 
We simulate upper N equals 500N = 500 normal random variables having mean 100 and standard 
deviation 3 (V). Similarly, 500 normal random variables having mean 55 and 
standard deviation 1.67 are simulated and stored in variable f. We then calculate 
the values of Y and calculate mean and variance. 
np.random.seed(seed=1) 
def Y(R, L, V, f): 
d = R**2 + (2*np.pi*f*L)**2 
return V / np.sqrt(d) 
R = 5  
L = 0.02 
V = stats.norm.rvs(loc=100, scale=np.sqrt(9), size=500) 
f = stats.norm.rvs(loc=55, scale=np.sqrt(25/9), size=500) 
Ysimulated = Y(R, L, V, f) 
Ymean = np.mean(Ysimulated) 
Yvar = np.var(Ysimulated) 
delta = 2 * np.sqrt(Yvar / 500) 
CI = (Ymean - delta, Ymean + delta) 

238
6 Quality by Design 
The results obtained are upper Y overbar Subscript 500 Baseline equals ¯Y500 = 11.739 and upper S 500 squared equalsS2 
500 = 0.16942. The analytical 
approximations are very close to the simulation estimates. Actually, a .95 conﬁdence 
interval for upper E left brace upper Y right braceE{Y} is given by upper Y overbar Subscript 500 Baseline plus or minus 2 StartFraction upper S 500 Over StartRoot 500 EndRoot EndFraction ¯Y500 ± 2 S500
√
500 , which is (11.702, 11.776). The result 
of the analytical approximation, 11.7261, is within the conﬁdence limits. The 
approximation is quite good. 
It is interesting to estimate the effects of the design parameters R and L on upper E left brace upper Y right braceE{Y} 
and upper V left brace upper Y right braceV {Y}. We conduct a small experiment on the computer for estimating upper E left brace upper Y right braceE{Y} and 
upper V left brace upper Y right braceV {Y} by a 3 squared32 factorial experiment. The levels of R and L as recommended by G. 
Taguchi, in his review paper (see Ghosh 1990, pp. 1–34), are: 
0
1
2 
R
0.05 
5.00 
9.50 
L
0.01 
0.02 
0.03 
In each treatment combination, we simulate 500 y-values. The results are given in 
Table 6.3. 
from mistat.design import doe 
np.random.seed(1) 
# Build design from factors 
FacDesign = doe.full_fact({ 
'R': [0.05, 5, 9.5], 
'L': [0.01, 0.02, 0.03], 
}) 
results = [] 
for _, (R, L) in FacDesign.iterrows(): 
V = stats.norm.rvs(loc=100, scale=np.sqrt(9), size=500) 
f = stats.norm.rvs(loc=55, scale=np.sqrt(25/9), size=500) 
Ysimulated = Y(R, L, V, f) 
Ymean = np.mean(Ysimulated) 
Yvar = np.var(Ysimulated) 
target = 10 
MSE = (Ymean - target) **2 + Yvar 
results.append({ 
'R': R, 
'L': L, 
'Ymean': Ymean, 
'Yvar': Yvar, 
'MSE': MSE, 
}) 
results = pd.DataFrame(results) 
The objective is to ﬁnd the combinations of R and L that yield minimum MSE equals left parenthesis upper E left brace upper Y right brace minus upper M right parenthesis squared plus upper V left brace upper Y right braceMSE = 
(E{Y}−M)2 +V {Y}, where M is the target of 10 [upper Omega]. It seems that the best setting 
of the design parameters is upper R equals 9.5R = 9.5 [upper Omega] and upper L equals 0.01L = 0.01 [H]. The combination 
upper R equals 0.05R = 0.05 [upper Omega] and upper L equals 0.03L = 0.03 [H] also yields very small MSE. One should choose 
the least expensive setting.
black medium square■

6.3 Taguchi’s Designs
239 
Table 6.3 The means, 
variances, and MSE of Y in a 
3 squared32 experiment 
R
L
upper Y overbar Subscript 500 ¯Y500
upper S 500 squaredS2 
500
MSE 
0.050 
0.01 
28.984 
1.446 
361.84 
5.000 
0.01 
16.495 
0.275
42.45 
9.500 
0.01
9.889 
0.089
0.10 
0.050 
0.02 
14.510 
0.396
20.74 
5.000 
0.02 
11.721 
0.182
3.14 
9.500 
0.02
8.515 
0.072
2.28 
0.050 
0.03
9.668 
0.170
0.28 
5.000 
0.03
8.676 
0.126
1.88 
9.500 
0.03
7.109 
0.062
8.42 
6.3 Taguchi’s Designs 
In order to simulate the effect of noise factors, Taguchi advocates the combination 
of two experimental arrays, an inner array and an outer array. The inner array is 
used to determine factor-level combinations of factors that can be controlled by the 
designer of the product or process. The outer array is used to generate the variability 
due to noise factors that is experienced by the product or process under optimization, 
in its day-to-day operation. 
The experimental arrays used by Taguchi are orthogonal array designs. An 
example of a design with 15 factors at two levels each, using 16 experiments, is 
given in Table 6.4. The levels are indicated by 1 and 2, and the ﬁrst row consists 
of all factors at level 1. This experimental array was introduced in Chap. 5 as a 
2 Superscript 15 minus 11215−11 fractional factorial design that is a fully saturated design with 15 factors 
and 16 experimental runs. The corresponding full factorial design consists of 
2 Superscript 15 Baseline equals 32768215 = 32768 experiments. Taguchi labeled several experimental arrays using a 
convenient notation and reproduced them in tables that were widely distributed 
among engineers (see Taguchi and Konishi 1987). The availability of these tables 
made it convenient for practitioners to design and run such experiments. 
One can note from Table 6.4 that if we run the experiment using the order of the 
experiment array, we will ﬁnd it convenient to assign to column 1 a factor that is 
difﬁcult to change from level 1 to level 2. For example, if changing the temperature 
of a solder bath requires 5 h, the assignment of temperature to column 1 would 
require one change only. Column 15, on the other hand, has 9 changes between 
levels. Taguchi recommends that in some cases randomization be abandoned for 
the beneﬁt of simplicity and cost. If we choose to run the experiment in the order 
of the experimental array, we can reduce the practical difﬁculties in running the 
experiment by proper assignment of factors to columns. An easily changed factor 
can get assigned to column 15 with low penalty. However, assigning a factor that is 
difﬁcult to change to column 15 might make the whole experiment impractical. 
In order to simulate the noise factors, we can design a second experiment using 
an external array. In some cases, noise cannot be directly simulated, and the external 
array consists of replicating the internal array experiments over a speciﬁed length 

240
6 Quality by Design 
Table 6.4 Factor-level combinations of 15 factors at two levels each in an upper L 16 left parenthesis 2 Superscript 15 Baseline right parenthesisL16(215) orthogonal 
array 
Columns 
Trial
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15 
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 
2
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2 
3
1
1
1
2
2
2
2
1
1
1
1
2
2
2
2 
4
1
1
1
2
2
2
2
2
2
2
2
1
1
1
1 
5
1
2
2
1
1
2
2
1
1
2
2
1
1
2
2 
6
1
2
2
1
1
2
2
2
2
1
1
2
2
1
1 
7
1
2
2
2
2
1
1
1
1
2
2
2
2
1
1 
8
1
2
2
2
2
1
1
2
2
1
1
1
1
2
2 
9
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2 
10
2
1
2
1
2
1
2
2
1
2
1
2
1
2
1 
11
2
1
2
2
1
2
1
1
2
1
2
2
1
2
1 
12
2
1
2
2
1
2
1
2
1
2
1
1
2
1
2 
13
2
2
1
1
2
2
1
1
2
2
1
1
2
2
1 
14
2
2
1
1
2
2
1
2
1
1
2
2
1
1
2 
15
2
2
1
2
1
1
2
1
2
2
1
2
1
1
2 
16
2
2
1
2
1
1
2
2
1
1
2
1
2
2
1 
of time or amount of material. In Sect. 6.6, we describe two such experiments. The 
ﬁrst experiment deals with a speedometer cable where the percentage of shrinkage 
is measured on several pieces of cable taken from various parts of a spool and 
running a heat test. The external array simply consists of the sampled parts of the 
spool. The second experiment deals with optimizing the response time of a computer 
system. Here the inner array experiment was carried out on an operational system 
so that the variabilities induced by various users were not speciﬁcally simulated. A 
retrospective study veriﬁed that there was no bias in user methods. 
The design given in Table 6.4 can be used for up to 15 factors. It allows us to 
compute estimates of main effects, provided there are no interactions of any order 
between them. If, on the other hand, all ﬁrst order interactions can be potentially 
signiﬁcant, this design cannot be used with more than 5 factors. (The resolution of 
the above design is III.) In order to assist the engineer with the correct choice of 
columns from the table of orthogonal arrays, Taguchi devised a graphical method of 
presenting the columns of an orthogonal array table that are confounded with ﬁrst 
order interactions of some factors. These graphs are called linear graphs. 
In Fig. 6.2, we present two linear graphs associated with Table 6.4. Linear graph 
LGSubscript 11 corresponds to the case where all interactions might be signiﬁcant. The graph 
has ﬁve vertices and 10 lines connecting the vertices. The factors A, B, C, D, E are 
assigned to the columns with numbers at the vertices. Thus, the assignment of the 
factors to columns is, according to this linear graph: 

6.3 Taguchi’s Designs
241 
Fig. 6.2 Two linear graphs for upper L 16 left parenthesis 2 Superscript 15 Baseline right parenthesisL16(215) 
Factor
A 
B 
C 
D 
E 
Column 
1 
2 
4 
8 
15 
We also see that column 3 can be used to estimate the interaction AB, column 6 for 
the interaction BC, column 5 for the interaction AC, etc.  
The second linear graph, LGSubscript 22, represents the case where only some interactions 
are signiﬁcant. These are: AB, AC, AD, AE, FG, and FH. In this case, we can 
perform the 16 trials experiment with 8 factors and assign them to columns: 
Factor
A 
B 
C 
D 
E 
F 
G 
H 
Column 
1 
8 
10 
12 
14 
2 
4 
5 
The columns that can be used to estimate the interactions are 3, 6, 7, 9, 11, 13, 15. 
Although the emphasis in Taguchi’s methodology is on estimating main effects, 
one should not forget that interactions might exist. It is better to be cautious and not 
to over-saturate a small design with too many factors. Recall that when fractional 
replications are used, the estimates of main effects might be confounded. We wish 
to choose a design with sufﬁcient resolution (see Chap. 5), and this may require 
sufﬁciently large fractions. The table that we presented (Table 6.4) is a fraction of 
a 2 Superscript 15215 factorial experiment. Taguchi also prepared tables of orthogonal arrays for 
3 Superscript n3n factorial experiments, and for mixtures of factors with 2 and 3 levels left parenthesis 2 Superscript m Baseline times 3 Superscript k(2m × 3k 
factorials). The reader is referred to the tables of Taguchi and Konishi (1987) and 
also Appendix C of Phadke (1989). 

242
6 Quality by Design 
6.4 Quality by Design in the Pharmaceutical Industry 
6.4.1 
Introduction to Quality by Design 
A product or process in the pharmaceutical industry is well understood when all 
critical sources of variability are identiﬁed and explained, variability is proactively 
managed, and product quality attributes can be accurately and reliably predicted. 
Drug manufacturing processes must meet current good manufacturing practices 
(cGMP) to ensure that drug products meet safety and efﬁcacy requirements. 
Traditionally, the pharmaceutical industry has performed process validation studies 
on three batches. This approach, however, does not represent routine manufacturing 
and therefore is unlikely to cover all potential sources of variability (e.g., raw 
materials, operators, shifts, reactor vessels). The Ofﬁce of New Drug Quality 
Assessment at the Food and Drug Administration has identiﬁed this issue as a 
challenge to the regulatory process and launched a quality by design initiative with 
a focus on product and process understanding (Nasr 2007). 
Quality by design (QbD) in the pharmaceutical industry is a systematic approach 
to development of drug products and drug manufacturing processes that begins 
with predeﬁned objectives, emphasizes product, and process understanding and sets 
up process control based on sound science and quality risk management. In the 
traditional approach, product quality and performance are achieved predominantly 
by restricting ﬂexibility in the manufacturing process and by end product testing. 
Under the QbD paradigm, pharmaceutical quality is assured by understanding and 
controlling manufacturing and formulation variables. End product testing is used 
to conﬁrm the quality of the product and is not part of the ongoing consistency 
assurance and/or process control. The Food and Drug Administration (FDA) and 
the International Conference on Harmonization of Technical Requirements for 
Registration of Pharmaceuticals for Human Use (ICH) are promoting QbD in 
an attempt to curb rising development costs and regulatory barriers to innova-
tion and creativity (Kenett and Kenett 2008). ICH guidelines published several 
guidelines that deﬁne QbD for both the pharmaceutical and biopharmaceutical 
industry (ICH Q8-Q11). The implementation of QbD involves the application of 
statistical design of experiments (DoE) described in Chap. 5 in the development of 
products, processes, analytical methods and pharmaceutical formulations (Rathore 
and Mhatre 2009; Faltin et al. 2012). This section introduces QbD with a focus on 
the application of statistically designed experiments in this context. 
Under QbD, statistically designed experiments are used for efﬁciently and 
effectively investigating potential main effects and interactions among process and 
product factors. The mathematical models derived from such designed experiments 
are then used together with the acceptable boundaries of critical quality attributes 
to deﬁne a design space for a given process step. The normal operating range that 
is embedded within the design space yields quality attribute measurements that fall 
within the lower and upper control limits (LCL and UCL) representing the process 
performance (see Chaps. 2–4). When the LCL and UCL fall well within the lower 

6.4 Quality by Design in the Pharmaceutical Industry
243 
and upper speciﬁcation limits (LSL and USL), the process step is predicted to be 
highly capable of delivering product that meets the requirements of subsequent 
steps in the process. Excursions outside the normal operating range are expected 
to deliver product with quality attributes that are acceptable for further processing, 
as long as the operating parameters are held to limits deﬁned by the design space. 
When operations are affected by several quality attributes, the design space for the 
unit operation is obtained from overlays of the design spaces derived from analyses 
of multiple attributes, or from a multivariate analysis of the system. 
The ICH Q10 guideline for Quality Systems indicates that controls for a product 
consist not only of process controls and ﬁnal speciﬁcations for drug substance 
and drug product but also controls associated with the raw materials, excipients, 
container and closure, manufacturing equipment, and facility. It is a state of control 
in which all of the “planned controls” work together to ensure that the product deliv-
ered to the patient meets the patient’s needs. Design space boundaries, as described 
above, are an integral part of a comprehensive control strategy. The control strategy 
for a product is expected to evolve through the product lifecycle. The purpose of 
a control strategy for a product is to ensure that sufﬁcient controls are in place to 
maintain the risks associated with the product at a tolerable level. Risk management 
and control strategy principles are described in ICH Q9 (for a comprehensive 
treatment of operational risks, see Kenett and Raanan (2010)). A well-designed 
control strategy that results from appropriate leveraging of QbD principles, then, 
leads to reliable product quality and patient safety proﬁles. The steps to develop a 
QbD drug application consist of: (1) Determine the quality target product proﬁle, 
(2) Deﬁne the critical quality attributes (CQAs), (3) Conduct a risk assessment 
to identify potential critical process parameters, (4) Conduct statistically designed 
experiments (DoE) to identify actual critical process parameters, (5) Determine an 
appropriate control strategy, and (6) Revise the risk assessment. In the next section, 
we present a QbD case study focusing on the setting up of a design space. 
6.4.2 
A Quality by Design Case Study: The Full Factorial 
Design 
The case study is a steroid lotion formulation of a generic product designed to match 
the properties of an existing brand using in vitro tests. In vitro release is one of 
several standard methods that can be used to characterize performance characteris-
tics of a ﬁnished topical dosage form. Important changes in the characteristics of a 
drug product formula or the chemical and physical properties of the drug it contains 
should show up as a difference in drug release. Release is theoretically proportional 
to the square root of time when the formulation in question is in control of the release 
process because the release is from a receding boundary. In vitro release method 
for topical dosage forms described in SUPAC (1997) is based on an open chamber 
diffusion cell system such as a Franz cell system, ﬁtted usually with a synthetic 

244
6 Quality by Design 
Table 6.5 Risk assessment of manufacturing process variables 
Manufacturing process variables 
CQA 
Temperature 
of reaction
Blending time 
Cooling 
temperature 
Cooling time 
Order of 
ingredient 
addition 
Appearance
High
High
Low
Low
High 
Viscosity
High
High
Low
High
High 
Assay
Low
Low
Low
Low
Low 
In vitro permeability High
High
Low
High
High 
membrane. The test product is placed on the upper side of the membrane in the open 
donor chamber of the diffusion cell, and a sampling ﬂuid is placed on the other side 
of the membrane in a receptor cell. Diffusion of drug from the topical product to 
and across the membrane is monitored by assay of sequentially collected samples 
of the receptor ﬂuid. A plot of the amount of drug released per unit area (mcg/cm2) 
against the square root of time yields a straight line, the slope of which represents 
the release rate. This release rate measure is formulation-speciﬁc and can be used to 
monitor product quality. The typical in vitro release testing apparatus has six cells 
where the tested and brand products are being compared. A 90% conﬁdence interval 
for the ratio of the median in vitro release rate in the tested and brand products is 
computed and expressed in percentage terms. If the interval falls within the limits 
of 75% to 133.33%, the tested and brand products are considered equivalent. 
An initial risk assessment mapped risks in meeting speciﬁcations of critical 
quality attributes (CQAs). Table 6.5 presents expert opinions on the impact of man-
ufacturing process variables on various CQAs. Cooling temperature was considered 
to have low impact, and the order of ingredient addition was determined using risk 
contamination considerations. Later, both these factors were not studied in setting 
up the process design space. 
The responses that will be considered in setting up the process design space 
include 8 quality attributes: (1) Assay of active ingredient, (2) In vitro permeability 
lower conﬁdence interval, (3) In vitro permeability upper conﬁdence interval, (4) 
90 th90th percentile of particle size, (5) Assay of material A, (6) Assay of material B, (7) 
Viscosity, and (8) pH values. Three process factors are considered: (A) Temperature 
of reaction, (B) Blending time, and (C) Cooling time. 
In order to elicit the effect of the three factors on the eight responses, we will 
use a full factorial experiment with two center points (see Table 6.6 that presents the 
experimental array in standard order). 
We can compare different designs by comparing the relative prediction variance 
in the design space. The relative prediction variance at any point x in the design 
space is deﬁned as 
x prime left parenthesis upper X prime upper X right parenthesis Superscript negative 1 Baseline x commax′(X′X)−1x, 

6.4 Quality by Design in the Pharmaceutical Industry
245 
Table 6.6 Full factorial 
design with two center points 
Pattern 
Temperature 
Blending time 
Cooling time 
minus minus minus−−− 60
2
30 
plus minus minus+ −− 75
2
30 
minus plus minus−+ − 60
5
30 
plus plus minus+ + − 75
5
30 
0 0 00 0 0  
67.5
3.5
105 
0 0 00 0 0  
67.5
3.5
105 
minus minus plus−−+  60
2
180 
plus minus plus+ −+  75
2
180 
minus plus plus−+ +  60
5
180 
plus plus plus+ + +  75
5
180 
Fig. 6.3 Relative prediction variance proﬁles for the full factorial design of Table 6.6 and the 
related half-fraction design along the edge of the design space cube 
where X is the design matrix. For our full factorial design, the relative prediction 
variance is minimized at the center of the design and largest at the corners of the 
design space. Figure 6.3 shows the proﬁle of the relative prediction variance along 
one of the edges of the design space. If we choose a half-fraction replication with 
only four experimental runs on the corners of the cube, instead of the 9 points full 
factorial, our variance will double. 
The data with the experimental arrays are available in the dataset QBD.csv. After  
loading, we convert the data into factors and build a model to predict viscosity based 
on temperature, blending time, and cooling time with included interaction terms. 
The required Python code is shown in Table 6.7. 
The model coefﬁcients are summarized in Table 6.8. The predicted viscosity 
values are shown in the cube display in Fig. 6.4. Both results show that increasing 
temperature clearly increases viscosity. The two center points in the experimental 

246
6 Quality by Design 
Table 6.7 Preprocessing and model building for the QBD dataset 
# data loading and preprocessing 
df = mistat.load_data('QBD') 
df.columns = [s.replace(' ', '_').replace('-', '_') for s in df.columns] 
predictors = ['Temp', 'Blending_Time', 'Cooling_Time'] 
outcome = 'Viscosity' 
data = df[predictors + [outcome]] 
# convert data to factors 
FFdesign = data.copy() 
for predictor in predictors: 
values = FFdesign[predictor] 
toFactor = {min(values): -1, max(values): 1, np.mean(values): 0} 
values = [toFactor[v] for v in values] 
FFdesign[predictor] = values 
FFdesign = FFdesign.sort_values(by=['Cooling_Time', 'Blending_Time', 'Temp']) 
FFdesign = FFdesign.reset_index(drop=True) 
formula = 'Viscosity ~ (Temp + Blending_Time + Cooling_Time)**2' 
ff_model = smf.ols(formula=formula, data=FFdesign).fit() 
FFdesign['predicted'] = ff_model.predict(FFdesign) 
Table 6.8 Coefﬁcients of the model based on the full factorial design 
Coef.
Std. Err. 
t
upper P greater thanP > |t| 
[0.025
0.975] 
Intercept
4708.4 
173.6
27.127 
0.000
4156.0 
5260.8 
Temp
236.9 
194.1
1.221 
0.309
minus−380.7
854.4 
Blending_Time
32.9 
194.1
0.169 
0.876
minus−584.7
650.4 
Cooling_Time
minus−180.9 
194.1
minus−0.932 
0.420
minus−798.4
436.7 
Temp:Blending_Time
126.6 
194.1
0.653 
0.561
minus−490.9
744.2 
Temp:Cooling_Time
minus−105.6 
194.1
minus−0.544 
0.624
minus−723.2
511.9 
Blending_Time:Cooling_Time 
minus−162.1 
194.1
minus−0.835 
0.465
minus−779.7
455.4 
array allow us to test for non-linearity in the response surface by comparing the 
average responses at the center points, e.g., for viscosity (4135.5) with the average 
of the responses on the corners of the cube (4851.6). The difference between the 
averages of 716.1, in an analysis considering third order interactions as noise, is 
not found signiﬁcant at the 1% level of signiﬁcance (Table 6.9). None of the other 
effects was found signiﬁcant at that level. The analysis shows that no signiﬁcant 
non-linearity is observed. 

6.4 Quality by Design in the Pharmaceutical Industry
247 
Fig. 6.4 Cube display of 
predicted viscosity response 
Table 6.9 Analysis considering third order interactions as noise to test for non-linearity 
# add additional feature that identifies the center point 
FFdesign['center'] = [1 if t == 0 else 0 for t in FFdesign['Temp']] 
# and include it in the model 
formula = 'Viscosity ~ (Temp + Blending_Time + Cooling_Time)**2 + center' 
ff_model_center = smf.ols(formula=formula, data=FFdesign).fit() 
Coef.
Std. Err. 
t
P >  |t| 
[0.025
0.975] 
Intercept
4851.6
72.1
67.266 
0.000
4541.3 
5162.0 
Temp
236.9
72.1
3.284 
0.082 
−73.5
547.2 
Blending_Time
32.9
72.1
0.456 
0.693 
−277.5
343.2 
Cooling_Time
−180.9
72.1
−2.508 
0.129 
−491.2
129.5 
Temp:Blending_Time
126.6
72.1
1.756 
0.221 
−183.7
437.0 
Temp:Cooling_Time
−105.6
72.1
−1.464 
0.281 
−416.0
204.7 
Blending_Time:Cooling_Time 
−162.1
72.1
−2.248 
0.154 
−472.5
148.2 
Center
−716.1 
161.3 
−4.440 
0.047 
−1410.0 
−22.2 
6.4.3 
A Quality by Design Case Study: The Desirability 
Function 
The design space we are seeking is simultaneously addressing requirements on 8 
responses named: (1) Active assay, (2) In vitro lower (3) In vitro upper (4) D90 (5) A 
assay (6) B assay (7) Viscosity, and (8) pH. Our goal is to identify operating ranges 
of temperature, blending time, and cooling time that guarantee that all 8 responses 
are within speciﬁcation limits. To achieve this objective, we apply a popular solution 
called the desirability function (Derringer and Suich 1980). Other techniques exist 
such as principal components analysis and non-linear principal components (Figini 
et al. 2010). 
In order to combine the 8 responses simultaneously, we ﬁrst compute a desirabil-
ity function using the characteristics of each response upper Y Subscript i Baseline left parenthesis x right parenthesis comma i equals 1 comma ellipsis comma 8Yi(x), i = 1, . . . , 8. For each 
response, upper Y Subscript i Baseline left parenthesis x right parenthesisYi(x), the univariate desirability function d Subscript i Baseline left parenthesis upper Y Subscript i Baseline right parenthesisdi(Yi) assigns numbers between 
0 and 1 to the possible values of upper Y Subscript iYi, with d Subscript i Baseline left parenthesis upper Y Subscript i Baseline right parenthesis equals 0di(Yi) = 0 representing a completely 

248
6 Quality by Design 
Table 6.10 Deﬁnition of calculation of desirability functions for the 8 responses 
# define functions that generate a variety of profiles 
# note that the function returns a function profile(x) 
def rampProfile(lower, upper, reverse=False): 
def wrapped(x): 
condlist = [x < lower, x >= lower, x > upper] 
funclist = [0, lambda x: (x-lower)/(upper-lower), 1] 
if reverse: 
funclist = [1, lambda x: (upper-x)/(upper-lower), 0] 
return np.piecewise(x, condlist, funclist) 
return wrapped 
def triangleProfile(lower, middle, upper): 
def wrapped(x): 
condlist = [x < lower, x >= lower, x >= middle, x >= upper] 
funclist = [0, lambda x: (x-lower)/(middle-lower), 
lambda x: (upper-x) / (upper-middle), 0] 
return np.piecewise(x, condlist, funclist) 
return wrapped 
desirabilityProfiles = { 
'Active_Assay': rampProfile(95, 105), 
'In_Vitro_Lower': rampProfile(80, 125), 
'In_Vitro_Upper': rampProfile(110, 135, reverse=True), 
'D90': triangleProfile(1, 1.5, 2), 
'A_Assay': rampProfile(95, 105), 
'B_Assay': rampProfile(95, 105), 
'Viscosity': triangleProfile(4000, 5000, 5750), 
'pH': triangleProfile(4.7, 5.2, 5.6), 
} 
undesirable value of upper Y Subscript iYi and d Subscript i Baseline left parenthesis upper Y Subscript i Baseline right parenthesis equals 1di(Yi) = 1 representing a completely desirable or ideal 
response value. The desirability functions for the 8 responses are deﬁned and shown 
graphically in Table 6.10. For active assay, we want to be above 95% and up to 
105%. Assay values below 95% yield desirability of zero, and assay above 105% 
yield desirability of 1. We describe the desirability using a function that is zero 
below 95, then linearly increases until 105 to one, and remains a constant one above 
105. For in vitro upper, we do not want to be above 135% and therefore describe it 
using a reverse ramp function. In vitro lower and the two Assay A and B responses 
are deﬁned using similar desirability functions. Our target for D90 is 1.5 with results 
above 2 and below 1 having zero desirability. We describe this desirability proﬁle 
using a triangle function. Similar proﬁles are deﬁned for viscosity and pH. 
The desirability functions scale the various responses to a value between 0 and 1. 
We can assess the design space by an overall desirability index using the geometric 
mean of the individual desirabilities: 
Desirability Index equals left bracket d 1 left parenthesis upper Y 1 right parenthesis asterisk d 2 left parenthesis upper Y 2 right parenthesis asterisk ellipsis d Subscript k Baseline left parenthesis upper Y Subscript k Baseline right parenthesis right bracket Superscript StartFraction 1 Over k EndFractionDesirability Index = [d1(Y1) ∗d2(Y2) ∗ . . . dk(Yk)]
1 
k 

6.4 Quality by Design in the Pharmaceutical Industry
249 
Table 6.11 Calculation of individual and overall desirability for a given target setting 
def overallDesirability(x): 
x = np.array(list(x)) 
if any(xi == 0 for xi in x): # handle 0-desirability case 
return 0 
return stats.gmean(x) 
def calculateDesirability(target, models, desirabilityProfiles): 
targetPredictions = {response: model.predict(target).values[0] 
for response, model in models.items()} 
# determine overall desirability for targetPredictions 
targetDesirabilities = {response: float(desirabilityProfiles[response] 
(value)) for response, value in targetPredictions. 
items()} 
targetDesirabilities['overall'] = overallDesirability(targetDesirabilities. 
values()) 
return { 
'individual': pd.DataFrame({'predictions': targetPredictions, 
'desirability': targetDesirabilities}), 
'overall': overallDesirability(targetDesirabilities.values()), 
} 
models = {} 
for response in desirabilityProfiles: 
formula = f'{response} ~ (Temp + Blending_Time + Cooling_Time)**2' 
models[response] = smf.ols(formula=formula, data=df).fit() 
target = pd.DataFrame( 
{'Temp': 65, 'Blending_Time': 2.5, 'Cooling_Time': 150}, 
index=['target']) 
targetDesirabilities = calculateDesirability(target, models, 
desirabilityProfiles) 
targetDesirabilities['overall'] 
0.33976276040548653 
with k denoting the number of measures (in our case, k equals 8k = 8). Notice that if any 
response Yi is completely undesirable (d Subscript i Baseline left parenthesis upper Y Subscript i Baseline right parenthesis equals 0di(Yi) = 0), then the overall desirability is 
zero. 
Example 6.4 After training individual models for all the responses using the data 
from Sect. 6.4.2, we can determine individual and overall desirability indices for a 
setting of Temp equals= 65, Blending Time equals= 2.5, and Cooling Time equals= 150. In Table 6.11, 
we can see that setting gives us an overall desirability index equals= 0.34. 
We can also determine the effect of variability in the factor levels, similar to 
what is implemented in the piston simulator. We describe the variability of each 
factor level using normal distributions. 

250
6 Quality by Design 
Fig. 6.5 Variability of the factor levels around target settings 
variability = { 
'Temp': stats.norm(loc=65, scale=3), 
'Blending_Time': stats.norm(loc=2.5, scale=0.6), 
'Cooling_Time': stats.norm(loc=150, scale=30), 
} 
Figure 6.5 shows the three normal distributions for the settings of Temp, 
Blending Time, and Cooling Time. This variability is then transferred to the 8 
responses and to the overall desirability index. 
# simulate data with variability around the target settings 
np.random.seed(1) 
random_df = pd.DataFrame({ 
factor: variability[factor].rvs(5000) 
for factor in target 
}) 
predictions = {response: model.predict(random_df) 
for response, model in models.items()} 
variableDesirabilities = pd.DataFrame({ 
response: desirability(predictions[response].values) 
for response, desirability in desirabilityProfiles.items() 
}) 
variableDesirabilities['overall'] = \ 
variableDesirabilities.apply(overallDesirability, axis=1) 

6.4 Quality by Design in the Pharmaceutical Industry
251 
Fig. 6.6 Distribution of individual and overall desirabilities due to variability in factor levels: 
Tempequals= 65, Blending timeequals= 2.5, and Cooling timeequals= 150 
Figure 6.6 shows the distribution of the individual and overall desirabilities 
due to variability in factor levels. The graphs for individual responses show two 
distributions. The outline shows the distributions of the predicted response; the 
overlayed ﬁlled histogram is the distribution weighted by desirability. Viscosity and 
both in vitro responses show the smallest variability relative to the experimental 
range. As many of the simulated in vitro upper predictions are close to the 
undesirable 135% threshold, we see that the distribution of this response is strongly 
affected by desirability. In fact, about 14% of the simulated responses have an 
overall desirability of zero.
black medium square■
6.4.4 
A Quality by Design Case Study: The Design Space 
To conclude the analysis, we study the design space using the models ﬁtted to the 
experimental data with main effects and two-way interactions. Figure 6.7 shows 
contours of the models. The previous analysis showed that of all quality attributes, 
in vitro upper leads to the lowest undesirability. The dark shaded areas identify 
the region where the undesirability is zero (greater 135); in the light shaded area, 
its value is between 133 and 135. We can see that our initial target settings of 
temperature at 65, blending time of 2.5, and cooling time of 150 are in the light 
shaded area. If we increase the temperature slightly to 67, our process moves into a 

252
6 Quality by Design 
(a) 
(b) 
Fig. 6.7 Contour plots with overlay of eight responses for the initial (a) and changed (b) target  
settings. The shaded areas identify the areas with very low desirability of in vitro upper 
more desirable region with respect to in vitro upper. Indeed, our overall desirability 
increases from 0.34 to 0.38. 
Using these visualizations of the design space, we can identify operating regions 
with higher desirability. Once approved by the regulator, these areas of operations 
are deﬁned as the normal range of operation. Under QbD, any changes within these 
regions do not require pre-approval, only post change notiﬁcation. This change in 
regulatory strategy is considered a breakthrough in traditional inspection doctrines 
and provides a signiﬁcant regulatory relief. 
An essential component of QbD submissions to the FDA is the design of a control 
strategy. Control is established by determining expected results and tracking actual 
results in the context of expected results. The expected results are used to set up 

6.5 Tolerance Designs
253 
upper and control limits. The use of simulations, as presented in Fig. 6.7, can be used 
for this purpose. A ﬁnal step in a QbD submission is to revise the risk assessment 
analysis. At this stage, the experts agreed that with the deﬁned design space and 
an effective control strategy accounting for the variability presented in Fig. 6.6, all  
risks in Table 6.5 have been reset as low. 
In this section, we covered the essential steps in preparing a QbD submission. 
We focused on the application of statistically designed experiments and show how 
they can be used to achieve robust and optimized process design standard operating 
procedures. 
6.5 Tolerance Designs 
Usually, parts installed in systems, such as resistors, capacitors, transistors, and 
other parts of mechanical nature, have some deviations in their characteristics from 
the nominal ones. For example, a resistor with a nominal resistance of 8200 [Ohm] 
will have an actual resistance value that is a random deviate around the nominal 
value. Parts are classiﬁed according to their tolerances. Grade A could be with a 
tolerance interval plus or minus±1% of the nominal value. Grade B of plus or minus±5%, grade C of plus or minus±10%, 
etc. Parts with high-grade tolerances are more expensive than low-grade ones. Due 
to the non-linear dependence of the system output (performance characteristic) 
on the input values of its components, not all component variances contribute 
equally to the variance of the output. We have also seen that the variances of the 
components affect the means of the output characteristics. It is therefore important 
to perform experiments to determine which tolerance grade should be assigned to 
each component. We illustrate such a problem in the following example. 
Example 6.5 Taguchi (1987, Vol. 1, pp. 379) describes a tolerance design for a 
circuit that converts alternating current of 100 [V] AC into a direct current of 220 
[V] DC. This example is based on an experiment performed in 1974 at the Shin 
Nippon Denki Company. 
The output of the system, Y, depends in a complicated manner on 17 factors. The 
simulator PowerCircuitSimulation (mistat package) was designed to experiment 
with this system. 
In this example, we use PowerCircuitSimulation to execute a fractional replica-
tion of 2 Superscript 13 minus 8213−8 to investigate the effects of two tolerance grades of 13 components, 
10 resistors, and 3 transistors, on the output of the system. The two design levels 
for each factor are the two tolerance grades. For example, if we specify for a given 
factor a tolerance of 10%, then the experiment at level 1 will use at level 1 a tolerance 
of 5% and at level 2 a tolerance of 10%. The value of a given factor is simulated 
according to a normal distribution with mean at the nominal value of that factor. 
The standard deviation is 1/6 of the length of the tolerance interval. For example, if 
the nominal value for factor A is 8200 [Ohm], and the tolerance level is 10%, the 
standard deviation for level 1 is 136.67 [Ohm] and for level 2 is 273.33 [Ohm]. 

254
6 Quality by Design 
Table 6.12 Factor levels for 
the 2 Superscript 13 minus 8213−8 design 
A 
B 
C 
D 
E 
F 
G 
H 
I 
J 
K 
L 
M 
1
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
2
2 
1 
1 
1 
1 
2 
2 
2 
2 
2 
2 
1 
1 
3
1 
2 
1 
1 
1 
2 
2 
2 
1 
1 
1 
2 
2 
4
2 
2 
1 
1 
1 
1 
1 
1 
2 
2 
2 
2 
2 
5
1 
1 
2 
1 
1 
2 
1 
1 
2 
2 
1 
2 
2 
6
2 
1 
2 
1 
1 
1 
2 
2 
1 
1 
2 
2 
2 
7
1 
2 
2 
1 
1 
1 
2 
2 
2 
2 
1 
1 
1 
8
2 
2 
2 
1 
1 
2 
1 
1 
1 
1 
2 
1 
1 
9
1 
1 
1 
2 
1 
1 
2 
1 
2 
1 
2 
2 
1 
10 
2 
1 
1 
2 
1 
2 
1 
2 
1 
2 
1 
2 
1 
11 
1 
2 
1 
2 
1 
2 
1 
2 
2 
1 
2 
1 
2 
12 
2 
2 
1 
2 
1 
1 
2 
1 
1 
2 
1 
1 
2 
13 
1 
1 
2 
2 
1 
2 
2 
1 
1 
2 
2 
1 
2 
14 
2 
1 
2 
2 
1 
1 
1 
2 
2 
1 
1 
1 
2 
15 
1 
2 
2 
2 
1 
1 
1 
2 
1 
2 
2 
2 
1 
16 
2 
2 
2 
2 
1 
2 
2 
1 
2 
1 
1 
2 
1 
17 
1 
1 
1 
1 
2 
1 
1 
2 
1 
2 
2 
1 
2 
18 
2 
1 
1 
1 
2 
2 
2 
1 
2 
1 
1 
1 
2 
19 
1 
2 
1 
1 
2 
2 
2 
1 
1 
2 
2 
2 
1 
20 
2 
2 
1 
1 
2 
1 
1 
2 
2 
1 
1 
2 
1 
21 
1 
1 
2 
1 
2 
2 
1 
2 
2 
1 
2 
2 
1 
22 
2 
1 
2 
1 
2 
1 
2 
1 
1 
2 
1 
2 
1 
23 
1 
2 
2 
1 
2 
1 
2 
1 
2 
1 
2 
1 
2 
24 
2 
2 
2 
1 
2 
2 
1 
2 
1 
2 
1 
1 
2 
25 
1 
1 
1 
2 
2 
1 
2 
2 
2 
2 
1 
2 
2 
26 
2 
1 
1 
2 
2 
2 
1 
1 
1 
1 
2 
2 
2 
27 
1 
2 
1 
2 
2 
2 
1 
1 
2 
2 
1 
1 
1 
28 
2 
2 
1 
2 
2 
1 
2 
2 
1 
1 
2 
1 
1 
29 
1 
1 
2 
2 
2 
2 
2 
2 
1 
1 
1 
1 
1 
30 
2 
1 
2 
2 
2 
1 
1 
1 
2 
2 
2 
1 
1 
31 
1 
2 
2 
2 
2 
1 
1 
1 
1 
1 
1 
2 
2 
32 
2 
2 
2 
2 
2 
2 
2 
2 
2 
2 
2 
2 
2 
As mentioned earlier, the control factors are 10 resistors labeled A-J and 3 
transistors labeled K-M. The nominal levels of these factors are: 
StartLayout 1st Row 1st Column Blank 2nd Column upper A equals 8200 comma upper B equals 220000 comma upper C equals 1000 comma upper D equals 33000 comma upper E equals 56000 comma upper F equals 5600 comma 2nd Row 1st Column Blank 2nd Column upper G equals 3300 comma upper H equals 58.5 comma upper I equals 1000 comma upper J equals 120 comma upper K equals 130 comma upper L equals 100 comma upper M equals 130 EndLayout period
A = 8200, B  = 220000, C  = 1000, D  = 33000, E  = 56000, F  = 5600, 
G = 3300, H  = 58.5, I  = 1000, J  = 120, K  = 130, L  = 100, M  = 130 
. 
The levels of the 13 factors in the 2 Superscript 13 minus 8213−8 fractional replicate are given in 
Table 6.12. 

6.6 Case Studies
255 
We perform this experiment on the computer, using PowerCircuitSimulation. We  
wish to ﬁnd a treatment combination (run) that yields a small MSE at low cost per 
circuit. We will assume that grade B parts (5% tolerance) cost $1 and grade C parts 
(10% tolerance) cost $0.5. In order to obtain sufﬁciently precise estimates of the 
MSE, we perform at each run a simulated sample of size n equals 100n = 100. The results of 
this experiment are given in Table 6.13. 
np.random.seed(1) 
# Prepare design 
tolerances = [f'tl{c}' for c in 'ABCDEFGHIJKLM'] 
factors = {tl: [5, 10] for tl in tolerances} 
Design = doe.frac_fact_res(factors, 4) 
# Randomize and create replicates 
nrepeat = 100 
Design = Design.sample(frac=1).reset_index(drop=True) 
Design = Design.loc[Design.index.repeat(nrepeat)].reset_index(drop=True) 
# Run simulation 
simulator = mistat.PowerCircuitSimulation(**{k: list(Design[k]) for k in Design}) 
result = simulator.simulate() 
result = mistat.simulationGroup(result, nrepeat) 
Design['response'] = result['volts'] 
Design['group'] = result['group'] 
# calculate mean, standard deviation, and total cost for each group 
def groupAggregation(g): 
# calculate cost of design (tolerance 10 = 0.5, tolerance 5 = 1) 
groupTolerances = g.iloc[0,:][tolerances] 
tc = 0.5 * sum(groupTolerances == 10) + 1 * sum(groupTolerances == 5) 
return { 
'mean': g['response'].mean(), 
'STD': g['response'].std(), 
'MSE': g['response'].var(ddof=0), 
'TC': tc, 
**groupTolerances, 
} 
results = pd.DataFrame(list(Design.groupby('group').apply(groupAggregation))) 
We see that the runs having small mean squared errors (MSE) are 1, 12, 13, and 
31. Among these, the run with the smallest total cost (TC) is 13. We could, however, 
accept the slightly higher cost of run 31 to get the lower MSE.
black medium square■
6.6 Case Studies 
6.6.1 
The Quinlan Experiment 
This experiment was carried out at Flex Products in Midvale Ohio (Quinlan 1985). 
Flex Products is a subcontractor of General Motors, manufacturing mechanical 
speedometer cables. The basic cable design has not changed for ﬁfteen years, 
and General Motors had experienced many disappointing attempts at reducing the 
speedometer noise level. Flex Products decided to apply the off-line quality control 

256
6 Quality by Design 
Table 6.13 Performance characteristics of tolerance design experiment 
Mean
STD
MSE
TC 
24 
230.32 
3.5976 
12.8130 
13 
27 
229.43 
4.3597 
18.8173 
10 
29 
229.80 
4.4109 
19.2617 
10 
31 
230.99 
4.6552 
21.4542 
10 
15 
230.85 
4.7367 
22.2119 
10 
30 
230.91 
4.7774 
22.5954 
9 
7
229.93 
4.8452 
23.2415 
10 
3
230.03 
4.8563 
23.3477 
9 
21 
229.73 
4.9575 
24.3307 
9 
4
229.60 
5.1094 
25.8444 
9 
10 
229.93 
5.2639 
27.4314 
9 
23 
231.29 
5.3464 
28.2980 
10 
14 
230.08 
5.4595 
29.5084 
10 
13 
230.70 
5.5159 
30.1212 
9 
32 
229.79 
5.5959 
31.0005 
9 
2
230.59 
5.5979 
31.0229 
9 
Mean
STD
MSE
TC 
25 
230.24 
5.7056 
32.2279 
9 
22 
229.73 
5.7172 
32.3599 
9 
1
229.50 
5.7244 
32.4413 
9 
17 
230.44 
5.7973 
33.2726 
9 
8
230.88 
5.8922 
34.3706 
10 
20 
230.46 
5.9800 
35.4024 
10 
19 
230.27 
6.0310 
36.0092 
10 
6
230.46 
6.1385 
37.3047 
10 
11 
230.35 
6.2171 
38.2664 
9 
16 
229.50 
6.2814 
39.0608 
10 
18 
230.19 
6.2949 
39.2299 
10 
12 
229.77 
6.5423 
42.3742 
9 
28 
229.52 
6.6241 
43.4396 
10 
5
230.01 
6.8145 
45.9726 
9 
9
230.50 
7.1933 
51.2260 
10 
26 
230.26 
7.2022 
51.3528 
6 
and involve in the project customers, production personnel, and engineers with 
experience in the product and manufacturing process. A large experiment involving 
15 factors was designed and completed. The data showed that much improvement 
could be gained by few simple changes. The results were dramatic, and the loss per 
unit was reduced from $2.12 to $0.13 by changing the braid type, the linear material, 
and the braiding tension. 
We proceed to describe the experiment using an eight points template: 
1. Problem Deﬁnition. The product under investigation is an extruded thermo-
plastic speedometer casing used to cover the mechanical speedometer cable on 
automobiles. Excessive shrinkage of the casing is causing noise in the mechanical 
speedometer cable assembly. 
2. Response variable. The performance characteristic in this problem is the 
post-extrusion shrinkage of the casing. The percent shrinkage is obtained by 
measuring approximately 600 mm of casing that has been properly conditioned 
left parenthesis upper A right parenthesis(A), placing that casing in a two-hour heat soak in an air circulating oven, 
reconditioning the sample, and measuring the length left parenthesis upper B right parenthesis(B). Shrinkage is computed 
as: Shrinkage equals 100 times left parenthesis upper A minus upper B right parenthesis divided by upper A= 100 × (A − B)/A. 
3. Control factors: 
4. Factor Levels. Existing (1)—Changed (2). 
5. Experimental Array. upper L 16 left parenthesis 2 Superscript 15 Baseline right parenthesisL16(215) orthogonal array. 
6. The Number of Replications. Four random samples of 600mm from the 3000 
feet manufactured at each experimental run. 

6.6 Case Studies
257 
Liner Process:
A: Liner O.D. 
B: Liner die  
C: Liner material 
D: Liner line speed 
Wire braiding:
E: Wire braid type 
F: Braiding tension 
G: Wire diameter 
H: Liner tension 
I: Liner temperature 
Coating process:
J: Coating material 
K: Coating dye type 
L: Melt temperature 
M: Screen pack 
N: Cooling method 
O: Line speed 
7. Data Analysis. Signal-to-noise ratios left parenthesis upper S upper N right parenthesis(SN) are computed for each experimental 
run and analyzed using main effect plots and an ANOVA. Savings are derived 
from loss function computations. 
The signal-to-noise formula used by Quinlan is 
eta equals minus 10 log Subscript 10 Baseline left parenthesis StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts y Subscript i Superscript 2 Baseline right parenthesis periodη = −10 log10

1 
n 
n

i=1 
y2 
i

. 
For example, experimental run number 1 produced shrinkage factors of: 0.49, 
0.54, 0.46, 0.45. The SN is 6.26. The objective is to maximize the SN by proper 
setup of the 15 controllable factors. 
Table 6.14 shows the factor levels and the SN values, for all 16 experimental 
runs. 
Notice that Quinlan, by using the orthogonal array upper L 16 left parenthesis 2 Superscript 15 Baseline right parenthesisL16(215) for all the ﬁfteen 
factors, assumes that there are no signiﬁcant interactions. If this assumption is 
correct, then the main effects of the ﬁfteen factors are: 
Figure 6.8 presents the main effects plot for this experiment. Factors E and G 
seem to be most inﬂuential. These main effects, as deﬁned in Ch. 3, are the  
regression coefﬁcients of SN on the design coefﬁcients plus or minus 1±1. As mentioned in 
Chap. 5, these are sometimes called “half effects.” Only the effects of factors E 
and G are signiﬁcant. If the assumption of no interaction is wrong, and all the 
ﬁrst order interactions are signiﬁcant, then, as shown in the linear graph LGSubscript 11 
in Fig. 6.2, only the effects of factors A, B, D, H and O are not confounded. 
The effects of the other factors are confounded with ﬁrst order interactions. The 
main effect of factor E is confounded with the interaction AD, and that of G is 
confounded with HO. In order to conﬁrm the ﬁrst hypothesis that all interactions 
are negligible, an additional experiment should be performed, in which factors E 
and G will be assigned columns that do not represent possible interactions (like 

258
6 Quality by Design 
Table 6.14 Factor levels and SN values 
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
SN 
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
6.26 
2
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
4.80 
3
1
1
1
2
2
2
2
1
1
1
1
2
2
2
2
21.04 
4
1
1
1
2
2
2
2
2
2
2
2
1
1
1
1
15.11 
5
1
2
2
1
1
2
2
1
1
2
2
1
1
2
2
14.03 
6
1
2
2
1
1
2
2
2
2
1
1
2
2
1
1
16.69 
7
1
2
2
2
2
1
1
1
1
2
2
2
2
1
1
12.91 
8
1
2
2
2
2
1
1
2
2
1
1
1
1
2
2
15.05 
9
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
17.67 
10
2
1
2
1
2
1
2
2
1
2
1
2
1
2
1
17.27 
11
2
1
2
2
1
2
1
1
2
1
2
2
1
2
1
6.82 
12
2
1
2
2
1
2
1
2
1
2
1
1
2
1
2
5.43 
13
2
2
1
1
2
2
1
1
2
2
1
1
2
2
1
15.27 
14
2
2
1
1
2
2
1
2
1
1
2
2
1
1
2
11.20 
15
2
2
1
2
1
1
2
1
2
2
1
2
1
1
2
9.24 
16
2
2
1
2
1
1
2
2
1
1
2
1
2
2
1
4.68 
Factor
A
B
C
D
E
F
G
H 
Main effect
minus−1.145
0.29
1.14
minus−0.86
3.60
1.11
2.37
minus−0.82 
Factor
I
J
K
L
M
N
O 
Main effect
0.49
minus−0.34
minus−1.19
0.41
0.22
0.28
0.22 
columns 1 and 2 of Table 6.4). The results of the additional experiment should 
reconﬁrm the conclusions of the original experiment. 
8. Results. As a result of Quinlan’s analysis, factors E and G were changed. This 
reduced the average shrinkage index from 26% to 5%. The shrinkage standard 
deviation was also reduced, from 0.05 to 0.025. This was considered a substantial 
success in quality improvement. 
6.6.2 
Computer Response Time Optimization 
The experiment described in Pao et al. (1985) was part of an extensive effort to 
optimize a UNIX operating system running on a VAX 11-780 machine. The machine 
had 48 user terminal ports, two remote job entry links, four megabytes of memory, 
and ﬁve disk drives. The typical number of users logged on at a given time was 
between 20 to 30: 
1. Problem Deﬁnition. Users complained that the system performance was very 
poor, especially in the afternoon. The objective of the improvement effort was to 
both minimize response time and reduce variability in response. 

6.6 Case Studies
259 
Fig. 6.8 Main effects plot for Quinlan experiment 
2. Response variable. In order to get an objective measurement of the response 
time, two speciﬁc representative commands called standard and trivial were 
used. The standard command consisted of creating, editing, and removing a ﬁle. 
The trivial command was the UNIX system “date” command. Response times 
were measured by submitting these commands every 10 m and clocking the time 
taken for the system to complete their execution. 
3. Control factors: 
StartLayout 1st Row 1st Column upper A 2nd Column colon Disk drives 2nd Row 1st Column upper B 2nd Column colon File distribution 3rd Row 1st Column upper C 2nd Column colon Memory size 4th Row 1st Column upper D 2nd Column colon System buffers 5th Row 1st Column upper E 2nd Column colon Sticky bits 6th Row 1st Column upper F 2nd Column colon KMCs used 7th Row 1st Column upper G 2nd Column colon INODE table entries 8th Row 1st Column upper H 2nd Column colon Other system tables EndLayout
A: Disk drives  
B: File distribution 
C: Memory size 
D: System buffers 
E: Sticky bits 
F: KMCs used  
G: INODE table entries 
H: Other system tables 
4. Factor Levels. 

260
6 Quality by Design 
Factor
Levels 
A: RM05 & RP06
4 & 1  4 & 2  4 & 3  
B: File distribution
a
b
c 
C: Memory size (MB)
4
3
3.5 
D: System buffers
1/5
1/4
1/3 
E: Sticky bits
0
3
8 
F: KMCs used
2
0 
G: INODE table entries 
400
500
600 
H: Other system tables 
a
b
c 
5. Experimental Array. The design was an orthogonal array upper L 18 left parenthesis 3 Superscript 8 Baseline right parenthesisL18(38). This and the 
mean response are given in the following table. 
Each mean response in Table 6.15 is over n equals 96n = 96 measurements. 
6. Data Analysis. The measure of performance characteristic used was the upper S divided by upper NS/N 
ratio 
eta equals minus 10 log Subscript 10 Baseline left parenthesis StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts y Subscript i Superscript 2 Baseline right parenthesis commaη = −10 log10

1 
n 
n

i=1 
y2 
i

, 
where y Subscript iyi is the ith response time. 
Figure 6.9 is the main effects plot of these eight factors. 
We see that factors having substantial effects are A, C, D, E, and H. As a result, 
the number of disk drives was changed to 4 and 2. The system buffers were 
changed from 1/3 to 1/4. The number of sticky bits was changed from 0 to 8. 
After introducing these changes, the average response time dropped from 6.15 
(s) to 2.37 (s) with a substantial reduction in response times variability. 
Factor 
Linear 
Quadratic 
A
0.97
— 
B
0.19
minus−0.15 
C
minus−1.24 
minus−1.32 
D
minus−0.37 
minus−1.23 
E
1.72
1.86 
F
0.44
— 
G
0.17
minus−0.63 
H
0.05
1.29 

Table 6.15 Factor levels and 
mean responsea
F 
B 
C 
D 
E 
A 
G 
H 
Mean 
SN 
1
1 
1 
1 
1 
1 
1 
1 
1 
4.65
minus−14.66 
2
1 
1 
2 
2 
2 
2 
2 
2 
5.28
minus−16.37 
3
1 
1 
3 
3 
3 
3 
3 
3 
3.06
minus−10.49 
4
1 
2 
1 
1 
2 
2 
3 
3 
4.53
minus−14.85 
5
1 
2 
2 
2 
3 
3 
1 
1 
3.26
minus−10.94 
6
1 
2 
3 
3 
1 
1 
2 
2 
4.55
minus−14.96 
7
1 
3 
1 
2 
1 
3 
2 
3 
3.37
minus−11.77 
8
1 
3 
2 
3 
2 
1 
3 
1 
5.62
minus−16.72 
9
1 
3 
3 
1 
3 
2 
1 
2 
4.87
minus−14.67 
10 
2 
1 
1 
3 
3 
2 
2 
1 
4.13
minus−13.52 
11 
2 
1 
2 
1 
1 
3 
3 
2 
4.08
minus−13.79 
12 
2 
1 
3 
2 
2 
1 
1 
3 
4.45
minus−14.19 
13 
2 
2 
1 
2 
3 
1 
3 
2 
3.81
minus−12.89 
14 
2 
2 
2 
3 
1 
2 
1 
3 
5.87
minus−16.75 
15 
2 
2 
3 
1 
2 
3 
2 
1 
3.42
minus−11.65 
16 
2 
3 
1 
3 
2 
3 
1 
2 
3.66
minus−12.23 
17 
2 
3 
2 
1 
3 
1 
2 
3 
3.92
minus−12.81 
18 
2 
3 
3 
2 
1 
2 
3 
1 
4.42
minus−13.71 
a Factor A had only 2 levels. All levels 3 in the table were 
changed to level 2 
Fig. 6.9 Main effects plot 
6.6 Case Studies
261 

262
6 Quality by Design 
6.7 Chapter Highlights 
The main concepts and deﬁnitions introduced in this chapter include:
•
Design of experiments
•
Robust design
•
Quality planning
•
Quality engineering
•
Off-line quality control
•
Loss functions
•
Parameter design
•
Tolerance design
•
Response surfaces
•
Mixture designs
•
Inner array
•
Outer array
•
Linear graph
•
Signal-to-noise
•
Performance measures
•
Quality by design (QbD)
•
Design space
•
Control strategy
•
Risk management
•
Critical quality attributes (CQAs)
•
ICH Guidelines Q8-Q11
•
Desirability function
•
Current good manufacturing practices (cGMPs)
•
Desirability function 
6.8 Exercises 
Exercise 6.1 The objective is to ﬁnd the levels of the factors of the piston, which 
yield an average cycle time of 0.02 (sec). Execute a PistonSimulation, with sample 
size n = 100: 
(i) Determine which treatment combination yields the smallest 
upper M upper S upper E equals left parenthesis upper Y overbar minus 0.45 right parenthesis squared plus upper S squared periodMSE = ( ¯Y − 0.45)2 + S2. 
(ii) Determine which treatment combination yields the largest SN ratio, 
eta equals 10 log Subscript 10 Baseline left parenthesis StartFraction upper Y squared Over upper S squared EndFraction minus StartFraction 1 Over 100 EndFraction right parenthesis periodη = 10 log10
Y 2 
S2 − 1 
100

. 

6.8 Exercises
263 
What is the MSE at this treatment combination? 
The ﬁve factors that are varied are: piston weight, piston surface area, initial 
gas volume, spring coefﬁcient, and ambient temperature. The factors atmospheric 
pressure and ﬁlling gas temperature are kept constant at the midrange level. 
Exercise 6.2 Run a PistonSimulation with sample size of n = 100 and generate the 
sample means and standard deviation of the 27 = 128 treatment combinations of a 
full factorial experiment, for the effects on the piston cycle time. Perform regression 
analysis to ﬁnd which factors have signiﬁcant effects on the signal-to-noise ratio 
SN = log(( ¯X/S)2). 
Exercise 6.3 Let (X1, X2) have joint distribution with means (ξ1, ξ2) and covari-
ance matrix 
upper V equals Start 2 By 2 Matrix 1st Row 1st Column sigma 1 squared 2nd Column sigma 12 2nd Row 1st Column sigma 12 2nd Column sigma 2 squared EndMatrix periodV =
σ 2 
1 σ12 
σ12 σ 2 
2

. 
Find approximations to the expected values and variances of: 
(i) Y = X1/X2. 
(ii) Y = log(X2 
1/X2 
2). 
(iii) Y = (X2 
1 + X2 
2)1/2. 
Exercise 6.4 The relationship between the absorption ratio Y of a solid image in a 
copied paper and the light intensity X is given by the function 
upper Y equals 0.0782 plus StartFraction 0.90258 Over 1 plus 0.6969 upper X Superscript negative 1.4258 Baseline EndFraction periodY = 0.0782 +
0.90258 
1 + 0.6969X−1.4258 . 
Assuming that X has the gamma distribution G(1, 1.5), approximate the expected 
value and variance of Y. 
Exercise 6.5 Let ¯Xn and S2 
n be the mean and variance of a random sample of size 
n from a normal distribution N(μ, σ). We know that ¯Xn and S2 
n are independent, 
¯Xn ∼ N

μ, σ
√n

and S2 
n ∼ σ 2 
n−1χ2[n − 1]. Find an approximation to the expected 
value and variance of Y = log
 ¯X2 n 
S2 
n

. 
Exercise 6.6 An experiment based on an L18 orthogonal array involving eight 
factors gave the results listed in Table 6.16 (see Phadke et al. 1983). Each run had 
n = 5 replications. 
Analyze the effects of the factors of the SN ratio η = log( ¯X/S). 
Exercise 6.7 Using PistonSimulation, perform a full factorial (27), a 1/8 (27−3), 
1/4 (27−2), and 1/2 (27−1) fractional replications of the cycle time experiment. 
Estimate the main effects of the seven factors with respect to SN = log( ¯X/S) and 
compare the results obtained from these experiments. Use n = 5 replicates for each 
combination of factors. 

264
6 Quality by Design 
Table 6.16 Results of 
experiment based on L18 
orthogonal array 
Factors 
Run 
1 
2 
3 
4 
5 
6 
7 
8 
¯X
S 
1
1 
1 
1 
1 
1 
1 
1 
1 
2.500 
0.0827 
2
1 
1 
2 
2 
2 
2 
2 
2 
2.684 
0.1196 
3
1 
1 
3 
3 
3 
3 
3 
3 
2.660 
0.1722 
4
1 
2 
1 
1 
2 
2 
3 
3 
1.962 
0.1696 
5
1 
2 
2 
2 
3 
3 
1 
1 
1.870 
0.1168 
6
1 
2 
3 
3 
1 
1 
2 
2 
2.584 
0.1106 
7
1 
3 
1 
2 
1 
3 
2 
3 
2.032 
0.0718 
8
1 
3 
2 
3 
2 
1 
3 
1 
3.267 
0.2101 
9
1 
3 
3 
1 
3 
2 
1 
2 
2.829 
0.1516 
10
2 
1 
1 
3 
3 
2 
2 
1 
2.660 
0.1912 
11
2 
1 
2 
1 
1 
3 
3 
2 
3.166 
0.0674 
12
2 
1 
3 
2 
2 
1 
1 
3 
3.323 
0.1274 
13
2 
2 
1 
2 
3 
1 
3 
2 
2.576 
0.0850 
14
2 
2 
2 
3 
1 
2 
1 
3 
2.308 
0.0964 
15
2 
2 
3 
1 
2 
3 
2 
1 
2.464 
0.0385 
16
2 
3 
1 
3 
2 
3 
1 
2 
2.667 
0.0706 
17
2 
3 
2 
1 
3 
1 
2 
3 
3.156 
0.1569 
18
2 
3 
3 
2 
1 
2 
3 
1 
3.494 
0.0473 
Exercise 6.8 To see the effect of the variances of the random variables on the 
expected response, in non-linear cases, execute PistonSimulation, with n = 20 
replicates, and compare the output means to the values in Exercise 6.7. 
Exercise 6.9 Run PowerCircuitSimulation with 1% and 2% tolerances, and com-
pare the results to those of Table 6.13. 

Chapter 7 
Computer Experiments 
Preview Computer experiments are integrated in modern product and service 
development activities. Technology is providing advanced digital platforms for 
studying various properties of suggested designs, without the need to physically 
concretize them. This chapter is about computer experiments and the special 
techniques required for designing such experiments and analyzing their outcomes. 
A speciﬁc example of such experiments is the piston simulator used throughout 
the book to demonstrate statistical concepts and tools. In this simulator, random 
noise is induced on the control variables themselves, a non-standard approach 
in modeling physical phenomena. The experiments covered include space ﬁlling 
designs and Latin hypercubes. The analysis of the experimental outputs is based 
on Kriging or design and analysis of computer experiments (DACE) models. The 
chapter discusses the concept of a stochastic emulator where a model derived from 
the simulation outputs is used to optimize the design in a robust way. A special 
section is discussing several approaches to integrate the analysis of computer and 
physical experiments. 
7.1 
Introduction to Computer Experiments 
Experimentation via computer modeling has become very common in many areas of 
science and technology. In computer experiments, physical processes are simulated 
by running a computer code that generates output data for given input values. In 
physical experiments, data are generated directly from a physical process. In both 
physical and computer experiments, a study is designed to answer speciﬁc research 
questions, and appropriate statistical methods are needed to design the experiment 
and to analyze the resulting data. Chapters 5 and 6 present such methods and many 
examples. In this chapter, we focus on computer experiments and speciﬁc design 
and analysis methods relevant to such experiments. 
Supplementary Information The online version contains supplementary material available at 
(https://doi.org/10.1007/978-3-031-28482-3_7). 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3_7
265

266
7
Computer Experiments
Because of experimental error, a physical experiment will produce a different 
output for different runs at the same input settings. Computer experiments are 
deterministic, and the same inputs will always result in the same output. Thus, 
none of the traditional principles of blocking, randomization, and replication can 
be used in the design and analysis of computer experiments data. On the other hand, 
computer experiments use extensively random number generators, and these are 
described in Sect. 7.6. 
Computer experiments consist of a number of runs of a simulation code, and 
factor-level combinations correspond to a subset of code inputs. By considering 
computer runs as a realization of a stochastic process, a statistical framework is 
available both to design the experimental points and to analyze the responses. 
A major difference between computer numerical experiments and physical exper-
iments is the logical difﬁculty in specifying a source of randomness for computer 
experiments. 
The complexity of the mathematical models implemented in the computer 
programs can, by themselves, build equivalent sources of random noise. In complex 
code, a number of parameters and model choices give the user many degrees 
of freedom that provide potential variability to the outputs of the simulation. 
Examples include different solution algorithms (i.e., implicit or explicit methods for 
solving differential systems), approach to discretization intervals, and convergence 
thresholds for iterative techniques. In this very sense, an experimental error can 
be considered in the statistical analysis of computer experiments. The nature of 
the experimental error in both physical and simulated experiments is our ignorance 
about the phenomena and the intrinsic error of the measurements. Real-world phe-
nomena are too complex for the experimenter to keep under control by specifying 
all the factors affecting the response of the experiment. Even if it were possible, the 
physical measuring instruments, being not ideal, introduce problems of accuracy 
and precision. Perfect knowledge would be achieved in physical experiments only 
if all experimental factors can be controlled and measured without any error. Similar 
phenomena occur in computer experiments. A complex code has several degrees of 
freedom in its implementation that are not controllable. 
A speciﬁc case where randomness is introduced to computer experiments 
consists of the popular ﬁnite element method (FEM) programs. These models are 
applied in a variety of technical sectors such as electromagnetics, ﬂuid dynamics, 
mechanical design, and civil design. The FEM mathematical models are based on a 
system of partial differential equations deﬁned on a time–space domain for handling 
linear or non-linear, steady state, or dynamic problems. FEM software can deal with 
very complex shapes as well as with a variety of material properties, boundary 
conditions, and loads. Applications of FEM simulations require subdivision of 
the space domain into a ﬁnite number of subdomains, named ﬁnite elements, and 
solving the partial differential system within each subdomain, letting the ﬁeld 
function to be continuous on its border. 
Experienced FEM practitioners are aware that results of complex simulations 
(complex shapes, non-linear constitutive equations, dynamic problems, contacts 
among different bodies, etc.) can be sensitive to the choice of manifold model

7.1
Introduction to Computer Experiments
267
parameters. Reliability of FEM results is a critical issue for the single simulation 
and even more for a series of computer experiments. The model parameters used in 
the discretization of the geometry are likely to be the most critical. Discretization 
of the model geometry consists in a set of points (nodes of the mesh) and a set of 
elements (two-dimensional patches or three-dimensional volumes) deﬁned through 
a connectivity matrix whose rows list the nodes enclosing the elements. Many 
degrees of freedom are available to the analyst when deﬁning a mesh on a given 
model. Changing the location and the number of nodes, the shape, and the number 
of elements, an inﬁnity of meshes are obtained. Any of them will produce different 
results. How can we model the effects of different meshes on the experimental 
response? In principle, the ﬁner the discretization, the better the approximation 
of numerical solution, even if numerical instabilities may occur using very reﬁned 
meshes. Within a reasonable approximation, a systematical effect can be assigned to 
mesh density; it would be a ﬁxed-effect factor if it is included in the experiment. A 
number of topological features (node locations, element shape), which the analyst 
has no meaningful effect to assign to, are generators of random variability. One can 
assume that they are randomized along the experiment or random-effect factors with 
nuisance variance components if they are included as experimental factors. 
Mesh selection also has a direct economical impact as computational complexity 
grows with the power of the number of the elements. In the case of computer 
experiments, the problem of balancing reliability and cost of the experiment needs 
to be carefully addressed. In principle, for any output of a numerical code, the 
following deterministic model holds: 
y equals f left parenthesis bold x right parenthesis plus g left parenthesis bold x semicolon bold u right parenthesis commay = f (x) + g(x; u),
(7.1.1) 
where the function f represents the dependence of the output y on the vector 
x of experimental factors, and g describes the contribution of parameters, u, 
which are necessary for the setup of the computer model. Since the function 
g may have interactions with engineering parameters, x is also an argument of 
function g. Generally, an engineer is interested in the estimation of function 
f , while he considers g as a disturbance. In general, two options are available 
for analyzing computer experiments: (1) considering the model parameters as 
additional experimental factors or (2) ﬁxing them along the whole experiment. The 
ﬁrst option allows the estimation of the deterministic model written in (7.1.1). This  
is a good choice since the inﬂuence of both engineering and model parameters on 
the experimental response can be evaluated. This requires, however, an experimental 
effort that cannot be often affordable. Keeping every model parameter at a ﬁxed 
value in the experiment, only the ﬁrst term f of model (7.1.1) can be estimated. 
This results in a less expensive experiment but has two dangerous drawbacks: (1) 
the presence of effects of model parameters on the function g in (7.1.1) can cause a 
bias in the response and (2) the estimates of the effects of engineering parameters are 
distorted by the interactions between model and engineering parameters according 
to the function g. A different approach is to randomize along the experiment those

268
7
Computer Experiments
Table 7.1 Different models for computer experiments 
Option
Model
Model nature Advantages
Disadvantages 
Fixed model factors 
y equals f left parenthesis bold x right parenthesisy = f (x)
Deterministic Inexpensive
Possible bias 
and distortion of 
effects of 
engineering 
parameters 
Model of factor 
effects is included in 
the experiment 
y equals f left parenthesis bold x right parenthesis plus g left parenthesis bold x semicolon bold u right parenthesisy = f (x) + g(x; u)
Deterministic More accurate. 
Systematic 
effect of bold uu can 
be discovered 
More 
programming is 
required 
Randomizing model 
parameters with 
random effects 
y equals f left parenthesis bold x right parenthesis plus g Superscript asterisk Baseline left parenthesis bold x semicolon bold u right parenthesis plus epsilony = f (x) + g∗(x; u) + ϵ Stochastic
Possibility of 
calibrating 
experimental 
error. 
Even more 
programming is 
required. 
model parameters whose effects can reasonably be assumed to be normal random 
variables with zero average. In this case, the underlying model becomes a stochastic 
one: 
y equals f left parenthesis bold x right parenthesis plus g Superscript asterisk Baseline left parenthesis bold x semicolon bold u right parenthesis plus bold italic epsilon commay = f (x) + g∗(x; u) + ϵ,
(7.1.2) 
where g Superscript asteriskg∗in (7.1.2) is a function that represents the mixed contribution between 
engineering and ﬁxed-effects model parameters, after random-effects model param-
eters have been accounted for in building the experimental error. Any model 
parameter that is suspected to have a substantial interaction with some engineering 
parameters should be included as experimental factor so that the systematic 
deviation of effects of such engineering parameters is prevented. Randomization 
of model parameters yields two simultaneous beneﬁts. On the one hand, the model 
has acquired a random component equivalent to the experimental error of physical 
experiments; and in this way, the rationale of replications is again justiﬁed so that a 
natural measurement scale for effects is introduced and usual statistical signiﬁcance 
tests can be adopted. On the other hand, without any increase of experimental 
effort, possible interactions between randomized model parameters and engineering 
parameters do not give rise to distortion of effects of engineering parameters or 
experimental factors. Moreover, it becomes possible to tune the experimental error 
of the computer experiment to that of the experimental error of a related physical 
experiment. In the case where several u parameters are present, it is likely that 
the normality assumption for random errors is reasonable. Table 7.1, adapted from 
Romano and Vicario (2002), summarizes a variety of approaches to computer 
experiments that are presented below in some detail. 
One of the modeling methods applied to computer experiments data is Kriging 
also called Gaussian process models. Section 7.3 is dedicated to Kriging methods 
for data analysis. Throughout this chapter, we refer to the piston simulator that 
we already used in the previous chapters. A Python version of this simulator,

7.1
Introduction to Computer Experiments
269
PistonSimulation, is included in the mistat package. We describe next the 
mathematical foundation of the piston simulator. 
Example 7.1 The piston cycle time data are generated by software simulating a 
piston moving within a cylinder. The piston’s linear motion is transformed into 
circular motion by connecting a linear rod to a disk. The faster the piston moves 
inside the cylinder, the quicker the disk rotation and therefore the faster the engine 
will run. The piston’s performance is measured by the time it takes to complete one 
cycle, in seconds. The purpose of the simulator is to study the causes of variability 
in piston cycle time. The following factors (listed below with their units and ranges) 
affect the piston’s performance: 
M
Piston weight (Kg), 30–60 
S
Piston surface area (msquared2), 0.005–0.020 
upper V 0V0
Initial gas volume (mcubed3), 0.002–0.010 
k
Spring coefﬁcient left parenthesis upper N divided by m right parenthesis(N/m), 1000–5000 
upper P 0P0
Atmospheric pressure left parenthesis upper N divided by m squared right parenthesis(N/m2), 90 comma 000 minus 110 comma 00090,000−110,000
T
Ambient temperature left parenthesis Superscript ring Baseline upper K right parenthesis(◦K), 290–296 
upper T 0T0
Filling gas temperature left parenthesis Superscript ring Baseline upper K right parenthesis(◦K), 340–360 
These factors affect the cycle time via a chain of non-linear equations: 
Cycle Time equals 2 pi StartRoot StartStartFraction upper M OverOver k plus upper S squared StartFraction upper P 0 upper V 0 Over upper T 0 EndFraction StartFraction upper T Over upper V squared EndFraction EndEndFraction EndRoot commaCycle Time = 2π

M
k + S2 P0V0
T0
T
V 2
,
(7.1.3) 
where 
upper V equals StartFraction upper S Over 2 k EndFraction left parenthesis StartRoot upper A squared plus 4 k StartFraction upper P 0 upper V 0 Over upper T 0 EndFraction upper T EndRoot minus upper A right parenthesis and upper A equals upper P 0 upper S plus 19.62 upper M minus StartFraction k upper V 0 Over upper S EndFraction periodV = S
2k

A2 + 4k P0V0
T0
T −A

and A = P0S + 19.62M −kV0
S .
(7.1.4) 
Randomness in cycle time is induced by generating observations for factors set up 
around design points with noise added to the nominal values. 
We can run the simulator by ﬁxing a speciﬁc factor-level combination or by using 
statistically designed experimental arrays. In this chapter, the arrays we will refer 
to are called, in general, space ﬁlling experiments. The simulator was used in the 
context of statistical process control (Sect. 2.1). We use it here in the context of 
statistically designed computer experiments. The next section deals with designing 
computer experiments. We will discuss there space ﬁlling designs that are speciﬁc to 
computer experiments where the factor-level combinations can be set freely, without 
physical constraints at speciﬁc levels. The section after that, Sect. 7.3, deals with 
models used in the analysis of computer experiments. These models are called 
Kriging, Dace, or Gaussian process models. They will be introduced at a general 
level designed to provide basic understanding of their properties, without getting 
into their theoretical development.
black medium square■

270
7
Computer Experiments
7.2 
Designing Computer Experiments 
Experimentation via computer modeling has become very common. We introduce 
here two popular designs for such experiments: the uniform design and the Latin 
hypercube design. 
Suppose that the experimenter wants to estimate muμ, the overall mean of the 
response y on the experimental domain X. The best design for this purpose is one 
whose empirical distribution approximates the uniform distribution. This idea arose 
ﬁrst in numerical integration methods for high-dimensional problems, called quasi-
Monte Carlo methods that were proposed in the early 1960s. 
The discrepancy function, upper D left parenthesis dot right parenthesisD(·), or measure of uniformity, quantiﬁes the differ-
ence between the uniform distribution and the empirical distribution of the design. 
Designs with minimum discrepancy are called uniform designs. There are different 
possible forms of discrepancy functions, depending on the norm used to measure 
the difference between the uniform distribution and the empirical distribution of the 
design. 
In general, the discrepancy function is a Kolmogorov–Smirnov type goodness-
of-ﬁt statistic. For estimating muμ in the overall mean model, the uniform design has 
optimal average mean squared error assuming random h and optimal maximum 
mean square error assuming deterministic h. This implies that the uniform design is 
a type of robust design. 
Latin hypercube designs are easy to generate. They achieve maximum uniformity 
in each of the univariate margins of the design region, thus allowing the exper-
imenter to use models that are capable of capturing the complex dependence of 
the response variable on the input variables. Another reason that contributes to the 
popularity of Latin hypercube designs is that they have no repeated runs. 
In computer experiments, repeated runs do not provide additional information 
since running a deterministic computer code twice yields the identical output. Latin 
hypercube designs are a very large class of designs that, however, do not necessarily 
perform well in terms of criteria such as orthogonality or space ﬁlling. 
An n times mn × m matrix upper D equals left parenthesis d Subscript i j Baseline right parenthesisD = (dij) is called a Latin hypercube design of n runs for 
m factors if each column of D is a permutation of 1 comma ellipsis comma n1, . . . , n. What makes Latin 
hypercube designs distinctly different from other designs is that every factor in a 
Latin hypercube design has the same number of levels as the run size. 
Let y equals f left parenthesis x 1 comma ellipsis comma x Subscript m Baseline right parenthesisy = f (x1, . . . , xm) be a real-valued function with m variables deﬁned on 
the region given by 0 less than or equals x Subscript j Baseline less than or equals 10 ≤xj ≤1 for j equals 1 comma ellipsis comma mj = 1, . . . , m. The function represents the 
deterministic computer model in the case of computer experiments or the integrand 
in the case of numerical integration. There are two natural ways of generating design 
points based on a given Latin hypercube. The ﬁrst is through 
x Subscript i j Baseline equals left parenthesis d Subscript i j Baseline minus 0.5 right parenthesis divided by n commaxij = (dij −0.5)/n,
with the n points given by left parenthesis x Subscript i Baseline 1 Baseline comma ellipsis comma x Subscript i m Baseline right parenthesis(xi1, . . . , xim) with i equals 1 comma ellipsis comma ni = 1, . . . , n. The other is through

7.2
Designing Computer Experiments
271
Fig. 7.1 Three Latin hypercube designs with 5 runs and 2 factors 
x Subscript i j Baseline equals left parenthesis d Subscript i j Baseline minus u Subscript i j Baseline right parenthesis divided by n commaxij = (dij −uij)/n,
with the n points given by (x Subscript i Baseline 1 Baseline comma ellipsis comma x Subscript i m Baseline right parenthesisxi1, . . . , xim) with i equals 1 comma ellipsis comma ni
= 1, . . . , n, where u Subscript i juij are 
independent random variables with a common uniform distribution on left parenthesis 0 comma 1 right bracket(0, 1]. The  
difference between the two methods can be seen as follows. When projected 
onto each of the m variables, both methods have the property that one and only 
one of the n design points falls within each of the n small intervals deﬁned by 
left bracket 0 comma 1 divided by n right parenthesis comma left bracket 1 divided by n comma 2 divided by n right parenthesis comma ellipsis comma left bracket left parenthesis n minus 1 right parenthesis divided by n comma 1 right bracket[0, 1/n), [1/n, 2/n), . . . , [(n −1)/n, 1]. The ﬁrst method gives the mid-points of 
these intervals, while the second gives the points that uniformly distributed in their 
corresponding intervals. Figure 7.1 presents three Latin hypercube designs of n equals 5n = 5
runs for m equals 2m = 2 factors. 
Although they are all Latin hypercube designs, design D1 provides a higher 
coverage of the design region than design D2 or D3. This raises the need of 
developing speciﬁc methods for selecting better Latin hypercube designs. Basic 
Latin hypercube designs are very easy to generate. By simply combining several 
permutations of 1 comma ellipsis comma n1, . . . , n, one obtains a Latin hypercube design. There is no 
restriction whatsoever on the run size n and the number m of factors. Since a Latin 
hypercube design has n distinct levels in each of its factors, it achieves the maximum 
uniformity in each univariate margin. Two useful properties follow from this simple 
fact: (1) the maximum number of levels, a Latin hypercube design presents the 
experimenter with the opportunity of modeling the complex dependence of the 
response variable on each of the input variables and (2) there is no repeated levels 
in each factor. Since running computer code twice at the same setting of input 
variables produces the same output, using repeated runs in computer experiments 
is necessarily a waste of resources. 
By deﬁnition, a Latin hypercube does not guarantee any property in two-
or higher-dimensional margins. It is therefore up to the user to ﬁnd the “right 
permutations” so that the resulting design has certain desirable properties in two or 
higher dimensions. One simple strategy is to use a random Latin hypercube design 
in which the permutations are selected randomly. This helps eliminate the possible 
systematic patterns in the resulting design, but there is no guarantee that the design 
will perform well in terms of other useful design criteria. A Latin hypercube design

272
7
Computer Experiments
will provide a good coverage of the design region if all the points are farther apart, 
i.e., no two points are too close to each other. This idea can be formally developed 
using the maximin distance criterion, according to which designs should be selected 
by maximizing min Underscript i equals j Endscripts d left parenthesis p Subscript i Baseline comma p Subscript j Baseline right parenthesismini=j d(pi, pj), where d left parenthesis p Subscript i Baseline comma p Subscript j Baseline right parenthesisd(pi, pj) denotes the distance between 
design points p Subscript ipi and p Subscript jpj. Euclidean distance is commonly used, but other distance 
measures are also useful. 
Example 7.2 To design a space ﬁlling experiment, we can use various functions 
from the mistat package. The following Python code uses the space_ﬁlling_lhs 
to create a design and uses it with the piston simulator. 
from mistat.design import doe 
np.random.seed(1) 
Factors = { 
'm': [30, 60], 
's': [0.005, 0.02], 
'v0': [0.002, 0.01], 
'k': [1_000, 5_000], 
'p0': [90_000, 110_000], 
't': [290, 296], 
't0': [340, 360], 
} 
Design = doe.lhs(Factors, num_samples=14) 
# Randomize and create replicates 
nrepeat = 50 
Design = Design.sample(frac=1).reset_index(drop=True) 
Design = Design.loc[Design.index.repeat(nrepeat)].reset_index(drop=True) 
kwargs = {c: list(Design[c]) for c in Design.columns} 
simulator = mistat.PistonSimulator(**kwargs) 
result = simulator.simulate() 
result = mistat.simulationGroup(result, nrepeat) 
mean_result = result.groupby('group').mean() 
The design and average response time are shown in Table 7.2 and Fig. 7.2. The  
graphs show that the design covers the space well.
black medium square■
The next section is focused on models used for analyzing computer experiments. 
7.3 
Analyzing Computer Experiments1 
As already mentioned in Sect. 7.1, Kriging was developed for modeling spatial data 
in geostatistics. 
Matheron (1963) named this method after D. G. Krige, a South African mining 
engineer who in the 1950s developed empirical methods for estimating true ore 
grade distributions based on sample ore grades. At the same time, the same ideas 
were developed in meteorology under Gandin (1963) in the Soviet Union. Gandin
1 This section includes mathematical derivations and can be skipped without loss of continuity. 

7.3
Analyzing Computer Experiments
273
Table 7.2 Mean response from piston simulator using a Latin hypercube design 
Group
m
s
v0
k
p0
t
t0
Seconds 
1
43.5
0.0137
0.00691
4831
92,407
291.32
346.7
0.054 
2
45.3
0.0065
0.00338
1731
108,910
292.57
344.0
0.037 
3
53.4
0.0058
0.00910
1405
95,564
294.36
352.4
0.165 
4
38.5
0.0123
0.00639
2842
99,869
293.21
353.7
0.048 
5
59.9
0.0176
0.00288
2263
108,091
291.94
350.1
0.015 
6
30.9
0.0111
0.00745
3060
93,058
290.04
359.4
0.066 
7
50.2
0.0188
0.00411
1086
97,083
295.04
356.2
0.020 
8
41.4
0.0102
0.00976
3486
90,210
290.94
357.3
0.110 
9
48.4
0.0079
0.00533
4088
101,575
292.70
340.3
0.073 
10
38.9
0.0125
0.00549
4704
98,214
293.61
342.7
0.044 
11
56.3
0.0086
0.00870
2108
104,854
295.96
349.0
0.108 
12
54.3
0.0194
0.00200
2434
100,379
290.52
355.6
0.009 
13
34.3
0.0147
0.00779
3761
103,593
294.26
348.2
0.051 
14
32.9
0.0159
0.00434
4395
106,605
295.41
344.3
0.023 
named the method optimal interpolation. The central feature of Kriging models is 
that spatial trends can be modeled using spatial correlation structures, similar to time 
series models, in which observations are assumed to be dependent. Spatial models, 
however, need to be more ﬂexible than time series models, as there is dependence 
in a multitude of directions. In general, the approach is a method of optimal spatial 
linear prediction based on minimum mean squared error. The use of Kriging for 
modeling data from computer experiments was originally labeled DACE (Design 
and Analysis of Computer Experiments) by Sacks et al. (1989). Kriging models are 
also known as Gaussian process models. Computer experiments may have many 
input variables, whereas spatial models have just 2 or 3. The DACE algorithm uses 
a model that treats the deterministic output of a computer code as the realization of 
a stochastic process. This nonparametric model simultaneously identiﬁes important 
variables and builds a predictor that adapts to non-linear and interaction effects in 
the data. 
Assume there is a single scalar output y left parenthesis x right parenthesisy(x), which is a function of a d-
dimensional vector of inputs, x. The deterministic response y left parenthesis x right parenthesisy(x) is treated as a 
realization of a random function 
upper Y left parenthesis x right parenthesis equals beta plus upper Z left parenthesis x right parenthesis periodY(x) = β + Z(x).
(7.3.1) 
The random process upper Z left parenthesis x right parenthesisZ(x) is assumed to have mean 0 and covariance function 
Cov left parenthesis upper Z left parenthesis x Subscript i Baseline right parenthesis comma upper Z left parenthesis x Subscript j Baseline right parenthesis right parenthesis equals bold italic sigma squared upper R left parenthesis x Subscript i Baseline comma x Subscript j Baseline right parenthesisCov(Z(xi), Z(xj)) = σ 2R(xi, xj)
(7.3.2) 
between upper Z left parenthesis x Subscript i Baseline right parenthesisZ(xi) and upper Z left parenthesis x Subscript j Baseline right parenthesisZ(xj) at two vector-valued inputs x Subscript ixi and x Subscript jxj, where sigma squaredσ 2 is the 
process variance and upper R left parenthesis x Subscript i Baseline comma x Subscript j Baseline right parenthesisR(xi, xj) is the correlation.

274
7
Computer Experiments
Fig. 7.2 Latin hypercube design for piston simulator 
DACE is using the correlation function: 
upper R left parenthesis x Subscript i Baseline comma x Subscript j Baseline right parenthesis equals product Underscript k equals 1 Overscript d Endscripts exp left parenthesis minus theta Subscript k Baseline vertical bar x Subscript i k Baseline minus x Subscript j k Baseline vertical bar Superscript p Super Subscript k Superscript Baseline right parenthesis commaR(xi, xj) =
d
k=1
exp(−θk | xik −xjk|pk),
(7.3.3) 
where theta Subscript k Baseline greater than or equals 0θk ≥0 and 0 less than or equals p Subscript k Baseline less than or equals 20 ≤pk ≤2. 
The basic idea behind this covariance is that values of Y for points “near” each 
other in the design space should be more highly correlated than for points “far” from 
each other. Thus, we should be able to estimate the value of upper Y left parenthesis x right parenthesisY(x) at a new site by 
taking advantage of observed values at sites that have a high correlation with the 
new site. The parameters in the correlation function determine which of the input 
variables are important in measuring the distance between two points. For example,

7.3
Analyzing Computer Experiments
275
a large value of theta Subscript kθk means that only a small neighborhood of values on this variable 
is considered to be “close” to a given input site and will typically correspond to an 
input with a strong effect. 
All the unknown parameters are estimated using maximum likelihood estimation 
(MLE). Since the global maximization is very problematic from a computa-
tional perspective, a pseudo-maximization algorithm is applied using a “stepwise” 
approach, where at each step the parameters for one input factor are “free” and all 
the rest are equal. 
Given the correlation parameters thetaθ and p, the  MLE of betaβ is 
ModifyingAbove bold italic beta With caret equals left parenthesis bold upper J Superscript upper T Baseline bold upper R Subscript upper D Superscript negative 1 Baseline bold upper J right parenthesis Superscript negative 1 Baseline left parenthesis bold upper J Superscript upper T Baseline bold upper R Subscript upper D Superscript negative 1 Baseline y right parenthesis comma ˆβ = (JT R−1
D J)−1(JT R−1
D y),
(7.3.4) 
where J is a vector of ones and upper R Subscript upper DRD is the n times nn × n matrix of correlations upper R left parenthesis x Subscript i Baseline comma x Subscript j Baseline right parenthesisR(xi, xj). 
The generalized least squares estimator, and the MLE, of bold italic sigma squaredσ 2 is 
ModifyingAbove bold italic sigma With caret squared equals StartFraction left parenthesis y minus bold upper J ModifyingAbove beta With caret right parenthesis Superscript upper T Baseline bold upper R Subscript upper D Superscript negative 1 Baseline left parenthesis y minus bold upper J ModifyingAbove beta With caret right parenthesis Over n EndFraction period ˆσ 2 = (y −J ˆβ)T R−1
D (y −J ˆβ)
n
.
(7.3.5) 
The best linear unbiased predictor (BLUP) at an untried x is 
ModifyingAbove y With caret left parenthesis x right parenthesis equals ModifyingAbove beta With caret plus bold r Superscript upper T Baseline left parenthesis x right parenthesis ModifyingAbove bold upper R With caret Subscript upper D Superscript negative 1 Baseline left parenthesis y minus Underine upper J ModifyingAbove beta With caret right parenthesis comma ˆy(x) = ˆβ + rT (x) ˆR−1
D (y −J ˆβ),
(7.3.6) 
where r left parenthesis x right parenthesis equals left bracket upper R left parenthesis x 1 comma x right parenthesis comma ellipsis comma upper R left parenthesis x Subscript n Baseline comma x right parenthesis right bracket Superscript upper Tr(x) = [R(x1, x), . . . , R(xn, x)]T is the vector of correlations between Z’s 
at the design points and at the new point x. The BLUP interpolates the observed 
output at sites x that are in the training data. 
Example 7.3 We can continue with the result from Example 7.2. We used a Latin  
hypercube design to deﬁne 14 settings for the piston simulator. The result of the 
simulation is given in Table 7.2. 
The Python package pyKriging implements kriging models for a larger 
number of predictors. 
from pyKriging.krige import kriging 
random.seed(1) 
outcome = 'seconds' 
predictors = ['m', 's', 'v0', 'k', 'p0', 't', 't0'] 
model = kriging(mean_result[predictors].values, mean_result[outcome].values) 
model.train() 
We can assess the goodness of ﬁt of the kriging model using leave-one-out cross-
validation. Leave-one-out cross-validation removes an observation from the dataset, 
builds a model with the remaining data, and predicts the left out data point. This is 
repeated for all observations. The following Python code determines the leave-one-
out predictions for each data point.

276
7
Computer Experiments
Fig. 7.3 Leave-one-out cross-validation of kriging model 
def looValidation(data, seed=123): 
random.seed(seed) 
jackknife 
= [] 
for i, row in data.iterrows(): 
subset = data.drop(i) 
model = kriging(subset[predictors].values, subset[outcome].values) 
model.train() 
jackknife.append({ 
'actual': row[outcome], 
'predicted': model.predict(row[predictors].values), 
}) 
return pd.DataFrame(jackknife) 
validation = looValidation(mean_result) 
Using the calculated predicted and actual values, we get the following leave-one-
out performance metrics. 
from sklearn import metrics 
MAE = metrics.mean_absolute_error(validation['actual'], validation['predicted']) 
R2 = metrics.r2_score(validation['actual'], validation['predicted']) 
print(f'MAE = {MAE:.4f}') 
print(f'r2 {R2:.3f}', ) 
MAE = 0.0098 
r2 0.897 
Both metrics show that the model performs well. In Fig. 7.3, we compare the 
observed value with the value predicted from the leave-one-out models. Points lying 
close to the line of equality indicate a good ﬁt of the model since the observed data 
points are well predicted by the model. 
We can derive marginal effect of factors on cycle time using the kriging model. 
The result of this analysis is shown in Fig. 7.4. It conﬁrms again that only v0 and

7.4
Stochastic Emulators
277
Fig. 7.4 Marginal effect of factors on cycle time derived from the kriging model 
Fig. 7.5 Latin hypercube design for piston simulator 
s have a strong effect on the average cycle time; all other factors have only little 
effect on cycle time. Figure 7.5 shows the dependence of the cycle time across the 
full range of v0 and s.
black medium square■
7.4 
Stochastic Emulators 
Traditional engineering practice augments deterministic design system predic-
tions with factors of safety or design margins to provide some assurance of 
meeting requirements in the presence of uncertainty and variability in modeling

278
7
Computer Experiments
assumptions, boundary conditions, manufacturing, materials, and customer usage. 
Modern engineering practice is implementing quality by design methods to account 
for probability distributions of component or system performance characteristics. 
Chapter 4 provided several such examples, including the robust design approach 
developed by Genichi Taguchi in Japan. At Pratt and Whitney, in the USA, Grant 
Reinman and his team developed a methodology labeled design for variation (DFV) 
that incorporates the same principles (Reinman et al. 2012). In this chapter, we 
focus on an essential element of modern quality by design engineering and computer 
experiments. 
The new experimental framework of computer simulators has stimulated the 
development of new types of experimental designs and methods of analysis that 
are tailored to these studies. The guiding idea in computer simulation experimental 
design has been to achieve nearly uniform coverage of the experimental region. The 
most commonly used design has been the so-called Latin hypercube presented in 
Sect. 7.2. In Latin hypercube designs, each factor is given a large number of levels, 
an option that is virtually impossible in physical experiments but very easy when 
experimenting on a simulator. 
In using computer experiments for robust design problems, outcome variation is 
induced via uncertainty in the inputs. The most direct way to assess such variation 
is to generate simulator output for a moderate to large sample of input settings 
(see Sect. 7.1). However, if the simulator is slow and/or expensive, such a scheme 
may not be practical. The stochastic emulator paradigm, also called metamodel, 
provides a simple solution by replacing the simulator with an emulator for the bulk 
of the computations. It was introduced in Bates et al. (2006). The key steps of the 
stochastic emulator approach are as follows: 
1. Begin with a Latin hypercube (or other space ﬁlling) design of moderate size. 
2. Use the simulator to generate data at points in the design. 
3. Model the simulator data to create an emulator, called the stochastic emulator. 
4. Use cross-validation to verify that the emulator accurately represents the simula-
tor. 
5. Generate a new space ﬁlling design. Each conﬁguration in this design is a 
potential nominal setting at which we will assess properties of the output 
distribution. 
6. At each conﬁguration in the new design, sample a large number of points from 
the noise factors and compute output data from the stochastic emulator. 
7. Construct statistical models that relate features of the output distribution to the 
design factor settings. These models might themselves be emulators. 
This approach can dramatically reduce the overall computational burden by using 
the stochastic emulator, rather than the simulator, to compute the results in step 6. 
Stochastic emulators are a primary quality by design tool in organizations that have 
successfully incorporated simulation experiments in the design of drug products, 
analytical methods, and scale-up processes.

7.5
Integrating Physical and Computer Experiments
279
7.5 
Integrating Physical and Computer Experiments2 
Information from expert opinion, computer experiments, and physical experiments 
can be combined in a simple regression model of the form: 
bold upper Y equals f left parenthesis bold upper X comma bold italic beta right parenthesis plus bold italic epsilon periodY = f (X, β) + ϵ.
(7.5.1) 
In this model, bold upper XX represents the design space corresponding, the vector bold italic betaβ represents 
the values of the model coefﬁcients, and bold upper YY represents the k observations, for 
example, of method resolution. This is achieved by modeling physical experimental 
data as 
bold upper Y Subscript p Baseline tilde upper N left parenthesis bold upper X Subscript p Baseline bold italic beta comma bold italic sigma squared bold upper I right parenthesis commaYp ∼N(Xpβ, σ 2I),
(7.5.2) 
where bold italic sigma squaredσ 2 is the experimental variance representing the uncertainty of responses due 
to experimental conditions and measurement system. 
Instead of relying solely on the physical experiments to establish the distribution 
of the response in the design space, we start by ﬁrst eliciting estimates from expert 
opinion and, later, add results from computer experiments. Results from physical 
experiments are then superimposed on these two sources of information. Suppose 
there are e expert opinions. Expert opinions on the values of bold italic betaβ can be described as 
quantiles of 
bold upper Y 0 tilde upper N left parenthesis bold upper X 0 bold italic beta plus bold italic delta 0 comma bold italic sigma squared bold italic upper Sigma 0 right parenthesis commaY0 ∼N(X0β + δ0, σ 20),
(7.5.3) 
where bold italic delta 0δ0 is the expert-speciﬁc location bias. 
Assuming the following prior distributions for the unknown parameters bold italic betaβ and 
sigma squaredσ 2: 
StartLayout 1st Row bold italic beta vertical bar bold italic sigma squared tilde upper N left parenthesis bold italic mu 0 comma bold italic sigma squared bold upper C 0 right parenthesis EndLayout StartLayout 2nd Row bold italic sigma squared tilde upper I upper G left parenthesis alpha 0 comma gamma 0 right parenthesis comma EndLayoutβ | σ 2 ∼N(μ0, σ 2C0)
(7.5.4) 
σ 2 ∼ IG(α0, γ0),
(7.5.5) 
where upper N left parenthesis bold italic mu comma bold italic sigma squared right parenthesisN(μ, σ 2) stands for a normal distribution and upper I upper G left parenthesis bold italic alpha comma bold italic gamma right parenthesisIG(α, γ ) is the inverse 
gamma distribution that we will meet again in Sect. 10.1. Using Bayes’ theorem, 
the resulting posterior distribution of bold italic betaβ becomes 
pi left parenthesis bold italic beta vertical bar bold italic sigma squared comma bold italic eta comma bold y 0 right parenthesis tilde upper N left parenthesis left parenthesis bold upper X prime 0 bold italic upper Sigma 0 Superscript negative 1 Baseline bold upper X 0 plus bold upper C 0 Superscript negative 1 Baseline right parenthesis Superscript negative 1 Baseline bold z comma bold italic sigma squared left parenthesis bold upper X prime 0 bold italic upper Sigma 0 Superscript negative 1 Baseline bold upper X 0 plus bold upper C 0 Superscript negative 1 Baseline right parenthesis Superscript negative 1 Baseline right parenthesisπ(β | σ 2, η, y0) ∼N

X′
0−1
0 X0 + C−1
0
−1
z, σ 2 
X′
0−1
0 X0 + C−1
0
−1	
(7.5.6) 
with 
bold z equals bold upper X prime 0 bold italic upper Sigma prime 0 left parenthesis bold y 0 minus bold italic delta 0 right parenthesis plus bold upper C 0 Superscript negative 1 Baseline bold italic mu periodz = X′
0′
0(y0 −δ0) + C−1
0 μ.
(7.5.7)
2 This and the following section include mathematical derivations and can be skipped without loss 
of continuity. 

280
7
Computer Experiments
The computer experimental data can be described as 
bold upper Y Subscript c Baseline tilde upper N left parenthesis bold upper X Subscript c Baseline bold italic beta plus bold italic delta Subscript c Baseline comma sigma squared bold italic sigma Subscript c Baseline right parenthesis periodYc ∼N(Xcβ + δc, σ 2σ c).
(7.5.8) 
Combining these results with the expert opinion posteriors, we derive a second 
posterior distribution, and then adding estimates from physical experiments through 
Markov Chain Monte Carlo, we calculate the ﬁnal distribution for bold italic betaβ. 
Stage 1 left parenthesis bold upper Y 0 right parenthesis right arrow Stage 2 left parenthesis bold upper Y 0 plus bold upper Y Subscript c Baseline right parenthesis right arrow Stage 3 left parenthesis bold upper Y 0 plus bold upper Y Subscript c Baseline plus bold upper Y Subscript p Baseline right parenthesis periodStage 1 (Y0) →Stage 2 (Y0 + Yc) →Stage 3 (Y0 + Yc + Yp).
(7.5.9) 
A related approach called “variable ﬁdelity experiments” has been proposed in 
Huang and Allen (2005) to combine results from experiments conducted at various 
levels of sophistication. 
Consider, for example, combining simple calculations in Excel, to results from a 
mixing simulation software and actual physical mixing experiments. 
The combined model is 
upper Y left parenthesis bold x comma l right parenthesis equals bold f 1 left parenthesis bold x right parenthesis prime bold italic beta 1 plus bold f 1 left parenthesis bold x right parenthesis prime bold italic beta 2 plus upper Z Subscript sys Baseline left parenthesis bold x comma l right parenthesis plus epsilon Subscript means Baseline left parenthesis l right parenthesis commaY(x, l) = f1(x)′β1 + f1(x)′β2 + Zsys(x, l) + ϵmeans(l),
(7.5.10) 
where l equals 1 comma ellipsis comma ml = 1, . . . , m is the ﬁdelity level of the experimental system, upper Z Subscript sys Baseline left parenthesis bold x comma l right parenthesisZsys(x, l) is the 
systematic error, and epsilon Subscript means Baseline left parenthesis l right parenthesisϵmeans(l) is the random error (l equals 1l = 1 corresponds to the real 
system). There are also primary terms and potential terms, and only the primary 
terms, bold f 1 left parenthesis bold x right parenthesisf1(x), are included in the regression model. 
Assuming that the covariance matrix bold upper VV is known and bold upper YY is a vector that contains 
data from n experiments, the GLS estimator of beta 1β1 is 
ModifyingAbove beta With caret Subscript 1 Baseline equals left parenthesis bold upper X prime 1 bold upper V Superscript negative 1 Baseline bold upper X 1 right parenthesis Superscript negative 1 Baseline bold upper X prime 1 bold upper V Superscript negative 1 Baseline bold upper Y period ˆβ1 = (X′
1V−1X1)−1X′
1V−1Y.
(7.5.11) 
Both the integrated model, combining expert opinion with simulation, and physical 
experiments and the variable ﬁdelity level experiments have proven useful in 
practical applications where experiments are conducted in different conditions and 
prior experience has been accumulated. 
7.6 
Simulation of Random Variables 
7.6.1 
Basic Procedures 
Simulation is an artiﬁcial technique of generating on the computer a sequence of 
random numbers, from a given distribution, in order to explore the properties of 
a random phenomenon. Observation of random phenomena often takes long time 
of data collection, until we have sufﬁcient information for analysis. For example, 
consider patients arriving at random times to a hospital, to obtain a certain treatment

7.6
Simulation of Random Variables
281
in clinical trials. It may take several months until we have a large enough sample for 
analysis. Suppose that we assume that the epochs of arrival of the patients follow a 
Poisson process. If we can generate on the computer a sequence of random times, 
which follow a Poisson process, we might be able to predict how long the trial 
will continue. Physically, we can create random numbers by ﬂipping a “balanced” 
coin many times, throwing dice, or shufﬂing cards. One can use a Geiger counter 
to count how many particles are emitted from a decaying radio-active process. All 
these methods are slow and cannot be used universally. The question is, how can we 
generate random numbers on the computer, which follow a given distribution. 
The key for random numbers generation is the well-known result that, if a 
random variable X has a continuous distribution F, then upper F left parenthesis upper X right parenthesisF(X) is uniformly 
distributed on the interval left parenthesis 0 comma 1 right parenthesis(0, 1). Accordingly, if one can generate at random a 
variable upper U tilde upper R left parenthesis 0 comma 1 right parenthesisU ∼R(0, 1), then the random variable upper X equals upper F Superscript negative 1 Baseline left parenthesis upper U right parenthesisX = F −1(U) is distributed 
according to F. In order to generate a uniformly distributed random variable, 
computer programs such as Python and others generally apply an algorithm that 
yields, after a while (asymptotically), uniformly distributed results. In Python, we 
can calculate a uniform random variable upper R left parenthesis alpha comma beta right parenthesisR(α, β) using uniform(alphaα,betaβ) from the 
random standard library package. While this is sufﬁcient for many use cases, it is 
better to use the stats module in the scipy package. It provides a large number 
of continuous and discrete distributions. With stats.uniform.rvs(loc=alphaα, 
scale=beta minus alphaβ −α, size=n), we can generate an array of n random variables. The 
default values for alphaα and betaβ are 0 and 1. The standard uniform on (0,1) can therefore 
be obtained with stats.uniform.rvs(size=n). Note that we need to specify the number 
of random variables using the size keyword. 
We provide below a few examples of random numbers generation: 
1. The exponential distribution with mean 1 divided by lamda1/λ is upper F left parenthesis x right parenthesis equals 1 minus exp left brace minus lamda x right braceF(x) = 1 −exp{−λx}, for  0 less than or equals x less than or equals normal infinity0 ≤
x ≤∞. Thus, if  U has a uniform distribution on (0,1), then upper X equals minus left parenthesis 1 divided by lamda right parenthesis ln left parenthesis 1 minus upper U right parenthesisX = −(1/λ) ln(1 −
U) has an exponential distribution with mean 1 divided by lamda1/λ. In Python, we can generate 
an exponential (lamdaλ) with the function stats.expon.rvs(scale=1/lamdaλ, 
size=n). Note that we use the inverse of lamdaλ here. 
2. If X is distributed according to exponential with lamda equals 1λ = 1, then upper X Superscript 1 divided by alphaX1/α is distributed 
like Weibull with shape parameter alphaα and scale parameter 1. The scipy function 
stats.weibull_min.rvs(alphaα) generates such a random variable. If there is also a scale 
parameter betaβ, we can use stats.weibull_min(alphaα, scale=betaβ). 
3. A standard normal random variable can be generated by the Uth quantiles 
upper Phi Superscript negative 1 Baseline left parenthesis upper U right parenthesis
−1(U). In Python, these random variables are generated by stats.norm.rvs(). To  
generate the distribution upper N left parenthesis mu comma sigma right parenthesisN(μ, σ), use  stats.norm.rvs(loc=muμ, scale=sigmaσ, size=n). 
4. Random numbers following a discrete distribution can be similarly generated. 
For example, generating a random number from the binomial(20, 0.6) can be 
done by the Python function stats.binom.rvs(20, 0.6). If we wish to generate 
a random sample of n realizations, we add the keyword argument size. For  
example, stats.binom.rvs(20, 0.6, size=10).

282
7
Computer Experiments
7.6.2 
Generating Random Vectors 
Generating random vectors following a given multivariate distribution is done in 
stages. Suppose we are given a k-dimensional vector having a multivariate distribu-
tion, with density p left parenthesis x 1 comma x 2 comma ellipsis comma x Subscript k Baseline right parenthesisp(x1, x2, . . . , xk). If the variables are mutually independent, we 
can generate independently k variables according to their marginal densities. This is 
the simplest case. On the other hand, if the variables are dependent but exchangeable 
(all permutations of the components yield the same joint density function), we 
generate the ﬁrst variable according to its marginal density. Given a realization of 
upper X 1X1, say x 1x1, we generate a value of upper X 2X2 according to the conditional density p left parenthesis x 2 vertical bar x 1 right parenthesisp(x2|x1), 
then the value of upper X 3X3 according to the conditional density p left parenthesis x 3 vertical bar x 1 comma x 2 right parenthesisp(x3|x1, x2), and so on. We 
illustrate this process on the bivariate normal distribution. This distribution has ﬁve 
parameters mu equals upper E left brace upper X right braceμ = E{X}, sigma 1 squared equals upper V left brace upper X right braceσ 2
1 = V {X}, eta equals upper E left brace upper Y right braceη = E{Y}, sigma 2 squared equals upper V left brace upper Y right braceσ 2
2 = V {Y}, and rho equals upper C o r r left parenthesis upper X comma upper Y right parenthesisρ = Corr(X, Y). 
In step 1, we generate 
upper X tilde upper N left parenthesis mu comma sigma 1 squared right parenthesis periodX ∼N(μ, σ 2
1 ).
In step 2, we generate 
upper Y tilde upper N left parenthesis eta plus left parenthesis sigma 1 rho divided by sigma 2 right parenthesis left parenthesis upper X minus mu right parenthesis comma sigma 2 squared left parenthesis 1 minus rho squared right parenthesis right parenthesis periodY ∼N

η + (σ1ρ/σ2)(X −μ), σ 2
2 (1 −ρ2)

.
In Fig. 7.6, we present the generation of 1000 standard bivariate normal vectors 
(mu equals eta equals 0μ = η = 0, sigma 1 equals sigma 2 equals 1σ1 = σ2 = 1, rho equals 0.8ρ = 0.8), using the following Python code. 
Fig. 7.6 Standard bivariate normal distributed random vector

7.6
Simulation of Random Variables
283
np.random.seed(1) 
def standardBivariateNorm(rho, Ns): 
X = stats.norm.rvs(size=Ns) 
Y = rho*X + np.sqrt(1-rho**2)*stats.norm.rvs(size=Ns) 
return pd.DataFrame({'x': X, 'y': Y}) 
standardBivariateNorm(0.5, 1000).plot.scatter('x', 'y', alpha=0.5, color='gray') 
plt.show() 
The package scipy also provides a number of multivariate distributions, 
e.g., stats.multivariate_normal. We can create a sample of random 1000 standard 
bivariate normal vectors with this method as follows. 
rv = stats.multivariate_normal.rvs(mean=(0, 0), cov=[[1, 0.5], [0.5, 1]], 
size=1000) 
pd.DataFrame(rv, columns=('x', 'y')).plot.scatter('x', 'y') 
plt.show() 
7.6.3 
Approximating Integrals 
Let g left parenthesis upper X comma upper Y right parenthesisg(X, Y) be a function of a random vector left parenthesis upper X comma upper Y right parenthesis(X, Y) having a bivariate distribution, 
with a joint p.d.f. p left parenthesis x comma y right parenthesisp(x, y). The expected value of g left parenthesis upper X comma upper Y right parenthesisg(X, Y), if upper E left brace StartAbsoluteValue g left parenthesis upper X comma upper Y right parenthesis EndAbsoluteValue right brace less than normal infinityE{|g(X, Y)|} < ∞, is  
upper E left brace g left parenthesis upper X comma upper Y right parenthesis right brace equals integral Subscript negative normal infinity Superscript normal infinity Baseline integral Subscript negative normal infinity Superscript normal infinity Baseline g left parenthesis x comma y right parenthesis p left parenthesis x comma y right parenthesis normal d x normal d y periodE{g(X, Y)} =

 ∞
−∞

 ∞
−∞
g(x, y)p(x, y) dx dy.
According to the strong law of large numbers, under the above condition of absolute 
integrability, 
upper E left brace g left parenthesis upper X comma upper Y right parenthesis right brace equals limit Underscript n right arrow normal infinity Endscripts StartFraction 1 Over n EndFraction sigma summation Underscript j equals 1 Overscript n Endscripts g left parenthesis upper X Subscript j Baseline comma upper Y Subscript j Baseline right parenthesis comma a period s periodE{g(X, Y)} = lim
n→∞
1
n
n

j=1
g(Xj, Yj),
a.s.
Accordingly, the mean of g left parenthesis upper X comma upper Y right parenthesisg(X, Y) in very large samples approximates its expected 
value. The following is an example. 
Example 7.4 Let g left parenthesis upper X comma upper Y right parenthesis equals exp left parenthesis upper X plus upper Y right parenthesisg(X, Y) = exp(X + Y), where StartSet left parenthesis upper X Subscript j Baseline comma upper Y Subscript j Baseline right parenthesis comma j equals 1 comma ellipsis comma n EndSet{(Xj, Yj), j = 1, . . . , n} is a 
sample from the bivariate standard normal distribution. Using moment generating 
functions of the normal distribution, we obtain that 
upper E left brace e Superscript theta left parenthesis upper X plus upper Y right parenthesis Baseline right brace equals upper E left brace e Superscript theta upper X Baseline upper E left brace e Superscript theta upper Y Baseline vertical bar upper X right brace right brace equals exp left brace left parenthesis 1 plus rho right parenthesis theta squared right brace periodE{eθ(X+Y)} = E{eθXE{eθY |X}} = exp{(1 + ρ)θ2}.
Thus, for theta equals 1θ = 1 and rho equals 0.8ρ = 0.8, we obtain the exact result that upper E left brace e Superscript upper X plus upper Y Baseline right brace equals 6.049647E{eX+Y } = 6.049647. 
On the other hand, for a sample of size 100, we obtain 
mean left parenthesis exp left brace upper X plus upper Y right brace comma n equals 100 right parenthesis equals 7.590243 periodmean(exp{X + Y}, n = 100) = 7.590243.

284
7
Computer Experiments
Fig. 7.7 Integral approximations for different sample sizes (10 repeats) 
This sample size is not sufﬁciently large. For n equals 1000n = 1000, we obtain 
mean left parenthesis exp left brace upper X plus upper Y right brace comma n equals 1000 right parenthesis equals 5.961779 periodmean(exp{X + Y}, n = 1000) = 5.961779.
This result is already quite close to the exact value. These are just two possible 
approximations for the integral. Figure 7.7 shows the distribution of calculated 
approximations for sample sizes of 100, 1000, and 10,000. The distribution of 
approximations for size 100 is very large. It improves for 1000, and at a sample 
size of 10,000, the approximations are very close to the exact value. 
black medium square■
7.7 
Chapter Highlights 
The main concepts and tools introduced in this chapter include: 
• 
Simulation 
• 
Space ﬁlling designs 
• 
Latin hypercubes 
• 
Kriging 
• 
Metamodel 
• 
Emulator 
• 
Stochastic emulator 
• 
Physical experiments 
• 
Bayesian hierarchical model 
• 
Fidelity level

7.8
Exercises
285
• 
Integrating physical and computer experiments 
• 
Random number generator 
7.8 
Exercises 
Exercise 7.1 The birthday problem states that if there are more than 22 people at 
a birthday party, the probability that at least two people have the same birthday is 
greater than 0.5. Write a Python program to simulate this problem. Show that if 
there are more than 22 people in the party, the probability is greater than 1/2 that at 
least 2 will have birthdays on the same day. 
Exercise 7.2 The Deming funnel experiment was designed to show that an inappro-
priate reaction to common cause variation will make matters worse. Common cause 
and special causes affecting processes over time have been discussed in Chaps. 2–4. 
In the actual demonstration, a funnel is placed above a circular target. The 
objective is to drop a marble through the funnel as close to the target as possible. A 
pen or pencil is used to mark the spot where the marble actually hits. Usually, 20 
or more drops are performed in order to establish the pattern and extent of variation 
about the target. The funnel represents common causes affecting a system. Despite 
the operator’s best efforts, the marble will not land exactly on the target each time. 
The operator can react to this variability in one of four ways: 
1. Do not move the funnel. 
2. Measure the distance the hit is from the target and move the funnel an equal 
distance, but in the opposite direction (error relative to the previous position). 
3. Measure the distance the hit is from the target and move the funnel this distance 
in the opposite direction, starting at the target (error relative to the target). 
4. Move the funnel to be exactly over the location of the last hit. 
Use Python to compare these four strategies using simulation data. 
Exercise 7.3 Design a 50 runs experimental array for running the piston simulator 
using different options available in the mistat package: 
• 
Latin hypercube (simple): mistat.design.doe.lhs 
• 
Latin hypercube (space ﬁlling): mistat.design.doe.space_ﬁlling_lhs 
• 
Random k-means cluster: mistat.design.doe.random_k_means 
• 
Maximin reconstruction: mistat.design.doe.maximin 
• 
Halton sequence-based: mistat.design.doe.halton 
• 
Uniform random matrix: mistat.design.doe.uniform_random 
Compare the results. 
Exercise 7.4 Fit a Gaussian process model to data generated by the six designs 
listed in Exercise 7.3 and compare the MSE of the model ﬁts.

286
7
Computer Experiments
Exercise 7.5 Using a uniform random design, generate a stochastic emulator for 
the piston simulator in order to get 0.02 s cycle time with minimal variability. 
Exercise 7.6 Using a Latin hypercube design, generate a stochastic emulator for 
the piston simulator in order to achieve 0.02 s cycle time with minimal variability. 
Compare your results to what you got in Exercise 7.5.

Chapter 8 
Cybermanufacturing and Digital Twins 
Preview Cybermanufacturing is a name given to smart manufacturing systems 
which are part of the fourth industrial revolution. These systems combine sensor 
technologies with ﬂexible manufacturing and advanced analytics. In this chapter 
we introduce the main elements of cybermanufacturing as background and context 
to modern industrial analytics. Digital twins are related to cybermanufacturing 
but carry a wider scope. The idea is to provide a digital platform that parallels 
physical assets. This has implications, for example, in healthcare, where MRI 
imaging is interpreted on the spot with artiﬁcial intelligence models trained on 
millions of images (https://www.aidoc.com/). Other application domains of digital 
twins include agriculture, smart cities, transportation, autonomous vehicles, added 
manufacturing, and 3D printing, etc. The chapter covers the topics of computer 
complexity, computational pipelines, and reproducibility of analytic ﬁndings. It 
presents an integration of models for enhanced information quality, the Bayesian 
ﬂow analysis, and the Open ML community where datasets and data ﬂows are 
uploaded in the spirit of open data. An additional topic covered in the chapter covers 
customer survey models used in modern companies to map their improvement 
journey. 
8.1 
Introduction to Cybermanufacturing 
In Chap. 1, we present the evolution of industry, from the ﬁrst to the fourth industrial 
revolution—Industry 4.0. In this chapter, we focus on cybermanufacturing, the 
most representative realization of the fourth industrial revolution. We emphasize 
the analytic aspects of cybermanufacturing and relate them to other chapters in the 
book. 
In the era of Industry 4.0, advanced communication, computation, and control 
infrastructures get integrated with sensor technology into cyber-physical systems 
Supplementary Information The online version contains supplementary material available at 
(https://doi.org/10.1007/978-3-031-28482-3_8). 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3_8
287

288
8
Cybermanufacturing and Digital Twins
incorporating a network of multiple manufacturing steps. Such systems enable the 
transition from conventional manufacturing into cybermanufacturing. Conventional 
manufacturing relied on data-driven decision-making methods, such as statistical 
process control introduced in Chap. 2, to monitor and improve the performance of 
individual manufacturing systems. Cybermanufacturing, with physical equipment, 
sensor technologies, computer simulations, and advanced analytics, provides sig-
niﬁcantly enhanced monitoring capabilities. Moreover, digital twins, introduced in 
Sect. 1.4, enable high information quality monitoring, diagnostic, prognostic, and 
prescriptive analysis. 
With modern analytic methods, cybermanufacturing enhances manufacturing 
efﬁciency, cost reduction, product quality, ﬂexibility, and domain knowledge discov-
ery. This applies not only to large manufacturing corporations, like Stanley Black 
and Decker (Cisco 2019) and Siemens (2022) but also to small and medium-sized 
companies (Kuo et al. 2017). 
Within cybermanufacturing, computation services refer to manufacturing data 
automatically collected and processed in computation units such as the Cloud and 
Fog nodes (Chen and Jin 2021). Computation services provide real-time online 
computation results to meet on-demand computation requests from manufacturing 
processes, systems, and users. With the advancement in Internet of Things (IoT) 
and smart manufacturing (Yang et al. 2019), computation services minimize labor-
intensive training and effectively use data analytics and machine learning methods. 
In this chapter, we present modeling and analysis methods in cybermanufactur-
ing. The next section reviews challenges in manufacturing analytics. 
8.2 
Cybermanufacturing Analytics 
The third industrial revolution, introduced in Chap. 1, involved embedded systems 
such as sensors and programmable logic controllers to achieve automation in 
manufacturing. With the extensive use of embedded systems, the third industrial 
revolution signiﬁcantly improved throughput, efﬁciency, and product quality in the 
manufacturing industry, while reducing reliance on manual operations. This opened 
the era of “smart manufacturing,” which is utilizing sensor data to enable data-driven 
decision-making and equipment managed by numerical controllers (Kenett et al. 
2018b). Cybermanufacturing is the next phase in industry, leveraging advances in 
manufacturing technologies, sensors, and analytics (Kenett and Redman 2019; Kang 
et al. 2021b). Suppose that X is a set of process variables (scalar, vector, or matrix) 
and Y are performance variables (scalar, vector, or matrix). Modeling consists of 
identifying the relationship f , such that upper Y equals f left parenthesis upper X right parenthesisY = f (X), or an approximation f primef ′, such 
that upper Y equals f prime left parenthesis upper X right parenthesis plus epsilonY = f ′(X) + ε where epsilonε is an error term. Modeling and analysis provide the 
foundation for process monitoring, root-cause diagnosis, and control. 
Modeling can be based on physical principles such as thermodynamics, ﬂuid 
mechanics and dynamical systems and involve deriving exact solutions for ordinary

8.2
Cybermanufacturing Analytics
289
or partial differential equations (ODE/PDEs) or solving an approximation of 
ODE/PDEs via numerical methods such as ﬁnite element analysis. It aims at 
deriving the exact relationship f , or its approximation (or discretization) f overTilde ˜f , between 
process variables X and performance variables Y (Chinesta 2019; Dattner 2021). 
When there is a signiﬁcant gap between the assumption of physical principles and 
the actual manufacturing conditions, empirical models derived from statistically 
designed of experiments (DOE) presented in Chaps. 5–7 are needed. First principle 
models and empirically derived models can be integrated in a hybrid modeling 
approach (von Stosch et al. 2014). 
The statistical design of experiments (DOE) methodology originated in agri-
cultural experiments (Fisher 1919). DOE methods have been widely applied in 
manufacturing and in computer simulations (Box et al. 2005; Santner et al. 
2003; Wu and Hamada 2011; Kenett and Zacks 2021; Kenett and Vicario 2021). 
The design and analysis of physical experiments provides information about a 
manufacturing process under controllable settings. The design and analysis of 
digital computer experiments can seek information also when physical experiments 
are too costly or impossible to conduct. 
After identifying a set of process variables X, the goal of DOE is to derive a good 
approximation of the relationship between X and performance variables Y (denoted 
as f primef ′) so as to minimize the discrepancy between Y and f prime left parenthesis upper X right parenthesisf ′(X). Computer model 
calibration is conducted so that a model is calibrated with observational process 
data and provides more accurate results (Kennedy and O’Hagan 2001; Kenett and 
Vicario 2021). 
Smart manufacturing exploits advances in sensing technologies, with in situ 
process variables, having an impact on modeling and analysis. This enables online 
updates and provides improvements in real-time product quality and process 
efﬁciency (Kenett and Bortman 2021). An example is the control of machine tool 
positioning error using sensor data from multiple thermal sensors (Yang and Ni 
2005). Another advance has been the development of soft sensors which provide 
online surrogates to laboratory tests (Tham et al. 1989). Soft sensors provide online 
information that emulates traditionally time demanding lab tests (Reis and Kenett 
2018). 
There are two important challenges in manufacturing analytics: (1) data inte-
gration, also called data fusion, and (2) development and deployment of analytic 
methods. Data fusion refers to the methods of integrating different models and data 
sources or different types of datasets (Jin and Deng 2015; Dalla Valle and Kenett 
2018). Machine learning and data analytics refers to building a mathematical model 
based on data, such that the model can make predictions without being explicitly 
programmed to do so (see Chapter 7 and Chapter 8 in Modern Statistics, Kenett 
et al. 2022b). In particular, deep neural networks have shown superior performance 
in modeling complex manufacturing processes (Wang et al. 2018a). Data fusion 
and data analytics play crucial roles in cybermanufacturing as they provide accurate 
approximations of the relationship between process variable X and performance 
variables Y, denoted as f primef ′, by utilizing experimental and/or simulation data. In 
predictive analytics, models are validated by splitting the data into training and

290
8
Cybermanufacturing and Digital Twins
validation sets. The structure of data needs to be accounted in the cross-validation. 
An approach called beﬁtting cross-validation (BCV) is proposed in Kenett et al. 
(2022a). In the next section we review the information quality framework in the 
context of cybermanufacturing. 
8.3 
Information Quality in Cybermanufacturing 
Analytics in smart manufacturing is affected by some fundamental limitations. 
First, the scalability of conventional data fusion methods is limited. This is due 
to disconnected manufacturing systems and nonintegrated cyber resources (e.g., 
computation units, data storage). The reliability of external computation resources 
such as cloud computing is affected by network conditions and this can be 
problematic. Flexibility (i.e., the ability for re-conﬁguration) of the entire system 
is therefore necessary for supporting Industry 4.0 operations (e.g., production plan, 
facility layout, etc.). This requires effective deployment of cybermanufacturing 
solutions such as MLOps described below. 
MLOps is the process of taking an experimental Machine Learning model into 
a production web system. The acronym is a compound of “Machine Learning” 
and the continuous development practice of DevOps in the software ﬁeld. Machine 
Learning models are tested and developed in isolated experimental systems. When 
an algorithm is ready to be launched, data scientists, DevOps, and Machine Learning 
engineers transition the algorithm to production systems. MLOps applies to the 
entire lifecycle—from integration with model generation (software development 
lifecycle, continuous integration/continuous delivery), orchestration, and deploy-
ment, to health, diagnostics, governance, and business impact metrics (Kenett and 
Redman 2019). 
To streamline computation service in cybermanufacturing, it is necessary to 
identify a conﬁguration of data analytics and/or machine learning methods pro-
viding the best computation performances. A computation pipeline consists of 
sequences of method applications involving data collection, data preprocessing, 
and data analytics methods (Chen and Jin 2018, 2021). Section 8.5 is dedicated 
to computational pipelines. 
The literature is often discussing data quality but it is important to make the 
distinction between data quality and information quality. Data quality is an intrinsic 
feature of the collected data. Information quality is conditioned on the goals of 
the analysis and is affected by the analysis and management of its outcomes. 
Data quality concerns require preprocessing (e.g., ﬁltering or cleaning) before data 
modeling and analysis which determines the generation of information quality. In 
that transition, many dimensions need to be considered. To evaluate data quality, 
several assessment frameworks have been proposed (Wang et al. 1995). To ensure 
information quality, a framework based on four components and eight dimensions 
has been proposed in Kenett and Shmueli (2016), which is denoted as InfoQ.

8.3
Information Quality in Cybermanufacturing
291
The framework was presented in Modern Statistics (Kenett et al. 2022b). Here, 
we consider the application of InfoQ to Industry 4.0 and cybermanufacturing as 
reviewed in Kenett et al. (2018b). In the InfoQ framework, information quality is 
deﬁned as the utility, U, derived by applying a method of analysis, f , to a dataset, 
D, conditioned on the analysis goal, i.e., upper I n f o upper Q left parenthesis upper U comma f comma upper D comma g right parenthesis equals upper U left parenthesis f left parenthesis upper D vertical bar g right parenthesis right parenthesisInfoQ(U, f, D, g) = U(f (D|g)). 
To assess InfoQ, the following eight dimensions are considered: (1) Data 
Resolution, (2) Data Structure, (3) Data Integration, (4) Temporal Relevance, (5) 
Chronology of Data and Goal, (6) Generalizability, (7) Operationalization, and (8) 
Communication. Speciﬁcally, each of the eight dimensions can be quantiﬁed with 
respect to the components U, f , D, and g by answering a set of questions to 
help derive InfoQ values. A sample checklist, with such questions, is provided in 
Table 8.1. For more details on the calculation of InfoQ in a practical setting, the 
reader is referred to the literature, e.g., Reis and Kenett (2018). 
The InfoQ framework provides criteria for evaluating information quality in a 
contextualized computation task. Figure 8.1 provides a general view of information 
quality dimensions in the context of a computational pipeline sketch. 
Information-poor datasets lead to severe limitations in cybermanufacturing. First, 
machine learning methods may be affected by a low signal-to-noise ratio, which 
limits the effectiveness and efﬁciency of manufacturing modeling and analysis. 
Secondly, cybermanufacturing involves the collection of data from heterogeneous 
manufacturing systems which brings a wide variety of data types and heterogeneous 
data structures. This leads to unbalanced and misaligned datasets that are not easy to 
integrate. Lastly, in the era of Industry 4.0, a single type of data can involve multiple 
contextualized computation tasks. For example, the data related to equipment (e.g., 
equipment vibration) may be used for fault diagnosis, preventive maintenance 
scheduling, and quality prediction at the same time. 
However, as information quality depends on the objective of a contextualized 
computation task, a single dataset may or may not conform to the information 
quality standards required in the multiple contextualized computation tasks. This 
makes it challenging to consistently ensure information quality. 
The information quality can be used in the combination of models. Kenett and 
Salini (2011) propose this approach in the context of customer surveys. We expand 
on this example which provides an example of an analysis of ordinal data collected 
through a questionnaire. Section 8.3 in Modern Statistics (Kenett et al. 2022b) 
provides details on the survey. Here we focus on an information quality ensemble 
analysis where three models are combined for enhanced information quality. 
Assume you have 4 goals in running and analyzing an annual customer satisfac-
tion survey: 
1. Decide where to launch improvement initiatives 
2. Identify the drivers of overall satisfaction 
3. Detect positive or negative trends 
4. Set up improvement goals 
Based on these goals a questionnaire is designed and distributed to company 
customers. To demonstrate the approach we refer to the dataset ABC2.csv consisting

292
8 
Cybermanufacturing and Digital Twins 
Table 8.1 Information Quality (InfoQ) checklist 
Dimension
Questions 
1. Data resolution
1.1 Is the data scale used aligned with the stated goal? 
1.2 How reliable and precise are the measuring devices or data sources? 
1.3 Is the data analysis suitable for the data aggregation level? 
2. Data structure
2.1 Is the type of the data used aligned with the stated goal? 
2.2 Are data integrity details (corrupted/missing values) described and 
handled appropriately? 
2.3 Are the analysis methods suitable for the data structure? 
3. Data integration 
3.1 Are the data integrated from multiple sources? If so, what is the 
credibility of each source? 
3.2 How is the integration done? Are there linkage issues that lead to 
dropping crucial information? 
3.3 Does the data integration add value in terms of the stated goal? 
3.4 Does the data integration cause any privacy or conﬁdentiality 
concerns? 
4. Temporal 
relevance 
4.1 Considering the data collection, data analysis and deployment 
stages, is any of them time-sensitive? 
4.2 Does the time gap between data collection and analysis cause any 
concern? 
4.3 Is the time gap between the data collection and analysis and the 
intended use of the model (e.g., in terms of policy recommendations) of 
any concern? 
5. Chronology of 
data and goal 
5.1 If the stated goal is predictive, are all the predictor variables 
expected to be available at the time of prediction? 
5.2 If the stated goal is causal, do the causal variables precede the 
effects? 
5.3 In a causal study, are there issues of endogeneity 
(reverse-causation)? 
6. Generalizability 
6.1 Is the stated goal statistical or scientiﬁc generalizability? 
6.2 For statistical generalizability in the case of inference, does the 
paper answer the question “What population does the sample 
represent?” 
6.3 For generalizability in the case of a stated predictive goal 
(predicting the values of new observations; forecasting future values), 
are the results generalizable to the to-be-predicted data? 
6.4 Does the paper provide sufﬁcient detail for the type of needed 
reproducibility and/or repeatability, and/or replicability? 
7. 
Operationalization 
7.1 Are the measured variables themselves of interest to the study goal, 
or their underlying construct? 
7.2 Are there justiﬁcations for the choice of variables? 
7.3 What action items can be derived from the ﬁndings? 
7.4 Is it stated who can be affected (positively or negatively) by the 
ﬁndings? 
7.5 Can the affected parties do something about it? 
7.6 Would you know if you achieved your post study objectives? 
8. Communication 
8.1 Is the exposition of the goal, data, and analysis clear? 
8.2 Is the exposition level appropriate for the audience? 
8.3 Are all statements formulated without confusion or 
misunderstanding?

8.3
Information Quality in Cybermanufacturing
293
Fig. 8.1 Cybermanufacturing analytics from an information quality perspective 
of survey data from an electronic product company’s annual customer satisfaction 
survey collected from 266 companies (customers). The dataset includes, for each 
company, its location (country) and survey responses regarding: 
• 
Equipment 
• 
SalesSup (sales support) 
• 
TechnicalSup (technical support) 
• 
Suppliers 
• 
AdministrativeSup (administrative support) 
• 
TermsCondPrices (terms, conditions, and prices) 
• 
Satisfaction: overall satisfaction 
• 
Recommendation: recommending the product to others 
• 
Repurchase: intent to repurchase 
The response data are ordinal data ranging from 1 (very low satisfaction, very 
unlikely) to 5 (very high satisfaction, very likely). More information on this survey 
is available as Example 8.3 in Modern Statistics (Kenett et al. 2022b) and in Kenett 
and Salini (2009). 
In analyzing this survey data, we use Bayesian Networks, CUB models, and 
control charts. These three approaches are introduced below. This is followed 
by a qualitative information quality assessment on four InfoQ dimensions: Data 
Integration, Generalizability, Operationalization, and Communication. Since we use 
the same data in all three models, the remaining four InfoQ dimensions are identical 
for all models. 
An InfoQ assessment of the three models concludes this section with a discussion 
on how combining the three models enhances information quality. The approach 
can be generalized to cybermanufacturing applications where, for example, several 
models are combined in process control applications.

294
8
Cybermanufacturing and Digital Twins
Bayesian Networks 
Bayesian networks (BN) implement a graphical model structure known as a directed 
acyclic graph (DAG) that is popular in Statistics, Machine Learning, and Artiﬁcial 
Intelligence. BN are both mathematically rigorous and intuitively understandable. 
They enable an effective representation and computation of the joint probability 
distribution (JPD) over a set of random variables. 
The structure of a DAG is deﬁned by two sets: the set of nodes and the set of 
directed edges (arrows). The nodes represent random variables and are drawn as 
circles labeled by the variables’ names. The edges represent direct dependencies 
among the variables and are represented by arrows between nodes. In particular, 
an edge from node upper X Subscript iXi to node upper X Subscript jXj represents a statistical dependence between the 
corresponding variables. Thus, the arrow indicates that a value taken by variable upper X Subscript jXj
depends on the value taken by variable upper X Subscript iXi. Node upper X Subscript iXi is then referred to as a “parent” 
of upper X Subscript jXj and, similarly, upper X Subscript jXj is referred to as the “child” of upper X Subscript iXi. An extension of these 
genealogical terms is often used to deﬁne the sets of “descendants,” i.e., the set of 
nodes from which the node can be reached on a direct path. 
The structure of the acyclic graph guarantees that there is no node that can be its 
own ancestor or its own descendent. Such a condition is of vital importance to the 
factorization of the joint probability of a collection of nodes. Although the arrows 
represent direct causal connection between the variables, the reasoning process can 
operate on a BN by propagating information in any direction. A BN reﬂects a simple 
conditional independence statement, namely that each variable, given the state of 
its parents, is independent of its non-descendants in the graph. This property is 
used to reduce, sometimes signiﬁcantly, the number of parameters that are required 
to characterize the JPD of the variables. This reduction provides an efﬁcient way 
to compute the posterior probabilities given the evidence present in the data (See 
Chap. 10). In addition to the DAG structure, which is often considered as the 
“qualitative” part of the model, one needs to specify the “quantitative” parameters of 
the model. These parameters are described by applying the Markov property, where 
the conditional probability distribution at each node depends only on its parents. 
For discrete random variables, this conditional probability is often represented by a 
table, listing the local probability that a child node takes on each of the feasible 
values—for each combination of values of its parents. The joint distribution of 
a collection of variables can be determined uniquely by these local conditional 
probability tables. 
In the context of a customer survey, a BN analysis provides a visual causality map 
linking the various survey variables and target variables such as overall satisfaction; 
recommendation and repurchasing intentions. Figures 8.2 and 8.3 represent the BN 
of variables representing overall satisfaction from the various questionnaire topics, 
the country of the respondent and responses to Overall Satisfaction, Recommen-
dation and Repurchasing Intention. Figure 8.2 presents the DAG with the variable 
names as nodes; Fig. 8.3 shows the distribution of responses on a 1–5 scale. Many 
industrial companies are running such surveys. In case the company has a listing 
of its customers, the survey questionnaire is often sent to all customers. In such

8.3
Information Quality in Cybermanufacturing
295
Fig. 8.2 Bayesian Network of the ABC data (with names of variables) 
Fig. 8.3 Distribution of variables of the ABC data derived from Bayesian Network using belief 
propagation 
cases, the nonresponse patterns need to be evaluated for possible bias. Industrial 
companies operating through dealers or serving unlisted customers need to check 
that the target frame is well covered. 
By studying the BN in this case study one can see that an intervention to improve 
satisfaction levels from Technical Support or Equipment and Systems will increase 
Overall Satisfaction and eventually Recommendation and Repurchasing Intentions. 
As an example, consider the BN with and without conditioning on the highest 
recommendation level. Without conditioning, the highest level of satisfaction from 
Technical Support (percentage of “5”) is 26%. When conditioning the network on 
the response “5” to recommendation, 26% increases to 37%. The implication is that 
if the organization increases the percentage of customers with top level satisfaction

296
8
Cybermanufacturing and Digital Twins
from Technical Support from 26% to 37%, recommendation levels will reach their 
maximum. Management can use this analysis to justify a target of 37% for the 
percentage of customers rating “5” their overall satisfaction from Technical Support. 
We summarize now the InfoQ characteristics of a Bayesian Network analysis of 
the customer survey data. 
A: Data Integration: Bayesian Networks are particularly effective in integrating 
qualitative and quantitative data. 
B: Generalizability: The diagnostic and predictive capabilities of Bayesian Net-
works provide generalizability to population subsets. The causality relationship 
provides further generalizability to other contexts such as organizational pro-
cesses or speciﬁc job functions. 
C: Operationalization: The use of a model with conditioning capabilities provides 
an effective tool to set up improvement goals and diagnose pockets of dissatis-
faction. 
D: Communication: The visual display of a Bayesian Network makes it particularly 
appealing to decision makers who feel uneasy with mathematical models. 
In the next section we present CUB, a sophisticated approach to analyze 
categorical data, such a data generated by responses on rating scales. 
CUB Models 
Responses to customer satisfaction surveys are governed by speciﬁc experience 
and psychological considerations. When faced with discrete alternatives, people 
make choices by pairwise comparison of the items or by sequential removals. 
Such choices are affected by both uncertainty in the choice and pure randomness. 
Modeling the distribution of responses is far more precise than considering single 
summary statistics. Such considerations lead to the development of the CUB 
(Combination of uniform and shifted binomial random variables) model, originally 
proposed in Piccolo (2003). The CUB model is applied to the study of sampling 
surveys where subjects express a deﬁnite opinion selected from an ordered list of 
categories with m alternatives. The model differentiates between satisfaction level 
from an item and randomness of the ﬁnal choice. These unobservable components 
are deﬁned as feeling and uncertainty, respectively. 
Feeling is the result of several factors related to the respondent such as country 
of origin, position in the company, and years of experience. This is represented by a 
sum of random variables which converges to a unimodal continuous distribution. 
To model this, CUB models feeling by a shifted Binomial random variable, 
characterized by a parameter zetaζ and a mass b Subscript rbr for response r where: 
b Subscript r Baseline left parenthesis zeta right parenthesis equals StartBinomialOrMatrix m minus 1 Choose r minus 1 EndBinomialOrMatrix zeta Superscript m minus r Baseline left parenthesis 1 minus zeta right parenthesis Superscript r minus 1 Baseline comma r equals 1 comma 2 comma ellipsis m periodbr(ζ) =
m −1
r −1

ζ m−r(1 −ζ)r−1,
r = 1, 2, . . . m.
Uncertainty is a result of variables such as the time to answer, the degree of 
personal involvement of the responder with the topic being surveyed, the availability 
of information, fatigue, partial understanding of the item, lack of self-conﬁdence,

8.3
Information Quality in Cybermanufacturing
297
Table 8.2 CUB model estimates and their standard deviations 
Topic
Estimates of piπ (std)
Estimates of zetaζ (std) 
Overall satisfaction
0.875 (0.060)
0.338 (0.020) 
Equipment and systems
0.999 (0.043)
0.363 (0.018) 
Sales support
0.640 (0.091)
0.389 (0.028) 
Technical support
0.719 (0.067)
0.235 (0.023) 
Supplies and orders
1.000 (0.081)
0.404 (0.017) 
Contracts and pricing
1.000 (0.082)
0.490 (0.017) 
laziness, apathy, boredom, etc. A basic model for these effects is a discrete uniform 
random variable: 
upper U Subscript r Baseline left parenthesis m right parenthesis equals StartFraction 1 Over m EndFraction comma r equals 1 comma 2 comma ellipsis m periodUr(m) = 1
m,
r = 1, 2, . . . m.
The integrated CUB discrete choice model is: 
probability left parenthesis upper R equals r right parenthesis equals pi b Subscript r Baseline left parenthesis zeta right parenthesis plus left parenthesis 1 minus pi right parenthesis upper U Subscript r Baseline left parenthesis m right parenthesis period comma r equals 1 comma 2 comma ellipsis m semicolon 0 less than or equals pi less than or equals 1 commaPr(R = r) = πbr(ζ) + (1 −π)Ur(m).,
r = 1, 2, . . . m; 0 ≤π ≤1,
and 
upper E left parenthesis upper R right parenthesis equals StartFraction m plus 1 Over 2 EndFraction plus pi left parenthesis m minus 1 right parenthesis left parenthesis one half minus zeta right parenthesis periodE(R) = m + 1
2
+ π(m −1)
1
2 −ζ

.
In applying the CUB model to the ABC data, Iannario and Piccolo (2011) 
observe that customers express a judgment with no uncertainty parameters with 
regard to Equipment and Systems, Supplies and Orders and Contracts and Pricing 
and with a limited uncertainty in the other items. They also note that satisfaction 
is higher for questions on Technical Support and Equipment and Systems. Thus, in 
this case study, customers are relatively satisﬁed with the equipment supplied by the 
ABC Company and, to a lesser extent, with Sales Support. Speciﬁcally the authors 
report the estimates presented in Table 8.2. 
Summarizing the information quality characteristics of the CUB model provides 
the following remarks: 
A: Data Integration: CUB models integrate the intensity of feeling toward a certain 
item with the response uncertainty. These two components can be also explained 
by using appropriate covariates. 
B: Generalizability: The model is not generalizable per se. Its components offer, 
however, interesting cognitive and psychological interpretations. 
C: Operationalization: The model is mostly focused on explaining the outcomes 
of a survey. Insights on uncertainty and feelings can lead to interesting diverse 
initiatives. 
D: Communication: The model estimates can be visually presented with bar plots 
or otherwise.

298
8
Cybermanufacturing and Digital Twins
In the next section we discuss and application to survey data using control charts 
introduced in Chap. 2. 
Control Charts 
Perceived quality, satisfaction levels, and customer complaints can be effectively 
controlled with control charts used in the context of statistical process control (SPC). 
SPC methods are introduced in Chap. 2. Control charts are generally classiﬁed into 
two groups. When quality characteristics are measured on a continuous scale, we 
have a control chart for variables. When the quality characteristic is classiﬁed 
by attributes, then control charts for attributes are used. In analyzing customer 
satisfaction survey data, we can use control charts to identify a shift from previous 
surveys or investigate the achievement of pre-set targets. In general, we test the 
hypothesis: 
StartLayout Enlarged left brace 1st Row upper H 0 colon theta equals theta 0 2nd Row upper H 1 colon theta not equals theta 0 EndLayout

H0 : θ = θ0
H1 : θ ̸= θ0
where thetaθ can be the mean, the standard error, or a proportion, depending on the 
particular kind and scope of the control chart (i.e., for variables or for attributes). 
All the above details also hold when we are interested in testing a speciﬁc shift of 
the parameter such as theta greater than theta 0θ > θ0 or theta less than theta 0θ < θ0. In these cases, only one control limit, either 
upper control limit (UCL) or lower control limit (LCL), is reported on the control 
chart. Speciﬁcally, the p chart with control limits 
p overbar plus or minus k StartRoot StartFraction p overbar left parenthesis 1 minus p overbar right parenthesis Over n EndFraction EndRoot¯p ± k

¯p(1 −¯p)
n
is used to monitor the percentage of respondents who answered “5” (Very High) to 
a question on overall satisfaction, where n is the number of respondents and k is 
a constant multiplier of the binomial standard deviation used to set up the control 
limits. The value k equals 2k = 2 is often applied in applications of control charts to the 
analysis of customer satisfaction data. 
A p chart of the percentage of respondents which rated their satisfaction level 
as “5” in the Equipment and Systems and the Sales Support questions is presented 
in Fig. 8.4. We call these percentages “TOP5”. The chart shows an average TOP5 
proportion for Equipment and Systems questions of 14.4%. Question 9 on “uptime” 
is showing up with a TOP5 proportion signiﬁcantly higher than the average, 
indicating that “uptime” is an area that stands out as an area of excellence from the 
customer’s point of view. The Sales Support average TOP5 proportion is 18.2% with 
question 14, on satisfaction from response time by sales personnel, signiﬁcantly 
high. Because of the small number of questions, the UCL and LCL are positioned 
at 2 standard deviations above and below the average central line (CL). 
Speciﬁcally, the questions displayed in Fig. 8.4, and, in brackets, the number of 
response “5” out of 262 responses, are:

8.3
Information Quality in Cybermanufacturing
299
Fig. 8.4 p chart of proportion of “5” in questions on equipment and systems and sales support 
Equipment and Systems 
q6 The equipment’s features and capabilities meet your needs (32). 
q7 Improvements and upgrades provide value (40). 
q8 Output quality meets or exceeds expectations (30). 
q9 Uptime is acceptable (49). 
Sales Support 
q12 Verbal promises have been honored (39). 
q13 Sales personnel communicate frequently enough with you (50). 
q14 Sales personnel respond promptly to requests (60). 
q15 Sales personnel are knowledgeable about equipment (43). 
q16 Sales personnel are knowledgeable about market opportunities (45). 
Summarizing the information quality of the control chart analysis of the ABC 
data: 
A: Data Integration: Control charts can be split by covariate values. Basic univariate 
control charts do not provide an effective data integration approach. 
B: Generalizability: The analysis provides insights relevant to the data at hand 
without generalizable theory. 
C: Operationalization: The ﬁndings clearly distinguish signiﬁcant from random 
effects, thereby helping decision makers to effectively focus their improvement 
efforts. 
D: Communication: The visual display of a control chart makes it very appealing 
for communication and visualization of the analysis. 
Based on the above subjective evaluations of Bayesian Network, CUB model, 
and control chart analysis of the ABC data, we list in Table 8.3, InfoQ scores of 
the 4 assessed dimensions. These assessment account for the 4 goals listed at the 
beginning of this section.

300
8
Cybermanufacturing and Digital Twins
Table 8.3 InfoQ assessment of the Bayesian Network, CUB model, and control chart analysis 
Model 
InfoQ dimension
Bayesian Network
CUB model
Control chart 
A. Data integration
5
4
3 
B. Generalizability
3
5
2 
C. Operationalization
5
3
5 
D. Communication
4
3
5 
As we can see none of these models provides a consistent advantage. The 
approach suggested here is to apply all three models and combine them in a dash-
board in order to enhance information quality. This approach represents a general 
principle of ensemble modeling that can be easily deployed in cybermanufacturing. 
The next section is a review of general modeling in cybermanufacturing. 
8.4 
Modeling in Cybermanufacturing 
Cybermanufacturing focuses on the convergence of the physical entities (e.g., 
manufacturing equipment) and the cyber entities (e.g., simulated equipment), such 
that the dynamic changes of the physical entities can be predicted and analyzed 
through the corresponding cyber entities (Qi and Tao 2018). Therefore, the existing 
efforts aimed at achieving realistic cyber entities via accurate computer simulation 
models and machine learning methods (Qi and Tao 2018; Schluse et al. 2018). An 
example, in the context of decision tree, predictive analytics is the work of Ben-
Gal et al.  (2014). In that paper, the authors presented the dual information distance 
(DID) method, which selects features by considering both immediate contributions 
to the classiﬁcation as well as their future potential effects. Speciﬁcally, the DID 
method constructs classiﬁcation trees by ﬁnding the shortest paths over a graph 
of partitions that are deﬁned by the selected features. The authors demonstrated 
that the method takes into account both the orthogonality between the selected 
partitions as well as the reduction of uncertainty on the class partition, such 
that it outperforms popular classiﬁers in terms of average depth and classiﬁca-
tion accuracy. However, computation complexity involving computer simulations 
and data preprocessing/communication across distributed manufacturing system is 
becoming one of the most challenging issues in realizing realistic cyber entities. 
First, high-ﬁdelity computer simulations, which enable accurate prediction of the 
behavior of physical entities via the cyber entities, are typically too time-consuming. 
This issue becomes more challenging when the manufacturing process involves 
computationally intensive multiphysics computer simulations (Dbouk 2017). For 
example, recent work indicates that a single run of computer simulation for the 
fused deposition modeling process takes more than 20 h (Li et al. 2018), which 
could result in a signiﬁcant delay in updating the cyber entities. Secondly, as

8.4
Modeling in Cybermanufacturing
301
the advancement of sensing and communication systems enables the collection of 
enormous data from distributed manufacturing systems, machine learning methods 
to build the cyber entities require signiﬁcantly higher computation cost and/or 
communication bandwidth across cybermanufacturing infrastructure. 
The issue of computation complexity leads to limitations in cybermanufacturing 
(Singh et al. 2018; Modoni et al. 2019; Bevilacqua et al. 2020; Rasheed et al. 
2020). First, computation complexity involving modeling complex behaviors of het-
erogeneous manufacturing systems and their interactions delays the manufacturing 
modeling and analysis for personalized demands, which limits the timeliness of 
personalization. The issue is important, since cybermanufacturing aims at efﬁcient 
personalization of products by utilizing heterogeneous manufacturing systems 
connected. Second, computational complexity also affects the ability to generate 
information quality (Kenett and Shmueli 2016). Insufﬁcient storage or computation 
power to handle data with adequate resolution can negatively impact the delivery of 
outputs from analytical work to the right persons, in the right way, at the right time, 
thereby reducing information quality. 
Manufacturing processes involve complex physical mechanisms. Therefore, 
underlying engineering knowledge, such as cause-effect relationships, ﬁrst principle 
models, and computer simulation models (e.g., ﬁnite element analysis) and design 
rules, may be incomplete. Here, incomplete knowledge can be due to (1) incomplete 
understanding of the underlying physical mechanism (e.g., ﬁrst principle model, 
material properties), (2) incomplete information about model parameters, and (3) 
stochastic behavior or uncertainties associated with the system or numerical algo-
rithms. To address the issue, computer model calibration (Kennedy and O’Hagan 
2001; Higdon et al. 2008, 2013; Wong et al. 2017) has been continuously studied 
to compensate the incomplete knowledge with observational data, and design 
for variation (Reinman et al. 2012) has been studied to reduce the variation of 
products under the incomplete knowledge and uncertainties. However, in the era of 
Industry 4.0, it is becoming more challenging to address the issue for the following 
reasons. First, while the core of cybermanufacturing is to take advantage of multi-
modal manufacturing data, the data can mislead the decision-making processes 
if incomplete knowledge (e.g., invalid assumption in modeling a manufacturing 
process) involves the interpretation of the data. Second, since the existing efforts 
typically assume a single or only a few manufacturing systems (Feng et al. 2017), 
existing computer experimental design, modeling, and calibration are not easily 
scalable to the scenario where a lot of heterogeneous manufacturing systems con-
nected in cybermanufacturing. In these cases, the dependability and effectiveness of 
cybermanufacturing may be questioned, especially in case a delicate and complex 
situation requires decision-making in a real-time manner (Broy et al. 2012). These 
issues are more common in the newly introduced additive manufacturing (Babu and 
Goodridge 2015; Yang et al. 2017; Dilberoglu et al. 2017; Jared et al. 2017; Li et al.  
2018; Mahmoudi et al. 2018; Sabbaghi et al. 2018; Kenett et al. 2021a). 
In summary, incomplete engineering knowledge leads to limitations in cyber-
manufacturing. First, the manufacturing design process becomes inefﬁcient when 
underlying engineering knowledge is incomplete. Speciﬁcally, it has been specu-

302
8
Cybermanufacturing and Digital Twins
lated that 75% of the cost involving product development is committed early in the 
engineering design process when the knowledge of the product is unclear and/or 
incomplete (Chandrasegaran et al. 2013). Second, when the computer simulation or 
data-driven models for a manufacturing process are limited in scope, the models 
cannot provide adequate predictions for prognostics and health management in 
cybermanufacturing (Weiss et al. 2015). This can result in inefﬁcient planning, 
maintenance, and logistics due to the inaccurate prediction of equipment status 
(Davis et al. 2012; Edgar and Pistikopoulos 2018). 
Cybermanufacturing focuses on personalization and customized production, 
which will generate a wide variety of heterogeneous data (Thoben et al. 2017). In the 
meantime, the adequacy of a machine learning method to such heterogeneous data 
may be signiﬁcantly different due to the underlying statistical characteristics (e.g., 
the distribution of data) and/or contextualized computation tasks (e.g., fault diagno-
sis or quality control a speciﬁc manufacturing process) (Chen and Jin 2018). Here, 
we call the different adequacy as the “border” of the machine learning methods. 
Thus, it is important to match a speciﬁc dataset/contextualized computation task 
with a proper machine learning method within the border to ensure the efﬁciency 
and effectiveness of manufacturing modeling and analysis. 
In current practice, a typical paradigm to identify which machine learning 
method for use is often heuristic based on domain knowledge of a speciﬁc 
contextualized computation task and/or data scientist’s personal experience in data 
analysis. Clearly, such a heuristic manner could require a large number of trial-
and-errors for identifying an efﬁcient and effective machine learning method under 
a given contextualized computation task. It calls for a systematic methodology to 
understand the border among different machine learning methods, especially in the 
ﬁeld of manufacturing modeling and analysis. 
There are several challenges in Industry 4.0 due to the lack of systematic under-
standing of the borders among different machine learning methods. We list some of 
them below. First, considering the heterogeneous manufacturing systems connected 
in cybermanufacturing, it will require considerable lead time for identifying a proper 
machine learning method for each manufacturing system and computation task. For 
example, for a thermal spray coating process using heterogeneous spray guns, it 
is reported that linear regression model worked well for one spray gun was not 
applicable to the other spray guns, due to the violation of the assumption of samples 
from the same underlying distribution (Chen and Jin 2018). Second, it is known that 
manufacturing processes and systems are likely to be dynamic in model relationship, 
due to a number of factors from raw materials, equipment status, and environment. 
For example, it is reported that the model parameters for crystal growth process 
should be adjusted based on the degradation level of the equipment (Jin et al. 
2019). However, most of the machine learning methods cannot generate dynamic 
models. Therefore, it will be beneﬁcial to efﬁciently match the optimal machine 
learning method with the degradation levels. Third, cybermanufacturing often 
requires different accuracy of machine learning methods with the consideration 
of computational cost and utility costs. A lack of understanding of borders among 
machine learning methods could increase efforts to select the methods not only be

8.5
Computational Pipelines
303
adequate but also be reliable and responsive. As shown in Kang et al. (2021a), a 
tradeoff between the computational cost of designs and the accuracy of surrogate 
models could facilitate the identiﬁcation of the feasible design region, which 
is crucial in the timeliness of personalized product realization in Industry 4.0. 
Meanwhile, the time latency in machine learning training process, and the unreliable 
computation due to computation node failure or loss of communication to Cloud will 
prohibit the use of advanced, but computation intensive algorithms. 
When machine learning methods are employed, different researchers or prac-
titioners tend to choose different conﬁgurations (e.g., splitting of the samples for 
training and testing), even when they analyze the same dataset (Botvinik-Nezer 
et al. 2020). This ﬂexibility leads to difﬁculties in the reproducibility analytical 
studies and needs to be accounted for and controlled in manufacturing modeling 
and analysis. However, even though there have been consistent efforts to address 
the issue in science (Botvinik-Nezer et al. 2020; Kenett and Rubinstein 2021), 
the manufacturing industry is less concerned with the issue of reproducibility of 
analytical studies via machine learning (Kenett 2020). In other words, companies 
tend to overlook the experimental works designed to improve processes and 
products for reproducibility using adequate statistical criteria. In the meantime, 
ﬁerce competition in the era of Industry 4.0 allows only short-term opportunities 
to try out new products and/or new process setups, which calls for ensuring the 
reproducibility of analytical studies in a contextualized computation task. 
A lack of reproducibility of analytical studies via machine learning leads to the 
following challenges in cybermanufacturing. First, in the manufacturing industry, 
a lack of reproducibility of machine learning methods can result in misleading 
decision-making, which is very costly and time-consuming. For example, it is 
reported that around 50% of the costs incurred in new product development tend 
to be spent on unnecessary design iteration (Schulze and Störmer 2012), which can 
be avoided by accurate and reliable predictions. Second, since cybermanufacturing 
involves efﬁcient utilization of heterogeneous manufacturing systems connected 
to cybermanufacturing network (Lee et al. 2015; Jeschke et al. 2017; Wang et al. 
2020), reproducibility should be ensured such that consistent product quality can be 
achieved across the cybermanufacturing network. Lastly, a lack of reproducibility 
of machine learning methods can result in increased product variation, which can 
deteriorate customer satisfaction (Luo et al. 2005; Dharmesti and Nugroho 2013). 
It is an important issue, since improved customer satisfaction is one of the most 
important goals to be achieved in the context of Industry 4.0 (Bortolini et al. 2017; 
de Man and Strandhagen 2017; Bär et al. 2018). 
8.5 
Computational Pipelines 
The concept of computation pipelines for machine learning is suggested from 
the software engineering community to systematically organize a sequence of 
method options, including data collection, data preprocessing, data ﬁltering, feature

304
8
Cybermanufacturing and Digital Twins
selection (optional), data fusion/machine learning methods, computation, and post-
processing (Chen and Jin 2018). For example, Scikit-learn, a machine learning 
library for Python, applies a computation pipeline to assemble several steps that can 
be cross-validated together with different setting parameters (Pedregosa et al. 2011). 
Similarly, TensorFlow (Abadi et al. 2016) and PyTorch (Paszke et al. 2022), two 
widely used deep learning platforms, implement the idea of computational graph to 
organize computational pipelines. 
In the manufacturing industry, most relevant works involving computation 
pipelines have focused on constructing an autonomous framework to tune a speciﬁc 
computation pipeline or only a limited number of method options. Examples 
of such works include the application of computation pipelines for preventive 
maintenance operation (O’Donovan et al. 2015), fault prognostics (Kozjek et al. 
2017), and production planning (Wang et al. 2018b). While the aforementioned 
works are applicable to a speciﬁc contextualized computation task, they cannot 
be adequate in different tasks when modeling assumptions are violated (e.g., 
the underlying distribution of data is different). In other words, to ensure the 
effectiveness of manufacturing modeling and analysis, one should efﬁciently switch 
to a proper computation pipeline from a number of alternatives that ﬁts well with 
the scenario. However, the current practice relies on trial-and-errors according to 
domain knowledge and experiences, which is too time-consuming to identify the 
optimal method options for a proper computation pipeline. 
An open source tool to automate experimentation and model building is 
OpenML. It is available in https://www.openml.org. OpenML is a database from 
which entities can be downloaded and uploaded. Communication with the database 
goes through an open API which is a collaborative, automated machine learning 
environment that involves: 
• 
Datasets automatically analyzed, annotated, and organized online 
• 
Machine learning pipelines automatically shared from many libraries 
• 
Extensive APIs to integrate OpenML into your own tools and scripts 
• 
Reproducible results (e.g., models, evaluations) for easy comparison and reuse 
• 
Collaborate in real time, right from your existing tools 
• 
Make your work more visible, reusable, and easily citable 
The OpenML platform requires you to ﬁrst upload a data ﬂow. The data ﬂow 
identiﬁes a particular machine learning algorithm from a library or framework such 
as Weka, mlr, or scikit-learn. It needs to contain a name, details about its version 
and a list of settable hyperparameters. A run is a particular ﬂow, that is an algorithm, 
with a particular parameter setting, applied to a particular task. 
A data preparation step is needed. This involves assessing missing data, dupli-
cated records, missing values, outliers, typos, and many other issues that weaken 
the quality of the data and hinder advanced analysis. A precursor to modelling and 
analysis is data preparation pre-screen (Yi et al. 2019). 
We follow here a classiﬁcation of the status of data into quality bands proposed 
by Lawrence (2017); Castelijns et al. (2020) and introduced in Chapter 7 of Modern 
Statistics (Kenett et al. 2022b). The quality bands are labelled: C, B, A, AA, and 
AAA.

8.5
Computational Pipelines
305
These labels represent the level of usability of datasets: 
Band C (Conceive) refers to the stage that the data is still being ingested. If there 
is information about the dataset, it comes from the data collection phase and how 
the data was collected. The data has not yet been introduced to a programming 
environment or tool in a way that allows operations to be performed on the dataset. 
The possible analyses to be performed on the dataset in order to gain value from 
the data possibly have not been conceived yet, as this can often only be determined 
after inspecting the data itself. 
Band B (Believe) refers to the stage in which the data is loaded into an 
environment that allows cleaning operations. However, the correctness of the data is 
not fully assessed yet, and there may be errors or deﬁciencies that invalidate further 
analysis. Therefore, analyses performed on data at this level are often more cursory 
and exploratory with visualization methods to ascertain the correctness of the data. 
In band A (Analyze), the data is ready for deeper analysis. However, even if there 
are no more factual errors in the data, the quality of an analysis or machine learning 
model is greatly inﬂuenced by how the data is represented. For instance, operations 
such as feature selection and normalization can greatly increase the accuracy of 
machine learning models. Hence, these operations need to be performed before 
arriving at accurate and adequate machine learning models or analyses. 
In band AA (Allow Analysis), we consider the context in which the dataset is 
allowed to be used. Operations in this band detect, quantify, and potentially address 
any legal, moral, or social issues with the dataset, since the consequences of using 
illegal, immoral, or biased datasets can be enormous. Hence, this band is about 
verifying whether analysis can be applied without (legal) penalties or negative 
social impact. One may argue that legal and moral implications are not part of 
data cleaning, but rather distinct parts of the data process. However, we argue that 
readiness is about learning the ins and outs of your dataset and detecting and solving 
any potential problems that may occur when analyzing and using a dataset. 
Band AAA is reached when you determine that the dataset is clean. The data 
is self-contained and no further input is needed from the people that collected or 
created the data. 
A Python application providing scores to datasets based on these bands is 
available in https://github.com/pywash/pywash. Figure 8.5 shows step B of the 
application. Once missing values are handled, Band A allows detecting outliers 
and scaling of features. The application is unfortunately no longer maintained 
and requires speciﬁc Python library versions. We provide a conﬁguration ﬁle to 
setup and run the application inside a dedicated docker container on the mistat 
repository at https://github.com/gedeck/mistat. 
Computation pipeline recommendation has been adopted not only to improve 
the quality prediction but also to improve other important sectors of Industry 
4.0: informative visualization and efﬁcient human-machine collaboration. In the 
following-up research of Chen and Jin (2021), they extended the concept of com-
putation pipeline recommendation to the personalization of a visualization system, 
which is called Personalized Recommender System for Information visualization 
Methods via Extended matrix completion (PRIME). The main improvement of

306
8
Cybermanufacturing and Digital Twins
Fig. 8.5 Screenshot of PYWASH software 
PRIME is incorporating wearable sensor data for pipeline recommendation. This 
allows pipeline recommendation to include human-computer interaction in the 
acquisition of insights from complex datasets. Speciﬁcally, PRIME models covari-
ates (i.e., wearable sensor data) to predict recommendation scores (e.g., perceived 
complexity, mental workload, etc.). This allows users to adapt visualization speciﬁc 
to a contextualized computation task. In addition, PRIME can make accurate 
recommendations for new users or new contextualized computation tasks based 
on historical wearable sensor signals and recommendation scores. Chen and Jin 
(2021) demonstrate that PRIME achieves satisfactory recommendation accuracy 
for adapting visualization, even when there are limited historical datasets. This 
capability contributes to designing a new generation of visualization systems that 
adapt to users’ real-time status. PRIME can support researchers in reducing the 
sample size requirements to quantify individual differences, and practitioners in 
adapting visualizations according to user states and contextualized computation 
tasks in a real-time manner. 
The generalization and further advancement of computation pipeline recom-
mendation provides a systematic methodology to explore various method options. 
Speciﬁcally, statistical computational performance can be chosen to address infor-
mation quality, computation complexity, incomplete engineering knowledge, ade-
quacy of machine learning methods, or reproducibility issues. For example, an 
InfoQ scores can be used to recommend a computation pipeline with acceptable 
information quality. To achieve this a computational pipeline recommendation needs 
to incorporate such issues in a quantitative manner. Reis and Kenett (2018) use this 
approach in a computational pipeline used in the chemical process industry.

8.6
Digital Twins
307
8.6 
Digital Twins 
In the era of Industry 4.0, the integration of physical assets and digital assets in 
cybermanufacturing opens up new opportunities for enhanced productivity, higher 
ﬂexibility, and reduced costs. In the previous section we introduced PRIME, a 
pipeline recommendation system including human-computer interaction in the 
acquisition of insights from complex datasets. In this section we expand on the 
background and content of digital twins introduced in Sect. 1.4. 
A digital twin is a digital representation of a physical object or process (Kenett 
and Bortman 2021). The concept was initially conceived within the aerospace 
industry and evolved to general cyber-physical systems (Shafto et al. 2010). Modern 
understanding of system characteristics is inﬂuenced by the introduction of big 
data and the industrial internet of things (IIoT) solutions (Lee et al. 2015). The 
development of a digital twin implements a holistic approach to data acquisition, 
modelling and analysis, as multiple interconnected components are assembled 
to produce a decision support tool (Grieves and Vickers 2017; Grieves  2022). 
Simulation modelling is one of the key components of digital twins. It is used in 
the veriﬁcation and evaluation of engineering systems and their performance and 
functionality (Kenett and Vicario 2021). In manufacturing, the virtual representation 
of production enables the acquisition of information on the behavior of the 
product. A product’s features can be analyzed both online and off-line and their 
characteristics predicted prior to the end of the manufacturing process. 
The digital twin provides a platform for tracking and monitoring systems and 
processes implementing the methods described in Chaps. 2–4. An example of such 
capabilities is a modern car. There are many censors in new cars that work as one 
turns on the engine. Signals appear as soon as something is wrong. For examples air 
pressure in tires, temperature of the engine, driving close to another car, etc. These 
indications are derived from a built-in tracking and monitoring system installed on 
board. 
Condition-based maintenance (CBM) is also an application of digital twins. It 
is based on the concept that maintenance operations should be done only when 
necessary (see Chap. 9). The purpose of CBM is to prevent a deterioration in 
the effectiveness of a system which can lead to a total failure. It aims to reduce 
maintenance costs by optimizing planning of maintenance operations such as 
preventive replacement of damaged component. For an effective CBM, prediction 
of the remaining useful life (RUL) of components is required. Evaluation of the 
components’ RUL requires not only diagnosis faults but also the estimation of the 
fault location and severity. 
We proceed with an example of a rotating machine digital twin. Common health 
monitoring methods for rotating machines are based on signal monitoring, usually 
acceleration signals and recently strain sensing. These methods are developed to 
monitor and diagnose deterioration of critical components. Figure 8.6 is a schematic 
model of a typical damaged bearing.

308
8
Cybermanufacturing and Digital Twins
Fig. 8.6 A typical model of damaged bearing 
The effect of a damage on the dynamic behavior of the system is used in 
order to develop an efﬁcient tool to estimate the existence and the severity of 
a damage. Using a combination of experiments and simulations contributes to 
the understanding of how the system behaves in the presence of damage and, in 
addition, to study how the fault affects the vibration proﬁle. Such models are used 
during the lifecycle of a machine, as part of the machine’s digital twin, to diagnose 
the machine condition, to optimize the operation such that its key performance goals 
are achieved. 
The key issue is to understand the physical behavior of the machine, the vibration 
signal of the healthy state and in the presence of a fault. A dynamic model of 
the system is constructed. In most cases, a closed-form solution is not possible. 
Hence, a numerical solution for the healthy and the damaged cases are obtained. 
The solution is veriﬁed and validated with experimental data. The ﬁnal model results 
are used to develop physical understanding of the system behavior under different 
working conditions and manufacturing tolerances. The model results enable a better 
understating of the experimental data and play a major role in the development of 
diagnostic and prognostic algorithms, using Condition Indicators (CIs). On the other 
hand, the experimental results are used to validate the simulation and to establish the 
conﬁdence in the model and to understand its limitations. 
The rotating machine digital twin can incorporate a non-linear dynamic model 
of the motor behavior (Kenett and Bortman 2021). The model simulates the two 
gears, the stiffness of the bearings, the brake, and the torsion of the shafts, the 
backlash in the system, as well as the surface roughness of the gear teeth. The 
components included in the model are the ones affecting the system vibration signal.

8.6
Digital Twins
309
The components are assumed to be rigid bodies. The surface roughness is described 
by a displacement-excitation function along the pressure line. The contact between 
the gear teeth is simulated as slices of linear springs. 
A practical gear transmission contains surface imperfections that directly inﬂu-
ence the vibration signature. The surface imperfections may restrict the ability to 
diagnose faults using vibration analysis. The digital twin can simulate the gear tooth 
surface to better understand the limits of fault detection in the presence of different 
levels of gear imperfections. The tooth proﬁle deviation is deﬁned with respect to the 
involute proﬁle. The dynamic model is used to simulate the vibrations of gears with 
different types and sizes of faults. The model takes into consideration the irregularity 
of the gear teeth representing a realistic system. 
For the purpose of validation, simulations are compared with data from exper-
iments under similar conditions. After validation, the model is used to study the 
effect of surface quality. Low precision gear proﬁle grade reduces the ability to 
detect faults. However, gears with high-quality tooth surface allow detection of 
smaller faults and classiﬁcation of the fault size. A model estimates the distribution 
of results generated by proﬁle deviations, allowing robust analysis of diagnostic 
capabilities. This corresponds to robustness studies presented in Chap. 6. 
Another important issue in gear motors is the discontinuity contact occurs 
primarily from the presence of clearance between meshing teeth. The gap between 
the width of a tooth space and the thickness of the engaging tooth is called backlash. 
The main goal of this example is to develop a CI for detection backlash based 
on vibration signatures. Damping increase arising because of oil-ﬁlm in the gap 
between two teeth of a meshing gear needs to be taken into account. 
The effects of different backlash levels on the vibration signals indicate that 
acceleration increases with higher backlash level. The same trend was observed in 
the tangential axis. The energy level of the signal increases as function of backlash 
level. 
A dynamic model for gear system simulations takes into consideration the 
clearance between the gear teeth and oil-ﬁlm effects. Simulations using the model 
show that there is a signiﬁcant impact of the backlash on the dynamic response of 
the gear pair. A reliable dynamic model that simulates the dynamics of gear systems 
is an important tool for gear diagnostics. Using this model enables identiﬁcation of 
the inﬂuence of backlash on the vibration signal. 
This brief overview of a rotating machine digital twin presents an example where 
monitoring, diagnostic, prognostic, and prescriptive analytics is implemented. 
Example 8.1 To provide a hands on experience with a digital twin we use the piston 
simulator introduced in Chap. 7. Instead of actual data, we use the PistonSimulator 
to create sets of sensor data and cycle times for this example. 
Monitoring During normal operations, the cycle times of the experiment are similar 
to what is shown in Fig. 8.7. The data were created using a simulation with all factors 
set to the mid points of their respective ranges. While monitoring the operation of 
our actual system, we observe an unusual change in cycle times; it is shown in 
Fig. 8.8. What could have caused this change?

310
8
Cybermanufacturing and Digital Twins
Fig. 8.7 Piston cycle time under normal operation. Horizontal lines show the mean and plus or minus± one 
standard deviation 
Fig. 8.8 Piston cycle time shows an unusual change 
Diagnostics Figure 8.9 analyzes changes in the sensor data during the experiment. 
From this graph, we can assume that the noticeable changes in the initial gas volume 
v0 are likely responsible for the decrease in cycle time. To conﬁrm this hypothesis, 
we need some diagnostic investigation. We design an experimental setup with 1280 
(equals 10 dot 2 Superscript 7= 10 · 27) runs using a Latin hypersquare design with all seven factors varied.

8.6
Digital Twins
311
Fig. 8.9 Variation of sensor data during the unusual change in piston cycle time 
from mistat.design import doe 
Factors = { 
'm': (30, 60), 
's': (0.005, 0.02), 
'k': (1_000, 5_000), 
't': (290, 296), 
'p0': (90_000, 110_000), 
'v0': (0.002, 0.01), 
't0': (340, 360), 
} 
Design = doe.lhs(Factors, num_samples=1280) 
Design = Design.to_dict(orient = 'list') 
experiment = mistat.PistonSimulator(n_simulation=1, seed=1, **Design) 
experimentData = experiment.simulate() 
Using a neural network to analyze the 1280 experiments we can offer enhanced 
diagnostic and prognostic capabilities as part of a digital twin. Using a grid search 
we determine that a neural network with three layers (7, 6, 1) and six nodes in the 
hidden layer, adequately ﬁts the data.1 
from sklearn.neural_network import MLPRegressor 
from sklearn.preprocessing import MinMaxScaler 
from sklearn.pipeline import make_pipeline 
from sklearn.compose import TransformedTargetRegressor 
predictors = ['m', 's', 'k', 't', 'p0', 'v0', 't0'] 
X = experimentData[predictors] 
y = experimentData['seconds'] 
# create a pipeline that first scales the predictors to the range 0 to 1
1 The Python code for the grid search using cross validation can be found at https://gedeck.github. 
io/mistat-code-solutions/IndustrialStatistics. In some cases, a cross validation strategy that takes 
data structure into account can be more suitable; see, e.g., Kenett et al. (2022a). 

312
8
Cybermanufacturing and Digital Twins
# followed by a neural network model. The outcome variable is also 
# rescaled to a range of 0 to 1. This is achieved using TransformedTargetRegressor. 
model = TransformedTargetRegressor( 
regressor=make_pipeline( 
MinMaxScaler(), 
MLPRegressor(max_iter=1000, activation='tanh', 
hidden_layer_sizes=(5, ), learning_rate_init=0.01, 
early_stopping=True, random_state=0, 
) 
), 
transformer=MinMaxScaler(), 
) 
_ = model.fit(X, y) 
Applying such a model, we can determine the importance of individual factors on 
cycle time. We use the permutation_importance method to achieve this. Permutation 
importance measures the effect on model performance if a predictor is randomly 
shufﬂed. Besides neural network models we can use boosted trees, random forests, 
support vector machines or penalized regression such as lasso or elastic nets. 
from sklearn.inspection import permutation_importance 
result = permutation_importance(model, X, y, n_repeats=10, random_state=0) 
result.importances_mean 
result.importances_std 
permImportance = pd.DataFrame({ 
'predictor': predictors, 
'mean': result.importances_mean, 
'std': result.importances_std, 
'low': result.importances_mean - result.importances_std, 
'high': result.importances_mean + result.importances_std, 
}) 
permImportance 
predictor
mean
std
low
high 
0
m 
0.005183 
0.001588 
0.003596 
0.006771 
1
s 
0.956525 
0.027363 
0.929162 
0.983888 
2
k 
0.042135 
0.005193 
0.036942 
0.047329 
3
t 
0.001199 
0.000764 
0.000435 
0.001963 
4
p0 
0.005786 
0.001829 
0.003957 
0.007615 
5
v0 
0.832526 
0.026581 
0.805945 
0.859108 
6
t0 
0.003173 
0.001533 
0.001640 
0.004706 
The result of this analysis (see also Fig. 8.10) shows that the piston surface area 
s and the initial gas volume v0 have the strongest inﬂuence on cycle time. All other 
factors have only a small inﬂuence. Looking at a contour plot conﬁrms our analysis. 
Figure 8.11 shows the change of cycle time as a function of piston surface area s 
and the initial gas volume v0. The overlayed experimental sensor data from Fig. 8.8 
show that the drop in cycle time can be explained by a reduction in initial gas volume 
v0. 
Prognostics As v0 has such a strong inﬂuence, can we predict future values of 
v0? In the following Python code, an ARIMA model is ﬁtted to the sensor data 
that showed the effect. The best model for several combinations of p, d, and q is 
determined using the AIC of the model.

8.6
Digital Twins
313
Fig. 8.10 The importance of factors on cycle time derived from the analysis of the digital twin 
Fig. 8.11 Contour plot determined using the neural network model that shows the change in cycle 
time as a function of the two most important factors. All other factors were ﬁxed at the average 
value of the sensor data. The grey and black dots show the experimental observations before and 
after the observed change from Fig. 8.8 
import warnings 
warnings.filterwarnings('ignore', category=UserWarning) 
from statsmodels.tsa.api import SARIMAX 
# Define the p, d and q parameters to take any value between 0 and 2 
p = d = q =  range(0, 3) 
results = None 
best = None 
# generate all combinations of (p, d, q) triplets and keep the model

314
8
Cybermanufacturing and Digital Twins
Fig. 8.12 Long term forecast of v0 values using an ARIMA model 
# with the lowest AIC 
for param in list(itertools.product(p, d, q)): 
mod = SARIMAX(sensorDataShort['v0'], order=param) 
temp = mod.fit(method='nm', maxiter=600, disp=False) 
if results is None or results.aic > temp.aic: 
results = temp 
best = param 
print('ARIMA{} - AIC:{}'.format(best, results.aic)) 
ARIMA(2, 0, 0) - AIC:-226.33201069362354 
The prediction of the best model for v0 and their conﬁdence intervals are shown 
in Fig. 8.12. The rapid increase of the conﬁdence interval tells us that it is not 
possible to forecast future values of v0. 
Prescriptive Analytics With the digital twin model, we can derive an optimal 
combination of factors with an approach similar to the case study from Sect. 6.4. 
For example, as can be seen from the contour plot of Fig. 8.11, a larger surface area 
would move the operating space of the piston into a space where cycle time is less 
sensitive to changes in the initial gas volume v0; the response surface there is ﬂatter. 
For a discussion of systematic approaches see Bates et al. (2006).
black medium square■
8.7 
Chapter Highlights 
The main concepts and tools introduced in this chapter include: 
• 
Cybermanufacturing elements 
• 
Information quality

8.8
Exercises
315
• 
Modeling in cybermanufacturing 
• 
Computational pipelines 
• 
Digital twins 
• 
Monitoring analytics 
• 
Diagnostic analytics 
• 
Prognostic analytics 
• 
Prescriptive analytics 
• 
Ensemble modeling 
• 
Customer survey models 
• 
Computer experiments 
8.8 
Exercises 
Exercise 8.1 The PENSIM simulation software modelling penicillin production 
(Birol et al. 2002) is a fed-batch fermentor.2 It simulates a fed-batch penicillin 
production process and includes variables such as pH, temperature, aeration rate, 
agitation power, feed ﬂow rate of the substrate and a Raman probe. 
The PENSIM_100 dataset consists of 100 observations derived from the simula-
tor. These are observational data collected under the same process set up. Variability 
in responses is induced by the varying process variables 
A. Process set up: 
1. S0: initial sugar concentration (15 g/L) 
2. X0: initial biomass concentration (0.1 g/L) 
3. pH: pH set point (5) 
4. T: temperature set point (298 °K) 
5. air: aeration (8.6 L/min) 
6. stirring: agitation rate (29.9 W) 
7. time: culture time (350 h) 
8. feed: sugar feed rate (0.0426 L/h) 
B. Process outputs: 
1. P: Final penicillin concentration 
2. X: Final biomass concentration 
C. Process variables: 
1. Fg: aeration rate 
2. RPM: agitation rate 
3. Fs: subst. feed
2 http://www.industrialpenicillinsimulation.com. 

316
8
Cybermanufacturing and Digital Twins
4. Ts: subst. temp. 
5. S: substrate 
6. DO: dissolved oxygen 
7. Uvis: viscosity 
8. CO2: off-gas CO2 
9. Hi: heat inﬂow 
10. Ti: temperature inﬂow 
11. Ho: heat outﬂow 
12. Fw: water for injection 
Predict the process outputs P and X from the 12 process variables using two 
different models. Compare and contrast the models. 
Some options are multivariate least square regressions, regression trees, random 
forests and neural networks, Bayesian Networks (Chapters 4, 7, and 8 in Modern 
Statistics, Kenett et al. 2022b), response surfaces (Chap. 5) and Kriging (Gaussian) 
models (Chap. 7). 
Exercise 8.2 The PENSIM simulator introduced in Exercise 8.1 has been used 
to design and analyze a central composite design experiment (see Sect. 5.9). The 
dataset is available as PENSIM_CCD.csv. 
1. Evaluate the experimental design set up (see Sect. 5.10) 
2. Fit a second order response surface model to both X and P (see Sect. 5.9) 
3. Compose a qualitative description of the models 
Exercise 8.3 Example 4.5 provides an example of time series tracking engine 
vibrations of railway vehicle suspension systems. These suspensions can be affected 
by wheel ﬂats with signiﬁcant impact on system performance and safety. The dataset 
ORDER_PSD.csv includes three series where time is in units of revolution order. 
This angular resampling transformation is eliminating the variability in revolution 
time. The three time series correspond to vibrations in healthy suspensions and with 
wheel ﬂats of 10 mm and 20 mm. In the analysis use the log transformed data. 
1. Fit an ARIMA model to the healthy suspension vibration data (see Chapter 6, 
Modern Statistics, Kenett et al. 2022b) 
2. Fit the same model to the 10 mm and 20 mm wheel ﬂat 
3. Compare the model parameters 
Exercise 8.4 A company operates in 8 cities. The company product is temperature 
sensitive and management is interested in clustering the 8 locations by temperature 
characteristics. Monthly average daily minimum and maximum temperatures in 
these 8 cities, from 2000–2012, is available as dataset TEMP_WORLD.csv.3 
3 Also available for download: https://www.stat.auckland.ac.nz/~wild/data/data_from_iNZight/ 
TimeSeriesDatasets_130207/TempWorld1.csv.

8.8
Exercises
317
1. Use a smoother to compare the maximum and minimum monthly average 
temperatures in the 8 cities. 
2. Group the cities using maximum and minimum temperature patterns using 
hierarchical clusters. 
3. Group the cities using maximum and minimum temperature patterns using 
K-means clusters.

Chapter 9 
Reliability Analysis 
Preview The previous chapter dwelled on design decisions of product and process 
developers that are aimed at optimizing the quality and robustness of products and 
processes. This chapter is looking at performance over time and discusses basic 
notions of repairable and non-repairable systems. Graphical and nonparametric 
techniques are presented together with classical parametric techniques for estimat-
ing life distributions. Special sections cover reliability demonstration procedures, 
sequential reliability testing, burn-in procedures, and accelerated life testing. Design 
and testing of reliability is a crucial activity of organizations adopting advanced 
quality and industrial standards discussed in Chap. 1. 
Systems and products are considered to be of high quality if they conform to 
their design speciﬁcations and appeal to the customer. However, products can fail, 
due to degradation over time or due to some instantaneous shock. A system or a 
component of a system is said to be reliable if it continues to function, according 
to speciﬁcations, for a long time. Reliability of a product is a dynamic notion, over 
time. We say that a product is highly reliable if the probability that it will function 
properly for a speciﬁed period is close to 1. As will be deﬁned later, the reliability 
function, upper R left parenthesis t right parenthesisR(t), is the probability that a product will function in at least t units of 
time. 
We distinguish between the reliability of systems which are unrepairable and 
that of repairable systems. A repairable system, after failure, goes through a period 
of repair and then returns to function normally. Highly reliable systems need less 
repair. Repairable systems that need less repair are more available to operate and 
are therefore more desirable. Availability of a system at time t is the probability 
that the system will be up and running at time t. To increase the availability of 
repairable systems, maintenance procedures are devised. Maintenance schedules are 
designed to prevent failures of a system by periodic replacement of parts, tuning, 
cleaning, etc. It is very important to develop maintenance procedures, based on the 
Supplementary Information The online version contains supplementary material available at 
(https://doi.org/10.1007/978-3-031-28482-3_9). 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3_9
319

320
9
Reliability Analysis
reliability properties of the components of systems, which are cost effective and 
helpful to the availability of the systems. An alternative to scheduled maintenance is 
condition-based maintenance (CBM) introduced in Chaps. 1 and 8. Both scheduled 
maintenance and CBM require investigations of the reliability of systems and 
subsystems. 
One of the intriguing features of failure of components and systems is their 
random nature. We consider the length of time that a part functions till failure 
as a random variable, called the life length of the component or the system. The 
distribution functions of life length variables are called life distributions. The  role  
of statistical reliability theory is to develop methods of estimating the characteristics 
of life distributions from failure data and to design experiments called life tests. A 
subject connected to life testing is accelerated life testing. Highly reliable systems 
may take a long time till failure. In accelerated life tests, early failures are induced 
by subjecting the systems to higher than normal stress. In analyzing the results of 
such experiments, one has to know how to relate failure distributions under stressful 
conditions to those under normal operating conditions. The present chapter provides 
the foundations to the theoretical and practical treatment of life distributions and life 
testing. 
The following examples illustrate the economic importance of reliability analysis 
in industry. 
Florida Power and Light: A reduction of power plant outage rate from 14% to less 
than 4% generated $300 million savings to the consumer, on an investment of $5 
million for training and consulting. Customer service interruptions dropped from 
100 min per year to 42 min per year. 
Tennessee Valley Authority (TVA): The Athens Utilities Board is one of 160 
power distributors supplied by TVA with a service region of 100 square miles, 
10,000 customers, and a peak load of 80 MW. One year’s worth of trouble service 
data was examined in three South Athens feeders. The primary circuit failure rate 
was 15.3 failures/year/mile, restoring service using automatic equipment took, on 
the average, 3 min per switch, while manual switching requires approximately 
20 min. Line repair generally takes 45 min. The average outage cost for an 
industrial customer in the USA is $11.87/kWh. Without automation the yearly 
outage cost for a 6000 kW load per year is, on the average, $540K. The 
automation required to restore service in 3 min in South Athens costs about 
$35K. Automation has reduced outage costs to $340K. These improvements 
in reliability of the power supply have therefore produced an average return on 
investment of $9.7 for every dollar invested in automation. 
AT&T: An original plan for a transatlantic telephone cable called for three spares 
to back up each transmitter in the 200 repeaters that would relay calls across the 
seabed. A detailed reliability analysis with SUPER (System Used for Prediction 
and Evaluation of Reliability) indicated that one spare is enough. This reduced 
the cost of the project by 10%—and AT&T won the job with a bid just 5% less 
than that of its nearest competitor.

9.1
Basic Notions
321
AVX: The levels of reliability achieved by tantalum capacitors, along with their 
small size and high stability, are promoting their use in many applications that 
are electrically and environmentally more aggressive than in the past. The failure 
rates are 0.67 FIT (failures in 10Superscript 99 component hours) with shorts contributing 
approximately 67% of the total. 
Siemens: Broadband transmission systems use a signiﬁcant number of microwave 
components and these are expected to work without failure from ﬁrst switch-on. 
The 565 Mbit coaxial repeater uses 30 diodes and transistor functions in each 
repeater which adds up to 7000 SP87-11 transistors along the 250 km link. The 
link must not fail within 15 years and redundant circuits are not possible because 
of the complex circuitry. Accelerated life testing demonstrated that the expected 
failure rate of the SP87-11 transistor is less than 1 FIT, thus meeting the 15 years 
requirement. 
National Semiconductor: A single-bit error in microelectronic device can cause 
an entire system crash. In developing the BiCmos III component, one-third of 
the design team was assigned the job of improving the component’s reliability. 
Accelerated life tests under high temperature and high humidity (145 Superscript ring◦C, 85% 
relative humidity and under bias) proved the improved device to have a failure 
rate below 100 FIT. In a system using 256-kbit BiCmos III static random access 
memories, this translates to less than one failure in 18 years. 
Lockheed: Some 60% of the cost of military aircraft now goes for its electronic 
systems, and many military contracts require the manufacturer to provide service 
at a ﬁxed price for product defects that occur during the warranty period. 
Lockheed Corporation produces switching logic units used in the US Navy S-
3A antisubmarine aircraft to distribute communications within and outside the 
aircraft. These units were high on the Pareto of component failures. They were 
therefore often removed for maintenance, thereby damaging the chassis. The 
mean time between failures for the switching logic units was approximately 
100 h. Changes in the design and improved screening procedures increased the 
mean time between failures to 500 h. The average number of units removed each 
week from nine aircraft dropped from 1.8 to 0.14. 
9.1 
Basic Notions 
9.1.1 
Time Categories 
The following time categories play an important role in the theory of reliability, 
availability, and maintainability of systems: 
I. Usage-Related Time Categories 
(a) Operating time is the time interval during which the system is in actual 
operation.

322
9
Reliability Analysis
(b) Scheduled operating time is the time interval during which the system is 
required to properly operate. 
(c) Free time is the time interval during which the system is scheduled to be 
off duty. 
(d) Storage time is the time interval during which a system is stored as a spare 
part. 
II. Equipment Condition Time Categories 
(a) Up time is the time interval during which the system is operating or ready 
for operation. 
(b) Down time is the time interval out of the scheduled operating time during 
which the system is in state of failure (inoperable). 
Down time is the sum of 
(i) Administrative time 
(ii) Active repair time 
(iii) Logistic time (repair suspension due to lack of parts) 
III. Indices 
StartLayout 1st Row 1st Column bold upper S c h e d u l e d bold upper O p e r a t i n g bold upper T i m e 2nd Column equals operating time plus down time 2nd Row 1st Column bold upper I n t r i n s i c bold upper A v a i l a b i l i t y 2nd Column equals StartFraction operating time Over operating time plus active repair time EndFraction 3rd Row 1st Column bold upper A v a i l a b i l i t y 2nd Column equals StartFraction operating time Over operating time plus down time EndFraction 4th Row 1st Column bold upper O p e r a t i o n a l bold upper R e a d i n e s s 2nd Column equals StartFraction Up time Over total calendar time EndFraction period EndLayout
Scheduled Operating Time = operating time + down time
Intrinsic Availability =
operating time
operating time + active repair time
Availability =
operating time
operating time + down time
Operational Readiness =
Up time
total calendar time.
Example 9.1 A machine is scheduled to operate for two shifts a day (8 h each shift), 
ﬁve days a week. During the last 48 weeks, the machine was “down” ﬁve times. The 
average down time is partitioned into 
1. Average administrative time equals= 9 [hr]  
2. Average repair time equals= 30 [hr] 
3. Average logistic time equals= 7.6 [hr] 
Thus, the total down time in the 48 weeks is 
down time equals 5 times left parenthesis 9 plus 30 plus 7.6 right parenthesis equals 233 left bracket hr right bracket perioddown time = 5 × (9 + 30 + 7.6) = 233 [hr].
The total scheduled operating time is 48 times 16 times 5 equals 384048 × 16 × 5 = 3840 [hr]. Thus, the total 
operating time is 3607 [hr]. The indices of availability and intrinsic availability are

9.1
Basic Notions
323
Availability equals StartFraction 3607 Over 3840 EndFraction equals 0.9393 periodAvailability = 3607
3840 = 0.9393.
Intrinsic Availability equals StartFraction 3607 Over 3607 plus 150 EndFraction equals 0.9601 periodIntrinsic Availability =
3607
3607 + 150 = 0.9601.
Finally the operational readiness of the machine is 
Operational Readiness equals StartFraction 8064 minus 233 Over 8064 EndFraction equals 0.9711 periodOperational Readiness = 8064 −233
8064
= 0.9711.
black medium square■
9.1.2 
Reliability and Related Functions 
The length of life (lifetime) of a (product) system is the length of the time interval, 
T , from the initial activation of it till its failure. If a system is switched on and off, 
we consider the total active time of the system till its failure. T is a non-negative 
random variable. The distribution of T is called a life distribution. We generally 
assume that T is a continuous random variable, having a p.d.f. f Subscript upper T Baseline left parenthesis t right parenthesisfT (t) and a c.d.f. 
upper F Subscript upper T Baseline left parenthesis t right parenthesisFT (t). The  reliability function of a (product) system is deﬁned as 
StartLayout 1st Row 1st Column upper R left parenthesis t right parenthesis 2nd Column equals probability left brace upper T greater than or equals t right brace 2nd Row 1st Column Blank 2nd Column equals 1 minus upper F Subscript upper T Baseline left parenthesis t right parenthesis comma t greater than or equals 0 period EndLayoutR(t) = Pr{T ≥t}
= 1 −FT (t),
t ≥0.
(9.1.1) 
The expected life length of a product is called the mean time till failure (MTTF). 
This quantity is given by 
StartLayout 1st Row 1st Column mu 2nd Column equals integral Subscript 0 Superscript normal infinity Baseline t f Subscript upper T Baseline left parenthesis t right parenthesis normal d t 2nd Row 1st Column Blank 2nd Column equals integral Subscript 0 Superscript normal infinity Baseline upper R left parenthesis t right parenthesis normal d t period EndLayoutμ =
 ∞
0
tfT (t) dt
=
 ∞
0
R(t) dt.
(9.1.2) 
The instantaneous hazard function of a product, also called the failure rate 
function, is deﬁned as 
h left parenthesis t right parenthesis equals StartFraction f left parenthesis t right parenthesis Over upper R left parenthesis t right parenthesis EndFraction comma t greater than or equals 0 periodh(t) = f (t)
R(t),
t ≥0.
(9.1.3) 
Notice that h left parenthesis t right parenthesish(t) and f left parenthesis t right parenthesisf (t) have the dimension of 1 divided by upper T1/T . That is, if T is measured in 
hours, the dimension of h left parenthesis t right parenthesish(t) is [1/hr]. 
Notice that h left parenthesis t right parenthesis equals StartFraction normal d Over normal d t EndFraction log left parenthesis minus upper R left parenthesis t right parenthesis right parenthesish(t) = d
dt log(−R(t)). Accordingly, 
upper R left parenthesis t right parenthesis equals exp left brace minus integral Subscript 0 Superscript t Baseline h left parenthesis u right parenthesis normal d u right brace periodR(t) = exp

−
 t
0
h(u) du

.
(9.1.4)

324
9
Reliability Analysis
The function 
upper H left parenthesis t right parenthesis equals integral Subscript 0 Superscript t Baseline h left parenthesis u right parenthesis normal d uH(t) =
 t
0
h(u) du
(9.1.5) 
is called the cumulative hazard rate function. 
Example 9.2 In many applications of reliability theory, the exponential distribution 
with mean muμ is used as the distribution of T . In this case, 
f Subscript upper T Baseline left parenthesis t right parenthesis equals StartFraction 1 Over mu EndFraction exp left brace negative t divided by mu right brace comma t greater than or equals 0fT (t) = 1
μ exp{−t/μ},
t ≥0
and 
upper R left parenthesis t right parenthesis equals exp left brace negative t divided by mu right brace comma t greater than or equals 0 periodR(t) = exp{−t/μ},
t ≥0.
In the exponential distribution model, the reliability function diminishes from 1 to 
0 exponentially fast, relative to muμ. 
The hazard rate function of an exponential distribution is 
h left parenthesis t right parenthesis equals StartStartFraction StartFraction 1 Over mu EndFraction dot exp left brace negative t divided by mu right brace OverOver exp left brace negative t divided by mu right brace EndEndFraction equals StartFraction 1 Over mu EndFraction comma t greater than or equals 0 periodh(t) =
1
μ · exp{−t/μ}
exp{−t/μ}
= 1
μ,
t ≥0.
That is, the exponential model is valid for cases where the hazard rate function is a 
constant independent of time. If the MTTF is mu equals 100μ = 100 [hr], we expect 1 failure per 
100 [hr], i.e., h left parenthesis t right parenthesis equals StartFraction 1 Over 100 EndFractionh(t) =
1
100 left bracket StartFraction 1 Over hr EndFraction right bracket

1
hr

.
black medium square■
9.2 
System Reliability 
In this section we show how to compute the reliability function of a system, as a 
function of the reliability of its components (modules). Thus, if we have a system 
comprised of k subsystems (components or modules), having reliability functions 
upper R 1 left parenthesis t right parenthesis comma midline horizontal ellipsis comma upper R Subscript k Baseline left parenthesis t right parenthesisR1(t), · · · , Rk(t), the reliability of the system is given by 
upper R Subscript s y s Baseline left parenthesis t right parenthesis equals psi left parenthesis upper R 1 left parenthesis t right parenthesis comma midline horizontal ellipsis comma upper R Subscript k Baseline left parenthesis t right parenthesis right parenthesis semicolon t greater than or equals 0 periodRsys(t) = ψ(R1(t), · · · , Rk(t));
t ≥0.
(9.2.1) 
The function psi left parenthesis dot right parenthesisψ(·) is called a structure function. It reﬂects the functional relation-
ship between the subsystems and the system. In the present section we discuss some 
structure functions of simple systems. We also assume that the random variables 
upper T 1 comma midline horizontal ellipsis comma upper T Subscript k BaselineT1, · · · , Tk, representing the life length of the subsystems, are independent. 
Consider a system having two subsystems (modules) upper C 1C1 and upper C 2C2. We say that the 
subsystems are connected in series, if a failure of either one of the subsystems causes

9.2
System Reliability
325
Fig. 9.1 Block diagrams for 
systems in series and in 
parallel 
immediate failure of the system. We represent this series connection by a block 
diagram, as in Fig. 9.1. Let  upper I Subscript iIi left parenthesis i equals 1 comma midline horizontal ellipsis comma k right parenthesis(i = 1, · · · , k) be indicator variables, assuming the 
value 1 if the component upper C Subscript iCi does not fail during a speciﬁed time interval left parenthesis 0 comma t 0 right parenthesis(0, t0). If  
upper C Subscript iCi fails during left parenthesis 0 comma t 0 right parenthesis(0, t0), then upper I Subscript i Baseline equals 0Ii = 0. A  series structure function of k components is 
psi Subscript s Baseline left parenthesis upper I 1 comma midline horizontal ellipsis comma upper I Subscript k Baseline right parenthesis equals product Underscript i equals 1 Overscript k Endscripts upper I Subscript i Baseline periodψs(I1, · · · , Ik) =
k
i=1
Ii.
(9.2.2) 
The expected value of upper I Subscript iIi is 
upper E left brace upper I Subscript i Baseline right brace equals probability left brace upper I Subscript i Baseline equals 1 right brace equals upper R Subscript i Baseline left parenthesis t 0 right parenthesis periodE{Ii} = Pr{Ii = 1} = Ri(t0).
(9.2.3) 
If the system is connected in series, then, since upper T 1 comma ellipsis comma upper T Subscript k BaselineT1, . . . , Tk are independent, 
StartLayout 1st Row 1st Column upper R Subscript s y s Superscript left parenthesis s right parenthesis Baseline left parenthesis t 0 right parenthesis 2nd Column equals upper E left brace psi Subscript s Baseline left parenthesis upper I 1 comma midline horizontal ellipsis comma upper I Subscript k Baseline right parenthesis right brace equals product Underscript i equals 1 Overscript k Endscripts upper R Subscript i Baseline left parenthesis t 0 right parenthesis 2nd Row 1st Column Blank 2nd Column equals psi Subscript s Baseline left parenthesis upper R 1 left parenthesis t 0 right parenthesis comma midline horizontal ellipsis comma upper R Subscript k Baseline left parenthesis t 0 right parenthesis right parenthesis period EndLayoutR(s)
sys(t0) = E{ψs(I1, · · · , Ik)} = 	k
i=1 Ri(t0)
= ψs(R1(t0), · · · , Rk(t0)).
(9.2.4) 
Thus, the system reliability function for subsystems connected in series is given by 
psi Subscript s Baseline left parenthesis upper R 1 comma midline horizontal ellipsis comma upper R Subscript k Baseline right parenthesisψs(R1, · · · , Rk), where upper R 1 comma midline horizontal ellipsis comma upper R Subscript k BaselineR1, · · · , Rk are the reliability values of the components. 
A system comprised of k subsystems is said to be connected in parallel if the 
system fails when all subsystems fail. In a parallel connection, it is sufﬁcient that 
one of the subsystems will function for the whole system to function. 
The structure function for parallel connection is 
psi Subscript p Baseline left parenthesis upper I 1 comma midline horizontal ellipsis comma upper I Subscript k Baseline right parenthesis equals 1 minus product Underscript i equals 1 Overscript k Endscripts left parenthesis 1 minus upper I Subscript i Baseline right parenthesis periodψp(I1, · · · , Ik) = 1 −
k
i=1
(1 −Ii).
(9.2.5) 
The reliability function for a system in parallel is, in the case of independence, 
StartLayout 1st Row 1st Column upper R Subscript s y s Superscript left parenthesis p right parenthesis Baseline left parenthesis t 0 right parenthesis 2nd Column equals upper E left brace psi Subscript p Baseline left parenthesis upper I 1 comma midline horizontal ellipsis comma upper I Subscript k Baseline right parenthesis right brace 2nd Row 1st Column Blank 2nd Column equals 1 minus product Underscript i equals 1 Overscript k Endscripts left parenthesis 1 minus upper R Subscript i Baseline left parenthesis t 0 right parenthesis right parenthesis period EndLayoutR(p)
sys(t0) = E{ψp(I1, · · · , Ik)}
= 1 −	k
i=1(1 −Ri(t0)).
(9.2.6)

326
9
Reliability Analysis
Example 9.3 A computer card has 200 components, which should function cor-
rectly. The reliability of each component, for a period of 200 h of operation, is 
upper R equals 0.9999R = 0.9999. The components are independent of one another. What is the reliability 
of the card, for this time period? Since all the components should function, we 
consider a series structure function. Thus, the system reliability for t 0 equals 200t0 = 200 [hr] is 
upper R Subscript s y s Superscript left parenthesis s right parenthesis Baseline left parenthesis t 0 right parenthesis equals left parenthesis 0.9999 right parenthesis Superscript 200 Baseline equals 0.9802 periodR(s)
sys(t0) = (0.9999)200 = 0.9802.
Thus, despite the fact that each component is unlikely to fail, there is a probability 
of 0.02 that the card will fail within 200 h. If each of the components has only a 
reliability of 0.99, the card reliability is 
upper R Subscript s y s Superscript left parenthesis s right parenthesis Baseline left parenthesis t 0 right parenthesis equals left parenthesis 0.99 right parenthesis Superscript 200 Baseline equals 0.134 periodR(s)
sys(t0) = (0.99)200 = 0.134.
This shows why it is so essential in the electronic industry to demand from the 
vendors of the components highly reliable products. 
Suppose that there is room for some redundancy on the card. It is therefore 
decided to use parts having reliability of upper R equals 0.99R = 0.99 and duplicate each component in 
a parallel structure. The parallel structure of duplicated components is considered a 
module. The reliability of each module is upper R Subscript upper M Baseline equals 1 minus left parenthesis 1 minus 0.99 right parenthesis squared equals 0.9999RM = 1 −(1 −0.99)2 = 0.9999. The  
reliability of the whole system is now 
upper R Subscript s y s Superscript left parenthesis s right parenthesis Baseline equals left parenthesis upper R Subscript upper M Baseline right parenthesis Superscript 200 Baseline equals 0.9802 periodR(s)
sys = (RM)200 = 0.9802.
Thus, by changing the structure of the card, we achieve 0.98 reliability with 200 
pairs of components, each with a reliability value of 0.99.
black medium square■
Systems may have more complicated structures. In Fig. 9.2, we see the block 
diagram of a system consisting of 5 components. Let upper R 1 comma upper R 2 comma midline horizontal ellipsis comma upper R 5R1, R2, · · · , R5 denote the 
reliability values of the 5 components upper C 1 comma midline horizontal ellipsis comma upper C 5C1, · · · , C5, respectively. Let upper M 1M1 be the 
module consisting of components upper C 1C1 and upper C 2C2, and let upper M 2M2 be the module consisting 
of the other components. The reliability of upper M 1M1 for some speciﬁed time interval is 
upper R Subscript upper M 1 Baseline equals upper R 1 upper R 2 periodRM1 = R1R2.
Fig. 9.2 A parallel–series 
structure

9.2
System Reliability
327
The reliability of upper M 2M2 is 
StartLayout 1st Row 1st Column upper R Subscript upper M 2 2nd Column equals upper R 3 left parenthesis 1 minus left parenthesis 1 minus upper R 4 right parenthesis left parenthesis 1 minus upper R 5 right parenthesis right parenthesis 2nd Row 1st Column Blank 2nd Column equals upper R 3 left parenthesis upper R 4 plus upper R 5 minus upper R 4 upper R 5 right parenthesis 3rd Row 1st Column Blank 2nd Column equals upper R 3 upper R 4 plus upper R 3 upper R 5 minus upper R 3 upper R 4 upper R 5 period EndLayout
RM2 = R3(1 −(1 −R4)(1 −R5))
= R3(R4 + R5 −R4R5)
= R3R4 + R3R5 −R3R4R5.
Finally, the system reliability for that block diagram is 
StartLayout 1st Row 1st Column upper R Subscript s y s 2nd Column equals 1 minus left parenthesis 1 minus upper R Subscript upper M 1 Baseline right parenthesis left parenthesis 1 minus upper R Subscript upper M 2 Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals upper R Subscript upper M 1 Baseline plus upper R Subscript upper M 2 Baseline minus upper R Subscript upper M 1 Baseline upper R Subscript upper M 2 Baseline 3rd Row 1st Column Blank 2nd Column equals upper R 1 upper R 2 plus upper R 3 upper R 4 plus upper R 3 upper R 5 minus upper R 3 upper R 4 upper R 5 4th Row 1st Column Blank 2nd Column minus upper R 1 upper R 2 upper R 3 upper R 4 minus upper R 1 upper R 2 upper R 3 upper R 5 plus upper R 1 upper R 2 upper R 3 upper R 4 upper R 5 period EndLayout
Rsys = 1 −(1 −RM1)(1 −RM2)
= RM1 + RM2 −RM1RM2
= R1R2 + R3R4 + R3R5 −R3R4R5
−R1R2R3R4 −R1R2R3R5 + R1R2R3R4R5.
Another important structure function is that of bold kk out of bold nn subsystems. In 
other words, if a system consists of n subsystems, it is required that at least 
k, 1 less than or equals k less than n1 ≤k < n, subsystems will function, throughout the speciﬁed time period, 
in order that the system will function. Assuming independence of the lifetimes 
of the subsystems, we construct the reliability function of the system, by simple 
probabilistic considerations. For example, if we have 3 subsystems having reliability 
values, for the given time period, of upper R 1 comma upper R 2 comma upper R 3R1, R2, R3 and at least 2 out of the 3 should 
function, then the system reliability is 
StartLayout 1st Row 1st Column upper R Subscript s y s Superscript 2 left parenthesis 3 right parenthesis 2nd Column equals 1 minus left parenthesis 1 minus upper R 1 right parenthesis left parenthesis 1 minus upper R 2 right parenthesis left parenthesis 1 minus upper R 3 right parenthesis minus upper R 1 left parenthesis 1 minus upper R 2 right parenthesis left parenthesis 1 minus upper R 3 right parenthesis 2nd Row 1st Column Blank 2nd Column minus upper R 2 left parenthesis 1 minus upper R 1 right parenthesis left parenthesis 1 minus upper R 3 right parenthesis minus upper R 3 left parenthesis 1 minus upper R 1 right parenthesis left parenthesis 1 minus upper R 2 right parenthesis 3rd Row 1st Column Blank 2nd Column equals upper R 1 upper R 2 plus upper R 1 upper R 3 plus upper R 2 upper R 3 minus 2 upper R 1 upper R 2 upper R 3 period EndLayout
R2(3)
sys = 1 −(1 −R1)(1 −R2)(1 −R3) −R1(1 −R2)(1 −R3)
−R2(1 −R1)(1 −R3) −R3(1 −R1)(1 −R2)
= R1R2 + R1R3 + R2R3 −2R1R2R3.
If all the subsystems have the same reliability value R, for a speciﬁed time period, 
then the reliability function of the system, in a k out of n structure, can be computed 
by using the binomial c.d.f. upper B left parenthesis j semicolon n comma upper R right parenthesisB(j; n, R), i.e., 
upper R Subscript s y s Superscript k left parenthesis n right parenthesis Baseline equals 1 minus upper B left parenthesis k minus 1 semicolon n comma upper R right parenthesis periodRk(n)
sys = 1 −B(k −1; n, R).
(9.2.7) 
Example 9.4 A cooling system for a reactor has 3 identical cooling loops. Each 
cooling loop has two identical pumps connected in parallel. The cooling system 
requires that 2 out of the 3 cooling loops operate successfully. The reliability of a 
pump over the life span of the plant is upper R equals 0.6R = 0.6. We compute the reliability of the 
cooling system. 
First, the reliability of a cooling loop is 
StartLayout 1st Row 1st Column upper R Subscript c l 2nd Column equals 1 minus left parenthesis 1 minus upper R right parenthesis squared equals 2 upper R minus upper R squared 2nd Row 1st Column Blank 2nd Column equals 1.2 minus 0.36 equals 0.84 period EndLayout
Rcl = 1 −(1 −R)2 = 2R −R2
= 1.2 −0.36 = 0.84.

328
9
Reliability Analysis
Finally, the system reliability is 
upper R Subscript s y s Superscript 2 left parenthesis 3 right parenthesis Baseline equals 1 minus upper B left parenthesis 1 semicolon 3 comma 0.84 right parenthesis equals 0.9314 periodR2(3)
sys = 1 −B(1; 3, 0.84) = 0.9314.
This reliability can be increased by choosing pumps with higher reliability. If the 
pump reliability is 0.9, the loop’s reliability is 0.99 and the system’s reliability is 
0.9997.
black medium square■
9.3 
Availability of Repairable Systems 
Repairable systems alternate during their functional life through cycles of up phase 
and down phase. During the up phase, the system functions as required, till it fails. 
At the moment of failure, the system enters the down phase. The system remains 
in this down phase until it is repaired and activated again. The length of time the 
system is in the up phase is called the time till failure (TTF). The length of time 
the system is in the down phase is called the time till repair (TTR). Both TTF and 
TTR are modeled as random variables, T and S, respectively. We assume here that 
T and S are independent. The cycle time is the random variable upper C equals upper T plus upper SC = T + S. 
The process in which the system goes through these cycles is called a renewal 
process. Let  upper C 1 comma upper C 2 comma upper C 3 comma midline horizontal ellipsisC1, C2, C3, · · · be a sequence of cycles of a repairable system. We 
assume that upper C 1 comma upper C 2 comma midline horizontal ellipsisC1, C2, · · · are i.i.d. random variables. 
Let upper F left parenthesis t right parenthesisF(t) be the c.d.f. of the TTF and upper G left parenthesis t right parenthesisG(t) the c.d.f. of the TTR. Let f left parenthesis t right parenthesisf (t) and 
g left parenthesis t right parenthesisg(t) be the corresponding p.d.f. Let upper K left parenthesis t right parenthesisK(t) denote the c.d.f. of C. Since T and S are 
independent random variables, 
StartLayout 1st Row 1st Column upper K left parenthesis t right parenthesis 2nd Column equals probability left brace upper C less than or equals t right brace 2nd Row 1st Column Blank 2nd Column equals integral Subscript 0 Superscript t Baseline f left parenthesis x right parenthesis upper P left brace upper S less than or equals t minus x right brace normal d x 3rd Row 1st Column Blank 2nd Column equals integral Subscript 0 Superscript t Baseline f left parenthesis x right parenthesis upper G left parenthesis t minus x right parenthesis normal d x period EndLayout
K(t) = Pr{C ≤t}
=
 t
0 f (x)P{S ≤t −x} dx
=
 t
0 f (x)G(t −x) dx.
(9.3.1) 
Assuming that upper G left parenthesis 0 right parenthesis equals 0G(0) = 0, differentiation of upper K left parenthesis t right parenthesisK(t) yields the p.d.f. of the cycle time, 
k left parenthesis t right parenthesisk(t), namely 
k left parenthesis t right parenthesis equals integral Subscript 0 Superscript t Baseline f left parenthesis x right parenthesis g left parenthesis t minus x right parenthesis normal d x periodk(t) =
 t
0
f (x)g(t −x) dx.
(9.3.2) 
The operation of getting k left parenthesis t right parenthesisk(t) from f left parenthesis t right parenthesisf (t) and g left parenthesis t right parenthesisg(t) is called a convolution. 
The Laplace transform of an integrable function f left parenthesis t right parenthesisf (t), on 0 less than t less than normal infinity0 < t < ∞, is deﬁned 
as 
f Superscript asterisk Baseline left parenthesis s right parenthesis equals integral Subscript 0 Superscript normal infinity Baseline e Superscript minus t s Baseline f left parenthesis t right parenthesis normal d t comma s greater than or equals 0 periodf ∗(s) =
 ∞
0
e−tsf (t) dt,
s ≥0.
(9.3.3)

9.3
Availability of Repairable Systems
329
Notice that if f left parenthesis t right parenthesisf (t) is a p.d.f. of a non-negative continuous random variable, then 
f Superscript asterisk Baseline left parenthesis s right parenthesisf ∗(s) is its moment generating function (m.g.f.) at negative s−s. Since upper C equals upper T plus upper SC = T + S and T 
and S are independent, the m.g.f. of C is upper M Subscript upper C Baseline left parenthesis u right parenthesis equals upper M Subscript upper T Baseline left parenthesis u right parenthesis upper M Subscript upper S Baseline left parenthesis u right parenthesisMC(u) = MT (u)MS(u), for all u less than or equals u Superscript asterisku ≤u∗at 
which these m.g.f. exist. In particular, if k Superscript asterisk Baseline left parenthesis s right parenthesisk∗(s) is the Laplace Transform of k left parenthesis t right parenthesisk(t), 
k Superscript asterisk Baseline left parenthesis s right parenthesis equals f Superscript asterisk Baseline left parenthesis s right parenthesis g Superscript asterisk Baseline left parenthesis s right parenthesis comma s greater than or equals 0 periodk∗(s) = f ∗(s)g∗(s),
s ≥0.
(9.3.4) 
Example 9.5 Suppose that T is exponentially distributed like upper E left parenthesis beta right parenthesisE(β), and S is 
exponentially distributed like upper E left parenthesis gamma right parenthesisE(γ ), 0 less than beta0 < β, gamma less than normal infinityγ < ∞, i.e., 
StartLayout 1st Row 1st Column f left parenthesis t right parenthesis 2nd Column equals StartFraction 1 Over beta EndFraction exp left brace negative t divided by beta right brace comma 2nd Row 1st Column g left parenthesis t right parenthesis 2nd Column equals StartFraction 1 Over gamma EndFraction exp left brace negative t divided by gamma right brace period EndLayout
f (t) = 1
β exp{−t/β},
g(t) = 1
γ exp{−t/γ }.
The p.d.f. of C is 
StartLayout 1st Row 1st Column k left parenthesis t right parenthesis 2nd Column equals integral Subscript 0 Superscript t Baseline f left parenthesis x right parenthesis g left parenthesis t minus x right parenthesis normal d x 2nd Row 1st Column Blank 2nd Column equals StartLayout Enlarged left brace 1st Row 1st Column StartFraction 1 Over beta minus gamma EndFraction left parenthesis e Superscript negative t divided by beta Baseline minus e Superscript negative t divided by gamma Baseline right parenthesis comma 2nd Column if beta not equals gamma 2nd Row 1st Column Blank 3rd Row 1st Column StartFraction t Over beta squared EndFraction e Superscript negative t divided by beta Baseline comma 2nd Column if beta equals gamma period EndLayout EndLayout
k(t) =
 t
0
f (x)g(t −x) dx
=
⎧
⎪⎪⎨
⎪⎪⎩
1
β−γ (e−t/β −e−t/γ ),
ifβ ̸= γ
t
β2 e−t/β,
if β = γ.
The corresponding Laplace transforms are 
StartLayout 1st Row 1st Column f Superscript asterisk Baseline left parenthesis s right parenthesis 2nd Column equals left parenthesis 1 plus s beta right parenthesis Superscript negative 1 Baseline comma 2nd Row 1st Column g Superscript asterisk Baseline left parenthesis s right parenthesis 2nd Column equals left parenthesis 1 plus s gamma right parenthesis Superscript negative 1 Baseline comma 3rd Row 1st Column k Superscript asterisk Baseline left parenthesis s right parenthesis 2nd Column equals left parenthesis 1 plus s beta right parenthesis Superscript negative 1 Baseline left parenthesis 1 plus s gamma right parenthesis Superscript negative 1 Baseline period EndLayout
f ∗(s) = (1 + sβ)−1,
g∗(s) = (1 + sγ )−1,
k∗(s) = (1 + sβ)−1(1 + sγ )−1.
black medium square■
Let upper N Subscript upper F Baseline left parenthesis t right parenthesisNF (t) denote the number of failures of a system during the time interval 
left parenthesis 0 comma t right bracket(0, t]. Let  upper W left parenthesis t right parenthesis equals upper E left brace upper N Subscript upper F Baseline left parenthesis t right parenthesis right braceW(t) = E{NF (t)}. Similarly, let upper N Subscript upper R Baseline left parenthesis t right parenthesisNR(t) be the number of repairs during 
left parenthesis 0 comma t right bracket(0, t] and upper V left parenthesis t right parenthesis equals upper E left brace upper N Subscript upper R Baseline left parenthesis t right parenthesis right braceV (t) = E{NR(t)}. Obviously upper N Subscript upper R Baseline left parenthesis t right parenthesis less than or equals upper N Subscript upper F Baseline left parenthesis t right parenthesisNR(t) ≤NF (t) for all 0 less than t less than normal infinity0 < t < ∞. 
Let upper A left parenthesis t right parenthesisA(t) denote the probability that the system is up at time t. upper A left parenthesis t right parenthesisA(t) is the 
availability function of the system. In unrepairable systems, upper A left parenthesis t right parenthesis equals upper R left parenthesis t right parenthesisA(t) = R(t). 
Let us assume that upper W left parenthesis t right parenthesisW(t) and upper V left parenthesis t right parenthesisV (t) are differentiable, and let w left parenthesis t right parenthesis equals upper W prime left parenthesis t right parenthesisw(t) = W ′(t) and 
v left parenthesis t right parenthesis equals upper V prime left parenthesis t right parenthesisv(t) = V ′(t). 
The failure intensity function of repairable systems is deﬁned as 
lamda left parenthesis t right parenthesis equals StartFraction w left parenthesis t right parenthesis Over upper A left parenthesis t right parenthesis EndFraction comma t greater than or equals 0 periodλ(t) = w(t)
A(t),
t ≥0.
(9.3.5)

330
9
Reliability Analysis
Notice that if the system is unrepairable, then upper W left parenthesis t right parenthesis equals upper F left parenthesis t right parenthesisW(t) = F(t), w left parenthesis t right parenthesis equals f left parenthesis t right parenthesisw(t) = f (t), upper A left parenthesis t right parenthesis equals upper R left parenthesis t right parenthesisA(t) =
R(t), and lamda left parenthesis t right parenthesisλ(t) is the hazard function h left parenthesis t right parenthesish(t). Let  upper Q left parenthesis t right parenthesis equals 1 minus upper A left parenthesis t right parenthesisQ(t) = 1 −A(t) and v left parenthesis t right parenthesis equals upper V prime left parenthesis t right parenthesisv(t) = V ′(t). 
The repair intensity function is 
mu left parenthesis t right parenthesis equals StartFraction v left parenthesis t right parenthesis Over upper Q left parenthesis t right parenthesis EndFraction comma t greater than or equals 0 periodμ(t) = v(t)
Q(t),
t ≥0.
(9.3.6) 
The function upper V left parenthesis t right parenthesis equals upper E left brace upper N Subscript upper R Baseline left parenthesis t right parenthesis right braceV (t) = E{NR(t)} is called the renewal function. Notice that 
StartLayout 1st Row 1st Column probability left brace upper N Subscript upper R Baseline left parenthesis t right parenthesis greater than or equals n right brace 2nd Column equals probability left brace upper C 1 plus midline horizontal ellipsis plus upper C Subscript n Baseline less than or equals t right brace 2nd Row 1st Column Blank 2nd Column equals upper K Subscript n Baseline left parenthesis t right parenthesis comma t greater than or equals 0 comma EndLayoutPr{NR(t) ≥n} = Pr{C1 + · · · + Cn ≤t}
= Kn(t),
t ≥0,
(9.3.7) 
where upper K Subscript n Baseline left parenthesis t right parenthesisKn(t) is the c.d.f. of upper C 1 plus midline horizontal ellipsis plus upper C Subscript nC1 + · · · + Cn. 
Since upper N Subscript upper R Baseline left parenthesis t right parenthesisNR(t) is a non-negative random variable, the renewal function is 
StartLayout 1st Row 1st Column upper V left parenthesis t right parenthesis 2nd Column equals sigma summation Underscript n equals 1 Overscript normal infinity Endscripts probability left brace upper N Subscript r Baseline left parenthesis t right parenthesis greater than or equals n right brace 2nd Row 1st Column Blank 2nd Column equals sigma summation Underscript n equals 1 Overscript normal infinity Endscripts upper K Subscript n Baseline left parenthesis t right parenthesis period EndLayout
V (t) =
∞

n=1
Pr{Nr(t) ≥n}
=
∞

n=1
Kn(t).
(9.3.8) 
Example 9.6 Suppose that upper T upper T upper F tilde upper E left parenthesis beta right parenthesisT T F ∼E(β) and that the repair is instantaneous. Then, 
C is distributed like upper E left parenthesis beta right parenthesisE(β) and upper K Subscript n Baseline left parenthesis t right parenthesisKn(t) is the c.d.f. of upper G left parenthesis n comma beta right parenthesisG(n, β), i.e., 
upper K Subscript n Baseline left parenthesis t right parenthesis equals 1 minus upper P left parenthesis n minus 1 semicolon StartFraction t Over beta EndFraction right parenthesis comma n equals 1 comma 2 comma midline horizontal ellipsis commaKn(t) = 1 −P

n −1; t
β

,
n = 1, 2, · · · ,
where upper P left parenthesis j semicolon lamda right parenthesisP(j; λ) is the c.d.f. of a Poisson random variable with mean lamdaλ. Thus, in the 
present case, 
StartLayout 1st Row 1st Column upper V left parenthesis t right parenthesis 2nd Column equals sigma summation Underscript n equals 1 Overscript normal infinity Endscripts left parenthesis 1 minus upper P left parenthesis n minus 1 semicolon StartFraction t Over beta EndFraction right parenthesis right parenthesis 2nd Row 1st Column Blank 2nd Column equals upper E left brace Pois left parenthesis StartFraction t Over beta EndFraction right parenthesis right brace equals StartFraction t Over beta EndFraction comma t greater than or equals 0 period EndLayout
V (t) =
∞

n=1

1 −P

n −1; t
β

= E

Pois
 t
β

= t
β ,
t ≥0.
Here Poisleft parenthesis StartFraction t Over beta EndFraction right parenthesis

t
β

designates a random variable having a Poisson distribution with mean 
t divided by betat/β.
black medium square■
At time t, 0 less than t less than normal infinity0 < t < ∞, there are two possible events: 
upper E 1E1: 
The ﬁrst cycle is not yet terminated. 
upper E 2E2: 
The ﬁrst cycle has terminated at some time before t. 
Accordingly, upper V left parenthesis t right parenthesisV (t) can be written as 
upper V left parenthesis t right parenthesis equals upper K left parenthesis t right parenthesis plus integral Subscript 0 Superscript t Baseline k left parenthesis x right parenthesis upper V left parenthesis t minus x right parenthesis normal d x periodV (t) = K(t) +
 t
0
k(x)V (t −x) dx.
(9.3.9)

9.3
Availability of Repairable Systems
331
The derivative of upper V left parenthesis t right parenthesisV (t) is called the renewal density. Let  v left parenthesis t right parenthesis equals upper V prime left parenthesis t right parenthesisv(t) = V ′(t). Since 
upper V left parenthesis 0 right parenthesis equals 0V (0) = 0, we obtain by differentiating this equation that 
v left parenthesis t right parenthesis equals k left parenthesis t right parenthesis plus integral Subscript 0 Superscript t Baseline k left parenthesis x right parenthesis v left parenthesis t minus x right parenthesis normal d x periodv(t) = k(t) +
 t
0
k(x)v(t −x) dx.
(9.3.10) 
Let v Superscript asterisk Baseline left parenthesis s right parenthesisv∗(s) and k Superscript asterisk Baseline left parenthesis s right parenthesisk∗(s) denote the Laplace transforms of v left parenthesis t right parenthesisv(t) and k left parenthesis t right parenthesisk(t), respectively. 
Then, from the above equation, 
v Superscript asterisk Baseline left parenthesis s right parenthesis equals k Superscript asterisk Baseline left parenthesis s right parenthesis plus k Superscript asterisk Baseline left parenthesis s right parenthesis v Superscript asterisk Baseline left parenthesis s right parenthesis commav∗(s) = k∗(s) + k∗(s)v∗(s),
(9.3.11) 
or, since k Superscript asterisk Baseline left parenthesis s right parenthesis equals f Superscript asterisk Baseline left parenthesis s right parenthesis g Superscript asterisk Baseline left parenthesis s right parenthesisk∗(s) = f ∗(s)g∗(s), 
v Superscript asterisk Baseline left parenthesis s right parenthesis equals StartFraction f Superscript asterisk Baseline left parenthesis s right parenthesis g Superscript asterisk Baseline left parenthesis s right parenthesis Over 1 minus f Superscript asterisk Baseline left parenthesis s right parenthesis g Superscript asterisk Baseline left parenthesis s right parenthesis EndFraction periodv∗(s) =
f ∗(s)g∗(s)
1 −f ∗(s)g∗(s).
(9.3.12) 
The renewal density v left parenthesis t right parenthesisv(t) can be obtained by inverting v Superscript asterisk Baseline left parenthesis s right parenthesisv∗(s). 
Example 9.7 As before, suppose that the TTF is upper E left parenthesis beta right parenthesisE(β) and that the TTR is upper E left parenthesis gamma right parenthesisE(γ ). 
Let lamda equals StartFraction 1 Over beta EndFractionλ = 1
β and mu equals StartFraction 1 Over gamma EndFractionμ = 1
γ
f Superscript asterisk Baseline left parenthesis s right parenthesis equals StartFraction lamda Over lamda plus s EndFraction commaf ∗(s) =
λ
λ + s ,
and 
g Superscript asterisk Baseline left parenthesis s right parenthesis equals StartFraction mu Over mu plus s EndFraction periodg∗(s) =
μ
μ + s .
Then 
StartLayout 1st Row 1st Column v Superscript asterisk Baseline left parenthesis s right parenthesis 2nd Column equals StartFraction lamda mu Over s squared plus left parenthesis lamda plus mu right parenthesis s EndFraction 2nd Row 1st Column Blank 2nd Column equals StartFraction lamda mu Over lamda plus mu EndFraction left parenthesis StartFraction 1 Over s EndFraction minus StartFraction 1 Over s plus lamda plus mu EndFraction right parenthesis period EndLayoutv∗(s) =
λμ
s2 + (λ + μ)s
=
λμ
λ + μ
1
s −
1
s + λ + μ

.
StartFraction 1 Over s EndFraction1
s is the Laplace transform of 1, and StartFraction lamda plus mu Over s plus lamda plus mu EndFraction λ+μ
s+λ+μ is the Laplace transform of upper E left parenthesis StartFraction 1 Over lamda plus mu EndFraction right parenthesisE

1
λ+μ

. 
Hence 
v left parenthesis t right parenthesis equals StartFraction lamda mu Over lamda plus mu EndFraction minus StartFraction lamda mu Over lamda plus mu EndFraction e Superscript minus t left parenthesis lamda plus mu right parenthesis Baseline comma t greater than or equals 0 periodv(t) =
λμ
λ + μ −
λμ
λ + μe−t(λ+μ),
t ≥0.
Integrating v left parenthesis t right parenthesisv(t), we obtain the renewal function 
upper V left parenthesis t right parenthesis equals StartFraction lamda mu Over lamda plus mu EndFraction t minus StartFraction lamda mu Over left parenthesis lamda plus mu right parenthesis squared EndFraction left parenthesis 1 minus e Superscript minus t left parenthesis lamda plus mu right parenthesis Baseline right parenthesis comma 0 less than or equals t less than normal infinity periodV (t) =
λμ
λ + μt −
λμ
(λ + μ)2 (1 −e−t(λ+μ)),
0 ≤t < ∞.

332
9
Reliability Analysis
In a similar fashion, we can show that 
upper W left parenthesis t right parenthesis equals StartFraction lamda mu Over lamda plus mu EndFraction t plus StartFraction lamda squared Over left parenthesis lamda plus mu right parenthesis squared EndFraction left parenthesis 1 minus e Superscript minus t left parenthesis lamda plus mu right parenthesis Baseline right parenthesis comma 0 less than or equals t less than normal infinity periodW(t) =
λμ
λ + μt +
λ2
(λ + μ)2 (1 −e−t(λ+μ)),
0 ≤t < ∞.
Since upper W left parenthesis t right parenthesis greater than upper V left parenthesis t right parenthesisW(t) > V (t) if, and only if, the last cycle is still incomplete and the system 
is down, the probability, upper Q left parenthesis t right parenthesisQ(t), that the system is down at time t is 
StartLayout 1st Row 1st Column upper Q left parenthesis t right parenthesis 2nd Column equals upper W left parenthesis t right parenthesis minus upper V left parenthesis t right parenthesis 2nd Row 1st Column Blank 2nd Column equals StartFraction lamda Over lamda plus mu EndFraction minus StartFraction lamda Over lamda plus mu EndFraction e Superscript minus t left parenthesis lamda plus mu right parenthesis Baseline comma t greater than or equals 0 period EndLayoutQ(t) = W(t) −V (t)
=
λ
λ + μ −
λ
λ + μe−t(λ+μ),
t ≥0.
Thus, the availability function is 
StartLayout 1st Row 1st Column upper A left parenthesis t right parenthesis 2nd Column equals 1 minus upper Q left parenthesis t right parenthesis 2nd Row 1st Column Blank 2nd Column equals StartFraction mu Over lamda plus mu EndFraction plus StartFraction lamda Over lamda plus mu EndFraction e Superscript minus t left parenthesis lamda plus mu right parenthesis Baseline comma t greater than or equals 0 period EndLayoutA(t) = 1 −Q(t)
=
μ
λ + μ +
λ
λ + μe−t(λ+μ),
t ≥0.
Notice that the availability at large values of t is approximately 
limit Underscript t right arrow normal infinity Endscripts upper A left parenthesis t right parenthesis equals StartFraction mu Over lamda plus mu EndFraction equals StartFraction beta Over beta plus gamma EndFraction period lim
t→∞A(t) =
μ
λ + μ =
β
β + γ .
black medium square■
The availability function upper A left parenthesis t right parenthesisA(t) can be determined from upper R left parenthesis t right parenthesisR(t) and v left parenthesis t right parenthesisv(t) by solving 
the equation 
upper A left parenthesis t right parenthesis equals upper R left parenthesis t right parenthesis plus integral Subscript 0 Superscript t Baseline v left parenthesis x right parenthesis upper R left parenthesis t minus x right parenthesis normal d x periodA(t) = R(t) +
 t
0
v(x)R(t −x) dx.
(9.3.13) 
The Laplace transform of this equation is 
upper A Superscript asterisk Baseline left parenthesis s right parenthesis equals StartFraction upper R Superscript asterisk Baseline left parenthesis s right parenthesis Over 1 minus f Superscript asterisk Baseline left parenthesis s right parenthesis g Superscript asterisk Baseline left parenthesis s right parenthesis EndFraction comma 0 less than s less than normal infinity periodA∗(s) =
R∗(s)
1 −f ∗(s)g∗(s),
0 < s < ∞.
(9.3.14) 
This theory can be useful in assessing different system structures, with respect to 
their availability. The following asymptotic (large t approximations) results are very 
useful. Let muμ and sigma squaredσ 2 be the mean and variance of the cycle time. 
StartLayout 1st Row 1st Column left parenthesis 1 right parenthesis 2nd Column Blank 3rd Column Blank 4th Column limit Underscript t right arrow normal infinity Endscripts StartFraction upper V left parenthesis t right parenthesis Over t EndFraction equals StartFraction 1 Over mu EndFraction EndLayout StartLayout 2nd Row 1st Column left parenthesis 2 right parenthesis 2nd Column Blank 3rd Column Blank 4th Column limit Underscript t right arrow normal infinity Endscripts left parenthesis upper V left parenthesis t plus a right parenthesis minus upper V left parenthesis t right parenthesis right parenthesis equals StartFraction a Over mu EndFraction comma a greater than 0 EndLayout StartLayout 3rd Row 1st Column left parenthesis 3 right parenthesis 2nd Column Blank 3rd Column Blank 4th Column limit Underscript t right arrow normal infinity Endscripts left parenthesis upper V left parenthesis t right parenthesis minus StartFraction t Over mu EndFraction right parenthesis equals StartFraction sigma squared Over 2 mu squared EndFraction minus one half period EndLayout
(1)
lim
t→∞
V (t)
t
= 1
μ
(9.3.15) 
(2)
lim 
t→∞ (V (t  + a) − V (t)) = a 
μ ,
a
 
>
 
0
(9.3.16) 
(3)
lim 
t→∞

V (t) − t 
μ

= σ 2 
2μ2 − 1 
2 .
(9.3.17)

9.3
Availability of Repairable Systems
333
If the p.d.f. of C, k left parenthesis t right parenthesisk(t), is continuous, then 
StartLayout 1st Row 1st Column left parenthesis 4 right parenthesis 2nd Column Blank 3rd Column Blank 4th Column limit Underscript t right arrow normal infinity Endscripts v left parenthesis t right parenthesis equals StartFraction 1 Over mu EndFraction EndLayout StartLayout 2nd Row 1st Column left parenthesis 5 right parenthesis 2nd Column Blank 3rd Column Blank 4th Column limit Underscript t right arrow normal infinity Endscripts probability left brace StartFraction upper N Subscript upper R Baseline left parenthesis t right parenthesis minus t divided by mu Over left parenthesis sigma squared t divided by mu cubed right parenthesis Superscript 1 divided by 2 Baseline EndFraction less than or equals z right brace equals upper Phi left parenthesis z right parenthesis EndLayout StartLayout 3rd Row 1st Column left parenthesis 6 right parenthesis 2nd Column Blank 3rd Column Blank 4th Column upper A Subscript normal infinity Baseline equals limit Underscript upper T right arrow normal infinity Endscripts StartFraction 1 Over upper T EndFraction integral Subscript 0 Superscript upper T Baseline upper A left parenthesis t right parenthesis normal d t equals StartFraction upper E left brace upper T upper T upper F right brace Over upper E left brace upper T upper T upper F right brace plus upper E left brace upper T upper T upper R right brace EndFraction period EndLayout(4)
lim
t→∞v(t) = 1
μ
(9.3.18) 
(5)
lim 
t→∞Pr
NR(t) − t/μ 
(σ 2t/μ3)1/2 ≤ z

= (z)
(9.3.19) 
(6)
A∞ = lim 
T →∞ 
1 
T
 T 
0 
A(t) dt =
E{T T F} 
E{T T F} + E{T T R}.
(9.3.20) 
According to (1), the expected number of renewals, upper V left parenthesis t right parenthesisV (t), is approximately t divided by mut/μ, 
for large t. According to (2), we expect approximately a divided by mua/μ renewals in a time 
interval of length left parenthesis t comma t plus a right parenthesis(t, t + a), when t is large. The third result (3) says that t divided by mut/μ is 
an under (over) estimate, for large t, if the squared coefﬁcient of variation sigma squared divided by mu squaredσ 2/μ2 of 
the cycle time is larger (smaller) than 1. The last three properties can be interpreted 
in a similar fashion. We illustrate these asymptotic properties with examples. 
Example 9.8 Consider a repairable system. The TTF [hr] has a gamma distribution 
like upper G left parenthesis 2 comma 100 right parenthesisG(2, 100). The TTR [hr] has a Weibull distribution upper W left parenthesis 2 comma 2.5 right parenthesisW(2, 2.5). Thus, the 
expected TTF is mu Subscript upper T Baseline equals 200μT = 200 [hr], and the expected TTR is mu Subscript s Baseline equals 2.5 times upper Gamma left parenthesis three halves right parenthesis equals 1.25 StartRoot pi EndRoot equals 2.2μs = 2.5 × 

3
2

=
1.25√π = 2.2 [hr]. The asymptotic availability is 
upper A Subscript normal infinity Baseline equals StartFraction 200 Over 202.2 EndFraction equals 0.989 periodA∞= 200
202.2 = 0.989.
That is, in the long run, the proportion of total availability time is 98.9%. 
The expected cycle time is mu Subscript c Baseline equals 222.2μc = 222.2 and the variance of the cycle time is 
StartLayout 1st Row 1st Column sigma Subscript c Superscript 2 2nd Column equals 2 times 100 squared plus 6.25 left bracket upper Gamma left parenthesis 2 right parenthesis minus upper Gamma squared left parenthesis three halves right parenthesis right bracket 2nd Row 1st Column Blank 2nd Column equals 20,000 plus 1.34126 equals 20,001.34126 period EndLayoutσ 2
c = 2 × 1002 + 6.25

(2) −2
3
2

= 20,000 + 1.34126 = 20,001.34126.
Thus, during 2000 [hr] of scheduled operation, we expect close to StartFraction 2000 Over 202.2 EndFraction approximately equals 102000
202.2 ∼= 10
renewal cycles. The probability that upper N Subscript upper R Baseline left parenthesis 2000 right parenthesisNR(2000) will be less than 11 is 
probability left brace upper N Subscript upper R Baseline left parenthesis 2000 right parenthesis less than or equals 11 right brace approximately equals upper Phi left parenthesis StartFraction 2 Over 1.91 EndFraction right parenthesis equals upper Phi left parenthesis 1.047 right parenthesis equals 0.8525 periodPr{NR(2000) ≤11} ∼= 
 2
1.91

= (1.047) = 0.8525.
black medium square■
An important question is determining the probability, for large values of t, that 
we will ﬁnd the system operating and will continue to operate without a failure for 
at least u additional time units. This function is called the asymptotic operational 
reliability and is given by 
upper R Subscript normal infinity Baseline left parenthesis u right parenthesis equals upper A Subscript normal infinity Baseline dot StartFraction integral Subscript u Superscript normal infinity Baseline upper R left parenthesis u right parenthesis normal d u Over mu Subscript upper T Baseline EndFraction comma 0 less than or equals u commaR∞(u) = A∞·
 ∞
u R(u) du
μT
,
0 ≤u,
(9.3.21) 
where upper R left parenthesis u right parenthesis equals 1 minus upper F Subscript upper T Baseline left parenthesis u right parenthesisR(u) = 1 −FT (u).

334
9
Reliability Analysis
Example 9.9 We continue discussing the case of Example 9.8. In this case, 
StartLayout 1st Row 1st Column upper F Subscript upper T Baseline left parenthesis u right parenthesis 2nd Column equals probability left brace upper G left parenthesis 2 comma 100 right parenthesis less than or equals u right brace 2nd Row 1st Column Blank 2nd Column equals probability left brace upper G left parenthesis 2 comma 1 right parenthesis less than or equals StartFraction u Over 100 EndFraction right brace 3rd Row 1st Column Blank 2nd Column equals 1 minus upper P left parenthesis 1 semicolon StartFraction u Over 100 EndFraction right parenthesis equals 1 minus e Superscript negative u divided by 100 Baseline minus StartFraction u Over 100 EndFraction e Superscript negative u divided by 100 Baseline comma EndLayoutFT (u) = Pr{G(2, 100) ≤u}
= Pr

G(2, 1) ≤
u
100

= 1 −P

1; u
100

= 1 −e−u/100 −
u
100e−u/100,
and 
upper R left parenthesis u right parenthesis equals e Superscript negative u divided by 100 Baseline plus StartFraction u Over 100 EndFraction e Superscript negative u divided by 100 Baseline periodR(u) = e−u/100 +
u
100e−u/100.
Furthermore, mu Subscript upper T Baseline equals 200μT = 200 and upper A Subscript normal infinity Baseline equals 0.989A∞= 0.989. Hence 
StartLayout 1st Row 1st Column upper R Subscript normal infinity Baseline left parenthesis u right parenthesis 2nd Column equals 0.989 dot StartFraction integral Subscript u Superscript normal infinity Baseline left parenthesis 1 plus StartFraction x Over 100 EndFraction right parenthesis e Superscript negative x divided by 100 Baseline normal d x Over 200 EndFraction 2nd Row 1st Column Blank 2nd Column equals StartFraction 98.9 Over 200 EndFraction left parenthesis 2 plus StartFraction u Over 100 EndFraction right parenthesis e Superscript negative u divided by 100 Baseline period EndLayoutR∞(u) = 0.989 ·
 ∞
u

1 +
x
100

e−x/100 dx
200
= 98.9
200

2 +
u
100

e−u/100.
Thus, upper R Subscript normal infinity Baseline left parenthesis 0 right parenthesis equals 0.989R∞(0) = 0.989, upper R Subscript normal infinity Baseline left parenthesis 100 right parenthesis equals 0.546R∞(100) = 0.546, and upper R Subscript normal infinity Baseline left parenthesis 200 right parenthesis equals 0.268R∞(200) = 0.268.
black medium square■
We conclude the section by introducing two Python functions from the mistat 
package, availabilityEBD, and renewalEBD which provide the bootstrap EBD of 
the number of renewals in a speciﬁed time interval and the EBD of the asymptotic 
availability index upper A Subscript normal infinityA∞, based on observed samples of failure times and repair times. 
These programs provide computer-aided estimates of the renewal distribution and 
of the precision of upper A Subscript normal infinityA∞. We illustrate this in the following example. 
Example 9.10 Consider again the renewal process described in Example 9.8. 
Consider n equals 50n = 50 observed values of i.i.d. TTF from upper G left parenthesis 2 comma 100 right parenthesisG(2, 100) and n equals 50n = 50 observed 
repair times. We run renewalEBD with n equals 1000n = 1000 bootstrap samples to obtain an 
EBD of the number of renewals in 1000 [hr]. 
np.random.seed(1) 
ttf = stats.gamma(2, scale=100).rvs(50) 
ttr = stats.gamma(2, scale=1).rvs(50) 
_ = mistat.availabilityEBD(ttf, ttr, n=1000, seed=1) 
result = mistat.renewalEBD(ttf, ttr, time=1000, n=1000, seed=1) 
np.quantile(result, [0.025, 0.975]) 
The estimated MTTF from ttf is 199.50 
The estimated MTTR from ttr is 2.00 
The estimated asymptotic availability is 0.9901 
count
1000.000000 
mean
0.989978 
std
0.001434 
min
0.984414 
25%
0.989123 
50%
0.990071

9.4
Types of Observations on T T F
335
75%
0.990993 
max
0.993743 
Name: availability EBD, dtype: float64 
The estimated MEAN NUMBER Of RENEWALS is 6.70 
count
1000.000000 
mean
6.698000 
std
1.532039 
min
3.000000 
25%
6.000000 
50%
7.000000 
75%
8.000000 
max
13.000000 
Name: number of renewals EBD, dtype: float64 
array([ 4., 10.]) 
The program yields that the mean number of renewals for 1000 h of operation is 
6.70. This is the bootstrap estimate of upper V left parenthesis 1000 right parenthesisV (1000). The asymptotic approximation is 
1000 divided by 202.2 equals 4.9461000/202.2 = 4.946. The bootstrap conﬁdence interval for upper V left parenthesis 1000 right parenthesisV (1000) at 0.95 level of 
conﬁdence is (4,9). This conﬁdence interval covers the asymptotic approximation. 
Accordingly, the bootstrap estimate of 6.7 is not signiﬁcantly different from the 
asymptotic approximation.
black medium square■
Additional topics of interest are maintenance, repairability, and availability. 
The objective is to increase the availability by instituting maintenance procedures 
and by adding standby systems and repairmen. The question is what the optimal 
maintenance period is and how many standby systems and repairmen to add. The 
interested reader is referred to Zacks (1992, Ch. 4) and Gertsbakh (1989). 
In the following sections we discuss statistical problems associated with reliabil-
ity assessment, when one does not know deﬁnitely the model and the values of its 
parameters. 
9.4 
Types of Observations on T T F  
The analysis of data depends on the type of observations available. Dealing with 
TTF and TTR random variables, we wish to have observations which give us the 
exact length of time interval from activation (failure) of a system (component) till 
its failure (repair). However, one can ﬁnd that proper records have not been kept, 
and instead one can ﬁnd only the number of failures (repairs) in a given period of 
time. These are discrete random variables rather than the continuous ones under 
investigation. Another type of problem typical to reliability studies is that some 
observations are censored. For example, if it is decided to put n identical systems 
on test for a speciﬁed length of time t Superscript asteriskt∗, we may observe only a random number, upper K Subscript nKn, 
of failures in the time interval left parenthesis 0 comma t Superscript asterisk Baseline right bracket(0, t∗]. For the other n minus upper K Subscript nn−Kn systems which did not fail, 
we have only partial information, i.e., their TTF is greater than t Superscript asteriskt∗. The observations 
on these systems are called right censored. In the above example n units are put 
on test at the same time. The censoring time t Superscript asteriskt∗is a ﬁxed time. Sometimes we have 
observations with random censoring. This is the case when we carry a study for

336
9
Reliability Analysis
a ﬁxed length of time t Superscript asteriskt∗[years], but the units (systems) enter the study at random 
times between 0 and t Superscript asteriskt∗, according to some distribution. 
Suppose that a unit enters the study at the random time tauτ, 0 less than tau less than t Superscript asterisk0 < τ < t∗, and its 
TTF is T . We can observe only upper W equals min left parenthesis upper T comma t Superscript asterisk Baseline minus tau right parenthesisW = min(T, t∗−τ). Here the censoring time is the 
random variable t Superscript asterisk Baseline minus taut∗−τ. An example of such a situation is when we sell a product 
under warranty. The units of this product are sold to different customers at random 
times during the study period left parenthesis 0 comma t Superscript asterisk Baseline right parenthesis(0, t∗). Products that fail are brought back for repair. 
If this happens during the study period, we have an uncensored observation on the 
TTF of that unit; otherwise the observation is censored, i.e., upper W equals t Superscript asterisk Baseline minus tauW = t∗−τ. 
The censored observations described above are time censored. Another type of 
censoring is frequency censoring. This is done when n units are put on test at the 
same time, but the test is terminated the instant the rth failure occurs. In this case 
the length of the test is the rth order statistic of failure times upper T Subscript n comma rTn,r left parenthesis r equals 1 comma midline horizontal ellipsis comma n right parenthesis(r = 1, · · · , n). 
Notice that upper T Subscript n comma r Baseline equals upper T Subscript left parenthesis r right parenthesisTn,r = T(r), where upper T Subscript left parenthesis 1 right parenthesis Baseline less than upper T Subscript left parenthesis 2 right parenthesis Baseline less than midline horizontal ellipsis less than upper T Subscript left parenthesis n right parenthesisT(1) < T(2) < · · · < T(n) are the order statistics 
of n i.i.d. TTF’s. If T is distributed exponentially, upper E left parenthesis beta right parenthesisE(β), for example, the expected 
length of the experiment is 
upper E left brace upper T Subscript n comma r Baseline right brace equals beta left parenthesis StartFraction 1 Over n EndFraction plus StartFraction 1 Over n minus 1 EndFraction plus midline horizontal ellipsis plus StartFraction 1 Over n minus r plus 1 EndFraction right parenthesis periodE{Tn,r} = β
1
n +
1
n −1 + · · · +
1
n −r + 1

.
There may be substantial time saving if we terminate the study at the rth failure, 
when r less than nr < n. For example, in the exponential case, with upper E left brace upper T right brace equals beta equals 1000E{T } = β = 1000 [hr] and 
n equals 20n = 20, 
upper E left brace upper T Subscript 20 comma 20 Baseline right brace equals 1000 times left parenthesis 1 plus one half plus one third plus midline horizontal ellipsis plus one twentieth right parenthesis equals 3597.7 left bracket hr right bracket periodE{T20,20} = 1000 ×

1 + 1
2 + 1
3 + · · · + 1
20

= 3597.7 [hr].
On the other hand, for r equals 10r = 10 we have upper E left brace upper T Subscript 20 comma 10 Baseline right brace equals 668.8E{T20,10} = 668.8 [hr]. Thus, a frequency 
censored experiment with r equals 10r = 10 and n equals 20n = 20, beta equals 1000β = 1000 lasts on the average 
only 19% of the time length of an uncensored experiment. We will see later how 
one can determine the optimal n and r for estimating the mean TTF (MTTF) in the 
exponential case. 
9.5 
Graphical Analysis of Life Data 
In this section we discuss graphical procedures for ﬁtting a life distribution to failure 
data and derive estimates of the parameters from the graphs. 
Let 
t 1 comma t 2 comma midline horizontal ellipsis comma t Subscript n Baselinet1, t2, · · · , tn be n uncensored observation on i.i.d. random variables 
upper T 1 comma midline horizontal ellipsis comma upper T Subscript n BaselineT1, · · · , Tn, having some life distribution upper F left parenthesis t right parenthesisF(t). 
The empirical c.d.f., given t 1 comma midline horizontal ellipsis comma t Subscript n Baselinet1, · · · , tn, is deﬁned as

9.5
Graphical Analysis of Life Data
337
Fig. 9.3 The empirical c.d.f. of a random sample of 100 variables from upper W left parenthesis 1.5 comma 100 right parenthesisW(1.5, 100)
upper F Subscript n Baseline left parenthesis t right parenthesis equals StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts upper I left brace t Subscript i Baseline less than or equals t right brace commaFn(t) = 1
n
n

i=1
I{ti ≤t},
(9.5.1) 
where upper I left brace t Subscript i Baseline less than or equals t right braceI{ti ≤t} is the indicator variable, assuming the value 1 if t Subscript i Baseline less than or equals tti ≤t and the 
value 0 otherwise. A theorem in probability theory states that the empirical c.d.f. 
upper F Subscript n Baseline left parenthesis t right parenthesisFn(t) converges to upper F left parenthesis t right parenthesisF(t), as n right arrow normal infinityn →∞. 
In Fig. 9.3 we present the empirical c.d.f. of a random sample of 100 vari-
ables having the Weibull distribution upper W left parenthesis 1.5 comma 100 right parenthesisW(1.5, 100). Since upper F Subscript n Baseline left parenthesis t Subscript left parenthesis i right parenthesis Baseline right parenthesis equals StartFraction i Over n EndFractionFn(t(i)) =
i
n for i equals 1 comma 2 comma midline horizontal ellipsis comma ni =
1, 2, · · · , n, the  StartFraction i Over n EndFractioni
nth quantile of upper F Subscript n Baseline left parenthesis t right parenthesisFn(t) is the ordered statistic t Subscript left parenthesis i right parenthesist(i). Accordingly, if 
upper F left parenthesis t right parenthesisF(t) has some speciﬁc distribution, the scattergram of 
left parenthesis upper F Superscript negative 1 Baseline left parenthesis StartFraction i Over n EndFraction right parenthesis comma t Subscript left parenthesis i right parenthesis Baseline right parenthesis left parenthesis i equals 1 comma midline horizontal ellipsis comma n right parenthesis

F −1
 i
n

, t(i)

(i = 1, · · · , n)
should be around a straight line with slope 1. The plot of t Subscript left parenthesis i right parenthesist(i) versus upper F Superscript negative 1 Baseline left parenthesis StartFraction i Over n EndFraction right parenthesisF −1  i
n

is 
called a bold upper QQ–bold upper QQ Plot (quantile versus quantile probability). The Q–Q plot is the basic 
graphical procedure to test whether a given sample of failure times is generated by 
a speciﬁc life distribution. Since upper F Superscript negative 1 Baseline left parenthesis 1 right parenthesis equals normal infinityF −1(1) = ∞for the interesting life distribution, 
the quantile of F is taken at StartFraction i Over n plus 1 EndFraction
i
n+1 or at some other StartFraction i plus alpha Over n plus beta EndFraction i+α
n+β , which give better plotting 
position for a speciﬁc distribution. For the normal distribution, StartFraction i minus 3 divided by 8 Over n plus 1 divided by 4 EndFraction i−3/8
n+1/4 is used. 
If the distribution depends on location and scale parameters, we plot t Subscript left parenthesis i right parenthesist(i) against 
the quantiles of the standard distribution. The intercept and the slope of the line 
ﬁtted through the points yield estimates of these location and scale parameters. 
For example, suppose that t 1 comma midline horizontal ellipsis comma t Subscript n Baselinet1, · · · , tn are values of a sample from an upper N left parenthesis mu comma sigma squared right parenthesisN(μ, σ 2)
distribution. Thus, t Subscript left parenthesis i right parenthesis Baseline almost equals mu plus sigma upper Phi Superscript negative 1 Baseline left parenthesis StartFraction i Over n EndFraction right parenthesist(i) ≈μ + σ−1  i
n

. Thus, if we plot  t Subscript left parenthesis i right parenthesist(i) against upper Phi Superscript negative 1 Baseline left parenthesis StartFraction i Over n EndFraction right parenthesis−1  i
n

,

338
9
Reliability Analysis
we should have points around a straight line whose slope is an estimate of sigmaσ and 
intercept an estimate of muμ. 
We focus attention here on three families of life distributions: 
1. The shifted exponential 
2. The Weibull 
3. The lognormal 
The shifted exponential c.d.f. has the form 
upper F left parenthesis t semicolon mu comma beta right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column 1 minus exp left brace StartFraction t minus mu Over beta EndFraction right brace comma 2nd Column t greater than or equals mu 2nd Row 1st Column Blank 3rd Row 1st Column 0 comma 2nd Column t less than mu period EndLayoutF(t; μ, β) =
⎧
⎪⎪⎨
⎪⎪⎩
1 −exp

t−μ
β

,
t ≥μ
0,
t < μ.
(9.5.2) 
The starting point of the exponential distribution upper E left parenthesis beta right parenthesisE(β) is shifted to a point muμ. 
Location parameters of interest in reliability studies are mu greater than or equals 0μ ≥0. Notice that the 
pth quantile, 0 less than p less than normal infinity0 < p < ∞, of the shifted exponential is 
t Subscript p Baseline equals mu plus beta left parenthesis minus log left parenthesis 1 minus p right parenthesis right parenthesis periodtp = μ + β(−log(1 −p)).
(9.5.3) 
Accordingly, for exponential Q–Q plots, we plot t Subscript left parenthesis i right parenthesist(i) versus upper E Subscript i comma n Baseline equals minus log left parenthesis 1 minus StartFraction i Over n plus 1 EndFraction right parenthesisEi,n= −log

1−
i
n+1

. 
Notice that in this plot, the intercept estimates the location parameter muμ, and the 
slope estimates betaβ. In the Weibull case, upper W left parenthesis nu comma beta right parenthesisW(ν, β), the c.d.f. is 
upper F left parenthesis t semicolon nu comma beta right parenthesis equals 1 minus exp left brace minus left parenthesis StartFraction t Over beta EndFraction right parenthesis Superscript nu Baseline right brace comma t greater than or equals 0 periodF(t; ν, β) = 1 −exp

−
 t
β
ν
,
t ≥0.
(9.5.4) 
Thus, if t Subscript ptp is the pth quantile, 
log t Subscript p Baseline equals log beta plus StartFraction 1 Over nu EndFraction log left parenthesis minus log left parenthesis 1 minus p right parenthesis right parenthesis period log tp = log β + 1
ν log(−log(1 −p)).
(9.5.5) 
For this reason, we plot log t Subscript left parenthesis i right parenthesislog t(i) versus 
upper W Subscript i comma n Baseline equals log left parenthesis minus log left parenthesis 1 minus StartFraction i Over n plus 1 EndFraction right parenthesis right parenthesis comma i equals 1 comma midline horizontal ellipsis comma n periodWi,n = log

−log

1 −
i
n + 1

,
i = 1, · · · , n.
(9.5.6) 
The slope of the straight line estimates 1 divided by nu1/ν and the intercept estimates log betalog β. In the  
lognormal case, we plot log t Subscript left parenthesis i right parenthesislog t(i) against upper Phi Superscript negative 1 Baseline left parenthesis StartFraction i minus 3 divided by 8 Over n plus 1 divided by 4 EndFraction right parenthesis−1 
i−3/8
n+1/4

. 
Example 9.11 In Fig. 9.4, we present the Q–Q plot of 100 values generated at 
random from an exponential distribution upper E left parenthesis 5 right parenthesisE(5). We ﬁt a straight line through the 
origin to the points by the method of least squares.

9.5
Graphical Analysis of Life Data
339
Fig. 9.4 Q–Q plot of a sample of 100 values from upper E left parenthesis 5 right parenthesisE(5)
np.random.seed(1) 
rv = stats.expon(scale=5).rvs(100) 
# Use stats.probplot to get x and Ei values 
res = stats.probplot(rv, dist=stats.expon, rvalue=True) 
df = pd.DataFrame({'Ei': res[0][0], 'x': res[0][1]}) 
model = smf.ols('x ~ Ei - 1', data=df).fit() 
print(f'regression parameter beta={model.params[0]:.3f}') 
print(f'estimate median = {np.log(2) * model.params[0]:.3f}') 
print(f'true median = {np.median(rv):.3f}') 
regression parameter beta=4.774 
estimate median = 3.309 
true median = 3.185 
A linear regression routine provides the line 
ModifyingAbove x With caret equals 4.773 asterisk upper E periodˆx = 4.773 ∗E.
Accordingly, the slope of a straight line ﬁtted to the points provides an estimate of 
the true mean and standard deviation, beta equals 5β = 5. An estimate of the median is 
ModifyingAbove x With caret left parenthesis 0.693 right parenthesis equals 0.693 times 4.773 equals 3.309 periodˆx(0.693) = 0.693 × 4.773 = 3.309.
The true median is upper M e equals 3.185Me = 3.185.

340
9
Reliability Analysis
np.random.seed(1) 
rv = stats.weibull_min(2, scale=2.5).rvs(100) 
# Use stats.probplot to get x and Wi values 
res = stats.probplot(rv, dist=stats.weibull_min, sparams=[1], rvalue=True) 
df = pd.DataFrame({'Wi': np.log(res[0][0]), 'x': np.log(res[0][1])}) 
model = smf.ols('x ~ Wi', data=df).fit() 
intercept, slope = model.params 
print(f'intercept {intercept:.3f} / slope {slope:.3f}') 
z 
beta = np.exp(intercept) 
nu = 1/slope 
print(f'regression parameter nu={nu:.3f}') 
print(f'regression parameter beta={beta:.3f}') 
print(f'estimated median = {beta * np.log(2) ** (1/nu):.3f}') 
print(f'estimated mean = {beta * gamma(1 + 1/nu):.3f}') 
print(f'estimated std = {beta * np.sqrt(gamma(1 + 2/nu) - gamma(1+1/nu)**2):.3f}') 
print(f'sample median = {np.median(rv):.3f}') 
print(f'sample mean = {np.mean(rv):.3f}') 
print(f'sample std = {np.std(rv):.3f}') 
intercept 0.904 / slope 0.608 
regression parameter nu=1.646 
regression parameter beta=2.470 
estimated median = 1.977 
estimated mean = 2.209 
estimated std = 1.378 
sample median = 1.994 
sample mean = 2.148 
sample std = 1.146 
In Fig. 9.5, we provide a probability plot of n equals 100n = 100 values generated from a 
Weibull distribution with parameters nu equals 2ν = 2 and beta equals 2.5β = 2.5. 
Least squares ﬁtting of a straight line to these points yields the line 
ModifyingAbove y With caret equals 0.904 plus 0.608 upper W periodˆy = 0.904 + 0.608W.
Accordingly, we obtain the following estimates: 
StartLayout 1st Row 1st Column ModifyingAbove nu With caret 2nd Column equals 1 divided by .608 equals 1.646 2nd Row 1st Column ModifyingAbove beta With caret 2nd Column equals exp left parenthesis 0.904 right parenthesis equals 2.470 3rd Row 1st Column Median 2nd Column equals ModifyingAbove beta With caret left parenthesis ln 2 right parenthesis Superscript 1 divided by ModifyingAbove nu With caret Baseline equals 1.977 period EndLayoutˆν = 1/.608 = 1.646
ˆβ = exp(0.904) = 2.470
Median = ˆβ(ln 2)1/ˆν = 1.977.
The true median is equal to beta left parenthesis ln 2 right parenthesis Superscript 1 divided by nu Baseline equals 2.081β(ln 2)1/ν = 2.081. The estimate of the mean is 
StartLayout 1st Row 1st Column ModifyingAbove mu With caret 2nd Column equals ModifyingAbove beta With caret upper Gamma left parenthesis 1 plus StartFraction 1 Over ModifyingAbove nu With caret EndFraction right parenthesis 2nd Row 1st Column Blank 2nd Column equals 2.080 period EndLayoutˆμ = ˆβ 

1 + 1
ˆν

= 2.080.
The true mean  is  mu equals beta upper Gamma left parenthesis 1.5 right parenthesis equals 2.216μ = β (1.5) = 2.216. Finally, an estimate of the standard 
deviation is

9.5
Graphical Analysis of Life Data
341
Fig. 9.5 Q–Q plot of a sample of 100 from upper W left parenthesis 2 comma 2.5 right parenthesisW(2, 2.5)
StartLayout 1st Row 1st Column ModifyingAbove sigma With caret 2nd Column equals ModifyingAbove beta With caret left parenthesis upper Gamma left parenthesis 1 plus StartFraction 2 Over ModifyingAbove nu With caret EndFraction right parenthesis minus upper Gamma squared left parenthesis 1 plus StartFraction 1 Over ModifyingAbove nu With caret EndFraction right parenthesis right parenthesis Superscript 1 divided by 2 Baseline 2nd Row 1st Column Blank 2nd Column equals 1.054 period EndLayoutˆσ = ˆβ



1 + 2
ˆν

−2

1 + 1
ˆν
1/2
= 1.054.
The true value is sigma equals beta left parenthesis upper Gamma left parenthesis 2 right parenthesis minus upper Gamma squared left parenthesis 1.5 right parenthesis right parenthesis Superscript 1 divided by 2 Baseline equals 1.158σ = β((2) −2(1.5))1/2 = 1.158.
black medium square■
If observations are censored from the left or from the right, we plot the quantiles 
only from the uncensored part of the sample. The plotting positions take into 
consideration the number of censored values from the left and from the right. For 
example, if n equals 20n = 20 and the 2 smallest observations are censored, the plotting 
positions are 
StartEnclose left right StartLayout 1st Row 1st Column i 2nd Column t Subscript left parenthesis i right parenthesis 3rd Column StartFraction i Over n plus 1 EndFraction 2nd Row 1st Column 1 2nd Column hyphen hyphen hyphen 3rd Column hyphen hyphen hyphen 3rd Row 1st Column 2 2nd Column hyphen hyphen hyphen 3rd Column hyphen hyphen hyphen 4th Row 1st Column 3 2nd Column t Subscript left parenthesis 3 right parenthesis 3rd Column three twenty firsts 5th Row 1st Column vertical ellipsis 2nd Column Blank 3rd Column Blank 6th Row 1st Column 20 2nd Column t 20 3rd Column StartFraction 20 Over 21 EndFraction EndLayout EndEnclose
i
t(i)
i
n+1
1
—
—
2
—
—
3
t(3)
3
21
...
20
t20
20
21

342
9
Reliability Analysis
9.6 
Nonparametric Estimation of Reliability 
The nonparametric Kaplan–Meier method yields an estimate, called the Product 
Limit (PL) estimate of the reliability function, without an explicit reference to the 
life distribution. The estimator of the reliability function at time t is denoted by 
ModifyingAbove upper R With caret Subscript n Baseline left parenthesis t right parenthesis ˆRn(t), where n is the number of units put on test at time t equals 0t = 0. If all the failure 
times 0 less than t 1 less than t 2 less than midline horizontal ellipsis less than t Subscript n Baseline less than normal infinity0 < t1 < t2 < · · · < tn < ∞are known, then the PL estimator is equivalent 
to 
ModifyingAbove upper R With caret Subscript n Baseline left parenthesis t right parenthesis equals 1 minus upper F Subscript n Baseline left parenthesis t right parenthesis comma ˆRn(t) = 1 −Fn(t),
(9.6.1) 
where upper F Subscript n Baseline left parenthesis t right parenthesisFn(t) is the empirical CDF deﬁned earlier. 
In some cases either random or non-random censoring or withdrawals occur and 
we do not have complete information on the exact failure times. Suppose that 0 less than t 1 less than t 2 less than midline horizontal ellipsis less than t Subscript k Baseline less than normal infinity0 <
t1 < t2 < · · · < tk < ∞, k less than or equals nk ≤n, are the failure times and w equals n minus kw = n −k is the total 
number of withdrawals. 
Let upper I Subscript j Baseline equals left parenthesis t Subscript j minus 1 Baseline comma t Subscript j Baseline right parenthesisIj = (tj−1, tj), j equals 1 comma midline horizontal ellipsis comma k plus 1j = 1, · · · , k + 1, with t 0 equals 0t0 = 0, t Subscript k plus 1 Baseline equals normal infinitytk+1 = ∞, be the time 
intervals between recorded failures. Let upper W Subscript jWj be the number of withdrawals during 
the time interval upper I Subscript jIj. The PL estimator of the reliability function is then 
ModifyingAbove upper R With caret Subscript n Baseline left parenthesis t right parenthesis equals upper I left brace t less than t 1 right brace plus sigma summation Underscript i equals 2 Overscript k plus 1 Endscripts upper I left brace t Subscript i minus 1 Baseline less than or equals t less than or equals t Subscript i Baseline right brace product Underscript j equals 1 Overscript i minus 1 Endscripts left parenthesis 1 minus StartFraction 1 Over n Subscript j minus 1 Baseline minus w Subscript j Baseline divided by 2 EndFraction right parenthesis comma ˆRn(t) = I{t < t1} +
k+1

i=2
I{ti−1 ≤t ≤ti}
i−1

j=1

1 −
1
nj−1 −wj/2

,
(9.6.2) 
where n 0 equals nn0 = n, and n Subscript lnl is the number of operating units just prior to the failure time 
t Subscript ltl. 
Usually, when units are tested in the laboratory under controlled conditions, there 
may be no withdrawals. This is not the case, however, if tests are conducted in 
ﬁeld conditions, and units on test may be lost, withdrawn or destroyed for reasons 
different from the failure phenomenon under study. 
Suppose now that systems are installed in the ﬁeld as they are purchased (random 
times). We decide to make a follow-up study of the systems for a period of two years. 
The time till failure of systems participating in the study is recorded. We assume that 
each system operates continuously from the time of installment until its failure. If a 
system has not failed by the end of the study period, the only information available is 
the length of time it has been operating. This is a case of multiple censoring. At the 
end of the study period, we have the following observations left brace left parenthesis upper T Subscript i Baseline comma delta Subscript i Baseline right parenthesis{(Ti, δi), i equals 1 comma midline horizontal ellipsis comma n right bracei = 1, · · · , n}, 
where n is the number of systems participating in the study, upper T Subscript iTi is the length of 
operation of the ith system (TTF or time till censoring), and delta Subscript i Baseline equals 1δi = 1 if ith observation 
is not censored and delta Subscript i Baseline equals 0δi = 0 otherwise. 
Let upper T Subscript left parenthesis 1 right parenthesis Baseline less than or equals upper T Subscript left parenthesis 2 right parenthesis Baseline less than or equals midline horizontal ellipsis less than or equals upper T Subscript left parenthesis n right parenthesisT(1) ≤T(2) ≤· · · ≤T(n) be the order statistic of the operation times and let 
delta Subscript j 1 Baseline comma delta Subscript j 2 Baseline comma midline horizontal ellipsis comma delta Subscript j Sub Subscript n Subscript Baselineδj1, δj2, · · · , δjn be the deltaδ-values corresponding to the ordered T values where j Subscript iji is 
the index of the ith order statistic upper T Subscript left parenthesis i right parenthesisT(i), i.e., upper T Subscript left parenthesis i right parenthesis Baseline equals upper T Subscript jT(i) = Tj, left parenthesis i equals 1 comma midline horizontal ellipsis comma n right parenthesis(i = 1, · · · , n). 
The PL estimator of upper R left parenthesis t right parenthesisR(t) is given by

9.7
Estimation of Life Characteristics
343
StartLayout 1st Row 1st Column ModifyingAbove upper R With caret Subscript n Baseline left parenthesis t right parenthesis 2nd Column equals upper I left brace t less than upper T Subscript left parenthesis 1 right parenthesis Baseline right brace 2nd Row 1st Column Blank 2nd Column plus sigma summation Underscript i equals 1 Overscript n Endscripts upper I left brace upper T Subscript left parenthesis i right parenthesis Baseline less than or equals upper T Subscript left parenthesis i plus 1 right parenthesis Baseline right brace product Underscript j equals 1 Overscript i Endscripts left parenthesis 1 minus StartFraction delta Subscript j Baseline Over n minus j plus 1 EndFraction right parenthesis period EndLayout
ˆRn(t) = I{t < T(1)}
+
n
i=1
I{T(i) ≤T(i+1)}
i	
j=1

1 −
δj
n−j+1

.
(9.6.3) 
Another situation occurs in the laboratory or in ﬁeld studies when the exact 
failure times cannot be recorded. Let 0 less than t 1 less than t 2 less than midline horizontal ellipsis less than t Subscript k Baseline less than normal infinity0 < t1 < t2 < · · · < tk < ∞be ﬁxed 
inspection times. Let w Subscript iwi be the number of withdrawals and f Subscript ifi the number of failures 
in the time interval upper I Subscript iIi left parenthesis i equals 1 comma midline horizontal ellipsis comma k plus 1 right parenthesis(i = 1, · · · , k + 1). In this case the formula is modiﬁed to be 
StartLayout 1st Row 1st Column ModifyingAbove upper R With caret Subscript n Baseline left parenthesis t right parenthesis 2nd Column equals upper I left brace t less than t 1 right brace 2nd Row 1st Column Blank 2nd Column plus sigma summation Underscript i equals 2 Overscript k plus 1 Endscripts upper I left brace t Subscript i Baseline less than or equals t less than t Subscript i plus 1 Baseline right brace product Underscript j equals 1 Overscript i minus 1 Endscripts left parenthesis 1 minus StartStartFraction f Subscript j Baseline OverOver n Subscript j minus 1 Baseline minus StartFraction w Subscript j Baseline Over 2 EndFraction EndEndFraction right parenthesis period EndLayout
ˆRn(t) = I{t < t1}
+
k+1

i=2
I{ti ≤t < ti+1}
i−1
	
j=1

1 −
fj
nj−1−
wj
2

.
(9.6.4) 
This version of the estimator of upper R left parenthesis t right parenthesisR(t), when the inspection times are ﬁxed (not 
random failure times), is called the actuarial estimator. 
In the following examples we illustrate these estimators of the reliability 
function. 
Example 9.12 A machine is tested before shipping to the customer for a one week 
period (120 [hr]) or till failure, whichever comes ﬁrst. Twenty such machines were 
tested consecutively. In Table 9.1, we present the ordered time till failure or time till 
censor (TTF/TTC) of the 20 machines, the factors left parenthesis 1 minus delta Subscript i Baseline divided by left parenthesis n minus i plus 1 right parenthesis right parenthesis(1 −δi/(n −i + 1)), and the PL 
estimator ModifyingAbove upper R With caret left parenthesis t Subscript i Baseline right parenthesis ˆR(ti), i equals 1 comma midline horizontal ellipsis comma 20i = 1, · · · , 20.
black medium square■
9.7 
Estimation of Life Characteristics 
In Chapter 3 of Modern Statistics (Kenett et al. 2022b), we studied the estimation of 
parameters of distributions and of functions of these parameters. We discussed point 
estimators and conﬁdence intervals. In particular, we discussed unbiased estimators, 
least squares estimators, maximum likelihood estimators, and Bayes estimators. All 
these methods of estimation can be applied in reliability studies. We discuss here 
maximum likelihood estimation of the parameters of common life distributions, like 
the exponential and the Weibull, and some nonparametric techniques, for censored 
and uncensored data. 
9.7.1 
Maximum Likelihood Estimators for Exponential TTF 
Distribution 
We start with the case of uncensored observations. Thus, let upper T 1 comma upper T 2 comma midline horizontal ellipsis comma upper T Subscript n BaselineT1, T2, · · · , Tn
be i.i.d. random variables distributed with an exponential distribution, upper E left parenthesis beta right parenthesisE(β). Let

344
9
Reliability Analysis
Table 9.1 Failure times [hr] 
and PL estimates 
i
upper T Subscript left parenthesis i right parenthesisT(i)
left parenthesis 1 minus StartFraction delta Subscript i Baseline Over n minus i plus 1 EndFraction right parenthesis

1 −
δi
n−i+1

ModifyingAbove upper R With caret left parenthesis upper T Subscript i Baseline right parenthesisˆR(Ti)
1 
4.787715 
0.95
0.95 
2 
8.378821 
0.9473684
0.9 
3 
8.763973 
0.9444444
0.85 
4 
13.77360 
0.9411765
0.8 
5 
29.20548 
0.9375
0.75 
6 
30.53487 
0.9333333
0.7 
7 
47.96504 
0.9285714
0.65 
8 
59.22675 
0.9230769
0.6 
9 
60.66661 
0.9166667
0.55 
10 
62.12246 
0.9090909
0.5 
11 
67.06873 
0.9
0.45 
12 
92.15673 
0.8888889
0.4 
13 
98.09076 
0.875
0.35 
14 
107.6014 
0.8571429
0.3 
15 
120
1
0.3 
16 
120
1
0.3 
17 
120
1
0.3 
18 
120
1
0.3 
19 
120
1
0.3 
20 
120
1
0.3 
t 1 comma midline horizontal ellipsis comma t Subscript n Baselinet1, · · · , tn be their sample realization (random sample). The likelihood function of 
betaβ, 0 less than beta less than normal infinity0 < β < ∞, is  
upper L left parenthesis beta semicolon bold t right parenthesis equals StartFraction 1 Over beta Superscript n Baseline EndFraction exp left brace minus StartFraction 1 Over beta EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i Baseline right brace periodL(β; t) = 1
βn exp

−1
β
n

i=1
ti

.
(9.7.1) 
It is easy to check that the maximum likelihood estimator (MLE) of betaβ is the sample 
mean 
ModifyingAbove beta With caret Subscript n Baseline equals upper T overbar Subscript n Baseline equals StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts upper T Subscript i Baseline period ˆβn = ¯Tn = 1
n
n

i=1
Ti.
(9.7.2) 
upper T overbar Subscript n¯Tn is distributed like upper G left parenthesis n comma StartFraction beta Over n EndFraction right parenthesisG

n, β
n

. Thus, upper E left brace ModifyingAbove beta With caret Subscript n Baseline right brace equals betaE{ ˆβn} = β and upper V left brace ModifyingAbove beta With caret Subscript n Baseline right brace equals StartFraction beta squared Over n EndFractionV { ˆβn} =
β2
n . From the  
relationship between the Gamma and the chi squaredχ2 distributions, we have that ModifyingAbove beta With caret Subscript n Baseline tilde StartFraction beta Over 2 n EndFraction chi squared left bracket 2 n right bracketˆβn ∼
β
2nχ2[2n]. Thus, a left parenthesis 1 minus alpha right parenthesis(1−α) level conﬁdence interval for betaβ, based on the MLE ModifyingAbove beta With caret Subscript nˆβn, is  
left parenthesis StartFraction 2 n ModifyingAbove beta With caret Subscript n Baseline Over chi Subscript 1 minus alpha divided by 2 Superscript 2 Baseline left bracket 2 n right bracket EndFraction comma StartFraction 2 n ModifyingAbove beta With caret Subscript n Baseline Over chi Subscript alpha divided by 2 Superscript 2 Baseline left bracket 2 n right bracket EndFraction right parenthesis period

2n ˆβn
χ2
1−α/2[2n],
2n ˆβn
χ2
α/2[2n]

.
(9.7.3)

9.7
Estimation of Life Characteristics
345
Fig. 9.6 Q–Q plot for failure times of electric generators 
For large samples, we can use the normal approximation ModifyingAbove beta With caret Subscript n Baseline plus or minus z Subscript 1 minus alpha divided by 2 Baseline StartFraction ModifyingAbove beta With caret Subscript n Baseline Over StartRoot n EndRoot EndFraction ˆβn ± z1−α/2
ˆβn
√n. 
Example 9.13 The failure times of 20 electric generators (in [hr]) are 
121.5 
1425.5
2951.2 
5637.9 
1657.2
592.1 
10609.7 
9068.5 
848.2 
5296.6
7.5 
2311.1 
279.8 
7201.9
6853.7 
6054.3 
1883.6 
6303.9
1051.7
711.5 
failtime = mistat.load_data('FAILTIME.csv') 
fig, ax = plt.subplots(figsize=(4, 4)) 
res = stats.probplot(failtime, dist=stats.expon, plot=ax, rvalue=True) 
df = pd.DataFrame({'Ei': res[0][0], 'x': res[0][1]}) 
model = smf.ols('x ~ Ei - 1', data=df).fit() 
print(f'regression parameter beta={model.params[0]:.3f}') 
regression parameter beta=3644.933 
Exponential probability plotting (see Fig. 9.6) of these data yields a scatter 
around the line with a slope of 3644.93 and upper R squared equals 0.94R2 = 0.94. The exponential model 
ﬁts the failure times quite well. 

346
9 
Reliability Analysis 
We can also ﬁt an exponential model to the data using the lifelines package. 
epf = lifelines.ExponentialFitter().fit(failtime) 
epf.summary 
beta = epf.lambda_ 
Coef 
SE(coef) 
Coef lower 95% 
Coef upper 95% 
lambda_ 
3543.4
792.3
1990.4
5096.3 
The ExponentialFitter estimates lamda equals e Superscript betaλ = eβ. It calculates the conﬁdence inter-
val using the normal approximation. To calculate the conﬁdence interval using 
Eq. (9.7.3), use the following Python code: 
beta = epf.lambda_ 
n = len(failtime) 
ci_lower = 2*n*beta / stats.chi2.ppf(1-0.05/2, 2 * n) 
ci_upper = 2*n*beta / stats.chi2.ppf(0.05/2, 2 * n) 
ci_lower_approx = epf.summary.loc['lambda_', 'coef lower 95%'] 
ci_upper_approx = epf.summary.loc['lambda_', 'coef upper 95%'] 
The maximum likelihood estimator (MLE) of the mean time to failure (MTTF), 
betaβ, yields ModifyingAbove beta With caret Subscript 20 Baseline equals 3543.4 ˆβ20 = 3543.4 [hr]. Notice that the MLE is different, but not signiﬁcantly 
from the above graphical estimate of betaβ. Indeed, the standard error of ModifyingAbove beta With caret Subscript 20ˆβ20 is upper S period upper E period equals ModifyingAbove beta With caret Subscript 20 Baseline divided by StartRoot 20 EndRoot equals 792.328S.E. = 
ˆβ20/
√
20 = 792.328. 
Conﬁdence interval (Eq. (9.7.3)), at level of 0.95, for betaβ is given by (2388.5, 
5800.9). The normal approximation to the conﬁdence interval is (1990.4, 5096.3). 
The sample size is not sufﬁciently large for the normal approximation to be 
effective.
black medium square■
When the observations are time censored by a ﬁxed constant t Superscript asteriskt∗, let  upper K Subscript nKn denote 
the number of uncensored observations. upper K Subscript nKn is a random variable having the 
binomial distribution upper B left parenthesis n comma 1 minus exp left brace minus t Superscript asterisk Baseline divided by beta right brace right parenthesisB(n, 1 − exp{−t∗/β}). Let  ModifyingAbove p With caret Subscript n Baseline equals StartFraction upper K Subscript n Baseline Over n EndFraction ˆpn = Kn 
n . ModifyingAbove p With caret Subscript nˆpn is a consistent 
estimator of 1 minus exp left brace minus t Superscript asterisk Baseline divided by beta right brace1 − exp{−t∗/β}. Hence, a consistent estimator of betaβ is 
beta overTilde Subscript n Baseline equals minus t Superscript asterisk Baseline divided by log left parenthesis 1 minus ModifyingAbove p With caret Subscript n Baseline right parenthesis period ˜βn = −t∗/ log(1 −ˆpn).
(9.7.4) 
This estimator is not efﬁcient, since it is not based on observed failures. Moreover, 
if upper K Subscript n Baseline equals 0Kn = 0, beta overTilde Subscript n Baseline equals 0 ˜βn = 0. Using the expansion method shown in Sect. 9.3, we obtain the 
asymptotic variance of beta overTilde Subscript n˜βn: 
upper A upper V left brace beta overTilde Subscript n Baseline right brace approximately equals StartFraction beta Superscript 4 Baseline Over n t Superscript asterisk 2 Baseline EndFraction dot StartFraction 1 minus e Superscript minus t Super Superscript asterisk Superscript divided by beta Baseline Over e Superscript minus t Super Superscript asterisk Superscript divided by beta Baseline EndFraction periodAV { ˜βn} ∼= β4 
nt∗2 · 1 − e−t∗/β 
e−t∗/β 
.
(9.7.5) 

9.7 
Estimation of Life Characteristics
347 
The likelihood function of betaβ in this time censoring case is 
upper L left parenthesis beta semicolon upper K Subscript n Baseline comma bold upper T Subscript n Baseline right parenthesis equals StartFraction 1 Over beta Superscript upper K Super Subscript n Superscript Baseline EndFraction exp left brace minus StartFraction 1 Over beta EndFraction left parenthesis sigma summation Underscript i equals 1 Overscript upper K Subscript n Baseline Endscripts upper T Subscript i Baseline plus t Superscript asterisk Baseline left parenthesis n minus upper K Subscript n Baseline right parenthesis right parenthesis right brace periodL(β; Kn, Tn) = 1 
βKn exp

−1 
β
 Kn

i=1 
Ti + t∗(n − Kn)

.
(9.7.6) 
Also here, if upper K Subscript n Baseline equals 0Kn = 0, the  MLE of betaβ does not exist. If upper K Subscript n Baseline greater than or equals 1Kn ≥ 1, the  MLE is  
ModifyingAbove beta With caret Subscript n Baseline equals StartFraction upper S Subscript n comma upper K Sub Subscript n Subscript Baseline Over upper K Subscript n Baseline EndFraction comma ˆβn = Sn,Kn 
Kn 
,
(9.7.7) 
where upper S Subscript n comma upper K Sub Subscript n Subscript Baseline equals sigma summation Underscript i equals 1 Overscript upper K Subscript n Baseline Endscripts upper T Subscript i Baseline plus left parenthesis n minus upper K Subscript n Baseline right parenthesis t Superscript asteriskSn,Kn = Kn 
i=1 Ti + (n − Kn)t∗ is the total time on test of the n units. 
The theoretical evaluation of the properties of the MLE ModifyingAbove beta With caret Subscript nˆβn is complicated. We 
can, however, get information on its behavior by simulation. 
Example 9.14 For a sample of size n equals 50n = 50, with beta equals 1000β = 1000 and t Superscript asterisk Baseline equals 2000t∗ = 2000, we ﬁrst 
estimate beta overTilde Subscript n˜βn using the previous formula. 
# prepare sample 
np.random.seed(1) 
n = 50  
t_star = 2000 
ft = stats.expon.rvs(scale=1000, size=n) 
ft[ft>t_star] = t_star 
# calculation of MLE 
Kn = sum(ft < t_star) 
S_nKn = sum(ft[ft<t_star]) + (n - Kn) * t_star 
beta_n = S_nKn / Kn 
print(f' beta_n: {beta_n:.1f}') 
print(f' sd(beta_n): {beta_n / np.sqrt(n):.1f}') 
beta_n: 970.1 
sd(beta_n): 137.2 
The standard deviation of beta overTilde Subscript n˜βn with the above values of n, betaβ, and t Superscript asteriskt∗ is 137.1. 
We can also use bootstrap to estimate betaβ. As Prleft brace upper K 50 equals 0 right brace equals exp left brace negative 100 right brace approaches the limit 0{K50 = 0} =  exp{−100} .= 0, we  
expect that ModifyingAbove beta With caret Subscript nˆβn will exist in all the simulation runs. 
ev = ft < t_star 
idx = list(range(len(ft))) 
def stat_func(x): 
epf = lifelines.ExponentialFitter().fit(ft[x], ev[x]) 
return epf.params_['lambda_'] 
ci, dist = pg.compute_bootci(idx, func=stat_func, n_boot=100, confidence=0.95, 
method='per', seed=1, return_dist=True) 
print(f' Mean: {np.mean(dist):.1f}') 
print(f' Median: {np.median(dist):.1f}') 
print(f' Std: {np.std(dist):.1f}') 
Mean: 985.2 
Median: 979.5 
Std: 180.9 
For 100 MLE’s of betaβ, obtained by this simulation, we have a mean equals 985.2= 985.2, a  
median equals 979.5= 979.5, and a standard deviation equals 180.9= 180.9.
black medium square■

348
9 
Reliability Analysis 
Example 9.15 As an extension of Exercise 9.13, assume now that data were 
censored at 7000 [h]. 
# censor data 
at 7000 and derive event vector 
event = failtime < 7000 
failtime[failtime > 7000] = 7000 
We repeat the bootstrap analysis with the censored dataset. 
idx = list(range(len(failtime))) 
def stat_func(x): 
epf = lifelines.ExponentialFitter().fit(failtime[x], event[x]) 
return epf.params_['lambda_'] 
ci, dist = pg.compute_bootci(idx, func=stat_func, n_boot=100, confidence=0.95, 
method='per', seed=1, return_dist=True) 
print(f' Mean: {np.mean(dist):.1f}') 
print(f' 95%-CI: {ci[0]:.1f} - {ci[1]:.1f}') 
Mean: 3801.4 
95%-CI: 2398.8 - 5690.6 
The actual value of ModifyingAbove beta With caret Subscript 20ˆβ20, which is estimated from the uncensored data, is 
3543.4. This is within the 95% conﬁdence interval and close to the mean bootstrap 
value
black medium square■
Under failure censoring, the situation is simpler. Suppose that the censoring is 
at the rth failure. The total time on test is upper S Subscript n comma r Baseline equals sigma summation Underscript i equals 1 Overscript r Endscripts upper T Subscript left parenthesis i right parenthesis Baseline plus left parenthesis n minus r right parenthesis upper T Subscript left parenthesis r right parenthesisSn,r = r 
i=1 T(i) + (n − r)T(r). In this  
case, 
upper S Subscript n comma r Baseline tilde StartFraction beta Over 2 EndFraction chi squared left bracket 2 r right bracketSn,r ∼ β 
2 χ2[2r]
(9.7.8) 
and the MLE ModifyingAbove beta With caret Subscript n comma r Baseline equals StartFraction upper S Subscript n comma r Baseline Over r EndFraction ˆβn,r = Sn,r 
r is an unbiased estimator of betaβ, with variance 
upper V left brace ModifyingAbove beta With caret Subscript n comma r Baseline right brace equals StartFraction beta squared Over r EndFraction commaV { ˆβn,r} =  β2 
r , 
or 
upper S period upper E period StartSet ModifyingAbove beta With caret Subscript n comma r Baseline EndSet equals StartFraction beta Over StartRoot r EndRoot EndFraction periodS.E.{ ˆβn,r} =  β
√r . 
If we wish to have a certain precision, so that S.E.StartSet ModifyingAbove beta With caret Subscript n comma r Baseline EndSet equals gamma beta{ ˆβn,r} =  γβ, then r equals StartFraction 1 Over gamma squared EndFractionr = 
1 
γ 2 . 
Obviously n greater than or equals rn ≥ r. 
Suppose that we pay for the test c 2c2 $ per unit and c 1c1 $ per time unit, for the 
duration of the test. Then, the total cost of the test is 
upper T upper K Subscript n comma r Baseline equals c 1 upper T Subscript n comma r Baseline plus c 2 n periodT Kn,r = c1Tn,r + c2n.
(9.7.9) 

9.7 
Estimation of Life Characteristics
349 
For a given r, we choose n to minimize the expected total cost. The resulting formula 
is 
n Superscript 0 Baseline approaches the limit StartFraction r Over 2 EndFraction left parenthesis 1 plus left parenthesis 1 plus StartFraction 4 c 1 Over r c 2 EndFraction beta right parenthesis Superscript 1 divided by 2 Baseline right parenthesis periodn0 .= r 
2

1 +

1 + 4c1 
rc2 
β
1/2
.
(9.7.10) 
The problem is that the optimal sample size n Superscript 0n0 depends on the unknown betaβ. If one 
has some prior estimate of betaβ, it could be used to determine a good starting value for 
n. 
Example 9.16 Consider a design of a life testing experiment with frequency 
censoring and exponential distribution of the TTF. We require that S.E.StartSet ModifyingAbove beta With caret Subscript n comma r Baseline EndSet equals 0.2 beta{ ˆβn,r} =  
0.2β. Accordingly, r equals left parenthesis StartFraction 1 Over 0.2 EndFraction right parenthesis squared equals 25r =

1 
0.2
2 
= 25. Suppose that we wish to minimize the total 
expected cost, at beta equals 100β = 100 [hr], where c 1 equals c 2 equals 2c1 = c2 = 2 $. Then, 
n Superscript 0 Baseline approaches the limit StartFraction 25 Over 2 EndFraction left parenthesis 1 plus left parenthesis 1 plus four twenty fifths 100 right parenthesis Superscript 1 divided by 2 Baseline right parenthesis equals 64 periodn0 .= 25 
2

1 +

1 + 4 
25 100
1/2
= 64. 
The expected duration of this test is 
upper E left brace upper T Subscript 64 comma 25 Baseline right brace equals 100 sigma summation Underscript i equals 1 Overscript 25 Endscripts StartFraction 1 Over 65 minus i EndFraction equals 49.0 left bracket hr right bracket periodE{T64,25} =  100 
25

i=1 
1 
65 − i = 49.0 [hr]. 
black medium square■
9.7.2 
Maximum Likelihood Estimation of the Weibull 
Parameters 
Let t 1 comma midline horizontal ellipsis comma t Subscript n Baselinet1, · · ·  , tn be uncensored failure times of n random variables having a Weibull 
distribution upper W left parenthesis nu comma beta right parenthesisW(ν, β). The likelihood function of left parenthesis nu comma beta right parenthesis(ν, β) is 
upper L left parenthesis nu comma beta semicolon bold t right parenthesis equals StartFraction nu Superscript n Baseline Over beta Superscript n nu Baseline EndFraction left parenthesis product Underscript i equals 1 Overscript n Endscripts t Subscript i Baseline right parenthesis Superscript nu minus 1 Baseline exp left brace minus sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis StartFraction t Subscript i Baseline Over beta EndFraction right parenthesis Superscript nu Baseline right brace commaL(ν, β; t) = νn 
βnν
 n

i=1 
ti
ν−1 
exp

− 
n

i=1
ti 
β
ν
,
(9.7.11) 
0 less than beta comma nu less than normal infinity0 < β,  ν  <  ∞. The  MLE of nuν and betaβ are the solutions ModifyingAbove beta With caret Subscript nˆβn and ModifyingAbove nu With caret Subscript nˆνn of the equations 
ModifyingAbove beta With caret Subscript n Baseline equals left parenthesis StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i Superscript ModifyingAbove nu With caret Super Subscript n Superscript Baseline right parenthesis Superscript 1 divided by ModifyingAbove nu With caret Super Subscript n Superscript Baseline comma ˆβn =

1 
n 
n

i=1 
t ˆνn 
i
1/ˆνn 
,
(9.7.12) 

350
9 
Reliability Analysis 
and 
ModifyingAbove nu With caret Subscript n Baseline equals left bracket StartFraction sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i Superscript ModifyingAbove nu With caret Super Subscript n Superscript Baseline log t Subscript i Baseline Over sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i Superscript ModifyingAbove nu With caret Super Subscript n Superscript Baseline EndFraction minus StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts log left parenthesis t Subscript i Baseline right parenthesis right bracket Superscript negative 1 Baseline periodˆνn =
n 
i=1 t ˆνn 
i log ti
n 
i=1 t ˆνn 
i 
− 1 
n 
n

i=1 
log(ti)
−1 
.
(9.7.13) 
All logarithms are on base e (lnln). 
The equation for ModifyingAbove nu With caret Subscript nˆνn is solved iteratively by the recursive equation 
ModifyingAbove nu With caret Superscript left parenthesis j plus 1 right parenthesis Baseline equals left bracket StartFraction sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i Superscript ModifyingAbove nu With caret left parenthesis j right parenthesis Baseline log left parenthesis t Subscript i Baseline right parenthesis Over sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i Superscript ModifyingAbove nu With caret left parenthesis j right parenthesis Baseline EndFraction minus StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts log left parenthesis t Subscript i Baseline right parenthesis right bracket Superscript negative 1 Baseline comma j equals 0 comma 1 comma midline horizontal ellipsis commaˆν(j+1) =
n 
i=1 t ˆν(j) 
i 
log(ti)
n 
i=1 t ˆν(j) 
i 
− 1 
n 
n

i=1 
log(ti)
−1 
,
j
 
= 0, 1, · · ·  ,
(9.7.14) 
where ModifyingAbove nu With caret Superscript left parenthesis 0 right parenthesis Baseline equals 1ˆν(0) = 1. 
To illustrate, we simulated a sample of n equals 50n = 50 failure times from upper W left parenthesis 2.5 comma 10 right parenthesisW(2.5, 10). 
In order to obtain the MLE, we have to continue the iterative process until the 
results converge. We show here the obtained values, as functions of the number 
of iterations. 
StartEnclose left right StartLayout 1st Row 1st Column number sign iter 2nd Column ModifyingAbove beta With caret 3rd Column ModifyingAbove nu With caret 2nd Row 1st Column 10 2nd Column 11.437 3rd Column 2.314 3rd Row 1st Column 20 2nd Column 9.959 3rd Column 2.367 4th Row 1st Column 30 2nd Column 9.926 3rd Column 2.368 5th Row 1st Column 40 2nd Column 9.925 3rd Column 2.368 EndLayout EndEnclose
# iter 
ˆβ
ˆν 
10
11.437
2.314 
20
9.959
2.367 
30
9.926
2.368 
40
9.925
2.368 
It seems that 40 iterations yield sufﬁciently accurate solutions. 
Conﬁdence intervals for ModifyingAbove nu With caret Subscript nˆνn and ModifyingAbove beta With caret Subscript nˆβn can be determined, for large samples, by using 
large sample approximation formulae for the standard errors of the MLE, which are 
upper S upper E left brace ModifyingAbove beta With caret Subscript n Baseline right brace approximately equals StartFraction ModifyingAbove beta With caret Subscript n Baseline Over StartRoot n EndRoot ModifyingAbove nu With caret Subscript n Baseline EndFraction dot 1.053SE{ ˆβn} ∼= 
ˆβn
√n ˆνn 
· 1.053
(9.7.15) 
and 
upper S upper E left brace ModifyingAbove nu With caret Subscript n Baseline right brace approximately equals 0.780 StartFraction ModifyingAbove nu With caret Subscript n Baseline Over StartRoot n EndRoot EndFraction periodSE{ˆνn} ∼= 0.780 ˆνn
√n .
(9.7.16) 
The large sample conﬁdence limits are 
ModifyingAbove beta With caret Subscript n Baseline plus or minus z Subscript 1 minus alpha divided by 2 Baseline upper S period upper E period StartSet ModifyingAbove beta With caret Subscript n Baseline EndSet comma ˆβn ± z1−α/2S.E.{ ˆβn},
(9.7.17) 
and 
ModifyingAbove nu With caret Subscript n Baseline plus or minus z Subscript 1 minus alpha divided by 2 Baseline upper S period upper E period StartSet ModifyingAbove nu With caret Subscript n Baseline EndSet periodˆνn ± z1−α/2S.E.{ˆνn}.
(9.7.18) 

9.8 
Reliability Demonstration
351 
In the above numerical example, the MLE ModifyingAbove beta With caret Subscript 50 Baseline equals 9.925 ˆβ50 = 9.925 and ModifyingAbove nu With caret Subscript 50 Baseline equals 2.368ˆν50 = 2.368. Using  
these values, we obtain the large sample approximate conﬁdence intervals, with 
level of conﬁdence 1 minus alpha equals 0.951−α = 0.95, to be left parenthesis 8.898 comma 11.148 right parenthesis(8.898, 11.148) for betaβ and left parenthesis 1.880 comma 2.856 right parenthesis(1.880, 2.856) for 
nuν. 
We can derive bootstrapping conﬁdence intervals. The bootstrap conﬁdence 
limits are the alpha divided by 2α/2th and left parenthesis 1 minus alpha divided by 2 right parenthesis(1−α/2)th quantile of the simulated values which produced 
conﬁdence intervals left parenthesis 8.378 comma 11.201 right parenthesis(8.378, 11.201) for betaβ and left parenthesis 1.914 comma 3.046 right parenthesis(1.914, 3.046) for nuν. The difference 
between these conﬁdence intervals and the large sample approximation ones is not 
signiﬁcant. 
Maximum likelihood estimation in censored cases is more complicated and will 
not be discussed here. Estimates of nuν and betaβ in the censored case can be obtained 
from the intercept and slope of the regression line in the Q–Q plot. 
Example 9.17 Using the censored data from Exercise 9.16, we estimate the lamda equals e Superscript betaλ = eβ 
parameter of a ﬁt using the Weibull distribution. 
def stat_func(x): 
epf = lifelines.WeibullFitter().fit(failtime[x], event[x]) 
return epf.params_['lambda_'] 
ci, dist = pg.compute_bootci(idx, func=stat_func, n_boot=100, confidence=0.95, 
method='per', seed=1, return_dist=True) 
print(f' Mean: {np.mean(dist)}') 
print(f' 95%-CI: {ci[0]:.1f} - {ci[1]:.1f}') 
Mean: 3695.8021438582273 
95%-CI: 2188.6 - 5794.3 
black medium square■
9.8 
Reliability Demonstration 
Reliability demonstration is a procedure for testing whether the reliability of a given 
device (system) at a certain age is sufﬁciently high. More precisely, a time point t 0t0 
and a desired reliability upper R 0R0 are speciﬁed, and we wish to test whether the reliability 
of the device at age t 0t0, upper R left parenthesis t 0 right parenthesisR(t0), satisﬁes the requirement that upper R left parenthesis t 0 right parenthesis greater than or equals upper R 0R(t0) ≥ R0. If the life 
distribution of the device is completely known, including all parameters, there is no 
problem of reliability demonstration—one computes upper R left parenthesis t 0 right parenthesisR(t0) exactly and determines 
whether upper R left parenthesis t 0 right parenthesis greater than or equals upper R 0R(t0) ≥ R0. If, as is generally the case, either the life distribution or its 
parameters are unknown, then the problem of reliability demonstration is that of 
obtaining suitable data and using them to test the statistical hypothesis that upper R left parenthesis t 0 right parenthesis greater than or equals upper R 0R(t0) ≥ 
R0 versus the alternative that upper R left parenthesis t 0 right parenthesis less than upper R 0R(t0) < R0. Thus, the theory of testing statistical 
hypotheses provides the tools for reliability demonstration. In the present section we 
review some of the basic notions of hypothesis testing as they pertain to reliability 
demonstration. 
In the following subsections we develop several tests of interest in reliability 
demonstration. We remark here that procedures for obtaining conﬁdence intervals 
for upper R left parenthesis t 0 right parenthesisR(t0), which were discussed in the previous sections, can be used to test 

352
9 
Reliability Analysis 
hypotheses. Speciﬁcally, the procedure involves computing the upper conﬁdence 
limit of a left parenthesis 1 minus 2 alpha right parenthesis(1 − 2α)-level conﬁdence interval for upper R left parenthesis t 0 right parenthesisR(t0) and comparing it with 
the value upper R 0R0. If the upper conﬁdence limit exceeds upper R 0R0, then the null hypothesis 
upper H 0 colon upper R left parenthesis t 0 right parenthesis greater than upper R 0H0 : R(t0) > R0 is accepted, otherwise it is rejected. This test will have a 
signiﬁcance level of alphaα. 
For example, if the speciﬁcation of the reliability at age t equals t 0t = t0 is upper R equals 0.75R = 0.75 and 
the conﬁdence interval for upper R left parenthesis t 0 right parenthesisR(t0), at level of conﬁdence gamma equals 0.90γ = 0.90, is  left parenthesis 0.80 comma 0.85 right parenthesis(0.80, 0.85), 
the hypothesis upper H 0H0 can be immediately accepted at a level of signiﬁcance of alpha equals left parenthesis 1 minus gamma right parenthesis divided by 2 equals 0.05α = 
(1 −γ )/2 = 0.05. There is a duality between procedures for testing hypotheses and 
for conﬁdence intervals. 
9.8.1 
Binomial Testing 
A random sample of n devices is put on life test simultaneously. Let upper J Subscript nJn be the 
number of failures in the time interval left bracket 0 comma t 0 right parenthesis[0, t0), and upper K Subscript n Baseline equals n minus upper J Subscript nKn = n − Jn. We have seen that 
upper K Subscript n Baseline tilde upper B left parenthesis n comma upper R left parenthesis t 0 right parenthesis right parenthesisKn ∼ B(n, R(t0)). Thus, if upper H 0H0 is true, i.e., upper R left parenthesis t 0 right parenthesis greater than or equals upper R 0R(t0) ≥R0, the values of upper K Subscript nKn will tend to 
be larger, in a probabilistic sense. Thus, one tests upper H 0H0 by specifying a critical value 
upper C Subscript alphaCα and rejecting upper H 0H0 whenever upper K Subscript n Baseline less than or equals upper C Subscript alphaKn ≤ Cα. The critical value upper C Subscript alphaCα is chosen as the 
largest value satisfying 
upper F Subscript upper B Baseline left parenthesis upper C Subscript alpha Baseline semicolon n comma upper R 0 right parenthesis less than or equals alpha periodFB(Cα; n, R0) ≤α. 
The OC function of this test, as a function of the true reliability R, is  
StartLayout 1st Row 1st Column OC left parenthesis upper R right parenthesis 2nd Column equals probability StartSet upper K Subscript n Baseline greater than upper C Subscript alpha Baseline vertical bar upper R left parenthesis t 0 right parenthesis equals upper R EndSet 2nd Row 1st Column Blank 2nd Column equals 1 minus upper F Subscript upper B Baseline left parenthesis upper C Subscript alpha Baseline semicolon n comma upper R right parenthesis period EndLayoutOC(R) = Pr{Kn > Cα | R(t0) = R} 
= 1 − FB(Cα; n, R). 
(9.8.1) 
If n is large, then one can apply the normal approximation to the Binomial CDF. 
In these cases, we can determine upper C Subscript alphaCα to be the integer most closely satisfying 
upper Phi left parenthesis StartFraction upper C Subscript alpha Baseline plus 1 divided by 2 minus n upper R 0 Over left parenthesis n upper R 0 left parenthesis 1 minus upper R 0 right parenthesis right parenthesis Superscript 1 divided by 2 Baseline EndFraction right parenthesis equals alpha period
 Cα + 1/2 − nR0 
(nR0(1 − R0))1/2

= α.
(9.8.2) 
Generally, this will be given by 
upper C Subscript alpha Baseline equals integer closest to left brace n upper R 0 minus 1 divided by 2 minus z Subscript 1 minus alpha Baseline left parenthesis n upper R 0 left parenthesis 1 minus upper R 0 right parenthesis right parenthesis Superscript 1 divided by 2 Baseline right brace commaCα = integer closest to {nR0 − 1/2 − z1−α(nR0(1 − R0))1/2},
(9.8.3) 
where z Subscript 1 minus alpha Baseline equals upper Phi Superscript negative 1 Baseline left parenthesis 1 minus alpha right parenthesisz1−α = −1(1 − α). The OC function of this test in the large sample case is 
approximated by 
OC left parenthesis upper R right parenthesis approximately equals upper Phi left parenthesis StartFraction n upper R minus upper C Subscript alpha Baseline minus 1 divided by 2 Over left parenthesis n upper R left parenthesis 1 minus upper R right parenthesis right parenthesis Superscript 1 divided by 2 Baseline EndFraction right parenthesis periodOC(R) ∼= 
nR − Cα − 1/2 
(nR(1 −R))1/2

.
(9.8.4) 

9.8 
Reliability Demonstration
353 
The normal approximation is quite accurate whenever n greater than 9 divided by left parenthesis upper R left parenthesis 1 minus upper R right parenthesis right parenthesisn >  9/(R(1 − R)). 
If in addition to specifying alphaα we specify that the test has Type II error probability 
betaβ, when upper R left parenthesis t 0 right parenthesis equals upper R 1R(t0) = R1, then the normal approximation provides us with a formula for 
the necessary sample size: 
n approaches the limit StartFraction left parenthesis z Subscript 1 minus alpha Baseline sigma 0 plus z Subscript 1 minus beta Baseline sigma 1 right parenthesis squared Over left parenthesis upper R 1 minus upper R 0 right parenthesis squared EndFraction comman .= (z1−ασ0 + z1−βσ1)2 
(R1 − R0)2 
,
(9.8.5) 
where sigma Subscript i Superscript 2 Baseline equals upper R Subscript i Baseline left parenthesis 1 minus upper R Subscript i Baseline right parenthesisσ 2 
i = Ri(1 − Ri), i equals 0 comma 1i = 0, 1. 
Example 9.18 Suppose that we wish to test at signiﬁcance level alpha equals 0.05α = 0.05 the null 
hypothesis that the reliability at age 1000 [hr] of a particular system is at least 85%. 
If the reliability is 80% or less, we want to limit the probability of accepting the null 
hypothesis to beta equals 0.10β = 0.10. Our test is to be based on upper K Subscript nKn, the number of systems, out 
of a random sample of n, surviving at least 1000 h of operation. Setting upper R 0 equals 0.85R0 = 0.85 
and upper R 1 equals 0.80R1 = 0.80, we have  sigma 0 equals 0.357σ0 = 0.357, sigma 1 equals 0.4σ1 = 0.4, z 0.95 equals 1.645z0.95 = 1.645, z 0.90 equals 1.282z0.90 = 1.282. 
Substituting above, we obtain that the necessary sample size is n equals 483n = 483. The critical 
value is upper C 0.05 equals 397C0.05 = 397. 
We see that in binomial testing one may need very large samples to satisfy the 
speciﬁcations of the test. If in the above problem we reduce the sample size to 
n equals 100n = 100, then upper C 0.05 equals 79C0.05 = 79. However, now the probability of accepting the null 
hypothesis when upper R equals 0.80R = 0.80 is OCleft parenthesis 0.8 right parenthesis equals upper Phi left parenthesis 0.125 right parenthesis equals 0.55(0.8) = (0.125) = 0.55, which is considerably 
higher than the corresponding probability of 0.10 under n equals 483n = 483.
black medium square■
9.8.2 
Exponential Distributions 
Suppose that we know that the life distribution is exponential upper E left parenthesis beta right parenthesisE(β), but  betaβ is 
unknown. The hypotheses 
upper H 0 colon upper R left parenthesis t 0 right parenthesis greater than or equals upper R 0H0 : R(t0) ≥ R0 
versus 
upper H 1 colon upper R left parenthesis t 0 right parenthesis less than upper R 0H1 : R(t0) < R0 
can be rephrased in terms of the unknown parameter, betaβ, as  
upper H 0 colon beta greater than or equals beta 0H0 : β ≥ β0 
versus 
upper H 1 colon beta less than beta 0 commaH1 : β <  β0, 

354
9 
Reliability Analysis 
where beta 0 equals minus t 0 divided by ln upper R 0β0 = −t0/ ln R0. Let t 1 comma midline horizontal ellipsis comma t Subscript n Baselinet1, · · ·  , tn be the values of a (complete) random sample 
of size n. Let t overbar Subscript n Baseline equals StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i¯tn = 1 
n
n 
i=1 ti. The hypothesis upper H 0H0 is rejected if t overbar Subscript n Baseline less than upper C Subscript alpha¯tn < Cα, where 
upper C Subscript alpha Baseline equals StartFraction beta 0 Over 2 n EndFraction chi Subscript alpha Superscript 2 Baseline left bracket 2 n right bracket periodCα = β0 
2n χ2 
α[2n].
(9.8.6) 
The OC function of this test, as a function of betaβ, is  
StartLayout 1st Row 1st Column OC left parenthesis beta right parenthesis 2nd Column equals probability StartSet t overbar Subscript n Baseline greater than upper C Subscript alpha Baseline vertical bar beta EndSet 2nd Row 1st Column Blank 2nd Column equals probability left brace chi squared left bracket 2 n right bracket greater than StartFraction beta 0 Over beta EndFraction chi Subscript alpha Superscript 2 Baseline left bracket 2 n right bracket right brace period EndLayoutOC(β) = Pr{¯tn > Cα | β} 
= Pr{χ2[2n] > β0 
β χ2 
α[2n]}. 
(9.8.7) 
If we require that at beta equals beta 1β = β1 the OC function of the test will assume the value gammaγ , 
then the sample size n should satisfy 
StartFraction beta 0 Over beta 1 EndFraction chi Subscript alpha Superscript 2 Baseline left bracket 2 n right bracket greater than or equals chi Subscript 1 minus gamma Superscript 2 Baseline left bracket 2 n right bracket periodβ0 
β1 
χ2 
α[2n] ≥χ2 
1−γ [2n]. 
The quantiles of chi squared left bracket 2 n right bracketχ2[2n], for n greater than or equals 15n ≥ 15, can be approximated by the formula 
chi Subscript p Superscript 2 Baseline left bracket 2 n right bracket approximately equals one half left parenthesis StartRoot 4 n EndRoot plus z Subscript p Baseline right parenthesis squared periodχ2 
p[2n] ∼= 1 
2 (
√
4n + zp)2.
(9.8.8) 
Substituting this approximation and solving for n, we obtain the approximation 
n approximately equals one fourth StartFraction left parenthesis z Subscript 1 minus gamma Baseline plus z Subscript 1 minus alpha Baseline StartRoot zeta EndRoot right parenthesis squared Over left parenthesis StartRoot zeta EndRoot minus 1 right parenthesis squared EndFraction comman ∼= 1 
4 
(z1−γ + z1−α 
√ζ)2 
(√ζ − 1)2 
,
(9.8.9) 
where zeta equals beta 0 divided by beta 1ζ = β0/β1. 
Example 9.19 Suppose that in Example 9.18, we know that the system lifetimes are 
exponentially distributed. It is interesting to examine how many systems would have 
to be tested in order to achieve the same error probabilities as before, if our decision 
were now based on t overbar Subscript n¯tn. 
Since beta equals negative t divided by ln upper R left parenthesis t right parenthesisβ = −t/ ln R(t), the value of the parameter betaβ under upper R left parenthesis t 0 right parenthesis equals upper R left parenthesis 1000 right parenthesis equals 0.85R(t0) = R(1000) = 
0.85 is beta 0 equals negative 1000 divided by ln left parenthesis 0.85 right parenthesis equals 6153β0 = −1000/ ln(0.85) = 6153 [hr], while its value under upper R left parenthesis t 0 right parenthesis equals 0.80R(t0) = 0.80 is 
beta 1 equals negative 1000 divided by ln left parenthesis 0.80 right parenthesis equals 4481β1 = −1000/ ln(0.80) = 4481 [hr]. Substituting these values into (9.3.5), along 
with alpha equals 0.05α = 0.05 and gamma equals 0.10γ = 0.10 (gammaγ was denoted by betaβ in Example 9.18), we obtain the 
necessary sample size n approximately equals 87n ∼= 87. 
Thus we see that the additional knowledge that the lifetime distribution is 
exponential, along with the use of complete lifetime data on the sample, allows 
us to achieve a greater than ﬁvefold increase in efﬁciency in terms of the sample 
size necessary to achieve the desired error probabilities.
black medium square■
We remark that if the sample is censored at the rth failure, then all the formulae 
developed above apply after replacing n by r and t overbar Subscript n¯tn by ModifyingAbove beta With caret Subscript n comma r Baseline equals upper T Subscript n comma r Baseline divided by r ˆβn,r = Tn,r/r. 

9.8 
Reliability Demonstration
355 
Example 9.20 Suppose that the reliability at age t equals 250t = 250 [hr] should be at least 
upper R 0 equals 0.85R0 = 0.85. Let upper R 1 equals 0.75R1 = 0.75. The corresponding values of beta 0β0 and beta 1β1 are 1538 [hr] and 
869 [hr], respectively. Suppose that the sample is censored at the r equals 25r = 25th failure. 
Let ModifyingAbove beta With caret Subscript n comma r Baseline equals upper T Subscript n comma r Baseline divided by 25 ˆβn,r = Tn,r/25 be the MLE of betaβ. upper H 0H0 is rejected, with level of signiﬁcance 
alpha equals 0.05α = 0.05 if 
ModifyingAbove beta With caret Subscript n comma r Baseline less than or equals StartFraction 1538 Over 50 EndFraction chi 0.05 squared left bracket 50 right bracket equals 1069 left bracket hr right bracket periodˆβn,r ≤ 1538 
50 χ2 
0.05[50] =  1069 [hr]. 
The Type II error probability of this test, at beta equals 869β = 869, is  
StartLayout 1st Row 1st Column OC left parenthesis 869 right parenthesis 2nd Column equals probability left brace chi squared left bracket 50 right bracket greater than StartFraction 1538 Over 869 EndFraction chi 0.05 squared left bracket 50 right bracket right brace 2nd Row 1st Column Blank 2nd Column equals probability left brace chi squared left bracket 50 right bracket greater than 61.5 right brace 3rd Row 1st Column Blank 2nd Column approaches the limit 1 minus upper Phi left parenthesis StartFraction 61.5 minus 50 Over StartRoot 100 EndRoot EndFraction right parenthesis 4th Row 1st Column Blank 2nd Column equals 0.125 period 5th Row 1st Column q u a d EndLayout
OC(869) = Pr{χ2[50] > 1538 
869 χ2 
0.05[50]} 
= Pr{χ2[50] > 61.5} 
.= 1 −
61.5 − 50
√
100

= 0.125.
black medium square■
Sometimes in reliability demonstration an overriding concern is keeping the 
number of items tested to a minimum, subject to whatever accuracy requirements 
are imposed. This could be the case, for example, when testing very complex and 
expensive systems. In such cases, it may be worthwhile applying a sequential testing 
procedure, where items are tested one at a time in sequence until the procedure 
indicates that testing can stop and a decision be made. Such an approach would 
also be appropriate when testing prototypes of some new design, which are being 
produced one at a time at a relatively slow rate. 
In Chap. 3 we have introduced the Wald SPRT for testing hypotheses with 
binomial data. Here we reformulate this test for reliability testing. 
9.8.2.1 
The SPRT for Binomial Data 
Without any assumptions about the lifetime distribution of a device, we can test 
hypotheses concerning upper R left parenthesis t 0 right parenthesisR(t0) by simply observing whether or not a device survives 
to age t 0t0. Letting upper K Subscript nKn represent the number of devices among n randomly selected 
ones surviving to age t 0t0, we have  upper K Subscript n Baseline tilde upper B left parenthesis n comma upper R left parenthesis t 0 right parenthesis right parenthesisKn ∼ B(n, R(t0)). The likelihood ratio is given 
by 
lamda Subscript n Baseline equals left parenthesis StartFraction 1 minus upper R 1 Over 1 minus upper R 0 EndFraction right parenthesis Superscript n Baseline left parenthesis StartFraction upper R 1 left parenthesis 1 minus upper R 0 right parenthesis Over upper R 0 left parenthesis 1 minus upper R 1 right parenthesis EndFraction right parenthesis Superscript upper K Super Subscript n Superscript Baseline periodλn =
1 − R1 
1 − R0
n R1(1 − R0) 
R0(1 − R1)
Kn 
.
(9.8.10) 

356
9 
Reliability Analysis 
Thus, 
ln lamda Subscript n Baseline equals n ln left parenthesis StartFraction 1 minus upper R 1 Over 1 minus upper R 0 EndFraction right parenthesis minus upper K Subscript n Baseline ln left parenthesis StartFraction upper R 0 left parenthesis 1 minus upper R 1 right parenthesis Over upper R 1 left parenthesis 1 minus upper R 0 right parenthesis EndFraction right parenthesis periodln λn = n ln
1 − R1 
1 − R0

− Kn ln
R0(1 − R1) 
R1(1 − R0)

. 
It follows that the SPRT can be expressed in terms of upper K Subscript nKn as follows: 
StartLayout 1st Row 1st Column Blank 2nd Column Continue sampling if minus h 1 plus s n less than upper K Subscript n Baseline less than h 2 plus s n comma 2nd Row 1st Column Blank 2nd Column Accept upper H 0 if upper K Subscript n Baseline greater than or equals h 2 plus s n comma 3rd Row 1st Column Blank 2nd Column Reject upper H 0 if upper K Subscript n Baseline less than or equals minus h 1 plus s n comma EndLayout
Continue sampling if − h1 + sn < Kn < h2 + sn, 
Accept H0 if Kn ≥ h2 + sn, 
Reject H0 if Kn ≤−h1 + sn, 
where 
StartLayout Enlarged left brace 1st Row s equals ln left parenthesis StartFraction 1 minus upper R 1 Over 1 minus upper R 0 EndFraction right parenthesis divided by ln left parenthesis StartFraction upper R 0 left parenthesis 1 minus upper R 1 right parenthesis Over upper R 1 left parenthesis 1 minus upper R 0 right parenthesis EndFraction right parenthesis comma 2nd Row Blank 3rd Row h 1 equals ln left parenthesis StartFraction 1 minus gamma Over alpha EndFraction right parenthesis divided by ln left parenthesis StartFraction upper R 0 left parenthesis 1 minus upper R 1 right parenthesis Over upper R 1 left parenthesis 1 minus upper R 0 right parenthesis EndFraction right parenthesis comma 4th Row Blank 5th Row h 2 equals ln left parenthesis StartFraction 1 minus alpha Over gamma EndFraction right parenthesis divided by ln left parenthesis StartFraction upper R 0 left parenthesis 1 minus upper R 1 right parenthesis Over upper R 1 left parenthesis 1 minus upper R 0 right parenthesis EndFraction right parenthesis period EndLayout
⎧ 
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨ 
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ 
s = ln

1−R1 
1−R0
  
ln

R0(1−R1) 
R1(1−R0)

, 
h1 = ln

1−γ 
α
  
ln

R0(1−R1) 
R1(1−R0)

, 
h2 = ln

1−α 
γ
  
ln

R0(1−R1) 
R1(1−R0)

. 
(9.8.11) 
alphaα and gammaγ are the prescribed probabilities of Type I and Type II errors. Note that if 
we plot upper K Subscript nKn vs. n, the accept and reject boundaries are parallel straight lines with 
common slope s and intercepts h 2h2 and minus h 1−h1, respectively. 
The OC function of this test is expressible (approximately) in terms of an implicit 
parameter psiψ. Letting 
upper R Superscript left parenthesis psi right parenthesis Baseline equals StartLayout Enlarged left brace 1st Row 1st Column StartFraction 1 minus left parenthesis StartFraction 1 minus upper R 1 Over 1 minus upper R 0 EndFraction right parenthesis Superscript psi Baseline Over left parenthesis StartFraction upper R 1 Over upper R 0 EndFraction right parenthesis Superscript psi Baseline minus left parenthesis StartFraction 1 minus upper R 1 Over 1 minus upper R 0 EndFraction right parenthesis Superscript psi Baseline EndFraction comma 2nd Column psi not equals 0 2nd Row 1st Column Blank 3rd Row 1st Column s comma 2nd Column psi equals 0 comma EndLayoutR(ψ) = 
⎧ 
⎪⎪⎪⎪⎨ 
⎪⎪⎪⎪⎩ 
1−
 1−R1 
1−R0
ψ
 R1 
R0
ψ 
−
 1−R1 
1−R0
ψ ,
ψ ̸= 0 
s,
ψ = 0, 
(9.8.12) 
we have that the OC function at upper R left parenthesis t 0 right parenthesis equals upper R Superscript left parenthesis psi right parenthesisR(t0) = R(ψ) is given by 
OC left parenthesis upper R Superscript left parenthesis psi right parenthesis Baseline right parenthesis almost equals StartLayout Enlarged left brace 1st Row 1st Column StartFraction left parenthesis StartFraction 1 minus gamma Over alpha EndFraction right parenthesis Superscript psi Baseline minus 1 Over left parenthesis StartFraction 1 minus gamma Over alpha EndFraction right parenthesis Superscript psi Baseline minus left parenthesis StartFraction gamma Over 1 minus alpha EndFraction right parenthesis Superscript psi Baseline EndFraction comma 2nd Column psi not equals 0 2nd Row 1st Column Blank 3rd Row 1st Column StartStartFraction ln left parenthesis StartFraction 1 minus gamma Over alpha EndFraction right parenthesis OverOver ln left parenthesis StartFraction left parenthesis 1 minus alpha right parenthesis left parenthesis 1 minus gamma right parenthesis Over alpha gamma EndFraction right parenthesis EndEndFraction comma 2nd Column psi equals 0 period EndLayoutOC(R(ψ) ) ≈ 
⎧ 
⎪⎪⎪⎪⎪⎪⎨ 
⎪⎪⎪⎪⎪⎪⎩
 1−γ 
α
ψ 
−1
 1−γ 
α
ψ 
−

γ 
1−α
ψ ,
ψ ̸= 0 
ln
 1−γ 
α

ln
 (1−α)(1−γ )  
αγ
,
ψ
 
= 0. 
(9.8.13) 

9.8 
Reliability Demonstration
357 
It is easily veriﬁed that for psi equals 1ψ = 1, upper R Superscript left parenthesis psi right parenthesisR(ψ) equals upper R 0R0 and OCleft parenthesis upper R Superscript left parenthesis psi right parenthesis Baseline right parenthesis(R(ψ) ) equals 1 minus alpha1 − α, 
while for psi equals negative 1ψ = −1, upper R Superscript left parenthesis psi right parenthesisR(ψ) equals upper R 1R1 and OCleft parenthesis upper R Superscript left parenthesis psi right parenthesis Baseline right parenthesis(R(ψ) ) equals gammaγ . 
The expected sample size, or average sample number (ASN), as a function of 
upper R Superscript left parenthesis psi right parenthesisR(ψ), is given by 
ASN left parenthesis upper R Superscript left parenthesis psi right parenthesis Baseline right parenthesis almost equals StartLayout Enlarged left brace 1st Row 1st Column StartStartFraction ln StartFraction 1 minus gamma Over alpha EndFraction minus OC left parenthesis upper R Superscript left parenthesis psi right parenthesis Baseline right parenthesis ln left parenthesis StartFraction left parenthesis 1 minus alpha right parenthesis left parenthesis 1 minus gamma right parenthesis Over alpha gamma EndFraction right parenthesis OverOver ln StartFraction 1 minus upper R 1 Over 1 minus upper R 0 EndFraction minus upper R Superscript left parenthesis psi right parenthesis Baseline ln left parenthesis StartFraction upper R 0 left parenthesis 1 minus upper R 1 right parenthesis Over upper R 1 left parenthesis 1 minus upper R 0 right parenthesis EndFraction right parenthesis EndEndFraction comma 2nd Column psi not equals 0 2nd Row 1st Column Blank 3rd Row 1st Column StartFraction h 1 h 2 Over s left parenthesis 1 minus s right parenthesis EndFraction comma 2nd Column psi equals 0 period EndLayoutASN(R(ψ) ) ≈ 
⎧ 
⎪⎪⎪⎪⎨ 
⎪⎪⎪⎪⎩ 
ln 1−γ 
α −OC(R(ψ) ) ln
 (1−α)(1−γ )  
αγ

ln 1−R1 
1−R0 −R(ψ) ln
 R0(1−R1) 
R1(1−R0)

,
ψ ̸= 0 
h1h2 
s(1−s) ,
ψ
 
= 0. 
(9.8.14) 
The ASN function will typically have a maximum at some value of R between 
upper R 0R0 and upper R 1R1 and decrease as R moves away from the point of maximum in either 
direction. 
Example 9.21 Consider Example 9.19, where we had t equals 1000t = 1000 [hr], upper R 0 equals 0.85R0 = 0.85, 
upper R 1 equals 0.80R1 = 0.80, alpha equals 0.05α = 0.05, gamma equals 0.10γ = 0.10. Suppose now that systems are tested sequentially, 
and we apply the SPRT based on the number of systems still functioning at 1000 
[hr]. The parameters of the boundary lines are s equals 0.826s = 0.826, h 1 equals 8.30h1 = 8.30, and h 2 equals 6.46h2 = 6.46. 
The OC and ASN functions of the test are given in Table 9.2, for selected values 
of psiψ. 
Compare the values in the ASN column to the sample size required for the 
corresponding ﬁxed-sample test, n equals 483n = 483. It is clear that the SPRT effects a 
considerable saving in sample size, particularly when upper R left parenthesis t 0 right parenthesisR(t0) is less than upper R 1R1 or greater 
than upper R 0R0. Note also that the maximum ASN value occurs when upper R left parenthesis t 0 right parenthesisR(t0) is near s. 
black medium square■
9.8.2.2 
The SPRT for Exponential Lifetimes 
When the lifetime distribution is known to be exponential, we have seen the increase 
in efﬁciency gained by measuring the actual failure times of the parts being tested. 
By using a sequential procedure based on these failure times, further gains in 
efﬁciency can be achieved. 
Expressing the hypotheses in terms of the parameter betaβ of the lifetime distribution 
upper E left parenthesis beta right parenthesisE(β), we wish to test  upper H 0 colon beta greater than or equals beta 0H0 : β ≥ β0 vs. upper H 1 colon beta less than beta 0H1 : β <  β0, with signiﬁcance level alphaα and 
Type II error probability gammaγ , when beta equals beta 1β = β1, where beta 1 less than beta 0β1 < β0. Letting bold t Subscript n Baseline equals left parenthesis t 1 comma midline horizontal ellipsis comma t Subscript n Baseline right parenthesistn = (t1, · · ·  , tn) 
be the times till failure of the ﬁrst n parts tested, the likelihood ratio statistic is given 
by 
lamda Subscript n Baseline left parenthesis bold t Subscript n Baseline right parenthesis equals left parenthesis StartFraction beta 0 Over beta 1 EndFraction right parenthesis Superscript n Baseline exp left parenthesis minus left parenthesis StartFraction 1 Over beta 1 EndFraction minus StartFraction 1 Over beta 0 EndFraction right parenthesis sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i Baseline right parenthesis periodλn(tn) =
β0 
β1
n 
exp

−
 1 
β1 
− 1 
β0

n

i=1 
ti

.
(9.8.15) 

358
9 
Reliability Analysis 
Table 9.2 OC and ASN 
values for the SPRT 
psiψ
upper R Superscript left parenthesis psi right parenthesisR(ψ)
OCleft parenthesis upper R Superscript left parenthesis psi right parenthesis Baseline right parenthesis(R(ψ) ) 
ASNleft parenthesis upper R Superscript left parenthesis psi right parenthesis Baseline right parenthesis(R(ψ) ) 
minus−2.0 
0.7724 
0.0110
152.0 
minus−1.8 
0.7780 
0.0173
167.9 
minus−1.6 
0.7836 
0.0270
186.7 
minus−1.4 
0.7891 
0.0421
208.6 
minus−1.2 
0.7946 
0.0651
234.1 
minus−1.0 
0.8000 
0.1000
263.0 
minus−0.8 
0.8053 
0.1512
294.2 
minus−0.6 
0.8106 
0.2235
325.5 
minus−0.4 
0.8158 
0.3193
352.7 
minus−0.2 
0.8209 
0.4357
370.2 
0.0
0.8259 
0.5621
373.1 
0.2
0.8309 
0.6834
360.2 
0.4
0.8358 
0.7858
334.8 
0.6
0.8406 
0.8629
302.6 
0.8
0.8453 
0.9159
269.1 
1.0
0.8500 
0.9500
238.0 
1.2
0.8546 
0.9709
210.8 
1.4
0.8590 
0.9833
187.8 
1.6
0.8634 
0.9905
168.6 
1.8
0.8678 
0.9946
152.7 
2.0
0.8720 
0.9969
139.4 
Thus, 
ln lamda Subscript n Baseline left parenthesis bold t Subscript n Baseline right parenthesis equals n ln left parenthesis beta 0 divided by beta 1 right parenthesis minus left parenthesis StartFraction 1 Over beta 1 EndFraction minus StartFraction 1 Over beta 0 EndFraction right parenthesis sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i Baseline periodln λn(tn) = n ln(β0/β1) −
 1 
β1 
− 1 
β0

n

i=1 
ti. 
The SPRT rules are accordingly 
StartLayout 1st Row 1st Column Blank 2nd Column Continue sampling if minus h 1 plus s n less than sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i Baseline less than h 2 plus s n comma 2nd Row 1st Column Blank 2nd Column Accept upper H 0 if sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i Baseline greater than or equals h 2 plus s n comma 3rd Row 1st Column Blank 2nd Column Reject upper H 0 if sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i Baseline less than or equals minus h 1 plus s n comma EndLayout
Continue sampling if − h1 + sn < 
n

i=1 
ti < h2 + sn, 
Accept H0 if 
n

i=1 
ti ≥ h2 + sn, 
Reject H0 if 
n

i=1 
ti ≤−h1 + sn, 
where 

9.8 
Reliability Demonstration
359 
StartLayout 1st Row 1st Column s 2nd Column equals ln left parenthesis beta 0 divided by beta 1 right parenthesis divided by left parenthesis StartFraction 1 Over beta 1 EndFraction minus StartFraction 1 Over beta 0 EndFraction right parenthesis comma 2nd Row 1st Column h 1 2nd Column equals ln left parenthesis left parenthesis 1 minus gamma right parenthesis divided by alpha right parenthesis slash left parenthesis StartFraction 1 Over beta 1 EndFraction minus StartFraction 1 Over beta 0 EndFraction right parenthesis comma 3rd Row 1st Column h 2 2nd Column equals ln left parenthesis left parenthesis 1 minus alpha right parenthesis divided by gamma right parenthesis slash left parenthesis StartFraction 1 Over beta 1 EndFraction minus StartFraction 1 Over beta 0 EndFraction right parenthesis period EndLayout
s = ln(β0/β1)
 
1 
β1 − 1 
β0

, 
h1 = ln((1 − γ )/α)
 
1 
β1 − 1 
β0

, 
h2 = ln((1 − α)/γ )
 
1 
β1 − 1 
β0

. 
(9.8.16) 
Thus, if we plot sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript in 
i=1 ti vs. n, the accept and reject boundaries are again parallel 
straight lines. 
As before, let psiψ be an implicit parameter, and deﬁne 
beta Superscript left parenthesis psi right parenthesis Baseline equals StartLayout Enlarged left brace 1st Row 1st Column StartStartFraction left parenthesis beta 0 divided by beta 1 right parenthesis Superscript psi Baseline minus 1 OverOver psi left parenthesis StartFraction 1 Over beta 1 EndFraction minus StartFraction 1 Over beta 0 EndFraction right parenthesis EndEndFraction comma 2nd Column psi not equals 0 2nd Row 1st Column Blank 3rd Row 1st Column StartStartFraction ln left parenthesis beta 0 divided by beta 1 right parenthesis OverOver StartFraction 1 Over beta 1 EndFraction minus StartFraction 1 Over beta 0 EndFraction EndEndFraction comma 2nd Column psi equals 0 period EndLayoutβ(ψ) = 
⎧ 
⎪⎪⎪⎪⎨ 
⎪⎪⎪⎪⎩ 
(β0/β1)ψ−1 
ψ

1 
β1 − 1 
β0
 ,
ψ ̸= 0 
ln(β0/β1) 
1 
β1 − 1 
β0 
,
ψ
 
= 0. 
(9.8.17) 
Then the OC and ASN functions are approximately given by 
OC left parenthesis beta Superscript left parenthesis psi right parenthesis Baseline right parenthesis almost equals StartLayout Enlarged left brace 1st Row 1st Column StartFraction left parenthesis left parenthesis 1 minus gamma right parenthesis divided by alpha right parenthesis Superscript psi Baseline minus 1 Over left parenthesis left parenthesis 1 minus gamma right parenthesis divided by alpha right parenthesis Superscript psi Baseline minus left parenthesis gamma divided by left parenthesis 1 minus alpha right parenthesis right parenthesis Superscript psi Baseline EndFraction comma 2nd Column psi not equals 0 2nd Row 1st Column Blank 3rd Row 1st Column StartFraction ln left parenthesis left parenthesis 1 minus gamma right parenthesis divided by alpha right parenthesis Over ln left parenthesis left parenthesis 1 minus alpha right parenthesis left parenthesis 1 minus gamma right parenthesis divided by alpha gamma right parenthesis EndFraction comma 2nd Column psi equals 0 EndLayoutOC(β(ψ) ) ≈ 
⎧ 
⎪⎪⎨ 
⎪⎪⎩ 
((1−γ )/α)ψ−1 
((1−γ )/α)ψ−(γ/(1−α))ψ ,
ψ ̸= 0 
ln((1−γ )/α)  
ln((1−α)(1−γ )/αγ )  ,
ψ
 
= 0 
(9.8.18) 
and 
ASN left parenthesis beta Superscript left parenthesis psi right parenthesis Baseline right parenthesis almost equals StartLayout Enlarged left brace 1st Row 1st Column StartStartFraction ln left parenthesis left parenthesis 1 minus gamma right parenthesis divided by alpha right parenthesis minus OC left parenthesis beta Superscript left parenthesis psi right parenthesis Baseline right parenthesis ln left parenthesis left parenthesis 1 minus alpha right parenthesis left parenthesis 1 minus gamma right parenthesis divided by alpha gamma right parenthesis OverOver ln left parenthesis beta 0 divided by beta 1 right parenthesis minus beta Superscript left parenthesis psi right parenthesis Baseline left parenthesis StartFraction 1 Over beta 1 EndFraction minus StartFraction 1 Over beta 0 EndFraction right parenthesis EndEndFraction comma 2nd Column psi not equals 0 2nd Row 1st Column Blank 3rd Row 1st Column StartFraction h 1 h 2 Over s squared EndFraction comma 2nd Column psi equals 0 period EndLayoutASN(β(ψ) ) ≈ 
⎧ 
⎪⎪⎪⎨ 
⎪⎪⎪⎩ 
ln((1−γ )/α)−OC(β(ψ) ) ln((1−α)(1−γ )/αγ )  
ln(β0/β1)−β(ψ)

1 
β1 − 1 
β0

,
ψ ̸= 0 
h1h2 
s2 ,
ψ = 0. 
(9.8.19) 
Note that when psi equals 1ψ = 1, beta Superscript left parenthesis psi right parenthesisβ(ψ) equals beta 0β0, while when psi equals negative 1ψ = −1, beta Superscript left parenthesis psi right parenthesisβ(ψ) equals beta 1β1. 
Example 9.22 Continuing Example 9.19, recall we had alpha equals 0.05α = 0.05, gamma equals 0.10γ = 0.10, beta 0 equals 6153β0 = 
6153, beta 1 equals 4481β1 = 4481. The parameters of the boundaries of the SPRT are s equals 5229s = 5229, 
h 1 equals 47662h1 = 47662, h 2 equals 37124h2 = 37124. The OC and ASN functions, for selected values of psiψ, are  
given in Table 9.3. 
In Example 9.19, we saw that the ﬁxed-sample test with the same alphaα and gammaγ 
requires a sample size of n equals 87n = 87. Thus, in the exponential case as well, we see 
that the SPRT can result in substantial savings in sample size.
black medium square■
It is impractical to perform a sequential test, like the one described in the above 
example by running one system, waiting till it fails, renewing it and running it again 
and again until a decision can be made. In the above example, if the MTTF of the 
system is close to the value of beta 0 equals 6153β0 = 6153 [hr], it takes on the average about 256 days 

360
9 
Reliability Analysis 
Table 9.3 OC and ASN 
values for the SPRT 
psiψ
beta Superscript left parenthesis psi right parenthesisβ(ψ) 
OCleft parenthesis beta Superscript left parenthesis psi right parenthesis Baseline right parenthesis(β(ψ) ) 
ASNleft parenthesis beta Superscript left parenthesis psi right parenthesis Baseline right parenthesis(β(ψ) ) 
minus−2.0 
3872 
0.0110
34.3 
minus−1.8 
3984 
0.0173
37.1 
minus−1.6 
4101 
0.0270
40.2 
minus−1.4 
4223 
0.0421
43.8 
minus−1.2 
4349 
0.0651
47.9 
minus−1.0 
4481 
0.1000
52.4 
minus−0.8 
4618 
0.1512
57.1 
minus−0.6 
4762 
0.2235
61.4 
minus−0.4 
4911 
0.3193
64.7 
minus−0.2 
5067 
0.4357
66.1 
0.0
5229 
0.5621
64.7 
0.2
5398 
0.6834
60.7 
0.4
5575 
0.7858
54.8 
0.6
5759 
0.8629
48.1 
0.8
5952 
0.9159
41.5 
1.0
6153 
0.9500
35.6 
1.2
6363 
0.9709
30.6 
1.4
6582 
0.9833
26.4 
1.6
6811 
0.9905
22.9 
1.8
7051 
0.9946
20.1 
2.0
7301 
0.9969
17.8 
between failures, and on the average about 36 failures till decision is reached. This 
trial may take over 25 years. There are three ways to overcome this problem. The 
ﬁrst is to put on test several systems simultaneously. Thus, if in the trial described 
in the above example 25 systems are tested simultaneously, the expected duration 
of the test will be reduced to one year. Another way is to consider a test based 
on a continuous time process, not on discrete samples of failure times. The third 
possibility of reducing the expected test duration is to perform accelerated life 
testing. In the following sections we discuss these alternatives. 
9.8.2.3 
The SPRT for Poisson Processes 
Suppose that we put n systems on test starting at t equals 0t = 0. Suppose also that any system 
which fails is instantaneously renewed, and at the renewal time it is as good as new. 
In addition we assume that the life characteristics of the systems are identical, the 
TTF of each system is exponential (with the same betaβ), and failures of different 
systems are independent of each other. 
Under these assumptions, the number of failures in each system, in the time 
interval left parenthesis 0 comma t right bracket(0, t], is a Poisson random variable with mean lamda tλt, where lamda equals 1 divided by betaλ = 1/β. 

9.8 
Reliability Demonstration
361 
Let upper X Subscript n Baseline left parenthesis t right parenthesisXn(t) be the total number of failures among all the n systems during the time 
interval left parenthesis 0 comma t right bracket(0, t]. upper X Subscript n Baseline left parenthesis t right parenthesis tilde Pos left parenthesis n lamda t right parenthesisXn(t) ∼ Pos(nλt), and the collection left brace upper X Subscript n Baseline left parenthesis t right parenthesis semicolon 0 less than t less than normal infinity right brace{Xn(t); 0 < t  <  ∞} is called 
a Poisson process. We add the initial condition that upper X Subscript n Baseline left parenthesis 0 right parenthesis equals 0Xn(0) = 0. 
The random function upper X Subscript n Baseline left parenthesis t right parenthesisXn(t), 0 less than t less than normal infinity0 < t  <  ∞, is a non-decreasing step function which 
jumps one unit at each random failure time of the system. The random functions 
upper X Subscript n Baseline left parenthesis t right parenthesisXn(t) satisfy: 
(i) 
.Xn(t), ∼Pos(nλt), all 0 less than t less than normal infinity0 < t  <  ∞. 
(ii) For any t 1 less than t 2t1 < t2, upper X Subscript n Baseline left parenthesis t 2 right parenthesis minus upper X Subscript n Baseline left parenthesis t 1 right parenthesisXn(t2) − Xn(t1) is independent of upper X Subscript n Baseline left parenthesis t 1 right parenthesisXn(t1). 
(iii) For any t 1t1, t 2t2, 0 less than t 1 less than t 2 less than normal infinity0 < t1 < t2 < ∞, upper X Subscript n Baseline left parenthesis t 2 right parenthesis minus upper X Subscript n Baseline left parenthesis t 1 right parenthesis tilde Pos left parenthesis n lamda left parenthesis t 2 minus t 1 right parenthesis right parenthesisXn(t2) −Xn(t1) ∼ Pos(nλ(t2 − t1)). 
We develop now the SPRT based on the random functions upper X Subscript n Baseline left parenthesis t right parenthesisXn(t). 
The hypotheses upper H 0 colon beta greater than or equals beta 0H0 : β ≥ β0 versus upper H 1 colon beta less than or equals beta 1H1 : β ≤ β1, for  0 less than beta 1 less than beta 0 less than normal infinity0 < β1 < β0 < ∞, are  
translated to the hypotheses upper H 0 colon lamda less than or equals lamda 0H0 : λ ≤ λ0 versus upper H 1 colon lamda greater than or equals lamda 1H1 : λ ≥ λ1 where lamda equals 1 divided by betaλ = 1/β. The  
likelihood ratio at time t is 
upper Lamda left parenthesis t semicolon upper X Subscript n Baseline left parenthesis t right parenthesis right parenthesis equals left parenthesis StartFraction lamda 1 Over lamda 0 EndFraction right parenthesis Superscript upper X Super Subscript n Superscript left parenthesis t right parenthesis Baseline exp left brace minus n t left parenthesis lamda 1 minus lamda 0 right parenthesis right brace period(t; Xn(t)) =
λ1 
λ0
Xn(t) 
exp{−nt(λ1 − λ0)}.
(9.8.20) 
The test continues as long as the random graph of left parenthesis upper T Subscript n Baseline left parenthesis t right parenthesis comma upper X Subscript n Baseline left parenthesis t right parenthesis right parenthesis(Tn(t), Xn(t)) is between the two 
linear boundaries 
b Subscript upper U Baseline left parenthesis t right parenthesis equals h 2 plus s upper T Subscript n Baseline left parenthesis t right parenthesis comma 0 less than or equals t less than normal infinitybU(t) = h2 + sTn(t), 
0 ≤ t <  ∞ 
and 
b Subscript upper L Baseline left parenthesis t right parenthesis equals minus h 1 plus s upper T Subscript n Baseline left parenthesis t right parenthesis comma 0 less than or equals t less than normal infinity commabL(t) = −h1 + sTn(t), 
0 ≤ t <  ∞, 
where upper T Subscript n Baseline left parenthesis t right parenthesis equals n tTn(t) = nt is the total time on test at t, 
h 1 equals StartStartFraction ln left parenthesis StartFraction 1 minus alpha Over gamma EndFraction right parenthesis OverOver ln left parenthesis StartFraction lamda 1 Over lamda 0 EndFraction right parenthesis EndEndFraction commah1 = 
ln

1−α 
γ

ln

λ1 
λ0
 , 
h 2 equals StartStartFraction ln left parenthesis StartFraction 1 minus gamma Over alpha EndFraction right parenthesis OverOver ln left parenthesis StartFraction lamda 1 Over lamda 0 EndFraction right parenthesis EndEndFraction commah2 = 
ln

1−γ 
α

ln

λ1 
λ0
 ,
(9.8.21) 
and 
s equals StartStartFraction lamda 1 minus lamda 0 OverOver ln left parenthesis StartFraction lamda 1 Over lamda 0 EndFraction right parenthesis EndEndFraction periods = λ1 − λ0 
ln

λ1 
λ0
 . 
The instant upper X Subscript n Baseline left parenthesis t right parenthesisXn(t) jumps above b Subscript upper U Baseline left parenthesis t right parenthesisbU(t), the test terminates, and upper H 0H0 is rejected; on 
the other hand, the instant upper X Subscript n Baseline left parenthesis t right parenthesis equals b Subscript upper L Baseline left parenthesis t right parenthesisXn(t) = bL(t) the test terminates and upper H 0H0 is accepted. 

362
9 
Reliability Analysis 
Acceptance of upper H 0H0 entails that the reliability meets the speciﬁed requirement. 
Rejection of upper H 0H0 may lead to additional engineering modiﬁcation to improve the 
reliability of the system. 
The OC function of this sequential test is the same as that in the exponential case. 
Let tauτ denote the random time of termination. It can be shown that PrSubscript lamda Baseline left brace tau less than normal infinity right brace equals 1λ{τ <  ∞} = 1 
for all 0 less than lamda less than normal infinity0 < λ <  ∞. The expected deviation of the test is given approximately by 
upper E Subscript lamda Baseline left brace tau right brace equals StartFraction 1 Over lamda n EndFraction upper E Subscript lamda Baseline left brace upper X Subscript n Baseline left parenthesis tau right parenthesis right brace commaEλ{τ} =  1 
λn Eλ{Xn(τ)},
(9.8.22) 
where 
upper E Subscript lamda Baseline left brace upper X Subscript n Baseline left parenthesis tau right parenthesis right brace approximately equals StartLayout Enlarged left brace 1st Row 1st Column StartFraction h 2 minus OC left parenthesis lamda right parenthesis left parenthesis h 1 plus h 2 right parenthesis Over 1 minus s divided by lamda EndFraction comma 2nd Column if lamda not equals s 2nd Row 1st Column Blank 3rd Row 1st Column h 1 h 2 comma 2nd Column if lamda equals s period EndLayoutEλ{Xn(τ)} ∼= 
⎧ 
⎪⎪⎨ 
⎪⎪⎩ 
h2−OC(λ)(h1+h2) 
1−s/λ 
,
if λ ̸= s 
h1h2,
if λ = s. 
(9.8.23) 
It should be noticed that the last formula yields the same values as the formula in the 
exponential case for lamda equals 1 divided by beta Superscript left parenthesis psi right parenthesisλ = 1/β(ψ). The SPRT of the previous section can terminate 
only after a failure, while the SPRT based on upper X Subscript n Baseline left parenthesis t right parenthesisXn(t) may terminate while crossing 
the lower boundary b Subscript upper L Baseline left parenthesis t right parenthesisbL(t), before a failure occurs. 
The minimal time required to accept upper H 0H0 is tau 0 equals h 1 divided by n sτ0 
= h1/ns. In the case of 
Example 9.21, with n equals 20n = 20, tau 0 equals 9.11536 divided by left parenthesis 20 times 0.0001912 right parenthesis equals 2383.2τ0 = 9.11536/(20 × 0.0001912) = 2383.2 [hr], 
that is, over 99 days of testing without any failure. The SPRT may be, in addition, 
frequency censored by ﬁxing a value x Superscript asteriskx∗ so that as soon as upper X Subscript n Baseline left parenthesis t right parenthesis greater than or equals x Superscript asteriskXn(t) ≥ x∗ the test 
terminates and upper H 0H0 is rejected. In Example 9.21 we see that the expected number of 
failures at termination may be as large as 66. We can censor the test at x Superscript asterisk Baseline equals 50x∗ = 50. 
This will reduce the expected duration of the test but will increase the probability 
of a Type I error, alphaα. Special programs are available for computing the operating 
characteristics of such censored tests, but these are beyond the scope of the present 
text. 
9.9 
Accelerated Life Testing 
It is often impractical to test highly reliable systems, or components, under normal 
operating conditions because no failures may be observed during long periods of 
time. In accelerated life testing, the systems are subjected to higher than normal 
stress conditions in order to generate failures. The question is how to relate failure 
distributions, under higher than normal stress conditions, to those under normal 
conditions? 
Accelerated life testing is used by engineers in testing materials like food and 
drugs, lubricants, concrete and cement, building materials, and nuclear reactor 
materials. The stress conditions are generally, mechanical load, vibrations, high 

9.9 
Accelerated Life Testing
363 
temperatures, high humidity, high contamination, etc. Accelerated testing is used 
for semiconductors including transistors, electronic devices such as diodes, random 
access memories, plastic encapsulants, etc. The reader is referred to Nelson (2004) 
for a survey of methods and applications. The statistical methodology of accelerated 
life testing is similar to the methods described earlier in this chapter, including 
graphical analysis and maximum likelihood estimation. The reader is referred to 
Meeker et al. (2021), Nelson (2004) and Mann et al. (1974), for details. We describe 
below some of the models used to relate failures under various stress conditions. 
9.9.1 
The Arrhenius Temperature Model 
This model is widely used when the product failure time is sensitive to high 
temperature. Applications include electrical insulations and dielectric (see Goba 
1969), solid state and semiconductors (Peck et al. 1994), battery cells, lubricants 
and greases, plastics, and incandescent light ﬁlaments. 
The Arrhenius law states that the rate of simple chemical reaction depends on 
temperature as follows: 
lamda equals upper A exp left brace negative upper E divided by left parenthesis k upper T right parenthesis right brace commaλ = A exp{−E/(kT )},
(9.9.1) 
where E is the activation energy (in electron volts, k is the Boltzmann constant, 
8.6171 times 10 Superscript negative 58.6171×10−5 electron volts per Superscript ring◦C, T is the absolute Kelvin temperature left parenthesis 273.16 plus Superscript ring(273.16+
◦C), and A is a product parameter, which depends on the test conditions and failure 
characteristics. In applying the Arrhenius model to failure times distribution, we 
ﬁnd the Weibull–Arrhenius life distribution, in which the scale parameter betaβ of the 
Weibull distribution is related to temperature, T , according to the function 
beta left parenthesis upper T right parenthesis equals lamda exp left brace upper A plus StartFraction upper B Over upper T EndFraction right brace comma upper B greater than 0 commaβ(T ) = λ exp

A + B 
T

,
B
 
>
 
0,
(9.9.2) 
where lamdaλ, A, and B are ﬁtted empirically to the data. 
9.9.2 
Other Models 
Another model is called the lognormal Arrhenius model, in which the log failure 
time is normally distributed with mean upper A plus upper B divided by upper TA+B/T and variance sigma squaredσ 2. According to this 
model, the expected failure time is exp left brace upper A plus upper B divided by upper T plus sigma squared divided by 2 right braceexp{A+B/T +σ 2/2}. Another model prevalent 
in the literature relates the expected failure time to a stress level V according to the 
inverse power model 

364
9 
Reliability Analysis 
upper M upper T upper T upper F left parenthesis upper V right parenthesis equals StartFraction upper C Over upper V Superscript p Baseline EndFraction comma upper C greater than 0 periodMT T F(V ) = C 
V p ,
C
 
>
 
0.
(9.9.3) 
The statistical data analysis methodology is to ﬁt an appropriate model to the 
data, usually by maximum likelihood estimation, and then predict the MTTF of 
the system under normal conditions or some reliability or availability function. 
Tolerance intervals, for the predicted value, should be determined. 
9.10 
Burn-In Procedures 
Many products show a relatively high frequency of early failures. For example, if 
a product has an exponential distribution of the TTF with MTTF of beta equals 10,000β = 10,000 
[hr], we do not expect more than 2% of the product to fail within the ﬁrst 200 
[hr]. Nevertheless, many products designed for high value of MTTF show a higher 
than expected number of early failures. This phenomenon led to the theory that the 
hazard rate function of products is typically a U-shaped function. In its early life, 
the product is within a phase with monotone decreasing hazard rate. This phase is 
called the “infant mortality” phase. After this phase, the product enters a phase of 
“maturity” in which the hazard rate function is almost constant. Burn-in procedures 
are designed to screen (burn) the weak products within the plant, by setting the 
product to operate for several days, in order to give the product a chance to fail in 
the plant and not in the ﬁeld, where the loss due to failure is high. 
How long should a burn-in procedure last? Jensen and Petersen (1991) discuss 
this and other issues in designing burn-in procedures. We present here some basic 
ideas. 
Burn-in procedures discussed by Jensen and Petersen are based on a model 
of a mixed life distribution. For example, suppose that experience shows that the 
life distribution of a product is Weibull, upper W left parenthesis nu comma beta 1 right parenthesisW(ν, β1). A small proportion of units 
manufactured may have generally short life, due to various reasons, which is 
given by another Weibull distribution, say upper W left parenthesis nu comma beta 0 right parenthesisW(ν, β0), with beta 0 less than beta 1β0 < β1. Thus, the life 
distribution of a randomly chosen product has a distribution which is a mixture of 
upper W left parenthesis nu comma beta 0 right parenthesisW(ν, β0) and upper W left parenthesis nu comma beta 1 right parenthesisW(ν, β1), i.e., 
upper F left parenthesis t right parenthesis equals 1 minus left bracket p exp left brace minus left parenthesis StartFraction t Over beta 0 EndFraction right parenthesis Superscript nu Baseline right brace plus left parenthesis 1 minus p right parenthesis exp left brace minus left parenthesis StartFraction t Over beta 1 EndFraction right parenthesis Superscript nu Baseline right brace right bracket commaF(t) = 1 −

p exp

−
 t 
β0
ν
+ (1 − p) exp

−
 t 
β1
ν
,
(9.10.1) 
for t greater than 0t >  0. The objective of the burn-in is to let units having the upper W left parenthesis nu comma beta 0 right parenthesisW(ν, β0) distribution 
an opportunity to fail in the plant. The units that do not fail during the burn-in have, 
for their remaining life, a life distribution closer to the desired upper W left parenthesis nu comma beta 1 right parenthesisW(ν, β1). 
Suppose that a burn-in continues for t Superscript asteriskt∗ time units. The conditional distribution 
of the time till failure T , given that left brace upper T greater than t Superscript asterisk Baseline right brace{T >  t∗}, is  

9.10 
Burn-In Procedures
365 
upper F Superscript asterisk Baseline left parenthesis t right parenthesis equals StartFraction integral Subscript t Superscript asterisk Baseline Superscript t Baseline f left parenthesis u right parenthesis normal d u Over 1 minus upper F left parenthesis t Superscript asterisk Baseline right parenthesis EndFraction comma t greater than or equals t Superscript asterisk Baseline periodF ∗(t) =
 t 
t∗ f (u)  du 
1 − F(t∗) ,
t
 
≥ t∗.
(9.10.2) 
The c.d.f. upper F Superscript asterisk Baseline left parenthesis t right parenthesisF ∗(t), of units surviving the burn-in, starts at t Superscript asteriskt∗, i.e., upper F Superscript asterisk Baseline left parenthesis t Superscript asterisk Baseline right parenthesis equals 0F ∗(t∗) = 0 and has 
MTTF 
beta Superscript asterisk Baseline equals t Superscript asterisk Baseline plus integral Subscript t Superscript asterisk Baseline Superscript normal infinity Baseline left parenthesis 1 minus upper F Superscript asterisk Baseline left parenthesis t right parenthesis right parenthesis normal d t periodβ∗ = t∗+
 ∞ 
t∗ (1 −F ∗(t)) dt.
(9.10.3) 
We illustrate this in the following example on mixtures of exponential life times. 
Example 9.23 Suppose that a product is designed to have an exponential life 
distribution, with mean of beta equals 10,000β = 10,000 [hr]. A proportion p equals 0.05p = 0.05 of the products 
comes out of the production process with a short MTTF of gamma equals 100γ = 100 [hr]. Suppose 
that all products go through a burn-in for t Superscript asterisk Baseline equals 200t∗ = 200 [hr]. 
The c.d.f. of the TTF of units which did not fail during the burn-in is 
StartLayout 1st Row 1st Column upper F Superscript asterisk Baseline left parenthesis t right parenthesis 2nd Column equals 1 minus StartStartFraction 0.05 exp left brace minus StartFraction t Over 100 EndFraction right brace plus 0.95 exp left brace minus StartFraction t Over 10 comma 000 EndFraction right brace OverOver 0.05 exp left brace negative StartFraction 200 Over 100 EndFraction right brace plus 0.95 exp left brace minus StartFraction 200 Over 10 comma 000 EndFraction right brace EndEndFraction 2nd Row 1st Column Blank 2nd Column equals 1 minus StartFraction 1 Over 0.93796 EndFraction left bracket 0.05 exp left brace minus StartFraction t Over 100 EndFraction right brace plus 0.95 exp left brace minus StartFraction t Over 10 comma 000 EndFraction right brace right bracket EndLayoutF ∗(t) = 1 − 
0.05 exp{− t 
100} +  0.95 exp{− 
t 
10,000} 
0.05 exp{−200 
100} +  0.95 exp{− 200 
10,000} 
= 1 −
1 
0.93796

0.05 exp

− t 
100

+ 0.95 exp

−
t 
10, 000

for t greater than or equals 200t ≥ 200. The mean time till failure, for units surviving the burn-in, is thus 
StartLayout 1st Row 1st Column beta Superscript asterisk 2nd Column equals 200 plus StartFraction 1 Over 0.93796 EndFraction integral Subscript 200 Superscript normal infinity Baseline left parenthesis 0.05 exp left brace minus StartFraction t Over 100 EndFraction right brace plus 0.95 exp left brace minus StartFraction t Over 10 comma 000 EndFraction right brace right parenthesis normal d t 2nd Row 1st Column Blank 2nd Column equals 200 plus StartFraction 5 Over 0.93796 EndFraction exp left brace negative StartFraction 200 Over 100 EndFraction right brace plus StartFraction 9500 Over 0.93796 EndFraction exp left brace minus StartFraction 200 Over 10 comma 000 EndFraction right brace 3rd Row 1st Column Blank 2nd Column equals 10,128.53 left bracket hr right bracket period EndLayoutβ∗ = 200 + 
1 
0.93796
 ∞ 
200

0.05 exp

− t 
100

+ 0.95 exp

−
t 
10, 000

dt 
= 200 + 
5 
0.93796 exp

−200 
100

+ 9500 
0.93796 exp

− 200 
10, 000

= 10,128.53 [hr]. 
A unit surviving 200 h of burn-in is expected to operate an additional 9928.53 h in 
the ﬁeld. The expected life of these units without the burn-in is 0.05 times 200 plus 0.95 times 10,000 equals 95100.05×200+ 0.95× 
10,000 = 9510 [hr]. The burn-in of 200 h in the plant is expected to increase the 
mean life of the product in the ﬁeld by 418 h. Whether this increase in the MTTF 
justiﬁes the burn-in depends on the relative cost of burn-in in the plant to the cost of 
failures in the ﬁeld. The proportion p of “short life” units plays also an important 
role. If this proportion is p equals 0.1p = 0.1 rather than 0.05, the burn-in increases the MTTF 
in the ﬁeld from 9020 h to 9848.95 h. One can easily verify that for p equals 0.2p = 0.2, if the  
income for an hour of operation of one unit in the ﬁeld is upper C Subscript p Baseline equals 5 normal dollar signCp = 5$ and the cost of 
the burn-in per unit is 0.15$ per hour, then the length of burn-in which maximizes 
the expected proﬁt is about 700 h.
black medium square■

366
9 
Reliability Analysis 
9.11 
Chapter Highlights 
The main concepts and deﬁnitions introduced in this chapter include: 
• 
Life distributions 
• 
Accelerated life testing 
• 
Availability 
• 
Time categories 
• 
Up time 
• 
Down time 
• 
Intrinsic availability 
• 
Operational readiness 
• 
Mean time to Failure (MTTF) 
• 
Reliability function 
• 
Failure rate 
• 
Structure function 
• 
Time till failure (TTF) 
• 
Time till repair (TTR) 
• 
Cycle time 
• 
Renewal function 
• 
Censored data 
• 
Product limit (PL) estimator 
• 
Average sample number (ASN) 
• 
Sequential probability ratio test (SPRT) 
• 
Burn-in procedure 
9.12 
Exercises 
Exercise 9.1 During 600 h of manufacturing time, a machine was up 510 h. It had 
100 failures which required a total of 11 h of repair time. What is the MTTF of this 
machine? What is its mean time till repair, MTTR? What is the intrinsic availability? 
Exercise 9.2 The frequency distribution of the lifetime in a random sample of n = 
2000 solar cells, under accelerated life testing is the following: 
StartEnclose left right StartLayout 1st Row 1st Column t left bracket 10 cubed left bracket h r right bracket 2nd Column 0 minus 1 3rd Column 1 minus 2 4th Column 2 minus 3 5th Column 3 minus 4 6th Column 4 minus 5 7th Column 5 minus 2nd Row 1st Column Prof period freq period 2nd Column 0.15 3rd Column 0.25 4th Column 0.25 5th Column 0.10 6th Column 0.10 7th Column 0.15 EndLayout EndEnclose
t[103[hr]
0−1
1−2
2−3
3−4
4−5
5− 
Prof. freq.
0.15
0.25
0.25
0.10
0.10
0.15 
The relationship of the scale parameters of the life distributions, between normal 
and accelerated conditions, is 10:1. 
(i) Estimate the reliability of the solar cells at age t = 4.0 [yr]. 
(ii) What proportion of solar cells are expected to survive 40,000 [hr] among those 
which survived 20,000 [hr]? 

9.12 
Exercises
367 
Exercise 9.3 The CDF of the lifetime [months] of an equipment is 
upper F left parenthesis t right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column t Superscript 4 Baseline divided by 20736 comma 2nd Column 0 less than or equals t less than 12 2nd Row 1st Column Blank 3rd Row 1st Column 1 comma 2nd Column 12 less than or equals t period EndLayoutF(t) = 
⎧ 
⎪⎪⎨ 
⎪⎪⎩ 
t4/20736, 0 ≤ t <  12 
1,
12 ≤ t. 
(i) What is the failure rate function of this equipment? 
(ii) What is the MTTF? 
(iii) What is the reliability of the equipment at 4 months? 
Exercise 9.4 The reliability of a system is 
upper R left parenthesis t right parenthesis equals exp left brace minus 2 t minus 3 t squared right brace comma 0 less than or equals t less than normal infinity periodR(t) = exp{−2t − 3t2}, 
0 ≤ t <  ∞. 
(i) What is the failure rate of this system at age t = 3? 
(ii) Given that the system reached the age of t = 3, what is its reliability for two 
additional time units? 
Exercise 9.5 An aircraft has four engines but can land using only two engines. 
(i) Assuming that the reliability of each engine, for the duration of a mission, 
is R = 0.95, and that engine failures are independent, compute the mission 
reliability of the aircraft. 
(ii) What is the mission reliability of the aircraft if at least one functioning engine 
must be on each wing? 
Exercise 9.6 
(i) Draw a block diagram of a system having the structure function 
upper R Subscript sys Baseline equals psi Subscript s Baseline left parenthesis psi Subscript p Baseline left parenthesis psi Subscript upper M 1 Baseline comma psi Subscript upper M 2 Baseline right parenthesis comma upper R 6 right parenthesis comma psi Subscript upper M 1 Baseline equals psi Subscript p Baseline left parenthesis upper R 1 comma upper R 2 upper R 3 right parenthesis comma psi Subscript upper M 2 Baseline equals psi 2 left parenthesis upper R 4 comma upper R 5 right parenthesisRsys = ψs(ψp(ψM1, ψM2), R6), ψM1 = ψp(R1, R2R3), ψM2 = ψ2(R4, R5) 
(ii) Determine Rsys if all the components act independently and have the same 
reliability R = 0.8. 
Exercise 9.7 Consider a system of n components in a series structure. Let 
R1, · · ·  , Rn be the reliabilities of the components. Show that 
upper R Subscript sys Baseline greater than or equals 1 minus sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis 1 minus upper R Subscript i Baseline right parenthesis periodRsys ≥1 − 
n

i=1 
(1 − Ri). 

368
9 
Reliability Analysis 
Exercise 9.8 A 4 out of 8 system has identical components whose life lengths 
T [weeks] are independent and identically distributed like a Weibull W

1 
2, 100

. 
What is the reliability of the system at t0 = 5 weeks? 
Exercise 9.9 A system consists of a main unit and two standby units. The lifetimes 
of these units are exponential with mean β = 100 [hr]. The standby units undergo 
no failure while idle. Switching will take place when required. What is the MTTF 
of the system? What is the reliability function of this system? 
Exercise 9.10 Suppose that the TTF in a renewal cycle has a W(α, β) distribution 
and that the TTR has a lognormal distribution LN(μ, σ). Assume further that TTF 
and TTR are independent. What are the mean and standard deviation of a renewal 
cycle. 
Exercise 9.11 Suppose that a renewal cycle has the normal distribution N(100, 10). 
Determine the p.d.f. of NR(200). 
Exercise 9.12 Let the renewal cycle C be distributed like N(100, 10). Approximate 
V (1000). 
Exercise 9.13 Derive the renewal density v(t) for a renewal process with C ∼ 
N(100, 10). 
Exercise 9.14 Two identical components are connected in parallel. The system is 
not repaired until both components fail. Assuming that the TTF of each component 
is exponentially distributed, E(β), and the total repair time is G(2, γ  ), derive the 
Laplace transform of the availability function A(t) of the system. 
Exercise 9.15 Simulate a sample of 100 TTF of a system comprised of two 
components connected in parallel, where the life distribution of each component (in 
hours) is E(100). Similarly, simulate a sample of 100 repair times (in hours), having 
a G(2, 1) distribution. Estimate the expected value and variance of the number of 
renewals in 2000 [hr]. 
Exercise 9.16 In a given life test, n = 15 units are placed to operate independently. 
The time till failure of each unit has an exponential distribution with mean 2000 
[hr]. The life test terminates immediately after the 10th failure. How long is the test 
expected to last? 
Exercise 9.17 If n units are put on test and their TTF are exponentially distributed 
with mean β, the time elapsed between the rth and (r + 1)th failure, i.e., n,r = 
Tn,r+1−Tn,r, is exponentially distributed with mean β/(n−r), r = 0, 1, · · ·  , n−1. 
Also, n,0, n,2, · · ·  , n,n−1 are independent. What is the variance of Tn,r? Use  
this result to compute the variance of the test length in the previous exercise. 
Exercise 9.18 Consider again the previous exercise. How would you estimate 
unbiasedly the scale parameter β if the r failure times Tn,1, Tn,2, · · ·  , Tn,r are given? 
What is the variance of this unbiased estimator? 

9.12 
Exercises
369 
Exercise 9.19 Simulate a random sample of 100 failure times, following the 
Weibull distribution W(2.5, 10). Draw a Weibull Probability plot of the data. 
Estimate the parameters of the distribution from the parameters of the linear 
regression ﬁtted to the Q–Q plot. 
Exercise 9.20 The following is a random sample of the compressive strength of 20 
concrete cubes [kg/cm2]: 
StartLayout 1st Row 1st Column Blank 2nd Column 94.9 comma 106.9 comma 229.7 comma 275.7 comma 144.5 comma 112.8 comma 159.3 comma 153.1 comma 270.6 comma 322.0 comma 2nd Row 1st Column Blank 2nd Column 216.4 comma 544.6 comma 266.2 comma 263.6 comma 138.5 comma 79.0 comma 114.6 comma 66.1 comma 131.2 comma 91.1 EndLayout
94.9, 106.9, 229.7, 275.7, 144.5, 112.8, 159.3, 153.1, 270.6, 322.0, 
216.4, 544.6, 266.2, 263.6, 138.5, 79.0, 114.6, 66.1, 131.2, 91.1 
Make a lognormal Q–Q plot of these data and estimate the mean and standard 
deviation of this distribution. 
Exercise 9.21 The following data represent the time till ﬁrst failure [days] of 
electrical equipment. The data were censored after 400 days. 
13 comma 157 comma 172 comma 176 comma 249 comma 303 comma 350 comma 400 Superscript plus Baseline comma 400 Superscript plus Baseline period13, 157, 172, 176, 249, 303, 350, 400+, 400+. 
(Censored values appear as x+.) Make a Weibull Q–Q plot of these data and 
estimate the median of the distribution. 
Exercise 9.22 Make a PL (Kaplan–Meier) estimate of the reliability function of an 
electronic device, based on 50 failure times in dataset ELECFAIL.csv. 
Exercise 9.23 Assuming that the failure times in dataset ELECFAIL.csv come 
from an exponential distribution E(β), compute the MLE of β and of R(50; β) = 
exp{−50/β}. [The MLE of a function of a parameter is obtained by substituting the 
MLE of the parameter in the function.] Determine conﬁdence intervals for β and for 
R(50; β) at level of conﬁdence 0.95. 
Exercise 9.24 The following are values of 20 random variables having an exponen-
tial distribution E(β). The values are censored at t∗ = 200. 
StartLayout 1st Row 1st Column Blank 2nd Column 96.88 comma 154.24 comma 67.44 comma 191.72 comma 173.36 comma 200 comma 140.81 comma 200 comma 154.71 comma 120.73 comma 2nd Row 1st Column Blank 2nd Column 24.29 comma 10.95 comma 2.36 comma 186.93 comma 57.61 comma 99.13 comma 32.74 comma 200 comma 39.77 comma 39.52 period EndLayout
96.88, 154.24, 67.44, 191.72, 173.36, 200, 140.81, 200, 154.71, 120.73, 
24.29, 10.95, 2.36, 186.93, 57.61, 99.13, 32.74, 200, 39.77, 39.52. 
Determine the MLE of β. Use  β equal to the MLE, to estimate the standard deviation 
of the MLE and to obtain conﬁdence interval for β, at level  1  − α = 0.95. [This 
simulation is called an empirical Bootstrap.] 
Exercise 9.25 Determine n0 and r for a frequency censoring test for the exponen-
tial distribution, where the cost of a unit is 10 times bigger than the cost per time 
unit of testing. We wish that S.E.{ ˆβn} =  0.1β, and the expected cost should be 
minimized at β = 100 [hr]. What is the expected cost of this test, at β = 100, when 
c1 = $1 [hr]. 

370
9 
Reliability Analysis 
Exercise 9.26 Dataset WEIBUL.csv contains the values of a random sample of 
size n = 50 from a Weibull distribution. 
(i) Obtain MLE of the scale and shape parameters β and ν. 
(ii) Use the MLE estimates 
of β and ν, to obtain parametric bootstrap EBD of the distribution of ˆβ, ˆν, with 
M = 500 runs. Estimate from this distribution the standard deviations of ˆβ and ˆν. 
Compare these estimates to the large sample approximations. 
Exercise 9.27 In binomial life testing by a ﬁxed size sample, how large should 
the sample be in order to discriminate between R0 = 0.99 and R1 = 0.90, with 
α = β = 0.01? [α and β denote the probabilities of error of Type I and II.] 
Exercise 9.28 Design the Wald SPRT for binomial life testing, in order to discrim-
inate between R0 = 0.99 and R1 = 0.90, with α = β = 0.01. What is the expected 
sample size, ASN, if R = 0.9? 
Exercise 9.29 Design a Wald SPRT for exponential life distribution, to discrimi-
nate between R0 = 0.99 and R1 = 0.90, with α = β = 0.01. What is the expected 
sample size, ASN, when R = 0.90? 
Exercise 9.30 n = 20 computer monitors are put on accelerated life testing. The 
test is an SPRT for Poisson processes, based on the assumption that the TTF 
of a monitor, in those conditions, is exponentially distributed. The monitors are 
considered to be satisfactory if their MTBF is β ≥ 2000 [hr] and considered to be 
unsatisfactory if β ≤1500 [hr]. What is the expected length of the test if β = 2000 
[hr]. 
Exercise 9.31 A product has an exponential life time with MTTF β = 1, 000 [hr]. 
1% of the products come out of production with MTTF of γ = 500 [hr]. A burn-in 
of t∗ = 300 [hr] takes place. What is the expected life of units surviving the burn-in? 
Is such a long burn-in justiﬁed? 

Chapter 10 
Bayesian Reliability Estimation and 
Prediction 
Preview It is often the case that information is available on the parameters of 
the life distributions from prior experiments or prior analysis of failure data. The 
Bayesian approach provides the methodology for formal incorporation of prior 
information with current data. This chapter presents reliability estimation and 
prediction from a Bayesian perspective. It introduces the reader to prior and pos-
terior distributions used in Bayesian lamdaλ reliability inference, discusses loss functions 
and Bayesian estimators and nonparametric distribution-free Bayes estimators of 
reliability. A section is dedicated to Bayesian credibility and prediction intervals. A 
ﬁnal section covers empirical Bayes methods. 
10.1 
Prior and Posterior Distributions 
Let upper X 1 comma midline horizontal ellipsis comma upper X Subscript n BaselineX1, · · · , Xn be a random sample from a distribution with a p.d.f. f left parenthesis x semicolon bold italic theta right parenthesisf (x; θ), where 
bold italic theta equals left parenthesis theta 1 comma midline horizontal ellipsis comma theta Subscript k Baseline right parenthesisθ = (θ1, · · · , θk) is a vector of k parameters, belonging to a parameter space upper Theta. So  
far we assumed that the bold italic thetaθ is an unknown constant. In the Bayesian framework, bold italic thetaθ
is considered a random vector with speciﬁed distribution. The distribution of bold italic thetaθ is 
called a prior distribution. The problem of which prior distribution to adopt for 
the Bayesian model is challenging, since the values of bold italic thetaθ are not directly observable. 
The discussion of this problem is beyond the scope of the book. 
Let h left parenthesis theta 1 comma midline horizontal ellipsis comma theta Subscript k Baseline right parenthesish(θ1, · · · , θk) denote the joint p.d.f. of left parenthesis theta 1 comma midline horizontal ellipsis comma theta Subscript k Baseline right parenthesis(θ1, · · · , θk), corresponding to the 
prior distribution. This p.d.f. is called the prior p.d.f. of bold italic thetaθ. The joint p.d.f. of X and 
bold italic thetaθ is 
g left parenthesis x comma bold italic theta right parenthesis equals f left parenthesis x semicolon bold italic theta right parenthesis h left parenthesis bold italic theta right parenthesis periodg(x, θ) = f (x; θ)h(θ).
(10.1.1) 
The marginal p.d.f. of X, which is called the predictive p.d.f., is  
f Superscript asterisk Baseline left parenthesis x right parenthesis equals integral midline horizontal ellipsis Underscript upper Theta Endscripts integral f left parenthesis x semicolon bold italic theta right parenthesis h left parenthesis bold italic theta right parenthesis normal d theta 1 midline horizontal ellipsis normal d theta Subscript k Baseline periodf ∗(x) =

· · ·


f (x; θ)h(θ) dθ1 · · · dθk.
(10.1.2) 
Supplementary Information The online version contains supplementary material available at 
(https://doi.org/10.1007/978-3-031-28482-3_10). 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3_10
371

372
10
Bayesian Reliability Estimation and Prediction
Furthermore, the conditional p.d.f. of bold italic thetaθ given upper X equals xX = x is 
h left parenthesis bold italic theta vertical bar x right parenthesis equals g left parenthesis x comma bold italic theta right parenthesis divided by f Superscript asterisk Baseline left parenthesis x right parenthesis periodh(θ | x) = g(x, θ)/f ∗(x).
(10.1.3) 
This conditional p.d.f. is called the posterior p.d.f. of bold italic thetaθ, given  x. Thus, starting with 
a prior p.d.f., h left parenthesis bold italic theta right parenthesish(θ), we convert it, after observing the value of x, to the posterior p.d.f. 
of bold italic thetaθ given x. 
If x 1 comma midline horizontal ellipsis comma x Subscript n Baselinex1, · · · , xn is a random sample from a distribution with a p.d.f. f left parenthesis x semicolon bold italic theta right parenthesisf (x; θ) then 
the posterior p.d.f. of bold italic thetaθ, corresponding to the prior p.d.f. h left parenthesis bold italic theta right parenthesish(θ), is  
h left parenthesis bold italic theta vertical bar bold x right parenthesis equals ContinuedFraction product Underscript i equals 1 Overscript n Endscripts f left parenthesis x Subscript i Baseline semicolon bold italic theta right parenthesis h left parenthesis bold italic theta right parenthesis Over integral midline horizontal ellipsis Underscript upper Theta Endscripts integral product Underscript i equals 1 Overscript n Endscripts f left parenthesis x Subscript i Baseline semicolon theta right parenthesis h left parenthesis bold italic theta right parenthesis normal d theta 1 midline horizontal ellipsis normal d theta Subscript k Baseline periodh(θ | x) =
n
i=1 f (xi; θ)h(θ)

· · ·

 n
i=1 f (xi; θ)h(θ) dθ1 · · · dθk
.
(10.1.4) 
For a given sample, bold xx, the posterior p.d.f. h left parenthesis bold italic theta vertical bar bold x right parenthesish(θ | x) used in most types of Bayesian 
inference. 
Example 10.1 Binomial Distributions upper X tilde upper B left parenthesis n semicolon theta right parenthesisX ∼B(n; θ), 0 less than theta less than 10 < θ < 1. 
The p.d.f. of X is 
f left parenthesis x semicolon theta right parenthesis equals StartBinomialOrMatrix n Choose x EndBinomialOrMatrix theta Superscript x Baseline left parenthesis 1 minus theta right parenthesis Superscript n minus x Baseline comma x equals 0 comma midline horizontal ellipsis comma n periodf (x; θ) =
n
x

θx(1 −θ)n−x,
x = 0, · · · , n.
Suppose that thetaθ has a prior Beta distribution, with p.d.f. 
h left parenthesis theta semicolon nu 1 comma nu 2 right parenthesis equals StartFraction 1 Over upper B left parenthesis nu 1 comma nu 2 right parenthesis EndFraction theta Superscript nu 1 minus 1 Baseline left parenthesis 1 minus theta right parenthesis Superscript nu 2 minus 1 Baseline commah(θ; ν1, ν2) =
1
B(ν1, ν2)θν1−1(1 −θ)ν2−1,
(10.1.5) 
0 less than theta less than 10 < θ < 1, 0 less than nu 10 < ν1, nu 2 less than normal infinityν2 < ∞, where upper B left parenthesis a comma b right parenthesisB(a, b) is the complete beta function 
StartLayout 1st Row 1st Column upper B left parenthesis a comma b right parenthesis 2nd Column equals integral Subscript 0 Superscript 1 Baseline x Superscript a minus 1 Baseline left parenthesis 1 minus x right parenthesis Superscript b minus 1 Baseline normal d x 2nd Row 1st Column Blank 2nd Column equals StartFraction upper Gamma left parenthesis a right parenthesis upper Gamma left parenthesis b right parenthesis Over upper Gamma left parenthesis a plus b right parenthesis EndFraction period EndLayout
B(a, b) =
 1
0
xa−1(1 −x)b−1 dx
= (a)(b)
(a + b) .
The posterior p.d.f. of thetaθ, given upper X equals xX = x, is  
h left parenthesis theta vertical bar x right parenthesis equals StartFraction 1 Over upper B left parenthesis nu 1 plus upper X comma nu 2 plus n minus upper X right parenthesis EndFraction theta Superscript nu 1 plus x minus 1 Baseline left parenthesis 1 minus theta right parenthesis Superscript nu 2 plus n minus x minus 1 Baseline comma 0 less than theta less than 1 periodh(θ | x) =
1
B(ν1 + X, ν2 + n −X)θν1+x−1(1 −θ)ν2+n−x−1,
0 < θ < 1.
(10.1.6) 
Notice that the posterior p.d.f. is also that of a Beta distribution, with parameters 
nu 1 plus xν1 + x and nu 2 plus n minus xν2 + n −x. The expected value of the posterior distribution of thetaθ, given  
upper X equals xX = x, is  
StartLayout 1st Row 1st Column upper E left brace theta vertical bar x right brace 2nd Column equals StartFraction 1 Over upper B left parenthesis nu 1 plus x comma nu 2 plus n minus x right parenthesis EndFraction integral Subscript 0 Superscript 1 Baseline theta Superscript nu 1 plus x Baseline left parenthesis 1 minus theta right parenthesis Superscript nu 2 plus n minus x minus 1 Baseline normal d theta 2nd Row 1st Column Blank 2nd Column equals StartFraction upper B left parenthesis nu 1 plus x plus 1 comma nu 2 plus n minus x right parenthesis Over upper B left parenthesis nu 1 plus x comma nu 2 plus n minus x right parenthesis EndFraction equals StartFraction nu 1 plus x Over nu 1 plus nu 2 plus n EndFraction period EndLayout
E{θ | x} =
1
B(ν1 + x, ν2 + n −x)
 1
0
θν1+x(1 −θ)ν2+n−x−1 dθ
= B(ν1 + x + 1, ν2 + n −x)
B(ν1 + x, ν2 + n −x)
=
ν1 + x
ν1 + ν2 + n.
(10.1.7)

10.1
Prior and Posterior Distributions
373
The function updateBetaMixture, available in the mistat package computes 
the posterior distribution for a mixture of betas prior distribution with proportion p. 
The inputs to this function are a mixture of beta distributions betaMixture and 
data, a list of the number of successes and number of failures in the sample. The 
mixture is characterized by the beta distributions and their mixing probabilities. 
The example below consists of 12 Binomial trials. The Beta prior ﬁrst parameter 
can be interpreted as the number of successes in 12 trials. The output of the function 
is an updated deﬁnition of the mixture—probabilities are the posterior 
mixing probabilities, and distributions the updated posterior beta densities. 
In the example below, the prior distribution of the binomial parameter, thetaθ, is  
split evenly between two Beta distributions, with expected values, 0.5 and 0.88, 
respectively. The Binomial experiment produced 10 successes in 12 trials, an 
estimated probability of success of 0.83, much closer to the second beta. 
from mistat import bayes 
betaMixture = bayes.Mixture( 
probabilities=[0.5, 0.5], 
distributions=[bayes.BetaDistribution(a=1, b=1), 
bayes.BetaDistribution(a=15, b=2)]) 
data = [10, 2] 
result = bayes.updateBetaMixture(betaMixture, data) 
thetas = [round(d.theta(), 2) for d in result.distributions] 
print(f'A posteriori: {result.probabilities}') 
print(f'Updated beta distributions:\n{result.distributions}') 
print(f'Update theta values:\n{thetas}') 
A posteriori: [0.28455284552845506, 0.7154471544715449] 
Updated beta distributions: 
[BetaDistribution(a=11, b=3), BetaDistribution(a=25, b=4)] 
Update theta values: 
[0.79, 0.86] 
A posteriori, the mix of beta distributions is 0.28 and 0.72, respectively, strongly 
favoring the second beta distribution. The expected values of the posterior beta 
distributions are 0.79 and 0.86, clearly more in line with the observed data.
black medium square■
Example 10.2 Poisson Distributions upper X tilde upper P left parenthesis lamda right parenthesisX ∼P(λ), 0 less than lamda less than normal infinity0 < λ < ∞. 
The p.d.f. of X is 
f left parenthesis x semicolon lamda right parenthesis equals e Superscript negative lamda Baseline StartFraction lamda Superscript x Baseline Over x factorial EndFraction comma x equals 0 comma 1 comma midline horizontal ellipsis periodf (x; λ) = e−λ λx
x! ,
x = 0, 1, · · · .
Suppose that the prior distribution of lamdaλ is the gamma distribution, upper G left parenthesis nu comma tau right parenthesisG(ν, τ). The prior 
p.d.f. is thus 
h left parenthesis lamda semicolon nu comma tau right parenthesis equals StartFraction 1 Over tau Superscript nu Baseline upper Gamma left parenthesis nu right parenthesis EndFraction lamda Superscript nu minus 1 Baseline e Superscript negative lamda divided by tau Baseline periodh(λ; ν, τ) =
1
τ ν(ν)λν−1e−λ/τ.
(10.1.8) 
The posterior p.d.f. of lamdaλ, given upper X equals xX = x, is

374
10
Bayesian Reliability Estimation and Prediction
h left parenthesis lamda vertical bar x right parenthesis equals StartFraction lamda Superscript nu plus x minus 1 Baseline Over left parenthesis StartFraction tau Over 1 plus tau EndFraction right parenthesis Superscript nu plus x Baseline upper Gamma left parenthesis nu plus x right parenthesis EndFraction e Superscript minus lamda left parenthesis 1 plus tau right parenthesis divided by tau Baseline periodh(λ | x) =
λν+x−1

τ
1+τ
ν+x
(ν + x)
e−λ(1+τ)/τ.
(10.1.9) 
That is, the posterior distribution of lamdaλ, given upper X equals xX = x, is upper G left parenthesis nu plus x comma StartFraction tau Over 1 plus tau EndFraction right parenthesisG(ν +x,
τ
1+τ ). The posterior 
expectation of lamdaλ, given upper X equals xX = x, is left parenthesis nu plus x right parenthesis tau divided by left parenthesis 1 plus tau right parenthesis(ν + x)τ/(1 + τ). 
The function updateGammaMixture in the mistat package, computes the 
posterior distribution of lamdaλ. The inputs to this function are similar to the inputs to 
the function updateBetaMixture described above. 
mixture = bayes.Mixture( 
probabilities=[0.5, 0.5], 
distributions=[ 
bayes.GammaDistribution(shape=1, rate=1), 
bayes.GammaDistribution(shape=15, rate=2), 
] 
) 
data = {'y': [5], 't': [1]} 
result = bayes.updateGammaMixture(mixture, data) 
print(f'A posteriori: {result.probabilities}') 
print(f'Updated beta distributions:\n{result.distributions}') 
A posteriori: [0.1250977996957064, 0.8749022003042937] 
Updated beta distributions: 
[GammaDistribution(shape=6, rate=2), GammaDistribution(shape=20, 
rate=3)] 
black medium square■
Example 10.3 Exponential Distributions upper X tilde upper E left parenthesis beta right parenthesisX ∼E(β). 
The p.d.f. of X is 
f left parenthesis x semicolon beta right parenthesis equals StartFraction 1 Over beta EndFraction e Superscript negative x divided by beta Baseline periodf (x; β) = 1
β e−x/β.
Let betaβ have an inverse-gamma prior distribution, IGleft parenthesis nu comma tau right parenthesis(ν, τ). That is, StartFraction 1 Over beta EndFraction tilde upper G left parenthesis nu comma tau right parenthesis 1
β ∼G(ν, τ). 
The prior p.d.f. is 
h left parenthesis beta semicolon nu comma tau right parenthesis equals StartFraction 1 Over tau Superscript nu Baseline upper Gamma left parenthesis nu right parenthesis beta Superscript nu plus 1 Baseline EndFraction e Superscript negative 1 divided by beta tau Baseline periodh(β; ν, τ) =
1
τ ν(ν)βν+1 e−1/βτ.
(10.1.10) 
Then, the posterior p.d.f. of betaβ, given upper X equals xX = x, is  
h left parenthesis beta vertical bar x right parenthesis equals StartFraction left parenthesis 1 plus x tau right parenthesis Superscript nu plus 1 Baseline Over tau Superscript nu plus 1 Baseline upper Gamma left parenthesis nu plus 1 right parenthesis beta Superscript nu plus 2 Baseline EndFraction e Superscript minus left parenthesis x plus 1 divided by tau right parenthesis divided by beta Baseline periodh(β | x) =
(1 + xτ)ν+1
τ ν+1(ν + 1)βν+2 e−(x+1/τ)/β.
(10.1.11) 
That is, the posterior distribution of betaβ, given  upper X equals xX = x, is IGleft parenthesis nu plus 1 comma StartFraction tau Over 1 plus x tau EndFraction right parenthesis(ν + 1,
τ
1+xτ ). The  
posterior expectation of betaβ, given upper X equals xX = x, is left parenthesis x plus 1 divided by tau right parenthesis divided by left parenthesis nu plus 1 right parenthesis(x + 1/τ)/(ν + 1).
black medium square■
The likelihood function upper L left parenthesis bold italic theta semicolon bold x right parenthesisL(θ; x) is a function over a parameter space upper Theta. In the  
deﬁnition of the posterior p.d.f. of bold italic thetaθ, given bold xx, we see that any factor of upper L left parenthesis bold italic theta semicolon bold x right parenthesisL(θ; x) which 
does not depend on bold italic thetaθ is irrelevant. For example, the binomial p.d.f., under thetaθ, is

10.2
Loss Functions and Bayes Estimators
375
f left parenthesis x semicolon theta right parenthesis equals StartBinomialOrMatrix n Choose x EndBinomialOrMatrix theta Superscript x Baseline left parenthesis 1 minus theta right parenthesis Superscript n minus x Baseline comma x equals 0 comma 1 comma midline horizontal ellipsis comma n commaf (x; θ) =
n
x

θx(1 −θ)n−x,
x = 0, 1, · · · , n,
0 less than theta less than 10 < θ < 1. The factor StartBinomialOrMatrix n Choose x EndBinomialOrMatrix
	n
x

can be omitted from the likelihood function in Bayesian 
calculations. The factor of the likelihood which depends on thetaθ is called the kernel 
of the likelihood. In the above binomial example, theta Superscript x Baseline left parenthesis 1 minus theta right parenthesis Superscript n minus xθx(1 −θ)n−x is the kernel of the 
binomial likelihood. If the prior p.d.f. of thetaθ, h left parenthesis theta right parenthesish(θ), is of the same functional form 
(up to a proportionality factor which does not depend on thetaθ) as that of the likelihood 
kernel, we call that prior p.d.f. a conjugate one. As shown in Examples 10.1 to 10.3, 
the beta prior distributions are conjugate to the binomial model, the gamma prior 
distributions are conjugate to the Poisson model and the inverse-gamma priors are 
conjugate to the exponential model. 
If a conjugate prior distribution is applied, the posterior distribution belongs to 
the conjugate family. 
One of the fundamental problems in Bayesian analysis is that of the choice of 
a prior distribution of thetaθ. From a Bayesian point of view, the prior distribution 
should reﬂect the prior knowledge of the analyst on the parameter of interest. It 
is often difﬁcult to express the prior belief about the value of bold italic thetaθ in a p.d.f. form. 
We ﬁnd that analysts apply, whenever possible, conjugate priors whose means and 
standard deviations may reﬂect the prior beliefs. Another common approach is to 
use a “diffused,” “vague” or Jeffrey’s prior, which is proportional to StartAbsoluteValue upper I left parenthesis theta right parenthesis EndAbsoluteValue Superscript 1 divided by 2|I(θ)|1/2, where 
upper I left parenthesis theta right parenthesisI(θ) is the Fisher information function (matrix). For further reading on this subject 
the reader is referred to Box and Tiao (1992), Good (2003) and Press (1989). 
10.2 
Loss Functions and Bayes Estimators 
In order to deﬁne Bayes estimators we must ﬁrst specify a loss function, upper L left parenthesis ModifyingAbove bold italic theta With caret comma bold italic theta right parenthesisL(ˆθ, θ), 
which represents the cost involved in using the estimate ModifyingAbove bold italic theta With caretˆθ when the true value is bold italic thetaθ. 
Often this loss is taken to be a function of the distance between the estimate and the 
true value, i.e., StartAbsoluteValue ModifyingAbove bold italic theta With caret minus bold italic theta EndAbsoluteValue|ˆθ −θ|. In such cases, the loss function is written as 
upper L left parenthesis ModifyingAbove bold italic theta With caret comma bold italic theta right parenthesis equals upper W left parenthesis StartAbsoluteValue ModifyingAbove bold italic theta With caret minus bold italic theta EndAbsoluteValue right parenthesis periodL(ˆθ, θ) = W(|ˆθ −θ|).
Examples of such loss functions are 
StartLayout 1st Row 1st Column Blank 2nd Column Squared hyphen error loss colon 3rd Column Blank 4th Column upper W left parenthesis StartAbsoluteValue ModifyingAbove bold italic theta With caret minus bold italic theta EndAbsoluteValue right parenthesis equals left parenthesis ModifyingAbove bold italic theta With caret minus bold italic theta right parenthesis squared comma 2nd Row 1st Column Blank 2nd Column Absolute error loss colon 3rd Column Blank 4th Column upper W left parenthesis StartAbsoluteValue ModifyingAbove bold italic theta With caret minus bold italic theta EndAbsoluteValue right parenthesis equals StartAbsoluteValue ModifyingAbove bold italic theta With caret minus bold italic theta EndAbsoluteValue period EndLayout
Squared-error loss:
W(|ˆθ −θ|) = (ˆθ −θ)2,
Absolute error loss:
W(|ˆθ −θ|) = |ˆθ −θ|.
The loss function does not have to be symmetric. For example, we may consider the 
function

376
10
Bayesian Reliability Estimation and Prediction
upper L left parenthesis ModifyingAbove theta With caret comma theta right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column alpha left parenthesis theta minus ModifyingAbove theta With caret right parenthesis comma 2nd Column if ModifyingAbove theta With caret less than or equals theta 2nd Row 1st Column beta left parenthesis ModifyingAbove theta With caret minus theta right parenthesis comma 2nd Column if ModifyingAbove theta With caret greater than theta EndLayout commaL( ˆθ, θ) =

α(θ −ˆθ),
if ˆθ ≤θ
β( ˆθ −θ),
if ˆθ > θ
,
where alphaα and betaβ are some positive constants. 
The Bayes estimator of bold italic thetaθ, with respect to a loss function upper L left parenthesis ModifyingAbove bold italic theta With caret comma bold italic theta right parenthesisL(ˆθ, θ), is deﬁned as 
the value of ModifyingAbove bold italic theta With caretˆθ which minimizes the posterior risk, given  x, where the posterior risk 
is the expected loss with respect to the posterior distribution. For example, suppose 
that the p.d.f. of X depends on several parameters theta 1 comma midline horizontal ellipsis comma theta Subscript k Baselineθ1, · · · , θk, but we wish to derive 
a Bayes estimator of theta 1θ1 with respect to the squared-error loss function. We consider 
the marginal posterior p.d.f. of theta 1θ1, given bold xx, h left parenthesis theta 1 vertical bar x right parenthesish(θ1 | x). The posterior risk is 
upper R left parenthesis ModifyingAbove theta With caret Subscript 1 Baseline comma bold x right parenthesis equals integral left parenthesis ModifyingAbove theta With caret Subscript 1 Baseline minus theta 1 right parenthesis squared h left parenthesis theta 1 vertical bar bold x right parenthesis normal d theta 1 periodR( ˆθ1, x) =

( ˆθ1 −θ1)2h(θ1 | x) dθ1.
It is easily shown that the value of ModifyingAbove theta With caret Subscript 1ˆθ1 which minimizes the posterior risk upper R left parenthesis ModifyingAbove theta With caret Subscript 1 Baseline comma bold x right parenthesisR( ˆθ1, x) is 
the posterior expectation of theta 1θ1: 
upper E left brace theta 1 vertical bar bold x right brace equals integral theta 1 h left parenthesis theta 1 vertical bar bold x right parenthesis normal d theta 1 periodE{θ1 | x} =

θ1h(θ1 | x) dθ1.
If the loss function is upper L left parenthesis ModifyingAbove theta With caret Subscript 1 Baseline comma ModifyingAbove theta With caret right parenthesis equals StartAbsoluteValue ModifyingAbove theta With caret Subscript 1 Baseline minus theta 1 EndAbsoluteValueL( ˆθ1, ˆθ) = | ˆθ1 −θ1|, the Bayes estimator of theta 1θ1 is the median 
of the posterior distribution of theta 1θ1 given bold xx. 
10.2.1 
Distribution-Free Bayes Estimator of Reliability 
Let upper J Subscript nJn denote the number of failures in a random sample of size n, during the period 
left bracket 0 comma t right parenthesis[0, t). The reliability of the device on test at age t is upper R left parenthesis t right parenthesis equals 1 minus upper F left parenthesis t right parenthesisR(t) = 1 −F(t), where upper F left parenthesis t right parenthesisF(t)
is the CDF of the life distribution. Let upper K Subscript n Baseline equals n minus upper J Subscript nKn = n −Jn. The distribution of upper K Subscript nKn is 
the binomial upper B left parenthesis n comma upper R left parenthesis t right parenthesis right parenthesisB(n, R(t)). Suppose that the prior distribution of upper R left parenthesis t right parenthesisR(t) is uniform on 
left parenthesis 0 comma 1 right parenthesis(0, 1). This prior distribution reﬂects our initial state of ignorance concerning the 
actual value of upper R left parenthesis t right parenthesisR(t). 
The uniform distribution is a special case of the Beta distribution with nu 1 equals 1ν1 = 1
and nu 2 equals 1ν2 = 1. Hence, according to Example 10.1, the posterior distribution of upper R left parenthesis t right parenthesisR(t), 
given upper K Subscript nKn, is a Beta distribution with parameters nu 1 equals upper K Subscript n Baseline plus 1ν1 = Kn + 1 and nu 2 equals 1 plus n minus upper K Subscript nν2 = 1 + n −Kn. 
Hence, the Bayes estimator of upper R left parenthesis t right parenthesisR(t), with respect to the squared-error loss function, 
is 
StartLayout 1st Row 1st Column ModifyingAbove upper R With caret left parenthesis t semicolon upper K Subscript n Baseline right parenthesis 2nd Column equals upper E left brace upper R left parenthesis t right parenthesis vertical bar upper K Subscript n Baseline right brace 2nd Row 1st Column Blank 2nd Column equals StartFraction upper K Subscript n Baseline plus 1 Over n plus 2 EndFraction period EndLayout
ˆR(t; Kn) = E{R(t) | Kn}
= Kn + 1
n + 2 .
(10.2.1)

10.2
Loss Functions and Bayes Estimators
377
If the sample size is n equals 50n = 50, and upper K 50 equals 27K50 = 27, the Bayes estimator of upper R left parenthesis t right parenthesisR(t) is 
ModifyingAbove upper R With caret left parenthesis t semicolon 27 right parenthesis equals 28 divided by 52 equals 0.538 ˆR(t; 27) = 28/52 = 0.538. Notice that the MLE of upper R left parenthesis t right parenthesisR(t) is ModifyingAbove upper R With caret Subscript 50 Baseline equals 27 divided by 50 equals 0.540 ˆR50 = 27/50 = 0.540. 
The sample size is sufﬁciently large for the MLE and the Bayes estimator to be 
numerically close. If the loss function is StartAbsoluteValue ModifyingAbove upper R With caret minus upper R EndAbsoluteValue| ˆR −R|, the Bayes estimator of R is the 
median of the posterior distribution of upper R left parenthesis t right parenthesisR(t) given upper K Subscript nKn, i.e., the median of the beta 
distribution with parameters nu 1 equals upper K Subscript n Baseline plus 1ν1 = Kn + 1 and nu 2 equals n minus upper K Subscript n Baseline plus 1ν2 = n −Kn + 1. 
Generally, if nu 1ν1 and nu 2ν2 are integers then the median of the beta distribution is 
Me equals StartFraction nu 1 upper F 0.5 left bracket 2 nu 1 comma 2 nu 2 right bracket Over nu 2 plus nu 1 upper F 0.5 left bracket 2 nu 1 comma 2 nu 2 right bracket EndFraction commaMe =
ν1F0.5[2ν1, 2ν2]
ν2 + ν1F0.5[2ν1, 2ν2],
(10.2.2) 
where upper F 0.5 left bracket j 1 comma j 2 right bracketF0.5[j1, j2] is the median of the upper F left bracket j 1 comma j 2 right bracketF[j1, j2] distribution. Substituting nu 1 equals upper K Subscript n Baseline plus 1ν1 =
Kn + 1 and nu 2 equals n minus upper K Subscript n Baseline plus 1ν2 = n −Kn + 1 in (10.2.2), we obtain that the Bayes estimator of upper R left parenthesis t right parenthesisR(t)
with respect to the absolute error loss is 
ModifyingAbove upper R With caret left parenthesis t right parenthesis equals StartFraction left parenthesis upper K Subscript n Baseline plus 1 right parenthesis upper F 0.5 left bracket 2 upper K Subscript n Baseline plus 2 comma 2 n plus 2 minus 2 upper K Subscript n Baseline right bracket Over n plus 1 minus upper K Subscript n Baseline plus left parenthesis upper K Subscript n Baseline plus 1 right parenthesis upper F 0.5 left bracket 2 upper K Subscript n Baseline plus 2 comma 2 n plus 2 minus 2 upper K Subscript n Baseline right bracket EndFraction period ˆR(t) =
(Kn + 1)F0.5[2Kn + 2, 2n + 2 −2Kn]
n + 1 −Kn + (Kn + 1)F0.5[2Kn + 2, 2n + 2 −2Kn].
(10.2.3) 
Numerically, for n equals 50n = 50, upper K Subscript n Baseline equals 27Kn = 27, upper F 0.5 left bracket 56 comma 48 right bracket equals 1.002F0.5[56, 48] = 1.002, and ModifyingAbove upper R With caret left parenthesis t right parenthesis equals 0.539 ˆR(t) = 0.539. The  
two Bayes estimates are very close. 
10.2.2 
Bayes Estimator of Reliability for Exponential Life 
Distributions 
Consider a Type II censored sample of size n from an exponential distribution, upper E left parenthesis beta right parenthesisE(β), 
with censoring at the r-th failure. Let t Subscript left parenthesis 1 right parenthesis Baseline less than or equals t Subscript left parenthesis 2 right parenthesis Baseline less than or equals midline horizontal ellipsis less than or equals t Subscript left parenthesis r right parenthesist(1) ≤t(2) ≤· · · ≤t(r) be the ordered failure 
times. For squared-error loss, the Bayes estimator of upper R left parenthesis t right parenthesis equals e Superscript negative t divided by betaR(t) = e−t/β is given by 
StartLayout 1st Row 1st Column ModifyingAbove upper R With caret left parenthesis t right parenthesis 2nd Column equals upper E left brace upper R left parenthesis t right parenthesis vertical bar t Subscript left parenthesis 1 right parenthesis Baseline comma midline horizontal ellipsis comma t Subscript left parenthesis r right parenthesis Baseline right brace 2nd Row 1st Column Blank 2nd Column equals upper E left brace e Superscript negative t divided by beta Baseline vertical bar t Subscript left parenthesis 1 right parenthesis Baseline comma midline horizontal ellipsis comma t Subscript left parenthesis r right parenthesis Baseline right brace period EndLayout
ˆR(t) = E{R(t) | t(1), · · · , t(r)}
= E{e−t/β | t(1), · · · , t(r)}.
(10.2.4) 
This conditional expectation can be computed by integrating e Superscript negative t divided by betae−t/β with respect to 
the posterior distribution of betaβ, given t Subscript left parenthesis 1 right parenthesis Baseline comma midline horizontal ellipsis comma t Subscript left parenthesis r right parenthesis Baselinet(1), · · · , t(r). 
Suppose that the prior distribution of betaβ is IGleft parenthesis nu comma tau right parenthesis(ν, τ). One can easily verify that 
the posterior distribution of betaβ given t Subscript left parenthesis 1 right parenthesis Baseline comma midline horizontal ellipsis comma t Subscript left parenthesis r right parenthesis Baselinet(1), · · · , t(r) is the inverted-gamma IGleft parenthesis nu plus r comma StartFraction tau Over 1 plus upper T Subscript n comma r Baseline tau EndFraction right parenthesis(ν +
r,
τ
1+Tn,rτ ) where upper T Subscript n comma r Baseline equals sigma summation Underscript i equals 1 Overscript r Endscripts t Subscript left parenthesis i right parenthesis Baseline plus left parenthesis n minus r right parenthesis t Subscript left parenthesis r right parenthesisTn,r = r
i=1 t(i) + (n −r)t(r). Hence, the Bayes estimator of 
upper R left parenthesis t right parenthesis equals exp left parenthesis negative t divided by beta right parenthesisR(t) = exp(−t/β) is, for squared-error loss,

378
10
Bayesian Reliability Estimation and Prediction
StartLayout 1st Row 1st Column ModifyingAbove upper R With caret left parenthesis t right parenthesis 2nd Column equals StartFraction left parenthesis 1 plus upper T Subscript n comma r Baseline tau right parenthesis Superscript r plus nu Baseline Over tau Superscript r plus nu Baseline upper Gamma left parenthesis r plus nu right parenthesis EndFraction integral Subscript 0 Superscript normal infinity Baseline StartFraction 1 Over beta Superscript r plus nu plus 1 Baseline EndFraction exp left parenthesis minus StartFraction 1 Over beta EndFraction left parenthesis upper T Subscript n comma r Baseline plus StartFraction 1 Over tau EndFraction plus t right parenthesis right parenthesis normal d beta 2nd Row 1st Column Blank 2nd Column equals left parenthesis StartFraction 1 plus upper T Subscript n comma r Baseline tau Over 1 plus left parenthesis upper T Subscript n comma r Baseline plus t right parenthesis tau EndFraction right parenthesis Superscript r plus nu Baseline period EndLayout
ˆR(t) = (1 + Tn,rτ)r+ν
τ r+ν(r + ν)
 ∞
0
1
βr+ν+1 exp

−1
β

Tn,r + 1
τ + t

dβ
=

1 + Tn,rτ
1 + (Tn,r + t)τ
r+ν
.
(10.2.5) 
Note that the estimator only depends on n through upper T Subscript n comma rTn,r. 
In the following table we provide a few values of the Bayes estimator ModifyingAbove upper R With caret left parenthesis t right parenthesis ˆR(t) for 
selected values of t, when nu equals 3ν = 3, r equals 23r = 23, upper T Subscript n comma r Baseline equals 2242Tn,r = 2242 and tau equals 10 Superscript negative 2τ = 10−2, along with 
the corresponding MLE, which is 
MLE equals e Superscript negative t divided by ModifyingAbove beta With caret Super Subscript n comma r Superscript Baseline equals e Superscript minus r t divided by upper T Super Subscript n comma r Superscript Baseline periodMLE = e−t/ ˆβn,r = e−rt/Tn,r.
t
50
100
150
200 
ModifyingAbove upper R With caret left parenthesis t right parenthesis ˆR(t)
0.577 
0.337 
0.199 
0.119 
MLE 
0.599 
0.359 
0.215 
0.129 
If we have a series structure of k modules, and the TTF of each module is 
exponentially distributed, then formula (10.2.5) is extended to 
ModifyingAbove upper R With caret Subscript s y s Baseline left parenthesis t right parenthesis equals product Underscript i equals 1 Overscript k Endscripts left parenthesis 1 minus StartFraction t tau Subscript i Baseline Over 1 plus upper T Subscript n comma r Sub Subscript i Subscript Superscript left parenthesis i right parenthesis Baseline tau Subscript i Baseline plus t tau Subscript i Baseline EndFraction right parenthesis Superscript gamma Super Subscript i Superscript plus nu Super Subscript i Superscript Baseline comma ˆRsys(t) =
k
i=1

1 −
tτi
1 + T (i)
n,riτi + tτi
γi+νi
,
(10.2.6) 
where upper T Subscript n comma r Sub Subscript i Subscript Superscript left parenthesis i right parenthesisT (i)
n,ri is the total time on test statistic for the i-th module, r Subscript iri is the censoring 
frequency of the observations on the i-th module, tau Subscript iτi and nu Subscript iνi are the prior parameters 
for the i-th module. As in (10.2.5), (10.2.6) is the Bayes estimator for the squared-
error loss, under the assumption that the MTTFs of the various modules are priorly 
independent. In a similar manner one can write a formula for the Bayes estimator of 
the reliability of a system having a parallel structure. 
10.3 
Bayesian Credibility and Prediction Intervals 
Bayesian credibility intervals at level gammaγ are intervals upper C Subscript gamma Baseline left parenthesis bold x right parenthesisCγ (x) in the parameter space 
upper Theta, for which the posterior probability that bold italic theta element of upper C Subscript gamma Baseline left parenthesis bold x right parenthesisθ ∈Cγ (x) is at least gammaγ , i.e., 
probability StartSet theta element of upper C Subscript gamma Baseline left parenthesis bold x right parenthesis vertical bar bold x EndSet greater than or equals gamma periodPr{θ ∈Cγ (x) | x} ≥γ.
(10.3.1)

10.3
Bayesian Credibility and Prediction Intervals
379
PrStartSet upper E vertical bar bold x EndSet{E | x} denotes the posterior probability of the event E, given  bold xx. The Bayesian 
credibility interval for thetaθ, given  bold xx, has an entirely different interpretation than that 
of the conﬁdence intervals discussed in the previous sections. While the conﬁdence 
level of the classical conﬁdence interval is based on the sample-to-sample variability 
of the interval, for ﬁxed thetaθ, the credibility level of the Bayesian credibility interval is 
based on the presumed variability of thetaθ, for a ﬁxed sample. 
10.3.1 
Distribution-Free Reliability Estimation 
In Sect. 10.2.1 we develop the Bayes estimator, with respect to squared-error loss, 
of the reliability at age t, upper R left parenthesis t right parenthesisR(t), when the data available are the number of sample 
units which survive at age t, namely upper K Subscript nKn. We have seen that the posterior distribution 
of upper R left parenthesis t right parenthesisR(t), given upper K Subscript nKn, for a uniform prior is the Beta distribution with nu 1 equals upper K Subscript n Baseline plus 1ν1 = Kn + 1 and 
nu 2 equals n minus upper K Subscript n Baseline plus 1ν2 = n −Kn + 1. The Bayesian credibility interval at level gammaγ is the interval whose 
limits are the epsilon 1ϵ1- and epsilon 2ϵ2-quantiles of the posterior distribution, where epsilon 1 equals left parenthesis 1 minus gamma right parenthesis divided by 2ϵ1 = (1−γ )/2, 
epsilon 2 equals left parenthesis 1 plus gamma right parenthesis divided by 2ϵ2 = (1 + γ )/2. These limits can be determined with aid of R, getting the quantile 
of the F-distribution, according to the formulae 
Lower limit equals StartFraction left parenthesis upper K Subscript n Baseline plus 1 right parenthesis Over left parenthesis upper K Subscript n Baseline plus 1 right parenthesis plus left parenthesis n minus upper K Subscript n Baseline plus 1 right parenthesis upper F Subscript epsilon 2 Baseline left bracket 2 n plus 2 minus 2 upper K Subscript n Baseline comma 2 upper K Subscript n Baseline plus 2 right bracket EndFractionLower limit =
(Kn + 1)
(Kn + 1) + (n −Kn + 1)Fϵ2[2n + 2 −2Kn, 2Kn + 2]
(10.3.2) 
and 
Upper limit equals StartFraction left parenthesis upper K Subscript n Baseline plus 1 right parenthesis upper F Subscript epsilon 2 Baseline left bracket 2 upper K Subscript n Baseline plus 2 comma 2 n plus 2 minus 2 upper K Subscript n Baseline right bracket Over left parenthesis n minus upper K Subscript n Baseline plus 1 right parenthesis plus left parenthesis upper K Subscript n Baseline plus 1 right parenthesis upper F Subscript epsilon 2 Baseline left bracket 2 upper K Subscript n Baseline plus 2 comma 2 n plus 2 minus 2 upper K Subscript n Baseline right bracket EndFraction periodUpper limit =
(Kn + 1)Fϵ2[2Kn + 2, 2n + 2 −2Kn]
(n −Kn + 1) + (Kn + 1)Fϵ2[2Kn + 2, 2n + 2 −2Kn].
(10.3.3) 
In Sect. 10.2.1 we considered the case of n equals 50n = 50 and upper K Subscript n Baseline equals 27Kn = 27. For  gamma equals 0.95γ = 0.95 we 
need 
upper F 0.975 left bracket 48 comma 56 right bracket equals 1.725F0.975[48, 56] = 1.725
and 
upper F 0.975 left bracket 56 comma 48 right bracket equals 1.746 periodF0.975[56, 48] = 1.746.
Thus, the Bayesian credibility limits obtained for upper R left parenthesis t right parenthesisR(t) are 0.403 and 0.671. Recall 
the Bayes estimator was 0.538.

380
10
Bayesian Reliability Estimation and Prediction
10.3.2 
Exponential Reliability Estimation 
In Sect. 10.2.2 we develop a formula for the Bayes estimator of the reliability 
function upper R left parenthesis t right parenthesis equals exp left parenthesis negative t divided by beta right parenthesisR(t) = exp(−t/β) for Type II censored data. We saw that if the prior on betaβ
is IGleft parenthesis nu comma tau right parenthesis(ν, τ) then the posterior distribution of betaβ, given the data, is IGleft parenthesis nu plus r comma tau divided by left parenthesis 1 plus tau upper T Subscript n comma r Baseline right parenthesis(ν + r, τ/(1 +
τTn,r). Thus, gammaγ level Bayes credibility limits for betaβ are given by beta Subscript upper L comma gammaβL,γ (lower limit) 
and upper B Subscript upper U comma gammaBU,γ (upper limit), where 
beta Subscript upper L comma gamma Baseline equals StartFraction upper T Subscript n comma r Baseline plus 1 divided by tau Over upper G Subscript epsilon 2 Baseline left parenthesis nu plus r comma 1 right parenthesis EndFractionβL,γ =
Tn,r + 1/τ
Gϵ2(ν + r, 1)
(10.3.4) 
and 
beta Subscript upper U comma gamma Baseline equals StartFraction upper T Subscript n comma r Baseline plus 1 divided by tau Over upper G Subscript epsilon 1 Baseline left parenthesis nu plus r comma 1 right parenthesis EndFraction periodβU,γ =
Tn,r + 1/τ
Gϵ1(ν + r, 1).
(10.3.5) 
Moreover, if nuν is an integer then we can replace upper G Subscript p Baseline left parenthesis nu plus r comma 1 right parenthesisGp(ν + r, 1) by one half chi Subscript p Superscript 2 Baseline left bracket 2 nu plus 2 r right bracket 1
2χ2
p[2ν + 2r]. 
Finally, since upper R left parenthesis t right parenthesis equals exp left parenthesis negative t divided by beta right parenthesisR(t) = exp(−t/β) is an increasing function of betaβ, the  gammaγ -level Bayes 
credibility limits for upper R left parenthesis t right parenthesisR(t) are 
upper R Subscript upper L comma gamma Baseline left parenthesis t right parenthesis equals exp left parenthesis negative t divided by beta Subscript upper L comma gamma Baseline right parenthesisRL,γ (t) = exp(−t/βL,γ )
(10.3.6) 
and 
upper R Subscript upper U comma gamma Baseline left parenthesis t right parenthesis equals exp left parenthesis negative t divided by beta Subscript upper U comma gamma Baseline right parenthesis periodRU,γ (t) = exp(−t/βU,γ ).
(10.3.7) 
If we consider the values nu equals 3ν = 3, r equals 23r = 23, upper T Subscript n comma r Baseline equals 2242Tn,r = 2242, and tau equals 10 Superscript negative 2τ = 10−2 we need 
for gamma equals 0.95γ = 0.95, chi 0.025 squared left bracket 52 right bracket equals 33.53χ2
0.025[52] = 33.53 and chi 0.975 squared left bracket 52 right bracket equals 73.31χ2
0.975[52] = 73.31. Thus, 
beta Subscript upper L comma 0.95 Baseline equals 63.91 and beta Subscript upper U comma 0.95 Baseline equals 139.73 periodβL,0.95 = 63.91
and
βU,0.95 = 139.73.
The corresponding Bayesian credibility limits for upper R left parenthesis t right parenthesisR(t), at t equals 50t = 50, are upper R Subscript upper L comma 0.95 Baseline left parenthesis 50 right parenthesis equals 0.457RL,0.95(50) =
0.457 and upper R Subscript upper U comma 0.95 Baseline left parenthesis 50 right parenthesis equals 0.699RU,0.95(50) = 0.699. 
10.3.3 
Prediction Intervals 
In Sect. 10.3.2 we introduce the notion of prediction intervals of level gammaγ . This notion 
can be adapted to the Bayesian framework in the following manner. 
Let bold upper XX be a sample from a distribution governed by a parameter thetaθ; we assume that 
thetaθ has a prior distribution. Let h left parenthesis theta vertical bar bold x right parenthesish(θ | x) denote the posterior p.d.f. of thetaθ, given bold upper X equals bold xX = x. 
bold xx represents the values of a random sample already observed. We are interested 
in predicting the value of some statistic upper T left parenthesis bold upper Y right parenthesisT (Y) based on a future sample bold upper YY from the

10.3
Bayesian Credibility and Prediction Intervals
381
same distribution. Let g left parenthesis t semicolon theta right parenthesisg(t; θ) denote the p.d.f. of upper T left parenthesis bold upper Y right parenthesisT (Y) under thetaθ. Then the predictive 
distribution of upper T left parenthesis bold upper Y right parenthesisT (Y), given bold xx, is  
g Superscript asterisk Baseline left parenthesis t vertical bar bold x right parenthesis equals integral Underscript upper Theta Endscripts g left parenthesis t semicolon theta right parenthesis h left parenthesis theta vertical bar bold x right parenthesis normal d theta periodg∗(t | x) =


g(t; θ)h(θ | x) dθ.
(10.3.8) 
A Bayesian prediction interval of level gammaγ for upper T left parenthesis upper Y right parenthesisT (Y), given  bold xx, is an interval 
left parenthesis upper T Subscript upper L Baseline left parenthesis bold x right parenthesis comma upper T Subscript upper U Baseline left parenthesis bold x right parenthesis right parenthesis(TL(x), TU(x)) which contains a proportion gammaγ of the predictive distribution, i.e., 
satisfying 
integral Subscript upper T Subscript upper L Baseline left parenthesis bold x right parenthesis Superscript upper T Subscript upper U Baseline left parenthesis bold x right parenthesis Baseline g Superscript asterisk Baseline left parenthesis t vertical bar bold x right parenthesis normal d t equals gamma period
 TU (x)
TL(x)
g∗(t | x) dt = γ.
(10.3.9) 
Generally, the limits are chosen so that the tail areas are each left parenthesis 1 minus gamma divided by 2 right parenthesis(1−γ/2). We illustrate 
the derivation of a Bayesian prediction interval in the following example. 
Example 10.4 Consider a device with an exponential lifetime distribution upper E left parenthesis beta right parenthesisE(β). We  
test a random sample of n of these, stopping at the r-th failure. Suppose the prior 
distribution of betaβ is IGleft parenthesis nu comma tau right parenthesis(ν, τ). Then, as seen in Sect. 10.2.2, the posterior distribution 
of betaβ given the ordered failure times t Subscript left parenthesis 1 right parenthesis Baseline comma midline horizontal ellipsis comma t Subscript left parenthesis r right parenthesis Baselinet(1), · · · , t(r) is IGleft parenthesis nu plus r comma StartFraction tau Over 1 plus upper T Subscript n comma r Baseline tau EndFraction right parenthesis(ν +r,
τ
1+Tn,rτ ), where upper T Subscript n comma r Baseline equals sigma summation Underscript i equals 1 Overscript r Endscripts t Subscript left parenthesis i right parenthesis Baseline plus left parenthesis n minus r right parenthesis t Subscript left parenthesis r right parenthesisTn,r =
r
i=1 t(i) + (n −r)t(r). 
Suppose we have an additional s such devices, to be used one at a time in some 
system, replacing each one immediately upon failure by another. We are interested 
in a prediction interval of level gammaγ for T , the time until all s devices have been used 
up. Letting bold upper Y equals left parenthesis upper Y 1 comma midline horizontal ellipsis comma upper Y Subscript s Baseline right parenthesisY = (Y1, · · · , Ys) be the lifetimes of the devices, we have upper T left parenthesis bold upper Y right parenthesis equals sigma summation Underscript i equals 1 Overscript s Endscripts upper Y Subscript iT (Y) =
s
i=1 Yi. Thus, upper T left parenthesis bold y right parenthesisT (y) has a upper G left parenthesis s comma beta right parenthesisG(s, β) distribution. Substituting in (10.3.8), it is easily  
shown that the predictive p.d.f. of upper T left parenthesis bold upper Y right parenthesisT (Y), given t Subscript left parenthesis 1 right parenthesis Baseline comma midline horizontal ellipsis comma t Subscript left parenthesis r right parenthesis Baselinet(1), · · · , t(r), is  
StartLayout 1st Row 1st Column g Superscript asterisk Baseline left parenthesis t vertical bar t Subscript left parenthesis 1 right parenthesis Baseline comma midline horizontal ellipsis comma t Subscript left parenthesis r right parenthesis Baseline right parenthesis 2nd Column equals left parenthesis upper B left parenthesis s comma nu plus r right parenthesis left parenthesis upper T Subscript n comma r Baseline plus 1 divided by tau right parenthesis right parenthesis Superscript negative 1 Baseline 2nd Row 1st Column Blank 2nd Column dot left parenthesis StartFraction tau Over t plus upper T Subscript n comma r Baseline plus 1 divided by tau EndFraction right parenthesis Superscript s minus 1 Baseline left parenthesis StartFraction upper T Subscript n comma r Baseline plus 1 divided by tau Over t plus upper T Subscript n comma r Baseline plus 1 divided by tau EndFraction right parenthesis Superscript r plus nu plus 1 Baseline period EndLayoutg∗(t | t(1), · · · , t(r)) =
	
B(s, ν + r)(Tn,r + 1/τ)

−1
·

τ
t + Tn,r + 1/τ
s−1 
Tn,r + 1/τ
t + Tn,r + 1/τ
r+ν+1
.
(10.3.10) 
Making the transformation 
upper U equals left parenthesis upper T Subscript n comma r Baseline plus 1 divided by tau right parenthesis divided by left parenthesis upper T left parenthesis bold upper Y right parenthesis plus upper T Subscript n comma r Baseline plus 1 divided by tau right parenthesisU = (Tn,r + 1/τ)/(T (Y) + Tn,r + 1/τ)
one can show that the predictive distribution of U given t Subscript left parenthesis 1 right parenthesis Baseline comma midline horizontal ellipsis comma t Subscript left parenthesis r right parenthesis Baselinet(1), · · · , t(r) is the Betaleft parenthesis r plus nu comma s right parenthesis(r +
ν, s) distribution. If we let BeSubscript epsilon 1 Baseline left parenthesis r plus nu comma s right parenthesisϵ1(r + ν, s) and BeSubscript epsilon 2 Baseline left parenthesis r plus nu comma s right parenthesisϵ2(r + ν, s) be the epsilon 1ϵ1—and epsilon 2ϵ2-
quantiles of Beta left parenthesis r plus nu comma s right parenthesis(r + ν, s), where epsilon 1 equals left parenthesis 1 minus gamma right parenthesis divided by 2ϵ1 = (1 −γ )/2 and epsilon 2 equals left parenthesis 1 plus gamma right parenthesis divided by 2ϵ2 = (1 + γ )/2, then the 
lower and upper Bayesian prediction limits for upper T left parenthesis bold upper Y right parenthesisT (Y) are 
upper T Subscript upper L Baseline equals left parenthesis upper T Subscript n comma r Baseline plus StartFraction 1 Over tau EndFraction right parenthesis left parenthesis StartFraction 1 Over Be Subscript epsilon 2 Baseline left parenthesis nu plus r comma s right parenthesis EndFraction minus 1 right parenthesisTL =

Tn,r + 1
τ
 
1
Beϵ2(ν + r, s) −1

(10.3.11)

382
10
Bayesian Reliability Estimation and Prediction
and 
upper T Subscript upper U Baseline equals left parenthesis upper T Subscript n comma r Baseline plus StartFraction 1 Over tau EndFraction right parenthesis left parenthesis StartFraction 1 Over Be Subscript epsilon 1 Baseline left parenthesis nu plus r comma s right parenthesis EndFraction minus 1 right parenthesis periodTU =

Tn,r + 1
τ
 
1
Beϵ1(ν + r, s) −1

.
(10.3.12) 
If nuν is an integer, the prediction limits can be expressed as 
upper T Subscript upper L Baseline equals left parenthesis upper T Subscript n comma r Baseline plus StartFraction 1 Over tau EndFraction right parenthesis StartFraction s Over nu plus r EndFraction upper F Subscript epsilon 1 Baseline left bracket 2 s comma 2 nu plus 2 r right bracketTL =

Tn,r + 1
τ

s
ν + r Fϵ1[2s, 2ν + 2r]
(10.3.13) 
and 
upper T Subscript upper U Baseline equals left parenthesis upper T Subscript n comma r Baseline plus StartFraction 1 Over tau EndFraction right parenthesis StartFraction s Over nu plus r EndFraction upper F Subscript epsilon 2 Baseline left bracket 2 s comma 2 nu plus 2 r right bracket periodTU =

Tn,r + 1
τ

s
ν + r Fϵ2[2s, 2ν + 2r].
Formulae (10.3.12) and (10.3.13) are applied in the following example: 
Twenty computer monitors are put on test starting at time t 0 equals 0t0 = 0. The test is 
terminated at the sixth failure left parenthesis r equals 6 right parenthesis(r = 6). The total time on test was upper T Subscript 20 comma 6 Baseline equals 75,805.6T20,6 = 75,805.6
[hr]. We wish to predict the time till failure [hr] of monitors which are shipped 
to customers. Assuming that TTF tilde upper E left parenthesis beta right parenthesis∼E(β) and ascribing betaβ a prior  IGleft parenthesis 5 comma 10 Superscript negative 3 Baseline right parenthesis(5, 10−3)
distribution, we compute the prediction limits upper T Subscript upper LTL and upper T Subscript upper UTU for s equals 1s = 1, at level  
gamma equals 0.95γ = 0.95. 
In this case 2 nu plus 2 r equals 222ν + 2r = 22 and upper F 0.025 left bracket 2 comma 22 right bracket equals 1 divided by upper F 0.975 left bracket 22 comma 2 right bracket equals 1 divided by 39.45 equals 0.0253F0.025[2, 22] = 1/F0.975[22, 2] = 1/39.45 =
0.0253. Moreover, upper F 0.975 left bracket 2 comma 22 right bracket equals 4.38F0.975[2, 22] = 4.38. Thus, 
upper T Subscript upper L Baseline equals 76805.6 and one eleventh times 0.0253 equals 176.7 left bracket hr right bracketTL = 76805.6 1
11 × 0.0253 = 176.7 [hr]
and 
upper T Subscript upper U Baseline equals 76805.6 and one eleventh times 4.38 equals 30 comma 582.6 left bracket hr right bracket periodTU = 76805.6 1
11 × 4.38 = 30, 582.6 [hr].
We have high conﬁdence that a monitor in the ﬁeld will not fail before 175 h of 
operation.
black medium square■
10.3.4 
Applications with Python: Lifelines and pymc 
In Sect. 9.7.2 we discussed the maximum likelihood estimation of the scale param-
eter betaβ and the shape parameter nuν of the Weibull life distribution. We saw that the 
estimation of the scale parameter, when the shape parameter is known, is straight 
forward and simple. On the other hand, when the shape parameter is unknown, the 
estimation requires an iterative solution which converges to the correct estimates. 
The analysis there was done for uncensored data. In reliability life testing we often

10.3
Bayesian Credibility and Prediction Intervals
383
encounter right censored data, when at termination of the study some systems have 
not failed. With censored data, the likelihood function has to be modiﬁed, and the 
maximum likelihood estimation is more complicated. 
In this section we show how to use Python, in order to derive an analysis 
of right censored data. We apply methods described in Chaps. 9 and 10 to the 
dataset SYSTEMFAILURE.csv. The dataset SYSTEMFAILURE.csv consists of 
208 observations on systems operating at 90 geographically dispersed sites. Twelve 
systems are newly installed and are labeled as “Young.” All the other systems are 
labeled “Mature.” Out of the 208 observations, 68 (33%) report time stamps of a 
failure (uncensored). The other observations are censored, as indicated by the value 
1 in the censor variable column. We are interested in the estimated failure rate at 
3,000,000 time units of operation. 
A measure of time, the time stamp, is recorded for each observation in the data. 
This variable is presented in operational units (activity time), at time of observation. 
The bigger the time, the longer the system performed. The observations with a value 
0 of the censor variable, represent length of operation till failure of the systems. 
Before we build models, we preprocess the data. To avoid numerical problems 
due to the large values of the time unit, we divide the time stamp by 1,000,000. 
systemFailure = mistat.load_data('SYSTEMFAILURE') 
systemFailure['Time stamp'] = systemFailure['Time stamp'] / 1_000_000 
systemFailure['Young'] = [0 if v == 'Mature' else 1 
for v in systemFailure['System Maturity']] 
systemFailure = systemFailure[['Time stamp', 'Censor', 'Young']] 
systemFailure.head() 
Time stamp 
Censor 
Young 
0
1.574153
1
0 
1
2.261043
0
0 
2
1.726097
1
0 
3
1.178089
1
0 
4
1.354856
1
0 
As a ﬁrst analysis we ﬁt a parametric Weibull model using lifelines. 
kmf = lifelines.WeibullFitter() 
kmf.fit(systemFailure['Time stamp'], event_observed=systemFailure['Censor']==0) 
print(f'Scale: {kmf.lambda_:.3f}') 
print(f'Shape: {kmf.rho_:.4f}') 
Scale: 4.351 
Shape: 0.7226 
The estimated Weibull scale and shape parameters are 4.351 and 0.723, respec-
tively. Note that notation in the lifelines package are lambda_ for the scale betaβ
and rho_ for the shape nuν parameters of the Weibull distribution upper W left parenthesis nu comma beta right parenthesisW(ν, β). 
In Fig. 10.1 we see the estimated failure rate together with its conﬁdence region. 
The proﬁle shows an estimate of the probability that a system fails before the 
designated 3,000,000 time units. At this designated time, the estimated probability 
is P equals= 53.4%. This estimator has a 95% conﬁdence interval of (44.1%–63.2%).

384
10
Bayesian Reliability Estimation and Prediction
Fig. 10.1 Distribution proﬁle of parametric Weibull model ﬁtted to system failure data 
failureRate = 1 - kmf.predict(3_000_000 / 1_000_000) 
ciCumDensity = kmf.confidence_interval_cumulative_density_ 
fr_low, fr_high = ciCumDensity[ciCumDensity.index > 3].iloc[0,] 
print('Parameteric model:') 
print(f'Mean of failure rates at 3,000,000: {failureRate:.3f}') 
print(f'95%-confidence interval at 3,000,000: [{fr_low:.3f}, {fr_high:.3f}]') 
Parameteric model: 
Mean of failure rates at 3,000,000: 0.534 
95%-confidence interval at 3,000,000: [0.441, 0.632] 
An alternative to the parameteric ﬁt, is to use Bayesian inference. The pymc 
package is a Python package that uses advanced Markov chain Monte Carlo 
(MCMC) algorithms for Bayesian statistical modeling and probabilistic machine 
learning. 
We describe the likelihood for the observed data using a Weibull distribution 
upper W left parenthesis nu comma beta right parenthesisW(ν, β). The Weibull distribution is implemented in pymc using pm.Weibull (the 
argument alpha represents the shape parameter nuν and beta the scale betaβ). To 
include the censored data, we require the logarithm of the Weibull survival function 
deﬁned as: 
upper W Subscript s u r v i v a l Baseline left parenthesis t semicolon nu comma beta right parenthesis equals e Superscript minus left parenthesis StartFraction t Over beta EndFraction right parenthesis Super Superscript nu Superscript Baseline periodWsurvival(t; ν, β) = e
−

t
β
ν
.
def weibull_log_sf(y, nu, beta): 
return - T.exp(nu * T.log(y / beta))

10.3
Bayesian Credibility and Prediction Intervals
385
Using this function, the pymc model is deﬁned as: 
# extract observed and censored observations a numpy arrays 
censored = systemFailure['Censor'].values == 1 
y_observed = systemFailure['Time stamp'][~censored].values 
y_censored = systemFailure['Time stamp'][censored].values 
with pm.Model() as weibull_model_uninformative: 
beta = pm.Uniform('beta', lower=0.5, upper=10) # scale 
nu = pm.Uniform('nu', lower=0.5, upper=1.1) # shape 
y_obs = pm.Weibull('y_obs', alpha=nu, beta=beta, observed=y_observed) 
y_cens = pm.Potential('y_cens', weibull_log_sf(y_censored, nu, beta)) 
The model ﬁrst deﬁnes the prior distributions for the beta and nu parameters 
of the Weibull distribution. This is then associated with the observed failures. The 
likelihood of the uncensored data is combined with the likelihood, the logarithm of 
the survival function, of the censored failure information. 
The pymc package uses Hamiltonian Monte Carlo with a no U-turn sampler 
(NUTS) to explore the left parenthesis beta comma nu right parenthesis(β, ν) parameter space with respect to the likelihood 
information. For details see Salvatier et al. (2016). The model is trained using the 
sample function. 
with weibull_model_uninformative: 
trace_uninformative = pm.sample(1000, random_seed=123, progressbar=False, 
return_inferencedata=True) 
Auto-assigning NUTS sampler... 
Initializing NUTS using jitter+adapt_diag... 
Multiprocess sampling (4 chains in 4 jobs) 
NUTS: [beta, nu] 
Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 
4_000 draws total) took 3 seconds. 
with weibull_model_uninformative: 
az.plot_trace(trace_uninformative) 
plt.tight_layout() 
There are a number of diagnostic plots used to analyze the model training. The 
trace plot shows the resulting posterior distribution and the change of the parameters 
betaβ and nuν in the chains. The trace plot in Fig. 10.2 shows no unusual behavior. 
Fig. 10.2 Distribution proﬁle of parametric Weibull model ﬁtted to system failure data

386
10
Bayesian Reliability Estimation and Prediction
Fig. 10.3 Parameter values sampled from prior and posterior distribution of a Weibull model ﬁtted 
to system failure data using uninformative priors 
It is also interesting to compare the prior and posterior distributions of the model 
parameters. Figure 10.3 shows that the uniform prior distribution is uninformative 
and not biased toward the actual values. If domain knowledge is available, the 
uninformative priors can be replaced with biased, informative priors. The following 
model replaces the uniform distribution with gamma distributions centered around 
the area of high probability of the parameters. 
with pm.Model() as weibull_model_informative: 
beta = pm.Gamma('beta', alpha=4.5 * 7, beta=7) # scale 
nu = pm.Gamma('nu', alpha=0.7 * 100, beta=100) #shape 
y_obs = pm.Weibull('y_obs', alpha=nu, beta=beta, observed=y_observed) 
y_cens = pm.Potential('y_cens', weibull_log_sf(y_censored, nu, beta)) 
trace_informative = pm.sample(1000, random_seed=123, progressbar=False, 
return_inferencedata=True) 
Auto-assigning NUTS sampler... 
Initializing NUTS using jitter+adapt_diag... 
Multiprocess sampling (4 chains in 4 jobs) 
NUTS: [beta, nu] 
Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 
4_000 draws total) took 2 seconds. 
Figure 10.4 shows that using informative priors leads to a more thorough 
sampling of the area of higher likelihood, leading to a tighter distribution of the 
parameter values. In both cases, the resulting 
We can now use the Bayesian model to derive estimates for the probability of 
system failure at 3,000,000 time units. In order to make our analysis more concise 
we ﬁrst deﬁne two utility functions. The function sampleFailureRates uses the 
model and the training trace to sample pairs of nuν and betaβ parameters from the posterior 
distribution. The values are used with the weibull_min distribution from scipy to 
derive failure rate curves for time points in the range 0 to 10.

10.3
Bayesian Credibility and Prediction Intervals
387
Fig. 10.4 Distribution of sampled parameter values using models deﬁned with uninformative and 
informative priors 
def sampleFailureRates(model, model_trace, nu_var='nu', beta_var='beta'): 
t_plot = np.linspace(0, 10, 101) 
t_plot[0] = 0.00001 
with model: 
pp_trace = pm.sample_posterior_predictive(model_trace, 
var_names=[nu_var, beta_var], progressbar=False, random_seed=123) 
sampled = az.extract_dataset(pp_trace.posterior_predictive) 
curves = [stats.weibull_min.cdf(t_plot, nu, scale=beta) 
for beta, nu in zip(sampled[beta_var], sampled[nu_var])] 
curves = np.array(curves) 
return {'time': t_plot, 'failure_rates': curves, 
'mean_failure_rate': np.mean(curves, axis=0)} 
Using the ensemble of failure rate curves, we derive the mean and 95%-highest 
posterior density (HPD) interval at a given time using the function estimated-
FailureRate. 
def estimatedFailureRate(failureRates, at_time=3.0): 
idx = np.argmax(failureRates['time'] >= at_time) 
curves = failureRates['failure_rates'] 
failure_rate = np.mean(curves, axis=0)[idx] 
print(f'Mean of failure rates at 3,000,000: {failure_rate:.3f}') 
hdi_95 = az.hdi(np.array(curves), hdi_prob=0.95)[idx,:] 
print(f'95%-HPD at 3,000,000: {hdi_95.round(3)}') 
return failure_rate 
We now apply these utility functions to our two models. 
sampledCurves = sampleFailureRates(weibull_model_uninformative, 
trace_uninformative) 
_ = estimatedFailureRate(sampledCurves) 
sampledCurvesInformative = sampleFailureRates(weibull_model_informative, 
trace_informative) 
_ = estimatedFailureRate(sampledCurvesInformative) 
Sampling: [beta, nu] 
Sampling: [beta, nu] 
Mean of failure rates at 3,000,000: 0.530 
95%-HPD at 3,000,000: [0.247 0.907] 
Mean of failure rates at 3,000,000: 0.534 
95%-HPD at 3,000,000: [0.446 0.625]

388
10
Bayesian Reliability Estimation and Prediction
Table 10.1 Estimated failure rates and 95%-HPD at 3,000,000 time units 
Model
Failure rate 
95%-CI or 95%-HPD 
Parametric estimate 
53.4%
[44.1%–63.2%] 
Uninformative priors 
52.0%
[42.2%–61.4%] 
Informative priors
53.1%
[46.8%–59.0%] 
The results are summarized in Table 10.1. We can see that the mean failure rate 
estimates of all three models are comparable. The uniform prior is reﬂecting the 
lack of prior information. It reduced the point estimate. The informative prior is 
a more pointed prior and this produced a predictive value equal to the parametric 
point estimate with a shorter HPD than the corresponding conﬁdence interval. Note 
that the interpretation of the HPD is probabilistic, making assertions on the posterior 
failure rate probabilities while the conﬁdence interval refers to a statement on the 
long-term coverage of the intervals constructed by this method. 
To conclude Chaps. 9 and 10 on reliability, we refer again to SYSTEMFAIL-
URE.csv and assess the impact on reliability of the maturity level of the system. As 
mentioned, we have 12 systems labeled as “Young,” are they behaving differently? 
We extend the model with the vague, uninformative priors to estimate the 
parameters for both mature and young systems at the same time. The trace of the 
sampling process is shown in Fig. 10.5. 
# split the events both by their censored and young status 
censored = systemFailure['Censor'].values == 1 
young = systemFailure['Young'].values == 1 
y_observed = systemFailure['Time stamp'][~censored & ~young].values 
y_censored = systemFailure['Time stamp'][censored & ~young].values 
y_observed_young = systemFailure['Time stamp'][~censored & young].values 
y_censored_young = systemFailure['Time stamp'][censored & young].values 
with pm.Model() as weibull_model_maturity: 
beta = pm.Uniform('beta', lower=0.5, upper=10) # scale 
nu = pm.Uniform('nu', lower=0.5, upper=1.1) # shape 
beta_young = pm.Uniform('beta_young', lower=0, upper=10) # scale 
nu_young = pm.Uniform('nu_young', lower=0, upper=1.1) # shape 
y_obs = pm.Weibull('y_obs', alpha=nu, beta=beta, observed=y_observed) 
y_cens = pm.Potential('y_cens', weibull_log_sf(y_censored, nu, beta)) 
y_obs_young = pm.Weibull('y_obs_young', alpha=nu_young, beta=beta_young, 
observed=y_observed_young) 
y_cens_young = pm.Potential('y_cens_young', weibull_log_sf(y_censored_young, 
nu_young, beta_young)) 
trace_maturity = pm.sample(2000, tune=3500, random_seed=123, 
progressbar=False, return_inferencedata=True)

10.3
Bayesian Credibility and Prediction Intervals
389
Fig. 10.5 Trace of sampling of Weibull model ﬁtted to system failure data separating between 
young and mature systems. Observed events are overlaid in circles (mature) and squares (young) 
Auto-assigning NUTS sampler... 
Initializing NUTS using jitter+adapt_diag... 
Multiprocess sampling (4 chains in 4 jobs) 
NUTS: [beta, nu, beta_young, nu_young] 
Sampling 4 chains for 3_500 tune and 2_000 draw iterations (14_000 + 
8_000 draws total) took 11 seconds. 
We see a difference between the mature and young systems. The estimated 
failure rates and their 95% HPD intervals are shown in Fig. 10.6. As expected, 
mature systems have lower failure rates. This is often experienced in new product 
introduction with early deployment problems affecting the young systems. The 
failure rates at 3,000,000 time units are: 
Failure rate 
95%-HPD 
Mature systems 
51.8%
[42.3%–61.4%] 
Young system
64.7%
[43.0%–94.6%] 
The uncertainty is very wide for the young systems, due to the small number of data 
points.

390
10
Bayesian Reliability Estimation and Prediction
Fig. 10.6 Bayesian estimated failure rates 95% HPD intervals for mature (dark grey, solid line) 
and young (light grey, dotted line) systems 
10.4 
Credibility Intervals for the Asymptotic Availability of 
Repairable Systems: The Exponential Case 
Consider a repairable system. We take observations on n consecutive renewal cycles. 
It is assumed that in each renewal cycle, TTF tilde upper E left parenthesis beta right parenthesis∼E(β) and TTR tilde upper E left parenthesis gamma right parenthesis∼E(γ ). Let  
t 1 comma midline horizontal ellipsis comma t Subscript n Baselinet1, · · · , tn be the values of TTF in the n cycles and s 1 comma midline horizontal ellipsis comma s Subscript n Baselines1, · · · , sn be the values of TTR. 
One can readily verify that the likelihood function of betaβ depends on the statistic 
upper U equals sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript iU = n
i=1 ti and that of gammaγ depends on upper V equals sigma summation Underscript i equals 1 Overscript n Endscripts s Subscript iV = n
i=1 si. U and V are called the 
likelihood (or minimal sufﬁcient) statistics. Let  lamda equals 1 divided by betaλ = 1/β and mu equals 1 divided by gammaμ = 1/γ . The  
asymptotic availability is upper A Subscript normal infinity Baseline equals mu divided by left parenthesis mu plus lamda right parenthesisA∞= μ/(μ + λ). 
In the Bayesian framework we assume that lamdaλ and muμ are priorly independent, 
having prior gamma distributions upper G left parenthesis nu comma tau right parenthesisG(ν, τ) and upper G left parenthesis omega comma zeta right parenthesisG(ω, ζ), respectively. One can verify 
that the posterior distributions of lamdaλ and muμ, given  U and V , are  upper G left parenthesis n plus nu comma upper U plus tau right parenthesisG(n + ν, U + τ)
and upper G left parenthesis n plus omega comma upper V plus zeta right parenthesisG(n + ω, V + ζ), respectively. Moreover, lamdaλ and muμ are posteriorly independent. 
Routine calculations yield that 
StartStartFraction StartFraction 1 minus upper A Subscript normal infinity Baseline Over upper A Subscript normal infinity Baseline EndFraction left parenthesis upper U plus tau right parenthesis OverOver StartFraction 1 minus upper A Subscript normal infinity Baseline Over upper A Subscript normal infinity Baseline EndFraction left parenthesis upper U plus tau right parenthesis plus left parenthesis upper V plus zeta right parenthesis EndEndFraction tilde Beta left parenthesis n plus nu comma n plus omega right parenthesis comma
1−A∞
A∞(U + τ)
1−A∞
A∞(U + τ) + (V + ζ)
∼Beta(n + ν, n + ω),
where Betaleft parenthesis p comma q right parenthesis(p, q) denotes a random variable having a Beta distribution, with 
parameters p and q, 0 less than p comma q less than normal infinity0 < p, q < ∞. Let  epsilon 1 equals left parenthesis 1 minus gamma right parenthesis divided by 2ϵ1 = (1 −γ )/2 and epsilon 2 equals left parenthesis 1 plus gamma right parenthesis divided by 2ϵ2 = (1 + γ )/2. 
We obtain that the lower and upper limits of the gammaγ -level credibility interval for upper A Subscript normal infinityA∞
are upper A Subscript normal infinity comma epsilon 1A∞,ϵ1 and upper A Subscript normal infinity comma epsilon 2A∞,ϵ2 where

10.4
Credibility Intervals for the Asymptotic Availability of Repairable...
391
upper A Subscript normal infinity comma epsilon 1 Baseline equals left bracket 1 plus StartFraction upper V plus zeta Over upper U plus tau EndFraction dot StartFraction Be Subscript epsilon 2 Baseline left parenthesis n plus nu comma n plus omega right parenthesis Over Be Subscript epsilon 1 Baseline left parenthesis n plus omega comma n plus nu right parenthesis EndFraction right bracket Superscript negative 1A∞,ϵ1 =

1 + V + ζ
U + τ · Beϵ2(n + ν, n + ω)
Beϵ1(n + ω, n + ν)
−1
(10.4.1) 
and 
upper A Subscript normal infinity comma epsilon 2 Baseline equals left bracket 1 plus StartFraction upper V plus zeta Over upper U plus tau EndFraction dot StartFraction Be Subscript epsilon 1 Baseline left parenthesis n plus nu comma n plus omega right parenthesis Over Be Subscript epsilon 2 Baseline left parenthesis n plus omega comma n plus nu right parenthesis EndFraction right bracket Superscript negative 1 Baseline commaA∞,ϵ2 =

1 + V + ζ
U + τ · Beϵ1(n + ν, n + ω)
Beϵ2(n + ω, n + ν)
−1
,
(10.4.2) 
where BetaSubscript epsilon Baseline left parenthesis p comma q right parenthesisϵ(p, q) is the epsilonϵ-th quantile of Betaleft parenthesis p comma q right parenthesis(p, q). Moreover, the quantiles of the 
Beta distribution are related to those of the F-distribution according to the following 
formulae: 
Be Subscript epsilon 2 Baseline left parenthesis a 1 comma a 2 right parenthesis equals StartStartFraction StartFraction a 1 Over a 2 EndFraction upper F Subscript epsilon 2 Baseline left bracket a 1 comma a 2 right bracket OverOver 1 plus StartFraction a 1 Over a 2 EndFraction upper F Subscript epsilon 2 Baseline left bracket a 1 comma a 2 right bracket EndEndFractionBeϵ2(a1, a2) =
a1
a2 Fϵ2[a1, a2]
1 + a1
a2 Fϵ2[a1, a2]
(10.4.3) 
and 
Be Subscript epsilon 1 Baseline left parenthesis a 1 comma a 2 right parenthesis equals StartStartFraction 1 OverOver 1 plus StartFraction a 2 Over a 1 EndFraction upper F Subscript epsilon 2 Baseline left bracket a 2 comma a 1 right bracket EndEndFraction periodBeϵ1(a1, a2) =
1
1 + a2
a1 Fϵ2[a2, a1].
(10.4.4) 
We illustrate these results in the following example. 
Example 10.5 Observations were taken on n equals 72n = 72 renewal cycles of an insertion 
machine. It is assumed that TTF tilde upper E left parenthesis beta right parenthesis∼E(β) and TTR tilde upper E left parenthesis gamma right parenthesis∼E(γ ) in each cycle. The 
observations gave the values upper U equals 496.9U = 496.9 [min] and upper V equals 126.3V = 126.3 [min]. According to 
these values, the MLE of upper A Subscript normal infinityA∞is ModifyingAbove upper A With caret Subscript normal infinity Baseline equals 496.9 divided by left parenthesis 496.9 plus 126.3 right parenthesis equals 0.797 ˆA∞= 496.9/(496.9 + 126.3) = 0.797. Assume 
the gamma prior distributions for lamdaλ and muμ, with nu equals 2ν = 2, tau equals 0.001τ = 0.001, omega equals 2ω = 2 and 
zeta equals 0.005ζ = 0.005. We obtain from (10.4.3) and (10.4.4) for gamma equals 0.95γ = 0.95, 
Be Subscript 0.025 Baseline left parenthesis 74 comma 74 right parenthesis equals 0.4198 comma Be Subscript 0.975 Baseline left parenthesis 74 comma 74 right parenthesis equals 0.5802 periodBe0.025(74, 74) = 0.4198,
Be0.975(74, 74) = 0.5802.
Finally, the credibility limits obtained from (10.4.1) and (10.4.2) are upper A Subscript normal infinity comma 0.025 Baseline equals 0.740A∞,0.025 =
0.740, and upper A Subscript normal infinity comma 0.975 Baseline equals 0.845A∞,0.975 = 0.845. To conclude this example we remark that the Bayes 
estimator of upper A Subscript normal infinityA∞, for the absolute deviation loss function, is the median of the 
posterior distribution of upper A Subscript normal infinityA∞, given left parenthesis upper U comma upper V right parenthesis(U, V ), namely upper A Subscript normal infinity comma 0.5A∞,0.5. 
In the present example n plus nu equals n plus omega equals 74n + ν = n + ω = 74. The  Betaleft parenthesis 74 comma 74 right parenthesis(74, 74) distribution is 
symmetric. Hence BeSubscript 0.5 Baseline left parenthesis 74 comma 74 right parenthesis equals 0.50.5(74, 74) = 0.5. To obtain the upper A Subscript normal infinity comma 0.5A∞,0.5 we solve the equation 
StartStartFraction StartFraction 1 minus upper A Subscript normal infinity comma 0.5 Baseline Over upper A Subscript normal infinity comma 0.5 Baseline EndFraction left parenthesis upper U plus tau right parenthesis OverOver StartFraction 1 minus upper A Subscript normal infinity comma 0.5 Baseline Over upper A Subscript normal infinity comma 0.5 Baseline EndFraction left parenthesis upper U plus tau right parenthesis plus left parenthesis upper V plus zeta right parenthesis EndEndFraction equals Be Subscript 0.5 Baseline left parenthesis n plus nu comma n plus omega right parenthesis period
1−A∞,0.5
A∞,0.5 (U + τ)
1−A∞,0.5
A∞,0.5 (U + τ) + (V + ζ)
= Be0.5(n + ν, n + ω).
In the present case we get

392
10
Bayesian Reliability Estimation and Prediction
upper A Subscript normal infinity comma 0.5 Baseline equals left parenthesis 1 plus StartFraction upper V plus zeta Over upper U plus tau EndFraction right parenthesis Superscript negative 1 Baseline equals StartStartFraction 1 OverOver 1 plus StartFraction 126.305 Over 496.901 EndFraction EndEndFraction equals 0.797 periodA∞,0.5 =

1 + V + ζ
U + τ
−1
=
1
1 + 126.305
496.901
= 0.797.
This is equal to the value of the MLE.
black medium square■
10.5 
Empirical Bayes Method 
Empirical Bayes estimation is designed to utilize the information in large samples to 
estimate the Bayes estimator, without specifying the prior distribution. We introduce 
the idea in relation to estimating the parameter, lamdaλ, of a Poisson distribution. 
Suppose that we have a sequence of independent trials, in each trial a value of 
lamdaλ (failure rate) is chosen from some prior distribution upper H left parenthesis lamda right parenthesisH(λ), and then a value of X 
is chosen from the Poisson distribution script upper P left parenthesis lamda right parenthesisP(λ). If this is repeated n times we have n 
pairs left parenthesis lamda 1 comma x 1 right parenthesis comma midline horizontal ellipsis comma left parenthesis lamda Subscript n Baseline comma x Subscript n Baseline right parenthesis(λ1, x1), · · · , (λn, xn). The statistician, however, can observe only the values 
x 1 comma x 2 comma midline horizontal ellipsis comma x Subscript n Baselinex1, x2, · · · , xn. Let  f Subscript n Baseline left parenthesis i right parenthesisfn(i), i equals 0 comma 1 comma 2 comma midline horizontal ellipsisi = 0, 1, 2, · · · , be the empirical p.d.f. of the observed 
variable X, i.e., f Subscript n Baseline left parenthesis i right parenthesis equals StartFraction 1 Over n EndFraction sigma summation Underscript j equals 1 Overscript n Endscripts upper I left brace x Subscript j Baseline equals i right bracefn(i) = 1
n
n
j=1 I{xj = i}. 
A new trial is to be performed. Let Y be the observed variable in the new trial. It 
is assumed that Y has a Poisson distribution with mean lamdaλ which will be randomly 
chosen from the prior distribution upper H left parenthesis lamda right parenthesisH(λ). The statistician has to estimate the new 
value of lamdaλ from the observed value y of Y. Suppose that the loss function for 
erroneous estimation is the squared-error loss, left parenthesis ModifyingAbove lamda With caret minus lamda right parenthesis squared(ˆλ −λ)2. The Bayes estimator, if 
upper H left parenthesis lamda right parenthesisH(λ) is known, is 
upper E Subscript upper H Baseline left brace lamda vertical bar y right brace equals StartFraction integral Subscript 0 Superscript normal infinity Baseline lamda Superscript y plus 1 Baseline e Superscript negative lamda Baseline h left parenthesis lamda right parenthesis normal d lamda Over integral Subscript 0 Superscript normal infinity Baseline lamda Superscript y Baseline e Superscript negative lamda Baseline h left parenthesis lamda right parenthesis normal d lamda EndFraction commaEH{λ | y} =
 ∞
0
λy+1e−λh(λ) dλ
 ∞
0
λye−λh(λ) dλ ,
(10.5.1) 
where h left parenthesis lamda right parenthesish(λ) is the prior p.d.f. of lamdaλ. 
The predictive p.d.f. of Y, under H, is  
f Subscript upper H Baseline left parenthesis y right parenthesis equals StartFraction 1 Over y factorial EndFraction integral Subscript 0 Superscript normal infinity Baseline lamda Superscript y Baseline e Superscript negative lamda Baseline h left parenthesis lamda right parenthesis normal d lamda periodfH(y) = 1
y!
 ∞
0
λye−λh(λ) dλ.
(10.5.2) 
The Bayes estimator of lamdaλ (10.5.1) can be written in the form 
upper E Subscript upper H Baseline left brace lamda vertical bar y right brace equals left parenthesis y plus 1 right parenthesis StartFraction f Subscript upper H Baseline left parenthesis y plus 1 right parenthesis Over f Subscript upper H Baseline left parenthesis y right parenthesis EndFraction comma y equals 0 comma 1 comma midline horizontal ellipsis periodEH{λ | y} = (y + 1)fH(y + 1)
fH(y)
,
y = 0, 1, · · · .
(10.5.3) 
The empirical p.d.f. f Subscript n Baseline left parenthesis y right parenthesisfn(y) converges (by the Strong Law of Large Numbers) in a 
probabilistic sense, as n right arrow normal infinityn →∞, to f Subscript upper H Baseline left parenthesis y right parenthesisfH(y). Accordingly, replacing f Subscript upper H Baseline left parenthesis y right parenthesisfH(y) in (10.5.3) 
with f Subscript n Baseline left parenthesis y right parenthesisfn(y) we obtain an estimator of upper E Subscript upper H Baseline left brace lamda vertical bar y right braceEH{λ | y} based on the past n trials. This 
estimator is called an empirical Bayes estimator (EBE) of lamdaλ:

10.5
Empirical Bayes Method
393
Table 10.2 Empirical 
distribution of number of 
soldering defects (per 
100,000 points) 
x
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
f left parenthesis x right parenthesisf (x)
4 
21 
29 
32 
19 
14 
13 
5 
8 
5 
x
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
f left parenthesis x right parenthesisf (x)
9 
1 
2 
4 
4 
1 
4 
2 
1 
1 
x
20 
21 
22 
23 
24 
25 
26 
Total 
f left parenthesis x right parenthesisf (x)
1 
1 
1 
2 
1 
2 
1 
188 
ModifyingAbove lamda With caret Subscript n Baseline left parenthesis y right parenthesis equals left parenthesis y plus 1 right parenthesis StartFraction f Subscript n Baseline left parenthesis y plus 1 right parenthesis Over f Subscript n Baseline left parenthesis y right parenthesis EndFraction comma y equals 0 comma 1 comma midline horizontal ellipsis periodˆλn(y) = (y + 1)fn(y + 1)
fn(y)
,
y = 0, 1, · · · .
(10.5.4) 
In the following example we illustrate this estimation method. 
Example 10.6 
.n = 188 batches of circuit boards were inspected for soldering 
defects. Each board has typically several hundred soldering points, and each batch 
contained several hundred boards. It is assumed that the number of soldering defects, 
X (per 10 Superscript 5105 points), has a Poisson distribution. In Table 10.2 we present the frequency 
distribution of X among the 188 observed batches. 
Accordingly, if in a new batch the number of defects (per 10 Superscript 5105 points) is y equals 8y = 8, 
the EBE of lamdaλ is ModifyingAbove lamda With caret Subscript 188 Baseline left parenthesis 8 right parenthesis equals 9 times five eighths equals 5.625ˆλ188(8) = 9 × 5
8 = 5.625 (per 10 Superscript 5105), or 56.25 (per 10 Superscript 6106 points), i.e., 
56.25 PPM. After observing y 189 equals 8y189 = 8 we can increase f 188 left parenthesis 8 right parenthesisf188(8) by 1, i.e., f 189 left parenthesis 8 right parenthesis equals f 188 left parenthesis 8 right parenthesis plus 1f189(8) =
f188(8) + 1, and observe the next batch.
black medium square■
The above method of deriving an EBE can be employed for any p.d.f. f left parenthesis x semicolon theta right parenthesisf (x; θ) of 
a discrete distribution, such that 
StartFraction f left parenthesis x plus 1 semicolon theta right parenthesis Over f left parenthesis x semicolon theta right parenthesis EndFraction equals a left parenthesis x right parenthesis plus b left parenthesis x right parenthesis theta periodf (x + 1; θ)
f (x; θ)
= a(x) + b(x)θ.
In such a case, the EBE of thetaθ is 
ModifyingAbove theta With caret Subscript n Baseline left parenthesis x right parenthesis equals StartFraction f Subscript n Baseline left parenthesis x plus 1 right parenthesis Over f Subscript n Baseline left parenthesis x right parenthesis b left parenthesis x right parenthesis EndFraction minus StartFraction a left parenthesis x right parenthesis Over b left parenthesis x right parenthesis EndFraction period ˆθn(x) = fn(x + 1)
fn(x)b(x) −a(x)
b(x).
(10.5.5) 
Generally, however, it is difﬁcult to obtain an estimator which converges, as n 
increases, to the value of the Bayes estimator. A parametric EB procedure is 
one in which, as part of the model, we assume that the prior distribution belongs 
to a parametric family, but the parameter of the prior distribution is consistently 
estimated from the past data. For example, if the model assumes that the observed 
TTF is upper E left parenthesis beta right parenthesisE(β) and that beta tilde IG left parenthesis nu comma tau right parenthesisβ ∼IG(ν, τ), instead of specifying the values of nuν and tauτ, 
we use the past data to estimate. We may obtain an estimator of upper E left brace theta vertical bar upper T comma nu comma tau right parenthesisE{θ | T, ν, τ)
which converges in a probabilistic sense, as n increases, to the Bayes estimator. An 
example of such a parametric EBE is given below. 
Example 10.7 Suppose that upper T tilde upper E left parenthesis beta right parenthesisT ∼E(β) and betaβ has a prior IGleft parenthesis nu comma tau right parenthesis(ν, τ). The Bayes 
estimator of the reliability function is given by (10.2.5). 
Let t 1 comma t 2 comma midline horizontal ellipsis comma t Subscript n Baselinet1, t2, · · · , tn be past independent observations on T .

394
10
Bayesian Reliability Estimation and Prediction
The expected value of T under the predictive p.d.f. is 
upper E Subscript tau comma nu Baseline left brace upper T right brace equals StartFraction 1 Over tau left parenthesis nu minus 1 right parenthesis EndFraction commaEτ,ν{T } =
1
τ(ν −1),
(10.5.6) 
provided nu greater than 1ν > 1. The second moment of T is 
upper E Subscript tau comma nu Baseline left brace upper T squared right brace equals StartFraction 2 Over tau squared left parenthesis nu minus 1 right parenthesis left parenthesis nu minus 2 right parenthesis EndFraction commaEτ,ν{T 2} =
2
τ 2(ν −1)(ν −2),
(10.5.7) 
provided nu greater than 2ν > 2. 
Let upper M Subscript 1 comma n Baseline equals StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript iM1,n =
1
n
n
i=1 ti and upper M Subscript 2 comma n Baseline equals StartFraction 1 Over n EndFraction sigma summation Underscript i equals 1 Overscript n Endscripts t Subscript i Superscript 2M2,n =
1
n
n
i=1 t2
i . upper M Subscript 1 comma nM1,n and upper M Subscript 2 comma nM2,n converge in a 
probabilistic sense to upper E Subscript tau comma nu Baseline left brace upper T right braceEτ,ν{T } and upper E Subscript tau comma nu Baseline left brace upper T squared right braceEτ,ν{T 2}, respectively. We estimate tauτ and nuν by 
the method of moment equations, by solving 
upper M Subscript 1 comma n Baseline equals StartFraction 1 Over ModifyingAbove tau With caret left parenthesis ModifyingAbove nu With caret minus 1 right parenthesis EndFractionM1,n =
1
ˆτ(ˆν −1)
(10.5.8) 
and 
upper M Subscript 2 comma n Baseline equals StartFraction 2 Over ModifyingAbove tau With caret squared left parenthesis ModifyingAbove nu With caret minus 1 right parenthesis left parenthesis ModifyingAbove nu With caret minus 2 right parenthesis EndFraction periodM2,n =
2
ˆτ 2(ˆν −1)(ˆν −2).
(10.5.9) 
Let upper D Subscript n Superscript 2 Baseline equals upper M Subscript 2 comma n Baseline minus upper M Subscript 1 comma n Superscript 2D2
n = M2,n −M2
1,n be the sample variance. Simple algebraic manipulations 
yield the estimators 
ModifyingAbove tau With caret Subscript n Baseline equals StartFraction left parenthesis upper D Subscript n Superscript 2 Baseline minus upper M Subscript 1 comma n Superscript 2 Baseline right parenthesis Over left bracket upper M Subscript 1 comma n Baseline left parenthesis upper D Subscript n Superscript 2 Baseline plus upper M Subscript 1 comma n Superscript 2 Baseline right parenthesis right bracket EndFraction commaˆτn =
(D2
n −M2
1,n)
[M1,n(D2n + M2
1,n)],
(10.5.10) 
ModifyingAbove nu With caret Subscript n Baseline equals StartFraction 2 upper D Subscript n Superscript 2 Baseline Over upper D Subscript n Superscript 2 Baseline minus upper M Subscript 1 comma n Superscript 2 Baseline EndFraction commaˆνn =
2D2
n
D2n −M2
1,n
,
(10.5.11) 
provided upper D Subscript n Superscript 2 Baseline greater than upper M Subscript 1 comma n Superscript 2D2
n > M2
1,n. It can be shown that for large values of n, upper D Subscript n Superscript 2 Baseline greater than upper M Subscript 1 comma n Superscript 2D2
n > M2
1,n with 
high probability. 
Substituting the empirical estimates ModifyingAbove tau With caret Subscript nˆτn and ModifyingAbove nu With caret Subscript nˆνn in (10.2.5) we obtain a parametric 
EBE of the reliability function.
black medium square■
For additional results on the EBE of reliability functions, see Martz and Waller 
(1982) and Tsokos and Shimi (1977). 
10.6 
Chapter Highlights 
The main concepts and deﬁnitions introduced in this chapter include: 
• 
Prior distribution 
• 
Predictive distribution

10.7
Exercises
395
• 
Posterior distribution 
• 
Beta function 
• 
Conjugate distributions 
• 
Bayes estimator 
• 
Posterior risk 
• 
Posterior expectation 
• 
Distribution-free estimators 
• 
Credibility intervals 
• 
Minimal sufﬁcient statistics 
• 
Empirical Bayes method 
10.7 
Exercises 
Exercise 10.1 Suppose that the TTF of a system is a random variable having 
exponential distribution, E(β). Suppose also that the prior distribution of λ = 1/β 
is G(2.25, 0.01). 
(i) What is the posterior distribution of λ, given  T = 150 [hr]? 
(ii) What is the Bayes estimator of β, for the squared-error loss? 
(iii) What is the posterior SD of β? 
Exercise 10.2 Let J(t) denote the number of failures of a device in the time interval 
(0, t]. After each failure the device is instantaneously renewed. Let J(t) have a 
Poisson distribution with mean λt. Suppose that λ has a gamma prior distribution, 
with parameters ν = 2 and τ = 0.05. 
(i) What is the predictive distribution of J(t)? 
(ii) Given that J(t)/t = 10, how many failures are expected in the next time unit? 
(iii) What is the Bayes estimator of λ, for the squared-error loss? 
(iv) What is the posterior SD of λ? 
Exercise 10.3 The proportion of defectives, θ, in a production process has a 
uniform prior distribution on (0, 1). A random sample of n = 10 items from this 
process yields K10 = 3 defectives. 
(i) What is the posterior distribution of θ? 
(ii) What is the Bayes estimator of θ for the absolute error loss? 
Exercise 10.4 Let X ∼ P(λ) and suppose that λ has the Jeffrey improper prior 
h(λ) = 1
√
λ . Find the Bayes estimator for squared-error loss and its posterior SD. 
Exercise 10.5 Apply formula (10.2.3) to determine the Bayes estimator of the 
reliability when n = 50 and K50 = 49.

396
10
Bayesian Reliability Estimation and Prediction
Exercise 10.6 A system has three modules, M1, M2, M3. M1 and M2 are connected 
in series and these two are connected in parallel to M3, i.e., 
upper R Subscript s y s Baseline equals psi Subscript p Baseline left parenthesis upper R 3 comma psi Subscript s Baseline left parenthesis upper R 1 comma upper R 2 right parenthesis right parenthesis equals upper R 3 plus upper R 1 upper R 2 minus upper R 1 upper R 2 upper R 3 commaRsys = ψp(R3, ψs(R1, R2)) = R3 + R1R2 −R1R2R3,
where Ri is the reliability of module Mi. The TTFs of the three modules are 
independent random variables having exponential distributions with prior IG(νi, τi) 
distributions of their MTTF. Moreover, ν1 = 2.5, ν2 = 2.75, ν3 = 3, τ1 = τ2 = 
τ3 = 1/1000. In separate independent trials of the TTF of each module we obtained 
the statistics T (1) 
n 
= 4565 [hr], T (2) 
n 
= 5720 [hr] and T (3) 
n 
= 7505 [hr], where in 
all three experiments n = r = 10. Determine the Bayes estimator of Rsys, for  the  
squared-error loss. 
Exercise 10.7 n = 30 computer monitors were put on test at a temperature of 
100◦F and relative humidity of 90% for 240 [hr]. The number of monitors which 
survived this test is K30 = 28. Determine the Bayes credibility interval for R(240), 
at level γ = 0.95, with respect to a uniform prior on (0, 1). 
Exercise 10.8 Determine a γ = .95 level credibility interval for R(t) at t = 25 [hr] 
when TTF ∼ E(β), β ∼IG(3, 0.01), r = 27, Tn,r = 3500 [hr]. 
Exercise 10.9 Under the conditions of Exercise 10.8 determine a Bayes prediction 
interval for the total life of s = 2 devices. 
Exercise 10.10 A repairable system has exponential TTF and exponential TTR, 
which are independent of each other. n = 100 renewal cycles were observed. The 
total times till failure were 10,050 [hr] and the total repair times were 500 [min]. 
Assuming gamma prior distributions for λ and μ with ν = ω = 4 and τ = 0.0004 
[hr], ζ = 0.01 [min], ﬁnd a γ = 0.95 level credibility interval for A∞. 
Exercise 10.11 In reference to Example 10.6, suppose that the data of Table 10.2 
were obtained for a Poisson random variable where λ1, · · ·  , λ188 have a gamma 
(ν, τ) prior distribution. 
(i) What is the predictive distribution of the number of defects per batch? 
(ii) Find the formulae for the ﬁrst two moments of the predictive distribution. 
(iii) Find, from the empirical frequency distribution of Table 10.2, the ﬁrst two 
sample moments. 
(iv) Use the method of moment equations to estimate the prior parameters ν and τ. 
(v) What is the Bayes estimator of λ if X189 = 8?

Chapter 11 
Sampling Plans for Batch and Sequential 
Inspection 
Preview Traditional supervision consists of keeping close control of operations 
and progress, the focus of attention being the product or process outputs. A direct 
implication of this approach is to guarantee product quality through inspection and 
screening. The chapter discusses sampling techniques and measures of inspection 
effectiveness. Performance characteristics of sampling plans are discussed and 
guidelines for choosing economic sampling plans are presented. The basic theory 
of single-stage acceptance sampling plans for attributes is ﬁrst presented including 
the concepts of Acceptable Quality Level and Limiting Quality Level. Formulas for 
determining sample size, acceptance levels, and operating characteristic functions 
are provided. Moving on from single-stage sampling, the chapter covers double 
sampling and sequential sampling using Wald’s sequential probability ratio test. 
One section deals with acceptance sampling for variable data. Other topics covered 
include computations of Average Sample Numbers and Average Total Inspection for 
rectifying inspection plans. Modern Skip-Lot sampling procedures are introduced 
and compared to the standard application of sampling plans where every lot is 
inspected. The Deming “all or nothing” inspection criterion is presented and the 
connection between sampling inspection and statistical process control is made. 
Special sections are dedicated to sequential methods of software applications such 
as one- and two-arm bandit models used in A/B testing and software reliability 
models used in determining release readiness of software versions. Throughout the 
chapter we show Python code which is used to perform various calculations and 
generate appropriate tables and graphs. 
Supplementary Information The online version contains supplementary material available at 
(https://doi.org/10.1007/978-3-031-28482-3_11). 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3_11
397

398
11
Sampling Plans for Batch and Sequential Inspection
11.1 General Discussion 
Sampling plans for product inspection are quality assurance schemes, designed to 
test whether the quality level of a product conforms with the required standards. 
These methods of quality inspection are especially important when products are 
received from suppliers or vendors on whom we have no other assessment of the 
quality level of their production processes. Generally, if a supplier has established 
procedures of statistical process control which assure the required quality standards 
are met (see Chaps. 2, 3, and 4), then sampling inspection of his shipments may not 
be necessary. However periodic auditing of the quality level of certiﬁed suppliers 
might be prudent to ensure that these do not drop below the acceptable standards. 
Quality auditing or inspection by sampling techniques can also be applied within 
the plant, at various stages of the production process, e.g., when lots are transferred 
from one department to another. Another area of application relates to software 
applications where alternative versions are tested by splitting real-time trafﬁc 
(A/B testing) or test results are tracked to determine shipping readiness (software 
reliability). 
In this chapter we discuss various sampling and testing procedures, designed to 
maintain quality standards. In particular, single, double, and sequential sampling 
plans for attributes and single sampling plans for continuous measurements are 
studied. We discuss also testing via tolerance limits, sequential bandit problems, and 
software reliability models. The chapter describes some of the established standards, 
and in particular the Skip Lot procedure, which appears in modern standards. 
We introduce a range of concepts and tools associated with sampling inspection 
schemes. The methods presented below can be implemented in Python and we pro-
vide implementations in the mistat package. Modern nomenclature has evolved. 
A product unit which did not meet the quality speciﬁcations or requirements was 
called defective. This term has been changed to nonconforming. Thus, in early 
standards, like MIL-STD 105E, we ﬁnd the term “defective items” and “number of 
defects.” In modern standards like ANSI/ASQC Z1.4 and the international standard 
ISO 2859, the term used is nonconforming. We use the two terms interchangeably. 
Similarly, the acronyms LTPD and LQL, which will be explained later, will also be 
used interchangeably. 
A lot is a collection of N elements which are subject to quality inspection. 
Accordingly, a lot is a ﬁnite real population of products. Acceptance of a lot is 
a quality approval, providing the “green light” for subsequent use of the elements 
of the lot. Generally, we refer to lots of raw material, of semi-ﬁnished or ﬁnished 
products, etc., which are purchased from vendors or produced by subcontractors. 
Before acceptance, a lot is typically subjected to quality inspection unless the vendor 
has been certiﬁed and its products are delivered directly, without inspection, to the 
production line. The purchase contracts typically specify the acceptable quality level 
and the method of inspection. 
In general, it is expected that a lot contains no more than a certain percentage of 
nonconforming (defective) items, where the test conditions that classify an item as

11.1
General Discussion
399
defective are usually well speciﬁed. One should decide if a lot has to be subjected 
to a complete inspection, item by item, or whether it is sufﬁcient to determine 
acceptance using a sample from the lot. If we decide to inspect a sample, we must 
determine how large it is and what is the criterion for accepting or rejecting the 
lot. Furthermore, the performance characteristics of the procedures in use should be 
understood. 
The proportion of nonconforming items in a lot is the ratio p equals upper M divided by upper Np = M/N, where 
M is the number of defective items in the whole lot and N is the size of the lot. 
If we choose to accept only lots with zero defectives, we have to inspect each 
lot completely, item by item. This approach is called 100% inspection. This is 
the case, for example, when the items of the lots are used in a critical or very 
expensive system. A communication satellite is an example of such a system. In 
such cases, the cost of inspection is negligible compared to the cost of failure. On 
the other hand, there are many situations in which complete inspection is impossible 
(e.g., destructive testing) or impractical (because of the large expense involved). 
In this situation, the two parties involved, the customer and its supplier, specify 
an acceptable quality level (AQL) and a limiting quality level (LQL). When 
the proportion of defectives, p, in the lot is not larger than the AQL, the lot is 
considered good and should be accepted with high probability. If, on the other hand, 
the proportion of defectives in the lot is greater than the LQL, the lot should be 
rejected with high probability. If p is between the AQL and the LQL, then either 
acceptance or rejection of the lot can happen with various probability levels. 
How should the parties specify the AQL and LQL levels? Usually, the AQL 
is determined by the quality requirements of the customer who is going to use 
the product. The producer of the product, which is the supplier, tries generally to 
demonstrate to the customer that his production processes maintain a capability 
level in accordance with the customer’s or consumer’s requirements. Both the AQL 
and LQL are speciﬁed in terms of proportions p 0p0 and p Subscript tpt of nonconforming in the 
process. 
The risk of rejecting a good lot, i.e., a lot with p less than or equalsp ≤AQL, is called the producer’s 
risk, while the risk of accepting a bad lot, i.e., a lot for which p greater than or equalsp ≥LQL, is called 
the consumer’s risk. Thus, the problem of designing an acceptance sampling plan 
is that of choosing: 
1. The method of sampling 
2. The sample size 
3. The acceptance criteria for testing the hypothesis 
upper H 0 colon p less than or equals AQL commaH0 : p ≤AQL,
against the alternative 
upper H 1 colon p greater than or equals LQL commaH1 : p ≥LQL,

400
11
Sampling Plans for Batch and Sequential Inspection
so that the probability of rejecting a good lot will not exceed a value alphaα (the level 
of signiﬁcance) and the probability of accepting a bad lot will not exceed betaβ. In  
this context, alphaα and betaβ are called the producer’s risk and the consumer’s risk, 
respectively. 
11.2 Single-Stage Sampling Plans for Attributes 
A single-stage sampling plan for an attribute is an acceptance/rejection procedure 
for a lot of size N, according to which a random sample of size n is drawn from 
the lot, without replacement. Let M be the number of defective items (elements) 
in the lot, and let X be the number of defective items in the sample. Obviously, 
X is a random variable whose range is StartSet 0 comma 1 comma 2 comma midline horizontal ellipsis comma n Superscript asterisk Baseline EndSet{0, 1, 2, · · · , n∗}, where n Superscript asterisk Baseline equals min left parenthesis n comma upper M right parenthesisn∗= min(n, M). 
The distribution function of X is the hypergeometric distribution upper H left parenthesis upper N comma upper M comma n right parenthesisH(N, M, n), 
(see Section 2.3.2, Modern Statistics, Kenett et al. 2022b) with the probability 
distribution function (p.d.f.) 
h left parenthesis x semicolon upper N comma upper M comma n right parenthesis equals StartFraction StartBinomialOrMatrix upper M Choose x EndBinomialOrMatrix StartBinomialOrMatrix upper N minus upper M Choose n minus x EndBinomialOrMatrix Over StartBinomialOrMatrix upper N Choose n EndBinomialOrMatrix EndFraction comma x equals 0 comma midline horizontal ellipsis comma n Superscript asteriskh(x; N, M, n) =
M
x
N−M
n−x

N
n

,
x = 0, · · · , n∗
(11.2.1) 
and the cumulative distribution function (c.d.f.) 
upper H left parenthesis x semicolon upper N comma upper M comma n right parenthesis equals sigma summation Underscript j equals 0 Overscript x Endscripts h left parenthesis j semicolon upper N comma upper M comma n right parenthesis periodH(x; N, M, n) =
x

j=0
h(j; N, M, n).
(11.2.2) 
In Python, you can use the hypergeom distribution of the scipy.stats package. 
The relevant methods are hypergeom.pmf(x, N, n, M) for p.d.f. and hypergeom.cdf(x, 
N, n, M) for the c.d.f. Note that in comparison to our nomenclature the order for N, 
M, and n the scipy functions is different. 
Suppose we consider a lot of upper N equals 100N = 100 items to be acceptable if it has no more 
than upper M equals 5M = 5 nonconforming items and non-acceptable if it has more than upper M equals 10M = 10
nonconforming items. For a sample of size n equals 10n = 10, we derive the hypergeometric 
distribution upper H left parenthesis 100 comma 5 comma 10 right parenthesisH(100, 5, 10) and upper H left parenthesis 100 comma 10 comma 10 right parenthesisH(100, 10, 10). From Table 11.1, we see that, if such 
Table 11.1 The p.d.f. and c.d.f. of upper H left parenthesis 100 comma 5 comma 10 right parenthesisH(100, 5, 10) and upper H left parenthesis 100 comma 10 comma 10 right parenthesisH(100, 10, 10)
j
h left parenthesis j semicolon 100 comma 5 comma 10 right parenthesish(j; 100, 5, 10)
upper H left parenthesis j semicolon 100 comma 5 comma 10 right parenthesisH(j; 100, 5, 10) 
0 
0.5838
0.5838 
1 
0.3394
0.9231 
2 
0.0702
0.9934 
3 
0.0064
0.9997 
4 
0.0003
1.0000 
j 
h left parenthesis j semicolon 100 comma 10 comma 10 right parenthesish(j; 100, 10, 10) 
upper H left parenthesis j semicolon 100 comma 10 comma 10 right parenthesisH(j; 100, 10, 10) 
0 
0.3305
0.3305 
1 
0.4080
0.7385 
2 
0.2015
0.9400 
3 
0.0518
0.9918 
4 
0.0076
0.9993 
5 
0.0006
1.0000 

11.2 
Single-Stage Sampling Plans for Attributes
401 
a lot is accepted whenever upper X equals 0X = 0, the consumer’s risk of accepting a lot which 
should be rejected is 
beta equals upper H left parenthesis 0 semicolon 100 comma 10 comma 10 right parenthesis equals 0.3305 periodβ = H(0; 100, 10, 10) = 0.3305. 
The producer’s risk of rejecting an acceptable lot is 
alpha equals 1 minus upper H left parenthesis 0 semicolon 100 comma 5 comma 10 right parenthesis equals 0.4162 periodα = 1 − H(0; 100, 5, 10) = 0.4162. 
As before, let p 0p0 denote the AQL and p Subscript tpt the LQL. Obviously, 0 less than p 0 less than p Subscript t Baseline less than 10 < p0 < pt < 1. 
Suppose that the decision is to accept a lot whenever the number of nonconforming 
X is not greater than c, i.e., upper X less than or equals cX ≤c. c is called the acceptance number. For speciﬁed 
values of p 0p0, p Subscript tpt, alphaα, and betaβ, we can determine n and c so that 
probability StartSet upper X less than or equals c vertical bar p 0 EndSet greater than or equals 1 minus alphaPr{X ≤ c | p0} ≥1 − α
(11.2.3) 
and 
probability StartSet upper X less than or equals c vertical bar p Subscript t Baseline EndSet less than or equals beta periodPr{X ≤ c | pt} ≤β.
(11.2.4) 
Notice that n and c should satisfy the inequalities 
StartLayout 1st Row 1st Column upper H left parenthesis c semicolon upper N comma upper M 0 comma n right parenthesis 2nd Column greater than or equals 1 minus alpha EndLayout StartLayout 2nd Row 1st Column upper H left parenthesis c semicolon upper N comma upper M Subscript t Baseline comma n right parenthesis 2nd Column less than or equals beta comma EndLayoutH(c; N, M0, n)  ≥ 1 −α
(11.2.5) 
H(c; N, Mt, n)  ≤ β,
(11.2.6) 
where upper M 0 equals left bracket upper N p 0 right bracketM0 = [Np0] and upper M Subscript t Baseline equals left bracket upper N p Subscript t Baseline right bracketMt = [Npt] and left bracket a right bracket[a] is the integer part of a. In Table 11.2, 
a few numerical results show how n and c depend on p 0p0 and p Subscript tpt, when the lot is of 
size n equals 100n = 100 and alpha equals beta equals 0.05α = β = 0.05. To achieve this in Python, we use the ﬁndPlan 
function from the mistat package. 
from mistat.acceptanceSampling import findPlan 
findPlan(PRP=[0.01, 0.95], CRP=[0.08, 0.05], oc_type='hypergeom', N=100) 
Plan(n=46, c=1, r=2) 
We see that, even if the requirements are not very stringent, for example, when 
p 0 equals 0.01p0 = 0.01 and p Subscript t Baseline equals 0.05pt = 0.05, the required sample size is n equals 65n = 65. If in such a sample 
there is more than 1 defective item, then the entire lot is rejected. Similarly, if p 0 equals 0.03p0 = 
0.03 and p Subscript t Baseline equals 0.05pt = 0.05, then the required sample size is n equals 92n = 92, which is almost the 
entire lot. On the other hand, if p 0 equals 0.01p0 = 0.01 and p Subscript tpt is greater than .20, we need no 
more than 20 items in the sample. If we relax the requirement concerning alphaα and 
betaβ and allow higher producer’s and consumer’s risks (PRP and CRP), the required 
sample size will be smaller, as shown in Table 11.3. 
An important characterization of an acceptance sampling plan is given by its 
operating characteristic (OC) function. This function, denoted by OCleft parenthesis p right parenthesis(p), yields 

402
11 
Sampling Plans for Batch and Sequential Inspection 
Table 11.2 Sample size, n, 
and critical level, c, for  
single-stage acceptance 
sampling with upper N equals 100N = 100 and 
alpha equals beta equals 0.05α = β = 0.05 
p 0p0
p Subscript tpt
n
c 
r 
0.01 
0.05 
65 
1 
2 
0.01 
0.08 
46 
1 
2 
0.01 
0.11 
36 
1 
2 
0.01 
0.14 
29 
1 
2 
0.01 
0.17 
24 
1 
2 
0.01 
0.20 
20 
1 
2 
0.01 
0.23 
18 
1 
2 
0.01 
0.26 
16 
1 
2 
0.01 
0.29 
14 
1 
2 
0.01 
0.32 
13 
1 
2 
p 0p0
p Subscript tpt
n
c 
r 
0.03 
0.05 
92 
3 
4 
0.03 
0.08 
71 
3 
4 
0.03 
0.11 
56 
3 
4 
0.03 
0.14 
37 
2 
3 
0.03 
0.17 
31 
2 
3 
0.03 
0.20 
27 
2 
3 
0.03 
0.23 
24 
2 
3 
0.03 
0.26 
21 
2 
3 
0.03 
0.29 
19 
2 
3 
0.03 
0.32 
13 
1 
2 
Table 11.3 Sample size, n, 
and critical level, c, for  
single-stage acceptance 
sampling, upper N equals 100N = 100, 
alpha equals 0.10α = 0.10, and beta equals 0.20β = 0.20 
p 0p0
p Subscript tpt
n
c 
r 
0.01 
0.05 
49 
1 
2 
0.01 
0.08 
33 
1 
2 
0.01 
0.11 
25 
1 
2 
0.01 
0.14 
20 
1 
2 
0.01 
0.17 
9 
0 
1 
0.01 
0.20 
7 
0 
1 
0.01 
0.23 
6 
0 
1 
0.01 
0.26 
6 
0 
1 
0.01 
0.29 
5 
0 
1 
0.01 
0.32 
5 
0 
1 
p 0p0
p Subscript tpt
n
c 
r 
0.03 
0.05 
83 
3 
4 
0.03 
0.08 
46 
2 
3 
0.03 
0.11 
35 
2 
3 
0.03 
0.14 
28 
2 
3 
0.03 
0.17 
16 
1 
2 
0.03 
0.20 
14 
1 
2 
0.03 
0.23 
12 
1 
2 
0.03 
0.26 
11 
1 
2 
0.03 
0.29 
10 
1 
2 
0.03 
0.32 
9 
1 
2 
the probability of accepting a lot having proportion p of defective items. If we let 
upper M Subscript p Baseline equals left bracket upper N p right bracketMp = [Np], then we can calculate the OC function by 
OC left parenthesis p right parenthesis equals upper H left parenthesis c semicolon upper N comma upper M Subscript p Baseline comma n right parenthesis periodOC(p) = H(c; N, Mp, n).
(11.2.7) 
We can calculate and visualize the operating characteristics curve using the 
function OperatingCharacteristics2c from the mistat package as follows: 
from mistat.acceptanceSampling import OperatingCharacteristics2c 
X = OperatingCharacteristics2c(50, 1, oc_type='hypergeom', N=100, 
pd=np.linspace(0, 0.15, 300)) 
df = pd.DataFrame({'p': X.pd, 'OC(p)': X.paccept}) 
ax = df.plot(x='p', y='OC(p)', legend=False, linestyle=':', color='grey') 
ax.set_ylabel('OC(p)') 
X = OperatingCharacteristics2c(50, 1, oc_type='hypergeom', N=100, 
pd=[i / 100 for i in range(16)]) 
df = pd.DataFrame({'p': X.pd, 'OC(p)': X.paccept}) 
ax = df.plot.scatter(x='p', y='OC(p)', legend=False, ax=ax, color='black') 
plt.show() 

11.3 
Approximate Determination of the Sampling Plan
403 
Table 11.4 The OC function 
of a single-stage acceptance 
sampling plan, upper N equals 100N = 100, 
n equals 50n = 50, and c equals 1c = 1 
p
OC left parenthesis p right parenthesisOC(p) 
0.00 
1.000000 
0.01 
1.000000 
0.02 
0.752525 
0.03 
0.500000 
0.04 
0.308654 
0.05 
0.181089 
0.06 
0.102201 
0.07 
0.055875 
p
OC left parenthesis p right parenthesisOC(p) 
0.08 
0.029723 
0.09 
0.015429 
0.10 
0.007830 
0.11 
0.003890 
0.12 
0.001894 
0.13 
0.000904 
0.14 
0.000423 
0.15 
0.000194 
Fig. 11.1 Operating characteristics curve for a single-stage acceptance sampling plan, upper N equals 100N = 100, 
n equals 50n = 50, and c equals 1c = 1 
In Table 11.4, we present a few values of the OC function for single-stage 
acceptance sampling, for lot size upper N equals 100N = 100, sample size n equals 50n = 50, and acceptance 
number c equals 1c = 1. In Fig. 11.1, we present the graph of the OC function, corresponding 
to Table 11.4. 
11.3 Approximate Determination of the Sampling Plan 
If the sample size, n, is not too small, the c.d.f. of the hypergeometric distribution 
can be approximated by the normal distribution. More speciﬁcally, for large values 
of n, we have the following approximation: 
upper H left parenthesis a semicolon upper N comma upper M comma n right parenthesis approaches the limit upper Phi left parenthesis StartFraction a plus 0.5 minus n upper P Over left parenthesis n upper P upper Q left parenthesis 1 minus StartFraction n Over upper N EndFraction right parenthesis right parenthesis Superscript 1 divided by 2 Baseline EndFraction right parenthesis commaH(a; N, M, n) .= 

a + 0.5 − nP

nPQ

1 − n 
N
1/2

,
(11.3.1) 
where upper P equals upper M divided by upper NP = M/N and upper Q equals 1 minus upper PQ = 1 −P. 

404
11 
Sampling Plans for Batch and Sequential Inspection 
Table 11.5 Hypergeometric c.d.f.’s and their normal approximations 
H(j;100,30,20)
H(j;100,50,20)
H(j;100,80,20) 
a
Hypergeometric 
Normal 
Hypergeometric 
Normal 
Hypergeometric 
Normal 
0
0.0003
0.0013 
0.0000
0.0000 
0.0000
0.0000 
1
0.0039
0.0070 
0.0000
0.0000 
0.0000
0.0000 
2
0.0227
0.0281 
0.0000
0.0001 
0.0000
0.0000 
3
0.0824
0.0863 
0.0004
0.0006 
0.0000
0.0000 
4
0.2092
0.2066 
0.0025
0.0030 
0.0000
0.0000 
5
0.4010
0.3925 
0.0114
0.0122 
0.0000
0.0000 
6
0.6151
0.6075 
0.0392
0.0401 
0.0000
0.0000 
7
0.7954
0.7934 
0.1054
0.1056 
0.0000
0.0000 
8
0.9115
0.9137 
0.2270
0.2266 
0.0000
0.0000 
9
0.9693
0.9719 
0.4016
0.4013 
0.0001
0.0000 
10 
0.9915
0.9930 
0.5984
0.5987 
0.0006
0.0003 
11 
0.9982
0.9987 
0.7730
0.7734 
0.0039
0.0025 
12 
0.9997
0.9998 
0.8946
0.8944 
0.0181
0.0144 
13 
1.0000
1.0000 
0.9608
0.9599 
0.0637
0.0591 
14
0.9886
0.9878 
0.1727
0.1743 
15
0.9975
0.9970 
0.3647
0.3773 
16
0.9996
0.9994 
0.6084
0.6227 
17
1.0000
0.9999 
0.8242
0.8257 
18
0.9502
0.9409 
19
0.9934
0.9856 
The ﬁrst question to ask is how large should n be? The answer to this question 
depends on how close we wish the approximation to be. Generally, if 0.2 less than upper P less than 0.80.2 < P  <  
0.8, n equals 20n = 20 is large enough to yield a good approximation, as illustrated in 
Table 11.5. 
If upper P less than 0.2P <  0.2 or upper P greater than 0.8P >  0.8, we usually need larger sample sizes to attain good 
approximation. We show now how the constants left parenthesis n comma c right parenthesis(n, c) can be determined. The two 
requirements to satisfy are OCleft parenthesis p 0 right parenthesis equals 1 minus alpha(p0) = 1 − α and OCleft parenthesis p Subscript t Baseline right parenthesis equals beta(pt) = β. These requirements 
are expressed approximately by the following two equations: 
StartLayout 1st Row 1st Column c plus one half minus n p 0 2nd Column equals z Subscript 1 minus alpha Baseline left parenthesis n p 0 q 0 left parenthesis 1 minus StartFraction n Over upper N EndFraction right parenthesis right parenthesis Superscript 1 divided by 2 Baseline 2nd Row 1st Column c plus one half minus n p Subscript t 2nd Column equals minus z Subscript 1 minus beta Baseline left parenthesis n p Subscript t Baseline q Subscript t Baseline left parenthesis 1 minus StartFraction n Over upper N EndFraction right parenthesis right parenthesis Superscript 1 divided by 2 Baseline period EndLayout
c + 1 
2 − np0 = z1−α

np0q0

1 − n 
N
1/2 
c + 1 
2 −npt = −z1−β

nptqt

1 − n 
N
1/2 
. 
(11.3.2) 
Approximate solutions to n and c, n Superscript asteriskn∗, and c Superscript asteriskc∗ respectively, are 
n Superscript asterisk Baseline approximately equals StartFraction n 0 Over 1 plus n 0 divided by upper N EndFraction comman∗ ∼=
n0 
1 + n0/N ,
(11.3.3) 

11.3 
Approximate Determination of the Sampling Plan
405 
Table 11.6 Exact and approximate single-stage sampling plans for alpha equals beta equals 0.05α = β = 0.05, upper N equals 500N = 500, 
1000, 2000, p 0 equals 0.01p0 = 0.01, and p Subscript t Baseline equals 0.03pt = 0.03, 0.050.05 
p 0 equals 0.01p0 = 0.01, p Subscript t Baseline equals 0.03pt = 0.03
p 0 equals 0.01p0 = 0.01, p Subscript t Baseline equals 0.05pt = 0.05 
N
Method
n
c
ModifyingAbove alpha With caretˆα
ModifyingAbove beta With caret ˆβ
n
c
ModifyingAbove alpha With caretˆα
ModifyingAbove beta With caretˆβ 
500
Exact
254
4
0.033
0.050
139
3
0.023
0.050 
Approx.
248
4
0.029
0.060
127
2
0.107
0.026 
1000
Exact
355
6
0.028
0.050
146
3
0.045
0.049 
Approx.
330
5
0.072
0.036
146
3
0.045
0.049 
2000
Exact
414
7
0.038
0.049
176
4
0.026
0.050 
Approx.
396
6
0.082
0.032
157
3
0.066
0.037 
where 
n 0 equals StartFraction left parenthesis z Subscript 1 minus alpha Baseline StartRoot p 0 q 0 EndRoot plus z Subscript 1 minus beta Baseline StartRoot p Subscript t Baseline q Subscript t Baseline EndRoot right parenthesis squared Over left parenthesis p Subscript t Baseline minus p 0 right parenthesis squared EndFraction comman0 = (z1−α √p0q0 + z1−β √ptqt)2 
(pt − p0)2
,
(11.3.4) 
and 
c Superscript asterisk Baseline approximately equals n Superscript asterisk Baseline p 0 minus one half plus z Subscript 1 minus alpha Baseline StartRoot n Superscript asterisk Baseline p 0 q 0 left parenthesis 1 minus n Superscript asterisk Baseline divided by upper N right parenthesis EndRoot periodc∗ ∼= n∗p0 − 1 
2 + z1−α
	
n∗p0q0(1 − n∗/N).
(11.3.5) 
In Table 11.6, we present several single-stage sampling plans left parenthesis n comma c right parenthesis(n, c) and their 
approximations left parenthesis n Superscript asterisk Baseline comma c Superscript asterisk Baseline right parenthesis(n∗, c∗). We provide also the corresponding attained risk levels ModifyingAbove alpha With caretˆα 
and ModifyingAbove beta With caretˆβ. We see that the approximation provided for n and c yields risk levels which 
are generally close to the nominal ones. 
from mistat.acceptanceSampling import findPlanApprox 
def attainedRiskLevels(plan, p0, pt): 
hat_alpha = 1 - stats.hypergeom(N, int(p0 * N), plan.n).cdf(plan.c) 
hat_beta = stats.hypergeom(N, int(pt * N), plan.n).cdf(plan.c) 
return np.array([hat_alpha, hat_beta]) 
print('Exact results (p0=0.01, pt=0.03)') 
for N in (500, 1000, 2000): 
plan = findPlan(PRP=[0.01, 0.95], CRP=[0.03, 0.05], oc_type='hypergeom', N=N) 
print(N, plan, attainedRiskLevels(plan, 0.01, 0.03).round(3)) 
print('Approximate results (p0=0.01, pt=0.03)') 
for N in (500, 1000, 2000): 
plan = findPlanApprox(PRP=[0.01, 0.95], CRP=[0.03, 0.05], N=N) 
print(N, plan, attainedRiskLevels(plan, 0.01, 0.03).round(3)) 
print('Exact results (p0=0.01, pt=0.05)') 
for N in (500, 1000, 2000): 
plan = findPlan(PRP=[0.01, 0.95], CRP=[0.05, 0.05], oc_type='hypergeom', N=N) 
print(N, plan, attainedRiskLevels(plan, 0.01, 0.05).round(3)) 
print('Approximate results (p0=0.01, pt=0.05)') 
for N in (500, 1000, 2000): 
plan = findPlanApprox(PRP=[0.01, 0.95], CRP=[0.05, 0.05], N=N) 
print(N, plan, attainedRiskLevels(plan, 0.01, 0.05).round(3)) 

406
11 
Sampling Plans for Batch and Sequential Inspection 
Exact results (p0=0.01, pt=0.03) 
500 Plan(n=254, c=4, r=5) [0.033 0.05 ] 
1000 Plan(n=355, c=6, r=7) [0.028 0.05 ] 
2000 Plan(n=414, c=7, r=8) [0.038 0.049] 
Approximate results (p0=0.01, pt=0.03) 
500 Plan(n=248, c=4, r=5) [0.029 0.06 ] 
1000 Plan(n=330, c=5, r=6) [0.072 0.036] 
2000 Plan(n=396, c=6, r=7) [0.082 0.032] 
Exact results (p0=0.01, pt=0.05) 
500 Plan(n=139, c=3, r=4) [0.023 0.05 ] 
1000 Plan(n=146, c=3, r=4) [0.045 0.049] 
2000 Plan(n=176, c=4, r=5) [0.026 0.05 ] 
Approximate results (p0=0.01, pt=0.05) 
500 Plan(n=127, c=2, r=3) [0.107 0.026] 
1000 Plan(n=146, c=3, r=4) [0.045 0.049] 
2000 Plan(n=157, c=3, r=4) [0.066 0.037] 
11.4 Double Sampling Plans for Attributes 
A double sampling plan for attributes is a two-stage procedure. In the ﬁrst stage, a 
random sample of size n 1n1 is drawn, without replacement, from the lot. Let upper X 1X1 denote 
the number of defective items in this ﬁrst stage sample. Then the rules for the second 
stage are the following: if upper X 1 less than or equals c 1X1 ≤ c1, sampling terminates and the lot is accepted; if 
upper X 1 greater than or equals c 2X1 ≥ c2, sampling terminates and the lot is rejected; and if upper X 1X1 is between c 1c1 and c 2c2, 
a second stage random sample, of size n 2n2, is drawn, without replacement, from the 
remaining items in the lot. Let upper X 2X2 be the number of defective items in this second 
stage sample. Then, if upper X 1 plus upper X 2 less than or equals c 3X1 + X2 ≤ c3, the lot is accepted, and if upper X 1 plus upper X 2 greater than c 3X1 + X2 > c3, the  
lot is rejected. 
Generally, if there are very few (or very many) defective items in the lot, the 
decision to accept or reject the lot can be reached after the ﬁrst stage of sampling. 
Since the ﬁrst stage samples are smaller than those needed in a single-stage sampling 
, a considerable saving in inspection cost may be attained. 
In this type of sampling plan, there are ﬁve parameters to select, namely, n 1n1, 
n 2n2, c 1c1, c 2c2, and c 3c3. Variations in the values of these parameters affect the operating 
characteristics of the procedure, as well as the expected number of observations 
required (i.e., the total sample size). Theoretically, we could determine the optimal 
values of these ﬁve parameters by imposing ﬁve independent requirements on the 
OC function and the function of expected total sample size, called the Average 
Sample Number or ASN function, at various values of p. However, to simplify this 
procedure, it is common practice to set n 2 equals 2 n 1n2 = 2n1 and c 2 equals c 3 equals 3 c 1c2 = c3 = 3c1. This reduces 
the problem to that of selecting just n 1n1 and c 1c1. Every such selection will specify a 
particular double sampling plan. For example, if the lot consists of upper N equals 150N = 150 items 
and we choose a plan with n 1 equals 20n1 = 20, n 2 equals 40n2 = 40, c 1 equals 2c1 = 2, and c 2 equals c 3 equals 6c2 = c3 = 6, we will 
achieve certain properties. On the other hand, if we set n 1 equals 20n1 = 20, n 2 equals 40n2 = 40, c 1 equals 1c1 = 1, 
and c 2 equals c 3 equals 3c2 = c3 = 3, the plan will have different properties. 
The formula of the OC function associated with a double sampling plan 
left parenthesis n 1 comma n 2 comma c 1 comma c 2 comma c 3 right parenthesis(n1, n2, c1, c2, c3) is 

11.4 
Double Sampling Plans for Attributes
407 
StartLayout 1st Row 1st Column OC left parenthesis p right parenthesis 2nd Column equals upper H left parenthesis c 1 semicolon upper N comma upper M Subscript p Baseline comma n 1 right parenthesis plus 2nd Row 1st Column Blank 2nd Column sigma summation Underscript j equals c 1 plus 1 Overscript c 2 minus 1 Endscripts h left parenthesis j semicolon upper N comma upper M Subscript p Baseline comma n 1 right parenthesis upper H left parenthesis c 3 minus j semicolon upper N minus n 1 comma upper M Subscript p Baseline minus j comma n 2 right parenthesis comma EndLayout
OC(p) = H(c1; N, Mp, n1) + 
c2−1

j=c1+1 
h(j; N, Mp, n1)H(c3 −j; N −n1, Mp −j, n2), 
(11.4.1) 
where upper M Subscript p Baseline equals left bracket upper N p right bracketMp = [Np]. Obviously, we must have c 2 greater than or equals c 1 plus 2c2 ≥c1+2, for otherwise the plan is a 
single-stage plan. The probability upper Pi left parenthesis p right parenthesis(p) of stopping after the ﬁrst stage of sampling 
is 
StartLayout 1st Row 1st Column upper Pi left parenthesis p right parenthesis 2nd Column equals upper H left parenthesis c 1 semicolon upper N comma upper M Subscript p Baseline comma n 1 right parenthesis plus 1 minus upper H left parenthesis c 2 minus 1 semicolon upper N comma upper M Subscript p Baseline comma n 1 right parenthesis 2nd Row 1st Column Blank 2nd Column equals 1 minus left bracket upper H left parenthesis c 2 minus 1 semicolon upper N comma upper M Subscript p Baseline comma n 1 right parenthesis minus upper H left parenthesis c 1 semicolon upper N comma upper M Subscript p Baseline comma n 1 right parenthesis right bracket period EndLayout
(p) = H(c1; N, Mp, n1) + 1 −H(c2 −1; N, Mp, n1) 
= 1 −[H(c2 −1; N, Mp, n1) − H(c1; N, Mp, n1)]. 
(11.4.2) 
The expected total sample size, ASN, is given by the formula 
StartLayout 1st Row 1st Column ASN left parenthesis p right parenthesis 2nd Column equals n 1 upper Pi left parenthesis p right parenthesis plus left parenthesis n 1 plus n 2 right parenthesis left parenthesis 1 minus upper Pi left parenthesis p right parenthesis right parenthesis 2nd Row 1st Column Blank 2nd Column equals n 1 plus n 2 left bracket upper H left parenthesis c 2 minus 1 semicolon upper N comma upper M Subscript p Baseline comma n 1 right parenthesis minus upper H left parenthesis c 1 semicolon upper N comma upper M Subscript p Baseline comma n 1 right parenthesis right bracket period EndLayout
ASN(p) = n1(p) + (n1 + n2)(1 −(p)) 
= n1 + n2[H(c2 − 1; N, Mp, n1) − H(c1; N, Mp, n1)]. 
(11.4.3) 
In Table 11.7, we present the OC function and the ASN function for the double 
sampling plan left parenthesis 20 comma 40 comma 2 comma 6 comma 6 comma right parenthesis(20, 40, 2, 6, 6, ), for a lot of size upper N equals 150N = 150. 
We see from Table 11.7 that the double sampling plan illustrated here is not 
stringent. The probability of accepting a lot with 10% defectives is 0.80 and the 
probability of accepting a lot with 15% defectives is 0.40. If we consider the plan 
left parenthesis 20 comma 40 comma 1 comma 3 comma 3 right parenthesis(20, 40, 1, 3, 3), a more stringent procedure is obtained, as shown in Table 11.8 and 
Fig. 11.2. The probability of accepting a lot having 10% defectives has dropped to 
0.32 and that of accepting a lot with 15% defectives has dropped to 0.15. Table 11.8 
shows that the ASN is 23.1 when p equals 0.025p = 0.025 (most of the time the sampling is 
terminated after the ﬁrst stage), and the ASN is 29.1 when p equals 0.15p = 0.15. The maximum 
ASN occurs around p equals 0.10p = 0.10. 
Table 11.7 The OC and ASN of a double sampling plan (20,40,2,6,6), upper N equals 150N = 150 
p
OC(p)
ASN(p) 
0.000
1.0000
20.0 
0.025
1.0000
20.3 
0.050
0.9969
22.9 
0.075
0.9577
26.6 
0.100
0.8011
32.6 
0.125
0.5871
38.3 
0.150
0.4000
42.7 
0.175
0.2930
44.6 
0.200
0.1891
45.3 
0.225
0.1184
44.1 
0.250
0.0715
41.4 
p
OC(p)
ASN(p) 
0.275
0.0477
38.9 
0.300
0.0268
35.2 
0.325
0.0145
31.6 
0.350
0.0075
28.4 
0.375
0.0044
26.4 
0.400
0.0021
24.3 
0.425
0.0009
22.7 
0.450
0.0004
21.6 
0.475
0.0002
21.1 
0.500
0.0001
20.6 

408
11 
Sampling Plans for Batch and Sequential Inspection 
Table 11.8 The OC and ASN for the double sampling plan (20,40,1,3,3), upper N equals 150N = 150 
p
OC(p)
ASN(p) 
0.000
1.0000
20.0 
0.025
0.9851
23.1 
0.050
0.7969
28.6 
0.075
0.6018
31.2 
0.100
0.3881
32.2 
0.125
0.2422
31.2 
0.150
0.1468
29.1 
0.175
0.0987
27.3 
0.200
0.0563
25.2 
0.225
0.0310
23.5 
0.250
0.0165
22.2 
p
OC(p)
ASN(p) 
0.275
0.0100
21.5 
0.300
0.0050
20.9 
0.325
0.0024
20.5 
0.350
0.0011
20.3 
0.375
0.0006
20.2 
0.400
0.0003
20.1 
0.425
0.0001
20.0 
0.450
0.0000
20.0 
0.475
0.0000
20.0 
0.500
0.0000
20.0 
Fig. 11.2 Comparison of double sampling plans (20,40,2,6,6) and (20,40,1,3,3), upper N equals 150N = 150 
To determine an acceptable double sampling plan for attributes, suppose, for 
example, that the population size is upper N equals 1000N = 1000. Deﬁne AQL equals 0.01= 0.01 and 
LQL equals= 0.03. If n 1 equals 200n1 = 200, n 2 equals 400n2 = 400, c 1 equals 3c1 = 3, c 2 equals 9c2 = 9, and c 3 equals 9c3 = 9, then 
OCleft parenthesis 0.01 right parenthesis equals 0.957(0.01) = 0.957 and OCleft parenthesis 0.03 right parenthesis equals 0.151(0.03) = 0.151. Thus, alpha equals 0.011α = 0.011 and beta equals 0.119β = 0.119. 
The double sampling plan with n 1 equals 120n1 = 120, n 2 equals 240n2 = 240, c 1 equals 0c1 = 0, and c 2 equals c 3 equals 7c2 = c3 = 7 
yields alpha equals 0.044α = 0.044 and beta equals 0.084β = 0.084. For the last plan, the expected sample sizes are 
ASNleft parenthesis 0.01 right parenthesis equals 288(0.01) = 288 and ASNleft parenthesis 0.03 right parenthesis equals 336(0.03) = 336. These expected sample sizes are smaller 
than the required sample size of n equals 355n = 355 in a single-stage plan. Moreover, with 
high probability, if p less than or equals p 0p ≤ p0 or p greater than or equals p Subscript tp ≥ pt, the sampling will terminate after the ﬁrst 
stage with only n 1 equals 120n1 = 120 observations. This is a factor of threefold decrease in the 
sample size, over the single sampling plan. There are other double sampling plans 
which can do even better. 

11.4 
Double Sampling Plans for Attributes
409 
If the lot is very large and we use large samples in stage one and stage two, the 
formulae for the OC and ASN function can be approximated by 
StartLayout 1st Row OC left parenthesis p right parenthesis approximately equals upper Phi left parenthesis StartFraction c 1 plus 1 divided by 2 minus n 1 p Over left parenthesis n 1 p q left parenthesis 1 minus n 1 divided by upper N right parenthesis right parenthesis Superscript 1 divided by 2 Baseline EndFraction right parenthesis 2nd Row plus sigma summation Underscript j equals c 1 plus 1 Overscript c 2 minus 1 Endscripts left bracket upper Phi left parenthesis StartFraction j plus 1 divided by 2 minus n 1 p Over left parenthesis n 1 p q left parenthesis 1 minus n 1 divided by upper N right parenthesis right parenthesis Superscript 1 divided by 2 Baseline EndFraction right parenthesis minus upper Phi left parenthesis StartFraction j minus 1 divided by 2 minus n 1 p Over left parenthesis n 1 p q left parenthesis 1 minus n 1 divided by upper N right parenthesis right parenthesis Superscript 1 divided by 2 Baseline EndFraction right parenthesis right bracket 3rd Row dot upper Phi left parenthesis StartFraction c 3 minus j plus 1 divided by 2 minus n 2 p Over left parenthesis n 2 p q left parenthesis 1 minus StartFraction n 2 Over upper N n 1 EndFraction right parenthesis right parenthesis Superscript 1 divided by 2 Baseline EndFraction right parenthesis comma EndLayoutOC(p) ∼= 

c1 + 1/2 − n1p 
(n1pq(1 − n1/N))1/2

+ 
c2−1

j=c1+1



j + 1/2 − n1p 
(n1pq(1 −n1/N))1/2

−

j − 1/2 − n1p 
(n1pq(1 − n1/N))1/2

· 
⎛ 
⎜ 
⎝ c3 − j + 1/2 − n2p

n2pq

1 − n2 
Nn1
1/2 
⎞ 
⎟ 
⎠ ,
(11.4.4) 
and 
StartLayout 1st Row ASN left parenthesis p right parenthesis equals n 1 plus n 2 left bracket upper Phi left parenthesis StartFraction c 2 minus 1 divided by 2 minus n 1 p Over n 1 p q left parenthesis 1 minus n 1 divided by upper N right parenthesis right parenthesis Superscript 1 divided by 2 Baseline EndFraction right parenthesis 2nd Row minus upper Phi left parenthesis StartFraction c 1 plus 1 divided by 2 minus n 1 p Over left parenthesis n 1 p q left parenthesis 1 minus n 1 divided by upper N right parenthesis right parenthesis Superscript 1 divided by 2 Baseline EndFraction right parenthesis right bracket period EndLayoutASN(p) = n1 + n2



c2 − 1/2 −n1p 
n1pq(1 − n1/N))1/2

−

c1 + 1/2 − n1p 
(n1pq(1 −n1/N))1/2

.
(11.4.5) 
In Table 11.9, we present the OC and the ASN functions for double sampling 
from a population of size upper N equals 1000N = 1000, when the parameters of the plan are 
left parenthesis 100 comma 200 comma 3 comma 6 comma 6 comma right parenthesis(100, 200, 3, 6, 6, ). The exact values thus obtained are compared to the values 
obtained from the large sample approximation formulae. In the next section the 
idea of double sampling is generalized in an attempt to reach acceptance decisions 
quicker and therefore at reduced costs. 
Table 11.9 The exact and 
approximate OC and ASN 
functions for the double 
sampling plan 
(100,200,3,6,6), upper N equals 1000N = 1000 
OC(p)
ASN(p) 
p
Exact 
Approx. 
Exact 
Approx. 
0.01 
0.998 
0.999
102.4 
100.8 
0.02 
0.897 
0.896
124.1 
125.0 
0.03 
0.658 
0.640
156.4 
163.5 
0.04 
0.421 
0.401
175.6 
179.2 
0.05 
0.243 
0.236
174.7 
172.3 
0.06 
0.130 
0.134
160.6 
155.7 
0.07 
0.064 
0.074
142.7 
138.7 
0.08 
0.030 
0.040
127.1 
125.1 
0.09 
0.014 
0.021
115.8 
115.5 

410
11 
Sampling Plans for Batch and Sequential Inspection 
11.5 Sequential Sampling and A/B Testing 
The A/B testing is a common practice these days of testing which treatment or 
action, A or B, is preferred by a customer. Two alternative actions are presented 
before a customer, who should choose one of the two in order to maximize the 
expected reward. The reward is not certain in any case, and the probability of reward 
is unknown. This problem is similar to the classical “Two-Armed Bandit” problem. 
A gambler is standing in front of a slot machine and has the opportunity to try his 
luck in N trials. If he pulls the left hand, the probability of reward is p 1p1, and the 
probability of reward on the right hand is p 2p2. If the probabilities of success are 
known, the gambler will always pull the hand having the largest probability. What 
should be his strategy when the probabilities are unknown? 
Much research was done on this problem in the 70s and the 80s. The reader is 
referred to the books of Gittins et al. (2011) and Berry and Fristedt (1985). In this 
section we start with the One-Armed Bandit (OAB) for Bernoulli trials and then 
discuss the Two-Armed Bandit (TAB) problem. 
11.5.1 
The One-Armed Bernoulli Bandits 
The one-armed Bernoulli bandit is a simpler case, where the probability of success 
in arm A is known, lamdaλ say. The probability p of success in arm B is unknown. It 
is clear that in this case we have to start with a sequence of trials on arm B (the 
learning phase) and move to arm A as soon as we are convinced that p less than lamdap <  λ. The  
trials are Bernoulli trials, which means all trials on arm A or arm B are independent. 
The results of each trial are binary (upper J equals 1J = 1 for success and upper J equals 0J = 0 for failure). The 
probabilities of success in all trials at the same arm are equal. 
Suppose that n trials have been performed on arm B. Let upper X Subscript n Baseline equals sigma summation Underscript j equals 1 Overscript n Endscripts upper J Subscript jXn = n 
j=1 Jj. The  
distribution of upper X Subscript nXn is binomial, upper B left parenthesis n comma p right parenthesisB(n, p). 
I. The Bayesian Strategy 
In a Bayesian framework, we start with a uniform prior distribution for p. The  
posterior distribution of p, given left parenthesis n comma upper X Subscript n Baseline right parenthesis(n, Xn), is upper B left parenthesis upper X Subscript n Baseline plus 1 comma n plus 1 minus upper X Subscript n Baseline right parenthesisB(Xn + 1, n  + 1 −Xn), i.e., 
StartLayout 1st Row upper P left brace p less than or equals xi vertical bar n comma upper X Subscript n Baseline right brace equals StartFraction 1 Over upper B left parenthesis upper X Subscript n Baseline plus 1 comma n plus 1 minus upper X Subscript n Baseline right parenthesis EndFraction integral Subscript 0 Superscript xi Baseline u Superscript upper X Super Subscript n Superscript Baseline left parenthesis 1 minus u right parenthesis Superscript n minus upper X Super Subscript n Superscript Baseline d u comma 2nd Row 0 less than xi less than 1 period EndLayoutP{p ≤ ξ|n, Xn} =
1 
B(Xn + 1, n + 1 − Xn)
 ξ 
0 
uXn (1 − u)n−Xn du, 
0 < ξ  <  1.
(11.5.1) 
The function upper B left parenthesis a comma b right parenthesis equals upper Gamma left parenthesis a right parenthesis upper Gamma left parenthesis b right parenthesis divided by upper Gamma left parenthesis a plus b right parenthesisB(a, b) = (a)(b)/(a+b) is called the complete beta function. 
The right-hand side of (11.5.1) is called the incomplete beta function ratio and is 
denoted as upper I Subscript x Baseline left parenthesis a comma b right parenthesisIx(a, b). The predictive distribution of upper J Subscript n plus 1Jn+1, given upper X Subscript nXn, is  
upper P left brace upper J Subscript n plus 1 Baseline equals 1 vertical bar upper X Subscript n Baseline right brace equals StartFraction upper X Subscript n Baseline plus 1 Over n plus 2 EndFraction periodP{Jn+1 = 1|Xn} =  Xn + 1 
n + 2 . 

11.5 
Sequential Sampling and A/B Testing
411 
The posterior probability that left brace p less than lamda right brace{p
<
 λ} is the lamdaλ-quantile of the above beta 
distribution, which is the incomplete beta function ratio upper I Subscript lamda Baseline left parenthesis upper X Subscript n Baseline plus 1 comma n plus 1 minus upper X Subscript n Baseline right parenthesisIλ(Xn + 1, n  + 1 − Xn). 
A relatively simple Bayesian stopping rule for arm B is the ﬁrst n greater than 
or equal to an initial sample size k, at which the posterior probability is greater than 
some speciﬁed value gammaγ , i.e., 
upper M Subscript gamma Baseline equals min left brace n greater than or equals k colon upper I Subscript lamda Baseline left parenthesis upper X Subscript n Baseline plus 1 comma n plus 1 minus upper X Subscript n Baseline right parenthesis greater than or equals gamma right brace periodMγ = min{n ≥ k : Iλ(Xn + 1, n  + 1 −Xn) ≥ γ }.
(11.5.2) 
That is, at the ﬁrst time on arm B in which (11.5.2) is satisﬁed, one moves to arm A 
and stays there for the rest of the upper N minus nN − n trials. 
Example 11.1 In the present example, we simulate a Bayesian OAB Bernoulli 
process, in which lamda equals 0.5λ = 0.5 and upper N equals 50N = 50. If we choose to play all the N trials on 
arm A, our expected reward is upper N lamda equals 25Nλ = 25 (reward units). On the other hand, in the 
OAB, we start with an initial sample of size k equals 10k = 10 on arm B. We also wish to 
switch from arm B to arm A with conﬁdence probability gamma equals 0.95γ = 0.95. We illustrate the 
random process with two cases: 
Case (i) p equals 0.6p = 0.6: 
This probability for arm B of p equals 0.6p = 0.6 is unknown. We make 
ﬁrst k trials on arm B and get at random k results according to binomial 
upper B left parenthesis 10 comma 0.6 right parenthesisB(10, 0.6). Using  scipy, the 0.1-quantile of the binomial upper B left parenthesis 10 comma 0.6 right parenthesisB(10, 0.6) is 
print(f'0.1-quantile B(10,0.6): 
{stats.binom(10, 0.6).ppf(0.1)}') 
0.1-quantile B(10,0.6): 
4.0 
In 90% of the cases, we get 4 or more wins. Assume that we get 4 wins. The 
posterior distribution is in this case upper B left parenthesis 4 plus 1 comma 10 plus 1 minus 4 right parenthesis equals upper B left parenthesis 5 comma 7 right parenthesisB(4 + 1, 10 + 1 − 4) = B(5, 7). Using this 
posterior, the probability that nothing has changed is 
print(f'B(5,7)-cdf : 
{stats.beta(5, 7).cdf(0.5)}') 
B(5,7)-cdf : 
0.7255859375 
Thus we expect that in 90% of the possible results we stay in the k plus 1k + 1 trial at 
arm B. Notice that even if we have another loss, upper X 11 equals 4X11 = 4, we have upper B left parenthesis 4 plus 1 comma 11 plus 1 minus 4 right parenthesis equals upper B left parenthesis 5 comma 8 right parenthesisB(4+1, 11+ 
1 − 4) = B(5, 8) and therefore 
print(f'B(5,8)-cdf : 
{stats.beta(5, 8).cdf(0.5)}') 
B(5,8)-cdf : 
0.80615234375 
and we stay with arm B. Thus, with high probability, we will stay with arm B all 
the 50 trials, with an expected reward of 50 times 0.6 equals 3050 × 0.6 = 30. 
Case (ii) p equals 0.3p = 0.3: 
We might get at random 2 wins. 
np.random.seed(5) 
print(f'{stats.binom(10, 0.3).rvs(1)} wins') 
[2] wins 

412
11 
Sampling Plans for Batch and Sequential Inspection 
Table 11.10 Simulation 
estimates of the expected 
stopping time and the 
associated reward, for 
upper N equals 50N = 50, lamda equals 0.5λ = 0.5, k equals 10k = 10, 
gamma equals 0.95γ = 0.95, and number of runs 
upper N Subscript s Baseline equals 1000Ns = 1000 
p
upper E left brace upper M Subscript gamma Baseline right braceE{Mγ } 
std StartSet upper M Subscript gamma Baseline EndSetstd{Mγ } 
upper E left brace Reward right braceE{Reward} 
std StartSet Reward EndSetstd{Reward} 
0.40 
31.631 
16.938
21.950
2.016 
0.45 
39.078 
15.676
23.352
2.882 
0.50 
44.359 
12.756
25.360
3.395 
0.55 
46.846 
9.910
27.637
3.738 
0.60 
49.013 
5.791
30.404
3.665 
0.70 
49.882 
2.152
35.753
3.251 
In this case, upper B left parenthesis 2 plus 1 comma 10 plus 1 minus 2 right parenthesis equals upper B left parenthesis 3 comma 9 right parenthesisB(2 + 1, 10 + 1 −2) = B(3, 9) and therefore, 
print(f'B(3,9)-cdf : 
{stats.beta(3, 9).cdf(0.5)}') 
B(3,9)-cdf : 
0.96728515625 
We move immediately to arm A, with the expected reward of 2 plus 40 times 0.5 equals 222 + 40 × 0.5 = 22. 
In the following table we present the results of 1000 simulation runs. 
np.random.seed(1) 
N=50; lambda_=0.5; k0=10; gamma=0.95; Ns=1000 
results = [] 
for p in (0.4, 0.45, 0.5, 0.55, 0.6, 0.7): 
r = acceptanceSampling.simulateOAB(N, p, lambda_, k0, gamma, Ns) 
results.append({ 
'p': p, 
'Mgamma_mean': r.mgamma.mean, 
'Mgamma_std': r.mgamma.std, 
'Reward_mean': r.reward.mean, 
'Reward_std': r.reward.std, 
}) 
In each run we recorded the mean value of the stopping time upper M Subscript gammaMγ , its standard 
deviation, and the mean value of the expected reward and its standard deviation. 
The computations were done with the program acceptanceSampling.simulateOAB 
and are summarized in Table 11.10. 
We see in Table 11.10 that, according to the present strategy, the cost of ignorance 
about p (loss of reward) is pronounced only if p less than 0.5p <  0.5. For example, if we knew 
that p equals 0.4p = 0.4, we would have started with arm A, with an expected reward of 25 
rather than 21.950. The loss in expected reward when p equals 0.45p = 0.45 is not signiﬁcant. 
The expected reward when p greater than or equals 0.5p ≥ 0.5 is not signiﬁcantly different from what we could 
achieve if we knew p.
black medium square■
II. The Bayesian Optimal Strategy for Bernoulli Trials 
As in the previous strategy, we assume that the prior distribution of p is uniform on 
(0,1). The optimal strategy is determined by Dynamic Programming. The principle 
of Dynamic Programming is to optimize the future possible trials, irrespective of 
what has been done in the past. We consider here as before a truncated game, 
in which only N trials are allowed. Thus we start with the last trial and proceed 
inductively backward. 

11.5 
Sequential Sampling and A/B Testing
413 
(i) Suppose we have already done upper N minus 1N −1 trials. The expected reward for the Nth 
trial is 
StartLayout 1st Row 1st Column upper R Subscript upper N Baseline left parenthesis upper X Subscript upper N minus 1 Baseline right parenthesis 2nd Column equals upper I left brace if arm upper A is chosen right brace 2nd Row 1st Column Blank 2nd Column equals lamda plus upper I left brace if arm upper B is chosen right brace StartFraction upper X Subscript upper N minus 1 Baseline plus 1 Over upper N plus 1 EndFraction period EndLayout
RN(XN−1) = I{if arm A is chosen} 
= λ + I{if arm B is chosen}XN−1 + 1 
N + 1 
. 
(11.5.3) 
Thus, the maximal predictive expected reward for the last trial is 
StartLayout 1st Row 1st Column rho Superscript left parenthesis 0 right parenthesis Baseline left parenthesis upper X Subscript upper N minus 1 Baseline right parenthesis 2nd Column equals max left brace lamda comma StartFraction upper X Subscript upper N minus 1 Baseline plus 1 Over upper N plus 1 EndFraction right brace EndLayout StartLayout 2nd Row 1st Column Blank 2nd Column equals lamda upper I left brace upper X Subscript upper N minus 1 Baseline less than lamda left parenthesis upper N plus 1 right parenthesis minus 1 right brace 3rd Row 1st Column Blank 2nd Column plus StartFraction upper X Subscript upper N minus 1 Baseline plus 1 Over upper N plus 1 EndFraction upper I left brace upper X Subscript upper N minus 1 Baseline greater than or equals lamda left parenthesis upper N plus 1 right parenthesis minus 1 right brace period EndLayoutρ(0) (XN−1) = max

λ, XN−1 + 1 
N + 1

(11.5.4) 
= λI{XN−1 < λ(N  + 1) − 1} 
+ XN−1 + 1 
N + 1 
I{XN−1 ≥λ(N + 1) − 1}.
(11.5.5) 
(ii) After upper N minus 2N −2 trials, if we are at arm A, we stay there, but if we are at arm B our 
predictive reward is 
StartLayout 1st Row upper R Subscript upper N minus 1 Baseline left parenthesis upper X Subscript upper N minus 2 Baseline right parenthesis equals 2 lamda upper I left brace if we are at arm upper A right brace plus StartFraction upper X Subscript upper N minus 2 Baseline plus 1 Over upper N EndFraction 2nd Row plus upper E left brace rho Superscript left parenthesis 0 right parenthesis Baseline left parenthesis upper X Subscript upper N minus 2 Baseline plus upper J Subscript upper N minus 1 Baseline right parenthesis vertical bar upper X Subscript upper N minus 2 Baseline right brace upper I left brace if we chose arm upper B right brace period EndLayoutRN−1(XN−2) = 2λI{if we are at arm A} +  XN−2 + 1 
N 
+ E{ρ(0) (XN−2 + JN−1)|XN−2}I{if we chose arm B}.
(11.5.6) 
The maximal predictive expected reward is 
StartLayout 1st Row rho Superscript left parenthesis 1 right parenthesis Baseline left parenthesis upper X Subscript upper N minus 2 Baseline right parenthesis equals max left brace 2 lamda comma StartFraction upper X Subscript upper N minus 2 Baseline plus 1 Over upper N EndFraction rho Superscript left parenthesis 0 right parenthesis Baseline left parenthesis upper X Subscript upper N minus 2 Baseline plus 1 right parenthesis 2nd Row q u a d plus StartFraction upper N minus 1 minus upper X Subscript upper N minus 1 Baseline Over upper N EndFraction rho Superscript left parenthesis 0 right parenthesis Baseline left parenthesis upper X Subscript upper N minus 2 Baseline right parenthesis plus StartFraction upper X Subscript upper N minus 2 Baseline plus 1 Over upper N EndFraction right brace period EndLayoutρ(1) (XN−2) = max

2λ, XN−2 + 1 
N 
ρ(0) (XN−2 + 1) 
+N − 1 − XN−1 
N 
ρ(0) (XN−2) + XN−2 + 1 
N

.
(11.5.7) 
(iii) By backward induction, we deﬁne for all 1 less than or equals n less than or equals upper N minus 21 ≤ n ≤ N − 2 
StartLayout 1st Row rho Superscript left parenthesis n right parenthesis Baseline left parenthesis upper X Subscript upper N minus n minus 1 Baseline right parenthesis equals max left brace left parenthesis n plus 1 right parenthesis lamda comma StartFraction upper X Subscript upper N minus n minus 1 Baseline plus 1 Over upper N minus n plus 1 EndFraction rho Superscript left parenthesis n minus 1 right parenthesis Baseline left parenthesis upper X Subscript upper N minus n minus 1 Baseline plus 1 right parenthesis 2nd Row q u a d plus StartFraction upper N minus n minus upper X Subscript upper N minus n minus 1 Baseline Over upper N minus n plus 1 EndFraction rho Superscript left parenthesis n minus 1 right parenthesis Baseline left parenthesis upper X Subscript upper N minus n minus 1 Baseline right parenthesis plus StartFraction upper X Subscript upper N minus n minus 1 Baseline plus 1 Over upper N minus n plus 1 EndFraction right brace EndLayoutρ(n) (XN−n−1) = max

(n + 1)λ, XN−n−1 + 1 
N −n + 1 ρ(n−1) (XN−n−1 + 1) 
+N − n −XN−n−1 
N − n + 1 
ρ(n−1) (XN−n−1) + XN−n−1 + 1 
N −n + 1

(11.5.8) 
and 
StartLayout 1st Row rho Superscript left parenthesis upper N minus 2 right parenthesis Baseline left parenthesis upper X 1 right parenthesis equals max left brace left parenthesis upper N minus 1 right parenthesis lamda comma StartFraction upper X 1 plus 1 Over 3 EndFraction rho Superscript left parenthesis upper N minus 2 right parenthesis Baseline left parenthesis upper X 1 plus 1 right parenthesis 2nd Row q u a d plus StartFraction 2 minus upper X 1 Over 3 EndFraction rho Superscript left parenthesis upper N minus 2 right parenthesis Baseline left parenthesis upper X 1 right parenthesis plus StartFraction upper X 1 plus 1 Over 3 EndFraction right brace period EndLayoutρ(N−2) (X1) = max

(N − 1)λ, X1 + 1 
3 
ρ(N−2) (X1 + 1) 

414
11 
Sampling Plans for Batch and Sequential Inspection 
Table 11.11 Values of rho Superscript left parenthesis n right parenthesis Baseline left parenthesis upper X Subscript upper N minus n minus 1 Baseline right parenthesisρ(n) (XN−n−1), for upper N equals 10N = 10, n equals 0 comma ellipsis comma 9n = 0, . . . , 9 
n 
0
1
2
3
4
5
6
7
8
9
10 
0 
0.500 
0.500 
0.500 
0.500 
0.500 
0.545 
0.636 
0.727 
0.818 
0.909 
1.000 
1 
1.000 
1.000 
1.000 
1.000 
1.023 
1.200 
1.400 
1.600 
1.800 
2.000 
2 
1.500 
1.500 
1.500 
1.500 
1.677 
2.000 
2.333 
2.667 
3.000 
3 
2.000 
2.000 
2.000 
2.088 
2.504 
3.000 
3.500 
4.000 
4 
2.500 
2.500 
2.500 
2.897 
3.573 
4.286 
5.000 
5 
3.000 
3.000 
3.199 
4.014 
5.000 
6.000 
6 
3.500 
3.500 
4.288 
5.603 
7.000 
7 
4.000 
4.394 
6.024 
8.000 
8 
4.500 
6.147 
9.000 
9 
5.824 
10.000 
+2 − X1 
3 
ρ(N−2) (X1) + X1 + 1 
3

.
(11.5.9) 
Finally, since the procedure starts at arm B and upper P left brace upper X 1 equals 0 right brace equals upper P left brace upper X 1 equals 1 right brace equals 1 divided by 2P{X1 = 0} =  P{X1 = 1} =  
1/2, we get that the maximal expected reward is 
rho Superscript left parenthesis upper N minus 1 right parenthesis Baseline equals StartFraction rho Superscript left parenthesis upper N minus 2 right parenthesis Baseline left parenthesis 0 right parenthesis plus rho Superscript left parenthesis upper N minus 2 right parenthesis Baseline left parenthesis 1 right parenthesis Over 2 EndFraction periodρ(N−1) = ρ(N−2) (0) + ρ(N−2) (1) 
2 
. 
Example 11.2 In the following example we illustrate the values of rho Superscript left parenthesis n right parenthesis Baseline left parenthesis upper X Subscript upper N minus n minus 1 Baseline right parenthesisρ(n) (XN−n−1), 
for given left parenthesis upper N comma lamda right parenthesis(N, λ), where n designates the number of available future trials. These 
values are computed with the function optimalOAB from the mistat package. 
Since the table is too big, we illustrate these for upper N equals 10N = 10. We get the values shown 
in Table 11.11. 
from mistat.acceptanceSampling import optimalOAB 
result = optimalOAB(10, 0.5) 
print(f'Case (10, 0.5): {result.max_reward:.3f}') 
print(f'Case (50, 0.5): {optimalOAB(50, 0.5).max_reward:.3f}') 
Case (10, 0.5): 7.912 
Case (50, 0.5): 40.175 
Accordingly, the maximal predictive reward in following this strategy is left parenthesis 5.824 plus 10.000 right parenthesis divided by 2 equals 7.912(5.824+ 
10.000)/2 = 7.912. For each n, we stay in arm A as long as the maximal reward 
is left parenthesis n plus 1 right parenthesis times 0.5(n + 1) × 0.5. Since the table is quite big, the function optimalOAB returns the 
maximal predictive reward in addition to the table. To compare this with the case 
of Table 11.10, optimalOAB yields for N=50 and lamda equals 0.5λ = 0.5 the maximal reward of 
40.175.
black medium square■

11.5 
Sequential Sampling and A/B Testing
415 
The two-armed bandit (TAB) is the case when the probabilities of reward at 
the two arms are unknown. We consider here the truncated game, in which 
N trials are allowed at both arms. The sufﬁcient statistics after n trials are 
left parenthesis upper N Superscript left parenthesis 1 right parenthesis Baseline left parenthesis n right parenthesis comma upper X Subscript upper N Sub Superscript left parenthesis 1 right parenthesis Subscript Baseline comma upper N Superscript left parenthesis 2 right parenthesis Baseline left parenthesis n right parenthesis comma upper Y Subscript upper N Sub Superscript left parenthesis 2 right parenthesis Subscript Baseline right parenthesis(N(1) (n), XN(1), N(2) (n), YN(2)), where upper N Superscript left parenthesis 1 right parenthesis Baseline left parenthesis n right parenthesisN(1) (n) and upper N Superscript left parenthesis 2 right parenthesis Baseline left parenthesis n right parenthesisN(2) (n) are the number of trials 
on arm A and arm B for the ﬁrst n trials, respectively. upper X Subscript upper N Sub Superscript left parenthesis 1 right parenthesisXN(1) and upper Y Subscript upper N Sub Superscript left parenthesis 2 right parenthesisYN(2) are the number 
of successes at arms A and B. The optimal strategy is much more complicated than 
in the OAB case. If the two arms act independently, the situation is less complicated. 
The rewards from the two arms are cumulative. In principle one could ﬁnd an 
optimal strategy by dynamic programming. It is however much more complicated 
than in the OAB case. Gittins and Jones (1974) proved that one can compute an 
index for each arm separately and apply in the next trial the arm having the maximal 
index value. We provide here a strategy which yields very good results for a large 
total number of trials N. This strategy starts with k trials on any arm, say arm A. If 
all the k trials yield k successes, i.e., upper X Subscript k Baseline equals kXk = k, we continue all the rest upper N minus kN −k trials 
on arm A. If the ﬁrst m equals left floor k divided by 2 right floorm = ⌊k/2⌋trials are all failures, i.e., upper X Subscript m Baseline equals 0Xm = 0, we immediately 
switch to arm B. We then compute the posterior estimator of the probability of 
success in arm A, p Subscript upper ApA, and use it in the dynamic programming for one arm known 
probability (function optimalOAB) starting at arm B. 
optimalOAB(45, 0.143).max_reward 
33.870065529505496 
Example 11.3 Consider the case of TAB with upper N equals 50N = 50 and k equals 10k = 10. If  upper X 5 equals 0X5 = 0, we  
switch to arm B. According to the b e t a left parenthesis 1 comma 1 right parenthesisbeta(1, 1) prior for p Subscript upper ApA, the Bayesian estimator is 
ModifyingAbove p With caret Subscript upper A Baseline equals 1 divided by 7 equals 0.143 ˆpA = 1/7 = 0.143 with predictive expected reward of optimalOAB left parenthesis 45 comma 0.143 right parenthesis equals 33.87optimalOAB(45, 0.143) = 
33.87. On the other hand, if upper X 10 equals 10X10 = 10, we stay at arm A till the end, with expected 
reward of 10 plus 40 times left parenthesis 11 divided by 12 right parenthesis equals 46.6710 + 40 × (11/12) = 46.67. In Table 11.12, we present all the other 
possibilities.
black medium square■
There is ample literature on the armed-bandits problem, for example, see 
Chapter 8 of Zacks (2009). 
Table 11.12 Expected 
reward when upper N equals 50N = 50 and 
k equals 10k = 10 
upper X 10X10 
upper E left brace Reward right braceE{Reward} 
1
31.150 
2
32.413 
3
33.819 
4
35.380 
5
37.093 
6
38.970 
7
41.033 
8
43.249 
9
45.667 
11.5.2 
Two-Armed Bernoulli Bandits 

416
11 
Sampling Plans for Batch and Sequential Inspection 
11.6 Acceptance Sampling Plans for Variables 
It is sometimes possible to determine whether an item is defective or not by 
performing a measurement on the item which provides a value of a continuous 
random variable X and comparing it to speciﬁcation limits. For example, in 
Chapter 1 of Modern Statistics (Kenett et al. 2022b), we discussed measuring the 
strength of yarn. In this case a piece of yarn is deemed defective if its strength X is 
less than xiξ, where xiξ is the required minimum strength, i.e., its lower speciﬁcation 
limit. The proportion of defective yarn pieces in the population (or very large lot) is 
the probability that upper X less than or equals xiX ≤ ξ. 
Suppose now that X has a normal distribution with mean muμ and variance sigma squaredσ 2. (If 
the distribution is not normal, we can often reduce it to a normal one by a proper 
transformation.) Accordingly, the proportion of defectives in the population is 
p equals upper Phi left parenthesis StartFraction xi minus mu Over sigma EndFraction right parenthesis periodp = 

ξ − μ 
σ

.
(11.6.1) 
We have to decide whether p less than or equals p 0p ≤ p0 (= AQL) or p greater than or equals p Subscript tp ≥ pt (= LQL), in order to 
accept or reject the lot. 
Let x Subscript pxp represent the pth quantile of a normal distribution with mean muμ and 
standard deviation sigmaσ. Then 
x Subscript p Baseline equals mu plus z Subscript p Baseline sigma periodxp = μ + zpσ.
(11.6.2) 
If it were the case that x Subscript p 0 Baseline greater than or equals xixp0 ≥ ξ, we should accept the lot since the proportion 
of defectives is less than p 0p0. Since we do not know muμ and sigmaσ, we must make our 
decision on the basis of estimates from a sample of n measurements. We decide to 
reject the lot if 
upper X overbar minus k upper S less than xi¯X − kS < ξ 
and accept the lot if 
upper X overbar minus k upper S greater than or equals xi period¯X − kS ≥ ξ. 
Here, upper X overbar¯X and S are the usual sample mean and standard deviation, respectively. The 
factor k is chosen so that the producer’s risk (the risk of rejecting a good lot) does 
not exceed alphaα. The values of the factor k are given approximately by the formula 
k approaches the limit t Subscript 1 minus alpha comma p 0 comma n Baseline commak .= t1−α,p0,n, 
where 
t Subscript 1 minus a comma b comma n Baseline equals StartFraction z Subscript 1 minus b Baseline Over 1 minus z Subscript 1 minus a Superscript 2 Baseline divided by 2 n EndFraction plus StartStartFraction z Subscript 1 minus a Baseline left parenthesis 1 plus StartFraction z Subscript b Superscript 2 Baseline Over 2 EndFraction minus StartFraction z Subscript 1 minus a Superscript 2 Baseline Over 2 n EndFraction right parenthesis Superscript 1 divided by 2 Baseline OverOver StartRoot n EndRoot left parenthesis 1 minus StartFraction z Subscript 1 minus a Superscript 2 Baseline Over 2 n EndFraction right parenthesis EndEndFraction periodt1−a,b,n =
z1−b 
1 − z2 
1−a/2n + 
z1−a

1 + z2 
b 
2 − 
z2 
1−a 
2n
1/2 
√n

1 − 
z2 
1−a 
2n

.
(11.6.3) 

11.6 
Acceptance Sampling Plans for Variables
417 
The OC function of such a test is given approximately (for large samples) by 
OC left parenthesis p right parenthesis almost equals 1 minus upper Phi left parenthesis StartFraction left parenthesis z Subscript p Baseline plus k right parenthesis divided by StartRoot n EndRoot Over left parenthesis 1 plus k squared divided by 2 right parenthesis Superscript 1 divided by 2 Baseline EndFraction right parenthesis commaOC(p) ≈ 1 −

 (zp + k)/√n 
(1 + k2/2)1/2

,
(11.6.4) 
where k equals t Subscript 1 minus alpha comma p comma nk = t1−α,p,n. We can thus determine n and k so that 
OC left parenthesis p 0 right parenthesis equals 1 minus alphaOC(p0) = 1 − α 
and 
OC left parenthesis p Subscript t Baseline right parenthesis equals beta periodOC(pt) = β. 
These two conditions yield the equations 
left parenthesis z Subscript p Sub Subscript t Subscript Baseline plus k right parenthesis StartRoot n EndRoot equals z Subscript 1 minus beta Baseline left parenthesis 1 plus k squared divided by 2 right parenthesis Superscript 1 divided by 2(zpt + k)√n = z1−β(1 + k2/2)1/2 
and
(11.6.5) 
left parenthesis z Subscript p 0 Baseline plus k right parenthesis StartRoot n EndRoot equals z Subscript alpha Baseline left parenthesis 1 plus k squared divided by 2 right parenthesis Superscript 1 divided by 2 Baseline period(zp0 + k)√n = zα(1 + k2/2)1/2. 
The solution for n and k yields 
n equals StartFraction left parenthesis z Subscript 1 minus alpha Baseline plus z Subscript 1 minus beta Baseline right parenthesis squared left parenthesis 1 plus k squared divided by 2 right parenthesis Over left parenthesis z Subscript p Sub Subscript t Subscript Baseline minus z Subscript p 0 Baseline right parenthesis squared EndFraction comman = (z1−α + z1−β)2(1 + k2/2) 
(zpt − zp0)2 
,
(11.6.6) 
and 
k equals left parenthesis z Subscript p Sub Subscript t Subscript Baseline z Subscript alpha Baseline plus z Subscript p 0 Baseline z Subscript beta Baseline right parenthesis divided by left parenthesis z Subscript 1 minus alpha Baseline plus z Subscript 1 minus beta Baseline right parenthesis periodk = (zpt zα + zp0zβ)/(z1−α + z1−β).
(11.6.7) 
In other words, if the sample size n is given by the above formula, we can replace 
t Subscript 1 minus alpha comma p comma nt1−α,p,n by the simpler term k and accept the lot if 
upper X overbar minus k upper S greater than or equals xi period¯X − kS ≥ ξ. 
The statistic upper X overbar minus k upper S ¯X − kS is called a lower tolerance limit. 
Example 11.4 Consider the example of testing the compressive strength of concrete 
cubes presented in Chapter 1 in Modern Statistics (Kenett et al. 2022b). It is required 
that the compressive strength be larger than 240 [kg/cmsquared2]. We found that upper Y equals ln upper XY = ln X 
had an approximately normal distribution. Suppose that it is required to decide 
whether to accept or reject this lot with the following speciﬁcations: p 0 equals 0.01p0 = 0.01, 
p Subscript t Baseline equals 0.05pt = 0.05, and alpha equals beta equals 0.05α = β = 0.05. According to the normal distribution, 
z Subscript p 0 Baseline equals negative 2.326 comma z Subscript p Sub Subscript t Subscript Baseline equals negative 1.645zp0 = −2.326, zpt = −1.645 

418
11 
Sampling Plans for Batch and Sequential Inspection 
and 
z Subscript 1 minus alpha Baseline equals z Subscript 1 minus beta Baseline equals 1.645 periodz1−α = z1−β = 1.645. 
Thus, according to the above formulas, we ﬁnd k equals 1.9855k = 1.9855 and n equals 70n = 70. Hence, 
with a sample size of 70, we can accept the lot if upper Y overbar minus 1.9855 upper S greater than or equals xi ¯Y −1.9855S ≥ ξ, where xi equals ln left parenthesis 240 right parenthesis equals 5.48ξ = 
ln(240) = 5.48.
black medium square■
The sample size required in this single-stage sampling plan for variables is 
substantially smaller than the one we determined for the single-stage sampling plan 
for attributes (which was n equals 176n = 176). However, the sampling plan for attributes is 
free of any assumption about the distribution of X, while in the above example 
we had to assume that upper Y equals ln upper XY = ln X is normally distributed. Thus, there is a certain 
trade-off between the two approaches. In particular, if our assumptions concerning 
the distribution of X are erroneous, we may not have the desired producer’s and 
consumer’s risks. 
The above procedure of acceptance sampling for variables can be generalized to 
upper and lower tolerance limits, double sampling, and sequential sampling. The 
interested reader can ﬁnd more information on the subject in (Duncan 1986, Ch. 12– 
15). Berry and Fristedt (1985) applied tolerance limits for the appraisal of ceramic 
substrates in the multivariate case. 
11.7 Rectifying Inspection of Lots 
Rectifying inspection plans are those plans which call for a complete inspection of 
a rejected lot for the purpose of replacing the defectives by non-defective items. 
(Lots that are accepted are not subjected to rectiﬁcation.) We shall assume that the 
tests are non-destructive, that all the defective items in the sample are replaced by 
good ones, and that the sample is replaced in the lot. 
If a lot contains N items and has a proportion p of defectives before the 
inspection, the proportion of defectives in the lot after inspection is 
p prime equals StartLayout Enlarged left brace 1st Row 1st Column 0 comma 2nd Column if lot is rejected comma 2nd Row 1st Column Blank 3rd Row 1st Column p left parenthesis upper N minus upper X right parenthesis divided by upper N comma 2nd Column if lot is accepted comma EndLayoutp′ = 
⎧ 
⎪⎪⎨ 
⎪⎪⎩ 
0,
if lot is rejected, 
p(N − X)/N, if lot is accepted, 
(11.7.1) 
where X is the number of defectives in the sample. If the probability of accepting 
a lot  by  a given  sampling plan is OCleft parenthesis p right parenthesis(p), then the expected proportion of outgoing 
defectives is when sampling is single stage by attribute, 
upper E left brace p Superscript prime Baseline right brace equals p OC left parenthesis p right parenthesis left parenthesis 1 minus StartFraction n Over upper N EndFraction upper R Subscript s Superscript asterisk Baseline right parenthesis commaE{p′} = p OC(p)

1 − n 
N R∗
s

,
(11.7.2) 

11.7 
Rectifying Inspection of Lots
419 
Table 11.13 AOQ values for 
rectifying plan upper N equals 1000N = 1000, 
n equals 250n = 250, and c equals 5c = 5 
p
OCleft parenthesis p right parenthesis(p)
upper R Subscript s Superscript asteriskR∗
s
AOQ 
0.005 
1.000
1.0000 
0.004 
0.010 
0.981
0.9710 
0.007 
0.015 
0.853
0.8730 
0.010 
0.020 
0.618
0.7568 
0.010 
0.025 
0.376
0.6546 
0.008 
0.030 
0.199
0.5715 
0.005 
0.035 
0.094
0.5053 
0.003 
where 
upper R Subscript s Superscript asterisk Baseline equals StartFraction upper H left parenthesis c minus 1 semicolon upper N minus 1 comma left bracket upper N p right bracket minus 1 comma n minus 1 right parenthesis Over upper H left parenthesis c semicolon upper N comma left bracket upper N p right bracket comma n right parenthesis EndFraction periodR∗
s = H(c − 1; N − 1, [Np] −1, n  − 1) 
H(c; N, [Np], n)  
.
(11.7.3) 
If n divided by upper Nn/N is small, then 
upper E left brace p Superscript prime Baseline right brace approximately equals p OC left parenthesis p right parenthesis periodE{p′} ∼= pOC(p).
(11.7.4) 
The expected value of p primep′ is called the Average Outgoing Quality and is denoted 
by AOQ. 
The formula for upper R Subscript s Superscript asteriskR∗
s depends on the method of sampling inspection. If the 
inspection is by double sampling, the formula is considerably more complicated. 
In Table 11.13, we present the AOQ values corresponding to a rectifying plan, 
when upper N equals 1000N = 1000, n equals 250n = 250, and c equals 5c = 5. 
The AOQL (Average Outgoing Quality Limit) of a rectifying plan is deﬁned 
as the maximal value of AOQ. Thus the AOQL corresponding to the plan of 
Table 11.13 is approximately 0.01. The AOQ given is presented graphically in 
Fig. 11.3. 
We also characterize a rectifying plan by the average total inspection (ATI) 
associated with a given value of p. If a lot is accepted, only n items (the sample 
size) have been inspected, while if it is rejected, the number of items inspected is N. 
Thus, 
StartLayout 1st Row 1st Column ATI left parenthesis p right parenthesis 2nd Column equals n OC left parenthesis p right parenthesis plus upper N left parenthesis 1 minus OC left parenthesis p right parenthesis right parenthesis 2nd Row 1st Column Blank 2nd Column equals n plus left parenthesis upper N minus n right parenthesis left parenthesis 1 minus OC left parenthesis p right parenthesis right parenthesis period EndLayout
ATI(p) = nOC(p) + N(1 −OC(p)) 
= n + (N −n)(1 − OC(p)). 
(11.7.5) 
This function is increasing from n (when p equals 0p = 0) to  N (when p equals 1p = 1). 
In our example, the lot contains upper N equals 1000N = 1000 items and the sample size is n equals 250n = 250. 
The graph of the ATI function is presented in Fig. 11.4. 
Dodge and Romig (1998) published tables for the design of single and double 
sampling plans for attributes, for which the AOQL is speciﬁed and the ATI is 
minimized at a speciﬁed value of p. In the following table, we provide a few 
values of n and c for such a single sampling plan, for which the AOQL equals 0.01= 0.01 
(Table 11.14). 

420
11 
Sampling Plans for Batch and Sequential Inspection 
Fig. 11.3 AOQ curve for single sampling plan with upper N equals 1000N = 1000, n equals 250n = 250, and c equals 5c = 5 
According to this table, for a lot of size 2000, to guarantee an AOQL of 1% and 
minimal ATI at p equals 0.01p = 0.01, one needs a sample of size n equals 180n = 180, with c equals 3c = 3. For  
another method of determining n and c, see (Duncan 1986, Ch. 16). 
Rectifying sampling plans with less than 100% inspection of rejected lots have 
been developed and are available in the literature. 
11.8 National and International Standards 
During World War II, the US Army developed standards for sampling acceptance 
schemes by attributes. Army Ordinance tables were prepared in 1942 and the Navy 
issued its own tables in 1945. Joint Army and Navy standards were issued in 1949. 

11.8 
National and International Standards
421 
Fig. 11.4 ATI curve for single sampling plan with upper N equals 1000N = 1000, n equals 250n = 250, and c equals 5c = 5 
These standards were superseded in 1950 by the common standards, named MIL-
STD-105A. The MIL-STD-105D was issued by the US Government in 1963 and 
slightly revised as MIL-STD-105E in 1989. These standards, however, are gradually 
being phased out by the Department of Defense. The American National Standards 
Institute (ANSI) adopted the military standards with some minor modiﬁcations, as 
ANSI Z1.4 standards. These were adopted in 1974 by the International Organization 
for Standardization as ISO 2859. In 1981, ANSI Z1.4 was adopted by the American 
Society for Quality Control with some additions, and the standards issued were 
named ANSI/ASQC Z1.4. 
The military standards were designed to inspect incoming lots from a variety of 
suppliers. The requirement from all suppliers is to satisfy speciﬁed quality levels 
for the products. These quality levels are indexed by the AQL. It is expected that a 
supplier sends in continuously series of lots (shipments). All these lots are subjected 

422
11 
Sampling Plans for Batch and Sequential Inspection 
Table 11.14 Selected values 
of left parenthesis n comma c right parenthesis(n, c) for a single sampling 
plan with AOQLequals 0.01 = 0.01 and 
ATI minimum at p 
p
0.004–0.006 
0.006–0.008 
0.008–0.01 
N
n
c
n
c
n
c 
101–200
32 
0
32 
0
32 
0 
201–300
33 
0
33 
0
65 
1 
501–600
75 
1
75 
1
75 
1 
1001–2000 
130 
2
130 
2
180 
3 
to quality inspection. At the beginning an AQL value is speciﬁed for the product. 
The type of sampling plan is decided (single, double, sequential, etc.). For a given lot 
size and type of sampling, the parameters of the sampling procedure are determined. 
For example, if the sampling is single stage by attribute, the parameters left parenthesis n comma c right parenthesis(n, c) are 
read from the tables. The special feature of the MIL-STD-105E is that lots can be 
subjected to normal, tightened, or  reduced inspection. Inspection starts at a normal 
level. If two out of ﬁve consecutive lots have been rejected, a switch to tightened 
inspection level takes place. Normal inspection is reinstituted if ﬁve consecutive 
lots have been accepted. If ten consecutive lots remain under tightened inspection, 
an action may take place to discontinue the contract with the supplier. On the other 
hand, if the last ten lots have all been accepted at a normal inspection level and the 
total number of defective units found in the samples from these ten lots is less than 
a speciﬁed value, then a switch from normal to reduced inspection level can take 
place. 
We do not reproduce here the MIL-STD-105 tables. The reader can ﬁnd detailed 
explanation and examples in (Duncan 1986, Ch. 10). We conclude with the 
following example. 
Example 11.5 Suppose that for a given product AQL equals 0.01= 0.01 (1%). The size of the 
lots is upper N equals 1000N = 1000. The military standard speciﬁes that a single-stage sampling for 
attributes, under normal inspection, has the parameters n equals 80n = 80 and c equals 2c = 2. Using  
Python, we get the following QC values for this plan: 
plan = acceptanceSampling.SSPlanBinomial(1000, 80, 2, 
p=(0.01, 0.02, 0.03, 0.04, 0.05)) 
p
0.01 
0.02 
0.03 
0.04 
0.05 
OCleft parenthesis p right parenthesis(p) 
0.953 
0.784 
0.568 
0.375 
0.231 
Thus, if the proportion of nonconforming, p, of the supplier is less than AQL, the 
probability of accepting a lot is larger than 0.953. A supplier that continues to ship 
lots with p equals 0.01p = 0.01 has a probability of left parenthesis 0.953 right parenthesis Superscript 10 Baseline equals 0.621(0.953)10 = 0.621 that all the 10 lots will 
be accepted, and the inspection level will be switched to a reduced one. Under the 
reduced level, the sample size from the lot is reduced to n equals 32n = 32. The corresponding 
acceptance number is c equals 1c = 1. Thus, despite the fact that the level of quality of the 
supplier remains good, there is a probability of 0.38 equals 1 minus 0.953 Superscript 100.38 = 1 − 0.95310 that there will 

11.9 
Skip-Lot Sampling Plans for Attributes
423 
be no switch to reduced inspection level after the tenth lot. On the other hand, the 
probability that there will be no switch to tightened level of inspection before the 
sixth lot is inspected is 0.9859. This is the probability that after each inspection the 
next lot will continue to be inspected under normal level. If there is no deterioration 
in the quality level of the supplier and p equals 0.03p = 0.03, the probability that the inspection 
level will be switched to “tightened” after ﬁve inspections is 0.722.
black medium square■
11.9 Skip-Lot Sampling Plans for Attributes 
We have seen in the previous section that according to the MIL-STD-105E, if a 
supplier keeps shipping high-quality lots, then after a while his lots are subjected 
to inspection under reduced level. All lots are inspected under a reduced level 
inspection scheme, as long as their quality level remains high. The Skip-Lot 
Sampling Plans (SLSP), which was proposed by Liebesman and Saperstein (1983), 
introduces a new element of savings if lots continue to have very low proportions 
of nonconforming items. As we will see below, instead of just reduced level of 
inspection of high-quality lots, the SLSP plans do not necessarily inspect such 
lots. If the lots coming in from a given supplier qualify for skipping, then they are 
inspected only with probability 0.5. This probability is later reduced to 0.33 and to 
0.2, if the inspected lots continue to be almost free of nonconforming items. Thus, 
suppliers that continue to manufacture their product, with proportion of defectives 
p, considerably smaller than the speciﬁed AQL stand a good chance to have only a 
small fraction of their lots inspected. The SLSP which will be speciﬁed below was 
adopted as the ISO2859/3 standard in 1986. 
11.9.1 
The ISO 2859 Skip-Lot Sampling Procedures 
Am SLSP has to address three main issues: 
1. What are the conditions for beginning or reinstating the Skip-Lot (SL) state? 
2. What is the fraction of lots to be skipped? 
3. Under what conditions should one stop skipping lots, on a temporary or 
permanent basis? 
The fraction of lots to be skipped is the probability that a given lot will not be 
inspected. If this probability for example is 0.8, we generate a random number, U, 
with uniform distribution on left parenthesis 0 comma 1 right parenthesis(0, 1). If upper U less than 0.8U <  0.8, inspection is skipped; otherwise the 
lot is inspected. 
We deﬁne three states: 
State 1. 
Every lot is inspected. 
State 2. 
Some lots are skipped and not inspected. 

424
11 
Sampling Plans for Batch and Sequential Inspection 
State 3. 
All lots are inspected, pending a decision of disqualiﬁcation (back to 
state 1) or resumption of SL (back to state 2). 
Lot by lot inspection is performed during state 3, but the requirements to 
requalify for Skip-Lot inspection are less stringent than the initial qualiﬁcation 
requirements. 
Switching rules apply to 4 transitions between states: Qualiﬁcation (State 1 to 
State 2), Interruption (State 2 to State 3), Resumption (State 3 to State 2), and 
Disqualiﬁcation (State 3 to State 1). 
The switching rules for the SLSP procedure are listed below. 
Skip-Lot Switching Rules 
We specify here the rules appropriate for single sampling by attributes. Other rules 
are available for other sampling schemes. 
(A) Qualiﬁcation. (State 1 right arrow→ State 2). 
1. Ten consecutive lots are accepted. 
2. The total number of defective items in the samples from the ten lots is 
smaller than critical level given in Table 11.15. 
3. The number of defective items in each one of the last two lots is smaller 
than the values speciﬁed in Table 11.16. 
4. Supplier has a stable manufacturing organization, continuous production 
and other traits which qualify him to be high quality stable manufacturer. 
(B) Interruption. (State 2 right arrow→ State 3) 
1. An inspected lot has in the sample more defectives than speciﬁed in 
Table 11.16. 
(C) Resumption. (State 3 right arrow→State 2) 
1. Four consecutive lots are accepted. 
2. The last two lots satisfy the requirements of Table 11.16. 
(D) Disqualiﬁcations. (State 3 right arrow→ State 1) 
1. Two lots are rejected within ten consecutively inspected lots or 
2. Violation of the supplier qualiﬁcation criteria (item A4 above) 
We have seen in the previous section that, under normal inspection, MIL-STD-
105E speciﬁes that, for AQL equals 0.01= 0.01 and lots of size upper N equals 1 comma 000N = 1, 000, random samples 
of size n equals 80n = 80 should be drawn. The critical level was c equals 2c = 2. If 10 lots have been 
accepted consecutively, the total number of observed defectives is upper S 10 less than or equals 20S10 ≤ 20. The  
total sample size is 800, and according to Table 11.15, upper S 10S10 should not exceed 3 to 
qualify for a switch to State 2. Moreover, according to Table 11.16, the last two 
samples should each have less than 1 defective item. Thus, the probability to qualify 
for State 2, on the basis of the last 10 samples, when p equalsp = AQL equals 0.01= 0.01 is 

Cumulative no. of defectives 
AQL (%) 
0.65
1.0
1.5
2.5
4.0
6.5
10.0 
0
400
260
174
104
65
40
26 
1
654
425
284
170
107
65
43 
2
883
574
383
230
144
88
57 
3
1098
714
476
286
179
110
71 
4
1306
849
566
340
212
131
85 
5
1508
980
653
392
245
151
98 
6
1706
1109
739
444
277
171
111 
7
1902
1236
824
494
309
190
124 
8
2094
1361
907
544
340
209
136 
9
2285
1485
990
594
371
229
149 
10
2474
1608
1072
643
402
247
161 
11
2660
1729
1153
692
432
266
173 
12
2846
1850
1233
740
463
285
185 
13
3031
1970
1313
788
493
303
197 
14
3214
2089
1393
836
522
321
209 
15
3397
2208
1472
883
552
340
221 
16
3578
2326
1550
930
582
358
233 
17
3758
2443
1629
977
611
376
244 
18
3938
2560
1707
1024
640
394
256 
19
4117
2676
1784
1070
669
412
268 
20
4297
2793
1862
1117
698
430
279 
Table 11.16 Individual lot 
acceptance numbers for 
skip-lot qualiﬁcation 
AQL(%) 
Sample size 
0.65 
1.0 
1.5 
2.5 
4.0 
6.5 
10.0 
2
–
–
–
–
–
0
0 
3
–
–
–
–
0
0
0 
5
–
–
–
0
0
0
1 
8
–
–
0
0
0
1
1 
13
–
0
0
0
1
1
2 
20
0
0
0
1
1
2
3 
32
0
0
1
1
2
3
5 
50
0
1
1
2
3
5
7 
80
1
1
2
3
5
7
11 
125
1
2
3
4
7
11 
16 
200
2
3
4
7
11 
17 
25 
315
3
4
7
11 
16 
25 
38 
500
5
7
10 
16 
25 
39 
58 
800
7
11 
16 
25 
38 
60 
91 
1250
11
16 
23 
38 
58 
92 
138 
2000
17
25 
36 
58 
91 
144 
217 
11.9 
Skip-Lot Sampling Plans for Attributes
425 
Table 11.15 Minimum cumulative sample size in ten lots for skip-lot qualiﬁcations 

426
11 
Sampling Plans for Batch and Sequential Inspection 
StartLayout 1st Row 1st Column upper Q upper P 2nd Column equals b squared left parenthesis 0 semicolon 80 comma 0.01 right parenthesis upper B left parenthesis 3 semicolon 640 comma 0.01 right parenthesis 2nd Row 1st Column Blank 2nd Column equals left parenthesis 0.4475 right parenthesis squared times 0.1177 equals 0.0236 period EndLayout
QP = b2(0; 80, 0.01)B(3; 640, 0.01) 
= (0.4475)2 × 0.1177 = 0.0236. 
Thus, if the fraction defective level is exactly at the AQL value, the probability for 
qualiﬁcation is only 0.02. On the other hand, if the supplier maintains the production 
at fraction defective of p equals 0.001p = 0.001, then the qualiﬁcation probability is 
StartLayout 1st Row 1st Column upper Q upper P 2nd Column equals b squared left parenthesis 0 semicolon 80 comma 0.001 right parenthesis upper B left parenthesis 3 semicolon 640 comma 0.001 right parenthesis 2nd Row 1st Column Blank 2nd Column equals left parenthesis 0.9231 right parenthesis squared times 0.9958 equals 0.849 period EndLayout
QP = b2(0; 80, 0.001)B(3; 640, 0.001) 
= (0.9231)2 × 0.9958 = 0.849. 
Thus, a supplier who maintains a level of p equals 0.001p = 0.001, when the AQL equals 0.01= 0.01, will 
probably be qualiﬁed after the ﬁrst ten inspections and will switch to State 2 of 
skipping lots. Eventually only 20% of his lots will be inspected, under this SLSP 
standard, with high savings to both producer and consumer. This illustrates the 
importance of maintaining high-quality production processes. In Chaps. 2 and 3, we  
discussed how to statistically control the production processes, to maintain stable 
processes of high quality. Generally, for the SLSP to be effective, the fraction 
defective level of the supplier should be smaller than half of the AQL. For p level 
close to the AQL, the SLSP and the MIL-STD 105E are very similar in performance 
characteristics. 
11.10 The Deming Inspection Criterion 
Deming (1982) has derived a formula to express the expected cost to the ﬁrm caused 
by sampling of lots of incoming material. The importance of the Deming inspection 
criterion formula is that it provides an economic perspective on the alternatives of 
management by inspection or management by process control and process improve-
ment (see Sect. 1.2). In some cases, investing in inspection equipment that provides 
high throughput sorting capabilities is more economical than investing in process 
control methods and technology (Chaps. 2–4). The drawback of inspection is that 
it is focused on product characteristics and not on production processes affecting 
product characteristics. The knowledge required in implementing inspection-based 
systems is minimal compared to process control which combines technological 
and operator dimensions. More knowledge is an industry 4.0 requirement so that 
decisions to remain at the level of screening and sorting induce limiting industrial 
capabilities (see Chap. 8). Decisions on investments in process control can be 
justiﬁed with Deming’s formula we develop next. Let us deﬁne 
N 
= 
the number of items in a lot 
k1 
= 
the cost of inspecting one item at the beginning of the process 
q
= 
the probability of a conforming item 

11.11 
Published Tables for Acceptance Sampling
427 
p 
= 
the probability of a nonconforming item 
Q 
= OC(p) = probability of accepting a lot 
k2 
= 
the cost to the ﬁrm when one nonconforming item is moved downstream 
to a customer or to the next stage of the production process 
p′′
= 
the probability of nonconforming items being in an accepted lot 
n 
= 
the sample size inspected from a lot of size N 
Thus, the total expected cost per lot is 
upper E upper C equals StartFraction upper N k 1 Over q EndFraction left bracket 1 plus upper Q q left parenthesis StartFraction k 2 Over k 1 EndFraction p Superscript double prime Baseline minus 1 right parenthesis left parenthesis 1 minus StartFraction n Over upper N EndFraction right parenthesis right bracket periodEC = Nk1 
q

1 + Qq

k2 
k1 
p′′ − 1
 
1 − n 
N

.
(11.10.1) 
If (k2/k1)p′′ > 1, then any sampling plan increases the cost to the ﬁrm and 
n = N (100% inspection) becomes the least costly alternative. 
If (k2/k1)p′′ < 1, then the value n = 0 yields the minimum value of EC so that 
no inspection is the alternative of choice. 
Now p′′ can be only somewhat smaller than p. For example, if N = 50, n = 10, 
c = 0, and p = 0.04, then p′′ = 0.0345. Substituting p for p′′ gives us the following 
rule: 
StartLayout 1st Row 1st Column Blank 2nd Column If left parenthesis k 2 divided by k 1 right parenthesis p greater than 1 comma inspect every item in the lot 2nd Row 1st Column Blank 2nd Column If left parenthesis k 2 divided by k 1 right parenthesis p less than 1 comma accept the lot without inspection period EndLayoutIf (k2/k1)p > 1, inspect every item in the lot 
If (k2/k1)p < 1, accept the lot without inspection. 
The Deming assumption is that the process is under control and that p is known. 
Sampling plans such as MIL-STD-105D do not make such assumptions and, in fact, 
are designed for catching shifts in process levels. 
To keep the process under control, Deming suggests the use of control charts and 
Statistical Process Control (SPC) procedures which are discussed in Chaps. 2 and 3. 
The assumption that a process is under control means that the ﬁrm has absorbed 
the cost of SPC as internal overhead or as a piece-cost. Deming’s assertion then is 
that assuming up front the cost of SPC implementation is cheaper, in the long run, 
than doing business in a regime where a process may go out of control undetected 
until its output undergoes acceptance sampling. 
11.11 Published Tables for Acceptance Sampling 
In this section we list some information on published tables and schemes for 
sampling inspection by attribute and by variables. The material given here follows 
Chapters 24–25 of Juran (1979). We shall not provide an explanation here concern-
ing the usage of these tables. The interested practitioner can use the instructions 
attached to the tables and/or read more about the tables in (Juran 1979, Ch. 24–25) 
or in Duncan (1986). 

428
11 
Sampling Plans for Batch and Sequential Inspection 
I. Sampling by Attributes 
1. MIL-STD-105E 
Type of sampling: Single, double, and multiple 
Type of application: General 
Key features: Maintains average quality at a speciﬁed level; aims to minimize 
rejection of good lots; and provides single sampling plans for speciﬁed AQL 
and producer’s risk. 
Reference: MIL-STD-105E, Sampling Procedures and Tables for Inspection by 
Attributes, Government Printing Ofﬁce, Washington, D.C. 
2. Dodge-Romig 
Type of sampling: Single and double. Type of application: Where 100% recti-
fying of lots is applicable. Key features: One type of plan uses a consumer’s 
risk of beta equals 0.10β = 0.10. Another type limits the AOQL. Protection is provided with 
minimum inspection per lot. Reference: Dodge and Romig (1998) 
3. H107 
Type of sampling: Continuous single stage 
Type of application: When production is continuous and inspection is non-
destructive. 
Key features: Plans are indexed by AQL, which generally start with 100% 
inspection until some consecutive number of units free of defects are found. 
Then inspection continues on a sampling basis until a speciﬁed number of 
defectives are found. 
Reference: H-107, Single-Level Continuous Sampling Procedures and Tables 
for Inspection by Attribute, Government Printing Ofﬁce, Washington, D.C. 
II. Sampling by Variables 
1. MIL-STD-414 
Assumed distribution: Normal 
Criteria speciﬁed: AQL 
Features: Lot evaluation by AQL. It includes tightened and reduced inspection. 
Reference: Sampling Procedures and Tables for Inspection by Variables for 
Percent Defectives, MIL-STD-414, Government Printing Ofﬁce, Washington, 
D.C. 
2. H-108 
Assumed distribution: Exponential 
Criteria speciﬁed: Mean Life (MTBF) 
Features: Life testing for reliability speciﬁcations 
Reference: H-108, Sampling Procedures and Tables for Life and Reliability 
Testing (Based on Exponential Distribution), US Department of Defense, 
Quality Control and Reliability Handbook, Government Printing Ofﬁce, 
Washington, D.C. 

11.12 
Sequential Reliability Testing
429 
11.12 Sequential Reliability Testing 
Sequential methods in reliability testing have been in use from the pioneering work 
of Dvoretzky et al. (1953), Epstein and Sobel (1955), and Kiefer and Wolfowitz 
(1956). Sequential life testing and sampling acceptance for the exponential distribu-
tion had been codiﬁed in the Military Standards 781C document. Later, in the 60s 
and 70s, many papers appeared on survival analysis and among them also studies on 
sequential methods. Some of these are Epstein (1960), Basu (1971), Mukhopadhyay 
(1974), and Bryant and Schmee (1979), among others. In this section we review 
these methods in an introductory approach. A comprehensive summary of these 
methods is given in the article of Basu (1991). 
Let T denote the cumulative operating time till failure (TTF) of a system. 
The distribution of T is the “life distribution” and there are several life distribu-
tion models including the Exponential, Shifted-Exponential, Erlang, Weibull, and 
Extreme-Value distribution (see Sect. 9.7). 
Consider, for example, the sequential analysis of the reliability of a system with 
an Exponential life distribution, with mean time between failures (MTBF)equals theta = θ. 
The cumulative distribution function upper F Subscript upper T Baseline left parenthesis t semicolon theta right parenthesis equals 1 minus exp left parenthesis t divided by theta right parenthesisFT (t; θ) = 1 −exp(t/θ), and the reliability 
function upper R left parenthesis t right parenthesis equals exp left parenthesis t divided by theta right parenthesisR(t) = exp(t/θ). Estimation of upper R left parenthesis t 0 right parenthesisR(t0) at a speciﬁed time t 0t0 is based on 
consecutive sequence of continuous failure times observations. 
A particularly interesting question is establishing when, during testing, the 
MTBF is shown to be bigger than a prespeciﬁed value theta Subscript oθo. Testing hypotheses about 
the MTBF is based on the Wald SPRT. Such testing can be evaluated with an 
operating characteristic curve that plots the probability of acceptance as a function 
of theta Subscript oθo. The evaluation of the operating characteristics of sequential methods can 
be done via Wald’s approximations, asymptotic analysis, or numerical techniques. 
The difﬁculty is in identifying the exact distributions of stopping times, i.e., the 
time instance when the MTBF is declared to be bigger than theta Subscript oθo. Algorithms for the 
numerical determination of the operating characteristics of sequential procedures 
were developed by Aroian and Robison (1969), Aroian (1976), Zacks (1980, 1992, 
1997), and others. In this section we focus on the application of sequential reliability 
testing to system and software testing. 
The deﬁnition of system and software reliability is the probability of execution 
without failure for some speciﬁed interval, called the mission time (Kenett 2007). 
This deﬁnition is compatible with that used for hardware reliability, though the 
failure mechanisms in software components may differ signiﬁcantly. System and 
software reliability is applicable both as a tool complementing development testing, 
in which faults are found and removed, and for certiﬁcation testing, when a 
system or software product is either accepted or rejected as meeting its quality 
requirements. When software is in operation, failure rates can be computed by 
computing number of failures, say, per hours of operation. The predicted failure rate 
corresponding to the steady state behavior of the software is usually a key indicator 
of great interest. The predicted failure rate may be regarded as high when compared 
to other systems. We should keep in mind, however, that the speciﬁc weight of the 

430
11 
Sampling Plans for Batch and Sequential Inspection 
failures indicating severity of impact is not accounted for within the failure category 
being tracked. In the bug tracking data base, all failures within one category are 
equal. Actual interpretation of the predicted failure rates, accounting for operational 
proﬁle and speciﬁc failure impact, is therefore quite complex. Predicted failure rates 
should therefore be considered in management decisions at an aggregated level. For 
instance, the decision to promote software from system test status to acceptance test 
status can be based on a comparison of predicted failure rates to actual, as illustrated 
below. 
Reliability speciﬁcations should be veriﬁed with a speciﬁed level of conﬁdence. 
Interpreting failure data is also used to determine if a system can be moved from 
development to beta testing with selected customers and then from beta testing to 
ofﬁcial shipping. Both applications to development testing and certiﬁcation testing 
rely on mathematical models for tracking and predicting software reliability. Many 
system and software reliability models have been suggested. Moreover lessons 
learned from object oriented development are used to better design Web Services 
and Service Oriented Architecture systems for both better reliability and usability 
(Bai and Kenett 2009). In this section we review the classical reliability models that 
apply to software and overall system reliability. 
The Jelinski–Moranda model (Jelinski and Moranda 1972) is the ﬁrst published 
Markov model, which has profound inﬂuences on software reliability modeling 
thereafter. The main assumptions of this model are: 
(a) The number of initial faults is an unknown but ﬁxed constant. 
(b) A detected fault is removed immediately and no new faults are introduced. 
(c) Times between failures are independent, exponentially distributed random 
quantities. 
(d) Each remaining software fault contributes the same amount to the software 
failure intensity. 
Denote by upper N 0N0 the number of initial faults in the software before the testing starts, 
then the initial failure intensity is upper N 0 phiN0φ, where phiφ is a constant of proportionality 
denoting the failure intensity contributed by each fault. Denote by 
upper T Subscript iTi, 
i equals 1 comma 2 comma ellipsis comma upper N 0i = 
1, 2, . . .  , N0, the time between left parenthesis i minus 1 right parenthesis(i − 1)th and ith failures, then upper T Subscript iTi’s are independent, 
exponentially distributed random variables with parameter 
lamda Subscript i Baseline equals phi left bracket upper N 0 minus left parenthesis i minus 1 right parenthesis right bracket comma i equals 1 comma 2 comma ellipsis comma upper N 0 periodλi = φ [N0 − (i − 1)] ,
i
 
= 1, 2, . . . , N0. 
Many modiﬁed versions of the Jelinski–Moranda model have been studied in 
the literature. Schick and Wolverton (1978) proposed a model assuming that times 
between failures are not exponential but follow Rayleigh distribution. Shanthikumar 
(1981) generalized the Jelinski–Moranda model by using general time-dependent 
transition probability function. Xie (1991) developed a general decreasing failure 
intensity model allowing for the possibility of different fault sizes. Whittaker et al. 
(2000) considered a model that allows use of prior testing data to cover the real-
world scenario in which the release build is constructed only after a succession 
of repairs to buggy pre-release builds. Boland and Singh (2003) consider a birth-

11.12 
Sequential Reliability Testing
431 
process approach to a related software reliability model and Lipow considers data 
aggregated over various time intervals (Lipow 1978). The Lipow model assumes 
that: 
(a) The rate of error detection is proportional to the current error content of a 
program. 
(b) All errors are equally likely to occur and are independent of one another. 
(c) Each error is of the same order of severity as any other error. 
(d) The error rate remains constant over the testing interval. 
(e) The software is operated in a similar manner as the anticipated operational usage 
(f) During a testing interval i, f Subscript ifi errors are discovered, but only n Subscript ini errors are 
corrected in the time frame. 
When software is in operation, failure rates can be computed by computing num-
ber of failures, say, per hours of operation. The predicted failure rate corresponding 
to the steady state behavior of the software is usually a key indicator of great interest. 
The predicted failure rate may be regarded as high when compared to other systems. 
We should keep in mind, however, that the speciﬁc weight of the failures indicating 
severity of impact is not accounted for within the failure category being tracked. In 
the bug tracking data base, all failures within one category are equal. Actual micro-
interpretation of the predicted failure rates, accounting for operational proﬁle and 
speciﬁc failure impact, is therefore quite complex. Predicted failure rates should 
therefore be considered in management decisions at a macro, aggregated level. For 
instance, the decision to promote software from system test status to acceptance test 
status can be based on a comparison of predicted failure rates to actual, as illustrated 
below. Since the error rate remains constant during each of the M testing periods 
(assumption (d)), the failure rate during the ith testing period is 
upper Z left parenthesis t right parenthesis equals phi left bracket upper N minus upper F Subscript i Baseline right bracket comma t Subscript i minus 1 Baseline less than or equals t less than or equals t Subscript i Baseline commaZ(t) = φ [N − Fi] ,
ti−1 ≤ t ≤ ti, 
where phiφ is the proportionality constant, N is again the total number of errors initially 
present in the program, upper F Subscript i minus 1 Baseline equals sigma summation Underscript j equals 1 Overscript i minus 1 Endscripts n Subscript jFi−1 = i−1 
j=1 nj is the total number of errors corrected up 
through the left parenthesis i minus 1 right parenthesis(i − 1)-th testing intervals, and t Subscript iti is the time measured in either CPU 
or wall clock time at the end of the ith testing interval, x Subscript i Baseline equals t Subscript i Baseline minus t Subscript i minus 1xi = ti − ti−1. The  t Subscript iti’s are 
ﬁxed and thus are not ﬁxed as in the Jelinski–Moranda model. Taking the number of 
failures, f Subscript ifi, in the  ith interval to be a Poisson random variable with mean upper Z left parenthesis t Subscript i Baseline right parenthesis x Subscript iZ(ti)xi, 
the likelihood is 
upper L left parenthesis f 1 comma ellipsis comma f Subscript upper M Baseline right parenthesis equals product Underscript i equals 1 Overscript upper M Endscripts StartFraction left parenthesis phi left bracket upper N minus upper F Subscript i minus 1 Baseline right bracket x Subscript i Baseline right parenthesis Superscript f Super Subscript i Superscript Baseline exp left parenthesis minus phi left bracket upper N minus upper F Subscript i minus 1 Baseline right bracket x Subscript i Baseline right parenthesis Over f Subscript i Baseline factorial EndFraction periodL(f1, . . . , fM) = 
M

i=1 
(φ[N −Fi−1]xi)fi exp (−φ[N −Fi−1]xi) 
fi!
. 
Taking the partial derivatives of upper L left parenthesis f right parenthesisL(f ) with respect to phiφ and N and setting 
the resulting equations to zero, we derive the following equations satisﬁed by the 
maximum likelihood estimators ModifyingAbove phi With caretˆφ and ModifyingAbove upper N With caret ˆN of phiφ and N: 

432
11 
Sampling Plans for Batch and Sequential Inspection 
ModifyingAbove phi With caret equals StartFraction upper F Subscript upper M Baseline divided by upper A Over ModifyingAbove upper N With caret plus 1 minus upper B divided by upper A EndFraction and StartFraction upper F Subscript upper M Baseline Over ModifyingAbove upper N With caret plus 1 minus upper B divided by upper A EndFraction equals sigma summation Underscript i equals 1 Overscript upper M Endscripts StartFraction f Subscript i Baseline Over ModifyingAbove upper N With caret minus upper F Subscript i minus 1 Baseline EndFraction commaˆφ =
FM/A 
ˆN + 1 − B/A 
and 
FM 
ˆN + 1 −B/A 
= 
M

i=1 
fi 
ˆN − Fi−1 
, 
where 
StartLayout 1st Row 1st Column Blank 2nd Column upper F Subscript upper M Baseline equals sigma summation Underscript i equals 1 Overscript upper M Endscripts f Subscript i Baseline comma the total number of errors found in the upper M periods of testing comma 2nd Row 1st Column Blank 2nd Column upper B equals sigma summation Underscript i equals 1 Overscript upper M Endscripts left parenthesis upper F Subscript i minus 1 Baseline plus 1 right parenthesis x Subscript i Baseline comma and 3rd Row 1st Column Blank 2nd Column upper A equals sigma summation Underscript i equals 1 Overscript upper M Endscripts x Subscript i Baseline comma the total length of the testing period period EndLayoutFM = 
M

i=1 
fi, 
the total number of errors found in the M periods of testing, 
B = 
M

i=1 
(Fi−1 + 1)xi, 
and 
A = 
M

i=1 
xi, 
the total length of the testing period. 
From these estimates, the maximum likelihood estimate of the mean time until the 
next failure (MTTF) given the information accumulated in the M testing periods is 
equal to StartFraction 1 Over ModifyingAbove phi With caret left parenthesis ModifyingAbove upper N With caret minus upper F Subscript upper M Baseline right parenthesis EndFraction
1 
ˆφ( ˆN−FM) . 
The Jelinski–Moranda model and the various extensions to this model are 
classiﬁed as time domain models. They rely on a physical modeling of the 
appearance and ﬁxing of software failures. The different sets of assumptions are 
translated into differences in mathematical formulations. 
For non-independent data, a non-homogeneous Poisson process (NHPP) can be 
assumed. Denote by upper N left parenthesis t right parenthesisN(t) the number of observed failures until time t, and the main 
assumptions of this type of models are: 
(a) 
.N(0) = 0. 
(b) 
.{N(t), t ≥o} has independent increments. 
(c) At time t, upper N left parenthesis t right parenthesisN(t) follows a Poisson distribution with parameter m left parenthesis t right parenthesism(t), i.e., 
upper P left brace upper N left parenthesis t right parenthesis equals n right brace equals StartFraction left parenthesis m left parenthesis t right parenthesis right parenthesis Superscript n Baseline Over n factorial EndFraction exp left parenthesis minus m left parenthesis t right parenthesis right parenthesis comma n equals 0 comma 1 comma 2 comma midline horizontal ellipsis commaP{N(t) = n} =  (m(t))n 
n! 
exp (−m(t)) ,
n
 
= 0, 1, 2, · · ·  , 
where m left parenthesis t right parenthesism(t) is called the mean value function of the NHPP, which describes the 
expected cumulative number of experienced failures in time interval left parenthesis 0 comma t right bracket(0, t]. 
The failure intensity function, lamda left parenthesis t right parenthesisλ(t), is deﬁned as 
lamda left parenthesis t right parenthesis identical to limit Underscript upper Delta t right arrow 0 Endscripts StartFraction upper P left brace upper N left parenthesis t plus upper Delta t right parenthesis minus upper N left parenthesis t right parenthesis greater than 0 right brace Over upper Delta t EndFraction equals StartFraction normal d m left parenthesis t right parenthesis Over normal d t EndFraction comma t greater than or equals 0λ(t) ≡ lim
t→0 
P {N(t + t) − N(t) > 0}
t
= dm(t) 
dt ,
t
 
≥ 0 
Generally, by using a different mean value function of m left parenthesis t right parenthesism(t), we get different NHPP 
models (Fig. 11.5). 

11.12 
Sequential Reliability Testing
433 
Fig. 11.5 An example of failure intensity function lamda left parenthesis t right parenthesisλ(t) and mean value function m left parenthesis t right parenthesism(t) 
An NHPP model was proposed by Goel and Okumoto (1979) and many other 
NHPP models are modiﬁcation or generalization of this model. The mean value 
function of the Goel–Okumoto model is 
m left parenthesis t right parenthesis equals a left bracket 1 minus exp left parenthesis minus b t right parenthesis right bracket comma a greater than 0 comma b greater than 0 commam(t) = a

1 − exp(−bt)

,
a
 
>
 
0, b  >  0, 
where a is the expected number of faults to be eventually detected and b is the failure 
occurrence rate per fault. The failure intensity function of the Goel–Okumoto model 
is 
lamda left parenthesis t right parenthesis equals a b exp left parenthesis minus b t right parenthesis periodλ(t) = ab exp(−bt). 
Musa and Okumoto (1984) developed a logarithmic Poisson execution time 
model. The mean value function is 
m left parenthesis t right parenthesis equals StartFraction 1 Over phi EndFraction ln left parenthesis lamda 0 phi t plus 1 right parenthesis comma phi greater than 0 comma lamda 0 greater than 0 commam(t) = 1 
ϕ ln (λ0ϕt + 1) ,
ϕ
 
>
 
0, λ0 > 0, 
where lamda 0λ0 is the initial failure intensity and phiϕ is the failure intensity decay parameter. 
Since this model allows an inﬁnite number of failures to be observed, it is also called 
an inﬁnite failure model. 
It is sometimes observed that the curve of the cumulative number of faults is S-
shaped. Several different S-shaped NHPP models have been proposed in the existing 
literature. Among them, the most interesting ones are the delayed S-shaped NHPP 
model (Yamada et al. 1984) and the inﬂected S-shaped NHPP model (Ohba 1984). 
The mean value function of the delayed S-shaped NHPP model is 
m left parenthesis t right parenthesis equals a left bracket 1 minus left parenthesis 1 plus b t right parenthesis exp left parenthesis minus b t right parenthesis right bracket comma a greater than 0 comma b greater than 0m(t) = a

1 −(1 + bt) exp(−bt)

,
a
 
>
 
0, b  >  0 

434
11 
Sampling Plans for Batch and Sequential Inspection 
and the mean value function of the inﬂected S-shaped NHPP model is 
m left parenthesis t right parenthesis equals StartFraction a left bracket 1 minus exp left parenthesis minus b t right parenthesis right bracket Over 1 plus c exp left parenthesis minus b t right parenthesis EndFraction comma a greater than 0 comma b greater than 0 comma c greater than 0 periodm(t) = a

1 − exp(−bt)

1 + c exp(−bt) ,
a
 
>
 
0, b  >  0, c  >  0. 
An interesting model called log-power model was proposed in Xie and Zhao 
(1993). It has the mean value function 
m left parenthesis t right parenthesis equals a left bracket ln left parenthesis 1 plus t right parenthesis right bracket Superscript b Baseline comma a greater than 0 comma b greater than 0 periodm(t) = a [ln(1 + t)]b ,
a
 
>
 
0, b  >  0. 
This model is a modiﬁcation of the traditional Duane model for general repairable 
system. A useful property is that if we take the logarithmic on both sides of the mean 
value function, we have 
ln m left parenthesis t right parenthesis equals ln a plus b ln ln left parenthesis 1 plus t right parenthesis periodln m(t) = ln a + b ln ln(1 + t). 
Hence a graphical procedure is established. If we plot the observed cumulative 
number of failures versus left parenthesis t plus 1 right parenthesis(t + 1), the plot should tend to be on a straight line on a 
log–log scale. This can be used to easily estimate the model parameters and, more 
importantly, to validate the model. 
Example 11.6 The dataset FAILURE_J3 contains the cumulative number of fail-
ures on a software project over a period of 41 weeks. The shape of the curve suggests 
that the Goel–Okumoto or the S-shaped Yamada model should provide good ﬁts. We 
ﬁrst deﬁne the two functions using Python. 
def GoelOkumoto(t, a, b): 
return a * (1 - np.exp(-b * t)) 
def Yamada(t, a, b): 
return a * (1 - (1+b*t)*np.exp(-b*t)) 
The scipy function curve_ﬁt allows to ﬁt the function to the data. 
def optimizeModelFit(model, data): 
fit = optimize.curve_fit(model, data['T'], data['CFC']) 
popt = fit[0] 
# add the fit to the dataset 
data[model.__name__] = [model(t, *popt) for t in data['T']] 
return popt 
data = mistat.load_data('FAILURE_J3') 
goFit = optimizeModelFit(GoelOkumoto, data) 
ohbaFit = optimizeModelFit(Yamada, data) 
Figure 11.6 shows the resulting ﬁt curves. The S-shaped Yamada model leads to 
a better ﬁt and describes the data better.
black medium square■
Bayesian assumptions in an NHPP model have been proposed by Littlewood and 
Verrall (1973). This Bayesian software reliability model assumes that times between 

11.12 
Sequential Reliability Testing
435 
Fig. 11.6 Goel–Okumoto and Yamada models ﬁtted to cumulative failure count data FAIL-
URE_J3 
failures are exponentially distributed with a parameter that is treated as a random 
variable with a Gamma prior distribution. 
In system and software reliability model, parameters have to be estimated with 
historical data and maximum likelihood estimation (MLE) is a commonly adopted 
method for parameter estimation. 
Failure data is of two different types, i.e., numbers of failures or failure times. 
For the ﬁrst case, denote by n Subscript ini the number of failures observed in time interval 
left parenthesis s Subscript i minus 1 Baseline comma s Subscript i Baseline right bracket(si−1, si], where 0 identical to s 0 less than s 1 ellipsis s Subscript k Baseline0 ≡s0 < s1 . . . sk and s Subscript isi (i greater than 0i >  0) is the prescribed time point in the 
software testing process, and then the likelihood function for an NHPP model with 
mean value function m left parenthesis t right parenthesism(t) is 
upper L left parenthesis n 1 comma n 2 comma ellipsis comma n Subscript k Baseline right parenthesis equals product Underscript i equals 1 Overscript k Endscripts StartFraction left bracket m left parenthesis s Subscript i Baseline right parenthesis minus m left parenthesis s Subscript i minus 1 Baseline right parenthesis right bracket Superscript n Super Subscript i Superscript Baseline exp left brace minus left bracket m left parenthesis s Subscript i Baseline right parenthesis minus m left parenthesis s Subscript i minus 1 Baseline right parenthesis right bracket right brace Over n Subscript i Baseline factorial EndFraction periodL(n1, n2, . . . , nk) = 
k
i=1 
[m(si) − m(si−1)]ni exp {−[m(si) − m(si−1)]} 
ni!
. 
The parameters in m left parenthesis t right parenthesism(t) can be estimated by maximizing the likelihood function 
given above. Usually, numerical procedures have to be used. 
For the second case, denote by upper T Subscript iTi, i equals 1 comma 2 comma ellipsis comma ki = 1, 2, . . . , k, the observed k failure times 
in a software testing process, and then the likelihood function is 
upper L left parenthesis upper T 1 comma upper T 2 comma ellipsis comma upper T Subscript k Baseline right parenthesis equals exp left bracket minus m left parenthesis upper T Subscript k Baseline right parenthesis right bracket product Underscript i equals 1 Overscript k Endscripts lamda left parenthesis upper T Subscript i Baseline right parenthesis periodL(T1, T2, . . . , Tk) = exp [−m(Tk)] 
k
i=1 
λ(Ti). 
In order to ﬁnd asymptotic conﬁdence intervals for the k model parameters, the 
derivation of the Fisher information matrix is needed, which is given by 

436
11 
Sampling Plans for Batch and Sequential Inspection 
upper I left parenthesis theta 1 comma ellipsis comma theta Subscript k Baseline right parenthesis identical to Start 4 By 4 Matrix 1st Row 1st Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential theta 1 squared EndFraction right bracket 2nd Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential theta 2 partial differential theta 1 EndFraction right bracket 3rd Column midline horizontal ellipsis 4th Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential theta Subscript k Baseline partial differential theta 1 EndFraction right bracket 2nd Row 1st Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential theta 1 partial differential theta 2 EndFraction right bracket 2nd Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential theta 2 squared EndFraction right bracket 3rd Column midline horizontal ellipsis 4th Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential theta Subscript k Baseline partial differential theta 2 EndFraction right bracket 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column Blank 4th Column vertical ellipsis 4th Row 1st Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential theta 1 partial differential theta Subscript k Baseline EndFraction right bracket 2nd Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential theta 2 partial differential theta Subscript k Baseline EndFraction right bracket 3rd Column midline horizontal ellipsis 4th Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential theta Subscript k Superscript 2 Baseline EndFraction right bracket EndMatrix periodI(θ1, . . . , θk) ≡ 
⎡ 
⎢⎢⎢ 
⎢⎢⎢⎢⎢⎢ 
⎣ 
−E

∂2 ln L 
∂θ2 
1

−E
"
∂2 ln L 
∂θ2∂θ1
#
· · ·  −E
"
∂2 ln L 
∂θk∂θ1
#
−E
"
∂2 ln L 
∂θ1∂θ2
#
−E

∂2 ln L 
∂θ2 
2

· · ·  −E
"
∂2 ln L 
∂θk∂θ2
#
... 
... 
... 
−E
"
∂2 ln L 
∂θ1∂θk
#
−E
"
∂2 ln L 
∂θ2∂θk
#
· · · −E

∂2 ln L 
∂θ2 
k

⎤ 
⎥⎥⎥ 
⎥⎥⎥⎥⎥⎥ 
⎦ 
. 
From the asymptotic theory of MLE, when n approaches to inﬁnity, left bracket ModifyingAbove theta With caret Subscript 1 Baseline comma ellipsis comma ModifyingAbove theta With caret Subscript k Baseline right bracket
"
ˆθ1, . . . ,  ˆθk
#
converges in distribution to k-variate normal distribution with mean left bracket theta 1 comma ellipsis comma theta Subscript k Baseline right bracket[θ1, . . . , θk] 
and covariance matrix upper I Superscript negative 1I −1. That is, the asymptotic covariance matrix of the MLEs 
is 
upper V Subscript e Baseline identical to Start 4 By 4 Matrix 1st Row 1st Column Var left parenthesis ModifyingAbove theta With caret Subscript 1 Baseline right parenthesis 2nd Column Cov left parenthesis ModifyingAbove theta With caret Subscript 1 Baseline comma ModifyingAbove theta With caret Subscript 2 Baseline right parenthesis 3rd Column midline horizontal ellipsis 4th Column Cov left parenthesis ModifyingAbove theta With caret Subscript 1 Baseline comma ModifyingAbove theta With caret Subscript k Baseline right parenthesis 2nd Row 1st Column Cov left parenthesis ModifyingAbove theta With caret Subscript 2 Baseline comma ModifyingAbove theta With caret Subscript 1 Baseline right parenthesis 2nd Column Var left parenthesis ModifyingAbove theta With caret Subscript 2 Baseline right parenthesis 3rd Column midline horizontal ellipsis 4th Column Cov left parenthesis ModifyingAbove theta With caret Subscript 2 Baseline comma ModifyingAbove theta With caret Subscript k Baseline right parenthesis 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column Blank 4th Column vertical ellipsis 4th Row 1st Column Cov left parenthesis ModifyingAbove theta With caret Subscript k Baseline comma ModifyingAbove theta With caret Subscript 1 Baseline right parenthesis 2nd Column Cov left parenthesis ModifyingAbove theta With caret Subscript k Baseline comma ModifyingAbove theta With caret Subscript 2 Baseline right parenthesis 3rd Column midline horizontal ellipsis 4th Column Var left parenthesis ModifyingAbove theta With caret Subscript k Baseline right parenthesis EndMatrix equals upper I Superscript negative 1 Baseline periodVe ≡ 
⎡ 
⎢⎢⎢ 
⎣ 
Var( ˆθ1) 
Cov( ˆθ1, ˆθ2) · · · Cov( ˆθ1, ˆθk) 
Cov( ˆθ2, ˆθ1) 
Var( ˆθ2) 
· · ·  Cov( ˆθ2, ˆθk) 
... 
... 
... 
Cov( ˆθk, ˆθ1) Cov( ˆθk, ˆθ2) · · ·  Var( ˆθk) 
⎤ 
⎥⎥⎥ 
⎦= I −1. 
Therefore, the asymptotic 100 left parenthesis 1 minus alpha right parenthesis100(1 − α)% conﬁdence interval for ModifyingAbove theta With caret Subscript iˆθi is 
left parenthesis ModifyingAbove theta With caret Subscript i Baseline minus z Subscript alpha divided by 2 Baseline StartRoot Var left parenthesis ModifyingAbove theta With caret Subscript i Baseline right parenthesis EndRoot comma ModifyingAbove theta With caret Subscript i Baseline plus z Subscript alpha divided by 2 Baseline StartRoot Var left parenthesis ModifyingAbove theta With caret Subscript i Baseline right parenthesis EndRoot right parenthesis comma i equals 1 comma ellipsis comma k comma

ˆθi − zα/2
'
Var( ˆθi), 
ˆθi + zα/2
'
Var( ˆθi)

,
i
 
= 1, . . . , k,  
where z Subscript alpha divided by 2zα/2 is the 1 minus alpha divided by 21 − α/2 percentile of the standard normal distribution and the 
quantity Var left parenthesis ModifyingAbove theta With caret Subscript i Baseline right parenthesisVar( ˆθi) can be obtained from the covariance matrix given above. 
Since the true values of ModifyingAbove theta With caret Subscript iˆθi’s are unknown, the observed information matrix is 
often used 
ModifyingAbove upper I With caret left parenthesis ModifyingAbove theta With caret Subscript 1 Baseline comma ellipsis comma ModifyingAbove theta With caret Subscript k Baseline right parenthesis identical to Start 4 By 4 Matrix 1st Row 1st Column minus StartFraction partial differential squared ln upper L Over partial differential theta 1 squared EndFraction 2nd Column minus StartFraction partial differential squared ln upper L Over partial differential theta 2 partial differential theta 1 EndFraction 3rd Column midline horizontal ellipsis 4th Column minus StartFraction partial differential squared ln upper L Over partial differential theta Subscript k Baseline partial differential theta 1 EndFraction 2nd Row 1st Column minus StartFraction partial differential squared ln upper L Over partial differential theta 1 partial differential theta 2 EndFraction 2nd Column minus StartFraction partial differential squared ln upper L Over partial differential theta 2 squared EndFraction 3rd Column midline horizontal ellipsis 4th Column minus StartFraction partial differential squared ln upper L Over partial differential theta Subscript k Baseline partial differential theta 2 EndFraction 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column Blank 4th Column vertical ellipsis 4th Row 1st Column minus StartFraction partial differential squared ln upper L Over partial differential theta 1 partial differential theta Subscript k Baseline EndFraction 2nd Column minus StartFraction partial differential squared ln upper L Over partial differential theta 2 partial differential theta Subscript k Baseline EndFraction 3rd Column midline horizontal ellipsis 4th Column minus StartFraction partial differential squared ln upper L Over partial differential theta Subscript k Superscript 2 Baseline EndFraction EndMatrix Subscript StartLayout 1st Row theta 1 equals ModifyingAbove theta With caret Sub Subscript 1 Subscript 2nd Row q u a d midline horizontal ellipsis 3rd Row q u a d theta Sub Subscript k Subscript equals ModifyingAbove theta With caret Sub Subscript k Subscript EndLayout Baseline commaˆI( ˆθ1, . . . ,  ˆθk) ≡ 
⎡ 
⎢ 
⎢⎢⎢⎢⎢ 
⎣ 
−∂2 ln L 
∂θ2 
1 
−∂2 ln L 
∂θ2∂θ1 · · · −∂2 ln L 
∂θk∂θ1 
−∂2 ln L 
∂θ1∂θ2 −∂2 ln L 
∂θ2 
2 
· · · −∂2 ln L 
∂θk∂θ2 
... 
... 
... 
−∂2 ln L 
∂θ1∂θk −∂2 ln L 
∂θ2∂θk · · ·  −∂2 ln L 
∂θ2 
k 
⎤ 
⎥ 
⎥⎥⎥⎥⎥ 
⎦ 
θ1= ˆθ1
··· 
θk= ˆθk 
, 
and the conﬁdence interval for ModifyingAbove theta With caret Subscript iˆθi can be calculated. 
If upper Phi identical to g left parenthesis theta 1 comma ellipsis comma theta Subscript k Baseline right parenthesis ≡ g(θ1, . . . , θk), where g left parenthesis dot right parenthesisg(·) is a continuous function, then as n converges to 
inﬁnity, ModifyingAbove upper Phi With caretˆ converges in distribution to normal distribution with mean upper Phi and variance 
Var left parenthesis ModifyingAbove upper Phi With caret right parenthesis equals sigma summation Underscript i equals 1 Overscript k Endscripts sigma summation Underscript j equals 1 Overscript k Endscripts StartFraction partial differential g Over partial differential theta Subscript i Baseline EndFraction dot StartFraction partial differential g Over partial differential theta Subscript j Baseline EndFraction dot nu Subscript i j Baseline commaVar( ˆ) = 
k

i=1 
k

j=1 
∂g 
∂θi 
· ∂g 
∂θj 
· νij, 

11.12 
Sequential Reliability Testing
437 
where nu Subscript i jνij is the element of the ith row and jth column of the matrix above. The 
asymptotic 100 left parenthesis 1 minus alpha right parenthesis100(1 − α)% conﬁdence interval for ModifyingAbove upper Phi With caretˆ is 
left parenthesis ModifyingAbove upper Phi With caret minus z Subscript alpha divided by 2 Baseline StartRoot Var left parenthesis ModifyingAbove upper Phi With caret right parenthesis EndRoot comma ModifyingAbove upper Phi With caret plus z Subscript alpha divided by 2 Baseline StartRoot Var left parenthesis ModifyingAbove upper Phi With caret right parenthesis EndRoot right parenthesis period

ˆ −zα/2
'
Var( ˆ),
ˆ + zα/2
'
Var( ˆ)

. 
The above result can be used to obtain the conﬁdence interval for some useful 
quantities such as failure intensity or reliability. 
Example 11.7 Assume that there are two model parameters, a and b, which is quite 
common in most of the software reliability models, and the Fisher information 
matrix is given by 
upper I left parenthesis theta 1 comma ellipsis comma theta Subscript k Baseline right parenthesis identical to Start 2 By 2 Matrix 1st Row 1st Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential a squared EndFraction right bracket 2nd Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential a partial differential b EndFraction right bracket 2nd Row 1st Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential a partial differential b EndFraction right bracket 2nd Column minus upper E left bracket StartFraction partial differential squared ln upper L Over partial differential b squared EndFraction right bracket EndMatrix periodI(θ1, . . . , θk) ≡ 
⎡ 
⎣−E
"
∂2 ln L 
∂a2
#
−E
"
∂2 ln L 
∂a∂b
#
−E
"
∂2 ln L 
∂a∂b
#
−E
"
∂2 ln L 
∂b2
#
⎤ 
⎦ . 
The asymptotic covariance matrix of the MLEs is the inverse of this matrix: 
upper V Subscript e Baseline equals upper I Superscript negative 1 Baseline equals Start 2 By 2 Matrix 1st Row 1st Column Var left parenthesis ModifyingAbove a With caret right parenthesis 2nd Column Cov left parenthesis ModifyingAbove a With caret comma ModifyingAbove b With caret right parenthesis 2nd Row 1st Column Cov left parenthesis ModifyingAbove a With caret comma ModifyingAbove b With caret right parenthesis 2nd Column Var left parenthesis ModifyingAbove b With caret right parenthesis EndMatrix periodVe = I −1 =
 Var(ˆa) Cov(ˆa, ˆb) 
Cov(ˆa, ˆb) Var(ˆb)

. 
For illustrative purposes, we take the data cited in Table 1 in Zhang and Pham 
(1998). The observed information matrix is 
ModifyingAbove upper I With caret identical to Start 2 By 2 Matrix 1st Row 1st Column minus StartFraction partial differential squared ln upper L Over partial differential a squared EndFraction 2nd Column minus StartFraction partial differential squared ln upper L Over partial differential a partial differential b EndFraction 2nd Row 1st Column minus StartFraction partial differential squared ln upper L Over partial differential a partial differential b EndFraction 2nd Column minus StartFraction partial differential squared ln upper L Over partial differential b squared EndFraction EndMatrix Subscript StartLayout 1st Row a equals ModifyingAbove a With caret 2nd Row q u a d b equals ModifyingAbove b With caret EndLayout Baseline equals Start 2 By 2 Matrix 1st Row 1st Column 0.0067 2nd Column 1.1095 2nd Row 1st Column 1.1095 2nd Column 4801.2 EndMatrix periodˆI ≡
(
−∂2 ln L 
∂a2 
−∂2 ln L 
∂a∂b 
−∂2 ln L 
∂a∂b −∂2 ln L 
∂b2
)
a=ˆa 
b=ˆb 
=
0.0067 1.1095 
1.1095 4801.2

. 
We can then obtain the asymptotic variance and covariance of the MLE as follows: 
StartLayout 1st Row Var left parenthesis ModifyingAbove a With caret right parenthesis equals 154.85 comma Var left parenthesis ModifyingAbove b With caret right parenthesis equals 2.17 times 10 Superscript negative 4 Baseline comma 2nd Row Cov left parenthesis ModifyingAbove a With caret comma ModifyingAbove b With caret right parenthesis equals negative 0.0358 period EndLayoutVar(ˆa) = 154.85,
Var(ˆb) = 2.17 × 10−4, 
Cov(ˆa, ˆb) = −0.0358. 
The 95% conﬁdence intervals on parameters a and b are left parenthesis 117.93 comma 166.71 right parenthesis(117.93, 166.71) and 
left parenthesis 0.0957 comma 0.1535 right parenthesis(0.0957, 0.1535), respectively. Plugging these numbers in 
m left parenthesis t right parenthesis equals a left bracket ln left parenthesis 1 plus t right parenthesis right bracket Superscript b Baseline comma a greater than 0 comma b greater than 0 periodm(t) = a [ln(1 + t)]b ,
a
 
>
 
0, b  >  0. 
gives us a range of estimates of the software reliability growth performance. With 
such models, we can determine if and when the system is ready for deployment in 
the market.
black medium square■
In general, a software development process consists of the following four major 
phases: speciﬁcation, design, coding, and testing. The testing phase is the most 

438
11 
Sampling Plans for Batch and Sequential Inspection 
(a) A software system is composed of n independent modules which are developed 
and tested independently during the unit testing phase. 
(b) In the unit testing phase, each software module is subject to failures at random 
times caused by faults remaining in the software module. 
(c) The failure observation process of software module i is modeled by an NHPP 
with mean value function m Subscript i Baseline left parenthesis t right parenthesismi(t) or failure intensity function lamda Subscript i Baseline left parenthesis t right parenthesis identical to StartFraction normal d m Subscript i Baseline left parenthesis t right parenthesis Over normal d t EndFractionλi(t) ≡ dmi(t) 
dt . 
A total amount of testing time T is available for the whole software system that 
consists of a few modules. Testing time should be allocated to each software module 
in such a way that the reliability of the system after unit testing phase will be 
maximized. 
Finally, Kenett and Pollak (1986, 1996) propose the application of the Shiryaev– 
Roberts sequential change control procedure to determine if a system or software 
under test has reached a required level of reliability. This approach does not require 
a speciﬁc model, such as those listed above, and has been therefore labeled “semi 
parametric.” The Shiryaev–Roberts procedure is based on a statistic that is a sum of 
likelihood ratios. If there is no change, it is assumed that the observations follow a 
known distribution whose density is denoted as f Subscript nu equals normal infinityfν=∞, and if there is a change at 
time nu equals kν = k, then the density of the observations is denoted as f Subscript nu equals kfν=k. Denoting the 
likelihood ratio of the observations upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript n BaselineX1, X2, . . . , Xn when nu equals kν = k by 
upper Lamda Subscript n comma k Baseline equals StartFraction f Subscript nu equals k Baseline left parenthesis upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript n Baseline right parenthesis Over f Subscript nu equals normal infinity Baseline left parenthesis upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript n Baseline right parenthesis EndFractionn,k = fν=k(X1, X2, . . . , Xn) 
fν=∞(X1, X2, . . . , Xn) 
where the observations may be dependent, the Shiryaev-Roberts surveillance 
statistic is 
upper R Subscript n Baseline equals sigma summation Underscript k equals 1 Overscript n Endscripts upper Lamda Subscript n comma k Baseline periodRn = 
n

k=1
n,k. 
The scheme calls for releasing the system for distribution at 
upper N Subscript upper A Baseline equals min left brace n vertical bar upper R Subscript n Baseline greater than or equals upper A right brace commaNA = min{n | Rn ≥ A}, 
where A is chosen so that the average number of observations until false alarm 
(premature release) is equal to (or larger than) a prespeciﬁed constant B. Under 
fairly general conditions, there exists a constant c greater than 1c >  1 such that 
limit Underscript upper A right arrow normal infinity Endscripts upper E Subscript nu equals normal infinity Baseline left parenthesis upper N Subscript upper A Baseline divided by upper A right parenthesis equals c comma lim 
A→∞ Eν=∞(NA/A) = c, 
costly and time-consuming one. It is thus very much important for the management 
to spend the limited testing-resource efﬁciently. For the optimal testing-resource 
allocation problem, the following assumptions are made: 

11.13 
Chapter Highlights
439 
where upper E left parenthesis right parenthesisE() stands for the expected value so that setting upper A equals upper B divided by cA = B/c yields a scheme 
that approximately has B as its average number of observations until a possible 
premature release of the software. 
If f Subscript theta Baseline left parenthesis x right parenthesisfθ(x) represents an exponential distribution with parameter thetaθ, where theta equals w 0θ = w0 
is a prespeciﬁed level of failure rate where the system is deemed unreliable if its 
failure rate exceeds it, and theta equals w 1θ = w1 is an acceptable failure rate (w 1 less than w 0w1 < w0), then 
upper R Subscript n Baseline equals sigma summation Underscript k equals 1 Overscript n Endscripts upper Lamda Subscript n comma k Baseline equals sigma summation Underscript k equals 1 Overscript n Endscripts left parenthesis StartFraction w 1 Over w 0 EndFraction right parenthesis Superscript n minus k plus 1 Baseline exp left parenthesis left parenthesis w 0 minus w 1 right parenthesis sigma summation Underscript i equals k Overscript n Endscripts upper X Subscript i Baseline right parenthesis periodRn = 
n

k=1
n,k = 
n

k=1

w1 
w0
n−k+1 
exp

(w0 −w1) 
n

i=k 
Xi

. 
In this case, the computation is recursive and 
upper R Subscript n plus 1 Baseline equals left parenthesis StartFraction w 1 Over w 0 EndFraction right parenthesis exp left parenthesis left parenthesis w 0 minus w 1 right parenthesis upper X Subscript n plus 1 Baseline right parenthesis left parenthesis 1 plus upper R Subscript n Baseline right parenthesis periodRn+1 =

w1 
w0

exp ((w0 − w1)Xn+1) (1 + Rn). 
Tracking upper R Subscript nRn, and comparing it to A, provides an easily implemented procedure for 
determining when a system or software version is ready for release. 
In this section we reviewed a range of models used in sequential reliability 
testing. The inputs to these models consist of data on time of failure or a number 
of failures. Sequential and sampling based methods are used to monitor product 
and process quality. Designed experiments are used to design and improve products 
and processes. The book covers traditional and modern methods used to achieve 
enhanced monitoring, optimized designs, and reliable systems and products. Python 
is used throughout so that the reader can try the methods described within, reproduce 
the examples, and assess the learning with exercises that are accompanied by 
solutions. 
11.13 Chapter Highlights 
The main concepts and deﬁnitions introduced in this chapter include: 
• 
Lot 
• 
Acceptable quality level 
• 
Limiting quality level 
• 
Producer’s risk 
• 
Consumer’s risk 
• 
Single-stage sampling 
• 
Acceptance number 
• 
Operating characteristic 
• 
Double sampling plan 
• 
ASN—function 
• 
Sequential sampling 
• 
Sequential probability ratio test (SPRT) 

440
11 
Sampling Plans for Batch and Sequential Inspection 
• 
Rectifying inspection 
• 
Average outgoing quality (AOQ) 
• 
Average total inspection (ATI) 
• 
Tightened, normal or reduced inspection levels 
• 
Skip-lot sampling plans 
• 
Sequential reliability testing 
• 
One arm bandit (OAT) 
• 
Two-arm bandit (TAB) 
• 
A/B testing 
• 
Software reliability testing 
11.14 Exercises 
Exercise 11.1 Determine single sampling plans for attributes, when the lot is N = 
2500, α = β = 0.01, and 
(i) AQL = 0.005, LQL = 0.01 
(ii) AQL = 0.01, LQL = 0.03 
(iii) AQL = 0.01, LQL = 0.05 
Exercise 11.2 Investigate how the lot size, N, inﬂuences the single sampling plans 
for attributes, when α = β = 0.05, AQL = 0.01, LQL = 0.03, by computing the 
plans for N = 100, N = 500, N = 1, 000, N = 2, 000. 
Exercise 11.3 Compute the OC(p) function for the sampling plan computed in 
Exercise 11.1(iii). What is the probability of accepting a lot having 2.5% of 
nonconforming items? 
Exercise 11.4 Compute the large sample approximation to a single sample plan for 
attributes (n∗, c∗), with α = β = 0.05 and AQL = 0.025, LQL = 0.06. Compare 
these to the exact results. The lot size is N = 2, 000. 
Exercise 11.5 Repeat the previous Exercise with N = 3000, α = β = 0.10, 
AQL = 0.01, and LQL = 0.06. 
Exercise 11.6 Obtain the OC and ASN functions of the double sampling plan, 
with n1 = 200, n2 = 2n1, c1 = 5, and c2 = c3 = 15, when N = 2, 000. 
(i) What are the attained α and β when AQL = 0.015 and LQL = 0.05? 
(ii) What is the ASN when p = AQL? 
(iii) What is a single sampling plan having the same α and β? How  many  
observations we expect to save if p = AQL? Notice that if p = LQL, the  
present double sampling plan is less efﬁcient than the corresponding single 
sampling plan. 
Exercise 11.7 Compute the OC and ASN values for a double sampling plan with 
n1 = 150, n2 = 200, c1 = 5, and c2 = c3 = 10, when N = 2, 000. Notice 

11.14 
Exercises
441 
how high β is when LQL = 0.05. The present plan is reasonable if LQL = 0.06. 
Compare this plan to a single sampling one for α = 0.02, β = 0.10, AQL = 0.02, 
and LQL = 0.06. 
Exercise 11.8 Determine 
a 
sequential 
plan 
for 
the 
case 
of 
AQL = 0.02, 
LQL = 0.06, and α = β = 0.05. Compute the OC and ASN functions of this 
plan. What are the ASN values when p = AQL, p = LQL, and p = 0.035? 
Exercise 11.9 Compare the single sampling plan and the sequential one when 
AQL = 0.01, LQL = 0.05, α = β = 0.01, and N = 10, 000. What are the expected 
savings in sampling cost if each observation costs $1, and p = AQL? 
Exercise 11.10 Use the mistat function simulateOAB to simulate the expected 
rewards, for p = 0.4(0.05)0.8, when N = 75, λ = 0.6, k = 15, γ = 0.95, and 
Ns = 1000. 
Exercise 11.11 Use the mistat function optimalOAB to predict the expected 
reward under the optimal strategy, when N = 75, λ = 0.6. 
Exercise 11.12 Consider the two-armed bandit (TAB) with N = 40 and K = 10. 
Make a table of all the possible predicted rewards. 
Exercise 11.13 Determine n and k for a continuous variable size sampling plan, 
when (p0) = AQL = 0.01 and (pt) = LQL = 0.05, α = β = 0.05. 
Exercise 11.14 Consider dataset ALMPIN.csv. An aluminum pin is considered as 
defective if its cap diameter is smaller than 14.9 [mm]. For the parameters p0 = 0.01 
and α = 0.05, compute k and decide whether to accept or reject the lot, on the 
basis of the sample of n = 70 pins. What is the probability of accepting a lot with 
proportion of defectives of p = 0.03? 
Exercise 11.15 Determine the sample size and k for a single sampling plan by a 
normal variable, with the parameters AQL = 0.02, LQL = 0.04, and α = β = 
0.10. 
Exercise 11.16 A single sampling plan for attributes, from a lot of size N = 500, 
is given by n = 139 and c = 3. Each lot that is not accepted is rectiﬁed. Compute 
the AOQ, when p = 0.01, p = 0.02, p = 0.03, and p = 0.05. What are the 
corresponding AT I values? 
Exercise 11.17 A single sampling plan, under normal inspection, has probability 
α = 0.05 of rejection, when p = AQL. What is the probability, when p = AQL 
in 5 consecutive lots, that there will be a switch to tightened inspection? What is the 
probability of switching to a tightened inspection if p increases so that OC(p) = 
0.7? 
Exercise 11.18 Compute the probability for qualifying for State 2, in a Skip-Lot 
sampling plan, when n = 100, c = 1. What is the upper bound on S10, in order to 
qualify for State 2, when AQL = 0.01? Compute the probability QP for State 2 
qualiﬁcation. 

442
11 
Sampling Plans for Batch and Sequential Inspection 
Exercise 11.19 The FAILURE_J2 dataset contains the cumulative failure counts 
of a software project collected over a period of 181 weeks. Fit the following models 
to the data and visualize the results: 
• 
Goel–Okumoto f (t)  = a[1 − exp(−bt)] 
• 
Musa–Okumoto f (t)  = 1 
ϕ log(λ0ϕt + 1) 
• 
S-shaped Yamada f (t)  = a(1 − (1 + bt) exp(−bt)) 
• 
Inﬂected S-shaped Ohba f (t)  = a(1−exp(−bt)) 
1+c exp(−bt) 
Which model describes the data best? 
Exercise 11.20 Continuing with Exercise 11.19, simulate cases where you only 
have data for the ﬁrst 25, 50, 75, 100, or 125 weeks and ﬁt Goel–Okumoto and 
inﬂected S-shaped models to these subsets. 
Discuss the results with respect to using the models to extrapolate the expected 
failure count to predict the expected failure count into the future. 
Exercise 11.21 The dataset FAILURE_DS2 contains cumulative failure counts of 
a software project collected over a period of 18 weeks. Fit an inﬂected S-shaped 
models to these data. 
You will see that the large initial number of failures leads to an insufﬁcient ﬁt 
of the data. Create a second ﬁt where the initial number of failures is ignored and 
discuss the results. 

Appendix A 
Introduction to Python 
There are many excellent books and online resources that can introduce you to 
Python. Python itself comes with an excellent tutorial that you can ﬁnd at https:// 
docs.python.org/3/tutorial/. Instead of duplicating here what has been improved over 
many years, we suggest the reader to follow the Python tutorial. In particular, we 
recommend reading the following chapters in the tutorial: 
• 
An Informal Introduction to Python 
• 
More Control Flow Tools 
• 
Data Structures 
In the following, we will point out a selection of more specialized topics that we 
use in the code examples throughout the book. 
A.1 
List, Set, and Dictionary Comprehensions 
Many data handling tasks require the creation of lists or dictionaries. We can use a 
for loop in this case: 
the_list = [] 
for i in range(10): 
the_list.append(2 * i) 
the_list 
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18] 
Instead of using the for loop, Python has a more concise way of achieving the 
same outcome using what is called a list comprehension: 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3
443

444
A
Introduction to Python
the_list = [2 * i for i in range(10)] 
the_list 
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18] 
List comprehensions can also be used if the addition to the list is conditional. In 
the following example, we create a list of numbers divisible by 3. 
the_list = [] 
for i in range(20): 
if i % 3 ==  0:  
the_list.append(i) 
the_list = [i for i in range(20) if i % 3 == 0]  
the_list 
[0, 3, 6, 9, 12, 15, 18] 
The list comprehension is easier to read. 
A similar construct can also be used to create sets: 
letters = ['a', 'y', 'x', 'a', 'y', 'z'] 
unique_letters = {c for c in letters} 
unique_letters 
{'a', 'x', 'y', 'z'} 
The set comprehension uses curly brackets instead of the square brackets in list 
comprehensions. 
Dictionary comprehensions create dictionaries. The following example creates a 
dictionary that maps a number to its square. We show ﬁrst the implementation using 
a for loop and then the dictionary comprehension: 
squares = {} 
for i in range(10): 
squares[i] = i * i 
squares = {i: i * i for i in range(10)} 
squares 
{0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81} 
A.2 
Scientiﬁc Computing Using numpy and scipy 
The Python packages numpy and scipy provide fast data structures and algo-
rithms for scientiﬁc computing. They are very popular and form the foundation on 
which many other data science packages in Python are built on. 
For example, numpy implements multidimensional arrays and operations for 
their manipulation. Here is an example:

A
Introduction to Python
445
import numpy as np 
data = np.array([[1, 2, 3], [4, 5, 6]]) 
print(data) 
print('Shape:', data.shape) 
print('Total sum:', data.sum()) 
print('Column sum:', data.sum(axis=0)) 
print('Row sum:', data.sum(axis=1)) 
[[1 2 3] 
[4 5 6]] 
Shape: (2, 3) 
Total sum: 21 
Column sum: [5 7 9] 
Row sum: [ 6 15] 
The function np.array creates the numpy array using a Python list of lists. Using 
the sum method of the numpy array, we can calculate the sum of all array elements. 
If we want to know the column or row sums, we can use the same method with the 
axis keyword argument. 
The scipy package is used in this book for a variety of tasks. Many of our appli-
cations make use of the numerous probability distributions in the scipy.stats 
module. The implementations allow to generate random numbers for a distribution 
and give access to its probability density function (p.d.f.), the cumulative distribu-
tion function (c.d.f.), and its inverse. For example: 
from scipy import stats 
# define normal distribution with mean 27 and standard deviation 5 
normal_distribution = stats.norm(loc=27, scale=5) 
# generate 5 random variables 
print(normal_distribution.rvs(size=5)) 
# calculate 5th-percentile 
print(f'5th-percentile: {normal_distribution.ppf(0.05):.2f}') 
[25.74640076 26.26699152 25.4434667 
26.80802939 21.18634297] 
5th-percentile: 18.78 
Both packages provide a large amount of functionality, which we cannot cover 
here. We highly recommend studying their documentation in detail. 
A.3 
Pandas Data Frames 
Most of the datasets used in this book are either in list form or tabular. The pandas 
package (https://pandas.pydata.org/) implements these data structures. The mistat 
package returns the data as either pandas DataFrame or Series objects.

446
A
Introduction to Python
import mistat 
almpin = mistat.load_data('ALMPIN') 
print('ALMPIN', type(almpin)) 
steelrod = mistat.load_data('STEELROD') 
print('STEELROD', type(steelrod)) 
ALMPIN <class 'pandas.core.frame.DataFrame'> 
STEELROD <class 'pandas.core.series.Series'> 
The DataFrame and Series objects offer additional functionality to use them in 
an efﬁcient and fast manner. As an example, here is the calculation of the column 
means: 
almpin.mean() 
diam1
9.992857 
diam2
9.987286 
diam3
9.983571 
capDiam
14.984571 
lenNocp
49.907857 
lenWcp
60.027857 
dtype: float64 
The describe method returns basic statistics for each column in a DataFrame. 
almpin.describe().round(3) 
diam1
diam2
diam3 
capDiam 
lenNocp 
lenWcp 
count 
70.000 
70.000 
70.000
70.000
70.000 
70.000 
mean
9.993
9.987
9.984
14.985
49.908 
60.028 
std
0.016
0.018
0.017
0.019
0.044
0.048 
min
9.900
9.890
9.910
14.880
49.810 
59.910 
25%
9.990
9.982
9.980
14.980
49.890 
60.000 
50%
10.000
9.990
9.990
14.990
49.910 
60.020 
75%
10.000 
10.000
9.990
14.990
49.928 
60.050 
max
10.010 
10.010 
10.010
15.010
50.070 
60.150 
As the pandas package is used frequently in many machine learning packages, 
we recommend that you make yourself familiar with it by reading the documenta-
tion. 
A.4 
Data Visualization Using pandas and matplotlib 
Packages like pandas or seaborn support a variety of visualizations that are 
often sufﬁcient for exploratory data analysis. However there may be cases where 
you want to customize the graph further to highlight aspects of your analysis. 
As these packages often use the matplotlib package (https://matplotlib.org/) 
as their foundation, we can achieve this customization using basic matplotlib 
commands.

A
Introduction to Python
447
(a) 
(b) 
Fig. A.1 Data visualization using pandas and customization. (a) Default graph created using 
pandas. (b) Customization of (b) using matplotlib commands 
This is demonstrated in Fig. A.1. Here, we use the matplotlib axis object that 
is returned from the pandas plot function to add additional lines to the graph. 
Each ﬁgure in this book was created using Python. The source code can be found 
in the accompanying repository at https://github.com/gedeck/mistat-code-solutions 
and in the mistat package maintained at https://github.com/gedeck/mistat.

Appendix B 
List of Python Packages 
arviz: 
Exploratory analysis of Bayesian models 
https://pypi.org/project/arviz/ 
https://github.com/arviz-devs/arviz 
dtreeviz: 
A Python 3 library for sci-kit learn, XGBoost, LightGBM, and Spark decision 
tree visualization 
https://pypi.org/project/dtreeviz/ 
https://github.com/parrt/dtreeviz 
lifelines: 
Survival analysis in Python, including Kaplan Meier, Nelson Aalen, and regres-
sion https://pypi.org/project/lifelines/ 
https://lifelines.readthedocs.io/ 
https://github.com/CamDavidsonPilon/lifelines 
matplotlib: 
Python plotting package 
https://pypi.org/project/matplotlib/ 
https://matplotlib.org/ 
mistat: 
Modern Statistics/Industrial Statistics: A Computer Based Approach with Python 
https://pypi.org/project/mistat/ 
numpy: 
NumPy is the fundamental package for array computing with Python 
https://pypi.org/project/numpy/ 
https://numpy.org/ 
pandas: 
Powerful data structures for data analysis, time series, and statistics 
https://pypi.org/project/pandas/ 
https://pandas.pydata.org/ 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3
449

450
B
List of Python Packages
pingouin 
Pingouin: statistical package for Python 
https://pypi.org/project/pingouin/ 
https://pingouin-stats.org/ 
pwlf 
pwlf: ﬁt piecewise linear functions to data 
https://pypi.org/project/pwlf/ 
https://github.com/cjekel/piecewise_linear_ﬁt_py 
pyDOE2: 
A Python 3 library for sci-kit learn, XGBoost, LightGBM, and Spark decision 
tree visualization 
https://pypi.org/project/pyDOE2/ 
https://github.com/clicumu/pyDOE2 
pyKriging: 
A Kriging Toolbox for Python 
https://pypi.org/project/pyKriging/ 
https://github.com/clicumu/pyKriging 
pymc3: 
Probabilistic Programming in Python: Bayesian Modeling and Probabilistic 
Machine Learning with Theano 
https://pypi.org/project/pymc3/ https://github.com/pymc-devs/pymc 
scikit-learn (sklearn): 
A set of python modules for machine learning and data mining 
https://pypi.org/project/scikit-learn/ 
https://scikit-learn.org/ 
scipy: 
SciPy: Scientiﬁc Library for Python 
https://pypi.org/project/scipy/ 
https://www.scipy.org/ 
seaborn: 
seaborn: statistical data visualization 
https://pypi.org/project/seaborn/ 
https://seaborn.pydata.org/ 
statsmodels: 
Statistical computations and models for Python 
https://pypi.org/project/statsmodels/ 
https://www.statsmodels.org/ 
theano-pymc: 
Optimizing compiler for evaluating mathematical expressions on CPUs and 
GPUs 
https://pypi.org/project/Theano-PyMC/ http://deeplearning.net/software/theano/

Appendix C 
Code Repository and Solution Manual 
The source code for code examples and all ﬁgures in this book both are avail-
able from the GitHub repository https://github.com/gedeck/mistat-code-solutions or 
from https://gedeck.github.io/mistat-code-solutions/. 
The repository also contains solutions for the exercises. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3
451

Bibliography 
Abadi M, Barham P, Chen J, Chen Z, Davis  A,  Dean  J,  Devin M, Ghemawat S, Irving G, Isard  
M, Kudlur M, Levenberg J, Monga R, Moore S, Murray DG, Steiner B, Tucker P, Vasudevan 
V, Warden P, Wicke M, Yu Y, Zheng X (2016) TensorFlow: a system for large-scale machine 
learning. In: 12th USENIX symposium on operating systems design and implementation (OSDI 
16), pp 265–283 
Abadie A, Diamond A, Hainmueller J (2015) Comparative politics and the synthetic control 
method. Am J Polit Sci 59(2):495–510. https://doi.org/10.1111/ajps.12116 
ANSI (2010) ANSI/ISA-95.00.01 enterprise-control system integration – part 1: models and 
terminology. Technical report. American National Standards Institute, Washington 
Aoki M (1989) Optimization of stochastic systems, 2nd edn. Topics in Discrete-time dynamics. 
Academic Press, Boston 
Aroian LA (1976) Applications of the direct method in sequential analysis. Technometrics 
18(3):301–306. https://doi.org/10.2307/1268739 
Aroian LA, Robison DE (1969) Direct methods for exact truncated sequential tests of the mean of 
a normal distribution. Technometrics 11(4):661–675. https://doi.org/10.1080/00401706.1969. 
10490729 
Babu SS, Goodridge R (2015) Additive manufacturing. Mat Sci Technol 31(8):881–883. https:// 
doi.org/10.1179/0267083615Z.000000000929 
Bai X, Kenett RS (2009) Risk-based adaptive group testing of semantic web services. In: 2009 33rd 
annual IEEE international computer software and applications conference, vol 2, pp 485–490. 
https://doi.org/10.1109/COMPSAC.2009.180 
Bal M, Hashemipour M (2009) Virtual factory approach for implementation of holonic control 
in industrial applications: a case study in die-casting industry. Robot Comput Integr Manuf 
25(3):570–581. https://doi.org/10.1016/j.rcim.2008.03.020 
Bär K, Herbert-Hansen ZNL, Khalid W (2018) Considering industry 4.0 aspects in the supply 
chain for an SME. Prod Eng 12(6):747–758. https://doi.org/10.1007/s11740-018-0851-y 
Barnard GA (1959) Control charts and stochastic processes. J R Stat Soc Ser B 21(2):239–271 
Basu AP (1971) On a sequential rule for estimating the location parameter of an exponential 
distribution. Nav Res Logist Q 18(3):329–337. https://doi.org/10.1002/nav.3800180305 
Basu AP (1991) Sequential methods. In: Ghosh BK, Sen PK (eds) Handbook of sequential analysis. 
CRC Press, Boca Raton, pp 581–592 
Bates RA, Kenett RS, Steinberg DM, Wynn HP (2006) Achieving robust design from computer 
simulations. Qual Technol Quantit Manag 3(2):161–177. https://doi.org/10.1080/16843703. 
2006.11673107 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3
453

454
Bibliography
Ben-Gal I, Dana A, Shkolnik N, Singer G (2014) Efﬁcient construction of decision trees by the 
dual information distance method. Qual Technol Quantit Manag 11(1):133–147. https://doi. 
org/10.1080/16843703.2014.11673330 
Ben-Michael E, Feller A, Rothstein J (2021a) The augmented synthetic control method. J Am Stat 
Assoc 116(536):1789–1803. https://doi.org/10.1080/01621459.2021.1929245 
Ben-Michael E, Feller A, Rothstein J (2021b) Synthetic controls with staggered adoption. J R Stat 
Soc Ser B (Stat Methodol). https://doi.org/10.1111/rssb.12448 
Berry DA, Fristedt B (1985) Bandit problems: sequential allocation of experiments. Monographs 
on statistics and applied probability. Springer Netherlands, Berlin. https://doi.org/10.1007/978-
94-015-3711-7 
Bevilacqua M, Bottani E, Ciarapica FE, Costantino F, Di Donato L, Ferraro A, Mazzuto G, 
Monteriù A, Nardini G, Ortenzi M, Paroncini M, Pirozzi M, Prist M, Quatrini E, Tronci 
M, Vignali G (2020) Digital twin reference model development to prevent operators’ risk in 
process plants. Sustainability 12(3):1088. https://doi.org/10.3390/su12031088 
Birol G, Ündey C, Çinar A (2002) A modular simulation package for fed-batch fermentation: 
penicillin production. Comput Chem Eng 26(11):1553–1565. https://doi.org/10.1016/S0098-
1354(02)00127-8 
Boland P, Singh H (2003) A birth-process approach to Moranda’s geometric software-reliability 
model. IEEE Trans Reliab 52(2):168–174. https://doi.org/10.1109/TR.2003.813166 
Bortolini M, Ferrari E, Gamberi M, Pilati F, Faccio M (2017) Assembly system design in the 
Industry 4.0 era: a general framework. IFAC-PapersOnLine 50(1):5700–5705. https://doi.org/ 
10.1016/j.ifacol.2017.08.1121 
Botvinik-Nezer R, Holzmeister F, Camerer CF, Dreber A, Huber J, Johannesson M, Kirchler M, 
Iwanir R, Mumford JA, Adcock RA, Avesani P, Baczkowski BM, Bajracharya A, Bakst L, Ball 
S, Barilari M, Bault N, Beaton D, Beitner J, Benoit RG, Berkers RMWJ, Bhanji JP, Biswal BB, 
Bobadilla-Suarez S, Bortolini T, Bottenhorn KL, Bowring A, Braem S, Brooks HR, Brudner 
EG, Calderon CB, Camilleri JA, Castrellon JJ, Cecchetti L, Cieslik EC, Cole ZJ, Collignon O, 
Cox RW, Cunningham WA, Czoschke S, Dadi K, Davis CP, Luca AD, Delgado MR, Demetriou 
L, Dennison JB, Di X, Dickie EW, Dobryakova E, Donnat CL, Dukart J, Duncan NW, Durnez 
J, Eed A, Eickhoff SB, Erhart A, Fontanesi L, Fricke GM, Fu S, Galván A, Gau R, Genon S, 
Glatard T, Glerean E, Goeman JJ, Golowin SAE, González-García C, Gorgolewski KJ, Grady 
CL, Green MA, Guassi Moreira JF, Guest O, Hakimi S, Hamilton JP, Hancock R, Handjaras G, 
Harry BB, Hawco C, Herholz P, Herman G, Heunis S, Hoffstaedter F, Hogeveen J, Holmes 
S, Hu CP, Huettel SA, Hughes ME, Iacovella V, Iordan AD, Isager PM, Isik AI, Jahn A, 
Johnson MR, Johnstone T, Joseph MJE, Juliano AC, Kable JW, Kassinopoulos M, Koba C, 
Kong XZ, Koscik TR, Kucukboyaci NE, Kuhl BA, Kupek S, Laird AR, Lamm C, Langner 
R, Lauharatanahirun N, Lee H, Lee S, Leemans A, Leo A, Lesage E, Li F, Li MYC, Lim 
PC, Lintz EN, Liphardt SW, Losecaat Vermeer AB, Love BC, Mack ML, Malpica N, Marins 
T, Maumet C, McDonald K, McGuire JT, Melero H, Méndez Leal AS, Meyer B, Meyer KN, 
Mihai G, Mitsis GD, Moll J, Nielson DM, Nilsonne G, Notter MP, Olivetti E, Onicas AI, Papale 
P, Patil KR, Peelle JE, Pérez A, Pischedda D, Poline JB, Prystauka Y, Ray S, Reuter-Lorenz PA, 
Reynolds RC, Ricciardi E, Rieck JR, Rodriguez-Thompson AM, Romyn A, Salo T, Samanez-
Larkin GR, Sanz-Morales E, Schlichting ML, Schultz DH, Shen Q, Sheridan MA, Silvers JA, 
Skagerlund K, Smith A, Smith DV, Sokol-Hessner P, Steinkamp SR, Tashjian SM, Thirion B, 
Thorp JN, Tinghög G, Tisdall L, Tompson SH, Toro-Serey C, Torre Tresols JJ, Tozzi L, Truong 
V, Turella L, van ’t Veer AE, Verguts T, Vettel JM, Vijayarajah S, Vo K, Wall MB, Weeda WD, 
Weis S, White DJ, Wisniewski D, Xifra-Porxas A, Yearling EA, Yoon S, Yuan R, Yuen KSL, 
Zhang L, Zhang X, Zosky JE, Nichols TE, Poldrack RA, Schonberg T (2020) Variability in the 
analysis of a single neuroimaging dataset by many teams. Nature 582(7810):84–88. https://doi. 
org/10.1038/s41586-020-2314-9 
Box G, Kramer T (1992) Statistical process monitoring and feedback adjustment: a discussion. 
Technometrics 34(3):251–267. https://doi.org/10.2307/1270028 
Box GEP, Tiao GC (1992) Bayesian inference in statistical analysis, 1st edn. Wiley-Interscience, 
New York

Bibliography
455
Box G, Bisgaard S, Fung C (1988) An explanation and critique of Taguchi’s contributions to quality 
engineering. Qual Reliab Eng Int 4(2):123–131. https://doi.org/10.1002/qre.4680040207 
Box GEP, Hunter JS, Hunter WG (2005) Statistics for experimenters: design, innovation, and 
discovery, 2nd edn. Wiley, Hoboken 
Box GEP, Jenkins GM, Reinsel GC, Ljung GM (2015) Time series analysis: forecasting and 
control, 5th edn. Wiley, New York 
Broy M, Cengarle MV, Geisberger E (2012) Cyber-physical systems: imminent challenges. In: 
Proceedings of the 17th monterey conference on large-scale complex IT systems: development, 
operation and management. Springer, Berlin, pp 1–28. https://doi.org/10.1007/978-3-642-
34059-8_1 
Bryant CM, Schmee J (1979) Conﬁdence limits on MTBF for sequential test plans of MIL-STD 
781. Technometrics 21(1):33–42. https://doi.org/10.1080/00401706.1979.10489720 
Castelijns LA, Maas Y, Vanschoren J (2020) The ABC of data: a classifying framework for 
data readiness. In: Cellier P, Driessens K (eds) Machine learning and knowledge discovery 
in databases. Communications in computer and information science. Springer International 
Publishing, Cham, pp 3–16. https://doi.org/10.1007/978-3-030-43823-4_1 
Chandler AD Jr (1993) The visible hand: the managerial revolution in American Business, 
unknown edn. Belknap Press: An Imprint of Harvard University Press, Cambridge 
Chandrasegaran SK, Ramani K, Sriram RD, Horváth I, Bernard A, Harik RF, Gao W (2013) 
The evolution, challenges, and future of knowledge representation in product design systems. 
Comput Aided Des 45(2):204–228. https://doi.org/10.1016/j.cad.2012.08.006 
Chen H (1994) A multivariate process capability index over a rectangular solid tolerance zone. Stat 
Sin 4(2):749–758 
Chen X, Jin R (2018) Data fusion pipelines for autonomous smart manufacturing. In: 2018 IEEE 
14th international conference on automation science and engineering (CASE), pp 1203–1208. 
https://doi.org/10.1109/COASE.2018.8560567 
Chen X, Jin R (2021) AdaPipe: a recommender system for adaptive computation pipelines in cyber-
manufacturing computation services. IEEE Trans Industr Inform 17(9):6221–6229. https://doi. 
org/10.1109/TII.2020.3035524 
Chinesta F (2019) Hybrid twins: the new data-driven and physics-based alliance. In: Kongoli 
F, Aifantis E, Chan A, Gawin D, Khalil N, Laloui L, Pastor M, Pesavento F, Sanavia 
L (eds) 2019 – sustainable industrial processing summit SIPS2019 volume 7: Schreﬂer 
Intl. Symp./geomechanics and applications for sustainable development, vol 7. Flogen Star 
Outreach, Montreal, pp 185–186 
Choi S, Kim BH, Do Noh S (2015) A diagnosis and evaluation method for strategic planning and 
systematic design of a virtual factory in smart manufacturing systems. Int J Precis Eng Manuf 
16(6):1107–1115. https://doi.org/10.1007/s12541-015-0143-9 
Cisco (2019) Leading tools manufacturer transforms operations with IoT. Technical report. Cisco 
Dalla Valle L, Kenett RS (2018) Social media big data integration: a new approach based on 
calibration. Expert Syst Appl 111:76–90. https://doi.org/10.1016/j.eswa.2017.12.044 
Dattner I (2021) Differential equations in data analysis. WIREs Comput Stat 13(6):e1534. https:// 
doi.org/10.1002/wics.1534 
Davidyan G, Bortman J, Kenett RS (2021) Towards the development of an operational digital twin 
of a railway system. In: 9th PHM conference, Tel Aviv 
Davis S (1997) Future perfect: tenth anniversary edition, updated edn. Basic Books, Reading 
Davis J, Edgar T, Porter J, Bernaden J, Sarli M (2012) Smart manufacturing, manufacturing 
intelligence and demand-dynamic performance. Comput Chem Eng 47:145–156. https://doi. 
org/10.1016/j.compchemeng.2012.06.037 
Dbouk T (2017) A review about the engineering design of optimal heat transfer sys-
tems using topology optimization. Appl Therm Eng 112:841–854. https://doi.org/10.1016/j. 
applthermaleng.2016.10.134 
Debevec M, Simic M, Herakovic N (2014) Virtual factory as an advanced approach for production 
process optimization. Int J Simul Model 13:66–78. https://doi.org/10.2507/IJSIMM13(1)6.260

456
Bibliography
Dehnad K (ed) (1989) Quality control, robust design, and the Taguchi Method. Springer US, 
Boston. https://doi.org/10.1007/978-1-4684-1472-1_1 
de Man JC, Strandhagen JO (2017) An industry 4.0 research agenda for sustainable business 
models. Procedia CIRP 63:721–726. https://doi.org/10.1016/j.procir.2017.03.315 
Deming WE (1967) In memoriam: Walter A. Shewhart, 1891–1967. Am Stat 21(2):39–40. https:// 
doi.org/10.1080/00031305.1967.10481808 
Deming WE (1982) Quality productivity and competitive position, 1st edn. Massachusetts Inst 
Technology, Cambridge 
Deming WE (1991) Out of the crisis. The MIT Press, Boston 
Derringer G, Suich R (1980) Simultaneous optimization of several response variables. J Qual 
Technol 12(4):214–219. https://doi.org/10.1080/00224065.1980.11980968 
Dharmesti MDD, Nugroho SS (2013) The Antecedents of online customer satisfaction and 
customer loyalty. J Bus Retail Manag Res 7(2) 
Dilberoglu UM, Gharehpapagh B, Yaman U, Dolen M (2017) The role of additive manufacturing 
in the era of industry 4.0. Procedia Manuf 11:545–554. https://doi.org/10.1016/j.promfg.2017. 
07.148 
Dodge HF, Romig HG (1998) Sampling inspection tables: single and double sampling, 2nd edn. 
Wiley, Hoboken 
Draper NR, Smith H (1998) Applied regression analysis, 3rd edn. Wiley, New York 
Duncan AJ (1956) The economic design of X charts used to maintain current control of a process. 
J Am Stat Assoc 51(274):228–242. https://doi.org/10.1080/01621459.1956.10501322 
Duncan AJ (1971) The economic design of upper X overbar ¯X-charts when there is a multiplicity of assignable 
causes. J Am Stat Assoc 66(333):107–121. https://doi.org/10.1080/01621459.1971.10482230 
Duncan AJ (1978) The economic design of p-charts to maintain current control of o process: 
some numerical results. Technometrics 20(3):235–243. https://doi.org/10.1080/00401706. 
1978.10489667 
Duncan AJ (1986) Quality control and industrial statistics. Irwin, Homewood 
Dvoretzky A, Kiefer J, Wolfowitz J (1953) Sequential decision problems for processes with 
continuous time parameter. Testing hypotheses. Ann Math Stat 24(2):254–264. https://doi.org/ 
10.1214/aoms/1177729031 
Edgar TF, Pistikopoulos EN (2018) Smart manufacturing and energy systems. Comput Chem Eng 
114:130–144. https://doi.org/10.1016/j.compchemeng.2017.10.027 
Epstein B (1960) Statistical life test acceptance procedures. Technometrics 2(4):435–446. https:// 
doi.org/10.1080/00401706.1960.10489910 
Epstein B, Sobel M (1955) Sequential life tests in the exponential case. Ann Math Stat 26(1):82– 
93. https://doi.org/10.1214/aoms/1177728595 
Faltin FW, Kenett RS, Ruggeri F (eds) (2012) Statistical methods in healthcare. Wiley, New York 
Feng W, Wang C, Shen ZJM (2017) Process ﬂexibility design in heterogeneous and unbalanced 
networks: a stochastic programming approach. IISE Trans 49(8):781–799. https://doi.org/10. 
1080/24725854.2017.1299953 
Figini S, Kenett RS, Salini S (2010) Optimal scaling for risk assessment: merging of operational 
and ﬁnancial data. Qual Reliab Eng Int 26(8):887–897. https://doi.org/10.1002/qre.1158 
Fisher RA (1919) XV.—The correlation between relatives on the supposition of Mendelian 
inheritance. Earth Environ Sci Trans R Soc Edinb 52(2):399–433 
Fisher RA (1935) The design of experiments. Oliver and Boyd, Ltd., Edinburgh 
Fox B (1988) Duracell loses prestige defence contract. New Scientist 119:39 
Fuchs C, Kenett RS (1987) Multivariate tolerance regions and F-tests. J Qual Technol 19(3):122– 
131. https://doi.org/10.1080/00224065.1987.11979053 
Gandin L (1963) Objective analysis of meteorological ﬁelds: GIMIZ, Gidrometeorologicheskoe 
Izdatelstvo, Leningrad 1963: Transl. from the Russian. Israel program for scientiﬁc translations, 
GIMIZ, Gidrometeorologicheskoe Izdatelstvo, Leningrad 
Gertsbakh IB (1989) Statistical reliability theory. Marcel Dekker, New York 
Ghosh S (1990) Statistical design and analysis of industrial experiments. Marcel Dekker, New 
York

Bibliography
457
Gibra IN (1971) Economically optimal determination of the parameters of ¯X-control chart. Manag 
Sci 17(9):635–646. https://doi.org/10.1287/mnsc.17.9.635 
Gittins JC, Jones DM (1974) A dynamic allocation index for the sequential design of experiments. 
In: Gani JM, Sarkadi K, Vincze I (eds) Progress in statistics, vol 1. North-Holland Pub. Co., 
Amsterdam, pp 241–266 
Gittins J, Glazebrook K, Weber R (2011) Multi-armed bandit allocation indices, 2nd edn. Wiley, 
New York 
Goba FA (1969) Bibliography on thermal aging of electrical insulation. IEEE Trans Electr Insul 
EI-4(2):31–58. https://doi.org/10.1109/TEI.1969.299070 
Godfrey AB (1986) Report: the history and evolution of quality in AT&T. AT&T Tech J 65(2):9– 
20. https://doi.org/10.1002/j.1538-7305.1986.tb00289.x 
Godfrey AB, Kenett RS (2007) Joseph M. Juran, a perspective on past contributions and future 
impact. Qual Reliab Eng Int 23(6):653–663. https://doi.org/10.1002/qre.861 
Goel AL, Okumoto K (1979) Time-dependent error-detection rate model for software reliability 
and other performance measures. IEEE Trans Reliab R-28(3):206–211. https://doi.org/10.1109/ 
TR.1979.5220566 
Good IJ (2003) The estimation of probabilities: an essay on modern Bayesian methods, 1st edn. 
The MIT Press, Cambridge 
Goos (2011) Optimal design of experiments: a case study approach, 1st edn. Wiley, Hoboken 
Grieves M (2022) Intelligent digital twins and the development and management of complex 
systems. https://digitaltwin1.org/articles/2-8/v1 
Grieves M, Vickers J (2017) Digital twin: mitigating unpredictable, undesirable emergent behavior 
in complex systems. In: Kahlen FJ, Flumerfelt S, Alves A (eds) Transdisciplinary perspectives 
on complex systems: new ﬁndings and approaches. Springer International Publishing, Cham, 
pp 85–113. https://doi.org/10.1007/978-3-319-38756-7_4 
Gruber A, Yanovski S, Ben Gal I (2021) Condition-based maintenance via a targeted Bayesian Net-
work Meta-Model. In: Kenett RS, Swarz RS, Zonnenshain A (eds) Systems engineering in the 
fourth industrial revolution big data. Novel Technologies, and Modern Systems Engineering, 
Wiley, Hoboken 
Haridy S, Wu Z, Castagliola P (2011) Univariate and multivariate approaches for evaluating the 
capability of dynamic-behavior processes (case study). Stat Methodol 8(2):185–203. https:// 
doi.org/10.1016/j.stamet.2010.09.003 
Hermann M, Pentek T, Otto B (2015) Design principles for industrie 4.0 scenarios: a literature 
review. Working Paper No. 01/2015. Technische Universität Dortmund, Dortmund. https://doi. 
org/10.13140/RG.2.2.29269.22248 
Higdon D, Gattiker J, Williams B, Rightley M (2008) Computer model calibration using high-
dimensional output. J Am Stat Assoc 103(482):570–583 
Higdon D, Gattiker J, Lawrence E, Pratola M, Jackson C, Tobis M, Habib S, Heitmann K, 
Price S (2013) Computer model calibration using the ensemble Kalman ﬁlter. Technometrics 
55(4):488–500 
Hints R, Vanca M, Terkaj W, Marra ED (2011) A virtual factory tool to enhance the integrated 
design of production systems. In: Proceedings of the DET2011 7th international conference on 
digital enterprise technology, Athens, pp 28–30 
Hoadley B (1981) The quality measurement plan (QMP). Bell Syst Tech J 60(2):215–273. https:// 
doi.org/10.1002/j.1538-7305.1981.tb00239.x 
Huang D, Allen TT (2005) Design and analysis of variable ﬁdelity experimentation applied to 
engine valve heat treatment process design. J R Stat Soc Ser C Appl Stat 54(2):443–463. https:// 
doi.org/10.1111/j.1467-9876.2005.00493.x 
Iannario M, Piccolo D (2011) CUB models: statistical methods and empirical evidence. In: Modern 
analysis of customer surveys. John Wiley & Sons, Ltd., Hoboken, chap 13, pp 231–258. https:// 
doi.org/10.1002/9781119961154.ch13 
IMT S (2013) Are virtual factories the future of manufacturing? Technical report. 
Ishikawa K (1986) Guide to quality control, revised, subsequent edn. Asian Productivity Organi-
zation, White Plains

458
Bibliography
Jain S, Shao G (2014) Virtual factory revisited for manufacturing data analytics. In: Proceedings 
of the 2014 winter simulation conference, WSC ’14. IEEE Press, Savannah, pp 887–898 
Jalili M, Bashiri M, Amiri A (2012) A new multivariate process capability index under both 
unilateral and bilateral quality characteristics. Qual Reliab Eng Int 28(8):925–941. https://doi. 
org/10.1002/qre.1284 
Jared BH, Aguilo MA, Beghini LL, Boyce BL, Clark BW, Cook A, Kaehr BJ, Robbins J (2017) 
Additive manufacturing: toward holistic design. Scr Mater 135:141–147. https://doi.org/10. 
1016/j.scriptamat.2017.02.029 
Jelinski Z, Moranda P (1972) Software reliability research. In: Freiberger W (ed) Statistical 
computer performance evaluation. Academic Press, Cambridge, pp 465–484. https://doi.org/ 
10.1016/B978-0-12-266950-7.50028-1 
Jensen F, Petersen NE (1991) Burn-in: an engineering approach to the design and analysis of burn-
in procedures, 1st edn. Wiley, Chichester 
Jeschke S, Brecher C, Meisen T, Özdemir D, Eschert T (2017) Industrial internet of things and 
cyber manufacturing systems. In: Jeschke S, Brecher C, Song H, Rawat DB (eds) Indus-
trial internet of things: cybermanufacturing systems. Springer series in wireless technology. 
Springer International Publishing, Cham, pp 3–19. https://doi.org/10.1007/978-3-319-42559-
7_1 
Jin R, Deng X (2015) Ensemble modeling for data fusion in manufacturing process scale-up. IIE 
Trans 47(3):203–214. https://doi.org/10.1080/0740817X.2014.916580 
Jin R, Deng X, Chen X, Zhu L, Zhang J (2019) Dynamic quality-process model in consideration 
of equipment degradation. J Qual Technol 51(3):217–229. https://doi.org/10.1080/00224065. 
2018.1541379 
John S (1963) A tolerance region for multivariate normal distributions. Sankhya; Series A 25:363– 
368 
John PWM (1990) Statistical methods in engineering and quality assurance, 1st edn. Wiley-
Interscience, New York 
Juran JM (1979) Quality control handbook. Mcgraw-Hill, New York 
Juran JM (1986) The quality trilogy: a universal approach to managing for quality. In: 40th annual 
quality congress – American Society for quality control, ASQC transactions of: 19–21 May 
1986, Anaheim, California, 1st edn. American Society for Quality Control, Inc., Milwaukee 
Juran JM (ed) (1995) A history of managing for quality, 1st edn. Asq Pr, Milwaukee 
Kackar RN (1985) Off-line quality control, parameter design, and the Taguchi Method. J Qual 
Technol 17(4):176–188. https://doi.org/10.1080/00224065.1985.11978964 
Kang S, Deng X, Jin R (2021a) A cost-efﬁcient data-driven approach to design space exploration 
for personalized geometric design in additive manufacturing. J Comput Inf Sci Eng 21(6). 
https://doi.org/10.1115/1.4050984 
Kang S, Jin R, Deng X, Kenett RS (2021b) Challenges of modeling and analysis in cybermanufac-
turing: a review from a machine learning and computation perspective. J Intell Manuf https:// 
doi.org/10.1007/s10845-021-01817-9 
Kelly T, Kenett RS, Newton E, Roodman G, Wowk A (1991) Total quality management also applies 
to a school of management. In: Proceedings of the 9th IMPRO conference, Atlanta 
Kenett RS (1991) Two methods for comparing Pareto charts. J Qual Technol 23(1):27–31. https:// 
doi.org/10.1080/00224065.1991.11979280 
Kenett RS (2007) Software failure data analysis. In: Ruggeri F, Kenett RS, Faltin FW (eds) 
Encyclopedia of statistics in quality and reliability. Wiley, Hoboken 
Kenett RS (2008) From data to information to knowledge. Six Sigma Forum Magazine, pp 32–33 
Kenett RS (2019) Applications of Bayesian networks. Trans Mach Learn Data Mining 12(2):33–54 
Kenett RS (2020) Reviewing of applied research with an industry 4.0 perspective. SSRN scholarly 
paper ID 3591808. Social Science Research Network, Rochester. https://doi.org/10.2139/ssrn. 
3591808 
Kenett RS, Bortman J (2021) The digital twin in industry 4.0: a wide-angle perspective. Qual 
Reliab Eng Int 38(3):1357–1366. https://doi.org/10.1002/qre.2948

Bibliography
459
Kenett RS, Coleman S (2021) Data and the fourth industrial revolution. Signiﬁcance 18(3):8–9. 
https://doi.org/10.1111/1740-9713.01523 
Kenett RS, Kenett DA (2008) Quality by design applications in biosimilar pharmaceutical 
products. Accred Qual Assur 13(12):681–690. https://doi.org/10.1007/s00769-008-0459-6 
Kenett RS, Pollak M (1986) A semi-parametric approach to testing for reliability growth, with 
application to software systems. IEEE Trans Reliab 35(3):304–311. https://doi.org/10.1109/ 
TR.1986.4335439 
Kenett RS, Pollak M (1996) Data-analytic aspects of the Shiryayev-Roberts control chart: 
surveillance of a non-homogeneous Poisson process. J Appl Stat 23(1):125–138. https://doi. 
org/10.1080/02664769624413 
Kenett RS, Raanan Y (eds) (2010) Operational risk management: a practical approach to intelligent 
data analysis, 1st edn. Wiley, Chichester 
Kenett RS, Redman TC (2019) The real work of data science: turning data into information, better 
decisions, and stronger organizations, 1st edn. Wiley, Hoboken 
Kenett RS, Rubinstein A (2021) Generalizing research ﬁndings for enhanced reproducibility: 
an approach based on verbal alternative representations. Scientometrics 126(5):4137–4151. 
https://doi.org/10.1007/s11192-021-03914-1 
Kenett RS, Salini S (2009) New frontiers: Bayesian networks give insight into survey-data analysis. 
Qual Prog 42:30–36 
Kenett RS, Salini S (2011) Modern analysis of customer satisfaction surveys: comparison of 
models and integrated analysis. Appl Stoch Model Bus Ind 27(5):465–475. https://doi.org/10. 
1002/asmb.927 
Kenett RS, Shmueli G (2014) On information quality. J R Stat Soc A Stat Soc 177(1):3–38. https:// 
doi.org/10.1111/rssa.12007 
Kenett RS, Shmueli G (2016) Information quality: the potential of data and analytics to generate 
knowledge, 1st edn. Wiley, Chichester 
Kenett RS, Vicario G (2021) Challenges and opportunities in simulations and computer exper-
iments in industrial statistics: an industry 4.0 perspective. Adv Theory Simul 4(2):2000254. 
https://doi.org/10.1002/adts.202000254 
Kenett RS, Vogel B (1991) Going beyond main-effect plots. Qual Prog 24(2):71–73 
Kenett RS, Zacks S (2021) Modern industrial statistics: with applications in R, MINITAB, and 
JMP, 3rd edn. Wiley, Hoboken 
Kenett RS, Ruggeri F, Faltin FW (eds) (2018a) Analytic methods in systems and software testing. 
Wiley, Hoboken 
Kenett RS, Zonnenshain A, Fortuna G (2018b) A road map for applied data sciences supporting 
sustainability in advanced manufacturing: the information quality dimensions. Procedia Manuf 
21:141–148. https://doi.org/10.1016/j.promfg.2018.02.104 
Kenett RS, Swarz RS, Zonnenshain A (eds) (2021a) Systems engineering in the fourth industrial 
revolution: big data, Novel Technologies, and Modern Systems Engineering, 1st edn. Wiley, 
Hoboken 
Kenett RS, Yahav I, Zonnenshain A (2021b) Analytics as an enabler of advanced manufacturing. 
In: Kenett RS, Swarz RS, Zonnenshain A (eds) Systems engineering in the fourth industrial 
revolution big data, Novel Technologies, and Modern Systems Engineering. Wiley, Hoboken 
Kenett RS, Gotwalt C, Freeman L, Deng X (2022a) Self-supervised cross validation using data 
generation structure. Appl Stoch Model Bus Ind. https://doi.org/10.1002/asmb.2701 
Kenett RS, Zacks S, Gedeck P (2022b) Modern statistics: a computer-based approach with python, 
1st edn. Springer, Birkhäuser 
Kennedy MC, O’Hagan A (2001) Bayesian calibration of computer models. J R Stat Soc Ser B 
(Stat Methodol) 63(3):425–464. https://doi.org/10.1111/1467-9868.00294 
Kiefer J (1959) Optimum experimental designs. J R Stat Soc Ser B (Methodol) 21(2):272–319 
Kiefer J, Wolfowitz J (1956) Sequential tests of hypotheses about the mean occurrence time of a 
continuous parameter Poisson process. Naval Res Logist Q 3(3):205–219. https://doi.org/10. 
1002/nav.3800030308 
Kotz S, Johnson NL (1993) Process capability indices, 1st edn. Chapman and Hall/CRC, London

460
Bibliography
Kozjek D, Vrabiˇc R, Kralj D, Butala P (2017) A data-driven holistic approach to fault prognostics 
in a cyclic manufacturing process. Procedia CIRP 63:664–669. https://doi.org/10.1016/j.procir. 
2017.03.109 
Kuo CJ, Ting KC, Chen YC, Yang DL, Chen HM (2017) Automatic machine status prediction in 
the era of industry 4.0. J Syst Archit EUROMICRO J 81(C):44–53. https://doi.org/10.1016/j. 
sysarc.2017.10.007 
Lawrence ND (2017) Data readiness levels. arXiv:170502245 [cs] 1705.02245 
Lee J, Bagheri B, Kao HA (2015) A cyber-physical systems architecture for industry 4.0-based 
manufacturing systems. Manuf Lett 3:18–23. https://doi.org/10.1016/j.mfglet.2014.12.001 
Li J, Jin R, Yu HZ (2018) Integration of physically-based and data-driven approaches for thermal 
ﬁeld prediction in additive manufacturing. Mat Des 139:473–485. https://doi.org/10.1016/j. 
matdes.2017.11.028 
Liebesman BS, Saperstein B (1983) A proposed attribute skip-lot sampling program. J Qual 
Technol 15(3):130–140. https://doi.org/10.1080/00224065.1983.11978860 
Lin KM, Kacker RN (2012) Optimizing the wave soldering process. In: Dehnad K (ed) Quality 
control, robust design, and the Taguchi Method, 1989th edn., Wadsworth BrooksCole, Paciﬁc 
Grove, pp 143–157 
Lipow M (1978) Models for software reliability. In: Proceedings of the winter meetings of the 
aerospace division of the American Society for Mechanical Engineers, vol 78-WA/Aero-18, 
pp 1–11 
Littlewood B, Verrall JL (1973) A Bayesian reliability growth model for computer software. J R 
Stat Soc Ser C Appl Stat 22(3):332–346. https://doi.org/10.2307/2346781 
Lucas JM (1982) Combined Shewhart-CUSUM quality control schemes. J Qual Technol 14(2):51– 
59. https://doi.org/10.1080/00224065.1982.11978790 
Lucas JM, Crosier RB (1982) Fast initial response for CUSUM quality-control schemes: give your 
CUSUM a head start. Technometrics 24(3):199–205. https://doi.org/10.1080/00401706.1982. 
10487759 
Luo L, Kannan PK, Besharati B, Azarm S (2005) Design of robust new products under variability: 
marketing meets design*. J Prod Innov Manag 22(2):177–192. https://doi.org/10.1111/j.0737-
6782.2005.00113.x 
Mahmoudi M, Tapia G, Karayagiz K, Franco B, Ma J, Arroyave R, Karaman I, Elwany A (2018) 
Multivariate calibration and experimental validation of a 3D ﬁnite element thermal model for 
laser powder bed fusion metal additive manufacturing. Integ Mat Manuf Innov 7(3):116–135. 
https://doi.org/10.1007/s40192-018-0113-z 
Mann NR, Schafer RE, Singpurwalla ND (1974) Methods for statistical analysis of reliability and 
life data, 1st edn. Wiley, New York 
Martz HF, Waller RA (1982) Bayesian reliability analysis, 1st edn. Wiley, New York 
Matheron G (1963) Principles of geostatistics. Econ Geol 58(8):1246–1266. https://doi.org/10. 
2113/gsecongeo.58.8.1246 
Meeker WQ, Escobar LA, Pascual FG (2021) Statistical methods for reliability data, 2nd edn. 
Wiley, Hoboken 
Modoni GE, Caldarola EG, Sacco M, Terkaj W (2019) Synchronizing physical and digital factory: 
beneﬁts and technical challenges. Procedia CIRP 79:472–477. https://doi.org/10.1016/j.procir. 
2019.02.125 
Mukhopadhyay N (1974) Sequential estimation of location parameter in exponential distributions. 
Calcutta Statist Assoc Bull 23(1-4):85–96. https://doi.org/10.1177/0008068319740105 
Musa JD, Okumoto K (1984) A logarithmic Poisson execution time model for software reliability 
measurement. In: Proceedings of the 7th international conference on software engineering, 
ICSE ’84. IEEE Press, Orlando, pp 230–238 
Nasr M (2007) Quality by design (QbD) – a modern system approach to pharmaceutical 
development and manufacturing – FDA perspective. In: FDA quality initiatives workshop, 
Maryland 
Nelson WB (2004) Accelerated testing: statistical models, test plans, and data analyses, paperback 
edn. Wiley-Interscience, Hoboken

Bibliography
461
Nguyen NK, Pham TD (2016) Small mixed-level screening designs with orthogonal quadratic 
effects. J Qual Technol 48(4):405–414. https://doi.org/10.1080/00224065.2016.11918176 
O’Donovan P, Leahy K, Bruton K, O’Sullivan DTJ (2015) An industrial big data pipeline for data-
driven analytics maintenance applications in large-scale smart manufacturing facilities. J Big 
Data 2(1):25. https://doi.org/10.1186/s40537-015-0034-z 
Ohba M (1984) Software reliability analysis models. IBM J Res Dev 28(4):428–443. https://doi. 
org/10.1147/rd.284.0428 
Oikawa T, Oka T (1987) New techniques for approximating the stress in pad-type nozzles attached 
to a spherical shell. Trans Am Soc Mechan Eng 109:188–192 
Olavsrud T (2017) 15 data and analytics trends that will dominate 2017. https://www.cio.com/ 
article/3166060/15-data-and-analytics-trends-that-will-dominate-2017.html 
Page ES (1954) Continuous inspection schemes. Biometrika 41(1–2):100–115. https://doi.org/10. 
1093/biomet/41.1-2.100 
Page ES (1962) A modiﬁed control chart with warning lines. Biometrika 49(1–2):171–176. https:// 
doi.org/10.1093/biomet/49.1-2.171 
Pao TW, Phadke MS, Sherrerd CS (1985) Computer response time optimization using orthogonal 
array experiments. In: IEEE international communication conference, Chicago, pp 890–895 
Paszke A, Gross S, Chintala S, Chanan G (2022) PyTorch: tensors and dynamic neural networks 
in Python with strong GPU acceleration. PyTorch 
Peck DS, Trapp OD, Components Technology Institute (1994) Accelerated testing handbook, 5th 
edn. Components Technology Institute, Huntsville 
Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, 
Weiss R, Dubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay 
E (2011) Scikit-learn: machine learning in Python. J Mach Learn Res 12:2825–2830 
Phadke MS (1989) Quality engineering using robust design, illustrated edn. Prentice Hall, 
Englewood Cliffs 
Phadke MS, Kackar RN, Speeney DV, Grieco MJ (1983) Off-line quality control in integrated 
circuit fabrication using experimental design. Bell Syst Tech J 62(5):1273–1309. https://doi. 
org/10.1002/j.1538-7305.1983.tb02298.x 
Piccolo D (2003) On the moments of a mixture of uniform and shifted binomial random variables. 
Quaderni di Statistica 5:85–104 
Press SJ (1989) Bayesian statistics: principles, models, and applications, 1st edn. Wiley, New York 
Qi Q, Tao F (2018) Digital twin and big data towards smart manufacturing and industry 4.0: 
360 degree comparison. IEEE Access 6:3585–3593. https://doi.org/10.1109/ACCESS.2018. 
2793265 
Quinlan J (1985) Product improvement by application of Taguchi Methods. In: Third supplier 
symposium on Taguchi Methods. American Supplier Institute, Inc, Dearborn 
Rasheed A, San O, Kvamsdal T (2020) Digital twin: values, challenges and enablers from a 
modeling perspective. IEEE Access 8:21980–22012. https://doi.org/10.1109/ACCESS.2020. 
2970143 
Rathore AS, Mhatre R (eds) (2009) Quality by design for biopharmaceuticals: principles and case 
studies, 1st edn. Wiley-Interscience, Hoboken 
Reinman G, Ayer T, Davan T, Devore M, Finley S, Glanovsky J, Gray L, Hall B, Jones CC, Learned 
A, Mesaros E, Morris R, Pinero S, Russo R, Stearns E, Teicholz M, Teslik-Welz W, Yudichak D 
(2012) Design for variation. Qual Eng 24(2):317–345. https://doi.org/10.1080/08982112.2012. 
651973 
Reis MS, Kenett RS (2018) Assessing the value of information of data-centric activities in the 
chemical processing industry 4.0. AIChE J 64(11):3868–3881. https://doi.org/10.1002/aic. 
16203 
Romano D, Vicario G (2002) Reliable estimation in computer experiments on ﬁnite-element codes. 
Qual Eng 14(2):195–204. https://doi.org/10.1081/QEN-100108676 
Sabbaghi A, Huang Q, Dasgupta T (2018) Bayesian model building from small samples of 
disparate data for capturing in-plane deviation in additive manufacturing. Technometrics 
60(4):532–544. https://doi.org/10.1080/00401706.2017.1391715

462
Bibliography
Sacks J, Welch WJ, Mitchell TJ, Wynn HP (1989) Design and analysis of computer experiments. 
Stat Sci 4(4):409–423. https://doi.org/10.1214/ss/1177012413 
Salvatier J, Wiecki TV, Fonnesbeck C (2016) Probabilistic programming in Python using PyMC3. 
PeerJ Comput Sci 2:e55. https://doi.org/10.7717/peerj-cs.55 
Santner TJ, Williams BJ, Notz WI (2003) The design and analysis of computer experiments. 
Springer series in statistics. Springer, New York. https://doi.org/10.1007/978-1-4757-3799-8 
Schick G, Wolverton R (1978) An analysis of competing software reliability models. IEEE Trans 
Softw Eng SE-4(2):104–120. https://doi.org/10.1109/TSE.1978.231481 
Schluse 
M, 
Priggemeyer 
M, 
Atorf 
L, 
Rossmann 
J 
(2018) 
Experimentable 
digital 
twins—streamlining simulation-based systems engineering for industry 4.0. IEEE Trans 
Ind Inf 14(4):1722–1731. https://doi.org/10.1109/TII.2018.2804917 
Schulze A, Störmer T (2012) Lean product development – enabling management factors for waste 
elimination. Int J Technol Manag 57(1/2/3):71–91. https://doi.org/10.1504/IJTM.2012.043952 
Shafto M, Conroy M, Doyle R, Glaessgen E, Kemp C, LaMoigne J, Wang L (2010) DRAFT 
modeling, simulation, information technology & processing roadmap – technology area 11. 
Technical report. National Aeronautics and Space Administration 
Shanthikumar JG (1981) A general software reliability model for performance prediction. Micro-
electron Reliab 21(5):671–682. https://doi.org/10.1016/0026-2714(81)90059-7 
Shewhart WA (1926) Quality control charts. Bell Syst Tech J 5(4):593–603. https://doi.org/10. 
1002/j.1538-7305.1926.tb00125.x 
Siemens (2022) Transforming manufacturing—the future happens now. https://new.siemens.com/ 
us/en/company/topic-areas/transforming-manufacturing.html 
Singh S, Shehab E, Higgins N, Fowler K, Tomiyama T, Fowler C (2018) Challenges of digital twin 
in high value manufacturing: SAE 2018 4th aerospace systems and technology conference, 
ASTC 2018. SAE technical papers 2018 November. https://doi.org/10.4271/2018-01-1928 
SUPAC (1997) SUPAC-SS: nonsterile semisolid dosage forms; Scale-up and post-approval 
changes: chemistry, manufacturing and controls; In vitro release testing and in vivo bioequiva-
lence documentation. Guidance for industry, Center for Drug Evaluation and Research (CDER) 
Taguchi G (1987) Systems of experimental design, vols 1–2. UNIPUB/Kraus International 
Publications, New York 
Taguchi G, Konishi S (1987) Taguchi methods orthogonal arrays and linear graphs: tools for quality 
engineering. Amer Supplier Inst, Dearborn 
Tang B, Deng LY (1999) Minimum G2-aberration for nonregular fractional factorial designs. Ann 
Stat 27(6):1914–1926 
Terkaj W, Tolio T, Urgo M (2015) A virtual factory approach for in situ simulation to support 
production and maintenance planning. CIRP Ann Manuf Technol 64:451–454. https://doi.org/ 
10.1016/j.cirp.2015.04.121 
Tham MT, Morris AJ, Montague GA (1989) Soft-sensing: a solution to the problem of measure-
ment delays. Chem Eng Res Des 67:547–554 
Thoben KD, Wiesner S, Wuest T (2017) “Industrie 4.0” and smart manufacturing – a review of 
research issues and application examples. Int J Automat Technol 11(1):4–16. https://doi.org/ 
10.20965/ijat.2017.p0004 
Tolio T, Sacco M, Terkaj W, Urgo M (2013) Virtual factory: an integrated framework for 
manufacturing systems design and analysis. Procedia CIRP 7:25–30. https://doi.org/10.1016/j. 
procir.2013.05.005 
Tsokos CP, Shimi IN (eds) (1977) The theory and applications of reliability with emphasis on 
Bayesian and nonparametric methods. Academic Press, New York 
Tsong Y, Hammerstrom T, Sathe P, Shah VP (1996) Statistical assessment of mean differences 
between two dissolution data sets. Drug Inform J 30(4):1105–1112. https://doi.org/10.1177/ 
009286159603000427 
Volvo Group Global (2017) Virtual twin plant shorten lead times. https://www.volvogroup.com/ 
en/news-and-media/news/2017/mar/virtual-twin-plant-shorten-lead-times.html

Bibliography
463
von Stosch M, Oliveira R, Peres J, Feyo de Azevedo S (2014) Hybrid semi-parametric modeling in 
process systems engineering: past, present and future. Comput Chem Eng 60:86–101. https:// 
doi.org/10.1016/j.compchemeng.2013.08.008 
Wang RY, Storey VC, Firth CP (1995) A framework for analysis of data quality research. IEEE 
Trans Knowl Data Eng 7(4):623–640. https://doi.org/10.1109/69.404034 
Wang J, Ma Y, Zhang L, Gao RX, Wu D (2018a) Deep learning for smart manufacturing: methods 
and applications. J Manuf Syst 48:144–156. https://doi.org/10.1016/j.jmsy.2018.01.003 
Wang J, Yang J, Zhang J, Wang X, Zhang WC (2018b) Big data driven cycle time parallel 
prediction for production planning in wafer manufacturing. Enterp Inf Syst 12(6):714–732. 
https://doi.org/10.1080/17517575.2018.1450998 
Wang J, Xu C, Zhang J, Bao J, Zhong R (2020) A collaborative architecture of the industrial 
internet platform for manufacturing systems. Robot Comput Integ Manuf 61:101854. https:// 
doi.org/10.1016/j.rcim.2019.101854 
Weindling JI (1967) Statistical properties of a general class of control charts treated as a Markov 
process. PhD thesis. Columbia University, New York 
Weindling JI, Littauer SB, Oliveira JTD (1970) Mean action time of the ¯X control chart with 
warning limits. J Qual Technol 2(2):79–85. https://doi.org/10.1080/00224065.1970.11980418 
Weiss BA, Vogl G, Helu M, Qiao G, Pellegrino J, Justiniano M, Raghunathan A (2015) 
Measurement science for prognostics and health management for smart manufacturing systems: 
key ﬁndings from a roadmapping workshop. In: Proceedings of the annual conference of 
the prognostics and health management society prognostics and health management society 
conference 6:046 
Whittaker JA, Rekab K, Thomason MG (2000) A Markov chain model for predicting the 
reliability of multi-build software. Inform Softw Technol 42(12):889–894. https://doi.org/10. 
1016/S0950-5849(00)00122-1 
Wong RKW, Storlie CB, Lee TCM (2017) A frequentist approach to computer model calibration. 
J R Stat Soc Ser B (Stat Methodol) 79(2):635–648 
Wu CFJ, Hamada MS (2011) Experiments: planning, analysis, and optimization. John Wiley & 
Sons, Hoboken 
Wynn HP (1972) Results in the theory and construction of D-optimum experimental designs. J R 
Stat Soc Ser B (Methodol) 34(2):133–147 
Xie M (1991) Software reliability modelling. World Scientiﬁc, Singapore 
Xie M, Zhao M (1993) On some reliability growth models with simple graphical interpretations. 
Microelectron Reliab 33(2):149–167. https://doi.org/10.1016/0026-2714(93)90477-G 
Yamada S, Ohba M, Osaki S (1984) S-shaped software reliability growth models and their 
applications. IEEE Trans Reliab R-33(4):289–292. https://doi.org/10.1109/TR.1984.5221826 
Yang H, Ni J (2005) Dynamic neural network modeling for nonlinear, nonstationary machine tool 
thermally induced error. Int J Mach Tools Manuf 45(4–5):455-465. https://doi.org/10.1016/j. 
ijmachtools.2004.09.004 
Yang Z, Eddy D, Krishnamurty S, Grosse I, Denno P, Lu Y, Witherell P (2017) Investigating grey-
box modeling for predictive analytics in smart manufacturing. In: ASME 2017 international 
design engineering technical conferences and computers and information in engineering 
conference. American Society of Mechanical Engineers Digital Collection. https://doi.org/10. 
1115/DETC2017-67794 
Yang H, Kumara S, Bukkapatnam ST, Tsung F (2019) The internet of things for smart manufactur-
ing: a review. IISE Trans 51(11):1190–1216. https://doi.org/10.1080/24725854.2018.1555383 
Yashchin E (1985) On a uniﬁed approach to the analysis of two-sided cumulative sum control 
schemes with headstarts. Adv Appl Probab 17(3):562–593. https://doi.org/10.2307/1427120 
Yashchin E (1991) Some aspects of the theory of statistical control schemes. IBM J Res Develop 
31:199–205. https://doi.org/10.1147/RD.312.0199 
Yi G, Herdsman C, Morris J (2019) A MATLAB toolbox for data pre-processing and multivariate 
statistical process control. Chemom Intell Lab Syst 194:103863. https://doi.org/10.1016/j. 
chemolab.2019.103863

464
Bibliography
Zacks S (1973) Sequential design for a ﬁxed width interval estimation of the common mean of two 
normal distributions. I. The case of one variance known. J Am Stat Assoc 68(342):422–427. 
https://doi.org/10.2307/2284090 
Zacks S (1980) Numerical determination of the distributions of stopping variables associated 
with sequential procedures for detecting epochs of shift in distributions of discrete ran-
dom variables numerical determination of the distributions of stopping variables associated 
with sequential procedures. Commun Stat Simul Comput 9(1):1–18. https://doi.org/10.1080/ 
03610918008812134 
Zacks S (1992) Introduction to reliability analysis: probability models and statistical methods. 
Springer texts in statistics. Springer, New York. https://doi.org/10.1007/978-1-4612-2854-7 
Zacks S (1997) Distributions of ﬁrst exit times for Poisson Processes with lower and upper linear 
boundaries. In: Johnson NL, Balakrishnan N (eds) Advances in the theory and practice of 
statistics: a volume in honor of Samuel Kotz, 1st edn. Wiley-Interscience, New York 
Zacks S (2009) Stage-wise adaptive designs, 1st edn. Wiley, Hoboken 
Zahran A, Anderson-Cook CM, Myers RH (2003) Fraction of design space to assess prediction 
capability of response surface designs. J Qual Technol 35(4):377–386 
Zhang X, Pham H (1998) A software cost model with error removal times and risk costs. Int J Syst 
Sci 29(4):435–442. https://doi.org/10.1080/00207729808929534

Index 
A 
A/B testing, 398, 410 
Accelerated life testing, 360, 362, 366 
Acceptable quality level (AQL), 399, 439 
Acceptance, 362, 398, 399, 401–403, 409, 416, 
418, 420, 422, 425, 427, 429–431, 439 
Acceptance number, 401, 403, 422, 425, 439 
Acceptance sampling, 399, 402, 403, 416, 418, 
427 
Accuracy, 227, 231, 266, 300, 302, 305, 306, 
355 
Actuarial estimator, 343 
Adjusted treatment average, 158 
Aliases, 196, 198, 216 
Alternative hypothesis, 60, 63 
American Society for Quality (ASQ), 421 
Analysis of variance (ANOVA), 164, 166, 220 
Analytic study, 99 
ANOVA Table, 153, 155, 159, 170 
AOQ, see Average outgoing quality (AOQ) 
AOQL, see Average outgoing quality limit 
(AOQL) 
AQL, see Acceptable quality level (AQL) 
ARIMA model, 312 
ARL, see Average run length (ARL) 
Arrhenius law, 363 
Artiﬁcial intelligence (AI), 294 
ASN, see Average sample number (ASN) 
ASN function, 357, 359, 407, 409 
ASQ, see American Society for Quality (ASQ) 
Assignable causes, 23, 34, 54, 127 
ATI, see Average total inspection (ATI) 
Attained signiﬁcance level, 65 
Automatic process control, 102, 104 
Availability, 7, 215, 239, 296, 321, 322, 328, 
329, 332–335, 364, 366, 390 
Availability function, 329, 332, 364 
Average outgoing quality (AOQ), 419, 440 
Average outgoing quality limit (AOQL), 419 
Average run length (ARL), 67, 83, 90 
Average sample number (ASN), 357, 406 
Average total inspection (ATI), 440 
B 
Balanced incomplete block designs (BIBD), 
157 
Batches, 2, 6, 100, 129, 130, 133, 134, 242, 
393 
Bayes estimation of the current mean, 94 
Bayes estimator, 94, 343, 375–379, 391–393 
Bayesian credibility intervals (BCI), 378 
Bayesian detection, 86, 88 
Bayesian hierarchical model, 284 
Bayesian networks (BN), 6, 293, 294, 296, 299 
Bayesian reliability, 371 
Bayesian strategy, 410 
BCI, see Bayesian credibility intervals (BCI) 
BECM procedure, 94 
Beﬁtting cross validation (BCV), 290 
Bernoulli trials, 410, 412 
Best linear unbiased prediction (BLUP), 99, 
275 
Beta distribution, 125, 372, 373, 376, 379, 390, 
391, 411 
Beta function, 372, 410 
BI, see Business intelligence (BI) 
BIBD, see Balanced incomplete block designs 
(BIBD) 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 
R. S. Kenett et al., Industrial Statistics, Statistics for Industry, Technology, and 
Engineering, https://doi.org/10.1007/978-3-031-28482-3
465

466
Index
Binomial experiments, 373 
Binomial testing, 352, 353 
Bivariate normal distribution, 120, 282 
Block designs, 147, 149, 153, 157, 219 
Block diagram, 23, 325, 326 
Blocking, 146, 155, 160, 161, 163, 193, 215, 
219, 266 
BLUP, see Best linear unbiased prediction 
(BLUP) 
BN, see Bayesian networks (BN) 
Boosted tree, 312 
Bootstrap, 29, 49, 351 
Bootstrap conﬁdence intervals, 335 
Bootstrap conﬁdence limits, 351 
Bootstrap estimate, 335 
Burn-in procedure, 364, 366 
Business intelligence (BI), 8 
C 
CAD, see Computer-aided design (CAD) 
CAM, see Computer-aided manufacturing 
(CAM) 
Canonical form, 213 
Canonical representation, 211 
Causality, 134, 294, 296 
CBD, see Complete block design (CBD) 
CBM, see Condition-based maintenance 
(CBM) 
C-chart, 22, 25 
c.d.f., see Cumulative distribution function 
(c.d.f) 
CED, see Conditional expected delay (CED) 
Censored observation, 335, 336, 341, 343, 
383 
Center line, 22, 23, 26 
Central composite design, 205 
Change point, 83, 95 
Check sheets, 33, 35, 54 
Chi-squared test, 36 
Chronic problems, 14, 20, 21, 54 
CI, see Condition indicator (CI) 
Circuit pack assemblies (CPA), 144 
Clustering, 60 
Coding, 118, 437 
Coefﬁcient of variation, 333 
Combinatoric designs, 157, 159 
Common causes, 14, 16, 21–23, 26, 29, 54, 75, 
100 
Complete block design (CBD), 147 
Computation pipeline, 290, 303 
Computer-aided design (CAD), 2 
Computer-aided manufacturing tool (CAM), 2 
Computer experiment, 7, 265, 289 
Computer response time optimization, 258 
Conditional distribution, 123 
Conditional expectation, 377 
Conditional expected delay (CED), 83, 86, 89, 
90 
Conditional independence, 294 
Condition-based maintenance (CBM), 6, 8, 
307, 320 
Condition indicator (CI), 308, 309 
Conﬁdence level, 31, 131, 379 
Consistent estimator, 100, 346 
Consumer’s risk, 399, 401, 418, 428, 439 
Continuous ﬂow production, 2 
Continuous random variable, 416 
Continuous variable, 145, 147 
Contrasts, 154, 171, 183, 195 
Control charts, 2, 3, 6, 18, 22, 23, 25, 39–41, 
44, 46, 49, 54, 67, 68, 73, 79, 81, 83, 84, 
90, 113, 114, 118, 124–127, 293, 298, 
299 
Control charts for attributes, 22, 40, 54, 298 
Control charts for variables, 22, 43, 54 
Controllable factors, 141, 145, 218, 231, 232, 
257 
Control strategy, 243, 252, 262 
Convolution, 328 
Covariance matrix, 114, 115, 120, 122, 124, 
125, 127–129, 235, 280, 436, 437 
Covariates, 50, 297, 299, 306 
CPA, see Circuit pack assemblies (CPA) 
CPS, see Cyber-physical systems (CPS) 
CQA, see Critical quality attributes (CQA) 
Credibility intervals, 378, 390 
Critical quality attributes (CQA), 243 
Critical region, 61 
Cross-validation, 275, 278, 290, 311 
CUB model (combination of uniform and 
shifted binomial random variables) 
model, 296 
Cumulative distribution function (c.d.f.), 400 
Cumulative hazard rate, 324 
Cumulative sum (CUSUM), 73 
Cumulative sum control charts, 73 
Customer requirement, 227 
Customer’s tolerance interval, 227 
CUSUM, see Cumulative sum (CUSUM) 
Cybermanufacturing, 287–315 
Cyber-physical systems (CPS), 7, 287, 307 
Cycle time, 11, 12, 14, 16, 21–23, 26, 28, 29, 
169–171, 175, 181, 184, 206, 207, 211, 
217, 226, 269, 276, 277, 309, 310, 312, 
314, 328, 332, 333, 366

Index
467
D 
DACE, see Design and analysis of computer 
experiments (DACE) 
Data analytics, 289 
Data fusion, 289 
Datasets 
ABC, 298 
ABC2, 291 
ALMPIN, 114–116, 124 
COAL, 79 
COMPURESP, 260 
CONTACTLEN, 44 
CUSTOMDESIGN_169, 218 
CUSTOMDESIGN_35, 218 
CUSTOMDESIGN_80, 218 
DISS, 131 
DOJO1935, 96 
FAILTIME, 345 
FAILURE_J3, 434 
FILMSP, 105 
FLEXPROD, 257 
GASTURBINE, 40 
HADPAS, 155 
IPL, 75 
JANDEFECT, 41, 71 
KEYBOARDS, 163 
ORDER_PSD, 316 
OTURB1, 12 
PBX, 34 
PENSIM_100, 315 
PENSIM_CCD, 316 
PLACE, 116, 118, 125 
PROCESS_SEGMENT, 50 
QBD, 245 
RNORM10, 62 
SCM_WHEELS, 136 
SOLDEF, 100 
STRESS, 189 
SYSTEMFAILURE, 383, 388 
TEMP_WORLD, 316 
THICKDIFF, 81 
YARNSTRG, 65 
Decision trees, 50, 51, 53, 192, 300 
Deﬁning parameters, 194–196, 198 
Degree of fractionation, 195, 199 
Degrees of freedom, 53, 128, 130, 266, 267 
Deming inspection criterion, 426 
Design and analysis of computer experiments 
(DACE), 266, 273 
Design for variation (DFV), 278, 301 
Design of experiments (DoE), 141, 215, 242, 
262, 289 
Design parameters, 230, 231, 233, 235, 238 
Design space, 215, 217, 218, 242, 244, 245, 
247, 248, 251, 262, 274, 279 
Desirability, 247 
Desirability function, 247, 248 
DevOps, 290 
DFV, see Design for variation (DFV) 
Diagnostic, 1, 288, 310 
Digital twins, 1, 287–315 
Directed acyclic graph (DAG), 294 
Discrete random variables, 294, 335 
Discrete variable, 145 
Discretization, 267, 289 
Disqualiﬁcation, 424 
DLM, see Dynamic linear model (DLM) 
DOE, see Design of experiments (DoE) 
D-optimality, 215 
Double-sampling plan, 406 
Dow-Jones ﬁnancial index, 98 
Down time, 322, 366 
Dynamic linear model (DLM), 102 
Dynamic programming, 412, 415 
Dynamic time warping, 6 
E 
EBE, see Empirical Bayes estimator (EBE) 
Economic design, 68 
Empirical Bayes estimator (EBE), 392 
Empirical Bayes method, 100, 392 
Emulator, 277, 278, 284 
Enumerative study, 99 
Event, 16, 23, 25, 30, 33–35, 87, 134, 379, 389 
Expected loss, 201, 229, 376 
Experimental array, 142, 217, 219, 239, 
244–246, 256, 260, 269 
Experimental layout, 142, 143 
Exponential distribution, 229, 281, 324, 338, 
343, 349, 353, 364, 374, 428, 439 
Exponential reliability estimation, 380 
External feedback loop, 20, 21, 23, 54 
Externally assigned target, 113, 124, 128 
External reference sample, 113, 124, 127 
External sources, 231 
F 
Factorial designs, 146, 166, 174, 175, 177, 185, 
193, 205, 216, 220, 239, 243, 245 
Factorial experiments, 166, 168, 169, 175, 193, 
232, 233, 238, 241, 244 
Factor levels, 146, 175, 185, 219, 240, 250, 
251, 254, 256, 259, 269 
Failure censoring, 348

468
Index
Failure intensity function, 329, 432, 433, 438 
Failure rate, 75, 323, 366, 383, 386, 388–390, 
392, 429, 431, 439 
FDA, see Functional data analysis (FDA); 
Food and drug administration (FDA) 
FDS, see Fraction of design space (FDS) 
FEM, see Finite element method (FEM) 
Fidelity level, 280, 284 
Finite element method (FEM), 266, 289 
Fishbone charts, 35 
Fixed effect, 99 
Flow charts, 33, 54 
Food and drug administration (FDA), 130, 242 
Fractional factorial designs, 175, 216, 220, 239 
Fractional replications, 146, 176, 193–195, 
197, 199, 241, 253 
Fraction of design space (FDS), 217, 218 
Free time, 322 
Frequency censoring, 336, 349 
Functional data analysis (FDA), 50, 52–54 
G 
Gamma distribution, 99, 279, 333, 373, 386, 
390 
Generalized linear regression, 99 
Generators, 197–200, 266, 267, 345 
Geometric mean, 248 
Graphical analysis, 336 
H 
Hazard function, 323, 330 
Headstart values, 81 
Highest posterior density (HPD), 387 
Homogeneous groups, 160 
HPD, see Highest posterior density (HPD) 
Hypergeometric distribution, 400, 403 
I 
ICH guidelines Q8-Q11, 262 
IIOT, see Industrial internet of things (IIOT) 
Incomplete beta function ratio, 411 
Independent random variables, 96, 99, 148, 
150, 167, 178, 271, 328 
Independent trials, 392 
Industrial internet of things (IIOT), 307 
Industry 4.0, 5, 226, 287, 302, 303, 305, 307 
InfoQ, see Information quality (InfoQ) 
Information quality (InfoQ), 4, 5, 9, 141, 216, 
290–293, 297, 299–301, 306, 314 
Inner array, 239, 240, 262 
Inspection, 2, 3, 8, 20, 70, 129, 145, 252, 343, 
398, 406, 418–420, 422–424, 426, 427, 
440 
Interactions, 7, 13, 149, 153, 161, 166, 169– 
171, 173, 174, 177, 179, 182, 183, 185, 
187, 191, 197, 198, 207, 216–218, 240, 
242, 245, 246, 251, 257, 267, 273, 301, 
306, 307 
Internal feedback loop, 20, 25, 54 
Internally derived targets, 113, 124, 125 
Internal sources, 231 
Internet of things (IOT), 288, 307 
Interruption, 424 
Intervention, 134 
Intrinsic availability, 322, 366 
Inventory management, 2 
Inverse power model, 363 
I-optimality, 216 
IOT, see Internet of things (IOT) 
Ishikawa diagrams, 35 
J 
Job shop, 2 
Joint distribution, 120 
Joint probability distribution (JPD), 294 
JPD, see Joint probability distribution (JPD) 
K 
Kalman Filter, 91, 95–97, 103 
Kriging, 2, 268, 269, 272, 275–277, 284 
L 
Lagrangian, 210 
Laplace transform, 328, 329, 331, 332 
Lasso regression, 135 
Latin hypercubes, 2, 270, 271, 273, 275, 277, 
278, 284 
Latin squares, 160, 161, 163, 164, 220 
Law of large numbers (LLN), 283, 392 
LCL, see Lower control limit (LCL) 
Least squares, 50, 96, 97, 158, 173, 179, 180, 
187, 275 
Least squares estimator, 173, 179, 180, 187, 
275 
Length of runs, 66 
Level of signiﬁcance, 62, 65, 150, 183, 246, 
352, 355, 400 
Life distribution, 320, 323, 336, 337, 351, 353, 
363, 364, 366, 376, 377, 382, 429 
Life length, 229, 323, 324

Index
469
Likelihood function, 87, 344, 347, 374, 383, 
390, 435 
Likelihood ratio, 76, 88, 355, 357, 361, 438 
Limiting quality level (LQL), 399, 439 
Linear combination, 114 
Linear graph, 240, 257, 262 
Linear model, 102, 146, 147, 149, 153, 
178–180, 210 
Loss function, 227, 257, 262, 375, 376, 391 
Lot, 129, 301, 398, 400–403, 406, 407, 409, 
416–421, 424–426, 428, 439 
Lower control limit (LCL), 22, 23, 40, 125, 298 
Lower speciﬁcation limit (LSL), 28, 227, 416 
Lower tolerance limit, 124, 417, 418 
Lower warning limit (LWL), 40 
LQL, see Limiting quality level (LQL) 
LSL, see Lower speciﬁcation limit (LSL) 
LWL, see Lower warning limit (LWL) 
M 
Machine learning, 289, 290, 303, 384 
Mahalanobis distance, 131 
Main effects, 148, 149, 153, 161, 163, 165, 
166, 169–172, 174, 177, 179, 181–183, 
186, 191, 198, 207, 216–218, 232, 233, 
240–242, 251, 257, 261 
Maintenance, 8, 20, 291, 319, 335 
Manufacturer’s tolerance, 228 
Marginal distribution, 120 
Maximum likelihood estimator (MLE), 87, 
122, 344, 346 
Mean squared error (MSE), 229, 234 
Mean time between failures (MTBF), 429 
Mean time to failure (MTTF), 346, 366 
Mean vector, 115, 128 
Measurement units, 113, 124, 129 
Metamodel, 278, 284 
MIL-STD-105, 422 
Minimal sufﬁcient statistic, 390 
Mixed effect, 99 
Mixed model equation, 99 
Mixing, 60, 280, 373 
Mixture design, 262 
MLE, see Maximum likelihood estimator 
(MLE) 
Moment generating function (m.g.f.), 329 
Moments, 283, 328, 329, 394 
Monitoring, 1, 93, 288, 307, 309 
Monitoring indices, 129 
Moving average, 91 
MSE, see Mean squared error (MSE) 
MTBF, see Mean time between failures 
(MTBF) 
MTTF, see Mean time to failure (MTTF) 
Multiple regression, 179, 185, 187 
Multivariate control charts, 114, 118, 124 
Multivariate process capability indices, 120 
Multivariate statistical process control, 116 
Multivariate tolerance region, 130 
N 
Neural network, 289, 311–313 
Nonconforming item, 399, 400, 423, 427 
Normal approximation, 61, 71, 345, 346, 352, 
404 
Normal inspection level, 422 
Normal probability plot, 12 
Np-chart, 22, 23 
O 
OAB, see One-armed bandit (OAB) 
Objectives, 2, 6, 22, 26, 103, 114, 141, 142, 
145, 227, 229, 231, 232, 234, 238, 242, 
247, 257, 258, 291, 335, 364 
OC, see Operating characteristic function (OC) 
OC curve, 71, 72 
Off-line quality control, 226, 256, 262 
One-armed bandit (OAB), 410 
Ongoing chronic problems, 20 
Operating characteristic function (OC), 70, 72, 
429 
Operating time, 321, 429 
Operational readiness, 322, 366 
Operational risk, 243 
Optimality, 215 
Optimal regions, 210 
Order statistics, 336, 342 
Ordinary differential equations (ODE), 289 
Orthogonal array, 215, 239–241, 256, 257, 260 
Outer array, 239, 262 
Outliers, 304, 305 
P 
Page’s control schemes, 78, 79, 81, 85 
Paired comparisons, 149, 151 
Parallel connection, 325 
Parameter design, 226, 229–232, 262 
Parameter space, 371, 374, 378, 385 
Parametric empirical Bayes, 100 
Parametric family, 393 
Pareto chart, 33, 35, 36, 39, 54

470
Index
Partial differential equations (PDE), 289 
P-chart, 22, 23 
Performance measures, 226, 229, 262 
PFA, see Probability of false alarm (PFA) 
Physical experiments, 8, 231, 265, 268, 
278–280, 284, 289 
Piston, 11, 14, 16, 18, 21–23, 26–29, 169, 170, 
175, 181, 206, 207, 211, 214, 217, 226, 
249, 268, 272, 273, 275, 277, 309, 310, 
312, 314 
PL, see Product limit (PL) 
Poisson distribution, 78, 79, 85, 99, 330, 373, 
392, 432 
Poisson process, 281, 432 
Positive deﬁnite matrix, 114, 120 
Posterior distribution, 94, 99, 279, 280, 371, 
373, 374, 376, 377, 379, 385, 386, 410, 
411 
Posterior expectation, 99, 374, 376 
Posterior probability, 87, 294, 379, 411 
Posterior risk, 376 
Precision, 18, 21, 90, 142, 143, 146, 266, 309, 
334, 348 
Predicted values, 364 
Prediction intervals, 378, 380, 381 
Predictive distribution, 100, 381 
Prescriptive analytics, 1, 288, 314 
Principal components, 122, 247 
Principal component vector, 122 
Prior distribution, 87, 94, 279, 371, 373–377, 
380, 381, 385, 386, 391–393, 410, 412, 
435 
Probability distribution function (p.d.f.), 400 
Probability function, 430 
Probability of false alarm (PFA), 83, 89 
Process capability, 20, 22, 25–30, 33, 49, 54, 
59, 120, 124, 133, 228, 230 
Process capability analysis, 26, 27, 49, 59 
Process capability indices, 28, 120 
Process capability study, 20, 22, 26, 54, 124, 
125 
Process control, 3, 4, 7, 8, 11, 14, 20, 21, 32, 
49, 66, 102, 104, 116, 127, 130, 242, 
243, 269, 288, 293, 298, 398, 427 
Process tracking, 90 
Producer’s risk, 399, 401, 416, 428, 439 
Production, 2, 13, 114, 129, 229, 290, 304, 307, 
398, 399, 424, 426, 428 
Product limit (PL), 342 
Prognostic, 1, 288, 312 
Proportional rule, 102 
Protocol, 36, 142, 146 
Python packages 
lifelines, 346, 383 
mistat, 11, 27, 62, 83, 131, 170, 175, 206, 
211, 253, 269, 272, 305, 334, 373, 374, 
398, 401, 402, 414 
plotly, 118 
pwlf, 52 
pymc, 384, 385 
random, 281 
scikit-learn, 137 
scipy, 152, 281, 283, 386, 400, 411, 434 
statsmodels, 155 
Q 
QbD, see Quality by design (QbD) 
QMP, see Quality measurement plan (QMP) 
Quadratic model, 185, 201 
Qualiﬁcation, 424–426 
Quality by design (QbD), 8, 242, 243, 247, 
251, 278 
Quality engineering, 262 
Quality management, 3 
Quality measurement plan (QMP), 98 
Quality planning, 262 
R 
Random censoring, 335 
Random component, 268 
Random forest, 312 
Randomization, 146, 147, 151, 152, 219, 239, 
266, 268 
Randomization test, 151, 152 
Randomized complete block design (RCBD), 
147, 149, 153 
Randomness, 59, 62–66, 266, 269, 296 
Random numbers, 266, 280, 281, 335, 423 
Random numbers generation, 281 
Random order, 142 
Rational subgroups, 26, 40, 54 
RCBD, see Randomized complete block 
design (RCBD) 
R-charts, 25 
Rectifying inspection, 418, 440 
Reduced inspection level, 440 
Reference distribution, 151 
Reference sample, 113, 129 
Regression coefﬁcients, 202, 216, 257 
Regularization, 135 
Rejection region, 61 
Reliability, 267, 290, 319 
Reliability demonstration, 351, 355 
Reliability function, 323–325, 327, 342, 343, 
366, 380, 393, 394, 429

Index
471
Reliable, 243, 292, 303, 309, 326, 362, 439 
Remaining useful life (RUL), 8, 307 
Renewal density, 331 
Renewal function, 330, 331, 366 
Renewal process, 328, 334 
Repairable system, 319, 328, 329, 390, 434 
Repeatability, 292 
Reproducibility, 292, 303, 306 
Resolution, 198, 216, 240, 241, 279, 291, 292, 
301 
Response surface, 201, 206, 207, 210, 213, 
220, 246, 262, 314 
Response variable, 141, 142, 145, 146, 218, 
236, 256, 259, 270, 271 
Resumption, 424 
Risk management, 242, 262 
Robust design, 4, 262, 278 
Rotating machine, 307–309 
RUL, see Remaining useful life (RUL) 
Run charts, 12, 22, 33, 35, 50, 54 
Run length, 67, 83, 90, 127 
Runs, 8, 12, 16, 22, 23, 33, 35, 50, 54, 60–63, 
65–67, 72, 83–85, 89, 90, 93, 100, 
127, 207, 218, 239, 245, 255, 256, 266, 
269–271, 300, 304, 305, 310, 333, 334, 
347, 412, 427 
Runs above or below, 63 
Runs up and down, 63, 66 
Run test, 62, 72 
S 
Saddle point, 213 
Sample covariance, 122 
Sample median, 62 
Sample range, 25, 43, 44, 46 
Sample realization, 344 
Sample standard deviation, 29, 44, 46, 48 
Sampling distribution, 30, 46 
Sampling inspection, 398, 419, 427 
Scaled prediction variance (SPV), 218 
Scatterplots, 33, 34, 116, 133 
S-charts, 25 
Scheduled operating time, 322 
SCM, see Synthetic control method (SCM) 
SE, see Standard error (SE) 
Second order designs, 202, 205, 211 
Sensors, 3, 4, 6, 7, 50, 288, 289 
Sequential probability ratio test (SPRT), 76 
Sequential reliability testing, 429 
Sequential sampling, 398, 410, 418, 439 
Series structure function, 325, 326 
Shewhart control charts, 39, 67, 68, 73, 81 
Shift in the mean, 68, 70, 74, 93 
Shiryayev-Roberts procedure, 90, 438 
Signal to noise, 234, 257, 262 
Signal to noise ratio, 234, 257 
Signiﬁcance level, 38, 39, 65, 353, 357 
Simulation, 2, 6, 7, 9, 16, 18, 83, 89, 90, 93, 
206, 207, 231, 235, 237, 238, 253, 266, 
275, 278, 280, 284, 288, 289, 300–302, 
307–309, 347, 412 
Simultaneous conﬁdence interval, 156, 172 
Single-stage sampling plan, 400, 405, 406, 418, 
439 
Skewness, 12 
Skip lot (SL), 423, 424, 440 
Skip lot sampling plans (SLSP), 423, 440 
Skip lot switching rules, 424 
SL, see Skip lot (SL) 
SLSP, see Skip lot sampling plans (SLSP) 
Smart manufacturing, 287–290 
Software reliability, 398, 429, 434 
Space ﬁlling designs, 284 
SPC, see statistical process control (SPC) 
Special causes, 14, 18, 20, 21, 23, 26, 54, 75 
Sporadic spikes, 14, 20, 21, 26, 54 
SPRT, see Sequential probability ratio test 
(SPRT) 
SPV, see Scaled prediction variance (SPV) 
SST, see Total sum of squares (SST) 
Stability, 23, 25, 86 
Stable process, 14, 26, 426 
Standard error (SE), 91, 171, 298, 346 
Standardized residual, 36, 38, 39 
Standard order, 175 
Statistical dependence, 294 
Statistical hypotheses, 351 
Statistical model, 2, 143, 146, 147, 166, 187, 
219, 278 
Statistical process control (SPC), 3, 11, 21, 49, 
116, 269, 288, 298, 398, 427 
Steepest ascent, 211, 220 
Stochastic control, 102 
Stochastic emulator, 277, 278, 284 
Stopping threshold, 88–90 
Storage time, 322 
Structure function, 324, 325, 327, 366 
Sufﬁcient statistic, 390, 415 
Sum of squares of deviations (SSD), 167 
Symmetric matrix, 114, 115, 120, 213 
Synthetic control method (SCM), 134 
System design, 7, 229, 230 
System reliability, 324–328, 430

472
Index
T 
TAB, see Two-armed bandit (TAB) 
Taguchi method, 226 
Taylor series, 235 
Taylor system, 2 
Testing hypotheses, 352, 355, 429 
Testing statistical hypotheses, 351 
Time categories, 321, 366 
Time till censoring (TTC), 342 
Time till failure (TTF), 328, 342, 364–366, 429 
Tolerance design, 229–231, 253, 256, 262 
Tolerance interval, 49, 227, 253, 364 
Tolerance region (TR), 120, 123, 130 
Total sum of squares (SST), 163, 167 
Total time on test (TTT), 347, 348, 361, 378, 
382 
TR, see Tolerance region (TR) 
Treatment combinations, 148, 166, 170, 173, 
175, 177, 180, 181, 185, 193–195, 232, 
238, 255 
Treatments, 134, 146–149, 153–155, 157–159, 
161–163, 166, 170, 173, 175, 177, 180, 
181, 185, 193–195, 232, 238, 243, 255, 
281, 410 
Trial, 142, 146, 157, 159–161, 166, 194, 240, 
241, 281, 360, 373, 392, 410–412, 414, 
415 
TTC, see Time till censoring (TTC) 
TTR, see Time till repair (TTR) 
TTT, see Total time on test (TTT) 
Two-armed bandit (TAB), 410, 415 
Type I error, 127, 362 
Type II error, 355–357 
U 
U-chart, 22, 25, 26 
UCL, see Upper control limit (UCL) 
Unbiased estimator, 91, 150, 155, 173, 188, 
234, 343 
Unbiased predictor, 275 
Uncensored observation, 336, 343, 346, 383 
Uniform distribution, 270, 281, 376, 386, 423 
Unrepairable system, 319, 330 
Upper control limit (UCL), 22, 23, 40, 41, 124, 
298 
Upper Page’s control scheme, 78 
Upper speciﬁcation limit (USL), 28, 122, 227 
Upper tolerance limit, 124 
Upper warning limit (UWL), 40 
Up time, 322, 366 
USL, see Upper speciﬁcation limit (USL) 
Utility function, 197 
UWL, see Upper warning limit (UWL) 
V 
Validity, 29, 146 
Variability, 13, 14, 22, 25, 46, 48, 60, 120, 227, 
229, 231, 266 
Variable decomposition, 129 
W 
Wald SPRT, 355, 429 
Wave soldering process (WSP), 144 
Weibull distribution, 333, 337, 340, 349, 351, 
363, 364, 383–385 
Western Electric, 2, 99 
WSP, see Wave soldering process (WSP) 
X 
X-bar chart, 16, 22, 23, 25, 26


Ling Feng
Context-Aware Computing
Unauthenticated

Advances in Computer Science
Volume 3
Unauthenticated

Ling Feng
Context-Aware
Computing
Unauthenticated

This work is co-published by Tsinghua University Press and Walter de Gruyter GmbH.
Author
Prof. Ling Feng
Tsinghua University
Dept. of Computer Science and Technology
Haidian District
30 Shuangqing Road
100084 Beijing, China
Ling Feng is a professor of Computer Science and Technology at Tsinghua University, in China. Her
research interests include context-aware data management and services towards Ambient
Intelligence, knowledge-based information systems, data mining and warehousing, and distributed
object-oriented database management systems. Her work received the 2004 innovational VIDI Award
by the Netherlands Organization for Scientiﬁc Research, the 2006 Chinese Chang Jiang Professorship
Award by the Ministry of Education, and the 2006 Tsinghua Hundred-Talents Award.
ISBN 978-3-11-055568-4
e-ISBN (PDF) 978-3-11-055667-4
e-ISBN (EPUB) 978-3-11-055569-1
Set-ISBN 978-3-11-055668-1
ISSN 2509-7253
Library of Congress Cataloging-in-Publication Data
A CIP catalog record for this book has been applied for at the Library of Congress.
Bibliographic information published by the Deutsche Nationalbibliothek
The Deutsche Nationalbibliothek lists this publication in the Deutsche Nationalbibliograﬁe; detailed
bibliographic data are available on the Internet at http://dnb.dnb.de.
© 2018 Walter de Gruyter GmbH, Berlin/Boston
Typesetting: Integra Software Services Pvt. Ltd.
Printing and binding: CPI books GmbH, Leck
@ Printed on acid-free paper
Printed in Germany
www.degruyter.com
Unauthenticated

To My Parents
Shaoqian and Shihuan
Unauthenticated

Unauthenticated

Preface
Ambient Intelligence is a vision of future Information Society, where people are sur-
rounded by an electronic environment that is sensitive to their needs, personalized to
their requirements, anticipatory of their behavior, and responsive to their presence.
It emphasizes on greater user-friendliness, user-empowerment, and more effective
service support, with an aim to make people’s daily activities more convenient, thus
improving the quality of human life. To make Ambient Intelligence real, effective data
management support is indispensable. High-quality information must be available to
any user, anytime and anywhere on any lightweight device. Beyond that, Ambient In-
telligence also raises many new challenges related to context-awareness and natural
user interaction, entailing us to rethink current data management technologies.
The objective of this book is to tackle the impact of Ambient Intelligence, partic-
ularly its user-centric context-awareness requirement on data management strategies
and solutions. Techniques of conceptualizing, capturing, protecting, modeling, and
querying context information, as well as using diverse user and environment cent-
ric context information in building context-aware data management applications are
discussed.
The book is valuable for the computer professional who wishes to gain an
understanding of advanced context-aware computing techniques in the Ambient
Intelligent era.
It is our hope that the book could stimulate an interest in context-aware data man-
agement research by combining the manifold areas of context-awareness and data
management in the solid belief that a wide spectrum of relevant problems could be
derived and addressed.
Ling Feng
Beijing China, 2017
DOI 10.1515/9783110556674-202
Authenticated
:35 PM

Authenticated
:35 PM

Contents
Acknowledgments
XVII
Part I: Fundamental
1
Context-Aware Data Management Toward Ambient Intelligence
3
1.1
Ambient Intelligence
3
1.1.1
A Scenario
4
1.1.2
Challenges of Context-Awareness
5
1.2
What Is Context?
6
1.2.1
Context Deﬁnitions
6
1.2.2
Context Categorization
6
1.3
Characteristics of Context
7
1.3.1
Being Sensed
7
1.3.2
Through Constrained Devices
8
1.3.3
From Distributed Sources
8
1.3.4
Continuous Change
8
1.3.5
Mobility
9
1.3.6
Temporality and Spatiality
9
1.3.7
Imperfectness and Uncertainty
9
1.4
What Does Context-Awareness Imply?
10
1.4.1
Users’ Perspectives
10
1.4.2
Systems’ Perspectives
11
1.5
Context-Aware Querying
13
1.5.1
Basics of Context-Aware Queries
13
1.5.2
Context-Aware Querying Strategies
14
1.6
Supporting Context-Aware Data Management
17
1.6.1
Context Provider
17
1.6.2
Service Registry
17
1.6.3
Context Processor
19
1.6.4
Context Consumer
20
1.7
Recapitulation
20
Literature
21
Part II: Context
2
Modeling Context
27
2.1
Six Context Representation Methods
27
2.1.1
Key-Value
27
2.1.2
Entity-Relationship
27
2.1.3
Object-Orientation
28
Unauthenticated

X
Contents
2.1.4
Markup Schema
28
2.1.5
Logics
28
2.1.6
Ontology
28
2.1.7
Methods Comparison
29
2.2
Context Ontology
30
2.2.1
Context Formalism
30
2.2.2
Ontology Language – OWL
31
2.2.3
Logic Foundation of OWL – Description Logic (DL)
32
2.2.4
Correspondence Between OWL and DL
35
2.2.5
A Simple Context Ontology Example
36
2.3
Context Events
37
2.3.1
Interpreting Context as Context Events
37
2.3.2
Temporality of Context Events
38
2.3.3
Temporal Operators on Context Events
40
2.4
Recapitulation
42
Literature
42
3
Addressing Context Uncertainty
44
3.1
Uncertainty
44
3.1.1
Aleatory Uncertainty
44
3.1.2
Epistemic Uncertainty
45
3.2
Uncertainty Processing Theories
45
3.2.1
Theory Overview
45
3.2.2
Probability Theory
46
3.2.3
Fuzzy Theory
52
3.2.4
Information-Gap Theory
53
3.2.5
Derived Uncertainty Theory
56
3.3
Uncertainty Processing Practices
57
3.3.1
Practice Overview
57
3.3.2
In Economics
57
3.3.3
In Engineering
62
3.3.4
In Ecology
69
3.3.5
In Information Science
72
3.3.6
In Databases
74
3.4
Context Uncertainty Management
87
3.4.1
Context Uncertainty
87
3.4.2
Probabilistic Context Events
88
3.4.3
Advanced Techniques of Context Uncertainty Management
92
3.5
Recapitulation
94
Literature
95
4
Acquiring Context
102
4.1
Challenges in Context Acquisition
102
4.2
Three Context Acquisition Mechanisms
103
Unauthenticated

Contents
XI
4.2.1
Offering Context Acquisition Services
103
4.2.2
Unifying Context Acquisition Interfaces
103
4.2.3
Building a General Context Provision and Acquisition Adaptor
104
4.3
A Declarative Speciﬁcation Language for Context Acquisition
105
4.3.1
Point Descriptors
106
4.3.2
Connector Descriptors
106
4.3.3
Constraint Descriptors
107
4.3.4
Quality Descriptors
110
4.4
Quality-Assured Context Acquisition
113
4.4.1
The Least Squared Error of Redundant Context Values
113
4.4.2
Failure of Context Providers
114
4.5
Recapitulation
115
Literature
115
5
Protecting Context Privacy
117
5.1
Balancing Privacy and Smartness
117
5.2
Privacy Protection Techniques
118
5.2.1
Access Control
118
5.2.2
Platform for Privacy Preferences (P3P)
120
5.2.3
Hippocratic Databases
121
5.2.4
Anonymity
122
5.2.5
Encryption
123
5.3
Search Over Encrypted XML Context Information
124
5.3.1
A Three-Phased Search Framework
124
5.3.2
Data Encoding Phase
126
5.3.3
Candidate Identiﬁcation Phase
132
5.3.4
Tree Search Phase
137
5.4
Life-Cycle Management of Context Information
142
5.4.1
Requirements for Self-Regulation of Context Information
142
5.4.2
Limitations of Traditional Privacy Preservation Strategies
143
5.4.3
A Life-Cycle Policy (LCP) Model
145
5.4.4
Use Scenarios of LCPs
147
5.4.5
LCP-based Context Privacy Protection Diagram
150
5.4.6
Challenges with LCP-Based Privacy Protection
151
5.5
Recapitulation
153
Literature
153
Part III: Context Awareness
6
Querying Context
159
6.1
Seven Context Query Mechanisms
159
6.1.1
SQL Based
159
6.1.2
XML Based
161
Unauthenticated

XII
Contents
6.1.3
Ontology Based
162
6.1.4
Event Based
163
6.1.5
Logic Based
164
6.1.6
Programming API Based
167
6.1.7
Graphic Interface Based
168
6.1.8
Mechanisms Comparison
168
6.2
Recapitulation
171
Literature
171
7
Detecting Context Events
173
7.1
Integration of Stream and Event Processing
173
7.1.1
Stream and Event Processing Operators
173
7.1.2
Stream and Event Processing Methods
176
7.1.3
Stream and Event Processing Systems
177
7.2
High-Performance Context Event Detection
178
7.2.1
Indexing Stream Records
178
7.2.2
Condensed Composition of Stream Records
178
7.2.3
Partitioning Stream Records for Parallel Processing
186
7.3
Recapitulation
198
Literature
198
8
Energy Management in Context Querying
201
8.1
Energy-Efﬁciency Requirement
201
8.2
Energy-Efﬁciency Problem
202
8.3
Solution Guidelines
202
8.4
Energy Management at Different Computing Levels
203
8.4.1
Energy-Efﬁcient Hardware
204
8.4.2
Energy-Efﬁcient Computer Systems
204
8.4.3
Energy-Efﬁcient Cluster Systems
205
8.4.4
Energy-Efﬁcient Applications
208
8.5
Models and Benchmarks of Energy Efﬁciency
209
8.5.1
Models and Metrics
209
8.5.2
Benchmarks
210
8.6
Energy-Efﬁcient Query Processing and Optimization
210
8.6.1
Energy Management at Sensor Networks
211
8.6.2
Energy Management at Mobile Front Ends
212
8.6.3
Energy-Efﬁcient Query Engines
213
8.7
Discussion
215
8.7.1
Redesigning Physical Context Database
215
8.7.2
Energy-Aware Query Processing and Optimization Strategies
215
8.7.3
Dynamic Workload and Resource Management
216
8.8
Recapitulation
216
Literature
216
Unauthenticated

Contents
XIII
9
Context Query Efﬁciency Versus Expense
221
9.1
Basics of Cloud Computing
221
9.1.1
Service Models of Cloud Computing
221
9.1.2
Characteristics and Beneﬁts of Cloud Computing
222
9.1.3
Key Beneﬁts of Cloud Computing
222
9.2
Concerns of Context Query Performance and Expense
222
9.2.1
Performance Management
223
9.2.2
Resource Charging
224
9.3
Tuning of Query Performance and Expense
224
9.3.1
Problem Formulation
225
9.3.2
Multiple Objective Optimization
227
9.3.3
A Genetic Approach for Multiple Objective Optimization
228
9.3.4
Performance Evaluation
229
9.4
Recapitulation
234
Literature
234
Part IV: Context-Aware Data Management
10
Context-Aware Preference Querying
239
10.1
Query Preferences in Databases
239
10.1.1
Qualitative Representation of Preferences
239
10.1.2
Quantitative Representation of Preferences
240
10.2
Implanting Context-Aware Query Preferences upon a Relational Database
Management System (RDBMS)
241
10.2.1
A Knowledge-Based Context-Aware Preference Model
241
10.2.2
Explicating Context-Aware Preferences in a Database World
243
10.2.3
Personalized Querying with Context-Aware Preferences
245
10.3
Contextual Ranking of Database Querying Results
249
10.3.1
A Motivation Example
249
10.3.2
Database and Context Space
250
10.3.3
Ranking Database Tuples Under Context Instances
250
10.3.4
Building Contextual Ranking Functions by Regression
251
10.3.5
Reducing Context Dimensionality in Contextual Ranking
255
10.4
Recapitulation
260
Literature
260
11
Analyzing Sensitivity of Answer Ordering Change
262
11.1
Motivation
262
11.2
Sensitivity Analysis Techniques in Databases
264
11.2.1
Attribute Selection
264
11.2.2
Provenance and Lineage
265
11.2.3
Causality and Responsibility
265
11.2.4
Sensitivity Measurement and Computation
266
Unauthenticated

XIV
Contents
11.3
Sensitivity Analysis Problem for Answer Ordering Change
266
11.3.1
Review of the Probabilistic Database Model
266
11.3.2
Answer Ordering Change
268
11.3.3
Measurement of Answer Ordering Change
268
11.3.4
Sensitivity of Answer Ordering Change
272
11.4
Sensitivity Computation for Answer Ordering Change
273
11.4.1
Five Computation Modules
273
11.4.2
Sensitivity Computation Method
279
11.4.3
Performance
280
11.5
Recapitulation
288
Literature
289
12
Explaining and Scrubbing Context-Aware Query Results
291
12.1
Motivation
291
12.2
Result Explanation and Uncertain Data Cleaning Techniques
292
12.2.1
Explanation in General
292
12.2.2
Explanation in Databases
293
12.2.3
User Feedback
293
12.2.4
Cleaning Data Uncertainty
294
12.3
Design Principles for Answer Explanation Facility
294
12.4
Involving Users in Querying Uncertain Context Data
295
12.4.1
Result Explanation
296
12.4.2
Query Recomputation
307
12.4.3
Performance
308
12.4.4
Discussion
310
12.5
Recapitulation
312
Literature
312
13
Context-Based Information Reﬁnding
314
13.1
Characteristics of Information Reﬁnding
314
13.1.1
Reﬁnding Is a Common Activity
314
13.1.2
Differences from Information Finding
315
13.1.3
Difﬁculties in Information Reﬁnding
316
13.2
Overview of Information Reﬁnding Techniques
316
13.2.1
Web Information Reﬁnding
317
13.2.2
Personal Information Reﬁnding
320
13.2.3
Comparison of Different Reﬁnding Techniques
325
13.3
Nature-Inspired Context-Based Reﬁnding
327
13.3.1
Brain’s Memory Recall
327
13.3.2
Information Reﬁnding by Structured Context
330
13.3.3
Web Revisitation by Context and Content Keywords
353
13.4
Recapitulation
380
Literature
380
Unauthenticated

Contents
XV
Part V: Context-Aware Applications
14
A Context-Aware Ad-Hoc Meeting Planner Program
389
14.1
Early Pioneering Context-Aware Applications
389
14.2
Motivation
390
14.3
Context-Awareness in ConPlan
391
14.4
ConPlan Design Considerations
391
14.5
ConPlan Framework
392
14.6
ConPlan Context Management
393
14.7
ConPlan Implementation
396
14.8
Recapitulation
398
Literature
398
15
Context-Aware Learning
400
15.1
Learning in an Ambient Intelligent World
400
15.2
Motivation of Context-Aware Learning
400
15.3
Five Kinds of Learning Context
402
15.4
Enabling Techniques for Context-Aware Learning
402
15.4.1
GPS-based Learning
402
15.4.2
Sensor-Based Learning
403
15.4.3
Personalized Learning
403
15.4.4
RFID-based Learning
405
15.5
Some Context-Aware Learning Prototypes
407
15.6
Challenges upon Context-Aware Learning
408
15.7
Recapitulation
409
Literature
410
16
Context-Aware Management of Bilingual Aviation MRO Task Cards
412
16.1
Motivation
412
16.2
Generating Bilingual MRO Task Cards (TaskCardGeneratore2c)
413
16.2.1
TaskCardGeneratore2c Architecture
413
16.2.2
TaskCardGeneratore2c Implementation
415
16.3
Searching Bilingual MRO Task Cards (TaskCardFinder)
419
16.3.1
Existing Search Engines
419
16.3.2
TaskCardFinder Functionalities
421
16.3.3
TaskCardFinder Architecture
421
16.3.4
TaskCardFinder Implementation
422
16.4
User Study
429
16.4.1
On TaskCardGeneratore2c
429
16.4.2
On TaskCardFinder
429
16.5
Recapitulation
430
Literature
430
Unauthenticated

XVI
Contents
17
FireVGuide: A Context-Aware Fire Victims Guide
432
17.1
Motivation
432
17.1.1
Two Real Fire Disasters
432
17.1.2
Reﬂection of the Real Fire Disasters
433
17.1.3
Necessity of FireVGuide
433
17.1.4
Assumptions of FireVGuide
434
17.1.5
Principles of FireVGuide
434
17.2
State-of-Art Fireﬁghting Techniques
435
17.2.1
Supporting Fireﬁghters
435
17.2.2
Supporting Fire Victims
436
17.3
Solution Requirements
436
17.3.1
Building Structure
437
17.3.2
Timeliness
438
17.3.3
Simple Interaction
438
17.3.4
Reliability
438
17.4
FireVGuide Architecture
439
17.4.1
Hardware Deployment
439
17.4.2
Software Architecture
440
17.5
FireVGuide Guidance Generation
441
17.5.1
Evacuation Route Generation
441
17.5.2
To-Do-List Generation for Trapped Occupants
444
17.6
Evaluation of FireVGuide
446
17.6.1
User Interviews
447
17.6.2
Empirical Experience with FireVGuide
449
17.6.3
Effectiveness of FireVGuide
450
17.6.4
Efﬁciency of FireVGuide
453
17.7
Recapitulation
455
Literature
455
Index
459
Unauthenticated

Acknowledgments
We would like to express our sincere thanks to all those who have worked or are
currently working with us on ambient smart and context-aware data management re-
search and projects. These include Peter M.G. Apers, Tharam S. Dillon, Willem Jonker,
Pieter Hartel, Nicolas Anciaux, Yuanchun Shi, Arthur van Bunningen, Harold van
Heerde, Richard Brinkman, Wenwei Xue, Yuanping Li, Xiang Li, Tangjian Deng, Jian-
wen Chen, Hao Wang, Shoubin Kong, Jun Wang, Yiping Li, Qingwei Liu, Liang Zhao,
Li Jin, and Bo Wang. We also wish to thank Lijun Bai, our executive editor, for his
enthusiasm, patience, and support during the writing of this book.
The work is supported by the Ministry of Science and Technology of China and the
National Natural Science Foundation of China.
DOI 10.1515/9783110556674-204
Authenticated
:35 PM

Authenticated
:35 PM

1 Context-Aware Data Management Toward
Ambient Intelligence
Abstract: Developments in ubiquitous computing and ubiquitous communication, to-
gether with intelligent user-friendly interfaces, eventually lead to a world in which
computing functionality will be embedded in all kinds of objects, which are capable
of recognizing and responding to individual human needs in a seamless, unobtrusive,
and often invisible way. Such a vision is coined Ambient Intelligence (AmI), whose
aim is to bring information technology to everyone, every home, every school, and
every business. An example is a hotel room that can adapt automatically to its cus-
tomer’s favorite room temperature and music choice. In the AmI world, for computers
to be able to ﬁt human environments, they must be in proper size and shape, appropri-
ate for their users, and adaptable to the users’ world; in other words, they should be
context-aware. This chapter addresses challenges raised by context-awareness on data
management. Characteristics of context and its implications from the standpoints of
both users and systems are discussed. Reference to relevant research activities and ap-
plicable technologies are meanwhile provided. Six context-aware data management
strategies, using context-aware querying as a case in point, are further illustrated.
A context-aware data management supporting platform, consisting of context pro-
vider, service registry, context processor, and context consumer, is provided, where
context-aware data management is situated as context consumers.
1.1 Ambient Intelligence
In comparison with several other terms like ubiquitous computing, pervasive com-
puting, and wearable computing [14, 54, 64], which share a common vision with
AmI on integration of networked embedded devices into people’s background, AmI
takes this integration one step further by realizing environments that are sensitive
to people’s needs, personalized to their requirements, anticipatory of their behaviors,
and responsive to their presence [1, 13, 45, 51]. It thus emphasizes more on greater user-
friendliness, user-empowerment, and more effective service support. Such a user-
centric focus introduces several basic problems related to natural user interaction,
context awareness, and ubiquitous wireless access to information, communication,
and services in such environments like home and work space.
Adding intelligence to the ambience of users raises many challenges to data
management [4]. High-quality information must be available to any user anytime any-
where and on any device. The implementation of the AmI landscape relies heavily
on these constant information ﬂows from numerous sensors and services, monitoring
not only the users but also external environments. Here, how to intelligently capture
DOI 10.1515/9783110556674-001
Authenticated
:23 PM

4
1 Context-Aware Data Management Toward Ambient Intelligence
and map the data to appropriate behaviors of users in context is a key issue to be
addressed.
This section addresses the impact of AmI, in particular, its context-awareness re-
quirement on data management strategies and solutions. Before eliciting concrete
database requirements from AmI, let’s ﬁrst look at a scenario envisioned in an AmI
world.
1.1.1 A Scenario
John is a business man working for a global chain restaurant corporation. One
morning just after arriving at the ofﬁce, he got an urgent message to travel to the
headquarters situated in a nearby country for an important decision-making meet-
ing held in the afternoon. Through his electronic personal assistant (PA), John got to
know the earliest ﬂight that he could catch up with will depart in 3 hours, and his
PA further checked out that there were still some seats available. Considering that
he needed to arrange several reports and have them ready on his PA for the meet-
ing, John commanded his PA to ﬁnd the fastest transportation to the airport, as he
wanted to win as much time as possible for his preparation before setting out. There
were three choices, i.e., by train, by car, and by shuttle bus. John’s PA queried two
separate external databases for the departure times and durations of the train and
bus. It meanwhile calculated the approximate time needed to drive to the airport, tak-
ing John’s driving speed and trafﬁc status during that period into account. Following
the PA’s advice, John arrived at the airport on schedule by his car and boarded the
plane.
On the ﬂight, John browsed through several articles issued from a news center.
One of them about IT services left him a deep impression. However, at that time, he
had not realized the inﬂuence of this article on the acceptance of his proposal at the
afternoon’s meeting.
John’s speech at the meeting was well received. Speciﬁcally, he was able to justify
his proposal by referring the attendants to the article that he read a few hours ago
in the plane. With the help of John’s PA, this article was re-localized and retrieved
from the news center site. This time it was presented clearly on a big screen inside the
meeting room.
The meeting ﬁnished very late that day. John wanted to go ﬁrst for dinner before
ﬂying back home. His PA recommended a few nearby restaurants that were still open,
with an oriental restaurant coming in ﬁrst, as John’s PA knew that John liked oriental
food very much.
It was almost midnight when John’s plane landed on his home city. For safety
reason, the PA suggested John taking a route different from the morning one. Although
this route was a bit longer, it avoided passing through a dark wood. Accompanied with
his private PA, which broadcasted the selected news, John ﬁnally arrived home safely.
Authenticated
:23 PM

1.1 Ambient Intelligence
5
1.1.2 Challenges of Context-Awareness
The above scenario depicts an ambient data management paradigm, distinct from
the current conventional one. It breaks away from the traditional stationary desktop
computing paradigm, giving users the capability of acquiring the most desirable
information anytime anywhere with any lightweight handheld device via wireless
communication channel [2, 3]. In the scenario, user’s database access does not oc-
cur at a single location in a single context, as in desktop computing, but rather span a
multitude of situations and locations covering the ofﬁce, plane, meeting room, home,
and so on.
Till now, decades of efforts have been made in improving content-based data
access due to the long-historical stationary database constraint. Nevertheless, AmI
promotes us to go further for context-based data access. “Get the report which I pre-
pared last night before dinner in the hotel for this afternoon’s meeting” and “Find
restaurants nearby which I have not visited for half a year” are examples of such quer-
ies. Apparently, exploiting various context information can assist data managers to
better understand users’ information needs, and facilitate them to make the best of
data in carrying out daily life and work. From a data manager’s point of view, this
context information also provides hints on how to process data requests in the most
optimal way, as it carries a kind of semantics related to what, why, when, where, and
how to use data sources. We believe that by context-awareness, the interaction and
understanding between data managers and users can be enriched than ever.
On the other hand, keeping data managers aware of context entails a thorough
re-examination of currently existing data management techniques, raising a number
of interesting and challenging questions as follows.
–
By context-awareness, can we make data managers more adaptable, responsive,
personalized, dynamic, and anticipatory, as charted by AmI, than before?
–
Compared with traditional data management, what are the fundamental issues
underlying context-aware data management?
–
To bring context-awareness feature into data management, how to acquire, cat-
egorize, model, and protect context information?
–
How to use context information to answer a user’s data request?
–
What are the context-aware data management strategies? How to support, man-
age, and execute these strategies?
–
How to provide context-aware data management supports to users? How to
design a friendly and easy-to-use context-aware query language for users? How
to effectively and efﬁciently interact with users, given a small device with a
constrained computing capability and energy power?
The purpose of this book is to answer these questions.
Authenticated
:23 PM

6
1 Context-Aware Data Management Toward Ambient Intelligence
1.2 What Is Context?
1.2.1 Context Deﬁnitions
Context is an essential element in AmI. There are several attempts in the literature to
deﬁne the notion of context, ranging from being very broad to being very narrow and
application-speciﬁc.
Broadly, according to Dourish, “Context is a slippery notion. Perhaps appropri-
ately, it is a concept that keeps to the periphery, and slips away when one attempts
to deﬁne it [19]”. Dourish opposed seeing context as something separable from the
content of an activity. As an example, during a conversation the location of this con-
versation could turn from context into content when it becomes the subject of the
conversation.
Lieberman and Selker viewed context from a computer programming’s perspect-
ive. Traditionally, the ﬁeld of computer science tries to be context-independent: the
same input provides the same output, independent of the context of the input [44].
They thus came up with a more concrete deﬁnition of context. “Context can be
considered to be everything that affects the computation except explicit input and
output [44].”
Getting close to the application side, one of the most cited deﬁnitions of context
is from Dey et al. “Context is any information that can be used to characterize the
situation of an entity. An entity can be a person, place, or object that is considered
relevant to the interaction between a user and application, including the user and
applications themselves [9].”
According to Dey, a system is context-aware if it uses context to provide relevant
information and services to the user, where relevancy depends on the users’ task. Gray
and Salber further clariﬁed the term interaction by indicating whether it points to what
is achieved by doing this interaction (e.g., the task), or the interaction itself (e.g., the
user interface or dialogue), and provided a deﬁnition for sensed context [26]. “Sensed
context are properties that characterize a phenomenon. They are sensed and are po-
tentially relevant to the tasks supported by an application or the means by which those
tasks are performed [26].”
From a data management perspective, this book refers context to user’s external
objective environment and internal subjective status. It implies user’s information
need during the interaction between the user and data management system. Context-
awareness is to use context in serving user’s information need [23].
1.2.2 Context Categorization
There are many possible ways to categorize context information [9, 10, 23, 65].
Two typical categorization methods are operational categorization and conceptual
categorization.
Authenticated
:23 PM

1.3 Characteristics of Context
7
via service providers or
(propagated) communication
or inferred from user’s activity
inferred from user’s 
environment and activity
Emotional State
(e.g., happiness, sadness, disgust, fear,
  anger, surprise, calm, etc.) 
Social Environment
(e.g., traffic jam, discount information,
  surrounding people, etc.)
Background
(e.g., interest, habit, preference,
  working area, subjective opinion, etc.)
Computational Environment
(e.g., surrounding devices, etc.)
Context Acquisition
Context Categorization
from user’s agenda
from body sensors
from sensors like GPS
from user’s profile
via multimodal analysis of user’s
visual & acoustical features
Physiological State
(e.g., body temperature, heart rate, etc.)
Environmental
Context
User−Centric
Context
Physical Environment
(e.g., time, location, temperature,
 humidity, noise, light, vibration, etc.)
Dynamic Behavior
(e.g., intention, task, activity, etc.)
Context
Figure 1.1: Conceptual categorization of context information [23].
Based on how context information is acquired, modeled, and treated, Henricksen
and Indulska categorized context information into sensed, static, proﬁled, or de-
rived context [29]. As this categorization is closely related to the way how context
information is handled, it is an operational categorization.
The book conceptually distinguishes user-centric context from environmental
context, as shown in Figure 1.1.
Most of the context categorizations in the literature fall into either of the above
two kinds [10, 18].
1.3 Characteristics of Context
As AmI evolves from distributed mobile computing [55], the acquisition of context
information inherently takes place among distributed sources in a mobile environ-
ment. The characteristics of context are highly inﬂuenced and determined by its
acquisition way.
1.3.1 Being Sensed
A majority of context like location and temperature is sensed through sensors or
sensor networks. Data management solutions view a sensor network as a database.
Authenticated
:23 PM

8
1 Context-Aware Data Management Toward Ambient Intelligence
Some architectural issues, including sensor modeling, imprecise data replication,
data compression and prediction, in-network processing, fault tolerance, and timeli-
ness, etc., are addressed in Ref. [7, 41]. A quality-driven approach is usually adopted
where users can indicate the conﬁdence they want from query answers (e.g., ±1∘C of
the exact answer).
TinyDB [46] is another system delivering a database solution to sensor networks.
It focuses more on when, where, and how the data is acquired. It works with sensors
running the TinyOS operating system, and does processing like ﬁltering and aggreg-
ation as much as possible in the network. A TinyOS query example is like SELECT
temp FROM sensors WHERE temp>threshold.
1.3.2 Through Constrained Devices
What makes the sensing more complicated is that it is done most of the time by
cheap, small, and therefore constrained devices. The limited computing power of such
devices incurs unreliability and difﬁculty in running applications on such a low-level
platform [12]. Another serious factor inﬂuencing the sensing quality is battery capacity
and energy cost. One effective way is to put energy management to a high application
level, and switch applications to modes with lower power consumption when idle [55].
There is a trade-off of having a sensor-based database system. On one hand, due to op-
timization on the sensor level, there is less power spent on transmission, but on the
other hand, more power is required by sensors for processing this data.
1.3.3 From Distributed Sources
Being sensed by many different sensors, context information come from diverse
distributed sources. To get desirable information from these distributed sources, ag-
gregators are usually used to gather context about an entity (e.g., a person in [17]).
Some special sensor-oriented querying techniques, such as the one developed in
Quasar [41], can also be used to address this issue. This characteristic brings about
the requirement of high-interrelation on context-aware data management.
1.3.4 Continuous Change
A crucial property of many sorts of context is the continuity, i.e., the user’s context
constantly changes. This may lead to a proactiveness requirement on data man-
agement and applications [33]. It causes enormous amount of data to be stored,
compressed, and discretized, resulting in data impreciseness.
Authenticated
:23 PM

1.3 Characteristics of Context
9
1.3.5 Mobility
Closely related to the previous characteristic is the mobility of objects from which to
get context information. According to Gareth Jones and Peter Brown, mobility is a
prime ﬁeld for context-aware retrieval due to the three reasons [33]: (a) Information
is now being made available in situations where it was not available before. (b) A
mobile user is often in an unfamiliar environment and needs information about that
environment. (c) It is favorable to use context to help to select the information which
is needed in this new environment.
Adaptation to the current situation and caching are two techniques to handle the
mobility during mobile information access in Ref. [53].
1.3.6 Temporality and Spatiality
Mobility draws forth the prominent temporal-spatial character of contextual informa-
tion. Examples for reasoning with time in temporal ontologies for context-awareness
are given in Ref. [11]. Ter Horst et al. introduced the notion of extended spacetime to
reason about context events, taking time and space into account [62]. Koile et al.
introduced the notion of activity zones, i.e., regions in which the same activities oc-
cur, to trigger certain speciﬁc events [36]. Harter et al. described a context-aware
application which particularly focuses on users’ location using Bats - an ultrasound
position determination system [28]. Hightower and Borriello integrated WLAN with
ultrasound and infrared tracking technologies, and used particle ﬁlters for location
estimation [31].
1.3.7 Imperfectness and Uncertainty
Due to the characteristics of dynamics, constrained devices, diverse distributed
sources, and continuity, etc. there is a high chance that the acquired context informa-
tion is not perfect. Henricksen and Indulska characterized four types of imperfectness
about contextual information [65].
–
Unknown. No information is available about the property.
–
Ambiguous. There are several different reports about the property, which could
for example be someone who is tracked by GPS and by WLAN.
–
Imprecise. The reported information is correct but too imprecise, for example,
from a person it is only known that s/he is in a certain building, but we need the
exact room in which he or she is.
–
Erroneous. The information about the property is not the same as the actual
information.
Authenticated
:23 PM

10
1 Context-Aware Data Management Toward Ambient Intelligence
Gu et al. and Ranganathan et al. provided some modeling solutions for uncertainty by
adding a probability predicate [27, 50]. The work of Antifakos et al. demonstrated that
indicating the degree of imperfectness of information and using it in the computation
can lead to better decision making [5].
1.4 What Does Context-Awareness Imply?
Beyond the traditional so-called “ilities” - nonfunctional requirements like reliabil-
ity, availability, maintainability, responsiveness, manageability, and scalability, etc.,
context-awareness raises a number of particular expectations for data management
from both users and systems’ perspectives.
1.4.1 Users’ Perspectives
1.4.1.1 Adaptiveness and Personalization
There is a growing demand for adaptiveness on small and constrained devices in mo-
bile computing environments. For example, choosing whether to fetch the header or
the whole e-mail message is subject to the network speed. Adaptiveness and person-
alization are continuing to be a key to context-aware data management. Three typical
ways to achieve personalization are the following [49].
–
Rule-Based Matching (based on user proﬁles or communities). An example is “If
the user is a sportsman, display the sport’s equipment advertisement.”
–
Context-Based Matching (depending on the current context). An example is “If
the user is on the sport’s page, display the sport’s equipment advertisement.”
–
Category-Based Matching (based on attributes/features). Content producers
classify their contents based on certain attributes, and users rate their priorities
according to these attributes.
1.4.1.2 Proactiveness
Proactiveness is one of the most important requirements in AmI [23]. It means to pro-
cess information on behalf of a user in such a way that an action can be taken without
requiring his/her attention [33]. This implies knowing what a user would want to do
with the requested information, and detecting his/her behaviorial patterns. Tennen-
house even coins the new term proactive computing, which stands for the movement
from human-centered to human-supervised or even unsupervised computing [61].
Proactiveness calls for effective information extraction techniques to identify situ-
ations and some forms of reasoning mechanisms to determine an appropriate action
to take.
Authenticated
:23 PM

1.4 What Does Context-Awareness Imply?
11
1.4.1.3 Privacy and Security
The mostly mentioned concern for context-awareness in AmI environments is about
users’ privacy, trust, and security. Early work on context-awareness done by Newman
et al. evidenced that during experiments in tracing users during the day with badges,
users did not wear them because of privacy issues [48].
Kindberg et al. argued that using visible tangible objects to do transactions (e.g.
a barcode scanner) can help make transactions more trusted by the users [35]. But
meanwhile, they also point out that some other aspects such as ease-of-use are at
least equally important to users.
Gandon and Sadeh proposed to deal with the privacy and security issue through
privacy preferences [24]. They used access control rules to express who has the
right to see certain context information under different conditions. They also used
obfuscation rules to hinder users from certain details, Leonhardt and Magee applied
the context-based obfuscation approach to access control, but with a focus on users’
location information [42].
1.4.1.4 Traceability
For systems’ proactiveness to be understandable and controllable by users, traceabil-
ity is in need [16]. It means that a user should be able to know what is happening in
the background, and why it happened. Having the system or tool present-at-hand is a
way to realize its traceability [9]. One example is the dashboard of a car, by which the
user can have the car present-at-hand in case something goes wrong. Another example
is the network signal indicator of a mobile phone. Three system design principles are
listed in Ref. [9].
–
Systems should display their own internal states and conﬁguration to the users.
–
The deep system structure should be revealed so as to support inspection and
adaption.
–
Interfaces should offer direct experience of the structures by which information is
organized.
1.4.2 Systems’ Perspectives
1.4.2.1 Dynamic Connection
Because of highly-constrained sensors and mobile objects, one serious issue con-
fronting context-aware data management is dynamic connection. That is, connection
could be lost when a sensor is out of reach or temporary unavailable, and it has to be
re-established once available again.
To address this issue, on one hand, we can cache data. On the other hand,
observing that information from a disconnected sensor can also be acquired via
Authenticated
:23 PM

12
1 Context-Aware Data Management Toward Ambient Intelligence
another sensor or combinations of sensors, Goslar and Schill suggested that a context
database should store how to read values instead of the actual values [25]. DeVaul and
Pentland presented a dynamic decentralized resource discovery framework. It uses
semantic descriptions to see what kind of services are available. Different compon-
ents can be registered to a directory registration service when they are available, and
de-registered when they are not available anymore [16].
Other applicable techniques toward dynamically interchangeable components in-
clude agent techniques and goal-oriented solutions. For example, the goal of ﬁnding
a certain user’s location can be achieved via WLAN-triangulation. However, when
WLAN is not available at the moment, some alternatives like GSM-triangulation or a
GPS will be in place.
Dynamics in different connections challenges the underlying data management
strategies. A lot of existing data management solutions are based on the assumption
that the network topology changes only slowly, which are apparently inapplicable to
data management in AmI environments [5].
1.4.2.2 Tight Interrelationship
Not only does high-level inferred context depend on low-level sensed context but
also different kinds of low-level context parameters are interrelated. For instance,
the amount of computers in a room and the energy usage of this room are closely
related.
The tight interrelation makes it possible to predict some context parameters based
on others [30]. Deshpande et al. exploited such interrelations to do optimizations over
TinyDB by using correlation between voltage and temperature [15]. However, because
contextual data structures are so highly interconnected, when modeling context, we
have to ensure that the models are not too complex due to the limited capabilities of
human users and local devices. Breaking the data structures down into smaller parts
can be a help to address this issue [25].
1.4.2.3 Learning and Reasoning
Due to the interrelationship among different levels of context, some inference mech-
anisms are needed in order to derive some context from other context.
Schmidt was one of the ﬁrst who did so by using cues, which take the value of
one sensor and provide a symbolic or subsymbolic output [57]. Taking the output cues
“the user is running” and “the user has a high pulse,” for example, a context such as
“the user is jogging” can be determined.
Korpipää et al. exploited a set of techniques including Bayesian networks to re-
cognize high-level context [37]. Doing reasoning calls for a way to represent context
knowledge [60].
Authenticated
:23 PM

1.5 Context-Aware Querying
13
1.4.2.4 Alternative Representation and Conversion
Confronted with different context information from diverse sensors and possibly
from different domains, a ﬂexible context representation mechanism is needed so as
to provide conversion among different kinds of context information. Bressan et al.
discussed a method of using Prolog rules to convert across different context repres-
entations [8]. Such an alternative context representation problem bears similarity to
the schema or data integration problem in the database community, which has existed
for over 20 years and has extensively been tackled using Description Logics [43].
1.4.2.5 Metadata about Context Information
Metadata about context information are like temporal-spatial features and certainty
degree, etc. They indicate the time and place at which a context-associated meas-
urement takes place, accuracy of the measurement, as well as the trade-off between
requested accuracy and energy consumption cost, etc. [26].
Metadata incurs collaboration between the data management side and the sensor
side. To deal with context metadata, proper representation of these meta information
is demanded in modeling context.
1.4.2.6 Storage and Logging of Context Information
Context-awareness requires data management systems to be proactive and to detect
patterns according to users’ behaviors. Relevant reactions in response to different
contexts thus need to be stored beforehand. A number of questions related to what,
where, and how to store context information arise.
Meyers and Kern recommended to store context information at a high level [48].
This has two advantages. First, storage space can be reduced by only storing the
high-level context information (e.g., being in a meeting) instead of storing all sensory
details like temperature and exact location, since the former can be derived from the
latter. The second advantage is that at a high level, more computing power is available
for data compression to reduce the enormous amount of sensed context information.
1.5 Context-Aware Querying
Query is the most fundamental operation users pose to database systems, where
context plays an essential role in context-aware querying.
1.5.1 Basics of Context-Aware Queries
A context-aware query is a query whose query answering depends not only on the data
stored but also on the context under which the query is issued.
Authenticated
:23 PM

14
1 Context-Aware Data Management Toward Ambient Intelligence
A context-aware query can thus be viewed as a parameterized query with two
parameters – database and context. The same query, raised by different users, or by
the same user under different contexts, may lead to different kinds of answer deliv-
ery. This is different from the traditional noncontext-aware query, whose answering
depends only on the database.
Deﬁnition 1. Let db denote a database including both database schemas and database
records, and let [[Context]] denote a multidimensional contextual space. A context-
aware query is a triple CQ(db, [[Context]]) = (INexp, INimp, OUT), where
–
INexp is the explicit query request input from the user;
–
INimp is a further constraint over the user’s explicit query request, which makes
user’s implicit query assumptions explicit.
–
OUT is the query output sent to the user.
A traditional noncontext-aware query is a tuple NCQ(db)=(INexp, OUT), with an
explicit query input from the user INexp and a query output OUT to the user.
◻
1.5.2 Context-Aware Querying Strategies
Context-awareness penetrates three phases of a database query, i.e., user’s query
request, system’s query reﬁnement, and query answer [23].
1.5.2.1 User’s Query Request
Strategy 1: Context as On-the-Spot Query Condition
Highly dynamic, intelligent, and responsive ambient environments prompt users to
ask ad hoc queries anytime and anywhere. These on-the-spot queries usually involve
the current context (like time, location, etc.) as the query referential points. Some
typical examples are listed below.
–
“Look for the earliest ﬂight that I can catch.” Only the ﬂights whose departure
time is later than the current context time are meaningful query candidates.
–
“Look for a nearby restaurant for eating.” Only the restaurants near the user and
meanwhile are open are useful query results.
–
“Look for the fastest route to the airport.” The current context – trafﬁc status
must be taken into account in computing the fastest route. Traditional location-
dependent queries in wireless mobile environments [20, 32, 38, 58, 59, 65, 66] fall
into this scope.
Strategy 2: Context as Recall-Based Query Condition/Query Target
For human users, context under which data was accessed in the past is always easier
to remember than detailed data content itself. Identifying data items by context
Authenticated
:23 PM

1.5 Context-Aware Querying
15
besides content empowers users with more convenient and friendly query capabilit-
ies. For example, a user might feel difﬁcult to recall the title of an article. By contrast,
the context under which to read the article, such as the place where the article was
read, the people present when it was read, or the activity being carried out at the
same time, etc., can be easier to remember. In fact, the observation that context
can serve as a powerful cue for recall has a solid foundation in the psychological
ﬁeld, where researchers have developed a theory about episodic or autobiographical
memory [6, 21, 63]. They noticed that human beings naturally organize the memories
for past events into episodes, and that the location of the episode, who was there, what
was going on, and what happened before or after are all strong cues for recall [56].
Studies by Eldridge et. al also conﬁrmed this theory and lead to the construction
of a prosthetic episodic memory device called memory prosthesis [11, 22, 39], and a
wearable rememberance agent [52].
Here are three query-by-context examples.
–
“Look for the article about ‘IT services’ which I read on the plane this morn-
ing.” The query condition is based on the previous context time and simultaneous
activity.
–
“Look for the name of the restaurant which I went to for dinner most frequently
last year.” This is an aggregate query whose query condition is context time. In
addition to retrieving database content by context, it is also possible to query the
context under which a certain database access happens.
–
“Look for the place where I read that news about ‘IT services’.” The query condi-
tion is based on content, but requesting the past context information location.
1.5.2.2 System’s Query Reﬁnement
With Strategies 1 and 2, users can directly pose context-based queries, where con-
text information is explicitly used in query formulation. Beyond that, context itself
can also help the database system to better understand user’s need, since it conveys
a rich set of semantics related to what, why, when, and where to use the data. In
order to make query results truly usable and supportive, thus achieving greater user-
friendliness as demanded by AmI, it is desirable to capture such an implicit query
background, hidden behind context, and translate it into explicit query constraints.
This stage is treated as query reﬁnement stage.
Strategy 3: Context as Query Constraint
The query reﬁnement process infers different kinds of query knowledge from context
to make implicit users’ assumptions explicit in their queries.
–
Understanding user’s query intention. Database systems aims at facilitating users
to ﬁnd useful information to solve their problems. When a user issues a query, s/he
usually has some purpose in mind. For example, s/he retrieves restaurant inform-
ation in the city because s/he wants to invite the clients to a lunch in a few minutes
Authenticated
:23 PM

16
1 Context-Aware Data Management Toward Ambient Intelligence
according to his/her agenda. In this case, only open restaurants are meaningful to
the user, which depends on the current context time. From the system’s point of
view, database access should be directed by user’s speciﬁc task, and this could be
found out from user’s agenda. Here, trying to understand user’s query intention
and enforcing corresponding query constraints is an important step to improve
the usability of databases.
–
Personalizing user’s data request. The usefulness of data is also quite subjective,
and varies from context to context even for the same user. For instance, for safety
reason, a user driving at midnight does not like the database showing him/her the
roads, which need to pass through a dark forest. Also, during the daytime rush
hours, the database system should be considerate enough not to return the roads
going through the city center, or the sites having trafﬁc jam at that moment.
–
Tuning abstraction level of query content. Besides constraining query conditions,
context can be exploited to adjust the level and granularity of abstraction for
querying of the same data content. For example, a user with a big screen nearby
would prefer to display a picture at high-resolution, while with only a handheld
mobile device, a low-resolution requirement is ﬁne enough. Similarly, data at a
high aggregate/summarized level is more appropriate than the one at detailed
low level, which may otherwise cause the user to scroll down the small screen
for reading the answer. Apparently, by tuning the query content to an appropriate
level, and integrating this requirement into the query request, query processing
and optimization can be performed in a single step, and the cost incurred is
proportionate to what the user wants and gets.
1.5.2.3 System’s Query Answer
Apart from assisting query formulation, another important use of context for queries
in AmI environments is to determine the manner such as what, when, where, and how
to deliver query outputs to the users. Clearly, sending query results must not interrupt
or distract the users from performing their primary tasks or annoy nearby people. Here
are three strategies regarding query output.
Strategy 4: Context as Protective Screen for Sensitive Query Results
In ambient environments, a sheer amount of data will be shared and disseminated
in response to different users’ requests. In order to build up trust and conﬁdence to
ambient data managers, it is important for the database system to protect sensitive
data from being disclosed to the third party. For example, a police querying a trafﬁc
accident scene may want the license plates of the damaged cars to be superimposed
with a black bar within the query result, if there is any unauthorized person around
him/her. As another example, consider the viewing of news items that contain shock-
ing scenes. An adult may access the whole content, however, a child may not. Also,
the system may not want an adult to see it in a public environment, given that there
may be some children around [34].
Authenticated
:23 PM

1.6 Supporting Context-Aware Data Management
17
Strategy 5: Context as Criteria for Query Result Measurement
Given the limitation of small devices, it is more convenient for users if the query an-
swer could be sorted in such a way that the most potentially useful items shown in
front. Such an ordering work can be performed based on user’s interests, obtained
from the proﬁle, For example, if the user likes oriental food, the restaurants serving
Asian meals can be displayed ahead of others.
Strategy 6: Context as Guide to Query Result Delivery
The output modality must be adapted to user’s current context. For example, if the
user is driving, it would be convenient to have a speech query output. However, if
the user is talking with someone, postponing the delivery of query results by giving
a vibration alert or screen-displaying the query results will be more appropriate. The
presentation of query results on a big screen would be welcomed for a group of people
who are interested in the query answer.
In addition, context-aware queries differ in how deep they delve into the con-
text notion, for example, the necessity of time and probability information and the
inclusion of conﬁdentiality functions. The execution of context-aware queries also de-
pends on the level of conceptualization of the context to which extent reasoning and
inference must be applied to get from low-level to high-level context.
1.6 Supporting Context-Aware Data Management
To tackle the challenges of proactiveness, tractability, inference, etc. raised by context-
awareness in AmI, a context-aware data management supporting framework consists
of four major components, namely, context provider, service registry, context pro-
cessor, and context consumer, as illustrated in Figure 1.2. Different challenges raised
by context-awareness are tackled by different components, as shown in Tables 1.1
and 1.2.
1.6.1 Context Provider
Context providers are responsible for providing context in the form of services, taking
into account all characteristics of context. Different kinds of contexts are taken care
by different services, like location services, temperature services, and multimodal ser-
vices, etc. Due to the interrelationship, more reliable information can be sought by
combining several context parameters. These services are also responsible for acquir-
ing and supplying metadata about the context, and furthermore can provide part of
the privacy and security features at a sensor level.
1.6.2 Service Registry
The communication between context providers and context processors is done via the
service registry. The distributed context providers register themselves at the registry.
Authenticated
:23 PM

18
1 Context-Aware Data Management Toward Ambient Intelligence
SERVICE REGISTRY
Context
Like:
   Temperature sensors
   Location sensors
   Rain sensors
   User agenda
   etc.
CONTEXT PROVIDERS
CONTEXT
PROCESSOR
CONTEXT CONSUMERS
Context
Context
augmented
with
knowledge
Like:
Restaurants
Cooking programs
Museums
Art galleries
Context-aware DBMS’
Televisions
Phones
Answering machines
External parties
Privacy
and
obfuscation
rules
Interaction
Context DB
  Log / History
  User preferences
  Proactive rules
USERS
Learner
Learning and
reasoning
Figure 1.2: A context-aware data management supporting platform.
Table 1.1: Characteristics of context tackled by different components.
Characteristic
Context
Provider
Service
Registry
Rules
Learner
Context
Database
Being sensed
X
Through constrained devices
X
From distributed sources
X
X
Continuous change
X
X
Mobility
X
X
Temporality and Spatiality
X
X
X
Imperfectness and uncertainty
X
X
X
Context processors access context information, provided by context providers, by do-
ing a request to the registry. The later ensures that only appropriate context processors
can have access to certain context information. For example, the body heat of a person
returned from a body sensor can only be accessed by his/her delegated context pro-
cessors. In this way, privacy and security can be accommodated. Another advantage
of having a service registry is that it is possible, to dynamically (de-)register services.
By doing this, dynamic connections resulting from mobility and the continuous change
of context information can be supported. Finally, the service registry can incorporate
conversion services which can deal with alternative representations using metadata.
Authenticated
:23 PM

1.6 Supporting Context-Aware Data Management
19
Table 1.2: Implications of context-awareness tackled by different components.
Implication
Context
Provider
Service
Registry
Rules
Learner
Context
Database
Adaptiveness and personalization
X
Privacy and security
X
X
X
X
Proactiveness
X
X
Traceability
X
Dynamic connection
X
X
Interrelationship
X
Learning and reasoning
X
Alternative representations
X
Meta data
X
X
X
X
Storage and logging
X
Here, techniques developed in the ﬁeld of Web services can be applied [24]. For in-
stance, with Jini, a Java-based connection technology, various Jini-compatible devices
can form a dynamic network and interact with each other. This requires relatively
much processing power which, together with the fact that we are dealing with small
constrained devices, leads among others to the question of how much processing
power and intelligence shall be injected into context providers.
1.6.3 Context Processor
The context processor stores and logs some of past, present, and future context in-
formation related to a user, environments, and corresponding past actions of the user
in a context database. For the sake of privacy and security, data in this database is
protected using encryption and access rights.
There are two reasons for having this context database available. First, to deal
with dynamic connections, caching some context information at the context pro-
cessor’s side can ensure the consistent providing of user related information, even
when a user is not connected to the network.
More importantly, from these logged context information and related actions, the
context processor can learn and reason about user’s preferences and behavior pat-
terns which will lead to proactively generated rules by the system. Because of the
nature of context, learning and reasoning techniques for metadata, particularly with
uncertainty, in the discourse of time and space, appear to be more important than
in noncontext-aware computing systems. For instance, according to a user’s context,
his/her next possible action can be inferred. In this way, context consumers can adapt
to a user based on these rules, by which the system becomes personalized. As an ex-
ample, consider a person who each time when s/he enters a room, turns on the light.
After several times, the proactive rules can be learned by the learner and then stored in
Authenticated
:23 PM

20
1 Context-Aware Data Management Toward Ambient Intelligence
the context database. With this rule, the context-aware light button will automatically
turn on the light, whenever this user enters the room.
For the context processor, it is important to structure the preferences and rules in
a way which is clear to the user, so that s/he can view and edit them. Furthermore, the
rules need to take into account metadata, especially accuracy, among others for giving
the user the possibility to specify a minimum accuracy level for triggering a rule. Since
the behavior of the consumers is completely based on the rules and preferences stored
in the database, the proactiveness in this way becomes traceable.
Note that, in supplying context consumers with these actionable context know-
ledge, the context processor will invoke a set of privacy and obfuscation rules to avoid
the misuse of context and context-awareness.
1.6.4 Context Consumer
Context consumers can be either context-aware data management systems or external
parties like restaurants, museums, machines, etc., since all of them make use of con-
text during functioning. database management systems (DBMS) are positioned as
context consumers.
A context consumer example could be a context-aware multimedia database sys-
tem, which stores all videos and scenes one watched before. When the person poses
to this database a query like “which scene of The Bourne Identity did I watch yes-
terday before going to the supermarket?”, the system could (if allowed) consult this
user’s delegated context processor about the time the user went to the supermarket
and based on this, present the right scene to the user.
Subsection gives a series of context-aware database query examples to be ex-
ecuted by a context-aware database management system, which is also a context
consumer.
1.7 Recapitulation
One key requirement for computer systems to be Ambient Intelligent is to be context-
aware. This chapter addressed the impact of context-awareness on data management,
from context, context-awareness, to context-aware data management platform. Hav-
ing followed the approach of identifying the characteristics of context in a practical
manner in combination with a functional data management architecture, it is possible
to say where to tackle which problems and to what extent existing techniques can be
used. We can thereby, on one hand, work on context processors, particularly focusing
on a context modeling technique to express and store context information, with sup-
port for metadata and preference rules. The latter is generated proactively and deﬁned
by users. On the other hand, we can focus on context providers and service registry,
where existing platforms for providing context information to the context processor
Authenticated
:23 PM

Literature
21
can be employed. Desired techniques in delivering context-aware data management
solutions will be addressed in the following chapters.
Literature
[1]
E. Aarts and S. Marzano (eds.). The New Everyday: Visions of Ambient Intelligence, Rotterdam,
The Netherlands: 010 Publishing, 2003.
[2]
G. D. Abowd, A. Dey, R. Orr, and J. Brotherton. Context-awareness in wearable and ubiquitous
computing. Virtual Reality, 3:200–211, 1998.
[3]
G. D. Abowd and E. D. Mynatt. Charting past, present, and future research in ubiquitous
computing. ACM Transactions on Human-Computer Interaction, 7(1):29–58, 2000.
[4]
J. Ahola. Ambient intelligence: Plenty of challenges by 2010. In Proc. of EDBT, Czech Republic:
Prague, page 14, March 2002.
[5]
S. Antifakos, A. Schwaninger, and B. Schiele. Evaluating the effects of displaying uncertainty in
context-aware applications, Proceedings of UbiComp, N. Davies, E. Mynatt, and I. Siio (eds.),
Springer-Verlag Heidelberg, pages 54–69, 2004.
[6]
L. W. Barsalou. The content and organization of autobiographical memories, Remembering
reconsidered: Ecological and traditional approaches to the study of memory, U. Neisser and E.
Winograd (eds.), Cambridge University Press, Cambridge, pages 193–243, 1988.
[7]
P. Bonnet, J. Gehrke, and P. Seshadri. Towards sensor database systems. In Proc. of MDM,
pages 3–14, 2001.
[8]
S. Bressan, K. Fynn, C. H. Goh, S. E. Madnick, T. Pena, and M. D. Siegel. Overview of a prolog
implementation of the context interchange mediator. In Proc. of the Intl. Conf. and Exhibition on
the Practical Applications of Prolog, pages 83–93, 1997.
[9]
M. Chalmers. A historical view of context. Journal of Collaborative Computing, 13:223–247,
2004.
[10]
G. Chen and D. Kotz. A survey of context-aware mobile computing research. Technical report
TR2000-381, Dept. of Computer Science, Dartmouth College, 2000.
[11]
H. Chen, T. Finin, and A. Joshi. Semantic web in a pervasive context-aware architecture, In Proc.
of AIMS, Seattle, USA, pages 33–40, 2003.
[12]
M. Cherniack, M. J. Franklin, and S. B. Zdonik. Data management for pervasive computing.
Tutorial at VLDB, Rome, Italy, pages 71–140, 2001.
[13]
European Commission. Scenarios for ambient intelligence in 2010. http:// www.cordis.lu/ist/
istag.htm, 2001.
[14]
M. Dertouzos. The future of computing. Scientiﬁc American, 281(2):52–55, 1999.
[15]
A. Deshpande, C. Guestrin, S. R. Madden, J. M. Hellerstein, and W. Hong. Model-driven data
acquisition in sensor networks. In Proc. of VLDB, pages 588–599, 2004.
[16]
R. W. DeVaul and A. Pentland. The ektara architecture: The right framework for context-aware
wearable and ubiquitous computing applications, MIT Technical Report, USA, 2000.
[17]
A. K. Dey. Understanding and using context. Personal Ubiquitous Computing, 5(1):4–7, 2001.
[18]
A. K. Dey and G. D. Abowd. Towards a better understanding of context and context-awareness.
Technical report GIT-GVU-99-22, Georgia Institute of Technology, 1999.
[19]
P. Dourish. What we talk about when we talk about context. Personal and Ubiquitous
Computing, 8(1):19–30, 2004.
[20] M. Dunham and V. Kumar. Location dependent data and its management in mobile databases.
In Proc. of DEXA, Vienna, Austria, pages 414–419, August 1998.
[21]
M. Eldridge, P. Barnard, and D. Bekerian. Autobiographical memory and daily schemas at work.
Memory, 2(1):51–74, March 1994.
Authenticated
:23 PM

22
1 Context-Aware Data Management Toward Ambient Intelligence
[22] M. Eldridge, M. Lamming, and M. Flynn. Does a video diary help recall, People and Computers
VII, A. Monk, D. Diaper, and M. D. Harrison (eds.), Cambridge University Press, Cambridge, UK,
pages 257–269, 1992.
[23] L. Feng, P. M. G. Apers, and W. Jonker. Towards context-aware data management for ambient
intelligence. In Proc. of DEXA, pages 422–431, 2004.
[24] F. L. Gandon and N. M. Sadeh. Semantic web technologies to reconcile privacy and context
awareness. Web Semantics Journal, 1(3):241–260, 2004.
[25]
K. Goslar and A. Schill. Modeling contextual information using active data structures. In Proc. of
the Intl. Workshop for Pervasive Information Management, pages 325–334, 2004.
[26] P. D. Gray and D. Salber. Modelling and using sensed context information in the design of
interactive applications. In Proc. of the 8th IFIP Intl. Conf. on Engineering for Human-Computer
Interaction, Springer–Verlag, pages 317–335, 2001.
[27]
T. Gu, H. K. Pung, and D. Q. Zhang. A Bayesian approach for dealing with uncertain contexts. In
Proc. of Pervasive Computing, 2004.
[28] A. Harter, A. Hopper, P. Steggles, A. Ward, and P. Webster. The anatomy of a context-aware
application. In Proc. of Mobicom, ACM Press, pages 59–68, 1999.
[29] K. Henricksen and J. Indulska. Modelling and using imperfect context information. In Proc. of
the Intl. Workshop on Context Modelling and Reasoning (CoMoRea’04), IEEE Computer Society,
pages 33–37, 2004.
[30] K. Henricksen, J. Indulska, and A. Rakotonirainy. Modeling context information in pervasive
computing systems. In Proc. of Pervasive Computing, pages 167–180, 2002.
[31]
J. Hightower and G. Borriello. Particle ﬁlters for location estimation in ubiquitous computing: A
case study. In N. Davies, E. Mynatt, and I. Siio, editors, Proc. of UbiComp, Heidelberg:
Springer–Verlag, pages 88–106, 2004.
[32] T. Imielinski and B. Badrinath. Querying in highly mobile distributed environments. In Proc. of
VLDB, Vancouver, CA, pages 41–52, 1992.
[33] G. J. F. Jones and P. J. Brown. Context-aware retrieval for ubiquitous computing environments. In
Proc. of the Mobile HCI Workshop on Mobile and Ubiquitous Information Access, Springer,
pages 227–243, 2004.
[34] W. Jonker. XML and secure data management in an ambient world. Computer Systems Science
and Engineering, 18(5):311–317, September 2003.
[35]
T. Kindberg, A. Sellen, and E. Geelhoed. Security and trust in mobile interactions: A study of
users perceptions and reasoning, N. Davies, E. Mynatt, and I. Siio (eds.), Proc. of UbiComp,
Heidelberg: Springer–Verlag, pages 196–213, 2004.
[36] K. Koile, K. Tollmar, D. Demirdjian, H. Shrobe, and T. Darell. Activity zones for context-aware
computing. In Proc. of Ubicomp, pages 90–106, 2003.
[37]
P. Korpipaa, M. Koskinen, J. Peltola, S. Makela, and T. Seppanen. Bayesian approach to
sensor-based context awareness. Personal Ubiquitous Computing, 7(2):113–124, 2003.
[38] V. Kumar and M. Dunham. Deﬁning location data dependency, transaction mobility and
commitment. Technical Report 98-CSE-01, Southern Methodist University, USA, 1998.
[39] M. Lamming, P. Brown, K. Carter, M. Eldridge, M. Flynn, G. Louie, P. Robinson, and A. Sellen. The
design of a human memory prosthesis. The Computer, 37(3):153–163, 1994.
[40] M. Lamming and M. Flynn. Forget-Me-Not: Intimate computing in support of human memory. In
Proc. of the Intl. Symposium on next Generation Human Interface, pages 125–128, Japan, 1994.
[41]
I. Lazaridis, Q. Han, X. Yu, S. Mehrotra, N. Venkatasubramanian, D. V. Kalashnikov, and W. Yang.
Quasar: Quality aware sensing architecture. ACM SIGMOD Record, 33(1):26–5, 2004.
[42] U. Leonhardt and J. Magee. Security considerations for a distributed location service. Journal of
Network and Systems Management, 6(1):51–70, 1998.
[43] A. Y. Levy. Logic-based techniques in data integration, Logic-based artiﬁcial intelligence. Kluwer
Academic Publishers, MA, USA, pages 575–595, 2000.
Authenticated
:23 PM

Literature
23
[44] H. A. Lieberman and T. Selker. Out of context: computer systems that adapt to, and learn from,
context. IBM Systems Journal, 39(3–4):617–632, 2000.
[45]
E. Liikanen. Ambient intelligence in future EU research efforts. News,
http://www.ercim.org/publication/Ercim_News/enw47/ keynote.html, October 2003.
[46] S. R. Madden, W. Hong, J. M. Hellerstein, and M. J. Franklin. Tinydb web page.
http://telegraph.cs.berkeley.edu/tinydb/, 2004.
[47] B. Meyers and A. Kern. <context-aware> schema </context-aware>. In Proc. of the CHI Workshop
on The What, Who, When, Where, Why, and How of Context-Awareness, 2000.
[48] W. M. Newman, M. Eldridge, and M. Lamming. Pepys: Generating autobiographies by automatic
tracking. In Proc. of the 2nd European Conf. on Computer-Supported Cooperative Work, pages
175–188, 1991.
[49] T. S. Raghu, P. K. Kannan, H. R. Rao, and A. B. Whinston. Dynamic proﬁling of consumers for
customized offerings over the internet: A model and analysis. Decision Support Systems,
32(2):117–134, 2001.
[50] A. Ranganathan, J. Al-Muhtadi, and R. H. Campbell. Reasoning about uncertain contexts in
pervasive computing environments. IEEE Pervasive Computing, 3(2):62–70, 2004.
[51]
Philips Research. What is ambient intelligence. http://www.research. philips.com/, 2002.
[52]
B. J. Rhodes and T. Starner. Remembrance agent – a continuously running automated
information retrieval system. In Proc. of the 1st Intl. Conf. on the Practical Application of
Intelligent Agents and Multi Agent Technology, pages 487–495, 1996.
[53]
M. Satyanarayanan. Accessing information on demand at any location: Mobile information
access. IEEE Personal Communications, 3(1):26–33, 1996.
[54] M. Satyanarayanan. Pervasive computing, vision and challenges. IEEE Personal
Communications, 6(8):11–17, August 2001.
[55]
M. Satyanarayanan. Pervasive computing: Vision and challenges. IEEE Personal
Communications, 8:10–7, 2001.
[56] K. Saywitz, G. Bornstein, and E. Geiselman. Effects of cognitive interviewing and practice on
children’s recall performance. Applied Psychology, 77(5):3–15, 1992.
[57]
A. Schmidt. There is more to context than location. Computers and Graphics Journal,
23(6):893–901, 1999.
[58] A. Seydim, M. Dunham, and V. Kumar. An architecture for location dependent query processing.
In Proc. of DEXA, Muich, Germany, pages 549–555, September 2001.
[59] A. Seydim, M. Dunham, and V. Kumar. Location dependent query processing. In Proc. of the 2nd
ACM Intl. Workshop on Data Engineering for Wireless and Mobile Access (MobiDE01), Canada,
USA, pages 47–53, May 2001.
[60] T. Strang. A context modeling survey. In Proc. of UbiComp Workshop on Advanced Context
Modelling, Reasoning and Management, 2004.
[61]
D. Tennenhouse. Proactive computing. Communications of the ACM, 43(5):43–50, 2000.
[62] H. ter Horst, M. van Doorn, N. Kravtsova, W. ten Kate, and D. Siahaan. Context-aware music
selection using knowledge on the semantic web. In Proc. of the 14th Fourteenth
Belgium-Netherlands Conference on Artiﬁcial Intelligence, pages 131–138, 2002.
[63] E. Tulving. Elements of Episodic Memory. Oxford University Press, 1983.
[64] M. Weiser. The computer for the 21st century. Scientiﬁc American, 165(3):94–104, 1991.
[65] J. Xu and D. Lee. Querying location-dependent data in wireless cellular environments. In Proc. of
the WAP Forum/W3C Workshop on Position Dependent Information Services, France: Sophia
Antipolis, February 2000.
[66] B. Zheng and D. Lee. Processing location-dependent queries in a multi-cell wireless
environments. In Proc. of the 2nd ACM Intl. Workshop on Data Engineering for Wireless and
Mobile Access (MobiDE01), Canada, USA, pages 54–6, May 2001.
Authenticated
:23 PM

Authenticated
:23 PM

2 Modeling Context
Abstract: Modeling context is the ﬁrst step in building context-aware computing sys-
tems and applications. It determines the organization and access manner of context
information in context-aware applications. This chapter lists six typical context rep-
resentation methods. An ontological context representation language OWL with its
Description Logic foundation is particularly described. To tackle context dynamics,
context can further be interpreted as context events, whose temporal properties and
relationships can be captured and expressed through temporal operators.
2.1 Six Context Representation Methods
Six typical context representation methods exist in academia and industry. They are
key-value, entity-relationship, object-orientation, markup-schema, logics, and
ontology. Each has strength and weakness in handling context reasoning, inform-
ation distribution, validation, quality assurance, formalization, applicability, and
suitable users.
2.1.1 Key-Value
Key-value is the most simple context representation method. It uses a key-value
data structure to describe different context elements and their values. For instance,
Location:campus describes that context element Location takes the value cam-
pus. A recursion description like Address:(Building:FIT, Room:216) is also
possible.
Due to the simply and easy support to context data management and pattern-
matching queries, the key-value representation method is widely adopted by early
context-aware systems [20, 21]. However, it is not powerful to capture rich semantics
and relationships of context elements. It is also weak in coping with distributed con-
text information with associated properties, such as incompleteness, fuzziness, and
veriﬁcation.
2.1.2 Entity-Relationship
To enrich the simple key-value representation, the entity-relationship method is de-
veloped. It structures context information using a set of entities, each describing
a physical or conceptual object [12–14]. Properties of entities are denoted as attrib-
utes. An entity can be linked to its attributes or other entities via uni-directional
relationships called associations. Associations could be viewed as assertions about
the associated entities. A context description is just a set of such assertions.
DOI 10.1515/9783110556674-002
Authenticated
:23 PM

28
2 Modeling Context
2.1.3 Object-Orientation
Beyond entity-relationship, the object-oriented modeling method is also adopted to
achieve encapsulation and re-use of context information. For instance, the concept
cue in Ref. [22] behaves like a class container. It abstracts and encapsulates one or
more physical or logical sensors. When new sensors of different characteristics are
present, only changes in the involved cues are made.
As the object-oriented representation method deﬁnes classes, objects, types, and
instances for context information in an abstract way, it enables easy integration and
fusion of distributed context sources. It can also make changes of the hardware easy
and transparent to the upper context-aware applications. Such general-purpose mod-
eling languages and tools as ORM (Object-Role Modeling Language) and UML (Uniﬁed
Modeling Language) can be used to describe context information.
2.1.4 Markup Schema
A markup schema can be deﬁned to describe context information in a level-wise
manner. It usually contains a set of attributes and contents, each associated with a
markup tag to convey the corresponding semantic meaning. The modeling language
used is generally XML (eXtensible Markup Language). Two typical applications of the
markup-schematic representation method are UAProf (User Agent Proﬁle) [28] and
CC/PP (Composite Capabilities/Preferences Proﬁle) [26].
The markup-schematic modeling method can tackle the partial veriﬁcation and
formalism requirements, since scheme deﬁnition of context information is highly
formalized. There also exist some veriﬁcation tools for type checking.
2.1.5 Logics
To facilitate context reasoning, logic is incorporated in context modeling, where con-
text information is usually deﬁned in terms of fact, expression, and rule. From basic
facts and expressions, derived facts and expressions can be logically inferred. For in-
stance, the sensed context information in Ref. [11] was represented in the ﬁrst-order
predicate logic, and is further composed into more complex sensed context expres-
sions with meta-propositional properties, such as the quality of context information
or the operational parameters of the underlying sensors, etc.
The logic-based context representation approach is good at formalization and
reasoning, but weak in partial veriﬁcation and applicability.
2.1.6 Ontology
Many context-aware applications model context information using various domain-
speciﬁc ontologies. The term ontology origins from its usage in philosophy, where it
Authenticated
:23 PM

2.1 Six Context Representation Methods
29
means the study of being or existence as well as the basic categories [25]. An onto-
logy in the area of computer science represents the effort to formulate an exhaustive
and rigorous conceptual schema within a given domain, typically a hierarchical data
structure containing all the relevant elements and their relationships, as well as the
rules and regulations within the domain [10].
As ontology deﬁnes a common vocabulary for information sharing in a speciﬁc do-
main, it has many advantages in constructing context-aware systems. First, ontology
is suitable for representing personalized information, because it provides an adequate
ground for the representation of coarse to ﬁne-grained user interests in a hierarchical
way. It can act as a key enabler to deal with user preferences. Second, it offers the
possibility to explore the potential capability of context reasoning based on Semantic
Web technologies.
COBRA-ONT [10] was an example ontology for context-aware systems. It uses OWL
(Web Ontology Language) to describe places, agents, events, and associated prop-
erties in an intelligent meeting room domain. Ref. [16] presented an ontology for
sensor-based context information, using RDF (Resource Description Framework) as
the description syntax. With RDF, context information sharing and communication
among collaborative sensory devices become easy.
So far, ontology-based languages are preferable for context representation due
to their powerful capabilities of OWL and RDF in semantics, formality, distributed
knowledge composition, and partial knowledge validation [24].
2.1.7 Methods Comparison
Table 2.1 outlines the capabilities of the six context representation methods in ad-
dressing a variety of context-related requirements, including reasoning, distributed
Table 2.1: Comparisons of six context representation approaches.
Key-value
Entity-
relationship
Object-
orientation
Markup-
schema
Logics
Ontology
Reasoning
poor
poor
poor
poor
good
good
Distribution
heterogeneity
poor
poor
good
good
good
good
Validation
poor
poor
good
good
good
good
Quality
assurance
poor
good
good
poor
poor
good
Formalization
poor
good
good
good
good
good
Applicability
good
good
good
good
poor
good
Suitable users
system-
developer
end-user
system-
developer
end-user
system-
developer
system-
developer
system-
developer
system-
developer
end-user
Authenticated
:23 PM

30
2 Modeling Context
heterogeneity and knowledge composition, validation, quality assurance, formaliza-
tion, and applicability of context information, as well as suitable users. The choice of
an appropriate context model is subject to the requirements of speciﬁc context-aware
applications.
Among the six approaches, ontology is the most powerful one. Many context-
aware application developers turn to the ontology-based solution [24, 25], since it
can offer a shared conceptualization of the domain, ensuring a common ground
and a critical semantic foundation for distributed collaboration and interaction in
context-aware applications. If context information is mapped onto an ontology, co-
herent communication among multiple parties involved in context-aware applications
is supported, and understanding the meaning of the shared context information can
substantially be enhanced.
2.2 Context Ontology
While ontology sets up a conceptual framework to enforce an agreement on how con-
text is described, it does not lose any of the ﬂexibility of allowing upper application
developers and users to express and view parts in their own familiar expression lan-
guages. A mainstream ontology language is OWL, endorsed by W3C (World Wide Web
Consortium). This section describes the formalism of context ontology, OWL, and the
logical foundation of OWL.
2.2.1 Context Formalism
Deﬁnitions in an ontology associate the names of concepts in a speciﬁc domain with a
description of what the concepts mean, as well as some formal axioms that constrain
the interpretation and well-formed use of these concepts. In line with the formal deﬁn-
ition of ontology in Ref. [9], an ontology consists of a static structure, an L -axiom set,
and the instantiation of the static structure.
Deﬁnition 2. The structure of an ontology CO is a 5-tuple COstruct = (C, ≤C, R, ≤R, 3),
where
–
C is a set of concepts;
–
R is a set of relations;
–
≤C is a partial order on C, forming hierarchies of concepts;
–
≤R is a partial order on R, forming hierarchies of relations;
–
3 is a 3 : R →C+, binding concepts to relations with the following prop-
erty. Assume 3(r) = (c1, ⋅⋅⋅, cm) and 3(r󸀠) = (c󸀠
1, ⋅⋅⋅, c󸀠
n), where r, r󸀠∈R and
c1, ⋅⋅⋅, cm, c󸀠
1, ⋅⋅⋅, c󸀠
n ∈C. If (r ≤R r󸀠), then (m = n) and ∀(1 ≤i ≤m) (ci ≤C c󸀠
i).
◻
Authenticated
:23 PM

2.2 Context Ontology
31
Deﬁnition 3. For a relation r ∈R binding two concepts c1, c2 ∈C, 3(r) = (c1, c2), c1 is
called the domain of r and c2 is called the range of r.
◻
The relationships between two concepts and the relationships between two relations
in the ontology have the following deﬁnitions.
Deﬁnition 4. Let C be a set of concepts, and R be a set of relations.
–
For ∀c1, c2 ∈C, if (c1 ≤C c2), then c1 is called a subconcept of c2, and c2 is called a
superconcept of c1.
–
For ∀r1, r2 ∈R, if (r1 ≤R r2), then r1 is called a subrelation of r2, and r2 is called a
superrelation of r1.
◻
Deﬁnition 5. Let L be a logical language. An L-axiom set for the ontology CO is a set
of L -axioms in the logical language L , stating true assertions and general knowledge
about the static structure of the context ontology CO.
◻
Deﬁnition 6. Let I be a set of instances. The instantiation of the ontology CO is a pair
COins = (IC, IR), where
–
IC is a function IC : C →IA, mapping concepts to a set of concept instances;
–
IR is a function IR : R →IA, mapping relations to a set of relation instances.
◻
Deﬁnition 7. An ontology is a triple CO = (COstruct, COaxiom, COins), where COstruct is the
structure of CO, COaxiom is an L -axiom set for CO, and COins is the instantiation of CO’s
structure COstruct.
◻
2.2.2 Ontology Language – OWL
The OWL, endorsed by the W3C, attracts great academic and commercial interests
at the implementation level of context ontology. It constitutes a family of knowledge
representation languages for authoring ontologies [27]. The OWL family languages are
characterized by formal semantics and RDF/XML-based serializations.
2.2.2.1 Variants of OWL
OWL Lite, OWL DL, and OWL Full are three variants of OWL in the order of expressive
capability. Each of them is a syntactic extension of its simpler predecessor [30].
OWL Lite supports classiﬁcation hierarchy and simple constraints. It permits car-
dinality values of 0 or 1 in terms of cardinality constraint. Development of OWL Lite
tools has proven almost as difﬁcult as development of tools for OWL DL. OWL Lite is
not widely used today.
Authenticated
:23 PM

32
2 Modeling Context
OWL DL takes Description Logic as its formal foundation. It provides the
maximum expressiveness while retaining computational completeness (either 8 or
¬8), decidability (there is an effective procedure to determine whether 8 is derivable
or not), and the availability of practical reasoning algorithms.
OWL Full is based on different semantics from OWL Lite or OWL DL. It preserves
some compatibility with RDF Schema, allowing an ontology to augment the meaning
of the pre-deﬁned (RDF or OWL) vocabulary. It is unlikely that any reasoning software
will be able to support complete reasoning for OWL Full [18].
2.2.2.2 Main Constructors of OWL
Languages in the W3C OWL family are capable of creating classes, properties, deﬁning
instances, and its operations [27].
Instance. An instance is an object.
Class. A class is a collection of objects. A class may contain individuals, instances
of the class. A class may have any number of instances. An instance may belong to
none, one, or more classes. Operators Languages in the OWL family support various
operations on classes such as union, intersection, and complement. They also allow
class enumeration, cardinality, and disjointness.
Besides, a class may be a subclass of another, inheriting characteristics from its
superclass superclass. All classes are subclasses of the root class owl:Thing. The
empty class owl:Nothing is the subclasses of all the rest classes. No instances are
members of owl:Nothing.
owl:Thing and owl:Nothing can be used to assert
facts about all or no instances.
Property. A property is a directed binary relation that speciﬁes class character-
istics. It can be an attribute of instances and sometimes acts as a data value or link to
other instance(s). Properties may possess logical capabilities such as being transitive,
symmetric, inverse, and functional. Properties may also have domains and ranges.
2.2.3 Logic Foundation of OWL – Description Logic (DL)
Description Logic (DL) is a family of formal knowledge representation languages [6],
which are (most of the time decidable) fragments of ﬁrst-order logic, and are closely
related to Propositional Modal & Dynamic Logics. It is the descendant of semantic
networks and KL-ONE, and is used in artiﬁcial intelligence for formal reasoning on
the concepts of an application domain (known as terminological knowledge).
DL is of particular importance in providing a formal and computational founda-
tion for ontological languages such as the W3C standard OWL. In fact, many current
OWL inference tools do reasoning by ﬁrst translating the OWL ontology into a DL
knowledge base, and rely on the latter to perform logic inferences. Correspondence
exists between OWL class and axiom constructors and DL syntax.
Authenticated
:23 PM

2.2 Context Ontology
33
2.2.3.1 Description Logic in General
Description Logic describes a domain in terms of concepts (classes), roles (relation-
ships), and individuals. A DL knowledge base is comprised of two parts, i.e., a TBox
(schema) and an ABox (data) [18].
TBox introduces the terminology (i.e., the vocabulary of a domain). It is com-
prised of concepts (e.g., Person, Employee, Role, Company) gathering sets of
individuals of the common characters, and roles (e.g., hasActivity, hasRole)
representing binary relationships between individuals.
ABox contains assertions about named individuals (e.g., Person(Peter) ,
Role(engineer) , hasRole(Peter, engineer)) in the TBox vocabulary.
2.2.3.2 Description Logic ALCF
There is a variety of Description Logics with different constructors and operators. The
base description logic language of practical interest is Attribute Language (AL), which
is the minimal description logic language [23].
Concept. AL allows atomic negation, concept intersection, universal restrictions,
and limited existential quantiﬁcation. Complementing AL with any concept (not just
atomic concepts) allowed, ALC constitutes a centrally important description logic. The
top concept ⊤and bottom concept ⊥denote the concept with all or no individuals,
respectively.
Role. Roles can have full quantiﬁcation restriction ∀or existential quantiﬁc-
ation restriction ∃. For instance, ∀hasRole.Role denotes all individuals having
roles, and ∃hasRole.Role denotes some individuals having roles. Number restric-
tion (≥nR and ≤nR) and min/max cardinality constraint (≥nR.C and ≤nR.C) can be
enforced for rich expressivity [2, 23].
In ALC, concepts and roles can be either atomic or constructed using operators of
intersection ⊓, union ⊔, or negation ¬ (e.g., Person ⊓Employee). The inverse role
of R is represented as R– (e.g., hasEmployee is the inverse role of IsEmployeeOf–).
Concept Expression. A concept expression can contain a set of concepts and
(quantiﬁed) roles, which are connected via ⊓, ⊔, or ¬ (e.g., Employee ⊓∃hasRole.
{manager}).
Feature. ALCF introduces the notion of feature (similar to attribute), which can
be viewed as a function between two individuals [15]. For example, hasID can be
viewed as a feature of concept Person, but hasFriend can only be a role of Person,
since a Person instance can have many Friend instances. Similar to concept and
role, feature can be either atomic or composite built through a chain of features, called
feature chain. For example, feature chain (hasPartener ∘hasID) of Person de-
notes the Partener’s ID of a Person instance. Let f and f 󸀠be two feature chains
of a concept. f ↓f 󸀠shows feature agreement (for each individual of the concept,
its feature chains f and f 󸀠have the same value). f ↑f 󸀠shows feature disagreement
Authenticated
:23 PM

34
2 Modeling Context
(for each individual of the concept, its feature chains f and f 󸀠have different values).
f
↑shows an undeﬁned feature (individuals of the concept don’t have a
value for f).
2.2.3.3 Description Logic with Concrete Domains ALCF(D)
Gray and Salber [11], Hollunder and Nutt [15], and Schmidt-Schaub" and Smolka [23]
incorporate concrete domain D into ALCF, leading to the description logic language
with concrete domains ALCF(D). The concrete domain D in ALCF(D) is comprised of a
set dom(D) (domain of D) and a set pred(D) (predicates of D). Each predicate name P
is associated with an arity n and an n-ary predicate PD ⊆dom(D)n.
A concrete domain D is admissible, if and only if (1) the set of its predicate names
is closed under negation and contains a name for BD, and (2) the satisﬁability problem
for ﬁnite conjunctions of the above mentioned form is decidable. Formal deﬁnitions
of the syntax and semantics of ALCF(D) can be found in Refs. [11, 15, 23].
Deﬁnition 8. Let Concept, Role, and Feature be disjoint sets of concept, role, and feature
names. Let C, D ∈Concept be two concept names, R ∈Role ∪Feature be a role or feature
name, Pred ∈pred(D) be an n-ary predicate name, and u1, u2, ⋅⋅⋅, un be feature chains.
The following expressions are also concepts.
–
C ⊔D (disjunction), C ⊓D (conjunction), and ¬C (negation);
–
∃R.C (existential quantiﬁcation) and ∀R.C (full quantiﬁcation);
–
Pred(u1, u2, ⋅⋅⋅, un) (predicate);
–
u1 ↓u2 (feature agreement) and u1 ↑u2 (feature disagreement).
◻
Deﬁnition 9. The semantics of ALCF(D) can be deﬁned via an interpretation I =
(dom(I), .I), where
–
dom(I) is a set representing the abstract domain of the interpretation. The abstract
domain and the concrete domain are disjoint, i.e., dom(I) ∩dom(D) = 6.
–
.I is an interpretation function, which
–
maps each concept name C ∈Concept to a subset CI of dom(I).
–
maps each role name R ∈Role to a subset RI of dom(I) × dom(I),
–
maps each feature name f ∈Feature to a partial function f I from dom(I) to
dom(I) ∪dom(D), where f I(x) = y is written as (x, y) ∈f I.
If u = f1 f2 ⋅⋅⋅fn is a feature chain, then uI is deﬁned as the composition f I
1 ∘f I
2 ∘
⋅⋅⋅∘f I
k of the partial functions f I
1 , f I
2, ⋅⋅⋅, f I
k.
Let symbols C, D, R, P and u1, u2, ⋅⋅⋅, un be deﬁned as in Deﬁnition 8. The interpreta-
tion function can be extended to complex concepts as follows.
–
(C ⊔D)I = CI ∪DI, (C ⊓D)I = CI ∩DI, (¬C)I = dom(I) \ CI;
Authenticated
:23 PM

2.2 Context Ontology
35
–
(∃R.C)I = {x ∈dom(I) | ∃y : (x, y) ∈RI ∧y ∈CI},
(∀R.C)I = {x ∈dom(I) | ∀y : (x, y) ∈RI →y ∈CI};
–
Pred(u1, u2, ⋅⋅⋅, un)I = {x ∈dom(I) | ∃r1, ⋅⋅⋅, rn ∈dom(D) :
uI
1(x) = r1, ⋅⋅⋅, uI
n(x) = rn ∧(r1, ⋅⋅⋅, rn) ∈PD};
–
(u1 ↓u2)I = {a ∈dom(I) | ∃b ∈dom(I) : (a, b) ∈uI
1 ∧(a, b) ∈uI
2},
(u1 ↑u2)I = {a ∈dom(I) | ∃b1, b2 ∈dom(I) : (b1
̸= b2) ∧(a, b1) ∈uI
1 ∧
(a, b2) ∈uI
2}.
Let T be a TBox. An interpretation I is a model of T , if and only if it satisﬁes AI = BI
for all terminological terms A = B in the T .
◻
2.2.3.4 Logic Inference in ALCF(D)
A DL-based system not only stores terminologies and assertions but also offers ser-
vices that reason about them. DL do two types of reasoning: one for TBox and the
other for ABox.
TBox Reasoning. The main deduction service for a TBox is to test whether a
generic relationship is a logical consequence of the declarations in the TBox. Sub-
sumption is a key reasoning problem, which computes the subconcept-superconcept
relationships between the concepts in the TBox. Let T be a TBox, and let C, D be two
concept names. D subsumes C with respect to T (written as C ⊑D), if and only if
CI ⊆DI holds for all models I of T .
Other important reasoning problems considered include satisﬁability, equival-
ence, and disjointness. Satisﬁability problem is to check whether in some interpret-
ation a concept denotes a nonempty set (i.e., noncontradictory). Equivalence problem
is to check whether two concepts are equivalent. Disjointness problem is to check
whether the disjoint result of two concepts is an empty set.
In the AL language, the satisﬁability, equivalence, and disjointness reasoning
problems can be reduced to the subsumption problem. Furthermore, if the DL sys-
tem allows the full negative constructor, these problems can be reduced to the
unsatisﬁability problem, and have the same reasoning complexity.
ABox Reasoning. The basic reasoning problem for the ABox is consistency check-
ing, i.e., checking whether there is an ABox that is consistent to the TBox. As the
ABox contains assertions about individuals, its basic instance checking task is to test
whether a given individual is an instance of a speciﬁed concept. Instance checking
can be reduced to the consistency problem. Shaerf [19] shows that ABox consistency
problem can be reduced to concept satisﬁability.
2.2.4 Correspondence Between OWL and DL
Correspondence exists between W3C OWL and Description Logics. An instance in
OWL corresponds to a DL individual. A class in OWL corresponds to a DL concept.
Authenticated
:23 PM

36
2 Modeling Context
Table 2.2: OWL class constructors in Description Logics.
Class constructor in OWL
Expression in DL
Example
intersectionOf
C ⊔D
Person ∪Employee
unionOf
C ⊓D
Person ∩Employee
complementOf
¬C
¬ Employee
one of
{x1 ⋅⋅⋅xn}
{Jone, Patrick, ⋅⋅⋅, Bill}
toClass
∀R.C
∀hasRole.Role
hasClass
∃R.C
∃hasRole.Role
hasValue
∃R.{x}
∃hasActivity.{repair}
minCardinalityQ
≥nR.C
≥10 hasRole.Role
maxCardinalityQ
≤nR.C
≤1 hasRole.Role
cardinalityQ
= R.C
= 1 hasRole.Role
Table 2.3: OWL axiom constructors in Description Logics.
Axiom constructor in OWL
Expression in DL
Example
subClassOf
C ⊑D
Person ⊑Employee
sameClassAs
C ≐D
Employee ≐Staff
subPropertyOf
R ⊑S
hasEmployee ⊑
hasContactPerson
sameIndividualOf
x1 ≐x2
Jone = J.Smith
disjointWith
C ⊑¬D
Employee ⊑¬ Customer
differentIndividualFrom
{x1} ⊑¬{x2}
{Jone} ⊑¬ {Patrick}
inverseOf
R ≐S–
hasEmployee ≐IsEmployeeOf–
The subclass-superclass relationship in OWL corresponds to logical subsumption and
DL concept inclusion relationship. The root class owl:Thing in OWL corresponds to
DL top concept ⊤. The empty class owl:Nothing in OWL corresponds to DL bottom
concept ⊥. A class property in OWL corresponds to a DL role.
Tables 2.2 and 2.3 illustrate the expression of major OWL constructors in DL, where
C and D are DL concept names, R and S are DL role names, and x1, ⋅⋅⋅, xn are DL
individual names.
2.2.5 A Simple Context Ontology Example
Figure 2.1 shows a context ontology example containing user-centric internal con-
text and external context. User’s static proﬁle, dynamic behavior, emotion, and
physiological status are internal context information, while physical, social, and
computational environments are external context. A DL description of the context
ontology is shown in Figure 2.2.
Authenticated
:23 PM

2.3 Context Events
37
Physiological
Status
Class
Class
Class
Class
Class
subClassof
InternalContext
ExternalContext
Context
domain
hasContext
ObjectProperty
range
User
Class
subClassof
subClassof
Class
Class
Class
Class
Class
Physical
Environment
Social
Environment
Computational
Environment
Emotion
Behavior
Profile
Figure 2.1: A context ontology example.
User ⊑∀hasContext.Context
ExternalContext ⊑Context
InternalContext ⊑Context
PhysicalEnvironment ⊑ExternalContext
Proﬁle ⊑InternalContext
Behavior ⊑InternalContext
Emotion ⊑InternalContext
PhysiologicalStatus ⊑InternalContext
SocialEnvironment ⊑ExternalContext
ComputationalEnvironment ⊑ExternalContext
Figure 2.2: A DL description of the context ontology example.
2.3 Context Events
Context can be represented as a set of Description Logic (DL) concept expressions.
To capture context’s dynamics, context can be interpreted and translated to a set of
context events, described in context event expressions. Context events occupy the
time when they occur. Temporal properties and relationships of context events can
be expressed through temporal operators.
2.3.1 Interpreting Context as Context Events
Each Description Logic concept expression can be interpreted as a basic or complex
event. The DL concept expression, containing only the membership of an atomic DL
Authenticated
:23 PM

38
2 Modeling Context
concept C(a) or role R(a, b) (where a and b are concept instances), corresponds to
a basic context event. A combination of basic context events through the Boolean
operators (∧, ∨, ̸=) forms a complex context event, corresponding to a DL concept ex-
pression, where concepts and/or roles are connected via the DL constructors (⊓, ⊔, ¬).
The DL special top concept ⊤and bottom concept ⊥correspond to any event and an
impossible event, respectively.
Deﬁnition 10. Function Expr(ce) maps a DL concept expression ce to a context event
expression, which is recursively deﬁned as follows.
–
Expr(C(a)) is a basic context event expression, where C(a) denotes membership of
atomic concept C;
–
Expr(R(a, b)) is a basic context event expression, where R(a, b) denotes member-
ship of atomic role R;
–
Expr(⊤) = ⊤e is a basic context event expression, representing any event;
–
Expr(⊥) = ⊥e is a basic context event expression, representing an impossible
event;
–
Expr(Pred(u1, u2, ⋅⋅⋅, un))
is
a
basic
context
event
expression,
where
Pred(u1, u2, ⋅⋅⋅, un) denotes holding of predicate P over feature chains u1,
u2, ⋅⋅⋅, un;
–
Expr(¬C(a)) is a complex context event expression, where
Expr(¬C(a)) = ¬Expr(C(a));
–
Expr(¬R(a, b)) is a complex context event expression, where
Expr(¬R(a, b)) = ¬Expr(R(a, b));
–
Expr(C ⊓D(a)) is a complex context event expression, where
Expr(C ⊓D(a)) = Expr(C(a)) ∧Expr(D(a));
–
Expr(C ⊔D(a)) is a complex context event expression, where
Expr(C ⊔D(a)) = Expr(C(a)) ∨Expr(D(a));
–
Expr(∃R.C(a)) is a complex context event expression, where Expr(∃R.C(a))
=(Expr(R(a, b1) ∧Expr(C(b1))) ∨⋅⋅⋅∨(Expr(R(a, bn))∧Expr(C(bn)));
–
Expr(∀R.C(a)) is a complex context event expression, where Expr(∀R.C(a))
=(Expr(R(a, b1)) ∧Expr(C(b1))) ∧⋅⋅⋅∧(Expr(R(a, bn))∧Expr(C(bn))).
◻
2.3.2 Temporality of Context Events
A context event is an atomic occurrence (happening completely or not at all) in the
world [3]. It can be either an instantaneous event (e.g., turning off the light in a room,
entering the room, etc.) or a persistent event (e.g., staying in a shop to buy some-
thing, having a trafﬁc jam, etc.). The former lasts for a very short moment [3, 5], while
the later may take some time, represented as a time range. From a discrete collection
and sampling standpoint, an instantaneous context event occurs at a time point.
Authenticated
:23 PM

2.3 Context Events
39
2.3.2.1 Absolute Occurrence of Context Events
Expressing the temporal occurrence of a context event by means of a precise and
absolute time point or time range along a time line is apparently one way.
(1) Time Point vs. Time Interval
There is a distinction between time point and time range of the following Extended
Backus-Naur Form (EBNF) syntax speciﬁcations.
time-point =
clock-time;
time-range =
’[’,clock-time,clock-time,’]’;
clock-time =
[Year:Month:Week:Day:]Hour:Minute:Second;
time-interval=
number, time-unit;
time-unit =
Year|Month|Week|Day|Hour|Minute|Second|
Microsecond.
where [Year:Month:Week:Day:] is optional. The domain of time point t, de-
noted as Dom(t), is assumed to be discrete with an equal-distance. A partial temporal
order ≤t is used to declare two time points t and s in sequence. t <t (>t) s states that
t is earlier (later) than s in the time line. (t =t s) indicates two exactly the same time
points. ≤t (≥t) denotes not earlier (later) than temporal relationship.
A time range [t–, t+] has a starting time point and an ending time point, where
(t–, t+ ∈Dom(t)) and (t1 ≤t t2). The time length of [t–, t+] is the time difference of t+
and t–, represented as (t+ –t t–). When (t– =t t+), the time range degrades to a time
point, becoming a special case of time range with a zero time length.
A time range [s–, s+] is a sub-range of [t–, t+], denoted as [s–, s+] ⊆t [t–, t+], if and
only if (s– ≥t t–) and (s+ ≤t t+). A time point t belongs to time range [t–, t+], denoted
as t ∈t [s–, s+], if and only if (iff) t falls in the time range, i.e., (t– ≤t t) ∧(t ≤t t+).
(2) Temporal Relationships of Time Intervals
The temporal logic community has deﬁned the following temporal range relation-
ships [2, 3]. Let t = [t–, t+] and s = [s–, s+] be two time ranges.
–
BEFORE(t, s)= TRUE, and inversely, AFTER(s, t)=TRUE, if and only if (t+ <t s–).
–
EQUALS(s, t)= TRUE, if and only if (t– =t s–) ∧(t+ =t s+).
–
MEETS(t, s)= TRUE, and inversely, MET-BY(s, t)=TRUE, if and only if (t+ =t s–).
–
OVERLAPS(t, s)= TRUE, and inversely, OVERLAPPED-BY(s, t)=TRUE, if and
only if (t– <t s–) ∧(t+ >t s–) ∧(t+ <t s+).
–
STARTS(t, s)= TRUE, and inversely, STARTED-BY(s, t)=TRUE, if and only if
(t– =t s–).
–
DURING(t, s)= TRUE, and inversely, CONTAINS(s, t)=TRUE, if and only if ((t– >t
s–) ∧(t+ ≤t s+)) ∨((t– ≥t s–) ∧(t+ <t s+)).
–
FINISHES(t, s)= TRUE, and inversely, FINISHED-BY(s, t)=TRUE, if and only if
(t+ =t s+).
Authenticated
:23 PM

40
2 Modeling Context
2.3.2.2 Relative Occurrence of Context Events
In practice, a lot of temporal knowledge is imprecise and relative, and thus has little
relation to an absolute time point or range. Temporal references of context events
can implicitly be presented with the description of how one event is related to others
temporal properties of events. See the following context examples.
–
“Joe and Mars arrived at the meeting room at exactly the same time” (two
temporally concurrent events).
–
“Joe arrived at the meeting room 10 minutes earlier than his colleague Mar” (two
temporally sequential events)
–
“A car accident happened while Joe was jogging in the street yesterday” (two
temporally containing events).
–
“When Joe was at home yesterday, a ﬁre went on, and he immediately ran out of
the burning house” (two temporally overlapping events between being at home
and the ﬁre).
2.3.3 Temporal Operators on Context Events
To address temporal properties and relationships of context events, a few unary and
binary temporal operators can be used. They enrich the expressiveness of context dy-
namics. For uniformly describing temporality of instantaneous and persistent events,
a time range is afﬁliated with context event E to signify E’s happening period, denoted
as [E.t–,E.t+], where (E.t– ≤t E.t+).
2.3.3.1 Unary Temporal Operators on Context Events
Temporal properties of a context event can be declared through three unary
operators.
–
Within(E, [s–, s+])=TRUE, if and only if ([E.t–, E.t+] ⊆t [s–, s+]).
Within operator examines whether the happening time range of context event
E is within a larger time scope [s–, s+]. For example, the temporal occurrence of
context event “someone entered the supermarket at mid-night during 1am and
3am” can be expressed as Within(E, [1 : 0 : 0, 3 : 0 : 0]).
–
Last(E, Bt)=TRUE, if and only if (E.t+ –t E.t–) =t Bt.
Last operator examines whether context event E lasts for a time period of length
Bt. For example, the temporal occurrence of context event “someone has been at
the AShop for 1 hour” can be expressed as Last(E1, 1 Hour).
–
Periodic(E, {n, }{Bt, }[s–, s+])=TRUE, if and only if event E occurs n times with
a frequency Bt within time range [s–, s+], where ([E.t–, E.t+] ⊆t [s–, s+]) and n is a
positive integer. {n} and {Bt} are optional.
Authenticated
:23 PM

2.3 Context Events
41
For example, the Periodic operator expresses the temporality of con-
text event “someone went to the AShop four times from 8am to 12am” as
Periodic(E, 4, [8 : 0 : 0, 12 : 0 : 0]).
2.3.3.2 Binary Temporal Operators on Context Events
Considering that a lot of temporal knowledge is relative in real life, Allen deﬁnes a few
binary operators in the domain of temporal logic [2–5]. These operators are complete
and powerful enough to describe temporal relationships between two events.
–
Concur(E1, E2)=TRUE, if and only if (E1.t– =t E2.t–) ∧(E1.t+ =t E2.t+).
Concur operator indicates that the two context events happen concurrently in
the same time range. For example, the temporal relationship between two context
events “person 1 and person 2 went together to the cinema to watch the movie”
can be stated as Concur(E1, E2).
–
Sequence(E1, E2, {Bt})=TRUE, if and only if (E1.t+ ≤t E2.t–)∧(E2.t–– tE1.t+ = tBt).
Sequence operator is similar to the traditional sequential operator, except that
it speciﬁcally indicates that context event E2 happens Bt time later than context
event E1. The omission of Bt signiﬁes any value not less than 0:0:0 of time length.
When Bt =t 0 : 0 : 0, the Sequence operator states a temporally joining relation-
ship that one event is followed exactly by another event without any temporal gap
in between. For example, the sequential relationship between two context events
“someone went to a shop ﬁrst and 40 minutes later entered the cinema” can be
describes as Sequence(E1, E2, 0 : 40 : 0).
–
During(E1, E2)=TRUE, if and only if ((E1.t– >t E2.t–)∧(E1.t+ ≤t E2.t–))∨((E1.t– ≥t
E2.t–) ∧(E1.t+ <t E2.t–)).
During operator states that context event E1 happens within the time range of
E2. For example, the statement “person 1 arrived at the campus when person 2
was at the lecture hall to give a lecture” shows the During relationship between
two context events, expressed as During(E1, E2).
The During operator is different from the Within operator in that the former
associates two events temporally, while the latter indicates an absolute time scope
where an event happens.
–
Overlap(E1, E2, {Bt})=TRUE, if and only if (E1.t– <t E2.t–)∧(E1.t+ <t E2.t+) ∧
(E1.t+ –t E2.t– =t Bt).
Overlap operator indicates that context event E1 happens earlier than event E2,
and the two events have an intersectional time range of length Bt. The omission of
Bt signiﬁes any value not less than 0:0:0 of time length. For example, the state-
ment “when person 1 saw person 2 entered the building, person 1 ran out of the
building” exhibits the temporally overlapping relationship between two context
events, expressed as Overlap(E1, E2).
Authenticated
:23 PM

42
2 Modeling Context
2.4 Recapitulation
In this chapter, six typical context representation methods were reviewed. Their mod-
eling and applicability power was compared. Among them, the ontological context
representation approach is preferable, mainly because of its possibility for distributed
knowled ge composition, partial validation of the knowledge, and formalism. The
widely used ontology language W3C OWL, together with its Description Logic (DL)
foundation, were further presented. By interpreting context as a set of context events,
context temporality can be addressed through temporal context events. Context events
occur and occupy the time. Temporal occurrence of a context event can be described
in either an absolute or relative manner. A set of temporal operators on context events
are deﬁned to explicate temporality and temporal relationships of context events,
enriching context modeling in the temporality dimension.
Following the same philosophy, the next chapter will address context’s inherent
uncertainty via uncertain context events.
Literature
[1]
R. Adaikkalavan and S. Chakravarthy. Snoopib: Interval-based event speciﬁcation and
detection for active databases. IEEE Transactions on Knowledge and Data Engineering,
59(1):139–165, 2006.
[2]
J. F. Allen. Maintaining knowledge about temporal intervals. Communication of ACM,
26(11):832–843, 1983.
[3]
J. F. Allen. Time and time again: The many ways to represent time. Journal of Intelligent Systems,
6(4):341–355, 1991.
[4]
J. F. Allen and G. Fergusons. Actions and events in interval temporal logic. Journal of Logic and
Computation, 4:531–579, 1994.
[5]
J. F. Allen and P. J. Hayes. Moments and points in an interval-based temporal logic. Journal of
Computational Intelligence, 5(3):225–238, 1989.
[6]
F. Baader, D. Calvanese, D. McGuinness, D. Nardi, and P. Patel-Schneider(eds.). The Description
Logic Handbook – Theory, Implementation, and Applications Cambridge University Press,
Cambridge, UK, 2003.
[7]
F. Baader and P. Hanschke. A scheme for integrating concrete domains into concept languages.
In Proc. of IJCAI, pages 452–457, 1991.
[8]
F. Baader, I. Horrocks, and U. Sattler. Handbook of Knowledge Representation, chapter
Description Logics. Elsevier, Netherlands, 2007.
[9]
E. Bozsak, M. Ehrig, and et al. S. Handschuh. KAON – towards a large scale semantic web. In
Proc. of E-Commerce and Web Technology, 2002.
[10]
H. Chen, T. Finin, and A. Joshi. An ontology for context aware pervasive computing
environments. Knowledge Engineering Review, 3(18):197–207, 2004.
[11]
P. Gray and D. Salber. Modeling and using sensed context in the design of interactive
applications. In Proc. of the Intl. Conf. on Engineering for Human-Computer Interaction, Toronto,
CA, pages 317–336, 2001.
[12]
K. Henricksen and J. Indulska. Developing context-aware pervasive computing applications:
Models and approach. Pervasive and Mobile Computing, 2(1):37–64, 2006.
Authenticated
:23 PM

Literature
43
[13]
K. Henricksen, J. Indulska, and A. Rakotonirainy. Modeling context information in pervasive
computing systems. In Proc. of Pervasive Computing, Zurich, Switzerland, pages 167–180,
August 2002.
[14]
K. Henricksen, J. Indulska, and A. Rakotonirainy. Generating context management infrastructure
from high-level context models. In Proc. of Mobile Data Management, Zurich, Switzerland, AUS,
pages 1–6, 2003.
[15]
B. Hollunder and W. Nutt. Subsumption algorithms for concept languages. Research Report
RR-90-04, Deutsches Forschungszentrum fur Kunstliche Intelligenz GmbH (DFKI), Germany,
1990.
[16]
P. Korpipaa, J. Mantyjarvi, J. Kela, H. Keranen, and E. Malm. Managing context information in
mobile devices. IEEE Pervasive Computing Magazine, 2(3):42–51, 2003.
[17]
C. Lutz. Reasoning with concrete domains. In Proc. of IJCAI, pages 90–95, 1999.
[18]
D. McGuinness and F. van Harmelen. Owl web ontology language overview.
http://www.w3.org/TR/2004/REC-owl-features-20040210/, 2010.
[19]
A. Schaerf. Reasoning with individuals in concept languages. In Proc. of the Third Congress of
the Italian Association for Artiﬁcial Intelligence on Advances in Artiﬁcial Intelligence, London,
pages 108–119, 1993.
[20] B. Schilit and M. Theimer. Disseminating active map information to mobile hosts. IEEE Network,
8:22–32, 1994.
[21]
B. Schilit, M. Theimer, and B. Welch. Customizing mobile applications. In Proc. of USENIX
Mobile and Location-Indepedent Computing Symposium, Cambridge, MA, pages 129–138,
August 1993.
[22] A. Schmidt and K.V. Laerhoven. How to build smart appliances. IEEE Personal Communications,
pages 66–71, 2001.
[23] M. Schmidt-Schaub" and G. Smolka. Attributive concept descriptions with complements.
Artiﬁcial Intelligence, 48(1):1–26, 1991.
[24] T. Strang. A context modeling survey. In Proc. of the Intl. Workshop on Advanced Context
Modelling, Reasoning and Management, London, September 2004.
[25]
A. van Bunningen. Context-aware Querying – Better Answers With Less Effort. PhD thesis, Centre
for Telematics and Information Technology, University of Twente, The Netherlands, 2006.
[26] W3C. Composite capabilities/preference proﬁle (CC/PP). http://www.w3. org/Mobile/CCPP/.
[27]
W3C. Owl 2 web ontology language document overview. http://www.w3. org/TR/owl2-
overview/, 2009.
[28] WAPForum. User agent proﬁle (UAProf). http://www.wapforum.org.
[29] Wikipedia. Ontology (computer science) from wikipedia, the free encyclopedia.
http://en.wikipedia.org/wiki/.
[30] Wikipedia. Web ontology language. http://en.wikipedia.org/wiki/Web_Ontology_
Language#OWL_sublanguages.
[31]
G. Witmer. Dictionary of philosophy of mind – ontology. http://www. artsci.wustl.edu/
philos/MindDict/ontology.html, 2004.
Authenticated
:23 PM

3 Addressing Context Uncertainty
Abstract: Context data sensed from external environments and internal users is largely
uncertain due to faulty sensors and lack of precision in computation and measure-
ment. Uncertain context distinguishes from certain context in the degree of belief or
conﬁdence in the occurrence of the context. If certainty is referred to as a perception
or belief that a certain context holds, uncertainty indicates a lack of conﬁdence or
trust in an article of knowledge about the holding of the context. Context uncertainty
complicates and affects computation and decision-making in a number of unfavorable
aspects. This chapter reviews uncertainty processing theories and practices in diverse
ﬁelds, ranging from economy, engineering, ecology, information science, to database.
They shed lights on context uncertainty management to be discussed in this chapter.
3.1 Uncertainty
Uncertainty reﬂects one’s unsureness about something or somebody, ranging from
just short of complete sureness to an almost complete lack of conviction about an
outcome [110]. Whether the train will delay or not is uncertain. Whether it rains or not
tomorrow is uncertain. Just as ancient Greek Socrates said, “as for me, all I know is I
know nothing [105].”
Context uncertainty brings difﬁculty to context-aware computing in a number of
unfavorable aspects [58, 150]. Even worse, some attempts that one takes to manage
and reduce context uncertainty are accompanied with uncertainty, and sometimes,
more uncertainty may appear with the development of technologies. Here is a simple
example. Suppose someone wants to go to a place. He could only walk or take a horse
before the industrial revolution. Nowadays, however, he has many choices of trafﬁc
tools, which may be bike, bus, car, or plane. Then there is more uncertainty about the
way to travel.
Though it is hard to completely eliminate uncertainty, it is worthwhile to recog-
nize and cope with context uncertainty to avoid unfavorable hazards for high-quality
decisions in context-aware applications. In the literature, uncertainty processing cov-
ers almost all ﬁelds of scientiﬁc studies [14]. Different kinds of uncertainty call for
different uncertainty processing techniques.
In general, uncertainty arises from different sources in various forms, falling into
either aleatory uncertainty or epistemic uncertainty [48, 64, 101, 110, 114, 137, 138].
3.1.1 Aleatory Uncertainty
Aleatory uncertainty derives from natural variability of the world. It reﬂects
the inherent randomness in nature, and exists naturally regardless of human’s
DOI 10.1515/9783110556674-003
Authenticated
:23 PM

3.2 Uncertainty Processing Theories
45
knowledge. For example, in the event of ﬂipping a coin, the coin comes up heads or
tails with some randomness. Even if we do many experiments and know the prob-
ability of coming up heads, we still cannot predict the exact result in the next turn.
Aleatory uncertainty cannot be eliminated or reduced by collecting more informa-
tion and knowledge. No matter whether we know it or not, this uncertainty stays
there all the time. Aleatory uncertainty is thus also referred to as natural variabil-
ity [110], objective uncertainty [111], external uncertainty [76], random uncertainty [63],
stochastic uncertainty [61], inherent uncertainty, irreducible uncertainty, fundamental
uncertainty, real world uncertainty, or primary uncertainty [87].
In the literature, probability distribution is the most common way to represent
and deal with aleatory uncertainty.
3.1.2 Epistemic Uncertainty
Epistemic uncertainty origins from human’s lack of knowledge and ability of meas-
uring and modeling the physical world. Unlike aleatory uncertainty, given more
knowledge of the problem and proper methods, epistemic uncertainty can be re-
ducible and sometimes can even be eliminated. For example, the estimation of the
distance between Boston and Washington can be more precise if one has known
the distance from Boston to New York. In the literature, epistemic uncertainty is
also called knowledge uncertainty [110], subjective uncertainty [61, 111], internal uncer-
tainty [76], incompleteness [139], functional uncertainty, informative uncertainty [83],
or secondary uncertainty [87].
Taking a possible river ﬂood for example, the probability distribution of the ﬂood
frequency curve is a representation of aleatory uncertainty, reﬂecting an inherent
randomness of the physical world. One cannot reduce this type of uncertainty. On
the other hand, parameters of the frequency curve imply a kind of epistemic uncer-
tainty, constrained by the existing knowledge and corresponding model. However,
with the increase of information, one can always modify and reﬁne the model to make
it approach the realistic situation.
Although context uncertainty can be categorized into aleatory uncertainty or
epistemic uncertainty, there is not a clear boundary between them, and they may
even be handled in the same way. Nevertheless, this categorization reminds one
what should be noticed in representing and processing diverse context uncertainty
in context-aware computing and applications.
3.2 Uncertainty Processing Theories
3.2.1 Theory Overview
Probability theory [86] and fuzzy theory [146–148] are two well-established
uncertainty theories. They can model both aleatory uncertainty and epistemic
Authenticated
:23 PM

46
3 Addressing Context Uncertainty
User ⊑∀hasContext.Context
ExternalContext ⊑Context InternalContext ⊑Context
PhysicalEnvironment ExternalContext
Proﬁle ⊑InternalContext Behavior ⊑InternalContext
Emotion ⊑InternalContext PhysiologicalStatus ⊑InternalContext
SocialEnvironment ⊑ExternalContext
ComputationalEnvironment ⊑ExternalContext
Figure 3.1: Uncertainty theories.
uncertainty. The probability theory leads to three uncertainty processing methods,
namely, Monte Carlo method, Bayesian method, and Evidence Theory. Beyond
these, info-gap theory originally developed for decision making [18, 19], as well as re-
cently derived uncertainty theory [94, 95] from probability theory and fuzzy theory
are also developed. Figure 3.1 lists the four typical uncertainty handling theories.
(1) Probability theory is the most widely used method in almost every ﬁeld. It can
deal with natural aleatory uncertainty through random experiments and subjective
aleatory uncertainty by statistics from questionnaires. Based on probability theory,
Monte Carlo method, Bayesian method, and Dempster-Shafer evidence theory are
developed.
–
Monte Carlo method can solve complicated situations where computing an exact
result with a deterministic algorithm is hard. It approximates the exact value by
repeated random sampling.
–
Bayesian method pursues an exact value based on a graphical model with prior
and conditional probabilities. It is a good tool for inference.
–
Dempster-Shafer theory avoids the prior probability assumption. It computes
the conﬁdence interval, containing the exact probability, by evidences collected
from different sources.
(2) Fuzzy theory is good at handling human ambiguity by modeling epistemic
uncertainty through fuzzy sets with membership functions.
(3) Info-gap theory can address severe uncertainty whose probability cannot be eas-
ily measured or computed by using a range around the expected value to represent
epistemic uncertainty.
(4) Derived uncertainty theory from probability and fuzzy theories aims at human’s
subjective epistemic uncertainty.
3.2.2 Probability Theory
Probability theory [43, 86] originally aims at random phenomena, such as ﬂipping a
coin. It states knowledge or belief that an event will occur or has occurred by means
Authenticated
:23 PM

3.2 Uncertainty Processing Theories
47
of probability, and the probability value is obtained based on statistics and random
experiments through repeated trials and analysis. That is, in N times of independent
random experiments, the occurrence times of an event approach a constant N0. Then,
N0/N is the probability of the random event.
A probability density function P is a way to represent the value of probability. Let
K be a sample space. Each subset of K is called an event, denoted as A1, A2, ⋅⋅⋅. Assume
A is an event, then P satisﬁes:
(1) (Normality)
P(K) = 1
(2) (Nonnegativity) P(A) ≥0
(3) (Additivity)
For mutually disjoint events A1, A2, ⋅⋅⋅,
P (
∞
⋃
i=1
Ai) =
∞
∑
i=1
P(Ai)
Some further properties can be inferred from the basic deﬁnition of P, where A and B
are two events, and AC is the complement (opposite) of A.
(1) P(⌀) = 0
(2) P(A) ∈[0, 1]
(3) P(A) + P(AC) = 1
(4) P(A ∪B) = P(A) + P(B) – P(A ∩B)
(5) P(A ∩B) = P(A|B)P(B) = P(B|A)P(A)
(5󸀠) For two independent events A and B,P(A ∩B) = P(A)P(B).
Probability theory can deal with both aleatory and epistemic uncertainty. Random
experiments usually deal with natural variability, which satisﬁes the deﬁnition of
aleatory uncertainty. Through random experiments, one can calculate the frequency
of a certain event, which is close to the real probability with the increase of running
times. With the introduction of subjective probability, it is applied to subjective objects
and situations which are not suitable for random experiments. Currently in dealing
with uncertainty, probability theory is at a dominant position. In most of today’s
applications, uncertainty problems are considered to be probabilistic ones.
3.2.2.1 Extensions of Probability Theory
Based on the classic probability theory, a few methods such as Monte Carlo method,
Bayesian method, and Dempster-Shafer evidence theory are developed.
(1) Monte Carlo Methods
The idea of Monte Carlo methods origins from a famous experiment conducted by
Buffon who simulates the value of 0 by dropping needles on a ﬂoor made of paral-
lel strips of wood in 1777. It inspires researchers to simulate some values of interest by
Authenticated
:23 PM

48
3 Addressing Context Uncertainty
Figure 3.2: Using Monte Carlo method to compute 0.
random sampling [104]. Now, Monte Carlo method has become a well-known numer-
ical calculation method based on the probability theory. It relies on repeated random
sampling to compute the result (e.g., value of a parameter). By statistical analysis, the
frequency of the value is then obtained to estimate the result.
Example 1. Figure 3.2 illustrates how a Monte Carlo algorithm computes the value of 0.
It ﬁrst randomly generates uniformly distributed points in the circumscribed square
of the unit circle for a number of trials. Then, it computes the ratio of the number
of points in the unit circle and the total number of points in the square. Finally, it
multiplies the ratio by 4, yielding the approximation of 0.
◻
In the random sampling, the Chernoff bound [37] can be used to determine the number
of runs for a value by majority agreement. In n runs of random experiments ﬂipping
coins, p is the probability of heads coming up. For the assurance of 1 – % accuracy
which is the probability for majority agreement, the number of runs should satisfy
n ≥
1
(p – 1
2)
2 ln 1
√%.
(3.1)
Practically, Chernoff bound gives bounds on tail distributions of sums of independent
random variables. Let X1, ⋅⋅⋅, Xn be independent random variables, X =
n
∑
i=1
Xi, and , is
the expectation of X, then for any $ > 0,
P(X > (1 + $),) < (
e$
(1 + $)(1+$) )
,
.
(3.2)
This bound measures how far the sum of random variables deviates from the expect-
ation in n runs of random experiments.
Monte Carlo methods can generally solve complicated situations where comput-
ing an exact result with a deterministic algorithm is hard. It is especially good at simu-
lating and modeling phenomena with signiﬁcant uncertainty in inputs, such as ﬂuids,
disordered materials, strongly coupled solids, and cellular structures. It is widely
Authenticated
:23 PM

3.2 Uncertainty Processing Theories
49
used in mathematics, for instance to evaluate multidimensional deﬁnite integrals
with complicated boundary conditions. When Monte Carlo simulations are applied
in space exploration and oil exploration, their prediction of failures, cost overruns,
and schedule overruns are routinely better than human intuition or alternative soft
methods.
Furthermore, Markov chains join Monte Carlo simulations to have Markov chain
Monte Carlo method [53]. A Markov chain, named for Andrey Markov, is a mathemat-
ical system that undergoes transitions from one state to another, between a ﬁnite or
countable number of possible states. The widely used ﬁrst-order Markov chain is like:
P(Xn = xn|X1 = x1, X2 = x2, ⋅⋅⋅, Xn–1 = xn–1) = P(Xn = xn|Xn–1 = xn–1),
where Xi(1 ≤i ≤n) is a random variable of value xi, stating that the next state depends
only on the current state and not on the sequence of events that preceded it.
With the Markov chain Monte Carlo sampling method, sequences of random
numbers satisfying a Markov property can be generated to reﬂect desired probabil-
ity distributions. Often it is used as a mathematical model for some random physical
process or complex stochastic systems. If the parameters of the chain are known,
quantitative predictions can then be made.
(2) Bayesian Methods
Bayes Theorem, proposed by Bayes in 1763 [15], is based on the probability theory. It
expresses relations between two or more events through conditional probabilities and
makes inferences.
P(H|D) = P(D|H)P(H)
P(D)
,
where H is a hypothesis with a prior probability P(H), and P(H|D) is H’s posterior prob-
ability given observed data D. The value of P(H|D) can be evaluated based on P(D|H),
P(H), and P(D).
Confronted with mutually exclusive hypotheses H1, H2, ⋅⋅⋅, Hn, we have
P(D) =
n
∑
i=1
P(D|Hi)P(Hi). Therefore, the posterior probability of Hk is:
P(Hk|D) = P(D|Hk)P(Hk)
n
∑
i=1
P(D|Hi)P(Hi)
(k = 1, 2, ⋅⋅⋅, n).
(3.3)
Bayes Theorem suits situations which are lack of direct information about an event.
It involves logic reasoning, rather than random sampling as in Monte Carlo method.
Based on the Bayes theorem, a probabilistic graphical model, Bayesian network, is
developed to represent a set of random variables and their conditional dependencies
via a directed acyclic graph.
Authenticated
:23 PM

50
3 Addressing Context Uncertainty
P(flu) =0.1
P(fever|flu,cold) = 0.9
P(fever|~flu,cold) = 0.2
Conditional probabilities
cold
cough
fever
flu
Figure 3.3: A Bayesian network example.
Example 2. Figure 3.3 is a Bayesian network representing the probabilistic relation-
ships between disease and symptom. Each node represents a random variable with a
prior probability. Edges represent the dependencies between nodes with conditional
probabilities. Given the values of P(ﬂu), P(fever|ﬂu, cold), and P(fever| ∼ﬂu, cold),
according to the Bayes theorem, we have
P(fever|cold)
= P(fever, ﬂu|cold) + P(fever, ∼ﬂu|cold)
= P(fever|ﬂu, cold) ∗P(ﬂu|cold) + P(fever| ∼ﬂu, cold) ∗P(∼ﬂu|cold)
= P(fever|ﬂu, cold) ∗P(ﬂu) + P(fever| ∼ﬂu, cold) ∗P(∼ﬂu)
= 0.9 ∗0.1 + 0.2 ∗0.9 = 0.27.
◻
(3) Dempster-Shafer Theory (Evidence Theory)
Dempster-Shafer Theory (also called Evidence Theory) was ﬁrst proposed by Demp-
ster and Shafer [44, 125]. It combines evidence from different sources and arrives at a
degree of belief by taking into account all the available evidence. It deﬁnes a space of
mass and the belief mass as a function:
m : 2X →[0, 1],
where X is the universal set including all possible states, 2X is the set of all the subsets
of X. For a subset S ∈2X, m(S) is derived from the evidence that supports S.
∑
S∈2X
m(S) = 1
Note that m(A ∪B) may or may not be equal to m(A) + m(B) – m(A ∩B), whose
value is based on the evidence supporting every element in A ∪B. Also, the equa-
tion (m(A) + m(AC) = 1) may or may not hold. This reveals one advantage of Evidence
Theory compared to the probability theory, which can represent the state “unknown”
with an explicit mass value m(X) based on the evidence.
In Evidence Theory, belief and plausibility are further deﬁned as the low and up-
per boundary about a proposition. Belief summarizes all the masses of the subsets of
S, meaning all the evidence that fully supports S.
belief(S) = ∑
T⊆S
m(T)
Authenticated
:23 PM

3.2 Uncertainty Processing Theories
51
Table 3.1: Evidence Theory-based probability scope.
Hypothesis
Mass
Belief
Plausibility
Prob. Scope
⌀
0.0
0.0
0.0
[0.0, 0.0]
{cold}
0.5
0.5
0.6
[0.5, 0.6]
{ﬂu}
0.4
0.4
0.5
[0.4, 0.5]
{cold, ﬂu}
0.1
1.0
1.0
[1.0, 1.0]
Plausibility summarizes all the masses of the sets that have intersection with S,
meaning all the evidence that partly or fully supports S.
plausibility(S) =
∑
T∩S ̸=⌀
m(T)
The probability of a set S ∈2X falls into the range of [belief(S), plausibility(S)].
Example 3. Reverting to the disease and symptom example in Figure 3.3, a patient
may be diagnosed to catch a cold or have a ﬂu, i.e., X = {cold, ﬂu}. The mass values
m({cold}), m({ﬂu}), m(⌀), m({cold, ﬂu})(cold or ﬂu) are determined according to the
collected evidence from medical instruments or experiences of doctors, as shown in
the second column of Table 3.1. Accordingly, the beliefs and plausibilitys of m({cold}),
m({ﬂu}), m(⌀), and m({cold, ﬂu}) can be calculated. belief({cold}) = m(⌀)+m({cold}) =
0 + 0.5 = 0.5, plausibility({cold}) = m({cold}) + m({cold, ﬂu}) = 0.5 + 0.1 = 0.6. Differ-
ent from the property of probability (that is, m(A) + m(B) – m(A ∩B)), m({cold, ﬂu})
̸=
m({cold})+m({ﬂu})–m({⌀}). Instead, m({cold, ﬂu}) derives from the evidence that sup-
ports both cold and ﬂu, such as the symptom of fever. However, m({cold}) derives from
the evidence that only support cold.
◻
The combination of two mass functions (e.g., m1, m2) derived from different (possibly
conﬂicting) sources of evidence (e.g., different diagnoses from two doctors) is deﬁned
by Dempster as follows:
m1,2(S) =
1
1 – K
∑
A∩B=S ̸=⌀
m1(A)m2(B)
where K =
∑
A∩B=⌀
m1(A)m2(B).
Here K is a normalization factor to ensure the total sum m1,2 to be 1. It measures
the amount of conﬂict between the two diagnoses.
Some researchers propose different rules for combining evidence, often with a
view to handle conﬂict in evidence, like the Transferable Belief Model (TBM) [127] and
Coherent Upper and Lower Previsions method [140].
Authenticated
:23 PM

52
3 Addressing Context Uncertainty
Compared to the prior and error assumptions needed by the Bayesian method
which are sensitive to the results, Evidence Theory doesn’t enforce any applicable
conditions and assumptions. It can thus deal with more uncertainty (including
subjective uncertainty arising from experts) than the former Bayesian method. A
comparison between Evidence Theory and Bayesian method in handling epistemic
uncertainty has been given by Soundappan with some experiments [130].
In summary, in dealing with aleatory uncertainty, Monte Carlo method offers an
easy yet efﬁcient way to do random sampling. Epistemic uncertainty can be handled
appropriately with Bayesian method and Evidence Theory. Evidence Theory is based
on the evidence to get useful information. This can also help decide whether it is worth
gaining more evidence to reduce uncertainty [130].
3.2.3 Fuzzy Theory
Fuzzy theory, proposed by Zadeh in 1965 [146], is another good way to deal with vague-
ness uncertainty arising from human linguistic labels. It provides a framework for
modeling the interface between human conceptual categories and data, thus reducing
cognitive dissonance in problem modeling so that the way that humans think about
the decision process is much closer to the way it is represented in the machine. The
concept of fuzzy set extends the notion of a regular crisp set and expresses classes with
ill-deﬁned boundaries such as young, good, and important, etc. Within this frame-
work, there is a gradual rather than sharp transition between nonmembership and
full membership. A degree of membership in the interval [0,1] is associated with every
element in the universal set X. Such a membership assigning function (,A : X →[0, 1])
is called a membership function and the set (A) deﬁned by it is called a fuzzy set.
The membership function of a fuzzy set is a subjective concept, depending on ob-
servers. Different people may deﬁne different membership functions. However, the
value of probability is not inﬂuenced by observers, because probability is usually
gained or simulated by random experiments, which approaches a certain frequency.
For example, the probability of head coming up in a fair coin is close to 0.5. How-
ever, as fuzzy sets represent imprecisely deﬁned classes with ambiguous criteria, it
can express concepts quite like human thinking.
Example 4. When we are not sure about the exact centigrade degree of the day, we
usually estimate the weather to be warm, cool, cold, and hot, and put on more or less
clothes accordingly. The concept warm can be described through a fuzzy set and its
membership function ,warm(x). We think that 26∘C is the most appropriate temperature
for the set warm. Then the grade of membership function of x = 26∘is 1, denoted as
,warm(26∘) = 1. Similarly, ,warm(20∘) = 0, ,warm(23∘) = 1, ,warm(26∘) = 1, ,warm(29∘) = 0.
The fuzzy set warm can thus be represented as {0/20∘, 0.33/21∘, 1/23∘, 1/26∘, 0/29∘}. It
can also be expressed in a function, as shown in Figure 3.4.
◻
Authenticated
:23 PM

3.2 Uncertainty Processing Theories
53
Membership degree
μwarm
Temperature 
Warm
20 ̊C
29 ̊C 
26 ̊C
1.0
23 ̊C
Figure 3.4: A warm fuzzy set example.
Let A and B be two fuzzy sets.
,A∪B(x) = max(,A(x), ,B(x)), ,A∩B(x) = min(,A(x), ,B(x)).
Based on the fuzzy set theory, fuzzy logic is developed [148]. A fuzzy proposition is
deﬁned on the basis of the universal set X, and a fuzzy set F, representing a fuzzy
predicate, such as “tall”. Then the fuzzy proposition “Tom is tall” can be written as
,F(Tom), representing the membership degree. Different from the classic two-value
logic (either true or false), a fuzzy proposition can take values in the interval [0,1].
,F(Tom) = 0.6 means the truth degree of “Tom is tall” is 0.6. Given two elements h1,
h2 ∈X, the standard rules for evaluating the fuzzy truth value are summarized as: (1)
,F(h1 ∨h2) = max(,F(h1), ,F(h2)); (2) ,F(h1 ∧h2) = min(,F(h1), ,F(h2)); (3) ,¬F(h1) =
1 – ,F(h1).
Possibility theory extends fuzzy set and fuzzy logic [147] as a counterpart of
probability theory. Let K be the universe of discourse. A1, A2, ⋅⋅⋅are events, which are
subsets of K. The possibility distribution Pos is a function from K to [0,1], conforming
to the following axioms:
(1) (Normality)
Pos(K) = 1
(2) (Nonnegativity)
Pos(⌀) = 0
(3) (Maximality)
For disjoint events A1, A2, ⋅⋅⋅,
Pos (
∞
⋃
i=1
Ai) =
∞
max
i=1 Pos(Ai).
The above axioms shows that the possibility distribution satisﬁes the maximality,
which is distinct from the additivity property of probability theory.
Fuzzy theory is good at processing ambiguous statements, thus epistemic uncer-
tainty.
3.2.4 Information-Gap Theory
Information-gap (short as Info-gap) theory is proposed by Ben-Haim in the 1980s
[18, 19]. It models uncertainty mainly for decision making, and comes up with
model-based decisions involving severe uncertainty independent of probabilities.
Authenticated
:23 PM

54
3 Addressing Context Uncertainty
Robust
Function
Uncertainty Model
Opportuneness
Function
Performance
Requirements
System Model
Figure 3.5: Constitutes of info-gap decision theory.
This severe uncertainty belongs to the epistemic uncertainty category, and is usu-
ally immeasurable or uncalculated with probability distributions and is considered
to be an incomplete understanding of the system being managed, thus reﬂecting the
information gap between what one does know and what one needs to know.
The info-gap decision theory consists of three components (i.e., performance re-
quirements, uncertainty model and system model) and two functions (i.e., robustness
function and opportuneness function), as shown in Figure 3.5.
The performance requirements state the expectations of the decision makers, such
as the minimally acceptable values, the loss limitations, and the proﬁt requirements.
These requirements form the basis of decision making. Different from probability the-
ory which models uncertainty with probability distributions, the uncertainty model
of info-gap theory models uncertainty in the form of nested subsets: U(!,
∼u), where
∼u
is a point estimate of an uncertain parameter, and !(≥0) is the deviation around
∼u.
The robustness and opportuneness functions determine the settings of !. An example
uncertainty model is:
U(!,
∼u) = {u : |u –
∼u | ≤!
∼u} ,
satisfying two basic axioms:
(1) (Nesting)
For (! < !󸀠), U(!,
∼u) ⊆U(!󸀠,
∼u)
(2) (Contraction)
U(0,
∼u) = {
∼u}.
The robustness function represents the greatest level of uncertainty, at which minimal
performance requirements are satisﬁed and failure cannot happen, addressing the
pernicious aspect of uncertainty. The opportuneness function exploits the favorable
uncertainty leading to better outcomes, focusing on the propitious aspect of uncer-
tainty. Through the two functions, uncertainty can be modeled, and the information
gap can be quantiﬁed and further be reduced with some actions [18, 19]. The decision-
making process actually involves the construction, calculation, and optimization of
the two functions.
The third component of info-gap theory is an overall model of system considering
all factors and requirements.
Authenticated
:23 PM

3.2 Uncertainty Processing Theories
55
Example 5. A worker faces a choice of cities to live: city A or city B. The salary s he
could earn is uncertain. In city A, he might earn 80$ as an estimate every week, i.e., ̃s =
80$. If he earns less than 60$, he cannot afford the lodging and is in danger of sleeping
in the street. But if he earns more than 95$, he can afford a night’s entertainment as a
windfall. In city B, he might earn 100$ as an estimate. The lodging costs 80$, and the
entertainment costs 150$.
Based on the system requirements (avoiding sleeping in the street, or affording a
night’s entertainment), for city A, the uncertain salary of a worker can be represented
as a subset: U(!, 80$) = {s : |s – 80$ | ≤80$∗!, ! ≥0}. That is, the worker’s income
s falls into the interval [80$∗(1 – !), 80$∗(1 + !)]. Then the robustness/opportuneness
functions determining ! are:
R obust(s, 60$) = max {! : mins∈U(!,80$) s ≥60$}
= max {! : 80$ ∗(1 – !) ≥60$} = max {! : ! ≤0.25} = 0.25
O pportune(s, 95$) = min {! : maxs∈U(!,80$) s ≥95$}
= min{! : 80$ ∗(1 + !) ≥95$} = min{! : ! ≥0.1875} = 0.1875
In a similar fashion, for city B, the ! values returned from the robustness and oppor-
tuneness functions are 0.2 and 0.5, respectively. As ! represents the deviation from
the estimate value, the bigger ! takes, the less danger the salary is below the hazard
threshold 60$ (or 100$); and the smaller ! takes, the higher chance to enjoy a night’s
entertainment. Therefore, moving to city A appears to be better than to city B.
◻
Info-gap theory applies to the situations of limited information, especially when there
isn’t enough data for other uncertainty handling techniques such as probability the-
ory. Ben-Haim once argues that probability theory is too sensitive to the assumptions
on the probabilities of events [18]. In comparison, info-gap theory stands upon an
uncertain range rather than a probability, and is thus more robust. In terms of this,
Info-Gap Decision Theory looks like a model more than a theory.
In the literature, there are also some doubts about Info-Gap Decision Theory.
Sniedovich calls it “voodoo decision-making” [128]. He questions the availability of
the info-gap theory in the situation of severe uncertainty. In his opinion, people know
little information in severe uncertainty, and it is hard to determine an appropriate es-
timate value. In this case, it would be more practical to deﬁne a global distribution
rather than a local interval.
So far, info-gap theory has been exploited in many decision making applications,
including engineering analysis and design, life science and medicine, economics and
ﬁnance, homeland security, project management, and philosophy [17].
Authenticated
:23 PM

56
3 Addressing Context Uncertainty
3.2.5 Derived Uncertainty Theory
A derived uncertainty theory from probability and fuzzy theories is later presented by
Liu [94, 95] in 2007 to handle human ambiguity uncertainty. Its three key concepts,
i.e., uncertain measure, uncertain variable, and uncertain distribution, are deﬁned as
follows.
Let A be a nonempty set. L is a 3-algebra of A. A 3-algebra over a set A is deﬁned as
a nonempty collection of all subsets of A (including A itself). Each element in L is an
event, expressed as D1, D2, ⋅⋅⋅. The uncertain measure M(D) represents the occurrence
level of an event. (A, L , M) is called an uncertainty space with the following axioms.
(1) (Normality)
M(A) = 1
(2) (Self-Duality)
M(D) + M(DC) = 1
(3) (Subadditivity)
M (
∞
⋃
i=1
Di) ≤
∞
∑
i=1
M(Di)
(4) (Product Measure) M (
n
∏
k=1
Dk) = min
1≤k≤n Mk(Dk)
An uncertain variable is a function .(A, L , M) = 2R, where (A, L , M) is an uncertain
space and 2R is a set of real numbers. For any real number x, the uncertain distribution
of an uncertain variable . is an increasing function deﬁned as: U(x) = M(. ≤x).
In the derived uncertainty theory, instead of uncertainty distribution functions, a
discrete 99-Table (Figure 3.6) is used to state the uncertainty distribution. A 99-Table
usually accommodates 99 points in the curve of uncertain distribution, and is helpful
Uncertainty distribution U(x)
Distance between
A and B
0
30km
0.3
300km
900km
1.0
x
U(x)
x
0.01
30
0.02
32
99-Table depicting uncertain distance between A and B
...
...
0.30
90
0.31
91
...
...
0.99
300
Figure 3.6: An uncertainty distribution example in a 99-Table.
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
57
when uncertainty functions are unknown. Considering comprehensive requirements
of storage capacity and precision, 99 points are usually taken from the uncertain dis-
tribution for calculation. However, this is not strictly restricted. One can also take 80
or 150 points according to different precision and storage requirements.
Example 6. Suppose an application is interested in the city distances below a
threshold. We can view the distance between city A and city B as an uncertain vari-
able. The uncertain distribution U(x) with its discrete 99-Table expression is plotted in
Figure 3.6. The second row of 99-Table represents the values that the uncertain vari-
able can take, and the ﬁrst row means the corresponding uncertainty. U(90km) =
M(x ≤90km) = 0.3 states that the distance between A and B is lower than 90
kilometers with the uncertainty degree 0.3.
◻
Derived uncertainty theory also possesses contradiction and excluded-middle proper-
ties. Let H denote a proposition (e.g., “restaurant ABC has a good reputation”) with a
truth value T(H). T(H ∨¬H) = 1 and T(H ∧¬H) = 0. Besides, it conforms to the mono-
tonicity. It does well in describing interval-based uncertainty measures, and is suitable
to handle subjective epistemic uncertainty in risk analysis, reliability analysis, and
ﬁnance [95].
Table 3.2 summarizes the above four uncertainty theories in managing diverse
uncertainty in the real world.
3.3 Uncertainty Processing Practices
3.3.1 Practice Overview
Activities to process uncertainty using the above theories have been carried out for a
long time. Table 3.3 lists some prototypical practices in the ﬁelds of economics, en-
gineering, ecology, and information science. Associated applications as well as some
popularly-used data structures (such as event and fault trees) are detailed in the
following.
3.3.2 In Economics
Economics is a classical ﬁeld for uncertainty studying. Knight once made a well-
known statement that “you cannot be certain about uncertainty [84]”. A large amount
of economic investigation has been devoted to the foundation of uncertainty and its
counterpart risk in economics [59, 82, 84, 103, 121]. While uncertainty is generally con-
sidered to be lack of certainty with possible states and multiple outcomes, risk refers
to a potential loss or undesired effect, which may cause something bad or unexpec-
ted [68]. In economic risk analysis, risk is usually expressed as a quantity, measured
Authenticated
:23 PM

58
3 Addressing Context Uncertainty
Table 3.2: Comparison of the four uncertainty handling theories.
Managed uncertainty
Probability Theory
Randomness
Fuzzy Theory
Ambiguity with ill-deﬁned boundaries
Derived Uncertainty Theory
Human’s subjectiveness uncertainty
Info-Gap Theory
Immeasurable factors of incomplete understanding
Uncertainty measurement
Probability Theory
Probability measure P(x)
Fuzzy Theory
Membership function ,A(x)
Derived Uncertainty Theory
Uncertain measure M(x)
Info-Gap Theory
A nested set U(!,
∼u)
Uncertainty distribution
Probability Theory
Probability density function (pdf) and cumulative
distribution function (cdf)
Fuzzy Theory
Possibility distribution Pos(x)
Derived Uncertainty Theory
99-Table (Uncertain distribution U(x))
Info-Gap Theory
-
Properties
Probability Theory
Additivity P(
∞∪
i=1 Ai) =
∞
∑
i=1
P(Ai)
Self-duality P(A) + P(Ac) = 1
Fuzzy Theory
Nonadditivity Pos(
∞∪
i=1 Ai) =
∞
max
i=1 Pos(Ai)
No self-duality
Derived Uncertainty Theory
Sub-additivity M(
∞∪
i=1 Ai) ≤
∞
∑
i=1
M(Ai)
Self-duality M(A) + M(Ac) = 1
Info-Gap Theory
Nesting for ! < !󸀠U(!,
∼u) ⊆U(!󸀠,
∼u)
Contraction U(0,
∼u) = {
∼u}
Operations
Probability Theory
Add when mutually exclusive P(A ∪B) = P(A) + P(B)
Multiply when independent P(A ∩B) = P(A) ∗P(B)
Fuzzy Theory
Maximum Pos(A ∪B) = max(Pos(A), Pos(B))
Minimum Pos(A ∩B) = min(Pos(A), Pos(B))
Derived Uncertainty Theory
Maximum when independent
M(A ∪B) = max(M(A), M(B))
Minimum when independent
M(A ∩B) = min(M(A), M(B))
Info-Gap Theory
-
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
59
Table 3.3: Uncertainty handling techniques in practice.
Economics
Probability Theory
Decision Tree in budget making
Fuzzy Theory
-
Derived Uncertainty Theory
-
Info-Gap Theory
Credit risk analysis
Engineering
Probability Theory
Risk analysis: Event Tree Analysis,
Fault Tree Analysis
Probabilistic life cycle assessment
Fuzzy Theory
Fuzzy life cycle assessment
Derived Uncertainty Theory
Reliability analysis, Risk analysis
Info-Gap Theory
-
Ecology
Probability Theory
Population forecasting
Fuzzy Theory
-
Derived Uncertainty Theory
-
Info-Gap Theory
Conservation management
Information Science
Probability Theory
Social networking
Fuzzy Theory
Social networking
Derived Uncertainty Theory
Project scheduling
Info-Gap Theory
-
with the use of probabilities. In comparison, severe uncertainty is restricted to the
nonquantitative case, caused by the lack of information or knowledge [84], where the
underlying statistical distribution are unknown. The difference between measurable
risk and immeasurable uncertainty is thus ascribed to the matter of knowledge. In
the following, various kinds of risk analysis efforts, which are of particular economic
importance to decision making, are described.
3.3.2.1 Probability-Based Economic Risk Analysis
Economic budget planning is a common activity involving risk analysis, where a prob-
abilistic decision tree is usually exploited in the process. All possible situations with
respective probabilities are outlined in the decision tree, based on which possible
target beneﬁts are computed.
Authenticated
:23 PM

60
3 Addressing Context Uncertainty
Products well-sold
Small
workshop
Large
workshop
Products badly-sold
Products well-sold
P=0 .8
P=0 .8
P=0 .2
Products badly-sold
90k
–10k
20k
30k
Revenue
Yearly revenue after workshop construction
Large Workshop
Small Workshop
90K
30K
–1OK
20K
Products Badly-Sold
Products Well-Sold
P=0 .2
Figure 3.7: A probabilistic decision tree in an example plan.
Example 7. Suppose a factory wants to build a new workshop to produce products for
10 years. The construction cost for a large workshop is 90K dollars, and 40K for a small
workshop. The factory expects to get some revenues from the products according to
the sales situation. Figure 3.7 is the decision tree that illustrates all possible ﬁnancial
solutions. Based on it, the expected 10-year revenues are computed as follows:
Rlarge = 0.8 × 90 × 10 + 0.2 × (–10) × 10 – 90 = 610K
Rsmall = 0.8 × 30 × 10 + 0.2 × 20 × 10 – 40 = 240K
The result signiﬁes that building a large workshop is better than building a small one.
◻
There is a hypothesis that people are rational in economics. Sometimes, a probabilistic
approach is hard to measure a consumer’s relative preference based on repeated trials.
In this way, only the objective data is used to study uncertainty, rather than subject-
ive probability. However, it is argued that this probabilistic approach is not complete
enough to cover all situations. For example, a woman agreed to go out with a man
with a probability of 20% in all the past invitations. But one cannot infer that a par-
ticular man has a chance of 20% to invite her out, because the romantic attraction
is uncertain at all [81]. Apparently, such epistemic uncertainty must be handled by
adding more information/knowledge.
3.3.2.2 Info-Gap-Based Credit Risk Analysis
Besides probability theory, info-gap theory also initiates a new way to assist economic
risk evaluation during decision making [20].
Example 8. In offering loans, ﬁnancial institutions try to make optimal decisions to
manage credit risk by reducing potential losses from arrears on loans. In credit risk
analysis [21], they can classify customers into categories 1, 2, ⋅⋅⋅, N of different credit
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
61
rates. The corresponding probability of default (arrears) in category i (1 ≤i ≤N) is de-
noted as di, and d = (d1, d2, ⋅⋅⋅, dN). Let
∼
d be the best estimate of d, and ! constrain the
scope of uncertainty. In info-gap theory, the uncertainty model of d can be represented
as a nested subset:
U(!,
∼
d) = {d : d =
∼
d + %, %TC–1% ≤!2}
(where ! ≥0),
where C is an N × N positive symmetric matrix, whose entry Cij is a model parameter
which could be taken as an element of a correlation matrix.
Let li and ri denote the loan amount and interest rate to customers in category i.
Let l = (l1, l2, ⋅⋅⋅, lN) and r = (r1, r2, ⋅⋅⋅, rN). The total loan amount L = l1 + l2 + . . . + lN.
Further, let $ and 1 denote the fractions of loss and proﬁt in terms of the percentage
of L. To ensure the normal business, two performance requirements must be satisﬁed.
That is, the loss is not greater than $ ∗L, and the proﬁt is not less than 1 ∗L.
Based on the above uncertainty model, as well as the loss and proﬁt requirements,
one can compute the relations among the arrear probabilities, loan amounts, and
interest rates with the system model:
RobustLoss = max
{
{
{
! : ( max
d∈U(!,
∼
d)
lTd) ≤$ ∗L
}
}
}
RobustProﬁt = max
{
{
{
! : ( min
d∈U(!,
∼
d)
(lTr – l󸀠Td)) ≥1L
}
}
}
,
l󸀠is a vector, and l󸀠
i = (1 + ri) ∗li (i = 1, 2, ⋅⋅⋅, N). lTr expresses the income from loan
interests, and l󸀠Td expresses the loss due to arrears.
With the analysis of the credit risk system model, the ﬁnancial institution can
make the following decision: the higher probability of arrear a customer has, the
higher interest rate and less amount of loan the bank should assign to him/her. The
robust-optimal decisions can be obtained by maximizing RobustLoss and RobustProﬁt
with respect to decision variables such as l and r. For detailed numerical analysis,
please refer to Ref. [21].
◻
Apart from credit risk analysis, info-gap decision theory is also applied to invest-
ment risk analysis, policy formulation, micro-economics of demands, and the equity
premium puzzle [20].
3.3.2.3 Derived Uncertainty Theory-Based Stock/Bond Analysis
Liu [95] applies the derived uncertainty theory to resolve some ﬁnancial problems
related to stock, insurance, and currency. It is assumed that a stock price follows a
Authenticated
:23 PM

62
3 Addressing Context Uncertainty
canonical process, containing a sequence of uncertain variables (C0, C1, . . . , Cn) within
a time or space interval, where
–
C0 = 0 and almost all sample paths are Lipschitz continuous;
–
Ct ∈{C0, C1, . . . , Cn} has stationary and independent increments;
–
every increment Cs+t-Cs is a normal uncertain variable with an expected value 0
and variance t2.
Let r denote the riskless interest rate of a stock. The bond price Bt and stock price St
can be estimated as: (1) d(Bt) = rBtd(t) and (2) d(St) = eStd(t) + 3Std(Ct), where r, e
and 3 are real numbers, e is the log-drift, 3 is the log-diffusion, and Ct is a canon-
ical process. The ﬁrst equation states that the bond price Bt increases exponentially
over time t, considering the time value of money. The second equation is a derivation
of Black-Scholes’s mathematical model [27] of a ﬁnancial market, which is a founda-
tion of the widely used Black–Scholes formula for calculating the stock prices of the
European-style options. It indicates that the stock price St can be determined by both
the canonical process Ct and the time value of money. The above single stock model is
further extended to a multi-factor stock model, addressing multiple stocks’ prices and
a number of inﬂuential factors for investment portfolio.
In a similar fashion, a ruin index [95] is deﬁned to measure the ruin possibility of
an insurance company, where the ruin time is calculated to estimate the time when
the ruin happens. Also, currency models can be established to estimate the uncertain
exchange rate as well.
3.3.3 In Engineering
Uncertainty exists in engineering risk management, machine control, and life-cycle
assessment. Probability theory serves as the major tool to model and quantify involved
uncertainty in the engineering ﬁeld.
3.3.3.1 Leveled-Uncertainty Framework for Engineering Risk Management
Like economic risk analysis, engineering risk management is concerned about risk
analysis and evaluation. According to the decision requirements and degrees of
quantitative assessment, Paté-Cornell outlines a six-leveled uncertainty handling
framework for engineering risk analysis [114].
Level-0 only needs to identify hazard (“yes” or “no”). Therefore, it doesn’t involve
any quantitative risk assessment. For example, if the hazard of drowning in the pond
is identiﬁed and the cost of building a fence around the pond is cheap, then the de-
cision can be easily made without the need of a thorough analysis. This bottom-level
treatment suits the cases where the decision is obvious with a low execution cost.
Level-1 treatment only considers the worst case which may cause a maximal loss.
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
63
Level-2 treatment considers the quasi-worst case if the worst case is uncertain or
unlikely happens.
Level-3 treatment aims at the best (most probable) estimate or a mean value.
Level-4 treatment calls for detailed analysis, and most of the time probabilistic
risk analysis is needed for providing a more sufﬁcient description of a certain risk
than the lower levels.
While Level-4 treatment estimates only a single risk curve, Level-5 treatment
considers a set of risk curves. These curves may come from the opinions of differ-
ent experts, and are analyzed through a more sophisticated statistical or Bayesian
method.
With the increase of treatment levels, uncertainty is presented in more details,
and processed through a more thorough analysis. The choice of uncertainty treatment
level depends on problem requirements. If users only want to focus on the worst/best
situation, then a full analysis of probability distribution will not be necessary.
3.3.3.2 Probabilistic Engineering Risk Analysis
Back to the 1970s, probabilistic risk analysis has been used to evaluate the risk of oper-
ations of nuclear power plants [136]. In probabilistic risk analysis, two data structures,
i.e., Event Tree Analysis (ETA) and Fault Tree Analysis (FTA), are usually adopted [26].
They both model the problem or process in the form of tree, and can make both
quantitative and qualitative analysis.
Event Tree Analysis (ETA) illustrates an inductive process from reasons to res-
ults. It starts from an initial event, constructs the tree following casual relations, and
comes up to some outcome events. ETA can aid to make predictions. In comparison,
Fault Tree Analysis (FTA) plots a deductive process from results to reasons to facilit-
ate discovery of some potential risks or failure causes. ETA and FTA can be combined
for synthetic research.
Example 9. Figure 3.8(a) is an ETA tree example in measuring a dam’s reliability,
where I is an initiating event, E1, E2, E3 are different events, and S is an outcome
event. The possibility of outcome S can be gained by P(S) = P(I)P(E1)P(E2)P(∼E3).
Figure 3.8(b) shows a top-down analysis by listing factors causing dam collapse.
◻
I
(Initial Event)
S
(Outcome Event)
E1 (Rain≥100mm)
~E1 (Rain<100mm)
E2 (Flood)
~E2 (No Flood)
~E3 (Dam is safe)
E3 (Dam collapse)
OR
Dam collapse
Engineering
Quality problem
OR
Lack of experience
Jerry-build
Flood
(a)
(b)
Figure 3.8: Dam risk analysis example: (a) Event Tree Analysis (ETA); (b) Fault Tree Analysis (FTA).
Authenticated
:23 PM

64
3 Addressing Context Uncertainty
Following the classical probability theory, Monte Carlo and Bayesian methods play a
role in risk analysis. Monte Carlo risk assessment (MCRA) is exploited to approximate
real values of parameters. It assigns a probability value to each sensitivity parameter
in many trials, and then summarizes the results in a statistical way. Greenland [57]
describes the use of MCRA and Bayesian uncertainty assessment in analyzing skin
cancer risks from coal tar containing products.
3.3.3.3 Derived Uncertainty Theory-Based Reliability Analysis
Liu [95] analyzes system reliability with respect to the factors of lifetime, production
rate, cost, and proﬁt based on Derived Uncertainty Theory. The term reliability index
is used to measure the degree of hazard, representing the uncertainty of system
reliability.
Assume a cascading system is composed of n components. It fails if any of its
components does not work. Let .1, .2, ⋅⋅⋅, .n denote the lifetimes of the n components.
The system lifetime is: min(.1, .2, ⋅⋅⋅, .n).
If the system lifetime is expected to be longer than T, the system reliability
function can be deﬁned as: f(.1, .2, ⋅⋅⋅.n) = min(.1, .2, ⋅⋅⋅, .n) – T.
Then the system can work reliably, if and only if
f(.1, .2, ⋅⋅⋅.n) = min(.1, .2, ⋅⋅⋅, .n) – T ≥0.
The reliability index of the system is deﬁned as:
Reliability = M(f(.1, .2, ⋅⋅⋅.n) ≥0) = !.
Due to M(. ≤x) = U(x), we can get
M(f(.1, .2, ⋅⋅⋅.n) ≥0) = 1 – M(f(.1, .2, ⋅⋅⋅.n) < 0) = 1 – U(0) = !
(3.4)
According to the 99-Table method, if .i(i = 1, 2, ⋅⋅⋅, n) can be represented as the
following 99-table,
Ui(x)
0. 01
0. 02
⋅⋅⋅
0. 99
x
xi
1
xi
2
⋅⋅⋅
xi
99
then the uncertainty distribution of f(.1, .2, ⋅⋅⋅, .n) = min(.1, .2, ⋅⋅⋅, .n) –T can be
represented as the 99-table:
U(x)
0.01
0.02
⋅⋅⋅
0.99
x
min
1≤i≤n xi
1 – T
min
1≤i≤n xi
2 – T
⋅⋅⋅
min
1≤i≤n xi
99 – T
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
65
Then according to eq. (3.4), we can use the 99-Table of f(.1, .2, ⋅⋅⋅.n) to compute the
value of !. In the uncertainty distribution 99-table, we ﬁnd the value x = 0, then the
corresponding U(x = 0) = 1 – !. Thus the risk index ! of the cascading system can be
calculated.
With the risk index, decision makers can evaluate systems, make predictions, and
take prevention actions. Similarly, the reliability index can also be presented based on
a Boolean system.
3.3.3.4 Fuzzy Control Systems
Fuzzy theory sets a good theoretic foundation for machine control, an dispensable
component in engineering. It enables ones to cope with uncertainty that makes the
system too complicated to be described in a precise way. A fuzzy logic-based fuzzy
control system [52], as a mathematical system, analyzes analog input values that take
continuous values between 0 and 1 in the physical world rather than binary discrete
value of either 0 or 1 (true or false). Fuzzy control system was ﬁrst demonstrated to be
superior in the management of Sendai railway in the 1980s [51]. Since then, it becomes
more and more popular. Many electronic equipments today use fuzzy control systems
to execute operations automatically. For instance, a washing machine can automatic-
ally adjust the wash-cycle according to the amount of water and detergent. A vacuum
cleaner can change the suction power in accord with dust sensors. Canon develops a
fuzzy control system to manage the auto-focus of a camera, involving 13 rules and 1.1
kilobytes of memory [51].
The typical structure of a fuzzy control system is outlined in Figure 3.9. The in-
put values are real numbers, such as brake temperature. The fuzzy controller fuzziﬁes
the input values to a fuzzy set with truth degrees, and executes the appropriate rules
accordingly. The consequents of the rules are combined and defuzziﬁed into real
numbers for output.
Example 10. A control rule example is:
IF brake temperature is warm AND speed is not very fast,
input
output
Real numbers
centroid method
max-min inference
method
Fuzzify
Defuzzify
execute control
rules
Fuzzy sets with membership functions
Figure 3.9: A typical fuzzy control diagram.
Authenticated
:23 PM

66
3 Addressing Context Uncertainty
THEN brake pressure is slightly decreased.
Let A and B be the two antecedents of the rule and C be the consequent of the rule.
Let x, y, and z be the crisp values of brake temperature, speed, and brake pressure,
respectively. The fuziﬁcation process is to get the truth value of fuzzy set A and B,
denoted as fA(x) and fB(y). Applying the classical “max-min” inference method, the
truth value of C is: fC(z) = min[fA(x), fB(y)]
According to the fuzzy sets and corresponding truth values, the crisp value of
brake pressure can be gained. If there is more than one rule that can be triggered
by the input, the results of all the rules are combined and defuziﬁed. One method
to do it is called “centroid” method [51]. It summarizes all the results, and calculates
the “center of mass”. The crisp brake pressure is then output to control the brake. The
“max-min” inference method and “centroid” method are only two alternatives of the
fuzzy controller, and some other methods also exist.
◻
Nowadays, many fuzzy-control chips have been applied in fuzzy control systems.
3.3.3.5 Probabilistic and Fuzzy Life Cycle Assessment
Life cycle assessment (LCA) refers to the evaluation of environmental and social im-
pacts relating to the life cycle of products, going through all the stages of the products
from raw materials, semi-ﬁnished products, ﬁnished products, to products’ waste and
recovery. By life cycle analysis, ones can assess damages, make company strategies,
and optimize environment impacts. For instance, through the LCA, Levi 501 jeans Inc.
found that the climate impacts of their products mainly lie in the use of dryer in the
customer phase [89]. Consequently, an air drying movement is advocated to save
energy and reduce carbon footprints.
In order to measure different impacts of a product system, and hereby approxim-
ate environmental outcomes, various uncertainties arising from data sources, models,
measurement errors, preferences of analysts and physical systems need to be coped
with for comparison and decision-making purposes [12]. Here, three speciﬁc kinds of
uncertainties particularly exist in LCA [55, 69, 97].
–
Parameter uncertainty is caused by the incomplete knowledge of parameters,
mainly resulting from the measurement errors in input data.
–
Scenario uncertainty lies in choices regarding functional units, natural context,
use of environmental threshold, and some other factors.
–
Model uncertainty exists in mathematical relationships, caused by the limitations
in modeling processes.
Among them, parameter uncertainty is the most common uncertainty in LCA. Monte
Carlo simulation, Bayesian statistics [60], and fuzzy theory have been applied to
LCA. Although some researchers think that fuzzy sets are more suitable than random
sampling methods, not only because of the ambiguity in LCA but also because of the
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
67
less computing time with fuzzy theory [131, 132], Monte Carlo is the most widely used
approach in the literature [97]. In the following, we illustrate a fuzzy LCA approach
and a probabilistic LCA approach.
Fuzzy LCA Approach. R. R. Tan [131] presents a matrix-based fuzzy model to deal
with data variability during the evaluation of pollutant emission in LCA. Assume in a
two-process life cycle system, two commodities P1 and P2 are involved, and pollutant
E is released. As illustrated in Figure 3.10, Process I consumes P2 and produces P1 and
pollutant E, and process II consumes P1 and produces P2 and pollutant E. Through the
life cycle assessment, one can compute the total emissions of pollutant E.
The amount ratios of commodities P1, P2, and pollutant E are uncertain, which
can be represented as a fuzzy number. As shown in Figure 3.11, the amount ratio of
commodity 1 to commodity 2 in Process I is represented as a fuzzy number, written
as (0.5, 0.6, 1)T, where 0.5 and 1 are the least plausible values as the lower and upper
boundaries of the ratio, 0.6 is the most plausible value, and the subscript T expresses
that the fuzzy number is in the shape of triangle distribution. At any given degree of
possibility !, it is possible to ﬁnd a corresponding interval (L, U). In an !-cut where ! =
0.4, the resulting interval is (0.54, 0.84). Other amount ratios of the life cycle system
are shown in Figure 3.12.
According to the uncertain ratios in the second and third columns, and the output
amount of the ﬁnal column, the emission x of pollutant E can be calculated based on
the following matrix-based fuzzy model.
gU,! = BU,!A–1
L,!f,
gL,! = BL,!A–1
U,!f
Process II
50kg
Process I
Pollutant E
Product P2
Product P1
50kg
15kg
150kg
Output
100kg
250kg
250kg
Output
Figure 3.10: A life cycle system example [131].
Degree of possibility
The ratio of P1/P2 in Process I
α–cut, α=0.4
0.5
0
1
0.6
1.0
0.4
ratio
Figure 3.11: A triangular distribution of
fuzzy numbers in LCA.
Authenticated
:23 PM

68
3 Addressing Context Uncertainty
Process I
Process II
Ratio of Amount
Output
Amount
–1
(3, 5, 6)T
(0.9, 1, 1.1)T
(0.5, 0.6, 1)T
–1
(0.05, 0.06, 0.07)T
100kg
0kg
x
Commodity P1
Commodity P2
Pollutant E
Matrix A
Matrix B
Vector f
Vector g
Figure 3.12: A matrix-based fuzzy model for LCA [131].
In this model, at the degree of possibility !, gU,! and gL,! are the upper and
lower bounds of the emission inventory vectors, BU,! and BL,! are the upper and lower
bounds of the emission intervention matrix, AU,! and AL,! are the upper and lower
bounds of commodity-relevant technology matrix, and f is the functional unit vector
of output. With this equation, the lower and upper bound of emission can be estimated
at the degree of possibility !.
According to Figure 3.12, at ! = 1,
gU,! = ( x ) , BU,! = ( 0.06
1 ) ,
AL,! = ( 0.6
– 1
–1
5 ) , f = ( 100
0 )
Using the fuzzy model, we can get gU,! = 65. That is, the most plausible value of pol-
lutant E is 65kg, as shown in Figure 3.10. Varying the value of !, we can gain different
emissions, forming a fuzzy distribution of pollutant E. The environmental impacts can
thus be derived.
Probabilistic LCA Approach.
Figure 3.13 shows a Monte Carlo-based incin-
eration system [70]. Wastes are collected in different cities, and transported to
incineration plants. After incineration, electricity is recovered and ash is disposed.
Assume the probability distribution of CO2 emission satisﬁes the normal distribution
N(1kg, 0.05kg) in the use of diesel. Suppose one chooses 0.9kg as one CO2 emission
Collection
Transportation
Ash disposal
Incineration
Use of diesel
Recovery of
electricity
Waste
Figure 3.13: An incineration system
example [70].
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
69
value. A concrete total CO2 emission from the transportation process and the inciner-
ation process can then be computed according to a predeﬁned formula. To repeat this
process for hundreds of times with different parameter settings by the Monte Carlo
method, one can obtain a statistic ﬁnal result of CO2 emission [70].
3.3.4 In Ecology
Ecological problems such as climate change and population explosion pressurize
human beings. Good forecasting of ecological phenomena can help policy making
and social/environmental planning to relieve some potential problems. Inherently,
uncertainty handling runs throughout all the analytic processes for prediction.
3.3.4.1 Probabilistic Forecasting of Climate Change
Along with the global warming in the recent decades, the recognition of the im-
portance of climate-change research is increasing [106], where scientists usually
need to consider various uncertainty in anthropogenic emissions of greenhouse
gases, anthropogenic emissions of short-lived climate-relevant air pollutants, climate
sensitivity, oceanic heat uptake, speciﬁc aerosol forcing, and so on [142].
Judgment of climate change is usually evaluated according to two factors, i.e.,
the amount of evidence and the degree of scientiﬁc agreement or consensus [107].
Figure 3.14 shows four states of knowledge about climate change, giving rise to two
speciﬁc kinds of uncertainty to be considered [106].
–
Uncertainty in Empirical Quantities. Empirical quantities refer to the properties of
the real world which can be measured, including quantities in natural, engineer-
ing, and social sciences. For example, the thermal efﬁciency of a power plant, the
oxidation rate of atmospheric pollutants, or judgmental biases in psychology are
all empirical quantities.
–
Uncertainty in Model Functional Forms. To describe the reality, one needs to
choose an appropriate model as a simpliﬁed approximation of the underly-
ing causal structure. The choice and determination of the model possesses
uncertainty.
Scientific agreement
Well
established
Competing
explanations
Amount of evidence
Established but
incomplete
Speculative
Figure 3.14: State of knowledge about climate
change [107].
Authenticated
:23 PM

70
3 Addressing Context Uncertainty
Uncertainty in empirical quantities can be due to either natural variability or know-
ledge uncertainty, while uncertainty in model functional forms is neither.
Atmosphere and ocean are two components related to climate change, and AO-
GCM (Atmospheric and Oceanic GCMs) is a combination of them to make full climate
change analysis. A General Circulation Model (GCM) is widely used to model the
circulation of atmosphere or ocean [134]. A simple general circulation model has
a dynamical core with some equations to predict or diagnose phenomena. One
fundamental model of radiative equilibrium is the zero-dimensional model, which
calculates the temperature of the earth as: T =
4√(1–a)S
4%3 , where a and % are the earth’s
albedo and effective emissivity, respectively, and S and 3 are solar constant and
Stefan–Boltzmann constant, respectively.
However, the above model doesn’t consider the inﬂuence of atmosphere around
the earth [141]. For instance, under the circumstance of global warming, the green-
house gas must be taken into account and evaluated. The inﬂuence of these paramet-
ers to the temperature can be measured with some techniques of uncertainty analysis,
such as sensitivity analysis, scenario analysis, and Monte Carlo simulation ana-
lysis [80]. Distinct from the limited scenarios in sensitivity analysis and scenario
analysis, Monte Carlo method can simulate quite a number of scenarios by generat-
ing some values from the probability distribution. In addition, Bayesian Method can
also play a role in the climate change analysis [56]. Basically, three steps are involved
in the Bayesian statistical paradigm [80]:
–
Determine the joint prior probability distribution of the parameters.
–
Generate a likelihood function through the joint conditional distribution.
–
Combine the prior distribution and the likelihood function to determine the
posterior probability distribution.
3.3.4.2 Probabilistic Forecasting of Population
Confronted with the threat of global population increase, population forecasting
constitutes another focal research point in ecology [6, 28, 98, 99]. Basically, three
factors (i.e., fertility, mortality, and migration) affect demographic change. Popula-
tion forecasting needs to analyze each individual factor and their combined effects.
Throughout the analysis procedures, a variety of uncertainty lying in population
growth has to be coped with. According to the statistics made against demographic
forecasting from 1985 to 2005, errors in forecasting come from four sources, which
are model mis-speciﬁcation, parameter estimation, random variation, and informed
judgment [7, 29].
To address them, probabilistic forecasting methods are introduced to deal with
some component uncertainty in the manner of stochastic population renewal. Three
complementary approaches are particularly developed to estimate forecast uncer-
tainty, which are model-based ex-ante error estimation, expert-based ex-ante error
estimation, and ex-post error estimation, relying on the extrapolative techniques,
expert knowledge, and past forecast, respectively.
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
71
Besides, Bayesian Method is also applied to population forecasting with some
ﬂexibility [1]. The relation between the uncertain parameter ( and observed data
y{T} = {y1, y2, ⋅⋅⋅yT} is formulated as:
f((|y{T}) = f(y{T}|()f(()
f(y{T})
,
where f(() is the prior distribution.
According to Bayes Theorem, precedent values of yT can be predicted:
f(yT+1, ⋅⋅⋅, yT+K|y{T}) = ∫f((|y{T})
K
∏
k=1
f(yT+k|y{T+k–1}, ()d(.
In this way, the next K values can be calculated with the joint predictive and posterior
distribution based on the observed data y{T}.
3.3.4.3 Info-Gap-Based Decision Making in Conservation Management
Conservation management intends to avoid potential risk of population decline or ex-
tinction, and maximize opportunities of population persistence. Conservation biolo-
gists usually need to decide appropriate actions taken for endangered and threatened
species. In the following example, a utility measurement is used to represent the
environmental outcome of a conservation action.
Assume there are three options of conservation management which are transloca-
tion, new reserve, and captive breeding, denoted as aj (j=1, 2, 3). The three options may
lead to four possible outcomes which are poaching, loss of habitat, demographic acci-
dents, and disease. Let pi (i=1, 2, 3, 4) denote the probability of each outcome. Further,
let vij represent the utility association between the j-th option and the i-th outcome.
Then the expected utility of the j-th option is
EV(aj) =
4
∑
i=1
pivij (where j = 1, 2, 3).
In the info-gap decision theory, the uncertainty vector p and v can both be represented
with a subset, Up(!,
∼p) and Uv(!,
∼v), satisfying
pi –
∼pi
∼pi
≤!,
vij –
∼vij
∼vij
≤! (where ! ≥0, i = 1, 2, 3, 4, j = 1, 2, 3).
There is a critical value EVc below which the utility is unacceptable. Then the
robustness function for option aj (j = 1,2,3) can be formulated by
Robust(aj, EVc) = max{! :
min
p∈Up(!,
∼p)
v∈Uv(!,
∼v)
4
∑
i=1
pivij ≥EVc}.
Authenticated
:23 PM

72
3 Addressing Context Uncertainty
Robust is the robustness function, representing the greatest horizon of uncertainty up
to which all probabilities and utilities result in an expected utility no worse than EVc.
Through experiments, when EVc = 0.07, the option “new reverse” holds the larger
robustness Robust = 0.34, compared to the other two options. Therefore it is a good
choice.
3.3.5 In Information Science
In the information domain, there exist unreliable information sources, system errors,
imprecise information gathering methods, and/or model restrictions [108]. Unreliable
information sources may be due to fault-reading instruments, incorrect input forms,
etc. System errors lie in transmission noises, or delays in processing updated transac-
tions, etc. Information gathering may be affected by constantly varying phenomena.
In the modeling process, some approximation techniques may be required and used,
which result in uncertainty as well.
3.3.5.1 Probabilistic and Fuzzy Social Networking
A social network accommodates various uncertainty relationships of people such as
the belief degree of two friends to be handled, where probability theory and fuzzy
theory are generally brought in to deal with the uncertain situations. We use a music
recommendation example to show how the two approaches are applied.
(1) Probability Approach
People may easily be inﬂuenced by friends in choosing music [3]. Figure 3.15 gives a
probabilistic social networking example, where directed edges indicate the inﬂuence
upon song selection by each other. For example, Alice inﬂuences Kim with a prob-
ability of 0.3. Three probabilistic tables, Preference(Name, Genre, Prob.), Song(Name,
Genre, Prob.), and MusicInﬂuence (Name1, Name2, Prob.) record users’ song prefer-
ences, songs’ genres, and music inﬂuence with certain probabilities, respectively.
To ﬁnd out users who probably like song A, the following SQL can be issued:
SELECT Preference.Name
FROM
Preference P, Song S
WHERE
P.Genre=S.Genre AND S.Name=‘A’
The probability that Kim likes song A is thus 0.75 × 0.1 = 0.075. Furthermore, con-
sidering MusicInﬂuence in Figure 3.15 and song A is of genre Country with probability
0.1 and Pop with probability 0.5, Alice, Bob, and Fred may all recommend song A to
Kim. The ﬁnal result probability that Kim accepts song A is: 0.075 × (1 – (1 – 0.3) × (1 –
0.9) × (1 – 0.7)) = 0.073.
(2) Fuzzy Approach
Considering the ambiguous properties of human thinking, fuzzy sets are employed to
represent and evaluate users’ social relationships. Some intuitive linguistic concepts
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
73
Alice
Bob
0.8
Kim
0.3
0.9
Fred
0.7
(b)
Name
Genre
Probability
Alice
Classical
0.8
Alice
Pop
0.8
Bob
Rap
0.5
Bob
Pop
0.5
(a)
Name1
Name2
Probability
Alice
Bob
0.8
Alice
Kim
0.3
Bob
Kim
0.9
Fred
Kim
0.7
(c)
Name
Genre
Probability
A
Country
0.1
A
Pop
0.5
B
Rap
0.5
C
Classical
0.8
Kim
Country
0.75
D
Country
0.6
Fred
Pop
0.2
Figure 3.15: A graphical model with factors representation: (a) MusicInﬂuence; (b) Preference;
(c) Song.
Table 3.4: Fuzzy MusicInﬂuence relation and similarity matrix.
(a) MusicInfluence
Name1
Name2
Inﬂuence Degree
Alice
Kim
Weak
Alice
Bob
Strong
Bob
Kim
Strong
Fred
Kim
Medium
(b) Similarity
Inﬂuence
Strong
Medium
Weak
Strong
1
0.7
0.2
Medium
0.7
1
0.4
Weak
0.2
0.4
1
(like strong, weak or media), rather than a raw probability value, can be used to
express the users’ inﬂuence factor upon song selection, as shown in Table 3.4(a).
A similarity relation of inﬂuence can be deﬁned in a matrix (Table 3.4(b) to replace
classical membership functions of fuzzy sets – strong, weak and medium). Here, 0.7
means that “medium” satisﬁes the concept “strong” with a degree of 0.7. Users who
strongly inﬂuence Kim include (Bob, 1), (Fred, 0.7), and (Alice, 0.2).
3.3.5.2 Uncertain Decision Making in Project Management
(1) Fuzzy Approach
Efﬁcient project management needs to be aware of project goal and various con-
straints toward a high-quality project output. Three factors (project cost, project
Authenticated
:23 PM

74
3 Addressing Context Uncertainty
duration, and project quality) can generally quantify the internal efﬁciency of project
management in terms of how well the project is managed and executed [11, 126].
Greenland [47] conducts a fuzzy decision making to evaluate the internal efﬁ-
ciency of project management. It describes the project cost (PC), project duration (PT),
and project quality (PQ) through fuzzy sets. For instance, the project cost is fuzziﬁed
into very low, low, medium, high, and very high. Different factors have different prior-
ities to the project management, which are deﬁned as project cost weighting factor
(PCWF), project time weighting factor (PTWF), and project quality weighting factor
(PQWF). It satisﬁes PCWF + PTWF + PQWF = 1.
The project management internal efﬁciency can be measured based on some fuzzy
rules like “IF project cost is low AND project cost weighting factor is high, THEN
project management internal efﬁciency is very high.”
(2) Derived Uncertainty Theory Approach
The derived uncertainty theory is used to solve the project scheduling problem in the
form of uncertain programming. This process involves allocating resources to reduce
total cost and complete the project in time. In the project scheduling model [95], these
two factors are considered.
{
{
{
min E(C(x, .))
s.t. M(T(x, .) ≤T0) ≥!, x ≥0
Here, x is the allocating time vector needed for all activities. . is the uncertain duration
time vector of all activities. E(C(x, .)) is the expectation of the total cost C(x, .) to be
minimized, T is the complete time of the project which should be earlier than the due
date T0, with an occurrence level not less than !.
If the uncertain distribution of the total cost C(x, .) can be represented with a 99-
table:
U(C)
0. 01
0. 02
⋅⋅⋅
0. 99
C
c1
c2
⋅⋅⋅
c99
then the expectation E(C(x, .)) = (c1 + c2 + ⋅⋅⋅+ c99)/99.
3.3.6 In Databases
In the early attempts of database community, null values were commonly used to
manage uncertain information [38]. Until now, probabilistic, fuzzy, and possibilistic
databases constitute major ways for uncertain data management. A variety of uncer-
tainty handling efforts by the database community are illustrated, with an emphasis
on uncertainty database model and query processing.
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
75
Table 3.5: Codd table [72].
Supplier
Location
Product
Smith
London
Nails
@
NewYork
Bolts
Jones
@
Nails
Table 3.6: V-table [72].
Supplier
Location
Product
Smith
x
Nails
Brown
y
Bolts
Jones
x
Nails
Table 3.7: Conditional table [72].
Supplier
Location
Product
con
x
London
Nails
x=Smith
Brown
NewYork
Nails
x
̸= Simth
3.3.6.1 Early Attempts with Null Values
In the early attempts, null values are usually employed to represent incomplete in-
formation in databases. They can be viewed as variables, different from normal
constant values in relational databases.
In Ref. [72], three types of tables (shown in Tables 3.5, 3.6, and 3.7) are adopted to
express different levels of information incompleteness.
–
Codd-table uses symbol to represent an unknown value.
–
V-tables uses different symbols (such as x, y) to represent different unknown val-
ues. For example, Table 3.6 illustrates that although locations of Smith and Jones
are unknown, they are the same.
–
Conditional tables are extensions of V-tables, where an extra column con is added
to represent extra conditions. For example, the last column in Table 3.7 means that
nails are supplied either by Smith in London or by Brown in New York, but not by
both at a time.
Based on these tables, relational operators are extended. Abiteboul, Abiteboul, and
Abiteboul [2] makes a deep investigation into data-complexity in representing and
querying databases containing null values.
Authenticated
:23 PM

76
3 Addressing Context Uncertainty
3.3.6.2 Probabilistic Databases
Databases based on the classic probability theory, Monte carlo methods, and evidence
theory are described.
(1) Classic Probabilistic Data Management
In a tuple-wise probabilistic database relation, each tuple can be regarded as a de-
scription of a basic probabilistic event and associated with an explicitly given event
identiﬁer [42, 50], as shown in Table 3.8(a). The column Probability means the probab-
ility that a tuple belongs to the relation (e.g. tuple r1 belongs to the relation Restaurant
with a probability of 0.7, or r1 doesn’t appear in Restaurant with a probability of 0.3).
Tuples within a probabilistic relation as well as among relations are assumed to be in-
dependent. The probabilistic database is based on the possible world semantics which
is a probability distribution on all database instances [42]. It can be regarded as a ﬁ-
nite set of database instances with the same schema. Each tuple in the database may
or may not appear in a database instance, and the database instance is associated
with a probability. Table 3.8(b) shows all the possible worlds of relation Restaurant.
Table 3.8: Probabilistic Restaurant relation and its possible worlds.
(a) Probabilistic relation Restaurant
ID
RName
Type
Discount
Probability
r1
ABucks
Dinning
8
0.7
r2
AHut
Dinning
7
0.8
r3
AFC
Dinning
9.5
0.55
(b) Possible worlds of relation Restaurant
Possible world
Probability
Tuple list sorted by Discount
W1 = {r1, r2, r3}
0.7 ∗0.8 ∗0.55 = 0.308
r3, r1, r2
W2 = {r1, r3}
0.07 ∗(1 – 0.8) ∗0.55 = 0.077
r3, r1
W3 = {r2, r3}
(1 – 0.7) ∗0.8 ∗0.55 = 0.132
r3, r2
W4 = {r3}
(1 – 0.7) ∗(1 – 0.8) ∗0.55 = 0.033
r3
W5 = {r1, r2}
0.7 ∗0.8 ∗(1 – 0.55) = 0.252
r1, r2
W6 = {r1}
0.7 ∗(1 – 0.8) ∗(1 – 0.55) = 0.063
r1
W7 = {r2}
(1 – 0.7) ∗0.8 ∗(1 – 0.55) = 0.108
r2
W8 = ⌀
(1 – 0.7) ∗(1 – 0.8) ∗(1 – 0.55) = 0.027
-
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
77
Table 3.9: Probabilistic Customer and CustomerLoc relations.
(a) Probabilistic Customer relation (tuple-level uncertainty)
ID
CName
Interest
Probability
c1
Lily
Dinning
0.9
c2
Tom
Photo
0.6
c3
Jessica
Entertainments
0.8
(b) Probabilistic CustomerLoc relation (attribute-level uncertainty), where N(,, 32) is
normal distribution with probabilistic density function f(x) =
1
√203 e
– (x–,)2
232 , , is mean
and 32 is variance.
ID
CName
Location
l1
Lily
N(2, 1)
l2
Tom
N(5, 1)
l3
Jessica
N(3, 1)
The probability of each possible world can be computed by multiplying its tuples’
probabilities.
Besides tuple-wise uncertainty, attribute-level uncertainty (e.g., Location attribute
in the CustomerLoc relation in Table 3.9(b)) considers an attribute value as a set of
discrete possible values or continuous values modeled through a probability density
function [35, 36].
Querying over a probabilistic database will deliver a probabilistic result relation
where each result tuple is associated with a complex event expression which is a
Boolean combination of the events corresponding to the base tuples from which it
is derived. Evaluation of typical aggregate, join, range, top-k, as well as lineage and
correlation-based queries is done based on the possible world semantics.
(Aggregate Query) “Find the average discount of restaurants.”
SELECT AVG(Discount) AS avgDiscount
FROM
Restaurant
Each possible world in Table 3.8(c) leads to a partial result, and the ﬁnal outcome is
(avgDiscount, Prob.) = {(8.17, 0.308), (8.75, 0.077), (8.25, 0.132), (9.5, 0.033), (7.5, 0.252),
(8, 0.063), (7, 0.108), (NULL, 0.027)}. As the world ⌀doesn’t include any tuple, NULL
is returned as a result alternative according to [109].
Instead of listing all of the alternatives in exhaustive aggregation, some variants
of aggregation just consider a low bound, a high bound, or a mean aggregate value
of non-NULL alternatives with the conﬁdence setting as 1.0 (i.e., (7, 1), (9.5, 1), or
Authenticated
:23 PM

78
3 Addressing Context Uncertainty
Table 3.10: Join query result on relation RestInt.
ID
RName
Probability
t1
ABucks
0.7*0.9 = 0.63
t2
AHut
0.8*0.9 = 0.72
t3
AFC
0.55*0.9 = 0.495
(7.68,1)) for query efﬁciency [109]. Aggregation on data streams can also be computed
efﬁciently given a speciﬁc accuracy [75].
(Join Query) “Find restaurants of interest to customers.”
SELECT RName
FROM
Restaurant, Customer
WHERE
Restaurant.Type=Customer.Interest
The above join query involves operations over multiple probabilistic relations, and the
result is shown in Table 3.10. The approach of evaluating join queries on each possible
world, called the intentional semantics, is precise yet not efﬁcient, and the complex-
ity increases exponentially with the number of tuples. Dalvi and Suciu [42] present
an efﬁcient evaluation method without listing all possible worlds based on the exten-
sional semantics, where SQL queries are represented in an algebra and operators are
modiﬁed to compute the probabilities of output tuples. To avoid dependencies in the
querying process which may lead to incorrect results, they give a safe-plan algorithm
that can compute most queries efﬁciently and correctly, and prove that the complexity
of evaluating a query with a safe query plan is in PTIME. However, there exist some
queries with a #P-complete data complexity, implying that these queries do not admit
any efﬁcient evaluation method. Two out of the 10 TPC/H queries fall in this category,
and only when all their predicates are uncertain. For these queries, a few techniques
have been developed [40, 42, 118], including aggressively using previously computed
query results (materialized views) to rewrite a query in terms of views; using heuristics
to choose a plan that avoids large errors; using a Monte Carlo simulation algorithm,
which is more expensive but can guarantee arbitrarily small errors.
(Range Query) “Find customers whose locations are between 1km and 3km.”
SELECT CName
FROM CustomerLoc
WHERE
Location≥1km AND Location≤3km
In sensing and moving objects’ applications, data recordings usually change con-
tinuously, making the query of an interval more meaningful than a single value.
In Table 3.8(b), each customer’s location is considered to be a continuous random
variable, satisfying the normal distribution N(,, 32), where , is the mean, and 32
is the variance [35]. For c1(Lily), her location within [1km, 3km] has a probability
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
79
∫
3
1
1
√20e– (x–2)2
2
dx = 0.68. Combined with the overall tuple probability 0.9, one can get
Lily : 0.68∗0.9 = 0.61. Similarly, one can compute other customers’ probabilities, with
the ﬁnal result as {(Lily, 0.61), (Tom, 0.01), (Jessica, 0.38)}. As the value distribution of
an attribute is regarded as a probabilistic density function, this approach incurs a high
complexity due to the expensive cost of integration computation. Frequently one must
resort to approximation.
(Top-k Query) “Find top 2 restaurants with the highest discounts.”
SELECT * FROM Restaurant
ORDER BY Discount DESC
LIMIT 2
Several factors inﬂuence result ranking, including Discount and Prob. of each tuple,
probability of each possible world, and the ranked tuple position in the possible
world. They make top-k ranking an intriguing issue, leading to multiple query
semantics [71].
(1) Re, Dalvi, and Suciu [118] rank query results based on tuples’ probabilities, and
returns < r2, r1 > (Table 3.8), since their probabilities are among the top-2.
(2) Uncertain Top-K (U-TopK) Query [129] returns a tuple vector with the maximum
aggregated probability of being top-k across all possible worlds. It returns < r3, r1 >,
contributed by world W1 and W2 as the result.
(3) Uncertain Rank-k (U-kRanks) Query [129] returns a list of k tuples, where the
i-th tuple appears at rank i with the highest probability in all possible worlds. The
top-2 query results are thus < r3, r1 >, since r3 has the highest probability to be ranked
ﬁrst in all possible worlds (r3 is ranked ﬁrst in possible world W1, W2, W3, W4 whose
corresponding probabilities sum up to be 0.55), and r1 has the highest probability to
be ranked second in all possible worlds.
(4) Probabilistic Threshold Top-k Query [66] returns all tuples whose probabilities
in the top-k list are larger than a pre-speciﬁed threshold. When threshold = 0.25, the
returned results are < r1, r3 > (r2 is ignored with a probability of 0.132).
(5) Expected Rank Query [39] returns a list of k tuples that have the highest ex-
pected ranks, computed by summarizing the product of ranked tuple position and
probability in each possible world. The top-2 results are < r3, r1 >.
(6) Expected Score Query [39] returns a list of k tuples that have the highest expec-
ted discounts, computed by multiplying tuple’s Discount and Prob. The top-2 results
are < r1, r2 >.
(7) Parameterized Ranking Function based Query [92]. A parameterized ranking
function is ﬁrst deﬁned Υ9(t) = ∑i>0 9(t, i)Pr(r(t) = i), where r(t) is a random variable
denoting the rank of t in all possible worlds, and 9(t, i) is a weight function: T ×N →C
(T is the set of all tuples, N is the set of all ranking positions, and C is the set of complex
numbers). The top-k query returns k tuples whose |Υ9(t)| values are among the top-k.
Authenticated
:23 PM

80
3 Addressing Context Uncertainty
In Table 3.8, when the weight function is set to 9(t, i) = 4 – i, meaning the weight
function is independent of t, the top-2 results are < r1, r3 >. By setting appropriate
weights, the parameterized ranking function can approximate many of the previously
proposed ranking semantics, except for the Uncertain Top-k Query.
k-nearest neighbor queries over uncertain data can be classiﬁed into two types:
one is PNN (Probabilistic Nearest Neighbor), which ranks uncertain objects based on
their probabilities of being the nearest neighbor of a query point [4, 25, 33]; the other
approach is based on a distance metric, where similar query semantics like uncertain
top-k query, uncertain rank-k query, expected score query, and expected rank query
can be applied [24, 34, 96, 149].
In evaluating top-k queries based on ranking functions, two efﬁcient techniques
are mainly involved.
(1) Generating function technique. Li et al. [92] presented a polynomial algorithm
based on generating functions avoiding listing all possible worlds in exponential time
complexity. Let a list of tuples Ti = {t1, ⋅⋅⋅, ti} in a nonincreasing order by their score,
a generating function can be constructed in the form of
F i(x) = ( ∏
t∈Ti–1
(1 – Pr(t) + Pr(t) ⋅x)) (Pr(ti) ⋅x) = ∑
j≥0
cjxj to compute the probability that
the tuple ti is at rank j (i.e., Pr(r(ti) = j)). In this formula, the coefﬁcient cj of xj in F i is
exactly the value of Pr(r(ti) = j). Besides, F i can be obtained from F i–1, which further
simpliﬁes the calculation. This approach is also applied in Ref. [22, 91].
(2) Poisson binomial recurrence technique. The probability of a tuple ti to be ranked
in the top-k list can be computed as Pr(ti, j) = Pr(ti)Pr(Sti, j – 1), where Pr(ti) is the
probability that ti appears, Sti is the set of all the tuples that satisfy the query and
are ranked higher than ti, and Pr(Sti, j – 1) is the probability that j – 1 tuples in
Sti appear in possible worlds. Based on the Poisson binomial recurrence Pr(Sti, j) =
Pr(Sti–1, j – 1)Pr(ti) + Pr(Sti–1, j)(1 – Pr(ti)) [88], the value of Pr(Sti, j) can be computed
recursively. Poisson binomial recurrence and variants have been applied in Ref. [66]
to efﬁciently answer probabilistic threshold top-k queries, in Ref. [93] to do prun-
ing in inverse ranking query (for a user-speciﬁed query point q, it computes all the
possible ranks of q with probability greater than a pre-deﬁned threshold), in Ref.
[24] to compute probabilistic similarity ranking on uncertain vector data, and other
works [23, 65, 144, 145].
(Lineage Query) “Find customers contributing to ‘ABucks’ in RestInt.”
SELECT Customer.CName FROM RestInt, Customer
WHERE
lineage(RestInt, Customer) AND
RestInt.RName=‘ABucks’
The Trio system [5] accommodates tuples’ lineage/provence information. For the rela-
tion RestInt (Table 3.10), lineage(t1) = {r1, c1}, lineage(t2) = {r2, c1}, lineage(t3) = {r3, c1}.
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
81
Xc1
Xt1
(i)
Xr1
fxr1
(ii)
Xc1
fxc1
0
0.3
1
0.7
0
0.1
1
0.9
(iii)
Xt1
Xr1
Xc1
0
0
0
1
0
0
1
1
0
1
0
1
0
1
1
0
1
0
0
0
1
0
1
0
1
1
0
0
1
1
1
1
Xr1
f and
t1, r1, c1
Figure 3.16: A graphical model with factors representation: (i) fr1; (ii) fc1; and (iii) fand t1,r1,c1.
lineage(RestInt, Customer) in the WHERE clause holds true if the lineage of RestInt
tuples includes a Customer tuple. As t1.RName=
“ABucks” and lineage(t1) = {r1, c1}, the above query result is “Lily”.
(Correlation-based Query) “Find restaurants of interest to Lily.”
SELECT RName
FROM Restaurant, Customer
WHERE
Restaurant.Type=Customer.Interest AND
CName=‘Lily’
Computation of result probability of t1 (Table 3.10) after joining r1 with c1 (Table 3.8)
involves a logical-and relationship. Sen and Deshpande [122] present the correlation
among tuples in probabilistic graphical model, as shown in Figure 3.16.
In the model, each tuple is associated with a Boolean random variable, where
1 represents tuple existence, and 0 otherwise (i.e. Xr1, Xc1, and Xt1). The correlation
in the graphical model can then be represented with random variables and factor
functions in Figure 3.16, and a 3-argument factor f and
t1,r1,c1 is deﬁned to express this and
correlation. Then query evaluation can be casted as inference over the probabilistic
graphical model by computing the marginal probability of result tuples with all the
tuples (regarded as random variables) and factors (representing tuple correlations)
involved.
P(Xt1 = 1) = ∑Xr1,Xc1
fr1(Xr1)fc1(Xc1)f and
t1,r1,c1(Xt1, Xr1, Xc1)
= 0.7 ∗0.9 = 0.63
Based on the model, various forms of correlation/dependency such as implication,
mutually exclusivity, xor relationship can be represented. To reduce the storage and
computing complexity, the random variable elimination graph (rv-elim graph) is built
where nodes are marked with labels allowing the recognition of shared correlation
factors [123]. The compression of the rv-elim graph enables to speed up query pro-
cessing signiﬁcantly [124]. A PrDB model for managing and exploiting rich correlation
Authenticated
:23 PM

82
3 Addressing Context Uncertainty
for query evaluation is further presented [78, 124]. Antova et al. [8, 67] handle complex
correlations by vertical partitioning, where a databases relation is partitioned into
several U-relations, so that only the involved partition is used in the query process.
Correlations in probabilistic streams are also examined in [77, 119].
(2) Monte-Carlo-Based Data Management
Data imprecision usually results in a lot of possible query answers. To address queries
of a #P-complete data complexity, Dalvi and Suciu [42] propose the use of the pseudor-
andom Monte Carlo method [79] to approximately compute the result probability. The
simulation algorithm can run in polynomial time and approximate the probabilities to
an arbitrary precision. Basically, given a DNF formula with N clauses and any : and
$, the algorithm runs in time O( N
:2 ln 1
$), guaranteeing that the probability of the error
being greater that : is less than $.
Monte Carlo simulations are also exploited to evaluate top-k queries. Consider-
ing the importance of correct ranking rather than exact probabilities, Ré et al. [118]
develop a Monte Carlo approximation algorithm, which calculates the top-k answers
for many steps. After N steps, an approximation interval [aN, bN] is returned for a
result tuple’s probability p, whose width shrinks as N increases. The ranking among
all the result tuples is conducted to ﬁnd the top-k probabilities according to the
approximation intervals.
Jampani et al. [74, 115] present a Monte Carlo-based uncertain data management
approach called MCDB. It does not encode uncertainty within the data model itself,
and all its query processing is over the classical relational data model. MCDB allows a
user to deﬁne arbitrary variable generation (VG) functions that embody the database
uncertainty. It then uses these functions to pseudorandomly generate realized values
for the uncertain attributes, and runs queries over the realized values. Moreover, these
VG functions can be parameterized on the results of SQL queries over parameter table
that are stored in the database. By storing parameters rather than probabilities, it is
easy to change the exact form of the uncertainty dynamically, according to the global
state of the database [74, 115]. In MCDB, each query is evaluated only once, regardless
of value N (number of Monte Carlo iterations) supplied by the user. Each “database
tuple” that is processed by MCDB is actually an array or a “bundle” of tuples, where
t[i] for tuple bundle t denotes the value of t in the ith Monte Carlo database instance.
The performance beneﬁt of such a “tuple bundles” approach is that relational op-
erations can efﬁciently operate in batch across all N Monte Carlo iterations that are
encoded in a single tuple bundle. Most of the classic relational operations can be mod-
iﬁed slightly to handle the fact that tuple bundles move through the query plan. Some
additional operators are also deﬁned to facilitate uncertain database querying, such
as seed operators, instantiate operator, split operator, and inference operator.
Arumugam et al. [9] further extend MCDB by randomly generating database
samples and exploring the tails of the query-result distribution. By adapting the Gibbs
sampling (a special case of a Markov Chain Monte Carlo) and cloning techniques de-
veloped in the simulation ﬁeld, they present a statistical method for both estimating
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
83
a user-speciﬁed quantile on a query-result distribution and deriving a set of samples
from the tail. The approach ﬁnds a good place in risk analysis [9].
Example 11. MCDB approximately simulates the quantile of ﬁnancial loss in enterprise
risk analysis. In the relation Loss(CustomerID, Val), Val is an uncertain attribute whose
values are generated through a predeﬁned variable generation function in the query
process. The quantile is deﬁned as “the value c such that there is a probability p of
seeing a total-loss of c or more”, Prob.(SUM(Val) ≥c) = p.
Assume there are r customers. The computation of quantile c is as follows.
(1) Generate 4 DB instances S = {D(1), D(2), D(3), D(4)}, each including r tuples.
(2) Implement SUM query Q on each DB instance, and discard the DB instances
whose SUM values are in the lowest 100(1 – p1/m)% percentile. The remaining DB
instances are cloned to ensure there are still 4 DB instances in S.
(3) Implement Gibbs-update in the newly cloned DB instances to eliminate duplic-
ate instances for a new version of S, where tuples in a DB instance are updated one by
one in a conditional way.
(4) Repeat m times from step 2.
(5) Finally, quantile c is obtained through the SUM query on the ﬁnal version of S.
◻
(3) Evidence-Based Data Management
Based on Evidence Theory, evidence-oriented database was proposed in the 1990s [90]
to support data ignorance. In an evidence-oriented database, the value of an uncer-
tain attribute in a relation is represented as a probability distribution on the power
set of its domain. Each tuple has an additional conﬁdence attribute in the form of
[belief, plausibility], stating the conﬁdence level of its belonging to the relation. For
an uncertain attribute A, its belief mass function is deﬁned as m : 2dom(A) →[0, 1].
Table 3.11(a) shows an evidence-based Customer relation. John’s Interest attribute
is expressed as < {i1}, 0.7 >, < {i1, i2}, 0.3 >, meaning that John is interested in i1 with
probability 0.7, or interested in i2 or i3 with probability 0.3. That is, m({i1})=0.7 and
m({i1, i2})=0.3. Due to ∑S⊆2dom(A) m(S) = 1, m(⌀) = 0, m({i2}) = 0, m({i3}) = 0, m({i2, i3}) =
0, m({i1, i3}) = 0, m({i1, i2, i3}) = 0.
(Evidence-based Query) “Find customers who may be interested in i1 or i3”.
SELECT CName,Interest FROM Customer
WHERE
(Interest=i1) OR (Interest=i3)
The resulting table is shown in Table 3.11(b). Taking John tuple for example,
belief({i1, i3}) =
∑
B⊆{i1,i3}
m(B) = m({i1}) = 0.7
plausibility({i1, i3}) =
∑
B∩{i1,i3} ̸=⌀
m(B) = m({i1}) + m({i1, i2}) = 1
Authenticated
:23 PM

84
3 Addressing Context Uncertainty
Table 3.11: An evidence database example.
(a) Evidence-based Customer relation
CID
CName
Interest
Conﬁdence
c1
Lily
unknown
[1,1]
c2
Tom
{i1, i2, i3}
[1,1]
c3
John
< {i1}, 0.7 >, < {i1, i2}, 0.3 >
[1,1]
c4
Jessica
< {i1, i3}, 0.7 >, < {i2}, 0.3 >
[1,1]
i1:Dinning
i2:Entertainment
i3:Photo
(b) Result of evidence-based query
CID
CName
Interest
Conﬁdence
c1
Lily
unknown
[0,1]
c2
Tom
{i1, i2, i3}
[0,1]
c3
John
< i1, 0.7 >, < {i1, i2}, 0.3 >
[0.7,1]
c4
Jessica
< {i1, i3}, 0.7 >, < i2, 0.3 >
[0.7,0.7]
Then the conﬁdence level of John tuple in the resulting table is [0.7, 1].
Evidence-based compound query has more query conditions connected by lo-
gical connectives (conjunction, disjunction, or negation). The conjunction of two
independent events A and B can be computed as:
belief(A ∧B) = belief(A) ∗belief(B)
plausibility(A ∧B) = plausibility(A) ∗plausibility(B)
Similarly, due to A ∨B = ¬(¬A ∧¬B), the disjunction of two events A and B can also be
computed according to the following equations in independent situations:
belief(A ∨B) = 1 – (1 – belief(A)) ∗(1 – belief(B))
plausibility(A ∨B) = 1 – (1 – plausibility(A)) ∗(1 – plausibility(B))
3.3.6.3 Fuzzy Databases
Fuzzy data management deals with imprecisely deﬁned, vaguely bounded linguistic
terms and statements like the discount is high (or around 4) rather than the dis-
count is 4. Fuzzy approximate queries like “the discount is around 4” are explained
according to the deﬁnition of fuzzy sets with vague boundaries.
Developed fuzzy data models can represent tuple-level, attribute-level, or both-
leveled uncertainty. To show different representation and query mechanisms between
probabilistic and fuzzy databases, we use the same example about customers’ in-
terests in restaurants with discounts. The tuple-level fuzzy data model considers a
relation as a fuzzy set which includes each tuple as an element with a membership
Authenticated
:23 PM

3.3 Uncertainty Processing Practices
85
Table 3.12: Tuple-attribute-wise fuzzy Restaurant relation.
RID
RName
Discount
Reputation
,
r1
ABucks
{0.5/7, 1/8, 0.6/9}
High
0.9
r2
AHut
{0.6/9, 1/9.5}
VeryHigh
0.8
r3
AFC
{1.0/8}
Medium
0.5
degree [13]. As shown in Table 3.12, attribute , gives the fuzzy measure of the asso-
ciation among RName, Discount, and Reputation. The attribute-level possibility data
model represents an uncertain attribute value which is represented with a possibil-
ity distribution [46, 116, 117] (e.g., Discount in Table 3.12), or a linguistic term (e.g.,
Reputation in Table 3.12). The possibility-distribution-fuzzy data model combines the
above two methods [32, 135].
Based on fuzzy theory, the fuzzy membership degree of a conjunctive operation
on fuzzy set A and B is ,A∪B(x) = max(,A(x), ,B(x)), and the fuzzy membership degree
of a disjunctive operation on A and B is: ,A∩B(x) = min(,A(x), ,B(x)) [31]. We illustrate
three typical fuzzy queries over Table 3.12.
(Fuzzy Query over Possibility Distributions) “Find restaurants which offer a
High discount.”.
SELECT RName
FROM Restaurant
WHERE
Discount=High
Assume term HighDiscount is represented as a possibility distribution
,HighDiscount(x) = {0.2/8, 0.7/9, 1/9.5}, and ABucks tuple has the discount possibility
distribution ,ABucks.Discount(x) = {0.5/7, 1/8, 0.6/9}. We use the notation A and B to de-
note the fuzzy sets HighDiscount and ABucks.Discount separately. The degree that A
matches B can be computed as follows.
Ma [100] deﬁned the Semantic Inclusion Degree (SID) for two fuzzy sets ,A and ,B
as follows.
Deﬁnition 11. Let ,A and ,B be fuzzy sets. In the universe X, the degree that ,A
semantically includes ,B is
SID(,A(x), ,B(x)) = ∑
xi∈X
min(,A(xi), ,B(xi))/ ∑
xi∈X
,B(xi).
◻
For the above query, in the universe of discount X = {7, 8, 9, 9.5},
SID(,A(x), ,B(x)) = (0.2 + 0.6)/(0.5 + 1 + 0.6) = 0.38,
SID(,B(x), ,A(x)) = (0.2 + 0.6)/(0.2 + 0.7+ 1) = 0.42.
Authenticated
:23 PM

86
3 Addressing Context Uncertainty
Table 3.13: Similarity representation for Reputation.
Very low
Low
Medium
High
Very high
Very low
1.0
0.0
0.0
0.0
0.0
Low
0.0
1.0
0.6
0.6
0.6
Medium
0.0
0.6
1.0
0.8
0.8
High
0.0
0.6
0.8
1.0
0.9
Very high
0.0
0.6
0.8
0.9
1.0
Then the similarity between the possibility distribution ,B(x) and possibility distribu-
tion ,A(x) is min(SID(,A(x), ,B(x)), SID(,B(x), ,A(x))) = 0.38.
Considering the overall ABucks tuple uncertainty 0.9, the result ABucks tuple sat-
isﬁes high discount with the degree of min(0.38, 0.9) = 0.38. In the same way, we can
obtain the ﬁnal result as (ABucks, 0.38), (AHut, 0.8), and (AFC, 0.13). Thus we ﬁnd that
AHut is the most likely to offer High discount.
(Fuzzy Query over Fuzzy Terms) “Find restaurants that have a high reputation.”.
SELECT RName FROM Restaurant WHERE Reputation=High
A similarity measurement between two fuzzy linguistic terms can be expressed in
Table 3.13.
As ABucks has reputation High, its satisfying degree of HighReputation is 1.0 ac-
cording to Table 3.13. Thus, its membership degree in the ﬁnal answer is min(1.0, 0.9) =
0.9. The query result is {(ABucks, 0.9), (AHut, 0.8), (AFC, 0.5)}.
(Fuzzy Query with Vague Condition) “Find restaurants whose discounts are
around 8”.
SELECT RName FROM Restaurant WHERE Discount≈8
The fuzzy condition about 8 can be deﬁned a membership function
,around 8(Discount) = {0.2/6, 0.7/7, 1.0/8, 0.7/9, 0.2/9.5}, which is similar to possibility
distribution in the previous fuzzy query example. Result computation can follow
exactly the same way through the SID.
We summarize tuple-wise and attribute-wise uncertainty representation in uncer-
tain data management in Table 3.14. In tuple-wise uncertainty, two ways are used
to express tuple existence uncertainty: a single value (probability in probabilistic
databases, or membership degree in fuzzy databases) or a range of conﬁdence de-
gree (evidence databases). In attribute-wise uncertainty, various representations are
used: a set of discrete attribute values and corresponding probabilities (e.g. (am, pam)),
a number of value sets with probabilities (e.g. (Ai, m(Ai)), a probabilistic density
function f(A) denoting the distribution, several characteristic values for a distribu-
tion function (Monte Carlo-based databases), and a possibility distribution (fuzzy
databases).
Authenticated
:23 PM

3.4 Context Uncertainty Management
87
Table 3.14: Uncertainty representation in data management.
Tuple-level (Tuple X)
Attribute-level (Attribute A)
Prob. DB:
Prob. DB:
1) (X, pX),
1) {(a1, pa1), ⋅⋅⋅, (am, pam)},
where pX ∈[0, 1] is the probability [42, 50]
where pai ∈[0, 1], ai ∈dom(A) (1 ≤i ≤m)
[109]
2) (X, [beliefX, plausibilityX]),
2) < A1, m(A1) >, ⋅⋅⋅, < Am, m(Am) >,
where [beliefX, plausibilityX] ⊆[0, 1] [90]
where m(Ai) ∈[0, 1] is the mass
(degree of belief), Ai ⊆dom(A) (1 ≤i ≤m)
[90]
3) Probabilistic density function f(A)
[16, 35, 133]
4) Characteristic values for pseudo-random
generation [74, 115]
Fuzzy DB:
Fuzzy DB:
(X, ,X),
{a1/,A(a1), ⋅⋅⋅, am/,A(am)},
where ,X ∈[0, 1] is the possibility [13]
where ,A(ai) ∈[0, 1] (1 ≤i ≤m)
[46, 116, 117]
3.4 Context Uncertainty Management
3.4.1 Context Uncertainty
Great achievements have been made on uncertain data management. When we look
at different origins of context uncertainty as well as different handling practices in
diverse ﬁelds, we may ﬁnd appropriate solutions to address context uncertainty.
Due to the dynamics, constrained devices, diverse distributed sources, and sens-
ing continuity, there is a high chance that the acquired context information in context-
aware systems and applications is imperfect, as reﬂected from four aspects [62].
–
Unknown. No information is available about the context.
–
Ambiguous. There are several different reports about the context.
–
Imprecise. The reported information is correct but too imprecise, for example,
from a person it is only known that s/he is in a certain building, but the exact
room where s/he locates is required.
–
Erroneous. The information about the context is not the same as the actual
information.
Authenticated
:23 PM

88
3 Addressing Context Uncertainty
3.4.2 Probabilistic Context Events
Every context event is accompanied with uncertainty. As the logic foundation of the
context model is Description Logics, approaches on addressing uncertainty in De-
scription Logics are ﬁrstly reviewed. As most existing work in the area of Description
Logics focuses on domain-oriented uncertainty rather than common data-oriented
membership uncertainty in context-aware systems, an approach based on probabil-
istic context events is presented here to address context uncertainty.
3.4.2.1 Domain-Oriented Probabilistic Description Logics
Baader et al. [10] provide a good overview of various approaches to deal with
uncertainty in Description Logics. Previous work on probabilistic reasoning in De-
scription Logics mainly focus on deﬁning probabilities on domains [54, 73, 85]. An
example of such kind of probability is: “the probability that a random chosen PhD
student is working is greater than 0.8”, represented as a probabilistic assertion:
P(Working|PhDStudent) > 0.8. This kind of assertions states statistical information and
usually results from an experiment or trial.
In comparison, as context information usually comes by sensing the physical and
social worlds, its uncertainty exhibits more a kind of degree of belief. An example of
such kind of probability is: “the probability that Joe (a particular PhD student) is work-
ing is 0.2”, represented as a probabilistic assertion: P(Working(Joe)) = 0.2. It implicitly
assumes multiple possible worlds with a probability over these possibilities, proper-
ties may hold in some worlds but not in others. The assertion states that the total
probability of the set of possible worlds in which Joe is working is 0.2.
Although [73] presents a cross-entropy minimization approach to tackle both
probabilities on domains and degrees of belief, it does not support probabilistic know-
ledge on DL role instances. Furthermore, combining both types of probabilities into
one framework makes reasoning too complex for context uncertainty. This is because
the later is mostly concerned with data membership uncertainty (e.g., there is uncer-
tainty about whether Joe is working), rather than domain uncertainty (e.g., there is no
uncertainty about whether writing a research paper is a working activity).
3.4.2.2 Basic and Complex Probabilistic Context Events
As membership checking over complete data bears similarity to database query-
ing [30], the challenges for addressing context uncertainty are similar to those for
probabilistic databases, which view each data record in a database as a probabilistic
basic event. By keeping track of the events that contribute to a derived fact through an
event expression, the probability of this derived result can be derived [50].
Following the same philosophy, and meanwhile adhering to the closed world as-
sumption where the instance data is assumed to be complete, each context event is
afﬁliated with a probability, expressing its occurrence uncertainty.
Authenticated
:23 PM

3.4 Context Uncertainty Management
89
(1) Basic Probabilistic Context Events
Similar to the probabilistic relational algebra [50], only the probabilities of basic con-
text events are given explicitly via a basic event probability assignment function Prob,
reﬂecting the uncertainty degrees of DL concept or role memberships.
Deﬁnition 12. The basic event probability assignment function satisﬁes the follow-
ing conditions:
–
Prob(⊥e) = 0;
–
Prob(⊤e) = 1;
–
For all basic events e, 0 ≤Prob(e) ≤1.
◻
(2) Correlation of Basic Context Events
Given the fact that many important properties in the domain of context-awareness are
usually dependent of each other (e.g., a person can only be at one location at one
time), it is necessary to take into account various kinds of correlations among basic
context events. Here, four typical event correlations are considered.
–
Independence. The occurrence of one event has nothing to do with the occurrence
of the other. For example, weather is independent of the day of the week.
–
Disjointness. Two events by deﬁnition cannot happen at the same time. For
example, if a person is on the bus, s/he cannot be at home at the same time.
–
Identicalness. Two events by deﬁnition always happen together. For example,
when a person is sailing, s/he is in a sailing boat. In this situation, event sailing
and event being in a sailing boat are identical.
–
Inclusion. It implies a positive correlation where in case one event happens, by
deﬁnition, the other event happens as well. For example, if a person is in a room,
s/he is also located in the building which contains this room.
Often these event correlations are determined from statistical analysis. Many use-
ful dependencies for context-awareness can be acquired from the ontology using
inference.
(3) Complex Probabilistic Context Events
To compute the probability of an arbitrary complex context event expression E, in-
volving n basic context events e1, e2, . . . , en, one can ﬁrst convert it into an equivalent
disjunctive normal form (DNF) E = E1 ∨E2 ∨⋅⋅⋅∨Em, where Ei = ei1 ∧ei2 ∧. . . ∧ein,
and ei1, ei2, . . . , ein are basic event identiﬁers or negated basic event event identiﬁers.
A complete conjunction is a conjunction which contains each basic event exactly once,
in either positive or negative form [50].
Deﬁnition 13. Let E, E1, E2 be context event expressions. The general probability
function Prob(E) maps context event expression E to a probability value, satisfying
the following conditions:
Authenticated
:23 PM

90
3 Addressing Context Uncertainty
Table 3.15: Inﬂuence of event correlation on probability computation, where e1 and e2 are two basic
context events.
Event Correlation between e1 and e2
Event Probability Prob(e1 ∧e2)
Independence
= Prob(e1) ∗Prob(e2)
Disjointness
= 0
Identicalness
= Prob(e1) = Prob(e2)
Inclusion
> Prob(e1) ∗Prob(e2)
–
Prob(E) ∈[0, 1];
–
Prob(¬E) = 1 – Prob(E);
–
Prob(E1 ∨E2) = Prob(E1) + Prob(E2) – Prob(E1) ∗Prob(E2);
–
Prob(E1 ∧E2) (ref. Table 3.15).
◻
3.4.2.3 Probabilities of Correlated Context Events
Table 3.15 illustrates the inﬂuence of correlation between two basic context events on
the probability Prob(e1 ∧e2).
For conjunctions of literals with basic context events in the following inclusion-
exclusion formula [50]:
Prob(E1 ∨E2 ∨⋅⋅⋅∨Em)
=
m
∑
i=1
Prob(Ei) – ∑
i<j
Prob(Ei ∧Ej) + ∑
i<j<k
Prob(Ei ∧Ej ∧Ek) –
∑
i<j<r<s
Prob(Ei ∧Ej ∧Er ∧Es) + ⋅⋅⋅+ (–1)(m+1)Prob(E1 ∧E2 ∧⋅⋅⋅∧Em)
The above correlations can be taken into account by comparing each event with all
other events in the same conjunction. Let e1 and e2 be two basic events, and let E
be either a basic or complex event expression, the implications of correlation for the
probability of the conjunctions, by straightforward application of basic probability
theory, are then as follows:
Case 1
Event e1 is disjoint from event e2, the computation of the probability of the
conjunct can be rewritten as follows:
Prob(E ∧e1 ∧e2) = 0;
Prob(E ∧¬e1 ∧e2) = Prob(E ∧e2);
Prob(E ∧e1 ∧¬e2) = Prob(E ∧e1);
Prob(E ∧¬e1 ∧¬e2) = Prob(E) – (Prob(E ∧e1) + Prob(E ∧e1)).
Case 2
Event e1 includes event e2, the computation of the probability of the conjunct
can be rewritten as follows:
Prob(E ∧e1 ∧e2) = Prob(E ∧e1);
Authenticated
:23 PM

3.4 Context Uncertainty Management
91
Prob(E ∧¬e1 ∧e2) = 0;
Prob(E ∧e1 ∧¬e2) = Prob(E) – Prob(E ∧e1) + Prob(E ∧e2);
Prob(E ∧¬e1 ∧¬e2) = Prob(E ∧¬e1).
Case 3
Events e1 and e2 are identical, the computation of the probability of the
conjunct can be rewritten as follows:
Prob(E ∧e1 ∧e2) = Prob(E ∧e1);
Prob(E ∧¬e1 ∧e2) = 0;
Prob(E ∧e1 ∧¬e2) = 0;
Prob(E ∧¬e1 ∧¬e2) = Prob(E ∧¬e1).
After rewriting, the right-hand side of each equation contains no correlated events in
the same conjunction. That is, all ontology-related dependencies have been removed.
As a result, the remaining conjunctions only contain basic independent events. For
these conjunctions, their probability calculation is equal to the multiplication of the
probabilities of their basic events.
In a DNF with complete conjunctions, all conjunctions are disjoint and the prob-
ability of the complex event is equal to the sum of probabilities of the different
conjunctions. To illustrate, let us see the following example.
Example 12. Table 3.16 shows the basic events related to person Joe. A simple context
that “Joe is working at either home or ofﬁce” can be denoted as a complex context
event expression E = ew ∧(eh ∨eo), where ew, eh, eo are three basic context event
identiﬁers.
Rewriting the event expression Prob(E) to its DNF with complete conjunctions and
adding up the probabilities of the conjunctions gives
Prob(ew ∧(eh ∨eo))
= Prob(ew ∧eh ∧eo) + Prob(ew ∧eh ∧¬eo) + Prob(ew ∧eo ∧¬eh).
As eh and eo are disjoint events,
Prob(ew ∧eo ∧eh) = 0,
Prob(ew ∧eh ∧¬eo) = Prob(ew ∧eh), and
Table 3.16: Probabilities of example basic context events.
DL Role membership
Basic Context Event e
Probability Prob(e)
hasActType(Joe, work)
ew
0.7
hasActType(Joe, lunch)
el
0.3
hasRoom(Joe, ofﬁce)
eo
0.6
hasRoom(Joe, home)
eh
0.3
hasRoom(Joe, lobby)
el
0.1
Authenticated
:23 PM

92
3 Addressing Context Uncertainty
Prob(ew ∧eo ∧¬eh) = Prob(ew ∧eo).
Prob(ew ∧(eh ∨eo)) = Prob(ew ∧eh) + Prob(ew ∧eo).
In the above resulting equation, there are no more ontology-related correlations
between two context events, and independence is assumed among the rest of the basic
events (that is, ew and eh, ew and eo are independent). Thus,
Prob(ew ∧(eh ∨eo)) = Prob(ew) ∗Prob(eh) + Prob(ew) ∗Prob(eo)
= 0.7 ∗0.3 + 0.7 ∗0.6 = 0.63.
◻
3.4.3 Advanced Techniques of Context Uncertainty Management
From user and domain perspectives, more advanced techniques and tools could be
developed to cope with context uncertainty in the real world.
3.4.3.1 Leveled Context Uncertainty Representation
Dalvi et al. [40], Dalvi et al. [41], Dalvi and Suciu [42], and Re and Suciu [120]
have made great efforts to reduce the complexity of uncertain database query pro-
cessing. Inspired by uncertainty handling activities in diverse ﬁelds, domain-speciﬁc
knowledge could be a help to further reduce the complexity based on some speciﬁc
uncertain context representation.
For example, in a group-purchase application scenario, customers may be inter-
ested in such restaurants which offer discounts less than 4, between 3 and 5, etc. On the
other hand, the possible discounts available from restaurants themselves may fall into
a scope rather than as a speciﬁc value. A tabular representation of uncertain discount
values for a restaurant is illustrated in Table 3.17, whose pair-wise uncertainty val-
ues are increasingly ordered. For instance, the ﬁrst pair ⟨0.1, ≤1⟩represents that the
probability of (Discount ≤1) is 0.1. Apparently, the probability of (Discount ≤2), in-
cluding (Discount ≤1), is greater than 0.1, as shown in Table 3.17. Such a property
could be exploited to query optimization, and is particularly good at range queries.
It is a compromise between discrete and continuous probability distributions, and of-
fers a complementary way when a continuous probability distribution is unavailable
or impractical due to the high integration computation complexity.
The size (or granularity) of the uncertainty table reﬂects the precision level of un-
certainty handling behavior itself, forming a hierarchy of uncertainty tables (Tu, ≺u),
where Tu = (T1, T2, . . . , Ts) of s levels, and ≺u is a partial order among the levels of
Tu, such that (T1 ≺u Ti ≺u Ts) where (1 < i < s). Given a query or inference task,
dynamically selecting the right uncertainty table from the hierarchy based on certain
measurements is needed for different applications.
Authenticated
:23 PM

3.4 Context Uncertainty Management
93
Table 3.17: Tabular form of restaurant’s uncertain discount.
Uncertainty
0. 1
0.15
0.4
⋅⋅⋅
1.0
Discount
≤1
≤2
≤5
⋅⋅⋅
≤9
3.4.3.2 Domain-Driven Context Uncertainty Management
Different applications have different requirements on uncertain context management.
Bringing application logics to this uncertainty management is important. Taking
sensing and monitoring domains for example, real-time sensing and response with
low latency are very much desirable. However, due to the inherent context uncertainty
in the real world, the execution time and waiting time of a query may vary. It is thus
hard to precisely predicate and ensure real-time query response performance.
For instance, for a query Q issued at time Ts and expected to ﬁnish at Td, let Te
denote its uncertain waiting and execution time, represented in the form of a value
scope around the estimated value:
U(!,
∼
Te) = {Te : |Te –
∼
Te | ≤!
∼
Te} ,
where
∼
Te is the estimated time for waiting and execution, and ! ∈[0, 1] is the deriva-
tion. A robust function can be deﬁned to compute the largest allowable scope of query
waiting and execution time in order to meet the query deadline:
Robust = max
{
{
{
{
{
! :
max
Te∈U(!,
∼
Te)
(Ts + Te) ≤Td
}
}
}
}
}
.
Based on the robust value, the variation range of query waiting and execution time,
[
∼
Te(1 – !),
∼
Te(1 + !)], can be derived, which could then be used to guide time-sensitive
query scheduling and resource management of the system.
3.4.3.3 Leveraging User Knowledge
Human users are good at and highly successful in coping with context uncertainty
throughout their daily lives, as most human knowledge in the real world is uncertain.
While working with probabilistic database query and inference mechanisms to in-
fer sensible and actionable information from underlying uncertain context, we could
involve users in the loop of query evaluation for feedback.
For example, based on the observation that a user is usually more likely to re-
cognize mistakes in basic uncertain tuples leading to the ﬁnal ranked answer, than
mistakes in the answer itself, the query engine could consider to display those inﬂu-
ential underlying probabilistic tuples to the ranked query result, and then leverage
Authenticated
:23 PM

94
3 Addressing Context Uncertainty
user’s personal knowledge to clarify the uncertainty degrees and precisions of the
basic tuples. After that, the query engine can recompute the query and tailor its un-
certain query result towards a better quality from the perspective of the speciﬁc user.
More important, by opening the black-box of the query engine and showing to the
user how it comes up with the answer and which uncertain tuples it is based on, the
user with his/her knowledge can decide how much conﬁdence to be placed on the
system, thus enhancing both the intelligibility of system behavior and accountability
of human users.
Here, a few critical questions need to be answered, like how to interact with
the user for result explanation and uncertainty clariﬁcation without bringing much
burden on the user? How to correct the query/inference result after user’s uncer-
tainty clariﬁcation without incurring much computing overhead on the query engine?
And, how to reconcile different users’ uncertainty clariﬁcation upon the uncer-
tain database? Solutions to the above questions determine the effectiveness of the
approach.
3.4.3.4 Crowdsourcing for Uncertain Context Management
As queries may require information from human knowledge which is missing in the
databases, such as the recognition of misspelling words, efforts to deeply involve
crowd on the internet in query processing have been made [45, 49, 102, 112, 113, 143].
For instance, CrowdDB [49] leverages human capability by crowdsourcing missing val-
ues of tuples, crowdsourcing new tuples from the inner relation that matches the tuple
of the outer relation in join operations, and crowdsourcing comparison work. Qurk
system [102] addresses the workﬂow management of crowd-powered querying tasks
by balancing monetary cost, spending time, and result accuracy.
Currently, crowdsourcing in context-aware data management is still at an early
stage, leaving some challenges to be solved, such as quality assessment and improve-
ment, latency, scheduling, cost optimization, privacy, and social issues [45]. The high
ambiguity and multiple sources from human inputs need to be tackled for a smart
interface to machine processing.
3.5 Recapitulation
To address context uncertainty problem, two kinds of uncertainty, i.e., aleatory un-
certainty and epistemic uncertainty, were discussed. Four uncertainty processing
theories and their applications in the ﬁelds of economics, engineering, ecology, in-
formation science, and databases were reviewed in this chapter. Standing upon the
previous work, an approach based on probabilistic context events was particularly
given to deal with context uncertainty. Advanced techniques toward context uncer-
tainty management were also discussed. The next chapter describes quality-assured
mechanisms for context acquisition.
Authenticated
:23 PM

Literature
95
Literature
[1]
G. J. Abel, J. Bijak, J. J. Forster, J. Raymer, and P. W. F. Smith. What do Bayesian methods offer
population forecasters? Technical Report Working Paper 6/2010, Centre for Population
Change, ESRC Research Centre for Population Change, University of Southampton, 2010.
[2]
A. Abiteboul, P. Kanellakis, and G. Grahne. On the representation and querying of sets of
possible worlds. SIGMOD Record, 16(3):34–48, 1987.
[3]
E. Adar and C. R. Managing uncertainty in social networks. IEEE Data Engineering Bulletin,
30(2):15–22, 2007.
[4]
P. K. Agarwal, A. Efrat, S. Sankararaman, and W. Zhang. Nearest neighbor searching under
uncertainty. In Proc. of PODS, 2012.
[5]
P. Agrawal, O. Benjelloun, A. Das Sarma, C. Hayworth, S. Nabar, T. Sugihara, and J. Widom.
Trio: A system for data, uncertainty, and lineage. In Proc. of VLDB, 2006.
[6]
D. A. Ahlburg and K. C. Land. Population forecasting: Guest editors’ introduction. Journal of
Forecasting, 8(3):289–299, 1992.
[7]
J. M. Alho. Stochastic methods in population forecasting. Journal of Forecasting, 6:521–530,
1990.
[8]
L. Antova, T. Jansen, C. koch, and D. Olteanu. Fast and simple relational processing of
uncertain data. In Proc. of ICDE, 2008.
[9]
S. Arumugam, R. Jampani, L. Perez, F. Xu, C. Jermaine, and P. Haas. MCDB-R: Risk analysis in
the database. In Proc. of VLDB, pages 782–793, 2010.
[10]
F. Baader, R. Kusters, and F. Wolter. Extensions to description logics. The Description Logic
Handbook, F. Baader, D. Calvanese, D. McGuinness, D. Nardi, and P. Patel-Schneider (eds.),
Cambridge University Press, pages 219–261, 2003.
[11]
D. Baccarini. The logical framework method for deﬁning project success. Project Management
Journal, UK, 30(4):25–32, 1999.
[12]
J. Baker and M. Lepech. Treatment of uncertainties in life cycle assessment. In Proc. of the 10th
Intl. Congress on Structural Safety and Reliability, Japan, 2009.
[13]
J. F. Baldwin. A fuzzy relational inference language for expert systems. Proc. of the 13th IEEE
Intl. Symp. on Multiple-Valued Logic, pages 416–423, 1983.
[14]
G. Bammer and M. Smithson. Uncertainty and Risk: Multidisciplinary Perspectives, London,
UK: Earthscan Publications, 2008.
[15]
T. Bayes and R. Price. An Essay towards solving a Problem in the Doctrine of Chance. By the
late Rev. Mr. Bayes, communicated by Mr. Price, in a letter to John Canton, M. A. and F. R. S.
Philosophical Transactions of the Royal Society of London, 53(0):370–418, 1763.
[16]
C. Böhm, M. Gruber, P. Kunath, A. Pryakhin, and M. Schubert. ProVeR: Probabilistic video
retrieval using the gauss-tree. In Proc. of ICDE, 2007.
[17]
Y. Ben-Haim. Info-gap theory and its applications. http://www. technion. ac.il/yakov/
IGT/igt.htm.
[18]
Y. Ben-Haim. Information-Gap Theory: Decisions Under Severe Uncertainty, London, UK,
Academic Press, 2001.
[19]
Y. Ben-Haim. Info-Gap Theory: Decisions Under Severe Uncertainty, London, UK, Academic
Press, 2nd ed., 2006.
[20]
Y. Ben-Haim. Info-Gap Economics: An Operational Introduction, Basingstoke, Palgrave
Macmillan, 2010.
[21]
B. Beresford-Smith and C. J. Thompson. Managing credit risk with infogap uncertainty. Journal
of Risk Finance, 8(1):24–34, 2007.
[22]
T. Bernecker, T. Emrich, H. P. Kriegel, N. Mamoulis, M. Renz, and Züﬂe. A novel probabilistic
pruning approach to speed up similarity queries in uncertain databases. In Proc. of ICDE, IEEE
Computer Society, pages 339–350, 2011.
Authenticated
:23 PM

96
3 Addressing Context Uncertainty
[23]
T. Bernecker, H.-P. Kriegel, M. Renz, F. Verhein, and A. Züﬂe. Probabilistic frequent itemset
mining in uncertain databases. In Proc. of KDD, 2009.
[24]
T. Bernecker, H.-P. Kriegel, N. Mamoulis, M. Renz, and A. Zü eﬂe. Scalable probabilistic
similarity rankling in uncertain databases. IEEE TKDE, 2010.
[25]
G. Beskales, M. A. Soliman, and I. F. Ilyas. Efﬁcient search for the top-k probable nearest
neighbors in uncertain databases. In Proc. of VLDB, 2008.
[26]
V. M. Bier and L. A. Cox. Probabilistic risk analysis for engineered systems. Advances in
Decision Analysis, W. Edwards, R. F. Miles, and D. von Winterfeldt (eds.), Cambridge, UK:
Cambridge University Press, pages 279–301, 2007.
[27]
F. Black and M. Scholes. The pricing of options and corporate liabilities. Journal of Political
Economy, 81(3):637–654, 1973.
[28]
J. Bongaarts and R. A. Bulatao. Beyond Six Billion: Forecasting the World’s Population,
Washington, DC: National Academy Press, 2000.
[29]
H. Booth. Demographic forecasting: 1980 to 2005 in review. Journal of Forecasting,
22(3):547–581, 2006.
[30]
A. Borgida, M. Lenzerini, and R. Rosati. Description logics for databases. The Description Logic
Handbook, Cambridge University Press, pages 462–484, 2003.
[31]
P. Bosc and O. Pivert. SQLf: A relational database language for fuzzy querying. IEEE
Transactions on Fuzzy Systems, 3:1–17, 1995.
[32]
P. Bosc and O. Pivert. Imprecise data management and ﬂexible querying in databases. Fuzzy
Sets, Neural Networks and Soft Computing, R. R. Yager and L. A. Zadeh (eds.), Thomson
Learning, New York, Chapter 19, pages 368–395, 1994.
[33]
R. Cheng, J. Chen, M. Mokbel, and C. Y. Chow. Probabilistic veriﬁers: Evaluating constrained
nearist-neighbor queries over uncertain data. In Proc. of ICDE, 2008.
[34]
R. Cheng, L. Chen, J. Chen, and X. Xie. Evaluating probability threshold k-nearest-neighbor
queries over uncertain data. In Proc. of EDBT, 2009.
[35]
R. Cheng, D. V. Kalashnikov, and S. Prabhakar. Evaluating probabilistic queries over imprecise
data. In Proc. of ACM SIGMOD, 2003.
[36]
R. Cheng, S. Singh, and S. Prabhakar. Efﬁcient join processing over uncertain data. In Proc. of
CIKM, 2006.
[37]
H. Chernoff. A measure of asymptotic efﬁciency for tests of a hypothesis based on the sum of
observations. The Annals of Mathematical Statistics, 23(4):493–507, 1952.
[38]
E. F. Codd. A relational model of data for large shared data banks. In Communications of the
ACM, 1970.
[39]
G. Cormode, F. Li, and K. Yi. Semantics of ranking queries for probabilistic data and expected
ranks. In Proc. of ICDE, 2009.
[40]
N. Dalvi, C. Re, and D. Suciu. Queries and materialized views on probabilistic databases.
Journal of Computer System Science, 77(3), 2011.
[41]
N. Dalvi, K. Schnaitter, and D. Suciu. Computing query probability with incidence algebras. In
Proc. of PODS, 2010.
[42]
N. Dalvi and D. Suciu. Efﬁcient query evaluation on probabilistic databases. In Proc. of VLDB,
2004.
[43]
P. S. de Laplace. Analytical theory of probability, Ve Courcier, Paris, 1820.
[44]
A. P. Dempster. Upper and lower probabilities induced by a multivalued mapping. The Annals
of Mathematical Statistics, 38(2):325–339, 1967.
[45]
A. Doan, M. J. Franklin, D. Kossmann, and T. Kraska. Crowdsourcing applications and
platforms: A data management perspective. In Proc. of VLDB, 2011.
[46]
D. Dubois and H. Prade. Possibility Theory: An Approach to Computerized Processing of
Uncertainty, New York: Plenum Press, 1988.
Authenticated
:23 PM

Literature
97
[47]
F. T. Dweiri and M. M. Kablan. Using fuzzy decision making for the evaluation of the project
management internal efﬁciency. Decision Support Systems, 42:712–726, 2006.
[48]
Environment Agency. Climate adaptation risk and uncertainty: Draft decision framework.
Technical Report 21, Environment Agency Report, June 2000.
[49]
M. J. Franklin, D. Kossmann, and T. Kraska. Crowddb: Answering queries with crowdsourcing.
In Proc. of ACM SIGMOD, 2011.
[50]
N. Fuhr and T. Rolleke. A probabilistic relational algebra for the integration of information
retrieval and database systems. In ACM Trans. Information Systems, 1997.
[51]
Fuzzy control system. http://en.wikipedia.org/wiki/Fuzzy_control.
[52]
G. Gerla. Fuzzy logic programming and fuzzy control. Studia Logica, 79:231–254, 2005.
[53]
W. R. Gilks, S. Richardson, and D. J. Spiegelhalter. Markov Chain Monte Carlo in Practice,
London: Chapman & Hall, 1996.
[54]
R. Giugno and T. Lukasiewicz. P-shoq(d): A probabilistic extension of shoq(d) for probabilistic
ontologies in the semantic web. Proceedings of JELIA, S. Flesca, S. Greco, N. Leone, and G.
Ianni (eds.), pages 86–97, Springer-Verlag Berlin Heidelberg, 2002.
[55]
Y. M. Goh, L. B. Newnes, A. R. Mileham, C. A. McMahon, and M. E. Saravi. Uncertainty in
through-life costing-review and perspectives. IEEE Transaction on Engineering Management,
57(4):689–701, November 2010.
[56]
M. Goldstein and J. Rougier. Bayes linear calibrated prediction for complex systems. Journal of
the American Statistical Association, 101:1132–114, 2006.
[57]
S. Greenland. Sensitivity analysis, Monte Carlo risk analysis, and Bayesian uncertainty
assessment. Journal of Risk Analysis, 21:579–583, 2001.
[58]
A. J. Hatﬁeld and K. W. Hipel. Understanding and managing uncertainty and information.
Proceedings of the SMC, 5:1007–1012, 1999.
[59]
F. A. Hayek. The use of knowledge in society. The American Economic Review, 51(2):209–223,
1945.
[60]
R. Heijungs. Uncertainty analysis in LCA: Concepts, tools, and practices. Institute of
Environmental Sciences (CML). Leiden University. http://formations. cirad.fr/analyse-
cycle-de-vie/pdf/Heijungs_2.pdf.
[61]
J. C. Helton. Treatment of uncertainty in performance assessments for complex systems.
Journal of Risk Analysis, 14(4):483–511, 1994.
[62]
K. Henricksen and J. Indulska. Modelling and using imperfect context information. In Proc. of
the CoMoRea Workshop on Context Modelling and Reasoning, IEEE Computer Society, pages
33–37, 2004.
[63]
M. Henrion and B. Fischhoff. Assessing uncertainty in physical constants. Annual Journal of
Physics, 54(9):791–797, 1986.
[64]
F. O. Hoffman and J. S. Hammonds. Propagation of uncertainty in risk assessment: The need to
distinguish between uncertainty due to lack of knowledge and uncertainty due to variability.
Journal of Risk Analysis, 14(5):707–712, 1994.
[65]
M. Hua and J. Pei. Continuously monitoring top-k uncertain data streams: A probabilistic
threshold method. Journal of DPD, 26(1):29–65, 2009.
[66]
M. Hua, J. Pei, W. Zhang, and X. Lin. Ranking queries on uncertain data: A probabilistic
threshold approach. In Proc. of ACM SIGMOD, 2008.
[67]
J. Huang, L. Antova, C. Koch, and D. Olteanu. Maybms: A probabilistic database management
system. In Proc. of ACM SIGMOD, 2009.
[68]
D. Hubbard. How to Measure Anything: Finding the Value of Intangibles in Business, Hoboken,
NJ: John Wiley & Sons, 2007.
[69]
M. A. J. Huijbregts, W. Gilijamse, A. M. J. Ragas, and L. Reijnders. Evaluating uncertainty in
environmental life-cycle assessment. Journal of Environmental Science and Technology,
37:2600–2608, 2003.
Authenticated
:23 PM

98
3 Addressing Context Uncertainty
[70]
M. L. Hung and H. W. Ma. Quantifying system uncertainty of life cycle assessment based on
Monte Carlo simulation. Journal of Life Cycle Assessment, 14(1):19–27, 2009.
[71]
I. F. Ilyas, G. Beskales, and M. A. Solimam. A survey of tok-k query processing techniques in
relational database systems. ACM Computing Surveys, 40(4):11.1–11.58, 2008.
[72]
T. Imieli´nski and W. Lipski. Incomplete information in relational databases. Journal of ACM,
31(4):761–791, 1984.
[73]
M. Jaeger. Probabilistic reasoning in terminological logics. In Proc. of KR, pages 305–316,
1994.
[74]
R. Jampani, Fei Xu, and M. Wu. MCDB: A Monte Carlo approach to managing uncertain data. In
Proc. of ACM SIGMOD, 2008.
[75]
T. S. Jayram, S. Kale, and E. Vee. Efﬁcient aggregation algorithms for probabilistic data. In
Proc. of SODA, 2007.
[76]
D. Kahneman and A. Tversky. Variants of uncertainty. Variants of Uncertainty, D. Kahneman, P.
Slvic, and A. Tversky (eds.), Cambridge, UK: Cambridge University Press, 1982.
[77]
B. Kanagal and A. Deshpande. Efﬁcient query evaluation over temporally correlated
probabilistic streams. In Proc. of ICDE, 2009.
[78]
B. Kanagal and A. Deshpande. Lineage processing over correlated probabilistic databases. In
Proc. of ACM SIGMOD, 2010.
[79]
R. M. Karp and M. Luby. Monte Carlo algorithms for enumeration and reliability problems. In
Proc. of the 24th IEEE Symposium on Foundations of Computer Science, pages 56–64, 1983.
[80]
R. W. Katz. Techniques for estimating uncertainty in climate change scenarios and impact
studies. Journal of Climate Research, 20:167–185, 2002.
[81]
S. Keen. Debunking Economics: The Naked Emperor of the Social Sciences, Australia: Pluto
Press, 1944.
[82]
J. Keynes. The general theory of employment. The Quarterly Journal of Economics,
51(2):209–223, 1937.
[83]
G. J. Klir. Uncertainty Theories, Measures and Principles, Berlin, Germany: Akademie Verlag,
1996.
[84]
F. H. Knight. Risk, Uncertainty, and Proﬁt, Boston: Houghton Mifﬂin Company, 1921.
[85]
D. Koller, A. Levy, and A. Pfeffer. P-CLASSIC: A tractable probabilistic description logic. In Proc.
of AAAI/IAAI, pages 390–397, 1997.
[86]
A. N. Kolmogorov. Foundations of the Theory of Probability, New York: Chelsea Publishing
Company, 1956.
[87]
T. C. Koopmans. Three Essays on the State of Economic Science, McGraw Hill, New York, 1957.
[88]
K. Lange. Numerical analysis for statisticians. Springer-Verlag, New York, Inc., 1999.
[89]
LCA Links! – Links to LCA topics found on the web. http://www.life-cycle. org.
[90]
S. K. Lee. Imprecise and uncertain information in databases: An evidential approach. In Proc.
of ICDE, 1992.
[91]
J. Li and A. Deshpande. Ranking continuous probabilistic datasets. In Proc. of VLDB, 2010.
[92]
J. Li, B. Saha, and A. Deshpande. A uniﬁed approach to ranking in probabilistic databases. In
Proc. of VLDB, 2009.
[93]
X. Lian and L. Chen. Probabilistic inverse ranking queries over uncertain data. In Proc. of
DASFAA, 2009.
[94]
B. Liu. Uncertainty Theory: An Introduction to Its Axiomatic Foundations, Berlin, Germany:
Springer–Verlag, 2nd ed., 2007.
[95]
B. Liu. Uncertainty Theory: A Branch of Mathematics for Modeling Human Uncertainty, Berlin,
Germany: Springer–Verlag, 4th ed., 2010.
[96]
V. Ljosa and A. K. Singh. Apla: Indexing arbitrary probability distributions. In Proc. of ICDE,
2007.
Authenticated
:23 PM

Literature
99
[97]
S. M. Lloyd and R. Ries. Characterizing, propagating, and analyzing uncertainty in life-cycle
assessment: A survey of quantitative approaches. Journal of Industrial Ecology, 11(1):161–179,
2007.
[98]
W. Lutz and J. R. Goldstein. Introduction: How to deal with uncertainty in population
forecasting? International Statistical Review, 72:1–4, 2004.
[99]
W. Lutz, J. W. Vaupel, and D. A. Ahlburg. Frontiers of population forecasting. In A Supplement
to Vol.24, 1998, Population and Development Review, New York: The Population Council, 1999.
[100]
Z. M. Ma. Fuzzy Databases Modeling with XML, Springer-Verlag New York, Inc., 2005.
[101]
MAFF. Flood and Coastal Defence Project Appraisal Guidance Notes: Approaches to Risk.
FCDPAG4, February 2000.
[102]
A. Marcus, E. Wu, D. R. Karger, S. Madden, and R. C. Miller. Crowdsourced databases: Query
processing with people. In Proc. of CIDR, 2011.
[103]
S. Markridakis, R. M. Hogarth, and A. Gaba. Forecasting and uncertainty in the economics and
business world. Journal of Forecasting, 25:794–812, 2009.
[104]
N. Metropolis and S. Ulam. The Monte Carlo method. Journal of the American Statistical
Association, 44(247):335–341, 1949.
[105]
S. Michael. Apology of Socrates, Warminster, UK: Aris & Phillips, 1997.
[106]
G. Morgan, H. Dowlatabadi, M. Henrion, D. Keith, R. Lempert, S. McBrid, M. Small, and T.
Wilbanks. Best practice approaches for characterizing, communicating, and incorporating
scientiﬁc uncertainty in decision making. Synthesis and assessment product 5.2, U.S. Climate
Change Science Program and the Subcommittee on Global Change Research, Washington, DC:
NOAA, 2009.
[107]
R. Moss and S. H. Schneider. Uncertainties in the IPCC TAR: Recommendations to lead authors
for more consistent assessment and reporting. In Guidance Papers on the Cross Cutting Issues
of the Third Assessment Report of the IPCC, World Meteorological Organisation, pages 33–51,
2000.
[108]
A. Motro. Management of uncertainty in database systems. Modern Database Systems: The
Object Model, Interoperability, and Beyond, W. Kim (ed.), New York: ACM Press, pages
457–476, 1994.
[109]
R. Murthy and J. Widom. Making aggregation work in uncertain and probabilistic databases. In
Proc. of MUD, pages 76–90, 2007.
[110]
National Research Council (NRC). Risk Analysis and Uncertainty in Flood Damage Reduction
Studies, Washington, DC: National Academy Press, 2000.
[111]
H. G. Natke and Y. Ben-Haim. Uncertainty: Models and Measures, Berlin, Germany: Akademie
Verlag, 1996.
[112]
A. Parameswaran and N. Polyzotis. Answering queries using humans, algorithms and
databases. In Proc. of CIDR, 2011.
[113]
A. Parameswaran, A. D. Sarma, and H. Garcia-Molina. Human-assisted graph search: It’s okay
to ask questions. In Proc. of VLDB, 2011.
[114]
M. E. Pate-Cornell. Uncertainties in risk analysis: Six levels of treatment. Journal of Reliability
Engineering and System Safety, 54:95–111, 1996.
[115]
L. L. Perez, S. Arumugam, and C. M. Jermaine. Evaluation of probabilistic threshold queries in
MCDB. In Proc. of ACM SIGMOD, 2010.
[116]
H. Prade and C. Testemale. Generalizing database relational algebra for the treatment of
incomplete or uncertain information and vague queries. Journal of Information Sciences,
34:115–143, 1984.
[117]
H. Prade and C. Testemale. Fuzzy relational databases: Representational issues and reduction
using similarity measures. Journal of the American Society for Information Science,
38(20):118–126, 1988.
Authenticated
:23 PM

100
3 Addressing Context Uncertainty
[118]
C. Re, N. Dalvi, and D. Suciu. Efﬁcient top-k query evaluation on probabilistic data. In Proc. of
ICDE, 2007.
[119]
C. Re, J. Letchner, M. Balazinska, and D. Suciu. Event queries on correlated probabilistic
streams. In Proc. of ACM SIGMOD, 2008.
[120]
C. Re and D. Suciu. Understanding cardinality estimation using entropy maximization. In Proc.
of PODS, 2010.
[121]
C. Schinckus. Economic uncertainty and econophysics. Physica A, 388:4415–4423, 2009.
[122]
P. Sen and A. Deshpande. Representing and querying correlated tuples in probabilistic
databases. In Proc. of ICDE, 2007.
[123]
P. Sen, A. Deshpande, and L. Getoor. Exploiting shared correlations in probabilistic databases.
In Proc. of VLDB, 2008.
[124]
P. Sen, A. Deshpande, and L. Getoor. PrDB: Managing and exploiting rich correlations in
probabilistic databases. In Proc. of VLDB, 2009.
[125]
G. Shafer. A Mathematical Theory of Evidence, Princeton University Press, Princeton, New
Jersey, 1976.
[126]
A. J. Shenhar, O. Levy, and D. Dvir. Mapping the dimensions of project success. Project
Management Journal, 28(2):5–13, 1997.
[127]
P. Smets. Belief functions and the transferable belief model. The Imprecise Probabilities
Project. http://ippserv.rug.ac.be.
[128]
M. Sniedovich. Voodoo decision-making under severe uncertainty. http://voodoo.
moshe-online.com/.
[129]
M. A. Soliman, I. F. Ilyas, and C. C. Chang. Top-k query processing in uncertain databases. In
Proc. of ICDE, 2007.
[130]
P. Soundappan, E. Nikolaidis, R. T. Haftka, R. Grandhi, and R. Canﬁeld. Comparison of
evidence theory and Bayesian theory for uncertainty modelling. Journal of Reliability
Engineering & System Safety, 85(1–3):295–311, 2004.
[131]
R. R. Tan. Using fuzzy numbers to propagate uncertainty in matrix-based lci. Journal of Life
Cycle Assessment, 13:585–592, 2008.
[132]
R. R. Tan, A. B. Culaba, and M. R. I. Purvis. Application of possibility theory in the life-cycle
inventory assessment of biofuels. Journal of Energy Research, 26:737–745, 2002.
[133]
Y. Tao, R. Cheng, X. Xiao, W. K. Ngai, B. Kao, and S. Prabhakai. Indexing multi-dimensional
uncertain data with arbitrary probability density functions. In Proc. of VLDB, 2005.
[134]
The ﬁrst climate model. NOAA 200th Celebration, http://celebrating 200years.noaa.gov/
breakthroughs/climate_model/welcome.html, 2007.
[135]
M. Umano and S. Fukami. Fuzzy relational algebra for possibility distribution-fuzzy-relational
model of fuzzy data. Journal of Intelligent Information Systems, 3:7–27, 1994.
[136]
United States Nuclear Regulatory Commission (USNRC). An assessment of accident risks in us
commercial nuclear power plants. Technical Report Reactor safety study, WASH-1400
(NUREG-75/OI4), Washington, DC, 1975.
[137]
M. B. A. van Asselt and J. Rotmans. Uncertainty in integrated assessment modelling – from
positivism to pluralism. Journal of Climatic Change, 54:75–105, 2002.
[138]
Van Gelder. Statistical Methods for the Risk Based Design of Civil Structures, PhD thesis, Delft
University, 1999.
[139]
R. von Schomberg. Controversies and Political Decision Making, Dordrecht: Kluwer Academic
Publishers, 1993.
[140]
P. Walley. Coherent upper and lower previsions. The imprecise probabilities project.
http://ippserv.rug.ac.be., 1998.
Authenticated
:23 PM

Literature
101
[141]
W. Wang and P. H. Stone. Effect of ice-albedo feedback on global sensitivity in a
one-dimensional radiative-convective climate model. Journal of Atmospheric Sciences,
37:545–552, 1980.
[142]
M. Webster, C. Forest, J. Reilly, M. Babiker, D. Kicklighter, M. Mayer, R. Prinn, M. Saroﬁm, A.
Sokolov, P. Stone, and C. Wang. Uncertainty analysis of climate change and policy response.
Journal of Climatic Change, 61:295–320, 2003.
[143]
S. A. Yahia, A. Doan, J. Kleinberg, N. Koudas, and M. J. Franklin. Crowds, clouds, and
algorithms: Exploring the human side of big data applications. In Proc. of ACM SIGMOD, 2010.
[144]
K. Yi, F. Li, and G. Kollios. Efﬁcient processing of top-k queries in uncertain databases with
x-relations. In IEEE TKDE, 2008.
[145]
K. Yi, F. Li, G. Kollios, and D. Srivastava. Efﬁcient processing of top-k queries in uncertain
databases. In Proc. of ICDE, 2008.
[146]
L. A. Zadeh. Fuzzy sets. Journal of Information and Control, 8:338–353, 1965.
[147]
L. A. Zadeh. Fuzzy sets as a basis for a theory of possibility. Journal of Fuzzy Sets and Systems,
1:3–28, 1978.
[148]
L. A. Zadeh. Fuzzy logic. IEEE Computer Magazine, 21(4):83–93, April 1988.
[149]
Y. Zhang, X. Lin, G. Zhu, W. Zhang, and Q. Lian. Efﬁcient rank based knn query processing over
uncertain data. In Proc. of ICDE, 2010.
[150]
H. J. Zimmermann. Uncertainty modelling and fuzzy sets. Uncertainty: Models and Measures,
H. G. Natke and Y. Ben-Haim (eds.), Berlin, Germany: Akademie-Verlag, pages 84–100, 1997.
Authenticated
:23 PM

4 Acquiring Context
Abstract: Timely and high-quality context is critical to the success of context-aware
applications. Almost all context-aware systems have two indispensable components,
namely, context provider and context consumer. The former is responsible for supply-
ing diverse context information about users and environments, while the later makes
use of context information in building context-aware applications. The diversity and
quantity of context information from heterogeneous context suppliers to different con-
sumers justify the need for a generic re-usable mechanism to manage context data
ﬂow from low-level providers to high-level consumers. This chapter presents a declar-
ative speciﬁcation language for quality-aware context acquisition. With the speciﬁca-
tion, a generic context provision and acquisition adaptor can be built. Furthermore, a
quality-assured strategy by leveraging the redundancy of context providers is given,
where redundant imprecise and unstable context providers can be used to consti-
tute a more precise and stable context provision. Such a strategy provides a basis to
implement the speciﬁcation-based context provision and acquisition adaptor.
4.1 Challenges in Context Acquisition
Interesting context-aware applications have appeared in tour guide, smart place,
health monitor, content adaptation on mobile devices, ofﬁce and conference assist-
ant, communication assistant, etc. For instance, in the museum of Cooltown [14],
a visitor holds a smart phone, which can display the exhibits’ information when
the visitor approaches the exhibits. In MusixMix [1], when a person enters a room,
his/her preferred music can be played automatically. In these context-aware applica-
tions, timely and high-quality context is critical to the success of these context-aware
applications. In fact, almost all the context-aware systems have two indispensable
components – context provider and context consumer. Context providers are re-
sponsible for supplying context information related to human people, physical and
computing environments. They are usually such devices that can provide sensor data,
or be some data sources of some social context information, such as users’ habits and
preferences.
Providing context data from context providers to upper context consumers relates
to the lowest layer of context-aware programming models and is in the fundamental
tiers of context-aware systems [7, 23]. It is not a trivial task due to several salient
challenges.
The ﬁrst challenge is to acquire high-level context from low-level hardware or soft-
ware sensors, e.g., to track the location or to estimate the activity of a person by using
load sensors [20].
The second challenge is to manage the distribution, heterogeneity, and scalability
issues in context acquisition [1]. It is not rare that the context providers are distributed
DOI 10.1515/9783110556674-004
Authenticated
:23 PM

4.2 Three Context Acquisition Mechanisms
103
and heterogeneous in a context-aware system. In effect, context consumers may also
be distributed and heterogeneous.
The third challenge is to acquire high-quality real-time context data from context
providers whose quality and availability cannot be guaranteed.
The fourth challenge is to deal with large quantities of context providers and
consumers.
4.2 Three Context Acquisition Mechanisms
Three kinds of context acquisition methods are discussed in this section, including (1)
offering context acquisition services, (2) unifying context acquisition interfaces to di-
verse context consumers, and (3) building a context provision and acquisition adaptor
between context suppliers and consumers.
4.2.1 Offering Context Acquisition Services
The ﬁrst context acquisition method is to make context providers as services. Advant-
ages exist in a service-oriented platform, which provides the functionalities of service
registration, discovery, and composition. Some of the efforts are listed below.
–
Yang et al. implemented the context acquisition function as a kind of Web service,
where GPS and RFID are treated as Web services [26].
–
Costa et al. introduced a service architecture to support context-aware applica-
tions [5], in which a subscription language was deﬁned to conﬁgure the plat-
form to react to a given correlation of events, potentially involving contextual
information.
–
Gu et al. proposed a service-oriented middleware for building context-aware ser-
vice [8]. They further generalized Dey et al.’s widget method to a service-oriented
platform [4].
–
Ritchie proposed the peripheralware to wrap the context service functionalities
to tackle such issues as access control, cost and importance trade-off, visibility,
handing-off requests, and minimization of user interruption [19].
–
Gui et al. used a sever as a repository of context providers. Context providers are
registered and managed by the server [9].
–
He et al. illustrated that context information could also be provided as a service in
a cloud platform [10].
4.2.2 Unifying Context Acquisition Interfaces
A number of frameworks or middlewares for context-aware computing have been de-
veloped in the literature. Although many of them eventually do not focus on context
Authenticated
:23 PM

104
4 Acquiring Context
acquisition, but as a whole framework for building context-aware applications, they
usually incorporate context acquisition as a component.
From the standpoint of context consumers, a uniﬁed context acquisition interface
can be achieved through the middleware. Typical efforts include the following.
–
Dey et al. introduced the concept of context widgets to mediate environments and
applications in the same way as graphical widgets mediate users and applica-
tions [6]. It is a pioneer study on context-aware computing architecture, with the
focus on reusable widgets and the separation of high-level context applications
from the detailed acquisition of low-level context information. The advantage of
context widgets is that it allows a uniﬁed interface for context consumers to use
various context data.
–
After Dey’s context toolkits, Hong presented an infrastructure for context-aware
computing called ContextFabric, focusing on context modeling, context speciﬁc-
ation language, and protection mechanism for safeguarding privacy needs [12].
On top of ContextFabric is a query processing engine called liquid for distributed
continuous query processing of context data [11].
–
Chen et al. also built a context-aware platform called Solar. In the platform, con-
textual data sources are in the form of stream publishers, and a peer-to-peer
overlay is used to support data-driven services. Solar emphasizes the protocols
of resource discovery and data dissemination [3].
–
Martinerz and Salavert deﬁned APIs (Application Programming Interfaces) to set
context acquisition frequency and cache size [17].
–
Presecan and Tomai provided a middleware architecture to query context inform-
ation in a standard REST style queries [18].
–
The framework Citron, developed by Yamabe et al., acquired context data for mul-
tiple sensory personal devices [25], but it focused on the mobile phone side rather
than on the server side.
4.2.3 Building a General Context Provision and Acquisition
Adaptor
4.2.3.1 Insufﬁciency of Unifying Context Acquisition Interfaces
While unifying the context acquisition interfaces to context suppliers simpliﬁes the
development of context-aware applications, the developers still need to take efforts to
implement the interfaces for different context providers. If the interfaces to the upper
context consumer are supposed to be implemented by each device (e.g., hardware or
software sensor), the efforts of implementation cannot be easily reused. In the context
toolkit [6], if it is the role of different context providers and third-parties to provide
corresponding widgets for upper context-aware applications, the consequence is that
each context provider may need to have a widget for its supplied context information,
Authenticated
:23 PM

4.3 A Declarative Speciﬁcation Language for Context Acquisition
105
making the widget’s reuse rate (which is very important for a software middleware)
very low in industry. In fact, many times context-aware application developers cannot
completely control which formats and standards that context providers (e.g., sensory
data acquisition cards and software services) conform to, since the later has already
been deﬁned by the manufacturers.
4.2.3.2 Speciﬁcation-based Context Acquisition via Standard Communication
Protocols
The third context acquisition approach takes diverse context suppliers into consid-
eration, aiming at a generic reusable context provision and acquisition middleware,
situating between heterogeneous context providers and different consumers. To im-
plement an efﬁcient context acquisition middleware, Li and Feng [15] presented a
few acquisition performance optimization techniques for Web service-based context
providers and ontology-based context consumers.
Li and Feng [16] further made such a software middleware behave as an adaptor,
just like an electric hardware adaptor to connect different cables. The realization of
such a software adaptor is based on a declarative context acquisition speciﬁcation
language (to be discussed in the next section). A context-aware application developer
only needs to declare the context acquisition requirements, and the adaptor imple-
ments the acquisition functionality by directly using standard provision protocols and
consumption models [16].
Compared with the ﬁrst context widget interface solution [6] in Subsection 4.2.2,
the context provision and acquisition adaptor is not oriented toward speciﬁc con-
text providers and consumers, but oriented toward the existing data communication
standards (e.g., USB, R232/R485 serial port, ﬁeld bus protocol, SOAP protocol, uPnP,
etc.). This means that the sensor manufacturers do not need to support context widget
interfaces, or the solar planet interfaces [3], or JCAF interfaces [2]. Instead, they only
need to provide the hardware/software sensors as before adopting international data
communication standards. At the meantime, the context-aware application designers
do not need to implement the interfaces to collect data for the context consumers, the
adaptor can be reused.
Some techniques can also be utilized in the speciﬁcation-based method to auto-
matically select context providers for the context consumers, e.g., the similarity-based
mapping proposed in Ref. [24].
4.3 A Declarative Speciﬁcation Language
for Context Acquisition
The quality-aware declarative context provision and acquisition speciﬁcation lan-
guage offers an uniform way for context-aware application developers to describe
Authenticated
:23 PM

106
4 Acquiring Context
their context data acquisition, integration, and transformation requirements from
suppliers to consumers, as well as associated functionality and nonfunctionality re-
quirements and constraints. Detailed management and implementation of context
data ﬂow in heterogeneous context-aware environments are completely left to the
adaptor. Application developers just simply invoke the adaptor to enable or disable
context acquisition and transformation process based on the speciﬁcation [15].
The context provision and acquisition speciﬁcation language contains four
types of descriptors, namely, Point Descriptors, Connector Descriptors, Constraint
Descriptors, and Quality Descriptors [16].
4.3.1 Point Descriptors
Point descriptors deﬁne origins and destinations of context data ﬂow from context
providers to consumers.
Deﬁnition 14. A context provider point P is a 4-tuple P=(P.name,P.val,
P.protocol,P.method), where
–
P.name is the name of P;
–
P.val is the context value that P provides;
–
P.protocol is the communication protocol (such as USB, R232 serial port,
Bluetooth, RMIService, etc.) through which P provides the context value;
–
P.method is the method used to acquire context value P.val from P.
◻
Different communication protocols correspond to different methods for context ac-
quisition. For instance, when P.protocol=“USB”, method details the device in-
formation, data packet format, etc. When P.protocol=“RMIService”, P.method
details the registry URL of the provider server, method name, and an argument list.
Deﬁnition 15. A context consumer point C is a triple C=(C.name,C.val,
C.model), where
–
C.name is the name of C;
–
C.val is the value that C consumes;
–
C.model is the model (like ontology model, relational model, etc.) that C uses to
represent the acquired context value C.val.
◻
4.3.2 Connector Descriptors
Connector descriptors specify the binary connections and value transformations from
context provider points to context consumer points.
Authenticated
:23 PM

4.3 A Declarative Speciﬁcation Language for Context Acquisition
107
Deﬁnition 16. Let P be the set of context provider points, and C be the set
of context consumer points. A context connector PC links a set of context
provider points to a context consumer pointer, and can be deﬁned as a triple
PC=(PC.PSet,PC.C,ConnectExp(PC.PSet,PC.C)), where
–
PC.PSet ⊆2P is a set of context provider points;
–
PC.C ∈C is a context consumer point;
–
ConnectExp(PC.PSet,PC.C) is a connector expression, which maps the val-
ues of a set of context provider points PC.PSet to a value of a context consumer
point PC.C.
◻
A connection expression observes the following Extended Backus-Naur Form
(EBNF) [13].
ConnectExp(PSet,C) = C.val, ’=’, rExp(PSet);
rExp(PSet) =
P.val | P.val.sub-element |
function(rExp) | rExp(PSet),
arithmetic-operator, rExp(PSet)
(where P ∈PSet and . locates the sub-element
value in the hierarchically structured value P.val);
arithmetic-operator =’+’|’-’|’*’|’/’|’%’;
function =
type-function | string-function;
type-function =
int|double|string|char;
string-function =
concatenate|equals|trim|upper|lower.
The default type of P.val is string. The type-function is to cast a type to a
literal string.
Example 13. The acquisition of the location context of a mobile phone is done via
the context connector PC=(PC.PSet,PC.C,ConnectExp(PC.PSet, PC.C)),
where PC.Pset={P1,P2}, P1.val provides the base location value of a mo-
bile phone, P2.val provides the offset of the location value, and Connect-
Exp(PC.PSet,PC.C) is PC.C.val =P1.val+P2.val.
◻
Invoking various functions and sub-elements can result in very complex context con-
nector expressions. For simplicity, the value P.val and C.val are interchangeably
expressed using P and C, respectively.
4.3.3 Constraint Descriptors
Constraint descriptors deﬁne structural and semantic requirements upon context con-
nectors, typically including context dependency and cascading constraints, context
refresh frequency and interval constraints, context ﬁlters, and context connection
modes.
Authenticated
:23 PM

108
4 Acquiring Context
4.3.3.1 Dependency Constraints
Dependency relationships exist among context provider points, consumer points, and
both. A default dependency is that for a connector, its context provider points Ps
should have values available before its context consumer point C consumes the value.
In other words, C depends on Ps. Dependency relationships also exist among con-
text provider points, when P1.val is needed to determine the other P2.val. For
example, the discovery APIs in Bluetooth are often used to discover the identiﬁers of
nearby Bluetooth devices, and these identiﬁers are further used to query the speciﬁc
information of the Bluetooth devices.
Context dependency relationships can be expressed via context dependency
expressions. Let PC=(PC.PSet,PC.C,ConExp(PC.PSet,PC.C)) be a context
connector. Function GetValue(p) (where p ∈PC.PSet ∪{PC.C}) returns true if
the context provider or consumer point p gets its value.
ContextDependExp(PC) =
GetValue(p) {,GetValue(p’)}, ’󳨃→’ ,
[time-constraint]
GetValue(p) {,GetValue(p’)}
(where p,p’ ∈PC.PSet ∪{PC.C});
time-constraint =
’[’,time-interval,’,’,
time-interval,’]’
time-interval =
number, time-unit;
time-unit =
Year|Month|Week|Day|Hour|Minute|
Second|MSecond.
A context dependency expression states the order in which to get context value. After
the precondition is satisﬁed (corresponding points get their values), within a speciﬁed
time interval, the postcondition is satisﬁed (corresponding points get or refresh the
context values).
Example 14. Associated with the same context connector PC in Example 13, a con-
text dependency constraint ContextDependExp(PC)=GetValue(P1) ∧Get-
Value(P2) 󳨃→[0, 100Second] GetValue(PC.C) states that after P1 and P2
both get the values, PC.C must get the context value within the next 100 seconds.
◻
4.3.3.2 Context Refresh Interval Constraints
The context frequency and refresh interval specify how frequent to activate context
connector (i.e., to acquire and transform context values from provider points to con-
sumer points). Context refresh frequency is the inverse of refresh interval. They are
important indicators to the real-time performance for context-aware systems and ap-
plications. Many times, it is not the faster, the better, but the more consistent with the
speed requirement, the better.
Authenticated
:23 PM

4.3 A Declarative Speciﬁcation Language for Context Acquisition
109
For a context connector PC, requesting and receiving context data from its
provider points and writing to its consumer point constitute a context refresh
process. The context refresh time requirement is stated in the following form:
RefreshTimeExp(PC)=time-constraint.
Example 15. For the same context connector PC in Example 13,
RefreshTimeExp(PC)=[490Second,510Second] states that the effective con-
text refresh interval should be greater than 490 seconds and less than 510
seconds.
◻
4.3.3.3 Context Filter
While context refresh interval/frequency indicates when to acquire and refresh con-
text data, context ﬁlters specify what context data is allowed to be fed to context
consumers for further use. The beneﬁt of introducing this constraint constructor is
to avoid unnecessary context data entering context-aware systems to save computing
resources.
A context ﬁlter expression ContextFilterExp(PC) for the context connector
PC has the following EBNF form.
ContextFilterExp(PC) = ’Write-When-Changed’ |
logic-exp(PC) |
aggregate-exp(PC);
logic-exp(PC) =
conjunct(PC) {,’∨’,conjunct(PC)};
conjunct(PC) =
disjunct(PC){,’∧’,disjunct(PC)};
disjunct(PC) =
PC.C.val,comparison-op,constant;
comparison-op =
=|>|>=|<|<=;
aggregate-exp(PC) =
aggregate-function,’(’,p.val,
integral-number,’)’
(where p ∈PC.PSet ∪{PC.C});
aggregate-function =
Sum|Avg|Max|Min.
’Write-When-Changed’ indicates that only changed context data is sent and writ-
ten into the context consumer point. As context data may come as a data stream,
aggregate-filter-exp speciﬁes aggregate operations on the stream, and then
the aggregated value is written into the context consumer point.
Example 16. Assume a context connector has a temperature sensor as its context
provider point. ContextFilterExp(PC)=PC.C.val>25 states only when the ob-
tained temperature value is above 25 degrees, it can be passed and written to the
consumer point.
Authenticated
:23 PM

110
4 Acquiring Context
Similarly, ContextFilterExp(PC)=Avg(PC.C.val,100) demands the av-
erage value of every 100 values obtained.
◻
4.3.3.4 Context Connection Modes
Context connection modes outline various ways to build context connectors. These
include pull push, and automatic modes. A context connection mode expression for
the context connector PC is declared as:
ConnectModeExp(PC)= PULL|PUSH|AUTOMATIC.
For the pull-based connection mode, context data will be explicitly called by the
adaptor, while for the push-based mode, context provider points decide when to send
out context data. Sometimes, it is desirable that the adaptor can automatically ﬁnd
and build connections from providers to consumers. This is possible when both pro-
viders and consumers adopt some standard models, for instance, when providers are
wrapped as Web services and consumers use an ontology model.
The following shows two kinds of automation mechanisms in real functioning of
the adaptor.
The ﬁrst mechanism is in the situation where the context consumer data model
is known, but the context providers are not. Different data communication protocols
have different service discovery methods. If the context provider is a Web service, they
can be discovered by UDDI repository or WSIL ﬁle. The adaptor can use the keyword
search to discover the existing Web service, and order them by keyword similarity, so
that the most suitable Web service can be chosen. The user can also specify the dis-
covery method and discovery attributes in the corresponding discovery method, e.g.,
the URL of the WSIL ﬁle, the keyword, and the account if necessary.
The second mechanism is automatically generating some structure information of
the context model. It is used in the situation where the location of context providers
is known, but the context data model is not. For example, if it is known that context
data returned from RMI invocation is an object of java class Sensor with two ﬁelds
data and precision, then a corresponding key-value context model with two key-value
pairs (sensor-data, sensor-data-value) and (sensor-precision, sensor-precision-value)
can be generated.
4.3.4 Quality Descriptors
Context-aware system/application developers not only concern about functionalit-
ies of the context adaptor but also the quality of the context adaptor [21, 22]. Some
quality indicators are timeliness, correctness, freshness, uncertainty, precision, spa-
tial and temporal resolution, etc. Thus, in addition to static structural and semantic
constraints, dynamic quality constraints are to be declared to require and evalu-
ate run-time performance of the context adaptor. In the following, three quality
Authenticated
:23 PM

4.3 A Declarative Speciﬁcation Language for Context Acquisition
111
descriptors, i.e., context retrogress, context latency, and context uncertainty, are
particularly discussed.
4.3.4.1 Context Retrogress
There exists the possibility that the arrival sequence of context data at the context ad-
aptor is inconsistent with their real sense sequence from provider points due to some
communication exception. In other words, the previously sensed context data arrives
at the context adaptor later than the newly sensed data, and thus replaces the cor-
rect value. Such a problem is referred to as context retrogress. It brings out-of-dated
context data to consumer points.
Given a context connector PC=(PC.PSet,PC.C,ConExp(PC.PSet,PC.C))
at its i-th refresh process, some notations for time measurement in the refresh process
are as follows.
–
Req(PC,i): the time for the context consumer point to start context request to
context provider points;
–
Sense(PC,i): the time when all the context providers have sensed the context
data;
–
Return(PC,i): the time when all the requests to context provider points are
responded with context values returned;
–
WrtStart(PC,i): the time when the context consumer point starts to write the
context value;
–
WrtDone(PC,i): the time when the context consumer point ﬁnishes writing the
context value.
Deﬁnition 17. A context retrogress happens to a context connector PC, if and
only if there exist two i-th and j-th refresh processes (where i<j), such that
(Sense(PC,i) <t Sense(PC,j)) ∧(WrtDone(PC,i) >t WrtDone(PC, j)) is
true.
◻
Figure 4.1 illustrates a context retrogress example.
time
Provider PSet
Sense(PC,i)
WrtDone(PC,j)
WrtDone(PC,i)
Context Connector PC
Consumer C
Sense(PC,j)
Figure 4.1: A context retrogress example.
Authenticated
:23 PM

112
4 Acquiring Context
If no context retrogress happens to a context connector PC, such a connector is called
retrogress free. A retrogress free requirement is declared as:
RetrogressExp(PC) = ’RetrogressFree(’,PC,’)’.
4.3.4.2 Context Latency
It is desirable that the time to consume the context data should be close to the time
when the context information is sensed from sensors (context providers). Many times,
the acquired context information needs to be logged and stored in a context database.
There are two reasons for having this context database available. First, to deal with
dynamic connections, caching some context information at the consumer’s side can
ensure the consistent providing of user-related information, even when a user is not
connected to the network. More important, from these logged context information and
related actions, context consumers can learn and reason about users’ preferences and
behavior patterns which will lead to pro-actively generated rules by the context-aware
systems.
Given a context connector PC at its i-th context refresh process, the time to start to
use the transformed context information by its consumer is called use time, sometimes
also referred to as the writing-done time.
Deﬁnition 18. The context latency at the i-th context refresh process of a context con-
nector PC is the time difference between context use time (i.e., when ﬁnishing context
writing) by its consumer point and context sense time by its providers, computed as
Latency(PC,i)=WrtDone(PC,i)-Sense(PC,i).
◻
The context latency quality statement is expressed as:
LatencyExp(PC) = ’Latency(PC) =’, time-constraint.
4.3.4.3 Context Uncertainty
Context-aware systems are typically highly sensor-driven, but sensors provide only
evidence of fact rather than facts themselves.The uncertainty of sensor data will
decrease the quality of the acquired context information. Context-aware system de-
velopers may want to constrain the uncertainty level on the acquired context data, so
that the effect of uncertainty on upper context-aware applications can be minimized.
Such a quality requirement is declared in terms of the allowed deviation from the real
value with a conﬁdence probability.
UncertaintyExp(PC) = ’Uncertainty(PC) =’, deviation,’,’,
confidence-prob.;
deviation =
decimal-number;
confidence-prob. =
non-negative-number, ’%’.
Authenticated
:23 PM

4.4 Quality-Assured Context Acquisition
113
Example 17. For the temperature context, the acquired value shall be less than 1∘de-
viating from the real temperature in 95.44% cases, stated as
Uncertainty(PC)=1∘,95.44%.
◻
4.4 Quality-Assured Context Acquisition
Uncertainty of context data incurs difﬁculties of assuring quality of context. Assuring
context quality is no less challenging than modeling it. Difﬁculties lie in acquiring
high-quality context from the context providers whose quality and availability cannot
be guaranteed. As shown in Example 17, the system developer needs the acquired tem-
perature to be less than 1 degree deviating from the real temperature in a room in 95%
cases. However, sometimes context providers are subject to noise disturbance, and the
sensors are likely to fail to work. It may be possible that the best temperature sensor
in the system may only provide a temperature less than 2 degrees deviating from the
real temperature in 95% cases. This section presents a quality assurance strategy for
controlling and improving the quality of context in the above speciﬁcation-based con-
text acquisition adaptor, especially when any single context provider could not meet
the quality requirement. The basic idea is to use redundant imprecise and unstable
context providers for precise and stable context provision by minimizing the squared
error.
4.4.1 The Least Squared Error of Redundant Context Values
Let C be a context consumer, and P1,P2,⋅⋅⋅,Pm be a set of redundant context pro-
viders, which can independently provide a context value to C. At a certain context
refresh process, assume x1, x2, ⋅⋅⋅, xm are a set of values provided by P1,P2,⋅⋅⋅,Pm to
C, and the real context value desired by C is y. The estimation of y can thus be based
on the least squared error.
min
m
∑
i=1
d(y, xi)2,
(4.1)
where d(y, xi) is the distance between y and xi.
Assume xi (1 ≤i ≤m) is from a context space with continuous values, and the
variance of xi is 32. The least squared error formulation (4.1) becomes
min
m
∑
i=1
(y – xi)2.
(4.2)
Let f = ∑m
i=1(y – xi)2. Since f is convex, by resolving the ﬁrst-order condition 𝜕f
𝜕y = 0, we
have the minimum of f when y =
∑m
i=1 xi
m
.
Authenticated
:23 PM

114
4 Acquiring Context
Assume the context value xi complies with a normal distribution N (,i, 32
i ) with
mean ,i and variance 32
i . With m independent x1, x2, ⋅⋅⋅, xm, the variance of y is 32
y =
∑m
i=1 32
i
m2
.
In the special case where 32
1 = 32
2 = ⋅⋅⋅= 32
x, 32
y =
32
x
m . It shows that by incor-
porating m redundant context providers, the variance of y decreases to 1
m of that of
one context provider. Theoretically, any small variance requirement can be satisﬁed
by increasing m. For y to have a variance not larger than a speciﬁed threshold 3s,
that is,
32
y = 32
x
m ≤32
s,
(4.3)
the required least number of redundant context providers must be
m ≥32
x
32
s
.
(4.4)
Furthermore, in the case where context providers have different uncertainty levels in
context provision, a weighted least squared error can be employed as
min
m
∑
i=1
wi(y – xi)2.
(4.5)
Similarly, eq. (4.5) is minimal when y =
∑m
i=1 wixi
∑m
i=1 wi . The variance of y is 32
y =
∑m
i=1 w2
i 32
i
(∑m
i=1 wi)2 .
When ∑m
i=1 wi = 1, y = ∑m
i=1 wixi and 32
y = ∑m
i=1 w2
i 32
i .
Example 18. Assume two context providers P1 and P2 redundantly provide context
values x1 and x2, respectively. The variance of x1 is 32
1 = 1 and the variance of x2 is
32
2 = 4. Let w1 = 2
3, w2 = 1
3. y = w1x1 + w2x2 = 2
3x1 + 1
3x2, and 32
y = 2
332
1 + 1
332
2 = 4
9 + 4
9 = 8
9,
showing the variance of y is less than the variance of either x1 or x2.
◻
4.4.2 Failure of Context Providers
The analysis in the above section assumes that all context providers work, no matter
how precise their supplied context values are. It is possible that one or more con-
text providers may fail in supplying context in reality. Consider two availability states
(work and fail) of context providers. The expected mean of the ﬁnal context value’s
variance, 32
y, is
E(32
y) =
∑S∈2 ̄P,S ̸=6 Pr(Work(S))32
y(S)
1 – Pr(Fail( ̄P))
,
Authenticated
:23 PM

Literature
115
where
–
̄P is the set of redundant context providers, 2
̄P is the power set of ̄P;
–
Work(S) is the predicate for the situation where all the context providers in S ∈
̄P
and S
̸= 6 work;
–
Fail( ̄P) is the predicate for the situation where all the context providers in ̄P fail in
providing context values;
–
32
y(S) is the variance of y obtained by using the least squared error method when
all the context providers in S provide the context data simultaneously.
4.5 Recapitulation
The diversity of context information from heterogeneous context providers to vari-
ous context consumers justiﬁes the need for a generic reusable middleware solu-
tion to manage context data ﬂow from low-level providers to high-level consumers.
This chapter proposed a context adaptor method which abstracts the connection
from context suppliers to context consumers. A declarative quality-aware speciﬁca-
tion language, including four kinds of descriptors (i.e., point descriptors, connector
descriptors, constraint descriptors, and quality descriptors), was described for con-
text provision and acquisition. A context quality assurance strategy by leveraging the
redundancy of context providers was also discussed. In future, some more advanced
issues like dependencies among context providers and outliers of context data are to
be considered in qualify-assured context acquisition.
The next chapter will address how to protect context privacy, when context is
linked to a surrounding individual (e.g., location of a given person) and falls under
privacy directives.
Literature
[1]
M. Assad, D. J. Carmichael, J. Kay, and B. Kummerfeld. Personisad: Distributed, active, scrutable
model framework for context-aware services. In Proc. of Pervasive Computing, pages 55–72,
2007.
[2]
J. E. Bardram. The Java Context Awareness Framework (JCAF). In Proc. of Pervasive Computing,
pages 98–115, 2005.
[3]
G. Chen, M. Li, and D. Kotz. Data-centric middleware for context-aware pervasive computing.
Pervasive and Mobile Computing, pages 216–253, 2007.
[4]
P. Costa and L. Botelho. Generic context acquisition and management framework. In Proc. of the
1st European Young Researchers Workshop on Service Oriented Computing, 2005.
[5]
P. D. Costa. Towards a Services Platform for Context-Aware Applications. In Proc. of IWUC, 2004.
[6]
A. K. Dey, G. D. Abowd, and D. Salber. A context-based infrastructure for smart environments. In
Managing Interactions in Smart Environments, Springer–Verlag, pages 114–128, 1999.
[7]
W. Du and L. Wang. Context-aware application programming for mobile devices. In Proc. of
C3S2E, pages 215–227, New York, NY: ACM, 2008.
Authenticated
:23 PM

116
4 Acquiring Context
[8]
T. Gu, H. K. Pung, and D. Q. Zhang. A service-oriented middleware for building context-aware
services. Journal of Network and Computer Applications, 28(1):1–18, 2005.
[9]
F. Gui, M. Guillen, N. Rishe, A. Barreto, J. Andrian, and M. Adjouadi. A client-server architecture
for context-aware search application. In Proc. of the Intl. Conf. on Network-Based Information
Systems, 2009.
[10]
Y. He, L. Li, K. He, and X. Chen. A contextual information acquisition approach based on
semantics and mashup technology. In Proc. of Cloud – Comp, 2009.
[11]
J. Heer, A. Newberger, C. Beckmann, and J. I. Hong. Liquid: Context aware distributed queries. In
Proc. of UbiComp, Springer, pages 140–148, 2003.
[12]
J. I. Hong. The context fabric: an infrastructure for context-aware computing. In Proc. of Intl.
Conf. on Human Factors in Computing Systems, New York, NY: ACM, pages 554–555, 2002.
[13]
ISO/IEC 14977:1996(E) Information Technology – Syntactic Metalanguage – Extended BNF,
1996.
[14]
T. Kindberg, J. Barton, J. Morgan, G. Becker, D. Caswell, P. Debaty, G. Gopal, M. Frid, V. Krishnan,
and H. Morris. People, places, things: Web presence for the real world. Mobile Networks and
Applications, 7(5):365– 376, 2002.
[15]
Y. Li and L. Feng. Bridging web service-based context suppliers and ontology-based context
consumers. In Proc. of the 4th Intl. Conf. on Ubiquitous Information Management and
Communication, 2010.
[16]
Y. Li and L. Feng. A quality-aware context middleware speciﬁcation for context-aware
computing. In Proc. of the 2nd IEEE Intl. Workshop on Software Engineering for Context-Aware
Systems and Applications, 2009.
[17]
J.J. Martinez and I.R. Salavert. A conceptual model for context-aware dynamic architectures. In
Proc. of the 23rd Intl. Conf. on Distributed Computing Systems Workshops, pages 138–143,
2003.
[18]
S. Presecan and N. Tomai. Distributed context provisioning and reaction middleware. In Proc. of
the 5th Intl. Conf. on Intelligent Computer Communication and Processing, pages 351–354,
2009.
[19]
M. Ritchie. Pre-& Post-Processing for Service Based Context-Awareness. Technical report,
Technical Report Equator-02-023, University of Glasgow/ Department of Computing Science,
September 2002.
[20] A. Schmidt et al. Context acquisition based on load sensing. In Proc. of UbiComp, pages
333–350, 2002.
[21]
K. Sheikh, M. Wegdam, and M. Van Sinderen. Quality-of-context and its use for protecting
privacy in context aware systems. Journal of Software, volume 3, page 83, 2008.
[22] K. Sheikh, M. Wegdam, and M. van Sinderen. Middleware support for quality of context in
pervasive context-aware systems. In Proc. of the 5th Intl. Conf. on Pervasive Computing and
Communications Workshops, pages 461–466, 2007.
[23] K. Wan, V. Alagar, and J. Paquet. An architecture for developing context aware systems. Lecture
Notes in Computer Science, 3946:48–61, 2006.
[24] W. Xue, H. Pung, P.P. Palmes, and T. Gu. Schema matching for context aware computing. In Proc.
of UbiComp, 2008.
[25]
T. Yamabe, A. Takagi, and T. Nakajima. Citron: A context information acquisition framework for
personal devices. In Proc. of the 11th Intl. Conf. on Embedded and Real-Time Computing Systems
and Applications, 2005.
[26] S.J.H. Yang, A.F.M. Huang, R. Chen, S.S. Tseng, and Y.S. Shen. Context model and context
acquisition for ubiquitous content access in ULearning environments. In Proc. of the Intl. Conf.
on Sensor Networks, Ubiquitous, and Trustworthy Computing, 2006.
Authenticated
:23 PM

5 Protecting Context Privacy
Abstract: Context-awareness and privacy protection is a pair of contradictory require-
ments in context-aware computing. On one hand, context-aware systems must collect
sufﬁcient context information reﬂecting physical surroundings such as users’ loca-
tion, activity, habits, etc. to make intelligent decisions without user interaction. On
the other hand, context is often linked to individuals (e.g., location of a person) and as
such falls under privacy directives. To reconcile privacy protection (automatically ful-
ﬁlling users’ privacy wishes) and context-awareness (provided by context histories),
this chapter ﬁrst reviews privacy protection techniques in the database ﬁeld. Two con-
text privacy protection strategies, including encrypted context information searching
and life-cycle management of privacy-sensitive context information, are particularly
detailed.
5.1 Balancing Privacy and Smartness
Undeniably, computing systems are more and more aware of their surroundings,
thanks to widely spread smart devices able to continuously nourish the computing
infrastructure with information describing the real world [35]. Not only lightweight
devices like cell phones, PDA, RFID tags localizing objects [17, 60] and employees but
also chips penetrate commonly used objects such as clothes [43], televisions [44], GPS-
equipped cars, ﬂoors [41], etc., and even target at the human body [50, 61]. This new
combination of technologies is pushing toward so called pervasive computing having
real-time knowledge of the surroundings, and this knowledge is generally referred to
as context.
Context gives the ability to computing systems to become context-awareness, i.e.,
endowing the systems with enough smartness to (1) minimize user-machine inter-
action, and (2) be deployed in daily life areas (like streets, supermarkets, homes,
etc.) with nontraditional stationary desktop-based computing interfaces. Continu-
ously sensed context, reﬂecting individuals’ location, behavior, mood, or habits etc., is
needed to reach those smartness requirements, i.e., to adapt applications to ﬁt the en-
vironment; infer information or draw conclusions from rules and observations; learn
in the sense of using experiences to improve performance, and to anticipate what to do
next [20]. However, an obvious but important remark is that the smartness of context-
aware systems is tightly coupled to accuracy quality and quantity of the available past
and present context, as exposed in [52, 62] dedicated to usage of context histories in
smart environments.
Although context information might be considered less sensible than traditional
data like health care folders or banking information, it falls under privacy regula-
tion when linked to an individual (e.g., location of a person), given the deﬁnition
of personal data in the worldwide privacy acts [40]. Particularly, with context-aware
DOI 10.1515/9783110556674-005
Authenticated
:25 PM

118
5 Protecting Context Privacy
ambient intelligent spaces being planned to be developed everywhere, this will lead
in the extreme to sense and archive everyone’s actions and moves everyday [20, 37].
Imagine how many of us would feel comfortable if every of our activities was sensed
and tracked by our surroundings. Also, with sufﬁcient accuracy and volumes, context
may become of main interest [35] for malicious usage, e.g., to monitor employees in
companies, detect behavior of inhabitants and/or company staff to prepare a robbery
or for marketing purposes, or check people behaviors before contracting insurance
contracts. This inevitably constitutes a critical privacy threat.
Nevertheless, enforcing privacy protection in context-aware applications gener-
ates the difﬁculty in controlling and exploiting the content (volume and accuracy)
of context histories. In fact, privacy has widely been recognized as being applica-
tion killer in context-aware ambient intelligent computing, forming a crucial problem.
The goal of this chapter is to investigate this issue and provide sensible solutions to
reconcile context-awareness and privacy in the ambient intelligent world.
5.2 Privacy Protection Techniques
Privacy is the right of individuals to determine for themselves when, how, and to what
extent information about them is collected, stored, and communicated to others. In
the data management ﬁeld, several research efforts have been devoted to address
issues related to the development of privacy preserving data management techniques.
5.2.1 Access Control
5.2.1.1 Overview of Access Control
The function of access control is to control which principals (persons, processes, ma-
chines, etc.) have access to which resources in the system [6]. It is the traditional center
of gravity of computer security. In the database ﬁeld, access control is widely used to
exert control who can get access to a subset of the database where much sensitive
structured data resides. The current SQL standard allows coarse-grained access both
to database tables and to views. Fine-grained access control policies based on author-
ization predicates are also enforced in Refs. [15, 16, 34]. For instance, each employee
in an organization is authorized to access his/her own record in the employee table.
Typical access control models include discretionary access control, mandatory access
control, role-based access control, and purpose-based access control.
(1) Discretionary Access Control
Discretionary access control (DAC) is an access policy determined by the owner of
the resource. The owner decides who is allowed to access the resource and what
privileges he/she has [36]. The resource’s initial owner is usually the subject that
Authenticated
:25 PM

5.2 Privacy Protection Techniques
119
caused it to be created. DAC is the traditional ﬁle access restriction mechanism in unix
systems.
(2) Mandatory Access Control
Mandatory access control (MAC) is an access policy, allowing access to the resource
if and only if corresponding rules exist that allow a speciﬁc user to access the re-
source [48]. In MAC, sensitive resource is usually protected using a partial ordering
of sensitivity levels (e.g., top-secret, conﬁdential, classiﬁed, etc.). Every user and re-
source have sensitivity labels assigned to them. A user’s sensitivity label speciﬁes its
level of trust. A resource’s sensitivity label speciﬁes the level of trust required for ac-
cess. In order to access a given resource, the user must have a sensitivity level equal
to or higher than the requested resource. Two methods are commonly used for apply-
ing mandatory access control. (1) Rule-based (or label-based) access control, which
deﬁnes speciﬁc conditions for access to a requested resource. (2) Lattice-based access
control, which deﬁnes complex access control involving multiple resources and/or
users.
(3) Role-based Access Control
Role-based access control (RBAC) is an access policy determined by the system, not
the owner [23, 49]. Unlike DAC which allows users to control access to their resources,
RBAC controls resource access at the system level outside of the user’s control. It
deﬁnes which permissions belong to which role. A role in RBAC can thus be viewed as
a set of permissions. Roles can then be assigned to users. This makes administration of
access to sensitive resources easier: a particular user is simply assigned an appropri-
ate role, and this role deﬁnes the permissions. Roles can be combined in a hierarchy
where higher-level roles subsume permissions owned by sub-roles.
(4) Purposed-based Access Control
Purposed-based access control (PBAC) is an access policy, which associates purpose
information to the resources [12]. It regulates access to those resources based on the
purpose for which it needs to be accessed. By using the concept of intended purposes,
it is possible to describe for which purposes the resources can be accessed, and which
purposes cannot be used to access the resources. It is the system’s responsibility to
determine the access purpose, and match this with the set of intended purposes to
decide whether or not access will be granted. Access purposes can be associated with
roles, which can be managed using regular RBAC techniques.
5.2.1.2 Combining Access Control and Privacy Protection
Access control can be employed to restrict access to and use of data, thereby,
limit information disclosure according to privacy policies. Recently, Chaudhuri et al.
presented an architecture to integrate access control primitives and privacy preserving
mechanisms within a database system in a principled manner [16]. The basic idea is to
enhance an authorization policy with the abstraction of noisy views that encapsulate
previously proposed privacy mechanisms. These noisy views represent authorization
Authenticated
:25 PM

120
5 Protecting Context Privacy
views and are implemented based on differentially private algorithms. By combin-
ing authorizations and differentially private views in this manner, queries that refer
to both the base tables and the differentially private views can be supported, res-
ulting in a system that is more powerful than using access control techniques or
differential privacy techniques in isolation. On a cloud setting, Roy et al. also com-
bined access control primitives with differential privacy, where the execution engine
is MapReduce [45].
5.2.2 Platform for Privacy Preferences (P3P)
To give users control of their personal information when browsing websites, the World
Wide Web Consortium (W3C) develops the Platform for Privacy Preferences (P3P). It
allows websites to declare their intended use of information they collect about web
browser users in a machine-readable XML format, known as P3P policy, which can
programmatically be compared against user’s privacy preferences [59].
5.2.2.1 Overview of P3P
The basic mechanism of P3P is as follows. When a website uses P3P, it sets up a set
of policies that allow it to state the intended uses of personal information that may
be gathered from the site visitors. When a user decides to use P3P, s/he sets the own
set of policies and states what personal information s/he allows to be seen by the web
sites that they visit. Then when a user visits a site, P3P will compare what personal
information the user is willing to release, and what information the server wants to
get. If the two do not match, P3P will inform the user and ask if he/she is willing to
proceed to the site, As an example, a user may store in the browser preferences that
information about their browsing habits should not be collected. If the policy of a
website states that a cookie is used for this purpose, the browser automatically rejects
the cookie [42].
The P3P protocol is comprised of two parts.
(1) Privacy policies indicate which information will be collected? for what pur-
pose? who may see the information? and for how long the information will be kept.
They are described in an XML format as a sequence of elements.
(2) Privacy preferences are a machine-readable speciﬁcation of user’s prefer-
ences that can be programmatically compared against a privacy policy. They are
expressed as a list of rules. Each rule includes a rule behavior and a rule body. The rule
behavior speciﬁes the action to be taken if the rule ﬁres. The behavior can be request,
implying that the policy conforms to preferences speciﬁed in the rule body. It can be
block, implying that the policy does not respect user’s preferences. See Ref. [58] for
other behaviors. The rule body provides the pattern that is matched against a policy.
The format of a pattern follows the format used in specifying privacy policies.
Authenticated
:25 PM

5.2 Privacy Protection Techniques
121
5.2.2.2 Limitations of P3P
Although P3P enables users to have control over the information a website collects, it
does not specify any mechanisms for enforcing that sites act according to their stated
privacy policies [3]. P3P policies published by websites are thus not trusted by users.
The resulting P3P framework does not provide a coherent view of available privacy
protection mechanisms to the user [26]. Besides, low P3P adoption impedes client ad-
option by users. The languages available to describe user privacy preferences are also
not sufﬁciently expressive.
5.2.2.3 Implementing P3P in Databases and Enterprises
Agrawal et al. applied the database technology to checking P3P privacy policies
against users’ privacy preferences at the server side [3]. A web site deploying P3P ﬁrst
installs its privacy policies in a database system. User’s privacy preferences are trans-
lated into SQL or XQuery. Then database querying technology is used for matching
user’s preferences against privacy policies. The performance study in [3] showed that
it can lead to adequate performance in practical deployment of P3P [18].
Similar to websites, enterprises often collect large amounts of personal data from
their customers. To enforce P3P-like policies in enterprises, Ashley et al. outlined an
architecture based on access control techniques [7], where enterprises publish privacy
statements that outline how data is used and shared. The Platform for Enterprise Pri-
vacy Practices, referred to as E-P3P, is then deﬁned as a ﬁne-grained privacy policy
model. A chief privacy ofﬁcer uses E-P3P to formalize the internal handling of col-
lected data in the enterprise. A user is allowed to use certain collected data for a
given purpose if and only if the E-P3P authorization engine allows this request based
on the applicable E-P3P policy. By enforcing such privacy practices, E-P3P ensures
enterprises to keep their promises and prevent accidental privacy violations.
5.2.3 Hippocratic Databases
The Hippocratic Oath has guided the conduct of physicians for centuries, serving as
the basis of doctor–patient relationship.
And about whatever I may see or hear in treatment, or even without treatment, in the life of
human beings – things that should not ever be blurted out outside – I will remain silent, holding
such things to be unutterable.
– Hippocratic Oath
In the same spirit as the Hippocratic Oath by doctors – they swear to ethically prac-
tice medicine — Agrawal et al. introduce the concept of hippocratic databases to urge
the database systems to respect and enforce the privacy once users’ privacy-sensitive
Authenticated
:25 PM

122
5 Protecting Context Privacy
data enters the systems [2]. Hippocratic databases are built upon the following ten
principles, derived from real-world privacy regulations and guidelines [18, 45].
(1) Purpose speciﬁcation, requiring that the purpose for which the personal
information was collected shall be stored with that information in the database.
(2) Consent, requiring that the purpose for which the personal information was
collected shall have the consent of the donor.
(3) Limited collection, requiring that the personal information collected shall only
be disclosed for purposes for which consent has been given.
(4) Limited use, requiring that the database shall only support queries that are
consistent with the speciﬁed purpose.
(5) Limited disclosure, requiring that the personal information shall not be distrib-
uted for purposes other than those for which there is donor consent.
(6) Limited retention, requiring that the personal information shall be retained
only as long as necessary to fulﬁll the purpose for which it was collected.
(7) Accuracy, requiring that the personal information stored in the database
should be accurate and up-to-date.
(8) Safety, requiring that the personal information shall be protected by security
safeguards against theft and other misappropriation.
(9) Openness, requiring that the donor shall be able to access all information about
him/her stored in the database.
(10) Compliance, requiring that the donor shall be able to verify compliance with
the stated policy and the database shall be able to address any challenges.
Agrawal et al. sketched out a reference architecture for Hippocratic databases.
An important characteristic of the architecture is that it uses some privacy metadata
tables, including privacy-policies tables and privacy authorizations table. The former
captures the privacy policies, while the later captures the access controls that support
the privacy policies [2].
5.2.4 Anonymity
Another well-known technique to address the privacy of released data is to modify
the data by removing all information that can directly link data items with indi-
viduals. Such a process is called data anonymity, which typically refers to the state
of an individual’s personal identity, or personally identiﬁable information, being
publicly unknown. However, simply removing identity information, like names or
social-security numbers, from the released data may not be enough to anonymize the
data. There are many examples showing that even when such information is removed
from the released data, the remaining data combined with other information sources
may still link the information to the individuals it refers to.
To overcome this problem, approaches based on generalization and suppression
techniques have been proposed, the most well known of which is based on the notion
of k-anonymity, proposed by Sweeney in Ref. [54]. A release provides k-anonymity
Authenticated
:25 PM

5.2 Privacy Protection Techniques
123
Table 5.1: A 2-anonymity database relation.
ID
Gender
Birth
ZIP
1
male
1965
0214*
2
male
1965
0214*
3
female
1964
0213*
4
female
1964
0213*
protection if the information for each person contained in the release cannot be
distinguished from at least (k-1) individuals, whose information also appears in the
release.
Two methods for achieving k-anonymity are: (1) generalization which replaces
an individual attribute value with a broader category (e.g., Age = 25 becoming Age ∈
[27, 64]), and (2) suppression which replaces an individual attribute value or part of an
attribute value with symbol ∗.
Table 5.1 shows an example of a database relation that adheres to k-anonymity,
where k = 2, tuples of ID = 1 and ID = 2 are identical, and tuples of ID = 3 and ID = 4 are
identical.
Obviously, k-anonymity can be guaranteed with the replacement of every cell with
a ∗, but this renders data useless. A minimum cost k-anonymity solution suppresses
the fewest number of cells necessary to guarantee k-anonymity.
5.2.5 Encryption
Although not completely preventable, the chance of trail disclosure because of a
piracy attack can be limited by applying various security methods. Encryption is a
prototypical technique used in secured data management, where privacy-sensitive
information is distributed and stored in encrypted form [33].
In an encryption scheme, the clear plaintext information is encrypted through an
encryption algorithm into an unreadable ciphertext, such that hackers cannot read it,
while authorized parties can [24]. This is usually achieved by means of an encryption
key, which speciﬁes how the information is to be encoded. Any adversary that can see
the ciphertext should not be able to determine anything about the original informa-
tion. However, an authorized party is able to decode the ciphertext using a decryption
algorithm, that usually requires a secret decryption key. There are two basic types
of encryption schemes: symmetric-key encryption and public-key encryption [24]. In
symmetric-key schemes, the encryption and decryption keys are the same. Thus com-
municating parties must agree on a secret key before they wish to communicate. In
public-key schemes, the encryption key is published for anyone to use and encrypt
information. However, only the receiving party has access to the decryption key and is
capable of reading the encrypted ciphertext.
Authenticated
:25 PM

124
5 Protecting Context Privacy
W3C also recommends an XML Encryption Syntax to allow the encryption of XML
data using a combination of symmetric and public keys, where element content is
encrypted by means of a symmetric key that in turn is encrypted by means of the
public key of the recipient [64].
5.3 Search Over Encrypted XML Context Information
Nevertheless, securing private information in ciphertext should not hinder its efﬁcient
processing by context-aware systems. As search is one of the basic operations carried
out on context data, this section addresses the issue on effective and efﬁcient search-
ing for context information in encrypted data. A straightforward approach to search
on encrypted data is to decrypt the ciphertext ﬁrst, and then do the search on the clear
decrypted XML data. However, this inevitably incurs a lot of unnecessary decryption
efforts, leading to a very poor query performance. especially when the searched data
is huge, while the search target comes only from a small portion of it. To tackle this
problem, a question arises: “Is it possible to directly search encrypted data without
decryption?”
Hacigümü¸s et al. proposed to execute SQL-based queries over encrypted rela-
tional tables in a database-service provider model, where an algebraic framework is
described for query rewriting over encrypted attributed representation [29, 30]. With
XML becoming the dominant standard for describing and interchanging data in a
format that is both human-readable and machine-readable, a three-phased frame-
work for encrypted XML-formatted context information (like personal information)
search is presented in the following subsection.
5.3.1 A Three-Phased Search Framework
Assume the availability of DTD for each XML data, and all XML DTDs and XML data
are encrypted and stored in a database.
5.3.1.1 The Framework
Figure 5.1 gives a generic framework for conducting efﬁcient queries on encrypted
XML data. It is comprised of three phases: data encoding, candidate identiﬁcation, and
query execution.
Phase-1 (Data Encoding)
The aim of this phase is to prepare for efﬁcient querying over encrypted XML data
by encoding each XML DTD and associated documents before they are encrypted and
stored in the database. Such an encoding is carried out in two steps – Encode XML DTD
and Encode XML Document. The coding results of XML DTDs and documents are stored
in two separate databases, called XML DTD Encoding Database and XML Document
Encoding Database.
Authenticated
:25 PM

5.3 Search Over Encrypted XML Context Information
125
query
Data Encoding (off-line)
Encode XML DTD
Encode XML Document
Encrypted Candidate
XML DTDs
Encrypted XML
DTDs & Documents
XML DTDs
Encoding DB
XML Documents
Encoding DB
Identify Candidate
XML DTDs 
Identify Candidate
XML Documents
Encrypted Candidate
XML Documents
Encrypted XML Data Search
Tree Search (on-line)
result
Candidate Identification (on-line)
Figure 5.1: A framework for querying over encrypted XML data.
In response to a query at run-time, these encodings can be used to pre-select potential
target documents without the need to decrypt the whole document set in the database.
These potential target documents are called candidate documents.
Phase-2 (Candidate Identiﬁcation)
It is obvious that decrypting all encrypted XML documents to answer a query inevit-
ably incurs an excessive overhead, especially when the target data constitutes a small
portion of the database. In order to make encrypted XML query processing truly prac-
tical and computationally tractable, and meanwhile preserve security, for each query,
we incorporate a pre-processing stage, whose aim is to ﬁlter out impossible candid-
ates so that decryption and query execution can be more focused on potentially target
documents.
Two steps are conducted in this phase. First, a set of candidate XML DTDs are
identiﬁed through the step – Identify Candidate XML DTDs, which examines query
conditions, expressed in terms of XPath expressions [63, 66], against DTD encodings
in the DTD Encoding Database. Then, corresponding to each selected candidate DTD,
the Identify Candidate XML Documents step further ﬁlters out candidate documents
based on documents’ encodings in the Document Encoding Database. The candidate
DTD set and document set returned are subsets of the original encrypted DTD set and
document set, respectively.
Phase-3 (Tree Search)
The identiﬁed candidate DTDs and documents, returned from Phase-2, constitute
search targets, upon which a tree search is directly performed without data decryp-
tion [10].
5.3.1.2 Design Philosophies
The design of the three phases in the framework is guided by the following philosoph-
ical considerations [22].
Authenticated
:25 PM

126
5 Protecting Context Privacy
Phase-1 (Data Encoding) (ofﬂine)
–
Effective encoding schemas. The encoding schemas devised in this phase should
be effective enough to facilitate fast identiﬁcation of candidate DTDs and docu-
ments given a query rising in Phase-2.
–
Safe encodings. It must be guaranteed that encodings do not leak any information
to externals. In other words, from the encodings, readers cannot learn anything
about the encoded DTDs and XML documents.
–
Condensed encoding size. The space used to store the encodings of XML DTDs and
documents should be as small as possible.
Phase-2 (Candidate Identiﬁcation) (online)
–
Completeness.
To ensure the correctness of query execution, the query pre-
processing phase must be complete, which means that its pre-selected candidate
DTDs and documents must constitute the supersets of the real target DTDs and
documents for the query.
–
High selectivity. It is clear that for efﬁciency, Phase-2 should only deliver candidate
DTDs and documents with high likelihood of being real target DTDs and docu-
ments, because for each selected candidate XML data, we need to decrypt it ﬁrst
and then query it.
–
Efﬁciency. Efﬁcient strategies are needed in this phase to speed up the candidate
pre-selection process by checking the queries against the encodings of XML data.
–
Hidden query support. For deep security, it is sometimes also desirable to realize
a query over encrypted XML data even without revealing the query itself. That is,
the query itself is encrypted. We leave this issue to our further work.
Phase-3 (Tree Search) (online)
–
Efﬁciency. Efﬁcient search algorithms exploiting tree structures are needed to
handle large amounts of semi-structured XML encrypted data.
5.3.2 Data Encoding Phase
In this subsection, a hash-based strategy is proposed to encode encrypted XML data
for the candidate identiﬁcation phase. Based on the encodings obtained, the later can
effectively ﬁlter out search candidates, i.e., potential targets, from among a large set
of blind documents in the database. Due to different characteristics and functions of
XML DTDs and documents, XML DTDs and XML documents are encoded separately
using different encoding schemas.
In the following, the computation method for encoding XML DTDs, followed by
the method for encoding XML documents, is given. For ease of explanation, a run-
ning example shown in Figure 5.2 is used throughout the discussion. A graphical
Authenticated
:25 PM

5.3 Search Over Encrypted XML Context Information
127
      <number> 123456789 </number>
  </creditCard>
</payInfo>
   <amount> 100.0 </amount>
<payInfo>
   <creditCard  limit=1000>
      <address> Twente 7500 AE, Netherlands </address>
]>
      <name> Alice </name>
<!DOCTYPE  payInfo [
     <!ATTLIST  creditCard  limit  CDATA  #IMPLIED>
     <!ELEMENT  payInfo (creditCard?, amount+)>
     <!ELEMENT  number (#PCDATA)> 
     <!ELEMENT  creditCard (number, name, address)>
     <!ELEMENT  name (#PCDATA)> 
     <!ELEMENT  address (#PCDATA)> 
     <!ELEMENT  amount (#PCDATA)>
(c) 
Twente
Enschede
Netherlands
(a) 
100.0
(b) 
limit
number
name
address
amount
creditCard
payInfo
1000
123456789
Alice
Figure 5.2: A running example of an XML document with its DTD: (a) An XML DTD example – DTD1;
(b) An XML document example that conforms to DTD1; (c) A graphical representation of the DOM tree
structure of DTD1 with the example document.
representation of the DOM tree structure of the example DTD, DTD1, and the example
document is outlined in Figure 5.2(c).
5.3.2.1 Encoding XML DTDs
An XML DTD deﬁnes the legal building blocks of its conforming XML documents, like
what elements, attributes, etc. are permitted in the documents [65]. These components
construct a hierarchical tree structure that underlies the contents of the documents,
with each path of the tree addressing a certain part of an document.
(1) Path p in an XML DTD
As the data encoding phase works on the basis of XPath expressions embedded in
a query like XQuery, to prepare for efﬁcient candidate selection, each XML DTD is
encoded in the unit of path. The notions of path and path length are deﬁned as follows.
Deﬁnition 19. A path p is a sequence of nodes n1, n2, ⋅⋅⋅, nk, denoted as p
=
(n1/n2/ ⋅⋅⋅/nk), where for any two consecutive nodes, ni and ni+1 (1 ≤i ≤k – 1, k ≥1),
there exists an edge between them.
The length of path p, denoted as |p|, is the total number of edges in the path. That
is, |p = (n1/n2/ ⋅⋅⋅/nk)| = k – 1.
◻
Table 5.2 lists all the paths, which are of various lengths, extracted from the example
DTD DTD1 in Figure 5.2. Here, the content nodes under the dotted line are exempt from
consideration, since they do not appear in the DTD.
Authenticated
:25 PM

128
5 Protecting Context Privacy
Table 5.2: Paths extracted from the example XML DTD.
Path Length
Path
2
p1 = (payInfo/creditCard/limit)
p2 = (payInfo/creditCard/number)
p3 = (payInfo/creditCard/name)
p4 = (payInfo/creditCard/address)
1
p5 = (payInfo/creditCard)
p6 = (payInfo/amount)
p7 = (creditCard/limit)
p8 = (creditCard/number)
p9 = (creditCard/name)
p10 = (creditCard/address)
0
p11 = (payInfo)
p12 = (creditCard)
p13 = (amount)
p14 = (limit)
p15 = (number)
p16 = (name)
p17 = (address)
(2) Hash Function HashFunc(p)
In essence, hashing is applied to each path of an XML DTD for DTD encod-
ings. Paths of different lengths are hashed into different hash tables named
DTDHashTable0, DTDHashTable1, ⋅⋅⋅, DTDHashTablemax_pathLen. All paths of length l
(where 1 ≤l ≤max_pathLen), no matter which DTD it comes from, will share one
single hash table DTDHashTablel, with each bucket indicating a set of DTDs, whose
paths have been hashed into the bucket. Suppose a path p is extracted from DTD1, the
hash function HashFunc (p) computes its hash value, i.e., bucket address in the hash
table DTDHashTable|p|. Detailed computation of hash values will be given shortly. The
corresponding bucket entry is marked with an indicator of DTD1, signifying the DTD
where p locates.
To ﬁlter out noncandidate DTDs for a query, we compute the hash values for all
XPaths in the query using the same hash function, and then check the corresponding
buckets in the DTD hash tables to obtain a subset of DTDs that possibly contain the
requested paths. These DTDs are candidate DTDs to be considered for the query.
(3) Computing Hash Function HashFunc(p)
Algorithm 1 elaborates the procedures in computing the hash value for path p =
(n1/n2/ ⋅⋅⋅/nk). It proceeds in the following three steps.
Step 1. Node names in path p which could be of different lengths are uni-
formly chopped into the same size s, given by users as an input parameter,
through the function ChopName (Algorithm 1, line 1). For example, let s = 4.
ChopName(“creditCard”, 4) = “cred”, ChopName(“payInfo”, 4) = “payI”,
ChopName(“name”, 4) = “name”.
Authenticated
:25 PM

5.3 Search Over Encrypted XML Context Information
129
Algorithm 1: Hash function HashFunc(p).
Input: path p = (n1/n2/ ⋅⋅⋅/nk),
a ﬁxed size s for node names,
hash table size SizeDTDHashTable|p|;
Output: hash value of p.
1
For each node ni (1 ≤i ≤k), chop its name uniformly into an s-letter
string. ChopName(ni, s) = xni,1xni,2 ⋅⋅⋅xni,s, where xni,1, xni,2, ⋅⋅⋅,
xni,s are letters in the name string of node n.
2
For each s-letter node name xni,1xni,2 ⋅⋅⋅xni,s, convert it into a
decimal integer.
Base26ValueOf(xni,1xni,2 ⋅⋅⋅xni,s) = offset(xni,1) ∗26s–1+
offset(xni,2) ∗26s–2 + ⋅⋅⋅+ offset(xni,s) ∗260 = Vni,
where offset(xni,j) (1 ≤j ≤s) returns the position of letter xni,j
among 26 letters.
3
Compute hash value of p = (n1/n2/ ⋅⋅⋅/nk).
HashFunc(n1/n2/ ⋅⋅⋅/nk) = (Vn1 ∗10k–1 + Vn2 ∗10k–2 + ⋅⋅⋅+
Vnk ∗100) mod SizeDTDHashTable|p|.
Step 2. The chopped node name strings which are of ﬁxed size after Step 1 are fur-
ther converted into decimal integers via function Base26ValueOf (Algorithm 1, line 2).
Example 19 explicates how it works when the size of node name string is set to 4.
Example 19. When we let a 4-letter node name x1x2x3x4, which are case insensitive,
represent a base-26 integer, we let the letter “a” represent the digit-value 0, the letter
“b” represent the digit-value 1, the letter “c” represent the digit-value 2, the letter “d”
represent the digit-value 3, and so on, up until the letter “z”, which represents the
digit-value 25. Given a letter, function offset returns such a digit-value. The 4-letter
node name x1x2x3x4 can thus be converted into a decimal integer using the formula:
Base26ValueOf(x1x2x3x4) =
offset(x1) ∗263 + offset(x2) ∗262 + offset(x3) ∗261 + offset(x4) ∗260.
Assume that x1x2x3x4 = “name”, since the digit-values of “n”, “a”, “m” and “e” are
offset(“n󸀠󸀠) = 13, offset(“a󸀠󸀠) = 0, offset(“m󸀠󸀠) = 12, and offset(“e󸀠󸀠) = 4 respectively, we
have
Base26ValueOf(“name”) = 13 ∗263 + 0 ∗262 + 12 ∗261 + 4 ∗260
= 13 ∗17576 + 0 + 312 + 4 = 228802.
In a similar way, we have
Base26ValueOf(“cred”) = 2 ∗263 + 17 ∗262 + 4 ∗261 + 3 ∗260
= 2 ∗17576 + 17 ∗676 + 104 + 3
= 35152 + 11492 + 104 + 3 = 46751.
◻
Authenticated
:25 PM

130
5 Protecting Context Privacy
A general calculation of Base26ValueOf is:
Base26ValueOf(x1x2 ⋅⋅⋅xs) =
offset(x1) ∗26s–1 + offset(x2) ∗26s–2 + ⋅⋅⋅+ offset(xs) ∗260.
Step 3. Hash function HashFunc derives the hash value of p = (n1/n2/ ⋅⋅⋅/nk) based
on the value Vni returning from function Base26ValueOf on each node ni (Algorithm 1,
line 3).
HashFunc(n1/n2/ ⋅⋅⋅/nk) =
(Vn1 ∗10k–1 + Vn2 ∗10k–2 + ⋅⋅⋅+ Vnk ∗100) mod SizeDTDHashTable|p|
To illustrate, let’s see the following example.
Example 20. Given a path p = (creditCard/name), where k = 2 and |p| = 1, let s=4 and
SizeDTDHashTable|p| = SizeDTDHashTable1 = 8.
Step 1: ChopName(“creditCard”, 4) = “cred”,
ChopName(“name”, 4) = “name”.
Step 2:Base26ValueOf(“name”) = 228802,
Base26ValueOf(“cred”) = 46751.
Step 3:HashFunc(creditCard/name) = (Base26ValueOf(“cred”) ∗101 +
Base26ValueOf(“name”) ∗100) mod SizeDTDHashTable1
= (46751∗10 + 228802) mod 8 = 0
Therefore, path p=(creditCard/name) is hashed to the ﬁrst bucket of the hash
table DTDHashTable1. We mark this bucket with a symbol to indicate that DTD1
contains p.
◻
In order to provide a more complete overview on the hash-based encoding method,
we introduce another DTD example DTD2 as shown in Figure 5.3. Using the same hash
function, Figure 5.4 illustrates the hash results for all the paths from DTD1 and DTD2.
<!DOCTYPE  payInfo [
    <!ELEMENT  order (person, invoice)>
     <!ELEMENT  name (#PCDATA)> 
     <!ELEMENT  gender (#PCDATA)> 
     <!ELEMENT  person (name, gender)>
]>
     <!ELEMENT  dueDate (#PCDATA)> 
     <!ELEMENT  invoice (dueDate)> 
(a)
(b)
order
dueDate
invoice
person
gender
name
Figure 5.3: Another DTD example with its DOM tree structure: (a) Another DTD example – DTD2l (b) A
tree structure of DTD2.
Authenticated
:25 PM

5.3 Search Over Encrypted XML Context Information
131
DTD1
DTD2
DTD1
DTD2
DTD1
4
3
1
DTD1
DTD2
DTD2
DTD1
DTD2
DTD1
DTD2
DTD2
DTD1
DTD1
(sizeDTDHashTable2=8) 
DTDHashTable2
(sizeDTDHashTable1=8) 
DTDHashTable1
Hash Address
Hash Address
2
DTD2
DTD2
DTD1
DTD2
DTD1
DTD1
DTD1
4
3
7
6
5
2
1
0
5
6
7
0
4
3
7
6
5
2
1
0
Hash Address
(sizeDTDHashTable0=8) 
DTDHashTable0 
DTD1
DTD2
DTD2
Figure 5.4: Encodings of the example DTD1 and DTD2 (DTDHashTable0, DTDHashTable1 and
DTDHashTable2).
5.3.2.2 Encoding XML Documents
XML documents that conform to one XML DTD possess a similar structure, but with
possibly different element contents and/or attribute values to distinguish different
documents. For instance, one conforming document of the example DTD shown in
Figure 5.2 has a limit attribute of value 1000, represented as limit=1000 for simplicity.
Its elements number, name, address, and amount have contents 123456789, “Alice”,
“Twente, Enschede, Netherlands” and 100.0, respectively.
After encoding XML DTDs, i.e., all possible paths with each containing a sequence
of nodes corresponding to elements or attributes, the second task of the query pre-
paration phase is to encode their conforming documents, i.e., all pairs of element
and element content (element, element content), attribute and attribute value (attrib-
ute, attribute value). Due to the different nature of contents, encoding documents
is conducted in a different way from encoding DTDs, with the result stored in the
Document Encoding Database. In the following, we describe the method of encod-
ing a pair, c = (cname, cval) (where cname denotes the element/attribute, and cval
denotes the corresponding element content/attribute value), into a hash table named
DOCHashTable.
A separate chaining strategy can be adopted to resolve hashing collision for
DOCHashTable. That is, all pairs that collide at a single hash address are placed on
a linked list starting at that address. The hash address of each pair is calculated
via function HashFunc(p) (Algorithm 1), using a different hash table size, which is
SizeDOCHashTable rather than SizeDTdHashTable|p|. In this case, path p always con-
tains only one node, which is p = (cname) and |p| = 0. For example, let s = 4, and
the size of hash table DOCHashTable equal to 4 (i.e., SizeDOCHashTable = 4). We have
Authenticated
:25 PM

132
5 Protecting Context Privacy
Table 5.3: Pairs of element/attribute with content/value in the example XML document, together with
their hash and mapped values.
(Element/Attribute cname, , Content/Value cval)
HashFunc(cname)
MapFunc(cval)
c1 = (limit, 1000)
0
1
c2 = (number, 123456789)
1
10
c3 = (name, “Alice”)
0
0
c4 = (address, “Twente, Enschede, Netherlands”)
2
25
c5 = (amount, 100.0)
1
7
ChopName(“limit”) = “limi”. Base26ValueOf(“limi”) = 11∗263 + 8∗262 + 12∗26 + 8 =
199064, HashFunc(limit) = 199064∗100 mod 4 = 0.
After the derivation of bucket address in the hash table DOCHashTable from cname,
the entry to be put into the corresponding bucket is computed based on cval, using the
technique developed in Ref. [29]. The basic idea is to ﬁrst divide the domain of node
cname into a set of complete and disjoint partitions. That is, these partitions taken to-
gether cover the whole domain; and any two partitions do not overlap. Each partition
is assigned a unique integer identiﬁer. The value cval of element/attribute node cname
is then mapped to an integer, corresponding to the partition where it falls [29]. For ex-
ample, we can partition the domain of attribute limit into [0, 500], (500, 1000], (1000,
∞] of identiﬁer 0, 1, 2, respectively. The limit value 1000 is thus mapped to integer 1,
and stored in the ﬁrst bucket of DOCHashTable, since HashFunc(limit) = 0. The hash
values for other pairs in the example document are calculated in the same way, which
are shown in Table 5.3.
Note that the partition of a domain can be done based on the semantics of data and
relevant applications. For instance, the domain of element name can be categorized
according to the alphabetical order. The domain of element address can be partitioned
according to province or country where located. For simplicity, order preserving con-
straint is enforced on such a mapping MapFunc : domain(cname) →Integer, which
means that for any two values cval1 and cval2 in the domain of cname, if (cval1 ≤cval2),
then MapFunc(cval1) ≤MapFunc(cval2).
Assume the mapping functions for number, name, address, and amount return
identiﬁers, as indicated in Table 5.3. Figure 5.5 plots the resulting encoding, i.e.,
DOCHashTable, for the example XML document given in Figure 5.2.
5.3.3 Candidate Identiﬁcation Phase
The aim of the query candidate identiﬁcation phase is to identify candidate DTDs
and documents by checking the query against the encodings of DTDs and docu-
ments, obtained after the data encoding phase. This subsection ﬁrst reviews XPath
expressions used in query representation, and then describes a method to match such
Authenticated
:25 PM

5.3 Search Over Encrypted XML Context Information
133
7
10
1
0
1
2
3
Hash Address
DOCHashTable
(sizeDOCHashTable =4)
c1
c3
c5
c2
c4
The mapped value from C4’s element content
25
0
Figure 5.5: Encodings of the example document (DOCHashTable).
XPath expressions to paths as described in the above subsection to facilitate candid-
ate DTD and document selection. A two-step procedure is ﬁnally illustrated to identify
candidate DTDs, followed by candidate documents for each selected candidate DTD.
5.3.3.1 XPath Expressions
The XPath language is a W3C proposed standard for addressing parts of an XML doc-
ument [66]. It treats XML documents as a tree of nodes corresponding to elements/
attributes, and offers an expressive way to specify and locate nodes within this tree.
XPath expressions state structural patterns that can be matched to paths, con-
sisting of a sequence of nodes in the XML data tree [5, 13]. Such paths can be either
absolute paths from the root of the data tree or relative one starting with some known
context nodes. The hierarchical relationships between the nodes are speciﬁed in
XPath expressions using parent–child operator (/) and ancestor–descendant operator
(//). For example, the XPath expression “/payInfo/creditCard/@limit” addresses limit
attribute of creditCard which is a child element of the payInfo root element in the docu-
ment. The name element in the relative path expression “//creditCard/name” is a child
relative to its parent creditCard element. The expression “/payInfo//name” addresses
name descendant element of the payInfo root element.
XPath also allows the use of a wildcard operator (∗or @∗), which can match
any element or attribute node with respect to the context node in the document data
tree. In addition, predicates, enclosed in square brackets ([ ]), can also be applied
to further reﬁne the selected set of nodes in XPath expressions. For example, “/pay-
Info/creditCard[@limit<1000]/name” selects the name elements of the XML document
if the attribute limit of creditCard has a value less than 1000.
Authenticated
:25 PM

134
5 Protecting Context Privacy
Operators like (|) and (and) can also be applied to select constituent nodes of
paths [66]. For instance, “/payInfo/(creditCard|cash)/name” expression selects every
name element that has a parent that is either a creditCard or a cash element, that in
turn is a child of a root element payInfo. On the contrary, “/payInfo/creditCard[@limit
and @dueDate]” indicates all the creditCard children of the root element payInfo that
must have both a limit attribute and a dueDate attribute.
5.3.3.2 Mapping XPath Expressions to Paths
For efﬁcient encoding-based query candidate pre-selection, an XPath expression e,
which is used to locate parts of a data tree, shall be matched to a set of paths through
the following three steps.
Step 1. Decompose XPath expression e into several ones at the point of // operator.
Since paths to be encoded by the ofﬂine query preparation phase have only
parent–child relationships (/) between two consecutive nodes (as shown in Table 5.2),
we break an XPath expression from the points where the // operator locates, into sev-
eral ones where each node, except for the ﬁrst one, is preﬁxed only by /. The resulting
XPath expressions thus contain no ancestor–descendant relationships (//) between
every two consecutive nodes.
Example 21. An XPath expression e = “payInfo[amount>100]//name” can be de-
composed into two shorter XPath expressions e󸀠
1 = “/payInfo[amount>100]” and
e󸀠
2 = “//name”. We use e
󳨐⇒1
e󸀠
1 ∧e󸀠
2 to denote such a semantically equivalent
decomposition.
◻
For ease of explanation, we signify the XPath expressions derived after Step 1 using a
prime symbol like e󸀠. They form the input of Step 2.
Step 2. Simplify predicate constraints in each XPath expression e󸀠to only hierarchical
relationships.
As DTD encoding relieves value constraints on path nodes, and focuses only
on their hierarchical relationships, to facilitate candidate DTD ﬁltering based on
path encodings, we relax value constraints on nodes like “[amount>100]” and
“[@limit=1000]”, speciﬁed in XPath predicate expressions, and keep only their inher-
ent parent-child or element-attribute relationships.
Example 22. The predicate constraint in e󸀠
1 = “/payInfo[amount>100]” implies that
amount is a child element of payInfo, whose value constraint is eliminated by aug-
menting a parent–child relationship between payInfo and amount, resulting in a more
relaxed XPath expression e󸀠󸀠
1 = “/payInfo/amount” after Step 2. We use 󳨐⇒2 to denote
such a simpliﬁcation transformation, i.e., e󸀠
1 󳨐⇒2 e󸀠󸀠
1 .
◻
Authenticated
:25 PM

5.3 Search Over Encrypted XML Context Information
135
Example 23. A predicate situated in the intermediate of an XPath expression like
“/payInfo[amount>100]/creditCard” leads to two XPath expressions being generated
after Step 2, which are “/payInfo/amount” and “/payInfo/creditCard”. That is, “/pay-
Info[amount>100]/creditCard” ⇒2 “/payInfo/amount” ∧
“/payInfo/creditCard”.
◻
Let e󸀠󸀠denote an XPath expression returned after Step 2.
Step 3. Eliminate logical | and and operators in each XPath expression e󸀠󸀠by rewriting
the expression into several ones logically connected with ∧or ∨.
To match the notion of path in Deﬁnition 19, every XPath expression after Step 2
which contains the logical operators | and and is substituted by a set of shorter XPath
expressions, which are logically connected via ∧or ∨.
Example 24. The
XPath
expression
e󸀠󸀠= “/payInfo/(creditCard|cash)/name”
can
be
viewed
as
two
disjunctive
expressions:
e󸀠󸀠󸀠
1 = “/payInfo/creditCard/name”,
e󸀠󸀠󸀠
2 = “/payInfo/cash/name”, denoted as e󸀠󸀠󳨐⇒3 e󸀠󸀠󸀠
1 ∨e󸀠󸀠󸀠
2 .
Similarly, the expression “/payInfo/creditCard[name and dueDate]” can be
equally transformed into
“/payInfo/creditCard/name” ∧“/payInfo/creditCard/dueDate”.
◻
After undergoing the above three steps, an original XPath expression is transformed
into a set of simple XPath expressions, which contain no ancestor–descendant re-
lationships between two consecutive nodes, no value constraints on nodes, and no
logical operators | and and. Each such kind of simple XPath expressions corresponds
to a path deﬁned in Deﬁnition 19.
Example 25. From an original XPath expression “/payInfo[amount>100]//
name”, we can derive two simple XPath expressions “/payInfo/amount” ∧
“//name”.
An XPath expression with a predicate constraint and operator (|) like
“/payInfo[amount>100]/(creditCard|cash)/name” leads to three simple XPath expres-
sions which are: “/payInfo/amount” ∧
(“/payInfo/creditCard/name” ∨“/payInfo/cash/name”).
◻
5.3.3.3 Identiﬁcation of Candidate DTDs and Documents
On the basis of simple XPath expressions generated from XPath expressions embed-
ded in a query, the concepts of candidate DTDs and documents can then be deﬁned
for a given query.
An XML DTD is called a candidate DTD for a query, if for every simple XPath ex-
pression derived from the query, there possibly exists a path p in the DTD, that matches
this simple XPath expression. In a similar fashion, we deﬁne that an XML document
Authenticated
:25 PM

136
5 Protecting Context Privacy
is a candidate document for a query, if and only if: (1) its DTD is a candidate DTD;
and (2) it possibly satisﬁes all predicate constraints on the nodes inside all the XPath
expressions embedded in the query.
The pre-selection of potential query targets starts with the identiﬁcation of can-
didate DTDs, followed by the identiﬁcation of candidate documents under each
candidate DTD that has been identiﬁed.
(1) Identifying Candidate DTDs by Hashing Paths
Given a query, to check out which encrypted DTDs are candidate DTDs for each simple
XPath expression generated from the query, we match it to a path p, and compute
the hash value for p using the same hash function HashFunc(p) (Algorithm 1) while
encoding the DTDs. According to the hash value (i.e., bucket address) returned, we
consult with the corresponding bucket in the hash table DTDHashTable|p|, which gives
all the DTDs that may possibly contain path p. The rationale for this is straightforward:
if path p is present in the DTD, it will be hashed to the bucket in DTDHashTable|p|, leaving
a mark for this DTD in the bucket entry.
Example 26. Suppose a query consists of only one simple XPath expression, corres-
ponding to the path p = (payInfo/creditCard/dueDate). Referring to the DTD encoding
schema illustrated in Figure 5.4, where s = 4 and SizeDTDHashTable2 = 8, its hash
value is computed as follows:
Step 1: ChopName(“payInfo”, 4) = “payI”,
ChopName(“creditCard”, 4) = “cred”,
ChopName(“dueDate”, 4) = “dueD”.
Step 2:Base26ValueOf(“payI”) = 264272, Base26ValueOf(“cred”) = 46751,
Base26ValueOf(“dueD”) = 66355.
Step 3:HashFunc(payInfo/creditCard/dueDate)
= (Base26ValueOf(“PayI”)∗102 + (Base26ValueOf(“cred”) ∗101
+ Base26ValueOf(“dueD”)∗100) mod SizeDTDHashTable2
= (264272∗100 + 46751∗10 + 66355) mod 8 = 1
Due to its hash value 1, we can be sure that the example DTD2 does not contain that
path, since the entry at address 1 in DTDHashTable2 only signiﬁes DTD1. As a result,
only DTD1 will be returned as the candidate DTD, DTD2 and its associated documents
can thus be discarded from the further search.
◻
(2) Identifying Candidate Documents by Hashing Element/Attribute and
Content/Value Pairs
After pre-selecting the candidate DTD set for the given query, we are now in the posi-
tion to ﬁlter out candidate documents underneath each candidate DTD. At this stage,
various value constraints in the form of [cname ( cval] (where cname denotes the name
Authenticated
:25 PM

5.3 Search Over Encrypted XML Context Information
137
of an element/attribute node, ( is one of the operators in {=, ̸=, <, ≤, >, ≥}, and cval de-
notes the element content/attribute value) on path nodes are taken into consideration.
Clearly, a candidate document must not violate any of the value constraints speciﬁed
within the XPath expressions in the query. We perform such kind of examination
based on the document encodings (i.e., DOCHashTable).
Taking the constraint [cname ( cval] for example, the node name cname (i.e., a
path containing only one node) is ﬁrst hashed into DOCHashTable via hash function
HashFunc(cname). Meanwhile, the range identiﬁer of cval can be calculated using the or-
der preserving function MapFunc(cval). Finally, each entry value v linked to the bucket
address HashFunc(cname) in DOCHashTable is compared: if ∃v (v ( MapFunc(cval)), then
the constraint [cname ( cval] possibly holds.
Example 27. Assume a query embeds an XPath expression
“/payInfo/creditCard[@limit>2000]/name”, which enforces a constraint [@limit >
2000] on creditCard element. Referring to the document encoding schema in Fig-
ure 5.5, where s = 4 and SizeDOCHashTable = 4. We have HashFunc(limit) = 0 and
MapFunc(2000) = 2.
Since all the entries at address 0 in DOCHashTable are either 1 or 0, which is not
greater than 2 (= MapFunc(2000)), therefore, the example document is not a candidate
document for this query, and can thus be discarded.
◻
Correctness. The key to the correctness of the above candidate identiﬁcation method
lies in the following lemma.
Lemma 1. The identiﬁed candidate DTDs (candidate documents) form the superset of
the target DTDs (candidate documents).
◻
Beweis. Stage I only removes DTDs which surely do not belong to a query target. This
is because if an XML DTD contains a path matching a simple XPath expression derived
from the query, the corresponding hash entry will surely have an indicator of this DTD,
which will return the DTD as a candidate.
Similarly, Stage II does not drop out potential target documents since any candid-
ate document to be queried may possibly satisfy the value constraints, as speciﬁed in
the XPath expressions.
◻
Lemma 1 ensures the completeness of our candidate pre-selection phase.
5.3.4 Tree Search Phase
This phase executes tree search against the encrypted candidate XML data, returned
from the candidate identiﬁcation phase. The search strategy is based on Song et al.’s
linear keyword search protocol on encrypted textual data [51].
Authenticated
:25 PM

138
5 Protecting Context Privacy
5.3.4.1 Song et al.’s Linear Keyword Search Protocol on Encrypted Textual Data
Assume client Alice stores her privacy-sensitive text data on an untrusted server
named Bob. She wants to search the information without revealing the plain text of
either the stored data, the query, or the query result. Song et al.’s linear search protocol
consists of three parts: (1) encryption, (2) keyword search, and (3) decryption.
(1) Encryption
Before Alice stores her sensitive information on Bob, she ﬁrst fragments her whole
plain text W into l ﬁxed-sized words W1, W2, ⋅⋅⋅, Wl. Each Wi has a ﬁxed length n.
She also generates encryption keys k󸀠and k󸀠󸀠, and a sequence of l random val-
ues S1, S2, ⋅⋅⋅, Sl using a pseudo random generator. Then she has or calculates the
following for each word Wi (1 ≤i ≤l).
The encrypted value Ci of Wi can then be sent to Bob for storage. Alice may now
forget the values of Wi, Xi, Li, Ri, ki, Ti, and Ci, but should still remember k󸀠, k󸀠󸀠, and Si.
(2) Keyword Search
After the encrypted data is stored by Bob, Alice can perform hidden search (where
search keywords are not revealed) over the encrypted data without decryption. Alice
provides Bob with an encrypted version of a search word Wj and asks him if and where
Wj occurs in the original text document. If Wj exists at the j-th position of the original
text, then ⟨j, Cj⟩is returned. To do that, Alice has or calculates the following.
Alice sends the value of Xj and kj to Bob, with which Bob is able to compute for
each word Cp (1 ≤p ≤l) in the encrypted text:
Tp = Cp ⊕Xj = ⟨Sp, S󸀠
p⟩;
IF S󸀠
p = Fkj(Sp) THEN return ⟨p, Cp⟩.
It is possible that multiple positions satisfy the above condition. In this case, Alice
may need to further examine each answer tuple by decryption. The search speed is
linear in the size of the text.
(3) Decryption
Knowing keys k󸀠, k󸀠󸀠, and the seed for S, Alice can decrypt a cipher text word Cp at any
position p in the encrypted text into a clear word Wp.
According to Tables 5.4, 5.5, and 5.6, Alice can encrypt, search, and decrypt textual
words while Bob cannot read anything of the text. The only word-wise information
Bob gets from Alice is Ci in the encryption phase, and Xj and kj in the search phase.
Since Ci and Xj are both encrypted with a key only known to Alice and kj is only used to
hash one particular random value, Bob does not learn anything of the plain text. The
only information Bob learns from a search query is the location where an encrypted
word is stored.
Authenticated
:25 PM

5.3 Search Over Encrypted XML Context Information
139
Table 5.4: Song et al.’s word encryption [51].
Notation
Meaning
Wi
plain word of length n
k󸀠󸀠
encryption key
Xi = Ek󸀠󸀠(Wi) = ⟨Li, Ri⟩
encrypted word, and E is a deterministic encryption function E : key ×
{0, 1}n →{0, 1}n, Li has length (n-m), and Ri has length m (where n > 0
and 0 < m ≤n/2)
k󸀠
key for f
ki = fk󸀠(Li)
key for F, and f is a keyed pseudo random function f : key × {0, 1}n–m →key
Si
random value of length (n-m)
Ti = ⟨Si, Fki(Si)⟩
F is a keyed pseudo random function
F : key × {0, 1}n–m →{0, 1}m
Ci = Xi ⊕Ti
encrypted value of Wi (where ⊕is bitwise exclusive or)
Table 5.5: Song et al.’s word search [51].
Notation
Meaning
k󸀠󸀠
encryption key
k󸀠
key for f
Wj
clear search word
Xj = Ek󸀠󸀠(Wj) = ⟨Lj, Rj⟩
hidden encrypted search word
kj = fk󸀠(Lj)
key for F
Table 5.6: Song et al.’s word decryption [51].
Notation
Meaning
p
word position
Cp = ⟨Lp, Rp⟩
encrypted word at the p-th position
Sp
random value
Xp,l = Lp ⊕Sp
left part of encrypted word
kp = fk󸀠(Xp,l)
key for F
Tp = ⟨Sp, Fkp(Sp)⟩
Xp = Cp ⊕Tp
encrypted word
Wp = Dk󸀠󸀠(Xp)
plain word, and D is the decryption function D : key × {0, 1}n →{0, 1}n
such that Dk󸀠󸀠(Ek󸀠󸀠(Wp)) = Wp
5.3.4.2 Integration of XML Tree-Structured Clear Search with Linear
Encrypted Text Search
The above linear keyword search protocol can be extended to deal with tree-structured
search upon encrypted XML data.
Authenticated
:25 PM

140
5 Protecting Context Privacy
a
b
c
d
e
< a >
         < b >
          < /b >
          < c
d=
>
< e / >
          < /c >
< /a >
pre
post
paraent
1
2
1
0
2
1
3
1
4
3
5
3
3
4
5
XML Tree Structure
post
pre
0
ascendants
descendants
following
siblings
previous
siblings
(a)
(b)
Figure 5.6: XML tree’s pre, post, and parent ﬁelds and their relationships.
(1) Tree-Structured Search on Clear XML Data
Grust et al. introduced a way to store clear XML data in a relational database such
that search queries can be handled efﬁciently [27, 28]. They translate an XML docu-
ment into a relational table with a pre-deﬁned structure. Each table record consists
of the name of the tag or attribute and its corresponding value. In addition, the tree-
structured information underlying the original XML document is also captured and
stored in the pre, post, and parent ﬁelds, where the pre and post values count the num-
bers of open tags and close tags, respectively, and the parent value is the pre value of
the parent element. Figure 5.6(a) illustrates the calculation of the three ﬁelds. All the
ﬁelds can be computed in a single pass over the XML document.
The XPath axes like ascendant, descendant, child, and sibling, etc. can be reﬂected
from the values of pre, post, and parent ﬁelds, as illustrated in Figure 5.6(b).
v is a parent of v󸀠↔(v.parent = v󸀠.pre);
v is a descendant of v󸀠↔(v󸀠.pre < v.pre) ∧(v󸀠.post > v.post);
v is a following sibling of v󸀠↔(v󸀠.pre < v.pre) ∧(v󸀠.post < v.post).
(2) Tree-Structured Search on Encrypted XML Data
Grust et al. stored XML data in the clear. To protect sensitive XML data cryptographic-
ally, and meanwhile enabling direct tree search over encrypted XML data, extensions
of Song et al.’s linear search protocol over encrypted text data can be made.
First, the input ﬁle is not an unstructured text ﬁle, but a tree-structured XML docu-
ment. The division of the data into ﬁxed sized words does not seem natural. Therefore,
variable block lengths that depend on the lengths of the tag names, attribute names,
attribute values and the text between tags are adopted.
Second, the sequence number of a block is no longer appropriate to deﬁne the
location within a document. The pre value can thus be used instead.
For simplicity, only the encryption of tag names is described in the following. Ex-
actly the same scheme is used for attribute names (preﬁxed with sign ) and the data
value itself by simply substituting value for tag.
Authenticated
:25 PM

5.3 Search Over Encrypted XML Context Information
141
Table 5.7: XML tag encryption [51].
Notation
Meaning
Wtag
plain tag text
k󸀠󸀠
encryption key
Xtag = Ek󸀠󸀠(Wtag) = ⟨Ltag, Rtag⟩
encrypted tag text
k󸀠
key for f
ktag = fk󸀠(Ltag)
key for F
Spre
random value
Tpre,tag = ⟨Spre, Fktag(Spre)⟩
tuple used by search
Cpre,tag = Xtag ⊕Tpre,tag
encrypted value of Wtag
Table 5.8: XML tag decryption [51].
Notation
Meaning
k󸀠
key for f
k󸀠󸀠
encryption key
pre
search location
Cpre,tag = ⟨Lpre,tag, Rpre,tag⟩
stored block
Spre
random value
Xtag,l = Lpre,tag ⊕Spre
left part of encrypted block
ktag = fk󸀠(Xtag,l)
key for F
Ttag = ⟨Spre, Fktag(Spre)⟩
check tuple
Xtag = Cpre,tag ⊕Ttag
encrypted block
Wtag = Dk󸀠󸀠(Xtag)
plain text block
–
Encryption. Spre does not depend on the tag name but on the location (expressed
in the pre ﬁeld), because all elements with the same tag name should be stored
differently.
–
Tree Search. An XPath query like //tag1//tag2/[tag3 =“value”/] is encrypted to
//Xtag1, ktag1////⟨Xtag2, ktag2⟩/[⟨Xtag3, ktag3⟩= “⟨Xvalue, kvalue⟩”/] before sending it to
the server. The server computes the result by traversing the XPath query from left
to right.
–
Evaluating the XPath axis //, ////, /[, and /] through the pre, post, and parent
ﬁelds. It is possible to ﬁnd all children (//) or all descendants (////) of elements
found in a previous step by just using the pre, post, and parent ﬁelds.
–
Filtering out the records that do not satisfy S󸀠
p = Fktag(Sp) in Tp,tag = Cp,tag ⊕
Xtag = ⟨Sp, S󸀠
p⟩.
–
Filtering out the records with an incorrect value ﬁeld.
–
Decryption.
Authenticated
:25 PM

142
5 Protecting Context Privacy
5.4 Life-Cycle Management of Context Information
Although encryption can be applied to privacy protection, but as long as the encryp-
tion keys are managed by the system, it cannot prevent trail disclosure when the
system administrator cannot be fully and permanently trusted [9, 56].
Also, controlling the content of context histories cannot be simply delegated to
access control. Indeed, as opposed to traditional sensitive data like health care, ﬁn-
ancial, or insurance information which are subject to privacy protection, context does
not require durability requirement from users’ point of view. That is, context inform-
ation hurting users’ privacy and discarded to anyone will not be useful for its owner.
Retaining this data in the system should be proscribed for the following reasons.
First, some internals (e.g., system or database administrators) often have access
privileges to the whole contents for administration purpose, which constitutes a major
privacy threat while half attacks are actually conducted by insiders according to the
computer crime and security survey [25].
Second, retaining more data increases motivations (and beneﬁts) of privacy at-
tacks conducted, and currently no solution can fully guarantee that access control
mechanisms cannot be bypassed.
Third, protecting data by access control has a strong impact on system perform-
ance (e.g., views complexity in databases). Particularly, the amount of consistently
sensed context information in context-aware computing is huge.
For these reasons, controlling the content of context histories cannot simply be
delegated to disclosure mechanisms, but is a problem in itself. In combination with
security and access control mechanisms, aiming to limit the disclosure of privacy-
sensitive context information to third parties, self-regulating the content of context
histories itself can be explored. This following subsections do not address access con-
trol, and consider it as an orthogonal problem aiming at properly regulating disclosure
of the content of histories to third parties.
5.4.1 Requirements for Self-Regulation of Context Information
Providing automatic control over context histories’ life-cycle is a major challenge,
notably due to the following context-awareness properties.
Smartness property. The smartness requirement of context-awareness imposes
to make available any context information which is deﬁned as non-private by its
owner. This mainly leads to devise-rich and customized (e.g., per user) regulation of
context histories, precluding uniform or coarse grain processes.
Usefulness property. Context information might always be useful to make
autonomous decisions, which makes it difﬁcult to calibrate lifetime at the system
level. This is as opposed to traditional data (e.g., credit card numbers or a mail
address), which might be deleted after use.
Authenticated
:25 PM

5.4 Life-Cycle Management of Context Information
143
Distribution property. Context-aware ambient intelligent spaces are composed
of many devices, including sensors, network components like routers, databases, and
user devices. Many of those might constitute context histories about surrounding in-
dividuals, thus any of them should be able to regulate the accumulated content given
privacy policies.
Performance property. Performance is highly demanded at data management
level to process real-time sensed/acquired huge amount of data, store large con-
text histories, and answer complex queries, required to take autonomous decisions.
Performance issues of such a database have been studied in Ref. [14], inducing par-
ticular data management techniques, which of course proscribes too heavy privacy
techniques (e.g., encryption intensive operations).
5.4.2 Limitations of Traditional Privacy Preservation Strategies
Existing privacy preserving solutions do not ﬁt the above main requirements.
5.4.2.1 Problems with Access Control Mechanisms
Many privacy studies in database systems focus on information disclosure. However,
privacy cannot exclusively be achieved by controlling accesses. Laws [55] and research
studies [2, 31, 35, 38] stress the importance of limiting collection and retention of
information to achieve privacy. This is accentuated while dealing with smartness in
context-aware systems and applications, given the lower interest of storing context in-
formation durably from a user’s perspective (as opposed to ﬁnancial or health folders).
Moreover, establishing a fair balance between information usefulness and privacy is
regarded highly important [2, 32, 38]. Although limiting data disclosure and providing
multi-leveled views [8, 11] can meet the smartness and performance of context-aware
computing, it cannot be substituted to life-cycle management.
5.4.2.2 Problems with Purposes-based Mechanisms
Purpose-based
techniques
constitute
another
major
attempt
toward
privacy
protection.
(1) The Platform for Privacy Preferences (P3P) [58, 59] applies notice and con-
sent practice to web browsing. Websites describe their privacy policies (i.e., acquired
data, retention period, and usage) in machine-readable XML, and browsers are
parameterized to accept or reject policies if hurting user’s privacy settings.
(2) Langheinrich [38] implemented privacy principles derived from laws in ubi-
quitous environments using P3P and trusted devices.
(3) Hippocratic database [2] was proposed following the principles derived from
laws. It binds the responsibility for data privacy to the database, compliantly with P3P
Authenticated
:25 PM

144
5 Protecting Context Privacy
policy expression. Data is stored with the purpose for which it has been acquired and
privacy parameters are derived from this purpose.
It is obvious that in many situations purposes can induce privacy parameters,
deﬁning accuracy and amount of acquired data, retention period, and access priv-
ilege. When users consent to a purpose, they consent to the associated list of privacy
parameters. While this clearly shows the importance of controlling contents, such
approaches hurt the smartness and usefulness properties.
First, the enforced privacy protection is good for atomic purposes (i.e., completely
achieved or not at all). Privacy parameters are set to a lower bound in terms of privacy
violation such that purpose can be achieved. However, while dealing with purposes
which exhibit non-atomic nature (i.e., realizable partially), such purpose-based tech-
niques are inadequate. For example, the purpose “perform smart web search” can be
achieved partially, whose efﬁciency highly relies on accuracy and amount of gran-
ted users’ context histories (e.g., current readings, accessed web sites, etc.). Note
that smartness-related purposes in context-aware computing are typically nonatomic,
making privacy parameters difﬁcult to ﬁgure out by service providers, and requiring
rich user-based expressions such that information considered as nonsensible by the
user might be granted to the system.
Second, the notice and consent practice leads to the application of the server-
side privacy policy. This is even more difﬁcult to set properly, given the usefulness
property. It may lead to an important amount of nonprivate context information being
discarded. For example, context regarding users holding weaker privacy wishes would
be governed by stricter parameters deﬁned at the server and users holding stricter
policies would simply not be monitored.
Third, content life-cycle is regulated by deleting expired data after a given period
or when the associated purpose is achieved. This coarse grain attempt toward data
degradation is justiﬁed for data becoming completely useless after usage (e.g., credit
card number), but does not suit context-aware computing environments.
5.4.2.3 Problems with Data Degradation Mechanisms
Data downgrading has been further investigated to provide privacy, in balance with
data usefulness, in data mining [57] and statistical databases [1]. Interesting tech-
niques have been proposed, based on randomization [4] or data suppression and
generalization [46, 47, 53, 54] to obtain k-anonym datasets. However, the downgrad-
ing process is uniform, where identical rules apply to the complete dataset. Besides, it
performs only once to generate a public release, which hurts the smartness property.
In addition, techniques are often calibrated for given mining algorithms or statistics
computations, and require the whole original dataset [46, 47] to produce the pub-
lic release, which strongly impacts the feasibility of a transposition to context-aware
environments.
Authenticated
:25 PM

5.4 Life-Cycle Management of Context Information
145
5.4.3 A Life-Cycle Policy (LCP) Model
The design of the context life-cycle policy (LCP) is driven by the following key con-
siderations. To comply with the distribution property, the LCP must be self-contained,
i.e., containing the whole required knowledge to be interpreted and simple to apply.
To comply with the smartness property, LCP must be rich enough to avoid useless
information loss (regarding privacy). In addition, the usefulness property claims for
user-deﬁned LCP. Finally, LCP should be easily transmissible to third parties to enable
controlled data replication.
5.4.3.1 Context State
We model context information sensed in a context-aware environment as a triplet (ID,
Time, Value), consisting of the ID of the donor, the Time when this context was ac-
quired, and the context concrete Value itself (e.g., location coordinates). Such a triplet
is called a context state. Each element of the triplet can have different levels of ac-
curacies based on domain generalization techniques [39]. For sake of simplicity, we
consider that those different accuracies can be classiﬁed, given the privacy of the
information they represent, as illustrated in Table 5.9.
Note that although generalization domains might be a graph (non-linear), any
element of the graph can be ranked regarding the privacy concern of information
attached to it, depending on speciﬁc application scenarios.
The different combinations of accuracies in Table 5.9 thus form a cube. Each co-
ordinate in the cube corresponds to a context state at a certain accuracy level. For
example, the triplet (3, 3, 2) designates a context state, stating the donor’s identiﬁer as
a department number, the time in hour, and the location as a room number.
A binary partial order relationship ≥a can be further deﬁned in the cube to com-
pare the accuracy levels of two context states (si ≥a sj), if and only if each of the three
dimensions of si has a higher or equal accuracy level than that of sj.
Note that the cubic representation of the data is already used, e.g., in data ware-
house to represent the complex result of a given query. The main difference with the
representation here is that each dimension takes different data accuracies linked to a
given domain of values (more or less generalized) or to a given data type (more or less
Table 5.9: Different levels of context accuracies.
Context Element
Level-1
Level-2
Level-3
Level-4
Level-5
Donor’s ID
employee
group
department
university
Acquisition Time
second
minute
hour
day
month
Value (e.g., Location)
coordinate
room
ﬂoor
building
region
Authenticated
:25 PM

146
5 Protecting Context Privacy
precise), ordered from the more accurate (e.g., exact coordinates for location) to the
less accurate (e.g., building), instead representing an ordered set of discrete values
(e.g., years) or intervals (e.g., age between 0-10) having the same accuracy.
5.4.3.2 Context State Degradation
Triggered by events, the accuracy level of a context state can be consistently down
degraded, forming the life cycle of a certain type of context information. According to
the happening places, three kinds of events are categorized.
–
Universal events, which can be generated and thus be available in any ambient
intelligent environment (e.g., a time predicate);
–
Internal events, which originate from a speciﬁc ambient intelligent environment,
and are usually monitored by sensors or related to a component like a database
access or a printer error;
–
External events, which are monitored in another ambient intelligent environ-
ment, thus originating externally.
Deﬁnition 20. The life-cycle of a context state for a certain type of context can be
deﬁned as a deterministic ﬁnite automata (S, G, $, s0, sf), where
–
S is a set of context states in the form of a triplet (ID, Time, Value);
–
G is a set of events;
–
$ is a set of transition functions S×G →S, satisfying that for a transition $(si, ek) =
si+1 (si ≥a si+1);
–
s0 ∈S is the context starting state; and
–
sf ∈S is the context ﬁnal state, which can be empty value 6, corresponding to the
context state deletion.
◻
Example 28. Given an automata:
S = s0, s1, s2, s3, s4, sf = {(1, 1, 2), (1, 1, 3), (1, 4, 3), (1, 3, 3), (2, 4, 3), 6},
G = {e1, e2, e3, e4} = {10 minutes later, leave through back door before 5pm, 8 hours
later, 1 week later, 1 month later},
$(s0, e1) = s1, $(s1, e2) = s2, $(s1, e3) = s3, $(s2, e4) = s4, $(s3, e4) = s4, $(s4, e5) = sf,
s0 = (1, 1, 2), sf = 6.
It describes an LCP example to preserve the privacy in case one decides to leave
the ofﬁce building earlier. Its pictorial representation is shown in Figure 5.7.
The ﬁnal state of context data Location will store ID as the working group of the
donor, Time as a day, and Value as the building identiﬁer, which will be retained
durably in the context database, and considered as nonprivate by the donor.
◻
As a result, the LCP-based context degradation policy can be viewed as a path in a
cube using the automata model.
Authenticated
:25 PM

5.4 Life-Cycle Management of Context Information
147
10 minutes
later
1,1,2
1,1,3
1,3,3
ID
group
Time
day
Loc.
floor
Loc.
floor
ID
employee identifier
Time of acquisition
seconds
Location
room identifier
8 hours
later
1,4,3
1 week
later
Backdoor before 5pm
1 week
later
Time
day
Time
hour
1 month
later
2,4,3
Figure 5.7: An LCP automaton example.
5.4.3.3 The One-Way Property
The correctness of the LCP model in providing privacy lies in its one-way property
guarantee. That is, from an already degraded context value, the system (even the DBA
of the Context-DB) is not able to derive previous accurate values. However, this one-
way property could potentially be violated, causing users or organizations’ privacy to
be violated.
In particular, compliance with this property induces constraints on the automata
itself, impacts the logging process, and requires some particular techniques in the
particular case of Time and ID degradation.
5.4.4 Use Scenarios of LCPs
5.4.4.1 Organization-Oriented LCPs
Consider LCPs deﬁned by an organization (e.g., a company or a country) to preserve
its own privacy, preventing useless context retention that could be subject to attacks
from its competitive organizations. To achieve its privacy goal, an organization has to
minimize the available (retained) data within its own information system in order to
avoid potential spying. Using LCPs, an organization can parameterize its own inform-
ation system to only retain context information which is strictly required in providing
the services to improve its efﬁciency.
For example, a company could require
1)
phone call redirection,
2)
automatic ﬁlling-in daily timetable forms,
3)
room availability forecasting for the next week, and
4)
statistics in terms of visibility of different teams (i.e., number of days per week a
team is represented by one of its members in the organization), and so on.
Authenticated
:25 PM

148
5 Protecting Context Privacy
Few minutes
later
Accurate
ID,
Hour,
Room
Team,
Hour,
Room
1 day
later
Team,
Day,
1 week
later
Figure 5.8: An organization-oriented LCP example.
To provide these services with privacy in mind, the LCP pictured in Figure 5.8 could
regulate employees’ location information acquired by the context-aware ambient
intelligent space.
Following this LCP, the context-aware system retains
(1) accurate location states (employee ID, precise acquisition Time, and Room
identiﬁer) for a few minutes (allowing phone call redirection),
(2) then the exact date of acquisition is degraded to Hour of acquisition (enough
to allow automatic ﬁll-in of daily timetables),
(3) one day later, employee’s ID is degraded to her Team identiﬁer (enabling room
availability forecast for the next week), and ﬁnally,
(4) one week later, identiﬁer of Room is deleted (still allowing day-per-week visib-
ility of the team at work). In this particular case, the last context state is considered as
nondangerous for the organization’s privacy, and can thus be retained durably in the
system to enable further statistic computations and long-term historical analysis.
Although this LCP is shared by all the employees of the company, it reduces
the amount of context information available in the context-aware ambient intelli-
gent space, which could be accessible in case of a spying attack conducted by a
competitive.
5.4.4.2 User-Oriented LCPs
Besides organization-oriented LCPs, user-leveled service acceptance based on the
well-known notice and consent strategy (as promoted in worldwide laws and direct-
ives) leads to deﬁning individual LCP per user. Indeed, the list of services a given user
consents to determines the LCP regulating his/her context information. To a certain
extent, this LCP might serve as a quasi identiﬁer of this user. It gives a fuzzy joining
key to gather accurate and degraded context values belonging to this user when LCPs
linked to data are applied.
In the spirit of notice and consent, LCPs are set by default to prevent from stor-
ing any information about the donors. For example, in a road system continuously
monitoring location of cars, context states would be dropped immediately when ac-
quired or received. Services available in the context-aware environment notify users
of the context information they need for offering a given beneﬁt. Driven by the service
requirements, they ask users to consent adding intermediate context states to their
current LCPs. Users, balancing the loss of privacy and the beneﬁts offered by the ser-
vice, would possibly consent to additional intermediate states. On this compromise
Authenticated
:25 PM

5.4 Life-Cycle Management of Context Information
149
Few minutes
later
Accurate
ID,
Minute,
Road
Type,
Hour,
Road
1 hour
later
,
Hour,
Road
1 week
later
1 month
later
Figure 5.9: A user-oriented LCP example.
basis, services can progressively ask for more data in exchange of additional beneﬁts,
which might lead to a rich and highly personalized LCP. Note that each user might
consent to a speciﬁc pool of services.
For example, in a car environment, some users could consent to an LCP, as shown
in Figure 5.9. Such an LCP leads to
(1) storing the accurate context state for a few minutes to estimate car speed and
trafﬁc overload which are necessary for advising personalized (car ID is stored) driving
direction efﬁciently, e.g., avoiding trafﬁc jams, localizing colleagues, etc.,
(2) degrading the accurate location to Road and storing it for one hour to enable
personalized forecasts (one hour history is stored), e.g., to advise user’s colleagues on
car pooling possibilities. Here data is retained for one hour because we assume that
people susceptible to make their cars available for car pooling wait for maximum one
hour to ﬁll their cars before moving on,
(3) degrading car ID to car Type and keeping it for one week to enable per type
car forecasts (one week history), e.g., providing general car pooling service (e.g.,
estimating the best places to be given a lift) for the next week,
(4) deleting the ID ﬁeld from the context state triplet to enable general forecasts
for the next month, e.g., enabling to plan road directions and avoid trafﬁc jams in
advance, and ﬁnally
(5) removing the whole context state triplet.
5.4.4.3 Customized LCPs
Both organization- and user-oriented LCPs can be reﬁned by their owners to reﬂect
special privacy wishes. In particular, a certain owner can complement her LCP with
his/her individual personal requirements with extra context states and transitions.
To illustrate the shape of such an LCP, let’s consider the example presented in
Example 28. This LCP can be considered as containing an initial subpart either issued
by an organization or in a user-oriented fashion, plus a user-speciﬁc subpart coping
with particular preoccupations, as shown in Figure 5.10. In this case, the third context
state of the initial LCP subpart, referred to as (Group, Hour, Floor), will not be reached
when the owner of the LCP leaves the building before 5pm. Context information will
however be degraded to the additional user’s speciﬁc state referred to as (Group, Day,
Floor). Such a customization of the LCP would enable the owner to keep private the
hour when s/he left her work, in case s/he left earlier than expected.
Two interesting remarks can be made regarding such a policy.
Authenticated
:25 PM

150
5 Protecting Context Privacy
10 minutes
later
Accurate
ID,
Minute,
Room
Group,
Hour,
Floor
8 hours
later
1 week
later
Group,
Day,
Floor
Backdoor
before 5pm
1 week
later
Owner specific
LCP subpart
Initial
LCP subpart
1 month
later
Dept.,
Day,
Building
Figure 5.10: A user-oriented LCP example.
First, while the system knows during one week that the user left incognito by the back-
door (the tuple belongs to a different context state), the exact hour when she left is
effectively hidden.
Second, the interest of the user to deﬁne the additional state (Group, Day, Floor)
might be discussed compared to simply forcing to skip not enough degraded interme-
diate state(s). Indeed, this state offers information that does not exactly correspond
to those required by services based on (Group, Hour, Floor). It may be useless for any
application. In that case, direct degradation to the next degraded state (Dept, Day,
Building) fulﬁlling the particular user’s privacy requirement might be more appro-
priate. Here, this user-deﬁned intermediate state is considered to maximize context
information accuracy in the spirit of the usefulness requirement.
5.4.5 LCP-based Context Privacy Protection Diagram
To reconcile privacy and smartness in context-aware computing, With the LCP model,
progressive degradation of context histories can be speciﬁed, and context-aware sys-
tems in charge only retain the desired souvenir. Such an LCP can be viewed as a
program bound to the context data it regulates, so that any computing component
like user devices, databases, and routers can compile it and operate the degradation.
To conform to the distribution nature of context-aware ambient intelligent spaces,
as the diagram in Figure 5.11 shown, a policy binder associates sensed context with
corresponding user-deﬁned LCP, regulating its life cycle. This policy binder is placed
close to or even within the acquisition devices. A policy compiler situated at the con-
text database side translates the LCP into a language, which is understandable at the
component (e.g., SQL for a relational database). Then the component will be in charge
Context Acquisition
Devices
Context DB
LCPs
User
Context
LPC Binder
LPC Compiler
Context-Aware
Applications
Figure 5.11: Life-cycle-policy-based context privacy protection.
Authenticated
:25 PM

5.4 Life-Cycle Management of Context Information
151
(possibly with the help of the translator) of managing properly the context life cycle
following the LCP.
5.4.6 Challenges with LCP-Based Privacy Protection
While the LCP approach tackles the important privacy problem in context-aware
computing, the LCP model itself also leads to a number of further research issues and
open problems.
5.4.6.1 Multi-Usage
The presented LCP is based on a linear generalization model for a context cube. How-
ever, since generalization/speciﬁcation hierarchies are often application dependent,
the generalization trees might have several branches, potentially one required per ap-
plication. The LCP model needs to be adapted to rich generalization schemas so as to
cover a broad range of applications. One potential way to resolve this issue could be
based on duplicating information, which must follow different generalization paths
for different applications. Also other degradation techniques might be envisioned
(e.g., progressively deleting bits) for speciﬁc attributes (e.g., IP addresses collected
by network access points) in the context-aware soft meeting planner application.
5.4.6.2 Trustworthiness
The LCP model offers a resistance to a posteriori (i.e., after LCP appliance) snoop-
ing attacks, which constitute a major threat in context-aware environments. Note that
a posteriori privacy violation can be performed against individuals by malicious or-
ganisms before credit or insurance acceptance, or against companies before ﬁnancial
operations, insider trading, etc. For the irreversible degradation, such a protection can
be achieved by only applying the LCP model once against context data.
However, it is worth mentioning that only privacy-enabled systems instead of
privacy-enforced systems are addressed here. That is, without any additional security
feature, trust in the underlying data management system that really degrades the data
is needed. In particular, attacks can also be conducted by altering (even randomly)
data involved in LCPs, replacing some LCPs with other valid (and weaker) LCPs or by
a previously deﬁned (weaker) LCP deﬁned by the same user. How to enforce proper
degradation process, e.g., by means of cryptography or secure hardware, is a very in-
teresting and difﬁcult open issue. Another perhaps more tractable problem would be
to detect misuse at the Context-DB level, based on database audit techniques (like
monitoring database events, incoming queries, and produced results). Indeed, in the
above settings, nothing can prevent a malicious third party holding sufﬁcient access
privileges to continuously query the fresh (accurate) subsets of the context database
in order to constitute the complete accurate history of the context-aware ambient
intelligent space.
Authenticated
:25 PM

152
5 Protecting Context Privacy
5.4.6.3 Distribution
Context-aware ambient intelligent spaces incorporate many components. Moreover,
several contiguous spaces might coexist. Such a distribution nature introduces many
challenging issues. One of them is related to event detection (for triggering context
degradation in the LCP model). Although some local events (monitored within the
internal AmI space) might be broadcast to any internal recipient responsible for man-
aging context data associated with the LCP, it is difﬁcult to assume that external events
(monitored in another context-aware space) will be. In fact, events themselves consti-
tute parts of context information, and as such may be regulated by LCPs leading to
their own progressive degradation (which can occur rapidly). This will deﬁnitely pre-
vent intensive external broadcast. Besides, through events, some information which
should not be disclosed to external ambient intelligent spaces might be revealed. In
addition to event detection, distribution also incurs some other difﬁcult issues like
management of knowledge of LCP and knowledge of the appropriate generalization
hierarchy for any surrounding (visitor) user, etc.
5.4.6.4 Querying Multi-Accuracy Data
The LCP model leads to the management of multi-accuracy information leading to fur-
ther interesting research issues, including query language and processing techniques
to cope with this multi-accuracy data. Notably, a query language is necessary to en-
able applications to express queries involving different granularities of data provided
by the context cubes, e.g., in an SQL-like language, SELECT Id [department] FROM
Location_cube WHERE Value[room] = “thisRoom”.
Also, accelerating computation techniques on data samples could beneﬁt from
the different accuracy levels to present to applications results with more or less
precision given the current load of the database system (assuming less accurate res-
ults, based on more degraded data, are faster to compute). More generally, query
processing and index techniques should be devised to tackle special properties of
multi-accurate data in insertion-intensive database systems.
5.4.6.5 One-Way Data Degradation vs. Dynamic Computation
While one-way (irreversible) data degradation is prompted here, it would also be inter-
esting to investigate some other degradation schemes based on dynamic computation
of current context states. For instance, one might imagine going backward along a
generalization hierarchy. This could be studied by adopting a multi-accuracy access
control strategy. Indeed, while traditional systems propose access control policies
based on selection and projection, this approach is not ﬂexible enough to fully sat-
isfy donors’ particular wishes when applied to pervasive environments, where control
should be given at different data accuracy, or offer micro-views as shown in Ref. [11],
and should also be able to evolve with the time. In the spirit of the LCP model, the
Authenticated
:25 PM

Literature
153
provided accuracy may either degrade or upgrade under certain circumstances, e.g.,
current location can be considered as private and become accessible one week later.
5.4.6.6 Static vs. Dynamic Life-Cycle Models
Addressing the limitation of static models would lead to delegate further intelligence
to context-aware components (e.g., database), to enable to compute dynamically (i.e.,
at runtime) the most appropriate next context state given the current database con-
tent. For example, we could imagine to deﬁne degradation policies by means of
k-anonymous wishes (e.g., each ten minutes, anonymize my context 5 time more, tak-
ing into account that I would prefer degrading in favor of ID, then Time, and then
context accuracy), delegating to the system the determination of the most appropriate
degraded state, given the current database content.
5.5 Recapitulation
Context-aware computing systems continuously monitor surrounding individuals’ be-
havior to make existing applications smarter, i.e., make decision without requiring
user interaction. While this smartness ability is tightly coupled to the quality and
quantity of the available (past and present) information, context linked to surround-
ing individuals falls under privacy directives. This chapter presented two techniques
to deal with this paradox between smart context-awareness and privacy in context-
aware computing environments. The ﬁrst is on encrypted context information search-
ing without decryption, so that the deployed encryption technique should on the
one hand satisfy the privacy protection requirements on context information, while
at the same time allow efﬁcient manipulation of context information without loss
of conﬁdentiality. The second is on life-cycle management of privacy-sensitive con-
text information, where life-cycle policies are bound to context data to regulate the
progressive degradation of the accuracy of context information.
Literature
[1]
N. R. Adam and J. C. Worthmann. Security-control methods for statistical databases: A
comparative study. ACM Computing Surveys, 21(4), 1989.
[2]
R. Agrawal, J. Kiernan, R. Srikant, and Y. Xu. Hippocratic databases. In Proc. of VLDB, 2002.
[3]
R. Agrawal, J. Kiernan, R. Srikant, and Y. Xu. Implementing P3P using database technology. In
Proc. of ICDE, pages 595–606, 2003.
[4]
R. Agrawal and R. Srikant. Privacy-preserving data mining. In Proc. of ACM SIGMOD, 2000.
[5]
M. Altinel and M. Franklin. Efﬁcient ﬁltering of XML documents for selective dissemination of
information. In Proc. of VLDB, pages 53–64, 2000.
[6]
R. Anderson. Security Engineering: A Guide to Building Dependable Distributed Systems, John
Wiley, Indianapolis, Indiana, 2008.
Authenticated
:25 PM

154
5 Protecting Context Privacy
[7]
P. Ashley, S. Hada, G. Karjoth, and M. Schunter. E-P3P privacy policies and privacy
authorization. In Proc. of the ACM workshop on Privacy in the Electronic Society, pages
103–109, 2002.
[8]
E. Bertino, J. W. Byun, and N. Li. Privacy-preserving database systems. In Proc. of FOSAD, pages
178–206, 2005.
[9]
L. Bouganim and P. Pucheral. Chip-secured data access: Conﬁdential data on untrusted servers.
In Proc. of VLDB, 2002.
[10]
R. Brinkman, L. Feng, J. M. Doumen, P. Hartel, and W. Jonker. Efﬁcient tree search in encrypted
data. Intl. J. of Information Systems Security, 13(3):14–21, 2004.
[11]
J. W. Byun and E. Bertino. Micro-views, or on how to protect privacy while enhancing data
usability: Concepts and challenges. SIGMOD Record, 35(1):9–13, 2006.
[12]
J. W. Byun, E. Bertino, and N. Li. Purpose-based access control of complex data for privacy
protection. In Proc. of the 10th ACM symposium on Access Control Models and Technologies,
pages 102–110, 2005.
[13]
C. Chan, P. Felber, M. Carofalakis, and R. Rastogi. Efﬁcient ﬁltering of XML documents with
XPath expressions. In Proc. of ICDE, 2002.
[14]
S. Chandrasekaran and M. J. Franklin. Remembrance of streams past: Overload-sensitive
management of archived streams. In Proc. of VLDB, pages 53–64, 2004.
[15]
S. Chaudhuri, T. Dutta, and S. Sudarshan. Fine grained authorization through predicated
grants. In Proc. of ICDE, 2007.
[16]
S. Chaudhuri, R. Kaushik, and R. Ramamurthy. Database access control & privacy: Is there a
common ground? In Proc. of CIDR, 2011.
[17]
S. Chawathe, V. Krishnamurthyy, S. Ramachandrany, and S. Sarma. Managing RFID data. In
Proc. of VLDB, 2004.
[18]
L. Cranor. P3P: Making privacy policies more useful. IEEE Security & Privacy Magazine,
1(6):50–55, 2003.
[19]
Dept. of Justice. Privacy act of 1974. http://www.justice.gov/opcl/ privacyact1974. htm, 2012.
[20] C. Doom. Get smart: How intelligent technology will enhance our world. Computer Sciences
Corporation (CSC), U.S., 2001.
[21]
European Commission. Regulation of the European parliament and of the council on the
protection of individuals with regard to the processing of personal data and on the free
movement of such data (general data protection regulation).
http://ec.europa.eu/justice/dataprotection/ document/review2012/com_2012_11_en.pdf,
2012.
[22] L. Feng and W. Jonker. Preparations for encrypted xml metadata querying. International Journal
of Computer Systems, Science & Engineering, 19(3/4):95–103, 2004.
[23] D. Ferraiolo and D. Kuhn. Role-based access control. In Proc. of Intl. Conf. on Computer Security,
1992.
[24] O. Goldreich. Foundations of Cryptography: Volume 2 (Basic Applications), Cambridge
University Press, Cambridge, UK, 2004.
[25]
L. A. Gordon, M. P. Loeb, W. Lucyshyn, and R. Richardson. CSI/FBI computer crime and security
survey. http://i.cmpnet.com/gocsi/db_area/ pdfs/fbi/FBI2005.pdf, 2005.
[26] R. Grimm and A. Rossnagel. Can P3P help to protect privacy worldwide? In Proc. of the ACM
workshop on Multimedia, pages 157–160, 2000.
[27]
T. Grust. Accelerating Xpath location steps. In Proc. of ACM SIGMOD, pages 109–120, 2002.
[28] T. Grust, M. van Keulen, and J. Teubner. Staircase join: Teach a relational DBMS to watch its
(axis) steps. In Proc. of VLDB, pages 53–64, 2003.
[29] H. Hacigumu¸s, B. Lyer, C. Li, and S. Mehrotra. Executing SQL over encrypted data in the
database-service-provider model. In Proc. of ACM SIGMOD, pages 216–227, 2002.
Authenticated
:25 PM

Literature
155
[30] H. Hacigumu¸s, B. Lyer, and S. Mehrotra. Providing database as a service. In Proc. of ICDE, 2002.
[31]
J. I. Hong and J. A. Landay. An architecture for privacy-sensitive ubiquitous computing. In Proc.
of MobiSys, 2004.
[32] G. Iachello and G D. Abowd. Privacy and proportionality: Adapting legal evaluation techniques
to inform design in ubiquitous computing. In Proc. of CHI, 2005.
[33] W. Jonker. XML and secure data management in an ambient world. Computer Systems Science &
Engineering, 18(5), 2003.
[34] G. Kabra, R. Ramamurthy, and S. Sudarshan. Redundancy and information leakage in
ﬁne-grained access control. In Proc. of ACM SIGMOD, 2006.
[35]
S. Lahlou, M. Langheinrich, and C. Rucker. Privacy and trust issues with invisible computers.
Communications of the ACM, 48(3):59–60, 2005.
[36] B. W. Lampson. Protection. SIGOPS Operating Systems Review, 8(1):18–24, 1974.
[37]
M. Langheinrich. Privacy by design – principles of privacy-aware ubiquitous systems. In Proc. of
Ubicomp, 2001.
[38] M. Langheinrich. A privacy awareness system for ubiquitous computing environments. In Proc.
of UbiComp, 2002.
[39] S. Lederer, J. Mankoff, and A. K. Dey. Who wants to know what when? Privacy preference
determinants in ubiquitous computing. In Proc. of CHI, 2003.
[40] OECD. The international legal framework for data protection and its transportation to
developing transitional countries. http://www.internetpolicy. net/privacy/20041228privacy.
pdf, 2004.
[41]
R. J. Orr and G. D. Abowd. The smart ﬂoor: A mechanism for natural user identiﬁcation and
tracking. In Proc. of CHI, 2000.
[42] P3P Toolbox. What is P3P and how does it work? http://www. p3ptoolbox.org/guide/
section2.shtml, 2005.
[43] E. R. Post, M. Orth, P. R. Russo, and G. N. Ebroidery. E-broidery: Design and fabrication of
textile-based computing. IBM Systems Journal, 39(3&4):840–859, 2000.
[44] O. Potonniee. A decentralized privacy-enabling TV personalization framework. In Proc. of
European Conf. on Interactive Television, 2004.
[45]
I. Roy, S. Setty, A. Kilzer, V. Shmatikov, and E. Witchel. Airavat: Security and privacy for
MapReduce. In Proc. of NSDI, 2010.
[46] P. Samarati and L. Sweeney. Generalizing data to provide anonymity when disclosing
information. In Proc. of the ACM Symposium on Principles of Database Systems, 1998.
[47] P. Samarati and L. Sweeney. Protecting privacy when disclosing information: k-anonymity and
its enforcement through generalization and suppression. In Proc. of the IEEE Symposium on
Research in Security and Privacy, 1998.
[48] R. S. Sandhu. Handbook of Information Security Management, chapter Relational Database
Access Controls, Auerbach Publications, CRC Press LLC, pages 145–160, 1994.
[49] R. S. Sandhu, E. J. Coyne, H. L. Feinstein, and C. E. Youman. Role-based access control models.
Computer, 29(2):38–47, 1996.
[50] V. Shnayder, B. Chen, K. Lorincz, T. R. F. Fulford-Jones, and M. Welsh. Sensor networks for
medical care. Technical Report TR-08-05, Harvard University, 2005.
[51]
D. Song, D. Wagner, and A. Perrig. Practical techniques for searches on encrypted data. In Proc.
of the IEEE Symposium on Security and Privacy, pages 53–64, 2000.
[52]
M. Spence, C. Driver, and S. Clarke. Sharing context history in mobile, context-aware
trails-based applications. In Proc. of the 1st Intl. Workshop on Exploiting Context Histories in
Smart Environments, 2005.
[53]
L. Sweeney. Achieving k-anonymity privacy protection using generalization and suppression.
Journal of Uncertainty, Fuzziness, and Knowledge-Based Systems, 10(5):571–588, 2002.
Authenticated
:25 PM

156
5 Protecting Context Privacy
[54] L. Sweeney. k-anonymity: A model for protecting privacy. Journal of Uncertainty, Fuzziness, and
Knowledge-based Systems, 10(5):557–570, 2002.
[55]
United Nations. Guidelines on the use of computerized personal ﬁles. Economic and Social
Council, resolution 1990/38, 1990.
[56] H. van Heerde. Privacy-Aware Data Management by Means of Data Degradation – Making Private
Data Less Sensitive Over Time, PhD thesis, CTIT, University of Twente, The Netherlands, 2010.
[57]
V. S. Verykios, E. Bertino, I. N. Fovino, L. P. Provenza, Y. Saygin, and Y. Theodoridis.
State-of-the-art in privacy preserving data mining. SIGMOD Record, 33(1):50–57, 2004.
[58] W3C. A P3P preference exchange language 1.0. http://www.w3.org/TR /P3P-preferences/,
2002.
[59] W3C. Platform for privacy preferences (P3P) project – enabling smarter privacy tools for the
web. http://www.w3.org/P3P/, 2007.
[60] F. Wang and P. Liu. Temporal management of RFID data. In Proc. of VLDB, 2005.
[61]
B. Warneke, M. Last, B. Liebowitz, and K. S. J. Pister. Smart dust: Communicating with a
cubic-millimeter computer. IEEE Computer, 34(1):44–51, 2001.
[62] D. H. Wilson, D. Wyatt, and M. Philipose. Using context history for data collection in the home.
In Proc. of the 1st Intl. Workshop on Exploiting Context Histories in Smart Environments, 2005.
[63] World Wide Web Consortium. XML encryption requirements. http://www. w3.org/TR/xml-
encryption-req, 2002.
[64] World Wide Web Consortium. XML encryption syntax and processing. http://www. w3.org/
TR/xmlenc-core/, 2002.
[65] World Wide Web Consortium. Extensible Markup Language (XML) 1.0. http:// www.w3.org/TR/
REC-xml, 2008.
[66] World Wide Web Consortium. XML path language (XPath) 2.0. http:// www.w3.org/TR/
xpath20/, 2010.
Authenticated
:25 PM

6 Querying Context
Abstract: To supply context-aware applications with a variety of sensed or inferred
context information, a powerful context query mechanism is needed. Depending on
context representation and storage models, this chapter presents and analyzes seven
typical kinds of context query languages and mechanisms (i.e., Structured Query Lan-
guage (SQL) based, eXtensible Markup Language (XML) based, ontology based, event
based, logic based, programming application program interface based, and graphic
interface based). Their supports for context heterogeneity and context reasoning, as
well as applicable users, are compared and analyzed.
6.1 Seven Context Query Mechanisms
Acquired context information is either kept in a context database, which could be a
normal relational, object-oriented, XML, or stream database, or consumed up without
being saved. Context querying is closely related to context representation and storage
models, among which context query languages are pivotal for querying context and
determining the way in which queries are expressed and what information needs to
be obtained [9]. Typical context query languages are based on SQL, XML, ontology,
event, logic, programming APIs, and graphic interfaces. They are evaluated and
compared according to their support to context reasoning, context heterogeneity, and
applicable users.
6.1.1 SQL Based
SQL is the standard database query language, which forms the backbone of most mod-
ern database systems. Many computer users and programmers are familiar with SQL.
It is thus natural to formulate context queries in an SQL-like format with little learn-
ing time required. Such kind of context queries typically views context information as
traditional database tables with auxiliary context-associated attributes like accuracy,
conﬁdence, update time, sample interval, and so on.
6.1.1.1 SQL-Like Querying of Context Databases
McFadden et al. [15] presented a general method called CML (Context Modelling
Language) to describe context information, which can be automatically mapped to
a relational database. The following code snippet in CML declares a proﬁled tem-
poral context type PersonEngagedInAct, which has two attributes personID and
ActivityName.
CREATE PROFILED TEMPORAL FACT TYPE PersonEngagedInAct
DEPENDS (PersonLocatedAt)
DOI 10.1515/9783110556674-006
Authenticated
:25 PM

160
6 Querying Context
( person
personID KEY,
activity
ActivityName)
It is mapped to SQL data deﬁnition language as follows:
CREATE TABLE
DSTC_PAGE_COMM.PersonEngagedInAct
( person
DSTC_PACE_COMM.PersonID,
activity
DSTC_PACE_COMM.ActivityName,
fStartTime TIMESTAMP,
fEndTime
TIMESTAMP,
PRIMARY
KEY(person, fstartTime) )
One characteristic of such a mapping lies in the metadata generated according to the
type of context information. Since PersonEngagedInAct is a temporal context, its
start time fStartTime and end time fEndTime are annotated. As it is difﬁcult to
unify all the metadata that different context information may have, such a mechanism
in CML is useful in describing different kinds of context information which does not
have the same metadata.
Judd and Steenkiste [13] considered four typical types of context entities (i.e.,
device, access point, people, and space), as well as relationships among context en-
tities, and stored them in a relational database directly. It queries Alice’s location in
an SQL-like style as follows:
SELECT
Location
FROM
PersonLocation
WHERE
PersonID = Alice’s UID
REQUIRE
location.updateTime within 2 minutes of
present time;
location.accuracy within 500 meters of
actual location
TIMELIMIT
1 minute
where PersonLocation is a table denoting the relationship between people and
space. PersonID is the ID of a person in the table PersonLocation. A special func-
tion of this query is to explicitly require the query execution time to be less than 1
min. The update time and accuracy requirement of the location data are declared as
well. To improve the performance of query execution, intermediate query results can
be stored and synthesized.
6.1.1.2 SQL-Like Querying of Sensor Readings
Madden and Franklin [14] viewed both static proﬁles and dynamic sensor readings as
normal database tables. It allowed to pose queries directly upon sensors. For example,
a query for the average people’s car speed within period w at the WangFu road can be
phrased as follows:
Authenticated
:25 PM

6.1 Seven Context Query Mechanisms
161
SELECT
avg(speed, w)
FROM
sensorReadings AS s
WHERE
s.segment ∈{“WangFu”}
where w stands for the time window size when calculating the average people’s car
speed.
Two kinds of sensor queries are considered in Ref. [14], namely, push query and
pull query. For push query, results are pushed from the sensors out, and are de-
livered as soon as they become available. The sensor data received are usually put
into an input queue, and then the data pass through a ﬁlter, and are ﬁnally put in the
output queue only when they satisfy certain query conditions. In comparison, pull
query requests data to the system and the system responses the request with results.
When querying sensor data, push queries can be used, because the sensors transmit
data back in a speciﬁed interval continuously. When querying people’s proﬁles, pull
queries can be used since the context information is static.
A fully ﬂedged SQL-like context query example is as follows [14]. It combines
push and pull modes, and queries the trafﬁc incident(s) in the district only when the
average speed on the WangFu road is lower than a certain threshold.
SELECT
avg(s.speed, w), i.description
FROM
incidents AS I, sensorReadings AS s
WHERE
i.time ≥now-timeWindow
GROUP BY
i.description
HAVING
speedThreshold >
( SELECT
avg(s.speed, w)
FROM
sensorReadings AS s
WHERE
i.segment = s.segment AND
s.segment ∈{“WangFu”})
To improve query performance, the framework in Java for operators on remote data
streams (Fjords) had been exploited for multiple query execution.
6.1.2 XML Based
Apart from SQL, XML as the standard for data exchange and electronic communic-
ation is also employed to represent context queries and query results. XML-based
context query mechanism is often associated with speciﬁc XML schema designed
by system designers. Due to the existing tools for XML parsing, XML-based context
queries are ﬂexible enough in schema deﬁnition.
For example, Ref. [3] formulated context queries in an XML format called
CoColanguage. Its underlying model is CoCoGraph, a subclass of Petri nets. One virtue
of the model is that it can easily represent parallel query processing.
CoColanguage deﬁnes two basic concepts: CI_object and CI_factory.
CI_object is a context information object, describing a speciﬁc aspect of a context
Authenticated
:25 PM

162
6 Querying Context
entity, which CI_factory is used to request a CI_object. Correspondingly,
factory_node is the core node of CoCoGraph, representing queries. operator_
node of CoCoGraph can adapt, select, or aggregate data.
innerGraph_node of CoCoGraph capsules a part of the CoCograph, contain-
ing multiple other nodes to form a combined node. For example, a context query
for the ofﬁce of user John can be expressed through the following XML-based
factory_node [3]:
<factory_node id=“getUserLocation” class=“office”>
<entity class=“user”>
<identity class=“name”> John</identity>
</entity>
</factory_node>
The execution of the query will return a CI_object with the requested context
information in any of the allowed scales [3]:
<context_information class=“office”>
<entity class=“user”>
<identity class=“name”> John</identity>
</entity>
<scale class=“office_address”>
<room> 208 </room>
<building> Zilverling </building>
<campus> East Lake </campus>
</scale>
</context_information>
Similarly, Heer et al. [10] leveraged XML to represent both queries and query results.
A context-aware distributed query execution engine called liquid is built to handle
distributed and continuous query processing of context data. An operator tree is
employed to support the query execution logic. Hönle et al. [11] also used XML to
incorporate metadata of context in its query processing mechanism.
6.1.3 Ontology Based
As ontology is good at concept deﬁnition and interoperation, ontology-based lan-
guages are used to formulate context queries [8, 9]. As known, OWL (Ontology Web
Language) is an extension of Resource Description Framework, and is a revision of
DAML+OIL web ontology language.
Gu et al. [7, 8] represented contextual information in ﬁrst-order predicate calculus,
e.g., Posture(John,sleeping) denotes that John’s posture is sleeping.
In response to a query (hasPosture John ?x) for John’s posture, the con-
text query engine will ﬁrst load the context ontology stored in the database, and then
Authenticated
:25 PM

6.1 Seven Context Query Mechanisms
163
ﬁnd out the appropriate context service provider that can provide John’s posture
information. Query results are returned in OWL.
For example, a query result for John’s posture is
<socam:Person rdf:about=“John”>
<socam::hasPosture rdf:resource=
“http://ambient.ddns.com/posture#LIEDOWN”>
</socam:Person>
Both context-based reasoning and user-deﬁned rule-based reasoning can be
incorporated in context querying [7, 8].
An ontology-based reasoning rule example is
(?a rdfs:subClassOf ?b),(?b rdfs:subClassOf ?c)
→(?a rdfs:subClassOf ?c)
A user-deﬁned rule-based reasoning example is
(?user rdf:type Elderly) ∧(?user locatedIn Bedroom) ∧
(?user hasPosture LieDown) ∧(Bedroom doorStatus Closed)
∧(Bedroom lightLevel low) →(?user status Sleeping)
By means of DAML+OIL, Mohania et al. [18] generally expressed a context query as a
5-tuple (O, 4, (, G, 4), where O is a set of used ontologies, 3 is a selection list, ( is a
ﬁltering statement, G is the cardinality, and 4 is temporal constraints.
6.1.4 Event Based
As context can be viewed as context events derived from low-level raw contextual in-
formation, high-level event processing languages are applicable to context querying
and detection.
Originally, event processing language integrates knowledge from the areas of
active databases for complex event speciﬁcation and detection [16, 17, 21]. Typ-
ical event operators are sequence, conjunction (concurrent events), disjunction (a
choice between two events), negation, kleene closure (event happening zero, one,
or many times), periodic (a regularly happening event), and aperiodic (an irregularly
happening event) [1, 4, 6].
In Zstream system [16], the following ﬁve operators are adopted for event detec-
tion from data streams. They are Sequence(E1;E2), Negation(¬E), Conjunc-
tion(E1&E2), Disjunction(E1|E2), Kleene Closure(E∗/ E+ /En), where
E,E1,E2 are events.
The ability to synthesize events based upon the ordering of previous events
is a basic and powerful event language construct. While SEQ(E1;E2) repres-
ents event of type E1 occurring before event of type E2, Rahman and Bhalla
[21] used TSEQ(E1;E2,4l, 4u) to represent event of type E1 occurring before
that of E2 and the temporal distance between the two events is bounded by
[4l, 4u]. SEQ+(E) represents the event of type E occurring more than one time.
Authenticated
:25 PM

164
6 Querying Context
TSEQ+(E,4l, 4u) represents the event of type E occurring not less than one time,
where the distance between every two event occurrences is bounded by [4l, 4u].
Within(E,4) represents event of type E lasts no more than 4 period. Inter-
val(E1,E2) returns the interval between two events of type E1 and E2, which
is equal to max{t_end(E2),t_end(E1)} - min{t_begin(E2),t_begin(E1)},
where t_begin(E) and t_end(E) denote the beginning time and ending time
of event of type E [21]. Apart from these, some unary temporal operators such as
Within,Last,Periodic, as well as binary temporal operators such as Con-
cur,Sequence,Overlap, During, described in Chapter 2, are also applicable.
In the complex event processing system (SASE) [22], SEQ(A1,A2,. . .,An)
indicates
A1,A2,
. . . ,
An
must
occur
in
a
ﬁxed
sequential
order.
SEQ_WITHOUT(S1,{B},S2) speciﬁes that no event of the B type can appear
between the two event sequences S1 and S2. Select operator 3 can be used within
the SEQ and SEQ_WITHOUT operators to further constrain the events’ occurrence.
3(SEQ(A1,A2,. . .,An), P) and 3(SEQ_WITHOUT(S1,{B},S2),P) where P is
a set of predicates connected using ∧and ∨. A WITHIN operator is also introduced
for SEQ and SEQ_WITHOUT, WITHIN(SEQ(A1, . . . ,An), T), denoting a time
window T where events happen. SASE’s Any(A1,A2, . . . ,An) operator evaluates to
true if any of the events of event types A1,A2,. . .,An occurs.
Event Correlation and Pattern Detection (CEDR) [2, 12] provided a few high-level
event operators with intuitive and well-deﬁned semantics. It bounds the sequence
operator with w. For example, SEQUENCE(E1,E2,w) outputs a sequence event
at the occurrence of an E2 event, if there has been an E1 event occurrence in
the last w time units. Almost all its operators, including atleast(n, E1,. . .,Ek,
w), atmost(n, E1,. . .,En, w), all(E1,. . .,Ek, w), any(E1,. . ., Ek), un-
less(E1,E2,[n],w), and not(E, sequence(E1,. . .,Ek, w)), have a time-
based scope w. unless(E1,E2,[n],w) speciﬁes that the start valid time of the
negation scope for E2 is the nth contributor to the E1 event, where n is optional. Op-
erator not(E, sequence(E1,. . .,Ek,w)) indicates that event E does not occur in
the sequence E1,. . .,Ek. Operator cancel-when(E1,E2) stops the detection of E1
when E2 occurs.
6.1.5 Logic Based
As description logics (DL) form the formal foundation of ontology, it can also be used
as a presentation language for context to describe context-aware database queries un-
der the speciﬁed grammar and semantics, and can be easily transformed into other
context presentation methods.
In line with the temporal description logic with concrete domain ALCF(D), a DL-
based context query language speciﬁcation can be employed. Basically, a context
query CQ includes some predicate symbols connected via boolean operator ∧(AND), ∨
(OR), or ¬ (NOT). Each predicate symbol is associated with some arguments, denoted
as p(a1,a2,. . .,an) (n>0). Different predicate symbols can have different numbers
Authenticated
:25 PM

6.1 Seven Context Query Mechanisms
165
of arguments. These arguments can be either constants or variable with name iden-
tiﬁers. The meaning (or answer) of the query CQ is to deduce the different constant
combinations that, when bound (assigned) to the variables, can make all the involved
predicates true.
6.1.5.1 Predicate
Predicates are used to declare context query conditions and can be categorized
into three types, which are (1) DL concept/role/feature associated predicates, (2)
system built-in predicates, and (3) user-deﬁned predicates.
(1) A DL concept can be designated by an unary predicate, while a DL role/feature
can be designated by a binary predicate. In other words, the logic term C(x) is true if
variable x is an instance of the DL concept C. Similarly, R(x,y) is true if variables x
and y are two concept instances observing the DL role/feature relation R.
Example 29. Raw
global
positioning
system
(GPS)
readings
can
be
deﬁned
through a DL concept of three features (attributes): GPSRead(PersonId,
Timestamp,Position), where Timestamp takes the time-clock format
[Year:Month:Week:Day:] Hour:Minute:Second.
For any of its instances e, GPSRead(e)=TRUE.
High-level events lasting for a period of time can be deﬁned through another
DL concept GPSEvent(PersonId,TimeInterval,Position), where TimeIn-
terval takes the format [StartTimestamp, EndTimestamp].
Given an event E=(PersonId,TimeInterval,Position),
GPSEvent(E)=TRUE, if and only if
∀t∈tE.TimeInterval ∃e((GPSRead(e)=TRUE) ∧
(e.Timestamp=tt) ∧(e.PersonId=E.PersonId) ∧
(e.Position=E.Position)).
Similarly, for the binary DL role WorkFor, if person of identiﬁer e.PersonId
works for “TeleCom”, WorkFor(e.PersonID,“TeleCom”)=TRUE.
◻
(2) System built-in predicates include the ﬁve classical comparison predicates, i.e.,
EQUAL (equal), GE (greater than), GEQ (not less than), LE (less than), and LEQ (not
greater than).
In addition, to facilitate context event querying and detection, temporal compar-
ison predicates on time intervals (e.g., BEFORE/AFTER,EQUALS,MEETS/MET-
BY, OVERLAPS/OVERLAPPED-BY,STARTS/STARTED-BY, DURING,
CONTAINS, FINISHES/FINISHED-BY), together with the high-level temporal op-
erators on context events (e.g., Within,Last,Periodic,Concur,
Sequence,Overlap, During) can be further incorporated to enrich the system
built-in predicate set.
(3) User-deﬁned predicates are deﬁned by users. One such predicate example is
Near(e.Position,placeName). It returns TRUE if position e.Position is near
the place placeName according to a certain user-deﬁned formula.
Authenticated
:25 PM

166
6 Querying Context
Aggregate functions (Sum,Avg,Max,Min,Count) and some other user-deﬁned
functions can also be deﬁned in the language to enforce the operation capability
against context information.
6.1.5.2 Logic Based
A context query consists of four parts (i.e., query condition, query output, query
quality, and output channel), where
–
query condition is a conjunction of disjunctions of query predicates of the form
(Q1,1 ∨⋅⋅⋅∨Q1,n1) ∧⋅⋅⋅∧(Qm,1 ∨⋅⋅⋅∨Qm,nm), where Qi,j can be either positive or
negative predicate deﬁned above;
–
query output is a list of variables of instances or instance attributes;
–
query quality constrains query execution time, query latency, and result uncer-
tainty, since most context information sensed or inferred is uncertain in nature;
–
query output channel speciﬁes how the query output is delivered to the users (as a
ﬁle, message, or through pipe or socket).
Query output, query quality, and output channel can be omitted. When there is no
query output, a Boolean TRUE or False will be returned based on the evaluation of
query condition. The Extended Backus-Naur Form speciﬁcation for the logic-based
context query language is as follows:
context-query =
query-output, ’ ←’, query-condition
[,’QoS:’, QoS-Format]
[,’OutChannel:’, channel-format];
query-output =
’(’, output-item {’,’ output-item},
’)’;
output-item =
variable | variable, ’.’,DL concept-
feature-name;
boolean-operator = ∧| ∨| ¬ ;
query-condition =
’(’,conjunction, ’)’
{’∧’, ’(’, conjunction, ’)’};
conjunction =
predicate | ’¬’ , predicate | predicate,
{’∨’ , predicate};
QoS-Format =
’[ExecTime’, time-interval-format,’]’
| ’[Latency’, number, ’]’ |
’[Uncertainty’, number, ’]’;
channel-format =
’file’, (filename) | ’pipe’ | ’message’ |
’socket’.
Example 30. Here are a few context query examples in the logic-based query language,
where GPSRead,GPSEvent,WorkFor are DL concepts and roles in Example 29.
Authenticated
:25 PM

6.1 Seven Context Query Mechanisms
167
–
Find out people who work for TeleCom and were near XiDan in the period of 7:00
am to 8:00 am?
(E. PersonId) ←GPSEvent(E) ∧WorkFor(E.PersonId,
“TeleCom”) ∧Near(E.Position,“XiDan”) ∧During(E,
[7:0:0,8:0:0]).
–
Find out people who go to XiDan ﬁrst and later go to WangFuJing?
(E1. PersonId) ←GPSEvent(E1) ∧GPSEvent(E1) ∧
EQ(E1.PersonId,E2.PersonId) ∧Near(E1.Position,
“XiDan”) ∧Near(E2.Position,“WangFuJing”) ∧
Sequence(E1,E2).
–
Find out people who go to XiDan three times in one day?
(E. PersonId) ←GPSEvent(E) ∧Near(E.Position,“XiDan”)
∧Periodic(E,3,[0:0:0,24:0:0]).
◻
6.1.6 Programming API Based
Some researchers design new programming languages to support context queries. For
instance, the programming language iQL proposed in Ref. [5] can not only realize the
querying of context information but also directly write program to derive and compose
high-level context information from low-level context data. Besides, it can deﬁne data
types. In iQL, context data sources include active context sources and passive con-
text sources. Passive context sources supply context data upon request. Active context
sources get an initiative signal and then push the context data in a stream to the sub-
scriber. Hybrid data sources are those that not only supply context data upon request
but also push context data stream. For example, the following programming segment
is to query nearby employees whose physical distances are not greater than a certain
threshold [5]:
type Point { double x; double y;}
type EmployeeID schema(“http://acmebadges.com/empID”);
type BadgeAd schema(“http://acmebadges.com/badgeAd”);
boolean function withinDistance(Point p1, Point p2,
double distance)
{ double dx is p2.x-p1.x;
double dy is p2.y-p1.y;
output dx*dx+dy*dy ≤distance*distance;
}
list(EmployeeID) composer function
AllNearbyEmployees(EmployeeID myID, double threshold)
{ tagged(EmployeeID) myTaggedID is
input(BadgeAd ba: ba.empID=myID && ba.tagged=“yes”);
Authenticated
:25 PM

168
6 Querying Context
BadgeAd myAd is myTaggedID.source;
Point myPoint is myAd.coordinates;
list(EmployeeID) nearbyIDs is
input every (BadgeAd ba:withinDistance(ba.coordinates,
myPoint, threshold));
output nearbyIDs;
}
The data type Point is deﬁned in the program, while data type EmployeeID
and BadgeAd are deﬁned in external schemas. A context composer function All-
NearbyEmployees is implemented to return a list of EmployeeIDs, whose distance
to the employee of myID is within the threshold, as measured by function with-
inDistance [5].
6.1.7 Graphic Interface Based
In comparison to program API-based query mechanism, another type of context
query mechanism is graphic interface based. For instance, to query information in
location-based applications, Polyviou et al. [20] added a spatial condition entry to the
traditional Query-by-Example language. It looks like relational tables, where users ﬁll
in a blank cell with what they know, and the program will supply the required in-
formation satisfying users’ query conditions. The spatial condition cell is used for the
entry of geographic conditions. For instance, a user may query the buildings that are
less than 100 m from the building where the user is located by entering their spatial
condition. Perich et al. [19] also proposed a graphical context querying mechanism
called query-by-browsing. The context information is organized in a folder-like hier-
archy. Users can browse context information by clicking on the folders. The folders’
attributes stand for the metadata of the context. Graphical representation of query and
query results is more suitable for the users who are not system designer or technical
person, as this kind of query languages may be easier for them to learn.
6.1.8 Mechanisms Comparison
Before comparing different context query languages and mechanisms, let’s re-
examine what is needed for a context-aware system to query context information in
a mobile and ubiquitous environment.
6.1.8.1 Evaluation Criteria for Context Query Mechanisms
According to such context properties as dynamic versus static, continuous data
streams, spatial–temporal situations, and metadata of contextual information (e.g.,
Authenticated
:25 PM

6.1 Seven Context Query Mechanisms
169
quality of service), the following supports are important for a context query
mechanism.
(1) Support for context reasoning. The contextual information provided by
sensors is often low-level raw information and could not satisfy a user’s need for high-
level meaningful contextual information. For example, the sensors may only sense the
number of people at a meeting room and the state of the projector, but the user may
want to know whether a meeting is being held at the meeting room.
(2) Support for heterogeneous context handling. The contextual information
provided by the provider may be different from what is requested by the target system.
For example, the context provider may supply the contextual information through web
services, say in the SOAP (Simple Object Access Protocol) protocol, while the applic-
ation may query the context in the format of an ontology instance stored in a context
database.
(3) Support for different types of users. Different users expect and need dif-
ferently leveled contextual information. Applicable users should be considered when
designing a suitable context query language.
6.1.8.2 Comparison Summary
Table 6.1 compares different context querying languages based on such criteria as sup-
port for context reasoning, support for handling context heterogeneity, and applicable
users.
(1) The SQL-based context query mechanism is close to SQL, and many techniques
used here are analogues to the techniques in relational databases. However, little work
Table 6.1: Comparisons of different context query mechanisms.
Reasoning
Heterogeneity
Suitable users
Data model
SQL
No speciﬁc
No speciﬁc
System developer/
Relation
based
support
support
end user
model
XML
No speciﬁc
By semantic
System developer
Tree/
based
support
annotation
graph model
Ontology
Good
By ontology
System developer/
Ontology/DL
based
support
mapping
end user
model
Event
No speciﬁc
No speciﬁc
System developer/
Tree/automata
based
support
support
end user
model
Logic
Good
No speciﬁc
System developer/
Logic
based
support
support
end user
model
Programming
By
By
System developer
No speciﬁc
API based
programming
programming
data model
Graphic
No speciﬁc
No speciﬁc
End user
Relational/
interface
support
support
hierarchical
based
model
Authenticated
:25 PM

170
6 Querying Context
has been done in the SQL-based context query mechanism to support context reas-
oning and handling context heterogeneity. The SQL-based mechanism is suitable for
both system developers and end users.
(2) XML is a useful data format to store and exchange context information. So
the XML-based mechanism can help handle the heterogeneity caused by network
transmission. However, existing work so far provides no speciﬁc support for context
reasoning. This mechanism is primarily used by system developers. To implement
the XML-based context query mechanism, it is often required to deﬁne context XML
schema by system designers. Thus, different system designers might design different
schemas, hindering the reuse of context schemas.
(3) Ontology can easily be shared by users and can effectively support context
reasoning. The heterogeneity among different ontologies can be handled through on-
tology mapping, but little work has been done to handle the heterogeneity between
ontology and other context formats. This mechanism is appropriate for system de-
velopers and end users.
(4) The event-based processing language is good at explicating context events
at a high lever. However, it offers limited reasoning and heterogeneity handling
capabilities. This mechanism is appropriate for both system developers and end users.
(5) The logic-based context query language is good at context reasoning, and is a
good option for building intelligent and adaptive behaviors in context-aware comput-
ing and applications. On the other hand, it also incurs the computational complexity
problem in context query processing, calling for high-performance parallel querying
techniques. The logic-based language is applicable to both system developers and end
users.
(6) The programming-based context query language is efﬁcient to express con-
text composition logic. It can be primarily used by system developers who are mastery
of programming. The context reasoning and context heterogeneity handling can be
achieved by the user program if the language is expressive enough. Besides, the pro-
gramming API-based context query mechanism is ﬂexible and ad hoc, and can be
invoked in an embedded way. It is easier to be integrated into existing programs. This
method may be primarily used by system developers. Also no speciﬁc work has been
done within this mechanism to support context reasoning and context heterogeneity
handling.
(7) The graphical interface-based context query mechanism is not expressive
enough for technical professionals in some aspects, but it can give non-professionals
an easy-to-use interface. No speciﬁc support has been provided for context reasoning
and context heterogeneity handling.
In addition, some critical techniques are to be considered in representing and
processing context queries. These techniques include representing metadata of the
context, discriminating push query and pull querying, managing internal and ex-
ternal context sources, distributed query execution, and catching intermediary query
results.
Authenticated
:25 PM

Literature
171
6.2 Recapitulation
Many query execution mechanisms in context querying are generally based on ex-
isting database techniques. This chapter analyzed and compared seven typical kinds
of context query languages and mechanisms according to their support for context
reasoning, support for handling context heterogeneity, applicable users, and data
models. Although not all of these context query mechanisms are implemented in one
single system, it would be helpful to extend a certain context query mechanism with
corresponding techniques when necessary.
The next chapter will detail high-performance context event detection over sens-
ory data coming as streams.
Literature
[1]
R. Adaikkalavan and S. Chakravarthy. SnoopIB: Interval-based event speciﬁcation and
detection for active databases. Data Knowl. Eng., 59, 2006.
[2]
Roger S. Barga, Jonathan Goldstein, Mohamed H. Ali, and Mingsheng Hong. Consistent
streaming through time: A vision for event stream processing. In Proc. of CIDR, pages 363–374,
2007.
[3]
T. Buchholz, M. Krause, C. Linnhoff-Popien, and M. Schiffers. CoCo: Dynamic composition of
context information. In Proc. of MobiQuitous, pages 335–343, 2004.
[4]
S. Catziu and K.R. Dittrich. SAMOS: An active object-oriented database system. Data
Engineering, 15, 1992.
[5]
N.H. Cohen, H. Lei, P. Castro, J. S. Davis II, and A. Purakayastha. Composing pervasive data
using iQL. In Proc. of WMCSA, pages 99–104, 2002.
[6]
U. Dayal, B. Blaustein, A. Buchmann, and et al. The HiPAC project: Combining active databases
and timing constraints. SIGMOD Record, 17, 1988.
[7]
T. Gu, H.K. Pung, and D.Q. Zhang. A middleware for building context-aware mobile services. In
Proc. of Vehicular Technology Conference, pages 2656–2660, 2004.
[8]
T. Gu, H.K. Pung, and D.Q. Zhang. Toward an OSGi-based infrastructure for context-aware
applications. Pervasive Computing, 3(4):66–74, 2004.
[9]
P.D. Haghighi, A.B. Zaslavsky, and S. Krishnaswamy. An evaluation of query languages for
context-aware computing. In Proc. of DEXA, pages 455–462, 2006.
[10]
J. Heer, A. Newberger, C. Beckmann, and J.I. Hong. liquid: Context-aware distributed queries. In
Proc. of Ubicomp, pages 140–148. Springer, 2003.
[11]
N. Honle, U.P. Kappeler, Nicklas D., T. Schwarz, and M. Grossmann. Beneﬁts of integrating meta
data into a context model. In Proc. of CoMoRea, pages 25–29, 2005.
[12]
Mohamed Ali Jonathan Goldstein, Mingsheng Hong and Roger Barga. Consistency sensitive
operators in cedr. Technical Report MSR-TR-2007-158, Microsoft Research, 2007.
[13]
G. Judd and P. Steenkiste. Providing contextual information to pervasive computing
applications. In Proc. of PerCom, pages 133–142, 2003.
[14]
S. Madden and M. J. Franklin. Fjording the stream: An architecture for queries over streaming
sensor data. In Proc. of ICDE, pages 555–566, 2002.
[15]
T. McFadden, K. Henricksen, and J. Indulska. Automating context-aware application
development. In Proc. of the Workshop on Advanced Context Modelling, Reasoning and
Management, pages 90–95, 2004.
Authenticated
:25 PM

172
6 Querying Context
[16]
Y. Mei and S. Madden. Zstream: A cost-based query processor for adaptively detecting
composite events. In Proc. of ACM SIGMOD, pages 193–206, 2009.
[17]
M. Mohania, D. Swamini, S.K. Gupta, S. Bhowmick, and T. Dillon. Event composition and
detection in data stream management systems. In Proc. of DEXA, 2005.
[18]
F. Perich, A. Joshi, Y. Yesha, and T. Finin. Collaborative joins in a pervasive computing
environment. J. of VLDB, 14(2):182–196, 2005.
[19]
S. Polyviou, P. Evripidou, and G. Samaras. Context-aware queries using query by browsing and
chiromancer. In Proc. of PerComp, 2004.
[20] S.A. Rahman and S. Bhalla. Supporting spatial data queries for mobile services. In Proc. of WI,
pages 696–699, 2005.
[21]
F. Wang, S. Liu, P. Liu, and Y. Bai. Bridging physical and virtual worlds: Complex event
processing for RFID data streams. In Proc. of EDBT, 2006.
[22] E. Wu, Y. Diao, and S. Rizvi. High-performance complex event processing over streams. In Proc.
of ACM SIGMOD, pages 407–418, 2006.
Authenticated
:25 PM

7 Detecting Context Events
Abstract: Context information can be either low level (e.g., raw sensor data) or high
level (e.g., posture and activity of a person). High-level context information can be in-
ferred from low-level context information, and its semantics can be explicated through
context events. Querying of high-level context information involves event detection
from low-level context data. As a large portion of low-level context data sources are
sensors, this chapter focuses on high-performance context event detection from sens-
ory data coming as streams. Stream records indexing and condensed composition,
magniﬁed with parallel detection mechanisms, are discussed.
7.1 Integration of Stream and Event Processing
Although event processing and lately streaming data processing evolve independently
with different processing languages and computing methodologies, they enhance
and complement each other during the execution of context-aware applications.
Adaikkalavan and Chakravarthy [4] and Jiang et al. [31, 32] plotted a four-staged frame-
work, which synergistically integrates stream and event processing, where Stage 1
focuses on continuous stream query processing; Stage 2 converts streams into prim-
itive event sequences; Stage 3 is dedicated to event processing; and Stage 4 is the
traditional rule processing level. Some systems like Zstream [40], Cougar [9], and
Tinydb [38] integrate stream and event processing models by directly treating input
stream as a primitive event sequence, and omit the stream-to-event conversion step.
The section overviews the state-of-the-art stream and event processing techniques
as a whole, with the emphasis on stream and event processing languages and pro-
cessing strategies and methods. Some typical stream and event processing prototype
systems are also listed.
7.1.1 Stream and Event Processing Operators
We describe the stream processing at the stream/relation level and event level, re-
spectively. The stream/relation-level processing usually takes stream (or converted
stream relation) as both input and output; while the event-level processing has event
sequences as input and output. Besides, event-level processing does not perform
any computation on the attributes of input event sequences. Instead, they focus on
deﬁning and expressing the evaluation on events and try to reduce the number of
outputs through a certain computation mode. In contrast, stream/relation-level pro-
cessing aims to deﬁne and express the computation on the tuple attributes of the input
stream/relation [32].
DOI 10.1515/9783110556674-007
Authenticated
:25 PM

174
7 Detecting Context Events
7.1.1.1 Stream/Relation-Level Processing Operators
The stream/relation-level operators can be divided into four types based on the
forms of input and output, namely, stream-to-stream operators, stream-to-relation
operators, relation-to-stream operators, and relation-to-relation operators.
(1) Stream-to-stream operators whose inputs and outputs are both streams. For
example, Tribeca system supports three groups of such operators, i.e., qualiﬁcation,
projection, and aggregate [39]. Tribeca’s qualiﬁcation operators specify a source
stream, a result stream, and a list of qualiﬁcation operators to be applied to the source.
Records from the source stream that pass the qualiﬁcations are placed on the result
stream. Tribeca’s projection operators select one or more ﬁelds from each record in
the source stream, assemble the ﬁelds into a new record, and put the record onto the
result stream. Tribeca’s aggregate operators aggregate the values in a stream and pro-
duce a single value. Besides, Tribeca provides demux operator (demultiplex) and mux
operator (multiplex). The former splits a stream into a few substreams, and the later
merges a few substreams into a single stream [39].
GSQL [18] language developed in the Gigascope system is a pure stream query
language with the SQL (Structured Query Language)-like syntax. Its four primary op-
erators over streams are selection, join, aggregation, and merge. Selection ﬁlters the
input stream with some rules or attributes. Join operator is used to join two streams.
Aggregation operator evaluates on a group of streams that have the same special at-
tribute(s). Merge operator combines streams from multiple sources into one stream.
In addition, GSQL allows users to deﬁne custom functions/operators by adding the
codes to the function library and registering them to the system. An example GSQL
query is like:
SELECT FROM
stock s WHERE
s.time<10 AND s.price>3
Spade system supports eight user-deﬁned operators, including Functor (for ﬁltering,
projection, mapping, attribute creation, and transformation), Aggregate (aggregate
functions on unions or groups), Join (for correlating two streams), Sort (for rearranging
stream order), Barrier (for synchronizing input streams), Punctor (for punctuation
generation), Split (for dividing input streams), and Delay (for delaying streams for a
user-deﬁned interval) [23].
The Aurora system deﬁnes ﬁve operators, i.e., Union (for merging two or more
streams into a single output stream), BSort (for rearranging a sensitive order over
input stream based on time stamps), Aggregate (for splitting an input stream into
subwindows), Join (for joining two input streams within a time range), and Resample
(for aligning the pair of streams). It then lets users specify query plans via a graph-
ical interface by arranging boxes (corresponding to query operators) and joining them
with directed arcs to specify data ﬂow. The system later rearranges, adds, or removes
operators in the optimization phase [2, 28].
Authenticated
:25 PM

7.1 Integration of Stream and Event Processing
175
(2) Stream-to-relation operators convert streams into database relations, where
the notion of window is adopted to split a stream into database tuples [7, 32]. There
exist four classes of sliding window speciﬁcation methods, namely, time based, tuple
based, attribute based, and partitioned based.
–
Time-based windows take a time interval T as a parameter and all the stream
records within T form a database tuple (denoted as S [Range T]).
–
Tuples-based windows take a positive integer N as a parameter, and choose N
sequential stream records as a tuple (denoted as S [Rows N]).
–
Attribute-based windows are measured based on speciﬁc stream attribute(s) [2].
–
Partitioned windows take a positive integer N and a subset of attributes of stream
record as parameters. In partitioned windows, a stream is split into substreams by
attribute(s), and then N records are chosen from each substream to form a tuple
(denoted as S [partition by A1, A2, . . . , An rows N]).
StreaQuel of the telegraphCQ system [36] enables more rich windowing schemes over
both the portions of the stream that has already arrived, as well as those portions that
will arrive in the future. This is done using a for-loop construct to declare the sequence
of windows over which the user desires the answers to the query: a variable t moves
over the timeline as the for-loop iterates, and the left and right ends (inclusive) of each
window in the sequence, and the stopping condition for the query can be deﬁned with
respect to this variable t [36].
(3) Relation-to-stream operators transfer database tuples to streams. For in-
stance, the CQL (Continuous Query Language) language [7] offers three relation-to-
stream operators, i.e., Istream, Dstream, and Rstream, where Istream for inserting
tuples into a stream, Dstream for deleting tuples from a stream, and Rstream for con-
verting a relation to a stream. Aurora system deﬁnes Filter operator for routing input
tuples to alternative streams, and Map operator for projecting tuples into a new stream
[2, 28].
(4) Relation-to-relation operators inherit SQL with the same semantic like se-
lect, project, join, union, etc., and usually express time-varying relations with the
support of window and ordering facilities [8, 25, 35].
7.1.1.2 Event-Level Processing Operators
Event-level processing language integrates knowledge from the areas of active data-
bases for complex event speciﬁcation and detection over data streams [40, 43, 50].
Typical event operators are sequence, conjunction (concurrent events), disjunction (a
choice between two events), negation, kleenE closure (event happening zero, one, or
many times), periodic (a regularly happening event), and aperiodic (an irregularly
happening event) [3, 13, 14, 19].
Detailed usage of the event-level processing operators can be found in
Section 6.1.4 of Chapter 6.
Authenticated
:25 PM

176
7 Detecting Context Events
7.1.2 Stream and Event Processing Methods
Three typical ways to process stream data for context event detection are (1) non-
deterministic ﬁnite automata (NFA)-based query mode with matching buffers, (2)
tree-based query mode for logically and physically representing event patterns, and
(3) string-based query mode.
7.1.2.1 NFA-Based Detection Model
The NFA-based model uses NFA or its variants [5, 20, 22, 52]. It parses a query into a
series of state transitions. If a query is satisﬁed, the ﬁnal state will arrive. Wu et al. [52]
built a high-performance complex event detection system over streams using NFA
model. It optimizes system performance by using active instance stacks, pushing
predicates down to sequence scan and construction and pushing windows down.
Agrawal et al. [5] explored the use of shared match buffers to merge equivalent mul-
tiple runs. Cao et al. [12] further incorporated an inference mechanism into event
detection for distributed radiofrequency identiﬁcation tracking and monitoring. It
uses writable on-board tag storage to transfer queries. Woods et al. [51] detected com-
plex events based on ﬁeld-programmable gate arrays. Cadonna et al. [11] implemented
the PERMUTE operator, which allows to express patterns that match any permutation
of a set of events.
Although the NFA-based model is common, it has some limitations [52].
(1)
It evaluates a query in a ﬁxed order in the state transition diagram, and a ﬁxed
evaluation order could be inefﬁcient.
(2)
It is difﬁcult to detect a negative event directly. It usually places a negative ﬁlter
at the end of the stream, which is an inefﬁcient method.
(3)
It is hard to support concurrent events due to an explicit order of state transitions
to follow.
7.1.2.2 Tree-Based Detection Model
The tree-based event detection method is proposed in Ref. [40, 41]. It employs a tree
structure to represent the event detection logic and process. The method allows dy-
namic tree structure optimization according to data characteristics. The optimization
method ﬁrst establishes a cost model, which utilizes the predicate selectivity to estim-
ate the cost of each query tree candidate, and chooses the tree plan with the lower cost
estimate. Then a batch-iterator mode is used to process streaming data online [40]. The
cost-based tree optimization ensures comparable or even better efﬁciency perform-
ance than that of the former NFA-based method. It is also more natural for SQL-like
queries than the NFA-based method [41].
Authenticated
:25 PM

7.1 Integration of Stream and Event Processing
177
7.1.2.3 String-Based Detection Model
A string-based approach is also employed to detect events from streaming data. For
instance, Sadri et al. [46] built a generalized KMP (Knuth–Morris–Pratt) algorithm
to minimize repeated passes over the same data for sequential pattern detection. It
uses SQL-TS (Simple Query Language for Time Series) to express complex sequential
patterns in databases, and then integrates the KMP algorithm and SQL-TS together.
7.1.3 Stream and Event Processing Systems
A few stream and event processing prototype systems have been developed, including
single-site systems (e.g., Aurora [2, 54], TelegraphCQ [15], Stream [6], OpenCQ [37],
NiagaraCQ [17], COUGAR [53], etc.) and distributed systems (e.g., Medusa [54],
Borealis [1], SPADE [23], Gigascope [18], Nile [26], etc).
7.1.3.1 Single-Site Prototype Systems
–
Aurora [2, 54] offered a graphical query interface, and supports both continuous
and ad hoc queries for stream monitoring. Its query optimization is quality aware.
–
TelegraphCQ [15] adjusted the processing on the ﬂy in response to changes in
data availability, system and network characteristics, and user needs. It adopts
Eddy [15] as its processing mechanism, and processes uncertain conditions.
–
Stream [6] emphasized on stream storage management and approximate query
semantics.
–
OpenCQ system [37] handled large-scale event-driven continuous query pro-
cessing and optimization for Internet trafﬁc monitoring.
–
NiagaraCQ [17] ran multiple XML-QL-based continuous queries by grouping sim-
ilar queries, sharing one query plan.
–
COUGAR [53] focused on in-network-styled query processing in sensor networks.
–
CACQ [26] provided a way to execute multiple stream queries simultaneously by
sharing physical operators and work space.
–
Psoup [16] in the Telegraph project allocated each query a unique code and stores
it in a query structure. The query result is stored in a result structure which is
carefully designed, so that result sharing among different queries can be easily
implemented. Users can view the result by the unique code of query anytime.
–
Flux system [47] tackled fault-tolerant and load-balancing problems in stream
processing.
–
Cayuga system [10] scaled out the handling ability according to the arrival rates of
streams and queries.
Authenticated
:25 PM

178
7 Detecting Context Events
7.1.3.2 Distributed Prototype Systems
–
Medusa [54] was a distributed version of Aurora. It dispatches tasks to query
nodes, and each query node uses core stream processing techniques of Aurora.
Some optimal methods such as dynamic query result modifying and dynamic
query change are implemented.
–
Borealis [1] inherited the functionality from Aurora [2] and distribution function-
ality from Medusa [54], and is sometimes called the second-generation distributed
stream processing. Borealis tackles two speciﬁc optimization problems. One is
server-heavy problem where high-volume data streams on a collection of massive
servers need to be processed. The other is the sensor-heavy problem when ex-
tracting and processing sensor data from a network of resource-constrained
devices.
–
System S (short for SPADE System [23]) provided an implementation platform to
allow users to develop applications that can potentially receive, ﬁlter, analyze,
and associate a large number of data streams. It extracts user queries and trans-
forms them into data ﬂow diagrams. System S also considers security and privacy
of system.
7.2 High-Performance Context Event Detection
High-performance context event detection is desirable in real context-aware applica-
tions. Stream records indexing and condensed composition, magniﬁed with parallel
detection mechanisms, are discussed.
7.2.1 Indexing Stream Records
Index is a widely used effective mechanism in traditional database querying and op-
timization. It is applicable to context event detection. For instance, Demers et al. [20]
built two types of indexes on predicates to improve the NFA-based detection perform-
ance. The ﬁrst index is static, focusing on the comparison between an attribute’s value
and a constant. The second index is dynamic, focusing on the comparison between
two attributes’ values. Index schemes have also been designed for selecting data and
queries in streams (e.g., on geospatial streaming data [26, 42, 45]). Index-based joining
is also applicable to tree-based event detection.
7.2.2 Condensed Composition of Stream Records
Compared with index optimization which focuses on index structures, condensed
composition of similar data records focuses on the optimal organization of
Authenticated
:25 PM

7.2 High-Performance Context Event Detection
179
intermediate evaluation results. It is more like merging equivalent runs [5] for the NFA-
based detection model and applies to the tree-based event detection model, which can
utilize dynamic tree structure optimization.
7.2.2.1 A Motivation Example
Taking a social context – stock data Stock(timestamp,name,price), for ex-
ample, where each stream record can be viewed as a stock event, stating the price
of a stock at a certain timestamp. Suppose a fund manager may want to monitor the
stock movement for buying or selling purposes. S/he may issue a query that detects a
certain stock whose price is no less than Google’s stock price plus 2.0 dollars and
later is less than Google’s stock price. Mei and Madden [40] expressed the query in
the following SQL-like manner:
Query 1:
PATTERN T1;T2;T3
WHERE
T1.name=T3.name AND T2.name=’Google’ AND
T1.price≥T2.price+2.0 AND
T3.price<T2.price
WITHIN
500
RETURN
T1,T2,T3
In the PATTERN clause, T1,T2,T3 represent three event classes, the semicolon; de-
notes the sequential relationship of T1,T2, and T3. The predicates in the WHERE
clause express the constraints over T1,T2,T3. In the WITHIN clause, a window size
w=500 is speciﬁed, requiring |T3.timestamp-T1.timestamp| ≤w. The RETURN
clause speciﬁes result composite events of pattern T1;T2;T3 satisfying the query
condition.
Two equivalent query trees for the above query Q are shown in Figure 7.1. In the
trees, basic events are fed into its leaf nodes, and the predicates are pushed down
to the nodes of the trees as lower as possible. In this way, disqualiﬁed events can
be ﬁltered out earlier before they are transferred to the upper tree nodes. The two
nonleaf tree nodes handle two sequential joins, which are T1;T2 and T1;T2;T3 in
Figure 7.1(a), and T2;T3 and T1;T2;T3 in Figure 7.1(b).
In both query trees, a sequence join operator is a nonleaf tree node. It has a left
child and a right child in which each has an event buffer that contains events to be
joined. The sequence operator node also has an event buffer. The composed event is
added to the end of the sequence operator’s buffer to facilitate further processing.
Although the cost-based tree optimization is a state-of-the-art technique, the com-
putational complexity for sequence operator is high. Suppose there are N composite
events satisfying the query predicates, these predicates have to be evaluated at least
N times. This bound is irrelevant to the change of tree structure. In addition, com-
putations are also needed to exclude the unqualiﬁed primitive events by evaluating
Authenticated
:25 PM

180
7 Detecting Context Events
SEQ
SEQ
T1
T2
T3
T1.name = T3.name
T3.price <   T2.price
(a)
SEQ
SEQ
T1
T2
T3
T1.name = T3.name
T1.price >= T2.price + 2.0
(b)
T2.name = 'Google'
T2.name = 'Google'
T3.price <   T2.price
T1.price >= T2.price + 2.0
Figure 7.1: Two equivalent query trees for Query 1: (a) left-deep tree plan and (b) right-deep tree plan.
the predicates on them. Hence, the computations of predicate evaluations are more
than N.
The tree-based event detection method evaluates incoming streaming records in
an iterative manner. Considering continuous arrival of massive streaming records
(basic events), and some of them share similar attribute values, grouping similar
records into a condensed one can eliminate predicate evaluation times for faster
event detection. To illustrate, let’s see Figure 7.2, which corresponds to the left and
right children of the root node in Figure 7.1(a). The connecting lines show prim-
itive (composite) event combinations that need predicate evaluations. Originally,
eight combinations are needed to evaluate the predicates T1.name=T3.name and
(1,3),'IBM',6;(5,7),'Google',4
(8),'IBM',2
(9),'Sun',2
(1),'IBM',6;  (5),'Google',4
(3),'IBM',6;  (5),'Google',4
(1),'IBM',6;  (7),'Google',4
(3),'IBM',6;  (7),'Google',4
Left Event Buffer
       (T1;T2)
(8),'IBM',2
(9),'Sun',2
   Right Event Buffer
(T3)
   Right Event Buffer
(T3)
Left Event Buffer
       (T1;T2)
T1.name = T3.name
 T3.price < T2.price
Condense
the events
Figure 7.2: Nested loop join for sequential join of two events.
Authenticated
:25 PM

7.2 High-Performance Context Event Detection
181
T3.price<T2.price. By means of condensed composition of events, only two
combinations are needed to evaluate the predicates.
Apparently, a condensed composition is useful to reduce the computations for
predicate evaluation. Multiple basic (or composite) events are grouped together ac-
cording to the predicates. They take part in join predicate evaluation as one condensed
event. In this way, the number of predicate evaluations can be reduced.
In the following discussion, it is assumed that basic events in streaming data are
ordered by timestamp, and the query tree plan has already been optimized by the tree
structure optimization method [40].
7.2.2.2 Condensation Term
For condensed composition, condensation terms must ﬁrst be selected according to
the predicates. A condensation term is represented as event-class.attribute.
When an optimal tree plan is generated, condensation terms are added onto each
tree node TN if the following two conditions are satisﬁed:
–
The condensation term event-class.attribute appears in the predicate(s) of the
ancestor tree node(s) of TN.
–
The event class in event-class.attribute appears in the leaf nodes of the subtree of
root TN.
The left-deep tree plan of Query 1, after the condensation terms are added, is shown
in Figure 7.3. {T1.name,T2.price} are two condensation terms for the left SEQ
root SEQ
 left SEQ
T1
T2
T3
T1.name , T2.price
timestamp
name
price
1
IBM
6
2
Sun
7
3
IBM
6
4
IBM
4
5
Google
4
6
IBM
2
7
Google
4
8
IBM
2
9
Sun
2
T3.name , T3.price
T2.price
T1.name , T1.price
Figure 7.3: Condensation terms for Query 1.
Authenticated
:25 PM

182
7 Detecting Context Events
tree node. {T1.name,T1.price} are two condensation terms for the leaf node T1.
{T2.price} is the condensation term for the leaf node T2. {T3.name,T3.price}
are two condensation terms for the leaf node T3.
Mei and Madden [40] called a list of basic events a composite event. In condensed
composition, two types of condensed events, i.e., condensed composite event and con-
densed basic event, are considered accordingly. A condensed composite event is a list
of condensed basic events. A condensed basic event contains a set of basic events
having the same values for the condensation terms.
7.2.2.3 Event Buffer
Each tree node has an event buffer to dynamically maintain intermediate evaluation
results. Taking Query 1, for example, the event buffers associated with the tree nodes
evolve along with each new event arrival from the stock stream.
–
When the ﬁrst event has been processed, the T1’s event buffer has one event
“(1),IBM,6.”
–
When the second event has been processed, the T1’s event buffer has two events
“(1),IBM,6” and “(2),Sun,7.”
–
When the third event has been processed, since the T1’s condensation terms
are T1.name and T1.price, and the third event has the same T1.name and
T1.price values, “(1),IBM,6” and “(3),IBM,6” are condensed into a con-
densed event “(1,3),IBM,6” and it is added to the end of the buffer. The T1’s
event buffer has two events “(2),Sun,7” and “(1,3),IBM,6.”
–
When the fourth event has been processed, the T1’s event buffer has three events
“(2),Sun,7,” “(1,3),IBM,6,” and “(4),IBM,4.”
–
When the ﬁfth event has been processed, the T2’s predicate T2.name=Google
holds.
Thus,
T1
and
T2
are
joined
by
T1.price≥T2.price+2 AND
T2.name=’Google’ at the left SEQ tree node for T1;T2, whose event
buffer has two condensed composite events “(2),Sun,7; (5),Google,4”
and “(1,3),IBM,6; (5),Google,4,” as shown in Figure 7.4.
–
When the sixth event “(6),IBM,2” arrives, it triggers the join of the left
SEQ tree node and T3 according to the conditions T1.name=T3.name AND
T3.price<T2.price. The join result “(1,3),IBM,6;
(5),Google,4; (6),IBM,2” is ﬁnally pushed into the root node’s event
buffer for T1;T2;T3 and output as one query result.
7.2.2.4 Performance Evaluation
The performance of the condensed composition technique is measured by the sys-
tem throughput, which is the maximum number of incoming events the system can
handle per second. A Java-based prototype system is implemented on Java virtual
Authenticated
:25 PM

7.2 High-Performance Context Event Detection
183
T1.name
T2.price
'Sun'
4
'IBM'
4
(2),'Sun',7; (5),'Google',4
(1,3),'IBM',6; (5),'Google',4
 Condensation
Terms' values
  Condensed Composite
   Event Buffer ''T1;T2''
T1.price >= T2.price+2.0
   T2.name = 'Google'
Figure 7.4: The event buffer of the left SEQ node after the ﬁfth basic event is processed in Query 1.
machine 1.6.0_16 with Intel Core2 Quad Q9400 2.66 GHz CPU, and 2 GB RAM, run-
ning Windows 7. In each of the following experiments, the program is run 30 times
and the average measurement is taken.
(1) Experiments on Synthetic Data
The ﬁrst experiment is for Query 2 and is based on the optimal tree structure after op-
timization. The reason for designing such a query is that one can analyze the inﬂuence
of window size and class number by varying them.
Query 2:
PATTERN T1;T2;...;Tn
WHERE
T1.price>T2.price AND T1.price>T3.price
AND ... AND T1.price>Tn.price AND
T1.name=’name1’ AND T2.name=’name2’
AND ... AND Tn.name=’namen’
WITHIN
windowsize
RETURN
T1,T2,...,Tn
Stream data of n event classes (stocks) are synthesized. Their prices are n-dimensional
vector x. The stock price at timestamp t is Xt = Xt–1 + $.
For a stock x, x0 is randomly generated from a uniform distribution on [0.00,
100.00). Each element in $ is randomly selected from a uniform distribution on [–5.00,
5.00). With this random walk data generation method, if xt > 100.00, xt = 100.00 and
if xt < 0.00, xt = 0.00.
The experiment results are as follows: CC stands for tree-based event detection us-
ing condensed composition and Non-CC stands for tree-based event detection without
condensed composition.
Window size. As shown in Figure 7.5(a), the throughput of CC outperforms that of
non-CC in the synthetic data.
Event class number. In Figure 7.5(b), the throughput of non-CC and CC both de-
crease when the class number increases. The throughput of CC still outperforms that
of non-CC.
Authenticated
:25 PM

184
7 Detecting Context Events
 0
50,000
100,000
150,000
200,000
40 50 60 70 80 90 100 110 120 130 140 150 160
Throughput (events/s)
Window size
Class number = 4, CC
Class number = 4, Non-CC
(a)
 0
10,000
20,000
30,000
40,000
50,000
60,000
70,000
80,000
90,000
2
3
4
5
6
7
8
9
10
11
12
13
14
Throughput (events/s)
Class number
Window size = 80, CC
Window size = 100, CC
Window size = 80, Non-CC
Window size = 100, Non-CC
(b)
Figure 7.5: Performance on synthetic data (class number = 4, Query 2): (a) throughput versus window
size and (b) throughput versus class number.
Table 7.1: A piece of the real stock data.
Company
Code
Price
Volume
Timestamp
...
...
...
...
...
WuGangGuFen
600005
6.70
11720135
2010-03-09 10:48:28
DongFengQiChe
600006
6.34
5863751
2010-03-09 10:48:28
ZhongGuoGuoMao
600007
11.50
1086220
2010-03-09 10:48:28
...
...
...
...
...
(2) Experiments on Real Stock Data
The ﬁrst 800 company stocks were collected from the Shanghai Stock Ex-
change Market in the period of 2010-03-09 10:48:28 and 2010-03-09 11:20:13
(http://www.cnblogs.com/raymond19840709/archive/2008/07/31/1257048.html). The
stock prices were published every 5 s. A piece of the real stock data is shown in
Table 7.1.
Query 3 is the experimental query on the real data, aiming to detect a stock whose
price is lower than the price of the stock “600210” at the beginning and is higher than
the price of the stock “600210” later.
Query 3:
PATTERN T1;T2;T3;T4
WHERE
T1.code=T3.code AND T2.code=T4.code AND
T2.code=‘600210’ AND T1.price<T2.price-0.02 AND
T3.price>T4.price+0.02
WITHIN
windowsize
RETURN
T1,T2,T3,T4
From the result presented in Figure 7.6, the condensed composition technique on the
real stock data can also improve the system performance greatly. When the window
Authenticated
:25 PM

7.2 High-Performance Context Event Detection
185
0
500
1,000
1,500
2,000
2,500
3,000
0
10
20
30
40
50
60
70
80
90 100
Throughput (events/s)
Window size (s)
CC
Non-CC
Figure 7.6: Performance on real stock
data (class number = 800, Query 3)
Illustration shows the throughput versus
window size.
size increases, the beneﬁt of condensed composition is evident. For example, when
the window size is 90 s, the throughput of using condensed composition is about 100
times of the throughput of not using condensed composition.
The reason for such huge performance improvement lies in the unique char-
acteristics of the real stock data. First, every 800 data have the same timestamp.
When the window size grows large, say 60 s, the number of data in a window is
about 800∗60/5 = 9,600. Hence, the computational complexity is high for the non-
CC method. Second, when the window size is 60 s, the condensed event number for
the CC method in nonleaf node is about 1/30–1/40 of the event number of the non-CC
method. In other words, the events are condensed 30–40 times by the CC method.
7.2.2.5 Discussion
The performance gain of condensed composition is subject to the data condensation
degree. Let d be the average condensation degree for the event buffer of each tree
node, obtained by sampling in a certain period.
d = size
size󸀠≤1,
where size󸀠and size are the event numbers after and before condensation,
respectively.
For a sequence join operator node, the beneﬁt of condensed composition is:
beneﬁt = a ∗(sizeL ⋅sizeL – dL ⋅dR ⋅sizeL ⋅sizeR).
The cost incurred by the new data structure and operations is:
cost = b ∗(sizeL + sizeR).
Here, a and b are the coefﬁcients for speciﬁc implementations, sizeL is the num-
ber of events in the left child before condensation, sizeR is the number of events in
the right child before condensation, dL, dR are the condensation degrees of the left
child and the right child, respectively. One can set a threshold for beneﬁt–cost and the
system can automatically switch condensed composition on/off for the buffer.
Authenticated
:25 PM

186
7 Detecting Context Events
7.2.3 Partitioning Stream Records for Parallel Processing
Parallelism magniﬁes the power of individual computers and offers high computing
performance [34]. However, it must deal with such issues as parallel query splitting,
query execution, and data partitioning.
7.2.3.1 Parallel Mechanisms
(1) Parallel Query Execution
Three types of parallel query processing approaches, inter-query (tree), intra-query
(inter-operator), and intra-operator, are developed. Inter-query (tree) parallelism
enables parallel execution of multiple queries generated by concurrent transactions.
Intra-query (interoperator) parallelism makes the parallel execution of multiple, inde-
pendent operations possible within the same query. Both inter-query and intra-query
parallelism can be realized through data partitioning. Intra-operator parallelism ex-
ecutes the same operation as many suboperations through both data and function
partitioning [30, 34].
(2) Control of Parallel Query Processing
Control ﬂow and data ﬂow are two basic types of parallel mechanisms adopted by
traditional parallel database systems [21, 49]. In the former control ﬂow mode, one
single node controls the entire system. It starts all the processing nodes, dispatches
queries, adjusts load schedules to balance the workload. In the data-ﬂow mode, there
is no control node, and the processing nodes are driven by data. When the pervi-
ous node generates enough available input data, the next node starts to process data
automatically.
(3) Data Partitioning
Parallelism accompanies data partitioning, which is traditionally based on round-
robin, hash, range, or hybrid range [24]. Ivanova and Risc [29] showed two stream-
ing data partitioning strategies, i.e., window split and window distribute. Window
split splits logical windows into smaller ones depending on applications. It needs to
join the result from each node and reconstruct a new result. Window split is useful
when the complexity is growing by exponential order over the window size. Win-
dow distribute distributes windows of streams in arrival order using the round-robin
strategy. It doesn’t reduce the window size, and only merges the result from each
node. Some common parallel mechanisms lie in partitioning of the query execution
plans or partitioning of the input data. Johnson et al. [33] provided a query-aware ap-
proach that analyzes a query set and chooses the optimal partitioning scheme. Two
load shedding approaches, i.e., a solver-based centralized approach and a distributed
approach, were detailed in Ref. [48].
Authenticated
:25 PM

7.2 High-Performance Context Event Detection
187
Dispatch task
Receive detected
events
Send detection request
and partitioned data
Event Detection Request
Node1
Node2
Node n
Event Detection Result
Stream (s)
Input buffer
Determine
execution strategy
Convert to atomic 
event stream
Feedback
Figure 7.7: Parallel processing of stream data for event detection.
7.2.3.2 Parallel Processing Framework
Figure 7.7 outlines a parallel processing framework. Incoming streams are ﬁrst con-
verted to atomic event streams by the stream-to-event conversion module, and then
put to the input buffer. The task dispatching module breaks down the incoming events
data into smaller pieces according to the user’s event detection request, as well as
the stream partition scheme adopted. The request-and-data sending module then dis-
tributes different workload (i.e., detection request and partitioned stream data) to
different computer nodes to process. The events’ receiving module ﬁnally collects and
combines events detected by different nodes and returns them to the user. The feed-
back module is responsible for dynamic tuning partition granularity and partition
strategy based on the overall system workload and performance.
7.2.3.3 Two Stream Partitioning Strategies
To ensure the event stream does not undergo semantic change during partitioning,
two correctness criteria are enforced in partitioning stream records.
–
Completeness. If an event stream S is partitioned into several pieces S1, S2, ⋅⋅⋅, Sn,
each record in S must appear in one or more of Si. This property ensures data
lossless decomposition, which is identical to lossless fragmentation in distributed
databases [44].
–
Integrity. If an event E can be detected from event stream S, it can also be de-
tected from one and only one partitioned piece of Si. This property ensures each
Authenticated
:25 PM

188
7 Detecting Context Events
processing node to obtain an integral set of event atomic events in order to detect
an event independently.
We examine two types of data partitioning schemes, i.e., time driven and task driven,
for parallel detection of complex events from event streams. In the following, we use
fragment_num to denote the total number of fragments, and the length of a fragment
is denoted as |fragment|.
(1) Time-Driven Event Stream Partitioning
An event stream is partitioned into smaller fragments based on time-stamps of atomic
events and temporal properties of events to be detected. Each fragment contains a
temporally consecutive list of events. The temporal length of each fragment |fragment|
is thus the same.
Since the chapter focuses on temporal operators, we detail our time-driven par-
titioning for different types of operators in the following diagrams. Stream fragments
are allocated to computer nodes in a round-robin manner.
In Figure 7.8, when the duration |E| of E is unknown from the detection require-
ments, we set it to the time distance between every two atomic events, i.e., the
sampling frequency when acquiring stream data records. No overlap exists in this
fragmentation.
In Figure 7.9, a fragment may contain one or more pieces of length |E| equal to Bt
in this case. We use ' to represent the number of pieces in a fragment. Here, there
is |E| data overlap among every two consecutive fragmentation. This is for the sake
of complete detection of all events spanning a Bt time length. In other words, events
starting at the last |E| region of a fragment will be detected within another fragment.
Here, the total number of data overlap is overlap_rec_num = |E|∗(fragment_num–1) =
Bt ∗⌈(stream_rec_num/(' ∗Bt)⌉.
Figures 7.10–7.14 show the time-driven partitioning for processing Periodic,
Concur, Sequence, Overlap, and During operators, respectively. The length of frag-
ments, as well as the amount of data overlaps among the nodes, vary from the
operators to ensure partitioning completeness and integrity.
Stream
t–ʹ
t+ʹ
Node 1:
Node 2:
...
...
...
...
...
ǀEǀ
ǀFragmentǀ =  η*ǀEǀ
Fragment
Node 3:
Within (E, [t–ʹ,t+ʹ])
Figure 7.8: Time-driven partition for operator Within.
Authenticated
:25 PM

7.2 High-Performance Context Event Detection
189
Stream
Node 1:
Node 2:
...
...
...
......
...
...
ǀEǀ
ǀFragmentǀ =  η*ǀEǀ=η*Δt
Last (E, Δt)
Fragment
Node 3:
Figure 7.9: Time-driven partition for operator Last.
t–ʹ
t+ʹ
ǀEǀ
ǀEǀ
ǀEǀ
Periodic (E, {n,} {Δt,} [ t–ʹ, t+ʹ])
ǀFragmentǀ = (n*(ǀEǀ+ΔT ) – ΔT ) *η
Fragment
Stream
Node 1:
Node 2:
Node 3:
ǀEǀ+Δt
Figure 7.10: Time-driven partition for operator Periodic.
......
...
...
...
...
ǀE1ǀ
ǀE2ǀ
ǀFragmentǀ = η*ǀE1ǀ = η*ǀE2ǀ
Fragment
Stream
Node 1:
Node 2:
Concur(E1, E2)
Node 3:
Figure 7.11: Time-driven partition for operator Concur.
(2) Task-Driven Stream Partitioning
The task-driven partitioning is to decompose an event detection task into a few sub-
tasks. Each subtask is executed by a computer node independently. The results from
all the computer nodes are later combined to have the ﬁnal result. Data needed for
each subtask will be allocated to the corresponding node for processing.
Authenticated
:25 PM

190
7 Detecting Context Events
Stream
Node 1:
Node 2:
Node 3:
Fragment
ǀE1ǀ+ǀE2ǀ+Δt
......
ǀFragmentǀ = η*(ǀE1ǀ + ǀE2ǀ+Δt)
Sequence (E1, E2 {, ΔT })
Figure 7.12: Time-driven partition for operator Sequence.
Stream
Node 1:
Node 2:
Node 3:
Fragment
ǀFragmentǀ = η*(ǀE1ǀ + ǀE2ǀ–Δt)
Overlap (E1, E2 {, ΔT })
......
ǀE1ǀ+ǀE2ǀ+Δt
Figure 7.13: Time-driven partition for operator Overlap.
Stream
Node 1:
Node 2:
Node 3:
ǀFragmentǀ = η*ǀE2ǀ
Fragment
During (E1, E2), where ǀE2ǀ≤t ǀE1ǀ
......
ǀE2ǀ
Figure 7.14: Time-driven partition for operator During.
For instance, to detect an event that “someone appeared at a park for 3 hours” from a
global positioning system (GPS) stream, we can divide this detection task into several
subtasks based on person’s id. In this case, we can assign node 1 to process atomic
events related to person of id 1, assign node 2 to process records related to person of id
2, node 3 to person of id 3, and so on. In this case, no fragment overlaps exist. However,
in another example, when checking “whether two people stayed in the same place
Authenticated
:25 PM

7.2 High-Performance Context Event Detection
191
id = 1 or 2
id = 1 or 3
id = 2 or 3
id = 1 or 2
id = 1 or 3
id = 2 or 3
Stream
Node 1:
Node 2:
Node 3:
Concur (E1, E2)
......
Fragment
Figure 7.15: Task-driven partition for operator Concur based on id.
from the beginning to the end,” and assuming the atomic events of three people’s GPS
data, such a partition schema decomposes the detection into three subtasks and as-
signs nodes 1, 2, and 3 to process Concur(E1
1, E2
2), Concur(E1
1, E3
2), and Concur(E2
1, E3
2),
respectively, where Ei
1 (i ∈{1, 2, 3}) represents the event subject is person of id i. Inevit-
ably, fragments detailing person of id 1 are distributed to nodes 1 and 2, and fragments
detailing person of id 2 are to nodes 1 and 3, and fragments detailing person of id 3 are
to nodes 2 and 3, as shown in Figure 7.15. Big overlaps exist in this case.
7.2.3.4 Performance Evaluation
We evaluate the performance of the two stream partitioning methods when processing
different temporal operators described in Section 3. We use four computers (Win7)
for performance test. One computer (Intel(R) Q9650 and 4 GB RAM) is used as dis-
patcher node and other three computers (Intel(R) E5400 2.7 G and 2 GB RAM) are used
as processing nodes. Four computers are connected by 1 GB network switcher. C++
and MPICH2 parallel programming tools, which follow the Message Passing Interface
language standard, are used in our programming. System performance is measured
by the rate at which input stream data is processed, i.e.,
Throughput = stream_rec_num
elapsed_time
,
where stream_rec_num is the number of stream records to be processed, and elapsed_
time is the total elapsed time from sending partitioned fragments and requests to dif-
ferent processing nodes by the dispatch node to the receipt of the ﬁnal combined
results.
(1) Experiments on Synthetic Stream Data
We study the scalability of the two partitioning methods on a synthetic GPS-like
stream of scheme Sgps
= (t, id, xyPosi), where for each id, we simulate its GPS
Authenticated
:25 PM

192
7 Detecting Context Events
movement in four possible directions (i.e., east, south, west, or north), and then
randomly assign a distance from its previous physical location to get the current
xyPosi. The number of stream records (SynGPS_rec_num) we generate varies from
10K to 50K, and the number of ids (SynGPS_id_num) considered varies from 1 to 50.
Our experiments 1 and 2 examine the system throughput under different settings of
SynGPS_rec_num and SynGPS_id_num, respectively.
In addition, as the time-driven partitioning strategy produces temporally equal-
length fragments, we also conduct a study of the inﬂuence of fragment length
|fragment| on its performance, where |fragment| varies from different temporal op-
erators. Among all of them, an important parameter that determines the fragment
length is ' (i.e., the number of pieces in a fragment), as shown from Figures 7.8–7.14.
It represents the number of an integral temporal length of event(s) to be detected. We
vary the value of ' to see the behavior of the time-driven partitioning strategy in our
experiment 3.
Experiment 1: Effect of Total Number of Stream Records
Figure 7.16 shows the behaviors of the two stream partitioning strategies. It is inter-
esting to note that when processing binary-event operators (i.e, Concur, Sequence,
Overlap, and During), the time-driven partitioning strategy performs consistently bet-
ter than the task-driven strategy, as shown in Figure 7.16(d)–(g). For example, at
SynGPS_rec_num = 10K, the throughput under the task-driven strategy to process
Overlap operator is 10,322 records/s, while that under the time-driven strategy is 11,668
records/s. The later is 13% more than the ﬁrst.
This is because the task-partitioning strategy has to allocate a large volume of
stream records to more than one processing node to ensure partition integrity, i.e.,
to ensure all necessary data available for a subtask’s completion by one node. For
example, for Overlap, Duptask = 300% > Duptime = 48.8%. More data to process by the
nodes leads to more processing time, thus lower overall throughput performance. In
comparison, the total amount of data overlaps among the nodes is much less under
the time-driven approach.
However, when processing the two unary-event operators (Last and Periodic), as
each node is responsible for detecting events of certain ids without id duplication
among nodes, there are no data overlaps under the task-driven partitioning. It thus
achieves a higher throughput than the time-driven partitioning which has a data over-
lap among every two nodes, as presented in Figures 7.9 and 7.10. For example, in Last,
Duptask = 0% < Duptime = 48.8%.
Surprisingly, unary-event operator Within in Figure 7.16(a) gives a contradictory
result. Because, ﬁrst both partitions for Within operator don’t have duplication data in
Figures 7.8 and 7.15. Second, by examining the real amount of data to be processed on
each node, we ﬁnd an imbalanced workload (subtask) distribution among the nodes
under the task-driven approach (ImBtask = 4% > ImBtime = 0.8%). As the total number
of stream ids is set to 25 and the number of processing nodes is 3 in this experiment,
Authenticated
:25 PM

(d)
(e)
(f)
(g)
16,000
13,000
11,600
Throughput (records/s)
Throughput (records/s)
Throughput (records/s)
Throughput (records/s)
15,000
12,200
14,000
13,000
12,000
11,000
10,000
10k
20k
Task driven
30k
40k
50k
Number of GPS stream records
10k
20k
30k
40k
50k
Number of GPS stream records
10k
20k
30k
40k
50k
Number of GPS stream records
10k
20k
30k
40k
50k
Number of GPS stream records
Time driven
Task driven
Time driven
Task driven
Time driven
Task driven
Time driven
Task driven
Time driven
Task driven
Time driven
Task driven
Time driven
12,000
11,800
11,600
11,400
11,200
11,000
10,800
10,600
10,400
12,500
12,000
11,500
11,000
10,500
11,400
11,200
11,000
10,800
10,600
10,400
10,200
(b)
(c)
24,000
30,000
Throughput (records/s)
Throughput (records/s)
10k
20k
30k
40k
50k
Number of GPS stream records
10k
20k
30k
40k
50k
Number of GPS stream records
23,500
23,000
22,500
22,000
21,500
21,000
20,500
20,000
28,000
26,000
24,000
22,000
20,000
(a)
31,000
31,500
30,000
29,500
29,000
28,500
10k
Throughput (records/s)
20k
30k
40k
50k
Number of GPS stream records
Figure 7.16: Experiment 1 – Number of stream records versus throughput under SynGPS_id_num = 25,
process_node_num = 3, and ' = 2, where (a) Within(E, [13 : 00 : 00, 14 : 0 : 0]), (b) Last(E, 0 : 0 : 3),
(c) Periodic(E, 3, 5, [13 : 0 : 0, 14 : 0 : 0]), (d) Sequence(E1, E2, 0 : 0 : 20), (e) Overlap(E1, E2, 0 : 0 : 20),
(f) Concur(E1, E2), and (g) During(E1, E2, 0 : 0 : 5).
Authenticated
:25 PM

194
7 Detecting Context Events
node 1 needs to take care of events of nine different ids, while nodes 2 and 3 only
take care of events of eight different ids, respectively. Such an imbalance subtask as-
signment causes more extra data allocated to node 1 for computing integrity, thus
decreasing the overall performance accordingly. Although the imbalance problem also
holds in processing Last and Periodic operators, its inﬂuence is weakened compared
with the data overlaps between every two nodes. In other words, the amount of extra
data to be processed by a node is far less than the total amount of data overlaps across
every two nodes.
Experiment 2: Effect of Total Number of Record ids
The number of different ids in a stream affects the throughput performance of
binary-event operators negatively, as evidenced in Figure 7.17(d)–(g). This is obvious
since there are more event combinations of two different ids, bringing more tasks
(and subtasks) to be detected under both partitioning schemes. Duptask = 30%, but
Duptime = 48.5%. Therefore, the time-driven method performs better.
Nevertheless, keeping stream_rec_num unchanged, when processing unary-
event operators, the inﬂuence of stream_id_num upon processing time is not that
much, as illustrated in Figure 7.17(a)–(c). Taking Last for example, Duptask = 0% and
Duptime > 0, so the task-driven approach is better.
For the Within operator, both partitions have no data duplication, but the imbal-
ance degree varies. When ! (number of ids) varies at 20, 25, 30, 40, 50, ImBtime = 0.53%,
ImBtask is 5%, 4%, 0% (! = 30), 2.5%, 2%, respectively. So only when total id number
is 30, the task-driven partition is better than the time-driven partition.
Experiment 3: Effect of Fragment Length in Time-Driven Partition
From the experimental results presented in Figure 7.18, we can ﬁnd that fragmentation
length affects the throughout performance of the time-driven partitioning scheme. The
reason is obvious, as a larger fragment length leads to a less number of total fragments
to be dispatched to the processing nodes, thus the less amount of data overlaps among
the nodes to process. That’s why we can see a growing trend with the increase of the
number of pieces per fragment (i.e., '). However, when ' reaches a certain value, such
an improvement gets lost. The reason for this is that one or two nodes need to process
one more fragment while the third node has no fragment allocated when the stream
comes to the end.
To set ' properly, for the Within operator, Duptime = 0, so we only consider the
imbalance factor. Let ' be 4, 6, 8, 10. ImBtime is 1%, 0.51%, 1%, 1%. As we can see in
Figure 7.18(a), when ' = 6, the system obtains the best throughput.
For the rest operators, say the Overlap operator, we consider Duptime and ImBtime
in order. Let ' be 4, 6, 8, 10, 12. Duptime is 3.2%, 1.9%, 1.3%, 1.1%, 0.85%. As we can
see in Figure 7.18(b), when ' = 12, the system obtains the best throughput. This is
consistent to our previous analysis result.
Authenticated
:25 PM

20,000
10
15
20
25
30
35
40
45
50
55
Number of GPS stream Ids
10
15
20
25
30
35
40
45
50
55
Number of GPS stream Ids
10
15
20
25
30
35
40
45
50
55
Number of GPS stream Ids
15
10
20
25
30
35
40
45
50
55
Number of GPS stream Ids
Throughput (records/s)
18,000
16,000
14,000
12,000
10,000
8000
20,000
Throughput (records/s)
18,000
16,000
14,000
12,000
10,000
8000
20,000
Throughput (records/s)
18,000
16,000
14,000
12,000
10,000
8000
20,000
Throughput (records/s)
18,000
16,000
14,000
12,000
10,000
8000
(d)
(e)
(f)
(g)
36,000
Task driven
Time driven
Task driven
Time driven
Task driven
Time driven
Task driven
Time driven
Task driven
Time driven
Task driven
Time driven
Task driven
Time driven
34,000
32,000
30,000
28,000
26,000
24,000
20
10
31,000
28,000
Number of GPS stream Ids
Number of GPS stream Ids
Throughput (records/s)
Throughput (records/s)
Throughput (records/s)
25
30
35
40
45
50
20
Number of GPS stream Ids
25
30
40
50
15
20
25
30
35
40
45
50
55
30,000
29,000
28,000
27,000
26,000
25,000
24,000
23,000
22,000
21,000
20,000
19,000
18,000
17,000
26,000
24,000
22,000
20,000
18,000
(a)
(b)
(c)
Figure 7.17: Experiment 2 – Number of stream ids versus throughput under SynGPS_stream_
rec_num = 25K, process_node_num = 3, and ' = 2, where (a) Within(E, [13 : 00 : 00, 14 : 0 : 0]),
(b) Last(E, 0 : 0 : 3), (c) Periodic(E, 3, 5, [13 : 0 : 0, 14 : 0 : 0]), (d) Sequence(E1, E2, 0 : 0 : 20),
(e) Overlap(E1, E2, 0 : 0 : 20), (f) Concur(E1, E2), and (g) During(E1, E2, 0 : 0 : 5).
Authenticated
:25 PM

196
7 Detecting Context Events
30,000
28,000
Concur
Throughput (records/s)
Throughput (records/s)
28,000
29,000
26,000
27,000
24,000
Number of pieces in a fragment
4
25,000
6
8
10
12
Number of pieces in a fragment
4
6
8
10
12
27,000
26,000
25,000
Within
24,000
23,000
22,000
21,000
20,000
During
Overlap
Sequence
Last
Periodic
(a)
(b)
Figure 7.18: Experiment 3 – Number of pieces in a fragment ' versus throughput under the time-driven
partitioning strategy under SynGPS_rec_num = 50K and process_node_num = 3.
(2) Experiments on Real Stock and GPS Stream Data
The applicability of the two partitioning methods is investigated upon a real GPS
stream Sgps = (t, id, xyPosi) and a real stock stream Sstock = (t, name, price). The
stock stream records 75 different kinds of stocks’ prices every 5 s per trading day
from 2010-11-23 to 2010-11-25, i.e., Stock_id_num = 75. The total number of records
is 50K, i.e., Stock_rec_num = 50K). We also trace a person’s GPS location every 1 s
for 15 days, and obtain 12K number of stream records. That is, GPS_rec_num = 12K
and GPS_id_num = 1.
Experimental results on the real stock stream are consistent with those on the
synthetic data, as shown in Figure 7.19. For the real GPS stream, as the total number
of ids is 1, the task-driven partitioning based on id allocates all the tasks to only one
node. In this situation, it is equivalent to a single node’s processing. Compared with
the parallel execution under the time-based partitioning strategy, the later performs
consistently better than the former as expected, as illustrated in Figure 7.19(a)–(c).
7.2.3.5 Discussion
Two types of temporal operators upon events are used in event detection from stream
data. One is upon unary events, including Within, Last, and Periodic which spe-
cify temporal properties of an event. The other is upon binary events, including
Concur, Sequence, Overlap, and During, which describe relatively temporal relation-
ships between two events. Two stream partitioning methods (i.e., time driven and task
driven) are presented for parallel processing of the temporal operators. The experi-
ments with both synthetic and real life stream data sets demonstrate that the amount
of data overlaps among processing nodes, as well as the balanced subtask assignment
(under the second task-driven approach), inﬂuences the throughput performance of
the overall system substantially. Adopting which partitioning strategy must take these
two factors into consideration. However, confronted with out-of-order data arrival
Authenticated
:25 PM

7.2 High-Performance Context Event Detection
197
45,000
40,000
35,000
30,000
25,000
20,000
15,000
10,000
5000
1,600
600
12,000
0
50,000
50,000
Number of GPS/stock stream records (12K/50K)
Number of stock stream records
50,000
Number of stock stream records
50,000
Number of stock stream records
50,000
Number of stock stream records
12,000
50,000
Number of GPS/stock stream records (12K/50K)
12,000 (GPS)
50,000
Number of GPS/stock stream records (12K/50K)
Throughput (records/s)
Throughput (records/s)
Throughput (records/s)
600
Throughput (records/s)
1,600
Throughput (records/s)
40,000
35,000
30,000
25,000
20,000
15,000
10,000
5000
0
Throughput (records/s)
40,000
35,000
30,000
25,000
20,000
15,000
10,000
5000
0
Throughput (records/s)
Task driven
Time driven
Task driven
Time driven
Task driven
Time driven
Task driven
Time driven
Task driven
Time driven
1,400
1,200
1,000
800
600
400
200
0
1,400
1,200
1,000
800
600
400
200
0
550
500
450
400
350
300
250
200
150
100
50
0
550
500
450
400
350
300
250
200
150
100
50
0
(d)
(e)
(f)
(g)
(a)
(b)
(c)
Task driven
Time driven
Task driven
Time driven
Figure 7.19: Experiments on real stock and GPS streams under GPS_id_num = 1, Stock_id_num = 75,
and ' = 2, where (a) Within(E, [13 : 00 : 00, 14 : 0 : 0]), (b) Last(E, 0 : 0 : 3), (c) Periodic(E, 3, 5, [13 :
0 : 0, 14 : 0 : 0]), (d) Sequence(E1, E2, 0 : 0 : 20), (e) Overlap(E1, E2, 0 : 0 : 20), (f) Concur(E1, E2), and
(g) During(E1, E2, 0 : 0 : 5).
Authenticated
:25 PM

198
7 Detecting Context Events
with variable latencies and rates, adaptive partitioning strategies based on feedback
and dynamic status of the overall system are desirable for further investigation.
7.3 Recapitulation
High-performance context event detection from sensory streaming data is demanded
in real context-aware applications. This chapter reviewed state-of-the-art stream and
event processing techniques, with the emphasis on processing languages, methods,
and prototype systems. Three high-performance event context detection techniques
by indexing, condensed composing, and partitioning stream data were particularly
addressed.
The next chapter will present techniques for ﬂexible control of context query
performance and expenses in a cloud environment.
Literature
[1]
D. J. Abadi, Y. Ahmad, M. Balazinska, et al. The design of the borealis stream processing engine.
In Proc. of CIDR, pages 277–289, 2005.
[2]
D. J. Abadi, D. Carney, M. Cherniack, et al. Aurora: A new model and architecture for data stream
management. VLDB Journal, 12, 2003.
[3]
R. Adaikkalavan and S. Chakravarthy. SnoopIB: Interval-based event speciﬁcation and
detection for active databases. Date & Knowledge Engineering, 59(1):139–165, 2006.
[4]
R. Adaikkalavan and S. Chakravarthy. Seamless event and data stream processing: Reconciling
windows and consumption modes. In Proc. of DASFAA, pages 341–356, 2011.
[5]
J. Agrawal, Y. Diao, D. Gyllstrom, and N. Immerman. Efﬁcient pattern matching over event
streams. In Proc. of ACM SIGMOD, ACM, pages 147–160, 2008.
[6]
A. Arasu, B. Babcock, S. Babu, et al. STREAM: The Stanford stream data manager. In Proc. of
ACM SIGMOD, page 665, 2003.
[7]
A. Arasu, S. Babu, and J. Widom. CQL: A language for continuous queries over streams and
relations. In Proc. of DBPL, pages 1–19, 2004.
[8]
Y. Bai, F. Wang, P. Liu, C. Zaniolo, and S. Liu. Rﬁd data processing with a data stream query
language. In Proc. of ICDE, 2007.
[9]
P. Bonnet, J. Gehrke, and P. Seshadri. Towards sensor database systems. In Proc. of MDM,
pages 3–14, 2001.
[10]
L. Brenna, A. Demers, J. Gehrke, et al. Cayuga: A high-performance event processing engine. In
Proc. of ACM SIGMOD, pages 1100–1102, 2007.
[11]
B. Cadonna, J. Gamper, and M. H. Bohlen. Sequenced event set pattern matching. In Proc. of
EDBT, pages 33–44, 2011.
[12]
Z. Cao, Y. Diao, and P. Shenoy. Architectural considerations for distributed RFID tracking and
monitoring. In Proc. of NetDB, 2009.
[13]
S. Catziu, H. Fritschi, and A. Vaduva. SAMOS an active object-oriented database system.
Technical report, University of Zurich, 1996.
[14]
S. Chakravarthy and D. Mishra. Snoop: An expressive event speciﬁcation language for active
databases. Data & Knowledge Engineering, 14(1):1–26 ,1993.
Authenticated
:25 PM

Literature
199
[15]
S. Chandrasekaran, O. Cooper, A. Deshpande, et al. TelegraphCQ: Continuous dataﬂow
processing. In Proc. of ACM SIGMOD, page 668, 2003.
[16]
S. Chandrasekaran and M. J. Franklin. PSoup: A system for streaming queries over streaming
data. VLDB Journal, 12(2):140–156, 2003.
[17]
J. Chen, D. J. Dewitt, F. Tian, and Y. Wang. NiagaraCQ: A scalable continuous query system for
internet databases. In Proc. of ACM SIGMOD, pages 379–390, 2000.
[18]
C. Cranor, T. Johnson, O. Spataschek, and V. Shkapenyuk. Gigascope: A stream database for
network applications. In Proc. of ACM SIGMOD, 2003.
[19]
U. Dayal, B. Blaustein, A. Buchmann, et al. The HiPAC project: Combining active databases and
timing constraints. SIGMOD Record, 17:51–70, 1988.
[20] A. Demers, J. Gehrke, B. Panda, M. Riedewald, V. Sharma, and W. M. White. Cayuga: A general
purpose event monitoring system. In Proc. of CIDR, 2007.
[21]
D. DeWitt and J. Gray. Parallel database systems: The future of high performance database
systems. Communications of ACM, 35(6):85–98, 1992.
[22] F. Fabret, H. A. Jacobsen, F. Llirbat, J. Pereira, K. A. Ross, and D. Shasha. Filtering algorithms and
implementation for very fast publish/subscribe systems. ACM SIGMOD Record, 30(2):126, 2001.
[23] B. Gedik, H. Andrade, K. L. Wu, P. S. Yu, and M. Doo. SPADE: The system S declarative stream
processing engine. In Proc. of ACM SIGMOD, pages 1123–1134, 2008.
[24] S. Ghandeharizadeh and D. J. DeWitt. Hybrid-range partitioning strategy: A new declustering
strategy for multiprocessor database machines. In Proc. of VLDB, pages 481–492, 1990.
[25]
L. Golab and M. Tamer Ozsu. Issues in data stream management. SIGMOD Record, 32(2):5–14,
2003.
[26] M. Hammad, M. F. Mokbel, A. Ali, and X. Xiong. Nile: A query processing engine for data
streams. In Proc. of ICDE, page 851, 2004.
[27]
Q. Hart and M. Gertz. Indexing query regions for streaming geospatial data. In Proc. of STDBM,
2004.
[28] S. S. Henrike, S. Schmidt, H. Berthold, and W. Lehner. QStream: Deterministic querying of data
streams. In Proc. of VLDB, pages 1365–1368, 2004.
[29] M. Ivanova and T. Risch. Customizable parallel execution of scientiﬁc stream queries. In Proc. of
VLDB, pages 157–168, 2005.
[30] U. Jaeger and J. K. Obermaier. Parallel event detection in active database systems: The heart of
the matter. In Proc. of the VLDB Workshop on Active, Real-Time, and Temporal Database
Systems, pages 159–175, 1998.
[31]
Q. Jiang, R. Adaikkalavan, and S. Chakravarthy. Estreams: Towards an integrated model for
event and stream processing. Technical Report CSE-2004-3, University of Texas at Arlington,
USA, July 2004.
[32] Q. Jiang, R. Adaikkalavan, and S. Chakravarthy. MavEStream: Synergistic integration of stream
and event processing. In Proc. of the Second Intl. Conf. on Digital Telecommunications. IEEE
Computer Society, 2007.
[33] T. Johnson, S. Muthukrishnan, V. Shkapenyuk, and O. Spatscheck. Query-aware partitioning for
monitoring massive network data streams. In Proc. of ICDE, pages 1528–1530, 2008.
[34] M. F. Khan, R. Paul, I. Ahmed, and A. Ghafoor. Intensive data management in parallel systems: A
survey. IEEE Trans. Parallel and Distributed Systems, 7(4):383–414, 1999.
[35]
Y. N. Law, H. Wang, and C. Zaniolo. AQuery: Query languages and data models for database
sequences and data streams. In Proc. of VLDB, pages 492–503, 2004.
[36] A. Lerner and D. Shasha. AQuery: Query language for ordered data, optimization techniques,
and experiments. In Proc. of VLDB, pages 345–356, 2003.
[37]
L. Ling, C. Pu, and T. Wei. Continual queries for internet scale event driven information delivery.
IEEE Transactions on Knowledge and Data Engineering, 11(4):610–628, 1999.
Authenticated
:25 PM

200
7 Detecting Context Events
[38] S. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong. The design of an acquisitional query
processor for sensor networks. In Proc. of ACM SIGMOD, pages 491–502, 2003.
[39] S. Mark and H. Andrew. Tribeca: A system for managing large databases of network trafﬁc. In
Proc. of the USENIX Annual Technical Conference, pages 2–2, 1998.
[40] Y. Mei and S. Madden. Zstream: A cost-based query processor for adaptively detecting
composite events. In Proc. of ACM SIGMOD, pages 193–206, 2009.
[41]
J. A. Mikkelsen. Efﬁcient complex event processing over data streams. Master’s thesis,
University of Southern Denmark, 2009.
[42] J. K. Min. A query index for stream data using interval skip lists exploiting locality. In Proc. of
ICCS, pages 245–252, 2007.
[43] M. Mohania, D. Swamini, S. K. Gupta, et al. Event composition and detection in data stream
management systems. In Proc. of DEXA, 2005.
[44] T. M. Ozsu and P. Valduriez. Principles of Distributed Database Systems, 3rd ed., Springer, 2011.
[45]
J. Park, B. Hong, and C. Ban. A continuous query index for processing queries on RFID data
stream. In Proc. of RTCSA, pages 138–145, 2007.
[46] R. Sadri, C. Zaniolo, A. Zarkesh, and J. Adibi. Expressing and optimizing sequence queries in
database systems. ACM Transactions on Database Systems, 29(2):282–318, 2004.
[47] M. A. Shah, J. M. Hellerstein, S. Chandrasekaran, and M. J. Franklin. Flux: An adaptive
partitioning operator for continuous query systems. In Proc. of ICDE, Bangalore, India, pages
25–36, 2003.
[48] N. Tatbul U. Cetintemel and S. Zdonik. Staying FIT: Efﬁcient load shedding techniques for
distributed stream processing. In Proc. of VLDB, pages 159–170, 2007.
[49] W. B. Teeuw and H. M. Blanken. Control versus data ﬂow in parallel database machines. IEEE
Trans. Parallel Distrib. Syst., 11(4):1256–1279, 1993.
[50] F. Wang, S. Liu, P. Liu, and Y. Bai. Bridging physical and virtual worlds: Complex event
processing for RFID data streams. In Proc. of EDBT, 2006.
[51]
L. Woods, J. Teubner, and G. Alonso. Complex event detection at wire speed with FPGAs. J. of
VLDB, 3(1):660–669, 2010.
[52]
E. Wu, Y. Diao, and S. Rizvi. High-performance complex event processing over streams. In Proc.
of ACM SIGMOD, pages 407–418, 2006.
[53]
Y. Yao and J. Gehrke. The COUGAR approach to in-network query processing in sensor networks.
SIGMOD Record, 31(3):9–18, 2002.
[54] S. Zdonik, M. Stonebraker, M. Cherniack, et al. The Aurora and Medusa projects. IEEE Data
Engineering Bulletin, 26:3–10, 2003.
Authenticated
:25 PM

8 Energy Management in Context Querying
Abstract: In context-aware computing, one big concern with querying contextual
information is power consumption, particularly when involving energy-constrained
sensor nodes and mobile services. This chapter depicts the energy-efﬁcient computa-
tion problem in general, followed by some guidelines to tackle the problem in context
querying. From the lowest hardware, computer server, cluster system, to the highest
application levels, existing energy-saving strategies, techniques, evaluation models
and benchmarks are reviewed. Some energy-efﬁcient context querying techniques at
the sides of sensors, mobile front ends, and servers are particularly addressed.
8.1 Energy-Efﬁciency Requirement
Over the past few decades, performance remains as the main goal of query processing
in the data management ﬁeld. However, with the limited primary sources of energy
and rapid climbing of energy demanded by computing, the commitment to reduce
power consumption and environmental impact becomes increasingly important.
It is expected that now and in the future, green computing will be a key challenge
for both information technology and business, advocating environmentally sustain-
able computing and responsible use of computers and related resources efﬁciently
and effectively with minimal or no impact on the environment [54]. Energy efﬁciency
so far constitutes a focal point for green computing.
In fact, most people in the world today are aware of the energy problem at a high
level: even if our primary sources of energy are running out, the demand for energy
in both commercial and domestic environments is increasing, and the side effect of
consistent energy use inﬂuences negatively our global environment. Based on the re-
port of the US Environmental Protection Agency [18], “the servers and data centers in
USA alone consumed about 61 billion kilowatt-hours (kWh) at a cost of $4.5 billion,
which was about 1.5% of the total U.S. electricity consumption in 2006, and this en-
ergy consumption is expected to double by 2011 if continuously powering computer
servers and data centers using the same methods”. Xu et al. [82] showed that elec-
tricity consumed by computer servers and cooling systems in a typical data center
contributes to around 20% of the total ownership cost, equivalent to one third of the
total maintenance cost. When a data center reaches its maximum provisioned power,
it has to be replaced or augmented at a great expense [63]. In the very near future,
energy efﬁciency is expected to be one of the key purchasing arguments in the society.
Nowadays, power and energy have started to severely constrain the design of
components, sensors, algorithms, systems, and context-aware applications. Better
equipment design and better energy management policies are desirable to limit energy
costs without sacriﬁcing scalability [1].
DOI 10.1515/9783110556674-008
Authenticated
:25 PM

202
8 Energy Management in Context Querying
8.2 Energy-Efﬁciency Problem
Energy consumption can be generally computed as
Energy = AvgPower × Time,
where energy and AvgPower are measured in joule and watt, respectively, and 1 joule =
1 watt × 1 s.
Energy efﬁciency is equivalent to the ratio of performance, measured as the rate
of work done, to the power used [73] and the performance can be represented by
response time or throughput of the computing system:
Energy Efﬁciency = Workdone
Energy
=
Workdone
Power × Time = Performance
Power
.
The main approach toward energy efﬁciency is efﬁcient power management. Accord-
ing to eq. (8.1), there are two ways to enhance energy-efﬁcient computing: either
improving the performance with the same power, or reducing power consumption
without sacriﬁcing too much performance. For energy-efﬁcient systems, while max-
imal performance for some tasks (or the whole workload) is still desirable in some
cases, the systems must also ensure the energy usage is minimized. Preferably, a com-
puting system consumes the minimum amount of energy to perform a task at the
maximal performance level [17].
Note that the relationship between performance and energy efﬁciency is not mu-
tually exclusive. A maximal performance could also be achieved by deactivating some
resources or lowering certain individual performance without affecting the workload’s
best possible completion time or throughput in order to optimize energy usage. Brown
and Reams [17] treated energy efﬁciency as an optimization problem. To minimize the
total energy, an energy-efﬁcient system must adjust the system’s hardware resources
dynamically, so that only what is needed to execute tasks is made available. Rivoire
et al. [66] pointed out two major complementary ways to solve the energy efﬁciency
problem: either building energy efﬁciency into the initial design of computer com-
ponents and systems, or adaptively managing the power consumption of systems
or groups of systems in response to changing conditions related to the workload or
environment.
8.3 Solution Guidelines
To deliver effective solutions to the energy efﬁciency problem, the following six
considerations can be taken as the solution design guidelines.
(1) Comprehensive Examination of System Components. To save power consump-
tion, we shall ﬁrst investigate where the power is spent and how to optimize the
Authenticated
:25 PM

8.4 Energy Management at Different Computing Levels
203
power usage. Within a computer system, there are generally ﬁve energy consumers,
namely, processor, disk, memory, input/output (I/O) devices, and chipset. Achieving
energy efﬁciency requires improvements in the energy usage proﬁle of every system
component.
(2) Adopting Power-Manageable Hardware Components. Adopting power man-
ageable hardware components could help improve energy efﬁciency. For example,
the voltage of hardware components can be increased or decreased through dy-
namic voltage scaling (DVS), which is a power management technique in computer
architecture, depending upon circumstances. DVS to decrease voltage is known as
undervolting, and this situation can conserve power [19]. In addition, employing
small form factor disk drives, solid state disk drives, large memory conﬁgurations,
low power processors and memories could decrease power consumption [59]. HP
(Hewlett-Packard Company) and IDC (International Data Corporation) also estimated
that about 69% energy reduction can be achieved within a 3-year period for inform-
ation technology (IT) organizations that migrate to blade self-contained architecture,
where blades can span from servers and storage devices to workstations and virtual
desktops [34, 61].
(3) Building Power Models for Computing Systems. Also, one needs to know how
a computing system is constructed and how an energy-efﬁcient system operates. It is
important to construct a power model that allows the system to know how the power
is consumed, and how the system can manipulate and tune that power [17].
(4) Understanding and Measuring System Performance. To counter for performance
with the least power consumption, computing systems must have ways to timely un-
derstand and measure system performance related to task execution under different
dynamic workloads.
(5) Constructing Energy Optimizers. The system must accommodate an energy
optimizer component, which is responsible for an energy-efﬁcient hardware conﬁg-
uration throughout the system operation at all times. The optimization approaches
may be based on either heuristic or analytical techniques [17].
(6) Reducing Peak Power. Barroso and Holzle [7] explained that current desktop
and server processors can consume less than one third of their peak power at very low
activity modes, which can thus save around 70% of peak power. Tsirogiannis et al. [73]
showed that almost 50% of peak power is actually consumed at idle.
8.4 Energy Management at Different Computing
Levels
From the lowest hardware, computer server, cluster system, to the highest applica-
tions, several energy-saving strategies and techniques have been developed in the lit-
erature. They are general and inspiring enough to the development of energy-efﬁcient
context-aware computing and applications.
Authenticated
:25 PM

204
8 Energy Management in Context Querying
8.4.1 Energy-Efﬁcient Hardware
Computer power can be saved through energy-efﬁcient hardware. The hardware effort
is promoted by labels such as the US Energy Star or the European TCO Certiﬁcation.
At the very low chip level, designers have considered techniques such as dynamic
voltage and frequency scaling, clock routing optimizations, low-power logic, asym-
metric multi-cores, and so on. Some power-down processors like Intel SpeedStep [37],
AMD PowerNow!TM [4], AMD Cool’n’QuietTM [3], and Intel DemandBased Switch-
ing [25] enable slowing down CPU clock speeds, or powering off parts of the chips
if they are idle.
By sensing the lack of user–machine interaction, different redundant hardware
components (e.g., display and disk) can incrementally be turned off or put in the hi-
bernating mode. The advanced conﬁguration and power interface speciﬁcation [32]
provides an open standard for uniﬁed operating system-centric device conﬁguration
and power management, which is an open industry speciﬁcation codeveloped by
Hewlett-Packard, Intel, Microsoft, Phoenix, and Toshiba.
8.4.2 Energy-Efﬁcient Computer Systems
Energy consumption of hardware components in a single or multi-core computer sys-
tem could be controlled by adjusting voltage levels. For example, researchers have
suggested strategies for dynamically turning off DRAM, disk speed control, and disk
spin down [10, 38, 71].
Merkel and Bellosa [53] argued that memory energy consumption can be reduced
with proper scheduling techniques, as this will impact the effectiveness of frequency
scaling by combining the effect of tasks running on a multi-core system, including
memory contention and the technical constraint of chip-wide frequency and voltage
setting.
To minimize energy for periodic preemptive hard real-time tasks that are sched-
uled on an identical multiprocessor platform, AlEnawy and Aydin [2] proposed the
partitioned scheduling of tasks which are assigned rate-monotonic priorities. The in-
tegrated approach considers rate monotonic scheduling, an admission control test, a
partitioning heuristic, and a speed assignment algorithm.
Xian et al. [79] and Zhong and Xu [84] also presented an energy-aware method to
schedule multiple real-time tasks in multiprocessor systems that support DVS. Tak-
ing the probabilistic distributions of tasks’ execution time into consideration, the
method partitions the workload for better energy reduction. Xian et al. [79] ana-
lyzed the problem of energy-aware scheduling for multiprocessors with probabilistic
workload information and derived its mathematical formulation. It also presents
a polynomial-time heuristic method to transform the problem into a probability-
based load balancing problem which could then be solved with worst-ﬁt decreasing
bin-packing heuristic.
Authenticated
:25 PM

8.4 Energy Management at Different Computing Levels
205
8.4.3 Energy-Efﬁcient Cluster Systems
8.4.3.1 Virtualization
Recent studies have found that the virtualization technology can increase energy
efﬁciency [9, 11, 50]. The approach is based on server consolidation by service
virtualization in data centers [13, 35, 36, 55]. Virtualization partitions computational
resources and allows the sharing of hardware. Many services can be visualized and
run within a virtual machine (VM) resulting in signiﬁcant increases in overall energy
efﬁciency. Depending on their utilization, many VMs can run on a single hardware
unit (server consolidation). Therefore, less hardware is needed overall, thus reducing
energy wasted for cooling, while the deployed hardware utilization increases. This
consolidation of shared hardware fosters energy efﬁciency, measured as work accom-
plished per unit of consumed energy. Resources can be visualized on different layers.
Different forms of virtualization can be implemented like hardware virtualization,
operating system-level virtualization and desktop virtualization. Hardware virtualiz-
ation is a virtualization of computers, which hides the physical characteristics of a
computing platform from users, instead showing another abstract computing plat-
form. Operating system-level virtualization is a server virtualization method where
the kernel of an operating system allows for multiple isolated user-space instances, in-
stead of just one. Desktop virtualization involves encapsulating and delivering either
access to an entire information system environment or the environment itself to a
remote client device.
In data center, there are two kinds of virtualization technologies that are studied
a lot recently. One is full virtualization technology such as VMWare [74]. Full virtualiz-
ation is used to provide a certain kind of VM environment that mediates between the
guest operating systems and the native hardware. Certain protected instructions must
be trapped and handled within the hypervisor. The reason is that the underlying hard-
ware isn’t owned by an operating system but is instead shared through the hypervisor.
The other one is paravirtualization, which presents a software interface to VMs that is
similar but not identical to that of the underlying hardware. This technique uses a
hypervisor for shared access to the underlying hardware but integrates virtualization
aware code into the operating system itself. This approach obviates the need for any
recompilation or trapping because the operating systems themselves cooperate in the
virtualization process. A typical paravirtualization product is Xen [77].
The use of VMs comes with many beneﬁts such as safe isolation of colocated
workloads, enabling multiple workloads to be consolidated on fewer servers, res-
ulting in improved resource utilization and reduced idle power costs. Kansal et al.
[42] presented a VM power metering approach, allowing virtualized data centers to
achieve the same savings that nonvirtualized data centers can achieve through phys-
ical server power capping, thus enabling further saving in provisioning costs with
virtualization.
Authenticated
:25 PM

206
8 Energy Management in Context Querying
8.4.3.2 Energy Strategies for Homogeneous and Heterogeneous Clusters
At the cluster level, there has been an emphasis on energy optimization across
computing nodes, including shifting workloads and power budgets by considering
power and temperature constraints across multiple domains and the data center as a
whole [7, 68].
The energy efﬁciency of a cluster can basically be improved in two ways: (1) by
matching the number of active nodes to the current needs of the workload, and placing
the remaining nodes in low-power standby modes; (2) by engineering the compute
and storage features of each node to match its workload and avoid energy waste on
oversized components [48]. For instance, Liu et al. [49] considered energy-efﬁcient
scheduling at a data grid system supporting real-time and data-intensive applica-
tions. They use the location of data and application properties to design a novel
distributed energy-efﬁcient scheduler that aims to seamlessly integrate scheduling
tasks with data placement strategies to provide energy savings. The main energy sav-
ings are obtained by reducing the amount of data replication and task transfers [49].
For a cluster of servers running data-intensive workloads, Lang et al. also made a
connection between replication, energy management, and load balancing [46].
Apart from conserving energy for homogeneous clusters, energy efﬁciency het-
erogeneous clusters were also considered [14, 58, 62, 67]. Designing energy-aware
scheduling algorithms for heterogeneous clusters is technically challenging because
one has to take into account multiple design objectives, including performance (meas-
ured by throughput and schedule length), energy efﬁciency, and heterogeneities.
Basically, processors, networks, disks, and cooling system are four major power con-
sumers in a cluster computing system. Heath et al. [31] argued that designing efﬁcient
servers for heterogeneous clusters requires deﬁning an efﬁciency metric, modeling the
different types of nodes with respect to the metric, and searching for request distribu-
tions that optimize the metric. A cooperative web server for a heterogeneous cluster
that used modeling and optimization is thus designed to minimize the energy con-
sumed per request [31]. Zong et al. [85] proposed a novel scheduling strategy called
energy-efﬁcient task duplication schedule, which can signiﬁcantly conserve power by
judiciously shrinking communication energy cost when allocating parallel tasks to
heterogeneous computing nodes.
8.4.3.3 Energy Efﬁciency on Hadoop and MapReduce Platforms
Energy considerations are also important for Internet data center operators, and
MapReduce is a common Internet data center application. For instance, servers of Ya-
hoo and Microsoft in Quincy, Washington and Google in Dalles, Oregon are all placed
near the Columbia River and hydroelectric dams. These places are chosen by proximity
to the electricity generation as well as to water for cooling. In fact, power and cooling
are currently considered to be the greatest challenges by enterprise data servers [62].
Authenticated
:25 PM

8.4 Energy Management at Different Computing Levels
207
Leverich and Kozyrakis [48] ran Hadoop clusters in fractional conﬁgurations,
keeping only a small fraction of the nodes powered up during periods of low utiliz-
ation. The Covering Set (CS) strategy was proposed for energy management of cluster
system. The CS strategy exploits the replication that is provided by a distributed ﬁle
system (DFS), which keeps multiple copies of each data block spread across nodes
in the cluster. Although scaling down operational clusters , clusters can save 9% to
50% of energy consumption. But there are three drawbacks when deploying CS, which
are storage over provisioning, response time degradation, and DFS modiﬁcation. Re-
cently, Lang and Patel [45] proposed a cluster energy management strategy, All-In
Strategy (AIS). In AIS, the cluster essentially wakes up all nodes and power down the
entire system when there is no work, rather than selectively powering down the nodes.
The main effective factors are the computational complexity of the workload and the
transition time of nodes from a low-power state to a high performance state.
Chen et al. [21] used the energy efﬁciency of MapReduce as a new perspective
for increasing Internet data center productivity. A general framework was provided
to analyze software energy efﬁciency and MapReduce energy efﬁciency in particular.
The method adopted was that: ﬁrst, characterizing the performance of the Hadoop
implementation of MapReduce under different workloads, and then creating quant-
itative models and introducing to guide operators and developers in improving the
performance of MapReduce/Hadoop.
8.4.3.4 Green Cloud Computing
In the context of cloud computing, Berl et al. [12] reviewed the usage of methods and
technologies currently used for energy-efﬁcient operation of computer hardware and
network infrastructure. They also identiﬁed some of the remaining key research chal-
lenges that arise when such energy-saving techniques are extended for use in cloud
computing.
Cloud computing platform as the next generation IT infrastructure enables en-
terprises to consolidate computing resources, reduce management complexity and
speed the response to business dynamics. Xian et al. [78] presented the Green-Cloud
architecture to reduce data center power consumption in guarantee the real-time
performance for many performance-sensitive applications. The architecture can help
consolidate workload and achieve signiﬁcant energy saving for cloud computing en-
vironment, and enable comprehensive online monitoring, live VM migration, and VM
placement optimization.
Manasa and Anirban [51] proposed the architectural principles for energy-efﬁcient
management of Clouds, and energy-efﬁcient resource allocation policies. The schedul-
ing algorithms consider quality-of-service expectations and devices power usage
characteristics. Efﬁcient cloud computing model has immense potential as it offers
signiﬁcant performance gains as regards to response time and cost saving under
dynamic workload scenarios.
Authenticated
:25 PM

208
8 Energy Management in Context Querying
Cloud providers have to apply energy-efﬁcient resource management strategies
for maximizing return on investment (ROI), such as dynamic consolidation of VMs
and switching idle servers off. Beloglazov and Buyya [8] proposed a novel tech-
nique for the energy-efﬁcient threshold-based dynamic consolidation of VMs with
auto-adjustment of the threshold values, which ensures a high level of meeting the
service-level agreements (SLA).
8.4.4 Energy-Efﬁcient Applications
Application-level power-aware computing has become a hot research issue recently.
There are two reasons for the situation. One is low-level power optimization, which
depends heavily on accurate estimation of the power proﬁles of applications, while
much information and statistics needed for such estimations are only available inside
the applications. The other one is many applications having different execution paths
to accomplish the same computational task. Power-aware applications can provide
additional opportunities for power saving through adaptively adjusting its behaviors
according to the power-related states of underlying systems.
Many research projects in this area are about web services, such as [20, 24, 33, 47].
Horvath et al. [33] address DVS in multistage service pipelines, instead on individual
servers and on load-balanced server replicas. A distributed power management ser-
vice is designed for coordinating DVS settings in a way that minimizes global energy
consumption while meeting end-to-end delay constraints. Elnozahy et al. [24] de-
scribed three policies designed to reduce energy consumption in web servers, which
employ two power management mechanisms: DVS and request batching. The ﬁrst
policy uses DVS in isolation. The second policy uses request batching to conserve en-
ergy during periods of low workload intensity. The third policy uses both DVS and
request batching mechanisms to reduce processor energy usage over a wide range of
workload intensities. All the policies trade off system responsiveness to save energy.
Xian et al. [78] presented a general-purpose programming framework for easy
construction of energy-aware applications. It allows applications to be executed with
different plans according to their power costs. This framework provides a program-
ming interface to obtain the estimated energy consumption for choosing a particular
option, as well as the procedure and general guidelines for using the environment
to construct energy-aware programs. It estimates energy based on runtime energy
characterization that records a set of runtime conditions correlated with the energy
consumption of the options.
Application-level power-aware computing has become a hot research issue re-
cently. There are two reasons for the situation. One is low-level power optimization,
which depends heavily on accurate estimation of the power proﬁles of applications,
while much information and statistics needed for such estimations are only available
inside the applications. The other one is many applications having different execution
Authenticated
:25 PM

8.5 Models and Benchmarks of Energy Efﬁciency
209
paths to accomplish the same computational task. Power-aware applications can
provide additional opportunities for power saving through adaptively adjusting its be-
haviors according to the power-related states of underlying systems. Many research
projects in this area are about web services present in Refs. [20, 24, 33, 47].
8.5 Models and Benchmarks of Energy Efﬁciency
How to compare different energy-efﬁcient computing methods against each other and
how to estimate the developed methods whether or not they correspond to reality
constitutes another important question. Substantial efforts on evaluation metrics and
models for energy efﬁciency computing have also been made in both academia and
industry.
8.5.1 Models and Metrics
The real power consumption of a speciﬁc system depends on many factors, such
as workload, system balance, and environmental parameters. Measuring power con-
sumption needs accurate power and thermal models on individual components,
systems, data and computing centers, and applications.
Three different types of modeling approaches exist in the literature, includ-
ing simulation-based, detailed analytical, and high-level black-box approaches
[63, 64].
(1) Considering the difﬁculty in obtaining detailed knowledge about many com-
ponents in a full system, simulation-based approaches intend to model individual
components rather than the whole system or a collection of systems by simulation
[16, 28, 30].
(2) Without simulation, detailed analytical approaches periodically collect hard-
ware and software metrics [6, 40]. For instance, Bircher and John [15] used micro-
processor performance counters for online measurement of complete system power
consumption. Xu et al. [81, 82] presented a power model to accurately measure the en-
ergy costs of database query execution plans with the hypothesis that the peak power
consumption of an entire system during the measurement interval is identical to the
aggregate of the individual nameplate power consumption.
(3) High-level black-box approaches construct a real-time model by ﬁtting a model
to the real-time metrics collected without relying on implementation knowledge. For
instance, Economou et al. [23] used an one-time calibration phase to generate a power
consumption model by correlating AC power measurements with user-level system
utilization metrics at a system level. Fan et al. [26] aggregated power usage of large col-
lections of servers for different classes of applications over history data. Meisner et al.
[52] incorporated suspending and waking transitions to the power model. Lang and
Patel [45] proposed a mathematical model for the energy consumption of a MapReduce
Authenticated
:25 PM

210
8 Energy Management in Context Querying
cluster, which adopted the workload characteristics and hardware characteristics as
abstract meta-models. Poess and Nambiar [59] developed a power consumption model
based on data readily available in the TPC-C (Transaction Processing Performance
Council - Benchmark™C) full disclosure report of published benchmarks.
8.5.2 Benchmarks
Researchers, governmental agencies, and industry standard consortia for perform-
ance measurements, including Transaction Processing Performance Council (TPC),
Standard Performance Evaluation Corporation (SPEC), and Storage Performance
Council, have also actively developed benchmarks to measure energy consumption
of computer systems. Poess et al. [59, 60] provided a very comprehensive overview
of the currently available energy benchmarks, and analyzed their commonalities and
differences along various dimensions, including hardware components, workload
and type of application, along with metric attributes and accuracy and calibration
requirements.
Basically, there are two types of energy benchmarks being developed so far. They
are specialized benchmarks and extended benchmarks with additional energy metrics
added to the existing benchmarks.
For example, TPC establishes a working group which adds energy efﬁciency met-
rics to all its benchmarks. The primary metric, reported by TPC-Energy [72], is in the
form of “Watts per Performance” for the overall System Under Test, where the perform-
ance unit is speciﬁc to each TPC Benchmark. SPEC presents the SPECpower_ssj2008
benchmark [69], which examines the relationship of power and performance and
power consumption for servers at different performance levels, spanning from 100%
utilization to idle in 10% segments, over a set period of time [60].
Rivoire et al. [65] developed a benchmark called JouleSort for evaluating energy
efﬁciency of various sorting algorithms. It uses the same workload as the other ex-
ternal sort benchmarks, but its metric covered total energy, which is a combination of
power consumption and performance. JouleSort is now an I/O-centric benchmark that
measures the energy efﬁciency of systems at peak use.
8.6 Energy-Efﬁcient Query Processing
and Optimization
To build energy-efﬁcient context query mechanisms, solutions from both hardware
and software perspectives are desirable. That is, query engines not only need to accur-
ately estimate and online measure the hardware energy consumption characteristics
under both static and dynamic loads, but also dynamically adapt and tune query
strategies to meet response time and energy goals when initial prediction for energy
consumption deviates from the real case [29, 44, 73, 81, 82].
Authenticated
:25 PM

8.6 Energy-Efﬁcient Query Processing and Optimization
211
8.6.1 Energy Management at Sensor Networks
A variety of context-aware applications like health care, agri-food, environmental and
security sectors rely on sensors and thus call for effective sensor data management
techniques. As the power source of sensors often comes from a battery with a limited
energy, reducing energy consumption of sensors is critical to provide a long enough
lifetime service and avoid inconvenient replacement of the battery in a hostile or un-
practical environment. For a sensor node, the communication cost is often several
orders of magnitude higher than the computation cost, and in wireless sensor net-
works, the majority of the energy is actually consumed for sensor’s communication
rather than computation. Reducing communication workloads among sensor nodes
is apparently the most effective energy-efﬁcient operation.
Anastasi et al. [5] listed three categories of energy-efﬁcient sensing techniques,
including duty cycling, data-driven, and mobility-based schemes. Detailed description
of each scheme can be found in Ref. [5].
(1) Duty cycling schemes are oblivious to data that are sampled by sensor nodes.
Radio transceiver nodes are put in a sleep mode whenever communication is not
required. Duty cycling schemes can be achieved through two different and com-
plementary approaches, i.e., topology control and power management. The topology
control mechanism dynamically adapts the network topology based on the applic-
ation needs so as to allow network operations while minimizing the number of
active nodes. The power management mechanism aims at certain sensor nodes,
which can sleep or wake-up by an on-demand, scheduled rendezvous, or asynchronous
protocol [5].
(2) Data-driven schemes are designed to reduce the amount of sampled data
without sacriﬁcing sensing accuracy required by applications through data reduction
and energy-efﬁcient data acquisition. Data reduction approaches address the case of
unneeded samples. Techniques developed involve in-network processing, data com-
pression, and data prediction. Energy-efﬁcient data acquisition approaches mainly
focus on reducing the energy spent in sensing by means of adaptive, hierarchical, and
model-based active sampling [5].
For instance, to reduce the communication cost, a dual Kalman ﬁlter architecture
is described as a general and adaptive ﬁltering solution to the stream resource man-
agement problem [39]. The architecture lays two same Kalmoan ﬁlters between clients
and servers. The dual ﬁlters predict future data values. Only when the ﬁlter at the
remote source fails to predict future data within the precision constraint, the sensor
sends update value to the server.
Such computation as information aggregation is usually also done on the sensor
nodes so as to transmit less data in the sensor network. Jain et al. [39] delivered an
Acquisitive Query Processing (ACQP) system called TinyDB for querying sensor net-
work. In ACQP, a semantic routing tree is built to effectively manage the routing and
query processing in wireless sensor networks. Acquisitive issues of communication
Authenticated
:25 PM

212
8 Energy Management in Context Querying
scheduling, prioritizing data delivery, and adapting transmission rate are considered.
ACQP techniques can decrease the energy consumption in sensor networks.
(3) Mobility has been considered as an alternative solution for energy-efﬁcient
data collection in wireless sensor networks. The sensors are equipped with mobilities
for changing their location. Mobility-based schemes can be classiﬁed as mobile-sink
and mobile-relay schemes, depending on the type of the mobile entity [5].
8.6.2 Energy Management at Mobile Front Ends
Mobile devices play a more and more important role in context-aware applications.
They are usually equipped with more computing and sensing facilities, including GPS,
WiFi, Bluetooth, accelerometers, audio, video, light sensors, and so on. These em-
bedded sensors in mobile devices are also major power consumers, while the devices
have still limited power sources such as batteries. Recently, the problem of energy-
efﬁcient data management on mobile devices has received much attention. Whang
et al. [76] proposed to support functionalities with low power consumption, which is
also a crucial requirement for a ubiquitous database management systems.
Energy-efﬁcient techniques developed at the mobile side center around position
tracking, continuous context monitoring, and complex event detection.
(1) Position tracking is an important feature of a modern mobile device. The most
common method is to use GPS. However, GPS is extremely power hungry. To minim-
ize energy consumption and improve accurate, Kjargaard et al. [43] and Paek et al.
[57] proposed to periodically duty-cycle GPS. The idea is to use historic information
to estimate and predict users’ movement while concurrently utilizing other sensors
such as accelerometer and Bluetooth. In a mobile broadcast environment, index
has extensively been adopted to support efﬁcient location-based data access and
query [22, 80, 83], since efﬁcient indexing structures can contribute signiﬁcantly to
reduce the tuning time, which is frequently used to estimate the power consumption
of a mobile client.
(2) For continuous context monitoring, Kang et al. [41] proposed a middle-tier
framework between applications and embedded sensors in a mobile environment. An
energy-efﬁcient mobile sensing framework is also presented in Ref. [75] to automatic-
ally recognize user state and detect state transition. These two works also turned on a
minimum set of sensors and used appropriate sensor duty cycles to reduce unneces-
sary, expensive computation, and communication in the context monitoring process
for energy savings.
(3) With the widespread use of wireless connectivity and end-user mobile devices,
complex event detection can be carried out by mobile devices. Considering the interac-
tion between front end and server, Neophytou et al. [56] described three power-aware
query operator placement algorithms that determine which part of a continuous query
plan is executed at the stream management server and which part is executed at
Authenticated
:25 PM

8.6 Energy-Efﬁcient Query Processing and Optimization
213
the users’ wireless devices. Gedik and Liu [27] also presented a distributed real-time
approach to monitor moving object. They utilized the computational power at mobile
objects to alleviate server-side load and communication cost.
8.6.3 Energy-Efﬁcient Query Engines
8.6.3.1 Hardware-Based Approach
Under the assumptions that a CPU consumes a signiﬁcant amount of power compared
to other components in a database system, and the performance of speed and energy
of hard disk drive is close but does not exactly follow the change in the read block
size, Lang and Patel [44] proposed a PVC (processor voltage/frequency control) mech-
anism to trade energy consumption for performance. It aims to execute instructions
at a lower processor voltage and frequency by leveraging the ability of modern pro-
cessors. The CPU frequency is determined by two settings: the front side bus (FSB)
speed and the CPU multiplier. There are basically two methods to modulate CPU fre-
quency, namely, p-state transitioning and underclocking. P-states are characterized
by the combination of CPU multiplier and CPU voltage settings. Underclocking has
the ability to more ﬁnely tune the CPU speed by slowing the FSB speed. Lang and
Patel [44] adopted underclocking to modulate the CPU frequency, and their exper-
iments showed that PVC can be used to reduce the CPU energy consumption by
20% and 49%, while incurring 6% and 3% response time penalties on MySQL and
a commercial database system, respectively.
8.6.3.2 Software-Based Approach
Hardware-based approaches constitute only part of solutions. Considering hardware
heterogeneity and limited power knobs that most hardware offers today, data man-
agement software shall play an effective role in energy optimization as well. Physical
data independence and query optimization do provide opportunities for software-
level control over power-performance trade-offs. Harizopoulos et al. [29] summarized
three kinds of software-based approaches for reducing energy in databases.
(1) Energy-aware optimization, i.e., using existing system-wide knobs and internal
query optimization parameters to achieve the most energy-efﬁcient conﬁguration for
the underlying hardware. Xu et al. [81, 82] gave a strategy to ﬁnd query plans with
low power costs. To do that, a static power proﬁle for each basic database operation
in query processing is deﬁned and maintained as system parameters. The power cost
can be obtained from the speciﬁcations of hardware components and divided by re-
lated estimated time through an iterative approach. The power cost of a plan can be
calculated from those of the higher level operations, containing such basic operations
like CPU power cost per tuple/indexed tuple, power cost for reading/writing one page
Authenticated
:25 PM

214
8 Energy Management in Context Querying
without buffering, and so on. Different power cost functions can thus be constructed
for accessing single relation via different access methods and join operations.
(2) Resource use consolidation, i.e., shifting computation and relocating data to
consolidate resource use in time and space. Whenever system resources are not fully
utilized, the system may allow other concurrent tasks to utilize the resources or allow
the resource to enter a suspended or reduced power mode to save energy.
(3) Redesign software components to minimize energy use, reduce code bloat, and
sacriﬁce certain properties (or allow under-perform in certain metrics) to improve en-
ergy efﬁciency. For instance, Lang and Patel [44] proposed a QED (improved query
energy efﬁciency by introducing explicit delays) mechanism, which uses query ag-
gregation to leverage common components of queries in a workload. In QED, queries
are delayed and placed into a queue on arrival. When the queue reaches a certain
threshold, all the queries in the queue are examined to determine if they can be
aggregated into a small number of groups, such that queries in each group can be
evaluated together. On a workload with simple selection queries on MySQL, QED saves
54% of the CPU energy consumption while increasing the average query response time
by 43%.
8.6.3.3 Trade-Off Between Energy and Performance
For getting the greatest energy efﬁciency in computing, one must ﬁnd the relation-
ship between energy (or power) and performance. It is interesting to note two different
opinions in the literature.
–
Harizopoulos et al. [29], Lang and Patel [44], and Xu et al. [82] claimed that energy
efﬁciency and performance are two different optimization goals, and there exists
the trade-off of energy efﬁciency and performance.
–
Tsirogiannis et al. [73] claimed that energy efﬁciency and performance are con-
sistent, and it was said that “within a single node system (intended for use in
scale-out architectures), the most energy-efﬁcient conﬁguration is typically the
highest performing one.”
The cause of appearing the above two different statements may be their different
assumptions and different estimation methods used. In the ﬁrst line of work, the
baseline power (i.e., idle power) is not included in calculating the energy cost of
the database system after processing the workload. The second work considered CPU
power only rather than the system’s overall active power. By experiments, they found
out that the CPU power does not vary linearly with CPU utilization (which is also the
number of cores in the multicore machine), and utilization is a poor proxy for CPU
power. The CPU power used by various operators can vary up to 60%, even when
they have the same utilization. By measuring the power of system components from
idle to full utilization, almost 50% of peak power is consumed at idle regardless of
query complexity and strategy. This ﬁxed power cost adds a large constant term to
Authenticated
:25 PM

8.7 Discussion
215
the denominator in energy efﬁciency equation (8.1), which makes all subsequent re-
lative power increases worth the added performance, especially when the dynamic
power range is small [73]. However, there do exist database queries that require many
CPU operations, as well as I/O-intensive queries. Considering CPU alone, the mar-
gin of improvements might be greatly reduced by factoring in the power costs of all
components.
8.7 Discussion
Hardware-level efforts to tackle the energy efﬁciency are well known, largely due to
the importance for portable computers and mobile devices in context-aware applica-
tions. The obvious problem is how underlying data management software can exploit
and cooperate with these techniques, and what other hardware-related techniques
can contribute to the data management software. Several research opportunities are
ahead.
8.7.1 Redesigning Physical Context Database
Since some research shows that most power consumption is concentrated in the stor-
age system [59], decisions on context data placement and strategy are expected to
have a signiﬁcant impact on database energy use. In data placement, we expect to
take advantage of more energy efﬁcient choices of physical locations for storing data
and incorporate those into the design process. There exist choices such as different
types of solid state drives, different sets of disk arrays that vary in performance/power
characteristics, remote storage (accessible over a network). Meanwhile, designed to
reduce the amount of I/O in large-range scans and reduce disk bandwidth require-
ments, column stores [70] and data compression may contribute to reduce overall
energy consumption.
8.7.2 Energy-Aware Query Processing and Optimization
Strategies
The query optimization and processing algorithms are crucial to absolute perform-
ance. In present, the fundamental assumptions of query processing algorithms are
regarding the size of available memory, the kind and number of storage, CPU re-
quirement, etc. The primary objective is to improve query response and maximize the
utilization of available system resources. For improving the energy efﬁciency of data
management, we must change the query optimizer to estimate costs and chooses a
query plan in the implementation of the algorithms. In the query process, we expect
Authenticated
:25 PM

216
8 Energy Management in Context Querying
that the buffer management policies reﬂect energy cost for accessing and storing data
in the memory hierarchy. Moreover, we can increase the batch factor to avoid frequent
commits on stable storage in the logging mechanisms.
8.7.3 Dynamic Workload and Resource Management
Resource management is crucially inﬂuenced on overall system performance and
energy consumption, which scheduling the various operators within a query and
deciding on resource allocation for each operator in complex queries. We expect to
build the model of energy and workload, which is based on the resource manage-
ment strategies and workload characteristics. The workload characteristics describe
the job features, such as performance goals, an expected resource consumption, com-
putational complexity, etc. Using the model to evaluate the energy consumption,
we investigate power management policies that optimize the utility of resource and
maximum energy efﬁciency of system.
8.8 Recapitulation
This chapter presented the concept, techniques, and evaluation metrics of energy efﬁ-
ciency problem in context querying. Ranging from the levels of low-level hardware,
computer systems, cluster systems, to high-level applications, some energy man-
agement efforts were reviewed. Energy-efﬁcient context querying and optimization
techniques were particularly discussed. Several research opportunities were pointed
out at the end of the chapter.
The next chapter will discuss a way for ﬂexible control of context query expenses
and query performance in a cloud computing environment.
Literature
[1]
R. Agrawal, A. Ailamaki, P.A. Bernstein, E.A. Brewer, M.J. Carey, S. Chaudhuri, A.H. Doan, D.
Florescu, M.J. Franklin, H. Garcia-Molina, et al. The Claremont report on database research.
Communications of the ACM, 52(6):56–65, 2009.
[2]
T.A. AlEnawy and H. Aydin. Energy-aware task allocation for rate monotonic scheduling. In Proc.
of RTAS, pages 213–223, 2005.
[3]
AMD Cool‘n’, 2002 Quiet Technology. http://www.amd.com/us/products/technologies/
cool-n-quiet/Pages/cool-n-quiet.aspx.
[4]
AMD PowerNow! Technology. http://www.amd.com/us/products/ technologies/amd-
powernow-technology/pages/amd-powernow-technology.aspx, 2000.
[5]
G. Anastasi, M. Conti, M. Di Francesco, and A. Passarella. Energy conservation in wireless
sensor networks: A survey. Journal of Ad Hoc Networks, 7(3):537–568, 2009.
Authenticated
:25 PM

Literature
217
[6]
R. Azimi, M. Stumm, and R.W. Wisniewski. Online performance analysis by statistical sampling
of microprocessor performance counters. In Proc. of ICS, pages 101–110, 2005.
[7]
L.A. Barroso and U. Holzle. The case for energy-proportional computing. IEEE Computer,
40(12):33–37, 2007.
[8]
A. Beloglazov and R. Buyya. Adaptive threshold-based approach for energy-efﬁcient
consolidation of virtual machines in cloud data centers. In Proc. of MGC, 2010.
[9]
A. Beloglazov and R. Buyya. Energy Efﬁcient Allocation of Virtual Machines in Cloud Data
Centers. In Proc. of CCGrid, pages 577–578, 2010.
[10]
R. Bergamaschi, G. Han, A. Buyuktosunoglu, H. Patel, I. Nair, G. Dittmann, G. Janssen,
N. Dhanwada, Z. Hu, P. Bose, et al. Exploring power management in multi-core systems. In Proc.
of ASP-DAC, pages 708–713, 2008.
[11]
A. Berl and H. de Meer. An energy-efﬁcient distributed ofﬁce environment. In Proc. of
EMERGING, pages 117–122, 2009.
[12]
A. Berl, E. Gelenbe, M. Di Girolamo, G. Giuliani, H. De Meer, M.Q. Dang, and K. Pentikousis.
Energy-efﬁcient cloud computing. The Computer Journal, 53(7):1045–1051, 2009.
[13]
A. Berl, R. Weidlich, M. Schrank, H. Hlavacs, and H. de Meer. Network virtualization in future
home environments. In Proc. of DSOM, pages 177–190, 2009.
[14]
R. Bianchini and R. Rajamony. Power and energy management for server systems. Computer,
37(11):68–76, 2004.
[15]
W.L. Bircher and L.K. John. Complete system power estimation: A trickledown approach based
on performance events. In Proc. of ISPASS, pages 158–168, 2007.
[16]
D. Brooks, V. Tiwari, and M. Martonosi. Wattch: A framework for architectural-level power
analysis and optimizations. ACM SIGARCH Computer Architecture News, 28(2):83–94, 2000.
[17]
D.J. Brown and C. Reams. Toward energy-efﬁcient computing. Communications of the ACM,
53(3):50–58, 2010.
[18]
R. Brown, E. Masanet, B. Nordman, et al. Report to congress on server and data center energy
efﬁciency. Lawrence Berkeley National Laboratory. http://eetd.lbl.gov/sites/all/ﬁles/
pdf_4.pdf, 2007.
[19]
J.J. Chen and C.F. Kuo. Energy-efﬁcient scheduling for real-time systems on dynamic voltage
scaling (DVS) platforms. In Proc. of RTCSA, pages 28–38, 2007.
[20] Y. Chen, A. Das, W. Qin, A. Sivasubramaniam, Q. Wang, and N. Gautam. Managing server energy
and operational costs in hosting centers. ACM SIGMETRICS Performance Evaluation Review,
33(1):303–314, 2005.
[21]
Y. Chen, L. Keys, and R. Katz. Towards energy efﬁcient MapReduce. Technical Report
UCB/EECS-2009-109 http://www.eecs.berkeley.edu/Pubs/
TechRpts/2009/EECS-2009-109.html, University of California Berkeley, 2009.
[22] Y.D. Chung, S. Yoo, and M.H. Kim. Energy and Latency Efﬁcient Processing of Full-text Searches
on a Wireless Broadcast Stream. IEEE TKDE, 22(2):207–218, 2010.
[23] D. Economou, S. Rivoire, C. Kozyrakis, and P. Ranganathan. Full-system power analysis and
modeling for server environments. In Proc. of MOBS, 2006.
[24] M. Elnozahy, M. Kistler, and R. Rajamony. Energy conservation policies for web servers. In Proc.
of USENIX, 2003.
[25]
Enhanced Intel SpeedStep Technology and Demand-Based Switching on Linux.
http://software.intel.com/en-us/articles/enhanced-intel-speedsteprtechnology- and-demand-
based-switching-on-linux/.
[26] X. Fan, W.D. Weber, and L.A. Barroso. Power provisioning for a warehouse-sized computer. In
Proc. of ISCA, pages 13–23, 2007.
[27]
B. Gedik and L. Liu. Mobieyes: Distributed processing of continuously moving queries on
moving objects in a mobile system. In Proc. of EDBT, pages 523–524, 2004.
Authenticated
:25 PM

218
8 Energy Management in Context Querying
[28] S. Gurumurthi, A. Sivasubramaniam, M.J. Irwin, N. Vijaykrishnan, M. Kandemir, T. Li, and L.K.
John. Using complete machine simulation for software power estimation: The SoftWatt
approach. In Proc. of HPCA, page 114, 2002.
[29] S. Harizopoulos, M.A. Shah, J. Meza, and P. Ranganathan. Energy efﬁciency: The new holy grail
of data management systems research. In Proc. of CIDR, 2009.
[30] T. Heath, A.P. Centeno, P. George, L. Ramos, Y. Jaluria, and R. Bianchini. Mercury and Freon:
Temperature emulation and management for server systems. In Proc. of ASPLOS, pages
106–116, 2006.
[31]
T. Heath, B. Diniz, E.V. Carrera, et al. Energy conservation in heterogeneous server clusters. In
Proc. of PPoPP, pages 186–195, 2005.
[32] Hewlett-Packard, Intel, Microsoft, Phoenix, and Toshiba. Advanced conﬁguration and power
interface speciﬁcation Revision 4.0a. http://www.acpi. info/DOWNLOADS/ACPIspec40a.
pdf, 2010.
[33] T. Horvath, T. Abdelzaher, K. Skadron, and X. Liu. Dynamic voltage scaling in multitier web
servers with end-to-end delay control. IEEE Transactions on Computers, pages 444–458, 2007.
[34] HP BladeSystem. http://www8.hp.com/us/en/products/servers/ bladesystem/.
[35]
IBM. Project Big Green. http://www-03.ibm.com/press/us/en/ pressrelease/ 21524.wss, 2007.
[36] IBM. The green data center. http://www-900.ibm.com/cn/systems/pdf/
CIO_Guide_to_Green_Data_Center.pdf, 2007.
[37]
Intel SpeedStep Technology. http://www.intel.com/support/processors/sb/ CS-028855.htm.
[38] C. Isci, A. Buyuktosunoglu, C.Y. Cher, P. Bose, and M. Martonosi. An analysis of efﬁcient
multi-core global power management policies: Maximizing performance for a given power
budget. In Proc. of MICRO, pages 347–358, 2006.
[39] A. Jain, E.Y. Chang, and Y.F. Wang. Adaptive stream resource management using Kalman ﬁlters.
In Proc. of ACM SIGMOD, pages 11–22, 2004.
[40] R. Joseph and M. Martonosi. Run-time power estimation in high performance microprocessors.
In Proc. of ISLPED, pages 135–140, 2001.
[41]
S. Kang, J. Lee, H. Jang, H. Lee, Y. Lee, S. Park, T. Park, and J. Song. Seemon: Scalable and
energy-efﬁcient context monitoring framework for sensor-rich mobile environments. In Proc. of
MobiSys, pages 267–280, 2008.
[42] A. Kansal, F. Zhao, J. Liu, N. Kothari, and A.A. Bhattacharya. Virtual machine power metering
and provisioning. In Proc. of SoCC, pages 39–50, 2010.
[43] M.B. Kjargaard, J. Langdal, T. Godsk, and T. Toftkjar. Entracked: Energy-efﬁcient robust position
tracking for mobile devices. In Proc. of MobiSys, pages 221–234, 2009.
[44] W. Lang and J.M. Patel. Towards eco-friendly database management systems. In Proc. of
CIDR, 2009.
[45]
W. Lang and J.M. Patel. Energy management for MapReduce clusters. Journal of VLDB,
3(1):129–139, 2010.
[46] W. Lang, J.M. Patel, and J.F. Naughton. On Energy Management, Load Balancing and
Replication. SIGMOD Record, 38(4):35–42, 2009.
[47] C. Lefurgy, K. Rajamani, F. Rawson, W. Felter, M. Kistler, and T.W. Keller. Energy management for
commercial servers. Computer, 36(12):39–48, 2003.
[48] J. Leverich and C. Kozyrakis. On the energy (in) efﬁciency of Hadoop clusters. ACM SIGOPS
Operating Systems Review, 44(1):61–65, 2010.
[49] C. Liu, X. Qin, S. Kulkarni, C.Wang, S. Li, A. Manzanares, and S. Baskiyar. Distributed
energy-efﬁcient scheduling for data-intensive applications with deadline constraints on data
grids. In Proc. of IPCCC, pages 26–33, 2008.
[50] L. Liu, H. Wang, X. Liu, X. Jin, W.B. He, Q.B. Wang, and Y. Chen. Green-Cloud: A new architecture
for green data center. In Proc. of ICAC, pages 29–38, 2009.
Authenticated
:25 PM

Literature
219
[51]
H.B. Manasa and B. Anirban. Energy aware resource allocation in cloud datacenter. Journal of
Engineering and Advanced Technology, 2(5):277–281, 2013.
[52]
D. Meisner, B.T. Gold, and T.F. Wenisch. PowerNap: Eliminating server idle power. ACM SIGPLAN
Notices, 44(3):205–216, 2009.
[53]
A. Merkel and F. Bellosa. Memory-aware scheduling for energy efﬁciency on multicore
processors. In Proc. of HotPower, pages 123–130, 2008.
[54] S. Murugesan. Harnessing green it: Principles and practices. IT Professional,
10(1):24–33, 2008.
[55]
R. Nathuji, K. Schwan, A. Somani, and Y. Joshi. Vpm tokens: Virtual machine-aware power
budgeting in datacenters. Cluster Computing, 12(2):189–203, 2009.
[56] P. Neophytou, M.A. Sharaf, P.K. Chrysanthis, and A. Labrinidis. Power-aware operator placement
and broadcasting of continuous query results. In Proc. of MobiDE, pages 49–56, 2010.
[57]
J. Paek, J. Kim, and R. Govindan. Energy-efﬁcient rate-adaptive GPS-based positioning for
smartphones. In Proc. of MobiSys, pages 299–314, 2010.
[58] A. Pavlo, E. Paulson, A. Rasin, D.J. Abadi, D.J. DeWitt, S. Madden, and M. Stonebraker. A
comparison of approaches to large-scale data analysis. In Proc. of ACM SIGMOD, pages
165–178, 2009.
[59] M. Poess and R.O. Nambiar. Energy cost, the key challenge of today’s data centers: A power
consumption analysis of TPC-C results. Journal of VLDB, 1(2):1229–1240, 2008.
[60] M. Poess, R.O. Nambiar, K. Vaid, J.M. Stephens Jr, K. Huppler, and E. Haines. Energy
benchmarks: A detailed analysis. In Proc. of e-Energy, pages 131–140, 2010.
[61]
K. Quinn and D. Fleischer. Forecasting total cost of ownership for initial deployments of server
blades. IDC white paper, ftp://ftp.hp.com/
%2F/pub/c-products/blades/idc-tco-deployment.pdf, 2006.
[62] R. Raghavendra, P. Ranganathan, V. Talwar, Z. Wang, and X. Zhu. No power struggles:
Coordinated multi-level power management for the data center. ACM SIGPLAN Notices,
43(3):48–59, 2008.
[63] P. Ranganathan, S. Rivoire, and J. Moore. Models and metrics for energy-efﬁcient computing.
Advances in Computers, 75:159–233, 2009.
[64] S. Rivoire, P. Ranganathan, and C. Kozyrakis. A comparison of high-level full-system power
models. In HotPower, pages 3–3, 2008.
[65] S. Rivoire, M.A. Shah, P. Ranganathan, and C. Kozyrakis. JouleSort: A balanced
energy-efﬁciency benchmark. In Proc. of ACM SIGMOD, pages 365–376, 2007.
[66] S. Rivoire, M.A. Shah, P. Ranganathan, C. Kozyrakis, and J. Meza. Models and metrics to enable
energy-efﬁciency optimizations. IEEE Computer, 40(12):39–48, 2007.
[67] C. Rusu, A. Ferreira, C. Scordino, and A. Watson. Energy-efﬁcient real-time heterogeneous
server clusters. In Proc. of RTSS, pages 418–428, 2006.
[68] R.K. Sharma, C.E. Bash, C.D. Patel, R.J. Friedrich, and J.S. Chase. Balance of power: Dynamic
thermal management for internet data centers. IEEE Internet Computing, 9(1):42–49, 2005.
[69] SPECpower_ssj2008. http://www.spec.org/power_ssj2008/.
[70] M. Stonebraker, D.J. Abadi, A. Batkin, X. Chen, M. Cherniack, M. Ferreira, E. Lau, A. Lin, S.
Madden, E. O’Neil, et al. C-store: A column-oriented DBMS. In Proc. of VLDB, pages
553–564, 2005.
[71]
R. Teodorescu and J. Torrellas. Variation-aware application scheduling and power management
for chip multiprocessors. In Proc. of ISCA, pages 363– 374, 2008.
[72]
TPC Energy Speciﬁcation Version 1.2.0. http://www.tpc.org/tpc_energy/ spec/.
[73]
D. Tsirogiannis, S. Harizopoulos, and M.A. Shah. Analyzing the energy efﬁciency of a database
server. In Proc. of ACM SIGMOD, pages 231–242, 2010.
[74]
VMWare. http://www.vmware.com.
Authenticated
:25 PM

220
8 Energy Management in Context Querying
[75]
Y. Wang, J. Lin, M. Annavaram, Q.A. Jacobson, J. Hong, B. Krishnamachari, and N. Sadeh. A
framework of energy efﬁcient mobile sensing for automatic user state recognition. In Proc. of
MobiSys, pages 179–192, 2009.
[76] K.Y. Whang, I.Y. Song, T.Y. Kim, and K.H. Lee. The ubiquitous DBMS. SIGMOD Record,
38(4):14–22, 2009.
[77]
Xen user manual. http://bits.xensource.com/Xen/docs/user.pdf.
[78] C. Xian, Y.H. Lu, and Z. Li. A programming environment with runtime energy characterization for
energy-aware applications. In Proc. of ISLPED, pages 141–146, 2007.
[79] C. Xian, Y.H. Lu, and Z. Li. Energy-aware scheduling for real-time multiprocessor systems with
uncertain task execution time. In Proc. of DAC, pages 664–669, 2007.
[80] J. Xu, B. Zheng, W.C. Lee, and D.L. Lee. Energy efﬁcient index for querying location-dependent
data in mobile broadcast environments. In Proc. of ICDE, pages 239–250, 2003.
[81]
Z. Xu. Building a power-aware database management system. In Proc. of IDAR, pages 1–6, 2010.
[82] Z. Xu, Y.C. Tu, and X. Wang. Exploring power-performance tradeoffs in database systems. In
Proc. of ICDE, pages 485–496, 2010.
[83] B. Zheng, W.C. Lee, K.C. Lee, D.L. Lee, and M. Shao. A distributed spatial index for error-prone
wireless data broadcast. Journal of VLDB, 18(4):959–986, 2009.
[84] X. Zhong and C.Z. Xu. Energy-aware modeling and scheduling for dynamic voltage scaling with
statistical real-time guarantee. IEEE Transactions on Computers, pages 358–372, 2007.
[85] Z. Zong, X. Qin, X. Ruan, K. Bellam, M. Nijim, and M. Alghamdi. Energy-efﬁcient scheduling for
parallel applications running on heterogeneous clusters. In Proc. of Parallel Processing, pages
19–26, 2007.
Authenticated
:25 PM

9 Context Query Efﬁciency Versus Expense
Abstract: Cloud computing is the latest evolution of Internet-based computing, driven
by enormous tangible and powerful beneﬁts in specialization and economies of scale.
With cloud computing becoming a major change in the computing industry, shifting
context querying to a cloud platform is desirable. However, attaining the beneﬁts of
cloud computing requires context query engines that support the key principles of
the cloud computing paradigm. One of them is the pay-as-you-go charging mode. It
brings about a few critical concerns about query expenses paid and query perform-
ance received. This chapter reviews the computing paradigm and beneﬁts of cloud
computing, and then presents a weighted multiple objective optimization approach
for ﬂexible control of context query expenses and query performance in a cloud
environment.
9.1 Basics of Cloud Computing
The increase in distributed, parallel, grid, and utility computing, storage, and commu-
nication power on the one hand, in combination with virtualization and load balance
technologies on the other hand, will eventually lead to the cloud computing era,
which delivers computing as a service rather than a product.
According to the U.S. NIST (National Institute of Standards and Technology) [6],
“cloud computing is a model for enabling convenient, on-demand network access to
a shared pool of conﬁgurable computing resources (e.g., networks, servers, storage,
applications and services) that can be rapidly provisioned and released with minimal
management effort or service provider interaction.”
9.1.1 Service Models of Cloud Computing
Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as
a Service (IaaS) are generally regarded as three typical service models in cloud
computing.
SaaS: Delivering software as a service over the Internet, eliminating the need to
install and run the application on the customer’s own computers, and simplifying
maintenance and support efforts.
PaaS: Delivering a computing platform as a service, so that customers can
consume cloud infrastructure and meanwhile sustain cloud applications.
IaaS: Delivering computer infrastructure as a service, freeing customers from pur-
chasing servers, software, data-center space, or network equipment, and allowing
customers to buy and pay for the needed resources as fully outsourced services.
DOI 10.1515/9783110556674-009
Authenticated
:25 PM

222
9 Context Query Efﬁciency Versus Expense
9.1.2 Characteristics and Beneﬁts of Cloud Computing
Key characteristics of cloud computing lie in the following aspects:
On-demand self-service: Instantaneous need at a particular time slot avails com-
puting resources in an automatic fashion.
Broad network access: Computing resources are delivered over the network and
used by various applications.
Resource pooling: Computing resources are pooled together to serve multiple
consumers through multitenancy or virtualization, resulting in reduced costs.
Rapid elasticity: Resources become immediate on an as-needed and pay-as-you-go
basis, and can scale up whenever they are wanted, and release them once ﬁnished to
scale down.
Measured service: Mechanisms are provided to measure the usage of these re-
sources for each individual consumer through metering.
Cost saving: In eyes of cloud consumers, money is shifted from capital expenses
to operating expenses, enabling the companies to focus on adding value in their areas
of core competence, such as business and process insight, instead of building and
maintaining information technology (IT) infrastructure. Along with the faster man-
agement approval, faster development, and simpliﬁed maintenance of applications,
the heartbeat rate of the companies can be increased.
9.1.3 Key Beneﬁts of Cloud Computing
The beneﬁts of cloud computing are overwhelming in eyes of cloud consumers, in-
cluding (1) elasticity, (2) service-level agreement and quality of services (QoS), (3) robust
and reliable, (4) ubiquitous access, (5) unawareness of which resources being used, and
(6) availability. Its elasticity implying that
–
no upfront commitment and contract (pay as you go);
–
use it whenever you want, and let it go once you ﬁnish;
–
scale up and down (computing on demand);
–
scale horizontally (various services on demand);
–
inﬁnite, immediate, and invisible computing resources.
9.2 Concerns of Context Query Performance
and Expense
IaaS is a typical application of virtualization, provided by a few well-known IT com-
panies such as Amazon, IBM, etc. Virtualization in computing refers to the creation
of a virtual (rather than actual) version of something, such as a hardware platform,
Authenticated
:25 PM

9.2 Concerns of Context Query Performance and Expense
223
operating system, a storage device or network resources. In an IaaS cloud environ-
ment, the service provider conﬁgures virtual machines (VMs) with physical computing
resources (e.g., CPU and memory) and lease them to IaaS customers to run their
applications. The customers then pay for the resources they use.
Under the pay-as-you-go charging mode, from the standpoint of IaaS customers,
a few critical concerns about expenses paid for the service and obtained performance
of their applications arise: how to minimize the expenses while ensuring the perform-
ance? how to optimize the performance within the budge limit? how to compromise
the expenses and performance, or balance performance of applications running on
different VMs? For the IaaS provider, how to reasonably conﬁgure VMs so as to meet
various requirements from different customers is also a challenge.
Consider the following scenario. An IaaS customer wants to run several query ap-
plications on a few VMs in an IaaS cloud environment. How to conﬁgure these VMs
with reasonable physical computing resources is the ﬁrst issue that the IaaS pro-
vider needs to solve. Above that, the customer may also have a number of doubts
and questions related to the expenses paid for the resources, given performance
requirements.
Q1: Is it possible to achieve the best performance within a budget limit?
Q2: Is it possible to minimize the expenses while still guaranteeing the performance?
Q3: Is it possible to make a compromise between expenses and performance?
Q4: Is it possible to achieve a balanced performance when running different applic-
ations on different VMs under the premise of guaranteeing the overall performance?
Answers to the above questions inﬂuence the acceptance of IaaS in the future.
With the popularity of the virtualized IaaS cloud computing, issues on performance
management and resource charging are becoming focal points to be resolved.
9.2.1 Performance Management
Performance is a critical concern in virtualization for cloud customers. To deliver
satisfactory application performance, tremendous researches have been conducted,
including performance management and application behavior analysis [16], applica-
tion performance isolation in a virtualized environment [12], power and performance
management in virtualized computing environments via lookahead control [5], auto-
mated control of multiple virtualized resources [8], proper resource conﬁguration
for VMs [1, 9], balancing power and application performance for virtualized server
clusters [15], control of resource allocation and power management in virtualized data
centers [14].
The performance of database applications in a virtualized and cloud environment
can be improved through different methods, including on-demand provisioning of
VMs [11] and dynamic resource conﬁguration of VMs [13].
Authenticated
:25 PM

224
9 Context Query Efﬁciency Versus Expense
9.2.2 Resource Charging
Besides satisfactory performance, minimizing the expenses that customers pay for
using cloud resources given performance requirements is another critical issue in
IaaS cloud environments [2]. Henzinger et al. [4] describe a ﬁne-grained charging
model in a simulated cloud environment. The charging model refers to how IaaS pro-
viders charge for their services. Usually IaaS providers set the leasing price for each
kind of computing resource, and customers pay for the resources conﬁgured for their
VMs. The current charging model of IaaS is that providers offer and charge for ﬁxed
conﬁgured VMs. For example, Amazon EC2 offers 10 different kinds of resource con-
ﬁgurations for VMs. If a customer’s application running on one VM, s/he needs to pay
the amount corresponding to that VM of a ﬁxed resource conﬁguration. Rogers et al.
[10] further presented a framework for minimizing the operational cost on Amazon
EC2 within target QoS expectations. It assumes each VM runs a single type of queries,
and takes operational cost as the only optimization objective.
9.3 Tuning of Query Performance and Expense
Under the assumptions that (1) each customer can ask for a ﬂexible (rather than ﬁxed)
amount of resources (i.e., CPU and memory) for personalized conﬁguration as needed,
(2) each VM can be assigned different types of query workload (Tables 9.1 and 9.2),
and (3) each customer may have more than one VM to run his/her query applications,
Table 9.1: Query workload.
Symbol
Meaning
N
The total number of query workloads with mixed queries,
also the total number of virtual machines (VMs)
Wi
The workload running on the ith VM, where (1 ≤i ≤N)
ni
The number of query types in the workload Wi
Qij
The jth type of query in the workload Wi, where (1 ≤j ≤ni)
pij
The percentage of Qij in the workload Wi
tij
The execution time of query Qij
Ti
The total execution time of workload Wi
Table 9.2: Resource conﬁguration of VMs.
Symbol
Meaning
ci
The CPU capacity of the ith VM (e.g., 1 GHz)
mi
The memory size of the ith VM (e.g., 1 GB)
Authenticated
:25 PM

9.3 Tuning of Query Performance and Expense
225
the concerns of performance and expense can be formulated into a multiple object-
ive optimization problem. Query performance, query expense, as well as performance
balance across different VMs constitute its optimization objectives.
9.3.1 Problem Formulation
Suppose an IaaS customer wants to run N query workloads on N VMs. A workload
running on a VM instance is composed of one or more (i.e., mixed) types of queries.
In other words, one workload corresponds to one VM, and one workload may contain
many types of queries.
Assume that the query composition of each workload, including query types and
distribution of them (i.e., the percentage of each query type), is available through
sampling and statistics.
9.3.1.1 Performance Model for Queries
To establish relationships between query expense and query performance, a mapping
function from computing resources (CPU and memory) to query response time, i.e., a
query performance model, is necessary. Such a mapping function can be obtained
by ﬁtting some sample data from two sources. One is the estimate from the query
optimizer [13], and the other is from the experiment.
To ensure data accuracy, TPC-H queries are executed on VMs with different
resources conﬁguration. Among the nonlinear surface ﬁtting functions of the nu-
meric analysis software Origin 8.0 over the experimental results, function Rational2D
ﬁts the data well, whose R2 value (an indicator of the data ﬁtness degree) reaches
0.99. The closer R2 is to 1, the better the ﬁtness is. Rational2D function is based on
LMA (Levenberg–Marquardt algorithm [7]), a robust iterative algorithm, effective to
nonlinear ﬁtting problems. The standard form of the Rational2D function is as follows:
tij =
Aij0 + Aij1 ∗xi + Aij2 ∗yi + Aij3 ∗y2
i + Aij4 ∗y3
i
1 + Aij5 ∗xi + Aij6 ∗x2
i + Aij7 ∗x3
i + Aij8 ∗yi + Aij9 ∗y2
i
,
(9.1)
where tij is the execution time of query Qij; xi =
1
ci , wherein ci is the CPU capacity of
the ith VM; y =
1
mi , wherein mi is the memory size of the ith VM; Aijk (0 ≤k ≤9) is the
coefﬁcient obtained by ﬁtting the sample data.
Figure 9.1(a) and (b) shows the ﬁtted results for two typical query types (Q13 and
Q21) in the TPC-H benchmark, which represent CPU-intensive and memory-intensive
queries, respectively. The execution time of CPU-intensive queries is mainly inﬂu-
enced by CPU capacity, while that of memory-intensive queries is mainly inﬂuenced
by memory size.
Authenticated
:25 PM

226
9 Context Query Efﬁciency Versus Expense
0.0
2.0
1.5
1.0
0.5
0.5 1.01.52.02.5
200
0
400
600
800
Execution time (s)
Execution time (s)
(a)
(b)
1/CPU(1/GHz)
1/Memory(1/GB)
1/CPU(1/GHz)
1/Memory(1/GB)
0.0
2.0
1.5
1.0
0.5
0.5 1.01.52.02.5
20
40
60
80
Figure 9.1: The ﬁtted results for two TPC-H benchmark queries: TPC-H Query Q13 and (b) TPC-H
Query Q21.
9.3.1.2 Performance Model for Workloads
After obtaining the mapping function (9.1), the performance of a workload, consisting
of a set of queries of different types, can be formalized. To compute the execution time
Ti of the workload Wi, a simple way is to add the execution time of all the queries in Wi:
Ti =
ni
∑
j=1
pij × tij,
(9.2)
where pij is the percentage of Qij in the workload Wi, tij is the execution time of query
Qij, and ni is the number of query types in Wi.
As the execution time of a workload may not properly reﬂect the CPU or memory
demanding, formula (9.2) may lead to unreasonable resources conﬁguration. For ex-
ample, the workload on a VM instance contains two different types of queries, one
is CPU intensive and the other is memory intensive; the order of magnitude of the
execution time of the former is 1 ms, and that of the latter is 1,000 ms. Besides, the
CPU-intensive queries account for 90% of the workload, and the memory-intensive
ones 10%. Obviously, the former queries play a dominant role in this workload. So
the reasonable way to improve the performance of the workload should be giving it
more CPU capacities. In order to satisfy most query requests, 90% of the optimization
efforts shall go to the CPU-intensive queries. However, based on formula (9.2), the res-
ult of 90% multiplied by 1 ms is 0.9 ms, much less than 100 ms (10% multiplied by
1,000 ms). In this case, the memory-intensive queries will be mistaken for the dom-
inator of the workload, leading to incorrect resource conﬁguration. In other words,
difference of the order of magnitude of the execution time may lead to undesirable
and even wrong conﬁguration strategies. So it’s necessary to normalize the execution
time of different types of queries.
Formula (9.3) gives a normalization method by multiplying the execution time
with a normalization factor, whose denominator is the average execution time of
a preselected standard query type, and the numerator is the execution time tij of
Authenticated
:25 PM

9.3 Tuning of Query Performance and Expense
227
query Qij. Here, the average execution time of each type of query can be obtained from
sample data:
t󸀠
ij = +ij × tij,
+ij = tij/ts
(1 ≤j ≤ni),
(9.3)
where t󸀠
ij is the normalization result of tij; +ij is the normalization factor of tij; ts is the
execution time of the standard query type selected beforehand.
Hereby, the normalized execution time Ti is formed as
Ti =
ni
∑
j=1
pij × +ij × tij.
(9.4)
In the following discussion, unless otherwise speciﬁed, any execution time is the
normalized execution time.
9.3.2 Multiple Objective Optimization
Three objectives can be formulated based on the above information:
objective1 : min {
N
∑
i=1
pricecpu × ci + pricemem × mi} ,
(9.5)
objective2 : min
N
∑
i=1
Ti = min
N
∑
i=1
ni
∑
j=1
pij × +ij × tij,
(9.6)
objective3 : min{max{Ti}} = min
{
{
{
max
{
{
{
ni
∑
j=1
pij × +ij × tij
}
}
}
}
}
}
,
(9.7)
where pricecpu is the leasing price of CPU (e.g., 0.02$/GHz ⋅hour), pricemem is the
leasing price of memory (e.g., 0.01$/GB ⋅hour). In formula (9.7), objective1 aims to
minimize the total expenses per leasing interval, objective2 aims to minimize the total
execution time of all the workloads (i.e., optimizing the overall performance), and
objective3 aims to minimize the maximal execution time of different workloads to
balance the performance of different workloads.
Through a linear weighting method, the multiple objectives can be combined into
a single objective:
objective0 : min
3
∑
i=1
!i × objectivei,
(9.8)
where !i (1 ≤i ≤3) are the weights of the three different objectives.
Authenticated
:25 PM

228
9 Context Query Efﬁciency Versus Expense
Three constraints are as follows:
constraint1 :
ni
∑
j=1
pij = 1,
(9.9)
constraint2 : tij ≤boundqij,
(9.10)
constraint3 :
N
∑
i=1
pricecpu × ci + pricemem × mi ≤budget.
(9.11)
In formula (9.11), constraint1 ensures the percentages of different types of queries in
the workload adds up to 1, constraint2 ensures the execution time for each query type
is below a certain bound, and constraint3 ensures the total expense below a budget
limit.
9.3.3 A Genetic Approach for Multiple Objective Optimization
A genetic approach can be exploited to solve the problem. Genetic algorithms (GA) are
a class of robust and efﬁcient search methods based on the concept of the adaptation
in natural organisms [3]. They have been successfully applied to complex problems in
diverse ﬁelds, including the traveling salesperson problem, facility layout design, rule
induction, communication network design, and VLSI cell placement, etc.
The basic ideas of GAs are: (1) a representation of solutions, typically in the form
of bit strings, likened to genes in a living organism; (2) a pool of solutions likened to
a population or generation of living organisms, each having a genetic makeup; (3) a
Darwinian notion of ﬁtness, which governs the selection of parents who will produce
offspring in the next generation; (4) genetic operators like crossover and mutation,
which derive the genetic makeup of an offspring from that of its parents; and (5) sur-
vival of the ﬁttest where the least ﬁt solutions are removed from the solution pool at
each generation (i.e., do not survive into the next generation) [3].
GA begins by randomly generating an initial pool of solutions (i.e., the popula-
tion). Each solution constitutes a random CPU and memory conﬁguration for a query
workload. The pool size is a parameter of GA. The pool should be large enough to in-
sure a reasonable sample of the actual solution space, but not so large as to make the
algorithm approach exhaustive enumeration.
During each iteration, called a generation, the solutions in the pool are evaluated
through the objective functions (9.8). After evaluating the ﬁtness performance of each
solution in the pool, some of the solutions are selected to be parents. The probability
of any solution being selected is proportional to its ﬁtness.
Parents are paired and genetic crossover and mutation operators are applied to
produce new solutions, called offspring. A new generation is formed by selecting
solutions (parents and offspring) based on their performance, i.e., keeping only the
best solutions.
Authenticated
:25 PM

9.3 Tuning of Query Performance and Expense
229
Crossover operates on two solutions (parents) at a time and generates offspring
by combining segments from both parents. Mutation generates a new solution by in-
dependently modifying one or more gene values of an existing solution, selected at
random.
Parents with above average performance are expected to contain partial good
solutions. Due to the stochastic selection process, such parents are likely to produce
more offspring than those with below average performance. Over successive iterations
(generations), the number of good solutions represented in the pool tends to increase,
and the number of bad ones tends to decrease. Therefore, the average performance of
the pool tends to improve. The GA ﬁnally stops after a predeﬁned number of iterations.
The time complexity of the GA is O(|Generation|∗|Population| ∗|Workload|), where
|Generation| is the number of generations, |Population| is the size of population, and
|Workload| is the number of query workloads. Some empirical results show that when
|Population| is set to 30, the algorithm can give a near-optimal conﬁguration strategy,
with |Generation| reaching two or three orders of magnitude; and when |Workload| is
less than 10, it takes less than 1 s to execute the algorithm.
9.3.4 Performance Evaluation
The effectiveness of the proposed multiobjective optimization strategy for CPU-
memory resource conﬁguration under different kinds of query workloads is evaluated
through a series of experiments.
9.3.4.1 Experimental Setup
(1) Computing Environment
Two identical computers construct a simulated cloud environment, each with a 2.5
GHz Intel Xeon Quad-Core processor and a 5 GB memory. XenServer, a powerful open-
source industry standard based on Xen, is used for virtualization. CentOS-5.4-x86_64
is used as the operation system of VMs.
(2) Charging Model
The absolute leasing prices of CPU and memory do not inﬂuence the evaluation, but
the ratio between them should be reasonable. According to the market prices of CPU
and memory, the price ratio is approximately 2:1. In the experiments, the leasing prices
of CPU and memory are set to 0.02$/GHz ⋅hour and 0.01$/GB ⋅hour, respectively.
(3) Query Workloads
Table 9.3 illustrates different kinds of query workloads. For instance, workload W1
consists of TPC-H query Q11 and Q21 with 9:1 quantity ratio, and workload W2 is com-
posed of Q11 and Q6 with 9:1 quantity ratio. So both workloads are CPU intensive
because Q11 plays a dominant role in them.
Authenticated
:25 PM

230
9 Context Query Efﬁciency Versus Expense
Table 9.3: Query workloads (TPC-H query Q11 and Q13 are CPU intensive, Q21 is memory intensive,
and Q6 is not sensitive to either CPU or memory. Q11 and Q6 have the same order of magnitude in
execution time, while Q21 has nearly two orders higher than them).
Workload
TPC-H queries
Quantity ratio
Query category
W1
Q11, Q21
9:1
CPU intensive
W2
Q11, Q6
9:1
CPU intensive
W3
Q6, Q21
1:9
Memory intensive
W4
Q6, Q13
1:9
CPU intensive
W5
Q6, Q21
9:1
Insensitive to either CPU or memory
W6
Q6, Q13
n:n-1
When n increases, CPU intensiveness
(for 1 ≤n ≤9)
is decreased.
W7
Q13, Q21
9:1
CPU intensive
W8
Q13, Q21
1:9
Memory intensive
W9
Q13, Q21
5:5
CPU intensive
W10
Q13, Q21
n:(10-n)
From memory intensive to
(for 1 ≤n ≤9)
CPU intensiveness as n increases.
9.3.4.2 Effectiveness of Normalizing Query Execution Time
Queries in a workload may have different orders of magnetic of execution time. To
verify the necessity and effectiveness of normalizing query execution time in for-
mula (9.4) Ti
= ∑ni
j=1 pij × +ij × tij, two CPU-intensive workloads (W1 and W2 in
Table 9.3) are executed on two VMs, respectively. The longer query execution time
of the memory-intensive Q21 may mislead to resource conﬁguration. However, after
normalizing all the execution time, a more reasonable conﬁguration can be derived.
Table 9.4 compares two conﬁguration results with and without time normalization
under various budget constraints. It can be seen that the strategy with time normal-
ization is more reasonable, giving priority to CPU capacity for both VMs. In contrast,
the strategy without time normalization, giving priority to memory capacity for W1,
doesn’t correspond to the actual demand.
Table 9.4: Resource conﬁguration for two CUP-intensive workloads with different budgets.
Budget ($)
Workload
With normalization
Without normalization
CPU (GHz)
Memory (GB)
CPU (GHz)
Memory (GB)
0.04
W1
0.7
0.5
0.6
1.3
W2
0.8
0.5
0.5
0.5
0.06
W1
1.2
0.6
1.1
1.9
W2
1.2
0.5
0.7
0.5
0.08
W1
1.6
1.0
1.5
2.1
W2
1.6
0.5
1.1
0.5
Authenticated
:25 PM

9.3 Tuning of Query Performance and Expense
231
Table 9.5: Three typical CPU and memory conﬁgurations for VM instances from Amazon EC2, where
the conﬁguration ratio between CPU and memory is about 1.872.
Type of VM instances
CPU (GHz)
Memory (GB)
Small
1
1.7
Large
4
7.5
Extra large
8
15
9.3.4.3 Evaluating Query Efﬁciency Under Different Workloads
For comparison, the conﬁguration solution from Amazon EC2 is taken as a default
setting. Table 9.5 illustrates three typical CPU and memory conﬁgurations for VM in-
stances from Amazon EC2. The linear regression result shows that its conﬁguration
ratio between CPU and memory is about 1.872. Thus, in our default resource conﬁgur-
ation, to average the given budget for all VMs, for each VM instance, the memory size
is set to 1.872 times as CPU capacity.
Consider query efﬁciency only, i.e., objective0 : min ∑3
i=1 !i × objectivei = min!2 ×
objective2, where !1 = !3 = 0 and !2 = 1. The improvement of query efﬁciency is
computed as Improvement = (Tdefault – Toptimal)/Tdefault, where Tdefault and Toptimal are
the total execution times of all the workloads by taking the default and optimized
conﬁguration strategies, respectively.
Two different combinations of query workloads from Table 9.3 are executed by two
VMs, respectively.
Case 1: two CPU-intensive workloads (W1 and W2). Figure 9.2(a) compares the total
execution time with the default conﬁguration and the proposed optimized conﬁgura-
tion. Over 20% performance improvement can be achieved by optimizing the resource
conﬁguration.
Case 2: a memory-intensive workload (W3) and a CPU-intensive workload (W4).
The optimized resource conﬁguration can also lead to over 20% performance improve-
ment, as shown in Figure 9.2(b).
Case 3: a workload insensitive to either CPU or memory (W5) and a varied
CPU-intensive workload (W6). According to the result presented in Figure 9.2(c),
the performance improvement drops signiﬁcantly when the percentage of Q6 in W6
increases.
The basic experiments show that the optimized resource conﬁguration can lead
to performance improvement when the query workloads are sensitive to resources, or
else the improvement beneﬁt isn’t much.
9.3.4.4 Adjusting Query Expense–Efﬁciency Ratios under Different Budgets
Consider query efﬁciency and query expense, i.e., objective0 : min ∑3
i=1 !i×objectivei =
min(!1 × objective1 + !2 × objective2), where !3 = 0 and the ratio of (!1 : !2) is set to
(0:1), (10:1), (50:1), and (90:1), respectively.
Authenticated
:25 PM

232
9 Context Query Efﬁciency Versus Expense
350
250
150
50
0.04 0.06 0.08
0.1
0%
10%
20%
30%
0.12
Budget ($)
Execution time (s)
Improvement
350
450
250
150
50
0%
10%
20%
30%
0.04 0.06 0.08
0.1
0.12
Budget ($)
Execution time (s)
Improvement
(a)
(c)
(b)
10%
130
150
170
190
30%
50%
70%
90%
0%
5%
10%
15%
20%
25%
Percentage of Q6 in W2
Execution time (s)
Improvement
Default execution time
Optimal execution time
Improvement
Figure 9.2: Response time under different workloads: (a) Case 1: two CPU-intensive workloads;
(b) Case 2: a CPU-intensive workload and a memory-intensive workload; (c) Case 3: a resource-
intensive workload and a varied CPU-intensive workload.
Three different workloads, each containing TPC-H query Q6, Q21 and Q13 in Table 9.3
with a random quantity ratio, are executed on three VMs, respectively.
Figure 9.3(a) shows the query execution time when ignoring the expense-
performance ratio by setting (!1 : !2) to (0:1). After the budget increases to a certain
value, the total query execution time reduces slowly, implying less performance gain
despite more expense spent. This is because when the computing resources reach
a certain level, the performance nearly reaches a saturation point. In this case,
single objective optimization fails to give a ﬁne conﬁguration strategy. Hereby, ad-
justing the weight ratio between expense and efﬁciency is desirable. As illustrated
in Figure 9.3(b), the trade-off between expense and efﬁciency can be achieved by ad-
justing the ratio between query expense and query execution time. Especially with
a higher budget, more expense can be saved without damaging the query response
performance much.
9.3.4.5 Balancing Execution Time of Different Workloads
To balance the query execution time of different workloads, the third objective
objective3 is to be considered. That is, objective0 : min ∑3
i=1 !i × objectivei = min(!2 ×
objective2 + !3 × objective3), where !1 = 0 and the ratio of (!2 : !3) is set to (1:0), (1:1),
Authenticated
:25 PM

9.3 Tuning of Query Performance and Expense
233
Table 9.6: Balancing two workloads’ execution time under different ratios !2 : !3.
Ratio !2 : !3
W1 exec time (s)
W2 exec time (s)
Total exec time (s)
1:0
60.05
29.55
89.60
1:1
53.12
39.36
92.48
1:9
49.30
49.29
98.59
0.07
130
(a)
(b)
180
230
280
330
380
130
0.07 0.1 0.130.160.190.220.250.28
180
230
280
330
380
0.05
0.1
0.15
0.2
0.25
0.3
0.05
0.1
0.15
0.2
0.25
0.3
0.1 0.13 0.160.19
Execution times (S)
Execution times (S)
Expenses ($)
Expenses ($)
Budget ($)
Budget ($)
0.220.250.28
Time0
Expenses0
Time0
Time2
Expenses0
Expenses2
Time1
Time3
Expenses1
Expenses3
Figure 9.3: Query expense and efﬁciency under different budgets, where Time0 & Expense0 under
ratio (0:1), Time1 & Expense1 under ratio (10:1), Time2 & Expense2 under (50:1), Time3 & Expense3
under (90:1): (a) not considering query expenses and (b) considering query expenses.
and (1:9), respectively. Workloads W7 and W8 in Table 9.3 are assigned to execute on
two VMs, respectively. Table 9.6 shows the query response time of the two workloads
under different ratios of !2 : !3.
It can be seen that the ratio of (!2 : !3) inﬂuences the balance of two workloads’
execution time. The more weight given to the third balance optimization objective, the
more balanced performance can be achieved without sacriﬁcing the total execution
time too much, since the latter is also taken as another optimization objective.
9.3.4.6 Adaptability to Different Workloads’ CPU–Memory Requirements
Workloads W9 and W10 get executed on two VMs, where W9 is CPU intensive, and
along with the increase of n in (n:(10-n)) from 1 to 9, W10 changes from a memory-
intensive workload to a CPU-intensive one. So a reasonable resource conﬁguration
is to give it more CPU capabilities and less memory. Figure 9.4 shows the three con-
ﬁguration results for W2 under low, middle, and high budget constraints. They are
coincident with the actual CPU and memory requirements.
Authenticated
:25 PM

234
9 Context Query Efﬁciency Versus Expense
CPU1
CPU2
Memory1
Memory3
CPU3
Memory2
0.25
CPU capacity (GHz)
Memory Size  (GB)
10%
30%
Percentage of Q13 in W2
50% 70% 90%
 0.75
1.25
1.75
2.25
0.25
0.75
1.25
1.75
2.25
Figure 9.4: Adaptive resource conﬁguration,
where CPU1-3 and Memory1-3 correspond to CPU
capabilities and memory sizes under low, middle,
and high budget constraints, respectively.
9.4 Recapitulation
An approach for ﬂexible control of performance and expenses in IaaS cloud en-
vironments with different requirements of customers was presented. It focused on
the workloads with mixed types of queries in database applications. Based on a
ﬁne-grained charging model, the chapter proposed a model of multiple objective
optimization, which covers different aspects cloud customers care about, such as
expenses, performance, cost–performance ratio, the performance tradeoff of applic-
ations on different VMs, etc. Under this model, these complicated problems were
turned into an optimization problem, which could be addressed by a GA we have
implemented. From the results of some typical experiments, it can be seen that the
effectiveness of the approach is signiﬁcant.
Literature
[1]
X. Bu, J. Rao, and C. Z. Xu. CoTuner: A framework for coordinated auto conﬁguration of
virtualized resources and appliances. In Proc. of the Intl. Conf. on Autonomic Computing, 2009.
[2]
D. Florescu and D. Kossmann. Rethinking cost and performance of database systems. ACM
SIGMOD Record, 38(1):43–48, 2009.
[3]
D. Goldberg. Genetic Algorithms in Search, Optimization and Machine Learning, Reading, MA:
Addison-Wesley Professional, 1989.
[4]
T. A. Henzinger, A. V. Singh, V. Singh, T. Wies, and D. Zufferey. Flex- PRICE: Flexible provisioning
of resources in a cloud environment. In Proc. of the IEEE Intl.Conf. on Cloud Computing, 2010.
[5]
D. Kusic, J. O. Kephart, J. E. Hanson, N. Kandasamy, and G. Jiang. Power and performance
management of virtualized computing environments via look ahead control. Cluster Computing,
12(1):1–15, 2009.
[6]
P. Mell and T. Grance. The NIST deﬁnition of cloud computing. Technical Report
doi:10.6028/NIST.SP.800-145, National Institute of Standards and Technology: U.S.
Department of Commerce, September 2011.
Authenticated
:25 PM

Literature
235
[7]
J. More. The Levenberg-Marquardt algorithm: Implementation and theory. In Numerical
Analysis, pages 105–116, 1978.
[8]
P. Padala, K. Y. Hou, K. G. Shin, et al. Automated control of multiple virtualized resources. In
Proc. of the ACM European Conf. on Computer Systems, 2009.
[9]
J. Rao, X. Bu, C. Z. Xu, L. Wang, and G. Yin. VCONF: A reinforcement learning approach to virtual
machines auto-conﬁguration. In Proc. of the Intl. Conf. on Autonomic Computing, 2009.
[10]
J. Rogers, O. Papaemmanouil, and U. Cetintemel. A generic auto provisioning framework for
cloud databases. In Proc. of ICDE Workshop, 2010.
[11]
P. Shivam, A. Demberel, P. Gunda, et al. Automated and on-demand provisioning of virtual
machines for database applications. In Proc. of ACM SIGMOD, 2007.
[12]
G. Somani and S. Chaudhary. Application performance isolation in virtualization. In Proc. of the
IEEE Intl. Conf. on Cloud Computing, 2009.
[13]
A. A. Soror, U. F. Minhas, A. Aboulnaga, et al. Automatic virtual machine conﬁguration for
database workloads. In Proc. of ACM SIGMOD, 2008.
[14]
R. Urgaonkar, U. C. Kozat, K. Igarashi, and M. J. Neely. Dynamic resource allocation and power
management in virtualized data centers. In Proc. Of NOMS, 2010.
[15]
X. Wang and Y. Wang. Co-con: Coordinated control of power and application performance for
virtualized server clusters. In Proc. of the Intl. Workshop on Quality of Service, 2009.
[16]
P. Xiong, Z. Wang, G. Jung, and C. Pu. Study on performance management and application
behavior in virtualized environments. In Proc. of NOMS, 2010.
Authenticated
:25 PM

Authenticated
:25 PM

10 Context-Aware Preference Querying
Abstract: Users’ preferences have traditionally been exploited in query personaliza-
tion to better serve their information needs. With the emerging ubiquitous computing
technologies, users will be situated in an ambient intelligent (AmI) environment,
where users’ database accesses will not occur at a single location in a single context
as in the traditional stationary desktop computing, but rather span a multitude of
contexts like ofﬁce, home, hotel, plane, etc. To deliver personalized query answering
in this environment, the need for context-aware preferences arises accordingly. This
chapter examines two lines of research on context-aware preference querying, namely,
qualitative and quantitative in general. A quantitative way to measure the impact of
context upon query results is then illustrated. A knowledge-based way to qualitatively
model users’ context-aware query preferences is also presented, and database query
results are ranked according to the weights of preference rules and degrees of context
uncertainty.
10.1 Query Preferences in Databases
Users’ preferences have traditionally been exploited in query personalization to better
serve their information needs. The notion of preference query was ﬁrst introduced
to the database area in the work [14]. It extended the domain relational calculus to
express preferences for tuples satisfying certain logical conditions. Recently, situated
and contextual preferences start to receive attention because user preferences may
only hold under certain situations [23]. Two typical ways exist to represent and deal
with user preferences, namely, quantitative and qualitative [4].
10.1.1 Qualitative Representation of Preferences
The qualitative approach intends to directly specify preferences between the tuples
in the query answer, typically using binary preference relations. A preference relation
example is “Prefer one book tuple to another if and only if their ISBNs are the same
and the price of the ﬁrst is lower.”
These kinds of preference relations can be embedded into relational query lan-
guages through relational operators or special preference constructors, which se-
lect from its input the set of the most preferred tuples (e.g., winnow operator [4],
PreferenceSQL [11], skyline queries [3]). For example, a preference of white wine over
red when ﬁsh is served, and red wine over white when meat is served over a database
DOI 10.1515/9783110556674-010
Authenticated
:27 PM

240
10 Context-Aware Preference Querying
relation Meal(Dish,DishType, Wine,WineType) can be qualitatively expressed
in Ref. [4] as follows:
(d,dt,w,wt) ≻(d’,dt’,w’,wt’) ≡
(d=d’ ∧dt=“fish” ∧wt=“white” ∧dt’=“fish” ∧wt’=“red”)∨
(d=d’ ∧dt=“meat” ∧wt=“red” ∧dt’=“meat” ∧wt’=“white”)
Another qualitative representation of the example preference red or blue cars
and (equally important) maximal fuel_economy is via the preference
XPATH [12] as follows:
/CARS/CAR #[ (@color) in (‘‘red’’,‘‘blue’’)
and (@fuel_economy) maximal ]#
It involves the composition of preferences through the and operator.
The ER model was used in Ref. [9] to model situation. Each situation has an id
and consists of a temporal timestamp and a location. Each situation is linked with
one or more uniquely identiﬁed preferences through an m:n relation. An XML-based
preference repository was explored to store and manage these situated preferences.
10.1.2 Quantitative Representation of Preferences
The quantitative approach expresses preferences using scoring functions, which as-
sociate a numeric score with every tuple of the query. Then tuple t1 is preferred
to tuple t2, if and only if the score of t1 is higher than the score of t2. A frame-
work for expressing and combining such kinds of preference functions was provided
in Ref. [1]. Koutrika and Ioannidis [13] presented a richer preference model that
can associate degrees of interest (like scores) with preferences over a database
schema.
With the proliferation of ubiquitous computing which integrates information
processing with everyday activities, it is indispensable to provide effective data man-
agement support which is affected by preferences and needs of different users under
different context. Therefore, context-aware preference querying techniques are in
great demand. Stefanidis et al. [22, 24] also took context as any attribute that is
not part of the database schema and focused on the hierarchical nature of contex-
tual attributes (such as street, city, and country for the location attribute).
The proposed contextual preference consists of a context state, a predicate, and
a score. The predicate speciﬁes conditions on the values of attributes that hold
in the context state. As an example of a preference in the model, the preference
(((id1,youth,male), friends), (genre=thriller),0.9) denotes that the user
with identiﬁer id1, who is a young male, when accompanied with friends, enjoys
to see movies of thriller genre.
Authenticated
:27 PM

10.2 Implanting Context-Aware Query Preferences upon an RDBMS
241
10.2 Implanting Context-Aware Query Preferences
upon a Relational Database Management
System (RDBMS)
With the anytime and anywhere ubiquitous data access paradigm, users’ data ac-
cesses will not occur at a single location as in the traditional stationary desktop
computing, but rather span a multitude of context like ofﬁce, home, hotel, plane, etc.
To deliver the right personalized query answering in this environment, it is important
to take users’ situated context into consideration.
This section investigates users’ query preferences in context. A knowledge-based
context-aware query preference model, catering for both pull and push queries, is
given. The model is further interpreted in the domain of relational databases, and im-
planted on top of a traditional database management system (DBMS) to demonstrate
its applicability and feasibility.
10.2.1 A Knowledge-Based Context-Aware Preference Model
A context-aware query preference states, among a set of alternatives, a particular like
or dislike for some of these alternatives under certain speciﬁc context, like prefer-
ring sport TV programs when dining, prefer TV programs of human_interest genre while
doing some free time activity with some friend(s) around, etc.
10.2.1.1 Design Guidelines
The design of the context-aware preference model is inﬂuenced and guided by the
following criteria.
Smartness requirement. The smartness requirement implies reasoning and
learning capabilities that the preference model must possess, calling for an inevitable
knowledge ingredient. For example, a user may input a preference like preferring a
nearby restaurant when the weather is bad. With the model, it should be able to infer
the applicability of the preference no matter whether it is raining or snowing, since
both are bad weather.
Proactiveness requirement. Following the smartness requirement, database
systems should proactively deliver useful information to their users at anytime and
anywhere. Besides the traditional pull access mode, where users actively query
databases to pull relevant information, the push access mode, where the system
pushes proactively possibly relevant information to users, shall be accommodated.
The context-aware query preference model must be ﬂexible and easy enough to assist
both types of access modes.
Closure requirement. Preferences and applicable context should be represented
uniformly and the model should have the closure property, so that the output pref-
erence could serve as the input context of other preferences. Suppose a user has two
Authenticated
:27 PM

242
10 Context-Aware Preference Querying
preferences: “when I have a bad mood, I like cheerful TV programs” and “when I want
cheerful TV programs, I usually like channel 5.” In this case, when the user has a bad
mood, cheerful TV programs on channel 5 will be the most preferable programs for
the user.
Scalability requirement. Performance is highly demanded at the data manage-
ment level to process real-time queries raised by different users at anytime and any-
where. The context-aware preference model must easily be interpreted and executed
in the database world in order to achieve scalability.
Traceability requirement. The behaviors of database querying systems and thus
the preference model should be traceable by the users. In other words, it should be
possible for a human to enter, view, and edit context-aware preferences in a way which
is close to the world model of the users. As a consequence, an intuitive friendly user
interface for preference declaration is needed.
10.2.1.2 Modeling Context-Aware Preferences in Descriptive Logistics
Following the design guidelines, the context-aware query preference model shall
tightly couple a preference with its applicable context and express both in a uni-
form way.
In a similar fashion as context, DL (description logics) concept expressions are
exploited to express context-aware query preferences, since DL concept expressions
offer a natural way to represent information requests. For instance, the DL concept
expression TvProgram⊓(∃hasGenre.{HUMAN_INTEREST}) can be viewed as a
query which selects all TvProgram instances having the HUMAN_INTEREST genre.
Deﬁnition 21. A context-aware query preference is a tuple of the form (Context,
Preference), where Context and Preference are DL concept expressions.
◻
A user’s context-aware query preference example like “Peter prefers TV programs of
human_interest genre while doing some free time activity with some friend(s) at the
same room” can be stated as:
Context:
{PETER} ⊓(∃hasActivityType.FreeTimeActivity) ⊓
(∃hasFriend.(∃hasRoom.(∃roomOf.{PETER})))
Preference:
TvProgram ⊓(∃hasGenre.{HUMAN_INTEREST})
When a preference is applicable to any context, Context=⊤.
In comparison with the above context-aware query preference example, where the
preferred genre of TV programs is a constant HUMAN_INTEREST, sometimes, a user’s
preference varies with the concrete context. For instance, “while with some friend(s),
PETER prefers TV programs of the common genre interest.” In this case, the preferred
Authenticated
:27 PM

10.2 Implanting Context-Aware Query Preferences upon an RDBMS
243
genres of TV programs depend on whom the user is with at that moment. A variable v
denotes it in the following notation:
Context:
{PETER} ⊓(∃hasFriend.((∃hasRoom.(∃roomOf.{PETER}))⊓
v))
Preference:
TvProgram ⊓(∃hasGenre.((∃tvInterestOf.{PETER}) ⊓
(∃tvInterestOf.v)))
This kind of preferences is called a variable context-aware query preference, and the
former a deterministic context-aware query preference.
10.2.2 Explicating Context-Aware Preferences in a Database World
To integrate context-aware preferences with database queries, there needs a way to ex-
plicate context-aware query preferences (i.e., Context and Preference DL concept
expressions in description logics) in a database world, speciﬁcally to relate Context
and Preference DL concept expressions to database tables. For the Preference
notion, this is a rational translation, since it affects the query. For the Context, it
is less evident, since dynamic context (e.g., location, surrounding people, etc.) must
be acquired real time from external sources or services like sensor networks. This is in
line with the efforts of the sensor network community which has embraced declarative
queries as a key programming paradigm for large sets of sensors [5, 15]. Furthermore,
logging context for later analysis purpose also implies having an underlying storage
system that provides this view.
10.2.2.1 Related Work on Mapping DL to Databases
Previous mappings of DL to databases include work [10], which described a DL
instance store to provide an infrastructure for reasoning with a large number of
instances. Borgida and Brachman [2] described a method for loading data in DL
reasoners, where concept and role views were introduced to process DL queries over
the database. Transformation of DL to other types of databases (such as deductive
and object-oriented databases) has been done as well in Refs. [21, 25]. The following
mapping adopts the technique developed in Ref. [2].
10.2.2.2 Mapping DL Concepts and Roles to Database Tables
The basic elements of DL are concepts and role. A DL concept can be viewed as a
table with the concept name as the table name. The table contains one ID attribute.
The tuples of the table contains all the DL individuals of the concept. A virtual table
Authenticated
:27 PM

244
10 Context-Aware Preference Querying
PETER
ERIC
ERIC
ERIC
PETER
SLEEPING
READING
PLAYPIANO
PETER
PETER
MAARTEN
ERIC
PETER
MAARTEN
COFFEEROOM
ROOM3061
ROOM3061
PETER
DESTINATION
DESTINATION
SOURCE
hasGenre
hasFriend
hasActivityType
Person
Room
TvProgram
Relaxing
hasRoom
SOURCE
DESTINATION
DESTINATION
SOURCE
SOURCE
ID
ID
ID
ID
ERIC
MAARTEN
OPRAH
24
VOYAGER
SCIFI
THRILLER
HUMAN_INTEREST
...
...
...
...
...
MAARTEN
(a)
(b)
ROOM3061
OPRAH
READING
SLEEPING
PLAYPIANO
24
ROOM4061
COFFEEROOM
VOYAGER
...
...
...
...
...
...
...
Figure 10.1: DL concept (a) and role (b) tables.
named TOPTABLE corresponds to all the individuals in the domain. Similarly, each
DL role can be viewed as a table, with the role name as its table name. The table has
two attributes SOURCE and DESTINATION. For each tuple of the table, the role relates
the SOURCE individual with the DESTINATION individual. Figure 10.1 gives a few DL
concept and role table examples.
10.2.2.3 Mapping DL Concept Expressions to Structured Query Language
Statements
Table 10.1 shows the way to translate a DL concept expression into a corresponding
Structured Query Language (SQL) query statement, as adapted from Ref. [2].
With the mapping mechanism in Table 10.1, DL concept expressions Context
and Preference can be rewritten into SQL queries. In other words, SQL can be taken
as a uniform interface to various context and preference.
Example 31. The Context expression in the following context-aware preference:
Context:
{PETER} ⊓(∃hasFriend.(∃hasRoom.(∃roomOf.{PETER})))
Preference:
TvProgram ⊓(∃hasGenre.{HUMAN_INTEREST})
Authenticated
:27 PM

10.2 Implanting Context-Aware Query Preferences upon an RDBMS
245
Table 10.1: Mapping DL concept expressions to SQL statements, where C, D, E are DL concepts, R is a
DL role, and a is a DL individual (adapted from Ref. [2]).
DL concept expression
Corresponding SQL statement
C
SELECT ID FROM C
a
VALUES(’a’)
⊤
SELECT ID FROM TOPTABLE
⊥
NULL
¬D
(SELECT ID FROM TOPTABLE) EXCEPT (SELECT ID FROM D)
D ⊓E
(SELECT ID FROM D) INTERSECT (SELECT ID FROM E)
D ⊔E
(SELECT ID FROM D) UNION (SELECT ID FROM E)
∃R.D
SELECT R.SOURCE FROM R WHERE R.DESTINATION
IN (SELECT ID FROM D)
∀R.D
(SELECT ID FROM TOPTABLE) EXCEPT
(SELECT R.SOURCE FROM R WHERE R.DESTINATION
IN ((SELECT ID FROM TOPTABLE) EXCEPT
(SELECT ID FROM D)))
can be translated into the SQL query:
VALUES(’PERTER’)
INTERSECT
(SELECT hasFriend.SOURCE FROM hasFriend
WHERE
hasFriend.DESTINATION IN
(SELECT hasRoom.SOURCE FROM hasRoom
WHERE
hasRoom.DESTINATION IN
(SELECT roomOf.SOURCE FROM roomOf
WHERE
roomOf.DESTINATION IN (VALUES(’PETER’)))
))
which can then be optimized into
SELECT * FROM hasFriend, hasRoom, roomOf
WHERE
hasFriend.SOURCE=’PETER’ AND
hasFriend.DESTINATION=hasRoom.SOURCE AND
hasRoom.DESTINATION=roomOf.SOURCE AND
roomOf.DESTINATION=’PETER’
◻
10.2.3 Personalized Querying with Context-Aware Preferences
Context-aware preferences can assist two kinds of database accesses: pull and push.
1) In the pull access mode, context-aware preferences are used for query augment-
ation (e.g., enforcing the query constraint genre=“HUMAN_INTEREST” to the user’s
query over TvProgram when s/he is with some friends in the same room).
Authenticated
:27 PM

246
10 Context-Aware Preference Querying
2) In the push access mode, context-aware preferences are used as query triggers
(e.g., showing TV programs of the HUMAN_INTEREST genre when the user is with
some friend(s)).
10.2.3.1 The Enhanced Database Query Framework
Figure 10.2 shows the pull-push query execution framework equipped with context-
aware preferences. It contains six major components.
1) The context acquisition is responsible for acquiring the current query context.
Some static context information such as user’s background, friends, TV programs,
etc., can be obtained from the context database; while some dynamic context such as
user’s location, people nearby, etc., can be obtained from sensors or external service
providers.
2) The preference selector selects from the preference repository applicable
context-aware query preferences, if necessary by reasoning. The preference selector
selects from the preference repository relevant context-aware query preferences, if
necessary by reasoning. A context-aware preference (Context, Preference) is
relevant if it is applicable in the current situation and the preference can be mapped
to some database tables included in the user’s query (pull query) or included in the
database (push query).
Context
Context
User
[q]
Context acquisition
Context
database
Preference
repository
Preference manager
Preference miner
Preference selector
Query trigger
(push mode)
Query adapter
(push mode)
Database
Contextual ranking
Access log
r
[q´]
qtrigger
Figure 10.2: The preference aware pull–push query framework.
Authenticated
:27 PM

10.2 Implanting Context-Aware Query Preferences upon an RDBMS
247
3) In the pull mode, query adapter augments user’s query q with the relev-
ant preference(s), and optimizes it further into q󸀠. In the push mode, query trigger
proactively generates a query qtrigger according to the preference(s), and sends it to the
underlying DBMS to execute.
4) User–system interactions are recorded in the access log, from which prefer-
ence miner can discover users’ context-aware preferences. Users can also directly
input their context-aware preferences.
5) Preference manager is responsible for storing, maintaining, and managing
users’ context-aware preferences.
For a context-aware preference (Context,Preference) expressed in DL, the
translated SQL query from the context expression is executed over the context
database. A nonempty query result implies that the current context is consistent
with Context. The associated Preference is then activated for either query adapt-
ation or query trigger, where Preference is translated into an SQL statement as a
proactive push query.
10.2.3.2 Augmenting Pull Queries with Context-Aware Preferences
Pull queries, augmented with relevant context-aware preferences, focus on the deliv-
ery of user-preferred query results.
Example 32. Suppose a user raises a pull query for TV programs when she/he is with
some friend(s).
(Pull Query)
SELECT ID FROM Document
User’s following context-aware preference rule contains a concept TvProgram
which appears in the original SQL query.
Context:
{PETER} ⊓(∃hasFriend.(∃hasRoom.(∃roomOf.{PETER})))
Preference:
TvProgram ⊓(∃hasGenre.{HUMAN_INTEREST})
To determine whether the preference is relevant, the following concept expression
Context is translated straightforwardly into a context query:
(Context Query)
VALUES(’PERTER’)
INTERSECT
(SELECT hasFriend.SOURCE FROM hasFriend
WHERE
hasFriend.DESTINATION IN
(SELECT hasRoom.SOURCE FROM hasRoom
WHERE
hasRoom.DESTINATION IN
(SELECT roomOf.SOURCE FROM roomOf
WHERE
roomOf.DESTINATION IN (VALUES(’PETER’)))
))
Authenticated
:27 PM

248
10 Context-Aware Preference Querying
It can then be further simpliﬁed into
(Context Query)
SELECT hasFriend.SOURCE
FROM
hasFriend, hasRoom, roomOf
WHERE
hasFriend.SOURCE=’PETER’ AND
hasFriend.DESTINATION=hasRoom.SOURCE AND
hasRoom.DESTINATION=roomOf.SOURCE AND
roomOf.DESTINATION=’PETER’
When this context query returns a nonempty result, the original pull query is augmen-
ted with the additional constraint Preference in the WHERE clause:
(Pull Query)
SELECT ID FROM TvProgram WHERE ID IN
(SELECT hasGenre.SOURCE FROM hasGenre
WHERE hasGenre.DESTINATION IN (VALUES
(’HUMAN_INTEREST’)))
◻
10.2.3.3 Triggering Push Queries with Context-Aware Preferences
A relevant context-aware preference can act as a push query trigger.
Example 33. Consider a preference “Query TV programs of HUMAN_INTEREST when
Peter enters room 3061”.
Context:
{PETER} ⊓∃hasRoom.{ROOM3061}
Preference:
TvProgram ⊓∃hasGenre.{HUMAN_INTEREST}
It triggers a DB2 query trigger:
(Push Query)
CREATE TRIGGER queryTvProgram AFTER INSERT ON hasRoom
REFERENCING NEW AS n FOR EACH ROW
WHEN (n.SOURCE IN VALUES(’PETER’)) AND
(n.DESTINATION IN VALUES(’ROOM3061’))
SELECT TvProgram.ID
FROM
TvProgram, hasGenre
WHERE
TvProgram.ID=hasGenre.SOURCE AND
hasGenre.DESTINATION=’HUMAN_INTEREST’
It states that when Peter enters the room, the TV programs of genre HUMAN_
INTEREST will be shown.
◻
Authenticated
:27 PM

10.3 Contextual Ranking of Database Querying Results
249
10.2.3.4 Discussion
Revisiting the challenges raised by AmI, the presented context-aware preference
model takes a knowledge-based approach, which facilitates reasoning (smartness
requirement). The natural correspondence between DL concept expressions and
data requests determines the ﬂexibility of the model for both pull and push quer-
ies (proactiveness requirement). Preferences and associated applicable contexts are
treated uniformly using DL concept expressions (closure requirement). Interpreting
the knowledge-based preference model into a database world enables to address
the scalability requirement. The traceability requirement is achieved through user-
deﬁned preferences. The model can be implanted on top of a DBMS to provide person-
alized query answers and trigger proactive personalized information delivery. Further
investigation of the uncertainty issue due to the imprecise context measurement and
its impact on the context-aware query preference model is desirable.
10.3 Contextual Ranking of Database Querying
Results
Contextual ranking of relational database tuples and query results according to how
well they match user preferences offers a way to make the right information avail-
able to the right user under the right context. To quantitatively measure the impact
of context upon database tuples and query results, statistic correlations between con-
textual attributes and database attributes are to be identiﬁed. The regression models
developed in the statistics ﬁeld can be employed to approximate contextual rank-
ing functions with contextual attributes and their inﬂuential database attributes as
parameters.
10.3.1 A Motivation Example
Example 34. Table 10.2 shows a sun protection product relation with attributes
TupleId, Name, Sun Protection Factor (SPF), Net Weight, and Price, and Table 10.3
shows some user-related context information, including monthly income and
Ultraviolet Index (UVI) of the place where the user is located, together with some
Table 10.2: The Product relation.
TupleID
Name
SPF
NetWT (ml)
Price (USD)
t1
EL Cyber White
50
50
47
t2
AVON SUN
40
50
9.99
t3
Biotherm Sunﬁtness
15
125
21.50
t4
CD Bronze
15
150
26
t5
Sisley Sunleya
15
50
180
Authenticated
:27 PM

250
10 Context-Aware Preference Querying
Table 10.3: Contextual ranking of Product tuples.
ContextInstanceID
Income
UVI
t1
t2
t3
t4
t5
u1
5,000
10 (Honolulu)
0.190
0.100
0.088
0.092
0.050
u2
3,000
2 (Seattle)
0.362
0.259
0.305
0.353
0.064
u3
8,000
1 (Chicago)
0.150
0.107
0.138
0.147
0.099
u4
6,500
7 (Miami)
0.228
0.110
0.096
0.100
0.062
ranking scores that different users assigned to different products. For instance, when
a user is at a place, say Honolulu, with a high UVI, s/he will highly rank the sun
protection cream with a high SPF.
◻
10.3.2 Database and Context Space
Let R(A1, A2, . . . , Am) be a database relation schema containing m attributes Att =
{A1, A2, . . . , Am} with domain Dom(A1), Dom(A2), . . . , Dom(Am), respectively. A rela-
tion of the relation schema R(A1, A2, . . . , Am), denoted by r(R), is a set of m-tuples
r(R)
=
{t1, t2, . . . , ts}. Each m-tuple t in r(R) is an ordered list of m values t
=
⟨t.A1, t.A2, . . . , t.Am⟩, where each value t.Ai ∈Dom(Ai) (1 ≤i ≤m).
In a similar fashion, an n-dimensional context space CS(C1, C2, . . . , Cn) contains
n contextual attributes CAtt = {C1, C2, . . . , Cn}, with domain Dom(C1), Dom(C2), . . . ,
Dom(Cn), respectively. A context in the context space, denoted by c(CS), consists of
a set of context instances c(CS) = {u1, u2, . . . , us}. Each n-tuple u is an ordered list of n
values u = ⟨u.C1, u.C2, . . . , u.Cn⟩, where each value u.Cj ∈Dom(Cj) (1 ≤j ≤n).
Database attributes are distinguished from contextual attributes in that: database
attributes (such as Price and NetWT) are subject oriented and inherent characteristics
of database entity product, while contextual attributes (such as Income and UVI) are
more user/environment related whose values vary under different query context.
10.3.3 Ranking Database Tuples Under Context Instances
Deﬁnition 22. Let t = ⟨t.A1, t.A2, . . . , t.Am⟩∈r(R) be a tuple in relation r(R), and let
u = ⟨u.C1, u.C2, . . . , u.Cn⟩∈c(CS) be a context instance in context c(CS). Contextual
ranking of database tuple t under context instance u is deﬁned as a function:
f(t, u) = s ∈[0, 1], which is also represented as
f(t.A1, t.A2, . . . , t.Am, u.C1, u.C2, . . . , u.Cn) = s ∈[0, 1].
◻
Deﬁnition 23. Let t, t󸀠∈r(R) be two tuples in relation r(R), and let u ∈c(CS) be a
context instance in context c(CS). Database tuple t is preferred to t󸀠under context
instance u, if and only if f(t, u) ≥f(t󸀠, u), that is,
f(t.A1, . . . , t.Am, u.C1, . . . , u.Cn) ≥f(t󸀠.A1, . . . , t󸀠.Am, u.C1, . . . , u.Cn).
◻
With the contextual ranking function, query results can be ranked accordingly.
Authenticated
:27 PM

10.3 Contextual Ranking of Database Querying Results
251
Deﬁnition 24. Given a database query q over relation r(R) under context instance
u ∈c(CS), let Tq ⊆{t1, t2, . . . , tq} be a set of tuples satisfying the query condition of
q without loss of generality. Contextual ranking of query result Tq is deﬁned as a
function: CQ(Tq, u) = {(ti, si) | (ti ∈Tq) ∧(si = f(ti, u))}
◻
Example 35. Suppose a query is issued to Table 10.2 under context instance u3 to
request sun products where (NetWT < 100), with {t1, t2, t5} as the result tuples.
According to Table 10.3, f(t1, u3) = 0.150, f(t2, u3) = 0.107, f(t5, u3) = 0.099. Thus,
CQ(Tq, u3) = {(t1, 0.150), (t2, 0.107), (t5, 0.099)} is the ranked query result.
◻
10.3.4 Building Contextual Ranking Functions by Regression
The establishment of an appropriate ranking function relies on users’ subjective
knowledge within a speciﬁc problem domain. Based on users’ sample input prefer-
ences (measured in scores) to database tuples under different context instances, the
contextual ranking function f, which is later used to predict user inclination to tuples
under future (un)known context, can then be derived by regression.
In fact, a wide range of studies in practical applications encounter the problem
of reconstructing an unknown function f from a ﬁnite set of discrete data. Regres-
sion analysis is one of the most widely used methods of investigating the statistical
relationship between a set of predictor variables and a response variable [20]. It is
helpful when the predictor variables cannot be controlled, particularly when they are
sampled in an observational study. The form of the function that is thought to relate
the response variable to a set of predictor variables can be speciﬁed initially by the ex-
perts in the area of study based on their knowledge or their objective and/or subjective
judgments. The hypothesized function can then be either conﬁrmed or refuted by the
analysis of the collected data.
10.3.4.1 Problem Generalization
Building a contextual ranking function can thus be generalized to approximate a func-
tion of several to many variables, given only the values of the function at various
points in a dependent variable space. The goal is to model the dependence of a re-
sponse variable score y on predictor variables x1, x2, . . . , xr as y = f(x1, x2, . . . , xr) +
:, where : is the independent and identically distributed random error following
N(0, 32).
10.3.4.2 Regression Models in Statistics
In statistics, regression analysis includes many techniques for modeling and ana-
lyzing several variables. Three regression models, i.e., the generalized linear model
Authenticated
:27 PM

252
10 Context-Aware Preference Querying
(GLM) [16], polynomial regression [6], and multivariate adaptive regression splines
(MARS) [7], are exploited to learn the contextual ranking function.
(1) Linear Regression Model
The linear regression model with a single response takes the form
y = "0 + "1x1 + ⋅⋅⋅+ "rxr + :.
With n independent observations on y and the associated regressor variable xij
(the ith observation of variable xj where 1 ≤j ≤r), the complete model is
y1 = "0 + "1x11 + "2x12 + ⋅⋅⋅+ "rx1r + :1
y2 = "0 + "1x21 + "2x22 + ⋅⋅⋅+ "rx2r + :2
...
...
...
...
yn = "0 + "1xn1 + "2xn2 + ⋅⋅⋅+ "rxnr + :n
where the error terms are assumed to have the following properties:
–
E(:j) = 0;
–
Var(:j) = 32;
–
Cov(:j, :k) = 0 where j
̸= k.
In matrix terms, y = X" + :, where
y = (
y1
y2
...
yn
) ,
X = (
x11 ⋅⋅⋅x1r
x21 ⋅⋅⋅x2r
...
...
...
xn1 ⋅⋅⋅xnr
) ,
" = (
"1
"2
...
"n
) ,
: = (
:1
:2
...
:n
) .
(2) Polynomial Regression Model
The linear model previously considered assumes a linear relationship between the
predictor variables and the response variable. This restriction excludes many non-
linear mathematical forms. Polynomial regression model extends the simple linear
regression to represent the relationship between the independent variables xs and the
dependent variable y as an nth order polynomial.
The second-degree polynomial model in two variables is like
y = "0 + "1x1 + "2x2 + "1x2
1 + "2x1x2 + "2x2
2 + :.
(3) MARS-based Regression Model
MARS is a form of regression analysis introduced by Friedman in 1991 [7]. It is a non-
parametric regression technique and can be seen as an extension of linear models that
automatically models nonlinearities and interactions between variables in a piecewise
regression.
Authenticated
:27 PM

10.3 Contextual Ranking of Database Querying Results
253
MARS builds models of the form y = ∑M
i=1 ciBi(x), where each ci is a constant coef-
ﬁcient and each Bi(x) is a basic function. Each basic function Bi(x) takes one of the
following three forms:
–
A constant 1.
–
A hinge function of the form max(0, x-const) or max(0, const-x), where const is a
constant, called knot. As a hinge function is zero for part of its range, it can be
used to partition the data into disjoint regions, each of which can be treated inde-
pendently. MARS automatically selects variables and values of those variables for
knots of the hinge functions.
–
A product of two or more hinge functions. These basis functions can model
interaction between two or more variables.
MARS builds the model in two phases: the forward and the backward phases.
Implementation details can be founded in Ref. [18].
10.3.4.3 Performance
To examine the effectiveness of the three regression models (i.e., GLM, polynomial re-
gression, and MARS) in learning the contextual ranking function, two experiments are
performed on a computer with Intel Core Duo CPU L7500 1.60 GHz and 2G of Memory,
running the Windows Vista operation system.
(1) Performance Metric
The deviance residual [17, 19] is used to measure how good the regression models ﬁt
the data. For the ith observation, the deviance residual is deﬁned as rD
i = rD( yi, ̂,i) =
sign( yi –
̂,i)√wi ⋅d( yi, ̂,i), where
–
sign(x): function sign(x)=1 for x > 0, or sign(x)=-1 for (x < 0),
–
̂,i: the ﬁtted value corresponding to the ith observation,
–
wi: the weight, which is 1 in the experiment, and
–
d( yi, ̂,i): the unit deviance.
Thus, the total deviance residual value is ∑n
i=1 rD
i .
(2) Experimental Data
Taking the motivation example in Section 10.3.1 for reference, assume the contextual
attribute UVI and the Product’s attribute SPF have the ranking correlation. Let t and u
be a database tuple and a context instance, respectively.
Synthetic Data. The ﬁrst synthetic data set follows the contextual ranking func-
tion f(t.SPF, u.UVI) = –|t.SPF – k ∗u.UVI + a| ∗0.935 + 100.311, where k = 7.667 and
a = –15 in the test, the values of t.SPF and u.UVI are uniformly generated in the range
of [15, 130] and [0, 15], respectively, with some Gaussian random noises added. In total,
Authenticated
:27 PM

254
10 Context-Aware Preference Querying
(a)
(b)
0
3
120
120 90
60
30
90
60
SPF
SPF
30
6
9
12
15
0
UVI
UVI
3
6
9
12
15
.000
.250 .500.750 1.000
.300
Score
.600.900
Figure 10.3: Scatterplot of two synthetic data sets: (a) synthetic data set 1 and (b) Synthetic data set 2.
10k records of the form (t.SPF, u.UVI, f(t.SPF, u.UVI)) are generated. Figure 10.3(a)
demonstrates synthetic data set 1.
The second synthetic data set follows the contextual ranking function
f(t.SPF, u.UVI) =
5
|t.SPF – k ∗u.UVI + a|, where k = 7.667 and a = –15 in the test, the
values of t.SPF and u.UVI are generated in the same way as in Dataset 1. Totally 10k re-
cords are generated. Figure 10.3(b) shows uniformly sampled 200 records of synthetic
data set 2.
Real Data. Ten users are invited to rank different product tuples under differ-
ent context instances, and produce 20(record) * 10(people) = 200 records of the
form (t.SPF, u.UVI, score) for learning and testing the contextual ranking function.
Figure 10.4 plots the surveyed result in a three-dimensional space with u.UVI as x-axis,
t.SPF as y-axis, and score as z-axis.
For both synthetic and real data sets, 80% of each are used to learn the contextual
ranking function by regression, while 20% for testing purpose.
(3) Experimental Results
Experimental results on both synthetic data and real data demonstrate the feasibil-
ity of regression models for learning contextual ranking functions, as illustrated in
Figure 10.5. MARS and polynomial regression behave more robust than GLM. Among
the three, MARS produces the least absolute value of median of deviance residuals,
and is particularly ﬁt for approximating the contextual ranking function when the
120 90 60 30
0
SPF
0
UVI
3
6
9
12
15
.25
.00
.50 .75
Figure 10.4: Scatter plot of real data.
Authenticated
:27 PM

10.3 Contextual Ranking of Database Querying Results
255
0.00
GLM
Abstract value of median
of deviance residuals
Abstract value of median
of deviance residuals
Polynomial
regression
Regression models
MARS
GLM
Polynomial
regression
Regression models
MARS
0.02
0.04
0.06
0.08
0.010
0.008
0.006
0.004
0.002
0.000
(b)
(a)
Dataset 1
Dataset 2
Figure 10.5: Absolute value of median of deviance residuals on synthetic (a) and real (b) data.
relationship among contextual attributes, database attributes, and ranking score is
unknown.
10.3.5 Reducing Context Dimensionality in Contextual Ranking
10.3.5.1 Motivation
To learn an appropriate contextual ranking function, it is also desirable to reduce the
dimensionality of context space ﬁrst due to the following reasons:
(1) In many real situations, a contextual attribute is inherently correlated to some
speciﬁc database attribute. For example, users at the areas with very high ultravi-
olet radiation levels are usually more interested in high SPFs provided by sunscreen
lotions.
(2) Some contextual attributes play dominant roles in producing accurate predic-
tion within a reasonable cost.
(3) Data collection usually leads to an information overload. Working with fewer
context dimensions can not only increase computational efﬁciency but also make
it easier to identify the most informative association patterns among contextual
attributes and database attributes.
(4) Correlations among contextual attributes and database attributes tend to be
obscure in certain complex situations. It is not easy to manually select contextual
attributes which affect the ranking of query results.
10.3.5.2 Bijective Database–Contextual Attribute Pairs
Two assumptions are made related to contextual ranking functions.
(1) Contextual attribute set and database attribute set have a bijective relationship,
which is prespeciﬁed by users.
(2) The bijective association between a contextual attribute and a database
attribute has a monotonic property.
Authenticated
:27 PM

256
10 Context-Aware Preference Querying
Deﬁnition 25. Contextual attribute set C and database attribute set A have a biject-
ive relationship, if and only if every element Cj ∈C has a certain relationship with
only one corresponding element Ai ∈A, and vice versa. A bijective pair of database
attribute Ai and contextual attribute Cj is denoted as (Ai, Cj).
◻
With the above bijective relationship assumption, the examination of context inﬂu-
ence upon database query results is translated into individually examining the effect
of each contextual attribute upon its corresponding bijective database attribute.
Deﬁnition 26. Let (Ai, Cj) be a bijective pair, where (Ai ∈A) and (Cj ∈C). (Ai, Cj)
is a monotonic bijective database–contextual attribute pair, if and only if for
∀t, t󸀠∈r(R) and ∀u, u󸀠∈c(CS), f(t, u) ≤f(t󸀠, u󸀠) subject to either of the following
two conditions:
–
if (t.Ai ≤t󸀠.Ai), then (u.Cj ≤u󸀠.Cj); or
–
if (u.Cj ≤u󸀠.Cj), then (t.Ai ≤t󸀠.Ai).
◻
Example 36. Referring to Tables 10.2 and 10.3, the product attribute SPF and contex-
tual attribute UVI is a monotonic bijective pair: the higher the value of contextual
attribute UVI is, the higher the value of product’s SPF is preferred by users, leading to
a higher rank value assigned to the corresponding product tuple.
◻
Therefore, selection task of contextual attributes degrades to search for a subset of
bijective contextual and database attribute pairs according to a certain criterion. With
each state in the search space specifying a subset of bijective pairs, the search space
is O(2N), where N is the number of bijective contextual and database attribute pairs.
When N is large, exhaustive search will be computationally prohibitive.
10.3.5.3 Impact of a Bijective Monotonic Database–Contextual Attribute Pair
For each bijective monotonic attribute pair, the impact of the contextual attribute
upon its paired database attribute can then be quantitatively deﬁned. Some notations
are ﬁrst introduced for ease of explanation.
Deﬁnition 27. Given a list of comparable values L = ⟨l1, l2, . . . , lx⟩, a position list
function over L, denoted as Posi(L) = L󸀠, maps L to a new list L󸀠, whose elements
are nonnegative integers, indicating the ordering positions of the elements in L. The
position of element li of L is deﬁned as posi(li, L), i.e.,
L󸀠= Posi(L) = ⟨posi(l1, L), posi(l2, L), . . . , posi(lx, L)⟩.
◻
Example 37. Given a list L = ⟨100.0, 80.0, 200.5⟩, since posi(100.0, L) = 2,
posi(80.0, L) = 1, and posi(200.5, L) = 3. Therefore, Posi(L) = ⟨2, 1, 3⟩.
◻
Authenticated
:27 PM

10.3 Contextual Ranking of Database Querying Results
257
Deﬁnition 28. Let L = ⟨l1, l2, . . . , lx⟩and L󸀠= ⟨l󸀠
1, l󸀠
2, . . . , l󸀠
x⟩be two lists of the same
length. A minus operator upon L and L󸀠is deﬁned as:
L – L󸀠= ⟨l1 – l󸀠
1, l2 – l󸀠
2, . . . , lx – l󸀠
x⟩.
◻
The binary minus operator deﬁned above is a closed operator.
Deﬁnition 29. Given x number of contextually ranked database tuples
⟨(t1, u1, s1), (t2, u2, s2), . . . , (tx, ux, sx)⟩, where t1, . . . , tx are database tuples in r(R),
u1, . . . , ux are context instances in c(CS), and s1, . . . , sx are the corresponding contex-
tual ranking scores in [0,1] given by users beforehand, let
–
(Ai, Cj) be a monotonic bijective pair;
–
LAi = ⟨t1.Ai, t2.Ai, . . . , tx.Ai⟩be a list of database attribute Ai’s values from Dom(Ai);
–
LCj = ⟨u1.Cj, u2.Cj, . . . , ux.Cj⟩be a list of contextual attribute Cj’s values from
Dom(Cj);
–
Ldiff = Posi(LAi) – Posi(LCj), and
–
LS = ⟨s1, s2, . . . , sx⟩be a list of ranked values.
The impact of the database–contextual attribute pair (Ai, Cj) is deﬁned as
Impact(Ai, Cj) = |1(Ldiff, LS)|. Here, 1 is the well-known Spearman rank correlation
coefﬁcient [8]. 1(Ldiff, LS) = 1 –
6 ∑n
i=1 d2
i
n(n2–1) , where di = posi(ldiff,i, Ldiff) –posi(lS,i, LS), the
difference between the two statistical ranks of the ith elements in Ldiff and LS, and n is
the length of Ldiff and LS.
◻
The impact factor of a contextual attribute upon its paired database attribute quantit-
atively measures the association strength of the two in terms of whether a contextual
attribute value coordinates well with the corresponding database attribute value. In
other words, when their positions’ difference within respective value lists is small, the
ranking score is high.
10.3.5.4 Selection of Inﬂuential Bijective Monotonic Database–Contextual
Attribute Pairs
According to the impact factors, those inﬂuential bijective monotonic database-
contextual attribute pairs, whose impact factors are above a certain threshold
Impact(Ai, Cj) ≥
3, are identiﬁed and context dimension Cj is thus kept in the
contextual ranking function.
An algorithm showing how to select inﬂuential monotonic bijective database–
contextual attribute pairs for context space reduction (CSR) is given in Algorithm 2.
Complexity Analysis. Given a set of contextually ranked database tuples T un-
der context instances, let w be the total number of monotonic bijective database–
contextual attribute pairs. Function Posi executes the sorting operation which takes
O(|T| ∗log|T|) time complexity. Other operations within the inner loop take linear time.
Therefore, the time complexity of the CSR algorithm is O(w ∗|T| ∗log|T|).
Authenticated
:27 PM

258
10 Context-Aware Preference Querying
10.3.5.5 Performance
To investigate how much can be gained by CSR, three experiments are conducted on
the same machine reported in Section 10.3.4.3.
(1) Performance Metric
Besides running time, the ranking accuracy of the contextual ranking functions after
dropping less inﬂuential contextual attributes is used to measure the effectiveness of
different context reduction techniques. An output ranking result is called an inverse
ranking if t is preferred to t󸀠in the testing data but t󸀠is preferred to t in the predictive
output. With it, the ranking accuracy is deﬁned as one minus the proportion of the
number of inverse rankings to the total number of possible inverse rankings:
Accuracy = 1 –
inverse_rank_num
total_inverse_rank_num
where total_inverse_rank_num = |L| ∗(|L| – 1)
c
, and |L| is the number of result tuples.
(2) Experimental Data
Synthetic Data. Assume (SPF, UVI) and (Price, Income) are two monotonic biject-
ive database–contextual attribute pairs in the motivation example in Section 10.3.1.
Following the contextual ranking function of the form
f(t.SPF, t.Price, u.UVI, u.Income) =
10
|t.SPF – k ∗u.UVI + a| + |t.Price – l ∗u.Income + b|
Algorithm 2: The context space reduction algorithm.
Input:
A set of monotonic bijective database–contextual attribute pairs; x contextually
ranked tuples ⟨(t1, u1, s1), (t2, u2, s2), . . . , (tx, ux, sx)⟩, where t1, . . . , tx are
database tuples in r(R), u1, . . . , ux are context instances in c(CS), and s1, . . . , sx
are the corresponding contextual ranking scores in [0,1] given by users
beforehand.
Output:
A selected subset of monotonic bijective database–contextual attribute pairs.
1:
for all monotonic bijective pairs (Ai, Cj)
do
2:
LAi ←⟨t1.Ai, t2.Ai, . . . , t|T|.Ai⟩
3:
LCj ←⟨u1.Cj, u2.Cj, . . . , u|T|.Cj⟩
4:
LS ←⟨s1, s2, . . . , s|T|⟩
5:
Ldiff ←Posi(LAi) – Posi(LCj)
6:
compute rs(Ldiff, LS)
7:
if |rs(Ldiff, LS)| < 3
then
8:
drop Cj
Authenticated
:27 PM

10.3 Contextual Ranking of Database Querying Results
259
where k = 7.667, l = 0.0095, a = –15, and b = –10.5, the values of t.SPF and u.UVI
are uniformly generated in the range of [15, 130] and [0, 15], respectively, and the val-
ues of t.Price and u.Income are generated with a uniform distribution in the range
of [20, 200] and [1,000, 20,000], respectively. The total number of records of the form
(t.SPF, t.Price, u.UVI, u.Income) varies from 100k to 500k.
To test CSR, one more contextual attribute value u.Age in the range of [15, 90] and
one more database attribute value t.ProductionDate in the range of [2004, 2008] are
added to each record.
Also, some Gaussian random noises are added to the generated synthetic data to
examine whether the experimental results are easily disturbed by the noises or not.
Real
Data.
Ten
users
of
different
ages
and
family
incomes
are
invited
to
rank
different
product
tuples
under
different
context
in-
stances, and produce 20(record) * 10(people) = 200 records of the form
(t.SPF, t.Price, t.ProductionDate,
u.UVI, u.Income, u.Age, score) for learning and
testing the contextual ranking function.
For both synthetic and real data sets, 80% of each are used to learn the contextual
ranking function by regression, while 20% for testing purpose.
(3) Experimental Results
The original synthetic data set contains three database-contextual attribute pairs. The
context selection algorithm successfully eliminates the pair (ProductionDate, Age).
From the results presented in Figure 10.6, it is interesting to see that context selection
can not only increase execution efﬁciency but also improve the ranking effectiveness
compared to the noncontext selection strategy, especially for MARS and polynomial
regression which usually need to take longer time than GLM.
Also, due to the impact of contextual attribute Age upon ProductionDate is less
than 3 (=0.85 in the test), the algorithm eliminates this pair. Figure 10.7 shows almost
no loss in accuracy after reducing context dimensionality.
200s
0.90
0.88
0.86
0.84
0.82
0.80
Polynomial regression on original data set
MARS on original data set
MARS on reduced data set
MARS on original data set
MARS on reduced data set
GLM on original data set
GLM on original data set
GLM on reduced data set
GLM on reduced data set
150s
100s
50s
0s
100K
(a)
(b)
200K
300K
400K
Size
500K
100K
200K
300K
400K
Size
500K
Time cost
Accuracy
Polynomial regression on original
data set
Polynomial regression on reduced
data set
Polynomial regression on reduced data set
Figure 10.6: Performance with/without context space reduction on synthetic data set (a) time cost (s)
and (b) ranking accuracy.
Authenticated
:27 PM

260
10 Context-Aware Preference Querying
Accuracy
original data set
reduced data set
0.0
GLM
Polynomial
regression
Regression models
MARS
0.2
0.4
0.6
0.8
Figure 10.7: Ranking accuracy with/without
context space reduction on real data set.
10.4 Recapitulation
There has been an increasing interest in context awareness and preferences for data-
base querying. The chapter ﬁrst presents a context-aware query preference model,
which can be implanted on top of a DBMS to provide personalized query answers and
trigger proactive personalized information delivery. Querying results can be further
ranked based on the impact of context attributes upon database attributes. Reduc-
tion of the context space not only increases performance efﬁciency but also enhances
our understanding of the inherent relationships between contexts and database
attributes.
The next chapter will analyze sensitivity of ordering change of context database
query results.
Literature
[1]
R. Agrawal and E. Wimmers. A framework for expressing and combining preferences. In Proc. of
ACM SIGMOD, 2000.
[2]
A. Borgida and R. J. Brachman. Loading data into description reasoners. In Proc. of ACM
SIGMOD, pages 217–226, 1993.
[3]
S. Borzsonyi, D. Kossmann, and K. Stocker. The skyline operators. In Proc. of ICDE, pages
421–430, 2001.
[4]
J. Chomicki. Preference formulas in relational queries. ACM Transactions on Database Systems,
28(4):427–466, 2003.
[5]
A. Deshpande, C. Guestrin, S. R. Madden, J. M. Hellerstein, and W. Hong. Model-driven data
acquisition in sensor networks. In Proc. of VLDB, pages 588–599, 2004.
[6]
J. Fan and I. Gijbels. Local Polynomial Modelling and Its Applications, Chapman and Hall,
London, UK, 1996.
[7]
J. Friedman. Multivariate adaptive regression splines. The Annual of Statistics, 19(1):1–67, 1991.
[8]
J. Higgins. Introduction to Modern Nonparametric Statistics, Duxbury Press, USA, 2003.
[9]
S. Holland and W. Kiesling. Situated preferences and preference repositories for personalized
database applications. In Proc. of ER, pages 511–523, 2004.
Authenticated
:27 PM

Literature
261
[10]
T. Horrocks, L. Li, D. Turi, and S. Bechhofer. The instance store: DL reasoning with large
numbers of individuals. In Proc. of the Intl. Workshop on Description Logics (DL), 2004.
[11]
W. Kiesling. Foundations of preferences in database systems. In Proc. of VLDB, pages 311–322,
2002.
[12]
W. Kiesling, B. Hafenrichter, S. Fischer, and S. Holland. Preference XPATH: A query language for
e-commerce. In Proc. of Intern. Konf. Fur Wirtschaftsinformatik, pages 425–440, 2001.
[13]
G. Koutrika and Y. Ioannidis. Personalized queries under a generalized preference model. In
Proc. of ICDE, pages 841–852, 2005.
[14]
M. Lacroix and P. Lavency. Preferences: Putting more knowledge into queries. In Proc. of VLDB,
pages 217–225, 1987.
[15]
S. R. Madden, M. J. Franklin, J. M. Hellerstein, and W. Hong. TinyDB: An acquisitional query
processing system for sensor networks. ACM Transactions on Database Systems,
30(1):122–173, 2005.
[16]
H. Madsen and P. Thyregod. Introduction to General and Generalized Linear Models, Chapman
and Hall, London, 2011.
[17]
P. McCullagh and J. Nelder. Generalized Linear Models, Chapman & Hall, London, 1989.
[18]
S. Milborrow. Earth – multivariate adaptive regression splines in orange (python machine
learning library). http://orange.biolab.si/blog/2011/12/20/earth-multivariate-adaptive-
regression-splines/, 2011.
[19]
R. Myers, D. Montgomery, and G. Vining. Generalized Linear Models with Applications in
Engineering and the Sciences, John Wiley & Sons, New York, 2002.
[20] R. H. Myers. Classical and Modern Regression with Applications, Duxbury Press, USA, 2000.
[21]
M. Peim, E. Franconi, N. W. Paton, and C. A. Goble. Query processing with description logic
ontologies over object-wrapped databases. In Proc. of SSDBM, 2002.
[22] K. Stefanidis and E. Pitoura. Fast contextual preferences scoring of database. In Proc. of EDBT,
2008.
[23] K. Stefanidis, E. Pitoura, and P. Vassiliadis. On supporting context-aware preferences in
relational database systems. In Proc. of the Intl. Workshop on Managing Context Information in
Mobile and Pervasive Environments (MCMP), 2005.
[24] K. Stefanidis, E. Pitoura, and P. Vassiliadis. Adding context to preferences. In Proc. of ICDE,
2007.
[25]
T. Weithoner, T. Liebig, and G. Specht. Storing and querying ontologies in logic databases. In
Proc. of SWDB, pages 329–348, 2003.
Authenticated
:27 PM

11 Analyzing Sensitivity of Answer
Ordering Change
Abstract: Queries over probabilistic context databases result in probabilistic answers,
which are often ranked according to certain ranking criteria. As the probabilities of
the basic tuples may be imprecise and erroneous, and their perturbations may lead
to great changes in answer ordering, sensitivity analysis like “Which basic probability
change will make a certain element top-ranked?”, “Which basic probability change
will swap the positions of the ﬁrstly and secondly ranked elements?”, and “To what
extent the top-3 ranked results keep remaining at the top?” thus arise. This chapter
categorizes ordering sensitivity questions into list-oriented or element-oriented, and
formulates the sensitivity analysis problem for answer ordering returned from prob-
abilistic top-k (aggregation) queries. A modular approach to quantitatively compute
sensitivity of answer ordering is developed with ﬁve basic processing modules being
identiﬁed. Optimization strategies are illustrated regarding each processing module
for performance improvement.
11.1 Motivation
Uncertain data are consistently emerging in a variety of application domains like
sensor network, statistical data analysis, machine learning and data mining, inform-
ation extraction and integration, etc. To manage data uncertainty, a community of
efforts have centered around probabilistic data management [1, 13]. In probabilistic
databases, basic tuples are stored with probabilities reﬂecting their conﬁdence. De-
rived result tuples based on the basic ones are also accompanied with probabilities
accordingly. Thus, they are usually ranked according to certain ranking criteria, whose
orderings are sensitive to perturbations of basic tuples’ probabilities in the database.
Example 38. Consider a context-aware application, which extracts context informa-
tion about electronics companies from the Web. A probabilistic database in Table 11.1,
including three example tables (Company,ComReputation, and ComDiscount)
is formed, showing electronics companies’ possible locations, reputations, and dis-
counts. Each basic tuple represents a probabilistic basic event, and has an event
identiﬁer and a probability attribute. Table 11.1 shows the top-3 answer list (ranked
according to result probability) to the probabilistic query for the electronics compan-
ies with good reputation and discount less than 9. Two sensitivity concerns regarding
the top-3 answer list may arise. (1) “Which basic input probability change will make
Phil drop out of top-3?” (2) “Which basic input probability will swap the positions of
Phil and Aigo?” Apparently, when p(e13) changes from 0.7 to 0.167, result tuple Phil will
drop out of top-3 as shown in Table 11.2. Also, if p(e13) drops from 0.7 to 0.467, result
tuple Phil and Aigo will swap their ﬁrst-second positions, as shown in Table 11.3.
◻
DOI 10.1515/9783110556674-011
Authenticated
:27 PM

11.1 Motivation
263
Company
Event
cid
Name
District
Prob.
e1
1
Phil
West
0.9
e2
2
Aigo
West
0.8
e3
3
HT
East
0.7
e4
4
SNY
South
0.6
e5
5
SSG
North
0.5
e6
6
Pana
North
0.4
ComReputation
Event
cid
Reputation
Prob.
e7
1
Good
0.8
e8
2
Good
0.7
e9
3
Good
0.6
e10
4
Good
0.5
e11
5
Medium
0.4
e12
6
Bad
0.3
ComDiscount
Event
cid
Discount
Prob.
e13
1
8
0.7
e14
2
7
0.6
e15
3
6
0.5
e16
4
5
0.4
e17
5
9
0.3
e18
6
9
0.2
Figure 11.1: A probabilistic database example.
Query 1: Find top-3 electronics companies with good reputation and discount less than
9, i.e.,
SELECT name
FROM
Company,ComReputation,ComDiscount
WHERE
(Company.cid=ComCompany.cid) AND
(Company.cid=ComDiscount.cid) AND
(ComReputation.reputation=’Good’) AND
(ComDiscount.discount<9)
ORDER BY prob. DESC LIMIT 3
As illustrated, the answer ordering returned from a probabilistic database query is
rather sensitive to the underlying input tuples, which may be imprecise and erroneous
in nature. The following sections will investigate the sensitivity of answer ordering re-
turned from probabilistic database queries, aiming to ﬁnd the M basic tuples to which
a certain answer ordering is most sensitive.
Authenticated
:27 PM

264
11 Analyzing Sensitivity of Answer Ordering Change
Table 11.1: The top-3 answers to query 1.
Name
Prob.
Event expression
Phil
0.504 = 0.9∗0.8∗0.7
e1 ∧e7 ∧e13
Aigo
0.336 = 0.8∗0.7∗0.6
e2 ∧e8 ∧e14
HT
0.21 = 0.7∗0.6∗0.5
e3 ∧e9 ∧e15
Table 11.2: Sensitivity question 1 on the probabilistic top-k query answer:
“Which basic input probability change will make Philips drop out of top-3?”
Answer: p(e13) : 0.7 →0.167
Name
Prob.
Event expression
Aigo
0.336 = 0.8∗0.7∗0.6
e2 ∧e8 ∧e14
HT
0.21 = 0.7∗0.6∗0.5
e3 ∧e9 ∧e15
SNY
0.12 = 0.6∗0.5∗0.4
e4 ∧e10 ∧e16
Table 11.3: Sensitivity question 2 on the probabilistic top-k query answer:
“Which basic input probability change will swap the positions of Philips and Aigo?”
Answer: p(e13) : 0.7 →0.46
Name
Prob.
Event expression
Aigo
0.336 = 0.8∗0.7∗0.6
e2 ∧e8 ∧e14
Phil
0.331 = 0.9∗0.8∗0.46
e1 ∧e7 ∧e13
HT
0.21 = 0.7∗0.6∗0.5
e3 ∧e9 ∧e15
11.2 Sensitivity Analysis Techniques in Databases
Sensitivity analysis, in general, assesses the impact of the input perturbation on the
output of a model [37], and has been investigated in a variety of models such as
neural network [31], Bayesian network [5, 9, 11, 20, 23, 24], Markov decision pro-
cess [6, 25, 36, 40], etc. In the database context, related work for sensitivity analysis
includes attribute selection, provenance and lineage, causality and responsibility,
sensitivity measurement, and computation.
11.2.1 Attribute Selection
Das et al. [14] aimed to choose the most useful (inﬂuential) attributes (not basic prob-
abilistic tuples) in the ranking of query results. Three variants of the attribute selection
problem are given.
1) Score-based attribute selection, which selects the top-m attributes that have the
most inﬂuence in the computed scores of the top-n returned tuples of a query.
Authenticated
:27 PM

11.2 Sensitivity Analysis Techniques in Databases
265
2) Rank-based attribute selection, which selects the top-m attributes that are most
inﬂuential in ranking the top-n tuples higher than the rest of the tuples also satisfying
the query conditions.
3) Relative rank-based attribute selection, which selects the top-m attributes that
are most inﬂuential in preserving the relative rankings of the returned top-n tuples
amongst each other.
Algorithms for the three variants of attribute selection, based on greedy heuristics
and partial computations, were proposed accordingly. Unlike these work focusing on
the selection of inﬂuential database attributes, the chapter focuses on the sensitivity
of basic database tuples in probabilistic databases.
11.2.2 Provenance and Lineage
Different notions of provenance, sometimes called lineage, for database queries were
proposed and studied in the literature [4, 9, 16, 41]. Data provenance falls mainly into
three categories: why-, how-, or where-provenance.
(1) Why-provenance [4, 12] associates each result tuple t with a set of tuples present
in the input.
(2) How-provenance [16] describes how each result tuple is derived from the input
tuples as a polynomial hinting at the structure of the proofs by which the output tuple
is derived.
(3) Where-provenance [4] describes where a piece of result data is copied from,
e.g., a column of a tuple in a relation.
The relationships among the three types of provenances were analyzed in Ref. [9].
Methods for efﬁcient lineage and provenance processing were investigated in Refs. [9,
21]. As a Boolean lineage formula for a tuple t in large probabilistic databases can be
huge, Re et al. [35] proposed to use a much smaller approximate lineage formula to
keep track of the most important derivations of t. Two kinds of approximate lineage
are thus given:
(1) a conservative approximation called sufﬁcient lineage that records the most
important derivations for each tuple, and
(2) a more aggressive polynomial lineage which can provide higher comprehen-
sion ratios based on Fourier approximations of Boolean expressions.
Algorithms to compute approximate lineage are detailed in Ref. [35]. The work
on lineage forms the base of the chapter, from which an ordering-sensitivity analysis
platform can be built.
11.2.3 Causality and Responsibility
Causality in the database context has a close relationship to the lineage of query
results, such as why-provenance or where-provenance [8, 18]. Meliou et al. [28–30]
developed the notion of causality of answers and nonanswers to database queries
Authenticated
:27 PM

266
11 Analyzing Sensitivity of Answer Ordering Change
based on the deﬁnitions of causality and responsibility in Refs. [10, 17]. For an answer
tuple ̄a, a tuple t is a counterfactual cause, if by removing it from the database, ̄a is re-
moved from the answer. The tuple t is an actual cause if one can ﬁnd a set A of tuples
such that after removing A from the database, t will become a counterfactual cause.
The responsibility of t for the answer tuple ̄a is computed as 1t =
1
1 + minA |A|. Computing
the causality for conjunctive queries has been proved to have PTIME data complexity,
and a dichotomy theorem is proved for responsibility and conjunctive queries [30].
The sensitivity of ordering change is not the focus of this study.
11.2.4 Sensitivity Measurement and Computation
Soliman et al. [39] investigated the inﬂuence of uncertain scoring functions (not basic
probabilistic tuples) on top-k join query results, where multiple data sources with their
own scores are joined. The weights uncertainty in the scoring functions are modeled as
a uniform probability density function. When a weight vector ̄w is given as input, a cor-
responding ordering result + can be computed. Two sensitivity analysis problems were
particularly addressed. The ﬁrst problem is to quantify the largest weights change in
the neighborhood of the input weight vector
̄w that does not introduce perturbations
to the result ordering +. All the weights that can induce the same top-k result as + form
a polyhedron, denoted as 0( ̄+K). This problem was resolved by ﬁnding the radius of
the maximum hypersphere centered at the input vector, and enclosed in 0( ̄+K). The
second problem is to quantify the likelihood of obtaining an ordering identical to +,
given a random weight vector. This problem can be resolved by computing the volume
of 0( ̄+K) [39].
11.3 Sensitivity Analysis Problem for Answer
Ordering Change
Before the problem statement for sensitivity analysis of answer ordering in probab-
ilistic databases, let’s ﬁrst review the probabilistic data model and corresponding
probabilistic database queries.
11.3.1 Review of the Probabilistic Database Model
Dalvi et al. proposed a probabilistic database model with tuple uncertainty in Ref. [12],
where Each tuple ti in a probabilistic database represents a probabilistic basic event ei
with probability p(ei). Assume all the probabilistic events are independent. Each result
tuple ri in a derived relation would be assigned a complex event expression eei called
lineage which is a Boolean combination of the basic events corresponding to the basic
tuples from which it was derived. Each of the basic operation in relational algebra
Authenticated
:27 PM

11.3 Sensitivity Analysis Problem for Answer Ordering Change
267
(selection, projection, union, difference, and Cartesian product) has a corresponding
event expression generating rule. Cui et al. [12] detailed the process of computing the
result tuple event expressions and the equivalent possible worlds semantics. From the
event expression eei of result tuple ri, the probability P(eei) can be computed based on
the underlying basic events’ probabilities. In the following, p(ti) and p(ei), p(ri) and
P(eei) are used interchangeably.
11.3.1.1 Probabilistic Top-k Queries
From a set of basic tuples B = {t1, t2, . . . , tn} in a probabilistic database, a set of result
tuples S = {r1, r2, . . . , rm} is derived with complex event expressions {ee1, ee2, . . . , eem},
whose probabilities {p(ee1), p(ee2), . . . , p(eem)} are computed according to the basic
input tuples’ probabilities. The probabilistic top-k query is to ﬁnd K result tuples from
S whose complex event expressions have the highest probabilities [34].
11.3.1.2 Probabilistic Top-k Aggregation Queries
From a set of basic tuples B = {t1, t2, . . . , tn} in a probabilistic database, a set of result
tuples S = {r1, r2, . . . , rm} is derived, and further divided into w groups GroupSet = {G1,
G2, . . ., Gw} according to the grouping attribute set A.
Each group G ∈GroupSet consists of a set of tuples G = {r1, r2, . . . , r|G|}, derived
with the complex event expressions {ee1, ee2, . . . , ee|G|} of probabilities {p(ee1), p(ee2),
. . . , p(ee|G|)}. Each tuple ri ∈G has a score value vi.
The expected aggregate value (sum, count, max, min, or avg) of group G can be
further computed by the following function F(G) based on input tuples’ probabilities
p(ee1), p(ee2), . . . , p(ee|G|) and score values {v1, v2, . . ., v|G|} as follows [19, 22]:
1) SUM Fsum(G) = ∑|G|
i=1 p(eei) ∗vi.
2) COUNT The count operator is just a special case of sum, where all the score values
take the value 1. Fcount(G) = ∑|G|
i=1 p(eei).
3) MAX Assume all the score values {v1, v2, . . ., v|G|} are sorted in a non-increasing
order. Fmax(G) = ∑|G|
i=1 p(eei ∧(⋀1≤j<i ∼eej)) ∗vi.
4) MIN Assume all the score values {v1, v2, . . ., v|G|} are sorted in a non-decreasing
order. Fmin(G) = ∑|G|
i=1 p(eei ∧(⋀1≤j<i ∼eej)) ∗vi.
5) AVG Favg(G) = ∑x1,...,x|G|
∑|G|
i=1 7xi(vi)
∑|G|
i=1 xi
∗p(⋀|G|
i=1 7xi(eei)), where xi takes value 0 or 1, and
7xi(vi) = {
0
if xi = 0
vi
if xi = 1
(11.1)
7xi(eei) = {
∼eei
if xi = 0
eei
if xi = 1
(11.2)
Authenticated
:27 PM

268
11 Analyzing Sensitivity of Answer Ordering Change
All the groups in GroupSet are sorted by their expected aggregate values, with the
top-k groups returned as the probabilistic top-k aggregation query answer.
11.3.2 Answer Ordering Change
For both probabilistic top-k queries and probabilistic top-k aggregation queries, the
answer is an ordered list of K elements ranked based on certain ranking criteria.
Without loss of generality, let R = ⟨r1, r2, . . . , rK⟩be a top-k answer list with Rank(r1) ≥
Rank(r2) ≥⋅⋅⋅≥Rank(rK). We use R.ri (1 ≤i ≤K) to represent an answer element ri,
which can be either speciﬁed by its relative position in R like ri = R[i] or by its attribute
value like ri.att = val.
The ranking order of answer R is sensitive to perturbations of basic tuples’ prob-
abilities, e.g., some element may drop out of the top-k list, some new element may
enter the top-k list, some element may swap the position with another one, etc. after
the change of some basic tuples’ probabilities. Table 11.4 illustrates two types of order-
ing changes (i.e., list-oriented ordering change and element-oriented ordering change)
from a top-k answer list R to a new top-k answer list R󸀠, generally expressed via an or-
dering change predicate Pred(R, R󸀠). We call the ordering change Pred(R, R󸀠) happens if
and only if Pred(R, R󸀠) = TRUE.
11.3.3 Measurement of Answer Ordering Change
For the element-oriented ordering change, to measure the answer ordering change,
the distance between top-k list R and R󸀠is deﬁned as
Distance(R, R󸀠)=
{
{
{
1 if Pred(R, R󸀠)=TRUE;
0if Pred(R, R󸀠)=FALSE.
(11.3)
For the list-oriented ordering change, to measure the ordering change from a top-k
answer list R1 to R2, we extend the classic Spearman’s footrule metric [15] to evaluate
more highly the position changes at the top places than the bottom places.
In the following, for a list 3, we use S3 to denote the set which contains all the
elements in 3 and call it the element set of 3, and 3(x) to denote the position of element
x in 3.
11.3.3.1 Classic Spearman’s Footrule Metric
For two lists 31 and 32 with the same element set, the Spearman’s footrule distance
between 31 and 32 is deﬁned as
Distance(31, 32) = ∑
x∈S31
|31(x) – 32(x)|,
Authenticated
:27 PM

11.3 Sensitivity Analysis Problem for Answer Ordering Change
269
Table 11.4: Ordering change from R to R󸀠, where R = ⟨r1, r2, . . . , rK⟩is the top-k answer list returned
from a probabilistic top-k (aggregation) query, and R󸀠= ⟨r󸀠
1, r󸀠
2, . . . , r󸀠
K⟩is another top-k answer list
from the same query after changing basic tuples’ probabilities.
Category
Ordering change predicate Pred (R, R󸀠)
List-
lChange(R, [R󸀠])
oriented
R changes to R󸀠(if R󸀠is given), otherwise, R changes.
eUpPosi(R, R󸀠, R.r, [Bp], [K])
R.r goes up Bp positions or enters top-k in R󸀠, i.e.,
when Bp appears:
∃i (1 ≤i ≤K) (R.r = R[i]) ∧(R.r = R󸀠[i + Bp]).
when K appears:
∀i (1 ≤i ≤K) (∃j (1 ≤j ≤K) (R.r
̸= R[i])∧(R.r = R󸀠[j])).
Element-
eDownPosi(R, R󸀠, R.r, [Bp], [K])
oriented
R.r drops down Bp positions or drops out of top-k in R󸀠, i.e.,
when Bp appears:
∃i (1 ≤i ≤K) (R.r = R[i]) ∧(R.r = R󸀠[i – Bp]).
when K appears:
∀i (1 ≤i ≤K) (∃j (1 ≤j ≤K) (R.r
̸= R󸀠[i])∧(R.r = R[j])).
eRemainPosi(R, R󸀠, R.r)
R.r’s position remains unchanged, i.e.,
∃i (1 ≤i ≤K) (R.r = R[i]) ∧(R.r = R󸀠[i]).
eSwapPosi(R, R󸀠, R.r, R.r󸀠)
R.r and R.r󸀠swap the positions, i.e.,
∃i, j (1 ≤i, j ≤K) (R.r = R[i]) ∧(R.r󸀠= R[j]) ∧(R.r = R󸀠[j]) ∧
(R.r󸀠= R󸀠[i]).
eKeepOrder(R, R󸀠, R.r, R.r󸀠)
The order of R.r and R.r󸀠remain the same, i.e.,
∃i, j (1 ≤i, j ≤K) (R.r = R[i]) ∧(R.r󸀠= R[j])
∃i󸀠, j󸀠(1 ≤i󸀠, j󸀠≤K) (R.r = R[i󸀠]) ∧(R.r󸀠= R󸀠[j󸀠])
(i > i󸀠→j > j󸀠) ∨(i < i󸀠→j < j󸀠).
eReverseOrder(R, R󸀠, R.r, R.r󸀠)
The order of R.r and R.r󸀠changes reversely, i.e.,
∃i, j (1 ≤i, j ≤K) (R.r = R[i]) ∧(R.r󸀠= R[j])
∃i󸀠, j󸀠(1 ≤i󸀠, j󸀠≤K) (R.r󸀠= R󸀠[i󸀠]) ∧(R.r󸀠= R󸀠[j󸀠])
(i > i󸀠→j < j󸀠) ∨(i < i󸀠→j > j󸀠).
which represents the summation of all the elements’ position changes. For example,
if 31 = ⟨b, c, a, d⟩, 32 = ⟨a, c, d, b⟩, the distance between 31 and 32 is Distance(31, 32) =
|31(a) – 32(a)| + |31(b) – 32(b)| + |31(c) – 32(c)| + |31(d) – 32(d)| = 6.
11.3.3.2 Extension of Spearman’s Footrule Metric
A weight is assigned to every two consecutive position change. For two lists 31 and
32 of the same n elements, (n – 1) weights w1, w2, . . . , wn–1 are introduced with (w1 >
w2 > ⋅⋅⋅> wn–1 > 0). Weight wi measures the contribution of moving an element from
Authenticated
:27 PM

270
11 Analyzing Sensitivity of Answer Ordering Change
position i to position i + 1 or from position i + 1 to position i to the distance of two lists.
For any two positions i and j, let
C(i, j) =
{
{
{
{
{
{
{
∑j–1
k=i wk
if i < j;
0
if i = j;
∑i–1
k=j wk
if i > j
(11.4)
be the contribution of moving an element from position i to position j. For 31 and 32,
the contribution of moving element x is C(31(x), 32(x)). For the weighted Spearman’s
footrule metric, the distance between lists 31 and 32 of the same elements is deﬁned as
Footrulew(31, 32) = ∑
x∈S31
C(31(x), 32(x)).
(11.5)
For example, let 31 = ⟨b, c, a, d⟩and 32 = ⟨a, c, d, b⟩. The introduced weights are w1,
w2, w3. Then, for the distance between 31 and 32, the contribution of moving element a
is C(31(a), 32(a)) = w1 + w2. Similarly, C(31(b), 32(b)) = w1 + w2 + w3, C(31(c), 32(c)) = 0,
C(31(d), 32(d)) = w3. The weighted distance between 31 and 32 is Footrulew(31, 32) =
2w1 + 2w2 + 2w3.
Modiﬁcations of the weighted Spearman’s footrule metric Footrulew can be made
for the case when there are only top-k members of the ordering. Let R1 and R2 be two
top-k lists, which may contain different sets of elements, i.e. SR1
̸= SR2. The minim-
izing weighted Spearman’s footrule distance Footrulew
min(R1, R2) between R1 and R2 is
deﬁned to be the minimum value of Footrulew(31, 32), where 31 and 32 are any two
lists with element set SR1 ∪SR2 and 31 ⪰R1, 32 ⪰R2. Here, for two lists R and 3 with
SR ⊂S3, we use 3 ⪰R to denote R(x) = 3(x) for all x ∈SR and call 3 an extension of R.
Similarly, the maximizing weighted Spearman’s footrule distance Footrulew
max(R1, R2)
and the averaging weighted Spearman’s footrule distance Footrulew
avg(R1, R2) can be
deﬁned by using the maximum and average value of Footrulew(31, 32), respectively.
Proposition 1. For any two top-k lists R1 and R2,
Footrulew
min(R1, R2) = Footrulew
avg(R1, R2) = Footrulew
max(R1, R2)
= Footrulew(31, 32).
(11.6)
For any two lists 31 and 32 of the same element, set SR1 ∪SR2 with 31 ⪰R1 and
32 ⪰R2.
◻
Beweis. If each of 31 and 32 has only one more element than R1 and R2, then, there is
only one possible case for 31 and 32, the conclusion is obviously true.
Otherwise, we ﬁrst prove that for any list 3󸀠
2 which is obtained from 32 by swapping
two adjacent elements x ∈R1 \ R2, y ∈R1 \ R2, Footrulew(31, 32) = Footrulew(31, 3󸀠
2).
Here, in 32 and 3󸀠
2, x and y are both ordered after all the elements in SR2. Now,
Authenticated
:27 PM

11.3 Sensitivity Analysis Problem for Answer Ordering Change
271
Footrulew(31, 32)
= C(31(x), 32(x)) + C(31(y), 32(y)) +
∑
z∈(SR1∪SR2)\{x,y}
C(31(z), 32(z))
(11.7)
Footrulew(31, 3󸀠
2)
= C(31(x), 3󸀠
2(x)) + C(31(y), 3󸀠
2(y)) +
∑
z∈(SR1∪SR2)\{x,y}
C(31(z), 3󸀠
2(z)).
(11.8)
For all z ∈(SR1 ∪SR2) \ {x, y}, 32(z) = 3󸀠
2(z). Therefore,
∑
z∈(SR1∪SR2)\{x,y}
C(31(z), 32(z)) =
∑
z∈(SR1∪SR2)\{x,y}
C(31(z), 3󸀠
2(z)).
(11.9)
For x and y, we have 31(x) ≤K < 32(x), 31(x) ≤K < 3󸀠
2(x), 31(y) ≤K < 3󸀠
2(y), and
31(y) ≤K < 3󸀠
2(y). If 32(x) < 32(y), then 3󸀠
2(x) > 3󸀠
2(y). We thus have
C(31(x), 3󸀠
2(x)) = C(31(x), 32(x)) + w32(x),
(11.10)
C(31(y), 3󸀠
2(y)) = C(31(y), 32(y)) – w32(x),
(11.11)
where w32(x) is the contribution of moving x from position 32(x) to 32(x) + 1 to the over-
all distance. Notice that w32(x) is also the contribution of moving y from position 32(y)
to 32(y) – 1.
From formulas (11.10) and (11.11), we get
C(31(x), 3󸀠
2(x)) + C(31(y), 3󸀠
2(y)) = C(31(x), 32(x)) + C(31(y), 32(y)).
(11.12)
Similarly, formula (11.12) is also true for the case 32(x) > 32(y).
From formulas (11.7), (11.8), (11.9), and (11.12), we get
Footrulew(31, 32) = Footrulew(31, 3󸀠
2).
(11.13)
Now, for each list 3∗
2 of SR1 ∪
SR2 with 3∗
2
⪰R2, we have Footrulew(31, 32) =
Footrulew(31, 3∗
2 ), as 3∗
2 can be got from 32 by a ﬁnite adjacent swapping of elements
in R1 \ R2. Symmetrically, for each list 3∗
1 of SR1 ∪SR2, 3∗
1 ⪰R1,
Footrulew(31, 32) = Footrulew(31, 3∗
2 ) = Footrulew(3∗
2 , 31)
= Footrulew(3∗
2 , 3∗
1 ) = Footrulew(3∗
1 , 3∗
2 ).
(11.14)
◻
For
two
top-k
lists
R1
and
R2,
we
also
use
Footrulew(R1, R2)
to
denote
Footrulew
min(R1, R2), Footrulew
avg(R1, R2), or Footrulew
max(R1, R2), since they all have
the same value, e.g., if R1 = ⟨a, b, c⟩, R2 = ⟨b, a, d⟩, then
Authenticated
:27 PM

272
11 Analyzing Sensitivity of Answer Ordering Change
Footrulew(R1, R2) = Footrulew(⟨a, b, c, d⟩, ⟨b, a, d, c⟩) = 2w1 + 2w3, when the corres-
ponding weights are w1, w2, and w3, respectively.
The following uses Distance(R, R󸀠) to denote the distance between answer list
R and R󸀠uniformly for the list-oriented ordering change and the element-ordering
change. For the list-oriented ordering change, it represents Footrulew(R, R󸀠); for the
element-ordering change, it is deﬁned as in eq. (11.3).
11.3.4 Sensitivity of Answer Ordering Change
Deﬁnition 30. Let A ⊂B be a set of basic tuples in the probabilistic database. Let R be
a top-k answer list returned from a probabilistic top-k (aggregation) query. Let R󸀠be
another top-k answer list from the same query after the change of the probabilities of
the tuples in A. If the probability change of the tuples in A makes the answer ordering
change Pred(R, R󸀠) = TRUE (listed in Table 11.4), we call the answer ordering change
Pred(R, R󸀠) is sensitive to the basic tuples in A.
◻
Deﬁnition 31. Suppose that A
=
{ti1, ti2, . . . , ti|A|} is a set of basic tuples in the
probabilistic database, and the basic events corresponding to the tuples in A is
{ei1, ei2, . . . , ei|A|}. After the probabilities of the tuples in A changes from P(ei1), P(ei2),
. . . , P(ei|A|) to P(ei1)󸀠, P(ei2)󸀠, . . . , P(ei|A|)󸀠, the top-k answer list changes from R to R󸀠.
The change of the probabilities of the tuples in A is deﬁned as the Euclidean distance
between (P(ei1), P(ei2), . . . , P(ei|A|)) and (P(ei1)󸀠, P(ei2)󸀠, . . . , P(ei|A|)󸀠),
Bp(ei1, ei2, . . . , ei|A|) = √
|A|
∑
u=1
(p(eiu)󸀠– p(eiu))2.
(11.15)
◻
Based on the distance measurement between two top-k answer lists, we can deﬁne the
sensitivity degree of an answer ordering change to underlying basic tuples. Intuitively,
when an ordering change happens, the less the deriving tuples’ probabilities change,
the more sensitive it is.
Deﬁnition 32. Let A ⊂B be a set of basic tuples in the probabilistic database. Let R and
R󸀠(R
̸= R󸀠) be two top-k answer lists returned from a probabilistic top-k (aggregation)
query before and after the change of the probabilities of the tuples in A. Assume the
answer ordering change Pred(R, R󸀠) is sensitive to the basic tuples in A. The sensitivity
degree of the basic tuples in A with respect to Pred(R, R󸀠) is computed as follows.
Case 1 When the change result R󸀠is given,
Sensitivity(A) |Pred(R,R󸀠) =
Distance(R, R󸀠)
minBp(ei1, ei2, . . . , ei|A|) |Pred(R,R󸀠)=TRUE
(11.16)
where minBp(ei1, ei2, . . . , ei|A|) returns the minimum change of the probabilities of
the tuples in A that makes Pred(R, R󸀠) = TRUE.
Authenticated
:27 PM

11.4 Sensitivity Computation for Answer Ordering Change
273
Case 2 When the change result R󸀠is omitted,
Sensitivity(A) |Pred(R,–)
= maxRx
Distance(R, Rx)
minBp(ei1, ei2, . . . , ei|A|) | Pred(R,Rx)=TRUE
(11.17)
where Rx is any possible ordering change derived from the original top-k an-
swer list R, and min Bp(ei1, ei2, . . . , ei|A|) returns the minimum change of the
probabilities of the tuples in A that makes Pred(R, Rx)=TRUE.
◻
The sensitivity analysis of answer ordering in probabilistic databases can thus be
formally stated as follows:
Given a probabilistic top-k (aggregation) query answer R, the sensitivity computation of R’s or-
dering change Pred(R, R󸀠) is to ﬁnd M basic tuples from the probabilistic database with the highest
sensitivity degrees.
In the formulation, M = 1 corresponds to the one-way sensitivity analysis, whose goal
is to understand the sensitivity of the answer ordering to a single basic tuple’s prob-
ability change; M > 1 corresponds to the multi-way sensitivity analysis, whose goal
is to understand the sensitivity of the answer ordering to multiple simultaneously
changing tuples’ probabilities.
A declarative way for users to specify their ordering sensitivity analysis require-
ments is as follows:
Ordering-Sensitivity-Analyze [M]
<Ordering-Change-Predicate>
Answer-Of <Query-Expression>
where <Ordering-Change-Predicate> is given in Table 11.4, and <Query-
Expression> is a probabilistic top-k (aggregation) query statement, whose answer
ordering sensitivity is to be analyzed.
11.4 Sensitivity Computation for Answer
Ordering Change
Five computational modules contribute to the sensitivity analysis of answer ordering
change.
11.4.1 Five Computation Modules
The sensitivity computation of an answer ordering change Pred(R, R󸀠) involves the fol-
lowing computation modules. First, for each answer element r in the answer list R,
we need to examine how a basic tuple t’s probability change affects its rank value.
Second, since an answer ordering change corresponds to a series of answer tuples’
Authenticated
:27 PM

274
11 Analyzing Sensitivity of Answer Ordering Change
swaps, for every two elements, we need to examine how a basic tuple t’s probabil-
ity change will swap their order. Third, the distance between two orderings needs to
be measured, based on which the sensitivity degrees of basic tuples can be computed.
Fourth, when investigating the sensitivity of the answer ordering to multiple simultan-
eously changing basic tuples, a constrained optimization problem needs to be solved.
Fifth, we need to compute the sensitivity degree of a tuple.
11.4.1.1 Module 1 – Computing the Effect of a Tuple’s Probability Change to an
Answer Element
We start with the probabilistic top-k query. The lineage of each result tuple r is an
event expression ee which is a Boolean Formula of its deriving basic events. For each
deriving basic event e (corresponding to an input tuple t), p(ee) is a linear function
of p(e),
p(ee) = ! ∗p(e) + "
(11.18)
where ! and " are functions of probabilities of other deriving basic events except e,
and can be considered as constants with respect to p(e). From formula (11.18), we can
derive 𝜕p(ee)
𝜕p(e) = !, which represents the change rate of p(ee) with respect to p(e). The
linear property of p(ee) with respect to p(e) and the computing process of 𝜕p(ee)
𝜕p(e) can be
found in Ref. [22].
Suppose that the probability value of basic event e corresponding to the input
tuple t stored in the database is p0(e), and the corresponding probability value of res-
ult tuple r is p0(ee). Then, the relationship of p(ee) and p(e) can be considered as a
straight line in a plane with equation
p(ee) = 𝜕p(ee)
𝜕p(e) (p(e) – p0(e)) + p0(ee).
(11.19)
For input tuple t, there are K straight lines corresponding to the K result tuples. If e
does not occur in the lineage of r, then 𝜕p(ee)
𝜕p(e) = 0, and the corresponding straight line
is p(ee) = p0(ee). When the probability of e changes in [0, 1], each change of the result
ranking corresponds to an intersection point of two of the K straight lines.
For the probabilistic top-k aggregation query, the average aggregation for each
group (including SUM, COUNT, MAX, MIN, AVG) is also a linear function of the deriv-
ing basic event probability. Hence, the idea that the intersection point of two straight
lines corresponds to a change of the result ranked list also applies to the probabilistic
top-k aggregation query.
(1) Basic Approach
For the probabilistic top-k query, we adopt the approach proposed in Ref. [22] to
compute 𝜕p(ee)
𝜕p(e) .
Authenticated
:27 PM

11.4 Sensitivity Computation for Answer Ordering Change
275
For the probabilistic top-k aggregation query, the partial derivative of the aver-
age aggregation for each group G with respect to basic event probability p(e) can be
computed as follows.
1) SUM 𝜕Fsum(G)
𝜕p(e)
= ∑|G|
i=1
𝜕p(eei)
𝜕p(e) ∗vi.
2) COUNT 𝜕Fcount(G)
𝜕p(e)
= ∑|G|
i=1
𝜕p(eei)
𝜕p(e) .
3) MAX 𝜕Fmax(G)
𝜕p(e)
= ∑|G|
i=1
𝜕p(eei∧(⋀1≤j<i∼eej))
𝜕p(e)
∗vi.
4) MIN 𝜕Fmin(G)
𝜕p(e)
= ∑|G|
i=1
𝜕p(eei∧(⋀1≤j<i∼eej))
𝜕p(e)
∗vi.
5) AVG The computing process in Section 3.1 is exponential. We adopt the approximate
approach [33], where
FAVG =
FSUM
FCOUNT and 𝜕FAVG
𝜕p(e) =
𝜕FSUM
𝜕p(e) FCOUNT–FSUM
𝜕FCOUNT
𝜕p(e)
F2
COUNT
.
(2) Optimization
For a basic event e which occurs in result tuple r but not in the top-k results, when
p0(r) + 𝜕p(r)
𝜕p(e)(1 – p0(e)) < p0(rK),
(11.20)
where rK represents the Kth result element, the probability change of e will not lead to
a change of the result ranked list. Formula (11.20) is equivalent to
𝜕p(r)
𝜕p(e) < p0(rK) – p0(r)
1 – p0(e)
.
(11.21)
Therefore, in the process of computing 𝜕p(r)
𝜕p(e), if the condition in formula (11.21) is sat-
isﬁed, then the computing process for 𝜕p(r)
𝜕p(e) can be stopped, avoiding the computation
of exact value.
If the event expression for a result tuple is of the form ee = e1 ∧e2 ∧⋅⋅⋅∧en, for
the basic events which only occur in ee, the basic tuples except the M basic ones with
the smallest probabilities can be ignored. The partial derivative of p(ee) with respect
to p(e) can be computed directly as follows:
𝜕p(ee)
𝜕p(e) = p0(ee)
p0(e) .
If the event expression for a result tuple is of the form ee = e1 ∨e2 ∨⋅⋅⋅∨en, for the
basic events which only occur in ee, the basic tuples except the M basic ones with the
largest probabilities can be ignored. The partial derivative of p(ee) with respect to p(e)
can be computed directly as follows:
𝜕p(ee)
𝜕p(e) = 1 – p0(ee)
1 – p0(e) .
Authenticated
:27 PM

276
11 Analyzing Sensitivity of Answer Ordering Change
If the event expression for a result tuple is of the DNF form, for the basic event
e which only occurs in ee, if it only occurs in a clause C of ee, the basic tuples
in C except the M basic ones with the smallest probabilities can be ignored.
The partial derivative of p(ee) with respect to p(e) can be computed directly as
follows:
𝜕p(ee)
𝜕p(e) = p0(C)
p0(e)
1 – p0(ee)
1 – p0(C) .
Example 39. We continue with Example 1. The result ranked list of Query 1 includes
Phil (e1 ∧e7 ∧e13), Aigo (e2 ∧e8 ∧e14), HT (e3 ∧e9 ∧e15), and SNY (e4 ∧e10 ∧e16). For basic
tuple e13, they correspond to four straight lines p(Phil) = 0.72 ∗p(e13), p(Aigo) = 0.336,
p(HT) = 0.21, p(SNY) = 0.12, as shown in Figure 11.2. When p(e13) changes in [0, 1],
three intersection points are produced. At the intersection point p(e13) = 0.467, Phil
and Aigo swap positions. At the intersection point p(e13) = 0.167, Phil drops out of
top-3.
◻
11.4.1.2 Module 2 – Computing the Effect of a Tuple’s Probability Change
to Ordering of Two Elements
(1) Basic Approach
For a probabilistic top-k query, for basic event e, the intersection point of the two
straight lines corresponding to ru and rv with the following two equations
p(eeu) = 𝜕p(eeu)
𝜕p(e) (p(e) – p0(e)) + p0(eeu)
0.504
0.336
P1(e13) – P0(e13) = (P0(Phil) – (P0(Aigo))/
e1Λe7Λe13
δP (e13)
δP (Phil)
Phil
e2Λe8Λe14
Aigo
e3Λ e9Λe15
HT
e4Λe10Λe16
SNY
P2(e13) – P0(e13) = (P0(Phil) – (P0(HT) ) / δP (e13)
δP (Phil)
P3(e13) – P0(e13) = (P0(Phil) – (P0(SNY) ) /
P3(e13) = 0.167 P2(e13) = 0.292 P1(e13) = 0.467 P0(e13) = 0.7
P(e13)
δP (e13)
δP (Phil)
0.21
Result probability
0.12
B
A
Figure 11.2: Effect of the change of p(e13) to the top-3 answer list in Example 1.
Authenticated
:27 PM

11.4 Sensitivity Computation for Answer Ordering Change
277
and
p(eev) = 𝜕p(eev)
𝜕p(e) (p(e) – p0(e)) + p0(eev)
are represented as
⟨p(e) = p0(e) + p0(eeu) – p0(eev)
𝜕p(eev)
𝜕p(e) – 𝜕p(eeu)
𝜕p(e)
, ru, rv⟩,
meaning that ru and rv are swapped in the ranked list at p(e) = p0(e) + p0(eeu) – p0(eev)
𝜕p(eev)
𝜕p(e) – 𝜕p(eeu)
𝜕p(e)
.
For the case p(e) ∉[0, 1] or 𝜕p(eeu)
𝜕p(e)
= 𝜕p(eev)
𝜕p(ej) , the straight lines corresponding to ru and
rv do not intersect and the positions of ru and rv do not change in the ranked list.
For the probabilistic top-k aggregation query, the process of computing intersec-
tion points is similar. For example, the SUM aggregate corresponds to substituting eeu
and eev with Fsum(Gu) and Fsum(Gv) in the above computing process.
For the lChange ordering change, for each basic event e, when its probability
change in [0, 1], all the intersection points of the straight lines corresponding to the
results in the ranked lists are computed, and are sorted according to their abscissa on
the left-hand and right-hand sides of p0(e).
(2) Optimization
For the basic event e, in the basic algorithm, to compute its sensitivity degree, all the
intersection points of all the straight lines corresponding to all the result tuples need
to be computed. In fact, when computing the intersection points, only the straight
lines corresponding to the top-k result tuples in which e does not occur and all the
result tuples in which e occurs need to be considered, the intersection points produced
by other straight lines will not correspond to a change of result ranked list and can be
ignored.
11.4.1.3 Module 3 – Measuring the Distance between Two Top-k Answer Lists
(1) Basic Approach
The distance between two ranked lists can be computed by using formula (11.6)
directly.
(2) Optimization
Based on Module 2, which sorts all the intersection points according to their abscissas
on the left-hand and right-hand sides of the p0(e), respectively, the distance compu-
tation can be further simpliﬁed. Suppose that when the probability of e changes from
p0(e) to the right-hand side to p󸀠
1(e), p󸀠
2(e), . . ., p󸀠
s1(e) successively, the result ranked list
changes to R󸀠
1, R󸀠
2, . . ., R󸀠
s1 accordingly; when the probability of e changes from p0(e) to
Authenticated
:27 PM

278
11 Analyzing Sensitivity of Answer Ordering Change
the left-hand side to p󸀠󸀠
1 (e), p󸀠󸀠
2 (e), . . ., p󸀠󸀠
s2(e), the result ranked list changes to R󸀠󸀠
1 , R󸀠󸀠
2 ,
. . ., R󸀠󸀠
s2 accordingly.
Suppose that when the probability of e changes from p󸀠
j(e) to p󸀠
j+1(e), the positions
of result tuple x and y are inverted (in R󸀠
j, x is ranked before y; in R󸀠
j+1, x is ranked after
y). If we use x0 and xj to denote the position of x in R0 and R󸀠
j, respectively, and y0 and
yj the position of y in R0 and R󸀠
j, respectively, then
Distancew(R󸀠
j+1, TopK0) = Distancew(R󸀠
j, TopK0)+
{
{
{
{
{
{
{
2wxj
if x0 ≤xj and y0 ≥yj;
–2wxj
if x0 > xj and y0 < yj;
0
if (x0 > xj and y0 ≥yj) or (x0 ≤xj and y0 < yj).
(11.22)
Hence, all the distances Distancew(R󸀠
j, R0) can be computed by combining for-
mula (11.22) and the initial condition
Distancew(R󸀠
0, R0) = 0.
All the distances Distancew(R󸀠󸀠
j , R0) in the left-hand side of p0(e) can be computed sim-
ilarly. This optimization reduces the time complexity of computing distance between
two top-k lists from O(K) to O(1).
11.4.1.4 Module 4 – Resolving the Optimization Problem
(1) Basic Approach
For a selected tuple set A, the computation of its sensitivity degree can be reduced
to a constrained optimization problem and solved by applying Newton’s method. For
example, if the sensitivity analysis problem is to examine to which M tuples the or-
der of answer tuples r1 and r2 is most sensitive, we can transform it to the following
equivalent constrained optimization problem:
min
|A|
∑
u=1
(p(eiu)󸀠– p(eiu))2
s.t.
f1(p(ei1)󸀠, p(ei2)󸀠, . . . , p(eis)󸀠) = f2(p(eis+1)󸀠, p(eis+2)󸀠, . . . , p(ei|A|)󸀠),
(11.23)
where p(ei1), p(ei2), . . . , p(ei|A|) are the initial probabilities of the selected tuples in A,
p(ei1)󸀠, p(ei2)󸀠, . . . , p(ei|A|)󸀠are the corresponding changed probabilities. p(ei1)󸀠, p(ei2)󸀠,
. . . , p(eis)󸀠correspond to the tuples in r1, and p(eis+1)󸀠, p(eis+2)󸀠, . . . , p(ei|A|)󸀠corres-
pond to the tuples in r2. f1 is the function of the probability of r1 with respect to
p(ei1)󸀠, p(ei2)󸀠, . . . , p(eis)󸀠, and f2 is the function of the probability of r2 with respect to
p(eis+1)󸀠, p(eis+2)󸀠, . . . , p(ei|A|)󸀠.
Authenticated
:27 PM

11.4 Sensitivity Computation for Answer Ordering Change
279
(2) Optimization
In the constrained optimization problem, we can decide whether the constraints can
be satisﬁed beforehand, and therefore avoid the time cost of applying the Newton’s
method. For example, in the constrained optimization problem (11.23), suppose that
the original probability of r1 is greater than the probability of r2, we can compare the
value of f1(p(ei1)󸀠, p(ei2)󸀠, . . . , p(eis)󸀠) when p(ei1)󸀠, p(ei2)󸀠, . . . , p(eis)󸀠) are all set to 0 and
the value of f2(p(eis+1)󸀠, p(eis+2)󸀠, . . . , p(ei|A|)󸀠) when p(eis+1)󸀠, p(eis+2)󸀠, . . . , p(ei|A|)󸀠are all
set to 1. If the former is greater than the latter, then the selected tuples in A cannot
reverse the order of r1 and r2 due to the monotonicity of the answer probability with
the basic tuples’ probabilities, and the time cost of applying the Newton’s method is
avoided.
11.4.1.5 Module 5 – Computing the Sensitivity Degree
(1) Basic Approach
The sensitivity degree can be computed by using formulas (11.16) and (11.17).
(2) Optimization
For the basic event e which only occurs in result ri, the computation of sensitivity
degree for e can be further simpliﬁed. Suppose that when the probability of e changes
to 1, the probability of ri changes to
p(ri) = p0(ri) + 𝜕p(ri)
𝜕p(e) (1 – p0(e))
and ri is changed to the i󸀠th position; when the probability of e changes to 0, the
probability of ri changes to
p(ri) = p0(ri) + 𝜕p(ri)
𝜕p(e) (–p0(e))
and ri is changed to the i󸀠󸀠th position. Then,
Sensitivity(e)
= 𝜕p(ri)
𝜕p(e) max(
i–1
max
x=i󸀠
1
p0(rx) – p0(ri)W(i, x),
i󸀠󸀠
max
x=i+1
1
p0(ri) – p0(rx)W(i, x))
(11.24)
where W(i, x) = 2 ∑max(i,x)–1
y=min(i,x) wy.
11.4.2 Sensitivity Computation Method
To solve the sensitivity analysis problem, an optimal algorithm calculates all possible
combinations of M tuples, and compute their sensitivity degrees, and then select the
Authenticated
:27 PM

280
11 Analyzing Sensitivity of Answer Ordering Change
M5
M2
M3
(
M5
M2
M1
M1
M1
(a)
(b)
(c)
(d)
M5
M4
M1
M5
M4
M3
)*
(
)*
(
)*
(
)*
Figure 11.3: The computation modules for ordering change sensitivity analysis: (a) CGA algorithm
for list-oriented analysis; (b) CGA algorithm for element-oriented analysis; (c) NCGA algorithm for
list-oriented analysis; and (d) NCGA algorithm for element-oriented analysis.
M tuples with the largest sensitivity degree. The optimal algorithm (OA) is exponential
on M. The cumulative greedy algorithm (CGA) starts with an empty set A, and in each
iteration, adds tuple ti not already in A such that A ∪{ti} has the largest sensitivity
degree, stopping when A contains M tuples. In contrast, the noncumulative greedy
algorithm (NCGA) starts with an empty set A, and in each iteration, adds tuple ti not
already in A such that {ti} has the largest sensitivity degree, stopping when A contains
M tuples.
Figure 11.3 illustrates the modules for the list/element-oriented ordering change
sensitivity analysis. Arrows represent calling relationship between modules, and “∗”
represents a loop where each iteration computes the sensitivity degree of the selected
basic tuples.
11.4.3 Performance
We evaluated our modular approach for sensitivity analysis in probabilistic data-
bases on both synthetic and real dataset. All the relations were stored in MySQL.
We implemented the experiments on a machine with a 2.70 GHz Core 2 Duo CPU and
2 GB RAM.
11.4.3.1 Experiments on Synthetic Data
(1) Datasets
We synthesized 20M to 100M TPC-H dataset augmented with tuple uncertainty for
each tuple. The tuple existence probabilities were generated randomly in [0, 1] in a
uniform distribution.
(2) Queries and Ordering-Sensitivity Analyses
TPC-H queries Q2, Q3, Q5, Q7, and Q10 were adopted as our probabilistic top-k queries,
with all the aggregation constructs removed.
Authenticated
:27 PM

11.4 Sensitivity Computation for Answer Ordering Change
281
TPC-H aggregation query Q2 was modiﬁed as our base aggregation query, and im-
plemented ﬁve probabilistic top-k aggregation queries, involving aggregate functions
Count, Sum, Min, Max, and Avg.
For each query, we conducted seven ordering-sensitivity analyses on its query
answer, belonging to different ordering change categories, as shown in Table 11.4.
(3) Experimental Results
Accuracy of the CGA and the noncumulative algorithm. Table 11.5 shows the sensitivity
degrees of the top-5 tuples returned from the OA, the CGA, and NCGA for the sensitivity
analysis of the probabilistic top-5 query Q5. We can see that the CGA produces a better
approximation for the OA (with an error up to 8%) than the noncumulative algorithm
(with an error up to 22%).
Answer-ordering sensitivity analysis performance for different probabilistic top-k
(aggregation) queries. Figures 11.4 and 11.5 show the analysis times of the OA, the
CGA, and the noncumulative algorithm, for different top-5 and aggregation queries,
respectively. The analysis time for top-k query Q10 in Figure 11.4 is around 60 times of
Q3. It is long enough that we do not plot the analysis time of Q10 in Figure 11.4. From
the ﬁgures, we can see that the CGA takes less time than the OA, and the NCGA takes
less time than the CGA.
Answer-ordering sensitivity analysis performance with/without optimization.
Figures 11.6 and 11.7 show the analysis times of the algorithms, before and after tak-
ing optimizations, for probabilistic top-5 query Q5 and probabilistic top-5 aggregation
query SUM, respectively. We can see that the running time after taking our optimiz-
ation strategies is much less than the basic algorithms without optimization for all
the seven analyses. That is because considering all possible ordering changes in a
large dataset is quite time-consuming. A speciﬁed basic tuple may only inﬂuence a
limited set of result tuples compared to the whole list. By optimization, intersections
and basic events are ignored if they are found not to contribute to the change of
top-k list by prejudging some conditions. Besides, in computing the distance of two
top-k lists, we adopt an incremental approach. In an ordered sequence of top-k lists
Table 11.5: Sensitivity degree of the tuples returned from the sensitivity analysis of the probabilistic
top-5 query Q5.
Ordering change
OA
CGA
NCGA
lChange
18.09
16.64
14.89
eUpPosi
9.09
8.33
7.17
eDownPosi
7.90
7.14
6.85
eRemainPosi
7.69
7.10
6.70
eSwapPosi
9.09
9.09
7.55
eKeepOrder
8.33
7.69
7.35
eReverseOrder
10.00
10.00
7.76
Authenticated
:27 PM

 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
Time (s)
 0
 0.01
 0.02
 0.03
 0.04
 0.05
Time (s)
 0
 20
 40
 60
 80
 100
 120
 140
Q2
Q3
Q5
Q7
Time (s)
Q2
Q3
Q5
Q7
Q2
Q3
Q5
Q7
(a)
(c)
(b)
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Figure 11.4: Answer-ordering
analysis
performance
for
different
probabilistic
top-k
queries
(DBsize = 20 M). (a) optimal algorithm; (b) cumulative greedy algorithm; and (c) noncumulative greedy
algorithm.
1000
(a)
(c)
(b)
800
600
400
200
0
Time (s)
Time (s)
Time (s)
Count
Sum
Max
Min
Avg
140
120
100
80
60
20
0
40
Count
Sum
Max
Min
Avg
0.05
0.04
0.03
0.02
0.01
0
Count
Sum
Max
Min
Avg
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Figure 11.5: Answer-ordering analysis performance for different probabilistic top-k aggregation quer-
ies (DBsize = 20 M). (a) optimal algorithm; (b) cumulative greedy algorithm; and (c) noncumulative
greedy algorithm.
Authenticated
:27 PM

 0
 1000
 2000
 3000
 4000
 5000
(a)
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Time (s)
Basic
Optimal
 0
 50
 100
 150
 200
 250
 300
 350
 400
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Time (s)
Basic
Optimal
 0
 0.02
 0.04
 0.06
 0.08
 0.1
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Basic
Optimal
Time (s)
(b)
(c)
Figure 11.6: Time costs of probabilistic top-5 query Q5 before and after optimizations for differ-
ent analyses. (a) optimal algorithm; (b) cumulative greedy algorithm; and (c) noncumulative greedy
algorithm.
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
(a)
(c)
(b)
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Time (s)
Basic
Optimal
 0
 50
 100
 150
 200
 250
 300
 350
 400
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Time (s)
Basic
Optimal
 0
 0.05
 0.1
 0.15
 0.2
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Time (s)
Basic
Optimal
Figure 11.7: Time costs of probabilistic top-5 aggregation query SUM before and after optimizations
for different analyses. (a) optimal algorithm; (b) cumulative greedy algorithm; and (c) noncumulative
greedy algorithm.
Authenticated
:27 PM

284
11 Analyzing Sensitivity of Answer Ordering Change
0
Data size (M)
Data size (M)
Time (ms)
Time (ms)
Time (ms)
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseorder
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseorder
The value of K
The value of K
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseorder
Time (ms)
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseorder
200
20
400
600
800
(a)
(b)
(c)
(d)
0
200
400
600
0
200
400
600
800
250
150
50
0
40
60
80
100
20
40
60
80
100
20
40
60
80
100
20
40
60
80
100
Figure 11.8: Scalability performance of ordering-sensitivity analysis with the CGA algorithm, where
(a), (b) under different DB sizes, and (c), (d) under different K values. (a) Top-5 query Q5; (b) Top-
5 aggregation query SUM; (c) Query Q5 (DB Size = 100 M); and (d) aggregation query SUM (DB
Size = 100 M).
TopK0, . . . TopKn stemming from the change of an basic event’s probability, the dis-
tance D(TopK0, TopKi) of TopK0 and TopKi(i > 1) can be computed quickly by adding
a small increment to the nearby distance result D(TopK0, TopKi–1). All these strategies
can contribute to reduce the time costs of analysis.
Scalability of answer-ordering sensitivity analysis. Figure 11.8(a) and (b) shows the
analysis time of probabilistic top-5 query Q5 and probabilistic top-5 aggregation query
SUM which adopt all the optimizations under different database sizes. In both queries,
eKeepOrder and eReverseOrder have a linear time complexity, and other analyses have
a near linear time complexity. The basic tuples occurring in multiple result tuples’
lineages lead to the derivation from time linearity. Figure 11.8(c) and (d) shows the
analysis time of probabilistic top-k query Q5 and probabilistic top-k aggregation query
SUM with different K values. The experimental results show that the time cost is linear
for eKeepOrder and eReverseOrder and near linear for all other queries as well.
Comparison with the set semantic-based implementation. The ordering change
sensitivity analysis can also be implemented based on the set semantic proposed in
Authenticated
:27 PM

11.4 Sensitivity Computation for Answer Ordering Change
285
 0
 1000
 2000
 3000
 4000
 5000
 6000
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Time (s)
Set semantic-based
implementation
Our implementation
Set semantic-based
implementation
Our implementation
Set semantic-based
implementation
Our implementation
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
lChange
eUpPosi
eDowanPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Time (s)
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
Time (s)
lChange
eUpPosi
eDowanPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
(a)
(c)
(b)
Figure 11.9: Comparison with the set semantic-based implementation under algorithm CGA
(DB size = 20 M). (a) Top-5 query Q5; (b) top-5 aggregation query SUM; (c) top-5 aggregate query (MAX).
Ref. [22], by analyzing the change of the top-1 set, the top-2 set and so on until the top-
k set. For example, for the lChange ordering change, the change of the answer lists
can be measured by the weighted sum of the change of the top-i set (1 ≤i ≤K); for the
eReverseOrder(R, R󸀠, R.r, R.r󸀠) ordering change, if R.r is ranked before R.r󸀠in R, then by
looking at the top-i set (1 ≤i ≤K) successively we can decide whether R.r occurs before
or after R.r󸀠in R󸀠, that is, whether eReverseOrder(R, R󸀠, R.r, R.r󸀠) occurs. Figure 11.9
compares the running times of the set semantic-based implementation and our imple-
mentation, for the probabilistic top-5 query Q5 and the probabilistic top-5 aggregation
query SUM, respectively. We can see that the running time of our implementation is
much less than the set semantic-based implementation, with at most 2.0% for the
probabilistic top-5 query Q5 and 2.1% for the probabilistic top-5 aggregation query
SUM, 2.4% for the probabilistic top-5 aggregation query MAX.
11.4.3.2 Experiments on Real Data
(1) Dataset
The real dataset integrates MovieLens data [32], with actors and directors informa-
tion from IMDB movies database. MovieLens data records 100,000 ratings (1–5) given
Authenticated
:27 PM

286
11 Analyzing Sensitivity of Answer Ordering Change
by 943 users on 1682 movies. Based on the ratings from different users, we com-
puted the probabilities that movies are liked by females and males and produced two
probabilistic tables with 1682 rows in each table.
(2) Queries and Ordering-Sensitivity Analyses
Three top-k queries are issued on the database:
RQ1: Find the movies which are liked by females and males.
RQ2: Find the years where at least one movie is liked by females.
RQ3: Find the actors whose movies are liked by females and males.
Two top-k aggregation queries are issued on the database:
AQ1: Find the years which have the largest sum (avg, max, min) prices of movies
liked by females, denoted as RSUM (RAVG, RMAX, RMIN), respectively, and
AQ2: Find the years which have the largest count of movies liked by females, denoted
as RCOUNT.
The following seven analyses were conducted on the movie data.
 0
 50
 100
 150
 200
 250
 300
 350
 400
R1
R2
R3
Time (s)
 0
 5
 10
 15
 20
R1
R2
R3
Time (s)
 0
 0.002
 0.004
 0.006
 0.008
 0.01
R1
R2
R3
Time (s)
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
(a)
(c)
(b)
Figure 11.10: Answer-ordering analysis performance for different probabilistic top-k queries on real
data. (a) optimal algorithm; (b) cumulative greedy algorithm; and (c) noncumulative greedy algorithm.
Authenticated
:27 PM

11.4 Sensitivity Computation for Answer Ordering Change
287
0
200
400
600
800
1000
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Time (s)
Time (s)
 0
 10
 20
 30
 40
 50
 60
 70
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
Time (s)
eReverseOrder
0
0.005
0.01
0.015
0.02
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Basic
Optimal
Basic
Optimal
Basic
Optimal
(a)
(c)
(b)
Figure 11.11: Time costs of probabilistic top-5 query RQ2 before and after optimizations for differ-
ent analyses. (a) optimal algorithm; (b) cumulative greedy algorithm; and (c) noncumulative greedy
algorithm.
(3) Experimental Results
Figure 11.10 shows the time costs of different sensitivity analyses for the three probabil-
istic top-5 queries (RQ1, RQ2, RQ3). Figure 11.11 compares the time costs of probabilistic
top-5 query RQ2 before and after optimizations for different analyses. The effect of op-
timization in Figure 11.11 is more obvious than that in Figure 11.6. The reason is that
many tuples in RQ2 appear only once, which can be processed quickly in our optimiz-
ation strategy. For all the seven analyses, the time costs are signiﬁcantly reduced. We
get the similar conclusion from the top-k aggregation queries, and the corresponding
ﬁgures are omitted here.
Figure 11.12 compares the time costs of our approach and the set semantic-based
approach for the answer-ordering sensitivity analysis on real data. We can see that the
time cost of our approach is at most 2.2% of the set semantic-based approach for the
top-5 query RQ2, and 2.1% for the top-5 aggregation query RSUM, 2.0% for the top-5
aggregation query RMAX.
In summary, the experimental results on both synthetic data and real data show
that the optimization of our modular approach for answer ordering sensitivity analysis
can improve the computational efﬁciency greatly.
Authenticated
:27 PM

288
11 Analyzing Sensitivity of Answer Ordering Change
0
500
1000
1500
2000
2500
3000
3500
4000
(a)
lChange
eUpPosi
eDownPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Time (s)
0
1000
2000
3000
4000
5000
lChange
eUpPosi
eDowanPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Time (s)
 0
 1000
2000
3000
4000
5000
lChange
eUpPosi
eDowanPosi
eRemainPosi
eSwapPosi
eKeepOrder
eReverseOrder
Time (s)
1000
Set semantic-based
implementation
Our implementation
Set semantic-based
implementation
Our implementation
Set semantic-based
implementation
Our implementation
(b)
(c)
Figure 11.12: Comparison with the set semantic-based implementation on real data under algorithm
CGA. (a) Top-5 query RQ2; (b) top-5 aggregation query (RSUM); (c) top-5 aggregation query (RMAX).
11.5 Recapitulation
The problem of sensitivity analysis for answer ordering in probabilistic databases was
casted into list-oriented or element-oriented questions. Five basic processing modules
with optimizations were given for resolving the sensitivity analysis problem. While
the optimal algorithm can always ﬁnd the M tuples which are most inﬂuential to the
answer ordering, it is exponential. The cumulative algorithm and the noncumulat-
ive algorithm make a tradeoff between accuracy and efﬁciency. The former performs
better with respect to the accuracy, and the latter performs better with respect to the
efﬁciency. The CGA produces an error up to 8%, and the noncumulative produces
an error up to 22%. The running time of the NCGA is at most 1% of the CGA for all
the seven sensitivity analyses. The sensitivity analysis based on the approach could
be extended to other probabilistic top-k queries (e.g., Refs. [26, 27, 38]), uncertain K-
nearest neighbor queries (e.g., Ref. [2]), uncertain K-probable nearest neighbor quer-
ies (e.g., Ref. [3]), and other ordering concerned probabilistic queries proposed in the
literature.
Authenticated
:27 PM

Literature
289
The next chapter aims to leverage users, personal knowledge to clarify some un-
certain information so that the query engine can scrub the result and re-generate an
improved query answer.
Literature
[1]
P. Agrawal, O. Benjelloun, A. D. Sarma, C. Hayworth, S. Nabar, T. Sugihara, and J. Widom. Trio: A
system for data, uncertainty, and lineage. In Proc. of VLDB, 2006.
[2]
T. Bernecker, H. P. Kriegel, N. Mamoulis, M. Renz, and A. Zueﬂe. Scalable probabilistic similarity
ranking in uncertain databases. TKDE, 22(9):1234–1246, 2010.
[3]
G. Beskales, M. Soliman, and I. Ilyas. Efﬁcient search for the top-k probable nearest neighbors
in uncertain databases. In Proc. OF VLDB, 2008.
[4]
P. Buneman, S. Khanna, and W. C. Tan. Why and where: A characterization of data provenance.
In Proc. of ICDT, 2001.
[5]
E. Castillo, J. M. Gutierrez, and A. S. Hadi. Sensitivity analysis in discrete bayesian networks.
IEEE Transactions on Systems, Man and Cybernetics, 1997.
[6]
H. Chan and A. Darwiche. Sensitivity analysis in Markov networks. In Proc. of IJCAI, 2005.
[7]
H. Chan and A. Darwiche. Sensitivity analysis in bayesian networks: From single to multiple
parameters. In Proc. of UAI, 2004.
[8]
A. Chapman and H. V. Jagadish. Why not?. In Proc. of ACM SIGMOD, 2009.
[9]
J. Cheney, L. Chiticariu, and W. C. Tan. Provenance in databases: Why, how, and where.
Foundations and Trends in Databases, 1(4):379–474, 2009.
[10]
H. Chockler and J. Y. halpern. Responsibility and blame: A structural-model approach. Journal of
Artiﬁcial Intelligence Research, 22(2004):93–115, 2004.
[11]
V M. H. Coupe and L. C. van der Gaag. Practicable sensitivity analysis of bayesian belief
networks. In Proc. of the 6th Prague Symposium of Asymptotic Statistics and the 13th Prague
Conf. on Information Theory, Statistical Decision Functions and Random Processes, 1998.
[12]
Y. Cui, J. Widom, and J. L. Wiener. Tracing the lineage of view data in a warehousing enviroment.
TODS, 25(2):179–227, 2000.
[13]
N. Dalvi and D. Suciu. Efﬁcient query evaluation on probabilistic databases. VLDB Journal,
16(1):523–544, 2007.
[14]
G. Das, V. Hristidis, N. Kapoor, and S. Sudarshan. Ordering the attributes of query results. In
Proc. of ACM SIGMOD, 2006.
[15]
R. Fagin, R. Kumar, and D. Sivakuma. Comparing top k lists. SIAM Journal on Discrete
Mathematics, 17(1):134–160, 2003.
[16]
T. J. Green, G. Karvounarakis, and V. Tannen. Provenance semirings. In Proc. of PODS, 2007.
[17]
J. Y. Halpern and J. Pearl. Causes and explanations: A structural model approach-part i: Causes.
In Proc. of UAI, 2001.
[18]
J. Huang, T. Chen, A. Doan, and J. F. Naughton. On the provenance of non-answer to queries over
extracted data. In Proc. of VLDB, 2008.
[19]
T. S. Jayram, S. Kale, and E. Vee. Efﬁcient aggregation algorithms for probabilistic data. In Proc.
of SODA, 2007.
[20] F V. Jensen. Gradient descent training of bayesian networks. In Proc. of the Fifth European Conf.
on Symbolic and Quantitative Approaches to Reasoning with Uncertainty, 1999.
[21]
B. Kanagal and A. Deshpande. Lineage processing over correlated probabilistic databases. In
Proc. of ACM SIGMOD, 2010.
[22] B. Kanagal, J. Li, and A. Deshpande. Sensitivity analysis and explanations for robust query
evaluation in probabilistic databases. In Proc. of ACM SIGMOD, 2011.
Authenticated
:27 PM

290
11 Analyzing Sensitivity of Answer Ordering Change
[23] U. Kjarulff, and L. C. van der Gaag. Making sensitivity analysis computationallly efﬁcient. In
Proc. of UAI, 2000.
[24] K. B. Laskey. Sensitivity analysis for probability assessments in bayesian networks. IEEE
Transctions on Systems, Man and Cybernetics, 25(7):901–909, 1995.
[25]
J. B. Lasserre. Exact formula for sensitivity analysis of Markov chains. Optimization Theory and
Applications, 71(2):407–413, 1991.
[26] X. Lian and L. Chen. Probabilistic inverse ranking queries in uncertain databases. In Proc. of
VLDB, 2011.
[27]
X. Lian and L. Chen. Shooting top-k stars in uncertain databases. In Proc. of VLDB, 2011.
[28] A. Meliou, W. Gatterbauer, J. Halpern, C. Koch, K. F. Moore, and D. Suciu. Causality in
databases. IEEE Data Engineering Bulletin, 33(3):59–67, 2010.
[29] A. Meliou, W. Gatterbauer, K. F. Moore, and D. Suciu. Why so? Or why no? Functional causality
for explaining query answers. In Proc. of MUD, 2009.
[30] A. Meliou, W. Gatterbauer, K. F. Moore, and D. Suciu. The complexity of causality and
responsibility for query answers and non-answers. In Proc. of VLDB, 2010.
[31]
J. J. Montaˇno and A. Palmer. Numeric sensitivity analysis applied to feed forward neural
networks. Neural Computing and Applications, 12:119–125, 2003.
[32] Movielens data. http://www.grouplens.data, 2017.
[33] R. Murthy, R. Ikeda, and J. Widom. Making aggregation work in uncertain and probabilistic
databases. TKDE, 23(8):1261–1273, 2011.
[34] C. Re, N. Dalvi, and D. Suciu. Efﬁcient top-k query evaluation on probabilistic data. In Proc. of
ICDE, 2007.
[35]
C. Re and D. Suciu. Approximate lineage for probabilistic databases. In Proc. of VLDB, 2008.
[36] S. Renooij. Efﬁcient sensitivity analysis in hidden Markov models. International Journal of
Approximate Reasoning, 53(9):1397–1414, 2012.
[37]
A. Saltelli, M. Ratto, T. Andres, F. Campolongo, J. Cariboni, D. Gatelli, M. Saisana, and S.
Tarantola. Global Sensitivity Analysis, John Wiley & Sons, England, 2008.
[38] M. A. Soliman, I. F. Ilyas, and K. C. C. Chang. Top-k query processing in uncertain databases. In
Proc. of ICDE, 2007.
[39] M. A. Soliman, I. F. Ilyas, and D. Martinenghi. Ranking with uncertain scoring functions:
Semantics and sensitivity measures. In Proc. of ACM SIGMOD, 2011.
[40] C. H. Tan and J. C. Hartman. Sensitivity analysis in Markov decision processes with uncertain
reward parameters. Journal of Applied Probability, 48(4):954–967, 2011.
[41]
J. W. Trio: A system for integrated management of data, accuracy, and lineage. In Proc. of CIDR,
2005.
Authenticated
:27 PM

12 Explaining and Scrubbing Context-Aware
Query Results
Abstract: A context-aware query engine works with huge amounts of uncertain data,
coming from sensor networks, subjective preferences, statistical data analysis, ma-
chine learning and data mining, information extraction and integration, etc. As a
consequence, the answers to a context-aware query may be incorrect, possibly mak-
ing the query system questionable and unacceptable by its end-users. In order to help
users to assess the given answers, it is desirable for context-aware systems to give an
explanation on how it arrives at an answer and on which probabilistic assumptions
the derived answer is based. In this way, the user based on his/her knowledge about
uncertain information can not only decide how much conﬁdence to be placed on the
query engine, but also help clarify some uncertain information so that the query en-
gine could scrub the answer and regenerate an improved query result. This chapter
ﬁrst reviews existing result explanation and uncertain data cleaning techniques, and
then gives a framework of involving users in querying uncertain context database.
Two issues associated with such a query framework, i.e., (1) how to interact with a
user for answer explanation and uncertainty clariﬁcation without bringing much bur-
den to the user, and (2) how to scrub (correct) query results without incurring much
computation overhead to the query engine, are particularly addressed.
12.1 Motivation
Substantial research has been done in the literature for representing and query-
ing uncertain data. A community of efforts have centered around probabilistic data
management [8]. Complementary to the fruitful work upon probabilistic data mod-
els [1, 14, 27] and querying [12], another line of research conducted recently aims
to improve the quality of query answers by cleaning uncertain data. Techniques de-
veloped include detecting and merging duplicates by assigning a probability to each
tuple in the clean database [2], consistent query answers in inconsistent probabilistic
databases [22], and eliminating data and thus result ambiguity by probing some data
sources like sensors and changing their probabilistic values into certain ones which
have either 1.0 or 0.0 probability [10].
Along with consistently emerging uncertain data due to various reasons, we be-
lieve that various constructive methods of error correctness as above for resolution of
data ambiguity through mediation are necessary and desirable to advance the ﬁeld of
uncertain context data management.
DOI 10.1515/9783110556674-012
Authenticated
:27 PM

292
12 Explaining and Scrubbing Context-Aware Query Results
12.2 Result Explanation and Uncertain Data
Cleaning Techniques
12.2.1 Explanation in General
The notion of explanation is very broad. Russell’s book on artiﬁcial intelligence [26]
deﬁnes an explanation of a sentence P as a set E of sentences that together imply P. If
the sentences are known to be true, they can be used to prove P. But explanations can
also include assumptions; sentences that would sufﬁce to prove P if they were true. In
most cases, an explanation is preferred to be minimal, meaning that no proper subset
of it is also an explanation.
Explanation takes two roles in the area of information systems. Either the sys-
tem provides knowledge and explanations necessary for the user to carry out his task,
or alternatively the system carries out some action and then explains the need and
reason for the action that the system itself has taken to the user [16]. As a result,
most of the explanation facilities were offered by those expert systems that used rule-
based reasoning to arrive at conclusions. An early backward chaining expert system
(MYCIN) [5] was a prominent example. Buchanan et al. [5] outlined ﬁve speciﬁc reas-
ons for an explanation from an expert system perspective, namely: understanding,
debugging, education, acceptance, and persuasion. MYCIN realizes explanations by
translating traces of applied rules from the LISP (List Processing) language to English,
with these two outstanding options: why the expert system asks the user a certain
question, and how the system deduces a certain conclusion.
In recommender systems [31], explanation is exploited to achieve the following
seven targets: transparency, explaining how the system works; scrutability, allowing
users to tell the system it is wrong; trust, increasing the user’s conﬁdence in the sys-
tem; effectiveness, helping users make good decisions; persuasiveness, convincing
users to try or buy the recommended items; efﬁciency, helping users make decisions
faster; and satisfaction, increasing the ease of usability or enjoyment.
In addition, there have also been considerable studies into cognitive psychology
and philosophy of questioning and question answering with humans and how explan-
ation can be applied to human–computer interaction [21, 29]. Most of the research in
this area is concerned with language analysis of questions and generation of answers
in a natural language.
In a logical setting, Kalyanpur et al. [17] researched how to provide explanations
to justify why an ontology entails a certain statement. Let it sufﬁce here to say that for
simple cases the system could explain its reasoning step-by-step by explaining a mem-
bership with the relevant deﬁnitions from the ontology, until it arrives at stored data
(that is unexplainable, since it is asserted). This may lead to a considerable amount of
steps.
In the more speciﬁc setting of exploiting user preferences in query answering [7],
there is also the problem of the amount of data that can contribute to the belief that
answer is preferred.
Authenticated
:27 PM

12.2 Result Explanation and Uncertain Data Cleaning Techniques
293
Furthermore, the problem is even more complex since there is also uncertainty
involved, and elements down in the explanation tree might have more inﬂuence than
the top elements for some typical examples.
12.2.2 Explanation in Databases
In the database ﬁeld, answer explanations are traditionally concerned with query
plans and path selections. The purpose is mainly to tune applications to take advant-
age of indices, instead of explaining why an answer is returned.
The ﬁrst work that considered the data provenance (or lineage) as a problem in
itself was the work of Buneman et al. [6] and Cui et al. [11]. The former work dif-
ferentiates between the sources that inﬂuence the answer (why-provenance) and the
locations in source databases where the answer is extracted from (where-provenance).
For explaining answers under uncertainty, a ﬁrst step is the translation of the
work on lineage to probabilistic databases by Benjelloun et al. [4], as implemented
in the Trio system [24]. Their approach makes it possible to identify for each tuple in
an answer set the complete set of source data items that produced the tuple.
Nevertheless, astute readers may argue that presenting all base tuples in the data-
base related to an answer tuple may not be practical and user-friendly, particularly in
situations where the user has or wants limited amount of time to examine the result,
and the underlying uncertain base tuples are overwhelming. In other words, an an-
swer explanation provided by the system must be concise and vital enough, so that
users can quickly and easily justify the query answer based on their own knowledge.
Das et al. [13] did good research in this direction, focusing on concise answers.
Their contribution is a method to select the top-m attributes of a query. The top-m is
identiﬁed as those attributes that best explain the ranking of an answer based on a
score function. They differentiate between the top-k answers that are presented to the
user and the other answers to the query that are not presented. Based on this distinc-
tion, they consider three measures of explanatory power of an attribute: its inﬂuence
on the scores, its inﬂuence on the ranks (compared to the answers that are not in the
top-k), and its inﬂuence on the relative rank (compared to the other answers in the
top-k).
Kießling et al. [20] conﬁrmed the need for explaining results of preferences, be-
cause preferences lead to nonexplicit query results. They focused on justiﬁcation
by introducing functions that indicate to which degree a certain item matches the
preferences.
12.2.3 User Feedback
User relevance feedback is widely studied in the information retrieval ﬁeld due to
the inherent ambiguity in users’ queries and redundancy in retrieved (multimedia)
Authenticated
:27 PM

294
12 Explaining and Scrubbing Context-Aware Query Results
documents. During the iteration of feedback, users were usually required to rate some
of the retrieved results according to certain criteria or preferences. The systems then
updated the matching criteria and returned new results which were more desirable
to the users [28, 30]. Researchers in the ubiquitous computing ﬁeld also advocated
user feedback in context-aware applications, since some human aspects of context
information cannot be sensed or even inferred by technological means [3].
12.2.4 Cleaning Data Uncertainty
Eliminating data uncertainty starts to receive attention recently. Andritsos et al. [2]
merged duplicate tuples by assigning a probability to each tuple in the clean data-
base. Khoussainova et al. [19] extended this probabilistic approach to streaming data,
and explored integrity constraints to deal with missing and conﬂicting values. Jeffrey
et al. [15] recovered missing and unreliable sensor readings through temporal and spa-
tial aggregation. The authors in Refs. [9, 10] further presented an entropy-based PWS
(Possible World Semantics)-quality metric to quantify the impreciseness of query an-
swers, and eliminating data and thus result ambiguity by probing some data sources
and changing their probabilistic values into certain ones which have either 1.0 or 0.0
probability aiming to achieve the maximal improvement in PWS quality.
12.3 Design Principles for Answer Explanation
Facility
Building an explanation facility can beneﬁt systems and users in various ways. To ﬁll
the gap between giving only an answer probability (non explanation), and giving the
full answer derivation (over-explanation), some criteria are needed to set the border.
The role that explanations play in the human–computer interaction determines
the principles which guide the deﬁnition and the associated quality measurement.
Since the database has uncertain data and produces uncertain answers, both of which
could be wrong, the role of our explanation facility is thus aiming at justiﬁcation.
This bears most similarity to understanding and debugging from an expert system
perspective, as well as scrutability from a recommender system perspective.
Since a user is more likely to recognize mistakes in simple, basic assumptions
leading to an answer, than mistakes in the answer itself, by revealing possibly wrong
or correct assumptions used in answer derivation, the system could let the user decide,
based on his knowledge about these assumptions, how much conﬁdence to place in
the ﬁnal derived answer. This explanation can also help enforce the acceptance of the
system, since if the user understands why an answer is wrong, he would not question
much about the behavior of the system.
In view of its role for justiﬁcation, the following four considerations are taken into
account in building the answer explanation facility.
Authenticated
:27 PM

12.4 Involving Users in Querying Uncertain Context Data
295
Veriﬁability. In response to the justiﬁcation requirement, the contents of an
explanation provided must be veriﬁable by the users.
Concision. From a user’s viewpoint, a long list of explanation contents for an an-
swer is normally undesirable and overwhelming, particularly when the user has lim-
ited amount of time to examine the query result. Therefore, the explanation provided
must be concise enough, standing in between the traditional nonexplanation and
over-explanation.
Inﬂuenceability. Closely related to the previous concision requirement, the ex-
planation contents offered must have a maximal inﬂuence upon the answer. That is, if
these contents are changed, there is a high chance that the answer will be affected and
changed accordingly. There is no use in explaining something whose change makes no
difference for the answer.
Doubt. The contents of an explanation preferably exhibits a certain degree of un-
certainty. In other words, there is a relatively high chance that these contents may be
wrong under certain circumstances. It makes no sense to explain something to the
user, while the system is pretty sure about its correctness.
12.4 Involving Users in Querying Uncertain
Context Data
This section explores a particular way of user feedback and involvement in order to
deal with uncertain data in databases. The proposal is based on the fact that users
are actually used to and highly successful in coping with uncertain data through-
out daily lives [3], as most human knowledge in the real world is uncertain. Besides
probing sensors for correct values to deliver the most sensible answer, we propose to
display this uncertainty explicitly and leverage user’s personal knowledge to clarify
the uncertainty. In this way, the query engine can recompute the query and tailor its
probabilistic query result towards a better quality from the perspective of the speciﬁc
user. More importantly, by opening the black-box of the query engine and explaining
to the user how it comes up with the answer and which uncertain tuples it is based on,
the user with his/her knowledge can decide how much conﬁdence to be placed on the
system, thus enhancing both the intelligibility of system behavior and accountability
of human users.
A framework of involving users in uncertain database querying is sketched in
Figure 12.1, which proceeds in two steps: (1) Computing Query Result with Explanation,
User’s uncertainy
clarification
Recompute query result
with explanation
Query
Phase I
Compute query result
with explanation
Prob. answer
With explanation
Phase II
Result
Figure 12.1: A framework of involving users in uncertain database querying.
Authenticated
:27 PM

296
12 Explaining and Scrubbing Context-Aware Query Results
and (2) Recomputing Query Result with Explanation with user’s uncertainty clariﬁca-
tion. Two questions critical to such an uncertain database querying framework are the
following.
Question 1: How to interact with the user for result explanation and uncertainty
clariﬁcation without bringing much burden on the user?
As a probabilistic result is usually derived from a large volume of uncertain tuples
in the probabilistic database, it is neither practical nor user-friendly to present to
the user all the basic uncertain tuples in the database related to resulting tuples for
his/her clariﬁcation. In other words, the answer explanation provided by the system
must be concise enough, standing in between the traditional no-explanation and over-
explanation, so that the user can quickly and easily justify the query answer and
underlying uncertain tuples based on his/her personal knowledge. Besides, the ex-
planation content offered must have a high inﬂuence upon the answer. That is, if these
contents are changed, there is a high chance that the answer will be affected and
changed accordingly. There is no use to explain something whose change makes no
difference for the answer.
R ́e and Suciu have made a great investigation into approximate lineage semantics
and computation, with an aim to compress probabilistic databases for efﬁciently
querying and data understanding [25]. In this work, two instantiations of approx-
imate lineage were developed. The sufﬁcient lineage identiﬁes the most inﬂuential
derivations for each result tuple, and the polynomial lineage maps each result tuple
to a real-valued polynomial on Boolean variables based on Fourier approximation of
Boolean expressions.
Question 2: How to correct the query result after user’s uncertainty clariﬁcation
without incurring much computing overhead on the query engine?
Based on user’s input about data uncertainty, the query engine recomputes the
query. Efﬁcient techniques need to be developed to allow faster processing of the
query than from scratch.
12.4.1 Result Explanation
12.4.1.1 Preliminaries
In a tuple-wise probabilistic database relation [12], each tuple can be regarded as
a description of a basic probabilistic event and associated with an explicitly-given
event identiﬁer. Tuples within a relation as well as among relations are assumed to
be independent. Figure 12.2 illustrates three probabilistic relation examples - Movie,
NLike, and SLike. Tuples in the deterministic relation Movie are associated with an
event identiﬁer, denoted as T, to indicate a deterministic event with probability 1.0.
In the NLike relation, the ﬁrst tuple describes a probabilistic event of EventID=x1 that
a movie of MovieID=1 is liked by people from northern regions with a probability of
0.2 (in terms, 20% of people from northern regions in a poll give a high rating to this
Authenticated
:27 PM

12.4 Involving Users in Querying Uncertain Context Data
297
Relation Movie
EventID
Prob.
MovieID
Title
ReleaseYear
T
1.0
1
Treasure-house
1948
T
1.0
2
Raw Deal
1948
T
1.0
3
He Walked by Night
1948
T
1.0
4
Roman Holiday
1953
T
1.0
5
Band Wagon
1953
Relation N Like
EventID
Prob.
MovieID
x1
0.2
1
x2
0.1
2
x3
0.7
3
x4
0.9
4
x5
0.2
5
Figure 12.2: A probabilistic database example.
Relation S Like
EventID
Prob.
MovieID
y1
0.7
1
y2
0.9
2
y3
0.2
3
y4
0.8
4
y5
0.4
5
movie). Similarly, each tuple in relation SLike represents a probabilistic event that a
movie is liked by people from southern regions.
Each result tuple in a derived relation would be assigned an event expression
which is a Boolean combination of the event identiﬁers corresponding to the base
tuples from which it was derived. The authors in Ref. [12] described how to pro-
duce event expressions for each result tuple during the query evaluating process.
The probabilities of the result tuples can be computed from the corresponding event
expressions.
Throughout the chapter, the following two typical queries upon the example
database in Figure 12.2 are used to illustrate the key concepts.
Query 1: Look for the year(s) where at least one movie was liked by people from northern
regions, i.e.,
SELECT ReleaseYear
FROM
NLike, Movie
WHERE
NLike.MovieID=Movie.MovieID
Authenticated
:27 PM

298
12 Explaining and Scrubbing Context-Aware Query Results
Result of Query 1
Event expression
Prob.
ReleaseYear
x1 ∨x2 ∨x3
0.784
1948
x4 ∨x5
0.92
1953
Result of Query 2
Event expression
Prob.
ReleaseYear
(x1∧∼y1) ∨(x2∧∼y2) ∨(x3∧∼y3)
0.591
1948
(x4∧∼y4) ∨(x5∧∼y5)
0.278
1953
Figure 12.3: Results of (a) query 1 and (b) query 2.
Figure 12.3 shows the query result. The probabilities of the two result tuples are
computed as follows:
P(x1 ∨x2 ∨x3)
= P(x1) + P(x2) + P(x3) – P(x1 ∧x2) – P(x1 ∧x3) – P(x2 ∧x3) + P(x1 ∧x2 ∧x3)
= P(x1) + P(x2) + P(x3) – P(x1) ∗P(x2) – P(x1) ∗P(x3) – P(x2) ∗P(x3)+
P(x1) ∗P(x2) ∗P(x3)
= 0.2 + 0.1 + 0.7 – 0.2 ∗0.1 – 0.2 ∗0.7 – 0.1 ∗0.7+ 0.2 ∗0.1 ∗0.7
= 0.784,
P(x4 ∨x5)
= P(x4) + P(x5) – P(x4 ∧x5)
= P(x4) + P(x5) – P(x4) ∗P(x5)
= 0.9 + 0.2 – 0.9 ∗0.2
= 0.92.
Query 2: Look for the year(s) where at least one movie was liked by people from northern
regions but not by people from southern regions, i.e.,
SELECT ReleaseYear
FROM (NLike EXCEPT SLike) AS A, Movie
WHERE
A.MovieID=Movie.MovieID
The probabilities of the two result tuples are computed in a similar way as in Query 1.
Figure 12.3 shows the query result.
Authenticated
:27 PM

12.4 Involving Users in Querying Uncertain Context Data
299
In general, to compute the probability of an arbitrary event expression ee =
F(e1, e2, . . ., en), one can ﬁrst convert it into an equivalent disjunctive normal form
ee = C1 ∨C2 ∨⋅⋅⋅∨Cm, where Ci = ei1 ∧ei2 ∧. . . ∧eisi, ei1, ei2, . . . , eisi are event iden-
tiﬁers or negated event identiﬁers, and then apply the following inclusion–exclusion
formula [14]:
P(C1 ∨C2 ∨⋅⋅⋅∨Cm) =
m
∑
i=1
P(Ci) – ∑
i<j
P(Ci∧Cj) + ∑
i<j<k
P(Ci∧Cj∧Ck)
– ⋅⋅⋅+ (–1)(m+1)P(C1 ∧C2 ∧⋅⋅⋅∧Cm).
(12.1)
From the above discussion, we can see that the user can issue a query upon a probab-
ilistic database just as in traditional relational database, and the query plan has the
same semantics as in traditional relational database, except that an event expression
is produced for each result tuple during the query evaluating process. According to the
event expression, the probability of each result tuple can be computed. The system
can also compute the probability during the query process for each subevent expres-
sion produced by each operator, and use them as input to compute the probabilities
for the following operators. This method of computing probabilities is quite efﬁcient.
However, the probabilities computed in this way are wrong in certain circumstances.
Dalvi and Suciu [12] have analyzed the reason and pointed out that “even if all tuples
in the base relations are independent probabilistic events, the tuples in the intermedi-
ate results of a query plan have often correlated probabilities, making it impossible to
compute the new probabilities precisely.” For a query, a query plan is a safe plan if the
above method of computing probabilities during the query process can produce a cor-
rect probability value. Dalvi and Suciu [12] showed that by pushing projections early
and choosing join orders carefully, there exists a safe query plan for most queries, and
80% of TPC/H queries fall into this category.
12.4.1.2 Deﬁnitions
For an arbitrary event e, we use P(e) to denote its probability. Let e1, e2, . . . , en denote n
basic independent probabilistic events in a probabilistic database with probabilities
P(e1), P(e2), ⋅⋅⋅, P(en), respectively. ee = F(e1, e2, . . . , en) returns a complex event ex-
pression ee by the combination of the basic events e1, e2, . . . , en through the Boolean
operators ∧, ∨, and ∼. ee = e1 ∨e2 ∨e3, ee = e1 ∧e2 ∧e3, ee = e1 ∧e2∨∼e3 are such com-
plex event expression examples. The probability of ee can thus be derived from the
probabilities of basic events e1, e2, . . . , en, denoted as P(ee) = f(P(e1), P(e2),. . ., P(en)).
Example 40. For the event expression ee = e1∨e2∨e3, where e1, e2, e3 are independent,
and P(e1) = 0.2, P(e2) = 0.1, P(e3) = 0.7, we have P(ee) = P(e1 ∨e2 ∨e3) = 1 – (1 – P(e1)) ∗
(1 – P(e2)) ∗(1 – P(e3)) = 0.784.
◻
Authenticated
:27 PM

300
12 Explaining and Scrubbing Context-Aware Query Results
Deﬁnition 33. For an event expression ee = F(e1, e2, . . . , en) derived from basic events
e1, e2, . . . , en, whose probability is P(ee) = f(P(e1), P(e2), . . . , P(en)), the inﬂuence of the
basic event ei (1 ≤i ≤n) on ee is deﬁned as the absolute value of partial derivative
of P(ee) with respect to P(ei), denoted as Inﬂuence(ee, ei) = | 𝜕P(ee)
𝜕P(ei) |, representing the
change rate of P(ee) with respect to P(ei).
◻
Obviously, the change of an uncertain basic event’s probability with a high inﬂuence
value leads to a high change of the result tuple’s probability.
Example 41. In Example 40, the functional relationship between P(ee) and P(e1), P(e2),
P(e3) is P(ee) = 1 – (1 – P(e1)) ∗(1 – P(e2)) ∗(1 – P(e3)), so the inﬂuences of e1, e2, e3 can
be computed as follows:
Inﬂuence(ee, e1) = |𝜕P(ee)
𝜕P(e1) |=(1 – P(e2)) ∗(1 – P(e3)) = (1 – 0.1) ∗(1 – 0.7) = 0.27
Inﬂuence(ee, e2) = |𝜕P(ee)
𝜕P(e2) |=(1 – P(e1)) ∗(1 – P(e3)) = (1 – 0.2) ∗(1 – 0.7) = 0.24
Inﬂuence(ee, e3) = |𝜕P(ee)
𝜕P(e3) |=(1 – P(e1)) ∗(1 – P(e2)) = (1 – 0.1) ∗(1 – 0.2) = 0.72.
◻
In the realistic situations, the amount of basic events may be very large and the user
has limited amount of time to examine all of them, so it is a must for the system to
choose the k (user or system deﬁned) most inﬂuential basic events for presenting to
the user.
Deﬁnition 34. For an event expression ee = F(e1, e2, . . . , en) with probability P(ee) =
f(P(e1), P(e2), . . . , P(en)), the explanation of ee is deﬁned as ⟨ei1, P(ei1)⟩, ⟨ei2, P(ei2)⟩,
. . ., ⟨eik, P(eik)⟩, where ei1, ei2, . . ., eik have the k largest inﬂuences on ee, and k is input
by either the user or the system.
◻
Example 42. In Example 41, as Inﬂuence(ee, e3) = | 𝜕P(ee)
𝜕P(e3)| is the largest among the
three, e3 is the most inﬂuential basic event. When k = 1, the explanation of ee is
⟨e3, 0.7⟩.
◻
12.4.1.3 Computing Result Explanation
(1) Naive Solution
With the above deﬁnitions, we discuss the computation of explanation for a result
tuple, represented by event expression ee = F(e1, e2, . . . , en). Assume P(ei) (1 ≤i ≤n)
are known. A basic approach to compute the explanation of ee is ﬁrst computing the
inﬂuences of all the basic events, that is, Inﬂuence(ee, ei) = | 𝜕P(ee)
𝜕P(ei) | (1 ≤i ≤n), and
then selecting the k largest inﬂuences of basic events. To compute Inﬂuence(ee, ei) =
Authenticated
:27 PM

12.4 Involving Users in Querying Uncertain Context Data
301
| 𝜕P(ee)
𝜕P(ei) | for each ei, we need to analyze how P(ee) is computed. By ﬁrst constructing
ee’s equivalent full disjunctive normal form from its truth table, and then applying
the inclusion-exclusion formula (12.1), we can get P(ee) is a linear function of P(ei),
P(ee) = !i ∗P(ei) + "i,
(12.2)
where !i and "i are sub-expressions irrelevant to P(ei), then 𝜕P(ee)
𝜕P(ei) = !i.
Note that the event expressions studied in Ref. [25] are restricted to the disjunctive
normal form which contains no negated event identiﬁers. With this restriction, Re and
Suciu [25] gave an inﬂuence deﬁnition which is equivalent to partial computing result
of our derivative-based method. It is obvious that our partial derivative-based deﬁn-
ition is more simple and intuitive. With the help of the idea of partial derivative and
its properties, we optimize the algorithm for computing the k most inﬂuential basic
events for safe query plans into one with linear time complexity, as we will show later.
This is not a trivial result.
The detailed computing process for Inﬂuence(ee, ei) = | 𝜕P(ee)
𝜕P(ei) | using naive solution
and its relationship with Re and Suciu’s [25] deﬁnition are provided below.
(2) Details of Naive Solution for Computing Inﬂuence
For an event expression ee = F(e1, e2, . . . , en), we describe here the detailed pro-
cess of the naive solution to compute the inﬂuences of all the basic events, that is,
Inﬂuence(ee, ei) = | 𝜕P(ee)
𝜕P(ei) | (1 ≤i ≤n), and its relationship with Re and Suciu’s [25]
deﬁnition.
We construct ee’s equivalent full disjunctive normal form from its truth table. ee =
F(e1, e2, . . . , en) can also be considered as a Boolean formula, where each basic event
ei (1 ≤i ≤n) is considered as a Boolean variable. In the truth table of ee, each row
corresponds to an assignment.
Deﬁnition 35. For an event expression ee = F(e1, e2, . . . , en), an assignment > gives
a truth value True or False for each Boolean variable e1, e2, ⋅⋅⋅, en, and can be
considered as a function from the set {e1, e2, . . ., en} to the set {True, False}.
◻
Each assignment > will produce a corresponding truth value for ee, denoted as V>(ee).
>󸀠(ei) is used to represent ei if >(ei) is True, and ∼ei if >(ei) is False. The probability of
an assignment > is deﬁned as P(>) = ∏n
i=1 P(>󸀠(ei)).
Deﬁnition 36. For an assignment >, > ⊕{i} is deﬁned as an assignment which gives a
different truth value for ei, and same values for all other basic events.
◻
For an event expression ee = F(e1, e2, . . . , en), from its truth table we can construct its
equivalent full disjunctive normal form as follows:
ee =
⋁
>:V>(ee)=True
n
⋀
i=1
>
󸀠
(ei).
(12.3)
Authenticated
:27 PM

302
12 Explaining and Scrubbing Context-Aware Query Results
Since the different conjunctions in the full disjunctive normal form represent disjoint
events, and e1, e2, . . ., en are assumed to be independent, we have
P(ee) =
∑
>:V>(ee)=True
n
∏
i=1
P(>
󸀠
(ei)).
(12.4)
If >
󸀠(ei) is a negated identiﬁer ∼ei, P(>
󸀠(ei)) = 1 – P(ei). We can see that P(ee) is a
one-order function of P(ei), P(ee) = !i ∗P(ei) + "i, where !i and "i are sub-expressions
irrelevant to P(ei), then 𝜕P(ee)
𝜕P(ei) = !i.
By analyzing carefully all the assignments occurring in formula (12.3), we can get
the formula for computing | 𝜕P(ee)
𝜕P(ei) |. For an assignment > which occurs in formula (12.3),
that is, V>(ee) = True, the following three cases are considered:
(1) > and >⊕{i} occur simultaneously in eq. (12.3): in the two sum items corresponding
to > and > ⊕{i} in eq. (12.4), the coefﬁcients of P(ei) sum to zero.
(2) > occurs in eq. (12.3), but > ⊕{i} does not, and >(ei) = True : in the sum item
corresponding to > in eq. (12.4), the coefﬁcient of P(ei) is P(>) + P(> ⊕{i}).
(3) > occurs in eq. (12.3), but > ⊕{i} does not, and >(ei) = False: in this case, >
󸀠(ei) =∼
ei, P(>
󸀠(ei)) = 1 – P(ei), so in the sum item corresponding to > in formula (12.4), the
coefﬁcient of P(ei) is –(P(>) + P(> ⊕{i})).
If we use A1 to denote the set of all assignments in case 2, that is, A1 = {>
󵄨󵄨󵄨󵄨󵄨󵄨󵄨>(ei) =
True, V>(ee) = True, V>⊕i(ee) = False}, and A2 to denote the set of all assignments in
case 3, that is, A2 = {>
󵄨󵄨󵄨󵄨󵄨󵄨󵄨>(ei) = False ∨V>(ee) = True ∨V>⊕i(ee) = False}, then by
summing all the coefﬁcients of P(ei) in the sum items of formula (12.4) corresponding
to the assignments in case 2 and in case 3, we obtain
|𝜕P(ee)
𝜕P(ei) | = |∑
>∈A1
(P(>)+P(> ⊕{i}))–∑
>∈A2
(P(>)+P(> ⊕{i}))|.
(12.5)
Note that the event expressions studied in Ref. [25] are restricted to the disjunctive
normal form which contains no negated event identiﬁers. With this restriction, A2 is
empty, Re and Suciu [25] gave an inﬂuence deﬁnition which is equivalent to the-right
hand side of formula (12.5). It is obvious that our partial derivative-based deﬁnition is
more simple and intuitive.
Because the naive solution depends on the construction of a truth table, it has
an exponential time complexity O(2n), where n is the number of basic events in
ee = F(e1, e2, . . . , en). Next, we shall give the algorithm for computing explanations
for queries having a safe plan.
(3) Basic Approach for Queries with a Safe Plan
Computing the probability of a complex event expression has been proved to be either
in PTIME or #P-complete [12]. For some query expressions with independent basic
Authenticated
:27 PM

12.4 Involving Users in Querying Uncertain Context Data
303
events, it is possible to reformulate them equivalently in such a way that each event
identiﬁer does not occur repeatedly, and different subexpressions do not overlap and
are also independent. For example, ee = (e1∧e2)∨(e1∧e3) can be transformed into ee󸀠=
e1 ∧(e2 ∨e3), where e1, e2, e3 are independent. The latter’s probability can thus be com-
puted as P(ee󸀠) = P(e1∧(e2∨e3)) = P(e1)∗P(e2∨e3) = P(e1)∗(P(e2)+P(e3)–P(e2)∗P(e3)).
This kind of queries are called queries with a safe plan [12]. Eighty percent of the TPC/H
queries are queries with a safe plan. Through the above equivalent transformation, the
probability calculation of safe-plan query expressions can be efﬁciently done with a
linear time complexity [12].
Here, we focus on queries with a safe plan. Given an event expression ee contain-
ing n independent basic events e1, e2, . . . , en, which do not occur repeatedly, we can
compute P(ee) and 𝜕P(ee)
𝜕P(ei) (1≤i≤n) recursively as follows:
(1) For an event expression ei (1 ≤i ≤n) (where ei is a basic event), P(ei) is known
beforehand, 𝜕P(ei)
𝜕P(ei) = 1.
(2) For an event expression ∼ee where ee is an event expression,
P(∼ee) = 1 – P(ee), 𝜕P(∼ee)
𝜕P(ei)
= –𝜕P(ee)
𝜕P(ei) .
(12.6)
(3) For an event expression ee1 ∧ee2, where ee1 and ee2 are two event expressions,
P(ee1 ∧ee2) = P(ee1) ∗P(ee2),
𝜕P(ee1 ∧ee2)
𝜕P(ei)
=
{
{
{
𝜕P(ee1)
𝜕P(ei) ∗P(ee2), if ei occurs in ee1
P(ee1) ∗𝜕P(ee2)
𝜕P(ei) , if ei occurs in ee2
.
(12.7)
(4) For an event expression ee1 ∨ee2, where ee1 and ee2 are two event expressions,
P(ee1 ∨ee2) = P(ee1) + P(ee2) – P(ee1) ∗P(ee2),
𝜕P(ee1 ∨ee2)
𝜕P(ei)
=
{
{
{
𝜕P(ee1)
𝜕P(ei) ∗(1 – P(ee2)), if ei occurs in ee1
𝜕P(ee2)
𝜕P(ei) ∗(1 – P(ee1)), if ei occurs in ee2
.
(12.8)
Implementation Details. An event expression ee can be represented by a binary tree T
where each leaf node denotes a basic event identiﬁer and each non-leaf node denotes
an operator ∧, ∨or ∼. Figure 12.4 gives an example of a binary tree which represents
the event expression (e1∧∼e2) ∨(e3∧∼e4) ∨(e5∧∼e6).
For each node N in the binary tree T, we use LChild(N) to denote its left child,
RChild(N) to denote its right child, subTree(N) to denote the subtree which has N as
root node, and ee(N) to denote the event expression represented by subTree(N). An ad-
ditional ﬁeld is added to each node N to store P(ee(N)). Suppose en1, en2, . . . , enk are all
event identiﬁers which occur in ee(N) and correspond to the leaf nodes of subTree(N),
Authenticated
:27 PM

304
12 Explaining and Scrubbing Context-Aware Query Results
e1
e2
e4
e3
e5
e6
~
~
~
∨
∨
∧
∧
∧
Figure 12.4: Tree representation of
(e1∧∼e2) ∨(e3∧∼e4) ∨(e5∧∼e6).
∧
∨
∧
∧
∂P(ee(N)) = 0.131
∂P(e1)
∂P(ee(N)) = –0.087
∂P(e2)
∂P(ee(N)) = 0.041
∂P(e3)
∂P(ee(N)) = –0.041
∂P(e4)
∂P(ee(N)) = 0.744
∂P(e5)
∂P(ee(N)) = –0.651
∂P(e6)
∂P(ee(N)) = 0.297
∂P(e1)
∂P(ee(N)) = –0.094
∂P(e2)
∂P(ee(N)) = 0.094
∂P(e3)
∂P(ee(N)) = –0.094
∂P(e4)
∂P(ee(N)) = 0.8
∂P(e5)
∂P(ee(N)) = –0.7
∂P(e6)
∂P(ee(N)) = –1.0
∂P(e6)
∂P(ee(N)) = 0.1
∂P(e3)
∂P(ee(N)) = –0.1
∂P(e4)
∂P(ee(N)) = 0.3
∂P(e1)
∂P(ee(N)) = –0.2
∂P(e2)
∂P(ee(N)) = 1.0
∂P(e1)
∂P(ee(N))= 1.0
∂P(e2)
e1
e2
∂P(ee(N)) = 1.0
∂P(e3)
∂P(ee(N)) = –1.0
∂P(e2)
e3
e4
∂P(ee(N)) = 1.0
∂P(e4)
∂P(ee(N)) = –1.0
∂P(e4)
e5
∂P(ee(N)) = 1.0
∂P(e5)
e6
∂P(ee(N)) = 1.0
∂P(e6)
∨
~
~
~
Figure 12.5: Computing the partial derivatives of P((e1∧∼e2) ∨(e3∧∼e4) ∨(e5∧∼e6)) with respect to
P(e1), P(e2), . . . , P(e6).
a list is appended to node N to hold { 𝜕P(ee(N))
𝜕P(en1) , 𝜕P(ee(N))
𝜕P(en2) , . . ., 𝜕P(ee(N))
𝜕P(enk) }, which is denoted
as 𝜕list(N). The probability P(ee) and the k largest values in {| 𝜕P(ee)
𝜕P(e1)|, | 𝜕P(ee)
𝜕P(e2)|, . . . , | 𝜕P(ee)
𝜕P(en)|}
are computed based on the binary tree structure.
Next, we use the event expression ee = (e1∧∼e2) ∨(e3∧∼e4) ∨(e5∧∼e6)
as an example to illustrate the detailed computing process. Suppose P(e1) = 0.2,
P(e2) = 0.7, P(e3) = 0.1, P(e4) = 0.9, P(e5) = 0.7, P(e6) = 0.2, and the corresponding
binary Tree is T, Figure 12.5 depicts the process of computing the partial derivatives
of P(ee) with respect to P(e1), P(e2), . . ., P(e6). The algorithm makes a post-order tra-
versal on the binary tree T. When each node N gets its turn, its left child LChild(N)
and right child RChild(N) have already been processed. Hence, for the left child of
Authenticated
:27 PM

12.4 Involving Users in Querying Uncertain Context Data
305
node N, P(ee(LChild(N))) and 𝜕list(LChild(N)), and for the right child of node N,
P(ee(RChild(N))) and 𝜕list(RChild(N)), have already been derived. So, P(ee(N)) and
𝜕list(N) = { 𝜕P(ee(N))
𝜕P(en1) , 𝜕P(ee(N))
𝜕P(en2) , . . ., 𝜕P(ee(N))
𝜕P(enk) } can be computed by using the principle we
have stated above.
After obtaining 𝜕P(ee)
𝜕P(ei) (1≤i≤n), we pick up the k largest absolute values, and the
corresponding tuples and probabilities are then returned as the result explanation.
Complexity Analysis. Let n denote the length of the expression ee, which is the total
number of operands and operators in ee, equal to the number of nodes of the binary
tree T representing ee. The basic approach combines the computation of P(ee) and
𝜕P(ee)
𝜕P(e1), 𝜕P(ee)
𝜕P(e2), . . . , 𝜕P(ee)
𝜕P(en) . It makes a post-order traversal over the binary tree T. The time
cost on each node N is proportional to the number of items in 𝜕list(N) which is equal
to the number of leaf nodes of subTree(N). The total time cost of the basic approach is
O(n2). The proof is given below.
Proposition 2. For an algorithm which makes a post-order traversal over a binary tree T,
if the time cost on each node N is proportional to the number of leaf nodes of the subtree
which has N as root node, then the total time cost is O(n2), where n is the number of
nodes of T.
◻
Proof. During the post-order traversal, since the time cost on each node N of tree T is
proportional to the number of leaf nodes of the subtree which has N as root node, this
number is less than n, so the total time cost (sum over all nodes of tree T) is less than
O(n2). This gives the upper bound of the total time cost.
Next we shall show that the quadratic growth rate can be reached. Given an event
expression ee(m) = (e11∧∼e12) ∨. . . (em1∧∼em2), where n = 5 ∗m – 1, suppose the time
cost of post-order traversal of the corresponding binary tree is Time(m). Figure 12.6
shows a binary tree T representing event expression ee(m). To make a post-order tra-
versal over T, ﬁve additional nodes are visited besides the nodes of the subtree which
can be viewed as the representation of event expression ee(m – 1). Each N of the ﬁve
e11
e12
e21
e22
~
~
em–1 2
em1
~
~
. .
.
em–1 1
1 em2
1
1
2
2*m
∨
∧
∧
∨
∨
∧
∧
Figure 12.6: Tree representation of ee(m) .
Authenticated
:27 PM

306
12 Explaining and Scrubbing Context-Aware Query Results
additional nodes is labeled with the number of leaf nodes of the subtree which has N
as root node. Since the time cost on each node N is proportional to the number of leaf
nodes of subtree which has N as root node, we have the following formula: Time(m) =
Time(m–1) + 2∗m + 5 . Notice that omitting a constant multiplier in the item 2 ∗m + 5
will not affect the estimation of the growth rate of Time(m) with m. With T(1) = 5, we
can derive recursively that Time(m) = m2 + 6 ∗m – 2. O(m2) is equivalent to O(n2) here
because n = 5∗m–1. For the speciﬁc expression ee(m) = (e11∧∼e12)∨⋅⋅⋅∨(em1∧∼em2),
Time(m) = m2 + 6 ∗m – 2 shows that the quadratic growth rate can be reached.
◻
(4) Optimization for Queries with a Safe Plan
As result explanation only needs to supply k uncertain tuples with the k largest inﬂu-
ences on P(ee), instead of computing all the partial derivatives for P(ee) as in the basic
approach, actually only k largest values in {| 𝜕P(ee)
𝜕P(e1)|, | 𝜕P(ee)
𝜕P(e2)|, . . . , | 𝜕P(ee)
𝜕P(en)|} are necessary
to be computed. During the post-order traversal, for each node N, suppose en1, en2,
. . ., enk are the leaf nodes of subTree(N), then all the elements whose absolute values
are not among the k largest items of {| 𝜕P(ee(N))
𝜕P(en1) |, | 𝜕P(ee(N))
𝜕P(en2) |, . . ., | 𝜕P(ee(N))
𝜕P(enk) |} can be thrown
away earlier in node N, and the following computations on the numbers are avoided.
Correctness of the Optimized Approach. During the post-order traversal, from each
node N up to the top root R, suppose 𝜕list(N) = { 𝜕P(ee(N))
𝜕P(en1) , 𝜕P(ee(N))
𝜕P(en2) , . . ., 𝜕P(ee(N))
𝜕P(enk) } and
{ 𝜕P(ee)
𝜕P(en1),
𝜕P(ee)
𝜕P(en2), . . .,
𝜕P(ee)
𝜕P(enk) } will be part of 𝜕list(R) = { 𝜕P(ee)
𝜕P(e1), 𝜕P(ee)
𝜕P(e2), . . ., 𝜕P(ee)
𝜕P(en) }, where
en1, en2, . . . , enk are the leaf nodes of subTree(N) and e1, e2, . . . , en the leaf nodes of Tree
T. We have to prove that for each eni, if | 𝜕P(ee(N))
𝜕P(eni) | is not among the k largest items of
{|x|
󵄨󵄨󵄨󵄨󵄨󵄨󵄨x ∈𝜕list(N)}, then | 𝜕P(ee)
𝜕P(eni)| is not among the k largest items of {|x|
󵄨󵄨󵄨󵄨󵄨󵄨󵄨x ∈𝜕list(R)}.
Along the path from node N to R, three types of nodes corresponding to operators
∧, ∨, ∼are processed. A corresponding constant " is multiplied to the items in 𝜕list(N).
From the probability and derivative computing principle, we can get that for operator
∼, " = –1, for operator ∧, " = P(ee(S)), and for operator ∨, " = 1 – P(ee(S)) where
S is the sibling node of N. At the last step, { 𝜕P(ee)
𝜕P(en1),
𝜕P(ee)
𝜕P(en2), . . .,
𝜕P(ee)
𝜕P(enk)} will be in the
form of {! ∗𝜕P(ee(N))
𝜕P(en1) , ! ∗𝜕P(ee(N))
𝜕P(en2) , . . ., ! ∗𝜕P(ee(N))
𝜕P(enk) }, where ! is a constant. For each eni,
if | 𝜕P(ee(N))
𝜕P(eni) | is not among the k largest items of {|x|
󵄨󵄨󵄨󵄨󵄨󵄨󵄨x ∈𝜕list(N)}, then |!| ∗| 𝜕P(ee(N))
𝜕P(eni) |
cannot be among the k largest items of {|!| ∗|x|
󵄨󵄨󵄨󵄨󵄨󵄨󵄨x ∈𝜕list(N)}.
◻
Complexity Analysis. In the optimized approach, to compute the answer and identify
the top-k uncertain tuples, we also make a post-order traversal over the binary tree
T representing the event expression ee. For each node N, when it gets the turn for
processing, both of its children have already been kept as an ordered list of at most k
items. The process time of producing 𝜕list(N) from the two ordered list is proportional
to k, thus the total time complexity of the optimized approach is O(k ∗n).
Authenticated
:27 PM

12.4 Involving Users in Querying Uncertain Context Data
307
12.4.2 Query Recomputation
Users may clarify some uncertain tuples, i.e., tuples’ probabilities, underlying the
query result, based on which the query engine recomputes the query towards an
improved result.
Figure 12.7 shows the graphical interface for a user to modify the incorrect prob-
abilities. The tuples in the explanation are presented as a list of their attributes and
corresponding values in the form ⟨att1 : v1, att2 : v2, . . . , attn : vn⟩, as well as their
tables and probabilities. Figure 12.8 gives an example of recomputing the probabil-
ity of ee = (e1∧∼e2) ∨(e3∧∼e4) ∨(e5∧∼e6), where P(e2) and P(e5) are modiﬁed to
the new values 0.4 and 0.3, respectively. For each node N in the binary tree T, after
applying Phase I (Compute Query and Explanation), P(ee(N)) has already been com-
puted and saved as a ﬁeld. With the probability change of the two leaf nodes e2 and e5,
only the nodes along the paths from e2 and e5 to the root node are affected and their
probabilities need to be recalculated, as marked with a star in Figure 12.8, where the
new computed values are put in brackets.
Figure 12.7: A user modiﬁes a tuple’s
probability.
e1
e2
e3
e4
e5
e6
~
~
~
*
*
*
*
*
*
*
(0.4)
0.7
0.2
0.9
0.2
0.8
(0.24)
0.56
(0.338)
0.591
(0.1288)
0.0694
(0.12)
 0.06
0.1
(0.6)
0.3
(0.3)
0.7
0.1
0.01
∨
∨
∧
∧
∧
Figure 12.8: Recomputing
P((e1∧∼e2) ∨(e3∧∼e4) ∨(e5∧∼e6)) with modiﬁed
P(e2) and P(e5).
Authenticated
:27 PM

308
12 Explaining and Scrubbing Context-Aware Query Results
Complexity Analysis. As a binary tree T is constructed out of n items from the event
expression, the time complexity of recomputing a query is proportional to the path
length of the tree, which is log2 n at the best case and n at the worst case. As a result,
the time complexity for recomputing the result is O(k ∗log2 n) at the best case and
O(k ∗n) at the worst case where k is the number of uncertain tuples clariﬁed.
12.4.3 Performance
To investigate extra overhead incurred due to query result explanation and scrub-
bing, we performed a set of experiments using Java on a Windows PC with double
2.20 GHz CPUs and 2 GB RAM. We produced our probabilistic data set by conduct-
ing some statistical computation on the real MovieLens data [23]. MovieLens data
records 100,000 ratings (1–5) given by 943 users on 1,682 movies. We adopted its
three relations, i.e., Users(User_id, Age, Gender, Occupation, Zip_code) (containing
943 tuples), Movies(Movie_id, Title, Release_year) (containing 1,682 tuples), Rat-
ings(User_id, Movie_id, Rating) (containing 100,000 tuples). For each movie, we
computed the probability that it is liked by people from northern regions, and the
probability that it is liked by people from southern regions. We thus obtained three
probabilistic relations in Figure 12.2, each of which accommodated 1,682 tuples.
Whether a person is from northern or southern regions is decided by his/her Zip_code.
The query process was divided into three stages: (1) computing answer plus an-
swer explanation (i.e., the k most inﬂuential uncertain tuples); (2) user’s clariﬁcation
of the presented uncertain tuples; (3) recomputing the answer.
Without loss of generality, we executed three representative queries, whose event
expressions and running times are shown in Tables 12.1 to 12.3. In the experiments,
n varies from 1 to 298 for scalability investigation. We ﬁnd that (1) computing query
results without explanation and scrubbing has a linear time complexity, while com-
puting answer with explanation and scrubbing has a quadratic time complexity; (2)
after optimization, the performance of the latter increases to the linear complexity;
(3) due to the storage of previous computing results, the time to recompute the query
is very little. In the experiment, we set k = 5, which means that min(k, n) number of
uncertain tuples will have their probabilities clariﬁed.
To examine the gain of result explanation and scrubbing, we build a measure
Gain=
|P(ee) – P(ee)󸀠|
√(P(e1)–P(e1)󸀠)2 + ⋅⋅⋅+ (P(en)–P(en)󸀠)2
where P(ee)󸀠, P(e1)󸀠, . . . , P(en)󸀠denote the corrected new probabilities of respective
complex and basic events. Note that only the k uncertain tuples (i.e., probabilities
of k basic events) will be modiﬁed. The formula reﬂects the gain as the change of the
ﬁnal result probability upon underlying modiﬁed uncertain tuples. Figure 12.9 shows
Authenticated
:27 PM

Table 12.1: Time in nanoseconds to compute ee(n) = e1 ∧e2 ∧⋅⋅⋅∧en.
No-scrub
Scrub
n
Basic approach
Basic approach
Optimization approach
Recomputation
1
0
0
0
0
2
3
0
2
3
3
0
5
5
0
4
1
5
2
1
5
1
6
2
0
6
2
7
5
2
7
2
15
3
0
8
0
9
9
0
9
0
15
0
0
11
16
15
0
0
13
8
31
0
16
15
7
31
15
0
24
8
70
16
0
40
16
172
16
0
53
16
296
31
15
130
47
2,860
47
15
235
78
15,625
110
0
237
78
15,937
187
16
296
93
30,391
188
0
298
93
30,844
250
0
Table 12.2: Time in nanoseconds to compute ee(n) = e1 ∨e2 ∨⋅⋅⋅∨en.
No-scrub
Scrub
n
Basic approach
Basic approach
Optimization approach
Recomputation
1
0
0
0
0
2
3
9
0
1
3
0
2
0
0
4
1
5
5
1
5
1
4
2
0
6
2
7
0
0
7
2
13
3
0
8
0
12
6
0
9
0
16
8
0
11
0
15
0
0
13
8
31
15
0
15
7
32
15
8
24
8
70
31
0
40
16
172
16
0
53
15
313
31
16
130
47
2,890
62
16
235
79
15,750
94
15
237
78
16,063
188
0
296
94
30,797
203
16
298
93
31,109
250
0
Authenticated
:27 PM

310
12 Explaining and Scrubbing Context-Aware Query Results
Table 12.3: Time in nanoseconds to compute ee(n) = (e11∧∼e12) ∨⋅⋅⋅∨(en1∧∼en2).
No-scrub
Scrub
n
Basic approach
Basic approach
Optimization approach
Recomputation
1
5
5
2
0
2
2
8
2
0
3
5
10
2
3
4
2
14
7
0
5
3
19
10
1
6
10
20
5
0
7
6
27
9
2
8
6
34
15
0
9
8
39
15
0
11
0
47
16
0
13
16
63
16
0
15
15
94
31
0
24
16
156
47
0
40
16
172
47
0
53
31
454
78
0
130
63
1,734
125
15
235
94
12,515
235
15
237
187
43,422
468
0
296
188
45,078
453
16
298
234
66,078
687
0
that using partial derivative approach to choose the two most inﬂuential uncertain
tuples is more effective than randomly choosing two uncertain tuples for clariﬁcation.
Interestingly, the beneﬁt of scrubbing the query result decreases along with n. This is
due to the reason that more factors contribute to P(ee), and changing a few of them
inﬂuences less than before.
12.4.4 Discussion
We have presented our result scrubbing solution for queries having a safe plan. For
queries which do not have a safe plan, as their probability computing complexities are
#P-complete [32], approximate computing strategy is usually taken to trade accuracy
for efﬁciency. A well-known approach to compute probability for event expressions
that do not have a safe plan is the Monte Carlo algorithm [12, 18]: given a DNF for-
mula with N clauses and any : and $, the algorithm runs in time O(N/:2 ln 1/$), and
guarantees that the probability of the error being greater than : is less than $.
Confronted with an unsafe-planned query expression ee containing basic events
e1, e2, ⋅⋅⋅, en, the main task of computing its result explanation is to compute the par-
tial derivatives. For probability relationship P(ee) = f(P(e1), P(e2), . . . , P(en)), we have
Authenticated
:27 PM

12.4 Involving Users in Querying Uncertain Context Data
311
4
6
8
10
12
14
16
18
20
0.0
0.4
0.8
1.2
n
(a)
(b)
(c)
gain
o
o
o
o o
o o o o
o o o o
o
o
o
o
o
* *
*
* * * * * * * * * *
*
*
*
*
*
4
8
10
12
14
16
18
20
0.0
0.4
0.8
1.2
n
gain
o o
o o
o o o
o
o
o
o o
o o o o o o
* *
* *
* * *
*
*
*
* *
* * * * * *
6
4
6
8
10
12
14
16
18
20
0.0
0.4
0.8
1.2
n
gain
o
o o
o o o
o o o
o
o o o o
o
o
o o
* * * * * * * * * * * * * * * * * *
o
*
Partial derivative
Random
o
*
Partial derivative
Random
o
*
Partial derivative
Random
Figure 12.9: Gain with query result explanation and scrubbing: (a) e1 ∧e2 ∧⋅⋅⋅∧en; (b) e1 ∨e2 ∨⋅⋅⋅∨en;
and (c) (e11 ∧∼e12) ∨⋅⋅⋅∨(en1 ∧∼en2)
shown that P(ee) can be written as a linear function of P(ei) in formula (12.2), and the
partial derivative of P(ee) with respect to P(ei) at point (p1, p2, . . . , pn) can further be
written as
𝜕P(ee)
𝜕P(ei)
󵄨󵄨󵄨󵄨󵄨󵄨󵄨P(e1)=p1,...,P(ei)=pi,...,P(en)=pn
=
P(ee)|P(e1)=p1,...,P(ei)=p(1)
i ,...,P(en)=pn – P(ee)|P(e1)=p1,...,P(ei)=p(2)
i
,...,P(en)=pn
(p(1)
i
– p(2)
i )
.
(12.9)
The two numbers p(1)
i
and p(2)
i
can be selected arbitrarily due to the linear property of
P(ee) with respect to P(ei). We set p(1)
i
and p(2)
i
to 1 and 0, respectively. Using the Monte
Carlo simulation, P(ee)|P(e1)=p1,...,P(ei)=1,...,P(en)=pn and P(ee)|P(e1)=p1,...,P(ei)=0,...,P(en)=pn can
ﬁrst be computed, and then 𝜕P(ee)
𝜕P(ei) is computed according to formula (12.9). After get-
ting 𝜕P(ee)
𝜕P(ei) (1≤i≤n), from them the k largest absolute values are picked up and their
corresponding tuples and probabilities are returned as result explanation. For a DNF
event expression with n event identiﬁers and N clauses, the time cost of providing
answer explanation is O(n ∗N).
Authenticated
:27 PM

312
12 Explaining and Scrubbing Context-Aware Query Results
To further recompute the event probability of ee after user’s clariﬁcation on
certain P(ei)’s, two situations are considered.
(1) A user clariﬁes only one probability of the uncertain tuples (say, change P(ei)
to P(ei)󸀠), then the new answer can be computed as P(ee)󸀠= P(ee)+ 𝜕P(ee)
𝜕P(ei) (P(ei)󸀠–P(ei)).
The time cost is therefore a constant.
(2) A user clariﬁes more than one probability, the new answer then needs to be
recomputed from scratch by using approximation method. In this case, the time cost
of recomputing is O(N). The analysis result has been validated by our experimental
results, which are omitted due to space constraint.
12.5 Recapitulation
Query engines on uncertain data take the risk of delivering incorrect query results due
to the underlying uncertain tuples. This chapter proposed to leverage users’ personal
knowledge to cope with data uncertainty. A simple yet effective result explanation
and result correcting approach based on users’ uncertainty clariﬁcation was presen-
ted. The approach used a derivative-based metric to compute the k most inﬂuential
uncertain tuples underlying a probabilistic answer, and returned them as the result
explanation to the user for justiﬁcation and modiﬁcation. Based on the user’s feed-
back, the approach recomputed the query. The time for recomputing the answer was
reduced largely by using the prestored computing results, which is proportional to
the number of uncertain tuples in the explanation and the height of the binary tree
representing the query expression.
The next chapter will discuss another kind of context-aware querying, which
uniﬁes context and content clues for information reﬁnding.
Literature
[1]
P. Agrawal, O. Benjelloun, A. D. Sarma, C. Hayworth, S. Nabar, T. Sugihara, and J. Widom. Trio: A
system for data, uncertainty, and lineage. In Proc. of VLDB, 2006.
[2]
A. Fuxman, P. Andritsos, and R. Miller. Clean answers over dirty databases: a probabilistic
approach. In Proc. of ICDE, page 30, 2006.
[3]
S. Antifakos, A. Schwaninger, and B. Schiele. Evaluating the effects of displaying uncertainty in
context-aware applications. In Proc. of UbiComp, pages 54–69, 2004.
[4]
O. Benjelloun, A. D. Sarma, A. Y. Halevy, and J. Widom. ULDBs: Databases with uncertainty and
lineage. In Proc. of VLDB, pages 953–964, 2006.
[5]
B. G. Buchanan and E. H. Shortliffe. Rule Based Expert Systems: The MYCIN Experiments of the
Stanford Heuristic Programming Project, Boston, MA: Addison-Wesley Longman Publishing Co.,
Inc., 1984.
[6]
P. Buneman, S. Khanna, and W. C. Tan. Why and where: A characterization of data provenance.
In Proc. of ICDT, page 316, 2001.
[7]
A. van Bunningen. Context-Aware Querying – Better Answers with Less Effort. PhD thesis,
University of Twente, Netherlands, 2008. CTIT Ph.D. Thesis Series No. 08-115, ISSN 1381–3617.
Authenticated
:27 PM

Literature
313
[8]
R. Cavallo and M. Prittarelli. The theory of probabilistic databases. In Proc. of VLDB, pages
71–81, 1987.
[9]
J. Chen and R. Cheng. Quality-aware probing of uncertain data with resource constraints. In
Proc. of SSDBM, pages 491–508, 2008.
[10]
R. Cheng, J. Chen, and X. Xie. Cleaning uncertain data with quality guarantees. In Proc. of VLDB,
pages 722–735, 2008.
[11]
Y. Cui and J. Widom. Lineage tracing in a data warehousing system. In Proc. of ICDE, pages
683–684, 2000.
[12]
N. Dalvi and D. Suciu. Efﬁcient query evaluation on probabilistic databases. VLDB Journal,
16(4):523–544, 2007.
[13]
G. Das, V. Hristidis, N. Kapoor, and S. Sudarshan. Ordering the attributes of query results. In
Proc. of SIGMOD, pages 395–406, 2006.
[14]
N. Fuhr and T. Rolleke. A probabilistic relational algebra for the integration of information
retrieval and database systems. ACM Transactions on Information Systems, 15(1):32–66, 1997.
[15]
S. Jeffrey, G. Alonso, M. Franklin, W. Hong, and J. Widom. Declarative support for sensor data
cleaning. In Proc. of Pervasive Computing, pages 83–100, 2006.
[16]
H. Johnson and P. Johnson. Explanation facilities and interactive systems. In Proc. of IUI, pages
159–166, 1993.
[17]
A. Kalyanpur, B. Parsia, B. C. Grau, and E. Sirin. Justiﬁcations for entailments in expressive
description logics. Technical report, University of Maryland Institute for Advanced Computer
Studies (UMIACS), 2006.
[18]
R. M. Karp and M. Luby. Monte-Carlo algorithms for enumeration and reliability problems. In
Proc. of FOCS, pages 56–64, 1983.
[19]
N. Khoussainova, M. Balazinska, and D. Suciu. Towards correcting input data errors
probabilistically using integrity constraints. In Proc. of MobiDE, pages 43–50, 2006.
[20] W. Kiesling and G. Kostler. Preference SQL – design, implementation, experiences. In Proc. of
VLDB, Hong Kong, China, pages 990–1001, 2002.
[21]
J. C. Lester and B. W. Porter. Developing and empirically evaluating robust explanation
generators: The knight experiments. Computational Linguistics, 23(1):65–101, 1997.
[22] X. Lian, L. Chen, and S. Song. Consistent query answers in inconsistent probabilistic databases.
In Proc. of SIGMOD, pages 156–171, 2010.
[23] Movielens data. http://www.grouplens.data. 2017.
[24] M. Mutsuzaki, M. Theobald, Ander de Keijzer, J. Widom, P. Agrawal, O. Benjelloun, A. D. Sarma,
R. Murthy, and T. Sugihara. Trio-one: Layering uncertainty and lineage on a conventional DBMS.
In Proc. Of CIDR, pages 269–274, 2007.
[25]
C. Re and D. Suciu. Approximate lineage for probabilistic databases. In Proc. of VLDB, pages
797–808, 2008.
[26] S. Russell and P. Norvig. Artiﬁcial Intelligence: A Modern Approach. Pearson Education Limited,
England, 2014.
[27]
P. Sen and A. Deshpandbe. Representing and querying correlated tuples in probabilistic
databases. In Proc. of ICDE, pages 596–605, 2007.
[28] X. Shen, B. Tan, and C. Zhai. Context sensitive information retrieval using implicit feedback. In
Proc. of SIGIR, pages 43–50, 2005.
[29] R. F. Simmons. Natural language question-answering systems: 1969. Communications of ACM,
13(1):15–30, 1970.
[30] A. Stein, J. A. Gulla, A. Muller, and U. Thiel. Conversational interaction for semantic access to
multimedia information. Intelligent Multimedia Information Retrieval, M. T. Maybury (ed.),
Menlo Park, CA: AAAI/The MIT Press, pages 399–421, 1997.
[31]
N. Tintarev and J. Masthoff. A survey of explanations in recommender systems. In Proc. of ICDE
Workshop on Recommender Systems and Intelligent User Interfaces, pages 801–810, 2007.
Authenticated
:27 PM

13 Context-Based Information Reﬁnding
Abstract: Reﬁnding what one has seen or used before is a common behavior in real life.
Psychological studies show that context under which information was accessed can
serve as a powerful cue for information recall. “Finding the sweet recipe that I read
at the hotel on the trip to Africa last year” and “Revisit the website which I visited
last month and that website describes the regulations of behaviors in micro-blog” are
two context-based reﬁnding request examples. From request formulation, query tar-
get, to result evaluation, such kind of context-based recall querying differs from the
traditional content-based database querying. After a comprehensive overview of in-
formation reﬁnding techniques and tools developed in the ﬁelds of web and personal
information management (PIM), this chapter proposes to leverage human’s natural
recall characteristics, and allow users to reﬁnd web pages and local ﬁles according to
the previous access context and page content.
13.1 Characteristics of Information Reﬁnding
To study new knowledge, people learn from old lessons. To ﬁnish a current assign-
ment, people rely on past experiences. To make free from confusion, people look for
the known evidences. To accomplish a task or reach a certain goal, one often needs to
reaccess information she/he has ever come across occasionally or intentionally in the
past. When people look back for references, they frequently revisit their earlier visited
web pages, documents, emails, or recall some past experienced events associated with
information access in their daily lives. To be able to effectively support users’ reﬁnding
requests, it is necessary to understand how and how frequently users reﬁnd inform-
ation, which strategies users tend to employ to reﬁnd information, and how domain
artifacts and contextual information are used to aid the reﬁnding process.
13.1.1 Reﬁnding Is a Common Activity
Teevan et al. [95] once analyzed one-year web queries issued by 114 users to the Yahoo
search engine, and discovered that 40% of queries were actually reﬁnding requests.
Tauscher and Greenberg [89] had a similar ﬁnding from a six-week usage data collec-
ted from 23 users that over 58% of web pages accessed were revisits to pages previously
seen. The statistic result of Cockburn et al. [27, 28] showed that the average proportion
of revisits to web pages was over 81%. Teevan [94] researched the behaviors of people’s
web search results recall, recognition, and reuse, and found out that people are more
likely to reuse their previously found information contained in the search histories,
quickly ﬁnd what they are looking for, be happy with the result quality, and resolve
the task at hand easily.
DOI 10.1515/9783110556674-013
Authenticated
:27 PM

13.1 Characteristics of Information Reﬁnding
315
Capra et al. [14] conducted a user study as a collaborative exercise with pairs of
participants, where one participant acted as a retriever, helping the other participant
to reﬁnd information by telephone. The result showed that reﬁnding is an iterative
process of two steps. Users ﬁrst attempt to locate an information source (search); and
once it is found, they then start to ﬁnd the speciﬁc information being sought (browse).
Some explicit annotations added by the users on their viewed web pages before are
utilized extensively.
Pu and Jiang [74] investigated how users search web information for the ﬁrst time
and then locate previously found results on a subsequent effort. The authors set up
a two-staged (ﬁnding and reﬁnding) experiment and exploited a variety of methods
(such as experiments, observations, interviews, and questionnaires) to compare users’
search performance over different search tasks based on users’ search purposes, fre-
quencies of exposure to the search tasks, familiarities with the search topics, and the
types of information searched. The results showed that in some cases the search per-
formance of reﬁnding is surprisingly lower than that of ﬁnding, calling for good tools
to support users’ reﬁnding requirements.
Whittaker et al. [104] studied how users reﬁnd their emails while managing
their daily work tasks. They investigated the advantages/disadvantages of two email
management and reﬁnding strategies: preparatory organization and opportunistic
method. Adapting the former strategy, users need to pay considerable preparatory
efforts to manually create complex folder structures in order to promote effective
reﬁnding through simple browsing. For the latter strategy, however, users rely on mod-
ern email clients that support search, folders, tagging, and threading. A ﬁeld study
of 345 long-term users was conducted over 85,000 reﬁnding actions, and the result
showed that both search and threading facilitate effective reﬁnding.
As for how frequent people revisit web contents, Adar et al. [3] examined ﬁve
weeks of web interaction logs of over 612,000 users. Their analysis result revealed that
there are four primary revisitation patterns on the web, namely, fast, medium, slow,
and hybrid, according to the average time interval users revisited their accessed web-
site again, for example, less than an hour, longer than an hour but less than a day,
longer than a day, and so on.
13.1.2 Differences from Information Finding
Different from information ﬁnding, information reﬁnding has the following character-
istics, posing special requirements on reﬁnding techniques.
1) Information reﬁnding is a more directed and targeted search task than informa-
tion ﬁnding. Finding information is an exploratory activity which involves recognition
(e.g., “is this the information that I am looking for?”), while reﬁnding information
is a focused task which involves both recognition and recall (e.g., “where did I see
that information?”). If users remember all details, they would just go directly to their
Authenticated
:27 PM

316
13 Context-Based Information Reﬁnding
desired information. Therefore, the process of ﬁnding information usually involves
uncertainty to some extent because users do not know enough details about the in-
formation they are looking for, while reﬁnding is a more directed focused process
because users have already seen the information before [14].
2) Information reﬁnding may happen in a short term (i.e., reﬁnding information
just visited), medium term (i.e., reﬁnding information in a few days), or long term (i.e.,
reﬁnding information after a relatively long period of time like several months, several
years, or even several decades later).
3) Due to the dim memory of the past, users usually have difﬁculties in the long-
term information reﬁnding because they may have forgotten the detailed content of
the information after a relatively long period of time. They may even have difﬁculties
in ﬁguring out the precise search terms used at that time.
4) Information reﬁnding is a recall-based searching, where context can serve as a
powerful cue for revisitation.
13.1.3 Difﬁculties in Information Reﬁnding
However, due to the dim memory of the past, information reﬁnding is not always easy.
Teevan [93] discovered that the original queries are remembered wrongly 28% of the
time due to users’ misremembering or forgetting. Aula et al. [5] found that experi-
enced users also have such problems as “I think my main problem in web searches
is nowadays that I can’t remember which were the terms that I used when I found a
relevant site” when they use search engines to reaccess web pages.
For long-ago accesses, it is also a difﬁcult and time-consuming task for users to
reﬁnd what they want if they only rely on the remembrance of the previous informa-
tion contents. However, users can commonly remember some context such as when,
where, who, etc., related to the accessed information. Psychological studies show that
the context under which information was accessed in the past can serve as a powerful
cue for information recall, as it is always easier to remember than detailed information
content itself [61]. For example, it may be hard to recall a recipe’s detail accessed one
year ago, but the time (last summer), place (at a hotel), and concurrent activity (travel-
ing to Africa), etc., associated with the happening of the previous accessing event may
leave a deeper impression, and can thus serve as powerful cues to reﬁnd the target
recipe information [23, 41, 59, 93].
13.2 Overview of Information Reﬁnding Techniques
In response to users’ common reﬁnding requests, substantial efforts have been made
in the literature to address the topic of information reﬁnding. The section provides a
comprehensive overview of information reﬁnding techniques and tools developed in
the ﬁelds of web and PIM.
Authenticated
:27 PM

13.2 Overview of Information Reﬁnding Techniques
317
13.2.1 Web Information Reﬁnding
On the web, a diversity of methods has been devised to organize web information for
reaccess and reuse. Mayer [68] made a good survey on the existing approaches and
tools for web revisitation. Some typical techniques supporting web revisitation are
back and forward buttons [28, 69], bookmarks (also referred to as favorites) [1], and
history tool [108] in web browsers. Search engines can also help users return to the
web pages they once visited [18]. Moreover, using the methods of search service, par-
tial completion of a site’s web address, and hyperlinks from another website can also
help people return to their desired websites [8, 50, 51]. Based on their respective char-
acteristics, we summarize the existing tools and approaches that support information
reﬁnding on the web into four categories, namely, (1) reﬁnding tools in web browsers,
(2) history service, (3) reﬁnding search engine, and (4) voice-based reﬁnding.
13.2.1.1 Reﬁnding Tools in Web Browsers
People tend to reaccess pages just visited, or revisit only a few pages frequently. They
may browse in very small clusters of related pages, and generate only short sequences
of repeated URL paths [89]. To support personal web revisitation, a number of tech-
niques and tools are developed, including bookmarks, history tools, search engines,
metadata annotation and exploitation, and contextual recall systems.
(1) Bookmarks
Apart from back and forward buttons, manually or automatically bookmarking favor-
ite web pages in web browsers enables users to get back to the previously accessed
pages.
According to users’ every visited web page and browsing preferences, the authors
of Refs [53, 87] built bookmarks automatically and organized them into a recency
list [87] or layered structure [53], respectively.
Gamez et al. [42] further used classiﬁers to forecast a few of the bookmarks that are
more probably to be visited later and showed them in the browser bookmarks personal
toolbar, so that the user can access the desired web page through a single mouse click.
Bearing similarities to Refs [42, 87], Kawase et al. [57] recommended visited pages
relevant to the currently viewed pages, and presented them in a dynamic browser
toolbar.
The SearchBar tool developed by Morris et al. [72] organizes a user’s web access
history in a hierarchy, which includes user’s recent search topics, queries, results vis-
ited, as well as user’s notes on the topics. By browsing the hierarchy of access history,
the user can re-acquire previous search information (such as queries, results visited,
and their notes).
MacKay et al. [66] proposed an efﬁcient tool for reﬁnding information on a spe-
ciﬁc web page, called landmark, which is an extension to the traditional bookmarks.
Authenticated
:27 PM

318
13 Context-Based Information Reﬁnding
It is a user-directed technique that aids users in returning to speciﬁc content within
a previously visited web page. Despite the use of traditional bookmarks which en-
able users to return to a previously visited page, sometimes it is still hard to reﬁnd
speciﬁc content within that page. Similar to bookmarks, landmarks are not effective
if the URL is broken. If marked information on a page changes, a set landmark may
be lost.
(2) History Tools
History tools of web browsers maintain a user’s accessed URLs chronologically ac-
cording to visit time (e.g., today, yesterday, last week, etc.), and accessed page titles
and contents.
Tauscher and Greenberg [90, 91] analyzed 6 weeks of usage data collected from
23 participants when using a commercial browser Mosaic, and discovered that people
tend to revisit pages just visited, access only a few pages frequently, browse in very
small clusters of related pages, and generate only short sequences of repeated URL
paths, which can be used to develop guidelines for the design of history mechanism.
Google
Web
History
(http://www.google.com/history)
keeps
users’
search
keywords and clicked pages, and classiﬁes each user’s history data into different
topics, such as web, images, news, videos, etc. Based on that, it allows users to
browse their historically accessed pages at a selected date or during different time
periods (Google provides four options, namely, newest, newer, older and oldest),
using all history or one topic as a ﬁltering condition. It also enables users to search
accessed web pages under all history or different topics by keywords of page titles
and page contents, etc.
Contextual Web History [109] improved the visual appearance of the web browser
history by combining website thumbnails and content snippets to assist users to easily
browse or search their histories by time. It enriches the content of a page by providing
a set of cues, including time of visit and visual appearance. It also enables text search
for web history. Furthermore, some browsers also provide URL auto-completion func-
tionality based on users’ web pages accessed histories, and users only need to enter
some sequences of the URL paths.
Visual History Tool [31] encoded four features of a visited web page, which consists
of user’s page interests measured by dwell time, the frequency and recency of the visit,
and navigational associations between pages. List- and graph-based forms are then
adopted to provide navigation histories.
xMem [77] improved history mechanisms by intermixing semantic aspects with
the temporal dimension of the accessed pages. It organized the pages into groups and
presented a navigational history instead of simply exploiting time-sort history.
SearchPanel [75] combined web page and process metadata into an interactive rep-
resentation of the retrieved documents that can be used for sense-making, navigation,
and reﬁnding documents.
Authenticated
:27 PM

13.2 Overview of Information Reﬁnding Techniques
319
13.2.1.2 Reﬁnding Search Engine
Commonly, users are able to reﬁnd some previously visited web pages through util-
izing web search engines [5, 18]. As long as users can provide some appropriate
keywords, the search engines will return them with their desired results in the ﬁrst
place.
However, while matching the expectations a person develops during an initial
search is clearly important for reﬁnding, expectation matching does not necessarily
interfere with the ﬁnding of new information [94]. New and more relevant results can
be placed where old results are forgotten to create a list that matches expectation but
contains valuable new information.
Tyler and Teevan [101] studied how search engines are used for reﬁnding previ-
ously found search results. It explored the differences between queries that had sub-
stantial/minimal changes between the previous query and the revisit query. Through
observing the differences between reﬁnding behavior occurring within the same ses-
sion and across multiple sessions, the results showed that cross-session reﬁnding may
be a way to bridge a task between two different sessions.
Due to the dynamic nature and contextual diversity of the web, information re-
ﬁnding is often impeded by fast content changes as well as the frequent updated
result rankings in search engines [95]. Based on that, Teevan [92, 95] built a search
engine called Re:Search Engine which can support not only ﬁnding of new informa-
tion but also reﬁnding of old information. Past queries are indexed to identify repeated
searches, and the most recently viewed results are stored in a result cache. A user’s in-
teraction cache is used to decide how likely the past results are to be memorable. In
the merge phase, the system uses a measuring function of beneﬁt of new information
and memorability of old results to decide the ranking of web pages. The process of
re-search is described as the following:
When a person issues a query to the Re:Search Engine that is similar to a previous search, the
engine fetches the current results for the query from the preexisting search engine and fetches rel-
evant previously viewed results from its cache. The newly available results are then merged with
the previously viewed results to create a list that supports intuitive reﬁnding and contains new
information.
Adar et al. [2] analyzed ﬁve-week web interaction logs from over 612,000 users, and
interview studies from 20 participants who installed software to log web page vis-
its for one to two months. They identiﬁed 12 different types of revisitation curves
corresponding to four groups (i.e., fast, medium, slow, and hybrid revisits), and
regarded each of them as a signature of user behavior in accessing a given web
page. The analysis of revisitation behaviors for web users in various contexts could
empower search engines to better support fast, fresh, and effective ﬁnding and
reﬁnding.
Authenticated
:27 PM

320
13 Context-Based Information Reﬁnding
13.2.1.3 Metadata Annotation and Exploitation
Haystack [55] stored arbitrary objects of interest to a user, and recorded arbitrary
(predeﬁned or user-deﬁned) properties of and relationships between the stored in-
formation. It coined a uniform resource identiﬁer (URI) to name anything of interest,
including a document, a person, a task, a command/menu operation, or an idea.
Once named, the object can be annotated, related to other objects, viewed, and re-
trieved through arbitrary properties, which served as useful query arguments, as
facets for metadata-based browsing, or as relational links to support the associative
web browsing.
Bearing the similarity to Haystack, a SQL-based MyLifeBits platform [43] was built
for recording, storing, and accessing a personal lifetime archive. It stored content and
metadata for a variety of item types, including contacts, documents, email, events,
photos, music and video, which were linked together implicitly using “time”, or expli-
citly linked with typed links such as a “person in photo” link between a contact and
a photo, or a “comment” link between a voice comment and a document. With link-
ing, the traditional folder (directory) tree was replaced by a more general “collections”
function using a directed acyclic graph (DAG).
13.2.1.4 Voice-Based Reﬁnding
Observing the pervasiveness of voice communication over mobile phones and the re-
quirements of mobile users’ information reﬁnding, some researchers paid attention
to exploring a telephone-based voice interface to support mobile users’ needs for re-
ﬁnding speciﬁc information previously found on the web [15], and a prototype system
called WebContext has already been developed [17, 20].
The system WebContext is used as a test bed to examine concepts related to provid-
ing a remotely accessible voice interface for reﬁnding information viewed before on
the web. The core of the prototype is a set of modules for extracting and combining
information found on saved web pages, such as phone numbers and addresses. Be-
sides, the text content of the page is scanned and keywords and phrases are extracted
and indexed to support reﬁnding later on. In WebContext, a voice interface was imple-
mented and used to ask the user to supply some pieces of information to help identify
a web page and the type of information to be retrieved.
Table 13.1 summarizes different web information reﬁnding mechanisms.
13.2.2 Personal Information Reﬁnding
In PIM, as the amount of individual users’ information available in digital form
has increased greatly in recent years, users need more effective ways of organizing
and searching their data. Besides through browsing ﬁles over hierarchical director-
ies, users can also utilize search tools such as Google Desktop, Microsoft Windows
Authenticated
:27 PM

13.2 Overview of Information Reﬁnding Techniques
321
Table 13.1: Summary of different web information reﬁnding methods.
Web reﬁnding mechanisms
Characteristics
Back/Forward buttons
Easily go back by backtracking
Restrict to short-term reﬁnding
Bookmarks
URL-based references for single web pages
Require users’ extra efforts; be suitable to keep a small number of web
pages
Landmark
Bookmarks’ extension, enable users to return to speciﬁc content
within a web page
Require users’ extra efforts; be suitable to keep a small number of
page contents
History list
Record a user’s visited pages automatically
Hard to locate a desired item in a long list
Contextual web history
Visibility improvement, text search for history
Require users to remember the details of a web page
SearchBar
Organize web history in a hierarchy based on topics
Require users’ extra efforts; be suitable to maintain a part of web
histories
Google web history
Support browse-based and search-based reﬁnding
Restrict to web history on Google
Re:Search engine
Combine ﬁnding new with reﬁnding old
Insufﬁciently support reﬁnding a particular item
WebContext
Voice-based interface, support mobile users’ reﬁnding needs
Limited interaction
Desktop Search, and Spotlight for Apple’s OS X, to perform keyword searches and
locate personal information. Many PIM systems have also been developed such as
MyLifeBits [44], Beagle++ [26], and so on.
To re-access desktop resources, including local ﬁles, emails, cached web pages,
etc., users commonly rely on two methods: browse all the information or search them
via using desktop search tools or the applications’ search functionality. We summarize
the existing approaches for reﬁnding information in PIM into three categories, i.e., (1)
search by browsing; (2) content-based search; (3) context-based search.
13.2.2.1 Search by Browsing
In the current desktop ﬁle systems, users are used to organize their personal data in
directory hierarchies and browse information through ﬁle explorer. In order to look for
the data facilely in the future, users always create deliberately well-organized folder
structures. However, along with the explosion of personal data and the vagueness of
memories, reﬁnding local ﬁles becomes a time-consuming task.
Dong and Halevy [32] and Cai et al. [11] developed a SEMEX system which offers
users a ﬂexible platform for PIM. The SEMEX system enables users to browse personal
Authenticated
:27 PM

322
13 Context-Based Information Reﬁnding
information by semantically meaningful associations that are created from data items
on one’s desktop, e.g., AuthoredBy, Cites, and AttachedTo.
13.2.2.2 Content-Based Search
Considering that users may forget the right places of certain ﬁles and consequently
have difﬁculties to locate them, some desktop search tools such as Google Desktop,
Microsoft Windows Desktop Search, and Spotlight for Apple’s OS X could be used to
conquer these problems. Cohen et al. [29] addressed the desktop search problem by
considering various ranking techniques, such as those based on different ﬁle features
(e.g., ﬁle name, access date, ﬁle size, etc.), learning-based ranking schemes, and those
based on query selectiveness.
Dittrich and Salles [30] presented an iMeMex Data Model (iDM) for PIM, where un-
structured, semi-structured and structured data are represented inside a single model.
They built index for personal data resources and developed a simple query language
to support users’ query requirements. Based on iDM, Blunschi et al. [7] implemented a
personal dataspace management system called iMeMex, which offers some contextual
information (graph connections, time and lineage) on query results.
Wang et al. [103] presented uniﬁed data and query processing models and a
fuzzy search approach which considers approximate matches to structure and con-
tent query condition in personal information reﬁnding. Peery et al. [73] proposed a
multi-dimensional (content, metadata and structure) approach which allows users to
provide fuzzy structure and metadata conditions in addition to keyword conditions to
search semi-structured data in PIM systems.
13.2.2.3 Context-Based Search
To conduct a successful information reﬁnding task by search-based approaches, a
user must remember sufﬁcient details about the target information. Unfortunately,
due to the characteristic of amnesia of human memory, users sometimes are not able
to recall the detailed information content but some relevant contexts [61]. Therefore,
contextual cues and association can well be utilized to assist information reﬁnding.
Contextual Cues and Associations. To understand what contextual cues and as-
sociations impact reﬁnding, many researchers pay considerable attentions to users’
real reﬁnding tasks in different applications. In Ref. [41], context as metadata proper-
ties of data contents is incorporated and indexed for personal information retrieval.
Salles et al. [82] presented association trails to deﬁne associations among items in
a dataspace. For example, people who go to the same university or graduate on the
same year or share a hobby are associated. Based on those associations between data
items, when users search their dataspaces with keyword search engines, the systems
not only can return the matched items, but also can enrich results with other relev-
ant associated information. Kelly et al. [59] conducted a user study of remembered
Authenticated
:27 PM

13.2 Overview of Information Reﬁnding Techniques
323
context data and its utility in PIM, showing that much of the textual content failed
to be recalled, while some context information (such as time and location) and ﬁle
types are well remembered over a six-month period. Capra [16] studied the factors of
task type and the elapsed time in information reﬁnding. Moon and Fu [70, 71] stud-
ied how spatial locations and luminance affect ﬁnding and reﬁnding information in a
desktop environment. Capra and Perez-Quinones [19] studied several factors such as
task, individual and time that can affect users’ information reﬁnding process. Eldridge
et al. [35] explored the value of video recording in aiding the recall of work activities,
and found that people and objects are particularly valuable cues in aiding the recall
of work activities.
Email Reﬁnding. Elsweiler et al. [37] performed a user study concentrating on email
reﬁnding. They investigated what attributes (time, user’s experience, etc.) of email
messages users remember in their reﬁnding actions, and looked at how the attrib-
utes change in different situations and studied the factors which impact on what is
remembered. They further investigated the contextual factors that inﬂuence users’
perception of task difﬁculty in the situation of reﬁnding email messages, where 21 par-
ticipants perform reﬁnding tasks on their own personal collections. They found that
the time lapsed since a message was read, when the sought-after email was sent, the
other recipients of the email, the experience of the user and the user’s ﬁling strategy
have strong relationships with difﬁculty [38].
Forget-me-not. Lamming and Flynn [61]’s Forget-me-not project explored mobile
and ubiquitous technologies to help users’ everyday memory problems: ﬁnding a
lost document, remembering somebody’s name, recalling how to operate a piece of
machinery. In Forget-me-not, a user interface is exploited to search and display the
collected data, which are represented by respective icons, and users can use the visible
and familiar icons as ﬁltering conditions to reﬁnd certain information.
MyLifeBits. The PIM system MyLifeBits [44] exploited contextual cues such as
time, author and association information to search personal information. MyLifeBits
keeps all digital media of each person, including documents, images, sounds and
videos. It organizes them into collections and connects related resources with links
which indicate that one resource annotates another. Haystack automatically creates
connections between documents with similar content and it exploits usage analysis
to extend the desktop search results set.
Stuff I’ve Seen. Dumais et al. [33] developed a system called Stuff I’ve Seen to fa-
cilitate personal information reuse. It builds an index for what a person has seen,
and uses some cues such as ﬁle-type, access date, and author for ﬁltering and sorting
results.
Haystack. Karger et al. [56] developed a customizable general-purpose informa-
tion management tool called Haystack for end users of semi-structured data. It stores
by reference arbitrary objects of interest to a user, and exploits associations among
documents to assist reﬁnding.
Connections. Soules and Ganger [86] developed a ﬁle system search tool called
Connections that combines content-based search with context information (temporal
Authenticated
:27 PM

324
13 Context-Based Information Reﬁnding
relationships between ﬁles) which is gathered from user activities (system ﬁle calls),
where ﬁles are connected through the actions which make up user tasks during a
certain period of time. There are two stages in using this ﬁle search tool: ﬁrst it loc-
ates ﬁles through content-based search and second it, extends those results with
contextually related ﬁles.
Remembrance Agent. Rhodes and Starner [76] built a system called Remembrance
Agent (RA) which augments human memory by displaying a list of documents accord-
ing to the user’s current context and runs continuously without user intervention. For
example, while a user reads an email about a project, the RA reminds him of pro-
ject schedules, status reports, and other resources related to the project. When the
user stops reading the email and starts editing a ﬁle, the RA automatically changes it
recommendations accordingly.
Beagle++. Chirita et al.’s [26] PIM system Beagle++ proposes various activity spe-
ciﬁc heuristics to generate links between resources that associates desktop resources.
It also logs predeﬁned user actions such as attachment saving and ﬁle downloading
to generate associations.
RealFind. As the actions of users in the real and virtual worlds are often inter-
connected, Câmara et al. [12] proposed a tool called RealFind which provides a way
to integrate all information about the users’ items, real or not, helping them to man-
age and retrieve both physical and digital items in meaningful ways. RealFind collects
context-based information about the users’ interactions with their computers and ob-
jects in the physical world, and allows them to search and browse all information in
an integrated way.
Feldspar. Chau et al. [22] developed a system called Feldspar, which supports
multi-level associative retrieval of desktop information, providing a novel interface
that allows users to construct, edit, and visualize a chain of associations as retrieval
queries.
iMecho. Chen et al. [23, 24] built a desktop search system called iMecho (called
XSearcher in [24]) which exploits semantic associations among ﬁles to enhance
full-text keyword-based search. The associations are mined from contents such as
similar-to relationship and users’ such operations as jump-to, copy-from, same-task,
and so on. In iMecho, users ﬁrst search by keywords, and then ﬁnd the target through
navigation in the association graphs.
C-Query. Li and Meng [64] proposed a query approach called C-Query to reﬁnd ref-
erenced ﬁles, given some ﬁle items as input. They identiﬁed three types of reference
relations based on users’ sequence operations, such as adjacent, inclusive, and lin-
eage relations, based on which three adjacency matrixes of ﬁle items are produced.
Query results can thus be computed by multiplying the entry vector with the matrixes.
YouPivot. Hailpern et al. [47] presented a system called YouPivot, which is a con-
textual history-based search tool that supports contextual search by using contextual
cues. For example, a user once sent a Twitter message (a “Tweet”) while she/he was
working on a document, then the user can search for the Tweet (contextual cue) in
Authenticated
:27 PM

13.2 Overview of Information Reﬁnding Techniques
325
order to locate the document. YouPivot allows a user to search through all his/her per-
sonal data or computer usage history (e.g., ﬁles, URLs, physical location, meetings,
and events) for the context she/he does remember. After getting a list of semantic
matched results (context list), the user can Pivot the right context, and then sees what
was going on while that context was active, and ﬁnally locates the desired results. Fur-
ther, YouPivot also displays a visualization of the user’s activity, which allows users to
ﬁnd context by visually browsing their own computer usage. Based on that, a user can
ﬁnd key moments in time by identifying “landmarks” of activity (e.g., sending Twitter
message while working on a document). Besides, YouPivot proposed an annotation
method for contextual recall called TimeMarks, enabling a user to mark a moment
in time as being important, linking the entire user’s activity at that moment (open
web pages, ﬁles, songs, physical location, etc.). This can effectively leave a temporal
landmark for later contextual recall.
Memento. Kulkarni et al. [60] uniﬁed context and content to aid web page revis-
itation. It deﬁned the context of a web page as other pages in the browsing session
that immediately precede or follow the current page, and then extracted topic phrases
from these browsed pages based on the Wikipedia topic list. The full text of web pages
were regarded as content terms in Ref. [60].
Table 13.2 summarizes different personal information reﬁnding mechanisms.
13.2.3 Comparison of Different Reﬁnding Techniques
Different reﬁnding mechanisms emphasize different aspects, and thus possess differ-
ent characteristics. For short- and medium-term information reﬁnding tasks, users
commonly have no difﬁculties to reﬁnd what they want. Some basic reﬁnding tools
and approaches are enough for them to accomplish reﬁnding. For example, if users
want to return to the web pages they just visited, they can directly click the browser’s
back/forward buttons or easily seek help from the browser’s history list. Because users
are often able to remember clearly what they accessed or created a short period of
time ago in their personal desktops, they can also easily locate the desired desktop
resources through simple browsing.
For long-term information reﬁnding tasks, users may encounter some troubles in
reﬁnding their desired results because of their dim memories of the past. Accordingly,
users usually rely on some effective and efﬁcient approaches to reﬁnd what they want.
Search tools are good alternatives. Observing that content-based search tools require
users to provide some detailed content of the information, which is a challenge to
users, context-based search tools that enable users to utilize contextual cues in re-
ﬁnding their desired information are better and more convenient to them. In reality,
contextual information that can be either internal user-related such as user name,
activity and agenda or external environment related such as time, place and surround-
ing people, is always easier to remember than detailed data content and plays an
Authenticated
:27 PM

326
13 Context-Based Information Reﬁnding
Table 13.2: Summary of different personal information reﬁnding methods.
Personal information
reﬁnding mechanisms
Characteristics
File Explorer
Browse-based reﬁnding
Require users to remember the location of ﬁles
SEMEX
Browse information by semantic associations between data items
Restrict to a few associations such as AuthoredBy, Cites and AttachedTo
Google Desktop
Full text search over personal information
Require users to remember detailed contents
Microsoft Windows
Search-based reﬁnding
Desktop Search
Require users to remember ﬁles or folders’ names
Spotlight
Search-based reﬁnding
Require users to remember ﬁles or folders’ names
iMeMex
Offer contextual information on query results
Restrict to a few contextual information
Uniﬁed Structure and
Fuzzy search over structure and content
Content Search
Cannot utilize contextual information to assist reﬁnding
Multi-Dimensional
Fuzzy search over content, metadata, and structure
Search
Cannot utilize contextual information to assist reﬁnding
Forget-me-not
Utilize visible icons to assist reﬁnding
Require users to remember each icon’s meaning
MyLifeBits
Exploit annotations to search desktop information
Support a few contextual cues such as annotations
Stuff I’ve Seen
Enable users to ﬁlter and sort search results by some contextual cues
Support a few contextual cues such as ﬁle type, date, author and mail-to
Haystack
Exploit associations between documents to assist reﬁnding
Support a few contextual cues such as relationships between documents
Connections
Extend search results with contextually related ﬁles
Contexts are only used automatically to enrich the results of ﬁle search
Beagle++
Exploit contextual cues to search desktop information
Restrict to a few contextual information
RealFind
Support reﬁnding both physical and digital items
Limitations in gathering information of physical objects
Remembrance Agent
Recommend users with documents under their current context
Without users’ initiatives
Feldspar
Reﬁnd information through multi-level of associations
Associations are general, need much efforts to locate a particular item
iMecho
Utilize semantic associations to enhance full-text search
Restrict to a few associations
C-Query
Reﬁnd referenced ﬁles based on reference relations
Limitations in the sources of reference relations
YouPivot
Enable contextual search, time marks
The reﬁnding process involves many steps of searching and browsing
Memento
Utilize topic-phrases from access context and page content as reﬁnding
keywords
The access context of a web page is limited to other pages in the browsing
session that immediately precede or follow the current page
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
327
important role in the process of information reﬁnding. Therefore, better context-based
reﬁnding approaches should be developed to support users’ long-term information
reﬁnding tasks.
13.3 Nature-Inspired Context-Based Reﬁnding
13.3.1 Brain’s Memory Recall
Human brains play a wonderful job in memory recall [37]. Let’s ﬁrst learn the natural
way of reﬁnding in human memory, which could offer some inspirations on providing
users with convenient approaches to reﬁnd what they want.
13.3.1.1 Human Memory
(1) Episodic Memory and Semantic Memory
Life scientists discover that there are two kinds of memories in a human brain, i.e.,
episodic memory and semantic memory. Episodic memory receives and stores tem-
porally dated episodes or events, together with their spatial-temporal relations, while
human’s semantic memory, on the other hand, is a structured record of facts, mean-
ings, concepts, and skills that one has acquired from the external world. Semantic
information is derived from accumulated episodic memory. Episodic memory can be
thought of as a “map” that ties together items in semantic memory. The two memor-
ies make up the category of human user’s declarative memory, and work together in
users’ information recollecting activities [105].
(2) Short-Term and Long-Term Memory
Human memory is divided into a short-term working memory and a long-term per-
manent memory [4]. According to Tarpy and Mayer [88], there are two apparent
peculiarities of the short-term memory: the memory store is limited in capacity and it
requires rehearsal. Information may be recorded slightly but still retain its basically
nonmeaningful form for short-term memory, while the characteristics of long-term
memory are quite different: information is organized rather than kept in its original
form.
(3) Memory Encoding
Based on the research results of psychologists Tulving and Thomson [100], what is
stored is determined by what is perceived and how it is encoded, and what is stored
determines what retrieval cues are effective in providing access to what is stored, and
how well a thing is remembered does not only depend on what it is, but also on
how it is stored in memory. They believed that the memory trace, which is the cent-
ral representation of the to-be-remembered event, is a multi-dimensional collection
of elements, features, or attributes. Memory encoding is to transform the incoming
Authenticated
:27 PM

328
13 Context-Based Information Reﬁnding
information, for example, what we see, what we hear, what we think, and what we
feel, into the form the brain memory can remember [83].
To represent and form a memory of an organism’s experience, the brain relies
on a large population of neurons and creates different associations among neurons.
Through experiments, biologist Tsien [98] discovered that a linear ﬂow of signals from
one neuron to another is not enough to explain how the brain represents perceptions
and memories. Rather the coordinated activity of a large number of neurons is needed.
During the process of memory encoding, different associations among neurons are
created. To remember a new coming event, the brain will always relate it to other facts
or knowledge already known [36]. The process of memory encoding is illustrated in
Figure 13.1.
(4) Memory Retrieval
When a person recalls a past event, information is retrieved from episodic memory,
which involves an interaction between a retrieval cue that is self-generated or
provided by the environment and a memory trace, leading to the reconstruction of
some or all aspects of the episode represented by the trace [80]. Maratos et al. [67]
stated that neural activity mediating episodic retrieval of contextual information
and its subsequent processing is modulated by emotion in at least two ways. First,
there is enhancement of activity in networks supporting episodic retrieval of neut-
ral information. Second, regions known to be activated when emotional information
is encountered in the environment are also active when emotional information is
retrieved from memory.
James [49] described the memory retrieval process as follows:
We make search in our memory for a forgotten idea, just as we rummage our house for a lost object.
In both cases we visit what seems to us the probable neighborhood of that which we miss. We turn
over the things under which, or within which, or alongside of which, it may possibly be; and if it lies
near them, it soon comes to view.
According to Buzan and Buzan [10], association is one of the main factors in recall.
Things are not isolated, and they are related to each other to some extent. A cent-
ral characteristic of episodic memory is the retrieval of the rich context of real-life
events [9]. During retrieval, a subset of the neocortical event representation can cause
reactivation of the entire simple representation in the hippocampus and thence react-
ivation of the entire neocortical representation. As a result, association and context
are generally viewed as two of the central ideas in the history of episodic memory
What we see, hear,
think, and feel
Encoding
process
Memory
Figure 13.1: Memory encoding [83].
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
329
Cue A
Cue B
......
Induce
Target item
Figure 13.2: Cue-target retrieval in human
memory.
research [54], where retrieval of episodic memories is a cue-dependent process that
reﬂects the temporal contiguity and the semantic relationship of the cue and the target
entities. The role of both temporal and semantic factors in retrieval appears strikingly
in the analysis of retrieval transitions in free recall. The way of cue-target retrieval is
illustrated in Figure 13.2, which shows that a few pieces of cues may induce the target
item people want to recall.
13.3.1.2 Contextual Cues
During the cue-target retrieval process, contextual cues play an important role in the
process of helping users reﬁnd their desired information. Utilizing contextual cues to
assist reﬁnding is a natural way in human memory.
Despite many PIM tools being developed so that users do not need to hold all of
their information in their brain and can be helped reﬁnd their previously stored in-
formation, both the searching and managing approaches place the load for successful
recovery of information on the user’s memory [39]. Barreau [6] examined context that
can be easily remembered in human memory as a factor in PIM systems to suggest how
it may inﬂuence classiﬁcation decisions and ultimately retrieval. The use of contextual
cues for reﬁnding information is often suggested, such as location [58]. Besides, users’
recall reliability is also taken into account and memory cues are used to enhance tra-
ditional content-based personal information searching [23, 25]. The study of Teevan
et al. [96] showed that people use contextual information associated with their targets
to guide their navigation in relatively small steps, as they gradually recall pieces of
information associated with the targets.
In order to understand what contextual cues impact reﬁnding, many research-
ers pay considerable attention to users’ real reﬁnding tasks in different applications.
Elsweiler et al. [37] performed a user study concentrating on email reﬁnding. They
investigated what attributes (time, user’s experience, etc.) of email messages users
remember in their reﬁnding actions, and looked at how the attributes change in differ-
ent situations and studied the factors which impact on what is remembered. Elsweiler
et al. [38] investigated the contextual factors that inﬂuence users’ perception of task
difﬁculty in the situation of reﬁnding email messages, where 21 participants perform
reﬁnding tasks on their own personal collections. They found that the time lapsed
since a message was read, when the sought-after email was sent, the other recipients
of the email, the experience of the user and the user’s ﬁling strategy have strong re-
lationships with difﬁculty. Capra [16] studied the factors of task type and the elapsed
time in information reﬁnding. Moon and Fu [70, 71] studied how spatial locations and
Authenticated
:27 PM

330
13 Context-Based Information Reﬁnding
luminance affect ﬁnding and reﬁnding information in a desktop environment. Capra
and Pérez-Quiñones [19] studied several factors such as task, individual and time that
can affect users’ information reﬁnding process. Kelly et al. [59] conducted a user study
of remembered context data and its utility in personal information retrieval, showing
that much of the textual content failed to be recalled, while some context informa-
tion (such as time and location) and ﬁle types are well remembered over a 6-month
period.
13.3.2 Information Reﬁnding by Structured Context
Inspired by human memory and its natural recall characteristics, this section details
an information reﬁnding strategy based on users’ previous access context. It works
upon a context memory, which speciﬁes clustering and associative structures of ac-
cess context instances, as well as their dynamic life-cycle evolution strategies. Based
on the context memory, a query-by-context model is built to support users reﬁnding
previously encountered web pages or ﬁles.
13.3.2.1 Context Memory Management
To support context-based information reﬁnding, the ﬁrst thing to do is to build a
context memory model.
(1) Short-Term and Long-Term Context Memory Units
In human brain memory, what is stored is determined by what is perceived and how it
is encoded; and what is stored determines what retrieval cues are effective in provid-
ing access to what is stored [88, 100]. Such a human memory is simulated as a context
memory organized into a short-term context memory unit (SCM) and a long-term
context memory unit (LCM), as illustrated in Figure 13.3.
–
SCM is limited in capacity and lasts for a short period of time in the order of
seconds.
–
LCM is unlimited in capacity and lasts as short as a few days or as long as dec-
ades. There are two types of memory units in LCM: permanent and evolving.
The former records life-long accessing experiences and is immutable, while the
latter will decay.
The circumstance under which a user’s information access occurs is called an access
context. Contextual information can be either internal user-related (like user name,
activity, agenda, etc.) or external environment-related (like time, place, surrounding
people, etc.) [40]. If the accessed information is of interest to the user, a linkage
between the access context instance and the information identiﬁer (context-instance,
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
331
Access
event
Retrieval
Lost
Ø
Ø
Effective
Decay
Lost
Reinforce
repeatedly
Short-team content
memory unit
General unit
Permanent unit
Contextually accessed
entity repository
Long-team content
memory unit
Figure 13.3: The framework of context memory.
ID) is created and stored into the contextually accessed entity repository. Such an
access event is called an effective access event.
(2) Interaction between Two Context Memory Units
Context memory is dynamic and information transits across the two memory units as
follows.
–
For an accessing event received by SCM, if it is effective, i.e., the user engages in
“rote” rehearsal of it by storing the information into the contextually accessed
entity repository, it will be transferred to LCM; otherwise, it will be lost very
quickly.
–
In LCM, if the access context is profound or biting to the user (e.g., dangerous
disaster situation), it will be stored in the permanent unit; or else it will be stored
in the evolving unit. Most effective accessing events are memorized in the evolving
unit due to the infrequency of permanent cases in one’s life.
–
Contextual information in the evolving unit will decay gradually in life-cycles as
time goes by.
–
When a context instance in LCM is recalled, it is brought back to SCM to strengthen
its freshness and retention, thus slowing down the degradation.
In human memories, SCM only acts as a temporary working memory, lasting for a very
short period. It drops unnecessary events and passes only effective accessing events
into LCM. As user’s memorized access context for recall is organized in LCM, i.e., only
LCM plays a role in reﬁnding queries, this study considers LCM and leaves SCM to
a further study. Also, because permanent access context is far less than ordinarily
evolving context in reality, we focus on the long-term evolving context memory unit
in the following.
Authenticated
:27 PM

332
13 Context-Based Information Reﬁnding
(3) Static Status of Context Memory
1) Contextual Attributes and Hierarchies of Contextual Attribute Values
Access context is comprised of n contextual attributes (A1, A2, ⋅⋅⋅, An), and the do-
main of each contextual attribute forms an ordered hierarchy of levels of abstraction.
The hierarchy of context attribute A is a lattice (H, ≺h), where H = (h1, h2, ⋅⋅⋅, hs–1, All)
of s levels corresponding to the levelId (1, 2, ⋅⋅⋅, s–1, s), and ≺h is a partial order among
the levels of H, such that (h1 ≺h hi ≺h All) for every 0 < i < s. The edge linking two
consecutive hierarchical levels hi and hi+1 in H has a weight in [0, 1] to express the
hierarchical similarity between hi and hi+1. As attribute values at two higher levels
(e.g., 2010 and 2010-8) are more general and less discriminative than those at two
lower levels (e.g., Early Oct. and 2010-10-1), the hierarchical similarity si,i+1 should be
smaller than sj,j+1 when (i > j).
Deﬁnition 37. Given an s-leveled contextual attribute hierarchy (h1, h2, ⋅⋅⋅, hs–1, All),
the hierarchical similarity si,i+1 between the two consecutive levels hi and hi+1 is
si,i+1 = 1-i/s, where (0 < i < s).
◻
Example 43. Figure 13.4 depicts two hierarchy examples for contextual attribute Time
and Location. Time has ﬁve hierarchical levels, namely, s = 5. s1,2 = 1-1/5 = 0.8, s2,3 = 0.6,
s3,4 = 0.4, s4,5 = 0.2.
◻
We further order nodes at each hierarchical level based on the distances of their attrib-
ute values from a selected referential value. The ordering of the contextual attribute
values is application dependent.
Example 44. In Figure 13.4, taking 2010-10-1 as the referential time value, as 2010-10-3
is more adjacent to 2010-10-1 than 2010-10-8, it is listed before the latter at the day
level. Also, Kansas is geographically closer to the referential location value New York
than Idaho, and thus stays before Idaho at the State level in Location hierarchy.
◻
Deﬁnition 38. Let c and c󸀠be two contextual attribute values of A, where (c, c󸀠∈
Dom(A)), and c and c󸀠may be located at the same or different hierarchical levels in
H. Assume function h(A, v) returns the id of the hierarchical level which v belongs to.
c󸀠is called an ancestor of c, denoted as c ≺a c󸀠(reversely, c is called a descendant of
c󸀠), if and only if h(A, c) < h(A, c󸀠) and there exists an upward path from c to c󸀠.
◻
2) Similarity between Two Contextual Attribute Values
The computation of the similarity between two contextual attribute values is sub-
ject to their distances at hierarchical levels, as well as their ordering distances when
prompted to the same level.
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
333
h5
h3
h2
h1
h4
h3
h2
h1
2010–
10–1
2010–
10–3
2010–8
2010–10
2010–11
2010
All
(0.08, 0.26)
h4
All
(0.08, 0.305)
(0.305, 0.53)
(0.53, 0.755)
(0.26, 0.44)
(0.44, 0.62)
(0.62, 0.8)
(0.8, 0.98)
(0.755, 0.98)
Retention
internal
Retention
internal
Early Oct.
Mid Oct.
Late Oct.
2010–
10–8
2010–
10–18
2010–
10–24
Time
2010–
10–25
0.2
0.25
0.5
0.75
New York
Kansas
Idaho
USA
North America
Canada
Quebec
Ontario
Location
0.4
0.6
0.8
Similarity
Si,i+1
Similarity
Si,i+1
Figure 13.4: Contextual hierarchies for Time and Location.
Deﬁnition 39. Given two contextual attribute values c and c󸀠in Dom(A), let k = h(A, c),
k󸀠= h(A, c󸀠). The similarity between c and c󸀠, denoted as sim(A, c, c󸀠), is deﬁned as
follows:
–
if c = c󸀠, sim(A, c, c󸀠) = 1;
–
if c󸀠≺a c, sim(A, c, c󸀠) = ∏k–1
i=k󸀠si,i+1;
–
if c ≺a c󸀠, sim(A, c, c󸀠) = ∏k󸀠–1
i=k si,i+1;
–
if c and c󸀠are two brothers with the same parent p, let n be the total number of p’s
children, nc and nc󸀠are the ordered positions of c and c󸀠among the p’s children
sim(A, c, c󸀠) = sk,k+1 – |nc – nc󸀠|
n – 1
⋅(sk,k+1 – s2
k,k+1);
–
if c and c󸀠are at two different hierarchical levels with a common ancestor p, let
m = h(A, p),
sim(A, c, c󸀠) =
m–1
∏
i=k
si,i+1 ⋅
m–1
∏
j=k󸀠
sj,j+1.
◻
Authenticated
:27 PM

334
13 Context-Based Information Reﬁnding
If c and c󸀠are the same, the similarity between them is deﬁnitely equal to 1; otherwise,
their similarity depends on the hierarchical edges linking them. Generally, the number
of edges from c to c󸀠(or c󸀠to c) represents the difference between them. Particularly, if c
and c󸀠are two brothers, their ordering distance is also taken into account in computing
their similarity.
Example 45. In Figure 13.4, sim(Time, 2010-10-1, 2010-10-3) = 0.8 – |2–1|
3–1 ⋅(0.8 –
0.82) = 0.72, sim(Time, 2010-10-1, Early Oct)
=
0.8 and sim(Time, 2010-10-8,
2010-10) = 0.48.
◻
With the similarity deﬁnition, given three contextual attribute values c1, c2, c3 in
Dom(A), we can deduce the following theorems easily:
–
If c1 ≺a c2 and c2 ≺a c3, then sim(A, c1, c3) < sim(A, c2, c3);
–
If c and c󸀠share the same parent, let h(A, c)
=
h(A, c󸀠)
=
k, then s2
k,k+1 ≤
sim(A, c, c󸀠) < sk,k+1.
3) Context Instances and Similarity Between Two Context Instances
A context instance is an instantiation of its n contextual attributes, represented as
a tuple C = (c1, c2, ⋅⋅⋅, cn), where ci ∈Dom(Ai) for every 1 ≤i ≤n. Based on the simil-
arities of their respective contextual attribute values, we can measure the similarity of
two context instances as follows.
Deﬁnition 40. The similarity between two context instances C = (c1, c2, ⋅⋅⋅, cn) and
C󸀠= (c󸀠
1, c󸀠
2, ⋅⋅⋅, c󸀠
n) is
Sim(C, C󸀠) =√1
n
n
∑
i=1
sim2(Ai, ci, c󸀠
i)
(13.1)
◻
Example 46. For two 2-dimensional context instances C = (2010-10-1, New York) and
C󸀠= (2010-10-3, New York) in Figure 13.4, the similarity between C and C󸀠is:
Sim(C, C󸀠) = √1
2 × (0.722 + 1) = 0.87.
◻
Deﬁnition 41. Assume context is an n-dimensional vector (A1, A2, ⋅⋅⋅, An). Let C = (c1,
c2, ⋅⋅⋅, cn) and C󸀠= (c󸀠
1, c󸀠
2, ⋅⋅⋅, c󸀠
n) be two context instances.
–
C is equal to C󸀠, denoted as C󸀠= C, if and only if ∀i ∈{1, 2, ⋅⋅⋅, n}
(c󸀠
i = ci).
–
C is more general than C󸀠, denoted as C󸀠≺C, if and only if
∀i ∈{1, 2, ⋅⋅⋅, n} ((c󸀠
i ≺a ci) ∨(c󸀠
i = ci)) ∧∃j ∈{1, 2, ⋅⋅⋅, n} (c󸀠
j ≺a cj).
–
C is associated with C󸀠on contextual attribute Ai, denoted as C
Ai↔C󸀠, if and only
if sim(A, ci, c󸀠
i) ≥(, where ( is a threshold value set beforehand.
◻
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
335
Example 47. Referring to the hierarchies of Time and Location in Figure 13.4,
C2 = (2010-10, USA) is strictly more general than C1 = (2010-10-1, Idaho).
C1 is associated with C2 through the contextual attribute Location when ( = 0.7,
since sim(Location, USA, Idaho) = 0.75 > (.
◻
Deﬁnition 42. Centered around a representative contextual attribute value r ∈
Dom(A), a set of context instances form a cluster, denoted as CC(A, r), ∀C =
(c1, c2, ⋅⋅⋅, cn) ∈CC(A, r) sim(A, ci, r) ≥$ ∧(ci ∈Dom(A)) ∧((ci = r) ∨(ci ≺a r)), where $
is a clustering threshold value in [0,1].
◻
Example 48. In Figure 13.4, two context instances (2010-10-1, New York) and (2010-10,
USA) can form a cluster CC(Location, USA) when $ = 0.7.
◻
4) Context Memory Snapshots
Deﬁnition 43. A context memory snapshot is a graph CM = (VCC, ECC), where VCC is
a set of vertices (representing context instance clusters) and ECC is a set of edges on
vertices (representing association relationships of context instances).
◻
The context memory snapshot evolves as time goes by.
Example 49. Consider CM2 in Figure 13.5 which displays several context memory
snapshots. Each box in CM2 represents a context instance Ci, comprising of three
attributes: ci1 means Time, ci2 means Location, and ci3 means Activity. Each dotted
ellipse expresses a cluster, and the edges linking context instances are associations
(e.g., C1↔A2C4 means that context instances C1 and C4 are associated by the second
attribute, Location).
◻
(4) Dynamic Evolution of Context Memory
1) Context Degradation
The context memory undergoes a life-cycle degradation, where contextual at-
tribute values independently decay upwards along the attribute hierarchy. When a
context instance has all its attribute values decayed to All, we think the context
instance has been forgotten and thus delete it from the context memory.
In order to measure context degradation, we exploit the memorized state of
a contextual attribute value. Psychology studies have shown that the exponential
in the square root of time is an appropriate function affecting one’s memorizing
strength [79].
Deﬁnition 44. Let A be a contextual attribute with a value c ∈Dom(A) (which could
be a scalar value, a string, or All representing any possible value). The retention
Authenticated
:27 PM

336
13 Context-Based Information Reﬁnding
CM1
CM4
Ø
CM'4
CM'3
CM3
CM2
30 Days
later
60 Days later
60 Days later
C3 was recalled
and reinforces
Without
reinforcement
90 Days
later
90 Days
later
180 Days later
C1 (Pie)
C1 (Pie)
C2 (Dessert)
C3 (Tuna)
C3 (Tuna)
C2 (Dessert)
C3 (Tuna)
C3 (Tuna)
C2 (Dessert)
C4 (Cake)
C3 (Tuna)
C4 (Cake)
C2 (Dessert)
C11 = “2010–09–23”
C11 = “2010–09”
C21 = “2010–09”
C22 = “Park”
C23 = “Chat”
C1
C4
A2
C12 = “Public”
C13 = “Free”
C31 = “2010–10–08”
C31 = “2010–10”
C41 = “2010–10”
C42 = “Public”
C43 = “Busy”
C32 = “Home”
C33 = “Watch TV”
C41 = “2010–10–12”
C21 = “2010”
C31 = “2010”
C32 = “Home”
C33 = “Free”
C21 = “2010”
C22 = “Public”
C23 = “Free”
C31 = “2010–10”
C32 = “Home”
C33 = “Watch TV”
C31 = “2010”
C32 = “Home”
C33 = “Free”
C22 = “Public”
C23 = “Free”
C42 = “Office”
C43 = “Read”
C32 = “Living room”
C33 = “Watch TV”
C21 = “2010–09–30”
C22 = “Park”
C23 = “Chat”
C12 = “Subway Station”
C13 = “Free”
Figure 13.5: An example of context degradation: from CM1 to CM2, some clusters were merged and
new associations were created; from CM2 to CM󸀠
3, C3 was reinforced and thus decayed more slowly
than others did.
strength of c, denoted as R(A, c, t), is a real number R ∈[0, 1], characterizing the
memorized state of c after elapsing time t (also called age).
R(A, c, t) =
{
{
{
{
{
{
{
r0 ⋅0
if r0 < (min
r0 ⋅e–+√t if (min ≤r0 < (max
r0 ⋅1
if r0 ≥(max
(13.2)
where r0 is the initial value of retention strength, + is the decay rate, (max and (min
are the maximum and minimum thresholds, indicating the upper and lower bounds
of the long-term evolving memory unit.
–
If (r0 < (min): c’s retention strength R(A, c, t) = 0. c falls into the short-term memory
unit and will be dropped out of the context memory.
–
If (r0 ≥(max): c’s retention strength R(A, c, t) remains unchanged, and is situated
in the long-term permanent memory unit without degradation.
–
If ((min ≤r0 < (max): c’s retention strength R(A, c, t) is weakened in the long-term
evolving memory unit as time passes.
◻
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
337
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
10
20
30
40
50
60
70
80
90
Retention strength R
Age t of contextual attribute value c (in day)
A1(0.87, 0.11)
A2(0.66, 0.25)
A3(0.38, 0.32)
A4(0.73, 0.54)
A5(0.53, 0.82)
Figure 13.6: Retention strength of
contextual attribute value c under
different settings of (r0, +).
Parameters r0, +, (max, and (min are user-dependent, since different people memorize
events differently. The bigger + is, the more R(A, c, t) drops, signifying the faster c de-
grades. According to Deﬁnition 44, the attribute value c is clearly remembered when
its R(A, c, t) approaches 1, and tends to be forgotten when its R(A, c, t) is close to 0.
Example 50. Figure 13.6 shows c’s retention strengths under different settings of (r0,
+), where (max = 0.98 and (min = 0.08 in the study. At the beginning, the retention
strengths decrease very rapidly. As time goes by, however, they decrease slower and
slower.
◻
Example 51. For the contextual attribute A = Time, assume its contextual attribute
value c is 2010-10-18. Let the initial retention strength r0 = 0.9 and decay rate + = 0.1.
In 30 days (t = 30), c’s retention strength becomes R(A, c, t) = 0.9 ⋅e–0.1√30 = 0.52.
◻
After obtaining c’s retention strength R(A, c, t), we then need to compute c’s concrete
attribute value. Since c degrades along the hierarchy, as long as we can identify the
corresponding level to which c degrades, we can obtain c’s value. To do that, we bind
a retention strength interval [(i, (󸀠
i) to each level hi of a contextual attribute hierarchy.
The level whose retention strength interval holds R(A, c, t) (i.e., (i ≤R(A, c, t) < (󸀠
i) is
the one (i.e., level hi) where c is situated.
Deﬁnition 45. For an s-leveled contextual attribute hierarchy (h1, h2, ⋅⋅⋅, hs–1, All), the
retention strength interval of level hi where 1 ≤i < s, denoted as [(i, (󸀠
i), is computed
by equally dividing [(min, (max) into s intervals of the same length len = ((max –(min)/s.
That is, (i = (max – i ⋅len and (󸀠
i = (max – (i – 1) ⋅len.
◻
The lower level value c is at, the bigger its retention strength should be. The equal-
length retention strength intervals guarantee attribute values at the higher levels
decay slower, thus lasting longer than those at the lower levels. This complies to
Figure 13.6.
Authenticated
:27 PM

338
13 Context-Based Information Reﬁnding
Example 52. Consider the contextual attribute Time in Figure 13.4. ((max – (min)/5 =
0.18. The retention strength intervals from h1 to h5 are [0.8, 0.98), [0.62, 0.8),
[0.44, 0.62), [0.26, 0.44) and [0.08, 0.26), respectively.
◻
Example 53. Consider the contextual attribute value 2010-10-18 in Example 51. After 30
days, its retention strength decreases to 0.52, falling into the retention strength inter-
val [0.44, 0.62) of level h3. Thus, the contextual attribute value will decay to be 2010-10.
If its retention strength decreases to 0.8, namely, still at level h1, then it remains to be
2010-10-18.
◻
Comparing the retention strength interval [(i, (󸀠
i) of level hi and the hierarchical sim-
ilarity si,i+1 between hi and hi+1, the former reﬂects human’s context memorization
mechanism and dynamic status, which is quite subjective and user-dependant, while
the latter expresses the similarity between two attribute values at the two consecutive
hierarchical levels, which is objective and user-independent. Hence, we treat reten-
tion strength intervals and hierarchical similarities differently, and deal with them
separately in the study.
2) Context Reinforcement
In the process of decay, certain parts of context memory are sometimes reinforced
through users’ context-based recalling and feedback conﬁrmation. For example, the
value c of contextual attribute A is referred to multiple times for recalling. In this case,
we slow down the decay rate of c (i.e., + in eq. (13.2) by a percentage of $+ to get a
bigger retention strength returned by R(A, c, t). + is set to 90% (when c is used to formu-
late a reﬁnding request) and 95% (when the reﬁnding result based on c is conﬁrmed
positively by the user) of its previous value in this study. The smaller + is, the slower
the value degrades. When + approaches 0, the attribute value will nearly remain un-
changed, implying the user can always remember the context value clearly. Figure 13.5
illustrates the degradation and reinforcement of context memory.
13.3.2.2 Context-Based Reﬁnding Model and Process
Context-based reﬁnding differs from the traditional database query conceptually in
three aspects. First, request formulation is based on contextual attributes rather than
database contents. Second, query target is context memory snapshot rather than data-
base. Third, an intermediate query result is a ranked list of context instances, with
their linked information as the ﬁnal query result. At the implementation level, the
query target (i.e., context memory snapshot) is organized in a hierarchical, cluster
and associated manner, and dynamically evolves in life cycles according to query
user’s memorization strength. As the ﬁnal result generation via the context instances
is quite straightforward, the following discussion focuses on the intermediate result
computation.
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
339
(1) Context-Based Reﬁnding Model
A context-based reﬁnding query can be denoted as a function RF(Q, CM) = ⟨C1,
C2, ⋅⋅⋅, Cm⟩, where Q is the query request formulated in the form of a context instance,
CM is the query target that is the context memory snapshot, and the intermediate
query result of Q upon CM is a ranked list of context instances in CM, ⟨C1, C2, ⋅⋅⋅, Cm⟩,
whose ranking is determined by a ranking function. In the study, three different
ranking methods are considered based on simple similarity, weighted similarity, and
negative dissimilarity between Q and C. Let Q = (q1, q2, ⋅⋅⋅, qn) and C = (c1, c2, ⋅⋅⋅, cn)
without loss of generality.
Ranking by simple similarity. A straightforward way is to use the similarity
function (eq. (13.1)) to rank context instances in the memory snapshot against Q:
Rank(Q, C) = Sim(Q, C) = √1
n
n
∑
i=1
sim2(Ai, qi, ci)
(13.3)
Ranking by weighted similarity. Considering that a user’s query request Q may be
vague as well due to the vague memory along with time, and some contextual attrib-
ute values like activity may leave a deeper impression than others such as time, we
incorporate a weight vector (w1, w2, ⋅⋅⋅, wn) for different contextual attribute values in
Q to state their precise degrees, where wi ∈[0, 1] for every 1 ≤i ≤n, and ∑n
i=1 wi = 1.
Rank(Q, C) = √
n
∑
i=1
wi ⋅sim2(Ai, qi, ci).
(13.4)
Ranking by negative dissimilarity. The similarity between Q and C can also be
calculated via their dissimilarity:
Dissim(Q, C) = 1 –
n
min
i = 1 sim(Ai, qi, ci)
Rank(Q, C) = 1 – Dissim(Q, C) =
n
min
i = 1 sim(Ai, qi, ci).
(13.5)
The three similarity ranking methods exhibit the same performance in the experi-
mental study.
(2) Context-Based Reﬁnding Process
A query (context instance) may or may not exactly match to a context instance in the
memory due to the degradation of query target (context memory snapshot). Three
kinds of matching between Q and C are considered, as illustrated in Figure 13.7(a–
c), which, respectively, are (1) exactly matching where Q = C; (2) speciﬁcally matching
where C ≺Q; and (3) generally matching where Q ≺C. In this study, given a query Q,
the exactly and speciﬁcally matching contexts are included in the reﬁnding results. It
is natural for a user to get the contexts satisfying Q = C or C ≺Q when she/he issues a
query Q.
Authenticated
:27 PM

340
13 Context-Based Information Reﬁnding
All
Busy
Free
q3 = “Free”
Write
Read
Chat
Watch TV
(b)
All
Busy
Free
Write
Read
Chat
q3 = “Watch TV”
Watch TV
(a)
All
Busy
Free
q3 = “Watch TV”
Write
Read
Chat
Watch TV
(c)
Figure 13.7: Three matching between query Q and context instances in the context memory snapshot:
(a) exactly matching; (b) speciﬁcally matching; and (c) generally matching.
A straightforward way to reﬁnd information by Q is to scan existing context instances
in CM, and return those exactly or speciﬁcally matching ones. The obtained context
instances are then ranked by the ranking function. The ﬁrst dominate matching part
has a time complexity O(n ⋅|CM|), where n is the number of context dimension, and
|CM| is the total number of context instances in CM. Apparently, this naive solution
cannot scale up well with a large volume of existing and consistently incoming context
instances in the memory. Efﬁcient reﬁnding strategies are needed. In the following,
we describe a reﬁnding approach making use of cluster and association relationships
among context instances.
1) Clustering of Context Instances
For each contextual attribute Ai, we generate a set of context instance clusters as
follows.
–
Identifying a representative attribute value r for a new cluster CC(Ai, r). From the
set of unclustered context instances in CM, ﬁnd the one whose attribute value of Ai
situates at the highest hierarchical level, and take it as the representative attribute
value r of a cluster.
–
Populating the cluster with unclustered context instances in CM. For each un-
clustered context instance C in CM, if its attribute value is the same as r or is
descendant of r, and its similarity to r is no less than a cluster threshold, then
put C into CC(Ai, r), where CC(Ai, r) = {C|(C ∈CM is unclustered) ∧((ci ≺a r) ∨(ci =
r)) ∧sim(Ai, ci, r) ≥$}.
–
Repeat steps 1 and 2 until all instances in the memory are clustered.
We obtain n cluster sets CL(A1), CL(A2), ⋅⋅⋅, CL(An). For ∀i (1 ≤i ≤n), CL(Ai) =
{CC(Ai, r1), CC(Ai, r2), ⋅⋅⋅, CC(Ai, rs)}, where s is the total number of clusters in CL(Ai)
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
341
and CC(Ai, rj) is a context instance cluster for every 1 ≤j ≤s. The obtained clusters are
exclusive, i.e., a context instance only belongs to a single cluster.
After clustering, we can check the matching of Q on a cluster base instead of in-
stance base. We call a cluster CC(Ai, r) a candidate cluster of Q, if it contains instances
that may possibly match Q, i.e., satisfying one of the following conditions: (1) r = qi;
(2) r ≺a qi; (3) qi ≺a r and sim(Ai, qi, r) ≥$. As Q may indicate n attribute values
for matching, we choose the attribute value leading to the least number of candidate
clusters to start matching, namely, the one locating at the lowest hierarchical level in
all n attributes.
2) Association of Context Instances
For each contextual attribute Ai, and every value in its hierarchy, we build an
association chain Chain(Ai, v), which consists of all the context instances with the
same attribute value of Ai. That is, for any context instance C ∈Chain(Ai, v), (ci = v).
Figure 13.8 illustrates six 3-dimensional context instances in the memory snapshot. A
few chains are illustrated in the left-hand side of Figure 13.8 .
To facilitate exactly and speciﬁcally matching between the query and context in-
stances, we extend association chains to include all the descendants based on the
contextual attribute hierarchies, and obtain EChain(Ai, v), so that for any context in-
stance C ∈EChain(Ai, v), (ci = v) or (ci ≺a v). For example, since 2010-09, 2010-10 ≺a
2010, Chain(A1, 2010) is extended to include {2010, 2010-09, 2010-10}, as shown in the
right-hand side of Figure 13.8.
Given a query Q = (2010, Home, Chat), to ﬁnd the matching context instances, we
can start from the extended association chain with the shortest length, and then check
the matching of each chained context instance against other contextual attribute
values requested in Q.
C1(2010, Home, Chat)
C2(2010–09, Bedroom, Free)
C3(2010–10, Living room, Chat)
C6(2010–10, Living room, Free)
C5(2010–09, Home, Chat)
C4(2010, Bedroom, Free)
2010
C1
C2
C3
C1
C2
C3
C2
C1
C3
C5
C1
C3
C5
C4
C6
C2
C4
C6
C6
C3
C6
C4
C2
C4
C5
C1
C5
C4
C1
C4
C5
C2
C5
C6
C3
C6
2010–09
2010–10
Home
Bedroom
Living
room
Free
Chat
2010
2010–09
2010–10
Home
Bedroom
Living
room
Free
Chat
Association chain
Extended association chain
Figure 13.8: Context–association relationships.
Authenticated
:27 PM

342
13 Context-Based Information Reﬁnding
3) Cluster–Association-Based Reﬁnding Algorithm
We explore context cluster and association relationships in reﬁnding. Given a
query Q, the cluster-based approach checks every context instance in the candidate
clusters. If we build association chains for the context instances within each cluster
on the right attribute, we probably could skim the irrelevant chains immediately. The
time cost of association-based reﬁnding approach depends on the length of the selec-
ted extended association chain, which guides us to choose an appropriate attribute to
build association chains for context clusters, i.e., select the attribute that can scatter the
context instances by the greatest extent.
We denote the modiﬁed cluster as CC†(Ai, r). We can also obtain n cluster sets
CL†(A1), CL†(A2), ⋅⋅⋅, CL†(An). For ∀i (1 ≤i ≤n), CL†(Ai) = {CC†(Ai, r1), CC†(Ai, r2), ⋅⋅⋅,
CC†(Ai, rs)}, where each cluster CC†(Ai, rj) (1 ≤j ≤s) has a set of extended associ-
ation chains EChain†(Aji, v) on attribute Aji. Figure 13.9 shows an example of context
cluster–association relationships. The context instances are clustered on the time
attribute. There are three clusters with 2010, 2010–09 and 2010–10 as their repres-
entative attribute values respectively. The ﬁrst two are built association chains on the
location attribute (Asso-dim = 2) and the third one is built on the activity attribute
(Asso-dim = 3).
Algorithm 3 shows the pseudocode of cluster–association-based reﬁnding ap-
proach. Compared to cluster-based approach, after identifying each candidate cluster,
it gets the attribute on which associations built in the cluster, and then check each
context instance in the corresponding extended association chain.
Correctness. The candidate clusters contain all possible context instances that
match Q. In each candidate cluster, all the context instances that possibly match the
C1(2010, Home, Chat)
C2(2010–09, Bedroom, Free) C3(2010–10, Living room, Chat)
C6(2010–10, Living room, Free)
C5(2010–09, Home, Chat)
C4(2010, Bedroom, Free)
C1
Home
Bedroom
2010
2010–10
2010–09
Home
Bedroom
Free
Chat
C4
C5
C2
C6
C3
Cluster
Representative
attribute value
Asso-dim = 3
Asso-dim = 2
Asso-dim = 2
Figure 13.9: Context cluster–association relationships.
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
343
query Q are included in the selected extended association chain EChain†(Ak, qk). So
the result list contains all matching context instances.
Time Complexity Analysis. Selection of a starting contextual attribute Ai from
among the n cluster sets takes O(n). Filtering out each candidate cluster CC†(Ai, rj)
in CL†(Ai) takes O(|CL†(Ai)|). Get the extended association chain EChain†(Ak, qk) from
CC†(Ai, rj) takes O(1). Checking whether a context instance in EChain†(Ak, qk) matches
the query Q takes O(n ⋅Length(EChain†(Ak, qk))). Therefore, the total time cost is
O(|CL†(Ai)|) + O(n ⋅Length(EChain†(Ak, qk))). Apparently, the time cost depends on
the number of clusters and the size of clusters, which are sensitive to the clustering
threshold $.
Space Complexity Analysis. As for each cluster CC†(Ai, rj) in CL†(Ai), we need to
store its representative contextual attribute value and the attribute values for the
extended association chains. Thus the additional space cost can be estimated as
∑n
i=1 ((' + "k ⋅|EChain†(Ak)|) ⋅|CL†(Ai)|), where ' is the cost for storing a representat-
ive attribute value of a cluster, "k is the cost for storing a attribute value in Dom(Ak),
|EChain†(Ak)| is the number of extended association chains of a cluster, and |CL†(Ai)|
is the number of context clusters in CL†(Ai).
13.3.2.3 A Context-Based Reﬁnding System CxReFinder
A context-based information reﬁnding system called CxReFinder is implemented. It
facilitates users to annotate any interesting web pages or local ﬁles encountered with
Algorithm 3: Cluster-$-Association-Based Reﬁnding.
Input:
A context memory snapshot CM and query Q
Output:
A ranked list of context instances L that match Q
1: Let CLUSTER-SET† = {CL†(A1),CL†(A2), ⋅⋅⋅, CL†(An)} be a set of cluster
2: sets, where CL†(Ai) = {CC†(Ai, r1), CC†(Ai, r2), ⋅⋅⋅, CC†(Ai, rs)} for every (1 ≤i ≤n);
3: L = 0;
4: Ai = SelectAtt(CLUSTER-SET†, Q);
5: for each CC†(Ai, rj) ∈CL†(Ai) do
6:
if (rj = qi) ∨(qi ≺a rj ∧sim(Ai, qi, rj) ≥$) ∨(rj ≺a qi) then
7:
Ak = GetAssociationAtt(CC†(Ai, rj));
8:
EChain†(Ak, qk) = GetEChain(CC†(Ai, rj), qk);
9:
for each C ∈EChain†(Ak, qk) do
10:
if (C = Q) ∨(C ≺Q) then
11:
Add C to L;
12: L = Rank(L, Q);
13: return L;
Authenticated
:27 PM

344
13 Context-Based Information Reﬁnding
access contextual information, and enables the users to reﬁnd them later by the
previous access context.
(1) The Architecture
Figure 13.10 illustrates the overall architecture of CxReFinder. Its main components
include information access, context memory management, information reﬁnd, and a
database of contextually accessed ﬁle paths and URLs.
–
Information access. This component facilitates users to annotate their accessed
interesting ﬁles/web pages with the access context.
–
Context memory management. To process context-based information reﬁnding
requests, the core context memory management component needs to do a bundle
of work related to the organization, maintenance, degradation, reinforcement, and
matching (i.e., querying) of the personal context memory.
Information
Context
Decay
Lost
Ø
Reinforce
repeatedly
Local (Removable) Disk Files
Global Web Pages
Contextually accessed
File Paths    URLs
Context
User feedback
2
2
2
1
1
1
1
Contextualized
information
Contextualized
Information
Short-term context memory
Evolving long-term
context memory
Access request
Permanent long-term
context memory
Information access
Information refind
User interaction
Context annotation/
maintenance
Context memory
organization
Context memory
maintenance
Context memory management
Personal context memory
Information resources
Context memory
degradation
Context memory
reinforcement
Context memory
matching (Querying)
Context input/
maintenance
Figure 13.10: CxReFinder architecture.
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
345
–
Information reﬁnd. This component accepts users’ context-based reﬁnding re-
quests, and returns the result ﬁles/web pages.
–
Database of contextually accessed ﬁle paths and URLs. Each context instance
in the context memory links to the accessed ﬁles or web pages, whose ﬁle paths
and URLs as well as the titles are kept in the database of contextually accessed ﬁle
paths and URLs.
(2) User Interaction
Users interact with CxReFinder during their information access phase and informa-
tion reﬁnding phase.
1) Information Access Phase
When a user accesses (read, write, or modify) ﬁles from the local computer, or web
pages from the global Internet, CxReFinder system allows the user to annotate any of
the encountered interesting ﬁles or web pages with the access context. CxReFinder
designs an IE browser plug-in to acquire the titles and URLs of the currently vis-
ited web pages and provides an interface to record user-input contextual information
(Figure 13.11(a)). A desktop-based right button pop-up plug-in is also implemented
for context annotation to the selected ﬁles (Figure 13.11(b)). Alternatively, users can
double click the small translucent window at the right corner of the computer screen
to do context annotation to any opened ﬁle or viewed web page.
(a)
(b)
Figure 13.11: Two ways of context annotation: (a) for a web page; and (b) for a local ﬁle.
Authenticated
:27 PM

346
13 Context-Based Information Reﬁnding
Three contextual attributes (time, place, and activity) are considered in CxReFinder.
Time is the current date to be ﬁlled in automatically by the system. The user can either
manually enter location and concurrent activity, or select appropriate ones from a
pre-deﬁned Location hierarchy and Activity hierarchy by clicking the right-hand zoom
button. CxReFinder allows the user to maintain the hierarchies of contextual attrib-
utes by adding, deleting, or renaming attribute values. A context annotation example
for an encountered web page is as follows:
Time:2012-02-17, Place:Lab, Activity:Do project
2) Information Reﬁnding Phase
A user requests to reﬁnd previously accessed web pages and ﬁles by indicating
corresponding access context through the main interface of CxReFinder (Figure 13.12).
For example, the user can type in the following context instance:
Time:2012-02, Place:-, Activity:Do project
Here, user’s context input may not be as precise as the original context annotation
due to the natural fading of human memory as time goes by. In the above case, in-
stead of the exact date (2012-02-17), the user may only remember the year and
the month. Also, the contextual information Place is missing. It is the responsibility
of CxReFinder’s context memory manager to identify those closely matching context
units from the personal context memory, and return the linked ﬁle(s) and web page(s)
via ﬁle path(s) and url(s) stored in the personal linkage repository. CxReFinder ensures
the smallest yet correct policy for information reﬁnding, i.e., the target items that the
user demands must be in the result set, and the size of the result is the minimal one.
In the example, only one matching result is returned. The user can double click
the result and open the corresponding web page or ﬁle in the default web browser or
desktop. She/he can also choose to conﬁrm the correctness of the reﬁnding results as
system feedback.
(3) Evaluation of the CxReFinder System
Two sets of experiments were performed to examine the performance of CxReFinder
system. The ﬁrst experiment aims to study its scalability issue on large synthetic data,
Figure 13.12: Context-based information
reﬁnding interface.
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
347
and the second one aims at its applicability and acceptance issues through an eight-
week real user study.
Two performance measurements (reﬁnding response time and reﬁnding qual-
ity) are adopted throughout the experiments, where the latter is based on reﬁnding
precision, recall, and F-measure due to the degradation of context memory.
precision = num_of_true_results_being_matched
num_of_matched_results
recall = num_of_true_results_being_matched
num_of_true_results
F-measure = 2 ⋅precision ⋅recall
precision + recall
where num_of_true_results is the number of context instances that truly satisfy the
reﬁnding request, and num_of_matched_results is the result instances returned from
the system. The system in the ﬁrst experiment is implemented in Java, running on a
PC with 2.2 GHz Intel Core 2 Duo CPU, and 2 GB memory on Windows 7 OS and Sun’s
1.6.0_22 JVM.
1) Experiment on Synthetic Data
Data Generation. Synthetic data generation lies in two aspects.
Generation of Context Memory Snapshot. In an N-dimensional context (where
N
=
2, ⋅⋅⋅, 7), values of each contextual attribute form a seven-leveled hier-
archy, containing 1,000 nodes. We generate a six-month access history, containing
num_of_context_instance (100 K, ⋅⋅⋅, 600 K) context instances, under each of which
an information access event happens. Attribute values of context instances are ran-
domly picked up from the lowest levels (with levelId 1–3) of respective contextual
hierarchies. Their decay rates +s are randomly taken from [0.05, 0.08]. The time inter-
val between two consecutive access events is randomly chosen from [ 3
4 ⋅unit, 5
4 ⋅unit),
where unit = six-month period in second/num_of_context_instance.
After six months, the context instances decay based on their ages (measured
in day), attributes’ decay rates, and the initial retention strengths of their located
hierarchical levels (randomly taken from the corresponding retention intervals [0.92,
1), [0.84, 0.92), [0.68, 0.84), [0.54, 0.68), [0.32, 0.54), [0.08, 0.32), [0, 0.08), corres-
ponding to the 1st, ⋅⋅⋅, 7th hierarchical level, respectively). Contextual attributes may
also get reinforced age/2 times. Each time, 5% of chance its decay rate + is slowed
down by 10%.
Generation of Reﬁnding Requests. We formulate 100 reﬁnding requests against the
above generated context memory snapshot. We ﬁrst randomly select 100 context in-
stances from the original six-month access history. To reﬂect degradation discrepancy
Authenticated
:27 PM

348
13 Context-Based Information Reﬁnding
Queries decay 10% faster
Consistency
Queries decay 10% slower
4
3.5
3
2.5
2
1.5
1
0.5
0
100
200
Number of context instances (K)
(a)
(b)
Avg. response time (ms)
300
400
500
600
4
3.5
3
2.5
2
1.5
1
0.5
0
2
3
Number of context dimensions
Avg. response time (ms)
4
5
6
7
Queries decay 10% faster
Consistency
Queries decay 10% slower
Figure 13.13: Average reﬁnding response time on the synthetic data: (a) number of context dimen-
sions = 3; (b) number of context instances = 400 K.
between the context memory snapshot and reﬁnding requests, we adjust their ini-
tial decay rates from 10% slower to 10% faster than the original ones. The degraded
context instances after six months are used as the reﬁnding requests.
Reﬁnding Response Time. Figure 13.13 shows the average response time of the 100
reﬁnding requests. As a larger context memory snapshot contains more context in-
stances, it takes more time for the system to do reﬁnding. However, increasing the
number of context dimensions leads to the decreased response time. As the system
always starts with a speciﬁc contextual attribute for matching, more contextual attrib-
utes enable the system to ﬁnd out a better one, which can prune out more irrelevant
context instances.
Context instances in reﬁnding requests degrade at a certain speed, which may
be different from the one in the context memory snapshot. Reﬁnding queries based
on fast degraded context instances need more processing time than those based on
slowly degraded context instances, as shown in Figure 13.13. This is because the
former works on a larger search space, scattering across more hierarchical levels from
the general to the speciﬁc ones.
Reﬁnding Quality. A fast degraded reﬁnding context leads to better precision and re-
call performance than a slowly degraded reﬁnding context, as shown in Figures 13.14
and 13.15. The reason is obvious. When reﬁnding context degrades slower than
context in the context memory snapshot, the system will reﬁnd less true results,
leading to a worse query quality. On the other hand, speeding up the decay rates
of reﬁnding context gets more matched results and thus decreases the query pre-
cision to some extent. While the recall rate keeps at 100% because of the system
can reﬁnd out all the true context instances. Also, with more context instances
in the memory snapshot, more results are matched, leading to a worse F-measure
query quality. On the contrary, with more contextual dimensions, more query con-
ditions on the dimensional attribute values are imposed, leading to a better query
quality.
Authenticated
:27 PM

0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
100
200 300
400 500
600
Precision
Number of context instances (K)
100
200 300
400 500
600
Number of context instances (K)
Queries decay 10% faster
Consistency
Queries decay 10% slower
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
100
200 300
400 500
600
Recall
Number of context instances (K)
Queries decay 10% faster
Consistency
Queries decay 10% slower
F-measure
Queries decay 10% faster
Consistency
Queries decay 10% slower
Figure 13.14: Reﬁnding quality on the synthetic data, where the number of context dimensions is 3.
0
0.2
0.4
0.6
0.8
1
1.2
2
3
4
5
6
7
Precision
Number of context dimensions
Queries decay 10% faster
Consistency
Queries decay
10% slower
Queries decay 10% faster
Consistency
Queries decay
10% slower
0.4
0.6
0.8
1
1.2
2
3
4
5
6
7
Recall
Number of context dimensions
Queries decay 10% faster
Consistency
Queries decay 10% slower
0.2
0.4
0.6
0.8
1
1.2
2
3
4
5
6
7
F-measure
Number of context dimensions
Figure 13.15: Reﬁnding quality on the synthetic data, where the number of context instances is 400K.
Authenticated
:27 PM

350
13 Context-Based Information Reﬁnding
2) User Study
To examine the applicability of CxReFinder, we conduct a user study to test
if CxReFinder could support and improve users’ reﬁnding experiences, given some
contextual cues, compared to existing techniques.
Set-up. Seven participants (three female and four male, aged 21 to 34) took part in
the study by recording their information reﬁnding activities and efforts with/without
CxReFinder system during an eight-week period from 26th December 2011 through
20th February 2012.
After each reﬁnding, the participants wrote down his/her reﬁnding target (a web
page or a local ﬁle), reﬁnding mechanism (with or without CxReFinder), spent time,
and ﬁnally reﬁnding success or not.
For reﬁnding without CxReFinder, the participants speciﬁed the adopted method,
such as by browsing, desktop search, bookmarks, history list, search engine, and so
on. For reﬁnding with CxReFinder, the participants noted down or selected from the
contextual hierarchies the corresponding contextual information (e.g., time, place,
activity), based on which the reﬁnding request was processed. At the end of the
study, we administered a questionnaire to further understand users’ satisfaction and
preferences with CxReFinder system.
Results. The user study with CxReFinder returns both quantitative results, as well
as some valuable qualitative feedbacks. Among the total 104 reﬁnding activities with
CxReFinder and 103 without CxReFinder, web pages revisitation occupied 57.97%, and
local ﬁles’ reaccess 42.03%.
The participates utilized time, place, and activity as reﬁnding context cues at the
percentage of 56.73%, 44.23%, and 61.54%, respectively. On average, the participants
spent the mean time of 15.53 seconds (3 = 13.67) in reﬁnding with CxReFinder, and
84.42 seconds (3 = 85.68) with other methods. The participants could relocalize the
targets for 98.08% reﬁnding tasks by CxReFinder, and 91.26% targets could be refound
by other methods. The reason why CxReFinder failed in some tasks is due to the decay
of context memory: certain participants’ reﬁnding requests can’t ﬁnd matched context
instances in the memory snapshot any more. Failures of other reﬁnding channels (e.g.,
through desktop search or ﬁle explorer) lie in the participants’ miss-remembering
of speciﬁc ﬁle names or locations, and difﬁculty in browsing through a long list of
historically visited web pages.
The average precision, recall, and F-measure for CxReFinder are (74.38%, 98.08%,
84.60%), and (60.41%, 91.26%, 72.70%) without CxReFinder, as shown in Figure 13.16.
The calculation of precision without CxReFinder is as follows: when reﬁnding ﬁles,
the denominator (i.e., num_of_matched_results) is the number of browsed folders
(through ﬁle explorer) or the number of results (by desktop search); when reﬁnding
web pages, the denominator is the ranked order of the true result (by search engine),
or the number of checked folders (through bookmarks) or the number of browsed
records (in the historical list).
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
351
0.4
0.5
0.6
0.7
0.8
0.9
1
F-measure Precision
Recall
Without CxReFinder
With CxReFinder
Figure 13.16: Reﬁnding quality with and without
CxReFinder.
Table 13.3: Questionnaire result.
Question
Rating
I can reﬁnd things easily with CxReFinder.
4.28
I am satisﬁed with CxReFinder’s reﬁnding results.
4.24
It is easy to interact with CxReFinder.
3.72
Contextual information (time,place, and activity) constitutes useful cues for
information reﬁnding
3.61
I prefer using CxReFinder to revisit accessed web pages.
3.56
I prefer using CxReFinder to relocalize my local ﬁles.
3.44
It is easy to annotate access context for local ﬁles and viewed web pages.
3.39
The provided context hierarchical trees are helpful when I could only remember the
past access context vaguely.
3.33
With CxReFinder, I could put less efforts into maintaining my disk ﬁles.
3.28
It is easy to input reﬁnding context with the contextual hierarchical trees.
3.11
After an eight-week interaction with CxReFinder, the participants completed a ques-
tionnaire, whose result is presented in Table 13.3. These questions were answered
through the Likert-type scale approach, where 1 represents strongly disagree and 5
represents strongly agree. It is shown that CxReFinder is useful as another reﬁnding
channel for local ﬁles and global web pages.
Regarding context annotation, the participants preferred to annotate access con-
text by double clicking the translucent window on the right corner of the screen.
I like the small translucent window solution. It helps ones to quickly annotate access context when
working on whatever content in Word, PDF, Google Chrome, etc.
However, some participants also criticized the extra effort on context annotation by
the users.
It is a distraction to annotate context from the current task. To be practical, CxReFinder must
consider automatic context annotation.
Authenticated
:27 PM

352
13 Context-Based Information Reﬁnding
This suggests that future CxReFinder system must improve its context annotation
phase to show its advantage over the traditional bookmark facility. Yet overall, our
initial study shows that participants could more quickly reﬁnd web pages and local
ﬁles with better query qualities, compared with the exiting reﬁnding methods by
bookmarks, desktop search, and navigation, etc.
13.3.2.4 Discussion
CxReFinder draws on the characteristics of human brain memory in organizing epis-
odic events, where context instances in the memory snapshot are organized in a
clustered and associated way, and dynamically evolve by degradation and reinforce-
ment in life cycles. CxReFinder accepts users’ reﬁnding requests by their previous
access context. It then ﬁnds matched context instances, linking to the recalled inform-
ation, in the context memory snapshot. Underlying the reﬁnding approach are context
degradation and context annotation mechanisms.
(1) Why Degrading Context?
Astute readers may argue the necessity of context degradation in the context memory
snapshot, since keeping every detailed access contextual information can help the
system do a better context matching job. In the study, we choose to degrade con-
text for the following reasons. Compared to the enormous ﬁles and web pages one
has ever accessed or created, users’ reﬁnding targets constitute only a small por-
tion at a certain period. As time goes by, information accessed long time ago has a
small chance to be recalled by the user. Degrading very old access context shrinks
the search space so that the system can localize the most desirable candidates based
on user’s context input. There must be a way to discard useless contextual inform-
ation out of the memory for processing and also storage considerations. Context
degradation offers a natural solution in accordance with human memory’s decay
mechanism.
(2) How to Degrade Context?
Ideally, context memory and users’ memory decay at the same speed, so that context
instances in users’ reﬁnding requests can be found in the memory snapshot for match-
ing purpose. In the study, a contextual attribute value degrades along a hierarchy from
speciﬁc to general. We exploit a modiﬁed exponential-power function to control its de-
gradation process. However, in reality, user’s memorized contextual information may
not decay strictly along such a hierarchy step by step. Different contextual informa-
tion may experience different degradation processes, e.g., users may easily recall time
along the line of today, yesterday, a week ago or two months ago, etc., taking now as
reference. Thus, the decay strategies for context memory should consider the speciﬁc
characteristics of diverse contextual information. Also the degradation speeds should
be adjusted and tuned dynamically based on users’ feedbacks in the future study.
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
353
(3) How to Automatically Annotate Context?
In the current version of CxReFinder, users need to manually annotate access context
(place and activity) for their encountered interesting ﬁles and web pages. To free users
from this distraction task, it is desirable to let the system acquire and infer contextual
information automatically. In fact, automatical context annotation is not so difﬁcult,
as the access time and place can be easily obtained, and users’ concurrent activities
could be derived based upon their computers’ or cell phones’ running programs, or
users’ calendar information. The real challenge behind is to let the system identify
among the accessed contents (web pages or ﬁles) which ones will be recalled later, and
which part of a web page or ﬁle is of interest. Analysis of user’s access behavior, access
history, accessed information, and user’s activity could support decision making. This
constitutes CxReFinder’s another focal point that deserves further study.
(4) How to Unify Context and Content Cues in Reﬁnding Tasks?
In reality, humans rely on both episodic memory and semantic memory to recall in-
formation or events from the past. When a user performs a reﬁnding task, she/he
tends to utilize episodic memory, interweaved with semantic memory, to recall the
previously encountered information. Here, semantic memory accommodates content
information of previously focused pages, and episodic memory keeps these pages’
access context (e.g., time, location, concurrent activities, etc.) [61, 99]. How to ex-
ploit users’ episodic and semantic memory cues to enhance personal reﬁnding is also
interesting to explore.
The next section will address issue (3) on automatic context recognition and an-
notation and issue (4) on web revisitation by context and content keywords, aiming to
make personal reﬁnding more user-friendly and practical.
13.3.3 Web Revisitation by Context and Content Keywords
This section presents a personal web revisitation strategy, called WebPagePrev, allow-
ing users to get back to their previously focused pages through access context and
page content keywords. Underlying automatic context and content memories’ acquis-
ition, storage, and utilization for web page recall are discussed. A relevance feedback
mechanism is incorporated to tailor to individual’s memory strength and recall habits.
13.3.3.1 The Framework
A personal web revisitation framework with relevance feedback is shown in
Figure 13.17.
(1) Preparation for Web Revisitation
When a user accesses a web page, which is of potential to be revisited later by the
user (i.e., page access time is over a threshold), the context acquisition and man-
agement module captures the current access context (i.e., time, location, activities
Authenticated
:27 PM

354
13 Context-Based Information Reﬁnding
USER
Content extraction
and management
Context acquisition
and management
Result generation and
feedback adjustment
Reaccess by
context keywords
Reaccess by
content keywords
Probabilistic
context tree
Probabilistic
term list
Bounded with the accessed page URL
Web page access
Web page reaccess
Figure 13.17: The personal web revisitation framework.
inferred from the currently running computer programs) into a probabilistic con-
text tree. Meanwhile, the content extraction and management module performs the
unigram-based extraction from the displayed page segments and obtains a list of prob-
abilistic content terms. The probabilities of acquired context instances and extracted
content terms reﬂect how likely the user will refer to them as memory cues to get back
to the previously focused page.
(2) Web Revisitation
Later, when a user requests to get back to a previously focused page through context
and/or content keywords, the reaccess by context keywords module and reaccess by
content keywords module search the probabilistic context tree repository and probab-
ilistic term list repository, respectively. The result generation and feedback adjustment
module combines the two search results and returns to the user a ranked list of vis-
ited page URLs. The relevance feedback mechanism dynamically tunes inﬂuential
parameters (including memories’ decay rates, page reading time threshold, inter-
leaved window size threshold, weight vectors in computing context instances’ and
content terms’ probabilities), which are critical to the construction and management
of context and content memories for personal web revisitation.
13.3.3.2 Preparation for Web Revisitation
This section describes the acquisition and management of user’s previous access
context and content-related information to prepare for user’s web revisitation.
(1) Context Acquisition and Management Module
1) Context Acquisition
Three kinds of user’s access context, i.e., access time, access location, and
concurrent activities, are captured.
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
355
While access time is determinate, access location can be derived from the IP ad-
dress of user’s computing device. By calling the public IP localization API, we can
map the IP address (e.g., 166.111.71.XXX) to a region (e.g., Beijing, Tsinghua
University). In order to get a high-precision location, we further build an IP re-
gion geocoding database, which could translate a static IP address to a concrete place
like Lab E216. If the user’s GPS information is available, a public GPS localization
application could also help localize the user to a point of interest (POI) in the region.
A user’s concurrent activities are inferred from his/her computer programs, run-
ning before and after the page access. We continuously monitor the change of a user’s
focused program windows, which can be either a web page, a word ﬁle, or a chatting
program window, etc., during a user’s interaction with the computer. Once a user visits
a web page longer than a threshold 4c, computer programs that run interleaving with
the current web access program for over 4c time are taken as the associated computer
programs (i.e., context activities).
Let c[ts, te] = (c.title, c.dur, c.freq) denote a computer program within the time win-
dow [ts, te], where c.title is a set of words after removing stop words and nonWordNet
words from the title of the computer program, c.dur is the total running time of the
program within time window [ts, te], and c.freq is the total focus frequency within time
window [ts, te]. There are two ways to have a focus program. One is done by the user
to manually switch to the program window, and others are the automatically running
programs like audio/video players.
Deﬁnition 46. Assume a web page access program w[ws, we] = (w.title, w.dur, w. freq)
accesses a web page at time ws, leaves the page at time we, and the total visit time
of the page (i.e., the total focus duration time of program w) is longer than 4c.
Computer program c is called an associated computer program (context activ-
ity) within the w’s interleaving window [ws-B, we+B], denoted as c[ws-B, we+B] =
(c.title, c.dur, c.freq), if and only if (c.dur > 4c).
Parameters 4c and B are subject to an individual’s memorization and recall char-
acteristics, and will be dynamically tuned based on the user’s relevance feedback.
Initially, 4c = 90 seconds, B = 600 seconds.
◻
For each associated computer program (context activity) c of w, we bind an associ-
ation score cAs(w, c) to express how likely the user will use it as a memory cue to
reaccess the web page later. Intuitively, a program with a longer focus duration and
more focus frequency leaves the user a deeper impression than the one with a shorter
focus duration and less focus frequency. Similarity/contrast and temporal contiguity
also strengthen the association of the program with the web access according to the
laws of association during human memory’s recollection [85]. Hence, we compute the
association score of access context based on the following four features.
–
c’s total focus duration c.dur.
–
c’s total focus frequency c.freq.
Authenticated
:27 PM

356
13 Context-Based Information Reﬁnding
–
c’s temporal distance from web page w, D(c, w), which is deﬁned as the min-
imal distance between c’s focus time period and w’s start/end period [ws, we].
Assume a computer program c is focused within [s1, e1], ⋅⋅⋅, [sk, ek] ⊆[ws-B, we+B],
respectively. D(c, w) = arg min1≤i≤k dist([si, ei], [ws, we]), where
dist([si, ei], [ws, we]) =
{
{
{
{
{
{
{
0
if [si, ei] overlaps [ws, we]
ws – ei
if [si, ei] precedes [ws, we]
si – we
if [si, ei] succeeds [ws, we]
–
c and w’s title similarity, Sim(c, w) = |w.title∩c.title|
|w.title|
∈[0, 1], where w.title and c.title
are a set of title words after removing stop words and words not in WordNet.
Deﬁnition 47. Let C be the set of associated computer programs for the web page
access program w[ws, we] = (w.title, w.dur, w.freq). Given an associated computer pro-
gram c ∈C of w, where c[ws-B, we+B] = (c.title, c.dur, c.freq), the association score
of c with w is deﬁned as:
cAs(w, c) = !1Dur(w, c) + !2Freq(w, c) + !3(1 – Dist(w, c)) + !4Sim(w, c), where
–
Dur(w, c) =
c.dur
w.dur+2B;
–
Freq(w, c) =
c.freq
∑c∈C (c.freq);
–
Dist(w, c) = D(c,w)
B
;
–
Sim(w, c) = |w.title∩c.title|
|w.title|
;
–
∑4
i=1 !i = 1. Initially, !i (for i = 1, 2, 3, 4) is set to 1
4, and will be dynamically tuned
based on users’ relevance feedback.
◻
Example 54. Figure 13.18 illustrates three associated computer programs c1, c2, c3 of
the web page w, which are visual studio program, adobe reader program, and mu-
sic player program, respectively, where w.title = “How to: Retarget a project using
DTE,” and w.dur = 265 s. c1.title = “(visual studio) DTE Command”, c1.dur = 417 + 146
= 563 s, and c1.freq = 2. Thus, Dur(w, c1) = 563/(265 + 2 ∗600) ≈0.38, Freq(w, c1)
= 2/4 = 0.5, Dist(w, c1) = 114/600 = 0.19, Sim(w, c1) = 1/3 ≈0.33. Thus, cAs(w, c) =
(0.38 + 0.5 + (1 – 0.19) + 0.33)/4 ≈0.51.
◻
2) Construction of Probabilistic Context Trees
Access context (i.e., time, location, and concurrent computer programming activ-
ities) is organized in a probabilistic context tree to support generalized revisit
queries due to human user’s cognitive understanding and progressive decay during
learning and recalling processes [36]. Each leaf node is bounded with a score in [0, 1],
stating the likelihood that this context node is used as a contextual cue. In the activ-
ity subtree, leaf nodes’ scores are the association scores deﬁned in Deﬁnition 2. As
time and location are deterministic, leaf nodes in the time and location sub-trees
are set to 1.0. With the scores of all the independent leaf nodes available, we can
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
357
c2.dur=1032 s, c2.freq=1, D(c3, w)=0
c3.dur=251 s, c3.freq=1, D(c3, w) = 71 s
71s
114s
265s
480s
w.title=''How to: Retarget a project using DTE'', w.dur = 265 s
ws– Δ
we+ Δ
ws
we
c1.dur=(417+146) s, c1.freq=2, D(c1, w)=min(114 s, 480 s)
Figure 13.18: Three associated computer programs of the web page access program w.
compute the scores of their parent nodes through Jordan formula [34], which is deﬁned
as the union of n random events based on the inclusion–exclusion principle. Let
{child1, child2, ⋅⋅⋅, childn} be a set of child nodes of the same parent node parent, the
score after union operation is calculated by
cAs(w,parent) = (–1)2 ⋅
n
∑
i=1
cAs(w, childi)+
(–1)3 ⋅
∑
1≤i<j≤n
cAs(w, childi) ⋅cAs(w, childj)+
(–1)4 ⋅
∑
1≤i<j<k≤n
cAs(w, childi) ⋅cAs(w, childj) ⋅cAs(w, childk)
+ ⋅⋅⋅+
(–1)n+1 ⋅cAs(w, child1) ⋅cAs(w, child2) ⋅⋅⋅⋅⋅cAs(w, childn).
In this way, we can assign scores to all the nodes in the context tree. Figure 13.19 gives
a leveled probabilistic context tree example for w, whose activity leaf nodes corres-
pond to context activities c1, c3, c2 in Figure 13.18. Here, busy as a context node is a
general activity status to describe whether the associated computer programs are con-
cerning about working or learning. On the contrary, relaxed describes the status of
entertainment and leisure.
We apply the Dewey encoding scheme to probabilistic context trees based on [45,
46, 63]. Dewey code is a widely used coding scheme for tree structure, where each
node is assigned a Dewey number to represent the path from the root to the node.
Each component of the path represents the local order of an ancestor node. For ex-
ample, a tree node n encoded as n1.n2 ⋅⋅⋅nk is a descendant of tree node m encoded
as m1.m2 ⋅⋅⋅mf iff k>f and n1.n2 ⋅⋅⋅nf = m1.m2 ⋅⋅⋅mf. In our probabilistic context trees,
the Dewey number of the root is actually the tree id. For each node in a probabilistic
context tree, we build a Trie-based index according to its keywords.
Authenticated
:27 PM

358
13 Context-Based Information Reﬁnding
Access context
All
All
...
...
...
All
Busy
Relaxed
Tsinghua
University
Spring
Mid-April
Morning,
2014–4–17
Room 216
(visual studio)
DTE Command
cAs(w,c2) =0.49
cAs(w,c3) = 0.41
cAs(w,c1)=0.51
(pdf) Video retargeting: A
visual-friendly dynamic
programming approach
(kuwo) Adele,
Hometown Glory
Lab building
Programming
Reading/Writing
Listening to Music
1
Dewey
number:
1.1
1.1...1
1.1...1.1
1.1...1.1.1
Location
Time
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
0.85
0.49
Activity
Leveled
decay rate:
0.71
0.51
0.41
0.49
λ3
λ2
λ1
Figure 13.19: Probabilistic context tree of the web page access program w.
The time complexity of building context trees is O(nc ⋅h + nc ⋅h ⋅|c|), where nc is the
number of captured context instances, |c| is the average instance length, and h is the
height of context tree.
3) Decay and Reinforcement of Probabilistic Context Trees
The obtained probabilistic context trees will evolve dynamically in life cycles to
reﬂect the gradual degradation of human’s episodic memorization as well as the con-
text keywords that users will use for recall. That is, for each node in the probabilistic
context tree, its association score will progressively decay with time. Psychological
study [79] showed that the memorization status of a value v can be expressed as a
function of the exponential in the square root of elapsing time (also called age), that
is, v(t) = v(t0) ⋅e–+√t–t0, where v(t0) is the original value of v at the start time t0, v(t) is
the degraded value of v at time t ≥t0, and + is the value’s decay rate.
For different hierarchical values in the probabilistic context tree, as speciﬁc val-
ues at lower levels usually degrade faster than general ones at upper levels in human’s
memory, different decay rates +leveli (i = 1, 2, 3, ⋅⋅⋅) are assigned in line with the Ebbing-
haus Forgetting Curve [106], a graph illustrating how we forget information over time.
It was formulated in 1885 by psychologist Hermann Ebbinghaus, who conducted ex-
periments on himself to understand how long the human mind retains information
over time. Ebbinghaus took himself as a test subject to examine his own capacity
to recollect information by creating a set of 2,300 three-letter, meaningless words to
memorize. He studied multiples lists of these words and tested his recall of them at dif-
ferent time intervals over a period of one year. Ebbinghaus discovered that 58.2% was
remembered after 20 minutes, 44.2% after 1 hour, 35.8% after 8–9 hours, 33.7% after
1 day, 27.8% after two days, and 25.4% after six days. Fitting formula v(t) = v(t0)⋅e–+√t–t0
with these experimental values, we can calculate and obtain seven different decay
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
359
rates, and the average decay rate approximates to 0.05. Further similar memorization
experiments on meaningful essays and poems demonstrate similar exponential de-
cay patterns in the square root of elapsing time, whose corresponding decay rates
exhibit linear relationships with that on words, i.e., +word : +essay : +poem ≈12 : 6 : 1.
Based on these ﬁndings, we initialize the decay rates at different hierarchical levels
by +level1 = 0.05, +level2 =
1
2×1+level1, +level3 =
1
3×2+level2 =
1
12+level1. Overall, +leveli+1 =
1
(i+1)×i+leveli (i = 1, 2, 3, ⋅⋅⋅), whose values will be dynamically adjusted according to
user’s queries and feedbacks. The association score of access context value thus
degrades as cAs(w, c, t) = cAs(w, c, t0) ⋅e–+leveli√t–t0.
Apart from memory degradation, the probabilistic context tree may also experi-
ence reinforcement due to user’s recall by keyword-based query. That is, if user types
in a context value in the context tree, its possibly degraded association score is reset to
the original one, and all its ancestors’ probabilities (if degraded) are also re-computed
based on this original value. The decay starting time for its located level is meanwhile
reset to the current time.
(2) Content Extraction and Management
Apart from access context, users may also get back to the previous pages through
some content keywords. Instead of extracting content terms from the full web page, we
only consider the page segments shown on the screen. There are many term weighting
schemes in the information retrieval ﬁeld. The most generic one is to calculate term
frequency–inverse document frequency (tf-idf) [62]. For personalized web revisitation,
merely counting the occurrence of a term in the presented page segment is not enough.
Also, users’ web page browsing behaviors (e.g., visitation time length and highlight-
ing or not), as well as page’s subject headings, are counted as users’ impression and
potential interest indicators for later recall.
In a similar manner as access context, we bind an impression score to each ex-
tracted content term d, showing how likely the user will refer to it for recall based on
the four normalized features.
Deﬁnition 48. Let d be a content term extracted from the web page segment, shown
on the screen of the access program w[ws, we]. The impression score of d with w is
deﬁned as:
dIs(w, d) = "1Len(w, d) + "2Highlight(w, d) + "3Head(w, d) + "4Tﬁdf(w, d), where
–
Len(w, d) is ratio of the time length when the page segment containing d was dis-
played on the screen versus the maximal display time length of all the viewed
page segments;
–
Hightlight(w, d) = 1 if user highlights d, and 0 otherwise;
–
Head(w, d) = 1 if d occurs in the page title, and 0 otherwise;
–
Tf–idf(w, d) is the ratio of term d’s tf-idf value tf-idf(d, w) versus the maximal tf-idf
value of all the terms extracted from page w;
–
∑4
i=1 "i = 1. Initially, "i (for i = 1, 2, 3, 4) is set to 1
4, and will be dynamically tuned
based on user’s relevance feedback.
◻
Authenticated
:27 PM

360
13 Context-Based Information Reﬁnding
Figure 13.20: An accessed web page with user’s highlight.
Figure 13.21 shows a few content terms extracted from an accessed web page example
w in Figure 13.20, where extracted term d’s total focus time duration d.dur is more than
threshold 4d = 30 seconds. We organize all the extracted content terms, together with
their initial impression scores into a Trie tree based on the longest common preﬁx. For
each term at the leaf node of the Trie tree, an inverted index recording the IDs of all the
accessed web pages containing the term is built to facilitate content-based re-search.
Like probabilistic context trees in the episodic memory, terms’ impression scores
in the semantic memory will also progressively decay with time as dIs(w, d, t) =
dIs(w, d, t0) ⋅e–+󸀠√t–t0, where the terms’ decay rate +󸀠is 0.05 in this study, and get
reinforced to the original impression scores once the user utilizes them as the content
cues for web revisitation.
To gain the speed beneﬁts of indexing at retrieval time, we apply a Trie tree to
organize the extracted term lists based on longest common preﬁx. For each term at
Trie tree, inverted index is then built to store a mapping from extracted term lists in
advance as shown in Figure 13.21. Within a target page collection, we assume that each
page has a unique serial number, known as the page identiﬁer PageID. During index
construction, the input is term lists for the web pages, we insert the terms into the
Trie tree. Meanwhile instances of the same term are grouped together, and the result
is split into a dictionary and postings as shown in the right column of Figure 13.21. The
dictionary records some statistics, such as the number of web pages that contain each
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
361
For page w with PageID = 1, 150 terms are extracted, where term
"retarget" appears 6 times, and term "project" appers 11 times. Among
the totally accessed 1000 web pages, 10 pages contain "retarget", and
100 pages contain "project". Considering focus time length, d1.dur = 
160s and d2.dur = 220s. The maximal display time lenth is 500 s.
Term
d1 = “retarget”
1
Len (w, d1) = 160/500 = 0.32;
Highligh(w, d1) = 0; Head(w, d1) = 1;
Tfidf(w,d1) = 6/150*log(1000/10)0.16 = 0.5
Len (w, d2) = 220/500 = 0.44;
Highligh(w, d2) = 1; Head(w, d2) = 1;
Tfidf(w,d2) = 11/150*log(1000/100)0.16 = 0.44
max(tf-idf(d1,w), tf-idf(d2, w), ...) = 0.16
Trie with inverted list at leaf nodes
Root
P
A
A
A
A
D
A
G
T
T
E
E
E
E
C
C
T
Pagefreq: 10
PageID: 1
..
. . .
...
. . .
.
...
Postings list
Postings list
PageID: 1
Impr.Score: 0.72
Impr.Score: 0.46
Pagefreq: 50
E
S
S
S
E
E
N
L
L
L
L
E
R
R
O
J
O
L
R
R
1
d2 = “project”
Page  ID
Impression score
dIs(w, d)
0.46
0.72
Figure 13.21: Probabilistic term list extraction and management of the focused web page w.
Authenticated
:27 PM

362
13 Context-Based Information Reﬁnding
term Pagefreq, which also corresponds to the length of each postings list. And postings
list stores a list of pairs of impression score dIs(w, d, t) and PageID for a term d.
Here, the time complexity of building content term lists is O(nd ⋅|d|), where nd is
the number of extracted terms, and |d| is the average term length.
13.3.3.3 Web Revisitation by Context and Content Keywords
Now each user’s accessed web page w is bounded with a probabilistic context tree
(denoted as w#tree) and a probabilistic term list (denoted as w#list). Let W be the set
of user’s previously accessed web pages. A revisit query posted by the user at time t is
expressed as Wm = Q(W, Qc, Qd, t), where Qc is a set of context keywords, Qd is a set of
content keywords, and answer Wm is a ranked list of web pages from W, satisfying the
revisit query request. We call page w satisﬁes the query, if and only if its probabilistic
context tree w#tree contains all the context keywords in Qc (i.e., for any qc ∈Qc, there
exists a node c in w#tree such that qc ∈c.title) and its probabilistic term list w#list
contains all the content keywords in Qd (i.e., for any qd ∈Qd, there exists a term d in
w#list such that qd = d). The ranking of answer w’s query satisfaction is the product
of content ranking and context ranking. That is, Rank(w| Q, t) = dRank(w#list| Qd, t) ⋅
cRank(w#tree| Qc, t).
The computation of content ranking is straightforward, i.e.,
dRank(w#list| Qd, t) = ∏qd∈Qd dIs(w, qd, t),
which is the product of matching terms’ impression scores against content
keywords Qd.
A content ranking example with respect to Qd = {“retarget”, “project”} is illus-
trated in Figure 13.22(a).
Comparatively, as a context keyword may appear in the titles of multiple tree
nodes, the computation of context ranking cRank(w#tree| Qc, t) is a bit complex.
We ﬁrstly split a context tree into multiple satisfactory sub-trees, so that each
sub-tree contains all the search keywords once and only once. We then com-
pute the ranking of each sub-tree, and ﬁnally merge their ranking results by
cRank(w#tree| Qc, t) = ∑n
i=1 cRank(w#treesubi| Qc, t). To calculate the ranking score
of each sub-tree, we ﬁrst determine the matched node set V
= {-1, -2, ⋅⋅⋅}. For
each context node - in V, we calculate the matching probability by mPr(Qc, -, t) =
|Qc∩-.title|
|-.title|
⋅cAs(w, -, t). Considering the ancestor node -i has a matching child node
-j (-j ≺a -i), we calculate -i and -j’s matching probabilities by mPr(Qc, {-i, -j}, t) =
mPr(Qc, -i ⋂-j, t) = cAs(Qc, -j, t). The reason is that ancestor node -i can be de-
rived from -j in context tree, where there exists a dependency relationship. If user
remembers the context nodes detailed in lower level, he can directly infer the cor-
responding nodes in upper level along an upward path. Therefore, the ancestor
nodes which has matched child nodes can be ﬁrst pruned to keep the rest independ-
ent, and cRank(w#treesub| Qc, t) is the product of the remaining nodes’ probabilities.
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
363
(a) Content keywords Qd = {“retarget”, “project”} (t – t0 = 25 days)
PageID: 1
Impr. Score:    0.46
Content Ranking:
Satisfied Subtrees:
Access context
1.0 Location
Location
Activity
Activity
0.85
1.0
0.85
0.71
0.41
1.0
1.0
All
All
University
Lab building
Busy
University
Busy
All
Lab building
Read/Write
cPr(w,c  )=0.41
All
Programming
Read/Write
0.71
0.51
0.41
1.0
1.0
1.0×
×e
1
2
= 0.4412
Context ranking:
(c) Web page ranking:
cRank( w # tree          | Q   ,t ) = 0.4412 × 0.4501 × 0.3618 = 0.0923   cRank( w # tree        | Q   ,t) = 0.4412 × 0.0532 = 0.0235
cRank( w # tree | Q   ,t) = ∑     cRank( w # tree          | Q   ,t) = 0.0923 + 0.0235 = 0.1158
=0.4501
=0.3618
=0.4412
1
6
= 0.0532
0.41×
×e
(pdf) video retargeting: A
visual-friendly dynamic
programming approach
1
2
1.0×    e
0.51×e
0.41 × e
Access context
(b)  Context keywords Qc = {“busy”, “programming”, “read”, “at lab”}
PageID: 1
Impr. Score:    0.72
“retarget”
dRank( w # list | Qd,t) = 0.46 × 0.72 = 0.3312
“project”
√25
0.05
2
√25
0.05
√25
0.05
2
√25
0.05
2
√25
0.05
2
Rank(w | Q,t) = dRank(w # list | Q   ,t) cRank(w # tree| Q  ,t) = 0.3312 × 0.1158 = 0.0384
Figure 13.22: A web page ranking example for a revisit query containing Qc and Qd.
Figure 13.22(b) gives two smallest subtrees that satisfy Qc = {“busy”, “programming”,
“read”, “at lab”}, where “at” as a stop word is removed from “at lab”.
In response to a user’s web revisit request, consisting of a set of context keywords
Qc and a set of content keywords Qd, issued at time t, all the context trees and term
lists of the user’s accessed pages W will be examined, with pages that match Q being
extracted as the candidate matched page set Wc. Then the pages with higher matching
probability will be returned as query result. We call probabilistic context tree w#tree
contains Qc, if and only if for each context keyword qc ∈Qc, there exists a node c in
Authenticated
:27 PM

364
13 Context-Based Information Reﬁnding
w#tree such that qc ∈c.title, denoted as Qc ⊆c w#tree. Similarly, we call probabilistic
term list w#list contains Qd, if and only if for each content keyword qd ∈Qd, there
exists a term d in w#list such that qd = d, denoted as Qd ⊆d w#list.
Web page w satisﬁes query request Q, if and only if (Qc
⊆c
w#tree) and
(Qd ⊆d w#list). Its satisfaction rank can be computed based on the context ranking
function cRank(w#tree| Qc, t) and content ranking function dRank(w#list| Qd, t),
that is, Rank(w| Q, t) = cRank(w#tree| Qc, t) ⋅dRank(w#list| Qd, t). Figure 13.22(c)
exempliﬁes the ﬁnal ranking computation of the matched page w for keyword-based
query (Qc, Qd).
The detailed procedure is illustrated in Algorithm 4. Through scanning inverted
index, the candidate matched page set Wc can be determined based on matched con-
text trees and matched term lists against query Q (lines 2–4). To compute context
ranking, it ﬁrstly divides the matched context tree into multiple satisfactory sub-trees,
then traverses the matched nodes to merge ancestor nodes with child nodes along
the same hierarchical path. After calculating the matching probability, we can de-
termine each sub-tree’s ranking score cRank(w#treesub| Qc, t) and add them up (lines
5–15). The content ranking dRank(w#list| Qd, t) is calculated by multiplying impres-
sion score of each content keyword (line 16). The matched web pages’ ranking score
is the product of context ranking and content ranking (line 17). The matched pages
with lower ranking score are removed, where the parameter $ is assigned to 0.2 ini-
tially, and dynamically tuned based on user’s relevance feedback. Finally the quick
sorting algorithm is conducted on the rest of pages to generate a ranked list of web
pages Wm (lines 18–22). The time complexity of Algorithm 4 is O(n ⋅logn).
13.3.3.4 Relevance Feedback
Relevance feedback is an interactive approach that has been shown to work par-
ticularly well in classical information retrieval and more recently in web search
domain [81]. When a user interacts with WebPagePrev during web revisitation phase,
she/he can either manually enter some context keywords, or pick up suggested val-
ues from contextual hierarchies by clicking the left-side buttons of time, location,
and activity bars as shown in Figure 13.24. Each contextual hierarchy is dynamically
maintained by analyzing the user’s clicking behaviors and the statistical frequen-
cies of captured context instances. Frequently accessed context items are top listed
in the corresponding contextual hierarchy. User’s typos in reﬁnding requests are
automatically corrected by the system based on its indexed content and context
keywords.
Figure 13.23 shows top-4 previously visited web pages under the reﬁnding context
keywords {“busy”, “programming”, “at lab”, “in April”}, and content keywords {“re-
target”, “project”}. The user can scroll up and down with the mouse wheel to view
all the result pages. If the user double-clicks and dwells on a page by printing, down-
loading, or reading for a while, we treat the page query relevant. With this feedback
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
365
(a)
(b)
(c)
Figure 13.23: Web revisitation interface.
(a)
(b)
(c)
Figure 13.24: Suggested values for context keywords input.
Authenticated
:27 PM

366
13 Context-Based Information Reﬁnding
Algorithm 4: Web page revisitation algorithm.
Input : a revisit query Q(W, Qc, Qd, t)
Output: Wm
1 begin
2
Trees = getMatchContextTrees(W, Qc, t);
3
Lists = getMatchTermLists(W, Qd, t);
4
determine candidate matched page set Wc based on Trees and Lists;
5
foreach w ∈Wc do
6
split w#tree into n smallest subtrees w#treesubi (i = 1, ⋅⋅⋅, n);
7
for i = 1; i ≤n; i + + do
8
determine matched nodes Vsubi of w#treesubi;
9
foreach - ∈Vsubi do
10
if - has a matched child node in Vsubi then
11
delete - from Vsubi;
12
else
13
mAs(Qc, -, t) = |Qc∩-.title|
|-.title|
⋅cAs(w, -, t);
14
cRank(w#treesubi| Qc, t) = ∏-∈Vsubi mAs(Qc, -, t);
15
cRank(w#tree| Qc, t)=∑n
i=1 cRank(w#treesubi| Qc, t);
16
dRank(w#list| Qd, t) = ∏qd∈Qd dIs(w, qd, t);
17
Rank(w| Q, t) = cRank(w#tree| Qc, t)⋅
18
dRank(w#list| Qd, t);
19
determine the matched page w4 with highest ranking score;
20
foreach w ∈Wc do
21
if Rank(w| Q, t) < $ × Rank(w4| Q, t) then
22
determine W󸀠
c by deleting w from Wc;
23
Wm = Quicksort(W󸀠
c, Rank(W󸀠
c| Q, t));
information, the web revisitation engine gets to know the system performance, and
tune-related inﬂuential parameters to improve it gradually. Meanwhile, to keep pace
with the user’s context memorization strength, the engine tunes the leveled-decay
rates for probabilistic context memory according to the located levels of typed context
keywords.
With this feedback information, the web revisitation engine gets to know its
performance, and tune-related inﬂuential parameters to improve the performance.
Meanwhile, to keep pace with the user’s context memorization strength, the en-
gine tunes the leveled-decay rates for probabilistic context memory according to the
located levels of context keywords used by the user.
(1) Performance Metrics
The web revisitation performance metrics include pages’ ﬁnding rate, average preci-
sion, average recall and average rank error for a set of reﬁnding requests.
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
367
Deﬁnition 49. Assume a user’s web revisitation request Q returns a ranked list of n res-
ult pages, from which the user aims to reﬁnd 8 target pages, and conﬁrms m relevant
result pages {w1, ⋅⋅⋅, wm}.
–
The ﬁnding of revisitation Q: Find(Q) = 1 if the user conﬁrms one or more
relevant result pages (i.e., m > 0), and 0 otherwise.
–
The precision of revisitation Q: Precision(Q) = m
n .
–
The recall of revisitation Q: Recall(Q) = m
8 .
–
The rank error of revisitation Q: RankError(Q) = ∑m
j=1
Pos(Q,wi)–j
Pos(Q,wi) /m, where func-
tion Pos(Q, wi) returns the position of the i-th conﬁrmed page wi in the result page
list.
◻
Deﬁnition 50. Let Q be a set of user’s web revisitation requests. The ﬁnding rate, av-
erage precision, average recall and average rank error of Q are thus deﬁned as
follows.
–
FindRate(Q) =
∑Q∈Q Find(Q)
|Q|
;
–
AvgPrecision(Q) =
∑Q∈Q Precision(Q)
|Q|
;
–
AvgRecall(Q) =
∑Q∈Q Recall(Q)
|Q|
;
–
AvgRankError(Q) =
∑Q∈Q RankError(Q)
|Q|
.
◻
(2) Inﬂuential Parameters to be Tuned
1) Parameters used in constructing and managing probabilistic context trees
–
A weight vector (!1, !2, !3, !4) with ∑4
i=1 !i = 1, used in computing the association
score of a context activity c with a web page access w in Deﬁnition 47. It quanti-
ﬁes the importance of c’s focus duration, focus frequency, temporal distance from
the page access, and title similarity between c and w, respectively. It impacts the
ranking position of result pages.
–
Context focus duration threshold 4c and interleaving window size threshold B
between c and w in context acquisition. Reducing 4c and enlarging B can capture
more associated context activities into the probabilistic context tree, and deliver
more result pages.
–
Leveled decay rates in probabilistic context trees +leveli (where i = 1, 2, 3, 4 in this
study), with which the association score of a context activity c with a web page
access w is dynamically computed to match a user’s memorization strength on
context keywords in recall.
2) Parameters used in constructing probabilistic content term lists
–
A weight vector ("1, "2, "3, "4) with ∑4
i=1 "i = 1, used in computing the impression
score of a content term d with a web page access w in Deﬁnition 48. It quantiﬁes
Authenticated
:27 PM

368
13 Context-Based Information Reﬁnding
the importance of d’s focus duration, highlighted or not, heading words or not,
and tf-idf value, respectively. Similar to (!1, !2, !3, !4), it impacts the ranking of
result pages.
–
Content focus duration threshold 4d for content extraction. A lower setting of 4d
enables more content terms to be extracted from the browsed web page and add
them to the probabilistic term list.
3) Parameter used to adjust the length of result list
–
The parameter $ is used to remove the pages with lower ranking score from result
list.
(3) Tuning Strategies
The parameters tuning is carried out when any of the following three conditions holds.
1) Periodically (e.g., once every two weeks since last tuning).
2) When one of the performance metrics drops below a threshold (e.g., 4FindRate = 0.8,
4Precision = 0.2, 4RankError = 0.4) since last tuning.
3) When the user presses “≫” button at the right-bottom of the screen if she/he is not
satisﬁed with the result and wants more pages.
The tuning consists of three steps.
Step 1: Optimize the settings of weight vector (!1, !2, !3, !4) and ("1, "2, "3, "4) to
ensure the user’s conﬁrmed relevant result pages are ranked higher than unconﬁrmed
ones, thus decreasing the average revisitation rank error. Because the computation
of context ranking is independent from that of content ranking. The weight vector !i
and "i (for i = 1, 2, 3, 4) can be, respectively, optimized in a four-dimensional weight
space to improve context ranking and content ranking of the conﬁrmed relevant result
pages. In order to precisely specify the weights in our linear aggregation function, we
propose a weak partial ordering graph (WPOG) model to generate a user’s preferred
ranked list as shown in Figure 13.25.
Deﬁnition 51. Given a set of web pages W
= {w1, w2, ⋅⋅⋅, wn} and corresponding
normalized feature values S = {s1, s2, ⋅⋅⋅, sm}, a WPOG is a directly graph G(V, E),
where E is a set of edges deﬁning the relative ordering between pages, and V is a set
of vertices describing the PageID. There exists a edge from wi to wj when one of the
following two conditions holds:
–
∀s ∈S, wi.s ≥wj.s;
–
∀s ∈S – {sk}, wi.s ≥wj.s, where sk ∈S and wi.sk < wj.sk.
◻
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
369
0.1
0.1
0.3
0.5
0.4
0.8
0.3
0.2
0.3
0.7
0.4
0.5
0.7
0.2
0.6
0.4
0.2
0.5
0.7
0.8
s1
w1
w1
w2
w3
w4
w5
w2
w3
w4
w5
s2
s3
s4
s3
s3
s3
s1
s4
Weak partial ordering graph:
N
N
N
Figure 13.25: A weak partial ordering graph example.
Example 55. Figure 13.25 illustrates ﬁve web pages with four normalized feature val-
ues. For pages w1 and w3, the edge e13 satisfying condition 2 is labeled by s3,
where w1.s3 < w3.s3. For pages w2 and w5, the edge e25 satisfying condition 1 is
labeled by N.
◻
Taking context ranking as an example illustrated in Figure 13.25, we detail the ad-
justment of weight vector (!1, !2, !3, !4) into two cases: (1) When a user conﬁrms one
result page as relevant, the adjustment is based on the label of edges incident to the
conﬁrmed page’s node. For example, when the user conﬁrms w5 as a relevant page, we
can clearly determine enlarging !1 can change the relative order between w4 and w5 by
the label s1 of e45; (2) When the user conﬁrms two or more pages as relevant, the con-
ﬁrmed pages’ nodes are ﬁrst clustered as a set. By neglecting the inner edges between
graph nodes of conﬁrmed pages, the adjustment is based on the label of edges in-
cident to the cluster set. For example, when the user conﬁrms w3 and w4 as relevant
pages, the inner edge e34 is neglected for the clustered pages {w3, w4}. And the labels
of incident edges are ranked based on their frequencies. Through enlarging s3 and s4,
the RankError(Q) can be reduced.
During the adjustment of weight vector, the linear constraint ∑4
i=1 !i = 1 should
be satisﬁed. The objective function of weight adjustment is to minimize the value
of RankError(Q). For case 1, after increasing !1 by 󳵻!1, other weights should be de-
creased to satisfy the constraint ∑4
i=1 !i = 1. We adopt B!k =
w4.sk–w5.sk
∑4
i=2 w4.si–w5.si ⋅B!1 (for
k = 2, 3, 4) to allocate 󳵻!1 to (!1, !2, !3). The larger w4.sk – w5.sk is, the more corres-
ponding weight decreases. Through traversing the label of other edges {e15, e35, e25}
concerning w5, the upper bound of w5’s ranking position is 4. For case 2, we ﬁrst
enlarge !3 and !4 sequentially to reduce RankError(Q). When increasing !3 by 󳵻!3,
(!1, !2) should be decreased and !4 remains the same. The decreased amount for
(!1, !2) is calculated by B!k =
(w1.sk–w3.sk)+(w2.sk–w4.sk)
∑2
i=1 [(w1.si–w3.si)+(w2.si–w4.si)] ⋅B!3 (for k = 1, 2). When
context ranking satisﬁes condition cRank(w4#tree| Qc, t) > cRank(w2#tree| Qc, t), we
begin to increase !4. For the adjusted weight values that minimize RankError(Q), we
Authenticated
:27 PM

370
13 Context-Based Information Reﬁnding
add them into the candidate set. Based on the following user-typed queries and con-
ﬁrmed operations, a suitable weight vector value can be learned from 9!. Considering
a set of user queries Q, the adjustment of weight vector is conducted on each graph to
minimize AvgRankError(Q).
The weight adjustment for ("1, "2, "3, "4) can be conducted in a similar manner.
Step 2: Decrease the context and content focus duration thresholds 4c and 4d by
half and doubly increasing the interleaving window size threshold B in context acquis-
ition, if the revisitation ﬁnding rate is low and less than 4FindRate. In this way, more
pages could be returned in the result list.
The other way around, to increase the average revisitation precision, context and
content focus duration threshold 4c and 4d are to be increased, and the context win-
dow size B is to be shortened, aiming to drop out more irrelevant web pages from the
results. To this end, we examine all user’s conﬁrmed relevant pages since last tuning,
and take the smallest 4c and 4d, as well as the largest B, of their associated context
activities and content terms as the new values of 4c, 4d, and B.
Here, the tuning of 4c, 4d, and B for the improvement of FindRate(Q) and
AvgPrecision(Q) is in two different directions. If the former is less than the later, our
tuning is FindRate-oriented, and otherwise AvgPrecision-oriented.
Step 3: Adjust leveled decay rates of context trees +leveli (for i = 1, 2, 3, 4) according
to the hierarchical levels of context keywords that the user exploited in his/her revisit
queries, since the later reﬂects user’s context memorization status. To match the user’s
context memory degradation, we count the number of typed context keywords at each
level as counti, and approximate the user’s context memorization amount at leveli by
mem_amounti = counti+counti–1+⋅⋅⋅+count1
∑4
j=1 countj
. It is assumed that if a user remembers a con-
text value at a lower level (e.g., “Lab”), there is no doubt that she/he also remembers
its upper-leveled value (e.g., “University”).
Let mem_amounti = e+leveli⋅√t0–t. We can adjust the context memory’s decay rate
at leveli to +leveli = log (mem_amounti)
√t–t0
, where t0 is the initial time that the context tree is
constructed, and t is the current tuning time.
Step 4: Decrease $ when conﬁrmed pages with lower ranking score are still re-
moved from result list after Step 1. Meanwhile, increase $ step by step to improve
system’s AvgPrecision(Q) when metric FindRate(Q) keeps stable.
Each time when the tuning strategy is conducted, the inﬂuential parameters are
adjusted to gradually approximate the inherent browsing behaviors and recall habits.
Repeating such parameter adjustment, the strategy can guarantee that there is a better
solution to improve the average reﬁnding performance after a period of time.
The time complexity of relevance feedback mechanism is O(Nq ⋅–+ ⋅log n + Nq ⋅
∑L
i=1 |Si|), where Nq is the number of revisit queries, –+ is the number of candidate
paths, n is the number of graph nodes for G(V, E), L is the number of hierarchical
levels, and Si is the context node set at the i-th level.
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
371
13.3.3.5 Evaluation
We examine the applicability of the proposed web revisitation technique through a
user study, where the performance metrics used include FindRate, Precision, Recall,
F1-measure and RankError. We also study the scalability of the approach on a large
synthetic data set based on the response time metric. The experiment on synthetic
data is running on a PC with 3.10 GHz Intel i5-3450 CPU, and 10 GB memory on
Windows 7 OS.
(1) User Study
1) Experimental Set-up
A six-month user study was conducted from 16 February 2014 to 16 August 2014
to investigate the reﬁnding performance of four different methods, i.e., WebPagePrev,
Memento, History List Searching, and Search Engine. Fourteen students, two teachers,
three ofﬁce staffs, and two engineers (totally 21 users, 13 males and 8 females, aged
between 18 and 35) from Tsinghua University, Beijing were invited to participate in
the user study. We ﬁrst installed WebPagePrev and Memento on each user’s laptop.
A javascript-based cross-platform plug-in was deployed on each user’s Chrome web
browser to obtain his/her page visit behavior, and a context monitor developed by C++
was deployed in the background to capture running computer programs by calling the
windows API functions.
We then provided the following instructions to the users before starting the
experiment:
a) Each user is suggested to execute at least one reﬁnding task per day.
b) For each reﬁnding task, four different methods are to be invoked. The execution
sequence is random.
c) When using WebPagePrev to do reﬁnding, context keywords (from time, loc-
ation, and activity context hierarchies) and content keywords (from page’s title and
focused body text) could be input. With Memento, content keywords (i.e., topic-
phrases extracted from the page) and context keywords (i.e., topic-phrases extracted
from the preceding and following pages) could be input. The users could leave either
context or content keywords empty if they do not want to. With History List Search-
ing, content keywords from page’s title could be input. With Search Engine, content
keywords from page’s title, body text, and other descriptive information could be
input.
d) The users should connect the external GPS module with the laptops when they
are outside the campus.
e) At the end of a reﬁnding task, the users should to record their search keywords,
the number of target pages they looked for, the number of returned target pages, and
the ranking positions of target pages in the result list.
During our six-month user study, the total number of focused pages is 111,042,
and about 5,288 per user. Each user raises about 618 revisit queries. On average, with
Authenticated
:27 PM

372
13 Context-Based Information Reﬁnding
(0–10]
(10–20]
(20–30]
(30–40]
Refinding days
(a)
(b)
(40–50]
(50–60]
>60
0
0.0
0.2
0.4
Ratio
0.6
0.8
1.0
Nonheading
Title
Level
Level
Level
500
# of revisit queries
1000
1500
2000
2500
3000
(0–10]
(10–20]
(20–30]
(30–40]
Refinding days
(50–60]
>60
(40–50]
Figure 13.26: Statistics of revisit queries in different reﬁnding time intervals for WebPagePrev:
(a) number of typed queries; (b) ratio of query keywords.
WebPagePrev, the users use 4.37 context keywords and 2.26 content keywords to ex-
ecute a revisit query. With Memento, 1.82 context keywords and 2.07 content keywords
are used. Besides, 1.41 content keywords and 3.84 content keywords are used with
History List Searching and Search Engine, respectively.
From Figure 13.26(a), we can see the number of revisit queries in different reﬁnd-
ing time intervals for WebPagePrev, where reﬁnding days dayR is the elapsed days
since the page was accessed. The results show that participants are more likely to
input revisit queries by WebPagePrev when dayR∈(10, 40], accounting for 58.72%.
Figure 13.26(b) shows the ratio of query keywords belonging to different context hier-
archical levels and page segments with varying reﬁnding days. Owning to the decay
of episodic memory, as time elapses, more general context keywords from level 2 and
3 are typed by the participants, and the ratio is over 52.74% when dayR>40. Besides
using title terms, they also tend to recall more content keywords from the nonheading
page segments, over 46.22% when dayR>40. Note that, if the typed content keyword
meanwhile belongs to title and the no-heading page segments, the count of title will
add 1.
2) Experimental Results
Performance Comparison with Existing Approaches. We compare the per-
formance of our personal web revisitation approach with three conventional methods.
From Figure 13.29, WebPagePrev delivers the best average F1-measure, about 2.15
times, 1.51 time and 1.29 time than that of Search Engine method, History List Searching
method, and Memento. For the precision metric of Search Engine method, parameter
n corresponds to the number of browsed pages in user’s visual ﬁeld before getting the
desired targets. The ﬁnding rate of WebPagePrev is 92.10% compared to Search Engine
method 81.11%, History List Searching method 84.40%, and Memento 89.31%. Further,
the average rank error of WebPagePrev is 0.3145, compared to Search engine method
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
373
0.6105, History List Searching method 0.4717, and Memento 0.4322. The reason includes
several aspects. History List Searching method needs exact matching of page title, and
participants sometimes could not remember the title terms very well, especially for
the previously accessed pages with longer reﬁnding days. While the query results and
their rankings are frequently updated within the search engine, participants some-
times felt difﬁcult to relocate the target pages. Depending on contextual cues, the
revisitation performance of Memento has improved a lot. However, Memento does not
make full use of activity context, which only considers browsed pages running before
or after the focused page. Leaving out user’s browsing behaviors, Memento collects
more redundant content terms and causes irrelevant query results returned.
We further perform t-test and all the p-values are < 0.01, which indicate that the
improvement of WebPagePrev over the comparison methods is statistically signiﬁcant.
Effectiveness of Memories Decay and Relevance Feedback. Through removing
decay and relevance feedback mechanism from WebPagePrev, we evaluate the ef-
fectiveness by comparing four different cases: (1) without decay (WD); (2) without
relevance feedback (WF); (3) without decay and relevance feedback (WDF); (4) with
decay and relevance feedback (DF). From Figure 13.27, DF’s ﬁnding rate increases by
0.88%, average F1-measure increases by 15.27%, and average rank error decreases by
4.71% than WD. In comparison with stable memory management strategy, DF’s ﬁnding
rate increases by 9.82%, average F1-measure increases by 47.09%, and average rank er-
ror decreases by 19.44% than WDF. Considering relevance feedback, DF’s ﬁnding rate
increases by 7.16%, average F1-measure increases by 39.22%, and average rank error
decreases by 16.14% than WF.
1
1
0
0.1
0.2
0.3
0.4
0.5
0.5
0.6
0.7
0.8
0.9
1
2
2
3
3
4
4
5
5
6
6
6
WD
WF
WDF
DF
WD
WF
WDF
DF
Avg. Recall
Avg. F1-measure
Time (Month)
Time (Month)
(a)
(b)
(c)
(d)
(e)
0.5
0.6
1
1
0
0.1
0.2
0.3
0.4
0.5
2
2
3
3
4
4
5
5
6
0.7
Finding Rate
Avg. Rank Error
0.8
WD
WF
WDF
DF
Time (Month)
Time (Month)
WD
WF
WDF
DF
0.9
1
0
1
2
3
4
5
6
0.1
0.2
0.3
0.4
0.5
WD
WF
WDF
DF
Avg. Precision
Time (Month)
Figure 13.27: Evaluating the effectiveness of memories decay and relevance feedback: (a) Finding
rate; (b) average rank error; (c) average precision; (d) average recall; and (e) average F1-measure.
Authenticated
:27 PM

374
13 Context-Based Information Reﬁnding
(a)
1.0
0.5
(b)
0.9
1
0.8
0.7
0.6
0.5
Avg. Fingding rate
Avg. Rank Error
2
3
4
Time (Month)
5
6
1
2
3
4
Time (Month)
5
6
0.4
0.3
0.2
0.1
0
WPOG
POT
WPOG
POT
(d)
(e)
Avg. Recall
Avg. F1-Measure
1.0
0.9
0.8
0.7
0.6
0.5
WPOG
POT
WPOG
POT
1
2
3
4
Time (Month)
5
6
1
2
3
4
Time (Month)
5
6
0.5
0.4
0.3
0.2
0.1
0
(c)
Avg. Precision
WPOG
POT
1
2
3
4
Time (Month)
5
6
0.5
0.4
0.3
0.2
0.1
0
Figure 13.28: Evaluating the effectiveness of weight adjustment method in relevance feedback:
(a) Finding rate; (b) average rank error; (c) average precision; (d) average recall; and (e) average
F1-measure.
(a)
1.0
Finding rate
Average rank error
Average precision
Average recall
Average F1-measure
(b)
0.8
0.6
Re-findling Performance
0.4
0.2
WebPagePrev Search
Engine
History List
Searching
Memento
WebPagePrev Search
Engine
History List
Searching
Memento
0
1.0
0.8
0.6
Re-findling Performance
0.4
0.2
0
Figure 13.29: Performance comparison with the existing approaches: (a) ﬁnding rate and average rank
error; (b) average precision, recall, and F1-measure.
Here, AvgRecall(Q) and FindRate(Q) are quite close. This is because when there
is only one wanted target, AvgRecall(Q) = FindRate(Q). Around 83.37% of reﬁnding
queries look for one target page.
Effectiveness of Weight Adjustment in Relevance Feedback. We compare WPOG
with possible orderings tree (POT) as baseline algorithm [84] to evaluate the effect-
iveness on weight adjustment as shown in Figure 13.28. For performance metrics, our
method’s FindRate increases by 3.2%, F1-measure increases by 11.4%, and RankError
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
375
Table 13.4: Performance comparison between context and content factors in WebPagePrev.
Query keywords
Finding rate
Average rank error
Average
precision
Average recall
Average
F1-measure
Content
0.8745
0.4033
0.1903
0.8577
0.3115
Context
0.9011
0.3256
0.2619
0.8830
0.4036
Context+Content
0.9388
0.2849
0.3185
0.9212
0.4733
decreases by 8.2% than baseline. This is because POT makes an assumption that
the weights satisfy a uniform distribution, which does not always hold for different
users. During the adjustment of weight vector, there are a set of candidate solutions
to minimize RankError. Our method can determine a suitable weight vector value
considering users’ preference instead of the mean value.
Contribution Analysis of Context and Content Factors. To examine the import-
ance of different factors (i.e., time context, location context, activity context, and
keyword content) in WebPagePrev, we divide reﬁnding queries into three types, i.e.,
(1) querying based on content keywords only, (2) context keywords only, and (3) con-
text plus content keywords. In the study, around 14.52% of queries belong to query
type 1, 18.89% belong to query type 2, and 66.59% of queries belong to query type
3. For three query types, we randomly select the same amount of queries in different
time intervals, and obtain 1855 queries of each type. Table 13.4 shows that queries us-
ing context and content keywords perform the best in ﬁnding rate (93.88%), average
rank error (0.2849), and average F1-measure (0.4733). Queries using context keywords
perform better than queries using content keywords by increasing 3.04% in ﬁnding
rate, decreasing 19.27% in average rank error, and increasing 29.57% in average F1-
measure. The main reason for these performance differences could lie in the number
of query keywords used. Content-based queries use around 2.48 keywords on average,
context-based queries use around 3.61 keywords, and context+content-based queries
use around 5.05 keywords (3.12 context keywords and 1.93 content keywords). We ob-
serve that the users tend to enter more than one activity element in the hierarchy like
“busy” followed by “programming”.
Furthermore, we investigate the behaviors of time, location, and activity contex-
tual elements in WebPagePrev. In the study, about 7.89% of queries use time, 5.24%
use location, 15.19% use activity keyword(s), 8.51% use time plus location, 18.10% use
time plus activity, 13.17% use location plus activity, and 22.90% use all the three. Sim-
ilarly, we randomly select the same amount of queries in different time intervals, and
obtain 481 queries per group. From Table 13.5, we ﬁnd that activity is the best recall
cue, followed by time and location. This also holds for activity+time. The reason is due
to the smallest search space of activity context compared to that of time and location
context. More candidate accessed pages are associated with a time or location cue. As
Authenticated
:27 PM

376
13 Context-Based Information Reﬁnding
Table 13.5: Performance comparison of different context factors in WebPagePrev.
Contextual factor
Finding rate
Average rank error
Average
precision
Average recall
Average
F1-measure
Time
0.8873
0.3474
0.2574
0.8681
0.3971
Loc.
0.8716
0.3907
0.2433
0.8533
0.3786
Act.
0.9192
0.3221
0.2950
0.8922
0.4434
Time+Loc.
0.9066
0.3423
0.2805
0.8784
0.4252
Time+Act.
0.9491
0.2630
0.3149
0.9344
0.4711
Loc.+Act.
0.9379
0.2857
0.3013
0.9207
0.4540
Time+Loc.+Act.
0.9443
0.2489
0.3414
0.9309
0.4996
context_node_num
query_keyword_num
context_term_num
Total_page_num = 100 K
Total_page_num = 500 K
Total_page_num = 1 M
Total_page_num = 100 K
Total_page_num = 500 K
Total_page_num = 1 M
Total_page_num = 100 K
Total_page_num = 500 K
Total_page_num = 1 M
(a)
600
(b)
(c)
500
400
300
Avg. response time (ms)
200
100
20
0
700
600
500
400
300
Avg. response time (ms)
200
100
0
500
400
300
Avg. response time (ms)
200
100
0
25
30
35
40
45
50
100
6
150
200
250
300
350
400
7
8
9
10
11
12
Figure 13.30: Average revisitation response time on synthetic data (a) content_term_num=300,
query_keyword_num=10;
(b)
context_node_num=40,
query_keyword_num=10;
(c)
content_
term_num=300, context_node_num=40.
the activity context is inferred from the user’s computer programs, it binds with page
access more closely, leading to the best performance.
(2) Experiment on Synthetic Data
Assume a user has accessed and focused total_page_num (100K, 200K, ⋅⋅⋅, 1M) web
pages. Associated with each of the web page, there is a probabilistic context tree
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
377
and a probabilistic content term list, over which a user’s page re-visitation requests
are executed. Each context tree has four hierarchical levels with context_node_num
(20, 25, ⋅⋅⋅, 50) tree nodes in total. The node number at level 1, 2, and 3 occupies 70%,
20%, and 10% of the total node number. Except for the highest level 4, which has
only one node, representing context ALL, nodes at the same level have the same fan-
out value. For time context, we generate a set of time stamps during 12 months. For
location context, we select a set of regions and extract a set of POIs from a city map
(Beijing). The POIs are randomly assigned to each context tree, where the leaf nodes’s
repetition for location context is 35%. For activity context, we create a data set by cap-
turing computer programs from working environment of seven users, where the data
set contains 30,115 computer programs. The association score of each child node at
the lowest level is a random value from 0 to 1, and the association scores of the parent
nodes at upper levels are merged through the Jordan formula [34]. Each probabil-
istic content term list accommodates maximally content_term_num (100, 150, ⋅⋅⋅, 400)
unique stemmed terms extracted from page crawled from common Internet websites
(e.g., shopping site, blog site, Quora, etc.). The impression score of each term is
randomly determined in the range of (0, 1).
Assume a user raises
1
100 ⋅total_page_num revisit queries. Each query con-
tains query_keyword_num (6, 7, ⋅⋅⋅, 12) keywords, and about half of them are context
keywords, and the other half are content keywords. Both are randomly taken from the
context tree and content term list bounded with a certain web page.
From the result presented in Figure 13.30(a),(b), we can ﬁnd that the average re-
visitation response time increases accordingly with the increase of context_node_num
and content_term_num. The reason is obvious, as more context nodes and content
terms linked with every page access need to be examined and matched with the user’s
query keywords. More pages being accessed, more context trees and term lists need to
be checked, thus more time to process the page revisit queries. On the other hand,
the response time does not increase sharply with the number of query keywords,
as illustrated in Figure 13.30(c). For example, when the user’s query contains six
keywords, the response time is 28 ms (when total_page_num = 100K), 102 ms (when
total_page_num = 500K), and 198 ms (when total_page_num = 1M). It increases to
45 ms, 173 ms, and 307 ms, respectively, when query_keyword_num = 9. Then the re-
sponse time cost decreases a little when query_keyword_num continues goes up. This
might be because more query keywords may also quickly ﬁlter mismatched context
trees, thus reducing the search space of candidate term lists.
13.3.3.6 Discussion
(1) Summarization
Leveraging human’s natural recall process of using episodic and semantic memory
cues to facilitate recall, this section presented a personal web revisitation tech-
nique called WebPagePrev through context and content keywords. The underlying
Authenticated
:27 PM

378
13 Context-Based Information Reﬁnding
techniques for context and content memories’ acquisition, storage, and utilization
for web page recall are discussed. Dynamic tuning strategies to tailor to individual’s
memorization strength and recall habits based on relevance feedback (e.g., weight
preference calculation, decay rate adjustment, etc.) are developed for performance
improvement.
The effectiveness of WebPagePrev is evaluated through a six-month user study
with 21 participants. Compared with the existing web revisitation tool Memento,
History List Searching method, and Search Engine method, the proposed WebPage-
Prev delivers the best reﬁnding quality in ﬁnding rate (92.10%), average F1-measure
(0.4318), and average rank error (0.3145). With relevance feedback, the ﬁnding rate of
WebPagePrev increases by 9.82%, average F1-measure increases by 47.09%, and aver-
age rank error decreases by 19.44% compared to stable memory management strategy.
Among time, location, and activity context factors in WebPagePrev, activity is the best
recall cue, and context+content-based reﬁnding performs better than context-based
reﬁnding and content based reﬁnding.
(2) Ambiguity Issue in Reﬁnding Requests
So far, answering reﬁnding queries targets at precise answers, which are only con-
cerned with the data matching context and content keywords exactly. The ambiguity
of human memory about context as reﬁnding cues, however, is not considered. In
fact, human memory exhibits life-cycle degradation nature, and human’s recall pro-
cess may also easily be inﬂuenced by various interferences [97]. For instance, newly
accessed events may better be recalled than old ones. The ability to recall a certain
piece of information from memory could be impaired by either the newly learned
or previously learned information [52, 107]. Some nontarget items that are similar to
the target ones may compete with the target ones as potential responses in memory
recall [65, 78]. This may lead to some miss-memorized and ambiguous context and
content keywords that are used as reﬁnding requests by users. Taking context for
example, let’s see a real case.
Lily wants to buy a sofa. After comparing many similar styles of sofas for a while,
she ﬁnally selects the one which she saw at 2013/4/17 when listening to Adele’s songs
on the Internet, for that one had a good discount. Unfortunately, she cannot relocate it
any more on the web by simple “sofa” keyword, since she forgets any useful information
about it, such as detailed title, name of the online shop, etc. What Lily can remember
is that she was listening to some music someday in April when coming across that sofa.
Finally Lily has to recall that sofa web page through ambiguous contextual keywords
(“in April” and “listen to music”) rather than the exact ones (“2013/4/17” and “listen to
Adele music”).
Due to human memory decay and interference nature, ambiguous contextual
keywords may be used in information recall requests. Such context ambiguity could
be categorized into three kinds.
Authenticated
:27 PM

13.3 Nature-Inspired Context-Based Reﬁnding
379
–
Context Degradation. Owing to human memory decay, contextual keywords for
searching degrade to a more general level, e.g., instead of giving the exact date
“2013/4/17”, the user may only recall via an approximate date “in April”.
–
Context Confusion. Due to memory interference and lots of information access
events, access context mixes up, e.g., “in April” is mixed with “in March.” This
happens frequently when the events occur temporally close by. Also, access con-
text may be interfered with each other due to context similarity, e.g., the user
may input “listen to Ying music” as the concurrent activity rather than the cor-
rect one “listen to Adele music,” since both Ying’s and Adele’s songs are listened
frequently by the user. This may also happen due to the similarity in the accessed
information contents themselves.
–
Context Error. Unlike context confusion, the user may take some non-access-
context wrongly as an access context for information reﬁnding. For example, the
use may input “in April” to query the information which she accessed in March,
while no information access happened in April at all.
From the reﬁnding system perspective, it is not easy to get users’ desired results, given
such ambiguous reﬁnding requests, which would often result in plenty of irrelevant
results or null results. It would be quite frustrating for users, as unlike information
ﬁnding, information reﬁnding is a more directed process, where users have already
seen the information before, and can recognize the popped-up target [13]. For the ex-
isting context- and content-based reﬁnding systems, this usually means that the user
is forced to try repeatedly with alternative keyword values until it ﬁnally matches the
desired data. If the user fails in recalling the access context and content, then even this
solution is infeasible. Therefore, an approximate search solution to deal with users’
reﬁnding requests with ambiguous context and content retrieval cues is desirable.
(3) Involving More User-Centric Context Factors for Reﬁnding
When a user does reﬁnding, she/he usually has certain purposes in mind, like pre-
paring a project proposal, writing codes, etc. WebPagePrev strives to support users to
reﬁnd what they accessed through previous access time, location, concurrent activ-
ities, and content keywords. Beyond that, more user-centric context factors (e.g.,
access purpose, expertise, background, interest, etc.), as well as social context factors
(e.g., external events, surrounding people, etc.), could be inferred from user’s proﬁle,
agenda, and external service providers, and bounded with the accessed pages. In this
way, not only the user him/herself could beneﬁt from such rich contextual cues during
reﬁnding process, but also other users with similar access purpose and background
could share the more directed page access. This is in line with the spirit of social
search [21, 48, 102], which advocates two paradigms (namely, library paradigm and
village paradigm) in information retrieval. According to Ref. [48], in a library, people
Authenticated
:27 PM

380
13 Context-Based Information Reﬁnding
use keywords to search documents, and the trust is based on authority, while in a vil-
lage, people use natural language to ask questions, answers are generated in real-time
by anyone with the expertise in the community, and trust is based on intimacy.
In social search, a lot of data about the people is used, bringing in privacy protec-
tion issues. Life-cycle management of people’s information with degradation policies
from high to low precision, as done with the context memory mechanism could be
exploited.
13.4 Recapitulation
Getting back to previously accessed information is a common yet uneasy task for
users. This chapter reviewed existing information reﬁnding techniques developed in
the web and PIM ﬁelds. Drawing on the characteristics of human brains in organizing
and exploiting episodic events and semantic words in memory recall, techniques on
information reﬁnding via structured context information (time, place, and activity),
as well as context and content keywords, were particularly discussed. Other issues,
such as (1) prediction of users’ revisitation, (2) extending the technique to support
users’ ambiguous reﬁnding requests, and (3) incorporating social context factors in
information reﬁnding, deserve further investigation.
Literature
[1]
D. Abrams, R. Baecker, and M. Chignell. Information archiving with bookmarks: Personal web
space construction and organization. In Proc. Of CHI, pages 41–48, 1998.
[2]
E. Adar, J. Teevan, and S. T. Dumais. Large scale analysis of web revisitation patterns. In Proc.
of CHI, pages 1197–1206, 2008.
[3]
E. Adar, J. Teevan, and S. T. Dumais. Large scale analysis of web revisitation patterns. In Proc.
of CHI, pages 1197–1206, 2008.
[4]
R. C. Atkinson and R. M. Shiffrin. The control processes of short-term memory. Scientiﬁc
American, 225(2):82–90, 1971.
[5]
A. Aula, N. Jhaveri, and M. Kaki. Information search and re-access strategies of experienced
web users. In Proc. of WWW, pages 583–592, 2005.
[6]
D. K. Barreau. Context as a factor in personal information management systems. Journal of the
American Society for Information Science, 46(5):327–339, 1995.
[7]
L. Blunschi, J. P. Dittrich, O. R. Girard, S. K. Karakashian, and M. V. Salles. A dataspace
odyssey: The imemex personal data space management system. In Proc. of CIDR, pages
114–119, 2007.
[8]
H. Bruce and W. Jones. Keeping and reﬁnding information on the web: What do people do and
what do they need? In Proc. of the American Society for Information Science and Technology,
pages 129–137, 2004.
[9]
N. Burgess, S. Becker, J. A. King, and J. O’Keefe. Memory for events and their spatial context:
Models and experiments. Philosophical Transactions of the Royal Society, 356(2001):
1493–1503, 2001.
[10]
T. Buzan and B. Buzan. The Mind Map Book: Radiant Thinking, London UK: BBC Books, 1993.
Authenticated
:27 PM

Literature
381
[11]
Y. Cai, X. L. Dong, A. Halevy, J. M. Liu, and J. Madhavan. Personal information management
with SEMEX. In Proc. of ACM SIGMOD, pages 921–923, 2005.
[12]
L. Camara, T. Guerreiro, D. Goncalves, and J. A. Jorge. Realﬁnd: Managing personal items in the
physical world. In Proc. of CHI, pages 3321–3326, 2008.
[13]
R. Capra and A. Manuel. Using web search engines to ﬁnd and reﬁnd information. IEEE
Computer, 38(10):36–42, 2005.
[14]
R. Capra, M. Pinney, and A. S. Manuel. Reﬁnding is not ﬁnding again. Technical report, August
2005.
[15]
R. G. Capra. Mobile information reﬁnding as a continuing dialogue. In Proc. of CHI, pages
664–665, 2003.
[16]
R. G. Capra. Studying elapsed time and task factors in reﬁnding electronic information. In
Proc. of CHI Workshop on Personal Information Management, 2008.
[17]
R. G. Capra and M. A. Perez-Quinones. Mobile reﬁnding of web information using a voice
interface: An exploratory study. In Proc. of the 2005 Latin American Conference on
Human-Computer Interaction, pages 88–99, 2005.
[18]
R. G. Capra and M. A. Perez-Quinones. Using web search engines to ﬁnd and reﬁnd
information. IEEE Computer, 38(10):36–42, 2005.
[19]
R. G. Capra and M. A. Perez-Quinones. Factors and evaluation of reﬁnding behaviors. In Proc.
of SIGIR Workshop on Personal Information Management, pages 16–19, 2006.
[20]
R. G. Capra, M. A. Perez-Quinones, and N. Ramakrishnan. Webcontext: Remote access to
shared context. In Proc. of the 2001 Workshop on Perceptive User Interfaces, pages 1–9, 2001.
[21]
D. Carmel, N. Zwerdling, I. Guy, S. Ofek-Koifman, N. Harel, I. Ronen, E. Uziel, S. Yogev, and S.
Chernov. Personalized social search based on the user’s social network. In Proc. of CIKM,
pages 1227–1236, 2009.
[22]
D. H. Chau, B. Myers, and A. Faulring. What to do when search fails: Finding information by
association. In Proc. of CHI, pages 999–1008, 2008.
[23]
J. Chen, H. Guo, W. Wu, and W. Wang. iMecho: An associative memory based desktop search
system. In Proc. of CIKM, pages 731–740, 2009.
[24]
J. Chen, H. Guo, W.Wu, and C. Xie. Search your memory! – An associative memory based
desktop search system. In Proc. of ACM SIGMOD, pages 1099–1102, 2009.
[25]
Y. Chen and G. Jones. Integrating memory context into personal information reﬁnding. In Proc.
of the 2nd Symposium on Future Directions in Information Access, pages 14–21, 2008.
[26]
P. A. Chirita, S. Costache, W. Nejdl, and R. Paiu. Beagle++: Semantically enhanced searching
and ranking on the desktop. In Proc. of ESWC, pages 348–362, 2006.
[27]
A. Cockburn, S. Greenberg, S. Jones, B. Mckenzie, and M. Moyle. Improving web page
revisitation: Analysis, design and evaluation. International Journal of IT & Society,
1(3):159–183, 2003.
[28]
A. Cockburn, B. McKenzie, and M. Jason-Smith. Pushing back: Evaluating a new behaviour for
the back and forward buttons in web browsers. International Journal of Human-Computer
Studies, 57(5):397–414, 2002.
[29]
S. Cohen, C. Domshlak, and N. Zwerdling. On ranking techniques for desktop search. ACM
Transactions on Information Systems, 26(2):11:1–24, 2008.
[30]
J. P. Dittrich and M. A. Salles. iDM: A uniﬁed and versatile data model for personal dataspace
management. In Proc. of VLDB, pages 367–378, 2006.
[31]
T. V. Do and R. A. Ruddle. The design of a visual history tool to help users reﬁnd information
within a website. In Proc. of ECIR, pages 459–462, 2012.
[32]
X. Dong and A. Halevy. A platform for personal information management and integration. In
Proc. of CIDR, pages 119–130, 2005.
[33]
S. Dumais, E. Cutrell, J. Cadiz, G. Jancke, R. N. Sarin, and D. C. Robbins. Stuff I’ve seen: A
system for personal information retrieval and re-use. In Proc. of SIGIR, pages 72–79, 2003.
Authenticated
:27 PM

382
13 Context-Based Information Reﬁnding
[34]
R. Durrett. Probability: Theory and examples, Cambridge University Press, New York, 4th ed.,
2010.
[35]
M. Eldridge, M. Lamming, and M. Flynn. Does a video diary help recall? In Proc. of Intl. Conf. on
People and Computers, pages 257–269, 1993.
[36]
H. C. Ellis and R. R. Hunt. Fundamentals of Human Memory and Cognition, William C. Brown
Publishers, U.S., 3rd ed., 1983.
[37]
D. Elsweiler, M. Baillie, and I. Ruthven. Exploring memory in email reﬁnding. ACM Transactions
on Information Systems, 26(4):21:1–36, 2008.
[38]
D. Elsweiler, M. Baillie, and I. Ruthven. What makes reﬁnding information difﬁcult? A study of
email reﬁnding. In Proc. of ECIR, pages 568–579, 2011.
[39]
D. Elsweiler and I. Ruthven. Towards task-based personal information management
evaluations. In Proc. of SIGIR, pages 23–30, 2007.
[40]
L. Feng, P.G. Apers, and W. Jonker. Towards context-aware data management for ambient
intelligence. In Proc. of DEXA, pages 422–431, 2004.
[41]
M. Fuller, L. Kelly, and G. J. F. Jones. Applying contextual memory cues for retrieval from
personal information archives. In Proc. of the CHI Workshop PIM, 2008.
[42]
J. A. Gamez, J. L. Mateo, and J. M. Puerta. Improving revisitation browsers capability by using a
dynamic bookmarks personal toolbar. In Proc. of WISE, pages 643–652, 2007.
[43]
J. Gemmell, G. Bell, and R. Lueder. MyLifeBits: A personal database for everything.
Communications of the ACM, 49(1):88–95, 2006.
[44]
J. Gemmell, G. Bell, R. Lueder, S. Drucker, and C. Wong. MyLifeBits: Fulﬁlling the memex
vision. In Proc. of Multimedia, pages 235–238, 2002.
[45]
H. Georgiadis and V. Vassalos. Improving the efﬁciency of Xpath execution on relational
systems. In Proc. of EDBT, pages 570–587, 2006.
[46]
L. Guo, F. Shao, C. Botev, and J. Shanmugasundaram. XRANK: Ranked keyword search over xml
documents. In Proc. of SIGMOD, pages 16–27, 2003.
[47]
J. Hailpern, N. Jitkoff, A. Warr, R. Karahalios, K.and Sesek, and N. Shkrob. Youpivot: Improving
recall with contextual search. In Proc. of CHI, pages 1521–1530, 2011.
[48]
D. Horowitz and S. D. Kamvar. The anatomy of a large-scale social search engine. In Proc. of
WWW, pages 431–440, 2010.
[49]
W. James. Principles of Psychology, New York: Holt, 1890.
[50]
W. Jones and H. Bruce. How do people get back to information on the web? How can they do it
better? In Proc. of INTERACT, pages 793–796, 2003.
[51]
W. Jones, H. Bruce, and S. Dumais. Keeping found things found on the web. In Proc. of CIKM,
pages 119–126, 2001.
[52]
J. Jonides and D. E. Nee. Brain mechanisms of proactive interference in working memory.
Neuroscience, 139(1):181–193, 2006.
[53]
S. Kaasten and S. Greenberg. Integrating back, history and bookmarks in web browsers. In
Proc. of HCI, pages 379–380, 2001.
[54]
M. J. Kahana, M. W. Howard, and S. M. Polyn. Associative retrieval processes in episodic
memory. In Learning and Memory: A Comprehensive Reference, Academic Press, pages 1–24,
2008.
[55]
D. R. Karger, K. Bakshi, D. Huynh, D. Quan, and V. Sinha. Haystack: A customizable
general-purpose information management tool for end users of semistructured data. In Proc.
of CIDR, pages 13–26, 2003.
[56]
D. R. Karger, K. Bakshi, D. Huynh, D. Quan, and V. Sinha. Haystack: A customizable
general-purpose information management tool for end users of semistructured data. In Proc.
of CIDR, 2003.
[57]
R. Kawase, G. Papadakis, E. Herder, and W. Nejdl. Beyond the usual suspects: Context-aware
revisitation support. In Proc. of HT, pages 27–36, 2011.
Authenticated
:27 PM

Literature
383
[58]
L. Kelly, D. Byrne, and G. J. F. Jones. The role of places and spaces in lifelog retrieval. In Proc. of
Personal Information Management Workshop, pages 43–46, 2009.
[59]
L. Kelly, Y. Chen, M. Fuller, and G. J. F. Jones. A study of remembered context for information
access from personal digital archives. In Proc. of the 2nd Intl. Symposium on Information
Interaction in Context, pages 44–50, 2008.
[60]
C. Kulkarni, S. Raju, and R. Udupa. Memento: Unifying content and context to aid webpage
re-visitation. In Proc. of UIST, pages 435–436, 2010.
[61]
M. Lamming and M. Flynn. “Forget-me-not” intimate computing in support of human memory.
In Proc. of FRIEND21 Intl. Symposium on Next Generation Human Interface, 1994.
[62]
D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka. Training algorithms for linear text
classiﬁers. In Proc. of SIGIR, pages 298–306, 1996.
[63]
J. Li, C. Liu, R. Zhou, and W. Wang. Top-k keyword search over probabilistic xml data. In Proc. of
ICDE, pages 673–684, 2011.
[64]
Y. Li and X. Meng. Supporting context-based query in personal dataspace. In Proc. of CIKM,
pages 1437–1440, 2009.
[65]
C. Lustig and L. Hasher. Implicit memory is not immune to interference. Psychological Bulletin,
127(5):629–650, 2001.
[66]
B. MacKay, M. Kellar, and C. Watters. An evaluation of landmarks for reﬁnding information on
the web. In Proc. of CHI, pages 1609–1612, 2005.
[67]
E. J. Maratos, R. J. Dolan, J. S. Morris, R. N. A. Henson, and M. D. Rugg. Neural activity
associated with episodic memory for emotional context. Neuropsychologia, 39(9):910–920,
2001.
[68]
M. Mayer. Web history tools and revisitation support: A survey of existing approaches and
directions. Foundations and Trends in HCI, 2(3):173–278, 2009.
[69]
N. Milic-Frailing, R. Jones, K. Rodder, G. Smyth, A. Blackwell, and R. Sommerer. Smartback:
Supporting users in back navigation. In Proc. of WWW, pages 63–71, 2004.
[70]
J. M. Moon and W. Fu. Effects of spatial locations and luminance on ﬁnding and reﬁnding
information in a desktop environment. In Proc. of CHI, pages 3365–3370, 2009.
[71]
J. M. Moon and W. Fu. Where is my stuff? Augmenting ﬁnding and reﬁnding information by
spatial locations and icon luminance. In Proc. of Intl. Conf. on Foundations of Augmented
Cognition, pages 58–67, 2009.
[72]
D. Morris, M. R. Morris, and G. Venolia. Searchbar: A search-centric web history for task
resumption and information reﬁnding. In Proc. of CHI, pages 1207–1216, 2008.
[73]
C. Peery, W. Wang, A. Marian, and T. D. Nguyen. Multi-dimensional search for personal
information management systems. In Proc. of EDBT, pages 464–475, 2008.
[74]
H. Pu and X. Jiang. A comparison of how users search on web ﬁnding and reﬁnding tasks. In
Proc. of the 2011 iConference, pages 446–451, 2011.
[75]
P. Qvarfordt, S. Tretter, G. Golovchinsky, and T. Dunnigan. Search panel: Framing complex
search needs. In Proc. of SIGIR, pages 495–504, 2014.
[76]
B. J. Rhodes and T. Starner. Remembrance agent: A continuously running automated
information retrieval system. In Proc. of Intl. Conf. on the Practical Application of Intelligent
Agents and Multi Agent Technology, pages 487–495, 2006.
[77]
F. Rizzo, F. Daniel, M. Matera, S. Albertario, and A. Nibioli. Evaluating the semantic memory of
web interactions in the Xmem project. In Proc. of AVI, pages 185–192, 2006.
[78]
J. A. Robinson and K. L. Swanson. Autobiographical memory: The next phase. Applied
Cognitive Psychology, 4(4):321–335, 1990.
[79]
D. C. Rubin and A. E. Wenzel. One hundred years of forgetting: A quantitative description of
retention. Psychological Review, 103(4):734–760, 1996.
Authenticated
:27 PM

384
13 Context-Based Information Reﬁnding
[80]
M. D. Rugg and E. L. Wilding. Retrieval processing and episodic memory. Trends in Cognitive
Sciences, 4(3):108–115, 2000.
[81]
I. Ruthven and M. Lalmas. A survey on the use of relevance feedback for information access
systems. Knowledge Engineering Review, 18(2):95–145, 2003.
[82]
M. A. Salles, J. Dittrich, and L. Blunschi. Intensional associations in dataspaces. In Proc. of
ICDE, pages 984–987, 2010.
[83]
D. L. Schacter. Searching for Memory: The Brain, the Mind, the Past, New York: Basic Books,
1996.
[84]
M. A. Soliman, I. F. Ilyas, D. Martinenghi, and M. Tagliasacchi. Ranking with uncertain scoring
functions: Semantics and sensitivity measures. In Proc. of ACM SIGMOD, pages 805–816,
2011.
[85]
R. Sorabji. Aristotle on Memory, University of Chicago Press, Chicago, 2nd ed., 2006.
[86]
C. A. N. Soules and G. R. Ganger. Connections: Using context to enhance ﬁle search. In Proc. of
SOSP, pages 119–132, 2005.
[87]
H. Takano and T. Winograd. Dynamic bookmarks for the WWW. In Proc. of HYPERTEXT, pages
297–298, 1998.
[88]
R. M. Tarpy and R. E. Mayer. Foundations of Learning and Memory, Pearson Scott Foresman,
U.S.A. 1978.
[89]
L. Tauscher and S. Greenberg. How people revisit web pages: Empirical ﬁndings and
implications for the design of history systems. International Journal of Human Computer
Studies, 47:97–137, 1997.
[90]
L. Tauscher and S. Greenberg. How people revisit web pages: Empirical ﬁndings and
implications for the design of history systems. International Journal of Human Computer
Studies, 47(1):97–137, 1997.
[91]
L. Tauscher and S. Greenberg. Revisitation patterns in world wide web navigation. In Proc. of
CHI, pages 399–406, 1997.
[92]
J. Teevan. The re:search engine: Helping people return to information on the web. In Proc. of
SIGIR Doctoral Consortium, 2005.
[93]
J. Teevan. The re:search engine: Simultaneous support for ﬁnding and reﬁnding. In Proc. of
UIST, pages 23–32, 2007.
[94]
J. Teevan. How people recall, recognize, and reuse search results? ACM Transactions on
Information Systems, 26(4):19:1–27, 2008.
[95]
J. Teevan, E. Adar, R. Jones, and Mi. A. S. Potts. Information re-retrieval: Repeat queries in
Yahoo’s logs. In Proc. of SIGIR, pages 151–158, 2007.
[96]
J. Teevan, C. Alvarado, M. S. Ackerman, and D. R. Karger. The perfect search engine is not
enough: A study of orienteering behavior in directed search. In Proc. of CHI, pages 415–422,
2004.
[97]
T. D. Tomlinson, D. E. Huber, C. A. Rieth, and E. J. Davelaar. An interference account of
cue-independent forgetting in the no-think paradigm. Proceedings of the National Academy of
Sciences, 106(37):15588–15593, 2009.
[98]
J. Z. Tsien. The memory. Scientiﬁc American, Inc., U.S., 2007.
[99]
E. Tulving. What is episodic memory? Current Directions in Psychological Science, 2(3):67–70,
1993.
[100]
E. Tulving and D. M. Thomson. Encoding speciﬁcity and retrieval processes in episodic
memory. Psychological Review, 80(5):359–380, 1973.
[101]
S. Tyler and J. Teevan. Large scale query log analysis of reﬁnding. In Proc. of WSDM, pages
191–200, 2010.
[102]
M. V. Vieira, B. M. Fonseca, R. Damazio, P. B. Golgher, D. D. C. Reis, and B. Ribeiro-Neto.
Efﬁcient search ranking in social networks. In Proc. of CIKM, pages 563–572, 2007.
Authenticated
:27 PM

Literature
385
[103]
W. Wang, A. Marian, and T. D. Nguyen. Uniﬁed structure and content search for personal
information management systems. In Proc. of EDBT, pages 201–212, 2011.
[104]
S. Whittaker, T. Matthews, J. Cerruti, H. Badenes, and J. Tang. Am i wasting my time organizing
email? A study of email reﬁnding. In Proc. of CHI, pages 3449–3458, 2011.
[105]
L. C. Wiggs, J. Weisberg, and A. Martin. Neural correlates of semantic and episodic memory
retrieval. Neuropsychologia, 37(1999):103–118, 1999.
[106]
wiseGEEK. What is the forgetting curve? http://www.wisegeek. com/what-is-the-
forgetting-curve.htm, 2015.
[107]
E. L. Wohldmann, A. F. Healy, and L. E. Bourne. A mental practice superiority effect: Less
retroactive interference and more transfer than physical practice. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 34(4):823–833, 2008.
[108]
S. S. Won, J. Jin, and J. I. Hong. Contextual web history: Using visual and contextual cues to
improve web browser history. In Proc. of CHI, pages 1457–1466, 2009.
[109]
S. S. Won, J. Jin, and J. I. Hong. Contextual web history: Using visual and contextual cues to
improve web browser history. In Proc. of CHI, pages 1457–1466, 2009.
Authenticated
:27 PM

Authenticated
:27 PM

14 A Context-Aware Ad-Hoc Meeting
Planner Program
Abstract: Pioneering context-aware applications aim to serve as ofﬁce and meet-
ing tools, (tourist) guides, context-aware ﬁeldwork tools, and memory aids. The main
context information explored is user identity, time, and location. Presentation of in-
formation and services to a user, automatic execution of a service, and tagging of context
to information for later retrieval constitute the main usage domain of the context. This
chapter ﬁrst reviews early attempts in building context-aware applications, and then
details a context-aware ad-hoc meeting planner application.
14.1 Early Pioneering Context-Aware Applications
Early killer applications for context-awareness aim at proactive triggering, streamlining
interaction, memory for past events, reminders for future context, optimizing patterns of
behavior, and sharing experiences [3].
The Active Badge System developed at the Olivetti Research Lab is considered to
be one of the ﬁrst context-aware application [17]. The ofﬁce personnel wears badges
that transmitted IR-signals. A network of sensors placed around the building picks up
the signals and a central location server polled these sensors. With the system, people
can be located in the ofﬁce and calls can be forwarded to the closest phone.
The ParcTab System from the Xerox Palo Alto Research Center is based on palm-
sized wireless ParcTab computers and an infrared cellular network. The ParcTab
works as a mobile personal digital ofﬁce assistant, presenting information about the
room the user is in, helping the user ﬁnd local resources like the nearest printer, and
locate other people carrying a ParcTab, etc. [18].
With the Context Toolkit, Georgia Institute of Technology developed several
context-aware applications. The In/Out board uses the identity information, returned
from the tag worn by the employee, and the arrival/departure time of the employee, to
show if a certain person is in the ofﬁce building or not [13]. The Conference Assistant
aims to assist conference attendants [5]. Based on the conference schedule, topics of
presentations, user’s location and interests, it can suggest the presentations to attend.
When the user enters a presentation room, it automatically displays the information
about the presenter and presentation material. Available audio and video equipment
automatically record the slides of current presentation, comments, and questions. The
Cyberguide is a tour-guide assistant [1]. It can provide information services to a tourist
about the current location, and recommend places of interest to visit.
The GUIDE system from the University of Lancaster is another context-sensitive
tourist guide for visitors to the city of Lancaster, England. It was also adapted to assist
museum/exhibition visitors [4].
DOI 10.1515/9783110556674-014
Authenticated
:30 PM

390
14 A Context-Aware Ad-Hoc Meeting Planner Program
The Shopping Assistant built by the AT&T Bell Laboratory can guide the shoppers
through the store, provide details of items, help locate items, point out items on sale,
and do a comparative price analysis, etc., on the basis of customers’ location within
the shop [2].
The Forget-me-Not and Satchel projects at Xerox Research Center Europe aim to
help ﬁnd documents and extend the range of available services by taking advantages
of the user’s context including activities conducted, and nearby devices, etc. [7, 9].
The ComMotion project at MIT Media Laboratory utilizes both location and time
context to deliver reminder messages via voice synthesis when the intended recipient
arrives at certain locations [10].
More detailed descriptions of context-aware applications can be found in good
surveys [6, 11, 14].
14.2 Motivation
It is a good tradition for people to meet and talk based on their pre-made appointments
in the agendas. However, in many real situations, people may have ad-hoc meeting
requirements arising instantaneously, and thus may not always follow strict plans to
perform their conversations.
Consider the following scenarios which may happen quite often in daily life.
1) Alice is a database person who was asked to suggest some classical books on
Artiﬁcial Intelligence. She has no idea and therefore wants to consult her colleague in
this ﬁeld either by phone call or just dropping by in his ofﬁce.
2) Allan is a new graduate student who needs to enroll three courses this semester.
He likes to make selection from among the list together with his supervisor. Therefore, he
wants to meet his supervisor for a brief discussion before the enrolling deadline.
3) Bill is willing to attend an overseas summer school, but needs the approval of his
supervisor. He wants to talk to his supervisor when his supervisor is relatively free and
preferably in good mood.
4) Having been blocked by a scientiﬁc challenge for a while, John is very excited
about a new idea just pumping out of his mind, and hereby wants to discuss it with his
collaborator as soon as possible. A problem instantaneously appears in Joe’s email box,
and Joe wants to ask the technician of the group to help ﬁx it as soon as possible.
5) Peter is a busy manager who is thinking about a new scientiﬁc program to be
launched. He wants to chat with relevant people to get some initial feedback.
6) Smith is at a seminar, hearing something which is highly relevant to his students’
research work. He is wondering whether they can come over immediately and hereby
wants to contact them.
Different from people’s normal appointment-based meetings, the above meet-
ing requests usually arise ad-hoc without arrangements in advance. Moreover, the
duration of these meetings may last short in minutes. The communication channels
Authenticated
:30 PM

14.4 ConPlan Design Considerations
391
can also be ﬂexible enough to be done face-to-face, via phone call, instant-message,
video-conference, or even skype, etc. These kinds of meetings are called ad-hoc
meetings.
The goal here is to deliver ubiquitous computing and communication supports
to such ad-hoc meeting planning, particularly considering how to make people more
aware of appropriate opportunities while maintaining maximal ﬂexibility for ad-hoc
meetings and how to use their time as effective as possible, thus speeding up the
decision-making processes and the heartbeat rate of the organizations.
14.3 Context-Awareness in ConPlan
The aim of the ConPlan program is to facilitate busy users to make the best use of their
time by providing a list of possibilities in response to their ad-hoc meeting requests.
Two types of users will be served by the ConPlan program, i.e., the one who requests
an ad-hoc meeting by entering his/her request; and the requested one who can always
check the list of his/her requestors and decides whom to meet at the moment. ConPlan
can be viewed as an application of advanced context-aware mobile and ubiquitous
computing technologies.
As ConPlan plays a human-like role in a real setting, the design principle of Con-
Plan must stay closely to the user-side, where usability serves as the most fundamental
criterion to assess the quality of the delivered solution. According to the International
Organization for Standardization (ISO), usability is the extent to which a product can
be used by speciﬁc users to achieve speciﬁed goals with effectiveness, efﬁciency, and
satisfaction in a speciﬁc context of use (ISO 9241-11). While effectiveness refers to the
capability of providing functions for which it was designed, and efﬁciency refers to the
efforts (mainly the time and space) required to offer these functions, satisfaction is re-
garded as the most subjective aspect from the standpoint of users, which expresses
the feeling that a human user experiences during and after the use of the system [8].
In line with this deﬁnition, usability is not an absolute value intrinsic to the sys-
tem, but rather a factor that can vary depending on the goal, task, background, and
experiences of the user, in other words, the context of use. Hereby, to deliver really
useful conversation opportunities, the ConPlan program must be context-aware, as
reﬂected through various users’ preferences, conveying one’s beliefs and attitudes to
plan and conduct an ad-hoc meeting.
14.4 ConPlan Design Considerations
Planning an ad-hoc meeting must take the following inherent attributes into account.
–
Initiator. An ad-hoc meeting may be initiated by one or multiple people.
–
Participant(s)/Requestor. The initiator(s) may want to talk to one person or a group
of people.
Authenticated
:30 PM

392
14 A Context-Aware Ad-Hoc Meeting Planner Program
–
Topic to be discussed during the ad-hoc meeting.
–
Goal to be achieved after the ad-hoc meeting. It can be knowledge acquisi-
tion, sharing, and transfer (e.g., advice on buying/selling something, ﬁxing some
problems which instantly arise or exist for a while, etc.).
–
Channel. The ad-hoc meeting can be done by phone, face-to-face, instant-
message, video-conference, or skype, etc.
–
Duration. The ad-hoc meeting can be instantaneous (around 1–2 minutes), short
(around 10 minutes), or long (30 minutes or so).
–
Urgency/Importance Level. The urgency level to conduct the ad-hoc meeting can
be urgent, moderate, light, etc.
–
Completion Constraints to accomplish the ad-hoc meeting. For instance, the con-
versation must be done within the campus by this weekend, and the later, the
more urgent it becomes.
–
Preferences for conducting the ad-hoc meeting. For example, a casual-chat-
purpose conversation is preferably done when the requested person is having a
coffee break. For the goal of seeking approval or signature, the conversation is
preferably done when the wanted person has a good mood. The chat about some
health issue is preferably done when the requested person is alone.
14.5 ConPlan Framework
Figure 14.1 outlines how ConPlan works in planning an ad-hoc meeting.
Upon an ad-hoc meeting request, ConPlan ﬁrst queries the outlook agenda of the
wanted person to ﬁnd out some meeting possibilities that satisfy users’ constraints
and preferences. Meanwhile, it also consults the previous conversation history (log)
of the wanted person via the context management module to enforce or complement
the possibilities, which will then be returned to the user as an initial answer in the
order of time (i.e., a ranked list of opportunities).
After that, it starts to monitor these possibilities, and meanwhile keeps eyes open
on new unanticipated opportunities. When the behavior of the requested person devi-
ates from the original schedule in his/her agenda, an immediate ad-hoc meeting might
be possible as well. For instance, the requested person ﬁnishes an appointment and
returns to his/her ofﬁce 10 minutes earlier than scheduled, exhibiting a possibly good
opportunity for a short conversation.
The context monitor is responsible for checking context information in a timely
fashion and decides whether a particular possibility is indeed a good ad-hoc meeting
opportunity. Once this is the case, a chance alert will be sent to the user, so that a
conversation between the user and the wanted person can be conducted right away.
To do this, the context manager plays an important role. It gathers low-level con-
text information from various context suppliers such as sensors, and performs context
Authenticated
:30 PM

14.6 ConPlan Context Management
393
User
Meeting planner
Ad-hoc meeting
request
Agenda
history
Meeting
request
Context
Context
maintain
Context
inference
Context
aggregate
Environment & Body Sensors
Context
acquire
Request
manage
Context
monitor
Opportunity
notice
Agenda
query
History
consult
Initial
plan
Ad-hoc meeting
opportunities
Interaction
opportunities
Ad-hoc meeting
notice
High-level context info.
Low-level context info.
Dynamic monitor
Context manager
Figure 14.1: ConPlan architecture.
aggregation and context inference so as to derive high-level context. Necessary con-
text is stored into a context database for later retrieval and analysis. Besides, the
context manager has the duty to answer pull-context queries, and actively execute
push-context actions, in response to the requests of the context monitor. The ﬁnal an-
swer to the request will be logged (memorized), so that the context manager can do
learning and reasoning in order to deliver smart solutions later on.
14.6 ConPlan Context Management
In the ConPlan program, various dynamic situational factors constitute the users’ con-
versation preferences [15]. Central around the wanted person, ﬁve kinds of context
factors are considered, which are physical environment, computational environment,
social environment, activity, and emotional state of the person to be met. The physical
environment of the wanted person is comprised of his/her location, noise, brightness,
and temperature, etc. The computational environment describes the surrounded com-
puting and communication facilities, such as PDA (Personal Digital Assistant) devices,
large LCD displays, etc. The social environment is mainly oriented towards surround-
ing people. Besides, two other important human factors in ConPlan are the current
status of the requested person (e.g., busy, free, relaxing, etc.), as well as his/her mood
(happy, sad, angry, calm, etc.).
Authenticated
:30 PM

394
14 A Context-Aware Ad-Hoc Meeting Planner Program
Person ⊑∀hasLocation.Location ⊓∀hasStatus.Status ⊓∀hasMood.Mood ⊓∀
surroundedByPerson.Person ⊓∀hasFacility.Facility
RequestedPerson ⊑Person Colleague ⊑Person Visitor ⊑Person
InsideCampus ⊑Location OutsideCampus ⊑Location
Building ⊑InsideCampus Room ⊑Building Corridor ⊑Building
Ofﬁce ⊑Room CoffeeRoom ⊑Room
Busy ⊑Status Free ⊑Status
Meeting ⊑Busy Talking Busy ⊑PhoneCall ⊑Busy
Reading ⊑Free Writing ⊑Free Alone ⊑Free
Happy ⊑Mood Relaxed ⊑Mood Sad ⊑Mood Angry ⊑Mood
Computer ⊑Facility Network ⊑Facility Printer ⊑Facility
LCD-Display ⊑Facility PDA ⊑Computer
hasLocation ≡locationOf –1
Figure 14.2: ConPlan DL (Description Logic)-based context representation.
Figure 14.3: Context ontology management in Protégé.
The latter two items can be viewed as high-level context, which can be derived
from low-level context information acquired via various sensors. For example, if the
requested person is surrounded by a few people, a meeting is probably going on.
Based on this, it can be concluded that the requested person is busy at the moment.
Authenticated
:30 PM

14.6 ConPlan Context Management
395
Figure 14.4: Individual-related context ontology update in Protégé.
Figure 14.5: Requesting an ad-hoc meeting.
Authenticated
:30 PM

396
14 A Context-Aware Ad-Hoc Meeting Planner Program
Figure 14.6: Answering an ad-hoc meeting request.
On the contrary, if the requested person is at the coffee room with a coffee cup in the
hand, it implies that she/he is free at the moment.
Figure 14.2 gives the DL-based ontology for ConPlan, describing the context re-
lated to the requested person and preferred ad-hoc meeting situations. Along with the
use of ConPlan, more atomic concepts and atomic roles can be captured and added to
this initial ontology.
14.7 ConPlan Implementation
The whole ConPlan application is implemented using Java language (jdk1.5.0_03 and
jre1.5.0_03), and the ConPlan context ontology is managed through Protégé OWL API
3.3.1 [12], as illustrated in Figure 14.3. Individual-related context is monitored and
refreshed every 5 minutes, as reﬂected in Figure 14.4.
Authenticated
:30 PM

14.7 ConPlan Implementation
397
Figure 14.7: ConPlan interface for requested ones.
5
2575 2696 3011 3265 4698 6204 1132
1759 2217 2696 3125 3706 4495 5146 5575
0
10000
20000
30000
40000
50000
60000
105 1005 2005 5005 1000
5
2000
5
3000
5
4000
5
5000
5
6000
5
7000
5
8000
5
9000
5
1000
05
E.e Time(ms)
Figure 14.8: Context ontology reading time under different user numbers.
Authenticated
:30 PM

398
14 A Context-Aware Ad-Hoc Meeting Planner Program
ConPlan serves two types of users, i.e., the ad-hoc meeting requestor and the reques-
ted person. The requesting person issues a request (Figure 14.5) and gets a ranked list
of opportunities for selection (Figure 14.6) [16].
Besides, ConPlan enables the requested one to view the complete requesting list
(Figure 14.7). In case that she/he allows the conversation at the viewing moment,
she/he can just press the OK button, and an opportunity alert message will be
immediately sent to the corresponding requestor.
Figure 14.8 presents the performance of context ontology reading time versus the
total number of individuals (people) that ConPlan serves.
14.8 Recapitulation
This chapter listed some early pioneering context-aware applications, and detailed the
context-aware design considerations, architecture, and implementation of a Context-
aware ad-hoc meeting Planning application ConPlan. ConPlan intends to complement
people’s regular face-to-face meeting arrangements. It is hoped that the ConPlan ap-
plication described in this chapter could facilitate one to better understand how
context can assist ad-hoc meeting promptly and adaptively. There is great potential
for further work to deliver a uniﬁed complete solution to ad-hoc meeting planning
program. Of particular interest are the following technological issues: (1) deriving
high-level context from low-level sensed context, and anticipating users’ intentions;
(2) managing historical experiences in the context memory, including memory re-
cording, updating, refreshing, discarding, and maintenance; (3) decision making by
reasoning based on speciﬁc application logic, context memory, and supplied current
uncertain context information; (4) resolving information overload and unwanted in-
terruptions and striking a balance between context-awareness and privacy protection;
(5) evaluating the ConPlan program from both user-centric subjective standpoint and
system-centric performance standpoint.
More use of context in the ﬁelds of e-learning, context-aware search, and
FireGuide will be illustrated in the following chapters.
Literature
[1]
G. Abowd, C. Atkeson, J. Hong, S. Long, R. Kooper and M. Pinkerton. Cyberguide: A mobile
context-aware tour guide. Wireless Networks, 3(5):421–433, 1997
[2]
A. Asthana, M. Cravatts, and P. Krzyzanowski. An indoor wireless system for personalized
shopping assistance. In Proc. of the IEEE Workshop on Mobile Computing Systems and
Applications, USA, pages 69–74, 1994.
[3]
P. J. Brown, W. Burleson, M. Lamming, O. W. Rahlff, G. Romano, J. Scholtz, and D. Snowdon.
Context-awareness: Some compelling applications. In Proc. of CHI Workshop on the What, Who,
Where, When, Why and How of Context-Awareness, 2000.
[4]
N. Davies, K. Mitchell, K. Cheverst, and G. Blair. Developing a context sensitive tourist guide.
Technical report, Computing Department, Lancaster University, 1998.
Authenticated
:30 PM

Literature
399
[5]
A. Dey, D. Salber, D. Abowd, and M. Futakawa. The conference assistant: Combining
context-awareness with wearable computing. In Proc. of the 3rd Intl. Symposium on Wearable
Computers, USA, pages 21–28, 1999.
[6]
A. K. Dey and G. D. Abowd. Towards a better understanding of context and context-awareness.
Technical Report GIT-GVU-99-22, Georgia Institute of Technology, 1999.
[7]
F. Flynn, D. Pendlebury, C. Jones, M. Eldridge, and M. Lamming. The Satchel system
architecture: Mobile access to documents and services. Mobile Networks and Applications,
5(4):243–258, 2000.
[8]
ISO 9241. Part 11: Guidance on usability. http://www.userfocus.co.uk/resources/iso9241/
part11.html.
[9]
M. Lamming and M. Flynn. Forget-Me-Not: Intimate computing in support of human memory. In
Proc. of the Intl. Symposium on Next Generation Human Interface, pages 125–128, 1994.
[10]
N. Marmasse and C. Schmandt. Location-aware information delivery with ComMotion. In Proc.
of the 2nd Intl. Symposium on Handheld and Ubiquitous Computing, pages 157–171, UK, 2000.
[11]
K. Mitchell. A survey of context-aware computing. Technical report, Lancaster University, 2002.
[12]
Protege OWL api programmer’s guide. Part 11: Guidance on usability. http://protege.
stanford.edu/plugins/owl/api/guide.html.
[13]
D. Salber, A. Dey, and G. Abowd. The context toolkit: Aiding the development of
context-enabled applications. Technical Report GIT-GVU-98-33, USA, 1998.
[14]
B. N. Schilit, N. Adams, and R. Want. Context-aware computing applications. In Proc. of the
Workshop on Mobile Computing Systems and Applications, IEEE Computer Society, pages
85–90, 1994.
[15]
A. van Bunningen, L. Feng, and P. Apers. A context-aware preference model for database
querying in an ambient intelligent environment. In Proc. of DEXA, pages 33–43, 2006.
[16]
B. Wang. A spontaneous meeting planning application. Technical report, Dept. of Computer
Science and Technology, Tsinghua University, 2008.
[17]
R. Want, A. Hopper, V. Falcao, and J. Gibbons. The active badge location system. ACM
Transactions on Information Systems, 10(1):91–102, 1992.
[18]
R. Want, B. Schilit, N. Adams, R. Gold, K. Petersen, D. Goldberg, J. Ellis, and M. Weiser. An
overview of the PARCTAB ubiquitous computing experiment. IEEE Personal Communications,
2(6):28–43, 1995.
Authenticated
:30 PM

15 Context-Aware Learning
Abstract: With the rapid development of computing and communication technologies
and their increasing use in educational settings during the last decade, the learning
environments have experienced a few evolutions – from the most traditional in-class
learning, computer-aided learning, web-based learning, to the present day’s ambient
ubiquitous learning. This chapter shows how context-awareness unprecedentedly en-
hances learning experiences by endowing the users with the opportunities of learning
in context. Some context-aware learning prototype systems are also described.
15.1 Learning in an Ambient Intelligent World
According to Naismith et al., in the future, the success of learning and teaching will be
measured by how seamlessly it weaves itself into our daily lives, with the greatest success
paradoxically occurring at the point where we don’t recognize it as learning at all [26].
The ambient intelligent (AmI) world opens a world of unprecedented learning ex-
periences. Learning is moving more and more outside of the traditional classroom and
merging into the learner’s environment. It can happen anytime, anywhere, and by any
means in either a self-motivated or spontaneously activated manner as the learner in-
teracts with the physical environment and its objects (e.g., a plant in the street). For
the latter, the learning is usually integrated with the learner’s activity in a natural, un-
obtrusive, and often invisible way. The synergy of learning, activity, and environment
will beneﬁt the learner to more extensively reﬂect on, explain, and hypothesize about
the surrounding physical world in relation to his/her formal learning experience from
the classroom [13].
Furthermore, as an AmI environment can sense the status, judge, and anticipate
the behavior of the learner in the real world, it is possible to provide adaptive, per-
sonal, dynamic, and spontaneous supports to learning activities, laying down a solid
foundation to guarantee the right contents to be delivered to the learner at the right
time by the right means.
Along with such a wider scope for learning raised by AmI, the challenges for both
educators and e-learning technology developers of the future will be to ﬁnd ways
to ensure the new learning is highly explorative, situated, contextual, personalized,
collaborative, seamless, and long term to achieve the truly learner-centric education.
15.2 Motivation of Context-Aware Learning
Learning can take place anytime and anywhere. The tangible environment becomes
parts of learning stimulation, learning objects, and supporting tools. Learners can
locate resources and information in the context. Exploration can be integrated into
DOI 10.1515/9783110556674-015
Authenticated
:30 PM

15.2 Motivation of Context-Aware Learning
401
the learning context. Such contextualized learning embedded into the real world has
the following beneﬁts.
1) Motivating learning. Learning in the physical real world can increase not only
the authenticity of the learning task, but also the stimulation of the learner [31]. Ac-
tually, people are more sensitive to the authentic physical world around them than to
the rigid books and videos. Learners are more inclined to accept the style of learning a
new topic when they are carrying out a physical activity. According to Ackermann [1],
to learn from the experience, it is necessary to step back from it and to ponder it before
diving back into the experience. It is apparent that learners’ goals and interests are
closely related where the intrinsic motivation plays a vital part in the learning process.
As a matter of fact, when situated outside the educational institutions, the learner can
better harness the understanding of the affective factors in an AmI environment which
she/he can interact with [15].
2) Informal learning. Learning in context can enhance informal learning, which
can happen all of the time through nonstructured or nonsponsored process (e.g., at
school). Almost everyone is involved in some form of informal learning activities that
can be identiﬁed as signiﬁcant in one’s life. A survey by Livingstone reported that
the average number of hours devoted to informal learning activities by all Canadian
adults over the past years was around 15 hours per week, while the time spent in organ-
ized education courses is an average of about 3 hours per week if including the entire
population [21].
Informal learning is strongly affected both by the environment and the particular
situation the learner is facing. AmI constitutes a large playground to encourage and
facilitate such informal learning.
3) Adaptive learning. The context of learning and the learner’s knowledge, mo-
tivation, and background may vary greatly. In an AmI environment, the learner’s
situation, as well as the situation of the real world in which the learner is located can
be sensed. Context-awareness means gathering such contextual information related to
the learner and physical environment, based on which to provide a measure of what is
currently going on, and what is the desire of the learner by analyzing his/her cognitive
traits. Learning course and content that are particularly relevant to that situation can
then be made available. By context-awareness, the support to learning can reach the
adaptive, dynamic, and personalized level.
4) Seamless learning across contexts. Learning has a continuity nature. The move-
ment of the learner across different contexts will not inﬂuence the continuity of
his/her learning. That is, the learning process can be continued without being inter-
rupted. An AmI environment offers an open smart space, allowing the spontaneous
inter-operations among different information appliances which may dynamically join
in and leave out at will. It enables the learner to chunk the learning so that informa-
tion or resources that have been captured/presented in one context can be easily and
transparently transported to another.
Authenticated
:30 PM

402
15 Context-Aware Learning
15.3 Five Kinds of Learning Context
Frohberg [11] illustrated ﬁve different kinds of learning context.
1) Free context is irrelevant to learning activities. Free context learning activities
do not explicitly consider the context of the learner. For example, one may learn in
such an environment as subway, airport, or beach, each of which has no direct relation
to the learning task. Lehner et al. described a number of free context learning projects
in Ref. [20].
2) Formalized context refers to the traditional educational setting where learning
is in a well-deﬁned curriculum, offered by some educational institutions through a
central-educator-directed strategy. Examples of formal context are classroom, lecture
hall, auditorium, and so on. Formal context can also be virtual classrooms equipped
with real-time video communication.
3) Digital context is different from free and formalized context, and relevant to
learning. A digital context is set by computers as playground for the learner. Digital
context is a completely virtual environment, where learners are restricted to the screen
of the computer. Thus it admits of no physical, social, or emotional activities during
the learning process. The beneﬁts of digital context lie in the overwhelming control
of the educator over the entire learning environment, the autonomy by removing
physical constraints, as well as the reduction and adaptivity of system complexity.
4) Physical context is dedicated to contextual and explorative learning in a real
setting. It can be enriched by a digital context. AmI environment embodies a good
synergy of physical and digital context settings.
5) Informal context corresponds to the informal learning, which involves the
pursuit of understanding, knowledge, or skill, and occurs without the presence of ex-
ternally imposed curricular criteria [21]. Examples of informal context are like social
skill cooperation, change of learner’s habit, etc.
15.4 Enabling Techniques for Context-Aware Learning
Among various contexts that can be sensed from both the physical and virtual sub-
jective worlds, a user’s location, status, and identity are the most fundamental and
widely used contexts in ambient learning. The former physical contexts can usually
be sensed by hardware devices such as GPS devices, sensors, etc., while the latter is
usually obtained through the use of software components and services.
15.4.1 GPS-based Learning
Although some educators argue that schools are as much the real world as any other
place, there is no denying that what happens in classrooms is different from what
Authenticated
:30 PM

15.4 Enabling Techniques for Context-Aware Learning
403
happens elsewhere [38]. Tracing back to the end of nineteenth century, Dewey [6] has
already advocated that incorporating place into education activities can introduce a
lot of beneﬁts to students, including the improvement of their critical thinking skills
and learning engagement, the clear link between learning and their own lives, and
their integral development as whole social beings [16, 43]. In fact, place-aware edu-
cation can closely connect schools with the local community by grounding learning
in local phenomena and lived experiences. It has a deep root in the unique history,
culture, environment, and economy of a particular place [36, 38]
Nowadays, the convergence of handheld computing devices with GPS technology
exhibits huge potential for radically altering the way we experience places and under-
stand the past. The Old Man River project [30] began with a focus on towns in western
Illinois and expanded to include communities in ten states along Mississippi River, in-
corporating the science and history of the river into historical education. It used GPS
to present the learning content that is targeted to speciﬁc places along the river, and
offered contextual increasing relevancy so as to help the students capture the rich and
colorful history. CAERUS [27] employed GPS technology to provide personalized learn-
ing opportunities in an authentic context. Multimedia content was location-based and
available to the learner where and when it was appropriate. One could view his/her
location presented on the map-based interface automatically with audio content when
entering a region of interest.
15.4.2 Sensor-Based Learning
Apart from GPS, the sensor and wireless sensor network technologies can be applied
to detect any change in an ambient learning environment. Sensors embedded in an
environment can detect not only physical contexts like ambient light, noise, temperat-
ure, humidity, luminous intensity, etc., but also a wide range of learning state signals,
such as presence of learners, proximity, movement, learner’s concentration degree,
physiological state, psychological state, and so on. The sensed data can be further ag-
gregated by intelligent information processing to derive learning context at high-levels
of abstraction [41].
In this way, the learners’ status, information and environment can be dynamic-
ally integrated over time and space. When the learners’ knowledge base is changed,
the teaching contents can be adapted accordingly to the various context informa-
tion and learners’ requests in time, hereby, connecting and broadening learners’
understandings both in real world and classroom settings [32].
15.4.3 Personalized Learning
AmI advocates user-centered design where the user is placed in the center of the
design activity. In an AmI learning environment, as learning activities can happen
Authenticated
:30 PM

404
15 Context-Aware Learning
anytime anywhere to any learner, it is crucial to overcome the improper one-size-
ﬁts-all approach and provide learners with individual learning experiences. We call
this personalized learning, where user-centric context (such as interest, preference,
and background knowledge, etc.) is usually exploited to cater to each individual
learner.
In the literature, there exist quite a number of proposals about how to obtain
learner-related context. Normally, user proﬁles could be gathered either explicitly or
implicitly, i.e., through questionnaire or by observing and learning user’s behavior.
Undoubtedly, results obtained from questionnaires may not have the desired effect-
iveness on the imminent e-course selection and evaluation unless major focus is given
to the individual user proﬁle and corresponding user behavior [28].
Tzouveli et al. tackled the user proﬁle extraction problem using data mining
techniques [28], where knowledge is initially collected and stored in the form of e-
questionnaires. They then presented a novel mechanism which could automatically
analyze users’ responses to the e-questionnaires in order to create, update, and ﬁnally
utilize user proﬁles during the e-course selection. The proﬁle model was designed
in such a way that it could facilitate both the process of using preferences in pro-
ﬁle creation, and the process of tracking of these preferences throughout the whole
re-evaluation procedure. This work also dealt with the problems of multiple prefer-
ences and irrelevancy of some user’s usage log data with the interest to the user, by
clustering the user’s input.
Fink and Kobsa proposed to learn a user’s interests by modelling the user’s beha-
vior with three components: User Learning Component, Mentor Learning Component,
and Domain Inferences Component [9]. The univariate signiﬁcance analysis was ap-
plied to User Learning Component for its incremental learning and scalability. Mentor
Learning Component predicted missing values in individual user models, comparing
the current user with similar users. Spearman’s Rank Correlation was employed for
determining the proximity between users in Mentor Learning Component. Domain In-
ferences Component inferred interests and preferences in individual user models by
applying domain inferences to assumptions that were explicitly provided by users or
implicitly acquired by the User and Mentor Learning Components.
Henze et al. deﬁned ontologies for domain, user, and observation on the basis
of IEEE personal and private information (PAPI) [14] for the learner to administrate
learners’ information, including: personal information, relation information, security
information, preference information, performance information, and portfolio inform-
ation [12]. At runtime, learners interacted with the system. The users’ interactions
could be used to draw conclusions about users’ possible interests. After that, rules
encoded in TRIPLE were applied to reason over distributed information sources.
Denaux et al. discussed the need for using interactive ontology-based user model-
ing to empower on-the-ﬂy adaptation in learning information systems. They presented
an approach to adapt the learning content based on the integration of two existing
systems – AIMS and STyLE-OLM [5].
Authenticated
:30 PM

15.4 Enabling Techniques for Context-Aware Learning
405
Yu and Chen in [44] proposed to use Bayesian networks to determine learners’
styles for an unseen situation based on Felder–Silverman learning style theory, which
deﬁned the Felder–Silverman model and identiﬁed teaching practices that should
meet the needs of learners with the full spectrum of styles [8].
Chen employed a context-aware collaborative ﬁltering mechanism to predict a
user’s preference in different context situations based on past user-experiences and
help users ﬁnd the items that mostly matches their different preferences under con-
texts. Context similarity was calculated to determine which ratings were more relevant
for the current context. The author used Pearson’s correlation coefﬁcient to measure
the correlation between two different context variables with respect to their ratings for
an item. Each rating had an associated context which was the environment, that is,
the rating was given. The similarity between the rating’s context with the active user’s
context determined how relevant this ratings was, so that it could be incorporated into
the weight [2].
15.4.4 RFID-based Learning
Radio frequency identiﬁcation devices (RFID) technology as a means to seamlessly
fuse the physical world and digital world is one of the driving forces to make this
vision real.
15.4.4.1 RFID Primer
RFID is a system that uses radio frequency identiﬁcation devices for identiﬁcation and
tracking. RFID systems consist of two parts: RFID tags which are located on or in-
corporated into the object to be identiﬁed and are actually the data carrying device,
and RFID reader which reads or writes to the tags. RFID tags usually contain a small
integrated circuit (or silicon chip) which is electrically connected to an antenna for
receiving and transmitting signals. The data read from the tags are then processed
by software and can present real-time information about the associated items. The
obtained information can then be analyzed or shared online [40]. There are two fun-
damentally different RFID design approaches related to transferring power from the
reader to the tag, i.e., magnetic induction and electromagnetic wave capture.
RFID enables computers to read the identity of the tag from a distance without a
line of sight from which the barcode technology suffers. Some key advantages of RFID
over universal product code (UPC) are:
1) It does not require a line of sight to be read.
2) Due to its large capacity, it can be used to provide detailed annotations about
the unique object it is attached to.
3) It can be written, while the barcode is static after printed.
4) It can be extended to store larger volume of data and read at longer distances.
Authenticated
:30 PM

406
15 Context-Aware Learning
RFID has found wide applications in diverse ﬁelds, from tracking work in process
to speeding up throughput in a warehouse.
15.4.4.2 Use of RFID in Learning
It is a challenge for learning in context to present the right information at the right
time in the right place to the right person [10]. RFID offers a good solution to tackle this
challenge. Actually, there are a lot of exemplars where RFID technology is adopted for
learning, as evidenced from quite a number of research and industrial efforts [42].
(1) Academic Efforts
In the Ambient Wood Project [35], RFID tags were embedded in a woodland. Chil-
dren walked through the woodland and triggered the RFID marked items’ information
to appear on their displays. The Ambient Wood created an authentic learning en-
vironment, where children can intermittently switch from their experiences of the
physical world to reﬂect upon relevant knowledge behind. The project provided hand-
held devices and visualization tools to the children to access contextually relevant
digital information and share their explorations when indoors and outdoors, making
the children more proactive in their learning.
In the TANGO Tag Added LearNinG Objects System [29], RFID was used to help
beginners with their language (vocabulary) learning. In the room, a label on which
had been written the name of the object was attached to the corresponding object in
order to remind learners of the meaning of the word so that the learners could learn
in the real situation. Learners stuck RFID tags on real objects, made comments about
them, and shared them between each other. These tags acted as a bridge between real
objects and their information in the virtual world.
A digital library infrastructure with RFID was described in [24], which included a
layered data representation, support of adaptability, dual broadcast, and on-demand
query-based RFID tags, caching, and mobile-speciﬁc user interfaces.
Electronic note-taking was a means of recording the classroom experiences
including personal notes, ideas, results of collaborative work, and making them
available for off-line use. Miura et al. [23] promoted the note-sharing activities by
introducing RFID to bring a smart operation in projecting student notes.
(2) Industrial Efforts
RFID has been broadly used in the Museum and Art Sector often from an interpret-
ive and a virtual campus perspective. Examples include Refs. [4, 17, 25, 33, 39]. Some
typical ones are listed below.
1) The Personal Science Assistant [33] is a mobile device with RFID tags for child
visitors to the Michigan 4-H Children’s Garden to learn about the plants in the garden
and enhance their understanding of these plants. A learner can browse the garden
with a mobile device and stop in front of different plants – in front of which there are
corresponding physical signs. If the learner waves the mobile device over the sign,
Authenticated
:30 PM

15.5 Some Context-Aware Learning Prototypes
407
information about the plant appears in the interface. Thus, the mobile device has be-
come something like a plant-scanner which helps the user “see” what is invisible. As
the learner continues to explore the garden, other plant information is displayed as
they pass near the “trigger” locations. Triggering the RFID transmitters would bring
up information on that plant including the name of the plant, a picture of the plant,
and some other information that may not be visible when looking at the real plant.
2) Museum of Natural History Aarhus, Denmark [17]. Its visitors are able to explore
the world of birds with the help of RFID. In order to single out a bird and receive exhibit
information , a visitor uses RFID-enabled mobile device. The museum also introduces
mobile device incorporating Wi-Fi to help visitors access the museum’s WLAN and the
central database. When logged in, the visitor can access the exhibit in three different
modes: encyclopedia, theme, and game. With the encyclopedia mode, a person can
access information on any bird just by holding the mobile device close to an RFID tag
that is scanning the right bird. The theme mode allows narrations of several themes,
such as “Wings,” “Birds,” or “Airplanes.” In game mode, the user is involved in playful
problems to solve, such as identifying a given bird based on hints provided by the
mobile device.
3) Museum of Siena University [25] uses RFID to enable patrons to email them-
selves information about the exhibits before they leave. With the wave of the staff,
pieces of history the visitor has captured are presented on a screen. The visitor then de-
cides what she/he wants, and then with an virtual keyboard on the screen, the visitor
enters his/her email and the selected information is sent via email.
4) Augmented Museum Experience [39] describes the design of a study of visitors
to a science museum. The visitors are equipped with wirelessly connected handheld
devices. The museum environment is augmented with technologies developed by the
CoolTown research program at HP Labs. In CoolTown, all physical entities obtain
web presence. Nomadic users navigate from the real world environment to the virtual
environment by picking up URLs using sensing technologies.
5) Granite State MetalWorks [4] attaches RFID tags next to artwork to help visitors
and consumers ﬁnding out the descriptions about the artwork and its artist they may
be interested in.
Using the Art-FID system from Sapago, the RFIDs are placed beside works in the
gallery, and visitors use a pen-shaped tag reader in combination with a Dell Axim
handheld to read a piece of unique ID number and call up its information.
15.5 Some Context-Aware Learning Prototypes
A number of context-aware learning systems have been developed to engage in
adaptive, personalized learning activities.
Economides embedded an adaptive engine at the core of his learning system [7].
This module was responsible for acquiring input data and producing adapted learning
Authenticated
:30 PM

408
15 Context-Aware Learning
results. The input data consisted of various states about the learner, educational
activity, infrastructure, and environment. The engine made either deterministic or
nondeterministic decisions based on these information. Learning automata was
exploited to make the nondeterministic adaptation decision. Finally, the adapted
educational activities (contents, presentation of contents) were output.
A context-sensitive middle-ware for Learning Content Management System was
also developed by Chu et al. [3] to help instructors to prepare learning contents
through sensing and analyzing context from various sources. The same learning
contents were transformed to cater to the requirements of different portable devices.
The context-awareness subsystem designed by Lonsdale et al. accommodated a
central context-aware engine [22], which used a set of context features to derive a con-
text state. This context state was then used to eliminate unsuitable contents and rank
the remaining contents, obtaining the top-k options.
Sharma and Kitchens proposed to exploit web service architecture to build a
learning system [37]. The proposed architecture consisted of four layers: application
layer, the integration through web services standards layer, the delivery devices layer,
and the human layer. The application layer provided admission services, library ser-
vices, language translation, etc. The integration layer incorporated all the contents
and applications that might be available in different formats into a unit. The delivery
layer delivered the content, employing Internet-enabled multiple devices. The human
layer consisted of learners, administrators, and instructors.
In order to particularly adapt presentation of contents irrespective of learning
devices, three approaches have generally been employed to describe various device
capabilities. They are HTTP Request Header, CC/PP (composite capability/preferences
proﬁle), and UAPROF (user agent proﬁle). With the information of devices’ capabil-
ities, adaptation of content presentation can either be performed at the server side
(XML/XSLT, Cocoon, Axkit), proxy side (AvantGo, Palm Web Clipping), or client side
(XHTML/CSS).
15.6 Challenges upon Context-Aware Learning
Although people recognize advantages of context-aware learning, they underline that
lack of time due to job and family commitments make them fail to embrace the bene-
ﬁts [18]. Contextualized Learning enables people to learn anytime anywhere. However,
its characteristics pose numerous challenges.
First, high-quality learning services on constrained devices. Ambient learning is
characterized by the tiny size device which makes learning portable. High-quality
ambient learning service often depends on high volume graphical contents sent to
terminals. It is required that the designers make best use of the limited resources to
allow easy reliable access to personalized, high-quality learning materials.
Second, learning environments. Ubiquitous technologies provide the means to
promote situated learning, as they allow to enhance the learners’ context by
Authenticated
:30 PM

15.7 Recapitulation
409
establishing embedded ubiquitous environments in realistic settings [19]. So far, the
ambient learning environments are restricted to speciﬁc scenarios such as museums
and botanical garden. It is a great challenge for the researchers to integrate the
pervasive technologies to construct a general ambient learning environment.
Third, seamless learning support. In ambient learning, a learner shall be al-
lowed to learn without being interrupted while moving from place to place. This
calls for an underlying solid infrastructure, supporting spontaneous inter-operation
of information appliances as well as the physical integration technologies.
Fourth, personalized learning guidance in depth. Ambient learning system can
provide personalized service based on real-world context and learner’s proﬁle [13];
however, personalization is still limited to some basic personal data such as longest
and shortest acceptable time period. Systems fail to capture the detail preferences
according to user behavior.
Fifth, learning content integration. The ambient learning platform integrates vari-
ous kinds of learning contents with meta information to guarantee the supply of
high-quality knowledge units [18]. Documents on similar topics are grouped in one
category, however, the correct classiﬁcation of learning content with huge volume of
knowledge is still a demanding task.
Sixth, high-level learning context. Advanced technologies such as Wireless sensor
network, GPS, RFID, etc. have played increasingly important roles in collecting
learners’ contextual information. Recent ambient learning projects mainly focus on
low-level context obtained via these devices (e.g., location, activity, identity, etc.),
and high-level contextual information such as emotional condition has not been fully
explored yet [18]. Also, some robust and simple-to-use biosensor technology has not
seen wide applications in ambient learning. How to acquire various valid learning
state measurements and grasp the knowledge about the current situation of learners
is one of the challenging research directions.
Finally, learning assessment. The assessment of new educational goals such as
higher-order thinking, social skills or working in groups various projects are presented
such as digital portfolios [34]. Ambient learning raises adaptive assessment chal-
lenges upon us, as it is quite difﬁcult to evaluate the learning results when different
users join and leave learning traces.
15.7 Recapitulation
Aml stretches the boundaries of learning from traditional educational settings to
enable more learning paradigms. Major characteristics of such an AmI learning en-
vironment include learning in context, context-awareness, natural human–computer
interaction, and seamless learning across contexts with a ﬁnal target towards
learner-centric. This chapter illustrated how context-awareness empowers learning
experiences by endowing the users with the opportunities of learning in context. Some
enabling techniques and practices for context-aware learning were reviewed in the
chapter.
Authenticated
:30 PM

410
15 Context-Aware Learning
Literature
[1]
E. Ackerman. Perspective-taking and object construction: Two keys to learning. Constructionism
in Practice: Designing, Thinking and Learning in a Digital World, Y. Kafai and M. Resnick (eds.),
Mahwah, New Jersey: Lawrence Erlbaum Associates. Part 1, Chap. 2. pages 25–37, 1996.
[2]
A. Chen. Context-aware collaborative ﬁltering system: Predicting the user’s preferences in
ubiquitous computing. In Proc. of CHI Doctoral Consortium, 2005.
[3]
W. Chu, H. Lin, J. Chen, and X. Lin. Context-sensitive content representation for mobile learning.
In Proc. of the 4th Intl. Conf. on Web-based Learning, pages 349–354, 2005.
[4]
J. Collins. RFID for the art shy. http://www.rﬁdjournal.com/article/article view/1540/1/1/, 2005.
[5]
R. Denaux, V. Dimitrova, and L. Aroyo. Interactive ontology-based user modeling for
personalized learning content management. In Proc. of SWEL, pages 560–574, 2004.
[6]
J. Dewey. School and society. Dewey on Education, Martin Dworkin (ed.), New York: Teachers
College Press, pages 76–78, 1959.
[7]
A. Economides. Adaptive mobile learning. In Proc. of the Workshop on Wireless, Mobile and
Ubiquitous Technology in Education, pages 26–28, 2006.
[8]
R. Felder and L. Silverman. Learning and teaching styles in engineering education. Engineering
Education, 78(7):674–681, 1988.
[9]
J. Fink and A. Kobsa. User modeling for personalized city tours. Artiﬁcial Intelligence Review,
18:33–74, 2002.
[10]
G. Fischer. User modeling in human-computer interaction. Journal of User Modeling and
User-Adapted Interaction, 11(1/2):65–86, 2001.
[11]
D. Frohberg. Mobile learning is coming of age-what we have and what we still miss. In Proc. of
DeLFI, pages 327–338, 2006.
[12]
N. Henze, P. Dolog, and W. Nejdl. Reasoning and ontologies for personalized e-learning in the
semantic web. Educational Technology and Society, 7(4):82–97, 2004.
[13]
G. Hwang. Criteria and strategies of ubiquitous learning. In Proc. of SUTC, pages 72–77, 2006.
[14]
IEEE P1484.2/D7. Draft standard for learning technology – Public and private information (PAPI)
for learner (PAPI Learner), 2000.
[15]
A. Jones and K. Issroff. Learning technologies: Affective and social issues. Perspective on
E-learning Research, 2005.
[16]
U. Kelly. Marketing place: cultural politics, regionalism and reading. Fernwood Publishing,
Halifax, Canada, 1993.
[17]
F. Khan. Museum puts tags on stuffed birds. RFID Journal, 9(2004):1–2, 2004.
[18]
B. Kolmel. Ambient learning ﬁnal report. http://www.ambientlearning. com/ambient/
download/Files/AMBIENT_LEARNING_FINAL_REPORT.pdf/, 2006.
[19]
A. Kurti, D. Spikol, M. Milrad, M. Svensson, and O. Pettersson. Exploring how pervasive
computing can support situated learning. In Proc. of Pervasive Learning Workshop, pages
19–26, 2007.
[20] F. Lehner, H. Nosekable, and G. Bremen. M-learning and M-education. Mobile und drahtlose
Anwendungen im Unterricht. Bericht 63, Universitat Regensburg – Lehrstuhl fur
WirtsChaftsinformatik III, 2003.
[21]
D. Livingstone. Exploring the icebergs of adult learning: Findings of the ﬁrst Canadian survey of
informal learning practices. NALL (New Approaches to Lifelong Learning), OISE/UT, Ontario,
2000.
[22] P. Lonsdale, C. Baber, M. Sharples, and T. Arvanitis. A context-awareness architecture for
facilitating mobile learning. In Proc. of MLEARN, pages 304–312, 2003.
Authenticated
:30 PM

Literature
411
[23] M. Miura, S. Kunifuji, B. Shizuki, and J. Tanaka. AirTransNote: Augmented classrooms with
digital pen devices and RFID tags. In Proc. of the Workshop on Wireless and Mobile
Technologies in Education, pages 56–58, 2005.
[24] R. Morales-Salcedo, H. Ogata, and Y. Yano. Towards a new digital library infrastructure with RFID
for mobile e-learning. In Proc. of the Workshop on Wireless and Mobile Technologies in
Education, pages 261–263, 2005.
[25]
Museum of Siena University. http://www.adesignedpath.com/htmlPath/news/Museum/index.
html, 2003.
[26] L. Naismith, P. Lonsdale, G. Vavoula, and M. Sharples. Literature review in mobile technologies
and learning. Technical Report for NESTA Futurelab, Bristol, UK, 2004.
[27]
L. Naismith, M. Sharples, and J. Ting. Evaluation of CAERUS: A context aware mobile guide. In
Proc. of mLearning, pages 112–115, 2005.
[28] NTUA. SPERO tele informatics system for continuous collection, processing, diffusion of
material for teacher training in special education. http://www.image.ntua.gr/spero, 1970.
[29] H. Ogata, R. Akamatsu, H. Mitsuhara, Y. Yano, and K. Matsuura. TANGO: Supporting vocabulary
learning with RFID tags. In Proc. of the Workshop Series on RFID, 2004.
[30] Old Man River Project. http://eduscapes.com/omrp/, 2011.
[31]
R. Oppermann. User-interface design. Handbook of Information Technologies for Education and
Training, pages 233–248, 2002.
[32] N. Pachler. Mobile learning towards a research agenda. WLE Centre, Occasional Papers in
Work-based Learning, 2007.
[33] A. Ramchandran. Personal science assistant – A handheld PDA using RFID tags for child visitors
to the children’s garden designed to teach children introductory plant biology. Master’s thesis,
Michigan State University, 2004.
[34] J. Ridgway, S. McCusker, and D. Pead. Literature review of e-assessment. Report 10: FutureLab
Series, 2004.
[35]
Y. Rogers, S. Price, C. Randell, D. Fraser, M. Weal, and G. Fitzpatrick. UBI-learning integrates
indoor and outdoor experiences. Communications of the ACM, 48(1):55–59, 2005.
[36] The Rural School and Community Trust. What does place-based learning look like? Examples of
place-based learning portfolios. http://portfolio. ruraledu.org/, 2009.
[37]
S. Sharma and F. Kitchens. Web services architecture for m-learning. Electronic Journal on
e-Learning, 2(1):203–216, 2004.
[38] G. Smith. Place-based education: Learning to be where we are. Phi Delta Kappan,
83(3):584–594, 1983.
[39] M. Spasojevic and T. Kindberg. A study of an augmented museum experience.
http://www.hpl.hp.com/techreports/2001/HPL-2001-178.pdf, 2001.
[40] Victorian Government. Getting the most out of RFID: A starting guide to radio frequency
identiﬁcation for SMEs. Commonwealth of Australia, 2006.
[41]
M. Wang, L. Ci, P. Zhan, and Y. Xu. Applying wireless sensor networks to context-awareness in
ubiquitous learning. In Proc. of ICNC, pages 791–795, 2007.
[42] I. Whitehouse. E-learning using radio frequency identiﬁcation (RFID) device scoping study.
Australian Flexible Learning Framework, 2010.
[43] J. Woodhouse. Over the river and through the hood: Re-viewing place as a focus of pedagogy.
Thresholds in Education, 27(3–4):1–5, 2001.
[44] D. Yu and X. Chen. Using Bayesian networks to implement adaptivity in mobile learning. In Proc.
of SKG, page 97, 2006.
Authenticated
:30 PM

16 Context-Aware Management of Bilingual
Aviation MRO Task Cards
Abstract: In aviation, the consistency of usage and translations of MRO documents
and terminologies is a critical requirement for ﬂight operation and aircraft mainten-
ance to ensure ﬂight safety. Although English is one of the ofﬁcial languages in the
current aviation MRO (Maintenance, Repair, and Overhaul) industry, language barrier
often exists in the non-English speaking countries when working on an MRO docu-
ment in English. Given the recent advancement in the natural language translation
and advanced context-aware search techniques, being able to apply this progress to
the MRO industry and resolving language barrier would be a great technical break thru
and beneﬁt to MRO business operations. This chapter describes key techniques for
ambient smart and context-aware data management, which enable a more efﬁcient,
effective, and competitive MRO service for international customers. The approach fo-
cuses on how to generate, organize, manage, and integrate bilingual MRO task cards
based on its relevant contextual background, with an ultimate goal to quickly com-
pose, retrieve, and deliver information at the right time, to the right customers, under
the speciﬁc task orders, with a higher degree of task relevancy. An English-Chinese
bilingual MRO task cards generator system called TaskCardGeneratore2c, followed
by a context-aware enterprise search engine for bilingual MRO task cards called
TaskCardFinder, are particularly described in this chapter.
16.1 Motivation
Aviation industry is a competitive ﬁeld. Each year, aviation industry spends a whop-
ping amount on MRO services, and second on fuel. In 2007, for the global MRO
industry, it reached US$ 45 billion, and is expected to be US$ 61 billion by 2017. As per
industry projections, the size of the worldwide air transport ﬂeet will expand by nearly
50% to 2017, and consequently spur rapid growth of the MRO business [6]. With the for-
tunes of MRO business mirroring that of the airline industry in the global downturn,
suppliers of MRO services are under pressure to improve competitive international
productivity and cut costs.
Aviation MRO intends to ﬁx air crafts when they are out of order, or performing
routine actions to keep the air crafts in working order. MRO services must be con-
ducted under the instruction of MRO task cards, prepared in word documents. They
provide MRO employees with all different kinds of information, related to technical
and corresponding administrative, managerial, and supervision actions, in order to
successfully complete MRO services. Thus some task cards may possible run into over
a hundred pages.
MRO task cards are made available since the production of an aircraft, and most
of the time in one language such as English. However, with the globalization of
DOI 10.1515/9783110556674-016
Authenticated
:30 PM

16.2 Generating Bilingual MRO Task Cards (TaskCardGeneratore2c)
413
aviation services, for foreign subsidiaries of international aviation MRO service com-
panies, MRO task cards in multi-languages are needed. Figure 16.1 shows a sample
English-Chinese bilingual MRO task card document in MS Word format.
To provide competitive international MRO services, an efﬁcient and assistant
workplace is indispensable for employees in carrying out their MRO services. One par-
ticular problem for them to resolve is to deliver quick turnaround time and improved
accuracy for cost estimation, airplane maintenance schedule, resources availability
checking, etc.
Aiming to allow MRO service planners and technicians to quickly accurately
generate and locate necessary multi-language MRO task cards in MS Word format,
providing a better awareness of the MRO task at hand, particularly when one is faced
with language barrier, this chapter describes the design and implementation of two
prototype systems, i.e., TaskCardGeneratore2c and TaskCardFinder.
16.2 Generating Bilingual MRO Task Cards
(TaskCardGeneratore2c)
Translation of monolingual MRO task cards into the target language to obtain bilingual
MRO task cards is needed for international MRO services providers. Manual gener-
ation of Chinese MRO task cards by experienced MRO engineers is repetitive, time
consuming, and error prone. As any tiny mistake in MRO task cards may inﬂuence the
quality of MRO service, which may eventually lead to a plane crash, the generation of
bilingual MRO task cards must be error-free. Therefore, building a bilingual MRO task
cards generator with language translation is highly demanded.
Enigma [4] once built an MRO task cards generator system called InService Task
Cards Generator to increase aviation MRO maintenance productivity and compliance.
It is a web-based application that automates the creation of serial number or tail
number-speciﬁc task cards required for performing heavy, shop and line maintenance.
However, It doesn’t support bilingual MRO task cards generation, where language
translation is desirable.
This section demonstrates a bilingual (English-Chinese) MRO task cards gener-
ation prototype system called TaskCardGeneratore2c, aiming to save human users’s
repetitive time-consuming efforts on bilingual task cards translation and preparation,
thus improving working efﬁciency and quality of MRO services. So far, there is no such
kind of tool in the literature.
16.2.1 TaskCardGeneratore2c Architecture
Given the work content input by the user, TaskCardGeneratore2c examines the exist-
ence of similar bilingual aviation MRO task cards in the local repository, and returns
Authenticated
:30 PM

414
16 Context-Aware Management of Bilingual Aviation MRO Task Cards
Critical Task
Job Card
General airplane mechanic
LEFT ENGINE
A/C Type
A/C Tail No.
Maint Station
Work Order
WorkOrder Date
Print Date
JC SEQ NO
Written/Revised
By
NN-YY-DD
Reviewed/Approved By
NN-YY-DD
MRS No. MRS
XY-11-001
Mech
Insp
1 of n
Page
Aircraft Affectivity
Work Content
Parts & Material
Tool & Equipment
JC Title
411
Zone
N/A
N/A
Station
HRS
Stringe
HRS
6000
5000
Repeat
XYZ 0101010
Man Hours
Labor
Elapsed
Revision
Version
R1
ManufacturerCard
XYZ-010-01-01
Access Panel
413 414
Replace the left engine fuel filter
Related Card
NN-YY-D
N/A
D
Interval
Figure 16.1: A sample bilingual MRO task card.
Authenticated
:30 PM

16.2 Generating Bilingual MRO Task Cards (TaskCardGeneratore2c)
415
Translation Memory
Management
Translation
Memory
Bilingual /
Monolingual
Task Cards
Managing 
Translation Memory
Extracting & Indexing
Task Cards
Searching Bilingual
Task Cards
Assisting Requests
Feedback
Translating Monolingual
Task Cards
Bilingual Task Cards
Generation
User Interaction
MRO User
Task Card
Request
Generated
Task Card
Figure 16.2: Architecture of TaskCardGeneratoree2c.
the one to the user directly if it exists, or otherwise, does the English-to-Chinese trans-
lation over the corresponding monolingual English task card. The translated bilingual
task card will be sent back to the user for ﬁnal proof.
Due to the poor accuracy of machine translation [8], TaskCardGeneratore2c ad-
opts the translation memory (TM) technique for language translation [11, 13, 18].
The basic idea of TM is to do the translation based on existing translated segments
(which can be sentences or sentence-like units) stored in a database. Ten repres-
entation standards (TMX, TBX, UTX, SRX, GMX, OLIF, XLIFF, TransWS, XML:tm,
PO) [18] are used for TM. Among them, TMX standard [17] has been regarded
by the translation community as the best way to store and exchange translation
memories.
Figure 16.2 shows the architecture of our bilingual MRO task cards generator. It
consists of three components, namely, TM management, bilingual task cards genera-
tion, and user interaction.
16.2.2 TaskCardGeneratore2c Implementation
16.2.2.1 TM Management
The ﬁrst TM management component prepares for bilingual MRO task cards gen-
eration. It parses, extracts, and indexes structured information from the existing
bilingual/monolingual task cards, and puts the translated sentences from bilingual
task cards into the TM. It contains two modules.
(1) Extracting and Indexing Bilingual Task Cards
This module is responsible for parsing and extracting ﬁne-grained information from
bilingual task card word documents with the help of a bilingual aviation dictionary.
A task card document contains several items like job card number, A/C type, service
interval, critical task, general airline mechanic, work content (in both Chinese and
English or in either Chinese or English), labor hours, etc. Each item is extracted as a
database attribute and stored into a MySQL database. Each task card corresponds to
a relation tuple in the database. We also build indexes for all different attributes. For
example, Table 16.1 lists two different items being extracted. We take “Interval
”
Authenticated
:30 PM

416
16 Context-Aware Management of Bilingual Aviation MRO Task Cards
Table 16.1: Two bilingual structured fragments in an MRO task card.
Interval
JC Title
3000 HRS
Replace the left engine fuel ﬁlter
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE tmx SYSTEM "tmx11.dtd">
<tmx version="1.1">
<header tool="generator" version="1.0" segtype="sentence">
</header>
<body>
 <tu>
<tuv lang="EN-GB">
<seg> 29-11-00-860-807 Main Hydraulic System and Research 
Depressurization </seg>
</tuv>
      <tuv lang="ZH-CN" changedate="20111121T130154Z">
        <seg> "
"  </seg>
      </tuv>
    </tu>
  </body>
</tmx>
Figure 16.3: A translation memory eXchange example.
and “
” as two table attributes with “6000 HRS” and “Replace the left engine
fuel ﬁlter
” as their attribute values.
(2) Managing TM
This module stores all the translated bilingual sentences into an XML-formatted TMX
ﬁle for the TM. Figure 16.3 shows a fragment of the TM.
In bilingual English-Chinese task cards, each English sentence is followed by a
corresponding Chinese sentence. We put them into a sentence pair (English-Sentence,
Chinese-Sentence). This pair is called a tmx unit. We put the tmx unit into the
TMXUnitsSet to ﬁlter the same units. All the tmx units are kept in an xml ﬁle according
to the TMX format standard. The following shows a way of creating a TMX ﬁle.
HashSet <EnglishEntry, ChineseEntry> -> TmxUnitsSet;
foreach TmxUnits {
TmxUnitsSet.put(englishEntry,chineseEntry);
}
TMXWriter ~writer=writer.write(TmxUnitsSet);
Authenticated
:30 PM

16.2 Generating Bilingual MRO Task Cards (TaskCardGeneratore2c)
417
16.2.2.2 Bilingual Task Cards Generation
The bilingual task card generation component works together with the former TM man-
agement component. Upon a user’s task card generation request formulated using
task card’s work content, two cases exist.
Case 1. The requested bilingual MRO task card already exists in the task card
repository. Then the requested task card will be delivered to the user right away.
Case 2. The requested bilingual MRO task card does not exist in the task card
repository. It is assumed that a monolingual MRO task card in English is available.
Then the translation work from English to Chinese is incorporated.
The above two cases are handled by the search and translation module,
respectively.
16.2.2.3 User Interaction
The user interaction component facilitates users’ easy and effective use of the system.
After a user logins in the system (Figure 16.4(a)), the user can either directly enter
the work content of the requested task card through the interface (Figure 16.4(b)) or
navigate among different categories of work contents (as shown in Figure 16.5) for
click. The user interaction component contains two modules.
1) Assisting Requests module guides users to easily formulate their task card
generation requests via work content by navigation.
2) Feedback module presents the generated result to the user. Users may an-
notate or revise any inappropriate translation parts, and return to the system. The
module then propagates the newly derived bilingual task card to the ﬁrst component
to incrementally maintain the TM and task card repository.
16.2.2.4 Language Translation
Bilingual (English-to-Chinese) translation is an essential part of TaskCardGeneratore2c
system. It translates an English MRO task card into Chinese on a sentence basis.
For each sentence in the requested English task card, the system searches the TM
for the corresponding Chinese translation. Since the one to be translated may not exist
(a)
(b)
Figure 16.4: User login and request to generate a task card in TaskCardGeneratore2c: (a) User Login;
(b) User Request.
Authenticated
:30 PM

418
16 Context-Aware Management of Bilingual Aviation MRO Task Cards
Figure 16.5: Request assistant for task card generation.
4
Open this circuit breaker and install safety tag:
    a) A7, ELEV PCU LIB, LOB (On the left power supply assembly (PSA-L)
in the E1-6 rack
A7, ELEVPCULIB, LOB (
PSA-L),  E1-6
[matching 90%]
b) A7, ELEV PCU LIB, ROB (On the right power supply assembly (PSA-R)
in the E5-1 rack
A7, ELEVPCULIB, ROB (
PSA-R), E5-1
[matching 90%]
    c) A7, ELEV PCU LIB, COB (On the center power supply assembly (PSA-C)
in the E2-6 rack
A7, ELEVPCULIB, COB (
PSA-C), E2-6
[matching 90%]
5
Operate the manual bypass values on the two elevator or PCUs.
PCU
           a)     Install the elevator PCU bypass pin, SPL-1708, on the manual bypass
                      values (Figure 301 or Figure 302). 
PCU
301
302
[matching 90%]
6
Push the trailing edge of the elevator up.
  Note: The elevator will only move slowly because of the resistance in
         the PCUs.
PCU
7
Install the rudder and elevator power control unit lock, SPL-1712, on
   the elevator or PCUs.
PCU
PCU
8
Remove the elevator PCU bypass pin, SPL-1708, from the manual bypass 
values.
PCU
Figure 16.6: A fragment of bilingual translation.
Authenticated
:30 PM

16.3 Searching Bilingual MRO Task Cards (TaskCardFinder)
419
in the TM, in other words, there is no corresponding Chinese translation, the system
ﬁnds from among the existing English sentences the closest English sentence based
on their matching degree.
MatchDegree(se, sc) = max(|se.len – sc.len|) – ld
max(|se.len – sc.len|)
∗100
where se is the existing English sentence, sc is the English sentence to be translated
into Chinese, and ld is the Levenshtein distance [16], which is deﬁned as the minimum
number of edits needed to transform one string into the other. It measures the amount
of difference between two sequences. For example, when the system meets “right
power supply assembly”, there is a tmx unit (left power supply assembly, “
”).
Thus, MatchDegree(“right power supply assembly”, “
”) = 85.
The translated Chinese sentence, together with its matching degree (reﬂecting
the translation conﬁdence), will then be added in red color to the new bilingual task
card, and presented to the user for proof and possible revision. Figure 16.6 shows a
translation fragment of a bilingual MRO task card.
16.3 Searching Bilingual MRO Task Cards
(TaskCardFinder)
Compared to conventional internet search engines, the complexity of enterprise in-
formation space raises a number of challenges to enterprise search engines in the
nature of unstructured search contents, task-relevance, dynamic result presentation,
and multiple languages. This section shows how the above challenges are tackled in
a context-aware aviation-oriented enterprise search engine for English-Chinese bilin-
gual MRO task cards, called TaskCardFinder. TaskCardFinder is expected to enable
MRO service planners and technicians to quickly ﬁnd out a list of bilingual MRO task
cards which are relevant to a speciﬁc service order/request coming from airlines.
16.3.1 Existing Search Engines
16.3.1.1 General-Purpose Search Engines
The ﬁrst search engine in the world is called archie [15], a tool used for searching web
information. From then on, search engine techniques have a sharp development. Cur-
rently, search engines can be mainly classiﬁed into ﬁve types, namely, crawler-based
(traditional, common) engines, directories engines, meta search engines, vertical
search engines, and pay per click search engines. Most of the commercial engines
nowadays are based on crawler search. They collect information from websites, in-
dex the obtained information, and provide search interfaces to search users, such as
the most popular search engine Google, Yahoo, Chinese search engine Baidu, Bing,
Authenticated
:30 PM

420
16 Context-Aware Management of Bilingual Aviation MRO Task Cards
Ask, and AOL. Directories engines are not real search engines in a strict way, and
they only just classify different websites in hierarchical levels by humans, such as
Google directories or Yahoo directories. Meta search engines [9] present results which
are from several search engines. Vertical search engines focus on a special ﬁeld, such
as video search, shopping search, and so on. Pay per click search engines are used for
advertising products and companies, namely advertise search engines.
In addition to the above popular search engines, more advanced search tech-
niques are investigated. Cohen et al. [2] presented a semantic search engine for XML.
It has a customized simple query language, and its return results, which are ranked
by both the degree of the semantic relationship and the relevance of the keywords,
satisfy the users’ query requests. Bajracharya et al. [1] depicted an engine used to re-
trieve source codes, enabling users to search contents and the structural properties
and relations from programming languages. Frankel et al. and Kahn and Thao [5, 10]
provided a search engine over images. Cole [3] illustrated a desktop search engine
over documents, e-mails, ﬁles, and programs on local personal computers. Hawking,
and Mukherjee and Mao [7, 12] indicated importance of enterprise searches and the
challenges in these search engines.
16.3.1.2 Enterprise Search Engines
Enterprise search is an essential part of business intelligence technologies [14].
Effective and efﬁcient search over enterprise’s massive data sources has become in-
creasingly critical for today’s enterprise information workers in carrying out their
duties. However, the complexity of enterprise information space raises a number of
challenges to enterprise search engines. Hawking [7] and Mukherjee and Mao [12] dis-
cussed the differences between enterprise search engines and conventional internet
search engines in the nature of content, user behavior, and economic motivations,
which could be summarized as follows.
–
In an enterprise, a majority of information to be searched is unstructured and
possibly in multi-languages.
–
Search task is highly task-relevant, where search context (like user role, activity,
company’s regulation, etc.) shall be taken into account for personalized search.
–
Domain-speciﬁc guided navigation search and reﬁnement are desirable.
–
Many times users may know and have previously seen the wanted information,
where searching is recall-based.
–
The search result may be small containing the right information rather than the
best matching one.
–
Result presentation shall include summary, category, and aggregate information
to enhance the usability of the search result.
These challenges have led to a formidable problem but also mean enormous potential
beneﬁt.
Authenticated
:30 PM

16.3 Searching Bilingual MRO Task Cards (TaskCardFinder)
421
16.3.2 TaskCardFinder Functionalities
TaskCardFinder intends to empower MRO service planners and technicians with
the right information at the right time under the right task orders with a high
degree of task-relevancy coming from airline customers. It shall provide several
context-awareness functionalities, including context-aware preference search and re-
commendation, recall-based search by context, navigational, suggested, and analysis-
oriented search, bilingual (English and Chinese) search, dynamic result presentation.
The major functionalities of TaskCardFinder include:
–
Keyword and structured MRO bilingual task cards search with (1) bilingual
support; (2) topic-speciﬁc navigation assistance; (3) keywords suggestion; (4)
context-aware preferences support;
–
Context-aware task cards recommendation;
–
Search results are a ranked list of tasks cards, allowing online structured view and
document preview;
–
Recall-based re-search by time by maintaining user’s search history and task card
view history.
16.3.3 TaskCardFinder Architecture
Figure 16.7 shows the framework of TaskCardFinder. It consists of three modules.
(1) Storage of Task Cards prepares for searching by parsing, extraction, index-
ing, and storing task cards into a structured database. Some analysis results are also
computed and stored in the database.
(2) Search over Task Cards is responsible for performing users’ keyword-based
or structured search requests, taking access context into account. Recall-based search
based on users’ previous search and viewing histories is also allowed.
Bilingual
Task Cards
XML Mapping Files
Context-Aware Search
& Recommend
Technicians /
Planners
context
Aviation Ontology
Result Presentation
Search Suggestion
Navigation Support
Dictionary
Management
Recall Search
by Context
Context
Management
Context-Aware
Preference &
Search History
Task Cards
(un)structured,
aggregated)
Analysis
Index
Parse & Extract
Storage
Search
User Interaction
Bilingual Dictionary
Figure 16.7: Architecture of TaskCardFinder.
Authenticated
:30 PM

422
16 Context-Aware Management of Bilingual Aviation MRO Task Cards
(3) User Interaction facilitates users’ easy and simple interaction with the system
by means of navigational search, suggested search, bilingual language support, and
multi-viewed result presentation.
16.3.4 TaskCardFinder Implementation
16.3.4.1 Extraction, Segmentation, and Indexing of MRO Task Cards
MRO task cards are word documents in both English and Chinese. Differently-
structured MRO task cards are used for/by different airline companies. We use XML
to resolve the heterogeneity problem. Every type of task card corresponds to an XML
mapping ﬁle, characterizing content position, storage property, index strategy (e.g.,
term vector, analysis and storage method), and other item attributes of the task
cards. We extract each item (in paragraphs) from a task card according to the XML
mapping ﬁle, and store it as a table attribute in the structured task card database.
For efﬁcient retrieval, we adopt a Chinese word segmentation open tool ICTCLAS
(http://www.ictclas.org/index.html), which segments Chinese words based on a Hid-
den Markov approach. Fine-grained index for all attributes’ values in accordance with
the indexing strategies provided by the XML mapping ﬁle can then be built based on
Apache Lucene (http://lucene.apache.org).
16.3.4.2 Navigational Keyword or Structured Search
After a user logins in the TaskCardFinder system, s/he can perform either keyword
or structured search over bilingual task cards through the main search interface
(Figure 16.8).
For easy and handy interaction, guided search according to different MRO topics
(e.g., aircraft type, main fault, work content, etc.) is provided to allow users to navigate
to their target search domains.
Figure 16.8: Keyword or structured task cards search interface.
Authenticated
:30 PM

16.3 Searching Bilingual MRO Task Cards (TaskCardFinder)
423
Figure 16.9: Statistics about the main fault task cards.
Under each topic, some statistic and aggregation results (like total number of relevant
task cards, total and average man hours needed for the corresponding MRO task) are
provided for cost estimation and schedules planning. Figure 16.9 shows the number
of involved task cards in the main fault domain.
16.3.4.3 Result Presentation
The search result of TaskCardFinder is a ranked list of bilingual tasks cards, allowing
structured and online preview. The former helps the user to quickly judge if a task card
is what (s)he is really looking for, while the latter allows the user to online preview the
document. The user can also download interested task cards to his/her local disk for
further actions (Figure 16.10).
16.3.4.4 Keywords Suggestion
Relevant keywords are also suggested based on user’s search keywords and the avi-
ation MRO ontology for search reﬁnement. Figure 16.11(a) shows part of the MRO
ontology concept tree. The system ﬁrst locates the input keyword in the tree, and then
decides the association direction such as upward to recommend some high-level tech-
nical terms, downward for a few detailed ones, or just at the same level, as shown
in Figure 16.11(b). In case multiple keywords exist in the user’s request, the system
Authenticated
:30 PM

424
16 Context-Aware Management of Bilingual Aviation MRO Task Cards
Figure 16.10: Task cards search result in TaskCardFinder.
Tolls_and_Equip
ment
extension_bar
vhf_vor_system
L_type_wrench
Passenger_Addre
ss_System
multi-functional
_system_module
Part_and_Material
AC_TYPE
Thing
ratchet_wheel
cabin_air_condi
tioning_distrib...
Work_Content
(a)
(b)
Current keyword node
Association direction
same level
Upward
downward
Figure 16.11: MRO ontology and keyword association.
ﬁrst constructs a minimal query tree including all these keywords, and then suggests
keywords in the tree that don’t show up in the user’s query. Figure 16.12 shows an
example of keywords suggestion.
Authenticated
:30 PM

16.3 Searching Bilingual MRO Task Cards (TaskCardFinder)
425
Figure 16.12: Keywords suggestion according to user’s search keyword jack.
16.3.4.5 Bilingual Support
To support bilingual search over bilingual MRO task card documents, a good
Chinese-English dictionary/lexicon to facilitate language translation is needed.
TaskCardFinder is equipped with a bilingual aviation dictionary containing a
total of 153,627 simple word and phrase translation pairs, and is incrementally pop-
ulated by more complicated technical terms through the online translation from the
Internet. For more domain-speciﬁc words whose corresponding translation is not
available in the dictionary, the system will seek help from the internet. The Blue
Sky Flight Translation Corporation provides an online civil aviation domain diction-
ary (http://air.cidian.cc/). By simulating the http translation request, the translation
result page can be returned and then analyzed by a regular expression to acquire
the exact translation. This online translation result will also be populated into the
local dictionary for later translation. Apart from this, the system is designed to have a
bilingual version for users in different languages.
In the textual summary of each result task card, keywords or keywords after
English-to-Chinese or Chinese-to-English translation that match the search request
are highlighted. For instance, in Figure 16.10, both search keywords (cabin reﬁtment)
and correspondences in Chinese (
) are highlighted.
16.3.4.6 Context-Aware Preference Search and Recommendation
Awareness of users’ search context could enhance the relevancy of search results
remarkably. TaskCardFinder considers ﬁve aspects of aviation context: (1) search user-
centric; (2) aviation service company-centric; (3) airline customer-centric; (4) aviation
service-centric; and (5) environment-centric. An aviation service company provides
Authenticated
:30 PM

426
16 Context-Aware Management of Bilingual Aviation MRO Task Cards
Class
AviationCompany
domain
ObjectProperty
range
range
range
ObjectProperty
Class
Environment
domain
ObjectProperty
existIn
range
domain
involvedIn
ObjectProperty
domain ObjectProperty
use
range
Class
AirlineCustomer
domain
AviationService
Class
provide
Class
SearchUser
workFor
Figure 16.13: Core aviation context in TaskCardFinder.
Class
Class
Manager
Class
Class
Class
Engineer
GroupLeader
Inspector
SysUser
range
ObjectProperty
hasRole
domain
Class
Role
range
ObjectProperty
hasControlRight
ControlRight
domain
ObjectProperty
hasRolePreference
domain
Class
Class
RoleReference
subClassof
Figure 16.14: Search user-centric context in TaskCardFinder.
MRO services to different international airline customers. Search users, working at the
aviation service company, play different roles in services, running in different phys-
ical and social environments. Figures 16.13 and 16.14 show the TaskCardFinder core
aviation ontology and search user relevant context ontology, respectively.
Authenticated
:30 PM

16.3 Searching Bilingual MRO Task Cards (TaskCardFinder)
427
Figure 16.15: Management of context-aware preferences.
To tailor to different search context, context-aware preferences are employed to re-
ﬂect users’ different information preferences under different context. A context-aware
search preference can be denoted as a tuple (Context,Preference), where con-
text and preference are uniformly described in DL (Description Logics) concept
expressions. For example, a context-aware search preference like “if the search user
belongs to the engine team in some MRO services, preferably the work content of
the service involved in the returned MRO task cards is related to engine” can be
expressed as:
Context:
{user} ⊓Engineer(user) ⊓∃involvedIn.{AviationService}
Preference:
AviationService ⊓∃hasWorkContent.{engine}
Both context and preferences are represented by description logic and organized by
the ontology technique. Through the interface of Figure 16.15, one can manage and
maintain (add, rename, update, and delete) the aviation context ontology and context-
aware preference rules, respectively.
Once a user logins in the TaskCardFinder system, the context information of the
user such as his identity will be sent to the context service which is responsible for se-
lecting the corresponding context-aware searching preferences before submitting the
search request to the search module. The search adapter augments the original request
with the preference provided by the context service into the ﬁnal search request, and
submits it to the search service. The context-aware search engine returns the result
Authenticated
:30 PM

428
16 Context-Aware Management of Bilingual Aviation MRO Task Cards
list sorted by the relevancy, taking user’s preference into consideration. As a result,
for different user, the order of the search result under the same keyword is different.
Also, some recommended task cards could be delivered once a user logins in the
system. This can be easily achieved by means of context-aware preferences.
16.3.4.7 Recall-Based Search by Context
Recall-based search (that is, re-search of what have ever been encountered before) is
a common activity for enterprise search engines, as the users are normally involved
in regular and repetitive duties and need to consult similar task cards frequently.
Psychological studies show that context under which information was accessed in
the past can serve as a powerful cue for information recall, as it is always easier
to remember than detailed information content itself. Complementary to keyword-
based/structured search, TaskCardFinder employs user’s historic access context,
including search keywords, viewed documents, and access time or period for re-
ﬁnding purpose. Figure 16.16 shows the recall-based search interface, including a
re-search result organized in time.
Note that recall-based search, preference search, and recommendation are only
possible for login users whose preferences and search histories are maintained by
TaskCardFinder.
Figure 16.16: Recall-based search in TaskCardFinder.
Authenticated
:30 PM

16.4 User Study
429
16.4 User Study
A ﬁeld study was conducted at an international aviation service company, where 12
staffs from engineering, operation, quality, continuous improvement, and R&D de-
partments participated. Positive feedbacks were received about the bilingual task
cards generator and task cards ﬁnder, as well as some constructive comments for
further improvement.
16.4.1 On TaskCardGeneratore2c
(1) The bilingual language still remains as a big difﬁculty for the company staff.
(2) Assisting the bilingual MRO task card generation is regarded as a very urgent IT
support that is demanded in the daily running of the MRO services. Currently, the
translation from English task cards to Chinese ones is still done manually by ex-
perienced engineers at the company. It once took an MRO engineer half a month to
translate a 600-paged English MRO task card into the bilingual (English-Chinese)
MRO task card. With TaskCardGeneratore2c, such repetitive, time-consuming, and
even error-prone manpower could be saved. The efﬁciency and quality of MRO services
could thus be improved.
(3) Keeping different versions of bilingual MRO task cards is necessary and signiﬁcant
and must be elegantly taken care of by the task cards generator.
16.4.2 On TaskCardFinder
(1) All participants recognize the usefulness of the system in enterprise information
automation, and believe that such a system can help improve their working efﬁ-
ciency in MRO cost estimation and analysis, MRO scheduling and planning, new card
generation.
(2) Context-aware preference search and recommendation is very desirable, as differ-
ent airline customers usually have different MRO demands in terms of work content,
timeline, and cost. For example, the maintenance request of a aircraft whose air route
is coastal is different from the one ﬂying on the highland. Users (like engineers) also
play different roles in MRO services and therefore have different search focus.
(3) Version management is signiﬁcant in MRO business and must be elegantly taken
care by the search engine. Fourth, the bilingual language remains as a difﬁculty for the
company staff. The system is expected to provide more assistance in bilingual cards’
generation and reviewing.
Authenticated
:30 PM

430
16 Context-Aware Management of Bilingual Aviation MRO Task Cards
16.5 Recapitulation
This chapter ﬁrst reported the design, implementation, and user evaluation of a
bilingual (English-Chinese) MRO (Maintenance, Repair, and Overhaul) task cards gen-
erator prototype system called TaskCardGeneratore2c for aviation. The framework and
the TM-based English-to-Chinese translation mechanism were detailed. A user study
conducted at an international aviation service company was also reported. After that,
an aviation-oriented enterprise search engine for a collection of bilingual MRO task
cards called TaskCardFinder was then presented. TaskCardFinder demonstrates sev-
eral novel context-aware features speciﬁcally designed for aviation industry, including
topic-speciﬁc search assistance, relevant keywords suggestion, context-aware prefer-
ences support, recall-based search by time according to user’s search history and/or
task card viewing history, and MRO-service-relevant task cards recommendation. In
addition to that, TaskCardFinder is also a bilingual (Chinese and English) search en-
gine. It allows users to use either Chinese or English or the combination of both to
search relevant technical data in both English and Chinese.
User studies show TaskCardGeneratore2c and TaskCardFinder improve the work-
ing efﬁciency and quality of MRO services. Three directions related to ambient smart
and context awareness could be explored in the further work. First, version manage-
ment of bilingual MRO task cards will be incorporated in the bilingual task cards
generator. Second, besides recall-based re-search by time, a wider scope of context
(like user’s activity, task, etc.) could be used as search keywords. Third, a more intel-
ligent question-answer functionality to be offered by the search engine is envisioned
in the future. To achieve this, construction of a task card warehouse and associated
OLAP (Online Analytical Processing) techniques need to be explored.
Literature
[1]
S. Bajracharya, T. Ngo, E. Linstead, Y. Dou, P. Rigor, P. Baldi, and C. Lopes. Sourcerer: A search
engine for open source code supporting structure-based search. In Proc. of OOPSLA, 2006.
[2]
S. Cohen, J. Mamou, Y. Kanza, and Y. Sagiv. Xsearch: A semantic search engine for xml. In Proc.
of VLDB, pages 45–56, 2003.
[3]
B. Cole. Search engines tackle the desktop. Computer, 38(3):14–17, 2005.
[4]
Enigma Company. Enigma inservice job card generator. In http://www. enigma.com/, 2010.
[5]
C. Frankel, M. J. Swain, and V. Athitsos. WebSeer: An image search engine for the World Wide
Web. Technical Report TR-96-14, University of Chicago, 1996.
[6]
P. Gupta, S. Dewangan, and S. Gade. Time to enable mobility in aviation MRO software
solutions. In Proc. of Info. System, 2009.
[7]
D. Hawking. Challenges in enterprise search. In Proc. of ADC, pages 15–24, 2004.
[8]
J. Hutchins and H. L. Somers. An introduction to machine translation. In Academic Press, 1992.
[9]
Infospace. http://www.infospace.com/.
[10]
C. E. Kahn and C. Thao. GoldMiner: A radiology image search engine. American Journal of
Roentgenology, 188:1475–1478, 2007.
Authenticated
:30 PM

Literature
431
[11]
E. Lagoudak. Translation memories survey: users’ perceptions around TM use. Translating and
the Computer, 28:1–29, 2006.
[12]
R. Mukherjee and J. Mao. Enterprise search: Tough Stuff. Queue, 2:36–46, 2004.
[13]
S. Pado, D. Cer, M. Galley, D. Jurafsky, and C. D. Manning. Measuring machine translation
quality as semantic equivalence: A metric based on entailment features. Machine Translation,
23:181–193, 2009.
[14]
K. U. Schmidt, D. Oberle, and K. Deissner. Taking enterprise search to the next level.
http://ceur-ws.org/Vol-401/iswc2008pd_submission_22.pdf, 2009.
[15]
Wiki. Internet history – search engines. http://en.wikipedia.org/wiki/Web_ search_engine,
2017.
[16]
WiKi. Levenshtein distance. http://en.wikipedia.org/wiki/Levenshtein_ distance, 2017.
[17]
WiKi. Translation memory exchange standard. http://en.wikipedia.org/
wiki/TranslationMemoryeXchange, 2017.
[18]
Wikipedia. Translation memory. http://en.wikipedia.org/wiki/Translation_ memory, 2017.
Authenticated
:30 PM

17 FireVGuide: A Context-Aware
Fire Victims Guide
Abstract: While various ambient computing and intelligence techniques have been
used to deliver tour guide or museum guide, this chapter addresses their potential
supports to mission-critical scenarios such as ﬁreﬁghting. After a brief review of some
recently developed ubiquitous ﬁreﬁghting techniques, the design and implementation
of an ambient intelligent Fire Victims response Guide system (FireVGuide) are then
reported. By sensing the physical environment and occupants in a ﬁre building, the
system suggests the safest and fastest route along which the building occupants could
evacuate; and when escaping from the building is not possible, the system tries to
calm down and inform the trapped ones an action-list. The feasibility of the system
through both user studies and experiments are evaluated, which show that context-
awareness in such a ﬁre response guide can help speedup the egress time of building
occupants. Lessons learned in designing such a mission critical ambient intelligent
application are also discussed.
17.1 Motivation
In the past, ﬁre hazards have caused many injuries and casualties. According to the
statistics done by Ref. [4], each year there are around 7–8 million ﬁre disasters all
over the world, which killed 70–80 thousand people and injured 500–800 thousand
more. In 2008 in China, the number of ﬁre disasters was 1,33,000 (excluding forest,
grassland, army, and underground mine ﬁre). In total, 1,385 people were killed and
684 people were injured. The direct loss estimate was about 1.5 billion RMB Yuans [7].
17.1.1 Two Real Fire Disasters
In ﬁre disasters, many people lost lives due to bad judgment. Poor decisions are
likely made in life-critical urgent situations. Let us take a close look at two recent ﬁre
disasters. One happened in a college dormitory, and the other in a club.
Case 1
On 20 September 2008, a ﬁre occurred at Wuwang Club at Long Gang Street,
Shenzhen, China. According to the police, the ﬁre originated from the ﬁre-
works being set off on the stage. The disaster killed 44 people with 88 people
being injured [26]. At the beginning of the ﬁre, people in the club mistook
it for a show. The ﬁre quickly led to lots of smoke, and many people were
smothered. As the customers were not familiar with the back door where
the emergency exit is located, hundreds of people rushed to the front door
through which they entered, causing many people being stumbled to the
ground. In contrast, as the employees of the club knew the emergency exit,
DOI 10.1515/9783110556674-017
Authenticated
:30 PM

17.1 Motivation
433
the majority of them succeeded in escaping from the ﬁre via this exit gate.
The death rate for the total 150 employees is far less than that for the club
customers.
Case 2
On 14 November 2008, at the Shanghai Business College in China, a dorm-
itory room at the sixth ﬂoor caught ﬁre at 6:10am in the morning. The ﬁre
expanded very quickly and gave off a lot of smoke. The door was incidentally
closed. The four trapped girls fell back to try to keep away from the ﬁre. Even-
tually, the ﬁre ﬂame spread to the balcony. The girls jumped from the balcony
before the ﬁreﬁghters arrived. They may have thought it was safer to do so,
yet none of them could survive. The police received the ﬁre report at 6:12am.
The ﬁreﬁghters soon arrived, and the ﬁre was put out at 6:30am [19].
17.1.2 Reﬂection of the Real Fire Disasters
A critical lesson we learn from the historic tragedies is that on-site occupants’ assist-
ance in an unexpected and urgent ﬁre situation is very much desirable. While lots of
great efforts have been made to deliver tour guide or museum guide, we ask ourselves:
“can we design a mission critical ﬁre response guide to calm and assist on-site victims
in a ﬁre building to survive?”
If there had been such an assistance system, which could point nervous and
chaotic people to the right emergency exits along with the proper escape path (in the
above case 1); or which could adaptively advise the four trapped girls to use the sheets
in the dormitory and tie them to their bodies and climb down to the lower ﬂoors, e.g.
the ﬁfth ﬂoor, rather than directly jump down to the ground (in the above case 2), their
lives might have been saved.
17.1.3 Necessity of FireVGuide
According to the physiological, psychological, and social behavioral studies, at times
of urgency, people tend to make unsound situational judgments and take wrong ac-
tions and, at times, even lose consciousness. Shouting loudly, losing conﬁdence,
disregarding exhortations, running in all directions, etc. are typical behaviors of
people confronted with emergency situations. Even worse, poisonous smoke from
the ﬁre, e.g., CO, HCL, HCN, etc., can easily cause physiological disorders, such
as impairment in the sense of smell, breathing difﬁculty, blurred vision, damage
to the visceral and cranial nerve system, etc., which soon lead to mental confu-
sion, behavior disorders, dizziness, coma, and suffocation (Song, 2002). Even worse,
common human behavior makes the problem particularly challenging. There is no
doubt that prompting ambient context-aware computing support to occupants is in
demand.
Authenticated
:30 PM

434
17 FireVGuide: A Context-Aware Fire Victims Guide
17.1.4 Assumptions of FireVGuide
The aim of this study is to investigate ambient smart context-aware techniques for as-
sisting ﬁre victims to react properly when confronted with an emergent ﬁre disaster
from a technical perspective. Such an assisting system is not meant to substitute exist-
ing ﬁreﬁghting facilities and systems, but as a complementary user-centric technology
to calm and help some people in a ﬁre building. The system is mainly used before
ﬁreﬁghters come, and works under the following assumptions.
–
The layout of the ﬁre building is available beforehand.
–
Fire locations can be detected by smoke and temperature sensors.
–
IDs of frequent occupants and number of occupants in each cell unit (room or
corridor) of the ﬁre building can be sensed by techniques such as RFID.
–
Occupants may or may not carry mobile phones. Mobile phone numbers of oc-
cupants are available from a database. For those carrying mobile phones, their
mobile phone numbers can either be available from a database (for frequent
occupants) or be detected, e.g., by bluetooth beacons.
17.1.5 Principles of FireVGuide
Upon the rise of a ﬁre in a building, by sensing the context of the building on ﬁre
and the occupants in the building, the FireVGuide system needs to quickly ﬁgure out
the fastest safe evacuation route or works out an action-list for trapped no-way-out
people.
FireVGuide models the ﬁre building using a graph, containing a number of ver-
tices corresponding to doors, rooms, certain points at corridors, and exits. The system
generates the escaping route guide based on referential location (vertex in the graph).
To achieve real-time response, the system precomputes all possible evacuation routes,
and stores them in a database beforehand. In case of a ﬁre, the fastest and safest
routes will be picked up immediately for differently localized occupants, taking into
account the current distribution of building occupants so as to avoid potential es-
caping bottleneck due to human’s inherent collective crow behaviors in the chaotic
situation [10, 11, 21, 26, 37, 38].
The FireVGuide system tries to evenly disperse people to valid exits. Based on
the locations of installed speakers in the building, occupants per room, and referen-
tial location, the ﬁre assistance system will announce directional guide by means of
lights and speakers without detecting the initial orientation of the people. For occu-
pants with mobile phones, a brief guide will also be delivered to their mobile phone
screens.
Authenticated
:30 PM

17.2 State-of-Art Fireﬁghting Techniques
435
17.2 State-of-Art Fireﬁghting Techniques
Emergency response to ﬁre is a topic that has been investigated for a long history.
People have built a variety of excellent modern ﬁre ﬁghting systems, which can detect
a ﬁre, sound an alarm, and start corresponding activities to extinguish the ﬁre. Such
systematic facilities react quite well as a whole against a ﬁre disaster. However, its
assistance may not always reach each individual on site. Taking the above Wuwang
club ﬁre as an example (case 1), when the ﬁre started, the ﬁre ﬁghting equipments,
e.g., water showers, inside the club did begin to work. However, the turmoil still led
to serious ﬁre casualties. In fact, insufﬁcient direction to individual occupants is a
shortcoming of many modern ﬁre ﬁghting systems. With the development of pervasive
computing technology, technical supports to both ﬁreﬁghters and on-site victims are
becoming important.
17.2.1 Supporting Fireﬁghters
Fischer and Gellersen [11] have made a good survey on indoor location and navig-
ation support products and projects for emergency responders, and concluded that
techniques of wireless sensor networks, inertial sensing, and pre-installed location
systems all have beneﬁts and drawbacks when considering emergency response re-
quirements [16, 22, 24, 35] presented a concept and implementation for supporting
tactical navigation of ﬁreﬁghters in structural ﬁres called LifeNet. It is based on a
sensor network that ﬁreﬁghters deploy on-the-ﬂy during an intervention and a wear-
able system that provides them with navigational support. Wada et al. [32] proposed
a device that can indicate the direction through the tactile sensation of the head in
an invisible ﬁre environment full of smoke. Chen et al. [6] described an application
in helping emergency ﬁrst responders in a ﬁre exercise by the local police depart-
ment and ﬁre department at Lebanon airport (NH) in the spring of 2004. Blood oxygen
saturation, as well as cardiac and respiratory states, is monitored for the ﬁrst respon-
ders, and the surrounding environment is deployed with sensors. In the system, the
ﬁre state of propagation and location of each ﬁrst responder are monitored. Besides,
the system can also report injured responders and their physiological status. Respon-
ders wear PDAs as the user interface, and the commanders can probe the history
of sensor data and issue one-shot queries for situation assessment. Landgren and
Nulden [17] analyzed the patterns of mobile interaction in the emergency response
work. Salam et al. [25] evaluated a class of strategies to support efﬁcient sensor energy
workforce management used in mission-critical wireless sensor networks. Chen et al.
[6] presented a map synchronization table strategy to support ﬁreﬁghters to cooperat-
ively make decisions in ad hoc situations. Jiang et al. [14] presented a context-aware
system called Siren to help professional ﬁreﬁghters. The system can support tacit
communication between ﬁreﬁghters and support multi-channel communication. Fire-
ﬁghters take PDAs both as a communication tool and as a wireless sensing device. One
Authenticated
:30 PM

436
17 FireVGuide: A Context-Aware Fire Victims Guide
ﬁreﬁghter can place the wireless-enabled sensors on the ﬂy when he sizes up the situ-
ation. A new design of head-mounted displays for increased indoor ﬁreﬁghting safety
and efﬁciency was also proposed in Refs. [28, 34, 36]. Luyten et al. [20] designed a
system to support ﬁre brigades with a role-based focus+context user interface on PDAs
and Tablet computers. Capote et al. [15] described an approach of using game-like
techniques to engage ﬁremen from the Paris Fire Brigade into a participatory design
process of wearable computing for emergency response. Denef [8] and Denef et al. [9]
did ﬁeld study for design of new interactive technologies for ﬁreﬁghting. Toups and
Kerne [30] also discussed developing education systems for teaching ﬁre emergency
responders.
17.2.2 Supporting Fire Victims
While the previous research greatly extends the mission critical disaster applications
of context-awareness techniques, they mainly focused on saving professionals’ lives,
such as ﬁreﬁghters or people involved in dangerous work. While we recognize that pro-
tecting professionals is very important, civilian occupants rather than professionals
are the main casualties of disasters today. Occupants are different from professionals
in some respects. For instance, we can have every professional wear certain electronic
devices, but it is more difﬁcult to have all the ordinary people wear electronic devices,
although in some places this is possible if a certain regulation is set up for wearing
such devices. Instead of posing requirements for ordinary people, we may turn to
the ubiquitous computing support and let the environment wear electronic eyes and
mouths. Capote et al. [5] presented an evacuation model of ﬁre scenarios in passenger
trains. Wang and Luh [33] further gave a probabilistic model to comprehensively char-
acterize how ﬁre propagation affects crowds in stressful conditions and in turn egress
time performance. Ferscha and Zia [10] proposed a belt-like wearable device for vibro
tactile directional guidance, called LifeBelt, to notify individuals in panic about exits.
Blohberger and Grundler [3] had a patent which can dynamically activate the escape
identiﬁcation lights according to the situation.
Research reported in this chapter builds upon the above great research results,
and study the potential use of mobile phones of building occupants in a ﬁre situation.
17.3 Solution Requirements
To deliver a context-aware ubiquitous computing support for occupants to survive in
a ﬁre, a ﬁeld study was conducted by analyzing real ﬁre disaster cases like the ones in
Shanghai and Shenzhen described in Section 1, reading ﬁreﬁghting literatures on how
to behave and save lives on a ﬁre occasion, interviewing ﬁreﬁghting administrators
and a professional ﬁreﬁghting system manufacturer. The following criteria thus serve
as design guidelines for the context-aware ﬁre response guide.
Authenticated
:30 PM

17.3 Solution Requirements
437
17.3.1 Building Structure
As ﬁre situations are complex, the type of building greatly inﬂuences the design of
the ﬁre victims guidance system greatly. As a ﬁrst step, two typical building models
are considered, where the former Shanghai College Dormitory Building and Shenzhen
Club Building are taken for reference. The ﬁrst building represents a large public room,
such as a restaurant, a supermarket, a club, etc., where customers are relatively unfa-
miliar with the building, while the second has a regular-sized small room construction
and residents are relatively familiar with the building construction. Their lego mod-
els are illustrated in Figure 17.1. The developed technologies can be extended to more
types of buildings.
Exit
Stage
Stairs
Balcony
Shanghai Business College
Dormitory Model
Shenzhen Wuwang
Club Model
Exit
Figure 17.1: The Shanghai and Shenzhen ﬁre building models.
Authenticated
:30 PM

438
17 FireVGuide: A Context-Aware Fire Victims Guide
17.3.2 Timeliness
Fire usually expands in minutes to the whole building. At public gathering places, it
takes about 7 minutes for ﬁre to spread. Only 6 minutes after the starting of a ﬁre,
the actual site temperature reaches 300–400oC. At a ﬁre scene, people tend to be ex-
tremely sensitive to environmental and thermal radical temperature and can tolerate
65oC for a very limited time. At 120oC, people can endure for 15 minutes, and less than
1 minute at 175oC. When thermal radical temperature is 12,000 W/m2, people can only
stand for a few seconds. After that, people will go into a state of coma and shock (Song,
2002). Thus, the ﬁre victims guidance system should have very high performance in
decision making.
17.3.3 Simple Interaction
Discussion with the engineers in a ﬁreﬁghting manufacture company was done about
whether to use more user-machine interaction, like voice interaction to solve some
ambiguities. The engineers commented that using complex interactions might not be
a good idea, but repeating the same instruction words several times might be helpful
so that occupants can grasp the meaning of the words.
17.3.4 Reliability
As a mission-critical application, the ﬁre victims guidance system must be reliable in
terms of both hardware and software in ﬁre emergencies. The underlying strategies
should be as much reliable as possible. For instance, correct decisions depend on
highly reliable context information. This is similar to the patient safety system de-
veloped by Ref. [1], who used short-range RFID tags rather than long-range ones in
order to get more precise context information. We once tried to incorporate several
sophisticated ﬁre expansion prediction models into the design of the ﬁre response
assistance system. One of the leading ﬁre simulation systems called Fire Dynam-
ics Simulator (FDS) was speciﬁcally tested in our experiment. However, as such a
system needs a comprehensive model of the building (including furniture, window
status, wall materials, etc.), it takes hours to compute a simulated ﬁre expansion
result. More important, solid validation is still lacking for the precision of predic-
tion in the real world, since in reality there are too many details to be modeled. In
other words, we cannot predict exactly where the ﬁre will expand in the next minute.
Therefore, the real-time sensor data was mainly used in the ﬁre victims guidance
system.
Authenticated
:30 PM

17.4 FireVGuide Architecture
439
17.4 FireVGuide Architecture
17.4.1 Hardware Deployment
The ﬁre victims guidance system works in a building, where each room has a smoke
sensor at the middle of the ceiling, and another sensor, which can be either smoke
or temperature sensor, at the entrance of the room. In addition, smoke sensors are
attached on the corridors and exits of the whole building. Both kinds of sensors are
connected to the ﬁre victims guidance system’s backend server via sensory data ac-
quisition cards. Multiple sensor data acquisition methods can be supported here.
For output channels, each room, corridor, and exit have a speaker. Emergency red
lights are also installed at each exit and along each corridor. For frequent occu-
pants whose mobile phone numbers are known and stored in a database beforehand,
corresponding ﬁre response guides will also be sent to their mobile phones via a
GPRS modem. Hardware deployment of the ﬁre victims guidance system is given in
Figure 17.2.
Smoke Sensor
......
Temperature Sensor
Backend Server
Date Acquistion Card
Audio Controller
Light Controller
GPRS Modem
Speaker
Light
Mobile Phone
Figure 17.2: Hardware setting of FireVGuide.
Authenticated
:30 PM

440
17 FireVGuide: A Context-Aware Fire Victims Guide
17.4.2 Software Architecture
Figure 17.3 plots the software architecture of the ﬁre victims assistance system. It
contains four major components, namely, context acquisition, ﬁre status judgment,
guidance generating, and guidance delivery. In response to a ﬁre, the context acquis-
ition module quickly gathers low-leveled raw sensor values, and performs context
aggregation and interpretation according to the building and ﬁre-related ontologies,
which is manipulated using the protégé API. These ontologies are used to model the
devices, building structure, and their relationships, as illustrated in Figure 17.4. Based
Fire Status
Judgment
Context
Acquisition
Building
Context
Smoke
way out
trapped
Temperature
Occupant
Infrastructure
Guidance
Generation
Assistance
Delivery
Sensing
Aggregation
Interpretation
Reasoning
Learning
Domain
Knowledge
Decision of
Speaker
Mobile
Phone
Light
Evacuation
Route
To-Do List
Figure 17.3: Software structure of FireVGuide.
Figure 17.4: FireVGuide ontology.
Authenticated
:30 PM

17.5 FireVGuide Guidance Generation
441
on derived high-level context information and domain knowledge, the ﬁre status judg-
ment decides the pattern of expansion and extinguishing of the ﬁre in the ﬁre building.
The change of the ﬁre status triggers the update of the occupants’ situations and in-
ﬂuences the most appropriate ﬁre response manner for them. There are basically two
choices for occupants: (1) ﬁnding the shortest safe way out, meanwhile ensuring occu-
pants are evenly guided to different safe exits to avoid clogging and exit bottlenecks;
or (2) taking proper self-protection actions if trapped.
17.5 FireVGuide Guidance Generation
The ﬁre victims guidance system delivers two types of guidance for two types of ﬁre
victims, i.e., ones who can escape and ones who are trapped.
17.5.1 Evacuation Route Generation
The emergency evacuation route algorithms have been already studied in Refs. [2, 23,
27, 29, 31, 33, 39]. The method reported here stands upon on these great research result
to generate the guide output. Experiences indicate the right way to escape from a ﬁre
is via emergency exits, which could be the inner or outer stairs for a building. How-
ever, the guidance using the command “Go west!” may be hard for the occupants to
accurately follow, particularly in an unfamiliar building. Also, if the system guides the
occupants to “Go to the stairs!” directly, the stairs might be too far away to see. Hence,
a nearby referential vertex approach to give the route guide is adopted. “Go left of the
elevator!” is one such guidance example.
17.5.1.1 Floor Plan Annotation
The administrator of the ﬁre victims guidance system annotates the ﬂoor plan of the
building off-line, as shown in Figure 17.5, where circles stand for ordinary vertices and
the boxes stand for referential vertices. The annotated ﬂoor plan is then converted into
a graph model by the program.
17.5.1.2 Referential Vertex
Two heuristics are employed to identify referential vertices.
(1) Crossings in the path are taken as referential vertices. This is judged by whether
the out-degree of a vertex is greater than two. For example, in Figure 17.7, A is a room,
E is its door, and B is a crossing out of the room. Taking B as the referential vertex for
E, we can go left or right at this vertex.
(2) The referential vertices can be obtained by user-deﬁned rules, e.g., all the big
screens on the wall are treated as referential vertices.
Authenticated
:30 PM

442
17 FireVGuide: A Context-Aware Fire Victims Guide
You Are Here
Stairs
601
603
602
Stairs
605
604
607
606
609
608
611
610
Figure 17.5: Converting an annotated ﬂoor plan to a
graph.
17.5.1.3 Computing the Shortest Safe Path
To point out the correct way at run-time, the ﬁre victims guidance system computes the
shortest safe path between every two vertices in the graph and stores them in advance.
In case of a ﬁre, dangerous vertices will be detected. If the ﬁre does block the shortest
path between the two vertices, the ﬁre victims assistance system will recompute the
shortest path. For example, in Figure 17.6, if we compute the route when there is no
ﬁre, the shortest route from “You are here” to the exit will be Path-1. However, when
a ﬁre occurs nearby, the stair vertex is blocked, so the shortest safe route becomes
Path-2. The classic shortest-path Dijkstra algorithm is applied to ﬁnd the shortest path
between two vertices with a time complexity of O(Num_of_Vertices_Per_Floor2). The
weight of edges connected to the unsafe vertices is increased to make the computed
path pass around unsafe vertices.
17.5.1.4 Optimization
However, using the shortest safe path algorithm directly turns out poor system scalab-
ility, making it difﬁcult to meet the real-time response requirement in large buildings.
Some further special optimizations are made in the algorithm, which greatly de-
crease the time complexity for a ﬁnal acceptable performance, as evidenced by the
experiments.
Authenticated
:30 PM

17.5 FireVGuide Guidance Generation
443
You Are Here
603
Stairs
Stairs
602
601
605
604
606
607
608
609
610
611
Path1
Path2
Figure 17.6: Dynamic route selection in FireVGuide.
(Cross Road)
A(Room)
E(Doom)
C(Left Way)
D(Right Way)
B
Figure 17.7: Referential vertex selection example in
FireVGuide.
(1) All the shortest paths for each vertex have been initiated off-line. In the run-
time period, only a subset of the paths will be recomputed online according to the
context.
(2) If a ﬁre occurs at or expands to stairs, the structural characteristics of the build-
ing are leveraged. That is, in a vertical staircase, if one location of the staircase is
blocked by the ﬁre, the ﬁre victims guidance system will not guide people to pass
down through the ﬁring stairs in the upper ﬂoors. This optimization reduces the com-
plexity from the three-dimensional graph model to an approximate two-dimensional
graph model.
Authenticated
:30 PM

444
17 FireVGuide: A Context-Aware Fire Victims Guide
17.5.1.5 Direction Announcement in a Natural Language
Another issue that needs to be tackled is to announce directions in a natural language.
Let A(xA, yA, zA) and C(xC, yC, zC) are different locations, B(xB, yB, zB) is the referential
vertex from A to C. Let vector
⃗a = B – A = (xa, ya, za) and
⃗b = C – B = (xb, yb, zb).
Pc = axby – aybx, Pi = axbx + ayby and r = √axax + ayay ⋅√bxbx + byby. Then the
direction D is decomposed into three elements (d1, d2, d3):
d1 =
{
{
{
{
{
{
{
{
{
{
{
left
Pc > 0 and | Pc
r | > %1
right
Pc < 0 and | Pc
r | > %1
center
| Pc
r | ≤%1
(17.1)
d2 =
{
{
{
{
{
{
{
{
{
{
{
front
Pi > 0 and | Pi
r | > %2
back
Pi < 0 and | Pi
r | > %2
center
| Pi
r | ≤%2
(17.2)
where %1 and %2 are the thresholds, %1 ∈[0, 1), and %2 ∈[0, 1).
d3 =
{
{
{
{
{
{
{
{
{
{
{
up
zC > zB
down
zC < zB
center
zC = zB
(17.3)
With all the methods stated above, the ﬁre victims guidance system generates guide
information in the natural language such as “In the corridor, turn right!”. An algorithm
description of a typical transformation from an evacuation path to guide speech is
shown in Figure 17.8.
In addition, in a big room like a dance hall illustrated in Figure 17.9, there may be
more than one exit, and occupants might not see all the exits. To attract the occupants
to the safe exits, speakers are added at the exits, repeating “Here is the exit!”
17.5.2 To-Do-List Generation for Trapped Occupants
The following two rules are applied to determine whether one is trapped by the ﬁre
or not.
Rule 1 The door of the room where one is situated has been blocked by the ﬁre.
Rule 2 All the paths from which one can escape out of the ﬂoor have been blocked by
the ﬁre.
Authenticated
:30 PM

17.5 FireVGuide Guidance Generation
445
Figure 17.8: A typical transformation from an evacuation path to guide speech in FireVGuide.
Situations complying with Rule 1 could be detected by either a temperature sensor
or a smoke sensor close to the door, while those complying with Rule 2 could be
detected via smoke sensors. There exist the possibility that even if one is blocked,
with wet covers she/he could dash out of the burning place. This challenges the ﬁre
victims guidance system to be smart enough to have a great insight into the surround-
ing and give the most appropriate instruction. This issue is left to further research. A
To-Do list advised by the ﬁre victims guidance system for trapped ones includes the
following:
Action 1 Tie knotted sheets to the window/balcony and your body, climb downward!
Action 2 Close the door and keep smoke out!
Action 3 Ask for help!
Action 4 Stick to the wall foot!
Figure 17.10 illustrates the ﬁre response guides sent to a victim’s mobile phone. This is
done by using a GPRS modem.
Authenticated
:30 PM

446
17 FireVGuide: A Context-Aware Fire Victims Guide
KTV
Rooms
Exit
Exit
Passage
Seats
Dance Hall
Big Screen
Corridor
KTV
Rooms
Exit
Exit
Stage
Band
Seats
Corridor
Figure 17.9: A big room example in FireVGuide..
Route picture sent to the
mobile phone
Text guide sent to the
mobile phone
Figure 17.10: Guide sent to the mobile phone of a building occupant.
17.6 Evaluation of FireVGuide
The FireVGuide prototype system was implemented using Java and C++ programming
languages. C++ code is used for sensors and speakers, which are called by the Java
Native Interface (JNI). The speech is generated by the Microsoft Text to Speech (TTS)
SDK. The smoke sensors are currently connected to the ﬁre victims guidance system
server directly by a data acquisition card. Regarding the output channels, a control
component has been implemented for multiple audio cards connected to the ﬁre vic-
tims assistance system server, which is a desktop computer of 3 GHz CPU and 2 GB
Authenticated
:30 PM

17.6 Evaluation of FireVGuide
447
RAM. Experiments were conducted to evaluate efﬁciency and effectiveness of the ﬁre
victims assistance system through user studies and experiments.
17.6.1 User Interviews
17.6.1.1 Interviewing with Domain Experts
The ﬁreﬁghting administration department of the university were ﬁrst interviewed. It
is a subordinate division of the Public Safety Ministry. They provided a brochure of ﬁre
response principles which were summarized by experienced ﬁreﬁghters and research-
ers. These principles were implemented in the ﬁre victims assistance application,
especially in the To-Do list.
Three senior experts in a professional ﬁreﬁghting system manufacturer were con-
sulted. They acknowledged the usefulness of the system, and suggested that the
system is suited for the places like hotels and dormitories. One of the experts is
the member of the ﬁreﬁghting product standardization committee of the country.
He emphasized the hazard of smoke inhalation, and suggested, if possible, guiding
people to put on smoke-proof masks. This suggestion was implemented as general
information delivered in our ﬁre victims assistance application, besides the adaptive
route or To-Do list guide.
17.6.1.2 Interviewing with Fire-Experiencing People
Two people who experienced real ﬁres were also interviewed. One of them said that
the particles in the smoke were large. The smoke burned the nose badly. When the ﬁre
started, almost all the people around the ﬁre were in a sudden confusion. Some people
even used a cotton quilt to put out the ﬁre, but the ﬁre grew even faster. A system like
ﬁre victims assistance application is in demand to help the people protect themselves.
The other person said, in the real ﬁre which he experienced, all people were in a panic,
running and colliding in the passageway chaotically. Both of them argue that people
do need a guide to reduce the collision so as to reduce the number of the injuries.
17.6.1.3 Interviewing with General Users
Two questions were presented to 52 undergraduate and graduate students in different
majors at the university in order to understand ordinary users’ attitude towards an
ambient intelligent ﬁre victims assistance application. The answers are illustrated in
Figure 17.11. 51 out of 52 people believed that the guide could increase the possibility
of survival in the Shanghai dormitory ﬁre disaster. 42 of 52 people thought that alarm
together with voice route guide is the best in guiding occupants in unfamiliar places.
ﬁve people argued that voice route guide is the best in the second question, and over
90% people selected voice route guide.
Authenticated
:30 PM

448
17 FireVGuide: A Context-Aware Fire Victims Guide
Q2: Which guide method is the 
       best in unfamiliar places?
Q1: Can the guide increase
the possibility of survival?
A(Yes)
Number of Selection
Number of Selection
B(No)
60
50
40
30
20
10
0
45
40
35
30
25
20
10
5
0
15
A(Alarm)
B(Speech without route guide)
C(Speech with route guide)
E(Alarm + Speech with route guide)
D(Alarm + Speech without route guide)
Figure 17.11: People’s attitudes toward
the ﬁre victims guidance system.
17.6.1.4 Discussion
The interviews above indirectly identiﬁed the requirement for ﬁre victims assistance
application. To obtain an on-site experience, two of the lab members participated in
a ﬁre drill in a 14-ﬂoor dormitory building organized by the ﬁreﬁghting department.
Hundreds of people took part in the drill. The experiences told us that when we were
at the very stage of deciding where to go at the door, the ﬂashlights which show
Authenticated
:30 PM

17.6 Evaluation of FireVGuide
449
the arrows and diagrams marking exit routes were not always directly in front of us.
Whichever way is selected, we were not conﬁdent. The ﬁre victims guidance applica-
tion does not mean to substitute the ﬂashlights, but it will be useful if it could calm
and help people encounter the right ﬂashlights more quickly.
17.6.2 Empirical Experience with FireVGuide
An empirical experience study with FireVGuide was conducted, where 20 subjects (in-
cluding three university staff, three administrative staff, and 14 graduate students)
participated in the experiment.
17.6.2.1 Method
Before the study, the subjects were told that one kind of emergency might happen,
and we were to see whether they could behave correctly. But we did not tell them what
would actually happen. We designed two scenarios. The ﬁrst scenario was to guide
them to a safe exit. But this safe exit was not the one through which people entered
the building. The second scenario was to simulate a situation that they are trapped by
the ﬁre outside near the door. The system guided them to close the door and wait for
rescue. After the test, the subjects were asked to ﬁll in a questionnaire.
17.6.2.2 Result
Except for one invalid questionnaire which selected more than one item for one ques-
tion, we collected 19 completed valid questionnaires. The results are tabulated in
Table 17.1 (Likert scale, 1–5 point, 5 for best and 1 for worst). Overall, the system was
perceived as useful (4.33) and had almost no adverse interference with people’s nor-
mal life (4.64). People scored high on usefulness of voice guiding (4.14) and the clarity
of short message guide on mobile phones (4.41). The reaction time of voice guiding
was ﬁne (4.11). However, the message receiving time was complained by some sub-
jects (3.81). The mobile phones are provided by us, and the delay of short messages
sent to the mobile phones was compared to the voice guide delivery.
17.6.2.3 Discussion
(1) Beneﬁts and Constraints of Using Mobile Phones
Using mobile phones are better than nothing. We want the victims to leave or take
other self-protection actions before ﬁremen come. People can adjust the position of
mobile phones by themselves to see the guide more clearly. The mobile phone can
use the public wireless communication channels which are not destroyed by the ﬁre.
However, there are some constraints with mobile phones. Firstly, not all occupants
have access to mobile phones. Secondly, a slow message receiving time was incurred
Authenticated
:30 PM

450
17 FireVGuide: A Context-Aware Fire Victims Guide
Table 17.1: Questionnaire result.
Empirical experience
Mean
Standard deviation
System usefulness
4.33
0.60
Usefulness time of voice guidance
4.14
0.78
Reaction time of voice guidance
4.11
0.93
Usefulness of short message guidance
3.92
0.84
Clarity effect of short message guidance
4.41
0.79
Reaction time of the system
3.81
0.69
Unobtrusiveness in people’s normal life
4.64
0.61
when we adopted the public mobile network to connect the system to users’ mobile
phones. A more efﬁcient method might be using the combination of both public and
local networks. Thirdly, it is still an open question that whether the mobile phone
is useful in a very crowded building, where watching mobile phone might become
difﬁcult for users. Displaying very simple guide, such as only a directional arrow on
the screen with the aid of the compass in the mobile phone, might be a good choice.
(2) Crowd, Noise, and People’s Communications
In this user study, we observed that people communicated with each other when they
were in a small crowd. Even if some people did not catch the guide, they could fol-
low those who catch the guide. The people who had mobile phones could also help
those who did not have. But the crowd also impeded the speed of evacuation. These
phenomena deserve further researches [28].
17.6.3 Effectiveness of FireVGuide
To investigate more rigorously whether context-awareness in the guide takes effect
in an emergent state, one challenge lies in designing an emergency state, because
one cannot put people into real danger. Previous researches on disaster rescue and
evacuation were often carried out in simulation environments [3, 18]. Hence, the user
study was also conducted as a drill.
The famous bonus method successfully used by Helen Muir’s group at Cranﬁeld
in the domain of aircraft evacuations [18] was followed. In their pioneering aircraft
egress experiments, bonus was used to simulate an emergent state among the sub-
jects. Another challenge is that, as in reality, people’s attention is not always on the
emergency. To make the subjects’ attentions not fully on ﬁre alarms, we let the subjects
focus on some work in the experiment, i.e., doing some mathematical exercises.
17.6.3.1 Method
24 subjects, 8 female, and 16 male of age 19–48 participated in the experiment. They
were recruited through an open internet advertisement in the city and were arranged
Authenticated
:30 PM

17.6 Evaluation of FireVGuide
451
Room 2
Room 1
35.2m
50.7m
2.7m
Corridor
positions for
two subjects
positions for
two subjects
Stair
Stair
Figure 17.12: Experiment setting.
into six groups randomly, with four subjects per group. Each group was allocated to
two rooms, with two subjects per room. Each group attended eight tests. Figure 17.12
plots the test setting. To eliminate the effect of position difference, the four subjects
changed their positions clockwise in every test in turn. As soon as a voice alarm arose,
the subjects at different rooms needed to evacuate to a safe place, i.e., one of the two
stairs at the two ends of the corridor.
To investigate the effectiveness of the ﬁre route guide, we considered two types of
voice alarms, one with route guide “The building is on ﬁre, please cover your mouth
with wet cloth, in the corridor turn right/left, go to the safe exit quickly!”, and the
other without route guide “The building is on ﬁre, please cover your mouth with wet
cloth!”. The same voice alarm was repeated in one test until the test ﬁnished. In each
test, one of the two stairs was blocked by a ﬁre randomly. When a subject reached the
ﬁre stairs, a staff would notify her/him about being blocked by ﬁre, and the subject
then needed to turn around, and run to the opposite side, i.e., the safe stairs. Which
type of voice alarm would arise in one test was a random choice. The experiment took
place in a real building, and the distances from the two rooms to the two stairs were
not equal. This might have inﬂuenced the egress time. Therefore, for each kind of voice
alarms, we let the number of ﬁre on the left stairs be equal to the number of ﬁre on the
right stairs. We recorded both the egress time and the mathematical exercise score
for each subject. The bonus was divided into three categories for a competitive atmo-
sphere. (1) If a subject obtains top 50% exercise score and also is among top 50% to
arrive at a safe place, s/he will get 40 RMB (Yuan) bonus; (2) If a subject does not ob-
tain top 50% exercise score but is among top 50% to arrive at a safe place, s/he will
get 30 RMB (Yuan) bonus; (3) The rest will get 20 RMB (Yuan) bonus.
17.6.3.2 Result
In total, 96 measurements of egress time for voice route guide and 96 measurements
for voice without route guide were noted down. The ranked measurements are shown
Authenticated
:30 PM

452
17 FireVGuide: A Context-Aware Fire Victims Guide
Table 17.2: Sample medians and means of egress time.
Voice alarm type
Sample size
Median
Mean
Without route guide
96
28.35
31.53
With route guide
96
18.83
21.39
Without route guide
With route guide
80
70
60
50
40
30
20
10
0
1
10
20
30
40
50
60
70
80
90
Index
Egress Time Measurements (second)
Figure 17.13: Egress time measurements
for voice alarms with/without route
guide.
in Figure 17.13. The Wilcoxon–Mann–Whitney statistic [17] was used to do the infer-
ence test. Let f be the continuous cumulative distribution function for egress time
e1 without route guide, and g be the continuous cumulative distribution function for
egress time e2 with route guide. We examined the hypothesis H0 : f = g against the
alternative H1 : f(x) > g(x) for every x (i.e., e1 is statistically larger than e2). The stat-
istic U is 2516.5 and z is 5.43. The statistics reject H0 in favor of H1 with the signiﬁcance
level a < 0.001. The sample medians and means are shown in Table 17.2. The result
supports a ﬁrst but valuable ﬁnding, that is, the voice alarm with context-aware route
guide leads to less egress time than the voice alarm without route guide in a similar
building layout statistically, although we observed not all the subjects followed the
guide exactly in this experiment.
17.6.3.3 Discussion
Building type. It is noteworthy that this user study was conducted in an ofﬁce building.
The result is for the situation when the ﬁre is just beginning and the crowd is relatively
small. This might not be suitable for a very crowded club.
Equipment failure. In this study, the result was obtained in a situation where
the equipments, e.g., the sensors, or at least some of them work. Equipment failure
should be considered for the system deployment. For example, reserve battery and
ﬁre proof materials should be used to protect the equipments. Actually, the inﬂuence
of equipment failure and how to reduce such a negative inﬂuence deserve further
research.
Authenticated
:30 PM

17.6 Evaluation of FireVGuide
453
17.6.4 Efﬁciency of FireVGuide
The total time cost Tcost of FireVGuide can be divided into following ﬁve parts:
Tcost = Tsense + Tjudge + Tpath + Ttrapped + Tdelivery, where
–
Tsense: sensors’ reading and processing time, which mainly depends on the ac-
quisition speed of data acquisition cards. Reading the data buffer of the data
acquisition card once costs about 380 ms in the experiment.
–
Tjudge: context judgement time, used to judge the ﬁre status in the building.
–
Tpath: escaping path ﬁnding time, used to discover the shortest safe path for each
room of the building.
–
Ttrapped: To-Do list generation time for trapped victims.
–
Tdelivery: guidance delivery time, used to send out FireVGuide guidance to speakers,
victims’ mobile phones, and emergency lights. It takes about 0.6ms to deliver the
guidance to a speaker.
As context judgment, path ﬁnding, and To-Do-list generation constitutes the inner
kernels of the system, while context sensing and delivery time are more beyond the
control of the system, we evaluate the time costs of part (2), (3), and (4), which we call
system reaction time, i.e., the time interval when a smoke sensor value is read till the
guidance is issued to the last user in a ﬁre building.
Simulating the two real building models (i.e., Shanghai Business College Dorm-
itory and Shenzhen Wuwang Club) (Figure 17.1), where the former has 11 rooms per
ﬂoor and the later has seven small rooms and one big room (dance hall) per ﬂoor,
we increased the ﬂoor number, exit number, and ﬁre spot number of the buildings to
examine the performance of the FireVGuide system.
17.6.4.1 Floor Number
The total ﬂoor number (ﬂoor_num) is increased from 1 to 30. When the ﬁre status
changes, all the speakers will speak corresponding content. We recorded the ﬁrst
speaker that began to speak and the last speaker that began to speak. These two times
are called minimum and maximum reaction time here. Figure 17.14(a) presents the
minimum and maximum reaction times when there is one ﬁre spot (i.e. ﬁre source)
in the dormitory building. It is obvious that more ﬂoors with more rooms and occu-
pants need more time for FireVGuide to complete its overall guide mission. Here, the
maximum time always happened when one exit was on ﬁre, and the minimum time
always happened when a room was the ﬁre spot. This is because when one exit was
on ﬁre, some of the paths we calculated ofﬂine become unsafe, therefore the system
has to recalculate and search the shortest safe path.
Comparing the two building models, the maximum reaction time on the dance-
hall model is always larger than that on the dormitory model. For instance, when
Authenticated
:30 PM

454
17 FireVGuide: A Context-Aware Fire Victims Guide
400
Time Cost(ms)
350
300
250
200
150
100
50
0
(a)
(b)
(c)
Time Cost(ms)
350
300
250
200
150
100
50
0
1
2
3
4
5
6
Exit Number
Max
Min
1
5
10
15
20
25
30
700
Time Cost(ms)
600
500
300
200
100
0
1
11
21
31
41
51
61
71
81
91
101
111
121
131
141
151
161
171
181
191
201
Fire Sport
Number
Dorm
Dance-hall
400
Dorm_Max
Dorm_Min
Dance-hall_Max
Dance-hall_Min
Floor Number
Figure 17.14: Computing time under different numbers of ﬂoors, ﬁre spots, and exits : (a) Floor number
vs. time; (b) Exit number vs. time; (c) Exit number vs. time.
the ﬂoor number is 30, the former needs 341ms, while the later needs 132ms. This is
because the dance-hall graph has more nodes than the dormitory graph to process.
17.6.4.2 Fire Spot Number
Figure 17.14(b) gives system reaction times when there are multiple consecutive ﬁre
spots due to ﬁre spreading or line ﬁre. As the searching node space for escaping
routes shrinks with more ﬁre spots, the system reaction time becomes shorter. When
all the exits on the ﬁrst ﬂoor are on ﬁre, implying all the people in the building are
trapped, the reaction times upon the two building models reach the highest due to the
expensive To-Do-list table look-up.
17.6.4.3 Exit Number
The exit number per ﬂoor is varied from 1 to 6. When there is one ﬁre spot,
Figure 17.14(c) shows that the minimum time is quite steady when the exit number
increases. This is because the minimum time happens when one room is on ﬁre. The
system doesn’t need to recalculate the safe path for any room. So exit number has little
effect on the system reaction time.
When there is one exit in the building, the maximum time which is 328ms is longer
than the rest. Because when the only exit is on ﬁre, people in each room are trapped.
Authenticated
:30 PM

Literature
455
The system needs to look up the To-Do list table to match the most appropriate trapped
actions, which is the most expensive operation in the system.
17.7 Recapitulation
This chapter reported the technical design of an ambient intelligent context-aware ﬁre
victims response guide, aiming to calm and guide panicked occupants to properly re-
act to an emergent ﬁre. The presented ambient smart ﬁre victims guidance system is
supposed to work at an early stage of ﬁre in a building, and serve as a complement to
the existing optical and acoustic guidance techniques. The performance of the system
through user studies and experiments were evaluated. The experimental results show
the feasibility, effectiveness, and some limitations of the system. To have the system
truly work in the real world, a number of open questions still need to be resolved.
First, the success of an evacuation process relies on precise behavioral, psycholo-
gical, physical, and social studies related with both individuals and groups. Users’
collective effects must be seriously considered when designing an evacuation as-
sistance system. To evaluate and improve the work, the guidance obtained by the
approach developed in the chapter must be incorporated in crowd simulation.
Second, for a mission-critical application like the one presented in the chapter, it
is very important to address the liability issue introduced by potential faulty sensors,
burning hardware, and software bugs of the system in a life-threatening situations.
Third, although the shortest safe path solution is presented to direct ﬁre victims
to different exits, ﬁnding the optimal guiding strategy still remains challenging, for
example, when some exits are blocked by the crowd or collapsed by secondary effects
from the ﬁre. In addition to sensor data, some other factors, like dynamic crowd dens-
ity detection and phased evacuation, can be incorporated in computing appropriate
escape routes.
Finally, besides displaying evacuation routes and action-list, users’ mobile
phones could be employed to communicate with outside for help. Some public
available displays in buildings can also be explored as an assistance interface.
Literature
[1]
J. Bardram and N. Norskov. A context-aware patient safety system for the operating room. In
Proc. of Ubicom, pages 272–281, 2008.
[2]
M. Barnes, H. Leather, and D. Arvind. Emergency evacuation using wireless sensor networks. In
Proc. of the 32nd Intl. Conf. on Local Computer Networks, pages 851–857, 2007.
[3]
F. Blohberger and G. Grundler. Evacuation system having escape identiﬁcation lights. United
States Patent 20090102619, 2009.
[4]
N. Brushlinsky, S. Sokolov, I.P. Wagner, and J. Hall. World ﬁre statistics.
http://ec.europa.eu/consumers/conssafe/presentations/21-02/ctif.pdf, 2006.
[5]
J. Capote, D. Alvear, O. Abreu, and A. Cuesta. Evacuation modeling of ﬁre scenarios in
passenger trains. In Proc. of the Intl. Conf. on Pedestrian and Evacuation, pages 705–711, 2008.
Authenticated
:30 PM

456
17 FireVGuide: A Context-Aware Fire Victims Guide
[6]
G. Chen, M. Li, and D. Kotz. Data-centric middleware for context-aware pervasive computing.
Journal of Pervasive and Mobile Computing, 4(2):216–253, 2007.
[7]
Chinese Ministry of Public Security. 2008 ﬁre statistics in china. http:// www.mps.gov.cn/n16/
n1282/n3553/1778249.html, 2008.
[8]
S. Denef. Human-computer interaction techniques in ﬁreﬁghting. In Proc. of the 12th IFIP TC 13
Intl. Conf. on Human-Computer Interaction, pages 864–867, 2009.
[9]
S. Denef, L. Ramirez, and T. Dyrks. Letting tools talk: Interactive technology for ﬁreﬁghting. In
Proc. of the 27th Intl. Conf. on Human Factors in Computing Systems, pages 4447–4452, 2009.
[10]
A. Ferscha and K. Zia. LifeBelt: Silent directional guidance for crowd evacuation. In Proc. of the
13th Intl. Symposium on Wearable Computers, pages 19–26, 2009.
[11]
C. Fischer and H. Gellersen. Location and navigation support for emergency responders: A
survey. Journal of IEEE Pervasive Computing, 9(1):38–47, January–March 2010.
[12]
D. Helbing, I. Farkas, and T. Vicsek. Simulating dynamical features of escape panic. Journal of
Letters to Nature, 407(6803):487–490, 2000.
[13]
D. Helbing, A. Johansson, and H. A. Abideen. Dynamics of crowd disasters: An empirical study.
Journal of Physical Review, 75:1–7, 2007.
[14]
X. Jiang, N. Chen, J. Hong, K. Wang, L. Takayama, and J. Landay. Siren: Context-aware computing
for ﬁreﬁghting. In Proc. of the 2nd Intl. Conf. on Pervasive Computing, pages 87–105, 2004.
[15]
M. Klann. Playing with ﬁre: User-centered design of wearable computing for emergency
response. In Proc. of the Intl. Conf. on Mobile Response pages 116–125, 2007.
[16]
M. Klann. Tactical navigation support for ﬁreﬁghters: The lifenet ad-hoc sensor-network and
wearable system. In Proc. of the Intl. Conf. on Mobile Response, pages 41–56, 2009.
[17]
J. Landgren and U. Nulden. A study of emergency response work: Patterns of mobile phone
interaction. In Proc. of the Intl. conf. on Human Factors in Computing Systems, pages
1323–1332, 2007.
[18]
Q. Li, M. De Rosa, and D. Rus. Distributed algorithms for guiding navigation across a sensor
network. In Proc. of the 9th Intl. Conf. on Mobile Computing and Networking, pages 313–325,
2003.
[19]
D. Liu. Shanghai business college dormitory ﬁre: Four students dead. http://
news.xinhuanet.com/newscenter/2008-11/14/content103572551.ht m, 2008.
[20] K. Luyten, F. Winters, K. Coninx, D. Naudts, and I. Moerman. A situationaware mobile system to
support ﬁre brigades in emergency situations. In Proc. of the OTM Workshops, pages
1966–1975, 2006.
[21]
M. Mataric. Designing emergent behaviors: From local interactions to collective intelligence. In
Proc. of the 2nd Intl. Conf. on Simulation of Adaptive Behavior, pages 1–10, 1993.
[22] L. Miller. Indoor navigation for ﬁrst responders: A feasibility study. Technical report, US
National Institute Standards and Technology, 2006.
[23] H. Muir, D. Bottomley, and C. Marrison. Effects of motivation and cabin conﬁguration on
emergency aircraft evacuation behavior and rates of egress. Journal of Aviation Psychology
6(1):57–77, 1996.
[24] V. Renaudin, O. Yalak, and P. Tome. Indoor navigation of emergency agents. European Journal of
Navigation, 5:36–45, 2007.
[25]
H. Salam, S. Rizvi, S. Ainsworth, and S. Olariu. A durable sensor enabled lifeline support for
ﬁreﬁghters. In Proc. of the Computer Communications Workshop, pages 1–6, 2008.
[26] Sina News. http://news.sina.com.cn/z/shzhtdhz/index.shtml
[27]
J. Shastri. Safe navigation during ﬁre hazards using specknets. Master’s thesis, The University
of Edinburgh, 2006.
Authenticated
:30 PM

Literature
457
[28] D. Steingart, J. Wilson, A. Redfern, P. Wright, R. Romero, and L. Lim. Augmented cognition for
ﬁre emergency response: An iterative user study. In Proc. of the 1st Intl. Conf. on Augmented
Cognition, 2005.
[29] T. Tabirca, K. Brown, and C. Sreenan. A dynamic model for ﬁre emergency evacuation based on
wireless sensor networks. In Proc. of the 8th Intl. Symposium on Parallel and Distributed
Computing, pages 29–36, 2009.
[30] Z. Toups and A. Kerne. Implicit coordination in ﬁreﬁghting practice: Design implications for
teaching ﬁre emergency responders. In Proc. of the SIGCHI conference on Human factors in
Computing Systems, pages 707–716, 2007.
[31]
Y. Tseng, M. Pan, and Y. Tsai. A distributed emergency navigation algorithm for wireless sensor
networks. IEEE Computers, 39(7):55–62, 2006.
[32] C. Wada, Y. Yoneda, and Y. Sugimura. Proposal of a direction guidance system for evacuation. In
Proc. of HCI, pages 221–227, 2009.
[33] P. Wang and P. Luh. Modeling and optimization of crowd guidance for building emergency
evacuation. In Proc. of the 4th IEEE Intl. Conf. on Automation Science and Engineering, pages
328–334, 2008.
[34] J. Wilson. Design of monocular head-mounted displays for increased indoor ﬁreﬁghting safety
and efﬁciency. In Proc. of SPIE on Helmet-and Headmounted Displays X: Technologies and
Applications, pages 103–114, 2005.
[35]
J. Wilson. A wireless sensor network and incident command interface for urban ﬁreﬁghting. In
Proc. of the 4th IEEE Intl. Conf. on Mobile and Ubiquitous Systems: Network and Services, pages
1–7, 2007.
[36] J. Wilson and P. Wright. Head-mounted display efﬁcacy study to aid ﬁrst responder indoor
navigation. In Proc. of the Institution of Mechanical Engineers, Part C: Journal of Mechanical
Engineering Science, pages 675–688, 2009.
[37]
M. Wirz, D. Roggen, and G. Troster. Decentralized detection of group formations from wearable
acceleration sensors. In Proc. of the Intl. Conf. on Computational Science and Engineering,
pages 952–959, 2009.
[38] M. Wirz, D. Roggen, and G. Troster. A methodology towards the detection of collective behavior
patterns by means of body-worn sensors. In Proc. Of the Workshop in conjunction with the 8th
Pervasive Computing Conference, 2010.
[39] Y. Zeng, S. Murphy, L. Sitanayah, T. Tabirca, T. Truong, K. Brown, and C. Sreenan. Building ﬁre
emergency detection and response using wireless sensor networks. In Proc. of the 9th IT & T
Conference, 2009.
Authenticated
:30 PM

Authenticated
:30 PM

Index
k-anonymity protection 123
n-dimensional context space 250
access context 359
access control 118
ad-hoc meeting planner 389
aleatory uncertainty 45, 94
Ambient Intelligence 3
answer explanation and uncertainty
clariﬁcation 291
answer ordering 262
Bayesian method 46
bilingual aviation MRO task cards 413
bilingual MRO task cards generator 412
Causality and Responsibility 265
characteristics of context 7
concept expression 33
conceptual categories 52
conceptual categorization 6
context 6
context awareness 430
context cluster and association
relationships 342
context consumer 102
context database 159
context dimension 257
context event 37
context event detection 171, 173
context events 173
context instance 334
context instances 250
context keyword 362
context manager 392
context memory 330
context modeling 42
context monitor 392
context ontology 30
context provider 102
context provision 102
context provision and acquisition 102
context query languages and mechanisms 159
context querying 201
context reasoning 171
context representation 27
context space reduction 258
context tree 362
context trees 357
Context uncertainty 44
context uncertainty 239
context uncertainty management 44
context-aware applications 102, 201, 398
context-aware data management 3, 94
context-aware enterprise search engine 412
context-aware ﬁre victims response guide 455
context-aware learning 400
context-aware preference querying 239
context-aware query 14
context-aware query preferences 239
Context-awareness 6
context-awareness 3
contextual attributes 255
contextual cues 329
contextual ranking function 255
continuous stream query processing 173
Correlation-based Query 81
crowd-powered querying 94
data partitioning 186
database-contextual attribute pairs 259
declarative quality-aware speciﬁcation
language 115
decryption 138
Dempster-Shafer evidence theory 46
derived uncertainty theory 46, 56
Description Logic 32
discretionary access control 118
domain-speciﬁc knowledge 92
encrypted context information searching 117
encrypted XML data 124
encryption 124, 138
energy efﬁciency 202
energy efﬁciency heterogeneous clusters 206
energy efﬁciency of MapReduce 207
energy-constrained sensor nodes and mobile
services 201
energy-efﬁcient computation 201
energy-efﬁcient context querying 201
energy-efﬁcient hardware 204
energy-efﬁcient resource management 208
DOI 10.1515/9783110556674-018
Authenticated
:30 PM

460
Index
energy-efﬁcient sensing 211
episodic memory 353
epistemic uncertainty 45, 94
event processing 173
event sequences 173
Evidence-based Query 83
evidence-oriented database 83
explanation 292
external environments and internal users 44
ﬁne-grained charging model 234
ﬂexible control of context query expenses and
query performance 221
Fuzzy Databases 84
fuzzy set theory 53
fuzzy theory 45
green computing 201
high-level context information 173
hippocratic databases 121
impact of context attributes upon database
attributes 260
info-gap theory 46
information recall 314
information reﬁnding 314
interact with a user 291
keyword search 138
knowledge-based context-aware query
preference model 241
LCP-based context degradation 146
learning context 402
life-cycle degradation 335
life-cycle management of privacy-sensitive
context information 117
life-cycle policy (LCP) 145
Lineage Query 80
linguistic labels 52
low-level context data 173
machine translation 415
mandatory access control 118
membership degree 85
memory recall 380
Modeling context 27
Monte Carlo method 46
Monte Carlo simulation 78
Monte-Carlo-Based Data Management 82
multiple objective optimization 234
non-deterministic ﬁnite automata (NFA)-based
query mode 176
noncontext-aware query 14
ordering sensitivity 262
parallel detection 173
parallel detection of complex events from event
streams 188
parallel query processing 186
partitioning stream records 187
physical surroundings 117
Platform for Privacy Preferences (P3P) 120
possibility distribution 53, 85
Possibility theory 53
possible world semantics 77
preferences 239
previous access context 330
privacy 117, 118
privacy protection 117
probabilistic answers 262
probabilistic context databases 262
probabilistic context events 88
probabilistic event 76
probabilistic top-k (aggregation) queries 262
probabilistic top-k aggregation queries 268
probabilistic top-k queries 268
probability distribution 92
probability theory 46, 53
Provenance and Lineage 265
purpose-based access control 118
quality-aware context acquisition 102
query expenses 221
query personalization 239
query results 239
query trees 179
query-by-context model 330
Range Query 78
recall characteristics 314
reconcile context-awareness and privacy 118
reﬁnd web pages and local ﬁles 314
relevance feedback 293
result explanation 291
retention strength 337
role-based access control 118
Authenticated
:30 PM

Index
461
scrub (correct) query results 291
search upon encrypted XML data 139
semantic memory 353
sensitivity analysis 262
sensitivity computation of an answer ordering
change 273
sensitivity degree of an answer ordering
change 272
sensitivity of answer ordering 262
sensory data 173
stream processing 173
streaming data processing 173
streams 173
string-based query mode 176
task relevancy 412
task-driven partitioning 189
temporal operators 188
temporal relationships 42
time-driven partitioning 188
Top-k Query 79
tree-based query mode 176
tuple-level fuzzy data model 84
tuple-wise probabilistic database 76
uncertain context management 93
uncertain context representation 92
uncertain distribution 56
uncertain measure 56
uncertain variable 56
uncertainty 44
uncertainty processing theories and
practices 44
uncertainty table 92
vagueness uncertainty 52
virtualization 205
web and personal information management
(PIM) 314
web revisitation 353
weighted Spearman’s footrule metric 270
XPath query 141
Authenticated
:30 PM

Authenticated
:30 PM

Advances in Computer Science
Volume 2
D. Feng
Trusted Computing, 2017
ISBN 978-3-11-048757-2, e-ISBN (PDF) 978-3-11-048773-2,
e-ISBN (EPUB) 978-3-11-048763-3, Set-ISBN 978-3-11-048776-3
Volume 1
Q. Su
Color Image Watermarking, 2016
ISBN 978-3-11-047604-0, e-ISBN (PDF) 978-3-11-047759-7,
e-ISBN (EPUB) 978-3-11-047609-5, Set-ISBN 978-3-11-047760-3
www.degruyter.com
Authenticated
:30 PM

Authenticated
:30 PM


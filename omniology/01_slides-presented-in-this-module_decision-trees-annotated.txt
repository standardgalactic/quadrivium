Machine Learning Specialization 
Decision Trees 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Predicting potential loan defaults 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
3 
What makes a loan risky? 
©2015-2016 Emily Fox & Carlos Guestrin 
I want a to buy 
a new house! 
Credit  
★★★★ 
Income 
★★★ 
Term 
★★★★★ 
Personal Info 
★★★ 
Loan  
Application 

Machine Learning Specialization 
4 
Credit history explained 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit History  
★★★★ 
Income 
★★★ 
Term 
★★★★★ 
Personal Info 
★★★ 
Did I pay previous 
loans on time? 
 
Example: excellent, 
good, or fair 
  

Machine Learning Specialization 
5 
Income 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit History  
★★★★ 
Income 
★★★ 
Term 
★★★★★ 
Personal Info 
★★★ 
What’s my income? 
 
Example:  
$80K per year 

Machine Learning Specialization 
6 
Loan terms 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit History  
★★★★ 
Income 
★★★ 
Term 
★★★★★ 
Personal Info 
★★★ 
How soon do I need to 
pay the loan? 
 
Example: 3 years,     
5 years,… 

Machine Learning Specialization 
7 
Personal information 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit History  
★★★★ 
Income 
★★★ 
Term 
★★★★★ 
Personal Info 
★★★ 
Age, reason for the 
loan, marital status,… 
 
Example: Home loan 
for a married couple 

Machine Learning Specialization 
9 
Intelligent application 
©2015-2016 Emily Fox & Carlos Guestrin 
Safe 
✓ 
Risky 
✘ 
Risky 
 ✘ 
Intelligent loan application  
review system 
Loan  
Applications 

Machine Learning Specialization 
11 
Classiﬁer review 
©2015-2016 Emily Fox & Carlos Guestrin 
Loan 
Application 
Classiﬁer 
MODEL 
Input:  xi 
Output: ŷ  
Predicted  
class 
Safe 
ŷi = +1  
Risky 
ŷi = -1  

Machine Learning Specialization 
12 
This module ... decision trees 
©2015-2016 Emily Fox & Carlos Guestrin 
Start 
Credit? 
Safe 
excellent 
Income? 
poor 
Term? 
Risky 
Safe 
fair 
5 years 
3 years 
Risky 
Low 
Term? 
Risky 
Safe 
high 
5 years 
3 years 

Machine Learning Specialization 
Decision trees: Intuition 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
14 
What does a decision tree represent? 
©2015-2016 Emily Fox & Carlos Guestrin 
3 year loans with fair 
credit history are risky 
Credit? 
Term? 
Risky 
Start 
fair 
3 years 
Safe 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
excellent 
poor 
5 years 
Low 
high 
5 years 
3 years 

Machine Learning Specialization 
15 
What does a decision tree represent? 
©2015-2016 Emily Fox & Carlos Guestrin 
3 year loans with high 
income & poor credit 
history are risky 
Credit? 
Income? 
Term? 
Risky 
Start 
poor 
high 
Safe 
excellent 
Term? 
Risky 
Safe 
fair 
5 years 
3 years 
Risky 
Low 
Safe 
5 years 
3 years 

Machine Learning Specialization 
16 
Scoring a loan application 
©2015-2016 Emily Fox & Carlos Guestrin 
xi = (Credit = poor, Income = high, Term = 5 years) 
Credit? 
Safe 
Term? 
Risky 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
Start 
excellent 
poor 
fair 
5 years 
3 years 
Low 
high 
5 years 
3 years 
Credit? 
Safe 
Term? 
Risky 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
Start 
excellent 
poor 
fair 
5 years 
3 years 
Low 
high 
5 years 
3 years 
ŷi = Safe 

Machine Learning Specialization 
18 
©2015-2016 Emily Fox & Carlos Guestrin 
Decision tree model 
T(xi) = Traverse decision tree 
 
 
 
 
 
 
Loan 
Application 
Input:  xi 
ŷi 
Credit? 
Safe 
Term? 
Risky 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
start 
excellent 
poor 
fair 
5 years 
3 years 
Low 
high 
5 years 
3 years 

Machine Learning Specialization 
Decision tree learning task 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
21 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
h(x) 
T(x) 
x 
ŷ 

Machine Learning Specialization 
23 
Learn decision tree from data? 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
5 yrs 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
low 
safe 
poor 
3 yrs 
high 
risky 
poor 
5 yrs 
low 
safe 
fair 
3 yrs 
high 
safe 
Credit? 
Safe 
Term? 
Risky 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
Start 
excellent 
poor 
fair 
5 years 
3 years 
Low 
high 
5 years 
3 years 

Machine Learning Specialization 
25 
Decision tree learning problem 
©2015-2016 Emily Fox & Carlos Guestrin 
Optimize  
quality metric  
on training data 
Training data: N observations (xi,yi) 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
5 yrs 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
low 
safe 
poor 
3 yrs 
high 
risky 
poor 
5 yrs 
low 
safe 
fair 
3 yrs 
high 
safe 
T(X) 

Machine Learning Specialization 
26 
Quality metric: Classiﬁcation error 
•  Error measures fraction of mistakes 
- Best possible value : 0.0  
- Worst possible value: 1.0 
©2015-2016 Emily Fox & Carlos Guestrin 
Error =   # incorrect predictions  
                          # examples 

Machine Learning Specialization 
27 
Find the tree with lowest classiﬁcation error 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
5 yrs 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
low 
safe 
poor 
3 yrs 
high 
risky 
poor 
5 yrs 
low 
safe 
fair 
3 yrs 
high 
safe 
T(X) 
Minimize  
classiﬁcation error  
on training data 

Machine Learning Specialization 
28 
How do we ﬁnd the best tree? 
©2015-2016 Emily Fox & Carlos Guestrin 
Exponentially large number of possible 
trees makes decision tree learning hard!
(NP-hard problem) 
T1(X) 
T2(X) 
T3(X) 
T4(X) 
T5(X) 
T6(X) 

Machine Learning Specialization 
29 
Simple (greedy) algorithm ﬁnds “good” tree 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
5 yrs 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
low 
safe 
poor 
3 yrs 
high 
risky 
poor 
5 yrs 
low 
safe 
fair 
3 yrs 
high 
safe 
T(X) 
Approximately  
minimize  
classiﬁcation error  
on training data 

Machine Learning Specialization 
Greedy decision tree learning:  
 Algorithm outline 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
31 
(all data) 
Step 1: Start with an empty tree 
©2015-2016 Emily Fox & Carlos Guestrin 
All points in the 
dataset 
Histogram 
of y values 
Safe 
Risky 

Machine Learning Specialization 
32 
Step 2: Split on a feature 
©2015-2016 Emily Fox & Carlos Guestrin 
(all data) 
Credit? 
Subset of data with 
Credit = excellent 
excellent 
Subset of data with 
Credit = poor 
poor 
Subset of data with 
Credit = fair 
fair 
Split/partition 
data on Credit 

Machine Learning Specialization 
34 
(all data) 
Feature split explained 
©2015-2016 Emily Fox & Carlos Guestrin 
excellent 
 
fair 
poor 
Data points where  
Credit = excellent 
Credit? 
Safe 
Risky 
Split/partition 
data on Credit 

Machine Learning Specialization 
35 
(all data) 
Step 3: Making predictions 
©2015-2016 Emily Fox & Carlos Guestrin 
excellent 
 
fair 
poor 
Credit? 
Predict Safe 
Safe 
Risky 
Here, all examples 
are Safe loans 

Machine Learning Specialization 
36 
Step 4: Recursion 
©2015-2016 Emily Fox & Carlos Guestrin 
(all data) 
excellent 
 
fair 
poor 
Credit? 
Build tree from 
these data points 
Build tree from 
these data points 
Safe 
Risky 
Nothing more 
to do here 
Predict Safe 

Machine Learning Specialization 
37 
Problem 1: Feature 
split selection 
Recursion 
Problem 2: 
Stopping condition 
Greedy decision tree learning 
©2015-2016 Emily Fox & Carlos Guestrin 
•  Step 1: Start with an empty tree 
 
•  Step 2: Select a feature to split data 
 
•  For each split of the tree: 
•  Step 3: If nothing more to,           
make predictions 
 
•  Step 4: Otherwise, go to Step 2 & 
continue (recurse) on this split 

Machine Learning Specialization 
Feature split learning 
    =  
Decision stump learning 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
40 
Start with the data 
©2015-2016 Emily Fox & Carlos Guestrin 
Assume N = 40, 3 features 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
5 yrs 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
low 
safe 
poor 
3 yrs 
high 
risky 
poor 
5 yrs 
low 
safe 
fair 
3 yrs 
high 
safe 

Machine Learning Specialization 
41 
(all data) 
Start with all the data 
©2015-2016 Emily Fox & Carlos Guestrin 
Loan status:    Safe   Risky 
N = 40 examples 
Number of Safe 
loans 
22 
Number of Risky 
loans 
18 

Machine Learning Specialization 
42 
Root 
22    18 
Compact visual notation: Root node 
©2015-2016 Emily Fox & Carlos Guestrin 
Loan status:    Safe   Risky 
N = 40 examples 
Number of safe 
loans 
Number of risky 
loans 

Machine Learning Specialization 
43 
Decision stump: Single level tree 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
     22    18 
excellent 
9     0 
fair 
9     4 
 
poor 
4     14 
Loan status:  
Safe Risky 
Credit? 
Split on Credit 
Subset of data with 
Credit = excellent 
Subset of data with 
Credit = fair 
Subset of data with 
Credit = poor 
excellent 
fair 
poor 
(all data) 

Machine Learning Specialization 
44 
Visual Notation: Intermediate nodes 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
     22    18 
excellent 
9     0 
fair 
9     4 
 
poor 
4     14 
Loan status:  
Safe Risky 
Credit? 
Intermediate nodes 

Machine Learning Specialization 
45 
Making predictions with  
a decision stump 
©2015-2016 Emily Fox & Carlos Guestrin 
root 
22  18 
excellent 
9    0 
fair 
9   4 
 
poor 
4   14 
Loan status:  
Safe  Risky 
credit? 
For each intermediate node, 
set ŷ = majority value 
Safe 
Safe 
Risky 

Machine Learning Specialization 
Selecting best feature to split on 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
47 
How do we learn a decision stump? 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
     22    18 
excellent 
9     0 
fair 
9     4 
 
poor 
4     14 
Loan status:  
Safe Risky 
Credit? 
Find the “best” 
feature to split on! 

Machine Learning Specialization 
49 
How do we select the best feature? 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
22  18 
excellent 
9    0 
fair 
9   4 
 
poor 
4   14 
Loan status:  
Safe  Risky 
Credit? 
Choice 1: Split on Credit 
Root 
22  18 
3 years 
16   4 
5 years 
6    14 
Loan status:  
Safe  Risky 
Term? 
Choice 2: Split on Term 
OR 

Machine Learning Specialization 
50 
How do we measure  
eﬀectiveness of a split? 
©2015-2016 Emily Fox & Carlos Guestrin 
Error =   # mistakes  
             # data points 
Root 
22  18 
poor 
4   14 
Loan status:  
Safe  Risky 
Credit? 
excellent 
9    0 
fair 
9   4 
 
Idea: Calculate 
classiﬁcation error of 
this decision stump 

Machine Learning Specialization 
51 
Calculating classiﬁcation error 
©2015-2016 Emily Fox & Carlos Guestrin 
•  Step 1: ŷ = class of majority of data in node 
•  Step 2: Calculate classiﬁcation error of 
predicting ŷ for this data 
Root 
22    18 
Loan status:  
Safe  Risky 
Error =            . 
                
         = 
18 mistakes 
22 correct 
ŷ = majority class 
Safe 
Tree 
Classiﬁcation error 
(root) 
  0.45 

Machine Learning Specialization 
52 
Choice 1: Split on credit history? 
©2015-2016 Emily Fox & Carlos Guestrin 
Does a split on Credit 
reduce classiﬁcation 
error below 0.45? 
Root 
22  18 
excellent 
9    0 
fair 
9   4 
 
poor 
4   14 
Loan status:  
Safe  Risky 
Credit? 
Choice 1: Split on Credit 

Machine Learning Specialization 
53 
How good is the split on Credit? 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
22  18 
excellent 
9    0 
fair 
9   4 
 
poor 
4   14 
Loan status:  
Safe  Risky 
Credit? 
Choice 1: Split on Credit 
Step 1: For each 
intermediate node, 
set ŷ = majority value 
Safe 
Safe 
Risky 

Machine Learning Specialization 
54 
Split on Credit: Classiﬁcation error 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
22  18 
excellent 
9    0 
fair 
9   4 
 
poor 
4   14 
Loan status:  
Safe  Risky 
Credit? 
0 mistakes 
4 mistakes 
4 mistakes 
Safe 
Safe 
Risky 
Choice 1: Split on Credit 
Error =            . 
                
         = 
Tree 
Classiﬁcation error 
(root) 
  0.45 
Split on credit 
0.2 

Machine Learning Specialization 
55 
Choice 2: Split on Term? 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
22  18 
3 years 
16   4 
5 years 
6   14 
Loan status:  
Safe  Risky 
Term? 
Safe 
Risky 
Choice 2: Split on Term 

Machine Learning Specialization 
56 
Evaluating the split on Term 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
22  18 
3 years 
16   4 
5 years 
6   14 
Loan status:  
Safe  Risky 
Term? 
4 mistakes 
6 mistakes 
Safe 
Risky 
Error =            . 
                
         = 
Tree 
Classiﬁcation error 
(root) 
0.45 
Split on credit 
0.2 
Split on term 
0.25 
Choice 2: Split on Term 

Machine Learning Specialization 
57 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
22  18 
excellent 
9    0 
fair 
8   4 
 
poor 
4   14 
Loan status:  
Safe  Risky 
Root 
22  18 
3 years 
16   4 
5 years 
6    14 
Loan status:  
Safe  Risky 
OR 
Credit? 
Term? 
Tree 
Classiﬁcation error 
(root) 
0.45 
split on credit 
0.2 
split on loan term 
0.25 
WINNER 
Choice 1 vs Choice 2 
Choice 2: Split on Term 
Choice 1: Split on Credit 

Machine Learning Specialization 
58 
Feature split selection algorithm 
©2015-2016 Emily Fox & Carlos Guestrin 
•  Given a subset of data M (a node in a tree) 
 
•  For each feature hi(x): 
1.  Split data of M according to feature hi(x)  
 
2.  Compute classiﬁcation error split 
 
•  Chose feature h*(x) with lowest 
classiﬁcation error 

Machine Learning Specialization 
59 
Pick feature split 
leading to lowest 
classiﬁcation error 
Greedy decision tree learning 
©2015-2016 Emily Fox & Carlos Guestrin 
•  Step 1: Start with an empty tree 
 
•  Step 2: Select a feature to split data 
 
•  For each split of the tree: 
•  Step 3: If nothing more to,           
make predictions 
 
•  Step 4: Otherwise, go to Step 2 & 
continue (recurse) on this split 

Machine Learning Specialization 
Decision Tree Learning:  
Recursion & Stopping conditions 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
62 
Learn decision tree from data? 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit 
Term 
Income 
y 
excellent 
3 yrs 
high 
safe 
fair 
5 yrs 
low 
risky 
fair 
3 yrs 
high 
safe 
poor 
5 yrs 
high 
risky 
excellent 
3 yrs 
low 
risky 
fair 
5 yrs 
low 
safe 
poor 
3 yrs 
high 
risky 
poor 
5 yrs 
low 
safe 
fair 
3 yrs 
high 
safe 
Credit? 
Safe 
Term? 
Risky 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
Start 
excellent 
poor 
fair 
5 years 
3 years 
Low 
high 
5 years 
3 years 

Machine Learning Specialization 
63 
We’ve learned a decision stump, what next? 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
     22    18 
excellent 
9     0 
fair 
9     4 
 
poor 
4     14 
Loan status:  
Safe Risky 
Credit? 
Safe 
All data points are Safe è 
nothing else to do with 
this subset of data 
Leaf node 

Machine Learning Specialization 
64 
Tree learning = Recursive stump learning 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
     22    18 
excellent 
9     0 
fair 
9     4 
 
poor 
4     14 
Loan status:  
Safe Risky 
Credit? 
Safe 
Build decision stump 
with subset of data 
where Credit = poor 
Build decision stump 
with subset of data 
where Credit = fair 

Machine Learning Specialization 
65 
Second level 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
22    18 
Loan status:  
Safe Risky 
Credit? 
excellent 
9     0 
fair 
9     4 
 
poor 
4     14 
Safe 
3 years 
0     4 
5 years 
9     0 
Term? 
Risky 
Safe 
Build another stump 
these data points 
high 
4     5 
Low 
0     9 
Income? 
Risky 

Machine Learning Specialization 
66 
Final decision tree 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
22    18 
excellent 
9     0 
Fair 
9     4 
 
poor 
4     14 
Loan status:  
Safe Risky 
Credit? 
Safe 
5 years 
9     0 
3 years 
0     4 
Term? 
Risky 
Safe 
low 
0   9 
high 
4   5 
Income? 
5 years 
4   3 
3 years 
0    2 
Term? 
Risky 
Safe 
Risky 

Machine Learning Specialization 
67 
Simple greedy decision tree learning 
Pick best feature to split on 
Learn decision stump with 
this split 
For each leaf of decision 
stump, recurse 
©2015-2016 Emily Fox & Carlos Guestrin 
When do we stop??? 

Machine Learning Specialization 
68 
Stopping condition 1: All data agrees on y 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
22    18 
excellent 
9     0 
Fair 
9     4 
 
poor 
4     14 
Loan status:  
Safe Risky 
Credit? 
Safe 
5 years 
9     0 
3 years 
0     4 
Term? 
Risky 
Safe 
low 
0   9 
high 
4   5 
Income? 
5 years 
4   3 
Term? 
Risky 
Safe 
Risky 
3 years 
0    2 
3 years 
0    2 
All data in these 
nodes have same  
y value è  
Nothing to do 
excellent 
9     0 
5 years 
9     0 
3 years 
0     4 
low 
0   9 

Machine Learning Specialization 
69 
Stopping condition 2: Already split on all features 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
22    18 
excellent 
9     0 
Fair 
9     4 
 
poor 
4     14 
Loan status:  
Safe Risky 
Credit? 
Safe 
5 years 
9     0 
3 years 
0     4 
Term? 
Risky 
Safe 
low 
0   9 
high 
4   5 
Income? 
5 years 
4   3 
Term? 
Risky 
Safe 
Risky 
3 years 
0    2 
Already split on all 
possible features 
è  
Nothing to do 
5 years 
4   3 

Machine Learning Specialization 
71 
Recursion 
Stopping 
conditions 1 & 2 
Greedy decision tree learning 
©2015-2016 Emily Fox & Carlos Guestrin 
•  Step 1: Start with an empty tree 
 
•  Step 2: Select a feature to split data 
 
•  For each split of the tree: 
•  Step 3: If nothing more to,           
make predictions 
 
•  Step 4: Otherwise, go to Step 2 & 
continue (recurse) on this split 
Pick feature split 
leading to lowest 
classiﬁcation error 

Machine Learning Specialization 
Predictions with decision trees 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
74 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
h(x) 
T(x) 
x 
ŷ 

Machine Learning Specialization 
75 
©2015-2016 Emily Fox & Carlos Guestrin 
Decision tree model 
T(xi) = Traverse decision tree 
 
 
 
 
 
 
Loan 
Application 
Input:  xi 
ŷi 
Credit? 
Safe 
Term? 
Risky 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
start 
excellent 
poor 
fair 
5 years 
3 years 
Low 
high 
5 years 
3 years 

Machine Learning Specialization 
76 
Traversing a decision tree 
©2015-2016 Emily Fox & Carlos Guestrin 
xi = (Credit = poor, Income = high, Term = 5 years) 
Credit? 
Safe 
Term? 
Risky 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
Start 
excellent 
poor 
fair 
5 years 
3 years 
Low 
high 
5 years 
3 years 
Credit? 
Safe 
Term? 
Risky 
Safe 
Income? 
Term? 
Risky 
Safe 
Risky 
Start 
excellent 
poor 
fair 
5 years 
3 years 
Low 
high 
5 years 
3 years 
ŷi = Safe 

Machine Learning Specialization 
77 
Decision tree prediction algorithm 
©2015-2016 Emily Fox & Carlos Guestrin 
predict(tree_node, input) 
•  If current tree_node is a leaf: 
o  return majority class of  
data points in leaf 
•  else: 
o  next_note = child node of 
tree_node whose feature value 
agrees with input 
o  return predict(next_note, input) 

Machine Learning Specialization 
Multiclass classiﬁcation &  
predicting probabilities 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
79 
Multiclass prediction 
©2015-2016 Emily Fox & Carlos Guestrin 
Loan 
Application 
Classiﬁer 
MODEL 
Input:  xi 
Output:  ŷi  
Predicted  
class 
Safe 
Risky 
Danger 

Machine Learning Specialization 
81 
Multiclass decision stump 
©2015-2016 Emily Fox & Carlos Guestrin 
Credit 
y 
excellent 
safe 
fair 
risky 
fair 
safe 
poor 
danger 
excellent 
risky 
fair 
safe 
poor 
danger 
poor 
safe 
fair 
safe 
… 
… 
N = 40,  
1 feature,  
3 classes 
Root 
18   12   10 
Loan status:  
Safe Risky Danger 
excellent 
9   2   1 
fair 
6   9   2 
 
poor 
3   1   7 
Credit? 
Safe 
Risky 
Danger 

Machine Learning Specialization 
82 
Predicting probabilities with  
decision trees 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
18   12   10 
excellent 
9   2   1 
fair 
6   9   2 
 
poor 
3   1   7 
Loan status:  
Safe Risky Danger 
Credit? 
Safe 
Risky 
Danger 
P(y = danger | x)  
 
    =        7         = 0.64 
          3 + 1+ 7 

Machine Learning Specialization 
Decision tree learning:  
Real valued features 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
85 
How do we use real values inputs? 
©2015-2016 Emily Fox & Carlos Guestrin 
Income 
Credit 
Term 
y 
$105 K 
excellent 
3 yrs 
Safe 
$112 K 
good 
5 yrs 
Risky 
$73 K 
fair 
3 yrs 
Safe 
$69 K 
excellent 
5 yrs 
Safe 
$217 K 
excellent 
3 yrs 
Risky 
$120 K 
good 
5 yrs 
Safe 
$64 K 
fair 
3 yrs 
Risky 
$340 K 
excellent 
5 yrs 
Safe 
$60 K 
good 
3 yrs 
Risky 

Machine Learning Specialization 
87 
Split on each numeric value? 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
22    18 
 
$30K 
0    1 
$39.5K 
0    1 
$31.4K 
1    0 
Income? 
Loan status:  
Safe Risky  
$61.1K 
0    1 
$91.3K 
0    1 
Danger: May only 
contain one data point 
per node  
Can’t trust prediction  
(overﬁtting) 

Machine Learning Specialization 
88 
Alternative: Threshold split 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
     22    18 
Loan status:  
Safe Risky 
Split on the 
feature Income 
< $60K 
8     13 
>= $60K 
14     5 
Income?  
Subset of data with 
Income >= $60K 
Many data points è  
lower chance of overﬁtting 

Machine Learning Specialization 
90 
Income <  $60K 
Income >= $60K 
Threshold splits in 1-D 
©2015-2016 Emily Fox & Carlos Guestrin 
Safe 
Risky 
Income 
$120K 
$10K 
Threshold split is the line 
Income = $60K 

Machine Learning Specialization 
91 
Visualizing the threshold split 
©2015-2016 Emily Fox & Carlos Guestrin 
0 
10 
20 
30 
40 
… 
    $0K 
$40K 
$80K 
… 
Age 
Income 
Threshold split is 
the line Age  = 38 

Machine Learning Specialization 
92 
Split on Age >= 38 
©2015-2016 Emily Fox & Carlos Guestrin 
Age 
Income 
age >= 38 
age < 38 
Predict Safe 
Predict Risky 
0 
10 
20 
30 
40 
… 
    $0K 
$40K 
$80K 
… 

Machine Learning Specialization 
93 
Depth 2: Split on Income >= $60K 
©2015-2016 Emily Fox & Carlos Guestrin 
Age 
Income 
0 
10 
20 
30 
40 
… 
    $0K 
$40K 
$80K 
… 
Threshold split is the 
line Income = 60K 

Machine Learning Specialization 
95 
Each split partitions the 2-D space 
©2015-2016 Emily Fox & Carlos Guestrin 
Age 
Age >= 38 
Income >= 60K 
Age < 38 
Age >= 38 
Income < 60K 
Income 
0 
10 
20 
30 
40 
… 
    $0K 
$40K 
$80K 
… 

Machine Learning Specialization 
Finding the best threshold split 
©2015-2016 Emily Fox & Carlos Guestrin 
OPTIONAL 

Machine Learning Specialization 
98 
Finding the best threshold split 
©2015-2016 Emily Fox & Carlos Guestrin 
Income = t* 
Inﬁnite possible 
values of t 
Income <  t* 
Income >= t* 
Safe 
Risky 
Income 
$120K 
$10K 

Machine Learning Specialization 
99 
Consider a threshold between points 
©2015-2016 Emily Fox & Carlos Guestrin 
Safe 
Risky 
Income 
$120K 
$10K 
vA 
vB 
Same classiﬁcation error for any 
threshold split between vA and vB 

Machine Learning Specialization 
100 
Only need to consider mid-points 
©2015-2016 Emily Fox & Carlos Guestrin 
Safe 
Risky 
Income 
$120K 
$10K 
Finite number of 
splits to consider 

Machine Learning Specialization 
102 
Threshold split selection algorithm  
©2015-2016 Emily Fox & Carlos Guestrin 
•  Step 1: Sort the values of a feature hj(x) :  
                  Let {v1, v2, v3, … vN} denote sorted values 
•  Step 2:  
- For i = 1 … N-1  
•  Consider split ti  = (vi + vi+1) / 2 
•  Compute classiﬁcation error for treshold 
split hj(x) >= ti 
- Chose the t*
 with the lowest classiﬁcation error 

Machine Learning Specialization 
Decision trees vs logistic regression: 
Example 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
106 
Logistic regression 
©2015-2016 Emily Fox & Carlos Guestrin 
Feature 
Value 
Weight 
Learned 
h0(x) 
  1 
0.22 
h1(x) 
x[1] 
1.12 
h2(x) 
x[2] 
-1.07 

Machine Learning Specialization 
107 
Depth 1: Split on x[1] 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
18    13 
x[1] >= -0.07 
4    11 
x[1] < -0.07 
13    3 
x[1] 
y values  
-  + 

Machine Learning Specialization 
108 
Depth 2 
©2015-2016 Emily Fox & Carlos Guestrin 
Root 
18    13 
x[1] < -0.07 
13    3 
x[1] >= -0.07 
4    11 
x[1] 
x[1] < -1.66 
7    0 
x[1] >= -1.66 
6    3 
x[1] 
x[2] < 1.55 
1    11 
x[2] >=  1.55  
3    0 
x[2] 
y values  
-  + 

Machine Learning Specialization 
109 
Threshold split caveat 
©2015-2016 Emily Fox & Carlos Guestrin 
For threshold splits, 
same feature can be 
used multiple times 
Root 
18    13 
x[1] < -0.07 
13    3 
x[1] >= -0.07 
4    11 
x[1] < -1.66 
7    0 
x[1] >= -1.66 
6    3 
x[2] < 1.55 
1    11 
x[2] >=  1.55  
3    0 
x[1] 
x[1] 
x[2] 
y values  
-  + 

Machine Learning Specialization 
110 
Decision boundaries 
©2015-2016 Emily Fox & Carlos Guestrin 
                Depth 1                                      Depth 2                               Depth 10 

Machine Learning Specialization 
111 
Comparing decision boundaries 
©2015-2016 Emily Fox & Carlos Guestrin 
Logistic Regression 
Decision Tree 
Degree 2 features 
Degree 1 features 
Depth 3 
Depth 1 
Depth 10 
Degree 6 features 

Machine Learning Specialization 
Summary of decision trees 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
114 
What you can do now 
©2015-2016 Emily Fox & Carlos Guestrin 
•  Deﬁne a decision tree classiﬁer 
•  Interpret the output of a decision trees 
•  Learn a decision tree classiﬁer using 
greedy algorithm 
•  Traverse a decision tree to make 
predictions 
- Majority class predictions 
- Probability predictions 
- Multiclass classiﬁcation 

Machine Learning Specialization 
115 
Thank you to Dr. Krishna Sridhar  
Dr. Krishna Sridhar 
Staﬀ Data Scientist, Dato, Inc. 
©2015-2016 Emily Fox & Carlos Guestrin 


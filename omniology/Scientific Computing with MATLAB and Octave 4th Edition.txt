1 3
2
Alﬁ o Quarteroni
Fausto Saleri · Paola Gervasio
Scientiﬁ c Computing 
with MATLAB and 
Octave
Fourth Edition
Editorial Board
T. J.Barth
M.Griebel
D.E.Keyes
R.M.Nieminen
D.Roose
T.Schlick

Texts in Computational
Science and Engineering
2
Editors
Timothy J. Barth
Michael Griebel
David E. Keyes
Risto M. Nieminen
Dirk Roose
Tamar Schlick
For further volumes:
http://www.springer.com/series/5151


Alﬁo Quarteroni
• Fausto Saleri •
Paola Gervasio
Scientiﬁc Computing
with MATLAB
and Octave
Fourth Edition
123

Alﬁo Quarteroni
MATHICSE-CMCS
Ecole Polytechnique F´ed´erale
de Lausanne
Lausanne
Switzerland
Paola Gervasio
DICATAM
Universit`a degli Studi di Brescia
Brescia
Italy
Fausto Saleri (1965-2007)
MOX - Politecnico di Milano
Milano
Italy
ISSN 1611-0994
ISBN 978-3-642-45366-3
ISBN 978-3-642-45367-0 (eBook)
DOI 10.1007/978-3-642-45367-0
Springer Heidelberg New York Dordrecht London
Library of Congress Control Number: 2014932425
Mathematics Subject Classiﬁcation (2010): 65-01, 68U01, 68N15
c⃝Springer-Verlag Berlin Heidelberg 2003, 2006, 2010, 2014
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or
part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illus-
trations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by
similar or dissimilar methodology now known or hereafter developed. Exempted from this legal
reservation are brief excerpts in connection with reviews or scholarly analysis or material supplied
speciﬁcally for the purpose of being entered and executed on a computer system, for exclusive
use by the purchaser of the work. Duplication of this publication or parts thereof is permitted only
under the provisions of the Copyright Law of the Publisher’s location, in its current version, and
permission for use must always be obtained from Springer. Permissions for use may be obtained
through RightsLink at the Copyright Clearance Center. Violations are liable to prosecution under
the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt
from the relevant protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of
publication, neither the authors nor the editors nor the publisher can accept any legal responsibility
for any errors or omissions that may be made. The publisher makes no warranty, express or im-
plied, with respect to the material contained herein.
The picture on the cover shows a re-entrant electrical wave on a slab of homogeneous excitable
medium, exhibiting spiral turbulence and spatiotemporal chaos. Computation by Ricardo Ruiz-
Baier, IST, University of Lausanne, CH.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

To the memory of
Fausto Saleri


Preface
Preface to the First Edition
This textbook is an introduction to Scientiﬁc Computing. We will
illustrate several numerical methods for the computer solution of cer-
tain classes of mathematical problems that cannot be faced by paper
and pencil. We will show how to compute the zeros or the integrals
of continuous functions, solve linear systems, approximate functions by
polynomials and construct accurate approximations for the solution of
diﬀerential equations.
With this aim, in Chapter 1 we will illustrate the rules of the game
that computers adopt when storing and operating with real and complex
numbers, vectors and matrices.
In order to make our presentation concrete and appealing we will
adopt the programming environment MATLAB ® 1 as a faithful com-
panion. We will gradually discover its principal commands, statements
and constructs. We will show how to execute all the algorithms that we
introduce throughout the book. This will enable us to furnish an im-
mediate quantitative assessment of their theoretical properties such as
stability, accuracy and complexity. We will solve several problems that
will be raised through exercises and examples, often stemming from spe-
ciﬁc applications.
Several graphical devices will be adopted in order to render the read-
ing more pleasant. We will report in the margin the MATLAB command
along side the line where that command is being introduced for the ﬁrst
time. The symbol
will be used to indicate the presence of exercises,
the symbol
to indicate the presence of a MATLAB program, while
1 MATLAB is a trademark of TheMathWorks Inc., 24 Prime Park Way,
Natick, MA 01760, Tel: 001+508-647-7000, Fax: 001+508-647-7001.
VII

VIII
Preface
the symbol
will be used when we want to attract the attention of
the reader on a critical or surprising behavior of an algorithm or a pro-
cedure. The mathematical formulae of special relevance are put within a
frame. Finally, the symbol
indicates the presence of a display panel
summarizing concepts and conclusions which have just been reported
and drawn.
At the end of each chapter a speciﬁc section is devoted to mentioning
those subjects which have not been addressed and indicate the biblio-
graphical references for a more comprehensive treatment of the material
that we have carried out.
Quite often we will refer to the textbook [QSS07] where many issues
faced in this book are treated at a deeper level, and where theoretical re-
sults are proven. For a more thorough description of MATLAB we refer
to [HH05]. All the programs introduced in this text can be downloaded
from the web address
mox.polimi.it/qs
No special prerequisite is demanded of the reader, with the exception
of an elementary course of Calculus.
However, in the course of the ﬁrst chapter, we recall the principal re-
sults of Calculus and Geometry that will be used extensively throughout
this text. The less elementary subjects, those which are not so neces-
sary for an introductory educational path, are highlighted by the special
symbol
.
We express our thanks to Thanh-Ha Le Thi from Springer-Verlag
Heidelberg, and to Francesca Bonadei and Marina Forlizzi from Springer-
Italia for their friendly collaboration throughout this project. We grate-
fully thank Prof. Eastham of CardiﬀUniversity for editing the language
of the whole manuscript and stimulating us to clarify many points of our
text.
Milano and Lausanne
Alﬁo Quarteroni
May 2003
Fausto Saleri
Preface to the Second Edition
In this second edition we have enriched all the Chapters by intro-
ducing several new problems. Moreover, we have added new methods
for the numerical solution of linear and nonlinear systems, the eigen-
value computation and the solution of initial-value problems. Another
relevant improvement is that we also use the Octave programming en-
vironment. Octave is a reimplementation of part of MATLAB which

Preface
IX
includes many numerical facilities of MATLAB and is freely distributed
under the GNU General Public License.
Throughout the book, we shall often make use of the expression
“MATLAB command”: in this case, MATLAB should be understood
as the language which is the common subset of both programs MAT-
LAB and Octave. We have striven to ensure a seamless usage of our
codes and programs under both MATLAB and Octave. In the few cases
where this does not apply, we shall write a short explanation notice at
the end of each corresponding section.
For this second edition we would like to thank Paola Causin for hav-
ing proposed several problems, Christophe Prud´homme, John W. Eaton
and David Bateman for their help with Octave, and Silvia Quarteroni
for the translation of the new sections. Finally, we kindly acknowledge
the support of the Poseidon project of the Ecole Polytechnique F´ed´erale
de Lausanne.
Lausanne and Milano
Alﬁo Quarteroni
May 2006
Fausto Saleri
Preface to the Third Edition
This third edition features a complete revisitation of the whole book,
many improvements in style and content to all the chapters, as well as a
substantial new development of those chapters devoted to the numerical
approximation of boundary-value problems and initial-boundary-value
problems. We remind the reader that all the programs introduced in
this text can be downloaded from the web address
mox.polimi.it/qs
Lausanne, Milano and Brescia
Alﬁo Quarteroni
March 2010
Paola Gervasio
Preface to the Fourth Edition
The fourth edition features the addition of a new chapter on numeri-
cal optimization of both univariate and multivariate functions in which
several methods are presented, discussed and analyzed.
For unconstrained minimization, we consider derivative free methods,
descent (or line search) methods, and trust region methods.
For constrained minimization we restrict our discussion to penaliza-
tion methods and augmented Lagrangian methods.

X
Preface
As for the other chapters of this book, also this new chapter is sup-
ported by examples, exercises and programs written in both MATLAB
and Octave environments.
The addition of this chapter made it necessary a renumbering of
several other chapters with respect to the previous editions. Moreover,
new sections have been added in some other chapters.
Finally we remind the reader that all programs presented in this book
can be downloaded from the web address
http://mox.polimi.it/qs
Lausanne, Milano and Brescia
Alﬁo Quarteroni
December 2013
Paola Gervasio

Contents
1
What can’t be ignored . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
The MATLAB and Octave environments . . . . . . . . . . . . . .
1
1.2
Real numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2.1
How we represent them . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2.2
How we operate with ﬂoating-point numbers . . . . . .
6
1.3
Complex numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.4
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.4.1
Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.5
Real functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.5.1
The zeros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.5.2
Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.5.3
Integration and diﬀerentiation . . . . . . . . . . . . . . . . . .
22
1.6
To err is not only human . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
1.6.1
Talking about costs . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
1.7
The MATLAB language . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
1.7.1
MATLAB statements . . . . . . . . . . . . . . . . . . . . . . . . . .
32
1.7.2
Programming in MATLAB . . . . . . . . . . . . . . . . . . . . .
34
1.7.3
Examples of diﬀerences between MATLAB
and Octave languages . . . . . . . . . . . . . . . . . . . . . . . . . .
38
1.8
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
1.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
2
Nonlinear equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.1
Some representative problems . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.2
The bisection method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.3
The Newton method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.3.1
How to terminate Newton’s iterations . . . . . . . . . . . .
50
2.4
The secant method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
2.5
Systems of nonlinear equations . . . . . . . . . . . . . . . . . . . . . . . .
52
XI

XII
Contents
2.6
Fixed point iterations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
2.6.1
How to terminate ﬁxed point iterations . . . . . . . . . .
62
2.7
Acceleration using Aitken method . . . . . . . . . . . . . . . . . . . . .
63
2.8
Algebraic polynomials. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
2.8.1
H¨orner’s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
2.8.2
The Newton-H¨orner method . . . . . . . . . . . . . . . . . . . .
70
2.9
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
2.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
3
Approximation of functions and data . . . . . . . . . . . . . . . . . .
77
3.1
Some representative problems . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.2
Approximation by Taylor’s polynomials . . . . . . . . . . . . . . . .
79
3.3
Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
3.3.1
Lagrangian polynomial interpolation . . . . . . . . . . . . .
81
3.3.2
Stability of polynomial interpolation . . . . . . . . . . . . .
86
3.3.3
Interpolation at Chebyshev nodes . . . . . . . . . . . . . . .
87
3.3.4
Barycentric interpolation formula. . . . . . . . . . . . . . . .
90
3.3.5
Trigonometric interpolation and FFT . . . . . . . . . . . .
93
3.4
Piecewise linear interpolation . . . . . . . . . . . . . . . . . . . . . . . . .
98
3.5
Approximation by spline functions . . . . . . . . . . . . . . . . . . . . . 100
3.6
The least-squares method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
3.7
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
3.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
4
Numerical diﬀerentiation and integration . . . . . . . . . . . . . . 113
4.1
Some representative problems . . . . . . . . . . . . . . . . . . . . . . . . . 113
4.2
Approximation of function derivatives . . . . . . . . . . . . . . . . . . 115
4.3
Numerical integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
4.3.1
Midpoint formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
4.3.2
Trapezoidal formula . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.3.3
Simpson formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
4.4
Interpolatory quadratures . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
4.5
Simpson adaptive formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
4.6
Monte Carlo Methods for Numerical Integration . . . . . . . . . 131
4.7
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
4.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
5
Linear systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
5.1
Some representative problems . . . . . . . . . . . . . . . . . . . . . . . . . 137
5.2
Linear system and complexity . . . . . . . . . . . . . . . . . . . . . . . . . 142
5.3
The LU factorization method . . . . . . . . . . . . . . . . . . . . . . . . . 143
5.4
The pivoting technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
5.4.1
The ﬁll-in of a matrix . . . . . . . . . . . . . . . . . . . . . . . . . 157
5.5
How accurate is the solution of a linear system? . . . . . . . . . 158
5.6
How to solve a tridiagonal system . . . . . . . . . . . . . . . . . . . . . 162

Contents
XIII
5.7
Overdetermined systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
5.8
What is hidden behind the MATLAB command \ . . . . . . 166
5.9
Iterative methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
5.9.1
How to construct an iterative method . . . . . . . . . . . . 169
5.10 Richardson and gradient methods . . . . . . . . . . . . . . . . . . . . . 174
5.11 The conjugate gradient method . . . . . . . . . . . . . . . . . . . . . . . 177
5.12 When should an iterative method be stopped? . . . . . . . . . . 180
5.13 To wrap-up: direct or iterative? . . . . . . . . . . . . . . . . . . . . . . . 182
5.14 What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
5.15 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
6
Eigenvalues and eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . 193
6.1
Some representative problems . . . . . . . . . . . . . . . . . . . . . . . . . 194
6.2
The power method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
6.2.1
Convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 199
6.3
Generalization of the power method . . . . . . . . . . . . . . . . . . . 201
6.4
How to compute the shift. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
6.5
Computation of all the eigenvalues. . . . . . . . . . . . . . . . . . . . . 206
6.6
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
6.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
7
Numerical optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
7.1
Some representative problems . . . . . . . . . . . . . . . . . . . . . . . . . 214
7.2
Unconstrained optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
7.3
Derivative free methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
7.3.1
Golden section and quadratic interpolation
methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
7.3.2
Nelder and Mead method . . . . . . . . . . . . . . . . . . . . . . 223
7.4
The Newton method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
7.5
Descent (or line search) methods . . . . . . . . . . . . . . . . . . . . . . 228
7.5.1
Descent directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
7.5.2
Strategies for choosing the steplength αk . . . . . . . . . 231
7.5.3
The descent method with Newton’s directions . . . . . 237
7.5.4
Descent methods with quasi-Newton directions . . . . 238
7.5.5
Gradient and conjugate gradient
descent methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
7.6
Trust region methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
7.7
The nonlinear least squares method . . . . . . . . . . . . . . . . . . . . 248
7.7.1
Gauss-Newton method . . . . . . . . . . . . . . . . . . . . . . . . . 249
7.7.2
Levenberg-Marquardt’s method . . . . . . . . . . . . . . . . . 252
7.8
Constrained optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
7.8.1
The penalty method . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
7.8.2
The augmented Lagrangian method. . . . . . . . . . . . . . 264
7.9
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
7.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268

XIV
Contents
8
Ordinary diﬀerential equations . . . . . . . . . . . . . . . . . . . . . . . . 271
8.1
Some representative problems . . . . . . . . . . . . . . . . . . . . . . . . . 271
8.2
The Cauchy problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
8.3
Euler methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
8.3.1
Convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 278
8.4
The Crank-Nicolson method . . . . . . . . . . . . . . . . . . . . . . . . . . 282
8.5
Zero-stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
8.6
Stability on unbounded intervals . . . . . . . . . . . . . . . . . . . . . . 286
8.6.1
The region of absolute stability . . . . . . . . . . . . . . . . . 289
8.6.2
Absolute stability controls perturbations . . . . . . . . . 290
8.6.3
Stepsize adaptivity for the forward Euler
method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
8.7
High order methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
8.8
The predictor-corrector methods . . . . . . . . . . . . . . . . . . . . . . 305
8.9
Systems of diﬀerential equations . . . . . . . . . . . . . . . . . . . . . . . 307
8.10 Some examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
8.10.1 The spherical pendulum . . . . . . . . . . . . . . . . . . . . . . . . 313
8.10.2 The three-body problem . . . . . . . . . . . . . . . . . . . . . . . 317
8.10.3 Some stiﬀproblems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
8.11 What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
8.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
9
Numerical approximation of boundary-value
problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
9.1
Some representative problems . . . . . . . . . . . . . . . . . . . . . . . . . 330
9.2
Approximation of boundary-value problems . . . . . . . . . . . . . 332
9.2.1
Finite diﬀerence approximation of the
one-dimensional Poisson problem . . . . . . . . . . . . . . . . 333
9.2.2
Finite diﬀerence approximation of a
convection-dominated problem . . . . . . . . . . . . . . . . . . 336
9.2.3
Finite element approximation of the
one-dimensional Poisson problem . . . . . . . . . . . . . . . . 337
9.2.4
Finite diﬀerence approximation of the
two-dimensional Poisson problem . . . . . . . . . . . . . . . . 341
9.2.5
Consistency and convergence of ﬁnite diﬀerence
discretization of the Poisson problem . . . . . . . . . . . . 347
9.2.6
Finite diﬀerence approximation of the
one-dimensional heat equation . . . . . . . . . . . . . . . . . . 348
9.2.7
Finite element approximation of the
one-dimensional heat equation . . . . . . . . . . . . . . . . . . 352
9.3
Hyperbolic equations: a scalar pure advection
problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
9.3.1
Finite diﬀerence discretization of the scalar
transport equation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357

Contents
XV
9.3.2
Finite diﬀerence analysis for the scalar transport
equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
9.3.3
Finite element space discretization of the scalar
advection equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
9.4
The wave equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
9.4.1
Finite diﬀerence approximation of the wave
equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369
9.5
What we haven’t told you . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
9.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374
10 Solutions of the exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
10.1 Chapter 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
10.2 Chapter 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
10.3 Chapter 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
10.4 Chapter 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
10.5 Chapter 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
10.6 Chapter 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
10.7 Chapter 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
10.8 Chapter 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
10.9 Chapter 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435


Index of MATLAB and Octave programs
All the programs introduced in this text can be downloaded from
http://mox.polimi.it/qs
2.1
bisection: bisection method . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
2.2
newton: Newton method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
2.3
newtonsys: Newton method for nonlinear systems . . . . . . . . .
53
2.4
aitken: Aitken method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
2.5
horner: synthetic division algorithm . . . . . . . . . . . . . . . . . . . . .
69
2.6
newtonhorner: Newton-H¨orner method . . . . . . . . . . . . . . . . .
71
3.1
barycentric: barycentric interpolation . . . . . . . . . . . . . . . . . . .
92
3.2
cubicspline: interpolating cubic spline . . . . . . . . . . . . . . . . . . . 101
4.1
midpointc: composite midpoint quadrature formula . . . . . . . 120
4.2
simpsonc: composite Simpson quadrature formula . . . . . . . . . 122
4.3
simpadpt: adaptive Simpson formula . . . . . . . . . . . . . . . . . . . . 130
5.1
lugauss: Gauss LU factorization . . . . . . . . . . . . . . . . . . . . . . . . 149
5.2
itermeth: general iterative method. . . . . . . . . . . . . . . . . . . . . . 171
6.1
eigpower: power method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
6.2
invshift: inverse power method with shift . . . . . . . . . . . . . . . . 202
6.3
gershcircles: Gershgorin circles . . . . . . . . . . . . . . . . . . . . . . . . . 203
6.4
qrbasic: method of QR iterations . . . . . . . . . . . . . . . . . . . . . . . 207
7.1
golden: golden section method . . . . . . . . . . . . . . . . . . . . . . . . . 221
7.2
backtrack: backtracking strategy . . . . . . . . . . . . . . . . . . . . . . . 234
7.3
descent: descent method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
7.4
trustregion: trust region method . . . . . . . . . . . . . . . . . . . . . . . 246
7.5
gaussnewton: Gauss-Newton method . . . . . . . . . . . . . . . . . . . 250
7.6
penalty: penalty method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
7.7
auglagrange: augmented Lagrangian method . . . . . . . . . . . . . 265
8.1
feuler: forward Euler method . . . . . . . . . . . . . . . . . . . . . . . . . . 276
8.2
beuler: backward Euler method . . . . . . . . . . . . . . . . . . . . . . . . 277
XVII

XVIII
Index of MATLAB and Octave programs
8.3
cranknic: Crank-Nicolson method . . . . . . . . . . . . . . . . . . . . . . 283
8.4
predcor: predictor-corrector method . . . . . . . . . . . . . . . . . . . . . 306
8.5
feonestep: one step of the forward Euler method. . . . . . . . . . 307
8.6
beonestep: one step of the backward Euler method . . . . . . . 307
8.7
cnonestep: one step of the Crank-Nicolson method . . . . . . . . 307
8.8
newmark: Newmark method . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
8.9
fvinc: forcing term for the spherical pendulum problem . . . . . 316
8.10 threebody: forcing term for the simpliﬁed three
body system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
9.1
bvp: approximation of a two-point
diﬀusion-convection-reaction problem by the ﬁnite
diﬀerence method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
9.2
poissonfd: approximation of the Poisson problem with
Dirichlet boundary data by the ﬁve-point ﬁnite
diﬀerence method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
9.3
heattheta: θ-method for the one-dimensional
heat equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
9.4
newmarkwave: Newmark method for the wave equation . . . 369
10.1 gausslegendre: Gauss-Legendre composite quadrature
formula, with n = 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
10.2 rk2: Heun (or RK2) method . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
10.3 rk3: explicit Runge-Kutta method of order 3 . . . . . . . . . . . . . . 416
10.4 neumann: numerical solution of a Neumann boundary-value
problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
10.5 hyper: Lax-Friedrichs, Lax-Wendroﬀand upwind schemes . . . 427

1
What can’t be ignored
In this book we will systematically use elementary mathematical con-
cepts which the reader should know already, yet he or she might not
recall them immediately.
We will therefore use this chapter to refresh them and we will con-
dense notions which are typical of courses in Calculus, Linear Algebra
and Geometry, yet rephrasing them in a way that is suitable for use in
Scientiﬁc Computing. At the same time we will introduce new concepts
which pertain to the ﬁeld of Scientiﬁc Computing and we will begin to ex-
plore their meaning and usefulness with the help of MATLAB (MATrix
LABoratory), an integrated environment for programming and visual-
ization. We shall also use GNU Octave (in short, Octave), an intepreter
for a high-level language mostly compatible with MATLAB which is
distributed under the terms of the GNU GPL free-software license and
which reproduces a large part of the numerical facilities of MATLAB.
In Section 1.1 we will give a quick introduction to MATLAB and
Octave, while we will present the elements of programming in Section
1.7. However, we refer the interested readers to [Att11] for a description
of the MATLAB language and to [EBH08] for a description of Octave.
1.1 The MATLAB and Octave environments
MATLAB and Octave are integrated environments for Scientiﬁc Com-
puting and visualization. They are written mostly in C and C++ lan-
guages.
MATLAB is distributed by The MathWorks (see the website www.
mathworks.com). The name stands for MATrix LABoratory since origi-
nally it was developed for matrix computation.
Octave, also known as GNU Octave (see the website www.octave.
org), is a freely redistributable software. It can be redistributed and/or
A. Quarteroni et al., Scientiﬁc Computing with MATLAB and Octave,
Texts in Computational Science and Engineering 2,
DOI 10.1007/978-3-642-45367-0 1, © Springer-Verlag Berlin Heidelberg 2014
1

2
1 What can’t be ignored
modiﬁed under the terms of the GNU General Public License (GPL) as
published by the Free Software Foundation.
There are diﬀerences between MATLAB and Octave environments,
languages and toolboxes (i.e. a collection of special-purpose MATLAB
functions). However, there is a level of compatibility that allows us to
write most programs of this book and run them seamlessly both in MAT-
LAB and Octave. When this is not possible, either because some com-
mands are spelt diﬀerently, or because they operate in a diﬀerent way,
or merely because they are just not implemented, a note will be written
at the end of each section to provide an explanation and indicate what
could be done.
Through the book, we shall often make use of the expression “MAT-
LAB command”: in this case, MATLAB should be understood as the
language which is the common subset of both programs MATLAB and
Octave.
Just as MATLAB has its toolboxes, Octave has a richful set of func-
tions available through a project called Octave-forge (see the website
octave.sourceforge.net). This function repository grows steadily in
many diﬀerent areas. Some functions we use in this book don’t belong
to the Octave core, nevertheless they can be downloaded by the website
octave.sourceforge.net.
Once installed, the execution of MATLAB or Octave yield the access
to a working environment characterized by the prompt >> or octave:1>,
>>
octave:1>
respectively. For instance, when executing MATLAB on our personal
computer, the following message is generated:
< M A T L A B (R) >
Copyright 1984-2013 The MathWorks, Inc.
R2013b (8.2.0.701) 64-bit (glnxa64)
August 13, 2013
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
>>
When executing Octave on our personal computer we read the following
text:
GNU Octave, version 3.6.4
Copyright (C) 2013 John W. Eaton and others.
This is free software; see the source code for copying
conditions. There is ABSOLUTELY NO WARRANTY; not even
for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
For details, type ‘warranty’.
Octave was configured for "x86_64-unknown-linux-gnu".
Additional information about Octave is available at
http://www.octave.org.

1.2 Real numbers
3
Please contribute if you find this software useful.
For more information, visit
http://www.octave.org/get-involved.html
Read http://www.octave.org/bugs.html to learn how to
submit bug reports.
For information about changes from previous versions,
type ‘news’.
octave:1>
In this chapter we will use the prompt >>, however, from Chapter 2
on the prompt will be always neglected in order to simplify notations.
1.2 Real numbers
While the set R of real numbers is known to everyone, the way in which
computers treat them is perhaps less well known. On one hand, since
machines have limited resources, only a subset F of ﬁnite dimension of
R can be represented. The numbers in this subset are called normalized
ﬂoating-point numbers. On the other hand, as we shall see in Section
1.2.2, F is characterized by properties that are diﬀerent from those of R.
The reason is that any real number x is in principle truncated by the
machine, giving rise to a new number (called the ﬂoating-point number),
denoted by fl(x), which does not necessarily coincide with the original
number x.
1.2.1 How we represent them
To become acquainted with the diﬀerences between R and F, let us make
a few experiments which illustrate the way that a computer deals with
real numbers. Note that whether we use MATLAB or Octave rather
than another language is just a matter of convenience. The results of
our calculation, indeed, depend primarily on the manner in which the
computer works, and only to a lesser degree on the programming lan-
guage. Let us consider the rational number x = 1/7, whose decimal
representation is 0.142857. This is an inﬁnite representation, since the
number of decimal digits is inﬁnite. To get its computer representation,
let us introduce after the prompt the ratio 1/7 and obtain
>> 1/7
ans =
0.1429
which is a number with only four decimal digits, the last being diﬀerent
from the fourth digit of the original number.
Should we now consider 1/3 we would ﬁnd 0.3333, so the fourth dec-
imal digit would now be exact. This behavior is due to the fact that real

4
1 What can’t be ignored
numbers are rounded on the computer. This means, ﬁrst of all, that only
an a priori ﬁxed number of decimal digits are returned, and moreover
the last decimal digit which appears is increased by unity whenever the
ﬁrst disregarded decimal digit is greater than or equal to 5.
The ﬁrst remark to make is that using only four decimal digits to
represent real numbers is questionable. Indeed, the internal representa-
tion of the number is made of as many as 16 decimal digits, and what we
have seen is simply one of several possible MATLAB output formats.
The same number can take diﬀerent expressions depending upon the
speciﬁc format declaration that is made. For instance, for the number
1/7, some possible output formats are available in MATLAB:
format
format short
yields 0.1429,
format short e
”
1.4286e −01,
format short g
”
0.14286,
format long
”
0.142857142857143,
format long e
”
1.428571428571428e−01,
format long g
”
0.142857142857143.
The same formats are available in Octave, but the yielded results do not
necessarily coincide with those of MATLAB:
format short
yields 0.14286,
format short e
”
1.4286e −01,
format short g
”
0.14286,
format long
”
0.142857142857143,
format long e
”
1.42857142857143e−01,
format long g
”
0.142857142857143.
Obviously, these diﬀerences, even if slight, will imply possible diﬀerent
results in the treatment of our examples.
Some of these formats are more coherent than others with the internal
computer representation. As a matter of fact, in general a computer
stores a real number in the following way
x = (−1)s · (0.a1a2 . . . at) · βe = (−1)s · m · βe−t,
a1 ̸= 0
(1.1)
where s is either 0 or 1, β (a positive integer larger than or equal to 2)
is the basis adopted by the speciﬁc computer at hand, m is an integer
called the mantissa whose length t is the maximum number of digits ai
(with 0 ≤ai ≤β −1) that are stored, and e is an integral number called
the exponent. The format long e is the one which most resembles this
representation, and e stands for exponent; its digits, preceded by the
sign, are reported to the right of the character e. The numbers whose
form is given in (1.1) are called ﬂoating-point numbers, since the position

1.2 Real numbers
5
of the decimal point is not ﬁxed. The digits a1a2 . . . ap (with p ≤t) are
often called the p ﬁrst signiﬁcant digits of x.
The condition a1 ̸= 0 ensures that a number cannot have multiple
representations. For instance, without this restriction the number 1/10
could be represented (in the decimal basis) as 0.1 · 100, but also as 0.01 ·
101, etc..
The set F is therefore fully characterized by the basis β, the number
of signiﬁcant digits t and the range (L, U) (with L < 0 and U > 0) of
variation of the index e. Thus it is denoted as F(β, t, L, U). For instance,
in MATLAB we have F = F(2, 53, −1021, 1024) (indeed, 53 signiﬁcant
digits in basis 2 correspond to the 15 signiﬁcant digits that are shown by
MATLAB in basis 10 with the format long). Floating point numbers
of F(2, 53, −1021, 1024) are stored in registers of 8 Bytes, more precisely
the sign s is stored in 1 bit, the exponent e in 11 bits, and the mantissa
m in 52 bits. Note that, although we have 52 bits for m, we can count
t = 53 digits when β = 2. As a matter of fact, since the ﬁrst digit a1 of
every ﬂoating point number must be diﬀerent from 0, when β = 2 it is
worthless to store it as it must necessarily be 1. The digits a2, . . . , a53
are therefore stored in the 52 bits associated with m.
Fortunately, the roundoﬀerror that is inevitably generated whenever
a real number x ̸= 0 is replaced by its representative fl(x) in F, is small,
since
|x −fl(x)|
|x|
≤1
2ϵM
(1.2)
where ϵM = β1−t, which is called machine epsilon, provides the distance
between 1 and its closest ﬂoating-point number greater than 1. Note that
ϵM depends on β and t. For instance, in MATLAB ϵM can be obtained
through the command eps, and we obtain ϵM = 2−52 ≃2.22 ·10−16. Let
eps
us point out that in (1.2) we estimate the relative error on x, which is
undoubtedly more meaningful than the absolute error |x −fl(x)|. As a
matter of fact, the latter doesn’t account for the order of magnitude of
x whereas the former does.
The number u = 1
2ϵM is the maximum relative error that the com-
puter can make while representing a real number by ﬁnite arithmetic.
For this reason, it is sometimes named roundoﬀunity.
Number 0 does not belong to F, as in that case we would have a1 = 0
in (1.1): it is therefore handled separately. Moreover, L and U being
ﬁnite, one cannot represent numbers whose absolute value is either arbi-
trarily large or arbitrarily small. Precisely, the smallest and the largest
positive real numbers of F are given respectively by
xmin = βL−1, xmax = βU(1 −β−t).

6
1 What can’t be ignored
In MATLAB these values can be obtained through the commands
realmin and realmax, yielding
realmin
realmax
xmin = 2.225073858507201 · 10−308,
xmax = 1.797693134862316 · 10+308.
A positive number smaller than xmin produces a message of under-
ﬂow and is treated either as 0 or in a special way (see, e.g., [QSS07],
Chapter 2). A positive number greater than xmax yields instead a mes-
sage of overﬂow and is stored in the variable Inf (which is the computer
Inf
representation of +∞).
The elements in F are more dense near xmin, and less dense while
approaching xmax. As a matter of fact, the number in F nearest to xmax
(to its left) and the one nearest to xmin (to its right) are, respectively
x−
max = 1.797693134862315 · 10+308,
x+
min = 2.225073858507202 · 10−308.
Thus x+
min −xmin ≃10−323, while xmax −x−
max ≃10292 (!). However,
the relative distance is small in both cases, as we can infer from (1.2).
1.2.2 How we operate with ﬂoating-point numbers
Since F is a proper subset of R, elementary algebraic operations on
ﬂoating-point numbers do not enjoy all the properties of analogous op-
erations on R. Precisely, commutativity still holds for addition (that is
fl(x + y) = fl(y + x)) as well as for multiplication (fl(xy) = fl(yx)),
but other properties such as associativity and distributivity are violated.
Moreover, 0 is no longer unique. Indeed, let us assign the variable a the
value 1, and execute the following instructions:
>> a = 1; b=1; while a+b ~= a; b=b/2; end
The variable b is halved at every step as long as the sum of a and b
remains diﬀerent (~=) from a. Should we operate on real numbers, this
program would never end, whereas in our case it ends after a ﬁnite
number of steps and returns the following value for b: 1.1102e-16=
ϵM/2. There exists therefore at least one number b diﬀerent from 0 such
that a+b=a. This is possible since F is made up of isolated numbers; when
adding two numbers a and b with b<a and b less than ϵM, we always
obtain that a+b is equal to a. The MATLAB number a+eps(a) is the
smallest number in F larger than a. Thus the sum a+b will return a for
all b < eps(a).
Associativity is violated whenever a situation of overﬂow or underﬂow
occurs. Take for instance a=1.0e+308, b=1.1e+308 and c=-1.001e+308,
and carry out the sum in two diﬀerent ways. We ﬁnd that
a + (b + c) = 1.0990e + 308, (a + b) + c = Inf.

1.2 Real numbers
7
-1.5
-1
-0.5
0
0.5
1
1.5 x 10
-14
Figure 1.1. Oscillatory behavior of the function (1.3) caused by cancellation
errors
This is a particular instance of what occurs when one adds two num-
bers with opposite sign but similar absolute value. In this case the result
may be quite inexact and the situation is referred to as loss, or cancel-
lation, of signiﬁcant digits. For instance, let us compute ((1 + x) −1)/x
(the obvious result being 1 for any x ̸= 0):
>> x =
1.e-15; ((1+x)-1)/x
ans =
1.1102
This result is rather imprecise, the relative error being larger than 11%!
Another case of numerical cancellation is encountered while evaluat-
ing the function
f(x) = x7 −7x6 + 21x5 −35x4 + 35x3 −21x2 + 7x −1
(1.3)
at 401 equispaced points with abscissa in [1 −2 · 10−8, 1 + 2 · 10−8]. We
obtain the chaotic graph reported in Figure 1.1 (the real behavior is that
of (x−1)7, which is substantially constant and equal to the null function
in such a tiny neighborhood of x = 1). The MATLAB commands that
have generated this graph will be illustrated in Section 1.5.
Finally, it is interesting to notice that in F there is no place for
indeterminate forms such as 0/0 or ∞/∞. Their presence produces what
is called not a number (NaN in MATLAB or in Octave), for which the
NaN
normal rules of calculus do not apply.
Remark 1.1 Whereas it is true that roundoﬀerrors are usually small, when
repeated within long and complex algorithms, they may give rise to catas-
trophic eﬀects. Two outstanding cases concern the explosion of the Ariane
missile on June 4, 1996, engendered by an overﬂow in the computer on board,

8
1 What can’t be ignored
5
10
15
20
25
30
10
-10
10
-8
10
-6
10
-4
10
-2
10
0
Figure 1.2. Relative error |π −zn|/π versus n
and the failure of the mission of an American Patriot missile, during the Gulf
War in 1991, because of a roundoﬀerror in the computation of its trajectory.
An example with less catastrophic (but still troublesome) consequences is
provided by the sequence
z2 = 2, zn+1 = 2n−1/2
1 −√1 −41−nz2n,
n = 2, 3, . . .
(1.4)
which converges to π when n tends to inﬁnity. (This sequence is a revised form
of the better known formula of Fran¸cois Vi`ete (french mathematician of the
XVI century) for the approximation of π [Bec71].)
When MATLAB is used to compute zn, the relative error found between
π and zn decreases for the 16 ﬁrst iterations, then grows because of roundoﬀ
errors (as shown in Figure 1.2).
.
■
See the Exercises 1.1-1.2.
1.3 Complex numbers
Complex numbers, whose set is denoted by C, have the form z = x + iy,
where i = √−1 is the imaginary unit (that is i2 = −1), while x = Re(z)
and y = Im(z) are the real and imaginary part of z, respectively. They
are generally represented on the computer as pairs of real numbers.
Unless redeﬁned otherwise, MATLAB variables i as well as j denote
the imaginary unit. To introduce a complex number with real part x and
imaginary part y, one can just write x+i*y; as an alternative, one can
use the command complex(x,y). Let us also mention the exponential
complex
and the trigonometric representations of a complex number z, that are
equivalent thanks to the Euler formula
z = ρeiθ = ρ(cos θ + i sin θ);
(1.5)
ρ =

x2 + y2 is the modulus of the complex number (it can be obtained
by setting abs(z)) while θ is its argument, that is the angle between the
abs

1.3 Complex numbers
9
  1
  2
  3
  4
  5
30
210
60
240
90
270
120
300
150
330
180
0
Figure 1.3. Output of the MATLAB command compass
x axis and the straight line issuing from the origin and passing from the
point of coordinate x, y in the complex plane. θ can be found by typing
angle(z). The representation (1.5) is therefore:
angle
abs(z)*( cos(angle(z))+i*sin(angle(z))).
The graphical polar representation of one or more complex numbers
can be obtained through the command compass(z), where z is either
compass
a single complex number or a vector whose components are complex
numbers. For instance, by typing
>> z = 3+i*3; compass (z);
one obtains the graph reported in Figure 1.3.
For any given complex number z, one can extract its real part with
the command real(z) and its imaginary part with imag(z). Finally, the
real imag
complex conjugate ¯z = x −iy of z, can be obtained by simply writing
conj(z).
conj
In MATLAB all operations are carried out by implicitly assuming
that the operands as well as the result are complex. We may therefore
ﬁnd some apparently surprising results. For instance, if we compute the
cube root of −5 with the MATLAB command (-5)^(1/3), instead of
−1.7100 . . . we obtain the complex number 0.8550 + 1.4809i. (We antic-
ipate the use of the symbol ^ for the power exponent.) As a matter of
^
fact, all numbers of the form ρei(θ+2kπ), with k an integer, are indistin-
guishable from z = ρeiθ. By computing the complex roots of z of order
three, we ﬁnd
3√ρei(θ/3+2kπ/3), that is, the three distinct roots
z1 =
3√ρeiθ/3,
z2 =
3√ρei(θ/3+2π/3),
z3 =
3√ρei(θ/3+4π/3).
MATLAB will select the one that is encountered by spanning the com-
plex plane counterclockwise beginning from the real axis. Since the polar

10
1 What can’t be ignored
Re(z)
Im(z)
z1
z2
z3
π
3
3√ρ
Figure 1.4. Representation in the complex plane of the three complex cube
roots of the real number −5
representation of z = −5 is ρeiθ with ρ = 5 and θ = π, the three roots
are (see Figure 1.4 for their representation in the Gauss plane)
z1 =
3√
5(cos(π/3) + i sin(π/3)) ≃0.8550 + 1.4809i,
z2 =
3√
5(cos(π) + i sin(π)) ≃−1.7100,
z3 =
3√
5(cos(−π/3) + i sin(−π/3)) ≃0.8550 −1.4809i.
The ﬁrst root is the one which is selected.
Finally, by (1.5) we obtain
cos(θ) = 1
2

eiθ + e−iθ
,
sin(θ) = 1
2i

eiθ −e−iθ
.
(1.6)
1.4 Matrices
Let n and m be positive integers. A matrix with m rows and n columns
is a set of m×n elements aij, with i = 1, . . . , m, j = 1, . . . , n, represented
by the following table:
A =
⎡
⎢⎢⎢⎣
a11 a12 . . . a1n
a21 a22 . . . a2n
...
...
...
am1 am2 . . . amn
⎤
⎥⎥⎥⎦.
(1.7)
In compact form we write A = (aij). Should the elements of A be real
numbers, we write A ∈Rm×n, and A ∈Cm×n if they are complex.
Square matrices of dimension n are those with m = n. A matrix
featuring a single column is a column vector, whereas a matrix featuring
a single row is a row vector.

1.4 Matrices
11
In order to introduce a matrix in MATLAB one has to write the
elements from the ﬁrst to the last row, introducing the character ; to
separate the diﬀerent rows. For instance, the command
>> A = [ 1 2 3; 4 5 6]
produces
A =
1
2
3
4
5
6
that is, a 2 × 3 matrix whose elements are indicated above. The m × n
matrix zeros(m,n) has all null entries, eye(m,n) has all null entries
zeros
eye
unless aii, i = 1, . . . , min(m, n), on the diagonal that are all equal to 1.
The n×n identity matrix is obtained with the command eye(n) (which
is an abridged version of eye(n,n)): its elements are δij = 1 if i = j,
0 otherwise, for i, j = 1, . . . , n. Finally, by the command A=[ ] we can
[ ]
initialize an empty matrix.
We recall the following matrix operations:
1. if A = (aij) and B = (bij) are m × n matrices, the sum of A and B
is the matrix A + B = (aij + bij);
2. the product of a matrix A by a real or complex number λ is the
matrix λA = (λaij);
3. the product of two matrices is possible only for compatible sizes,
precisely if A is m × p and B is p × n, for some positive integer p. In
that case C = AB is an m × n matrix whose elements are
cij =
p

k=1
aikbkj,
for i = 1, . . . , m, j = 1, . . . , n.
Here is an example of the sum and product of two matrices.
>> A=[1 2 3; 4 5 6];
>> B=[7 8 9; 10 11 12];
>> C=[13
14; 15 16; 17 18];
>> A+B
ans =
8
10
12
14
16
18
>> A*C
ans =
94
100
229
244
Note that MATLAB returns a diagnostic message when one tries to
carry out operations on matrices with incompatible dimensions. For in-
stance:
>> A=[1 2 3; 4 5 6];
>> B=[7 8 9; 10 11 12];
>> C=[13
14; 15 16; 17 18];

12
1 What can’t be ignored
>> A+C
??? Error using ==> plus
Matrix
dimensions
must
agree.
>> A*B
??? Error using ==> mtimes
Inner matrix
dimensions
must
agree.
If A is a square matrix of dimension n, its inverse (provided it exists)
is a square matrix of dimension n, denoted by A−1, which satisﬁes the
matrix relation AA−1 = A−1A = I. We can obtain A−1 through the
command inv(A). The inverse of A exists iﬀthe determinant of A, a
inv
number denoted by det(A) and computed by the command det(A), is
det
non-zero. The latter condition is satisﬁed iﬀthe column vectors of A are
linearly independent (see Section 1.4.1). The determinant of a square
matrix is deﬁned by the following recursive formula (Laplace rule):
det(A) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
a11
if n = 1,
n

j=1
Δijaij, for n > 1, ∀i = 1, . . . , n,
(1.8)
where Δij = (−1)i+jdet(Aij) and Aij is the matrix obtained by eliminat-
ing the ith row and jth column from matrix A. (The result is independent
of the row index i.) In particular, if A ∈R2×2 one has
det(A) = a11a22 −a12a21,
while if A ∈R3×3 we obtain
det(A) = a11a22a33 + a31a12a23 + a21a13a32
−a11a23a32 −a21a12a33 −a31a13a22.
We recall that if A = BC, then det(A) = det(B)det(C).
To invert a 2×2 matrix and compute its determinant we can proceed
as follows:
>> A=[1 2; 3 4];
>> inv(A)
ans =
-2.0000
1.0000
1.5000
-0.5000
>> det(A)
ans =
-2
Should a matrix be singular, MATLAB returns a diagnostic message,
followed by a matrix whose elements are all equal to Inf, as illustrated
by the following example:

1.4 Matrices
13
>> A=[1 2; 0 0];
>> inv(A)
Warning : Matrix is singular
to working
precision .
ans =
Inf
Inf
Inf
Inf
For special classes of square matrices, the computation of inverses and
determinants is rather simple. In particular, if A is a diagonal matrix, i.e.
one for which only the diagonal elements akk, k = 1, . . . , n, are non-zero,
its determinant is given by det(A) = a11a22 · · · ann. In particular, A is
non-singular iﬀakk ̸= 0 for all k. In such a case the inverse of A is still
a diagonal matrix with elements a−1
kk .
Let v be a vector of dimension n. The command diag(v) produces
diag
a diagonal matrix whose elements are the components of vector v. The
more general command diag(v,m) yields a square matrix of dimension
n+abs(m) whose mth upper diagonal (i.e. the diagonal made of elements
with indices i, i + m) has elements equal to the components of v, while
the remaining elements are null. Note that this extension is valid also
when m is negative, in which case the only aﬀected elements are those of
lower diagonals.
For instance if v = [1 2 3] then:
>> A=diag (v,-1)
A =
0
0
0
0
1
0
0
0
0
2
0
0
0
0
3
0
Other special cases are the upper triangular and lower triangular
matrices. A square matrix of dimension n is lower (respectively, upper)
triangular if all elements above (respectively, below) the main diagonal
are zero. Its determinant is simply the product of the diagonal elements.
Through the commands tril(A) and triu(A), one can extract from
tril
triu
the matrix A of dimension n its lower and upper triangular part. Their
extensions tril(A,m) or triu(A,m), with m ranging from -n and n,
allow the extraction of the triangular part augmented by, or deprived of,
extradiagonals.
For instance, given the matrix A =[3 1 2; -1 3 4; -2 -1 3], by the
command L1=tril(A) we obtain
L1 =
3
0
0
-1
3
0
-2
-1
3
while, by L2=tril(A,1), we obtain
L2 =
3
1
0
-1
3
4
-2
-1
3

14
1 What can’t be ignored
We recall that if A ∈Rm×n its transpose AT ∈Rn×m is the matrix
obtained by interchanging rows and columns of A. When n = m and A =
AT the matrix A is called symmetric. Finally, A’ denotes the transpose
A’
of A if A is real, or its conjugate transpose (that is, AH) if A is complex. A
square complex matrix that coincides with its conjugate transpose AH
is called hermitian.
Octave 1.1 Also Octave returns a diagnostic message when one tries
to carry out operations on matrices having non-compatible dimensions.
If we repeat the previous MATLAB examples we obtain:
octave :1> A=[1 2 3; 4 5 6];
octave :2> B=[7 8 9; 10 11 12];
octave :3> C=[13
14; 15 16; 17 18];
octave :4> A+C
error: operator +: nonconformant arguments (op1 is
2x3, op2 is 3x2)
octave :5> A*B
error: operator *: nonconformant arguments (op1 is
x3, op2 is 2x3)
■
1.4.1 Vectors
Vectors will be indicated in boldface; precisely, v will denote a column
vector whose ith component is denoted by vi. When all components are
real numbers we can write v ∈Rn.
In MATLAB, vectors are regarded as particular cases of matrices.
To introduce a column vector one has to insert between square brackets
the values of its components separated by semi-colons, whereas for a row
vector it suﬃces to write the component values separated by blanks or
commas. For instance, through the instructions v = [1;2;3] and w =
[1 2 3] we initialize the column vector v and the row vector w, both
of dimension 3. The command zeros(n,1)(respectively, zeros(1,n))
produces a column (respectively, row) vector of dimension n with null
elements, which we will denote by 0. Similarly, the command ones(n,1)
ones
generates the column vector, denoted with 1, whose components are all
equal to 1.
A system of vectors {y1, . . . , ym} is linearly independent if the rela-
tion
α1y1 + . . . + αmym = 0
implies that all coeﬃcients α1, . . . , αm are null. A system B = {y1, . . . ,
yn} of n linearly independent vectors in Rn (or Cn) is a basis for Rn (or
Cn), that is, any vector w in Rn can be written as a linear combination
of the elements of B,

1.4 Matrices
15
w =
n

k=1
wkyk,
for a unique possible choice of the coeﬃcients {wk}. The latter are called
the components of w with respect to the basis B. For instance, the canon-
ical basis of Rn is the set of vectors {e1, . . . , en}, where ei has its ith
component equal to 1, and all other components equal to 0 and is the
one which is normally used.
The scalar product of two vectors v, w ∈Rn is deﬁned as
(v, w) = wT v =
n

k=1
vkwk,
{vk} and {wk} being the components of v and w, respectively. The
corresponding command is w’*v or else dot(v,w), where now the apex
dot
denotes transposition of the vector. For a vector v with complex compo-
nents, v’ denotes its conjugate transpose vH, that is a row-vector whose
v’
components are the complex conjugate ¯vk of vk.
The length (or modulus) of a vector v is given by
∥v∥=

(v, v) =




n

k=1
v2
k
and can be computed through the command norm(v); ∥v∥is also said
norm
euclidean norm of the vector v.
The vector product between two vectors v, w ∈R3, v × w or v ∧w,
is the vector u ∈R3 orthogonal to both v and w whose modulus is
|u| = |v| |w| sin(α), where α is the smaller angle formed by v and w. It
can be obtained by the command cross(v,w).
cross
The visualization of a vector can be obtained by the MATLAB com-
mand quiver in R2 and quiver3 in R3.
quiver
quiver3
.*
./ .^
The MATLAB command x.*y, x./y or x.^2 indicates that these
operations should be carried out component by component. For instance
if we deﬁne the vectors
>> x = [1; 2; 3]; y = [4; 5; 6];
the instruction
>> y’*x
ans =
32
provides their scalar product, while
>> x.*y
ans =
4
10
18

16
1 What can’t be ignored
returns a vector whose ith component is equal to xiyi. Note that the
product y*x is not well-deﬁned.
Finally, we recall that a vector v ∈Cn, with v ̸= 0, is an eigenvector
of a matrix A ∈Cn×n associated with the complex number λ if
Av = λv.
The complex number λ is called eigenvalue of A. In general, the com-
putation of eigenvalues is quite diﬃcult. Exceptions are represented by
diagonal and triangular matrices, whose eigenvalues are their diagonal
elements.
See the Exercises 1.3-1.6.
1.5 Real functions
This section deals with manipulation of real functions. More particularly,
for a given function f deﬁned on an interval (a, b), we aim at computing
its zeros, its integral and its derivative, as well as drawing its graph.
Let us consider a real function, for example f(x) = 1/(1 + x2); we
are going to show the MATLAB instructions to deﬁne it, evaluate it at
a point (or on a set of points), and plot it.
The simplest way to deﬁne a mathematical function consists in using
anonymous function and function handle @ as follows:
@
>> fun=@(x) 1/(1+x^2)
and we can evaluate f at x = 3 by the instruction
>> y=fun (3)
y =
0.1000
An anonymous function is a function that is not stored in a pro-
gram ﬁle, but is associated with a variable whose data type is a function
handle, that is a MATLAB standard variable that provides a means
of calling a function. Function handles can be passed in calls to other
MATLAB functions.
The common syntax to create a handle associated with an anonymous
function reads
>> fun=@(arg1 , arg2 ,..., argn)expr
where fun is the function handle, arg1, arg2,...,argn are the inde-
pendent variables of the anonymous function, while expr contains the
expression of the anonymous function we want to deﬁne; it could be
included between round or square brackets.
Some parameters can be used inside expr, even if they do not appear
in the list of variables (arg1, arg2,...,argn). If this is the case, such

1.5 Real functions
17
parameters must be set before the function deﬁnition. For example, to
evaluate f(x) = a/(1 + x2) at x = 2 and with a = 3, we write:
>> a=3; fun=@(x) a/(1+ x^2); y=fun (2)
and the result is
y =
0.6000
To modify the value of the parameter a, e.g. to set a = 8, we have to
deﬁne again the function handle fun, otherwise MATLAB holds on the
value a=3 in fun. Then we must write
>> a=8; fun=@(x) a/(1+ x^2); y=fun (2)
y =
1.6000
The command fplot(fun,lims) plots the graph of the function asso-
fplot
ciated with the function handle fun on the interval (lims(1), lims(2)).
For instance, to represent f(x) = 1/(1 + x2) on the interval (−5, 5), we
can write
>> fun =@(x) 1/(1+x^2);
lims =[-5,5];
fplot(fun ,lims );
or, more directly, by invoking the anonymous function without function
handle,
>> fplot(@(x) 1/(1+x^2),[-5
5]);
In MATLAB the graph is obtained by sampling the function on a
set of non-equispaced abscissae and reproduces the true graph of f with
a tolerance of 0.2%. To improve the accuracy we could use the command
>> fplot(fun,lims,tol,n,LineSpec)
where tol indicates the desired tolerance and the parameter n(≥1)
ensures that the function will be plotted with a minimum of n+1 points.
LineSpec is a string specifying the style or the color of the line used for
plotting the graph. For example, LineSpec=’--’ is used for a dashed
line, LineSpec=’r-.’ for a red dashed-dotted line, etc. To use default
values for tol, n or LineSpec one can pass empty matrices ([ ]).
By writing grid on after the command fplot, we can obtain the
grid
background-grid as that in Figure 1.1.
An alternative way for deﬁning mathematical functions consists in
writing MATLAB functions, also called user-deﬁned functions. (See
Sect. 1.7.2.) For instance, we can write the following instructions
function y=fun(x)
y=1/(1+x^2);
end
and save them into the ﬁle fun.m. It is suggested (although not manda-
tory) that the ﬁlename coincides with the name written in the ﬁrst row
of the ﬁle itself. As a matter of fact, should we save the above three rows

18
1 What can’t be ignored
in a ﬁle with a diﬀerent name, e.g. funct.m, MATLAB will recognise
the user-deﬁned function funct, but it will not be able to locate fun.
To plot on the interval [−π, π] the function deﬁned in fun.m, we can
use one of the following commands:
>> fplot(@fun ,[-pi ,pi])
or
>> fplot(’fun ’,[-pi ,pi])
(The special character @ generates the function handle associated
with function fun.)
If the variable x is an array, the operations /, * and ^ acting on ar-
rays have to be replaced by the corresponding dot operations ./, .*
and .^ which operate component-wise. For instance, the instruction
fun=@(x)[1/(1+x^2)] is replaced by fun=@(x)[1./(1+x.^2)].
The command plot can be used as alternative to fplot, provided
plot
that the mathematical function has been evaluated on a set of abscissa.
The following instructions
>> x=linspace ( -2 ,3 ,100);
>> y=exp(x).*( sin(x).^2) -0.4;
>> plot (x,y,’c’,’Linewidth ’ ,2); grid on
produce a graph in linear scale, precisely the command linspace(a,b,n)
linspace
generates a row array of n equispaced points from a to b, while the com-
mand plot(x,y,’c’,’Linewidth’,2) creates a linear piecewise curve
connecting the points (xi, yi) (for i = 1, . . . , n) with a cyan line width of
2 points.
Remark 1.2 Function handles can also be associated with vector functions.
In that case, the common syntax to deﬁne arrays is used, e.g. spaces or commas
are used to separate diﬀerent elements of a row, while semicolons to split
rows. For example, to deﬁne the vector function g : R2 →R2, with g(x, y) =
[ex sin(y), x2 −y]t we can use the command
>> g=@(x,y) [exp(x).* sin(y); x.^2-y]
In order to prevent wrong function deﬁnitions, we warn the reader that
blanks should be avoided when unnecessary. For instance, by writing f(x) =
2x −sin(x) introducing a blank between 2*x and -sin(x)
>> f=@(x) [2*x
-sin(x)],
the evaluation
>> y=f(pi /2)
provides the row arrow
y=
3.1416
-1.0000
instead of the scalar value y=3.1416. Actually, the space between 2*x and
-sin(x) is interpreted as separation character and this means that the above

1.5 Real functions
19
instruction deﬁnes the vector function f : R →R2 f(x) = [2x, sin(x)] instead
of f(x) = 2x −sin(x).
■
1.5.1 The zeros
We recall that if f(α) = 0, α is called zero of f or root of the equation
f(x) = 0. A zero is simple if f ′(α) ̸= 0, multiple otherwise.
From the graph of a function one can infer (within a certain tolerance)
which are its real zeros. The direct computation of all zeros of a given
function is not always possible. For functions which are polynomials with
real coeﬃcients of degree n, that is, of the form
pn(x) = a0 + a1x + a2x2 + . . . + anxn =
n

k=0
akxk,
ak ∈R, an ̸= 0,
we can obtain the only zero α = −a0/a1, when n = 1 (i.e. p1 represents
a straight line), or the two zeros, α+ and α−, when n = 2 (this time p2
represents a parabola) α± = (−a1 ±

a2
1 −4a0a2)/(2a2).
However, there are no explicit formulae for the zeros of an arbitrary
polynomial pn when n ≥5.
In what follows we will denote with Pn the space of polynomials of
degree less than or equal to n,
pn(x) =
n

k=0
akxk
(1.9)
where the ak are given coeﬃcients, real or complex.
Also the number of zeros of a function cannot in general be deter-
mined a priori. An exception is provided by polynomials, for which the
number of zeros (real or complex) coincides with the polynomial degree.
Moreover, should α = x + iy with y ̸= 0 be a zero of a polynomial
with degree n ≥2, if ak are real coeﬃcients, then its complex conjugate
¯α = x −iy is also a zero.
To compute in MATLAB one zero of a function fun, near a given
value x0, either real or complex, the command fzero(fun,x0) can be
fzero
used. The result is an approximate value of the desired zero, and also the
interval in which the search was made. Alternatively, using the command
fzero(fun,[x0 x1]), a zero of fun is searched for in the interval whose
endpoints are x0,x1, provided f changes sign between x0 and x1.
Let us consider, for instance, the function f(x) = x2−1+ex. Looking
at its graph we see that there are two zeros in (−1, 1). To compute them
we need to execute the following commands:
>> fun=@(x)[x^2 - 1 + exp(x)];
>> fzero(fun ,-1)

20
1 What can’t be ignored
ans =
-0.7146
>> fzero(fun ,1)
ans =
5.4422e-18
Alternatively, after noticing from the function plot that one zero is
in the interval [−1, −0.2] and another in [−0.2, 1], we could have written
>> fzero(fun ,[-1
-0.2])
ans =
-0.7146
>> fzero(fun ,[ -0.2 1])
ans =
-5.2609e-17
The result obtained for the second zero is slightly diﬀerent than the
one obtained previously, due to a diﬀerent initialization of the algorithm
implemented in fzero. In Chapter 2 we will introduce and investigate
several methods for the approximate computation of the zeros of an
arbitrary function.
If fun is deﬁned by a user-deﬁned function, we can choose one be-
tween these two calls:
>> fzero(@fun ,1)
or
>> fzero(’fun ’, 1)
Octave 1.2 When the command fzero(fun,x0) is used being x0 a
scalar value, Octave suggests to use the function fsolve.
■
1.5.2 Polynomials
Polynomials are very special functions for whose treatment special
MATLAB functions are available. The command polyval is apt to
polyval
evaluate a polynomial at one or several points. Its input arguments are
a vector p and a vector x, where the components of p are the polyno-
mial coeﬃcients stored in decreasing order, from an down to a0, and
the components of x are the abscissae where the polynomial needs to be
evaluated. The result can be stored in a vector y by writing
>> y = polyval(p,x)
For instance, the values of p(x) = x7+3x2−1, at the equispaced abscissae
xk = −1+k/4 for k = 0, . . . , 8, can be obtained by proceeding as follows:

1.5 Real functions
21
>> p = [1 0 0 0 0 3 0 -1]; x = [-1:0.25:1];
>> y = polyval(p,x)
y =
Columns 1 through 5:
1.00000
0.55402
-0.25781
-0.81256
-1.00000
Columns 6 through 9:
-0.81244
-0.24219
0.82098
3.00000
Alternatively, one could use anonymous functions and function han-
dles. However, in such case one should provide the entire analytic expres-
sion of the polynomial in the input string, and not simply its coeﬃcients.
The program roots provides an approximation of the zeros of a poly-
roots
nomial and requires only the input of the vector p. For instance, we can
compute the zeros of p(x) = x3 −6x2 + 11x −6 by writing
>> p = [1
-6 11
-6];
format
long ;
>> roots(p)
ans =
3.00000000000000
2.00000000000000
1.00000000000000
Unfortunately, the result is not always that accurate. For instance,
for the polynomial p(x) = (x + 1)7, whose unique zero is α = −1 with
multiplicity 7, we ﬁnd (quite surprisingly)
>> p = [1 7
21 35
35
21
7
1];
>> roots(p)
ans =
-1.0101
-1.0063 + 0.0079i
-1.0063 - 0.0079i
-0.9977 + 0.0099i
-0.9977 - 0.0099i
-0.9909 + 0.0044i
-0.9909 - 0.0044i
In fact, numerical methods for the computation of the polynomial
roots with multiplicity larger than one are particularly subject to round-
oﬀerrors (see Section 2.8.2).
The command p=conv(p1,p2) returns the coeﬃcients of the poly-
conv
nomial given by the product of two polynomials whose coeﬃcients are
contained in the vectors p1 and p2.
Similarly, the command [q,r]=deconv(p1,p2) provides the coeﬃcients
deconv
of the polynomials obtained on dividing p1 by p2, i.e. p1 = conv(p2,q)
+ r. In other words, q and r are the quotient and the remainder of the
division.

22
1 What can’t be ignored
Table 1.1. MATLAB commands for polynomial operations
command
yields
y=polyval(p,x)
y = values of p(x)
z=roots(p)
z = roots of p such that p(z) = 0
p=conv(p1,p2)
p = coeﬃcients of the polynomial p1p2
[q,r]=deconv(p1,p2)
q = coeﬃcients of q, r = coeﬃcients of r
such that p1 = qp2 + r
y=polyder(p)
y = coeﬃcients of p′(x)
y=polyint(p)
y = coeﬃcients of
x

0
p(t) dt
Let us consider for instance the product and the ratio between the
two polynomials p1(x) = x4 −1 and p2(x) = x3 −1 :
>> p1 = [1 0 0 0
-1];
>> p2 = [1 0 0
-1];
>> p=conv (p1 ,p2)
p =
1
0
0
-1
-1
0
0
1
>> [q,r]= deconv(p1 ,p2)
q =
1
0
r =
0
0
0
1
-1
We therefore ﬁnd the polynomials p(x) = p1(x)p2(x) = x7 −x4 −x3 + 1,
q(x) = x and r(x) = x −1 such that p1(x) = q(x)p2(x) + r(x).
The commands polyint(p) and polyder(p) provide respectively the
polyint
polyder
coeﬃcients of the primitive (vanishing at x = 0) and those of the deriva-
tive of the polynomial whose coeﬃcients are given by the components of
the vector p.
If x is a vector of abscissae and p (respectively, p1 and p2) is a vector
containing the coeﬃcients of a polynomial p (respectively, p1 and p2),
the previous commands are summarized in Table 1.1.
A further command, polyfit, allows the computation of the n+ 1 poly-
polyfit
nomial coeﬃcients of a polynomial p of degree n once the values attained
by p at n + 1 distinct nodes are available (see Section 3.3.1).
1.5.3 Integration and diﬀerentiation
The following two results will often be invoked throughout this book:
1. the fundamental theorem of integration: if f is a continuous function
in [a, b), then
F(x) =
x

a
f(t) dt
∀x ∈[a, b),

1.5 Real functions
23
is a diﬀerentiable function, called a primitive of f, which satisﬁes,
F ′(x) = f(x)
∀x ∈[a, b);
2. the ﬁrst mean-value theorem for integrals: if f is a continuous func-
tion in [a, b) and x1, x2 ∈[a, b) with x1 < x2, then ∃ξ ∈(x1, x2) such
that
f(ξ) =
1
x2 −x1
x2

x1
f(t) dt.
Even when it does exist, a primitive might be either impossible to
determine or diﬃcult to compute. For instance, knowing that ln |x| is a
primitive of 1/x is irrelevant if one doesn’t know how to eﬃciently com-
pute the logarithms. In Chapter 4 we will introduce several methods to
compute the integral of an arbitrary continuous function with a desired
accuracy, irrespectively of the knowledge of its primitive.
We recall that a function f deﬁned on an interval [a, b] is diﬀerentiable
in a point ¯x ∈(a, b) if the following limit exists and is ﬁnite
f ′(¯x) = lim
h→0
1
h(f(¯x + h) −f(¯x)).
(1.10)
The value of f ′(¯x) provides the slope of the tangent line to the graph
of f at the point ¯x.
We say that a function which is continuous together with its deriva-
tive at any point of [a, b] belongs to the space C1([a, b]). More generally,
a function with continuous derivatives up to the order p (a positive in-
teger) is said to belong to Cp([a, b]). In particular, C0([a, b]) denotes the
space of continuous functions in [a, b].
A result that will be often used is the mean-value theorem, according
to which, if f ∈C0([a, b]) and it is diﬀerentiable in (a, b), there exists
ξ ∈(a, b) such that
f ′(ξ) = (f(b) −f(a))/(b −a).
Finally, it is worth recalling that a function that is continuous with
all its derivatives up to the order n in a neighborhood of x0, can be
approximated in such a neighborhood by the so-called Taylor polynomial
of degree n at the point x0:
Tn(x) = f(x0) + (x −x0)f ′(x0) + . . . + 1
n!(x −x0)nf (n)(x0)
=
n

k=0
(x −x0)k
k!
f (k)(x0).

24
1 What can’t be ignored
Figure 1.5. Graphical interface of the command funtool
The MATLAB toolbox symbolic provides the commands diff, int,
diff int
and taylor which allow us to obtain the analytical expression of the
taylor
derivative, the indeﬁnite integral (i.e. a primitive) and the Taylor poly-
nomial, respectively, of a given function.
First of all, the variable x must be declared symbolic by the command
syms x. This will allow its algebraic manipulation without specifying its
syms
value.
Therefore, having deﬁned the expression f of the function on which
we intend to operate, diff(f,n) provides its derivative of order n,
int(f) its indeﬁnite integral, and taylor(f,’Order’,n+1) the asso-
ciated Taylor polynomial of degree n in a neighborhood of x0 = 0.
In order to do this for the function f(x) = (x2 + 2x + 2)/(x2 −1), we
proceed as follows:
>> syms x
>> f = (x^2+2*x+2)/(x^2-1) ;
>> diff (f)
ans =
(2*x+2)/(x^2 -1) -(2*x*(x^2+2*x+2))/(x^2 -1)^2
>> int(f)
ans =
x+(5* log(x -1))/2 - log(x+1)/2
>> taylor(f,’Order’ ,6)
ans =
-2*x^5-3*x^4-2*x^3-3*x^2-2*x-2
We observe that using the command simple it is possible to simplify
simple
the expressions generated by diff, int and taylor in order to make
them as simple as possible. The command funtool, by the graphical
funtool
interface illustrated in Fig. 1.5, allows a very easy symbolic manipulation
of arbitrary functions.

1.6 To err is not only human
25
xn =

k
φ(tk)αk
x =
T

0
φ(t)dt
x
MP
PP
NP
xph
em
et
ea
ec
Figure 1.6. Types of errors in a computational process
Octave 1.3 In Octave symbolic calculations can be performed by the
Octave-Forge Symbolic package. Note, however, that the syntax of
Octave-Forge is not in general compatible with that of the MATLAB
symbolic toolbox.
■
See the Exercises 1.7-1.8.
1.6 To err is not only human
As a matter of fact, by re-phrasing the Latin motto errare humanum est,
we might say that in numerical computation to err is even inevitable.
As we have seen, the simple fact of using a computer to represent real
numbers introduces errors. What is therefore important is not to strive
to eliminate errors, but rather to be able to control their eﬀect.
Generally speaking, we can identify several levels of errors that oc-
cur during the approximation and resolution of a physical problem (see
Figure 1.6).
At the highest level stands the error em which occurs when forcing
the physical reality (PP stands for physical problem and xph denotes
its solution) to obey some mathematical model (MP, whose solution is
x). Such errors will limit the applicability of the mathematical model to
certain situations and are beyond the control of Scientiﬁc Computing.
The mathematical model (whether expressed by an integral as in the
example of Figure 1.6, an algebraic or diﬀerential equation, a linear or

26
1 What can’t be ignored
nonlinear system) is generally not solvable in explicit form. Its resolu-
tion by computer algorithms will surely involve the introduction and
propagation of roundoﬀerrors at least. Let’s call these errors ea.
On the other hand, it is often necessary to introduce further errors
since any procedure of the mathematical model involving an inﬁnite
sequence of arithmetic operations cannot be performed by the computer
unless approximately. For instance the computation of the sum of a series
will necessarily be accomplished in an approximate way by considering
a suitable truncation.
It will therefore be necessary to introduce a numerical problem, NP,
whose solution xn diﬀers from x by an error et which is called trunca-
tion error. Such errors do not only occur in mathematical models that
are already set in ﬁnite dimension (for instance, when solving a linear
system). The sum of the errors ea and et constitutes the computational
error ec, the quantity we are interested in.
The absolute computational error is the diﬀerence between x, the
exact solution of the mathematical model, and x, the solution obtained
at the end of the numerical process,
eabs
c
= |x −x|,
while (if x ̸= 0) the relative computational error is
erel
c
= |x −x|/|x|,
where | · | denotes the modulus, or other measure of size, depending on
the meaning of x.
The numerical process is generally an approximation of the math-
ematical model obtained as a function of a discretization parameter,
which we will refer to as h and suppose positive. If, as h tends to 0,
the numerical process returns the solution of the mathematical model,
we will say that the numerical process is convergent. Moreover, if the
(absolute or relative) error can be bounded as a function of h as
ec ≤Chp
(1.11)
where C is independent of h and p is a positive number, we will say
that the method is convergent of order p. It is sometimes even possible
to replace the symbol ≤with ≃, in the case where, besides the upper
bound (1.11), a lower bound C′hp ≤ec is also available (C′ being another
constant independent of h and p).
Example 1.1 Suppose we approximate the derivative of a function f at a
point ¯x with the incremental ratio that appears in (1.10). Obviously, if f is
diﬀerentiable at ¯x, the error committed by replacing f ′ by the incremental
ratio tends to 0 as h →0. However, as we will see in Section 4.2, the error can
be considered as Ch only if f ∈C2 in a neighborhood of ¯x.
■

1.6 To err is not only human
27
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
1
1
1
2
0
0.02
0.04
0.06
0.08
0.1
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
Figure 1.7. Plot of the same data in log-log scale (left) and in linear-linear
scale (right)
While studying the convergence properties of a numerical procedure
we will often deal with graphs reporting the error as a function of h in a
logarithmic scale, which shows log(h) on the abscissae axis and log(ec)
on the ordinates axis. The purpose of this representation is easy to see:
if ec = Chp then log ec = log C + p log h. In logarithmic scale therefore
p represents the slope of the straight line log ec, so if we must compare
two methods, the one presenting the greater slope will be the one with
a higher order. (The slope will be p = 1 for ﬁrst-order methods, p = 2
for second-order methods, and so on.) To obtain graphs in a logarithmic
scale one just needs to type loglog(x,y), x and y being the vectors
loglog
containing the abscissae and the ordinates of the data to be represented.
As an instance, in Figure 1.7, left, we report the straight lines relative
to the behavior of the errors in two diﬀerent methods. The continuous
line represents a ﬁrst-order approximation, while the dashed line repre-
sents a second-order one. In Figure 1.7, right, we show the same data
plotted on the left, but now using the plot command, that is a linear
scale for both x−and y−axis. It is evident that the linear representation
of these data is not optimal, since the dashed curve appears thickened on
the x−axis when x ∈[10−6, 10−2], even if the corresponding ordinates
range from 10−12 to 10−4, spanning 8 orders of magnitude.
There is an alternative to the graphical way of establishing the order
of a method when one knows the errors ei relative to some given values
hi of the parameter of discretization, with i = 1, . . . , N: it consists in
conjecturing that ei is equal to Chp
i , where C does not depend on i. One
can then approach p with the values:
pi = log(ei/ei−1)/ log(hi/hi−1),
i = 2, . . . , N.
(1.12)
Actually the error is not a computable quantity since it depends on
the unknown solution. Therefore it is necessary to introduce computable
quantities that can be used to estimate the error itself, the so called error
estimator. We will see some examples in Sections 2.3.1, 2.6 and 4.5.

28
1 What can’t be ignored
Sometimes, instead of using the log-log scale, we will use the semi-
logarithmic one, i.e. logarithmic scale on the y-axis and linear scale on
the x-axis. This representation is preferable, for instance, in plotting the
error of an iterative method versus the iterations, as done in Figure 1.2,
or in general, when the ordinates span a wider interval than abscissae.
Let us consider the following 3 sequences, all converging to
√
2:
x0 = 1,
xn+1 = 3
4xn +
1
2xn
,
n = 0, 1, . . .,
y0 = 1,
yn+1 = 1
2yn + 1
yn
,
n = 0, 1, . . .,
z0 = 1,
zn+1 = 3
8zn +
3
2zn
−
1
2z3n
,
n = 0, 1, . . ..
In Figure 1.8 we plot the errors ex
n = |xn −
√
2|/
√
2 (solid line), ey
n =
|yn −
√
2|/
√
2 (dashed line) and ez
n = |zn −
√
2|/
√
2 (dashed-dotted line)
versus iterations and in semi-logarithmic scale. It is possible to prove
that
ex
n ≃ρn
xex
0,
ey
n ≃ρn2
y ey
0,
ez
n ≃ρn3
z ez
0,
where ρx, ρy, ρz ∈(0, 1), thus, by applying the logarithm only to the
ordinates, we have
log(ex
n) ≃C1 + log(ρx)n,
log(ey
n) ≃C2 + log(ρy)n2,
log(ez
n) ≃C3 + log(ρz)n3,
i.e., a straight line, a parabola and a cubic, respectively, exactly as we
can see in Figure 1.8, left.
The MATLAB command for semi-logharitmic scale is semilogy(x,y),
semilogy
where x and y are arrays of the same size.
In Figure 1.8, right, we display the errors ex
n, ey
n and ez
n versus iterations,
in linear-linear scale and by using the command plot. It is evident that
the use of semi-logarithmic instead of linear-linear scale is more appro-
priate.
1.6.1 Talking about costs
In general a problem is solved on the computer by an algorithm, which
is a precise directive in the form of a ﬁnite text specifying the execution
of a ﬁnite series of elementary operations. We are interested in those
algorithms which involve only a ﬁnite number of steps.
The computational cost of an algorithm is the number of ﬂoating-
point operations that are required for its execution. Often, the speed
of a computer is measured by the maximum number of ﬂoating-point
operations which the computer can execute in one second (ﬂops). In

1.6 To err is not only human
29
0
10
20
30
40
50
10
−15
10
−10
10
−5
10
0
0
10
20
30
40
50
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Figure 1.8. Errors ex
n (solid line), ey
n (dashed line) and ez
n (dashed-dotted line)
in semi-logarithmic scale (left) and linear-linear scale (right)
particular, the following abridged notations are commonly used: Mega-
ﬂops, equal to 106 flops, Giga-ﬂops equal to 109 flops, Tera-ﬂops equal
to 1012 flops, Peta-ﬂops equal to 1015 flops. The fastest computer nowa-
days (1st of the top500 supercomputer list as of November 2013) reaches
as many as 33 Peta-ﬂops and is the Tianhe-2 (MilkyWay-2) Cluster, Intel
Xeon 2.200GHz, of National University of Defense Technology, China.
In general, the exact knowledge of the number of operations required
by a given algorithm is not essential. Rather, it is useful to determine
its order of magnitude as a function of a parameter d which is related to
the problem dimension. We therefore say that an algorithm has constant
complexity if it requires a number of operations independent of d, i.e.
O(1) operations, linear complexity if it requires O(d) operations, or,
more generally, polynomial complexity if it requires O(dm) operations,
for a positive integer m. Other algorithms may have exponential (O(cd)
operations) or even factorial (O(d!) operations) complexity. We recall
that the symbol O(dm) means “it behaves, for large d, like a constant
times dm”.
Example 1.2 (Matrix-vector product) Le A be a square matrix of order
n and let v be a vector of Rn. The jth component of the product Av is given
by
aj1v1 + aj2v2 + . . . + ajnvn,
and requires n products and n −1 additions. One needs therefore n(2n −1)
operations to compute all the components. Thus this algorithm requires O(n2)
operations, so it has a quadratic complexity with respect to the parameter n.
The same algorithm would require O(n3) operations to compute the product of
two square matrices of order n. However, there is an algorithm, due to Strassen,
which requires “only” O(nlog2 7) operations and another, due to Winograd and
Coppersmith, requiring O(n2.376) operations.
■
Example 1.3 (Computation of a matrix determinant) As already men-
tioned, the determinant of a square matrix of order n can be computed us-
ing the recursive formula (1.8). The corresponding algorithm has a factorial

30
1 What can’t be ignored
complexity with respect to n and would be usable only for matrices of small
dimension. For instance, if n = 24, a computer capable of performing as many
as 1 Peta-ﬂops (i.e. 1015 ﬂoating-point operations per second) would require 59
years to carry out this computation. One has therefore to resort to more eﬃ-
cient algorithms. Indeed, there exists an algorithm allowing the computation of
determinants through matrix-matrix products, with henceforth a complexity
of O(nlog2 7) operations by applying the Strassen algorithm previously men-
tioned (see [BB96]).
■
The number of operations is not the sole parameter which matters
in the analysis of an algorithm. Another relevant factor is represented
by the time that is needed to access the computer memory (which de-
pends on the way the algorithm has been coded). An indicator of the
performance of an algorithm is therefore the CPU time (CPU stands
for central processing unit), and can be obtained using the MATLAB
command cputime. The total elapsed time between the input and output
cputime
phases can be obtained by the command etime.
etime
Example 1.4 In order to compute the time needed for a matrix-vector mul-
tiplication we set up the following program:
>> n=10000;
step =100;
>> A=rand (n,n); v=rand (n ,1);
>> T=[ ]; sizeA=[ ];
>> for k = 500: step :n
AA = A(1:k,1:k); vv = v(1:k);
t = cputime ;
b = AA*vv;
tt = cputime - t;
T = [T, tt]; sizeA = [sizeA ,k];
end
The instruction a:step:b appearing in the for cycle generates all numbers
a:step:b
having the form a+step*k where k is an integer ranging from 0 to the largest
value kmax for which a+step*kmax is not greater than b (in the case at hand,
a=500, b=10000 and step=100). The command rand(n,m) deﬁnes an n×m ma-
rand
trix of random entries. Finally, T is the vector whose components contain the
CPU time needed to carry out every single matrix-vector product, whereas
cputime returns the CPU time in seconds that has been used by the MAT-
LAB process since MATLAB started. The time necessary to execute a single
program is therefore the diﬀerence between the actual CPU time and the one
computed before the execution of the current program which is stored in the
variable t. Figure 1.9, which is obtained by the command plot(sizeA,T,’o’),
shows that the CPU time grows like the square of the matrix order n.
■
1.7 The MATLAB language
After the introductory remarks of the previous section, we are now ready
to work in either the MATLAB or Octave environments. As said above,

1.7 The MATLAB language
31
0
2000
4000
6000
8000
10000
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Figure 1.9. Matrix-vector product: the CPU time (in seconds) versus the
dimension n of the matrix (on an Intel
R
⃝CoreTM2 Duo, 2.53 GHz processor)
from now on MATLAB should be understood as the subset of commands
which are common to both MATLAB and Octave.
After pressing the enter key (or else return), all what is written af-
ter the prompt will be interpreted.1 Precisely, MATLAB will ﬁrst check
whether what is written corresponds either to variables which have al-
ready been deﬁned or to the name of one of the programs or commands
deﬁned in MATLAB. Should all those checks fail, MATLAB returns
an error warning. Otherwise, the command is executed and an output
will possibly be displayed. In all cases, the system eventually returns the
prompt to acknowledge that it is ready for a new command. To close a
quit
exit
MATLAB session one should write the command quit (or else exit)
and press the enter key. From now it will be understood that to execute
a program or a command one has to press the enter key. Moreover, the
terms program, function or command will be used in an equivalent man-
ner. When our command coincides with one of the elementary structures
characterizing MATLAB (e.g. a number or a string of characters that
are put between apices) they are immediately returned in output in the
default variable ans (abbreviation of answer). Here is an example:
ans
>>
’home ’
ans =
home
If we now write a diﬀerent string (or number), ans will assume this
new value.
We can turn oﬀthe automatic display of the output by writing a
semicolon after the string. Thus if we write ’home’; MATLAB will
simply return the prompt (yet assigning the value ’home’ to the variable
ans).
1 Thus a MATLAB program does not necessarily have to be compiled as
other languages do, e.g. Fortran or C.

32
1 What can’t be ignored
More generally, the command = allows the assignment of a value (or
=
a string of characters) to a given variable. For instance, to assign the
string ’Welcome to Milan’ to the variable a we can write
>> a=’Welcome to Milan’;
Thus there is no need to declare the type of a variable, MATLAB
will do it automatically and dynamically. For instance, should we write
a=5, the variable a will now contain a number and no longer a string
of characters. This ﬂexibility is not cost-free. If we set a variable named
quit equal to the number 5 we are inhibiting the use of the MATLAB
command quit. We should therefore try to avoid using variables having
the name of MATLAB commands. However, by the command clear
clear
followed by the name of a variable (e.g. quit), it is possible to cancel
this assignment and restore the original meaning of the command quit.
By the command save all the session variables (that are stored in
save
the so-called base workspace) are saved in the binary ﬁle matlab.mat.
Similarly, the command load restores in the current session all variables
load
stored in matlab.mat. A ﬁle name can be speciﬁed after save or load.
One can also save only selected variables, say v1, v2 and v3, in a given
ﬁle named, e.g., area.mat, using the command save area v1 v2 v3.
By the command help one can see the whole family of commands
help
and pre-deﬁned variables, including the so-called toolboxes which are sets
of specialized commands. Among them let us recall those which deﬁne
the elementary functions such as sine (sin(a)), cosine (cos(a)), square
sin cos
sqrt exp
root (sqrt(a)), exponential (exp(a)).
There are special characters that cannot appear in the name of a
variable or in a command, for instance the algebraic operators (+, -,
+ - * /
& | ˜
* and /), the logical (or boolean) operators and (&), or (|), not (˜),
the relational operators greater than (>), greater than or equal to (>=),
less than (<), less than or equal to (<=), equal to (==). Finally, a name
> >= <
<= ==
can never begin with a digit, and it cannot contain a bracket or any
punctuation mark.
1.7.1 MATLAB statements
A special programming language, the MATLAB language, is also avail-
able enabling the users to write new programs. Although its knowledge
is not required for understanding how to use the several programs which
we will introduce throughout this book, it may provide the reader with
the capability of modifying them as well as producing new ones.
The MATLAB language features standard statements, such as con-
ditionals and loops.
The if-elseif-else conditional has the following general form:
if <condition 1>
<statement
1.1>
<statement
1.2>

1.7 The MATLAB language
33
...
elseif <condition
2>
<statement
2.1>
<statement
2.2>
...
...
else
<statement n.1>
<statement n.2>
...
end
where <condition 1>, <condition 2>, ... represent MATLAB sets of
logical expressions, with values 0 or 1 (false or true) and the entire con-
struction allows the execution of that statement corresponding to the
condition taking value equal to 1. Should all conditions be false, the ex-
ecution of <statement n.1>, <statement n.2>, ... will take place. In
fact, if the value of <condition k> is zero, the statements <statement
k.1>, <statement k.2>, ... are not executed and the control moves
on.
For instance, to compute the roots of a quadratic polynomial ax2 +
bx + c one can use the following instructions (the command
disp(.) simply displays what is written between brackets):
disp
>> if
a
~= 0
sq = sqrt(b*b - 4*a*c);
x(1) = 0.5*(-b + sq)/a;
x(2) = 0.5*(-b - sq)/a;
elseif
b
~= 0
x(1) = -c/b;
elseif
c
~= 0
disp(’ Impossible
equation’);
else
disp(’ The
given
equation
is
an
identity’);
end
(1.13)
Note that MATLAB does not execute the entire construction until the
statement end is typed.
Logical expressions that appear inside conditional statements can
be obtained by combining elementary logical expressions using boolean
operators &, |, &&, and ||. The two latter operators && and || implement
the short-circuiting capability of the corresponding element-by-element
ones & and |. As a matter of fact, element-by-element boolean operators
are often suﬃcient for performing most logical operations. However, it
is sometimes desirable to stop evaluating a logical expression as soon as
the overall truth value can be determined.
More precisely, the second operand of the logical expression (expr1
&& expr2) (or (expr1 || expr2)) is evaluated only if the result is not

34
1 What can’t be ignored
fully determined by the ﬁrst one. For instance, let us consider the fol-
lowing expression
>> (nit
<= nmax ) & (err > tol)
MATLAB ﬁrst computes the value of both operands and then the result
of the boolean operation and. Instead, by the instruction
>> (nit
<= nmax ) && (err > tol)
MATLAB computes the value of the ﬁrst operand and, only if it is equal
to 1 (that is true), MATLAB computes the value of the second one.
MATLAB allows two types of loops, a for-loop (comparable to a
for
while
Fortran do-loop or a C for-loop) and a while-loop. A for-loop repeats the
statements in the loop as the loop index takes on the values in a given
row vector. For instance, to compute the ﬁrst six terms of the Fibonacci
sequence fi = fi−1 + fi−2, for i ≥3, with f1 = 0 and f2 = 1, one can
use the following instructions:
>> f(1) = 0; f(2) = 1;
>> for i = [3 4 5 6]
f(i) = f(i-1) + f(i-2);
end
Note that a semicolon can be used to separate several MATLAB instruc-
tions typed on the same line. Also, note that we can replace the second
instruction by the equivalent >> for i = 3:6. The while-loop repeats
as long as the given condition is true. For instance, the following set of
instructions can be used as an alternative to the previous set:
>> f(1) = 0; f(2) = 1; k = 3;
>> while k <= 6
f(k) = f(k-1) + f(k-2); k = k + 1;
end
Other statements of perhaps less frequent use exist, such as switch,
case, otherwise. The interested reader can have access to their meaning
by the help command.
1.7.2 Programming in MATLAB
Let us now explain brieﬂy how to write MATLAB programs. A new
program must be put in a ﬁle with a given name with extension m, which
is called m-ﬁle. They must be located in one of the directories in which
MATLAB automatically searches for m-ﬁles; their list can be obtained
by the command path (see help path to learn how to add a directory
path
to this list). The ﬁrst directory scanned by MATLAB is the current
working directory.
It is important at this level to distinguish between scripts and user-
deﬁned functions. A script is simply a collection of MATLAB commands
in an m-ﬁle and can be used interactively. For instance, the set of instruc-
tions (1.13) can give rise to a script (which we could name equation)

1.7 The MATLAB language
35
by copying it in the ﬁle equation.m. To launch it, one can simply write
the instruction equation after the MATLAB prompt >>. We report two
examples below:
>> a = 1; b = 1; c = 1;
>> equation
>> x
x =
-0.5000 + 0.8660i
-0.5000
- 0.8660i
>> a = 0; b = 1; c = 1;
>> equation
>> x
x =
-1
Since we have no input/output interface, all variables used in a script
are also the variables of the working session and are therefore cleared
only upon an explicit command (clear). This is not at all satisfactory
when one intends to write complex programs involving many temporary
variables and comparatively fewer input and output variables, which are
the only ones that can be eﬀectively saved once the execution of the
program is terminated. Much more ﬂexible than scripts are functions.
A user-deﬁned function (in brief, function) is still deﬁned in a m-
ﬁle, e.g. name.m, but it has a well deﬁned input/output interface that is
introduced by the command function as follows
function
function [out1 ,..., outn ]= name (in1 ,...,inm)
where out1,...,outn are the output variables and in1,...,inm are the
input variables.
The following ﬁle, called det23.m, deﬁnes a new function called det23
which computes, according to the formulae given in Section 1.4, the
determinant of a matrix whose dimension could be either 2 or 3:
function
det=det23(A)
%DET23 computes
the determinant
of a square matrix
% of dimension
2 or 3
[n,m]= size (A);
if n==m
if n==2
det = A(1 ,1)*A(2,2)- A(2 ,1)*A(1 ,2);
elseif n == 3
det = A(1 ,1)* det23(A([2 ,3] ,[2 ,3])) -...
A(1 ,2)* det23(A([2 ,3] ,[1 ,3]))+...
A(1 ,3)* det23(A([2 ,3] ,[1 ,2]));
else
disp (’ Only 2x2 or 3x3 matrices
’);
end
else
disp (’ Only
square
matrices
’);
end
return

36
1 What can’t be ignored
Notice the use of the continuation characters ... meaning that the
...
instruction is continuing on the next line and the character % to begin
%
comments. The instruction A([i,j],[k,l]) allows the construction of
a 2 × 2 matrix whose elements are the elements of the original matrix A
lying at the intersections of the ith and jth rows with the kth and lth
columns.
When a function is invoked, MATLAB creates a local workspace
(the function’s workspace). The commands in the function cannot refer
to variables from the base (interactive) workspace unless they are passed
as input.2 In particular, variables used in a function are erased when the
execution terminates, unless they are returned as output parameters.
Functions usually terminate when the end of the function is reached,
however a return statement can be used to force an early return (upon
return
the fulﬁllment of a certain condition).
For instance, in order to approximate the golden section number α =
1.6180339887 . . ., which is the limit for k →∞of the quotient of two
consecutive Fibonacci numbers fk/fk−1, by iterating until the diﬀerence
between two consecutive ratios is less than 10−4, we can construct the
following function:
function [golden ,k]= fibonacci0
% FIBONACCI0 : Golden
section
number
approximation
f(1) = 0; f(2) = 1; goldenold
= 0;
kmax = 100;
tol = 1.e-04;
for k = 3: kmax
f(k) = f(k-1) + f(k -2);
golden = f(k)/f(k -1);
if abs(golden - goldenold ) < tol
return
end
goldenold
= golden;
end
return
Its execution is interrupted either after kmax=100 iterations or when
the absolute value of the diﬀerence between two consecutive iterates is
smaller than tol=1.e-04. Then, we can write
>> [alpha ,niter]= fibonacci0
alpha =
1.61805555555556
niter =
14
After 14 iterations the function has returned an approximate value which
shares with α the ﬁrst 5 signiﬁcant digits.
The number of input and output parameters of a user-deﬁned func-
tion can vary. For instance, we could modify the Fibonacci function as
follows:
2 A third type of workspace, the so called global workspace, is available and is
used to store global variables. These variables can be used inside a function
even if they are not among the input parameters.

1.7 The MATLAB language
37
function [golden ,k]= fibonacci1 (tol ,kmax )
% FIBONACCI1 : Golden
section
number
approximation
%
Both
tolerance
and maximum
number of iterations
%
can be assigned
in input
if nargin == 0
kmax = 100;
tol = 1.e-04; % default
values
elseif
nargin == 1
kmax = 100; % default
value of kmax
end
f(1) = 0; f(2) = 1; goldenold
= 0;
for k = 3: kmax
f(k) = f(k-1) + f(k-2);
golden = f(k)/f(k-1);
if abs(golden - goldenold ) < tol
return
end
goldenold = golden;
end
return
The nargin function counts the number of input parameters (in a simi-
nargin
lar way the nargout function counts the number of output parameters).
nargout
In the new version of the fibonacci function we can prescribe a spe-
ciﬁc tolerance tol and the maximum number of inner iterations allowed
(kmax). When this information is missing the function must provide de-
fault values (in our case, tol = 1.e-04 and kmax = 100). A possible
use of it is as follows:
>> [alpha ,niter]= fibonacci1 (1.e -6 ,200)
alpha =
1.61803381340013
niter =
19
Note that using a stricter tolerance we have obtained a new approximate
value that shares with α as many as 8 signiﬁcant digits.
The nargin function can be used externally to a given function to obtain
the number of input parameters. Here is an example:
>> nargin(’fibonacci1 ’)
ans =
2
After this quick introduction, our suggestion is to explore MATLAB
using the command help, and get acquainted with the implementation of
various algorithms by the programs described throughout this book. For
instance, by typing help for we get not only a complete description on
the command for but also an indication on instructions similar to for,
such as if, while, switch, break and end. By invoking their help we
can progressively improve our knowledge of MATLAB.

38
1 What can’t be ignored
1.7.3 Examples of diﬀerences between MATLAB and Octave
languages
As already mentioned, what has been written in the previous section
about the MATLAB language applies to both MATLAB and Octave
environments without changes. However, some diﬀerences exist for the
language itself. So programs written in Octave may not run in MATLAB
and viceversa. For example, Octave supports strings with single and
double quotes
octave :1> a=" Welcome to Milan"
a = Welcome to Milan
octave :2> a=’Welcome to Milan’
a = Welcome to Milan
whereas MATLAB supports only single quotes, double quotes will result
in parsing errors.
Here we provide a list of few other incompatibilities between the two
languages:
- MATLAB does not allow a blank before the transpose operator. For
instance, [0 1]’ works in MATLAB, but [0 1] ’ does not. Octave
properly parses both cases;
- MATLAB always requires ...,
rand (1, ...
2)
while both
rand (1,
2)
and
rand (1, \
2)
work in Octave in addition to ...;
- for exponentiation, Octave can use ^ or **; MATLAB requires ^;
- for not equal comparison, Octave can use ~= or !=; MATLAB requires
~=;
- for ends, Octave can use end but also endif, endfor, . . .; MATLAB
requires end.
See Exercises 1.9-1.14.
1.8 What we haven’t told you
A systematic discussion on ﬂoating-point numbers can be found in
[¨Ube97], [Hig02] and in [QSS07].

1.9 Exercises
39
For matters concerning the issue of complexity, we refer, e.g., to
[Pan92].
For a more systematic introduction to MATLAB the interested
reader can refer to the MATLAB manual [HH05] as well as to speciﬁc
books such as [Att11], [HLR06], [Pra06], [EKM05], [Pal08] or [MH03].
For Octave we recommend the manual book mentioned at the begin-
ning of this chapter.
1.9 Exercises
Exercise 1.1 How many numbers belong to the set F(2, 2, −2, 2)? What is
the value of ϵM for such set?
Exercise 1.2 Show that the set F(β, t, L, U) contains precisely 2(β −1)βt−1
(U −L + 1) elements.
Exercise 1.3 Prove that ii is a real number, then check this result using
MATLAB.
Exercise 1.4 Write the MATLAB instructions to build an upper (respec-
tively, lower) triangular matrix of dimension 10 having 2 on the main diagonal
and −3 on the second upper (respectively, lower) diagonal.
Exercise 1.5 Write the MATLAB instructions which allow the interchange
of the third and seventh row of the matrices built up in Exercise 1.4, and
then the instructions allowing the interchange between the fourth and eighth
column.
Exercise 1.6 Verify whether the following vectors in R4 are linearly indepen-
dent:
v1 = [0 1 0 1], v2 = [1 2 3 4], v3 = [1 0 1 0], v4 = [0 0 1 1].
Exercise 1.7 Write the following functions and compute their ﬁrst and sec-
ond derivatives, as well as their primitives, using the symbolic toolbox of MAT-
LAB:
f(x) =

x2 + 1,
g(x) = sin(x3) + cosh(x).
Exercise 1.8 For any given vector v of dimension n, using the command
c=poly(v) one can construct the n + 1 coeﬃcients of the polynomial p(x) =
poly
n+1
k=1 c(k)xn+1−k which is equal to Πn
k=1(x −v(k)). In exact arithmetics,
one should ﬁnd that v = roots(poly(v)). However, this cannot occur due to
roundoﬀerrors, as one can check by using the command roots(poly([1:n])),
where n ranges from 2 to 25.

40
1 What can’t be ignored
Exercise 1.9 Write a program to compute the following sequence:
I0 = 1
e (e −1),
In+1 = 1 −(n + 1)In, for n = 0, 1, . . . .
Compare the numerical result with the exact limit In →0 for n →∞.
Exercise 1.10 Explain the behavior of the sequence (1.4) when computed in
MATLAB.
Exercise 1.11 Consider the following algorithm to compute π. Generate n
couples {(xk, yk)} of random numbers in the interval [0, 1], then compute the
number m of those lying inside the ﬁrst quarter of the unit circle. Obviously,
π turns out to be the limit of the sequence πn = 4m/n. Write a MATLAB
program to compute this sequence and check the error for increasing values of
n.
Exercise 1.12 Since π is the sum of the series
π =
∞

n=0
16−n

4
8n + 1 −
2
8n + 4 −
1
8n + 5 −
1
8n + 6

,
we can compute an approximation of π by summing up to the nth term, for a
suﬃciently large n. Write a MATLAB function to compute ﬁnite sums of the
above series. How large should n be in order to obtain an approximation of π
at least as accurate as the one stored in the variable π?
Exercise 1.13 Write a program for the computation of the binomial coef-
ﬁcient ( n
k ) = n!/(k!(n −k)!), where n and k are two natural numbers with
k ≤n.
Exercise 1.14 Write a recursive MATLAB function that computes the nth
element fn of the Fibonacci sequence. Noting that
	 fi
fi−1

=
	 1 1
1 0

 	 fi−1
fi−2

(1.14)
write another function that computes fn based on this new recursive form.
Finally, compute the related CPU-time.

2
Nonlinear equations
Computing the zeros of a real function f (equivalently, the roots of the
equation f(x) = 0) is a problem that we encounter quite often in Scien-
tiﬁc Computing. In general, this task cannot be accomplished in a ﬁnite
number of operations. For instance, we have already seen in Section 1.5.1
that when f is a generic polynomial of degree greater than four, there
do not exist explicit formulae for the zeros. The situation is even more
diﬃcult when f is not a polynomial.
Iterative methods are therefore adopted. Starting from one or several
initial data, the methods build up a sequence of values x(k) that hopefully
will converge to a zero α of the function f at hand.
The chapter will start with the formulation of some simple problems
of practical interest, which lead to the solution of nonlinear equations.
Such problems will be solved after the presentation of several numerical
methods. This planning will be proposed in all the next chapters of the
book.
2.1 Some representative problems
Problem 2.1 (Investment fund) At the beginning of every year a
bank customer deposits v euros in an investment fund and withdraws,
at the end of the nth year, a capital of M euros. We want to compute the
average yearly rate of interest r of this investment. Since M is related
to r by the relation
M = v
n

k=1
(1 + r)k = v 1 + r
r
[(1 + r)n −1] ,
we deduce that r is the root of the algebraic nonlinear equation:
f(r) = 0,
where f(r) = M −v 1 + r
r
[(1 + r)n −1].
A. Quarteroni et al., Scientiﬁc Computing with MATLAB and Octave,
Texts in Computational Science and Engineering 2,
DOI 10.1007/978-3-642-45367-0 2, © Springer-Verlag Berlin Heidelberg 2014
41

42
2 Nonlinear equations
This problem will be solved in Example 2.1.
■
Problem 2.2 (State equation of a gas) We want to determine the
volume V occupied by a gas at temperature T and pressure p. The state
equation (i.e. the equation that relates p, V and T ) is

p + a(N/V )2
(V −Nb) = kNT,
(2.1)
where a and b are two coeﬃcients that depend on the speciﬁc gas, N is
the number of molecules which are contained in the volume V and k is
the Boltzmann constant. We need therefore to solve a nonlinear equation
whose root is V (see Exercise 2.2).
■
Problem 2.3 (Rods system) Let us consider the mechanical system
represented by the four rigid rods ai of Figure 2.1. For any admissible
value of the angle β, let us determine the value of the corresponding
angle α between the rods a1 and a2. Starting from the vector identity
a1 −a2 −a3 −a4 = 0
and noting that the rod a1 is always aligned with the x-axis, we can
deduce the following relationship between β and α:
a1
a2
cos(β) −a1
a4
cos(α) −cos(β −α) = −a2
1 + a2
2 −a2
3 + a2
4
2a2a4
,
(2.2)
where ai is the known length of the ith rod. This is called the Freuden-
stein equation, and we can rewrite it as f(α) = 0, where
f(x) = a1
a2
cos(β) −a1
a4
cos(x) −cos(β −x) + a2
1 + a2
2 −a2
3 + a2
4
2a2a4
.
A solution in explicit form is available only for special values of β. We
would also like to mention that a solution does not exist for all values of
β, and may not even be unique. To solve the equation for any given β
lying between 0 and π we should invoke numerical methods (see Exercise
2.9).
■
Problem 2.4 (Population dynamics) In the study of populations
(e.g. bacteria), the equation x+ = φ(x) = xR(x) establishes a link be-
tween the number of individuals in a generation x and the number of
individuals in the following generation. Function R(x) models the vari-
ation rate of the considered population and can be chosen in diﬀerent
ways. Among the most known, we can mention:
1. Malthus’s model (Thomas Malthus, 1766-1834),
R(x) = RM(x) = r,
r > 0;

2.2 The bisection method
43
a1
a2
a3
a4
β
α
x
y
Figure 2.1. System of four rods of Problem 2.3
2. the growth with limited resources model, (known as Beverton-Holt’s
or discrete Verhulst’s model)
R(x) = RV (x) =
r
1 + xK ,
r > 0, K > 0,
(2.3)
which improves on Malthus’s model in considering that the growth
of a population is limited by the available resources;
3. the predator/prey model with saturation,
R(x) = RP =
rx
1 + (x/K)2 ,
(2.4)
which represents the evolution of Beverton-Holt’s model in the pres-
ence of an antagonist population.
The dynamics of a population is therefore deﬁned by the iterative process
x(k) = φ(x(k−1)),
k ≥1,
(2.5)
where x(k) represents the number of individuals present k generations
later than the initial generation x(0). Moreover, the stationary (or equi-
librium) states x∗of the considered population are the solutions of prob-
lem
x∗= φ(x∗),
or, equivalently, x∗= x∗R(x∗) i.e. R(x∗) = 1. Equation (2.5) is an
instance of a ﬁxed point method (see Section 2.6).
■
2.2 The bisection method
Let f be a continuous function in [a, b] which satisﬁes f(a)f(b) < 0. Then
necessarily f has at least one zero in (a, b). (This result is known as the
theorem of zeros of continuous functions.) Let us assume for simplicity

44
2 Nonlinear equations
I(0)
I(1)
I(2)
I(3)
a(0)
x(0)
x(1)
x(2)
x
y
b(0)
f
f
Figure 2.2. A few iterations of the bisection method
that it is unique, and let us call it α. (In the case of several zeros, by
the help of the command fplot we can locate an interval which contains
only one of them.)
The strategy of the bisection method is to halve the given inter-
val and select that subinterval where f features a sign change. More
precisely, having named I(0) = (a, b) and, more generally, I(k) the sub-
interval selected at step k, we choose as I(k+1) the sub-interval of I(k)
at whose end-points f features a sign change. Following such procedure,
it is guaranteed that every I(k) selected this way will contain α. The se-
quence {x(k)} of the midpoints of these subintervals I(k) will inevitably
tend to α since the length of the subintervals tends to zero as k tends to
inﬁnity.
Precisely, the method is started by setting
a(0) = a, b(0) = b, I(0) = (a(0), b(0)), x(0) = (a(0) + b(0))/2.
At each step k ≥1 we select the subinterval I(k) = (a(k), b(k)) of the
interval I(k−1) = (a(k−1), b(k−1)) as follows:
given x(k−1) = (a(k−1) + b(k−1))/2,
if f(x(k−1)) = 0,
then α = x(k−1)
and the method terminates;
otherwise,
if f(a(k−1))f(x(k−1)) < 0
set a(k) = a(k−1), b(k) = x(k−1);
if f(x(k−1))f(b(k−1)) < 0
set a(k) = x(k−1), b(k) = b(k−1).
Then we deﬁne x(k) = (a(k) + b(k))/2 and increase k by 1.

2.2 The bisection method
45
For instance, in the case represented in Figure 2.2, which corresponds
to the choice f(x) = x2 −1, by taking a(0) = −0.25 and b(0) = 1.25, we
would obtain
I(0) = (−0.25, 1.25),
x(0) = 0.5,
I(1) = (0.5, 1.25),
x(1) = 0.875,
I(2) = (0.875, 1.25),
x(2) = 1.0625,
I(3) = (0.875, 1.0625), x(3) = 0.96875.
Notice that each subinterval I(k) contains the zero α. Moreover, the
sequence {x(k)} necessarily converges to α since at each step the length
|I(k)| = b(k) −a(k) of I(k) halves. Since |I(k)| = (1/2)k|I(0)|, the error at
step k satisﬁes
|e(k)| = |x(k) −α| < 1
2|I(k)| =
1
2
k+1
(b −a).
In order to guarantee that |e(k)| < ε, for a given tolerance ε it suﬃces to
carry out kmin iterations, kmin being the smallest integer satisfying the
inequality
kmin > log2
b −a
ε

−1
(2.6)
Obviously, this inequality makes sense in general, and is not conﬁned to
the speciﬁc choice of f that we have made previously.
The bisection method is implemented in Program 2.1: fun is the
function handle associated with the function f, a and b are the endpoints
of the search interval, tol is the tolerance ε and nmax is the maximum
number of allowed iterations. Besides the ﬁrst argument which represents
the independent variable, the function fun can accept other auxiliary
parameters.
Output parameters are zero, which contains the approximate value
of α, the residual res which is the value of f in zero and niter which
is the total number of iterations that are carried out. The command
find(fx==0) ﬁnds those indices of the vector fx corresponding to null
find
components, while the command varargin allows the function fun to
varargin
accept a variable number of input parameters.
Program 2.1. bisection: bisection method
function [zero ,res ,niter]= bisection (fun ,a,b,tol ,...
nmax ,varargin )
% BISECTION
Finds function
zeros.
% ZERO =BISECTION (FUN ,A,B,TOL ,NMAX ) tries to find a zero
% ZERO of the
continuous
function
FUN in the
interval
% [A,B] using the
bisection
method. If

46
2 Nonlinear equations
% the search fails an error message is displayed .
% FUN is a function
handle
associated
with an
anonymous
% function
or a Matlab
function .
% ZERO =BISECTION (FUN ,A,B,TOL ,NMAX ,P1 ,P2 ,...) passes
% parameters
P1 ,P2 ,... to the
function
FUN(X,P1 ,P2 ,...)
% [ZERO ,RES ,NITER]= BISECTION (FUN ,...) returns
the value
% of the residual
in ZERO
and the
iteration
number at
% which ZERO
was
computed .
x = [a, (a+b)*0.5 , b];
fx = fun(x,varargin {:});
if fx (1)* fx (3) > 0
error([’ The
sign of the
function
at the ’ ,...
’endpoints
of the interval
must be
different \n’]);
elseif fx (1) == 0
zero = a; res = 0;
niter = 0; return
elseif fx (3) == 0
zero = b; res = 0;
niter = 0; return
end
niter = 0;
I = (b - a)*0.5;
while I >= tol & niter < nmax
niter = niter + 1;
if fx (1)* fx(2) <
0
x(3) = x(2);
x(2) = x(1)+(x(3)-x(1))*0.5;
fx = fun(x,varargin {:});
I = (x(3)-x(1))*0.5;
elseif fx (2)* fx (3) < 0
x(1) = x(2);
x(2) = x(1)+(x(3)-x(1))*0.5;
fx = fun(x,varargin {:});
I = (x(3)-x(1))*0.5;
else
x(2) = x(find (fx ==0)); I = 0;
end
end
if
(niter== nmax & I > tol)
fprintf ([’Bisection
stopped
without
converging
’ ,...
’to the desired
tolerance
because
the \n’ ,...
’maximum
number of iterations
was reached \n’]);
end
zero = x(2);
x = x(2);
res = fun(x,varargin {:});
return
Example 2.1 (Investment fund) Let us apply the bisection method to
solve Problem 2.1, assuming that v is equal to 1000 euros and that after 5
years M is equal to 6000 euros. The graph of the function f can be obtained
by the following instructions
M=6000; v=1000; f=@(r) (M-v*(1+ r).*((1+r).^5 - 1)./ r);
fplot(f ,[0.01 ,0.3]);
(we remind the reader that the prompt is neglected in order to simplify no-
tations). We see that f has a unique zero in the interval (0.01, 0.1), which
is approximately equal to 0.06. If we execute Program 2.1 with tol= 10−12,
a= 0.01 and b= 0.1 as follows

2.3 The Newton method
47
[zero ,res ,niter]= bisection (f ,0.01 ,0.1 ,1.e -12 ,1000)
after 36 iterations the method converges to the value 0.06140241153618, in
perfect agreement with the estimate (2.6) according to which kmin = 36.
Thus, we conclude that the interest rate r is approximately equal to 6.14%.
Instead of an anonymous function, we could use the function Rfuncv.m
function y=Rfuncv(r,M,v)
% RFUNCV
function
for example
2.1
y=M - v*(1+ r)./r.*((1+r).^5
- 1);
end
and call bisection.m with the following instructions:
M=6000; v=1000;
[zero ,res ,niter]= bisection (@Rfuncv ,0.01 ,0.1 ,...
1.e -12 ,1000 ,M,v)
Notice that now the variables M and v must be appended to the input list of the
function bisection.m. These parameters will be stored in the optional input
variable varargin of bisection.m.
.
■
In spite of its simplicity, the bisection method does not guarantee a
monotone reduction of the error, but simply that the search interval is
halved from one iteration to the next. Consequently, if the only stopping
criterion adopted is the control of the length of I(k), one might discard
approximations of α which are quite accurate.
As a matter of fact, this method does not take into proper account
the actual behavior of f. A striking fact is that it does not converge in
a single iteration even if f is a linear function (unless the zero α is the
midpoint of the initial search interval).
See Exercises 2.1-2.5.
2.3 The Newton method
The sign of the given function f at the endpoints of the subintervals is
the only information exploited by the bisection method. A more eﬃcient
method can be constructed by exploiting the values attained by f and
its derivative (in the case that f is diﬀerentiable). In that case,
y(x) = f(x(k)) + f ′(x(k))(x −x(k))
provides the equation of the tangent to the curve (x, f(x)) at the point
x(k).
If we pretend that x(k+1) is such that y(x(k+1)) = 0, we obtain:
x(k+1) = x(k) −f(x(k))
f ′(x(k)),
k ≥0
(2.7)

48
2 Nonlinear equations
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−6
−4
−2
0
2
4
6
8
 
 
x(0)
x(1)
x(2)
x(3)
f
α
Figure 2.3. The ﬁrst iterations generated by the Newton method with initial
guess x(0) for the function f(x) = x + ex + 10/(1 + x2) −5
provided f ′(x(k)) ̸= 0. This formula allows us to compute a sequence of
values x(k) starting from an initial guess x(0). This method is known as
Newton’s method and corresponds to computing the zero of f by locally
replacing f by its tangent line (see Figure 2.3).
As a matter of fact, by developing f in Taylor series in a neighborhood
of a generic point x(k) we ﬁnd
f(x(k+1)) = f(x(k)) + δ(k)f ′(x(k)) + O((δ(k))2),
(2.8)
where δ(k) = x(k+1) −x(k). Forcing f(x(k+1)) to be zero and neglecting
the term O((δ(k))2), we can obtain x(k+1) as a function of x(k) as stated
in (2.7). In this respect (2.7) can be regarded as an approximation of
(2.8).
Obviously, (2.7) converges in a single step when f is linear, that is
when f(x) = a1x + a0.
Example 2.2 Let us solve Problem 2.1 by Newton’s method, taking as initial
data x(0) = 0.3. After 6 iterations the diﬀerence between two subsequent
iterates is less than or equal to 10−12.
■
The Newton method in general does not converge for all possible
choices of x(0), but only for those values of x(0) which are suﬃciently
close to α, that is they belong to a suitable neighbourhood I(α) of α.
At ﬁrst glance, this requirement looks meaningless: indeed, in order to
compute α (which is unknown), one should start from a value suﬃciently
close to α!
In practice, a possible initial value x(0) can be obtained by resorting
to a few iterations of the bisection method or, alternatively, through
an investigation of the graph of f. If x(0) is properly chosen and α is
a simple zero (that is, f ′(α) ̸= 0) then the Newton method converges.
Furthermore, in the special case where f is continuously diﬀerentiable
up to its second derivative one has the following convergence result (see
Exercise 2.8),

2.3 The Newton method
49
0
5
10
15
20
25
30
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
10
2
Figure 2.4. Error in semi-logarithmic scale versus iteration number for the
function of Example 2.3. The dashed line corresponds to Newton’s method
(2.7), solid line to the modiﬁed Newton’s method (2.10) (with m = 2)
lim
k→∞
x(k+1) −α
(x(k) −α)2 = f ′′(α)
2f ′(α)
(2.9)
Consequently, if f ′(α) ̸= 0 Newton’s method is said to converge quadrat-
ically, or with order 2, since for suﬃciently large values of k the error at
step (k + 1) behaves like the square of the error at step k multiplied by
a constant which is independent of k.
In the case of zeros with multiplicity m larger than 1, i.e. if f ′(α) =
0, . . . , f (m−1)(α) = 0, Newton’s method still converges, but only if x(0)
is properly chosen and f ′(x) ̸= 0 ∀x ∈I(α) \ {α}. Nevertheless, in this
case the order of convergence of Newton’s method downgrades to 1 (see
Exercise 2.15). In such case one could recover the order 2 by modifying
the original method (2.7) as follows:
x(k+1) = x(k) −m f(x(k))
f ′(x(k)),
k ≥0
(2.10)
provided that f ′(x(k)) ̸= 0. Obviously, the modiﬁed Newton’s method
(2.10) requires the a-priori knowledge of m. If this is not the case, one
could develop an adaptive Newton method, still of order 2, as described
in [QSS07, Section 6.6.2].
Example 2.3 The function f(x) = (x −1) log(x) has a single zero α = 1 of
multiplicity m = 2. Let us compute it by both Newton’s method (2.7) and by
its modiﬁed version (2.10). In Figure 2.4 we report the error obtained using the
two methods versus the iteration number. Note that for the classical version
of Newton’s method the convergence is only linear.
■

50
2 Nonlinear equations
2.3.1 How to terminate Newton’s iterations
In theory, a convergent Newton’s method returns the zero α only after an
inﬁnite number of iterations. In practice, one requires an approximation
of α up to a prescribed tolerance ε. Thus the iterations can be terminated
at the smallest value of kmin for which the following inequality holds:
|e(kmin)| = |α −x(kmin)| < ε.
This is a test on the error. Unfortunately, since the error is unknown, one
needs to adopt in its place a suitable error estimator, that is, a quantity
that can be easily computed and through which we can estimate the
real error. At the end of Section 2.6, we will see that a suitable error
estimator for Newton’s method is provided by the diﬀerence between
two successive iterates. This means that one terminates the iterations at
step kmin as soon as
|x(kmin) −x(kmin−1)| < ε
(2.11)
This is a test on the increment. We will see in Section 2.6.1 that the test
on the increment is satisfactory when α is a simple zero of f. Alterna-
tively, one could use a test on the residual at step k, r(k) = f(x(k)) (note
that the residual is null when x(k) is a zero of the function f).
Precisely, we could stop the iteration at the ﬁrst kmin for which
|r(kmin)| = |f(x(kmin))| < ε
(2.12)
The test on the residual is satisfactory only when |f ′(x)| ≃1 in a neigh-
borhood Iα of the zero α (see Figure 2.5). Otherwise, it will produce
an over estimation of the error if |f ′(x)| ≫1 for x ∈Iα and an under
estimation if |f ′(x)| ≪1 (see also Exercise 2.6).
In Program 2.2 we implement Newton’s method (2.7). Its modiﬁed
form can be obtained simply by replacing f ′ with f ′/m. The input pa-
rameters fun and dfun are the function handles associated with the func-
tion f and its ﬁrst derivative, while x0 is the initial guess. The method
will be terminated when the absolute value of the diﬀerence between two
subsequent iterates is less than the prescribed tolerance tol, or when the
maximum number of iterations nmax has been reached.
Program 2.2. newton: Newton method
function [zero ,res ,niter]= newton(fun ,dfun ,x0 ,tol ,...
nmax ,varargin )
%NEWTON
Finds function
zeros.
% ZERO =NEWTON(FUN ,DFUN ,X0 ,TOL ,NMAX ) tries to find
the
% zero
ZERO
of the
continuous
and differentiable

2.4 The secant method
51
f(x(k))
f(x(k))
x(k)
x(k)
α
α
x
x
y
y
f
f
e(k)
e(k)
Figure 2.5. Two situations in which the residual is a poor error estimator:
|f ′(x)| ≫1 (left), |f ′(x)| ≪1 (right), with x belonging to a neighborhood of
α
% function
FUN nearest to X0 using the Newton
method.
% FUN and its
derivative
DFUN
accept
real
scalar
input
% x and return a real
scalar value. If the search
% fails an error message is displayed . FUN and DFUN
% are function
handles
associated
with
anonymous
fun -
% ctions or Matlab
functions .
% ZERO =NEWTON(FUN ,DFUN ,X0 ,TOL ,NMAX ,P1 ,P2 ,...) passes
% parameters
P1 ,P2 ,... to
functions : FUN(X,P1 ,P2 ,...)
% and DFUN (X,P1 ,P2 ,...).
% [ZERO ,RES ,NITER]= NEWTON(FUN ,...) returns
the value of
% the residual
in ZERO
and the
iteration
number at
% which ZERO
was
computed .
x = x0; fx = fun(x,varargin {:});
dfx = dfun (x,varargin {:});
niter = 0;
diff = tol +1;
while diff
>= tol & niter < nmax
niter = niter + 1;
diff = - fx/dfx;
x = x + diff ;
diff = abs(diff );
fx = fun(x,varargin {:});
dfx = dfun (x,varargin {:});
end
if (niter== nmax & diff > tol)
fprintf ([’Newton
stopped
without
converging
to’ ,...
’ the desired
tolerance
because
the maximum \n ’ ,...
’number of iterations
was
reached\n’]);
end
zero = x; res = fx;
2.4 The secant method
For the computation of the zeroes of a function f whose derivative is
not available in analytical form, the Newton method cannot be applied.
However, should we be able to compute the function f at any arbitrary
point, we could replace the exact value f ′(x(k)) with an incremental ratio

52
2 Nonlinear equations
based on previously computed values of f. The secant method exploits
this strategy and is deﬁned as follows: for any given couple of points x(0)
and x(1), for k ≥1 compute
x(k+1) = x(k) −
f(x(k)) −f(x(k−1))
x(k) −x(k−1)
−1
f(x(k))
(2.13)
If α is a simple zero of f and I(α) a suitable neighborhood of α, if
moreover x(0) and x(1) are suﬃciently close to α and f ′(x) ̸= 0 ∀x ∈
I(α) \ {α}, the secant method (2.13) converges to α. Moreover, if f ∈
C2(I(α)) and f ′(α) ̸= 0, there exists a constant c > 0 such that
|x(k+1) −α| ≤c|x(k) −α|p, with p = 1 +
√
5
2
≃1.618...
(2.14)
This shows that the secant method converges super-linearly (i.e. with
a convergence rate p > 1).
Should α be a multiple zero, the rate of
convergence would be linear, just as for Newton’s method.
Example 2.4 We use the secant method to solve the case of Example 2.1
starting from the initial data x(0) = 0.3 and x(1) = −0.3. The method con-
verges in 8 iterations, whereas 6 iterations would be necessary to the Newton
method to converge starting from the same x(0).
Choosing instead x(0) = 0.3 and x(1) = 0.1 the secant method would converge
in 6 iterations, just like the Newton method.
■
2.5 Systems of nonlinear equations
Let us consider a system of nonlinear equations of the form
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
f1(x1, x2, . . . , xn) = 0,
f2(x1, x2, . . . , xn) = 0,
...
fn(x1, x2, . . . , xn) = 0,
(2.15)
where f1, . . . , fn are nonlinear functions. Setting f = (f1, . . . , fn)T and
x = (x1, . . . , xn)T , system (2.15) can be written in a compact way as
f(x) = 0.
(2.16)
An example is given by the following nonlinear system

2.5 Systems of nonlinear equations
53
f1(x1, x2) = x2
1 + x2
2 = 1,
f2(x1, x2) = sin(πx1/2) + x3
2 = 0.
(2.17)
In order to extend Newton’s method to the case of a system, we replace
the ﬁrst derivative of the scalar function f with the Jacobian matrix Jf
of the vectorial function f whose components are
(Jf)ij = ∂fi
∂xj
,
i, j = 1, . . . , n.
The symbol ∂fi/∂xj represents the partial derivative of fi with respect
to xj (see deﬁnition (9.3)). With this notation, Newton’s method for
(2.16) then becomes: given x(0) ∈Rn, for k = 0, 1, . . ., until convergence
solve Jf(x(k))δx(k) = −f(x(k))
set
x(k+1) = x(k) + δx(k)
(2.18)
Therefore, Newton’s method applied to a system requires at each step
the solution of a linear system with matrix Jf(x(k)).
Program 2.3 implements this method by using the MATLAB com-
mand \ (see Section 5.8) to solve the linear system with the jacobian
matrix. In input we must deﬁne a column vector x0 representing the
initial datum and two functions, the function handles Ffun and Jfun,
to evaluate (respectively) the function f and the jacobian matrix Jf, for
a generic vector x. The method stops when the diﬀerence between two
consecutive iterates has an euclidean norm smaller than tol or when
nmax, the maximum number of allowed iterations, has been reached.
Program 2.3. newtonsys: Newton method for nonlinear systems
function [x,res ,niter] = newtonsys (Ffun ,Jfun ,x0 ,tol ,...
nmax , varargin )
% NEWTONSYS
Finds a zero of a nonlinear
system
% [ZERO ,RES ,NITER]= NEWTONSYS (FFUN ,JFUN ,X0 ,TOL ,NMAX )
% tries to find
the
vector ZERO , zero
of a nonlinear
% system
defined in FFUN
with
jacobian
matrix
defined
% in the function
JFUN , nearest to the vector X0.
% The variable
RES
returns the
residual
in ZERO
% while NITER returns
the number of iterations
needed
% to compute
ZERO . FFUN
and JFUN
are
function
handles
% associated
with
anonymous
functions
or MATLAB
% functions
stored in M-files.
niter = 0; err = tol + 1; x = x0;
while err
>= tol & niter < nmax
J = Jfun (x, varargin {:});
F = Ffun (x, varargin {:});
delta = - J\F;
x = x + delta;
err = norm(delta);

54
2 Nonlinear equations
niter = niter + 1;
end
res = norm (Ffun (x,varargin {:}));
if (niter== nmax & err > tol)
fprintf ([’ Fails to converge
within
maximum ’ ,...
’number of iterations .\n’ ,...
’The iterate
returned
has relative
’ ,...
’residual
%e\n’],F);
else
fprintf ([’The method
converged
at iteration
’ ,...
’%i with
residual %e\n’],niter ,F);
end
return
Example 2.5 Let us consider the nonlinear system (2.17) which allows the
two (graphically detectable) solutions (0.4761, −0.8794) and (−0.4761, 0.8794)
(where we only report the four ﬁrst signiﬁcant digits). In order to use Program
2.3 we deﬁne the following functions
function J=Jfun (x)
pi2 = 0.5* pi;
J(1,1) = 2*x(1);
J(1,2) = 2*x(2);
J(2,1) = pi2*cos(pi2*x(1));
J(2,2) = 3*x(2)^2;
return
function F=Ffun (x)
F(1,1) = x(1)^2 + x(2)^2 - 1;
F(2,1) = sin(pi*x(1)/2) + x(2)^3;
return
Starting from an initial datum of x0=[1;1] Newton’s method, launched
with the command
x0 =[1;1]; tol =1e -5; nmax =10;
[x,F,niter] = newtonsys (@Ffun ,@Jfun ,x0 ,tol ,nmax );
converges in 8 iterations to the values
4.760958225338114 e -01
-8.793934089897496 e -01
(The special character @ generates the function handles associated with func-
tions Ffun and Jfun, which are passed to newtonsys.)
Notice that the method converges to the other root starting from
x0=[-1;-1]. In general, exactly as in the case of scalar functions, convergence
of Newton’s method will actually depend on the choice of the initial datum
x(0) and in particular we should guarantee that det(Jf(x(0))) ̸= 0.
■
The secant method can be adapted to the solution of systems of
nonlinear equations still featuring super-linear rate of convergence. The
idea consists in replacing the Jacobian matrices Jf(x(k)) (for k ≥0) of
Newton’s method with suitable matrices Bk, recursively deﬁned starting
from a convenient matrix B0, representing a suitable approximation of
Jf(x(0)). (Alternative strategies will be addressed in Section 4.2 and

2.5 Systems of nonlinear equations
55
in Chapter 9.) The most popular method of this kind is the following
Broyden method. From a given x(0) ∈Rn and a given B0 ∈Rn×n, for
k = 0, 1, . . . , until convergence
solve
Bkδx(k) = −f(x(k))
set
x(k+1) = x(k) + δx(k)
set
δf (k) = f(x(k+1)) −f(x(k))
compute Bk+1 = Bk + (δf (k) −Bkδx(k))δx(k)T
δx(k)T δx(k)
(2.19)
Notice that we do not require the sequence {Bk} to converge to the
true Jacobian matrix Jf(α) (α being the root of the system). Rather, it
can be proved that
lim
k→∞
∥(Bk −Jf(α))(x(k) −α)∥
∥x(k) −α∥
= 0.
This property guarantees that Bk is a convenient approximation of
Jf(α) along the error direction x(k) −α.
At every step, the virtual cost O(n3) for the computation of δx(k)
can be reduced to O(n2), by recursively using QR factorization of ma-
trices Bk (see e.g., [GM72]). Because of the equality (δf (k) −Bkδx(k)) =
f(x(k+1)), we can avoid implementing matrix-vector products for the
computation of Bk+1.
For a more in depth description of Broyden method and other se-
cant type methods, also called quasi-Newton methods, we refer to [JS96]
[Deu04], [SM03] and [QSS07, Ch. 6].
Example 2.6 Let us use Broyden method (2.19) for the solution of the prob-
lem of Example 2.5. Setting B0 = I, the tolerance ε = 10−5 for the stop-
ping test on the increment and x(0) = (1, 1)T , convergence to the point
(0.476095825652119, −0.879393405072448)T is achieved in 10 iterations, with
a residual whose norm is 1.324932e −08. The Newton method would converge
in 8 iterations with the norm of the ﬁnal residual equal to 2.235421e −11.
Still using B0 = I and changing the initial point x(0) = (−1, −1)T , conver-
gence would be achieved in 17 iterations with a (norm of the) residual equal
to 5.744382e−08 (versus 8 iterations and residual 2.235421e−11 for Newton’s
method). Choosing instead B0 = 2I, the number of Broyden iterations reduces
to 12; this shows how crucial is the choice of the initial matrix.
As for accuracy, Newton’s method is better than Broyden’s, as the value
of the residual shows.
■

56
2 Nonlinear equations
Let us summarize
1. Methods for the computation of the zeros of a function f are usually
of iterative type;
2. the bisection method computes a zero of a function f by generating
a sequence of intervals whose length is halved at each iteration. This
method is convergent provided that f is continuous in the initial
interval and has opposite signs at the endpoints of this interval;
3. Newton’s method computes a zero α of f by taking into account
the values of f and of its derivative. A necessary condition for con-
vergence is that the initial datum belongs to a suitable (suﬃciently
small) neighborhood of α;
4. Newton’s method is quadratically convergent only when α is a simple
zero of f, otherwise convergence is linear;
5. the Newton method can be extended to the case of a nonlinear system
of equations;
6. the secant method can be regarded as a variant of Newton’s method
where the ﬁrst derivative of the function is replaced by a suitable
incremental ratio. For simple roots, it converges super-linearly (how-
ever less than quadratically); for multiple roots, the convergence rate
is only linear. As for Newton’s method, the initial points should be
suﬃciently close to the root for the method to converge.
See Exercises 2.6-2.14.
2.6 Fixed point iterations
Playing with a pocket calculator, one may verify that by applying repeat-
edly the cosine key to the real value 1, one gets the following sequence
of real numbers:
x(1) = cos(1) = 0.54030230586814,
x(2) = cos(x(1)) = 0.85755321584639,
...
x(10) = cos(x(9)) = 0.74423735490056,
...
x(20) = cos(x(19)) = 0.73918439977149,
which should tend to the value α = 0.73908513 . . .. Since, by construc-
tion, x(k+1) = cos(x(k)) for k = 0, 1, . . . (with x(0) = 1), the limit α
satisﬁes the equation cos(α) = α. For this reason α is called a ﬁxed

2.6 Fixed point iterations
57
y
y = x
x
φ
α
y
y = x
x
φ
Figure 2.6. The function φ(x) = cos x admits one and only one ﬁxed point
(left), whereas the function φ(x) = ex does not have any (right)
point of the cosine function. We may wonder how such iterations could
be exploited in order to compute the zeros of a given function. In the
previous example, α is not only a ﬁxed point for the cosine function,
but also a zero of the function f(x) = x −cos(x), hence the previously
proposed method can be regarded as a method to compute the zeros of
f. On the other hand, not every function has ﬁxed points. For instance,
by repeating the previous experiment using the exponential function and
x(0) = 1 one encounters a situation of overﬂow after 4 steps only (see
Figure 2.6).
Let us clarify the intuitive idea above by considering the following
problem. Given a function φ : [a, b] →R, ﬁnd α ∈[a, b] such that
α = φ(α).
If such an α exists it will be called a ﬁxed point of φ and it could be
computed by the following algorithm:
x(k+1) = φ(x(k)),
k ≥0
(2.20)
where x(0) is an initial guess. This algorithm is called ﬁxed point itera-
tions and φ is said to be the iteration function. The introductory example
is therefore an instance of ﬁxed point iterations with φ(x) = cos(x).
A geometrical interpretation of (2.20) is provided in Figure 2.7 (left).
One can guess that if φ is a continuous function and the limit of the
sequence {x(k)} exists, then such limit is a ﬁxed point of φ. We will
make this result more precise in Propositions 2.1 and 2.2.
Example 2.7 The Newton method (2.7) can be regarded as an algorithm of
ﬁxed point iterations whose iteration function is

58
2 Nonlinear equations
y = x
y
x
x(0)
x(1)
x(2)
φ
α
y = x
y
x
x(0)
x(1)
x(2)
φ
α
Figure 2.7. Representation of a few ﬁxed point iterations for two diﬀerent
iteration functions. At left, the iterations converge to the ﬁxed point α, whereas
the iterations on the right produce a divergence sequence
φ(x) = x −f(x)
f ′(x).
(2.21)
From now on this function will be denoted by φN (where N stands for Newton).
This is not the case for the bisection method since the generic iterate x(k+1)
depends not only on x(k) but also on x(k−1).
■
As shown in Figure 2.7 (right), ﬁxed point iterations may not con-
verge. Indeed, the following result holds.
Proposition 2.1 Let us consider the sequence (2.20).
1. Let us suppose that φ(x) is continuous in [a, b] and such that
φ(x) ∈[a, b] for every x ∈[a, b]; then there exists at least a ﬁxed
point α ∈[a, b].
2. Moreover, if
∃L < 1 s.t. |φ(x1)−φ(x2)| ≤L|x1 −x2| ∀x1, x2 ∈[a, b], (2.22)
then there exists a unique ﬁxed point α ∈[a, b] of φ and the
sequence deﬁned in (2.20) converges to α, for any choice of intial
guess x(0) in [a, b].
Proof.
1. We start by proving existence of ﬁxed points for φ. The
function g(x) = φ(x)−x is continuous in [a, b] and, thanks to assumption
made on the range of φ, it holds g(a) = φ(a) −a ≥0 and g(b) =
φ(b) −b ≤0. By applying the theorem of zeros of continuous functions,
we can conclude that g has at least one zero in [a, b], i.e. φ has at least
one ﬁxed point in [a, b]. (See Figure 2.8 for an instance.)

2.6 Fixed point iterations
59
a
a
b
b
φ
x
y
y = x
α1
α2
α3
a
a
b
b
φ
x
y
y = x
α
x(0)
x(1)
x(2)
x(3)
Figure 2.8. At left, an iteration function φ featuring 3 ﬁxed points, at right,
an iteration function satisfying the assumption (2.22) and the ﬁrst elements
of sequence (2.24) converging to the unique ﬁxed point α
2. Uniqueness of ﬁxed points follows from assumption (2.22). Indeed,
should two diﬀerent ﬁxed points α1 and α2 exist, then
|α1 −α2| = |φ(α1) −φ(α2)| ≤L|α1 −α2| < |α1 −α2|,
which cannot be.
We prove now that the sequence x(k) deﬁned in (2.20) converges to the
unique ﬁxed point α when k →∞, for any choice of initial guess x(0) ∈
[a, b]. It holds
0 ≤|x(k+1) −α| = |φ(x(k)) −φ(α)|
≤L|x(k) −α| ≤. . . ≤Lk+1|x(0) −α|,
i.e., ∀k ≥0,
|x(k) −α|
|x(0) −α| ≤Lk.
(2.23)
Passing to the limit as k →∞, we obtain limk→∞|x(k) −α| = 0, which
is the desired result.
■
In practice it is often very diﬃcult to choose a priori an interval [a, b]
for which the assumptions of Proposition 2.1 are fulﬁlled; in such cases
the following local convergence result will be useful. We refer to [OR70]
for a proof.

60
2 Nonlinear equations
Theorem 2.1 (Ostrowski’s theorem) Let α be a ﬁxed point of
a function φ which is continuous and continuously diﬀerentiable in
a suitable neighbourhood J of α. If |φ′(α)| < 1, then there exists
δ > 0 for which {x(k)} converges to α, for every x(0) such that
|x(0) −α| < δ. Moreover, it holds
lim
k→∞
x(k+1) −α
x(k) −α
= φ′(α)
(2.24)
Proof. We limit ourselves to verify property (2.24). Thanks to Lagrange
theorem, for any k ≥0, there exists a point ξk between x(k) and α such
that x(k+1) −α = φ(x(k)) −φ(α) = φ′(ξk)(x(k) −α), that is
(x(k+1) −α)/(x(k) −α) = φ′(ξk).
(2.25)
Since x(k) →α and ξk lies between x(k) and α, it holds limk→∞ξk = α.
Finally, passing to the limit in both terms of (2.25) and recalling that φ′
is continuous in a neighbourhood of α, we obtain (2.24).
■
From both (2.23) and (2.24) one deduces that the ﬁxed point iterations
converge at least linearly, that is, for k suﬃciently large the error at step
k + 1 behaves like the error at step k multiplied by a constant (which
concides with either L in (2.23) and φ′(α) in (2.24)) which is independent
of k and whose absolute value is strictly less than 1. For this reason, this
constant is named asymptotic convergence factor. Finally, we remark that
the smaller the asymptotic convergence factor, the faster the convergence.
Remark 2.1 When |φ′(α)| > 1, it follows from (2.25) that if x(k) is suﬃciently
close to α, such that |φ′(x(k))| > 1, then |α −x(k+1)| > |α −x(k)|, and the
sequence cannot converge to the ﬁxed point. On the contrary, when |φ′(α)| = 1,
no conclusion can be drawn since either convergence or divergence could take
place, depending on properties of the iteration function φ(x).
■
Example 2.8 The function φ(x) = cos(x) satisﬁes all the assumptions of
Theorem 2.1. Indeed, |φ′(α)| = | sin(α)| ≃0.67 < 1, and thus by continuity
there exists a neighborhood Iα of α such that |φ′(x)| < 1 for all x ∈Iα. The
function φ(x) = x2 −1 has two ﬁxed points α± = (1 ±
√
5)/2, however it
does not satisfy the assumption for either since |φ′(α±)| = |1 ±
√
5| > 1. The
corresponding ﬁxed point iterations will not converge.
■
Example 2.9 (Population dynamics) Let us apply the ﬁxed point itera-
tions to the function φV (x) = rx/(1 + xK) of Verhulst’s model (2.3) and to
the function φP (x) = rx2/(1 + (x/K)2), for r = 3 and K = 1, of the preda-
tor/prey model (2.4). Starting from the initial point x(0) = 1, we ﬁnd the ﬁxed

2.6 Fixed point iterations
61
0
1
2
3
4
5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
 
 
 
 
Figure 2.9. Two ﬁxed points for two diﬀerent population dynamics: Verhulst’s
model (solid line) and predator/prey model (dashed line)
point α = 2 in the ﬁrst case and α = 2.6180 in the second case (see Figure
2.9). The ﬁxed point α = 0, common to either φV and φP , can be obtained
using the ﬁxed point iterations on φP but not those on φV . In fact, φ′
P (α) = 0,
while φ′
V (α) = r > 1. The third ﬁxed point of φP , α = 0.3820 . . ., cannot be
obtained by ﬁxed point iterations since φ′
P (α) > 1.
■
The Newton method is not the only iterative procedure featuring
quadratic convergence. Indeed, the following general property holds.
Proposition 2.2 Assume that all hypotheses of Theorem 2.1 are
satisﬁed. In addition assume that φ is twice continuously diﬀeren-
tiable and that
φ′(α) = 0, φ′′(α) ̸= 0.
Then the ﬁxed point iterations (2.20) converge with order 2 and
lim
k→∞
x(k+1) −α
(x(k) −α)2 = 1
2φ′′(α)
(2.26)
Proof. In this case it suﬃces to prove that there exists a point η(k) lying
between x(k) and α such that
x(k+1) −α = φ(x(k)) −φ(α) = φ′(α)(x(k) −α) + φ′′(η(k))
2
(x(k) −α)2.
■
Example 2.7 shows that the ﬁxed point iterations (2.20) could also be
used to compute the zeros of the function f. Clearly for any given f the

62
2 Nonlinear equations
function φ deﬁned in (2.21) is not the only possible iteration function.
For instance, for the solution of the equation log(x) = γ, after setting
f(x) = log(x) −γ, the choice (2.21) could lead to the iteration function
φN(x) = x(1 −log(x) + γ).
Another ﬁxed point iteration algorithm could be obtained by adding
x to both sides of the equation f(x) = 0. The associated iteration func-
tion is now φ1(x) = x+log(x)−γ. A further method could be obtained by
choosing the iteration function φ2(x) = x log(x)/γ. Not all these meth-
ods are convergent. For instance, if γ = −2, the methods corresponding
to the iteration functions φN and φ2 are both convergent, whereas the
one corresponding to φ1 is not since |φ′
1(x)| > 1 in a neighborhood of
the ﬁxed point α.
2.6.1 How to terminate ﬁxed point iterations
In general, ﬁxed point iterations are terminated when the absolute value
of the diﬀerence between two consecutive iterates is less than a prescribed
tolerance ε.
Since α = φ(α) and x(k+1) = φ(x(k)), using the mean value theorem
(see Section 1.5.3) we ﬁnd
α −x(k+1) = φ(α) −φ(x(k)) = φ′(ξ(k)) (α −x(k))
with ξ(k) ∈Iα,x(k),
Iα,x(k) being the interval with endpoints α and x(k). Using the identity
α −x(k) = (α −x(k+1)) + (x(k+1) −x(k)),
it follows that
α −x(k) =
1
1 −φ′(ξ(k))(x(k+1) −x(k)).
(2.27)
Consequently, if φ′(x) ≃0 in a neighborhood of α, the diﬀerence between
two consecutive iterates provides a satisfactory error estimator. This
is the case for methods of order 2, including Newton’s method. This
estimate becomes the more unsatisfactory the more φ′ approaches 1.
Example 2.10 Let us compute with Newton’s method the zero α = 1 of the
function f(x) = (x −1)m−1 log(x) for m = 11 and m = 21, whose multiplicity
is equal to m. In this case Newton’s method converges with order 1; moreover,
it is possible to prove (see Exercise 2.15) that φ′
N(α) = 1 −1/m, φN being the
iteration function of the method, regarded as a ﬁxed point iteration algorithm.
As m increases, the accuracy of the error estimate provided by the diﬀerence
between two consecutive iterates decreases. This is conﬁrmed by the numerical
results in Figure 2.10 where we compare the behavior of the true error with
that of our estimator for both m = 11 and m = 21. The diﬀerence between
these two quantities is greater for m = 21.
■

2.7 Acceleration using Aitken method
63
0
100
200
300
400
500
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
(1)
(2)
Figure 2.10. Absolute values of the errors (solid line) and absolute values of
the diﬀerence between two consecutive iterates (dashed line), plotted versus
the number of iterations for the case of Example 2.10. Graphs (1) refer to
m = 11, graphs (2) to m = 21
2.7 Acceleration using Aitken method
In this paragraph we will illustrate a technique which allows to accel-
erate the convergence of a sequence obtained via ﬁxed point iterations.
Therefore, we suppose that x(k) = φ(x(k−1)), k ≥1. If the sequence
{x(k)} converges linearly to a ﬁxed point α of φ, we have from (2.24)
that, for a given k, there must be a value λ (to be determined) such that
φ(x(k)) −α = λ(x(k) −α),
(2.28)
where we have deliberately avoided to identify φ(x(k)) with x(k+1). In-
deed, the idea underlying Aitken’s method consists in deﬁning a new
value for x(k+1) (and thus a new sequence) which is a better approxima-
tion for α than that given by φ(x(k)). As a matter of fact, from (2.28)
we have that
α = φ(x(k)) −λx(k)
1 −λ
= φ(x(k)) −λx(k) + x(k) −x(k)
1 −λ
or
α = x(k) + (φ(x(k)) −x(k))/(1 −λ)
(2.29)
We must now compute λ. To do so, we introduce the following sequence
λ(k) = φ(φ(x(k))) −φ(x(k))
φ(x(k)) −x(k)
(2.30)
and verify that the following property holds:

64
2 Nonlinear equations
Lemma 2.1 If the sequence of elements x(k+1) = φ(x(k)) converges
to α, then lim
k→∞λ(k) = φ′(α).
Proof. If x(k+1) = φ(x(k)), then x(k+2) = φ(φ(x(k))) and from (2.30),
we obtain that λ(k) = (x(k+2) −x(k+1))/(x(k+1) −x(k)) or
λ(k) = x(k+2) −α −(x(k+1) −α)
x(k+1) −α −(x(k) −α)
=
x(k+2) −α
x(k+1) −α −1
1 −
x(k) −α
x(k+1) −α
from which, computing the limit and recalling (2.24), we ﬁnd
lim
k→∞λ(k) =
φ′(α) −1
1 −1/φ′(α) = φ′(α).
■
Thanks to Lemma 2.1 we can conclude that, for a given k, λ(k) can be
considered as an approximation of the previously introduced unknown
value λ. Thus, we use (2.30) in (2.29) and deﬁne a new x(k+1) as follows:
x(k+1) = x(k) −
(φ(x(k)) −x(k))2
φ(φ(x(k))) −2φ(x(k)) + x(k) , k ≥0
(2.31)
This expression is known as Aitken’s extrapolation formula and it can be
considered as a new ﬁxed point iteration for the new iteration function
φΔ(x) = xφ(φ(x)) −[φ(x)]2
φ(φ(x)) −2φ(x) + x.
This method is sometimes called Steﬀensen’s method. Clearly, function
φΔ is undetermined for x = α as the numerator and denominator vanish.
However, by applying de l’Hˆopital’s formula and assuming that φ is
diﬀerentiable with φ′(α) ̸= 1 one ﬁnds
lim
x→αφΔ(x) = φ(φ(α)) + αφ′(φ(α))φ′(α) −2φ(α)φ′(α)
φ′(φ(α))φ′(α) −2φ′(α) + 1
= α + α[φ′(α)]2 −2αφ′(α)
[φ′(α)]2 −2φ′(α) + 1
= α.
Consequently, φΔ(x) can be extended by continuity to x = α by setting
φΔ(α) = α.

2.7 Acceleration using Aitken method
65
When φ(x) = x −f(x), the case φ′(α) = 1 corresponds to a root
with multiplicity of at least 2 for f (since φ′(α) = 1 −f ′(α)). In such
situation however, we can once again prove by evaluating the limit that
φΔ(α) = α. Moreover, we can also verify that the ﬁxed points of φΔ are
all and exclusively the ﬁxed points of φ.
Aitken’s method can thus be applied for any ﬁxed point method.
Indeed, the following theorem holds:
Theorem 2.2 Let x(k+1) = φ(x(k)) be the ﬁxed point iterations
(2.20) with φ(x) = x −f(x) for computing the roots of f. Then
if f is suﬃciently regular we have:
- if the ﬁxed point iterations converge linearly to a simple root of f,
then Aitken’s method converges quadratically to the same root;
- if the ﬁxed point iterations converge with order p ≥2 to a simple
root of f, then Aitken’s method converges to the same root with
order 2p −1;
- if the ﬁxed point iterations converge linearly to a root with multi-
plicity m ≥2 of f, then Aitken’s method converges linearly to the
same root with an asymptotic convergence factor of C = 1−1/m.
In particular, if p = 1 and the root of f is simple, Aitken’s extrapola-
tion method converges even if the corresponding ﬁxed point iterations
diverge.
In Program 2.4 we report an implementation of Aitken’s method.
Here phi is a function handle associated with the expression of the iter-
ation function of the ﬁxed point method to which Aitken’s extrapolation
technique is applied. The initial datum is deﬁned by the variable x0,
while tol and nmax are the stopping criterion tolerance (on the absolute
value of the diﬀerence between two consecutive iterates) and the max-
imum number of iterations allowed, respectively. If undeﬁned, default
values nmax=100 and tol=1.e-04 are assumed.
Program 2.4. aitken: Aitken method
function [x,niter]= aitken(phi ,x0 ,tol ,nmax , varargin )
%AITKEN
Aitken ’s method.
% [ALPHA ,NITER]= AITKEN(PHI ,X0) computes
an
% approximation
of a fixed point ALPHA of function
PHI
% starting
from
the
initial
datum X0 using Aitken ’s
% extrapolation
method. The method
stops after 100
% iterations
or after the absolute
value of the
% difference
between
two
consecutive
iterates
is
% smaller
than 1.e -04.
PHI is a function
handle
% associated
with an
anonymous
function
or a function
% stored in a m-file .

66
2 Nonlinear equations
% [ALPHA ,NITER]= AITKEN(PHI ,X0 ,TOL ,NMAX ) allows to
% define the
tolerance
on the
stopping
criterion
and
% the maximum
number of iterations .
if nargin == 2
tol = 1.e -04;
nmax = 100;
elseif
nargin == 3
nmax = 100;
end
x = x0;
diff = tol + 1;
niter = 0;
while niter < nmax & diff
>= tol
gx = phi(x, varargin {:});
ggx = phi(gx ,varargin {:});
xnew = (x*ggx -gx ^2)/(ggx -2* gx+x);
diff = abs(x-xnew );
x = xnew ;
niter = niter
+ 1;
end
if (niter== nmax & diff >tol)
fprintf ([’Fails to converge
within maximum ’ ,...
’number of iterations \n’]);
end
return
Example 2.11 In order to compute the single root α = 1 for function f(x) =
ex(x −1) we apply Aitken’s method starting from the two following iteration
functions
φ0(x) = log(xex),
φ1(x) = ex + x
ex + 1 .
We use Program 2.4 with tol=1.e-10, nmax=100, x0=2 and we deﬁne the two
iteration functions as follows:
phi0 = @(x)log(x*exp(x));
phi1 = @(x)( exp(x)+x)/( exp(x)+1);
We now run Program 2.4 as follows:
[alpha ,niter]= aitken(phi0 ,x0 ,tol ,nmax )
alpha =
1.0000 + 0.0000i
niter =
10
[alpha ,niter]= aitken(phi1 ,x0 ,tol ,nmax )
alpha =
1
niter =
4
As we can see, the convergence is extremely rapid. For comparison the ﬁxed
point method with iteration function φ1 and the same stopping criterion would
have required 18 iterations, while the method corresponding to φ0 would not
have been convergent as |φ′
0(1)| = 2.
■

2.8 Algebraic polynomials
67
Let us summarize
1. A number α satisfying φ(α) = α is called a ﬁxed point of φ. For its
computation we can use the so-called ﬁxed point iterations: x(k+1) =
φ(x(k));
2. ﬁxed point iterations converge under suitable assumptions on the
iteration function φ and its ﬁrst derivative. Typically, convergence is
linear, however, in the special case when φ′(α) = 0, the ﬁxed point
iterations converge quadratically;
3. ﬁxed point iterations can also be used to compute the zeros of a
function;
4. given a ﬁxed point iteration x(k+1) = φ(x(k)), it is always possible to
construct a new sequence using Aitken’s method, which in general
converges faster.
See Exercises 2.15-2.18.
2.8 Algebraic polynomials
In this section we will consider the case where f is a polynomial of
degree n ≥0 of the form (1.9). As already anticipated, the space of all
polynomials (1.9) is denoted by the symbol Pn. We recall that if pn ∈Pn,
n ≥2, is a polynomial whose coeﬃcients ak are all real, if α ∈C is a
complex root of pn, then ¯α (the complex conjugate of α) is a root of pn
too.
Abel’s theorem guarantees that there does not exist an explicit form
to compute all the zeros of a generic polynomial pn, when n ≥5. This
fact further motivates the use of numerical methods for computing the
roots of pn.
As we have previously seen for such methods it is important to choose
an appropriate initial datum x(0) or a suitable search interval [a, b] for
the root. In the case of polynomials this is sometimes possible on the
basis of the following results.
Theorem 2.3 (Descartes’s sign rule) Let us denote by ν the
number of sign changes of the coeﬃcients {aj} and with k the num-
ber of real positive roots of a given polynomial pn ∈Pn, each counted
with its own multiplicity. Then k ≤ν and ν −k is even.
Example 2.12 The polynomial p6(x) = x6 −2x5 + 5x4 −6x3 + 2x2 + 8x −8
has zeros {±1, ±2i, 1 ± i} and thus has 1 real positive root (k = 1). Indeed,

68
2 Nonlinear equations
the number of sign changes ν of its coeﬃcients is 5 and thereafter k ≤ν and
ν −k = 4 is even.
■
Theorem 2.4 (Cauchy) All of the zeros of pn are included in the
circle Γ in the complex plane
Γ = {z ∈C : |z| ≤1 + η}, where η =
max
0≤k≤n−1|ak/an|. (2.32)
This property is barely useful when η ≫1 (for polynomial p6 in Example
2.12 for instance, we have η = 8, while all of the roots are in circles with
clearly smaller radii).
2.8.1 H¨orner’s algorithm
In this paragraph we will illustrate a method for the eﬀective evaluation
of a polynomial (and its derivative) in a given point z. Such algorithm
allows to generate an automatic procedure, called deﬂation method, for
the progressive approximation of all the roots of a polynomial.
From an algebraic point of view, (1.9) is equivalent to the following
representation
pn(x) = a0 + x(a1 + x(a2 + . . . + x(an−1 + anx) . . .)).
(2.33)
However, while (1.9) requires n sums and 2n −1 products to evaluate
pn(x) (for a given x), (2.33) only requires n sums and n products. The
expression (2.33), also known as the nested product algorithm, is the
basis for H¨orner’s algorithm. This method allows to eﬀectively evaluate
the polynomial pn in a point z by using the following synthetic division
algorithm
bn = an,
bk = ak + bk+1z, k = n −1, n −2, ..., 0
(2.34)
In (2.34) all of the coeﬃcients bk with k ≤n −1 depend on z and we
can verify that b0 = pn(z). The polynomial
qn−1(x; z) = b1 + b2x + ... + bnxn−1 =
n

k=1
bkxk−1,
(2.35)
of degree n−1 in x, depends on the z parameter (via the bk coeﬃcients)
and is called the associated polynomial of pn. Algorithm (2.34) is im-
plemented in Program 2.5. The aj coeﬃcients of the polynomial to be
evaluated are stored in vector a starting from an up to a0.

2.8 Algebraic polynomials
69
Program 2.5. horner: synthetic division algorithm
function [y,b] = horner(a,z)
%HORNER
Horner
algorithm
%
Y=HORNER(A,Z) computes
%
Y = A(1)* Z^N + A(2)* Z^(N-1) + ... + A(N)*Z + A(N+1)
%
using Horner ’s synthetic
division
algorithm .
n = length(a)-1;
b = zeros(n+1 ,1);
b(1) = a(1);
for j=2:n+1
b(j) = a(j)+b(j -1)* z;
end
y = b(n+1);
b = b(1:end -1);
return
We now want to introduce an eﬀective algorithm which, knowing the
root of a polynomial (or its approximation), is able to remove it and
then to allow the computation of the following one until all roots are
determinated.
In order to do this we should recall the following property of polyno-
mial division:
Proposition 2.3 Given two polynomials hn ∈Pn and gm ∈Pm
with m ≤n, there are a unique polynomial δ ∈Pn−m and a unique
polynomial ρ ∈Pm−1 such that
hn(x) = gm(x)δ(x) + ρ(x).
(2.36)
Thus, by dividing a polynomial pn ∈Pn by x −z, one deduces by (2.36)
that
pn(x) = b0 + (x −z)qn−1(x; z),
having denoted by qn−1 the quotient and by b0 the remainder of the
division. If z is a root of pn, then we have b0 = pn(z) = 0 and therefore
pn(x) = (x−z)qn−1(x; z). In this case the algebric equation qn−1(x; z) =
0 provides the n −1 remaining roots of pn(x). This remark suggests to
adopt the following deﬂation criterion to compute all the roots of pn.
For m = n, n −1, . . . , 1:
1. ﬁnd a root rm for pm with an appropriate approximation method;
2. compute qm−1(x; rm) using (2.34)-(2.35) (having set z = rm);
3. set pm−1 = qm−1.
In the following paragraph we propose the most widely known
method in this group, which uses Newton’s method for the approxi-
mation of the roots.

70
2 Nonlinear equations
2.8.2 The Newton-H¨orner method
As its name suggests, the Newton-H¨orner method implements the deﬂa-
tion procedure using Newton’s method to compute the roots rm. The
advantage lies in the fact that the implementation of Newton’s method
conveniently exploits H¨orner’s algorithm (2.34).
As a matter of fact, if qn−1 is the polynomial associated with pn
deﬁned in (2.35), since
p′
n(x) = qn−1(x; z) + (x −z)q′
n−1(x; z),
one has
p′
n(z) = qn−1(z; z).
Thanks to this identity, the Newton-H¨orner method for the approxima-
tion of a (real or complex) root rj of pn (j = 1, . . . , n) takes the following
form:
given an initial estimation r(0)
j
of the root, compute for each k ≥0 until
convergence
r(k+1)
j
= r(k)
j
−
pn(r(k)
j
)
p′n(r(k)
j
)
= r(k)
j
−
pn(r(k)
j
)
qn−1(r(k)
j
; r(k)
j
)
(2.37)
We now use the deﬂation technique, exploiting the fact that pn(x) =
(x −rj)pn−1(x). We can then proceed to the approximation of a zero of
pn−1 and so on until all the roots of pn are processed.
Consider that when rj ∈C, it is necessary to perform the computa-
tion in complex arithmetics, taking r(0)
j
as the non-null imaginary part.
Otherwise, the Newton-H¨orner method would generate a sequence {r(k)
j
}
of real numbers.
The Newton-H¨orner method is implemented in Program 2.6. The
coeﬃcients aj of the polynomial for which we intend to compute the
roots are stored in vector a starting from an up to a0. The other input
parameters, tol and nmax, are the stopping criterion tolerance (on the
absolute value of the diﬀerence between two consecutive iterates) and
the maximum number of iterations allowed, respectively. If undeﬁned,
the default values nmax=100 and tol=1.e-04 are assumed. As an out-
put, the program returns in vectors roots and iter the computed roots
and the number of iterations required to compute each of the values,
respectively.

2.8 Algebraic polynomials
71
Program 2.6. newtonhorner: Newton-H¨orner method
function [roots ,iter ]= newtonhorner (a,x0 ,tol ,nmax )
% NEWTONHORNER
Newton -Horner method
% [ROOTS ,ITER ]= NEWTONHORNER (A,X0) computes
the roots of
% polynomial
% P(X)= A(1)* X^N + A(2)* X^(N-1) + ... + A(N)*X + A(N+1)
% using the Newton -Horner method
starting
from
the
% initial
guess X0. The method stops for
each
root
% after 100
iterations
or after the absolute
value of
% the difference
between two
consecutive
iterates
is
% smaller
than 1.e -04.
% [ROOTS ,ITER ]= NEWTONHORNER (A,X0 ,TOL ,NMAX ) allows to
% define the
tolerance
on the
stopping
criterion
and
% the maximum
number of iterations .
if nargin == 2
tol = 1.e -04;
nmax = 100;
elseif
nargin == 3
nmax = 100;
end
n=length(a)-1;
roots = zeros(n ,1);
iter = zeros(n,1);
for k = 1:n
% Newton iterations
niter = 0; x = x0; diff = tol + 1;
while niter < nmax & diff
>= tol
[pz ,b] = horner(a,x);
[dpz ,b] = horner(b,x);
xnew = x - pz/dpz;
diff = abs(xnew -x);
niter = niter + 1;
x = xnew ;
end
if (niter == nmax & diff > tol)
fprintf ([’ Fails to converge
within maximum ’ ,...
’number of iterations \n ’]);
end
% Deflation
[pz ,a] = horner(a,x); roots(k) = x; iter(k) = niter;
end
Remark 2.2 In order to minimize the propagation of roundoﬀerrors, during
the deﬂation process it is better to ﬁrst approximate the root r1 with smallest
absolute value and then to proceed to the computation of the following roots
r2, r3, . . ., until the one with the largest absolute value is reached (to learn
more, see for instance [QSS07]).
■
Example 2.13 To compute the roots {1, 2, 3} of the polynomial p3(x) =
x3 −6x2 + 11x −6 we use Program 2.6
a=[1
-6 11
-6]; [x,niter]= newtonhorner (a,0,1.e -15 ,100)
x =
1
2
3
niter =
8
8
2

72
2 Nonlinear equations
The method computes all three roots accurately and in few iterations. As
pointed out in Remark 2.2 however, the method is not always so eﬀective. For
instance, if we consider the polynomial p4(x) = x4 −7x3 + 15x2 −13x + 4
(which has the root 1 of multiplicity 3 and a single root with value 4) we ﬁnd
the following results
a=[1
-7 15
-13 4];
format
long ;
[x,niter]= newtonhorner (a,0,1.e -15 ,100)
x =
1.000006935337374
0.999972452635761
1.000020612232168
3.999999999794697
niter =
61
100
6
2
The loss of accuracy is quite evident for the computation of the multiple root,
and becomes as more relevant as the multiplicity increases. More in general, it
can be shown (see [QSS07]) that the problem of root-ﬁnding for a function f
becomes ill-conditioned (that is, very sensitive to perturbations on the data)
as the derivative f ′ gets small at the roots. For an instance, see Exercise 2.6.
.
■
2.9 What we haven’t told you
The most sophisticated methods for the computation of the zeros of
a function combine diﬀerent algorithms. In particular, the MATLAB
function fzero (see Section 1.5.1) adopts the so called Dekker-Brent
method (see [QSS07], Section 6.2.3). In its basic form fzero(fun,x0)
computes the zero of the function associated with the function handle
fun, starting from x0.
For instance, we could solve the problem in Example 2.1 also by
fzero, using the initial value x0=0.3 (as done by Newton’s method) via
the following instructions:
M=6000; v=1000; f=@(r) M-v*(1+ r)/r*((1+r)^5 -1);
x0 =0.3;
[alpha ,res ,flag ,info ]= fzero(f,x0);
We ﬁnd the root alpha=0.06140241153653 after 7 iterations and 29
evaluations of the function f, with a residual res=-1.8190e-12.
The variable info is a structure with 5 subﬁelds. Precisely the ﬁelds
info.iterations and info.funcCount contain number of iterations
and global number of function evaluations performed during the call,
respectively. We note that when output parameter flag assumes a neg-
ative occurrence, then the function fzero failed in searching the zero.

2.9 What we haven’t told you
73
For a comparison, Newton method converges in 6 iterations to the value
0.06140241153653 with a residual equal to 9.0949e-13, but it requires
the knowledge of the ﬁrst derivative of f and a total of 12 function
evaluations.
In order to compute the zeros of a polynomial, in addition to the
Newton-H¨orner method, we can cite the methods based on Sturm se-
quences, M¨uller’s method, (see [Atk89] or [QSS07]) and Bairstow’s
method ([RR01], page 371 and following). A diﬀerent approach con-
sists in characterizing the zeros of a function as the eigenvalues of a
special matrix (called the companion matrix) and then using appropri-
ate techniques for their computation. This approach is adopted by the
MATLAB function roots which has been introduced in Section 1.5.2.
We have mentioned in Section 2.5 how to set up a Newton method for
a nonlinear system, like (2.15). More in general, any ﬁxed point iteration
can be easily extended to compute the roots of nonlinear systems. Other
methods exist as well, such as the Broyden and quasi-Newton methods,
which can be regarded as generalizations of Newton’s method (see [JS96],
[Deu04], [SM03] and [QSS07, Chapter 7]).
The MATLAB instruction
fsolve
zero =fsolve(@fun ,x0)
allows the computation of one zero of a nonlinear system deﬁned via the
user-deﬁned function fun starting from the vector x0 as initial guess.
The function fun returns the n values fi(¯x1, . . . , ¯xn), i = 1, . . . , n, for
any given input vector (¯x1, . . . , ¯xn)T .
For instance, in order to solve the nonlinear system (2.17) using
fsolve the corresponding user-deﬁned function, which we call systemnl,
is deﬁned as follows:
function
fx=systemnl (x)
fx (1) = x(1)^2+x(2)^2 -1;
fx (2) = sin(pi *0.5*x(1))+x(2)^3;
The MATLAB instructions to solve this system are therefore:
x0 = [1 1];
alpha=fsolve(@systemnl ,x0)
alpha =
0.4761
-0.8794
Using this procedure we have found only one of the two roots. The other
can be computed starting from the initial datum -x0.
Octave 2.1 The commands fzero and fsolve have exactly the same
purpose in MATLAB and Octave, however their interface diﬀer slightly
in what concerns the optional arguments. We encourage the reader to
study the help documentation of both commands in each environment.
.
■

74
2 Nonlinear equations
2.10 Exercises
Exercise 2.1 Given the function f(x) = cosh x+cos x−γ, for γ = 1, 2, 3 ﬁnd
an interval that contains the zero of f. Then compute the zero by the bisection
method with a tolerance of 10−10.
Exercise 2.2 (State equation of a gas) For carbon dioxide (CO2) the co-
eﬃcients a and b in (2.1) take the following values: a = 0.401Pa m6,
b = 42.7 · 10−6m3 (Pa stands for Pascal). Find the volume occupied by 1000
molecules of CO2 at a temperature T = 300K and a pressure p = 3.5 · 107 Pa
by the bisection method, with a tolerance of 10−12 (the Boltzmann constant
is k = 1.3806503 · 10−23 Joule K−1).
Exercise 2.3 Consider a plane whose slope varies with constant rate ω, and
a dimensionless object which is steady at the initial time t = 0. At time t > 0
its position is
s(t, ω) =
g
2ω2 [sinh(ωt) −sin(ωt)],
where g = 9.8 m/s2 denotes the gravity acceleration. Assuming that this object
has moved by 1 meter in 1 second, compute the corresponding value of ω with
a tolerance of 10−5.
Exercise 2.4 Prove inequality (2.6).
Exercise 2.5 Motivate why in Program 2.1 the instruction x(2) = x(1)+
(x(3)- x(1))*0.5 has been used instead of the more natural one x(2)=(x(1)+
x(3))*0.5 in order to compute the midpoint.
Exercise 2.6 Apply Newton’s method to solve Exercise 2.1. Why is this
method not accurate when γ = 2?
Exercise 2.7 Apply Newton’s method to compute the square root of a pos-
itive number a. Proceed in a similar manner to compute the cube root of
a.
Exercise 2.8 Assuming that Newton’s method converges, show that (2.9)
is true when α is a simple root of f(x) = 0 and f is twice continuously
diﬀerentiable in a neighborhood of α.
Exercise 2.9 (Rods system) Apply Newton’s method to solve Problem 2.3
for β ∈[0, 2π/3] with a tolerance of 10−5. Assume that the lengths of the rods
are a1 = 10 cm, a2 = 13 cm, a3 = 8 cm and a4 = 10 cm. For each value of β
consider two possible initial data, x(0) = −0.1 and x(0) = 2π/3.
Exercise 2.10 Notice that the function f(x) = ex −2x2 has 3 zeros, α1 < 0,
α2 and α3 positive. For which value of x(0) does Newton’s method converge
to α1?

2.10 Exercises
75
L
l1
l2
γ
α
Figure 2.11. The problem of a rod sliding in a corridor
Exercise 2.11 Use Newton’s method to compute the zero of f(x) = x3 −
3x22−x + 3x4−x −8−x in [0, 1] and explain why convergence is not quadratic.
Exercise 2.12 A projectile is ejected with velocity v0 and angle α in a tunnel
of height h and reaches its maximum range when α is such that sin(α) =

2gh/v2
0, where g = 9.8 m/s2 is the gravity acceleration. Compute α using
Newton’s method, assuming that v0 = 10 m/s and h = 1 m.
Exercise 2.13 (Investment fund) Solve Problem 2.1 by Newton’s method
with a tolerance of 10−12, assuming M = 6000 euros, v = 1000 euros and
n = 5. As an initial guess take the result obtained after 5 iterations of the
bisection method applied on the interval (0.01, 0.1).
Exercise 2.14 A corridor has the form indicated in Figure 2.11. The maxi-
mum length L of a rod that can pass from one extreme to the other by sliding
on the ground is given by
L = l2/(sin(π −γ −α)) + l1/ sin(α),
where α is the solution of the nonlinear equation
l2 cos(π −γ −α)
sin2(π −γ −α) −l1 cos(α)
sin2(α) = 0.
(2.38)
Compute α by Newton’s method when l2 = 10, l1 = 8 and γ = 3π/5.
Exercise 2.15 Let φN be the iteration function of Newton’s method when
regarded as a ﬁxed point iteration. Show that φ′
N(α) = 1 −1/m where α
is a zero of f with multiplicity m. Deduce that Newton’s method converges
quadratically if α is a simple root of f(x) = 0, and linearly otherwise.
Exercise 2.16 Deduce from the graph of f(x) = x3 + 4x2 −10 that this
function has a unique real zero α. To compute α use the following ﬁxed point
iterations: given x(0), deﬁne x(k+1) such that
x(k+1) = 2(x(k))3 + 4(x(k))2 + 10
3(x(k))2 + 8x(k)
,
k ≥0
and analyze its convergence to α.

76
2 Nonlinear equations
Exercise 2.17 Analyze the convergence of the ﬁxed point iterations
x(k+1) = x(k)[(x(k))2 + 3a]
3(x(k))2 + a
,
k ≥0,
for the computation of the square root of a positive number a.
Exercise 2.18 Repeat the computations carried out in Exercise 2.11 this time
using the stopping criterion based on the residual. Which result is the more
accurate?

3
Approximation of functions and data
Approximating a function f consists of replacing it by another function
˜f of simpler form that may be used as its surrogate. This strategy is
used frequently in numerical integration where, instead of computing
 b
a f(x)dx, one carries out the exact computation of
 b
a ˜f(x)dx, ˜f being
a function simple to integrate (e.g. a polynomial), as we will see in the
next chapter. In other instances the function f may be available only
partially through its values at some selected points. In these cases we
aim at constructing a continuous function ˜f that could represent the
empirical law which is behind the ﬁnite set of data. We provide some
examples which illustrate this kind of approach.
3.1 Some representative problems
Problem 3.1 (Climatology) The air temperature near the ground de-
pends on the concentration K of the carbon acid (H2CO3) therein. In
Table 3.1 (taken from Philosophical Magazine 41, 237 (1896)) we report
for diﬀerent latitudes on the Earth and for four diﬀerent values of K,
the variation δK = θK −θ ¯
K of the average temperature with respect
to the average temperature corresponding to a reference value ¯K of K.
Here ¯K refers to the value measured in 1896, and is normalized to one.
In this case we can generate a function that, on the basis of the available
data, provides an approximate value of the average temperature at any
possible latitude and for other values of K (see Example 3.1).
■
Problem 3.2 (Finance) In Figure 3.1 we report the price of a stock
at the Zurich stock exchange over two years. The curve was obtained by
joining with a straight line the prices reported at every day’s closure. This
simple representation indeed implicitly assumes that the prices change
linearly in the course of the day (we anticipate that this approximation
A. Quarteroni et al., Scientiﬁc Computing with MATLAB and Octave,
Texts in Computational Science and Engineering 2,
DOI 10.1007/978-3-642-45367-0 3, © Springer-Verlag Berlin Heidelberg 2014
77

78
3 Approximation of functions and data
Table 3.1. Variation of the average yearly temperature on the Earth for four
diﬀerent values of the concentration K of carbon acid at diﬀerent latitudes
δK
Latitude
K = 0.67
K = 1.5
K = 2.0
K = 3.0
65
-3.1
3.52
6.05
9.3
55
-3.22
3.62
6.02
9.3
45
-3.3
3.65
5.92
9.17
35
-3.32
3.52
5.7
8.82
25
-3.17
3.47
5.3
8.1
15
-3.07
3.25
5.02
7.52
5
-3.02
3.15
4.95
7.3
-5
-3.02
3.15
4.97
7.35
-15
-3.12
3.2
5.07
7.62
-25
-3.2
3.27
5.35
8.22
-35
-3.35
3.52
5.62
8.8
-45
-3.37
3.7
5.95
9.25
-55
-3.25
3.7
6.1
9.5
is called composite linear interpolation). We ask whether from this graph
one could predict the stock price for a short time interval beyond the
time of the last quotation. We will see in Section 3.6 that this kind of
prediction could be guessed by resorting to a special technique known as
least-squares approximation of data (see Example 3.12).
■
Problem 3.3 (Biomechanics) We consider a mechanical test to es-
tablish the link between stresses and deformations of a sample of biolog-
ical tissue (an intervertebral disc, see Figure 3.2). Starting from the data
collected in Table 3.2 (taken from P.Komarek, Chapt. 2 of Biomechan-
ics of Clinical Aspects of Biomedicine, 1993, J.Valenta ed., Elsevier) in
nov00
may01
nov01
may02
0
2
4
6
8
10
12
14
16
Figure 3.1. Price variation of a stock over two years

3.2 Approximation by Taylor’s polynomials
79
A
L
ΔL
σ = F/A
F
ϵ = ΔL/L
Figure 3.2. A schematic representation of an intervertebral disc
Table 3.2. Values of the deformation for diﬀerent values of a stress applied
on an intervertebral disc
test
stress σ
stress ϵ
test
stress σ
stress ϵ
1
0.00
0.00
5
0.31
0.23
2
0.06
0.08
6
0.47
0.25
3
0.14
0.14
7
0.60
0.28
4
0.25
0.20
8
0.70
0.29
Example 3.13 we will estimate the deformation corresponding to a stress
σ = 0.9 MPa (MPa= 100 N/cm2).
■
Problem 3.4 (Robotics) We want to approximate the planar trajec-
tory followed by a robot (idealized as a material point) during a working
cycle in an industry. The robot should satisfy a few constraints: it must
be steady at the point (0, 0) in the plane at the initial time (say, t = 0),
transit through the point (1, 2) at t = 1, get the point (4, 4) at t = 2,
stop and restart immediately and reach the point (3, 1) at t = 3, return
to the initial point at time t = 5, stop and restart a new working cycle.
In Example 3.10 we will solve this problem using the splines functions.
■
3.2 Approximation by Taylor’s polynomials
A function f in a given interval can be replaced by its Taylor polynomial,
which was introduced in Section 1.5.3. This technique is computationally
expensive since it requires the knowledge of f and its derivatives up to
the order n (the polynomial degree) at a given point x0. Moreover, the
Taylor polynomial may fail to accurately represent f far enough from
the point x0. For instance, in Figure 3.3 we compare the behavior of
f(x) = 1/x with that of its Taylor polynomial of degree 10 built around
the point x0 = 1. This picture also shows the graphical interface of
the MATLAB function taylortool which allows the computation of taylortool
Taylor’s polynomial of arbitrary degree for any given function f. The

80
3 Approximation of functions and data
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
0
0.5
1
1.5
2
2.5
3
TN(x) =
(x − 1)2 − x − (x − 1)3 + (x − 1)4 − (x − 1)5 + (x − 1)6 −...+ 2
Figure 3.3. Comparison between the function f(x) = 1/x (solid line) and its
Taylor polynomial of degree 10 related to the point x0 = 1 (dashed line). The
explicit form of the Taylor polynomial is also reported
agreement between the function and its Taylor polynomial is very good
in a small neighborhood of x0 = 1 while it becomes unsatisfactory when
x−x0 gets large. Fortunately, this is not the case of other functions such
as the exponential function which is approximated quite nicely for all
x ∈R by its Taylor polynomial related to x0 = 0, provided that the
degree n is suﬃciently large.
In the course of this chapter we will introduce approximation methods
that are based on alternative approaches.
Octave 3.1 taylortool is not available in Octave.
■
3.3 Interpolation
As seen in Problems 3.1, 3.2 and 3.3, in several applications it may
happen that a function is known only through its values at some given
points. We are therefore facing a (general) case where n + 1 couples
{xi, yi}, i = 0, . . . , n, are given; the points xi are all distinct and are
called nodes.
For instance in the case of Table 3.1, n is equal to 12, the nodes xi are
the values of the latitude reported in the ﬁrst column, while the yi are
the corresponding values (of the temperature variation) in the remaining
columns.

3.3 Interpolation
81
In such a situation it seems natural to require the approximate func-
tion ˜f to satisfy the set of relations
˜f(xi) = yi, i = 0, 1, . . . , n
(3.1)
Such an ˜f is called interpolant of the set of data {yi} and equations (3.1)
are the interpolation conditions.
Several kinds of interpolants could be envisaged, such as:
-
polynomial interpolant:
˜f(x) = a0 + a1x + a2x2 + . . . + anxn;
-
trigonometric interpolant:
˜f(x) = a−Me−iMx + . . . + a0 + . . . + aMeiMx
where M is an integer equal to n/2 if n is even, (n + 1)/2 if n is odd,
and i is the imaginary unit;
-
rational interpolant:
˜f(x) =
a0 + a1x + . . . + akxk
ak+1 + ak+2x + . . . + ak+n+1xn .
For simplicity we only consider those interpolants which depend lin-
early on the unknown coeﬃcients ai. Both polynomial and trigonometric
interpolation fall into this category, whereas the rational interpolant does
not.
3.3.1 Lagrangian polynomial interpolation
Let us focus on the polynomial interpolation. The following result holds:
Proposition 3.1 For any set of couples {xi, yi}, i = 0, . . . , n, with
distinct nodes xi, there exists a unique polynomial of degree less
than or equal to n, which we indicate by Πn and call interpolating
polynomial of the values yi at the nodes xi, such that
Πn(xi) = yi, i = 0, . . . , n
(3.2)
In the case where the {yi, i = 0, . . . , n} represent the values of a
continuous function f, Πn is called interpolating polynomial of f
(in short, interpolant of f) and will be denoted by Πnf.

82
3 Approximation of functions and data
0
0.5
1
1.5
2
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
x
Figure 3.4. The polynomial ϕ2 ∈P4 associated with a set of 5 equispaced
nodes
To verify uniqueness we proceed by contradiction and suppose that
there exist two distinct polynomials of degree n, Πn and Π∗
n, both sat-
isfying the nodal relation (3.2). Their diﬀerence, Πn −Π∗
n, would be a
polynomial of degree n which vanishes at n + 1 distinct points. Owing
to a well known theorem of Algebra, such a polynomial should vanish
identically, and then Π∗
n must coincide with Πn.
In order to obtain an expression for Πn, we start from a very special
case where yi vanishes for all i apart from i = k (for a ﬁxed k) for which
yk = 1. Then setting ϕk(x) = Πn(x), we must have (see Figure 3.4)
ϕk ∈Pn, ϕk(xj) = δjk =
 1
if j = k,
0
otherwise,
where δjk is the Kronecker symbol.
The functions ϕk have the following expression:
ϕk(x) =
n

j=0
j̸=k
x −xj
xk −xj
,
k = 0, . . . , n.
(3.3)
We move now to the general case where {yi, i = 0, . . . , n} is a set of
arbitrary values. Using an obvious superposition principle we can obtain
the following expression for Πn
Πn(x) =
n

k=0
ykϕk(x)
(3.4)
Indeed, this polynomial satisﬁes the interpolation conditions (3.2), since
Πn(xi) =
n

k=0
ykϕk(xi) =
n

k=0
ykδik = yi,
i = 0, . . . , n.

3.3 Interpolation
83
Due to their special role, the functions ϕk are called Lagrange char-
acteristic polynomials, and (3.4) is the Lagrange form of the interpolant.
In MATLAB we can store the n+1 couples {(xi, yi)} in the vectors
x and y, and then the instruction c=polyfit(x,y,n) will provide the
polyfit
coeﬃcients of the interpolating polynomial. Precisely, c(1) will contain
the coeﬃcient of xn, c(2) that of xn−1, . . . and c(n+1) the value of
Πn(0). (More on this command can be found in Section 3.6.) As already
seen in Chapter 1, we can then use the instruction p=polyval(c,z)
to compute the value p(j) attained by the interpolating polynomial at
z(j), j=1,...,m, the latter being a set of m arbitrary points.
In the case when the explicit form of the function f is available, the
Lagrange interpolant of f will be denoted by Πnf. In order to obtain the
vector y of values of f at some speciﬁc nodes (which should be stored in
a vector x) we can use the instruction y=f(x).
Example 3.1 (Climatology) To obtain the interpolating polynomial for the
data of Problem 3.1 relating to the value K = 0.67 (ﬁrst column of Table 3.1),
using only the values of the temperature for the latitudes 65, 35, 5, -25, -55,
we can use the following MATLAB instructions:
x=[-55
-25 5 35 65]; y=[ -3.25
-3.2
-3.02
-3.32
-3.1];
format
short e; c=polyfit(x,y ,4)
c =
8.2819e-08
-4.5267e-07
-3.4684e-04
3.7757e-04
-3.0132e+00
The graph of the interpolating polynomial can be obtained as follows:
z= linspace (x(1),x(end ) ,100);
p= polyval(c,z);
plot (z,p,x,y,’o’); grid on;
In order to obtain a smooth curve we have evaluated our polynomial at 101
equispaced points in the interval [−55, 65] (as a matter of fact, MATLAB plots
are always constructed on piecewise linear interpolation between neighboring
points). Note that the instruction x(end) picks up directly the last component
of the vector x, without specifying the length of the vector. In Figure 3.5 the
ﬁlled circles correspond to those values which have been used to construct the
interpolating polynomial, whereas the empty circles correspond to values that
have not been used. We can appreciate the qualitative agreement between the
curve and the data distribution.
■
Using the following result we can evaluate the error obtained by re-
placing f with its interpolating polynomial Πnf:

84
3 Approximation of functions and data
−60
−40
−20
0
20
40
60
80
−3.45
−3.35
−3.25
−3.15
−3.05
−2.95
Figure 3.5. The interpolating polynomial of degree 4 introduced in Example
3.1
Proposition 3.2 Let I be a bounded interval, and consider n + 1
distinct interpolation nodes {xi, i = 0, . . . , n} in I. Let f be contin-
uously diﬀerentiable up to order n + 1 in I. Then ∀x ∈I ∃ξx ∈I
such that
Enf(x) = f(x) −Πnf(x) = f (n+1)(ξx)
(n + 1)!
n

i=0
(x −xi)
(3.5)
Obviously, Enf(xi) = 0, i = 0, . . . , n. Result (3.5) can be better speciﬁed
in the case of a uniform distribution of nodes, that is when xi = xi−1 +h
for i = 1, . . . , n, for a given h > 0 and a given x0. As stated in Exercise
3.1, ∀x ∈(x0, xn) one can verify that

n

i=0
(x −xi)
 ≤n!hn+1
4
,
(3.6)
and therefore
max
x∈I |Enf(x)| ≤
max
x∈I |f (n+1)(x)|
4(n + 1)
hn+1.
(3.7)
Unfortunately, we cannot deduce from (3.7) that the error tends to
0 when n →∞, in spite of the fact that hn+1/[4(n + 1)] tends to 0. In
fact, as shown in Example 3.2, there exist functions f for which the limit
can even be inﬁnite, that is
lim
n→∞max
x∈I |Enf(x)| = ∞.

3.3 Interpolation
85
This striking result indicates that by increasing the degree n of the
interpolating polynomial we do not necessarily obtain a better recon-
struction of f. For instance, should we use all data of the second column
of Table 3.1, we would obtain the interpolating polynomial Π12f repre-
sented in Figure 3.6, left, whose behavior in the vicinity of the left-hand
of the interval is far less satisfactory than that obtained in Figure 3.5
using a much smaller number of nodes. An even worse result may arise
for a special class of functions, as we report in the next example.
Example 3.2 (Runge) If the function f(x) = 1/(1 + x2) is interpolated
at equispaced nodes in the interval I = [−5, 5], the error maxx∈I |Enf(x)|
tends to inﬁnity when n →∞. This is due to the fact that if n →∞the
order of magnitude of maxx∈I |f (n+1)(x)| outweighs the inﬁnitesimal order of
hn+1/[4(n+1)]. This conclusion can be veriﬁed by computing the maximum of
f and its derivatives up to the order 21 by means of the following MATLAB
instructions:
syms x; n=20; f=1/(1+x^2);
df=diff (f,1);
cdf=matlabFunction (df);
for i = 1:n+1
df = diff (df ,1);
cdfn = matlabFunction (df);
x = fzero(cdfn ,0); M(i) = abs(cdf(x)); cdf = cdfn ;
end
The maximum of the absolute values of the functions f (n), n = 1, . . . , 21,
are stored in the vector M. Notice that the command matlabFunction converts
matlab-
Function
the symbolic expression df into a function handle that is passed to the function
fzero. In particular, the absolute values of f (n) for n = 3, 9, 15, 21 are:
format
short e; M([3 ,9 ,15 ,21])
ans =
4.6686e+00
3.2426e+05
1.2160e+12
4.8421e+19
while the corresponding values of the maximum of
n

i=0
(x −xi)/(n + 1)! are
z = linspace ( -5 ,5 ,10000);
for n=0:20; h=10/(n+1); x=[-5:h:5];
c=poly (x); r(n+1)= max(polyval (c,z));
r(n+1)= r(n+1)/ prod ([1: n+1]);
end
r([3 ,9 ,15 ,21])
ans =
1.1574e+01
5.1814e-02
1.3739e-05
4.7247e-10
where c=poly(x) is a vector whose components are the coeﬃcients of that
poly
polynomial whose roots are the elements of the vector x. It follows that
maxx∈I |Enf(x)| attains the following values:
5.4034e+01
1.6801e+04
1.6706e+07
2.2877e+10
for n = 3, 9, 15, 21, respectively. The lack of convergence is also indicated by
the presence of severe oscillations in the graph of the interpolating polynomial
with respect to the graph of f, especially near the endpoints of the interval
(see Figure 3.6, right). This behavior is known as Runge’s phenomenon.
■

86
3 Approximation of functions and data
−60
−40
−20
0
20
40
60
80
−3.5
−3.4
−3.3
−3.2
−3.1
−3
−2.9
−2.8
−2.7
−5
−3
−1
1
3
5
−4
−3
−2
−1
0
1
2
Figure 3.6. Two examples of Runge’s phenomenon: at left, Π12 computed for
the data of Table 3.1, column K = 0.67; at right, Π12f (solid line) computed
on 13 equispaced nodes for the function f(x) = 1/(1 + x2) (dashed line)
Besides (3.7), the following inequality can also be proved:
max
x∈I |f ′(x) −(Πnf)′(x)| ≤Chnmax
x∈I |f (n+1)(x)|,
where C is a constant independent of h. Therefore, if we approximate
the ﬁrst derivative of f by the ﬁrst derivative of Πnf, we loose an order
of convergence with respect to h.
In MATLAB, (Πnf)′ can be computed using the instruction [d]=
polyder(c), where c is the input vector in which we store the coeﬃcients
polyder
of the interpolating polynomial, while d is the output vector where we
store the coeﬃcients of its ﬁrst derivative (see Section 1.5.2).
Octave 3.2 The command matlabFunction is not available in Octave.
■
3.3.2 Stability of polynomial interpolation
What happens to the interpolating polynomials if, instead of consid-
ering exact values f(xi) we consider perturbed ones, say ˆf(xi), with
i = 0, . . . , n? Note that perturbations arise because of either rounding
errors or uncertainty in measuring data themselves.
Let Πn ˆf be the exact polynomial interpolating the values ˆf(xi). De-
noting by x the vector whose components are the interpolation nodes
{xi}, we have
max
x∈I |Πnf(x) −Πn ˆf(x)| = max
x∈I

n

i=0

f(xi) −ˆf(xi)

ϕi(x)

≤Λn(x) max
0≤i≤n
f(xi) −ˆf(xi)

(3.8)
where

3.3 Interpolation
87
Λn(x) = max
x∈I
n

i=0
|ϕi(x)|
(3.9)
is the so-called Lebesgue’s constant which depends on interpolation
nodes. Small variations on the nodal values f(xi) yield small changes
on the interpolating polynomial, provided that the Lebesgue’s constant
is small. Λn can therefore be regarded as a condition number of the inter-
polation problem. For Lagrange interpolation at equispaced nodes one
has
Λn(x) ≃
2n+1
en(log n + γ),
(3.10)
where e ≃2.71834 is the Napier (or Euler) number, while γ ≃0.547721
is the Euler constant (see [Hes98] and [Nat65]).
For large values of n, Lagrange interpolation on equispaced nodes
can therefore be unstable, as we can deduce from the following example.
(See also the Exercise 3.8.)
Example 3.3 To interpolate f(x) = sin(2πx) at 22 equispaced nodes in the
interval [−1, 1], let us generate the values ˆf(xi) by a random perturbation of
the exact values f(xi), such that
max
i=0,...,21 |f(xi) −ˆf(xi)| ≃9.5 · 10−4.
In Figure 3.7 the two interpolating polynomials Π21f and Π21 ˆf are com-
pared, the diﬀerence between the two polynomials is much larger than the
perturbations on data, precisely max
x∈I |Πnf(x) −Πn ˆf(x)| ≃3.1342, and the
gap is especially severe near the endpoints of the interval. Note that in this
example the Lebesgue’s constant is very high, being Λ21(x) ≃20454.
■
See the Exercises 3.1-3.4.
3.3.3 Interpolation at Chebyshev nodes
Runge’s phenomenon can be avoided if a suitable distribution of nodes
is used. In particular, in an arbitrary interval [a, b], we can consider the
so called Chebyshev-Gauss-Lobatto nodes (see Figure 3.8, right):
xi = a + b
2
+ b −a
2
xi, where xi = −cos(πi/n), i = 0, . . . , n
(3.11)
Obviously, xi = xi, i = 0, . . . , n, when [a, b] = [−1, 1]. Indeed, for this
special distribution of nodes it is possible to prove that, if f is a contin-
uous and diﬀerentiable function in [a, b], Πnf converges to f as n →∞
for all x ∈[a, b].

88
3 Approximation of functions and data
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−3
−2
−1
0
1
2
3
4
Figure 3.7. The eﬀect of instability on equispaced Lagrange interpolation.
Π21f (solid line) and Π21 ˆf (dashed line) represent the exact and perturbed
interpolation polynomials, respectively, for the Example 3.3
The Chebyshev-Gauss-Lobatto nodes, which are the abscissas of eq-
uispaced nodes on the unit semi-circumference, lie inside [a, b] and are
clustered near the endpoints of this interval (see Figure 3.8, right).
Another nonuniform distribution of nodes in the interval (a, b), shar-
ing the same convergence properties is provided by the Chebyshev-Gauss
nodes:
xi = a + b
2
−b −a
2
cos
2i + 1
n + 1
π
2

, i = 0, . . . , n
(3.12)
Example 3.4 We consider anew the function f of Runge’s example and com-
pute its interpolating polynomial at Chebyshev-Gauss-Lobatto nodes. The lat-
ter can be obtained through the following MATLAB instructions:
xc = -cos(pi *[0: n]/n); x = (a+b)*0.5+(b-a)*xc *0.5;
where n+1 is the number of nodes, while a and b are the endpoints of the
interpolation interval (in the sequel we choose a=-5 and b=5). Then we compute
the interpolating polynomial by the following instructions:
f= @(x) 1./(1+x.^2); y = f(x); c = polyfit(x,y,n);
Now let us compute the absolute values of the diﬀerences between f and
its interpolant relative to Chebyshev-Gauss-Lobatto nodes at as many as 1000
equispaced points in the interval [−5, 5] and take the maximum error values:
x1 = linspace ( -5 ,5 ,1000); p=polyval (c,x1);
f1 = f(x1); err = max(abs(p-f1 ));
As we see in Table 3.3, the maximum of the error decreases when n in-
creases.
■

3.3 Interpolation
89
−5
−3
−1
1
3
5
0
0.2
0.4
0.6
0.8
1
−1 = x0
xn = 1
xi
π/n
0
Figure 3.8. The left side picture shows the comparison between the function
f(x) = 1/(1+x2) (thin solid line) and its interpolating polynomials of degree 8
(dashed line) and 12 (solid line) at the Chebyshev-Gauss-Lobatto nodes. Note
that the amplitude of spurious oscillations decreases as the degree increases.
The right side picture shows the distribution of Chebyshev-Gauss-Lobatto
nodes in the interval [−1, 1]
Table 3.3. The interpolation error for Runge’s function f(x) = 1/(1 + x2)
when the Chebyshev-Gauss-Lobatto nodes (3.11) are used
n
5
10
20
40
En
0.6386
0.1322
0.0177
0.0003
When the Lagrange interpolant is deﬁned at the Chebyshev-Gauss-
Lobatto nodes (3.11), then the Lebesgue’s constant can be bounded as
follows ([Hes98])
Λn(x) < 2
π

log n + γ + log 8
π

+
π
72 n2 ,
(3.13)
while when interpolation is carried out on the Chebyshev-Gauss nodes
(3.12), then
Λn(x) < 2
π

log(n + 1) + γ + log 8
π

+
π
72(n + 1)2 .
(3.14)
As usual, γ ≃0.57721 denotes the Euler constant.
By comparing (3.13) and (3.14) with the estimate (3.10), we can
conclude that the Lagrange interpolation at Chebyshev nodes is much
less sensitive to perturbation errors than interpolation at equispaced
nodes.
Example 3.5 Let us use now interpolation at the Chebyshev nodes, either
(3.11) and (3.12). Starting from the same data perturbations considered in
Example 3.3, when n = 21 we have max
x∈I |Πnf(x)−Πn ˆf(x)| ≃1.0977·10−3 for

90
3 Approximation of functions and data
nodes (3.11), while max
x∈I |Πnf(x) −Πn ˆf(x)| ≃1.1052 · 10−3 for nodes (3.12).
This result is in good agreement with the estimates (3.13) and (3.14) which,
for n = 21 yield Λn(x) ≲2.9008 and Λn(x) ≲2.9304, respectively.
■
3.3.4 Barycentric interpolation formula
The interpolating polynomial Πn(x) introduced in Proposition 3.1 can
be computed by the following barycentric formula ([BT04])
Πn(x) =
n

k=0
wk
x −xk
yk
n

k=0
wk
x −xk
(3.15)
where
wk =
⎛
⎜
⎝
n

j=0
j̸=k
(xk −xj)
⎞
⎟
⎠
−1
,
k = 0, . . . , n,
(3.16)
are called barycentric weigths.
In order to deduce (3.15) from (3.4), we rewrite the Lagrange char-
acteristic polynomials (3.3) as
ϕk(x) =
n

j=0
j̸=k
x −xj
xk −xj
=
⎛
⎝
n

j=0
(x −xj)
⎞
⎠
&
'(
)
ℓ(x)
wk
x −xk
,
thus
Πn(x) = ℓ(x)
n

k=0
wk
x −xk
yk.
(3.17)
Noting that
ℓ(x)
n

k=0
wk
x −xk
= 1,
(this follows by (3.17) by taking yk = 1 for k = 0, . . . , n and noting that
in this case Πn(x) ≡1) it holds
Πn(x) =
ℓ(x)
n

k=0
wk
x −xk
yk
ℓ(x)
n

k=0
wk
x −xk
,

3.3 Interpolation
91
0
50
100
150
10
-20
10
-10
10
0
10
10
10
20
10
30
 
 
Barycentric form
Lagrange form        
Monomial form
n
En
0
50
100
150
10
-15
10
-10
10
-5
10
0
10
5
10
10
 
 
Barycentric form
Lagrange form        
Monomial form
n
En
Figure 3.9. At left, interpolation errors for equispaced nodes in [−1, 1] and
f(x) = sin(x). At right, interpolation errors for Chebyshev nodes in [−5, 5]
and f(x) = 1/(1 + x2)
that is (3.15).
Consider a set of equispaced nodes and a function f(x) such that
maxx∈[a,b] |f(x)−Πnf(x)| →0 for n →∞in exact arithmetics. Consider
the interpolatory polynomial in Lagrange form (see (3.4)), in monomial
form
Πnf(x) =
n

k=0
ck+1xn−k,
(3.18)
and in barycentric form (see (3.15)). For all (three) cases, the interpola-
tion error decreases until a certain value of n, then it starts increasing.
It diverges for the Lagrange and monomial form, and keeps bounded
(of the order of 1) for the barycentric form. See Figure 3.9, left, which
corresponds to the interpolation of the function f(x) = sin(x) on [−1, 1].
As a matter of fact, if the nodes are uniformely distributed, the
weights of the barycentric formula are wk = (−1)k( n
k ).
This can potentially generate large oscillations of the interpolatory
polynomial near the borders of the interval containing the interpolation
nodes. However, this Runge’s phenomenon (see Example 3.2) is not pe-
culiar of the barycentric formula, as a matter of fact it is intrinsic to
the Lagrange interpolation on equispaced nodes, as it is due to the bad
conditioning when n gets large (because of the asymptotic behavior of
the Lebesgue constant, see Sect. 3.3.2): small changes in the data can
generate big changes in the interpolant.
On the contrary, when we use Chebyshev interpolatory nodes, the
interpolation problem is well conditioned (see Sect. 3.3.3) and the weights
of the interpolatory formula are bounded. While both the Lagrange and
monomial forms are unstable with respect to the roundoﬀerrors, the
barycentric form is stable also for large values of n (see Fig. 3.9, on the
right).

92
3 Approximation of functions and data
In conclusion, the main strength of the barycentric formula (3.15)
is that it is stable with respect to the propagation of rounding errors,
provided that two matters described below are attended to (see [BT04,
Hig04]).
The ﬁrst matter is concerned with underﬂow and overﬂow. When n →
∞, the scale of the weights wj (3.16) will grow or decay exponentially
at the rate (4/(b −a))n, where [a, b] is the interval on which we seek
the interpolating polynomial. Underﬂow and overﬂow can be avoided by
multiplying each factor (xk −xj) in (3.16) by 4/(b −a).
The second matter is about the evaluation of Πn(x) when x = xk. A
na¨ıf implementation of (3.15) would yield Nan output, but it is suﬃcient
to replace the computation (3.15) of Πn(xk) with the given value yk.
When x is very close to xk the barycentric formula turns out to be
stable, as pointed out in [Hen79].
Program 3.1 implements the barycentric formula (3.15) by taking
into account the tricks suggested above. The input variables x, y, and
x1 takes on the same meaning as in the call to the MATLAB function
polyfit. The output variable y1 contains the values of Πn(x) at nodes
x1.
Program 3.1. barycentric: barycentric interpolation
function [y1 ]= barycentric (x,y,x1)
% BARYCENTRIC
Computes
the
barycentric
interpolating
% Y1=BARYCENTRIC (X,Y,X1) computes
the value at the
% abscissae
X1 of the polynomial
interpolating
data
% (X,Y), by using barycentric
formula.
np=length(x);
a=min(x); b=max(x);
w=ones (np ,1);
C=4/(b-a);
for j=1:np
for k=1:j-1
w(j)=w(j)*(x(j)-x(k))*C;
end
for k=j+1: np
w(j)=w(j)*(x(j)-x(k))*C;
end
end
w=1./ w;
num=zeros(size(x1 )); den=num; exa=num;
for j=1:np
xdiff=x1 -x(j); wx=w(j)./ xdiff;
den=den+wx;
num=num+wx*y(j);
exa(xdiff ==0)=j;
end
y1=num./ den;
for i=1: length(x1)
if exa(i)>0, y1(i)=y(exa(i)); end
end

3.3 Interpolation
93
Example 3.6 We interpolate f(x) = 1/(1 + x2) on the interval [−5, 5] at the
(n + 1) Chebyshev-Gauss-Lobatto nodes (3.11) by calling Program 3.1, that
implements barycentric formula (3.15), then a program that computes Πnf by
using the Lagrange form (3.4), and ﬁnally the MATLAB command polyfit
that computes the coeﬃcients of Πnf with respect to the basis of monomials
(see (3.18)).
According to the theory, the interpolation error En = max[−5,5] |f(x) −
Πnf(x)| should converge exponentially to zero when n →∞, because of the
nice properties of the Gaussian nodes.
In practice, as we can see from Figure 3.9, when Πnf(x) is computed by ei-
ther the Lagrange formula (3.4) or the expansion (3.18), the error En decreases
exponentially until n ≃40, while beyond it starts growing due to the propaga-
tion of rounding errors (for n ≥20 a warning message is printed by MATLAB
function polyfit, pointing out that “Polynomial is badly conditioned”).
On the contrary, the error associated with the barycentric formula (3.15)
keeps decreasing until machine epsilon. We have considered n = 4 : 8 : 128. ■
3.3.5 Trigonometric interpolation and FFT
We want to approximate a periodic function f : [0, 2π] →C, i.e. one
satisfying f(0) = f(2π), by a trigonometric polynomial ˜f which inter-
polates f at the equispaced n + 1 nodes xj = 2πj/(n + 1), j = 0, . . . , n,
i.e.
˜f(xj) = f(xj), for j = 0, . . . , n.
(3.19)
The trigonometric interpolant ˜f is obtained by a linear combination of
sines and cosines.
Let us consider at ﬁrst the case n even. Precisely we seek a function
˜f(x) = a0
2 +
M

k=1
[ak cos(kx) + bk sin(kx)] ,
(3.20)
with M = n/2, whose complex coeﬃcients ak, k = 0, . . . , M and bk
(for k = 1, . . . , M) are unknown. By recalling the Euler formula eikx =
cos(kx) + i sin(kx), the trigonometric polynomial (3.20) can be written
as
˜f(x) =
M

k=−M
ckeikx,
(3.21)
where i is the imaginary unit and the coeﬃcients ck, for k = 0, ..., M,
are related to the coeﬃcient ak and bk through the formulas
ak = ck + c−k,
bk = i(ck −c−k).
(3.22)
As a matter of fact, thanks to the parity properties of sine and cosine
functions, it holds

94
3 Approximation of functions and data
M

k=−M
ckeikx =
M

k=−M
ck (cos(kx) + i sin(kx))
= c0 +
M

k=1
[ck(cos(kx) + i sin(kx)) + c−k(cos(kx) −i sin(kx))]
= c0 +
M

k=1
[(ck + c−k) cos(kx) + i(ck −c−k) sin(kx))] .
When n is odd, the trigonometric polynomial ˜f can be deﬁned as
˜f(x) =
M+1

k=−(M+1)
ckeikx,
(3.23)
where M = (n −1)/2. Note that these are n + 2 unknown coeﬃcients in
(3.23), while the interpolation conditions (3.19) are only n+1. A possible
remedy consists of imposing c−(M+1) = c(M+1), as done by MATLAB
in the function interpft.
Even when n is odd we can write ˜f as a sum of sine and cosine functions,
obtaining a formula similar to (3.20) in which the index k of the sum
ranges now from 1 to M + 1. Coeﬃcients ck in (3.23) are still related
to coeﬃcients ak and bk through the formulas (3.22), however now k =
0, . . . , M + 1. Due to the choice c−(M+1) = c(M+1), we have a(M+1) =
2c(M+1) and b(M+1) = 0.
For the sake of generalization, we introduce a parameter μ that we set
to 0, if n is even, and to 1, if n is odd. Then the interpolation polynomial
can be written in a more general way as
˜f(x) =
M+μ

k=−(M+μ)
ckeikx =
M

k=−M
ckeikx + 2μc(M+1) cos((M + 1)x),
where we recall that M = (n −μ)/2 and cM+1 is meaningless for even n
(μ = 0).
Because of its analogy with Fourier series, ˜f is also named discrete
Fourier series of f. By imposing interpolation conditions at nodes xj =
jh, with h = 2π/(n + 1), we ﬁnd, for j = 0, . . . , n,
M

k=−M
ckeikjh + 2μc(M+1) cos((M + 1)jh) = f(xj).
(3.24)
In order to compute the coeﬃcients {ck}, with k = −M, . . . , M + μ,
we multiply equation (3.24) by e−imxj = e−imjh where m is an integer
ranging between −M and M + μ, and then sum with respect to j

3.3 Interpolation
95
n

j=0
M

k=−M
ckeikjhe−imjh
+2μc(M+1)
n

j=0
cos((M + 1)jh)e−imjh =
n

j=0
f(xj)e−imjh.
(3.25)
Let us consider the identity
n

j=0
eijh(k−m) = (n + 1)δkm,
k, m = −M, . . . , M
which is obviously true if k = m. When k ̸= m, it follows from the
property
n

j=0
eijh(k−m) = 1 −(ei(k−m)h)n+1
1 −ei(k−m)h
,
and the remark that the numerator on the right hand side is null, since
1 −ei(k−m)h(n+1) = 1 −ei(k−m)2π
= 1 −cos((k −m)2π) −i sin((k −m)2π).
By applying Euler formula and recalling the deﬁnitions of M ad h,
it holds
n

j=0
cos((M + 1)jh)e−imjh = (n + 1)δ(M+1)m,
thus, from (3.25) we draw the following explicit expression for the coef-
ﬁcients of ˜f
ck =
1
n + 1
n

j=0
f(xj)e−ikjh,
k = −M, . . . , M
c(M+1) = c−(M+1) =
1
2(n + 1)
n

j=0
(−1)jf(xj), only for odd n
(3.26)
We deduce from (3.26) that, if f is a real valued function, then
c(M+1) = c−(M+1) are real and c−k = ck for k = −M, . . . , M (this
follows from eikjh = e−ikjh). In view of (3.22) we have ak, bk ∈R (for
k = 0, . . . , M + μ), thus ˜f is a real valued function, too.
The computation of all the coeﬃcients {ck} can be accomplished with
an order n log2 n operations by using the Fast Fourier Transform (FFT),
which is implemented in the MATLAB program fft (see Example 3.7).
fft

96
3 Approximation of functions and data
Similar conclusions hold for the inverse transform through which we
obtain the values {f(xj)} from the coeﬃcients {ck}. The inverse fast
Fourier transform is implemented in the MATLAB program ifft.
ifft
Example 3.7 Consider the function f(x) = x(x −2π)e−x for x ∈[0, 2π]. To
use the MATLAB program fft we ﬁrst compute the values of f at the nodes
xj = jπ/5 for j = 0, . . . , 9 by the following instructions (recall that .* is the
component-by-component vector product):
n=9; x=2*pi /(n+1)*[0: n]; y=x.*(x -2*pi ).* exp(-x);
Now we compute by the FFT the vector of Fourier coeﬃcients, with the
following instructions:
Y=fft(y);
C= fftshift (Y)/(n+1)
C =
Columns 1 through 2
0.0870
0.0926 - 0.0214i
Columns 3 through 4
0.1098 - 0.0601i
0.1268 - 0.1621i
Columns 5 through 6
-0.0467 - 0.4200i
-0.6520
Columns 7 through 8
-0.0467 + 0.4200i
0.1268 + 0.1621i
Columns 9 through 10
0.1098 + 0.0601i
0.0926 + 0.0214i
Elements of Y are related to coeﬃcients ck deﬁned in (3.26) by the following re-
lation: Y= (n+1)[c0, . . . , cM, c−(M+μ), . . . , c−1]. When n is odd, the coeﬃcient
c(M+1) (which coincides with c−(M+1)) is neglected. The command fftshift
fftshift
sorts the elements of the input array, so that C= [c−(M+μ), . . . , c−1, c0, . . . , cM].
Note that the program ifft achieves the maximum eﬃciency when n is a
power of 2, even though it works for any value of n.
■
The command interpft provides the trigonometric interpolant of
interpft
a set of real data. It requires in input an integer m and a vector of
values which represent the values taken by a function (periodic with
period p) at the set of points xj = jp/(n + 1), j = 0, . . . , n. interpft
returns the m real values of the trigonometric interpolant, obtained by
the Fourier transform, at the nodes ti = ip/m, i = 0, . . . , m −1. For
instance, let us reconsider the function of Example 3.7 in [0, 2π] and
take its values at 10 equispaced nodes xj = jπ/5, j = 0, . . . , 9. The
values of the trigonometric interpolant at, say, the 100 equispaced nodes
ti = 2iπ/100, i = 0, . . . , 99 can be obtained as follows (see Figure 3.10)
n=9; x=2*pi /(n+1)*[0: n]; y=x.*(x -2*pi ).* exp(-x);
z= interpft (y ,100);
In some cases the accuracy of trigonometric interpolation can dra-
matically downgrade, as shown in the following example.
Example 3.8 Let us approximate the function f(x) = f1(x) + f2(x), with
f1(x) = sin(x) and f2(x) = sin(5x), using nine equispaced nodes in the interval

3.3 Interpolation
97
0
1
2
3
4
5
6
7
−2.5
−2
−1.5
−1
−0.5
0
0.5
Figure 3.10. The function f(x) = x(x −2π)e−x (dashed line) and the corre-
sponding trigonometric interpolant (solid line) relative to 10 equispaced nodes
0
1
2
3
4
5
6
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
1
2
3
4
5
6
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Figure 3.11. The eﬀects of aliasing. At left, comparison between the function
f(x) = sin(x) + sin(5x) (solid line) and its trigonometric interpolant (3.20)
with M = 3 (dashed line). At right, the functions sin(5x) (dashed line) and
−sin(3x) (solid line) take the same values at the interpolation nodes. This
circumstance explains the severe loss of accuracy shown at left
[0, 2π]. The result is shown in Figure 3.11, left. Note that in some intervals the
trigonometric approximant shows even a phase inversion with respect to the
function f.
■
This lack of accuracy can be explained as follows. At the nodes con-
sidered, the function f2 is indistinguishable from f3(x) = −sin(3x) which
has a lower frequency (see Figure 3.11, right). The function that is ac-
tually approximated is therefore F(x) = f1(x) + f3(x) and not f(x) (in
fact, the dashed line of Figure 3.11, left, does coincide with F).
This phenomenon is known as aliasing and may occur when the func-
tion to be approximated is the sum of several components having diﬀer-
ent frequencies. As soon as the number of nodes is not enough to resolve
the highest frequencies, the latter may interfere with the low frequen-
cies, giving rise to inaccurate interpolants. To get a better approximation

98
3 Approximation of functions and data
for functions with higher frequencies, one has to increase the number of
interpolation nodes.
A real life example of aliasing is provided by the apparent inversion
of the sense of rotation of spoked wheels. Once a certain critical velocity
is reached the human brain is no longer able to accurately sample the
moving image and, consequently, produces distorted images.
We refer to Chapter 7 for the solution of nonlinear least squares
problems, that is problems where the ˜f is a nonlinear function of the
unknown coeﬃcients aj.
Let us summarize
1. Approximating a set of data or a function f in [a, b] consists of ﬁnding
a suitable function ˜f that represents them with enough accuracy;
2. the interpolation process consists of determining a function ˜f such
that ˜f(xi) = yi, where the {xi} are given nodes and {yi} are either
the values {f(xi)} or a set of prescribed values;
3. if the n+ 1 nodes {xi} are distinct, there exists a unique polynomial
of degree less than or equal to n interpolating a set of prescribed
values {yi} at the nodes {xi};
4. for an equispaced distribution of nodes in [a, b] the interpolation
error at any point of [a, b] does not necessarily tend to 0 as n tends
to inﬁnity. However, there exist special distributions of nodes, for
instance the Chebyshev nodes, for which this convergence property
holds true for all continuously diﬀerentiable functions;
5. trigonometric interpolation is well suited to approximate periodic
functions, and is based on choosing ˜f as a linear combination of sine
and cosine functions. The FFT is a very eﬃcient algorithm which
allows the computation of the Fourier coeﬃcients of a trigonometric
interpolant from its node values and admits an equally fast inverse,
the IFFT.
3.4 Piecewise linear interpolation
The interpolant at Chebyshev nodes provides an accurate approximation
of any smooth function f whose expression is known. In the case when
f is nonsmooth or when f is only known through its values at a set of
given points (which do not coincide with the Chebyshev nodes), one can
resort to a diﬀerent interpolation method which is called linear composite
interpolation.
More precisely, given a distribution (not necessarily uniform) of nodes
x0 < x1 < . . . < xn, we denote by Ii the interval [xi, xi+1]. We approx-
imate f by a continuous function which, on each interval, is given by

3.4 Piecewise linear interpolation
99
−2
0
2
4
6
8
0
10
20
30
40
50
60
70
80
Figure 3.12. The function f(x) = x2 + 10/(sin(x) + 1.2) (solid line) and its
piecewise linear interpolation polynomial ΠH
1 f (dashed line)
the segment joining the two points (xi, f(xi)) and (xi+1, f(xi+1)) (see
Figure 3.12). This function, denoted by ΠH
1 f, is called piecewise linear
interpolation polynomial of f and its expression is:
ΠH
1 f(x) = f(xi) + f(xi+1) −f(xi)
xi+1 −xi
(x −xi)
for x ∈Ii.
The upper-index H denotes the maximum length of the intervals Ii.
The following result can be inferred from (3.7) setting n = 1 and
h = H:
Proposition 3.3 If f ∈C2(I), where I = [x0, xn], then
max
x∈I |f(x) −ΠH
1 f(x)| ≤H2
8 max
x∈I |f ′′(x)|.
Consequently, for all x in the interpolation interval, ΠH
1 f(x) tends to
f(x) when H →0, provided that f is suﬃciently smooth.
Through the instruction s1=interp1(x,y,z) one can compute the
interp1
values at arbitrary points, which are stored in the vector z, of the piece-
wise linear polynomial that interpolates the values y(i) at the nodes
x(i), for i = 1,...,n+1. Note that z can have arbitrary dimension. If
the nodes are in increasing order (i.e. x(i+1) > x(i), for i=1,...,n)
then we can use the quicker version interp1q (q stands for quickly).
interp1q
Notice that interp1q is quicker than interp1 on non-uniformly spaced
data because it does not make any input checking, nevertheless, we note
that all input variables of interp1q must be column vectors.
It is worth mentioning that the command fplot, which is used to
display the graph of a function f on a given interval [a, b], does in-

100
3 Approximation of functions and data
deed replace the function by its piecewise linear interpolant. The set of
interpolating nodes is generated automatically from the function, follow-
ing the criterion of clustering these nodes around points where f shows
strong variations. A procedure of this type is called adaptive.
3.5 Approximation by spline functions
As done for piecewise linear interpolation, piecewise polynomial interpo-
lation of degree n ≥2 can be deﬁned as well. For instance, the piece-
wise quadratic interpolation ΠH
2 f is a continuous function that on each
interval Ii replaces f by its quadratic interpolation polynomial at the
endpoints of Ii and at its midpoint. If f ∈C3(I), the error f −ΠH
2 f in
the maximum norm decays as H3 if H tends to zero.
The main drawback of this piecewise interpolation is that ΠH
k f with
k ≥1, is nothing more than a global continuous function. As a matter of
fact, in several applications, e.g. in computer graphics, it is desirable to
get approximation by smooth functions which have at least a continuous
derivative.
With this aim, we can construct a function s3 with the following
properties:
1. on each interval Ii = [xi, xi+1], for i = 0, . . . , n−1, s3 is a polynomial
of degree 3 which interpolates the pairs of values (xj, f(xj)) for j =
i, i + 1 (s3 is therefore a globally continuous function);
2. s3 has continuous ﬁrst and second derivatives in the nodes xi, i =
1, . . . , n −1.
For its complete determination, we need four conditions on each in-
terval, therefore a total of 4n equations, which we can provide as follows:
- n + 1 conditions arise from the interpolation requirement at the nodes
xi, i = 0, . . . , n;
- n −1 further equations follow from the requirement of continuity of
the polynomial at the internal nodes x1, . . . , xn−1;
- 2(n −1) new equations are obtained by requiring that both ﬁrst and
second derivatives be continuous at the internal nodes.
We still lack two further equations, which we can e.g. choose as
s′′
3(x0) = 0, s′′
3(xn) = 0.
(3.27)
The function s3 which we obtain in this way, is called a natural interpo-
lating cubic spline.
By suitably choosing the unknowns (see [QSS07, Section 8.7]) to rep-
resent s3 we arrive at a (n + 1) × (n + 1) system with a tridiagonal
matrix whose solution can be accomplished by a number of operations

3.5 Approximation by spline functions
101
proportional to n (see Section 5.6) whose solutions are the values s′′(xi)
for i = 0, . . . , n.
Using Program 3.2, this solution can be obtained with a number of
operations equal to the dimension of the system itself (see Section 5.6).
The input parameters are the vectors x and y of the nodes and the data
to interpolate, plus the vector zi of the abscissae where we want the
spline s3 to be evaluated.
Other conditions can be chosen in place of (3.27) in order to close
the system of equations; for instance we could prescribe the value of the
ﬁrst derivative of s3 at both endpoints x0 and xn.
Unless otherwise speciﬁed, Program 3.2 computes the natural inter-
polation cubic spline. The optional parameters type and der (a vec-
tor with two components) serve the purpose of selecting other types
of splines. With type=0 Program 3.2 computes the interpolating cubic
spline whose ﬁrst derivative is given by der(1) at x0 and der(2) at
xn. With type=1 we obtain the interpolating cubic spline whose values
of the second derivative at the endpoints is given by der(1) at x0 and
der(2) at xn.
Program 3.2. cubicspline: interpolating cubic spline
function s=cubicspline (x,y,zi ,type ,der)
% CUBICSPLINE
Computes a cubic spline
% S=CUBICSPLINE (X,Y,ZI) computes
the value at the
% abscissae
ZI of the natural
interpolating
cubic
% spline
that
interpolates
the values Y at the nodes X.
% S=CUBICSPLINE (X,Y,ZI ,TYPE ,DER) if TYPE =0
computes
the
% values at the
abscissae
ZI of the cubic spline
% interpolating
the values Y with
first derivative
at
% the endpoints
equal to the values DER (1) and DER (2).
% If TYPE =1 the values DER (1) and DER (2) are those of
% the second
derivative
at the
endpoints .
[n,m]= size (x);
if n == 1
x = x’;
y = y’;
n = m;
end
if nargin == 3
der0 = 0; dern = 0; type = 1;
else
der0 = der (1);
dern = der (2);
end
h = x(2: end)-x(1:end -1);
e = 2*[h(1); h(1:end -1)+ h(2: end ); h(end )];
A = spdiags ([[h; 0] e [0; h]],-1:1, n,n);
d = (y(2: end)-y(1: end -1))./h;
rhs = 3*(d(2: end)-d(1:end -1));
if type == 0
A(1,1) = 2*h(1);
A(1,2) = h(1);
A(n,n) = 2*h(end); A(end ,end -1) = h(end );
rhs = [3*(d(1)- der0 ); rhs; 3*(dern -d(end ))];
else
A(1,:) = 0; A(1,1) = 1;
A(n,:) = 0; A(n,n) = 1;

102
3 Approximation of functions and data
−60
−40
−20
0
20
40
60
−3.5
−3.4
−3.3
−3.2
−3.1
−3
−2.9
−2.8
−2.7
Figure 3.13. Comparison between the interpolating cubic spline (solid line)
and the Lagrange interpolant (dashed line) for the case considered in Example
3.9
rhs = [der0 ; rhs; dern ];
end
S = zeros(n ,4);
S(:,3) = A\rhs;
for m = 1:n-1
S(m,4) = (S(m+1,3)- S(m ,3))/3/ h(m);
S(m,2) = d(m) - h(m)/3*(S(m + 1 ,3)+2* S(m ,3));
S(m,1) = y(m);
end
S = S(1:n-1, 4: -1:1);
pp = mkpp (x,S);
s = ppval(pp ,zi);
return
The MATLAB command spline (see also the toolbox splines) en-
spline
forces the third derivative of s3 to be continuous at x1 and xn−1. To this
condition is given the curious name of not-a-knot condition. The input
parameters are the vectors x and y and the vector zi (same meaning as
before). The commands mkpp and ppval that are used in Program 3.2
mkpp
ppval
are useful to build up and evaluate a composite polynomial.
Example 3.9 Let us reconsider the data of Table 3.1 corresponding to the
column K = 0.67 and compute the associated interpolating cubic spline s3.
The diﬀerent values of the latitude provide the nodes xi, i = 0, . . . , 12. If we are
interested in computing the values s3(zi), where zi = −55 + i, i = 0, . . . , 120,
we can proceed as follows:
x = [ -55:10:65];
y = [ -3.25
-3.37
-3.35
-3.2
-3.12
-3.02
-3.02 ...
-3.07
-3.17
-3.32
-3.3
-3.22
-3.1];
zi = [ -55:1:65];
s = spline(x,y,zi);
The graph of s3, which is reported in Figure 3.13, looks more plausible than
that of the Lagrange interpolant at the same nodes.
■

3.5 Approximation by spline functions
103
Example 3.10 (Robotics) To ﬁnd the trajectory in the xy plane of the
robot satisfying the given constraints (see Problem 3.4), we split the time
interval [0, 5] in the two subintervals [0, 2] and [2, 5]. Then in each subinterval
we look for two splines, x = x(t) and y = y(t), that interpolate the given
values and have null derivative at the endpoints. Using Program 3.2 we obtain
the desired result by the following instructions:
x1 = [0 1 4]; y1 = [0 2 4];
t1 = [0 1 2]; ti1 = [0:0.01:2];
x2 = [0 3 4]; y2 = [0 1 4];
t2 = [0 2 3]; ti2 = [0:0.01:3];
d=[0 ,0];
six1 = cubicspline (t1 ,x1 ,ti1 ,0,d);
siy1 = cubicspline (t1 ,y1 ,ti1 ,0,d);
six2 = cubicspline (t2 ,x2 ,ti2 ,0,d);
siy2 = cubicspline (t2 ,y2 ,ti2 ,0,d);
The trajectory obtained is drawn in Figure 3.14.
■
The error that we obtain in approximating a function f (continuously
diﬀerentiable up to its fourth derivative) by the natural interpolating
cubic spline s3 satisﬁes the following inequalities ([dB01]):
max
x∈I |f (r)(x) −s(r)
3 (x)| ≤CrH4−rmax
x∈I |f (4)(x)|,
r = 0, 1, 2,
and
max
x∈I\{x0,...,xn}|f (3)(x) −s(3)
3 (x)| ≤C3Hmax
x∈I |f (4)(x)|,
where I = [x0, xn] and H = maxi=0,...,n−1(xi+1 −xi), while Cr (for
r = 0, . . . , 3) is a suitable constant depending on r, but independent of
H. It is then clear that not only f, but also its ﬁrst, second and third
derivatives are well approximated by s3 when H tends to 0.
0
0.5
1
1.5
2
2.5
3
3.5
4
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
Figure 3.14. The trajectory in the xy plane of the robot described in Problem
3.4. Circles represent the position of the control points through which the robot
should pass during its motion

104
3 Approximation of functions and data
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 3.15. Approximation of the ﬁrst quarter of the circumference of the
unitary circle using only 4 nodes. The dashed line is the cubic spline, while
the solid line is the piecewise cubic Hermite interpolant
Remark 3.1 In general cubic splines do not preserve monotonicity between
neighbouring nodes. For instance, by approximating the unitary circumference
in the ﬁrst quarter using the points (xk = sin(kπ/6), yk = cos(kπ/6)), for
k = 0, . . . , 3, we would obtain an oscillatory spline (see Figure 3.15). In these
cases, other approximation techniques can be better suited. For instance, the
MATLAB command pchip provides the Hermite piecewise cubic interpolant
pchip
([Atk89]) which is locally monotone and interpolates the function as well as
its ﬁrst derivative at the nodes {xi, i = 1, . . . , n −1} (see Figure 3.15). The
Hermite interpolant can be obtained by using the following instructions:
t = linspace (0,pi /2 ,4);
x = sin(t); y = cos(t);
xx = linspace (0 ,1 ,40);
plot (x,y,’o’,xx ,[ pchip(x,y,xx); spline(x,y,xx )])
■
See the Exercises 3.5-3.8.
3.6 The least-squares method
As already noticed, a Lagrange interpolation does not guarantee a bet-
ter approximation of a given function when the polynomial degree gets
large. This problem can be overcome by composite interpolation (such
as piecewise linear polynomials or splines). However, neither are suitable
to extrapolate information from the available data, that is, to generate
new values at points lying outside the interval where interpolation nodes
are given.
Example 3.11 (Finance) On the basis of the data reported in Figure 3.1,
we would like to predict whether the stock price will increase or diminish in

3.6 The least-squares method
105
the coming days. The Lagrange polynomial interpolation is impractical, as it
would require a (tremendously oscillatory) polynomial of degree 719 which
will provide a completely erroneous prediction. On the other hand, piecewise
linear interpolation, whose graph is reported in Figure 3.1, provides extrapo-
lated results by exploiting only the values of the last two days, thus completely
neglecting the previous history. To get a better result we should avoid the in-
terpolation requirement, by invoking least-squares approximation as indicated
below.
■
Assume that the data {(xi, yi), i = 0, . . . , n} are available, where now
yi could represent the values f(xi) attained by a given function f at the
nodes xi. For a given integer m ≥1 (usually, m ≪n) we look for a
polynomial ˜f ∈Pm which satisﬁes the inequality
n

i=0
[yi −˜f(xi)]2 ≤
n

i=0
[yi −pm(xi)]2
(3.28)
for every polynomial pm ∈Pm. Should it exist, ˜f will be called the least-
squares approximation in Pm of the set of data {(xi, yi), i = 0, . . . , n}.
Unless m ≥n, in general it will not be possible to guarantee that ˜f(xi) =
yi for all i = 0, . . . , n.
Setting
˜f(x) = a0 + a1x + . . . + amxm,
(3.29)
where the coeﬃcients a0, . . . , am are unknown, the problem (3.28) can
be restated as follows: ﬁnd a0, a1, . . . , am such that
Φ(a0, a1, . . . , am) =
min
{bi, i=0,...,m}Φ(b0, b1, . . . , bm)
where
Φ(b0, b1, . . . , bm) =
n

i=0
[yi −(b0 + b1xi + . . . + bmxm
i )]2 .
We solve this problem in the special case when m = 1. Since
Φ(b0, b1) =
n

i=0

y2
i + b2
0 + b2
1x2
i + 2b0b1xi −2b0yi −2b1xiyi

,
the graph of Φ is a convex paraboloid. The point (a0, a1) at which Φ
attains its minimum satisﬁes the conditions
∂Φ
∂b0
(a0, a1) = 0,
∂Φ
∂b1
(a0, a1) = 0,

106
3 Approximation of functions and data
where the symbol ∂Φ/∂bj denotes the partial derivative (that is, the rate
of variation) of Φ with respect to bj, after having frozen the remaining
variable (see the deﬁnition (9.3)).
By explicitly computing the two partial derivatives we obtain
n

i=0
[a0 + a1xi −yi] = 0,
n

i=0
[a0xi + a1x2
i −xiyi] = 0,
which is a system of two equations for the two unknowns a0 and a1:
a0(n + 1) + a1
n

i=0
xi =
n

i=0
yi,
a0
n

i=0
xi + a1
n

i=0
x2
i =
n

i=0
yixi.
(3.30)
Setting D = (n + 1) *n
i=0 x2
i −(*n
i=0 xi)2, the solution reads:
a0 = 1
D
⎡
⎣
n

i=0
yi
n

j=0
x2
j −
n

j=0
xj
n

i=0
xiyi
⎤
⎦,
a1 = 1
D
⎡
⎣(n + 1)
n

i=0
xiyi −
n

j=0
xj
n

i=0
yi
⎤
⎦
(3.31)
The corresponding polynomial ˜f(x) = a0 +a1x is known as the least-
squares straight line, or regression line.
The previous approach can be generalized in several ways. The ﬁrst
generalization is to the case of an arbitrary m. The associated (m+ 1)×
(m + 1) linear system, which is symmetric, will have the form:
a0(n + 1) +a1
n

i=0
xi
+ . . . + am
n

i=0
xm
i
=
n

i=0
yi,
a0
n

i=0
xi
+a1
n

i=0
x2
i
+ . . . + am
n

i=0
xm+1
i
=
n

i=0
xiyi,
...
...
...
...
a0
n

i=0
xm
i
+a1
n

i=0
xm+1
i
+ . . . + am
n

i=0
x2m
i
=
n

i=0
xm
i yi.
When m = n, the least-squares polynomial ˜f must coincide with the
Lagrange interpolating polynomial Πnf (see Exercise 3.9).
The MATLAB command c=polyfit(x,y,m) computes by default
the coeﬃcients of the polynomial of degree m which approximates n+1
pairs of data (x(i),y(i)) in the least-squares sense. As already no-
ticed in Section 3.3.1, when m is equal to n it returns the interpolating
polynomial.

3.6 The least-squares method
107
nov00
may01
nov01
may02
0
5
10
15
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
−0.1
0
0.1
0.2
0.3
0.4
0.5
σ
ε
Figure 3.16. At left: least-squares approximation of the data of Problem 3.2
with polynomials of degree 1 (dashed-dotted line), degree 2 (dashed line) and
degree 4 (thick solid line). The exact data are represented by the thin solid
line. At right: linear least-squares approximation of the data of Problem 3.3
Example 3.12 (Finance) In Figure 3.16, left, we draw the graphs of the
least-squares polynomials of degree 1, 2 and 4 that approximate in the least-
squares sense the data of Figure 3.1. The polynomial of degree 4 reproduces
quite reasonably the behavior of the stock price in the considered time interval
and suggests that in the near future the quotation will increase.
■
Example 3.13 (Biomechanics) Using the least-squares method we can an-
swer the question in Problem 3.3 and discover that the line which better ap-
proximates the given data has equation ϵ(σ) = 0.3471σ + 0.0654 (see Figure
3.16, right); when σ = 0.9 it provides the estimate ϵ = 0.2915 for the defor-
mation.
■
A further generalization of the least-squares approximation consists
of using in (3.28) ˜f and pm that are no-longer polynomials but func-
tions of a space Vm obtained by linearly combining m + 1 independent
functions {ψj, j = 0, . . . , m}. Special instances are provided, e.g., by the
trigonometric functions ψj(x) = cos(γjx) (for a given parameter γ ̸= 0),
by the exponential functions ψj(x) = eδjx (for some δ > 0), or by a
suitable set of spline functions.
The choice of the functions {ψj} is actually dictated by the conjec-
tured behavior of the law underlying the given data distribution. For
instance, in Figure 3.17 we draw the graph of the least-squares approxi-
mation of the data of the Example 3.1 computed using the trigonometric
functions ψj(x) = cos(γjx), j = 0, . . . , 4, with γ = π/60.
The reader can verify that the unknown coeﬃcients of
˜f(x) =
m

j=0
ajψj(x),
can be obtained by solving the following system (of normal equations)

108
3 Approximation of functions and data
−60
−40
−20
0
20
40
60
80
−3.4
−3.35
−3.3
−3.25
−3.2
−3.15
−3.1
−3.05
−3
Figure 3.17. The least-squares approximation of the data of the Problem 3.1
using a cosine basis. The exact data are represented by the small circles
BT Ba = BT y
(3.32)
where B is the rectangular matrix (n+1)×(m+1) of entries bij = ψj(xi),
a is the vector of the unknown coeﬃcients, while y is the vector of
the data. The linear system (3.32) can be eﬃciently solved by the QR
factorization or, alternativeley, by a Singular-Value Decomposition of
matrix B (see Section 5.7).
Let us summarize
1. The composite piecewise linear interpolant of a function f is a piece-
wise continuous linear function ˜f, which interpolates f at a given
set of nodes {xi}. With this approximation we avoid Runge’s type
phenomena when the number of nodes increases. It is also called
piecewise linear ﬁnite element interpolant (see Chapter 9);
2. interpolation by cubic splines allows the approximation of f by a
piecewise cubic function ˜f which is continuous together with its ﬁrst
and second derivatives;
3. in least-squares approximation we look for an approximant ˜f which
is a polynomial of degree m (typically, m ≪n) that minimizes the
mean-square error *n
i=0[yi −˜f(xi)]2. The same minimization cri-
terium can be applied for a class of functions that are not polyno-
mials.
See the Exercises 3.9-3.14.
3.7 What we haven’t told you
For a more general introduction to the theory of interpolation and ap-
proximation the reader is referred to, e.g., [Dav63], [Mei67] and [Gau97].

3.7 What we haven’t told you
109
Polynomial interpolation can also be used to approximate data and
functions in several dimensions. In particular, composite interpolation,
based on piecewise linear or spline functions, is well suited when the
region Ω at hand is partitioned into polygons in 2D (triangles or quadri-
laterals) and polyhedra in 3D (tetrahedra or prisms).
A special situation occurs when Ω is a rectangle or a parallelepiped
in which case the MATLAB commands interp2, and interp3, respec-
interp2
interp3
tively, can be used. In both cases it is assumed that we want to represent
on a regular, ﬁne lattice (or grid) a function whose values are available
on a regular, coarser lattice.
Consider for instance the values of f(x, y) = sin(2πx) cos(2πy) on
a (coarse) 6 × 6 lattice of equispaced nodes on the square [0, 1]2; these
values can be obtained using the commands:
[x,y]= meshgrid (0:0.2:1 ,0:0.2:1);
z=sin (2*pi*x).* cos (2*pi*y);
By the command interp2 a cubic spline is ﬁrst computed on this coarse
grid, then evaluated at the nodal points of a ﬁner grid of 21 × 21 equis-
paced nodes:
xi = [0:0.05:1];
yi =[0:0.05:1];
[xf ,yf]= meshgrid (xi ,yi);
pi3=interp2(x,y,z,xf ,yf);
The command meshgrid transforms the set of the couples (xi(k),
meshgrid
yi(j)) into two matrices xf and yf that can be used to evaluate func-
tions of two variables and to plot three dimensional surfaces. The rows
of xf are copies of the vector xi, the columns of yf are copies of yi. Al-
ternatively to the above procedure we can use the command griddata,
griddata
available also for three-dimensional data (griddata3) and for the ap-
proximation of n-dimensional surfaces (griddatan).
The commands described below are for MATLAB only.
When Ω is a two-dimensional domain of (almost) arbitrary shape, it can
be partitioned into triangles using the graphical interface pdetool.
pdetool
For a general presentation of spline functions see, e.g., [Die93] and
[PBP02]. The MATLAB toolbox splines allows one to explore several
applications of spline functions. In particular, the spdemos command
spdemos
gives the user the possibility to investigate the properties of the most
important type of spline functions. Rational splines, i.e. functions which
are the ratio of two splines functions, are accessible through the com-
mands rpmak and rsmak. Special instances are the so-called NURBS
rpmak
rsmak
splines, which are commonly used in CAGD (Computer Assisted Geo-
metric Design).
In the same context of Fourier approximation, we mention the ap-
proximation based on wavelets. This type of approximation is largely
used for image reconstruction and compression and in signal analysis
(for an introduction, see [DL92], [Urb02]). A rich family of wavelets (and
their applications) can be found in the MATLAB toolbox wavelet.
wavelet

110
3 Approximation of functions and data
Octave 3.3 The Octave-Forge Package msh provides an interface for im-
porting into the Octave workspace triangular or tetrahedral meshes gen-
erated with the graphical interface of GMSH (http://geuz.org/gmsh/).
There is a splines package in Octave-Forge but it has limited function-
ality and does not provide the spdemos command.
The Octave-Forge package nurbs provides a set of functions for creating
and managing NURBS surfaces and volumes.
■
3.8 Exercises
Exercise 3.1 Prove inequality (3.6).
Exercise 3.2 Provide an upper bound of the Lagrange interpolation error for
the following functions:
f1(x) = cosh(x), f2(x) = sinh(x), xk = −1 + 0.5k, k = 0, . . . , 4,
f3(x) = cos(x) + sin(x),
xk = −π/2 + πk/4, k = 0, . . . , 4.
Exercise 3.3 The following data are related to the life expectation of citizens
of two European regions:
Year
1975
1980
1985
1990
Western Europe
72.8
74.2
75.2
76.4
Eastern Europe
70.2
70.2
70.3
71.2
Use the interpolating polynomial of degree 3 to estimate the life expectation
in 1977, 1983 and 1988.
Exercise 3.4 The price (in euros) of a magazine has changed as follows:
Nov.87
Dec.88
Nov.90
Jan.93
Jan.95
Jan.96
Nov.96
Nov.00
4.5
5.0
6.0
6.5
7.0
7.5
8.0
8.0
Estimate the price in November 2002 by extrapolating these data.
Exercise 3.5 Repeat the computations carried out in Exercise 3.3, using now
the cubic interpolating spline computed by the function spline. Then compare
the results obtained with those obtained by solving Exercise 3.3.
Exercise 3.6 In the table below we report the values of the sea water density
ρ (in Kg/m3) corresponding to diﬀerent values of the temperature T (in degrees
Celsius):
T
4o
8o
12o
16o
20o
ρ
1000.7794
1000.6427
1000.2805
999.7165
998.9700
Compute the cubic spline s3 on the interval 4 ≤T ≤20, divided into 4 equal
subintervals. Then compare the results provided by the spline interpolant with
the following ones (which correspond to further values of T ):
T
6o
10o
14o
18o
ρ
1000.74088
1000.4882
1000.0224
999.3650

3.8 Exercises
111
Exercise 3.7 The Italian production of citrus fruit has changed as follows:
Year
1965
1970
1980
1985
1990
1991
production (×105 Kg)
17769
24001
25961
34336
29036
33417
Use interpolating cubic splines of diﬀerent kinds to estimate the production in
1962, 1977 and 1992. Compare these results with the real values: 12380, 27403
and 32059 (×105 Kg), respectively. Compare the results with those that would
be obtained using the Lagrange interpolating polynomial.
Exercise 3.8 Evaluate the function f(x) = sin(2πx) at 21 equispaced nodes
in the interval [−1, 1]. Compute the Lagrange interpolating polynomial and
the cubic interpolating spline. Compare the graphs of these two functions with
that of f on the given interval. Repeat the same calculation using the following
perturbed set of data: f(xi) = (−1)i+110−4 (i = 0, . . . , n), and observe that
the Lagrange interpolating polynomial is more sensitive to small perturbations
than the cubic spline.
Exercise 3.9 Verify that if m = n the least-squares polynomial of a function
f at the nodes x0, . . . , xn coincides with the interpolating polynomial Πnf at
the same nodes.
Exercise 3.10 Compute the least-squares polynomial of degree 4 that ap-
proximates the values of K reported in the diﬀerent columns of Table 3.1.
Exercise 3.11 Repeat the computations carried out in Exercise 3.7 using
now a least-squares approximation of degree 3.
Exercise 3.12 Express the coeﬃcients of system (3.30) in terms of the aver-
age M =
1
(n+1)
n
i=0 xi and the variance v =
1
(n+1)
n
i=0(xi −M)2 of the set
of data {xi, i = 0, . . . , n}.
Exercise 3.13 Verify that the regression line passes through the point whose
abscissa is the average of {xi} and ordinate is the average of yi.
Exercise 3.14 The following values
Flow rate
0
35
0.125
5
0
5
1
0.5
0.125
0
represent the measured values of the blood ﬂow-rate in a cross-section of the
carotid artery during a heart beat. The frequency of acquisition of the data is
constant and is equal to 10/T , where T = 1 s is the beat period. Represent
these data by a continuous function of period equal to T .

4
Numerical diﬀerentiation and integration
In this chapter we propose methods for the numerical approximation of
derivatives and integrals of functions. Concerning integration, quite often
for a generic function it is not possible to ﬁnd a primitive in an explicit
form. Even when a primitive is known, its use might not be easy. This
is, e.g., the case of the function f(x) = cos(4x) cos(3 sin(x)), for which
we have
π

0
f(x)dx = π
3
2
4 ∞

k=0
(−9/4)k
k!(k + 4)!;
the task of computing an integral is transformed into the equally trou-
blesome one of summing a series. In other circumstances the function
that we want to integrate or diﬀerentiate could only be known on a
set of nodes (for instance, when the latter represent the results of an
experimental measurement), exactly as happens in the case of function
approximation, which was discussed in Chapter 3.
In all these situations it is necessary to consider numerical methods
in order to obtain an approximate value of the quantity of interest, in-
dependently of how diﬃcult is the function to integrate or diﬀerentiate.
4.1 Some representative problems
Problem 4.1 (Hydraulics) The height q(t) reached at time t by a
ﬂuid in a straight cylinder of radius R = 1 m with a circular hole of
radius r = 0.1 m on the bottom, has been measured every 5 seconds
yielding the following values
t
0
5
10
15
20
q(t) 0.6350
0.5336
0.4410
0.3572
0.2822
A. Quarteroni et al., Scientiﬁc Computing with MATLAB and Octave,
Texts in Computational Science and Engineering 2,
DOI 10.1007/978-3-642-45367-0 4, © Springer-Verlag Berlin Heidelberg 2014
113

114
4 Numerical diﬀerentiation and integration
We want to compute an approximation of the emptying velocity q′(t) of
the cylinder, then compare it with the one predicted by Torricelli’s law:
q′(t) = −γ(r/R)2
2gq(t), where g is the modulus of gravity acceleration
and γ = 0.6 is a correction factor. For the solution of this problem, see
Example 4.1.
■
Problem 4.2 (Optics) In order to plan a room for infrared beams we
are interested in calculating the energy emitted by a black body (that
is, an object capable of irradiating in all the spectrum to the ambient
temperature) in the (infrared) spectrum comprised between 3μm and
14μm wavelength. The solution of this problem is obtained by computing
the integral
E(T ) = 2.39 · 10−11
14·10−4

3·10−4
dx
x5(e1.432/(T x) −1),
(4.1)
which is the Planck equation for the energy E(T ), where x is the wave-
length (in cm) and T the temperature (in Kelvin) of the black body. For
its computation see Exercise 4.17.
■
Problem 4.3 (Electromagnetism) Consider an electric wire sphere
of arbitrary radius r and conductivity σ. We want to compute the density
distribution of the current j as a function of r and t (the time), knowing
the initial distribution of the charge density ρ(r). The problem can be
solved using the relations between the current density, the electric ﬁeld
and the charge density and observing that, for the symmetry of the
problem, j(r, t) = j(r, t)r/|r|, where j = |j|. We obtain
j(r, t) = γ(r)e−σt/ε0, γ(r) =
σ
ε0r2
r

0
ρ(ξ)ξ2 dξ,
(4.2)
where ε0 = 8.859 · 10−12 farad/m is the dielectric constant of the void.
For the computation of this integral, see Exercise 4.16.
■
Problem 4.4 (Demography) We consider a population of a very large
number M of individuals. The distribution n(s) of their height can be
represented by a ”bell” function characterized by the mean value ¯h of
the height and the standard deviation σ
n(s) =
M
σ
√
2πe−(s−¯h)2/(2σ2).
Then

4.2 Approximation of function derivatives
115
1  
1.5
2  
2.5
1.8 1.9
0
100
200
300
400
500
600
700
800
s
n(s)
Figure 4.1. Height distribution of a population of M = 200 individuals
N[h,h+Δh] =
h+Δh

h
n(s) ds
(4.3)
represents the number of individuals whose height is between h and
h + Δh (for a positive Δh). An instance is provided in Figure 4.1, which
corresponds to the case M = 200, ¯h = 1.7 m, σ = 0.1 m, and the area of
the shadowed region gives the number of individuals whose height is in
the range 1.8÷1.9 m. For the solution of this problem see Example 4.2.
.
■
4.2 Approximation of function derivatives
Consider a function f : [a, b] →R continuously diﬀerentiable in [a, b].
We seek an approximation of the ﬁrst derivative of f at a generic point
¯x in (a, b).
In view of the deﬁnition (1.10), for h suﬃciently small and positive,
we can assume that the quantity
(δ+f)(¯x) = f(¯x + h) −f(¯x)
h
(4.4)
is an approximation of f ′(¯x) which is called the forward ﬁnite diﬀerence.
To estimate the error, it suﬃces to expand f in a Taylor series; if f ∈
C2((a, b)), we have
f(¯x + h) = f(¯x) + hf ′(¯x) + h2
2 f ′′(ξ),
(4.5)
where ξ is a suitable point in the interval (¯x, ¯x + h). Therefore

116
4 Numerical diﬀerentiation and integration
¯x
¯x −h
¯x + h
m1
m2
m3
f
Figure 4.2. Finite diﬀerence approximation of f ′(¯x): backward (solid line),
forward (dotted line) and centered (dashed line). The values m1 = (δ−f)(¯x),
m2 = (δ+f)(¯x) and m3 = (δf)(¯x) denote the slopes of the three straight lines
(δ+f)(¯x) = f ′(¯x) + h
2 f ′′(ξ),
(4.6)
and thus (δ+f)(¯x) provides a ﬁrst-order approximation to f ′(¯x) with
respect to h. Still assuming f ∈C2((a, b)), with a similar procedure we
can derive from the Taylor expansion
f(¯x −h) = f(¯x) −hf ′(¯x) + h2
2 f ′′(η)
(4.7)
with η ∈(¯x −h, ¯x), the backward ﬁnite diﬀerence
(δ−f)(¯x) = f(¯x) −f(¯x −h)
h
(4.8)
which is also ﬁrst-order accurate. Note that formulae (4.4) and (4.8) can
also be obtained by diﬀerentiating the linear polynomial interpolating f
at the points {¯x, ¯x+h} and {¯x−h, ¯x}, respectively. In fact, these schemes
amount to approximating f ′(¯x) by the slope of the straight line passing
through the two points (¯x, f(¯x)) and (¯x+h, f(¯x+h)), or (¯x−h, f(¯x−h))
and (¯x, f(¯x)), respectively (see Figure 4.2).
Finally, we introduce the centered ﬁnite diﬀerence formula
(δf)(¯x) = f(¯x + h) −f(¯x −h)
2h
(4.9)
If f ∈C3((a, b)), this formula provides a second-order approximation to
f ′(¯x) with respect to h. Indeed, by expanding f(¯x + h) and f(¯x −h)
at the third order around ¯x and summing up the two expressions, we
obtain
f ′(¯x) −(δf)(¯x) = −h2
12[f ′′′(ξ−) + f ′′′(ξ+)],
(4.10)

4.3 Numerical integration
117
where ξ−and ξ+ are suitable points in the intervals (¯x−h, ¯x) and (¯x, ¯x+
h), respectively (see Exercise 4.2).
By (4.9) f ′(¯x) is approximated by the slope of the straight line pass-
ing through the points (¯x −h, f(¯x −h)) and (¯x + h, f(¯x + h)).
Example 4.1 (Hydraulics) Let us solve Problem 4.1, using formulae (4.4),
(4.8) and (4.9), with h = 5, to approximate q′(t) at ﬁve diﬀerent points. We
obtain:
t
0
5
10
15
20
q′(t)
−0.0212
−0.0194
−0.0176
−0.0159
−0.0141
δ+q
−0.0203
−0.0185
−0.0168
−0.0150
−−
δ−q
−−
−0.0203
−0.0185
−0.0168
−0.0150
δq
−−
−0.0194
−0.0176
−0.0159
−−
The agreement between the exact derivative and the one computed from the
ﬁnite diﬀerence formulae with h = 5 is more satisfactory when using formula
(4.9) rather than (4.8) or (4.4).
■
In general, we can assume that the values of f are available at n + 1
equispaced points xi = x0 + ih, i = 0, . . . , n, with h > 0. In this case in
the numerical derivation f ′(xi) can be approximated by taking one of
the previous formulae (4.4), (4.8) or (4.9) with ¯x = xi.
Note that the centered formula (4.9) cannot be used at the extrema
x0 and xn. For these nodes we could use the values
1
2h [−3f(x0) + 4f(x1) −f(x2)]
at x0,
1
2h [3f(xn) −4f(xn−1) + f(xn−2)] at xn,
(4.11)
which are also second-order accurate with respect to h. They are ob-
tained by computing at the point x0 (respectively, xn) the ﬁrst deriva-
tive of the polynomial of degree 2 interpolating f at the nodes x0, x1, x2
(respectively, xn−2, xn−1, xn).
See Exercises 4.1-4.4.
4.3 Numerical integration
In this section we introduce numerical methods suitable for approximat-
ing the integral
I(f) =
b

a
f(x)dx,

118
4 Numerical diﬀerentiation and integration
where f is an arbitrary continuous function in [a, b]. We start by intro-
ducing some simple formulae, which are indeed special instances of the
family of Newton-Cotes formulae. Then we will introduce the so-called
Gaussian formulae, that feature the highest possible degree of exactness
for a given number of evaluations of the function f.
4.3.1 Midpoint formula
A simple procedure to approximate I(f) can be devised by partitioning
the interval [a, b] into subintervals Ik = [xk−1, xk], k = 1, . . . , M, with
xk = a + kH, k = 0, . . . , M and H = (b −a)/M. Since
I(f) =
M

k=1

Ik
f(x)dx,
(4.12)
on each sub-interval Ik we can approximate the exact integral of f by
that of a polynomial ˜f approximating f on Ik. The simplest solution
consists in choosing ˜f as the constant polynomial interpolating f at the
middle point of Ik:
¯xk = xk−1 + xk
2
.
In such a way we obtain the composite midpoint quadrature formula
Ic
mp(f) = H
M

k=1
f(¯xk)
(4.13)
The symbol mp stands for midpoint, while c stands for composite. This
formula is second-order accurate with respect to H. More precisely, if f
is continuously diﬀerentiable up to its second derivative in [a, b], we have
I(f) −Ic
mp(f) = b −a
24 H2f ′′(ξ),
(4.14)
where ξ is a suitable point in [a, b] (see Exercise 4.6). Formula (4.13)
is also called the composite rectangle quadrature formula because of its
geometrical interpretation, which is evident from Figure 4.3.
The classical midpoint formula (or rectangle formula) is obtained by
taking M = 1 in (4.13), i.e. using the midpoint rule directly on the
interval (a, b):
Imp(f) = (b −a)f[(a + b)/2]
(4.15)
The error is now given by

4.3 Numerical integration
119
f
f
a
b
(a + b)/2
¯x0
¯xk
¯xM
x
x
Figure 4.3. The composite midpoint formula (left); the midpoint formula
(right)
I(f) −Imp(f) = (b −a)3
24
f ′′(ξ),
(4.16)
where ξ is a suitable point in [a, b]. Relation (4.16) follows as a special
case of (4.14), but it can also be proved directly. Indeed, setting ¯x =
(a + b)/2, we have
I(f) −Imp(f) =
b

a
[f(x) −f(¯x)]dx
=
b

a
f ′(¯x)(x −¯x)dx + 1
2
b

a
f ′′(η(x))(x −¯x)2dx,
where η(x) is a suitable point in the interval whose endpoints are x and
¯x. Then (4.16) follows because
 b
a (x −¯x)dx = 0 and, by the mean value
theorem for integrals, there exists ξ ∈[a, b] such that
1
2
b

a
f ′′(η(x))(x −¯x)2dx = 1
2f ′′(ξ)
b

a
(x −¯x)2dx = (b −a)3
24
f ′′(ξ).
The degree of exactness of a quadrature formula is the maximum in-
teger r ≥0 for which the approximate integral (produced by the quadra-
ture formula) of any polynomial of degree r is equal to the exact integral.
We can deduce from (4.14) and (4.16) that the midpoint formula has de-
gree of exactness 1, since it integrates exactly all polynomials of degree
less than or equal to 1 (but not all those of degree 2).
The midpoint composite quadrature formula is implemented in Pro-
gram 4.1. Input parameters are the endpoints of the integration interval
a and b, the number of subintervals M and the MATLAB function f to
deﬁne the function f.

120
4 Numerical diﬀerentiation and integration
Program 4.1. midpointc: composite midpoint quadrature formula
function
Imp= midpointc (a,b,M,fun ,varargin )
% MIDPOINTC
Composite
midpoint
numerical
integration .
% IMP = MIDPOINTC (A,B,M,FUN) computes
an approximation
% of the integral
of the function
FUN via the
midpoint
% method (with M equal subintervals ).
FUN
accepts a
% real
vector input x and returns a real
vector
value.
% FUN can be either an inline
function , an
anonymous
% function , or it can be
defined by an external m-file .
% IMP=MIDPOINTC (A,B,M,FUN ,P1 ,P2 ,...) calls the function
% FUN passing the
optional
parameters
P1 ,P2 ,... as
% FUN(X,P1 ,P2 ,...).
H=(b-a)/M;
x = linspace (a+H/2,b-H/2,M);
fmp=fun(x,varargin {:}).* ones (1,M);
Imp=H*sum(fmp );
See the Exercises 4.5-4.8.
4.3.2 Trapezoidal formula
Another formula can be obtained by replacing f on Ik by the linear poly-
nomial interpolating f at the nodes xk−1 and xk (equivalently, replacing
f by ΠH
1 f, see Section 3.4, on the whole interval [a, b]). This yields
Ic
t (f) = H
2
M

k=1
[f(xk−1) + f(xk)]
= H
2 [f(a) + f(b)] + H
M−1

k=1
f(xk)
(4.17)
This formula is called the composite trapezoidal formula, and is second-
order accurate with respect to H. In fact, one can obtain the expression
I(f) −Ic
t (f) = −b −a
12 H2f ′′(ξ)
(4.18)
x0 = a
x0 = a
xk
xM = b
x
x
f
f
x1 = b
Figure 4.4. Composite trapezoidal formula (left); trapezoidal formula (right)

4.3 Numerical integration
121
for the quadrature error for a suitable point ξ ∈[a, b], provided that
f ∈C2([a, b]). When (4.17) is used with M = 1, we obtain
It(f) = b −a
2
[f(a) + f(b)]
(4.19)
which is called the trapezoidal formula because of its geometrical inter-
pretation. The error induced is given by
I(f) −It(f) = −(b −a)3
12
f ′′(ξ),
(4.20)
where ξ is a suitable point in [a, b]. We can deduce that (4.19) has degree
of exactness equal to 1, as is the case of the midpoint rule.
The composite trapezoidal formula (4.17) is implemented in the
MATLAB programs trapz and cumtrapz. If x is a vector whose com-
trapz
cumtrapz
ponents are the abscissae xk, k = 0, . . . , M (with x0 = a and xM = b),
and y that of the values f(xk), k = 0, . . . , M, z=cumtrapz(x,y) returns
the vector z whose components are zk ≃
 xk
a
f(x)dx, the integral be-
ing approximated by the composite trapezoidal rule. Thus z(M+1) is an
approximation of the integral of f on (a, b).
See the Exercises 4.9-4.11.
4.3.3 Simpson formula
The Simpson formula can be obtained by replacing the integral of f over
each Ik by that of its interpolating polynomial of degree 2 at the nodes
xk−1, ¯xk = (xk−1 + xk)/2 and xk,
Π2f(x) = 2(x −¯xk)(x −xk)
H2
f(xk−1)
+4(xk−1 −x)(x −xk)
H2
f(¯xk) + 2(x −¯xk)(x −xk−1)
H2
f(xk).
The resulting formula is called the composite Simpson quadrature
formula, and reads
Ic
s(f) = H
6
M

k=1
[f(xk−1) + 4f(¯xk) + f(xk)]
(4.21)
One can prove that it induces the error
I(f) −Ic
s(f) = −b −a
180
H4
16 f (4)(ξ),
(4.22)

122
4 Numerical diﬀerentiation and integration
where ξ is a suitable point in [a, b], provided that f ∈C4([a, b]). It is
therefore fourth-order accurate with respect to H. When (4.21) is applied
to only one interval, say [a, b], we obtain the so-called Simpson quadrature
formula
Is(f) = b −a
6
[f(a) + 4f((a + b)/2) + f(b)]
(4.23)
The error is now given by
I(f) −Is(f) = −1
16
(b −a)5
180
f (4)(ξ),
(4.24)
for a suitable ξ ∈[a, b]. Its degree of exactness is therefore equal to 3.
The composite Simpson rule is implemented in Program 4.2.
Program 4.2. simpsonc: composite Simpson quadrature formula
function [Isic ]= simpsonc (a,b,M,fun ,varargin )
% SIMPSONC
Composite
Simpson
numerical
integration .
% ISIC = SIMPSONC (A,B,M,FUN) computes
an approximation
% of the integral
of the function
FUN via the
Simpson
% method (using M equal subintervals ).
FUN
accepts
% real
vector input x and returns a real
vector
value.
% FUN can be either an inline
function , an
anonymous
% function , or it can be
defined by an external m-file .
% ISIC = SIMPSONC (A,B,M,FUN ,P1 ,P2 ,...) calls the
% function
FUN passing
the optional
parameters
% P1 ,P2 ,...
as FUN(X,P1 ,P2 ,...).
H=(b-a)/M;
x= linspace (a,b,M+1);
fpm=fun(x,varargin {:}).* ones (1,M+1);
fpm (2:end -1) = 2* fpm (2:end -1);
Isic =H*sum(fpm )/6;
x= linspace (a+H/2,b-H/2,M);
fpm=fun(x,varargin {:}).* ones (1,M);
Isic = Isic +2*H*sum(fpm )/3;
return
Example 4.2 (Demography) Let us consider Problem 4.4. To compute the
number of individuals whose height is between 1.8 and 1.9 m, we need to solve
the integral (4.3) for h = 1.8 and Δh = 0.1. For that we use the composite
Simpson formula with 100 sub-intervals
M = 200;
hbar = 1.7;
sigma = 0.1;
N = @(h)M/( sigma*sqrt (2*pi ))* exp(-(h-hbar ).^...
2./(2* sigma ^2));
int = simpsonc (1.8, 1.9, 100, N)
int =
27.1810
We therefore estimate that the number of individuals in this range of height
is 27.1810, corresponding to the 15.39 % of all individuals.
■

4.4 Interpolatory quadratures
123
10
−3
10
−2
10
−1
10
0
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
Figure 4.5. Logarithmic representation of the errors versus H for Simpson
(solid line with circles), midpoint (solid line) and trapezoidal (dashed line)
composite quadrature formulae
Example 4.3 We want to compare the approximations of the integral I(f) =
 2π
0
xe−x cos(2x)dx = −(10π −3 + 3e2π)/(25e2π) ≃−0.122122604618968 ob-
tained by using the composite midpoint, trapezoidal and Simpson formulae. In
Figure 4.5 we plot on the logarithmic scale the errors versus H. As pointed out
in Section 1.6, in this type of plot the greater the slope of the curve, the higher
the order of convergence of the corresponding formula. As expected from the
theoretical results, the midpoint and trapezoidal formulae are second-order
accurate, whereas the Simpson formula is fourth-order accurate.
■
4.4 Interpolatory quadratures
Quadrature formulas like (4.15), (4.19) or (4.23), refer to a single interval,
i.e. to M = 1, and for that they are said simple (or non-composite).
They can be regarded as special instances of a more general quadrature
formula of the form
Iappr(f) =
n

j=0
αjf(yj)
(4.25)
The real numbers {αj} are the quadrature weights, while the points {yj}
are the quadrature nodes. In general, one requires that (4.25) integrates
exactly at least a constant function: this property is ensured if *n
j=0 αj =
b −a. We can get a degree of exactness equal to (at least) n taking
Iappr(f) =
b

a
Πnf(x)dx ,

124
4 Numerical diﬀerentiation and integration
where Πnf ∈Pn is the Lagrange interpolating polynomial of the function
f at the nodes yi, i = 0, . . . , n, given by (3.4). This yields the following
expression for the weights
αi =
b

a
ϕi(x)dx,
i = 0, . . . , n,
where ϕi ∈Pn is the ith characteristic Lagrange polynomial such that
ϕi(yj) = δij, for i, j = 0, . . . , n, that was introduced in (3.3).
Example 4.4 For the trapezoidal formula (4.19) we have n = 1, y0 = a,
y1 = b and
α0 =
b

a
ϕ0(x)dx =
b

a
x −b
a −b dx = b −a
2
,
α1 =
b

a
ϕ1(x)dx =
b

a
x −a
b −a dx = b −a
2
.
■
The question that arises is whether suitable choices of the nodes
exist such that the degree of exactness is greater than n, more precisely,
equal to r = n + m for some m > 0. We can simplify our discussion by
restricting ourselves to a reference interval, say [−1, 1]. Indeed, once a
set of quadrature nodes {¯yj} and weights {¯αj} are available on [−1, 1],
then owing to the change of variable (3.11) we can immediately obtain
the corresponding nodes and weights,
yj = a + b
2
+ b −a
2
¯yj,
αj = b −a
2
¯αj
on an arbitrary integration interval [a, b ].
The answer to the previous question is furnished by the following
result (see, [QSS07, Chapter 10]):
Proposition 4.1 For a given m > 0, the quadrature formula
*n
j=0 ¯αjf(¯yj) has degree of exactness n + m iﬀit is of interpola-
tory type and the nodal polynomial ωn+1 = Πn
i=0(x −¯yi) associated
with the nodes {¯yi} is such that
1

−1
ωn+1(x)p(x)dx = 0,
∀p ∈Pm−1.
(4.26)

4.4 Interpolatory quadratures
125
Table 4.1. Nodes and weights for some quadrature formulae of Gauss-
Legendre type on the interval [−1, 1]. Weights corresponding to symmetric
couples of nodes are reported only once
n
{¯yj}
{¯αj}
1

±1/
√
3
1
1
{1}
2

±
√
15/5, 0
1
1
{5/9, 8/9}
3

±(1/35)

525 −70
√
30,

(1/36)(18 +
√
30),
1
1
±(1/35)

525 + 70
√
30
1
1
(1/36)(18 −
√
30)

4

0, ±(1/21)

245 −14
√
70
1
1

128/225, (1/900)(322 + 13
√
70)
±(1/21)

245 + 14
√
70
1
1
(1/900)(322 −13
√
70)

The maximum value that m can take is n + 1 and is achieved pro-
vided ωn+1 is proportional to the so-called Legendre polynomial of degree
n + 1, Ln+1(x). The Legendre polynomials can be computed recursively,
through the following three-term relation
L0(x) = 1,
L1(x) = x,
Lk+1(x) = 2k + 1
k + 1 xLk(x) −
k
k + 1Lk−1(x),
k = 1, 2, . . . .
For every n = 0, 1, . . . , every polynomial pn ∈Pn can be obtained by a
linear combination of the polynomials L0, L1, . . . , Ln. Moreover, Ln+1 is
orthogonal to all the Legendre polynomials of degree less than or equal
to n, i.e.,
 1
−1 Ln+1(x)Lj(x)dx = 0 for all j = 0, . . . , n. This explains why
(4.26) is true with m less than or equal to n + 1.
The maximum degree of exactness is therefore equal to 2n+1, and is
obtained for the so-called Gauss-Legendre formula (IGL in short), whose
nodes and weights are given by:
⎧
⎪
⎨
⎪
⎩
¯yj = zeros of Ln+1(x),
¯αj =
2
(1 −¯y2
j )[L′
n+1(¯yj)]2 ,
j = 0, . . . , n.
(4.27)
The weights ¯αj are all positive and the nodes are internal to the interval
[−1, 1]. In Table 4.1 we report nodes and weights for the Gauss-Legendre
quadrature formulae with n = 1, 2, 3, 4. If f ∈C(2n+2)([−1, 1]), the cor-
responding error is
I(f) −IGL(f) =
22n+3((n + 1)!)4
(2n + 3)((2n + 2)!)3 f (2n+2)(ξ),
where ξ is a suitable point in (−1, 1).
It is often useful to include also the endpoints of the interval among
the quadrature nodes. By doing so, the Gauss formula with the highest

126
4 Numerical diﬀerentiation and integration
Table 4.2. Nodes and weights for some quadrature formulae of Gauss-
Legendre-Lobatto on the interval [−1, 1]. Weights corresponding to symmetric
couples of nodes are reported only once
n
{¯yj}
{¯αj}
1
{±1}
{1}
2
{±1, 0}
{1/3, 4/3}
3
{±1, ±
√
5/5}
{1/6, 5/6}
4
{±1, ±
√
21/7, 0}
{1/10, 49/90, 32/45}
degree of exactness (2n−1) is the one that employs the so-called Gauss-
Legendre-Lobatto nodes (brieﬂy, GLL): for n ≥1
y0 = −1, yn = 1, yj = zeros of L′
n(x),
j = 1, . . . , n −1,
(4.28)
αj =
2
n(n + 1)
1
[Ln(¯yj)]2 ,
j = 0, . . . , n.
If f ∈C(2n)([−1, 1]), the corresponding error is given by
I(f) −IGLL(f) = −(n + 1)n322n+1((n −1)!)4
(2n + 1)((2n)!)3
f (2n)(ξ),
for a suitable ξ ∈(−1, 1). In Table 4.2 we give a table of nodes and
weights on the reference interval [−1, 1] for n = 1, 2, 3, 4. (For n = 1 we
recover the trapezoidal rule.)
Using the MATLAB instruction quadl(fun,a,b) it is possible to
quadl
compute an integral with a composite Gauss-Legendre-Lobatto quadra-
ture formula. The input arguments are: the function handle fun asso-
ciated with the function f, the endpoints a and b of the integration
interval. For instance, to integrate f(x) = 1/x over [1, 2], we must ﬁrst
deﬁne the function
fun=@(x) 1./x;
then call quadl(fun,1,2). Note that in the deﬁnition of function f we
have used an element by element operation (indeed MATLAB will evalu-
ate this expression component by component on the vector of quadrature
nodes).
The speciﬁcation of the number of subintervals is not requested as it
is automatically computed in order to ensure that the quadrature error is
below the default tolerance of 10−3. A diﬀerent tolerance can be provided
by the user through the extended command quadl(fun,a,b,tol). In
Section 4.5 we will introduce a method to estimate the quadrature error
and, consequently, to change H adaptively.

4.5 Simpson adaptive formula
127
Let us summarize
1. A quadrature formula is a formula to approximate the integral of
continuous functions on an interval [a, b];
2. it is generally expressed as a linear combination of the values of the
function at speciﬁc points (called nodes) with coeﬃcients which are
called weights;
3. the degree of exactness of a quadrature formula is the highest degree
of the polynomials which are integrated exactly by the formula. It
is one for the midpoint and trapezoidal rules, three for the Simpson
rule, 2n + 1 for the Gauss-Legendre formula using n + 1 quadrature
nodes, and 2n −1 for the Gauss-Legendre-Lobatto formula using
n + 1 nodes;
4. the order of accuracy of a composite quadrature formula is its order
with respect to the size H of the subintervals. The order of accuracy
is two for composite midpoint and trapezoidal formulae, four for
composite Simpson formula.
See the Exercises 4.12-4.18.
4.5 Simpson adaptive formula
The integration steplength H of a composite quadrature formula (4.21)
can be chosen in order to ensure that the quadrature error is less than
a prescribed tolerance ε > 0. For instance, when using the Simpson
composite formula, thanks to (4.22) this goal can be achieved if
b −a
180
H4
16 max
x∈[a,b]|f (4)(x)| < ε,
(4.29)
where f (4) denotes the fourth-order derivative of f. Unfortunately, when
the absolute value of f (4) is large only in a small part of the integra-
tion interval, the maximum H for which (4.29) holds true can be too
small. The goal of the adaptive Simpson quadrature formula is to yield
an approximation of I(f) within a ﬁxed tolerance ε by a nonuniform
distribution of the integration steplengths in the interval [a, b]. In such
a way we retain the same accuracy of the composite Simpson rule, but
with a lower number of quadrature nodes and, consequently, a reduced
number of evaluations of f.
To this end, we must ﬁnd an error estimator and an automatic pro-
cedure to modify the integration steplength H, according to the achieve-
ment of the prescribed tolerance. We start by analyzing this procedure,

128
4 Numerical diﬀerentiation and integration
which is independent of the speciﬁc quadrature formula that one wants
to apply.
In the ﬁrst step of the adaptive procedure, we compute an approxima-
tion Is(f) of I(f) =
 b
a f(x)dx. We set H = b−a and we try to estimate
the quadrature error. If the error is less than the prescribed tolerance,
the adaptive procedure is stopped; otherwise the steplength H is halved
until the integral
 a+H
a
f(x)dx is computed with the prescribed accu-
racy. When the test is passed, we consider the interval (a + H, b) and
we repeat the previous procedure, choosing as the ﬁrst steplength the
length b −(a + H) of that interval.
We use the following notations:
1. A: the active integration interval, i.e. the interval where the integral
is being computed;
2. S: the integration interval already examined, for which the error is
less than the prescribed tolerance;
3. N: the integration interval yet to be examined.
At the beginning of the integration process we have A = [a, b], N = ∅
and S = ∅, the situation at the generic step of the algorithm is de-
picted in Figure 4.6. Let JS(f) indicate the computed approximation of
 α
a f(x)dx, with JS(f) = 0 at the beginning of the process; if the algo-
rithm successfully terminates, JS(f) yields the desired approximation of
I(f). We also denote by J(α,β)(f) the approximate integral of f over the
active interval [α, β]. This interval is drawn in white in Figure 4.6. The
generic step of the adaptive integration method is organized as follows:
1. if the estimation of the error ensures that the prescribed tolerance is
satisﬁed, then:
(i) JS(f) is increased by J(α,β)(f), that is JS(f) ←JS(f) +
J(α,β)(f);
(ii) we let S ←S ∪A, A = N, N = ∅(corresponding to the path (I)
in Figure 4.6) and α ←β and β ←b;
2. if the estimation of the error fails the prescribed tolerance, then:
(j) A is halved, and the new active interval is set to A = [α, α′] with
α′ = (α + β)/2 (corresponding to the path (II) in Figure 4.6);
(jj) we let N ←N ∪[α′, β], β ←α′;
(jjj) a new error estimate is provided.
Of course, in order to prevent the algorithm from generating too small
steplengths, it is convenient to monitor the width of A and warn the user,
in case of an excessive reduction of the steplength, about the presence
of a possible singularity in the integrand function.
The problem now is to ﬁnd a suitable estimator of the error. To this
end, it is convenient to restrict our attention to a generic subinterval
[α, β] ⊂[a, b] in which we compute Is(f): of course, if on this interval

4.5 Simpson adaptive formula
129
                         
                               
(I)
(II)
S
S
S
N
N
A
A
A
α
α
α
α′
β
a
a
a
b
b
b
Figure 4.6. Distribution of the integration intervals at the generic step of the
adaptive algorithm and updating of the integration grid
the error is less than ε(β −α)/(b−a), then the error on the interval [a, b]
will be less than the prescribed tolerance ε. Since from (4.24) we get
Es(f; α, β) =
β

α
f(x)dx −Is(f) = −(β −α)5
2880
f (4)(ξ),
to ensure the achievement of the tolerance, it will be suﬃcient to ver-
ify that Es(f; α, β) < ε(β −α)/(b −a). In practical computation, this
procedure is not feasible since the point ξ ∈[α, β] is unknown.
To estimate the error Es(f; α, β) without using explicitly the value
f (4)(ξ), we employ again the composite Simpson formula to compute
 β
α f(x)dx, but with a steplength H = (β −α)/2. From (4.22) with
a = α and b = β, we deduce that
β

α
f(x)dx −Ic
s(f) = −(β −α)5
46080 f (4)(η),
(4.30)
where η is a suitable point diﬀerent from ξ. Subtracting the last two
equations, we get
ΔI = Ic
s(f) −Is(f) = −(β −α)5
2880
f (4)(ξ) + (β −α)5
46080 f (4)(η). (4.31)
Let us now make the assumption that f (4)(x) is approximately a con-
stant on the interval [α, β]. In this case f (4)(ξ) ≃f (4)(η). We can com-
pute f (4)(η) from (4.31) and, putting this value in the equation (4.30),
we obtain the following estimation of the error:
β

α
f(x)dx −Ic
s(f) ≃1
15ΔI.

130
4 Numerical diﬀerentiation and integration
The steplength (β−α)/2 (that is the steplength employed to compute
Ic
s(f)) will be accepted if |ΔI|/15 < ε(β −α)/[2(b −a)]. The quadrature
formula that uses this criterion in the adaptive procedure described pre-
viously, is called adaptive Simpson formula. It is implemented in Program
4.3. Among the input parameters, f is the string in which the function
f is deﬁned, a and b are the endpoints of the integration interval, tol
is the prescribed tolerance on the error and hmin is the minimum ad-
missible value for the integration steplength (in order to ensure that the
adaptation procedure always terminates).
Program 4.3. simpadpt: adaptive Simpson formula
function [JSf ,nodes]= simpadpt (fun ,a,b,tol ,hmin ,varargin )
% SIMPADPT
Adaptive
Simpson
quadrature
formula
% JSF = SIMPADPT (FUN ,A,B,TOL ,HMIN ) tries to approximate
% the integral
of function
FUN
from A to B within
% error TOL using recursive
adaptive
Simpson
% quadrature
with H>= HMIN . The
function
FUN should
% accept a vector
argument x and return a vector.
% FUN can be either an inline
function , an
anonymous
% function , or it can be
defined by an external m-file .
% JSF = SIMPADPT (FUN ,A,B,TOL ,HMIN ,P1 ,P2 ,...) calls the
% function
FUN passing
the optional
parameters
% P1 ,P2 ,...
as FUN(X,P1 ,P2 ,...).
% [JSF ,NODES] = SIMPADPT (...) returns the
distribution
% of nodes used in the
quadrature
process.
A=[a,b]; N=[]; S=[];
JSf = 0; ba = 2*(b - a); nodes =[];
while ~isempty(A),
[deltaI ,ISc ]= caldeltai (A,fun ,varargin {:});
if abs(deltaI) < 15* tol *(A(2)-A(1))/ ba;
JSf = JSf + ISc;
S = union(S,A);
nodes = [nodes , A(1) (A(1)+ A(2))*0.5 A(2)];
S = [S(1), S(end )]; A = N; N = [];
elseif A(2)-A(1) < hmin
JSf=JSf+ISc;
S = union(S,A);
S = [S(1), S(end )]; A=N; N=[];
warning(’Too small integration -step ’);
else
Am = (A(1)+ A(2))*0.5;
A = [A(1) Am];
N = [Am , b];
end
end
nodes=unique(nodes);
return
function [deltaI ,ISc]= caldeltai (A,fun ,varargin )
L=A(2)-A(1);
t=[0;
0.25; 0.5;
0.75; 1];
x=L*t+A(1); L=L/6;
w=[1; 4; 1]; wp =[1;4;2;4;1];
fx=fun(x,varargin {:}).* ones (5 ,1);
IS=L*sum(fx ([1 3 5]).*w);
ISc =0.5*L*sum(fx.*wp);
deltaI=IS -ISc;
return

4.6 Monte Carlo Methods for Numerical Integration
131
Example 4.5 Let us compute the integral I(f) =
 1
−1 20(1−x2)3dx by using
the adaptive Simpson formula. Using Program 4.3 with
fun=@(x)(1-x .^2).^3*20;
tol = 1.e-04;
hmin = 1.e -03; a=-1;b=1;
we ﬁnd the approximate value 18.2857116732797, instead of the exact value
18.2857142857143. The error is less than the prescribed tolerance tol=10−4,
precisely it is 2.6124 10−6. To obtain this result the adaptive formula requires
41 function evaluations; the corresponding composite formula with uniform
steplength would have required 90 function evaluations to yield an error of
2.5989 10−6.
■
4.6 Monte Carlo Methods for Numerical Integration
Monte Carlo methods are nowadays widely used for the solution of
stochastic diﬀerential equations and uncertainty quantiﬁcation problems.
Numerical integration methods based on Monte Carlo techniques are
a valid tool for approximating multidimensional integrals when the space
dimension of Rn gets large. These methods diﬀer from the approaches
considered thus far, since the choice of quadrature nodes is done statis-
tically according to the values attained by random variables having a
known probability distribution.
We recall that a random (or stochastic) variable X = X(ζ) =
(X1(ζ), . . . , Xn(ζ)) ∈Rn (or, more properly, random vector) is a func-
tion deﬁned for any outcome ζ of a random experiment, such that there
exists a probability density function p(X) associated with it (see [Pap87],
Chapter 4).
The probability density function p associated with the random vari-
able X ∈Rn is a real valued function satisfying
p(X1, . . . , Xn) ≥0,

Rn p(X1, . . . , Xn)dX = 1.
For a vector x = (x1, . . . , xn)T ∈Rn, the probability P{X ≤x} of the
random event {X1 ≤x1, . . . , Xn ≤xn} is given by
P{X ≤x} =
 xn
−∞
. . .
 x1
−∞
p(X1, . . . , Xn)dX1 . . . dXn.
Finally, given a function f deﬁned on the random variable X with
associated probability density function p(X), the statistical mean (or
expectation) of f is
μ(f) =

Rn f(X)p(X)dX.
(4.32)

132
4 Numerical diﬀerentiation and integration
The basic idea of Monte Carlo method is to interpret the integral of
a function f in terms of the statistical mean of the function itself. Iden-
tifying the generic point x = (x1, x2, . . . , xn)T ∈Rn with the random
variable X = (X1, X2, . . . , Xn)T we have

Ω
f(x)dx = |Ω|

Rn
|Ω|−1χΩ(X)f(X)dX = |Ω|μ(f),
where:
- |Ω| denotes the n-dimensional measure of Ω,
- χΩ(X) is the characteristic function of the set Ω, equal to 1 for X ∈Ω
and to 0 elsewhere,
- p(X) = |Ω|−1χΩ(X) is the probability density function associated
with the random variable X.
The numerical computation of the mean value μ(f) is carried out by
taking N mutually independent samples X1, . . . , XN ∈Rn of the random
variable X and evaluating the average
IN(f) = 1
N
N

i=1
f(Xi).
(4.33)
The strong law of large numbers ensures with probability 1 the con-
vergence of the average IN(f) to the mean value μ(f) as N →∞. In
computational practice the sequence of samples X1, . . . , XN is deter-
ministically produced by a random-number generator, giving rise to the
so-called pseudo-random integration formulae.
The quadrature error EN(f) = μ(f)−IN(f), as a function of N, can
be characterized through the variance
σ2(IN(f)) = μ

(μ(f) −IN(f))2
.
Moreover, it holds
σ2(IN(f)) = σ2(f)
N
,
(4.34)
from which, as N →∞, a convergence rate of O(N −1/2) follows for the
statistical estimate of the error EN(f), as EN(f) ∝

σ2(IN(f)) (see,
e.g. [KW08]).
Such convergence rate does not depend on the dimension n of the in-
tegration domain, and this is a most relevant feature of the Monte Carlo
method. However, it is worth noting that the convergence rate is inde-
pendent of the regularity of f; thus, unlike interpolatory quadratures,
Monte Carlo methods do not yield more accurate results when dealing
with smooth integrands.

4.7 What we haven’t told you
133
10
0
10
2
10
4
10
6
10
8
10
-5
10
-4
10
-3
10
-2
10
-1
10
0
N
10
0
10
2
10
4
10
6
10
8
10
-5
10
-4
10
-3
10
-2
10
-1
10
0
N
Figure 4.7. Absolute values of the errors EN(f) of Monte Carlo method
(continuous line) and N −1/2 (dashed line) for the functions of Example 4.6.
At left, the 2D case. At right, the 3D case.
The estimate (4.34) is extremely weak and in practice one does often
obtain poorly accurate results. A more eﬃcient implementation of Monte
Carlo methods is based on composite approach or semi-analytical meth-
ods; an example of these techniques is provided in the NAG (Numerical
Algorithms Group) Library, where a composite Monte Carlo method is
employed for the computation of integrals over hypercubes in Rn.
Example 4.6 Let us compute the integrals I2D =

[0,1]2 sin(πx) cos(πy)dxdy
and I3D =

[0,1]3 sin(πx) cos(πy) sin(πz)dxdydz by Monte Carlo formula (4.33)
with N = 10k/2 and k = 0, . . . , 15. The absolute value of the errors EN(f)
are plotted in Figure 4.7 (at left the 2D case, at right the 3D case). The
convergence rate O(N 1/2) of the error is observed in both cases.
■
4.7 What we haven’t told you
The midpoint, trapezoidal and Simpson formulae are particular cases of
a larger family of quadrature rules known as Newton-Cotes formulae. For
an introduction, see [QSS07, Chapter 9]. Similarly, the Gauss-Legendre
and the Gauss-Legendre-Lobatto formulae that we have introduced in
Section 4.4 are special cases of a more general family of Gaussian quadra-
ture formulae. These are optimal in the sense that they maximize the
degree of exactness for a prescribed number of quadrature nodes. For an
introduction to Gaussian formulae, see [QSS07, Chapter 10] or [RR01].
Further developments on numerical integration can be found, e.g., in
[DR75] and [PdDK¨UK83].
Numerical integration can also be used to compute integrals on un-
bounded intervals. For instance, to approximate
 ∞
0
f(x)dx, a ﬁrst pos-
sibility is to ﬁnd a point α such that the value of
 ∞
α f(x)dx can be

134
4 Numerical diﬀerentiation and integration
neglected with respect to that of
 α
0 f(x)dx. Then we compute by a
quadrature formula this latter integral on a bounded interval. A second
possibility is to resort to Gaussian quadrature formulae for unbounded
intervals (see [QSS07, Chapter 10]).
Finally, numerical integration can also be used to compute multidi-
mensional integrals. In particular, we mention the MATLAB instruction
dblquad(fun,xmin,xmax,ymin,ymax) by which it is possible to com-
dblquad
pute the integral of a given function f(x, y) on the rectangular domain
[xmin, xmax] × [ymin, ymax]. The function to be integrated is deﬁned
through the function handle fun that must be deﬁned on two variables
(e.g., x and y). If the function f is deﬁned in the MATLAB ﬁle fun.m,
the calling instruction is:
dblquad(@fun,xmin,xmax,ymin,ymax).
4.8 Exercises
Exercise 4.1 Verify that, if f ∈C3 in a neighborhood I0 of x0 (respectively,
In of xn) the error of formula (4.11) is equal to −1
3f ′′′(ξ0)h2 (respectively,
−1
3f ′′′(ξn)h2), where ξ0 and ξn are two suitable points belonging to I0 and In,
respectively.
Exercise 4.2 Verify that if f ∈C3 in a neighborhood of ¯x the error of the
formula (4.9) is equal to (4.10).
Exercise 4.3 Compute the order of accuracy with respect to h of the follow-
ing formulae for the numerical approximation of f ′(xi):
a.
−11f(xi) + 18f(xi+1) −9f(xi+2) + 2f(xi+3)
6h
,
b.
f(xi−2) −6f(xi−1) + 3f(xi) + 2f(xi+1)
6h
,
c.
−f(xi−2) −12f(xi) + 16f(xi+1) −3f(xi+2)
12h
.
Exercise 4.4 (Demography) The following values represent the time evo-
lution of the number n(t) of individuals of a given population whose birth rate
is constant (b = 2) and mortality rate is d(t) = 0.01n(t):
t (months)
0
0.5
1
1.5
2
2.5
3
n
100
147
178
192
197
199
200 .
Use this data to approximate as accurately as possible the rate of variation
of this population. Then compare the obtained results with the exact rate
n′(t) = 2n(t) −0.01n2(t).

4.8 Exercises
135
Exercise 4.5 Find the minimum number M of subintervals to approximate
with an absolute error less than 10−4 the integrals of the following functions:
f1(x) =
1
1 + (x −π)2 in [0, 5],
f2(x) = ex cos(x)
in [0, π],
f3(x) =

x(1 −x)
in [0, 1],
using the composite midpoint formula. Verify the results obtained using the
Program 4.1.
Exercise 4.6 Prove (4.14) starting from (4.16).
Exercise 4.7 Why does the midpoint formula lose one order of convergence
when used in its composite mode?
Exercise 4.8 Verify that, if f is a polynomial of degree less than or equal 1,
then Imp(f) = I(f) i.e. the midpoint formula has degree of exactness equal to
1.
Exercise 4.9 For the function f1 in Exercise 4.5, compute (numerically) the
values of M which ensure that the quadrature error is less than 10−4 when the
integral is approximated by the composite trapezoidal and composite Gauss-
Legendre (with n = 1) quadrature formulae.
Exercise 4.10 Let I1 and I2 be two values obtained by the composite trape-
zoidal formula applied with two diﬀerent steplengths, H1 and H2, for the
approximation of I(f) =
 b
a f(x)dx. Verify that, if f (2) has a mild variation
on (a, b), the value
IR = I1 + (I1 −I2)/(H2
2/H2
1 −1)
(4.35)
is a better approximation of I(f) than I1 and I2. This strategy is called the
Richardson extrapolation method. Derive (4.35) from (4.18).
Exercise 4.11 Verify that, among all formulae of the form Iappr(f) = αf(¯x)+
βf(¯z) where ¯x, ¯z ∈[a, b] are two unknown nodes and α and β two undetermined
weights, the Gauss formula with n = 1 of Table 4.1 features the maximum
degree of exactness.
Exercise 4.12 For the ﬁrst two functions of Exercise 4.5, compute the min-
imum number of intervals such that the quadrature error of the composite
Simpson quadrature formula is less than 10−4.
Exercise 4.13 Compute
 2
0 e−x2/2dx using the Simpson formula (4.23) and
the Gauss-Legendre formula of Table 4.1 in the case n = 1, then compare the
obtained results.

136
4 Numerical diﬀerentiation and integration
Exercise 4.14 To compute the integrals Ik =
 1
0 xkex−1dx for k = 1, 2, . . .,
one can use the following recursive formula: Ik = 1 −kIk−1, with I1 = 1/e.
Compute I20 using the composite Simpson formula in order to ensure that the
quadrature error is less than 10−3. Compare the Simpson approximation with
the result obtained using the above recursive formula.
Exercise 4.15 Derive the Richardson extrapolation method for both Simp-
son formula (4.23) and Gauss-Legendre formula of Table 4.1 for n = 1. Then
apply it for the approximation of the integral I(f) =
 2
0 e−x2/2dx, with H1 = 1
and H2 = 0.5. Verify that in both cases IR is more accurate than I1 and I2.
Exercise 4.16 (Electromagnetism) Compute using the composite Simp-
son formula the function j(r, 0) deﬁned in (4.2) for r = k/10 m with
k = 1, . . . , 10, ρ(ξ) = eξ and σ = 0.36 W/(mK). Ensure that the quadra-
ture error is less than 10−10. (Recall that: m=meters, W=watts, K=degrees
Kelvin.)
Exercise 4.17 (Optics) By using the composite
Simpson and Gauss-
Legendre with n = 1 formulae compute the function E(T ), deﬁned in (4.1),
for T equal to 213 K, up to at least 10 exact signiﬁcant digits.
Exercise 4.18 Develop a strategy to compute I(f) =
 1
0 |x2 −0.25|dx by the
composite Simpson formula such that the quadrature error is less than 10−2.

5
Linear systems
In applied sciences, one is quite often led to face a linear system of the
form
Ax = b,
(5.1)
where A is a square matrix of dimension n × n whose elements aij are
either real or complex, while x and b are column vectors of dimension
n: x represents the unknown solution while b is a given vector. Compo-
nentwise, (5.1) can be written as
a11x1 + a12x2 + . . . + a1nxn = b1,
a21x1 + a22x2 + . . . + a2nxn = b2,
...
...
...
an1x1 + an2x2 + . . . + annxn = bn.
Before proceeding we present four diﬀerent problems that give rise
to linear systems.
5.1 Some representative problems
Problem 5.1 (Hydraulic network) Let us consider the hydraulic
network made of the 10 pipelines in Figure 5.1, which is fed by a reservoir
of water at constant pressure p0 = 10 bar. In this problem, pressure val-
ues refer to the diﬀerence between the real pressure and the atmospheric
one. For the jth pipeline, the following relationship holds between the
ﬂow-rate Qj (in m3/s) and the pressure gap Δpj at pipe-ends
Qj =
1
RjLj
Δpj,
(5.2)
A. Quarteroni et al., Scientiﬁc Computing with MATLAB and Octave,
Texts in Computational Science and Engineering 2,
DOI 10.1007/978-3-642-45367-0 5, © Springer-Verlag Berlin Heidelberg 2014
137

138
5 Linear systems
Q1
Q2
Q3
Q4
Q5
Q6
Q7
Q8
Q9
Q10
p = 0
p = 0
p = 0
p = 0
1
2
3
4
Figure 5.1. The pipeline network of Problem 5.1
where Rj is the hydraulic resistance per unit length (in (bar s)/m4) and
Lj is the length (in m) of the jth pipeline. We assume that water ﬂows
from the outlets (indicated by a black dot) at atmospheric pressure,
which is set to 0 bar for coherence with the previous convention.
A typical problem consists in determining the pressure values at each
internal node 1, 2, 3, 4. With this aim, for each j = 1, 2, 3, 4 we can
supplement the relationship (5.2) with the statement that the algebraic
sum of the ﬂow-rates of the pipelines which meet at node j must be null
(a negative value would indicate the presence of a seepage).
Denoting by p = (p1, p2, p3, p4)T the pressure vector at the internal
nodes, we get a 4 × 4 system of the form Ap = b.
In the following table we report the relevant characteristics of the
diﬀerent pipelines identiﬁed by the index j:
j
Rj
Lj
j
Rj
Lj
j
Rj
Lj
1
0.2500
20
2
2.0000
10
3
1.0204
14
4
2.0000
10
5
2.0000
10
6
7.8125
8
7
7.8125
8
8
7.8125
8
9
2.0000
10
10
7.8125
8
Correspondingly, A and b take the following values (only the ﬁrst 4
signiﬁcant digits are provided):
A =
⎡
⎢⎢⎣
−0.370
0.050
0.050
0.070
0.050 −0.116
0
0.050
0.050
0 −0.116
0.050
0.070
0.050
0.050 −0.202
⎤
⎥⎥⎦, b =
⎡
⎢⎢⎣
−2
0
0
0
⎤
⎥⎥⎦.
The solution of this system is postponed to Example 5.5.
■
Problem 5.2 (Spectrometry) Let us consider a gas mixture of n non-
reactive unknown components. Using a mass spectrometer the compound
is bombarded by low-energy electrons: the resulting mixture of ions is

5.1 Some representative problems
139
analyzed by a galvanometer which shows peaks corresponding to speciﬁc
ratios mass/charge. We only consider the n most relevant peaks. One
may conjecture that the height hi of the ith peak is a linear combination
of {pj, j = 1, . . . , n}, pj being the partial pressure of the jth component
(that is the pressure exerted by a single gas when it is part of a mixture),
yielding
n

j=1
sijpj = hi,
i = 1, . . . , n,
(5.3)
where the sij are the so-called sensitivity coeﬃcients. The determination
of the partial pressures demands therefore the solution of a linear system.
For its solution, see Example 5.3.
■
Problem 5.3 (Economy: input-output analysis) We want to de-
termine the situation of equilibrium between demand and oﬀer of certain
goods. In particular, let us consider a production model in which m ≥n
factories (or production lines) produce n diﬀerent products. They must
face the internal demand of goods (the input) necessary to the factories
for their own production, as well as the external demand (the output)
from the consumers. Leontief proposed in (1930)1 the amount of a cer-
tain output is proportional to the quantity of input used. Under this
assumption the activity of the factories is completely described by two
matrices, the input matrix C= (cij) ∈Rn×m and the output matrix
P= (pij) ∈Rn×m. (“C” stands for consumables and “P” for products.)
The coeﬃcient cij (respectively, pij) represent the quantity of the ith
good absorbed (respectively, produced) by the jth factory for a ﬁxed pe-
riod of time. The matrix A=P−C is called input-output matrix: a positive
(resp., negative) entry aij denotes the quantity of the ith good produced
(respectively, absorbed) by the jth factory. Finally, it is reasonable to
assume that the production system satisﬁes the demand of goods from
the market, that can be represented by a vector b= (bi) ∈Rn (the vector
of the ﬁnal demand). The component bi represents the quantity of the
ith good absorbed by the market. The equilibrium is reached when the
vector x= (xi) ∈Rm of the total production equals the total demand,
that is,
Ax = b,
where A = P −C.
(5.4)
For simplicity we will assume that ith factory produces only the ith good
(see Figure 5.2). Consequently, n = m and P = I. For the solution of
this linear system see Exercise 5.19.
■
1 On 1973 Wassily Leontief was awarded the Nobel prize in Economy for his
studies. a linear production model for which

140
5 Linear systems
c33
1
2
3
c31
c12
c11
c22
b1
b2
b3
Figure 5.2. The interaction scheme between three factories and the market
Problem 5.4 (Capillary networks) Capillaries are tiny blood ves-
sels, the smallest units of the blood circulatory system. They group to-
gether giving rise to networks called capillary beds featuring a variable
number of elements, say from 10 to 100, depending upon the kind of
organ and the speciﬁc biological tissue. The oxygenated blood reaches
capillary beds from the arterioles, and from capillary beds it is released
to the surrounding tissue passing through the membrane of red blood
cells. Meanwhile, metabolic wastes are eliminated from the tissue by
ﬂowing into the capillary bed where it is gathered into small venules and
eventually conveyed to the heart and from there to lungs. A capillary
bed can be described by a network, similar to the hydraulic network con-
sidered in Problem 5.1; in this model, every capillary is assimilated to a
pipeline whose endpoints are called nodes. In the schematic illustration
of Figure 5.4, nodes are represented by empty little circles. From a func-
tional viewpoint, the arteriole feeding the capillary bed can be regarded
as a reservoir at uniform pressure (about 50 mmHg - note that one at-
mosphere corresponds to 760 mmHg). In our model we will assume that
at the exiting nodes (those indicated by small black circles in Figure
5.4) the pressure features a constant value, that of the venous pressure,
that we can normalize to zero. Blood ﬂows from arterioles to the exiting
nodes because of the pressure gap between one node and the following
ones (those standing at a hierarchically lower level).
Still referring to Figure 5.4, we denote by pj, j = 1, ..., 15 (measured
in mmHg) the pressure at the jth node and by Qm, m = 1, ..., 31 (mea-
sured in mm3/s) the ﬂow inside the mth capillary vessel. For any m,
denoting by i and j the end-points of the mth capillary, we adopt the
following constitutive relation
Qm =
1
RmLm
(pi −pj),
(5.5)

5.1 Some representative problems
141
Figure 5.3. A capillary bed
                                                                                                         Figure 5.4. Schematization of a capillary bed
where Rm denotes the hydraulic resistance per unit length (in (mmHg
s)/mm4) and Lm the capillary length (in mm). Obviously, in considering
the node number 1, we will take into account p0 = 50; similarly, in
considering the nodes from n. 8 to n. 15, we will set null pressure at
outﬂow nodes (from n. 16 to n. 31). Finally, at any node of the network
we will impose a balance equation between inﬂow and outﬂow, i.e.
+
m in
Qm
,
−
+ 
m out
Qm
,
= 0.
In this way the following linear system is obtained
Ap = b,
(5.6)

142
5 Linear systems
where p = [p1, p2, · · · , p15]T is the unknown vector of pressure at 15
nodes of the network, A is the matrix, while b is the array of known
data.For simplicity, let us suppose that all capillaries have the same hy-
draulic resistance Rm = 1 and that the length of the ﬁrst capillary
is L1 = 20, while capillary length halves at each bifurcation (then
L2 = L3 = 10, L4 = . . . = L7 = 5 etc.). The following matrix is
generated
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
−1
4
1
10
1
10
0
0
0
0
0
0
0
0
0
0
0
0
1
10
−1
2
0
1
5
1
5
0
0
0
0
0
0
0
0
0
0
1
10
0
−1
2
0
0
1
5
1
5
0
0
0
0
0
0
0
0
0
1
5
0
−1
0
0
0
0.4
0.4
0
0
0
0
0
0
0
1
5
0
0
−1
0
0
0
0
0.4
0.4
0
0
0
0
0
0
1
5
0
0
−1
0
0
0
0
0
0.4
0.4
0
0
0
0
1
5
0
0
0
−1
0
0
0
0
0
0
0.4
0.4
0
0
0
0.4
0
0
0
−2
0
0
0
0
0
0
0
0
0
0
0.4
0
0
0
0
−2
0
0
0
0
0
0
0
0
0
0
0.4
0
0
0
0
−2
0
0
0
0
0
0
0
0
0
0.4
0
0
0
0
0
−2
0
0
0
0
0
0
0
0
0
0.4
0
0
0
0
0
−2
0
0
0
0
0
0
0
0
0.4
0
0
0
0
0
0
−2
0
0
0
0
0
0
0
0
0.4
0
0
0
0
0
0
−2
0
0
0
0
0
0
0
0.4
0
0
0
0
0
0
0
−2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
while b = [−5/2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]T.
The solution of this system will be considered in Example 5.7.
■
5.2 Linear system and complexity
The solution of system (5.1) exists iﬀA is nonsingular. In principle, it
might be computed using the so-called Cramer rule
xi = det(Ai)
det(A) ,
i = 1, . . . , n,
where Ai is the matrix obtained from A by replacing the ith column by
b and det(A) denotes the determinant of A. If the n+1 determinants are
computed by the Laplace expansion (see Exercise 5.1), a total number
of approximately 3(n+1)! operations is required. As usual, by operation
we mean a sum, a subtraction, a product or a division. For instance,
a computer capable of carrying out 109 ﬂops (i.e. 1 Giga ﬂops), would
require about 17 hours to solve a system of dimension n = 15, 4860 years
if n = 20 and 10143 years if n = 100. See Table 5.1. Note that 109 ﬂops
is the characteristic speed of a current PC (e.g. with a processor Intel R
⃝
CoreTM2 Duo, 2.53 GHz) whereas the Tianhe-2 (MilkyWay-2) Cluster
of the National University of Defense Technology in China, 1st of the
top500 supercomputer list as of November 2013, features a rate of 33
Peta-ﬂops (i.e. 33 · 1015 ﬂops).

5.3 The LU factorization method
143
Table 5.1. Time required to solve a linear system of dimension n by the
Cramer rule. “o.r.” stands for “out of reach”
Flops
n
109 (Giga)
1010
1011
1012 (Tera)
1015 (Peta)
10
10−1sec
10−2sec
10−3sec
10−4sec
negligible
15
17 hours
1.74 hours
10.46 min
1 min
0.6 10−1 sec
20
4860 years
486 years
48.6 years
4.86 years
1.7 day
25
o.r.
o.r.
o.r.
o.r.
38365 years
The computational cost can be drastically reduced to the order of
about n3.8 operations if the n + 1 determinants are computed by the
algorithm quoted in Example 1.3. Yet, this cost is still too high for large
values of n, which often arise in practical applications.
Two alternative approaches will be pursued: they are called direct
methods if they yield the solution of the system in a ﬁnite number of
steps, iterative methods if they require (in principle) an inﬁnite number
of steps. Iterative methods will be addressed in Section 5.9. We warn
the reader that the choice between direct and iterative methods may
depend on several factors: primarily, the predicted theoretical eﬃciency
of the scheme, but also the particular type of matrix, the memory storage
requirements and, ﬁnally, the computer architecture (see, Section 5.13
for more details).
Finally, we note that a system with full matrix cannot be solved
by less than n2 operations. Indeed, if the equations are fully coupled,
we should expect that every one of the n2 matrix coeﬃcients would be
involved in an algebraic operation at least once.
Even though most of the methods that we will present in this Sec-
tion are valid for compex matrices too, for simplicity we will limit our
analysis to real matrices. In any case, we note that MATLAB and Oc-
tave programs for solving linear systems work on both real and complex
variables, with no need to modify the calling instructions.
We will explicitly refer to complex matrices only when the assump-
tions that we have to make on real matrices have to be replaced by spe-
ciﬁc conditions in the complex ﬁeld. This happens, for instance, when
we have to deﬁne positive deﬁnite matrices, or when we need to specify
the conditions that underly the Cholesky factorization of a matrix.
5.3 The LU factorization method
Let A∈Rn×n. Assume that there exist two suitable matrices L and U,
lower triangular and upper triangular, respectively, such that
A = LU
(5.7)

144
5 Linear systems
We call (5.7) an LU-factorization (or decomposition) of A. If A is non-
singular, so are both L and U, and thus their diagonal elements are
non-null (as observed in Section 1.4).
In such a case, solving Ax = b leads to the solution of the two
triangular systems
Ly = b,
Ux = y
(5.8)
Both systems are easy to solve. Indeed, L being lower triangular, the
ﬁrst row of the system Ly = b takes the form:
l11y1 = b1,
which provides the value of y1 since l11 ̸= 0. By substituting this value
of y1 in the subsequent n −1 equations we obtain a new system whose
unknowns are y2, . . . , yn, on which we can proceed in a similar manner.
Proceeding forward, equation by equation, we can compute all unknowns
with the following forward substitutions algorithm:
y1 = 1
l11
b1,
yi = 1
lii
⎛
⎝bi −
i−1

j=1
lijyj
⎞
⎠, i = 2, . . . , n
(5.9)
Let us count the number of operations required by (5.9). Since i −1
sums, i −1 products and 1 division are needed to compute the unknown
yi, the total number of operations required is
n

i=1
1 + 2
n

i=1
(i −1) = 2
n

i=1
i −n = n2.
The system Ux = y can be solved by proceeding in a similar manner.
This time, the ﬁrst unknown to be computed is xn, then, by proceeding
backward, we can compute the remaining unknowns xi, for i = n −1 to
i = 1:
xn =
1
unn
yn,
xi = 1
uii
⎛
⎝yi −
n

j=i+1
uijxj
⎞
⎠, i = n −1, . . . , 1
(5.10)
This is called backward substitutions algorithm and requires n2 opera-
tions too. At this stage we need an algorithm that allows an eﬀective
computation of the factors L and U of the matrix A. We illustrate a
general procedure starting from a couple of examples.

5.3 The LU factorization method
145
Example 5.1 Let us write the relation (5.7) for a generic matrix A ∈R2×2
	 l11 0
l21 l22

 	 u11 u12
0 u22

=
	 a11 a12
a21 a22

.
The 6 unknown elements of L and U must satisfy the following (nonlinear)
equations:
(e1) l11u11 = a11,
(e2) l11u12 = a12,
(e3) l21u11 = a21,
(e4) l21u12 + l22u22 = a22.
(5.11)
System (5.11) is underdetermined as it features less equations than un-
knowns. We can complete it by assigning arbitrarily the diagonal elements of
L, for instance setting l11 = 1 and l22 = 1. Now system (5.11) can be solved by
proceeding as follows: we determine the elements u11 and u12 of the ﬁrst row
of U using (e1) and (e2). If u11 is non-null then from (e3) we deduce l21 (that
is the ﬁrst column of L, since l11 is already available). Now we can obtain from
(e4) the only nonzero element u22 of the second row of U.
■
Example 5.2 Let us repeat the same computations in the case of a 3 × 3
matrix. For the 12 unknown coeﬃcients of L and U we have the following 9
equations:
(e1) l11u11 = a11, (e2) l11u12 = a12,
(e3) l11u13 = a13,
(e4) l21u11 = a21, (e5) l21u12 + l22u22 = a22, (e6) l21u13 + l22u23 = a23,
(e7) l31u11 = a31, (e8) l31u12 + l32u22 = a32, (e9) l31u13+l32u23+l33u33 =a33.
Let us complete this system by setting lii = 1 for i = 1, 2, 3. Now, the
coeﬃcients of the ﬁrst row of U can be obtained by using (e1), (e2) and (e3).
Next, using (e4) and (e7), we can determine the coeﬃcients l21 and l31 of the
ﬁrst column of L. Using (e5) and (e6) we can now compute the coeﬃcients u22
and u23 of the second row of U. Then, using (e8), we obtain the coeﬃcient l32
of the second column of L. Finally, the last row of U (which consists of the
only element u33) can be determined by solving (e9).
■
On a matrix A∈Rn×n of arbitrary dimension n we can proceed as
follows:
1. the elements of L and U satisfy the system of nonlinear equations
min(i,j)

r=1
lirurj = aij, i, j = 1, . . . , n;
(5.12)
2. system (5.12) is underdetermined; indeed, there are n2 equations
and n2 + n unknowns. Consequently, the LU factorization cannot be
unique; more precisely, inﬁnite pairs of matrices L and U satisfying
(5.12) can exist;
3. by forcing the n diagonal elements of L to be equal to 1, (5.12) turns
into a determined system which can be solved by the following Gauss
algorithm, set A(1) = A i.e. a(1)
ij = aij for i, j = 1, . . . , n;

146
5 Linear systems
for k = 1, . . . , n −1
for i = k + 1, . . . , n
lik = a(k)
ik
a(k)
kk
,
for j = k + 1, . . . , n
a(k+1)
ij
= a(k)
ij −lika(k)
kj
(5.13)
The elements a(k)
kk must all be diﬀerent from zero and are called pivot
elements. For every k = 1, . . . , n −1 the matrix A(k+1) = (a(k+1)
ij
) has
n −k rows and columns.
At the end of this procedure the elements of the upper triangular
matrix U are given by uij = a(i)
ij
for i = 1, . . . , n and j = i, . . . , n,
whereas those of L are given by the coeﬃcients lij generated by this
algorithm. In (5.13) there is no computation of the diagonal elements of
L, as we already know that their value is equal to 1.
This LU factorization is also called Gauss LU factorization; from now
on we will simply call it LU factorization. Determining the elements of
the factors L and U requires about 2n3/3 operations (see Exercise 5.4).
Remark 5.1 Storing all the matrices A(k) in the algorithm (5.13) is not nec-
essary; actually we can overlap the (n −k) × (n −k) elements of A(k+1) on
the corresponding last (n −k) × (n −k) elements of the original matrix A.
Moreover, since at step k, the subdiagonal elements of the kth column don’t
have any eﬀect on the ﬁnal U, they can be replaced by the entries of the kth
column of L (the so-called multipliers), as done in Program 5.1. Then, at step
k of the process the elements stored at location of the original entries of A are
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
a(1)
11 a(1)
12 . . .
l21 a(2)
22
...
... ...
. . . . . . a(1)
1n
a(2)
2n
...
lk1 . . . lk,k−1
...
...
ln1 . . . ln,k−1
a(k)
kk . . . a(k)
kn
...
...
a(k)
nk . . . a(k)
nn
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
where the boxed submatrix is A(k).
More precisely, this algorithm can be implemented by storing a unique
matrix, which is initialized equal to A and then modiﬁed at each step k ≥2
by overwriting the new entries a(k)
ij , for i, j ≥k + 1, as well as the multipliers
lik, for i ≥k + 1. Note that it is not indispensable to store diagonal elements
lii; in fact, it is understood that they are all equal to 1.
■

5.3 The LU factorization method
147
Remark 5.2 (Gauss Elimination Method (GEM)) Gauss
algorithm
(5.13) can be applied also to the right hand side of the linear system Ax = b
as follows:
set A(1) = A i.e. a(1)
ij = aij for i, j = 1, . . . , n; set b(1) = b i.e. b(1)
i
= bi for
i = 1, . . . , n;
for k = 1, . . . , n −1
for i = k + 1, . . . , n
lik = a(k)
ik
a(k)
kk
,
for j = k + 1, . . . , n
a(k+1)
ij
= a(k)
ij −lika(k)
kj
b(k+1)
i
= b(k)
i
−likb(k)
k
(5.14)
This algorithm is called Gauss Elimination Methods (GEM). At the end of
the process, the upper triangular system A(n)x = b(n), which is equivalent to
the original one, can be solved by the backward substitution algorithm (5.10).
■
Example 5.3 (Spectrometry) For the Problem 5.2 we consider a gas mix-
ture that, after a spectroscopic inspection, presents the following seven most
relevant peaks: h1 = 17.1, h2 = 65.1, h3 = 186.0, h4 = 82.7, h5 = 84.2,
h6 = 63.7 and h7 = 119.7. We want to compare the measured total pressure,
equal to 38.78 μm of Hg (which accounts also for those components that we
might have neglected in our simpliﬁed model) with that obtained using rela-
tions (5.3) with n = 7, where the sensitivity coeﬃcients are given in Table 5.2
(taken from [CLW69, p. 331]). The partial pressures can be computed solving
the system (5.3) for n = 7 using the LU factorization. We obtain
partpress=
0.6525
2.2038
0.3348
6.4344
2.9975
0.5505
25.6317
Using these values we compute an approximate total pressure (given by
sum(partpress)) of the gas mixture which diﬀers from the measured value
by 0.0252 μm of Hg.
■
Example 5.4 Consider the Vandermonde matrix
A = (aij) with aij = xn−j
i
, i, j = 1, . . . , n,
(5.15)
where the xi are n distinct abscissae. It can be constructed using the MATLAB
command vander. In Table 5.3 we report the time required to compute the
vander
LU factorization of A (which behaves like 2n3/3, see Figure 5.5) on diﬀerent

148
5 Linear systems
Table 5.2. The sensitivity coeﬃcients for a gas mixture
Components and indices
Peak Hydrogen Methane Etilene Ethane Propylene Propane n-Pentane
index
1
2
3
4
5
6
7
1
16.87
0.1650
0.2019
0.3170
0.2340
0.1820
0.1100
2
0.0
27.70
0.8620
0.0620
0.0730
0.1310
0.1200
3
0.0
0.0
22.35
13.05
4.420
6.001
3.043
4
0.0
0.0
0.0
11.28
0.0
1.110
0.3710
5
0.0
0.0
0.0
0.0
9.850
1.1684
2.108
6
0.0
0.0
0.0
0.0
0.2990
15.98
2.107
7
0.0
0.0
0.0
0.0
0.0
0.0
4.670
Table 5.3.
Time required to solve a full linear system of dimension n by
MEG. “o.r.” stands for “out of reach”
Flops
n
109 (Giga)
1012 (Tera)
1015 (Peta)
102
7 · 10−4sec
negligible
negligible
104
11 min
0.7 sec
7 · 10−4sec
106
21 years
7.7 months
11 min
108
o.r.
o.r.
21 years
0
20
40
60
80
100
0
1
2
3
4
5
6
7 x 10
5
Figure 5.5. The number of ﬂoating-point operations necessary to gener-
ate the LU factorization of the Vandermonde matrix, as a function of the
matrix dimension n. This function is a cubic polynomial obtained by ap-
proximating in the least-squares sense the values (represented by circles for
n = 10, 20, . . . , 100)
computers featuring 1 GigaFlops, 1 TeraFlops and 1PetaFlops performance,
respectively. In Figure 5.5 we plot the number of ﬂoating-point operations
necessary to generate the LU factorization of the Vandermonde matrix, as
a function of the matrix dimension n. These values were provided by the
command flops that was present in former versions of MATLAB.
■

5.3 The LU factorization method
149
The LU factorization is the basis of several MATLAB commands:
-
[L,U]=lu(A)
whose mode of use will be discussed in Section 5.4;
lu
-
inv that allows the computation of the inverse of a matrix;
inv
\
-
\ by which it is possible to solve a linear system with matrix A and
right hand side b by simply writing A\b (see Section 5.8).
Remark 5.3 (Computing a determinant) By means of the LU factoriza-
tion one can compute the determinant of A with a computational cost of O(n3)
operations, noting that (see Sect.1.4)
det(A) = det(L) det(U) =
n

k=1
ukk.
As a matter of fact, this procedure is also at the basis of the MATLAB com-
mand det.
■
In Program 5.1 we implement the algorithm (5.13). The factor L is
stored in the (strictly) lower triangular part of A and U in the upper tri-
angular part of A (for the sake of storage saving). After the program ex-
ecution, the two factors can be recovered by simply writing: L = eye(n)
+ tril(A,-1) and U = triu(A), where n is the size of A.
Program 5.1. lugauss: Gauss LU factorization
function A=lugauss (A)
%LUGAUSS LU
factorization
without
pivoting .
% A = LUGAUSS(A) stores an upper triangular
matrix in
% the upper
triangular
part
of A and a lower triangular
% matrix in the
strictly
lower part of A (the
diagonal
% elements
of L are 1).
[n,m]= size (A);
if n ~= m; error(’A is not a square
matrix ’); else
for k = 1:n-1
for i = k+1:n
A(i,k) = A(i,k)/A(k,k);
if A(k,k) == 0, error(’Null
diagonal
element ’); end
j = [k+1:n]; A(i,j) = A(i,j) - A(i,k)*A(k,j);
end
end
end
Example 5.5 Let us compute the solution of the system encountered in Prob-
lem 5.1 by using the LU factorization, then applying the backward and forward
substitution algorithms. We need to compute the matrix A and the right-hand
side b and execute the following instructions:
A= lugauss(A);
y(1)= b(1);
for i=2:4; y=[y; b(i)-A(i,1:i -1)* y(1:i -1)]; end
x(4)= y(4)/ A(4 ,4);
for i=3: -1:1
x(i)=(y(i)-A(i,i+1:4)*x(i+1:4) ’)/ A(i,i); end
p=x

150
5 Linear systems
The result is p = (8.1172, 5.9893, 5.9893, 5.7779)T .
■
Example 5.6 Suppose that we solve Ax = b with
A =
⎡
⎢⎣
1 1 −ε 3
2
2
2
3
6
4
⎤
⎥⎦, b =
⎡
⎢⎣
5 −ε
6
13
⎤
⎥⎦, ε ∈R,
(5.16)
whose solution is x = (1, 1, 1)T (independently of the value of ε).
Let us set ε = 1. The LU factorization of A obtained by the Program 5.1 yields
L =
⎡
⎣
1 0 0
2 1 0
3 3 1
⎤
⎦, U =
⎡
⎣
1 0 3
0 2 −4
0 0 7
⎤
⎦.
If we set ε = 0, despite the fact that A is non singular, the LU factorization
cannot be carried out since the algorithm (5.13) would involve divisions by 0.
■
The previous example shows that, unfortunately, the LU factorization
A=LU does not necessarily exist for every nonsingular matrix A. In this
respect, the following result can be proven:
Proposition 5.1 For a given matrix A ∈Rn×n, its LU factoriza-
tion exists and is unique iﬀthe principal submatrices Ai of A of
order i = 1, . . . , n −1 (that is those obtained by restricting A to its
ﬁrst i rows and columns) are nonsingular. (This result holds also for
any A ∈Cn×n [Zha99, Sect. 3.2].)
Going back to Example 5.6, we can notice that when ε = 0 the second
principal submatrix A2 of the matrix A is singular.
We can identify special classes of matrices for which the hypotheses
of Proposition 5.1 are fulﬁlled. In particular, we mention:
1. strictly diagonally dominant matrices.
A matrix is diagonally dominant by row if
|aii| ≥
n

j=1
j̸=i
|aij|,
i = 1, . . . , n,
by column if
|aii| ≥
n

j=1
j̸=i
|aji|,
i = 1, . . . , n.
When in the previous inequalities we can replace ≥by > we say
that matrix A is strictly diagonally dominant (by row or by column,

5.3 The LU factorization method
151
respectively). This deﬁnition holds also for any matrix A∈Cn×n (see
[GI04]);
2. real symmetric and positive deﬁnite matrices. A matrix A ∈Rn×n
is positive deﬁnite if
∀x ∈Rn with x ̸= 0,
xT Ax > 0
and semi positive deﬁnite if
∀x ∈Rn,
xT Ax ≥0;
3. complex deﬁnite positive matrices A ∈Cn×n, that is
∀x ∈Cn with x ̸= 0,
xHAx > 0;
note that these matrices are necessarily hermitian matrices (see
[Zha99, Sect. 3.2]).
If A∈Rn×n is symmetric and positive deﬁnite, it is moreover possible
to construct a special factorization:
A = RT R
(5.17)
where R is an upper triangular matrix with positive diagonal elements.
This is the so-called Cholesky factorization and requires about n3/3 op-
erations (half of those required by the LU factorization). Further, let us
note that, due to the symmetry, only the upper part of A is stored, and
R can be stored in the same area.
The elements of R can be computed by the following algorithm: we
set r11 = √a11 and, for j = 2, . . . , n, we set
rij = 1
rii
+
aij −
i−1

k=1
rkirkj
,
, i = 1, . . . , j −1
rjj =



ajj −
j−1

k=1
r2
kj
(5.18)
Cholesky factorization is available in MATLAB by setting R=chol
chol
(A). If we consider a complex positive deﬁnite matrix A∈Cn×n, formula
(5.17) becomes A=RHR, RH being the conjugate transpose matrix of
R.
Example 5.7 (Capillary networks) Consider problem 5.4 and change the
sign of the associated system (5.6). It turns out that −A is symmetric positive
deﬁnite, hence the system −Ap = −b can be solved by Cholesky factorization.
The corresponding solution is given by the following vector

152
5 Linear systems
0
20
40
60
80
100
120
0
20
40
60
80
100
120
nz = 379
0
20
40
60
80
100
120
0
20
40
60
80
100
120
nz = 4222
Figure 5.6. Pattern of matrices A and R of Example 5.7
p = [12.46, 3.07, 3.07, .73, .73, .73, .15, .15, .15, .15, .15, .15, .15, .15, .15]T .
Consequently, owing to relations (5.5), the following ﬂow-rates are found:
Q1
= 1.88
Q2,3
= 0.94
Q4,··· ,7
= 0.47
Q8,··· ,15 = 0.23
Q16,··· ,31 = 0.12.
Matrix A features a special banded structure, see, e.g., Figure 5.6 for an
instance corresponding to a capillary bed with 8 bifurcation levels. Colored
dots corresponds to non-null entries of A. On each row, there are at most 3
non-null entries. Besides, A is sparse (see at the end of this Example for the
deﬁnition of sparse matrices), as it features only 379 non-null entries over a
total of (127)2 = 16129 elements. The Cholesky factorization generates ﬁll-in
inside bands (see Sect. 5.4.1), as we can see from Figure 5.6 (right), where the
sparsity pattern of the upper triangular Cholesky factor R is shown. Reduction
of ﬁll-in is possibile provided suitable reordering algorithms are used on the
given matrix A. An example is given in Figure 5.7, where at left we display
the reordered matrix A (corresponding to the original matrix of Figure 5.6,
left) and at right the corresponding upper Cholesky factor R. For a discussion
about ordering techniques we refer the interested reader to [QSS07, Sect. 3.9].
■
A square matrix of size n is said sparse if the number of its nonzero
entries is of order n (therefore asymptotically less than the total number
n2 of the coeﬃcients of A). The pattern of a sparse matrix A is the
2D graphical representation of the positions of its nonnull entries. For
instance, the pattern of matrices A and R of Example 5.7 is shown in
Fig. 5.6; it is plotted by invoking the commands spy(A) and spy(R),
spy
respectively.
A matrix A ∈Rm×n (or in Cm×n) has lower band p if aij = 0 when
i > j + p and upper band q if aij = 0 when j > i + q. The maximum
between p and q is called the bandwidth of the matrix.

5.3 The LU factorization method
153
0
20
40
60
80
100
120
0
20
40
60
80
100
120
nz = 379
0
20
40
60
80
100
120
0
20
40
60
80
100
120
nz = 253
Figure 5.7. Pattern of matrices A and R of Example 5.7 after reordering
If A is a banded or a sparse large size matrix, only its non-zero
entries need to be stored. This can be conveniently done by means of the
MATLAB commands sparse or spdiags. For instance, the command
sparse
spdiags
A=sparse(n,m)
initializes to zero each entry of a sparse-array (a speciﬁc MATLAB
variable) of n rows and m columns.
The square matrix A such that
aii = 1
for i = 1, . . . , n,
a1j = 1
for j = 1, . . . , n,
ai1 = 1
for i = 1, . . . , n,
aij = 0
otherwise
(5.19)
with n = 25 can be deﬁned as follows:
n=25;
e=ones (n ,1);
A= spdiags(e,0,n,n);
A(1 ,:)=e’; A(: ,1)=e;
The command spdiags initializes a matrix with n rows and columns
according to the sparse-array format by positioning the column vector e
on the main diagonal (of index 0), then the following instructions (that
are valid for all kind of MATLAB arrays) update the ﬁrst row and the
ﬁrst column, respectively, of A.
When a system is solved by invoking the command \, MATLAB is
able to recognize the type of matrix and, in particular, whether it has
been stored as a sparse-array; consequently, MATLAB sorts out the
most appropriate solution algorithm as we will see in Sect. 5.8.
See Exercises 5.1-5.5.

154
5 Linear systems
5.4 The pivoting technique
We are going to introduce a special technique that allows us to achieve
the LU factorization for every nonsingular matrix, even when the as-
sumptions of Proposition 5.1 are not fulﬁlled.
Let us go back for a while to the case described in Example 5.6 and
take ε = 0. Setting A(1) = A after carrying out the ﬁrst step (k = 1) of
the procedure, the new entries of A are
⎡
⎣
1
1
3
2
0
-4
3
3
-5
⎤
⎦.
(5.20)
Since the pivot a22 is equal to zero, this procedure cannot be continued
further on. However, should we interchange the second and third rows
beforehand, we would obtain the matrix
⎡
⎣
1
1
3
3
3
-5
2
0
-4
⎤
⎦
and thus the factorization could be accomplished without involving a
division by 0.
We can state that a suitable row permutation of the original matrix
A would make the entire factorization procedure feasible even if the
hypotheses of Proposition 5.1 are not veriﬁed, under the sole condition
that det(A) ̸= 0. The decision on which row to permute can be made at
every step k at which a null diagonal element a(k)
kk is generated.
Let us return to matrix (5.20), whose (2,2) coeﬃcient is null. As the
element (3,2) is not null, we interchange the third and second row of this
matrix. By executing the second step of the factorization procedure we
ﬁnally ﬁnd the same matrix that we would have generated by an a priori
permutation of the same two rows of A.
Since a row permutation entails changing the pivot element, this tech-
nique is given the name of pivoting by row. The factorization generated in
this way returns the original matrix up to a row permutation. Precisely
we obtain
PA = LU
(5.21)
where P is a suitable permutation matrix that is initially set equal to
the identity matrix, then whenever in the course of the procedure two
rows of A are permuted, the same permutation must be performed on the
corresponding rows of P. At last, because of (5.21), we should sequentially
solve the following triangular systems
Ly = Pb,
Ux = y.
(5.22)

5.4 The pivoting technique
155
From the second equation of (5.13) we see that not only null pivot el-
ements a(k)
kk are troublesome, but so are those which are very small.
Indeed, should a(k)
kk be near zero, possible roundoﬀerrors aﬀecting the
coeﬃcients a(k)
kj will be severely ampliﬁed.
Example 5.8 Consider the nonsingular matrix
A =
⎡
⎣
1 1 + 0.5 · 10−15 3
2
2
20
3
6
4
⎤
⎦.
Although during the factorization procedure by Program 5.1 no null pivot
element is generated yet, the factors L and U turn out to be quite inaccurate,
as one can realize by computing the residual matrix A −LU (which should be
the null matrix if all operations were carried out in exact arithmetic):
A −LU =
⎡
⎣
0 0 0
0 0 0
0 0 4
⎤
⎦.
■
It is therefore recommended to carry out the pivoting at every step
of the factorization procedure, by searching among all virtual pivot ele-
ments a(k)
ik with i = k, . . . , n, the one with maximum modulus (see Fig.
5.8, left). The algorithm (5.13) with pivoting by row carried out at each
step takes the following form: set A(1) = A and P=I, then:
for k = 1, . . . , n −1,
ﬁnd ¯r such that |a(k)
¯rk | =
max
r=k,...,n|a(k)
rk |,
exchange row k with row ¯r
in both A and P,
for i = k + 1, . . . , n
lik = a(k)
ik
a(k)
kk
,
for j = k + 1, . . . , n
a(k+1)
ij
= a(k)
ij −lika(k)
kj
(5.23)
As already noticed for algorithm (5.13) (the one without permuta-
tions), a single matrix is suﬃcient to store either the entries (a(k)
ij ) and
the multipliers (lik). Consequently, for any k, the same permutation car-
ried out on both A and P acts on the multipliers, too.
The MATLAB program lu previously mentioned computes the
LU factorization with pivoting by row. Its complete syntax is indeed

156
5 Linear systems
k
k
k
k
r
r
q
0
0
Figure 5.8. Partial pivoting by rows (at left) and total pivoting (at right).
The submatrix where to seek the pivot at step k is coloured by a dark blue
[L,U,P]=lu(A), P being the permutation matrix. When called in the
shorthand mode [L,U]=lu(A), the matrix L is equal to P*M, where M is
lower triangular and P is the permutation matrix generated by the piv-
oting by row. The program lu automatically operates pivoting by row.
In particular, when A has a sparse storage organization (see Sections
5.6 and 5.8), permutation by rows is performed only when a null (or
exceedingly small) pivot element is encountered.
Total pivoting consists of searching the pivot within the whole A(k)
submatrix made by the elements a(k)
ij , i, j = k, . . . , n (see Fig. 5.8, right).
It involves both the rows and the columns of the matrix and yields two
permutation matrices, P on the rows and Q on the columns, such that
PAQ = LU
(5.24)
As
Ax = b ⇔PAQ
& '( )
LU
Q−1x
& '( )
x∗
= Pb,
the solution of system Ax = b is then obtained by solving two triangular
systems and ﬁnally operating a permuation on the vector components
as follows
Ly = Pb
Ux∗= y
x = Qx∗
(5.25)
The MATLAB command [L,U,P,Q]=lu(A) implements the total
pivoting on an input matrix featuring a sparse-array format (hence gen-
erated by the sparse or spdiags commands).
Total pivoting is computationally more expensive than partial pivot-
ing as many more comparisons need to be carried out at every step of
the factorization. However it can save storage and enhance the algorithm
stability as we will see in the next sections.

5.4 The pivoting technique
157
0
10
20
0
10
20
nz = 73
A
0
10
20
0
10
20
nz = 324
L
0
10
20
0
10
20
nz = 324
U
0
10
20
0
10
20
nz = 25
P
Figure 5.9. Fill-in of matrix A deﬁned in (5.19)
0
10
20
0
5
10
15
20
nz = 67
A
0
10
20
0
5
10
15
20
nz = 103
L
0
10
20
0
5
10
15
20
nz = 173
U
0
10
20
0
5
10
15
20
nz = 20
P
Figure 5.10. Fill-in for a matrix A whose proﬁle is indicated in the ﬁrst
ﬁgure on the left
5.4.1 The ﬁll-in of a matrix
In general, the LU factorization process does not preserve the sparsity
pattern of the original matrix A, as it may generate non-null elements
in L and U in some positions (i, j) where the corresponding original
elements aij were null. This phenomenon is called ﬁll-in and depends on
both the original pattern of A and the values of its entries.
Examples of ﬁll-in were already encountered in Figure 5.6 (referring
to Example 5.7), while Figure 5.9 refers to the ﬁll-in for the case of matrix
(5.19). Another example is shown in Figure 5.10: in this case the non-null
elements sitting on the ﬁrst row and ﬁrst column of A induce a complete
ﬁll-in of the corresponding columns of U and rows of L, respectively.
Moreover, the non-null elements lying on the upper and lower diagonals
of A yield a ﬁll-in of the upper diagonals of U and the lower diagonals
of L included between the main diagonal and those non-null of A.
The ﬁll-in can be avoided by operating a suitable reordering (through
row and column pemutations) of the matrix A before operating its fac-
torization. In many cases, however, the sole total pivoting allows the
achievement of the same result. An example is illustrated in Figure 5.11
which refers to the case of matrix A deﬁned in (5.19). We can appreciate
that no ﬁll-in is induced on the factors L and U, at the cost however of

158
5 Linear systems
0
10
20
0
10
20
nz = 49
L
0
10
20
0
10
20
nz = 49
U
0
10
20
0
10
20
nz = 25
P
0
10
20
0
10
20
nz = 25
Q
Figure 5.11. Matrices L, U, P and Q of the total pivoting factorization of A
deﬁned in (5.19)
0
10
20
0
5
10
15
20
nz = 54
L
0
10
20
0
5
10
15
20
nz = 47
U
0
10
20
0
5
10
15
20
nz = 20
P
0
10
20
0
5
10
15
20
nz = 20
Q
Figure 5.12. Matrices L, U, P and Q of the total pivoting factorization of
the matrix A given in Figure 5.10, left
reordering the rows and columns of A, as we can grasp by inspecting the
pattern of the permutation matrices P and Q.
In Figure 5.12 we report the matrices L, U, P, and Q obtained by
the factorization of the matrix A of Figure 5.10. The ﬁll-in generated by
the total pivoting is much lighter than the one that would be produced
if using partial pivoting by rows.
See Exercises 5.6-5.8.
5.5 How accurate is the solution of a linear system?
We have already noticed in Example 5.8 that, due to roundoﬀerrors,
the product LU does not reproduce A exactly. However, in general the
pivoting allows keeping these errors under control and therefore achieving
accurate solutions. (From the theoretical point of view, total pivoting is
more stable than partial pivoting by rows, nevertheless experience shows
that in general the latter provides accurate results by itself (see, e.g.,
[Hig02, Sez. 9.3]).)
Unfortunately, this is not always true, as the following example shows.

5.5 How accurate is the solution of a linear system?
159
0
20
40
60
80
100
10
−20
10
−15
10
−10
10
−5
10
0
10
5
Figure 5.13. Behavior versus n of En (solid line) and of maxi,j=1,...,n |rij|
(dashed line) in logarithmic scale, for the Hilbert system of Example 5.9. The
rij are the coeﬃcients of the matrix Rn
Example 5.9 Consider the linear system Anxn = bn, where An ∈Rn×n is
the so-called Hilbert matrix whose elements are
aij = 1/(i + j −1),
i, j = 1, . . . , n,
while bn is chosen in such a way that the exact solution is xn = (1, 1, . . . , 1)T .
The matrix An is clearly symmetric and one can prove that it is also positive
deﬁnite. For diﬀerent values of n we use the MATLAB function lu to get the
LU factorization of An with pivoting by row. Then we solve the associated
linear systems (5.22) and denote by xn the computed solution. In Figure 5.13
we report (in logarithmic scale) the relative errors
En = ∥xn −xn∥/∥xn∥,
(5.26)
having denoted by ∥· ∥the Euclidean norm introduced in the Section 1.4.1.
We have En ≥10 if n ≥13 (that is a relative error on the solution higher
than 1000%!), whereas Rn = LnUn −PnAn is the null matrix (up to machine
accuracy) for any given value of n. Similar results are obtained by using total
pivoting.
■
On the ground of the previous remark, we could speculate by saying
that, when a linear system Ax = b is solved numerically, one is indeed
looking for the exact solution x of a perturbed system
(A + δA)x = b + δb,
(5.27)
where δA and δb are respectively a matrix and a vector which depend on
the speciﬁc numerical method which is being used. We start by consid-
ering the case where δA = 0 and δb ̸= 0 which is simpler than the most
general case. Moreover, for simplicity we will also assume that A∈Rn×n
is symmetric and positive deﬁnite.
By comparing (5.1) and (5.27) we ﬁnd x −x = −A−1δb, and thus

160
5 Linear systems
∥x −x∥= ∥A−1δb∥.
(5.28)
In order to ﬁnd an upper bound for the right-hand side of (5.28), we
proceed as follows. Since A is symmetric and positive deﬁnite, the set of
its eigenvectors {vi}n
i=1 provides an orthonormal basis of Rn (see [QSS07,
Chapter 5]). This means that
Avi = λivi, i = 1, . . . , n,
vT
i vj = δij, i, j = 1, . . . , n,
where λi is the eigenvalue of A associated with vi and δij is the Kronecker
symbol. Consequently, a generic vector w ∈Rn can be written as
w =
n

i=1
wivi,
for a suitable (and unique) set of coeﬃcients wi ∈R. We have
∥Aw∥2 = (Aw)T (Aw)
= [w1(Av1)T + . . . + wn(Avn)T ][w1Av1 + . . . + wnAvn]
= (λ1w1vT
1 + . . . + λnwnvT
n )(λ1w1v1 + . . . + λnwnvn)
=
n

i=1
λ2
i w2
i .
Denote by λmax the largest eigenvalue of A. Since ∥w∥2 = *n
i=1 w2
i , we
conclude that
∥Aw∥≤λmax∥w∥
∀w ∈Rn.
(5.29)
In a similar manner, we obtain
∥A−1w∥≤
1
λmin
∥w∥,
upon recalling that the eigenvalues of A−1 are the reciprocals of those
of A. This inequality enables us to draw from (5.28) that
∥x −x∥
∥x∥
≤
1
λmin
∥δb∥
∥x∥.
(5.30)
Using (5.29) once more and recalling that Ax = b, we ﬁnally obtain
∥x −x∥
∥x∥
≤λmax
λmin
∥δb∥
∥b∥
(5.31)
We can conclude that the relative error in the solution depends on
the relative error in the data through the following constant (≥1)

5.5 How accurate is the solution of a linear system?
161
K(A) = λmax
λmin
(5.32)
which is called spectral condition number of the matrix A. K(A) can be
computed in MATLAB using the command cond.
cond
Remark 5.4 The MATLAB command cond(A) allows the computation of
the condition number of any type of matrix A, even those which are not sym-
metric and positive deﬁnite. It is worth mentioning that there exist various
deﬁnitions of condition number of a matrix. For a generic matrix A, the com-
mand cond(A) computes the value K2(A) = ∥A∥2 · ∥A−1∥2, where we deﬁne
∥A∥2 =

λmax(AT A). We note that if A is not symmetric and positive def-
inite, K2(A) can be very far from the spectral condition number K(A). For
a sparse matrix A, the command condest(A) computes an approximation (at
condest
low computational cost) of the condition number K1(A) = ∥A∥1 · ∥A−1∥1,
being ∥A∥1 = maxj
n
i=1 |aij| the so-called 1-norm of A. Other deﬁnitions for
the condition number are available for nonsymmetric matrices, see [QSS07,
Chapter 3].
■
A more involved proof would lead to the following more general re-
sult in the case where A is symmetric and positive deﬁnite and δA is
an arbitrary symmetric and positive deﬁnite matrix, “small enough” to
satisfy λmax(δA) < λmin(A):
∥x −x∥
∥x∥
≤
K(A)
1 −λmax(δA)/λmin(A)
λmax(δA)
λmax(A) + ∥δb∥
∥b∥

(5.33)
Finally, if A and δA are not symmetric positive deﬁnite matrices, and
δA is such that ∥δA∥2 ∥A−1∥2 < 1, the following estimate holds:
∥x −x∥
∥x∥
≤
K2(A)
1 −K2(A)∥δA∥2/∥A∥2
∥δA∥2
∥A∥2
+ ∥δb∥
∥b∥

(5.34)
If K(A) is “small”, that is of the order of unity, A is said to be well
conditioned. In that case, small errors in the data will lead to errors of
the same order of magnitude in the solution. This would not occur in
the case of ill conditioned matrices.
Example 5.10 For the Hilbert matrix introduced in Example 5.9, K(An) is
a rapidly increasing function of n. One has K(A4) > 15000, while if n > 13 the
condition number is so high that MATLAB warns that the matrix is “close to
singular”. Actually, K(An) grows at an exponential rate, K(An) ≃e3.5n (see,
[Hig02]). This provides an indirect explanation of the bad results obtained in
Example 5.9.
■

162
5 Linear systems
Inequality (5.31) can be reformulated by the help of the residual r
r = b −Ax.
(5.35)
Should x be the exact solution, the residual would be the null vector.
Thus, in general, r can be regarded as an estimator of the error x −x.
The extent to which the residual is a good error estimator depends on
the size of the condition number of A. Indeed, observing that δb =
A(x −x) = Ax −b = −r, we deduce from (5.31) that
∥x −x∥
∥x∥
≤K(A) ∥r∥
∥b∥
(5.36)
Thus if K(A) is “small”, we can be sure that the error is small pro-
vided that the residual is small, whereas this might not be true when
K(A) is “large”.
Example 5.11 The residuals associated with the computed solution of the
linear systems of Example 5.9 are very small (their norms vary between 10−16
and 10−11); however the computed solutions diﬀer remarkably from the exact
solution.
■
See Exercises 5.9-5.10.
5.6 How to solve a tridiagonal system
In many applications (see for instance Chapter 9), we have to solve a
system whose matrix has the form
A =
⎡
⎢⎢⎢⎢⎣
a1 c1
0
e2 a2
...
...
cn−1
0
en
an
⎤
⎥⎥⎥⎥⎦
.
This matrix is called tridiagonal since the only elements that can be
non-null belong to the main diagonal and to the ﬁrst super and sub
diagonals.
If the LU factorization of A exists, the factors L and U must be
bidiagonals (lower and upper, respectively), more precisely:
L =
⎡
⎢⎢⎢⎣
1
0
β2 1
... ...
0
βn 1
⎤
⎥⎥⎥⎦,
U =
⎡
⎢⎢⎢⎢⎣
α1 c1
0
α2
...
... cn−1
0
αn
⎤
⎥⎥⎥⎥⎦
.

5.7 Overdetermined systems
163
The unknown coeﬃcients αi and βi can be determined by requiring that
the equality LU = A holds. This yields the following recursive relations
for the computation of the L and U factors:
α1 = a1,
βi =
ei
αi−1
,
αi = ai −βici−1,
i = 2, . . . , n.
(5.37)
Using (5.37), we can easily solve the two bidiagonal systems Ly = b and
Ux = y, to obtain the following formulae:
(Ly = b)
y1 = b1,
yi = bi −βiyi−1,
i = 2, . . . , n,
(5.38)
(Ux = y)
xn = yn
αn
, xi = (yi −cixi+1) /αi, i = n −1, . . . , 1.
(5.39)
This is known as the Thomas algorithm and allows the solution of the
original system with a computational cost of the order of n operations.
The MATLAB command spdiags allows the construction of a tridi-
agonal matrix by storing only the non-null diagonals. For instance, the
commands
b=ones (10 ,1);
a=2*b;
c=3*b;
T= spdiags ([b a c] , -1:1 ,10 ,10);
compute the tridiagonal matrix T ∈R10×10 with elements equal to 2 on
the main diagonal, 1 on the ﬁrst subdiagonal and 3 on the ﬁrst super-
diagonal.
Note that T is stored in a sparse mode, according to which the only
elements stored are those diﬀerent than 0. When A is a tridiagonal ma-
trix generated in sparse mode, the Thomas algorithm is the solution
algorithm selected by the MATLAB command \. (See also Section 5.8
for a more general discussion on the MATLAB command \.)
5.7 Overdetermined systems
A linear system Ax=b with A∈Rm×n is called overdetermined if m > n,
underdetermined if m < n.
An overdetermined system generally has no solution unless the right
hand side vector b is an element of range(A), where
range(A) = {z ∈Rm : z = Ay for y ∈Rn}.
(5.40)
In general, for an arbitrary b we can search a vector x∗∈Rn that
minimizes the Euclidean norm of the residual, that is,

164
5 Linear systems
Φ(x∗) = ∥Ax∗−b∥2
2 ≤∥Ay −b∥2
2 = Φ(y)
∀y ∈Rn.
(5.41)
When it does exist, the vector x∗is called least-squares solution of
the overdetermined system Ax=b.
Similarly to what was done in Section 3.6, the solution of (5.41) can
be found by imposing the condition that the gradient of the function Φ
must be equal to zero at x∗. With similar calculations we ﬁnd that x∗is
in fact the solution of the square n × n linear system
AT Ax∗= AT b
(5.42)
which is called the system of normal equations. The system (5.42) is non-
singular if A has full rank (that is rank(A) = min(m,n), where the rank
of A, rank(A), is the maximum order of the nonvanishing determinants
extracted from A). In such a case B = AT A is a symmetric and positive
deﬁnite matrix, then the least-squares solution exists and is unique.
To compute it one could use the Cholesky factorization (5.17) ap-
plied to the matrix B. However, due to roundoﬀerrors, the computation
of AT A may be aﬀected by a loss of signiﬁcant digits, with a conse-
quent loss of the positive deﬁniteness of the matrix itself. Instead, it is
more convenient to use either the so-called QR factorization of A, or the
Singular Value Decomposition (SVD) of A.
Let us start from the former. Any full rank matrix A ∈Rm×n, with
m ≥n, admits a unique QR factorization
A = QR
(5.43)
Q ∈Rm×m is an orthogonal matrix (i.e. QT Q = I), while R ∈Rm×n is a
rectangular matrix whose entries below the main diagonal are equal to
zero, whereas all its diagonal entries are non-null. See Figure 5.14.
It is possible to prove that A = -Q-R, where -Q = Q(1 : m, 1 : n)
and -R = R(1 : n, 1 : n) are the submatrices indicated in Figure 5.14. -Q
has orthonormal column vectors, while -R is an upper triangular matrix,
which in fact coincides with the triangular factor R of Cholesky factor-
ization of the matrix AT A. Since -R is non-singular, the unique solution
of (5.43) reads
x∗= ˜R−1 ˜QT b.
(5.44)
Now let us turn to the singular value decomposition of a matrix:
for any given rectangular matrix A ∈Cm×n, there exist two unitary
matrices U ∈Cm×m and V ∈Cn×n, such that
UHAV = Σ = diag(σ1, . . . , σp) ∈Rm×n
(5.45)

5.7 Overdetermined systems
165
A
Q
R
0
0
n
n
n
n
m
m −n
m −n
Q
R
Figure 5.14. The QR factorization
where
p = min(m, n) and σ1 ≥. . . ≥σp ≥0. A matrix U is said
unitary if UHU = UUH = I. Formula (5.45) is named singular value
decomposition (SVD in short) of A and the entries σi of Σ are named
singular values of A. It holds that σi =

λi(AHA), while λi(AHA) are
the real positive eigenvalues of the matrix AHA.
If A is a real matrix, then also U and V are real matrices. Moreover,
U and V are orthogonal matrices and UH coincides with UT .
Let us now compute the singular value decomposition (5.45) of the
matrix A in (5.42). Since U is orthogonal, AT A = VΣT ΣV T , hence the
system of normal equations (5.42) is equivalent to the system
VΣT ΣVT x∗= VΣT UT b.
(5.46)
We note that also V is orthogonal and that ΣT Σ is a square non-singular
matrix whose diagonal entries are the square of the singular values of A.
Therefore, by a left multiplication of equation (5.46) by V(ΣT Σ)−1V T
we have
x∗= VΣ†U T b = A†b,
(5.47)
where Σ† = (ΣT Σ)−1ΣT = diag(1/σ1, . . . , 1/σn, 0, . . . , 0) and A† =
(AT A)−1AT = VΣ†U T . The latter matrix is called pseudoinverse of A.
We deduce from formula (5.47) that after computing the singular
values of A and the matrices U and V, by a small additional eﬀort we
can ﬁnd the solution of the normal equations (5.42).
Two functions are available in MATLAB for the computation of the
SVD of a given matrix: svd and svds. The former computes all singular
svd
svds
values of A, the latter only the largest k singular values, where k is a
parameter given in input (the default value is k=6). We refer to [ABB+99]
for an exhaustive description of algorithms used in MATLAB.
Example 5.12 Consider an alternative approach to the problem of ﬁnding
the regression line ϵ(σ) = a1σ + a0 (see Section 3.6) of the data of Problem
3.3. Using the data of Table 3.2 and imposing the interpolating conditions we
obtain the overdetermined system Aa = b, where a = (a1, a0)T and

166
5 Linear systems
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
1
0.06
1
0.14
1
0.25
1
0.31
1
0.47
1
0.60
1
0.70
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
b =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
0.08
0.14
0.20
0.23
0.25
0.28
0.29
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
In order to compute its least-squares solution we use the following instructions
[Q,R]=qr(A);
Qt=Q(: ,1:2);
Rt=R(1:2 ,:);
xstar = Rt \ (Qt ’*b)
xstar =
0.3741
0.0654
These are precisely the same coeﬃcients for the regression line computed in
the Example 3.13. Notice that this procedure is directly implemented in the
command \: in fact, the instruction xstar = A\b produces the same xstar
vector, computed by the formulae (5.43) and (5.44).
■
5.8 What is hidden behind the MATLAB command \
It is useful to know that the speciﬁc algorithm used by MATLAB when
the \ command is invoked depends upon the structure of the matrix A.
To determine the structure of A and select the appropriate algorithm,
MATLAB follows this precedence:
1. if A is sparse and banded, then banded solvers are used (like the
Thomas algorithm of Section 5.6);
2. if A is an upper or lower triangular matrix (or else a permutation
of a triangular matrix), then the system is solved by a backward
substitution algorithm for upper triangular matrices, or by a forward
substitution algorithm for lower triangular matrices. The check for
triangularity is done for full matrices by testing for zero elements
and for sparse matrices by accessing the sparse data structure;
3. if A is symmetric and has real positive diagonal elements (which does
not imply that A is positive deﬁnite), then a Cholesky factorization
is attempted (chol). If A is sparse, a preordering algorithm is applied
ﬁrst;
4. if none of previous criteria are fulﬁlled, then a general triangular fac-
torization is computed by Gaussian elimination with partial pivoting
(lu);
5. if A is sparse, then the UMFPACK library (which is part of the Suite-
sparse suite, see e.g. http://www.cise.ufl.edu/research/sparse
/SuiteSparse/) is used to compute the solution of the system;

5.8 What is hidden behind the MATLAB command \
167
6. if A is not square, proper methods based on the QR factorization
for undetermined systems are used (for the overdetermined case, see
Section 5.7).
The command \ is also available in Octave. On a system with dense
matrix, Octave operates as follows:
1. if the matrix is upper (resp., lower) triangular, Octave call backward
(resp., forward) substitutions of LAPACK (a widely used library of
linear algebra routines [ABB+99]);
2. if the matrix is symmetric and has real positive diagonal entries,
Octave attempts a Cholesky factorization by LAPACK;
3. if either the Cholesky factorization fails or the matrix is not symmet-
ric with positive diagonal entries, the system is solved by Gaussian
elimination with pivoting by rows by LAPACK;
4. if the matrix is not square, or any of the previous solvers ﬂags a
singular or near singular matrix, Octave looks for a solution in the
least-squares sense.
For a linear system with sparse matrix, like MATLAB Octave relies
on UMFPACK and other packages from the Suitesparse collection, in
particular:
1. for a square, banded matrix with “small enough” band density con-
tinue to a), else goto 2;
a) if the matrix is tridiagonal and the right-hand side is not sparse
continue, else goto b);
i. if the matrix is symmetric with positive diagonal entries,
Octave attempts a Cholesky factorization;
ii. if the above failed or the matrix is not symmetric with posi-
tive diagonal entries, then it uses Gaussian elimination with
pivoting;
b) if the matrix is symmetric with positive diagonal entries, Octave
attempts a Cholesky factorization;
c) if the above failed or the matrix is not symmetric with positive
diagonal entries, Octave uses Gaussian elimination with pivoting;
2. if the matrix is upper (with column permutations) or lower (with
row permutations) triangular, perform a sparse forward or backward
substitution;
3. if the matrix is square, symmetric with positive diagonal entries,
Octave attempts sparse Cholesky factorization;
4. if the sparse Cholesky factorization failed or the matrix is not sym-
metric with positive diagonal entries, Octave factorizes the matrix
using the UMFPACK library;
5. if the matrix is not square, or any of the previous solvers ﬂags a
singular or near singular matrix, Octave provides a solution in the
least-squares sense.

168
5 Linear systems
Let us summarize
1. The LU factorization of A∈Rn×n consists in computing a lower
triangular matrix L and an upper triangular matrix U such that
A = LU;
2. the LU factorization, provided it exists, is not unique. However, it can
be determined unequivocally by providing an additional condition
such as, e.g., setting the diagonal elements of L equal to 1. This is
called LU factorization;
3. the LU factorization exists and is unique if and only if the principal
submatrices of A of order 1 to n −1 are nonsingular (otherwise at
least one pivot element is null);
4. if a null pivot element is generated, a new pivot element can be
obtained by exchanging in a suitable manner two rows (or columns)
of our system. This is the pivoting strategy;
5. the computation of the LU factorization requires about 2n3/3 oper-
ations, and only an order of n operations in the case of tridiagonal
systems;
6. for symmetric and positive deﬁnite matrices we can use the Cholesky
factorization A = RT R, where R is an upper triangular matrix, and
the computational cost is of the order of n3/3 operations;
7. the sensitivity of the result to perturbation of data depends on the
condition number of the system matrix; more precisely, the accuracy
of the computed solution can be low for ill conditioned matrices;
8. the solution of an overdetermined linear system can be intended
in the least-squares sense and can be computed using either QR
factorization or singular value decomposition (SVD).
5.9 Iterative methods
Let us consider the linear system (5.1) with A∈Rn×n and b ∈Rn. An
iterative method for the solution of (5.1) consists in setting up a sequence
of vectors {x(k), k ≥0} of Rn that converges to the exact solution x, that
is
lim
k→∞x(k) = x,
(5.48)
for any given initial vector x(0) ∈Rn. A possible strategy able to realize
this process can be based on the following recursive deﬁnition
x(k+1) = Bx(k) + g,
k ≥0,
(5.49)
where B is a suitable matrix (depending on A) and g is a suitable vector
(depending on A and b), which must satisfy the consistency relation

5.9 Iterative methods
169
x = Bx + g.
(5.50)
Since x = A−1b this yields g = (I −B)A−1b.
Let e(k) = x −x(k) deﬁne the error at step k. By subtracting (5.49)
from (5.50), we obtain
e(k+1) = Be(k).
For this reason B is called the iteration matrix associated with (5.49). If
B is symmetric and positive deﬁnite, by (5.29) we have
∥e(k+1)∥= ∥Be(k)∥≤ρ(B)∥e(k)∥,
k ≥0.
We have denoted by ρ(B) the spectral radius of B, that is, the maxi-
mum modulus of eigenvalues of B. If B is a symmetric positive deﬁnite
matrix, then ρ(B) coincides with the largest eigenvalue of B. By iterating
the same inequality backward, we obtain
∥e(k)∥≤[ρ(B)]k∥e(0)∥,
k ≥0.
(5.51)
Thus e(k) →0 as k →∞for every possible e(0) (and henceforth x(0))
provided that ρ(B) < 1. Therefore, the method converges. Actually, this
property is also necessary for convergence.
Should, by any chance, an approximate value of ρ(B) be available,
(5.51) would allow us to deduce the minimum number of iterations kmin
that are needed to damp the initial error by a factor ε. Indeed, kmin
would be the lowest positive integer for which [ρ(B)]kmin ≤ε.
In conclusion, for a generic matrix the following result holds:
Proposition 5.2 For an iterative method of the form (5.49) whose
iteration matrix satisﬁes (5.50), convergence for any x(0) holds iﬀ
ρ(B) < 1. Moreover, the smaller ρ(B), the fewer the number of iter-
ations necessary to reduce the initial error by a given factor.
5.9.1 How to construct an iterative method
A general technique to devise an iterative method is based on a splitting
of the matrix A, A = P−(P−A), being P a suitable nonsingular matrix
(called the preconditioner of A). Then
Px = (P −A)x + b,
has the form (5.50) provided that we set B = P−1(P −A) = I −P−1A
and g = P−1b. Correspondingly, we can deﬁne the following iterative
method

170
5 Linear systems
P(x(k+1) −x(k)) = r(k),
k ≥0,
where
r(k) = b −Ax(k)
(5.52)
denotes the residual vector at iteration k. A generalization of this itera-
tive method is the following
P(x(k+1) −x(k)) = αkr(k),
k ≥0
(5.53)
where αk ̸= 0 is a parameter that may change at every iteration k and
which, a priori, will be useful to improve the convergence properties of
the sequence {x(k)}.
The method (5.53) requires to ﬁnd at each step the so-called precon-
ditioned residual z(k) which is the solution of the linear system
Pz(k) = r(k),
(5.54)
then the new iterate is deﬁned by x(k+1) = x(k) +αkz(k). For that reason
the matrix P ought to be chosen in such a way that the computational
cost for the solution of (5.54) be quite low (e.g., every P either diagonal
or triangular or tridiagonal will serve the purpose). Let us now consider
some special instance of iterative methods which take the form (5.53).
The Jacobi method
If the diagonal entries of A are nonzero, we can set P = D = diag(a11, a22,
. . . , ann), that is D is the diagonal matrix containing the diagonal entries
of A. The Jacobi method corresponds to this choice with the assumption
αk = 1 for all k. Then from (5.53) we obtain
Dx(k+1) = b −(A −D)x(k),
k ≥0,
or, componentwise,
x(k+1)
i
= 1
aii
⎛
⎝bi −
n

j=1,j̸=i
aijx(k)
j
⎞
⎠, i = 1, . . . , n
(5.55)
where k ≥0 and x(0) = (x(0)
1 , x(0)
2 , . . . , x(0)
n )T is the initial vector.
The iteration matrix is therefore
B = D−1(D −A) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0
−a12/a11 . . . −a1n/a11
−a21/a22
0
−a2n/a22
...
...
...
−an1/ann −an2/ann . . .
0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
(5.56)

5.9 Iterative methods
171
The following result allows the veriﬁcation of Proposition 5.2 without
explicitly computing ρ(B):
Proposition 5.3 If the matrix A∈Rn×n of system (5.1) is strictly
diagonally dominant by row, then the Jacobi method converges.
As a matter of fact, we can verify that ρ(B) < 1, where B is given
in (5.56), that is, all eigenvalues of B are in modulus less than 1. To
start with, we note that the diagonal elements of A are non-null owing
to the strict diagonal dominance (see Section 6.4). Let λ be a generic
eigenvalue of B and x an associated eigenvector. Then
n

j=1
bijxj = λxi, i = 1, . . . , n.
Assume for simplicity that maxk=1,...,n |xk| = 1 (this is not restrictive
since an eigenvector is deﬁned up to a multiplicative constant) and let
xi be the component whose modulus is equal to 1. Then
|λ| =

n

j=1
bijxj

=

n

j=1,j̸=i
bijxj

≤
n

j=1,j̸=i

aij
aii
 ,
having noticed that B has only null diagonal elements. Therefore |λ| < 1
thanks to the assumption made on A.
The Jacobi method is implemented in the Program 5.2 setting in the
input parameter P=’J’. Other input parameters are: the system matrix
A, the right hand side b, the initial vector x0, the maximum number
of iterations allowed, nmax and a given tolerance tol for stopping test.
The iterative procedure is terminated as soon as the ratio between the
Euclidean norm of the current residual and that of the initial residual
is less than or equal to tol (for a justiﬁcation of this stopping criterion,
see Section 5.12).
Program 5.2. itermeth: general iterative method
function [x,iter ]= itermeth (A,b,x0 ,nmax ,tol ,P)
% ITERMETH
General
iterative
method
% X = ITERMETH (A,B,X0 ,NMAX ,TOL ,P) attempts
to solve the
% system of linear
equations
A*X=B for X. The N-by -N
% coefficient
matrix A must
be non -singular
and the
% right hand
side
column vector B must
have
length
% N. If P=’J’ the
Jacobi method is used , if P=’G’ the
% Gauss -Seidel method is selected . Otherwise , P is a
% N-by -N matrix
that
plays the
role of a preconditioner
% for the gradient
method , which is a dynamic
% Richardson
method. Iterations
% stop
when
the
ratio between
the norm
of the kth

172
5 Linear systems
% residual
and the
norm of the
initial
residual
is
less
% than TOL , then
ITER is the
number of performed
% iterations . NMAX
specifies
the maximum
% number of
iterations . If P is not defined , the
% unpreconditioned
gradient
method is performed .
[n,n]= size (A);
if nargin == 6
if ischar(P)==1
if P==’J’
L=diag (diag(A)); U=eye(n); beta =1;
alpha =1;
elseif P == ’G’
L=tril (A); U=eye(n); beta =1;
alpha =1;
end
else
[L,U]= lu(P); beta = 0;
end
else
L = eye(n); U = L; beta = 0;
end
iter =0; x=x0; r=b-A*x0;
r0=norm (r);
err=r0;
while err > tol & iter < nmax
z = L\r;
z = U\z; iter = iter + 1;
if beta == 0
alpha = z’*r/(z’*A*z);
end
x = x + alpha*z;
r = b - A * x;
err = norm (r) / r0;
end
The Gauss-Seidel method
When applying the Jacobi method, each component x(k+1)
i
of the new
vector x(k+1) is computed independently of the others. This may sug-
gest that a faster convergence could be (hopefully) achieved if the new
components already available x(k+1)
j
, j = 1, . . . , i −1, together with the
old ones x(k)
j , j ≥i, are used for the calculation of x(k+1)
i
. This would
lead to modifying (5.55) as follows: for k ≥0 (still assuming that aii ̸= 0
for i = 1, . . . , n)
x(k+1)
i
= 1
aii
⎛
⎝bi −
i−1

j=1
aijx(k+1)
j
−
n

j=i+1
aijx(k)
j
⎞
⎠, i = 1, .., n
(5.57)
The updating of the components is made in sequential mode, whereas
in the original Jacobi method it is made simultaneously (or in parallel).
The new method, which is called the Gauss-Seidel method, corresponds
to the choice P = D −E and αk = 1, k ≥0, in (5.53), where E is a lower
triangular matrix whose non null entries are eij = −aij, i = 2, . . . , n,
j = 1, . . . , i −1. The corresponding iteration matrix is then
B = (D −E)−1(D −E −A).

5.9 Iterative methods
173
A possible generalization is the so-called in which P = 1
ωD−E, where
ω ̸= 0 is the relaxation parameter, and αk = 1, k ≥0 (see Exercise 5.13).
Also for the Gauss-Seidel method there exist special matrices A whose
associated iteration matrices satisfy the assumptions of Proposition 5.2
(those guaranteeing convergence). Among them let us mention:
1. matrices which are strictly diagonally dominant by row;
2. matrices which are real symmetric and positive deﬁnite.
The Gauss-Seidel method is implemented in Program 5.2 setting the
input parameter P equal to ’G’.
There are no general results stating that the Gauss-Seidel method
always converges faster than Jacobi’s. However, in some special instances
this is the case, as stated by the following proposition (see, e.g. [Saa03,
Thm. 4.7]):
Proposition 5.4 Let A∈Rn×n be a tridiagonal nonsingular matrix
whose diagonal elements are all non-null. Then the Jacobi method
and the Gauss-Seidel method are either both divergent or both con-
vergent. In the latter case, the Gauss-Seidel method is faster than
Jacobi’s; more precisely the spectral radius of its iteration matrix is
equal to the square of that of Jacobi.
Example 5.13 Let us consider a linear system Ax = b, where b is chosen in
such a way that the solution is the unit vector (1, 1, . . . , 1)T and A is the 10×10
tridiagonal matrix whose diagonal entries are all equal to 3, the entries of the
ﬁrst lower diagonal are equal to −2 and those of the upper diagonal are all equal
to −1. Both Jacobi and Gauss-Seidel methods converge since the spectral radii
of their iteration matrices are strictly less than 1. More precisely, by starting
from a null initial vector and setting tol =10−12, the Jacobi method converges
in 277 iterations while only 143 iterations are requested from Gauss-Seidel’s.
To get this result we have used the following instructions:
n=10;
A=3* eye(n)-2* diag (ones (n-1,1),1)- diag (ones(n-1,1),-1);
b=A*ones (n ,1);
x0=zeros(n ,1);
[x,iterJ]= itermeth (A,b,x0 ,400 ,1.e-12, ’J’); iterJ
[x,iterG]= itermeth (A,b,x0 ,400 ,1.e-12, ’G’); iterG
iterJ =
277
iterG =
143
■
See Exercises 5.11-5.14.

174
5 Linear systems
5.10 Richardson and gradient methods
Let us now reconsider a method that can be set in the general form
(5.53). We call stationary the case when αk = α (a given constant)
for any k ≥0, dynamic the case in which αk may change along the
iterations. In this framework the nonsingular matrix P is still called a
preconditioner of A.
The crucial issue is the way the parameters are chosen. In this respect,
the following results hold (see, e.g., [QV94, Chapter 2], [Axe94]).
Proposition 5.5 Let A∈Rn×n. For any non-singular matrix P ∈
Rn×n the stationary Richardson method converges iﬀ
|λi|2 < 2
αReλi
∀i = 1, . . . , n,
where λi are the eigenvalues of P−1A. If the latter are all real, then
it converges iﬀ
0 < αλi < 2
∀i = 1, . . . , n.
If both A and P are symmetric and positive deﬁnite matrices, the
stationary Richardson method converges for any possible choice of
x(0) iﬀ0 < α < 2/λmax, where λmax(> 0) is the maximum eigen-
values of P−1A. Moreover, the spectral radius ρ(Bα) of the iteration
matrix Bα = I −αP−1A is minimized for α = αopt, where
αopt =
2
λmin + λmax
(5.58)
λmin being the smallest eigenvalue of P−1A. Finally, always when
α = αopt, the following convergence estimate holds
∥e(k)∥A ≤
K(P−1A) −1
K(P−1A) + 1
k
∥e(0)∥A,
k ≥0
(5.59)
where ∥v∥A =
√
vT Av, v ∈Rn, is the so-called energy norm asso-
ciated to the matrix A.
Notice that when A and P are symmetric positive deﬁnite matrices,
then P−1A is similar to a symmetric positive deﬁnite matrix, then its
eigenvalues are all real and positive (see Exercise 5.17).

5.10 Richardson and gradient methods
175
Proposition 5.6 If A ∈Rn×n and P ∈Rn×n are symmetric and
positive deﬁnite matrices, the dynamic Richardson method converges
if, for instance, αk is chosen as follows:
αk =
(z(k))T r(k)
(z(k))T Az(k) ,
k ≥0
(5.60)
where z(k) = P−1r(k) is the preconditioned residual deﬁned in (5.54).
With such choice for αk, method (5.53) is called preconditioned gra-
dient method or, simply, gradient method when P is the identity
matrix.
Finally the following convergence estimate holds
∥e(k)∥A ≤
K(P−1A) −1
K(P−1A) + 1
k
∥e(0)∥A,
k ≥0
(5.61)
The parameter αk in (5.60) is the one that minimizes the new error
∥e(k+1)∥A (see Exercise 5.18).
In general, the dynamic version should be preferred to the stationary
one since it does not require the knowledge of the extreme eigenvalues
of P−1A. As a matter of fact, the parameter αk is determined in terms
of quantities which are already available from the previous iteration.
We can rewrite the preconditioned gradient method more eﬃciently
through the following algorithm (derivation is left as an exercise): given
x(0), set r(0) = b −Ax(0), then do
for k = 0, 1, . . .
Pz(k) = r(k),
αk = (z(k))T r(k)
(z(k))T Az(k) ,
x(k+1) = x(k) + αkz(k),
r(k+1) = r(k) −αkAz(k)
(5.62)
The same algorithm can be used to implement the stationary Richard-
son method by simply replacing αk with the constant value α.
From (5.59), we deduce that if P−1A is ill conditioned the convergence
rate will be very low even for α = αopt (as in that case ρ(Bαopt) ≃1).
This is overcome by a suitable choice of P. This is the reason why P is
called the preconditioner or the preconditioning matrix.

176
5 Linear systems
0
5
10
15
20
25
30
35
40
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
 
 
Jacobi
Gauss−Seidel
Gradient
Figure 5.15. Convergence history for Jacobi, Gauss-Seidel and gradient meth-
ods applied to system (5.63)
If A is a generic matrix it may be a diﬃcult task to ﬁnd a pre-
conditioner which guarantees an optimal trade-oﬀbetween damping the
condition number and keeping the computational cost for the solution of
the system (5.54) reasonably low. The choice of P should be done taking
into account the properties of the matrix A.
The dynamic Richardson method is implemented in Program 5.2
where the input parameter P stands for the preconditioning matrix (when
not prescribed, the program implements the unpreconditioned method
by setting P=I).
Example 5.14 This example, of theoretical interest only, has the purpose
of comparing the convergence behavior of Jacobi, Gauss-Seidel and gradient
methods applied to solve the following (mini) linear system:
2x1 + x2 = 1, x1 + 3x2 = 0
(5.63)
with initial vector x(0) = (1, 1/2)T . Note that the system matrix is symmetric
and positive deﬁnite, and that the exact solution is x = (3/5, −1/5)T . We
report in Figure 5.15 the behavior of the relative residual
E(k) = ∥r(k)∥/∥r(0)∥
(5.64)
for the three methods above. Iterations are stopped at the ﬁrst iteration kmin
for which E(kmin) ≤10−14. The gradient method appears to converge the
fastest.
■
Example 5.15 Let us consider a system Ax = b, where A ∈R100×100 is a
pentadiagonal matrix whose main diagonal has all entries equal to 4, while
the ﬁrst and third lower and upper diagonals have all entries equal to −1. As
customary, b is chosen in such a way that x = (1, . . . , 1)T is the exact solution
of our system. Let P be the tridiagonal matrix whose diagonal elements are all
equal to 2, while the elements on the lower and upper diagonal are all equal

5.11 The conjugate gradient method
177
to −1. Both A and P are symmetric and positive deﬁnite. With such a P as
preconditioner, Program 5.2 can be used to implement the dynamic precondi-
tioner Richardson method. We ﬁx tol=1.e-05, nmax=5000, x0=zeros(100,1).
The method converges in 43 iterations. The same Program 5.2, used with
P=’G’, implements the Gauss-Seidel method; this time as many as 1658 itera-
tions are required before satisfying the same stopping criterion.
■
5.11 The conjugate gradient method
In iterative schemes like (5.62) the new iterate x(k+1) is obtained by
adding to the old iterate x(k) a vector, named descent direction, that
is either the residual r(k) or the preconditioned residual z(k). A natural
question is whether it is possible to ﬁnd other descent directions, say
p(k), that ensure the convergence of the method in a lower number of
iterations.
When the matrix A∈Rn×n is symmetric and positive deﬁnite, the
conjugate gradient method (in short, CG) makes use of a sequence of
descent directions that are A-orthogonal (or A-conjugate), that is, ∀k ≥
0,
(Ap(j))T p(k+1) = 0,
j = 0, 1, . . . , k.
(5.65)
For any vector x(0), after setting r(0) = b −Ax(0) and p(0) = r(0), the
CG method takes the following form:
for k = 0, 1, . . .
αk =
p(k)T r(k)
p(k)T Ap(k) ,
x(k+1) = x(k) + αkp(k),
r(k+1) = r(k) −αkAp(k),
βk = (Ap(k))T r(k+1)
(Ap(k))T p(k) ,
p(k+1) = r(k+1) −βkp(k)
(5.66)
The parameter αk guarantees that the error ∥e(k+1)∥A is minimized
along the descent direction p(k), while βk is chosen to ensure that the new
direction p(k+1) is A-conjugate with p(k), that is (Ap(k))T p(k+1) = 0.
As a matter of fact, it can be proved (by the induction principle) that,
if the latter relation is satisﬁed, then all orthogonality relations in (5.65)
for j = 0, . . . , k −1 are satisﬁed, too. For a complete derivation of the

178
5 Linear systems
method, see for instance [QSS07, Chapter 4] or [Saa03]. It is possible to
prove the following important result:
Proposition 5.7 Let A be a symmetric and positive deﬁnite ma-
trix. In exact arithmetic, the conjugate gradient method for solving
(5.1) converges after at most n steps. Moreover, the error e(k) at the
kth iteration (with k < n) is orthogonal to p(j), for j = 0, . . . , k −1
and
∥e(k)∥A ≤
2ck
1 + c2k ∥e(0)∥A,
(5.67)
with c =

K(A) −1

K(A) + 1
.
Therefore, in absence of rounding errors, the CG method can be
regarded as a direct method, since it terminates after a ﬁnite number of
steps. However, for matrices of large size, it is usually employed as an
iterative scheme, and the iterations are stopped when an error estimator
(e.g. the relative residual (5.64)) falls below a ﬁxed tolerance. In this
respect, by comparing (5.67) with (5.61), it is readily seen that CG
iterations converge more rapidly than gradient iterations, because of the
presence of the square root of K(A).
Also for the CG method it is possible to consider a precondi-
tioned version (the PCG method), with a preconditioner P symmet-
ric and positive deﬁnite, which reads as follows: given x(0) and setting
r(0) = b −Ax(0), z(0) = P−1r(0) and p(0) = z(0), do
for k = 0, 1, . . .
αk =
p(k)T r(k)
p(k)T Ap(k) ,
x(k+1) = x(k) + αkp(k),
r(k+1) = r(k) −αkAp(k),
Pz(k+1) = r(k+1),
βk = (Ap(k))T z(k+1)
(Ap(k))T p(k) ,
p(k+1) = z(k+1) −βkp(k)
(5.68)

5.11 The conjugate gradient method
179
Table 5.4. Errors obtained using the preconditioned gradient method (PG),
the preconditioned conjugate gradient method (PCG), and the direct method
implemented in the MATLAB command \ for the solution of the Hilbert
system. For the iterative methods also the number of iterations is reported
\
PG
PCG
n
K(An)
Error
Error
Iter
Error
Iter
4
1.55e+04
7.72e-13
8.72e-03
995
1.12e-02
3
6
1.50e+07
7.61e-10
3.60e-03
1813
3.88e-03
4
8
1.53e+10
6.38e-07
6.30e-03
1089
7.53e-03
4
10
1.60e+13
5.24e-04
7.98e-03
875
2.21e-03
5
12
1.70e+16
6.27e-01
5.09e-03
1355
3.26e-03
5
14
6.06e+17
4.12e+01
3.91e-03
1379
4.32e-03
5
In this case the error estimate (5.64) still holds, however K(A) is replaced
by the more favourable K(P−1A).
The PCG method is implemented in the MATLAB function pcg.
pcg
Example 5.16 Let us go back to Example 5.9 on the Hilbert matrix and
solve the related system (for diﬀerent values of n) by the preconditioned gra-
dient (PG) and the preconditioned conjugate gradient (PCG) methods, using
as preconditioner the diagonal matrix D made of the diagonal entries of the
Hilbert matrix. We ﬁx x(0) to be the null vector and iterate until the relative
residual (5.64) is less than 10−6. In Table 5.4 we report the absolute errors
(with respect to the exact solution) obtained with PG and PCG methods, as
well as the errors obtained using the MATLAB command \. For the latter,
the error degenerates when n gets large. On the other hand, we can appreciate
the beneﬁcial eﬀect that a suitable iterative method such as the PCG scheme
can have on the number of iterations.
■
Remark 5.5 (Non-symmetric systems) The CG method is a special in-
stance of the so-called Krylov (or Lanczos) methods that can be used for the
solution of systems which are not necessarily symmetric. Their description is
provided, e.g., in [Axe94], [Saa03] and [vdV03].
Some of them share with the CG method the notable property of ﬁnite
termination, that is, in exact arithmetic they provide the exact solution in
a ﬁnite number of iterations also for nonsymmetric systems. A remarkable
example is the GMRES (Generalized Minimum RESidual) method, available
in MATLAB under the name of gmres.
gmres
Another method, the Bi-CGStab ([vdV03]), is very competitive with GM-
RES from the eﬃciency point of view. The MATLAB command is bicgstab.
bicgstab
.
■
See Exercises 5.15-5.19.

180
5 Linear systems
5.12 When should an iterative method be stopped?
In theory, iterative methods require an inﬁnite number of iterations to
converge to the exact solution of a linear system. Even when this is
not the case (see, e.g. the CG method), the number of iterations to
achieve the solution within machine accuracy is very high when the size
of the linear system gets large. In practice, aiming at the exact solution
is neither reasonable nor necessary. Indeed, what we do really need is
to obtain an approximation x(k) for which we can guarantee that the
error be lower than a desired tolerance ϵ. On the other hand, since the
error is itself unknown (as it depends on the exact solution), we need
a suitable a posteriori error estimator which predicts the error starting
from quantities that have already been computed.
The ﬁrst type of estimator is represented by the residual at the kth
iteration, see (5.52). More precisely, we could stop our iterative method
at the ﬁrst iteration step kmin for which
∥r(kmin)∥≤ε∥b∥.
Setting x = x(kmin) and r = r(kmin) in (5.36) we would obtain
∥e(kmin)∥
∥x∥
≤εK(A),
which is an estimate for the relative error. We deduce that the control
on the residual is meaningful only for those matrices whose condition
number is reasonably small.
Example 5.17 Let us consider the linear system (5.1) where A=A20 is the
Hilbert matrix of dimension 20 introduced in Example 5.9 and b is constructed
in such a way that the exact solution is x = (1, 1, . . . , 1)T . Since A is sym-
metric and positive deﬁnite the Gauss-Seidel method surely converges. We use
Program 5.2 to solve this system taking x0 to be the null initial vector and
setting a tolerance on the residual equal to 10−5. The method converges in
472 iterations; however the relative error is very large and equals 0.0586. This
is due to the fact that A is extremely ill conditioned, having K(A) ≃1017.
In Figure 5.16 we show the behavior of the residual (normalized to the initial
one) and that of the error as the number of iterations increases.
■
An alternative approach is based on the use of a diﬀerent error es-
timator, namely the increment δ(k) = x(k+1) −x(k). More precisely, we
can stop our iterative method at the ﬁrst iteration step kmin for which
∥δ(kmin)∥≤ε.
(5.69)
In the special case where B is symmetric and positive deﬁnite, we have
∥e(k)∥= ∥e(k+1) + δ(k)∥≤ρ(B)∥e(k)∥+ ∥δ(k)∥.

5.12 When should an iterative method be stopped?
181
0
100
200
300
400
500
10
−6
10
−4
10
−2
10
0
10
2
Figure 5.16. Behavior, versus iterations k, of the relative residual (5.64)
(dashed line) and of the error ∥x −x(k)∥/∥x∥(solid line) for Gauss-Seidel
iterations applied to the system of Example 5.17
Since ρ(B) should be less than 1 in order for the method to converge, we
deduce
∥e(k)∥≤
1
1 −ρ(B)∥δ(k)∥
(5.70)
From the last inequality we see that the control on the increment is
meaningful only if ρ(B) is much smaller than 1 since in that case the
error will be of the same size as the increment.
In fact, the same conclusion holds even if B is not symmetric and
positive deﬁnite (as it occurs for the Jacobi and Gauss-Seidel methods);
however in that case (5.70) is no longer true.
Should one be interested in relative errors, (5.69) could be replaced by
∥δ(kmin)∥
∥b∥
≤ε
and, consequently, (5.70) by
∥e(k)∥
∥b∥
≤
1
1 −ρ(B)ε.
Example 5.18 Let us consider a system whose matrix A∈R50×50 is tridiago-
nal and symmetric with entries equal to 2.001 on the main diagonal and equal
to 1 on the two other diagonals. As usual, the right hand side b is chosen in such
a way that the unit vector (1, . . . , 1)T is the exact solution. Since A is tridi-
agonal with strict diagonal dominance, the Gauss-Seidel method will converge
about twice as fast as the Jacobi method (in view of Proposition 5.4). Let us
use Program 5.2 to solve our system in which we replace the stopping criterion
based on the residual by that based on the increment, i.e. ∥δ(k)∥≤ε. Using
the initial vector whose components are (x0)i = 10 sin(100i) (for i = 1, . . . , n)

182
5 Linear systems
and setting the tolerance tol= 10−5, after 859 iterations the solution returned
by the program is such that ∥e(859)∥≃0.0021. The convergence is very slow
and the error is quite large since the spectral radius of the iteration matrix is
equal to 0.9952, which is very close to 1. Should the diagonal entries be set
equal to 3, after only 17 iterations we would have obtained convergence with
an error ∥e(17)∥≃8.96 · 10−6. In fact in that case the spectral radius of the
iteration matrix would be equal to 0.443.
■
Let us summarize
1. An iterative method for the solution of a linear system starts from
a given initial vector x(0) and builds up a sequence of vectors x(k)
which we require to converge to the exact solution as k →∞;
2. an iterative method converges for every possible choice of the initial
vector x(0) iﬀthe spectral radius of the iteration matrix is strictly
less than 1;
3. classical iterative methods are those of Jacobi and Gauss-Seidel. A
suﬃcient condition for convergence is that the system matrix be
strictly diagonally dominant by row (or symmetric and deﬁnite pos-
itive in the case of Gauss-Seidel);
4. in the Richardson method convergence is accelerated thanks to the
introduction of a parameter and (possibly) a convenient precondi-
tioning matrix;
5. with the conjugate gradient method the exact solution of a symmet-
ric positive deﬁnite system can be computed in a ﬁnite number of
iterations (in exact arithmetic). This method can be generalized to
the nonsymmetric case;
6. there are two possible stopping criteria for an iterative method:
controlling the residual or controlling the increment. The former is
meaningful if the system matrix is well conditioned, the latter if the
spectral radius of the iteration matrix is not close to 1.
5.13 To wrap-up: direct or iterative?
In this section we compare direct and iterative methods on several simple
test cases. For a linear system of small size, it doesn’t really matter since
every method will make the job. Instead, for large scale systems, the
choice will depend primarily on the matrix properties (such as symmetry,
positive deﬁniteness, sparsity pattern, condition number), but also on the
kind of available computer resources (memory access, fast processors,
etc.). We must admit that in our tests the comparison will not be fully
loyal. One direct solver that we will in fact use is the MATLAB built-
in function \ which is compiled and optimized, whereas the iterative

5.13 To wrap-up: direct or iterative?
183
solvers are not. Our computations were carried out on a processor Intel R
⃝
CoreTM2 Duo 2.53GHz with 3072KB cache and 3GByte RAM.
A sparse, banded linear system with small bandwidth
The ﬁrst test case concerns linear systems arising from the 5-point ﬁnite
diﬀerence discretizations of the Poisson problem on the square (−1, 1)2
with homogeneous Dirichlet boundary conditions (see Section 9.2.4).
Uniform grids of step h = 2/(N + 1) in both spatial coordinates are
considered, for several values of N. The corresponding ﬁnite diﬀerence
matrices, with (N + 2)2 rows and columns, are generated using Program
9.2. On Figure 5.17, left, we plot the matrix structure corresponding
to the value (N + 2)2 = 256 (obtained by the command spy): it is
spy
sparse, banded, with only 5 non-null entries per row. After eliminat-
ing those rows and columns associated to boundary nodes, we denote
by n = N 2 the size of the reduced matrix. Any such matrix is sym-
metric and positive deﬁnite but ill conditioned: its spectral condition
number behaves like a constant time h−2 for all values of h, that is
the smaller the parameter h, the worse the matrix condition number.
To solve the associated linear systems we will use the Cholesky fac-
torization, the preconditioned conjugate gradient method (PCG) with
preconditioner given by the incomplete Cholesky factorization, and the
MATLAB command \ that, in the current case, is in fact an ad hoc algo-
rithm for pentadiagonal symmetric matrices. The incomplete Cholesky
factorization of A is generated from an algebraic manipulation of the en-
tries of the R factor of A (see [QSS07]) and is computed by the command
ichol(A,struct(’type’,’ict’,’droptol’,1e-03)).
ichol
The stopping criterion for the PCG method is that the relative resid-
ual (5.64) be lower than 10−13; the CPU time is also inclusive of the
time necessary to construct the preconditioner.
In Figure 5.17, right, we compare the CPU time for the three diﬀer-
ent methods versus the matrix size. The direct method hidden by the
command \ is by far the cheapest: in fact, it is based on a variant of
the Gaussian elimination that is particularly eﬀective for sparse banded
matrices with small bandwith.
The PCG method, in its turn, is more convenient than the CG
method (with no preconditioning). For instance, if n = 4096
(corre-
sponding to N = 64) the PCG method requires 18 iterations, whereas
the CG method would require 154 iterations. Both methods, however,
are less convenient than the Cholesky factorization. We warn the reader
that the conclusions should be taken with a grain of salt, as they depend
on the way the algorithms are implemented and the kind of computer
used.

184
5 Linear systems
0
50
100
150
200
250
0
50
100
150
200
250
0
1
2
3
4
5
6
7
x 10
4
0
0.5
1
1.5
2
2.5
3
3.5
Figure 5.17.
The structure of the matrix for the ﬁrst test case (left), and
the CPU time (in sec.) needed for the solution of the associated linear system
(right): the solid line refers to the command \, the dashed-dotted line to the
use of the Cholesky factorization, the dashed line to the PCG iterative method.
The values in abscissa refer to the matrix dimension n
The case of a broad band
We still consider the same Poisson equation, however this time the dis-
cretization is based on spectral methods with Gauss-Legendre-Lobatto
quadrature formulae (see, for instance, [Qua13, CHQZ06]). Even though
the number of grid-nodes is the same as for the ﬁnite diﬀerences, with
spectral methods the derivatives are approximated using many more
nodes (in fact, at any given node the x-derivatives are approximated us-
ing all the nodes sitting on the same row, whereas all those on the same
column are used to compute y-derivatives). The corresponding matrices
are still sparse and structured, however the number of non-null entries is
deﬁnitely higher than in the former case. This is clear from the example
in Figure 5.18, left, where the spectral matrix has still N 2 = 256 rows
and columns, but the number of nonzero entries is 7936 instead of the
1216 of the ﬁnite diﬀerence matrix of Figure 5.17.
The CPU time reported in Figure 5.18, right, shows that for this
matrix the PCG algorithm, using the incomplete Cholesky factorization
as preconditioner, performs much better than the other two methods.
A ﬁrst conclusion to draw is that for sparse symmetric and pos-
itive deﬁnite matrices with large bandwidth, PCG is more eﬃcient
than the direct method implemented in MATLAB (which does not use
the Cholesky factorization since the matrix is stored with the format
sparse). We point out that a suitable preconditioner is however crucial
in order for the PCG method to become competitive.
Finally, we shoud keep in mind that direct methods require more
memory storage than iterative methods, a diﬃculty that could become
insurmontable in large scale applications.

5.13 To wrap-up: direct or iterative?
185
0
50
100
150
200
250
0
50
100
150
200
250
0
2000
4000
6000
8000
10000
12000
0
5
10
15
20
25
30
35
40
45
Figure 5.18. The structure of the matrix used in the second test case (left),
and the CPU time (in sec.) needed to solve the associated linear system (right):
the solid line refers to the command \, the dashed-dotted line to the use of
the Cholesky factorization, the dashed line to the PCG iterative method. The
values in abscissa refer to the matrix dimension n
Systems with full matrices
With the MATLAB command gallery we can get access to a collection
gallery
of matrices featuring diﬀerent structure and properties. In particular
for our third test case, by the command A=gallery(’riemann’,n) we
select the so-called Riemannn matrix of dimension n, that is a n × n
full, non-symmetric matrix whose determinant behaves like det(A) =
O(n!n−1/2+ϵ) for all ϵ > 0. The associated linear system is solved by the
iterative GMRES method (see Remark 5.5) and the iterations will be
stopped as soon as the norm of the relative residual (5.64) becomes less
than 10−13. Alternatively, we will use the MATLAB command \ that,
in the case at hand, implements the LU factorization.
For several values of n we will solve the corresponding linear system
whose exact solution is the unitary vector: the right-hand side is com-
puted accordingly. The GMRES iterations are obtained without precon-
ditioning. In Figure 5.19, right, we report the CPU time for n ranging
between 100 and 1000. On the left we report cond(A), the condition
number of A. As we can see, the direct factorization method is far less
expensive than the un-preconditioned GMRES method, however it be-
comes more expensive for large n when suitable preconditioners are used.
Octave 5.1 The gallery command is not available in Octave. However
a few are available such as the Hilbert, Hankel or Vandermonde matrices,
see the commands hankel, hilb, invhilb sylvester_matrix, toeplitz and
vander. Moreover if you have access to MATLAB, you can save a matrix
deﬁned in the gallery using the save command and then load it in Octave
using load. Here is an example:
In MATLAB:

186
5 Linear systems
100
200
300
400
500
600
700
800
900
1000
0
2000
4000
6000
8000
10000
12000
0
2
4
6
8
10
x 10
5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Figure 5.19.
On the left, the condition number of the Riemann matrix A.
On the right, the comparison between the CPU times (in sec.) for the solution
of the linear system: the solid line refers to the command \, the dashed line
refers to the GMRES iterative method with no preconditioning. The values in
abscissa refer to the matrix dimension n
riemann10 =gallery (’riemann ’ ,10);
save
’riemann10 ’ riemann10
In Octave:
load
’riemann10 ’ riemann10
■
Systems with sparse, nonsymmetric matrices
We consider linear systems that are generated by the ﬁnite element dis-
cretization of diﬀusion-transport-reaction boundary-value problems in
two dimensions. These problems are similar to the one reported in (9.17)
which refers to a one-dimensional case. Its ﬁnite element approximation,
that is illustrated in Section 9.2.3 in the one-dimensional case, makes use
of piecewise linear polynomials to represent the solution in each triangu-
lar element of a grid that partitions the region where the boundary-value
problem is set up. The unknowns of the associated algebraic system is
the set of values attained by the solution at the vertices of the internal
triangles. We refer to, e.g., [QV94] for a description of this method, as
well as for the determination of the entries of the matrix. Let us simply
point out that this matrix is sparse, but not banded (its sparsity pattern
depends on the way the vertices are numbered) and nonsymmetric, due
to the presence of the transport term. The lack of symmetry, however, is
not evident from the representation of its structure in Figure 5.20, left.
The smaller the diameter h of the triangles (i.e. the lengths of their
longest edge), the higher the matrix size. We are using unstructured
triangular grids generated by the MATLAB toolbox pdetool. We have
pdetool
compared the CPU time necessary to solve the linear system correspond-

5.13 To wrap-up: direct or iterative?
187
0
100
200
300
400
500
600
0
100
200
300
400
500
600
0
1
2
3
4
5
x 10
4
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
 
 
h=0.1, n=724, it=73
h=0.05, n=2849, it=161
h=0.025, n=11272, it=309
h=0.0125, n=44772, it=614
Figure 5.20.
The structure of one of the matrices used for the fourth test
case (left), and the CPU time (in sec.) needed for the solution of the associated
linear system (right): the solid line refers to the command \, the dashed line
to the Bi-CGStab iterative method. The values in abscissa refer to the matrix
dimension n, while it stands for Bi-CGStab iterations
ing to the case h = 0.1, 0.05, 0.025, and 0.0125. We have used the MAT-
LAB command \, that in this case uses the UMFPACK library and the
(MATLAB implementation of the) iterative method Bi-CGStab which
can be regarded as a generalization to nonsymmetric systems of the con-
jugate gradient method. In abscissae we have reported the number of
unknowns that ranges from 724 (for h = 0.1) to 44772 (for h = 0.0125).
Also in this case, the direct method is less expensive than the itera-
tive one. Should we use as preconditioner for the Bi-CGStab method
the incomplete LU factorization, the number of iterations would reduce,
however the CPU time would be higher than the one for the unpre-
conditioned case. The incomplete LU factorization of the matrix A is
generated from an algebraic manipulation of the entries of the factors
L and U of A (see, e.g., [QSS07]) and is computed by the command
ilu
ilu(A,struct(’type’,’ilutp’,’droptol’,1.e-3)).
In conclusion
The comparisons that we have carried out, although very limited, out-
lines a few relevant aspects. In general, direct methods (especially if
implemented in their most sophisticated versions, such as in the \ MAT-
LAB command) are more eﬃcient than iterative methods when the lat-
ter are used without eﬃcient preconditioners. However, they are more
sensitive to the matrix ill conditioning (see for instance the Example
5.16) and may require a substantial amount of storage.
A further aspect that is worth mentioning is that direct methods
require the knowledge of the matrix entries, whereas iterative methods
don’t. In fact, what is nedeed at each iteration is the computation of
matrix-vector products for given vectors. This aspect makes iterative

188
5 Linear systems
methods especially interesting for those problems in which the matrix is
not explicitely generated.
5.14 What we haven’t told you
Several eﬃcient variants of the LU factorization are available for sparse
systems of large dimension. Among the most advanced, we quote the
so-called multifrontal method which makes use of a suitable reordering
of the system unknowns in order to keep the triangular factors L and
U as sparse as possible. The multifrontal method is implemented in the
software package UMFPACK. More on this issue is available on [GL96]
and [DD99].
Concerning iterative methods, both the conjugate gradient method
and the GMRES method are special instances of Krylov methods. For a
description of Krylov methods see e.g. [Axe94], [Saa03] and [vdV03].
As it was pointed out, iterative methods converge slowly if the system
matrix is severely ill conditioned. Several preconditioning strategies have
been developed (see, e.g., [dV89] and [vdV03]). Some of them are purely
algebraic, that is, they are based on incomplete (or inexact) factoriza-
tions of the given system matrix, and are implemented in the already
quoted MATLAB functions ichol and ilu. Other strategies are devel-
oped ad hoc by exploiting the physical origin and the structure of the
problem which has generated the linear system at hand.
Finally it is worthwhile to mention the multigrid methods which are
based on the sequential use of a hierarchy of systems of variable dimen-
sions that “resemble” the original one, allowing a clever error reduction
strategy (see, e.g., [Hac85], [Wes04] and [Hac94]).
Octave 5.2 In Octave, ichol is not yet available. Only the incomplete
LU factorization has been implemented in the function luinc. See the
help of Octave.
■
5.15 Exercises
Exercise 5.1 For a given matrix A ∈Rn×n ﬁnd the number of operations (as
a function of n) that are needed for computing its determinant by the recursive
formula (1.8).
Exercise 5.2 Use the MATLAB command magic(n), n = 3, 4, . . . , 500, to
magic
construct the magic squares of order n, that is, those matrices having entries
for which the sum of the elements by rows, columns or diagonals are identical.
Then compute their determinants by the command det introduced in Section

5.15 Exercises
189
1.4 and the CPU time that is needed for this computation using the cputime
command. Finally, approximate this data by the least-squares method and
deduce that the CPU time scales approximately as n3.
Exercise 5.3 Find for which values of ε the matrix deﬁned in (5.16) does not
satisfy the hypotheses of Proposition 5.1. For which value of ε does this matrix
become singular? Is it possible to compute the LU factorization in that case?
Exercise 5.4 Verify that the number of operations necessary to compute the
LU factorization of a square matrix A of dimension n is approximately 2n3/3.
Exercise 5.5 Show that the LU factorization of A can be used for computing
the inverse matrix A−1. (Observe that the jth column vector of A−1, say xj,
satisﬁes the linear system Axj = ej, ej being the vector whose components
are all null except the jth component which is 1.)
Exercise 5.6 Compute the factors L and U of the matrix of Example 5.8 and
verify that the LU factorization is inaccurate.
Exercise 5.7 Explain why partial pivoting by row is not convenient for sym-
metric matrices.
Exercise 5.8 Consider the linear system Ax = b with
A =
⎡
⎣
2
−2
0
ε −2
2
0
0
−1
3
⎤
⎦,
and b such that the corresponding solution is x = (1, 1, 1)T and ε is a positive
real number. Compute the LU factorization of A and note that l32 →∞when
ε →0. Verify that the computed solution is not aﬀected by rounding errors
when ε = 10−k with k = 0, .., 9 and b = (0, ε, 2)T . Moreover, analyze the
relative error on the exact solution when ε = 1/3 · 10−k with k = 0, .., 9, and
the exact solution is xex = (log(5/2), 1, 1)T .
Exercise 5.9 Consider the linear systems Aixi = bi, i = 1, 2, 3, with
A1 =
⎡
⎢⎢⎣
15 6 8 11
6 6 5 3
8 5 7 6
11 3 6 9
⎤
⎥⎥⎦, Ai = (A1)i, i = 2, 3,
and bi such that the solution is always xi = (1, 1, 1, 1)T . Solve the system
by the LU factorization using partial pivoting by row, and comment on the
obtained results.
Exercise 5.10 Show that for a symmetric and positive deﬁnite matrix A we
have K(A2) = (K(A))2.

190
5 Linear systems
Exercise 5.11 Analyse the convergence properties of the Jacobi and Gauss-
Seidel methods for the solution of a linear system whose matrix is
A =
⎡
⎣
α 0 1
0 α 0
1 0 α
⎤
⎦,
α ∈R.
Exercise 5.12 Provide a suﬃcient condition on β so that both the Jacobi
and Gauss-Seidel methods converge when applied for the solution of a system
whose matrix is
A =
	 −10 2
β
5

.
(5.71)
Exercise 5.13 For the solution of the linear system Ax = b with A ∈Rn×n,
consider the relaxation method: given x(0) = (x(0)
1 , . . . , x(0)
n )T , for k = 0, 1, . . .
compute
r(k)
i
= bi −
i−1

j=1
aijx(k+1)
j
−
n

j=i+1
aijx(k)
j , x(k+1)
i
= (1 −ω)x(k)
i
+ ω r(k)
i
aii ,
for i = 1, . . . , n, where ω is a real parameter. Find the explicit form of the
corresponding iterative matrix, then verify that the condition 0 < ω < 2 is
necessary for the convergence of this method. Note that if ω = 1 this method
reduces to the Gauss-Seidel method. If 1 < ω < 2 the method is known as
SOR (successive over-relaxation).
Exercise 5.14 Consider the linear system Ax = b with A =
	 3 2
2 6

and say
whether the Gauss-Seidel method converges, without explicitly computing the
spectral radius of the iteration matrix. Repeat with A =
	 1 1
1 2

.
Exercise 5.15 Compute the ﬁrst iteration of the Jacobi, Gauss-Seidel and
preconditioned gradient method (with preconditioner given by the diagonal of
A) for the solution of system (5.63) with x(0) = (1, 1/2)T .
Exercise 5.16 Prove (5.58), then show that
ρ(Bαopt) = λmax −λmin
λmax + λmin = K(P−1A) −1
K(P−1A) + 1.
(5.72)
Exercise 5.17 Prove that if A and P are symmetric positive deﬁnite matrices,
then P−1A is similar to a symmetric positive deﬁnite matrix.
Exercise 5.18 Note that, in using an acceleration parameter α instead of αk,
from (5.62) we have x(k+1) = x(k) +αz(k) so that the error e(k+1) = x−x(k+1)
depends on α. Prove that the expression of αk given in (5.60) minimizes the
function Φ(α) = ∥e(k+1)∥2
A with respect to α ∈R.

5.15 Exercises
191
Exercise 5.19 Let us consider a set of n = 20 factories which produce 20
diﬀerent goods. With reference to the Leontief model introduced in Problem
5.3, suppose that the matrix C has the following integer entries: cij = i + j for
i, j = 1, . . . , n, while bi = i, for i = 1, . . . , 20. Is it possible to solve this system
by the gradient method? Propose a method based on the gradient method
noting that, if A is nonsingular, the matrix AT A is symmetric and positive
deﬁnite.

6
Eigenvalues and eigenvectors
Given a square matrix A ∈Cn×n, the eigenvalue problem consists in
ﬁnding a scalar λ (real or complex) and a nonnull vector x such that
Ax = λx
(6.1)
Any such λ is called an eigenvalue of A, while x is the associated eigen-
vector. The latter is not unique; indeed all its multiples αx with α ̸= 0,
real or complex, are also eigenvectors associated with λ. Should x be
known, λ can be recovered by using the Rayleigh quotient xHAx/∥x∥2,
xH = ¯xT being the vector whose ith component is equal to ¯xi.
A number λ is an eigenvalue of A if it is a root of the following
polynomial of degree n (called the characteristic polynomial of A):
pA(λ) = det(A −λI).
Consequently, a square matrix of dimension n has exactly n eigen-
values (real or complex), not necessarily distinct. Also, if A has real
entries, pA(λ) has real coeﬃcients, and therefore complex eigenvalues of
A necessarily occur in complex conjugate pairs.
Let us also recall that a matrix A∈Cn×n is said to be diagonalizable
if there exists a nonsingular matrix U∈Cn×n such that
U−1AU = Λ = diag(λ1, . . . , λn).
(6.2)
The columns of U are the eigenvectors of A and form a basis for Cn.
In the special case where A is either diagonal or triangular, its eigen-
values are nothing but its diagonal entries. However, if A is a general ma-
trix and its dimension n is suﬃciently large, seeking the zeros of pA(λ) is
not the most convenient approach. Ad hoc algorithms are better suited,
and some of them will be described in the next sections.
A. Quarteroni et al., Scientiﬁc Computing with MATLAB and Octave,
Texts in Computational Science and Engineering 2,
DOI 10.1007/978-3-642-45367-0 6, © Springer-Verlag Berlin Heidelberg 2014
193

194
6 Eigenvalues and eigenvectors
x
x1(t)
x2(t)
P1
P2
Figure 6.1. The system of two pointwise bodies of equal mass connected by
springs
6.1 Some representative problems
Problem 6.1 (Elastic springs) Consider the system of Figure 6.1
made of two pointwise bodies P1 and P2 of mass m, connected by two
springs and free to move along the line joining P1 and P2. Let xi(t) de-
note the position occupied by Pi at time t for i = 1, 2. Then from the
second law of dynamics we obtain
m
..x1= K(x2 −x1) −Kx1,
m
..x2= K(x1 −x2),
where K is the elasticity coeﬃcient of both springs. We are interested
in free oscillations whose corresponding solution is xi = ai sin(ωt + φ),
i = 1, 2, with ai ̸= 0. In this case we ﬁnd that
−ma1ω2 = K(a2 −a1) −Ka1,
−ma2ω2 = K(a1 −a2).
(6.3)
This is a 2 × 2 homogeneous system which has a non-trivial solution
a = (a1, a2)T iﬀthe number λ = mω2/K is an eigenvalue of the matrix
A =
.
2 −1
−1
1
/
.
With this deﬁnition of λ, (6.3) becomes Aa = λa. Since pA(λ) = (2 −
λ)(1 −λ) −1, the two eigenvalues are λ1 ≃2.618 and λ2 ≃0.382 and
correspond to the frequencies of oscillation ωi =

Kλi/m which are
admitted by our system.
■
Problem 6.2 (Population dynamics) Several mathematical models
have been proposed in order to predict the evolution of certain species
(either human or animal). The simplest population model, which was
introduced in 1920 by Lotka and formalized by Leslie 20 years later, is
based on the rate of mortality and fecundity for diﬀerent age intervals,
say i = 0, . . . , n. Let x(t)
i
denote the number of females (males don’t

6.1 Some representative problems
195
matter in this context) whose age at time t falls in the ith interval. The
values of x(0)
i
are given. Moreover, let si denote the rate of survival of
the females belonging to the ith interval, and mi the average number of
females generated from a female in the ith interval.
The model by Lotka and Leslie is described by the set of equations
x(t+1)
i+1
= x(t)
i si
i = 0, . . . , n −1,
x(t+1)
0
=
n

i=0
x(t)
i mi.
The n ﬁrst equations describe the population development, the last its
reproduction. In matrix form we have
x(t+1) = Ax(t),
where x(t) = (x(t)
0 , . . . , x(t)
n )T while A is the Leslie matrix
A =
⎡
⎢⎢⎢⎢⎢⎢⎣
m0 m1 . . . . . .
mn
s0 0
. . . . . .
0
0
s1
...
...
...
... ... ...
...
0
0
0
sn−1 0
⎤
⎥⎥⎥⎥⎥⎥⎦
.
We will see in Section 6.2 that the dynamics of this population is de-
termined by the eigenvalue of maximum modulus of A, say λ1, whereas
the distribution of the individuals in the diﬀerent age intervals (normal-
ized with respect to the whole population), is obtained as the limit of
x(t) for t →∞and satisﬁes Ax = λ1x. This problem will be solved in
Exercise 6.2.
■
Problem 6.3 (Interurban railway network) For n given cities, let
A be the matrix whose entry aij is equal to 1 if the ith city is directly
connected to the jth city, and 0 otherwise. One can show that the compo-
nents of the eigenvector x (of unit length) associated with the maximum
eigenvalue provides the accessibility rate (which is a measure of the ease
of access) to the various cities. In Example 6.2 we will compute this
vector for the case of the railways system of the eleven most important
cities in Lombardy (see Figure 6.2).
■
Problem 6.4 (Image compression) The problem of image compres-
sion can be faced using the singular-value decomposition of a matrix
introduced in (5.45). Indeed, a black and white image can be repre-
sented by a real m × n rectangular matrix A where m and n represent

196
6 Eigenvalues and eigenvectors
9
8
7
6
5
4
1
3
2
10
11
1 Milan
2 Pavia
3 Lodi
4 Brescia
5 Bergamo
6 Como
7 Varese
8 Lecco
9 Sondrio
10 Cremona
11 Mantua
Figure 6.2. A schematic representation of the railway network between the
main cities of Lombardy
the number of pixels that are present in the horizontal and vertical di-
rection, respectively, and the coeﬃcient aij represents the intensity of
gray of the (i, j)th pixel. Considering the singular value decomposition
(5.45) of A, and denoting by ui and vi the ith column vectors of U and
V, respectively, we ﬁnd
A = σ1u1vT
1 + σ2u2vT
2 + . . . + σpupvT
p .
(6.4)
We can approximate A by the matrix Ak which is obtained by truncating
the sum (6.4) to the ﬁrst k terms, for 1 ≤k ≤p. If the singular values σi
are in decreasing order, σ1 ≥σ2 ≥. . . ≥σp, disregarding the latter p−k
should not signiﬁcantly aﬀect the quality of the image. To transfer the
“compressed” image Ak (for instance from one computer to another) we
simply need to transfer the vectors ui, vi and the singular values σi for
i = 1, . . . , k and not all the entries of A. In Example 6.9 we will see this
technique in action.
■
Even though most of the methods that we will present in this Sec-
tion are valid for compex matrices too, for simplicity we will limit our
analysis to real matrices. In any case, we note that MATLAB and Oc-
tave programs for computing both eigenvalues and eigenvectors work
on both real and complex variables, with no need to modify the calling
instructions.
6.2 The power method
As noticed in Problems 6.2 and 6.3, the knowledge of the whole spectrum
of A (that is the set of all its eigenvalues) is not always required. Often,

6.2 The power method
197
only the extremal eigenvalues matter, that is, those having largest and
smallest modulus.
Suppose that A is a square matrix of dimension n, with real entries,
and assume that its eigenvalues are ordered as follows
|λ1| > |λ2| ≥|λ3| ≥. . . ≥|λn|.
(6.5)
Note, in particular, that |λ1| is distinct from the other moduli of the
eigenvalues of A. Let us indicate by x1 the eigenvector (with unit length)
associated with λ1. If the eigenvectors of A are linearly independent, λ1
and x1 can be computed by the following iterative procedure, commonly
known as the power method:
given an arbitrary initial vector x(0) ∈Cn and setting y(0) =
x(0)/∥x(0)∥, compute
for k = 1, 2, . . .
x(k) = Ay(k−1),
y(k) =
x(k)
∥x(k)∥,
λ(k) = (y(k))HAy(k)
(6.6)
Note that, by recursion, one ﬁnds y(k) = β(k)Akx(0) where β(k) =
(
k

i=0
∥x(i)∥)−1 for k ≥1. The presence of the powers of A justiﬁes the
name given to this method.
In the next section we will see that this method generates a sequence
of vectors {y(k)} with unit length which, as k →∞, align themselves
along the direction of the eigenvector x1.
It is possible to prove (see, e.g. [QSS07]) that, if x(0)Hx1 ̸= 0, both
quantities ∥y(k) −(y(k)Hx1)x1∥and |λ(k) −λ1| are proportional to the
ratio |λ2/λ1|k in the case of a generic matrix, and to |λ2/λ1|2k when the
matrix A is hermitian. In all cases λ(k) →λ1 for k →∞.
An implementation of the power method is given in the Program 6.1.
The iterative procedure is stopped at the ﬁrst iteration k when
|λ(k) −λ(k−1)| < ε|λ(k)|,
where ε is a desired tolerance. The input parameters are the real ma-
trix A, the tolerance tol for the stopping test, the maximum admissible
number of iterations nmax and the initial vector x0. Output parameters
are the maximum modulus eigenvalue lambda, the associated eigenvector
and the actual number of iterations which have been carried out.

198
6 Eigenvalues and eigenvectors
Program 6.1. eigpower: power method
function [lambda ,x,iter ]= eigpower (A,tol ,nmax ,x0)
% EIGPOWER
Computes
the eigenvalue
with
maximum
modulus
%
of a real
matrix.
%
LAMBDA=EIGPOWER (A) computes
with
the
power method
%
the eigenvalue
of A of maximum
modulus
from an
%
initial
guess which by default is an all one vector.
%
LAMBDA=EIGPOWER (A,TOL ,NMAX ,X0) uses
an
absolute
%
error tolerance
TOL (the default is 1.e-6) and a
%
maximum
number of iterations
NMAX (the
default is
%
100),
starting
from
the
initial
vector X0.
%
[LAMBDA ,V,ITER ]= EIGPOWER (A,TOL ,NMAX ,X0) also
returns
%
the eigenvector
V such
that A*V=LAMBDA*V and the
%
iteration
number at which V was computed .
[n,m] = size (A);
if n ~= m, error(’Only
for
square
matrices ’); end
if nargin == 1
tol = 1.e -06;
x0 = ones(n ,1);
nmax = 100;
end
x0 = x0/norm (x0);
pro = A*x0;
lambda = x0 ’*pro;
err = tol*abs(lambda) + 1;
iter = 0;
while err >tol*abs(lambda) & abs(lambda )~=0 & iter <= nmax
x = pro;
x = x/norm (x);
pro = A*x;
lambdanew
= x’*pro;
err = abs( lambdanew
- lambda );
lambda = lambdanew ;
iter = iter + 1;
end
return
Example 6.1 Consider the family of matrices
A(α) =
⎡
⎢⎢⎣
α 2
3 13
5 11 10 8
9 7
6 12
4 14 15 1
⎤
⎥⎥⎦,
α ∈R.
We want to approximate the eigenvalue with largest modulus by the power
method. When α = 30, the eigenvalues of the matrix are given by λ1 = 39.396,
λ2 = 17.8208, λ3 = −9.5022 and λ4 = 0.2854 (only the ﬁrst four signiﬁcant
digits are reported). The method approximates λ1 in 22 iterations with a
tolerance ε = 10−10 and x(0) = 1T . However, if α = −30 we need as many
as 708 iterations. The diﬀerent behavior can be explained by noting that in
the latter case one has λ1 = −30.643, λ2 = 29.7359, λ3 = −11.6806 and
λ4 = 0.5878. Thus, |λ2|/|λ1| = 0.9704 is close to unity.
■
Example 6.2 (Interurban railway network) We denote by A∈R11×11
the matrix associated to the railways system of Figure 6.2, i.e. the matrix
whose entry aij is equal to one if there is a direct connection between the ith
and the jth cities, zero otherwise. Setting tol=1.e-12 and x0=ones(11,1),
after 26 iterations Program 6.1 returns the following approximation of the

6.2 The power method
199
eigenvector (of unitary length) associated to the eigenvalue of maximum mod-
ulus of A:
x’ =
Columns 1 through 8
0.5271
0.1590
0.2165
0.3580
0.4690
0.3861
0.1590
0.2837
Columns 9 through 11
0.0856
0.1906
0.0575
The most reachable city is Milan, which is the one associated to the ﬁrst
component of x (the largest in modulus), the least one is Mantua, which is
associated to the last component of x, that of minimum modulus. Of course
our analysis accounts solely for the existence of connections among the cities
but not on how frequent these connections are.
■
6.2.1 Convergence analysis
Since we have assumed that the eigenvectors x1, . . . , xn of A are linearly
independent, they form a basis for Cn. By expanding x(0) and y(0) as
follows
x(0) =
n

i=1
αixi, y(0) = β(0)
n

i=1
αixi,
with β(0) = 1/∥x(0)∥and αi ∈C,
at the ﬁrst step the power method yields
x(1) = Ay(0) = β(0)A
n

i=1
αixi = β(0)
n

i=1
αiλixi
and, similarly,
y(1) = β(1)
n

i=1
αiλixi,
β(1) =
1
∥x(0)∥∥x(1)∥.
At a given step k we will have
y(k) = β(k)
n

i=1
αiλk
i xi,
β(k) =
1
∥x(0)∥· · · ∥x(k)∥
and therefore
y(k) = λk
1β(k)
+
α1x1 +
n

i=2
αi
λk
i
λk
1
xi
,
.
Since |λi/λ1| < 1 for i = 2, . . . , n, the vector y(k) tends to align along
the same direction as the eigenvector x1 when k tends to +∞, provided
α1 ̸= 0.
The previous argument would not apply if either the values |λk
1β(k)α1|
diverge to +∞when |λ1| > 1 or converge to 0 when |λ1| < 1. Fortunately,
this can not happen; actually, we can prove that |λk
1β(k)α1| →1 when
k →∞. As a matter of fact, by an induction argument we have

200
6 Eigenvalues and eigenvectors
0
5
10
15
20
25
30
35
40
-0.2
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
-15
-10
-5
0
5
10
15
20
25
30
35
Figure 6.3. The values y(k)Hx1 (at left) and λ(k) (at right) of Example 6.3,
when k = 1, . . . , 41
β(k) =
1
∥Akx(0)∥=
1
∥λk
1(α1x1 + r(k))∥
for k ≥1,
where r(k) =
n

i=2
αi(λi/λ1)kxi and, since r(k) →0 for k →∞and
∥x1∥= 1, it holds
|λk
1β(k)α1| =
|λk
1α1|
∥λk
1(α1x1 + r(k))∥→1.
Finally, we have y(k)Hx1 = λk
1β(k)α1, so that the values ∥y(k) −
(y(k)Hx1)x1∥behave like the dominant term in the vector r(k), that
is they converge to zero like |λ2/λ1|k for k →∞.
The condition on α1, which is impossible to ensure in practice since
x1 is unknown, is in fact not restrictive. Actually, the eﬀect of roundoﬀ
errors is the appearance of a non-null component along the direction of
x1, even though this was not the case for the initial vector x(0). (We can
say that this is one of the rare circumstances where roundoﬀerrors help
us!)
Example 6.3 Consider the matrix A(α) of Example 6.1, with α = 16. The
eigenvector x1 of unit length associated with λ1 is (1/2, 1/2, 1/2, 1/2)T . Let
us choose (on purpose!) the initial vector (2, −2, 3, −3)T , which is orthogonal
to x1. We report in Figure 6.3 the quantity cos(θ(k)) = (y(k))Hx1 versus k.
We can see that after about 30 iterations of the power method the cosine
tends to 1 and the angle θ(k) tends to 0, while the sequence λ(k) approaches
λ1 = 34. The power method has therefore generated, thanks to the roundoﬀ
errors, a sequence of vectors y(k) whose component along the direction of x1
is increasingly relevant.
■
It is possible to prove that the power method converges even if λ1
is a multiple root of pA(λ). On the contrary it does not converge when

6.3 Generalization of the power method
201
there exist two distinct eigenvalues both with maximum modulus. In
that case the sequence λ(k) does not converge to any limit, rather it
oscillates between two values.
See Exercises 6.1-6.3.
6.3 Generalization of the power method
A ﬁrst possible generalization of the power method consists in applying
it to the inverse of the matrix A (provided A is non singular!). Since the
eigenvalues of A−1 are the reciprocals of those of A, the power method
in that case allows us to approximate the eigenvalue of A of minimum
modulus. In this way we obtain the so-called inverse power method:
given an initial vector x(0), we set y(0) = x(0)/∥x(0)∥and compute
for k = 1, 2, . . .
x(k) = A−1y(k−1), y(k) =
x(k)
∥x(k)∥, μ(k) = (y(k))HA−1y(k)
(6.7)
If A admits n linearly independent eigenvectors, and if also the eigen-
value λn of minimum modulus is distinct from the others, then
lim
k→∞μ(k) = 1/λn,
i.e. (μ(k))−1 tends to λn for k →∞.
At each step k we have to solve a linear system of the form Ax(k) =
y(k−1). It is therefore convenient to generate the LU factorization of A
(or its Cholesky factorization if A is symmetric and positive deﬁnite)
once for all, and then solve two triangular systems at each iteration.
It is worth noticing that the lu command (in MATLAB and in
Octave) can generate the LU decomposition even for complex matrices.
A further generalization of the power method is useful to approximate
the (unknown) eigenvalue of A nearest to a given number μ (either real
or complex). Let λμ denote such eigenvalue and let us deﬁne the shifted
matrix Aμ = A−μI, whose eigenvalues are λ(Aμ) = λ(A)−μ. In order to
approximate λμ, we can at ﬁrst approximate the eigenvalue of minimum
length of Aμ, say λmin(Aμ), by applying the inverse power method to
Aμ, and then compute λμ = λmin(Aμ) + μ. This technique is known as
the power method with shift, and the number μ is called the shift.
In Program 6.2 we implement the inverse power method with shift.
The inverse power method (without shift) is recovered by simply setting
μ = 0.

202
6 Eigenvalues and eigenvectors
The input parameter mu is the shift, while the other parameters are
as in Program 6.1. Output parameters are the approximation of the
eigenvalue λμ of A, its associated eigenvector x and the actual number
of iterations that have been carried out.
Program 6.2. invshift: inverse power method with shift
function [lambda ,x,iter ]= invshift (A,mu ,tol ,nmax ,x0)
% INVSHIFT
Inverse
power method
with
shift
%
LAMBDA=INVSHIFT (A) computes
the
eigenvalue
of A of
%
minimum
modulus
with
the
inverse
power method.
%
LAMBDA=INVSHIFT (A,MU) computes
the eigenvalue
of A
%
closest to the given number (real or
complex ) MU.
%
LAMBDA=INVSHIFT (A,MU ,TOL ,NMAX ,X0) uses
an
absolute
%
error tolerance
TOL (the default is 1.e-6) and a
%
maximum
number of iterations
NMAX (the
default is
%
100),
starting
from
the
initial
vector X0.
%
[LAMBDA ,V,ITER ]= INVSHIFT (A,MU ,TOL ,NMAX ,X0) also
%
returns the
eigenvector
V such
that A*V=LAMBDA*V and
%
the iteration
number at which V was computed .
[n,m]= size (A);
if n ~= m, error(’Only
for
square
matrices ’); end
if nargin == 1
x0 = rand (n ,1);
nmax = 100;
tol = 1.e -06; mu = 0;
elseif
nargin == 2
x0 = rand (n ,1);
nmax = 100;
tol = 1.e -06;
end
[L,U]=lu(A-mu*eye(n));
if norm (x0) == 0
x0 = rand (n ,1);
end
x0=x0/norm (x0);
z0=L\x0;
pro=U\z0;
lambda=x0 ’* pro;
err=tol*abs(lambda )+1;
iter =0;
while err >tol*abs(lambda)& abs(lambda )~=0&iter <= nmax
x = pro; x = x/norm (x);
z=L\x;
pro=U\z;
lambdanew = x’* pro;
err = abs( lambdanew
- lambda );
lambda = lambdanew ;
iter = iter + 1;
end
lambda = 1/ lambda + mu;
return
Example 6.4 Let us apply the inverse power method to compute the mini-
mum modulus eigenvalue of the matrix A(30) deﬁned in Example 6.1. Program
6.2, called by the instruction
[lambda ,x,iter ]= invshift (A(30))
converges in 5 iterations to the value 0.2854.
■
Example 6.5 For the matrix A(30) of Example 6.1 we seek the eigen-
value closest to the value 17. For that we use Program 6.2 with mu=17, tol

6.4 How to compute the shift
203
=10−10 and x0=[1;1;1;1]. After 8 iterations the Program returns the value
lambda=17.82079703055703. A less accurate knowledge of the shift would in-
volve more iterations. For instance, if we set mu=13 the program returns the
value 17.82079703064106 after 19 iterations.
■
The value of the shift can be modiﬁed during the iterations, by setting
μ = λ(k). This yields a faster convergence; however the computational
cost grows substantially since now at each iteration the matrix Aμ does
change and the LU factorization has to be performed at each iteration.
See Exercises 6.4-6.6.
6.4 How to compute the shift
In order to successfully apply the power method with shift we need to
locate (more or less accurately) the eigenvalues of A in the complex
plane. To this end let us introduce the following deﬁnition.
Let A be a square matrix of dimension n. The Gershgorin circles
C(r)
i
and C(c)
i
associated with its ith row and ith column are respectively
deﬁned as
C(r)
i
= {z ∈C : |z −aii| ≤
n

j=1,j̸=i
|aij|},
C(c)
i
= {z ∈C : |z −aii| ≤
n

j=1,j̸=i
|aji|}.
C(r)
i
is called the ith row circle and C(c)
i
the ith column circle.
By the Program 6.3 we can visualize in two diﬀerent windows (that
are opened by the command figure) the row circles and the column
figure
circles of a matrix. The command hold on allows the overlapping of
hold on
hold off
subsequent pictures (in our case, the diﬀerent circles that have been
computed in sequential mode). This command can be neutralized by the
command hold off. The commands title, xlabel and ylabel have
title
xlabel
ylabel
the aim of visualizing the title and the axis labels in the ﬁgure.
The command patch was used in order to color the circles, while the
patch
axis
command axis image sets scaling for the x- and y-axes on the current
plot.
Program 6.3. gershcircles: Gershgorin circles
function
gershcircles (A)
% GERSHCIRCLES
plots the Gershgorin
circles
%
GERSHCIRCLES (A) draws the
Gershgorin
circles
for
%
the square matrix A and its
transpose .
n = size (A);

204
6 Eigenvalues and eigenvectors
if n(1) ~= n(2)
error(’Only
square
matrices ’);
else
n = n(1);
circler = zeros(n ,201);
circlec = circler;
end
center = diag (A);
radiic = sum(abs(A-diag (center )));
radiir = sum(abs(A’-diag (center )));
one = ones (1 ,201);
cosisin = exp(i*[0: pi /100:2* pi ]);
figure (1);
title(’Row circles ’,’Fontsize ’ ,18);
set(gca ,’Fontsize ’ ,18)
xlabel(’Re’); ylabel(’Im’);
figure (2);
title(’Column circles ’,’Fontsize ’ ,18);
set(gca ,’Fontsize ’ ,18)
xlabel(’Re’); ylabel(’Im’);
color =[0.8 ,1 ,1];
for k = 1:n
circlec(k,:) = center(k)*one + radiic(k)* cosisin;
circler(k,:) = center(k)*one + radiir(k)* cosisin;
figure (1);
patch(real (circler (k,:)), imag (circler(k ,:)) ,...
color (1 ,:));
hold on
plot (real (circler (k,:)), imag (circler(k,:)),’k-’ ,...
real (center(k)),imag (center(k)),’kx’);
figure (2);
patch(real (circlec (k,:)), imag (circlec(k ,:)) ,...
color (1 ,:));
hold on
plot (real (circlec (k,:)), imag (circlec(k,:)),’k-’ ,...
real (center(k)),imag (center(k)),’kx’);
end
for k = 1:n
figure (1);
plot (real (circler (k,:)), imag (circler(k,:)),’k-’ ,...
real (center(k)),imag (center(k)),’kx’);
figure (2);
plot (real (circlec (k,:)), imag (circlec(k,:)),’k-’ ,...
real (center(k)),imag (center(k)),’kx’);
end
return
Example 6.6 In Figure 6.4 we have plotted the Gershgorin circles associated
with the matrix
A =
⎡
⎢⎢⎣
30 1
2
3
4 15 −4 −2
−1 0
3
5
−3 5
0 −1
⎤
⎥⎥⎦.
The centers of the circles have been identiﬁed by a cross.
■
As previously anticipated, Gershgorin circles may be used to locate the
eigenvalues of a matrix, as stated in the following proposition.

6.4 How to compute the shift
205
-10
0
10
20
30
40
-10
-5
0
5
10
Row circles
Re
Im
-10
0
10
20
30
40
-10
-5
0
5
10
Column circles
Re
Im
Figure 6.4. Row circles (left) and column circles (right) for the matrix of
Example 6.6
Proposition 6.1 All the eigenvalues of a given matrix A∈Cn×n
belong to the region of the complex plane which is the intersection
of the two regions formed respectively by the union of the row circles
and column circles.
Moreover, should m row circles (or column circles), with 1 ≤m ≤n,
be disconnected from the union of the remaining n −m circles, then
their union contains exactly m eigenvalues.
There is no guarantee that a circle should contain eigenvalues, unless
it is isolated from the others. The information provided by Ghersghorin
circles are in general quite coarse, thus the previous result can provide
only a preliminary guess of the shift.
Note that from Proposition 6.1, all the eigenvalues of a strictly diag-
onally dominant matrix are non-null.
Example 6.7 From the analysis of the row circles of the matrix A(30) of
Example 6.1 we deduce that the real parts of the eigenvalues of A lie between
−32 and 48. Thus we can use Program 6.2 to compute the maximum modulus
eigenvalue by setting the value of the shift μ equal to 48. The convergence
is achieved in 15 iterations, whereas 22 iterations would be required using
the power method with the same initial guess x0=[1;1;1;1] and the same
tolerance tol=1.e-10.
■
Let us summarize
1. The power method is an iterative procedure to compute the eigen-
value of maximum modulus of a given matrix;
2. the inverse power method allows the computation of the eigenvalue
of minimum modulus; to eﬃciently implement the method, it is ad-
visible to factorize the given matrix before iterations start;
3. the power method with shift allows the computation of the eigenvalue
closest to a given number; its eﬀective application requires some a-

206
6 Eigenvalues and eigenvectors
priori knowledge of the location of the eigenvalues of the matrix,
which can be achieved by inspecting the Gershgorin circles.
See Exercises 6.7-6.8.
6.5 Computation of all the eigenvalues
Two square matrices A and B having the same dimension are called
similar if there exists a non singular matrix P such that
P−1AP = B.
Similar matrices share the same eigenvalues. Indeed, if λ is an eigenvalue
of A and x ̸= 0 is an associated eigenvector, we have
BP−1x = P−1Ax = λP−1x,
that is, λ is also an eigenvalue of B and its associated eigenvector is now
y = P−1x.
The methods which allow a simultaneous approximation of all the
eigenvalues of a matrix are generally based on the idea of transforming
A (after an inﬁnite number of steps) into a similar matrix with either
diagonal or triangular form, whose eigenvalues are therefore given by the
entries lying on its main diagonal.
Among these methods we mention the QR method which is imple-
mented in MATLAB in the function eig. More precisely, the command
eig
D=eig(A) returns a vector D containing all the eigenvalues of A. However,
by setting [X,D]=eig(A), we obtain two matrices: the diagonal matrix
D formed by the eigenvalues of A, and a matrix X whose column vectors
are the eigenvectors of A. Thus, A*X=X*D.
The method of QR iterations is called in this way since it makes a re-
peated use of the QR factorization introduced in Section 5.7 to compute
the eigenvalues of the matrix A. Here we present the QR method only
for real matrices and in its most elementary form (whose convergence is
not always guaranteed). For a more complete description of this method
we refer to [QSS07, Chapter 5], whereas for its extension to the complex
case we refer to [GL96, Section 5.2.10] and [Dem97, Section 4.2.1].
The idea consists in building a sequence of matrices A(k), each of
them similar to A. After setting A(0) = A, at each k = 0, 1, . . ., using
the QR factorization we compute the square matrices Q(k+1) and R(k+1)
such that
Q(k+1)R(k+1) = A(k),
whence we set A(k+1) = R(k+1)Q(k+1).

6.5 Computation of all the eigenvalues
207
The matrices A(k), k = 0, 1, 2, . . . are all similar, thus they share
with A their eigenvalues (see Exercise 6.9). Moreover, if A ∈Rn×n and
its eigenvalues satisfy |λ1| > |λ2| > . . . > |λn|, then
lim
k→+∞A(k) = T =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
λ1 t12
. . .
t1n
0 ...
...
...
...
λn−1 tn−1,n
0 . . .
0
λn
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
(6.8)
The rate of decay to zero of the lower triangular coeﬃcients, a(k)
i,j for
i > j, when k tends to inﬁnity, depends on maxi |λi+1/λi|. In practice,
the iterations are stopped when maxi>j |a(k)
i,j | ≤ϵ, ϵ > 0 being a given
tolerance.
Under the further assumption that A is symmetric, the sequence
{A(k)} converges to a diagonal matrix.
Program 6.4 implements the QR iteration method. The input param-
eters are the matrix A, the tolerance tol and the maximum number of
iterations allowed, nmax.
Program 6.4. qrbasic: method of QR iterations
function D=qrbasic (A,tol ,nmax )
%QRBASIC
computes
all the
eigenvalues
of a matrix A.
%
D=QRBASIC(A,TOL ,NMAX ) computes
by QR
iterations
all
%
the eigenvalues
of A within a tolerance
TOL and a
%
maximum
number of iteration
NMAX . The
convergence
of
%
this
method is not always
guaranteed .
[n,m]= size (A);
if n ~= m, error(’The matrix
must be
squared ’); end
T = A; niter = 0;
test = max(max(abs(tril(A , -1))));
while niter
<= nmax & test
> tol
[Q,R]=qr(T);
T = R*Q;
niter = niter + 1;
test = max(max(abs(tril (T , -1))));
end
if niter > nmax
warning ([ ’The method
does
not
converge
’...
’in the
maximum
number of iterations \n’]);
else
fprintf ([ ’The method
converges
in ’ ...
’%i iterations \n’],niter);
end
D = diag (T);
return
Example 6.8 Let us consider the matrix A(30) of Example 6.1 and call Pro-
gram 6.4 to compute its eigenvalues. We obtain
D= qrbasic(A(30) ,1.e -14 ,100)

208
6 Eigenvalues and eigenvectors
The
method converges
in 56
iterations
D =
39.3960
17.8208
-9.5022
0.2854
These eigenvalues are in good agreement with those reported in Example 6.1,
that were obtained with the command eig. The convergence rate decreases
when there are eigenvalues whose moduli are almost the same. This is the
case of the matrix corresponding to α = −30: two eigenvalues have about the
same modulus and the method requires as many as 1149 iterations to converge
within the same tolerance
D= qrbasic(A(-30),1.e-14 ,2000)
The
method converges
in
1149
iterations
D =
-30.6430
29.7359
-11.6806
0.5878
■
A special case is the one of large sparse matrices. In this case, if A is
stored in a sparse mode the command eigs(A,k) allows the computation
eigs
of the k ﬁrst eigenvalues of A having larger modulus.
Example 6.9 (Image compression)
With the MATLAB command A=
imread(’lena’.’jpg’) we upload a black and white JPEG image. (This is
imread
indeed a very popular image as it is commonly used by the scientiﬁc community
to test programs for image compression.) The variable A is a matrix of 512 by
512 eight-bit integer numbers (uint8) that represent the intensity of gray.
By the commands
image(A); colormap (gray (256));
we obtain the ﬁrst image on the left hand of Figure 6.5. To compute the SVD
of A we must ﬁrst convert A in a double precision matrix (the ﬂoating-point
numbers usually used by MATLAB), and then invoke the commands
A=double(A); [U,S,V]= svd(A);
In the middle of Figure 6.5 we report the image that is obtained by using
only the ﬁrst 20 singular values of S, through the commands
k=20; X=U(:,1:k)*S(1:k,1:k)*(V(:,1:k))’;
image(uint8(X)); colormap (gray (256));
The third image on the right-hand of Figure 6.5 is obtained using the ﬁrst 60
singular values. It requires the storage of 61500 coeﬃcients (two matrices of
512 × 60 entries plus the ﬁrst 60 singular values) instead of 262144 coeﬃcients
of the original image.
■
Octave 6.1 The syntax of the imread command in Octave reads
imread(’lena .jpg ’)

6.6 What we haven’t told you
209
Figure 6.5. The original image (left) and those obtained using the ﬁrst 20
(center) and 60 (right) singular values, respectively
Note that it slightly diﬀers from that of MATLAB.
■
Let us summarize
1. The method of QR iterations allows the approximation of all the
eigenvalues of a given matrix A;
2. in its basic version, this method is guaranteed to converge if A has
real coeﬃcients and distinct eigenvalues;
3. its asymptotic rate of convergence depends on the largest modulus
of the ratio of two successive eigenvalues.
See Exercises 6.9-6.10.
6.6 What we haven’t told you
We have not analyzed the issue of the condition number of the eigen-
value problem, which measures the sensitivity of the eigenvalues to the
variation of the entries of the matrix. The interested reader is advised
to refer to, for instance, [Wil88], [GL96] and [QSS07, Chapter 5].
Let us just remark that the eigenvalue computation is not necessarily
an ill conditioned problem when the condition number of the matrix is
large. An instance of this is provided by the Hilbert matrix (see Example
5.10): although its condition number is extremely large, the eigenvalue
computation of the Hilbert matrix is well conditioned thanks to the fact
that the matrix is symmetric and positive deﬁnite.
Besides the QR method, for computing simultaneously all the eigen-
values we can use the Jacobi method which transforms a symmetric
matrix into a diagonal matrix, by eliminating, step-by-step, through
similarity transformations, every oﬀ-diagonal element. This method does
not terminate in a ﬁnite number of steps since, while a new oﬀ-diagonal

210
6 Eigenvalues and eigenvectors
element is set to zero, those previously treated can reassume non-zero
values.
Other methods are the Lanczos method and the method which uses
the so-called Sturm sequences. For a survey of all these methods see
[Saa92].
The MATLAB library ARPACK (available through the command
arpackc) can be used to compute the eigenvalues of large matrices. The
arpackc
MATLAB function eigs is a command that uses this library.
Let us mention that an appropriate use of the deﬂation technique
(which consists in a successive elimination of the eigenvalues already
computed) allows the acceleration of the convergence of the previous
methods and hence the reduction of their computational cost.
6.7 Exercises
Exercise 6.1 Upon setting the tolerance equal to ε = 10−10, use the power
method to approximate the maximum modulus eigenvalue for the following
matrices, starting from the initial vector x(0) = (1, 2, 3)T :
A1 =
⎡
⎣
1 2 0
1 0 0
0 1 0
⎤
⎦, A2 =
⎡
⎣
0.1 3.8 0
1
0 0
0
1 0
⎤
⎦, A3 =
⎡
⎣
0 −1 0
1 0 0
0 1 0
⎤
⎦.
Then comment on the convergence behavior of the method in the three diﬀer-
ent cases.
Exercise 6.2 (Population dynamics) The features of a population of
ﬁshes are described by the following Leslie matrix introduced in Problem 6.2:
i
Age interval (months)
x(0)
i
mi
si
0
0-3
6
0
0.2
1
3-6
12
0.5
0.4
2
6-9
8
0.8
0.8
3
9-12
4
0.3
–
Find the vector x of the normalized distribution of this population for diﬀerent
age intervals, according to what we have seen in Problem 6.2.
Exercise 6.3 Prove that the power method does not converge for matrices
featuring an eigenvalue of maximum modulus λ1 = γeiϑ and another eigen-
value λ2 = γe−iϑ, where i = √−1, γ ∈R \ {0} and ϑ ∈R \ {kπ, k ∈Z}.
Exercise 6.4 Show that the eigenvalues of A−1 are the reciprocals of those
of A.

6.7 Exercises
211
Exercise 6.5 Verify that the power method is unable to compute the maxi-
mum modulus eigenvalue of the following matrix, and explain why:
A =
⎡
⎢⎢⎣
1
3
2
3
2
3
1 0 −1
2
0 0 −5
3 −2
3
0 0
1
0
⎤
⎥⎥⎦.
Exercise 6.6 By using the power method with shift, compute the largest
positive eigenvalue and the negative eigenvalue of largest modulus of
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
3 1 0 0 0 0 0
1 2 1 0 0 0 0
0 1 1 1 0 0 0
0 0 1 0 1 0 0
0 0 0 1 1 1 0
0 0 0 0 1 2 1
0 0 0 0 0 1 3
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
A is the so-called Wilkinson matrix and can be generated by the command
wilkinson(7).
wilkinson
Exercise 6.7 By using the Gershgorin circles, provide an estimate of the
maximum number of the complex eigenvalues of the following matrices:
A =
⎡
⎢⎢⎣
2
−1/2 0 −1/2
0
4
0
2
−1/2
0
6
1/2
0
0
1
9
⎤
⎥⎥⎦,
B =
⎡
⎢⎢⎣
−5
0
1/2 1/2
1/2
2
1/2
0
0
1
0
1/2
0
1/4 1/2
3
⎤
⎥⎥⎦.
Exercise 6.8 Use the result of Proposition 6.1 to ﬁnd a suitable shift for the
computation of the maximum modulus eigenvalue of
A =
⎡
⎢⎢⎣
5
0
1
−1
0
2
0 −1
2
0
1 −1
1
−1 −1 0
0
⎤
⎥⎥⎦.
Then compare the number of iterations as well the computational cost of the
power method both with and without shift by setting the tolerance equal to
10−14.
Exercise 6.9 Show that the matrices A(k) generated by the QR iteration
method are all similar to the matrix A.
Exercise 6.10 Use the command eig to compute all the eigenvalues of the
two matrices given in Exercise 6.7. Then check how accurate are the conclu-
sions drawn on the basis of Proposition 6.1.

7
Numerical optimization
Let f : Rn →R, n ≥1, be a function that we call cost function or
objective function. The problem
min
x∈Rn f(x)
(7.1)
is called unconstrained (or free) optimization problem, whereas
min
x∈Ω f(x)
(7.2)
over a closed proper subset Ω ⊂Rn, is called constrained optimization
problem. The set Ω will be determined by either equality or inequal-
ity constraints that will be dictated by the nature of the problem to
solve. For instance, should we ﬁnd the optimal allocation of n bounded
resources xi (i = 1, . . . , n), the constraints will be expressed by inequal-
ities as 0 ≤xi ≤Ci (with Ci given constants) and the set Ω will be the
subset of Rn determined by the fulﬁlment of the constraints
Ω = {x = (x1, . . . , xn) : 0 ≤xi ≤Ci, i = 1, . . . , n}.
Since computing the maximum of a function f is equivalent to com-
pute the minimum of g = −f, for the sake of simplicity we will only
consider algorithms suitable for minimization problems.
Often, more interesting than the minimum value of the given function
is the point at which that minimum is achieved, that we call minimizer,
the latter of course may not be unique.
This chapter will be essentially devoted to numerical methods for
unconstrained optimization and, at a lesser extent, to that of constrained
optimization.
A. Quarteroni et al., Scientiﬁc Computing with MATLAB and Octave,
Texts in Computational Science and Engineering 2,
DOI 10.1007/978-3-642-45367-0 7, © Springer-Verlag Berlin Heidelberg 2014
213

214
7 Numerical optimization
-5
-4
-3
-2
-1
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
t
fk(t; ak, σk)
 
 
a1 = 0, σ2
1 = 0.3
a2 = 0, σ2
2 = 3
a3 = 1, σ2
3 = 1.5
a4 = −2, σ2
4 = 0.2
Figure 7.1. Gaussian functions
7.1 Some representative problems
Problem 7.1 (Population dynamics) A colony of 250 bacteries liv-
ing in an isolated environment self reproduces according to the so called
Verhulst model
b(t) =
2500
1 + 9e−t/3 ,
t > 0
where t represents the time (expressed in days) past after the start of
the colture (t = 0). We would like to ﬁnd after how many days the
population growth rate is maximum. For the solution of this problem,
see Examples 7.1 and 7.2.
■
Problem 7.2 (Signal analysis) Applications involving automatic vo-
cal identiﬁcation, like those implemented on smartphones, compress the
acoustic signal into a set of parameters that fully characterize it. The
signal intensity is modeled as a sum of m Gaussian functions (also called
peaks)
fk(t; ak, σk) =
1

2πσ2
k
e−(t−ak)2/(2σ2
k),
for k = 1, . . . , m, t ∈[t0, tf],
(7.3)
characterized by 2 coeﬃcients for every peak: the expected value ak of
the kth peak, that is the center of the peak itself, and its variance σ2
k
(see Figure 7.1). The knowledge of the variance of each peak allows
the evaluation of both its height hk = 1/

2πσ2
k and amplitude wk =
2

log 4σ2
k.
We set now
f(t; a, σ) =
m

k=1
fk(t; ak, σk),
(7.4)

7.1 Some representative problems
215
where we have set a = [a1, . . . , am] and σ = [σ1, . . . , σm]. The com-
putation of the vectors a and σ entails the solution of the following
minimization problem
min
a,σ
n

i=1
(f(ti; a, σ) −yi)2,
(7.5)
where (ti, yi), i = 1, . . . , n represent a sampling of our signal. (7.5) is a
nonlinear least squares problem that is solved in Example 7.12.
The model (7.4) is also named Gaussian Mixture Model (GMM) and
is used in statistics for data mining and pattern recognition.
■
Problem 7.3 (Mesh optimization) Consider a given triangulation of
the domain D ⊂R2, as in Figure 7.2, left. We want to modify the position
of vertices of the internal triangles in order to optimize the triangles’
quality in the sense speciﬁed below. Let (x(k)
i
, y(k)
i
) (for i = 0, 1, 2) be
the coordinates of the vertices of the kth triangle Tk. Deﬁne the matrix
Ak =
+
x(k)
1
−x(k)
0
x(k)
2
−x(k)
0
y(k)
1
−y(k)
0
y(k)
2
−y(k)
0
,
and the scalar quantity
μk =
4 det(Ak)
√
3∥AkW−1∥2
F
,
(7.6)
where W = [1, 1/2; 0,
√
3/2] while ∥B∥F =
0*2
i,j=1 b2
ij denotes the
Frobenius norm of the matrix B. Should Tk be equilateral then μk = 1;
the more Tk departs from being an equilateral triangle, the more μk ap-
proaches zero. To optimize our mesh we minimize the function *Ne
k=1 μ−1
k
with respect to the position of the vertices of the internal triangles of
D, under the constraints det(Ak) ≥τ (for a given τ), and that no inver-
sion occurs in the ordering of the nodes ([Mun07]). The solution of this
problem will be given in Example 7.16.
In Figure 7.2 we display the original triangulation and the optimized
one. This kind of problems are faced in the numerical solution of partial
diﬀerential equations by the ﬁnite element method (see Chapter 9).
■
Problem 7.4 (Finance) A given capital is placed in investment funds
whose expected rate of interest is 6%, 10%, and 12%, respectively. The
risk associated with this investment is modeled by a function that de-
pends on the fractions xi of the entire capital invested into the three
funds, the risk probability of the funds, and their correlations
r(x) = 0.04x2
1 + 0.25x2
2 + 0.64x2
3 + 0.1x1x2 + 0.208x2x3.
(7.7)

216
7 Numerical optimization
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Figure 7.2. Mesh of Problem 7.3. At left the original one, at right that
optimized
The goal is to minimize the risk while guaranteeing that the expected
return be equal to 10.4%. The solution of this problem is provided in
Example 7.14.
■
Problem 7.5 (Road network) Consider a network of n roads and p
cross roads as represented in Figure 7.3. Every minute M vehicles travel
through the network; on the jth road the maximum speed limit is vj,m
km/min, moreover no more than ρj,m vehicles per km can transit on the
jth road sj.
We aim at ﬁnding the optimal density ρj (vehicles/km) on the road
sj (with 0 ≤ρj ≤ρj,m) in order to minimize the average travel time from
the inlet (1st node in Figure 7.3) and the outlet (7th node in Figure 7.3).
It is assumed that the speed of vehicles traveling along the jth road
depends on both the maximum speed vj,m and the maximum density
according to the formula vj = vj,m(1 −ρj/ρj,m) km/min. Consequently,
the ﬂowrate of vehicles on the jth street is qj = ρjvj = ρjvj,m(1 −
ρj/ρj,m) vehicles/min. By denoting with tj (in min) the time necessary
to cover the jth road of length Lj km, we ﬁnd tj = Lj/vj = Lj/(vj,m(1−
ρj/ρj,m)) min. The objective function to be minimized is
f(ρ) =
*n
j=1 tjρj
*n
j=1 ρj
.
(7.8)
At every node of the network the vehicles inﬂow should balance the
outﬂow. By denoting with positive sign those incoming in the ith node
(qi,jin) and negative sign those outgoing (qi,jout), the following equations
need to be satisﬁed

jin
qi,jin −

jout
qi,jout = 0
for i = 1, . . . , p.
(7.9)
This is a constrained minimization problem whose constraints are
expressed by both equations and inequalities. See Example 7.17 for its
solution.
■

7.2 Unconstrained optimization
217
in
out
1
2
3
4
5
6
7
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
Figure 7.3. The road network of Problem 7.5
7.2 Unconstrained optimization
When minimizing an objective function one might be interested in ﬁnd-
ing either a local or a global minimizer. A point x∗∈Rn is called a
global minimizer for f if
f(x∗) ≤f(x)
∀x ∈Rn,
while x∗is a local minimizer for f if there exists a ball Br(x∗) ⊂Rn
centered at x∗and with radius r > 0 such that
f(x∗) ≤f(x)
∀x ∈Br(x∗).
We denote by
∇f(x) =
 ∂f
∂x1
(x), . . . , ∂f
∂xn
(x)
T
(7.10)
the gradient of f at point x ∈Rn, provided f is diﬀerentiable in Rn.
Moreover we denote by H(x) the Hessian matrix of f at the point x,
whose elements are
hij(x) = ∂2f(x)
∂xj∂xi
,
i, j = 1, . . . , n,
provided that second derivatives of f at that point x do exist.
If f ∈C2(Rn), that is all ﬁrst and second order derivatives of f exist
and are continuous, than H(x) is symmetric for every x ∈Rn. A point
x∗is called a stationary (or critical) point for f if ∇f(x∗) = 0, a regular
point if ∇f(x∗) ̸= 0.
A function f deﬁned on Rn does not necessarily admit a mini-
mizer; moreover, should such point exist, it is not necessarily unique.
For instance f(x) = x1 + 3x2 is unbounded in R2, while f(x) =

218
7 Numerical optimization
sin(x1) sin(x2) · · · sin(xn) admits an inﬁnite number of minimizers and
maximizers in Rn (either local and global).
The function f : Rn →R is convex if ∀x, y ∈Rn and ∀α ∈[0, 1],
f(αx + (1 −α)y) ≤αf(x) + (1 −α)f(y);
(7.11)
it is Lipschitz continuous if there exists a costant L > 0 such that
|f(x) −f(y)| ≤L∥x −y∥
∀x, y ∈Rn.
(7.12)
The following result holds.
Proposition 7.1 (Optimality conditions) Let x∗∈Rn. If x∗is
a minimizer for f (either local or global) and if there exists r > 0
such that f ∈C1(Br(x∗)), then ∇f(x∗) = 0. Moreover, if f ∈
C2(Br(x∗)), the matrix H(x∗) is positive semideﬁnite.
Viceversa, let r > 0 exist such that f ∈C2(Br(x∗)). If ∇f(x∗) = 0
and H(x) is positive deﬁnite for all x ∈Br(x∗), then x∗is a local
minimizer for f.
Finally, if f is diﬀerentiable and convex in Rn and ∇f(x∗) = 0,
then x∗is a global minimizer for f.
In view of the numerical solution of optimization problems, an ideal
situation is that of a cost function featuring a unique global minimizer.
However, most often there exist several local minimizers. This is why in
this chapter we will describe numerical methods for the approximation
of local minimizers.
Most often, methods for numerical optimization are of iterative type.
They may be classiﬁed into two categories depending on whether or not
they require the knowledge of the derivative of the cost function.
The so called derivative free methods make use of direct compari-
son between values taken by the cost function in order to investigate
its local behavior. Among these methods, some make use of numerical
approximation (tipically, through ﬁnite diﬀerences, see Section 9.2.1) of
the derivatives, see Section 7.3.
Methods using exact derivatives can take advantage of accurate infor-
mation on the local function behaviour to achieve faster convergence to
the minimizer. As a matter of fact, if f is diﬀerentiable at x and ∇f(x)
is diﬀerent than zero, then the maximum growth of f, moving away from
x, occurs along the (signed) direction given by the vector ∇f(x).
Among the minimization methods that make use of exact derivatives,
the two most important classes (based on complementary strategies) are:
descent (or line search methods) and trust region methods that will be
described in Sections 7.5 and 7.6, respectively.

7.3 Derivative free methods
219
7.3 Derivative free methods
In this section we describe two simple numerical methods for the mini-
mization of univariate real valued functions or multivariate real valued
functions along a one-dimensional direction. We will then describe the
Nelder and Mead method for the minimization of functions of several
variables.
7.3.1 Golden section and quadratic interpolation methods
Let f : (a, b) →R be a continuous function featuring a unique minimizer
x∗∈(a, b). We set I0 = (a, b) and for k ≥0 we generate a sequence of
intervals Ik = (a(k), b(k)) of decreasing length, each of those containing
x∗.
For any given k, the next interval Ik+1 is determined as follows. Let
c(k), d(k) ∈Ik, with c(k) < d(k), be two points such that both
b(k) −a(k)
d(k) −a(k) = d(k) −a(k)
b(k) −d(k) = ϕ
(7.13)
and
b(k) −a(k)
b(k) −c(k) = b(k) −c(k)
c(k) −a(k) = ϕ
(7.14)
be the golden ratio ϕ = 1 +
√
5
2
≃1.628.
Thanks to (7.13), (7.14) we ﬁnd
c(k) = a(k) + 1
ϕ2 (b(k) −a(k))
and
d(k) = a(k) + 1
ϕ(b(k) −a(k))
(7.15)
The points c(k) and d(k) are symmetrically distributed about the mid-
point of Ik, that is
a(k) + b(k)
2
−c(k) = d(k) −a(k) + b(k)
2
.
(7.16)
Indeed, if we replace c(k) and d(k) in (7.16) with the correspond-
ing expressions given in (7.15), after dividing by the common factor
(b(k) −a(k))/ϕ2 we obtain the identity ϕ2 −ϕ −1 = 0.
Setting a(0) = a and b(0) = b, the golden section algorithm is formu-
lated as follows (see Figure 7.4): for k = 0, 1, . . . until convergence

220
7 Numerical optimization
a(k)
b(k)
c(k)
d(k)
b(k+1)
a(k+1) c(k+1)
x∗
Lk
ϕLk
Lk+1
ϕLk+1
x
y
f
Figure 7.4. A generic iteration of the golden section method for seeking the
minimizer of the function f; ϕ is the golden ratio, while Lk = c(k) −a(k)
compute c(k) and d(k) through (7.15)
if f(c(k)) ≥f(d(k))
set Ik+1 = (a(k+1), b(k+1)) = (c(k), b(k))
else
set Ik+1 = (a(k+1), b(k+1)) = (a(k), d(k))
endif
(7.17)
From (7.13) and (7.14) it also follows that c(k+1) = d(k) if Ik+1 =
(c(k), b(k)), while d(k+1) = c(k) if Ik+1 = (a(k), d(k)).
A stopping criterion for the previous iterations is
b(k+1) −a(k+1)
|c(k+1)| + |d(k+1)| < ε
(7.18)
where ε is a given tolerance. In the successful case, the midpoint of
the last interval Ik+1 can be taken as an approximation of the desired
minimizer x∗.
Still using (7.13) (or (7.14)) we obtain
|b(k+1) −a(k+1)| = 1
ϕ|b(k) −a(k)| = . . . =
1
ϕk+1 |b(0) −a(0)|,
(7.19)
that is the golden section method converges linearly with a convergence
rate ϕ−1 ≃0.618.
This method is implemented in Program 7.1: fun is either an anony-
mous function or an inline function representing the function f, a and b
are the endpoints of the search interval, tol is the tolerance ε and kmax
is the maximum allowed number of iterations.

7.3 Derivative free methods
221
The output variable xmin contains the value of the minimizer, fmin
the minimum value of f in (a, b), iter the number of iterations carried
out by the algorithm.
Program 7.1. golden: golden section method
function [xmin ,fmin ,iter ]= golden(fun ,a,b,tol ,...
kmax , varargin )
%GOLDEN
Approximates
a minimizer
of 1D-functions
%
XMIN =GOLDEN(FUN ,A,B,TOL ,KMAX ) approximates
a
%
minimizer
of the function
FUN in the interval
%
[A,B] by the golden
section
method.
%
If the search fails , the
program
returns an
%
error message . FUN is either an anonymous
%
function , or an
inline
function , or a function
%
defined in a M-file .
%
XMIN =GOLDEN(FUN ,A,B,TOL ,KMAX ,P1 ,P2 ,...) passes
%
parameters
P1 , P2 ,...
to the
function
%
FUN(X,P1 ,P2 ,...).
%
[XMIN ,FMIN ,ITER ]= GOLDEN(FUN ,...) returns
%
the value of FUN at XMIN
and the
number of
%
iterations
required
to compute
XMIN .
phi =(1+ sqrt (5))/2;
phi1 =1/ phi; phi2 =1/( phi +1);
c=a+phi2 *(b-a); d=a+phi1 *(b-a);
err=tol +1; k=0;
while err >tol & k< kmax
if(fun(c) >= fun(d))
a=c; c=d; d=a+phi1 *(b-a);
else
b=d; d=c; c=a+phi2 *(b-a);
end
k=k+1; err=abs(b-a)/( abs(c)+ abs(d));
end
xmin =(a+b)/2;
fmin =fun(xmin ); iter =k;
if
(iter == kmax & err > tol)
fprintf ([’The golden
section
method
stopped \n’ ,...
’without
converging
to the desired
tolerance
\n’ ,...
’because the maximum
number of iterations
was \n’ ,...
’reached\n’]);
end
Example 7.1 Let us solve Problem 7.1 using the golden section method. The
function f(t) = −b′(t) = −7500et/3/(et/3 + 9)2 admits a global minimizer in
the interval [6, 7] as it appears from its plot. We use Program 7.1 with tolerance
equal to 10−8 using the following instructions:
f=@(t)[ -(7500* exp(t/3))/( exp(t/3) + 9)^2]
a=0; b=10;
tol =1.e -8; kmax =100;
[tmin ,fmin ,iter ]= golden(f,a,b,tol ,kmax )
After 38 iterations we ﬁnd
xmin=6.591673759332620
fmin=-2.083333333333333e+02
The maximum growth rate is of 208.3 bacteria per day and occurs about after
6.59 days since the start of the colture.
■

222
7 Numerical optimization
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
3
−0.5
0
0.5
1
1.5
x(0)
x(1)
x(2)
x(3)
x∗
f(x)
p2(x)
Figure 7.5. The ﬁrst step of the quadratic interpolation method
The quadratic interpolation method is often used as an alternative
to the golden section algorithm.
Let f be a continuous function. Starting from three distinct points
x(0), x(1) and x(2), a sequence of points x(k), with k ≥3, is built in a
way that x(k+1) represents the vertex (and therefore the minimizer) of
the parabola p(k)
2
interpolating f at the points x(k), x(k−1), and x(k−2),
see Figure 7.5:
p(k)
2 (x) = f(x(k−2)) + f[x(k−2), x(k−1)](x −x(k−2))+
f[x(k−2), x(k−1), x(k)](x −x(k−2))(x −x(k−1)).
Here,
f[xi, xj] = f(xj) −f(xi)
xj −xi
,
f[xi, xj, xℓ] = f[xj, xℓ] −f[xi, xj]
xℓ−xi
(7.20)
are the so called Newton divided diﬀerences (see [QSS07, Ch. 8]). By
solving the ﬁrst order equation p(k)
2
′(x(k+1)) = 0 we obtain
x(k+1) = 1
2

x(k−2) + x(k−1) −
f[x(k−2), x(k−1)]
f[x(k−2), x(k−1), x(k)]

(7.21)
We iterate until |x(k+1) −x(k)| < ε for a prescribed tolerance ε > 0.
Provided for every k the divided diﬀerence f[x(k−2), x(k−1), x(k)] does
not vanish, this method converges super-linearly to the minimizer with
a convergence rate p ≃1.3247 (see [Bre02]). Otherwise, the method may

7.3 Derivative free methods
223
not terminate. For this reason the quadratic interpolation method is
tipically used in combination with other methods, such as the golden
section method, whose convergence is always guaranteed.
The command MATLAB fminbnd implements the combination of
fminbnd
these two methods. The calling sintax is x = fminbnd(fun,a,b) where
fun is the function handle associated with the cost function and a, b
represent the endpoints of the interval containing the minimizer. The
output x provides the approximation of the minimizer.
Example 7.2 We use function fminbnd to solve the same problem described
in Example 7.1. We use the following commands:
a=0; b=10;
tol =1.e -8; kmax =100;
[tmin1 ,fmin1 ,exitflag ,output ]= fminbnd(f,a,b ,...
optimset (’TolX ’ ,1.e -8));
Convergence to fmin1= 6.591673708945312 is achieved in 8 iterations, much
fewer than the 38 iterations requested by the golden section method. The
command optimset allows ﬁxing the tolerance to a desired value (tol=1.e-8
optimset
in the current case) diﬀerent than the one that would be otherwise set by
default (tol=1.e-4). The output optional parameters are: fmin1 containing
the evaluation of f at the minimizer, exitflag indicating the termination
state, and output containing the number of iterations carried out as well as
the global number of function evaluations requested by the whole algorithm.
■
As noticed, the two previous methods are genuinely one dimensional,
yet they can be used to solve multidimensional optimization problems
provided they are restricted to the search of optimizers along a given one
dimensional direction (see Section 7.5).
7.3.2 Nelder and Mead method
Let n > 1 and f : Rn →R be a continuous function.
The n−simplex with n + 1 vertices xi ∈Rn (with i = 0, . . . , n) is the
set
S = {y ∈Rn : y =
n

i=0
λixi, λi ∈R and λi ≥0 :
n

i=0
λi = 1}, (7.22)
under the assumption that the n vectors xi−x0 (i = 1, . . . , n) be linearly
independent (S is a segment in R, a triangle in R2 and a tethraedron in
R3).
The method of Nelder and Mead [NM65] is a derivative free mini-
mization algorithm which generates a sequence of simplices {S(k)}k≥0
in Rn that run after or circumscribe the minimizer x∗∈Rn of the cost
function f. It uses the evaluations of f at the simplices’ vertices, as well
as simple geometrical transformations such as reﬂections, expansions and
contractions.

224
7 Numerical optimization
−1
−0.9
−0.8
−0.7
−0.6
−0.5
−0.4
−0.3
−1
−0.9
−0.8
−0.7
−0.6
−0.5
−0.4
−0.3
x(k)
M
x(k)
x(k)
α
x(k)
γ
S(k)
S(k+1)
x∗
Figure 7.6. One step of the Nelder and Mead method, the point x(k)
M
is
replaced by x(k)
α
To generate the initial simplex S(0) we take a point ˜x ∈Rn and a
positive real number η, and set x(0)
i
= ˜x + ηei for i = 0, . . . , n, where
{ei} are the vectors of the standard basis in Rn.
For every k ≥0 (until convergence) we select the “worst” vertex
of S(k)
x(k)
M = argmax
0≤i≤n
f(x(k)
i
)
(7.23)
then replace it by a new point to form the new simplex S(k+1).
The new point is chosen as follows. First we select
x(k)
m = argmin
0≤i≤n
f(x(k)
i
)
and
x(k)
μ
= argmax
0≤i≤n
i̸=M
f(x(k)
i
)
(7.24)
and deﬁne the centroid of the hyperplane H(k) passing through the ver-
tices {x(k)
i
, i = 0, . . . , n, i ̸= M}
x(k) = 1
n
n

i=0
i̸=M
x(k)
i
.
(7.25)
(When n = 2, the centroid is the midpoint of the edge of S(k) opposite
to x(k)
M , see Fig. 7.6.)
Then we compute the reﬂection x(k)
α
of x(k)
M with respect to the hy-
perplane H(k), i.e.
x(k)
α
= (1 −α)x(k) + αx(k)
M ,
(7.26)
where α < 0 is the reﬂection coeﬃcient (tipically, α = −1). The point
x(k)
α
lies on the straight line joining x(k) and x(k)
M , on the side of x(k) far
from x(k)
M (see Fig. 7.6).

7.3 Derivative free methods
225
Now, we compare f(x(k)
α ) with the values of f at the other vertices
of the simplex, before accepting x(k)
α
as the new vertex. Meanwhile, we
try to move x(k)
α
on the straight line joining x(k) and x(k)
M , to set the new
simplex S(k+1). More precisely we proceed as follows.
- If f(x(k)
α ) < f(x(k)
m ), i.e. the reﬂection has produced a new minimum,
we compute
x(k)
γ
= (1 −γ)x(k) + γx(k)
M ,
(7.27)
where γ < −1 (tipically, γ = −2). Then, if f(x(k)
γ ) < f(x(k)
m ), x(k)
M is
replaced by x(k)
γ ; otherwise x(k)
M is replaced by x(k)
α ; then we proceed
by incrementing k by one.
- If f(x(k)
m ) ≤f(x(k)
α ) < f(x(k)
μ ), x(k)
M is replaced by x(k)
α ; then we proceed
by incrementing k by one.
- If f(x(k)
μ ) ≤f(x(k)
α ) < f(x(k)
M ) we compute
x(k)
β
= (1 −β)x(k) + βx(k)
α ,
(7.28)
where β > 0 (tipically, β = 1/2). Now, if f(x(k)
β ) > f(x(k)
M ) deﬁne
the vertices of the new simplex S(k+1) by
x(k+1)
i
= 1
2(x(k)
i
+ x(k)
m )
(7.29)
otherwise x(k)
M is replaced by x(k)
β ; then we proceed by incrementing
k by one.
- If f(x(k)
α ) > f(x(k)
M ) we compute
x(k)
β
= (1 −β)x(k) + βx(k)
M ,
(7.30)
(again β > 0), if f(x(k)
β ) > f(x(k)
M ) deﬁne the vertices of the new
simplex S(k+1) by (7.29), otherwise x(k)
M is replaced by x(k)
β ; then we
proceed by incrementing k by one.
As soon as the stopping criterium
max
i=0,...,n ∥x(k)
i
−x(k)
m ∥∞< ε is ful-
ﬁlled, x(k)
m will be retained as an approximation of the minimizer.
The convergence of Nelder and Mead method is guaranteed in very
special cases only (see example [LRWW99]); in fact a stagnation may oc-
cur in which case the algorithm needs to be restarted. Nevertheless, this
algorithm is quite robust and eﬃcient for small dimensional problems.
Its rate of convergence is severely aﬀected by the choice of the initial sim-
plex. The Nelder and Mead method is implemented by the MATLAB
command fminsearch; its sintax is described in the following example. fminsearch

226
7 Numerical optimization
1
1
10
10
10
10
50
50
50
50
100
100
100
100
100
200
200
200
500
500
500
1000
1000
1500
1500
1
1
10
10
10
10
50
50
50
50
100
100
100
100
100
200
200
200
500
500
500
1000
1000
1500
1500
1
1
10
10
10
10
50
50
50
50
100
100
100
100
100
200
200
200
500
500
500
1000
1000
1500
1500
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
1.5
2
2.5
3
x1
x2
x∗
x∗
Figure 7.7. Contour lines of the Rosenbrock function
Example 7.3 (The Rosenbrock function) The Rosenbrock function
f(x1, x2) = 100(x2 −x2
1)2 + (1 −x1)2,
whose contour lines are displayed in Figure 7.7 ([Ros61]), is often used to test
both eﬃciency and robustness of minimization algorithms. Its global minimum
is attained at the point x∗= (1, 1), however its variation around x∗is fairly
low, making algorithms’ convergence quite problematic. Through the following
command
fun=@(x) 100*(x(2)-x(1)^2)^2+(1 -x(1))^2; x0 =[ -1.2 ,1]
xstar=fminsearch (fun ,x0)
we get
xstar =
1.000022021783570
1.000042219751772
In MATLAB, by replacing the second instruction with the expanded one
[xstar ,fval ,exitflag ,output ]= fminsearch (fun ,x0)
we
would obtain additional information
on the minimum value of f
fval=8.1777e-10, on the number of iterations, output.iterations=85 as well
as the total number of function evaluations output.funcCount=159. Finally,
the error tolerance can be modiﬁed by using the command optimset, as al-
ready discussed in Example 7.2.
■
See Exercises 7.1-7.3.

7.4 The Newton method
227
7.4 The Newton method
Assume that f : Rn →R (n ≥1) is of class C2(Rn) and that we know
how to compute its ﬁrst and second order partial derivatives. We can
apply Newton’s method, already introduced in Chapter 2 for the solution
of the system F(x) = ∇f(x) = 0, whose Jacobian matrix JF(x(k)) is
nothing but the Hessian matrix of F computed at the generic iteration
point x(k). The method reads as follows: for a given x(0) ∈Rn, for
k = 0, 1, . . ., until convergence
solve H(x(k))δx(k) = −∇f(x(k))
set
x(k+1) = x(k) + δx(k)
(7.31)
For a given tolerance ε > 0, a suitable stopping test is ∥x(k+1)−x(k)∥≤ε.
Example 7.4 The function
f(x) = 2
5 −1
10(5x2
1 + 5x2
2 + 3x1x2 −x1 −2x2)e−(x2
1+x2
2)
(7.32)
is displayed in Figure 7.8, right. We apply Newton’s method to approximate
its global minimizer x∗≃(−0.63065832, −0.7007420) (rounded to the ﬁrst 7
signiﬁcant digits). We take x(0) = (−0.9, −0.9) and tolerance ε = 10−5. After 5
iterations the method (7.31) converges to x=[-0.63058;-0.70074]. Should we
have chosen x(0) = (−1, −1), after 400 iterations the stopping test would not
be fulﬁlled. In fact a necessary condition for convergence of Newton’s method
is that x(0) should be suﬃciently close to the minimizer x∗(see Section 2.3).
This is known as local convergence of Newton’s method.
The reader should be aware that Newton’s method may converge to any
stationary point (not necessarily to a minimizer). For instance, by taking
x(0) = (0.5, −0.5) after 5 iterations the method converges to the saddle point
x=[0.80659; -0.54010].
■
A general convergence criterium for the method (7.31) is as follows:
if f ∈C2(Rn), x∗is a stationary point, the Hessian matrix H(x∗) is
positive deﬁnite, the components of H(x) are Lipschitz continuous in a
neighbourhood of x∗and x(0) is suﬃciently close to x∗, then the Newton
method (7.31) converges quadratically to x∗(see, for instance [SY06,
pag. 132], [NW06]).
In spite of its simple implementation, Newton’s method is computa-
tionally demanding when n is large (as it requires the analytic expression
of the derivatives and, at each iteration, the computation of both the gra-
dient and the Hessian matrix of f). Besides, x(0) has to be chosen close
enough to x∗.
A suitable strategy to build up eﬃcient and robust minimization al-
gorithms relies on combining locally convergent with globally convergent
methods, as described in the next section.

228
7 Numerical optimization
−4
−2
0
2
4
−4
−3
−2
−1
0
1
2
3
4
∇f(x(k))
d(k)
−2
−1
0
1
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
x1
x2
x(k)
d(k)
−∇f(x(k))
x(k)
min
Figure 7.8. At left, countour lines of a function f(x), its gradient evaluated
at x(k) and a suitable descent direction d(k). At right, the restriction of the
function f(x) (7.32) along a descent direction d(k) and the minimizer x(k)
min
along d(k)
7.5 Descent (or line search) methods
In this Section we assume for simplicity that f ∈C2(R) and is bounded
from below.
Descent methods (also known as line search methods) are iterative
methods in which, for every k ≥0, x(k+1) depends on x(k), on a vector
d(k) depending on ∇f(x(k)) and on a suitable parameter αk ∈R. Given
x(0) ∈Rn, the method reads as follows:
for k = 0, 1, . . . , until convergence
ﬁnd a direction d(k) ∈Rn
compute the step αk ∈R
set x(k+1) = x(k) + αkd(k)
(7.33)
The vector d(k) must be a descent direction, meaning that
d(k)T ∇f(x(k)) < 0
if ∇f(x(k)) ̸= 0,
d(k) = 0
if ∇f(x(k)) = 0.
(7.34)
The name descent direction arises from the property that the vector
∇f(x(k)) provides in Rn the direction with sign of maximum positive
growth of f moving from x(k). As d(k)T ∇f(x(k)) represents the direc-
tional derivative of f along d(k), the ﬁrst condition in (7.34) ensures that
we are moving along a direction opposite to the gradient, that is towards
a minimizer of f, as displayed in Figure 7.8.
Some popular descent directions will be reported in the next Section.

7.5 Descent (or line search) methods
229
Once d(k) is determined, the optimum value of αk ∈R is the one that
guarantees the maximum variation of f along d(k) and can therefore be
computed by solving a one-dimensional minimization problem (that is
minimizing the restriction of f along d(k)), see Figure 7.8.
However, as the computation of αk is quite involved when f is not
a quadratic function, we will report in Section 7.5.2 some alternative
techniques aimed at approximating αk.
7.5.1 Descent directions
The most widely used descent directions are:
1. Newton’s directions
d(k) = −(H(x(k)))−1∇f(x(k))
(7.35)
2. quasi-Newton directions
d(k) = −H−1
k ∇f(x(k))
(7.36)
where Hk represents an approximation of the true Hessian matrix
H(x(k)). This choice is a valuable alternative to Newtons’ method
when second derivatives of f are heavy to compute (see Section
7.5.4);
3. gradient directions
d(k) = −∇f(x(k))
(7.37)
(they can be regarded as a trivial example of quasi-Newton direc-
tions);
4. conjugate gradient directions
d(0) = −∇f(x(0))
d(k+1) = −∇f(x(k+1))−βkd(k), k ≥0
(7.38)
The coeﬃcients βk can be chosen according to diﬀerent criteria, see
Section 7.5.5, however they coincide with those of Conjugate Gra-
dient method for linear systems (see (5.66)) when f is a quadratic
function.
The descent direction (7.37) fulﬁlls the condition (7.34), then (7.35) and
(7.36) assure that H(x(k)) and Hk, respectively, are positive deﬁnite ma-
trices. The vectors (7.38) fulﬁll (7.34) provided that the coeﬃcients βk
are suitably chosen, as we will see in Section 7.5.5.

230
7 Numerical optimization
Newton
descent Newton
descent grad
descent GC−PR
descent GC−FR
descent quasi−Newton
Newton
descent Newton
descent grad,
quasi−Newton, GC
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x(0)
1
x(0)
2
Figure 7.9. Convergence history of Newton’s and descent methods for the
function of the Example 7.5
Example 7.5 Consider again the function f(x) (7.32), featuring two local
minimizers, one local maximizer and two saddle points. See Figure 7.8, right.
We compare the sequences {x(k)} generated by Newton’s method (7.31) and
descent methods with descent directions given by (7.35)–(7.38).
Consider ﬁrst x(0)
1
= (0.5, −0.5) as initial point. In Figure 7.9 we see that
Newton’s method (7.31) converges to the saddle point (.8065, −.5401); the de-
scent method with Newton direction (7.35) breaks down at the second iteration
as it generates a matrix H(x(1)) which is not deﬁnite positive. (See Remark 7.2
on how to overcome this drawback.) The other descent methods with direc-
tions given by (7.36), (7.37), and (7.38) (for the latter, two diﬀerent criteria for
the determination of the parameters βk have been used, named GC-FR and
GC-PR, see Section 7.5.5) converge to the local minimizer (−0.6306, −0.7007).
The faster convergence is achieved in 9 iterations using quasi-Newton direc-
tions (7.36), see the blue path in Figure 7.9. By choosing a diﬀerent initial
point x(0)
2
= (0.4, 0.5), the Newton method diverges while method (7.35), even
though it shares the same ﬁrst descent direction with Newton’s method, builds
up a short steplength αk which then allows convergence to the local minimizer
(0.8095, 0.7097) in only 4 iterations. All the other descent methods with di-
rections (7.36), (7.37), and (7.38) converge in 10 to 15 iterations to the same
local minimizer.
■
The choice of the steplength αk will be discussed in Section 7.5.2,
while the analysis of diﬀerent descent directions is deferred to Sections
7.5.3–7.5.5.

7.5 Descent (or line search) methods
231
7.5.2 Strategies for choosing the steplength αk
Once the descent direction d(k) is chosen, the steplength αk has to be
determined in such a way that the new iterate x(k+1) be the minimizer
of f along such a direction, that is
αk = argmin
α∈R
f(x(k) + αd(k)).
A second order Taylor expansion of f around x(k) yields
f(x(k) + αd(k)) = f(x(k)) + αd(k)T ∇f(x(k))+
α2
2 d(k)T H(x(k))d(k) + o(∥αd(k)∥2).
(7.39)
In the special case in which f is a quadratic function, that is
f(x) = 1
2xT Ax −xT b + c
with A ∈Rn×n, b ∈Rn symmetric and positive deﬁnite and c ∈R,
the expansion in (7.39) is exact, that is the inﬁnitesimal residual is null.
As H(x(k)) = A for every k ≥0 and ∇f(x(k)) = Ax(k) −b = −r(k)
(see (5.35)), by diﬀerentiating (7.39) with respect to α and setting the
derivative equal to zero we obtain
αk =
d(k)T r(k)
d(k)T Ad(k)
(7.40)
In the case (7.37), we ﬁnd d(k) = r(k) thus we obtain the well known
gradient method described in Chapter 5, which obeys the convergence
estimate (5.59).
Instead, should the direction d(k) be chosen as in (7.38), by setting
βk = −(Ad(k))T r(k+1)
d(k)T Ad(k)
(7.41)
we would recover the conjugate gradient method (5.66) for linear systems
which fulﬁlls the convergence estimate (5.67).
If f is a generic (non quadratic) function, the computation of the
optimal αk would require an iterative method to solve numerically the
above minimization problem along the direction d(k). In these cases an
approximate (rather than exact) value of αk can be chosen by requiring
that the new iterate x(k+1) deﬁned in (7.33) ensures that
f(x(k+1)) < f(x(k)).
(7.42)

232
7 Numerical optimization
0
0.2
0.4
0.6
0.8
1
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
f(x(k) + αd(k))
f(x(k)) + σα(d(k))T ∇f(x(k))
α
0
0.2
0.4
0.6
0.8
1
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
α
(d(k))T ∇f(x(k))
δ(d(k))T ∇f(x(k))
Figure 7.10. At left, the terms comparing in the ﬁrst inequality in (7.43)
when σ = 0.2. (7.43)1 is satisﬁed for those values of α providing the continuous
lightblue line. At right, some straightlines with slope δd(k)T ∇f(x(k)) and δ =
0.9, (7.43)2 is fulﬁlled for those α corresponding to the continuous lightblue
line. The Wolfe conditions are simultaneously fulﬁlled when either 0.23 ≤α ≤
0.41 or 0.62 ≤α ≤0.77
In this respect, a natural strategy is that of assigning αk a large value
and then reducing it iteratively until when (7.42) is satisﬁed. Unfortu-
nately, this strategy does not guarantee that the associated sequence
{x(k)} converges to the desired minimizer x∗. See Exercise 7.4 and the
associated Figure 10.8, left, where steplengths are too long. See also Ex-
ercise 7.5 and the associated Figure 10.8, right, where steplengths are
now too short.
A better criterium for the choice of αk > 0 is the one based on the
Wolfe’s conditions:
f(x(k) + αkd(k)) ≤f(x(k)) + σαkd(k)T ∇f(x(k))
d(k)T ∇f(x(k) + αkd(k)) ≥δd(k)T ∇f(x(k))
(7.43)
where σ and δ, such that 0 < σ < δ < 1, are two given constants
and d(k)T ∇f(x(k)) represents the directional derivative of f along the
direction d(k).
The ﬁrst inequality in (7.43) is named Armijo’s rule, and it inhibits
too little variations of f with respect to the steplength αk (see Figure
7.10, left). More precisely, the larger αk the higher the variation of f.
The second Wolfe condition states that at the new point x(k)+αkd(k)
the value of the directional derivative of f should be larger than δ times
the same derivative at the previous value x(k) (see Figure 7.10, right).
From the example depicted in Figure 7.10 one can see that Wolfe’s
conditions might also be fulﬁlled far from the minimizer of f along d(k)
and even when the directional derivative of f takes large values. More
restricitive conditions than (7.43) are the strong Wolfe’s conditions

7.5 Descent (or line search) methods
233
0
0.2
0.4
0.6
0.8
1
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
slope= ±δ(d(k))T ∇f(x(k))
f(x(k)) + σα(d(k))T ∇f(x(k))
α
Figure 7.11. The strong Wolfe’s conditions (7.44) are fulﬁlled when α belongs
to small intervals around the minimizers, in correspondence with the thick
lightblue piece of curve. σ = 0.2 and δ = 0.9 have been considered.
f(x(k) + αkd(k)) ≤f(x(k)) + σαkd(k)T ∇f(x(k)),
|d(k)T ∇f(x(k) + αkd(k))| ≤−δd(k)T ∇f(x(k))
(7.44)
being 0 < σ < δ < 1 suitable ﬁxed constants.
The ﬁrst condition is the same as in (7.43), whereas the second
one gives rise to (7.43)2 as well as to d(k)T ∇f(x(k) + αkd(k)) ≤
−δd(k)T ∇f(x(k)) (having recalled that the right hand side of (7.44)2
is positive because of (7.34)1). Conditions (7.44)2 inhibits f to vary too
strongly at x(k) + αkd(k) (see Figure 7.11 for an example).
It can be proved (see, e.g., [NW06, Lemma 3.1]) that if d(k) is a
descent direction in x(k) and f ∈C1(Rn) is lower bounded in the set
{x(k) + αd(k), α > 0}, then for every σ, δ such that 0 < σ < δ < 1,
there exist intervals of steplengths αk satisfying (7.43) and (7.44).
In practice, σ is chosen very small, e.g. σ = 10−4 ([NW06]), while
δ is large (δ = 0.9) for Newton, quasi-Newton and gradient directions,
small (δ = 0.1) for the conjugate gradient directions.
A simple strategy to determine the steplength αk satisfying Wolfe’s
conditions is backtracking: it consists of starting with α = 1 and then
reducing it by a prescribed factor ρ (tipically, ρ ∈[1/10, 1/2)) until when
the ﬁrst condition (7.43) is satisﬁed. In pseudocode: for a given x(k) and
a descent direction d(k), for σ ∈(0, 1), ρ ∈[1/10, 1/2)

234
7 Numerical optimization
set α = 1
while f(x(k) + αd(k)) > f(x(k)) + σαd(k)T ∇f(x(k))
α = αρ
end
set αk = α
(7.45)
The second condition in (7.43) is never checked because the back-
tracking technique intrinsically computes steplengths that are not too
small.
Remark 7.1 The backtracking technique is often combined with replacing f
by a quadratic or cubic interpolant of f along d(k). The chosen steplength αk
yields a new point x(k+1) which represents the minimizer of the interpolant of
f along d(k). The corresponding algorithm is named quadratic or cubic line
search, respectively. See [NW06, Ch. 3] for further details on this approach. ■
The Program backtrack 7.2 implements the strategy (7.45). Param-
eters fun and grad are function handles rispectively associated with the
functions f(x) and ∇f(x); xk and dk respectively contain the point x(k)
and the descent direction d(k), while sigma and rho contain the param-
eter values σ and ρ. When sigma and rho are not speciﬁed, the default
values σ = 10−4 and ρ = 1/4 are set. The output variable x contains the
new point x(k+1).
Program 7.2. backtrack: backtracking strategy
function [x,alphak ]= backtrack (fun ,xk ,gk ,dk ,varargin )
% BACKTRACK
Backtracking
strategy
for line
search.
%
[X,ALPHAK] = BACKTRACK (FUN ,XK ,GK ,DK) computes
the
%
new point x_{k+1}= x_k+alpha_k d_k , where alpha_k
%
is determined
by the backtracking
technique
%
with
sigma=1.e-4 and rho =1/4.
%
[X,ALPHAK] = BACKTRACK (FUN ,XK ,GK ,DK ,SIGMA ,RHO)
%
allows to specify
the parameters
sigma and rho.
%
Tipically
1.e-4<sigma <0.1
and 1/10< rho
<1/2.
%
FUN is the function
handle
associated
with
the cost
%
function . XK , GK , and DK contain
respectively
the
%
point x_k , the
gradient
of f at x_k and the
%
descent
direction
d_k.
if nargin ==4
sigma =1.e-4; rho =1/4;
else
sigma=varargin {1};
rho=varargin {2};
end
alphamin =1.e-5; % minimum
value allowed for
alpha_k
alphak = 1; fk = fun(xk);
k=0; x=xk+alphak*dk;
while fun(x)>fk+sigma*alphak*gk ’*dk & alphak >alphamin
alphak = alphak*rho;
x = xk+alphak*dk; k = k+1;
end

7.5 Descent (or line search) methods
235
The Program descent 7.3 implements the descent method (7.33)
with directions (7.35)–(7.38) and steplengths αk determined according
to the backtracking strategy. The stopping criterium is (see [JS96])
max
1≤i≤n

[∇f(x(k+1))]i max{|x(k+1)
i
|, typ(xi)}
max{|f(x(k+1))|, typ(f(x))}
 ≤ε
(7.46)
for a given ε > 0, where typ(x) is a characteristic value expressing the
order of magnitude of the x variable. Its presence prevents test failure
when either x∗or f(x∗) are null.
Parameters fun and grad are function handles associated with f(x)
and ∇f(x), respectively, x0 contains the initial value of the sequence,
tol the tolerance of the stopping criterium and kmax the maximum al-
lowed number of iterations. The variable meth sorts the descent direction:
Newton’s directions correspond to meth=1, quasi-Newton’s to meth=2,
gradient directions to meth=3, while meth=41, 42, 43 select three dif-
ferent directions of the conjugate gradient: CG-FR, CG-PR, and CG-HS,
respectively, as we will see in Section 7.5.5.
Program 7.3. descent: descent method
function [x,err ,iter ]= descent (fun ,grad ,x0 ,tol ,kmax ,...
meth ,varargin )
%DESCENT
Descent
method for
optimization
%
[X,ERR ,ITER ]= DESCENT(FUN ,GRAD ,X0 ,TOL ,KMAX ,METH ,HESS )
%
computes a local minimizer
of function
FUN by the
%
descent
method
with
Newton
directions
(METH =1),
%
quasi -Newton
directions
(BFGS ) (METH =2),
gradient
%
directions
(METH =3) or
conjugate
gradient
directions
%
with
Fletcher
and Reeves beta_k (METH =41),
%
Polak and Ribiere
beta_k (METH =42),
%
Hestenes
and
Stiefel
beta_k (METH =43).
%
The steplength
is computed
by the backtracking
%
technique .
FUN , GRAD
and HESS (the
latter being
%
used
only
if METH =1) are
function
handles
associated
%
with
the cost
function , its
gradient
and its Hessian
%
matrix , respectively . If METH =2, HESS
is a matrix
%
approximating
the Hessian of FUN at the
initial
%
point X0. TOL is the tolerance
for the
stopping
%
test , while KMAX is the
maximum
allowed
number of
%
iterations . The
function
backtrack
is called
inside.
if nargin >6
if meth ==1,
hess =varargin {1};
elseif
meth ==2, H=varargin {1};
end
end
err=tol +1; k=0; xk=x0 (:);
gk=grad (xk); dk=-gk;
eps2 =sqrt (eps );
while err >tol & k< kmax
if meth ==1;
H=hess (xk); dk=-H\gk; % Newton
elseif
meth ==2
dk=-H\gk;
% BFGS
elseif
meth ==3
dk=-gk;
% gradient

236
7 Numerical optimization
end
[xk1 ,alphak ]= backtrack (fun ,xk ,gk ,dk);
gk1=grad (xk1 );
if meth ==2 % BFGS
update
yk=gk1 -gk; sk=xk1 -xk; yks=yk ’*sk;
if yks > eps2*norm (sk)* norm(yk)
Hs=H*sk;
H=H+(yk*yk ’)/yks -(Hs*Hs ’)/( sk ’*Hs);
end
elseif meth >=40 % CG update
if meth == 41
betak=-(gk1 ’* gk1 )/(gk ’* gk); % FR
elseif
meth
== 42
betak=-(gk1 ’*(gk1 -gk ))/( gk ’*gk); % PR
elseif
meth
== 43
betak=-(gk1 ’*(gk1 -gk ))/( dk ’*(gk1 -gk )); % HS
end
dk=-gk1 -betak*dk;
end
xk=xk1; gk=gk1; k=k+1; xkt=xk1;
for i=1: length(xk1 ); xkt(i)=max ([ abs(xk1(i)) ,1]);
end
err=norm (( gk1 .*xkt )/ max([ abs(fun(xk1 )),1]), inf);
end
x=xk; iter =k;
if (k== kmax & err > tol)
fprintf ([’Descent
method stopped \n’ ,...
’without
converging
to the desired
tolerance
\n’ ,...
’because the maximum
number of iterations
was \n’ ,...
’reached\n’]);
end
Example 7.6 Consider again function f(x) (7.32). To approximate its global
minimizer (−0.6306, −0.7007), we use the diff command introduced in Section
1.5.3 for the symbolic computation of both the gradient of f and the Hessian
matrix H of f. Then we deﬁne the function handles f, grad f, and hess
respectively associated with f, ∇f, and H and call the Program 7.3 with the
following instructions:
x0 =[0.5; -0.5]; tol =1.e-5;
kmax =200;
meth =1;
% Newton ’s directions
[x1 ,err1 ,k1 ]= descent (f,grad_f ,x0 ,tol ,kmax ,meth ,hess );
meth =2; hess =eye (2); % quasi -Newton
directions
[x2 ,err2 ,k2 ]= descent (f,grad_f ,x0 ,tol ,kmax ,meth ,hess );
meth =3; % gradient
directions
[x3 ,err3 ,k3 ]= descent (f,grad_f ,x0 ,tol ,kmax ,meth );
meth =42; % CG -PR
directions
[x4 ,err4 ,k4 ]= descent (f,grad_f ,x0 ,tol ,kmax ,meth );
We choose x(0) = (0.5, −0.5), tolerance 10−5 and maximum number of itera-
tions equal to 200 and obtain these results:
descent
Newton k=200,
x=[ 7.7015e-01 , -6.3212 e-01]
descent
quasi -Newton k=9, x=[ -6.3058 e-01 , -7.0075 e-01]
descent
gradient k=17,
x=[ -6.3058 e-01 , -7.0075 e-01]
descent CG -PR k=17,
x=[ -6.3060 e-01 , -7.0073 e-01]
Note that the descent method with Newton’s direction has not achieved conver-
gence because directions can be generated that do not fulﬁll condition (7.34).
■

7.5 Descent (or line search) methods
237
In the next sections we indicate how to compute the approximate Hes-
sian matrices Hk and the parameters βk in (7.36) and (7.38). Moreover,
we will comment on the convergence properties of the various methods
introduced so far.
7.5.3 The descent method with Newton’s directions
Consider a lower bounded function f ∈C2(Rn) and the descent method
(7.33) with Newton’s descent directions (7.35) and steplengths αk fulﬁll-
ing the Wolfe’s conditions (7.43).
Assume that for every k ≥0 the Hessian matrix H(x(k)) in (7.35),
besides being symmetric thanks to the assumption on f, is positive def-
inite. Moreover, setting Bk = H(x(k)) we suppose that
∃M > 0 : K(Bk) = ∥Bk∥∥B−1
k ∥≤M
∀k ≥0.
(7.47)
(Note that K(Bk) coincides with the spectral condition number of Bk,
see (5.31).)
Then the sequence x(k) generated by (7.33) converges to a stationary
point x∗of f. Moreover, by choosing αk = 1 from a given k on (that is
when we are suﬃciently close to x∗) the convergence order is quadratic.
See [NW06, Thm. 3.2] for the proof.
Remark 7.2 Since the Hessian matrices are positive deﬁnite, the stationary
point x∗must necessarily be a minimizer.
However, should H(x(k)) fail to be positive deﬁnite for a given k, the corre-
sponding d(k) in (7.35) could fail to be a descent direction and the Wolfe condi-
tions might become meaningless. To overcome this problem the Hessian matrix
could be replaced by Bk = H(x(k))+Ek for a suitable matrix Ek (either diago-
nal or not) in such a way that Bk is positive deﬁnite and d(k) = −B−1
k ∇f(x(k))
turns out to be a descent direction.
■
The descent method with Newton’s directions is implemented in Pro-
gram 7.3.
Example 7.7 Let us compute the global minimizer of the function f(x)
(7.32) by using the descent method (7.33), with the Newton’s directions
(7.35) and steplengths αk satisfying the Wolfe conditions. We use a toler-
ance ε = 10−5 for the stopping criterium and we start from x(0) = (−1, −1).
By using Program 7.3 with meth=1, after 4 iterations, we have convergence
to x=[-0.63058;-0.70074]. Choosing instead x(0) = (0.5, −0.5), the method
stagnates as H(x(0)) is not positive deﬁnite, yielding a vector d(0) which is not
a descent direction; consequently, the backtracking technique is unable to ﬁnd
a value α0 > 0 that fulﬁlls the Wolfe conditions.
■

238
7 Numerical optimization
7.5.4 Descent methods with quasi-Newton directions
When using the directions (7.36) we need a strategy to build Hk. For
a given symmetric and positive deﬁnite matrix H0, a popular recursive
technique is that based on the so called rank-one update of Broyden’s
method (2.19) for the solution of nonlinear systems. The matrices Hk
are required:
–
to satisfy the secant condition
Hk+1(x(k+1) −x(k)) = ∇f(x(k+1)) −∇f(x(k));
–
to be symmetric, as H(x);
–
to be positive deﬁnite to guarantee that vectors d(k) are descent
directions;
–
to satisfy the condition
lim
k→∞
∥(Hk −H(x∗))d(k)∥
∥d(k)∥
= 0,
which, from one hand, ensures that Hk is a good approximation of
H(x∗) along the descent direction d(k) and, from the other hand,
guarantees a super-linear rate of convergence.
The strategy due to Broyden, Fletcher, Goldfarb, and Shanno (BFGS)
is based on the following recursivity relationship
Hk+1 = Hk + y(k)y(k)T
y(k)T s(k) −Hks(k)s(k)T Hk
s(k)T Hks(k)
(7.48)
where s(k) = x(k+1) −x(k) and y(k) = ∇f(x(k+1))−∇f(x(k)). These ma-
trices are symmetric and positive deﬁnite under the condition y(k)T s(k) >
0, which is fulﬁlled provided the steplengths αk satisfy the Wolfe condi-
tions (either (7.43) or (7.44)). See [JS96].
The corresponding BFGS method (implemented in Program 7.3) can
be summarized as follows: for a given x(0) ∈Rn and a suitable symmetric
and positive deﬁnite matrix H0 ∈Rn×n which approximates H(x(0)), for
k = 0, 1, . . . , until convergence:
solve
Hkd(k) = −∇f(x(k))
compute αk satisfying Wolfe’s conditions
set
x(k+1) = x(k) + αkd(k)
s(k) = x(k+1) −x(k)
y(k) = ∇f(x(k+1)) −∇f(x(k))
compute Hk+1 using (7.48)
(7.49)

7.5 Descent (or line search) methods
239
Under the condition that f ∈C2(Rn) is lower bounded and the matri-
ces Hk are positive deﬁnite with a condition number uniformly bounded
(see (7.47)), the BFGS method converges to a minimizer with (super-
linear) convergence order p ∈(1, 2) (see for instance [JS96, NW06]).
Example 7.8 We apply the BFGS method (7.49) to compute the minimizer
of the (yet another time) function f(x) (7.32). We choose ε = 10−5 for
the stopping criterium and H0 equal to the identity matrix (which is obvi-
ously symmetric and positive deﬁnite). The latter choice is more convenient
than choosing H0 = H(x(0)), i.e. the exact Hessian in x(0), as it yields a
faster convergence. Program 7.3 with meth=2 and hess=eye(2) converges to
x=[-0.63058;-0.70074] in 6 iterations if x(0) = (−1, −1) and in 9 iterations
if x(0) = (0.5, −0.5).
■
Remark 7.3 As in Broyden method (2.19), the computational cost of order
O(n3) for the calculation of d(k) = −H−1
k ∇f(x(k)) can be reduced to order
O(n2), by using QR factorizations of Hk (see [GM72]).
An alternative strategy is based on the use of the inverse Hk of Hk both
in (7.48) and (7.49). This strategy can be implemented in order of O(n2)
operations per step, however in practice it is less stable than the more standard
(7.48).
■
The BFGS method (as well as several other minimization methods)
is implemented in the MATLAB function fminunc included in the op-
fminunc
timization toolbox. By the following commands:
fun=@(x) 100*(x(2)-x(1)^2)^2+(1 -x(1))^2; x0 =[1.2; -1];
options = optimset (’LargeScale ’,’off’);
[x,fval ,exitflag ,output ]= fminunc (fun ,x0 ,options )
the function fminunc computes the minimizer of the Rosenbrock function
using the BFGS method (which corresponds to using the value ’off’
to initialize the option ’LargeScale’). The output parameters have the
same meaning as those of the function fminsearch described in Example
7.3. Convergence is achieved in 24 iterations with a tolerance ε = 10−6;
this has required 93 function evaluations.
With the previous options the gradient of the function f is approxi-
mated in fminunc by using ﬁnite diﬀerence methods (see Section 9.2.1).
However, in case an exact expression of the gradient of f is available, it
can be passed to the function as follows:
fun=@(x) 100*(x(2)-x(1)^2)^2+(1 -x(1))^2; x0 =[1.2; -1];
grad_fun =@(x)[ -400*( x(2)-x(1)^2)* x(1) -2*(1 -x(1));
200*(x(2)-x(1)^2)];
options = optimset (’LargeScale ’,’off’,’GradObj ’,’on’);
[x,fval ,exitflag ,output ]= fminunc ({fun ,grad_fun },...
x0 ,options )
Note the changement in the command options. Convergence is achieved
in 25 iterations with 32 function evaluations.

240
7 Numerical optimization
Octave 7.1 The BFGS method is implemented in the Octave function
bfgsmin. The Octave command fminunc instead implements the trust
bfgsmin
region method that we describe in Section 7.6.
■
7.5.5 Gradient and conjugate gradient descent methods
Let us consider the descent method (7.33) with gradient directions (7.37).
As already noticed, the latter are descent directions.
If f ∈C2(Rn) is lower bounded and the steplengths αk satisfy Wolfe’s
conditions, this method converges linearly to a steady point ([NW06]).
See Program 7.3 for its implementation.
Example 7.9 We consider once more the function (7.32). We ﬁx the tol-
erance ε = 10−5 for the stopping criterium and call Program 7.3 setting
meth=3 (this corresponds to gradient directions). Choosing x(0) = (−0.9, −0.9),
x(0) = (−1, −1) and x(0) = (0.5, −0.5), the method converges to the global
minimizer x=[-0.63058;-0.70074] in 11, 12, and 17 iterations, respectively.
Choosing instead x(0) = (0.9, 0.9), which is closer to the local minimizer
x∗= (.8094399, .7097390), the method converges to the latter in 21 iterations.
■
Consider now the conjugate gradient directions (7.38). Several op-
tions are available for the choice of βk (see for instance [SY06, NW06]).
Among those we quote the following:
1. Fletcher–Reeves (1964)
βF R
k
= −∥∇f(x(k))∥2
∥∇f(x(k−1))∥2
(7.50)
2. Polak–Ribi`ere (1969) (also known as Polak–Ribi`ere–Polyak parame-
ters)
βP R
k
= −∇f(x(k))T (∇f(x(k)) −∇f(x(k−1)))
∥∇f(x(k−1))∥2
(7.51)
3. Hestenes–Stiefel (1952)
βHS
k
= −∇f(x(k))T (∇f(x(k)) −∇f(x(k−1)))
d(k−1)T (∇f(x(k)) −∇f(x(k−1)))
(7.52)
In fact, all these choices reduce to (7.41) if f is a quadratic convex
function.
For coherence, we will indicate with FR (respectively, PR, HS) the
directions associated with βF R
k
(respectively, βP R
k
, βHS
k
).

7.5 Descent (or line search) methods
241
The following are suﬃcient conditions for the FR conjugate gradient
converge to a steady point ([NW06, SY06]): f ∈C1(Rn), its gradient is
Lipschitz continuous, the initial point x(0) is such that the set A = {x :
f(x) ≤f(x(0))} is bounded and the steplengths αk satisfy the strong
Wolfe’s conditions (7.44) with 0 < σ < δ < 1/2.
Under the same assumptions on f and x(0) and under the condition
that βP R
k
is replaced by βP R+
k
= max{−βP R
k
, 0} also the PR conjugate
gradient method with these modiﬁed coeﬃcients converges to a steady
point, provided however that the steplengths αk undergo a variant of
the strong Wolfe’s conditions (7.44). Same conclusions hold for the HS
conjugate gradient algorithm. We refer to [Noc92, NW06, SY06] for the
proof and a more in-depth analysis.
The conjugate gradient method with FR, PR, and HS directions and
steplengths αk computed by the backtracking technique are all imple-
mented in Program 7.3.
Example 7.10 Still on the function (7.32) we ﬁx a tolerance ε = 10−5 for the
stopping criterium and call Program 7.3 by setting meth=41, 42, 43, which
correspond to the conjugate gradient method associated with directions FR,
PR, and HS, respectively. The number of iterations are reported in the table
below.
Directions
x(0)
(−1, −1)
(1, 1)
(0.5, −0.5)
FR
20
12
>400
PR
21
28
17
HS
23
40
28
For both choices x(0) = (−1, −1) and x(0) = (0.5, −0.5), the method converges
to the global minimizer x=[-0.63058;-0.70074], whereas with x(0) = (1, 1)
all the variants converge to the local minimizer x=[0.8094;0.7097].
■
Several remarks are in order.
From the previous table and Fig. 7.9, we see that directions PR and
HS are more eﬃcient than FR. The latter may be quite ineﬃcient and
generate very tiny steplengths. This may yield very slow convergence
or even stagnation; in the latter case the algorithm can be restarted by
using a gradient direction d(k) = −∇f(x(k)).
When the steplengths αk are computed exactly (as described at the
beginning of Sect. 7.5.1) the rate of convergence of the conjugate gradi-
ent method is simply linear, that of Newton methods quadratic, while
that of quasi-Newton’s super-linear. In spite of that, the conjugate gra-
dient method is simple to implement: it does not require the Hessian
matrix (neither its approximations) and only one evaluation of f and its
gradient is required at every iteration. It is deﬁnitely preferable on large
dimensional optimization problems, whereas Newton and quasi-Newton
methods are in general more eﬃcient on small dimensional problems.

242
7 Numerical optimization
See Exercises 7.4-7.6.
7.6 Trust region methods
At the generic kth step, line search methods determine the descent di-
rection d(k) ﬁrst and then the steplength αk. Instead trust region meth-
ods choose direction and steplength simultaneously by building a ball
centered at x(k) and radius δk (the so called trust region), a quadratic
approximation ˜fk of the objective function f and choosing the new value
x(k+1) as the minimizer of ˜fk in the trust region, see Figure 7.12.
More precisely, we start by a “trust” value δk > 0, we use second
order Taylor development of f about x(k) to compute ˜fk,
˜fk(s) = f(x(k)) + sT ∇f(x(k)) + 1
2sT Hks
∀s ∈Rn
(7.53)
where Hk is either Hessian of f at x(k) or a suitable approximation of it,
then we compute
s(k) =
argmin
s∈Rn: ∥s∥≤δk
˜fk(s).
(7.54)
At this stage we compute
ρk = f(x(k) + s(k)) −f(x(k))
˜fk(s(k)) −˜fk(0)
,
(7.55)
then we proceed as follows:
i) If ρk is close to one, we accept s(k) and move to the next iteration.
However, if the minimizer of ˜fk lies on the border of the trust region, we
extend the latter before proceeding with the next iteration.
ii) If ρk is either negative or positive and small (much smaller than
one), we reduce the trust region and look for a new s(k) by solving again
problem (7.54).
iii) Finally if ρk is much larger than one, we accept s(k), we keep the
trust region as is, and move to the next iteration.
Should the second derivative of f be available we could take Hk equal
to the Hessian (or, in case the latter fails to be positive deﬁnite, one of its
variants described in Remark 7.2). Otherwise, Hk can be built recursively
as done for quasi-Newton descent direction method (see Sect. 7.5.4).
Assume that: Hk is symmetric positive deﬁnite and ∥H−1
k ∇f(x(k))∥≤
δk; then (7.54) admits s(k) = H−1
k ∇f(x(k)) as minimizer in the trust
region. Otherwise the minimizer of ˜fk lies at the exterior of the trust
region; in that case one has to solve a minimization problem for ˜fk
constrained to the circumference centered at x(k) with radius δk, that is

7.6 Trust region methods
243
−2
−1.5
−1
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
0
0.5
1
1.5
x(k)
x(0)
x∗
x1
x2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
0
0.5
1
1.5
2
2.5
3
x(k)
x1
x2
x(k+1)
Figure 7.12. Convergence history of trust region method (at left) and the
quadratic model ˜fk at step k = 8 (at right)
min
s∈Rn: ∥s∥=δk
˜fk(s).
(7.56)
To solve (7.56) we can use the Lagrange multipliers approach (see Section
7.8.2), that is we look for the saddle point of the Lagrangian Lk(s, λ) =
˜fk(s)+ 1
2λ(sT s−δ2
k), i.e. for a vector s(k) and a scalar λ(k) > 0 satisfying:
(Hk + λ(k)I)s(k) = −∇f(x(k)),
(Hk + λ(k)I) is semideﬁnite positive
∥s(k)∥−δk = 0.
(7.57)
From (7.57)1 we formally derive s(k) = s(k)(λ(k)) and we replace it
into (7.57)3 to get the nonlinear equation
ϕ(λ(k)) =
1
∥s(k)(λ(k))∥−1
δk
= 0.
The reason for using instead of (7.57)3 its reciprocal is that the latter
is easier to solve numerically. Indeed few Newton iterations (tipically, 3
or less) suﬃce. Precisely, for a given λ(k)
0 , setting g(k) = ∇f(x(k)), we
proceed as follows:
for ℓ= 0, . . . , 2
compute s(k)
ℓ
= −(Hk + λ(k)
ℓI)−1g(k)
evaluate ϕ(λ(k)
ℓ) =
1
∥s(k)
ℓ∥
−1
δk
evaluate ϕ′(λ(k)
ℓ)
update λ(k)
ℓ+1 = λ(k)
ℓ
−ϕ(λ(k)
ℓ)
ϕ′(λ(k)
ℓ)

244
7 Numerical optimization
The vector s(k)
ℓ
is obtained by using the Cholesky factorization (5.18)
of B(k)
ℓ
= (Hk + λ(k)
ℓI) provided this matrix is positive deﬁnite. (Notice
that B(k)
ℓ
is symmetric, in view of the deﬁnition of Hk, and its eigenvalues
are all real.) More in general, instead of B(k)
ℓ
we use (B(k)
ℓ
+ βI) where β
is larger than the negative eigenvalue of maximum modulus of B(k)
ℓ.
By suitably representing the derivative of ϕ(λ(k)), problem (7.54) can
be solved by using the following algorithm: for g(k) = ∇f(x(k)) and a
given δk,
solve Hks = −g(k)
if ∥s∥≤δk and Hk is positive deﬁnite
set s(k) = s
else
compute β1 = the negative eigenvalue of Hk
with largest modulus
set λ(k)
0
= 2|β1|
for ℓ= 0, . . . , 2
compute R : RT R = Hk + λ(k)
ℓI
solve RT Rs = −g(k), RT q = s
update
λ(k)
ℓ+1 = λ(k)
ℓ
+
 ∥s∥
∥q∥
2 ∥s∥−δk
δk
set s(k) = s
endif
(7.58)
In conclusion, we provide the trust region algorithm in its sim-
plest form for the solution of the minimization problem (7.1) ([CL96a,
CL96b]). Consider an initial point x(0), a maximum value ˆδ > 0 for
the trust region radii and an initial radius 0 < δ0 < ˆδ. Consider then
four real parameters η1, η2, γ1 and γ2 such that 0 < η1 < η2 < 1 and
0 < γ1 < 1 < γ2 for updating the trust region and a real parameter
0 ≤μ < η1 for the acceptability of the solution. For k = 0, 1, . . ., until
convergence

7.6 Trust region methods
245
compute f(x(k)), ∇f(x(k)) and Hk,
solve min∥s∥2≤δk ˜fk(s) by (7.58)
compute ρk using (7.55),
if ρk > μ
set x(k+1) = x(k) + s(k)
else
set x(k+1) = x(k)
endif
if ρk < η1
set δk+1 = γ1δk
elseif η1 ≤ρk ≤η2
set δk+1 = δk
elseif ρk > η2 and ∥s(k)∥= δk
set δk+1 = min{γ2δk, ˆδ}
endif
(7.59)
A possible choice of parameters is η1 = 1/4, η2 = 3/4, γ1 = 1/4,
γ2 = 2 (see [NW06]). By choosing μ = 0 we accept any step yielding a
decrease of f; choosing instead μ > 0 we only accept steps for which the
variation of f be at least μ times that of its quadratic model ˜fk.
Remark 7.4 (Approximate solution of (7.54)) Problem (7.54) can be
solved approximately, using however an approximation that does not aﬀect
the convergence properties of the trust region method. A possible strategy
consists in solving the problem not in the whole Rn but rather in a subspace
of dimension two. More precisely, we look for the solution of
min
s∈Sk: ∥s∥≤δk
˜fk(s).
(7.60)
If Hk is positive (or negative) deﬁnite, Sk = span{∇f(x(k)), H−1
k ∇f(x(k))};
otherwise we compute its negative eigenvalue β1 with maximum modulus and
set Sk = span{∇f(x(k)), (Hk + αI)−1∇f(x(k))}, with α ∈(−β1, −2β1]. The
choice of these subspaces is motivated by the search of the so-called Cauchy
point, the minimizer of ˜fk along the directional gradient and internal to the
trust region ([NW06]). The most demanding computational eﬀort when solving
(7.60) consists in the factorization of either Hk or Hk+αI and in computing its
eigenvalue β1. However, the computational cost required by (7.60) is deﬁnitely
lower than that necessary to solve (7.54).
■
The algorithm (7.59) is implemented in Program 7.4. Parameters
fun, grad, x0, tol, kmax have the same meaning as in the Program
descent 7.3. Moreover, delta0 is the radius of the initial trust region,

246
7 Numerical optimization
meth characterizes the choice of matrices Hk: if meth=1, hess contains
the function handle of the Hessian of f and Hk is the exact Hessian. If
meth is diﬀerent than one there is no need to pass the input variable hess;
in this case Hk is a rank-one approximation of the Hessian computed as
in (7.48).
Program 7.4. trustregion: trust region method
function [x,err ,iter ]= trustregion (fun ,grad ,x0 ,...
delta0 ,tol ,kmax ,meth ,hess )
% TRUSTREGION
Trust region method for minimization
%
[X,ERR ,ITER ]= TRUSTREGION (FUN ,GRAD ,X0 ,TOL ,KMAX ,...
%
METH ,HESS ) computes a local minimizer
of function
%
f by the trust region method. FUN and
GRAD
%
(and HESS ) are the
function
handles of the
cost
%
function , its
gradient (and its Hessian ).
%
If METH =1, the
Hessian
HESS of f is used , otherwise
%
rank -one updates
approximations
of the Hessian
are
%
built as in BFGS
and the
variable
HESS
is not requi -
%
red. X0 is the
initial
point for the sequence
gene -
%
rated by the method. TOL is the tolerance
for the
%
stopping
test , KMAX is the
maximum
number of
%
iterations
allowed .
delta=delta0; err=tol +1; k=0; mu =0.1;
eta1 =0.25;
eta2 =0.75;
gamma1 =0.25;
gamma2 =2;
deltam =5;
xk=x0 (:);
gk=grad (xk);
eps2 =sqrt (eps );
if meth ==1 Hk=hess (xk); else Hk=eye(length(xk )); end
while err >tol & k< kmax
[s]= trustone (Hk ,gk ,delta);
rho =( fun(xk+s)-fun(xk ))/(s’*gk +0.5*s’* Hk*s);
if rho > mu , xk1=xk+s; else , xk1=xk; end
if rho <eta1
delta=gamma1*delta;
elseif rho > eta2 & abs(norm (s)-delta)<sqrt(eps)
delta=min ([ gamma2*delta ,deltam ]);
end
gk1=grad (xk1 );
err=norm (( gk1 .*xk1 )/ max([ abs(fun(xk1 )),1]), inf);
if meth ==1 % Newton
xk=xk1; gk=gk1; Hk=hess (xk);
else
% quasiNewton
gk1=grad (xk1 ); yk=gk1 -gk; sk=xk1 -xk;
yks=yk ’*sk;
if yks > eps2*norm (sk)* norm(yk)
Hs=Hk*sk;
Hk=Hk+(yk*yk ’)/yks -(Hs*Hs ’)/( sk ’*Hs);
end
xk=xk1; gk=gk1;
end
k=k+1;
end
x=xk; iter =k;
if (k== kmax & err > tol)
fprintf ([’The trust region method
stopped \n’ ,...
’without
converging
to the desired
tolerance
\n’ ,...
’because the maximum
number of iterations
was \n’ ,...
’reached\n’]);
end

7.6 Trust region methods
247
end
function [s]= trustone (Hk ,gk ,delta)
s=-Hk\gk; d = eigs (Hk ,1,’sa’);
if norm (s)>delta | d<0
lambda=abs (2*d); I=eye(size (Hk ));
for l=1:3
R=chol (Hk+lambda*I);
s=-R \ (R’\ gk); q=R’\s;
lambda=lambda +(s’*s)/(q’*q)*( norm (s)-delta)/ delta;
if lambda < -d, lambda=abs(lambda *2);
end
end
end
end
Example 7.11 Let us compute the minimizer of f(x1, x2) = (x1 + 2x2 +
2x1x2 −5x2
1 −5x2
2)/(5ex2
1+x2
2) + 7/5 using method (7.59). As seen in Figure
7.12, this function features a local maximum, a saddle point and two local
minima, one xm1 in proximity of (−1., 0.2) and the other xm2 in proximity
of (0.3, −0.9); the latter is also a global minimizer. We choose x(0) = (0, 0.5)
and compute the matrices Hk recursively, according to (7.48). By calling the
Program trustregion with the following instructions:
fun=@(x)7/5+(x(1)+2*x(2)+2*x(1)* x(2) -5*x(1)^2 -...
5*x(2)^2)/(5* exp(x(1)^2+x(2)^2));
grad_fun =@(x)[(1+2* x(2) -10* x(1) -2*x(1)*(x(1)+2*x(2)+...
2*x(1)* x(2) -5*x(1)^2 -5*x(2)^2))/(5* exp(x(1)^2+x(2)^2));
(2+2*x(1) -10* x(2) -2*x(2)*(x(1)+2*x(2)+...
2*x(1)* x(2) -5*x(1)^2 -5*x(2)^2))/(5* exp(x(1)^2+x(2)^2))];
delta0 =0.5; tol =1.e -5; kmax =100; meth =2; x0 =[0;0.5];
[x,err ,iter ]= trustregion (fun ,grad_fun ,x0 ,delta0 ,...
tol ,kmax ,meth )
convergence to the point (.27849, −.89695) is achieved in 24 iterations.
Using instead meth=1 and at each step the exact Hessian matrix, con-
vergence will be achieved in 12 iterations. In both cases a slowing down in
convergence is observed when the iterates x(k) are near the local minimum
xm1. The convergence history when using the exact Hessian is reported in
Figure 7.12, left, while Figure 7.13, corresponds to using a non-exact Hessian.
■
For the convergence analysis of the trust region method we refer to
[NW06, Sez. 4.2] and [SSB85].
The MATLAB command fminunc with the ’LargeScale’ option
initialized to the value ’on’ implements the trust region method, and the
function handle grad fun contains the gradient of the objective function.
For instance, using the following instructions:
fun=@(x)100*(x(2)-x(1)^2)^2+(1 -x(1))^2;
x0 =[1.2; -1];
grad_fun =@(x)[ -400*( x(2)-x(1)^2)* x(1) -2*(1 -x(1));
200*(x(2)-x(1)^2)];
options = optimset (’LargeScale ’,’on’,’GradObj ’,’on’);
[x,fval ,exitflag ,output ]= fminunc ({fun ,grad_fun },...
x0 ,options )

248
7 Numerical optimization
−2
−1.5
−1
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
0
0.5
1
1.5
x(0)
x∗
x1
x2
Figure 7.13. Convergence history of the trust region method when Hk are
built as in (7.48)
we converge to the minimizer of the Rosenbrock function in 8 iterations;
only 9 function evaluations are requested.
Octave 7.2 The fminunc command in Octave implements the trust re-
gion method with approximated Hessian matrices Hk, computed accord-
ing to the BFGS recursive formula (7.48). The option ’LargeScale’ is
not used in this case.
■
See Exercise 7.7.
7.7 The nonlinear least squares method
In Chapter 3 we have introduced the least squares method for the ap-
proximation of either functions or a discrete set of data, by a polynomial
(3.29) or another function ˜f linearly depending on a set of unknown
coeﬃcients aj, j = 1, . . . , m. When such a dependence is nonlinear, we
face a nonlinear least squares problem.
In abstract terms, let R(x) = (r1(x), . . . , rn(x))T , with ri : Rm →R,
be a given function, and consider the following minimization problem
min
x∈Rm Φ(x)
with
Φ(x) = 1
2∥R(x)∥2 = 1
2
n

i=1
r2
i (x).
(7.61)
When the function ri are nonlinear, Φ might not be convex, featuring
several stationary points. All the methods considered thus far, that is
Newton’s (7.31), descents (7.33) and trust region (7.59), can virtually be
used to solve (7.61).

7.7 The nonlinear least squares method
249
Thanks to the special form of Φ, its gradient and Hessian can be
written in terms of the Jacobian JR(x) ∈Rn×m and of the ﬁrst and
second derivatives of R, as follows:
∇Φ(x) = JR(x)T R(x),
H(x) = JR(x)T JR(x) + S(x),
with Sℓj(x) =
n

i=1
∂2ri
∂xℓ∂xj
(x)ri(x), ℓ, j = 1, . . . , m.
(7.62)
Exact calculation of the Hessian can be cumbersome when m and n
are large, especially due to the presence of the matrix S(x). On the other
hand, in several cases the latter matrix is less inﬂuent than JR(x)T JR(x)
and could be approximated or even neglected in the construction of H(x).
This is the case of the two methods that we are going to present in the
next two sections.
7.7.1 Gauss-Newton method
This method is a variant of the Newton method (7.31) for the solution
of (7.61) in which the exact Hessian H(x) is approximated by neglecting
S(x) in (7.62).
Its formulation is as follows: given x(0) ∈Rm, for k = 0, 1, . . . , until
convergence:
solve

JR(x(k))T JR(x(k))

δx(k) = −JR(x(k))T R(x(k))
set x(k+1) = x(k) + δx(k)
(7.63)
If JR(x(k)) has not full rank, the linear system (7.63)1 features in-
ﬁnitely many solutions, in which case the Gauss-Newton method can
stagnate, diverge, or converge to a non-stationary point.
If instead JR(x(k)) has full rank, system (7.63)1 features the form
(5.42) and can be solved using either a QR factorization or a singular
value decomposition of JR(x(k)) as seen in Section 5.7.
It can be proved (see Exercise 7.8) that neglecting S(x(k)) at the
step k of the minimization process amounts to approximate R(x) with
its Taylor development centered at x(k) and truncated at the ﬁrst order
-Rk(x) = R(x(k)) + JR(x(k))(x −x(k)).
(7.64)
The convergence of Gauss-Newton method is not always guaranteed.
It actually depends on both the property of Φ and the choice of the initial
point. The following result is proved in [JS96]: if x∗is a stationary point
for Φ and JR(x) has full rank in a suitable neighborhood of x∗, we have:

250
7 Numerical optimization
1. if S(x∗) = 0, which is the case if R(x) is linear or R(x∗) = 0, the
Gauss-Newton method is locally (quadratically) convergent (in fact
it coincides with Newton’s method);
2. if ∥S(x∗)∥2 is small with respect to the minimum (positive) eigen-
value of JR(x∗)T JR(x∗), then Gauss-Newton method converges lin-
early. This is for instance the case if R(x) is nonlinear with a small
non-linearity or if R(x∗) is small;
3. if ∥S(x∗)∥2 is large with respect to the minimum (positive) eigenvalue
of JR(x∗)T JR(x∗), the Gauss-Newton method might not converge
even if x(0) is very close to x∗. This happens if R(x) is strongly
nonlinear or its residual R(x∗) is large.
Remark 7.5 Line search techniques can be used in combination with the
Gauss-Newton method by replacing (7.63)2 with x(k+1) = x(k) + αkδx(k),
where the computation of the steplengths αk is described in Section 7.5.1. If
JR(x(k)) has full rank, the matrix JR(x(k))T JR(x(k)) is symmetric and positive
deﬁnite and δx(k) is a descent direction for Φ (see Exercise 7.9). In this case,
under suitable assumptions on Φ, we obtain a globally convergent method,
called damped Gauss-Newton method.
■
The Gauss-Newton method is implemented in Program 7.5; r and jr
are function handles associated with the function R(x) and its Jacobian
JR(x), respectively, x0 is the initial vector, while tol and kmax contain
the tolerance for the stopping test and the maximum number of iter-
ations allowed. The output variable x contains the computed solution,
err an estimate of the error at the last iteration and iter the number
of iterations required to converge.
Program 7.5. gaussnewton: Gauss-Newton method
function [x,err ,iter ]= gaussnewton (r,jr ,x0 ,tol ,...
kmax ,varargin )
% GAUSSNEWTON
Solves
nonlinear
least squares
problems
%
[X,ERR ,ITER ]= GAUSSNEWTON (R,JR ,X0 ,TOL ,KMAX )
%
solves the
nonlinear
least squares by the Gauss -
%
Newton
method. R and JR are the function
handles
%
associated
with
the
function R and its
Jacobian ,
%
respectively . X0 is the initial
point for the se -
%
quence. TOL is the tolerance
for the stopping
test ,
%
KMAX is the
maximum
number of allowed
iterations .
err=tol +1; k=0; xk=x0 (:);
rk=r(xk ,varargin {:}); jrk=jr(xk ,varargin {:});
while err >tol & k< kmax
[Q,R]=qr(jrk ,0); dk=-R \ (Q’*rk);
xk1=xk+dk;
rk1=r(xk1 ,varargin {:});
jrk1 =jr(xk1 ,varargin {:});
k=k+1;
err=norm (xk1 -xk);
xk=xk1; rk=rk1; jrk=jrk1 ;
end
x=xk; iter =k;
if (k== kmax & err > tol)

7.7 The nonlinear least squares method
251
fprintf ([’Gauss -Newton
method
stopped \n’ ,...
’without
converging
to the desired
tolerance
\n’ ,...
’because the maximum
number of iterations
was \n’ ,...
’reached\n’]);
end
Example 7.12 Let us consider Problem 7.2 under the form (7.5) (a special
case of (7.61)). We use the Gauss-Newton method (7.63), we storage vector a
in the upper part of x and σ in the lower one, yielding
ri(x) = f(ti; a, σ) −yi =
m

k=1
fk(ti; ak, σk) −yi,
∂ri
∂ak
= fk(ti; ak, σk)ti −ak
σ2
k
,
∂ri
∂σk
= fk(ti; ak, σk)
	(ti −ak)2
σ3
k
−
1
2σk

.
We generate the n points (ti, yi) with i = 1, . . . , n, 0 ≤ti ≤10, by summing
5 Gaussian functions of the form (7.3) taking a = [2.3, 3.25, 4.82, 5.3, 6.6],
σ = [0.2, 0.34, 0.50, 0.23, 0.39] and adding a random noise:
a=[2.3 ,3.25 ,4.82 ,5.3 ,6.6]; m=length(a);
sigma =[0.2 ,0.34 ,0.50 ,0.23 ,0.39];
gaussian =@(t,a,sigma )...
exp(-((t-a)/( sqrt (2)* sigma )).^2)/( sqrt (pi *2)* sigma );
n=2000; t=linspace (0,10, n)’; y=zeros(n ,1);
for k=1:m, y=y+ gaussian (t,a(k),sigma(k)); end
y=y+0.05* randn(n ,1);
We now call Program 7.5 using the following instructions:
x0 =[2 ,3 ,4 ,5 ,6 ,0.3 ,0.3 ,0.6 ,0.3 ,0.3];
tol =3.e-5; kmax =200;
[x,err ,iter ]= gaussnewton (@gmmr ,@gmmjr ,x0 ,tol ,kmax ,t,y)
xa=x(1:m); xsigma=x(m+1: end );
h=1./( sqrt (2* pi)* xsigma ); w=2* sqrt (log (4))* xsigma;
where gmmr and gmmjr are the functions deﬁning R(x) and JR(x), respectively.
function [R]= gmmr (x,t,y)
x=x(:);
m=length(x)/2; a=x(1:m); sigma=x(m+1: end );
n=length(t); R=zeros(n,1);
gaussian =@(t,a,sigma)[ exp (-((t-a)/( sqrt (2)* sigma ))...
.^2)/( sqrt(pi *2)* sigma)];
for k=1:m, R=R+ gaussian (t,a(k),sigma(k)); end , R=R-y;
function [Jr ]= gmmjr(x,t,y)
x=x(:); m=length(x)/2; a=x(1:m); sigma=x(m+1: end);
n=length(t); Jr=zeros(n,m*2);
gaussian =@(t,a,sigma)[ exp (-((t-a)/( sqrt (2)* sigma ))...
.^2)/( sqrt (pi *2)* sigma)];
fk=zeros(n,m);
for k=1:m, fk(:,k)= gaussian (t,a(k),sigma(k)); end
for k=1:m, Jr(:,k)=(fk(:,k).*(t-a(k))/ sigma(k)^2) ’; end
for k=1:m, Jr(:,k+m)=(fk(:,k).*((t-a(k)).^2/...
sigma(k)^3 -1/(2* sigma(k)))) ’; end
Convergence is achieved in 22 iterations. The vectors xa and xsigma contain
the approximation of vectors a and σ, respectively, while h and w contain the

252
7 Numerical optimization
0
1
2
3
4
5
6
7
8
9
10
−0.5
0
0.5
1
1.5
2
2.5
Figure 7.14. I dati (in azzurro) e la soluzione (in nero) dell’Esempio 7.12
height and amplitude, respectively, of the Gaussian functions we are looking
for.
We display in Figure 7.14 the points (ti, yi) (in blue) representing the
signal and the 5 Gaussian functions (7.3) (black lines) built on the obtained
numerical solution. This is the case with large residual: as a matter of fact
Φ(x∗) = 1.0385e + 03, x∗being the solution vector. By a slight change of the
initial data, for instance by simply modifying the last component of x(0) from
0.3 to 0.5, the method would not converge any more. This remark prompts us
to a convenient choice of x(0).
■
7.7.2 Levenberg-Marquardt’s method
This is a trust region method for the solution of the minimization prob-
lem (7.61). Following algorithm (7.59), after replacing f with Φ (see
(7.61)) and ˜f with ˜Φ, at each step k we solve the minimization problem
min
s∈Rn: ∥s∥≤δk
˜Φk(s)
with
˜Φk(s) = 1
2∥R(x(k)) + JR(x(k))s∥2.
(7.65)
Note that ˜Φk(x) (7.65) is a quadratic approximation of Φ(x) around
x(k), obtained by approximating R(x) with its linear model -Rk(x) (7.64)
(see Exercise 7.11).
Even though JR(x) does not have full rank, this method is well
suited for minimization problems featuring a strong non-linearity or a
large residual Φ(x∗) = 1
2∥R(x∗)∥2 in correspondence with a local mini-
mizer x∗.

7.8 Constrained optimization
253
Since the approximation of the Hessian matrix is the same as for the
Gauss-Newton method, the two methods share the same local conver-
gence properties. In particular, should the Levenberg-Marquardt itera-
tions converge, convergence rate is quadratic if the residual is null at
local minimizer, linear otherwise.
See Exercises 7.8-7.11.
Let us summarize
1. For the minimization of the function f, the derivative free methods
are those using only the functional values of f. They are quite robust
in practice even though very little is known about their theoretical
convergence;
2. descent methods exploit the knowledge of the function derivatives
and compute at each step a descent direction and a steplength, based
on line search strategies;
3. descent methods with Newton directions associated with linear search
strategies are globally convergent when the matrices H(x(k)) are pos-
itive deﬁnite. They feature quadratic convergence rate in proximity
of the minimizer. They are well suited for small and medium size
problems;
4. descent methods with quasi-Newton directions make use of approxi-
mate Hessian matrices Hk at every iteration. When associated with
line search strategies, they are globally convergent provided Hk are
positive deﬁnite, with superlinear convergence order. They too are
well suited for small and medium size problems;
5. descent methods with conjugate gradient type descent directions,
associated with line search strategies, are globally convergent with
linear rate of convergence. They are recommended for large size prob-
lems;
6. trust region strategies are more recent and less diﬀused than line
search ones. They replace the objective function with a quadratic ap-
proximation and look for a minimizer of the latter in a n-dimensional
ball.
7.8 Constrained optimization
In this Section we introduce two simple strategies for the solution of
minimization problems with constraints: the penalty method for prob-
lems with both equality and inequality constraints and the so-called aug-
mented Lagrangian method for problems featuring equality constraints
only.

254
7 Numerical optimization
These two methods allow the solutions of simple problems and pro-
vide the basic tools for more robust and complex algorithms that we will
not address here (see however [NW06, SY06, BDF+10]).
The constrained optimization problem is formulated as follows: we
consider the minimization problem (7.2) for which the domain Ω can be
either given by
Ω = {x ∈Rn : hi(x) = 0, for i = 1, . . . , p},
(7.66)
where hi : Rn →R for i = 1, . . . , p, are given functions, or by
Ω = {x ∈Rn : gj(x) ≥0, for j = 1, . . . , q},
(7.67)
where gj : Rn →R for j = 1, . . . , q; p and q are given natural numbers.
In the more general case, however, Ω is deﬁned by both equality and
inequality constraints, that is
Ω = {x ∈Rn : hi(x) = 0, for i = 1, . . . , p, gj(x) ≥0, for j = 1, . . . , q}.
(7.68)
The three diﬀerent situations (7.66), (7.67), and (7.68) undergo a
unique notation,
Ω = {x ∈Rn : hi(x) = 0, for i ∈Ih, gj(x) ≥0, for j ∈Ig},
for two suitable chosen sets Ih and Ig, under the convention that Ih = ∅
in (7.67) and Ig = ∅in (7.66).
Problem (7.2) can thus be written as
minx∈Rn f(x), subject to
hi(x) = 0
∀i ∈Ih,
gj(x) ≥0
∀j ∈Ig
(7.69)
Everywhere in this section we will assume that f, hi, and gj be C1
functions on Rn.
The points of x ∈Ω are called admissibile (as they fulﬁll all the
constraints); Ω is the set of admissible points.
A point x∗∈Ω ⊂Rn is a global minimizer for problem (7.2) if
f(x∗) ≤f(x)
∀x ∈Ω,
whereas x∗is a local minimizer for (7.2) if there exists a ball Br(x∗) ⊂Rn
with radius r > 0 and centered at x∗such that
f(x∗) ≤f(x)
∀x ∈Br(x∗) ∩Ω.
A constraint is said active at x ∈Ω if it is satisﬁed with equality at
x ∈Ω. According to this deﬁnition, active constraints at x are all the hi
as well as those gj such that gj(x) = 0.

7.8 Constrained optimization
255
Ω
x1
x2
x∗
Ω
x1
x2
x∗
Figure 7.15. The contour lines of the cost function f, the admissibility set
Ω and the global minimizer x∗constrained to Ω. The plot at left is relative to
Problem 1 (7.70), that at right to Problem 2 (7.71)
Example 7.13 Consider the following constrained optimization problems:
Problem 1:
min
x∈R2 f(x),
with f(x) = 3
5x2
1 + 1
2x1x2 −x2 + 3x1,
under the following constraint
h1(x) = x2
1 + x2
2 −1 = 0;
(7.70)
Problem 2:
min
x∈R2 f(x),
with f(x) = 100(x2 −x2
1)2 + (1 −x1)2,
under the following constraints:
g1(x) = −34x1 −30x2 + 19 ≥0,
g2(x) = 10x1 −5x2 + 11 ≥0,
g3(x) = 3x1 + 22x2 + 8 ≥0.
(7.71)
The contour lines of the two cost functions and the associated set of admissible
points Ω are displayed in Figure 7.15. Note that Ω is a closed curve for Problem
1, while it is a closed convex set in R2 for Problem 2. For both problems there
is one active constraint.
■
The Weierstrass theorem guarantees the existence of both the max-
imum and the minimum for f in Ω when the latter is a non-empty,
bounded and closed set. Consequently, problem (7.69) admits a solu-
tion.
We recall that a function f : Ω ⊆Rn →R is strongly convex in Ω if
∃ρ > 0 such that ∀x, y ∈Ω and ∀α ∈[0, 1],
f(αx + (1 −α)y) ≤αf(x) + (1 −α)f(y) −α(1 −α)ρ∥x −y∥2. (7.72)

256
7 Numerical optimization
This reduces to the deﬁnition of convexity (7.11) when ρ = 0.
Proposition 7.2 (Optimality conditions) Let Ω
⊂Rn be a
convex set, and x∗∈Ω be such that f ∈C1(Br(x∗)) for a suit-
able r > 0. If x∗is a local minimizer for (7.2) then
∇f(x∗)T (x −x∗) ≥0
∀x ∈Ω.
(7.73)
Moreover, if f is convex in Ω and (7.73) is satisﬁed, x∗is a global
minimizer for (7.2).
Finally, under the additional requirement for Ω to be closed and f
strongly convex, the minimizer for (7.2) is unique.
Let us introduce the Lagrangian function associated with problem
(7.2)
L(x, λ, μ) = f(x) −

i∈Ih
λihi(x) −

j∈Ig
μjgj(x).
(7.74)
Here λ = (λi) (for i ∈Ih) and μ = (μj) (for j ∈Ig) play the role of La-
grangian multipliers associated with equality and inequality constraints,
respectively. A point x∗is called a Karush–Kuhn–Tucker (KKT) point
for L if there exist λ∗and μ∗such that the triplet (x∗, λ∗, μ∗) satisﬁes
the following conditions, called Karush–Kuhn–Tucker conditions:
∇xL(x∗, λ∗, μ∗) = ∇f(x∗) −

i∈Ih
λ∗
i ∇hi(x∗) −

j∈Ig
μ∗
j∇gj(x∗) = 0
hi(x∗) = 0
∀i ∈Ih
gj(x∗) ≥0
∀j ∈Ig
μ∗
j ≥0
∀j ∈Ig
μ∗
jgj(x∗) = 0
∀j ∈Ig
For a given point x, the constraints are said to satisfy the LICQ
(linear independence constraint qualiﬁcation) condition at x if the gra-
dients ∇hi(x) and ∇gj(x) associated with the sole active constraints at
x provide a set of linear independent vectors.
The following result holds (see, e.g., [NW06, Thm. 12.1]).

7.8 Constrained optimization
257
Theorem 7.1 (First order KKT necessary conditions) If x∗
is a local minimizer for problem (7.69), the functions f, hi, and gj
are of class C1(Ω), and the constraints satisfy the LICQ condition
at x∗, then there exist λ∗and μ∗such that (x∗, λ∗, μ∗) is a KKT
point.
Thanks to this theorem, the local minimizers for (7.69) should be
sought for among the KKT points and those points where LICQ condi-
tion is not fulﬁlled.
When the set Ig is empty (only equality constraints are present) the
Lagrangian function reads L(x, λ) = f(x)−*
i∈Ih λihi(x) and the KKT
conditions reduce to the classical necessary (Lagrangian) conditions
∇xL(x∗, λ∗) = ∇f(x∗) −

i∈Ih
λ∗
i ∇hi(x∗) = 0
hi(x∗) = 0
∀i ∈Ih
(7.75)
Suﬃcient conditions for a KKT point to be a minimizer for f con-
strained in Ω would require the knowledge of the Hessian matrix of L
or else an assumption of strict convexity for both f and the constraint
functions ([NW06, SY06]).
In general terms, a constrained optimization problem can be written
as an unconstrained problem using either the penalized formulation or
the augmented Lagrangian formulation, as we will explain in the next
two sections.
Remark 7.6 If at a point x∗that minimizes f no active constraints are
present, the Lagrangian function coincides with the cost function f therein, as
Ih = ∅and μ∗
j = 0 for all j ∈Ig thanks to the KKT conditions. In this case
our problem reduces to an unconstrained minimization problem that can be
solved by using the methods discussed in the previous sections.
■
A remarkable instance of constrained optimization is that of
Quadratic Programming: this is precisely the case where f is a quadratic
function, the constraints are expressed by linear functions, thus problem
(7.69) can be written under the special form:
min
x∈Rn f(x),
f(x) = 1
2xT Ax + xT b
subject to the constraints
Cx −d = 0,
Dx −e ≥0
(7.76)
where A ∈Rn×n, b ∈Rn, C ∈Rp×n, d ∈Rp, D ∈Rq×n, e ∈Rq, p, q
are suitable positive integers and the notations v ≥0 means vi ≥0 for
all i. See [Bom10, NW06] for a presentation of Quadratic Programming.

258
7 Numerical optimization
In the special case where constraints are all expressed by equalities,
the matrix form of the Langrange conditions (7.75) reads (with obvious
choice of notations)
.A
−CT
C
0
/ . x
λ
/
=
. −b
d
/
.
(7.77)
If A is symmetric and positive deﬁnite on the kernel of C, that is
yT Ay > 0
∀y ∈ker(C) = {z : Cz = 0}, y ̸= 0,
and assuming that C has full rank, system (7.77) admits a unique so-
lution, thus there exists a unique global minimizer for the cost function
deﬁned in (7.76).
A quadratic programming problem can therefore be tackled by solv-
ing the linear system (7.77) using one of the methods of Chapter 5.
In general, the matrix M = [A, −CT ; C, 0] of (7.77) is not deﬁnite,
that is it features both positive and negative eigenvalues. Suitable it-
erative methods for its treatment are Krylov methods like GMRES or
Bi-CGStab. See, e.g., [Qua13] and [BGL05].
Example 7.14 To solve Problem 7.4 we note that the cost function deﬁned
in (7.7) (the risk) is quadratic, while the constraints read
h1(x) = 0.6x1 + x2 + 1.2x3 = 1.04,
h2(x) = x1 + x2 + x3 = 1.
(7.78)
The former states that the expected return be equal to 10.4%, while the latter
establishes that the sum of the fractions invested into the 3 funds be equal
to the entire capital. This is a quadratic programming problem that we can
rewrite under the form (7.77), where
A =
⎡
⎣
0.08 0.1
0
0.1
0.5
0.208
0
0.208 1.28
⎤
⎦,
b =
	 0
0

,
C =
	 0.6 1 1.2
1
1 1

,
d =
	 1.04
1

.
Matrix C has (maximum) rank equal to 2, its kernel ker(C) = {z =
α[1, −3, 2]T , α ∈R} has dimension 1.
As A is symmetric, we need to verify that it is positive deﬁnite on ker(C),
that is zT Az > 0 for all z = α[1, −3, 2]T , α ̸= 0. As a matter of fact,
zT Az = α2[1, −3, 2]T A[1, −3, 2] = 6.6040α2 > 0. Upon building the matrix
M = [A, −CT ; C, 0] and the right hand side f = [−b, d]T , we solve (7.77)
using the following instructions:
A=[0.08 0.1 0; 0.1 0.5 0.208; 0 0.208 1.28]; b=[0;0;0];
C=[0.6 1 1.2;1 ,1 ,1];
d=[1.04;1];
M=[A -C’; C, zeros (2)]; f=[-b;d];
xl=M\f
and obtain the solution

7.8 Constrained optimization
259
xl =
0.0606
0.6183
0.3211
0.7883
-0.4063
The ﬁrst 3 components of xl correspond to the 3 fractions of the capital to
invest in the 3 funds, whereas the last two components provide the values of the
Lagrangian multipliers associated with the constraints. The risk corresponding
to this capital splitting is given by the value of the cost function at the point
xl(1:3) and is approximately equal to 21%.
■
7.8.1 The penalty method
A strategy for solving problem (7.69) consists of turning it into a non-
constrained optimization problem for a modiﬁed penalty function
Pα(x) = f(x) + α
2

i∈Ih
h2
i (x) + α
2

j∈Ig
(max{−gj(x), 0})2
(7.79)
where α > 0 is a parameter to be chosen.
If the given constraints are not fulﬁlled at the point x, the sums
appearing in (7.79) provide a measure of how far x is from the admissible
set Ω. Since in this case x violates the constraints, choosing large values
of α would severely penalize such a violation. Every solution x∗of (7.69)
clearly provides a minimizer of P. Conversely, assuming f, hi, and gj
regular enough, and denoting with x∗(α) a minimizer of Pα(x), it holds
([Ber82])
lim
α→∞x∗(α) = x∗.
For α ≫1, x∗(α) can therefore be regarded as a convenient ap-
proximation of x∗. However, since numerical instabilities arising from
the minimization of Pα(x) increase with α, a better strategy consists of
solving a sequence of unconstrained minimization problems
x(k) = argmin
x∈Rn
Pαk(x)
(7.80)
where {αk} is a monotonically increasing unbounded sequence of param-
eters (with, e.g., α0 = 1). For every k, αk+1 is chosen as a function of
αk and x(k) provides the initial value for problem (7.80) at the new step
k + 1.
A heuristic approach consists of choosing αk+1 = δαk where δ is
small (say δ ∈[1.5, 2]) if many iterations have been necessary to solve
(7.80) at the step k, otherwise one could aﬀord a larger value for δ, say
δ ≃10.

260
7 Numerical optimization
As a matter of fact, in the course of the ﬁrst iterations, when using
a moderate (not too high) αk, there is no reason why the solution of
(7.80) should resemble that of (7.69). This legitimates the search for an
inexact solution of (7.80), diﬀering from the exact one x(k) by a small
enough tolerance εk.
The algorithm above is formulated as follows (note that a further
tolerance ε > 0 is requested to assess the behaviour of the gradient of P
at x(k)).
For given α0 (tipically, α0 = 1), ε0 (tipically, ε0 = 1/10), ε > 0 and
x(0)
0
∈Rn, for k = 0, 1, . . . until convergence
compute an approximation x(k) to (7.80) using an initial
data x(k)
0
and a tolerance εk on the stopping criterium;
if ∥∇xPαk(x(k))∥≤ε
set x∗= x(k) (convergence achieved)
else
choose αk+1 s.t. αk+1 > αk
choose εk+1 s.t. εk+1 < εk
set x(k+1)
0
= x(k)
endif
(7.81)
This alogorithm is implemented in Program 7.6. fun and grad fun
are function handles associated with the cost function and its gradient,
respectively; h and grad h are those associated with the equality con-
straint functions, while g and grad g those associated with inequality
constraint functions. When Ih (resp., Ig) is an empty set, h and grad h
(resp. g and grad g) are empty variables. The output of the functions
grad fun, grad h and grad g respectively contain: an n dimensional col-
umn vector y with components yi = ∂f/∂xi, an n × p matrix C whose
coeﬃcients are Cji = ∂hi/∂xj, an n × q matrix G whose entries are
Gjℓ= ∂gℓ/∂xj. The vector x0 contains x(0)
0 , tol and kmax the tolerance
and the maximum number of iterations for the penalty loop, while kmaxd
is the maximum number of iterations for the descent method, when the
latter is called at every step to solve the unconstrained minimization
problem. In this program the tolerance εk for the descent method is
chosen equal to 1/10 for k = 0 and then reduced at every iteration by
a factor 10 until the tolerance ε is reached. The variable meth is used
to select the unconstrained minimization method: if meth=0 the MAT-
LAB fminsearch function implementing the Nelder and Mead method
is chosen, while meth>1 has the same role played in Program descent

7.8 Constrained optimization
261
7.3 to select the descent method. Finally, if meth=1, the Hessian matrix
necessary to implement the descent method with Newton’s directions is
provided as input variable, while it provides H0 for the BFGS method
(7.49) if meth=2.
Program 7.6. penalty: penalty method
function [x,err ,k]= penalty(fun ,grad_fun ,h,grad_h ,...
g,grad_g ,x0 ,tol ,kmax ,kmaxd ,meth ,varargin )
% PENALTY
Constrained
optimization
with
penalty
%
[X,ERR ,K]= PENALTY (FUN ,GRAD_FUN ,H,GRAD_H ,...
%
G,GRAD_G ,X0 ,TOL ,KMAX ,KMAXD ,METH )
%
computes a local minimizer
of the cost
function
%
FUN under the
constraints
H=0 and G>=0, by the
%
penalty
method. X0 is the initial
point , TOL is
%
the tolerance
for the stopping
test , KMAX is the
%
maximum
number of allowed
iterations .
%
GRAD_FUN , GRAD_H , and
GRAD_G
contain the
gradient
%
of FUN , H, and G, respectively . The variables
%
H, G, GRAD_H , and
GRAD_G can be set to [], if they
%
are not present . The solution
of the
corresponding
%
unconstrained
minimization
problem is performed
%
by calling
either
Matlab
FMINSEARCH
function
%
(if METH =0) or DESCENT
function (if METH >0).
%
When
METH >0, KMAXD and
METH
contain
respectively
%
the maximum
number of allowed
iterations
for the
%
function
DESCENT
and the choice of the descent
%
directions . When
METH >1
%
[X,ERR ,K]= PENALTY (FUN ,GRAD_FUN ,H,GRAD_H ,...
%
G,GRAD_G ,X0 ,TOL ,KMAX ,KMAXD ,METH , HESS )
%
is the correct
calling
instruction .
%
If METH =1 HESS is the
function
handle
associated
%
with
the
Hessian is required , if METH =2 HESS is a
%
suitable
approximation
of the Hessian at the step
0.
xk=x0 (:);
alpha0 =1;
if meth ==1,
hess =varargin {1};
elseif
meth ==2, hess =varargin {1};
else
hess =[]; end
if ~isempty(h), [nh ,mh]= size(h(xk )); end
if ~isempty(g), [ng ,mg]= size(g(xk )); else , ng =[];
end
err=tol +1; k=0;
alphak=alpha0; alphak2 =alphak /2; told =.1;
while err >tol && k< kmax
P=@(x)Pf(x,fun ,g,h,alphak2 ,ng);
grad_P=@(x)grad_Pf (x,grad_fun ,h,g ,...
grad_h ,grad_g ,alphak ,ng);
if meth ==0
options =optimset (’TolX ’,told );
[x,err ,kd]= fminsearch (P,xk ,options );
err=norm (x-xk);
else
[x,err ,kd]= descent (P,grad_P ,xk ,told ,kmaxd ,meth ,hess );
err=norm (grad_P(x));
end
if kd <kmaxd , alphak=alphak *10;
alphak2=alphak /2;
else
alphak=alphak *1.5; alphak2 =alphak /2; end
k=k+1; xk=x; told =max([tol ,told /10]);
end

262
7 Numerical optimization
end % end of the
function
penalty
function y=Pf(x,fun ,g,h,alphak2 ,ng)
y=fun(x);
if ~isempty(h), y=y+alphak2*sum ((h(x)).^2);
end
if ~isempty(g), G=g(x);
for j=1:ng , y=y+alphak2*max([-G(j) ,0])^2;
end
end
end % end of function
Pf
function y=grad_Pf (x,grad_fun ,h,g ,...
grad_h ,grad_g ,alphak ,ng)
y= grad_fun (x);
if ~isempty(h), y=y+alphak*grad_h(x)*h(x); end
if ~isempty(g), G=g(x); Gg=grad_g(x);
for j=1:ng
if G(j)<0, y=y+alphak*Gg(:,j)*G(j); end
end , end
end % end of function
grad_Pf
Example 7.15 Let us solve Problem 2 of the Example 7.13 using Program
7.6. Setting x(0) = (1.2, 0.2) and tolerance ε = 10−5, by the following instruc-
tions
fun=@(x) 100*(x(2)-x(1).^2).^2+(1 -x(1)).^2;
grad_fun =@(x) [ -400*( x(2)-x(1)^2)* x(1) -2*(1 -x(1));
200*(x(2)-x(1)^2)];
g=@(x)[ -34*x(1) -30* x(2)+19; 10*x(1) -5*x(2)+11;
3*x(1)+22*x(2)+8];
grad_g=@(x)[ -34 ,10 ,3; -30 , -5 ,22];
x0 =[1.2 ,.2]; tol =1.e-5;
kmax =100; kmaxd =100;
meth =2; hess =eye (2);
[x,err ,k]= penalty (fun ,grad_fun ,[],[], g,grad_g ,...
x0 ,tol ,kmax ,kmaxd ,meth ,hess )
after 3 iterations we achieve convergence to the point (0.41183, 0.16660) with a
residual on the gradient ∥∇xPα3(x(3))∥≃2.6379·10−7. For the solution of the
unconstrained minimization problem we have used the program 7.3 descent,
more precisely the BFGS method described in Section 7.5.4. The constraints
at the minimizers are equal to g1(x) = 2.0036e −04, g2(x) = 1.4285e + 01 and
g3(x) = 1.2901e + 01.
■
Example 7.16 To solve Problem 7.3 with the penalty method, let Ω be the
circle centered at the origin with radius 2. Let us triangulate Ω with a grid
featuring 49 nodes (the vertices) and Ne = 72 triangles, as shown in Figure
7.2, left. The 24 boundary nodes are kept ﬁxed, whereas the coordinates of
the 25 internal nodes are collected in a vector x and represent the problem
independent variables. The cost function is
f(x) =
Ne

k=1
1
μk(x) =
Ne

k=1
√
3∥Ak(x)W−1∥2
F
4 det(Ak(x))
,
where we have used deﬁnition (7.6), while the inequality constraints are
gk(x) = det(Ak(x)) −τ ≥0,
k = 1, . . . , Ne,
with τ = 0.10876 being twice the value of the area of the smallest triangle of
the initial grid. The result shown in Figure 7.2, right, has been obtained after

7.8 Constrained optimization
263
21 iterations of the penalty algorithm, having set ϵ = 10−8 for the stopping
test. The maximum number of iterations for the Nelder and Mead method has
been ﬁxed to 100.
■
Example 7.17 We solve Problem 7.5 by considering the road network of
Figure 7.3. There are 11(= n) streets sj and 7(= p) cross roads. We assume
that at every minute M = 20 cars enter and leave the network, that the length
of the streets sj are collected in the vector L = (1, 1, 1.5, 1.5, 1.5, 2.2, 1.5, 1.5,
2.2, 1.5, 2.2) km (the jth component refers to the length of sj street, see
Figure 7.3), that the maximum speed allowed on every street is 1 km/min and
that the maximum car densities on every street are (the ordered components of
the vector) ρm = (60, 40, 20, 60, 60, 40, 60, 20, 40, 20, 60). Since we are dealing
with a constrained minimization problem with both equality and inequality
constraints, we can use the penalty method. The associated unconstrained
minimization problem will be solved by the descent method with quasi-Newton
directions, for which we need to provide the expression of the gradient of the
cost function as well as the constraints. By expressing the cost function f
and the functions associated with the constraints in terms of the independent
variables ρj, we have
f(ρ) =
 11

j=1
Lj
vj,m
ρj
1 −ρj/ρj,m

/
11

j=1
ρj,
h1(ρ) = M −
2

j=1
vj,m(1 −ρj/ρj,m)
h2(ρ) = v1,m(1 −ρ1/ρ1,m) −
4

j=3
vj,m(1 −ρj/ρj,m)
. . .
h7(ρ) =
11

j=9
vj,m(1 −ρj/ρj,m) −M
gj(ρ) = ρj
j = 1, . . . , 11
g11+j(ρ) = ρj,m −ρj
j = 1, . . . , 11.
(7.82)
For a given vector ρ, the gradient ∇f(ρ) and the matrices [∇h1(ρ), . . . , ∇hp(ρ)]
and [∇g1(ρ), . . . , ∇gn(ρ), ∇gn+1(ρ), . . . , ∇g2n(ρ)] can be built up through 3
distinct MATLAB functions grad fun.m, grad h.m, and grad g.m. By calling
the Program penalty.m, using an initial vector with unitary components and
the tolerance ε = 10−5 for the stopping test, after 5 iterations the method
converges to the vector
rho_opt =
15.0246
12.8942
4.6535
9.0594
5.5847
9.4996
0.5278
-0.0000
11.5494
6.9723
8.4272.
Its components, ordered by rows and represented in Figure 7.16, provide the
densities ρj of the cars on the streets sj that minimize the cost function. The
minimum average time founded is f(ρopt) = 2.0782 min.
■

264
7 Numerical optimization
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
2
-10
0
10
20
30
Figure 7.16. The densities ρj of the road network Problem 7.5
7.8.2 The augmented Lagrangian method
In this section we address minimization problems with equality con-
straints only, whence Ig = ∅in (7.69). The function
Lα(x, λ) = f(x) −

i∈Ih
λihi(x) + α
2

i∈Ih
h2
i (x)
(7.83)
obtained from (7.74) is called augmented Lagrangian; α > 0 is a suitable
large coeﬃcient to be assigned.
The augmented Lagrangian method is an iterative method that, at
the kth iteration, given αk and λ(k), computes
x(k) = argmin
x∈Rn
Lαk(x, λ(k))
(7.84)
in such a way that the sequence x(k) converges to a KKT point (see
(7.75)) for the Lagrangian L(x, λ) = f(x) −*
i∈Ih λihi(x).
The initial values α0 and λ(0) are set arbitrarily. The values for the
new iterations are generated as follows. The coeﬃcient αk+1 is obtained
from αk proceeding as in the penalty method discussed in Section 7.8.1.
On its hand, λ(k+1) is computed as follows. We compute ∇xLαk(x, λ(k))
and set it to zero, yielding:
∇xLαk(x(k), λ(k)) = ∇f(x(k)) −

i∈Ih
(λ(k)
i
−αkhi(x(k)))∇hi(x(k)) = 0.
By comparison with the optimality condition (7.75), we identify the new
value of λ(k+1)
i
as
λ(k+1)
i
= λ(k)
i
−μkhi(x(k)).
(7.85)

7.8 Constrained optimization
265
We now obtain x(k+1) by solving (7.84) with k replaced by k + 1.
We summarize the algorithm as follows: given α0 (tipically, α0 = 1),
ε0 (tipically, ε0 = 1/10), ε > 0, x(0)
0
∈Rn and λ(0)
0
∈Rp for k = 0, 1, . . .
until convergence
compute an approximation x(k) to (7.84) using an initial
data x(k)
0
and a tolerance εk on the stopping criterium;
if ∥∇xLαk(x(k), λ(k))∥≤ε
set x∗= x(k)(convergence achieved)
else
compute λ(k+1)
i
by (7.85)
choose αk+1 s.t. αk+1 > αk
choose εk+1 s.t. εk+1 < εk
set x(k+1)
0
= x(k)
endif
(7.86)
This algorithm is implemented in Program 7.7. Apart from lambda0
that contains the initial vector λ(0) of the Lagrange multipliers, all the
other input and output parameters coincide with those of Program 7.6.
Program 7.7. auglagrange: augmented Lagrangian method
function [x,err ,k]= auglagrange (fun ,grad_fun ,h,grad_h ,...
x0 ,lambda0 ,tol ,kmax ,kmaxd ,meth ,varargin )
% AUGLAGRANGE
Constrained
optimization
%
[X,ERR ,K]= AUGLAGRANGE (FUN ,GRAD_FUN ,H,GRAD_H ,...
%
X0 ,LAMBDA0 ,TOL ,KMAX ,KMAXD ,METH )
%
computes a local minimizer
of the cost
function
%
FUN under the
constraints
H=0, by the
augmented
%
Lagrangian
method. X0 is the
initial
point , TOL
%
the tolerance
for the stopping
test , KMAX
the
%
maximum
number of allowed
iterations .
%
GRAD_FUN
and GRAD_H
contain
the gradient
of FUN
%
and H respectively . The solution
of the
associated
%
unconstrained
minimization
problem is performed
%
by calling
either the Matlab
FMINSEARCH
function
%
(if METH =0) or the DESCENT
function (if METH >0).
%
When
METH >0, KMAXD and
METH
contain
respectively
%
the maximum
number of allowed
iterations
for the
%
function
DESCENT
and the choice of the descent
%
directions . When
METH >1
%
[X,ERR ,K]= AUGLAGRANGE (FUN ,GRAD_FUN ,H,GRAD_H ,...
%
X0 ,LAMBDA0 ,TOL ,KMAX ,KMAXD ,METHi , HESS )
%
is the correct
calling
instruction .
%
If METH =1 HESS is the
function
handle
associated

266
7 Numerical optimization
%
with
the
Hessian is required , if METH =2 HESS is a
%
suitable
approximation
of the Hessian at the step
0.
alpha0 =1;
if meth ==1,
hess =varargin {1};
elseif
meth ==2, hess =varargin {1};
else , hess =[]; end
err=tol +1; k=0; xk=x0 (:);
lambdak =lambda0 (:);
if ~isempty(h), [nh ,mh]= size(h(xk )); end
alphak=alpha0; alphak2 =alphak /2; told =0.1;
while err >tol && k< kmax
L=@(x)Lf(x,fun ,lambdak ,alphak2 ,h);
grad_L=@(x)grad_Lf (x,grad_fun ,lambdak ,alphak ,h,grad_h );
if meth ==0
options =optimset (’TolX ’,told );
[x,err ,kd]= fminsearch (L,xk ,options );
err=norm (x-xk);
else
[x,err ,kd]= descent (L,grad_L ,xk ,told ,kmaxd ,meth ,hess );
err=norm (grad_L(x));
end
lambdak =lambdak -alphak*h(x);
if kd <kmaxd , alphak=alphak *10;
alphak2=alphak /2;
else
alphak=alphak *1.5; alphak2 =alphak /2; end
k=k+1; xk=x; told =max([tol ,told /10]);
end
end % end auglagrange
function y=Lf(x,fun ,lambdak ,alphak2 ,h)
y=fun(x);
if ~isempty(h)
y=y-sum(lambdak ’*h(x))+ alphak2 *sum((h(x)).^2);
end
end % end function
Lf
function y=grad_Lf (x,grad_fun ,lambdak ,alphak ,h,grad_h)
y= grad_fun (x);
if ~isempty(h)
y=y+grad_h(x)*( alphak*h(x)- lambdak ); end
end % end function
grad_Lf
Example 7.18 To solve Problem 1 of Example 7.13 we use the augmented
Lagrangian method by calling Program 7.7 as follows:
fun=@(x)0.6*x (1).^2+0.5* x(2).*x(1)-x(2)+3*x(1);
grad_fun =@(x) [1.2*x(1)+0.5* x(2)+3;
0.5* x(1) -1];
h=@(x)x(1).^2+x(2).^2 -1;
grad_h=@(x)[2*x(1); 2*x(2)];
x0 =[1.2 ,.2]; tol =1.e-5;
kmax =500; kmaxd =100;
p=1; % number of equality
constraints
lambda0 =rand (p ,1);
meth =2; hess =eye (2);
[xmin ,err ,k]= auglagrange (fun ,grad_fun ,h,grad_h ,...
x0 ,lambda0 ,tol ,kmax ,kmax ,meth ,hess )
We have set the tolerance equal to 10−5 for the stopping test, and solved
the associated unconstrained minimization problem by quasi-Newton descent
directions (therefore setting meth=2 and hess=eye(2)).
After 5 iterations we reach convergence to the point
xmin =
-8.454667252699469e-01
5.340281045624525e-01

7.9 What we haven’t told you
267
The constraint function h at this point is equal to resh=5.6046-10. The solu-
tion to this problem is reported in Figure 7.15, left.
Should we use the penalty method instead, leaving unchanged all the other
settings, we would obtain convergence after 6 iterations to the point
xmin =
-8.454715822058602e-01
5.340328869427682e-01
with the value of h therein equal to resh=1.3320e-04. The latter value is
larger by 6 orders of magnitude than the one obtained using the augmented
Lagrangian method. Since this behaviour occurs quite often, the augmented
Lagrangian method is in general preferable in case of minimization problems
featuring only equality constraints.
■
See Exercises 7.12-7.14.
Let us summarize
1. For a constrained minimization problem, the minimizers should be
sought for among the KKT points associated with the Lagrangian
function, or among the points where the LICQ condition fails to be
satisﬁed;
2. a quadratic programming problem is one for which the cost function
is quadratic and the constraints are linear. Under suitable assump-
tions on the matrix associated with the quadratic terms and on the
constraint functions, it admits a unique minimizer that can be ob-
tained by solving a linear system;
3. a constrained minimization problem can be turned into an uncon-
strained one using a suitable penalty function. The corresponding
penalized problem can be severely ill-conditioned because of the large
value that is tipically assigned to the penalty parameter;
4. the augmented Lagrangian method is a penalty method suitable for
the search of KKT points.
7.9 What we haven’t told you
Large scale optimization problems are especially demanding in terms of
computational time and storage requirements. Both line search and trust
region methods require the factorization of the Hessian matrix or the con-
struction of suitable approximations that might be dense even when the
Hessian is sparse. Special variants featuring limited memory of the meth-
ods illustrated above have been developed, based on Conjugate Gradient
and Lanczos iterations. See for instance [Ste83, NW06, GOT05].

268
7 Numerical optimization
A classical and eﬃcient method for the solution of constrained min-
imization problems is the Sequential Quadratic Programming (SQP),
which transforms a minimization problem with cost function f and arbi-
trary constraints into the successive solution of quadratic programming
problems. At every iteration, f is approximated by a quadratic func-
tion like (7.76), then one looks for the KKT points of the associated
Lagrangian function (see for instance [Fle10], [NW06]).
In case of inequality constraints solely, the barrier methods repre-
sent an alternative to penalty methods: the cost function is modiﬁed by
adding a function depending on the inequality constraints which inhibits
an admissible point x ∈Ω to generate a successive point which is not
admissible. This barrier function is deﬁned only at the interior of the
admissible set and is unbounded on the boundary of Ω. These methods
require the initial point to be admissible, a condition hard to be fulﬁlled.
For a more in depth presentation we refer to [Ter10].
7.10 Exercises
Exercise 7.1 Compute the minimum of f(x) = (x −1)e−x2 using the golden
section method with or without quadratic interpolation.
Exercise 7.2 Two ships leave the harbour at the same time and move along
trajectories respectively described by the parametric curves
γ1(t) =
 7 cos
 t
3 + π
2

+ 5
−4 sin
 t
3 + π
2

−3 ,
γ2(t) =
 6 cos
 t
6 −π
3

−4
−6 sin
 t
3 −π
3

+ 5 .
The parameter t > 0 represents the time (in hours), whereas the positions are
expressed in miles with respect to the origin of the reference framework. Find
the minimum distance between the two ships along all their motion.
Exercise 7.3 Compute the global minima of f(x) = x4
1 + x4
2 + x3
1 + 3x1x2
2 −
3x2
1 −3x2
2 + 10 using the Nelder and Mead method.
Exercise 7.4 By setting x(0) = 3/2, d(k) = (−1)k+1, and αk = 2 + 2/3k+1,
show that the descent method generates a sequence that does not converge to
the minimizer of f(x) = x4 even though {f(x(k))} is monotonically decreasing.
Show moreover that the steplengths αk do not fulﬁll the Wolfe conditions
(7.43).
Exercise 7.5 Show that the same conclusions drawn for the previous exercise
hold by taking x(0) = −2, d(k) = 1, and αk = 3−(k+1).
Exercise 7.6 Approximate the minimizer of the Rosenbrock function deﬁned
in Example 7.3 using the descent method with diﬀerent choices of the descent
directions (7.35)–(7.38). Set x(0) = (−1.2, 1) and ε = 10−8 as tolerance for
the stopping test, plot the convergence histories for the diﬀerent choices of the
descent directions and comment on the eﬃciency of the diﬀerent methods.

7.10 Exercises
269
Exercise 7.7 Compute the minimum of f(x) = (x2
1 −x3
1x2 −2x2 + 2x1x2
2)2 +
(3 −x1x2)2 using the BFGS method and the trust region method with quasi-
Newton directions to solve problem (7.54). As initial guess try x(0) = (2, −1),
or x(0) = (2, 1), or else x(0) = (−1, −1).
Exercise 7.8 Show that the Gauss-Newton method (7.63) can be reformu-
lated as follows: for k = 0, 1, . . . until convergence, solve
min
x∈Rn
1
2∥Rk(x)∥2 with Rk(x) deﬁned in (7.64).
(7.87)
Exercise 7.9 Consider the Gauss-Newton method of Section 7.7.1. Show that
if JR(x(k)) has full rank, then the solution δx(k) of (7.63)1 is a descent direction
for the function f deﬁned in (7.61).
Exercise 7.10 Consider the table
ti
0.055
0.181
0.245
0.342
0.419
0.465
0.593
0.752
yi
2.80
1.76
1.61
1.21
1.25
1.13
0.52
0.28
and ﬁnd the least squares approximation φ(t) = x1 + x2t + x3t2 + x4e−x5t
(with unknown coeﬃcients x1, x2, . . . , x5) of the data set (ti, yi).
Exercise 7.11 Prove that the function ˜Φk(x) deﬁned in (7.65) is a quadratic
approximation of Φ obtained by approximating R(x) with its linear model
(7.64).
Exercise 7.12 We look for the optimal positioning of the warehouse that has
to provide goods to three selling points whose coordinates are reported in the
table below:
Selling point
coordinates (xi, yi) (km)
annual deliveries (units)
1
(6,3)
140
2
(-9,9)
134
3
(-8,-5)
88
The warehouse must be allocated within the region Ω = {(x, y) ∈R2 : y ≤
x −10}.
Exercise 7.13 Compute the minimum of the Quadratic Programming prob-
lem (7.76) featuring only equality constraints, with
A =
⎡
⎣
2 −1 1
−1
3 0
0
0 1
⎤
⎦,
b =
⎡
⎣
1
−2
−1
⎤
⎦,
C =
	 2 −2
0
2
1 −3

,
d =
	 1
1

.
Exercise 7.14 A material point moves with speed v(x, y) = (sin(πxy) + 1)
(2x + 3y + 4) along an elliptic trajectory whose equation is x2/4 + y2 = 1.
Find the maximum value of the velocity reached by the point as well as the
corresponding position.

8
Ordinary diﬀerential equations
A diﬀerential equation is an equation involving one or more derivatives
of an unknown function. If all derivatives are taken with respect to a
single independent variable we call it an ordinary diﬀerential equation,
whereas we have a partial diﬀerential equation when partial derivatives
are present.
A diﬀerential equation (ordinary or partial) has order p if p is the
maximum order of diﬀerentiation that is present. The next chapter will
be devoted to the study of partial diﬀerential equations, whereas in the
present chapter we will deal with ordinary diﬀerential equations of ﬁrst
order.
8.1 Some representative problems
Ordinary diﬀerential equations describe the evolution of many phenom-
ena in various ﬁelds, as we can see from the following four examples.
Problem 8.1 (Thermodynamics) Consider a body having internal
temperature T which is set in an environment with constant temperature
Te. Assume that its mass m is concentrated in a single point. Then the
heat transfer between the body and the external environment can be
described by the Stefan-Boltzmann law
v(t) = ϵγS(T 4(t) −T 4
e ),
where t is the time variable, ϵ the Stefan-Boltzmann constant (equal to
5.6·10−8J/(m2K4s) where J stands for Joule, K for Kelvin and, obviously,
m for meter, s for second), γ is the emissivity constant of the body, S
the area of its surface and v is the rate of the heat transfer. The rate
of variation of the energy E(t) = mCT (t) (where C denotes the speciﬁc
heat of the material constituting the body) equals, in absolute value,
A. Quarteroni et al., Scientiﬁc Computing with MATLAB and Octave,
Texts in Computational Science and Engineering 2,
DOI 10.1007/978-3-642-45367-0 8, © Springer-Verlag Berlin Heidelberg 2014
271

272
8 Ordinary diﬀerential equations
the rate v. Consequently, setting T (0) = T0, the computation of T (t)
requires the solution of the ordinary diﬀerential equation
dT
dt = −v
mC .
(8.1)
See Exercise 8.15 for its solution.
■
Problem 8.2 (Population dynamics) Consider a population of bac-
teria in a conﬁned environment in which no more than B elements can
coexist. Assume that, at the initial time, the number of individuals is
equal to y0 ≪B and the growth rate of the bacteria is a positive con-
stant C. In this case the rate of change of the population is proportional
to the number of existing bacteria, under the restriction that the total
number cannot exceed B. This is expressed by the diﬀerential equation
dy
dt = Cy

1 −y
B

,
(8.2)
whose solution y = y(t) denotes the number of bacteria at time t.
Assuming that two populations y1 and y2 be in competition, instead
of (8.2) we would have
dy1
dt = C1y1 (1 −b1y1 −d2y2) ,
dy2
dt = −C2y2 (1 −b2y2 −d1y1) ,
(8.3)
where C1 and C2 represent the growth rates of the two populations.
The coeﬃcients d1 and d2 govern the type of interaction between the
two populations, while b1 and b2 are related to the available quantity
of nutrients. The above equations (8.3) are called the Lotka-Volterra
equations and form the basis of various applications. For their numerical
solution, see Example 8.7.
■
Problem 8.3 (Baseball trajectory) We want to simulate the trajec-
tory of a ball from the pitcher to the catcher. By adopting the reference
frame of Figure 8.1, the equations describing the ball motion are (see
[Ada90], [GN06])
dx
dt = v,
dv
dt = F,
where x(t) = (x(t), y(t), z(t))T designates the position of the ball at time
t, v(t) = (vx(t), vy(t), vz(t))T its velocity, while F is the vector whose
components are

8.1 Some representative problems
273
x
y
z
Figure 8.1. The reference frame adopted for Problem 8.3
Fx = −F(v)vvx + Bω(vz sin φ −vy cos φ),
Fy = −F(v)vvy + Bωvx cos φ,
Fz = −g −F(v)vvz −Bωvx sin φ.
(8.4)
v is the modulus of v, B = 4.1 10−4 a normalized constant, φ is the
pitching angle, ω is the modulus of the angular velocity impressed to the
ball from the pitcher. F(v) is a friction coeﬃcient, normally deﬁned as
([GN06])
F(v) = 0.0039 +
0.0058
1 + e(v−35)/5 .
The solution of this system of ordinary diﬀerential equations is post-
poned to Exercise 8.20.
■
Problem 8.4 (Electrical circuits) Consider the electrical circuit of
Figure 8.2. We want to compute the function v(t) representing the po-
tential drop at the ends of the capacitor C starting from the initial time
t = 0 at which the switch I has been turned oﬀ. Assume that the induc-
tance L can be expressed as an explicit function of the current intensity
i, that is L = L(i). The Ohm law yields
e −d(i1L(i1))
dt
= i1R1 + v,
where R1 is a resistance. By assuming the current ﬂuxes to be directed
as indicated in Figure 8.2, upon diﬀerentiating with respect to t both
sides of the Kirchoﬀlaw i1 = i2 + i3 and noticing that i3 = Cdv/dt and
i2 = v/R2, we ﬁnd the further equation
di1
dt = C d2v
dt2 + 1
R2
dv
dt .

274
8 Ordinary diﬀerential equations
R1
R2
L
I
i1
i3
i2
e
C
Figure 8.2. The electrical circuit of Problem 8.4
We have therefore found a system of two diﬀerential equations whose
solution allows the description of the time variation of the two unknowns
i1 and v. The second equation has order two. For its solution see Example
8.8.
■
8.2 The Cauchy problem
We conﬁne ourselves to ﬁrst order diﬀerential equations, as an equation
of order p > 1 can always be reduced to a system of p equations of order
1. The case of ﬁrst order systems will be addressed in Section 8.9.
An ordinary diﬀerential equation in general admits an inﬁnite num-
ber of solutions. In order to ﬁx one of them we must impose a further
condition which prescribes the value taken by this solution at a given
point of the integration interval. For instance, the equation (8.2) admits
the family of solutions y(t) = Bψ(t)/(1 + ψ(t)) with ψ(t) = eCt+K, K
being an arbitrary constant. If we impose the condition y(0) = 1, we pick
up the unique solution corresponding to the value K = ln[1/(B −1)].
We will therefore consider the solution of the so-called Cauchy prob-
lem which takes the following form:
ﬁnd y : I ⊂R →R such that
 y′(t) = f(t, y(t))
∀t ∈I,
y(t0) = y0,
(8.5)
where I is an interval, f : I × R →R is a given function, y′ denotes the
derivative of y with respect to t, t0 is a point of I and y0 a given value
which is called the initial data.
In the following proposition we report a classical result of Analysis.

8.3 Euler methods
275
Proposition 8.1 Assume that the function f(t, y) is
1. continuous with respect to both its arguments;
2. Lipschitz-continuous with respect to its second argument, that
is, there exists a positive constant L (named Lipschitz constant)
such that
|f(t, y1) −f(t, y2)| ≤L|y1 −y2|
∀t ∈I, ∀y1, y2 ∈R.
Then the solution y = y(t) of the Cauchy problem (8.5) exists, is
unique and belongs to C1(I).
Unfortunately, explicit solutions are available only for very special
types of ordinary diﬀerential equations. In some other cases, the solution
is available only in implicit form. This is, for instance, the case with the
equation y′ = (y −t)/(y +t) whose solution satisﬁes the implicit relation
1
2 ln(t2 + y2) + arctgy
t = C,
where C is an arbitrary constant. In some other circumstances the solu-
tion is not even representable in implicit form, as in the case of the equa-
tion y′ = e−t2 whose general solution can only be expressed through a
series expansion. For all these reasons, we seek numerical methods capa-
ble of approximating the solution of every family of ordinary diﬀerential
equations for which solutions do exist.
The common strategy of all these methods consists of subdividing
the integration interval I = [t0, T ], with T < +∞, into Nh intervals of
length h = (T −t0)/Nh; h is called the discretization step, or time-step,
or steplength. Then, at each node tn = t0 + nh (for n = 1, . . . , Nh) we
seek the unknown value un which approximates yn = y(tn). The set of
values {u0 = y0, u1, . . . , uNh} represents our numerical solution.
8.3 Euler methods
A classical method, the forward Euler method, generates the numerical
solution as follows
un+1 = un + hfn,
n = 0, . . . , Nh −1
(8.6)
where we have used the shorthand notation fn = f(tn, un). This method
is obtained by considering the diﬀerential equation (8.5) at every node
tn, n = 1, . . . , Nh and replacing the exact derivative y′(tn) by means of
the incremental ratio (4.4).

276
8 Ordinary diﬀerential equations
In a similar way, using this time the incremental ratio (4.8) to ap-
proximate y′(tn+1), we obtain the backward Euler method
un+1 = un + hfn+1,
n = 0, . . . , Nh −1
(8.7)
Both methods provide an instance of a one-step method since for
computing the numerical solution un+1 at the node tn+1 we only need
the information related to the previous node tn. More precisely, in the
forward Euler method un+1 depends exclusively on the value un pre-
viously computed, whereas in the backward Euler method it depends
also on itself through the value fn+1. For this reason the ﬁrst method is
called the explicit Euler method and the second one the implicit Euler
method.
For instance, the discretization of (8.2) by the forward Euler method
requires at every step the simple computation of
un+1 = un + hCun (1 −un/B) ,
whereas using the backward Euler method we must solve the nonlinear
equation
un+1 = un + hCun+1 (1 −un+1/B) .
Thus, implicit methods are more costly than explicit methods, since, if
the function f in (8.5) is not linear, at every time level tn+1 we must
solve a nonlinear problem to compute un+1. However, we will see that
implicit methods enjoy better stability properties than explicit ones.
The forward Euler method is implemented in Program 8.1; the inte-
gration interval is tspan = [t0,tfinal], odefun is the function handle
associated with the function f(t, y(t)) which depends on the variables t
and y.
Program 8.1. feuler: forward Euler method
function [t,u]= feuler(odefun ,tspan ,y0 ,Nh ,varargin )
%FEULER
Solves
differential
equations
using the forward
%
Euler method.
%
[T,Y]= FEULER(ODEFUN ,TSPAN ,Y0 ,NH) with
TSPAN=[T0 ,TF]
%
integrates
the
system of differential
equations
%
y’=f(t,y) from
time T0 to TF with
initial
condition
%
Y0 using the
forward
Euler method on an equispaced
%
grid of NH intervals .
%
Function
ODEFUN(T,Y) must
return a vector , whose
%
elements
hold
the
evaluation
of f(t,y), of the
%
same
dimension
of Y.
%
Each
row in the
solution
array Y corresponds
to a
%
time
returned
in the
column
vector T.
%
[T,Y] = FEULER(ODEFUN ,TSPAN ,Y0 ,NH ,P1 ,P2 ,...) passes
%
the additional
parameters
P1 ,P2 ,...
to the
function
%
ODEFUN as ODEFUN(T,Y,P1 ,P2 ...).

8.3 Euler methods
277
h=( tspan(2)- tspan (1))/Nh;
y=y0 (:); % always
creates a column
vector
w=y; u=y.’;
tt=linspace (tspan(1), tspan(2), Nh +1);
for t = tt (1:end -1)
w=w+h*odefun(t,w,varargin {:});
u = [u; w.’];
end
t=tt ’;
return
The backward Euler method is implemented in Program 8.2. Note
that we have used the function fsolve for the solution of the nonlinear
problem at each step. As initial data for fsolve we use the last computed
value of the numerical solution.
Program 8.2. beuler: backward Euler method
function [t,u]= beuler(odefun ,tspan ,y0 ,Nh ,varargin )
%BEULER
Solves
differential
equations
using the
%
backward
Euler method.
%
[T,Y]= BEULER(ODEFUN ,TSPAN ,Y0 ,NH) with
TSPAN=[T0 ,TF]
%
integrates
the
system of differential
equations
%
y’=f(t,y) from
time T0 to TF with
initial
condition
%
Y0 using the
backward
Euler method on an equispaced
%
grid of NH intervals .
%
Function
ODEFUN(T,Y) must
return a vector , whose
%
elements
hold
the
evaluation
of f(t,y), of the
%
same
dimension
of Y.
%
Each
row in the
solution
array Y corresponds
to a
%
time
returned
in the
column
vector T.
%
[T,Y] = BEULER(ODEFUN ,TSPAN ,Y0 ,NH ,P1 ,P2 ,...) passes
%
the additional
parameters
P1 ,P2 ,...
to the
function
%
ODEFUN as ODEFUN(T,Y,P1 ,P2 ...).
tt=linspace (tspan(1), tspan(2), Nh +1);
y=y0 (:); % always
create a vector
column
u=y.’;
global
glob_h glob_t
glob_y
glob_odefun ;
glob_h =( tspan(2)- tspan (1))/Nh;
glob_y=y;
glob_odefun =odefun;
glob_t=tt (2);
if ( exist(’OCTAVE_VERSION ’) )
o_ver=OCTAVE_VERSION ;
version =str2num ([ o_ver(1), o_ver(3), o_ver (5)]);
end
if ( ~exist(’OCTAVE_VERSION ’) | version
>= 320 )
options =optimset ;
options .Display =’off’;
options .TolFun =1.e -12;
options .MaxFunEvals =10000;
end
for
glob_t=tt(2: end)
if ( exist(’OCTAVE_VERSION ’) & version < 320 )
w = fsolve(’beulerfun ’,glob_y );

278
8 Ordinary diﬀerential equations
else
w = fsolve(@(w) beulerfun (w),glob_y ,options );
end
u = [u; w.’];
glob_y = w;
end
t=tt ’;
clear glob_h glob_t
glob_y
glob_odefun ;
end
function [z]= beulerfun (w)
global
glob_h glob_t
glob_y
glob_odefun ;
z=w-glob_y -glob_h*glob_odefun (glob_t ,w);
end
8.3.1 Convergence analysis
A numerical method is convergent if
∀n = 0, . . . , Nh,
|yn −un| ≤C(h)
(8.8)
where C(h) is inﬁnitesimal with respect to h when h tends to zero. If
C(h) = O(hp) for some p > 0 (that is there exists a positive constant
c such that C(h) ≤chp and p is the maximum integer for which this
inequality holds), then we say that the method converges with order p.
In order to verify that the forward Euler method converges, we write
the error as follows:
en = yn −un = (yn −u∗
n) + (u∗
n −un),
(8.9)
where
u∗
n = yn−1 + hf(tn−1, yn−1)
denotes the numerical solution at time tn which we would obtain starting
from the exact solution at time tn−1; see Figure 8.3. The term yn −u∗
n in
(8.9) represents the error produced by a single step of the forward Euler
method (this error is inﬁnitesimal thanks to the consistency property),
whereas the term u∗
n −un represents the propagation from tn−1 to tn of
the error accumulated at the previous time level tn−1 (this propagation
is bounded thanks to the stability property). The method converges pro-
vided both terms tend to zero as h →0; otherwise said, convergence is
assured if the method is both consistent and stable.
Assuming that the second order derivative of y exists and is con-
tinuous, thanks to (4.6) we ﬁnd that there exists ξn ∈(tn−1, tn) such
that
yn −u∗
n = h2
2 y′′(ξn).
(8.10)

8.3 Euler methods
279
yn−1
un−1
u∗
n
un
tn
tn−1
yn
en
y = y(t)
hτn(h)
Figure 8.3. Geometrical representation of a step of the forward Euler method
The quantity
τn(h) = (yn −u∗
n)/h
is named local truncation error of the forward Euler method.
More in general, the local truncation error of a given method repre-
sents (up to a factor 1/h) the error that would be generated by forcing
the exact solution to satisfy that speciﬁc numerical scheme.
The global truncation error (or, more simply, truncation error) is
deﬁned as
τ(h) =
max
n=0,...,Nh|τn(h)|.
In view of (8.10), the truncation error for the forward Euler method
takes the following form
τ(h) = Mh/2,
(8.11)
where M = maxt∈[t0,T ] |y′′(t)|.
From (8.10) we deduce that limh→0 τ(h) = 0, and a method for which
this happens is said to be consistent. Further, we say that it is consistent
with order p if τ(h) = O(hp) for a suitable integer p ≥1.
Consider now the other term in (8.9). We have
u∗
n −un = en−1 + h [f(tn−1, yn−1) −f(tn−1, un−1)] .
(8.12)
Since f is Lipschitz-continuous with respect to its second argument, we
obtain
|u∗
n −un| ≤(1 + hL)|en−1|.

280
8 Ordinary diﬀerential equations
If e0 = 0, the previous relations yield
|en| ≤|yn −u∗
n| + |u∗
n −un|
≤h|τn(h)| + (1 + hL)|en−1|
≤

1 + (1 + hL) + . . . + (1 + hL)n−1
hτ(h)
= (1 + hL)n −1
L
τ(h) ≤eL(tn−t0) −1
L
τ(h).
We have used the identity
n−1

k=0
(1 + hL)k = [(1 + hL)n −1]/hL,
the inequality 1 + hL ≤ehL and we have observed that nh = tn −t0.
Therefore we ﬁnd
|en| ≤eL(tn−t0) −1
L
M
2 h
∀n = 0, . . . , Nh,
(8.13)
and thus we can conclude that the forward Euler method converges with
order 1. We can note that the order of this method coincides with the
order of its local truncation error. This property is shared by many
numerical methods for the numerical solution of ordinary diﬀerential
equations. The convergence estimate (8.13) is now obtained by simply
requiring f to be Lipschitz-continuous.
A better estimate, precisely
|en| ≤Mh(tn −t0)/2,
(8.14)
holds if ∂f/∂y exists and satisﬁes the further requirement ∂f(t, y)/∂y ≤
0 for all t ∈[t0, T ] and all −∞< y < ∞. Indeed, in that case, using
Taylor expansion, from (8.12) we obtain
u∗
n −un =

1 + h∂f
∂y (tn−1, ηn)

en−1,
where ηn belongs to the interval whose endpoints are yn−1 and un−1,
thus |u∗
n −un| ≤|en−1|, provided the inequality
0 < h < 2/ max
t∈[t0,T ]

∂f
∂y (t, y(t))

(8.15)
holds. Then |en| ≤|yn −u∗
n| + |en−1| ≤nhτ(h) + |e0|, whence (8.14)
owing to (8.11) and to the fact that e0 = 0. The limitation (8.15) on the
step h is in fact a stability condition, as we will see in the sequel.

8.3 Euler methods
281
Remark 8.1 (Consistency) The property of consistency is necessary in or-
der to get convergence. Actually, should it be violated, at each step the numer-
ical method would generate an error which is not inﬁnitesimal with respect to
h. The accumulation with the previous errors would inhibit the global error to
converge to zero when h →0.
■
For the backward Euler method the local truncation error reads
τn(h) = 1
h[yn −yn−1 −hf(tn, yn)].
Still using the Taylor expansion one obtains
τn(h) = −h
2 y′′(ξn)
for a suitable ξn ∈(tn−1, tn), provided y ∈C2. Thus also the backward
Euler method converges with order 1 with respect to h.
Example 8.1 Consider the Cauchy problem
⎧
⎨
⎩
y′(t) = cos(2y(t)),
t ∈(0, 1],
y(0) = 0,
(8.16)
whose solution is y(t) = 1
2arcsin((e4t −1)/(e4t + 1)). We solve it by the for-
ward Euler method (Program 8.1) and the backward Euler method (Pro-
gram 8.2). By the following commands we use diﬀerent values of h, 1/2,
1/4, 1/8, . . . , 1/512:
tspan =[0 ,1];
y0=0; f=@(t,y) cos (2*y);
u=@(t) 0.5* asin (( exp (4*t) -1)./( exp (4*t)+1));
Nh =2;
for k=1:10
[t,ufe]= feuler(f,tspan ,y0 ,Nh);
fe(k)= abs(ufe(end)-u(t(end )));
[t,ube]= beuler(f,tspan ,y0 ,Nh);
be(k)= max(abs(ube -u(t)));
Nh = 2* Nh;
end
The errors computed at the point t = 1 are stored in the variable fe (forward
Euler) and be (backward Euler), respectively. Then we apply formula (1.12)
to estimate the order of convergence. Using the following commands
p=log(abs(fe (1:end -1)./fe (2: end )))/ log (2); p(1:2: end)
1.2898
1.0349
1.0080
1.0019
1.0005
0.8770
0.9649
0.9908
0.9978
0.9994
we can verify that both methods are convergent with order 1.
■

282
8 Ordinary diﬀerential equations
Remark 8.2 (Roundoﬀerrors eﬀects) The error estimate (8.13) was de-
rived by assuming that the numerical solution {un} is obtained in exact arith-
metic. Should we account for the (inevitable) roundoﬀ-errors, the error might
blow up like O(1/h) as h approaches 0 (see, e.g., [Atk89]). This circumstance
suggests that it might be unreasonable to go below a certain threshold h∗
(which is actually extremely tiny) in practical computations.
■
See the Exercises 8.1-8.3.
8.4 The Crank-Nicolson method
By combining the generic steps of the forward and backward Euler meth-
ods we ﬁnd the so-called Crank-Nicolson method
un+1 = un + h
2 [fn + fn+1],
n = 0, . . . , Nh −1
(8.17)
This method can also be derived by applying the fundamental theorem
of integration (which we recalled in Section 1.5.3) to the Cauchy problem
(8.5), obtaining
yn+1 = yn +
tn+1

tn
f(t, y(t)) dt,
(8.18)
and then approximating the integral by the trapezoidal rule (4.19).
The local truncation error of the Crank-Nicolson method satisﬁes
τn(h) = 1
h[y(tn) −y(tn−1)] −1
2 [f(tn, y(tn)) + f(tn−1, y(tn−1))]
= 1
h
tn

tn−1
f(t, y(t)) dt −1
2 [f(tn, y(tn)) + f(tn−1, y(tn−1))] .
The last equality follows from (8.18) and expresses, up to a factor of 1/h,
the error associated with the trapezoidal rule for numerical integration
(4.19). If we assume that y ∈C3 and use (4.20), we deduce that
τn(h) = −h2
12y′′′(ξn) for a suitable ξn ∈(tn−1, tn).
(8.19)
Thus the Crank-Nicolson method is consistent with order 2, i.e. its lo-
cal truncation error tends to 0 as h2. Using a similar approach to that
followed for the forward Euler method, we can show that the Crank-
Nicolson method is convergent with order 2 with respect to h.

8.4 The Crank-Nicolson method
283
The Crank-Nicolson method is implemented in the Program 8.3. In-
put and output parameters are the same as for the Euler methods.
Program 8.3. cranknic: Crank-Nicolson method
function [t,u]= cranknic (odefun ,tspan ,y0 ,Nh , varargin )
% CRANKNIC
Solves
differential
equations
using the
%
Crank -Nicolson
method.
%
[T,Y]= CRANKNIC (ODEFUN ,TSPAN ,Y0 ,NH)
with
%
TSPAN=[T0 ,TF] integrates
the
system of differential
%
equations y’=f(t,y) from
time T0 to TF with
initial
%
condition
Y0 using the Crank -Nicolson
method on an
%
equispaced
grid of NH intervals .
%
Function
ODEFUN(T,Y) must
return a vector , whose
%
elements
hold
the
evaluation
of f(t,y), of the
%
same
dimension
of Y.
%
Each
row in the
solution
array Y corresponds
to a
%
time
returned
in the
column
vector T.
%
[T,Y] = CRANKNIC (ODEFUN ,TSPAN ,Y0 ,NH ,P1 ,P2 ,...)
%
passes the
additional
parameters
P1 ,P2 ,... to the
%
function
ODEFUN as ODEFUN(T,Y,P1 ,P2 ...).
tt=linspace (tspan(1), tspan(2), Nh +1);
y=y0 (:); % always
create a vector
column
u=y.’;
global
glob_h glob_t
glob_y
glob_odefun ;
glob_h =( tspan(2)- tspan (1))/Nh;
glob_y=y;
glob_odefun =odefun;
if ( exist(’OCTAVE_VERSION ’) )
o_ver=OCTAVE_VERSION ;
version =str2num ([ o_ver(1), o_ver(3), o_ver (5)]);
end
if( ~exist(’OCTAVE_VERSION ’)
| version
>= 320 )
options =optimset ;
options .Display =’off’;
options .TolFun =1.e -12;
options .MaxFunEvals =10000;
end
for
glob_t=tt(2: end)
if ( exist(’OCTAVE_VERSION ’) & version < 320 )
w = fsolve(’cranknicfun ’,glob_y );
else
w = fsolve(@(w) cranknicfun (w),glob_y ,options );
end
u = [u; w.’];
glob_y = w;
end
t=tt ’;
clear glob_h glob_t
glob_y
glob_odefun ;
end
function z=cranknicfun (w)
global
glob_h glob_t
glob_y
glob_odefun ;
z=w - glob_y - ...
0.5* glob_h *( glob_odefun (glob_t ,w) + ...
glob_odefun (glob_t -glob_h ,glob_y ));
end

284
8 Ordinary diﬀerential equations
Example 8.2 Let us solve the Cauchy problem (8.16) by using the Crank-
Nicolson method with the same values of h as used in Example 8.1. The results
show that the error tends to zero with order p = 2 with respect to h:
y0 =0;
tspan=[0 1]; N=2; f=@(t,y) cos (2*y);
y=@(t) 0.5* asin (( exp (4*t) -1)./( exp (4*t)+1));
for k=1:10
[tt ,u]= cranknic (f,tspan ,y0 ,N);
e(k)= max(abs(u-y(tt ))); N=2*N;
end
p=log(abs(e(1:end -1)./e(2: end )))/ log (2); p(1:2: end)
1.9627
1.9986
2.0001
1.9999
2.0000
■
See the Exercises 8.4-8.5.
8.5 Zero-stability
Commonly speaking, by stability of a numerical scheme we mean its
capability to keep the eﬀects on the solution of data perturbations under
control.
Among several concepts of stability, there is the zero-stability, which
guarantees that, in a ﬁxed bounded interval, small perturbations of data
yield bounded perturbations of the numerical solution when h →0.
More precisely, a numerical method for the approximation of problem
(8.5), with I = [t0, T ], is zero-stable if
∃h0 > 0, ∃C > 0, ∃ε0 > 0 s.t. ∀h ∈(0, h0], ∀ε ∈(0, ε0], if |ρn| ≤ε, 0 ≤
n ≤Nh, then
|zn −un| ≤Cε,
0 ≤n ≤Nh,
(8.20)
where:
- C is a constant which might depend on the length T −t0 of the inte-
gration interval I, but is independent of h;
- zn is the solution that would be obtained by applying the numerical
method at hand to a perturbed problem;
- ρn denotes the size of the perturbation introduced at the nth step;
- ε indicates the maximum size of the perturbation.
Obviously, ε0 and ε must be small enough to guarantee that the
perturbed problem still has a unique solution on the integration interval
I.
For instance, in the case of the forward Euler method un satisﬁes the
problem
un+1 = un + hf(tn, un),
n = 0, . . . , Nh −1
u0 = y0,
(8.21)

8.5 Zero-stability
285
whereas zn satisﬁes the perturbed problem
zn+1 = zn + h [f(tn, zn) + ρn+1] ,
n = 0, . . . , Nh −1
z0 = y0 + ρ0.
(8.22)
For a consistent one-step method zero-stability follows for the prop-
erty of f to be Lipschitz-continuous with respect to its second argument
(see, e.g. [QSS07]). In that case, the constant C that appears in (8.20)
depends on exp((T −t0)L), where L is the Lipschitz constant.
However, this is not necessarily true for other families of methods.
Assume for instance that the numerical method can be written in the
general form
un+1 =
p

j=0
ajun−j + h
p

j=0
bjfn−j + hb−1fn+1, n = p, p + 1, . . .
(8.23)
for suitable coeﬃcients {ak} and {bk} and for an integer p ≥0.
Formula (8.23) deﬁnes an important family of methods, the linear
multistep methods and p+1 denotes the number of steps. These methods
will be analyzed with more details in Section 8.7. The initial values
u0, u1, . . . ,up must be provided. Apart from u0, which is equal to y0, the
other values u1, . . . , up can be generated by suitable accurate methods
such as e.g., the Runge-Kutta methods that we will address in Section
8.7.
The polynomial
π(r) = rp+1 −
p

j=0
ajrp−j
(8.24)
is called the ﬁrst characteristic polynomial associated with the numer-
ical method (8.23), and we denote its roots by rj, j = 0, . . . , p. It can
be proved that the method (8.23) is zero-stable iﬀthe following root
condition is satisﬁed:

|rj| ≤1 for all j = 0, . . . , p,
furthermore π′(rj) ̸= 0 for those j such that |rj| = 1.
(8.25)
For example, for the forward Euler method we have
p = 0, a0 = 1, b−1 = 0, b0 = 1,
for the backward Euler method we have
p = 0, a0 = 1, b−1 = 1, b0 = 0,

286
8 Ordinary diﬀerential equations
and for the Crank-Nicolson method we have
p = 0, a0 = 1, b−1 = 1/2, b0 = 1/2.
In all cases there is only one root of π(r) which is equal to 1 and therefore
all these methods are zero-stable.
The following property, known as Lax-Richtmyer equivalence the-
orem, is most crucial in the theory of numerical methods (see, e.g.,
[IK66]), and highlights the fundamental role played by the property of
zero-stability:
Any consistent method is convergent iﬀit is zero-stable
Coherently with what done before, the local truncation error for the
multistep method (8.23) is deﬁned as follows
τn(h) = 1
h
⎧
⎨
⎩yn+1 −
p

j=0
ajyn−j
−h
p

j=0
bjf(tn−j, yn−j) −hb−1f(tn+1, yn+1)
⎫
⎬
⎭.
(8.26)
As already noticed, the method is said to be consistent if τ(h) =
max |τn(h)| tends to zero when h tends to zero. By a tedious use of
Taylor expansions we can prove that this condition is equivalent to re-
quire that
p

j=0
aj = 1,
−
p

j=0
jaj +
p

j=−1
bj = 1
(8.27)
which in turns amounts to say that r = 1 is a root of the polynomial
π(r) introduced in (8.24) (see, e.g., [QSS07, Chapter 11]).
8.6 Stability on unbounded intervals
In the previous section we considered the solution of the Cauchy problem
on bounded intervals. In that context, the number Nh of subintervals
becomes inﬁnite only if h goes to zero.
On the other hand, there are several situations in which the Cauchy
problem needs to be integrated on very large (virtually inﬁnite) time
intervals. In this case, even if h is ﬁxed, Nh tends to inﬁnity, and then
results like (8.13) become meaningless as the right hand side of the in-
equality contains an unbounded quantity. We are therefore interested in

8.6 Stability on unbounded intervals
287
methods that are able to approximate the solution for arbitrarily long
time intervals, even with a steplength h relatively “large”.
Unfortunately, the inexpensive forward Euler method does not enjoy
this property. To see this, let us consider the following model problem
y′(t) = λy(t),
t ∈(0, ∞),
y(0) = 1,
(8.28)
where λ is a negative real number. The exact solution is y(t) = eλt, which
tends to 0 as t tends to inﬁnity. Applying the forward Euler method to
(8.28) we ﬁnd that
u0 = 1,
un+1 = un(1 + λh) = (1 + λh)n+1,
n ≥0. (8.29)
Thus limn→∞un = 0 iﬀ
−1 < 1 + hλ < 1,
i.e.
h < 2/|λ|
(8.30)
This condition expresses the requirement that, for ﬁxed h, the numer-
ical solution should reproduce the behavior of the exact solution when
tn tends to inﬁnity. If h > 2/|λ|, then limn→∞|un| = +∞; thus (8.30) is
a stability condition. The property that
lim
n→∞un = 0
(8.31)
is called absolute stability.
Example 8.3 Let us apply the forward Euler method to solve problem (8.28)
with λ = −1. In that case we must have h < 2 for absolute stability. In Figure
8.4 we report the solutions obtained on the interval [0, 30] for 3 diﬀerent values
of h: h = 30/14 (which violates the stability condition), h = 30/16 (which
satisﬁes, although by a little amount only, the stability condition) and h = 1/2.
We can see that in the ﬁrst two cases the numerical solution oscillates. However
only in the ﬁrst case (which violates the stability condition) the absolute value
of the numerical solution does not vanish at inﬁnity (and actually it diverges).
.
■
Similar conclusions hold when λ is either a complex number (see Section
8.6.1) or when λ = λ(t) in (8.28) is a negative function of t in (8.28).
However in the latter case, |λ| must be replaced by maxt∈[0,∞) |λ(t)| in
the stability condition (8.30). This condition could however be relaxed
to one which is less restrictive by using a variable steplength hn which
accounts for the local behavior of |λ(t)| in every interval (tn, tn+1).
In particular, the following adaptive forward Euler method could be
used:
choose u0 = y0 and h0 = 2α/|λ(t0)|; then

288
8 Ordinary diﬀerential equations
0
5
10
15
20
25
30
−6
−4
−2
0
2
4
6
8
Figure 8.4. Solutions of problem (8.28), with λ = −1, obtained by the forward
Euler method, corresponding to h = 30/14 (> 2) (dashed line), h = 30/16 (<
2) (solid line) and h = 1/2 (dashed-dotted line)
for n = 0, 1, . . . , do
tn+1 = tn + hn,
un+1 = un + hnλ(tn)un,
hn+1 = 2α/|λ(tn+1)|,
(8.32)
where α is a constant which must be less than 1 in order to have an
absolutely stable method.
For instance, consider the problem
y′(t) = −(e−t + 1)y(t),
t ∈(0, 10),
with y(0) = 1. Since |λ(t)| is decreasing, the most restrictive condition for
absolute stability of the forward Euler method is h < h0 = 2/|λ(0)| = 1.
In Figure 8.5, left, we compare the solution of the forward Euler method
with that of the adaptive method (8.32) for three values of α. Note
that, although every α < 1 is admissible for stability purposes, to get
an accurate solution requires choosing α suﬃciently small. In Figure 8.5,
right, we also plot the behavior of hn on the interval (0, 10] corresponding
to the three values of α. This picture clearly shows that the sequence
{hn} increases monotonically with n.
In contrast to the forward Euler method, neither the backward Eu-
ler method nor the Crank-Nicolson method require limitations on h for
absolute stability. In fact, with the backward Euler method we obtain
un+1 = un + λhun+1 and therefore
un+1 =

1
1 −λh
n+1
,
n ≥0,

8.6 Stability on unbounded intervals
289
0.5
1
1.5
2
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
α = 0.4
α = 0.45
α = 0.3
t
0
2
4
6
8
10
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
α = 0.4
α = 0.45
α = 0.3
t
h
Figure 8.5. Left: the numerical solution on the time interval (0.5, 2) obtained
by the forward Euler method with h = αh0 (dashed line) and by the adaptive
variable stepping forward Euler method (8.32) (solid line) for three diﬀerent
values of α. Right: the behavior of the variable steplength hn for the adaptive
method (8.32)
which tends to zero as n →∞for all values of h > 0. Similarly, with
the Crank-Nicolson method we obtain
un+1 =
.
1 + hλ
2
 4 
1 −hλ
2
/n+1
,
n ≥0,
which still tends to zero as n →∞for all possible values of h > 0. We
can conclude that the forward Euler method is conditionally absolutely
stable, while both the backward Euler and Crank-Nicolson methods are
unconditionally absolutely stable.
8.6.1 The region of absolute stability
If in (8.28) λ is a complex number with negative real part, the solution
u(t) = eλt still tends to 0 when t tends to inﬁnity.
We call region of absolute stability A of a numerical method the
set of complex numbers z = hλ for which the method turns out to be
absolutely stable (that is, limn→∞un = 0).
The region of absolute stability of forward Euler method is given by
those numbers hλ ∈C such that |1 + hλ| < 1, thus it coincides with the
circle of radius one and with centre (−1, 0). This yields an upper bound
h < −2Re(λ)/|λ|2 for the steplength. For the backward Euler method
the property of absolute stability is instead satisﬁed by all values of hλ
which are exterior to the circle of radius one centered in (1, 0) (see Figure
8.6). Finally, the region of absolute stability of Crank-Nicolson method
coincides with the left hand complex plane of numbers with negative real
part.
Methods that are unconditionally absolutely stable for all complex
number λ in (8.28) with negative real part are called A-stable. Backward

290
8 Ordinary diﬀerential equations
1
−1
Im(hλ)
Im(hλ)
Im(hλ)
Re(hλ)
Re(hλ)
Re(hλ)
Figure 8.6. The absolute stability regions (in cyan) of the forward Euler
method (left), backward Euler method (centre) and Crank-Nicolson method
(right)
Euler and Crank-Nicolson method are therefore A-stable, and so are
many other implicit methods. This property makes implicit methods
attractive in spite of being computationally more expensive than explicit
methods.
Example 8.4 Let us compute the restriction on h when using the forward
Euler method to solve the Cauchy problem y′(t) = λy with λ = −1 + i. This
λ stands on the boundary of the absolute stability region A of the forward
Euler method. Thus, any h such that h ∈(0, 1) will suﬃce to guarantee that
hλ ∈A. If it were λ = −2 + 2i we should choose h ∈(0, 1/2) in order to bring
hλ within the stability region A.
■
8.6.2 Absolute stability controls perturbations
Consider now the following generalized model problem
 y′(t) = λ(t)y(t) + r(t),
t ∈(0, +∞),
y(0) = 1,
(8.33)
where λ and r are two continuous functions and −λmax ≤λ(t) ≤−λmin
with 0 < λmin ≤λmax < +∞. In this case the exact solution does not
necessarily tend to zero as t tends to inﬁnity; for instance if both r and
λ are constants we have
y(t) =

1 + r
λ

eλt −r
λ
whose limit when t tends to inﬁnity is −r/λ. Thus, in general, it does
not make sense to require a numerical method to be absolutely stable,
i.e. to satisfy (8.31), when applied to problem (8.33). However, we are
going to show that a numerical method which is absolutely stable on
the model problem (8.28), if applied to the generalized problem (8.33),
guarantees that the perturbations are kept under control as t tends to
inﬁnity (possibly under a suitable constraint on the time-step h).
For the sake of simplicity we will conﬁne our analysis to the forward
Euler method; when applied to (8.33) it reads

8.6 Stability on unbounded intervals
291
un+1 = un + h(λnun + rn),
n ≥0,
u0 = 1
and its solution is (see Exercise 8.9)
un = u0
n−1

k=0
(1 + hλk) + h
n−1

k=0
rk
n−1

j=k+1
(1 + hλj),
(8.34)
where λk = λ(tk) and rk = r(tk), with the convention that the last
product is equal to one if k + 1 > n −1. Let us consider the following
“perturbed” method

zn+1 = zn + h(λnzn + rn + ρn+1),
n ≥0,
z0 = u0 + ρ0,
(8.35)
where ρ0, ρ1, . . . are given perturbations which are introduced at every
time level. This is a simple model in which ρ0 and ρn+1, respectively,
account for the fact that neither u0 nor rn can be determined exactly.
(Should we account for all roundoﬀerrors which are actually introduced
at any step, our perturbed model would be far more involved and diﬃ-
cult to analyze.) The solution of (8.35) reads like (8.34), provided uk is
replaced by zk and rk by rk + ρk+1, for all k = 0, . . . , n −1. Then
zn −un = ρ0
n−1

k=0
(1 + hλk) + h
n−1

k=0
ρk+1
n−1

j=k+1
(1 + hλj).
(8.36)
The quantity |zn −un| is called the perturbation error at step n. It is
worth noticing that this quantity does not depend on the function r(t).
i. For the sake of exposition, let us consider ﬁrst the special case where
λk and ρk are two constants equal to λ and ρ, respectively. Assume that
h < h0(λ) = 2/|λ|, which is the condition on h that ensures the absolute
stability of the forward Euler method applied to the model problem
(8.28). Then, using the following identity for the geometric sum
n−1

k=0
ak = 1 −an
1 −a ,
if |a| ̸= 1,
(8.37)
we obtain
zn −un = ρ

(1 + hλ)n

1 + 1
λ

−1
λ
5
.
(8.38)
It follows that the perturbation error satisﬁes (see Exercise 8.10)

292
8 Ordinary diﬀerential equations
0
10
20
30
40
50
60
70
80
90
100
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0
10
20
30
40
50
60
70
80
90
100
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Figure 8.7. The perturbation error when r(t) ≡0, ρ = 0.1: λ = −2 (left) and
λ = −0.5 (right). In both cases h = h0(λ) −0.01
|zn −un| ≤ϕ(λ)|ρ|,
(8.39)
with ϕ(λ) = 1 if λ ≤−1, while ϕ(λ) = |1 + 2/λ| if −1 < λ < 0. The
conclusion that can be drawn is that the perturbation error is bounded
by |ρ| times a constant which depends on λ but is independent of both
n and h. Moreover, from (8.38) it follows
lim
n→∞|zn −un| = |ρ|
|λ|.
Figure 8.7 corresponds to the case where r(t) ≡0, ρ = 0.1, λ = −2
(left) and λ = −0.5 (right). In both cases we have taken h = h0(λ) −
0.01. Note that the estimate (8.38) is exactly satisﬁed. Obviously, the
perturbation error blows up when n increases if the stability limit h <
h0(λ) is violated.
Remark 8.3 If the unique perturbation is on the initial data, i.e. if ρk = 0,
k = 1, 2, . . ., from (8.36) we deduce that limn→∞|zn −un| = 0 under the
stability condition h < h0(λ).
■
ii. In the general case where λ and r are non-constant, let us require
h to satisfy the restriction h < h0(λ), where this time h0(λ) = 2/λmax.
Then,
|1 + hλk| ≤a(h) = max{|1 −hλmin|, |1 −hλmax|}.
Since 0 < λmax−λmin
λmax+λmin ≤a(h) < 1, we can still use the identity (8.37) in
(8.36) and obtain
|zn −un| ≤ρ

[a(h)]n + h1 −[a(h)]n
1 −a(h)

,
(8.40)

8.6 Stability on unbounded intervals
293
where ρ = supk |ρk|. First, let us take h ≤h∗= 2/(λmin + λmax), so
that a(h) = (1 −hλmin). It holds
|zn −un| ≤
ρ
λmin
[1 −[a(h)]n(1 −λmin)] ,
(8.41)
i.e.,
sup
n |zn −un| ≤
ρ
λmin
sup
n [1 −[a(h)]n(1 −λmin)].
If λmin = 1, we have
sup
n |zn −un| ≤ρ.
(8.42)
If λmin < 1, the sequence bn = [1 −[a(h)]n(1 −λmin)] monotonically
increases with n, so that supn bn = limn→∞bn = 1 and
sup
n |zn −un| ≤
ρ
λmin
.
(8.43)
Finally, if λmin > 1, the sequence bn monotonically decreases, supn bn =
b0 = λmin, and the estimate (8.42) holds too.
Let us take now h∗< h < h0(λ), we have
1 + hλk = 1 −h|λk| ≤1 −h∗|λk| ≤1 −h∗λmin.
(8.44)
Using (8.44), identity (8.37) in (8.36), and setting a = 1 −h∗λmin, we
ﬁnd
zn −un ≤ρ

an + h1 −an
1 −a

=
ρ
λmin

an

λmin −h
h∗

+ h
h∗

.
(8.45)
We note that two possible situations arise.
If λmin ≥h
h∗, then h
h∗≤an

λmin −h
h∗

+ h
h∗< λmin and we ﬁnd
zn −un ≤ρ
∀n ≥0.
(8.46)
Otherwise, if λmin < h
h∗, then λmin ≤an 
λmin −h
h∗

+ h
h∗<
h
h∗and
zn −un ≤
ρ
λmin
h
h∗≤
ρ
λmin
h0
h∗= ρ

1
λmin
+
1
λmax

.
(8.47)
Note that the right hand side (8.47) is also an upper bound for the
absolute value of zn −un. In Figure 8.8 we report the perturbation
errors computed on the problem (8.33), where r(t) ≡0, λk = λ(tk) =

294
8 Ordinary diﬀerential equations
0
50
100
150
200
250
300
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
ρ
0
50
100
150
200
250
300
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
ρ(1/λmin + 1/λmax)
Figure 8.8. The perturbation error when ρ(t) = 0.1 sin(t) and λ(t) = −2 −
sin(t) for t ∈(0, nh) with n = 500: the steplength is h = h∗−0.1 = 0.4 (left)
and h = h∗+ 0.1 = 0.6 (right). In this case λmin = 1, so that the estimate
(8.42) holds when h ≤h∗, while (8.47) holds when h > h∗
−2 −sin(tk), ρk = ρ(tk) = 0.1 sin(tk) with h ≤h∗(left) and with
h∗< h < h0(λ) (right).
iii. We consider now the Cauchy problem (8.5) with a general func-
tion f(t, y(t)). We claim that this problem can be related to the gener-
alized model problem (8.33), in those cases where
−λmax < ∂f
∂y (t, y) < −λmin
∀t ≥0, ∀y ∈(−∞, ∞),
(8.48)
for suitable values λmin, λmax ∈(0, +∞). To this end, for every t in the
generic interval (tn, tn+1), we subtract (8.6) from (8.22) to obtain the
following equation for the perturbation error
zn −un = (zn−1 −un−1) + h{f(tn−1, zn−1) −f(tn−1, un−1)} + hρn.
By applying the mean-value theorem we obtain
f(tn−1, zn−1) −f(tn−1, un−1) = λn−1(zn−1 −un−1),
where λn−1 = fy(tn−1, ξn−1), ξn−1 is a suitable point in the interval
whose endpoints are un−1 and zn−1 and fy is a shorthand notation for
∂f/∂y. Thus
zn −un = (1 + hλn−1)(zn−1 −un−1) + hρn.
By a recursive application of this formula we obtain the identity (8.36),
from which we derive the same conclusions drawn in ii., provided the
stability restriction 0 < h < 2/λmax holds. Note that this is precisely
the condition (8.15).

8.6 Stability on unbounded intervals
295
0
20
40
60
80
100
0
0.5
1
1.5
2
2.5
3
3.5
Figure 8.9. The perturbation errors when ρ(t) = sin(t) with h = h0 −0.01
(thick line) and h = h0 + 0.01 (thin line) for the Cauchy problem (8.49);
h0 = 2/3
Example 8.5 Let us consider the Cauchy problem
y′(t) = arctan(3y) −3y + t, t > 0, y(0) = 1.
(8.49)
Since fy = 3/(1 + 9y2) −3 is negative, we can choose λmax = max |fy| = 3
and set h < h0 = 2/3. Thus, we can expect that the perturbations on the
forward Euler method are kept under control provided that h < 2/3. This is
conﬁrmed by the results which are reported in Figure 8.9. Note that in this
example, taking h = 2/3+0.01 (thus violating the previous stability limit) the
perturbation error blows up as t increases.
■
Example 8.6 We seek an upper bound on h that guarantees stability for the
forward Euler method applied to approximate the Cauchy problem
y′ = 1 −y2,
t > 0,
(8.50)
with y(0) = e −1
e + 1. The exact solution is y(t) = (e2t+1 −1)/(e2t+1 + 1) and
fy = −2y. Since fy ∈(−2, −0.9) for all t > 0, we can take h less than h0 = 1.
In Figure 8.10, left, we report the solutions obtained on the interval (0, 35)
with h = 0.95 (thick line) and h = 1.05 (thin line). In both cases the solution
oscillates, but remains bounded. Moreover in the ﬁrst case, which satisﬁes the
stability constraint, the oscillations are damped and the numerical solution
tends to the exact one as t increases. In Figure 8.10, right, we report the
perturbation errors corresponding to ρ(t) = sin(t) with h = h∗= 2/2.9 (thick
solid line) and h = 0.9 (thin dashed line). In both cases the perturbation errors
remain bounded; precisely, estimate (8.42) is satisﬁed when h = h∗= 2/2.9,
while estimate (8.47) holds when h∗< h = 0.9 < h0.
■
In those cases where no information on y is available, ﬁnding the
value λmax = max |fy| is not a simple matter. A more heuristic approach
could be pursued in these situations, by adopting a variable stepping
procedure. Precisely, one could take tn+1 = tn + hn, where

296
8 Ordinary diﬀerential equations
0
5
10
15
20
25
30
35
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
0
10
20
30
40
50
60
70
80
90
100
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 8.10. At left, numerical solutions of problem (8.50) obtained by the
forward Euler method with h = 1.05 (thin line) and h = 0.95 (thick line). The
values of the exact solution are indicated by circles. On the right, perturbation
errors corresponding to ρ(t) = sin(t) with h = h∗= 2/2.9 (thick solid line)
and h = 0.9 (thin dashed line)
0
10
20
30
40
50
60
70
80
90
100
0
0.2
0.4
0.6
0.8
1
Figure 8.11. The perturbation errors corresponding to ρ(t) = sin(t) with
α = 0.8 (thick line) and α = 0.9 (thin line) for the Example 8.6, using the
adaptive strategy
0 < hn < 2
α
|fy(tn, un)|,
(8.51)
for suitable values of α strictly less than 1. Note that the denominator
depends on the value un which is known. In Figure 8.11 we report the
perturbation errors corresponding to the Example 8.6 for two diﬀerent
values of α.
The previous analysis can be carried out also for other kind of one-
step methods, in particular for the backward Euler and Crank-Nicolson
methods. For these methods which are A-stable, the same conclusions
about the perturbation error can be drawn without requiring any limita-
tion on the time-step. In fact, in the previous analysis one should replace
each term 1 + hλn by (1 −hλn)−1 in the backward Euler case and by
(1 + hλn/2)/(1 −hλn/2) in the Crank-Nicolson case.

8.6 Stability on unbounded intervals
297
8.6.3 Stepsize adaptivity for the forward Euler method
As seen in the previous sections, the steplength h should be chosen in or-
der to satisfy the absolute stability constraint, see e.g. (8.32) and (8.51).
More in general, at every time level we could in principle choose a
(variable) time-step that not only fulﬁls the stability constraint but also
guarantees that a desired accuracy be achieved. Such procedure, called
step adaptivity, requires a convenient estimate of the local error, that is
obtained from an appropriate a-posteriori error estimate. (A priori error
estimates like (8.13) or (8.14) do not serve this porpuse, as they would
require information on the second derivative of the unknown solution.)
For the sake of simplicity, we illustrate this technique on the forward
Euler method.
Assume that the numerical solution is computed up to a given time
level that, for simplicity, will be denoted t. We choose an initial guess
for h and denote by uh (respectively, uh/2) the solution at the time t+ h
provided by the forward Euler method with initial value u at time t with
time-step h (respectively, h/2), that is:
uh = u + hf(t, u),
v1 = u + h
2 f(t, u),
uh/2 = v2 = v1 + h
2 f

t + h
2 , v1

.
Let us examine the errors eh = y(t + h) −uh and eh/2 = y(t + h) −uh/2,
where now y(t) represents the exact solution to the Cauchy problem
y′(t) = f(t, y(t))
t ≥t,
y(t) = u.
(8.52)
Using (8.10) we ﬁnd
eh = h2
2 y′′(ξ)
(8.53)
for a suitable ξ ∈(t, t + h). By setting, for simplicity,
t0 = t,
t1 = t + h/2,
t2 = t + h
(see Figure 8.12) and rewriting eh/2 in the form (8.9), we ﬁnd
eh/2 = (y(t2) −v∗
2) + (v∗
2 −v2),
(8.54)
where v∗
2 = y(t1)+ h
2 f(t1, y(t1)). The former term on the right hand side
of (8.54) represents the local truncation error, thus

298
8 Ordinary diﬀerential equations
t0 = t
t1
t2 = t + h
u
v1
uh/2 = v2
uh
y(t2)
eh
eh/2
Figure 8.12. The numerical solution provided by forward Euler method with
either one step of size h and two steps of size h/2. The solid curve represents
the solution of (8.52)
y(t2) −v∗
2 = (h/2)2
2
y′′(η2)
for a suitable η2 ∈(t1, t2), whereas the latter, that is due to the error
propagation on an interval of length h/2, thanks to (8.12) reads
v∗
2 −v2 = (y(t1) −v1) + h
2 [f(t1, y(t1)) −f(t1, v1)] .
The term (y(t1) −v1) still represents a local truncation error which can
be written as y(t1) −v1 = (h/2)2
2
y′′(η1) for a suitable η1 ∈(t0, t1). On
the other hand, assuming f of class C1 and using the Lagrange theorem,
we obtain
f(t1, y(t1)) = f(t1, v1) + (y(t1) −v1)∂f
∂y (t1, ζ)
for a suitable ζ belonging to the interval whose endpoints are v1 and
y(t1). Consequently
v∗
2 −v2 = (y(t1) −v1)
.
1 + h∂f
∂y (t1, ζ)
/
= (h/2)2
2
y′′(η1) + o(h2).
Assuming that y′′ is continuous in (t, t + h), we have
eh/2 = (h/2)2
2
[y′′(η2) + y′′(η1)] + o(h2) = h2
4 y′′(η) + o(h2),
(8.55)
for a suitable η ∈(t, t + h).
A convenient estimate of y′′ can be obtained by subtracting (8.55)
from (8.53). Still assuming that y′′ is continuous in (t, t + h), we ﬁnd

8.6 Stability on unbounded intervals
299
uh/2 −uh = eh −eh/2 = h2
4 (2y′′(ξ) −y′′(η)) + o(h2) = h2
4 y′′(ˆξ) + o(h2),
for a convenient ˆξ ∈(t, t + h). On the other hand
|eh/2| ≃h2
4 |y′′(ˆξ)| ≃|uh/2 −uh|,
therefore the quantity |uh/2 −uh| provides an a-posteriori estimator of
the error |y(t + h) −uh/2| up to an inﬁnitesimal term o(h2).
To conclude, for a given tolerance ϵ, should
|uh/2 −uh| < ϵ
2
(the division by 2 is made conservatively), we accept the step h to ad-
vance and take uh/2 as our numerical solution at the new time level
t + h. Otherwise, h is halved and the above procedure is repeated until
convergence. In any case, to avoid too tiny steplengths we require that
the steplength satisﬁes h ≥hmin for a prescribed minimum value hmin.
We ﬁnally observe that sometimes the error estimator |uh/2 −uh|
is replaced by its relative counterpart |uh/2 −uh|/umax, where umax
represents the maximum value attained by the numerical solution in the
interval [t0, t].
Let us summarize
1. An absolutely stable method is one which generates a solution un of
the model problem (8.28) which tends to zero as tn tends to inﬁnity;
2. a method is said A-stable if it is absolutely stable for any possible
choice of the time-step (or steplength) h and any λ ∈C with Re(λ) <
0 (otherwise a method is called conditionally stable, and h should
be lower than a constant depending on λ);
3. when an absolutely stable method is applied to a generalized model
problem (like (8.33)), the perturbation error (that is the absolute
value of the diﬀerence between the perturbed and unperturbed so-
lution) is uniformly bounded with respect to h. In short, we can say
that absolutely stable methods keep the perturbation controlled;
4. the analysis of absolute stability for the linear model problem can be
exploited to ﬁnd stability conditions on the time-step when consider-
ing the nonlinear Cauchy problem (8.5) with a function f satisfying
(8.48). In that case the stability restriction requires the steplength
to be chosen as a function of ∂f/∂y. Precisely, the new integration
interval [tn, tn+1] is chosen in such a way that hn = tn+1 −tn satis-
ﬁes (8.51) for a suitable α ∈(0, 1), or (8.15) in the case of constant
time-step h.
See the Exercises 8.6-8.13.

300
8 Ordinary diﬀerential equations
8.7 High order methods
All methods presented so far are elementary examples of one-step meth-
ods. More sophisticated schemes, which allow the achievement of a higher
order of accuracy, are the Runge-Kutta methods and the multistep meth-
ods (whose general form was already introduced in (7.23)).
Runge-Kutta (brieﬂy, RK) methods are still one-step methods; how-
ever, they involve several evaluations of the function f(t, y) on every
interval [tn, tn+1]. In its most general form, a RK method can be written
as
un+1 = un + h
s

i=1
biKi,
n ≥0
(8.56)
where
Ki = f(tn + cih, un + h
s

j=1
aijKj),
i = 1, 2, . . ., s
and s denotes the number of stages of the method. The coeﬃcients {aij},
{ci} and {bi} fully characterize a RK method and are usually collected
in the so-called Butcher array
c
A
bT T ,
where A = (aij) ∈Rs×s, b = (b1, . . . , bs)T ∈Rs and c = (c1, . . . , cs)T ∈
Rs. If the coeﬃcients aij in A are equal to zero for j ≥i, with i =
1, 2, . . . , s, then each Ki can be explicitly computed in terms of the i −1
coeﬃcients K1, . . . , Ki−1 that have already been determined. In such a
case the RK method is explicit.
Otherwise, it is implicit and solving a nonlinear system of size s is nec-
essary for computing the coeﬃcients Ki.
One of the most celebrated Runge-Kutta methods reads
un+1 = un + h
6 (K1 + 2K2 + 2K3 + K4)
(8.57)
where
K1 = fn,
K2 = f(tn + h
2 , un + h
2K1),
K3 = f(tn + h
2 , un + h
2K2),
K4 = f(tn+1, un + hK3),
0
1
2
1
2
1
2
0
1
2
1 0
0 1
1
6
T
1
3
1
3
1
6
.

8.7 High order methods
301
This method can be derived from (8.18) by using the Simpson quadrature
rule (4.23) to evaluate the integral between tn and tn+1. It is explicit,
of fourth order with respect to h; at each time level, it involves four
new evaluations of the function f. Other Runge-Kutta methods, either
explicit or implicit, with arbitrary order can be constructed. For instance,
an implicit RK method of order 4 with 2 stages is deﬁned by the following
Butcher array
3−
√
3
6
1
4
3−2
√
3
12
3+
√
3
6
3+2
√
3
12
1
4
1
2
T
1
2
.
The absolute stability region A of the RK methods, including explicit
RK methods, can grow in surface with the order: an example is provided
by the left graph in Figure 8.14, where A has been reported for some
explicit RK methods of increasing order: RK1, i.e. the forward Euler
method; RK2, the so called improved Euler method that will be derived
later (see (8.64)); RK3, the method associated with the following Butcher
array
0
1
2
1
2
1 −1 2
1
6
T
2
3
1
6
(8.58)
and RK4, the method (8.57) introduced previously.
As done for the forward Euler method, also RK method, as one-step
methods, are well-suited for implementing a steplength adaptivity.
The error estimator for these methods can be constructed in two ways:
- using the same RK method, but with two diﬀerent steplengths (as done
for the Euler method);
- using two RK methods of diﬀerent order, but with the same number s
of stages.
The latter procedure is the one used by MATLAB inside the functions
ode23 and ode45, see below.
RK methods stand at the base of a family of MATLAB programs
whose names contain the root ode followed by numbers and letters. In
ode
particular, ode45 is based on a pair of explicit Runge-Kutta methods (the
ode45
so-called Dormand-Prince pair) of order 4 and 5, respectively. ode23 is
ode23
the implementation of another pair of explicit Runge-Kutta methods (the
Bogacki and Shampine pair). In these methods the integration step varies
in order to guarantee that the error remains below a given tolerance
(the default scalar relative error tolerance RelTol is equal to 10−3).
The program ode23tb is an implementation of an implicit Runge-Kutta
ode23tb

302
8 Ordinary diﬀerential equations
formula whose ﬁrst stage is the trapezoidal rule, while the second stage
is a backward diﬀerentiation formula of order two (see (8.61)).
Multistep methods (see (8.23)) achieve a high order of accuracy by
involving the values un, un−1, . . . , un−p for the determination of un+1.
They can be derived by applying ﬁrst the formula (8.18) and then ap-
proximating the integral by a quadrature formula which involves the in-
terpolant of f at a suitable set of nodes. A notable example of multistep
method is the three-step (p = 2), third order (explicit) Adams-Bashforth
formula (AB3)
un+1 = un + h
12 (23fn −16fn−1 + 5fn−2)
(8.59)
which is obtained by replacing f in (8.18) by its interpolating polynomial
of degree two at the nodes tn−2, tn−1, tn. Another important example is
the three-step, fourth order (implicit) Adams-Moulton formula (AM4)
un+1 = un + h
24 (9fn+1 + 19fn −5fn−1 + fn−2)
(8.60)
which is obtained by replacing f in (8.18) by its interpolating polynomial
of degree three at the nodes tn−2, tn−1, tn, tn+1.
Another family of multistep methods can be obtained by writing the
diﬀerential equation at time tn+1 and replacing y′(tn+1) by a one-sided
incremental ratio of high order. An instance is provided by the two-step,
second order (implicit) backward diﬀerence formula (BDF2)
un+1 = 4
3un −1
3un−1 + 2h
3 fn+1
(8.61)
or by the following three-step, third order (implicit) backward diﬀerence
formula (BDF3)
un+1 = 18
11un −9
11un−1 + 2
11un−2 + 6h
11 fn+1
(8.62)
All these methods can be recasted in the general form (8.23). It is easy
to verify that for all of them the relations (8.27) are satisﬁed, thus these
methods are consistent. Moreover, they are zero-stable. Indeed, in both
cases (8.59) and (8.60), the ﬁrst characteristic polynomial is π(r) =
r3 −r2 and its roots are r0 = 1, r1 = r2 = 0; that of (8.61) is π(r) =
r2 −(4/3)r + 1/3 and its roots are r0 = 1 and r1 = 1/3, while the ﬁrst
characteristic polynomial of (8.62) is π(r) = r3 −18/11r2 +9/11r −2/11
and its roots are r0 = 1, r1 = 0.3182 + 0.2839i, r2 = 0.3182 −0.2839i,

8.7 High order methods
303
-2
-1.5
-1
-0.5
0
-1
-0.5
0
0.5
1
Re(h λ)
Im(h λ)
AB1
AB2
AB3
AB4
-6
-4.5
-3
-1.5
0
-3
-1.5
0
1.5
3
Re(h λ)
Im(hλ)
AM3
AM4
AM5
Figure 8.13. The absolute stability regions of several Adams-Basforth (left)
and Adams-Moulton (right) methods
where i is the imaginary unit. In all cases, the root condition (8.25) is
satisﬁed.
When applied to the model problem (8.28), for any λ ∈R−the
method AB3 is absolutely stable if h < 0.545/|λ|, while AM4 is abso-
lutely stable if h < 3/|λ|. The method BDF2 is unconditionally abso-
lutely stable for any λ ∈C with negative real part (i.e., A-stable). If
λ ∈R−, BDF3 is unconditionally absolutely stable, however this is no
longer true for any λ ∈C with negative real part; in other words, BDF3
fails to be A-stable (see, Figure 8.14). More generally, according to the
second Dahlquist barrier there is no multistep A-stable method of order
strictly greater than two.
In Figures 8.13 the regions of absolute stability of several Adams-
Bashfort and Adams-Moulton methods are drawn. Note that their size
reduces as far as the order increases. In the right-hand side graphs of
Figure 8.14 we report the (unbounded) absolute stability regions of some
BDF methods: note that ABDF (k+1) ⊂ABDF (k) as opposed to those of
the Runge-Kutta methods (reported on the left) which instead increase
in surface when the order increases, that is ARK(k) ⊂ARK(k+1), k ≥1.
Remark 8.4 (How to draw absolute stability regions) The
boundary
∂A of the absolute stability region A of a multistep method can be regarded
as the set of the complex numbers hλ such that
hλ =

rp+1 −
p

j=0
ajrp−j

⧸

p

j=−1
bjrp−j

,
(8.63)
where r is a complex number of modulus equal to one. An approximation of ∂A
can be obtained by evaluating the right hand side of (8.63) for diﬀerent values
of r on the unit circle (for instance, by setting r = exp(i*pi*(0:2000)/1000),
where i is the imaginary unit). The graphs in Figures 8.13 and 8.14 have indeed
been obtained in this way.
■

304
8 Ordinary diﬀerential equations
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
Re(h λ)
Im(h λ)
RK1
RK2
RK3
RK4
-6
-4
-2
0
2
4
6
8
10
-9
-6
-3
0
3
6
9
Re(h λ)
Im(hλ)
BDF2
BDF3
BDF4
Figure 8.14. The absolute stability regions of several explicit RK (left) and
BDF methods (right). In the latter case the stability regions are unbounded
and they spread outside the closed curves
According to the ﬁrst Dahlquist barrier the maximum order q of a
p + 1-step method satisfying the root condition is q = p + 1 for explicit
methods and, for implicit methods q = p + 2 if p + 1 is odd, q = p + 3 if
p + 1 is even.
Remark 8.5 (Cyclic composite methods) Dahlquist
barriers
can
be
overcome by appropriately combining several multistep methods. For instance,
the two following methods
un+1 = −8
11un + 19
11un−1 + h
33(30fn+1 + 57fn + 24fn−1 −fn−2),
un+1 = 449
240un + 19
30un−1 −361
240un−2
+ h
720(251fn+1 + 456fn −1347fn−1 −350fn−2),
have order ﬁve, but are both unstable. However, combined in such a way that
the former is used for n even, the latter for n odd, they give rise to an A-stable
3-step method of order ﬁve.
■
Multistep methods are implemented in several MATLAB programs,
for instance in ode15s.
ode15s
Octave 8.1 ode23 and ode45 are also available in Octave-forge. The
optional arguments however diﬀer from MATLAB. Note that ode45 in
Octave-forge oﬀers two possible strategies: the default one based on the
Dormand and Prince method generally produces more accurate results
than the other option that is based on the Fehlberg method. The built-
in ODE and DAE (Diﬀerential Algebraic Equations) solvers in Octave
(lsode, daspk, dassl, not available in MATLAB) also use multistep

8.8 The predictor-corrector methods
305
methods, in particular lsode can use either Adams or BDF formulas
while dassl and daspk use BDF formulas.
■
8.8 The predictor-corrector methods
In Section 8.3 it was pointed out that if the function f of Cauchy problem
is nonlinear, implicit methods yield at each step a nonlinear problem for
the unknown value un+1. For its solution we can use one of the methods
introduced in Chapter 2, or else apply the function fsolve as we have
done with the Programs 8.2 and 8.3.
Alternatively, we can carry out ﬁxed point iterations at every time
level. For example, for the Crank-Nicolson method (8.17), for k =
0, 1, . . ., we compute until convergence
u(k+1)
n+1
= un + h
2
6
fn + f(tn+1, u(k)
n+1)
7
.
It can be proved that if the initial guess u(0)
n+1 is chosen conveniently,
a single iteration suﬃces in order to obtain a numerical solution u(1)
n+1
whose accuracy is of the same order as the solution un+1 of the original
implicit method. More precisely, if the original implicit method has order
p ≥2, then the initial guess u(0)
n+1 must be generated by an explicit
method of order (at least) p −1.
For instance, if we use the ﬁrst order (explicit) forward Euler method
to initialize the Crank-Nicolson method, we get the Heun method (also
called improved Euler method), already referred as RK2:
u∗
n+1 = un + hfn,
un+1 = un + h
2

fn + f(tn+1, u∗
n+1)

(8.64)
The explicit step is called a predictor, whereas the implicit one is called a
corrector. Another example combines the (AB3) method (8.59) as predic-
tor with the (AM4) method (8.60) as corrector. These kinds of methods
are therefore called predictor-corrector methods. They enjoy the order
of accuracy of the corrector method. However, being explicit, they un-
dergo a stability restriction which is typically the same as that of the
predictor method (see, for instance, the regions of absolute stability of
Figure 8.15). Thus they are not adequate to integrate a Cauchy problem
on unbounded intervals.
In Program 8.4 we implement a general predictor-corrector method.
The function handles predictor and corrector identify the type of
method that is chosen. For instance, if we use the functions feonestep
and cnonestep, which are deﬁned in Programs 8.5 and 8.7, respectively,
we can call predcor as follows

306
8 Ordinary diﬀerential equations
-4
-3
-2
-1
0
-2
-1
0
1
2
Re(h λ)
Im(hλ)
EE
PC
-4
-3
-2
-1
0
-2
-1
0
1
2
Re(h λ)
Im(hλ)
AB3
AM4
PC
Figure 8.15. The absolute stability regions of the predictor-corrector (PC)
methods obtained by combining the explicit Euler (EE) and Crank-Nicolson
methods (left) and AB3 and AM4 (right). Notice the reduced surface of the
region when compared to the corresponding implicit methods (in the ﬁrst case
the region of the Crank-Nicolson method hasn’t been reported as it coincides
with all the complex half-plane Re(hλ) < 0)
[t,u]= predcor(f,[t0 ,T],y0 ,N,@feonestep ,@cnonestep );
and obtain the Heun method.
Program 8.4. predcor: predictor-corrector method
function [t,u]= predcor (odefun ,tspan ,y0 ,Nh ,...
predictor ,corrector ,varargin )
%PREDCOR
Solves
differential
equations
using a
%
predictor - corrector
method
%
[T,Y]= PREDCOR (ODEFUN ,TSPAN ,Y0 ,NH ,PRED ,CORR ) with
%
TSPAN=[T0 TF] integrates
the
system of differential
%
equations y’ = f(t,y) from
time T0 to TF with
%
initial
condition
Y0 using a general
predictor
%
corrector
method on an
equispaced
grid
of NH
steps.
%
Function
ODEFUN(T,Y) must
return a vector , whose
%
elements
hold
the
evaluation
of f(t,y), of the
%
same
dimension
of Y.
%
Each
row in the
solution
array Y corresponds
to a
%
time
returned
in the
column
vector T.
%
[T,Y]= PREDCOR (ODEFUN ,TSPAN ,Y0 ,NH ,PRED ,CORR ,P1 ,..)
%
passes the
additional
parameters
P1 ,... to the
%
functions
ODEFUN ,PRED
and CORR as
ODEFUN(T,Y,P1 ,..),
%
PRED (T,Y,P1 ,P2...), CORR (T,Y,P1 ,P2 ...).
h=( tspan(2)- tspan (1))/Nh;
y=y0 (:); w=y; u=y.’;
tt=linspace (tspan(1), tspan(2), Nh +1);
for t=tt (1:end -1)
fn = odefun(t,w,varargin {:});
upre = predictor (t,w,h,fn);
w = corrector (t+h,w,upre ,h,odefun ,...
fn ,varargin {:});

8.9 Systems of diﬀerential equations
307
u = [u; w.’];
end
t = tt ’;
end
Program 8.5. feonestep: one step of the forward Euler method
function [u]= feonestep (t,y,h,f)
% FEONESTEP
one
step of the
forward
Euler method
u = y + h*f;
return
Program 8.6. beonestep: one step of the backward Euler method
function [u]= beonestep (t,u,y,h,f,fn ,varargin )
% BEONESTEP
one
step of the
backward
Euler method
u = u + h*f(t,y,varargin {:});
return
Program 8.7. cnonestep: one step of the Crank-Nicolson method
function [u]= cnonestep (t,u,y,h,f,fn ,varargin )
% CNONESTEP
one
step of the Crank -Nicolson
method
u = u + 0.5* h*(f(t,y,varargin {:})+fn);
return
The MATLAB program ode113 implements a combined Adams-
ode113
Bashforth-Moulton scheme with variable steplength.
See the Exercises 8.14-8.17.
8.9 Systems of diﬀerential equations
Let us consider the following system of ﬁrst-order ordinary diﬀerential
equations whose unknowns are y1(t), . . . , ym(t):
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
y′
1 = f1(t, y1, . . . , ym),
...
y′
m = fm(t, y1, . . . , ym),
where t ∈(t0, T ], with the initial conditions
y1(t0) = y0,1, . . . , ym(t0) = y0,m.

308
8 Ordinary diﬀerential equations
For its solution we could apply to each individual equation one of the
methods previously introduced for a scalar problem. For instance, the
nth step of the forward Euler method would read
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
un+1,1 = un,1 + hf1(tn, un,1, . . . , un,m),
...
un+1,m = un,m + hfm(tn, un,1, . . . , un,m).
By writing the system in vector form y′(t) = F(t, y(t)), with obvious
choice of notation, the extension of the methods previously developed
for the case of a single equation to the vector case is straightforward.
For instance, the method
un+1 = un + h(ϑF(tn+1, un+1) + (1 −ϑ)F(tn, un)),
n ≥0,
with u0 = y0, 0 ≤ϑ ≤1, is the vector form of the forward Euler method
if ϑ = 0, the backward Euler method if ϑ = 1 and the Crank-Nicolson
method if ϑ = 1/2.
Example 8.7 (Population dynamics) Let us apply the forward Euler me-
thod to solve the Lotka-Volterra equations (8.3) with C1 = C2 = 1, b1 = b2 = 0
and d1 = d2 = 1. In order to use Program 8.1 for a system of ordinary
diﬀerential equations, let us create a function f which contains the component
of the vector function F, which we save in the ﬁle f.m. For our speciﬁc system
we have:
function
fn = f(t,y,C1 ,C2 ,d1 ,d2 ,b1 ,b2)
[n,m]= size (y); fn=zeros(n,m);
fn (1)= C1*y(1)*(1 - b1*y(1)- d2*y(2));
fn (2)=-C2*y(2)*(1 - b2*y(2)- d1*y(1));
return
Now we execute Program 8.1 with the following instructions
C1 =1; C2 =1; d1=1; d2 =1; b1 =0; b2 =0;
[t,u]= feuler(@f ,[0 ,10] ,[2
2] ,20000 ,C1 ,C2 ,d1 ,d2 ,b1 ,b2 );
They correspond to solving the Lotka-Volterra system on the time interval
[0, 10] with a time-step h = 5 · 10−4.
The graph in Figure 8.16, left, represents the time evolution of the two
components of the solution. Note that they are periodic. The graph in Figure
8.16, right, shows the trajectory issuing from the initial value in the so-called
phase plane, that is, the Cartesian plane whose coordinate axes are y1 and y2.
This trajectory is conﬁned within a bounded region of the (y1, y2) plane. If we
start from the point (1.2, 1.2), the trajectory would stay in an even smaller
region surrounding the point (1, 1). This can be explained as follows. Our
diﬀerential system admits 2 points of equilibrium at which y′
1 = 0 and y′
2 = 0,
and one of them is precisely (1, 1) (the other being (0, 0)). Actually, they are
obtained by solving the nonlinear system

8.9 Systems of diﬀerential equations
309
0
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
2
2.5
3
0
0.5
1
1.5
2
2.5
3
0
0.5
1
1.5
2
2.5
3
Figure 8.16. Numerical solutions of system (8.3). At left, we represent y1 and
y2 on the time interval (0, 10), the solid line refers to y1, the dashed line to
y2. Two diﬀerent initial data are considered: (2, 2) (thick lines) and (1.2, 1.2)
(thin lines). At right, we report the corresponding trajectories in the phase
plane
⎧
⎨
⎩
y′
1 = y1 −y1y2 = 0,
y′
2 = −y2 + y2y1 = 0.
If the initial data coincide with one of these points, the solution remains con-
stant in time. Moreover, while (0, 0) is an unstable equilibrium point, (1, 1) is
stable, that is, all trajectories issuing from a point near (1, 1) stay bounded in
the phase plane.
■
When we use an explicit method, the steplength h should undergo a
stability restriction similar to the one encountered in Section 8.6. When
the real part of the eigenvalues λk of the Jacobian A(t) = [∂F/∂y](t, y)
of F are all negative, we can set λ = −maxt ρ(A(t)), where ρ(A(t)) is the
spectral radius of A(t). This λ is a candidate to replace the one entering
in the stability conditions (such as, e.g., (8.30)) that were derived for the
scalar Cauchy problem.
Remark 8.6 The MATLAB programs (ode23, ode45, ...) that we have men-
tioned before can be used also for the solution of systems of ordinary diﬀerential
equations. The syntax is odeXX(@f,[t0 tf],y0), where y0 is the vector of the
initial conditions, f is a function to be speciﬁed by the user and odeXX is one
of the methods available in MATLAB.
■
Now consider the case of an ordinary diﬀerential equation of order m
y(m)(t) = f(t, y, y′, . . . , y(m−1))
(8.65)
for t ∈(t0, T ], whose solution (when existing) is a family of functions de-
ﬁned up to m arbitrary constants. The latter can be ﬁxed by prescribing
m initial conditions
y(t0) = y0, y′(t0) = y1, . . . , y(m−1)(t0) = ym−1.

310
8 Ordinary diﬀerential equations
Setting
w1(t) = y(t), w2(t) = y′(t), . . . , wm(t) = y(m−1)(t),
the equation (8.65) can be transformed into a ﬁrst-order system of m
diﬀerential equations
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
w′
1 = w2,
w′
2 = w3,
...
w′
m−1 = wm,
w′
m = f(t, w1, . . . , wm),
with initial conditions
w1(t0) = y0, w2(t0) = y1, . . . , wm(t0) = ym−1.
Thus we can always approximate the solution of a diﬀerential equation
of order m > 1 by resorting to the equivalent system of m ﬁrst-order
equations, and then applying to this system a convenient discretization
method.
Example 8.8 (Electrical circuits) Consider the circuit of Problem 8.4 and
suppose that L(i1) = L is constant and that R1 = R2 = R. In this case v can
be obtained by solving the following system of two diﬀerential equations:
⎧
⎪
⎨
⎪
⎩
v′(t) = w(t),
w′(t) = −1
LC
 L
R + RC

w(t) −
2
LC v(t) +
e
LC ,
(8.66)
with initial conditions v(0) = 0, w(0) = 0. The system has been obtained from
the second-order diﬀerential equation
LC d2v
dt2 +
 L
R2 + R1C
 dv
dt +
R1
R2 + 1

v = e.
(8.67)
We set L = 0.1 Henry, C = 10−3 Farad, R = 10 Ohm and e = 5 Volt, where
Henry, Farad, Ohm and Volt are respectively the unit measure of inductance,
capacitance, resistance and voltage. Now we apply the forward Euler method
with h = 0.001 seconds in the time interval [0, 0.1], by the Program 8.1:
L=0.1; C=1.e -03; R=10; e=5;
[t,u]= feuler(@fsys ,[0 ,0.1] ,[0
0],100, L,C,R,e);
where fsys is contained in the ﬁle fsys.m:
function
fn=fsys (t,y,L,C,R,e)
LC = L*C;
[n,m]= size (y); fn=zeros(n,m);
fn (1)= y(2);
fn (2)= -(L/R+R*C)/( LC)*y(2) -2/( LC)*y(1)+ e/(LC);
return

8.9 Systems of diﬀerential equations
311
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0
0.5
1
1.5
2
2.5
3
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
−50
0
50
100
150
200
250
Figure 8.17. Numerical solutions of system (8.66). The potential drop v(t) is
reported on the left, its derivative w(t) on the right: the dashed line represents
the solution obtained for h = 0.001 with the forward Euler method, the solid
line is for the one generated via the same method with h = 0.004, and the
solid line with circles is for the one produced via the Newmark method (8.71)
(with ζ = 1/4 and θ = 1/2) with h = 0.004
In Figure 8.17 we report the approximated values of v(t) and w(t). As ex-
pected, v(t) tends to e/2 = 2.5 Volt for t →∞. In this case, the matrix
A = [∂F/∂y](t, y) = [0, 1; −20000, −200], hence does not depend on time. Its
eigenvalues are λ1,2 = −100 ± 100i, so that the bound on time-step which
guarantees absolute stability is h < −2Re(λi)/|λi|2 = 0.01.
■
Sometimes numerical approximations can be directly derived on the
high order equation without passing through the equivalent ﬁrst order
system. Consider for instance the case of the 2nd order Cauchy problem

y′′(t) = f(t, y(t), y′(t))
t ∈(t0, T ],
y(t0) = α0,
y′(t0) = β0.
(8.68)
Two sequences un and vn will approximate y(tn) and y′(tn), respectively.
A simple numerical scheme can be constructed as follows: ﬁnd un+1 such
that
un+1 −2un + un−1
h2
= f(tn, un, vn),
n = 1, . . . , Nh,
(8.69)
with u0 = α0 and v0 = β0. Moreover, since (yn+1 −2yn + yn−1)/h2 is
a second order approximation of y′′(tn), let us consider a second order
approximation for y′(tn) too, i.e. (see (4.9))
vn = un+1 −un−1
2h
, with v0 = β0.
(8.70)
The leap-frog method (8.69)-(8.70) is accurate of order 2 with respect to
h.

312
8 Ordinary diﬀerential equations
A more general method is the Newmark method, in which we build
two sequences with same meaning as before
un+1 = un + hvn + h2
ζf(tn+1, un+1, vn+1)
+(1/2 −ζ)f(tn, un, vn)

,
vn+1 = vn + h [(1 −θ)f(tn, un, vn) + θf(tn+1, un+1, vn+1)] ,
(8.71)
with u0 = α0 and v0 = β0, and ζ and θ are two non-negative real
numbers. This method is implicit unless ζ = θ = 0, second order if
θ = 1/2, whereas it is ﬁrst order accurate if θ ̸= 1/2. The condition
θ ≥1/2 is necessary to ensure stability. For θ = 1/2 and ζ = 1/4 we
ﬁnd a rather popular method that is unconditionally stable. However,
this method is not suitable for simulations on long time intervals as
it introduces oscillatory spurious solutions. For these simulations it is
preferable to use θ > 1/2 and ζ > (θ + 1/2)2/4 even though the method
degenerates to a ﬁrst order one.
In Program 8.8 we implement the Newmark method. The vector
param allows to specify the values of the coeﬃcients (param(1)=ζ,
param(2)=θ).
Program 8.8. newmark: Newmark method
function [t,u]= newmark (odefun ,tspan ,y0 ,Nh ,param ,...
varargin )
%NEWMARK
Solves second
order differential
equations
%
using the Newmark
method
%
[T,Y]= NEWMARK (ODEFUN ,TSPAN ,Y0 ,NH ,PARAM) with
TSPAN =
%
[T0 TF] integrates
the system of differential
%
equations y’’=f(t,y,y’)
from
time T0 to TF with
%
initial
conditions
Y0=(y(t0),y’(t0)) using the
%
Newmark
method on an equispaced
grid
of NH steps.
%
PARAM holds parameters
zeta
and
theta
%
Function
ODEFUN(T,Y) must
return a vector , whose
%
elements
hold
the
evaluation
of f(t,y), of the
%
same
dimension
of Y.
%
Each
row in the
solution
array Y corresponds
to a
%
time
returned
in the
column
vector T.
tt=linspace (tspan(1), tspan(2), Nh +1);
y=y0 (:); u=y.’;
global
glob_h glob_t
glob_y
glob_odefun ;
global
glob_zeta
glob_theta
glob_varargin
glob_fn ;
glob_h =( tspan(2)- tspan (1))/Nh;
glob_y=y; glob_odefun =odefun;
glob_zeta
= param (1);
glob_theta
= param (2);
glob_varargin =varargin ;
if ( exist(’OCTAVE_VERSION ’) )
o_ver=OCTAVE_VERSION ;
version =str2num ([ o_ver(1), o_ver(3), o_ver (5)]);
end
if ( ~exist( ’OCTAVE_VERSION ’ )
| version
>= 320 )
options =optimset ;

8.10 Some examples
313
options .Display =’off’;
options .TolFun =1.e -12;
options .MaxFunEvals =10000;
end
glob_fn =odefun(tt(1), glob_y , varargin {:});
for
glob_t=tt(2: end)
if ( exist( ’OCTAVE_VERSION ’ ) & version < 320 )
w = fsolve(’newmarkfun ’, glob_y );
else
w = fsolve(@(w) newmarkfun (w),glob_y ,options );
end
glob_fn =odefun(glob_t ,w,varargin {:});
u = [u; w.’]; glob_y = w;
end
t=tt ’;
clear glob_h glob_t
glob_y
glob_odefun ;
clear glob_zeta
glob_theta
glob_varargin
glob_fn ;
end
function z=newmarkfun (w)
global
glob_h glob_t
glob_y
glob_odefun ;
global
glob_zeta
glob_theta
glob_varargin
glob_fn;
fn1=glob_odefun (glob_t ,w, glob_varargin {:});
z(1)= w(1) - glob_y (1) -glob_h*glob_y (2) -...
glob_h ^2*( glob_zeta *fn1 +(0.5 - glob_zeta )* glob_fn );
z(2)= w(2) - glob_y (2)
-...
glob_h *((1- glob_theta )* glob_fn +glob_theta *fn1);
end
Example 8.9 (Electrical circuits) We consider again the circuit of Prob-
lem 8.4 and we solve the second order equation (8.67) with the Newmark
scheme. In Figure 8.17 we compare the numerical approximations of the func-
tion v computed using the forward Euler scheme (dashed line for h = 0.001
and continuous line for h = 0.004) and the Newmark scheme with θ = 1/2
and ζ = 1/4 (solid line with circles), with the time-step h = 0.004. The better
accuracy of the latter solution is due to the fact that the method (8.71) is
second order accurate with respect to h.
■
See the Exercises 8.18-8.20.
8.10 Some examples
We end this chapter by considering and solving three non-trivial exam-
ples of systems of ordinary diﬀerential equations.
8.10.1 The spherical pendulum
The motion of a point x(t) = (x1(t), x2(t), x3(t))T with mass m sub-
ject to the gravity force F = (0, 0, −gm)T (with g = 9.8 m/s2)
and constrained to move on the spherical surface of equation Φ(x) =

314
8 Ordinary diﬀerential equations
x2
1 + x2
2 + x2
3 −1 = 0 is described by the following system of ordinary
diﬀerential equations
..x = 1
m
+
F −m
.x
T H
.x +∇ΦT F
|∇Φ|2
∇Φ
,
for t > 0.
(8.72)
We denote by
.x the ﬁrst derivative with respect to t, with
..x the second
derivative, with ∇Φ the spatial gradient of Φ, equal to 2x, with H the
Hessian matrix of Φ whose components are Hij = ∂2Φ/∂xi∂xj for i, j =
1, 2, 3. In our case H is a diagonal matrix with coeﬃcients all equal to
2. System (8.72) must be provided with the initial conditions x(0) = x0
and
.x (0) = v0.
To numerically solve (8.72) let us transform it into a system of dif-
ferential equations of order 1 in the new variable y, a vector with 6
components. Having set yi = xi and yi+3 =
.xi with i = 1, 2, 3, and
λ = m(y4, y5, y6)T H(y4, y5, y6) + ∇ΦT F
|∇Φ|2
,
we obtain, for i = 1, 2, 3,
.yi= y3+i,
.y3+i= 1
m

Fi −λ ∂Φ
∂yi

.
(8.73)
We apply the Euler and Crank-Nicolson methods. Initially it is
necessary to deﬁne a MATLAB function (fvinc in Program 8.9)
which yields the expressions of the right-hand terms (8.73). Further-
more, let us suppose that the initial conditions are given by vector
y0=[0,1,0,.8,0,1.2]and that the integration interval is tspan=[0,25].
We recall the explicit Euler method in the following way
[t,y]= feuler(@fvinc ,tspan ,y0 ,nt);
(the backward Euler beuler and Crank-Nicolson cranknic methods can
be called in the same way), where nt is the number of intervals (of
constant width) used to discretize the interval [tspan(1),tspan(2)]. In
the graphs in Figure 8.18 we report the trajectories obtained with 10000
and 100000 discretization nodes. Only in the second case, the solution
looks reasonably accurate. As a matter of fact, although we do not know
the exact solution to the problem, we can have an idea of the accuracy
by noticing that the solution satisﬁes r(y) ≡|y2
1 +y2
2 +y2
3 −1| = 0 and by
consequently measuring the maximal value of the residual r(yn) when
n varies, yn being the approximation of the exact solution generated at
time tn. By using 10000 discretization nodes we ﬁnd r = 1.0578, while
with 100000 nodes we have r = 0.1111, in accordance with the theory
requiring the explicit Euler method to converge with order 1.

8.10 Some examples
315
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
y1
y2
y3
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
y1
y2
y3
Figure 8.18. The trajectories obtained with the explicit Euler method with
h = 0.0025 (on the left) and h = 0.00025 (on the right). The blackened point
shows the initial datum
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
y1
y2
y3
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
y1
y2
y3
Figure 8.19. The trajectories obtained using the implicit Euler method with
h = 0.00125 (on the left) and using the Crank-Nicolson method with h = 0.025
(on the right)
By using the implicit Euler method with 20000 steps we obtain the
solution reported in Figure 8.19, while the Crank-Nicolson method (of
order 2) with only 1000 steps provides the solution reported in the same
ﬁgure on the right, which is undoubtedly more accurate. Indeed, we ﬁnd
r = 0.5816 for the implicit Euler method and r = 0.0928 for the Crank-
Nicolson method.
As a comparison, let us solve the same problem using the explicit
adaptive methods of type Runge-Kutta ode23 and ode45, featured in
MATLAB. These (unless diﬀerently speciﬁed) modify the integration
step in order to guarantee that the relative error on the solution is less
than 10−3 and the absolute error is less than 10−6. We run them using
the following commands
[t1 ,y1]= ode23(@fvinc ,tspan ,y0);
[t2 ,y2]= ode45(@fvinc ,tspan ,y0);
obtaining the solutions in Figure 8.20.

316
8 Ordinary diﬀerential equations
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
y1
y2
y3
−2
−1
0
1
2
−2
−1
0
1
2
−2.5
−2
−1.5
−1
−0.5
0
0.5
y1
y2
y3
Figure 8.20. The trajectories obtained using methods ode23 (left) and ode45
(right) with the same accuracy criteria. In the second case the error control
fails and the solution obtained is less accurate
The two methods used 783, respectively 537, non-uniformly dis-
tributed discretization nodes. The residual r is equal to 0.0238 for ode23
and 3.2563 for ode45. Surprisingly, the result obtained with the highest-
order method is thus less accurate and this warns us as to using the ode
programs available in MATLAB. An explanation of this behavior is in
the fact that the error estimator implemented in ode45 is less constrain-
ing than that in ode23. By slightly decreasing the relative tolerance (it
is suﬃcient to set options=odeset(’RelTol’,1.e-04)) and renaming
the program to [t,y]=ode45(@fvinc,tspan,y0,options); we can in
fact ﬁnd results comparable with those of ode23. Precisely ode23 re-
quires 1751 discretization nodes and it provides a residual r = 0.003,
while ode45 requires 1089 discretization nodes and it provides a residual
r = 0.060.
Program 8.9. fvinc: forcing term for the spherical pendulum problem
function [f]= fvinc(t,y)
[n,m]= size (y); f=zeros(n,m);
phix =2*y(1);
phiy =2*y(2);
phiz =2*y(3);
H=2* eye (3);
mass =1;
F1 =0; F2 =0; F3=-mass *9.8;
xp=zeros (3 ,1);
xp (1:3)=y(4:6);
F=[F1;F2;F3 ];
G=[ phix ;phiy ;phiz ];
lambda =( mass *xp ’*H*xp+F’*G)/(G’*G);
f(1:3)=y(4:6);
for k=1:3;
f(k+3)=(F(k)-lambda*G(k))/ mass ;
end
return
Octave 8.2 ode23 requires 924 steps while ode45 requires 575 steps for
the same accuracy tol=1.e-03.

8.10 Some examples
317
-1
-0.8 -0.6 -0.4 -0.2
 0
 0.2  0.4  0.6  0.8
 1-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.2
y1(:,3)
y1(:,1)
y1(:,2)
y1(:,3)
-1
-0.8 -0.6 -0.4 -0.2
 0
 0.2  0.4  0.6  0.8
 1-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1 1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.2
y2(:,3)
y2(:,1)
y2(:,2)
y2(:,3)
Figure 8.21. The trajectories obtained using methods ode23 (left) and ode45
(right) with the same accuracy criteria.
Note that ode45 gives results similar to ode23 as opposed to ode45
in MATLAB, see Figure 8.21.
■
8.10.2 The three-body problem
We want to compute the evolution of a system composed by three bodies,
knowing their initial positions and velocities and their masses under the
inﬂuence of their reciprocal gravitational attraction. The problem can
be formulated by using Newton’s laws of motion. However, as opposed
to the case of two bodies, there are no known closed form solutions.
We suppose that one of the three bodies has considerably larger mass
than the two remaining, and in particular we study the case of the Sun-
Earth-Mars system, a problem studied by celeber mathematicians such
as Lagrange in the eighteenth century, Poincar´e towards the end of the
nineteenth century and Levi-Civita in the twentieth century.
We denote by Ms the mass of the Sun, by Me that of the Earth and
by Mm that of Mars. The Sun’s mass being about 330000 times that of
the Earth and the mass of Mars being about one tenth of the Earth’s, we
can imagine that the center of gravity of the three bodies approximately
coincides with the center of the Sun (which will therefore remain still in
this model) and that the three objects remain in the plane described by
their initial positions. In such case the total force exerted on the Earth
will be for instance
Fe = Fes + Fem = Me
d2xe
dt2 ,
(8.74)
where xe = (xe, ye)T denotes the Earth’s position with respect to the
Sun, while Fes and Fem denote the force exerted by the Sun and by Mars,
respectively, on the Earth. By applying the universal gravitational law,
denoting by G the universal gravity constant and by xm the position of
Mars with respect to the Sun, equation (8.74) becomes

318
8 Ordinary diﬀerential equations
Me
d2xe
dt2 = −GMeMs
xe
|xe|3 + GMeMm
xm −xe
|xm −xe|3 .
Now, let us take the astronomical unit (1AU) as unit length, the year
(1yr) as temporal unit and deﬁne the Sun mass as Ms = 4π2(1AU)3
G(1yr)2 . By
adimensionalizing the previous equations and denoting again with xe,
xm, xs and t the adimensionalized variables, we obtain the following
equation
d2xe
dt2 = 4π2
Mm
Ms
xm −xe
|xm −xe|3 −
xe
|xe|3

.
(8.75)
A similar equation for planet Mars can be obtained using a similar com-
putation
d2xm
dt2
= 4π2
Me
Ms
xe −xm
|xe −xm|3 −
xm
|xm|3

.
(8.76)
The second-order system (8.75)-(8.76) immediately reduces to a system
of eight equations of order one. Program 8.10 allows to evaluate a func-
tion containing the right-hand side terms of system (8.75)-(8.76).
Program 8.10. threebody: forcing term for the simpliﬁed three body system
function f=threebody (t,y)
[n,m]= size (y); f=zeros(n,m); Ms =330000;
Me=1; Mm =0.1;
D1 = ((y(5)-y(1))^2+( y(7)-y (3))^2)^(3/2);
D2 = (y(1)^2+y (3)^2)^(3/2);
f(1)= y(2); f(2)=4* pi ^2*( Me/Ms*(y(5)-y(1))/D1 -y(1)/ D2 );
f(3)= y(4); f(4)=4* pi ^2*( Me/Ms*(y(7)-y(3))/D1 -y(3)/ D2 );
D2 = (y(5)^2+y (7)^2)^(3/2);
f(5)= y(6); f(6)=4* pi ^2*( Mm/Ms*(y(1)-y(5))/D1 -y(5)/ D2 );
f(7)= y(8); f(8)=4* pi ^2*( Mm/Ms*(y(3)-y(7))/D1 -y(7)/ D2 );
return
Let us compare the implicit Crank-Nicolson method and the explicit
adaptive Runge-Kutta method implemented in ode23. Having set the
Earth to be 1 unit away from the Sun, Mars will be located at about
1.52 units: the initial position will therefore be (1, 0) for the Earth and
(1.52, 0) for Mars. Let us further suppose that the two planets initially
have null horizontal velocity and vertical velocity equal to −5.1 units
(Earth) and −4.6 units (Mars): this way they should move along reason-
ably stable orbits around the Sun. For the Crank-Nicolson method we
choose 2000 discretization steps:
[t23 ,u23]= ode23(@threebody ,[0 10] ,...
[1.52 0 0
-4.6 1 0 0
-5.1]);
[tcn ,ucn]= cranknic (@threebody ,[0 10] ,...
[1.52 0 0
-4.6 1 0 0
-5.1] ,2000);
The graphs in Figure 8.22 show that the two methods are both able to
reproduce the elliptical orbits of the two planets around the Sun. Method

8.10 Some examples
319
−1
−0.5
0
0.5
1
1.5
−1
−0.5
0
0.5
1
S
−1
−0.5
0
0.5
1
1.5
−1
−0.5
0
0.5
1
S
Figure 8.22. The Earth’s (inmost) and Mars’s orbit with respect to the Sun
as computed with the adaptive method ode23 (on the left) (with 543 steps)
and with the Crank-Nicolson method (on the right) (with 2000 steps)
ode23 only required 543 (nonuniform) steps to generate a more accurate
solution than that generated by an implicit method with the same order
of accuracy, but which does not use step adaptivity.
Octave 8.3 ode23 requires 847 steps to generate a solution with a tol-
erance of 1e-3.
■
8.10.3 Some stiﬀproblems
Let us consider the following diﬀerential problem, proposed by [Gea71],
as a variant of the model problem (8.28):
y′(t) = λ(y(t) −g(t)) + g′(t),
t > 0,
y(0) = y0,
(8.77)
where g is a regular function and λ < 0 has a very large absolute value,
whose solution
y(t) = (y0 −g(0))eλt + g(t),
t ≥0.
(8.78)
is the sum of two components, also called transient and persistent so-
lution, respectively. Initially, on a time interval of length O(1/λ), the
transient component prevails, whereas the persistent component becomes
predominant in the asymptotic regime (for suﬃciently large t).
In particular, we set g(t) = t, λ = −100, and y0 = 1 and solve
problem (8.77) over the interval (0, 100) using the explicit Euler method:
since in this case f(t, y) = λ(y(t) −g(t)) + g′(t) we have ∂f/∂y = λ, and
the stability analysis performed in Section 8.5 suggests that we choose
h < 2/100. This restriction is dictated by the presence of the component
behaving like e−100t and appears completely unjustiﬁed when we think

320
8 Ordinary diﬀerential equations
0
2
4
6
8
10
-6000
-4000
-2000
0
2000
4000
6000
0
2
4
6
8
10
0
1
2
3
4
5
6
7
8
9
10
Figure 8.23. Solutions obtained using method (8.59) for problem (8.77) vio-
lating the stability condition (h = 0.0055, left) and respecting it (h = 0.0054,
right)
of its weight with respect to the whole solution for suﬃciently large t
(to get an idea, for t = 1 we have e−100 ≈10−44). The situation gets
worse using a higher order explicit method, such as for instance the
Adams-Bashforth (8.59) method of order 3: the absolute stability region
reduces (see Figure 8.13) and, consequently, the restriction on h becomes
even stricter, h < 0.00545. Violating – even slightly – such restriction
produces unacceptable solutions (as shown in Figure 8.23 on the left).
We thus face an apparently simple problem, but one that becomes
diﬃcult to solve with an explicit method (and more generally with a
method which is not A-stable).
In fact, even though for large values of t it is the persistent component
of the solution that prevails (in the current case it is a straight line), yet
for its correct approximation we must enforce a strong limitation on h.
Such kind of problem is called stiﬀ, or, more precisely, it is a stiﬀproblem
on the interval on which the persistent solution prevails. As a matter of
fact the choice of h is subjected to stability constraints; in these cases, the
use of explicit methods, even if implemented using adaptive strategies,
is prohibitive.
Programs implementing adaptive methods do not explicitely check
that absolute stability condition is satisﬁed. Nevertheless, the error es-
timator provides steplength h such that hλ belongs to the absolute sta-
bility region.
We consider now a system of linear diﬀerential equations that reads
y′(t) = Ay(t) + ϕ(t),
A ∈Rn×n,
ϕ(t) ∈Rn,
(8.79)
where A has n distinct eigenvalues λj, j = 1, . . . , n with Re(λj) < 0. Its
exact solution is
y(t) =
n

j=1
Cjeλjtvj + ψ(t),
(8.80)

8.10 Some examples
321
where C1, . . . , Cn are n costants and {vj} is a basis of Rn whose com-
ponents are the eigenvectors of A, while ψ(t) is a special solution of
(8.79).
Similarly to the scalar case (8.78), Cjeλjtvj represent the transient
components of the solution and ψ(t) the persistent component (for large
t). If |Re(λj)| is large, the corresponding transient component will tend
to zero very quickly, while for small values of |Re(λj)|, the corresponding
transient components will decay more slowly. If we approximate (8.79)
by a numerical scheme that is not absolutely stable, the transient compo-
nent featuring the largest value of |Re(λj)| is the one that yields the most
stringent constraint on the steplength h, even though such component
is the quickest to decay to zero.
A parameter that is often used to measure the stiﬀcharacter of a
system is
rs = maxj |Re(λj)|
minj |Re(λj)| ,
even though by itself rs is not fully meaningful. As a matter of fact,
the stiﬀcharacter of a system depends on rs, the eigenvalues of A, the
initial conditions, the persistent component of the solution and the time
interval on which the system has to be solved.
On the other hand, the stiﬀcharacter depends not only on the form
of the exact solution of (8.79); as a matter of fact there exist diﬀerent
systems, some of them stiﬀ, some other non-stiﬀ, all featuring the same
exact solution, see, e.g., [Lam91, Ch. 6].
How can we therefore state whether a system is stiﬀor not? Let us quote
the following deﬁnition proposed by [Lam91, pag. 220].
Deﬁnition 8.1 A system of ordinary diﬀerential equations is said
stiﬀif, once approximated by a numerical method featuring an abso-
lute stability region of bounded extension, “forces” the said numerical
method, for every initial condition for which the given problem ad-
mits a solution, to use a steplength exceedingly small with respect to
the one that would be necessary to reasonably reproduce the behavior
of the exact solution.
In the case of problem (8.77) (or (8.79)) the system is not stiﬀin
the initial interval where the solution varies quickly, whence the need
of adopting a small h to well capture the sharp layer. Rather, it is stiﬀ
in the next interval where the solution features a mild slope. Within
this interval the fastest transient, although exhausted because negligible
with respect to the other components, still dictates the choice of a tiny
steplength h because of stability constraints.

322
8 Ordinary diﬀerential equations
A-stable numerical methods (those whose absolute stability region
comprises the half complex plane Reλ < 0) with adaptive choice of the
steplength are the most eﬃcient for stiﬀproblems. Their implicit char-
acter makes them more computationally involved than explicit methods,
however they can aﬀord much larger steplengths. Explicit methods, on
their turn, may be unaﬀordable because of the strong limitation on h.
The algorithm implemented in function ode15s is based on multi-
step methods and backward diﬀerentiation formulas BDF introduced in
Section 8.7. Its formal convergence order is variable and at most 5. This
method is very eﬀective also on systems that are non-stiﬀfor which
the Jacobian matrix of f(t, y) is either constant or features very small
variations.
The function ode23s implements a linear implicit multistep method
ode23s
based on Rosenbrock methods see [SR97] for a detailed description of
these two functions.
Example 8.10 Let us consider the system y′(t) = Ay(t), t ∈(0, 100) with
initial condition y(0) = y0, where y = (y1, y2)T , y0 = (y1,0, y2,0)T and
A =
⎡
⎣
0
1
−λ1λ2
λ1 + λ2
⎤
⎦,
where λ1 and λ2 are two diﬀerent negative numbers such that |λ1| ≫|λ2|.
Matrix A has eigenvalues λ1 and λ2 and eigenvectors v1 = (1, λ1)T , v2 =
(1, λ2)T . Thanks to (8.80) the exact solution is
y(t) =
⎛
⎝
C1eλ1t + C2eλ2t
C1λ1eλ1t + C2λ2eλ2t
⎞
⎠
T
.
(8.81)
The constants C1 and C2 are obtained by fulﬁlling the initial condition:
C1 = λ2y1,0 −y2,0
λ2 −λ1
,
C2 = y2,0 −λ1y1,0
λ2 −λ1
.
Based on the remarks made earlier, the integration step of an explicit method
used for the resolution of such a system will depend uniquely on the eigenvalue
having largest modulus, λ1. Let us assess this experimentally using the explicit
Euler method and choosing λ1 = −100, λ2 = −1 (therefore rs = 100), y1,0 =
y2,0 = 1. In Figure 8.24 we report the solutions computed by violating (left)
or respecting (right) the stability condition h < 1/50.
■
The deﬁnition of stiﬀproblem can be extended, with some care, to the
nonlinear case (see for instance [QSS07, Chapter 11]). One of the most
studied nonlinear stiﬀproblems is given by the Van der Pol equation
d2x
dt2 = μ(1 −x2)dx
dt −x,
(8.82)

8.10 Some examples
323
0
1
2
3
4
5
6
-4
-3
-2
-1
0
1
2
3
4 x 10
9
y1
y2
t
0
1
2
3
4
5
6
-1.5
-1
-0.5
0
0.5
1
1.5
y1
y2
t
Figure 8.24. Solutions to the problem in Example 8.10 for h = 0.0207 (left)
and h = 0.01 (right). In the ﬁrst case the condition h < 2/|λ1| = 0.02 is vio-
lated and the method is unstable. The second case features a strong variation
of the fast transient component y2. Consider the totally diﬀerent scale in the
two graphs
proposed in 1920 and used in the study of circuits containing thermionic
valves, the so-called vacuum tubes, such as cathodic tubes in television
sets or magnetrons in microwave ovens.
If we set y = (x, z)T , with z = dx/dt, (8.82) is equivalent to the
following nonlinear ﬁrst order system
y′ = F(t, y) =
8
z
−x + μ(1 −x2)z
9
.
(8.83)
Such system becomes increasingly stiﬀwith the increase of the μ pa-
rameter. In the solution we ﬁnd in fact two components which denote
totally diﬀerent dynamics with the increase of μ. The one having the
fastest dynamics imposes a limitation on the integration step which gets
more and more prohibitive with the increase of μ.
If we solve (8.82) using ode23 and ode45, we realize that these are
too costly when μ is large. With μ = 100 and initial condition y =
(1, 1)T , ode23 requires 7835 steps and ode45 23473 steps to integrate
between t = 0 and t = 100. Reading the MATLAB help we discover
that these methods are based on explicit schemes and therefore they
are not recommended for stiﬀproblems: for these, other procedures are
suggested, such as for instance the implicit methods ode23s or ode15s.
ode23s
The diﬀerence in terms of number of steps is remarkable, as shown in
Table 8.1. Notice however that the number of steps for ode23s is smaller
than that for ode23 only for large enough values of μ (thus for very stiﬀ
problems).
Example 8.11 (Chemical kinetics) We want to investigate the temporal
behavior of chemical reactions of species in homogeneous media. Quite often,

324
8 Ordinary diﬀerential equations
0
5
10
15
20
25
30
35
40
-3
-2
-1
0
1
2
3
x
z
t
0
5
10
15
20
25
30
35
40
-15
-10
-5
0
5
10
15
x
z
t
Figure 8.25. Behavior of the components of the solutions y to system (8.83)
for μ = 1 (left) and μ = 10 (right)
Table 8.1. Behavior of the number of integration steps for various approxi-
mation methods with growing μ parameter
μ
ode23
ode45
ode23s
ode15s
0.1
471
509
614
586
1
775
1065
838
975
10
1220
2809
1005
1077
100
7835
23473
299
305
1000
112823
342265
183
220
both fast and slow species cohexist, that evolve according to diﬀerente char-
acteristic times. Below we consider a mathematical model that represents a
simpliﬁed version of this process. This model, named Davis-Skodje (see, e.g.,
[VGCN05]), addresses two species y1(t) and y2(t) that evolve according to the
equations
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
dy1
dt = 1
ε

−y1 +
y2
1 + y2

−
y2
(1 + y2)2 ,
t > 0
dy2
dt = −y2,
t > 0
y1(0) = y1,0
y2(0) = y2,0,
(8.84)
where ε > 0, y1,0 and y2,0 are given.
The exact solution is:
y1(t) =

y1,0 −
y2,0
1 + y2,0

e−t/ε +
y2,0e−t
1 + y2,0e−t
y2(t) = y2,0e−t.
The ratio 1/ε is a measure of the system’s stiﬀness: the larger 1/ε the wider
the gap between the temporal scales of the evolution of the two species, than
the more complex is the numerical computation.
To numerically solve system (8.84) with ε = 10−6 and initial condition
y0 = (1.5, 1)T , we have deﬁned the function

8.11 What we haven’t told you
325
Table 8.2. Number of steps used by a few MATLAB functions to solve prob-
lem (8.84) for diﬀerent values of the parameter ε
ε
... ode23 .. ode45 .. ode23s .. ode15s
10−2
409
1241
73
73
10−3
3991
12081
84
81
10−4
39808
120553
87
85
function [f]= funds(t,y)
epsilon =1.e-6; [n,m]= size (y);f=zeros(n,m);
f(1)= -1/ epsilon *y(1)+( (1/ epsilon -1)* y(2)+...
1/ epsilon *y(2)* y(2))/(1+ y(2))^2;
f(2)=-y(2);
end
Then we call the MATLAB function ode23s by the following commands
y0 =[1.5 ,1];
tspan =[0 ,10];
[t,y]= ode23s(@funds ,tspan ,y0);
In Table 8.2 we report the number of steps required by the explicit methods
ode23, ode45, and by the implicit methods ode24s, ode15s. We can appreci-
ate the better eﬃciency of methods ode23s and ode15s, as they have been
speciﬁcally designed for stiﬀequations.
In Figure 8.26, left, we plot numerical solutions: the species y1 evolves very
quickly at the beginning of the simulation during a time interval of length
O(ε), and very slowly after. On the contrary, the species y2 varies slowly and
uniformly during the whole simulation time.
In Figure 8.26, right, trajectories of problem (8.84) are shown for ε =
10−6 and with several initial conditions [y1,0, y2,0]T . Horizontal stretches of
trajectories are covered in a very short initial time interval of length O(ε),
while the curved ways are covered in the remaining time of length 10 −O(ε).
Analysis of trajectories can be useful to acquire carachteristic information of
the chemical process.
■
Octave 8.4 While ode15s and ode23s are not available in Octave,
many ODE solvers capable of dealing with stiﬀproblems are available in
the Octave core (lsode, dassl, daspk) and in the odepkg package from
Octave-Forge (ode2r, ode5r, odebda, oders, odesx).
■
8.11 What we haven’t told you
For a complete derivation of the whole family of the Runge-Kutta meth-
ods we refer to [But87], [Lam91] and [QSS07, Chapter 11].
For derivation and analysis of multistep methods we refer to [Arn73]
and [Lam91].

326
8 Ordinary diﬀerential equations
-0.5
0
0.5
1
1.5
2
2.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
t
0
0.5
1
1.5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
y1
y2
Figure 8.26. At left, numerical solutions (y1(t) (continuous line) and y2(t)
(dashed line)) of system (8.84) with initial conditions y1,0 = 1.5, y2,0 = 1.
At right, trajectories of (8.84) for several initial data y0 = (y1,0, y2,0)T : y0 =
(1.5, 1)T (continuous line), (1.5, 3)T (dashed line), (0, 2)T (dotted-dashed line),
(0, 4)T (dotted line). ε = 10−6 in all simulations
8.12 Exercises
Exercise 8.1 Apply the backward Euler and forward Euler methods for the
solution of the Cauchy problem
y′ = sin(t) + y, t ∈(0, 1], with y(0) = 0,
(8.85)
and verify that both converge with order 1.
Exercise 8.2 Consider the Cauchy problem
y′ = −te−y, t ∈(0, 1], with y(0) = 0.
(8.86)
Apply the forward Euler method with h = 1/100 and estimate the number of
exact signiﬁcant digits of the approximate solution at t = 1 (use the property
that the value of the exact solution is included between −1 and 0).
Exercise 8.3 The backward Euler method applied to problem (8.86) re-
quires at each step the solution of the nonlinear equation: un+1 = un −
htn+1e−un+1 = φ(un+1). The solution un+1 can be obtained by the follow-
ing ﬁxed-point iteration:
for k = 0, 1, . . . , compute u(k+1)
n+1
= φ(u(k)
n+1), with u(0)
n+1 = un.
Find under which restriction on h these iterations converge.
Exercise 8.4 Repeat Exercise 8.1 for the Crank-Nicolson method.
Exercise 8.5 Verify that the Crank-Nicolson method can be derived from the
following integral form of the Cauchy problem (8.5)
y(t) −y0 =
 t
t0
f(τ, y(τ))dτ
provided that the integral is approximated by the trapezoidal formula (4.19).

8.12 Exercises
327
Exercise 8.6 Solve the model problem (8.28) with λ = −1+i by the forward
Euler method and ﬁnd the values of h for which we have absolute stability.
Exercise 8.7 Show that the Heun method deﬁned in (8.64) is consistent.
Write a MATLAB program to implement it for the solution of the Cauchy
problem (8.85) and verify experimentally that the method has order of con-
vergence equal to 2 with respect to h.
Exercise 8.8 Prove that the Heun method (8.64) is absolutely stable if −2 <
hλ < 0 where λ is real and negative.
Exercise 8.9 Prove formula (8.34).
Exercise 8.10 Prove the inequality (8.39).
Exercise 8.11 Prove the inequality (8.40).
Exercise 8.12 Verify the consistency of the RK3 method (8.58). Write a
MATLAB program to implement it for the solution of the Cauchy problem
(8.85) and verify experimentally that the method has order of convergence
equal to 3 with respect to h. The methods (8.64) and (8.58) stand at the
base of the MATLAB program ode23 for the solution of ordinary diﬀerential
equations.
Exercise 8.13 Prove that the method (8.58) is absolutely stable if −2.5 <
hλ < 0 where λ is real and negative.
Exercise 8.14 The modiﬁed Euler method is deﬁned as follows:
u∗
n+1 = un + hf(tn, un), un+1 = un + hf(tn+1, u∗
n+1).
(8.87)
Find under which condition on h this method is absolutely stable.
Exercise 8.15 (Thermodynamics) Solve equation (8.1) by the Crank-
Nicolson method and the Heun method when the body in question is a cube
with side equal to 1 m and mass equal to 1 Kg. Assume that T0 = 180K,
Te = 200K, γ = 0.5 and C = 100J/(Kg/K). Compare the results obtained by
using h = 20 and h = 10, for t ranging from 0 to 200 seconds.
Exercise 8.16 Use MATLAB to compute the region of absolute stability of
the Heun method.
Exercise 8.17 Solve the Cauchy problem (8.16) by the Heun method and
verify its order.
Exercise 8.18 The displacement x(t) of a vibrating system represented by a
body of a given weight and a spring, subjected to a resistive force proportional
to the velocity, is described by the second-order diﬀerential equation x′′+5x′+
6x = 0. Solve it by the Heun method assuming that x(0) = 1 and x′(0) = 0,
for t ∈[0, 5].

328
8 Ordinary diﬀerential equations
Exercise 8.19 The motion of a frictionless Foucault pendulum is described
by the system of two equations
x′′ −2ω sin(Ψ)y′ + k2x = 0,
y′′ + 2ω cos(Ψ)x′ + k2y = 0,
where Ψ is the latitude of the place where the pendulum is located, ω =
7.29 · 10−5 sec−1 is the angular velocity of the Earth, k =

g/l with g = 9.8
m/sec2 and l is the length of the pendulum. Apply the forward Euler method
to compute x = x(t) and y = y(t) for t ranging between 0 and 300 seconds
and Ψ = π/4.
Exercise 8.20 (Baseball trajectory) Using ode23, solve Problem 8.3 by
assuming that the initial velocity of the ball be v(0) = v0(cos(φ), 0, sin(φ))T ,
with v0 = 38 m/s, φ = 1 degree and an angular velocity equal to 180·1.047198
radiants per second. If x(0) = 0, after how many seconds (approximately) will
the ball touch the ground (i.e., z = 0)?
Exercise 8.21 (Chemical kinetics) Given the real values y1,0, y2,0 e y3,0,
the following system
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
dy1
dt = 1
ε (−5y1 −y1y2 + 5y2
2 + y3) + y2y3 −y1,
t > 0
dy2
dt = 1
ε (10y1 −y1y2 −10y2
2 + y3) −y2y3 + y1,
t > 0
dy3
dt = 1
ε (y1y2 −y3) −y2y3 + y1,
t > 0
y1(0) = y1,0
y2(0) = y2,0, y3(0) = y3,0,
(8.88)
simulates the evolution of the concentration of three species in a chemical
reaction. By ﬁxing the initial datum y0 = (1, 0.5, 0)T and setting ε = 10−2,
solve system (8.88) with t ∈[0, 10], calling ode23 and ode23s, then comment
on the stiﬀness of the system. Finally, plot the computed solution in the phase
space, for diﬀerent values of the initial datum y0 = (y1,0, y2,0, y3,0)T with
0 ≤yi,0 ≤1 and i = 1, 3.

9
Numerical approximation of
boundary-value problems
Boundary-value problems are diﬀerential problems set in an interval
(a, b) of the real line or in an open multidimensional region Ω ⊂Rd
(d = 2, 3) for which the value of the unknown solution (or its deriva-
tives) is prescribed at the end-points a and b of the interval, or on the
boundary ∂Ω of the multidimensional region.
In the multidimensional case the diﬀerential equation will involve
partial derivatives of the exact solution with respect to the space co-
ordinates. Equations depending also on time (denoted with t), like the
heat equation and the wave equation, are called initial-boundary-value
problems. In that case initial conditions at t = 0 need to be prescribed
as well.
Some examples of boundary-value problems are reported below.
1. Poisson equation:
−u′′(x) = f(x), x ∈(a, b),
(9.1)
or (in several dimensions)
−Δu(x) = f(x), x = (x1, . . . , xd)T ∈Ω,
(9.2)
where f is a given function and Δ is the so-called Laplace operator:
Δu =
d

i=1
∂2u
∂x2
i
.
The symbol ∂· /∂xi denotes partial derivative with respect to the xi
variable, that is, for every point x0
∂u
∂xi
(x0) = lim
h→0
u(x0 + hei) −u(x0)
h
,
(9.3)
where ei is ith unitary vector of Rd.
A. Quarteroni et al., Scientiﬁc Computing with MATLAB and Octave,
Texts in Computational Science and Engineering 2,
DOI 10.1007/978-3-642-45367-0 9, © Springer-Verlag Berlin Heidelberg 2014
329

330
9 Numerical approximation of boundary-valueproblems
2. Heat equation:
∂u(x, t)
∂t
−μ∂2u(x, t)
∂x2
= f(x, t), x ∈(a, b), t > 0,
(9.4)
or (in several dimensions)
∂u(x, t)
∂t
−μΔu(x, t) = f(x, t), x ∈Ω, t > 0,
(9.5)
where μ > 0 is a given coeﬃcient representing the thermal diﬀusivity,
and f is again a given function.
3. Wave equation:
∂2u(x, t)
∂t2
−c∂2u(x, t)
∂x2
= 0, x ∈(a, b), t > 0,
or (in several dimensions)
∂2u(x, t)
∂t2
−cΔu(x, t) = 0, x ∈Ω, t > 0,
where c is a given positive constant.
For a more complete description of general partial diﬀerential equations,
the reader is referred for instance to [Eva98], [Sal08], and for their nu-
merical approximation to [Qua13], [QV94], [EEHJ96] or [Lan03].
9.1 Some representative problems
Problem 9.1 (Hydrogeology) The study of ﬁltration in groundwater
can lead, in some cases, to an equation like (9.2). Consider a portion Ω
occupied by a porous medium (like ground or clay). According to the
Darcy law, the water velocity ﬁltration q = (q1, q2, q3)T is equal to the
variation of the water level φ in the medium, precisely
q = −K∇φ,
(9.6)
where K is the constant hydraulic conductivity of the porous medium
and ∇φ denotes the spatial gradient of φ. Assume that the ﬂuid density
is constant; then the mass conservation principle yields the equation
divq = 0, where divq is the divergence of the vector q and is deﬁned as
divq =
3

i=1
∂qi
∂xi
.
Thanks to (9.6) we therefore ﬁnd that φ satisﬁes the Poisson problem
Δφ = 0 (see Exercise 9.8).
■

9.1 Some representative problems
331
Problem 9.2 (Thermodynamics) Let Ω ⊂Rd be a volume occupied
by a continuous medium. Denoting by J(x, t) and T (x, t) the heat ﬂux
and the temperature of the medium, respectively, the Fourier law states
that heat ﬂux is proportional to the variation of the temperature T , that
is
J(x, t) = −k∇T (x, t),
where k is a positive constant expressing the thermal conductivity coef-
ﬁcient. Imposing the conservation of energy, that is, the rate of change of
energy of a volume equals the rate at which heat ﬂows into it, we obtain
the heat equation
ρc∂T
∂t = kΔT,
(9.7)
where ρ is the mass density of the continuous medium and c is the speciﬁc
heat capacity (per unit mass). If, in addition, heat is produced at the
rate f(x, t) by some other means (e.g., electrical heating), (9.7) becomes
ρc∂T
∂t = kΔT + f.
(9.8)
The coeﬃcient μ = k/(ρc) is the so-called thermal diﬀusivity. For the
solution of this problem see Example 9.4.
■
Problem 9.3 (Communications) We consider a telegraph wire with
resistance R and self-inductance L per unit length. Assuming that the
current can drain away to ground through a capacitance C and a conduc-
tance G per unith length (see Figure 9.1), the equation for the voltage
v is
∂2v
∂t2 −c∂2v
∂x2 = −α∂v
∂t −βv,
(9.9)
where c = 1/(LC), α = R/L + G/C and β = RG/(LC). Equation (9.9)
is an example of a second order hyperbolic equation and it is known
as telegrapher’s equation (or just telegraph equation) (see [Str07]). The
solution of this problem is given in Example 9.8.
■
L dx
R dx
C dx
1/(G dx)
x
x + dx
Figure 9.1. An element of cable of length dx

332
9 Numerical approximation of boundary-valueproblems
9.2 Approximation of boundary-value problems
The diﬀerential equations presented so far feature an inﬁnite number of
solutions. With the aim of obtaining a unique solution we must impose
suitable conditions on the boundary ∂Ω of Ω and, for the time-dependent
equations, suitable initial conditions at time t = 0.
In this section we consider the Poisson equations (9.1) or (9.2). In
the one-dimensional case (9.1), to ﬁx the solution one possibility is to
prescribe the value of u at x = a and x = b, obtaining
−u′′(x) = f(x)
for x ∈(a, b),
u(a) = α,
u(b) = β
(9.10)
where α and β are two given real numbers. This is a Dirichlet boundary-
value problem, and is precisely the problem that we will face in the next
section.
Performing double integration it is easily seen that if f ∈C0([a, b]), the
solution u exists and is unique; moreover it belongs to C2([a, b]).
Although (9.10) is an ordinary diﬀerential problem, it cannot be cast
in the form of a Cauchy problem for ordinary diﬀerential equations since
the value of u is prescribed at two diﬀerent points.
Instead to set Dirichlet boundary conditions (9.10)2 we can impose
u′(a) = γ, u′(b) = δ (where γ and δ are suitable constants such that
γ −δ =  b
a f(x)dx). A problem with these boundary conditions is named
Neumann problem. Note that its solution is known up to an additive
constant.
In the two-dimensional case, the Dirichlet boundary-value problem
takes the following form: being given two functions f = f(x) and g =
g(x), ﬁnd a function u = u(x) such that
−Δu(x) = f(x)
for x ∈Ω,
u(x) = g(x)
for x ∈∂Ω
(9.11)
Alternatively to the boundary condition on (9.11), we can prescribe a
value for the partial derivative of u with respect to the normal direction
to the boundary ∂Ω, that is
∂u
∂n(x) = ∇u(x) · n(x) = h(x)
for x ∈∂Ω,
where h is a suitable function such that

∂Ω
h = −

Ω
f (see Figure 9.2),
in which case we will get a Neumann boundary-value problem.

9.2 Approximation of boundary-value problems
333
Ω
∂Ω
n(x)
Figure 9.2. A two-dimensional domain Ω and the unit outward normal versor
to ∂Ω
It can be proven that if f and g are two continuous functions and
the boundary ∂Ω of the region Ω is regular enough, then the Dirichlet
boundary-value problem (9.11) has a unique solution (while the solution
of the Neumann boundary-value problem is unique up to an additive
constant).
The numerical methods which are used for its solution are based on
the same principles used for the approximation of the one-dimensional
boundary-value problem. This is the reason why in Sections 9.2.1 and
9.2.3 we will make a digression on the numerical solution of problem
(9.10) with either ﬁnite diﬀerence and ﬁnite element methods, respec-
tively.
With this aim we introduce on [a, b] a partition into intervals Ij =
[xj, xj+1] for j = 0, . . . , N with x0 = a and xN+1 = b. We assume for
simplicity that all intervals have the same length h = (b −a)/(N + 1).
9.2.1 Finite diﬀerence approximation of the one-dimensional
Poisson problem
The diﬀerential equation (9.10) must be satisﬁed in particular at any
point xj (which we call nodes from now on) internal to (a, b), that is
−u′′(xj) = f(xj),
j = 1, . . . , N.
We can approximate this set of N equations by replacing the second
derivative with a suitable ﬁnite diﬀerence as we have done in Chapter 4
for the ﬁrst derivatives. In particular, we observe that if u : [a, b] →R
is a suﬃciently smooth function in a neighborhood of a generic point
¯x ∈(a, b), then the quantity
δ2u(¯x) = u(¯x + h) −2u(¯x) + u(¯x −h)
h2
(9.12)
provides an approximation to u′′(¯x) of order 2 with respect to h (see
Exercise 9.3). This suggests the use of the following approximation to
problem (9.10): ﬁnd {uj}N
j=1 such that

334
9 Numerical approximation of boundary-valueproblems
−uj+1 −2uj + uj−1
h2
= f(xj),
j = 1, . . . , N
(9.13)
with u0 = α and uN+1 = β. Obviously, uj will be an approximation of
u(xj). Equations (9.13) provide a linear system
Auh = h2f,
(9.14)
where uh = (u1, . . . , uN)T is the vector of unknowns, f = (f(x1) +
α/h2, f(x2), . . . , f(xN−1), f(xN) + β/h2)T , and A is the tridiagonal ma-
trix
A = tridiag(−1, 2, −1) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
2 −1 0 . . . 0
−1 2
...
...
0 ... ... −1 0
...
−1 2 −1
0 . . . 0 −1 2
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
(9.15)
This system admits a unique solution since A is symmetric and positive
deﬁnite (see Exercise 9.1). Moreover, it can be solved by the Thomas
algorithm introduced in Section 5.6. We note however that, for small
values of h (and thus for large values of N), A is ill-conditioned. Indeed,
K(A) = λmax(A)/λmin(A) = Ch−2, for a suitable constant C indepen-
dent of h (see Exercise 9.2). Consequently, the numerical solution of sys-
tem (9.14), by either direct or iterative methods, requires special care. In
particular, when using iterative methods a suitable preconditioner ought
to be employed.
It is possible to prove (see, e.g., [QSS07, Chapter 12]) that if f ∈
C2([a, b]) then
max
j=0,...,N+1|u(xj) −uj| ≤(b −a)2h2
96
max
x∈[a,b]|f ′′(x)|
(9.16)
that is, the ﬁnite diﬀerence method (9.13) converges with order two with
respect to h.
In Program 9.1 we solve the following boundary-value problem (the
so-called diﬀusion-convection-reaction problem)
−μu′′(x) + ηu′(x) + σu(x) = f(x)
for x ∈(a, b),
u(a) = α
u(b) = β,
(9.17)
μ > 0, η and σ > 0 constants, which is a generalization of problem
(9.10).

9.2 Approximation of boundary-value problems
335
For this problem the ﬁnite diﬀerence method, which generalizes (9.13),
reads:
⎧
⎨
⎩
−μuj+1 −2uj + uj−1
h2
+ η uj+1 −uj−1
2h
+ σuj = f(xj),
j = 1, . . . , N,
u0 = α,
uN+1 = β.
The input parameters of Program 9.1 are the end-points a and b of
the interval, the number N of internal nodes, the constant coeﬃcients
μ, η and σ and the function handle bvpfun associated with the function
f(x). Finally, ua and ub represent the values that the solution should
attain at x=a and x=b, respectively. Output parameters are the vector
of nodes xh and the computed solution uh. Notice that the solutions can
be aﬀected by spurious oscillations if h ≥2μ/η (see next Section).
Program 9.1. bvp: approximation of a two-point diﬀusion-convection-reaction
problem by the ﬁnite diﬀerence method
function [xh ,uh]= bvp(a,b,N,mu ,eta ,sigma ,bvpfun ,...
ua ,ub ,varargin )
%BVP
Solves two -point boundary
value problems .
%
[XH ,UH]= BVP(A,B,N,MU ,ETA ,SIGMA ,BVPFUN ,UA ,UB)
%
solves the
boundary -value problem
%
-MU*D(DU/DX)/DX+ETA*DU/DX+SIGMA*U=BVPFUN
%
on the interval (A,B) with
boundary
conditions
%
U(A)=UA and U(B)=UB , by the
centered
finite
%
difference
method at N equispaced
nodes
%
internal
to (A,B). BVPFUN is a function
handle.
%
[XH ,UH]= BVP(A,B,N,MU ,ETA ,SIGMA ,BVPFUN ,UA ,UB ,...
%
P1 ,P2 ,...) passes the additional
parameters
%
P1 , P2 , ... to the function
BVPFUN.
%
XH contains
the nodes of the
discretization ,
%
including
the
boundary
nodes.
%
UH contains
the
numerical
solutions .
h = (b-a)/(N+1);
xh = (linspace (a,b,N+2)) ’;
hm = mu/h^2;
hd = eta /(2* h);
e =ones (N,1);
A = spdiags ([-hm*e-hd (2* hm+sigma)*e -hm*e+hd],...
-1:1, N, N);
xi = xh (2:end -1);
f =bvpfun(xi ,varargin {:});
f(1) =
f(1)+ ua*(hm+hd);
f(end) = f(end )+ub*(hm -hd );
uh = A\f;
uh =[ua; uh; ub];
return

336
9 Numerical approximation of boundary-valueproblems
9.2.2 Finite diﬀerence approximation of a
convection-dominated problem
We consider now the following generalization of the boundary-value
problem (9.10)
−μu′′(x) + ηu′(x) = f(x)
for x ∈(a, b),
u(a) = α,
u(b) = β,
(9.18)
μ and η being positive constants. This is the so-called convection-
diﬀusion problem since the terms −μu′′(x) and ηu′(x) are responsible
of diﬀusion and convection of the unknown function u(x), respectively.
The global P´eclet number, associated to equation (9.18), is deﬁned as
Pegl = η(b −a)
2μ
,
(9.19)
and it provides a measure of how much the convective term prevails
over the diﬀusive one. A problem featuring Pegl ≫1 will be named
convection-dominated problem.
A possible discretization of (9.18) reads
⎧
⎪
⎨
⎪
⎩
−μuj+1 −2uj + uj−1
h2
+ η uj+1 −uj−1
2h
= f(xj), j = 1, . . . , N,
u0 = α,
uN+1 = β,
(9.20)
in which the centered ﬁnite diﬀerence scheme (4.9) has been used to
approximate the convection term. As for the Poisson equation, one can
prove that the error between the solution of the discrete problem (9.20)
and that of the continuous problem (9.18) satisﬁes the following estimate
max
j=0,...,N+1 |u(xj) −uj| ≤Ch2 max
x∈[a,b]|f ′′(x)|.
(9.21)
The constant C is proportional to Pegl, therefore it is very large when
the convection dominates the diﬀusion. Thus, if the discretization step
h is not small enough, the numerical solution computed by the scheme
(9.20) may be highly inaccurate and exhibit strong oscillations which are
far from satisfying the continuous problem. For a more detailed analysis
of this phenomenon we introduce the so-called local P´eclet number (also
named “grid” P´eclet number)
Pe = ηh
2μ.
(9.22)
One can prove that the solution of the discrete problem (9.20) does not
exhibit oscillations if Pe < 1 (see [Qua13, Chap. 5]). Thus, in order to

9.2 Approximation of boundary-value problems
337
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Figure 9.3. Exact solution (solid line), centered ﬁnite diﬀerence approxi-
mation with h = 1/15 (Pe > 1) (dotted line), centered ﬁnite diﬀerence ap-
proximation with h = 1/32 (Pe < 1) (dashed line), upwind ﬁnite diﬀerence
approximation with h = 1/15 (dashed-dotted line) of the solution of problem
(9.18) with a = 0, b = 1, α = 0, β = 1, f(x) = 0, μ = 1/50 and η = 1. For
clearness, numerical solutions have been plotted on the interval [0.6, 1] instead
of [0, 1]
ensure a good numerical solution, we have to choose a discretization step
h < 2μ/η. Unfortunately, such a choice is not convenient when the ratio
2μ/η is very small.
A possible alternative consists in choosing a diﬀerent approximation
of the convective term u′; precisely, instead to use the centered ﬁnite
diﬀerence (4.9), we can employ the backward ﬁnite diﬀerence (4.8), so
that the system (9.20) is replaced by
⎧
⎨
⎩
−μuj+1 −2uj + uj−1
h2
+ η uj −uj−1
h
= f(xj),
j = 1, . . . , N,
u0 = α,
uN+1 = β,
(9.23)
which is known as upwind scheme. It is possible to prove that if (9.18)
is approximated by (9.23), then the yielded numerical solution will not
exhibit any oscillation, as the graphs reported in Figure 9.3 conﬁrm.
9.2.3 Finite element approximation of the
one-dimensional Poisson problem
The ﬁnite element method represents an alternative to the ﬁnite diﬀer-
ence method for the approximation of boundary-value problems and is
derived from a suitable reformulation of the diﬀerential problem (9.10).
Let us consider again (9.10) and multiply both sides of the diﬀeren-
tial equation by a generic function v ∈C1([a, b]). Integrating the corre-
sponding equality on the interval (a, b) and using integration by parts
we obtain

338
9 Numerical approximation of boundary-valueproblems
a
b
x1 x2
xN−1 xN
vh
xj−2 xj−1
xj+1 xj+2
xj
ϕj
1
Figure 9.4. At left, a generic function vh ∈V 0
h . At right, the basis function
of V 0
h associated with the jth node
b

a
u′(x)v′(x) dx −[u′(x)v(x)]b
a =
b

a
f(x)v(x) dx.
By making the further assumption that v vanishes at the end-points
x = a and x = b, problem (9.10) becomes: ﬁnd u ∈C1([a, b]) such that
u(a) = α, u(b) = β and
b

a
u′(x)v′(x) dx =
b

a
f(x)v(x) dx
(9.24)
for each v ∈C1([a, b]) such that v(a) = v(b) = 0. This is called weak
formulation of problem (9.10). (Indeed, both u and the test function v
can be less regular than C1([a, b]), see, e.g. [Qua13], [QSS07], [QV94].)
Its ﬁnite element approximation is deﬁned as follows:
ﬁnd uh ∈Vh such that uh(a) = α, uh(b) = β and
N

j=0
xj+1

xj
u′
h(x)v′
h(x) dx =
b

a
f(x)vh(x) dx,
∀vh ∈V 0
h
(9.25)
where
Vh =
:
vh ∈C0([a, b]) : vh|Ij ∈P1, j = 0, . . . , N
;
,
(9.26)
i.e. Vh is the space of continuous functions on [a, b] whose restrictions
on every sub-interval Ij are linear polynomials. Moreover, V 0
h is the sub-
space of Vh of those functions vanishing at the end-points a and b. Vh is
called space of ﬁnite-elements of degree 1.
The functions in V 0
h are piecewise linear polynomials (see Figure 9.4,
left). In particular, every function vh of V 0
h admits the representation

9.2 Approximation of boundary-value problems
339
vh(x) =
N

j=1
vh(xj)ϕj(x),
where for j = 1, . . . , N,
ϕj(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
x −xj−1
xj −xj−1
if x ∈Ij−1,
x −xj+1
xj −xj+1
if x ∈Ij,
0
otherwise.
Thus, ϕj is null at every node xi except at xj where ϕj(xj) = 1 (see Fig-
ure 9.4, right). The functions ϕj, j = 1, . . . , N are called shape functions
and provide a basis for the vector space V 0
h .
Consequently, to fulﬁll (9.25) for any function in Vh is equivalent to
fulﬁll it only for the shape functions ϕj, j = 1, . . . , N. By exploiting the
fact that ϕj vanishes outside the intervals Ij−1 and Ij, from (9.25) we
obtain

Ij−1∪Ij
u′
h(x)ϕ′
j(x) dx =

Ij−1∪Ij
f(x)ϕj(x) dx,
j = 1, . . . , N. (9.27)
On the other hand, we can write uh(x) = *N
j=1 ujϕj(x) + αϕ0(x) +
βϕN+1(x), where uj = uh(xj), ϕ0(x) = (x1 −x)/(x1 −a) for a ≤x ≤x1,
and ϕN+1(x) = (x−xN)/(b−xN) for xN ≤x ≤b, while both ϕ0(x) and
ϕN+1(x) are zero otherwise. By substituting this expression in (9.27),
we ﬁnd:
u1

I0∪I1
ϕ′
1(x)ϕ′
1(x) dx + u2

I1
ϕ′
2(x)ϕ′
1(x) dx
=

I0∪I1
f(x)ϕ1(x) dx +
α
x1 −a,
uj−1

Ij−1
ϕ′
j−1(x)ϕ′
j(x) dx + uj

Ij−1∪Ij
ϕ′
j(x)ϕ′
j(x) dx
+uj+1

Ij
ϕ′
j+1(x)ϕ′
j(x) dx =

Ij−1∪Ij
f(x)ϕj(x) dx,
j = 2, . . . , N −1,
uN−1

IN−1
ϕ′
N−1(x)ϕ′
N(x) dx + uN

IN−1∪IN
ϕ′
N(x)ϕ′
N(x) dx
=

IN−1∪IN
f(x)ϕj(x) dx +
β
b −xN
.

340
9 Numerical approximation of boundary-valueproblems
In the special case where all intervals have the same length h, then
ϕ′
j−1 = −1/h in Ij−1, ϕ′
j = 1/h in Ij−1 and ϕ′
j = −1/h in Ij, ϕ′
j+1 = 1/h
in Ij. Consequently, we obtain
2u1 −u2
= h

I0∪I1
f(x)ϕ1(x) dx + α,
−uj−1 + 2uj −uj+1 = h

Ij−1∪Ij
f(x)ϕj(x) dx,
j = 2, . . . , N −1,
−uN−1 + 2uN
= h

IN−1∪IN
f(x)ϕN(x) dx + β.
The yielded linear system has unknowns {u1, . . . , uN} and shares the
same matrix (9.15) as the ﬁnite diﬀerence system, however it has a dif-
ferent right-hand side (and a diﬀerent solution too, in spite of coincidence
of notation). Finite diﬀerence and ﬁnite element solutions share however
the same accuracy with respect to h when the nodal maximum error is
computed.
We notice that 2nd-order convergence with respect to h is guaranteed
for ﬁnite diﬀerence approximation if f ∈C2([a, b]) (see (9.21)), while for
ﬁnite elements it is suﬃcient that f be a square-integrable function in
(a, b), i.e.,
 b
a
f 2(x)dx < +∞.
Obviously the ﬁnite element approach can be generalized to problems
like (9.17) (also in the case when μ, η and σ depend on x) and (9.18).
To approximate the convection-dominated problem (9.18), the up-
wind scheme used for ﬁnite diﬀerences can be reproduced also for ﬁnite-
elements. More precisely, by noting that
ui −ui−1
h
= ui+1 −ui−1
2h
−h
2
ui+1 −2ui + ui−1
h2
,
we can conclude that decentralizing ﬁnite diﬀerences is equivalent to
perturb the centered incremental ratio by a term corresponding to a
second-order derivative. This additional term can be interpreted as an
artiﬁcial viscosity. In other words, using upwind with ﬁnite-elements is
equivalent to solve, by the (centered) Galerkin method, the following
perturbed problem
−μhu′′(x) + ηu′(x) = f(x),
(9.28)
where μh = (1 + Pe)μ is the augmented viscosity.

9.2 Approximation of boundary-value problems
341
A further generalization of linear ﬁnite element methods consists
of using piecewise polynomials of degree greater than 1, allowing the
achievement of higher convergence orders. In these cases, the ﬁnite ele-
ment matrix does not coincide anymore with that of ﬁnite diﬀerences.
See Exercises 9.1-9.7.
9.2.4 Finite diﬀerence approximation of the two-dimensional
Poisson problem
Let us consider the Poisson problem (9.2), in a two-dimensional region
Ω.
The idea behind ﬁnite diﬀerences relies on approximating the partial
derivatives that are present in the PDE again by incremental ratios com-
puted on a suitable grid (called the computational grid) made of a ﬁnite
number of nodes. Then the solution u of the PDE will be approximated
only at these nodes.
The ﬁrst step therefore consists of introducing a computational grid.
Assume for simplicity that Ω is the rectangle (a, b) × (c, d). Let us in-
troduce a partition of [a, b] in subintervals (xi, xi+1) for i = 0, . . . , Nx,
with x0 = a and xNx+1 = b. Let us denote by Δx = {x0, . . . , xNx+1} the
set of end-points of such intervals and by hx =
max
i=0,...,Nx(xi+1 −xi) their
maximum length.
In a similar manner we introduce a discretization of the y-axis Δy =
{y0, . . . , yNy+1} with y0 = c, yNy+1 = d and hy =
max
j=0,...,Ny(yj+1 −yj).
The cartesian product Δh = Δx×Δy provides the computational grid on
Ω (see Figure 9.5), and h = max{hx, hy} is a characteristic measure of
the grid-size. We are looking for values ui,j which approximate u(xi, yj).
We will assume for the sake of simplicity that the nodes be uniformly
spaced, that is, xi = x0 + ihx for i = 0, . . . , Nx + 1 and yj = y0 + jhy for
j = 0, . . . , Ny + 1.
The second order partial derivatives of a function can be approxi-
mated by a suitable incremental ratio, as we did for ordinary deriva-
tives. In the case of a function of two variables, we deﬁne the following
incremental ratios:
δ2
xui,j = ui−1,j −2ui,j + ui+1,j
h2x
,
δ2
yui,j = ui,j−1 −2ui,j + ui,j+1
h2y
.
(9.29)
They are second order accurate with respect to hx and hy, respectively,
for the approximation of ∂2u/∂x2 and ∂2u/∂y2 at the node (xi, yj). If
we replace the second order partial derivatives of u with the formula

342
9 Numerical approximation of boundary-valueproblems
x
y
x0 = a x1
x2
x3
x4 = b
y0 = c
y1
y2
y3
y4
y5
y6 = d
hx
hy
Figure 9.5. The computational grid Δh with only 15 internal nodes on a
rectangular domain
(9.29), by requiring that the PDE is satisﬁed at all internal nodes of Δh,
we obtain the following set of equations:
−(δ2
xui,j + δ2
yui,j) = fi,j,
i = 1, . . . , Nx, j = 1, . . . , Ny.
(9.30)
We have set fi,j = f(xi, yj). We must add the equations that enforce
the Dirichlet data at the boundary, which are
ui,j = gi,j
∀i, j such that (xi, yj) ∈∂Δh,
(9.31)
where ∂Δh indicates the set of nodes belonging to the boundary ∂Ω of
Ω. These nodes are indicated by small squares in Figure 9.5. If we make
the further assumption that the computational grid is uniform in both
cartesian directions, that is, hx = hy = h, instead of (9.30) we obtain
−1
h2 (ui−1,j + ui,j−1 −4ui,j + ui,j+1 + ui+1,j) = fi,j,
i = 1, . . . , Nx, j = 1, . . . , Ny
(9.32)
The system given by equations (9.32) (or (9.30)) and (9.31) allows the
computation of the nodal values ui,j at all nodes of Δh. For every ﬁxed
pair of indices i and j, equation (9.32) involves ﬁve unknown nodal values
as we can see in Figure 9.6. For that reason this ﬁnite diﬀerence scheme
is called the ﬁve-point scheme for the Laplace operator. We note that
the unknowns associated with the boundary nodes can be eliminated
using (9.31) and therefore (9.30) (or (9.32)) involves only N = NxNy
unknowns.
The resulting system can be written in a more interesting form if
we adopt the lexicographic order according to which the nodes (and,

9.2 Approximation of boundary-value problems
343
(i, j)
(i+1, j)
(i−1, j)
(i, j−1)
(i, j+1)
Figure 9.6. The stencil of the ﬁve point scheme for the Laplace operator
correspondingly, the unknown components) are numbered by proceeding
from left to right and from the bottom to the top. By so doing, we obtain
a system like (9.14), with a matrix A ∈RN×N which takes the following
block tridiagonal form:
A = tridiag(D, T, D).
(9.33)
There are Ny rows and Ny columns, and every entry (denoted by a
capital letter) consists of a Nx×Nx matrix. In particular, D ∈RNx×Nx is
a diagonal matrix whose diagonal entries are −1/h2
y, while T ∈RNx×Nx
is a symmetric tridiagonal matrix
T = tridiag(−1
h2x
, 2
h2x
+ 2
h2y
, −1
h2x
).
A is symmetric since all diagonal blocks are symmetric. It is also positive
deﬁnite, that is vT Av > 0 ∀v ∈RN, v ̸= 0. Actually, by partitioning v
in Ny vectors vk of length Nx we obtain
vT Av =
Ny

k=1
vT
k Tvk −2
h2y
Ny−1

k=1
vT
k vk+1.
(9.34)
We can write T = 2/h2
yI + 1/h2
xK where K is the (symmetric and
positive deﬁnite) matrix given in (9.15) and I is the identity matrix.
Consequently, using the identity 2a(a −b) = a2 −b2 + (a −b)2 and some
algebraic manipulation, (9.34) reads
vT Av = 1
h2x
Ny−1

k=1
vT
k Kvk
+ 1
h2y
⎛
⎝vT
1 v1 + vT
NyvNy +
Ny−1

k=1
(vk −vk+1)T (vk −vk+1)
⎞
⎠,
which is a strictly positive real number since K is positive deﬁnite and
at least one vector vk is non-null.

344
9 Numerical approximation of boundary-valueproblems
0
20
40
60
80
0
10
20
30
40
50
60
70
80
Figure 9.7. Pattern of the matrix associated with the ﬁve-point scheme using
the lexicographic ordering of the unknowns
Having proven that A is non-singular we can conclude that the ﬁnite
diﬀerence system admits a unique solution uh.
The matrix A is sparse; as such, it will be stored in the format sparse
of MATLAB (see Section 5.3). In Figure 9.7 (obtained by using the
command spy(A)) we report the structure of the matrix corresponding
to a uniform grid of 11 × 11 nodes, after having eliminated the rows and
columns associated to the nodes of ∂Δh. It can be noted that the only
nonzero elements lie on ﬁve diagonals.
Since A is symmetric and positive deﬁnite, the associated system can
be solved eﬃciently by either direct or iterative methods, as illustrated
in Chapter 5. Finally, it is worth pointing out that A shares with its
one-dimensional analog the property of being ill-conditioned: indeed, its
condition number grows like h−2 as h tends to zero.
In the Program 9.2 we construct and solve the system (9.30)-(9.31)
(using the command \, see Section 5.8). The input parameters a, b,
c and d denote the endpoints of the intervals generating the domain
Ω = (a, b) × (c, d), while nx and ny denote the values of Nx and Ny (the
case Nx ̸= Ny is admitted). Finally, the two function handles fun and
bound are associated with the right-hand side f = f(x, y) (otherwise
called the source term) and the Dirichlet boundary data g = g(x, y),
respectively. The output variable uh is a matrix whose j, ith entry is
ui,j, while xh and yh are vectors whose components are the nodes xi and
yj, respectively, all including the nodes of the boundary. The numerical
solution can be visualized by the command mesh(x,y,u). The (optional)
mesh
input function uex stands for the exact solution of the original problem
for those cases (of theoretical interest) where this solution is known. In
such cases the output parameter error contains the nodal relative error
between the exact and numerical solution, which is computed as follows:
error = max
i,j |u(xi, yj) −ui,j|
<
max
i,j |u(xi, yj)|.

9.2 Approximation of boundary-value problems
345
Program
9.2.
poissonfd: approximation of the Poisson problem with
Dirichlet boundary data by the ﬁve-point ﬁnite diﬀerence method
function [xh ,yh ,uh ,error]= poissonfd (a,b,c,d,nx ,ny ,...
fun ,bound ,uex ,varargin )
% POISSONFD
two - dimensional
Poisson
solver
%
[XH ,YH ,UH ]= POISSONFD (A,B,C,D,NX ,NY ,FUN ,BOUND) solves
%
by the five -point finite
difference
scheme the
%
problem -LAPL (U) = FUN in the
rectangle
(A,B)X(C,D)
%
with
Dirichlet
boundary
conditions
U(X,Y)= BOUND(X,Y)
%
at any (X,Y) on the boundary
of the rectangle .
%
[XH ,YH ,UH ,ERROR]= POISSONFD (A,B,C,D,NX ,NY ,FUN ,...
%
BOUND ,UEX) computes
also
the
maximum
nodal error
%
ERROR with
respect to the exact solution
UEX.
%
FUN ,BOUND and UEX are function
handles.
%
[XH ,YH ,UH ,ERROR]= POISSONFD (A,B,C,D,NX ,NY ,FUN ,...
%
BOUND ,UEX ,P1 ,P2 , ...)
passes the optional
arguments
%
P1 ,P2 ,...
to the
functions
FUN ,BOUND ,UEX.
if nargin == 8
uex = @(x,y)0+0*x+0*y;
end
nx1 = nx +2; ny1=ny +2; dim = nx1*ny1;
hx = (b-a)/( nx +1); hy = (d-c)/( ny +1);
hx2 = hx ^2;
hy2 = hy ^2;
kii = 2/ hx2 +2/ hy2; kix =
-1/hx2;
kiy =
-1/hy2;
K = speye(dim ,dim );
rhs = zeros(dim ,1);
y = c;
for m = 2:ny +1
x = a; y = y + hy;
for n = 2: nx+1
i = n+(m -1)* nx1; x = x + hx;
rhs(i) = fun(x,y,varargin {:});
K(i,i) = kii; K(i,i-1) = kix; K(i,i+1) = kix;
K(i,i+nx1) = kiy;
K(i,i-nx1) = kiy;
end
end
rhs1 = zeros(dim ,1); xh = [a:hx:b]’; yh = [c:hy:d];
rhs1 (1: nx1) = bound(xh ,c,varargin {:});
rhs1 (dim -nx -1: dim) = bound(xh ,d,varargin {:});
rhs1 (1: nx1:dim -nx -1) = bound(a,yh ,varargin {:});
rhs1 (nx1:nx1:dim) = bound(b,yh ,varargin {:});
rhs = rhs - K*rhs1 ;
nbound = [[1: nx1],[dim -nx -1: dim ],[1: nx1:dim -nx -1] ,...
[nx1:nx1:dim ]];
ninternal
= setdiff ([1: dim],nbound );
K = K(ninternal ,ninternal );
rhs = rhs(ninternal );
utemp = K\ rhs;
u = rhs1 ; u ( ninternal ) = utemp;
k = 1; y = c;
for j = 1: ny1
x = a;
for i = 1:nx1
uh(j,i) = u(k);
k = k + 1;
ue(j,i) = uex(x,y,varargin {:});
x = x + hx;
end
y = y + hy;

346
9 Numerical approximation of boundary-valueproblems
end
if
nargout == 4 & nargin
>= 9
error = max(max(abs(uh -ue )))/ max(max(abs(ue )));
elseif
nargout == 4 & nargin ==8
warning(’Exact solution
not
available ’);
error = [ ];
end
end
Example 9.1 The transverse displacement u of an elastic membrane from the
reference plane z = 0, under a load whose intensity is f(x, y) = 8π2 sin(2πx)
cos(2πy), satisﬁes a Poisson problem like (9.2) in the domain Ω = (0, 1)2. The
Dirichlet value of the displacement is prescribed on ∂Ω as follows: g = 0 on
the sides x = 0 and x = 1, and g(x, 0) = g(x, 1) = sin(2πx), 0 < x < 1.
This problem admits the exact solution u(x, y) = sin(2πx) cos(2πy). In Figure
9.8 we show the numerical solution obtained by the ﬁve-point ﬁnite diﬀerence
scheme on a uniform grid. Two diﬀerent values of h have been used: h = 1/10
(left) and h = 1/20 (right). When h decreases the numerical solution improves,
and actually the nodal relative error is 0.0292 for h = 1/10 and 0.0081 for
h = 1/20.
■
Also the ﬁnite element method can be easily extended to the two-
dimensional case. To this end the problem (9.2) must be reformulated
in an integral form and the partition of the interval (a, b) in one dimen-
sion must be replaced by a decomposition of Ω by polygons (typically,
triangles) called elements. The generic shape function ϕk will still be a
continuous function, whose restriction on each element is a polynomial of
degree 1 on each element, which is equal to 1 at the kth vertex (or node)
of the triangulation and 0 at all other vertices. For its implementation
one can use the MATLAB toolbox pde.
pde
Figure 9.8. Transverse displacement of an elastic membrane computed on
two uniform grids, coarser at left and ﬁner at right. On the horizontal plane
we report the isolines of the numerical solution. The triangular partition of Ω
only serves the purpose of the visualization of the results

9.2 Approximation of boundary-value problems
347
9.2.5 Consistency and convergence of ﬁnite diﬀerence
discretization of the Poisson problem
In the previous section we have shown that the solution of the ﬁnite
diﬀerence problem exists and is unique. Now we investigate the approx-
imation error. We will assume for simplicity that hx = hy = h. If
max
i,j |u(xi, yj) −ui,j| →0 as h →0
(9.35)
the method used to compute ui,j is called convergent.
As we have already pointed out (see Remark 8.1), consistency is
a necessary condition for convergence. A method is consistent if the
residual, that is the error obtained when the exact solution is plugged
into the numerical scheme, tends to zero when h tends to zero. If we
consider the ﬁve point ﬁnite diﬀerence scheme, at every internal node
(xi, yj) of Δh we deﬁne
τh(xi, yj) = −f(xi, yj)
−1
h2 [u(xi−1, yj) + u(xi, yj−1) −4u(xi, yj) + u(xi, yj+1) + u(xi+1, yj)] .
This is the local truncation error at the node (xi, yj). By (9.2) we obtain
τh(xi, yj) =
∂2u
∂x2 (xi, yj) −u(xi−1, yj) −2u(xi, yj) + u(xi+1, yj)
h2
5
+
∂2u
∂y2 (xi, yj) −u(xi, yj−1) −2u(xi, yj) + u(xi, yj+1)
h2
5
.
Thanks to the analysis that was carried out in Section 9.2.4 we can
conclude that both terms vanish as h tends to 0. Thus
lim
h→0τh(xi, yj) = 0,
(xi, yj) ∈Δh \ ∂Δh,
that is, the ﬁve-point method is consistent.
It is also convergent, as stated in the following Proposition (for its
proof, see, e.g., [IK66]):
Proposition 9.1 Assume that the exact solution u ∈C4( ¯Ω), i.e.
all its partial derivatives up to the fourth order are continuous in
the closed domain ¯Ω. Then there exists a constant C > 0 such that
max
i,j |u(xi, yj) −ui,j| ≤CMh2
(9.36)
where M is the maximum absolute value attained by the fourth order
derivatives of u in ¯Ω.

348
9 Numerical approximation of boundary-valueproblems
Example 9.2 Let us experimentally verify that the ﬁve-point scheme applied
to solve the Poisson problem of Example 9.1 converges with order two with
respect to h. We start from h = 1/4 and, then we halve subsequently the value
of h, until h = 1/64, through the following instructions:
a=0;b=1;c=0;d=1;
f=@(x,y) 8* pi^2* sin (2*pi*x).* cos (2*pi*y);
g=@(x,y) sin (2* pi*x).* cos (2* pi*y);
uex=g; nx =4; ny=4;
for n=1:5
[xh ,yh ,uh ,error(n)]= poissonfd (a,b,c,d,nx ,ny ,f,g,uex );
nx = 2* nx; ny = 2*ny;
The vector containing the error is
format short e; error
1.3565e-01
4.3393e-02
1.2308e-02
3.2775e-03
8.4557e-04
As we can verify using the following commands (see formula (1.12))
log(abs(error(1:end-1)./error(2:end)))/log(2)
1.6443e+00
1.8179e+00
1.9089e+00
1.9546e+00
this error decreases as h2 when h →0.
■
9.2.6 Finite diﬀerence approximation of the one-dimensional
heat equation
We consider the one-dimensional heat equation (9.4) with homogeneous
Dirichlet boundary conditions u(a, t) = u(b, t) = 0 for any t > 0 and
initial condition u(x, 0) = u0(x) for x ∈[a, b].
To solve this equation numerically we have to discretize both the x
and t variables. We can start by dealing with the x-variable, following the
same approach as in Section 9.2.1. We denote by uj(t) an approximation
of u(xj, t), j = 0, . . . , N +1, and approximate the Dirichlet problem (9.4)
by the scheme: for all t > 0
⎧
⎨
⎩
duj
dt (t) −μ
h2 (uj−1(t) −2uj(t) + uj+1(t)) = fj(t),
j = 1, . . . , N,
u0(t) = uN+1(t) = 0,
where fj(t) = f(xj, t) and, for t = 0,
uj(0) = u0(xj),
j = 0, . . . , N + 1.
This is actually a semi-discretization of the heat equation, yielding a
system of ordinary diﬀerential equations of the following form
⎧
⎨
⎩
du
dt (t) = −μ
h2 Au(t) + f(t)
∀t > 0,
u(0) = u0,
(9.37)

9.2 Approximation of boundary-value problems
349
where u(t) = (u1(t), . . . , uN(t))T is the vector of unknowns, f(t) =
(f1(t), . . . , fN(t))T , u0 = (u0(x1), . . . , u0(xN))T , and A is the tridiag-
onal matrix introduced in (9.15). Note that for the derivation of (9.37)
we have assumed that u0(x0) = u0(xN+1) = 0, which is coherent with
the homogeneous Dirichlet boundary conditions.
A popular scheme for the integration in time of (9.37) is the so-called
θ−method. Let Δt > 0 be a constant time-step, and denote by vk the
value of a variable v referred at the time level tk = kΔt. Then the θ-
method reads
uk+1 −uk
Δt
= −μ
h2 A(θuk+1 + (1 −θ)uk) + θf k+1 + (1 −θ)f k,
k = 0, 1, . . .
u0 given
(9.38)
or, equivalently,

I + μ
h2 θΔtA

uk+1 =

I −μ
h2 Δt(1 −θ)A

uk + gk+1,
(9.39)
where gk+1 = Δt(θf k+1 + (1 −θ)f k) and I is the identity matrix of order
N.
For suitable values of the parameter θ, from (9.39) we can recover
some familiar methods that have been introduced in Chapter 8. For
example, if θ = 0 the method (9.39) coincides with the forward Euler
scheme and we can obtain uk+1 explicitly; otherwise, a linear system
(with constant matrix I + μθΔtA/h2) needs to be solved at each time
level.
Regarding stability, when f = 0 the exact solution u(x, t) tends to
zero for every x as t →∞. Then we would expect the discrete solution to
have the same behavior, in which case we would call our scheme (9.39)
asymptotically stable, this being coherent with the absolute stability con-
cept deﬁned in Section 8.6 for ordinary diﬀerential equations.
In order to study asymptotic stability, let us consider the equation
(9.39) with g(k+1) = 0 ∀k ≥0.
If θ = 0, it follows that
uk = (I −μΔtA/h2)ku0,
k = 1, 2, . . .
whence uk →0 as k →∞iﬀ
ρ(I −μΔtA/h2) < 1.
(9.40)
On the other hand, the eigenvalues λj of A are given by
λj = 2 −2 cos(jπ/(N + 1)) = 4 sin2(jπ/(2(N + 1))),
j = 1, . . . , N

350
9 Numerical approximation of boundary-valueproblems
(see Exercise 9.2). Then (9.40) is satisﬁed if
Δt < 1
2μh2.
As expected, the forward Euler method is conditionally asymptotically
stable, under the condition that the time-step Δt should decay as the
square of the grid spacing h.
In the case of the backward Euler method (θ = 1), we would have
from (9.39)
uk =

(I + μΔtA/h2)−1k u0,
k = 1, 2, . . .
Since all the eigenvalues of the matrix (I+μΔtA/h2)−1 are real, positive
and strictly less than 1 for every value of Δt, this scheme is uncondi-
tionally asymptotically stable. More generally, the θ-scheme is uncon-
ditionally asymptotically stable for all the values 1/2 ≤θ ≤1, and
conditionally asymptotically stable if 0 ≤θ < 1/2 (see, for instance,
[QSS07, Chapter 13]).
As far as the accuracy of the θ-method is concerned, its local trunca-
tion error behaves like Δt+h2 if θ ̸= 1
2 while it is of the order of Δt2+h2
if θ = 1
2. The latter is the Crank-Nicolson method (see Section 8.4) and is
therefore unconditionally asymptotically stable; the corresponding global
(in both space and time) discretization scheme is second-order accurate
with respect to both Δt and h.
The same conclusions hold for the heat equation in a two-dimensional
domain. In this case in the scheme (9.38) one must substitute to the
matrix A/h2 the ﬁnite diﬀerence matrix deﬁned in (9.33).
Program 9.3 solves numerically the heat equation on the time interval
(0, T ) and on the domain Ω = (a, b) using the θ-method. The input
parameters are the vectors xspan=[a,b] and tspan=[0,T], the number
of discretization intervals in space (nstep(1)) and in time (nstep(2)),
the scalar mu which contains the positive real coeﬃcient μ, the function
handles u0, fun and g associated with the initial function u0(x), the right
hand side f(x, t) and the Dirichlet datum g(x, t), respectively. Finally,
the variable theta contains the coeﬃcient θ. The output variable uh
contains the numerical solution at the ﬁnal time t = T .
Program 9.3. heattheta: θ-method for the one-dimensional heat equation
function [xh ,uh]= heattheta (xspan ,tspan ,nstep ,mu ,...
u0 ,g,f,theta , varargin )
% HEATTHETA
Solves the heat
equation
with
the
% theta -method.
% [XH ,UH]= HEATTHETA (XSPAN ,TSPAN ,NSTEP ,MU ,U0 ,G,F,THETA)
% solves the
heat
equation D U/DT - MU D^2U/DX^2 = F
% in (XSPAN (1), XSPAN (2)) X (TSPAN(1), TSPAN (2))
using
% the theta -method
with
initial
condition U(X ,0)= U0(X)

9.2 Approximation of boundary-value problems
351
% and Dirichlet
boundary
conditions
U(X,T)=G(X,T) at
% X=XSPAN (1) and X=XSPAN (2).
% MU is a positive
constant , F=F(X,T), G=G(X,T) and
% U0=U0(X) are
function
handles .
% NSTEP (1) is the
number of space integration
intervals
% NSTEP (2) is the
number of time -integration
intervals
% XH contains
the nodes of the
discretization .
% UH contains
the
numerical
solutions
at time
TSPAN (2).
% [XH ,UH]= HEATTHETA (XSPAN ,TSPAN ,NSTEP ,MU ,U0 ,G,F ,...
% THETA ,P1 ,P2 ,...) passes the
additional
parameters
% P1 ,P2 ,... to the
functions U0 ,G,F.
h
= (xspan (2)- xspan (1))/ nstep (1);
dt = (tspan (2)- tspan (1))/ nstep (2);
N = nstep (1)+1;
e = ones (N ,1);
D = spdiags ([-e 2*e -e],[-1,0,1], N,N);
I = speye(N);
A = I+mu*dt*theta*D/h^2;
An = I-mu*dt *(1- theta)*D/h^2;
A(1,:) = 0; A(1,1) = 1;
A(N ,:) = 0; A(N,N) = 1;
xh = (linspace (xspan(1), xspan(2),N))’;
fn = f(xh ,tspan(1), varargin {:});
un = u0(xh ,varargin {:});
[L,U]=lu(A);
for t = tspan (1)+ dt:dt:tspan(2)
fn1 = f(xh ,t,varargin {:});
rhs = An*un+dt*( theta*fn1 +(1- theta )*fn);
temp = g([ xspan(1), xspan(2)],t,varargin {:});
rhs([1,N]) = temp ;
uh = L\rhs; uh = U\uh; fn = fn1; un = uh;
end
return
Example 9.3 We consider the heat equation (9.4) in (a, b) = (0, 1) with
μ = 1, f(x, t) = −sin(x) sin(t)+sin(x) cos(t), initial condition u(x, 0) = sin(x)
and boundary conditions u(0, t) = 0 and u(1, t) = sin(1) cos(t). In this case
the exact solution is u(x, t) = sin(x) cos(t). In Figure 9.9 we compare the
behavior of the errors maxi=0,...,N |u(xi, 1) −uM
i | with respect to the time-
step on a uniform grid in space with h = 0.002. {uM
i } are the values of the
ﬁnite diﬀerence solution computed at time tM = 1. As expected, for θ = 0.5
the θ-method is second order accurate until when the time-step is so small that
the spatial error dominates over the error due to the temporal discretization.
.
■
Example 9.4 (Thermodynamics) We consider a homogeneous, three me-
ters long aluminium bar with uniform section. We are interested in simulating
the evolution of the temperature in the bar starting from a suitable initial
condition, by solving the heat equation (9.5). If we impose adiabatic condi-
tions on the lateral surface of the bar (i.e. homogeneous Neumann conditions),
and Dirichlet conditions at the end sections of the bar, the temperature only
depends on the axial space variable (denoted by x). Thus the problem can
be modeled by the one-dimensional heat equation (9.7) with f = 0, com-
pleted by the initial condition at t = t0 and by Dirichlet boundary con-
ditions at the endpoints of the reduced computational domain Ω = (0, L)

352
9 Numerical approximation of boundary-valueproblems
10
−3
10
−2
10
−1
10
−8
10
−7
10
−6
10
−5
10
−4
10
−3
1
1
1
2
Figure 9.9. Error versus Δt for the θ-method (for θ = 1, solid line, and
θ = 0.5 dashed line), for three diﬀerent values of h: 0.008 (□), 0.004 (◦) and
0.002 (no symbols)
(L = 3m). Pure aluminium has thermal conductivity k = 237 W/(m K), den-
sity ρ = 2700kg/m3 and speciﬁc heat capacity c = 897 J/(kg K), then its
thermal diﬀusivity is μ = 9.786 · 10−5m2/s. Finally we consider the initial
condition T (x, 0) = 500 K if x ∈(1, 2), 250 K otherwise and the Dirich-
let boundary conditions T (0, t) = T (3, t) = 250 K. In Figure 9.10 we report
the evolution of the temperature starting from the initial data, computed by
the backward Euler method (θ = 1, left) and by the Crank-Nicolson method
(θ = 0.5, right) (using Program 9.3).
The results show that when the time-step is large (Δt = 20sec), the Crank-
Nicolson method is unstable because of the low smoothness of the initial datum
(about this point, see also [QV94, Chapter 11]). On the contrary, the implicit
Euler method provides a stable solution because it is more dissipative than
Crank-Nicolson. Both methods compute a solution that decays to the correct
value 250 K as t →∞.
■
9.2.7 Finite element approximation of the
one-dimensional heat equation
The space discretization of the heat equation (9.4) with homogeneous
Dirichlet boundary conditions u(a, t) = u(b, t) = 0, ∀t > 0 can be ac-
complished using the Galerkin ﬁnite element method by proceeding as
we did in Section 9.2.3 for the Poisson equation. First, for all t > 0 we
multiply (9.4) by a test function v = v(x) ∈C1([a, b]) and we integrate
the resulting equation over (a, b). For all t > 0 we therefore look for a
function t →u(x, t) ∈C1([a, b]) such that
 b
a
∂u
∂t (x, t)v(x)dx +
 b
a
μ∂u
∂x(x, t)dv
dx(x)dx =
(9.41)
=
 b
a
f(x)v(x)dx
∀v ∈C1([a, b]),

9.2 Approximation of boundary-value problems
353
0
0.5
1
1.5
2
2.5
3
250
300
350
400
450
500
t = 0 s
t = 2000 s
0
0.5
1
1.5
2
2.5
3
250
300
350
400
450
500
t = 0 s
t = 2000 s
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
250
300
350
400
450
500
0.5
0.6
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
250
300
350
400
450
500
Figure 9.10. Temperature proﬁles in an aluminium bar at diﬀerent time levels
(from t = 0 to t = 2000 seconds with time-step Δt of 0.25 seconds (top) and
20 seconds (bottom)), obtained using the backward Euler method (left) and
the Crank-Nicolson method (right). In both cases, the space discretization is
carried out by centered ﬁnite diﬀerences with steplength h = 0.01. The zoom
on the solutions for Δt = 20sec (at bottom) shows instability of the Crank-
Nicolson scheme
with u(x, 0) = u0(x). To simplify notations, from now on the dependence
on variable x in u, v and f will be understood.
Equation (9.41) keeps holding also for functions v less regular than
C1([a, b]), e.g., like those of the space Vh deﬁned in (9.26). Therefore,
we consider the following Galerkin formulation: ∀t > 0, ﬁnd uh(t) ∈Vh
such that
 b
a
∂uh
∂t (t)vhdx +
 b
a
μ∂uh
∂x (t)dvh
dx dx =
 b
a
f(t)vhdx
∀vh ∈Vh, (9.42)
where uh(0) = u0
h and u0
h ∈Vh is a convenient approximation of u0.
Formulation (9.42) is called semi-discretization of problem (9.41), since
only the space discretization (not yet the time) was carried out.
For what concerns the ﬁnite element discretization of (9.42), let us
consider the basis functions ϕj introduced in Section 9.2.3. Then, the
solution uh of (9.42) can be sought under the form
uh(t) =
N

j=1
uj(t)ϕj,

354
9 Numerical approximation of boundary-valueproblems
where {uj(t)} are the unknown coeﬃcients and N is the dimension of
Vh.
Then, from (9.42) we obtain
 b
a
N

j=1
duj
dt (t)ϕjϕidx + μ
 b
a
N

j=1
uj(t)dϕj
dx
dϕi
dx dx =
=
 b
a
f(t)ϕidx,
i = 1, . . . , N
that is,
N

j=1
duj
dt (t)
 b
a
ϕjϕidx + μ
N

j=1
uj(t)
 b
a
dϕj
dx
dϕi
dx dx =
=
 b
a
f(t)ϕidx,
i = 1, . . . , N.
Using the same notations as in (9.37) we obtain
Mdu
dt (t) + Afeu(t) = ffe(t),
(9.43)
where (Afe)ij
= μ
 b
a
dϕj
dx
dϕi
dx dx, (ffe(t))i =
 b
a f(t)ϕidx and Mij
=
(
 b
a ϕjϕidx) for i, j = 1, . . . , N. M is called the mass matrix. Since it
is not singular, the system of ordinary diﬀerential equations (9.43) can
be written in normal form as
du
dt (t) = −M−1Afeu(t) + M−1ffe(t).
(9.44)
To solve (9.43) approximately, we can still apply the θ-method and obtain
Muk+1 −uk
Δt
+ Afe

θuk+1 + (1 −θ)uk
= θf k+1
fe
+ (1 −θ)f k
fe.
(9.45)
As usual, the upper index k means that the quantity at hand is computed
at time tk = kΔt, Δt > 0 being the time discretization step. As in the
ﬁnite diﬀerence case, for θ = 0, 1 and 1/2, we respectively obtain the
forward Euler, backward Euler and Crank-Nicolson methods, the latter
being the only one which is second-order accurate with respect to Δt.
For each k, (9.45) is a linear system whose matrix is
K = 1
ΔtM + θAfe.
Since both matrices M and Afe are symmetric and positive deﬁnite, the
matrix K is also symmetric and positive deﬁnite. Moreover, K is in-
dependent of k and then it can be factorized once at t = 0. For the

9.3 Hyperbolic equations: a scalar pure advection problem
355
one-dimensional case that we are handling, this factorization is based
on the Thomas method (see Section 5.6) and it requires a number of
operation proportional to N. In the multidimensional case the use of the
Cholesky factorization K = RT R, R being an upper triangular matrix
(see (5.17)), will be more convenient. Consequently, at each time level
the following two linear triangular systems, each of size equal to N, must
be solved:
⎧
⎨
⎩
RT y =
. 1
ΔtM −(1 −θ)Afe
/
uk + θf k+1
fe
+ (1 −θ)f k
fe,
Ruk+1 = y.
When θ = 0, a suitable diagonalization of M would allow to decouple the
system equations (9.45). The procedure is carried out by the so-called
mass-lumping in which we approximate M by a non-singular diagonal
matrix -M. In the case of piecewise linear ﬁnite elements, -M can be ob-
tained using the composite trapezoidal formula over the nodes {xi} to
evaluate the integrals
 b
a ϕjϕi dx, obtaining ˜mij = hδij, i, j = 1, . . . , N.
If θ ≥1/2, the θ-method is unconditionally stable for every positive
value of Δt, while if 0 ≤θ < 1/2 the θ-method is stable only if
0 < Δt ≤
2
(1 −2θ)λmax(M−1Afe),
to this aim see [Qua13, Chap. 5]. Moreover, it is possible to prove that
there exist two positive constants c1 and c2, independent of h, such that
c1h−2 ≤λmax(M−1Afe) ≤c2h−2
(see [QV94, Section 6.3.2] for a proof). Thanks to this property, if 0 ≤
θ < 1/2 the method is stable only if
0 < Δt ≤C1(θ)h2,
(9.46)
where C1(θ) is a suitable constant independent of both discretization
parameters h and Δt.
9.3 Hyperbolic equations: a scalar pure advection
problem
Let us consider the following scalar hyperbolic problem
⎧
⎨
⎩
∂u
∂t + a∂u
∂x = 0,
x ∈R, t > 0,
u(x, 0) = u0(x),
x ∈R,
(9.47)
where a is a positive real number. Its solution is given by

356
9 Numerical approximation of boundary-valueproblems
t
x
α
β
P0
P
Q
t
x
t = 1
1
0
Figure 9.11. At left: examples of characteristics which are straight lines is-
suing from the points P and Q. At right: characteristic straight lines for the
Burgers equation (9.51)
u(x, t) = u0(x −at), t ≥0,
and represents a wave travelling with velocity a. The curves (x(t), t)
in the plain (x, t), that satisfy the following scalar ordinary diﬀerential
equation
⎧
⎨
⎩
dx
dt (t) = a,
t > 0,
x(0) = x0,
(9.48)
are called characteristic curves (or, simply, characteristics), and are the
straight lines x(t) = x0 + at, t > 0. The solution of (9.47) remains
constant along them since
du
dt = ∂u
∂t + ∂u
∂x
dx
dt = 0
on (x(t), t).
For the more general problem
⎧
⎨
⎩
∂u
∂t + a∂u
∂x + a0u = f,
x ∈R,
t > 0,
u(x, 0) = u0(x),
x ∈R,
(9.49)
where a, a0 and f are given functions of the variables (x, t), the charac-
teristic curves are still deﬁned as in (9.48). In this case, the solutions of
(9.49) satisfy along the characteristics the following diﬀerential equation
du
dt = f −a0u
on (x(t), t).
Let us now consider problem (9.47) on a bounded interval [α, β]

9.3 Hyperbolic equations: a scalar pure advection problem
357
⎧
⎨
⎩
∂u
∂t + a∂u
∂x = 0,
x ∈(α, β), t > 0,
u(x, 0) = u0(x),
x ∈(α, β).
(9.50)
Let us start with a > 0. Since u is constant along the characteristics,
from Figure 9.11, left, we deduce that the value of the solution at P
attains the value of u0 at P0, the foot of the characteristic issuing from
P. On the other hand, the characteristic issuing from Q intersects the
straight line x(t) = α at a certain time t = ¯t > 0. Thus, the point x = α
is an inﬂow point and it is necessary to assign there a boundary value for
u, for every t > 0. Notice that if a < 0 then the inﬂow point is x = β and
it is necessary to assign there a boundary value for u, for every t > 0.
Referring to problem (9.47) it is worth noting that if u0 is discon-
tinuous at a point x0, then such a discontinuity propagates along the
characteristics issuing from x0. This process can be made rigorous by in-
troducing the concept of weak solutions of hyperbolic problems, see e.g.
[GR96]. Another reason for introducing weak solutions is that in the case
of nonlinear hyperbolic problems the characteristic lines can intersect:
in this case the solution cannot be continuous and no classical solution
does exist.
Example 9.5 (Burgers equation) Let us consider the Burgers equation
∂u
∂t + u∂u
∂x = 0,
x ∈R,
t > 0,
(9.51)
which is perhaps the simplest nontrivial example of a nonlinear hyperbolic
equation. Taking as initial condition
u(x, 0) = u0(x) =
⎧
⎨
⎩
1,
x ≤0,
1 −x, 0 < x ≤1,
0,
x > 1,
the characteristic line issuing from the point (x0, 0) is given by
x(t) = x0 + tu0(x0) =
⎧
⎨
⎩
x0 + t,
x0 ≤0,
x0 + t(1 −x0), 0 < x0 ≤1,
x0,
x0 > 1.
Notice that the characteristic lines do not intersect only if t < 1 (see Figure
9.11, right).
■
9.3.1 Finite diﬀerence discretization of the scalar transport
equation
The half-plane {(x, t) : −∞< x < ∞, t > 0} is discretized by choosing
a spatial grid size Δx > 0 (the parameter named h until now), a temporal
step Δt > 0 and the grid points (xj, tn) as follows

358
9 Numerical approximation of boundary-valueproblems
xj = jΔx,
j ∈Z,
tn = nΔt,
n ∈N.
Let us set
λ = Δt/Δx,
and deﬁne xj+1/2 = xj + Δx/2. We look for discrete solutions un
j which
approximate the values u(xj, tn) of the exact solution for any j, n. Quite
often, explicit methods are employed for advancing in time hyperbolic
initial-value problems.
Any explicit ﬁnite-diﬀerence method can be written in the form
un+1
j
= un
j −λ(hn
j+1/2 −hn
j−1/2),
(9.52)
where hn
j+1/2 = h(un
j , un
j+1) for every j and h(·, ·) is a function, to be
properly chosen, that is called the numerical ﬂux.
In what follows we will illustrate several instances of explicit methods
for the approximation of problem (9.47):
1. forward Euler/centered
un+1
j
= un
j −λ
2 a(un
j+1 −un
j−1),
(9.53)
which can be cast in the form (9.52) by setting
hn
j+1/2 = 1
2a(un
j+1 + un
j );
(9.54)
2. Lax-Friedrichs
un+1
j
= 1
2(un
j+1 + un
j−1) −λ
2 a(un
j+1 −un
j−1),
(9.55)
which is of the form (9.52) with
hn
j+1/2 = 1
2[a(un
j+1 + un
j ) −λ−1(un
j+1 −un
j )];
(9.56)
3. Lax-Wendroﬀ
un+1
j
= un
j −λ
2 a(un
j+1 −un
j−1) + λ2
2 a2(un
j+1 −2un
j + un
j−1), (9.57)
which can be written in the form (9.52) provided that
hn
j+1/2 = 1
2[a(un
j+1 + un
j ) −λa2(un
j+1 −un
j )];
(9.58)
4. Upwind (or forward Euler/decentered)
un+1
j
= un
j −λ
2 a(un
j+1 −un
j−1) + λ
2 |a|(un
j+1 −2un
j + un
j−1),
(9.59)
which ﬁts the form (9.52) when the numerical ﬂux is deﬁned to be
hn
j+1/2 = 1
2[a(un
j+1 + un
j ) −|a|(un
j+1 −un
j )].
(9.60)

9.3 Hyperbolic equations: a scalar pure advection problem
359
Table 9.1. Artiﬁcial viscosity, artiﬁcial diﬀusion ﬂux, and truncation error for
Lax-Friedrichs, Lax-Wendroﬀand upwind methods
method
k
hdiff
j+1/2
τ(Δt, Δx)
Lax-Friedrichs
Δx2
−1
2λ(uj+1 −uj)
O

Δx2/Δt + Δt + Δx2
Lax-Wendroﬀ
a2Δt2
−λa2
2 (uj+1 −uj)
O

Δt2 + Δx2 + ΔtΔx2
upwind
|a|ΔxΔt
−|a|
2 (uj+1 −uj)
O(Δt + Δx)
Each one of the last three methods can be obtained from the forward
Euler/centered method by adding a term proportional to the centered
ﬁnite diﬀerence (4.9), so that they can be written in the equivalent form
un+1
j
= un
j −λ
2 a(un
j+1 −un
j−1) + 1
2k un
j+1 −2un
j + un
j−1
(Δx)2
.
(9.61)
The last term represents indeed a discretization of the second-order
derivative
k
2
∂2u
∂x2 (xj, tn).
The coeﬃcient k > 0 plays the role of artiﬁcial viscosity. Its expression
is given for the three previous cases in Table 9.1. Consequently, the
numerical ﬂux for each scheme can be equivalently written as
hj+1/2 = hF E
j+1/2 + hdiff
j+1/2,
where hF E
j+1/2 is the numerical ﬂux of the forward Euler/centered scheme
(which is given in (9.54)) and the artiﬁcial diﬀusion ﬂux hdiff
j+1/2 for the
three cases is also reported in Table 9.1.
The most classical implicit method is the backward Euler/centered scheme
un+1
j
+ λ
2 a(un+1
j+1 −un+1
j−1 ) = un
j .
(9.62)
It can still be written in the form (9.52) provided that hn is replaced by
hn+1. In the example at hand, the numerical ﬂux is the same as for the
forward Euler/centered method.
9.3.2 Finite diﬀerence analysis for the scalar transport
equation
The convergence analysis of ﬁnite diﬀerence methods introduced in the
previous Section requires that both consistency and stability hold.
Consider for instance, the forward Euler/centered method (9.53). As

360
9 Numerical approximation of boundary-valueproblems
done in Section 8.3.1, denoting by u the exact solution of problem (9.47),
the local truncation error at (xj, tn) represents, up to a factor 1/Δt, the
error that would be generated by forcing the exact solution to satisfy that
speciﬁc numerical scheme. In particular for the forward Euler/centered
method it is deﬁned as follows
τn
j = u(xj, tn+1) −u(xj, tn)
Δt
+ au(xj+1, tn) −u(xj−1, tn)
2Δx
,
while the (global) truncation error is deﬁned as
τ(Δt, Δx) = max
j,n |τ n
j |.
When τ(Δt, Δx) goes to zero as Δt and Δx tend to zero independently,
the numerical scheme is said to be consistent.
More in general, we say that a numerical method is of order p in
time and of order q in space (for suitable positive values p and q) if, for
a suﬃciently smooth solution of the exact problem,
τ(Δt, Δx) = O(Δtp + Δxq).
Finally, we say that a numerical scheme is convergent (in the maximum
norm) if
lim
Δt,Δx→0max
j,n |u(xj, tn) −un
j | = 0.
If the exact solution is regular enough, using Taylor’s expansion con-
veniently, we can characterize the truncation error of the methods previ-
ously introduced. For the forward (or backward) Euler/centered method
it is O(Δt + Δx2). For the other methods, see Table 9.1.
As of stability, we say that a numerical scheme for the approximation
of a hyperbolic (either linear or nonlinear) problem is stable if, for any
time T , there exist two constants CT > 0 (possibily depending on T )
and δ0 > 0, such that
∥un∥Δ ≤CT ∥u0∥Δ,
(9.63)
for any n such that nΔt ≤T and for any Δt, Δx such that 0 < Δt ≤δ0,
0 < Δx ≤δ0. The symbol ∥· ∥Δ stands for a suitable discrete norm,
there are three instances:
∥v∥Δ,p =
⎛
⎝Δx
∞

j=−∞
|vj|p
⎞
⎠
1
p
for p = 1, 2,
∥v∥Δ,∞= sup
j
|vj|. (9.64)
Courant, Friedrichs and Lewy [CFL28] have proved that a necessary and
suﬃcient condition for any explicit scheme of the form (9.52) to be stable

9.3 Hyperbolic equations: a scalar pure advection problem
361
is that the time and space discretization steps must obey the following
condition
|aλ| ≤1, i.e. Δt ≤Δx
|a|
(9.65)
which is known as the CFL condition. The adimensional number aλ (a
is a velocity) is commonly referred to as the CFL number. If a is not
constant the CFL condition becomes
Δt ≤
Δx
sup
x∈R, t>0
|a(x, t)|.
It is possible to prove that
1. the forward Euler/centered method (9.53) is unconditionally unsta-
ble, i.e. it is unstable for any possible choice of Δx > 0 and Δt > 0;
2. the upwind method (also called forward Euler/decentered method)
(9.59) is conditionally stable with respect to the ∥· ∥Δ,1 norm, i.e.
∥un∥Δ,1 ≤∥u0∥Δ,1
∀n ≥0,
provided that the CFL condition (9.65) is satisﬁed; the same result
can be proved also for both Lax-Friedrichs (9.55) and Lax-Wendroﬀ
(9.57) schemes;
3. the backward Euler/centered method (9.62) is unconditionally stable
with respect to the ∥· ∥Δ,2 norm, i.e., for any Δt > 0
∥un∥Δ,2 ≤∥u0∥Δ,2
∀n ≥0.
See Exercise 9.11.
For a proof of the these results see, e.g., [QSS07, Chap. 13] and [Qua13,
Chap. 12].
We want now to mention two important features of a numerical
scheme: dissipation and dispersion. To this aim, let us suppose that the
initial datum u0(x) of problem (9.47) is 2π−periodic so that it can be
expanded in a Fourier series as
u0(x) =
∞

k=−∞
αkeikx,
where
αk = 1
2π
 2π
0
u0(x)e−ikxdx
is the k−th Fourier coeﬃcient of u0(x). The exact solution u of problem
(9.47) satisﬁes (formally) the nodal conditions

362
9 Numerical approximation of boundary-valueproblems
u(xj, tn) =
∞

k=−∞
αkeikjΔx(gk)n,
j ∈Z, n ∈N
(9.66)
with gk = e−iakΔt, while the numerical solution un
j , computed by one of
the schemes introduced in Section 9.3.1, reads
un
j =
∞

k=−∞
αkeikjΔx(γk)n,
j ∈Z,
n ∈N.
(9.67)
The form of coeﬃcients γk ∈C depends on the particular numeri-
cal scheme used; for instance, for the scheme (9.53) we can show that
γk = 1 −aλi sin(kΔx).
We notice that, while |gk| = 1 for any k ∈Z, the values |γk| depend on the
CFL number aλ, and then also on the chosen discretization. Precisely,
by choosing ∥·∥Δ = ∥·∥Δ,2, one can prove that a necessary and suﬃcient
condition for a given numerical scheme to satisfy the stability inequality
(9.63) is that |γk| ≤1, ∀k ∈Z. The ratio ϵa(k) = |γk|/|gk| = |γk| is the
so-called dissipation coeﬃcient (or ampliﬁcation coeﬃcient) of the k−th
harmonic associated with the numerical scheme. We recall that the exact
solution of (9.47) is the travelling wave u(x, t) = u0(x −at) whose am-
plitude is independent of time; as of its numerical approximation (9.67),
the smaller ϵa(k), the higher the reduction of the wave amplitude and,
whence the higher the numerical dissipation. Moreover, if the stability
condition is violated, then the wave amplitude will increase and a blow-
up of the numerical solution will occur at suﬃciently large times.
Besides dissipation, numerical schemes introduce also dispersion, that
is either a delay or an advance in the wave propagation. To understand
this phenomenon we write gk and γk as follows:
gk = e−iaλφk,
γk = |γk|e−iωΔt = |γk|e−i ω
k λφk,
φk = kΔx being the so-called phase angle associated to the k−th har-
monic.
By comparing gk with γk and recalling that a is the propagation veloc-
ity of the “exact” wave, we deﬁne dispersion coeﬃcient associated to the
kth harmonic the value ϵd(k) =
ω
ak =
ωΔt
φkaλ.
In Figures 9.12 and 9.13 we report the exact solution of problem
(9.50) (for a = 1) and the numerical solutions obtained by some of the
schemes presented in Section 9.3.1. The initial datum is
u0(x) =
 sin(2πx/ℓ)
−1 ≤x ≤ℓ
0
ℓ< x < 3,
(9.68)
of wavelength ℓ= 1 (left) and ℓ= 1/2 (right). In both cases the CFL
number is equal to 0.8. For ℓ= 1 we have chosen Δx = ℓ/20 = 1/20, so

9.3 Hyperbolic equations: a scalar pure advection problem
363
−0.5
0
0.5
1
1.5
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Lax−Friedrichs   CFL=0.8,   φk=π/4,   t=0.4
x
u
−0.5
0
0.5
1
1.5
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Lax−Wendroff   CFL=0.8,   φk=π/4,   t=0.4
x
u
−0.5
0
0.5
1
1.5
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Upwind   CFL=0.8,   φk=π/4,   t=0.4
x
u
−0.5
0
0.5
1
1.5
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
backward Euler   CFL=0.8,   φk=π/4,   t=0.4
x
u
Figure 9.12. Exact solution (dashed line) and numerical solution (solid line)
of problem (9.50) at t = 0.4, with a = 1 and initial datum given by (9.68) with
equal wavelength ℓ= 1/2
that φk = 2πΔx/ℓ= π/10 and Δt = 1/25. For ℓ= 1/2 we have chosen
Δx = ℓ/8 = 1/16, so that φk = π/4 and Δt = 1/20.
In Figures 9.14 and 9.15 we display the dissipation and dispersion
coeﬃcients, respectively, versus the CFL number (at top) and the phase
angle φk = kΔx (at bottom).
Notice from Figure 9.14 that, when CFL=0.8, the Lax-Wendroﬀscheme
is the least dissipative one, this information is conﬁrmed by the numerical
solutions shown in Figure 9.13, for both φk = π/10 and φk = π/4. About
the dispersion error, still for CFL=0.8, from Figure 9.15 it emerges that
the upwind scheme features the lowest dispersion and shows a light phase
advance; the Lax-Friederichs scheme has a considerable phase advance,
while both Lax-Wendroﬀand implicit Euler/centered schemes show a
phase delay. These conclusions are conﬁrmed by the numerical solution
shown in Figure 9.12.
Notice that the dissipation coeﬃcient is responsible for the damping
of the wave amplitude, while the dispersion coeﬃcient is responsible for
the inexact propagation velocity.

364
9 Numerical approximation of boundary-valueproblems
−1
−0.5
0
0.5
1
1.5
2
2.5
3
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Lax−Friedrichs   CFL=0.8,   φk=π/10,   t=1
x
u
−1
−0.5
0
0.5
1
1.5
2
2.5
3
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Lax−Friedrichs   CFL=0.8,   φk=π/4,   t=1
x
u
−1
−0.5
0
0.5
1
1.5
2
2.5
3
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Lax−Wendroff   CFL=0.8,   φk=π/10,   t=1
x
u
−1
−0.5
0
0.5
1
1.5
2
2.5
3
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Lax−Wendroff   CFL=0.8,   φk=π/4,   t=1
x
u
−1
−0.5
0
0.5
1
1.5
2
2.5
3
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Upwind   CFL=0.8,   φk=π/10,   t=1
x
u
−1
−0.5
0
0.5
1
1.5
2
2.5
3
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Upwind   CFL=0.8,   φk=π/4,   t=1
x
u
−1
−0.5
0
0.5
1
1.5
2
2.5
3
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
backward Euler   CFL=0.8,   φk=π/10,   t=1
x
u
−1
−0.5
0
0.5
1
1.5
2
2.5
3
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
backward Euler   CFL=0.8,   φk=π/4,   t=1
x
u
Figure 9.13. Exact solution (dashed line) and numerical solution (solid line)
at t = 1 of problem (9.50) with a = 1 and initial datum given by (9.68) with
wavelength ℓ= 1 (left) and ℓ= 1/2 (right)

9.3 Hyperbolic equations: a scalar pure advection problem
365
0
0.2
0.4
0.6
0.8
1
0.95
0.955
0.96
0.965
0.97
0.975
0.98
0.985
0.99
0.995
1
 
 
Lax−Fr
Lax−We
Upwind
back Euler
ϵa(k)
φk = π/10
CFL
0
0.2
0.4
0.6
0.8
1
0.7
0.75
0.8
0.85
0.9
0.95
1
 
 
Lax−Fr
Lax−We
Upwind
back Euler
ϵa(k)
φk = π/4
CFL
0
0.5
1
1.5
2
2.5
3
3.5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 
 
Lax−Fr
Lax−We
Upwind
back Euler
ϵa(k)
φk = kΔx
CFL=0.5
0
0.5
1
1.5
2
2.5
3
3.5
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 
 
Lax−Fr
Lax−We
Upwind
back Euler
ϵa(k)
φk = kΔx
CFL=0.8
Figure 9.14. Dissipation coeﬃcients
0
0.2
0.4
0.6
0.8
1
0.95
0.96
0.97
0.98
0.99
1
1.01
1.02
1.03
1.04
 
 
Lax−Fr
Lax−We
Upwind
back Euler
ϵd(k)
φk = π/10
CFL
0
0.2
0.4
0.6
0.8
1
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
 
 
Lax−Fr
Lax−We
Upwind
back Euler
ϵd(k)
φk = π/4
CFL
0
0.5
1
1.5
2
2.5
3
3.5
0
0.5
1
1.5
2
2.5
 
 
Lax−Fr
Lax−We
Upwind
back Euler
ϵd(k)
φk = kΔx
CFL=0.5
0
0.5
1
1.5
2
2.5
3
3.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
 
 
Lax−Fr
Lax−We
Upwind
back Euler
ϵd(k)
φk = kΔx
CFL=0.8
Figure 9.15. Dispersion coeﬃcients

366
9 Numerical approximation of boundary-valueproblems
9.3.3 Finite element space discretization of the scalar
advection equation
Following Section 9.2.3, a Galerkin semi-discrete approximation of prob-
lem (9.47) can be introduced as follows. Let us assume that a = a(x) > 0
∀x ∈[α, β], so that the node x = α coincides with the inﬂow boundary.
For any t > 0, we complete system (9.47) with the boundary condition
u(α, t) = ϕ(t),
t > 0,
(9.69)
where ϕ is a given function of t.
After deﬁning the space
V in
h
= {vh ∈Vh : vh(α) = 0},
we consider the following ﬁnite element approximation of problem (9.47),
(9.69): for any t ∈(0, T ) ﬁnd uh(t) ∈Vh such that
⎧
⎪
⎪
⎨
⎪
⎪
⎩
β

α
∂uh(t)
∂t
vh dx +
β

α
a∂uh(t)
∂x
vh dx = 0
∀vh ∈V in
h ,
uh(t) = ϕ(t)
at x = α,
(9.70)
with uh(0) = u0
h ∈Vh being a suitable ﬁnite element approximation of
the initial datum u0, e.g. its piecewise polynomial interpolant.
The time discretization of (9.70) can be accomplished still by using
ﬁnite diﬀerence schemes. If, for instance, we use the backward Euler
method, for any n ≥0, we have: ﬁnd un+1
h
∈Vh such that
1
Δt
β

α
(un+1
h
−un
h)vh dx +
β

α
a∂un+1
h
∂x
vh dx = 0
∀vh ∈V in
h , (9.71)
with un+1
h
(α) = ϕn+1.
If ϕ = 0, we can conclude that
∥un
h∥L2(α,β) ≤∥u0
h∥L2(α,β)
∀n ≥0,
which means that the backward Euler scheme is unconditionally stable
with respect to the norm ∥v∥L2(α,β) =
 β
α v2(x)dx
1/2
.
See Exercises 9.10-9.14.

9.4 The wave equation
367
9.4 The wave equation
We consider now the following second-order hyperbolic equation in one
dimension
∂2u
∂t2 −c∂2u
∂x2 = f
(9.72)
where c is a given positive constant.
When f = 0, the general solution of (9.72) is the so-called d’Alembert
travelling-wave
u(x, t) = ψ1(√ct −x) + ψ2(√ct + x),
(9.73)
for arbitrary functions ψ1 and ψ2.
In what follows we consider problem (9.72) for x ∈(a, b) and t > 0,
therefore we need to complete the diﬀerential equation with the initial
data
u(x, 0) = u0(x) and ∂u
∂t (x, 0) = v0(x), x ∈(a, b),
(9.74)
and the boundary data
u(a, t) = 0 and u(b, t) = 0, t > 0.
(9.75)
In this case, u may represent the transverse displacement of an elastic
vibrating string of length b−a, ﬁxed at the endpoints, and c is a positive
coeﬃcient depending on the speciﬁc mass of the string and on its tension.
The string is subjected to a vertical force of density f. The functions
u0(x) and v0(x) denote respectively the initial displacement and the
initial velocity of the string.
The change of variables
ω1 = ∂u
∂x,
ω2 = ∂u
∂t ,
transforms (9.72) into the ﬁrst-order system
∂ω
∂t + A∂ω
∂x = f,
x ∈(a, b), t > 0
(9.76)
where
ω =
.
ω1
ω2
/
, A =
.
0 −1
−c 0
/
, f =
.
0
f
/
,
and the initial conditions are ω1(x, 0) = u′
0(x) and ω2(x, 0) = v0(x) for
x ∈(a, b).

368
9 Numerical approximation of boundary-valueproblems
In general, we can consider systems of the form (9.76) where ω, f :
R×[0, ∞) →Rp are two given vector functions and A ∈Rp×p is a matrix
with constant coeﬃcients. This system is said hyperbolic if A is diago-
nalizable and has real eigenvalues, that is, if there exists a nonsingular
matrix T ∈Rp×p such that
A = TΛT−1,
where Λ = diag(λ1, ..., λp) is the diagonal matrix of the real eigenvalues
of A, while T = (v1, v2, . . . , vp) is the matrix whose column vectors are
the right eigenvectors of A. Thus
Avk = λkvk,
k = 1, . . . , p.
Introducing the characteristic variables w = T−1ω, system (9.76) be-
comes
∂w
∂t + Λ∂w
∂x = g,
where g = T−1f. This is a system of p independent scalar equations of
the form
∂wk
∂t + λk
∂wk
∂x = gk,
k = 1, . . . , p.
When gk = 0, its solution is given by wk(x, t) = wk(x −λkt, 0), k =
1, . . . , p. Therefore the solution ω = Tw of problem (9.76) (for f = 0)
can be written as
ω(x, t) =
p

k=1
wk(x −λkt, 0)vk.
The curve (xk(t), t) in the plane (x, t) that satisﬁes x′
k(t) = λk is the kth
characteristic curve (see Section 9.3) and wk is constant along it. Then
ω(x, t) depends only on the initial datum at the points x −λkt. For this
reason, the set of p points that form the feet of the characteristics issuing
from the point (x, t),
D(t, x) = {x ∈R : x = x −λkt , k = 1, ..., p},
(9.77)
is called the domain of dependence of the solution ω(x, t).
If (9.76) is set on a bounded interval (a, b) instead of on the whole real
line, the inﬂow point for each characteristic variable wk is determined
by the sign of λk. Correspondingly, the number of positive eigenvalues
determines the number of boundary conditions that should be assigned
at x = a, whereas at x = b the number of conditions that must be as-
signed equals the number of negative eigenvalues.

9.4 The wave equation
369
Example 9.6 System (9.76) is hyperbolic since A is diagonalizable with ma-
trix
T =
⎡
⎣−1
√c
1
√c
1
1
⎤
⎦
and features two distinct real eigenvalues ±√c (representing the propagation
velocities of the wave). Moreover, one boundary condition needs to be pre-
scribed at every end-point, as in (9.75).
■
9.4.1 Finite diﬀerence approximation of the wave
equation
To discretize in time equation (9.72) we can use the Newmark method
formerly proposed in Chapter 8 for second-order ordinary diﬀerential
equations, see (8.71). Still denoting by Δt the (uniform) time-step and
using in space the classical ﬁnite diﬀerence method on a grid with
nodes xj = x0 + jΔx, j = 0, . . . , N + 1, x0 = a and xN+1 = b,
the Newmark scheme for (9.72) reads as follows: for any n ≥1 ﬁnd
{un
j , vn
j , j = 1, . . . , N} such that
un+1
j
= un
j + Δtvn
j
+Δt2 
ζ(cwn+1
j
+ f(xj, tn+1)) + (1/2 −ζ)(cwn
j + f(xj, tn))

,
vn+1
j
= vn
j + Δt

(1 −θ)(cwn
j + f(xj, tn)) + θ(cwn+1
j
+ f(xj, tn+1))

,
(9.78)
with u0
j = u0(xj) and v0
j = v0(xj) and wk
j = (uk
j+1 −2uk
j + uk
j−1)/(Δx)2
for k = n or k = n + 1. System (9.78) must be completed by imposing
the boundary conditions (9.75).
The Newmark method is implemented in Program 9.4. The input
parameters are the vectors xspan=[a,b] and tspan=[0,T], the number
of discretization intervals in space (nstep(1)) and in time (nstep(2)),
the scalar c (corresponding to the positive constant c), the function
handles u0 and v0 associated with the initial data u0(x) and v0(x),
respectively, and the function handles g and fun associated with the
functions g(x, t) and f(x, t), respectively. Finally, the vector param allows
to specify the values of the coeﬃcients (param(1)=θ, param(2)=ζ). This
method is second order accurate with respect to Δt if θ = 1/2, whereas
it is ﬁrst order if θ ̸= 1/2. Moreover, the condition θ ≥1/2 is necessary
to ensure stability (see Section 8.9).
Program 9.4. newmarkwave: Newmark method for the wave equation
function [xh ,uh]= newmarkwave (xspan ,tspan ,nstep ,param ,...
c,u0 ,v0 ,g,f,varargin )
% NEWMARKWAVE
solves the
wave
equation
with
the
Newmark

370
9 Numerical approximation of boundary-valueproblems
% method.
% [XH ,UH]= NEWMARKWAVE (XSPAN ,TSPAN ,NSTEP ,PARAM ,C ,...
% U0 ,V0 ,G,F)
% solves the
wave
equation D^2 U/DT^2 - C D^2U/DX^2 = F
% in (XSPAN (1), XSPAN (2)) X (TSPAN(1), TSPAN (2))
using
% Newmark
method
with
initial
conditions
U(X ,0)= U0(X),
% DU/DX(X,0)= V0(X) and Dirichlet
boundary
conditions
% U(X,T)=G(X,T) for X=XSPAN(1) and X=XSPAN (2). C is a
% positive
constant .
% NSTEP (1) is the
number of space integration
intervals
% NSTEP (2) is the
number of time -integration
intervals .
% PARAM (1)= ZETA
and
PARAM (2)= THETA.
% U0(X), V0(X), G(X,T) and F(x,T) are function
handles.
% XH contains
the nodes of the
discretization .
% UH contains
the
numerical
solutions
at time
TSPAN (2).
% [XH ,UH]= NEWMARKWAVE (XSPAN ,TSPAN ,NSTEP ,PARAM ,C ,...
% U0 ,V0 ,G,F,P1 ,P2 ,...) passes the additional
parameters
%
P1 ,P2 ,... to the
functions
U0 ,V0 ,G,F.
h
= (xspan (2)- xspan (1))/ nstep (1);
dt = (tspan (2)- tspan (1))/ nstep (2);
zeta = param (1);
theta = param (2);
N = nstep (1)+1;
e = ones (N ,1); D = spdiags ([e
-2*e e],[-1,0,1], N,N);
I = speye(N); lambda = dt/h;
A = I-c*lambda ^2* zeta *D;
An = I+c*lambda ^2*(0.5 - zeta )*D;
A(1,:) = 0; A(1,1) = 1; A(N,:) = 0; A(N,N) = 1;
xh = (linspace (xspan(1), xspan(2),N))’;
fn = f(xh ,tspan(1), varargin {:});
un = u0(xh ,varargin {:});
vn = v0(xh ,varargin {:});
[L,U]=lu(A);
alpha = dt ^2* zeta ; beta = dt ^2*(0.5 - zeta );
theta1 = 1-theta;
for t = tspan (1)+ dt:dt:tspan(2)
fn1 = f(xh ,t,varargin {:});
rhs = An*un+dt*I*vn+alpha*fn1+beta *fn;
temp = g([ xspan(1), xspan(2)],t,varargin {:});
rhs([1,N]) = temp ;
uh = L\rhs;
uh = U\uh;
v = vn + dt*((1- theta)*(c*D*un/h^2+ fn )+...
theta*(c*D*uh/h^2+ fn1 ));
fn = fn1;
un = uh;
vn = v;
end
An alternative to the Newmark method is provided by the following
Leap-Frog method
un+1
j
−2un
j + un−1
j
= c
 Δt
Δx
2
(un
j+1 −2un
j + un
j−1),
(9.79)
which is obtained by discretizing both time and space derivatives by the
centered ﬁnite diﬀerence formula (9.12).
Both Newmark (9.78) and Leap-Frog (9.79) schemes are second or-
der accurate with respect to Δt and Δx. About stability, the Leap-Frog
method is stable provided that the CFL condition Δt ≤Δx/√c is

9.4 The wave equation
371
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
Figure 9.16. Comparison between the solutions obtained using the Newmark
method for a discretization with Δx = 0.04 and Δt = 0.15 (dashed line),
Δt = 0.075 (solid line) and Δt = 0.0375 (dashed-dotted line)
satisﬁed, while the Newmark method is unconditionally stable if 2ζ ≥
θ ≥1
2 (see [Joh90]).
Example 9.7 Using Program 9.4 we study the evolution of the initial con-
dition u0(x) = e−10x2 for x ∈(−2, 2), by putting f = 0 and c = 1 in (9.72).
We assume v0 = 0 and homogeneous Dirichlet boundary conditions. In Figure
9.16 we compare the solutions obtained at time t = 3 using Δx = 0.04 and
time-steps Δt = 0.15 (dashed line), Δt = 0.075 (solid line) and Δt = 0.0375
(dashed-dotted line). The parameters of the Newmark method are θ = 1/2 and
ζ = 0.25, and they ensure a second order unconditionally stable method.
■
Example 9.8 (Communications) In this example we use the equation
(9.9) to model the way a telegraph wire transmits a pulse of voltage. The
equation is a combination of diﬀusion and wave equations, and accounts for
eﬀects of ﬁnite velocity in a standard mass transport equation. In Figure 9.17
we compare the evolution of one bump (precisely a cubic B-spline (see [QSS07,
Sect. 8.7.2])) centered in x = 3 and non-null in the interval (1,5) using the wave
equation (9.72) (dashed line) and the telegrapher’s equation (9.9) (solid line),
on the interval (0, 10) with c = 1, α = 0.5 and β = 0.04. The initial speed
is chosen to be v0(x) = −cu′
0(x) (v0(x) = −cu′
0(x) −α/2u0(x), resp.) for the
wave (telegrapher’s, resp.) equation, so that the bump travels with speed c.
We have solved both the wave equation and telegrapher’s equation by the
Newmark scheme using Δx = 0.025, time-step Δt = 0.1, ζ = 1/4 and θ = 1/2.
To approximate the wave equation we have called Program 9.4, while to solve
the telegrapher’s equation we have written a diﬀerent program implementing
the Newmark scheme (8.71) applied to equation (9.9). The presence of the
dissipation eﬀect is evident in the solution of the telegrapher’s equation.
■
An alternative approach consists of discretizing the ﬁrst-order system
(9.76) instead of the (equivalent) second order scalar equation (9.72).
When f = 0, Lax-Wendroﬀand upwind schemes for the hyperbolic sys-
tem (9.76) are deﬁned as follows:

372
9 Numerical approximation of boundary-valueproblems
0
2
4
6
8
10
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
t = 0sec
t = 2sec
0
2
4
6
8
10
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
t = 6sec
t = 10sec
Figure 9.17. Propagation of a pulse of voltage using the wave equation
(dashed line) and the telegrapher’s equation (solid line). At left, the thin solid
line represents the initial condition u0(x)
1. Lax-Wendroﬀmethod
ωn+1
j
= ωn
j −λ
2 A(ωn
j+1 −ωn
j−1)
+λ2
2 A2(ωn
j+1 −2ωn
j + ωn
j−1),
(9.80)
2. upwind (or forward Euler/decentered) method
ωn+1
j
= ωn
j −λ
2 A(ωn
j+1 −ωn
j−1)
+λ
2 |A|(ωn
j+1 −2ωn
j + ωn
j−1),
(9.81)
where |A| = T|Λ|T−1 and |Λ| is the diagonal matrix of the moduli
of the eigenvalues of A.
The Lax-Wendroﬀmethod is second order accurate (in both time
and space), while the upwind scheme is ﬁrst order.
About stability, all considerations made in Section 9.3.1 are still valid,
provided the CFL condition (9.65) is replaced by
Δt < Δx
ρ(A).
(9.82)
As usual, ρ(A) denotes the spectral radius of A.
For the proof of these results see, e.g., [QV94], [LeV02], [GR96],
[QSS07, Chapter 13].
See Exercises 9.8-9.9.

9.5 What we haven’t told you
373
Let us summarize
1. One-dimensional boundary value problems are set up on an interval;
boundary conditions on the solution (or on its derivative) must be
prescribed at the endpoints of the interval;
2. numerical approximation can be carried out by ﬁnite-diﬀerences
(arising from truncated Taylor series) or by ﬁnite-elements (aris-
ing from the weak formulation of the diﬀerential problem; in this
context, both test and trial functions are piecewise polynomials);
3. multidimensional problems can be faced by using similar arguments.
For two-dimensional boundary-value problems, ﬁnite element ap-
proximations make use of piecewise polynomials, where “piecewise”
refers to either triangles or quadrilaterals of a grid partitioning the
spatial domain;
4. matrices arising from both ﬁnite element and ﬁnite diﬀerence dis-
cretizations are sparse and ill-conditioned;
5. initial-boundary-value problems contain time derivatives of the so-
lution which are discretized by ﬁnite diﬀerence formulas, of either
explicit or implicit type;
6. when explicit schemes are used, stability conditions have to be satis-
ﬁed: the time-step turns out to be bounded by the spatial grid size.
On the other hand, when implicit schemes are used, a linear alge-
braic system (similar to that obtained for stationary problems) has
to be solved at each time level;
7. in this Chapter we have presented some simple linear problems of
elliptic, parabolic and hyperbolic type. For a more exhaustive treat-
ment of this subject we suggest the reader to refer to the bibliography
presented in the next Section.
9.5 What we haven’t told you
We could simply say that we have told you almost nothing, since the ﬁeld
of numerical analysis which is devoted to the numerical approximation
of partial diﬀerential equations is so broad and multifaceted to deserve
an entire monograph simply for addressing the most essential concepts
(see, e.g., [TW98], [EEHJ96]).
We would like to mention that the ﬁnite element method is nowadays
probably the most widely diﬀused method for the numerical solution of
partial diﬀerential equations (see, e.g., [Qua13], [QV94], [Bra97], [BS01]).
As already mentioned the MATLAB toolbox pde allows the solution of a
broad family of partial diﬀerential equations by the linear ﬁnite element
method, in particular for the discretization of space variables.

374
9 Numerical approximation of boundary-valueproblems
Other popular techniques are the spectral methods (see, e.g., [CHQZ06],
[CHQZ07], [Fun92], [BM92], [KS99]) and the ﬁnite volume method (see,
e.g., [Kr¨o98], [Hir88] and [LeV02]).
Octave 9.1 The Octave-Forge package bim oﬀers most of the main func-
tionalities of the pde toolbox, although its syntax is in general not com-
patible with that of MATLAB.
■
9.6 Exercises
Exercise 9.1 Verify that matrix (9.15) is positive deﬁnite.
Exercise 9.2 Verify that the eigenvalues of the matrix A∈RN×N, deﬁned in
(9.15), are
λj = 2(1 −cos(jθ)),
j = 1, . . . , N,
while the corresponding eigenvectors are
qj = (sin(jθ), sin(2jθ), . . . , sin(Njθ))T ,
where θ = π/(N + 1). Deduce that K(A) is proportional to h−2.
Exercise 9.3 Prove that the quantity (9.12) provides a second order approx-
imation of u′′(¯x) with respect to h.
Exercise 9.4 Compute the matrix and the right-hand side of the numerical
scheme that we have proposed to approximate problem (9.17).
Exercise 9.5 Use the ﬁnite diﬀerence method to approximate the boundary-
value problem
⎧
⎨
⎩
−u′′ + k
T u = w
T in (0, 1),
u(0) = u(1) = 0,
where u = u(x) represents the vertical displacement of a string of length
1, subject to a transverse load of intensity w(x) per unit length. T is the
tension and k is the elastic coeﬃcient of the string. For the case in which
w(x) = 1 + sin(4πx), T = 1 and k = 0.1, compute the solution corresponding
to h = 1/i, with i = 10, 20, 40, and deduce the order of accuracy of the method.
Exercise 9.6 Use the ﬁnite diﬀerence method to solve problem (9.17) in the
case where the following boundary conditions are prescribed at the endpoints
(called Neumann boundary conditions)
u′(a) = α,
u′(b) = β.
Use the formulae given in (4.11) to discretize u′(a) and u′(b).

9.6 Exercises
375
Exercise 9.7 Verify that, when using a uniform grid, the right-hand side of
the system (9.14) associated with the centered ﬁnite diﬀerence scheme coin-
cides, up a factor h, with that of the ﬁnite element scheme (9.27) provided
that the composite trapezoidal formula is used to compute the integrals on
the elements Ij−1 and Ij.
Exercise 9.8 Verify that div∇φ = Δφ, where ∇is the gradient operator
that associates to a function u the vector whose components are the ﬁrst
order partial derivatives of u.
Exercise 9.9 (Thermodynamics) Consider a square plate whose side
length is 20 cm and whose thermal conductivity is k = 0.2 cal/(sec·cm·C).
Denote by Q = 5 cal/(cm3·sec) the heat production rate per unit area. The
temperature T = T (x, y) of the plate satisﬁes the equation −ΔT = Q/k. As-
suming that T is null on three sides of the plate and is equal to 1 on the fourth
side, determine the temperature T at the center of the plate.
Exercise 9.10 Verify that the solution of problem (9.72), (9.74) – (9.75) (with
f = 0) satisﬁes the identity
 b
a
(ut(x, t))2dx + c
 b
a
(ux(x, t))2dx =
(9.83)
 b
a
(v0(x))2dx + c
 b
a
(u0,x(x))2dx,
provided that u0(a) = u0(b) = 0.
Exercise 9.11 Prove that the numerical solution provided by the backward
Euler/centered scheme (9.62) is unconditionally stable, that is ∀Δt > 0,
∥un∥Δ,2 ≤∥u0∥Δ,2
∀n ≥0.
(9.84)
Exercise 9.12 Prove that the solution provided by the upwind scheme (9.59)
satisﬁes the estimate
∥un∥Δ,∞≤∥u0∥Δ,∞
∀n ≥0,
(9.85)
provided that the CFL condition has been veriﬁed. The inequality (9.85) is
named discrete maximum principle.
Exercise 9.13 Solve problem (9.47) with a = 1, x ∈(0, 0.5), t ∈(0, 1),
initial datum u0(x) = 2 cos(4πx)+sin(20πx) and boundary condition u(0, t) =
2 cos(4πt)−sin(20πt) for t ∈(0, 1). Use both Lax-Wendroﬀ(9.57) and upwind
(9.59) schemes. Set the CFL number equal to 0.5. Verify experimentally that
the Lax-Wendroﬀscheme is second-order accurate with respect to Δx and Δt,
while the upwind scheme is ﬁrst-order accurate. To evaluate the error use the
norm ∥· ∥Δ,2.

376
9 Numerical approximation of boundary-valueproblems
0
0.1
0.2
0.3
0.4
0.5
−3
−2
−1
0
1
2
3
 
 
Lax−We
Upwind
exact sol
x
u
Figure 9.18. Numerical solutions at time t = 5 for the problem (9.47) by
using data of Exercise 9.13. The CFL number is 0.8
Exercise 9.14 In Figure 9.18 both exact and numerical solutions of problem
(9.47) at time t = 5 are shown. The latter are computed by the Lax-Wendroﬀ
(9.57) and upwind (9.59) schemes, using the same data of Exercise 9.13. By
knowing that the CFL number is 0.8 and that we have used Δt = 5.e −3,
comment on the dissipation and dispersion coeﬃcients that we have obtained.

10
Solutions of the exercises
In this chapter we will provide solutions of the exercises that we have
proposed at the end of the previous eight chapters. The expression “So-
lution n.m” is an abridged notation for “Solution of Exercise n.m” (mth
Exercise of the nth Chapter).
10.1 Chapter 1
Exercise 1.1 Only the numbers of the form ±0.1a2 · 2e with a2 = 0, 1 and
e = ±2, ±1, 0 belong to the set F(2, 2, −2, 2). For a given exponent, we can
represent in this set only the two numbers 0.10 and 0.11, and their opposites.
Consequently, the number of elements belonging to F(2, 2, −2, 2) is 20. Finally,
ϵM = 1/2.
Exercise 1.2 For any ﬁxed exponent, each of the digits a2, . . . , at can assume
β diﬀerent values, while a1 can assume only β−1 values. Therefore 2(β−1)βt−1
diﬀerent numbers can be represented (the 2 accounts for the positive and
negative sign). On the other hand, the exponent can assume U −L + 1 values.
Thus, the set F(β, t, L, U) contains 2(β −1)βt−1(U −L+ 1) diﬀerent elements.
Exercise 1.3 Thanks to the Euler formula i = eiπ/2; we obtain ii = e−π/2,
that is, a real number. In MATLAB
exp(-pi /2)
ans =
0.2079
i^i
ans =
0.2079
Exercise 1.4 Use the instructions U=2*eye(10)-3*diag(ones(8,1),2) and
L=2*eye(10)-3*diag(ones(8,1),-2).
A. Quarteroni et al., Scientiﬁc Computing with MATLAB and Octave,
Texts in Computational Science and Engineering 2,
DOI 10.1007/978-3-642-45367-0 10, © Springer-Verlag Berlin Heidelberg 2014
377

378
10 Solutions of the exercises
Exercise 1.5 We can interchange the third and seventh rows of the previous
matrix using the instructions: r=[1:10]; r(3)=7; r(7)=3; Lr=L(r,:). Notice
that the character : in L(r,:) ensures that all columns of L are spanned in the
L(r,:)
usual increasing order (from the ﬁrst to the last). To interchange the fourth
column with the eighth column we can write c=[1:10]; c(8)=4; c(4)=8;
Lc=L(:,c). Similar instructions can be used for the upper triangular matrix.
Exercise 1.6 We can deﬁne the matrix A = [v1;v2;v3;v4] where v1, v2,
v3 and v4 are the 4 given row vectors. They are linearly independent iﬀthe
determinant of A is diﬀerent from 0, which is not true in our case.
Exercise 1.7 The two given functions f and g have the symbolic expression:
syms x
f=sqrt (x^2+1); pretty(f)
(x2+1)1/2
g=sin(x^3)+ cosh (x);
pretty(g)
sin(x3) + cosh(x)
The command pretty(f) prints the symbolic expression f in a format that
pretty
resembles type-set mathematics. At this stage, the symbolic expression of the
ﬁrst and second derivatives and the integral of f can be obtained with the
following instructions:
diff (f,x)
ans =
1/(x^2+1)^(1/2)* x
diff (f,x,2)
ans =
-1/( x^2+1)^(3/2)* x^2+1/(x ^2+1)^(1/2)
int(f,x)
ans =
1/2* x*(x^2+1)^(1/2)+1/2* asinh(x)
Similar instructions can be used for the function g.
Exercise 1.8 The accuracy of the computed roots downgrades as the polyno-
mial degree increases. This experiment reveals that the accurate computation
of the roots of a polynomial of high degree can be troublesome.
Exercise 1.9 Here is a possible program to compute the sequence:
function I=sequence (n)
I = zeros(n+2 ,1); I(1) = (exp (1) -1)/ exp (1);
for i = 0:n, I(i+2) = 1 - (i+1)* I(i+1);
end
The sequence computed by this program doesn’t tend to zero (as n increases),
but it diverges with alternating sign. This behavior is a direct consequence of
rounding errors propagation.
Exercise 1.10 The anomalous behavior of the computed sequence is due to
the propagation of roundoﬀerrors from the innermost operation. In particular,
when 41−nz2
n is less than ϵM/2, the subsequent element zn+1 of the sequence
is equal to 0. This happens for n ≥30.

10.1 Chapter 1
379
Exercise 1.11 The proposed method is a special instance of the Monte Carlo
method and is implemented by the following program:
function
mypi = pimontecarlo (n)
x = rand (n ,1); y = rand (n ,1);
z = x.^2+ y.^2;
v = (z <= 1);
m=sum(v); mypi =4*m/n;
The command rand generates a sequence of pseudo-random numbers. The
instruction v = (z <= 1) is a shortand version of the following procedure: we
check whether z(k) <= 1 for any component of the vector z. If the inequality
is satisﬁed for the kth component of z (that is, the point (x(k),y(k)) belongs
to the interior of the unit circle) v(k) is set equal to 1, and to 0 otherwise.
The command sum(v) computes the sum of all components of v, that is, the
sum
number of points falling in the interior of the unit circle.
By launching the program mypi=pimontecarlo(n) for diﬀerent values of
n, when n increases, the approximation mypi of π becomes more accurate.
For instance, for n=1000 we obtain mypi=3.1120, whilst for n=300000 we have
mypi=3.1406. (Obviously, since the numbers are randomly generated, the re-
sult obtained with the same value of n may change at each run.)
Exercise 1.12 To answer the question we can use the following function:
function
pig= bbpalgorithm (n)
pig = 0;
for m=0:n
m8 = 8*m;
pig = pig + (1/16)^ m*(4/(m8 +1) -(2/( m8 +4)+
...
1/( m8 +5)+1/( m8 +6)));
end
For n=10 we obtain an approximation pig of π that coincides (up to MATLAB
precision) with the persistent MATLAB variable pi. In fact, this algorithm is
extremely eﬃcient and allows the rapid computation of hundreds of signiﬁcant
digits of π.
Exercise 1.13 The binomial coeﬃcient can be computed by the following
program (see also the MATLAB function nchoosek):
nchoosek
function
bc=bincoeff (n,k)
k = fix(k); n = fix(n);
if k > n, disp(’k must be between
0 and n’);
return; end
if k > n/2, k = n-k; end
if k <= 1,
bc = n^k; else
num = (n-k+1): n; den = 1:k; el = num ./den;
bc = prod (el);
end
The command fix(k) rounds k to the nearest integer smaller than k. The
fix
command disp(string) displays the string, without printing its name. The
command return terminates the execution of the function. Finally, prod(el)
return
prod
computes the product of all elements of the vector el.
Exercise 1.14 The following functions compute fn using the form fi =
fi−1 + fi−2 (fibrec) or using the form (1.14) (fibmat):

380
10 Solutions of the exercises
function f=fibrec(n)
if n == 0
f = 0;
elseif n == 1
f = 1;
else
f = fibrec(n -1)+ fibrec(n -2);
end
function f=fibmat(n)
f = [0;1];
A = [1 1; 1 0];
f = A^n*f;
f = f(1);
For n=20 we obtain the following results:
t= cputime; fn=fibrec (20), cpu=cputime -t
fn =
6765
cpu =
0.48
t= cputime; fn=fibmat (20), cpu=cputime -t
fn =
6765
cpu =
0
The recursive function fibrec requires much more CPU time than fibmat.
The latter requires to compute only the power of a matrix, an easy operation
in MATLAB.
10.2 Chapter 2
Exercise 2.1 The command fplot allows us to study the graph of the given
function f for various values of γ. For γ = 1, the corresponding function does
not have real zeros. For γ = 2, there is only one zero, α = 0, with multiplicity
equal to four (that is, f(α) = f ′(α) = f ′′(α) = f ′′′(α) = 0, while f (4)(α) ̸= 0).
Finally, for γ = 3, f has two distinct zeros, one in the interval (−3, −1) and
the other one in (1, 3). In the case γ = 2, the bisection method cannot be
used since it is impossible to ﬁnd an interval (a, b) in which f(a)f(b) < 0.
For γ = 3, starting from the interval [a, b] = [−3, −1], the bisection method
(Program 2.1) converges in 34 iterations to the value α = −1.85792082914850
(with f(α) ≃−3.6 · 10−12), using the following instructions:
f=@(x) cosh (x)+cos(x)-3; a=-3; b=-1;
tol =1.e-10;
nmax =200;
[zero ,res ,niter]= bisection (f,a,b,tol ,nmax )
zero =
-1.8579
res =
-3.6872e-12
niter =
34

10.2 Chapter 2
381
Similarly, choosing a=1 and b=3, for γ = 3 the bisection method converges after
34 iterations to the value α = 1.8579208291485 with f(α) ≃−3.6877 · 10−12.
Exercise 2.2 We have to compute the zeros of the function f(V ) = pV +
aN 2/V −abN 3/V 2−pNb−kNT , where N is the number of molecules. Plotting
the graph of f, we see that this function has just a simple zero in the interval
(0.01, 0.06) with f(0.01) < 0 and f(0.06) > 0. We can compute this zero using
the bisection method as follows:
f=@(x) 35000000* x+401000./ x -17122.7./ x.^2 -1494500;
[zero ,res ,niter]= bisection (f ,0.01 ,0.06 ,1.e -12 ,100)
zero =
0.0427
res =
-6.3814e-05
niter =
35
Exercise 2.3 The unknown value of ω is the zero of the function f(ω) =
s(1, ω)−1 = 9.8[sinh(ω)−sin(ω)]/(2ω2)−1. From the graph of f we conclude
that f has a unique real zero in the interval (0.5, 1). Starting from this interval,
the bisection method computes the value ω = 0.61214447021484 with the
desired tolerance in 15 iterations as follows:
f=@(omega) 9.8/2*( sinh (omega)-sin(omega ))./ omega .^2 -1;
[zero ,res ,niter]= bisection (f,0.5 ,1 ,1.e -05 ,100)
zero =
6.1214e-01
res =
3.1051e-06
niter =
15
Exercise 2.4 The inequality (2.6) can be derived by observing that |e(k)| <
|I(k)|/2 with |I(k)| < 1
2|I(k−1)| < 2−k−1(b −a). Consequently, the error at the
iteration kmin is less than ε if kmin is such that 2−kmin−1(b −a) < ε, that is,
2−kmin−1 < ε/(b −a), which proves (2.6).
Exercise 2.5 The implemented formula is less sensitive to roundoﬀerrors.
Exercise 2.6 In Solution 2.1 we have analyzed the zeros of the given func-
tion with respect to diﬀerent values of γ. Let us consider the case when γ = 2.
Starting from the initial guess x(0) = 1, the Newton method (Program 2.2)
converges to the value ¯α = 1.4961e −4 in 31 iterations with tol=1.e-10 while
the exact zero of f is equal to 0. This discrepancy is due to the fact that f is al-
most a constant in a neighborhood of its zero, whence the root-ﬁnding is an ill-
conditioned problem (see the comment at the end of Sect. 2.8.2). The method
converges at the same solution and with the same number of iterations even if
we set tol=ϵM. Actually, the corresponding residual computed by MATLAB
is 0. Let us set now γ = 3. The Newton method with tol=ϵMconverges to

382
10 Solutions of the exercises
the value 1.85792082915020 in 9 iterations starting from x(0) = 1, while if
x(0) = −1 after 9 iterations it converges to the value −1.85792082915020 (in
both cases the residuals are zero in MATLAB).
Exercise 2.7 The square and the cube roots of a number a are the solutions
of the equations x2 = a and x3 = a, respectively. Thus, the corresponding
algorithms are: for a given x(0) compute
x(k+1) = 1
2
%
x(k) +
a
x(k)
&
, k ≥0
for the square root,
x(k+1) = 1
3

2x(k) +
a
(x(k))2

, k ≥0 for the cube root.
Exercise 2.8 Setting δx(k) = x(k) −α, from the Taylor expansion of f we
ﬁnd:
0 = f(α) = f(x(k)) −δx(k)f ′(x(k)) + 1
2(δx(k))2f ′′(x(k)) + O((δx(k))3). (10.1)
The Newton method yields
δx(k+1) = δx(k) −f(x(k))/f ′(x(k)).
(10.2)
Combining (10.1) with (10.2), we have
δx(k+1) = 1
2(δx(k))2 f ′′(x(k))
f ′(x(k)) + O((δx(k))3).
After division by (δx(k))2 and letting k →∞we prove the convergence result.
Exercise 2.9 For certain values of β the equation (2.2) can have two roots
that correspond to diﬀerent conﬁgurations of the rods system. The two initial
values that are suggested have been chosen conveniently to allow the Newton
method to converge toward one or the other root, respectively. We solve the
problem for β = kπ/150 with k = 0, . . . , 100 (if β > 2.6389 the Newton
method does not converge since the system has no admissible conﬁguration).
We use the following instructions to obtain the solution of the problem (shown
in Figure 10.1, left):
a1 =10; a2 =13;
a3 =8; a4 =10;
ss = (a1^2 + a2^2 - a3 ^2+ a4 ^2)/(2* a2*a4);
n=150; x01 = -0.1; x02 =2*pi /3; nmax =100;
beta =zeros (100 ,1);
for k=0:100
w = k*pi/n; i=k+1; beta (i) = w;
f
= @(x) 10/13* cos(w)-cos(x)-cos(w-x)+ss;
df = @(x) sin(x)-sin(w-x);
[zero ,res ,niter]= newton(f,df ,x01 ,1e-5, nmax );
alpha1(i) = zero ; niter1(i) = niter;
[zero ,res ,niter]= newton(f,df ,x02 ,1e-5, nmax );
alpha2(i) = zero ; niter2(i) = niter;
end
plot (beta ,alpha1 ,’c--’,beta ,alpha2 ,’c’,’Linewidth ’ ,2)
grid
on

10.2 Chapter 2
383
The components of the vectors alpha1 and alpha2 are the angles computed for
diﬀerent values of β, while the components of the vectors niter1 and niter2
are the number of Newton iterations (between 2 and 6) necessary to compute
the zeros with the requested tolerance.
Exercise 2.10 From an inspection of its graph we see that f has two positive
real zeros (α2 ≃1.5 and α3 ≃2.5) and one negative (α1 ≃−0.5). The Newton
method converges in 4 iterations (having set x(0) = −0.5 and tol = 1.e-10)
to the value α1:
f=@(x) exp(x)-2*x^2; df=@(x) exp(x)-4*x;
x0 = -0.5; tol =1.e -10;
nmax =100;
format
long ; [zero ,res ,niter]= newton(f,df ,x0 ,tol ,nmax )
zero =
-0.53983527690282
res =
0
niter =
4
The given function has a maximum at ¯x ≃0.3574 (which can be obtained
by applying the Newton method to the function f ′): for x(0) < ¯x the method
converges to the negative zero. If x(0) = ¯x the Newton method cannot be
applied since f ′(¯x) = 0. For x(0) > ¯x the method converges to one of the two
positive zeros, either α2 or α3.
Exercise 2.11 Let us set x(0) = 0 and tol= ϵM. In MATLAB the Newton
method converges in 43 iterations to the value 0.641182985886554, while in
Octave it converges in 32 iterations to the value 0.641184396264531. By tak-
ing the MATLAB approximated value as the reference solution in our error
analysis, we can observe that the (approximate) errors decrease only linearly
when k increases (see Figure 10.1, right). This behavior is due to the fact that
α has a multiplicity greater than 1. To recover a second-order method we can
use the modiﬁed Newton method.
Exercise 2.12 We should compute the zero of the function f(x) = sin(x) −

2gh/v2
0. By inspecting its graph, we can conclude that f has one zero in
the interval (0, π/2). The Newton method with x(0) = π/4 and tol= 10−10
converges in 5 iterations to the value 0.45862863227859.
Exercise 2.13 Using the data given in the exercise, the solution can be ob-
tained with the following instructions:
M=6000; v=1000; f=@(r) M-v*(1+ r)./r.*((1+r).^5 -1);
df=@(r) v*((1+r).^5.*(1 -5*r) -1)./( r.^2);
[zero ,res ,niter]= bisection (f ,0.01 ,0.1 ,1.e -12 ,5);
[zero ,res ,niter]= newton(f,df ,zero ,1.e-12 ,100)
The Newton method converges to the desired result in 3 iterations.
Exercise 2.14 By a graphical study, we see that (2.38) is satisﬁed for a value
of α in (π/6, π/4). Using the following instructions:

384
10 Solutions of the exercises
0
0.5
1
1.5
2
2.5
−0.5
0
0.5
1
1.5
2
0
5
10
15
20
25
30
35
10
−6
10
−5
10
−4
10
−3
10
−2
10
−1
10
0
Figure 10.1. At left: the two curves represent the two possible conﬁgurations
of roads system in terms of the angle α versus β ∈[0, 2π/3] (Solution 2.9). At
right: error versus iteration number of the Newton method for the computation
of the zero of the function f(x) = x3 −3x22−x + 3x4−x −8−x (Solution 2.11)
l1 =8; l2 =10; g=3* pi /5;
f=@(a) -l2*cos(g+a)/ sin(g+a)^2- l1*cos(a)/sin(a)^2;
df=@(a) [l2/sin(g+a)+2* l2*cos(g+a)^2/ sin(g+a)^3+...
l1/sin(a)+2* l1*cos(a)^2/ sin(a)^3];
[zero ,res ,niter]= newton(f,df ,pi/4,1.e-15 ,100)
L=l2/sin (2* pi/5- zero )+l1/sin(zero )
the Newton method provides the approximate value 0.59627992746547 in 6
iterations, starting from x(0) = π/4. We deduce that the maximum length of
a rod that can pass in the corridor is L = 30.5484.
Exercise 2.15 If α is a zero of f with multiplicity m, then there exists a
function h such that h(α) ̸= 0 and f(x) = h(x)(x −α)m. By computing the
ﬁrst derivative of the iteration function φN of the Newton method, we have
φ′
N(x) = 1 −[f ′(x)]2 −f(x)f ′′(x)
[f ′(x)]2
= f(x)f ′′(x)
[f ′(x)]2 .
By replacing f, f ′ and f ′′ with the corresponding expressions as functions of
h(x) and (x −α)m, we obtain limx→α φ′
N(x) = 1 −1/m, hence φ′
N(α) = 0
if and only if m = 1. Consequently, if m = 1 the method converges at least
quadratically, according to (2.9). If m > 1 the method converges with order 1
according to Proposition 2.1.
Exercise 2.16 Let us inspect the graph of f by using the following com-
mands:
f=@(x) x^3+4*x^2 -10;
fplot(f ,[ -10 ,10]);
grid on;
fplot(f,[ -5 ,5]); grid on;
fplot(f,[0 ,2]);
grid on; axis ([0 ,2 , -10 ,15])
we can see that f has only one real zero, equal approximately to 1.36 (see
Figure 10.2, left, for the last graph generated by the previous instructions).
The iteration function and its derivative are:

10.3 Chapter 3
385
0
0.5
1
1.5
2
−10
−5
0
5
10
15
0.5
0.55
0.6
0.65
0.7
−9
−8
−7
−6
−5
−4
−3
−2
−1
0
1 x 10
−3
Figure 10.2. At left: graph of f(x) = x3 + 4x2 −10 for x ∈[0, 2] (Solution
2.16). At right: graph of f(x) = x3 −3x22−x + 3x4−x −8−x for x ∈[0.5, 0.7]
(Solution 2.18)
φ(x) = 2x3 + 4x2 + 10
3x2 + 8x
= −
f(x)
3x2 + 8x + x,
φ′(x) = (6x2 + 8x)(3x2 + 8x) −(6x + 8)(2x3 + 4x2 + 10)
(3x2 + 8x)2
= (6x + 8)f(x)
(3x2 + 8x)2 ,
and φ(α) = α. We easily deduce that φ′(α) = 0, since f(α) = 0. Consequently,
the proposed method converges (at least) quadratically.
Exercise 2.17 The proposed method is convergent at least with order 2 since
φ′(α) = 0.
Exercise 2.18 By keeping the remaining parameters unchanged, the method
converges after 30 iterations to the value 0.641182210863894 which diﬀers by
less than 10−7 from the result previously computed in Solution 2.11. However,
the behavior of the function, which is quite ﬂat near x = 0, suggests that the
result computed previously could be more accurate. In Figure 10.2, right, we
show the graph of f in (0.5, 0.7), obtained by the following instructions:
f=@(x) x^3-3*x^2*2^( - x)+3* x*4^(-x)-8^(- x);
fplot(f ,[0.5 0.7]);
grid
on
10.3 Chapter 3
Exercise 3.1 Since x ∈(x0, xn), there exists an interval Ii = (xi−1, xi) such
that x ∈Ii. We can easily see that maxx∈Ii |(x −xi−1)(x −xi)| = h2/4. If
we bound |x −xi+1| above by 2h, |x −xi−2| by 3h and so on, we obtain the
inequality (3.6).

386
10 Solutions of the exercises
Exercise 3.2 In all cases we have n = 4 and thus we should estimate the ﬁfth
derivative of each function in the given interval. We ﬁnd: maxx∈[−1,1] |f (5)
1
| ≃
1.18, maxx∈[−1,1] |f (5)
2
| ≃1.54, maxx∈[−π/2,π/2] |f (5)
3
| ≃1.41. Thanks to for-
mula (3.7), the upper bounds for the corresponding errors are about 0.0018,
0.0024 and 0.0211, respectively.
Exercise 3.3 Using the MATLAB command polyfit we compute the inter-
polating polynomials of degree 3 in the two cases:
year =[1975
1980
1985
1990];
west =[72.8
74.2
75.2
76.4];
east =[70.2
70.2
70.3
71.2];
cwest=polyfit(year ,west ,3);
ceast=polyfit(year ,east ,3);
estwest =polyval (cwest ,[1977
1983
1988]);
esteast =polyval (ceast ,[1977
1983
1988]);
The estimated values in 1977, 1983 and 1988 are
estwest =
73.4464
74.8096
75.8576
esteast =
70.2328
70.2032
70.6992
for the Western and Eastern Europe, respectively.
Exercise 3.4 We choose the month as time-unit. The initial time t0 = 1
corresponds to November 1987, while t7 = 157 to November 2000. With the
following instructions we compute the coeﬃcients of the polynomial interpo-
lating the given prices:
time = [1 14 37 63 87 99 109 157];
price = [4.5 5 6 6.5 7 7.5 8 8];
[c] = polyfit(time ,price ,7);
Setting [price2002]=polyval(c,181) we ﬁnd that the estimated price of the
magazine in November 2002 is approximately 11.24 euros.
Exercise 3.5 In this special case, since the number of interpolation nodes is
4, the interpolatory cubic spline, computed by the command spline, coincides
with the interpolating polynomial. As a matter of fact, the spline interpolates
the nodal data, moreover its ﬁrst and second derivatives are continuous while
the third derivative is continuous at the internal nodes x1 and x2, thanks to
the not-a-knot condition used by MATLAB. This wouldn’t be true for the
natural interpolating cubic spline.
Exercise 3.6 We use the following instructions:
T = [4:4:20];
rho =[1000.7794 ,1000.6427 ,1000.2805 ,999.7165 ,998.9700];
Tnew = [6:4:18];
format
long e;
rhonew = spline(T,rho ,Tnew )
rhonew =
Columns 1 through 2
1.000740787500000 e+03
1.000488237500000 e+03
Columns 3 through 4
1.000022450000000 e+03
9.993649250000000 e+02

10.3 Chapter 3
387
The comparison with the further measures shows that the approximation is
extremely accurate. Note that the state equation for the sea-water (UNESCO,
1980) assumes a fourth-order dependence of the density on the temperature.
However, the coeﬃcient of the fourth power of T is of the order of 10−9 and
the cubic spline provides a good approximation of the measured values.
Exercise 3.7 We compare the results computed using the interpolatory cubic
spline obtained using the MATLAB command spline (denoted with s3), the
interpolatory natural spline (s3n) and the interpolatory spline with null ﬁrst
derivatives at the endpoints of the interpolatory interval (s3d) (computed with
Program 3.2). We use the following instructions:
year =[1965
1970
1980
1985
1990
1991];
production =[17769
24001 25961 34336 29036 33417];
z =[1962:0.1:1992];
s3
= spline(year ,production ,z);
s3n = cubicspline (year ,production ,z);
s3d = cubicspline (year ,production ,z,0,[0 0]);
In the following table we resume the computed values (expressed in thousands
of tons of goods):
year
1962
1977
1992
s3
514.6
2264.2
4189.4
s3n
1328.5
2293.4
3779.8
s3d
2431.3
2312.6
2216.6
The comparison with the real data (1238, 2740.3 and 3205.9 thousands of tons,
respectively) shows that the values predicted by the natural spline are accurate
also outside the interpolation interval (see Figure 10.3, left). On the contrary,
the interpolating polynomial introduces large oscillations near this end-point
and underestimates the production of as many as −7768.5 ×106 Kg for 1962.
Exercise 3.8 The interpolating polynomial p and the spline s3 can be eval-
uated by the following instructions:
pert = 1.e -04;
x=[ -1:2/20:1]; y=sin (2*pi*x)+( -1).^[1:21]* pert ;
z=[ -1:0.01:1]; c=polyfit(x,y ,20);
p= polyval(c,z); s3=spline(x,y,z);
When we use the unperturbed data (pert=0) the graphs of both p and s3
are indistinguishable from that of the given function. The situation changes
dramatically when the perturbed data are used (pert=1.e-04). In particular,
the interpolating polynomial shows strong oscillations at the end-points of the
interval, whereas the spline remains practically unchanged (see Figure 10.3,
right). This example shows that approximation by splines is in general more
stable with respect to perturbation errors than the global Lagrange interpola-
tion.
Exercise 3.9 If n = m, setting ˜f = Πnf we ﬁnd that the ﬁrst member
of (3.28) is null. Thus in this case Πnf is the solution of the least-squares
problem. Since the interpolating polynomial is unique, we deduce that this is
the unique solution to the least-squares problem.

388
10 Solutions of the exercises
1960
1965
1970
1975
1980
1985
1990
1995
0.5
1
1.5
2
2.5
3
3.5
4
4.5 x 10
4
−1
−0.5
0
0.5
1
−1.5
−1
−0.5
0
0.5
1
1.5
Figure 10.3. At left: comparison among the cubic spline for the data of Ex-
ercise 3.7: s3 (solid line), s3d (dashed line) and s3n (dotted line). The circles
denote the values used in the interpolation. At right: the interpolating polyno-
mial (dashed line) and the interpolatory cubic spline (solid line) corresponding
to the perturbed data (Solution 3.8). Note the severe oscillations of the inter-
polating polynomial near the end-points of the interval
−60
−40
−20
0
20
40
60
80
−3.4
−3.35
−3.3
−3.25
−3.2
−3.15
−3.1
−3.05
−3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−10
−5
0
5
10
15
20
25
30
35
Figure 10.4. At left: least-squares polynomial of degree 4 (solid line) com-
pared with the data in the ﬁrst column of Table 3.1. (Solution 3.10). At right:
the trigonometric interpolant obtained using the instructions in Solution 3.14.
Circles refer to the available experimental data
Exercise 3.10 The coeﬃcients (obtained by the command polyfit) of the
requested polynomials are (only the ﬁrst 4 signiﬁcant digits are shown):
K = 0.67, a4 = 7.211 10−8, a3 = −6.088 10−7, a2 = −2.988 10−4, a1 =
1.650 10−3, a0 = −3.030;
K = 1.5, a4 = −6.492 10−8, a3 = −7.559 10−7, a2 = 3.788 10−4, a1 =
1.67310−3, a0 = 3.149;
K = 2, a4 = −1.050 10−7, a3 = 7.130 10−8, a2 = 7.044 10−4, a1 =
−3.828 10−4, a0 = 4.926;
K = 3, a4 = −2.319 10−7, a3 = 7.740 10−7, a2 = 1.419 10−3, a1 =
−2.574 10−3, a0 = 7.315.
In Figure 10.4, left, we show the graph of the polynomial computed using
the data in the column with K = 0.67 of Table 3.1.

10.4 Chapter 4
389
Exercise 3.11 By repeating the ﬁrst 3 instructions reported in Solution 3.7
and using the command polyfit, we ﬁnd the following values (in 105 Kg):
15280.12 in 1962; 27407.10 in 1977; 32019.01 in 1992, which represent good
approximations to the real ones (12380, 27403 and 32059, respectively).
Exercise 3.12 We can rewrite the coeﬃcients of the system (3.30) in terms
of mean and variance by noting that the variance can be expressed as v =
1
n+1
n
i=0 x2
i −M 2. Thus the coeﬃcients of the ﬁrst equation are (n + 1) and
M, while those of the second equation are M and (n + 1)(v + M 2).
Exercise 3.13 The equation of the least-squares straight line is y = a0 +a1x,
where a0 and a1 are the solutions of system (3.30). The ﬁrst equation of (3.30)
provides that the point, whose abscissa is M and ordinate is n
i=0 yi/(n + 1),
belongs to the least-squares straight line.
Exercise 3.14 We can use the command interpft as follows:
discharge
= [0 35 0.125 5 0 5 1 0.5 0.125 0];
y =interpft (discharge ,100);
The graph of the obtained solution is reported in Figure 10.4, right.
10.4 Chapter 4
Exercise 4.1 Using the following second-order Taylor expansions of f at the
point x0, we obtain
f(x1) = f(x0) + hf ′(x0) + h2
2 f ′′(x0) + h3
6 f ′′′(ξ1),
f(x2) = f(x0) + 2hf ′(x0) + 2h2f ′′(x0) + 4h3
3 f ′′′(ξ2),
where ξ1 ∈(x0, x1) and ξ2 ∈(x0, x2). Replacing these two expressions in the
ﬁrst formula of (4.11), yields
1
2h [−3f(x0) + 4f(x1) −f(x2)] = f ′(x0) + h2
3 [f ′′′(ξ1) −2f ′′′(ξ2)],
then the thesis follows for a suitable ξ0 ∈(x0, x2). A similar procedure can be
used for the formula at xn.
Exercise 4.2 By writing the second-order Taylor expansions of f(x ± h)
around x, we have
f(¯x ± h) = f(¯x) ± hf ′(¯x) + h2
2 f ′′(¯x) ± h3
6 f ′′′(ξ±),
with ξ−∈(¯x−h, ¯x) and ξ+ ∈(¯x, ¯x+h). Subtracting these two expressions and
dividing by 2h we obtain formula (4.10) which is a second-order approximation
of f ′(x).

390
10 Solutions of the exercises
Exercise 4.3 Assuming that f ∈C4 and proceeding as in Solution 4.2 we
obtain the following errors:
a.
−1
4f (4)(ξ)h3,
b.
−1
12f (4)(ξ)h3,
c.
1
6f (4)(ξ)h3.
Exercise 4.4 Using the approximation (4.9), we obtain the following values:
t (months)
0
0.5
1
1.5
2
2.5
3
δn
–
78
45
19
7
3
–
n′
–
77.91
39.16
15.36
5.91
1.99
–
By comparison with the exact values of n′(t) we can conclude that the com-
puted values are suﬃciently accurate.
Exercise 4.5 The quadrature error can be bounded by
(b −a)3/(24M 2) max
x∈[a,b] |f ′′(x)|,
where [a, b] is the integration interval and M the (unknown) number of subin-
tervals.
The function f1 is inﬁnitely diﬀerentiable. From the graph of f ′′
1 we infer
that |f ′′
1 (x)| ≤2 in the integration interval. Thus the integration error for f1
is less than 10−4 provided that 2 · 53/(24M 2) < 10−4, that is M > 322.
Also the function f2 is diﬀerentiable to any order. Since maxx∈[0,π] |f ′′
2 (x)|=
√
2e3π/4, the integration error is less than 10−4 provided that M > 439. These
inequalities actually provide an over estimation of the integration errors. In-
deed, the (eﬀective) minimum number of intervals which ensures that the error
is below the ﬁxed tolerance of 10−4 is much lower than that predicted by our
result (for instance, for the function f1 this number is 71). Finally, we note
that, since f3 is not diﬀerentiable at both x = 0 and x = 1, the theoretical
error estimate doesn’t hold.
Exercise 4.6 On each interval Ik, k = 1, . . . , M, the error is equal to
H3/24f ′′(ξk) with ξk ∈[xk−1, xk] and hence the global error will be H3/24
M
k=1 f ′′(ξk). Since f ′′ is a continuous function in [a, b] there exists a point
ξ ∈[a, b] such that f ′′(ξ) =
1
M
M
k=1 f ′′(ξk). Using this result and the fact
that MH = b −a, we derive equation (4.14).
Exercise 4.7 This eﬀect is due to the accumulation of local errors on each
sub-interval.
Exercise 4.8 By construction, the mid-point formula integrates exactly the
constants. To verify that the linear polynomials also are exactly integrated, it
is suﬃcient to verify that I(x) = IP M(x). As a matter of fact we have
I(x) =
b

a
x dx = b2 −a2
2
,
IP M(x) = (b −a)b + a
2
.

10.4 Chapter 4
391
Exercise 4.9 For the function f1 we ﬁnd M = 71 if we use the trapezoidal
formula and only M = 8 for the composite Gauss-Legendre formula with
n = 1. (For this formula we can use Program 10.1.) Indeed, the computational
advantage of this latter formula is evident.
Program 10.1. gausslegendre: Gauss-Legendre composite quadrature formula,
with n = 1
function
intGL= gausslegendre (a,b,f,M,varargin )
y = [-1/ sqrt (3) ,1/ sqrt (3)];
H2 = (b-a)/(2*M);
z = [a:2*H2:b];
zM = (z(1:end -1)+ z(2: end ))*0.5;
x = [zM+H2*y(1), zM+H2*y(2)];
f = f(x,varargin {:});
intGL = H2*sum(f);
return
Exercise 4.10 Equation (4.18) states that the quadrature error for the
composite trapezoidal formula with H = H1 is equal to CH2
1, with C =
−b −a
12 f ′′(ξ). If f ′′ does not vary “too much”, we can assume that also the
error with H = H2 behaves like CH2
2. Then, by equating the two expressions
I(f) ≃I1 + CH2
1,
I(f) ≃I2 + CH2
2,
(10.3)
we obtain C = (I1 −I2)/(H2
2 −H2
1). Using this value in one of the expressions
(10.3), we obtain equation (4.35), that is, a better approximation than the one
produced by I1 or I2.
Exercise 4.11 We seek the maximum positive integer p such that Iappr(xp) =
I(xp). For p = 0, 1, 2, 3 we ﬁnd the following nonlinear system with 4 equations
in the 4 unknowns α, β, ¯x and ¯z:
p = 0 →α + β = b −a,
p = 1 →α¯x + β¯z = b2 −a2
2
,
p = 2 →α¯x2 + β¯z2 = b3 −a3
3
,
p = 3 →α¯x3 + β¯z3 = b4 −a4
4
.
From the ﬁrst two equations we can eliminate α and ¯z and reduce the system
to a new one in the unknowns β and ¯x. In particular, we ﬁnd a second-order
equation in β from which we can compute β as a function of ¯x. Finally, the
nonlinear equation in ¯x can be solved by the Newton method, yielding two
values of ¯x that are the nodes of the Gauss-Legendre quadrature formula with
n = 1.

392
10 Solutions of the exercises
Exercise 4.12 Since
f (4)
1
(x) = 241 −10(x −π)2 + 5(x −π)4
(1 + (x −π)2)5
,
f (4)
2
(x) = −4ex cos(x),
we ﬁnd that the maximum of |f (4)
1
(x)| is bounded by M1 ≃23, while that of
|f (4)
2
(x)| by M2 ≃18. Consequently, from (4.22) we obtain H < 0.21 in the
ﬁrst case and H < 0.16 in the second case.
Exercise 4.13 The MATLAB commands:
syms x
I=int(exp(-x^2/2) ,0 ,2);
Iex=eval (I)
yields the value 1.19628801332261 for the integral at hand.
The Gauss-Legendre formula applied to the same interval with M = 1
would provide the value 1.20278027622354 (with an absolute error equal to
6.4923e-03), while the simple Simpson formula gives 1.18715264069572 with a
slightly larger error, equal to 9.1354e-03.
Exercise 4.14 We note that Ik > 0 ∀k, since the integrand is non-negative.
Therefore, we expect that all the values produced by the recursive formula
should be non-negative. Unfortunately, the recursive formula is unstable to
the propagation of roundoﬀerrors and produces negative elements:
I(1)=1/ exp (1); for k=2:20 , I(k)=1-k*I(k -1);
end
The result is I(20) = 104.86 in MATLAB, while Octave produces I(20)
= -30.1924. Using the composite Simpson formula, with M ≥16, we can
compute the integral with the desired accuracy, as a matter of fact, denoting
by f(x) the integrand function, the absolute value of its fourth derivative is
bounded by M ≃1.46 105. Consequently, from (4.22) we obtain H < 0.066.
Exercise 4.15 The idea of Richardson’s extrapolation is general and can be
applied to any quadrature formula. By proceeding as in Solution 4.10, recalling
that both Simpson and Gauss quadrature formulas are fourth-order accurate,
formula (4.35) reads
IR = I1 + (I1 −I2)/(H4
2/H4
1 −1).
For the Simpson formula we obtain
I1 = 1.19616568040561, I2 = 1.19628173356793, ⇒IR = 1.19628947044542,
with an absolute error I(f) −IR = −1.4571e −06 (we gain two orders of
magnitude with respect to I1 and a factor 1/4 with respect to I2). Using the
Gauss-Legendre formula we obtain (the errors are reported between parenthe-
ses):
I1 = 1.19637085545393 (−8.2842e −05),
I2 = 1.19629221796844 (−4.2046e −06),
IR = 1.19628697546941 (1.0379e −06).
The advantage of using the Richardson extrapolation method is evident.

10.4 Chapter 4
393
Exercise 4.16 We must compute by the Simpson formula the values j(r, 0) =
σ/(ε0r2)
 r
0 f(ξ)dξ with r = k/10, for k = 1, . . . , 10 and f(ξ) = eξξ2.
In order to estimate the integration error we need the fourth derivative
f (4)(ξ) = eξ(ξ2 + 8ξ + 12). The maximum of f (4) in the integration interval
[0, r] is attained at ξ = r since f (4) is monotonically increasing. For a given
r the error is below 10−10 provided that H4 < 10−102880/(rf (4)(r)). For
r = k/10 with k = 1, . . . , 10 by the following instructions we can compute the
minimum numbers of subintervals which ensure that the previous inequalities
are satisﬁed:
r =[0.1:0.1:1];
maxf4=exp(r).*( r.^2+8*r+12);
H =(10^( -10)*2880./( r.* maxf4 )).^(1/4);
M=fix(r./H)
M =
4
11
20
30
41
53
67
83
100
118
Therefore, the values of j(r, 0) are computed by running the following instruc-
tions:
sigma =0.36; epsilon0 = 8.859e -12;
f=@(x) exp(x).*x.^2;
for k = 1:10
r = k/10;
j(k)= simpsonc (0,r,M(k),f);
j(k) = j(k)* sigma/(r^2* epsilon0 );
end
Exercise 4.17 We compute E(213) using the Simpson composite formula by
increasing the number of intervals until the diﬀerence between two consecutive
approximations (divided by the last computed value) is less than 10−11:
f=@(x) 1./( x.^5.*( exp (1.432./(213* x)) -1));
a=3.e -04; b=14.e -04;
i=1; err = 1; Iold = 0;
while err
>= 1.e-11
I=2.39e-11* simpsonc (a,b,i,f);
err = abs(I-Iold )/ abs(I);
Iold =I;
i=i+1;
end
The procedure returns the value i = 59. Therefore, using 58 equispaced in-
tervals we can compute the integral E(213) with ten exact signiﬁcant digits.
The same result could be obtained by the Gauss-Legendre formula using 53
intervals. Note that as many as 1609 intervals would be nedeed if using the
composite trapezoidal formula.
Exercise 4.18 On the whole interval the given function is not regular enough
to allow the application of the theoretical convergence result (4.22). One pos-
sibility is to decompose the integral into the sum of two intervals, [0, 0.5] and
[0.5, 1], in which the function is regular (it is actually a polynomial of degree 2
in each sub-interval). In particular, if we use the Simpson rule on each interval
we can even integrate f exactly.

394
10 Solutions of the exercises
10.5 Chapter 5
Exercise 5.1 Let xn denote the number of algebraic operations (sums, sub-
tractions and multiplications) required to compute one determinant of a matrix
of order n × n by the Laplace rule (1.8). The following recursive formula holds
xk −kxk−1 = 2k −1,
k ≥2,
with x1 = 0. Multiplying both sides of this equation by 1/k!, we obtain
xk
k! −
xk−1
(k −1)! = 2k −1
k!
and summing both sides from 2 to n gives the solution:
xn = n!
n

k=2
2k −1
k!
.
Recalling that
∞

k=0
1
k! = e, it holds
n

k=2
2k −1
k!
= 2
n−1

k=1
1
k! −
n

k=2
1
k! ≃2.718,
whence xn ≃3n!. It is worth mentioning that the Cramer rule (see Section
5.2) requires about 3(n+1)! operations to solve a square linear system of order
n with full matrix.
Exercise 5.2 We use the following MATLAB commands to compute the
determinants and the corresponding CPU-times:
t = []; NN =3:500;
for n = NN
A=magic(n); tt=cputime ; d=det(A); t=[t, cputime -tt];
end
Let us compute the coeﬃcients of the cubic least-squares polynomial that
approximate the data NN=[3:500] and t
c= polyfit(NN ,t,3)
c =
1.4055e-10
7.1570e-08
-3.6686e-06
3.1897e-04
If we compute the fourth degree least-squares polynomial
c= polyfit(NN ,t,4)
we obtain the following coeﬃcients:
c =
7.6406e-15
1.3286e-10
7.4064e-08
-3.9505e-06
3.2637e-04
that is, the coeﬃcient of n4 is close to the machine precision while the other
ones are quite unchanged with respect to the projection on P3. From this result,
we can conclude that in MATLAB the CPU-time required for computing the
determinant of a matrix of dimension n scales as n3.

10.5 Chapter 5
395
Exercise 5.3 Denoting by Ai the principal submatrix of A of order i, we
have: detA1 = 1, detA2 = ε, detA3 = detA = 2ε + 12. Consequently, if ε = 0
the second principal submatrix is singular and the LU factorization of A does
not exist (see Proposition 5.1). The matrix A is singular if ε = −6. In this case
the LU factorization exists and yields
L =
⎡
⎣
1 0
0
2 1
0
3 1.25 1
⎤
⎦, U =
⎡
⎣
1 7
3
0 −12 −4
0 0
0
⎤
⎦.
Nevertheless, note that U is singular (as we could have predicted since A is
singular) and the upper triangular system Ux = y admits inﬁnite solutions.
We notice that the backward substitutions (5.10) cannot be applied because
of the same reason.
Exercise 5.4 Let us consider algorithm 5.13. At step k = 1, n −1 divisions
were used to calculate the li1 entries for i = 2, . . . , n. Then (n −1)2 multi-
plications and (n −1)2 additions were used to create the new entries a(2)
ij , for
i, j = 2, . . . , n. At step k = 2, the number of divisions is (n −2), while the
number of multiplications and additions will be (n−2)2. At ﬁnal step k = n−1
only 1 addition, 1 multiplication and 1 division is required. Thus, using the
identies
q

s=1
s = q(q + 1)
2
,
q

s=1
s2 = q(q + 1)(2q + 1)
6
,
q ≥1,
we can conclude that to complete the LU factorization we need the following
number of operations
n−1

k=1
n

i=k+1
⎛
⎝1 +
n

j=k+1
2
⎞
⎠=
n−1

k=1
(n −k)(1 + 2(n −k))
=
n−1

j=1
j + 2
n−1

j=1
j2 = (n −1)n
2
+ 2(n −1)n(2n −1)
6
= 2
3n3 −n2
2 −n
6 .
Exercise 5.5 By deﬁnition, the inverse X of a matrix A ∈Rn×n satisﬁes
XA = AX = I. Therefore, for j = 1, . . . , n the column vector xj of X is the
solution of the linear system Axj = ej, where ej is the jth vector of the
canonical basis of Rn with all components equal to zero except the jth that
is equal to 1. After computing the LU factorization of A, the computation of
the inverse of A requires the solution of n linear systems with the same matrix
and diﬀerent right-hand sides.
Exercise 5.6 Using the Program 5.1 we compute the L and U factors:
L =
⎡
⎣
1
0
0
2
1
0
3 −3.38 · 1015 1
⎤
⎦, U =
⎡
⎣
1
1
3
0 −8.88 · 10−16
14
0
0
4.73 · 10−16
⎤
⎦.
If we compute their product we obtain the matrix

396
10 Solutions of the exercises
L*U
ans =
1.0000
1.0000
3.0000
2.0000
2.0000
20.0000
3.0000
6.0000
0.0000
which diﬀers from A since the entry in position (3,3) is equal to 0 while in A
it is equal to 4.
The accurate computation of both L and U can be accomplished by invoking a
partial pivoting by rows, indeed by the instruction [L,U,P]=lu(A) we obtain
the correct results.
Exercise 5.7 Usually, only the triangular (upper or lower) part of a sym-
metric matrix is stored. Therefore, any operation that does not respect the
symmetry of the matrix is not optimal in view of the memory storage. This
is the case when row pivoting is carried out. A possibility is to exchange si-
multaneously rows and columns having the same index, limiting therefore the
choice of the pivot only to the diagonal elements. More generally, a pivoting
strategy involving exchange of rows and columns is called complete pivoting
(see, e.g., [QSS07, Chap. 3]).
Exercise 5.8 The symbolic computation of the L and U factors yields
L =
⎡
⎣
1
0
0
(ε −2)/2
1
0
0
−1/ε 1
⎤
⎦, U =
⎡
⎣
2 −2 0
0 ε 0
0 0 3
⎤
⎦,
thus l32 →∞, when ε →0. If we choose b = (0, ε, 2)T , it is easy to verify
that x = (1, 1, 1)T is the exact solution of Ax = b. To analyze the error with
respect to the exact solution for ε →0, let us take ε = 10−k, for k = 0, . . . , 9.
The following instructions
e=1; xex=ones (3 ,1); err =[];
for k=1:10
b=[0; e;2];
L=[1 0 0; (e -2)*0.5 1 0; 0
-1/e 1];
U=[2
-2 0; 0 e 0; 0 0 3];
y=L\b; x=U\y;
err(k)= norm (x-xex )/ norm (xex ); e=e*0.1;
end
yield
err =
0
0
0
0
0
0
0
0
0
0
i.e., the numerical solution is not aﬀected by rounding errors. This can be
explained by noticing that all the entries of L, U and b are ﬂoating point
numbers not aﬀected by rounding errors and, unusually, no rounding errors
are propagated during both forward and backward substitutions, even if the
condition number of A is proportional to 1/ε.
On the contrary, by setting b = (2 log(2.5) −2, (ε −2) log(2.5) + 2, 2)T , which
corresponds to the exact solution x = (log(2.5), 1, 1)T , and analyzing the rel-
ative error for ε = 1/3 · 10−k, for k = 0, . . . , 9, the instructions

10.5 Chapter 5
397
e=1/3; xex=[ log (5/2) ,1 ,1] ’; err =[];
for k=1:10
b=[2* log (5/2) -2 ,(e -2)* log (5/2)+2 ,2] ’;
L=[1 0 0; (e -2)*0.5 1 0; 0
-1/e 1];
U=[2
-2 0; 0 e 0; 0 0 3];
y=L\b; x=U\y;
err(k)= norm (x-xex )/ norm (xex ); e=e*0.1;
end
provide
err =
Columns 1 through 5
1.8635e-16
5.5327e-15
2.6995e-14
9.5058e-14
1.3408e-12
Columns 6 through 10
1.2828e-11
4.8726e-11
4.5719e-09
4.2624e-08
2.8673e-07
In the latter case the error depends on the condition number of A, which obeys
the law K(A) = C/ε and satisﬁes the estimate (5.34).
Exercise 5.9 The computed solutions become less and less accurate when
i increases. Indeed, the error norms are equal to 1.10 · 10−14 for i = 1, to
9.32 · 10−10 for i = 2 and to 2.51 · 10−7 for i = 3. (We warn the reader that
these results indeed change depending upon the diﬀerent MATLAB versions
used!!) This can be explained by observing that the condition number of Ai
increases as i increases. Indeed, using the command cond we ﬁnd that the
condition number of Ai is ≃103 for i = 1, ≃107 for i = 2 and ≃1011 for
i = 3.
Exercise 5.10 If (λ, v) are an eigenvalue-eigenvector pair of a matrix A, then
λ2 is an eigenvalue of A2 with the same eigenvector. Indeed, from Av = λv
follows A2v = λAv = λ2v. Consequently, if A is symmetric and positive
deﬁnite K(A2) = (K(A))2.
Exercise 5.11 The iteration matrix of the Jacobi method is:
BJ =
⎡
⎣
0
0 −α−1
0
0
0
−α−1 0
0
⎤
⎦.
Its eigenvalues are {0, α−1, −α−1}. Thus the method converges if |α| > 1.
The iteration matrix of the Gauss-Seidel method is
BGS =
⎡
⎣
0 0 −α−1
0 0
0
0 0 α−2
⎤
⎦
with eigenvalues {0, 0, α−2}. Therefore, the method converges if |α| > 1. In
particular, since ρ(BGS) = [ρ(BJ)]2, the Gauss-Seidel converges more rapidly
than the Jacobi method.

398
10 Solutions of the exercises
Exercise 5.12 A suﬃcient condition for the convergence of the Jacobi and
the Gauss-Seidel methods is that A is strictly diagonally dominant. The second
row of A satisﬁes the condition of diagonal dominance provided that |β| < 5.
Note that if we require directly that the spectral radii of the iteration matrices
are less than 1 (which is a suﬃcient and necessary condition for convergence),
we ﬁnd the (less restrictive) limitation |β| < 25 for both methods.
Exercise 5.13 The relaxation method in vector form is
(I −ωD−1E)x(k+1) = [(1 −ω)I + ωD−1F]x(k) + ωD−1b
where A = D −(E + F), D being the diagonal of A, and -E and -F the lower
(resp. upper) part of A. The corresponding iteration matrix is
B(ω) = (I −ωD−1E)−1[(1 −ω)I + ωD−1F].
If we denote by λi the eigenvalues of B(ω), we obtain
'''''
n

i=1
λi
''''' = |detB(ω)|
= |det[(I −ωD−1E)−1]| · |det[(1 −ω)I + ωD−1F)]|.
Noticing that, given two matrices A and B with A = I + αB, for any α ∈R
it holds λi(A) = 1 + αλi(B), and that all the eigenvalues of both D−1E and
D−1F are null, we have
'''''
n

i=1
λi
''''' =
'''''
n

i=1
(1 −ω) + ωλi(D−1F)
1 −ωλi(D−1E)
''''' = |1 −ω|n.
Therefore, at least one eigenvalue must satisfy the inequality |λi| ≥|1 −ω|.
Thus, a necessary condition to ensure convergence is that |1 −ω| < 1, that is,
0 < ω < 2.
Exercise 5.14 Matrix A =
	 3 2
2 6

is strictly diagonally dominant by rows, a
suﬃcient condition for the Gauss-Seidel method to converge. On the contrary,
matrix A =
	 1 1
1 2

is not strictly diagonally dominant by rows, however it
is symmetric. Moreover, we can easily verify that it is positive deﬁnite, i.e.
zT Az > 0 for any z ̸= 0 of R2. We perform the following computations by
MATLAB (obviously, for this simple case, we could perform them by hands!):
syms
z1 z2 real
z=[z1;z2]; A=[1 1; 1 2];
pos=z’*A*z; simple(pos)
ans =
z1 ^2+2*z1*z2+2* z2^2
ans =
z1 ^2+2*z1*z2+2* z2^2

10.5 Chapter 5
399
where the command syms z1 z2 real converts the variables z1 and z2 from
symbolic to real type, while the command simple tries several algebraic sim-
pliﬁcations of pos and returns the shortest. It is easy to see that the computed
quantity is positive since it can be rewritten as (z1+z2)ˆ2+z2ˆ2. Thus, the
given matrix is symmetric and positive deﬁnite, a suﬃcient condition for the
Gauss-Seidel method to converge.
Exercise 5.15 We ﬁnd:
for the Jacobi method:
(
x(1)
1
= 1
2(1 −x(0)
2 ),
x(1)
2
= −1
3(x(0)
1 );
⇒
(
x(1)
1
= 1
4,
x(1)
2
= −1
3;
for the Gauss-Seidel method:
(
x(1)
1
= 1
2(1 −x(0)
2 ),
x(1)
2
= −1
3x(1)
1 ,
⇒
(
x(1)
1
= 1
4,
x(1)
2
= −1
12 ;
for the gradient method, we ﬁrst compute the initial residual
r(0) = b −Ax(0) =
	 1
0

−
	 2 1
1 3

x(0) =
	 −3/2
−5/2

.
Then, since
P−1 =
	 1/2
0
0
1/3

,
we have z(0) = P−1r(0) = (−3/4, −5/6)T . Therefore
α0 =
(z(0))T r(0)
(z(0))T Az(0) = 77
107,
and
x(1) = x(0) + α0z(0) = (197/428, −32/321)T .
Exercise 5.16 In the stationary case, the eigenvalues of the matrix Bα =
I −αP−1A are μi(α) = 1 −αλi, λi being the ith eigenvalue of P−1A. Then
ρ(Bα) =
max
i=1,...,n|1 −αλi| = max{|1 −αλmin|, |1 −αλmax|}.
Thus, the optimal value of α (that is the value that minimizes the spectral
radius of the iteration matrix) is the root of the equation
1 −αλmin = αλmax −1
which yields (5.58). Formula (5.72) follows now by a direct computation of
ρ(Bαopt).

400
10 Solutions of the exercises
Exercise 5.17 We have to minimize the function Φ(α) = ∥e(k+1)∥2
A with
respect to α ∈R. Since e(k+1) = x −x(k+1) = e(k) −αz(k), we obtain
Φ(α) = ∥e(k+1)∥2
A = ∥e(k)∥2
A + α2∥z(k)∥2
A −2α(z(k))T Ae(k).
The minimum of Φ(α) is found in correspondence to the value αk such that
Φ′(αk) = 0, i.e.,
αk∥z(k)∥2
A −(z(k))T Ae(k) = 0,
so that αk = ((z(k))T Ae(k))/∥z(k)∥2
A. Finally, (5.60) follows by noticing that
Ae(k) = r(k).
Exercise 5.18 We provide two possible proofs.
1. Note that P−1A = P−1/2(P−1/2AP−1/2)P1/2 where P1/2 is the square root
of P (see, e.g. [QV94, Sect. 2.5]). Since P is symmetric positive deﬁnite, P1/2
is symmetric and positive deﬁnite and it is the unique solution of the matrix
equation X2 = P. This shows that P−1A is similar to the matrix P−1/2AP−1/2
which is symmetric positive deﬁnite.
2. The eigenpairs (μ, y) of P−1A satisfy the equation P−1Ay = μy, that is
Ay = μPy, therefore μ = (yT Ay)/(yT Py) > 0 since both A and P are
symmetric positive deﬁnite.
Exercise 5.19 The matrix associated to the Leontieﬀmodel is symmetric,
but not positive deﬁnite. Indeed, using the following instructions:
for i=1:20;
for j=1:20;
C(i,j)=i+j;
end;
end;
A=eye(20)-C;
[min(eig(A)), max(eig(A))]
ans =
-448.58
30.583
we can see that the minimum eigenvalue is a negative number and the maxi-
mum eigenvalue is a positive number. Therefore, the convergence of the gra-
dient method is not guaranteed. However, since A is nonsingular, the given
system is equivalent to the system AT Ax = AT b, where AT A is symmetric
and positive deﬁnite. We solve the latter by the gradient method requiring
that the norm of the residual be less than 10−10 and starting from the initial
data x(0) = 0:
b = [1:20] ’;
AA=A’*A; b=A’*b; x0 = zeros (20 ,1);
[x,iter ]= itermeth (AA ,b,x0 ,100 ,1.e -10);
The method converges in 15 iterations. A drawback of this approach is that the
condition number of the matrix AT A is, in general, larger than the condition
number of A.

10.6 Chapter 6
401
10.6 Chapter 6
Exercise 6.1 A1: the power method converges in 34 iterations to the value
2.00000000004989. A2: starting from the same initial vector, the power method
requires now 457 iterations to converge to the value 1.99999999990611. The
slower convergence rate can be explained by observing that the two largest
eigenvalues are very close one another. Finally, for the matrix A3 the method
doesn’t converge since A3 features two distinct eigenvalues (i and −i) of max-
imum modulus.
Exercise 6.2 The Leslie matrix associated with the values in the table is
A =
⎡
⎢⎢⎣
0 0.5 0.8 0.3
0.2 0
0
0
0 0.4 0
0
0
0 0.8 0
⎤
⎥⎥⎦.
Using the power method we ﬁnd λ1 ≃0.5353. The normalized distribution of
this population for diﬀerent age intervals is given by the components of the cor-
responding unitary eigenvector, that is, x1 ≃(0.8477, 0.3167, 0.2367, 0.3537)T .
Exercise 6.3 We rewrite the initial guess as
y(0) = β(0)

α1x1 + α2x2 +
n

i=3
αixi

,
with β(0) = 1/∥x(0)∥. By calculations similar to those carried out in Section
6.2, at the generic step k we ﬁnd:
y(k) = γkβ(k)

α1x1eikϑ + α2x2e−ikϑ +
n

i=3
αi λk
i
γk xi

.
Therefore, when k →∞, the ﬁrst two terms don’t vanish and, due to the
opposite sign of the exponents, the sequence of the y(k) oscillates and cannot
converge.
Exercise 6.4 If A is non-singular, from the eigenvalue equation Ax = λx, we
deduce A−1Ax = λA−1x, and therefore A−1x = (1/λ)x.
Exercise 6.5 The power method applied to the matrix A generates an oscil-
lating sequence of approximations of the maximum modulus eigenvalue (see,
Figure 10.5). This behavior is due to the fact that the matrix A has two distinct
eigenvalues of maximum modulus.
Exercise 6.6 Since the eigenvalues of a real symmetric matrix are all real,
they lie inside a closed bounded interval [λa, λb]. Our aim is to estimate both
λa and λb. To compute the eigenvalue of maximum modulus of A we use
Program 6.1:

402
10 Solutions of the exercises
0
20
40
60
80
100
−1.5
−1
−0.5
0
0.5
1
1.5
2
Figure 10.5.
The approximations of the maximum modulus eigenvalue of
the matrix of Solution 6.5 computed by the power method
A= wilkinson (7);
x0=ones (7 ,1); tol =1.e-15;
nmax =100;
[lambdab ,x,iter ]= eigpower (A,tol ,nmax ,x0 );
After 35 iterations we obtain lambdab=3.76155718183189. Since λa is the
eigenvalue of A farest from λb, in order to compute it we apply the power
method to the matrix Ab = A −λbI, that is we compute the maximum modu-
lus eigenvalue of the matrix Ab. Then we will set λa = λ+λb. The instructions
[lambda ,x,iter ]= eigpower (A-lambdab *eye (7),tol ,nmax ,x0 );
lambdaa =lambda+lambdab
yield lambdaa =-1.12488541976457 after 33 iterations. These results are sat-
isfactory approximations of the extremal eigenvalues of A.
Exercise 6.7 Let us start by considering the matrix A. We observe that there
is an isolated row circle centered at (9, 0) with radius equal to 1, that can
contain only one eigenvalue (say λ1), in view of Proposition 6.1. Therefore
λ1 ∈R, more precisely λ1 ∈(8, 10). Moreover, from Figure 10.6, right, we note
that A features two other isolated column circles centered at (2, 0) and (4, 0),
respectively, both with radius equal to 1/2. Therefore A has two other real
eigenvalues λ2 ∈(1.5, 2.5) and λ3 ∈(3.4, 4.5). Since all the coeﬃcients of A
are real, we can conclude that also the fourth eigenvalue will be real.
Let us consider now the matrix B that admits only one isolated column
circle (see Figure 10.7 right), centered at (−5, 0) and with radius 1/2. Then,
thanks to the previous consideration the corresponding eigenvalue must be
real and it will belong to the interval (−5.5, −4.5). The remaining eigenvalues
can be either all real, or one real and 2 complex.
Exercise 6.8 The row circles of A feature an isolated circle of center (5,0)
and radius 2 the maximum modulus eigenvalue must belong to. Therefore, we
can set the value of the shift equal to 5. The comparison between the number of
iterations and the computational cost of the power method with and without
shift can be found using the following commands:
A=[5 0 1
-1; 0 2 0
-1/2; 0 1 -1 1;
-1 -1 0 0];

10.6 Chapter 6
403
0
2
4
6
8
10
12
-3
-2
-1
0
1
2
3
Row circles
Re
Im
0
2
4
6
8
10
12
-3
-2
-1
0
1
2
3
Column circles
Re
Im
Figure 10.6. Row circles (at left) and column circles (at right) of the matrix
A of Solution 6.7
-6
-4
-2
0
2
4
-1
0
1
Row circles
Re
Im
-6
-4
-2
0
2
4
-1
0
1
Column circles
Re
Im
Figure 10.7. Row circles (at left) and column circles (at right) circles of the
matrix B of Solution 6.7
tol =1e -14;
x0=[1 2 3 4]’;
nmax =1000;
tic; [lambda ,x,iter ]= eigpower (A,tol ,nmax ,x0);
toc , iter
Elapsed
time
is 0.001854
seconds .
iter = 35
tic; [lambda ,x,iter ]= invshift (A,5,tol ,nmax ,x0);
toc , iter
Elapsed
time
is 0.000865
seconds .
iter = 12
The power method with shift requires in this case a lower number of iterations
(1 versus 3) and almost half the cost than the usual power method (also
accounting for the extra time needed to compute the LU factorization of A
oﬀ-line).
Exercise 6.9 It holds
A(k) = Q(k+1)R(k+1) and A(k+1) = R(k+1)Q(k+1)
and then
(Q(k+1))T A(k)Q(k+1) = R(k+1)Q(k+1) = A(k+1).
Since (Q(k+1))T = (Q(k+1))−1 we can conclude that matrix A(k) is similar to
A(k+1) for any k ≥0.
Exercise 6.10 We can use the command eig in the following way: [X,D]=eig
(A), where X is the matrix whose columns are the unit eigenvectors of A and D
is a diagonal matrix whose elements are the eigenvalues of A. For the matrices
A and B of Exercise 6.7 we should execute the following instructions:

404
10 Solutions of the exercises
A=[2
-1/2 0
-1/2; 0 4 0 2;
-1/2 0 6 1/2; 0 0 1 9];
sort (eig(A))
ans =
2.0000
4.0268
5.8003
9.1728
B=[-5 0 1/2 1/2;
1/2 2 1/2 0; 0 1 0 1/2; 0 1/4 1/2 3];
sort (eig(B))
ans =
-4.9921
-0.3038
2.1666
3.1292
The conclusions drawn on the basis of Proposition 6.1 are quite coarse.
10.7 Chapter 7
Exercise 7.1 By direct inspection on the plot of function f we ﬁnd that there
is a single minimizer in the interval [−2, 1]. We use the following instructions
to call Program 7.7:
a=-2; b=1; tol =1.e -8; kmax =100;
[xmin ,fmin ,iter ]= golden(f,a,b,tol ,kmax )
Note that the tolerance for the stopping test is set to 10−8. After 42 iterations
we obtain xmin=-3.660253989004456e-01 and fmin=-1.194742596743503. The
method converges linearly (see (7.19)).
Using now the MATLAB command fminbnd with the instructions:
options =optimset (’TolX ’ ,1.e -8);
[xminf ,fminf ,exitflag ,output ]= fminbnd(f,a,b,options)
the same problem is solved by the golden section method with quadratic in-
terpolation. In this case convergence is achieved in 9 iterations to the point
xmin=-3.660254076197302e-01.
Exercise 7.2 Given γi(t) = (xi(t), yi(t)), for i = 1, 2, we need to minimize
the distance
d(t) =

(x1(t) −x2(t))2 + (y1(t) −y2(t))2
or, equivalently, its square as function of t. To solve this one dimensional min-
imum problem we can use the golden section method with quadratic interpo-
lation implemented in the function fminbnd. Using the following instructions
x1=@(t)7* cos(t/3+ pi /2)+5; y1=@(t)-4* sin(t/3+ pi /2) -3;
x2=@(t)6* cos(t/6-pi /3) -4; y2=@(t)-6* sin(t/6-pi /3)+5;
d=@(t)(x1(t)-x2(t))^2+( y1(t)-y2(t))^2;
ta =0; tb =20;
options =optimset (’TolX ’ ,1.e -8);
[tmin ,dmin ,exitflag ,output ]= fminbnd (d,ta ,tb ,options)
we converge after 10 iterations to the solution tmin=8.438731484275010. At
that time, the two ships stand at minimal distance dmin=5.691754805947144
nautical miles, eigth hours and a half after their departure.

10.7 Chapter 7
405
Exercise 7.3 We deﬁne the cost function and represent it together with its
contour lines on a circular domain centered at (-1,0) with radius 3 by the
following instructions:
fun=@(x) x(1)^4+x(2)^4+x(1)^3+3* x(1)*...
x(2)^2 -3* x(1)^2 -3*x (2)^2+10;
[r,theta]= meshgrid (0:.1:3 ,0: pi /25:2* pi );
x1=r.* cos(theta)-1; y1=r.* sin(theta);
[n,m]= size (x1);z1=zeros(n,m);
for i=1:n, for j=1:m
z1(i,j)= fun ([x1(i,j); y1(i,j)]);
end , end
figure (1);
clf;
p1=mesh (x1 ,y1 ,z1);
set(p1 ,’Edgecolor ’ ,[0,1,1]); hold on
contour (x1 ,y1 ,z1 ,100,’Linecolor ’ ,[0.8 ,0.8 ,0.8]);
By a direct inspection we see that the cost function features a local maximizer,
a saddle point and two global minimizers (being this function even with respect
to the x2 variable). Choosing x(0) = (−3, 0) and setting a tolerance ε = 10−8
for the stopping test, using the commands:
x0 =[ -3;0];
options =optimset (’TolX ’ ,1.e -8);
[xm ,fval ,exitf ,out ]= fminsearch (fun ,x0 ,options )
we ﬁnd the minimizer xm=[-2.1861e+00, 2.1861e+00] after 181 iterations
and having used 353 function evaluations. The second minimizer is therefore
xm=[-2.1861e+00, -2.1861e+00] because of the parity property of the func-
tion.
We warn the reader that choosing x0=[1;0], the fminsearch MATLAB
function converges to the local maximizer (.75000, .61237) instead than to
the minimizer, whereas the fminsearch Octave function still converges to the
minimizer (−2.1861, 2.1861).
Exercise 7.4 Let us write the sequence x(k+1) = x(k) + αkd(k) as
x(k+1) = x(0) +
k

ℓ=0
αℓd(ℓ).
Since x(0) = 3/2 we ﬁnd
x(k+1) = 3
2 +

2 +
2
3k+1

(−1)k+1 = 3
2 −2
k

ℓ=0
(−1)ℓ−1
2 −1
6

−1
3
k
= (−1)k+1

1 +
1
6 · 3k

.
Note that x(k) does not converge to zero eventhough the sequence f(x(k)) is
decreasing, as can be seen from Figure 10.8, left. When the points x(k) are
near to +1 and −1, the ﬁrst Wolfe condition (7.43) is not fulﬁlled since the
variation of f between two steps becomes inﬁnitesimal while the steplength is
about the same (circa 2).
Exercise 7.5 By proceeding as done in the previous Exercise, we ﬁnd x(0) =
−2 and x(k+1) = −2 + (1 −3−k)/2 →−3/2 when k →∞. Also in this case

406
10 Solutions of the exercises
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
0
1
2
3
4
5
6
x(k)
x(k+1)
-2.5
-2
-1.5
-1
-0.5
0
0.5
0
2
4
6
8
10
12
14
16
18
x(k)
Figure 10.8. At left, the sequence yielded by the descent method of Exercise
7.4. Taking x(k) ≃−1, the point (x(k+1), f(x(k+1))) should stay beneath the
blue straight line in order to satisfy the ﬁrst Wolfe’s condition with σ = 0.2;
on the contrary it lies largely above, indeed (x(k+1), f(x(k+1))) ≃(1, 1). At
right, the sequence generated for Exercise 7.5. The point (x(k+1), f(x(k+1)))
should stay at the right to the point where the blue straight line is tangent
to the blue curve in order for the second Wolfe’s condition with δ = 0.9 to be
satisifed; instead, it is close to (−1.5, 5.06)
the sequence of values f(x(k)) is decreasing as we can see in Figure 10.8, right.
When the points x(k) are close to −3/2, the second Wolfe’s condition (7.43) is
not satisﬁed as f ′(x(k+1)) (with its own sign) should be larger than δf ′(x(k)).
Exercise 7.6 After the following initializations
fun=@(x) 100*(x(2)-x(1)^2)^2+(1 -x(1))^2;
grad =@(x) [ -400*( x(2)-x(1)^2)* x(1) -2*(1 -x(1));
200*(x(2)-x(1)^2)];
hess =@(x) [ -400*x(2)+1200* x(1)^2+2 ,
-400*x(1);
-400*x(1),
200];
x0 =[ -1.2 ,1];
tol =1.e-8; kmax =500;
we call Program 7.3 using the following instructions:
meth =1; % Newton
[x1 ,err1 ,k1 ]= descent (fun ,grad ,x0 ,tol ,kmax ,meth ,hess );
meth =2; H0=eye(length(x0 )); % BFGS
[x2 ,err2 ,k2 ]= descent (fun ,grad ,x0 ,tol ,kmax ,meth ,H0);
meth =3; %gradient
[x3 ,err3 ,k3 ]= descent (fun ,grad ,x0 ,tol ,kmax ,meth );
meth =41; % FR
conjugate
gradient
[x41 ,err41 ,k41 ]= descent(fun ,grad ,x0 ,tol ,kmax ,meth );
meth =42; % PR
conjugate
gradient
[x42 ,err42 ,k42 ]= descent(fun ,grad ,x0 ,tol ,kmax ,meth );
meth =43; % HS
conjugate
gradient
[x43 ,err43 ,k43 ]= descent(fun ,grad ,x0 ,tol ,kmax ,meth );
All the methods converge to the same global minimizer (1, 1), precisely:
Newton:
k1
=
22,
err = 1.8652e-12
BFGS:
k2
=
35,
err = 1.7203e-09
Grad:
k3
= 352,
err = 8.1954e-09
CG-FR:
k41 = 284,
err = 5.6524e-10

10.7 Chapter 7
407
−5
−4
−3
−2
−1
0
1
2
3
4
5
−5
−4
−3
−2
−1
0
1
2
3
4
5
x1
x2
Figure 10.9. Contour lines comprised between the values 0 and 20 of the cost
function of Exercise 7.7
CG-PR:
k42 = 129,
err = 5.8148e-09
CG-HS:
k43 =
65,
err = 9.8300e-09
The number of iterations (k1, k2, ..., k43) is in accordance with the theo-
retical convergence rate of the various methods: quadratic for Newton, super-
linear for BFGS, linear for the others. The variable err contains the last value
of the error estimation used for the stopping test.
Exercise 7.7 By evaluating the function f(x) on the square [−5, 5]2 and
graphically representing the contour lines corresponding to the values within
the interval [0, 20], we see that it features a saddle point near (0, 0) and two
local minimizers, one (x2) close to (−1, −1), the other (x1) to (2, 2) (see Fig-
ure 10.9). (One of them will coincide with the global minimizer we are looking
for.) Using tol=1.e-5 as tolerance for the stopping test and 100 as maximum
number of iterations, we take delta0=0.5 as initial radius for the trust region
method implemented in Program 7.4. After having deﬁned the function han-
dle of the cost function and its gradient, we set meth=2 for both Programs
7.4 and 7.3 in such a way that they use quasi-Newton descent directions (and
hess=eye(2)). Choosing x0 = (2, −1), the trust-region method converges in
28 iterations to the point x1=(1.8171, 1.6510), while the BFGS method
converges in 27 iterations to the other local minimizer x2=(-5.3282e-01,
-5.8850e-01). Correspondingly, f(x1) ≃3.6661 and f(x2) ≃8.2226. Tak-
ing instead x(0) = (2, 1), both methods converge to the global minimizer x1 in
11 iterations.
Exercise 7.8 Computing the stationary points of ˜fk(x) = 1
2∥Rk(x)∥2 amo-
unts to solve the linear system
∇˜fk(x) = J 
Rk(x)T Rk(x) = 0.
(10.4)

408
10 Solutions of the exercises
Thanks to the deﬁnition (7.64), J 
Rk(x) = JR(x(k)) for all x ∈Rn and system
(10.4) becomes
JR(x(k))T R(x(k)) + JR(x(k))T JR(x(k))(x −x(k)) = 0,
that is (7.63).
Exercise 7.9 We must show that δx(k) fulﬁlls conditions (7.34). We recall
that for every rectangular matrix A having full rank, the square matrix AT A
is symmetric and positive deﬁnite.
Let us prove (7.34)2. From ∇f(x(k)) = JR(x(k))T R(x(k)), it follows that
∇f(x(k)) = 0 iﬀR(x(k)) = 0 (as JR(x(k)) has full rank) then δx(k) = 0 thanks
to (7.63)1.
Suppose now that R(x(k)) ̸= 0. Then
(δx(k))T ∇f(x(k)) =
−
)
JR(x(k))T JR(x(k))
*−1
JR(x(k))T R(x(k))
+T
JR(x(k))T R(x(k))
−
%
JR(x(k))T R(x(k))
&T )
JR(x(k))T JR(x(k))
*−1 %
JR(x(k))T R(x(k))
&
< 0,
that is (7.34)1 is fulﬁlled.
Exercise 7.10 Setting ri(x) = x1 + x2ti + x3t2
i + x4e−x5ti −yi, for i =
1, . . . , 8, the desired coeﬃcients x1, . . . , x5, are those for which the associated
function (7.61) attains its minimum. We call Program 7.5 using the following
instructions:
t= [0.055;0.181;0.245;0.342;0.419;0.465;0.593;0.752];
y= [2.80;1.76;1.61;1.21;1.25;1.13;0.52;0.28];
tol =1.e-12;
kmax =500;
x0 =[2,-2.5,-.2,5,35];
[x,err ,iter ]= gaussnewton (@mqnlr ,@mqnljr ,...
x0 ,tol ,kmax ,t,y);
where mqnlr and mqnljr are the functions which deﬁne R(x) and JR(x) re-
spectively:
function r=mqnlr(x,t,y)
m=length(t); n=length(x);
r=zeros(m,1);
for i=1:m
r(i)= sqrt (2)*(x(1)+ t(i)*x(2)+ t(i)^2* x(3)+...
x(4)* exp(-t(i)*x(5))-y(i));
end
function
jr=mqnljr(x,t,y)
m=length(t); n=length(x); jr=zeros(m,n);
for i=1:m
jr(i ,1)=1; jr(i ,2)= t(i);
jr(i ,3)= t(i)^2; jr(i,4)= exp(-t(i)*x(5));
jr(i,5)=-t(i)*x(4)* exp(-t(i)*x(5));
end
jr=jr*sqrt (2);

10.7 Chapter 7
409
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0
0.5
1
1.5
2
2.5
3
−15
−10
−5
0
5
10
15
−15
−10
−5
0
5
10
15
P1
P2
P3
Figure 10.10. Left: the data and the solution of the Exercise 7.10. Right:
the solution of the Exercise 7.12. The grey curves represent the contour lines
of the cost function. The admissibility domain Ω is the unbounded portion of
the plane beneath the blue straight line
After 19 iterations convergence is achieved to the point
x=[2.2058e+00 -2.4583e+00 -2.1182e-01 5.2106e+00 3.5733e+01].
At that point the residual is far from zero, actually f(x) = 1.8428e −01.
Nevertheless we can classify the given problem as a small residual problem,
therefore convergence is linear. For the same problem, Newton’s method (7.31)
converges in 8 iterations. If the initial point is not close enough to the mini-
mizer, e.g. if x0 = [1,1,1,1,10], the Gauss-Newton method fails to converge,
while the damped Gauss-Newton method converges in 21 iterations. In Fig-
ure 10.10, left, we plot function φ(t) whose coeﬃcients x1, . . . , x5 are those
computed numerically. The empty circles represent the distribution of data
(ti, yi).
Exercise 7.11 Starting from Φ(x) =
1
2∥R(x)∥2 a quadratic approximation
of Φ around x(k) reads
˜Φk(s) = Φ(x(k)) + sT ∇Φ(x(k)) + 1
2sT Hks
∀s ∈Rn,
where Hk is a suitable approximation of the Hessian of Φ. By exploiting (7.62)
and taking
Hk = JR(x(k))T JR(x(k)),
it holds
˜Φk(s) = 1
2∥R(x(k))∥2 + sT JR(x(k))T R(x(k)) + 1
2sT JR(x(k))T JR(x(k))s
= 1
2∥R(x(k)) + JR(x(k))s∥2
= 1
2∥˜Rk(x)∥2.
In conclusion, ˜Φk can be regarded as a quadratic model of Φ around x(k),
obtained by replacing R(x) with ˜Rk(x).

410
10 Solutions of the exercises
Exercise 7.12 We need to solve the minimization problem (7.2) with cost
function f(x, y) = 3
i=1 vi

(x −xi)2 + (y −yi)2 and admissibility domain
Ω = {(x, y) ∈R2 :
y ≤x −10}. The values vi represent the number of
journeys toward the selling point Pi.
We deﬁne ﬁrst the cost function and the constraint functions, then we call
Program penalty.m, using the following instructions:
x1 =[6;
3]; x2 =[ -9;9];
x3 =[ -8; -5]; v=[140;134;88];
d=@(x)v(1)* sqrt ((x(1)- x1 (1)).^2+( x(2)- x1 (2)).^2)+...
v(2)* sqrt ((x(1)- x2 (1)).^2+( x(2)- x2 (2)).^2)+...
v(3)* sqrt ((x(1)- x3 (1)).^2+( x(2)- x3 (2)).^2);
g=@(x)[x(1)-x(2) -10];
meth =0; x0 =[10; -10]; tol =1.e-8;
kmax =200; kmaxd =200;
[xmin ,err ,k]= penalty (d,[],[],[],g,[],x0 ,tol ,...
kmax ,kmaxd ,meth );
This program makes use of the penalty algorithm coupled with the Nelder
and Mead method for unconstrained minimization. We have not used descent
method since the cost function features non-diﬀerentiable points, moreover the
matrices Hk used for the direction d(k) may be ill-conditioned. The optimal lo-
cation where to place the warehouse has coordinates xmin=[6.7734,-3.2266].
Convergence is achieved after 13 iterations of the penalty method.
Exercise 7.13 Since no inequality constraint is present, problem can be
rewritten under the form (7.77) and then we can proceed as done in Example
7.14. Matrix C has rank 2 and its kernel ker(C) = {z = α[1, 1, 1]T α ∈R} has
dimension 1. Matrix A is symmetric; as 3
i,j=1 aij > 0, it is positive deﬁnite
when restricted to the kernel of C. We built matrix M = [A, −CT ; C, 0] and
the right hand side f = [−b, d]T , then we solve the linear system (7.77) using
the instructions:
A=[2,-1,1;-1,3,4;1,4,1]; b=[1; -2; -1];
C=[2,-2,0;2,1,-3]; d=[1;1];
M=[A -C’; C, zeros (2)]; f=[-b;d];
xl=M\f;
We obtain the solution
xl =
5.7143e-01
7.1429e-02
7.1429e-02
1.0476e+00
2.3810e-02
The ﬁrst 3 components of xl provide the approximation of the minimizer,
whereas the Lagrangian multipliers associated to the constraints are given by
the last components. The minimum value attained by the cost function is
6.9388e-01.
Exercise 7.14 We represent the function v(x, y) on the square [−2.5, 2.5]2
and its restriction to the curve h(x, y) = x2/4 + y2 −1 = 0 representing the
constraint in Figure 10.11. As we can see, several local maximizers exist, the
global one lying in a neighborhoud of the point (2,0.5).
We use the following instructions to call Program 7.7

10.8 Chapter 8
411
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−20
−10
0
10
20
30
40
(x1, v(x1))
(x2, v(x2))
x
y
v(x, y)
Figure 10.11. The function v(x, y) of Exercise 7.14 and the two maxima
computed using the augmented Lagrangian method
fun=@(x)-(sin(pi*x(1)* x(2))+1)*(2* x(1)+3*x(2)+4);
grad_fun =@(x)[-pi*x(2)* cos(pi*x(1)* x(2))*...
(2*x(1)+3*x(2)+4) -( sin(pi*x(1)* x(2))+1)*2;
-pi*x(1)* cos(pi*x(1)* x(2))*(2* x(1)+3*x(2)+4) -...
(sin(pi*x(1)* x(2))+1)*3];
h=@(x)x(1)^2/4+ x(2)^2 -1;
grad_h=@(x)[x(1)/2;2* x(2)];
x0 =[1;0];
lambda0 =1; tol =1.e-8;
kmax =100; kmaxd =100;
meth =2; hess =eye (2);
[x,err ,k]= auglagrange (fun ,grad_fun ,h,grad_h ,...
x0 ,lambda0 ,tol ,kmax ,kmaxd ,meth ,hess )
To solve the unconstrained minimization problem for the function f(x, y) =
−v(x, y) inside the augmented Lagrangian method, we use BFGS method.
Choosing x(0) = (1, 0), convergence is achieved in 6 iterations to the point
x1 = (0.56833, 0.95877). The latter is a maximizer but not the global one, as
Figure 10.11 shows. Choosing instead x(0) = (2, 1) we obtain convergence (in 5
iterations) to the point x2 = (1.9242, 0.27265); note that v(x1) = 15.94 while
v(x2) = 17.307, x2 is therefore the global maximizer.
10.8 Chapter 8
Exercise 8.1 Let us approximate the exact solution y(t) =
1
2[et −sin(t) −
cos(t)] of the Cauchy problem (8.85) by the forward Euler method using dif-
ferent values of h: 1/2, 1/4, 1/8, . . . , 1/512. The associated error is computed
by the following instructions:
t0 =0; y0 =0; T=1; f=@(t,y) sin(t)+y;
y=@(t) 0.5*(exp(t)-sin(t)-cos(t));
Nh =2;
for k=1:10;
[tt ,u]= feuler(f,[t0 ,T],y0 ,Nh);
e(k)= max(abs(u-y(tt ))); Nh =2* Nh;

412
10 Solutions of the exercises
end
Now we apply formula (1.12) to estimate the order of convergence:
p=log(abs(e(1:end -1)./e(2: end )))/ log (2); p(1:2: end)
p =
0.7696
0.9273
0.9806
0.9951
0.9988
As expected the order of convergence is one. With the same instructions (sub-
stituting the call to Program 8.1 with that to Program 8.2) we obtain an
estimate of the convergence order of the backward Euler method:
p=log(abs(e(1:end -1)./e(2: end )))/ log (2); p(1:2: end)
p =
1.5199
1.0881
1.0204
1.0050
1.0012
Exercise 8.2 The numerical solution of the given Cauchy problem by the
forward Euler method can be obtained as follows:
t0 =0; T=1; N=100; f=@(t,y) -t*exp(-y);
y0 =0;[t,u]= feuler(f,[t0 ,T],y0 ,N);
To compute the number of exact signiﬁcant digits we can estimate the
constants L and M which appear in (8.13). Note that, since f(t, y(t)) < 0 in the
given interval, y(t) is a monotonically decreasing function, vanishing at t = 0.
Since f is continuous together with its ﬁrst derivative, we can approximate L
as L = max0≤t≤1 |L(t)| with L(t) = ∂f/∂y = te−y. Note that L(0) = 0 and
L′(t) > 0 for all t ∈(0, 1]. Thus, by using the assumption −1 < y < 0, we can
take L = e.
Similarly, in order to compute M = max0≤t≤1 |y′′(t)| with y′′ = −e−y −
t2e−2y, we can observe that this function has its maximum at t = 1, and then
M = e + e2. We can draw these conclusions by analyzing the graph of the
vector ﬁeld v(t, y) = [v1, v2]T = [1, f(t, y(t))]T associated to the given Cauchy
problem. Indeed, the solutions of the diﬀerential equation y′(t) = f(t, y(t)) are
tangential to the vector ﬁeld v. By the following instructions:
[T,Y]= meshgrid (0:0.05:1 , -1:0.05:0);
V1=ones (size (T)); V2=-T.* exp(Y);
quiver(T,Y,V1 ,V2)
we see that the solution of the Cauchy problem has a nonpositive second
derivative whose absolute value grows up with t. This fact leads us to conclude
that M = max0≤t≤1 |y′′(t)| is reached at t = 1.
The same conclusions can be drawn by noticing that the function −y is positive
and increasing, since y ∈[−1, 0] and f(t, y) = y′ < 0. Thus, also the functions
e−y and t2e−2y are positive and increasing, while the function y′′ = −e−y −
t2e−2y is negative and decreasing. It follows that M = max0≤t≤1 |y′′(t)| is
obtained at t = 1.
From (8.13), for h = 0.01 we deduce
|u100 −y(1)| ≤eL −1
L
M
200 ≃0.26.
Therefore, there is no guarantee that more than one signiﬁcant digit be exact.
Indeed, we ﬁnd u(end)=-0.6785, while the exact solution (y(t) = log(1−t2/2))
at t = 1 is y(1) = −0.6931.

10.8 Chapter 8
413
Exercise 8.3 The iteration function is φ(u) = u −htn+1e−u and the ﬁxed-
point iteration converges if |φ′(u)| < 1. This property is ensured if h(t0 +
(n + 1)h) < eu. If we substitute u with the exact solution, we can provide
an a priori estimate of the value of h. The most restrictive situation occurs
when u = −1 (see Solution 8.2). In this case the solution of the inequality
(n + 1)h2 < e−1 is h <

e−1/(n + 1).
Exercise 8.4 We repeat the same set of instructions of Solution 8.1, however
now we use the program cranknic (Program 8.3) instead of feuler. According
to the theory, we obtain the following result that shows second-order conver-
gence:
p=log(abs(e(1:end -1)./e(2: end )))/ log (2); p(1:2: end)
p =
2.0379
2.0023
2.0001
2.0000
2.0000
Exercise 8.5 Consider the integral formulation of the Cauchy problem (8.5)
in the interval [tn, tn+1]:
y(tn+1) −y(tn) =
tn+1

tn
f(τ, y(τ))dτ
≃h
2 [f(tn, y(tn)) + f(tn+1, y(tn+1))] ,
where we have approximated the integral by the trapezoidal formula (4.19).
By setting u0 = y(t0) and deﬁning un+1 as
un+1 = un + h
2 [f(tn, un) + f(tn+1, un+1)] ,
∀n ≥0,
we obtain precisely the Crank-Nicolson method.
Exercise 8.6 We know that the absolute stability region for the forward Euler
method is the circle centered at (−1, 0) with radius equal to 1, that is the set
A = {z = hλ ∈C : |1 + hλ| < 1}. By replacing λ = −1 + i we obtain the
bound on h: h2 −h < 0, i.e. h ∈(0, 1).
Exercise 8.7 Let us rewrite the Heun method in the following (Runge-Kutta
like) form:
un+1 = un + h
2 (K1 + K2),
K1 = f(tn, un),
K2 = f(tn+1, un + hK1).
(10.5)
We have hτn+1(h) = y(tn+1) −y(tn) −h( 
K1 + 
K2)/2, with 
K1 = f(tn, y(tn))
and 
K2 = f(tn+1, y(tn) + h 
K1). Since f is continuous with respect to both
arguments, it holds
lim
h→0τn+1 = y′(tn) −1
2[f(tn, y(tn)) + f(tn, y(tn))] = 0.

414
10 Solutions of the exercises
Therefore, the Heun method is consistent. We prove now that τn+1 is an in-
ﬁnitesimal of second order with respect to h. Suppose that y ∈C3([t0, T [). For
simplicity of notations, we set yn = y(tn) for any n ≥0. We have
τn+1 = yn+1 −yn
h
−1
2 [f(tn, yn) + f(tn+1, yn + hf(tn, yn))]
= yn+1 −yn
h
−1
2y′(tn) −1
2f(tn+1, yn + hy′(tn)).
Thanks to the error formula (4.20) related to the trapezoidal rule there exists
ξn ∈]tn, tn+1[ such that
yn+1 −yn =
 tn+1
tn
y′(t)dt = h
2
,
y′(tn) + y′(tn+1)
-
−h3
12 y′′′(ξn),
therefore
τn+1 = 1
2

y′(tn+1) −f(tn+1, yn + hy′(tn)) −h2
6 y′′′(ξn)

= 1
2

f(tn+1, yn+1) −f(tn+1, yn + hy′(tn)) −h2
6 y′′′(ξn)

.
Moreover, as the function f is Lipschitz continuous with respect to the second
variable (see Proposition 8.1), it holds
|τn+1| ≤L
2 |yn+1 −yn −hy′(tn)| + h2
12 |y′′′(ξn)|.
Finally, by applying the Taylor formula
yn+1 = yn + hy′(tn) + h2
2 y′′(ηn),
ηn ∈]tn, tn+1[,
we obtain
|τn+1| ≤L
4 h2|y′′(ηn)| + h2
12 |y′′′(ξn)| ≤Ch2.
The Heun method is implemented in Program 10.2. Using this program,
we can verify the order of convergence as in Solution 8.1. Precisely, by the
following instructions, we ﬁnd that the Heun method is second-order accurate
with respect to h
p=log(abs(e(1:end -1)./e(2: end )))/ log (2); p(1:2: end)
ans =
1.7642
1.9398
1.9851
1.9963
1.9991
Program 10.2. rk2: Heun (or RK2) method
function [tt ,u]= rk2(odefun ,tspan ,y0 ,Nh ,varargin )
tt=linspace (tspan(1), tspan(2), Nh +1);
h=( tspan(2)- tspan (1))/Nh;
hh=h*0.5;
u=y0;
for t=tt (1:end -1)
y = u(end ,:);
k1=odefun(t,y,varargin {:});

10.8 Chapter 8
415
t1 = t + h; y = y + h*k1;
k2=odefun(t1 ,y,varargin {:});
u = [u; u(end ,:) + hh*( k1+k2 )];
end
tt=tt ’;
Exercise 8.8 Applying the method (10.5) to the model problem (8.28) we ob-
tain K1 = λun and K2 = λun(1+hλ). Therefore un+1 = un[1+hλ+(hλ)2/2] =
unp2(hλ). To ensure absolute stability we must require that |p2(hλ)| < 1,
which is equivalent to 0 < p2(hλ) < 1, since p2(hλ) is positive. Solving the
latter inequality, we obtain −2 < hλ < 0, that is, h < 2/|λ|, since λ is a real
negative number.
Exercise 8.9 We prove the property (8.34), that we call for simplicity Pn,
by induction on n. To this aim, it is suﬃcient to prove that if P1 holds and if
Pn−1 implies Pn for any n ≥2, then Pn holds for any n ≥2.
It is easily veriﬁed that u1 = u0 +h(λ0u0 +r0). In order to prove that Pn−1 ⇒
Pn, it is suﬃcient to note that un = un−1(1 + hλn−1) + hrn−1.
Exercise 8.10 Since |1 + hλ| < 1, from (8.38) it follows
|zn −un| ≤|ρ|
''''1 + 1
λ
'''' +
''''
1
λ
''''

.
If λ ≤−1, we have 1/λ < 0 and 1 + 1/λ ≥0, then
''''1 + 1
λ
'''' +
''''
1
λ
'''' = 1 + 1
λ −1
λ = 1 = ϕ(λ).
On the other hand, if −1 < λ < 0, we have 1/λ < 1 + 1/λ < 0, then
''''1 + 1
λ
'''' +
''''
1
λ
'''' = −1 −2
λ =
''''1 + 2
λ
'''' = ϕ(λ).
Exercise 8.11 From (8.36) we have
|zn −un| ≤ρ[a(h)]n + hρ
n−1

k=0
[a(h)]n−k−1.
The result follows using (8.37).
Exercise 8.12 We have
hτn+1(h) = y(tn+1) −y(tn) −h
6 ( 
K1 + 4 
K2 + 
K3),

K1 = f(tn, y(tn)),

K2 = f(tn + h
2 , y(tn) + h
2 
K1),

K3 = f(tn+1, y(tn) + h(2 
K2 −
K1)).
Since f is continuous with respect to both arguments, we obtain

416
10 Solutions of the exercises
lim
h→0τn+1 = y′(tn) −1
6[f(tn, y(tn)) + 4f(tn, y(tn)) + f(tn, y(tn))] = 0,
which proves that the method is consistent.
This method is an explicit Runge-Kutta method of order 3 and is imple-
mented in Program 10.3. As in Solution 8.7, we can derive an estimate of its
order of convergence by the following instructions:
p=log(abs(e(1:end -1)./e(2: end )))/ log (2); p(1:2: end)
ans =
2.7306
2.9330
2.9833
2.9958
2.9990
Program 10.3. rk3: explicit Runge-Kutta method of order 3
function [tt ,u]= rk3(odefun ,tspan ,y0 ,Nh ,varargin );
tt=linspace (tspan(1), tspan(2), Nh +1);
h=( tspan(2)- tspan (1))/Nh; hh=h*0.5; h2 =2*h;
u=y0; h6=h/6;
for t=tt (1:end -1)
y = u(end ,:);
k1=odefun(t,y,varargin {:});
t1 = t + hh; y1 = y + hh* k1;
k2=odefun(t1 ,y1 ,varargin {:});
t1 = t + h; y1 = y + h*(2*k2 -k1);
k3=odefun(t1 ,y1 ,varargin {:});
u = [u; u(end ,:) + h6*( k1+4* k2+k3 )];
end
tt=tt ’;
Exercise 8.13 By following the same arguments used in Solution 8.8, we
obtain the relation
un+1 = un[1 + hλ + 1
2(hλ)2 + 1
6(hλ)3] = unp3(hλ).
By inspection of the graph of p3, obtained with the instruction
c=[1/6 1/2 1 1]; z=[ -3:0.01:1];
p= polyval(c,z); plot (z,abs(p))
we deduce that |p3(hλ)| < 1, provided that −2.5 < hλ < 0.
Exercise 8.14 The method (8.87) applied to the model problem (8.28) with
λ ∈R−gives the equation un+1 = un(1+hλ+(hλ)2). By solving the inequality
|1 + hλ + (hλ)2| < 1 we ﬁnd −1 < hλ < 0.
Exercise 8.15 To solve Problem 8.1 with the given values, we repeat the
following instructions with N=10 and N=20:
f=@(t,y)
-1.68e-9*y^4+2.6880;
[tc ,uc]= cranknic (f ,[0 ,200] ,180 , N);
[tp ,up]= rk2(f,[0 ,200] ,180 , N);
The graphs of the computed solutions are shown in Figure 10.12.

10.8 Chapter 8
417
0
50
100
150
200
180
182
184
186
188
190
192
194
196
198
200
0
50
100
150
200
180
182
184
186
188
190
192
194
196
198
200
Figure 10.12. Computed solutions with N = 10 (left) and N = 20 (right) for
the Cauchy problem of Solution 8.15: the solutions computed by the Crank-
Nicolson method (solid line), and by the Heun method (dashed line)
Exercise 8.16 Heun method applied to the model problem (8.28), gives
un+1 = un

1 + hλ + 1
2h2λ2

.
In the complex plane the boundary of the region of absolute stability is the set
of points hλ = x+iy such that |1+hλ+h2λ2/2|2 = 1. This equation is satisﬁed
by the pairs (x, y) such that f(x, y) = x4+y4+2x2y2+4x3+4xy2+8x2+8x = 0.
We can represent this curve as the 0-contour line of the function z = f(x, y).
This can be done by means of the following instructions:
f=@(x,y)[x.^4+y.^4+2*( x.^2).*( y.^2)+...
4*x.*y.^2+4*x.^3+8*x.^2+8*x];
[x,y]= meshgrid ([ -2.1:0.1:0.1] ,[ -2:0.1:2]);
contour (x,y,f(x,y),[0
0]);
grid on
The command meshgrid draws in the rectangle [−2.1, 0.1] × [−2, 2] a grid
with 23 equispaced nodes in the x-direction, and 41 equispaced nodes in the
y-direction. With the command contour we plot the contour line of f(x, y)
contour
corresponding to the value z = 0 (made precise in the input vector [0 0]
of contour). In Figure 10.13 the solid line delimitates the region of absolute
stability of the Heun method. This region is larger than the absolute stability
region of the forward Euler method (which corresponds to the interior of the
dashed circle). Both curves are tangent to the imaginary axis at the origin
(0, 0).
Exercise 8.17 We use the following instructions:
t0 =0; y0 =0; f=@(t,y)cos (2*y);
y=@(t) 0.5* asin (( exp (4*t) -1)./( exp (4*t)+1));
T=1; N=2; for k=1:10;
[tt ,u]= rk2(f,[t0 ,T],y0 ,N);
e(k)= max(abs(u-y(tt ))); N=2*N; end
p=log(abs(e(1:end -1)./e(2: end )))/ log (2); p(1:2: end)
2.4733
2.1223
2.0298
2.0074
2.0018

418
10 Solutions of the exercises
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Figure 10.13. Boundaries of the regions of absolute stability for the Heun
method (solid line) and the forward Euler method (dashed line). The corre-
sponding regions lie at the interior of the boundaries
As expected, we ﬁnd that the order of convergence of the method is 2. However,
the computational cost is comparable with that of the forward Euler method,
which is ﬁrst-order accurate only.
Exercise 8.18 The second-order diﬀerential equation of this exercise is equiv-
alent to the following ﬁrst-order system:
x′(t) = z(t),
z′(t) = −5z(t) −6x(t),
with x(0) = 1, z(0) = 0. We use the Heun method as follows:
t0 =0; y0 =[1 0]; T=5;
[t,u]= rk2(@fspring ,[t0 ,T],y0 ,N);
where N is the number of nodes and fspring.m is the following function:
function
fn=fspring (t,y)
b=5;
k=6;
[n,m]= size (y);
fn=zeros(n,m);
fn (1)= y(2);
fn (2)=-b*y(2)-k*y(1);
In Figure 10.14 we show the graphs of the two components of the solution,
computed with N=20 and N=40 and compare them with the graph of the exact
solution x(t) = 3e−2t −2e−3t and that of its ﬁrst derivative.
Exercise 8.19 The second-order system of diﬀerential equations is reduced
to the following ﬁrst-order system:
⎧
⎪
⎪
⎨
⎪
⎪
⎩
x′(t) = z(t),
y′(t) = v(t),
z′(t) = 2ω sin(Ψ)v(t) −k2x(t),
v′(t) = −2ω sin(Ψ)z(t) −k2y(t).
(10.6)
If we suppose that the pendulum at the initial time t0 = 0 is at rest in the
position (1, 0), the system (10.6) must be given the following initial conditions:

10.8 Chapter 8
419
0
1
2
3
4
5
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Figure 10.14. Approximations of x(t) (solid line) and x′(t) (dashed line)
computed with N=20 (at left) and N=40 (at right). Small circles and squares
refer to the exact functions x(t) and x′(t), respectively
x(0) = 1, y(0) = 0, z(0) = 0, v(0) = 0.
Setting Ψ = π/4, which is the average latitude of the Northern Italy, we use
the forward Euler method as follows:
[t,u]= feuler(@ffoucault ,[0 ,300] ,[1
0 0 0],N);
where N is the number of steps and ffoucault.m is the following function:
function
fn=ffoucault (t,y)
l=20; k2 =9.8/l; psi=pi /4; omega =7.29*1.e -05;
[n,m]= size (y); fn=zeros(n,m);
fn (1)= y(3);
fn (2)= y(4);
fn (3)=2* omega*sin(psi)*y(4)- k2*y(1);
fn (4)= -2* omega*sin(psi)*y(3)- k2*y(2);
By some numerical experiments we conclude that the forward Euler method
cannot produce acceptable solutions for this problem even for very small h.
For instance, on the left of Figure 10.15 we show the graph, in the phase
plane (x, y), of the motion of the pendulum computed with N=30000, that is,
h = 1/100. As expected, the rotation plane changes with time, but also the am-
plitude of the oscillations increases. Similar results can be obtained for smaller
h and using the Heun method. In fact, the model problem corresponding to
the problem at hand has a coeﬃcient λ that is purely imaginary. The corre-
sponding solution (a sine function) is bounded for any t, however it doesn’t
tend to zero.
Unfortunately, both the forward Euler and Heun methods feature a region
of absolute stability that doesn’t include any point of the imaginary axis (with
the exception of the origin). Thus, to ensure the absolute stability one should
choose the prohibited value h = 0.
To get an acceptable solution we should use a method whose region of
absolute stability includes a portion of the imaginary axis. This is the case,
for instance, for the adaptive Runge-Kutta method of order 3, implemented in
the MATLAB function ode23. We can invoke it by the following command:
[t,u]= ode23(@ffoucault ,[0 ,300] ,[1
0 0 0]);

420
10 Solutions of the exercises
−3
−2
−1
0
1
2
3
−0.04
−0.03
−0.02
−0.01
0
0.01
0.02
0.03
0.04
−1
−0.5
0
0.5
1
−0.015
−0.01
−0.005
0
0.005
0.01
0.015
Figure 10.15. Trajectories on the phase plane for the Foucault pendulum of
Solution 8.19 computed by the forward Euler method (left) and the third-order
adaptive Runge-Kutta method (right)
In Figure 10.15 (right) we show the solution obtained using only 1022 inte-
gration steps. Note that the numerical solution is in good agreement with the
exact one.
Exercise 8.20 We ﬁx the right hand side of the problem in the following
function
function
fn=baseball (t,y)
phi = pi /180;
omega = 1800*1.047198 e-01;
B = 4.1*1.e-4; g = 9.8;
[n,m]= size (y); fn=zeros(n,m);
vmodule = sqrt(y(4)^2+y(5)^2+y(6)^2);
Fv = 0.0039+0.0058/(1+ exp (( vmodule -35)/5));
fn (1)= y(4);
fn (2)= y(5);
fn (3)= y(6);
fn (4)=-Fv*vmodule *y(4)+...
B*omega*(y(6)* sin(phi)-y(5)* cos(phi ));
fn (5)=-Fv*vmodule *y(5)+ B*omega*y(4)* cos(phi );
fn (6)=-g-Fv*vmodule *y(6)-B*omega*y(4)* sin(phi );
At this point we only need to recall ode23 as follows:
[t,u]= ode23(@baseball ,[0 0.4] ,...
[0 0 0 38* cos(pi /180) 0 38* sin(pi /180)]);
Using command find we approximately compute the time at which the altitude
becomes negative, which corresponds to the exact time of impact with the
ground:
n=max(find (u(: ,3) >=0));
t(n)
ans =
0.1066
In Figure 10.16 we report the trajectories of the baseball with an inclination
of 1 and 3 degrees represented on the plane x1x3 and on the x1x2x3 space,
respectively.
Exercise 8.21 Let us deﬁne the function

10.8 Chapter 8
421
0
5
10
15
−0.6
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
x1
x3
0
5
10
0
0.1
0.2
0.3
0.4
−0.6
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
x1
x2
x3
Figure 10.16. The trajectories followed by a baseball launched with an initial
angle of 1 degree (solid line) and 3 degrees (dashed line), respectively
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
y1
y2
y3
Figure 10.17. Trajectories of the model (8.88) corresponding to several intial
data and with ε = 10−2
function f=fchem3(t,y)
e=1.e -2;
[n,m]= size (y);f=zeros(n,m);
f(1)=1/e*(-5*y(1)-y(1)* y(2)+5*y(2)^2+...
y(3))+y(2)* y(3)-y(1);
f(2)=1/e*(10*y(1)-y(1)* y(2) -10* y(2)^2+y(3))...
-y(2)* y(3)+ y(1);
f(3)=1/e*(y(1)* y(2)-y(3))-y(2)* y(3)+ y(1);
and execute the following instructions
y0 =[1 ,0.5 ,0];
tspan =[0 ,10];
[t1 ,y1]= ode23(@fchem3 ,tspan ,y0);
[t2 ,y2]= ode23s(@fchem3 ,tspan ,y0);
fprintf (’Passi ode23=%d, passi ode23s =%d\n’ ,...
length(t1),length(t2))
ode23 requires 8999 steps while ode23s only 43. Consequently we can state
that the given problem is stiﬀ. The computed numerical solutions are shown
in Figure 10.17.

422
10 Solutions of the exercises
10.9 Chapter 9
Exercise 9.1 We can verify directly that xT Ax > 0 for all x ̸= 0. Indeed,
[x1 x2 . . . xN−1 xN]
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
2 −1 0 . . . 0
−1 2
...
...
0
... ... −1 0
...
−1 2 −1
0 . . . 0 −1 2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎣
x1
x2
...
xN−1
xN
⎤
⎥⎥⎥⎥⎥⎦
= 2x2
1 −2x1x2 + 2x2
2 −2x2x3 + . . . −2xN−1xN + 2x2
N.
The last expression is equivalent to (x1−x2)2+. . .+(xN−1−xN)2+x2
1+x2
N,
which is positive, provided that at least one xi is non-null.
Exercise 9.2 We verify that Aqj = λjqj. Computing the matrix-vector prod-
uct w = Aqj and requiring that w is equal to the vector λjqj, we ﬁnd:
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
2 sin(jθ) −sin(2jθ) = 2(1 −cos(jθ)) sin(jθ),
−sin(j(k −1)θ) + 2 sin(jkθ) −sin(j(k + 1)θ) = 2(1 −cos(jθ)) sin(kjθ),
k = 2, . . . , N −1
2 sin(Njθ) −sin((N −1)jθ) = 2(1 −cos(jθ)) sin(Njθ).
The ﬁrst equation is an identity since sin(2jθ) = 2 sin(jθ) cos(jθ). The other
equations can be simpliﬁed in view of the sum-to-product formula
sin((k −1)jθ) + sin((k + 1)jθ) = 2 sin(kjθ) cos(jθ)
and noticing that sin((N +1)jθ) = 0 since θ = π/(N +1). Since A is symmetric
and positive deﬁnite, its condition number is K(A) = λmax/λmin, that is,
K(A) = λN/λ1 = (1−cos(Nπ/(N +1)))/(1−cos(π/(N +1))). By the identity
cos(Nπ/(N + 1)) = −cos(π/(N + 1)) and by using the Taylor expansion of
order 2 of the cosine function, we obtain K(A) ≃(N+1)2, that is, K(A) ≃h−2.
Exercise 9.3 We note that
u(¯x + h) = u(¯x) + hu′(¯x) + h2
2 u′′(¯x) + h3
6 u′′′(¯x) + h4
24 u(4)(ξ+),
u(¯x −h) = u(¯x) −hu′(¯x) + h2
2 u′′(¯x) −h3
6 u′′′(¯x) + h4
24 u(4)(ξ−),
where ξ+ ∈(x, x + h) and ξ−∈(x −h, x). Summing the two expression we
obtain
u(¯x + h) + u(¯x −h) = 2u(¯x) + h2u′′(¯x) + h4
24 (u(4)(ξ+) + u(4)(ξ−)),
which is the desired property.

10.9 Chapter 9
423
Exercise 9.4 The matrix is tridiagonal with entries ai,i−1 = −μ/h2 −η/(2h),
aii = 2μ/h2 + σ, ai,i+1 = −μ/h2 + η/(2h). The right-hand side, accounting
for the boundary conditions, becomes f = (f(x1) + α(μ/h2 + η/(2h)), f(x2),
. . . , f(xN−1), f(xN) + β(μ/h2 −η/(2h)))T .
Exercise 9.5 With the following instructions we compute the corresponding
solutions to the three given values of h:
f=@(x) 1+ sin (4* pi*x);
[x,uh11 ]= bvp (0,1,9,1,0,0.1, f,0 ,0);
[x,uh21 ]= bvp (0,1,19,1,0,0.1, f ,0 ,0);
[x,uh41 ]= bvp (0,1,39,1,0,0.1, f ,0 ,0);
We recall that h = (b −a)/(N + 1). Since we don’t know the exact solution,
to estimate the convergence order we compute an approximate solution on a
very ﬁne grid (for instance h = 1/1000), then we use this latter as a surrogate
for the exact solution. We ﬁnd:
[x,uhex ]= bvp (0,1,999,1,0,0.1,f ,0 ,0);
max(abs(uh11 -uhex (1:100: end )))
ans =
8.6782e-04
max(abs(uh21 -uhex (1:50: end )))
ans =
2.0422e-04
max(abs(uh41 -uhex (1:25: end )))
ans =
5.2789e-05
Halving h, the error is divided by 4, proving that the convergence order with
respect to h is 2.
Exercise 9.6 We should modify the Program 9.1 in order to impose Neumann
boundary conditions. In the Program 10.4 we show one possible implementa-
tion.
Program 10.4. neumann: numerical solution of a Neumann boundary-value
problem
function [xh ,uh]= neumann(a,b,N,mu ,eta ,sigma ,bvpfun ,...
ua ,ub ,varargin )
h = (b-a)/(N+1); xh = (linspace (a,b,N+2)) ’;
hm = mu/h^2;
hd = eta /(2* h); e =ones (N+2 ,1);
A = spdiags ([-hm*e-hd (2* hm+sigma)*e -hm*e+hd],...
-1:1, N+2, N+2);
A(1 ,1)=3/(2*h); A(1 ,2)= -2/h; A(1 ,3)=1/(2*h); f(1)= ua;
A(N+2,N+2)=3/(2* h); A(N+2,N+1)= -2/ h; A(N+2,N)=1/(2*h);
f =bvpfun(xh ,varargin {:});
f(1)= ua; f(N+2)= ub;
uh = A\f;
Exercise 9.7 The trapezoidal integration formula, used on the two subinter-
vals Ij−1 and Ij, produces the following approximation

424
10 Solutions of the exercises
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 10.18. The contour lines of the computed temperature for Δx = Δy =
1/10 (dashed lines) and for Δx = Δy = 1/80 (solid lines)
f

Ij−1∪Ij
(x)ϕj(x) dx ≃h
2 f(xj) + h
2 f(xj) = hf(xj),
since ϕj(xi) = δij for any i, j. When j = 1 or j = N we can proceed similarly,
taking into account the Dirichlet boundary conditions. Thus, we obtain the
same right-hand side of the ﬁnite diﬀerence system (9.14) up to the factor h.
Exercise 9.8 We have ∇φ = (∂φ/∂x, ∂φ/∂y)T and therefore div∇φ =
∂2φ/∂x2 + ∂2φ/∂y2, that is, the Laplacian of φ.
Exercise 9.9 To compute the temperature at the center of the plate, we solve
the corresponding Poisson problem for various values of Δx = Δy, using the
following instructions:
k=0; fun=@(x,y) 25+0*x+0*y;
bound=@(x,y) (x==1);
for N = [10 ,20 ,40 ,80 ,160]
[xh ,yh ,uh]= poissonfd (0,1,0,1,N,N,fun ,bound);
k=k+1; uc(k) = uh(N/2+1,N/2+1);
end
The components of the vector uc are the values of the computed temperature
at the center of the plate as the steplength h of the grid decreases. We have
uc
2.0168
2.0616
2.0789
2.0859
2.0890
We can therefore conclude that at the center of the plate the temperature is
about 2.08◦C. In Figure 10.18 we show the contour lines of the temperature
for two diﬀerent values of h.
Exercise 9.10 For sake of simplicity we set ut = ∂u/∂t and ux = ∂u/∂x. We
multiply by ut the equation (9.72) with f ≡0, integrate in space on (a, b) and
use integration by parts on the second term:

10.9 Chapter 9
425
 b
a
utt(x, t)ut(x, t)dx + c
 b
a
ux(x, t)utx(x, t)dx −c[ux(x, t)ut(x, t)]b
a = 0.
(10.7)
Now we integrate in time equation (10.7), from 0 up to t. By noticing that
uttut = 1
2(u2
t )t and that uxuxt = 1
2(u2
x)t, by applying the fundamental theorem
of integral calculus and recalling the initial conditions (9.74) (that is ut(x, 0) =
v0(x) and ux(x, 0) = u0x(x)), we obtain
 b
a
u2
t(x, t)dx + c
 b
a
u2
x(x, t)dx =
 b
a
v2
0(x)dx
+c
 b
a
u2
0x(x)dx + 2c
 t
0
(ux(b, s)ut(b, s) −ux(a, s)ut(a, s)) ds.
On the other hand, by integrating by parts and applying the homogeneous
Dirichlet boundary conditions for t > 0 and on the initial data we obtain
 t
0
(ux(b, s)ut(b, s) −ux(a, s)ut(a, s))ds = 0.
Then (9.83) follows.
Exercise 9.11 In view of deﬁnition (9.64) it is suﬃcient to verify that
∞

j=−∞
|un+1
j
|2 ≤
∞

j=−∞
|un
j |2.
(10.8)
In formula (9.62), let us move all terms to the left-hand side and then multiply
by un+1
j
. Owing to the identity 2(a −b)a = a2 −b2 + (a −b)2 we have
|un+1
j
|2 −|un
j |2 + |un+1
j
−un
j |2 + λa(un+1
j+1 −un+1
j−1 )un+1
j
= 0,
then, summing up on j and noticing that ∞
j=−∞(un+1
j+1 −un+1
j−1 )un+1
j
= 0, we
obtain
∞

j=−∞
|un+1
j
|2 ≤
∞

j=−∞
|un+1
j
|2 +
∞

j=−∞
|un+1
j
−un
j |2 ≤
∞

j=−∞
|un
j |2.
Exercise 9.12 The upwind scheme (9.59) can be rewritten in the simpliﬁed
form
un+1
j
=
 (1 −λa)un
j + λaun
j−1 if a > 0
(1 + λa)un
j −λaun
j+1 if a < 0.
Let us ﬁrst consider the case a > 0. If the CFL condition is satisﬁed, then
both coeﬃcients (1 −λa) and λa are positive and less than 1.
This fact implies that
min{un
j−1, un
j } ≤un+1
j
≤max{un
j−1, un
j }
and, by recursion on n, it holds

426
10 Solutions of the exercises
inf
l∈Z{u0
l } ≤un+1
j
≤sup
l∈Z
{u0
l }
∀n ≥0,
from which the estimate (9.85) follows.
When a < 0, using again the CFL condition, both coeﬃcients (1 + λa) and
−λa are positive and less than 1. By proceeding as we did before, the estimate
(9.85) follows also in this case.
Exercise 9.13 To numerically solve problem (9.47) we call the Program 10.5.
Note that the exact solution is the travelling wave with velocity a = 1, that is
u(x, t) = 2 cos(4π(x −t)) + sin(20π(x −t)). Since the CFL number is ﬁxed to
0.5, the discretization parameters Δx and Δt are related through the equation
Δt = CFL · Δx, thus we can arbitrarily choose only one of them.
In order to verify the accuracy of the scheme with respect to Δt we can use
the following instructions:
xspan =[0 ,0.5];
tspan =[0 ,1];
a=1; cfl =0.5;
u0=@(x) 2* cos (4* pi*x)+ sin (20* pi*x);
uex=@(x,t) 2*cos (4*pi*(x-t))+ sin (20* pi *(x-t));
ul=@(t) 2* cos (4* pi*t)-sin (20* pi*t);
DT =[1.e-2,5.e-3,2.e-3,1.e-3,5.e-4,2.e-4,1.e -4];
e_lw =[];
e_up =[];
for
deltat=DT
deltax=deltat*a/cfl;
[xx ,tt ,u_lw ]= hyper(xspan ,tspan ,u0 ,ul ,2 ,...
cfl ,deltax ,deltat );
[xx ,tt ,u_up ]= hyper(xspan ,tspan ,u0 ,ul ,3 ,...
cfl ,deltax ,deltat );
U=uex(xx ,tt(end ));
[Nx ,Nt]= size (u_lw );
e_lw =[ e_lw
sqrt (deltax )* norm(u_lw (Nx ,:)-U ,2)];
e_up =[ e_up
sqrt (deltax )* norm(u_up (Nx ,:)-U ,2)];
end
p_lw =log(abs(e_lw (1:end -1)./ e_lw (2: end )))./...
log(DT (1:end -1)./DT (2: end ))
p_up =log(abs(e_up (1:end -1)./ e_up (2: end )))./...
log(DT (1:end -1)./DT (2: end ))
p_lw =
0.1939
1.8626
2.0014
2.0040
2.0112
2.0239
p_up =
0.2272
0.3604
0.5953
0.7659
0.8853
0.9475
By implementing a similar loop for the parameter Δx, we can verify the accu-
racy of the scheme with respect to the space discretization. Precisely, for Δx
ranging from 10−4 to 10−2 we obtain
p_lw =
1.8113
2.0235
2.0112
2.0045
2.0017
2.0007
p_up =
0.3291
0.5617
0.7659
0.8742
0.9407
0.9734

10.9 Chapter 9
427
Program 10.5. hyper: Lax-Friedrichs, Lax-Wendroﬀand upwind schemes
function [xh ,th ,uh]= hyper(xspan ,tspan ,u0 ,ul ,...
scheme ,cfl ,deltax ,deltat)
% HYPER solves
hyperbolic
scalar
equations
% [XH ,TH ,UH ]= HYPER(XSPAN ,TSPAN ,U0 ,UL ,SCHEME ,CFL ,...
%
DELTAX ,DELTAT)
% solves the
hyperbolic
scalar
equation
%
DU/DT+ A * DU/DX=0
% in (XSPAN (1), XSPAN (2)) x(TSPAN(1), TSPAN (2))
% with A>0, initial
condition
U(X,0)= U0(X) and
% boundary
condition
U(T)= UL(T) given at XSPAN (1)
% with
several
finite
difference
schemes.
% scheme = 1 Lax - Friedrichs
%
2 Lax - Wendroff
%
3 Upwind
% The propagation
velocity
‘a’ is not required
as
% input of the function , since it can be derived
% from
CFL = A * DELTAT / DELTAX
% Output: XH is the vector of space nodes
% TH
is the vector of time
nodes
% UH is a matrix
containing
the
computed
solution
% UH(n ,:) contains
the solution
at time
TT(n)
% U0 and UL can be either inline , anonymous
% functions
or functions
defined by M-file.
Nt =( tspan(2)- tspan (1))/ deltat +1;
th=linspace (tspan(1), tspan(2), Nt);
Nx =( xspan(2)- xspan (1))/ deltax +1;
xh=linspace (xspan(1), xspan(2), Nx);
u=zeros(Nt ,Nx); cfl2 =cfl *0.5; cfl21=1-cfl ^2;
cflp1=cfl +1; cflm1=cfl -1; uh(1 ,:)= u0(xh );
for n=1:Nt -1
uh(n+1 ,1)= ul(th(n+1));
if scheme == 1
% Lax Friedrichs
for j=2:Nx -1
uh(n+1,j)=0.5*( - cflm1*uh(n,j+1)+ cflp1*uh(n,j -1));
end
j=Nx;
uh(n+1,j)=0.5*( - cflm1 *(2* uh(n,j)-uh(n,j -1))+...
cflp1*uh(n,j -1));
elseif
scheme == 2
% Lax Wendroff
for j=2:Nx -1
uh(n+1,j)= cfl21*uh(n,j)+...
cfl2 *( cflm1*uh(n,j+1)+ cflp1*uh(n,j -1));
end
j=Nx;
uh(n+1,j)= cfl21*uh(n,j)+...
cfl2 *( cflm1 *(2* uh(n,j)-uh(n,j -1))+ cflp1*uh(n,j -1));
elseif
scheme ==3
% Upwind
for j=2: Nx
uh(n+1,j)=- cflm1*uh(n,j)+ cfl*uh(n,j -1);
end
end
end

428
10 Solutions of the exercises
Exercise 9.14 The exact solution is the sum of two simple harmonics, the
former with low frequency and the latter with high frequency. If we choose
Δt = 5 · 10−2, since a = 1 and CFL=0.8, we have Δx = 6.25e −3 and the
phase angles associated to the harmonics are φk1 = 4π · 6.25e −3 ≃0.078 and
φk2 = 20π · 6.25e −3 ≃0.393, respectively. By inspecting Figure 9.18 we note
that the upwind scheme is more dissipative than Lax-Wendroﬀ’s. This fact is
conﬁrmed by the behavior of dissipation coeﬃcients (see the right graph at
the bottom of Figure 9.14). Indeed, when we take into account instances of φk
corresponding to the given harmonics, the curve relative to the Lax-Wendroﬀ
scheme is nearer to the constant 1 than the curve associated to the upwind
scheme.
For what concerns the dispersion coeﬃcient, we see from Figure 9.18 that
the Lax-Wendroﬀscheme features a phase delay, while the upwind scheme
presents a light phase advance. The right graph at the bottom of Figure 9.15
conﬁrms this conclusion. Moreover we can observe that the phase delay of the
Lax-Wendroﬀscheme is larger than the phase advance of the upwind scheme.

References
[ABB+99]
Anderson E., Bai Z., Bischof C., Blackford S., Demmel J., Don-
garra J., Croz J. D., Greenbaum A., Hammarling S., McKenney
A., and Sorensen D. (1999) LAPACK User’s Guide. 3rd edition.
SIAM, Philadelphia.
[Ada90]
Adair R. (1990) The Physics of Baseball.
Harper and Row,
New York.
[Arn73]
Arnold V. (1973) Ordinary Diﬀerential Equations. The MIT
Press, Cambridge.
[Atk89]
Atkinson K. (1989) An Introduction to Numerical Analysis. 2nd
edition. John Wiley & Sons Inc., New York.
[Att11]
Attaway S. (2011) MATLAB: A Practical Introduction to Pro-
gramming and Problem Solving, 2nd edition.
Butterworth-
Heinemann. Elsevier, Waltham, MA.
[Axe94]
Axelsson O. (1994) Iterative Solution Methods. Cambridge Uni-
versity Press, Cambridge.
[BB96]
Brassard G. and Bratley P. (1996) Fundamentals of Algorith-
mics. Prentice Hall Inc., Englewood Cliﬀs, NJ.
[BDF+10]
Bomze I., Demyanov V., Fletcher R., Terlaky T., and Polik I.
(2010) Nonlinear Optimization, volume 1989 of Lecture Notes in
Mathematics. Springer. Lectures given at the C.I.M.E. Summer
School held in Cetraro, July 2007. Edited by G. Di Pillo and F.
Schoen.
[Bec71]
Beckmann P. (1971) A history of π. 2nd edtion. The Golem
Press, Boulder, Colorado.
[Ber82]
Bertsekas D. (1982) Constrained optimization and Lagrange
multipliers methods. Academic Press, Inc., San Diego, CA.
[BGL05]
Benzi M., Golub G., and Liesen J. (2005) Numerical solution of
saddle point problems. Acta Numer. 14: 1–137.
[BM92]
Bernardi C. and Maday Y. (1992) Approximations Spectrales
des Probl´emes aux Limites Elliptiques. Springer-Verlag, Paris.
[Bom10]
Bomze M. (2010) Global Optimization: A Quadratic Program-
ming Perspective, volume 1989 of Lecture Notes in Mathematics,
chapter 3, pages 1–53. Springer. Lectures given at the C.I.M.E.
A. Quarteroni et al., Scientiﬁc Computing with MATLAB and Octave,
Texts in Computational Science and Engineering 2,
DOI 10.1007/978-3-642-45367-0, © Springer-Verlag Berlin Heidelberg 2014
429

430
References
Summer School held in Cetraro, July 2007. Edited by G. Di
Pillo and F. Schoen.
[Bra97]
Braess D. (1997) Finite Elements: Theory, Fast Solvers and
Applications in Solid Mechanics. Cambridge University Press,
Cambridge.
[Bre02]
Brent R. (2002) Algorithms for minimization without deriva-
tives.
Dover Publications Inc., Mineola, NY. Reprint of the
1973 original [Prentice-Hall, Inc., Englewood Cliﬀs, NJ].
[BS01]
Babuska I. and Strouboulis T. (2001) The Finite Element
Method and its Reliability.
Numerical Mathematics and Sci-
entiﬁc Computation. The Clarendon Press Oxford University
Press, New York.
[BT04]
Berrut J.-P. and Trefethen L.-N. (2004) Barycentric Lagrange
Interpolation. SIAM Review 46(3): 501–517.
[But87]
Butcher J. (1987) The Numerical Analysis of Ordinary Diﬀeren-
tial Equations: Runge-Kutta and General Linear Methods. Wi-
ley, Chichester.
[CFL28]
Courant R., Friedrichs K., and Lewy H. (1928)
¨Uber die
partiellen Diﬀerenzengleichungen der mathematischen Physik.
Math. Ann. 100(1): 32–74.
[CHQZ06]
Canuto C., Hussaini M. Y., Quarteroni A., and Zang T. A.
(2006) Spectral Methods: Fundamentals in Single Domains. Sci-
entiﬁc Computation. Springer-Verlag, Berlin.
[CHQZ07]
Canuto C., Hussaini M. Y., Quarteroni A., and Zang T. A.
(2007) Spectral Methods. Evolution to Complex Geometries
and Applications to Fluid Dynamics. Scientiﬁc Computation.
Springer, Heidelberg.
[CL96a]
Coleman T. and Li Y. (1996) An interior trust region approach
for nonlinear minimization subject to bounds. SIAM J. Optim.
6(2): 418–445.
[CL96b]
Coleman T. and Li Y. (1996) A reﬂective Newton method for
minimizing a quadratic function subject to bounds on some of
the variables. SIAM J. Optim. 6(4): 1040–1058.
[CLW69]
Carnahan B., Luther H., and Wilkes J. (1969) Applied Numer-
ical Methods. John Wiley & Sons, Inc., New York.
[Dav63]
Davis P. (1963) Interpolation and Approximation. Blaisdell Pub-
lishing Co. Ginn and Co. New York-Toronto-London, New York.
[dB01]
de Boor C. (2001) A practical guide to splines. Applied Math-
ematical Sciences. Springer-Verlag, New York.
[DD99]
Davis T. and DuﬀI. (1999) A combined unifrontal/multifrontal
method for unsymmetric sparse matrices. ACM Transactions
on Mathematical Software 25(1): 1–20.
[Dem97]
Demmel J. (1997) Applied Numerical Linear Algebra. SIAM,
Philadelphia.
[Deu04]
Deuﬂhard P. (2004) Newton Methods for Nonlinear Problems.
Aﬃne Invariance and Adaptive Algorithms. Springer Series in
Computational Mathematics. Springer-Verlag, Berlin.
[Die93]
Dierckx P. (1993) Curve and Surface Fitting with Splines.
Monographs on Numerical Analysis. The Clarendon Press Ox-
ford University Press, New York.

References
431
[DL92]
DeVore R. and Lucier B. (1992) Wavelets. In Acta numerica,
1992, pages 1–56. Cambridge Univ. Press, Cambridge.
[DR75]
Davis P. and Rabinowitz P. (1975) Methods of Numerical Inte-
gration. Academic Press, New York.
[dV89]
der Vorst H. V. (1989) High Performance Preconditioning.
SIAM J. Sci. Stat. Comput. 10: 1174–1185.
[EBH08]
Eaton J., Bateman D., and Hauberg S. (2008) GNU Octave
Manual Version 3. Network Theory Ltd., Bristol.
[EEHJ96]
Eriksson K., Estep D., Hansbo P., and Johnson C. (1996) Com-
putational Diﬀerential Equations. Cambridge Univ. Press, Cam-
bridge.
[EKM05]
Etter D., Kuncicky D., and Moore H. (2005) Introduction to
MATLAB 7. Prentice Hall, Englewood Cliﬀs.
[Eva98]
Evans L. (1998) Partial Diﬀerential Equations. American Math-
ematical Society, Providence.
[Fle10]
Fletcher R. (2010) The Sequential Quadratic Programming
Method, volume 1989 of Lecture Notes in Mathematics, chap-
ter 3, pages 165–214. Springer. Lectures given at the C.I.M.E.
Summer School held in Cetraro, July 2007. Edited by G. Di
Pillo and F. Schoen.
[Fun92]
Funaro D. (1992) Polynomial Approximation of Diﬀerential
Equations. Springer-Verlag, Berlin Heidelberg.
[Gau97]
Gautschi W. (1997) Numerical Analysis. An Introduction.
Birkh¨auser Boston Inc., Boston, MA.
[Gea71]
Gear C. (1971) Numerical Initial Value Problems in Ordinary
Diﬀerential Equations. Prentice-Hall, Upper Saddle River NJ.
[GI04]
George A. and Ikramov K. (2004) Gaussian elimination is stable
for the inverse of a diagonally dominant matrix. Math. Comp.
73(246): 653–657.
[GL96]
Golub G. and Loan C. V. (1996) Matrix Computations. 3rd
edition. The John Hopkins Univ. Press, Baltimore, MD.
[GM72]
Gill P. and Murray W. (1972) Quasi-Newton methods for un-
constrained optimization. J. Inst. Math. Appl. 9: 91–108.
[GN06]
Giordano N. and Nakanishi H. (2006) Computational Physics.
2nd edition. Prentice-Hall, Upper Saddle River NJ.
[GOT05]
Gould N., Orban D., and Toint P. (2005) Numerical methods for
large-scale nonlinear optimization. Acta Numerica 14: 299–361.
[GR96]
Godlewski E. and Raviart P.-A. (1996) Hyperbolic Systems of
Conservations Laws. Springer-Verlag, New York.
[Hac85]
Hackbusch W. (1985) Multigrid Methods and Applications.
Springer
Series
in
Computational
Mathematics.
Springer-
Verlag, Berlin.
[Hac94]
Hackbusch W. (1994) Iterative Solution of Large Sparse Systems
of Equations. Applied Mathematical Sciences. Springer-Verlag,
New York.
[Hen79]
Henrici P. (1979) Barycentric formulas for interpolating trigono-
metric polynomials and their conjugate. Numer. Math. 33: 225–
234.

432
References
[Hes98]
Hesthaven J. (1998) From electrostatics to almost optimal nodal
sets for polynomial interpolation in a simplex. SIAM J. Numer.
Anal. 35(2): 655–676.
[HH05]
Higham D. and Higham N. (2005) MATLAB Guide. 2nd edi-
tion. SIAM Publications, Philadelphia, PA.
[Hig02]
Higham N. (2002) Accuracy and Stability of Numerical Algo-
rithms. 2nd edition. SIAM Publications, Philadelphia, PA.
[Hig04]
Higham N.-J. (2004) The numerical stability of barycentric la-
grange interpolation. IMA J. Numer. Anal. 24(4): 547–556.
[Hir88]
Hirsh C. (1988) Numerical Computation of Internal and Exter-
nal Flows. John Wiley and Sons, Chichester.
[HLR06]
Hunt B., Lipsman R., and Rosenberg J. (2006) A guide to MAT-
LAB. For Beginners and Experienced Users. 2nd edition. Cam-
bridge University Press, Cambridge.
[IK66]
Isaacson E. and Keller H. (1966) Analysis of Numerical Meth-
ods. Wiley, New York.
[Joh90]
Johnson C. (1990) Numerical Solution of Partial Diﬀferential
Equations by the Finite Element Method. Cambridge University
Press, Cambridge.
[JS96]
Jr J. D. and Schnabel R. (1996) Numerical methods for un-
constrained optimization and nonlinear equations, volume 16 of
Classics in Applied Mathematics.
Society for Industrial and
Applied Mathematics (SIAM), Philadelphia, PA.
[Kr¨o98]
Kr¨oner D. (1998) Finite Volume Schemes in Multidimensions.
In Numerical analysis 1997 (Dundee), Pitman Res. Notes Math.
Ser., pages 179–192. Longman, Harlow.
[KS99]
Karniadakis G. and Sherwin S. (1999) Spectral/hp Element
Methods for CFD. Oxford University Press, New York.
[KW08]
Kalos M. and Whitlock P. (2008) Monte Carlo Methods, 2nd
ed. John Wiley & Sons.
[Lam91]
Lambert J. (1991) Numerical Methods for Ordinary Diﬀerential
Systems. John Wiley and Sons, Chichester.
[Lan03]
Langtangen H. (2003) Advanced Topics in Computational Par-
tial Diﬀerential Equations: Numerical Methods and Diﬀpack
Programming. Springer-Verlag, Berlin Heidelberg.
[LeV02]
LeVeque R. (2002) Finite Volume Methods for Hyperbolic Prob-
lems. Cambridge University Press, Cambridge.
[LRWW99]
Lagarias J., Reeds J., Wright M., and Wright P. (1999) Con-
vergence properties of the Nelder-Mead simplex method in low
dimensions. SIAM J. Optim. 9(1): 112–147.
[Mei67]
Meinardus G. (1967) Approximation of Functions: Theory and
Numerical Methods.
Springer Tracts in Natural Philosophy.
Springer-Verlag New York, Inc., New York.
[MH03]
Marchand P. and Holland O. (2003) Graphics and GUIs with
MATLAB. 3rd edition.
Chapman & Hall/CRC, London,
New York.
[Mun07]
Munson T. (2007) Mesh shape-quality optimization using the
inverse mean-ratio metric. Math. Program. 110(3, Ser. A): 561–
590.

References
433
[Nat65]
Natanson I. (1965) Constructive Function Theory. Vol. III. In-
terpolation and approximation quadratures.
Frederick Ungar
Publishing Co., New York.
[NM65]
Nelder J. and Mead R. (1965) A simplex method for function
minimization. The Computer Journal 7: 308–313.
[Noc92]
Nocedal J. (1992) Theory of algorithms for unconstrained opti-
mization. In Acta numerica, 1992, pages 199–242. Cambridge
Univ. Press, Cambridge.
[NW06]
Nocedal J. and Wright S. (2006) Numerical optimization.
Springer Series in Operations Research and Financial Engineer-
ing. Springer, New York, second edition.
[OR70]
Ortega J. and Rheinboldt W. (1970) Iterative Solution of
Nonlinear Equations in Several Variables.
Academic Press,
New York, London.
[Pal08]
Palm W. (2008) A Concise Introduction to Matlab. McGraw-
Hill, New York.
[Pan92]
Pan V. (1992) Complexity of Computations with Matrices and
Polynomials. SIAM Review 34(2): 225–262.
[Pap87]
Papoulis A. (1987) Probability, Random Variables, and Stochas-
tic Processes. McGraw-Hill, New York.
[PBP02]
Prautzsch H., Boehm W., and Paluszny M. (2002) Bezier and
B-Spline Techniques. Mathematics and Visualization. Springer-
Verlag, Berlin.
[PdDK¨UK83] Piessens R., de Doncker-Kapenga E., ¨Uberhuber C., and Ka-
haner D. (1983) QUADPACK: A Subroutine Package for Au-
tomatic Integration. Springer Series in Computational Mathe-
matics. Springer-Verlag, Berlin.
[Pra06]
Pratap R. (2006) Getting Started with MATLAB 7: A Quick
Introduction for Scientists and Engineers.
Oxford University
Press, New York.
[QSS07]
Quarteroni A., Sacco R., and Saleri F. (2007) Numerical Math-
ematics. 2nd edition. Texts in Applied Mathematics. Springer-
Verlag, Berlin.
[Qua13]
Quarteroni A. (2013) Numerical Models for Diﬀerential Prob-
lems. Series: MS&A , Vol. 2, 2nd edition. Springer-Verlag, Mi-
lano.
[QV94]
Quarteroni A. and Valli A. (1994) Numerical Approximation of
Partial Diﬀerential Equations. Springer-Verlag, Berlin.
[Ros61]
Rosenbrock H. (1960/1961) An automatic method for ﬁnding
the greatest or least value of a function. Comput. J. 3: 175–184.
[RR01]
Ralston A. and Rabinowitz P. (2001) A First Course in Numer-
ical Analysis. 2nd edition. Dover Publications Inc., Mineola,
NY.
[Saa92]
Saad Y. (1992) Numerical Methods for Large Eigenvalue Prob-
lems. Manchester University Press, Manchester; Halsted Press
(John Wiley & Sons, Inc.), Manchester; New York.
[Saa03]
Saad Y. (2003) Iterative Methods for Sparse Linear Systems.
2nd edition. SIAM publications, Philadelphia, PA.
[Sal08]
Salsa S. (2008) Partial Diﬀerential Equations in Action - From
Modelling to Theory. Springer, Milan.

434
References
[SM03]
S¨uli E. and Mayers D. (2003) An Introduction to Numerical
Analysis. Cambridge University Press, Cambridge.
[SR97]
Shampine L. and Reichelt M. (1997) The MATLAB ODE suite.
SIAM J. Sci. Comput. 18(1): 1–22.
[SSB85]
Shultz G., Schnabel R., and Byrd R. (1985) A family of trust-
region-based algorithms for unconstrained minimization with
strong global convergence properties. SIAM J. Numer. Anal.
22(1): 47–67.
[Ste83]
Steihaug T. (1983) The conjugate gradient method and trust re-
gions in large scale optimization. SIAM J. Numer. Anal. 20(3):
626–637.
[Str07]
Stratton J. (2007) Electromagnetic Theory. Wiley-IEEE Press,
Hoboken, New Jersey.
[SY06]
Sun W. and Yuan Y.-X. (2006) Optimization theory and meth-
ods, volume 1 of Springer Optimization and Its Applications.
Springer, New York. Nonlinear programming.
[Ter10]
Terlaky T. (2010) Interior Point Methods for Nonlinear Opti-
mization, volume 1989 of Lecture Notes in Mathematics, chap-
ter 3, pages 215–276. Springer. Lectures given at the C.I.M.E.
Summer School held in Cetraro, July 2007. Edited by G. Di
Pillo and F. Schoen.
[TW98]
Tveito A. and Winther R. (1998) Introduction to Partial Diﬀer-
ential Equations. A Computational Approach. Springer-Verlag,
Berlin Heidelberg.
[¨Ube97]
¨Uberhuber C. (1997) Numerical Computation: Methods, Soft-
ware, and Analysis. Springer-Verlag, Berlin.
[Urb02]
Urban K. (2002) Wavelets in Numerical Simulation. Lecture
Notes in Computational Science and Engineering. Springer-
Verlag, Berlin.
[vdV03]
van der Vorst H. (2003) Iterative Krylov Methods for Large Lin-
ear Systems. Cambridge Monographs on Applied and Compu-
tational Mathematics. Cambridge University Press, Cambridge.
[VGCN05]
Valorani M., Goussis D., Creta F., and Najm H. (2005) Higher
order corrections in the approximation of low-dimensional man-
ifolds and the construction of simpliﬁed problems with the CSP
method. J. Comput. Phys. 209(2): 754–786.
[Wes04]
Wesseling P. (2004) An Introduction to Multigrid Methods. R.T.
Edwards, Inc., Philadelphia.
[Wil88]
Wilkinson J. (1988) The Algebraic Eigenvalue Problem. Mono-
graphs on Numerical Analysis. The Clarendon Press Oxford
University Press, New York.
[Zha99]
Zhang F. (1999) Matrix theory. Universitext. Springer-Verlag,
New York.

Index
abs, 8
adaptive
Euler, 287
interpolation, 100
quadrature formulae, 127
Runge-Kutta, 301
algorithm, 28
backward substitutions, 144
forward substitutions, 144
Gauss, 145
H¨orner, 68
LU factorization, 145
Strassen, 29
synthetic division, 68
Thomas, 163, 334
Winograd and Coppersmith, 29
aliasing, 97
angle, 9
anonymous function, 16
ans, 31
arpackc, 210
artiﬁcial
diﬀusion ﬂux, 359
viscosity, 340, 359
average, 111
axis, 203
backtracking strategy, 233
backward diﬀerence formula, 302
barycentric interpolation, 90
basis, 4
bicgstab, 179
boundary conditions, 332, 374
Dirichlet, 332
Neumann, 332, 374
boundary-value problem, 186, 329
Butcher array, 300, 301
cancellation, 7
Cauchy point, 245
CFL
condition, 361, 372
number, 361, 362
characteristic
curves, 356
variables, 368
chol, 151
clear, 32
coeﬃcient
ampliﬁcation, 362
dispersion, 362, 363
dissipation, 362, 363
Fourier, 361
compass, 9
complex, 8
complexity, 29
computational cost, 28
Cramuer rule, 142
LU factorization, 148
cond, 161
condest, 161
condition number, 161, 183, 344
of interpolation, 87
conditions
A. Quarteroni et al., Scientiﬁc Computing with MATLAB and Octave,
Texts in Computational Science and Engineering 2,
DOI 10.1007/978-3-642-45367-0, © Springer-Verlag Berlin Heidelberg 2014
435

436
Index
Karush–Kuhn–Tucker, 256
Lagrange, 257
LICQ, 256
optimality, 218, 256
Wolfe, 232
strong, 232
conj, 9
consistency, 279, 281, 286, 347
constraint
active, 254
equality, 254
inequality, 254
contour lines, 226, 255, 405, 407, 409
conv, 21
convergence, 26, 65, 286
Euler method, 278, 280
factor
asymptotic, 60
ﬁnite diﬀerences, 347
Gauss-Seidel method, 173
global, 227
iterative method, 168, 169
Jacobi method, 173
local, 227
of interpolation, 86
order, 26
Newton method, 49
quadratic, 227
secant method, 52
super-linear, 52, 222, 238
power method, 199
Richardson method, 174
cos, 32
cputime, 30
cross, 15
cumtrapz, 121
Dahlquist barrier, 303, 304
dblquad, 134
deconv, 21
deﬂation, 68, 210
criterion, 69
descent direction, 177, 228
conjugate gradient, 229
gradient, 229
Newton, 229
quasi-Newton, 229
det, 12, 149
diag, 13
diff, 24
diﬀerential equation
ordinary, 271
partial, 271
digit
signiﬁcant, 5
discretization step, 275
disp, 33
dispersion, 361–363
dissipation, 361, 362
domain of dependence, 368
dot, 15
dot operation, 15, 18
eig, 206
eigenvalue, 16, 193
extremal, 197
problem, 193
eigenvector, 16, 193
eigs, 208
end, 30
eps, 5, 6
equation
Burgers, 357
convection-diﬀusion, 336, 340
heat, 330, 348
hyperbolic, 355
Poisson, 329, 332
pure advection, 355
telegrapher’s, 331
transport, 357, 366
Van der Pol, 322
wave, 330, 367
equations
normal, 107
error
a-priori estimate, 162
absolute, 5, 26
computational, 26
estimator, 27, 50, 62, 127
a-posteriori, 299
increment, 180
interpolation, 83
local truncation, 279, 360
of quadrature, 119
perturbation, 291
relative, 5, 26
roundoﬀ, 4, 5, 8, 26, 155, 158, 282
truncation, 26, 279, 347, 350

Index
437
etime, 30
exit, 31
exp, 32
expectation, 131
exponent, 4
extrapolation
Aitken, 64
Richardson, 135
eye, 11
F, 5
factorization
Cholesky, 151, 183, 201
Gauss LU, 146
incomplete Cholesky, 183
incomplete LU, 187
LU, 143, 146, 150, 158, 201
QR, 55, 164, 239
Fast Fourier Transform, 93, 95
inverse, 96
FFT, 93, 95
fft, 95
fftshift, 96
Fibonacci sequence, 34, 40
figure, 203
ﬁll-in, 157
find, 45
ﬁnite diﬀerence
backward, 116
centered, 116
forward, 115
fix, 379
ﬁxed point, 57
convergence, 61, 65
iteration function, 57
iterations, 57
ﬂoating-point
number, 3, 4
operation, 28
system, 5
ﬂux
numerical, 358
fminbnd, 223
fminsearch, 225
fminunc, 239, 247
for, 34
format, 4
formula
Euler, 8
fplot, 17, 99
fsolve, 73, 277
function, 16, 17, 35
convex, 218
cost, 213
derivative, 23
graph of, 17
interpolant, 81
iteration, 57, 62, 64
Lagrange characteristic, 83
Lagrangian, 256
augmented, 264
Lipschitz continuous, 218, 241
Lipschitz-continuous, 275, 285
objective, 213
penalty, 259
primitive, 23
Runge’s, 85, 89
shape, 339
strongly convex, 255
user-deﬁned, 17, 35
function, 35
function handle, 16, 18
funtool, 24
fzero, 20
fzero, 19, 72, 73
gallery, 185
Gauss plane, 10
Gershgorin circles, 203, 204, 211
gmres, 179
golden ratio, 219
gradient, 217
grid, 17
griddata, 109
griddata3, 109
griddatan, 109
help, 32, 37
hold off, 203
hold on, 203
ichol, 183
if, 30
ifft, 96
ilu, 187
imag, 9
image, 208
imread, 208

438
Index
Inf, 6
int, 24
interp1, 99
interp1q, 99
interp2, 109
interp3, 109
interpft, 96
interpolant, 81
Hermite, 104
Lagrange, 83
trigonometric, 93
interpolation
adaptive, 100
barycentric formula, 90
composite, 98, 109
convergence, 86
error, 83
Hermite piecewise, 104
Lagrange, 81
Gauss nodes, 87, 88
nodes, 80
piecewise linear, 98, 99
polynomial, 81
rational, 81
spline, 100
stability, 86
trigonometric, 81, 93
inv, 12
Kronecker symbol, 82
Lagrange
multipliers, 243, 256
LAPACK, 167
Laplace operator, 329
law
Fourier, 331
Kirchoﬀ, 273
Ohm, 273
least-squares
method, 104
solution, 164, 166
Lebesgue constant, 87, 89
lexicographic order, 342
linear system, 137
banded, 183
direct methods, 149
methods
direct, 143, 182
iterative, 143, 168, 182
overdetermined, 163
underdetermined, 163
linearly independent system, 14, 201
linspace, 18
load, 32
loglog, 27
Lotka-Volterra equations, 272
lu, 149
m-ﬁle, 34
machine epsilon, 5
magic, 188
mantissa, 4
mass-lumping, 355
matlabFunction, 85
matrix, 10
bandwidth of, 152, 183, 184
bidiagonal, 162
companion, 73
complex deﬁnite positive, 151
determinant of, 12, 149
diagonal, 13
diagonally dominant, 150, 171, 205
ﬁnite diﬀerence, 183
full, 185
Hankel, 185
hermitian, 14, 151
Hessian, 217
Hilbert, 159, 161, 179, 180, 185
identity, 11
ill conditioned, 161, 183
inverse, 12
iteration, 169, 174
Jacobian, 322
Leslie, 195, 210
lower triangular, 13
mass, 354
non-symmetric, 186
norm of, 161
orthogonal, 164
pattern of, 152
permutation, 154
product, 11
pseudoinverse, 165
rank of, 164
Riemann, 186
semi positive deﬁnite, 151
similar, 206

Index
439
singular value decomposition of,
164
sparse, 152, 156, 163, 166, 186, 344
spectrum, 196
splitting of, 169
square, 10
square root of, 400
strictly diagonal dominant, 173
sum, 11
symmetric, 14
symmetric positive deﬁnite, 151,
173
transpose of, 14
tridiagonal, 162, 173, 334
unitary, 165
upper triangular, 13
Vandermonde, 147, 185
well conditioned, 161
Wilkinson, 211
mesh, 344
contour, 417
meshgrid, 109, 417
method
θ−, 349
A-stable, 289
Adams-Bashforth, 302
Adams-Moulton, 302
adaptive Newton, 49
Aitken, 63
backward Euler/centered, 359
Bairstow, 73
barrier, 268
BFGS, 238
Bi-CGStab, 179, 187, 258
bisection, 43, 58
Bogacki and Shampine pair, 301
Broyden, 55, 73
conjugate gradient, 177, 231
consistent, 279, 347
Crank-Nicolson, 282, 350, 353
cyclic composite, 304
deﬂation, 68
Dekker-Brent, 72
derivative free, 218
descent, 218, 228, 235
Dormand-Prince pair, 301
Euler
adaptive forward, 287
backward, 276, 353
forward, 275, 287
forward adaptive, 297
improved, 305
explicit, 276
ﬁnite diﬀerence, 115, 239, 333, 336,
341, 357
ﬁnite element, 186, 337, 340, 366,
373
forward Euler/centered, 358
forward Euler/decentered, 358,
372
Gauss Elimination, 147
Gauss-Newton, 249
damped, 250
Gauss-Seidel, 172, 181
GMRES, 179, 185, 258
golden section, 219
gradient, 175, 231
Heun, 305, 306, 327
implicit, 276
inverse power, 201
iterative, 56
Jacobi, 170, 181
Krylov, 179, 188
Lanczos, 179, 210
Lax-Friedrichs, 358
Lax-Wendroﬀ, 358, 372
Leap-Frog, 311, 370
least-squares, 104
Levenberg-Marquardt, 252
line search, 218, 228
cubic, 234
quadratic, 234
modiﬁed Newton, 49
Monte Carlo, 131, 379
multifrontal, 188
multigrid, 188
multistep, 285, 302
M¨uller, 73
Newmark, 311, 312, 369
Newton, 47, 52, 62
Newton-H¨orner, 70
one-step, 276, 300
power, 197
power with shift, 201
preconditioned conjugate gradient,
178, 183
preconditioned gradient, 175
predictor-corrector, 305

440
Index
QR, 206
quadratic interpolation, 222
quasi-Newton, 55, 73
relaxation, 173, 190, 398
Richardson
dynamic, 174
stationary, 174
Runge-Kutta, 300, 305
adaptive, 301
secant, 52, 54
SOR, 190
spectral, 184, 374
Steﬀensen, 64
successive over-relaxation, 190
trust region, 218, 242, 252
upwind, 358, 372
minimizer
global, 217
constrained, 254
local, 217
constrained, 254
mkpp, 102
model
Leontief, 139
Lotka and Leslie, 195
multipliers, 146, 155
Lagrange, 243
NaN, 7
nargin, 37
nargout, 37
nchoosek, 379
Newton
divided diﬀerences, 222
nodes
Chebyshev-Gauss, 88
Chebyshev-Gauss-Lobatto, 87
Gauss-Legendre-Lobatto, 126
norm
of matrix, 161
energy, 174
euclidean, 15
Frobenius, 215
norm, 15
normal equations, 164
number
complex, 8
ﬂoating-point, 4
normalized, 3
real, 3
ode, 301
ode113, 307
ode15s, 304
ode15s, 322
ode23, 301, 309
ode23s, 322, 323, 325
ode23tb, 301
ode45, 301, 309
ones, 14
operator
boolean, 32, 33
divergence, 330
Laplace, 329, 342
logical, 32, 33
relational, 32
short-circuit, 33
optimset, 223
order
of convergence, 26
overﬂow, 6, 7
P´eclet number
global, 336
local, 336
partial derivative, 53, 329
patch, 203
path, 34
pcg, 179
pchip, 104
pde, 346
pdetool, 109, 186, 373
phase plane, 308
pivot, 156
elements, 146
pivoting, 154
by row, 154
complete, 396
total, 156
plot, 18, 27
Pn, 19
point
admissible, 254
critical, 217
regular, 217
stationary, 217
poly, 39, 85
polyder, 22, 86

Index
441
polyfit, 22, 83, 106
polyint, 22
polynomial, 20
characteristic, 193, 285
division of, 21, 69
Lagrangian interpolation, 81
Legendre, 125
product of, 21
roots of, 21
Taylor, 23, 79
polyval, 83
ppval, 102
preconditioner, 169, 174, 178
incomplete Cholesky factorization,
183
incomplete LU, 187
pretty, 378
problem
boundary value, 329
Cauchy, 274
convection-diﬀusion, 336, 340
convection-dominated, 336
Dirichlet, 332
least squares
nonlinear, 248
Neumann, 332
Poisson, 183, 184, 341
stiﬀ, 319
product
inner, 15
scalar, 15
quadl, 126
quadratic programming, 257
quadrature
nodes, 123
weights, 123
quadrature formulae, 117
adaptive Simpson, 127, 128
composite midpoint, 118
composite rectangle, 118
composite Simpson, 121
composite trapezoidal, 120
degree of exactness, 119
error, 120, 122
Gauss, 133
Gauss-Legendre, 125
Gauss-Legendre-Lobatto, 184
interpolatory, 123
midpoint, 118
Newton-Cotes, 133
rectangle, 118
Simpson, 122
trapezoidal, 121
quit, 31
quiver, 15
quiver3, 15
rand, 30
rank, 164
Rayleigh quotient, 193
real, 9
realmax, 6
realmin, 6
region of absolute stability, 289, 303
regression line, 106
relaxation method, 190
residual, 50, 162, 180, 231
preconditioned, 170
relative, 176
return, 36
root
multiple, 19, 21, 49
simple, 19, 48
root condition, 285
roots, 21, 73
roundoﬀ
error, 4, 5, 8, 26, 155, 158
unity, 5
rpmak, 109
rsmak, 109
rule
Armijo, 232
Cramer, 142
Laplace, 12
save, 32
scale
linear, 27, 28
logarithmic, 27
semi-logarithmic, 28
semi-discretization, 348, 353
semilogy, 28
Sequential Quadratic Programming,
268
series
discrete Fourier, 94
shift, 201

442
Index
simple, 24, 399
simplex, 223
sin, 32
Singular Value Decomposition, 108,
164, 165
singular values, 165
sparse, 153
spdemos, 109
spdiags, 153, 163
spectral radius, 169
spectrometry, 138
spline, 100
error, 103
natural cubic, 100
not-a-knot condition, 102
spline, 102
spy, 152, 183, 344
sqrt, 32
stability
of interpolation, 86
absolute, 287, 289, 290
asymptotic, 349
of Adams methods, 303
region of absolute, 289, 327
zero-, 284, 286
statistical mean, 131
stencil, 343
step adaptivity, 297
steplength, 275
stopping test, 50, 62, 180
Sturm sequences, 73, 210
sum, 379
svd, 165
svds, 165
syms, 24, 399
system
hyperbolic, 368
linear, 137
nonlinear equations, 52
triangular, 144
underdetermined, 145
taylor, 24
taylortool, 79
theorem
Abel, 67
Cauchy, 68
Descartes, 67
ﬁrst mean-value, 23
Lax-Richtmyer equivalence, 286
mean-value, 23
of integration, 22
Ostrowski, 59
zeros of continuous functions, 43
time-step, 275
title, 203
toolbox, 2, 32
trapz, 121
tril, 13
triu, 13
UMFPACK, 166, 167, 187
underﬂow, 6
vander, 147
varargin, 45
variance, 111, 389
vector
column, 10
component of, 15
conjugate transpose, 15
norm, 15
product, 15
row, 10
viscosity, 340, 359
wavelet, 109
wavelets, 109
weak
formulation, 338
solution, 357
while, 34
wilkinson, 211
workspace
workspace base, 32
xlabel, 203
ylabel, 203
zero
multiple, 19
of a function, 19
simple, 19, 48
zeros, 11, 14

Editorial Policy
1. Textbooks on topics in the ﬁeld of computational science and engineering
will be considered. They should be written for courses in CSE education.
Both graduate and undergraduate textbooks will be published in TCSE.
Multidisciplinary topics and multidisciplinary teams of authors are espe-
cially welcome.
2. Format: Only works in English will be considered. For evaluation pur-
poses, manuscripts may be submitted in print or electronic form, in the latter
case, preferably as pdf- or zipped ps-ﬁles. Authors are requested to use the
LaTeX style ﬁles available from Springer at: http://www.springer.com/
authors/book+authors/helpdesk?SGWID=0-1723113-12-971304-0
(for
monographs, textbooks and similar)
Electronic material can be included if appropriate. Please contact the pub-
lisher.
3. Those considering a book which might be suitable for the series are strongly
advised to contact the publisher or the series editors at an early stage.
General Remarks
Careful preparation of manuscripts will help keep production time short and
ensure a satisfactory appearance of the ﬁnished book.
The following terms and conditions hold:
Regarding free copies and royalties, the standard terms for Springer mathemat-
ics textbooks hold. Please write to martin.peters@springer.com for details.
Authors are entitled to purchase further copies of their book and other Springer
books for their personal use, at a discount of 33.3% directly from Springer-
Verlag.

Series Editors
Timothy J. Barth
NASA Ames Research Center
NAS Division
Moffett Field, CA 94035, USA
barth@nas.nasa.gov
Michael Griebel
Institut f¨ur Numerische Simulation
der Universit¨at Bonn
Wegelerstr. 6
53115 Bonn, Germany
griebel@ins.uni-bonn.de
David E. Keyes
Mathematical and Computer Sciences
and Engineering
King Abdullah University of Science
and Technology
P.O. Box 55455
Jeddah 21534, Saudi Arabia
david.keyes@kaust.edu.sa
and
Department of Applied Physics
and Applied Mathematics
Columbia University
500 W. 120th Street
New York, NY 10027, USA
kd2112@columbia.edu
Risto M. Nieminen
Department of Applied Physics
Aalto University School of Science
and Technology
00076 Aalto, Finland
risto.nieminen@aalto.ﬁ
Dirk Roose
Department of Computer Science
Katholieke Universiteit Leuven
Celestijnenlaan 200A
3001 Leuven-Heverlee, Belgium
dirk.roose@cs.kuleuven.be
Tamar Schlick
Department of Chemistry
and Courant Institute
of Mathematical Sciences
New York University
251 Mercer Street
New York, NY 10012, USA
schlick@nyu.edu
Editor for Computational Science
and Engineering at Springer:
Martin Peters
Springer-Verlag
Mathematics Editorial IV
Tiergartenstrasse 17
69121 Heidelberg, Germany
martin.peters@springer.com

Texts in Computational Science and Engineering
1. H. P. Langtangen, Computational Partial Diﬀerential Equations.
Numerical Methods and Diﬀpack Programming, 2nd Edition.
2. A. Quarteroni, F. Saleri, P. Gervasio, Scientiﬁc Computing with
MATLAB and Octave, 4th Edition.
3. H. P. Langtangen, Python Scripting for Computational Science, 3rd
Edition.
4. H. Gardner, G. Manduchi, Design Patterns for e-Science.
5. M. Griebel, S. Knapek, G. Zumbusch, Numerical Simulation in
Molecular Dynamics.
6. H. P. Langtangen, A Primer on Scientiﬁc Programming with Python,
3rd Edition.
7. A. Tveito, H. P. Langtangen, B. F. Nielsen, X. Cai, Elements of
Scientiﬁc Computing.
8. B. Gustafsson, Fundamentals of Scientiﬁc Computing.
9. M. Bader, Space-Filling Curves.
10. M.G. Larson, F. Bengzon, The Finite Element Method: Theory,
Implementation, and Practice.
For further information on these books please have a look at our mathematics
catalogue at the following URL: www.springer.com/series/5151
Monographs in Computational Science and Engineer-
ing
1. J. Sundnes, G.T. Lines, X. Cai, B.F. Nielsen, K.-A. Mardal, A. Tveito,
Computing the Electrical Activity in the Heart.
For further information on this book, please have a look at our mathematics
catalogue at the following URL: www.springer.com/series/7417
Lecture Notes in Computational Science and Engi-
neering
1. D. Funaro, Spectral Elements for Transport-Dominated Equations.
2. H.P. Langtangen, Computational Partial Diﬀerential Equations.
Numerical Methods and Diﬀpack Programming.
3. W. Hackbusch, G. Wittum (eds.), Multigrid Methods V.

4. P. Deuﬂhard, J. Hermans, B. Leimkuhler, A.E. Mark, S. Reich, R.D.
Skeel (eds.), Computational Molecular Dynamics: Challenges, Meth-
ods, Ideas.
5. D. Kr¨oner, M. Ohlberger, C. Rohde (eds.), An Introduction to Re-
cent Developments in Theory and Numerics for Conservation Laws.
6. S. Turek, Eﬃcient Solvers for Incompressible Flow Problems. An
Algorithmic and Computational Approach.
7. R. von Schwerin, Multi Body System SIMulation. Numerical Meth-
ods, Algorithms, and Software.
8. H.-J. Bungartz, F. Durst, C. Zenger (eds.), High Performance Sci-
entiﬁc and Engineering Computing.
9. T.J. Barth, H. Deconinck (eds.), High-Order Methods for Computa-
tional Physics.
10. H.P. Langtangen, A.M. Bruaset, E. Quak (eds.), Advances in Soft-
ware Tools for Scientiﬁc Computing.
11. B. Cockburn, G.E. Karniadakis, C.-W. Shu (eds.), Discontinuous
Galerkin Methods. Theory, Computation and Applications.
12. U. van Rienen, Numerical Methods in Computational Electrodynam-
ics. Linear Systems in Practical Applications.
13. B. Engquist, L. Johnsson, M. Hammill, F. Short (eds.), Simulation
and Visualization on the Grid.
14. E. Dick, K. Riemslagh, J. Vierendeels (eds.), Multigrid Methods VI.
15. A. Frommer, T. Lippert, B. Medeke, K. Schilling (eds.), Numerical
Challenges in Lattice Quantum Chromodynamics.
16. J. Lang, Adaptive Multilevel Solution of Nonlinear Parabolic PDE
Systems. Theory, Algorithm, and Applications.
17. B.I. Wohlmuth, Discretization Methods and Iterative Solvers Based
on Domain Decomposition.
18. U. van Rienen, M. G¨unther, D. Hecht (eds.), Scientiﬁc Computing
in Electrical Engineering.
19. I. Babuˇska, P.G. Ciarlet, T. Miyoshi (eds.), Mathematical Modeling
and Numerical Simulation in Continuum Mechanics.
20. T.J. Barth, T. Chan, R. Haimes (eds.), Multiscale and Multiresolu-
tion Methods. Theory and Applications.
21. M. Breuer, F. Durst, C. Zenger (eds.), High Performance Scientiﬁc
and Engineering Computing.
22. K. Urban, Wavelets in Numerical Simulation. Problem Adapted
Construction and Applications.
23. L.F. Pavarino, A. Toselli (eds.), Recent Developments in Domain
Decomposition Methods.

24. T. Schlick, H.H. Gan (eds.), Computational Methods for Macro-
molecules: Challenges and Applications.
25. T.J. Barth, H. Deconinck (eds.), Error Estimation and Adaptive
Discretization Methods in Computational Fluid Dynamics.
26. M. Griebel, M.A. Schweitzer (eds.), Meshfree Methods for Partial
Diﬀerential Equations.
27. S. M¨uller, Adaptive Multiscale Schemes for Conservation Laws.
28. C. Carstensen, S. Funken, W. Hackbusch, R.H.W. Hoppe, P. Monk
(eds.), Computational Electromagnetics.
29. M.A. Schweitzer, A Parallel Multilevel Partition of Unity Method
for Elliptic Partial Diﬀerential Equations.
30. T. Biegler, O. Ghattas, M. Heinkenschloss, B. van Bloemen Waan-
ders (eds.), Large-Scale PDE-Constrained Optimization.
31. M. Ainsworth, P. Davies, D. Duncan, P. Martin, B. Rynne (eds.),
Topics in Computational Wave Propagation. Direct and Inverse
Problems.
32. H. Emmerich, B. Nestler, M. Schreckenberg (eds.), Interface and
Transport Dynamics. Computa- tional Modelling.
33. H.P. Langtangen, A. Tveito (eds.), Advanced Topics in Computa-
tional Partial Diﬀerential Equations. Numerical Methods and Diﬀ-
pack Programming.
34. V. John, Large Eddy Simulation of Turbulent Incompressible Flows.
Analytical and Numerical Results for a Class of LES Models.
35. E. B¨ansch (ed.), Challenges in Scientiﬁc Computing - CISC 2002.
36. B.N. Khoromskij, G. Wittum, Numerical Solution of Elliptic Dif-
ferential Equations by Reduction to the Interface.
37. A. Iske, Multiresolution Methods in Scattered Data Modelling.
38. S.-I. Niculescu, K. Gu (eds.), Advances in Time-Delay Systems.
39. S. Attinger, P. Koumoutsakos (eds.), Multiscale Modelling and Sim-
ulation.
40. R. Kornhuber, R. Hoppe, J. P´eriaux, O. Pironneau, O. Wildlund,
J. Xu (eds.), Domain Decomposition Methods in Science and Engi-
neering.
41. T. Plewa, T. Linde, V.G. Weirs (eds.), Adaptive Mesh Reﬁnement
– Theory and Applications.
42. A. Schmidt, K.G. Siebert, Design of Adaptive Finite Element Soft-
ware. The Finite Element Toolbox ALBERTA.
43. M. Griebel, M.A. Schweitzer (eds.), Meshfree Methods for Partial
Diﬀerential Equations II.

44. B. Engquist, P. L¨otstedt, O. Runborg (eds.), Multiscale Methods in
Science and Engineering.
45. P. Benner, V. Mehrmann, D.C. Sorensen (eds.), Dimension Reduc-
tion of Large-Scale Systems.
46. D. Kressner, Numerical Methods for General and Structured Eigen-
value Problems.
47. A. Bori¸ci, A. Frommer, B. Jo´o, A. Kennedy, B. Pendleton (eds.),
QCD and Numerical Analysis III.
48. F. Graziani (ed.), Computational Methods in Transport.
49. B. Leimkuhler, C. Chipot, R. Elber, A. Laaksonen, A. Mark, T.
Schlick, C. Sch¨utte, R. Skeel (eds.), New Algorithms for Macro-
molecular Simulation.
50. M. B¨ucker, G. Corliss, P. Hovland, U. Naumann, B. Norris (eds.),
Automatic Diﬀerentiation: Applications, Theory, and Implementa-
tions.
51. A.M. Bruaset, A. Tveito (eds.), Numerical Solution of Partial Dif-
ferential Equations on Parallel Computers.
52. K.H. Hoﬀmann, A. Meyer (eds.), Parallel Algorithms and Cluster
Computing.
53. H.-J. Bungartz, M. Sch¨afer (eds.), Fluid-Structure Interaction.
54. J. Behrens, Adaptive Atmospheric Modeling.
55. O. Widlund, D. Keyes (eds.), Domain Decomposition Methods in
Science and Engineering XVI.
56. S. Kassinos, C. Langer, G. Iaccarino, P. Moin (eds.), Complex Eﬀects
in Large Eddy Simulations.
57. M. Griebel, M.A Schweitzer (eds.), Meshfree Methods for Partial
Diﬀerential Equations III.
58. A.N. Gorban, B. K´egl, D.C. Wunsch, A. Zinovyev (eds.), Principal
Manifolds for Data Visualization and Dimension Reduction.
59. H. Ammari (ed.), Modeling and Computations in Electromagnetics:
A Volume Dedicated to Jean-Claude N´ed´elec.
60. U. Langer, M. Discacciati, D. Keyes, O. Widlund, W. Zulehner
(eds.), Domain Decomposition Methods in Science and Engineering
XVII.
61. T. Mathew, Domain Decomposition Methods for the Numerical So-
lution of Partial Diﬀerential Equations.
62. F. Graziani (ed.), Computational Methods in Transport: Veriﬁcation
and Validation.
63. M. Bebendorf, Hierarchical Matrices. A Means to Eﬃciently Solve
Elliptic Boundary Value Problems.

64. C.H. Bischof, H.M. B¨ucker, P. Hovland, U. Naumann, J. Utke (eds.),
Advances in Automatic Diﬀerentiation.
65. M. Griebel, M.A. Schweitzer (eds.), Meshfree Methods for Partial
Diﬀerential Equations IV.
66. B. Engquist, P. L¨otstedt, O. Runborg (eds.), Multiscale Modeling
and Simulation in Science.
67. I.H. Tuncer, ¨U. G¨ulcat, D.R. Emerson, K. Matsuno (eds.), Parallel
Computational Fluid Dynamics 2007.
68. S. Yip, T. Diaz de la Rubia (eds.), Scientiﬁc Modeling and Simula-
tions.
69. A. Hegarty, N. Kopteva, E. O’Riordan, M. Stynes (eds.), BAIL 2008
– Boundary and Interior Layers.
70. M. Bercovier, M.J. Gander, R. Kornhuber, O. Widlund (eds.), Do-
main Decomposition Methods in Science and Engineering XVIII.
71. B. Koren, C. Vuik (eds.), Advanced Computational Methods in Sci-
ence and Engineering.
72. M. Peters (ed.), Computational Fluid Dynamics for Sport Simula-
tion.
73. H.-J. Bungartz, M. Mehl, M. Sch¨afer (eds.), Fluid Structure Inter-
action II – Modelling, Simulation, Optimization.
74. D. Tromeur-Dervout, G. Brenner, D.R. Emerson, J. Erhel (eds.),
Parallel Computational Fluid Dynamics 2008.
75. A.N. Gorban, D. Roose (eds.), Coping with Complexity: Model Re-
duction and Data Analysis.
76. J.S. Hesthaven, E.M. Rønquist (eds.), Spectral and High Order
Methods for Partial Diﬀerential Equations.
77. M. Holtz, Sparse Grid Quadrature in High Dimensions with Appli-
cations in Finance and Insurance.
78. Y. Huang, R. Kornhuber, O. Widlund, J. Xu (eds.), Domain De-
composition Methods in Science and Engineering XIX.
79. M. Griebel, M.A. Schweitzer (eds.), Meshfree Methods for Partial
Diﬀerential Equations V.
80. P.H. Lauritzen, C. Jablonowski, M.A. Taylor, R.D. Nair (eds.), Nu-
merical Techniques for Global Atmospheric Models.
81. C. Clavero, J.L. Gracia, F. Lisbona (eds.), BAIL 2010 – Boundary
and Interior Layers, Computational and Asymptotic Methods.
82. B. Engquist, O. Runborg, Y.R. Tsai (eds.), Numerical Analysis and
Multiscale Computations.
83. I.G. Graham, T.Y. Hou, O. Lakkis, R. Scheichl (eds.), Numerical
Analysis of Multiscale Problems.

84. A. Logg, K.-A. Mardal, G. Wells (eds.), Automated Solution of Dif-
ferential Equations by the Finite Element Method.
85. J. Blowey, M. Jensen (eds.), Frontiers in Numerical Analysis -
Durham 2010.
86. O. Kolditz, U.-J. Gorke, H. Shao, W. Wang (eds.), Thermo-Hydro-
Mechanical-Chemical Processes in Fractured Porous Media - Bench-
marks and Examples.
87. S. Forth, P. Hovland, E. Phipps, J. Utke, A. Walther (eds.), it Recent
Advances in Algorithmic Diﬀerentiation.
88. J. Garcke, M. Griebel (eds.), Sparse Grids and Applications.
89. M. Griebel, M. A. Schweitzer (eds.), Meshfree Methods for Partial
Diﬀerential Equations VI.
90. C. Pechstein, Finite and Boundary Element Tearing and Intercon-
necting Solvers for Multiscale Problems.
91. R. Bank, M. Holst, O. Widlund, J. Xu (eds.), Domain Decomposi-
tion Methods in Science and Engineering XX.
92. H. Bijl, D. Lucor, S. Mishra, C. Schwab (eds.), Uncertainty Quan-
tiﬁcation in Computational Fluid Dynamics.
93. M. Bader, H.-J. Bungartz, T. Weinzierl (eds.), Advanced Computing.
94. M. Ehrhardt, T. Koprucki (eds.), Advanced Mathematical Models
and Numerical Techniques for Multi-Band Eﬀective Mass Approxi-
mations.
95. M. Aza¨ıez, H. El Fekih, J.S. Hesthaven (eds.), Spectral and High Or-
der Methods for Partial Diﬀerential Equations ICOSAHOM 2012.
For further information on these books please have a look at our mathematics cata-
logue at the following URL: www.springer.com/series/3527

